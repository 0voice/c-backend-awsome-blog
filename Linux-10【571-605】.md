# 【NO.571】websocket协议介绍与基于reactor模型的websocket服务器实现

## 0.前言

  本文对websocket协议与参数进行详细的介绍，并基于reactor模型实现websocket服务器

  本专栏知识点是通过零声教育的线上课学习，进行梳理总结写下文章，对c/c++linux课程感兴趣的读者，可以点击链接 C/C++后台高级服务器课程介绍 详细查看课程的服务。

## 1.websocket介绍

### 1.1 websocket是什么

  websocket是基于tcp协议的应用层协议，也就是建立在tcp协议之上的自定义协议。这个协议比http协议更加的简单，因为websocket只对协议的格式做要求，只要符合数据格式就可以使用。

  websocket一般用来服务器主动推送消息给客户端，反观HTTP，HTTP是请求响应的模式，客户端来一个请求，服务器响应一个请求，服务器无法主动发送数据给客户端；并且使用websocket，客户端和服务器只需要一次“握手”，两者之间就成功建立了长连接，可以双向传输数据。

  现在有很多网站都有推送功能，比如现在有个人关注了我的CSDN号，或者给我点了赞，只要我这个浏览器在CSDN界面，就能立刻收到提醒，这就是推送功能，一般都是按照时间间隔轮询；如果我们使用HTTP去做的话，浏览器需要不断的向服务器发请求，而HTTP请求头又有很多无用数据，显而易见的是浪费带宽等资源。

  而websocket不一样，websocket的开销很小，并且主要是由服务器主动推送消息给客户端，不再需要轮询了，所以实时性很高。

### 1.2 websocket的优点

  总结一下websocket的优点：

1.websocket协议简单
2.可以基于websocket自定义协议
3.websocket一般用来服务器主动推送消息给客户端（实时性很高）
4.客户端和服务器建立连接只需要一次“握手”就可以保持长连接（开销很小）

### 1.3 websocket应用场景

举个登陆CSDN的例子：

1.用户选择微信扫码登陆，浏览器发送HTTP请求给CSDN服务器
2.服务器返回一个二维码给浏览器
3.用户通过微信扫码登陆
4.微信扫码成功，将消息传给微信服务器进行处理
5.微信服务器触发回调给CSDN服务器发一个通知
6.CSDN服务器给浏览器发送一个websocket通知浏览器登陆成功

![在这里插入图片描述](https://img-blog.csdnimg.cn/6fb7388a063f4907aa610fc04a78ee88.png)
  访客给我的文章点了个赞，我这里立刻收到提醒，可以看到这就是服务器主动推送消息给客户端，websocket的应用。

![![在这里插入图片描述](https://img-blog.csdnimg.cn/65debf3564f54db8bcd26fc98b5bb97f.png](https://img-blog.csdnimg.cn/68b9dd5aeb6343a69edf491d9764c785.png)

### 1.4 websocket协议剖析

**握手协议**
  websocket握手是HTTP的GET请求的升级版，现在假设客户端连接服务器，服务器返回一次握手信息，连接即可建立，具体步骤如下：

1.客户端使用ws://192.168.109.100:8081连接服务器，实际上是采用HTTP的GET请求来进行握手

```
GET / HTTP/1.1

# 对端主机

Host: 192.168.109.100:8081

# 协议书升级

Connection: Upgrade
Pragma: no-cache
Cache-Control: no-cache
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36

# 升级的websocket

Upgrade: websocket
Origin: http://www.websocket-test.com

# websocket版本号

Sec-WebSocket-Version: 13
Accept-Encoding: gzip, deflate
Accept-Language: zh-CN,zh;q=0.9

# 客户端随机生成的一个16字节随机数，作为简单的认证标识

Sec-WebSocket-Key: 9jEz8msH1BBH9H43adEMZQ==
Sec-WebSocket-Extensions: permessage-deflate; client_max_window_bits
```


2.服务器接收到对应的GET请求后，发现协议需要升级，返回响应，其中需要注意的是Sec-WebSocket-Accept

服务器接收到对应的GET请求后，发现协议需要升级，返回响应，其中需要注意的是Sec-WebSocket-Accept

```
# 101 表示切换协议

HTTP/1.1 101 Switching Protocols
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Accept: 9oFmMWgFISY8DBlo5xq1L1rc0+0=
```

3.Sec-WebSocket-Accept
  Sec-WebSocket-Accept需要做3次计算得出，主要是为了验证客户端合法性。客户端收到服务器发来的响应后，同样会进行下面的操作，然后将收到的结果与自身算的结果进行对比，如果一样则说明合法，握手成功（WebSocket建立成功）。后续数据传输不再使用HTTP协议，而是使用websocket自定义的一套协议规范。



```
# GUID是websocket规定好的全局唯一的不变的字符串

#define    GUID    "258EAFA5-E914-47DA-95CA-C5AB0DC85B11"

# 将客户端发来的GET请求中的Sec-WebSocket字段的字符串与全局唯一的GUID拼接

1. key=Sec-WebSocket-Key+GUID

# 对这个新字符串做SHA1运算，得到20B长的字符串sha_key

2. sha_key=SHA1(key)

# 对sha_key做base64_encode编码，就能得到Sec-WebSocket-Accept了

3. sec_key=base64_encode(sha_key)
```



### 1.5 传输协议



![在这里插入图片描述](https://img-blog.csdnimg.cn/7c8c794b758442729ed29915a6fc9e0f.png)


  写代码的时候只需要根据图中的协议来解析数据包即可，所以要将协议的情况分清楚。

### 1.6 参数介绍

```
FIN：1bit，当FIN值为0的时候代表，消息还没完整，只是其中的一个数据包；当值为1的时候代表这段消息已经完全发送了。
RSV 1 / 2 / 3：1bit，这个默认为 0 ，如果一定要启用，就得和服务端协商好，才具有意义。
opcode：4bit，该数据包类型。
0 代表数据不完整，这只是其中的一个，不是最后的那个数据包。（Continuation Frame）

1 代表数据包内容的类型为 文本类型（Text Frame）

2 代表数据内容类型为 二进制类型 （Binary Frame）

8 代表连接断开 （Connection Close Frame）

9 和 10 是心跳检测，如果服务端发出 Ping Frame 那么客户端就得发回 Pong Frame ，如果服务端接受不到 Pong Frame 就代表客户端可能已经下线了。

0 - 15 中现在除了这 6 个，都为保留帧。
```

MASK： 1bit，是否开启掩码。1开0关。如果开启了，下面的Masking-key就有意义了，一般是发送消息的数据包会开启，返回响应的数据包不开启，并且也没由Masking-key这4个bit。

```
// 如果Mask为1，则Payload Data 就需要通过 Mask 掩码解密（这里指接收消息）
void umask(char *payload, int length, char *mask_key) {
    int i = 0;
    for (i = 0; i < length; i++) {
        payload[i] ^= mask_key[i % 4];
    }
}
```


Payload len：7bit，内容数据（Payload Data）的长度

```
如果Payload len<126,则data len=Payload len

如果Payload len=126，则启用Extended payload length，多加了16bit，并且data len=Extended payload length

如果Payload len=127，则启用Extended payload length和Extended payload length continued，多加了16+48bit，并且data len=Extended payload length continued
```

Masking-key：32bit，掩码数据，如果 Mask 为 1 就启用，否则不启用

Payload Data：数据 0-127Byte，由上面3个如果决定大小

### 1.7 大白话

  协议中前面2Byte固定存在，后面的 Extended payload length 这2Byte和 Extended payload length continued 这6Byte存不存在由 Payload len决定。

```
如果Payload len<126,则data len=Payload len

如果Payload len=126，则启用Extended payload length，多加了16bit，并且data len=Extended payload length

如果Payload len=127，则启用Extended payload length和Extended payload length continued，多加了16+48bit，并且data len=Extended payload length continued
```

  如果MASK=1，则后面有4Byte的Masking-key，则MASK=0则没有这4Byte，再后面就是Payload Data了，多长就是前面Payload len的三种情况了。

## 2.websocket四问

### 2.1 websocket协议格式

websocket协议格式一共有两种：

一种是握手的GET请求

![在这里插入图片描述](https://img-blog.csdnimg.cn/170d0aedcce14126877c809feaeb065c.png)

一种是数据传输协议格式

![在这里插入图片描述](https://img-blog.csdnimg.cn/7c8c794b758442729ed29915a6fc9e0f.png)

### 2.2 websocket如何验证客户端合法

  Sec-WebSocket-Accept需要做3次计算得出，主要是为了验证客户端合法性。客户端收到服务器发来的响应后，同样会进行下面的操作，然后将收到的结果与自身算的结果进行对比，如果一样则说明合法，握手成功（WebSocket建立成功）。

```
# GUID是websocket规定好的全局唯一的不变的字符串

#define    GUID    "258EAFA5-E914-47DA-95CA-C5AB0DC85B11"

# 将客户端发来的GET请求中的Sec-WebSocket字段的字符串与全局唯一的GUID拼接

1. key=Sec-WebSocket-Key+GUID

# 对这个新字符串做SHA1运算，得到20B长的字符串sha_key

2. sha_key=SHA1(key)

# 对sha_key做base64_encode编码，就能得到Sec-WebSocket-Accept了

3. sec_key=base64_encode(sha_key)
```

### 2.3 明文与密文如何传输

使用参数 MASK： 1bit，是否开启掩码。1开0关。

如果是要发送密文，首先将MASK置1，然后将明文数据与Masking-key进行异或操作
如果是要解码成明文，将密文数据与Masking-key进行异或操作
如果是要发送明文，MASK置0即可

```
for (i = 0; i < length; i++) {
    payload[i] ^= mask_key[i % 4];
}
```



### 2.4 websocket如何断开

  我们知道websocket是建立在TCP之上的，直接close不就好了吗，为什么还要规定opcode=8的时候代表断开连接呢？

  客户端在调用close之前，先发送一个断开连接的包给服务器；服务器接收到这个包后，把对应的fd连接数据（相关联的用户数据，业务数据）做清空，然后再调用close断开TCP，这样就是优雅的断开连接，close流畅，不会出现大量的close_wait的情况

## 3.基于reactor模型的websocket服务器

### 3.1 握手代码介绍

  websocket有3个状态，握手，传输与关闭。所以我们定义一个状态机。
  在读完数据后，将数据交由 websocket_request(ev);管理；其会判断当前连接处于哪个状态，第一次就是握手状态；
  handshake(ev);对GET请求解析出Sec-WebSocket-Key，然后计算Sec-WebSocket-Accept，组装响应。

  这里就对着上面介绍的握手协议写代码即可

```
struct ntyevent {
    //...略
    int state_machine;
};
//state_machine
enum {
    WS_HANDSHAKE = 0,
    WS_TRANSMISSION = 1,
    WS_END = 2
};
```



```
int websocket_request(struct ntyevent *ev) {
    if (ev->state_machine == WS_HANDSHAKE) {
        handshake(ev);
        ev->state_machine = WS_TRANSMISSION;
    }
    else if (ev->state_machine == WS_TRANSMISSION) {
        transmission(ev);
    }
    else {

    }

}

int recv_cb(int fd, int events, void *arg) {
    struct ntyreactor *reactor = (struct ntyreactor *) arg;
    struct ntyevent *ev = ntyreactor_find_event_idx(reactor, fd);
    memset(ev->buffer, 0, BUFFER_LENGTH);
#if 0
    long len = recv(fd, ev->buffer, BUFFER_LENGTH, 0); //
#elif 1
    int len = 0;
    int n = 0;
    while (1) {
        n = recv(fd, ev->buffer + len, BUFFER_LENGTH - len, 0);
        printf("[recv data len = %d]\n", n);
        if (n != -1) {
            len += n;
        }
        else {
            break;
        }
    }
#endif
    nty_event_del(reactor->epfd, ev);
    printf("[recv buffer total len=%d]\n", len);
    printf("buffer:[%s]\n", ev->buffer);
    if (len > 0) {
        ev->length = len;
        ev->buffer[len] = '\0';

        websocket_request(ev);
    
        nty_event_set(ev, fd, send_cb, reactor);
        nty_event_add(reactor->epfd, EPOLLOUT, ev);
    }
    else if (len == 0) {
        close(ev->fd);
    }
    else {
        close(ev->fd);
    }
    return len;

}
```



```
int base64_encode(char *in_str, int in_len, char *out_str) {
    BIO *b64, *bio;
    BUF_MEM *bptr = NULL;
    size_t size = 0;

    if (in_str == NULL || out_str == NULL)
        return -1;
    
    b64 = BIO_new(BIO_f_base64());
    bio = BIO_new(BIO_s_mem());
    bio = BIO_push(b64, bio);
    
    BIO_write(bio, in_str, in_len);
    BIO_flush(bio);
    
    BIO_get_mem_ptr(bio, &bptr);
    memcpy(out_str, bptr->data, bptr->length);
    out_str[bptr->length - 1] = '\0';
    size = bptr->length;
    
    BIO_free_all(bio);
    return size;

}

int readline(char *all_buffer, int idx, char *line_buffer) {
    int len = strlen(all_buffer);
    for (; idx < len; idx++) {
        if (all_buffer[idx] == '\r' && all_buffer[idx + 1] == '\n') {
            return idx + 2;
        }
        else {
            *(line_buffer++) = all_buffer[idx];
        }
    }
    return -1;
}

int handshake(struct ntyevent *ev) {
    char line_buffer[1024] = {0};
    char sha_key[32] = {0};//实际只需20B
    char sec_key[32] = {0};//实际只需28B
    int idx = 0;
    //找到Sec-WebSocket-Key这一行
    while (!strstr(line_buffer, "Sec-WebSocket-Key")) {
        memset(line_buffer, 0, 1024);
        idx = readline(ev->buffer, idx, line_buffer);
        if (idx == -1)return -1;
    }
    //1. key=KEY+GUID
    //2. sha_key=SHA1(key)
    //3. sec_key=base64_encode(sha_key)

    strcpy(line_buffer, line_buffer + strlen("Sec-WebSocket-Key: "));
    //1. key=KEY+GUID
    strcat(line_buffer, GUID);
    //2.sha_key = SHA1(key)
    SHA1(line_buffer, strlen(line_buffer), sha_key);
    //3. sec_key=base64_encode(sha_key)
    base64_encode(sha_key, strlen(sha_key), sec_key);
    //set head
    memset(ev->buffer, 0, BUFFER_LENGTH);
    ev->length = sprintf(ev->buffer, "HTTP/1.1 101 Switching Protocols\r\n"
                                     "Upgrade: websocket\r\n"
                                     "Connection: Upgrade\r\n"
                                     "Sec-WebSocket-Accept: %s\r\n\r\n", sec_key);
    
    printf("[handshake response]\n%s\n", ev->buffer);
    return 0;

}
```

### 3.2 传输代码介绍

  transmission函数会调用decode_packet对数据包进行解析，将有效数据长度和数据读取出来。
  之后再调用encode_packet对数据包进行封装回发回去（这里做的是echo）。

  这里就对着上面介绍的传输协议写代码即可

```
//大端
typedef struct _ws_ophdr {
    unsigned char opcode: 4,
            rsv3: 1,
            rsv2: 1,
            rsv1: 1,
            fin: 1;
    unsigned char payload_len: 7,
            mask: 1;
} ws_ophdr;

typedef struct _ws_ophdr126 {
    unsigned short payload_len;
    char mask_key[4];
} ws_ophdr126;

typedef struct _ws_ophdr127 {
    long long payload_len;
    char mask_key[4];
} ws_ophdr127;

void umask(char *payload, int length, char *mask_key) {
    int i = 0;
    for (i = 0; i < length; i++) {
        payload[i] ^= mask_key[i % 4];
    }
}

char *decode_packet(struct ntyevent *ev, int *real_len, int *virtual_len) {
    ws_ophdr *hdr = (ws_ophdr *) ev->buffer;
    printf("decode_packet fin:%d rsv1:%d rsv2:%d rsv3:%d opcode:%d mark:%d\n",
           hdr->fin,
           hdr->rsv1,
           hdr->rsv2,
           hdr->rsv3,
           hdr->opcode,
           hdr->mask);
    char *payload = NULL;
    *virtual_len = hdr->payload_len;
    if (hdr->opcode == 8) {
        ev->state_machine = WS_END;
        close(ev->fd);
        return NULL;
    }

    if (hdr->payload_len < 126) {
        payload = ev->buffer + sizeof(ws_ophdr) + 4; // 6  payload length < 126
        if (hdr->mask) {
            umask(payload, hdr->payload_len, ev->buffer + 2);
        }
        *real_len = hdr->payload_len;
    }
    else if (hdr->payload_len == 126) {
        payload = ev->buffer + sizeof(ws_ophdr) + sizeof(ws_ophdr126);
        ws_ophdr126 *hdr126 = (ws_ophdr126 *) (ev->buffer + sizeof(ws_ophdr));
        hdr126->payload_len = ntohs(hdr126->payload_len);
        if (hdr->mask) {
            umask(payload, hdr126->payload_len, hdr126->mask_key);
        }
        *real_len = hdr126->payload_len;
    }
    else if (hdr->payload_len == 127) {
        payload = ev->buffer + sizeof(ws_ophdr) + sizeof(ws_ophdr127);
        ws_ophdr127 *hdr127 = (ws_ophdr127 *) (ev->buffer + sizeof(ws_ophdr));
        if (hdr->mask) {
            umask(payload, hdr127->payload_len, hdr127->mask_key);
        }
        *real_len = hdr127->payload_len;
    }
    printf("virtual len=%d  real_len=%d\n", hdr->payload_len, *real_len);
    return payload;

}

int encode_packet(struct ntyevent *ev, int real_len, int virtual_len, char *buf) {
    ws_ophdr head = {0};
    head.fin = 1;
    head.opcode = 1;
    head.payload_len = virtual_len;
    memcpy(ev->buffer, &head, sizeof(ws_ophdr));

    int head_offset = 0;
    if (virtual_len < 126) {
        head.payload_len = real_len;
        head_offset = sizeof(ws_ophdr);
    }
    else if (virtual_len == 126) {
        ws_ophdr126 hdr126 = {0};
        hdr126.payload_len = htons(real_len);
        memcpy(ev->buffer + sizeof(ws_ophdr), &hdr126, sizeof(unsigned short));//返回不需要mask，中间去掉4B
        head_offset = sizeof(ws_ophdr) + sizeof(unsigned short);
    }
    else if (virtual_len == 127) {
        ws_ophdr127 hdr127 = {0};
        hdr127.payload_len = real_len;
        memcpy(ev->buffer + sizeof(ws_ophdr), &hdr127, sizeof(long long));//返回不需要mask，中间去掉4B
        head_offset = sizeof(ws_ophdr) + sizeof(long long);
    }
    printf("encode_packet fin:%d rsv1:%d rsv2:%d rsv3:%d opcode:%d mark:%d \n",
           head.fin,
           head.rsv1,
           head.rsv2,
           head.rsv3,
           head.opcode,
           head.mask);
    memcpy(ev->buffer + head_offset, buf, real_len);
    return head_offset + real_len;//头+payload

}

int transmission(struct ntyevent *ev) {
    char *payload_buffer = NULL;
    int real_len = 0, virtual_len;
    payload_buffer = decode_packet(ev, &real_len, &virtual_len);

    printf("real_len=[%d] , buf=[%s]\n", real_len, payload_buffer);
    
    ev->length = encode_packet(ev, real_len, virtual_len, payload_buffer);

}
```

### 3.3 程序运行测试结果

![在这里插入图片描述](https://img-blog.csdnimg.cn/c68459d94a7f4387a91a0bc4f5d7a36b.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/8d7cc635d7c94961bb03f071a16b91a1.png)

### 3.4 完整代码

```
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <arpa/inet.h>
#include <fcntl.h>
#include <unistd.h>
#include <errno.h>
#include <openssl/sha.h>
#include <openssl/pem.h>
#include <openssl/bio.h>
#include <openssl/evp.h>

#define BUFFER_LENGTH           4096
#define MAX_EPOLL_EVENTS        1024
#define SERVER_PORT             8081
#define PORT_COUNT              100
#define    GUID    "258EAFA5-E914-47DA-95CA-C5AB0DC85B11"

typedef int (*NCALLBACK)(int, int, void *);

struct ntyevent {
    int fd;
    int events;
    void *arg;

    NCALLBACK callback;
    
    int status;
    char buffer[BUFFER_LENGTH];
    int length;
    
    int state_machine;

};
//state_machine
enum {
    WS_HANDSHAKE = 0,
    WS_TRANSMISSION = 1,
    WS_END = 2
};
struct eventblock {
    struct eventblock *next;
    struct ntyevent *events;
};

struct ntyreactor {
    int epfd;
    int blkcnt;
    struct eventblock *evblk;
};


int recv_cb(int fd, int events, void *arg);

int send_cb(int fd, int events, void *arg);

struct ntyevent *ntyreactor_find_event_idx(struct ntyreactor *reactor, int sockfd);

void nty_event_set(struct ntyevent *ev, int fd, NCALLBACK callback, void *arg) {
    ev->fd = fd;
    ev->callback = callback;
    ev->events = 0;
    ev->arg = arg;
}

int nty_event_add(int epfd, int events, struct ntyevent *ev) {
    struct epoll_event ep_ev = {0, {0}};
    ep_ev.data.ptr = ev;
    ep_ev.events = ev->events = events;
    int op;
    if (ev->status == 1) {
        op = EPOLL_CTL_MOD;
    }
    else {
        op = EPOLL_CTL_ADD;
        ev->status = 1;
    }
    if (epoll_ctl(epfd, op, ev->fd, &ep_ev) < 0) {
        printf("event add failed [fd=%d], events[%d]\n", ev->fd, events);
        return -1;
    }
    return 0;
}

int nty_event_del(int epfd, struct ntyevent *ev) {
    struct epoll_event ep_ev = {0, {0}};
    if (ev->status != 1) {
        return -1;
    }
    ep_ev.data.ptr = ev;
    ev->status = 0;
    epoll_ctl(epfd, EPOLL_CTL_DEL, ev->fd, &ep_ev);
    return 0;
}

int base64_encode(char *in_str, int in_len, char *out_str) {
    BIO *b64, *bio;
    BUF_MEM *bptr = NULL;
    size_t size = 0;

    if (in_str == NULL || out_str == NULL)
        return -1;
    
    b64 = BIO_new(BIO_f_base64());
    bio = BIO_new(BIO_s_mem());
    bio = BIO_push(b64, bio);
    
    BIO_write(bio, in_str, in_len);
    BIO_flush(bio);
    
    BIO_get_mem_ptr(bio, &bptr);
    memcpy(out_str, bptr->data, bptr->length);
    out_str[bptr->length - 1] = '\0';
    size = bptr->length;
    
    BIO_free_all(bio);
    return size;

}

int readline(char *all_buffer, int idx, char *line_buffer) {
    int len = strlen(all_buffer);
    for (; idx < len; idx++) {
        if (all_buffer[idx] == '\r' && all_buffer[idx + 1] == '\n') {
            return idx + 2;
        }
        else {
            *(line_buffer++) = all_buffer[idx];
        }
    }
    return -1;
}

int handshake(struct ntyevent *ev) {
    char line_buffer[1024] = {0};
    char sha_key[32] = {0};//实际只需20B
    char sec_key[32] = {0};//实际只需28B
    int idx = 0;
    //找到Sec-WebSocket-Key这一行
    while (!strstr(line_buffer, "Sec-WebSocket-Key")) {
        memset(line_buffer, 0, 1024);
        idx = readline(ev->buffer, idx, line_buffer);
        if (idx == -1)return -1;
    }
    //1. key=KEY+GUID
    //2. sha_key=SHA1(key)
    //3. sec_key=base64_encode(sha_key)

    strcpy(line_buffer, line_buffer + strlen("Sec-WebSocket-Key: "));
    //1. key=KEY+GUID
    strcat(line_buffer, GUID);
    //2.sha_key = SHA1(key)
    SHA1(line_buffer, strlen(line_buffer), sha_key);
    //3. sec_key=base64_encode(sha_key)
    base64_encode(sha_key, strlen(sha_key), sec_key);
    //set head
    memset(ev->buffer, 0, BUFFER_LENGTH);
    ev->length = sprintf(ev->buffer, "HTTP/1.1 101 Switching Protocols\r\n"
                                     "Upgrade: websocket\r\n"
                                     "Connection: Upgrade\r\n"
                                     "Sec-WebSocket-Accept: %s\r\n\r\n", sec_key);
    
    printf("[handshake response]\n%s\n", ev->buffer);
    return 0;

}

//暂时的小端
typedef struct _ws_ophdr {
    unsigned char opcode: 4,
            rsv3: 1,
            rsv2: 1,
            rsv1: 1,
            fin: 1;
    unsigned char payload_len: 7,
            mask: 1;
} ws_ophdr;

typedef struct _ws_ophdr126 {
    unsigned short payload_len;
    char mask_key[4];
} ws_ophdr126;

typedef struct _ws_ophdr127 {
    long long payload_len;
    char mask_key[4];
} ws_ophdr127;

void umask(char *payload, int length, char *mask_key) {
    int i = 0;
    for (i = 0; i < length; i++) {
        payload[i] ^= mask_key[i % 4];
    }
}

char *decode_packet(struct ntyevent *ev, int *real_len, int *virtual_len) {
    ws_ophdr *hdr = (ws_ophdr *) ev->buffer;
    printf("decode_packet fin:%d rsv1:%d rsv2:%d rsv3:%d opcode:%d mark:%d\n",
           hdr->fin,
           hdr->rsv1,
           hdr->rsv2,
           hdr->rsv3,
           hdr->opcode,
           hdr->mask);
    char *payload = NULL;
    *virtual_len = hdr->payload_len;
    if (hdr->opcode == 8) {
        ev->state_machine = WS_END;
        close(ev->fd);
        return NULL;
    }

    if (hdr->payload_len < 126) {
        payload = ev->buffer + sizeof(ws_ophdr) + 4; // 6  payload length < 126
        if (hdr->mask) {
            umask(payload, hdr->payload_len, ev->buffer + 2);
        }
        *real_len = hdr->payload_len;
    }
    else if (hdr->payload_len == 126) {
        payload = ev->buffer + sizeof(ws_ophdr) + sizeof(ws_ophdr126);
        ws_ophdr126 *hdr126 = (ws_ophdr126 *) (ev->buffer + sizeof(ws_ophdr));
        hdr126->payload_len = ntohs(hdr126->payload_len);
        if (hdr->mask) {
            umask(payload, hdr126->payload_len, hdr126->mask_key);
        }
        *real_len = hdr126->payload_len;
    }
    else if (hdr->payload_len == 127) {
        payload = ev->buffer + sizeof(ws_ophdr) + sizeof(ws_ophdr127);
        ws_ophdr127 *hdr127 = (ws_ophdr127 *) (ev->buffer + sizeof(ws_ophdr));
        if (hdr->mask) {
            umask(payload, hdr127->payload_len, hdr127->mask_key);
        }
        *real_len = hdr127->payload_len;
    }
    printf("virtual len=%d  real_len=%d\n", hdr->payload_len, *real_len);
    return payload;

}

int encode_packet(struct ntyevent *ev, int real_len, int virtual_len, char *buf) {
    ws_ophdr head = {0};
    head.fin = 1;
    head.opcode = 1;
    head.payload_len = virtual_len;
    memcpy(ev->buffer, &head, sizeof(ws_ophdr));

    int head_offset = 0;
    if (virtual_len < 126) {
        head.payload_len = real_len;
        head_offset = sizeof(ws_ophdr);
    }
    else if (virtual_len == 126) {
        ws_ophdr126 hdr126 = {0};
        hdr126.payload_len = htons(real_len);
        memcpy(ev->buffer + sizeof(ws_ophdr), &hdr126, sizeof(unsigned short));//返回不需要mask，中间去掉4B
        head_offset = sizeof(ws_ophdr) + sizeof(unsigned short);
    }
    else if (virtual_len == 127) {
        ws_ophdr127 hdr127 = {0};
        hdr127.payload_len = real_len;
        memcpy(ev->buffer + sizeof(ws_ophdr), &hdr127, sizeof(long long));//返回不需要mask，中间去掉4B
        head_offset = sizeof(ws_ophdr) + sizeof(long long);
    }
    printf("encode_packet fin:%d rsv1:%d rsv2:%d rsv3:%d opcode:%d mark:%d \n",
           head.fin,
           head.rsv1,
           head.rsv2,
           head.rsv3,
           head.opcode,
           head.mask);
    memcpy(ev->buffer + head_offset, buf, real_len);
    return head_offset + real_len;//头+payload

}

int transmission(struct ntyevent *ev) {
    char *payload_buffer = NULL;
    int real_len = 0, virtual_len;
    payload_buffer = decode_packet(ev, &real_len, &virtual_len);

    printf("real_len=[%d] , buf=[%s]\n", real_len, payload_buffer);
    
    ev->length = encode_packet(ev, real_len, virtual_len, payload_buffer);

}

int websocket_request(struct ntyevent *ev) {
    if (ev->state_machine == WS_HANDSHAKE) {
        handshake(ev);
        ev->state_machine = WS_TRANSMISSION;
    }
    else if (ev->state_machine == WS_TRANSMISSION) {
        transmission(ev);
    }
    else {

    }

}

int recv_cb(int fd, int events, void *arg) {
    struct ntyreactor *reactor = (struct ntyreactor *) arg;
    struct ntyevent *ev = ntyreactor_find_event_idx(reactor, fd);
    memset(ev->buffer, 0, BUFFER_LENGTH);
#if 0
    long len = recv(fd, ev->buffer, BUFFER_LENGTH, 0); //
#elif 1
    int len = 0;
    int n = 0;
    while (1) {
        n = recv(fd, ev->buffer + len, BUFFER_LENGTH - len, 0);
        printf("[recv data len = %d]\n", n);
        if (n != -1) {
            len += n;
        }
        else {
            break;
        }
    }
#endif
    nty_event_del(reactor->epfd, ev);
    printf("[recv buffer total len=%d]\n", len);
    printf("buffer:[%s]\n", ev->buffer);
    if (len > 0) {
        ev->length = len;
        ev->buffer[len] = '\0';

        websocket_request(ev);
    
        nty_event_set(ev, fd, send_cb, reactor);
        nty_event_add(reactor->epfd, EPOLLOUT, ev);
    }
    else if (len == 0) {
        close(ev->fd);
    }
    else {
        close(ev->fd);
    }
    return len;

}


int send_cb(int fd, int events, void *arg) {
    struct ntyreactor *reactor = (struct ntyreactor *) arg;
    struct ntyevent *ev = ntyreactor_find_event_idx(reactor, fd);
    printf("[send buffer]\n%s\n", ev->buffer);

    int len = send(fd, ev->buffer, ev->length, 0);
    if (len > 0) {
        nty_event_del(reactor->epfd, ev);
        nty_event_set(ev, fd, recv_cb, reactor);
        nty_event_add(reactor->epfd, EPOLLIN, ev);
    }
    else {
        nty_event_del(reactor->epfd, ev);
        close(ev->fd);
    }
    return len;

}

int accept_cb(int fd, int events, void *arg) {//非阻塞
    struct ntyreactor *reactor = (struct ntyreactor *) arg;
    if (reactor == NULL) return -1;

    struct sockaddr_in client_addr;
    socklen_t len = sizeof(client_addr);
    
    int clientfd;
    if ((clientfd = accept(fd, (struct sockaddr *) &client_addr, &len)) == -1) {
        printf("accept: %s\n", strerror(errno));
        return -1;
    }
    if ((fcntl(clientfd, F_SETFL, O_NONBLOCK)) < 0) {
        printf("%s: fcntl nonblocking failed, %d\n", __func__, MAX_EPOLL_EVENTS);
        return -1;
    }
    struct ntyevent *event = ntyreactor_find_event_idx(reactor, clientfd);
    
    nty_event_set(event, clientfd, recv_cb, reactor);
    event->status = WS_HANDSHAKE;
    nty_event_add(reactor->epfd, EPOLLIN, event);
    
    printf("new connect [%s:%d], pos[%d]\n",
           inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), clientfd);
    return 0;

}

int init_sock(short port) {
    int fd = socket(AF_INET, SOCK_STREAM, 0);
    fcntl(fd, F_SETFL, O_NONBLOCK);
    struct sockaddr_in server_addr;
    memset(&server_addr, 0, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(port);

    bind(fd, (struct sockaddr *) &server_addr, sizeof(server_addr));
    
    if (listen(fd, 20) < 0) {
        printf("listen failed : %s\n", strerror(errno));
    }
    return fd;

}


int ntyreactor_alloc(struct ntyreactor *reactor) {
    if (reactor == NULL) return -1;
    if (reactor->evblk == NULL) return -1;

    struct eventblock *blk = reactor->evblk;
    while (blk->next != NULL) {
        blk = blk->next;
    }
    
    struct ntyevent *evs = (struct ntyevent *) malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    if (evs == NULL) {
        printf("ntyreactor_alloc ntyevents failed\n");
        return -2;
    }
    memset(evs, 0, (MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    
    struct eventblock *block = (struct eventblock *) malloc(sizeof(struct eventblock));
    if (block == NULL) {
        printf("ntyreactor_alloc eventblock failed\n");
        return -2;
    }
    memset(block, 0, sizeof(struct eventblock));
    
    block->events = evs;
    block->next = NULL;
    
    blk->next = block;
    reactor->blkcnt++; //
    return 0;

}

struct ntyevent *ntyreactor_find_event_idx(struct ntyreactor *reactor, int sockfd) {
    int blkidx = sockfd / MAX_EPOLL_EVENTS;

    while (blkidx >= reactor->blkcnt) {
        ntyreactor_alloc(reactor);
    }
    int i = 0;
    struct eventblock *blk = reactor->evblk;
    while (i++ < blkidx && blk != NULL) {
        blk = blk->next;
    }
    return &blk->events[sockfd % MAX_EPOLL_EVENTS];

}


int ntyreactor_init(struct ntyreactor *reactor) {
    if (reactor == NULL) return -1;
    memset(reactor, 0, sizeof(struct ntyreactor));

    reactor->epfd = epoll_create(1);
    if (reactor->epfd <= 0) {
        printf("create epfd in %s err %s\n", __func__, strerror(errno));
        return -2;
    }
    
    struct ntyevent *evs = (struct ntyevent *) malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    if (evs == NULL) {
        printf("ntyreactor_alloc ntyevents failed\n");
        return -2;
    }
    memset(evs, 0, (MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    
    struct eventblock *block = (struct eventblock *) malloc(sizeof(struct eventblock));
    if (block == NULL) {
        printf("ntyreactor_alloc eventblock failed\n");
        return -2;
    }
    memset(block, 0, sizeof(struct eventblock));
    
    block->events = evs;
    block->next = NULL;
    
    reactor->evblk = block;
    reactor->blkcnt = 1;
    return 0;

}

int ntyreactor_destory(struct ntyreactor *reactor) {
    close(reactor->epfd);
    //free(reactor->events);

    struct eventblock *blk = reactor->evblk;
    struct eventblock *blk_next = NULL;
    
    while (blk != NULL) {
        blk_next = blk->next;
        free(blk->events);
        free(blk);
        blk = blk_next;
    }
    return 0;

}


int ntyreactor_addlistener(struct ntyreactor *reactor, int sockfd, NCALLBACK *acceptor) {
    if (reactor == NULL) return -1;
    if (reactor->evblk == NULL) return -1;

    struct ntyevent *event = ntyreactor_find_event_idx(reactor, sockfd);
    
    nty_event_set(event, sockfd, acceptor, reactor);
    nty_event_add(reactor->epfd, EPOLLIN, event);
    return 0;

}


_Noreturn int ntyreactor_run(struct ntyreactor *reactor) {
    if (reactor == NULL) return -1;
    if (reactor->epfd < 0) return -1;
    if (reactor->evblk == NULL) return -1;

    struct epoll_event events[MAX_EPOLL_EVENTS + 1];
    
    int i;
    
    while (1) {
        int nready = epoll_wait(reactor->epfd, events, MAX_EPOLL_EVENTS, 1000);
        if (nready < 0) {
            printf("epoll_wait error, exit\n");
            continue;
        }
        for (i = 0; i < nready; i++) {
            struct ntyevent *ev = (struct ntyevent *) events[i].data.ptr;
            if ((events[i].events & EPOLLIN) && (ev->events & EPOLLIN)) {
                ev->callback(ev->fd, events[i].events, ev->arg);
            }
            if ((events[i].events & EPOLLOUT) && (ev->events & EPOLLOUT)) {
                ev->callback(ev->fd, events[i].events, ev->arg);
            }
        }
    }

}


// <remoteip, remoteport, localip, localport,protocol>
int main(int argc, char *argv[]) {
    unsigned short port = SERVER_PORT; // listen 8081
    if (argc == 2) {
        port = atoi(argv[1]);
    }
    struct ntyreactor *reactor = (struct ntyreactor *) malloc(sizeof(struct ntyreactor));
    ntyreactor_init(reactor);
    int i = 0;
    int sockfds[PORT_COUNT] = {0};
    for (i = 0; i < PORT_COUNT; i++) {
        sockfds[i] = init_sock(port + i);
        ntyreactor_addlistener(reactor, sockfds[i], accept_cb);
    }
    ntyreactor_run(reactor);
    ntyreactor_destory(reactor);
    for (i = 0; i < PORT_COUNT; i++) {
        close(sockfds[i]);
    }
    free(reactor);
    return 0;
}
```



————————————————
版权声明：本文为CSDN博主「cheems~」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_42956653/article/details/125701357

# 【NO.572】redis7.0源码阅读（三）：哈希表扩容、缩容以及rehash

## 1.哈希冲突

当哈希值相同的时候会发生哈希冲突，可以通过拉链法，将将他们通过链表连接起来，链式哈希会产生一个问题，随着哈希表数据越来越多，哈希冲突越来越多，单个哈希桶链表上数据越来越多，查找时间复杂度退化到 O(n)，查找耗时增加，效率降低

![在这里插入图片描述](https://img-blog.csdnimg.cn/d46f94e1656e4a6c9082a00c05bba9ea.png)

可以通过负载因子(used/size)来表述哈希冲突的激烈程度,负载因子越大，冲突越激烈。
size表示哈希表的大小，也就是哈希桶的个数
used表示有多少个 键值对实体(dictEntry)，used越多，哈希冲突的情况越多

![在这里插入图片描述](https://img-blog.csdnimg.cn/0dbdb12584754e1fb9279ed3af51df73.png)

因此在必要时对哈希表进行扩容，将新添加的键值对，放入新的哈希桶，从而减少哈希冲突的情况

## 2.哈希表扩容

### 2.1 定位到相应的代码部分

首先要，找到对应代码,在setCommand中主要是设置string，在这里面可以找到。
_dictKeyIndex是为了获取要添加dict的索引位置，会出现扩容的情况

![在这里插入图片描述](https://img-blog.csdnimg.cn/8cb31a6072334d4285fa0d30b2bdfca8.png)

里面有_dictExpandIfNeeded这个函数，进行扩容判断和扩容

### 2.2 扩容条件

_dictExpandIfNeeded中判断扩容可分为3个情况

情况1：
在rehash的时候，是不能扩容的

情况2：
如果第一张哈希表的大小是0，就根据初始值DICT_HT_INITIAL_SIZE（默认为4）给它扩容

情况3：
条件1：负载因子大于1(d->ht_used[0] >= DICTHT_SIZE(d->ht_size_exp[0]))
条件2：允许扩容dict_can_resize（没有写时复制的时候）或者负载因子大于指定阈值dict_force_resize_ratio（默认为5）
当两条件都满足，才会进行扩容

```
static int _dictExpandIfNeeded(dict *d)
{
    /* Incremental rehashing already in progress. Return. */
    if (dictIsRehashing(d)) return DICT_OK;

    /* If the hash table is empty expand it to the initial size. */
    if (DICTHT_SIZE(d->ht_size_exp[0]) == 0) return dictExpand(d, DICT_HT_INITIAL_SIZE);
    
    /* If we reached the 1:1 ratio, and we are allowed to resize the hash
     * table (global setting) or we should avoid it but the ratio between
     * elements/buckets is over the "safe" threshold, we resize doubling
     * the number of buckets. */
    if (d->ht_used[0] >= DICTHT_SIZE(d->ht_size_exp[0]) &&
        (dict_can_resize ||
         d->ht_used[0]/ DICTHT_SIZE(d->ht_size_exp[0]) > dict_force_resize_ratio) &&
        dictTypeExpandAllowed(d))
    {
        return dictExpand(d, d->ht_used[0] + 1);
    }
    return DICT_OK;

}
```

上述条件2中可以看到dict_can_resize，那什么时候会修改dict_can_resize呢？

```
void dictEnableResize(void) {
    dict_can_resize = 1;
}

void dictDisableResize(void) {
    dict_can_resize = 0;
}
```

如果有子进程的情况下，dict_can_resize = 0
没有子进程的情况下，dict_can_resize = 1

```
/* This function is called once a background process of some kind terminates,

 * as we want to avoid resizing the hash tables when there is a child in order
 * to play well with copy-on-write (otherwise when a resize happens lots of
 * memory pages are copied). The goal of this function is to update the ability
 * for dict.c to resize the hash tables accordingly to the fact we have an
 * active fork child running. */
   void updateDictResizePolicy(void) {
    if (!hasActiveChildProcess())
        dictEnableResize();
    else
        dictDisableResize();
   }
```

为什么会fork子进程？redis有4种持久化方案，其中3种都是通过fork来进行持久化。

**写时复制（copy on write）：fork持久化**
父进程由于执行命令对页B1进行修改，指向内存中的原页B。子进程会指向页B副本（对原页B进行拷贝），来保证持久化。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2fc9967797ed4f27871a431339a13e5a.png)

### 2.3 扩容大小

通过<<左移位运算保证是2的n次幂，扩容到大于dictEntry的数量used为止。
注意下面的size并不是哈希桶的数量，而是d->ht_used[0] + 1，也就是dictEntry的数量

比如：哈希表（数组）长度为4，但是里面有9个哈希键值对（dictEntry），那么应扩容到16（比9大的最小2的n次幂）

```
static signed char _dictNextExp(unsigned long size)
{
    unsigned char e = DICT_HT_INITIAL_EXP;

    if (size >= LONG_MAX) return (8*sizeof(long)-1);
    while(1) {
        if (((unsigned long)1<<e) >= size)
            return e;
        e++;
    }

}
```

## 3.哈希表缩容(resize)

在tryResizeHashTables中通过htNeedsResize判断受否需要缩容，需要的话就重新调整大小（缩容）dictResize

```
void tryResizeHashTables(int dbid) {
    if (htNeedsResize(server.db[dbid].dict))
        dictResize(server.db[dbid].dict);
    if (htNeedsResize(server.db[dbid].expires))
        dictResize(server.db[dbid].expires);
}
```



### 3.1 缩容的条件 htNeedsResize

负载因子（used/size）小于0.1就会进行缩容，默认HASHTABLE_MIN_FILL=10,把100移过来就是0.1

比如哈希表size大小是32，那么实际used数量为3的时候，3/32<0.1那么会进行缩容

```
int htNeedsResize(dict *dict) {
    long long size, used;

    size = dictSlots(dict);
    used = dictSize(dict);
    return (size > DICT_HT_INITIAL_SIZE &&
            (used*100/size < HASHTABLE_MIN_FILL));

}
```

### 3.2 如何缩容 dictResize

```
int dictResize(dict *d)
{
    unsigned long minimal;

    if (!dict_can_resize || dictIsRehashing(d)) return DICT_ERR;
    minimal = d->ht_used[0];
    if (minimal < DICT_HT_INITIAL_SIZE)
        minimal = DICT_HT_INITIAL_SIZE;
    return dictExpand(d, minimal);

}
```

### 3.3 什么时候需要缩容呢？ databasesCron

Cron表示定时检查的意思
!hasActiveChildProcess()在没有子进程的时候，会尝试缩容(resize)

```
void databasesCron(void) {
    /* Expire keys by random sampling. Not required for slaves
     * as master will synthesize DELs for us. */
    if (server.active_expire_enabled) {
        if (iAmMaster()) {
            activeExpireCycle(ACTIVE_EXPIRE_CYCLE_SLOW);
        } else {
            expireSlaveKeys();
        }
    }

    /* Defrag keys gradually. */
    activeDefragCycle();
    
    /* Perform hash tables rehashing if needed, but only if there are no
     * other processes saving the DB on disk. Otherwise rehashing is bad
     * as will cause a lot of copy-on-write of memory pages. */
    if (!hasActiveChildProcess()) {//没有子进程的时候才会进行缩容
        /* We use global counters so if we stop the computation at a given
         * DB we'll be able to start from the successive in the next
         * cron loop iteration. */
        static unsigned int resize_db = 0;
        static unsigned int rehash_db = 0;
        int dbs_per_call = CRON_DBS_PER_CALL;
        int j;
    
        /* Don't test more DBs than we have. */
        if (dbs_per_call > server.dbnum) dbs_per_call = server.dbnum;
    
        /* Resize */
        for (j = 0; j < dbs_per_call; j++) {
            tryResizeHashTables(resize_db % server.dbnum);
            resize_db++;
        }
    
        /* Rehash */
        if (server.activerehashing) {
            for (j = 0; j < dbs_per_call; j++) {
                int work_done = incrementallyRehash(rehash_db);
                if (work_done) {
                    /* If the function did some work, stop here, we'll do
                     * more at the next cron loop. */
                    break;
                } else {
                    /* If this db didn't need rehash, we'll try the next one. */
                    rehash_db++;
                    rehash_db %= server.dbnum;
                }
            }
        }
    }

}
```



## 4.rehash

### 4.1 为什么需要rehash？

链式哈希会产生一个问题，随着哈希表数据越来越多，哈希冲突越来越多，单个哈希桶链表上数据越来越多，查找时间复杂度退化到 O(n)。
因此redis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突

四、rehash
1.为什么需要rehash？
链式哈希会产生一个问题，随着哈希表数据越来越多，哈希冲突越来越多，单个哈希桶链表上数据越来越多，查找时间复杂度退化到 O(n)。
因此redis 会对哈希表做 rehash 操作。rehash 也就是增加现有的哈希桶数量，让逐渐增多的 entry 元素能在更多的桶之间分散保存，减少单个桶中的元素数量，从而减少单个桶中的冲突

### 4.2 dictRehash

n就是移动的哈希桶是数量(从哈希表1移动到哈希表2)，也就是哈希表（数组）移动槽位

```
int dictRehash(dict *d, int n) {
    int empty_visits = n*10; /* Max number of empty buckets to visit. */ //最多访问n*10个空桶，否则会阻塞过长时间
    if (!dictIsRehashing(d)) return 0;

    while(n-- && d->ht_used[0] != 0) {
        dictEntry *de, *nextde;
    
        /* Note that rehashidx can't overflow as we are sure there are more
         * elements because ht[0].used != 0 */
        assert(DICTHT_SIZE(d->ht_size_exp[0]) > (unsigned long)d->rehashidx);
        while(d->ht_table[0][d->rehashidx] == NULL) {
            d->rehashidx++;
            if (--empty_visits == 0) return 1;
        }
        de = d->ht_table[0][d->rehashidx];//要移动的哈希桶
        /* Move all the keys in this bucket from the old to the new hash HT */
        while(de) {//遍历ht0中哈希桶中每个k-v对，都进行重新哈希
            uint64_t h;
    
            nextde = de->next;
            /* Get the index in the new hash table */
            h = dictHashKey(d, de->key) & DICTHT_SIZE_MASK(d->ht_size_exp[1]);
            de->next = d->ht_table[1][h];//如果一个哈希桶有多个元素就将dict接入哈希桶的头部
            d->ht_table[1][h] = de;
            d->ht_used[0]--;
            d->ht_used[1]++;
            de = nextde;
        }
        d->ht_table[0][d->rehashidx] = NULL;//删除哈希桶
        d->rehashidx++;
    }
    
    /* Check if we already rehashed the whole table... */
    if (d->ht_used[0] == 0) {
        zfree(d->ht_table[0]);
        /* Copy the new ht onto the old one */
        d->ht_table[0] = d->ht_table[1];
        d->ht_used[0] = d->ht_used[1];
        d->ht_size_exp[0] = d->ht_size_exp[1];
        _dictReset(d, 1);
        d->rehashidx = -1;
        return 0;
    }
    
    /* More to rehash... */
    return 1;

}
```


设置了empty_visits，最多访问n*10个空桶，否则会阻塞过长时间
移动过来的dictEntry都是往哈希桶的链表头部插入的

设置了empty_visits，最多访问n*10个空桶，否则会阻塞过长时间
移动过来的dictEntry都是往哈希桶的链表头部插入的

关键部分：
一般情况下, h=hash(key)%size,也就是根据哈希值取余，来确定存放在哪个哈希桶位置上。
由于新的哈希表size都是2的n次幂，因此可以优化成h=hash(key)&(size-1)(位运算速度更快)
rehash的时候，将哈希表1中的一个桶的每个键值对dictEntry都通过这种方式，放到哈希表2指定的哈希桶下面。

```
h = dictHashKey(d, de->key) & DICTHT_SIZE_MASK(d->ht_size_exp[1]);
```



### 4.3 渐进式rehash

如果服务器一口气，将第一个hashtable移动到第二个hashtable上，就会出现等待响应的时间过长，因此可以将一个大的任务拆分成一个个小的任务，一步一步挪，对一个哈希桶（同一个哈希桶中多个元素用一个链表保存）进行挪到第二个哈希table去，也就是每次只挪哈希表中的一个槽位（哈希桶）。

可以看到dictRehash(d,1)，参数1就是挪动的哈希桶个数为1

```
static void _dictRehashStep(dict *d) {
    if (d->pauserehash == 0) dictRehash(d,1);
}
```


什么时候会进行渐进式rehash呢？
当有很多命令的时候，比如增删改查的时候，需要渐进式rehash，这样响应时间就比较短。但是当空闲时间的时候，就可以进行大步大步挪了（hashtable1移动到hashtable2上）。

### 4.4 定时任务触发rehash

当定时任务检测到服务器空闲的时候，就可以大步移动hashtable1到hashtable2

dictRehashMilliseconds(dict *d, int ms)就是rehash移动指定毫秒时间，dictRehash(d,100)可以看到，一次挪100个槽位（哈希桶）
因此在指定毫秒时间内，每次挪动100个哈希桶，直到超时了为止。

```
/* Rehash in ms+"delta" milliseconds. The value of "delta" is larger 

 * than 0, and is smaller than 1 in most cases. The exact upper bound 

 * depends on the running time of dictRehash(d,100).*/
   int dictRehashMilliseconds(dict *d, int ms) {
    if (d->pauserehash > 0) return 0;

    long long start = timeInMilliseconds();
    int rehashes = 0;

    while(dictRehash(d,100)) {
        rehashes += 100;
        if (timeInMilliseconds()-start > ms) break;
    }
    return rehashes;
   }
```





### 4.5 rehash未完成的时候如何去查找数据呢

首先在第一张哈希表查找，如果没有，然后再去第二章哈希表查找

### 4.6 rehash结束后要做的事情

当哈希表1已经搬空了d->ht_used[0] == 0，说明rehash已经结束了
这一步主要是将哈希表1指向哈希表2，然后将哈希表2指向的清空（也就是说把rehash后的结果全部从哈希表2还给哈希表1了）
可以理解为哈希表1在rehash过程中，通过哈希表2临时存储rehash还未完成的结果，完成后把结果给哈希表1

```
/* Check if we already rehashed the whole table... */
    if (d->ht_used[0] == 0) {
        zfree(d->ht_table[0]);
        /* Copy the new ht onto the old one */
        d->ht_table[0] = d->ht_table[1];
        d->ht_used[0] = d->ht_used[1];
        d->ht_size_exp[0] = d->ht_size_exp[1];
        _dictReset(d, 1);
        d->rehashidx = -1;
        return 0;
    }

```

## 5.总结：哈希扩容和rehash

下图是扩容和rehash的结果

![在这里插入图片描述](https://img-blog.csdnimg.cn/cae784be799a4092afe2b48661163e73.png)

放在哪个哈希桶上，取决于h=hash(key)%size

比如：原先哈希表大小为4，新的为8
原先对应索引为0的，新的只可能对应索引0或者4
原先索引为2的，新的只可能是2或者6
————————————————
版权声明：本文为CSDN博主「菊头蝙蝠」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_21539375/article/details/124655399

# 【NO.573】eBPF学习 - 入门

## 1.BPF和eBPF是什么？

BPF是Berkeley Packet Filter（伯克利数据包过滤器）得缩写，诞生于1992年，其作用是提升网络包过滤工具得性能，并于2014年正式并入Linux内核主线。
BPF提供一种在各种内核事件和应用程序事件发生时允许运行一小段程序的机制，使得内核完全可编程，允许用户定制和控制他们的系统以解决相应的问题。
BPF是一项灵活而高效的技术，由指令集、存储对象和辅助函数等几部分组成。其采用了虚拟指令集规范，运行时BPF模块提供两个执行机制：解释器和即时编译器（JIT）。在实际执行前，BPF指令必须通过验证器（verifer）的安全性检查以确保BPF程序自身不会崩溃或者损坏内核。
扩展后的BPF通常缩写为eBPF，但是官方的说法仍然是BPF，并且内核中也只有一个执行引擎即BPF（扩展后的BPF）。

## 2.相关概念

跟踪(tracing)：基于事件的记录-BPF工具所使用的监测方式。
采样(sampilng)：通过获取全部观测的子集来绘制目标的大致图像。也被称为profiling即性能刨析样本，它是基于计时器来对运行中的代码进行定时采样。
可观测性(observability)：通过全面观测来理解一个系统，包括跟踪工具、采样工具和基于固定计数器的工具。
BCC(BPF编译器集合，BPF Compiler Collection)：最早用来开发BPF跟踪程序的高级框架。它提供了一个编写内核BPF程序的C语言环境，还提供Python、Lua和C++环境来实现用户接口。
bpftrace：创建BPF工具的高级语言支持。
动态跟踪技术（动态插装技术）：在生产环境中对正在运行的软件插入观测点的能力。

## 3.BCC工具图一览

![在这里插入图片描述](https://img-blog.csdnimg.cn/0b03f4781cd244af918c1409bec9f013.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAY29uc2ljZSBjb2Rl,size_20,color_FFFFFF,t_70,g_se,x_16)

## 4.快速入门

### 4.1 环境搭建

这里我们使用vagrant搭建虚拟机来快速搭建一个环境关于vagrant的使用请参考：https://blog.csdn.net/msdnchina/article/details/118461544

```
vagrant box add ubuntu/impish64

# 创建和启动Ubuntu 21.10虚拟机

vagrant init ubuntu/impish64
vagrant up

# 登录到虚拟机

vagrant ssh
```


然后我们再安装一些必备的组件

```
apt-get update
apt-get install -y make clang llvm libelf-dev git libbpf-dev bpfcc-tools libbpfcc-dev linux-tools-$(uname -r) linux-headers-$(uname -r)
apt-get install build-essential -y gdb libtool pkg-config automake autoconf wget
sudo apt install strace tar gcc-multilib
```



### 4.2 Hello ebpf

在开始前我们先来看下eBPF的开发步骤：

### 4.3 开发eBPF步骤

使用C开发一个eBPF程序
借助LLVM将eBPF程序编译成字节码
通过bpf系统调用将BPF程序提交给内核
内核验证并运行BPF字节码，并把相应得状态映射到BPF映射中
用户程序通过BPF映射查询BPF字节码的运行状态


开始开发一个eBPF程序
我们这里使用eBPF的高级封装BCC来开发一个eBPF程序

1.首先我们使用C开发一个eBPF程序hello.c

```
int hello_world(void *ctx)
{
    bpf_trace_printk("Hello, World!");
    return 0;
}
```


2.然后使用python编写用户态代码hello.py

```
#!/usr/bin/env python3

# 1. 导入BCC库

from bcc import BPF

# 2. 加载BPF程序

b = BPF(src_file="hello.c")

# 3. 将 BPF 程序挂载到内核探针（简称 kprobe）

b.attach_kprobe(event="do_sys_openat", fn_name="hello_world")

# 4. 读取内核调试文件 /sys/kernel/debug/tracing/trace_pipe 的内容，并打印到标准输出中

b.trace_print()
```

3.执行hellp.py

```
# python3 hello.py

b'              ps-2098    [002] d...   307.515292: bpf_trace_printk: Hello, World!'
b'              sh-2099    [001] d...   307.529035: bpf_trace_printk: Hello, World!'
b'              sh-2099    [001] d...   307.529155: bpf_trace_printk: Hello, World!'
b'     cpuUsage.sh-2100    [002] d...   307.531789: bpf_trace_printk: Hello, World!'
b'     cpuUsage.sh-2100    [002] d...   307.531938: bpf_trace_printk: Hello, World!'
```

trace_print输出的格式可由 /sys/kernel/debug/tracing/trace_options 来修改。比如前面这个默认的输出中，每个字段的含义如下所示：

ps-2098：表示进程名字和PID
[002]：表示CPU编号
[d…]：表示一系列选项
307.515292：表示时间戳
bpf_trace_printk：表示函数名
Hello, World!： bpf_trace_printk传入的字符串

### 4.4 使用BPF映射来获取文件信息

BPF 程序可以利用 BPF 映射（map）进行数据存储，而用户程序也需要通过 BPF 映射，同运行在内核中的 BPF 程序进行交互。BCC 定义了一系列的库函数和辅助宏定义。比如，你可以使用 BPF_PERF_OUTPUT 来定义一个 Perf 事件类型的 BPF 映射。
我们可以通过这些辅助函数将我们需要的数据映射到用户态程序中

**1.使用bpf辅助函数映射数据**

```
// 包含头文件
#include <uapi/linux/openat2.h>
#include <linux/sched.h>

// 定义数据结构
struct data_t {
  u32 pid;
  u64 ts;
  char comm[TASK_COMM_LEN];
  char fname[NAME_MAX];
};

// 定义性能事件映射
BPF_PERF_OUTPUT(events);


// 定义kprobe处理函数
int hello_world(struct pt_regs *ctx, int dfd, const char __user * filename, struct open_how *how)
{
  struct data_t data = { };

  // 获取进程的 TGID 和 PID，这儿定义的 data.pid 数据类型为 u32，所以高 32 位舍弃掉后就是进程的 PID
  data.pid = bpf_get_current_pid_tgid();
  // 获取系统自启动以来的时间，单位是纳秒
  data.ts = bpf_ktime_get_ns();

  // 获取进程名
  if (bpf_get_current_comm(&data.comm, sizeof(data.comm)) == 0)
  {
    //从中指针处读取固定大小的数据
    bpf_probe_read(&data.fname, sizeof(data.fname), (void *)filename);
  }

  // 提交性能事件
  events.perf_submit(ctx, &data, sizeof(data));
  return 0;
}
```

**2.在用户态中读取内核提交的性能事件信息**

```
from bcc import BPF

# 加载

b = BPF(src_file="hello.c")
b.attach_kprobe(event="do_sys_openat2", fn_name="hello_world")

# 打印头  TIME(s)            COMM             PID    FILE    

print("%-18s %-16s %-6s %-16s" % ("TIME(s)", "COMM", "PID", "FILE"))

# 定义性能事件回调函数

start = 0
def print_event(cpu, data, size):
    global start
    event = b["events"].event(data)
    if start == 0:
            start = event.ts
    time_s = (float(event.ts - start)) / 1000000000
    print("%-18.9f %-16s %-6d %-16s" % (time_s, event.comm, event.pid, event.fname))

# 注册性能事件回调函数

b["events"].open_perf_buffer(print_event)

# 循环等待性能事件回调

while 1:
    try:
        #读取映射内容，并执行回调函数
        b.perf_buffer_poll()
    except KeyboardInterrupt:
        exit()
```

**3.执行程序**

```
# python3 hello.py

<command line>:3:9: note: previous definition is here
#define __HAVE_BUILTIN_BSWAP16__ 1
        ^
3 warnings generated.
TIME(s)            COMM             PID    FILE            
0.000000000        b'node'          1429   b'/root/.vscode-server/data/User/workspaceStorage/f610d604c637ea4c98b5ca477b61ef94/vscode.lock'
0.013446614        b'node'          1265   b'/proc/6209/cmdline'
0.191411986        b'node'          1421   b'/proc/meminfo'
```



## 5.eBPF工作原理

eBPF是一个运行在内核的虚拟机，为了确保在内核中安全地执行，eBPF 只提供了非常有限的指令集。这些指令集可用于完成一部分内核的功能，但却远不足以模拟完整的计算机。为了更高效地与内核进行交互，eBPF 指令还有意采用了 C 调用约定，其提供的辅助函数可以在 C 语言中直接调用，极大地方便了 eBPF 程序的开发。eBPF运行时内部结构如图，主要由5大模块组成

第一个模块是eBPF 辅助函数。它提供了一系列用于 eBPF 程序与内核其他模块进行交互的函数。这些函数并不是任意一个 eBPF 程序都可以调用的，具体可用的函数集由 BPF 程序类型决定。
第二个模块是 eBPF 验证器。它用于确保 eBPF 程序的安全。验证器会将待执行的指令创建为一个有向无环图（DAG），确保程序中不包含不可达指令；接着再模拟指令的执行过程，确保不会执行无效指令。
第三个模块是由 11 个 64 位寄存器、一个程序计数器和一个 512 字节的栈组成的存储模块。这个模块用于控制 eBPF 程序的执行。其中，R0 寄存器用于存储函数调用和 eBPF 程序的返回值，这意味着函数调用最多只能有一个返回值；R1-R5 寄存器用于函数调用的参数，因此函数调用的参数最多不能超过 5 个；而 R10 则是一个只读寄存器，用于从栈中读取数据。
第四个模块是即时编译器，它将 eBPF 字节码编译成本地机器指令，以便更高效地在内核中执行。
第五个模块是 BPF 映射（map），它用于提供大块的存储。这些存储可被用户空间程序用来进行访问，进而控制 eBPF 程序的运行状态。

### 5.1 bpftool

Linux内核在4.15之后添加了bpftool这个工具可以用来查看和操作BPF对象，包括BPF程序和对应的映射表。它的源码位于Linux源代码的tools/bpf/bpftool中

### 5.2 命令简述

bpftool由对象和命令组成,在内核5.13中bpftool的对象包括 prog | map | link | cgroup | perf | net | feature | btf | gen | struct_ops | iter
选项支持：

-j、–json
-p、–pretty
-f、–bpffs
-m、–mapcompat
-n、-nomount

```
bpftool [OPTIONS] OBJECT {COMMAND | help}
```


对于每一类对象都有一个帮助文档，比如

```
# bpftool prog help

Usage: /usr/lib/linux-tools/5.13.0-39-generic/bpftool prog { show | list } [PROG]
       /usr/lib/linux-tools/5.13.0-39-generic/bpftool prog dump xlated PROG [{ file FILE | opcodes | visual | linum }]
       /usr/lib/linux-tools/5.13.0-39-generic/bpftool prog dump jited  PROG [{ file FILE | opcodes | linum }]
       /usr/lib/linux-tools/5.13.0-39-generic/bpftool prog pin   PROG FILE
       /usr/lib/linux-tools/5.13.0-39-generic/bpftool prog { load | loadall } OBJ  PATH \
                         [type TYPE] [dev NAME] \
                         [map { idx IDX | name NAME } MAP]\
                         [pinmaps MAP_DIR]
       /usr/lib/linux-tools/5.13.0-39-generic/bpftool prog attach PROG ATTACH_TYPE [MAP]
       /usr/lib/linux-tools/5.13.0-39-generic/bpftool prog detach PROG ATTACH_TYPE [MAP]
       /usr/lib/linux-tools/5.13.0-39-generic/bpftool prog run PROG \
                         data_in FILE \
                         [data_out FILE [data_size_out L]] \
                         [ctx_in FILE [ctx_out FILE [ctx_size_out M]]] \
                         [repeat N]
       /usr/lib/linux-tools/5.13.0-39-generic/bpftool prog profile PROG [duration DURATION] METRICs
       /usr/lib/linux-tools/5.13.0-39-generic/bpftool prog tracelog
       /usr/lib/linux-tools/5.13.0-39-generic/bpftool prog help

       MAP := { id MAP_ID | pinned FILE | name MAP_NAME }
       PROG := { id PROG_ID | pinned FILE | tag PROG_TAG | name PROG_NAME }
       TYPE := { socket | kprobe | kretprobe | classifier | action |
                 tracepoint | raw_tracepoint | xdp | perf_event | cgroup/skb |
                 cgroup/sock | cgroup/dev | lwt_in | lwt_out | lwt_xmit |
                 lwt_seg6local | sockops | sk_skb | sk_msg | lirc_mode2 |
                 sk_reuseport | flow_dissector | cgroup/sysctl |
                 cgroup/bind4 | cgroup/bind6 | cgroup/post_bind4 |
                 cgroup/post_bind6 | cgroup/connect4 | cgroup/connect6 |
                 cgroup/getpeername4 | cgroup/getpeername6 |
                 cgroup/getsockname4 | cgroup/getsockname6 | cgroup/sendmsg4 |
                 cgroup/sendmsg6 | cgroup/recvmsg4 | cgroup/recvmsg6 |
                 cgroup/getsockopt | cgroup/setsockopt | cgroup/sock_release |
                 struct_ops | fentry | fexit | freplace | sk_lookup }
       ATTACH_TYPE := { msg_verdict | stream_verdict | stream_parser |
                        flow_dissector }
       METRIC := { cycles | instructions | l1d_loads | llc_misses | itlb_misses | dtlb_misses }
       OPTIONS := { {-j|--json} [{-p|--pretty}] | {-f|--bpffs} |
                {-m|--mapcompat} | {-n|--nomount} }


```

### 5.3 bpftool perf

perf子命令用来显示哪些BPF程序正在通过perf_event_open进行挂载。

```
# bpftool perf

pid 16377  fd 5: prog_id 32  kprobe  func do_sys_openat2  offset 0
```

### 5.4 bpftool prog show

bpftool prog show会列出所有的程序

```
# bpftool prog show

3: cgroup_device  tag e3dbd137be8d6168  gpl
	loaded_at 2022-04-17T08:33:56+0000  uid 0
	xlated 504B  jited 309B  memlock 4096B
4: cgroup_skb  tag 6deef7357e7b4530  gpl
	loaded_at 2022-04-17T08:33:56+0000  uid 0
	xlated 64B  jited 54B  memlock 4096B
5: cgroup_skb  tag 6deef7357e7b4530  gpl
	loaded_at 2022-04-17T08:33:56+0000  uid 0
	xlated 64B  jited 54B  memlock 4096B
6: cgroup_device  tag 0ecd07b7b633809f  gpl
	loaded_at 2022-04-17T08:34:00+0000  uid 0
	xlated 496B  jited 307B  memlock 4096B
7: cgroup_skb  tag 6deef7357e7b4530  gpl
	loaded_at 2022-04-17T08:34:00+0000  uid 0
	xlated 64B  jited 54B  memlock 4096B
8: cgroup_skb  tag 6deef7357e7b4530  gpl
	loaded_at 2022-04-17T08:34:00+0000  uid 0
	xlated 64B  jited 54B  memlock 4096B
9: cgroup_device  tag ee0e253c78993a24  gpl
	loaded_at 2022-04-17T08:34:04+0000  uid 0
	xlated 416B  jited 255B  memlock 4096B
10: cgroup_device  tag e3dbd137be8d6168  gpl
	loaded_at 2022-04-17T08:34:06+0000  uid 0
	xlated 504B  jited 309B  memlock 4096B
11: cgroup_device  tag c8b47a902f1cc68b  gpl
	loaded_at 2022-04-17T08:34:06+0000  uid 0
	xlated 464B  jited 288B  memlock 4096B
12: cgroup_device  tag 8b9c33f36f812014  gpl
	loaded_at 2022-04-17T08:34:08+0000  uid 0
	xlated 744B  jited 447B  memlock 4096B
13: cgroup_skb  tag 6deef7357e7b4530  gpl
	loaded_at 2022-04-17T08:34:08+0000  uid 0
	xlated 64B  jited 54B  memlock 4096B
14: cgroup_skb  tag 6deef7357e7b4530  gpl
	loaded_at 2022-04-17T08:34:08+0000  uid 0
	xlated 64B  jited 54B  memlock 4096B
32: kprobe  name hello_world  tag 38dd440716c4900f  gpl
	loaded_at 2022-04-17T11:09:18+0000  uid 0
	xlated 104B  jited 70B  memlock 4096B
	btf_id 66
```

### 5.5 bpftool prog dump xlated

xlated将BPF指令翻译为汇编指令打印出来。

```
# bpftool prog dump xlated id 32

int hello_world(void * ctx):
; int hello_world(void *ctx)
   0: (b7) r1 = 33
; ({ char _fmt[] = "Hello, World!"; bpf_trace_printk_(_fmt, sizeof(_fmt)); });
   1: (6b) *(u16 *)(r10 -4) = r1
   2: (b7) r1 = 1684828783
   3: (63) *(u32 *)(r10 -8) = r1
   4: (18) r1 = 0x57202c6f6c6c6548
   6: (7b) *(u64 *)(r10 -16) = r1
   7: (bf) r1 = r10
; 
   8: (07) r1 += -16
; ({ char _fmt[] = "Hello, World!"; bpf_trace_printk_(_fmt, sizeof(_fmt)); });
   9: (b7) r2 = 14
  10: (85) call bpf_trace_printk#-61904
; return 0;
  11: (b7) r0 = 0
  12: (95) exit
```


正是我们前面写的 C 代码，而其他行则是具体的 BPF 指令。具体每一行的 BPF 指令又分为三部分：

第一部分，冒号前面的数字 0-12 ，代表 BPF 指令行数；
第二部分，括号中的 16 进制数值，表示 BPF 指令码。它的具体含义你可以参考 IOVisor BPF 文档，比如第 0 行的 0xb7 表示为 64 位寄存器赋值。
第三部分，括号后面的部分，就是 BPF 指令的伪代码。



## 6.BPF系统调用

在用户态与内核进行交互必须要使用系统调用来完成，在eBPF程序中使用的系统调

```
// cmd代表操作命令，比如BPF_PROG_LOAD是加载eBPF程序
// attr代表bpf_attr类型的eBPF属性指针，不同类型的操作命令需要传入不同的属性参数
// size代表属性的大小
int bpf(int cmd, union bpf_attr *attr, unsigned int size);
```


在内核5.13中已经支持了以下命令

```
enum bpf_cmd {
	BPF_MAP_CREATE,
	BPF_MAP_LOOKUP_ELEM,
	BPF_MAP_UPDATE_ELEM,
	BPF_MAP_DELETE_ELEM,
	BPF_MAP_GET_NEXT_KEY,
	BPF_PROG_LOAD,
	BPF_OBJ_PIN,
	BPF_OBJ_GET,
	BPF_PROG_ATTACH,
	BPF_PROG_DETACH,
	BPF_PROG_TEST_RUN,
	BPF_PROG_GET_NEXT_ID,
	BPF_MAP_GET_NEXT_ID,
	BPF_PROG_GET_FD_BY_ID,
	BPF_MAP_GET_FD_BY_ID,
	BPF_OBJ_GET_INFO_BY_FD,
	BPF_PROG_QUERY,
	BPF_RAW_TRACEPOINT_OPEN,
	BPF_BTF_LOAD,
	BPF_BTF_GET_FD_BY_ID,
	BPF_TASK_FD_QUERY,
	BPF_MAP_LOOKUP_AND_DELETE_ELEM,
	BPF_MAP_FREEZE,
	BPF_BTF_GET_NEXT_ID,
	BPF_MAP_LOOKUP_BATCH,
	BPF_MAP_LOOKUP_AND_DELETE_BATCH,
	BPF_MAP_UPDATE_BATCH,
	BPF_MAP_DELETE_BATCH,
	BPF_LINK_CREATE,
	BPF_LINK_UPDATE,
	BPF_LINK_GET_FD_BY_ID,
	BPF_LINK_GET_NEXT_ID,
	BPF_ENABLE_STATS,
	BPF_ITER_CREATE,
	BPF_LINK_DETACH,
	BPF_PROG_BIND_MAP,
};
```


其常用的命令如下：

![image-20230303172118775](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230303172118775.png)
BPF_MAP_UPDATE_ELEM
BPF_MAP_DELETE_ELEM
F_MAP_LOOKUP_AND_DELETE_ELEM
BPF_MAP_GET_NEXT_KEY | BPF映射相关的操作命令 |
| BPF_PROG_LOAD | 验证并加载BPF程序 |
| BPF_PROG_ATTACH
BPF_PROG_DETACH | 挂载/协助BPF程序 |
| BPF_OBJ_PIN | 把BPF程序或映射挂载到sysfs中的/sys/fs/bpf目录中 |
| BPF_OBJ_GET | 从/sys/fs/bpf目录中从查找BPF程序 |
| BPF_BTF_LOAD | 验证并加载BTF信息 |

## 7.BPF辅助函数

eBPF 程序并不能随意调用内核函数，因此，内核定义了一系列的辅助函数，用于 eBPF 程序与内核其他模块进行交互。比如 bpf_trace_printk() 就是最常用的一个辅助函数，用于向调试文件系统（/sys/kernel/debug/tracing/trace_pipe）写入调试信息。
eBPF 内部的内存空间只有寄存器和栈。所以，要访问其他的内核空间或用户空间地址，就需要借助 bpf_probe_read 这一系列的辅助函数。这些函数会进行安全性检查，并禁止缺页中断的发生。
而在 eBPF 程序需要大块存储时，就不能像常规的内核代码那样去直接分配内存了，而是必须通过 BPF 映射（BPF Map）来完成。
并不是所有的辅助函数都可以在 eBPF 程序中随意使用，不同类型的 eBPF 程序所支持的辅助函数是不同的。比如，对于 Hello World 示例这类内核探针（kprobe）类型的 eBPF 程序，你可以在命令行中执行 bpftool feature probe ，来查询当前系统支持的辅助函数列表：

```
# bpftool feature probe

Scanning system configuration...
bpf() syscall restriction has unknown value 2
JIT compiler is enabled
JIT compiler hardening is disabled
JIT compiler kallsyms exports are enabled for root
Global memory limit for JIT compiler for unprivileged users is 264241152 bytes
CONFIG_BPF is set to y
CONFIG_BPF_SYSCALL is set to y
CONFIG_HAVE_EBPF_JIT is set to y
CONFIG_BPF_JIT is set to y
CONFIG_BPF_JIT_ALWAYS_ON is set to y
CONFIG_DEBUG_INFO_BTF is set to y
CONFIG_DEBUG_INFO_BTF_MODULES is set to y
CONFIG_CGROUPS is set to y
CONFIG_CGROUP_BPF is set to y
CONFIG_CGROUP_NET_CLASSID is set to y
CONFIG_SOCK_CGROUP_DATA is set to y
CONFIG_BPF_EVENTS is set to y
CONFIG_KPROBE_EVENTS is set to y
CONFIG_UPROBE_EVENTS is set to y
CONFIG_TRACING is set to y
CONFIG_FTRACE_SYSCALLS is set to y
CONFIG_FUNCTION_ERROR_INJECTION is set to y
CONFIG_BPF_KPROBE_OVERRIDE is set to y
CONFIG_NET is set to y
CONFIG_XDP_SOCKETS is set to y
CONFIG_LWTUNNEL_BPF is set to y
CONFIG_NET_ACT_BPF is set to m
CONFIG_NET_CLS_BPF is set to m
CONFIG_NET_CLS_ACT is set to y
CONFIG_NET_SCH_INGRESS is set to m
CONFIG_XFRM is set to y
CONFIG_IP_ROUTE_CLASSID is set to y
CONFIG_IPV6_SEG6_BPF is set to y
CONFIG_BPF_LIRC_MODE2 is not set
CONFIG_BPF_STREAM_PARSER is set to y
CONFIG_NETFILTER_XT_MATCH_BPF is set to m
CONFIG_BPFILTER is set to y
CONFIG_BPFILTER_UMH is set to m
CONFIG_TEST_BPF is set to m
CONFIG_HZ is set to 250

Scanning system call availability...
bpf() syscall is available

Scanning eBPF program types...
eBPF program_type socket_filter is available
eBPF program_type kprobe is available
eBPF program_type sched_cls is available
eBPF program_type sched_act is available
eBPF program_type tracepoint is available
eBPF program_type xdp is available
eBPF program_type perf_event is available
eBPF program_type cgroup_skb is available
eBPF program_type cgroup_sock is available
eBPF program_type lwt_in is available
eBPF program_type lwt_out is available
eBPF program_type lwt_xmit is available
eBPF program_type sock_ops is available
eBPF program_type sk_skb is available
eBPF program_type cgroup_device is available
eBPF program_type sk_msg is available
eBPF program_type raw_tracepoint is available
eBPF program_type cgroup_sock_addr is available
eBPF program_type lwt_seg6local is available
eBPF program_type lirc_mode2 is NOT available
eBPF program_type sk_reuseport is available
eBPF program_type flow_dissector is available
eBPF program_type cgroup_sysctl is available
eBPF program_type raw_tracepoint_writable is available
eBPF program_type cgroup_sockopt is available
eBPF program_type tracing is NOT available
eBPF program_type struct_ops is available
eBPF program_type ext is NOT available
eBPF program_type lsm is NOT available
eBPF program_type sk_lookup is available

Scanning eBPF map types...
...
```

## 8.BPF映射

BPF 映射用于提供大块的键值存储，这些存储可被用户空间程序访问，进而获取 eBPF 程序的运行状态。eBPF 程序最多可以访问 64 个不同的 BPF 映射，并且不同的 eBPF 程序也可以通过相同的 BPF 映射来共享它们的状态。
创建BPF映射的最直接方法是使用bpf系统调用。如果该系统调用的第一个参数设置为BPF_MAP_CREATE，则表示创建一个新的映射。该调用将返回与创建映射相关的文件描述符。bpf系统调用的第二个参数是BPF映射的设置，如下所示：

```
union bpf_attr {
  struct {
    __u32 map_type;
    __u32 key_size;
    __u32 value_size;
    __u32 max_entries;
    __u32 map_flags;
  };
}
```


其中最关键的是映射的类型，在内核头文件 include/uapi/linux/bpf.h 中的 bpf_map_type 定义了所有支持的映射类型，你可以使用如下的 bpftool 命令，来查询当前系统支持哪些映射类型：

```
#  bpftool feature probe | grep map_type

eBPF map_type hash is available
eBPF map_type array is available
eBPF map_type prog_array is available
eBPF map_type perf_event_array is available
eBPF map_type percpu_hash is available
eBPF map_type percpu_array is available
eBPF map_type stack_trace is available
eBPF map_type cgroup_array is available
eBPF map_type lru_hash is available
eBPF map_type lru_percpu_hash is available
eBPF map_type lpm_trie is available
eBPF map_type array_of_maps is available
eBPF map_type hash_of_maps is available
eBPF map_type devmap is available
eBPF map_type sockmap is available
eBPF map_type cpumap is available
eBPF map_type xskmap is available
eBPF map_type sockhash is available
eBPF map_type cgroup_storage is available
eBPF map_type reuseport_sockarray is available
eBPF map_type percpu_cgroup_storage is available
eBPF map_type queue is available
eBPF map_type stack is available
eBPF map_type sk_storage is available
eBPF map_type devmap_hash is available
eBPF map_type struct_ops is NOT available
eBPF map_type ringbuf is available
eBPF map_type inode_storage is available
eBPF map_type task_storage is available
```


下表列出了一些常用的BPF映射：

![image-20230303171954053](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230303171954053.png)


版权声明：本文为CSDN博主「fun binary」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/hzb869168467/article/details/124239015

# 【NO.574】Nginx源码阅读：避免惊群以及负载均衡的原理与具体实现

## 1.惊群效应

### 1.1 发生惊群效应的原因

#### 1.1.1 使用accept

主进程（master进程）fork出⼀批⼦进程（worker进程），⼦进程继承了⽗进程的监听端⼝（sockfd），就会出现accept惊群效应。子进程的fd属于同一个文件，若两个⼦进程同时调⽤accept进⾏阻塞监听，两个进程都会被挂起来，内核会在这个socket的等待队列wait queue链表中将两个PID记录下来以便唤醒。Linux2.6版本之后引⼊了⼀个标记为WQ_FLAG_EXCLUSIVE解决了这种惊群效应。这个在内核就已经处理了。

#### 1.1.2 使用epoll监听公共的端口

epoll与直接accept不同，epoll需要先调⽤epoll_create在内核中创建⼀个epollfd
epoll会把当前进程挂在fd的等待队列下，但是默认情况下这种挂载不会设置互斥标志，意思着当设备有事情产⽣进⾏等待队列唤醒的时候，如果当前队列有多个进程在等待，则会全部唤醒，当多个进程共享同⼀个监听端⼝并且都使⽤epoll进⾏多路复⽤的监听时，epoll将这些进程都挂在同⼀个等待队列下。

在linux的后续版本中，解决了这个问题，通过设置标志位，使得只会唤醒队列中的⼀个进程。

既然linux内核中都解决了惊群效应，为什么nginx还去实现一下？
因此内核解决比较晚，nginx很早就使用该方法去避免惊群效应了，并且为了适应不同内核版本，一直保留着，用于避免惊群效应。

### 1.2 Nginx中惊群惊群效应

每个进程都有一个reactor（fork出来的）也就是epoll，都监听着一个公共的listenfd。如果该端口有一个（连接）事件时，只有一个进程能accept成功，那么其他epoll_wait都要被唤醒，这样多个子进程在 accept 建立新连接时会有争抢，且子进程数量越多问题越明显，从而造成系统性能下降的现象。

### 1.3 Nginx中惊群效应的解决方案

多个进程的epoll都监听公共的端口会出现惊群现象，那么在nginx中有采用accept_mutex的办法，轮流去监听epoll中的建立连接的事件，保证同一时刻只有一个进程在监听listenfd（用于建立连接，进行accept）

普通事件不会出现惊群效应吗？
出现惊群效应是由于多个进程都监听同一个fd，如listenfd，在接受新的连接（accept）时候，都是监听listenfd，是同一个文件。但是对于普通的读写事件来说，就是clientfd，多个进程中的epoll不会出现重复监听同一个clientfd。也就是说，每个clientfd只可能在某个进程中的epoll中

### 1.4 Nginx中的源码

#### 1.4.1 简述避免惊群效应的源码

**1.对进程加锁**

尝试加锁，加锁成功后会附加上一个标志位NGX_POST_EVENTS

![在这里插入图片描述](https://img-blog.csdnimg.cn/81e2bef003494d749898c04a41fd5b87.png)

**2.通过标志位，将事件加入到队列中**

后续执行ngx_epoll_process_events，才会把事件（accept_events或者events）分别加入到队列中(ngx_posted_accept_events和ngx_posted_events)，只有加入到队列中的事件，在后续才能执行

![在这里插入图片描述](https://img-blog.csdnimg.cn/e2e8eaec99e34d8a92f537619b32e815.png)

**3.执行队列中的事件，并解锁**

接下去就是将队列中的事件执行了，先执行accept事件，然后解锁，再执行普通事件

![在这里插入图片描述](https://img-blog.csdnimg.cn/1f189049a5c4488894702d45a3b859a6.png)

**4.概括：**

如果进程成功上锁，那么会进入NGX_POST_EVENTS状态，那么事件会延迟执行，accept事件和普通事件都会分别加入到各自的队列中，然后再执行
如果进程没有上锁成功，如果检测到普通事件，直接执行普通事件（不可能出现accept事件，只有上锁的进程的epoll才能监听到，是因为在加锁过程中还添加了listen监听事件，没有加锁的进程epoll是没法监听到的）。

![在这里插入图片描述](https://img-blog.csdnimg.cn/3c5513fbd5504a9da819bde5cfd9303d.png)

下面是源码实现的细节

#### 1.4.2 ngx_process_events_and_timers

在ngx_worker_process_cycle调用该函数。
该部分主要是对进程之间避免惊群效应和实现负载均衡
让ngx_epoll_process_events去检测事件，加入队列中（也有直接执行的情况）
然后通过ngx_event_process_posted去执行队列中的事件

```
void
ngx_process_events_and_timers(ngx_cycle_t *cycle)
{
    ngx_uint_t  flags;
    ngx_msec_t  timer, delta;
    //timer是用于epoll_wait等待的时间，后面传入的参数
    //delta 记录处理epoll_process_events花费的时间
    if (ngx_timer_resolution) {
        timer = NGX_TIMER_INFINITE;//时间设置无穷大(实际上就是ngx_msec_t的最大值）
        flags = 0;

    } else {
        flags = NGX_UPDATE_TIME;
        timer = ngx_event_find_timer();//找到最近一个定时器触发所需要的时间

#if (NGX_WIN32)

        /* handle signals from master in case of network inactivity */
    
        if (timer == NGX_TIMER_INFINITE || timer > 500) {
            timer = 500;
        }

#endif
    }

    if (ngx_use_accept_mutex) {
        //用于负载均衡，如果当前进程中连接池中连接数量较多 或者 待连接数比较少，那么进程会采用让出，不执行任务。
        if (ngx_accept_disabled > 0) {
            ngx_accept_disabled--;
    
        } else {
            //尝试加锁
            //1.如果获得锁，会将所有listenfd加入到epoll
            //2.如果没有获得锁，会删除当前进程epoll中的listenfd
            if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) {//加锁
                return;
            }
            //如果持有了锁
            if (ngx_accept_mutex_held) {
                flags |= NGX_POST_EVENTS;//获得锁后的标志位NGX_POST_EVENTS（网络读写事件延迟处理标志）
    
            } else {
                if (timer == NGX_TIMER_INFINITE
                    || timer > ngx_accept_mutex_delay)
                {
                    timer = ngx_accept_mutex_delay;
                }
            }
        }
    }
    //之前队列中剩下的一些未完成的事件放在队列ngx_posted_next_events中，也就是要优先处理的
    //把这些事件放入ngx_posted_events队列头部，并清空ngx_posted_next_events
    if (!ngx_queue_empty(&ngx_posted_next_events)) {
        ngx_event_move_posted_next(cycle);
        timer = 0;//这些任务是上一轮的,要紧急处理，后续epoll_wait要立刻返回，不延迟
    }
    
    delta = ngx_current_msec;


    //里面执行epoll_wait
    //网络连接事件，放入ngx_posted_accept_events队列
    //网络读写事件，放入ngx_posted_events队列
    (void) ngx_process_events(cycle, timer, flags);//执行ngx_epoll_process_events
    
    delta = ngx_current_msec - delta;
    
    ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle->log, 0,
                   "timer delta: %M", delta);
    //如果没有加锁成功，那么ngx_posted_accept_events会是空的               
    //执行accept事件（ngx_event_accept）
    ngx_event_process_posted(cycle, &ngx_posted_accept_events);
    
    if (ngx_accept_mutex_held) {//解锁
        ngx_shmtx_unlock(&ngx_accept_mutex);
    }
    
    ngx_event_expire_timers();
    //执行普通事件
    ngx_event_process_posted(cycle, &ngx_posted_events);

}
```

#### 1.4.3 ngx_trylock_accept_mutex

尝试加锁，非阻塞，立刻返回结果

如果加锁成功，那么将监听连接listenfd（多个端口）加入到epoll中
如果加锁失败，那么将epoll中的listenfd（多个端口）清空

```
ngx_int_t
ngx_trylock_accept_mutex(ngx_cycle_t *cycle)
{
    if (ngx_shmtx_trylock(&ngx_accept_mutex)) {//尝试加锁（通过CAS操作，将mtx->lock设置为当前worker进程的pid，没有占用则mtx->lock为0）

        ngx_log_debug0(NGX_LOG_DEBUG_EVENT, cycle->log, 0,
                       "accept mutex locked");
    
        //之前已经持有了锁，那么就直接返回，继续监听端口（ngx_accept_events 在 epoll 里不使用）
        //标志位 ngx_accept_mutex_held 为 1 表示当前进程已经获取了 ngx_accept_mutex 锁
        if (ngx_accept_mutex_held && ngx_accept_events == 0) {
            return NGX_OK;
        }
        
        // 注册accept事件（之前没有持有锁，需要注册 epoll 事件监听端口，遍历监听端口列表，加入 epoll 连接事件，开始接受请求）
        if (ngx_enable_accept_events(cycle) == NGX_ERROR) {
            ngx_shmtx_unlock(&ngx_accept_mutex);// 如果监听失败就需要立即解锁，函数结束
            return NGX_ERROR;
        }
    
        // 已经成功将监听事件加入 epoll
        ngx_accept_events = 0;
        ngx_accept_mutex_held = 1;// 设置已经获得锁的标志
    
        return NGX_OK;
    }
    // trylock失败，未获得锁
    ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle->log, 0,
                   "accept mutex lock failed: %ui", ngx_accept_mutex_held);
                   
    // 若当前进程获取 ngx_accept_mutex 锁失败，并且 ngx_accept_mutex_held 为 1，则为错误情况
    
    // 本次未获得锁，但之前持有锁，也就是说之前在监听端口
    if (ngx_accept_mutex_held) {
        // 遍历监听端口列表，删除 epoll 监听连接事件，不接受请求
        if (ngx_disable_accept_events(cycle, 0) == NGX_ERROR) {
            return NGX_ERROR;
        }
    
        ngx_accept_mutex_held = 0;//没有获得锁的标志
    }
    
    return NGX_OK;

}
```

### 1.5 补充

进程之间的锁如何实现呢？
通过CAS去设置进程之间共享内存的变量mtx->lock，如果该值是自身pid，表示锁被当前进程占有了，如果该值是0，表示没有进程占有锁.

什么时候会去争夺锁？
通过设置定时器（红黑树），获得下一个定时器触发时间，时间主要用于epoll_wait的等待，时间一到，就执行epoll_wait下面内容，执行完后，在worker process循环中又会回到ngx_process_events_and_timers，获得下一个定时器触发时间，然后去判断锁是否被占用，继续执行，以此循环。

## 2.负载均衡

在nginx不同进程之间，进程采用让出加锁机会的方式来实现负载均衡，通过当前进程拒绝监听新连接的次数ngx_accept_disabled来控制，需要让出的次数。它取决于，当前进程中的总连接数 和 待连接（空闲连接）的数量

### 2.1 实现方式

在ngx_event.c中
ngx_accept_disabled表示当前进程中拒绝accept新连接的次数，也就是说当通过定时器轮询到当前进程的时候，如果ngx_accept_disabled>0，那么就不会去获取accept_mutex锁（当前进程不会将accept_event加入epoll中去），并且ngx_accept_disabled-1

```
void
ngx_process_events_and_timers(ngx_cycle_t *cycle){
	...
		if (ngx_accept_disabled > 0) {
            ngx_accept_disabled--;

        } else {
        	//获取accept_mutex锁，用于后续accept连接
    	}
    ...

}
```


在ngx_event_accept.c有下面的内容，表示nginx单进程的所有连接总数的八分之一，减去剩下的空闲连接数量（还没连接的数量）。空闲连接数量小了，那么ngx_accept_disable越大，让出机会就更大了。

在ngx_event_accept.c有下面的内容，表示nginx单进程的所有连接总数的八分之一，减去剩下的空闲连接数量（还没连接的数量）。空闲连接数量小了，那么ngx_accept_disable越大，让出机会就更大了。

```
ngx_accept_disabled = ngx_cycle->connection_n / 8
                              - ngx_cycle->free_connection_n;
```


默认每个进程，每次只处理一条连接，如果想每次处理多条连接，需要开启multi_accept

实现进程间负载均衡的一些变量：
注意这些变量都是每个进程私有的，非共享内存的变量。
ngx_accept_disabled：当前进程连接拒绝的次数，先让出给其他进程
ngx_cycle->connection_n：当前进程已连接的总连接数，如果当前进程已经连接总数比较多的话，那么让出的情况会变大
ngx_cycle->free_connection_n：空闲连接数（待连接数量），还未连接的数量如果比较多，那么让出的情况就会变少

参考连接
https://wenku.baidu.com/view/83da1a1313661ed9ad51f01dc281e53a59025148.html
https://blog.csdn.net/u010285974/article/details/106643940
————————————————
版权声明：本文为CSDN博主「菊头蝙蝠」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_21539375/article/details/124774136

# 【NO.575】海量数据去重的hash，bitmap与布隆过滤器Bloom Filter

## 0.前言

  本文重点是bitmap和布隆过滤器

## 1.背景

在使⽤word⽂档时，word如何判断某个单词是否拼写正确？
⽹络爬⾍程序，怎么让它不去爬相同的url⻚⾯？允许有误差
垃圾邮件（短信）过滤算法如何设计？允许有误差
公安办案时，如何判断某嫌疑⼈是否在⽹逃名单中？控制误差 假阳率
缓存穿透问题如何解决？允许有误差

## 2.需求

  从海量数据中查询某字符串是否存在

## 3.红黑树


 ![在这里插入图片描述](https://img-blog.csdnimg.cn/5c0a16d380d24d118ce04957c64ff526.png) 不论是AVL还是红黑树，在“海量数据”数据面前都是不合适的，因为红黑树会将key，即数据存储起来，而海量的数据会导致内存不足。并且设计到字符串比较，效率也是很慢的。所以在这个需求下，用树相关的数据结构是不合适的。


拓展：

- c++标准库（STL）中的set和map结构都是采⽤红⿊树实现的，它增删改查的时间复杂度是O ( l o g 2 N ) 。set和map的关键区别是set不存储val字段
- 优点：存储效率⾼，访问速度⾼效
- 缺点：对于数据量⼤且查询字符串⽐较⻓且查询字符串相似时将会是噩梦

## 4.散列表hashtable

  散列表构成：数组+hash函数。它是将字符串通过hash函数⽣成⼀个整数再映射到数组当中(所以散列表不需要”比较字符串“，而红黑树需要)，它增删改查的时间复杂度是o(1)。

注意：散列表的节点中 kv 是存储在一起的

```
struct node {
	void *key;
	void *val;
	struct node *next;
};
```


  

拓展：

- c++标准库（STL）中的unordered_map<string, bool>是采⽤hashtable实现的
- hashtable中节点存储了key和val，hashtable并没有要求key的⼤⼩顺序，我们同样可以修改代码让插⼊存在的数据变成修改操作
- 优点：访问速度更快；不需要进⾏字符串⽐较
- 缺点：需要引⼊策略避免冲突，存储效率不⾼；空间换时间

## 5.hash函数


  **hash函数的作用**：避免插⼊的时候字符串的⽐较，hash函数计算出来的值通过对数组⻓度的取模能随机分布在数组当中。
  
  **hash冲突（hash碰撞）**：hash(key)=addr，hash 函数可能会把两个或两个以上的不同 key 映射到同一地址。hash函数⼀般返回的是64位整数，将多个⼤数映射到⼀个⼩数组中，必然会产⽣冲突。
  
  **负载因子**：用来形容散列表的存储密度。数组存储元素的个数 / 数据长度；负载因子越小，冲突越小，负载因子越大，冲突越大。
  
  

## 6.选择hash

如何选取hash函数？

1. 选取计算速度快

2. 强随机分布(等概率、均匀地分布在整个地址空间）

murmurhash1，murmurhash2，murmurhash3，siphash（redis6.0当中使⽤，rust等大多数语言选用的hash算法来实现hashmap），cityhash 都具备强随机分布性

测试地址如下：https://github.com/aappleby/smhasher

  siphash 主要解决字符串接近的强随机分布性,所以如果要hash字符串的话，优先选用siphash

## 7.冲突处理

- 链表法

  引用链表来处理哈希冲突，也就是将冲突元素用链表链接起来，这也是常用的处理冲突的⽅式。但是可能出现一种极端情况，冲突元素比较多，该冲突链表过长，这个时候可以将这个链表转换为红黑树。由原来链表时间复杂度 转换为红黑树时间复杂度 ，那么判断该链表过长的依据是多少？可以采⽤超过 256（经验值）个节点的时候将链表结构转换为红黑树结构。

![在这里插入图片描述](https://img-blog.csdnimg.cn/fd373b92612f4e9c8290cad1dd3ec004.png)

- 开放寻址法

  将所有的元素都存放在哈希表的数组中，不使用额外的数据结构；一般使用线性探查的思路解决

1. 当插⼊新元素的时，使⽤哈希函数在哈希表中定位元素位置

2. 检查数组中该槽位索引是否存在元素。如果该槽位为空，则插⼊，否则3
3. 在 2 检测的槽位索引上加⼀定步⻓接着检查2

加⼀定步⻓分为以下几种

![image-20230303201211520](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230303201211520.png)
  这两种都会导致同类hash聚集，也就是近似值它的hash值也近似。那么它的数组槽位也靠近，形成hash聚集。第⼀种同类聚集冲突在前，第⼆种只是将聚集冲突延后。

> 可以使⽤双重哈希来解决上⾯出现hash聚集现象
> 在.net HashTable类的hash函数Hk定义如下：
>
> Hk(key) = [GetHash(key) + k * (1 + (((GetHash(key) >> 5) + 1) % (hashsize – 1)))] % hashsize
>
> 在此 (1 + (((GetHash(key) >> 5) + 1) % (hashsize – 1))) 与 hashsize 互为素数（两数互为素数表示两者没有共同的质因⼦）
> 执⾏了 hashsize 次探查后，哈希表中的每⼀个位置都有且只有⼀次被访问到，也就是说，对于给定的 key，对哈希表中的同⼀位置不会同时使⽤ Hi 和 Hj；
> 具体原理：https://www.cnblogs.com/organic/p/6283476.html

## 8.bitmap

  先来介绍bitmap，下面再引出布隆过滤器。现在有一个需求：文件中有40亿个QQ号码，请设计算法对QQ号码去重，相同的QQ号码仅保留一个，内存限制1G。

- 如果先排序再去重，时间复杂度太高

- 如果用hashmap天然去重，空间复杂度太高
- 文件切割避免内存过大，太麻烦，效率不高
- 使用bitmap，可以顺利地同时解决时间问题和空间问题

  一个unsigned int类型，共有32位，可以标识0 ~ 31这32个整数的存在与否。两个unsigned int类型，共有64位，可以标识0 ~ 63这64个整数的存在与否。

  那么如果我们把整个整数范围都覆盖了，这样一来1代表第一个位，2代表第二个位，2的32次方代表最后一个位。40亿个数中，存在的数就在相应的位置1，其他位就是0。比如来了一个1234，就找一下第1234位，如果是1就存在，是0就不存在。

   所以我们只要有足够的”位“，就可以判断0~4亿这4亿个整数是否存在了。2的32次是4,294,967,296。也就是说我们要有4,294,967,296个位。

   4,294,967,296个位=2的32次方个位=2的29次方个字节=512MB。原来32位的整数,转化成了1位的布尔，所以数据空间就是原来的32分之一。

  可以看到，使用bitmap不但自动去重，甚至这个需求我们还排了序，从小到大遍历正整数，当bitmap位的值为1时，就输出该值，输出后的正整数序列就是排序后的结果。

m%2 n 2^n2 
n
  = m & (2 n 2^n2 
n
 -1) ------为了计算更高效，一般将 取模运算 变成 与运算

![在这里插入图片描述](https://img-blog.csdnimg.cn/1e5c764fbf4a450b878ba0c47c356730.png)

## 9.海量数据去重布隆过滤器

  红⿊树和hashtable都不能解决海量数据问题，它们都需要存储具体字符串，如果数据量⼤，提供不了⼏百G的内存；所以需要尝试探寻不存储key的⽅案，并且拥有hashtable的优点（不需要⽐较字符串）。而布隆过滤器就刚好满足这一需求，它不需要存储具体字符串，也不需要比较。时间和空间复杂度都低。

## 10.布隆过滤器介绍

**布隆过滤器定义**：布隆过滤器是⼀种概率型数据结构，它的特点是⾼效的插⼊和查询，能明确告知某个字符串⼀定不存在或者可能存在；

**优缺点：**布隆过滤器相⽐传统的查询结构（例如：hash，set，map等数据结构）更加⾼效，占⽤空间更⼩，但是确定是它返回的结果是概率性的，结果存在一定的误差，误差可控，同时不支持删除操作

**构成：**位图（bit数组）+ n个hash函数。

## 11.布隆过滤器原理

  当一个元素加入位图时：通过 K 个 hash 函数运算将这个元素映射到位图的 K 个点，并将它们置为1
  当检索一个元素时：通过通过 K 个 hash 函数运算检测位图的 K 个点是否都为1，如果都为1，则可能存在；如果有一个不为1，则一定不存在。

![在这里插入图片描述](https://img-blog.csdnimg.cn/8b6044cac51b40528a6a6e963972b4b3.png)

  为什么不支持删除操作？因为在位图中每个槽位只有两种状态（0 或者 1），一个槽位被设置为 1 状态，但不确定它被设置了多少次；也就是不知道被多少个 key 哈希映射而来以及是被具体哪个 hash 函数映射而来，所以不支持删除操作。

  如果想实现删除操作，可以用两个布隆过滤器，将删除的元素，放入第二个布隆过滤器里面，然后查询的时候去第二个里面查，如果第二个里面能查到说明可能被删除（注意也是存在误差的）。

## 12.布隆过滤如何应用

### 12.1 变量关系

  在实际应⽤过程中，布隆过滤器该如何使⽤？要选择多少个hash函数，要分配多少空间的位图，存储多少元素？另外如何控制假阳率（布隆过滤器能明确⼀定不存在，不能明确⼀定存在，那么存在的判断是有误差的，假阳率就是错误判断存在的概率）？

n - - 布隆过滤器中元素的个数，如上图 只有str1和str2 两个元素 那么 n=2。预取存储多少个值
p - - 假阳率，在0-1之间 0.0000001。能够接受的假阳率
m - - 位图所占空间
k - - hash函数的个数

```
公式如下：
n = ceil(m / (-k / log(1 - exp(log(p) / k))))
p = pow(1 - exp(-k / (m / n)), k)
m = ceil((n * log(p)) / log(1 / pow(2, log(2))))
k = round((m / n) * log(2))
```



### 12.2 确定n和p

  在实际使用布隆过滤器时，首先需要确定 n 和 p，通过上面的运算得出 m 和 k；通常可以在下面这个网站上选出合适的值

```
https://hur.st/bloomfilter/
```


  现在假设n = 4000，p = 0.000000001。我们可以自己带入公式计算m和k，也可以带入网站计算得出变量值，并且右边缩略图可以观察不同值的情况。

```
n = 4000
p = 0.000000001 (1 in 1000039473)
m = 172532 (21.06KiB)
k = 30
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/21cea7b7133544e487dac0dccc718861.png)

### 12.3 选择k个hash函数

  我们发现上面计算出需要30个hash函数，难道我们要去找30个不同的hash函数来吗，显然不该这样。我们应该选择一个 hash 函数，通过给 hash 传递不同的种子偏移值，采用线性探寻的方式构造多个 hash函数。

```
// 采⽤⼀个hash函数，给hash传不同的种⼦偏移值
// #define MIX_UINT64(v) ((uint32_t)((v>>32)^(v)))
uint64_t hash1 = MurmurHash2_x64(key, len, Seed);
uint64_t hash2 = MurmurHash2_x64(key, len, MIX_UINT64(hash1));
// k 是hash函数的个数
for (i = 0; i < k; i++) {
   Pos[i] = (hash1 + i*hash2) % m; // m 是位图的⼤⼩
}
//通过这种⽅式来模拟 k 个hash函数 跟我们前⾯开放寻址法 双重hash是⼀样的思路
```


    
    

  题外话，面试百度：hash 函数实现过程当中 为什么 会出现 i * 31?

i * 31 = i * (32-1) = i * (1<<5 -1) = i << 5 - i；
31 质数，hash 随机分布性很好

### 12.4 布隆过滤器应用场景

布隆过滤器通常用于判断某个 key 一定不存在的场景，同时允许判断存在时有误差的情况

常见处理场景：① 缓存穿透的解决；② 热 key 限流

![在这里插入图片描述](https://img-blog.csdnimg.cn/a8e6165ab80642408f28467fd2d4cbb3.png)

缓存场景：为了减轻数据库的访问压力，在server和mysql之间加入缓存来存储热点数据

缓存穿透：server请求数据时，缓存和数据库都没有该数据，最终导致压力全部给到数据库

读取步骤：

1. 先访问redis，如果存在则返回，不存在则 2

2. 访问mysql，如过不存在则返回，存在则 3
3. 将mysql的key写回redis

解决方案：

在redis设置<key,null>以此避免访问mysql，但是如果<key,null>过多会占用内存（可以设置过期时间解决这一问题）
在server端存储一个布隆过滤器，将mysql包含的key都放入布隆过滤器，布隆过滤器能过滤一定不存在的数据
————————————————
版权声明：本文为CSDN博主「cheems~」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_42956653/article/details/125581001

# 【NO.576】锁与原子操作CAS的底层实现

主要讲posix api的锁,主要
互斥锁,
自旋锁,
读写锁,
原子操作
再是在多进程的情况下面的 共享内存
还有比如说分布式锁，那个乐观锁,悲观锁,公平锁这些分布式的锁,在后面中间件讲
主要核心先把这些posix API里面这些底层的锁解释清楚，后面再讲一个自己实现一个try-catch怎么做，原理又怎么样，抛异常，捕获异常以及在finaly里面处理时这三个怎么去实现,这个异常是怎么抛的？

先思考为什么要有锁的概念，为什么要有锁呢？
因为随着在多线程,多进程这种多任务的操作系统情况下面,它避免不了多个任务在执行的时候会共享一些资源,这些资源就叫做临界资源,对于这些临界资源多个线程同时去使用的时候,那避免不了它就会出现一些意想不到的情况,a线程在执行同时b线程也在执行，会出现了一个我们根本无法预计的情况。所以对它加上一把锁为了避免副作用，异常的情况。

加锁是为了避免有副作用产生,这个副作用什么意思,就是我们避免那些我们意想不到的情况产生所以我们加锁,加锁的时候我们对这个加锁的粒度可能拿捏不稳,什么时候加多大的锁?

经典案例:多个线程同时对于一个变量进行count++;

![在这里插入图片描述](https://img-blog.csdnimg.cn/8f7abb890fdd421d9e0fdf2a6e4dfad4.png)

pthread_create(),
第一个参数线程在创建完之后会带回一个id,那这个线程id有什么用呢?我们在另外一个线程要去终止它的时候,会使用到这个id
第二个参数是线程的属性比如堆栈大小我们是可以自己定义的
第三个参数是线程的入口,
第四个参数是传到子线程的参数,
返回值等于0是成功

![在这里插入图片描述](https://img-blog.csdnimg.cn/1e8392055d3849d2b672e4634c2d6046.png)

这个地方是拿地址不是拿值

![在这里插入图片描述](https://img-blog.csdnimg.cn/767ce56bc1264093b42aed4512c282b0.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/5e550fb220d74cf885680f6650101118.png)


为什么做不到我们想要的东西。

count++里面到底发生了哪些事？

![在这里插入图片描述](https://img-blog.csdnimg.cn/969603fcca6844cdbd2d664443f1bd12.png)

一行代码翻译成汇编却有三行代码,但是由于线程的切换它变成了另外的状态

![在这里插入图片描述](https://img-blog.csdnimg.cn/2a776c5645434e8a8d72da7e59f30e85.png)

在线程2是51切换到线程1又变成了50,俩个线程执行idx++却只加了一次,所以副作用小于100万不会多于
线程切换是什么概念?
CPU每一次运行每一次指令执行的时候都是与寄存器打交道

![在这里插入图片描述](https://img-blog.csdnimg.cn/5168f47c70e544068991c0849ca63974.png)


与协程切换类似也是这样做的
这个寄存器的值我们用什么来存储,用指针,用无符号long型来存储寄存器里面的值,

把这一组寄存器的值加载上去就实现了寄存器的切换,
多核怎么做呢?原理一样,每一个CPU操作系统会为它准备一个就绪队列,运行那个就加载那个,并且切出运行完的,
**上下文切换会消耗那些资源?**无外乎所做的就是一个mov指令,看似只有这些消耗,还有关于文件系统虚拟内存相关的操作,单独从寄存器的更换上面是没有太多消耗的,消耗不大

站在一个微观的角度我们怎么加锁比较好?
第一种加锁我们在整个汇编代码段把它合并成不可分割的一段
把这三条汇编指令捆绑在一起

![在这里插入图片描述](https://img-blog.csdnimg.cn/8d7865db9b6440259da78fc86e54bb2b.png)

这个mutex的本质是什么东西?

![在这里插入图片描述](https://img-blog.csdnimg.cn/d92538e72f5941f98ee24941092a7bc4.png)

一进入这三条指令之前就给他加锁不允许另外一个线程进入,等待着这把锁释放才可以执行,
线程二发现线程一锁住了它会切换出资源执行一系列动作去休眠,进入等待状态也有表述叫阻塞状态,
mutex也知道这个过程代价比较大所以提供出来了另外一个接口叫做尝试加锁

![在这里插入图片描述](https://img-blog.csdnimg.cn/ec1d5b4189d8464589fe3a4d44fa7c57.png)

线程一以及加锁了,线程二再去时会发现已经被占,为了防止线程二休眠,就让线程二去尝试加锁一下,看它有没有被锁,如果被锁了它就不执行,不去做切换去做其它的事情,如果没有被锁它就去加锁

![在这里插入图片描述](https://img-blog.csdnimg.cn/f1fa8d20f32b4e9c8fc3dcd36efb38a0.png)

不断询问不断的轮训这把锁有没有被占用,trylock的好处至少线程不会被休眠,能够快速的第一反映知道锁有没有释放

trylock的过程有没有切换的发生,能不能发生切换?
这只是应用层循环的切换中间还是有可能引起线程的切换的
那么有没有锁在这个过程中间就一直在等没有引起线程切换?
这就是spin_lock,不去深究原理会发现spin_lock与lock调用接口是一模一样的

![在这里插入图片描述](https://img-blog.csdnimg.cn/65f784abc71e4ff0a2121f4a283f8bd6.png)


lock会引起线程切换代价比较大,spin不会就是一直敲门一直等,在使用场景上面对于时间比较长的我们会使用mutex_lock,对于时间比较短的我们会使用spin_lock,什么叫做时间长,什么叫做时间短,对于操作比较复杂的我们使用mutex,如果就是链表添加删除节点这种几行代码的我们使用spinlock,如果是对红黑树整个B树进行加锁我们采用mutex
在使用锁的时候一定要避免死锁产生,对于单核这只是纯软件的东西不会引起线程切换并不是说cpu不会切换往下执行,从锁法粒度上面lock,trylock,spin可以依次越来越小,
使用lock时可以优先使用trylock,只是trylock在写代码逻辑上面会稍微复杂一点,

线程的切换有两种情况:1.第一种时间片用完了,会进入ready就绪状态2.在等待着某种条件,比如io没有就绪或者就是在等待着某种条件,会进入wait等待的状态这两种切换是不一样的
spinlock时间片到了也不会引起切换

![在这里插入图片描述](https://img-blog.csdnimg.cn/7dc757f4e9144855b4d864fb82e9c02d.png)

有注意到现在10个线程在写,一个线程在读,在一定程度上面可以考虑使用读写锁来做,读写锁逻辑比较麻烦不太建议使用,对于读多写少的场景使用读写锁是合适的,

我们会发现一行代码翻译成3条指令的过程核心的解法就是把这几条指令合在一起,不被另外的线程拆开,就在前面加锁,这是一个加锁的概念,那有没有更好的方法我们把这三条指令就变成一条指令,压根就不用加锁的方法,接下来就是原子操作与这个CAS

volatile到底有什么用?
volatile最早的时候由于那些高低电平它在捕获的时候,它的数值在变,不是通过我们计算的方式,比如说这内存中间的一个值是通过一个串口也好或者是网线过来也好,它这里面的值在变,就是这个内存块里面的值在变,我们cpu操作的时候可以把这一个内存单元的值定义成volatile它是易变,这个易变的变的过程是由于外界客观资源所造成的它的数据是一直在变的,所以这个地方很难理解**到底定不定义volatile有什么区别?**核心点是对于volatile这个数据是改变的状态,我们在操作的时候有可能发生改变或者别的什么现象

![在这里插入图片描述](https://img-blog.csdnimg.cn/9f9642555604487e93ee75f9bf63cee0.png)


如果是简单的定义成int a=5,当另外一个线程把它修改成a=6时,a的值会出现一个现象从内存中间加载到寄存器里面,那就会出现两份,内存中间有一份,寄存器里面也有一份,而cpu肯定会优先计算寄存器里面的,所以内存中的值它影响就不会很大,我们加上volatile这个值计算的时候会优先于内存计算,如果不加它会以寄存器里面的值写回内存的方式去做,易变是直接把内存里面地址的值拿过来操作,这是两者之间的本质区别

这里写汇编的volatile与我们定义的volatile不是一个意思,

![在这里插入图片描述](https://img-blog.csdnimg.cn/9d2fd108cebb44fd9a40a082ddf9a93f.png)

把第二个参数加上第一个参数存储到第一个参数里面,
这个lock是什么意思?
在计算机体系结构里面cpu操作内存,内存块不只一个,在cpu和内存中间有一条总线,这个lock是锁总线的意思,只允许这一条指令执行完,避免了多个CPU同时去操作这一条,
这个memory什么意思呢?
在体系结构里面,cpu-高速缓存-主内存,当然这个高速缓存中间又划分很多层

![在这里插入图片描述](https://img-blog.csdnimg.cn/480787c67b394b0f87f9cd62749c3d57.png)


会出现一个什么现象呢?我们对于一个指令加载进来的时候,在CPU操作的时候在高速缓存和内存中间就会有一个一致性的问题

这四个MESI状态,
“M”:从CPU到高速缓存
“E”:高速缓存到主存
“S”:共享
“I”:

这个屏障是干啥的就是从高速缓存写回这个主内存中间,我们memory把它锁住,能够保证它落到内存的时候不受干扰,

![在这里插入图片描述](https://img-blog.csdnimg.cn/e78df2a8330e4f139b7660236134b01a.png)

这个地方old返回值不大,相当于填空题,把一个值填在这里其实没有多大意义

一行代码的力度还比不上一个函数的力度吗?函数调用是没有副作用的
我们核心的点是抓住这个副作用以及产生的效果,这里面的这个代码就是单独的把三条指令变成一条指令,

![在这里插入图片描述](https://img-blog.csdnimg.cn/ebd14b0807684c94bd9a717028c827e7.png)

汇编语法,汇编解释器

![在这里插入图片描述](https://img-blog.csdnimg.cn/a2cf2affddbf4088a1fb722ab569ae7c.png)


原子操作的力度比spin更小,但是原子操作有一个前提是需要指令系统里面有才能够去实现,比如说对于一个双向链表使用头插法虽然就只有几行代码,但是不能使用原子操作,因为没有同时对多个变量赋值的指令

![在这里插入图片描述](https://img-blog.csdnimg.cn/d4a302706a4a43f696eda6852a106df9.png)

对于类似于单例模式的这样一种结构,指令系统里面有没有可以单一的做这个事情的指令,
![在这里插入图片描述](https://img-blog.csdnimg.cn/3804a9b388ec4cae94238ffc386e0f85.png)先compare然后接下来swap赋值的意思,这里是有时间顺序的compare and swap,compare在前面,swap在后,

对于xchg这条指令如果a==b成功就返回c,否则就返回0;这就CAS的核心,所谓的无锁就是对应的原子操作,
假如问我能不能写一个线程安全的单例模式

![在这里插入图片描述](https://img-blog.csdnimg.cn/af41d26c52984651ac5837949ab7c0a1.png)

以上的分析是从多线程的角度下面思考的,那么多进程下面又该怎么办?
对于多进程按照我们刚刚那个mutex也好或者spin lock也好,我们定义一些变量加的锁,它不在一个进程里面,它的虚拟地址空间都不一样,由一份变成两份,就是我们定义变量压根就是不行的,所以对于多进程加锁的话是需要有一块共享的资源的,也就是A进程和B进程是共享这一块资源的,我们同样也是可以用原子操作,原子操作底层接口也是汇编,

![在这里插入图片描述](https://img-blog.csdnimg.cn/88c55a0df2e1498ebef7744a16fd610e.png)

这里可以创建10个子进程,break让子进程不继续创建,usleep就是让另外一个进程先走,
我们共享一个变量,使用原子操作是可行的,我们可以共享一个变量同样我们可不可以共享一个内存池,并且对它们的分配做一个统一分配和回收
这种mmap共享的方式只在父子进程之间,如果我们打开一个fd对于一个文件去做映射的话,父子进程之间是没有关系的,

![在这里插入图片描述](https://img-blog.csdnimg.cn/d2f89dbc6fee4097b7ad3cd1ed664eeb.png)

```
#include <stdio.h>
#include <pthread.h>
#include <unistd.h>

#include <sys/mman.h>

#define THREAD_SIZE     10

int count = 0;
pthread_mutex_t mutex;
pthread_spinlock_t spinlock;
pthread_rwlock_t rwlock;

// MOV dest, src;  at&t
// MOV src, dest;  x86

int inc(int *value, int add) {

    int old;
    
    __asm__ volatile ( 
        "lock; xaddl %2, %1;" // "lock; xchg %2, %1, %3;" 
        : "=a" (old)
        : "m" (*value), "a" (add)
        : "cc", "memory"
    );
    
    return old;

}


// 
void *func(void *arg) {

    int *pcount = (int *)arg;
    
    int i = 0;
    while (i++ < 100000) {

#if 0
        (*pcount) ++;
#elif 0

        pthread_mutex_lock(&mutex);
        (*pcount) ++;
        pthread_mutex_unlock(&mutex);

#elif 0

        if (0 != pthread_mutex_trylock(&mutex)) {
            i --;
            continue;
        }
        (*pcount) ++;
        pthread_mutex_unlock(&mutex);

#elif 0

        pthread_spin_lock(&spinlock);
        (*pcount) ++;
        pthread_spin_unlock(&spinlock);

#elif 0

        pthread_rwlock_wrlock(&rwlock);
        (*pcount) ++;
        pthread_rwlock_unlock(&rwlock);

#else

        inc(pcount, 1);

#endif
        usleep(1);
    }

}


int main() {
#if 0
    pthread_t threadid[THREAD_SIZE] = {0};

    pthread_mutex_init(&mutex, NULL);
    pthread_spin_init(&spinlock, PTHREAD_PROCESS_SHARED);
    pthread_rwlock_init(&rwlock, NULL);
    
    int i = 0;
    int count = 0;
    for (i = 0;i < THREAD_SIZE;i ++) {
        int ret = pthread_create(&threadid[i], NULL, func, &count);
        if (ret) {
            break;
        }
    }
    
    for (i = 0;i < 100;i ++) {
        pthread_rwlock_rdlock(&rwlock);
        printf("count --> %d\n", count);
        pthread_rwlock_unlock(&rwlock);
    
        sleep(1);
    }

#else

    int *pcount = mmap(NULL, sizeof(int), PROT_READ | PROT_WRITE, MAP_ANON|MAP_SHARED, -1, 0);


    int i = 0;
    pid_t pid = 0;
    for (i = 0;i < THREAD_SIZE;i ++) {
    
        pid = fork();
        if (pid <= 0) {
            usleep(1);
            break;
        }
    }


    if (pid > 0) { // 
    
        for (i = 0;i < 100;i ++) {
            printf("count --> %d\n",  (*pcount));
            sleep(1);
        }
    
    } else {
        
        int i = 0;
        while (i++ < 100000)  {

#if 0            
            (*pcount) ++;
#else
            inc(pcount, 1);
#endif
            usleep(1);
        }

    }




#endif 
}
```

————————————————
版权声明：本文为CSDN博主「我也要当昏君」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_46118239/article/details/124610993

# 【NO.577】http/https服务器的实现

## 1.HTTP简介

HTTP 协议（HyperText Transfer Protocol，超文本传输协议）是因特网上应用最为广泛的一种网络传输协议，所有的 WWW 文件都必须遵守这个标准。HTTP 是一个基于 TCP/IP 通信协议来传递数据（HTML 文件, 图片文件, 查询结果等）。

## 2.HTTP工作原理

HTTP 协议工作于客户端-服务端架构上。浏览器作为 HTTP 客户端通过 URL 向 HTTP 服务端即 WEB 服务器发送所有请求。
Web 服务器有：Apache 服务器，IIS 服务器（Internet Information Services）等。
Web 服务器根据接收到的请求后，向客户端发送响应信息。
HTTP 默认端口号为 80，但是你也可以改为 8080 或者其他端口。
HTTP 三点注意事项：
（1）HTTP 是无连接：无连接的含义是限制每次连接只处理一个请求。服务器处理完客户的请求，并收到客户的应答后，即断开连接。采用这种方式可以节省传输时间。
（2）HTTP 是媒体独立的：这意味着，只要客户端和服务器知道如何处理的数据内容，任何类型的数据都可以通过 HTTP 发送。客户端以及服务器指定使用适合的 MIME-type 内容类型。
（3）HTTP是无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传，这样可能导致每次连接传送的数据量增大。另一方面，在服务器不需要先前信息时它的应答就较快。
以下图表展示了 HTTP 协议通信流程：

## 3.HTTP消息结构

HTTP 是基于客户端/服务端（C/S）的架构模型，通过一个可靠的链接来交换信息，是一个无状态的请求/响应协议。
一个 HTTP"客户端"是一个应用程序（Web 浏览器或其他任何客户端），通过连接到服务器达到向服务器发送一个或多个 HTTP 的请求的目的。
一个 HTTP"服务器"同样也是一个应用程序（通常是一个 Web 服务，如 Apache Web 服务器或 IIS 服务器等），通过接收客户端的请求并向客户端发送 HTTP 响应数据。
HTTP 使用统一资源标识符（Uniform Resource Identifiers, URI）来传输数据和建立连接。一旦建立连接后，数据消息就通过类似 Internet 邮件所使用的格式[RFC5322]和多用途Internet 邮件扩展（MIME）[RFC2045]来传送。
1、客户端请求消息
客户端发送一个 HTTP 请求到服务器的请求消息包括以下格式：请求行（request line）、请求头部（header）、空行和请求数据四个部分组成，下图给出了请求报文的一般格式。

2、服务端请求消息
HTTP 响应也由四个部分组成，分别是：状态行、消息报头、空行和响应正文。

下面实例是一点典型的使用 GET 来传递数据的实例：
客户端请求：

```
GET /hello.txt HTTP/1.1
User-Agent: curl/7.16.3 libcurl/7.16.3 OpenSSL/0.9.7l zlib/1.2.3
Host: www.example.com
Accept-Language: en, mi
```


服务端响应:

```
HTTP/1.1 200 OK
Date: Mon, 27 Jul 2009 12:28:53 GMT
Server: Apache
Last-Modified: Wed, 22 Jul 2009 19:15:56 GMT
ETag: "34aa387-d-1568eb00"
Accept-Ranges: bytes
Content-Length: 51
Vary: Accept-Encoding
Content-Type: text/plain
```


输出结果：

```
Hello World! My payload includes a trailing CRLF.
```



## 4.HTTP请求方法

根据 HTTP 标准，HTTP 请求可以使用多种请求方法。
HTTP1.0 定义了三种请求方法： GET, POST 和 HEAD 方法。
HTTP1.1 新增了六种请求方法：OPTIONS、PUT、PATCH、DELETE、TRACE 和 CONNECT 方法。

## 5.HTTP状态码

当浏览者访问一个网页时，浏览者的浏览器会向网页所在服务器发出请求。当浏览器接收并显示网页前，此网页所在的服务器会返回一个包含 HTTP 状态码的信息头（server header）用以响应浏览器的请求。
HTTP 状态码的英文为 HTTP Status Code。
下面是常见的 HTTP 状态码：
（1）200 - 请求成功
（2）301 - 资源（网页等）被永久转移到其它 URL
（3）404 - 请求的资源（网页等）不存在
（4）500 - 内部服务器错误
HTTP 状态码由三个十进制数字组成，第一个十进制数字定义了状态码的类型，后两个数字没有分类的作用。HTTP 状态码共分为 5 种类型：

## 6.真实示例

http的tcp连接的生命周期：
先acceptcb、再readcb、再writecb。
1、acceptcb：建立http连接；
2、readcb：接受客户端http请求；
3、writecb：发送http响应。
本实例的重点是在reactor的基础上去实现http协议。

```
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <arpa/inet.h>

#include <fcntl.h>
#include <unistd.h>
#include <errno.h>
#include <time.h>

#include <sys/stat.h>
#include <sys/sendfile.h>

#define BUFFER_LENGTH		4096
#define MAX_EPOLL_EVENTS	1024
#define SERVER_PORT			8888
#define PORT_COUNT			1
#define HTTP_WEBSERVER_HTML_ROOT	"html"

#define HTTP_METHOD_GET		0
#define HTTP_METHOD_POST	1

typedef int NCALLBACK(int ,int, void*);

struct ntyevent {
	int fd;
	int events;
	void *arg;
	int (*callback)(int fd, int events, void *arg);
	

	int status;
	char buffer[BUFFER_LENGTH];
	int length;
	long last_active;
	
	// http param
	int method; // get / post
	char resource[BUFFER_LENGTH]; //资源位。url的长度
	int ret_code;

};

struct eventblock {
	struct eventblock *next;
	struct ntyevent *events;
};

struct ntyreactor {
	int epfd;
	int blkcnt;
	struct eventblock *evblk; //fd --> 100w
};


int recv_cb(int fd, int events, void *arg);
int send_cb(int fd, int events, void *arg);
struct ntyevent *ntyreactor_idx(struct ntyreactor *reactor, int sockfd);

void nty_event_set(struct ntyevent *ev, int fd, NCALLBACK callback, void *arg) {
	ev->fd = fd;
	ev->callback = callback;
	ev->events = 0;
	ev->arg = arg;
	ev->last_active = time(NULL);

	return ;

}


int nty_event_add(int epfd, int events, struct ntyevent *ev) {

	struct epoll_event ep_ev = {0, {0}};
	ep_ev.data.ptr = ev;
	ep_ev.events = ev->events = events;
	
	int op;
	if (ev->status == 1) {
		op = EPOLL_CTL_MOD;
	} else {
		op = EPOLL_CTL_ADD;
		ev->status = 1;
	}
	
	if (epoll_ctl(epfd, op, ev->fd, &ep_ev) < 0) {
		printf("event add failed [fd=%d], events[%d]\n", ev->fd, events);
		return -1;
	}
	
	return 0;

}

int nty_event_del(int epfd, struct ntyevent *ev) {

	struct epoll_event ep_ev = {0, {0}};
	
	if (ev->status != 1) {
		return -1;
	}
	
	ep_ev.data.ptr = ev;
	ev->status = 0;
	epoll_ctl(epfd, EPOLL_CTL_DEL, ev->fd, &ep_ev);
	
	return 0;

}


int readline(char *allbuf, int idx, char *linebuf) {//idx是读取的位置，第几行

	int len = strlen(allbuf);
	
	for(;idx < len;idx ++) {
		if (allbuf[idx] == '\r' && allbuf[idx+1] == '\n') {
			return idx+2; //  \r\n\r\n，是idx+4
		} else {
			*(linebuf++) = allbuf[idx];
		}
	}
	
	return -1;

}

int http_request(struct ntyevent *ev) { //在http数据接收完后，才处理的

	// GET, POST 先判断是GET 还是POST，DELETE HEAD不常用
	char linebuf[1024] = {0};
	int idx = readline(ev->buffer, 0, linebuf);//读一行，以\r\n分割的
	
	if (strstr(linebuf, "GET")) {
	
	    //例子： GET /index.html HTTP/1.1
	
		ev->method = HTTP_METHOD_GET;
	
		//uri, 即 /index.html
		int i = 0;
		while (linebuf[sizeof("GET ") + i] != ' ') i++;
		linebuf[sizeof("GET ")+i] = '\0';
	
		sprintf(ev->resource, "./%s/%s", HTTP_WEBSERVER_HTML_ROOT, linebuf+sizeof("GET "));
		
	} else if (strstr(linebuf, "POST")) {
	
	}

}

int recv_cb(int fd, int events, void *arg) {

	struct ntyreactor *reactor = (struct ntyreactor*)arg;
	struct ntyevent *ev = ntyreactor_idx(reactor, fd);
	
	int len = recv(fd, ev->buffer, BUFFER_LENGTH, 0); // 
	
	if (len > 0) {
		
		ev->length = len;
		ev->buffer[len] = '\0';
	
		printf("C[%d]:%s\n", fd, ev->buffer); //http
	
		http_request(ev);//这里是保证数据全部接收完成，再http request，再send_cb
	
		//send();
		
		nty_event_del(reactor->epfd, ev);
		nty_event_set(ev, fd, send_cb, reactor);
		nty_event_add(reactor->epfd, EPOLLOUT, ev);


​		

	} else if (len == 0) {
	
		nty_event_del(reactor->epfd, ev);
		close(ev->fd);
		//printf("[fd=%d] pos[%ld], closed\n", fd, ev-reactor->events);
		 
	} else {
	
		nty_event_del(reactor->epfd, ev);
		close(ev->fd);
		printf("recv[fd=%d] error[%d]:%s\n", fd, errno, strerror(errno));
		
	}
	return len;

}


int http_response(struct ntyevent *ev) {

	if (ev == NULL) return -1;
	memset(ev->buffer, 0, BUFFER_LENGTH);

#if 0
	const char *html = "<html><head><title>hello http</title></head><body><H1>King</H1></body></html>\r\n\r\n";
						  	   

	ev->length = sprintf(ev->buffer, 
		"HTTP/1.1 200 OK\r\n\
		 Date: Thu, 11 Nov 2021 12:28:52 GMT\r\n\
		 Content-Type: text/html;charset=ISO-8859-1\r\n\
		 Content-Length: 83\r\n\r\n%s", 
		 html);

#else

	printf("resource: %s\n", ev->resource);  //ev->resource即是index.html静态文件，存在磁盘上
	
	int filefd = open(ev->resource, O_RDONLY);
	if (filefd == -1) { // return 404
	
		ev->ret_code = 404;
		ev->length = sprintf(ev->buffer, 
			"HTTP/1.1 404 Not Found\r\n"
		 	"Date: Thu, 11 Nov 2021 12:28:52 GMT\r\n"
		 	"Content-Type: text/html;charset=ISO-8859-1\r\n"
			"Content-Length: 85\r\n\r\n"
		 	"<html><head><title>404 Not Found</title></head><body><H1>404</H1></body></html>\r\n\r\n" );
	
	} else {
	
		struct stat stat_buf;   //stat结构体，获取文件属性，主要是为了得到文件的总长度
		fstat(filefd, &stat_buf);
		close(filefd);
	
		if (S_ISDIR(stat_buf.st_mode)) {	
			ev->ret_code = 404; //HTTP FAILED
			ev->length = sprintf(ev->buffer, 
				"HTTP/1.1 404 Not Found\r\n"
				"Date: Thu, 11 Nov 2021 12:28:52 GMT\r\n"
				"Content-Type: text/html;charset=ISO-8859-1\r\n"
				"Content-Length: 85\r\n\r\n"
				"<html><head><title>404 Not Found</title></head><body><H1>404</H1></body></html>\r\n\r\n" ); // \n和\r，分别都算1个
		} else if (S_ISREG(stat_buf.st_mode)) {
			ev->ret_code = 200; //HTTP OK
			ev->length = sprintf(ev->buffer, 
				"HTTP/1.1 200 OK\r\n"
			 	"Date: Thu, 11 Nov 2021 12:28:52 GMT\r\n"
			 	"Content-Type: text/html;charset=ISO-8859-1\r\n"
				"Content-Length: %ld\r\n\r\n", 
			 		stat_buf.st_size );
		}
	}

#endif
	return ev->length;
}

int send_cb(int fd, int events, void *arg) {

	struct ntyreactor *reactor = (struct ntyreactor*)arg;
	struct ntyevent *ev = ntyreactor_idx(reactor, fd);
	
	http_response(ev);
	
	int len = send(fd, ev->buffer, ev->length, 0);
	if (len > 0) {
		printf("send[fd=%d], [%d]%s\n", fd, len, ev->buffer);
	
		if (ev->ret_code == 200) {
			int filefd = open(ev->resource, O_RDONLY);
			struct stat stat_buf;
			fstat(filefd, &stat_buf);
	
			sendfile(fd, filefd, NULL, stat_buf.st_size); // 发送文件，map 零拷贝
			close(filefd);
		}
		
		nty_event_del(reactor->epfd, ev);
		nty_event_set(ev, fd, recv_cb, reactor);
		nty_event_add(reactor->epfd, EPOLLIN, ev);
		
	} else {
	
		close(ev->fd);
	
		nty_event_del(reactor->epfd, ev);
		printf("send[fd=%d] error %s\n", fd, strerror(errno));
	
	}
	
	return len;

}

int accept_cb(int fd, int events, void *arg) {

	struct ntyreactor *reactor = (struct ntyreactor*)arg;
	if (reactor == NULL) return -1;
	
	struct sockaddr_in client_addr;
	socklen_t len = sizeof(client_addr);
	
	int clientfd;
	
	if ((clientfd = accept(fd, (struct sockaddr*)&client_addr, &len)) == -1) {
		if (errno != EAGAIN && errno != EINTR) {
			
		}
		printf("accept: %s\n", strerror(errno));
		return -1;
	}
	
	int flag = 0;
	if ((flag = fcntl(clientfd, F_SETFL, O_NONBLOCK)) < 0) {
		printf("%s: fcntl nonblocking failed, %d\n", __func__, MAX_EPOLL_EVENTS);
		return -1;
	}
	
	struct ntyevent *event = ntyreactor_idx(reactor, clientfd);
	
	nty_event_set(event, clientfd, recv_cb, reactor);
	nty_event_add(reactor->epfd, EPOLLIN, event);
	
	printf("new connect [%s:%d], pos[%d]\n", 
		inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), clientfd);
	
	return 0;

}

int init_sock(short port) {

	int fd = socket(AF_INET, SOCK_STREAM, 0);
	fcntl(fd, F_SETFL, O_NONBLOCK);
	
	struct sockaddr_in server_addr;
	memset(&server_addr, 0, sizeof(server_addr));
	server_addr.sin_family = AF_INET;
	server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
	server_addr.sin_port = htons(port);
	
	bind(fd, (struct sockaddr*)&server_addr, sizeof(server_addr));
	
	if (listen(fd, 20) < 0) {
		printf("listen failed : %s\n", strerror(errno));
	}
	
	return fd;

}


int ntyreactor_alloc(struct ntyreactor *reactor) {

	if (reactor == NULL) return -1;
	if (reactor->evblk == NULL) return -1;
	
	struct eventblock *blk = reactor->evblk;
	while (blk->next != NULL) {
		blk = blk->next;
	}
	
	struct ntyevent *evs = (struct ntyevent*)malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
	if (evs == NULL) {
		printf("ntyreactor_alloc ntyevents failed\n");
		return -2;
	}
	memset(evs, 0, (MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
	
	struct eventblock *block = (struct eventblock *)malloc(sizeof(struct eventblock));
	if (block == NULL) {
		printf("ntyreactor_alloc eventblock failed\n");
		return -2;
	}
	memset(block, 0, sizeof(struct eventblock));
	
	block->events = evs;
	block->next = NULL;
	
	blk->next = block;
	reactor->blkcnt ++; 
	
	return 0;

}

struct ntyevent *ntyreactor_idx(struct ntyreactor *reactor, int sockfd) {

	int blkidx = sockfd / MAX_EPOLL_EVENTS;
	
	while (blkidx >= reactor->blkcnt) {
		ntyreactor_alloc(reactor);
	}
	
	int i = 0;
	struct eventblock *blk = reactor->evblk;
	while(i ++ < blkidx && blk != NULL) {
		blk = blk->next;
	}
	
	return &blk->events[sockfd % MAX_EPOLL_EVENTS];

}


int ntyreactor_init(struct ntyreactor *reactor) {

	if (reactor == NULL) return -1;
	memset(reactor, 0, sizeof(struct ntyreactor));
	
	reactor->epfd = epoll_create(1);
	if (reactor->epfd <= 0) {
		printf("create epfd in %s err %s\n", __func__, strerror(errno));
		return -2;
	}
	
	struct ntyevent *evs = (struct ntyevent*)malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
	if (evs == NULL) {
		printf("ntyreactor_alloc ntyevents failed\n");
		return -2;
	}
	memset(evs, 0, (MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
	
	struct eventblock *block = (struct eventblock *)malloc(sizeof(struct eventblock));
	if (block == NULL) {
		printf("ntyreactor_alloc eventblock failed\n");
		return -2;
	}
	memset(block, 0, sizeof(struct eventblock));
	
	block->events = evs;
	block->next = NULL;
	
	reactor->evblk = block;
	reactor->blkcnt = 1;
	
	return 0;

}

int ntyreactor_destory(struct ntyreactor *reactor) {

	close(reactor->epfd);
	
	struct eventblock *blk = reactor->evblk;
	struct eventblock *blk_next = NULL;
	
	while (blk != NULL) {
	
		blk_next = blk->next;
	
		free(blk->events);
		free(blk);
	
		blk = blk_next;
	
	}
	
	return 0;

}



int ntyreactor_addlistener(struct ntyreactor *reactor, int sockfd, NCALLBACK *acceptor) {

	if (reactor == NULL) return -1;
	if (reactor->evblk == NULL) return -1;
	
	struct ntyevent *event = ntyreactor_idx(reactor, sockfd);
	
	nty_event_set(event, sockfd, acceptor, reactor);
	nty_event_add(reactor->epfd, EPOLLIN, event);
	
	return 0;

}



int ntyreactor_run(struct ntyreactor *reactor) {
	if (reactor == NULL) return -1;
	if (reactor->epfd < 0) return -1;
	if (reactor->evblk == NULL) return -1;
	

	struct epoll_event events[MAX_EPOLL_EVENTS+1];
	
	int checkpos = 0, i;
	
	while (1) {
		int nready = epoll_wait(reactor->epfd, events, MAX_EPOLL_EVENTS, 1000);
		if (nready < 0) {
			printf("epoll_wait error, exit\n");
			continue;
		}
	
		for (i = 0;i < nready;i ++) {
	
			struct ntyevent *ev = (struct ntyevent*)events[i].data.ptr;
	
			if ((events[i].events & EPOLLIN) && (ev->events & EPOLLIN)) {
				ev->callback(ev->fd, events[i].events, ev->arg);
			}
			if ((events[i].events & EPOLLOUT) && (ev->events & EPOLLOUT)) {
				ev->callback(ev->fd, events[i].events, ev->arg);
			}
		}
	}

}

// 3, 6w, 1, 100 == 
// <remoteip, remoteport, localip, localport>
int main(int argc, char *argv[]) {

	unsigned short port = SERVER_PORT; // listen 8888
	if (argc == 2) {
		port = atoi(argv[1]);
	}
	struct ntyreactor *reactor = (struct ntyreactor*)malloc(sizeof(struct ntyreactor));
	ntyreactor_init(reactor);
	
	int i = 0;
	int sockfds[PORT_COUNT] = {0};
	for (i = 0;i < PORT_COUNT;i ++) {
		sockfds[i] = init_sock(port+i);
	    ntyreactor_addlistener(reactor, sockfds[i], accept_cb);
	}
	
	ntyreactor_run(reactor);
	
	ntyreactor_destory(reactor);
	
	for (i = 0;i < PORT_COUNT;i ++) {
		close(sockfds[i]);
	}
	
	free(reactor);
	
	return 0;

}
```







————————————————
版权声明：本文为CSDN博主「当当响」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/zhpCSDN921011/article/details/124685893

# 【NO.578】随处可见的红黑树



## 1.背景

我们知道二叉搜索树（BST），在极端情况下，会退化为单支树，查找效率从 O(log2n) 退化为 O(n)。主要原因就是BST不够平衡(左右子树高度差太大)。既然如此，那么我们就需要通过一定的算法，将不平衡树改变成平衡树。因此，AVL树就诞生了。AVL要求左右子树的高度差不能超过1，是严格平衡的二叉搜索树，为了维持这种严格平衡，每次插入和删除的时候都需要旋转操作。在频繁插入、删除的场景下，AVL的性能会大打折扣。红黑树通过牺牲严格的平衡性质，换取减少每次插入和删除的旋转操作。

## 2.概述

红黑树是一种自平衡的二叉搜索树。不论哪种搜索树，通过中序遍历，结果就是有序的。

红黑树五大性质：

每个节点是红色或者黑色
根节点是黑色的
叶子节点是黑的
如果一个节点是红的，那么该节点的两个儿子都是黑的
每个节点，到叶子所有路径上的黑色节点个数相同。

![在这里插入图片描述](https://img-blog.csdnimg.cn/233fb546a38948699560dbbbcf245dbc.png)

## 3.红黑树代码实现

旋转：红黑树在插入或删除节点时，上面的性质可能会被破坏， 旋转操作就是为了使变化后的红黑树继续满足上面的性质。旋转操作分为左旋和右旋，旋转的本质，实际上是修改若干指针的指向。

![在这里插入图片描述](https://img-blog.csdnimg.cn/cf3098c57d1741ebaaf9557fab0e1bef.png)

左旋代码实现：

```
void rbtree_left_rotate(rbtree *T, rbtree_node *x) {

	rbtree_node *y = x->right;  // 获取x的右子树，保存到y
	x->right = y->left; // x的右子树指向y的左子树；
	
	//如果y的左子树不是叶子结点，需要改变y的左子树的父节点到x
	if (y->left != T->nil) { 
		y->left->parent = x;
	}
	
	y->parent = x->parent; // y的父节点指向x的父节点；
	// 此时需要判断x，x如果是根节点，则x的父节点为NULL，则需要让根节点指向此时的y；
	// 如果不是，则需要判断x是它父节点的左子树还是右子树,
	// 如果是左子树，就让父节点的左子树指向y，如果是右子树就让父节点的左=右子树指向y。
	if (x->parent == T->nil) { 
		T->root = y;
	} else if (x == x->parent->left) {
		x->parent->left = y;
	} else {
		x->parent->right = y;
	}
	
	y->left = x; // y的左子树指向x
	x->parent = y; // x的父节点指向y

}
```


右旋代码实现：只需要将左旋代码中的 x和y互换位置，left换成right，right换成left。

```
void rbtree_right_rotate(rbtree *T, rbtree_node *y) {
	rbtree_node *x = y->left;
	y->left = x->right;
	if (x->right != T->nil)
	{
		x->right->parent = y;
	}
	x->parent = y->parent;
	if (y->parent == T->nil) 
	{
		T->root = x;
	}
	else if (y == y->parent->right) 
	{
		y->parent->right = x;
	}
	else 
	{
		y->parent->left = x;
	}
	x->right = y;
	y->parent = x;
}



```


插入：每次插入都会插入到红黑树的最底层。并且上色为红色，因为红色不会影响第五条性质。插入完再继续进行调整（这里主要看自己与父节点是否为红色，如果是红色，迭代向上进行调整，如果是黑色就不用调整）。循环的中止条件就是遍历到叶子结点。如果插入的节点已经存在，也就是相等。取决于业务场景。比如定时任务，定时时间相同，可以稍稍修改一丢丢值。

始终需要扣住的点：红黑树不论是在什么时候，都是一棵红黑树，也就是说满足红黑树的性质。听起来像是废话，需要慢慢体会！

### 3.1 插入节点实现

红黑树插入节点z代码实现（看代码的时候结合着图看，会比较好理解一点）：

思考：假设插入节点z为红色，其父节点也为红色，那么z的祖父节点一定是黑色的！（红黑树性质决定），z的叔父节点的颜色就不确定了，需要分情况讨论：

要插入节点的父节点是祖父节点的左子树
a) 叔父节点是红色的情况：

![在这里插入图片描述](https://img-blog.csdnimg.cn/53c711e086cd4f1990f486154c6d27b8.png)

b) 叔父节点是黑色的情况：
这种情况下，需要考虑要插入的节点是其父节点的左孩子还是右孩子。
b.1) 要插入的节点是其父节点的右孩子

![在这里插入图片描述](https://img-blog.csdnimg.cn/83873e0e79784f1bba4955120cecddf0.png)

b.2) 要插入的节点是其父节点的左孩子

![在这里插入图片描述](https://img-blog.csdnimg.cn/f3a9747c91e24e48b98dbd36b87bffd9.png)

要插入节点的父节点是祖父节点的右子树（对照左子树情况理解就好）

```
void rbtree_insert_fixup(rbtree *T, rbtree_node *z) 
{
	 while (z->parent->color == RED) // 只要当前z与其父节点的颜色都是红的就进行调整
	 { 
		if (z->parent == z->parent->parent->left)  // 要插入节点的父节点是祖父节点的左子树
		{  
			rbtree_node *y = z->parent->parent->right;  // 拿到祖父节点的右节点，也就是叔父节点
			if (y->color == RED)  // 叔父节点的颜色是红色
			{ 
				z->parent->color = BLACK;  // 将插入节点z的父节点染成黑色
				y->color = BLACK;          // 将插入节点z的叔父节点染成黑色
				z->parent->parent->color = RED;   // 将插入节点z的祖父节点染成红色
				z = z->parent->parent;     // 更新z节点为它的祖父节点，再次进行判断
			} 
			else // 叔父节点的颜色是黑色
			{       
				if (z == z->parent->right)  // 当前插入节点z是其父节点的右孩子
				{
					z = z->parent;
					rbtree_left_rotate(T, z);  
				}
				z->parent->color = BLACK;
				z->parent->parent->color = RED;
				rbtree_right_rotate(T, z->parent->parent);
			}
		}
		else  // 要插入节点的父节点是祖父节点的右子树
		{
			rbtree_node *y = z->parent->parent->left; // 拿到叔父节点
			if (y->color == RED) 
			{
				z->parent->color = BLACK;
				y->color = BLACK;
				z->parent->parent->color = RED;
				z = z->parent->parent; //z --> RED
			}
			 else 
			 {
				if (z == z->parent->left) 
				{
					z = z->parent;
					rbtree_right_rotate(T, z);
				}
				z->parent->color = BLACK;
				z->parent->parent->color = RED;
				rbtree_left_rotate(T, z->parent->parent);
			}
		}
	}
	T->root->color = BLACK;
}


void rbtree_insert(rbtree *T, rbtree_node *z) {

	rbtree_node *y = T->nil;  
	rbtree_node *x = T->root;  // 从根节点开始遍历
	
	while (x != T->nil)  // 遍历找插入位置，循环到叶子节点终止
	{ 
		y = x;   // 保存x的上一级节点
		if (z->key < x->key) {
			x = x->left;
		} else if (z->key > x->key) {
			x = x->right;
		} else { //Exist
			return ;
		}
	}
	// 循环退出，找到插入位置，让待插入节点的父指针指向y（注意通过上述循环，x就是待插入的位置）
	z->parent = y;
	
	if (y == T->nil) {  // 当前红黑树为空
		T->root = z;
	} else if (z->key < y->key) {   // 当前红黑树不为空，需要判断是到y的左孩子还是右孩子
		y->left = z;
	} else {
		y->right = z;
	}
	
	// 因为每次插入，一定是插入到最后一行，所以需要让z节点的左右孩子都指向空
	z->left = T->nil;
	z->right = T->nil;
	z->color = RED; // 染成红色
	
	rbtree_insert_fixup(T, z);  // 迭代调整

}
```

### 3.2 删除及查找节点实现

```
void rbtree_delete_fixup(rbtree *T, rbtree_node *x) {

	while ((x != T->root) && (x->color == BLACK)) {
		if (x == x->parent->left) {
	
			rbtree_node *w= x->parent->right;
			if (w->color == RED) {
				w->color = BLACK;
				x->parent->color = RED;
	
				rbtree_left_rotate(T, x->parent);
				w = x->parent->right;
			}
	
			if ((w->left->color == BLACK) && (w->right->color == BLACK)) {
				w->color = RED;
				x = x->parent;
			} else {
	
				if (w->right->color == BLACK) {
					w->left->color = BLACK;
					w->color = RED;
					rbtree_right_rotate(T, w);
					w = x->parent->right;
				}
	
				w->color = x->parent->color;
				x->parent->color = BLACK;
				w->right->color = BLACK;
				rbtree_left_rotate(T, x->parent);
	
				x = T->root;
			}
	
		} else {
	
			rbtree_node *w = x->parent->left;
			if (w->color == RED) {
				w->color = BLACK;
				x->parent->color = RED;
				rbtree_right_rotate(T, x->parent);
				w = x->parent->left;
			}
	
			if ((w->left->color == BLACK) && (w->right->color == BLACK)) {
				w->color = RED;
				x = x->parent;
			} else {
	
				if (w->left->color == BLACK) {
					w->right->color = BLACK;
					w->color = RED;
					rbtree_left_rotate(T, w);
					w = x->parent->left;
				}
	
				w->color = x->parent->color;
				x->parent->color = BLACK;
				w->left->color = BLACK;
				rbtree_right_rotate(T, x->parent);
	
				x = T->root;
			}
	
		}
	}
	
	x->color = BLACK;

}

rbtree_node *rbtree_delete(rbtree *T, rbtree_node *z) {

	rbtree_node *y = T->nil;
	rbtree_node *x = T->nil;
	
	if ((z->left == T->nil) || (z->right == T->nil)) {
		y = z;
	} else {
		y = rbtree_successor(T, z);
	}
	
	if (y->left != T->nil) {
		x = y->left;
	} else if (y->right != T->nil) {
		x = y->right;
	}
	
	x->parent = y->parent;
	if (y->parent == T->nil) {
		T->root = x;
	} else if (y == y->parent->left) {
		y->parent->left = x;
	} else {
		y->parent->right = x;
	}
	
	if (y != z) {
		z->key = y->key;
		z->value = y->value;
	}
	
	if (y->color == BLACK) {
		rbtree_delete_fixup(T, x);
	}
	
	return y;

}

rbtree_node *rbtree_search(rbtree *T, KEY_TYPE key) {

	rbtree_node *node = T->root;
	while (node != T->nil) {
		if (key < node->key) {
			node = node->left;
		} else if (key > node->key) {
			node = node->right;
		} else {
			return node;
		}	
	}
	return T->nil;

}


void rbtree_traversal(rbtree *T, rbtree_node *node) {
	if (node != T->nil) {
		rbtree_traversal(T, node->left);
		printf("key:%d, color:%d\n", node->key, node->color);
		rbtree_traversal(T, node->right);
	}
}
```



## 4.完整代码

```


#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#define RED				1
#define BLACK 			2

typedef int KEY_TYPE;

typedef struct _rbtree_node {
	unsigned char color;
	struct _rbtree_node *right;
	struct _rbtree_node *left;
	struct _rbtree_node *parent;
	KEY_TYPE key;
	void *value;
} rbtree_node;

typedef struct _rbtree {
	rbtree_node *root;
	rbtree_node *nil;
} rbtree;

rbtree_node *rbtree_mini(rbtree *T, rbtree_node *x) {
	while (x->left != T->nil) {
		x = x->left;
	}
	return x;
}

rbtree_node *rbtree_maxi(rbtree *T, rbtree_node *x) {
	while (x->right != T->nil) {
		x = x->right;
	}
	return x;
}

rbtree_node *rbtree_successor(rbtree *T, rbtree_node *x) {
	rbtree_node *y = x->parent;

	if (x->right != T->nil) {
		return rbtree_mini(T, x->right);
	}
	
	while ((y != T->nil) && (x == y->right)) {
		x = y;
		y = y->parent;
	}
	return y;

}


void rbtree_left_rotate(rbtree *T, rbtree_node *x) {

	rbtree_node *y = x->right;  // x  --> y  ,  y --> x,   right --> left,  left --> right
	
	x->right = y->left; //1 1
	if (y->left != T->nil) { //1 2
		y->left->parent = x;
	}
	
	y->parent = x->parent; //1 3
	if (x->parent == T->nil) { //1 4
		T->root = y;
	} else if (x == x->parent->left) {
		x->parent->left = y;
	} else {
		x->parent->right = y;
	}
	
	y->left = x; //1 5
	x->parent = y; //1 6

}


void rbtree_right_rotate(rbtree *T, rbtree_node *y) {

	rbtree_node *x = y->left;
	
	y->left = x->right;
	if (x->right != T->nil) {
		x->right->parent = y;
	}
	
	x->parent = y->parent;
	if (y->parent == T->nil) {
		T->root = x;
	} else if (y == y->parent->right) {
		y->parent->right = x;
	} else {
		y->parent->left = x;
	}
	
	x->right = y;
	y->parent = x;

}

void rbtree_insert_fixup(rbtree *T, rbtree_node *z) {

	while (z->parent->color == RED) { //z ---> RED
		if (z->parent == z->parent->parent->left) {
			rbtree_node *y = z->parent->parent->right;
			if (y->color == RED) {
				z->parent->color = BLACK;
				y->color = BLACK;
				z->parent->parent->color = RED;
	
				z = z->parent->parent; //z --> RED
			} else {
	
				if (z == z->parent->right) {
					z = z->parent;
					rbtree_left_rotate(T, z);
				}
	
				z->parent->color = BLACK;
				z->parent->parent->color = RED;
				rbtree_right_rotate(T, z->parent->parent);
			}
		}else {
			rbtree_node *y = z->parent->parent->left;
			if (y->color == RED) {
				z->parent->color = BLACK;
				y->color = BLACK;
				z->parent->parent->color = RED;
	
				z = z->parent->parent; //z --> RED
			} else {
				if (z == z->parent->left) {
					z = z->parent;
					rbtree_right_rotate(T, z);
				}
	
				z->parent->color = BLACK;
				z->parent->parent->color = RED;
				rbtree_left_rotate(T, z->parent->parent);
			}
		}
		
	}
	
	T->root->color = BLACK;

}


void rbtree_insert(rbtree *T, rbtree_node *z) {

	rbtree_node *y = T->nil;
	rbtree_node *x = T->root;
	
	while (x != T->nil) {
		y = x;
		if (z->key < x->key) {
			x = x->left;
		} else if (z->key > x->key) {
			x = x->right;
		} else { //Exist
			return ;
		}
	}
	
	z->parent = y;
	if (y == T->nil) {
		T->root = z;
	} else if (z->key < y->key) {
		y->left = z;
	} else {
		y->right = z;
	}
	
	z->left = T->nil;
	z->right = T->nil;
	z->color = RED;
	
	rbtree_insert_fixup(T, z);

}

void rbtree_delete_fixup(rbtree *T, rbtree_node *x) {

	while ((x != T->root) && (x->color == BLACK)) {
		if (x == x->parent->left) {
	
			rbtree_node *w= x->parent->right;
			if (w->color == RED) {
				w->color = BLACK;
				x->parent->color = RED;
	
				rbtree_left_rotate(T, x->parent);
				w = x->parent->right;
			}
	
			if ((w->left->color == BLACK) && (w->right->color == BLACK)) {
				w->color = RED;
				x = x->parent;
			} else {
	
				if (w->right->color == BLACK) {
					w->left->color = BLACK;
					w->color = RED;
					rbtree_right_rotate(T, w);
					w = x->parent->right;
				}
	
				w->color = x->parent->color;
				x->parent->color = BLACK;
				w->right->color = BLACK;
				rbtree_left_rotate(T, x->parent);
	
				x = T->root;
			}
	
		} else {
	
			rbtree_node *w = x->parent->left;
			if (w->color == RED) {
				w->color = BLACK;
				x->parent->color = RED;
				rbtree_right_rotate(T, x->parent);
				w = x->parent->left;
			}
	
			if ((w->left->color == BLACK) && (w->right->color == BLACK)) {
				w->color = RED;
				x = x->parent;
			} else {
	
				if (w->left->color == BLACK) {
					w->right->color = BLACK;
					w->color = RED;
					rbtree_left_rotate(T, w);
					w = x->parent->left;
				}
	
				w->color = x->parent->color;
				x->parent->color = BLACK;
				w->left->color = BLACK;
				rbtree_right_rotate(T, x->parent);
	
				x = T->root;
			}
	
		}
	}
	
	x->color = BLACK;

}

rbtree_node *rbtree_delete(rbtree *T, rbtree_node *z) {

	rbtree_node *y = T->nil;
	rbtree_node *x = T->nil;
	
	if ((z->left == T->nil) || (z->right == T->nil)) {
		y = z;
	} else {
		y = rbtree_successor(T, z);
	}
	
	if (y->left != T->nil) {
		x = y->left;
	} else if (y->right != T->nil) {
		x = y->right;
	}
	
	x->parent = y->parent;
	if (y->parent == T->nil) {
		T->root = x;
	} else if (y == y->parent->left) {
		y->parent->left = x;
	} else {
		y->parent->right = x;
	}
	
	if (y != z) {
		z->key = y->key;
		z->value = y->value;
	}
	
	if (y->color == BLACK) {
		rbtree_delete_fixup(T, x);
	}
	
	return y;

}

rbtree_node *rbtree_search(rbtree *T, KEY_TYPE key) {

	rbtree_node *node = T->root;
	while (node != T->nil) {
		if (key < node->key) {
			node = node->left;
		} else if (key > node->key) {
			node = node->right;
		} else {
			return node;
		}	
	}
	return T->nil;

}


void rbtree_traversal(rbtree *T, rbtree_node *node) {
	if (node != T->nil) {
		rbtree_traversal(T, node->left);
		printf("key:%d, color:%d\n", node->key, node->color);
		rbtree_traversal(T, node->right);
	}
}

int main() {

	int keyArray[20] = {24,25,13,35,23, 26,67,47,38,98, 20,19,17,49,12, 21,9,18,14,15};
	
	rbtree *T = (rbtree *)malloc(sizeof(rbtree));
	if (T == NULL) {
		printf("malloc failed\n");
		return -1;
	}
	
	T->nil = (rbtree_node*)malloc(sizeof(rbtree_node));
	T->nil->color = BLACK;
	T->root = T->nil;
	
	rbtree_node *node = T->nil;
	int i = 0;
	for (i = 0;i < 20;i ++) {
		node = (rbtree_node*)malloc(sizeof(rbtree_node));
		node->key = keyArray[i];
		node->value = NULL;
	
		rbtree_insert(T, node);
		
	}
	
	rbtree_traversal(T, T->root);
	printf("----------------------------------------\n");
	
	for (i = 0;i < 20;i ++) {
	
		rbtree_node *node = rbtree_search(T, keyArray[i]);
		rbtree_node *cur = rbtree_delete(T, node);
		free(cur);
	
		rbtree_traversal(T, T->root);
		printf("----------------------------------------\n");
	}


​	
}
```



————————————————
版权声明：本文为CSDN博主「基层搬砖的Panda」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/weixin_46935110/article/details/125666526

# 【NO.579】Nginx反向代理与系统参数配置conf原理

## 1.nignx安装

### 1.1 安装步骤

安装pcre（用来正则表达式的）
下载地址 http://www.pcre.org/
安装zlib（用来压缩）
下载地址 http://www.zlib.net/
安装nginx（反向代理）
下载地址 http://www.nginx.net/

分别进入目录，执行以下命令就行

```
./configure
make 
sudo make install
```



### 1.2 查看

在/usr/local/nginx文件夹中，有4个文件

![在这里插入图片描述](https://img-blog.csdnimg.cn/0be8d03e57a04a1d8d05b2d2ca1912b5.png)

conf 存放配置文件
html 存放网页
logs 存放日志
sbin 运行时可执行程序

### 1.3 测试

```
./sbin/nginx -c conf/nginx.conf
```


在浏览器中，输入当前主机的ip地址即可
比如我的是192.168.192.128，输入到浏览器即可
在页面中可以看到下图内容（nginx中默认的）

![在这里插入图片描述](https://img-blog.csdnimg.cn/7f2ff520a7fe4c5b91d0e68ed66a6bef.png)

如果出现错误，查看端口是否被占用，如果占用就kill -9 [pid]

```
netstat -ntlp|grep 80
```


补充:
查看运行的nginx进程

```
ps aux|grep nginx
```


停止nginx

```
./sbin/nginx -s stop
```



## 2.配置conf文件

conf文件中有两类数据，一种是单行的，另外一种是block块

![在这里插入图片描述](https://img-blog.csdnimg.cn/09ddf854681d405899c23e79dc2bae61.png)

配置

### 2.1 worker_processes和worker_connections

worker_processes 4 表示会在主进程中启动4个子进程，也就是说总共会有5个进程，1个master进程，4个worker进程
worker_connections 1024;表示连接池的最大数量为1024，每一个work进程都有一个连接池，每个连接池最大数量是1024，如果有4个work进程，那么总共最大连接数是4096

```
worker_processes 4;
events {
	worker_connections 1024;
}
```


测试

启动该配置

在浏览器访问服务器ip，会看到connect refuse

然后查看进程可以看到，1个主进程和4个子进程

![在这里插入图片描述](https://img-blog.csdnimg.cn/0e2e19cbb5cd480e87750031989474e9.png)

查看网络状态，现在没有启动server，所以也就看不到东西

![在这里插入图片描述](https://img-blog.csdnimg.cn/38aaa689509346bc9c7438daf112dfc3.png)

### 2.2 http server

在原来基础上，再增加http server相关配置，
表示意思：启动一个http server，监听在端口8888上，

```
worker_processes 4;

events {
	worker_connections 1024;
}

http {
	server {
		listen 8888;
	}
}
```

测试

这时候再启动，进入浏览器，输入 192.168.192.128:8888是可以进入相关界面的，是nginx默认的页面

查看网络状态

第一行中代表 主进程，36384就是主进程的pid
但是第二行、第三行分别对应2个子进程，其中一个子进程是进行http请求，第二个子进程是(keepalive
)发送心跳包的

![在这里插入图片描述](https://img-blog.csdnimg.cn/1aa8cc13977846d3a138bf558951b5fa.png)

### 2.3 http中多个server

有4个server都去处理http协议

```
worker_processes 4;

events {
	worker_connections 1024;
}

http {
	server {
		listen 8888;
	}
	server {
		listen 8889;
	}
	server {
		listen 8890;
	}
	server {
		listen 8891;
	}
}
```

测试

可以发现4个端口都在一个进程（主进程）上面。listen在master进程上面，后面的处理在work进程上面

这时候访问192.169.192.128:8888，还是192.169.192.128:8889，都是nginx默认的界面，因为缺省了html

![在这里插入图片描述](https://img-blog.csdnimg.cn/4403b7d7fbe146d8bb8a090769dfbb71.png)

### 2.4 proxy_pass（反向代理）

根据http请求头，找到对应的location

![在这里插入图片描述](https://img-blog.csdnimg.cn/cc4fb8ffeda44b75ab320bf736979714.png)

也是11个状态里面的一个 ,NGX_HTTP_POST_REWRITE_PHASE

![在这里插入图片描述](https://img-blog.csdnimg.cn/53203820ad1b40a2879b95d4bc2ace67.png)

/前面是ip、端口或者 域名，/后面对应的是资源

![在这里插入图片描述](https://img-blog.csdnimg.cn/f353e349a67d4faba01581bda1349026.png)

下图中，server 8891访问的 html文件是/home/king/share/html
在server 8888中proxy_pass http://192.168.232.137:8891;重定向到8891。因此访问8888和8891是一样的内容

```
worker_processes 4;

events {
	worker_connections 1024;
}

http {
	server {
		listen 8888;
		location / {
			proxy_pass http://192.168.232.137:8891;
		}
	}
	server {
		listen 8889;
	}
	server {
		listen 8890;
	}

	server {
		listen 8891;
	
		location / {
			root /home/king/share/html;
			proxy_pass http://backend;
		}
	}

}
```



### 2.5 加入负载均衡

假设已经有两个已启动的nginx
通过设置upstream（把这个流转发出去的意思），，通过当前的主机去访问这两个服务器
proxy_pass http://backend;
去访问当前服务器的时候，就重定向到了指定的网页（upstream中的设置的两个）
weigth为权重，通过这种方式来实现负载均衡，weight可以自己调节

```
upstream backend {
		server 192.168.232.128 weight=2;
		server 192.168.232.132:8888 weight=1;
	}
	
server {
		listen 8891;

		location / {
			root /home/king/share/html;
			proxy_pass http://backend;
		}
	}


```

完整配置

```
#进程数量
worker_processes 4;

events {
	worker_connections 1024;
}

http {
	server {
		listen 8888;
		location / {
			proxy_pass http://192.168.232.137:8891;
		}
	}
	server {
		listen 8889;
	}
	server {
		listen 8890;
	}

	upstream backend {
		server 192.168.232.128 weight=2;
		server 192.168.232.132:8888 weight=1;
	}
	server {
		listen 8891;
	
		location / {
			root /home/king/share/html;
			proxy_pass http://backend;
		}
	}

}
```


测试

访问192.168.192.128:8891，可以发现，每3次就有2次进入192.168.232.128的页面，每3次就有1次进入192.168.232.132:8888

## 3.源码阅读

### 3.1 worker_processes设置

在nginx.conf中有

![在这里插入图片描述](https://img-blog.csdnimg.cn/f85ca96e474b4db692cf64a14da6fa9f.png)

ngx_core_commands中的命令如果找到nignx.conf配置中有话，那么就会执行相应的部分，比如下面"worker_processes"如果在conf文件中找到，那么就会执行ngx_set_worker_processes

```
static ngx_command_t  ngx_core_commands[] = {

	...
	
	{ ngx_string("worker_processes"),
	  NGX_MAIN_CONF|NGX_DIRECT_CONF|NGX_CONF_TAKE1,
	  ngx_set_worker_processes,
	  0,
	  0,
	  NULL },
	
	...

};
```


ngx_set_worker_processes
将解析的参数通过变量ccf->worker_processes赋值就行了

ngx_set_worker_processes
将解析的参数通过变量ccf->worker_processes赋值就行了

通过后续设置值，内容就存储到核心配置文件的变量里ngx_core_conf_t *ccf

```
static char *
ngx_set_worker_processes(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
{
    ngx_str_t        *value;
    ngx_core_conf_t  *ccf;

    ccf = (ngx_core_conf_t *) conf;
    
    if (ccf->worker_processes != NGX_CONF_UNSET) {
        return "is duplicate";
    }
    
    value = cf->args->elts;
    
    if (ngx_strcmp(value[1].data, "auto") == 0) {
        ccf->worker_processes = ngx_ncpu;
        return NGX_CONF_OK;
    }
    //value[0].data就是 命令本身，value[1].data是命令参数  value[1]这里指的是线程数，是字符串形式，要转成int
    ccf->worker_processes = ngx_atoi(value[1].data, value[1].len);
    
    if (ccf->worker_processes == NGX_ERROR) {
        return "invalid value";
    }
    
    return NGX_CONF_OK;

}
```

### 3.2 http

![在这里插入图片描述](https://img-blog.csdnimg.cn/0e2eba2b9d3e4df8adafbc09211f3072.png)


同样也有http的命令，这是一个块NGX_CONF_BLOCK，也就是上图用花括号括起来，称为BLOCK。
通过读取conf中的http配置来启动http server，执行ngx_http_block

```
static ngx_command_t  ngx_http_commands[] = {

    { ngx_string("http"),
      NGX_MAIN_CONF|NGX_CONF_BLOCK|NGX_CONF_NOARGS,
      ngx_http_block,
      0,
      0,
      NULL },
    
      ngx_null_command

};
```

ngx_http_block
ngx_http_block这个启动过程中，包含着tcp server和http协议的处理

在ngx_http_init_phase_handlers是http状态机启动，phase就是状态的意思，里面有很多枚举状态，如NGX_HTTP_SERVER_REWRITE_PHASE等

在ngx_http_block中，有ngx_http_optimize_servers就是启动tcp server，其中cmcf->ports是包含端口号的数组。

```
static char *
ngx_http_block(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
{
	...
	//http状态机启动
	if (ngx_http_init_phase_handlers(cf, cmcf) != NGX_OK) {
	        return NGX_CONF_ERROR;
	    }
	//启动tcp server
	if (ngx_http_optimize_servers(cf, cmcf, cmcf->ports) != NGX_OK) {
	        return NGX_CONF_ERROR;
	    }
	...
}
```

如何添加黑白名单？
方法1：tcp连接后：可以在建立连接（三次握手）之后，看它的ip地址，根据黑白名单，来看要不要执行后面的步骤
方法2：处理http头阶段：不对ip作限制，比如对pos或get请求作处理，在处理http头的阶段
方法3：对具体body 表单部分，进行处理，确定该部分是否要发送
————————————————
版权声明：本文为CSDN博主「菊头蝙蝠」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_21539375/article/details/124736034

# 【NO.580】多线程实践概述

## 0.线程背景

​        在单核CPU时代，程序的执行效率单纯的依赖更快的硬件；随着时间的变化，在2005年3月，Herb Sutter在Dobb's Journal发表了《The Free Lunch is over: A Fundamental Turn Toward Concurrency is Software》一文，文章分析处理器厂商改善CPU性能的传统方法，如提升时钟速度和指令吞吐量的方式，遇到了瓶颈，处理器发展方向趋向于超线程和多架构模式。

         为了让代码执行的更快、更高效，开发人员可通过编写并发代码，充分大会多核处理器的优势，使程序性能得到提升。

## 1.线程与进程

​        线程与进程有着千丝万缕的联系，两者密不可分。在Linux下程序或可执行文件只是一组指令的集合，没有执行的含义。进程拥有自己独立的进程地址空间和上下文堆栈，但对一个程序本身的执行操作来说，进程不执行任何代码，只是提供一个大环境容器，进程中实际执行体是线程，因此每个进程中至少的有一个线程，即主线程。

        多进程、多线程都能做并发，两者的差异何在？
    
        进程之间，彼此的地址空间是独立的，但线程会共享内存地址空间。同一个进程的多个线程共享一份全局内存区域，包括初始化数据段、未初始化数据段和动态分配的堆内存。

  ![img](https://img-blog.csdnimg.cn/646178764a12489a99deed12f0c34f6b.png)


 这种共享方式给线程带来了很多优势：

创建线程花费的时间少于创建进程花费的时间
线程间的上下文切换小于进程间的上下文切换
线程间数据共享比进程之间的共享要简单
线程的缺点：

多个线程中只要有一个线程出现BUG，就会出现SegmenFault导致进程退出，相比之下进程的地址空间互相独立，多个进程通过进程间通信(IPC),一个进程的异常退出不会影响其他进程
多线程控制流、时许很难控制，每次执行都不一样
        多进程和多线程都能够很好的发挥出多核处理器的性能，两者孰优孰劣需要依赖具体的应用场景去抉择。

## 2.线程属性

### 2.1 线程ID

​        在内核2.6以前的调度实体都是进程，内核并没有真正执行线程，为了遵循POSIX标准，Redhat发布了NPTL（native posix thread library）库,它的实现方式是，在内核中线程仍然被当作进程，也是用task_struct进行描述，这种线程又叫LWP（用户态线程）。

```
Struct 	task_struct {		
    …
	pid_t	pid;	//线程id
	pid_t	tgid;	//Thread Group ID(对应用户层的进程ID)

	…
	struct	task_struct	*group_leader;
	struct	list_head	thread_group;
	 
	…

}
```

​        没有线程之前，一个进程对应内核的一个进程描述符，对应一个进程ID。但是引入线程概念后，情况发生了变化，一个进程下管辖的N个线程。每个线程作为一个独立的调度实体在内核中有自己的进程描述符。进程和内核的进程描述符由1：1变成1：n。POSIX标准要求所有线程调用getpid函数返回相同的进程ID。为了解决这个问题，引入了线程组的概念。

        线程组内的第一个线程，在用户态被称为主线程，在内核中被称为Group Leader.内核在创建第一个线程时，会将线程组ID的值设置成第一个线程ID，group_leader指针则指向自身，即主线程的进程描述符。

```
//线程组ID等于主线程ID
p->tgid = p->pid;
p->group_leader = p;
INIT_LIST_HEAD(&p->thread_group);
```

​        通过group_leader指针，每个线程都能够找到主线程，主线程中存在一个链表头结构，后面创建的每一个线程都会被链入到该双向链表中。通过链表结构能偶遍历组内其他线程，也能够找到主线程。

Linux线程ID：

        用ps -eLf查询得，进程syslog（680）组内有4个线程，每个线程的ID号分别为680、703、704、705共4个线程。

```
yp@ubuntu:~$ ps -eLf
UID         PID   PPID    LWP  C NLWP STIME TTY          TIME CMD
........
syslog      680      1    680  0    4 01:02 ?        00:00:00 /usr/sbin/rsyslogd -n
syslog      680      1    703  0    4 01:02 ?        00:00:00 /usr/sbin/rsyslogd -n
syslog      680      1    704  0    4 01:02 ?        00:00:00 /usr/sbin/rsyslogd -n
syslog      680      1    705  0    4 01:02 ?        00:00:00 /usr/sbin/rsyslogd -n
```

​         在已知进程ID前提下，可在/proc/PID/task目录下的子目录来查看进程内的线程个数及其线程ID。示例如下：

```
yp@ubuntu:~$ ll /proc/680/task/
total 0
dr-xr-xr-x 6 syslog syslog 0 Jul  5 03:36 ./
dr-xr-xr-x 9 syslog syslog 0 Jul  5 01:02 ../
dr-xr-xr-x 7 syslog syslog 0 Jul  5 03:36 680/
dr-xr-xr-x 7 syslog syslog 0 Jul  5 03:36 703/
dr-xr-xr-x 7 syslog syslog 0 Jul  5 03:36 704/
dr-xr-xr-x 7 syslog syslog 0 Jul  5 03:36 705/
```

### 2.2 获取线程ID

调用pthread_create函数时，函数调用成功，通过第一个参数返回

```
#include <pthread.h>

pthread_t   tid;
pthread_create(&tid, NULL, thread_proc, NULL);
```

在对应线程中调用pthread_self()

```
#include <pthread.h>
 pthread_t   tid = pthread_self();
```

系统调用

```
#include <sys/syscall.h>
int tid = syscall(SYS_gettid);
```

​        pthread_t输出的是一块内存地址空间，由于进程的虚拟地址与物理地址的映射关系，pthread_t类型的线程ID可能不是全系统唯一的。通过syscall(SYS_gettid)获取的线程ID是系统范围内全局唯一的。

## 2.Linux同步对象

### 2.1 Linux互斥量

​        Linux互斥量与Windows的临界区对象用法相似，也是通过限制多个线程同时访问临界资源以达到保护的目的。Linux互斥量在NPTL中实现，使用数据结构pthread_mutex_t表示一个互斥体对象。

互斥量初始化

(1) 使用PTHREAD_MUTEX_INITIALIZER直接给互斥体变量赋值

```
#include <pthread.h>
pthread_mutex_t = PTHREAD_MUTEX_INITIALIZER;
```

(2) 若互斥量是动态分配或者需要设置其属性，则使用pthread_mutex_init函数

```
int pthread_mutex_init(pthread_mutex_t* restrict_mutex, 
                       const pthread_mutexattr_init* restrict_attr)
```

​        当我们不在需要一个互斥体对象时，可以使用pthread_mutex_destroy函数来销毁它，函数签名如下：

```
int pthread_mutex_lock(pthread_mutex_t* mutex);
int pthread_mutex_trylock(pthread_mutex_t* mutex);
int pthread_mutex_unlock(pthread_mutex_t* mutex);
```

设置互斥量的属性

        设置互斥体属性时需要创建一个pthread_mutexattr_t类型的对象，并用pthread_mutexattr_init进行初始化。

```
#include <pthread.h>

int main()
{
    ...

    pthread_mutexattr_t mtx_attr;
    pthread_mutexattr_init(&mtx_attr);
    pthread_mutexattr_settype(&mtx_attr, PTHREAD_MUTEX_NORMAL);  //普通锁
    pthread_mutex_init(&g_mutex, &mtx_attr);
     
    ...

}
```

互斥体的加锁解锁

        互斥量一般配合加锁、解锁操作完成对临界资源的安全访问，Linux相关函数签名如下：

```
int pthread_mutex_lock(pthread_mutex_t* mutex);
int pthread_mutex_trylock(pthread_mutex_t* mutex);
int pthread_mutex_unlock(pthread_mutex_t* mutex);
```

### 2.2 条件变量

1. **为什么使用环境变量？**

       不使用条件变量的情形，伪代码如下：

```
pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;

int WaitForTrue()
{
    pthread_mutex_lock(&mtx);

    while (condition is false)          // 条件不满足
    {
        pthread_mutex_unlock(&mtx);     //解锁，等待其他线程改变共享数据
        sleep(n);                       //睡眠等待，再次加锁验证条件是否满足
        pthread_mutex_lock(&mtx);       
    }

}
```

​        这段代码对共享资源进行访问时存在严重的效率问题。假设等待的条件突然满足了，但是while循环仍然会消耗一下时间，响应不及时。

         因此需要一种机制：线程在条件不满足的时候，主动让出互斥量，让其他线程执行，其他线程发现条件满足后向其发送信号，就可以立即唤醒它。

2. **条件变量的使用**

       环境变量的初始化和销毁，使用如下API函数：

```
int pthread_cond_init(pthread_cond_t* cond, const pthread_condattr_t* attr);
int pthread_cond_destory(pthread_cond_t* cond);
```

​        可使用如下API函数等待条件变量被唤醒：

```
int pthread_cond_wait(pthread_cond_t* restrict cond, pthread_mutex_t* restrict mutex);
int pthread_cond_timedwait(pthread_cond_t* restrict cond, pthread_mutex_t* restrict mutex
                           const struct timespec* restrict abstime);
```

​        如果条件变量等待的条件没有被满足，则条用pthread_cond_wait的线程会一直阻塞，pthread_cond_timedwait是带超时时间的阻塞，超过abstime设置的时间后，立即返回。

        因调用pthread_cond_wait而阻塞的线程可被如下API唤醒：

```
int pthread_cond_signal(pthread_cond_t* cond);
int pthread_cond_broadcast(pthread_cond_t* cond);
```

​        如果多个线程阻塞在pthread_cond_wait，由于pthread_cond_signal一次只能唤醒一个线程，具体那个线程被唤醒是随机的。pthread_cond_broadcast可以唤醒所有等待的线程。

3. **条件变量虚假唤醒**

```
if (tasks.empty()) {
    pthread_cond_wait(&cv, &mutex);
}
```

```
while (tasks.empty()) {
    pthread_cond_wait(&cv, &mutex);
}
```

​       没有其他线程向条件变量发送信号，但等待此条件变量的线程可能被唤醒，即操作系统唤醒pthread_cond_wait时，tasks.empty()仍然为true，这种情形称为虚假唤醒。因此需要在while循环中，当条件变量被唤醒时，还需要判断条件是否满足。

## 3.C++11同步对象

### 3.1  独占性互斥量 std::mutex

​       互斥量的使用一般是通过lock（）方法来阻塞线程，直到获得互斥量的所有权为止。在线程获得互斥量并完成任务后，就必须使用unlock（）来解除对互斥量的占用，lock （）和unlock（）必须成对出现。代码示例如下：

```
#include <iostream>
#include <thread>
#include <mutex>
#include <chrono>

std::mutex g_lock;
void ThreadProc()
{
    g_lock.lock();

    std::cout << "entered thread " << std::this_thread::get_id() << std::endl;
    std::this_thread::sleep_for(std::chrono::seconds(1));
    std::cout << "leaving thread " << std::this_thread::get_id() << std::endl;
     
    g_lock.unlock();

} 

int main(void)
{
    std::thread t1(ThreadProc);
    std::thread t2(ThreadProc);

    t1.join();
    t2.join();
     
    system("pause");
    return 0;

}
```



        使用lock_guard（）可以简化lock/unlock写法，避免忘记unlock（）。lock_guard（）出作用域会自动解锁。

```
void ThreadProc()
{
    std::lock_guard<std::mutex> locker(g_lock);

    std::cout << "entered thread " << std::this_thread::get_id() << std::endl;
    std::this_thread::sleep_for(std::chrono::seconds(1));
    std::cout << "leaving thread " << std::this_thread::get_id() << std::endl;

} 
```

### 3.2 递归互斥量 std::recursive_mutex

```
#include <iostream>
#include <thread>
#include <mutex>
#include <unistd.h>

int g_resourceID = 1;
std::recursive_mutex mtx;

void ThreadFunc_2() {
    mtx.lock();                     //第二次上锁

	g_resourceID = g_resourceID * g_resourceID ;
	printf("g_resourceID = %d \n", g_resourceID );
	mtx.unlock();

}

void ThreadFunc_1() {
	mtx.lock(); 

	g_resourceID = g_resourceID + 1;
	ThreadFunc_2();
	printf("g_resourceID = %d \n", g_resourceID );
	 
	mtx.unlock();

}

int main() {
	std::thread t1(ThreadFunc_1);
	t1.join();

	return 0;

}
```



        递归互斥锁可对同一互斥量多次加锁，可以用来解决同一线程对互斥量多次加锁导致死锁阻塞的问题。

### 3.3 条件变量

​         C++11使用condition_variable类表示条件变量，与上文介绍的Linux系统原生的条件变量一样，此外还提供了等待条件满足的wait方法（wait、wait_for、wait_until）。发送信号使用notify方法（notify_one、notify_all）。与Linux系统的条件变量相比，C++11的std::condition_variable不在需要显示地初始化和销毁。

```
#include <thread>
#include <condition_variable>
#include <mutex>
#include <list>
#include <iostream>

template<typename T>
class SimpleSyncQueue
{
    public:
    SimpleSyncQueue() { }
    void Put(const T& x)
    {
        std::lock_guard<std::mutex> locker(m_mutex);
        m_queue.push_back(x);
        m_notEmpty.notify_one();
```

————————————————
版权声明：本文为CSDN博主「FooNY」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/zouph/article/details/125560751

# 【NO.581】C++高性能协程分布式服务框架设计

本项目将从零开始搭建出一个基于[协程]的异步RPC框架。

> 学习本项目需要有一定的C++，网络，RPC知识

## 1.RPC框架设计

本项目将从零开始搭建出一个基于协程的异步RPC框架。

通过本项目你将学习到：

- 协程同步原语
- 序列化协议
- 通信协议
- 连接复用
- 服务注册
- 服务发现
- 负载均衡
- 健康检查
- 三种异步调用方式

相信大家对RPC的相关概念都已经很熟悉了，这里不做过多介绍，直接进入重点。

本文将简单介绍框架的设计，在最后的 examples 会给出完整的使用范例。

更多的细节需要仔细阅读源码。

本RPC框架主要有网络模块， 序列化模块，通信协议模块，客户端模块，服务端模块，服务注册中心模块，负载均衡模块

主要有以下三个角色：

![img](https://pic2.zhimg.com/80/v2-cfc863dfb3e441791735fa2b2906abbd_720w.webp)

**注册中心 Registry**

主要是用来完成服务注册和服务发现的工作。同时需要维护服务下线机制，管理了服务器的存活状态。

**服务提供方 Service Provider**

其需要对外提供服务接口，它需要在应用启动时连接注册中心，将服务名发往注册中心。服务端还需要启动Socket服务监听客户端请求。

**服务消费方 Service Consumer**

客户端需要有从注册中心获取服务的基本能力，它需要在发起调用时，

从注册中心拉取开放服务的服务器地址列表存入本地缓存，

然后选择一种负载均衡策略从本地缓存中筛选出一个目标地址发起调用，

并将这个连接存入连接池等待下一次调用。

### 1.1 **协程同步原语**

我们都知道，一旦协程阻塞后整个协程所在的线程都将阻塞，这也就失去了协程的优势。编写协程程序时难免会对一些数据进行同步，而Linux下常见的同步原语互斥量、条件变量、信号量等基本都会堵塞整个线程，使用原生同步原语协程性能将大幅下降，甚至发生死锁的概率大大增加。

框架实现了一套协程同步原语来解决原生同步原语带来的阻塞问题，在协程同步原语之上实现更高层次同步的抽象——Channel用于协程之间的便捷通信。

### 1.2 序列化协议

本模块支持了基本类型以及标准库容器的序列化，包括：

- 顺序容器：string, list, vector
- 关联容器：set, multiset, map, multimap
- 无序容器：unordered_set, unordered_multiset, unordered_map, unordered_multimap
- 异构容器：tuple

以及通过以上任意组合嵌套类型的序列化

序列化有以下规则：

1. 默认情况下序列化，8，16位类型以及浮点数不压缩，32，64位有符号/无符号数采用 zigzag 和 varints 编码压缩
2. 针对 std::string 会将长度信息压缩序列化作为元数据，然后将原数据直接写入。char数组会先转换成 std::string 后按此规则序列化
3. 调用 writeFint 将不会压缩数字，调用 writeRowData 不会加入长度信息

对于任意用户自定义类型，只要实现了以下的重载，即可参与传输时的序列化。

```text
template<typename T>
Serializer &operator >> (Serializer& in, T& i){
   return *this;
}
template<typename T>
Serializer &operator << (Serializer& in, T i){
    return *this;
}
```

rpc调用过程：

- 调用方发起过程调用时，自动将参数打包成tuple，然后序列化传输。
- 被调用方收到调用请求时，先将参数包反序列回tuple，再解包转发给函数。

### 1.3 通信协议

```text
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|  BYTE  |        |        |        |        |        |        |        |        |        |        |             ........                                                           |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
|  magic | version|  type  |          sequence id              |          content length           |             content byte[]                                                     |
+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+
```

封装通信协议，使RPC Server和RPC Client 可以基于同一协议通信。

采用私有通信协议，协议如下：

第一个字节是魔法数。

第二个字节代表协议版本号，以便对协议进行扩展，使用不同的协议解析器。

第四个字节开始是一个32位序列号，用来识别请求顺序。

第七个字节开始的四字节表示消息长度，即后面要接收的内容长度。

除了协议头固定7字节，消息不定长。

目前提供了以下几种请求

```text
enum class MsgType : uint8_t {
    HEARTBEAT_PACKET,       // 心跳包
    RPC_PROVIDER,           // 向服务中心声明为provider
    RPC_CONSUMER,           // 向服务中心声明为consumer
    
    RPC_REQUEST,            // 通用请求
    RPC_RESPONSE,           // 通用响应
    
    RPC_METHOD_REQUEST ,    // 请求方法调用
    RPC_METHOD_RESPONSE,    // 响应方法调用
    
    RPC_SERVICE_REGISTER,   // 向中心注册服务
    RPC_SERVICE_REGISTER_RESPONSE,
    
    RPC_SERVICE_DISCOVER,   // 向中心请求服务发现
    RPC_SERVICE_DISCOVER_RESPONSE
};
```

### 1.4 连接复用

对于短连接来说，每次发起rpc调用就创建一条连接，由于没有竞争实现起来比较容易，但开销太大。所以本框架实现了rpc连接复用来支持更高的并发。

连接复用的问题在于，在一条连接上可以有多个并发的调用请求，由于服务器也是并发处理这些请求的，所以导致了服务器返回的响应顺序与请求顺序不一致。

### 1.5 服务注册

每一个服务提供者对应的机器或者实例在启动运行的时候，
都去向注册中心注册自己提供的服务以及开放的端口。
注册中心维护一个服务名到服务地址的多重映射，一个服务下有多个服务地址，
同时需要维护连接状态，断开连接后移除服务。

```text
/**
 * 维护服务名和服务地址列表的多重映射
 * serviceName -> serviceAddress1
 *             -> serviceAddress2
 *             ...
 */
std::multimap<std::string, std::string> m_services;
```

### 1.6 服务发现

虽然服务调用是服务消费方直接发向服务提供方的，但是分布式的服务，都是集群部署的，

服务的提供者数量也是动态变化的，

所以服务的地址也就无法预先确定。

因此如何发现这些服务就需要一个统一注册中心来承载。



客户端从注册中心获取服务，它需要在发起调用时，

从注册中心拉取开放服务的服务器地址列表存入本地缓存，

### 1.7 负载均衡

实现通用类型负载均衡路由引擎（工厂）。
通过路由引擎获取指定枚举类型的负载均衡器，
降低了代码耦合，规范了各个负载均衡器的使用，减少出错的可能。

提供了三种路由策略（随机、轮询、一致性哈希）,
由客户端使用，在客户端实现负载均衡

```text
/**
 * @brief: 路由均衡引擎
 */
template<class T>
class RouteEngine {
public:
    static typename RouteStrategy<T>::ptr queryStrategy(typename RouteStrategy<T>::Strategy routeStrategyEnum) {
        switch (routeStrategyEnum){
            case RouteStrategy<T>::Random:
                return s_randomRouteStrategy;
            case RouteStrategy<T>::Polling:
                return std::make_shared<impl::PollingRouteStrategyImpl<T>>();
            case RouteStrategy<T>::HashIP:
                return s_hashIPRouteStrategy ;
            default:
                return s_randomRouteStrategy ;
        }
    }
private:
    static typename RouteStrategy<T>::ptr s_randomRouteStrategy;
    static typename RouteStrategy<T>::ptr s_hashIPRouteStrategy;
};
```

选择客户端负载均衡策略，根据路由策略选择服务地址。默认随机策略。

```text
RouteStrategy<int>::ptr strategy =
            RouteEngine<int>::queryStrategy(Strategy::Random);
```

客户端同时会维护RPC连接池，以及服务发现结果缓存，减少频繁建立连接。

通过上述策略尽量消除或减少系统压力及系统中各节点负载不均衡的现象。

### 1.8 健康检查

服务中心必须管理服务器的存活状态，也就是健康检查。

注册服务的这一组机器，当这个服务组的某台机器如果出现宕机或者服务死掉的时候，

就会剔除掉这台机器。这样就实现了自动监控和管理。



项目采用了心跳发送的方式来检查健康状态。



服务器端：

开启一个定时器，定期给注册中心发心跳包，注册中心会回一个心跳包。



注册中心：

开启一个定时器倒计时，每次收到一个消息就更新一次定时器。如果倒计时结束还没有收到任何消息，则判断服务掉线。

### 1.9 三种异步调用方式

整个框架最终都要落实在服务消费者。为了方便用户，满足用户的不同需求，项目设计了三种异步调用方式。
三种调用方式的模板参数都是返回值类型，对void类型会默认转换uint8_t 。

1、以同步的方式异步调用

整个框架本身基于协程池，所以在遇到阻塞时会自动调度实现以同步的方式异步调用

```text
Result<int> res = con->call<int>("add", 123, 321);
ACID_LOG_INFO(g_logger) << res.getVal();
```

2、future 形式的异步调用

调用时会立即返回一个future

```text
future<Result<int>> res = con->async_call<int>("add", 123, 321);
ACID_LOG_INFO(g_logger) << res.get().getVal();
```

3、异步回调

async_call的第一个参数为函数时，启用回调模式，回调参数必须是返回类型的包装。收到消息时执行回调。

```text
con->async_call<int>([](Result<int> res){
    ACID_LOG_INFO(g_logger) << res.getVal();
}, "add", 123, 321); 
```

对调用结果及状态的封装如下

```text
/**
 * @brief RPC调用状态
 */
enum RpcState{
    RPC_SUCCESS = 0,    // 成功
    RPC_FAIL,           // 失败
    RPC_NO_METHOD,      // 没有找到调用函数
    RPC_CLOSED,         // RPC 连接被关闭
    RPC_TIMEOUT         // RPC 调用超时
};

/**
 * @brief 包装 RPC调用结果
 */
template<typename T = void>
class Result {
    ...
private:
    /// 调用状态
    code_type m_code = 0;
    /// 调用消息
    msg_type m_msg;
    /// 调用结果
    type m_val;
}
```

## 2.最后

通过以上介绍，我们粗略地了解了分布式服务的大概流程。但篇幅有限，无法面面俱到，
更多细节就需要去阅读代码来理解了。

这并不是终点，项目只是实现了简单的服务注册、发现。后续将考虑加入注册中心集群，限流，熔断，监控节点等。

## 3.示例

rpc服务注册中心

```text
#include "acid/rpc/rpc_service_registry.h"

// 服务注册中心
void Main() {
    acid::Address::ptr address = acid::Address::LookupAny("127.0.0.1:8080");
    acid::rpc::RpcServiceRegistry::ptr server = std::make_shared<acid::rpc::RpcServiceRegistry>();
    // 服务注册中心绑定在8080端口
    while (!server->bind(address)){
        sleep(1);
    }
    server->start();
}

int main() {
    acid::IOManager loop;
    loop.submit(Main);
}
```

rpc 服务提供者

```text
#include "acid/rpc/rpc_server.h"

int add(int a,int b){
    return a + b;
}

// 向服务中心注册服务，并处理客户端请求
void Main() {
    int port = 9000;
    acid::Address::ptr local = acid::IPv4Address::Create("127.0.0.1",port);
    acid::Address::ptr registry = acid::Address::LookupAny("127.0.0.1:8080");

    acid::rpc::RpcServer::ptr server = std::make_shared<acid::rpc::RpcServer>();;

    // 注册服务，支持函数指针
    server->registerMethod("add",add);
    // 支持函数对象
    server->registerMethod("echo", [](std::string str){
        return str;
    });
    // 支持标准库容器
    server->registerMethod("revers", [](std::vector<std::string> vec) -> std::vector<std::string>{
        std::reverse(vec.begin(), vec.end());
        return vec;
    });

    // 先绑定本地地址
    while (!server->bind(local)){
        sleep(1);
    }
    // 绑定服务注册中心
    server->bindRegistry(registry);
    // 开始监听并处理服务请求
    server->start();
}

int main() {
    acid::IOManager loop;
    loop.submit(Main);
}
```

rpc 服务消费者，并不直接用RpcClient，而是采用更高级的封装，RpcConnectionPool。

提供了连接池和服务地址缓存。

```text
#include "acid/log.h"
#include "acid/rpc/rpc_connection_pool.h"

static acid::Logger::ptr g_logger = ACID_LOG_ROOT();

// 连接服务中心，自动服务发现，执行负载均衡决策，同时会缓存发现的结果
void Main() {
    acid::Address::ptr registry = acid::Address::LookupAny("127.0.0.1:8080");

    // 设置连接池的数量
    acid::rpc::RpcConnectionPool::ptr con = std::make_shared<acid::rpc::RpcConnectionPool>();

    // 连接服务中心
    con->connect(registry);

    // 第一种调用接口，以同步的方式异步调用，原理是阻塞读时会在协程池里调度
    acid::rpc::Result<int> sync_call = con->call<int>("add", 123, 321);
    ACID_LOG_INFO(g_logger) << sync_call.getVal();

    // 第二种调用接口，future 形式的异步调用，调用时会立即返回一个future
    std::future<acid::rpc::Result<int>> async_call_future = con->async_call<int>("add", 123, 321);
    ACID_LOG_INFO(g_logger) << async_call_future.get().getVal();

    // 第三种调用接口，异步回调
    con->async_call<int>([](acid::rpc::Result<int> res){
        ACID_LOG_INFO(g_logger) << res.getVal();
    }, "add", 123, 321);
    
    // 测试并发
    int n=0;
    while(n != 1000) {
        n++;
        con->async_call<int>([](acid::rpc::Result<int> res){
            ACID_LOG_INFO(g_logger) << res.getVal();
        }, "add", 0, n);
    }
}

int main() {
    acid::IOManager loop;
    loop.submit(Main);
}
```

原文地址：https://zhuanlan.zhihu.com/p/607919437

作者：linux

# 【NO.582】如何能够看懂TCP/IP 协议细节？

## **1.高门槛，勿入**

在Cisco平台上有一个很有用的Traceback log功能，实时记录当前Code运行到特定模块、特定函数，这样一旦系统崩溃、死机的时候，可以有效记录机器是在哪个模块、哪个函数里崩溃的。只要将此Traceback输入解析器，就可以定位是哪个模块出了问题，提交Bug的时候就可以Assign给负责此模块的Team，避免部门之间的推诿。

其实Traceback的原理也很简单，即每进入一个模块，就记录一下实时位置，类似于Debug。TCP/IP协议如果也拥有类似的traceback日志信息，那么只要用packet触发一下，就可以实时看一个packet如何在TCP/IP内部函数流转，在packet被递交给网卡之前，打印出每一个函数的名称，然后进入TCP/IP源码，按照函数调用的先后次序（流水线）依次进入阅读，只有这样才能详细了解TCP/IP细节。但是这样的学习门槛很高，源码也不是那么可以轻易看懂。对于大多数读者来说，更容易的方法是先易后难，在开始TCP/IP学习之前，先问自己一个问题，为什么要有TCP/IP协议？

## **2.计算机网络的标准件**

TCP/IP是计算机网络世界的标准件，这些标准件类似于汽车界的标准轮胎、汽车稳定系统ESP，即使看似牛叉的Tesla汽车生产商，也不是所有汽车零部件都是自己生产。除了电池、外壳、软件是自己生产的除外，其他的标准件全部是采购而来。通过使用标准件，平均只要几分钟即可生产一辆汽车。如果不使用标准件而全部采用自研，那生产的周期要大大加长。

既然TCP/IP是标准件，是不是也要像汽车轮胎一样提供标准的接口？否则怎么安装呢？是的，TCP/IP提供了诸多的标准接口，通过这些标准接口可以有效使用标准件，如DNS、TCP、UDP、IP、ICMP、IGMP、ARP、DHCP等等标准件。当然如果一一介绍这些标准件如何使用，这些标准件内部的工作原理是非常枯燥的，还是以小例子看看这些标准件是如何协作的。

## **3.一个小例子**

在浏览里输入知乎的网址（[http://zhihu.com](http://zhihu.com/)），然后输入回车，浏览器使用的TCP/IP第一个标准件是什么呢？

读者说，浏览器与知乎服务器通信使用http，而http通常是使用标准件TCP的，那么第一个标准件应该是TCP，对吗？不对。TCP这个标准件，对于http来说身份是服务员，尽管TCP身份低微，但是却有鲜明个性。就是**任何人差遣自己干活，必须使用IP地址**。浏览器不能使用网址（[http://zhihu.com](http://zhihu.com/)）这个字符串来差遣自己，罢工。

## **4.标准件之DNS**

所以，浏览器使用第一个TCP/IP标准件是DNS。DNS其实就是将知乎的网址（[http://zhihu.com](http://zhihu.com/)）解析成一个IP地址。有了知乎的IP地址，才能使用TCP这个标准件。

读者会说，DNS域名查询，会使用UDP这个标准件，UDP标准件会使用IP标准件，IP标准件直接会使用以太网卡标准件，对吗？不对。以太网卡标准件，作为IP标准件的服务员，尽管身份卑微，但是个性鲜明，凡是让自己干活的客人，必须提供接收方的MAC地址，否则恕不接待。所以IP标准件需要先使用ARP标准件，查询接收方的MAC地址。读者又说，ARP这个标准件使用的也是以太网卡标准件，没有接收方的MAC地址，以太网卡不是恕不接待的嘛？是的。但是ARP使用广播MAC地址（FFFFFFFFFFFF）来查询啊，所以以太网卡不得不执行查询任务啊。假设一切顺利，ARP得到了回复答案，将ARP答案返回IP标准件。既然IP标准件拥有了接收方的MAC地址，就可以使用以太网卡标准件，将packet提交给以太网卡，packet的进一步处理就和TCP/IP没有关系了。不一会DNS查询结果报文返回，按照时间先后次序进入网卡物理层、MAC层、IP标准件、UDP标准件、DNS标准件，由DNS标准件将查询结果告知浏览器。以上的DNS查询过程，浏览器其实只直接使用了DNS标准件，并没有直接使用其它标准件，其他标准件的使用都是由DNS标准件（进程）间接触发使用。有读者会说，DNS标准件、ARP标准件功能非常清晰也很有必要，多出来的那两个标准件IP、UDP看起来非常多余，为什么要有它们啊？

## **5.标准件之IP**

问读者一个问题，浏览器查询得到知乎的IP地址，是留给自己看的吗？

很显然不是，IP地址肯定是给互联网看的，准确地说是给互联网上的IP路由器看的，IP路由器看了知乎的IP地址，才能将IP报文快递到知乎服务器，不是吗？**知乎的IP地址写在什么地方呢？**写在packet的IP报文头。**由谁来写**？IP标准件，IP标准件所做的工作远不是在包裹的外层添加一个IP封装，并在外包装上写下知乎的IP地址（目的IP）、浏览器主机的IP地址（源IP）那么简单，还需要查询路由表，寻找合适的出口。否则一个主机有多个网卡，连接内网和互联网，主机怎么将访问知乎的packet发给连接互联网的网卡，而不是连接内网的网卡呢？这就是IP标准件的另外一项主要工作。而要完成种种复杂的工作，最好的方式就是以一个实例instance形式（进程）存在。至于UDP标准件，先不写，等写完TCP再写。

## **6.标准件之TCP**

再返回第一个问题，浏览器拥有了知乎的IP地址，是否就可以将http request用TCP标准件发出了呢？

依然不行。TCP这个怪癖狂，非常有个性，要想使用它必须遵守它的怪癖。这个怪癖是什么呢？**Create（）**需要先要调用TCP标准件一个Create指定，该指令其实就是在本地内存创建一个结构体（内存），用于缓存TCP连接所有相关的变量、数据，最最重要的是这个结构体有一个全局唯一的ID，浏览器以后要与TCP标准件沟通，只能使用这个唯一的ID（FD）。Okay，TCP结构体创建成功，是不是就可以发送http request了？依然不行。**Connect（）**TCP需要先建立连接，因为上文的结构体是空白的，需要建立连接将关键参数进行初始化，双方的ISN，MSS、Window Size、SACK、Scaling Window等等。建立TCP连接，使用TCP标准件的connect接口，填入合适的参数，最重要的是知乎IP地址以及知乎端口，是80还是443，这将决定接下来是否需要使用TLS标准件，前者不需要，后者需要。这就是大名鼎鼎的TCP三次握手，其实三次握手一点也不准确。真实的含义是三个packet完成连接，浏览器发2个packet，知乎服务器发1个packet。之所以一直就这样以讹传讹到现在也没有纠正，是因为英中翻译这样最顺口。需要指出的是，TCP尽管有很多怪癖，但是connect环节以及之后的环节还是非常友好的。你只要调用一次connect即可，至于浏览器是发2个packet连接成功，还是多次重传连接成功，这都是connect接口内部的实现，浏览器无需关心。Connect只会告知浏览器两个结果，成功或者失败。读者有点急不可耐了，TCP连接建立成功，http request是否就可以发送出去了？**Send（）**如果浏览器连接的是知乎TCP 端口80，是的，浏览器需要将http request数据通过调用TCP标准件send（）接口发送出去，浏览器调用接口时需要携带FD唯一ID。如果浏览器连接的是知乎TCP 端口443，即https服务，依然不能发送数据。浏览器需要使用TLS标准件，与知乎服务器完成TLS安全加密连接才能发送http request。

## **7.标准件之TLS**

尽管TLS是一个标准件，但是它却工作于TCP之上，即TLS依赖于TCP为自己提供服务。所以TLS不是传统意义上的TCP/IP标准件。

TLS安全连接，使用上文创建的唯一文件标识符FD，使用TCP标准件的Send（）、Receive（）完成双向数据的收发，当浏览器认证知乎服务器数字证书合法双方协商出加密/解密参数，至此TLS标准件通知安全连接创建成功或者失败。浏览器的http request终于可以发出了，先流经TLS标准件，添加TLS 报文头，数据加密+数据完整性保护。再流转到TCP标准件，添加TCP报文头并发出，等待对方确认，启动超时重传定时器。再流转到IP标准件，查路由找出口，添加IP报文头。最后到达网卡，使用IP标准件提供的参数，完成以太网封装，进入发送队列。一旦轮到该报文，由物理层添加Preamble + SFD + CRC将信号发送出去。上文标准件调用的过程中，最值得研究的就是TCP，只有它对来自浏览器的http request进行了数据缓存，并启动了定时器。假设第一次发出的http request在互联网被丢了，对该http request重传的，既不是浏览器、也不是TLS、也不是IP、更不是以太网卡，而是那个有诸多怪癖的TCP，通过多次重传以应对可能丢包对浏览器数据的影响，这也是TCP可靠性的真正含义。浏览器一梭子打出去之后就不管数据死活了，TCP接管数据的重传重任。

## **8.标准件之UDP**

最后说到UDP，看了上文的TCP发数据的过程，是不是觉得很繁琐？是的，不光繁琐而且非常耗时，如果连DNS查询也要使用TCP传输，意味着用户需要等待更长的时间才能开始传输真正的数据，需要用户有更长久的延迟满足，很显然耐心不够的用户会直接放弃。

而有了UDP就没有那么繁琐了，用户使用UDP发送数据数据，无需等待、无需建立连接，直接嗖得一声，数据就UDP、IP、以太网走起，紧凑整洁，节省了用户大量时间。比如基于IP的实时语音以及视频，为了良好的用户体验，通常需要50ms收发一个包裹，要到达这个低延迟，使用TCP很显然是无法完成的，剩下的唯一选择就是使用UDP来承载。TCP/IP做为计算机网络标准件的集中营，每一个标准件都可以直接调用，而不全是上文的间接调用。比如用户可以直接使用IP标准件、ICMP标准件写程序，甚至直接调用以太网卡标准件，而不仅仅局限于TCP、UDP、DNS、TLS。

原文地址：https://zhuanlan.zhihu.com/p/608306304

作者：linux

# 【NO.583】一文搞懂 mmap 涉及的所有内容

内存映射，简而言之就是将内核空间的一段内存区域映射到用户空间。映射成功后，用户对这段内存区域的修改可以直接反映到内核空间，相反，内核空间对这段区域的修改也直接反映用户空间。那么对于内核空间与用户空间两者之间需要大量数据传输等操作的话效率是非常高的。当然，也可以将内核空间的一段内存区域同时映射到多个进程，这样还可以实现进程间的共享内存通信。

系统调用mmap()就是用来实现上面说的内存映射。最长见的操作就是文件（在Linux下设备也被看做文件）的操作，可以将某文件映射至内存(进程空间)，如此可以把对文件的操作转为对内存的操作，以此避免更多的lseek()与read()、write()操作，这点对于大文件或者频繁访问的文件而言尤其受益。

## 1.**概述**

mmap将一个文件或者其它对象映射进内存。文件被映射到多个页上，如果文件的大小不是所有页的大小之和，最后一个页不被使用的空间将会清零。munmap执行相反的操作，删除特定地址区域的对象映射。

当使用mmap映射文件到进程后，就可以直接操作这段虚拟地址进行文件的读写等操作，不必再调用read，write等系统调用。但需注意，直接对该段内存写时不会写入超过当前文件大小的内容。

采用共享内存通信的一个显而易见的好处是效率高，因为进程可以直接读写内存，而不需要任何数据的拷贝。对于像管道和消息队列等通信方式，则需要在内核和用户空间进行四次的数据拷贝，而共享内存则只拷贝两次数据：一次从输入文件到共享内存区，另一次从共享内存区到输出文件。实际上，进程之间在共享内存时，并不总是读写少量数据后就解除映射，有新的通信时，再重新建立共享内存区域。而是保持共享区域，直到通信完毕为止，这样，数据内容一直保存在共享内存中，并没有写回文件。共享内存中的内容往往是在解除映射时才写回文件的。因此，采用共享内存的通信方式效率是非常高的。

通常使用mmap()的三种情况： **提高I/O效率**、**匿名内存映射**、**共享内存进程通信**。

用户空间 mmap()函数 void *mmap(void *start, size_t length, int prot, int flags,int fd, off_t offset)，下面就其参数解释如下：

- start：用户进程中要映射的用户空间的起始地址，通常为NULL（由内核来指定）
- length：要映射的内存区域的大小
- prot：期望的内存保护标志
- flags：指定映射对象的类型
- fd：文件描述符（由open函数返回）
- offset：设置在内核空间中已经分配好的的内存区域中的偏移，例如文件的偏移量，大小为PAGE_SIZE的整数倍
- 返回值：mmap()返回被映射区的指针，该指针就是需要映射的内核空间在用户空间的虚拟地址

![img](https://pic3.zhimg.com/80/v2-10a229c3b2bd64c17c41ffcfa927668a_720w.webp)

## 2.**内存映射的应用**

- X Window服务器
- 众多内存数据库如MongoDB操作数据,就是把文件磁盘内容映射到内存中进行处理,为什么会提高效率? 很多人不解. 下面就深入分析内存文件映射.
- 通过malloc来分配大内存其实调用的是mmap，可见在malloc(10)的时候调用的是brk, malloc(10 * 1024 * 1024)调用的是mmap

**mmap()用于共享内存的两种方式**

- 使用普通文件提供的内存映射：适用于任何进程之间；此时，需要打开或创建一个文件，然后再调用mmap()；典型调用代码如下：

```text
fd=open(name, flag, mode);   
if(fd<0)   
   ...   
ptr=mmap(NULL, len , PROT_READ|PROT_WRITE, MAP_SHARED , fd , 0);  
```

- 使用特殊文件提供匿名内存映射：适用于具有亲缘关系的进程之间；由于父子进程特殊的亲缘关系，在父进程中先调用mmap()，然后调用fork()。那么在调用fork()之后，子进程继承父进程匿名映射后的地址空间，同样也继承mmap()返回的地址，这样，父子进程就可以通过映射区域进行通信了。注意，这里不是一般的继承关系。一般来说，子进程单独维护从父进程继承下来的一些变量。而mmap()返回的地址，却由父子进程共同维护。 对于具有亲缘关系的进程实现共享内存最好的方式应该是采用匿名内存映射的方式。此时，不必指定具体的文件，只要设置相应的标志即可。

## 3.**示例**

**驱动+应用**

首先在驱动程序分配一页大小的内存，然后用户进程通过mmap()将用户空间中大小也为一页的内存映射到内核空间这页内存上。映射完成后，驱动程序往这段内存写10个字节数据，用户进程将这些数据显示出来。

```text
#include <linux/miscdevice.h>  
#include <linux/delay.h>  
#include <linux/kernel.h>  
#include <linux/module.h>  
#include <linux/init.h>  
#include <linux/mm.h>  
#include <linux/fs.h>  
#include <linux/types.h>  
#include <linux/delay.h>  
#include <linux/moduleparam.h>  
#include <linux/slab.h>  
#include <linux/errno.h>  
#include <linux/ioctl.h>  
#include <linux/cdev.h>  
#include <linux/string.h>  
#include <linux/list.h>  
#include <linux/pci.h>  
#include <linux/gpio.h>  
  
#define DEVICE_NAME "mymap"  
  
static unsigned char array[10]={0,1,2,3,4,5,6,7,8,9};  
static unsigned char *buffer;  
  
static int my_open(struct inode *inode, struct file *file)  
{  
    return 0;  
}  
  
static int my_map(struct file *filp, struct vm_area_struct *vma)  
{      
    unsigned long page;  
    unsigned char i;  
    unsigned long start = (unsigned long)vma->vm_start;  
    //unsigned long end =  (unsigned long)vma->vm_end;  
    unsigned long size = (unsigned long)(vma->vm_end - vma->vm_start);  
    //得到物理地址  
    page = virt_to_phys(buffer);      
    //将用户空间的一个vma虚拟内存区映射到以page开始的一段连续物理页面上  
    if(remap_pfn_range(vma,start,page>>PAGE_SHIFT,size,PAGE_SHARED))//第三个参数是页帧号，由物理地址右移PAGE_SHIFT得到  
        return -1;  
    //往该内存写10字节数据  
    for(i=0;i<10;i++)  
        buffer[i] = array[i];  
      
    return 0;  
}  
  
static struct file_operations dev_fops = {  
    .owner    = THIS_MODULE,  
    .open    = my_open,  
    .mmap   = my_map,  
};  
  
static struct miscdevice misc = {  
    .minor = MISC_DYNAMIC_MINOR,  
    .name = DEVICE_NAME,  
    .fops = &dev_fops,  
};  
  
static int __init dev_init(void)  
{  
    int ret;      
    //注册混杂设备  
    ret = misc_register(&misc);  
    //内存分配  
    buffer = (unsigned char *)kmalloc(PAGE_SIZE,GFP_KERNEL);  
    //将该段内存设置为保留  
    SetPageReserved(virt_to_page(buffer));  
  
    return ret;  
}  

static void __exit dev_exit(void)  
{  
    //注销设备  
    misc_deregister(&misc);  
    //清除保留  
    ClearPageReserved(virt_to_page(buffer));  
    //释放内存  
    kfree(buffer);  
}  

module_init(dev_init);  
module_exit(dev_exit);  
MODULE_LICENSE("GPL");  
MODULE_AUTHOR("LKN@SCUT");  

#include <unistd.h>  
#include <stdio.h>  
#include <stdlib.h>  
#include <string.h>  
#include <fcntl.h>  
#include <linux/fb.h>  
#include <sys/mman.h>  
#include <sys/ioctl.h>   
  
#define PAGE_SIZE 4096  
  
int main(int argc , char *argv[])  
{  
    int fd;  
    int i;  
    unsigned char *p_map;  
    //打开设备  
    fd = open("/dev/mymap",O_RDWR);  
    if(fd < 0)  
    {  
        printf("open fail\n");  
        exit(1);  
    }  
    //内存映射  
    p_map = (unsigned char *)mmap(0, PAGE_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED,fd, 0);  
    if(p_map == MAP_FAILED)  
    {  
        printf("mmap fail\n");  
        goto here;  
    }  
    //打印映射后的内存中的前10个字节内容  
    for(i=0;i<10;i++)  
        printf("%d\n",p_map[i]);  
        
here:  
    munmap(p_map, PAGE_SIZE);  
    return 0;  
}  
```

## 4. **进程间共享内存**

UNIX访问文件的传统方法是用open打开它们, 如果有多个进程访问同一个文件, 则每一个进程在自己的地址空间都包含有该文件的副本，这不必要地浪费了存储空间。下图说明了两个进程同时读一个文件的同一页的情形。系统要将该页从磁盘读到高速缓冲区中, 每个进程再执行一个存储器内的复制操作将数据从高速缓冲区读到自己的地址空间。

![img](https://pic2.zhimg.com/80/v2-60bf94d9289ecb0ce6fe2957a9f9f715_720w.webp)

现在考虑另一种处理方法共享存储映射: 进程A和进程B都将该页映射到自己的地址空间, 当进程A第一次访问该页中的数据时, 它生成一个缺页中断。内核此时读入这一页到内存并更新页表使之指向它。以后, 当进程B访问同一页面而出现缺页中断时, 该页已经在内存, 内核只需要将进程B的页表登记项指向次页即可。如下图所示:

![img](https://pic2.zhimg.com/80/v2-60bf94d9289ecb0ce6fe2957a9f9f715_720w.webp)

下面就是进程A和B共享内存的示例。两个程序映射同一个文件到自己的地址空间, 进程A先运行, 每隔两秒读取映射区域, 看是否发生变化。进程B后运行, 它修改映射区域, 然后退出, 此时进程A能够观察到存储映射区的变化。

进程A的代码:

```text
#include <sys/mman.h>    
#include <sys/stat.h>    
#include <fcntl.h>    
#include <stdio.h>    
#include <stdlib.h>    
#include <unistd.h>    
#include <error.h>    
    
#define BUF_SIZE 100    
    
int main(int argc, char **argv)    
{    
    int fd, nread, i;    
    struct stat sb;    
    char *mapped, buf[BUF_SIZE];    
    
    for (i = 0; i < BUF_SIZE; i++) {    
        buf[i] = '#';    
    }    
    /* 打开文件 */    
    if ((fd = open(argv[1], O_RDWR)) < 0) {    
        perror("open");    
    }    
    /* 获取文件的属性 */    
    if ((fstat(fd, &sb)) == -1) {    
        perror("fstat");    
    }   
    /* 将文件映射至进程的地址空间 */    
    if ((mapped = (char *)mmap(NULL, sb.st_size, PROT_READ |     
                    PROT_WRITE, MAP_SHARED, fd, 0)) == (void *)-1) {    
        perror("mmap");    
    }    
    /* 文件已在内存, 关闭文件也可以操纵内存 */    
    close(fd);    
    /* 每隔两秒查看存储映射区是否被修改 */    
    while (1) {    
        printf("%s\n", mapped);    
        sleep(2);    
    }    
    
    return 0;    
}    
```

进程B的代码:

```text
#include <sys/mman.h>    
#include <sys/stat.h>    
#include <fcntl.h>    
#include <stdio.h>    
#include <stdlib.h>    
#include <unistd.h>    
#include <error.h>    
    
#define BUF_SIZE 100    
    
int main(int argc, char **argv)    
{    
    int fd, nread, i;    
    struct stat sb;    
    char *mapped, buf[BUF_SIZE];    
    
    for (i = 0; i < BUF_SIZE; i++) {    
        buf[i] = '#';    
    }    
    /* 打开文件 */    
    if ((fd = open(argv[1], O_RDWR)) < 0) {    
        perror("open");    
    }    
    /* 获取文件的属性 */    
    if ((fstat(fd, &sb)) == -1) {    
        perror("fstat");    
    }    
    /* 私有文件映射将无法修改文件 */    
    if ((mapped = (char *)mmap(NULL, sb.st_size, PROT_READ |     
                    PROT_WRITE, MAP_PRIVATE, fd, 0)) == (void *)-1) {    
        perror("mmap");    
    }    
    /* 映射完后, 关闭文件也可以操纵内存 */    
    close(fd);    
    /* 修改一个字符 */    
    mapped[20] = '9';    
     
    return 0;    
}    
```

**匿名映射实现父子进程通信**

```text
#include <sys/mman.h>    
#include <stdio.h>    
#include <stdlib.h>    
#include <unistd.h>    
    
#define BUF_SIZE 100    
    
int main(int argc, char** argv)    
{    
    char    *p_map;    
    /* 匿名映射,创建一块内存供父子进程通信 */    
    p_map = (char *)mmap(NULL, BUF_SIZE, PROT_READ | PROT_WRITE,    
            MAP_SHARED | MAP_ANONYMOUS, -1, 0);    
    
    if(fork() == 0) {    
        sleep(1);    
        printf("child got a message: %s\n", p_map);    
        sprintf(p_map, "%s", "hi, dad, this is son");    
        munmap(p_map, BUF_SIZE); //实际上，进程终止时，会自动解除映射。    
        exit(0);    
    }    
    
    sprintf(p_map, "%s", "hi, this is father");    
    sleep(2);    
    printf("parent got a message: %s\n", p_map);    
    
    return 0;    
}    
```

## 5.**mmap进行内存映射的原理**

mmap系统调用的最终目的是将设备或文件映射到用户进程的虚拟地址空间，实现用户进程对文件的直接读写，这个任务可以分为以下三步:

1.在用户虚拟地址空间中寻找空闲的满足要求的一段连续的虚拟地址空间,为映射做准备(由内核mmap系统调用完成)

假如vm_area_struct描述的是一个文件映射的虚存空间，成员vm_file便指向被映射的文件的file结构，vm_pgoff是该虚存空间起始地址在vm_file文件里面的文件偏移，单位为物理页面。mmap系统调用所完成的工作就是准备这样一段虚存空间,并建立vm_area_struct结构体,将其传给具体的设备驱动程序.

2.建立虚拟地址空间和文件或设备的物理地址之间的映射(设备驱动完成) 建立文件映射的第二步就是建立虚拟地址和具体的物理地址之间的映射，这是通过修改进程页表来实现的。mmap方法是file_opeartions结构的成员:int (*mmap)(struct file *,struct vm_area_struct *);

linux有2个方法建立页表:

- 使用remap_pfn_range一次建立所有页表。int remap_pfn_range(struct vm_area_struct *vma, unsigned long virt_addr, unsigned long pfn, unsigned long size, pgprot_t prot)。
- 使用nopage VMA方法每次建立一个页表项。 struct page *(*nopage)(struct vm_area_struct *vma, unsigned long address, int *type);

3.当实际访问新映射的页面时的操作(由缺页中断完成)

- page cache及swap cache中页面的区分：一个被访问文件的物理页面都驻留在page cache或swap cache中，一个页面的所有信息由struct page来描述。struct page中有一个域为指针mapping ，它指向一个struct address_space类型结构。page cache或swap cache中的所有页面就是根据address_space结构以及一个偏移量来区分的。
- 文件与 address_space结构的对应：一个具体的文件在打开后，内核会在内存中为之建立一个struct inode结构，其中的i_mapping域指向一个address_space结构。这样，一个文件就对应一个address_space结构，一个 address_space与一个偏移量能够确定一个page cache 或swap cache中的一个页面。因此，当要寻址某个数据时，很容易根据给定的文件及数据在文件内的偏移量而找到相应的页面。
- 进程调用mmap()时，只是在进程空间内新增了一块相应大小的缓冲区，并设置了相应的访问标识，但并没有建立进程空间到物理页面的映射。因此，第一次访问该空间时，会引发一个缺页异常。
- 对于共享内存映射情况，缺页异常处理程序首先在swap cache中寻找目标页（符合address_space以及偏移量的物理页），如果找到，则直接返回地址；如果没有找到，则判断该页是否在交换区 (swap area)，如果在，则执行一个换入操作；如果上述两种情况都不满足，处理程序将分配新的物理页面，并把它插入到page cache中。进程最终将更新进程页表。 注：对于映射普通文件情况（非共享映射），缺页异常处理程序首先会在page cache中根据address_space以及数据偏移量寻找相应的页面。如果没有找到，则说明文件数据还没有读入内存，处理程序会从磁盘读入相应的页面，并返回相应地址，同时，进程页表也会更新.
- 所有进程在映射同一个共享内存区域时，情况都一样，在建立线性地址与物理地址之间的映射之后，不论进程各自的返回地址如何，实际访问的必然是同一个共享内存区域对应的物理页面。

原文地址：https://zhuanlan.zhihu.com/p/608556281

作者：linux

# 【NO.584】C++这么难，为什么我们还要学习C++？

**前言**

C++ 可算是一种声名在外的编程语言了。这个名声有好有坏，从好的方面讲，C++ 性能非常好，哪个编程语言性能好的话，总忍不住要跟 C++ 来单挑一下；从坏的方面讲，它是臭名昭著的复杂、难学、难用。当然，这样一来，熟练的 C++ 程序员也就自然而然获得了 “水平很高” 的名声，所以这也不完全是件坏事。

不管说 C++ 是好还是坏，不可否认的是，C++ 仍然是一门非常流行且非常具有活力的语言。继沉寂了十多年，并终于发布语言标准的第二版 —— C++11，再那之后，C++ 以每三年一版的频度发布着新的语言标准，每一版都在基本保留向后兼容性的同时，提供着改进和新功能。

## 1. 为什么难学

每次提到 C++ 编程，无论你是使用 C++ 的开发者，还是使用其他编程语言和开发环境的开发者，我们对 C++ 的评价往往都是 “复杂且难学”。为什么 C++ 会留下这样的口碑？追根溯源，主要有两个原因。

第一个原因是 C++ 的包容性，即向前兼容。

C++ 类似 Objective-C，是 C 语言的超集，它希望尽量向下兼容 C 的一切语法和特性（在 C99 标准之前甚至是完全兼容），因此足够接近硬件底层。但这是把双刃剑。

虽然 C99 之前语法足够简单，但实际使用的复杂性并不低，而 C++ 为了兼容 C 语言的语法付出了很大的代价，并在此基础上设计并发展出了多范式的编程模型，这意味着可以继续采用面向过程的编程模式，也可以转向面向对象。与此同时，现代 C++ 还提供了一组函数式编程工具。

因此，在现代 C++ 得到发展以前，实际开发时到底要选用何种范式或者如何合理组合，一直让我们很头痛。

C++ 兼容 C 有什么代价呢？比如，C 的指针类型声明就备受 C++ 之父 Bjarne Stroustrup 诟病，但是为了向前兼容，不得不在这种声明模式下继续扩展。

第二个原因是 C++ 的设计哲学，“不为任何抽象付出不可接受的多余运行时性能损耗”。

纵观 C++40 多年的演进历程，可以发现每一次演进所支持的都是和编译时相关的新特性，而相对来说，运行时特性非常少，除了在面向对象的编程模型基础上提出的多态以外，几乎再无运行时特性（其他的均以库的形式提供）。这是因为 C++ 是零成本抽象，也就是说，开发者在使用 C++ 表达抽象概念时，无需忍受多余的运行时性能开销。

因此，虽然 C++ 具备很多高级抽象的语法特性，但在设计与具体使用过程中，我们仍然需要考虑各种各样的问题，包括基础对象内存模型、虚函数的设计、基于模板的泛型系统、基于模板的静态反射体系，以及到目前为止都是由编译器决定可选的垃圾回收（在其他现代语言中可以说是必备的特性了），这就让我们学习和使用 C++ 变得更复杂了。

![img](https://pic3.zhimg.com/80/v2-10092b7607b7b0f32ec0b656f4215292_720w.webp)

的确，这真够复杂的。一门编程语言必定有其局限性，这也是为什么 “更为现代” 的 Go 和 Rust 出现了，试图解决一些问题，特别是安全性方面。

不过作为语言的使用者，你肯定会问，那今后的 C++ 学习和使用会有哪些变化呢？这个问题，有人曾经问过 C++ 之父 Bjarne Stroustrup。

> 诸如 Go 和 Rust 编程语言新贵，它们在发力解决安全性和易用性方面的问题，规避缓冲区溢出这样的漏洞，甚至 Linux kernel 也开始考虑或采纳对 Rust 的支持，您是否觉得这会成为 C++ 的一个潜在的巨大威胁和挑战？

他的回答简单明了。

> “每隔几年，就会出现 C++ 的挑战者，我相信它们一定会有支持者。但是，C++ 的独特的语言特性、应用场景，以及 C++ 标准发展的方向，会让 C++ 继续茁壮成长。”

我特别喜欢这个回答。是啊，劣势固然存在，但 C++ 经过历史的检验，在高性能计算、低延迟处理、图形学领域以及机器学习等前沿技术领域有着难以替代的优势。

C++ 的 “复杂且难学” 一定程度上取决于向前兼容的能力和设计哲学，但正因如此，维护多年的系统仍然能与全新开发的系统友好地对接和集成，C++ 的包容性和多样性也让它极具发展力。

自 C++11 标准诞生以来，我们正式迈入现代 C++ 世界，而 C++20 及后续演进标准作为继 C++11 之后的又一次重大变革，给我们带来了新思想、新工具，让我们从容面对以往难以解决的问题。

## 2. C++的意义

C++ 程序员应该都听到过下面这种说法：

- C++ 是一门多范式的通用编程语言。

多范式，是因为 C++ 支持面向过程编程，也支持面向对象编程，也支持泛型编程，新版本还可以说是支持了函数式编程。同时，上面这些不同的范式，都可以在同一项目中组合使用，这就大大增加了开发的灵活性。因此，C++ 适用的领域非常广泛，小到嵌入式，大到分布式服务器，到处可以见到 C++ 的身影。

下面是一些著名的用到 C++ 的场合：

- 大型桌面应用程序（如 Adobe Photoshop、Google Chrome 和 Microsoft Office）
- 大型网站后台（如 Google 的搜索引擎）
- 游戏（如 StarCraft）和游戏引擎（如 Unreal 和 Unity）
- 编译器（如 LLVM/Clang 和 GCC）
- 解释器（如 Java 虚拟机和 V8 JavaScript 引擎）
- 实时控制（如战斗机的飞行控制和火星车的自动驾驶系统）
- 视觉和智能引擎（如 OpenCV、TensorFlow）
- 数据库（如 Microsoft SQL Server、MySQL 和 MongoDB）

有些同学可能会觉得，这些应用场景似乎和平时的开发场景有点远啊！你的感觉是对的。有些传统上使用 C++ 的场合现在已经不一定使用 C++，最典型的是个人电脑上的桌面应用。以前 Windows 下开发桌面应用常常用 MFC，微软的 C++ 框架。目前很流行的 Visual Studio Code 主要是用 TypeScript 写的，不是 C++。

C++ 的传统领域有被侵蚀的风险，那是因为和它相竞争的语言远远不止一个，可以说是上下夹攻。

- 如果专注性能和最小内存占用的话，C 仍然是首选——嵌入式领域用 C 非常多，而 Linux 也是用纯 C 写的。
- 如果专注抽象表达和可读性的话，那 Python 之类的脚本语言则要方便得多。
- 图形界面（GUI）编程传统上是 C++ 的地盘，但近年来 C# 和 JavaScript 占领了很大一部分市场。
- 游戏算是 C++ 的经典强项了，但有了 C++ 写的游戏引擎，游戏用 C# 写也没啥问题了——你可能不一定知道，Unity 游戏引擎上的首选开发语言是 C#，而王者荣耀是用什么游戏引擎呢？答案正是 Unity —— 所以王者荣耀可以认为是用 C# 开发的。
- 还有，Go 和 Rust 也加入了战团，对 C++ 形成了一定的竞争……

目前，跟 C++ 定位差不多、能有直接竞争关系的，也就是既支持高度抽象、又追求高性能的通用编程语言，其实只有 Rust 一种。而 Rust 远没有达到跟 C++ 一样的成熟和普及程度。这也可以从 TIOBE 的排名看出来：C++ 是第 4 位，而 Rust 是第 25 位。

另外，和 C 的兼容性，也是 C++ 的一大优势。虽然现在很多大型程序都混杂了多种语言，但在小项目里，减少语言的数量可以简化开发和部署。

## 3. 什么时候该用C++

C++ 既然性能又好，又支持抽象，为什么没有更流行呢？

C++ 比起 C 来，要更安全，更不容易出现缓冲区溢出这类漏洞，但跟没有指针概念的语言比起来，它仍然是一种“不安全”的语言。我的个人经验，完成同样的功能，C++ 需要的代码行数一般是 Python 的三倍左右，而性能则可以达到 Python 的十倍以上。

**那么问题来了：你在开发上额外付出的时间，能从性能上省回来吗？**

显然，这取决于你开发软件的用途和开发时间。举个例子，如果你用 Python 开发需要一天，运行需要十秒，并且不需要反复运行；那么，转用 C++ 开发就意味着开发费用也许要增加两倍，开发加运行的总时间增加两天，大亏。

反之，如果用 Python 开发还是需要一天，单次运行需要十秒，但是软件会作为服务长时间运行、每天被调用十万次。在这种情况下，明显你就需要多台服务器来支撑其使用了。这时，如果用 C++ 开发会需要额外的两天，但跟 Python 相比，部署上有望节约十分之九的硬件和电费 —— 那就很值了。

简言之，当你的软件属于运算密集或者内存密集型，你需要性能、且愿意为性能付出额外代价的时候，应该考虑用 C++，特别在你的代码需要部署在多台服务器或者移动设备的场合。反之，如果性能不会成为你开发的软件的瓶颈，那 C++ 可能就不是一个最合适的工具。

此外，在嵌入式应用的场景，那就根本不是值不值、而是行不行的问题。如果程序完成一个功能不能在指定的若干毫秒、甚至微秒内完成，那产品根本是失败、不可用的。在这种场合，能和 C++ 竞争的只有 C，但 C 是一种开发效率更低、更需要堆人力的语言了。在嵌入式开发使用 C++ 的最大障碍可能不是技术，而是人力资源——搞嵌入式开发的程序员可能大多都习惯使用纯 C 了。

由于 C++ 是解决性能问题的利器，短时间里在市场上没有真正的竞争对手，对 C++ 的需求会在相当长的时间里一直存在，尤其在大公司和像金融机构一样对性能渴求的地方。

> 顺便提一句，C++ 之父 Bjarne Stroustrup 目前就职的地方便是摩根斯坦利。

## 4. 如何学习C++

作为很多聪明人使用过的语言，C++ 在某些场合也可能被用来炫技，写出除了本人之外谁都看不懂的高抽象代码。这恰恰是 Bjarne 想努力抵制的方向。他想让 C++ 对初学者变得更为友好，也明确提出过，他不希望 C++ 是一种让人们耍机灵的语言，而是一种让人们更易于使用的语言。

学习 C++ 语言就像学一门活跃使用中的外语，你不要期望能够掌握所有的单词和语法规则 —— 那对于世界上 99.999999% 的人来说是不可能的。但语言是服务于人的，语法规则也是服务于人的，是为了让人们能够更好地沟通和表达。虽然 C++ 的每一个新标准都是让语言从定义和规则的角度变得更复杂，但从用法上来说，新标准允许人们能够更简单地表达自己的计算意图。跟学外语一样，我们需要的是多看多写，掌握合适的 “语感”，而不是记住所有的规则。

Bjarne 有一个洋葱理论： 抽象层次就像一个洋葱，是层层嵌套的。如果想用较低的抽象层次表达较高的概念，就好比一次切过了很多层洋葱，你会把自己的眼泪熏出来的。与这个思路相反，教 C++ 往往有一种不好的倾向，从那些琐碎易错的底层教起，自底向上，使得很多人常常在尚未领悟到抽象的真谛之前就已经被 C++ 的复杂性吓翻，从入门到放弃；或者，在学了基本的 C 语法和 class 之后就满足了，错过了高级抽象带来的全新境界。他主张学习应当自顶向下，先学习高层的抽象，再层层剥茧、丝丝入扣地一步步进入下层。如果一次走太深的话，挫折可能就难免了。

## 5.C++后端开发学习路线分享

最后，分享一个c/c++后端开发的学习知识图谱（摘自零声教育的大纲：对标腾讯T9职级技术栈，腾讯认证的）

**c++后端开发是一个庞杂的技术栈，因为没有统一的开发框架并且应用行业非常广泛。所有涉猎广泛，这里就把c/c++后端开发的技术点进行整理总结，看完以后，不会让你失望的。**

1. 精进基石
2. 高性能网络设计
3. 基础组建设计
4. 中间件开发
5. 开源框架
6. 云原生
7. 性能分析
8. 分布式架构
9. 上线实战

**1、精进基石，分为四个方面（数据结构，设计模式，c++新特性，Linux工程管理）**

数据结构部分

![img](https://pic4.zhimg.com/80/v2-a5e872e3f4ebff5f2e47f2839381a697_720w.webp)

设计模式

![img](https://pic3.zhimg.com/80/v2-03372ea2ce2feba7b108b66afeeb6e86_720w.webp)

C++新特性

![img](https://pic4.zhimg.com/80/v2-e50b7ad8735d347813040f9f90659aff_720w.webp)

linux工程管理

![img](https://pic4.zhimg.com/80/v2-176af3c57e8866f90356046dac8cc39f_720w.webp)



**2. 高性能网络设计（网络编程，网络原理，协程ntyco，用户态协议栈ntytcp）**

网络编程

![img](https://pic4.zhimg.com/80/v2-5426286b52a4957bc46d48b4b9768563_720w.webp)

网络原理

![img](https://pic3.zhimg.com/80/v2-30bacbf284fb94be950b8e3596d8d0aa_720w.webp)

自研框架： 纯c实现的协程（2000行代码）

![img](https://pic3.zhimg.com/80/v2-b6c40b94153b578adcb44fe354126bda_720w.webp)

自研tcp协议栈

![img](https://pic3.zhimg.com/80/v2-55103cd32fabf773893fe9e4ab525bd6_720w.webp)

高性能异步io机制io_uring

![img](https://pic2.zhimg.com/80/v2-b340e0b3ad0f7ceaab837839532e4f11_720w.webp)



**3. 基础组建设计，分为3部分， 池式组件，高性能组件，开源组件**

池式结构

![img](https://pic4.zhimg.com/80/v2-72e17934511f04b79d01387dd94b86af_720w.webp)

高性能组件

![img](https://pic1.zhimg.com/80/v2-21b403e920c42f632a0dee44c271c858_720w.webp)

开源组件

![img](https://pic4.zhimg.com/80/v2-449701cb30c276c67d2623835aa9c6bf_720w.webp)

4、中间件开发专栏

redis

![img](https://pic2.zhimg.com/80/v2-bbd008b47aaad5448750f93ceb95107d_720w.webp)

mysql

![img](https://pic2.zhimg.com/80/v2-411af5395fd23586dadb09a155430a45_720w.webp)

kafka

![img](https://pic3.zhimg.com/80/v2-fd1a3b402cdadbc1c9cfc56aa51f5ce6_720w.webp)

gRPC

![img](https://pic1.zhimg.com/80/v2-a2cea85a67a10a5e8563cf7cd0caef04_720w.webp)

nginx

![img](https://pic4.zhimg.com/80/v2-e8f9b4ca68300f7c797012869cfdfe2f_720w.webp)

**5. 开源框架**

游戏后端开源框架 skynet

![img](https://pic3.zhimg.com/80/v2-ef9162678cafae13eefb9bd94f85c53a_720w.webp)

分布式API网关

![img](https://pic1.zhimg.com/80/v2-70a66668b090577855763d4dbcac07a0_720w.webp)

DPDK

![img](https://pic1.zhimg.com/80/v2-16aa9606a881b6caf0b5765d571adfb0_720w.webp)

高性能计算CUDA

![img](https://pic2.zhimg.com/80/v2-19977aff14842f4756a6cb5d505f6f91_720w.webp)

**6、云原生专栏**

**docker**

![img](https://pic3.zhimg.com/80/v2-94e4caa56462fa1e22e31c92eb8137de_720w.webp)

kubernetes

![img](https://pic2.zhimg.com/80/v2-4f5ab6da7b798ebdde603942cab39481_720w.webp)

**7、性能分析专栏**

**性能与测试工具**

![img](https://pic3.zhimg.com/80/v2-021a78849d8efbf9e55e58e9581f4c6e_720w.webp)

观测技术bpf与ebpf

![img](https://pic3.zhimg.com/80/v2-16af55bef7b272f54e2a52f9f2983fda_720w.webp)

内核源码机制

![img](https://pic3.zhimg.com/80/v2-d7d1469ca1fc6e4bcf75247f47ab0842_720w.webp)



**8、分布式架构专栏**

**rocksdb**

![img](https://pic3.zhimg.com/80/v2-e6ce9f016c9bdf3aa775e593b43199fa_720w.webp)

TiDB

![img](https://pic4.zhimg.com/80/v2-cc8fddf5551aaa0b4aa5155ea529be5b_720w.webp)

分布式服务

![img](https://pic3.zhimg.com/80/v2-f22f724500b4f8e7b4d63bed73f92392_720w.webp)

**9、上线项目实战（可以写入简历的两个实战项目，让面试不再为没有项目发愁）**

**1、图床共享云存储**

**2、微服务即时通讯**

原文地址：https://zhuanlan.zhihu.com/p/608825768

作者：linux

# 【NO.585】内存泄露定位手段（c语言hook malloc相关方式）

如何确定有内存泄露问题，如何定位到内存泄露位置，如何写一个内存泄漏检测工具？

## 1.概述

内存泄露本质：其实就是申请调用malloc/new，但是释放调用free/delete有遗漏，或者重复释放的问题。

内存泄露会导致的现象：作为一个服务器，长时间运行，内存泄露会导致进程虚拟内存被占用完，导致进程崩溃吧。（堆上分配的内存）

如何规避或者发现内存泄露呢？

===》1：如何检测有内存泄露？（除了内存监控工具htop，耗时，效果不明显）

===》2：如何定位内存泄露的代码问题（少量代码可以阅读代码排除，线上版本呢？）

=====》引入gc

=====》少量代码可以通过排查代码进行定位

=====》已经确定代码有内存泄露，可以用过valgrind/mtrace等市场上已有的一些工具

=====》本质是malloc和free的次数不一致导致，我们通过hook的方式，对malloc和free次数进行统计

## 2.通过hook的方式检测，定位内存泄露（四种方法）

在生产环境重定位内存泄露的问题，我们可以在产品中增加这些定位手段，通过配置文件开关控制其打开，方便内存泄露定位。

几种不同的方式本质：都是对malloc和free进行hook，增加一些处理进行检测。

### 2.1 测试代码描述内存泄露

如下代码，从代码看，明显可以看到是有内存泄露的，但是如果看不到代码，或者代码量过多，从运行现象上我们就很难发现了。

```text
#include <stdio.h>
#include <stdlib.h>

int main()
{
    void * ptr1 = malloc(10);
    void * ptr2 = malloc(20);
    
    free(ptr1);
    
    void * ptr3 = malloc(30);
    free(ptr3);
    return 0;
}

//代码运行是没有问题，也没有报错的，但是明显可以看到ptr2是没有内存释放的，如果是服务器有这种代码，长时间运行会有严重问题的。
```

### 2.2 通过dlsym库函数对malloc/free进行hook

我在 Linux/unix系统编程手册 这本书中了解相关dlsym函数的使用

要想知道有内存泄露，或者直接定位内存泄露的代码位置，本质还是对调用的malloc/free进行hook, 对调用malloc/free分别增加监控来分析。

使用dlsym库函数，获取malloc/free函数的地址，通过RTLD_NEXT进行比标记（这个标记适用于在其他地方定义的函数同名的包装函数，如在主程序中定义的malloc,代替系统的malloc）,实现用我们主程序中malloc代替系统调用malloc.

**2.2.1：第一版试着**

在调用malloc和free前，使用dlsym函数和RTLD_NEXT标记，获取系统库malloc/free地址，以及用本地定义的malloc/free代替系统调用。

```text
//1:使用void * dlsym(void* handle, char* symbool)函数和handle为RTLD_NEXT标记，对malloc/free进行hook
//2:RTLD_NEXT标记 需要在本地实现 symbool同名函数达到hook功能，即这里要实现malloc/free功能
//3:dlsym()返回的是symbool 参数对应的函数的地址，在同名函数中用该地址实现真正的调用

//RTLD_NEXT 是dlsym() 库中的伪句柄，定义_GNU_SOURCE宏才能识别
//可以通过 man dlsym
//测试发现 ：必须放在最顶部，不然编译报 RTLD_NEXT没有定义 
#define _GNU_SOURCE
#include <dlfcn.h>  //对应的头文件

//第一步功能，确定hook成功，先在我们的hook函数中增加一些打印信息验证
#include <stdio.h>
#include <stdlib.h>

//定义相关全局变量，获取返回的函数地址，进行实际调用
typedef void *(*malloc_t)(size_t size);
malloc_t malloc_f = NULL;

typedef void (*free_t)(void* p);
free_t free_f = NULL;

//要hook的同名函数
void * malloc(size_t size){
    printf("exec malloc \n");
    return malloc_f(size);
}

void free(void * p){
    printf("exec free \n");
    free_f(p);
}
//通过dlsym 对malloc和free使用前进行hook
static void init_malloc_free_hook(){
    //只需要执行一次
    if(malloc_f == NULL){
        malloc_f = dlsym(RTLD_NEXT, "malloc"); //除了RTLD_NEXT 还有一个参数RTLD_DEFAULT
    }
    
    if(free_f == NULL)
    {
        free_f =  dlsym(RTLD_NEXT, "free");
    }
    return ;
}


int main()
{
    init_malloc_free_hook(); //执行一次
    void * ptr1 = malloc(10);
    void * ptr2 = malloc(20);
    
    free(ptr1);
    
    void * ptr3 = malloc(30);
    free(ptr3);
    return 0;
}
```

上述代码是有问题的，现象及定位问题：

```text
hlp@ubuntu:~/mem_test$ gcc dlsym_hook.c -o dlsym_hook -ldl
hlp@ubuntu:~/mem_test$ ./dlsym_hook 
Segmentation fault (core dumped)


#使用gdb对问题进行定位 
hlp@ubuntu:~/mem_test$ gdb ./dlsym_hook 
(gdb) b 54    #加断点
Breakpoint 1 at 0x400729: file dlsym_hook.c, line 54.
(gdb) b 28	  #加断点
Breakpoint 2 at 0x400682: file dlsym_hook.c, line 28.
(gdb) r       #开始运行
Starting program: /home/hlp/mem_test/dlsym_hook 
Breakpoint 1, main () at dlsym_hook.c:54
54	    void * ptr1 = malloc(10);
(gdb) c		#单步执行
Continuing.
Breakpoint 2, malloc (size=10) at dlsym_hook.c:28   #第一个mallocy已经执行
28	    printf("exec malloc \n");	
(gdb) c
Continuing.
Breakpoint 2, malloc (size=1024) at dlsym_hook.c:28  #这里的1024不是我们代码里面的，
28	    printf("exec malloc \n");
(gdb) c
Continuing.

Breakpoint 2, malloc (size=1024) at dlsym_hook.c:28    #发现malloc 1024一直循环执行 怀疑是printf中会调用malloc,
28	    printf("exec malloc \n");
(gdb) c
Continuing.

Breakpoint 2, malloc (size=1024) at dlsym_hook.c:28
28	    printf("exec malloc \n");
(gdb) 

#通过gdb进行定位时，可以确定，我们hook malloc函数内部调用printf,printf底层其实是有调用malloc,从而printf内部成为递归，一直调用了。
#所以我们需要规避这种现象，让hook函数内部其他业务只执行一次，不要因为第三方库内部机制导致类似问题
```

增加特定标识，优化上述代码：

```text
//使用标识，使hook的函数内部只执行一次，不因为第三方库原因导致递归现象
#define _GNU_SOURCE
#include <dlfcn.h>  //对应的头文件
#include <stdio.h>
#include <stdlib.h>

typedef void *(*malloc_t)(size_t size);
malloc_t malloc_f = NULL;
typedef void (*free_t)(void* p);
free_t free_f = NULL;

//定义一个hook函数的标志 使内部逻辑只执行一次
int enable_malloc_hook = 1;
int enable_free_hook = 1;

//要hook的同名函数
void * malloc(size_t size){
    if(enable_malloc_hook) //对第三方调用导致的递归进行规避
    {
        enable_malloc_hook = 0;
        printf("exec malloc \n");
        enable_malloc_hook = 1;
    }
    return malloc_f(size);
}

void free(void * p){
    if(enable_free_hook){
        enable_free_hook = 0;
        printf("exec free \n");
        enable_free_hook = 1;
    }
    free_f(p);
}

//通过dlsym 对malloc和free使用前进行hook
static void init_malloc_free_hook(){
    //只需要执行一次
    if(malloc_f == NULL){
        malloc_f = dlsym(RTLD_NEXT, "malloc"); //除了RTLD_NEXT 还有一个参数RTLD_DEFAULT
    }
    
    if(free_f == NULL)
    {
        free_f =  dlsym(RTLD_NEXT, "free");
    }
    return ;
}
int main()
{
    init_malloc_free_hook(); //执行一次
    void * ptr1 = malloc(10);
    void * ptr2 = malloc(20);
    
    free(ptr1);
    
    void * ptr3 = malloc(30);
    free(ptr3);
    return 0;
}
```

上述代码执行成功，现象如下：

```text
hlp@ubuntu:~/mem_test$ gcc dlsym_hook_ok.c -o dlsym_hook_ok -ldl -g
hlp@ubuntu:~/mem_test$ ./dlsym_hook_ok 
exec malloc 
exec malloc 
exec free 
exec malloc 
exec free
#对比执行的 malloc和free次数 可以确定有内存泄露
```

如何增加行号标识呢？让我们确定到代码位置？

如何确定有内存泄露呢？直接通过代码，识别到malloc/free的对应次数，定位到有代码问题的位置。

**2.2.2：能识别到行号，以及有问题代码位置**

```text
//我们知道，一般可以通过__LINE__ 标识日志当前行号位置，但是这里不适用
//可以通过 __builtin_return_address 获取上级调用的退出的地址，可以设置时1级，也可以设置是2级别...

// 增加打印调用malloc和free位置的信息。 这里打印地址  通过addr2line进行地址和行号转换
#define _GNU_SOURCE
#include <dlfcn.h>  //对应的头文件
#include <stdio.h>
#include <stdlib.h>

typedef void *(*malloc_t)(size_t size);
malloc_t malloc_f = NULL;
typedef void (*free_t)(void* p);
free_t free_f = NULL;

int enable_malloc_hook = 1;
int enable_free_hook = 1;

void * malloc(size_t size){
    if(enable_malloc_hook) //对第三方调用导致的递归进行规避
    {
        enable_malloc_hook = 0;
        //打印上层调用的地址
        
        void *carrer = __builtin_return_address(0);
        printf("exec malloc [%p ]\n", carrer );
        enable_malloc_hook = 1;
    }
    return malloc_f(size);
}

void free(void * p){
    if(enable_free_hook){
        enable_free_hook = 0;
        void *carrer = __builtin_return_address(0);
        printf("exec free [%p]\n", carrer);
        enable_free_hook = 1;
    }
    free_f(p);
}

//通过dlsym 对malloc和free使用前进行hook
static void init_malloc_free_hook(){
    //只需要执行一次
    if(malloc_f == NULL){
        malloc_f = dlsym(RTLD_NEXT, "malloc"); //除了RTLD_NEXT 还有一个参数RTLD_DEFAULT
    }
    
    if(free_f == NULL)
    {
        free_f =  dlsym(RTLD_NEXT, "free");
    }
    return ;
}
int main()
{
    init_malloc_free_hook(); //执行一次
    void * ptr1 = malloc(10);
    void * ptr2 = malloc(20);
    
    free(ptr1);
    
    void * ptr3 = malloc(30);
    free(ptr3);
    return 0;
}
```

执行结果及查找对应行数：

```text
#执行结果如下
hlp@ubuntu:~/mem_test$ gcc dlsym_hook_addr.c -o dlsym_hook_addr -ldl
hlp@ubuntu:~/mem_test$ ./dlsym_hook_addr 
exec malloc [0x400797 ]
exec malloc [0x4007a5 ]
exec free [0x4007b5]
exec malloc [0x4007bf ]
exec free [0x4007cf]
#可以通过addr2line 获取到对应的代码行数 编译的时候要带 -g
hlp@ubuntu:~/mem_test$ addr2line -fe ./dlsym_hook_addr -a 0x400797
0x0000000000400797
main
/home/hlp/mem_test/dlsym_hook_addr.c:57
```

**2.2.3：通过策略，查找有问题的代码**

从上文可以知道，我们通过对malloc和free的hook，可以获得各自malloc和hook的次数。

以及我们可以通过__builtin_return_address 接口获取到实际调用malloc/free的位置。

除此之外，malloc之间有所关联的是申请内存的地址，

汇总：

===》可以思考，通过malloc和free关联的地址作为标识，对malloc和free的次数进行统计即可。

===》可以设计数据结构，对不同地址，malloc的地址和free的地址进行保存，malloc的次数和free的次数进行控制判断

===》这里根据老师的逻辑，用文件的方式进行控制。

测试代码如下：

```text
// malloc和free 之间的关联是申请内存的地址，以该地址作为基准
// malloc时写入一个文件，打印行数等必要信息  free时删除这个文件 通过有剩余文件判断内存泄露
#define _GNU_SOURCE
#include <dlfcn.h>  //对应的头文件
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

typedef void *(*malloc_t)(size_t size);
malloc_t malloc_f = NULL;
typedef void (*free_t)(void* p);
free_t free_f = NULL;

int enable_malloc_hook = 1;
int enable_free_hook = 1;

#define MEM_FILE_LENGTH 40
void * malloc(size_t size){
    if(enable_malloc_hook) //对第三方调用导致的递归进行规避
    {
        enable_malloc_hook = 0;
        //实际的内存申请，根据该地址写文件和free 相互关联
        void *ptr =malloc_f(size);
        //打印上层调用的地址
        void *carrer = __builtin_return_address(0);
        printf("exec malloc [%p ]\n", carrer );
        
        //通过写入文件的方式 对malloc和free进行关联  malloc时写入文件
        char file_buff[MEM_FILE_LENGTH] = {0};
        sprintf(file_buff, "./mem/%p.mem", ptr);
        
        //打开文件写入必要信息 使用前创建目录级别
        FILE *fp = fopen(file_buff, "w");
        fprintf(fp, "[malloc addr : +%p ] ---->mem:%p  size:%lu \n",carrer, ptr, size);
        fflush(fp); //刷新写入文件
        
        enable_malloc_hook = 1;
        return ptr;
    }else
    {
         return malloc_f(size);
    }
}

void free(void * p){
    if(enable_free_hook){
        enable_free_hook = 0;
        void *carrer = __builtin_return_address(0);
        
        //free时删除文件  根据剩余文件判断内存泄露
        char file_buff[MEM_FILE_LENGTH] = {0};
        sprintf(file_buff, "./mem/%p.mem", p);
        //删除文件 根据malloc对应的指针
        if(unlink(file_buff) <0)
        {
            printf("double free: %p, %p \n", p, carrer);
        }
        //这里的打印实际就没意义了
        printf("exec free [%p]\n", carrer);
        free_f(p);
        enable_free_hook = 1;
    }else
    {
        free_f(p);
    }
}

//通过dlsym 对malloc和free使用前进行hook
static void init_malloc_free_hook(){
    //只需要执行一次
    if(malloc_f == NULL){
        malloc_f = dlsym(RTLD_NEXT, "malloc"); //除了RTLD_NEXT 还有一个参数RTLD_DEFAULT
    }
    
    if(free_f == NULL)
    {
        free_f =  dlsym(RTLD_NEXT, "free");
    }
    return ;
}
int main()
{
    init_malloc_free_hook(); //执行一次
    void * ptr1 = malloc(10);
    void * ptr2 = malloc(20);
    
    free(ptr1);
    
    void * ptr3 = malloc(30);
    free(ptr3);
    return 0;
}
```

执行结果：

```text
# 这里的打印只是为了理解  没有多大意义，真正的分析还得依靠文件目录
hlp@ubuntu:~/mem_test$ mkdir mem
hlp@ubuntu:~/mem_test$ gcc dlsym_hook_file.c -o dlsym_hook_file -ldl -g
hlp@ubuntu:~/mem_test$ ./dlsym_hook_file 
exec malloc [0x400ad5 ]
exec malloc [0x400ae3 ]
exec free [0x400af3]
exec malloc [0x400afd ]
exec free [0x400b0d]
hlp@ubuntu:~/mem_test$ cd mem/

# 这里在我们的目标目录下  看到有文件存在，说明存在内存泄露
hlp@ubuntu:~/mem_test/mem$ ls
0xe38680.mem
# 通过文件中的日志信息，对其进行分析，找到问题代码位置
hlp@ubuntu:~/mem_test/mem$ cat 0xe38680.mem 
[malloc addr : +0x400ae3 ] ---->mem:0xe38680  size:20 
hlp@ubuntu:~/mem_test/mem$ cd ../
#通过地址转换 找到我们有问题代码 没有释放的代码位置 
hlp@ubuntu:~/mem_test$ addr2line -fe ./dlsym_hook_file 0x400ae3
main
/home/hlp/mem_test/dlsym_hook_file.c:85
```

### 2.3 通过宏定义的方式对malloc/free进行hook

本质其实就是对系统调用的malloc/free进行替换，调用我们的目标方法，可以通过hook或者重载的方法实现。

使用宏定义的方式，实现malloc/free的替换。

```text
#include <stdio.h>
#include <stdlib.h>

//不能放在这里  放在这里  会对malloc_hook 和 free_hook 内部实际调用的也替换，就形成的递归调用了 并且无法规避
//#define malloc(size) 	malloc_hook(size, __FILE__, __LINE__)
//#define free(p) 		free_hook(p,  __FILE__, __LINE__)

#define MEM_FILE_LENGTH 40
//实现目标函数
void *malloc_hook(size_t size, const char* file, int line)
{
    //这里还是通过文件的方式进行识别
    void *ptr =malloc(size);
    char file_name_buff[MEM_FILE_LENGTH] = {0};
    sprintf(file_name_buff, "./mem/%p.mem", ptr);
        
    //打开文件写入必要信息 使用前创建目录级别
    FILE *fp = fopen(file_name_buff, "w");
    fprintf(fp, "[file:%s  line:%d ] ---->mem:%p  size:%lu \n",file, line, ptr, size);
    fflush(fp); //刷新写入文件
    
    printf("exec malloc [%p:%lu], file: %s, line:%d \n", ptr, size, file, line );
    return ptr;
}

void free_hook(void *p, const char* file, int line)
{
    char file_name_buff[MEM_FILE_LENGTH] = {0};
    sprintf(file_name_buff, "./mem/%p.mem", p);
    
    if(unlink(file_name_buff) <0)
    {
        printf("double free: %p, file: %s. line :%d \n", p, file, line);
    }
    
    //这里的打印实际就没意义了
    printf("exec free [%p], file: %s line:%d \n", p, file, line);
    free(p);
}
//宏定义实现代码中调用malloc/free时调用我们目标函数
#define malloc(size) 	malloc_hook(size, __FILE__, __LINE__)
#define free(p) 		free_hook(p,  __FILE__, __LINE__)

int main()
{
    //init_malloc_free_hook(); //执行一次
    void * ptr1 = malloc(10);
    void * ptr2 = malloc(20);
    
    free(ptr1);
    
    void * ptr3 = malloc(30);
    free(ptr3);
    return 0;
}
```

代码执行如下：

```text
hlp@ubuntu:~/mem_test$ gcc define_hook.c -o define_hook
hlp@ubuntu:~/mem_test$ ./define_hook 
exec malloc [0x1f91010:10], file: define_hook.c, line:47 
exec malloc [0x1f92680:20], file: define_hook.c, line:48 
exec free [0x1f91010], file: define_hook.c line:50 
exec malloc [0x1f938e0:30], file: define_hook.c, line:52 
exec free [0x1f938e0], file: define_hook.c line:53 
#通过目录下文件 以及文件内容  可以分析到代码有问题的行号
#注意  测试前最好清空目录下的文件 
hlp@ubuntu:~/mem_test$ ls ./mem
0x1f92680.mem
hlp@ubuntu:~/mem_test$ cat ./mem/0x1f92680.mem 
[file:define_hook.c  line:48 ] ---->mem:0x1f92680  size:20
```

### 2.4 _libc_malloc

对malloc进行劫持 使用实际的内存申请_libc_malloc 进行申请内存以及其他控制

```text
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
//实际内存申请的函数
extern void *__libc_malloc(size_t size);
int enable_malloc_hook = 1;

extern void __libc_free(void* p);
int enable_free_hook = 1;

// func --> malloc() { __builtin_return_address(0)}
// callback --> func --> malloc() { __builtin_return_address(1)}
// main --> callback --> func --> malloc() { __builtin_return_address(2)}

//calloc, realloc
void *malloc(size_t size) {

	if (enable_malloc_hook) {
		enable_malloc_hook = 0;

		void *p = __libc_malloc(size); //重载达到劫持后 实际内存申请
		void *caller = __builtin_return_address(0); // 0
		
		char buff[128] = {0};
		sprintf(buff, "./mem/%p.mem", p);

		FILE *fp = fopen(buff, "w");
		fprintf(fp, "[+%p] --> addr:%p, size:%ld\n", caller, p, size);
		fflush(fp);

		//fclose(fp); //free
		
		enable_malloc_hook = 1;
		return p;
	} else {
		return __libc_malloc(size);
	}
	
	return NULL;
}


void free(void *p) {
	if (enable_free_hook) {

		enable_free_hook = 0;

		char buff[128] = {0};
		sprintf(buff, "./mem/%p.mem", p);

		if (unlink(buff) < 0) { // no exist
			printf("double free: %p\n", p);
		}
		
		__libc_free(p);

		// rm -rf p.mem
		enable_free_hook = 1;

	} else {
		__libc_free(p);
	}
}

int main()
{
    //init_malloc_free_hook(); //执行一次
    void * ptr1 = malloc(10);
    void * ptr2 = malloc(20);
    
    free(ptr1);
    
    void * ptr3 = malloc(30);
    free(ptr3);
    return 0;
}
```

代码运行及分析如下：

```text
hlp@ubuntu:~/mem_test$ gcc libc_hook.c -o libc_hook -g
hlp@ubuntu:~/mem_test$ ./libc_hook 
#通过查看目录下生成的文件   可以知道有内存泄露 通过日志内部信息  使用addr2line 通过打印地址找到对应的代码位置
hlp@ubuntu:~/mem_test$ cat ./mem/0x733270.mem 
[+0x4009f7] --> addr:0x733270, size:20
hlp@ubuntu:~/mem_test$ addr2line -fe ./libc_hook 0x4009f7
main
/home/hlp/mem_test/libc_hook.c:69   #找到问题代码位置
```

### 2.5 mem_trace

原理：glic提供__malloc_hook, __realloc_hook, __free_hook可以实现hook自定义malloc/free函数

```text
#include <stdlib.h>
#include <stdio.h>
#include <unistd.h>
#include <malloc.h>

	  /* #include <malloc.h>
       void *(*__malloc_hook)(size_t size, const void *caller);
       void *(*__realloc_hook)(void *ptr, size_t size, const void *caller);
       void *(*__memalign_hook)(size_t alignment, size_t size,
                                const void *caller);
       void (*__free_hook)(void *ptr, const void *caller);
       void (*__malloc_initialize_hook)(void);
       void (*__after_morecore_hook)(void);*/
//#include <mcheck.h>

typedef void *(*malloc_hook_t)(size_t size, const void *caller);
typedef void (*free_hook_t)(void *p, const void *caller);

malloc_hook_t 	old_malloc_f = NULL;
free_hook_t 	old_free_f = NULL;
int replaced = 0;

void mem_trace(void);
void mem_untrace(void);

void *malloc_hook_f(size_t size, const void *caller) {

	mem_untrace();
	void *ptr = malloc(size);
	//printf("+%p: addr[%p]\n", caller, ptr);
	char buff[128] = {0};
	sprintf(buff, "./mem/%p.mem", ptr);

	FILE *fp = fopen(buff, "w");
	fprintf(fp, "[+%p] --> addr:%p, size:%ld\n", caller, ptr, size);
	fflush(fp);

	fclose(fp); //free
	mem_trace();
	return ptr;
}

void *free_hook_f(void *p, const void *caller) {

	mem_untrace();
	//printf("-%p: addr[%p]\n", caller, p);

	char buff[128] = {0};
	sprintf(buff, "./mem/%p.mem", p);

	if (unlink(buff) < 0) { // no exist
		printf("double free: %p\n", p);
		return NULL;
	}
	free(p);
	mem_trace();
}
//对__malloc_hook 和__free_hook 重赋值 
void mem_trace(void) { //mtrace
	replaced = 1;      
	old_malloc_f = __malloc_hook; //malloc --> 
	old_free_f = __free_hook;

	__malloc_hook = malloc_hook_f;
	__free_hook = free_hook_f;
}

//还原 __malloc_hook 和__free_hook
void mem_untrace(void) {
	__malloc_hook = old_malloc_f;
	__free_hook = old_free_f;
	replaced = 0;
}

int main()
{
    mem_trace();  //mtrace();    //进行hook劫持
    void * ptr1 = malloc(10);
    void * ptr2 = malloc(20);
    
    free(ptr1);
    
    void * ptr3 = malloc(30);
    free(ptr3);
    mem_untrace();  //muntrace(); //取消劫持
    return 0;
}
```

这里编译的时候有一些警告，但是编译成功了，

除此之外 相关资料可以用 man __malloc_hook 去了解

```text
hlp@ubuntu:~/mem_test$ gcc 3.c -o 3 -g
hlp@ubuntu:~/mem_test$ ./3
hlp@ubuntu:~/mem_test$ cat mem/0x1789030.mem 
[+0x400ab2] --> addr:0x1789030, size:20
hlp@ubuntu:~/mem_test$ addr2line -fe ./3 0x400ab2
main
#可以确定问题代码位置
/home/hlp/mem_test/3.c:79
```

## 3.总结：

内存泄露的本质是，在堆上分配内存，使用malloc/calloc/realloc 以及内存的释放free不匹配导致的。

怎么检测，定位内存泄露？：

===》实际上对内存管理的相关函数进行劫持（hook），增加一些必要信息供我们分析。

===》不同的方案，其实就是劫持(hook的方式不一样)

===》可以使用重载，宏定义，操作系统提供的hook方式（__malloc_hook）等不通的方案

===》在hook的基础上，要进行分析，需要用相关的策略，这里用的多个文件，可以用数据结构管理。

===》除此之外，__builtin_return_address函数可以获取函数调用地址，以及addr2line命令对地址和代码行数进行转换，确定问题代码位置。

===》编译的时候，要加-g,addr2line才可用。

这里只是简单的内存泄露hook的一些方案demo，有关多线程等细节再实际项目中也需要考虑。

原文地址：https://zhuanlan.zhihu.com/p/609107360

作者：linux

# 【NO.586】linux：孤儿进程与僵尸进程产生及其处理

在探讨这个问题之前，我们先来弄清什么是进程。

进程(Process)是计算机中的程序关于某数据集合上的一次运行活动，是系统进行资源分配和调度的基本单位，是操作系统结构的基础。程序是指令、数据及其组织形式的描述，进程是程序的实体。进程是一个具有独立功能的程序关于某个数据集合的一次运行活动。它可以申请和拥有系统资源，是一个动态的概念，是一个活动的实体。它不只是程序的代码，还包括当前的活动，通过程序计数器的值和处理寄存器的内容来表示。通俗点讲，进程是一段程序的执行过程，是个动态概念。

## 1.进程状态

![img](https://pic3.zhimg.com/80/v2-112f2c463b1ea0b91dab286da25c17d6_720w.webp)

程序运行必须加载在内存中，当有过多的就绪态或阻塞态进程在内存中没有运行，因为内存很小，有可能不足。系统需要把他们移动到内存外磁盘中，称为挂起状态。就绪状态的进程挂起就是挂起就绪状态，阻塞进程挂起就称为阻塞挂起状态。

每个进程的产生都有自己的唯一的ID号(pid)，并且附带有一个它父进程的ID号(ppid)。进程死亡时，ID被回收。

进程间靠优先级获得CPU资源，时间片段轮换来更新优先级，以保证不会一个进程占据CPU时间过长。每个进程都得到轮换运行，因为这个时间非常短，所以给我们就好像是系统在同时运行好多进程。

## 2.僵尸进程

![img](https://pic4.zhimg.com/80/v2-b892e63b083cac8aba1d82a40775ad43_720w.webp)

那么什么称为僵尸进程呢？

即子进程先于父进程退出后，子进程的PCB需要其父进程释放，但是父进程并没有释放子进程的PCB，这样的子进程就称为僵尸进程，僵尸进程实际上是一个已经死掉的进程。我们用代码来看一下

```text
#include<stdio.h>
#include<stdlib.h>
#include<unistd.h>
#include<string.h>
#include<assert.h>
#include<sys/types.h>
 
int main()
{
	pid_t pid=fork();
 
	if(pid==0)  //子进程
	{
     	printf("child id is %d\n",getpid());
		printf("parent id is %d\n",getppid());
	}
	else  //父进程不退出，使子进程成为僵尸进程
	{
        while(1)
		{}
	}
	exit(0);
}
```

我们将它挂在后台执行，可以看到结果，用ps可以看到子进程后有一个<defunct> ,defunct是已死的，僵尸的意思,可以看出这时的子进程已经是一个僵尸进程了。因为子进程已经结束，而其父进程并未释放其PCB，所以产生了这个僵尸进程。

![img](https://pic4.zhimg.com/80/v2-82c09875b63801bbbb1f0d170c091fbb_720w.webp)

我们也可以用ps -aux | grep pid 查看进程状态

![img](https://pic3.zhimg.com/80/v2-66c26e8e52a463f1fb858e63c7ee63fa_720w.webp)

一个进程在调用exit命令结束自己的生命的时候，其实它并没有真正的被销毁，而是留下一个称为僵尸进程（Zombie）的数据结构（系统调用exit，它的作用是使进程退出，但也仅仅限于将一个正常的进程变成一个僵尸进程，并不能将其完全销毁）。在Linux进程的状态中，僵尸进程是非常特殊的一种，它已经放弃了几乎所有内存空间，没有任何可执行代码，也不能被调度，仅仅在进程列表中保留一个位置，记载该进程的退出状态等信息供其他进程收集，除此之外，僵尸进程不再占有任何内存空间。这个僵尸进程需要它的父进程来为它收尸，如果他的父进程没有处理这个僵尸进程的措施，那么它就一直保持僵尸状态，如果这时父进程结束了，那么init进程自动会接手这个子进程，为它收尸，它还是能被清除的。但是如果如果父进程是一个循环，不会结束，那么子进程就会一直保持僵尸状态，这就是为什么系统中有时会有很多的僵尸进程。

试想一下，如果有大量的僵尸进程驻在系统之中，必然消耗大量的系统资源。但是系统资源是有限的，因此当僵尸进程达到一定数目时，系统因缺乏资源而导致奔溃。所以在实际编程中，避免和防范僵尸进程的产生显得尤为重要。

## 3.孤儿进程

一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。

子进程死亡需要父进程来处理，那么意味着正常的进程应该是子进程先于父进程死亡。当父进程先于子进程死亡时，子进程死亡时没父进程处理，这个死亡的子进程就是孤儿进程。

但孤儿进程与僵尸进程不同的是，由于父进程已经死亡，系统会帮助父进程回收处理孤儿进程。所以孤儿进程实际上是不占用资源的，因为它终究是被系统回收了。不会像僵尸进程那样占用ID,损害运行系统。

下来我们上代码看看：

```text
#include<stdio.h>
#include<stdlib.h>
#include<unistd.h>
#include<string.h>
#include<assert.h>
#include<sys/types.h>
 
int main()
{
	pid_t pid=fork();
 
	if(pid==0)
	{
		printf("child ppid is %d\n",getppid());
		sleep(10);     //为了让父进程先结束
		printf("child ppid is %d\n",getppid());
	}
	else
	{
		printf("parent id is %d\n",getpid());
	}
 
	exit(0);
}
```

![img](https://pic1.zhimg.com/80/v2-356005d5cef486ff1c8e0826a581bff8_720w.webp)

从执行结果来看，此时由pid == 4168父进程创建的子进程，其输出的父进程pid == 1，说明当其为孤儿进程时被init进程回收，最终并不会占用资源，这就是为什么要将孤儿进程分配给init进程。

## 4.僵尸进程处理方式

任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。这是每个子进程在结束时都要经过的阶段。如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“defunct”。如果父进程能及时处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。所以孤儿进程不会占资源，僵尸进程会占用资源危害系统。我们应当避免僵尸进程的出现。

解决方式如下：

1）：一种比较暴力的做法是将其父进程杀死，那么它的子进程，即僵尸进程会变成孤儿进程，由系统来回收。但是这种做法在大多数情况下都是不可取的，如父进程是一个服务器程序，如果为了回收其子进程的资源，而杀死服务器程序，那么将导致整个服务器崩溃，得不偿失。显然这种回收进程的方式是不可取的，但其也有一定的存在意义。

2）：SIGCHLD信号处理

我们都知道wait函数是用来处理僵尸进程的，但是进程一旦调用了wait，就立即阻塞自己，由wait自动分析是否当前进程的某个子进程已经退出，如果让它找到了这样一个已经变成僵尸的子进程，wait就会收集这个子进程的信息，并把它彻底销毁后返回；如果没有找到这样一个子进程，wait就会一直阻塞在这里，直到有一个出现为止。我们先来看看wait函数的定义

\#include <sys/types.h> /* 提供类型pid_t的定义，实际就是int型 */

\#include <sys/wait.h>

pid_t wait(int *status)

参数status用来保存被收集进程退出时的一些状态，它是一个指向int类型的指针。但如果我们对这个子进程是如何死掉的毫不在意，只想把这个僵尸进程消灭掉，（事实上绝大多数情况下，我们都会这样想），我们就可以设定这个参数为NULL，就象下面这样：pid=wait(NULL)；如果成功，wait会返回被收集的子进程的进程ID，如果调用进程没有子进程，调用就会失败，此时wait返回-1，同时errno被置为ECHILD。

由于调用wait之后，就必须阻塞，直到有子进程结束，所以，这样来说是非常不高效的，我们的父进程难道要一直等待你子进程完成，最后才能执行自己的代码吗？难道就不能我父进程执行自己的代码，你子进程什么时候完成我就什么时候去处理你，不用一直等你？当然是有这种方式了。

实际上当子进程终止时，内核就会向它的父进程发送一个SIGCHLD信号，父进程可以选择忽略该信号，也可以提供一个接收到信号以后的处理函数。对于这种信号的系统默认动作是忽略它。我们不希望有过多的僵尸进程产生，所以当父进程接收到SIGCHLD信号后就应该调用 wait 或 waitpid 函数对子进程进行善后处理，释放子进程占用的资源。

下面是一个处理僵尸进程的简单的例子：

```text
#include<stdio.h>
#include<stdlib.h>
#include<unistd.h>
#include<string.h>
#include<assert.h>
#include<sys/types.h>
#include<sys/wait.h>
#include<signal.h>
 
void deal_child(int num)
{
	printf("deal_child into\n");
	wait(NULL);
}
 
int main()
{
	signal(SIGCHLD,deal_child);
	pid_t pid=fork();
	int i;
 
	if(pid==0)
	{
		printf("child is running\n");
		sleep(2);
		printf("child will end\n");
	}
	else
	{
		sleep(1);   //让子进程先执行
		printf("parent is running\n");
		sleep(10);    //一旦被打断就不能再进入睡眠
		printf("sleep 10 s over\n");
		sleep(5);
		printf("sleep 5s over\n");
	}
 
	exit(0);
}
```

进行测试后确定了是在父进程睡眠10s时子进程结束，父进程接收到了SIGCHLD信号，调用了deal_child函数，释放了子进程的PCB后又回到自己本身的代码中执行。我们看看运行结果

![img](https://pic3.zhimg.com/80/v2-79da0346021ba94716a3562cf8e8d45a_720w.webp)

说到这里，我们再来看看signal函数（不是阻塞函数）

signal（参数1，参数2）；

参数1：我们要进行处理的信号。系统的信号我们可以再终端键入 kill -l查看(共64个)。其实这些信号是系统定义的宏。

参数2：我们处理的方式（是系统默认还是忽略还是捕获）。

eg: signal(SIGINT ,SIG_ING ); //SIG_ING 代表忽略SIGINT信号

eg:signal(SIGINT,SIG_DFL); //SIGINT信号代表由InterruptKey产生，通常是CTRL +C或者是DELETE。发送给所有ForeGroundGroup的进程。 SIG_DFL代表执行系统默认操作，其实对于大多数信号的系统默认动作是终止该进程。这与不写此处理函数是一样的。

我们也可以给参数2传递一个信号处理函数的地址，但是这个信号处理函数需要其返回值为void，并且默认自带一个int类型参数

这个int就是你所传递的第一个信号参数的值（你用kill -l可以查看）

我们测试了一下，如果创建了5个子进程，但是销毁的时候仍然有两个仍是僵尸进程，这又是为什么呢？

这是因为当5个进程同时终止的时候，内核都会向父进程发送SIGCHLD信号，而父进程此时有可能仍然处于信号处理的deal_child函数中，那么在处理完之前，中间接收到的SIGCHLD信号就会丢失，内核并没有使用队列等方式来存储同一种信号

所以为了解决这一问题，我们需要调用waitpid函数来清理子进程。

```text
void deal_child(int sig_no)
 
{
 
    for (;;) {
 
        if (waitpid(-1, NULL, WNOHANG) == 0)
 
            break;
 
    }  
 
}
```

这样的话，只有检验没有僵尸进程，他才会返回0，这样就可以确保所有的僵尸进程都被杀死了。

原文地址：https://zhuanlan.zhihu.com/p/441204477

作者：linux

# 【NO.587】linux异步IO编程实例分析

![动图封面](https://pic2.zhimg.com/v2-0855a1508418e6ca6cb4b933f2dc3e61_b.jpg)



异步阻塞IO模型的流程图

在Direct IO模式下，异步是非常有必要的（因为绕过了pagecache，直接和磁盘交互）。linux Native AIO正是基于这种场景设计的，具体的介绍见：KernelAsynchronousI/O (AIO)SupportforLinux。下面我们就来分析一下AIO编程的相关知识。

阻塞模式下的IO过程如下：

```text
int fd = open(const char *pathname, int flags, mode_t mode);
ssize_t pread(int fd, void *buf, size_t count, off_t offset);
ssize_t pwrite(int fd, const void *buf, size_t count, off_t offset);
int close(int fd);
```

因为整个过程会等待read/write的返回，所以不需要任何额外的数据结构。但异步IO的思想是：应用程序不能阻塞在昂贵的系统调用上让CPU睡大觉，而是将IO操作抽象成一个个的任务单元提交给内核，内核完成IO任务后将结果放在应用程序可以取到的地方。这样在底层做I/O的这段时间内，CPU可以去干其他的计算任务。但异步的IO任务批量的提交和完成，必须有自身可描述的结构，最重要的两个就是iocb和io_event。

## 1.libaio中的structs

```text
struct iocb {
        void     *data;  /* Return in the io completion event */
        unsigned key;   /*r use in identifying io requests */
        short           aio_lio_opcode;
        short           aio_reqprio;
        int             aio_fildes;
        union {
                struct io_iocb_common           c;
                struct io_iocb_vector           v;
                struct io_iocb_poll             poll;
                struct io_iocb_sockaddr saddr;
        } u;
};
struct io_iocb_common {
        void            *buf;
        unsigned long   nbytes;
        long long       offset;
        unsigned        flags;
        unsigned        resfd;
};
```

iocb是提交IO任务时用到的，可以完整地描述一个IO请求：

data是留给用来自定义的指针：可以设置为IO完成后的callback函数；

aio_lio_opcode表示操作的类型：IO_CMD_PWRITE | IO_CMD_PREAD；

aio_fildes是要操作的文件：fd；

io_iocb_common中的buf, nbytes, offset分别记录的IO请求的mem buffer，大小和偏移。

```text
struct io_event {
        void *data;
        struct iocb *obj;
        unsigned long res;
        unsigned long res2;
};
```

io_event是用来描述返回结果的：

obj就是之前提交IO任务时的iocb；

res和res2来表示IO任务完成的状态。

## 2.libaio提供的API和完成IO的过程

libaio提供的API有：io_setup, io_submit, io_getevents, io_destroy。

\1. 建立IO任务

```text
int io_setup (int maxevents, io_context_t *ctxp);
```

io_context_t对应内核中一个结构，为异步IO请求提供上下文环境。注意在setup前必须将io_context_t初始化为0。

当然，这里也需要open需要操作的文件，注意设置O_DIRECT标志。

2.提交IO任务

```text
long io_submit (aio_context_t ctx_id, long nr, struct iocb **iocbpp);
```

提交任务之前必须先填充iocb结构体，libaio提供的包装函数说明了需要完成的工作：

```text
void io_prep_pread(struct iocb *iocb, int fd, void *buf, size_t count, long long offset)
{
        memset(iocb, 0, sizeof(*iocb));
        iocb->aio_fildes = fd;
        iocb->aio_lio_opcode = IO_CMD_PREAD;
        iocb->aio_reqprio = 0;
        iocb->u.c.buf = buf;
        iocb->u.c.nbytes = count;
        iocb->u.c.offset = offset;
}
void io_prep_pwrite(struct iocb *iocb, int fd, void *buf, size_t count, long long offset)
{
        memset(iocb, 0, sizeof(*iocb));
        iocb->aio_fildes = fd;
        iocb->aio_lio_opcode = IO_CMD_PWRITE;
        iocb->aio_reqprio = 0;
        iocb->u.c.buf = buf;
        iocb->u.c.nbytes = count;
        iocb->u.c.offset = offset;
}
```

这里注意读写的buf都必须是按扇区对齐的，可以用posix_memalign来分配。

3.获取完成的IO

```text
long io_getevents (aio_context_t ctx_id, long min_nr, long nr, struct io_event *events, struct timespec *timeout);
```

这里最重要的就是提供一个io_event数组给内核来copy完成的IO请求到这里，数组的大小是io_setup时指定的maxevents。

timeout是指等待IO完成的超时时间，设置为NULL表示一直等待所有到IO的完成。

4.销毁IO任务

```text
int io_destroy (io_context_t ctx);
```

## 3.libaio和epoll的结合

在异步编程中，任何一个环节的阻塞都会导致整个程序的阻塞，所以一定要避免在io_getevents调用时阻塞式的等待。还记得io_iocb_common中的flags和resfd吗？看看libaio是如何提供io_getevents和事件循环的结合：

```text
void io_set_eventfd(struct iocb *iocb, int eventfd)
{
        iocb->u.c.flags |= (1 << 0) /* IOCB_FLAG_RESFD */;
        iocb->u.c.resfd = eventfd;
}
```

这里的resfd是通过系统调用eventfd生成的。

```text
int eventfd(unsigned int initval, int flags);
```

eventfd是linux 2.6.22内核之后加进来的syscall，作用是内核用来通知应用程序发生的事件的数量，从而使应用程序不用频繁地去轮询内核是否有时间发生，而是由内核将发生事件的数量写入到该fd，应用程序发现fd**可读**后，从fd读取该数值，并马上去内核读取。

有了eventfd，就可以很好地将libaio和epoll事件循环结合起来：

\1. 创建一个eventfd

```text
efd = eventfd(0, EFD_NONBLOCK | EFD_CLOEXEC);
```

\2. 将eventfd设置到iocb中

```text
io_set_eventfd(iocb, efd);
```

\3. 交接AIO请求

```text
io_submit(ctx, NUM_EVENTS, iocb);
```

\4. 创建一个epollfd，并将eventfd加到epoll中

```text
epfd = epoll_create(1);
epoll_ctl(epfd, EPOLL_CTL_ADD, efd, &epevent);
epoll_wait(epfd, &epevent, 1, -1);
```

\5. 当eventfd可读时，从eventfd读出完成IO请求的数量，并调用io_getevents获取这些IO

```text
read(efd, &finished_aio, sizeof(finished_aio);
r = io_getevents(ctx, 1, NUM_EVENTS, events, &tms);
```



![动图封面](https://pic4.zhimg.com/v2-1d5d9ee387fab4bc680d85f9489ca1b7_b.jpg)



异步非阻塞IO模型的流程图

## 4.一个完整的编程实例

```text
#define _GNU_SOURCE
#define __STDC_FORMAT_MACROS


#include <stdio.h>
#include <errno.h>
#include <libaio.h>
#include <sys/eventfd.h>
#include <sys/epoll.h>
#include <stdlib.h>
#include <sys/types.h>
#include <unistd.h>
#include <stdint.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <inttypes.h>


#define TEST_FILE   "aio_test_file"
#define TEST_FILE_SIZE  (127 * 1024)
#define NUM_EVENTS  128
#define ALIGN_SIZE  512
#define RD_WR_SIZE  1024


struct custom_iocb
{
    struct iocb iocb;
    int nth_request;
};


void aio_callback(io_context_t ctx, struct iocb *iocb, long res, long res2)
{
    struct custom_iocb *iocbp = (struct custom_iocb *)iocb;
    printf("nth_request: %d, request_type: %s, offset: %lld, length: %lu, res: %ld, res2: %ld\n", 
            iocbp->nth_request, (iocb->aio_lio_opcode == IO_CMD_PREAD) ? "READ" : "WRITE",
            iocb->u.c.offset, iocb->u.c.nbytes, res, res2);
}


int main(int argc, char *argv[])
{
    int efd, fd, epfd;
    io_context_t ctx;
    struct timespec tms;
    struct io_event events[NUM_EVENTS];
    struct custom_iocb iocbs[NUM_EVENTS];
    struct iocb *iocbps[NUM_EVENTS];
    struct custom_iocb *iocbp;
    int i, j, r;
    void *buf;
    struct epoll_event epevent;


    efd = eventfd(0, EFD_NONBLOCK | EFD_CLOEXEC);
    if (efd == -1) {
        perror("eventfd");
        return 2;
    }


    fd = open(TEST_FILE, O_RDWR | O_CREAT | O_DIRECT, 0644);
    if (fd == -1) {
        perror("open");
        return 3;
    }
    ftruncate(fd, TEST_FILE_SIZE);
    
    ctx = 0;
    if (io_setup(8192, &ctx)) {
        perror("io_setup");
        return 4;
    }


    if (posix_memalign(&buf, ALIGN_SIZE, RD_WR_SIZE)) {
        perror("posix_memalign");
        return 5;
    }
    printf("buf: %p\n", buf);


    for (i = 0, iocbp = iocbs; i < NUM_EVENTS; ++i, ++iocbp) {
        iocbps[i] = &iocbp->iocb;
        io_prep_pread(&iocbp->iocb, fd, buf, RD_WR_SIZE, i * RD_WR_SIZE);
        io_set_eventfd(&iocbp->iocb, efd);
        io_set_callback(&iocbp->iocb, aio_callback);
        iocbp->nth_request = i + 1;
    }


    if (io_submit(ctx, NUM_EVENTS, iocbps) != NUM_EVENTS) {
        perror("io_submit");
        return 6;
    }


    epfd = epoll_create(1);
    if (epfd == -1) {
        perror("epoll_create");
        return 7;
    }


    epevent.events = EPOLLIN | EPOLLET;
    epevent.data.ptr = NULL;
    if (epoll_ctl(epfd, EPOLL_CTL_ADD, efd, &epevent)) {
        perror("epoll_ctl");
        return 8;
    }


    i = 0;
    while (i < NUM_EVENTS) {
        uint64_t finished_aio;


        if (epoll_wait(epfd, &epevent, 1, -1) != 1) {
            perror("epoll_wait");
            return 9;
        }


        if (read(efd, &finished_aio, sizeof(finished_aio)) != sizeof(finished_aio)) {
            perror("read");
            return 10;
        }


        printf("finished io number: %"PRIu64"\n", finished_aio);
    
        while (finished_aio > 0) {
            tms.tv_sec = 0;
            tms.tv_nsec = 0;
            r = io_getevents(ctx, 1, NUM_EVENTS, events, &tms);
            if (r > 0) {
                for (j = 0; j < r; ++j) {
                    ((io_callback_t)(events[j].data))(ctx, events[j].obj, events[j].res, events[j].res2);
                }
                i += r;
                finished_aio -= r;
            }
        }
    }
    
    close(epfd);
    free(buf);
    io_destroy(ctx);
    close(fd);
    close(efd);
    remove(TEST_FILE);


    return 0;
}
```


说明：

\1. 在centos 6.2 (libaio-devel 0.3.107-10) 上运行通过

\2. struct io_event中的res字段表示读到的字节数或者一个负数错误码。在后一种情况下，-res表示对应的

errno。res2字段为0表示成功，否则失败

\3. iocb在aio请求执行过程中必须是valid的

\4. 在上面的程序中，通过扩展iocb结构来保存额外的信息(nth_request)，并使用iocb.data

来保存回调函数的地址。如果回调函数是固定的，那么也可以使用iocb.data来保存额外信息。

原文地址：https://zhuanlan.zhihu.com/p/610147885

作者：linux

# 【NO.588】透视Linux内核，BPF 深度分析与案例讲解

本次主要对BPF的部分原理、应用案例上进行一次分析记录。

## 1.BPF介绍

当内核触发事件时，BPF虚拟机能够运行相应的BPF程序指令，但是并不是意味着BPF程序能访问内核触发的所有事件。将BPF目标文件加载到BPF虚拟机时，需要确定特定的程序类型，内核根据BPF程序类型决定BPF程序的上下文参数、在什么事件发生的时候触发，如BPF_PROG_TYPE_SOCKET_FILTER型程序(套接字过滤器程序)的上下文参数是struct __sk_buff(注意并不是struct sk_buff,其只是对struct sk_buff结构体的关键成员变量的封装)，套接字过滤器程序在嗅探到网络收发包事件上触发；BPF_PROG_TYPE_KPROBE(kprobe类型BPF程序)的上下文参数是struct pt regs *ctx，可以从中访问寄存器，BPF附件都kprobe后，启动并命中时触发。

上面的描述中涉及到的几个重要的点：

什么是BPF虚拟机？什么是BPF程序指令？BPF文件目标格式是什么？BPF程序类型有哪些？用户编写的程序是如何加载的？下面进行分析：

## 2.BPF指令与BPF虚拟机

BPF指令解析：

核心的结构体如下，每一条eBPF指令都以一个bpf_insn来表示，在cBPF中是其他的一个结构体(struct sock_filter	)，不过最终都会转换成统一的格式，这里我们只研究eBPF。

```text
struct bpf_insn {
 __u8 code;  /* opcode */
 __u8 dst_reg:4; /* dest register */
 __u8 src_reg:4; /* source register */
 __s16 off;  /* signed offset */
 __s32 imm;  /* signed immediate constant */
};
```

由结构体中`__u8	code;`可以知道，一条bpf指令是8个字节长。这8位的0，1，2位表示的是该操作指令的类别，共8种：

BPF_LD(0X00) 、BPF_LDX(0x01) 、BPF_ST(0x02)、BPF_STX(0x03) 、BPF_ALU(0x04)、BPF_JMP(0x05)、BPF_RET(0x06) 、BPF_MISC(0x07)

BPF(默认指eBPF非cBPF) 程序指令都是64位的，使用了 11 个 64位寄存器，32位称为半寄存器（subregister）和一个程序计数器（program counter），一个大小为 512 字节的 BPF 栈。所有的 BPF 指令都有着相同的编码方式。eBPF虚拟指令系统属于RISC，拥有11个虚拟寄存器，r0-r10，在实际运行时，虚拟机会把这11个寄存器一 一对应于硬件CPU的物理寄存器。如下是x64为例，虚拟寄存器与真实的寄存器的对应关系以及BPF指令的编码方式：

```text
   //R0 - 保存返回值
    //R1-R5 参数传递
    //R6-R9 保存临时变量
    //R10 只读，用做栈指针
    R0 – rax
    R1 - rdi
    R2 - rsi
    R3 - rdx
    R4 - rcx
    R5 - r8
    R6 - rbx
    R7 - r13
    R8 - r14
    R9 - r15
    R10 – rbp（帧指针，frame pointer）
```



```text
msb                                                        lsb
+------------------------+----------------+----+----+--------+
|immediate               |offset          |src |dst |opcode  |
+------------------------+----------------+----+----+--------+
```

从最低位到最高位分别是：

- 8 位的 opcode，有 BPF_X 类型的基于寄存器的指令，也有 BPF_K 类型的基于立即数的指令
- 4 位的目标寄存器 (dst)
- 4 位的原始寄存器 (src)
- 16 位的偏移（有符号），是相对于栈、映射值（map values）、数据包（packet data）等的相对偏移量
- 32 位的立即数 (imm)（有符号）

大多数指令都不会使用所有的域，未被使用的部分就会被置为0。编程时只需要将我们想要程序放到一个`bpf_insn`数组中,也就代表了整个BPF程序的所有指令，如下所示:

```text
struct bpf_insn insn[] = {
   BPF_MOV64_IMM(BPF_REG_1, 0xa21), 
      ......
    BPF_EXIT_INSN(),
};
```

如上面的 BPF_MOV64_IMM与 BPF_EXIT_INSN在内核include\linux\filter.h中的宏定义：

```text
#define BPF_MOV64_IMM(DST, IMM)     \
 ((struct bpf_insn) {     \
  .code  = BPF_ALU64 | BPF_MOV | BPF_K,  \
  .dst_reg = DST,     \
  .src_reg = 0,     \
  .off   = 0,     \
  .imm   = IMM })
  
#define BPF_EXIT_INSN()      \
 ((struct bpf_insn) {     \
  .code  = BPF_JMP | BPF_EXIT,   \
  .dst_reg = 0,     \
  .src_reg = 0,     \
  .off   = 0,     \
  .imm   = 0 })  
```

再例如，编写程序kprobe类型的BPF程序，当触发相应事件时输出helloword,。

将相应的BPF内核代码，写入结构体bpf_insn_prog中如下：

```text
struct bpf_insn prog[] = {
  BPF_MOV64_IMM(BPF_REG_1, 0xa21),        /* '!\n' */
        BPF_STX_MEM(BPF_H, BPF_REG_10, BPF_REG_1, -4),
        BPF_MOV64_IMM(BPF_REG_1, 0x646c726f),   /* 'orld' */
        BPF_STX_MEM(BPF_W, BPF_REG_10, BPF_REG_1, -8),
        BPF_MOV64_IMM(BPF_REG_1, 0x57202c6f),   /* 'o, W' */
        BPF_STX_MEM(BPF_W, BPF_REG_10, BPF_REG_1, -12),
        BPF_MOV64_IMM(BPF_REG_1, 0x6c6c6548),   /* 'Hell' */
        BPF_STX_MEM(BPF_W, BPF_REG_10, BPF_REG_1, -16),
        BPF_MOV64_IMM(BPF_REG_1, 0),   
        BPF_STX_MEM(BPF_B, BPF_REG_10, BPF_REG_1, -2),
        BPF_MOV64_REG(BPF_REG_1, BPF_REG_10),
        BPF_ALU64_IMM(BPF_ADD, BPF_REG_1, -16),
        BPF_MOV64_IMM(BPF_REG_2, 15),   
        BPF_RAW_INSN(BPF_JMP|BPF_CALL, 0,0,0, BPF_FUNC_trace_printk),
        BPF_MOV64_IMM(BPF_REG_0, 0),
        BPF_EXIT_INSN(),
 };
```

但是采用这种方式进行BPF编程需要熟悉BPF汇编，不建议使用，开发者往往使用C语言、python等高级语言(如BCC)编写BPF程序，然后通过llvm、clang等编译器将其编译成BPF字节码。

例如下面的BPF Hello word程序：

hello_kern.c:

```text
#include <uapi/linux/bpf.h>
#include <linux/version.h>
#include "bpf_helpers.h"
SEC("tracepoint/sched/sched_switch")
int bpf_prog(void *ctx){
   char msg[] = "hello BPF!\n";
   bpf_trace_printk(msg, sizeof(msg));
   return 0;
}

char _license[] SEC("license") = "GPL";
```

hello_user.c:

```text
#include <stdio.h>
#include "bpf_load.h"

int main(int argc, char **argv)
{
 if( load_bpf_file("hello_kern.o") != 0)
 {
  printf("The kernel didn't load BPF program\n");
  return -1;
 }

 read_trace_pipe();
 return 0;
}
```

编译运行该程序，可参考该链接([https://cloudnative.to/blog/compile-bpf-examples/](https://link.zhihu.com/?target=https%3A//cloudnative.to/blog/compile-bpf-examples/)) 主要是修改内核案例中写好的Makefile文件。

## 3.BPF目标文件

将内核给出的BPF案例进行编译后(BPF编程环境、编译参考[BPF编程环境搭建](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzg5MTU1ODgyMA%3D%3D%26mid%3D2247483953%26idx%3D1%26sn%3Df204e441e066302b9999cdcff690981d%26chksm%3Dcfcaccfaf8bd45ec2912ab7fa388a0286659fb6c315bf8244ba1bda5b981fe7f7ab5fbf6dc1d%26scene%3D21%23wechat_redirect))，编译后生成的ELF文件格式的目标文件，随便选择其中一个(sockex1_kern.o)进行分析：

**查看文件头信息**

```text
#如下Machine字段，显示：Linux BPF
dx@ubuntu:/usr/src/linux-4.15.0/samples/bpf$ readelf -h sockex1_kern.o
ELF Header:
  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 
  Class:                             ELF64
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              REL (Relocatable file)
  Machine:                           Linux BPF
  Version:                           0x1
  Entry point address:               0x0
  Start of program headers:          0 (bytes into file)
  Start of section headers:          528 (bytes into file)
  Flags:                             0x0
  Size of this header:               64 (bytes)
  Size of program headers:           0 (bytes)
  Number of program headers:         0
  Size of section headers:           64 (bytes)
  Number of section headers:         10
  Section header string table index: 1
```

**查看文件的节头信息**

如下所示，13行就是指明此BPF程序的类型，“socket”就是BPF_PROG_TYPE_SOCKET_FILTER类型，下面章节会进行介绍，其他的节头如"Map"、"license"是用户编程通过SEC()宏进行自定义的节头。

> SEC宏的定义：#define SEC(NAME) **attribute**((section(NAME), used))

```text
dx@ubuntu:/usr/src/linux-4.15.0/samples/bpf$ readelf -S sockex1_kern.o
There are 10 section headers, starting at offset 0x210:

Section Headers:
  [Nr] Name              Type             Address           Offset
       Size              EntSize          Flags  Link  Info  Align
  [ 0]                   NULL             0000000000000000  00000000
       0000000000000000  0000000000000000           0     0     0
  [ 1] .strtab           STRTAB           0000000000000000  000001b8
       0000000000000056  0000000000000000           0     0     1
  [ 2] .text             PROGBITS         0000000000000000  00000040
       0000000000000000  0000000000000000  AX       0     0     4
  [ 3] socket            PROGBITS         0000000000000000  00000040
       0000000000000078  0000000000000000  AX       0     0     8
  [ 4] .relsocket        REL              0000000000000000  00000198
       0000000000000010  0000000000000010           9     3     8
  [ 5] maps              PROGBITS         0000000000000000  000000b8
       000000000000001c  0000000000000000  WA       0     0     4
  [ 6] license           PROGBITS         0000000000000000  000000d4
       0000000000000004  0000000000000000  WA       0     0     1
  [ 7] .eh_frame         PROGBITS         0000000000000000  000000d8
       0000000000000030  0000000000000000   A       0     0     8
  [ 8] .rel.eh_frame     REL              0000000000000000  000001a8
       0000000000000010  0000000000000010           9     7     8
  [ 9] .symtab           SYMTAB           0000000000000000  00000108
       0000000000000090  0000000000000018           1     3     8
Key to Flags:
  W (write), A (alloc), X (execute), M (merge), S (strings), I (info),
  L (link order), O (extra OS processing required), G (group), T (TLS),
  C (compressed), x (unknown), o (OS specific), E (exclude),
  p (processor specific)
```

如程序sockex1_kern.c:

```text
#include <uapi/linux/bpf.h>
#include <uapi/linux/if_ether.h>
#include <uapi/linux/if_packet.h>
#include <uapi/linux/ip.h>
#include "bpf_helpers.h"
//定义了一个MAP头，声名要创建一个MAP，内核识别到"map"后会创建相应的内存
struct bpf_map_def SEC("maps") my_map = {
 .type = BPF_MAP_TYPE_ARRAY,
 .key_size = sizeof(u32),
 .value_size = sizeof(long),
 .max_entries = 256,
};
//定义了一个socket头，声明了该BPF的程序类型，而程序类型决定了什么时候触发BPF内核程序
SEC("socket1")
int bpf_prog1(struct __sk_buff *skb)
{
 int index = load_byte(skb, ETH_HLEN + offsetof(struct iphdr, protocol));
 long *value;

 if (skb->pkt_type != PACKET_OUTGOING)
  return 0;

 value = bpf_map_lookup_elem(&my_map, &index);
 if (value)
  __sync_fetch_and_add(value, skb->len);

 return 0;
}
//定义了一个license头，声名遵循的GPL协议
char _license[] SEC("license") = "GPL";
```

而声名的这些section，内核是如何识别、处理这些section的(也就是说BPF虚拟机是如何根据SEC属性进行执行相应程序的)将在后面章节进行分析。

下面将分析一下，BPF虚拟机是如何根据程序定义的section进行处理的：

上面的sockex1_kern.c通过编译成sockex1_kern.o的目标文件，并最终用户在用户态通过BPF辅助函数将sockex1_kern.o插入到内核，在BPF虚拟机上处理。

其中最重要的函数就是：`load_bpf_file` (samples/bpf/bpf_load.c)

> 用户态：main->`load_bpf_file` ->do_load_bpf_file

在`do_load_bpf_file`：

```text
static int do_load_bpf_file(const char *path, fixup_map_cb fixup_map)
{
    ......
        
// 循环读取elf中所有的sections,获取license和map相关信息
 for (i = 1; i < ehdr.e_shnum; i++) {
  if (get_sec(elf, i, &ehdr, &shname, &shdr, &data))//shname为“section”的名字
   continue;

  if (0) /* helpful for llvm debugging */
   printf("section %d:%s data %p size %zd link %d flags %d\n",
          i, shname, data->d_buf, data->d_size,
          shdr.sh_link, (int) shdr.sh_flags);
        //如果license字段
  if (strcmp(shname, "license") == 0) {
   processed_sec[i] = true;
   memcpy(license, data->d_buf, data->d_size);//把data->d_buf拷贝到license数组中
  } else if (strcmp(shname, "version") == 0) {   //如果是"version"字段
   processed_sec[i] = true;
   if (data->d_size != sizeof(int)) {
    printf("invalid size of version section %zd\n",
           data->d_size);
    return 1;
   }
   memcpy(&kern_version, data->d_buf, sizeof(int));//把data-d_buf拷贝到kern_version变量中
  } else if (strcmp(shname, "maps") == 0) { //如果是maps字段
   int j;

   maps_shndx = i;
   data_maps = data;
   for (j = 0; j < MAX_MAPS; j++)
    map_data[j].fd = -1;
  } else if (shdr.sh_type == SHT_SYMTAB) {
   strtabidx = shdr.sh_link;
   symbols = data;
  }
 }

   ......
       
 //解析map section,并创建bpf系统调用创建map
 if (data_maps) {
        //使用elf中的map section填充map_data
  nr_maps = load_elf_maps_section(map_data, maps_shndx,
      elf, symbols, strtabidx);
  if (nr_maps < 0) {
   printf("Error: Failed loading ELF maps (errno:%d):%s\n",
          nr_maps, strerror(-nr_maps));
   ret = 1;
   goto done;
  }
        /*
         使用系统调用syscall(__NR_bpf,0,attr,size);创建各个map,此时用户空间可以
         使用两个全局变量:map_fd,map_data来定位创建的map,全局变量map_data_count记录着
         该程序创建的map数量
        */
  if (load_maps(map_data, nr_maps, fixup_map))
   goto done;
  map_data_count = nr_maps;

  processed_sec[maps_shndx] = true;
 }

 /* process all relo sections, and rewrite bpf insns for maps */
 for (i = 1; i < ehdr.e_shnum; i++) {
  if (processed_sec[i])
   continue;

  if (get_sec(elf, i, &ehdr, &shname, &shdr, &data))
   continue;

  if (shdr.sh_type == SHT_REL) {
   struct bpf_insn *insns;

   /* locate prog sec that need map fixup (relocations) */
   if (get_sec(elf, shdr.sh_info, &ehdr, &shname_prog,
        &shdr_prog, &data_prog))
    continue;

   if (shdr_prog.sh_type != SHT_PROGBITS ||
       !(shdr_prog.sh_flags & SHF_EXECINSTR))
    continue;

   insns = (struct bpf_insn *) data_prog->d_buf;
   processed_sec[i] = true; /* relo section */

   if (parse_relo_and_apply(data, symbols, &shdr, insns,
       map_data, nr_maps))
    continue;
  }
 }

 /* load programs */
 for (i = 1; i < ehdr.e_shnum; i++) {

  if (processed_sec[i])
   continue;

  if (get_sec(elf, i, &ehdr, &shname, &shdr, &data))
   continue;
        /*判断定义的程序类型并加载，可以看到判断的时候，给出了判断的字符串的大小，也就是编程时，
        我们可以定义Section("xdp1")、Section("xdp2")、Section("socket1")...去区别相同程序类型的不同Section
        */
  if (memcmp(shname, "kprobe/", 7) == 0 ||
      memcmp(shname, "kretprobe/", 10) == 0 ||
      memcmp(shname, "tracepoint/", 11) == 0 ||
      memcmp(shname, "xdp", 3) == 0 ||
      memcmp(shname, "perf_event", 10) == 0 ||
      memcmp(shname, "socket", 6) == 0 ||
      memcmp(shname, "cgroup/", 7) == 0 ||
      memcmp(shname, "sockops", 7) == 0 ||
      memcmp(shname, "sk_skb", 6) == 0) {
   ret = load_and_attach(shname, data->d_buf,
           data->d_size);
   if (ret != 0)
    goto done;
  }
 }
......
```

上面的do_load_bpf_file函数判断完，程序类型后调用了load_and_attach函数：

```text
static int load_and_attach(const char *event, struct bpf_insn *prog, int size)
{   /*
     根据判断的程序类型，给以下变量赋值0、1
    */
 bool is_socket = strncmp(event, "socket", 6) == 0;
 bool is_kprobe = strncmp(event, "kprobe/", 7) == 0;
 bool is_kretprobe = strncmp(event, "kretprobe/", 10) == 0;
 bool is_tracepoint = strncmp(event, "tracepoint/", 11) == 0;
 bool is_xdp = strncmp(event, "xdp", 3) == 0;
 bool is_perf_event = strncmp(event, "perf_event", 10) == 0;
 bool is_cgroup_skb = strncmp(event, "cgroup/skb", 10) == 0;
 bool is_cgroup_sk = strncmp(event, "cgroup/sock", 11) == 0;
 bool is_sockops = strncmp(event, "sockops", 7) == 0;
 bool is_sk_skb = strncmp(event, "sk_skb", 6) == 0;
 size_t insns_cnt = size / sizeof(struct bpf_insn);
 enum bpf_prog_type prog_type;
 char buf[256];
 int fd, efd, err, id;
 struct perf_event_attr attr = {};

 attr.type = PERF_TYPE_TRACEPOINT;
 attr.sample_type = PERF_SAMPLE_RAW;
 attr.sample_period = 1;
 attr.wakeup_events = 1;
    /*
    开始创建SEC头和实际prog_type之间的关联
    */
     if (is_socket) {
  prog_type = BPF_PROG_TYPE_SOCKET_FILTER;
 } else if (is_kprobe || is_kretprobe) {
  prog_type = BPF_PROG_TYPE_KPROBE;
 } else if (is_tracepoint) {
  prog_type = BPF_PROG_TYPE_TRACEPOINT;
 } else if (is_xdp) {
  prog_type = BPF_PROG_TYPE_XDP;
 } else if (is_perf_event) {
  prog_type = BPF_PROG_TYPE_PERF_EVENT;
 } else if (is_cgroup_skb) {
  prog_type = BPF_PROG_TYPE_CGROUP_SKB;
 } else if (is_cgroup_sk) {
  prog_type = BPF_PROG_TYPE_CGROUP_SOCK;
 } else if (is_sockops) {
  prog_type = BPF_PROG_TYPE_SOCK_OPS;
 } else if (is_sk_skb) {
  prog_type = BPF_PROG_TYPE_SK_SKB;
 } else {
  printf("Unknown event '%s'\n", event);
  return -1;
 }
    fd = bpf_load_program(prog_type, prog, insns_cnt, license, kern_version,
         bpf_log_buf, BPF_LOG_BUF_SIZE);
    ......
}
```

通过分析上面的do_load_bpf_file函数，大致就明白了在写BPF程序时定义的Section如何进行判断然后进一步加载的了。

## 4.BPF程序类型

BPF 相关的程序，首先需要设置为相对应的的程序类型，截止Linux 内核 5.8 程序类型定义有 29 个，而且还是持续增加中，BPF 程序类型（prog_type）决定了程序可以调用的内核辅助函数的子集。BPF 程序类型也决定了程序输入上下文 -- bpf_context 结构的格式，其作为 BPF 程序中的第一个输入参数（数据blog），每类程序都有自己独特的上下文参数。例如，跟踪程序与套接字过滤器程序的辅助函数子集并不完全相同（尽管它们可能具有一些共同的帮助函数）。同样，跟踪程序的输入上下文（context）是一组寄存器值，如套接字过滤器的输入是一个网络数据包。

特定类型的eBPF程序可用的功能集将来可能会增加。

include/uapi/linux/bpf.h

```text
 161 enum bpf_prog_type {
 162         BPF_PROG_TYPE_UNSPEC,
 163         BPF_PROG_TYPE_SOCKET_FILTER,
 164         BPF_PROG_TYPE_KPROBE,
 165         BPF_PROG_TYPE_SCHED_CLS,
 166         BPF_PROG_TYPE_SCHED_ACT,
 167         BPF_PROG_TYPE_TRACEPOINT,
 168         BPF_PROG_TYPE_XDP,
 169         BPF_PROG_TYPE_PERF_EVENT,
 170         BPF_PROG_TYPE_CGROUP_SKB,
 171         BPF_PROG_TYPE_CGROUP_SOCK,
 172         BPF_PROG_TYPE_LWT_IN,
 173         BPF_PROG_TYPE_LWT_OUT,
 174         BPF_PROG_TYPE_LWT_XMIT,
 175         BPF_PROG_TYPE_SOCK_OPS,
 176         BPF_PROG_TYPE_SK_SKB,
 177         BPF_PROG_TYPE_CGROUP_DEVICE,
 178         BPF_PROG_TYPE_SK_MSG,
 179         BPF_PROG_TYPE_RAW_TRACEPOINT,
 180         BPF_PROG_TYPE_CGROUP_SOCK_ADDR,
 181         BPF_PROG_TYPE_LWT_SEG6LOCAL,
 182         BPF_PROG_TYPE_LIRC_MODE2,
 183         BPF_PROG_TYPE_SK_REUSEPORT,
 184         BPF_PROG_TYPE_FLOW_DISSECTOR,
 185         BPF_PROG_TYPE_CGROUP_SYSCTL,
 186         BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE,
 187         BPF_PROG_TYPE_CGROUP_SOCKOPT,
 188         BPF_PROG_TYPE_TRACING,
 189         BPF_PROG_TYPE_STRUCT_OPS,
 190         BPF_PROG_TYPE_EXT,
 191         BPF_PROG_TYPE_LSM,
 192 };
```

为了直观看出来这些程序类型，在bpftool源码中摘取了这些名字和类型对应的关系

```text
const char * const prog_type_name[] = {
        [BPF_PROG_TYPE_UNSPEC]                  = "unspec",
        [BPF_PROG_TYPE_SOCKET_FILTER]           = "socket_filter",
        [BPF_PROG_TYPE_KPROBE]                  = "kprobe",
        [BPF_PROG_TYPE_SCHED_CLS]               = "sched_cls",
        [BPF_PROG_TYPE_SCHED_ACT]               = "sched_act",
        [BPF_PROG_TYPE_TRACEPOINT]              = "tracepoint",
        [BPF_PROG_TYPE_XDP]                     = "xdp",
        [BPF_PROG_TYPE_PERF_EVENT]              = "perf_event",
        [BPF_PROG_TYPE_CGROUP_SKB]              = "cgroup_skb",
        [BPF_PROG_TYPE_CGROUP_SOCK]             = "cgroup_sock",
        [BPF_PROG_TYPE_LWT_IN]                  = "lwt_in",
        [BPF_PROG_TYPE_LWT_OUT]                 = "lwt_out",
        [BPF_PROG_TYPE_LWT_XMIT]                = "lwt_xmit",
        [BPF_PROG_TYPE_SOCK_OPS]                = "sock_ops",
        [BPF_PROG_TYPE_SK_SKB]                  = "sk_skb",
        [BPF_PROG_TYPE_CGROUP_DEVICE]           = "cgroup_device",
        [BPF_PROG_TYPE_SK_MSG]                  = "sk_msg",
        [BPF_PROG_TYPE_RAW_TRACEPOINT]          = "raw_tracepoint",
        [BPF_PROG_TYPE_CGROUP_SOCK_ADDR]        = "cgroup_sock_addr",
        [BPF_PROG_TYPE_LWT_SEG6LOCAL]           = "lwt_seg6local",
        [BPF_PROG_TYPE_LIRC_MODE2]              = "lirc_mode2",
        [BPF_PROG_TYPE_SK_REUSEPORT]            = "sk_reuseport",
        [BPF_PROG_TYPE_FLOW_DISSECTOR]          = "flow_dissector",
        [BPF_PROG_TYPE_CGROUP_SYSCTL]           = "cgroup_sysctl",
        [BPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE] = "raw_tracepoint_writable",
        [BPF_PROG_TYPE_CGROUP_SOCKOPT]          = "cgroup_sockopt",
        [BPF_PROG_TYPE_TRACING]                 = "tracing",
        [BPF_PROG_TYPE_STRUCT_OPS]              = "struct_ops",
        [BPF_PROG_TYPE_EXT]                     = "ext",
        [BPF_PROG_TYPE_LSM]                     = "lsm",
        [BPF_PROG_TYPE_SK_LOOKUP]               = "sk_lookup",
}
```

这些程序类型，我们可以认为是有两大类，其一是用来跟踪，编写程序来更好地了解系统正在发生什么，这类程序提供了系统行为以及系统硬件的直接信息，同时可以访问特定程序的内存区域，从运行进程中提取执行跟踪信息。其二是网络，这类程序可以监测和控制系统的网络流量，注意：“监测”、“控制”，“监测”也就是说我们可以观测，如套接字过滤器程序、“控制”可以去修改网络一些信息，如XDP程序做过滤、拒绝数据包、重定向数据包。

上面提到了每类BPF程序都有自己独特的上下文参数和自己触发的事件，下面将分析其中一种：BPF_PROG_TYPE_SOCKET_FILTER类型的上下文参数和触发的事件，其他的BPF程序类型将在今后的文章进行分析和讲解。

## 5.**套接字过滤器程序**

**BPF_PROG_TYPE_SOCKET_FILTER**

这类程序可以说是刚才提到的两种大类中：网络类，刚才提到这类程序可以监测和控制系统的网络流量，套接字过滤器程序就仅仅可以进行监测，不允许修改数据包内容或更改其目的地等。**每类程序都有自己的独特的上下文参数**，该类程序的上下文入口参数是`struct __sk_buff`，struct __sk_buff的定义在include/uapi/linux/bpf.h>，如下所示：

```text
struct __sk_buff {
 __u32 len;
 __u32 pkt_type;
 __u32 mark;
 __u32 queue_mapping;
 __u32 protocol;
 __u32 vlan_present;
 __u32 vlan_tci;
 __u32 vlan_proto;
 __u32 priority;
 __u32 ingress_ifindex;
 __u32 ifindex;
 __u32 tc_index;
 __u32 cb[5];
 __u32 hash;
 __u32 tc_classid;
 __u32 data;
 __u32 data_end;
 __u32 napi_id;

 /* Accessed by BPF_PROG_TYPE_sk_skb types from here to ... */
 __u32 family;
 __u32 remote_ip4; /* Stored in network byte order */
 __u32 local_ip4; /* Stored in network byte order */
 __u32 remote_ip6[4]; /* Stored in network byte order */
 __u32 local_ip6[4]; /* Stored in network byte order */
 __u32 remote_port; /* Stored in network byte order */
 __u32 local_port; /* stored in host byte order */
 /* ... here. */

 __u32 data_meta;
};
```

从上面的struct __sk_buff 可以看到，这个结构体来自于struct sk_buff中的一些关键字段。那么会有一个疑问，该结构体只有struct sk_buff的关键字段而且与struct sk_buff中的字段并没有做到对齐(位置对应)，是怎样做到访问到这些字段的？

> 参考：[https://lwn.net/Articles/636647/](https://link.zhihu.com/?target=https%3A//lwn.net/Articles/636647/)
> They were few other ideas considered. At the end the best seems to be to introduce a user accessible mirror of in-kernel sk_buff structure:
> struct __sk_buff { __u32 len; __u32 pkt_type; __u32 mark; __u32 ifindex; __u32 queue_mapping;
> .......
> };
> bpf programs will do:
> int bpf_prog1(struct __sk_buff *skb) { __u32 var = skb->pkt_type;
> which will be compiled to bpf assembler as:
> dst_reg = *(u32 *)(src_reg + 4) // 4 == offsetof(struct __sk_buff, pkt_type)
> bpf verifier will check validity of access and will convert it to:
> dst_reg = *(u8 *)(src_reg + offsetof(struct sk_buff, __pkt_type_offset)) dst_reg &= 7
> since 'pkt_type' is a bitfield.

上面这段参考，大概就是在说struct __sk_buff包含了这个一些sk_buff结构体的关键字段，内核的BPF程序将对这些关键字段的访问转换成“真实”sk_buff的偏移量，这一点很关键！！

该程序类型用于过滤进出口网络报文，功能上和 cBPF 类似，也就是和在tcpdump中所运用的cBPF相似,可以在tcpdump系统文章中进行查看相关的实现。

关于套接字过滤器程序的案例，还是选择sockex1:

sockex1_kern.c

```text
#include <uapi/linux/bpf.h>
#include <uapi/linux/if_ether.h>
#include <uapi/linux/if_packet.h>
#include <uapi/linux/ip.h>
#include "bpf_helpers.h"
/*
预先定义好的MAP对象，然而MAP的真正创建时机，是用户态程序调用load_bpf_file时
解析到section后进行创建；这里SEC宏会在obj文件中新增一个段(section)
这里定义的是一个数组型的MAP,索引是数组下标，值是数组对应的值
*/
struct bpf_map_def SEC("maps") my_map = {
 .type = BPF_MAP_TYPE_ARRAY,
 .key_size = sizeof(u32),
 .value_size = sizeof(long),
 .max_entries = 256,
};

SEC("socket1")
    //struct __sk_buff就是该程序类型的上下文参数，封装了struct sk_buff中的一些关键的成员变量
int bpf_prog1(struct __sk_buff *skb)
{   
    //load_byte实际指向了llvm的buit-in函数asm(llvm.bpf.load.byte)，该语句是读取输入报文的包头中的协议头
 int index = load_byte(skb, ETH_HLEN + offsetof(struct iphdr, protocol));
 long *value;
     //pkt_type表明了数据包的方向，PACKET_OUTGOING表示发往其他主机的报文和本地主机的环回报文
 if (skb->pkt_type != PACKET_OUTGOING)
  return 0;

 value = bpf_map_lookup_elem(&my_map, &index);
 if (value)
  __sync_fetch_and_add(value, skb->len);//统计流量包的大小

 return 0;
}
char _license[] SEC("license") = "GPL";
```

sockex1_user.c

```text
#include <stdio.h>
#include <assert.h>
#include <linux/bpf.h>
#include "libbpf.h"
#include "bpf_load.h"
#include "sock_example.h"
#include <unistd.h>
#include <arpa/inet.h>

int main(int ac, char **argv)
{
 char filename[256];
 FILE *f;
 int i, sock;

 snprintf(filename, sizeof(filename), "%s_kern.o", argv[0]);
    //加载sockex1_kern.c编译后的sockex1_kern.o，解析出定义的section,完成创建MAP，向内核注册插入BPF代码。
 if (load_bpf_file(filename)) {
  printf("%s", bpf_log_buf);
  return 1;
 }
     //主要是完成：sock = socket(PF_PACKET, SOCK_RAW | SOCK_NONBLOCK | SOCK_CLOEXEC, htons(ETH_P_ALL));
 sock = open_raw_sock("lo");
    /*因为 sockex1_kern.o 中 bpf 程序的类型为 BPF_PROG_TYPE_SOCKET_FILTER,所以这里需要用用 SO_ATTACH_BPF 来指明程序的 sk_filter 要挂载到哪一个套接字上，其中prof_fd为注入到内核的BPF程序的描述符*/
 assert(setsockopt(sock, SOL_SOCKET, SO_ATTACH_BPF, prog_fd,
     sizeof(prog_fd[0])) == 0);

 f = popen("ping -c5 localhost", "r");
 (void) f;
    //循环5次查询相应协议对应的数据包大小
 for (i = 0; i < 5; i++) {
  long long tcp_cnt, udp_cnt, icmp_cnt;
  int key;
  key = IPPROTO_TCP;
  assert(bpf_map_lookup_elem(map_fd[0], &key, &tcp_cnt) == 0);

  key = IPPROTO_UDP;
  assert(bpf_map_lookup_elem(map_fd[0], &key, &udp_cnt) == 0);

  key = IPPROTO_ICMP;
  assert(bpf_map_lookup_elem(map_fd[0], &key, &icmp_cnt) == 0);

  printf("TCP %lld UDP %lld ICMP %lld bytes\n",
         tcp_cnt, udp_cnt, icmp_cnt);
  sleep(1);
 }

 return 0;
}
```

sockex1_user.c中的：

```text
 if (load_bpf_file(filename)) {
  printf("%s", bpf_log_buf);
  return 1;
 }
```

用户空间通过辅助函数load_bpf_file加载BPF的elf文件后，将返回使用MAP两个全局变量:map_fd数组,map_data,同时也会创建 struct bpf_prog存储指令，struct bpf_prog内容如下所示,每一个load到内核的BPF程序都有一个fd(prog_fd)返回给用户态，它对应一个bpf_prog。将指令Load内核，创建struct bpf_prog存储指令外，只是第一步，成功运行还需要：

- 将bpf_prog与内核中的特定Hook点关联起来，也就是将程序挂到钩子上
- 在Hook点被访问到，取出bpf_prog，执行这些指令。

**那socket类型的BPF程序，也就是BPF_PROG_TYPE_SOCKET_FILTER类型的程序什么时候会执行注入到内核的指令？**

其实正如tcpdump原理一样，可以参考tcpdump1和tcpdump2,当绑定的指定的网卡在发包或者收包时会进行嗅探并执行注入的指令。

```text
    struct bpf_prog {
 u16   pages;  // Number of allocated pages 
 u16   jited:1, // Is our filter JIT'ed? 
    locked:1, // Program image locked? 
    gpl_compatible:1, // Is filter GPL compatible? 
    cb_access:1, // Is control block accessed? 
    dst_needed:1; // Do we need dst entry? 
 enum bpf_prog_type type;  // Type of BPF program 
 u32   len;  // Number of filter blocks 
 u32   jited_len; // Size of jited insns in bytes 
 u8   tag[BPF_TAG_SIZE];
 struct bpf_prog_aux *aux;  // Auxiliary fields 
 struct sock_fprog_kern *orig_prog; // Original BPF program 
 unsigned int  (*bpf_func)(const void *ctx,
         const struct bpf_insn *insn);
 // Instructions for interpreter 
 union {
  struct sock_filter insns[0];   //对应cBPF的指令集
  struct bpf_insn  insnsi[0];  //对应eBPF的指令集
 };
};
```

BPF_PROG_TYPE_SOCKET_FILTER用来干啥？

> filter操作包括丢弃数据包（如果程序返回0）或修改数据包（如果程序返回的长度小于原始长度）。请参见sk_filter_trim_cap（）及其对bpf_prog_run_save_cb（）的调用。请注意，我们不是修改或者丢弃原始的数据包，这些数据包都会到达原始的套接字。我们处理的是原始数据包元数据的copy，这些copy的数据包可以被raw socket访问到。除此之外，我们还可以做一些其他的事情。例如进行流量统计在BPF 的Maps中。

如何attach我们的BPF program?

> 它可以通过SO_ATTACH_BPF setsockopt()进行attach。

该类型上下文参数?

> struct __sk_buff 包含这个packet的metadata/data。他的结构在include/linux/ bpf.h中定义，并包含来自实际sk_buff的关键字段。bpf验证程序将对有效的__sk_buff字段的访问转换为“真实”sk_buff的偏移量。

它什么时候被执行？

> 绑定到指定的网卡后，在发包或者收包时会进行嗅探并执行注入的程序，该类型程序被各种协议（TCP，UDP，ICMP，原始套接字等）调用，可用于过滤inbound的流量。

原文地址：https://zhuanlan.zhihu.com/p/502555210

作者：linux

# 【NO.589】论fork()函数与Linux中的多线程编程

## 1.fork()函数

在操作系统的基本概念中进程是程序的一次执行，且是拥有资源的最小单位和调度单位（在引入线程的操作系统中，线程是最小的调度单位）。在Linux系统中创建进程有两种方式：一是由操作系统创建，二是由父进程创建进程（通常为子进程）。系统调用函数fork()是创建一个新进程的唯一方式，当然vfork()也可以创建进程，但是实际上其还是调用了fork()函数。fork()函数是Linux系统中一个比较特殊的函数，其一次调用会有两个返回值，下面是fork()函数的声明：

```text
#include <unistd.h>
// On success, The PID of the process is returned in the parent, and 0 is returned in the child. On failure,
// -1 is returned in the parent, no child process is created, and errno is set appropriately.
pid_t fork (void);
```

当程序调用fork()函数并返回成功之后，程序就将变成两个进程，调用fork()者为父进程，后来生成者为子进程。这两个进程将执行相同的程序文本，但却各自拥有不同的栈段、数据段以及堆栈拷贝。子进程的栈、数据以及栈段开始时是父进程内存相应各部分的完全拷贝，因此它们互不影响。从性能方面考虑，父进程到子进程的数据拷贝并不是创建时就拷贝了的，而是采用了写时拷贝（copy-on -write）技术来处理。调用fork()之后，父进程与子进程的执行顺序是我们无法确定的（即调度进程使用CPU），意识到这一点极为重要，因为在一些设计不好的程序中会导致资源竞争，从而出现不可预知的问题。下图为写时拷贝技术处理前后的示意图：



![img](https://pic1.zhimg.com/80/v2-930fe18a90ecb5342b18682f437fe1f0_720w.webp)



在Linux系统中，常常存在许多对文件的操作，fork()的执行将会对文件操作带来一些小麻烦。由于子进程会将父进程的大多数数据拷贝一份，这样在文件操作中就意味着子进程会获得父进程所有文件描述符的副本，这些副本的创建方式类似于dup()函数调用，因此父、子进程中对应的文件描述符均指向相同的打开的文件句柄，而且打开的文件句柄包含着当前文件的偏移量以及文件状态标志，所以在父子进程中处理文件时要考虑这种情况，以避免文件内容出现混乱或者别的问题。下图为执行fork()调用后文件描述符的相关处理及其变化：



![img](https://pic2.zhimg.com/80/v2-e50bd8d7e0f089ea36c4ad00324b0499_720w.webp)



## 2.线程

与进程类似，线程（thread）是允许应用程序并发执行多个任务的一种机制。一个进程中可以包含多个线程，同一个程序中的所有线程均会独立执行，且共享同一份全局内存区域，其中包括初始化数据段（initialized data），未初始化数据段（uninitialized data），以及堆内存段（heap segment）。在多处理器环境下，多个线程可以同时执行，如果线程数超过了CPU的个数，那么每个线程的执行顺序将是无法确定的，因此对于一些全局共享数据据需要使用同步机制来确保其的正确性。

在系统中，线程也是稀缺资源，一个进程能同时创建多少个线程这取决于地址空间的大小和内核参数，一台机器可以同时并发运行多少个线程也受限于CPU的数目。在进行程序设计时，我们应该精心规划线程的个数，特别是根据机器CPU的数目来设置工作线程的数目，并为关键任务保留足够的计算资源。如果你设计的程序在背地里启动了额外的线程来执行任务，那这也属于资源规划漏算的情况，从而影响关键任务的执行，最终导致无法达到预期的性能。很多程序中都存在全局对象，这些全局对象的初始化工作都是在进入main()函数之前进行的，为了能保证全局对象的安全初始化（按顺序的），因此在程序进入main()函数之前应该避免线程的创建，从而杜绝未知错误的发生。

## 3.fork()与多线程

在程序中fork()与多线程的协作性很差，这是POSIX系列操作系统的历史包袱。因为长期以来程序都是单线程的，fork()运转正常。当20世纪90年代初期引入线程之后，fork()的适用范围就大为缩小了。

在多线程执行的情况下调用fork()函数，仅会将发起调用的线程复制到子进程中。（子进程中该线程的ID与父进程中发起fork()调用的线程ID是一样的，因此，线程ID相同的情况有时我们需要做特殊的处理。）也就是说不能同时创建出于父进程一样多线程的子进程。其他线程均在子进程中立即停止并消失，并且不会为这些线程调用清理函数以及针对线程局部存储变量的析构函数。这将导致下列一些问题:

\1. 虽然只将发起fork()调用的线程复制到子进程中，但全局变量的状态以及所有的pthreads对象（如互斥量、条件变量等）都会在子进程中得以保留，这就造成一个危险的局面。例如：一个线程在fork()被调用前锁定了某个互斥量，且对某个全局变量的更新也做到了一半，此时fork()被调用，所有数据及状态被拷贝到子进程中，那么子进程中对该互斥量就无法解锁（因为其并非该互斥量的属主），如果再试图锁定该互斥量就会导致死锁，这是多线程编程中最不愿意看到的情况。同时，全局变量的状态也可能处于不一致的状态，因为对其更新的操作只做到了一半对应的线程就消失了。fork()函数被调用之后，子进程就相当于处于signal handler之中，此时就不能调用线程安全的函数（用锁机制实现安全的函数），除非函数是可重入的，而只能调用异步信号安全（async-signal-safe）的函数。fork()之后，子进程不能调用：

- malloc(3)。因为malloc()在访问全局状态时会加锁。
- 任何可能分配或释放内存的函数，包括new、map::insert()、snprintf() ……
- 任何pthreads函数。你不能用pthread_cond_signal()去通知父进程，只能通过读写pipe(2)来同步。
- printf()系列函数，因为其他线程可能恰好持有stdout/stderr的锁。
- 除了man 7 signal中明确列出的“signal安全”函数之外的任何函数。

\2. 因为并未执行清理函数和针对线程局部存储数据的析构函数，所以多线程情况下可能会导致子进程的内存泄露。另外，子进程中的线程可能无法访问（父进程中）由其他线程所创建的线程局部存储变量，因为（子进程）没有任何相应的引用指针。

由于这些问题，推荐在多线程程序中调用fork()的唯一情况是：其后立即调用exec()函数执行另一个程序，彻底隔断子进程与父进程的关系。由新的进程覆盖掉原有的内存，使得子进程中的所有pthreads对象消失。

对于那些必须执行fork()，而其后又无exec()紧随其后的程序来说，pthreads API提供了一种机制：fork()处理函数。利用函数pthread_atfork()来创建fork()处理函数。pthread_atfork()声明如下：

```text
#include <pthread.h>
// Upon successful completion, pthread_atfork() shall return a value of zero; otherwise, an error number shall be returned to indicate the error.
// @prepare 新进程产生之前被调用
// @parent  新进程产生之后在父进程被调用
// @child    新进程产生之后，在子进程被调用
int pthread_atfork (void (*prepare) (void), void (*parent) (void), void (*child) (void));
```

该函数的作用就是往进程中注册三个函数，以便在不同的阶段调用，有了这三个参数，我们就可以在对应的函数中加入对应的处理功能。同时需要注意的是，每次调用pthread_atfork()函数会将prepare添加到一个函数列表中，创建子进程之前会（按与注册次序相反的顺序）自动执行该函数列表中函数。parent与child也会被添加到一个函数列表中，在fork()返回前，分别在父子进程中自动执行（按注册的顺序）。

## 4.总结

fork()函数的调用会导致在子进程中除调用线程外的其它线程全都终止执行并消失，因此在多线程的情况下会导致死锁和内存泄露的情况。在进行多线程编程的时候尽量避免fork()的调用，同时在程序在进入main函数之前应避免创建线程，因为这会影响到全局对象的安全初始化。线程不应该被强行终止，因为这样它就没有机会调用清理函数来做相应的操作，同时也就没有机会来释放已被锁住的锁，如果另一线程对未被解锁的锁进行加锁，那么将会立即发生死锁，从而导致程序无法正常运行。

原文地址：https://zhuanlan.zhihu.com/p/608495890

作者：linux

# 【NO.590】Linux 直接I/O 原理与实现

## 1.缓存I/O

一般来说，当调用 open() 系统调用打开文件时，如果不指定 O_DIRECT 标志，那么就是使用缓存I/O来对文件进行读写操作。我们先来看看 open() 系统调用的定义：

```text
int open(const char *pathname, int flags, ... /*, mode_t mode */ );
```

下面说明一下各个参数的作用：

- pathname：指定要打开的文件路径。
- flags：指定打开文件的标志。
- mode：可选，指定打开文件的权限。

其中 flags 参数可选值如下表：

![img](https://pic4.zhimg.com/80/v2-84831861ccbc5e478c4823b7d4c3cc93_720w.webp)

flags 参数用于指定打开文件的标志，比如指定 O_RDONLY，那么就只能以只读方式对文件进行读写。这些标志都能通过 位或 (|) 操作来设置多个标志如：

```text
open("/path/to/file", O_RDONLY|O_APPEND|O_DIRECT);
```

但 O_RDONLY、O_WRONLY 和 O_RDWR 这三个标志是互斥的，也就是说这三个标志不能同时设置，只能设置其中一个。

当打开文件不指定 O_DIRECT 标志时，那么就默认使用 缓存I/O 方式打开。我们可以通过下图来了解 缓存I/O 处于文件系统的什么位置：

![img](https://pic2.zhimg.com/80/v2-15116640faca1d348f43af39a8704ab9_720w.webp)


上图中红色框部分就是 缓存I/O 所在位置，位于 虚拟文件系统 与 真实文件系统 中间。

也就是说，当虚拟文件系统读文件时，首先从缓存中查找要读取的文件内容是否存在缓存中，如果存在就直接从缓存中读取。对文件进行写操作时也一样，首先写入到缓存中，然后由操作系统同步到块设备（如磁盘）中。

## 2.缓存I/O 的优缺点

缓存I/O 的引入是为了减少对块设备的 I/O 操作，但是由于读写操作都先要经过缓存，然后再从缓存复制到用户空间，所以多了一次内存复制操作。如下图所示：

![img](https://pic1.zhimg.com/80/v2-fb194c0bfd30d86b3e15156460ba345c_720w.webp)

所以 缓存I/O 的优点是减少对块设备的 I/O 操作，而缺点就是需要多一次的内存复制。另外，有些应用程序需要自己管理 I/O 缓存的（如数据库系统），那么就需要使用 直接I/O 了。

## 3.直接I/O

直接I/O 就是对用户进行的 I/O 操作直接与块设备进行交互，而不进行缓存。

直接I/O 的优点是：由于不对 I/O 数据块进行缓存，所以可以直接跟用户数据进行交互，减少一次内存的拷贝。
直接I/O 的缺点是：每次 I/O 操作都直接与块设备进行交互，增加了对块设备的读写操作。
但由于应用程序可以自行对数据块进行缓存，所以更加灵活，适合一些对 I/O 操作比较敏感的应用，如数据库系统。

## 4.直接I/O 实现

当调用 open() 系统调用时，在 flags 参数指定 O_DIRECT 标志即可使用 直接I/O。我们从 虚拟文件系统 开始跟踪 Linux 对 直接I/O 的处理过程。

当调用 open() 系统调用时，会触发调用 sys_open() 系统调用，我们先来看看 sys_open() 函数的实现：

```text
asmlinkage long sys_open(const char *filename, int flags, int mode)
{
    char *tmp;
    int fd, error;
    ...
    tmp = getname(filename); // 把文件名从用户空间拷贝到内核空间
    fd = PTR_ERR(tmp);
    if (!IS_ERR(tmp)) {
        fd = get_unused_fd(); // 申请一个还没有使用的文件描述符
        if (fd >= 0) {
            // 根据文件路径打开文件, 并获取文件对象
            struct file *f = filp_open(tmp, flags, mode);
            error = PTR_ERR(f);
            if (IS_ERR(f))
                goto out_error;
            fd_install(fd, f); // 把文件对象与文件描述符关联起来
        }
out:
        putname(tmp);
    }
    return fd;
    ...
}
```

打开文件的整个流程比较复杂，但对我们分析 直接I/O 并没有太大关系.

我们主要关注的是，sys_open() 函数最后会调用 dentry_open() 把 flags 参数保存到文件对象的 f_flags 字段中，调用链：sys_open() -> filp_open() -> dentry_open()：

```text
struct file *dentry_open(struct dentry *dentry, struct vfsmount *mnt, int flags)
{
    struct file *f;
    ...
    f = get_empty_filp();
    f->f_flags = flags;
    ...
}
```

也就是说，sys_open() 函数会打开文件，然后把 flags 参数保存到文件对象的 f_flgas 字段中。接下来，我们分析一下读文件操作时，是怎么对 直接I/O 进行处理的。读文件操作使用 read() 系统调用，而 read() 最终会调用内核的 sys_read() 函数，代码如下：

```text
asmlinkage ssize_t sys_read(unsigned int fd, char *buf, size_t count)
{
    ssize_t ret;
    struct file *file;

    file = fget(fd);
    if (file) {
        ...
            if (!ret) {
                ssize_t (*read)(struct file *, char *, size_t, loff_t *);
                ret = -EINVAL;
                // ext2文件系统对应的是: generic_file_read() 函数
                if (file->f_op && (read = file->f_op->read) != NULL)
                    ret = read(file, buf, count, &file->f_pos);
            }
        ...
    }
    return ret;
}
```

由于 sys_read() 函数属于虚拟文件系统范畴，所以其最终会调用真实文件系统的 file->f_op->read() 函数，ext2文件系统 对应的是 generic_file_read() 函数，我们来分析下 generic_file_read() 函数：

```text
ssize_t generic_file_read(struct file *filp, char * buf, size_t count, loff_t *ppos)
{
    ssize_t retval;
    ...
    if (filp->f_flags & O_DIRECT) // 如果标记了使用直接IO
        goto o_direct;
    ...
 o_direct:
    {
        loff_t pos = *ppos, size;
        struct address_space *mapping = filp->f_dentry->d_inode->i_mapping;
        struct inode *inode = mapping->host;
        ...
        size = inode->i_size;
        if (pos < size) {
            if (pos + count > size)
                count = size - pos;
            retval = generic_file_direct_IO(READ, filp, buf, count, pos);
            if (retval > 0)
                *ppos = pos + retval;
        }
        UPDATE_ATIME(filp->f_dentry->d_inode);
        goto out;
    }
}
```

从上面代码可以看出，如果在调用 open() 时指定了 O_DIRECT 标志，那么 generic_file_read() 函数就会调用 generic_file_direct_IO() 函数对 I/O 操作进行处理。由于 generic_file_direct_IO() 函数的实现曲折迂回，所以下面主要分析重要部分：

```text
static ssize_t generic_file_direct_IO(int rw, struct file *filp, char *buf, size_t count, loff_t offset)
{
    ...
    while (count > 0) {
        iosize = count;
        if (iosize > chunk_size)
            iosize = chunk_size;

        // 为用户虚拟内存空间申请物理内存页
        retval = map_user_kiobuf(rw, iobuf, (unsigned long)buf, iosize);
        if (retval)
            break;

        // ext2 文件系统对应 ext2_direct_IO() 函数,
        // 而 ext2_direct_IO() 函数直接调用了 generic_direct_IO() 函数
        retval = mapping->a_ops->direct_IO(rw, inode, iobuf, (offset+progress) >> blocksize_bits, blocksize);
        ...
    }
    ...
}
```

generic_file_direct_IO() 函数主要的处理有两部分：

调用 map_user_kiobuf() 函数为用户虚拟内存空间申请物理内存页。
调用真实文件系统的 direct_IO() 接口对 直接I/O 进行处理。
map_user_kiobuf() 函数属于内存管理部分，这里就不做解释了。

generic_file_direct_IO() 函数最终会调用真实文件系统的 direct_IO() 接口，对于 ext2文件系统，direct_IO() 接口对应的是 ext2_direct_IO() 函数，而 ext2_direct_IO() 函数只是简单的封装了 generic_direct_IO() 函数，所以我们来分析下 generic_direct_IO() 函数的实现：

```text
int generic_direct_IO(int rw, struct inode *inode, struct kiobuf *iobuf,
        unsigned long blocknr, int blocksize, get_block_t *get_block)
{
    int i, nr_blocks, retval;
    unsigned long *blocks = iobuf->blocks;

    nr_blocks = iobuf->length / blocksize;
    // 获取要读取的数据块号列表
    for (i = 0; i < nr_blocks; i++, blocknr++) {
        struct buffer_head bh;

        bh.b_state = 0;
        bh.b_dev = inode->i_dev;
        bh.b_size = blocksize;

        retval = get_block(inode, blocknr, &bh, rw == READ ? 0 : 1);
        ...
        blocks[i] = bh.b_blocknr;
    }

    // 开始进行I/O操作
    retval = brw_kiovec(rw, 1, &iobuf, inode->i_dev, iobuf->blocks, blocksize);

 out:
    return retval;
}
```

generic_direct_IO() 函数的逻辑也比较简单，首先调用 get_block() 获取要读取的数据块号列表，然后调用 brw_kiovec() 函数进行 I/O 操作。所以 brw_kiovec() 函数才是 I/O 操作的最终触发点。我们继续分析：

```text
int brw_kiovec(int rw, int nr, struct kiobuf *iovec[],
           kdev_t dev, unsigned long b[], int size)
{
    ...
    for (i = 0; i < nr; i++) {
        ...
        for (pageind = 0; pageind < iobuf->nr_pages; pageind++) {
            map  = iobuf->maplist[pageind];
            ...
            while (length > 0) {
                blocknr = b[bufind++];
                ...
                tmp = bhs[bhind++];

                tmp->b_size = size;
                set_bh_page(tmp, map, offset); // 设置保存I/O操作后的数据的内存地址 (用户空间的内存)
                tmp->b_this_page = tmp;

                init_buffer(tmp, end_buffer_io_kiobuf, iobuf); // 设置完成I/O后的收尾工作回调函数为: end_buffer_io_kiobuf()
                tmp->b_dev = dev;
                tmp->b_blocknr = blocknr;
                tmp->b_state = (1 << BH_Mapped) | (1 << BH_Lock) | (1 << BH_Req);
                ...
                submit_bh(rw, tmp); // 提交 I/O 操作 (通用块I/O层)

                if (bhind >= KIO_MAX_SECTORS) {
                    kiobuf_wait_for_io(iobuf);
                    err = wait_kio(rw, bhind, bhs, size);
                    ...
                }

            skip_block:
                length -= size;
                offset += size;

                if (offset >= PAGE_SIZE) {
                    offset = 0;
                    break;
                }
            } /* End of block loop */
        } /* End of page loop */
    } /* End of iovec loop */
    ...
    return err;
}
```

brw_kiovec() 函数主要完成 3 个工作：

- 设置用于保存 I/O 操作后的数据的内存地址 (用户申请的内存)。
- 设置 I/O 操作完成后的收尾回调函数为: end_buffer_io_kiobuf()。
- 提交 I/O 操作到通用块层。

可以看出，对于 I/O 操作后的数据会直接保存到用户空间的内存，而没有通过内核缓存作为中转，从而达到 直接I/O 的目的。

原文地址：https://zhuanlan.zhihu.com/p/330515575

作者：linux

# 【NO.591】深入了解epoll模型（特别详细）

上网一搜epoll，基本是这样的结果出来：《多路转接I/O – epoll模型》，万变不离这个标题。

但是呢，不变的事物，我们就更应该抓出其中的重点了。

多路、转接、I/O、模型。

别急，先记住这几个词，我比较喜欢你们看我文章的时候带着问题。

## 1.什么是epoll？或者说，它和select有什么判别？

### 1.1 什么是select

有的朋友可能对select也不是很了解啊，我这里稍微科普一下：网络连接，服务器也是通过文件描述符来管理这些连接上来的客户端，既然是供连接的服务器，那就免不了要接收来自客户端的消息。那么多台客户端，消息那么的多，要是漏了一条两条重要消息，那也不要用TCP了，那怎么办？

前辈们就是有办法，轮询，轮询每个客户端文件描述符，查看他们是否带着消息，如果带着，那就处理一下；如果没带着，那就一边等着去。这就是select，轮询，颇有点领导下基层的那种感觉哈。

但是这个select的轮询呐，会有个问题，明眼人一下就能想到，那即是耗费资源啊，耗费什么资源，时间呐，慢呐（其实也挺快了，不过相对epoll来说就是慢）。

再认真想一下，还浪费什么资源，系统资源。有的客户端呐，占着那啥玩意儿不干那啥事儿，这种客户端呐，还不少。这也怪不得人家，哪儿有客户端时时刻刻在发消息，要是有，那就要小心是不是恶意攻击了。那把这么一堆偶尔动一下的客户端的文件描述符一直攥手里，累不累？能一次攥多少个？就像一个老板，一直想着下去巡视，那他可以去当车间组长了哈哈哈。

所以，select的默认上限一般是1024（FD_SETSIZE），当然我们可以手动去改，但是人家给个1024自然有人家的道理，改太大的话系统在这一块的负载就大了。

那句话怎么说的来着，你每次对系统的索取，其实都早已明码标价！哈哈哈。。。

所以，我们选用epoll模型。

讲到这里啊，如果对 select 不了解的朋友可以打开百度搜一下了，因为我们接下来要讲的内容需要你对 select 有点了解的。

### 1.2 为什么select最大只允许1024？

以前从没想过select和poll，一出来关注的就是epoll，后来明白了，对于select，也是有很多细节在里面的。源码之前，了无秘密！！！

```text
#define FD_SETSIZE 1024
#define NFDBITS (8 * sizeof(unsigned long))
#define __FDSET_LONGS (FD_SETSIZE/NFDBITS)

typedef struct {
  unsigned long fds_bits[__FDSET_LONGS];
} fd_set;
```

为什么源码要写成1024？

我觉得是历史遗留原因吧，不信你看一下你服务器在不改动的情况下能允许的最大文件描述符数。

如果有人再问：那 select 就只能监听1024个？那我就要说道说道了，谁说有1024？select 性能就这样了，再往上开意义不大，真要开，编源码去嘛。

### 1.3 什么是epoll

epoll接口是为解决Linux内核处理大量文件描述符而提出的方案。该接口属于Linux下多路I/O复用接口中select/poll的增强。其经常应用于Linux下高并发服务型程序，特别是在大量并发连接中只有少部分连接处于活跃下的情况 (通常是这种情况)，在该情况下能显著的提高程序的CPU利用率。

前面说，select就像亲自下基层视察的老板，那么epoll这个老板就要显得精明的多了。他可不亲自下基层，他找了个美女秘书，他只要盯着他的秘书看就行了，呸，他只需要听取他的秘书的汇报就行了。汇报啥呢？基层有任何消息，跟秘书说，秘书汇总之后一次性交给老板来处理。这样老板的时间不就大大的提高了嘛。

**epoll设计思路简介**

（1）epoll在Linux内核中构建了一个文件系统，该文件系统采用红黑树来构建，红黑树在增加和删除上面的效率极高，因此是epoll高效的原因之一。有兴趣可以百度红黑树了解，但在这里你只需知道其算法效率超高即可。

（2）epoll提供了两种触发模式，水平触发(LT)和边沿触发(ET)。当然，涉及到I/O操作也必然会有阻塞和非阻塞两种方案。目前效率相对较高的是 epoll+ET+非阻塞I/O 模型，在具体情况下应该合理选用当前情形中最优的搭配方案。

（3）epoll所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于1024,举个例子，在1GB内存的机器上大约是10万左右，具体数目可以下面语句查看，一般来说这个数目和系统内存关系很大。

```text
系统最大打开文件描述符数
cat /proc/sys/fs/file-max
```



```text
进程最大打开文件描述符数
ulimit -n
```

修改这个配置：

```text
sudo vi /etc/security/limits.conf
写入以下配置,soft软限制，hard硬限制

*                soft    nofile          65536
*                hard    nofile          100000
```

### 1.4 select、epoll原理图

![img](https://pic3.zhimg.com/80/v2-1b04c79ace11aa275bb76ca0f6d16eba_720w.webp)

缺点：

> （1）单进程可以打开fd有限制；
> （2）对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低；
> （3）用户空间和内核空间的复制非常消耗资源；

poll与select不同的地方：采用链表的方式替换原有fd_set数据结构,而使其没有连接数的限制。

![img](https://pic2.zhimg.com/80/v2-7ac8b45ead05cd4a73f11fe9d27ab4f9_720w.webp)

epoll支持的最大链接数是进程最大可打开的文件的数目。

**epoll一定优于select吗？什么时候不是？**

尽管如此，epoll 的性能并不必然比 select 高，对于 fd 数量较少并且 fd IO 都非常繁忙的情况 select 在性能上反而有优势。

## 2.epoll的工作流程？

**epol_create**

在epoll文件系统建立了个file节点，并开辟epoll自己的内核高速cache区，建立红黑树，建立一个双向链表，用于存储准备就绪的事件。

epoll_create传入一个size参数，size参数只要>0即可，没有任何意义。epoll_create调用函数sys_epoll_create1实现eventpoll的初始化。sys_epoll_create1通过ep_alloc生成一个eventpoll对象，并初始化eventpoll的三个等待队列，wait，poll_wait以及rdlist （ready的fd list）。同时还会初始化被监视fs的rbtree 根节点。

1、首先创建一个struct eventpoll对象；

2、然后分配一个未使用的文件描述符；

3、然后创建一个struct file对象，将file中的struct file_operations *f_op设置为全局变量eventpoll_fops，将void *private指向刚创建的eventpoll对象ep；

4、然后设置eventpoll中的file指针；

5、最后将文件描述符添加到当前进程的文件描述符表中，并返回给用户

![img](https://pic1.zhimg.com/80/v2-f47a8e75f33e160b4e48a77668ae8604_720w.webp)

**epoll_create返回的fd是什么？**

这个模块在内核初始化时（操作系统启动）注册了一个新的文件系统，叫"eventpollfs"（在eventpoll_fs_type结构里），然后挂载此文件系统。另外还创建两个内核cache（在内核编程中，如果需要频繁分配小块内存，应该创建kmem_cahe来做“内存池”）,分别用于存放struct epitem和eppoll_entry。这个内核高速cache区，就是建立连续的物理内存页，就是物理上分配好你想要的size的内存对象，每次使用时都是使用空闲的已分配好的内存。

现在想想epoll_create为什么会返回一个新的fd？

因为它就是在这个叫做"eventpollfs"的文件系统里创建了一个新文件！返回的就是这个文件的fd索引。完美地遵行了Linux一切皆文件的特色。

**epoll_ctl**

1、epoll_ctl()首先判断op是不是删除操作，如果不是则将event参数从用户空间拷贝到内核中。

2、接下来判断用户是否设置了EPOLLEXCLUSIVE标志，这个标志是4.5版本内核才有的，主要是为了解决同一个文件描述符同时被添加到多个epoll实例中造成的“惊群”问题，详细描述可以看这里。 这个标志的设置有一些限制条件，比如只能是在EPOLL_CTL_ADD操作中设置，而且对应的文件描述符本身不能是一个epoll实例。

3、接下来从传入的文件描述符开始，一步步获得struct file对象，再从struct file中的private_data字段获得struct eventpoll对象。（那这个头那个头的都在这里面了）

4、如果要添加的文件描述符本身也代表一个epoll实例，那么有可能会造成死循环，内核对此情况做了检查，如果存在死循环则返回错误。

5、接下来会从epoll实例的红黑树里寻找和被监控文件对应的epollitem对象，如果不存在，也就是之前没有添加过该文件，返回的会是NULL。

6、ep_find()函数本质是一个红黑树查找过程，红黑树查找和插入使用的比较函数是ep_cmp_ffd()，先比较struct file对象的地址大小，相同的话再比较文件描述符大小。struct file对象地址相同的一种情况是通过dup()系统调用将不同的文件描述符指向同一个struct file对象。

7、接下来会根据操作符op的不同做不同的处理，这里我们只看op等于EPOLL_CTL_ADD时的添加操作。首先会判断上一步操作中返回的epollitem对象地址是否为NULL，不是NULL说明该文件已经添加过了，返回错误，否则调用ep_insert()函数进行真正的添加操作。在添加文件之前内核会自动为该文件增加POLLERR和POLLHUP事件。

8、ep_insert()函数中，首先判断epoll实例中监视的文件数量是否已超过限制，没问题则为待添加的文件创建一个epollitem对象。

9、接下来是比较重要的操作：将epollitem对象添加到被监视文件的等待队列上去。等待队列实际上就是一个回调函数链表，定义在/include/linux/wait.h文件中。因为不同文件系统的实现不同，无法直接通过struct file对象获取等待队列，因此这里通过struct file的poll操作，以回调的方式返回对象的等待队列，这里设置的回调函数是ep_ptable_queue_proc。

10、结构体ep_queue的作用是能够在poll的回调函数中取得对应的epollitem对象，这种做法在Linux内核里非常常见。

11、在回调函数ep_ptable_queue_proc中，内核会创建一个struct eppoll_entry对象，然后将等待队列中的回调函数设置为ep_poll_callback()。也就是说，当被监控文件有事件到来时，比如socker收到数据时，ep_poll_callback()会被回调。

12、ep_item_poll()调用完成之后，会将epitem中的fllink字段添加到struct file中的f_ep_links链表中，这样就可以通过struct file找到所有对应的struct epollitem对象。

13、然后就是将epollitem插入到红黑树中。

14、最后再更新下状态就返回了，插入操作也就完成了。

（在这个实现时，将用户空间epoll_event拷贝到内核中，后续可以将其转化为epitem作为节点存入红黑树中，从eventpoll的红黑树中查找fd所对应的epitem实例（二分搜索），根据传入的op参数行为进行switch判断，对红黑树进行不同的操作。对于ep_insert，首先设置了对应的回调函数，然后调用被监控文件的poll方法（每个支持poll的设备驱动程序都要调用），其实就是在poll里调用了回调函数，这个回调函数实际上不是真正的回调函数，真正的回调函数(ep_poll_callback)在该函数内调用，这个回调函数只是创建了struct eppoll_entry，将真正回调函数和epitem关联起来，之后将其加入设备等待队列。当设备就绪，唤醒等待队列上的等待者，调用对应的真正的回调函数，这个回调函数实际上就是将红黑树上收到event的epitem插入到它的就绪队列中并唤醒调用epoll_wait进程。在ep_insert中还将epitem插入到eventpoll中的红黑树上，然后还会去判断当前插入的event是否是刚好发生，如果是直接将其加入就绪队列，然后唤醒epoll_wait。）

**epoll_wait**

观察就绪列表里面有没有数据，并进行提取和清空就绪列表，非常高效。

ep_poll_callback函数主要的功能是将被监视文件的等待事件就绪时，将文件对应的epitem实例添加到就绪队列中，当用户调用epoll_wait()时，内核会将就绪队列中的事件报告给用户。

在epoll_wait主要是调用了ep_poll，在ep_poll里直接判断就绪链表有无数据，有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。当有数据时，还需要将内核就绪事件拷贝到传入参数的events中的用户空间，就绪链表中的数据一旦拷贝就没有了，所以这里要区分LT和ET，如果是LT有可能会将后续的重新放入就绪链表。

ps：我们在调用ep_send_events_proc()将就绪队列中的事件拷贝给用户的期间，新就绪的events被挂载到eventpoll.ovflist所以我们需要遍历eventpoll.ovflist将所有已就绪的epitem 重新挂载到就绪队列中，等待下一次epoll_wait()进行交付…

![img](https://pic4.zhimg.com/80/v2-041903f671cd72b4c1278090cf8c946f_720w.webp)

## 3.LT和ET实现的区别？

由epoll_wait进行实现，如果是LT模式，它发现socket上还有未处理的事件，则在清理就绪列表后，重新把句柄放回刚刚清空的就绪列表。

LT(level triggered) 是 缺省 的工作方式 ，并且同时支持 block 和 no-block socket. 在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的 fd 进行 IO 操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的 select/poll 都是这种模型的代表．

ET(edge-triggered) 是高速工作方式 ，只支持 no-block socket 。在这种模式下，当描述符从未就绪变为就绪时，内核通过 epoll 告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了 ( 比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个 EWOULDBLOCK 错误）。但是请注意，如果一直不对这个 fd 作 IO 操作 ( 从而导致它再次变成未就绪 ) ，内核不会发送更多的通知 (only once), 不过在 TCP 协议中， ET 模式的加速效用仍需要更多的 benchmark 确认。

epoll 工作在 ET 模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读 / 阻塞写操作把处理多个文件描述符的任务饿死。最好以下面的方式调用 ET 模式的 epoll 接口，在后面会介绍避免可能的缺陷。

- 基于非阻塞文件句柄
- 只有当 read(2) 或者 write(2) 返回 EAGAIN 时才需要挂起，等待。但这并不是说每次 read() 时都需要循环读，直到读到产生一个 EAGAIN 才认为此次事件处理完成，当 read() 返回的读到的数据长度小于请求的数据长度时，就可以确定此时缓冲中已没有数据了，也就可以认为此事读事件已处理完成。

![img](https://pic4.zhimg.com/80/v2-1cdda3420065a89dbb5758788f0d60b7_720w.webp)

红黑树中每个成员由描述符值和所要监控的文件描述符指向的文件表项的引用等组成。

## 4.关键数据结构

```text
// epoll的核心实现,对应于一个epoll描述符  
struct eventpoll {  
    spinlock_t lock;  
    struct mutex mtx;  
    wait_queue_head_t wq; // sys_epoll_wait() 等待在这里  
    // f_op->poll()  使用的, 被其他事件通知机制利用的wait_address  
    wait_queue_head_t poll_wait;  
    //已就绪的需要检查的epitem 列表 
    struct list_head rdllist;  
    //保存所有加入到当前epoll的文件对应的epitem  
    struct rb_root rbr;  
    // 当正在向用户空间复制数据时, 产生的可用文件  
    struct epitem *ovflist;  
    /* The user that created the eventpoll descriptor */  
    struct user_struct *user;  
    struct file *file;  
    //优化循环检查，避免循环检查中重复的遍历
    int visited;  
    struct list_head visited_list_link;  
}  
```



```text
// 对应于一个加入到epoll的文件  
struct epitem {  
    // 挂载到eventpoll 的红黑树节点  
    struct rb_node rbn;  
    // 挂载到eventpoll.rdllist 的节点  
    struct list_head rdllink;  
    // 连接到ovflist 的指针  
    struct epitem *next;  
    /* 文件描述符信息fd + file, 红黑树的key */  
    struct epoll_filefd ffd;  
    /* Number of active wait queue attached to poll operations */  
    int nwait;  
    // 当前文件的等待队列(eppoll_entry)列表  
    // 同一个文件上可能会监视多种事件,  
    // 这些事件可能属于不同的wait_queue中  
    // (取决于对应文件类型的实现),  
    // 所以需要使用链表  
    struct list_head pwqlist;  
    // 当前epitem 的所有者  
    struct eventpoll *ep;  
    /* List header used to link this item to the &quot;struct file&quot; items list */  
    struct list_head fllink;  
    /* epoll_ctl 传入的用户数据 */  
    struct epoll_event event;  
};  
```



```text
// 与一个文件上的一个wait_queue_head 相关联，因为同一文件可能有多个等待的事件，
//这些事件可能使用不同的等待队列  
struct eppoll_entry {  
    // List struct epitem.pwqlist  
    struct list_head llink;  
    // 所有者  
    struct epitem *base;  
    // 添加到wait_queue 中的节点  
    wait_queue_t wait;  
    // 文件wait_queue 头  
    wait_queue_head_t *whead;  
}; 
```

## 5.红黑树在epoll中的作用

![img](https://pic3.zhimg.com/80/v2-c1030e59657ec2413bf526f043ba1b76_720w.webp)

哈希表. 空间因素,可伸缩性.

(1)频繁增删. 哈希表需要预估空间大小, 这个场景下无法做到.

间接影响响应时间,假如要resize,原来的数据还得移动.即使用了一致性哈希算法,

也难以满足非阻塞的timeout时间限制.(时间不稳定)

(2) 百万级连接,哈希表有镂空的空间,太浪费内存.

跳表. 慢于红黑树. 空间也高.

红黑树. 经验判断,内核的其他地方如防火墙也使用红黑树,实践上看性能最优.

AVL树.平衡二叉树能够保证在最坏的情况下也能达到lgN，要实现这一目标，我们就要保证在插入完成后始终保持平衡状态。在一棵具有N个节点的树中，我们希望该树的高度能够维持在lgN左右，这样我们就能保证只需要lgN次比较操作就可以查找到想要的值。不幸的是，每次插入元素之后维持树的平衡状态太昂贵。所以就出现一些新的数据结构来保证在最坏的情况下插入和查找效率都能保证在对数的时间复杂度内完成。

**文件描述符是以什么方式挂载在红黑树的节点中？是int吗？**

```text
/* 文件描述符信息fd + file, 红黑树的key */  
struct epoll_filefd ffd;
```

## 6.epoll惊群了解多少？

和虚假唤醒有点像。

考虑如下场景：

主进程创建socket, bind, listen之后，fork出多个子进程，每个子进程都开始循环处理（accept)这个socket。每个进程都阻塞在accpet上，当一个新的连接到来时，所有的进程都会被唤醒，但其中只有一个进程会accept成功，其余皆失败，重新休眠。这就是accept惊群。

那么这个问题真的存在吗？

事实上，历史上，Linux 的 accpet 确实存在惊群问题，但现在的内核都解决该问题了。即，当多个进程/线程都阻塞在对同一个 socket 的 accept 调用上时，当有一个新的连接到来，内核只会唤醒一个进程，其他进程保持休眠，压根就不会被唤醒。

如上所述，accept 已经不存在惊群问题，但 epoll 上还是存在惊群问题。即，如果多个进程/线程阻塞在监听同一个 listening socket fd 的 epoll_wait 上，当有一个新的连接到来时，所有的进程都会被唤醒。

accept 确实应该只能被一个进程调用成功，内核很清楚这一点。但 epoll 不一样，他监听的文件描述符，除了可能后续被 accept 调用外，还有可能是其他网络 IO 事件的，而其他 IO 事件是否只能由一个进程处理，是不一定的，内核不能保证这一点，这是一个由用户决定的事情，例如可能一个文件会由多个进程来读写。所以，对 epoll 的惊群，内核则不予处理。

## 7.epoll是线程安全的吗？

简要结论就是epoll是通过锁来保证线程安全的, epoll中粒度最小的自旋锁ep->lock(spinlock)用来保护就绪的队列, 互斥锁ep->mtx用来保护epoll的重要数据结构红黑树。

先来看一下epoll的核心数据结构

```text
struct eventpoll {
	...
	/* 一个自旋锁 */
	spinlock_t lock;
		
	/* 一个互斥锁 */
	struct mutex mtx;

	/* List of ready file descriptors */
	/* 就绪fd队列 */
	struct list_head rdllist;

	/* RB tree root used to store monitored fd structs */
	/* 红黑树 */
	struct rb_root_cached rbr;
	...
};
```

**epoll_ctl()**

以下代码即为epoll_ctl()接口的实现, 当需要根据不同的operation通过ep_insert() 或者ep_remove()等接口对epoll自身的数据结构进行操作时都提前获得了ep->mex锁.

```text
/ * epoll_ctl() 接口 */
SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd,
	struct epoll_event __user *, event)
{
	...
	/* 获得 mtx 锁 */
	mutex_lock_nested(&ep->mtx, 0);
	...

	epi = ep_find(ep, tf.file, fd);

	error = -EINVAL;
	switch (op) {
	case EPOLL_CTL_ADD:
		/* 
		 * 通过ep_insert()接口来完成EPOLL_CTL_ADD的操作
		 * /
		if (!epi) {
			epds.events |= POLLERR | POLLHUP;
			error = ep_insert(ep, &epds, tf.file, fd, full_check);
		} else
			error = -EEXIST;
		if (full_check)
			clear_tfile_check_list();
		break;
	case EPOLL_CTL_DEL:
		/* 
		 * 通过ep_insert()接口来完成EPOLL_CTL_ADD的操作
		 * /
		if (epi)
			error = ep_remove(ep, epi);
		else
			error = -ENOENT;
		break;
	case EPOLL_CTL_MOD:
		/* 
		 * 通过ep_insert()接口来完成EPOLL_CTL_ADD的操作
		 * /
		if (epi) {
			if (!(epi->event.events & EPOLLEXCLUSIVE)) {
				epds.events |= POLLERR | POLLHUP;
				error = ep_modify(ep, epi, &epds);
			}
		} else
			error = -ENOENT;
		break;
	}
	if (tep != NULL)
		mutex_unlock(&tep->mtx);
	mutex_unlock(&ep->mtx);		/* 释放mtx锁 */
	...
}
```

**epoll_wait()**

```text
SYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events,
		int, maxevents, int, timeout)
{
	...
	/* Time to fish for events ... */
	/* 
	 * 可以看到epoll_wait()是通过ep_poll()来等待就绪事件的.
	 * /
	error = ep_poll(ep, events, maxevents, timeout);
	...
}
```



```text
static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)
{
	int pwake = 0;
	unsigned long flags;
	struct epitem *epi = ep_item_from_wait(wait);
	struct eventpoll *ep = epi->ep;
	int ewake = 0;
	
	/* 获得自旋锁 ep->lock来保护就绪队列
	 * 自旋锁ep->lock在ep_poll()里被释放
	 * /
	spin_lock_irqsave(&ep->lock, flags);

	/* If this file is already in the ready list we exit soon */
	/* 在这里将就绪事件添加到rdllist */
	if (!ep_is_linked(&epi->rdllink)) {
		list_add_tail(&epi->rdllink, &ep->rdllist);
		ep_pm_stay_awake_rcu(epi);
	}
	...
}
```

## 8.epoll API

回头来看看API吧。

头文件

```text
#include<sys/epoll.h>
```

创建句柄

```text
int epoll_create(int size);
```

创建一个epoll句柄，参数size用于告诉内核监听的文件描述符个数，跟内存大小有关。

返回epoll 文件描述符

**epoll控制函数**

```text
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event ); // 成功，返0，失败返-1
```

控制某个epoll监控的文件描述符上的事件：注册，修改，删除

参数释义：

epfd：为epoll的句柄

op:表示动作，用3个宏来表示

··· EPOLL_CTL_ADD（注册新的 fd 到epfd）

··· EPOLL_CTL_DEL（从 epfd 中删除一个 fd）

··· EPOLL_CTL_MOD（修改已经注册的 fd 监听事件）

event：告诉内核需要监听的事件

```text
typedef union epoll_data
{
    void* ptr;
    int fd;
    __uint32_t u32;
    __uint64_t u64;
} epoll_data_t;  /* 保存触发事件的某个文件描述符相关的数据 */
 
struct epoll_event
{
    __uint32_t events;  /* epoll event */
    epoll_data_t data;  /* User data variable */
};
/* epoll_event.events:
  EPOLLIN  表示对应的文件描述符可以读
  EPOLLOUT 表示对应的文件描述符可以写
  EPOLLPRI 表示对应的文件描述符有紧急的数据可读
  EPOLLERR 表示对应的文件描述符发生错误
  EPOLLHUP 表示对应的文件描述符被挂断
  EPOLLET  设置ET模式
*/
```

**epoll消息读取**

```text
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

等待所监控文件描述符上有事件的产生

参数释义：

events：用来从内核得到事件的集合

maxevent：用于告诉内核这个event有多大，这个maxevent不能大于创建句柄时的size

timeout：超时时间

··· -1：阻塞

··· 0：立即返回

···>0：指定微秒

成功返回有多少个文件描述符准备就绪，时间到返回0，出错返回-1.

代码示例

```text
/* 实现功能：通过epoll, 处理多个socket
 * 监听一个端口,监听到有链接时,添加到epoll_event
 * xs
 */
 
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <poll.h>
#include <sys/epoll.h>
#include <sys/time.h>
#include <netinet/in.h>
#include <unistd.h> 

#define MYPORT 12345
 
//最多处理的connect
#define MAX_EVENTS 500
 
//当前的连接数
int currentClient = 0; 
 
//数据接受 buf
#define REVLEN 10
char recvBuf[REVLEN];
 
 
//epoll描述符
int epollfd;
//事件数组
struct epoll_event eventList[MAX_EVENTS];
 
void AcceptConn(int srvfd);
void RecvData(int fd);
 
int main()
{
    int i, ret, sinSize;
    int recvLen = 0;
    fd_set readfds, writefds;
    int sockListen, sockSvr, sockMax;
    int timeout;
    struct sockaddr_in server_addr;
    struct sockaddr_in client_addr;
    
    //socket
    if((sockListen=socket(AF_INET, SOCK_STREAM, 0)) < 0)
    {
        printf("socket error\n");
        return -1;
    }
    
    bzero(&server_addr, sizeof(server_addr));
    server_addr.sin_family  =  AF_INET;
    server_addr.sin_port = htons(MYPORT);
    server_addr.sin_addr.s_addr  =  htonl(INADDR_ANY); 
    
    //bind
    if(bind(sockListen, (struct sockaddr*)&server_addr, sizeof(server_addr)) < 0)
    {
        printf("bind error\n");
        return -1;
    }
    
    //listen
    if(listen(sockListen, 5) < 0)
    {
        printf("listen error\n");
        return -1;
    }
    
    // epoll 初始化
    epollfd = epoll_create(MAX_EVENTS);
    struct epoll_event event;
    event.events = EPOLLIN|EPOLLET;
    event.data.fd = sockListen;
    
    //add Event
    if(epoll_ctl(epollfd, EPOLL_CTL_ADD, sockListen, &event) < 0)
    {
        printf("epoll add fail : fd = %d\n", sockListen);
        return -1;
    }
    
    //epoll
    while(1)
    {
        timeout=3000;                
        //epoll_wait
        int ret = epoll_wait(epollfd, eventList, MAX_EVENTS, timeout);
        
        if(ret < 0)
        {
            printf("epoll error\n");
            break;
        }
        else if(ret == 0)
        {
            printf("timeout ...\n");
            continue;
        }
        
        //直接获取了事件数量,给出了活动的流,这里是和poll区别的关键
        int i = 0;
        for(i=0; i<ret; i++)
        {
            //错误退出
            if ((eventList[i].events & EPOLLERR) ||
                (eventList[i].events & EPOLLHUP) ||
                !(eventList[i].events & EPOLLIN))
            {
              printf ( "epoll error\n");
              close (eventList[i].data.fd);
              return -1;
            }
            
            if (eventList[i].data.fd == sockListen)
            {
                AcceptConn(sockListen);
        
            }else{
                RecvData(eventList[i].data.fd);
            }
        }
    }
    
    close(epollfd);
    close(sockListen);
    
 
    return 0;
}
 
/**************************************************
函数名：AcceptConn
功能：接受客户端的链接
参数：srvfd：监听SOCKET
***************************************************/
void AcceptConn(int srvfd)
{
    struct sockaddr_in sin;
    socklen_t len = sizeof(struct sockaddr_in);
    bzero(&sin, len);
 
    int confd = accept(srvfd, (struct sockaddr*)&sin, &len);
 
    if (confd < 0)
    {
       printf("bad accept\n");
       return;
    }else
    {
        printf("Accept Connection: %d", confd);
    }
    //将新建立的连接添加到EPOLL的监听中
    struct epoll_event event;
    event.data.fd = confd;
    event.events =  EPOLLIN|EPOLLET;
    epoll_ctl(epollfd, EPOLL_CTL_ADD, confd, &event);
}
 
//读取数据
void RecvData(int fd)
{
    int ret;
    int recvLen = 0;
    
    memset(recvBuf, 0, REVLEN);
    printf("RecvData function\n");
    
    if(recvLen != REVLEN)
    {
        while(1)
        {
            //recv数据
            ret = recv(fd, (char *)recvBuf+recvLen, REVLEN-recvLen, 0);
            if(ret == 0)
            {
                recvLen = 0;
                break;
            }
            else if(ret < 0)
            {
                recvLen = 0;
                break;
            }
            //数据接受正常
            recvLen = recvLen+ret;
            if(recvLen<REVLEN)
            {
                continue;
            }
            else
            {
                //数据接受完毕
                printf("buf = %s\n",  recvBuf);
                recvLen = 0;
                break;
            }
        }
    }
 
    printf("data is %s", recvBuf);
}
```

## 9.番外：高效的并发方式

并发编程的目的是让程序”同时”执行多个任务。如果程序是计算密集型的，并发编程并没有什么优势，反而由于任务的切换使效率降低。但如果程序是I/O密集型的，那就不同了。

并发模式是指I/O处理单元和多个逻辑单元之间协调完成任务的方法，服务器主要有两种并发编程模式:半同步/半异步(half-sync/half-async)模式和领导者/追随者(Leader/Followers)模式。

这里讲一个“半同步/半异步”。

下面的内容需要有一定的基础了，小白可以收藏一下以后变强了再看。

**半同步/半异步模式**

在半同步/半异步模式中，同步线程用于处理客户逻辑，异步线程用于处理I/O事件。异步线程监听到客户请求之后就将其封装成请求对象并插入到请求队列中。请求队列将通知某个工作在同步模式的工作线程来读取并处理该请求对象。

![img](https://pic2.zhimg.com/80/v2-2c7d71211a356909e592e6b090ce7521_720w.webp)

**半同步/半反应堆模式(half-sync/half-reactive模式)**

半同步/半反应堆模式是半同步/半异步模式的一种变体。

其结构如下图:

![img](https://pic2.zhimg.com/80/v2-48c896f892fbbca0a88211339369688d_720w.webp)

在上图中，异步线程只有一个，由主线程充当，负责监听socket上的事件。如果监听socket上有新的连接请求到来，主线程就接受新的连接socket，然后往epoll内核事件表中注册该socket上的读写事件。如果连接socket上有读写事件发生，即有新的客户请求到来或有数据要发送至客户端，主线程就将该连接socket插入到请求队列中，所有工作线程都睡眠在请求队列上，当有任务到来时，他们通过竞争来获取任务的接管权。

由于主线程插入请求队列中的任务是就绪的连接socket，所以该半同步/半反应堆模式所采用的事件处理模式是Reactor模式，即工作线程要自己从socket上读写数据。当然，半同步/半反应堆模式也可以用模拟的Proactor事件处理模式，即由主线程来完成数据的读写操作，此时主线程将应用程序数据、任务类型等信息封装为一个任务对象，然后将其插入到请求队列。

半同步/半反应堆模式的缺点:

主线程和工作线程共享请求队列，因而请求队列是临界资源，所以对请求队列操作的时候需要加锁保护。

每个工作线程在同一时间只能处理一个客户请求。如果客户数量增多，则请求队列中堆积任务太多，客户端的响应会越来越慢。如果增多工作线程的话，则线程的切花也将消耗大量的CPU时间。

**高效的半同步/半异步模式**

在半同步/半反应堆模式中，每个工作线程同时只能处理一个客户请求，如果并发量大的话，客户端响应会很慢。如果每个工作线程都能同时处理多个客户链接，则就能改善这种情况，所以就有了高效的半同步/半异步模式。

其结构如图:

![img](https://pic1.zhimg.com/80/v2-ace9af7e4a576a1d70b07446d184b520_720w.webp)

主线程只管监听socket,当有新的连接socket到来时，主线程就接受连接并返回新的连接socket给某个工作线程。此后该新连接socket上的任何I/O操作都由被选中的工作线程来处理，直到客户端关闭连接。当工作线程检测到有新的连接socket到来时，就把该新的连接socket的读写事件注册到自己的epoll内核事件表中。

主线程和工作线程都维持自己的事件循环，他们各自独立的监听不同事件。因此在这种高效的半同步/半异步模式中，每个线程都工作在异步模式中，所以它并非严格意义上的半同步/半异步模式。

原文地址：https://zhuanlan.zhihu.com/p/427512269

作者：CPP后端技术

# 【NO.592】内存泄漏-原因、避免和定位

作为C/C++开发人员，内存泄漏是最容易遇到的问题之一，这是由C/C++语言的特性引起的。C/C++语言与其他语言不同，需要开发者去申请和释放内存，即需要开发者去管理内存，如果内存使用不当，就容易造成段错误(segment fault)或者内存泄漏(memory leak)。

今天，借助此文，分析下项目中经常遇到的导致内存泄漏的原因，以及如何避免和定位内存泄漏。

本文的主要内容如下：

![img](https://pic1.zhimg.com/80/v2-5e58747ebd3203c05e74b30ef120dd34_720w.webp)

## 1.背景

C/C++语言中，内存的分配与回收都是由开发人员在编写代码时主动完成的，好处是内存管理的开销较小，程序拥有更高的执行效率；弊端是依赖于开发者的水平，随着代码规模的扩大，极容易遗漏释放内存的步骤，或者一些不规范的编程可能会使程序具有安全隐患。如果对内存管理不当，可能导致程序中存在内存缺陷，甚至会在运行时产生内存故障错误。

内存泄漏是各类缺陷中十分棘手的一种，对系统的稳定运行威胁较大。当动态分配的内存在程序结束之前没有被回收时，则发生了内存泄漏。由于系统软件，如操作系统、编译器、开发环境等都是由C/C++语言实现的，不可避免地存在内存泄漏缺陷，特别是一些在服务器上长期运行的软件，若存在内存泄漏则会造成严重后果，例如性能下降、程序终止、系统崩溃、无法提供服务等。

所以，本文从原因、避免以及定位几个方面去深入讲解，希望能给大家带来帮助。

## 2.概念

内存泄漏（Memory Leak）是指程序中己动态分配的堆内存由于某种原因程序未释放或无法释放，造成系统内存的浪费，导致程序运行速度减慢甚至系统崩溃等严重后果。

当我们在程序中对原始指针(raw pointer)使用new操作符或者free函数的时候，实际上是在堆上为其分配内存，这个内存指的是RAM，而不是硬盘等永久存储。持续申请而不释放(或者少量释放)内存的应用程序，最终因内存耗尽导致OOM(out of memory)。

![img](https://pic1.zhimg.com/80/v2-d6376878235a44f6999e02f6dd1fae64_720w.webp)

方便大家理解内存泄漏的危害，我们举个简单的例子。有一个宾馆，有100间房间，顾客每次都是在前台进行登记，然后拿到房间钥匙。如果有些顾客不需要该房间了，也不归还钥匙，久而久之，前台处可用房间越来越少，收入也越来越少，濒临倒闭。当程序申请了内存，而不进行归还，久而久之，可用内存越来越少，OS就会进行自我保护，杀掉该进程，这就是我们常说的OOM(out of memory)。

## 3.分类

内存泄漏分为以下两类：

- 堆内存泄漏：我们经常说的内存泄漏就是堆内存泄漏，在堆上申请了资源，在结束使用的时候，没有释放归还给OS，从而导致该块内存永远不会被再次使用
- 资源泄漏：通常指的是系统资源，比如socket，文件描述符等，因为这些在系统中都是有限制的，如果创建了而不归还，久而久之，就会耗尽资源，导致其他程序不可用

本文主要分析堆内存泄漏，所以后面的内存泄漏均指的是堆内存泄漏。

## 4.根源

内存泄漏，主要指的是在堆(heap)上申请的动态内存泄漏，或者说是指针指向的内存块忘了被释放，导致该块内存不能再被申请重新使用。

之前在知乎上看了一句话，指针是C的精髓，也是初学者的一个坎。换句话说，内存管理是C的精髓，C/C++可以直接跟OS打交道，从性能角度出发，开发者可以根据自己的实际使用场景灵活进行内存分配和释放。虽然在C++中自C++11引入了smart pointer，虽然很大程度上能够避免使用裸指针，但仍然不能完全避免，最重要的一个原因是你不能保证组内其他人不适用指针，更不能保证合作部门不使用指针。

那么为什么C/C++中会存在指针呢？

这就得从进程的内存布局说起。

## 5.进程内存布局

![img](https://pic4.zhimg.com/80/v2-70034ef213a987bf827bb3d647d1c883_720w.webp)

上图为32位进程的内存布局，从上图中主要包含以下几个块：

内核空间：供内核使用，存放的是内核代码和数据

stack：这就是我们经常所说的栈，用来存储自动变量(automatic variable)

mmap:也成为内存映射，用来在进程虚拟内存地址空间中分配地址空间，创建和物理内存的映射关系

heap:就是我们常说的堆，动态内存的分配都是在堆上

bss:包含所有未初始化的全局和静态变量，此段中的所有变量都由0或者空指针初始化，程序加载器在加载程序时为BSS段分配内存

ds:初始化的数据块

- 包含显式初始化的全局变量和静态变量
- 此段的大小由程序源代码中值的大小决定，在运行时不会更改
- 它具有读写权限，因此可以在运行时更改此段的变量值
- 该段可进一步分为初始化只读区和初始化读写区

text：也称为文本段

- 该段包含已编译程序的二进制文件。
- 该段是一个只读段，用于防止程序被意外修改
- 该段是可共享的，因此对于文本编辑器等频繁执行的程序，内存中只需要一个副本

由于本文主要讲内存分配相关，所以下面的内容仅涉及到栈(stack)和堆(heap)。

![img](https://pic1.zhimg.com/80/v2-441bb5bd2deb8c11e7ae7a02807507b0_720w.webp)



## 6.栈

栈一块连续的内存块，栈上的内存分配就是在这一块连续内存块上进行操作的。编译器在编译的时候，就已经知道要分配的内存大小，当调用函数时候，其内部的遍历都会在栈上分配内存；当结束函数调用时候，内部变量就会被释放，进而将内存归还给栈。

```text
class Object {
  public:
    Object() = default;
    // ....
};

void fun() {
  Object obj;
  
  // do sth
}
```

在上述代码中，obj就是在栈上进行分配，当出了fun作用域的时候，会自动调用Object的析构函数对其进行释放。

前面有提到，局部变量会在作用域（如函数作用域、块作用域等）结束后析构、释放内存。因为分配和释放的次序是刚好完全相反的，所以可用到堆栈先进后出（first-in-last-out, FILO）的特性，而 C++ 语言的实现一般也会使用到调用堆栈（call stack）来分配局部变量（但非标准的要求）。

因为栈上内存分配和释放，是一个进栈和出栈的过程(对于编译器只是一个移动指针的过程)，所以相比于堆上的内存分配，栈要快的多。

虽然栈的访问速度要快于堆，每个线程都有一个自己的栈，栈上的对象是不能跨线程访问的，这就决定了栈空间大小是有限制的，如果栈空间过大，那么在大型程序中几十乃至上百个线程，光栈空间就消耗了RAM，这就导致heap的可用空间变小，影响程序正常运行。

**设置**

在Linux系统上，可用通过如下命令来查看栈大小：

```text
ulimit -s
10240
```

在笔者的机器上，执行上述命令输出结果是10240(KB)即10m，可以通过shell命令修改栈大小。

```text
ulimit -s 102400
```

通过如上命令，可以将栈空间临时修改为100m，可以通过下面的命令：

```text
/etc/security/limits.conf
```

**分配方式**

**静态分配**

静态分配由编译器完成，假如局部变量以及函数参数等，都在编译期就分配好了。

```text
void fun() {
  int a[10];
}
```

上述代码中，a占10 * sizeof(int)个字节，在编译的时候直接计算好了，运行的时候，直接进栈出栈。

**动态分配**

可能很多人认为只有堆上才会存在动态分配，在栈上只可能是静态分配。其实，这个观点是错的，栈上也支持动态分配，该动态分配由alloca()函数进行分配。栈的动态分配和堆是不同的，通过alloca()函数分配的内存由编译器进行释放，无序手动操作。

**特点**

- 分配速度快：分配大小由编译器在编译器完成
- 不会产生内存碎片：栈内存分配是连续的，以FIFO的方式进栈和出栈
- 大小受限：栈的大小依赖于操作系统
- 访问受限：只能在当前函数或者作用域内进行访问

## 7.堆

堆（heap）是一种内存管理方式。内存管理对操作系统来说是一件非常复杂的事情，因为首先内存容量很大，其次就是内存需求在时间和大小块上没有规律（操作系统上运行着几十甚至几百个进程，这些进程可能随时都会申请或者是释放内存，并且申请和释放的内存块大小是随意的）。

堆这种内存管理方式的特点就是自由（随时申请、随时释放、大小块随意）。堆内存是操作系统划归给堆管理器（操作系统中的一段代码，属于操作系统的内存管理单元）来管理的，堆管理器提供了对应的接口_sbrk、mmap_等，只是该接口往往由运行时库进行调用，即也可以说由运行时库进行堆内存管理，运行时库提供了malloc/free函数由开发人员调用，进而使用堆内存。

**分配方式**

正如我们所理解的那样，由于是在运行期进行内存分配，分配的大小也在运行期才会知道，所以堆只支持动态分配，内存申请和释放的行为由开发者自行操作，这就很容易造成我们说的内存泄漏。

**特点**

- 变量可以在进程范围内访问，即进程内的所有线程都可以访问该变量
- 没有内存大小限制，这个其实是相对的，只是相对于栈大小来说没有限制，其实最终还是受限于RAM
- 相对栈来说访问比较慢
- 内存碎片
- 由开发者管理内存，即内存的申请和释放都由开发人员来操作

## 8.堆与栈区别

理解堆和栈的区别，对我们开发过程中会非常有用，结合上面的内容，总结下二者的区别。

对于栈来讲，是由编译器自动管理，无需我们手工控制；对于堆来说，释放工作由程序员控制，容易产生memory leak

空间大小不同

- 一般来讲在 32 位系统下，堆内存可以达到4G的空间，从这个角度来看堆内存几乎是没有什么限制的。
- 对于栈来讲，一般都是有一定的空间大小的，一般依赖于操作系统(也可以人工设置)

能否产生碎片不同

- 对于堆来讲，频繁的内存分配和释放势必会造成内存空间的不连续，从而造成大量的碎片，使程序效率降低。
- 对于栈来讲，内存都是连续的，申请和释放都是指令移动，类似于数据结构中的进栈和出栈

增长方向不同

- 对于堆来讲，生长方向是向上的，也就是向着内存地址增加的方向
- 对于栈来讲，它的生长方向是向下的，是向着内存地址减小的方向增长

分配方式不同

- 堆都是动态分配的，比如我们常见的malloc/new；而栈则有静态分配和动态分配两种。
- 静态分配是编译器完成的，比如局部变量的分配，而栈的动态分配则通过alloca()函数完成
- 二者动态分配是不同的，栈的动态分配的内存由编译器进行释放，而堆上的动态分配的内存则必须由开发人自行释放

分配效率不同

- 栈有操作系统分配专门的寄存器存放栈的地址，压栈出栈都有专门的指令执行，这就决定了栈的效率比较高
- 堆内存的申请和释放专门有运行时库提供的函数，里面涉及复杂的逻辑，申请和释放效率低于栈

截止到这里，栈和堆的基本特性以及各自的优缺点、使用场景已经分析完成，在这里给开发者一个建议，能使用栈的时候，就尽量使用栈，一方面是因为效率高于堆，另一方面内存的申请和释放由编译器完成，这样就避免了很多问题。

## 9.扩展

终于到了这一小节，其实，上面讲的那么多，都是为这一小节做铺垫。

在前面的内容中，我们对比了栈和堆，虽然栈效率比较高，且不存在内存泄漏、内存碎片等，但是由于其本身的局限性(不能多线程、大小受限)，所以在很多时候，还是需要在堆上进行内存。

我们先看一段代码：

```text
#include <stdio.h>
#include <stdlib.h>

int main() {
  int a;
  int *p;
  p = (int *)malloc(sizeof(int));
  free(p);

  return 0;
}
```

上述代码很简单，有两个变量a和p，类型分别为int和int *，其中，a和p存储在栈上，p的值为在堆上的某块地址(在上述代码中，p的值为0x1c66010)，上述代码布局如下图所示：

![img](https://pic2.zhimg.com/80/v2-4e8fa45166b831272c853ab637c08089_720w.webp)

**产生方式**

以产生的方式来分类，内存泄漏可以分为四类:

- 常发性内存泄漏
- 偶发性内存泄漏
- 一次性内存泄漏
- 隐式内存泄漏

**常发性内存泄漏**

产生内存泄漏的代码或者函数会被多次执行到，在每次执行的时候，都会产生内存泄漏。

**偶发性内存泄漏**

与常发性内存泄漏不同的是，偶发性内存泄漏函数只在特定的场景下才会被执行。

笔者在19年的时候，曾经遇到一个这种内存泄漏。有一个函数专门进行价格加密，每次泄漏3个字节，且只有在竞价成功的时候，才会调用此函数进行价格加密，因此泄漏的非常不明显。当时发现这个问题，是上线后的第二天，帮忙排查线上问题，发现内存较上线前上涨了点(大概几百兆的样子)，了解glibc内存分配原理的都清楚，调用delete后，内存不一定会归还给OS，但是本着宁可信其有，不可信其无的心态，决定来分析是否真的存在内存泄漏。

当时用了个比较傻瓜式的方法，通过top命令，将该进程所占的内存输出到本地文件，大概几个小时后，将这些数据导入Excel中，内存占用基本呈一条斜线，所以基本能够确定代码存在内存泄漏，所以就对新上线的这部分代码进行重新review，定位到泄漏点，然后修复，重新上线。

**一次性内存泄漏**

这种内存泄漏在程序的生命周期内只会泄漏一次，或者说造成泄漏的代码只会被执行一次。

有的时候，这种可能不算内存泄漏，或者说设计如此。就以笔者现在线上的服务来说，类似于如下这种：

```text
int main() {
  auto *service = new Service;
  // do sth
  service->Run();// 服务启动
  service->Loop(); // 可以理解为一个sleep，目的是使得程序不退出
  return 0;
}
```

这种严格意义上，并不算内存泄漏，因为程序是这么设计的，即使程序异常退出，那么整个服务进程也就退出了，当然，在Loop()后面加个delete更好。

**隐式内存泄漏**

程序在运行过程中不停的分配内存，但是直到结束的时候才释放内存。严格的说这里并没有发生内存泄漏，因为最终程序释放了所有申请的内存。但是对于一个服务器程序，需要运行几天，几周甚至几个月，不及时释放内存也可能导致最终耗尽系统的所有内存。所以，我们称这类内存泄漏为隐式内存泄漏。

比较常见的隐式内存泄漏有以下三种：

- 内存碎片：还记得我们之前的那篇文章深入理解glibc内存管理精髓，程序跑了几天之后，进程就因为OOM导致了退出，就是因为内存碎片导致剩下的内存不能被重新分配导致
- 即使我们调用了free/delete，运行时库不一定会将内存归还OS，具体深入理解glibc内存管理精髓
- 用过STL的知道，STL内部有一个自己的allocator，我们可以当做一个memory poll，当调用vector.clear()时候，内存并不会归还OS，而是放回allocator，其内部根据一定的策略，在特定的时候将内存归还OS，是不是跟glibc原理很像

**分类**

**未释放**

这种是很常见的，比如下面的代码：

```text
int fun() {
    char * pBuffer = malloc(sizeof(char));
    
    /* Do some work */
    return 0;
}
```

上面代码是非常常见的内存泄漏场景(也可以使用new来进行分配)，我们申请了一块内存，但是在fun函数结束时候没有调用free函数进行内存释放。

在C++开发中，还有一种内存泄漏，如下：

```text
class Obj {
 public:
   Obj(int size) {
     buffer_ = new char;
   }
   ~Obj(){}
  private:
   char *buffer_;
};

int fun() {
  Object obj;
  // do sth
  return 0;
}
```

上面这段代码中，析构函数没有释放成员变量buffer_指向的内存，所以在编写析构函数的时候，一定要仔细分析成员变量有没有申请动态内存，如果有，则需要手动释放，我们重新编写了析构函数，如下：

```text
~Object() {
  delete buffer_;
}
```

在C/C++中，对于普通函数，如果申请了堆资源，请跟进代码的具体场景调用free/delete进行资源释放；对于class，如果申请了堆资源，则需要在对应的析构函数中调用free/delete进行资源释放。

**未匹配**

在C++中，我们经常使用new操作符来进行内存分配，其内部主要做了两件事：

1. 通过operator new从堆上申请内存(glibc下，operator new底层调用的是malloc)
2. 调用构造函数(如果操作对象是一个class的话)

对应的，使用delete操作符来释放内存，其顺序正好与new相反：

1. 调用对象的析构函数(如果操作对象是一个class的话)
2. 通过operator delete释放内存

```text
void* operator new(std::size_t size) {
    void* p = malloc(size);
    if (p == nullptr) {
        throw("new failed to allocate %zu bytes", size);
    }
    return p;
}
void* operator new[](std::size_t size) {
    void* p = malloc(size);
    if (p == nullptr) {
        throw("new[] failed to allocate %zu bytes", size);
    }
    return p;
}

void  operator delete(void* ptr) throw() {
    free(ptr);
}
void  operator delete[](void* ptr) throw() {
    free(ptr);
}
```

为了加深多这块的理解，我们举个例子：

```text
class Test {
 public:
   Test() {
     std::cout << "in Test" << std::endl;
   }
   // other
   ~Test() {
     std::cout << "in ~Test" << std::endl;
   }
};

int main() {
  Test *t = new Test;
  // do sth
  delete t;
  return 0;
}
```

在上述main函数中，我们使用new 操作符创建一个Test类指针

1. 通过operator new申请内存(底层malloc实现)
2. 通过placement new在上述申请的内存块上调用构造函数
3. 调用ptr->~Test()释放Test对象的成员变量
4. 调用operator delete释放内存

上述过程，可以理解为如下：

```text
// new
void *ptr = malloc(sizeof(Test));
t = new(ptr)Test
  
// delete
ptr->~Test();
free(ptr);
```

好了，上述内容，我们简单的讲解了C++中new和delete操作符的基本实现以及逻辑，那么，我们就简单总结下下产生内存泄漏的几种类型。

**new 和 free**

仍然以上面的Test对象为例，代码如下：

```text
Test *t = new Test;
free(t)
```

此处会产生内存泄漏，在上面，我们已经分析过，new操作符会先通过operator new分配一块内存，然后在该块内存上调用placement new即调用Test的构造函数。而在上述代码中，只是通过free函数释放了内存，但是没有调用Test的析构函数以释放Test的成员变量，从而引起内存泄漏。

**new[] 和 delete**

```text
int main() {
  Test *t = new Test [10];
  // do sth
  delete t;
  return 0;
}
```

在上述代码中，我们通过new创建了一个Test类型的数组，然后通delete操作符删除该数组，编译并执行，输出如下：

```text
in Test
in Test
in Test
in Test
in Test
in Test
in Test
in Test
in Test
in Test
in ~Test
```

从上面输出结果可以看出，调用了10次构造函数，但是只调用了一次析构函数，所以引起了内存泄漏。这是因为调用delete t释放了通过operator new[]申请的内存，即malloc申请的内存块，且只调用了t[0]对象的析构函数，t[1..9]对象的析构函数并没有被调用。

**虚析构**

记得08年面谷歌的时候，有一道题，面试官问，std::string能否被继承，为什么？

当时没回答上来，后来过了没多久，进行面试复盘的时候，偶然看到继承需要父类析构函数为virtual，才恍然大悟，原来考察点在这块。

下面我们看下std::string的析构函数定义：

```text
~basic_string() { 
  _M_rep()->_M_dispose(this->get_allocator()); 
}
```

这块需要特别说明下，std::basic_string是一个模板，而std::string是该模板的一个特化，即std::basic_string

```text
typedef std::basic_string<char> string;
```

现在我们可以给出这个问题的答案：不能，因为std::string的析构函数不为virtual，这样会引起内存泄漏。

仍然以一个例子来进行证明。

```text
class Base {
 public:
  Base(){
    buffer_ = new char[10];
  }

  ~Base() {
    std::cout << "in Base::~Base" << std::endl;
    delete []buffer_;
  }
private:
  char *buffer_;

};

class Derived : public Base {
 public:
  Derived(){}

  ~Derived() {
    std::cout << "int Derived::~Derived" << std::endl;
  }
};

int main() {
  Base *base = new Derived;
  delete base;
  return 0;
}
```

上面代码输出如下：

```text
in Base::~Base
```

可见，上述代码并没有调用派生类Derived的析构函数，如果派生类中在堆上申请了资源，那么就会产生内存泄漏。

为了避免因为继承导致的内存泄漏，我们需要将父类的析构函数声明为virtual，代码如下(只列了部分修改代码，其他不变):

```text
~Base() {
    std::cout << "in Base::~Base" << std::endl;
    delete []buffer_;
  }
```

然后重新执行代码，输出结果如下：

```text
int Derived::~Derived
in Base::~Base
```

借助此文，我们再次总结下存在继承情况下，构造函数和析构函数的调用顺序。

派生类对象在创建时构造函数调用顺序：

1. 调用父类的构造函数
2. 调用父类成员变量的构造函数
3. 调用派生类本身的构造函数

派生类对象在析构时的析构函数调用顺序：

1. 执行派生类自身的析构函数
2. 执行派生类成员变量的析构函数
3. 执行父类的析构函数

为了避免存在继承关系时候的内存泄漏，请遵守一条规则：无论派生类有没有申请堆上的资源，请将父类的析构函数声明为virtual。

**循环引用**

在C++开发中，为了尽可能的避免内存泄漏，自C++11起引入了smart pointer，常见的有shared_ptr、weak_ptr以及unique_ptr等(auto_ptr已经被废弃)，其中weak_ptr是为了解决循环引用而存在，其往往与shared_ptr结合使用。

下面，我们看一段代码：

```text
class Controller {
 public:
  Controller() = default;

  ~Controller() {
    std::cout << "in ~Controller" << std::endl;
  }

  class SubController {
   public:
    SubController() = default;

    ~SubController() {
      std::cout << "in ~SubController" << std::endl;
    }

    std::shared_ptr<Controller> controller_;
  };

  std::shared_ptr<SubController> sub_controller_;
};

int main() {
  auto controller = std::make_shared<Controller>();
  auto sub_controller = std::make_shared<Controller::SubController>();

  controller->sub_controller_ = sub_controller;
  sub_controller->controller_ = controller;
  return 0;
}
```

编译并执行上述代码，发现并没有调用Controller和SubController的析构函数，我们尝试着打印下引用计数，代码如下：

```text
int main() {
  auto controller = std::make_shared<Controller>();
  auto sub_controller = std::make_shared<Controller::SubController>();

  controller->sub_controller_ = sub_controller;
  sub_controller->controller_ = controller;

  std::cout << "controller use_count: " << controller.use_count() << std::endl;
  std::cout << "sub_controller use_count: " << sub_controller.use_count() << std::endl;
  return 0;
}
```

编译并执行之后，输出如下：

```text
controller use_count: 2
sub_controller use_count: 2
```

通过上面输出可以发现，因为引用计数都是2，所以在main函数结束的时候，不会调用controller和sub_controller的析构函数，所以就出现了内存泄漏。

上面产生内存泄漏的原因，就是我们常说的循环引用。

![img](https://pic2.zhimg.com/80/v2-9f040f0db90f89242be50912390c770d_720w.webp)

为了解决std::shared_ptr循环引用导致的内存泄漏，我们可以使用std::weak_ptr来单面去除上图中的循环。

```text
class Controller {
 public:
  Controller() = default;

  ~Controller() {
    std::cout << "in ~Controller" << std::endl;
  }

  class SubController {
   public:
    SubController() = default;

    ~SubController() {
      std::cout << "in ~SubController" << std::endl;
    }

    std::weak_ptr<Controller> controller_;
  };

  std::shared_ptr<SubController> sub_controller_;
};
```

在上述代码中，我们将SubController类中controller_的类型从std::shared_ptr变成std::weak_ptr，重新编译执行，结果如下：

```text
controller use_count: 1
sub_controller use_count: 2
in ~Controller
in ~SubController
```

从上面结果可以看出，controller和sub_controller均以释放，所以循环引用引起的内存泄漏问题，也得以解决。

![img](https://pic3.zhimg.com/80/v2-06dfe5e79265dab1bd0ed9f8fb059592_720w.webp)

可能有人会问，使用std::shared_ptr可以直接访问对应的成员函数，如果是std::weak_ptr的话，怎么访问呢？我们可以使用下面的方式:

```text
std::shared_ptr controller = controller_.lock();
```

即在子类SubController中，如果要使用controller调用其对应的函数，就可以使用上面的方式。

## 10.避免

**避免在堆上分配**

众所周知，大部分的内存泄漏都是因为在堆上分配引起的，如果我们不在堆上进行分配，就不会存在内存泄漏了(这不废话嘛)，我们可以根据具体的使用场景，如果对象可以在栈上进行分配，就在栈上进行分配，一方面栈的效率远高于堆，另一方面，还能避免内存泄漏，我们何乐而不为呢。

**手动释放**

- 对于malloc函数分配的内存，在结束使用的时候，使用free函数进行释放
- 对于new操作符创建的对象，切记使用delete来进行释放
- 对于new []创建的对象，使用delete[]来进行释放(使用free或者delete均会造成内存泄漏)

**避免使用裸指针**

尽可能避免使用裸指针，除非所调用的lib库或者合作部门的接口是裸指针。

```text
int fun(int *ptr) {// fun 是一个接口或lib函数
  // do sth
  
  return 0;
}

int main() {}
  int a = 1000;
  int *ptr = &a;
  // ...
  fun(ptr);
  
  return 0;
}
```

在上面的fun函数中，有一个参数ptr,为int *，我们需要根据上下文来分析这个指针是否需要释放，这是一种很不好的设计

**使用STL中或者自己实现对象**

在C++中，提供了相对完善且可靠的STL供我们使用，所以能用STL的尽可能的避免使用C中的编程方式，比如：

- 使用std::string 替代char *, string类自己会进行内存管理，而且优化的相当不错
- 使用std::vector或者std::array来替代传统的数组
- 其它

## 11.智能指针

自C++11开始，STL中引入了智能指针(smart pointer)来动态管理资源，针对使用场景的不同，提供了以下三种智能指针。

**unique_ptr**

unique_ptr是限制最严格的一种智能指针，用来替代之前的auto_ptr，独享被管理对象指针所有权。当unique_ptr对象被销毁时，会在其析构函数内删除关联的原始指针。

unique_ptr对象分为以下两类：

- unique_ptr 该类型的对象关联了单个Type类型的指针
- std::unique_ptr<Type> p1(new Type); // c++11 auto p1 = std::make_unique<Type>(); // c++14
- unique_ptr<Type[]> 该类型的对象关联了多个Type类型指针，即一个对象数组
- std::unique_ptr<Type[]> p2(new Type[n]()); // c++11 auto p2 = std::make_unique<Type[]>(n); // c++14
- 不可用被复制
- unique_ptr<int> a(new int(0)); unique_ptr<int> b = a; // 编译错误 unique_ptr<int> b = std::move(a); // 可以通过move语义进行所有权转移

根据使用场景，可以使用std::unique_ptr来避免内存泄漏，如下：

```text
void fun() {
  unique_ptr<int> a(new int(0));
  // use a
}
```

在上述fun函数结束的时候，会自动调用a的析构函数，从而释放其关联的指针。

**shared_ptr**

与unique_ptr不同的是，unique_ptr是独占管理权，而shared_ptr则是共享管理权，即多个shared_ptr可以共用同一块关联对象，其内部采用的是引用计数，在拷贝的时候，引用计数+1，而在某个对象退出作用域或者释放的时候，引用计数-1，当引用计数为0的时候，会自动释放其管理的对象。

```text
void fun() {
  std::shared_ptr<Type> a; // a是一个空对象
  {
    std::shared_ptr<Type> b = std::make_shared<Type>(); // 分配资源
    a = b; // 此时引用计数为2
    {
      std::shared_ptr<Type> c = a; // 此时引用计数为3
    } // c退出作用域，此时引用计数为2
  } // b 退出作用域，此时引用计数为1
} // a 退出作用域，引用计数为0，释放对象
```

**weak_ptr**

weak_ptr的出现，主要是为了解决shared_ptr的循环引用，其主要是与shared_ptr一起来私用。和shared_ptr不同的地方在于，其并不会拥有资源，也就是说不能访问对象所提供的成员函数，不过，可以通过weak_ptr.lock()来产生一个拥有访问权限的shared_ptr。

```text
std::weak_ptr<Type> a;
{
  std::shared_ptr<Type> b = std::make_shared<Type>();
  a = b
} // b所对应的资源释放
```

**RAII**

RAII是Resource Acquisition is Initialization(资源获取即初始化)的缩写，是C++语言的一种管理资源，避免泄漏的用法。

利用的就是C++构造的对象最终会被销毁的原则。利用C++对象生命周期的概念来控制程序的资源,比如内存,文件句柄,网络连接等。

RAII的做法是使用一个对象，在其构造时获取对应的资源，在对象生命周期内控制对资源的访问，使之始终保持有效，最后在对象析构的时候，释放构造时获取的资源。

简单地说，就是把资源的使用限制在对象的生命周期之中，自动释放。

举个简单的例子，通常在多线程编程的时候，都会用到std::mutex,以下为例

```text
std::mutex mutex_;

void fun() {
  mutex_.lock();
  
  if (...) {
    mutex_.unlock();
    return;
  }
  
  mutex_.unlock()
}
```

在上述代码中，如果if分支多的话，每个if分支里面都要释放锁，如果一不小心忘记释放，那么就会造成故障，为了解决这个问题，我们使用RAII技术，代码如下：

```text
std::mutex mutex_;

void fun() {
  std::lock_guard<std::mutex> guard(mutex_);

  if (...) {
    return;
  }
}
```

在guard出了fun作用域的时候，会自动调用mutex_.lock()进行释放，避免了很多不必要的问题。

## 12.定位

在发现程序存在内存泄漏后，往往需要定位泄漏点，而定位这一步往往是最困难的，所以经常为了定位泄漏点，采取各种各样的方案，甭管方案优雅与否，毕竟管他白猫黑猫，抓住老鼠才是好猫，所以在本节，简单说下笔者这么多年定位泄漏点的方案，有些比较邪门歪道，您就随便看看就行。

**日志**

这种方案的核心思想，就是在每次分配内存的时候，打印指针地址，在释放内存的时候，打印内存地址，这样在程序结束的时候，通过分配和释放的差，如果分配的条数大于释放的条数，那么基本就能确定程序存在内存泄漏，然后根据日志进行详细分析和定位。

```text
char * fun() {
  char *p = (char*)malloc(20);
  printf("%s, %d, address is: %p", __FILE__, __LINE__, p);
  // do sth
  return p;
}

int main() {
  fun();
  
  return 0;
}
```

**统计**

统计方案可以理解为日志方案的一种特殊实现，其主要原理是在分配的时候，统计分配次数，在释放的时候，则是统计释放的次数，这样在程序结束前判断这俩值是否一致，就能判断出是否存在内存泄漏。

此方法可帮助跟踪已分配内存的状态。为了实现这个方案，需要创建三个自定义函数，一个用于内存分配，第二个用于内存释放，最后一个用于检查内存泄漏。代码如下：

```text
static unsigned int allocated  = 0;
static unsigned int deallocated  = 0;
void *Memory_Allocate (size_t size)
{
    void *ptr = NULL;
    ptr = malloc(size);
    if (NULL != ptr) {
        ++allocated;
    } else {
        //Log error
    }
    return ptr;
}
void Memory_Deallocate (void *ptr) {
    if(pvHandle != NULL) {
        free(ptr);
        ++deallocated;
    }
}
int Check_Memory_Leak(void) {
    int ret = 0;
    if (allocated != deallocated) {
        //Log error
        ret = MEMORY_LEAK;
    } else {
        ret = OK;
    }
    return ret;
}
```

**工具**

在Linux上比较常用的内存泄漏检测工具是valgrind，所以咱们就以valgrind为工具，进行检测。

我们首先看一段代码：

```text
#include <stdlib.h>

void func (void){
    char *buff = (char*)malloc(10);
}

int main (void){
    func(); // 产生内存泄漏
    return 0;
}
```

- 通过gcc -g leak.c -o leak命令进行编译
- 执行valgrind --leak-check=full ./leak

在上述的命令执行后，会输出如下：

```text
==9652== Memcheck, a memory error detector
==9652== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==9652== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info
==9652== Command: ./leak
==9652==
==9652==
==9652== HEAP SUMMARY:
==9652==     in use at exit: 10 bytes in 1 blocks
==9652==   total heap usage: 1 allocs, 0 frees, 10 bytes allocated
==9652==
==9652== 10 bytes in 1 blocks are definitely lost in loss record 1 of 1
==9652==    at 0x4C29F73: malloc (vg_replace_malloc.c:309)
==9652==    by 0x40052E: func (leak.c:4)
==9652==    by 0x40053D: main (leak.c:8)
==9652==
==9652== LEAK SUMMARY:
==9652==    definitely lost: 10 bytes in 1 blocks
==9652==    indirectly lost: 0 bytes in 0 blocks
==9652==      possibly lost: 0 bytes in 0 blocks
==9652==    still reachable: 0 bytes in 0 blocks
==9652==         suppressed: 0 bytes in 0 blocks
==9652==
==9652== For lists of detected and suppressed errors, rerun with: -s
==9652== ERROR SUMMARY: 1 errors from 1 contexts (suppressed: 0 from 0)
```

valgrind的检测信息将内存泄漏分为如下几类：

- definitely lost：确定产生内存泄漏
- indirectly lost：间接产生内存泄漏
- possibly lost：可能存在内存泄漏
- still reachable：即使在程序结束时候，仍然有指针在指向该块内存，常见于全局变量

主要上面输出的下面几句：

```text
==9652==    by 0x40052E: func (leak.c:4)
==9652==    by 0x40053D: main (leak.c:8)
SHELL 复制 全屏
```

提示在main函数(leak.c的第8行)fun函数(leak.c的第四行)产生了内存泄漏，通过分析代码，原因定位，问题解决。

valgrind不仅可以检测内存泄漏，还有其他很强大的功能，由于本文以内存泄漏为主，所以其他的功能就不在此赘述了，有兴趣的可以通过valgrind --help来进行查看

> 对于Windows下的内存泄漏检测工具，笔者推荐一款轻量级功能却非常强大的工具UMDH，笔者在十二年前，曾经在某外企负责内存泄漏，代码量几百万行，光编译就需要两个小时，尝试了各种工具(免费的和收费的)，最终发现了UMDH，如果你在Windows上进行开发，强烈推荐。

## 13.经验之谈

在C/C++开发过程中，内存泄漏是一个非常常见的问题，其影响相对来说远低于coredump等，所以遇到内存泄漏的时候，不用过于着急，大不了重启嘛。

在开发过程中遵守下面的规则，基本能90+%避免内存泄漏：

- 良好的编程习惯，只有有malloc/new，就得有free/delete
- 尽可能的使用智能指针，智能指针就是为了解决内存泄漏而产生
- 使用log进行记录
- 也是最重要的一点，谁申请，谁释放

对于malloc分配内存，分配失败的时候返回值为NULL，此时程序可以直接退出了，而对于new进行内存分配，其分配失败的时候，是抛出std::bad_alloc，所以为了第一时间发现问题，不要对new异常进行catch，毕竟内存都分配失败了，程序也没有运行的必要了。

如果我们上线后，发现程序存在内存泄漏，如果不严重的话，可以先暂时不管线上，同时进行排查定位；如果线上泄漏比较严重，那么第一时间根据实际情况来决定是否回滚。在定位问题点的时候，可以采用缩小范围法，着重分析这次新增的代码，这样能够有效缩短问题解决的时间。

## 14.结语

C/C++之所以复杂、效率高，是因为其灵活性，可用直接访问操作系统API，而正因为其灵活性，就很容易出问题，团队成员必须愿意按照一定的规则来进行开发，有完整的review机制，将问题暴露在上线之前。这样才可以把经历放在业务本身，而不是查找这些问题上，有时候往往一个小问题就能消耗很久的时间去定位解决，所以，一定要有一个良好的开发习惯。

原文地址：https://zhuanlan.zhihu.com/p/523628871

作者：CPP后端技术

# 【NO.593】一道腾讯面试题目：没有listen，能否建立TCP连接

TCP与UDP最大的不同，就是有连接的概念，而连接的建立是由内核完成的。系统调用listen，就是为了告诉内核，它要处理发给这个TCP端口的连接请求。所以对于这个题目，最直接的想法就是由应用层自己负责TCP的连接。为了能够收到TCP的握手数据包，可以尝试使用原始套接字来接收IP报文，这样就可以在应用层替代内核做TCP的三次握手了。这个想法不错，可惜现实比较残酷。

当没有对于TCP 套接字处于listen状态时，使用raw socket处理握手报文时，即使收到了syn报文并给对端发送了syn+ack报文，也无法完成连接。因为内核一般会提前发送RST中断该连接。七年前，我当时只知道这个结果。现在呢，可以明确的知道为什么内核会发送RST报文，中断连接。内核在ip_local_deliver_finish先将报文复制一份给原始套接字，然后会继续后面的处理，进入tcp的接收函数tcp_v4_rcv。在这个函数中，要进行套接字的查找。

![img](https://pic2.zhimg.com/80/v2-8144d1faa8cbf55b27ef593dfcfad341_720w.webp)

因为没有监听的tcp套接字，自然无法找到对应的套接字。于是跳转到no_tcp_socket。

![img](https://pic4.zhimg.com/80/v2-cf9f42c760a1d4d22772de4d82b7f917_720w.webp)

在这个错误处理中，只要数据包skb的校验和没错，内核就会调用tcp_v4_send_reset发送RST中止这个连接。因此，这个单独使用raw socket的方案是行不通的。我们需要找到一种方法，禁止内核处理该TCP报文。这时，可以使用iptables的NFQUEUE，在网络层将数据包取到应用层，并回复syn+ack，同时在reinject时通知内核该数据包被“偷”了，即NF_STOLEN。这样内核就不会继续处理该数据包skb了。虽然当时我没有继续实验尝试，但理论上，通过这个IPtables的NFQUEUE+NFSTOLEN的方案，是可以实现“没有listen，建立TCP连接”的。

可惜，在与那位同学的讨论中，腾讯面试题目的本意不是这个意思，而是对于普通的TCP套接字来说，如果没有listen调用，是否可以创建连接。即使限定了条件，答案依然是肯定的。只不过限定了条件之后，我们需要确定2个事情：

1. 与前面类似，如何避免内核发送RST。在不能使用iptable的前提下，这意味着在tcp_v4_rcv中，要能够找到对应的套接字。
2. 没有listen状态的套接字，内核是否能够完成TCP的三次握手呢？

确定第一个问题，比较简单。只需要对三次握手深入的思考一下，就可以得到答案。在正常的三次握手中，当服务端回复syn+ack时，客户端实际上也没有处于listen状态的套接字，但却可以完成三次握手。这意味着，客户端进行connect调用后，该套接字一定被加入到某个表中，并可以被匹配到。跟踪内核源码tcp_v4_connect->inet_hash_connect->__inet_check_established，可以看到当调用connect时，对应的套接字就被加入了全局的tcp已连接的表中，即tcp_hashinfo.ehash中。对应的匹配TCP套接字过程，如下__inet_lookup_skb->__inet_lookup

![img](https://pic1.zhimg.com/80/v2-67c79f1cf00f61fa7dd93e1c86448f4c_720w.webp)

内核是先在已经连接的表中查找，再进行listen表的查找。对于客户端来说，syn+ack报文必然可以在已连接表中匹配上对应的套接字。那么，对于本题目来说，要想两端都可以找到套接字，就要求在报文到达前，两端都调用了connect。也就是说，当两端同时调用connect时，两端的syn包就都可以匹配上本地的套接字。

接下来只需要确定对于客户端套接字来说，收到syn报文，是否可以正常处理。tcp_rcv_synsent_state_process函数是TCP套接字处于synsent状态（即调用了connect）的处理函数。下面是其一部分实现代码：

![img](https://pic2.zhimg.com/80/v2-84b12d0dfd1238480d8643fd70d74c01_720w.webp)

![img](https://pic2.zhimg.com/80/v2-eeb5afdef2e1acb5724ea54eb7d6a889_720w.webp)

从上，可以得出，对于处于synsent状态的套接字来说，如果收到了syn报文，则会正常回复synack，完成TCP三次握手。

对于腾讯的这道面试题目来说，其答案就是当两端同时发起connect调用时，即使没有listen调用，也可以成功创建TCP连接。

如果去掉“两端”的限制，还有一个答案就是，TCP套接字可以connect它本身bind的地址和端口，也可以达成要求。下面是测试代码，实现了一个TCP套接字成功连接自己，并发送消息。

```text
#include <sys/types.h>          /* See NOTES */
#include <sys/socket.h>
#include <netinet/in.h>
#include <unistd.h>
#include <stdlib.h>
#include <stdio.h>
#include <string.h>
#include <errno.h>

#define LOCAL_IP_ADDR		(0x7F000001)
#define LOCAL_TCP_PORT		(34567)

int main(void)
{
	struct sockaddr_in local, peer;
	int ret;
	char buf[128];
	int sock = socket(AF_INET, SOCK_STREAM, 0);

	memset(&local, 0, sizeof(local));
	memset(&peer, 0, sizeof(peer));

	local.sin_family = AF_INET;
	local.sin_port = htons(LOCAL_TCP_PORT);
	local.sin_addr.s_addr = htonl(LOCAL_IP_ADDR);

	peer = local;	

    int flag = 1;
    ret = setsockopt(sock, SOL_SOCKET, SO_REUSEADDR, &flag, sizeof(flag));
    if (ret == -1) {
        printf("Fail to setsocket SO_REUSEADDR: %s\n", strerror(errno));
        exit(1);
    }

	ret = bind(sock, (const struct sockaddr *)&local, sizeof(local));
	ret = connect(sock, (const struct sockaddr *)&peer, sizeof(peer));

	if (ret) {
		printf("Fail to connect myself: %s\n", strerror(errno));
		exit(1);
	}
	printf("Connect to myself successfully\n");

	strcpy(buf, "Hello, myself~");
	send(sock, buf, strlen(buf), 0);

	memset(buf, 0, sizeof(buf));
	recv(sock, buf, sizeof(buf), 0);

	printf("Recv the msg: %s\n", buf);
	close(sock);
	return 0;
}
```

其连接截图如下：

![img](https://pic4.zhimg.com/80/v2-0e6f0cb40fc5878c214a6397169aa5b3_720w.webp)

从截图中，可以看到TCP套接字成功的“连接”了自己，并发送和接收了数据包围。netstat的输出更证明了TCP的两端地址和端口是完全相同的。

原文地址：https://zhuanlan.zhihu.com/p/506042622

作者：CPP后端技术

# 【NO.594】一篇文章读懂dpdk——dpdk原理详解

## 1.DPDK特点

DPDK全称为Date planedevelopment kit，是一个用来进行包数据处理加速的软件库。与传统的数据包处理相比，DPDK具有以下特点：

\1) 轮询：在包处理时避免中断上下文切换的开销，

\2) 用户态驱动：规避不必要的内存拷贝和系统调用，便于快速迭代优化

\3) 亲和性与独占：特定任务可以被指定只在某个核上工作，避免线程在不同核间频繁切换，保证更多的cache命中

\4) 降低访存开销：利用内存大页HUGEPAGE降低TLB miss，利用内存多通道交错访问提高内存访问有效带宽

\5) 软件调优：cache行对齐，预取数据，多元数据批量操作

## 2.DPDK框架

![img](https://pic4.zhimg.com/80/v2-fe293ceb86fb21a3b39b065918e5f5e7_720w.webp)

图1 DPDK框图

在上图中，核心库Core Libs提供系统抽象、大页内存、缓存池、定时器及无锁环等基础组件。

PMD库，提供全用户态驱动，以便通过轮询和线程绑定得到极高网络吞吐，支持各种本地和虚拟网卡。

Classify库，支持精确匹配，最长匹配和通配符匹配，提供常用包处理的查表操作。

Qos库，提供网络服务质量相关组件，限速和调度。

Mellanox DPDK中保留了Linux内核态驱动，框图如下：

![img](https://pic4.zhimg.com/80/v2-61f413e987d620d9949a42c6c784265f_720w.webp)

图2 MellanoxDPDK框图

Mellanox DPDK在用户空间使用PMD驱动，与网卡之间有两条路径，控制路径使用user verbs，经过内核，用于对象的创建、初始化、修改、查询和释放。数据路径之间访问网卡，进行数据的收发。

Mellanox DPDK与传统的Linux内核态驱动可以共存，当前未被DPDK使用的端口可以通过Linux网络协议栈进行报文收发。

![img](https://pic2.zhimg.com/80/v2-6559edfae77641ebf0279ced3be03ab5_720w.webp)

图3 MellanoxDPDK与传统内核态驱动

## 3.Hugepage配置

DPDK中，如果有多个核可能需要同时访问一个网卡，那DPDK中会为每个核准备一个单独的接收队列/发送队列，这样避免了竞争，也避免了cache一致性问题。

一般的常规页大小为4K字节，使用大页时页大小设置为2M或1G字节。修改方式如下：

Bashrc文件内添加：

非NUMA系统

Echo 1024> /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages

预留了1024个2M大页，即2GB内存。

NUMA系统

Echo 1024>/sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages

Echo 1024>/sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages

上述命令为每个节点各预留了1024个2M大页，即4GB内存。

/etc/fstab内添加

挂载2M大页：Nodev /mnt/huge hugetlbfs defaults 0 0

挂载1G大页：Nodev /mnt/huge_1GB hugetlbfs pagesize=1GB 0 0

## 4.多线程配置

DPDK线程基于pthread接口创建，属于抢占式线程模型，受内核调度支配。通过在多核设备上创建多个线程，每个线程绑定到单独的核上，减少线程调度的开销，以提高性能。控制线程一般绑定到MASTER核上，接受用户配置，并传递配置参数给数据线程等；数据线程分布在不同核上处理数据包。

DPDK中初始化及执行任务分发示例如下图所示：

![img](https://pic2.zhimg.com/80/v2-0b5890c264fa87cf542cab9804eb5555_720w.webp)

图4 lcore初始化及执行任务分发

上图中一共使用了三个cpu核，master core负责进行任务初始化和分发，两个lcore执行任务。

## 5.转发模型

DPDK转发框架分为run tocompletion模型和pipeline模型，对比图如下：

![img](https://pic4.zhimg.com/80/v2-7ca8453946b03bcec05310837f2747c3_720w.webp)

图5 DPDK转发模型对比

DPDK run to completion模型中每个报文的生命周期只能在一个线程中出现，每个物理核都负责处理整个报文的生命周期从RX到TX。

DPDK pipeline模型中不同的工作交给不同的模块，每一个模块只单独处理特定的事务，各模块之间有输入输出，通过这些输入输出连接起来，完成复杂的网络功能。

## 6.内存管理

DPDK将内存封装在Mbuf结构体内，Mbuf主要用来封装网络帧缓存，所有的应用使用Mbuf结构来传输网络帧。对网络帧封装和处理时，将网络帧元数据和帧本身存放在固定大小的同一段缓存中，网络帧元数据的一部分内容由DPDK的网卡驱动写入，结构如下图：

![img](https://pic4.zhimg.com/80/v2-504628d4dffaa17e9633ae652d5c2513_720w.webp)

图6 单帧Mbuf结构

上图中，head room用来存储和系统中其他实体交互的信息，如控制信息、帧内容、事件等。Head room长度由RTE_PKTMBUF_HEADROOM定义，默认为128.

Rte_mbuf结构对象存放在内存池中，内存池使用环形缓存区来保存空闲对象，逻辑结构如下图所示：

![img](https://pic4.zhimg.com/80/v2-c1afca9627570b5ae3f7a00c8dab38bf_720w.webp)

图7 内存池双环形缓存区结构

当一个网络帧被网卡接收时，DPDK网卡驱动将其存储在一个高效的环形缓冲区中，同时Mbuf的环形缓存区中创建了一个Mbuf对象。Mbuf对象被创建好后，网卡驱动根据分析出的帧信息将其初始化，并将其和实际帧对象逻辑相连，对网络帧的分析处理都集中在Mbuf，仅在必要的时候访问实际网络帧。

## 7.性能优化

### **7.1 收发包流程概述**

DPDK中普遍采用纯轮询模式进行数据包收发，所有的收发包有关的中断在物理端口初始化的时候都会关闭。物理端口上的每一个收包队列，都会有一个对应的由收包描述符组成的软件队列来进行硬件和软件的交互。

DPDK驱动程序负责初始化好每一个收包描述符，其中包含把包缓冲内存块的物理地址填充到收包描述符对应的位置，并把对应的收包成功标志复位，通知网卡硬件把收到的包进行填充。网卡硬件将收到的包一一填充到对应的收包描述符表示的缓冲内存块中，将必要信息填充到收包描述符内，标记好收包成功标志。若一个缓冲区内存块大小不够存放一个完整数据包时，可能需要多个收包描述符来处理一个包。

针对每一个收包队列，DPDK都会有一个对应的软件线程负责轮询其中的收包描述符收包成功的标志位。当发现一个收包描述符的收包成功标志被硬件置位了，意味着有包进入网卡并且已经存储到描述符对应的缓冲区内存块中。驱动程序将解析相应的收包描述符，填充对应缓冲内存块头部，将收包缓冲内存块存放到收包函数提供的数组中，同时分配好一个新的缓冲内存块给这个描述符，以便下一次收包。

针对每一个发包队列，DPDK都有一个对应的软件线程负责设置需要发送出去的包，DPDK驱动程序负责提取发包缓冲内存块的有效信息，根据内存缓存块中包的内容来负责初始化好每一个发包描述符，驱动程序将每个包翻译为一个或者多个发包描述符内能够理解的内容，写入发包描述符。当驱动程序设置好相应的发包描述符，硬件就开始依据发包描述符内容来发包，驱动程序通过获取发包描述符内的RS发送状态位来决定发包是否结束以及对发包描述符和内存缓冲块进行回收。发包轮询就是轮询发包结束的硬件标志位，DPDK驱动程序会在发包线程内不断查询发包是否结束，网卡根据RS以写回方式通知发包结束。

### **7.2 Burst收发包**

Burst为一次完成多个数据包的收发，通过把收发包复杂的处理过程进行分解，打散成不同的相对较小的处理阶段，把相邻数据访问、相似的数据运算集中处理，尽可能减少对内存或者低一级的处理器缓存的访问次数。网卡收发包描述符一般为16字节或32字节，处理器缓存基本单位为64字节，可以存放2个或4个描述符，处理器预期机制会每次存取相邻多个缓存，若一次处理4个缓存单元，则可以更新16个描述符，Burst就可以设置为16.目前常用的Burst为32.

### **7.3 平台优化配置**

硬件：

PCIe Gen3 x 8,x16.

网卡多队列收发包，支持收包RSS时分发进多个队列。

软件：

关闭CPU以及设备的省电模式；

让内存运行在所支持的最高频率上；

内核初始化时设置好大页；

增加内核启动参数，预先指定DPDK使用的逻辑核，不被操作系统调度；

DPDK参数配置：

收包队列长度，即每个收包队列分配的收包描述符个数，反映了在软件驱动程序读取所收到包之前最大的缓存包的能力。太长占用资源，太短容易丢包，默认长度为128.

发包队列长度，即每个发包队列分配的发包描述符个数，默认长度为512.

收包队列可释放描述符数量阈值，DPDK驱动程序并没有每次收包都更新收包队列尾部索引寄存器，而是在可释放的收包描述符数量达到一个阈值时更新，设置合理，可以减少没有必要的过多收包队列尾部索引寄存器的访问。默认值为32.

发包队列可释放描述符数量阈值，通过将回写标记设置在一定间隔的发包描述符上，减少不必要的回写次数，改善性能，默认为32.

发包描述符释放阈值，当用来重新配置的发包描述符数量少于阈值时，才会启动描述符和Mbuf释放动作，设置太大释放频繁，影响性能，设置太小可用描述符少，容易丢包，默认值为32.

### **7.4 多队列配置**

DPDK中，通过将网卡的某个接收队列分配给某个核，从该队列中收到的所有报文都应当在该指定的核上处理结束，不同的核操作不同的队列。

目前可以通过RSS(接收方扩展)把数据包分配到不同的队列中，该机制根据关键字通过哈希函数计算出哈希值，再有哈希值确定队列。不同的数据包类型具有不同的关键字，例如IPv4 UDP四元组（源IP地址、目的IP地址、源端口号、目的端口号）。

另一种是Flow Director技术，根据包的字段精确匹配，查找Flow Director表项，将其分配到某个特定队列。

DPDK中可以采用RSS来负载均衡，将报文发到多个核上，同时使用Flow Director将控制报文分配到指定的队列上，使用单独的核来处理，如下图所示；

![img](https://pic2.zhimg.com/80/v2-fbf8782acc17403435c42db9c55c6ce1_720w.webp)

图8 DPDK中RSS和FDIR的应用

### **7.5 硬件加速功能**

VLAN Tag的插入和剥离由网卡硬件完成。

1）收包时VLAN tag过滤，网卡硬件端口设计了VLAN过滤表，无法在过滤表中匹配的VLAN包会被丢弃。

2）收包时VLAN tag剥离，网卡硬件会在硬件描述符中设置两个域，将需要的信息通知驱动软件，包含此包是否曾被剥离了VLAN tag以及被剥离的tag。

3）发包时VLAN tag插入，TPID值固定，可通过寄存器进行设置，TCL进行逐包设置。

TCP分片功能由硬件提供，将现有较大的TCP分片拆分为较小的TCP分片，仅需要复制TCP的包头，更新头里面的长度相关信息，重新计算校验和。

RSC组包功能针对TCP数据包，由硬件将拆分的TCP分片聚合成一个大的分片，从而减轻软件的处理。

原文地址：https://zhuanlan.zhihu.com/p/490946603

作者：CPP后端技术

# 【NO.595】深入理解无锁编程-译自《An Introduction to Lock-Free Programming》

今天介绍一下无锁编程基础知识，希望大家可以了解无锁编程基本原理。

![img](https://pic1.zhimg.com/80/v2-1a2c85236c0b7e873a4ce76ac5e58900_720w.webp)

无锁编程是一个挑战，不仅因为任务本身的复杂性，还因为从一开始就很难深入了解这个主题，因为该主题和底层技术（编译器，CPU，内存）息息相关，需要深厚底层功底。

我学习**无锁编程**是Bruce Dawson 出色而全面的白皮书Lockless Programming Considerations(无锁编程的思考)。和许多技术一样，需要将理论付诸实践，在平台上开发和调试无锁代码。

在这篇文章中，我想重新介绍无锁编程，首先是定义它，然后将大部分信息提炼为几个关键概念。我将使用流程图展示这些概念如何相互关联，然后我们将深入研究细节。至少，任何从事无锁编程的程序员都应该已经了解如何使用互斥锁和其他高级同步对象（如信号量和事件）编写正确的多线程代码。

## 1.**它是什么？**

人们通常将无锁编程描述为没有互斥锁的编程，互斥锁也称为锁。这是真的，但这只是故事的一部分。基于学术文献的普遍接受的定义更广泛一些。从本质上讲，无锁是一种用于描述某些代码的属性，而无需过多说明该代码的实际编写方式。

基本上，如果您的程序的某些部分满足以下条件，那么该部分可以理所当然地被认为是无锁的。相反，如果代码的给定部分不满足这些条件，则该部分不是无锁的。

![img](https://pic1.zhimg.com/80/v2-f581ac6a167a61f3b30c4dac09f64f24_720w.webp)

从这个意义上说，**无锁**中的锁并不直接指互斥锁，而是指以某种方式“锁定”整个应用程序的可能性，无论是死锁、活锁——甚至是由于由你最大的敌人。最后一点听起来很有趣，但这是关键。共享互斥锁被简单地排除在外，因为一旦一个线程获得互斥锁，您最大的敌人就再也不会调度该线程了。当然，真正的操作系统不是这样工作的——我们只是定义术语。

这是一个不包含互斥锁但仍然不是无锁的操作的简单示例。最初，X = 0。作为读者的练习，考虑如何以一种方式调度两个线程，使得两个线程都不退出循环。

```text
while(X == 0 ) { 
    X = 1 - X; 
}
```

没有人期望大型应用程序是完全无锁的。通常，我们从整个代码库中识别出一组特定的无锁操作。例如，在一个无锁队列中，有可能是无锁的操作，比如极少数的`push`，`pop`也许`isEmpty`等。

![img](https://pic4.zhimg.com/80/v2-e95b9ae5d6f795a225a53b42c332e463_720w.webp)

Herlihy & Shavit 是**The Art of Multiprocessor Programming**（多处理器编程的艺术） 的作者，倾向于将此类操作表示为类方法，并提供以下无锁的简洁定义：

“在无限执行中，某些方法调用会无限频繁地结束”

换句话说，只要程序能够继续调用那些无锁操作，无论发生什么，完成的调用次数都会不断增加。在这些操作期间，系统在算法上不可能锁定。

**无锁编程**的一个重要结论是，如果您挂起单个线程，它永远不会阻止其他线程作为一个组通过它们自己的无锁操作取得进展。**这暗示了在编写中断处理程序和实时系统时无锁编程的价值**，其中某些任务必须在一定的时间限制内完成，无论程序的其余部分处于什么状态。

最后一个说明：某些操作被设计为阻塞的并不意味是这就不是Lock-Free的。例如，当队列为空时，队列的弹出操作可能会故意阻塞。其余的代码路径仍然可以被认为是无锁的。

## 2.**无锁编程技术**

事实证明，当您尝试满足无锁编程的非阻塞条件时，会出现一整套技术：原子操作、内存屏障、避免 ABA 问题，仅举几例。这就是事情很快变得邪恶的地方。

那么这些技术如何相互关联呢？为了说明，我整理了以下流程图。下面我将逐一详述。

![img](https://pic4.zhimg.com/80/v2-bed6a67daa39710df5af769f3ddd1e77_720w.webp)

## 3.**原子读-修改-写操作**

**原子操作**是以一种看起来不可分割的方式操作内存的操作：没有线程可以观察到半完成的操作。在现代处理器上，许多操作已经是原子的。例如，简单类型的对齐读取和写入通常是原子的。

![img](https://pic3.zhimg.com/80/v2-b935a487339626f459c5b2060caff342_720w.webp)

读-修改-写(RMW) 操作更进一步，允许您以原子方式执行更复杂的事务。当无锁算法必须支持多个写入器时，它们特别有用，因为当多个线程在同一地址上尝试 RMW 时，它们将有效地排成一行并一次执行这些操作。我已经在这篇博客中谈到了 RMW 操作，例如实现轻量级互斥锁、递归互斥锁和轻量级日志系统时。

> RMW 操作的示例包括_InterlockedIncrementWin32、OSAtomicAdd32iOS 和std::atomic<int>::fetch_addC++11。请注意，C++11 原子标准并不能保证实现在每个平台上都是无锁的，因此最好了解您的平台和工具链的功能。你可以使用std::atomic<>::is_lock_free确认一下。

不同的 CPU 系列以不同的方式支持 RMW。诸如 PowerPC 和 ARM 之类的处理器公开了load-link/store-conditional）条件指令，这有效地允许您在低级别实现自己的 RMW 原语，尽管这并不常见。常见的 RMW 操作通常就足够了。

如流程图所示，即使在单处理器系统上，原子 RMW 也是无锁编程的必要部分。如果没有原子性，线程可能会在事务中途中断，从而可能导致状态不一致。

**Compare-And-Swap Loops**

也许最常讨论的 RMW 操作是**compare-and-swap(CAS)**。在 Win32 上，CAS 是通过一系列内在函数提供的，例如`_InterlockedCompareExchange`. 通常使用 CAS Loops 来完成对事务的原子处理：

```text
void LockFreeQueue::push(Node* newHead)
{
    for (;;)
    {
        // Copy a shared variable (m_Head) to a local.
        Node* oldHead = m_Head;

        // Do some speculative work, not yet visible to other threads.
        newHead->next = oldHead;

        // Next, attempt to publish our changes to the shared variable.
        // If the shared variable hasn't changed, the CAS succeeds and we return.
        // Otherwise, repeat.
        if (_InterlockedCompareExchange(&m_Head, newHead, oldHead) == oldHead)
            return;
    }
}
```

这样的循环仍然符合无锁的条件，因为如果一个线程的测试失败，则意味着它必须在另一个线程上成功——尽管某些架构提供了CAS的较弱变体，而这不一定是真的。每当实现 CAS 循环时，必须特别注意避免ABA 问题。

## 4.**顺序一致性**

**顺序一致性**是指所有线程都同意内存操作发生的顺序，并且该顺序与程序源代码中的操作顺序一致。

实现顺序一致性的一种简单（但显然不切实际）的方法是禁用编译器优化并强制所有线程在单个处理器上运行。处理器永远不会看到它自己的内存效果出问题，即使线程在任意时间被抢占和调度。

一些编程语言即使对于在多处理器环境中运行的优化代码也提供顺序一致性。在 C++11 中，您可以将所有共享变量声明为具有默认内存排序约束的 C++11 原子类型。在 Java 中，您可以将所有共享变量标记为`volatile`. 这是我上一篇文章中的示例，以 C++11 风格重写：

```text
std::atomic< int > X( 0 ), Y( 0 );
int r1, r2;

void thread1()
{ 
    X.store( 1 ); 
    r1 = Y.load();
}

void thread2()
{ 
    Y.store( 1 ); 
    r2 = X.load(); 
}
```

因为 C++11 原子类型保证顺序一致性，结果 r1 = r2 = 0 是不可能的。为了实现这一点，编译器会在幕后输出额外的指令——通常是内存栅栏和/或 RMW 操作。与程序员直接处理内存排序的指令相比，这些附加指令可能会降低实现的效率。

## 5.**内存排序**

正如流程图所暗示的那样，任何时候您对多核（或任何对称多处理器）进行**无锁编程**，并且您的环境不保证顺序一致性，您必须考虑如何防止内存重新排序。

在当今的体系结构中，强制执行正确内存排序的工具通常分为三类，它们可以防止编译器重新排序和处理器重新排序：

- 轻量级同步或栅栏指令；
- 一个完整的内存栅栏指令；
- 提供获取或释放语义的内存操作。

获取语义防止在程序顺序中跟随它的操作的内存重新排序，并且释放语义防止在它之前的操作的内存重新排序。这些语义特别适用于存在生产者/消费者关系的情况，即一个线程发布一些信息而另一个线程读取它。

## 6.**不同的处理器有不同的内存模型**

不同的 CPU 系列在内存重新排序方面有不同的习惯。这些规则由每个 CPU 供应商记录，并由硬件严格遵守。例如，PowerPC 和 ARM 处理器可以更改相对于指令本身的内存存储顺序，但通常情况下，Intel 和 AMD 的 x86/64 系列处理器不会。我们说前者的处理器具有更宽松的内存模型。

人们很容易抽象出这些特定于平台的细节，尤其是 C++11 为我们提供了一种编写可移植无锁代码的标准方法。但是目前，我认为大多数无锁程序员至少对平台差异有一些了解。如果要记住一个关键区别，那就是在 x86/64 指令级别，每次从内存加载都带有获取语义，并且每次存储到内存都提供释放语义——至少对于非 SSE 指令和非写组合内存. 因此，过去常常编写能在x86/64 上运行成功但在其他处理器上失败的无锁代码。

如果你对处理器需要内存排序的硬件细节感兴趣，我推荐附录的并行编程困难吗？ 请记住在任何情况下,由于编译器指令重排序也会导致内存重新排序。

在这篇文章中，我没有过多地谈论无锁编程的实际方面，例如：我们什么时候做？我们真正需要多少？我也没有提到验证无锁算法的重要性。尽管如此，我希望对于一些读者来说，这篇介绍已经提供了对无锁概念的基本熟悉，因此您可以继续深入阅读其他文章而不会感到太困惑。

原文地址：https://zhuanlan.zhihu.com/p/502580027

作者：CPP后端技术

# 【NO.596】网络编程：线上大量CLOSE_WAIT的原因深入分析

**这一次重启真的无法解决问题了：一次 MySQL 主动关闭，导致服务出现大量 CLOSE_WAIT 的全流程排查过程。**

近日遇到一个线上服务 **socket** 资源被不断打满的情况。通过各种工具分析线上问题,定位到问题代码。这里对该问题发现、修复过程进行一下复盘总结。

先看两张图。一张图是服务正常时监控到的 **socket** 状态，另一张当然就是异常啦！

![img](https://pic2.zhimg.com/80/v2-8f3018ad4fe06866dca6ba4e0a4d75cd_720w.webp)

**图一：正常时监控**

![img](https://pic2.zhimg.com/80/v2-8fb3e0f1f315cf82c45a8c4d68166c75_720w.webp)

**图二：异常时监控**

从图中的表现情况来看，就是从 **04:00** 开始，socket 资源不断上涨，每个谷底时重启后恢复到正常值，然后继续不断上涨不释放，而且每次达到峰值的间隔时间越来越短。

重启后，排查了日志，没有看到 **panic** ，此时也就没有进一步检查，真的以为重启大法好。

## 1.情况说明

该服务已经上线正常运行将近一年，提供给其它服务调用，主要底层资源有DB/Redis/MQ。

为了后续说明的方便，将服务的架构图进行一下说明。

![img](https://pic2.zhimg.com/80/v2-682793da0ae7bbd23d9b5e0c565e4a55_720w.webp)

**图三：服务架构**

架构是非常简单。 问题出现在早上 **08:20** 左右开始的，报警收到该服务出现 **504**，此时第一反应是该服务长时间没有重启（快两个月了），可能存在一些内存泄漏，没有多想直接进行了重启。也就是在图二第一个谷底的时候，经过重启服务恢复到正常水平（重启真好用，开心）。

将近 **14:00** 的时候，再次被告警出现了 **504** ，当时心中略感不对劲，但由于当天恰好有一场大型促销活动，因此先立马再次重启服务。直到后续大概过了1小时后又开始告警，连续几次重启后，发现需要重启的时间间隔越来越短。此时发现问题绝不简单。**这一次重启真的解决不了问题老**，因此立马申请机器权限、开始排查问题。下面的截图全部来源我的重现demo，与线上无关。

## 2.发现问题

出现问题后，首先要进行分析推断、然后验证、最后定位修改。根据当时的表现是分别进行了以下猜想。

**推断一**

> socket 资源被不断打满，并且之前从未出现过，今日突然出现，**怀疑是不是请求量太大压垮服务**

经过查看实时 **qps** 后，放弃该想法，虽然量有增加，但依然在服务器承受范围（远远未达到压测的基准值）。

**推断二**

> 两台机器故障是同时发生，重启一台，另外一台也会得到缓解，作为独立部署在两个集群的服务非常诡异

有了上面的的依据，推出的结果是肯定是该服务依赖的底层资源除了问题，要不然不可能独立集群的服务同时出问题。

由于监控显示是 **socket** 问题，因此通过 **netstat** 命令查看了当前tcp链接的情况（本地测试，线上实际值大的多）

```text
/go/src/hello # netstat -na | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
LISTEN 2 
CLOSE_WAIT 23 # 非常异常 
TIME_WAIT 1
```

发现绝大部份的链接处于**CLOSE_WAIT**状态，这是非常不可思议情况。然后用netstat -an命令进行了检查。

![img](https://pic4.zhimg.com/80/v2-1f548421104f9df1fc5e378b11501973_720w.webp)

**图四：大量的CLOSE_WAIT**

> CLOSED 表示socket连接没被使用。 LISTENING 表示正在监听进入的连接。 SYN_SENT 表示正在试着建立连接。 SYN_RECEIVED 进行连接初始同步。 ESTABLISHED 表示连接已被建立。 CLOSE_WAIT 表示远程计算器关闭连接，正在等待socket连接的关闭。 FIN_WAIT_1 表示socket连接关闭，正在关闭连接。 CLOSING 先关闭本地socket连接，然后关闭远程socket连接，最后等待确认信息。 LAST_ACK 远程计算器关闭后，等待确认信号。 FIN_WAIT_2 socket连接关闭后，等待来自远程计算器的关闭信号。 TIME_WAIT 连接关闭后，等待远程计算器关闭重发。

然后开始重点思考为什么会出现大量的mysql连接是**CLOSE_WAIT**呢？为了说清楚，我们来插播一点TCP的四次挥手知识。

**TCP四次挥手**

我们来看看**TCP**的四次挥手是怎么样的流程：

![img](https://pic2.zhimg.com/80/v2-a793ed2e4cb23af8c34d4394732c995d_720w.webp)

用中文来描述下这个过程：

Client: 服务端大哥，我事情都干完了，准备撤了，这里对应的就是客户端发了一个**FIN**

Server：知道了，但是你等等我，我还要收收尾，这里对应的就是服务端收到 **FIN** 后回应的 **ACK**

经过上面两步之后，服务端就会处于 **CLOSE_WAIT** 状态。过了一段时间 **Server** 收尾完了

Server：小弟，哥哥我做完了，撤吧，服务端发送了**FIN**

Client：大哥，再见啊，这里是客户端对服务端的一个 **ACK**

到此服务端就可以跑路了，但是客户端还不行。为什么呢？客户端还必须等待 **2MSL** 个时间，这里为什么客户端还不能直接跑路呢？主要是为了防止发送出去的 **ACK** 服务端没有收到，服务端重发 **FIN** 再次来询问，如果客户端发完就跑路了，那么服务端重发的时候就没人理他了。这个等待的时间长度也很讲究。

> **Maximum Segment Lifetime**报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃

这里一定不要被图里的 **client／server** 和项目里的客户端服务器端混淆，你只要记住：主动关闭的一方发出 **FIN** 包（Client），被动关闭（Server）的一方响应 **ACK** 包，此时，被动关闭的一方就进入了 **CLOSE_WAIT** 状态。如果一切正常，稍后被动关闭的一方也会发出 **FIN** 包，然后迁移到 **LAST_ACK** 状态。 既然是这样，**TCP**抓包分析下：

```text
/go # tcpdump -n port 3306
# 发生了 3次握手
11:38:15.679863 IP 172.18.0.5.38822 > 172.18.0.3.3306: Flags [S], seq 4065722321, win 29200, options [mss 1460,sackOK,TS val 2997352 ecr 0,nop,wscale 7], length 0

11:38:15.679923 IP 172.18.0.3.3306 > 172.18.0.5.38822: Flags [S.], seq 780487619, ack 4065722322, win 28960, options [mss 1460,sackOK,TS val 2997352 ecr 2997352,nop,wscale 7], length 0

11:38:15.679936 IP 172.18.0.5.38822 > 172.18.0.3.3306: Flags [.], ack 1, win 229, options [nop,nop,TS val 2997352 ecr 2997352], length 0

# mysql 主动断开链接
11:38:45.693382 IP 172.18.0.3.3306 > 172.18.0.5.38822: Flags [F.], seq 123, ack 144, win 227, options [nop,nop,TS val 3000355 ecr 2997359], length 0 # MySQL负载均衡器发送fin包给我

11:38:45.740958 IP 172.18.0.5.38822 > 172.18.0.3.3306: Flags [.], ack 124, win 229, options [nop,nop,TS val 3000360 ecr 3000355], length 0 # 我回复ack给它

... ... # 本来还需要我发送fin给他，但是我没有发，所以出现了close_wait。那这是什么缘故呢？
```

> **src > dst: flags data-seqno ack window urgent options** src > dst 表明从源地址到目的地址 flags 是TCP包中的标志信息,S 是SYN标志, F(FIN), P(PUSH) , R(RST) "."(没有标记) data-seqno 是数据包中的数据的顺序号 ack 是下次期望的顺序号 window 是接收缓存的窗口大小 urgent 表明数据包中是否有紧急指针 options 是选项

结合上面的信息，我用文字说明下：**MySQL负载均衡器** 给我的服务发送 **FIN** 包，我进行了响应，此时我进入了 **CLOSE_WAIT** 状态，但是后续作为被动关闭方的我，并没有发送 **FIN**，导致我服务端一直处于 **CLOSE_WAIT** 状态，无法最终进入 **CLOSED** 状态。 那么我推断出现这种情况可能的原因有以下几种：

**1、负载均衡器**异常退出了，

> 这基本是不可能的，他出现问题绝对是大面积的服务报警，而不仅仅是我一个服务

2、**MySQL负载均衡器**的超时设置的太短了，导致业务代码还没有处理完，**MySQL负载均衡器**就关闭tcp连接了

> 这也不太可能，因为这个服务并没有什么耗时操作，当然还是去检查了负载均衡器的配置，设置的是60s。

3、代码问题，**MySQL**连接无法释放

> 目前看起来应该是代码质量问题，加之本次数据有异常，触发到了以前某个没有测试到的点，目前看起来很有可能是这个原因

## 3.查找错误原因

由于代码的业务逻辑并不是我写的，我担心一时半会看不出来问题，所以直接使用perf把所有的调用关系使用火焰图给绘制出来。既然上面我们推断代码中没有释放mysql连接。无非就是：

1. 确实没有调用close
2. 有耗时操作（火焰图可以非常明显看到），导致超时了
3. mysql的事务没有正确处理，例如：rollback 或者 commit

由于火焰图包含的内容太多，为了让大家看清楚，我把一些不必要的信息进行了折叠。

![img](https://pic4.zhimg.com/80/v2-d174693ee7adbcfb27d19aa1a092b053_720w.webp)

**图五：有问题的火焰图**

火焰图很明显看到了开启了事务，但是在余下的部分，并没有看到**Commit**或者是**Rollback**操作。这肯定会操作问题。然后也清楚看到出现问题的是：

**MainController.update**方法内部，话不多说，直接到 update 方法中去检查。发现了如下代码：

```text
func (c *MainController) update() (flag bool) {
	o := orm.NewOrm()
	o.Using("default")
	
	o.Begin()
	nilMap := getMapNil()
	if nilMap == nil {// 这里只检查了是否为nil，并没有进行rollback或者commit
		return false
	}

	nilMap[10] = 1
	nilMap[20] = 2
	if nilMap == nil && len(nilMap) == 0 {
		o.Rollback()
		return false
	}

	sql := "update tb_user set name=%s where id=%d"
	res, err := o.Raw(sql, "Bug", 2).Exec()
	if err == nil {
		num, _ := res.RowsAffected()
		fmt.Println("mysql row affected nums: ", num)
		o.Commit()
		return true
	}

	o.Rollback()
	return false
}
```

至此，全部分析结束。经过查看**getMapNil**返回了nil，但是下面的判断条件没有进行回滚。

```text
if nilMap == nil {
    o.Rollback()// 这里进行回滚
    return false
}
```

## 4.总结

整个分析过程还是废了不少时间。最主要的是主观意识太强，觉得运行了一年没有出问题的为什么会突然出问题？因此一开始是质疑 SRE、DBA、各种基础设施出了问题（人总是先怀疑别人）。导致在这上面费了不少时间。

理一下正确的分析思路：

1、出现问题后，立马应该检查日志，确实日志没有发现问题；

2、监控明确显示了socket不断增长，很明确立马应该使用 netstat 检查情况看看是哪个进程的锅；

3、根据 netstat 的检查，使用 tcpdump 抓包分析一下为什么连接会**被动断开**（TCP知识非常重要）；

4、如果熟悉代码应该直接去检查业务代码，如果不熟悉则可以使用 perf 把代码的调用链路打印出来；

5、不论是分析代码还是火焰图，到此应该能够很快定位到问题。 那么本次到底是为什么会出现**CLOSE_WAIT**呢？大部分同学应该已经明白了，我这里再简单说明一下：

由于那一行代码没有对事务进行回滚，导致服务端没有主动发起close。因此 **MySQL负载均衡器** 在达到 60s 的时候主动触发了close操作，但是通过tcp抓包发现，服务端并没有进行回应，这是因为代码中的事务没有处理，因此从而导致大量的端口、连接资源被占用。在贴一下挥手时的抓包数据：

```text
# mysql 主动断开链接
11:38:45.693382 IP 172.18.0.3.3306 > 172.18.0.5.38822: Flags [F.], seq 123, ack 144, win 227, options [nop,nop,TS val 3000355 ecr 2997359], length 0 # MySQL负载均衡器发送fin包给我
11:38:45.740958 IP 172.18.0.5.38822 > 172.18.0.3.3306: Flags [.], ack 124, win 229, options [nop,nop,TS val 3000360 ecr 3000355], length 0 # 我回复ack给它
```

希望此文对大家排查线上问题有所帮助。为了便于帮助大家理解，下面附上正确情况下的火焰图与错误情况下的火焰图。大家可以自行对比。

![img](https://pic4.zhimg.com/80/v2-1b24d965d1ed017dc0b167d920ae7267_720w.webp)

正确的火焰图

![img](https://pic1.zhimg.com/80/v2-d9584323cde779a43079c884260f13bc_720w.webp)

错误的火焰图

这里提出两个思考题，我觉得非常有意义，大家自己思考下：

1. 为什么一台机器几百个 **CLOSE_WAIT** 就导致不可继续访问？我们不是经常说一台机器有 **65535** 个文件描述符可用吗？
2. 为什么我有负载均衡，而两台部署服务的机器确几乎同时出了 **CLOSE_WAIT** ?

原文地址：https://zhuanlan.zhihu.com/p/493915289

作者：cpp后端技术

# 【NO.597】记录一次腾讯c/c++ linux后台开发岗面试经历（面试题含答案）

腾讯c/c++ linux后台开发社招面试流程虽然因人而异，但就个人感受而言，腾讯的面试从考察内容方面来讲，还是有一定的共通性，这一点，可以从网上众多的面经可以看出。

腾讯的面试的流程持续一个多月

第一轮和第二轮电话面试，技术面；

第三轮和第四轮现场笔试+面试，技术面；

第六轮HR面；

最后offer call；

**第一面**

时间晚八点半，电话面，开始项目介绍，技术难点、实现细节。大概说了20分钟左右，可能是项目架构比较复杂，口述不是特别形象，又聊了10分钟，貌似双方谁也没说服谁。开始聊计算机网络、操作系统。内容包括TCP拥塞控制算法、TCP和UDP区别、进程和线程区别等等。都是常规的题目。这一块已经很好的复习过。没问题。时间过去50分钟，面试官总结网络协议这一块不错，原理和项目实战比较强，会有下一面。

**第二面**

距离一面隔了六天。电话面，大概20分钟左右。内容如下，不分先后：

Q1. 构造函数可以是虚函数吗？

A：不可以。原因有两点：

构造对象的时候，必须知道对象的实际类型。而虚函数行为是在运行期间确定实际类型的，在构造对象的时，对象还没有构造成功，编译器无法知道对象的实际类型是该类本身还是其派生类。

虚函数的运行依赖于虚函数指针，而虚函数指针在构造函数中进程初始化，让它指向正确的虚函数表，而在对象构造期间，虚函数指针还未构造完成。

Q2. 网络字节序是大端序还是小端序？

A：大端序。

Q3. Linux中如何创建进程以及创建进程后如何区分子进程？

A：使用fork()调用创建子进程，fork()调用返回两个值，大于0的表示父进程，等于0的表示子进程。

Q4. fork创建的子进程继承了父进程哪些内容

A：子进程继承了父进程的地址空间，打开的文件描述符等。

Q5. fork创建的子进程继承了父进程打开的文件描述符，如何让这种继承不发生

A：可以在打开文件的时候，设置FD_CLOSEXEC标志位，如果文件描述符中这个标志位置位，那么调用exec时会自动关闭对应的文件。

Q6. c++虚函数原理

A：虚函数是依赖于虚函数指针实现，每个拥有虚函数的类都有一个虚表，类的对象存在一个虚函数指针，指向实际类型的虚表。虚函数运行的时候，会根据虚函数指针找到正确的虚表，从而执行正确的虚函数。

Q7. c++多态的实现

A：多态分为两种，一种是运行时的多态，一种是编译时的多态。前者称为动态绑定，后者称为静态绑定。动态绑定时由虚函数来实现，静态绑定是由函数重载来实现。

Q8. c++ vector和list的区别？

A：vector是动态数组，会动态进行分配内存，进行扩容操作。list是双向链表。

Q9. 访问vector的迭代器的时候可以删减元素吗？list呢

A：任何对vector的修改都将导致vector的迭代器失效。list因为是双向链表，所以不会失效。

Q10. c++ vector的底层实现原理

A：vector底层是基于动态数组实现。

Q11. c++ map的底层实现

A：map的底层实现是基于红黑树的。

Q12. 红黑树的特点以及常见的二叉平衡树

A：红黑树性能比较高，插入删除时间复杂度保持在logn。和AVL相比，要求不是那么严格，它只要求到叶节点的最长路径不超过最短路径的两倍。相比之下，AVL要求左右子树相差高度不超过一，简单的插入或者删除都会导致树的不平衡需要旋转操作。

(我只说这么多，后来的同学建议说下红黑树的五个特点)

Q13. c++空类的sizeof大小

A：c++空类的大小为1，如果含有虚函数的话，大小为指针的大小。32位系统指针大小是4。c++要求即使是空类，对象也要有存储空间。含有虚函数的空类则是因为对象由虚函数指针。

Q14. 快速排序的时间复杂度

A：快速排序平均时间复杂度位nlogn，最差O(n^2)

Q15. nLogn是排序最好的时间复杂度吗？

A：不是，还有O(n)的算法，比如说基数排序。

Q16. 基数排序的原理以及应用

A：基数排序根据一个数的高低位进行排序。应用不知道，缺点是对负数的处理不太好。

Q17. 介绍负载均衡的应用

A：不知道。

Q18. http协议有用过吗？

A：这个没用过。

Q19. protobuf协议

A：我们公司之前使用的是json协议，没有用过protobuf协议。

Q20. 数据库

A：没用过。

Q21. redis

A：这个是自己学习使用的，生产环境没用过(这里说了下，我用它干嘛的，使用python的flask web框架基于redis的list结构开发一个网络聊天程序)

Q22. 解释线程安全和可重入函数

A：这个是关于多线程访问的吗？可重入函数这个名词听过，但是忘记干啥了，面试官说pass.

(一个可重入函数被称为可重入的，表明该函数被重入之后，不会产生任何不良后果，一个函数被重入，表示这个函数没有执行完成，由于内部原因或外部原因调用，又一次进入该函数执行)

Q23. top的命令cache和buffer区别

A：这个平常没有关注过。(buffer是块设备的读写缓冲区，比如磁盘，cache是文件系统的缓存，常用于文件)

Q24. 常见Linux命令是否用过，比如strace和netstat

A：这个有用过，strace用来跟踪程序的执行，top查看内存，以及tcpdump等进行抓包等等。

Q25. 多个动态库的连接顺序有区别吗，顺序怎么排

A：有顺序的，如果顺序错误的话可能导致编译失败。这里的编译顺序应该是被依赖的库放在后面。(这个我之前工作中遇到这个坑，所以知道。不过当时面试的时候，把顺序忘了)

Q26. 智力题，100本书，两个人轮流拿，每次拿1~5本，你先拿，有没有啥策略可以保证你可以拿到最后一本？

A：这个得承认，智力题不是我的强项，每次遇到的话都要很久才思考明白，当时考虑了大概30秒，想着100本这么多，要不先来10本试试，我一本，他一本，我一本…挣扎了几秒，老实承认，没想到啥好方法。

(此题解法可以是先手拿4本，后续双方每次拿6的倍数，这样可以保证最终可以拿到最后一本)。

**第三面**

现场面，时间下午2点，给了一张试卷，1.5小时完成。如果难度分成5个等级，感觉介于2和3之间。试卷内容保密。总之，考察比较基础也比较全面，C++，操作系统，计算机网络，算法和数据结构。

3点半，两个面试官。对着简历介绍项目，细节，技术难点，架构设计等等。接下来考察软件工程原理。这部分只给出题目。

1、Linux互斥锁里面递归锁和非递归锁的使用方式，返回值。

2、Golang Map是否是线程安全，如何设计一个无锁保护的Map(可能答案是使用CAS)。

3、程序的地址空间分布。

4、介绍Linux内存管理机制、涉及到的算法。

5、设计一个内存池。

6、设计一个定时器

7、解释时间轮

8、Golang里面CGO原理

9、awk和sed是啥。咋用。

10、TCP粘包怎么解决。

11、设计线程池。

12、Golang defer语句调用顺序。

13、TIME_WAIT状态有啥用。

14、画出四次挥手原理图。

15、define和inline区别。

16、定义常量指针和指针常量。

17、accept是在三次握手哪一次。

18、backlog作用。

**第四面**

一篇介绍memcpy的标准库函数的博客，让我优化。当时觉得，标准库里的函数咋优化，能优化标准库自己不就优化了，哪轮到我优化？？？但人家让优化，肯定有优化空间，硬着头皮上。对，就是下面这个函数，看了将近一分钟，咋看都觉得这个函数很完美。觉得一个一个复制已经很好了，还能咋办。问了下，能否给点提示，答曰，不急，慢慢来。又想了一会儿，还是没啥思路，这时候，总监来了一句，“你不觉得一个一个复制有点慢吗”。

到了这时候，还是没啥反应，不过既然嫌弃一个一个慢，那两个两个复制可以么？问了下两个两个复制算不算优化，答曰：算。于是想到使用一个short类型复制。后来总监看我有点思路，就提示我说，从汇编角度来优化，包括寄存器大小等等。

然后，我使用long long类型来复制，即每次8个。手写了实现，然后总监问我，如果地址不对齐咋办？到这一步，当时没想到地址对齐这个问题，没有答上来，后来想了下，即使没对齐，可以先一个一个复制等对齐了再每次复制8个。

当时对齐问题没啥思路就pass了。感觉面试官水平很高，这道问题的水准也很高。关于优化以及编译的问题，事后查了相关资料，这里推荐两本书：《深入理解计算机系统》、《程序员的自我修养—链接、装载与库》。对于优化的原理、编译器原理讲解的非常好。

```text
void *memmove (void *dest, const void *src, size_t len)
{
char *d = dest;
const char *s = src;
if (d < s)
while (len--)
      *d++ = *s++;
else
    {
char *lasts = s + (len-1);
char *lastd = d + (len-1);
while (len--)
        *lastd-- = *lasts--;
    }
return dest;
}
```

继续做题。

N个M长度数组求交集，求最优解并给出时间复杂度和空间复杂度。我给出的方案是归并、去重、全排序。然后问了下为啥用归并排序，于是介绍快排、归并、堆排各自优缺点和使用场景。然后问有没有更好的解法。想了下，给出了优化方案，仍然是归并，去重，然后hash。然后过。

第三题，c++手写单例模式，《剑指offer》第二题。

因为简历上有些会shell开发，然后给出第四题：

给定一个日志文件，每行包括日期，IP地址，错误码。然后让我使用shell搜索指定日期，指定IP，指定错误的日志出现次数。简单题，grep -rns “date + IP + error” | wc -l

看我的答案之后，在这个题目加了下扩展，除了前述三个内容，还有第四列content，每一行都不一样。马上给了答案：

awk -F"|" “print112 $3” | grep -rns “date + IP + error” | wc -l

应该是考察我awk的使用。这里提醒各位同学，自称会shell开发一定要掌握awk和sed的使用。基本上，面试官看到自称会shell必考。

整个面试流程持续四个小时

**第五面**

距离三面和四面隔了7天，期间没有任何消息，以为凉了。结果来了电话，约复试。

复试的内容没有特殊之处，依旧是基础。内容如下：

1、介绍前公司产品的功能、市场竞争力、自己负责的部分、产品的软件架构

简要介绍下前公司的产品、功能、市场竞争力，虽然咱是搞技术的，但是关于产品市场的事情，平常工作中还是多多少少有所关注。此外，就是介绍产品的软件架构图，以及自己负责的模块等等。很多同学遇到类似的问题，会说，自己只负责一个部分，不知道产品的整体架构。这样回答，往往给人的感觉就是很水，搞技术的，不会可以，但是不会可以学啊，虽然，整体架构没涉及过，但是，没吃过猪肉不能没见过猪跑。做了那么久，产品啥架构还不了解，这就有点说不过去了。所以，建议各位同学平常工作过程中，有空的话也从整体的角度思考下产品，万一哪天需要自己搞呢。

2、路由NAT如何实现

这一块没有接触过。画了一个映射图，大概描述了一下。

3、考察结构体字节对齐问题

4、概率题，两个红球一个白球，三个盒子。问第二个盒子至少一个红球的概率

考察概率论。刚开始拿到题，以为需要算法。想了一会儿没啥好方法，略显尴尬的说，使用枚举吧。所谓暴力法。画了一个多叉树的图，一目了然。事后想来，枚举才是最简单的方法。

5、编程题，字符串去空格

6、进程、线程区别。为什么有了多线程还是用多进程

基本的操作系统原理题。后面的问题只说了进程地址空间有限，不能创建任意多的线程。回来想了下，更好的回答应该是，进程是一个程序实体，多个程序需要多个进程。此外，进程地址空间相互隔离，安全。

7、平时如何定位问题，core dump怎么产生的

通常线上问题先复现、再定位。core dump怎么产生不知道。后来查了下，当程序有未处理的信号时，内核会自动生成core dump。能否生成core dump还有ulimit -c的限制。

8、构造函数调用虚函数可不可以

不可以。说了下虚函数的作用以及构造函数的调用顺序。后面又问，如果调用会出现啥情况。因为不确定所以说不知道。回来测试了下，是调用被调对象的自己的版本。

9、算法题：给定电话号码加区号，如何快速查找对应地区

区号占4位。给了长度10000的数组做hash，O(1)。问有没有办法提高空间利用率，没想到。想到的同学告诉我。然后给了红黑树存储，时间复杂度O(logN)太高。

10、常用的IDE

C语言使用source insight，Golang使用liteide。

11、线程调度问题

这里解释了下两种调度类以及它们的调度策略，然后是底层的实现原理，后来问调度过程的具体步骤，我记得内核的调度函数是schedule()，具体的操作没有分析过，这里只说了大概，保存寄存器、上下文，然后加载被调进程的上下文、寄存器等等。

12、不同编译器编译的库能否混用

没用过，不知道。后来查了下，不同版本的编译器编译出来库不能混用，不过网上的回答也没有解释清楚。觉得可能在问extern "C"问题。

13、离职原因，离职这么久都干嘛去了

14、阅读源码有啥好处？对以前的项目有没有啥改进之处？

15、有什么问题想问的

问了下直播业务涉及的终端以及后台开发过程中使用到哪些中间件技术。然后，问了下腾讯的服务器内核啥样子，回答说是定制的T-Linux内核。

整体感觉面试愉快，面试官也在考察知识深度，不会的也没关系。

**第六面**

距离复试三天时间，中午电话。

1、为什么学些德语

2、第一份工作学习到啥

3、对腾讯怎么看

4、期望薪资以及依据

5、当前薪资、福利

6、以前是通信的，对互联网怎么看

7、部门业务量很多，可能需要加班，你怎么看

8、有什么问题想问的

两天后，给了offer。腾讯的面试流程大体这样，技术面考察的都是基础，要有深度，不能停留在表面。自己前前后后准备了四个月，从第一次的惨败到后期的从容面对，一步步走下来，收获满满，最终得到想要的结果

![img](https://pic1.zhimg.com/80/v2-0f141f494f299642d870fdb807eb5840_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/474215143

作者：CPP后端技术

# 【NO.598】如何高效定位网络丢包问题？

## 1.引言

本期分享一个比较常见的网络问题--丢包。例如我们去ping一个网站，如果能ping通，且网站返回信息全面，则说明与网站服务器的通信是畅通的，如果ping不通，或者网站返回的信息不全等，则很可能是数据被丢包了，类似情况想必大家都不陌生。针对网络丢包，本人提供一些常见的丢包故障定位方法，希望能够帮助大家对网络丢包有更多的认识，遇到丢包莫要慌，且跟着一起来涨姿(知)势(识)···

## 2.什么是丢包

数据在Internet上是以数据包为单位传输的，单位为字节，数据在网络上传输，受网络设备，网络质量等原因的影响，使得接收到的数据少于发送出去的数据，造成丢包。

## 3.数据包接收、发送原理

![img](https://pic1.zhimg.com/80/v2-5851f0ed436b2689116ff419629aba90_720w.webp)

**发送数据包：**

1.应用程序的数据包，在TCP层增加TCP报文头，形成可传输的数据包。 2.在IP层增加IP报头，形成IP报文。 3.经过数据网卡驱动程序将IP包再添加14字节的MAC头，构成frame（暂⽆CRC），frame（暂⽆CRC）中含有发送端和接收端的MAC地址。 4.驱动程序将frame（暂⽆CRC）拷贝到网卡的缓冲区，由网卡处理。 5.⽹卡为frame（暂⽆CRC）添加头部同步信息和CRC校验，将其封装为可以发送的packet，然后再发送到网线上，这样说就完成了一个IP报文的发送了，所有连接到这个网线上的网卡都可以看到该packet。

**接收数据包：**

![img](https://pic3.zhimg.com/80/v2-e9e88cb3c8f7ab7dde9bde14c2db85be_720w.webp)

1.⽹卡收到⽹线上的packet，⾸先检查packet的CRC校验，保证完整性，然后将packet头去掉，得到frame。（⽹卡会检查MAC包内的⽬的MAC地址是否和本⽹卡的MAC地址⼀样，不⼀样则会丢弃。） 2.⽹卡将frame拷贝到预分配的ring buffer缓冲。 3.⽹卡驱动程序通知内核处理，经过TCP/IP协议栈层层解码处理。 4.应⽤程序从socket buffer 中读取数据。

## 5.核心思路

了解了收发包的原理，可以了解到丢包原因主要会涉及⽹卡设备、⽹卡驱动、内核协议栈三⼤类。以下我们将遵循“从下到上分层分析（各层可能性出现的丢包场景），然后查看关键信息，最终得出分析结果”的原则展开介绍。

## 6.目录--网络丢包情形概览

**> 硬件网卡丢包**

**> 网卡驱动丢包**

**> 以太网链路层丢包**

**> 网络IP层丢包**

**> 传输层UDP/TCP丢包**

**> 应用层socket丢包**

针对以上6种情形，分别作出如下详述~

### 6.1 硬件网卡丢包

**Ring Buffer溢出**

![img](https://pic4.zhimg.com/80/v2-7834b3041d3203255d317be31ff07743_720w.webp)

如图所示，物理介质上的数据帧到达后首先由NIC（网络适配器）读取，写入设备内部缓冲区Ring Buffer中，再由中断处理程序触发Softirq从中消费，Ring Buffer的大小因网卡设备而异。当网络数据包到达（生产）的速率快于内核处理（消费）的速率时，Ring Buffer很快会被填满，新来的数据包将被丢弃；

查看：

通过ethtool或/proc/net/dev可以查看因Ring Buffer满而丢弃的包统计，在统计项中以fifo标识：

```text
$ ethtool -S eth0|grep rx_fifo
rx_fifo_errors: 0
$ cat /proc/net/dev
Inter-|Receive | Transmitface |bytes packets errs drop fifo frame compressed 
multicast|bytes packets errs drop fifo colls carrier compressed
eth0: 17253386680731 42839525880 0 0 0 0 0 244182022 14879545018057 41657801805 0 0 0 0 0 0
```

\# 查看eth0网卡Ring Buffer最大值和当前设置

```text
$ ethtool -g eth0
```

解决方案：修改网卡eth0接收与发送硬件缓存区大小

```text
$ ethtool -G eth0 rx 4096 tx 4096
```

**网卡端口协商丢包**

\1. 查看网卡丢包统计：ethtool -S eth1/eth0

![img](https://pic1.zhimg.com/80/v2-91ba6c3a7b851589a290a89302b2db68_720w.webp)

\2. 查看网卡配置状态：ethtool eth1/eth0

![img](https://pic2.zhimg.com/80/v2-580391ef0739dc7c4d3370fc8e892b49_720w.webp)

主要查看网卡和上游网络设备协商速率和模式是否符合预期；

解决方案：

1 重新自协商： ethtool -r eth1/eth0;

2 如果上游不支持自协商，可以强制设置端口速率：

```text
ethtool -s eth1 speed 1000 duplex full autoneg off
```

**网卡流控丢包**

\1. 查看流控统计：

```text
ethtool -S eth1 | grep control
```

![img](https://pic1.zhimg.com/80/v2-8538144528f7c467a31fd01f7d2c5d54_720w.webp)

rx_flow_control_xon是在网卡的RX Buffer满或其他网卡内部的资源受限时，给交换机端口发送的开启流控的pause帧计数。对应的，tx_flow_control_xoff是在资源可用之后发送的关闭流控的pause帧计数。

2 .查看网络流控配置：ethtool -a eth1

![img](https://pic3.zhimg.com/80/v2-19677d3d4fb38f06235120f6aeb43786_720w.webp)

解决方案：关闭网卡流控

```text
ethtool -A ethx autoneg off //自协商关闭
ethtool -A ethx tx off //发送模块关闭
ethtool -A ethx rx off //接收模块关闭
```

**报文mac地址丢包**

一般计算机网卡都工作在非混杂模式下，此时网卡只接受来自网络端口的目的地址指向自己的数据，如果报文的目的mac地址不是对端的接口的mac地址，一般都会丢包，一般这种情况很有可能是源端设置静态arp表项或者动态学习的arp表项没有及时更新，但目的端mac地址已发生变化（换了网卡），没有更新通知到源端（比如更新报文被丢失，中间交换机异常等情况）；

查看：

1.目的端抓包，tcpdump可以开启混杂模式，可以抓到对应的报文，然后查看mac地址；

2.源端查看arp表或者抓包（上一跳设备），看发送的mac地址是否和下一跳目的端的mac地址一致；

解决方案：

1.刷新arp表然后发包触发arp重新学习（可能影响其他报文，增加延时，需要小心操作）；

2.可以在源端手动设置正确的静态的arp表项；

**其他网卡异常丢包**

这类异常比少见，但如果都不是上面哪些情况，但网卡统计里面任然有丢包计数，可以试着排查一下：

**网卡firmware版本:**

排查一下网卡phy芯片firmware是不是有bug，安装的版本是不是符合预期，查看 ethtool -i eth1:

![img](https://pic4.zhimg.com/80/v2-4a10244059932c745e1ea9ac45736bf3_720w.webp)

和厂家提case询问是不是已知问题，有没有新版本等；

**网线接触不良：**

如果网卡统计里面存在crc error 计数增长，很可能是网线接触不良，可以通知网管排查一下：

```text
ethtool -S eth0
```

![img](https://pic4.zhimg.com/80/v2-8324d1187d3d1ff4055d7e4b5a1ef1c3_720w.webp)

解决方案：一般试着重新插拔一下网线，或者换一根网线，排查插口是否符合端口规格等;

**报文长度丢包**

网卡有接收正确报文长度范围，一般正常以太网报文长度范围：64-1518，发送端正常情况会填充或者分片来适配，偶尔会发生一些异常情况导致发送报文不正常丢包；

查看：

```text
ethtool -S eth1|grep length_errors
```

![img](https://pic1.zhimg.com/80/v2-adbceb55e95a9dc2fbc10d8ea6cc3170_720w.webp)

解决方案：

1 调整接口MTU配置，是否开启支持以太网巨帧；

2 发送端开启PATH MTU进行合理分片；

简单总结一下网卡丢包：

![img](https://pic3.zhimg.com/80/v2-77885628288b9e551483031f79b7d846_720w.webp)

### 6.2 网卡驱动丢包

查看：ifconfig eth1/eth0 等接口

![img](https://pic4.zhimg.com/80/v2-50c859c7a6480ae61a3075694d21e4f7_720w.webp)

1.RX errors: 表示总的收包的错误数量，还包括too-long-frames错误，Ring Buffer 溢出错误，crc 校验错误，帧同步错误，fifo overruns 以及 missed pkg 等等。

2.RX dropped: 表示数据包已经进入了 Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃。

3.RX overruns: 表示了 fifo 的 overruns，这是由于 Ring Buffer(aka Driver Queue) 传输的 IO 大于 kernel 能够处理的 IO 导致的，而 Ring Buffer 则是指在发起 IRQ 请求之前的那块 buffer。很明显，overruns 的增大意味着数据包没到 Ring Buffer 就被网卡物理层给丢弃了，而 CPU 无法即使的处理中断是造成 Ring Buffer 满的原因之一，上面那台有问题的机器就是因为 interruprs 分布的不均匀(都压在 core0)，没有做 affinity 而造成的丢包。

\4. RX frame: 表示 misaligned 的 frames。

\5. 对于 TX 的来说，出现上述 counter 增大的原因主要包括 aborted transmission, errors due to carrirer, fifo error, heartbeat erros 以及 windown error，而 collisions 则表示由于 CSMA/CD 造成的传输中断。

**驱动溢出丢包**

netdev_max_backlog是内核从NIC收到包后，交由协议栈（如IP、TCP）处理之前的缓冲队列。每个CPU核都有一个backlog队列，与Ring Buffer同理，当接收包的速率大于内核协议栈处理的速率时，CPU的backlog队列不断增长，当达到设定的netdev_max_backlog值时，数据包将被丢弃。

查看:

通过查看/proc/net/softnet_stat可以确定是否发生了netdev backlog队列溢出：

![img](https://pic4.zhimg.com/80/v2-e2dc15cb2d3e6e18d048495e05483853_720w.webp)

其中：每一行代表每个CPU核的状态统计，从CPU0依次往下；每一列代表一个CPU核的各项统计：第一列代表中断处理程序收到的包总数；第二列即代表由于netdev_max_backlog队列溢出而被丢弃的包总数。从上面的输出可以看出，这台服务器统计中，并没有因为netdev_max_backlog导致的丢包。

解决方案：

netdev_max_backlog的默认值是1000，在高速链路上，可能会出现上述第二统计不为0的情况，可以通过修改内核参数 net.core.netdev_max_backlog来解决：

```text
$ sysctl -w net.core.netdev_max_backlog=2000
```

**单核负载高导致丢包**

单核CPU软中断占有高, 导致应用没有机会收发或者收包比较慢，即使调整netdev_max_backlog队列大小仍然会一段时间后丢包，处理速度跟不上网卡接收的速度;

查看：mpstat -P ALL 1

![img](https://pic1.zhimg.com/80/v2-fa0cba07837a5de86e4066910f13c14c_720w.webp)

单核软中断占有100%，导致应用没有机会收发或者收包比较慢而丢包；

**解决方案**：

1.调整网卡RSS队列配置：

查看：ethtool -x ethx；

调整：ethtool -X ethx xxxx；

2.看一下网卡中断配置是否均衡 cat /proc/interrupts

调整：

```text
1） irqbalance 调整；
# 查看当前运行情况
service irqbalance status
# 终止服务
service irqbalance stop
2） 中断绑CPU核 echo mask > /proc/irq/xxx/smp_affinity
```

3.根据CPU和网卡队列个数调整网卡多队列和RPS配置

-CPU大于网卡队列个数：

查看网卡队列 ethtool -x ethx；

协议栈开启RPS并设置RPS；

```text
echo $mask（CPU配置）> /sys/class/net/$eth/queues/rx-$i/rps_cpus
echo 4096（网卡buff）> /sys/class/net/$eth/queues/rx-$i/rps_flow_cnt
2）CPU小于网卡队列个数，绑中断就可以，可以试着关闭RPS看一下效果：
echo 0 > /sys/class/net/<dev>/queues/rx-<n>/rps_cpus
```

4.numa CPU调整，对齐网卡位置，可以提高内核处理速度，从而给更多CPU给应用收包，减缓丢包概率；

查看网卡numa位置：

```text
ethtool -i eth1|grep bus-info
lspci -s bus-info -vv|grep node
```

上面中断和RPS设置里面mask需要重新按numa CPU分配重新设置;

5.可以试着开启中断聚合（看网卡是否支持）

查看 :

```text
ethtool -c ethx
Coalesce parameters for eth1:
Adaptive RX: on  TX: on
stats-block-usecs: 0
sample-interval: 0
pkt-rate-low: 0
pkt-rate-high: 0


rx-usecs: 25
rx-frames: 0
rx-usecs-irq: 0
rx-frames-irq: 256


tx-usecs: 25
tx-frames: 0
tx-usecs-irq: 0
tx-frames-irq: 256


rx-usecs-low: 0
rx-frame-low: 0
tx-usecs-low: 0
tx-frame-low: 0


rx-usecs-high: 0
rx-frame-high: 0
tx-usecs-high: 0
tx-frame-high: 0
```

调整：

```text
ethtool -C ethx adaptive-rx on
```

简单总结一下网卡驱动丢包处理：

![img](https://pic3.zhimg.com/80/v2-c541bbfdda72c14ad7948e2eff08989a_720w.webp)

### 6.3 内核协议栈丢包

**以太网链路层丢包**

**neighbor系统arp丢包**

**arp_ignore配置丢包**

arp_ignore参数的作用是控制系统在收到外部的arp请求时，是否要返回arp响应。arp_ignore参数常用的取值主要有0，1，2，3~8较少用到；

查看：sysctl -a|grep arp_ignore

![img](https://pic3.zhimg.com/80/v2-c8dc39df7f8246cacfa5692c44ea0a5e_720w.webp)

**解决方案**：根据实际场景设置对应值；

0：响应任意网卡上接收到的对本机IP地址的arp请求（包括环回网卡上的地址），而不管该目的IP是否在接收网卡上。

1：只响应目的IP地址为接收网卡上的本地地址的arp请求。

2：只响应目的IP地址为接收网卡上的本地地址的arp请求，并且arp请求的源IP必须和接收网卡同网段。

3：如果ARP请求数据包所请求的IP地址对应的本地地址其作用域（scope）为主机（host），则不回应ARP响应数据包，如果作用域为全局（global）或链路（link），则回应ARP响应数据包。

![img](https://pic1.zhimg.com/80/v2-1e153aa9df985000afd03633a01e5808_720w.webp)

![img](https://pic2.zhimg.com/80/v2-09affee6e5ffada7598c140df8d9c459_720w.webp)

**arp_filter配置丢包**

在多接口系统里面（比如腾讯云的弹性网卡场景），这些接口都可以回应arp请求，导致对端有可能学到不同的mac地址，后续报文发送可能由于mac地址和接收报文接口mac地址不一样而导致丢包，arp_filter主要是用来适配这种场景；

查看：

```text
sysctl -a | grep arp_filter
```

![img](https://pic1.zhimg.com/80/v2-19c8d804c4cd71a7d345bccae4ad414c_720w.webp)

解决方案：

```text
根据实际场景设置对应的值，一般默认是关掉此过滤规则，特殊情况可以打开；
0：默认值，表示回应arp请求的时候不检查接口情况；
1：表示回应arp请求时会检查接口是否和接收请求接口一致，不一致就不回应；
```

**arp表满导致丢包**

比如下面这种情况，由于突发arp表项很多 超过协议栈默认配置，发送报文的时候部分arp创建失败，导致发送失败，从而丢包：

![img](https://pic4.zhimg.com/80/v2-232434337ad998a41a6ce4f53e8e2273_720w.webp)

查看：

- 查看arp状态：cat /proc/net/stat/arp_cache ，table_fulls统计：
- 查看dmesg消息（内核打印）：

![img](https://pic4.zhimg.com/80/v2-f1bdb9d80feda7ef195a9d65faca286f_720w.webp)

```text
dmesg|grep neighbour
neighbour: arp_cache: neighbor table overflow!
```

- 查看当前arp表大小：ip n|wc -l

查看系统配额：

```text
sysctl -a |grep net.ipv4.neigh.default.gc_thresh
gc_thresh1：存在于ARP高速缓存中的最少层数，如果少于这个数，垃圾收集器将不会运行。缺省值是128。


gc_thresh2 ：保存在 ARP 高速缓存中的最多的记录软限制。垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。缺省值是 512。
gc_thresh3 ：保存在 ARP 高速缓存中的最多记录的硬限制，一旦高速缓存中的数目高于此，垃圾收集器将马上运行。缺省值是1024。
```

一般在内存足够情况下，可以认为gc_thresh3 值是arp 表总大小；

![img](https://pic4.zhimg.com/80/v2-3c47ceb5dfaaf041a5109ff9114646df_720w.webp)

**解决方案**：根据实际arp最大值情况（比如访问其他子机最大个数），调整arp表大小

```text
$ sudo sysctl -w net.ipv4.neigh.default.gc_thresh1=1024
$ sudo sysctl -w net.ipv4.neigh.default.gc_thresh2=2048
$ sudo sysctl -w net.ipv4.neigh.default.gc_thresh3=4096
$ sudo sysctl  -p
```

**arp请求缓存队列溢出丢包**

查看：

```text
cat /proc/net/stat/arp_cache ，unresolved_discards是否有新增计数
```

解决方案：根据客户需求调整缓存队列大小unres_qlen_bytes：

![img](https://pic1.zhimg.com/80/v2-d25caeccc0aa2ebc5678d7163a732014_720w.webp)

### 6.4 网络IP层丢包

**接口ip地址配置丢包**

\1. 本机服务不通，检查lo接口有没有配置地址是127.0.0.1；

2 .本机接收失败， 查看local路由表：ip r show table local|grep 子机ip地址；这种丢包一般会出现在多IP场景，子机底层配置多ip失败，导致对应ip收不到包而丢包；

![img](https://pic2.zhimg.com/80/v2-ae7532b75c48908cb5d4e754f06f27ed_720w.webp)

解决方案：

1.配置正确接口ip地址；比如ip a add 1.1.1.1 dev eth0

2.如果发现接口有地址还丢包，可能是local路由表没有对应条目，紧急情况下，可以用手工补上：

比如ip r add local 本机ip地址 dev eth0 table local ；

**路由丢包**

**路由配置丢包**

查看：

1.查看配置 路由是否设置正确（是否可达），是否配置策略路由（在弹性网卡场景会出现此配置）ip rule：

![img](https://pic3.zhimg.com/80/v2-be579c18bd1eeae9902287f9ea005272_720w.webp)

然后找到对应路由表。查看路由表：

![img](https://pic3.zhimg.com/80/v2-87e87f2ded85854d40bba215cd9899ca_720w.webp)

或者直接用 ip r get x.x.x.x，让系统帮你查找是否存在可达路由，接口是否符合预期；

2.查看系统统计信息：

```text
netstat -s|grep "dropped because of missing route"
```

解决方案：重新配置正确的路由；

**反向路由过滤丢包**

反向路由过滤机制是Linux通过反向路由查询，检查收到的数据包源IP是否可路由（Loose mode）、是否最佳路由（Strict mode），如果没有通过验证，则丢弃数据包，设计的目的是防范IP地址欺骗攻击。

查看：

rp_filter提供三种模式供配置：

0 - 不验证

1 - RFC3704定义的严格模式：对每个收到的数据包，查询反向路由，如果数据包入口和反向路由出口不一致，则不通过

2 - RFC3704定义的松散模式：对每个收到的数据包，查询反向路由，如果任何接口都不可达，则不通过

查看当前rp_filter策略配置：

$cat /proc/sys/net/ipv4/conf/eth0/rp_filter

如果这里设置为1，就需要查看主机的网络环境和路由策略是否可能会导致客户端的入包无法通过反向路由验证了。

从原理来看这个机制工作在网络层，因此，如果客户端能够Ping通服务器，就能够排除这个因素了。

解决方案：

根据实际网络环境将rp_filter设置为0或2：

```text
$ sysctl -w net.ipv4.conf.all.rp_filter=2或
$ sysctl -w net.ipv4.conf.eth0.rp_filter=2
```

**防火墙丢包**

**客户设置规则导致丢包**

查看：

```text
iptables -nvL |grep DROP ;
```

解决方案： 修改防火墙规则；

**连接跟踪导致丢包**

**连接跟踪表溢出丢包**

kernel 用 ip_conntrack 模块来记录 iptables 网络包的状态，并把每条记录保存到 table 里（这个 table 在内存里，可以通过/proc/net/ip_conntrack 查看当前已经记录的总数），如果网络状况繁忙，比如高连接，高并发连接等会导致逐步占用这个 table 可用空间，一般这个 table 很大不容易占满并且可以自己清理，table 的记录会一直呆在 table 里占用空间直到源 IP 发一个 RST 包，但是如果出现被攻击、错误的网络配置、有问题的路由/路由器、有问题的网卡等情况的时候，就会导致源 IP 发的这个 RST 包收不到，这样就积累在 table 里，越积累越多直到占满。无论，哪种情况导致table变满，满了以后就会丢包，出现外部无法连接服务器的情况。内核会报如下错误信息：kernel: ip_conntrack: table full, dropping packet；

查看当前连接跟踪数 :

```text
cat /proc/sys/net/netfilter/nf_conntrack_max
```

解决方案：

```text
增大跟踪的最大条数
net.netfilter.nf_conntrack_max  = 3276800
减少跟踪连接的最大有效时间
net.netfilter.nf_conntrack_tcp_timeout_established = 1200
net.netfilter.nf_conntrack_udp_timeout_stream = 180
net.netfilter.nf_conntrack_icmp_timeout = 30
```

**ct创建冲突失导致丢包**

查看：当前连接跟踪统计：cat /proc/net/stat/nf_conntrack，可以查各种ct异常丢包统计

![img](https://pic4.zhimg.com/80/v2-7356c5bbf469b2630bd59c074efc2d27_720w.webp)

解决方案：内核热补丁修复或者更新内核版本（合入补丁修改）；

### 6.5 传输层UDP/TCP丢包

**tcp 连接跟踪安全检查丢包**

丢包原因：由于连接没有断开，但服务端或者client之前出现过发包异常等情况（报文没有经过连接跟踪模块更新窗口计数），没有更新合法的window范围，导致后续报文安全检查被丢包；协议栈用 nf_conntrack_tcp_be_liberal 来控制这个选项：

1：关闭，只有不在tcp窗口内的rst包被标志为无效；

0：开启; 所有不在tcp窗口中的包都被标志为无效；

查看：

查看配置 ：

```text
sysctl -a|grep nf_conntrack_tcp_be_liberal 
net.netfilter.nf_conntrack_tcp_be_liberal = 1
```

查看log：

一般情况下netfiler模块默认没有加载log，需要手动加载;

```text
modprobe ipt_LOG11
sysctl -w net.netfilter.nf_log.2=ipt_LOG
```

然后发包后在查看syslog；

解决方案：根据实际抓包分析情况判断是不是此机制导致的丢包，可以试着关闭试一下；

**分片重组丢包**

情况总结：**超时**

查看：

```text
netstat -s|grep timeout
601 fragments dropped after timeout
```

解决方法：调整超时时间

```text
net.ipv4.ipfrag_time = 30
sysctl -w net.ipv4.ipfrag_time=60
```

**frag_high_thresh, 分片的内存超过一定阈值会导致系统安全检查丢包**

查看：

```text
netstat -s|grep reassembles
8094 packet reassembles failed
```

解决方案：调整大小

```text
net.ipv4.ipfrag_high_thresh 
net.ipv4.ipfrag_low_thresh
```

**分片安全距检查离丢包**

查看：

```text
netstat -s|grep reassembles
8094 packet reassembles failed
```

解决方案： 把ipfrag_max_dist设置为0，就关掉此安全检查

![img](https://pic3.zhimg.com/80/v2-ec3b7052d7451b6ecbd03c40218dac8e_720w.webp)

pfrag_max_dist特性，在一些场景下其实并不适用：

1.有大量的网络报文交互

2.发送端的并发度很高，同时SMP架构，导致很容易造成这种乱序情况；

**分片hash bucket冲突链太长超过系统默认值128**

查看：

```text
dmesg|grep “Dropping fragment”
inet_frag_find: Fragment hash bucket 128 list length grew over limit. Dropping fragment.
```

解决方案：热补丁调整hash大小；

**系统内存不足，创建新分片队列失败**

查看方法：

```text
netstat -s|grep reassembles
8094 packet reassembles failed
```

dropwatch查看丢包位置 ：

![img](https://pic4.zhimg.com/80/v2-3cd61b778807e36cfb3e99887c7cb25b_720w.webp)

解决方案：

a.增大系统网络内存：

```text
net.core.rmem_default 
net.core.rmem_max 
net.core.wmem_default
```

b.系统回收内存：

紧急情况下，可以用 /proc/sys/vm/drop_caches, 去释放一下虚拟内存；

```text
To free pagecache:
# echo 1 > /proc/sys/vm/drop_caches
To free dentries and inodes:
# echo 2 > /proc/sys/vm/drop_caches
To free pagecache, dentries and inodes:
echo 3 > /proc/sys/vm/drop_caches
```

**MTU丢包**

![img](https://pic2.zhimg.com/80/v2-ae138a2f95101add812aea71ea1617fd_720w.webp)

查看：

1.检查接口MTU配置，ifconfig eth1/eth0，默认是1500；

2.进行MTU探测，然后设置接口对应的MTU值；

解决方案：

\1. 根据实际情况，设置正确MTU值；

\2. 设置合理的tcp mss，启用TCP MTU Probe:

```text
cat /proc/sys/net/ipv4/tcp_mtu_probing:
tcp_mtu_probing - INTEGER Controls TCP Packetization-Layer Path MTU Discovery.
Takes three values:
0 - Disabled 
1 - Disabled by default, enabled when an ICMP black hole detected
2 - Always enabled, use initial MSS of tcp_base_mss.
```

**tcp层丢包**

**TIME_WAIT过多丢包**

大量TIMEWAIT出现，并且需要解决的场景，在高并发短连接的TCP服务器上，当服务器处理完请求后立刻按照主动正常关闭连接。。。这个场景下，会出现大量socket处于TIMEWAIT状态。如果客户端的并发量持续很高，此时部分客户端就会显示连接不上；

查看：

查看系统log ：

```text
dmsg
TCP: time wait bucket table overflow；
```

查看系统配置：

```text
sysctl -a|grep tcp_max_tw_buckets
net.ipv4.tcp_max_tw_buckets = 16384
```

解决方案：

\1. tw_reuse，tw_recycle 必须在客户端和服务端timestamps 开启时才管用（默认打开）

\2. tw_reuse 只对客户端起作用，开启后客户端在1s内回收；

\3. tw_recycle对客户端和服务器同时起作用，开启后在3.5*RTO 内回收，RTO 200ms~ 120s具体时间视网络状况。内网状况比tw_reuse稍快，公网尤其移动网络大多要比tw_reuse 慢，优点就是能够回收服务端的TIME_WAIT数量；

在服务端，如果网络路径会经过NAT节点，不要启用net.ipv4.tcp_tw_recycle，会导致时间戳混乱，引起其他丢包问题；

\4. 调整tcp_max_tw_buckets大小，如果内存足够：

```text
sysctl -w net.ipv4.tcp_max_tw_buckets=163840；
```

**时间戳异常丢包**

当多个客户端处于同一个NAT环境时，同时访问服务器，不同客户端的时间可能不一致，此时服务端接收到同一个NAT发送的请求，就会出现时间戳错乱的现象，于是后面的数据包就被丢弃了，具体的表现通常是是客户端明明发送的SYN，但服务端就是不响应ACK。在服务器借助下面的命令可以来确认数据包是否有不断被丢弃的现象。

检查：

```text
netstat -s | grep rejects
```

解决方案：

如果网络路径会经过NAT节点，不要启用net.ipv4.tcp_tw_recycle；

**TCP队列问题导致丢包**

**原理：**

**tcp状态机（三次握手）**

![img](https://pic1.zhimg.com/80/v2-b5fdafc659ca988900140dc2f28ef528_720w.webp)

**协议处理：**

![img](https://pic2.zhimg.com/80/v2-622b97c791235cfc5d75fae818836121_720w.webp)

**一个是半连接队列（syn queue）：**

在三次握手协议中，服务器维护一个半连接队列，该队列为每个客户端的SYN包开设一个条目(服务端在接收到SYN包的时候，就已经创建了request_sock结构，存储在半连接队列中)，该条目表明服务器已收到SYN包，并向客户发出确认，正在等待客户的确认包（会进行第二次握手发送SYN＋ACK的包加以确认）。这些条目所标识的连接在服务器处于Syn_RECV状态，当服务器收到客户的确认包时，删除该条目，服务器进入ESTABLISHED状态。该队列为SYN队列，长度为max(64, /proc/sys/net/ipv4/tcp_max_syn_backlog), 机器的tcp_max_syn_backlog值在/proc/sys/net/ipv4/tcp_max_syn_backlog下配置;

**一个是全连接队列（accept queue）：**

第三次握手时，当server接收到ACK 报之后， 会进入一个新的叫 accept 的队列，该队列的长度为 min(backlog, somaxconn)，默认情况下，somaxconn 的值为 128，表示最多有 129 的 ESTAB 的连接等待 accept()，而 backlog 的值则应该是由 int listen(int sockfd, int backlog) 中的第二个参数指定，listen 里面的 backlog 可以有我们的应用程序去定义的;

查看：

连接建立失败,syn丢包：

```text
netstat -s |grep -i listen
SYNs to LISTEN sockets dropped
```

也会受到连接满丢包影响

解决方案： 增加大小 tcp_max_syn_backlog

连接满丢包

-xxx times the listen queue of a socket overflowed

查看：

- 查看accept队列大小 ：net.core.somaxconn
- ss -lnt查询socket队列 ：LISTEN 状态: Recv-Q 表示的当前等待服务端调用 accept 完成三次握手的 listen backlog 数值，也就是说，当客户端通过 connect() 去连接正在 listen() 的服务端时，这些连接会一直处于这个 queue 里面直到被服务端 accept()；Send-Q 表示的则是最大的 listen backlog 数值，这就就是上面提到的 min(backlog, somaxconn) 的值，
- 看一下是不是应用程序设置限制， int listen(int sockfd, int backlog)；

解决方案：

- Linux内核参进行优化，可以缓解压力 tcp_abort_on_overflow=1
- 调整net.core.somaxconn大小;
- 应用程序设置问题，通知客户程序修改；

**syn flood攻击丢包**

目前，Linux下默认会进行5次重发SYN-ACK包，重试的间隔时间从1s开始，下次的重试间隔时间是前一次的双倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s都知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s，TCP才会把断开这个连接。由于，SYN超时需要63秒，那么就给攻击者一个攻击服务器的机会，攻击者在短时间内发送大量的SYN包给Server(俗称 SYN flood 攻击)，用于耗尽Server的SYN队列。对于应对SYN 过多的问题;

查看： 查看syslog： kernel: [3649830.269068] TCP: Possible SYN flooding on port xxx. Sending cookies. Check SNMP counters.

解决方案：

- 增大tcp_max_syn_backlog
- 减少tcp_synack_retries
- 启用tcp_syncookies
- 启用tcp_abort_on_overflow， tcp_abort_on_overflow修改成 1，1表示第三步的时候如果全连接队列满了，server发送一个reset包给client，表示废掉这个握手过程和这个连接（本来在server端这个连接就还没建立起来）；

**PAWS机制丢包**

原理：PAWS(Protect Against Wrapped Sequence numbers)，高带宽下，TCP序列号可能在较短的时间内就被重复使用(recycle/wrapped) 就可能导致同一条TCP流在短时间内出现序号一样的两个合法的数据包及其确认包。

查看：

```text
$netstat -s |grep -e "passive connections rejected because of time 
stamp" -e "packets rejects in established connections because of 
timestamp” 
387158 passive connections rejected because of time stamp
825313 packets rejects in established connections because of timestamp
```

通过sysctl查看是否启用了tcp_tw_recycle及tcp_timestamp:

```text
$ sysctl net.ipv4.tcp_tw_recycle
net.ipv4.tcp_tw_recycle = 1 
$ sysctl net.ipv4.tcp_timestamps
net.ipv4.tcp_timestamps = 1
```

\1. tcp_tw_recycle参数。它用来快速回收TIME_WAIT连接，不过如果在NAT环境下会引发问题;

\2. 当多个客户端通过NAT方式联网并与服务端交互时，服务端看到的是同一个IP，也就是说对服务端而言这些客户端实际上等同于一个，可惜由于这些客户端的时间戳可能存在差异，于是乎从服务端的视角看，便可能出现时间戳错乱的现象，进而直接导致时间戳小的数据包被丢弃。如果发生了此类问题，具体的表现通常是是客户端明明发送的SYN，但服务端就是不响应ACK。

解决方案：

在NAT环境下，清除tcp时间戳选项，或者不开启tcp_tw_recycle参数；

**TLP问题丢包**

TLP主要是为了解决尾丢包重传效率的问题，TLP能够有效的避免较长的RTO超时，进而提高TCP性能，详细参考文章：

[http://perthcharles.github.io/2015/10/31/wiki-network-tcp-tlp/](https://link.zhihu.com/?target=http%3A//perthcharles.github.io/2015/10/31/wiki-network-tcp-tlp/)；

但在低时延场景下（短连接小包量），TLP与延迟ACK组合可能会造成无效重传，导致客户端感发现大量假重传包，加大了响应延迟；

查看：

查看协议栈统计：

```text
netstat -s |grep TCPLossProbes
```

查看系统配置：

```text
sysctl -a | grep tcp_early_retrans
```

![img](https://pic2.zhimg.com/80/v2-9ebc9757263871572c8b4d21fa89c789_720w.webp)

解决方案：

1.关掉延迟ack，打开快速ack；

2.linux实现nodelay语意不是快速ack，只是关闭nagle算法；

3.打开快速ack选项，socket里面有个 TCP_QUICKACK 选项， 需要每次recv后再设置一次。

**内存不足导致丢包**

查看：

查看log：

```text
dmesg|grep “out of memory”
```

查看系统配置：

```text
cat /proc/sys/net/ipv4/tcp_mem
cat /proc/sys/net/ipv4/tcp_rmem
cat /proc/sys/net/ipv4/tcp_wmem
```

解决方案：

根据TCP业务并发流量，调整系统参数，一般试着增大2倍或者其他倍数来看是否缓解；

```text
sysclt -w net.ipv4.tcp_mem=
sysclt -w net.ipv4.tcp_wmem=
sysclt -w net.ipv4.tcp_rmem=
sysctl -p
```

**TCP超时丢包**

查看：

抓包分析一下网络RTT：

![img](https://pic4.zhimg.com/80/v2-f5e293fd0fe3487a95e662f000f24483_720w.webp)

用其他工具测试一下当前端到端网络质量（hping等）；

```text
# hping -S 9.199.10.104 -A
HPING 9.199.10.104 (bond1 9.199.10.104): SA set, 40 headers + 0 data bytes
len=46 ip=9.199.10.104 ttl=53 DF id=47617 sport=0 flags=R seq=0 win=0 rtt=38.3 ms
len=46 ip=9.199.10.104 ttl=53 DF id=47658 sport=0 flags=R seq=1 win=0 rtt=38.3 ms
len=46 ip=9.199.10.104 ttl=53 DF id=47739 sport=0 flags=R seq=2 win=0 rtt=30.4 ms
len=46 ip=9.199.10.104 ttl=53 DF id=47842 sport=0 flags=R seq=3 win=0 rtt=30.4 ms
len=46 ip=9.199.10.104 ttl=53 DF id=48485 sport=0 flags=R seq=4 win=0 rtt=38.7 ms
len=46 ip=9.199.10.104 ttl=53 DF id=49274 sport=0 flags=R seq=5 win=0 rtt=34.1 ms
len=46 ip=9.199.10.104 ttl=53 DF id=49491 sport=0 flags=R seq=6 win=0 rtt=30.3 ms
```

解决方案：

- 关闭Nagle算法，减少小包延迟；
- 关闭延迟ack:

```text
sysctl -w net.ipv4.tcp_no_delay_ack=1
```

**TCP乱序丢包**

此时TCP会无法判断是数据包丢失还是乱序，因为丢包和乱序都会导致接收端收到次序混乱的数据包，造成接收端的数据空洞。TCP会将这种情况暂定为数据包的乱序，因为乱序是时间问题（可能是数据包的迟到），而丢包则意味着重传。当TCP意识到包出现乱序的情况时，会立即ACK，该ACK的TSER部分包含的TSEV值会记录当前接收端收到有序报文段的时刻。这会使得数据包的RTT样本值增大，进一步导致RTO时间延长。这对TCP来说无疑是有益的，因为TCP有充分的时间判断数据包到底是失序还是丢了来防止不必要的数据重传。当然严重的乱序则会让发送端以为是丢包一旦重复的ACK超过TCP的阈值，便会触发超时重传机制，以及时解决这种问题；

查看：抓包分析是否存在很多乱序报文：

![img](https://pic4.zhimg.com/80/v2-686210eed2073d7dfdfd36745d0e80fb_720w.webp)

解决方案：如果在多径传输场景或者网络质量不好，可以通过修改下面值来提供系统对TCP无序传送的容错率：

![img](https://pic1.zhimg.com/80/v2-65d7941aca8d30c54bf93c723a01fcec_720w.webp)

**拥塞控制丢包**

在互联网发展的过程当中，TCP算法也做出了一定改变，先后演进了

Reno、NewReno、Cubic和Vegas，这些改进算法大体可以分为基于丢包和基于延时的拥塞控制算法。基于丢包的拥塞控制算法以Reno、NewReno为代表，它的主要问题有Buffer bloat和长肥管道两种，基于丢包的协议拥塞控制机制是被动式的，其依据网络中的丢包事件来做网络拥塞判断。即使网络中的负载很高，只要没有产生拥塞丢包，协议就不会主动降低自己的发送速度。最初路由器转发出口的Buffer 是比较小的，TCP在利用时容易造成全局同步，降低带宽利用率，随后路由器厂家由于硬件成本下降不断地增加Buffer，基于丢包反馈的协议在不丢包的情况下持续占用路由器buffer，虽然提高了网络带宽的利用率，但同时也意味着发生拥塞丢包后，网络抖动性加大。另外对于带宽和RTT都很高的长肥管道问题来说，管道中随机丢包的可能性很大，TCP的默认buffer设置比较小加上随机丢包造成的cwnd经常下折，导致带宽利用率依旧很低； BBR（Bottleneck Bandwidth and Round-trip propagation time）是一种基于带宽和延迟反馈的拥塞控制算法。目前已经演化到第二版，是一个典型的封闭反馈系统，发送多少报文和用多快的速度发送这些报文都是在每次反馈中不断调节。在BBR提出之前，拥塞控制都是基于事件的算法，需要通过丢包或延时事件驱动；BBR提出之后，拥塞控制是基于反馈的自主自动控制算法，对于速率的控制是由算法决定，而不由网络事件决定，BBR算法的核心是找到最大带宽（Max BW）和最小延时（Min RTT）这两个参数，最大带宽和最小延时的乘积可以得到BDP(Bandwidth Delay Product), 而BDP就是网络链路中可以存放数据的最大容量。BDP驱动Probing State Machine得到Rate quantum和cwnd，分别设置到发送引擎中就可以解决发送速度和数据量的问题。

Linux 4.9内核首次采用BBR拥塞控制算法第一个版本，BBR抗丢包能力比其他算法要强，但这个版本在某些场景下面有问题（缺点），BBR在实时音视频领域存在的问题，深队列竞争不过Cubic。

问题现象就是：在深队列场景，BBR的ProbeRTT阶段只发4个包，发送速率下降太多会引发延迟加大和卡顿问题。

查看：

```text
ss -sti //在源端 ss -sti|grep 10.125.42.49:47699 -A 3 （ 10.125.42.49:47699 是目的端地址和端口号）
```

![img](https://pic1.zhimg.com/80/v2-cb01dbb092af4f71cc9ed4cf0766eda0_720w.webp)

![img](https://pic3.zhimg.com/80/v2-1f78761d69a65e84284d871cfaafb38a_720w.webp)

解决方案：

- ProbeRTT并不适用实时音视频领域，因此可以选择直接去除，或者像BBRV2把probe RTT缩短到2.5s一次，使用0.5xBDP发送；
- 如果没有特殊需求，切换成稳定的cubic算法；

*UDP层丢包*

**收发包失败丢包**

查看：netstat 统计

如果有持续的 receive buffer errors/send buffer errors 计数；

![img](https://pic3.zhimg.com/80/v2-ec63c0a62afbca78b84868b8c2d95eae_720w.webp)

解决方案：

1. CPU负载（多核绑核配置），网络负载（软中断优化，调整驱动队列netdev_max_backlog），内存配置（协议栈内存）；
2. 按峰值在来，增大buffer缓存区大小：

```text
net.ipv4.udp_mem = xxx
net.ipv4.udp_rmem_min = xxx
net.ipv4.udp_wmem_min = xxx
```

\3. 调整应用设计：

- UDP本身就是无连接不可靠的协议，适用于报文偶尔丢失也不影响程序状态的场景，比如视频、音频、游戏、监控等。对报文可靠性要求比较高的应用不要使用 UDP，推荐直接使用 TCP。当然，也可以在应用层做重试、去重保证可靠性
- 如果发现服务器丢包，首先通过监控查看系统负载是否过高，先想办法把负载降低再看丢包问题是否消失
- 如果系统负载过高，UDP丢包是没有有效解决方案的。如果是应用异常导致CPU、memory、IO 过高，请及时定位异常应用并修复；如果是资源不够，监控应该能及时发现并快速扩容
- 对于系统大量接收或者发送UDP报文的，可以通过调节系统和程序的 socket buffer size 来降低丢包的概率
- 应用程序在处理UDP报文时，要采用异步方式，在两次接收报文之间不要有太多的处理逻辑

### 6.6 应用层socket丢包

**socket缓存区接收丢包**

查看：

\1. 抓包分析是否存在丢包情况；

\2. 查看统计：

```text
netstat -s|grep "packet receive errors"
```

解决方案：

调整socket缓冲区大小：

```text
socket配置（所有协议socket）：
# Default Socket Receive Buffer
net.core.rmem_default = 31457280
# Maximum Socket Receive Buffer
net.core.rmem_max = 67108864
```

具体大小调整原理：

缓冲区大小没有任何设置值是最佳的，因为最佳大小随具体情况而不同

缓冲区估算原理：在数据通信中，带宽时延乘积（英语：bandwidth-delay product；或称带宽延时乘积、带宽延时积等）指的是一个数据链路的能力（每秒比特）与来回通信延迟（单位秒）的乘积。[1][2]其结果是以比特（或字节）为单位的一个数据总量，等同在任何特定时间该网络线路上的最大数据量——已发送但尚未确认的数据。

**BDP = 带宽 \* RTT**

可以通过计算当面节点带宽和统计平均时延来估算BDP，即缓冲区的大小，可以参考下面常见场景估计：

![img](https://pic1.zhimg.com/80/v2-06c7ac3b26f553c0d6d861ba491ebbf8_720w.webp)

**应用设置tcp连接数大小丢包**

查看：

请参考上面TCP连接队列分析；

解决方案：

设置合理的连接队列大小，当第三次握手时，当server接收到ACK 报之后， 会进入一个新的叫 accept 的队列，该队列的长度为 min(backlog, somaxconn)，默认情况下，somaxconn 的值为 128，表示最多有 129 的 ESTAB 的连接等待 accept()，而 backlog 的值则应该是由 int listen(int sockfd, int backlog) 中的第二个参数指定，listen 里面的 backlog 可以有我们的应用程序去定义的；

**应用发送太快导致丢包**

查看统计：

```text
netstat -s|grep "send buffer errors
```

解决方案：

- ICMP/UDP没有流控机制，需要应用设计合理发送方式和速度，照顾到底层buff大小和CPU负载以及网络带宽质量；
- 设置合理的sock缓冲区大小：

```text
setsockopt(s,SOL_SOCKET,SO_SNDBUF,  i(const char*)&nSendBuf,sizeof(int));
```

- 调整系统socket缓冲区大小：

```text
# Default Socket Send Buffer
   net.core.wmem_default = 31457280
   # Maximum Socket Send Buffer
   net.core.wmem_max = 33554432
```

**附：简单总结一下内核协议栈丢包：**

![img](https://pic1.zhimg.com/80/v2-e273e4680a89be35eee8724f56fbbaa8_720w.webp)

## 7.相关工具介绍

1.dropwatch工具

原理： 监听 kfree_skb（把网络报文丢弃时会调用该函数）函数或者事件吗，然后打印对应调用堆栈；想要详细了解 linux 系统在执行哪个函数时丢包的话，可以使用 dropwatch 工具，它监听系统丢包信息，并打印出丢包发生的函数：

![img](https://pic3.zhimg.com/80/v2-00f41347991ba42ad28c67deb745b1ba_720w.webp)

\2. tcpdump工具

原理: tcpdump 是一个Unix下一个功能强大的网络抓包工具，它允许用户拦截和显示发送或收到过网络连接到该计算机的TCP/IP和其他数据包

![img](https://pic3.zhimg.com/80/v2-47c7d4b5aecdaa9527af834bad7f48b2_720w.webp)

抓包命令参考：

[https://www.tcpdump.org/manpages/tcpdump.1.html](https://link.zhihu.com/?target=https%3A//www.tcpdump.org/manpages/tcpdump.1.html)

数据包分析：

1.用wireshark工具分析 参考：Wireshark数据包分析实战.pdf

2.可以转化生成CSV数据，用Excel或者shell去分析特定场景报文；

3.可以在linux上用tshark命令行工具进行分析:

[https://www.wireshark.org/docs/man-pages/tshark.html](https://link.zhihu.com/?target=https%3A//www.wireshark.org/docs/man-pages/tshark.html)

## 8.总结

本文只是分析大部分可能会丢包节点，提供了单个节点丢包排查和相关的解决方案, 丢包问题牵扯网络链路各个组件，尤其是在云网络时代，网络拓扑复杂多变，涉及运营商网络，IDC网络，专线等underlay网络，边界网关，VPC网络，CLB负载均衡等云上overlay网络，各种丢包问题排障起来非常复杂且困难，但掌握网络通信基本原理后，可以分解网络拓扑，对通信节点进行逐一排查，也可以找到丢包位置，后续会更加深入介绍云计算时代，云上网络丢包排查方法，网络架构解析等，达到任何丢包问题都可以快速排查和定位解决，帮助客户快速恢复业务，下期再会。

原文地址：https://zhuanlan.zhihu.com/p/533678588

作者：CPP后端技术

# 【NO.599】高并发的socket的高性能设计

## 1.引言

随着互联网和物联网的高速发展，使用网络的人数和电子设备的数量急剧增长，其也对互联网后台服务程序提出了更高的性能和并发要求。本文的主要目的是阐述在**单机**上如何进行高并发、高性能消息传输系统的框架设计，以及该系统的常用技术，但不对其技术细节进行讨论。如您有更好的设计方案和思路，望共分享之！[注：此篇用select来讲解，虽在大并发的情况下，epoll拥有更高的效率，但整体设计思路是一致的]

首先来看看课本和学习资料上关于处理并发网络编程的三种常用方案，以及对应的大体思路和优缺点：

**1) IO多路复用模型**

->思路：单进程（非多线程）调用select()函数来处理多个连接请求。

->优点：单进程（非多线程）可支持同时处理多个网络连接请求。

->缺点：最大并发为1024个，当并发数较大时，其处理性能很低。

**2) 多进程模型**

->思路：当连接请求过来时，主进程fork产生一个子进程，让子进程负责与客户端连接进行数据通信，当客户端主动关闭连接后，子进程结束运行。

->优点：模式简单，易于理解；连接请求很小时，效率较高。

->缺点：当连接请求过多时，系统资源很快被耗尽。比如：当连接请求达到10k时，难道要启动10k个进程吗？

**3) 多线程模型**

->思路：首先启动多个工作线程，而主线程负责接收客户端连接请求，工作线程负责与客户端通信；当连接请求过来时，ACCEPT线程将sckid放入一个数组中，工作线程中的空闲线程从数组中取走一个sckid，对应的工作线程再与客户端连接进行数据通信，当客户端主动关闭连接后，此工作线程又去从指定数组中取sckid，依次重复运行。

->优点：拥有方案２)的优点，且能够解决方案２)的缺点。

->缺点：不能支持并发量大的请求和量稍大的长连接请求。

通过对以上三种方案的分析，以上方案**均不能满足**高并发、高性能的服务器的处理要求。针对以上设计方案问题的存在，该如何设计才能做到高并发、高性能的处理要求呢？

## 2.设计方案

### 2.1 大体框架

为提高并发量和处理性能，在此采用2层的设计架构。第一层由接收线程组成，负责接收客户端数据；第二层由工作线程组成，负责对接收的数据进行相应处理。为了减少数据的复制和IO操作，将接收到的客户端数据使用队列进行存储；工作线程收到处理指令后，从指令中提取相应的参数，便可知道到哪个线程的队列中获取数据。因此，系统的大体架构如下所示：
**1) 框架－1**

![img](https://pic1.zhimg.com/80/v2-18984305f61fc61066cd09358f91dae8_720w.webp)

图1 大体框架-01


[注：接收线程数:接收队列数:工作线程数 = N:N:X]
优点：
1)、有效避免接收线程之间出现锁竞争的情况。
每个接收线程对应一个接收队列，每个接收线程将接收到的数据只放在自己对应的队列中；
2)、在数据量不是很大的情况下，此框架结构还是能够满足处理要求。

缺点：
1)、在连接数量很少、而数据量很大时，将会造成锁冲突严重，致使性能急剧下降。
假如：当前系统中只有1个TCP连接，由Recv线程2负责接收该连接中的所有数据。Recv线程2每收到一条数据，就将随机通知工作线程到该队列上取数据。在某个时刻，该连接的客户端发来大量数据，将造成所有工作线程同时到Recv队列2中来取数据。此时将会出现严重的锁冲突现象，性能急剧下降。

**2) 框架－2**

![img](https://pic2.zhimg.com/80/v2-139e2cb0ac51e236101e041e5e8f7f85_720w.webp)

图2 大体框架-02


[注：接收线程数:接收队列数:工作线程数 = X:Y:Y]

优点：

1)、有效避免工作线程之间出现锁竞争的情况。

每个工作线程对应一个接收队列，每个接收线程将接收到的数据只放在自己对应的队列中；

2)、工作线程数 >= 2*接收线程数 时，能够有效的减少接收线程之间的锁竞争的情况

在这种情况下，我想你可以得到你想要的处理性能！

缺点：

1)、需要为更高的性能，付出更多的系统资源(主要:内存和CPU)。

### 2.2 如何提高并发量?

“并发量”是指系统可接受的TCP连接请求数。首先需要明确的是："高并发"只是一个相对概念。如：有些系统1K并发就算是高并发，而有些系统100K并发也不能满足要求。因此，在此只给出提高并发量的设计思路。

众所周知，IO多路复用中1个select函数最多可管理FD_SETSIZE(该值一般为1024)个SOCKET套接字，而如果要求并发量达到100K时，显然已大大超过了1个select的管理能力，那该如何解决？

答案是：使用多个select可以有效的解决以上问题。100K约等于100 * 1024，故需大约100个select才能有效管理100k并发。那该如何调用100个select来管理100k的并发呢？

因FD的管理在进程之间是独立的，虽然子进程在创建之时，会继承父进程的FD，但后续连接产生的FD却无法让子进程继续继承，因此，要实现对100k并发的有效管理，使用**多线程**实现高并发**是理想的选择**。即：每个线程调用1个select，而每个select可以管理1024个并发。

在理想情况下，启动N个接收线程，系统便可处理N *1024的并发。如：启动100个接收线程，单机便可处理100 * 1024 = 100k的网络并发。但需要注意的是：线程越多，消耗的资源越多，操作系统调度的开销越大，如果调度开销超过多线程带来的性能提升，随着线程的增加，将导致系统性能越低。(如果要求处理5k以上的请求，我将毫不犹豫的选择"多线程+epoll"的方式)

### 2.3 如何提高处理性能?

**1) Recv流程**

为了提高Recv线程接收来自客户端的数据的性能，其处理过程需要使用到：IO多路复用技术，非阻塞IO技术、内存池技术、加锁技术、事件触发机制、负载均衡策略、UNINX-UDP技术、设计模式等，这需要研发人员对各技术有深刻的认识和理解。Recv线程的大体处理流程：

![img](https://pic3.zhimg.com/80/v2-ed998f1d6630395bbc09e9b680c4be0e_720w.webp)

图2 Recv线程处理流程

为了减少数据的复制，可以在接收数据开始时，Recv线程就为将要接收的数据从接收队列中分配一块空间。当Recv线程接收到一条完整的客户端数据后，则通过UNINX-UDP发送消息，告知某一Work线程到指定接收队列中取走数据进行处理。Recv线程通知Work线程的过程需要采用负载均衡策略。

**2) Work流程**

在无处理消息到来之前，一直处在阻塞状态，当有Recv线程的处理通知时，则接收消息内容，对消息进行分析，再根据消息的内容到指定的接收队列中取数据，再对数据进行相应的处理。其大体流程如下图所示：

![img](https://pic2.zhimg.com/80/v2-147f6517ec07214ce7b35ebb99030c49_720w.webp)

图3 Work线程处理流程

### 2.4 链路分发

说了这么多，但一直未提到各Recv线程是如何分配和获取客户端通信SOCKET的。众所周知，当一个线程通过TCP方式绑定指定端口后，其他线程或进程想再次绑定该端口时，必将返回错误。而如果让一个Listen套接字同时加入到多个Recv线程的select的可读集合进行监听，又会出现“惊群"现象：当有1个新的客户端连接请求到来时，所有的Recv线程都会被惊醒 —— 这显然是应该避免的。为了避免"惊群"的出现，通常有如下2种方案：

**方案1) 创建链路分发模块**

当有客户端连接请求过来时，该线程调用accept接收来自客户端的连接，再将新SOCKET-FD分发给某1个Recv线程，该Recv线程再将FD加入select的FD_SET中进行监听，从而实现Recv线程与客户端的通信。

**方案2) Lsn套接字流转侦听**

当一个线程/进程抢占到该Listen套接字后，该线程/进程将会开始接收来自客户端的连接请求和监听产生的套接字以及后续的数据接收等工作，当该线程接收的套接字超过一定量时，该线程将会主动放弃对Listen套接字的监听，而让其他线程/进程去抢占Listen套接字。NGINX采用的就是此方案。

在此采用的是方案1)的解决办法：Listen线程将接收的客户端请求产生的通信SOCKET均衡的分发给RECV线程[采用UNINX-UDP的方式发送]。其大体框架如下：



![img](https://pic2.zhimg.com/80/v2-6579d74d77393c9c97890d2171223049_720w.webp)

图4 链路分发

## 3.方案总结

以上设计方案适合客户端向服务端传输大量数据的场景，如果需要服务端反馈最终的处理结果，则需为Recv线程增加一个与之对应发送队列，在此不再赘述。总之，要做到高并发、高性能的网络通信系统，往往需要以下技术做支撑，这需要研发人员对以下技术拥有深刻的理解和认识，当然这还远远不够。

1)IO多路复用技术 2)非阻塞IO技术 3)事件驱动机制 4)线程池技术 5)负载均衡策略 6)内存池技术 7)缓存技术 8)锁技术 9)设计模式 10)高效算法和技巧的使用等等

原文地址：https://zhuanlan.zhihu.com/p/440032340

作者：CPP后端技术

# 【NO.600】C++开发常用的设计模式及其实现详解

软件领域中的设计模式为开发人员提供了一种使用专家设计经验的有效途径。设计模式中运用了面向对象编程语言的重要特性：封装、继承、多态，真正领悟设计模式的精髓是可能一个漫长的过程，需要大量实践经验的积累。最近看设计模式的书，对于每个模式，用C++写了个小例子，加深一下理解。

## 1.设计模式的分类

总体来说设计模式分为三大类

创建型模式，共五种：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。

结构型模式，共七种：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。

行为型模式，共十一种：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。

其实还有两类：并发型模式和线程池模式。

## 2.设计模式的六大原则

总原则：开闭原则（Open Close Principle）

开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，而是要扩展原有代码，实现一个热插拔的效果。所以一句话概括就是：为了使程序的扩展性好，易于维护和升级。想要达到这样的效果，我们需要使用接口和抽象类等，后面的具体设计中我们会提到这点。

1、单一职责原则

不要存在多于一个导致类变更的原因，也就是说每个类应该实现单一的职责，如若不然，就应该把类拆分。

2、里氏替换原则（Liskov Substitution Principle）

里氏代换原则(Liskov Substitution Principle LSP)面向对象设计的基本原则之一。 里氏代换原则中说，任何基类可以出现的地方，子类一定可以出现。 LSP是继承复用的基石，只有当衍生类可以替换掉基类，软件单位的功能不受到影响时，基类才能真正被复用，而衍生类也能够在基类的基础上增加新的行为。里氏代换原则是对“开-闭”原则的补充。实现“开-闭”原则的关键步骤就是抽象化。而基类与子类的继承关系就是抽象化的具体实现，所以里氏代换原则是对实现抽象化的具体步骤的规范。

历史替换原则中，子类对父类的方法尽量不要重写和重载。因为父类代表了定义好的结构，通过这个规范的接口与外界交互，子类不应该随便破坏它。

3、依赖倒转原则（Dependence Inversion Principle）

这个是开闭原则的基础，具体内容：面向接口编程，依赖于抽象而不依赖于具体。写代码时用到具体类时，不与具体类交互，而与具体类的上层接口交互。

4、接口隔离原则（Interface Segregation Principle）

这个原则的意思是：每个接口中不存在子类用不到却必须实现的方法，如果不然，就要将接口拆分。使用多个隔离的接口，比使用单个接口（多个接口方法集合到一个的接口）要好。

5、迪米特法则（最少知道原则）（Demeter Principle）

就是说：一个类对自己依赖的类知道的越少越好。也就是说无论被依赖的类多么复杂，都应该将逻辑封装在方法的内部，通过public方法提供给外部。这样当被依赖的类变化时，才能最小的影响该类。

最少知道原则的另一个表达方式是：只与直接的朋友通信。类之间只要有耦合关系，就叫朋友关系。耦合分为依赖、关联、聚合、组合等。我们称出现为成员变量、方法参数、方法返回值中的类为直接朋友。局部变量、临时变量则不是直接的朋友。我们要求陌生的类不要作为局部变量出现在类中。

6、合成复用原则（Composite Reuse Principle）

原则是尽量首先使用合成/聚合的方式，而不是使用继承。

## 3.工厂模式

工厂模式属于创建型模式，大致可以分为三类，简单工厂模式、工厂方法模式、抽象工厂模式。听上去差不多，都是工厂模式。下面一个个介绍，首先介绍简单工厂模式，它的主要特点是需要在工厂类中做判断，从而创造相应的产品。当增加新的产品时，就需要修改工厂类。有点抽象，举个例子就明白了。有一家生产处理器核的厂家，它只有一个工厂，能够生产两种型号的处理器核。客户需要什么样的处理器核，一定要显示地告诉生产工厂。下面给出一种实现方案。

```text
enum CTYPE {COREA, COREB};     
class SingleCore    
{    
public:    
    virtual void Show() = 0;  
};    
//单核A    
class SingleCoreA: public SingleCore    
{    
public:    
    void Show() { cout<<"SingleCore A"<<endl; }    
};    
//单核B    
class SingleCoreB: public SingleCore    
{    
public:    
    void Show() { cout<<"SingleCore B"<<endl; }    
};    
//唯一的工厂，可以生产两种型号的处理器核，在内部判断    
class Factory    
{    
public:     
    SingleCore* CreateSingleCore(enum CTYPE ctype)    
    {    
        if(ctype == COREA) //工厂内部判断    
            return new SingleCoreA(); //生产核A    
        else if(ctype == COREB)    
            return new SingleCoreB(); //生产核B    
        else    
            return NULL;    
    }    
};   
```

这样设计的主要缺点之前也提到过，就是要增加新的核类型时，就需要修改工厂类。这就违反了开放封闭原则：软件实体（类、模块、函数）可以扩展，但是不可修改。于是，工厂方法模式出现了。所谓工厂方法模式，是指定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method使一个类的实例化延迟到其子类。

听起来很抽象，还是以刚才的例子解释。这家生产处理器核的产家赚了不少钱，于是决定再开设一个工厂专门用来生产B型号的单核，而原来的工厂专门用来生产A型号的单核。这时，客户要做的是找好工厂，比如要A型号的核，就找A工厂要；否则找B工厂要，不再需要告诉工厂具体要什么型号的处理器核了。下面给出一个实现方案。

```text
class SingleCore    
{    
public:    
    virtual void Show() = 0;  
};    
//单核A    
class SingleCoreA: public SingleCore    
{    
public:    
    void Show() { cout<<"SingleCore A"<<endl; }    
};    
//单核B    
class SingleCoreB: public SingleCore    
{    
public:    
    void Show() { cout<<"SingleCore B"<<endl; }    
};    
class Factory    
{    
public:    
    virtual SingleCore* CreateSingleCore() = 0;  
};    
//生产A核的工厂    
class FactoryA: public Factory    
{    
public:    
    SingleCoreA* CreateSingleCore() { return new SingleCoreA; }    
};    
//生产B核的工厂    
class FactoryB: public Factory    
{    
public:    
    SingleCoreB* CreateSingleCore() { return new SingleCoreB; }    
};    
```

工厂方法模式也有缺点，每增加一种产品，就需要增加一个对象的工厂。如果这家公司发展迅速，推出了很多新的处理器核，那么就要开设相应的新工厂。在C++实现中，就是要定义一个个的工厂类。显然，相比简单工厂模式，工厂方法模式需要更多的类定义。

既然有了简单工厂模式和工厂方法模式，为什么还要有抽象工厂模式呢？它到底有什么作用呢？还是举这个例子，这家公司的技术不断进步，不仅可以生产单核处理器，也能生产多核处理器。现在简单工厂模式和工厂方法模式都鞭长莫及。抽象工厂模式登场了。它的定义为提供一个创建一系列相关或相互依赖对象的接口，而无需指定它们具体的类。具体这样应用，这家公司还是开设两个工厂，一个专门用来生产A型号的单核多核处理器，而另一个工厂专门用来生产B型号的单核多核处理器，下面给出实现的代码。

```text
//单核    
class SingleCore     
{    
public:    
    virtual void Show() = 0;  
};    
class SingleCoreA: public SingleCore      
{    
public:    
    void Show() { cout<<"Single Core A"<<endl; }    
};    
class SingleCoreB :public SingleCore    
{    
public:    
    void Show() { cout<<"Single Core B"<<endl; }    
};    
//多核    
class MultiCore      
{    
public:    
    virtual void Show() = 0;  
};    
class MultiCoreA : public MultiCore      
{    
public:    
    void Show() { cout<<"Multi Core A"<<endl; }    
    
};    
class MultiCoreB : public MultiCore      
{    
public:    
    void Show() { cout<<"Multi Core B"<<endl; }    
};    
//工厂    
class CoreFactory      
{    
public:    
    virtual SingleCore* CreateSingleCore() = 0;  
    virtual MultiCore* CreateMultiCore() = 0;  
};    
//工厂A，专门用来生产A型号的处理器    
class FactoryA :public CoreFactory    
{    
public:    
    SingleCore* CreateSingleCore() { return new SingleCoreA(); }    
    MultiCore* CreateMultiCore() { return new MultiCoreA(); }    
};    
//工厂B，专门用来生产B型号的处理器    
class FactoryB : public CoreFactory    
{    
public:    
    SingleCore* CreateSingleCore() { return new SingleCoreB(); }    
    MultiCore* CreateMultiCore() { return new MultiCoreB(); }    
}; 
```

至此，工厂模式介绍完了。给出三种工厂模式的UML图，加深印象。

简单工厂模式的UML图：

![img](https://pic2.zhimg.com/80/v2-b1b1dd7eeec87d2ee0a663f658cff4f5_720w.webp)

工厂方法的UML图：

![img](https://pic1.zhimg.com/80/v2-4945e4951f3768a27310045e58f0f35c_720w.webp)

抽象工厂模式的UML图：

![img](https://pic3.zhimg.com/80/v2-43f5e9cf7dfc87d51d95475387aa4b0a_720w.webp)

## 4.策略模式

策略模式是指定义一系列的算法，把它们一个个封装起来，并且使它们可相互替换。本模式使得算法可独立于使用它的客户而变化。也就是说这些算法所完成的功能一样，对外的接口一样，只是各自实现上存在差异。用策略模式来封装算法，效果比较好。下面以高速缓存（Cache）的替换算法为例，实现策略模式。

什么是Cache的替换算法呢？简单解释一下， 当发生Cache缺失时，Cache控制器必须选择Cache中的一行，并用欲获得的数据来替换它。所采用的选择策略就是Cache的替换算法。下面给出相应的UML图。

![img](https://pic4.zhimg.com/80/v2-0cfa4998967e04a41fa209ab6c4b9767_720w.webp)

ReplaceAlgorithm是一个抽象类，定义了算法的接口，有三个类继承自这个抽象类，也就是具体的算法实现。Cache类中需要使用替换算法，因此维护了一个 ReplaceAlgorithm的对象。这个UML图的结构就是策略模式的典型结构。下面根据UML图，给出相应的实现。

首先给出替换算法的定义。

```text
//抽象接口  
class ReplaceAlgorithm  
{  
public:  
    virtual void Replace() = 0;  
};  
//三种具体的替换算法  
class LRU_ReplaceAlgorithm : public ReplaceAlgorithm  
{  
public:  
    void Replace() { cout<<"Least Recently Used replace algorithm"<<endl; }  
};  
  
class FIFO_ReplaceAlgorithm : public ReplaceAlgorithm  
{  
public:  
    void Replace() { cout<<"First in First out replace algorithm"<<endl; }  
};  
class Random_ReplaceAlgorithm: public ReplaceAlgorithm  
{  
public:  
    void Replace() { cout<<"Random replace algorithm"<<endl; }  
};  
```

接着给出Cache的定义，这里很关键，Cache的实现方式直接影响了客户的使用方式，其关键在于如何指定替换算法。

方式一：直接通过参数指定，传入一个特定算法的指针。

```text
 //Cache需要用到替换算法  
class Cache  
{  
private:  
    ReplaceAlgorithm *m_ra;  
public:  
    Cache(ReplaceAlgorithm *ra) { m_ra = ra; }  
    ~Cache() { delete m_ra; }  
    void Replace() { m_ra->Replace(); }  
};  
```

如果用这种方式，客户就需要知道这些算法的具体定义。只能以下面这种方式使用，可以看到暴露了太多的细节。

```text
int main()  
{  
    Cache cache(new LRU_ReplaceAlgorithm()); //暴露了算法的定义  
    cache.Replace();  
    return 0;  
}  
```

方式二：也是直接通过参数指定，只不过不是传入指针，而是一个标签。这样客户只要知道算法的相应标签即可，而不需要知道算法的具体定义。

```text
//Cache需要用到替换算法  
enum RA {LRU, FIFO, RANDOM}; //标签  
class Cache  
{  
private:  
    ReplaceAlgorithm *m_ra;  
public:  
    Cache(enum RA ra)   
    {   
        if(ra == LRU)  
            m_ra = new LRU_ReplaceAlgorithm();  
        else if(ra == FIFO)  
            m_ra = new FIFO_ReplaceAlgorithm();  
        else if(ra == RANDOM)  
            m_ra = new Random_ReplaceAlgorithm();  
        else   
            m_ra = NULL;  
    }  
    ~Cache() { delete m_ra; }  
    void Replace() { m_ra->Replace(); }  
};  
```

相比方式一，这种方式用起来方便多了。其实这种方式将简单工厂模式与策略模式结合在一起，算法的定义使用了策略模式，而Cache的定义其实使用了简单工厂模式。

```text
int main()  
{  
    Cache cache(LRU); //指定标签即可  
    cache.Replace();  
    return 0;  
}  
```

上面两种方式，构造函数都需要形参。构造函数是否可以不用参数呢？下面给出第三种实现方式。

方式三：利用模板实现。算法通过模板的实参指定。当然了，还是使用了参数，只不过不是构造函数的参数。在策略模式中，参数的传递难以避免，客户必须指定某种算法。

```text
//Cache需要用到替换算法  
template <class RA>  
class Cache  
{  
private:  
    RA m_ra;  
public:  
    Cache() { }  
    ~Cache() { }  
    void Replace() { m_ra.Replace(); }  
};  
```

使用方式如下：

```text
int main()  
{  
    Cache<Random_ReplaceAlgorithm> cache; //模板实参  
    cache.Replace();  
    return 0;  
}  
```

## 5.适配器模式

DP上的定义：适配器模式将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。它包括类适配器和对象适配器，本文针对的是对象适配器。举个例子，在STL中就用到了适配器模式。STL实现了一种数据结构，称为双端队列（deque），支持前后两段的插入与删除。STL实现栈和队列时，没有从头开始定义它们，而是直接使用双端队列实现的。这里双端队列就扮演了适配器的角色。队列用到了它的后端插入，前端删除。而栈用到了它的后端插入，后端删除。假设栈和队列都是一种顺序容器，有两种操作：压入和弹出。下面给出相应的UML图，与DP上的图差不多。

![img](https://pic1.zhimg.com/80/v2-b436763668f2eb3e454482c2c993c5c8_720w.webp)

根据上面的UML图，很容易给出实现。

```text
//双端队列  
class Deque  
{  
public:  
    void push_back(int x) { cout<<"Deque push_back"<<endl; }  
    void push_front(int x) { cout<<"Deque push_front"<<endl; }  
    void pop_back() { cout<<"Deque pop_back"<<endl; }  
    void pop_front() { cout<<"Deque pop_front"<<endl; }  
};  
//顺序容器  
class Sequence  
{  
public:  
    virtual void push(int x) = 0;  
    virtual void pop() = 0;  
};  
//栈  
class Stack: public Sequence  
{  
public:  
    void push(int x) { deque.push_back(x); }  
    void pop() { deque.pop_back(); }  
private:  
    Deque deque; //双端队列  
};  
//队列  
class Queue: public Sequence  
{  
public:  
    void push(int x) { deque.push_back(x); }  
    void pop() { deque.pop_front(); }  
private:  
    Deque deque; //双端队列  
};  
```

使用方式如下：

```text
int main()  
{  
    Sequence *s1 = new Stack();  
    Sequence *s2 = new Queue();  
    s1->push(1); s1->pop();  
    s2->push(1); s2->pop();  
    delete s1; delete s2;  
    return 0;  
}  
```

## 6.单例模式

单例的一般实现比较简单，下面是代码和UML图。由于构造函数是私有的，因此无法通过构造函数实例化，唯一的方法就是通过调用静态函数GetInstance。

UML图：

![img](https://pic4.zhimg.com/80/v2-9f63c4803e2eda9bd45a4dc4235e1e43_720w.webp)

代码：

```text
//Singleton.h  
class Singleton    
{  
public:  
    static Singleton* GetInstance();  
private:  
    Singleton() {}  
    static Singleton *singleton;  
};  
//Singleton.cpp  
Singleton* Singleton::singleton = NULL;  
Singleton* Singleton::GetInstance()  
{  
    if(singleton == NULL)  
        singleton = new Singleton();  
    return singleton;  
}  
```

这里只有一个类，如何实现Singleton类的子类呢？也就说Singleton有很多子类，在一种应用中，只选择其中的一个。最容易就是在GetInstance函数中做判断，比如可以传递一个字符串，根据字符串的内容创建相应的子类实例。这也是DP书上的一种解法，书上给的代码不全。这里重新实现了一下，发现不是想象中的那么简单，最后实现的版本看上去很怪异。在VS2008下测试通过。

```text
//Singleton.h  
#pragma once  
#include <iostream>  
using namespace std;  
  
class Singleton    
{  
public:  
    static Singleton* GetInstance(const char* name);  
    virtual void Show() {}  
protected: //必须为保护，如果是私有属性，子类无法访问父类的构造函数  
    Singleton() {}  
private:  
    static Singleton *singleton; //唯一实例的指针  
};  
  
//Singleton.cpp  
#include "Singleton.h"  
#include "SingletonA.h"  
#include "SingletonB.h"  
Singleton* Singleton::singleton = NULL;  
Singleton* Singleton::GetInstance(const char* name)  
{  
    if(singleton == NULL)  
    {  
        if(strcmp(name, "SingletonA") == 0)  
            singleton = new SingletonA();  
        else if(strcmp(name,"SingletonB") == 0)  
            singleton = new SingletonB();  
        else   
            singleton = new Singleton();  
    }  
    return singleton;  
}  
```



```text
//SingletonA.h  
#pragma once  
#include "Singleton.h"  
class SingletonA: public Singleton  
{  
    friend class Singleton; //必须为友元类，否则父类无法访问子类的构造函数  
public:  
    void Show() { cout<<"SingletonA"<<endl; }  
private:   //为保护属性，这样外界无法通过构造函数进行实例化  
    SingletonA() {}   
};  
//SingletonB.h  
#pragma once  
#include "Singleton.h"  
class SingletonB: public Singleton  
{  
    friend class Singleton; //必须为友元类，否则父类无法访问子类的构造函数  
public:  
    void Show(){ cout<<"SingletonB"<<endl; }  
private:  //为保护属性，这样外界无法通过构造函数进行实例化  
    SingletonB() {}  
};  
```



```text
#include "Singleton.h"  
int main()  
{  
    Singleton *st = Singleton::GetInstance("SingletonA");  
    st->Show();  
    return 0;  
}  
```

上面代码有一个地方很诡异，父类为子类的友元，如果不是友元，函数GetInstance会报错，意思就是无法调用SingletonA和SIngletonB的构造函数。父类中调用子类的构造函数，我还是第一次碰到。当然了把SingletonA和SIngletonB的属性设为public，GetInstance函数就不会报错了，但是这样外界就可以定义这些类的对象，违反了单例模式。

看似奇怪，其实也容易解释。在父类中构建子类的对象，相当于是外界调用子类的构造函数，因此当子类构造函数的属性为私有或保护时，父类无法访问。为共有时，外界就可以访问子类的构造函数了，此时父类当然也能访问了。只不过为了保证单例模式，所以子类的构造函数不能为共有，但是又希望在父类中构造子类的对象，即需要调用子类的构造函数，这里没有办法才出此下策：将父类声明为子类的友元类。

## 7.原型模式、模板方法模式

DP书上的定义为：用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。其中有一个词很重要，那就是拷贝。可以说，拷贝是原型模式的精髓所在。举个现实中的例子来介绍原型模式。找工作的时候，我们需要准备简历。假设没有打印设备，因此需手写简历，这些简历的内容都是一样的。这样有个缺陷，如果要修改简历中的某项，那么所有已写好的简历都要修改，工作量很大。随着科技的进步，出现了打印设备。我们只需手写一份，然后利用打印设备复印多份即可。如果要修改简历中的某项，那么修改原始的版本就可以了，然后再复印。原始的那份手写稿相当于是一个原型，有了它，就可以通过复印（拷贝）创造出更多的新简历。这就是原型模式的基本思想。下面给出原型模式的UML图，以刚才那个例子为实例。

![img](https://pic4.zhimg.com/80/v2-de468f85270fe2affcdad7ac28805c4b_720w.webp)

原型模式实现的关键就是实现Clone函数，对于C++来说，其实就是拷贝构造函数，需实现深拷贝，下面给出一种实现。

```text
//父类  
class Resume  
{  
protected:  
    char *name;  
public:  
    Resume() {}  
    virtual ~Resume() {}  
    virtual Resume* Clone() { return NULL; }  
    virtual void Set(char *n) {}  
    virtual void Show() {}  
};  
```



```text
class ResumeA : public Resume  
{  
public:  
    ResumeA(const char *str);  //构造函数  
    ResumeA(const ResumeA &r); //拷贝构造函数  
    ~ResumeA();                //析构函数  
    ResumeA* Clone();          //克隆，关键所在  
    void Show();               //显示内容  
};  
ResumeA::ResumeA(const char *str)   
{  
    if(str == NULL) {  
        name = new char[1];   
        name[0] = '\0';   
    }  
    else {  
        name = new char[strlen(str)+1];  
        strcpy(name, str);  
    }  
}  
ResumeA::~ResumeA() { delete [] name;}  
ResumeA::ResumeA(const ResumeA &r) {  
    name = new char[strlen(r.name)+1];  
    strcpy(name, r.name);  
}  
ResumeA* ResumeA::Clone() {  
    return new ResumeA(*this);  
}  
void ResumeA::Show() {  
    cout<<"ResumeA name : "<<name<<endl;   
}  
```

这里只给出了ResumeA的实现，ResumeB的实现类似。使用的方式如下：

```text
int main()  
{  
    Resume *r1 = new ResumeA("A");  
    Resume *r2 = new ResumeB("B");  
    Resume *r3 = r1->Clone();  
    Resume *r4 = r2->Clone();  
    r1->Show(); r2->Show();  
    //删除r1,r2  
    delete r1; delete r2;     
    r1 = r2 = NULL;  
    //深拷贝所以对r3,r4无影响  
    r3->Show(); r4->Show();  
    delete r3; delete r4;  
    r3 = r4 = NULL;  
}  
```

最近有个招聘会，可以带上简历去应聘了。但是，其中有一家公司不接受简历，而是给应聘者发了一张简历表，上面有基本信息、教育背景、工作经历等栏，让应聘者按照要求填写完整。每个人拿到这份表格后，就开始填写。如果用程序实现这个过程，该如何做呢？一种方案就是用模板方法模式：定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤。我们的例子中，操作就是填写简历这一过程，我们可以在父类中定义操作的算法骨架，而具体的实现由子类完成。下面给出它的UML图。

![img](https://pic1.zhimg.com/80/v2-976317a74dadf36e98df50d18b7c9620_720w.webp)

其中FillResume() 定义了操作的骨架，依次调用子类实现的函数。相当于每个人填写简历的实际过程。接着给出相应的C++代码。

```text
//简历  
class Resume  
{  
protected: //保护成员  
    virtual void SetPersonalInfo() {}  
    virtual void SetEducation() {}  
    virtual void SetWorkExp() {}  
public:  
    void FillResume()   
    {  
        SetPersonalInfo();  
        SetEducation();  
        SetWorkExp();  
    }  
};  
class ResumeA: public Resume  
{  
protected:  
    void SetPersonalInfo() { cout<<"A's PersonalInfo"<<endl; }  
    void SetEducation() { cout<<"A's Education"<<endl; }  
    void SetWorkExp() { cout<<"A's Work Experience"<<endl; }  
};  
class ResumeB: public Resume  
{  
protected:  
    void SetPersonalInfo() { cout<<"B's PersonalInfo"<<endl; }  
    void SetEducation() { cout<<"B's Education"<<endl; }  
    void SetWorkExp() { cout<<"B's Work Experience"<<endl; }  
};  
```

使用方式如下：

```text
int main()  
{  
    Resume *r1;  
    r1 = new ResumeA();  
    r1->FillResume();  
    delete r1;  
    r1 = new ResumeB();  
    r1->FillResume();  
    delete r1;  
    r1 = NULL;  
    return 0;  
}  
```

## 8.建造者模式

建造者模式的定义将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示（DP）。《大话设计模式》举了一个很好的例子——建造小人，一共需建造6个部分，头部、身体、左右手、左右脚。与工厂模式不同，建造者模式是在导向者的控制下一步一步构造产品的。建造小人就是在控制下一步步构造出来的。创建者模式可以能更精细的控制构建过程，从而能更精细的控制所得产品的内部结构。下面给出建造者模式的UML图，以建造小人为实例。

![img](https://pic4.zhimg.com/80/v2-cdc5168d78848290c5d0f20d149b95e3_720w.webp)

对于客户来说，只需知道导向者就可以了，通过导向者，客户就能构造复杂的对象，而不需要知道具体的构造过程。下面给出小人例子的代码实现。

```text
class Builder    
{  
public:  
    virtual void BuildHead() {}  
    virtual void BuildBody() {}  
    virtual void BuildLeftArm(){}  
    virtual void BuildRightArm() {}  
    virtual void BuildLeftLeg() {}  
    virtual void BuildRightLeg() {}  
};  
//构造瘦人  
class ThinBuilder : public Builder  
{  
public:  
    void BuildHead() { cout<<"build thin body"<<endl; }  
    void BuildBody() { cout<<"build thin head"<<endl; }  
    void BuildLeftArm() { cout<<"build thin leftarm"<<endl; }  
    void BuildRightArm() { cout<<"build thin rightarm"<<endl; }  
    void BuildLeftLeg() { cout<<"build thin leftleg"<<endl; }  
    void BuildRightLeg() { cout<<"build thin rightleg"<<endl; }  
};  
//构造胖人  
class FatBuilder : public Builder  
{  
public:  
    void BuildHead() { cout<<"build fat body"<<endl; }  
    void BuildBody() { cout<<"build fat head"<<endl; }  
    void BuildLeftArm() { cout<<"build fat leftarm"<<endl; }  
    void BuildRightArm() { cout<<"build fat rightarm"<<endl; }  
    void BuildLeftLeg() { cout<<"build fat leftleg"<<endl; }  
    void BuildRightLeg() { cout<<"build fat rightleg"<<endl; }  
};  
//构造的指挥官  
class Director    
{  
private:  
    Builder *m_pBuilder;  
public:  
    Director(Builder *builder) { m_pBuilder = builder; }  
    void Create(){  
        m_pBuilder->BuildHead();  
        m_pBuilder->BuildBody();  
        m_pBuilder->BuildLeftArm();  
        m_pBuilder->BuildRightArm();  
        m_pBuilder->BuildLeftLeg();  
        m_pBuilder->BuildRightLeg();  
    }  
};  
```

客户的使用方式：

```text
int main()  int main()  
{  
    FatBuilder thin;  
    Director director(&thin);  
    director.Create();  
    return 0;  
}  
{  
    FatBuilder thin;  
    Director director(&thin);  
    director.Create();  
    return 0;  
}  
```

## 9.外观模式、组合模式

外观模式应该是用的很多的一种模式，特别是当一个系统很复杂时，系统提供给客户的是一个简单的对外接口，而把里面复杂的结构都封装了起来。客户只需使用这些简单接口就能使用这个系统，而不需要关注内部复杂的结构。DP一书的定义：为子系统中的一组接口提供一个一致的界面， 外观模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。举个编译器的例子，假设编译一个程序需要经过四个步骤：词法分析、语法分析、中间代码生成、机器码生成。学过编译都知道，每一步都很复杂。对于编译器这个系统，就可以使用外观模式。可以定义一个高层接口，比如名为Compiler的类，里面有一个名为Run的函数。客户只需调用这个函数就可以编译程序，至于Run函数内部的具体操作，客户无需知道。下面给出UML图，以编译器为实例。

![img](https://pic1.zhimg.com/80/v2-4750f1dc6fe3b747affc2e6e5bfa632c_720w.webp)

相应的代码实现为：

```text
class Scanner  
{  
public:  
    void Scan() { cout<<"词法分析"<<endl; }  
};  
class Parser  
{  
public:  
    void Parse() { cout<<"语法分析"<<endl; }  
};  
class GenMidCode  
{  
public:  
    void GenCode() { cout<<"产生中间代码"<<endl; }  
};  
class GenMachineCode  
{  
public:  
    void GenCode() { cout<<"产生机器码"<<endl;}  
};  
//高层接口  
class Compiler  
{  
public:  
    void Run()   
    {  
        Scanner scanner;  
        Parser parser;  
        GenMidCode genMidCode;  
        GenMachineCode genMacCode;  
        scanner.Scan();  
        parser.Parse();  
        genMidCode.GenCode();  
        genMacCode.GenCode();  
    }  
};  
```

客户使用方式：

```text
int main()  
{  
    Compiler compiler;  
    Console Compiler();  
    return 0;  
}  
```

这就是外观模式，它有几个特点（摘自DP一书），（1）它对客户屏蔽子系统组件，因而减少了客户处理的对象的数目并使得子系统使用起来更加方便。（2）它实现了子系统与客户之间的松耦合关系，而子系统内部的功能组件往往是紧耦合的。（3）如果应用需要，它并不限制它们使用子系统类。

结合上面编译器这个例子，进一步说明。对于（1），编译器类对客户屏蔽了子系统组件，客户只需处理编译器的对象就可以方便的使用子系统。对于（2），子系统的变化，不会影响到客户的使用，体现了子系统与客户的松耦合关系。对于（3），如果客户希望使用词法分析器，只需定义词法分析的类对象即可，并不受到限制。

外观模式在构建大型系统时非常有用。接下来介绍另一种模式，称为组合模式。感觉有点像外观模式，刚才我们实现外观模式时，在Compiler这个类中包含了多个类的对象，就像把这些类组合在了一起。组合模式是不是这个意思，有点相似，其实不然。

DP书上给出的定义：将对象组合成树形结构以表示“部分-整体”的层次结构。组合使得用户对单个对象和组合对象的使用具有一致性。注意两个字“树形”。这种树形结构在现实生活中随处可见，比如一个集团公司，它有一个母公司，下设很多家子公司。不管是母公司还是子公司，都有各自直属的财务部、人力资源部、销售部等。对于母公司来说，不论是子公司，还是直属的财务部、人力资源部，都是它的部门。整个公司的部门拓扑图就是一个树形结构。

下面给出组合模式的UML图。从图中可以看到，FinanceDepartment、HRDepartment两个类作为叶结点，因此没有定义添加函数。而ConcreteCompany类可以作为中间结点，所以可以有添加函数。那么怎么添加呢？这个类中定义了一个链表，用来放添加的元素。

![img](https://pic4.zhimg.com/80/v2-5c095155528ea3fe7ea25af365c71d47_720w.webp)

相应的代码实现为：

```text
class Company    
{  
public:  
    Company(string name) { m_name = name; }  
    virtual ~Company(){}  
    virtual void Add(Company *pCom){}  
    virtual void Show(int depth) {}  
protected:  
    string m_name;  
};  
//具体公司  
class ConcreteCompany : public Company    
{  
public:  
    ConcreteCompany(string name): Company(name) {}  
    virtual ~ConcreteCompany() {}  
    void Add(Company *pCom) { m_listCompany.push_back(pCom); } //位于树的中间，可以增加子树  
    void Show(int depth)  
    {  
        for(int i = 0;i < depth; i++)  
            cout<<"-";  
        cout<<m_name<<endl;  
        list<Company *>::iterator iter=m_listCompany.begin();  
        for(; iter != m_listCompany.end(); iter++) //显示下层结点  
            (*iter)->Show(depth + 2);  
    }  
private:  
    list<Company *> m_listCompany;  
};  
//具体的部门，财务部  
class FinanceDepartment : public Company   
{  
public:  
    FinanceDepartment(string name):Company(name){}  
    virtual ~FinanceDepartment() {}  
    virtual void Show(int depth) //只需显示，无限添加函数，因为已是叶结点  
    {  
        for(int i = 0; i < depth; i++)  
            cout<<"-";  
        cout<<m_name<<endl;  
    }  
};  
//具体的部门，人力资源部  
class HRDepartment :public Company    
{  
public:  
    HRDepartment(string name):Company(name){}  
    virtual ~HRDepartment() {}  
    virtual void Show(int depth) //只需显示，无限添加函数，因为已是叶结点  
    {  
        for(int i = 0; i < depth; i++)  
            cout<<"-";  
        cout<<m_name<<endl;  
    }  
};  
```

客户使用方式：

```text
int main()  
{  
    Company *root = new ConcreteCompany("总公司");  
    Company *leaf1=new FinanceDepartment("财务部");  
    Company *leaf2=new HRDepartment("人力资源部");  
    root->Add(leaf1);  
    root->Add(leaf2);  
  
    //分公司A  
    Company *mid1 = new ConcreteCompany("分公司A");  
    Company *leaf3=new FinanceDepartment("财务部");  
    Company *leaf4=new HRDepartment("人力资源部");  
    mid1->Add(leaf3);  
    mid1->Add(leaf4);  
    root->Add(mid1);  
    //分公司B  
    Company *mid2=new ConcreteCompany("分公司B");  
    FinanceDepartment *leaf5=new FinanceDepartment("财务部");  
    HRDepartment *leaf6=new HRDepartment("人力资源部");  
    mid2->Add(leaf5);  
    mid2->Add(leaf6);  
    root->Add(mid2);  
    root->Show(0);  
  
    delete leaf1; delete leaf2;  
    delete leaf3; delete leaf4;  
    delete leaf5; delete leaf6;   
    delete mid1; delete mid2;  
    delete root;  
    return 0;  
}  
```

上面的实现方式有缺点，就是内存的释放不好，需要客户自己动手，非常不方便。有待改进，比较好的做法是让ConcreteCompany类来释放。因为所有的指针都是存在ConcreteCompany类的链表中。C++的麻烦，没有垃圾回收机制。

## 10.代理模式

[DP]上的定义：为其他对象提供一种代理以控制对这个对象的访问。有四种常用的情况：（1）远程代理，（2）虚代理，（3）保护代理，（4）智能引用。本文主要介绍虚代理和智能引用两种情况。

考虑一个可以在文档中嵌入图形对象的文档编辑器。有些图形对象的创建开销很大。但是打开文档必须很迅速，因此我们在打开文档时应避免一次性创建所有开销很大的对象。这里就可以运用代理模式，在打开文档时，并不打开图形对象，而是打开图形对象的代理以替代真实的图形。待到真正需要打开图形时，仍由代理负责打开。这是[DP]一书上的给的例子。下面给出代理模式的UML图。

![img](https://pic2.zhimg.com/80/v2-a5e74a011b8064113a04058c46cd385d_720w.webp)

简单实现如下：

```text
class Image  
{  
public:  
    Image(string name): m_imageName(name) {}  
    virtual ~Image() {}  
    virtual void Show() {}  
protected:  
    string m_imageName;  
};  
class BigImage: public Image  
{  
public:  
    BigImage(string name):Image(name) {}  
    ~BigImage() {}  
    void Show() { cout<<"Show big image : "<<m_imageName<<endl; }  
};  
class BigImageProxy: public Image  
{  
private:  
    BigImage *m_bigImage;  
public:  
    BigImageProxy(string name):Image(name),m_bigImage(0) {}  
    ~BigImageProxy() { delete m_bigImage; }  
    void Show()   
    {  
        if(m_bigImage == NULL)  
            m_bigImage = new BigImage(m_imageName);  
        m_bigImage->Show();  
    }  
};  
```

客户调用：

```text
int main()  
{  
    Image *image = new BigImageProxy("proxy.jpg"); //代理  
    image->Show(); //需要时由代理负责打开  
    delete image;  
    return 0;  
}  
```

在这个例子属于虚代理的情况，下面给两个智能引用的例子。一个是C++中的auto_ptr，另一个是smart_ptr。自己实现了一下。先给出auto_ptr的代码实现：

```text
template<class T>    
class auto_ptr {    
public:    
    explicit auto_ptr(T *p = 0): pointee(p) {}    
    auto_ptr(auto_ptr<T>& rhs): pointee(rhs.release()) {}    
    ~auto_ptr() { delete pointee; }    
    auto_ptr<T>& operator=(auto_ptr<T>& rhs)    
    {    
        if (this != &rhs) reset(rhs.release());    
        return *this;    
    }    
    T& operator*() const { return *pointee; }    
    T* operator->() const { return pointee; }    
    T* get() const { return pointee; }    
    T* release()    
    {    
        T *oldPointee = pointee;    
        pointee = 0;    
        return oldPointee;    
    }    
    void reset(T *p = 0)    
    {    
        if (pointee != p) {    
               delete pointee;    
               pointee = p;    
            }    
        }    
private:    
    T *pointee;    
};    
```

阅读上面的代码，我们可以发现 auto_ptr 类就是一个代理，客户只需操作auto_prt的对象，而不需要与被代理的指针pointee打交道。auto_ptr 的好处在于为动态分配的对象提供异常安全。因为它用一个对象存储需要被自动释放的资源，然后依靠对象的析构函数来释放资源。这样客户就不需要关注资源的释放，由auto_ptr 对象自动完成。实现中的一个关键就是重载了解引用操作符和箭头操作符，从而使得auto_ptr的使用与真实指针类似。

我们知道C++中没有垃圾回收机制，可以通过智能指针来弥补，下面给出智能指针的一种实现，采用了引用计数的策略。

```text
template <typename T>  
class smart_ptr  
{  
public:  
    smart_ptr(T *p = 0): pointee(p), count(new size_t(1)) { }  //初始的计数值为1  
    smart_ptr(const smart_ptr &rhs): pointee(rhs.pointee), count(rhs.count) { ++*count; } //拷贝构造函数，计数加1  
    ~smart_ptr() { decr_count(); }              //析构，计数减1，减到0时进行垃圾回收，即释放空间  
    smart_ptr& operator= (const smart_ptr& rhs) //重载赋值操作符  
    {  
        //给自身赋值也对，因为如果自身赋值，计数器先减1，再加1，并未发生改变  
        ++*count;  
        decr_count();  
        pointee = rhs.pointee;  
        count = rhs.count;  
        return *this;  
    }    
    //重载箭头操作符和解引用操作符，未提供指针的检查  
    T *operator->() { return pointee; }  
    const T *operator->() const { return pointee; }  
    T &operator*() { return *pointee; }  
    const T &operator*() const { return *pointee; }  
    size_t get_refcount() { return *count; } //获得引用计数器值  
private:   
    T *pointee;       //实际指针，被代理    
    size_t *count;    //引用计数器  
    void decr_count() //计数器减1  
    {  
        if(--*count == 0)   
        {  
            delete pointee;  
            delete count;  
        }  
    }  
};  
```

## 11.享元模式

举个围棋的例子，围棋的棋盘共有361格，即可放361个棋子。现在要实现一个围棋程序，该怎么办呢？首先要考虑的是棋子棋盘的实现，可以定义一个棋子的类，成员变量包括棋子的颜色、形状、位置等信息，另外再定义一个棋盘的类，成员变量中有个容器，用于存放棋子的对象。下面给出代码表示：

棋子的定义，当然棋子的属性除了颜色和位置，还有其他的，这里略去。这两个属性足以说明问题。

```text
//棋子颜色  
enum PieceColor {BLACK, WHITE};  
//棋子位置  
struct PiecePos  
{  
    int x;  
    int y;  
    PiecePos(int a, int b): x(a), y(b) {}  
};  
//棋子定义  
class Piece  
{  
protected:  
    PieceColor m_color; //颜色  
    PiecePos m_pos;     //位置  
public:  
    Piece(PieceColor color, PiecePos pos): m_color(color), m_pos(pos) {}  
    ~Piece() {}  
    virtual void Draw() {}  
};  
class BlackPiece: public Piece  
{  
public:  
    BlackPiece(PieceColor color, PiecePos pos): Piece(color, pos) {}  
    ~BlackPiece() {}  
    void Draw() { cout<<"绘制一颗黑棋"<<endl;}  
};  
class WhitePiece: public Piece  
{  
public:  
    WhitePiece(PieceColor color, PiecePos pos): Piece(color, pos) {}  
    ~WhitePiece() {}  
    void Draw() { cout<<"绘制一颗白棋"<<endl;}  
};  
```

棋盘的定义：

```text
class PieceBoard  
{  
private:  
    vector<Piece*> m_vecPiece; //棋盘上已有的棋子  
    string m_blackName; //黑方名称  
    string m_whiteName; //白方名称  
public:  
    PieceBoard(string black, string white): m_blackName(black), m_whiteName(white){}  
    ~PieceBoard() { Clear(); }  
    void SetPiece(PieceColor color, PiecePos pos) //一步棋，在棋盘上放一颗棋子  
    {  
        Piece * piece = NULL;  
        if(color == BLACK) //黑方下的  
        {     
            piece = new BlackPiece(color, pos); //获取一颗黑棋  
            cout<<m_blackName<<"在位置("<<pos.x<<','<<pos.y<<")";  
            piece->Draw(); //在棋盘上绘制出棋子  
        }  
        else  
        {     
            piece = new WhitePiece(color, pos);  
            cout<<m_whiteName<<"在位置("<<pos.x<<','<<pos.y<<")";  
            piece->Draw();  
        }  
        m_vecPiece.push_back(piece);  //加入容器中  
    }  
    void Clear() //释放内存  
    {  
        int size = m_vecPiece.size();  
        for(int i = 0; i < size; i++)  
            delete m_vecPiece[i];  
    }  
};  
```

客户的使用方式如下：

```text
int main()  
{  
    PieceBoard pieceBoard("A","B");  
    pieceBoard.SetPiece(BLACK, PiecePos(4, 4));  
    pieceBoard.SetPiece(WHITE, PiecePos(4, 16));  
    pieceBoard.SetPiece(BLACK, PiecePos(16, 4));  
    pieceBoard.SetPiece(WHITE, PiecePos(16, 16));  
}  
```

可以发现，棋盘的容器中存放了已下的棋子，而每个棋子包含棋子的所有属性。一盘棋往往需要含上百颗棋子，采用上面这种实现，占用的空间太大了。如何改进呢？用享元模式。其定义为：运用共享技术有效地支持大量细粒度的对象。

在围棋中，棋子就是大量细粒度的对象。其属性有内在的，比如颜色、形状等，也有外在的，比如在棋盘上的位置。内在的属性是可以共享的，区分在于外在属性。因此，可以这样设计，只需定义两个棋子的对象，一颗黑棋和一颗白棋，这两个对象含棋子的内在属性；棋子的外在属性，即在棋盘上的位置可以提取出来，存放在单独的容器中。相比之前的方案，现在容器中仅仅存放了位置属性，而原来则是棋子对象。显然，现在的方案大大减少了对于空间的需求。

关注PieceBoard 的容器，之前是vector<Piece*> m_vecPiece，现在是vector<PiecePos> m_vecPos。这里是关键。

棋子的新定义，只包含内在属性：

```text
//棋子颜色  
enum PieceColor {BLACK, WHITE};  
//棋子位置  
struct PiecePos  
{  
    int x;  
    int y;  
    PiecePos(int a, int b): x(a), y(b) {}  
};  
//棋子定义  
class Piece  
{  
protected:  
    PieceColor m_color; //颜色  
public:  
    Piece(PieceColor color): m_color(color) {}  
    ~Piece() {}  
    virtual void Draw() {}  
};  
class BlackPiece: public Piece  
{  
public:  
    BlackPiece(PieceColor color): Piece(color) {}  
    ~BlackPiece() {}  
    void Draw() { cout<<"绘制一颗黑棋\n"; }  
};  
class WhitePiece: public Piece  
{  
public:  
    WhitePiece(PieceColor color): Piece(color) {}  
    ~WhitePiece() {}  
    void Draw() { cout<<"绘制一颗白棋\n";}  
};  
```

相应棋盘的定义为：

```text
class PieceBoard  
{  
private:  
    vector<PiecePos> m_vecPos; //存放棋子的位置  
    Piece *m_blackPiece;       //黑棋棋子   
    Piece *m_whitePiece;       //白棋棋子  
    string m_blackName;  
    string m_whiteName;  
public:  
    PieceBoard(string black, string white): m_blackName(black), m_whiteName(white)  
    {  
        m_blackPiece = NULL;  
        m_whitePiece = NULL;  
    }  
    ~PieceBoard() { delete m_blackPiece; delete m_whitePiece;}  
    void SetPiece(PieceColor color, PiecePos pos)  
    {  
        if(color == BLACK)  
        {  
            if(m_blackPiece == NULL)  //只有一颗黑棋  
                m_blackPiece = new BlackPiece(color);     
            cout<<m_blackName<<"在位置("<<pos.x<<','<<pos.y<<")";  
            m_blackPiece->Draw();  
        }  
        else  
        {  
            if(m_whitePiece == NULL)  
                m_whitePiece = new WhitePiece(color);  
            cout<<m_whiteName<<"在位置("<<pos.x<<','<<pos.y<<")";  
            m_whitePiece->Draw();  
        }  
        m_vecPos.push_back(pos);  
    }  
};  
```

客户的使用方式一样，这里不重复给出，现在给出享元模式的UML图，以围棋为例。棋盘中含两个共享的对象，黑棋子和白棋子，所有棋子的外在属性都存放在单独的容器中。

![img](https://pic4.zhimg.com/80/v2-00dae361536f9703272007e95d130a13_720w.webp)

## 12.桥接模式

[DP]书上定义：将抽象部分与它的实现部分分离，使它们都可以独立地变化。考虑装操作系统，有多种配置的计算机，同样也有多款操作系统。如何运用桥接模式呢？可以将操作系统和计算机分别抽象出来，让它们各自发展，减少它们的耦合度。当然了，两者之间有标准的接口。这样设计，不论是对于计算机，还是操作系统都是非常有利的。下面给出这种设计的UML图，其实就是桥接模式的UML图。

![img](https://pic2.zhimg.com/80/v2-0f82c904c73dc1f182302dafdd359245_720w.webp)

给出C++的一种实现：

```text
//操作系统  
class OS  
{  
public:  
    virtual void InstallOS_Imp() {}  
};  
class WindowOS: public OS  
{  
public:  
    void InstallOS_Imp() { cout<<"安装Window操作系统"<<endl; }   
};  
class LinuxOS: public OS  
{  
public:  
    void InstallOS_Imp() { cout<<"安装Linux操作系统"<<endl; }   
};  
class UnixOS: public OS  
{  
public:  
    void InstallOS_Imp() { cout<<"安装Unix操作系统"<<endl; }   
};  
//计算机  
class Computer  
{  
public:  
    virtual void InstallOS(OS *os) {}  
};  
class DellComputer: public Computer  
{  
public:  
    void InstallOS(OS *os) { os->InstallOS_Imp(); }  
};  
class AppleComputer: public Computer  
{  
public:  
    void InstallOS(OS *os) { os->InstallOS_Imp(); }  
};  
class HPComputer: public Computer  
{  
public:  
    void InstallOS(OS *os) { os->InstallOS_Imp(); }  
};  
```

客户使用方式：

```text
int main()  
{  
    OS *os1 = new WindowOS();  
    OS *os2 = new LinuxOS();  
    Computer *computer1 = new AppleComputer();  
    computer1->InstallOS(os1);  
    computer1->InstallOS(os2);  
}  
```

## 13.装饰模式

装饰模式：动态地给一个对象添加一些额外的职责。就增加功能来说，装饰模式相比生成子类更为灵活。有时我们希望给某个对象而不是整个类添加一些功能。比如有一个手机，允许你为手机添加特性，比如增加挂件、屏幕贴膜等。一种灵活的设计方式是，将手机嵌入到另一对象中，由这个对象完成特性的添加，我们称这个嵌入的对象为装饰。这个装饰与它所装饰的组件接口一致，因此它对使用该组件的客户透明。下面给出装饰模式的UML图。

![img](https://pic4.zhimg.com/80/v2-7320ae7a1ff181934ced3d02bd8ac2a7_720w.webp)

在这种设计中，手机的装饰功能被独立出来，可以单独发展，进而简化了具体手机类的设计。下面给出Phone类的实现：

```text
//公共抽象类  
class Phone  
{  
public:  
    Phone() {}  
    virtual ~Phone() {}  
    virtual void ShowDecorate() {}  
};  
```

具体的手机类的定义：

```text
//具体的手机类  
class iPhone : public Phone  
{  
private:  
    string m_name; //手机名称  
public:  
    iPhone(string name): m_name(name){}  
    ~iPhone() {}  
    void ShowDecorate() { cout<<m_name<<"的装饰"<<endl;}  
};  
//具体的手机类  
class NokiaPhone : public Phone  
{  
private:  
    string m_name;  
public:  
    NokiaPhone(string name): m_name(name){}  
    ~NokiaPhone() {}  
    void ShowDecorate() { cout<<m_name<<"的装饰"<<endl;}  
};  
```

装饰类的实现：

```text
//装饰类  
class DecoratorPhone : public Phone  
{  
private:  
    Phone *m_phone;  //要装饰的手机  
public:  
    DecoratorPhone(Phone *phone): m_phone(phone) {}  
    virtual void ShowDecorate() { m_phone->ShowDecorate(); }  
};  
//具体的装饰类  
class DecoratorPhoneA : public DecoratorPhone  
{  
public:  
    DecoratorPhoneA(Phone *phone) : DecoratorPhone(phone) {}  
    void ShowDecorate() { DecoratorPhone::ShowDecorate(); AddDecorate(); }  
private:  
    void AddDecorate() { cout<<"增加挂件"<<endl; } //增加的装饰  
};  
//具体的装饰类  
class DecoratorPhoneB : public DecoratorPhone  
{  
public:  
    DecoratorPhoneB(Phone *phone) : DecoratorPhone(phone) {}  
    void ShowDecorate() { DecoratorPhone::ShowDecorate(); AddDecorate(); }  
private:  
    void AddDecorate() { cout<<"屏幕贴膜"<<endl; } //增加的装饰  
};  
```

客户使用方式：

```text
int main()  
{  
    Phone *iphone = new NokiaPhone("6300");  
    Phone *dpa = new DecoratorPhoneA(iphone); //装饰，增加挂件  
    Phone *dpb = new DecoratorPhoneB(dpa);    //装饰，屏幕贴膜  
    dpb->ShowDecorate();  
    delete dpa;  
    delete dpb;  
    delete iphone;  
    return 0;  
}  
```

装饰模式提供了更加灵活的向对象添加职责的方式。可以用添加和分离的方法，用装饰在运行时刻增加和删除职责。装饰模式提供了一种“即用即付”的方法来添加职责。它并不试图在一个复杂的可定制的类中支持所有可预见的特征，相反，你可以定义一个简单的类，并且用装饰类给它逐渐地添加功能。可以从简单的部件组合出复杂的功能

## 14.备忘录模式

备忘录模式：在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样以后就可将该对象恢复到原先保存的状态[DP]。举个简单的例子，我们玩游戏时都会保存进度，所保存的进度以文件的形式存在。这样下次就可以继续玩，而不用从头开始。这里的进度其实就是游戏的内部状态，而这里的文件相当于是在游戏之外保存状态。这样，下次就可以从文件中读入保存的进度，从而恢复到原来的状态。这就是备忘录模式。

给出备忘录模式的UML图，以保存游戏的进度为例。

![img](https://pic4.zhimg.com/80/v2-d540b88bc98159d7a88b26a46458decb_720w.webp)

Memento类定义了内部的状态，而Caretake类是一个保存进度的管理者，GameRole类是游戏角色类。可以看到GameRole的对象依赖于Memento对象，而与Caretake对象无关。下面给出一个简单的是实现。

```text
//需保存的信息  
class Memento    
{  
public:  
    int m_vitality; //生命值  
    int m_attack;   //进攻值  
    int m_defense;  //防守值  
public:  
    Memento(int vitality, int attack, int defense):   
      m_vitality(vitality),m_attack(attack),m_defense(defense){}  
    Memento& operator=(const Memento &memento)   
    {  
        m_vitality = memento.m_vitality;  
        m_attack = memento.m_attack;  
        m_defense = memento.m_defense;  
        return *this;  
    }  
};  
//游戏角色  
class GameRole    
{  
private:  
    int m_vitality;  
    int m_attack;  
    int m_defense;  
public:  
    GameRole(): m_vitality(100),m_attack(100),m_defense(100) {}  
    Memento Save()  //保存进度，只与Memento对象交互，并不牵涉到Caretake  
    {   
        Memento memento(m_vitality, m_attack, m_defense);  
        return memento;  
    }  
    void Load(Memento memento)  //载入进度，只与Memento对象交互，并不牵涉到Caretake  
    {  
        m_vitality = memento.m_vitality;  
        m_attack = memento.m_attack;   
        m_defense = memento.m_defense;  
    }  
    void Show() { cout<<"vitality : "<< m_vitality<<", attack : "<< m_attack<<", defense : "<< m_defense<<endl; }  
    void Attack() { m_vitality -= 10; m_attack -= 10;  m_defense -= 10; }  
};  
//保存的进度库  
class Caretake    
{  
public:  
    Caretake() {}  
    void Save(Memento menento) { m_vecMemento.push_back(menento); }  
    Memento Load(int state) { return m_vecMemento[state]; }  
private:  
    vector<Memento> m_vecMemento;  
};  
```

客户使用方式：

```text
//测试案例  
int main()  
{     
    Caretake caretake;  
    GameRole role;   
    role.Show();   //初始值  
    caretake.Save(role.Save()); //保存状态  
    role.Attack();     
    role.Show();  //进攻后  
    role.Load(caretake.Load(0)); //载入状态   
    role.Show();  //恢复到状态0  
    return 0;  
}  
```

## 15.中介者模式

中介者模式：用一个中介对象来封装一系列的对象交互。中介者使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。中介者模式的例子很多，大到联合国安理会，小到房屋中介，都扮演了中间者的角色，协调各方利益。

本文就以租房为例子，如果没有房屋中介，那么房客要自己找房东，而房东也要自己找房客，非常不方便。有了房屋中介机构就方便了，房东可以把要出租的房屋信息放到中介机构，而房客可以去中介机构咨询。在软件中，就是多个对象之间需要通信，如果没有中介，对象就需要知道其他对象，最坏情况下，可能需要知道所有其他对象，而有了中介对象就方便多了，对象只需与中介对象通信，而不用知道其他的对象。这就是中介者模式，下面以租房为例，给出中介者模式的UML图。

![img](https://pic2.zhimg.com/80/v2-b186359760b39e81db4614a741271019_720w.webp)

实现不难，下面给出C++的实现：

```text
class Mediator;  
//抽象人  
class Person  
{  
protected:  
    Mediator *m_mediator; //中介  
public:  
    virtual void SetMediator(Mediator *mediator){} //设置中介  
    virtual void SendMessage(string message) {}    //向中介发送信息  
    virtual void GetMessage(string message) {}     //从中介获取信息  
};  
//抽象中介机构  
class Mediator  
{  
public:  
    virtual void Send(string message, Person *person) {}  
    virtual void SetA(Person *A) {}  //设置其中一方  
    virtual void SetB(Person *B) {}  
};  
//租房者  
class Renter: public Person  
{  
public:  
    void SetMediator(Mediator *mediator) { m_mediator = mediator; }  
    void SendMessage(string message) { m_mediator->Send(message, this); }  
    void GetMessage(string message) { cout<<"租房者收到信息"<<message; }  
};  
//房东  
class Landlord: public Person  
{  
public:  
    void SetMediator(Mediator *mediator) { m_mediator = mediator; }  
    void SendMessage(string message) { m_mediator->Send(message, this); }  
    void GetMessage(string message) { cout<<"房东收到信息："<<message; }  
};  
//房屋中介  
class HouseMediator : public Mediator  
{  
private:  
    Person *m_A; //租房者  
    Person *m_B; //房东  
public:  
    HouseMediator(): m_A(0), m_B(0) {}  
    void SetA(Person *A) { m_A = A; }  
    void SetB(Person *B) { m_B = B; }  
    void Send(string message, Person *person)   
    {  
        if(person == m_A) //租房者给房东发信息  
            m_B->GetMessage(message); //房东收到信息  
        else  
            m_A->GetMessage(message);  
    }  
};  
```

客户使用方式如下：

```text
//测试案例  
int main()  
{     
    Mediator *mediator = new HouseMediator();  
    Person *person1 = new Renter();    //租房者  
    Person *person2 = new Landlord();  //房东  
    mediator->SetA(person1);  
    mediator->SetB(person2);  
    person1->SetMediator(mediator);  
    person2->SetMediator(mediator);  
    person1->SendMessage("我想在南京路附近租套房子，价格800元一个月\n");  
    person2->SendMessage("出租房子：南京路100号，70平米，1000元一个月\n");  
    delete person1; delete person2; delete mediator;  
    return 0;  
}  
```

## 16.职责链模式

职责链模式：使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。其思想很简单，考虑员工要求加薪。公司的管理者一共有三级，总经理、总监、经理，如果一个员工要求加薪，应该向主管的经理申请，如果加薪的数量在经理的职权内，那么经理可以直接批准，否则将申请上交给总监。总监的处理方式也一样，总经理可以处理所有请求。这就是典型的职责链模式，请求的处理形成了一条链，直到有一个对象处理请求。给出这个例子的UML图。

![img](https://pic3.zhimg.com/80/v2-0b8c1ce17c8f0b5a4b98cb1b68ba58be_720w.webp)

代码的实现比较简单，如下所示：

```text
//抽象管理者  
class Manager  
{  
protected:  
    Manager *m_manager;  
    string m_name;  
public:  
    Manager(Manager *manager, string name):m_manager(manager), m_name(name){}  
    virtual void DealWithRequest(string name, int num)  {}  
};  
//经理  
class CommonManager: public Manager  
{  
public:  
    CommonManager(Manager *manager, string name):Manager(manager,name) {}  
    void DealWithRequest(string name, int num)   
    {  
        if(num < 500) //经理职权之内  
        {  
            cout<<"经理"<<m_name<<"批准"<<name<<"加薪"<<num<<"元"<<endl<<endl;  
        }  
        else  
        {  
            cout<<"经理"<<m_name<<"无法处理，交由总监处理"<<endl;  
            m_manager->DealWithRequest(name, num);  
        }  
    }  
};  
//总监  
class Majordomo: public Manager  
{  
public:  
    Majordomo(Manager *manager, string name):Manager(manager,name) {}  
    void DealWithRequest(string name, int num)   
    {  
        if(num < 1000) //总监职权之内  
        {  
            cout<<"总监"<<m_name<<"批准"<<name<<"加薪"<<num<<"元"<<endl<<endl;  
        }  
        else  
        {  
            cout<<"总监"<<m_name<<"无法处理，交由总经理处理"<<endl;  
            m_manager->DealWithRequest(name, num);  
        }  
    }  
};  
//总经理  
class GeneralManager: public Manager  
{  
public:  
    GeneralManager(Manager *manager, string name):Manager(manager,name) {}  
    void DealWithRequest(string name, int num)  //总经理可以处理所有请求  
    {  
        cout<<"总经理"<<m_name<<"批准"<<name<<"加薪"<<num<<"元"<<endl<<endl;  
    }  
};  
```

客户调用方式为：

```text
//测试案例  
int main()  
{     
    Manager *general = new GeneralManager(NULL, "A"); //设置上级，总经理没有上级  
    Manager *majordomo = new Majordomo(general, "B"); //设置上级  
    Manager *common = new CommonManager(majordomo, "C"); //设置上级  
    common->DealWithRequest("D",300);   //员工D要求加薪  
    common->DealWithRequest("E", 600);  
    common->DealWithRequest("F", 1000);  
    delete common; delete majordomo; delete general;  
    return 0;  
}
```

## 17.观察者模式

观察者模式：定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。它还有两个别名，依赖(Dependents)，发布-订阅(Publish-Subsrcibe)。可以举个博客订阅的例子，当博主发表新文章的时候，即博主状态发生了改变，那些订阅的读者就会收到通知，然后进行相应的动作，比如去看文章，或者收藏起来。博主与读者之间存在种一对多的依赖关系。下面给出相应的UML图设计。

![img](https://pic3.zhimg.com/80/v2-f0ba09772cbb11f708594086e0e5a436_720w.webp)

可以看到博客类中有一个观察者链表（即订阅者），当博客的状态发生变化时，通过Notify成员函数通知所有的观察者，告诉他们博客的状态更新了。而观察者通过Update成员函数获取博客的状态信息。代码实现不难，下面给出C++的一种实现。

```text
//观察者  
class Observer    
{  
public:  
    Observer() {}  
    virtual ~Observer() {}  
    virtual void Update() {}   
};  
//博客  
class Blog    
{  
public:  
    Blog() {}  
    virtual ~Blog() {}  
    void Attach(Observer *observer) { m_observers.push_back(observer); }     //添加观察者  
    void Remove(Observer *observer) { m_observers.remove(observer); }        //移除观察者  
    void Notify() //通知观察者  
    {  
        list<Observer*>::iterator iter = m_observers.begin();  
        for(; iter != m_observers.end(); iter++)  
            (*iter)->Update();  
    }  
    virtual void SetStatus(string s) { m_status = s; } //设置状态  
    virtual string GetStatus() { return m_status; }    //获得状态  
private:  
    list<Observer* > m_observers; //观察者链表  
protected:  
    string m_status; //状态  
};  
```

以上是观察者和博客的基类，定义了通用接口。博客类主要完成观察者的添加、移除、通知操作，设置和获得状态仅仅是一个默认实现。下面给出它们相应的子类实现。

```text
//具体博客类  
class BlogCSDN : public Blog  
{  
private:  
    string m_name; //博主名称  
public:  
    BlogCSDN(string name): m_name(name) {}  
    ~BlogCSDN() {}  
    void SetStatus(string s) { m_status = "CSDN通知 : " + m_name + s; } //具体设置状态信息  
    string GetStatus() { return m_status; }  
};  
//具体观察者  
class ObserverBlog : public Observer     
{  
private:  
    string m_name;  //观察者名称  
    Blog *m_blog;   //观察的博客，当然以链表形式更好，就可以观察多个博客  
public:   
    ObserverBlog(string name,Blog *blog): m_name(name), m_blog(blog) {}  
    ~ObserverBlog() {}  
    void Update()  //获得更新状态  
    {   
        string status = m_blog->GetStatus();  
        cout<<m_name<<"-------"<<status<<endl;  
    }  
};  
```

客户的使用方式：

```text
//测试案例  
int main()  
{  
    Blog *blog = new BlogCSDN("wuzhekai1985");  
    Observer *observer1 = new ObserverBlog("tutupig", blog);  
    blog->Attach(observer1);  
    blog->SetStatus("发表设计模式C++实现（15）——观察者模式");  
    blog->Notify();  
    delete blog; delete observer1;  
    return 0;  
}  
```

## 18.状态模式

状态模式：允许一个对象在其内部状态改变时改变它的行为。对象看起来似乎修改了它的类。它有两种使用情况：（1）一个对象的行为取决于它的状态, 并且它必须在运行时刻根据状态改变它的行为。（2）一个操作中含有庞大的多分支的条件语句，且这些分支依赖于该对象的状态。本文的例子为第一种情况，以战争为例，假设一场战争需经历四个阶段：前期、中期、后期、结束。当战争处于不同的阶段，战争的行为是不一样的，也就说战争的行为取决于所处的阶段，而且随着时间的推进是动态变化的。下面给出相应的UML图。

![img](https://pic1.zhimg.com/80/v2-1d5dd8c3ddf2f62a08caddc4d4a9d02c_720w.webp)

实现的代码比较简单，给出War类和State类，War类中含State对象（指针形式）。

```text
class War;  
class State   
{  
public:  
    virtual void Prophase() {}  
    virtual void Metaphase() {}  
    virtual void Anaphase() {}  
    virtual void End() {}  
    virtual void CurrentState(War *war) {}  
};  
//战争  
class War  
{  
private:  
    State *m_state;  //目前状态  
    int m_days;      //战争持续时间  
public:  
    War(State *state): m_state(state), m_days(0) {}  
    ~War() { delete m_state; }  
    int GetDays() { return m_days; }  
    void SetDays(int days) { m_days = days; }  
    void SetState(State *state) { delete m_state; m_state = state; }  
    void GetState() { m_state->CurrentState(this); }  
};  
```

给出具体的状态类：

```text
//战争结束  
class EndState: public State  
{  
public:  
    void End(War *war) //结束阶段的具体行为  
    {  
        cout<<"战争结束"<<endl;  
    }  
    void CurrentState(War *war) { End(war); }  
};  
//后期  
class AnaphaseState: public State  
{  
public:  
    void Anaphase(War *war) //后期的具体行为  
    {  
        if(war->GetDays() < 30)  
            cout<<"第"<<war->GetDays()<<"天：战争后期，双方拼死一搏"<<endl;  
        else  
        {  
            war->SetState(new EndState());  
            war->GetState();  
        }  
    }  
    void CurrentState(War *war) { Anaphase(war); }  
};  
//中期  
class MetaphaseState: public State  
{  
public:  
    void Metaphase(War *war) //中期的具体行为  
    {  
        if(war->GetDays() < 20)  
            cout<<"第"<<war->GetDays()<<"天：战争中期，进入相持阶段，双发各有损耗"<<endl;  
        else  
        {  
            war->SetState(new AnaphaseState());  
            war->GetState();  
        }  
    }  
    void CurrentState(War *war) { Metaphase(war); }  
};  
//前期  
class ProphaseState: public State  
{  
public:  
    void Prophase(War *war)  //前期的具体行为  
    {  
        if(war->GetDays() < 10)  
            cout<<"第"<<war->GetDays()<<"天：战争初期，双方你来我往，互相试探对方"<<endl;  
        else  
        {  
            war->SetState(new MetaphaseState());  
            war->GetState();  
        }  
    }  
    void CurrentState(War *war) { Prophase(war); }  
};  
```

使用方式：

```text
//测试案例  
int main()  
{  
    War *war = new War(new ProphaseState());  
    for(int i = 1; i < 40;i += 5)  
    {  
        war->SetDays(i);  
        war->GetState();  
    }  
    delete war;  
    return 0;  
}  
```

原文地址：https://zhuanlan.zhihu.com/p/529218441

作者：CPP后端技术



# 【NO.601】【linux】彻底搞懂零拷贝（Zero-Copy）技术

## 1.前言

Linux系统中一切皆文件，仔细想一下Linux系统的很多活动无外乎**读操作**和**写操作**，零拷贝就是为了提高读写性能而出现的。

## **2. 数据拷贝基础过程**

在Linux系统内部缓存和内存容量都是有限的，更多的数据都是存储在磁盘中。对于Web服务器来说，经常需要从磁盘中读取数据到内存，然后再通过网卡传输给用户：

![img](https://pic1.zhimg.com/80/v2-18654f1649f5c08d6367705eff5d8d74_720w.webp)

上述数据流转只是大框，接下来看看几种模式。

### **2.1 仅CPU方式**

- 当应用程序需要读取磁盘数据时，调用read()从用户态陷入内核态，read()这个系统调用最终由CPU来完成；
- CPU向磁盘发起I/O请求，磁盘收到之后开始准备数据；
- 磁盘将数据放到磁盘缓冲区之后，向CPU发起I/O中断，报告CPU数据已经Ready了；
- CPU收到磁盘控制器的I/O中断之后，开始拷贝数据，完成之后read()返回，再从内核态切换到用户态；

![img](https://pic2.zhimg.com/80/v2-8e97bf4d009f6f77869851f4709d38b1_720w.webp)

### **2.2 CPU&DMA方式**

CPU的时间宝贵，让它做杂活就是浪费资源。

直接内存访问（Direct Memory Access），是一种硬件设备绕开CPU独立直接访问内存的机制。所以DMA在一定程度上解放了CPU，把之前CPU的杂活让硬件直接自己做了，提高了CPU效率。

目前支持DMA的硬件包括：网卡、声卡、显卡、磁盘控制器等。

![img](https://pic3.zhimg.com/80/v2-f13f3f2bd3f05587c8b3852425550dd6_720w.webp)

有了DMA的参与之后的流程发生了一些变化：

![img](https://pic3.zhimg.com/80/v2-f8eaddb47acfc712111368a32dcc3f62_720w.webp)

最主要的变化是，CPU不再和磁盘直接交互，而是DMA和磁盘交互并且将数据从磁盘缓冲区拷贝到内核缓冲区，之后的过程类似。

> “【**敲黑板**】无论从仅CPU方式和DMA&CPU方式，都存在多次冗余数据拷贝和内核态&用户态的切换。 ”

我们继续思考Web服务器读取本地磁盘文件数据再通过网络传输给用户的详细过程。

## **3.普通模式数据交互**

一次完成的数据交互包括几个部分：系统调用syscall、CPU、DMA、网卡、磁盘等。

![img](https://pic1.zhimg.com/80/v2-abffda68664639789fb8b13d35a382d4_720w.webp)

系统调用syscall是应用程序和内核交互的桥梁，每次进行调用/返回就会产生两次切换：

- 调用syscall 从用户态切换到内核态
- syscall返回 从内核态切换到用户态

![img](https://pic3.zhimg.com/80/v2-fbcdf266265a92248b7a1062d1713eba_720w.webp)

来看下完整的数据拷贝过程简图：

![img](https://pic3.zhimg.com/80/v2-4ef1b254fb56c5586a4fda03f66611c6_720w.webp)

读数据过程：

- 应用程序要读取磁盘数据，调用read()函数从而实现用户态切换内核态，这是第1次状态切换；
- DMA控制器将数据从磁盘拷贝到内核缓冲区，这是第1次DMA拷贝；
- CPU将数据从内核缓冲区复制到用户缓冲区，这是第1次CPU拷贝；
- CPU完成拷贝之后，read()函数返回实现用户态切换用户态，这是第2次状态切换；

写数据过程：

- 应用程序要向网卡写数据，调用write()函数实现用户态切换内核态，这是第1次切换；
- CPU将用户缓冲区数据拷贝到内核缓冲区，这是第1次CPU拷贝；
- DMA控制器将数据从内核缓冲区复制到socket缓冲区，这是第1次DMA拷贝；
- 完成拷贝之后，write()函数返回实现内核态切换用户态，这是第2次切换；

综上所述：

- 读过程涉及2次空间切换、1次DMA拷贝、1次CPU拷贝；
- 写过程涉及2次空间切换、1次DMA拷贝、1次CPU拷贝；

可见传统模式下，涉及多次空间切换和数据冗余拷贝，效率并不高，接下来就该零拷贝技术出场了。

## **4. 零拷贝技术**

### **4.1 出现原因**

我们可以看到，如果应用程序不对数据做修改，从内核缓冲区到用户缓冲区，再从用户缓冲区到内核缓冲区。两次数据拷贝都需要CPU的参与，并且涉及用户态与内核态的多次切换，加重了CPU负担。

我们需要降低冗余数据拷贝、解放CPU，这也就是零拷贝Zero-Copy技术。

### **4.2 解决思路**

目前来看，零拷贝技术的几个实现手段包括：mmap+write、sendfile、sendfile+DMA收集、splice等。

![img](https://pic1.zhimg.com/80/v2-60367b931941389dd8199315f9b1a580_720w.webp)

#### **4.2.1 mmap方式**

mmap是Linux提供的一种内存映射文件的机制，它实现了将内核中读缓冲区地址与用户空间缓冲区地址进行映射，从而实现内核缓冲区与用户缓冲区的共享。

这样就减少了一次用户态和内核态的CPU拷贝，但是在内核空间内仍然有一次CPU拷贝。

![img](https://pic3.zhimg.com/80/v2-2bb29cb76bc6027b7f5b54de60cc723a_720w.webp)

mmap对大文件传输有一定优势，但是小文件可能出现碎片，并且在多个进程同时操作文件时可能产生引发coredump的signal。

#### **4.2.2 sendfile方式**

mmap+write方式有一定改进，但是由系统调用引起的状态切换并没有减少。

sendfile系统调用是在 Linux 内核2.1版本中被引入，它建立了两个文件之间的传输通道。

sendfile方式只使用一个函数就可以完成之前的read+write 和 mmap+write的功能，这样就少了2次状态切换，由于数据不经过用户缓冲区，因此该数据无法被修改。

![img](https://pic1.zhimg.com/80/v2-5bd306e1838167320a6a954742623830_720w.webp)

![img](https://pic1.zhimg.com/80/v2-289f0e6921b56ab66b3e4f2d829282e4_720w.webp)

从图中可以看到，应用程序只需要调用sendfile函数即可完成，只有2次状态切换、1次CPU拷贝、2次DMA拷贝。

但是sendfile在内核缓冲区和socket缓冲区仍然存在一次CPU拷贝，或许这个还可以优化。

#### **4.2.3 sendfile+DMA收集**

Linux 2.4 内核对 sendfile 系统调用进行优化，但是需要硬件DMA控制器的配合。

升级后的sendfile将内核空间缓冲区中对应的数据描述信息（文件描述符、地址偏移量等信息）记录到socket缓冲区中。

DMA控制器根据socket缓冲区中的地址和偏移量将数据从内核缓冲区拷贝到网卡中，从而省去了内核空间中仅剩1次CPU拷贝。

![img](https://pic2.zhimg.com/80/v2-21b528e0695d382f8bb10c926c2d1895_720w.webp)

这种方式有2次状态切换、0次CPU拷贝、2次DMA拷贝，但是仍然无法对数据进行修改，并且需要硬件层面DMA的支持，并且sendfile只能将文件数据拷贝到socket描述符上，有一定的局限性。

#### **4.2.4 splice方式**

splice系统调用是Linux 在 2.6 版本引入的，其不需要硬件支持，并且不再限定于socket上，实现两个普通文件之间的数据零拷贝。

![img](https://pic1.zhimg.com/80/v2-b910394097e2301a8fb092ab082cfccc_720w.webp)

splice 系统调用可以在内核缓冲区和socket缓冲区之间建立管道来传输数据，避免了两者之间的 CPU 拷贝操作。

![img](https://pic4.zhimg.com/80/v2-558c6409bac4096d4412375898eaa1e3_720w.webp)

splice也有一些局限，它的两个文件描述符参数中有一个必须是管道设备。

## **5.本文小结**

本文通过介绍数据交互的基本过程、传统模式的缺点，进而介绍了零拷贝的一些实现方法。

零拷贝技术是非常底层且重要的读写优化，对于服务并发能力的提升有很大帮助。

原文地址：https://zhuanlan.zhihu.com/p/500800127

作者：CPP后端技术

# 【NO.602】Linux C++的多线程编程（新手最全教程）

## 1. 引言

线程（thread）技术早在60年代就被提出，但真正应用多线程到操作系统中去，是在80年代中期，solaris是这方面的佼佼者。传统的Unix也支持线程的概念，但是在一个进程（process）中只允许有一个线程，这样多线程就意味着多进程。现在，多线程技术已经被许多操作系统所支持，包括Windows/NT，当然，也包括Linux。

　　为什么有了进程的概念后，还要再引入线程呢？使用多线程到底有哪些好处？什么的系统应该选用多线程？我们首先必须回答这些问题。

　　使用多线程的理由之一是和进程相比，它是一种非常"节俭"的多任务操作方式。我们知道，在Linux系统下，启动一个新的进程必须分配给它独立的地址空间，建立众多的数据表来维护它的代码段、堆栈段和数据段，这是一种"昂贵"的多任务工作方式。而运行于一个进程中的多个线程，它们彼此之间使用相同的地址空间，共享大部分数据，启动一个线程所花费的空间远远小于启动一个进程所花费的空间，而且，线程间彼此切换所需的时间也远远小于进程间切换所需要的时间。据统计，总的说来，一个进程的开销大约是一个线程开销的30倍左右，当然，在具体的系统上，这个数据可能会有较大的区别。

　　使用多线程的理由之二是线程间方便的通信机制。对不同进程来说，它们具有独立的数据空间，要进行数据的传递只能通过通信的方式进行，这种方式不仅费时，而且很不方便。线程则不然，由于同一进程下的线程之间共享数据空间，所以一个线程的数据可以直接为其它线程所用，这不仅快捷，而且方便。当然，数据的共享也带来其他一些问题，有的变量不能同时被两个线程所修改，有的子程序中声明为static的数据更有可能给多线程程序带来灾难性的打击，这些正是编写多线程程序时最需要注意的地方。

　　除了以上所说的优点外，不和进程比较，多线程程序作为一种多任务、并发的工作方式，当然有以下的优点：

  　　1) 提高应用程序响应。这对图形界面的程序尤其有意义，当一个操作耗时很长时，整个系统都会等待这个操作，此时程序不会响应键盘、鼠标、菜单的操作，而使用多线程技术，将耗时长的操作（time consuming）置于一个新的线程，可以避免这种尴尬的情况。

  　　2) 使多CPU系统更加有效。操作系统会保证当线程数不大于CPU数目时，不同的线程运行于不同的CPU上。

  　　3) 改善程序结构。一个既长又复杂的进程可以考虑分为多个线程，成为几个独立或半独立的运行部分，这样的程序会利于理解和修改。

　　下面我们先来尝试编写一个简单的多线程程序。

## 2. 简单的多线程编程

　　Linux系统下的多线程遵循POSIX线程接口，称为pthread。编写Linux下的多线程程序，需要使用头文件pthread.h，连接时需要使用库libpthread.a。顺便说一下，Linux下pthread的实现是通过系统调用clone()来实现的。clone()是Linux所特有的系统调用，它的使用方式类似fork，关于clone()的详细情况，有兴趣的读者可以去查看有关文档说明。下面我们展示一个最简单的多线程程序threads.cpp。

> //Threads.cpp
> \#include <iostream>
> \#include <unistd.h>
> \#include <pthread.h>
> using namespace std;
>
> void *thread(void *ptr)
> {
> for(int i = 0;i < 3;i++) {
> sleep(1);
> cout << "This is a pthread." << endl;
> }
> return 0;
> }
>
> int main() {
> pthread_t id;
> int ret = pthread_create(&id, NULL, thread, NULL);
> if(ret) {
> cout << "Create pthread error!" << endl;
> return 1;
> }
> for(int i = 0;i < 3;i++) {
> cout << "This is the main process." << endl;
> sleep(1);
> }
> pthread_join(id, NULL);
> return 0;
> }

　　我们编译并运行此程序，可以得到如下结果：

　　This is the main process.
　　This is a pthread.
　　This is the main process.
　　This is the main process.
　　This is a pthread.
　　This is a pthread.
　　再次运行，我们可能得到如下结果：
　　This is a pthread.
　　This is the main process.
　　This is a pthread.
　　This is the main process.
　　This is a pthread.
　　This is the main process.

　　前后两次结果不一样，这是两个线程争夺CPU资源的结果。上面的示例中，我们使用到了两个函数，pthread_create和pthread_join，并声明了一个pthread_t型的变量。

　　pthread_t在头文件/usr/include/bits/pthreadtypes.h中定义：

typedef unsigned long int pthread_t;

　　它是一个线程的标识符。函数pthread_create用来创建一个线程，它的原型为：

> extern int pthread_create __P ((pthread_t *__thread, __const pthread_attr_t *__attr,
> void *(*__start_routine) (void *), void *__arg));

　　第一个参数为指向线程标识符的指针，第二个参数用来设置线程属性，第三个参数是线程运行函数的起始地址，最后一个参数是运行函数的参数。这里，我们的函数thread不需要参数，所以最后一个参数设为空指针。第二个参数我们也设为空指针，这样将生成默认属性的线程。对线程属性的设定和修改我们将在下一节阐述。当创建线程成功时，函数返回0，若不为0则说明创建线程失败，常见的错误返回代码为EAGAIN和EINVAL。前者表示系统限制创建新的线程，例如线程数目过多了；后者表示第二个参数代表的线程属性值非法。创建线程成功后，新创建的线程则运行参数三和参数四确定的函数，原来的线程则继续运行下一行代码。

　　函数pthread_join用来等待一个线程的结束。函数原型为：

extern int pthread_join __P ((pthread_t __th, void **__thread_return));

　　第一个参数为被等待的线程标识符，第二个参数为一个用户定义的指针，它可以用来存储被等待线程的返回值。这个函数是一个线程阻塞的函数，调用它的函数将一直等待到被等待的线程结束为止，当函数返回时，被等待线程的资源被收回。一个线程的结束有两种途径，一种是象我们上面的例子一样，函数结束了，调用它的线程也就结束了；另一种方式是通过函数pthread_exit来实现。它的函数原型为：

extern void pthread_exit __P ((void *__retval)) __attribute__ ((__noreturn__));

　　唯一的参数是函数的返回代码，只要pthread_join中的第二个参数thread_return不是NULL，这个值将被传递给thread_return。最后要说明的是，一个线程不能被多个线程等待，否则第一个接收到信号的线程成功返回，其余调用pthread_join的线程则返回错误代码ESRCH。

　　在这一节里，我们编写了一个最简单的线程，并掌握了最常用的三个函数pthread_create，pthread_join和pthread_exit。下面，我们来了解线程的一些常用属性以及如何设置这些属性。

## 3. 修改线程的属性

　　在上一节的例子里，我们用pthread_create函数创建了一个线程，在这个线程中，我们使用了默认参数，即将该函数的第二个参数设为NULL。的确，对大多数程序来说，使用默认属性就够了，但我们还是有必要来了解一下线程的有关属性。

　　属性结构为pthread_attr_t，它同样在头文件/usr/include/pthread.h中定义，喜欢追根问底的人可以自己去查看。属性值不能直接设置，须使用相关函数进行操作，初始化的函数为pthread_attr_init，这个函数必须在pthread_create函数之前调用。属性对象主要包括是否绑定、是否分离、堆栈地址、堆栈大小、优先级。默认的属性为非绑定、非分离、缺省1M的堆栈、与父进程同样级别的优先级。

　　关于线程的绑定，牵涉到另外一个概念：轻进程（LWP：Light Weight Process）。轻进程可以理解为内核线程，它位于用户层和系统层之间。系统对线程资源的分配、对线程的控制是通过轻进程来实现的，一个轻进程可以控制一个或多个线程。默认状况下，启动多少轻进程、哪些轻进程来控制哪些线程是由系统来控制的，这种状况即称为非绑定的。绑定状况下，则顾名思义，即某个线程固定的"绑"在一个轻进程之上。被绑定的线程具有较高的响应速度，这是因为CPU时间片的调度是面向轻进程的，绑定的线程可以保证在需要的时候它总有一个轻进程可用。通过设置被绑定的轻进程的优先级和调度级可以使得绑定的线程满足诸如实时反应之类的要求。

　　设置线程绑定状态的函数为pthread_attr_setscope，它有两个参数，第一个是指向属性结构的指针，第二个是绑定类型，它有两个取值：PTHREAD_SCOPE_SYSTEM（绑定的）和PTHREAD_SCOPE_PROCESS（非绑定的）。下面的代码即创建了一个绑定的线程。

> \#include <pthread.h>
> pthread_attr_t attr;
> pthread_t tid;
> /*初始化属性值，均设为默认值*/
> pthread_attr_init(&attr);
> pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM);
> pthread_create(&tid, &attr, (void *) my_function, NULL);

线程的分离状态决定一个线程以什么样的方式来终止自己。在上面的例子中，我们采用了线程的默认属性，即为非分离状态，这种情况下，原有的线程等待创建的线程结束。只有当pthread_join（）函数返回时，创建的线程才算终止，才能释放自己占用的系统资源。而分离线程不是这样子的，它没有被其他的线程所等待，自己运行结束了，线程也就终止了，马上释放系统资源。程序员应该根据自己的需要，选择适当的分离状态。设置线程分离状态的函数为pthread_attr_setdetachstate（pthread_attr_t *attr, int detachstate）。第二个参数可选为PTHREAD_CREATE_DETACHED（分离线程）和 PTHREAD _CREATE_JOINABLE（非分离线程）。这里要注意的一点是，如果设置一个线程为分离线程，而这个线程运行又非常快，它很可能在pthread_create函数返回之前就终止了，它终止以后就可能将线程号和系统资源移交给其他的线程使用，这样调用pthread_create的线程就得到了错误的线程号。要避免这种情况可以采取一定的同步措施，最简单的方法之一是可以在被创建的线程里调用pthread_cond_timewait函数，让这个线程等待一会儿，留出足够的时间让函数pthread_create返回。设置一段等待时间，是在多线程编程里常用的方法。但是注意不要使用诸如wait（）之类的函数，它们是使整个进程睡眠，并不能解决线程同步的问题。

　　另外一个可能常用的属性是线程的优先级，它存放在结构sched_param中。用函数pthread_attr_getschedparam和函数pthread_attr_setschedparam进行存放，一般说来，我们总是先取优先级，对取得的值修改后再存放回去。下面即是一段简单的例子。

> \#include <pthread.h>
> \#include <sched.h>
> pthread_attr_t attr;
> pthread_t tid;
> sched_param param;
> int newprio=20;
> pthread_attr_init(&amp;attr);
> pthread_attr_getschedparam(&attr, &param);
> param.sched_priority=newprio;
> pthread_attr_setschedparam(&attr, &param);
> pthread_create(&tid, &attr, (void *)myfunction, myarg);

## 4. 线程的数据处理

　　和进程相比，线程的最大优点之一是数据的共享性，各个进程共享父进程处沿袭的数据段，可以方便的获得、修改数据。但这也给多线程编程带来了许多问题。我们必须当心有多个不同的进程访问相同的变量。许多函数是不可重入的，即同时不能运行一个函数的多个拷贝（除非使用不同的数据段）。在函数中声明的静态变量常常带来问题，函数的返回值也会有问题。因为如果返回的是函数内部静态声明的空间的地址，则在一个线程调用该函数得到地址后使用该地址指向的数据时，别的线程可能调用此函数并修改了这一段数据。在进程中共享的变量必须用关键字volatile来定义，这是为了防止编译器在优化时（如gcc中使用-OX参数）改变它们的使用方式。为了保护变量，我们必须使用信号量、互斥等方法来保证我们对变量的正确使用。下面，我们就逐步介绍处理线程数据时的有关知识。

4.1 线程数据

　　在单线程的程序里，有两种基本的数据：全局变量和局部变量。但在多线程程序里，还有第三种数据类型：线程数据（TSD: Thread-Specific Data）。它和全局变量很象，在线程内部，各个函数可以象使用全局变量一样调用它，但它对线程外部的其它线程是不可见的。这种数据的必要性是显而易见的。例如我们常见的变量errno，它返回标准的出错信息。它显然不能是一个局部变量，几乎每个函数都应该可以调用它；但它又不能是一个全局变量，否则在A线程里输出的很可能是B线程的出错信息。要实现诸如此类的变量，我们就必须使用线程数据。我们为每个线程数据创建一个键，它和这个键相关联，在各个线程里，都使用这个键来指代线程数据，但在不同的线程里，这个键代表的数据是不同的，在同一个线程里，它代表同样的数据内容。

　　和线程数据相关的函数主要有4个：创建一个键；为一个键指定线程数据；从一个键读取线程数据；删除键。

　　创建键的函数原型为：

extern int pthread_key_create __P ((pthread_key_t *__key,void (*__destr_function) (void *)));

　　第一个参数为指向一个键值的指针，第二个参数指明了一个destructor函数，如果这个参数不为空，那么当每个线程结束时，系统将调用这个函数来释放绑定在这个键上的内存块。这个函数常和函数pthread_once ((pthread_once_t*once_control, void (*initroutine) (void)))一起使用，为了让这个键只被创建一次。函数pthread_once声明一个初始化函数，第一次调用pthread_once时它执行这个函数，以后的调用将被它忽略。

　　在下面的例子中，我们创建一个键，并将它和某个数据相关联。我们要定义一个函数createWindow，这个函数定义一个图形窗口（数据类型为Fl_Window *，这是图形界面开发工具FLTK中的数据类型）。由于各个线程都会调用这个函数，所以我们使用线程数据。

> /* 声明一个键*/
> pthread_key_t myWinKey;
> /* 函数 createWindow */
> void createWindow ( void ) {
> Fl_Window * win;
> static pthread_once_t once= PTHREAD_ONCE_INIT;
> /* 调用函数createMyKey，创建键*/
> pthread_once ( & once, createMyKey) ;
> /*win指向一个新建立的窗口*/
> win=new Fl_Window( 0, 0, 100, 100, "MyWindow");
> /* 对此窗口作一些可能的设置工作，如大小、位置、名称等*/
> setWindow(win);
> /* 将窗口指针值绑定在键myWinKey上*/
> pthread_setpecific ( myWinKey, win);
> }
>
> /* 函数 createMyKey，创建一个键，并指定了destructor */
> void createMyKey ( void ) {
> pthread_keycreate(&myWinKey, freeWinKey);
> }
>
> /* 函数 freeWinKey，释放空间*/
> void freeWinKey ( Fl_Window * win){
> delete win;
> }

　　这样，在不同的线程中调用函数createMyWin，都可以得到在线程内部均可见的窗口变量，这个变量通过函数pthread_getspecific得到。在上面的例子中，我们已经使用了函数pthread_setspecific来将线程数据和一个键绑定在一起。这两个函数的原型如下：

> extern int pthread_setspecific __P ((pthread_key_t __key,__const void *__pointer)); extern void *pthread_getspecific __P ((pthread_key_t __key));

　　这两个函数的参数意义和使用方法是显而易见的。要注意的是，用pthread_setspecific为一个键指定新的线程数据时，必须自己释放原有的线程数据以回收空间。这个过程函数pthread_key_delete用来删除一个键，这个键占用的内存将被释放，但同样要注意的是，它只释放键占用的内存，并不释放该键关联的线程数据所占用的内存资源，而且它也不会触发函数pthread_key_create中定义的destructor函数。线程数据的释放必须在释放键之前完成。

　　4.2 互斥锁

　　互斥锁用来保证一段时间内只有一个线程在执行一段代码。必要性显而易见：假设各个线程向同一个文件顺序写入数据，最后得到的结果一定是灾难性的。

　　我们先看下面一段代码。这是一个读/写程序，它们公用一个缓冲区，并且我们假定一个缓冲区只能保存一条信息。即缓冲区只有两个状态：有信息或没有信息。

> void reader_function ( void );
> void writer_function ( void );
> char buffer;
> int buffer_has_item=0;
> pthread_mutex_t mutex;
> struct timespec delay;
>
> void main ( void ){
> pthread_t reader;
> /* 定义延迟时间*/
> delay.tv_sec = 2;
> delay.tv_nec = 0;
> /* 用默认属性初始化一个互斥锁对象*/
> pthread_mutex_init (&mutex,NULL);
> pthread_create(&reader, pthread_attr_default, (void *)&reader_function), NULL);
> writer_function( );
> }
>
> void writer_function (void){
> while(1){
> /* 锁定互斥锁*/
> pthread_mutex_lock (&mutex);
> if (buffer_has_item==0){
> buffer=make_new_item( );
> buffer_has_item=1;
> }
> /* 打开互斥锁*/
> pthread_mutex_unlock(&mutex);
> pthread_delay_np(&delay);
> }
> }
>
> void reader_function(void){
> while(1){
> pthread_mutex_lock(&mutex);
> if(buffer_has_item==1){
> consume_item(buffer);
> buffer_has_item=0;
> }
> pthread_mutex_unlock(&mutex);
> pthread_delay_np(&delay);
> }
> }

　　这里声明了互斥锁变量mutex，结构pthread_mutex_t为不公开的数据类型，其中包含一个系统分配的属性对象。函数pthread_mutex_init用来生成一个互斥锁。NULL参数表明使用默认属性。如果需要声明特定属性的互斥锁，须调用函数pthread_mutexattr_init。函数pthread_mutexattr_setpshared和函数pthread_mutexattr_settype用来设置互斥锁属性。前一个函数设置属性pshared，它有两个取值，PTHREAD_PROCESS_PRIVATE和PTHREAD_PROCESS_SHARED。前者用来不同进程中的线程同步，后者用于同步本进程的不同线程。在上面的例子中，我们使用的是默认属性PTHREAD_PROCESS_ PRIVATE。后者用来设置互斥锁类型，可选的类型有PTHREAD_MUTEX_NORMAL、PTHREAD_MUTEX_ERRORCHECK、PTHREAD_MUTEX_RECURSIVE和PTHREAD _MUTEX_DEFAULT。它们分别定义了不同的上锁、解锁机制，一般情况下，选用最后一个默认属性。

　　pthread_mutex_lock声明开始用互斥锁上锁，此后的代码直至调用pthread_mutex_unlock为止，均被上锁，即同一时间只能被一个线程调用执行。当一个线程执行到pthread_mutex_lock处时，如果该锁此时被另一个线程使用，那此线程被阻塞，即程序将等待到另一个线程释放此互斥锁。在上面的例子中，我们使用了pthread_delay_np函数，让线程睡眠一段时间，就是为了防止一个线程始终占据此函数。

　　上面的例子非常简单，就不再介绍了，需要提出的是在使用互斥锁的过程中很有可能会出现死锁：两个线程试图同时占用两个资源，并按不同的次序锁定相应的互斥锁，例如两个线程都需要锁定互斥锁1和互斥锁2，a线程先锁定互斥锁1，b线程先锁定互斥锁2，这时就出现了死锁。此时我们可以使用函数pthread_mutex_trylock，它是函数pthread_mutex_lock的非阻塞版本，当它发现死锁不可避免时，它会返回相应的信息，程序员可以针对死锁做出相应的处理。另外不同的互斥锁类型对死锁的处理不一样，但最主要的还是要程序员自己在程序设计注意这一点。

　　4.3 条件变量

　　前一节中我们讲述了如何使用互斥锁来实现线程间数据的共享和通信，互斥锁一个明显的缺点是它只有两种状态：锁定和非锁定。而条件变量通过允许线程阻塞和等待另一个线程发送信号的方法弥补了互斥锁的不足，它常和互斥锁一起使用。使用时，条件变量被用来阻塞一个线程，当条件不满足时，线程往往解开相应的互斥锁并等待条件发生变化。一旦其它的某个线程改变了条件变量，它将通知相应的条件变量唤醒一个或多个正被此条件变量阻塞的线程。这些线程将重新锁定互斥锁并重新测试条件是否满足。一般说来，条件变量被用来进行线承间的同步。

　　条件变量的结构为pthread_cond_t，函数pthread_cond_init（）被用来初始化一个条件变量。它的原型为：

extern int pthread_cond_init __P ((pthread_cond_t *__cond,__const pthread_condattr_t *__cond_attr));

　　其中cond是一个指向结构pthread_cond_t的指针，cond_attr是一个指向结构pthread_condattr_t的指针。结构pthread_condattr_t是条件变量的属性结构，和互斥锁一样我们可以用它来设置条件变量是进程内可用还是进程间可用，默认值是PTHREAD_ PROCESS_PRIVATE，即此条件变量被同一进程内的各个线程使用。注意初始化条件变量只有未被使用时才能重新初始化或被释放。释放一个条件变量的函数为pthread_cond_ destroy（pthread_cond_t cond）。　

　　函数pthread_cond_wait（）使线程阻塞在一个条件变量上。

　　它的函数原型为：

extern int pthread_cond_wait __P ((pthread_cond_t *__cond, pthread_mutex_t *__mutex));

　　线程解开mutex指向的锁并被条件变量cond阻塞。线程可以被函数pthread_cond_signal和函数pthread_cond_broadcast唤醒，但是要注意的是，条件变量只是起阻塞和唤醒线程的作用，具体的判断条件还需用户给出，例如一个变量是否为0等等，这一点我们从后面的例子中可以看到。线程被唤醒后，它将重新检查判断条件是否满足，如果还不满足，一般说来线程应该仍阻塞在这里，被等待被下一次唤醒。这个过程一般用while语句实现。

　　另一个用来阻塞线程的函数是pthread_cond_timedwait()，它的原型为：

extern int pthread_cond_timedwait __P ((pthread_cond_t *__cond, pthread_mutex_t *__mutex, __const struct timespec *__abstime));

　　它比函数pthread_cond_wait()多了一个时间参数，经历abstime段时间后，即使条件变量不满足，阻塞也被解除。

　　函数pthread_cond_signal()的原型为：

　　extern int pthread_cond_signal __P ((pthread_cond_t *__cond));

　　它用来释放被阻塞在条件变量cond上的一个线程。多个线程阻塞在此条件变量上时，哪一个线程被唤醒是由线程的调度策略所决定的。要注意的是，必须用保护条件变量的互斥锁来保护这个函数，否则条件满足信号又可能在测试条件和调用pthread_cond_wait函数之间被发出，从而造成无限制的等待。下面是使用函数pthread_cond_wait()和函数

> pthread_cond_signal()的一个简单的例子。
> pthread_mutex_t count_lock;
> pthread_cond_t count_nonzero;
> unsigned count;
> decrement_count () {
> pthread_mutex_lock (&count_lock);
> while(count==0)
> pthread_cond_wait( &count_nonzero, &count_lock);
> count=count -1;
> pthread_mutex_unlock (&count_lock);
> }
>
> increment_count(){
> pthread_mutex_lock(&count_lock);
> if(count==0)
> pthread_cond_signal(&count_nonzero);
> count=count+1;
> pthread_mutex_unlock(&count_lock);
> }

　　count值为0时，decrement函数在pthread_cond_wait处被阻塞，并打开互斥锁count_lock。此时，当调用到函数increment_count时，pthread_cond_signal（）函数改变条件变量，告知decrement_count（）停止阻塞。读者可以试着让两个线程分别运行这两个函数，看看会出现什么样的结果。

　　函数pthread_cond_broadcast（pthread_cond_t *cond）用来唤醒所有被阻塞在条件变量cond上的线程。这些线程被唤醒后将再次竞争相应的互斥锁，所以必须小心使用这个函数。

　　4.4 信号量

　　信号量本质上是一个非负的整数计数器，它被用来控制对公共资源的访问。当公共资源增加时，调用函数sem_post（）增加信号量。只有当信号量值大于０时，才能使用公共资源，使用后，函数sem_wait（）减少信号量。函数sem_trywait（）和函数pthread_ mutex_trylock（）起同样的作用，它是函数sem_wait（）的非阻塞版本。下面我们逐个介绍和信号量有关的一些函数，它们都在头文件/usr/include/semaphore.h中定义。

　　信号量的数据类型为结构sem_t，它本质上是一个长整型的数。函数sem_init（）用来初始化一个信号量。它的原型为：

　　extern int sem_init __P ((sem_t *__sem, int __pshared, unsigned int __value));

　　sem为指向信号量结构的一个指针；pshared不为０时此信号量在进程间共享，否则只能为当前进程的所有线程共享；value给出了信号量的初始值。

　　函数sem_post( sem_t *sem )用来增加信号量的值。当有线程阻塞在这个信号量上时，调用这个函数会使其中的一个线程不在阻塞，选择机制同样是由线程的调度策略决定的。

　　函数sem_wait( sem_t *sem )被用来阻塞当前线程直到信号量sem的值大于0，解除阻塞后将sem的值减一，表明公共资源经使用后减少。函数sem_trywait ( sem_t *sem )是函数sem_wait（）的非阻塞版本，它直接将信号量sem的值减一。

　　函数sem_destroy(sem_t *sem)用来释放信号量sem。

　　下面我们来看一个使用信号量的例子。在这个例子中，一共有4个线程，其中两个线程负责从文件读取数据到公共的缓冲区，另两个线程从缓冲区读取数据作不同的处理（加和乘运算）。

> /* File sem.c */
> \#include <stdio.h>
> \#include <pthread.h>
> \#include <semaphore.h>
> \#define MAXSTACK 100
> int stack[MAXSTACK][2];
> int size=0;
> sem_t sem;
>
> /* 从文件1.dat读取数据，每读一次，信号量加一*/
> void ReadData1(void){
> FILE *fp=fopen("1.dat","r");
> while(!feof(fp)){
> fscanf(fp,"%d %d",&stack[size][0],&stack[size][1]);
> sem_post(&sem);
> ++size;
> }
> fclose(fp);
> }
>
> /*从文件2.dat读取数据*/
> void ReadData2(void){
> FILE *fp=fopen("2.dat","r");
> while(!feof(fp)){
> fscanf(fp,"%d %d",&stack[size][0],&stack[size][1]);
> sem_post(&sem);
> ++size;
> }
> fclose(fp);
> }
> /*阻塞等待缓冲区有数据，读取数据后，释放空间，继续等待*/
> void HandleData1(void){
> while(1){
> sem_wait(&sem);
> printf("Plus:%d+%d=%d\n",stack[size][0],stack[size][1],
> stack[size][0]+stack[size][1]);
> --size;
> }
> }
>
> void HandleData2(void){
> while(1){
> sem_wait(&sem);
> printf("Multiply:%d*%d=%d\n",stack[size][0],stack[size][1],
> stack[size][0]*stack[size][1]);
> --size;
> }
> }
>
> int main(void){
> pthread_t t1,t2,t3,t4;
> sem_init(&sem,0,0);
> pthread_create(&t1,NULL,(void *)HandleData1,NULL);
> pthread_create(&t2,NULL,(void *)HandleData2,NULL);
> pthread_create(&t3,NULL,(void *)ReadData1,NULL);
> pthread_create(&t4,NULL,(void *)ReadData2,NULL);
> /* 防止程序过早退出，让它在此无限期等待*/
> pthread_join(t1,NULL);
> }

　　在Linux下，我们用命令gcc -lpthread sem.c -o sem生成可执行文件sem。 我们事先编辑好数据文件1.dat和2.dat，假设它们的内容分别为1 2 3 4 5 6 7 8 9 10和 -1 -2 -3 -4 -5 -6 -7 -8 -9 -10 ，我们运行sem，得到如下的结果：

　　Multiply:-1*-2=2
　　Plus:-1+-2=-3
　　Multiply:9*10=90
　　Plus:-9+-10=-19
　　Multiply:-7*-8=56
　　Plus:-5+-6=-11
　　Multiply:-3*-4=12
　　Plus:9+10=19
　　Plus:7+8=15
　　Plus:5+6=11

　　从中我们可以看出各个线程间的竞争关系。而数值并未按我们原先的顺序显示出来这是由于size这个数值被各个线程任意修改的缘故。这也往往是多线程编程要注意的问题。

## 5. 小结

　　多线程编程是一个很有意思也很有用的技术，使用多线程技术的网络蚂蚁是目前最常用的下载工具之一，使用多线程技术的grep比单线程的grep要快上几倍，类似的例子还有很多。希望大家能用多线程技术写出高效实用的好程序来。

原文地址：https://zhuanlan.zhihu.com/p/517076696

作者：CPP后端技术

# 【NO.603】TCP协议之Send和Recv原理及常见问题分析

## 1.Send函数

```text
int send( SOCKET s, const char FAR *buf, int len, int flags );  
```

不论是客户还是服务器应用程序都用send函数来向TCP连接的另一端发送数据。客户程序一般用send函数向服务器发送请求，而服务器则通常用send函数来向客户程序发送应答。

参数说明：

- 第一个参数指定发送端套接字描述符；
- 第二个参数指明一个存放应用程序要发送数据的缓冲区；
- 第三个参数指明实际要发送的数据的字节数。
- 第四个参数一般置0。

这里只描述同步Socket的send函数的执行流程。当调用该函数时，

send先比较待发送数据的长度len和套接字s的发送缓冲的长度，

1. 如果len大于s的发送缓冲区的长度，该函数返回SOCKET_ERROR；
2. 如果len小于或者等于s的发送缓冲区的长度，那么send先检查协议是否正在发送s的发送缓冲中的数据，如果是就等待协议把数据发送完，如果协议还没有开始发送s的发送缓冲中的数据或者s的发送缓冲中没有数据，那么 send就比较s的发送缓冲区的剩余空间和len；

针对第二种情况：

2.1 如果len大于剩余空间大小send就一直等待协议把s的发送缓冲中的数据发送完；

2.2 如果len小于剩余空间大小send就仅仅把buf中的数据copy到剩余空间里（注意并不是send把s的发送缓冲中的数据传到连接的另一端的，而是协议传的，send仅仅是把buf中的数据copy到s的发送缓冲区的剩余空间里）。如果send函数copy数据成功，就返回实际copy的字节数，如果send在copy数据时出现错误，那么send就返回SOCKET_ERROR；如果send在等待协议传送数据时网络断开的话，那么send函数也返回SOCKET_ERROR。

要注意send函数把buf中的数据成功copy到s的发送缓冲的剩余空间里后它就返回了，但是此时这些数据并不一定马上被传到连接的另一端。如果协议在后续的传送过程中出现网络错误的话，那么下一个Socket函数就会返回SOCKET_ERROR。(每一个除send外的Socket函数在执行的最开始总要先等待套接字的发送缓冲中的数据被协议传送完毕才能继续，如果在等待时出现网络错误，那么该Socket函数就返回 SOCKET_ERROR）

注意：在Unix系统下，如果send在等待协议传送数据时网络断开的话，调用send的进程会接收到一个SIGPIPE信号，进程对该信号的默认处理是进程终止。

通过测试发现，异步socket的send函数在网络刚刚断开时还能发送返回相应的字节数，同时使用select检测也是可写的，但是过几秒钟之后，再send就会出错了，返回-1。select也不能检测出可写了。

## 2.send函数发送原理

tcp协议本身是可靠的,并不等于应用程序用tcp发送数据就一定是可靠的.不管是否阻塞,send发送的大小,并不代表对端recv到多少的数据.

在阻塞模式下,send函数的过程是将应用程序请求发送的数据拷贝到发送缓存中发送并得到确认后再返回.但由于发送缓存的存在,表现为:如果发送缓存大小比请求发送的大小要大,那么send函数立即返回,同时向网络中发送数据;否则,send向网络发送缓存中不能容纳的那部分数据,并等待对端确认后再返回(接收端只要将数据收到接收缓存中,就会确认,并不一定要等待应用程序调用recv);

在非阻塞模式下,send函数的过程仅仅是将数据拷贝到协议栈的缓存区而已,如果缓存区可用空间不够,则尽能力的拷贝,返回成功拷贝的大小;如缓存区可用空间为0,则返回-1,同时设置errno为EAGAIN.

## 3.实例分析send／recv的异常情况

在实际应用中,如果发送端是非阻塞发送,由于网络的阻塞或者接收端处理过慢,通常出现的情况是,发送应用程序看起来发送了10k的数据,但是只发送了2k到对端缓存中,还有8k在本机缓存中(未发送或者未得到接收端的确认).那么此时,接收应用程序能够收到的数据为2k.假如接收应用程序调用recv函数获取了1k的数据在处理,在这个瞬间,发生了以下情况之一:

发送应用程序认为send完了10k数据,关闭了socket:

发送主机作为tcp的主动关闭者,连接将处于FIN_WAIT1的半关闭状态(等待对方的ack),并且,发送缓存中的8k数据并不清除,依然会发送给对端.如果接收应用程序依然在recv,那么它会收到余下的8k数据(这个前题是,接收端会在发送端FIN_WAIT1状态超时前收到余下的8k数据.),然后得到一个对端socket被关闭的消息(recv返回0).这时,应该进行关闭.

发送应用程序再次调用send发送8k的数据:

假如发送缓存的空间为20k,那么发送缓存可用空间为20-8=12k,大于请求发送的8k,所以send函数将数据做拷贝后,并立即返回8192;假如发送缓存的空间为12k,那么此时发送缓存可用空间还有12-8=4k,send()会返回4096,应用程序发现返回的值小于请求发送的大小值后,可以认为缓存区已满,这时必须阻塞(或通过select等待下一次socket可写的信号),如果应用程序不理会,立即再次调用send,那么会得到-1的值,在linux下表现为errno=EAGAIN.

接收应用程序在处理完1k数据后,关闭了socket:

接收主机作为主动关闭者,连接将处于FIN_WAIT1的半关闭状态(等待对方的ack).然后,发送应用程序会收到socket可读的信号(通常是select调用返回socket可读),但在读取时会发现recv函数返回0,这时应该调用close函数来关闭socket(发送给对方ack);如果发送应用程序没有处理这个可读的信号,而是继续调用send,那么第一次会像往常一样继续填充缓存区,然后返回,但如果再次调用send,进程会收到SIGPIPE信号,该信号的默认响应动作是退出进程.

交换机或路由器的网络断开:

接收应用程序在处理完已收到的1k数据后,会继续从缓存区读取余下的1k数据,然后就表现为无数据可读的现象,这种情况需要应用程序来处理超时.一般做法是设定一个select等待的最大时间,如果超出这个时间依然没有数据可读,则认为socket已不可用.发送应用程序会不断的将余下的数据发送到网络上,但始终得不到确认,所以缓存区的可用空间持续为0,这种情况也需要应用程序来处理.

如果不由应用程序来处理这种情况超时的情况,也可以通过tcp协议本身来处理,具体可以查看sysctl项中的:

net.ipv4.tcp_keepalive_intvl

net.ipv4.tcp_keepalive_probes

net.ipv4.tcp_keepalive_time

## 4.Recv函数

```text
int recv( SOCKET s,  char FAR *buf, int len, int flags);  
```

不论是客户还是服务器应用程序都用recv函数从TCP连接的另一端接收数据。

1. 第一个参数指定接收端套接字描述符；
2. 第二个参数指明一个缓冲区，该缓冲区用来存放recv函数接收到的数据；
3. 第三个参数指明buf的长度；
4. 第四个参数一般置0。

这里只描述同步Socket的recv函数的执行流程。当应用程序调用recv函数时，recv先等待s的发送缓冲中的数据被协议传送完毕，

如果协议在传送s的发送缓冲中的数据时出现网络错误，那么recv函数返回SOCKET_ERROR；

如果s的发送缓冲中没有数据或者数据被协议成功发送完毕后，recv先检查套接字s的接收缓冲区，如果s接收缓冲区中没有数据或者协议正在接收数据，那么recv就一直等待，直到协议把数据接收完毕。当协议把数据接收完毕，recv函数就把s的接收缓冲中的数据copy到buf中；

协议接收到的数据可能大于buf的长度，所以在这种情况下要调用几次recv函数才能把s的接收缓冲中的数据copy完。recv函数仅仅是copy数据，真正的接收数据是协议来完成的,recv函数返回其实际copy的字节数。如果recv在copy时出错，那么它返回SOCKET_ERROR；如果recv函数在等待协议接收数据时网络中断了，那么它返回0。

注意：在Unix系统下，如果recv函数在等待协议接收数据时网络断开了，那么调用recv的进程会接收到一个SIGPIPE信号，进程对该信号的默认处理是进程终止。

## 5.**send返回值**

在Unix系统下，如果send 、 recv 、 write在等待协议传送数据时 ， socket 被 shutdown，调用send的进程会接收到一个SIGPIPE信号，进程对该信号的默认处理是进程终止。

SIGPIPE 信号：

对一个已经收到FIN包的socket调用read方法, 如果接收缓冲已空, 则返回0, 这就是常说的表示连接关闭. 但第一次对其调用write方法 时, 如果发送缓冲没问题, 会返回正确写入(发送). 但发送的报文会导致对端发送RST报文, 因为对端的socket已经调用了close, 完全 关闭, 既不发送, 也不接收数据. 所以, 第二次调用write方法(假设在收到RST之后), 会生成SIGPIPE信号, 导致进程退出 。如果对 SIGPIPE 进行忽略处理， 二次调用write方法时, 会返回-1, 同时errno置为SIGPIPE。

处理方法：

在初始化时调用 signal(SIGPIPE,SIG_IGN) 忽略该信号（只需一次） ，SIGPIPE交给了系统处理。 此时 send 、recv或write 函数将返回-1，errno为EPIPE，可视情况关闭socket或其他处理

SIGPIPE 被忽略的情况下，如果 服务器采用了fork的话，要收集垃圾进程，防止僵尸进程的产生，可以这样处理： signal(SIGCHLD,SIG_IGN);　交给系统init去回收。 这样 子进程就不会产生僵尸进程了。

**小结**

在Linux环境下开发经常会碰到很多错误(设置errno)，其中EAGAIN是其中比较常见的一个错误(比如用在非阻塞操作中)。

从字面上来看，是提示再试一次。这个错误经常出现在当应用程序进行一些非阻塞(non-blocking)操作(对文件或socket)的时候。例如，以 O_NONBLOCK的标志打开文件/socket/FIFO，如果你连续做read操作而没有数据可读。此时程序不会阻塞起来等待数据准备就绪返 回，read函数会返回一个错误EAGAIN，提示你的应用程序现在没有数据可读请稍后再试。

又例如，当一个系统调用(比如fork)因为没有足够的资源(比如虚拟内存)而执行失败，返回EAGAIN提示其再调用一次(也许下次就能成功)。

Linux - 非阻塞socket编程处理EAGAIN错误

在linux进行非阻塞的socket接收数据时经常出现Resource temporarily unavailable，errno代码为11(EAGAIN)，这是什么意思？

这表明你在非阻塞模式下调用了阻塞操作，在该操作没有完成就返回这个错误，这个错误不会破坏socket的同步，不用管它，下次循环接着recv就可以。 对非阻塞socket而言，EAGAIN不是一种错误。在VxWorks和Windows上，EAGAIN的名字叫做EWOULDBLOCK。

另外，如果出现EINTR即errno为4，错误描述Interrupted system call，操作也应该继续。

最后，如果recv的返回值为0，那表明连接已经断开，我们的接收操作也应该结束。

当客户通过Socket提供的send函数发送大的数据包时，就可能返回一个EGGAIN的错误。该错误产生的原因是由于send函数 中的size变量大小超过了tcp_sendspace的值。tcp_sendspace定义了应用在调用send之前能够在kernel中缓存的数据 量。当应用程序在socket中设置了O_NDELAY或者O_NONBLOCK属性后，如果发送缓存被占满，send就会返回EAGAIN的错误。

为了消除该错误，有以下几种方法可以选择：

1.调大tcp_sendspace，使之大于send中的size参数

—no -p -o tcp_sendspace=65536

2.在调用send前，在setsockopt函数中为SNDBUF设置更大的值

你自己的缓冲区满了，会返回EAGAIN，你的没满，对方的缓冲区满了,肯定不关你事，可能会发送不成功，但是协议栈提供的系统调用，只管数据成功从你的缓冲区发出去，之后人家因为缓冲区满收不到数据，tcp自己有重传机制（参考Tcp/ip详解卷1）。

send()适用于已连接的数据包或流式套接口发送数据。对于数据报类套接口，必需注意发送数据长度不应超过通讯子网的IP包最大长度。IP包最大长度在WSAStartup()调用返回的WSAData的iMaxUdpDg元素中。如果数据太长无法自动通过下层协议，则返回WSAEMSGSIZE错误，数据不会被发送。

请注意成功地完成send()调用并不意味着数据传送到达。

如果传送系统的缓冲区空间不够保存需传送的数据，除非套接口处于非阻塞I/O方式，否则send()将阻塞。对于非阻塞SOCK_STREAM类型的套接口，实际写的数据数目可能在1到所需大小之间，其值取决于本地和远端主机的缓冲区大小。可用select()调用来确定何时能够进一步发送数据。

## 6.服务端判断客户端断开的经验方法

当recv()返回值小于等于0时，socket连接断开。但是还需要判断 errno是否等于 EINTR，如果errno == EINTR

则说明recv函数是由于程序接收到信号后返回的，socket连接还是正常的，不应close掉socket连接。

主动检测

```text
struct tcp_info info; 
int len=sizeof(info); 
getsockopt(sock, IPPROTO_TCP, TCP_INFO, &info, (socklen_t *)&len); 
if((info.tcpi_state==TCP_ESTABLISHED))  
    //则说明未断开  
else
    //断开
```

若使用了select等系统函数，若远端断开，则select返回1，recv返回0则断开。其他注意事项同法一。

keepalive

```text
int keepAlive = 1; // 开启keepalive属性
int keepIdle = 60; // 如该连接在60秒内没有任何数据往来,则进行探测 
int keepInterval = 5; // 探测时发包的时间间隔为5 秒
int keepCount = 3; // 探测尝试的次数.如果第1次探测包就收到响应了,则后2次的不再发.
 
setsockopt(rs, SOL_SOCKET, SO_KEEPALIVE, (void *)&keepAlive, sizeof(keepAlive));
setsockopt(rs, SOL_TCP, TCP_KEEPIDLE, (void*)&keepIdle, sizeof(keepIdle));
setsockopt(rs, SOL_TCP, TCP_KEEPINTVL, (void *)&keepInterval, sizeof(keepInterval));
setsockopt(rs, SOL_TCP, TCP_KEEPCNT, (void *)&keepCount, sizeof(keepCount));
设置后，若断开，则在使用该socket读写时立即失败，并返回ETIMEDOUT错误
```

自己实现一个心跳检测，一定时间内未收到自定义的心跳包则标记为已断开。

原文地址：https://zhuanlan.zhihu.com/p/516485158

作者：CPP后端技术

# 【NO.604】MySQL 死锁案例解析，能让你彻底理解死锁的原因！

## 1.**Mysql 锁类型和加锁分析**

MySQL有三种锁的级别：页级、表级、行级。

- 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高,并发度最低。
- 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低,并发度也最高。
- 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度

算法：

- next KeyLocks锁，同时锁住记录(数据)，并且锁住记录前面的Gap
- Gap锁，不锁记录，仅仅记录前面的Gap
- Recordlock锁（锁数据，不锁Gap）
- 所以其实 Next-KeyLocks=Gap锁+ Recordlock锁

## 2.**死锁产生原因和示例**

### 2.1 **产生原因**

所谓死锁<DeadLock>：是指两个或两个以上的进程在执行过程中,因争夺资源而造成的一种互相等待的现象,若无外力作用，它们都将无法推进下去.此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。表级锁不会产生死锁.所以解决死锁主要还是针对于最常用的InnoDB。

死锁的关键在于：两个(或以上)的Session加锁的顺序不一致。

那么对应的解决死锁问题的关键就是：让不同的session加锁有次序。

### 2.2 **示例**

### 2.2.1 **案例一**

需求：将投资的钱拆成几份随机分配给借款人。

起初业务程序思路是这样的：

投资人投资后，将金额随机分为几份，然后随机从借款人表里面选几个，然后通过一条条select for update 去更新借款人表里面的余额等。

例如两个用户同时投资，A用户金额随机分为2份，分给借款人1，2，B用户金额随机分为2份，分给借款人2，1。

由于加锁的顺序不一样，死锁当然很快就出现了。对于这个问题的改进很简单，直接把所有分配到的借款人直接一次锁住就行了。

Select * from xxx where id in (xx,xx,xx) for update

在in里面的列表值mysql是会自动从小到大排序，加锁也是一条条从小到大加的锁。

```text
例如（以下会话id为主键）：

Session1:

mysql> select * from t3 where id in (8,9) for update;
+----+--------+------+---------------------+
| id | course | name | ctime               |
+----+--------+------+---------------------+
|  8 | WA     | f    | 2016-03-02 11:36:30 |
|  9 | JX     | f    | 2016-03-01 11:36:30 |
+----+--------+------+---------------------+
rows in set (0.04 sec)
Session2:
select * from t3 where id in (10,8,5) for update;
锁等待中……

其实这个时候id=10这条记录没有被锁住的，但id=5的记录已经被锁住了，锁的等待在id=8的这里
不信请看

Session3:
mysql> select * from t3 where id=5 for update;
锁等待中


Session4:
mysql> select * from t3 where id=10 for update;
+----+--------+------+---------------------+
| id | course | name | ctime               |
+----+--------+------+---------------------+
| 10 | JB     | g    | 2016-03-10 11:45:05 |
+----+--------+------+---------------------+
row in set (0.00 sec)
在其它session中id=5是加不了锁的，但是id=10是可以加上锁的。
```

### 2.2.2 案例二

在开发中，经常会做这类的判断需求：根据字段值查询（有索引），如果不存在，则插入；否则更新。

```text
以id为主键为例，目前还没有id=22的行

Session1:
select * from t3 where id=22 for update;
Empty set (0.00 sec)

session2:
select * from t3 where id=23  for update;
Empty set (0.00 sec)

Session1:
insert into t3 values(22,'ac','a',now());
锁等待中……

Session2:
insert into t3 values(23,'bc','b',now());
ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction
```

当对`存在的行`进行锁的时候(主键)，mysql就只有行锁。

当对`未存在的行`进行锁的时候(即使条件为主键)，mysql是会锁住一段范围（有gap锁）。

锁住的范围为：

(无穷小或小于表中锁住id的最大值，无穷大或大于表中锁住id的最小值)

如：如果表中目前有已有的id为（11 ， 12），那么就锁住（12，无穷大）。如果表中目前已有的id为（11 ， 30），那么就锁住（11，30）。

对于这种死锁的解决办法是：

insert into t3(xx,xx) on duplicate key update `xx`='XX';

用mysql特有的语法来解决此问题。因为insert语句对于主键来说，插入的行不管有没有存在，都会只有行锁。

### 2.2.3 案例三

```text
mysql> select * from t3 where id=9 for update;

+----+--------+------+---------------------+
| id | course | name | ctime               |
+----+--------+------+---------------------+
|  9 | JX     | f    | 2016-03-01 11:36:30 |
+----+--------+------+---------------------+

row in set (0.00 sec)
Session2:
mysql> select * from t3 where id<20 for update;
锁等待中

Session1:
mysql> insert into t3 values(7,'ae','a',now());
ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction
```

这个跟案例一其它是差不多的情况，只是session1不按常理出牌了，Session2在等待Session1的id=9的锁，session2又持了1到8的锁（注意9到19的范围并没有被session2锁住），最后，session1在插入新行时又得等待session2,故死锁发生了。

这种一般是在业务需求中基本不会出现，因为你锁住了id=9，却又想插入id=7的行，这就有点跳了，当然肯定也有解决的方法，那就是重理业务需求，避免这样的写法。

**相关视频推荐**

[90分钟搞懂分布式锁以及数据库锁](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1uf4y1W7KJ/)

[腾讯、阿里等大厂面试，不了解这些MySQL, InnoDB技术，何以征服面试官](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1fV411d7oT/)

学习地址：[c/c++ linux服务器开发/后台架构师](https://link.zhihu.com/?target=https%3A//ke.qq.com/course/417774%3FflowToken%3D1013300)

**【文章福利】**需要C/C++ Linux服务器架构师学习资料加qun**[812855908](https://link.zhihu.com/?target=https%3A//jq.qq.com/%3F_wv%3D1027%26k%3DeVVOqR0S)**（资料包括C/C++，Linux，golang技术，Nginx，ZeroMQ，MySQL，Redis，fastdfs，MongoDB，ZK，流媒体，CDN，P2P，K8S，Docker，TCP/IP，协程，DPDK，ffmpeg等）

![img](https://pic4.zhimg.com/80/v2-08764f6c32e4bc2c2f33342defba99e3_720w.webp)

### 2.2.4 案例四

![img](https://pic4.zhimg.com/80/v2-27b92b8d35c512453e59019d644954e3_720w.webp)

一般的情况，两个session分别通过一个sql持有一把锁，然后互相访问对方加锁的数据产生死锁。

### 2.2.5 **案例五**

![img](https://pic1.zhimg.com/80/v2-48ea15bc3acc5f62cb7d043c8599a66c_720w.webp)

两个单条的sql语句涉及到的加锁数据相同，但是加锁顺序不同，导致了死锁。

### 2.2.6 案例六

死锁场景如下：

```text
CREATE TABLE dltask (
    id bigint unsigned NOT NULL AUTO_INCREMENT COMMENT ‘auto id’,
    a varchar(30) NOT NULL COMMENT ‘uniq.a’,
    b varchar(30) NOT NULL COMMENT ‘uniq.b’,
    c varchar(30) NOT NULL COMMENT ‘uniq.c’,
    x varchar(30) NOT NULL COMMENT ‘data’,   
    PRIMARY KEY (id),
    UNIQUE KEY uniq_a_b_c (a, b, c)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=’deadlock test’;
```

a，b，c三列，组合成一个唯一索引，主键索引为id列。

事务隔离级别：

RR (Repeatable Read)

每个事务只有一条SQL:

```text
delete from dltask where a=? and b=? and c=?;
```

SQL的执行计划

![img](https://pic1.zhimg.com/80/v2-bece3514b5e15f98e02270faaf35d660_720w.webp)

死锁日志

![img](https://pic1.zhimg.com/80/v2-f461f2de51c7dd25f13bef57079d376c_720w.webp)

众所周知，InnoDB上删除一条记录，并不是真正意义上的物理删除，而是将记录标识为删除状态。(注：这些标识为删除状态的记录，后续会由后台的Purge操作进行回收，物理删除。但是，删除状态的记录会在索引中存放一段时间。) 在RR隔离级别下，唯一索引上满足查询条件，但是却是删除记录，如何加锁？

InnoDB在此处的处理策略与前两种策略均不相同，或者说是前两种策略的组合：对于满足条件的删除记录，InnoDB会在记录上加next key lock X(对记录本身加X锁，同时锁住记录前的GAP，防止新的满足条件的记录插入。) Unique查询，三种情况，对应三种加锁策略，总结如下：

此处，我们看到了next key锁，是否很眼熟？对了，前面死锁中事务1，事务2处于等待状态的锁，均为next key锁。明白了这三个加锁策略，其实构造一定的并发场景，死锁的原因已经呼之欲出。但是，还有一个前提策略需要介绍，那就是InnoDB内部采用的死锁预防策略。

- 找到满足条件的记录，并且记录有效，则对记录加X锁，No Gap锁(lock_mode X locks rec but not gap)；
- 找到满足条件的记录，但是记录无效(标识为删除的记录)，则对记录加next key锁(同时锁住记录本身，以及记录之前的Gap：lock_mode X);
- 未找到满足条件的记录，则对第一个不满足条件的记录加Gap锁，保证没有满足条件的记录插入(locks gap before rec)；

## 3.**死锁预防策略**

InnoDB引擎内部(或者说是所有的数据库内部)，有多种锁类型：事务锁(行锁、表锁)，Mutex(保护内部的共享变量操作)、RWLock(又称之为Latch，保护内部的页面读取与修改)。

InnoDB每个页面为16K，读取一个页面时，需要对页面加S锁，更新一个页面时，需要对页面加上X锁。任何情况下，操作一个页面，都会对页面加锁，页面锁加上之后，页面内存储的索引记录才不会被并发修改。

因此，为了修改一条记录，InnoDB内部如何处理：

- 根据给定的查询条件，找到对应的记录所在页面；
- 对页面加上X锁(RWLock)，然后在页面内寻找满足条件的记录；
- 在持有页面锁的情况下，对满足条件的记录加事务锁(行锁：根据记录是否满足查询条件，记录是否已经被删除，分别对应于上面提到的3种加锁策略之一)；

死锁预防策略：相对于事务锁，页面锁是一个短期持有的锁，而事务锁(行锁、表锁)是长期持有的锁。因此，为了防止页面锁与事务锁之间产生死锁。InnoDB做了死锁预防的策略：持有事务锁(行锁、表锁)，可以等待获取页面锁；但反之，持有页面锁，不能等待持有事务锁。

根据死锁预防策略，在持有页面锁，加行锁的时候，如果行锁需要等待。则释放页面锁，然后等待行锁。此时，行锁获取没有任何锁保护，因此加上行锁之后，记录可能已经被并发修改。因此，此时要重新加回页面锁，重新判断记录的状态，重新在页面锁的保护下，对记录加锁。如果此时记录未被并发修改，那么第二次加锁能够很快完成，因为已经持有了相同模式的锁。但是，如果记录已经被并发修改，那么，就有可能导致本文前面提到的死锁问题。

以上的InnoDB死锁预防处理逻辑，对应的函数，是row0sel.c::row_search_for_mysql()。感兴趣的朋友，可以跟踪调试下这个函数的处理流程，很复杂，但是集中了InnoDB的精髓。

## 4.**剖析死锁的成因**

做了这么多铺垫，有了Delete操作的3种加锁逻辑、InnoDB的死锁预防策略等准备知识之后，再回过头来分析本文最初提到的死锁问题，就会手到拈来，事半而功倍。

首先，假设dltask中只有一条记录：(1, ‘a’, ‘b’, ‘c’, ‘data’)。三个并发事务，同时执行以下的这条SQL：

```text
delete from dltask where a=’a’ and b=’b’ and c=’c’;
```

并且产生了以下的并发执行逻辑，就会产生死锁：

![img](https://pic3.zhimg.com/80/v2-2df4c86c801f02dc552e48a28cd161f2_720w.webp)

上面分析的这个并发流程，完整展现了死锁日志中的死锁产生的原因。其实，根据事务1步骤6，与事务0步骤3/4之间的顺序不同，死锁日志中还有可能产生另外一种情况，那就是事务1等待的锁模式为记录上的X锁 + No Gap锁(lock_mode X locks rec but not gap waiting)。这第二种情况，也是”润洁”同学给出的死锁用例中，使用MySQL 5.6.15版本测试出来的死锁产生的原因。

此类死锁，产生的几个前提：

- Delete操作，针对的是唯一索引上的等值查询的删除；(范围下的删除，也会产生死锁，但是死锁的场景，跟本文分析的场景，有所不同)
- 至少有3个(或以上)的并发删除操作；
- 并发删除操作，有可能删除到同一条记录，并且保证删除的记录一定存在；
- 事务的隔离级别设置为Repeatable Read，同时未设置innodb_locks_unsafe_for_binlog参数(此参数默认为FALSE)；(Read Committed隔离级别，由于不会加Gap锁，不会有next key，因此也不会产生死锁)
- 使用的是InnoDB存储引擎；(废话！MyISAM引擎根本就没有行锁)

## 5.**查看死锁和解除锁**

解除正在死锁的状态有两种方法：

第一种：

\1. 查询是否锁表

```text
show OPEN TABLES where In_use > 0;
```

\2. 查询进程（如果您有SUPER权限，您可以看到所有线程。否则，您只能看到您自己的线程）

```text
show processlist
```

\3. 杀死进程id（就是上面命令的id列）

```text
kill id
```

第二种：

\1. 查看下在锁的事务

```text
SELECT * FROM INFORMATION_SCHEMA.INNODB_TRX;
```

\2. 杀死进程id（就是上面命令的trx_mysql_thread_id列）

```text
kill 线程ID
```

示例：

查出死锁进程：SHOW PROCESSLIST

杀掉进程 KILL 420821;

其它关于查看死锁的命令：

\1. 查看当前的事务

```text
SELECT * FROM INFORMATION_SCHEMA.INNODB_TRX;
```

\2. 查看当前锁定的事务

```text
SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS;
```

\3. 查看当前等锁的事务

```text
SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS;
```

原文地址：https://zhuanlan.zhihu.com/p/501865137

作者：CPP后端技术

# 【NO.605】C++之内存管理：申请与释放

## 1.C/C++内存分布

### **1.1虚拟内存分段**

一所学校，在设计的时候是有其规划的。要有宿舍楼，要有办公楼，要有教学楼，要有图书馆，要有体育馆，要有食堂，要有礼堂……，不同的建筑发挥不同的作用，使校园整体功能齐全，设计合理。

内存也是如此，我们称之为虚拟内存分段。

![img](https://pic4.zhimg.com/80/v2-2673b2b190081bee5bd42f4ec0acf367_720w.webp)

注意：

1. 栈又叫堆栈，非静态局部变量/函数参数/返回值等等，栈是向下增长的。
2. 内存映射段是高效的I/O映射方式，用于装载一个共享的动态内存库。用户可使用系统接口创建共享共享内存，做进程间通信。
3. 堆用于程序运行时动态内存分配，堆是可以上增长的。
4. 数据段–存储全局数据和静态数据。
5. 代码段–可执行的代码/只读常量。

### **1.2理解一些概念**

1.2.1栈帧向下增长

> 栈又叫堆栈，非静态局部变量/函数参数/返回值等等，栈是向下增长的。

那么，什么叫做向下增长呢？我们来看一个例子：

```text
#include<iostream>
using namespace std;

void f2()
{
	int b = 0;
	cout <<"b:"<< &b << endl;
	cout << endl;
	cout << endl;
}

void f1()
{
	int a = 0;
	cout<<"a:" << &a << endl;
	cout << endl;
	cout << endl;
	f2();
}

int main()
{
	f1();
	return 0;
}
```

在这段代码中，函数调用的时候会在内存上建立栈帧。程序由main函数进入，接着调用f1，接着再调用f2.

![img](https://pic4.zhimg.com/80/v2-f6a616d9002142da77881afc09ab7ebb_720w.webp)

我们来看一下输出的，a和b的地址的结果：

![img](https://pic1.zhimg.com/80/v2-6eeff770f633cd63619104182c7b2504_720w.webp)

我们发现，a的地址是比b的地址高的，也就是说在栈帧生长的过程中，地址是从高到低的，所以我们说，栈是向下生长的。

然后通过这个例子我们对局部变量的生命周期也可以有一个新的理解，出栈的时候栈会销毁，局部变量也随之消失，这就是局部变量的生命周期。

1.2.2堆向上生长

堆和栈类似，但堆是向上生长的。

![img](https://pic2.zhimg.com/80/v2-e5b80d53f68788bd1bd9878d1ba39e1d_720w.webp)

但是，后申请的空间的地址一定会比先申请的大吗？

当我们多申请几次，来看一下：

![img](https://pic4.zhimg.com/80/v2-6320d3ed14c505076f0db28013043663_720w.webp)

这又是为什么呢？因为内存不断申请和释放，所以我们有可能申请到前面释放过的空间，这就可能会导致这种情况的产生。所以堆是可以向上增长的，但不一定。

1.2.3栈和堆会碰撞吗？

我们上面已经知道，栈和堆一个是向下生长，一个是向上生长，这么双向生长难道不会碰撞吗？

![img](https://pic1.zhimg.com/80/v2-9bc6eba00a3fd7432bd562d6ede5be7c_720w.webp)

答案是不会的。

栈并不是一直可以向下走的，栈是有规定大小的。堆很大，但也是有上限的。所以会存在失败的栈溢出和malloc失败的情况。

1.2.4关于const的说明

我们说常量区是是储存常量的，那么const定义的常量是存在常量区吗？

答案是不是的，注意，const定义的是常变量，本质还是变量哦，记住，本质是变量，所以不在常量区。那么如果const定义在函数里面，就在栈区，如果定义成static就在静态区。

## 2.C语言中动态内存管理方式

### **2.1malloc/calloc/realloc和free**

```text
void Test ()
{
int* p1 = (int*) malloc(sizeof(int));
free(p1);
int* p2 = (int*)calloc(4, sizeof (int));
int* p3 = (int*)realloc(p2, sizeof(int)*10);
free(p3 );
}
```

### **2.2 malloc/calloc/realloc的区别**

![img](https://pic3.zhimg.com/80/v2-169738f8fd9b0830e16867221af7e39a_720w.webp)

## 3.C++内存管理方式

C语言内存管理方式在C++中可以继续使用，但有些地方就无能为力而且使用起来比较麻烦，因此C++又提出了自己的内存管理方式：通过new和delete操作符进行动态内存管理

### **3.1 new/delete操作内置类型**

![img](https://pic1.zhimg.com/80/v2-813ee9feb3c7552754abe73345a4e42c_720w.webp)

3.1.1malloc和freeVSnew和delete

我们以申请10个int的数组为例

![img](https://pic2.zhimg.com/80/v2-3ed5b6414a7bd4cf6a7be5c80275d47d_720w.webp)

申请和释放单个元素的空间，使用new和delete操作符，申请和释放连续的空间，使用new[]和delete[]

3.1.2 使用new动态申请的实例

```text
void Test()
{
// 动态申请一个int类型的空间
int* ptr4 = new int;
// 动态申请一个int类型的空间并初始化为10
int* ptr5 = new int(10);
// 动态申请10个int类型的空间
int* ptr6 = new int[3];
delete ptr4;
delete ptr5;
delete[] ptr6;
}
```

### 3.2 new和delete操作自定义类型

```text
#include<iostream>
using namespace std;

struct ListNode
{
	ListNode *_next;
	ListNode *_prev;
	int _val;
	ListNode(int val = 0)
		:_next(nullptr), _prev(nullptr), _val(val)
	{}
};
int main()
{
	//C
	struct ListNode *n1 = (struct ListNode *)malloc(sizeof(struct ListNode));

	//C++
	ListNode *n2 = new ListNode;

	return 0;
}
```

我们来看看上面的代码，我们分别开辟了n1和n2，他们会有什么不同吗？我们来看调试结果。

![img](https://pic3.zhimg.com/80/v2-2e295e455ef45bdab5c0213e4bd0cde6_720w.webp)

所以我们可以知道，malloc只是开空间，而new针对自定义类型时，是开空间加构造函数初始化。

如果我们再使用free和delete，再来看看。当我们屏蔽掉new。

![img](https://pic4.zhimg.com/80/v2-8dfb092c5addc0f1173d5155851c96e3_720w.webp)

而当我们屏蔽掉malloc

![img](https://pic1.zhimg.com/80/v2-4d5e6c83758ff443e57167b098b113e0_720w.webp)

我们发现，在delete的同时，也调用了析构函数。而free并不能调用析构函数。

于是，我们可以得出结论：申请自定义类型的空间时，new会调用构造函数，delete会调用析构函数，而malloc与free不会。

![img](https://pic1.zhimg.com/80/v2-f9f032a7c49c0cd44096a58e68964dfc_720w.webp)

## 4.operator new与operator delete函数

### **4.1 operator new与operator delete函数**

new和delete是用户进行动态内存申请和释放的操作符，operator new 和operator delete是系统提供的全局函数，new在底层调用operator new全局函数来申请空间，delete在底层通过operator delete全局函数来释放空间。

4.1.1对比申请失败处理方式

他们的用法和malloc和free一样，都是在堆上申请释放空间，但是失败了的处理方式不同，malloc是返回空指针，而operator new是抛异常

```text
#include<iostream>
using namespace std;

struct ListNode
{
	ListNode *_next;
	ListNode *_prev;
	int _val;
	ListNode(int val = 0)
		:_next(nullptr), _prev(nullptr), _val(val)
	{}
	~ListNode()
	{
		cout << "ListNode 析构" << endl;
	}
};
int main()
{
	struct ListNode *n1 = (struct ListNode *)malloc(sizeof(struct ListNode));
	free(n1);
	struct ListNode *n2 = (struct ListNode *)operator new(sizeof(struct ListNode));
	operator delete (n2);
	void *p3 = malloc(0x7fffffff);
	if (p3 == NULL)
	{
		cout << "malloc fail" << endl;
	}
	try
	{
		void *p4 = operator new(0x7fffffff);
	}
	catch (exception &e)
	{
		cout << e.what() << endl;
	}
	return 0;
}
```

![img](https://pic1.zhimg.com/80/v2-ec0379f3a365b22b19bbcd70198c13d8_720w.webp)

4.1.2 operator new和operator delete原理探寻

4.1.2.1 operator new实现原理

operator new：该函数实际通过malloc来申请空间，当malloc申请空间成功时直接返回；申请空间失败，尝试执行空 间不足应对措施，如果改应对措施用户设置了，则继续申请，否则抛异常。

```text
void *__CRTDECL operator new(size_t size) _THROW1(_STD bad_alloc)
{
// try to allocate size bytes
void *p;
while ((p = malloc(size)) == 0)
if (_callnewh(size) == 0)
{
// report no memory
// 如果申请内存失败了，这里会抛出bad_alloc 类型异常
static const std::bad_alloc nomem;
_RAISE(nomem);
}
return (p);
}
```

4.1.2.2 operator delete 实现原理

operator delete: 该函数最终是通过free来释放空间的

```text
void operator delete(void *pUserData)
{
_CrtMemBlockHeader * pHead;
RTCCALLBACK(_RTC_Free_hook, (pUserData, 0));
if (pUserData == NULL)
return;
_mlock(_HEAP_LOCK); /* block other threads */
__TRY
/* get a pointer to memory block header */
pHead = pHdr(pUserData);
/* verify block type */
_ASSERTE(_BLOCK_TYPE_IS_VALID(pHead->nBlockUse));
_free_dbg( pUserData, pHead->nBlockUse );
__FINALLY
_munlock(_HEAP_LOCK); /* release other threads */
__END_TRY_FINALLY
return;
}
```

### **4.2 operator new与operator delete的类专属重载**

4.2.1理解为什么要专属重载

专属重载，听起来很高端的样子。那么重载他们有什么好处呢？为了弄清楚这个问题，我们首先要先了解一个概念，池化技术。

什么叫池化技术，举一个例子，嗯，作为一名贫穷的大学生，生活费的来源是家长。如果我们的生活费是随花随给的，早上你上食堂吃了两根油条喝了一碗豆浆，要付款的时候你大手一挥和食堂大妈说等等，我要找我妈要钱。于是你给你妈妈打了一个电话，妈妈给你转了5块钱，你付了早饭钱（这一切建立在你早上能起床吃早饭的前提下，像我这种，笑死，早饭根本来不及，只能吃午饭了）。嗯，然后午饭你从食堂买了一碗面，再给妈妈打电话要了10元，然后你付了午饭的钱……就这样，你每花一次钱都需要找家长要一次，是不是很麻烦？还特别浪费你和妈妈之间的感情？

但是如果，哎，每个月月初，妈妈给你打1500元，对你说，孩儿啊，你这一个月生活费在这了哈，多退少不补哈。哎，这样你就爽了，油条豆浆随便刷啊，想买啥买啥，等到月底，呵呵，你懂。

池化技术就类似于给你一个月的生活费使你可以在需要花钱的时候自己就可以支付，而不再需要向妈妈要。在池化技术中，分为很多种类的池：

![img](https://pic4.zhimg.com/80/v2-92b7de916de16e4e541d5d5ac005aa33_720w.webp)

池中预存了我们需要的“生活费”，需要时直接自行支付，这样就提高了程序的效率。专属重载operator new和operator delete就是使用内存池进行申请和释放空间，可以提高效率。

![img](https://pic3.zhimg.com/80/v2-785663accc4a6b9fda4ec11ffff4fa8a_720w.webp)

4.2.2 operator new和operator delete 专属重载的实现

针对链表的节点ListNode通过重载类专属 operator new/ operator delete，实现链表节点使用内存池申请和释放内存，提高效率。

```text
#include<iostream>
using namespace std;

struct ListNode
{
	ListNode *_next;
	ListNode *_prev;
	int _val;
	//类中专属的重载
	void* operator new(size_t n)
	{
		void* p = nullptr;
		p = allocator<ListNode>().allocate(1);
		cout << "memory pool allocate" << endl;
		return p;
	}
	void operator delete(void* p)
	{
		allocator<ListNode>().deallocate((ListNode*)p, 1);
		cout << "memory pool deallocate" << endl;
	}
	ListNode(int val = 0)
		:_next(nullptr), _prev(nullptr), _val(val)
	{}
	~ListNode()
	{
		cout << "ListNode 析构" << endl;
	}
};

int main()
{
	ListNode* p = new ListNode(1);
	delete p;

}
```

我们来对比一下使用重载和不使用重载的反汇编代码

![img](https://pic1.zhimg.com/80/v2-4bbc6039438023fadb69d6724421194c_720w.webp)

专属重载还是很香的，毕竟谁不想要一笔可以自由支配的生活费呢？

## 5.new和delete的实现原理

### **5.1内置类型**

如果申请的是内置类型的空间，new和malloc，delete和free基本类似，不同的地方是：new/delete申请和释放的是单个元素的空间，new[]和delete[]申请的是连续空间，而且new在申请空间失败时会抛异常，malloc会返回NULL。

### **5.2自定义类型**

5.2.1 new的原理

调用operator new函数申请空间

在申请的空间上执行构造函数，完成对象的构造

5.2.2 delete的原理

在空间上执行析构函数，完成对象中资源的清理工作

调用operator delete函数释放对象的空间

5.2.3 new T[N]的原理

调用operator new[]函数，在operator new[]中实际调用operator new函数完成N个对象空间的申请。

在申请的空间上执行N次构造函数。

5.2.4 delete[]的原理

在释放的对象空间上执行N次析构函数，完成N个对象中资源的清理

调用operator delete[]释放空间，实际在operator delete[]中调用operator delete来释放空间

## 6.定位new表达式(placement-new)

定位new表达式是在已分配的原始内存空间中调用构造函数初始化一个对象。

### **6.1一个使用场景**

```text
#include<iostream>
using namespace std;

class A
{
public:
	A(int a = 0)
		:_a(a)
	{
		cout << "A" << this << endl;
	}
	~A()
	{
		cout << "~A" << this << endl;
	}
private:
	int _a;
};

int main()
{
	A*p = (A*)malloc(sizeof(A));
	return 0;
}
```

我们看上面的代码，我们申请了一块和A大小相同的空间，那么我们如何来初始化它呢？

这种直接初始化的方法肯定行不通

![img](https://pic3.zhimg.com/80/v2-4019c4140971eae8a14143d0b0f2867e_720w.webp)

我们说，调用构造函数初始化啊，这样可以吗？

![img](https://pic3.zhimg.com/80/v2-deff1c2ecd578b83b330b4610cf7246a_720w.webp)

还是报错的。为什么我们无法调用构造函数呢？

我们来想一下，我们是malloc了一块空间，空间的大小等于A而不是声明了一个A的对象，这块空间不是对象，然后将它强转为A*，所以我们是无法调用构造函数的。

那么我们如何初始化它呢？于是定位new横空出世。

### **6.2 使用格式**

> new (place_address) type或者new (place_address) type(initializer-list)
> place_address必须是一个指针，initializer-list 是类型的初始化列表

### **6.3 定位new的使用**

定位new表达式在实际中一般是配合内存池使用。因为内存池分配出的内存没有初始化，所以如果是自定义类型的对象，需要使用new的定义表达式进行显示调构造函数进行初始化。

```text
#include<iostream>
using namespace std;

class A
{
public:
	A(int a = 0)
		:_a(a)
	{
		cout << "A:" << _a<< endl;
	}
	~A()
	{
		cout << "~A:析构"  << endl;
	}
private:
	int _a;
};

int main()
{
	A*p = (A*)malloc(sizeof(A));
	new(p)A;//显示调用构造函数
	cout << endl;
	new(p)A(3);//显示调用构造函数并赋予初始化的值
	return 0;
}
```

![img](https://pic1.zhimg.com/80/v2-e8faafa1ee1f38bdc51262674f7c0814_720w.webp)

## 7.关于内存的常见问题

### **7.1 malloc和realloc的区别**

malloc/free和new/delete的共同点是：都是从堆上申请空间，并且需要用户手动释放。

不同的地方是：

1. malloc和free是函数，new和delete是操作符
2. malloc申请的空间不会初始化，new可以初始化
3. malloc申请空间时，需要手动计算空间大小并传递，new只需在其后跟上空间的类型即可
4. malloc的返回值为void*, 在使用时必须强转，new不需要，因为new后跟的是空间的类型
5. malloc申请空间失败时，返回的是NULL，因此使用时必须判空,new不需要，但是new需要捕获异常
6. 申请自定义类型对象时，malloc/free只会开辟空间，不会调用构造函数与析构函数，而new在申请空间后会调用构造函数完成对象的初始化，delete在释放空间前会调用析构函数完成空间中资源的清理

### **7.2 内存泄露**

7.2.1 什么是内存泄露，内存泄露的危害是什么？

什么是内存泄漏：内存泄漏指因为疏忽或错误造成程序未能释放已经不再使用的内存的情况。内存泄漏并不是指内存在物理上的消失，而是应用程序分配某段内存后，因为设计错误，失去了对该段内存的控制，因而造成了内存的浪费。

内存泄漏的危害：长期运行的程序出现内存泄漏，影响很大，如操作系统、后台服务等等，出现内存泄漏会导致响应越来越慢，最终卡死。

```text
void MemoryLeaks()
{
// 1.内存申请了忘记释放
int* p1 = (int*)malloc(sizeof(int));
int* p2 = new int;
// 2.异常安全问题
int* p3 = new int[10];
Func(); // 这里Func函数抛异常导致 delete[] p3未执行，p3没被释放.
delete[] p3;
}
```

7.2.1.1思考：内存泄露是指针丢了还是内存丢了？

答案是指针丢了。内存是不会丢的。我们在堆上申请了一块空间，我们拿着这块空间的指针可以访问这块空间。由于我们的疏忽，我们弄丢了指针，导致无法知道指针，无法通过指针来释放这块空间，导致内存泄露。

就像你钥匙丢了，进不去家门，你之所以进不去家门不是因为家丢了，而是钥匙丢了。这里指针就相当于钥匙，内存相当于家。内存是不会丢的，家永远在那里。

所以内存泄露丢的是指针哦。

7.2.1.2 关于申请和释放内存本质的思考

通过上面的问题，我们可以更加清晰的认识到:malloc和new申请一块内存的本质是向系统索要一块空间的使用权，free和delete的本质是当不使用这块空间时将这块空间归还给系统，系统可以再分配给别人。

就像租房，你向主人租了这间房（堆上的一块空间），房主把钥匙（指针）交给你，你可以通过钥匙访问这间房，你不再租这间房了。要把钥匙还给房东（释放指针），房东收回房子，他可以再把房子租给别人（再分配）。

7.2.2 内存泄露的分类

C/C++程序中一般我们关心两种方面的内存泄漏：

7.2.2.1 堆内存泄漏(Heap leak)

堆内存指的是程序执行中依据须要分配通过malloc / calloc / realloc / new等从堆中分配的一块内存，用完后必须通过调用相应的 free或者delete 删掉。假设程序的设计错误导致这部分内存没有被释放，那

么以后这部分空间将无法再被使用，就会产生Heap Leak。

7.2.2.2 系统资源泄漏

指程序使用系统分配的资源，比方套接字、文件描述符、管道等没有使用对应的函数释放掉，导致系统资源的浪费，严重可导致系统效能减少，系统执行不稳定。

### **7.3 如何申请4G的空间**

代码如下：

```text
#include <iostream>
using namespace std;
int main()
{
void* p = new char[0xfffffffful];
cout << "new:" << p << endl;
return 0;
}
```

## 8.后记

好的，这篇万字博客就先肝到这里了，博主在这里对内存的一些知识做了比较全面的介绍，希望对大家有所帮助。其实关于内存管理啊，我觉得重点就在于合理申请，及时释放，将有限的内存空间发挥出最大的价值。

原文地址：https://zhuanlan.zhihu.com/p/426417276

作者：CPP后端技术