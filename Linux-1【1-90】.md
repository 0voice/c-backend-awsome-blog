![image-20221221165457521](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221221165457521.png)

文章素材来源于微信公众号/知乎/csdn/头条/掘金/51cto/博客园等技术网站的优秀文章，版权属于原创作者（文章标注以及原文链接）
如有侵权，联系wangbojing@0voice.com 或留下的qq群联系，第一时间删除。由衷感谢各位优秀文章的作者，在互联网分享自己思想

# 【NO.1】【数据结构】史上最好理解的红黑树讲解，让你彻底搞懂红黑树

大家应该都学过平衡二叉树(AVLTree)，了解到AVL树的性质，其实平衡二叉树最大的作用就是查找,AVL树的查找、插入和删除在平均和最坏情况下都是O(logn)。AVL树的效率就是高在这个地方。如果在AVL树中插入或删除节点后，使得高度之差大于1。此时，AVL树的平衡状态就被破坏，它就不再是一棵二叉树；为了让它重新维持在一个平衡状态，就需要对其进行旋转处理, 那么创建一颗平衡二叉树的成本其实不小. 这个时候就有人开始思考，并且提出了红黑树的理论，红黑树在业界应用很广泛，比如 Java 中的 TreeMap，JDK 1.8 中的 HashMap、C++ STL 中的 map 均是基于红黑树结构实现的。那么红黑树到底比AVL树好在哪里？

## 1.红黑树简介

红黑树是一种自平衡的二叉查找树，是一种高效的查找树。它是由 Rudolf Bayer 于1978年发明，在当时被称为平衡二叉 B 树(symmetric binary B-trees)。后来，在1978年被 Leo J. Guibas 和 Robert Sedgewick 修改为如今的红黑树。红黑树具有良好的效率，它可在 O(logN) 时间内完成查找、增加、删除等操作。

## 2.为什么需要红黑树？

对于二叉搜索树，如果插入的数据是随机的，那么它就是接近平衡的二叉树，平衡的二叉树，它的操作效率（查询，插入，删除）效率较高，时间复杂度是O（logN）。但是可能会出现一种极端的情况，那就是插入的数据是有序的（递增或者递减），那么所有的节点都会在根节点的右侧或左侧，此时，二叉搜索树就变为了一个链表，它的操作效率就降低了，时间复杂度为O(N)，所以可以认为二叉搜索树的时间复杂度介于O（logN）和O(N)之间，视情况而定。那么为了应对这种极端情况，红黑树就出现了，它是具备了某些特性的二叉搜索树，能解决非平衡树问题，红黑树是一种接近平衡的二叉树（说它是接近平衡因为它并没有像AVL树的平衡因子的概念，它只是靠着满足红黑节点的5条性质来维持一种接近平衡的结构，进而提升整体的性能，并没有严格的卡定某个平衡因子来维持绝对平衡）。

## 3.红黑树的特性

在讲解红黑树性质之前，先简单了解一下几个概念：

- parent：父节点
- sibling：兄弟节点
- uncle：叔父节点（ parent 的兄弟节点）
- grand：祖父节点（ parent 的父节点）

首先，红黑树是一个二叉搜索树，它在每个节点增加了一个存储位记录节点的颜色，可以是RED,也可以是BLACK；通过任意一条从根到叶子简单路径上颜色的约束，红黑树保证最长路径不超过最短路径的二倍，因而近似平衡（最短路径就是全黑节点，最长路径就是一个红节点一个黑节点，当从根节点到叶子节点的路径上黑色节点相同时，最长路径刚好是最短路径的两倍）。它同时满足以下特性：

1. 节点是红色或黑色
2. 根是黑色
3. 叶子节点（外部节点，空节点）都是黑色，这里的叶子节点指的是最底层的空节点（外部节点），下图中的那些null节点才是叶子节点，null节点的父节点在红黑树里不将其看作叶子节点
4. 红色节点的子节点都是黑色
5. 红色节点的父节点都是黑色
6. 从根节点到叶子节点的所有路径上不能有 2 个连续的红色节点
7. 从任一节点到叶子节点的所有路径都包含相同数目的黑色节点

![image-20221206204445105](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204445105.png?lastModify=1670851010)

根据上面的性质，我们来判断一下下面这课树是不是红黑树

![image-20221206204501442](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204501442.png?lastModify=1670851010)

上面这棵树首先很容易就能知道是满足性质1-4条的，关键在于第5条性质，可能乍一看好像也是符合第5条的，但实际就会陷入一个误区，直接将图上的最后一层的节点看作叶子节点，这样看的话每一条从根节点到叶子结点的路径确实都经过了3个黑节点。

但实际上，在红黑树中真正被定义为叶子结点的，是那些空节点，如下图。

![image-20221206204513530](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204513530.png?lastModify=1670851010)

这样一来，路径1有4个黑色节点（算上空节点），路径2只有3个黑色节点，这样性质5就不满足了，所以这棵树并不是一个红黑树节点。

注：下面的讲解图中将省略红黑树的null节点，请自行脑补

## 4.红黑树的效率

### 4.1 红黑树效率

红黑树的查找，插入和删除操作，时间复杂度都是O(logN)。

查找操作时，它和普通的相对平衡的二叉搜索树的效率相同，都是通过相同的方式来查找的，没有用到红黑树特有的特性。

但如果插入的时候是有序数据，那么红黑树的查询效率就比二叉搜索树要高了，因为此时二叉搜索树不是平衡树，它的时间复杂度O(N)。

插入和删除操作时，由于红黑树的每次操作平均要旋转一次和变换颜色，所以它比普通的二叉搜索树效率要低一点，不过时间复杂度仍然是O(logN)。总之，红黑树的优点就是对有序数据的查询操作不会慢到O(logN)的时间复杂度。

### 4.2 红黑树和AVL树的比较

1. AVL树的时间复杂度虽然优于红黑树，但是对于现在的计算机，cpu太快，可以忽略性能差异
2. 红黑树的插入删除比AVL树更便于控制操作
3. 红黑树整体性能略优于AVL树（红黑树旋转情况少于AVL树）

## 5.红黑树的等价变换

![image-20221206204623858](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204623858.png?lastModify=1670851010)

上面这颗红黑树，我们来将所有的红色节点上移到和他们的父节点同一高度上，就会形成如下结构

![image-20221206204633358](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204633358.png?lastModify=1670851010)

这个结构很明显，就是一棵四阶B树（一个节点最多放三个数据），如果画成如下的样子大家应该就能看的更清晰了。

![image-20221206204646607](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204646607.png?lastModify=1670851010)

由上面的等价变换我们就可以得到如下结论：

1. 红黑树 和 4阶B树（2-3-4树）具有等价性
2. 黑色节点与它的红色子节点融合在一起，形成1个B树节点
3. 红黑树的黑色节点个数 与 4阶B树的节点总个数相等
4. 在所有的B树节点中，永远是黑色节点是父节点，红色节点是子节点。黑色节点在中间，红色节点在两边。

我们可以利用四阶B树与红黑树等价的性质，以红黑树转换成B树之后的节点情况来进行一个分类

![image-20221206204714100](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204714100.png?lastModify=1670851010)

## 6.红黑树的操作

红黑树的基本操作和其他树形结构一样，一般都包括查找、插入、删除等操作。前面说到，红黑树是一种自平衡的二叉查找树，既然是二叉查找树的一种，那么查找过程和二叉查找树一样，比较简单，这里不再赘述。相对于查找操作，红黑树的插入和删除操作就要复杂的多。尤其是删除操作，要处理的情况比较多，下面就来分情况讲解。

### 6.1 旋转操作

在分析插入和删除操作前，先说明一下旋转操作，这个操作在后续操作中都会用得到。旋转操作分为左旋和右旋，左旋是将某个节点旋转为其右孩子的左孩子，而右旋是节点旋转为其左孩子的右孩子。这话听起来有点绕，所以还是请看下图：

![image-20221206204739533](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204739533.png?lastModify=1670851010)

上图包含了左旋和右旋的示意图，这里以右旋为例进行说明，右旋节点 M 的步骤如下：

1. 将节点 M 的左孩子引用指向节点 E 的右孩子
2. 将节点 E 的右孩子引用指向节点 M，完成旋转

![image-20221206204755586](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204755586.png?lastModify=1670851010)

旋转操作本身并不复杂，上面分析了右旋操作，左旋操作与此类似，只是右旋转的逆操作。

### 6.2 插入操作

红黑树的插入过程和二叉查找树插入过程基本类似，不同的地方在于，红黑树插入新节点后，需要进行调整，以满足红黑树的性质。

性质1规定红黑树节点的颜色要么是红色要么是黑色，那么在插入新节点时，这个节点应该是红色还是黑色呢？答案是红色，原因也不难理解。如果插入的节点是黑色，那么这个节点所在路径比其他路径多出一个黑色节点，这个调整起来会比较麻烦（参考红黑树的删除操作，就知道为啥多一个或少一个黑色节点时，调整起来这么麻烦了）。如果插入的节点是红色，此时所有路径上的黑色节点数量不变，仅可能会出现两个连续的红色节点的情况。这种情况下，通过变色和旋转进行调整即可，比之前的简单多了。所以插入的时候将节点设置为红色，可以保证满足性质 1、2、3、5 ，只有性质4不一定满足，需要进行相关调整。如果是添加根节点，则将节点设定为黑色。

#### 6.2.1 插入操作的所有情况

我们在分析红黑树各种插入情况的时候，将其等价转换为B树，这样我们能够更直观的进行分类，首先确定几条性质：

1. B树中，新元素必定是添加到叶子节点中（最底层的节点）
2. 4阶B树所有节点的元素个数 x 都符合 1 ≤ x ≤ 3

![image-20221206204838937](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204838937.png?lastModify=1670851010)

在上一章节红黑树的等价变换中，我们讲到了红黑树转换成B树总共有四种情况，也就是上图中叶子节点这四种情况，那么在我们进行插入操作的时候，会将节点插入到所有的叶子节点中，总共就会有12种情况，其中四种情况满足红黑树的性质，8种情况不满足红黑树性质。

##### 6.2.1.1 满足红黑树性质4

有 4 种情况满足红黑树的性质 4 ：parent 为黑色节点。这四种情况不需要做任何额外的处理。

![image-20221206204904880](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204904880.png?lastModify=1670851010)

##### 6.2.1.2 不满足红黑树性质4

有 8 种情况不满足红黑树的性质 4 ：parent 为红色节点（ Double Red ），其中左面4种属于B树节点上溢的情况（一个4阶B树节点中最多存放三个数，这四种情况本来已经有3个了，又插入了1个，变成了4个，超出了4阶B树节点的容量范围，这种情况称为上溢）。这八种情况需要进行额外的处理。

![image-20221206204923844](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204923844.png?lastModify=1670851010)

#### 6.2.2 LL和RR插入情况

![image-20221206204942055](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206204942055.png?lastModify=1670851010)

如上图，插入52和60的位置分别是RR情况和LL情况。

**RR情况**：父节点为祖父节点的右节点，插入节点为父节点的右节点

**LL情况**：父节点为祖父节点的左节点，插入节点为父节点的左节点

这两种情况很明显，插入节点为红色，父节点也为红色，父节点的子节点为红色显然违背了红黑树的性质四，我们需要对这种情况进行修复，使其重新满足红黑树性质。

**判定条件**：uncle 不是红色节点。

这里的两种情况，他们的插入节点都是没有叔父节点的，所以叔父节点也不可能是红色。

**案例修复：**

我们在红黑树等价转换那一章节也讲过了，红黑树等价转换成B树之后，B树节点的中间节点（父节点）都是黑色，两边的节点（子节点）都是红色。但是上面两种情况插入后，插入位置的B树节点并不满足这个条件，所以我们对其进行修复，使其满足B树节点的条件之后，也就重新恢复了红黑树性质。

B树节点中的中间节点大小介于两个子节点之间。以上图RR情况为例，插入节点52的原父节点应该放在B树节点中间的位置，应当将其染成黑色。插入节点52的原祖父节点46，应当将其转换为插入节点原父节点的子节点，所以将其染成红色。LL情况同理

完成染色之后，需要对原祖父节点进行单旋操作，来进行父节点，子节点的重新分配。以上图为例：

- RR情况应该原祖父节点46左旋，将插入节点的原父节点50旋转到中间的位置。
- LL情况应当原祖父节点76右旋，将插入节点的原父节点72旋转到中间的位置。

修复之后的结果如下图：

![image-20221206205052353](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205052353.png?lastModify=1670851010)

修复步骤总结：

1. parent 染成黑色，grand 染成红色
2. grand 进行单旋操作

- LL：右旋转
- RR：左旋转

#### 6.2.3 LR和RL插入情况

![image-20221206205131992](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205131992.png?lastModify=1670851010)

如上图，插入48和74的位置分别是RL情况和LR情况。

**RL情况**：父节点为祖父节点的右节点，插入节点为父节点的左节点

**LR情况**：父节点为祖父节点的左节点，插入节点为父节点的右节点

这两种情况和上面的两种情况一样，插入节点为红色，父节点也为红色，父节点的子节点为红色显然违背了红黑树的性质四，我们需要对这种情况进行修复，使其重新满足红黑树性质。

**判定条件**：uncle 不是红色节点。

这两种情况的插入节点也是没有叔父节点的。

**案例修复**：

B树节点中的中间节点大小介于两个子节点之间。以上图RL情况为例，插入节点48大小介于原父节点和原祖父节点之间，它应该是B树节点中的中间节点，所以将插入节点48染成黑色，将原祖父节点46染成红色来作为插入节点的子节点。LR情况同理

完成染色之后，需要进行双旋操作，来进行父节点，子节点的重新分配。以上图为例：

- RL情况应该原父节点50右旋，将插入节点48上移到原父节点50的高度，然后将插入节点的原祖父节点46进行左旋，将插入节点48移动到中间位置，成为中间节点。
- LR情况应该原父节点72左旋，将插入节点74上移到原父节点72的高度，然后将插入节点的原祖父节点76进行右旋，将插入节点74移动到中间位置，成为中间节点。

修复之后的结果如下图：

![image-20221206205210410](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205210410.png?lastModify=1670851010)

修复步骤总结：

1. 插入节点染成黑色，grand 染成红色
2. 进行双旋操作

- LR：parent 左旋转， grand 右旋转
- RL：parent 右旋转， grand 左旋转

#### 6.2.4 上溢的LL插入情况

![image-20221206205238114](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205238114.png?lastModify=1670851010)

如上图，插入10的位置是上溢的LL情况。

**上溢LL情况**：父节点为祖父节点的左节点，插入节点为父节点的左节点。并且构成的新的B树节点已经超过了B树节点容量大小范围。

这种情况和之前非上溢的四种情况一样，插入节点为红色，父节点也为红色，父节点的子节点为红色显然违背了红黑树的性质四，我们需要对这种情况进行修复，使其重新满足红黑树性质。

**判定条件**：uncle 是红色节点。满足这个条件的就都是上溢的情况，上溢的修复只需要染色，不需要旋转。

**案例修复**：

像这种上溢的情况，就需要从溢出的B树节点中选出一个节点进行向上合并，选择B树节点中中间的树去进行向上合并，这里中间的两个节点就是原父节点17和原祖父节点25，选这两个哪一个向上合并都是对的，但是我们最好选择以后方便操作的，很显然，应该选择原祖父节点25来进行向上合并，因为向上合并就是和最上层的38和55来组合成新的B树节点，向上合并的节点肯定是一个子节点，需要与上层相连，而原祖父节点25本身就已经和上层连接了，相对更加方便后续的操作。原祖父节点向上合并后，将其染成红色。

原祖父节点25向上合并后，它原来左右两边的节点需要分裂成两个子树，也就是原父节点17和插入节点10形成一个子树，原叔父节点33形成一个子树。这两个分裂形成的树都是以后25的子树。左边的子树由原父节点作为中间节点，染成黑色，右边的子树由原叔父节点作为中间节点，染成黑色。

修复之后的结果如下图：

![image-20221206205259582](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205259582.png?lastModify=1670851010)

**修复步骤总结：**

1. parent、uncle 染成黑色
2. grand 向上合并

- 将向上合并的grand染成红色，相对上一层，就当做是新添加的节点，再次来一遍插入情况的判断，进行处理。

grand 向上合并时，可能继续发生上溢。这种情况就继续递归调用修复方法就可以了。若上溢持续到根节点，只需将根节点染成黑色即可（这个意思就是说断向上上溢，一直上溢到了B树的根节点位置了，只需要将向上合并的节点变成黑色作为红黑树的根节点即可。因为从B树根节点选择出来上溢的节点，肯定就是作为整个红黑树的根节点了）。

#### 6.2.5 上溢的RR插入情况

![image-20221206205334820](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205334820.png?lastModify=1670851010)

如上图，插入36的位置是上溢的RR情况。

**上溢RR情况**：父节点为祖父节点的右节点，插入节点为父节点的右节点。并且构成的新的B树节点已经超过了B树节点容量大小范围。

**判定条件**：uncle 是红色节点

**案例修复**：

上溢RR情况的修复，和上溢LL情况基本一致，只是修复的位置不同，这里中间的两个节点就是原父节点33和原祖父节点25，选择原祖父节点25来进行向上合并，原祖父节点向上合并后，将其染成红色。

原祖父节点25向上合并后，它原来左右两边的节点需要分裂成两个子树，也就是原父节点33和插入节点36形成一个子树，原叔父节点17形成一个子树。这两个分裂形成的树都是以后25的子树。左边的子树由原叔父节点作为中间节点，染成黑色，右边的子树由原父节点作为中间节点，染成黑色。

修复之后的结果如下图：

![image-20221206205351829](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205351829.png?lastModify=1670851010)

**修复步骤总结**：

1. parent、uncle 染成黑色
2. grand 向上合并

- 染成红色（其实染成红色就已经是完成了向上合并，因为祖父节点和祖父节点的父节点的连接指向并没有变），当做是新添加的节点进行处理

#### 6.2.6 上溢的LR插入情况

![image-20221206205432793](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205432793.png?lastModify=1670851010)

如上图，插入20的位置是上溢的LR情况。

**上溢LR情况**：父节点为祖父节点的左节点，插入节点为父节点的右节点。并且构成的新的B树节点已经超过了B树节点容量大小范围。

**判定条件**：uncle 是红色节点

**案例修复**：

上溢LR情况的修复，和其他上溢情况基本一致，只是修复的位置不同，这里中间的两个节点就是原父节点17和原祖父节点25，选择原祖父节点25来进行向上合并，原祖父节点向上合并后，将其染成红色。

原祖父节点25向上合并后，它原来左右两边的节点需要分裂成两个子树，也就是原父节点17和插入节点20形成一个子树，原叔父节点33形成一个子树。这两个分裂形成的树都是以后25的子树。左边的子树由原父节点作为中间节点，染成黑色，右边的子树由原叔父节点作为中间节点，染成黑色。

修复之后的结果如下图：

![image-20221206205448935](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205448935.png?lastModify=1670851010)

**修复步骤总结**：

1. parent、uncle 染成黑色
2. grand 向上合并

- 染成红色，当做是新添加的节点进行处理

#### 6.2.7 上溢的RL插入情况

![image-20221206205510158](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205510158.png?lastModify=1670851010)

如上图，插入30的位置是上溢的RL情况。

**上溢RL情况**：父节点为祖父节点的右节点，插入节点为父节点的左节点。并且构成的新的B树节点已经超过了B树节点容量大小范围。

**判定条件**：uncle 是红色节点

**案例修复**：

上溢RL情况的修复，和其他上溢情况基本一致，只是修复的位置不同，这里中间的两个节点就是原父节点33和原祖父节点25，选择原祖父节点25来进行向上合并，原祖父节点向上合并后，将其染成红色。

原祖父节点25向上合并后，它原来左右两边的节点需要分裂成两个子树，也就是原父节点33和插入节点30形成一个子树，原叔父节点17形成一个子树。这两个分裂形成的树都是以后25的子树。左边的子树由原叔父节点作为中间节点，染成黑色，右边的子树由原父节点作为中间节点，染成黑色。

修复之后的结果如下图：

![image-20221206205526041](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205526041.png?lastModify=1670851010)

**修复步骤总结**：

1. parent、uncle 染成黑色
2. grand 向上合并

- 染成黑色，当做是新添加的节点进行处理

#### 6.2.8 插入情况总结

插入一共有12种情况：

1.插入节点的父节点是黑色的情况有4种 这种情况仍然会维持红黑树的性质，则不需要进行额外处理。 2.插入节点的父节点是红色的情况有8种 这种情况不满足红黑树的性质4，需要进行额外的修复处理。

这8种情况中： 1.叔父节点不是红色的情况有4种 这些情况都是非上溢，需要通过重新染色和旋转来进行修复 2.叔父节点是红色的情况有4种 这些情况都是上溢的，只需要通过祖父节点上溢合并和染色即可完成修复

### 6.3 删除操作

相较于插入操作，红黑树的删除操作则要更为复杂一些。B树中，最后真正被删除的元素都在叶子节点中。所以在红黑树中，被删除的节点一定也在最后一层。

![image-20221206205647333](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205647333.png?lastModify=1670851010)

#### 6.3.1 删除操作的所有情况

上面我们说删除节点一定都在最后一层，最后一层有红色节点和黑色节点，我们就以删除节点的颜色来区分删除操作的所有情况。

##### 6.3.1.1 删除红色节点

如果删除的节点是红色直接删除，不用作任何调整。因为删除最后一层的红色节点，并没有影响红黑树的任何性质。

![image-20221206205703888](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205703888.png?lastModify=1670851010)

##### 6.3.1.2 删除黑色节点

有3种情况：

1.拥有 2 个红色子节点的黑色节点

- 不可能被直接删除，因为会找它的子节点替代删除，因此不用考虑这种情况

2.拥有 1 个红色子节点的黑色节点 3.黑色叶子节点

![image-20221206205805669](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205805669.png?lastModify=1670851010)

#### 6.3.2 删除拥有1个红色子节点的黑色节点

![image-20221206205815162](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205815162.png?lastModify=1670851010)

删除拥有1个红色子节点的黑色节点的情况，是需要我们做相关的处理的。这里删除的就是节点46和76，他们只有一个红色子节点。

对于一个二叉树来说，删除一个度为1的节点（度指的是一个节点的子节点个数），将其删除后需要用它唯一的子节点来进行替换。而红黑树的这种情况的判定条件，就是判定要替代删除节点的子节点是不是红色

**判定条件**：用以替代的子节点是红色节点

**案例修复**：

![image-20221206205831553](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205831553.png?lastModify=1670851010)

删除黑色节点46和76

第一步：

![image-20221206205838823](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205838823.png?lastModify=1670851010)

将46与父节点的连接断开

第二步：

![image-20221206205846108](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205846108.png?lastModify=1670851010)

46唯一的红色子节点50作为代替46的节点，将其与46的父节点进行连接

第三步：

![image-20221206205852846](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205852846.png?lastModify=1670851010)

断开46与50的连接，将46删除

删除节点76的过程与删除节点46相同

第一步：

![image-20221206205900124](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205900124.png?lastModify=1670851010)

第二步：

![image-20221206205907662](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206205907662.png?lastModify=1670851010)

第三步：

![image-20221206210339160](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206210339160.png?lastModify=1670851010)

但是现在我们发现，80是红色节点，它的子节点72还是红色节点，这样明显不符合红黑树的性质，还需要进一步修复。



将替代的子节点染成黑色即可保持红黑树性质，修复完成

**修复步骤总结**：

1. 用删除节点的唯一子节点对其进行替代
2. 将替代节点染成黑色

#### 6.3.3 删除黑色叶子节点——删除节点为根节点

一棵红黑树只有一个黑色根节点（也就是唯一的一个叶子节点，整个红黑树只有这一个黑色节点），可直接删除该节点，无需做其他操作。

#### 6.3.4 删除黑色叶子节点——删除节点的兄弟节点为黑色

讲这种删除情况前先举一个例子

![image-20221206210409634](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206210409634.png?lastModify=1670851010)

上面这个我们要删除节点88，该节点为黑色叶子节点，它的兄弟节点是黑色76。从B树的角度来看，如果删除88，因为四阶B树的节点中最少存有1个元素，如果不足，则会造成下溢。也就是需要从88的兄弟节点中借一个子节点出来。这就是这一节我们讨论的删除情况的核心修复思想。

##### 6.3.4.1 兄弟节点至少有1个红色子节点

下面三个图分别对应着兄弟节点至少有一个红色子节点的三种情况。删除节点为88，为黑色叶子节点，它的兄弟节点是76，为黑色。兄弟节点76都至少有一个红色子节点，三种情况分别为76拥有一个红色右子节点，76拥有一个红色左子节点，76拥有两个红色子节点。因为兄弟节点有红色子节点，所以可以借出一个节点来进行修复。

![image-20221206210837493](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206210837493.png?lastModify=1670851010)

这三种情况，黑色叶子节点被删除后，会导致B树节点下溢（比如删除88），就可以从兄弟节点中借出一个红色子节点来进行修复。

**判定条件**：兄弟节点至少有 1 个红色子节点

**案例修复**：

1、兄弟节点有一个右子节点：

![image-20221206210854123](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206210854123.png?lastModify=1670851010)

先将88节点删除

![image-20221206210906579](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206210906579.png?lastModify=1670851010)

删掉之后，从B树的角度来看就出现了下溢，这个时候就需要父节点下来，在兄弟节点的子结点中找一个，将他升上去代替。具体的实现就是要对节点进行旋转。

![image-20221206210914933](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206210914933.png?lastModify=1670851010)

我们可以看出，80、76、78组成的树是一个LR的情况，先对76进行左旋转（可以将76看作父节点），这样78就上去了，再对80进行右旋转（可以将80看成祖父节点），80就下去了。

![image-20221206210927368](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206210927368.png?lastModify=1670851010)

旋转完了之后，如上图。将旋转完之后的中心节点（就是78、76、80组成的树的最中心的节点，这里就是78）进行重新染色，继承删除节点的父节点80的颜色。最后再将78、76、80组成的树的左右两个节点染成黑色即可完成修复。

2、兄弟节点有一个左子节点：

![image-20221206210940906](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206210940906.png?lastModify=1670851010)

先将88节点删除

![image-20221206211007665](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211007665.png?lastModify=1670851010)

删掉之后，从B树的角度来看就出现了下溢，这个时候就需要父节点下来，在兄弟节点的子结点中找一个，将他升上去代替。具体的实现就是要对节点进行旋转。

![image-20221206211016938](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211016938.png?lastModify=1670851010)

我们可以看出，80、76、72组成的树是一个LL的情况，直接对80进行右旋（将80看成是祖父节点）。

![image-20221206211024793](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211024793.png?lastModify=1670851010)

旋转完了之后，如上图。将旋转完之后的中心节点（就是76、72、80组成的树的最中心的节点，这里就是76）进行重新染色，继承删除节点的父节点80的颜色。最后再将76、72、80组成的树的左右两个节点染成黑色即可完成修复。

3、兄弟节点有两个左右子节点：

![image-20221206211053071](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211053071.png?lastModify=1670851010)

先将88节点删除

![image-20221206211059812](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211059812.png?lastModify=1670851010)

删除之后，其实可以有两种旋转可以进行修复，既可以使用LL方式进行旋转，也可以使用LR方式进行旋转。但是因为LL方式只需要旋转一次，我们就选用LL方式。

![image-20221206211115877](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211115877.png?lastModify=1670851010)

直接对80进行右旋

![image-20221206211108589](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211108589.png?lastModify=1670851010)

旋转完了之后，如上图。将旋转完之后的中心节点（就是78、72、76、80组成的树的最中心的节点，这里就是76）进行重新染色，继承删除节点的父节点80的颜色。最后再将78、72、76、80组成的树的左右两个节点染成黑色即可完成修复。

**修复步骤总结**：

1. 进行旋转操作
2. 旋转之后的中心节点继承父节点（删除节点的父节点）的颜色
3. 旋转之后的左右节点染为黑色

##### 6.3.4.2 兄弟节点没有红色子节点

当删除节点的兄弟节点没有红色节点可以借出的情况下，就需要父节点来向下合并进行修复，父节点向下和兄弟节点合并成新的B树节点来解决下溢。

判定条件：兄弟节点没有1个红色子节点

**案例修复**：

1、父节点为红色：

![image-20221206211156449](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211156449.png?lastModify=1670851010)

删除节点88，出现下溢

![image-20221206211207479](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211207479.png?lastModify=1670851010)

因为兄弟节点76没有可以借出的红色节点，所以需要父节点80来向下与76合并进行修复



将兄弟节点76染成红色，父节点80染成黑色即可完成修复

2、父节点为黑色：

![image-20221206211215639](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211215639.png?lastModify=1670851010)

删除节点88，删除之后节点88就会出现下溢

![image-20221206211224207](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211224207.png?lastModify=1670851010)

删除之后父节点80应该向下合并进行修复，但是因为父节点80为黑色，如果向下合并之后，其实就相当于80这个节点也出现了下溢。

![image-20221206211235311](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211235311.png?lastModify=1670851010)

这个时候只需要把父节点当作被删除的节点进行处理即可

**修复步骤总结**：

1. 父节点向下与兄弟节点进行合并
2. 将兄弟染成红色、父节点染成黑色即可修复红黑树性质

- 如果父节点是黑色，直接将父节点当成被删除的节点处理，来修复父节点的下溢情况

#### 6.3.5 删除黑色叶子节点——删除节点的兄弟节点为红色

如果删除节点的兄弟节点为红色，这样删除节点出现下溢后没办法通过兄弟节点来进行修复。这就需要先把红黑树转换为兄弟节点为黑色的情况，就可以套用上面讲的修复方法来进行修复了。

![image-20221206211316257](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211316257.png?lastModify=1670851010)

**判定条件**：兄弟节点是红色

**案例修复**：

![image-20221206211327772](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211327772.png?lastModify=1670851010)

删除88节点之前，需要先转换成兄弟节点为黑色的情况，当前88的兄弟节点是红色55。可以将其看作LL情况，对父节点88进行右旋转，这样55就被移动上去了，成了80的父节点。76也被移动上去了，成了80的子节点。

![image-20221206211355127](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211355127.png?lastModify=1670851010)

这种情况，删除节点88的兄弟节点就变成了黑色，并且没有红色子节点，可以继续套用之前讲的方法来进行修复了。

![image-20221206211403820](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211403820.png?lastModify=1670851010)

删除掉88，将80染成黑色，76染成红色，完成修复。

**修复步骤总结**：

1. 兄弟节点染成 BLACK，父节点染成染成 RED，对父节点进行右旋
2. 于是又回到兄弟节点是黑色的情况（侄子节点变为兄弟节点），继续使用兄弟节点为黑色的方法进行修复

## 7.红黑树的平衡

AVL是靠平衡因子来保持平衡的，比如平衡因子为1，那么左右子树的高度差就不能超过1，是一种强平衡。

对于红黑树而言，为何那5条性质，就能保证红黑树是平衡的？

- 因为那5条性质，可以保证红黑树等价于4阶B树

![image-20221206211437217](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211437217.png?lastModify=1670851010)

B树比较矮，它本身就是平衡的，高度越小越平衡。

红黑树就是能保证这个树高度不会特别高，红黑树的最大高度是 2 ∗ log2(n + 1) ，依然是 O(logn) 级别，因为高度不会很大进而维持一种相对平衡的状态。相比AVL树，红黑树的平衡标准比较宽松：没有一条路径会大于其他路径的2倍。这是是一种弱平衡、黑高度平衡（黑高度只算黑色节点个数，红黑树的任何一条路径的黑色节点数一样，则黑高度都是一样）。

## 8.红黑树的平均时间复杂度

- 搜索：O(logn)
- 添加：O(logn)，O(1) 次的旋转操作
- 删除：O(logn)，O(1) 次的旋转操作

## 9.AVL树 vs 红黑树

### 9.1 AVL树

- 平衡标准比较严格：每个左右子树的高度差不超过1
- 最大高度是 1.44 ∗ log2 n + 2 − 1.328（100W个节点，AVL树最大树高28）
- 搜索、添加、删除都是 O(logn) 复杂度，其中添加仅需 O(1) 次旋转调整、删除最多需要 O(logn) 次旋转调整

### 9.2 红黑树

- 平衡标准比较宽松：没有一条路径会大于其他路径的2倍
- 最大高度是 2 ∗ log2(n + 1)（ 100W个节点，红黑树最大树高40）
- 搜索、添加、删除都是 O(logn) 复杂度，其中添加、删除都仅需 O(1) 次旋转调整

### 9.3 如何选择

- 搜索的次数远远大于插入和删除，选择AVL树；搜索、插入、删除次数几乎差不多，选择红黑树
- 相对于AVL树来说，红黑树牺牲了部分平衡性以换取插入/删除操作时少量的旋转操作，整体来说性能要优于AVL树
- 红黑树的平均统计性能优于AVL树，实际应用中更多选择使用红黑树

### 9.4 案例对比

10, 35, 47, 11, 5, 57, 39, 14, 27, 26, 84, 75, 63, 41, 37, 24, 96组成一棵树

#### 9.4.1 二叉搜索树

![image-20221206211548412](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211548412.png?lastModify=1670851010)

非常不平衡

#### 9.4.2 AVL树

![image-20221206211557995](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211557995.png?lastModify=1670851010)

最平衡

#### 9.4.3 红黑树

![image-20221206211607495](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206211607495.png?lastModify=1670851010)

相对比较平衡 ———————————————— 版权声明：本文为CSDN博主「小七mod」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/cy973071263/article/details/1225438260

# 【NO.2】红黑树(一)之 原理和算法详细介绍

## **1.R-B Tree简介**

  R-B Tree，全称是Red-Black Tree，又称为“红黑树”，它一种特殊的二叉查找树。红黑树的每个节点上都有存储位表示节点的颜色，可以是红(Red)或黑(Black)。

**红黑树的特性**: **（1）每个节点或者是黑色，或者是红色。** **（2）根节点是黑色。** **（3）每个叶子节点（NIL）是黑色。 [注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！]** **（4）如果一个节点是红色的，则它的子节点必须是黑色的。** **（5）从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。**

**注意**： (01) 特性(3)中的叶子节点，是只为空(NIL或null)的节点。 (02) 特性(5)，确保没有一条路径会比其他路径长出俩倍。因而，红黑树是相对是接近平衡的二叉树。

红黑树示意图如下：

[![img](https://images0.cnblogs.com/i/497634/201403/251730074203156.jpg)](https://images0.cnblogs.com/i/497634/201403/251730074203156.jpg)

 

## 2.**红黑树的应用**

红黑树的应用比较广泛，主要是用它来存储有序的数据，它的时间复杂度是O(lgn)，效率非常之高。 例如，Java集合中的[TreeSet](http://www.cnblogs.com/skywang12345/p/3311268.html)和[TreeMap](http://www.cnblogs.com/skywang12345/p/3310928.html)，C++ STL中的set、map，以及Linux虚拟内存的管理，都是通过红黑树去实现的。

 

## 3.**红黑树的时间复杂度和相关证明** 

**红黑树的时间复杂度为: O(lgn)** 下面通过“*数学归纳法*”对红黑树的时间复杂度进行证明。

定理：**一棵含有n个节点的红黑树的高度至多为2log(n+1)**.

证明：  "一棵含有n个节点的红黑树的高度至多为2log(n+1)" 的**逆否命题**是 "高度为h的红黑树，它的包含的内节点个数至少为 2h/2-1个"。  我们只需要证明逆否命题，即可证明原命题为真；即只需证明 "高度为h的红黑树，它的包含的内节点个数至少为 2h/2-1个"。

  从某个节点x出发（不包括该节点）到达一个叶节点的任意一条路径上，黑色节点的个数称为该节点的黑高度(x's black height)，记为**bh(x)**。关于bh(x)有两点需要说明：   第1点：根据红黑树的"**特性(5)** ，即*从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点*"可知，从节点x出发到达的所有的叶节点具有相同数目的黑节点。**这也就意味着，bh(x)的值是唯一的**！  第2点：根据红黑色的"特性(4)，即*如果一个节点是红色的，则它的子节点必须是黑色的*"可知，从节点x出发达到叶节点"所经历的黑节点数目">= "所经历的红节点的数目"。假设x是根节点，则可以得出结论"**bh(x) >= h/2**"。进而，我们只需证明 "高度为h的红黑树，它的包含的黑节点个数至少为 2bh(x)-1个"即可。

  到这里，我们将需要证明的定理已经由 **"一棵含有n个节点的红黑树的高度至多为2log(n+1)"**  转变成只需要证明 **"高度为h的红黑树，它的包含的内节点个数至少为 2bh(x)-1个"。**

下面通过"数学归纳法"开始论证高度为h的红黑树，它的包含的内节点个数至少为 2bh(x)-1个"。

(01) 当树的高度h=0时，  内节点个数是0，bh(x) 为0，2bh(x)-1 也为 0。显然，原命题成立。

(02) 当h>0，且树的高度为 h-1 时，它包含的节点个数至少为 2bh(x)-1-1。这个是根据(01)推断出来的！

  下面，由树的高度为 h-1 的已知条件推出“树的高度为 h 时，它所包含的节点树为 2bh(x)-1”。

  当树的高度为 h 时，  对于节点x(x为根节点)，其黑高度为bh(x)。  对于节点x的左右子树，它们黑高度为 bh(x) 或者 bh(x)-1。  根据(02)的已知条件，我们已知 "x的左右子树，即高度为 h-1 的节点，它包含的节点至少为 2bh(x)-1-1 个"；

  所以，节点x所包含的节点至少为 ( 2bh(x)-1-1 ) + ( 2bh(x)-1-1 ) + 1 = 2^bh(x)-1。即节点x所包含的节点至少为 2bh(x)-1。  因此，原命题成立。

  由(01)、(02)得出，"高度为h的红黑树，它的包含的内节点个数至少为 2^bh(x)-1个"。  因此，“一棵含有n个节点的红黑树的高度至多为2log(n+1)”。

 

## **4.红黑树的基本操作(一) 左旋和右旋**

红黑树的基本操作是**添加**、**删除**。在对红黑树进行添加或删除之后，都会用到旋转方法。为什么呢？道理很简单，添加或删除红黑树中的节点之后，红黑树就发生了变化，可能不满足红黑树的5条性质，也就不再是一颗红黑树了，而是一颗普通的树。而通过旋转，可以使这颗树重新成为红黑树。简单点说，旋转的目的是让树保持红黑树的特性。 旋转包括两种：**左旋** 和 **右旋**。下面分别对它们进行介绍。

 

**1. 左旋**

[![img](https://images0.cnblogs.com/i/497634/201403/251733282013849.jpg)](https://images0.cnblogs.com/i/497634/201403/251733282013849.jpg)

对x进行左旋，意味着"将x变成一个左节点"。

左旋的伪代码《算法导论》：参考上面的示意图和下面的伪代码，理解“红黑树T的节点x进行左旋”是如何进行的。

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
LEFT-ROTATE(T, x)  
01  y ← right[x]            // 前提：这里假设x的右孩子为y。下面开始正式操作
02  right[x] ← left[y]      // 将 “y的左孩子” 设为 “x的右孩子”，即 将β设为x的右孩子
03  p[left[y]] ← x          // 将 “x” 设为 “y的左孩子的父亲”，即 将β的父亲设为x
04  p[y] ← p[x]             // 将 “x的父亲” 设为 “y的父亲”
05  if p[x] = nil[T]       
06  then root[T] ← y                 // 情况1：如果 “x的父亲” 是空节点，则将y设为根节点
07  else if x = left[p[x]]  
08            then left[p[x]] ← y    // 情况2：如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子”
09            else right[p[x]] ← y   // 情况3：(x是它父节点的右孩子) 将y设为“x的父节点的右孩子”
10  left[y] ← x             // 将 “x” 设为 “y的左孩子”
11  p[x] ← y                // 将 “x的父节点” 设为 “y”
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

理解左旋之后，看看下面一个更鲜明的例子。你可以先不看右边的结果，自己尝试一下。

[![img](https://images0.cnblogs.com/i/497634/201403/251734577643655.jpg)](https://images0.cnblogs.com/i/497634/201403/251734577643655.jpg)

 

**2. 右旋**

[![img](https://images0.cnblogs.com/i/497634/201403/251735527958942.jpg)](https://images0.cnblogs.com/i/497634/201403/251735527958942.jpg)

对x进行左旋，意味着"将x变成一个左节点"。

右旋的伪代码《算法导论》：参考上面的示意图和下面的伪代码，理解“红黑树T的节点y进行右旋”是如何进行的。 

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
RIGHT-ROTATE(T, y)  
01  x ← left[y]             // 前提：这里假设y的左孩子为x。下面开始正式操作
02  left[y] ← right[x]      // 将 “x的右孩子” 设为 “y的左孩子”，即 将β设为y的左孩子
03  p[right[x]] ← y         // 将 “y” 设为 “x的右孩子的父亲”，即 将β的父亲设为y
04  p[x] ← p[y]             // 将 “y的父亲” 设为 “x的父亲”
05  if p[y] = nil[T]       
06  then root[T] ← x                 // 情况1：如果 “y的父亲” 是空节点，则将x设为根节点
07  else if y = right[p[y]]  
08            then right[p[y]] ← x   // 情况2：如果 y是它父节点的右孩子，则将x设为“y的父节点的左孩子”
09            else left[p[y]] ← x    // 情况3：(y是它父节点的左孩子) 将x设为“y的父节点的左孩子”
10  right[x] ← y            // 将 “y” 设为 “x的右孩子”
11  p[y] ← x                // 将 “y的父节点” 设为 “x”
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

理解右旋之后，看看下面一个更鲜明的例子。你可以先不看右边的结果，自己尝试一下。

[![img](https://images0.cnblogs.com/i/497634/201403/251737465769614.jpg)](https://images0.cnblogs.com/i/497634/201403/251737465769614.jpg)

**旋转总结**：

(01) 左旋 和 右旋 是相对的两个概念，原理类似。理解一个也就理解了另一个。

(02) 下面谈谈如何区分 左旋 和 右旋。 在实际应用中，若没有彻底理解 左旋 和 右旋，可能会将它们混淆。下面谈谈我对如何区分 左旋 和 右旋 的理解。

 

**3. 区分 左旋 和 右旋**

仔细观察上面"左旋"和"右旋"的示意图。我们能清晰的发现，它们是对称的。无论是左旋还是右旋，被旋转的树，在旋转前是二叉查找树，并且旋转之后仍然是一颗二叉查找树。

[![img](https://images0.cnblogs.com/i/497634/201403/251739385617803.jpg)](https://images0.cnblogs.com/i/497634/201403/251739385617803.jpg)

 

 

**左旋示例图**(以x为节点进行左旋)：

```
                               z
   x                          /                  
  / \      --(左旋)-->       x
 y   z                      /
                           y
```

对x进行左旋，意味着，将“x的右孩子”设为“x的父亲节点”；即，将 x变成了一个左节点(x成了为z的左孩子)！。 因此，**左旋中的“左”，意味着“被旋转的节点将变成一个左节点”**。

**右旋示例图**(以x为节点进行右旋)：

```
                               y
   x                            \                 
  / \      --(右旋)-->           x
 y   z                            \
                                   z
```

对x进行右旋，意味着，将“x的左孩子”设为“x的父亲节点”；即，将 x变成了一个右节点(x成了为y的右孩子)！ 因此，**右旋中的“右”，意味着“被旋转的节点将变成一个右节点”**。

 

## **5.红黑树的基本操作(二) 添加**

将一个节点插入到红黑树中，需要执行哪些步骤呢？首先，将红黑树当作一颗二叉查找树，将节点插入；然后，将节点着色为红色；最后，通过旋转和重新着色等方法来修正该树，使之重新成为一颗红黑树。详细描述如下：

**第一步: 将红黑树当作一颗二叉查找树，将节点插入。**    红黑树本身就是一颗二叉查找树，将节点插入后，该树仍然是一颗二叉查找树。也就意味着，树的键值仍然是有序的。此外，无论是左旋还是右旋，若旋转之前这棵树是二叉查找树，旋转之后它一定还是二叉查找树。这也就意味着，任何的旋转和重新着色操作，都不会改变它仍然是一颗二叉查找树的事实。    好吧？那接下来，我们就来想方设法的旋转以及重新着色，使这颗树重新成为红黑树！

**第二步：将插入的节点着色为"红色"。**    为什么着色成红色，而不是黑色呢？为什么呢？在回答之前，我们需要重新温习一下红黑树的特性： (1) 每个节点或者是黑色，或者是红色。 (2) 根节点是黑色。 (3) 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！] (4) 如果一个节点是红色的，则它的子节点必须是黑色的。 (5) 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。    将插入的节点着色为红色，不会违背"特性(5)"！少违背一条特性，就意味着我们需要处理的情况越少。接下来，就要努力的让这棵树满足其它性质即可；满足了的话，它就又是一颗红黑树了。o(∩∩)o...哈哈

**第三步: 通过一系列的旋转或着色等操作，使之重新成为一颗红黑树。**    第二步中，将插入节点着色为"红色"之后，不会违背"特性(5)"。那它到底会违背哪些特性呢？    对于"特性(1)"，显然不会违背了。因为我们已经将它涂成红色了。    对于"特性(2)"，显然也不会违背。在第一步中，我们是将红黑树当作二叉查找树，然后执行的插入操作。而根据二叉查找数的特点，插入操作不会改变根节点。所以，根节点仍然是黑色。    对于"特性(3)"，显然不会违背了。这里的叶子节点是指的空叶子节点，插入非空节点并不会对它们造成影响。    对于"特性(4)"，是有可能违背的！    那接下来，想办法使之"满足特性(4)"，就可以将树重新构造成红黑树了。

下面看看代码到底是怎样实现这三步的。

 

添加操作的伪代码《算法导论》

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
RB-INSERT(T, z)  
01  y ← nil[T]                        // 新建节点“y”，将y设为空节点。
02  x ← root[T]                       // 设“红黑树T”的根节点为“x”
03  while x ≠ nil[T]                  // 找出要插入的节点“z”在二叉树T中的位置“y”
04      do y ← x                      
05         if key[z] < key[x]  
06            then x ← left[x]  
07            else x ← right[x]  
08  p[z] ← y                          // 设置 “z的父亲” 为 “y”
09  if y = nil[T]                     
10     then root[T] ← z               // 情况1：若y是空节点，则将z设为根
11     else if key[z] < key[y]        
12             then left[y] ← z       // 情况2：若“z所包含的值” < “y所包含的值”，则将z设为“y的左孩子”
13             else right[y] ← z      // 情况3：(“z所包含的值” >= “y所包含的值”)将z设为“y的右孩子” 
14  left[z] ← nil[T]                  // z的左孩子设为空
15  right[z] ← nil[T]                 // z的右孩子设为空。至此，已经完成将“节点z插入到二叉树”中了。
16  color[z] ← RED                    // 将z着色为“红色”
17  RB-INSERT-FIXUP(T, z)             // 通过RB-INSERT-FIXUP对红黑树的节点进行颜色修改以及旋转，让树T仍然是一颗红黑树
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

结合伪代码以及为代码上面的说明，先理解RB-INSERT。理解了RB-INSERT之后，我们接着对 RB-INSERT-FIXUP的伪代码进行说明。

添加修正操作的伪代码《算法导论》

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
RB-INSERT-FIXUP(T, z)
01 while color[p[z]] = RED                                                  // 若“当前节点(z)的父节点是红色”，则进行以下处理。
02     do if p[z] = left[p[p[z]]]                                           // 若“z的父节点”是“z的祖父节点的左孩子”，则进行以下处理。
03           then y ← right[p[p[z]]]                                        // 将y设置为“z的叔叔节点(z的祖父节点的右孩子)”
04                if color[y] = RED                                         // Case 1条件：叔叔是红色
05                   then color[p[z]] ← BLACK                    ▹ Case 1   //  (01) 将“父节点”设为黑色。
06                        color[y] ← BLACK                       ▹ Case 1   //  (02) 将“叔叔节点”设为黑色。
07                        color[p[p[z]]] ← RED                   ▹ Case 1   //  (03) 将“祖父节点”设为“红色”。
08                        z ← p[p[z]]                            ▹ Case 1   //  (04) 将“祖父节点”设为“当前节点”(红色节点)
09                   else if z = right[p[z]]                                // Case 2条件：叔叔是黑色，且当前节点是右孩子
10                           then z ← p[z]                       ▹ Case 2   //  (01) 将“父节点”作为“新的当前节点”。
11                                LEFT-ROTATE(T, z)              ▹ Case 2   //  (02) 以“新的当前节点”为支点进行左旋。
12                           color[p[z]] ← BLACK                 ▹ Case 3   // Case 3条件：叔叔是黑色，且当前节点是左孩子。(01) 将“父节点”设为“黑色”。
13                           color[p[p[z]]] ← RED                ▹ Case 3   //  (02) 将“祖父节点”设为“红色”。
14                           RIGHT-ROTATE(T, p[p[z]])            ▹ Case 3   //  (03) 以“祖父节点”为支点进行右旋。
15        else (same as then clause with "right" and "left" exchanged)      // 若“z的父节点”是“z的祖父节点的右孩子”，将上面的操作中“right”和“left”交换位置，然后依次执行。
16 color[root[T]] ← BLACK 
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

根据被插入节点的父节点的情况，可以将"当节点z被着色为红色节点，并插入二叉树"划分为三种情况来处理。 ① 情况说明：被插入的节点是根节点。  处理方法：直接把此节点涂为黑色。 ② 情况说明：被插入的节点的父节点是黑色。  处理方法：什么也不需要做。节点被插入后，仍然是红黑树。 ③ 情况说明：被插入的节点的父节点是红色。  处理方法：那么，该情况与红黑树的“特性(5)”相冲突。这种情况下，被插入节点是一定存在非空祖父节点的；进一步的讲，被插入节点也一定存在叔叔节点(即使叔叔节点为空，我们也视之为存在，空节点本身就是黑色节点)。理解这点之后，我们依据"叔叔节点的情况"，将这种情况进一步划分为3种情况(Case)。

|        | **现象说明**                                                 | **处理策略**                                                 |
| ------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Case 1 | 当前节点的父节点是红色，且当前节点的祖父节点的另一个子节点（叔叔节点）也是红色。 | (01) 将“父节点”设为黑色。 (02) 将“叔叔节点”设为黑色。 (03) 将“祖父节点”设为“红色”。 (04) 将“祖父节点”设为“当前节点”(红色节点)；即，之后继续对“当前节点”进行操作。 |
| Case 2 | 当前节点的父节点是红色，叔叔节点是黑色，且当前节点是其父节点的右孩子 | (01) 将“父节点”作为“新的当前节点”。 (02) 以“新的当前节点”为支点进行左旋。 |
| Case 3 | 当前节点的父节点是红色，叔叔节点是黑色，且当前节点是其父节点的左孩子 | (01) 将“父节点”设为“黑色”。 (02) 将“祖父节点”设为“红色”。 (03) 以“祖父节点”为支点进行右旋。 |

上面三种情况(Case)处理问题的核心思路都是：将红色的节点移到根节点；然后，将根节点设为黑色。下面对它们详细进行介绍。

 

**1. (Case 1)叔叔是红色**

**1.1 现象说明** 当前节点(即，被插入节点)的父节点是红色，且当前节点的祖父节点的另一个子节点（叔叔节点）也是红色。

**1.2 处理策略** (01) 将“父节点”设为黑色。 (02) 将“叔叔节点”设为黑色。 (03) 将“祖父节点”设为“红色”。 (04) 将“祖父节点”设为“当前节点”(红色节点)；即，之后继续对“当前节点”进行操作。

  **下面谈谈为什么要这样处理。**(建议理解的时候，通过下面的图进行对比)  “当前节点”和“父节点”都是红色，违背“特性(4)”。所以，将“父节点”设置“黑色”以解决这个问题。  但是，将“父节点”由“红色”变成“黑色”之后，违背了“特性(5)”：因为，包含“父节点”的分支的黑色节点的总数增加了1。  解决这个问题的办法是：将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”。关于这里，说明几点：第一，为什么“祖父节点”之前是黑色？这个应该很容易想明白，因为在变换操作之前，该树是红黑树，“父节点”是红色，那么“祖父节点”一定是黑色。 第二，为什么将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”；能解决“包含‘父节点’的分支的黑色节点的总数增加了1”的问题。这个道理也很简单。“包含‘父节点’的分支的黑色节点的总数增加了1” 同时也意味着 “包含‘祖父节点’的分支的黑色节点的总数增加了1”，既然这样，我们通过将“祖父节点”由“黑色”变成“红色”以解决“包含‘祖父节点’的分支的黑色节点的总数增加了1”的问题； 但是，这样处理之后又会引起另一个问题“包含‘叔叔’节点的分支的黑色节点的总数减少了1”，现在我们已知“叔叔节点”是“红色”，将“叔叔节点”设为“黑色”就能解决这个问题。 所以，将“祖父节点”由“黑色”变成红色，同时，将“叔叔节点”由“红色”变成“黑色”；就解决了该问题。  按照上面的步骤处理之后：当前节点、父节点、叔叔节点之间都不会违背红黑树特性，但祖父节点却不一定。若此时，祖父节点是根节点，直接将祖父节点设为“黑色”，那就完全解决这个问题了；若祖父节点不是根节点，那我们需要将“祖父节点”设为“新的当前节点”，接着对“新的当前节点”进行分析。

**1.3 示意图**

[**![img](https://images0.cnblogs.com/i/497634/201403/251759273578917.jpg)**](https://images0.cnblogs.com/i/497634/201403/251759273578917.jpg)

 

**2. (Case 2)叔叔是黑色，且当前节点是右孩子**

**2.1 现象说明** 当前节点(即，被插入节点)的父节点是红色，叔叔节点是黑色，且当前节点是其父节点的右孩子

**2.2 处理策略** (01) 将“父节点”作为“新的当前节点”。 (02) 以“新的当前节点”为支点进行左旋。

   **下面谈谈为什么要这样处理。**(建议理解的时候，通过下面的图进行对比)   首先，将“父节点”作为“新的当前节点”；接着，以“新的当前节点”为支点进行左旋。 为了便于理解，我们先说明第(02)步，再说明第(01)步；为了便于说明，我们设置“父节点”的代号为F(Father)，“当前节点”的代号为S(Son)。 为什么要“以F为支点进行左旋”呢？根据已知条件可知：S是F的右孩子。而之前我们说过，我们处理红黑树的核心思想：将红色的节点移到根节点；然后，将根节点设为黑色。既然是“将红色的节点移到根节点”，那就是说要不断的将破坏红黑树特性的红色节点上移(即向根方向移动)。 而S又是一个右孩子，因此，我们可以通过“左旋”来将S上移！   按照上面的步骤(以F为支点进行左旋)处理之后：若S变成了根节点，那么直接将其设为“黑色”，就完全解决问题了；若S不是根节点，那我们需要执行步骤(01)，即“将F设为‘新的当前节点’”。那为什么不继续以S为新的当前节点继续处理，而需要以F为新的当前节点来进行处理呢？这是因为“左旋”之后，F变成了S的“子节点”，即S变成了F的父节点；而我们处理问题的时候，需要从下至上(由叶到根)方向进行处理；也就是说，必须先解决“孩子”的问题，再解决“父亲”的问题；所以，我们执行步骤(01)：将“父节点”作为“新的当前节点”。

**2.2 示意图**

[**![img](https://images0.cnblogs.com/i/497634/201403/251801031546918.jpg)**](https://images0.cnblogs.com/i/497634/201403/251801031546918.jpg)

 

**3. (Case 3)叔叔是黑色，且当前节点是左孩子**

**3.1 现象说明** 当前节点(即，被插入节点)的父节点是红色，叔叔节点是黑色，且当前节点是其父节点的左孩子

**3.2 处理策略** (01) 将“父节点”设为“黑色”。 (02) 将“祖父节点”设为“红色”。 (03) 以“祖父节点”为支点进行右旋。

   **下面谈谈为什么要这样处理。**(建议理解的时候，通过下面的图进行对比)   为了便于说明，我们设置“当前节点”为S(Original Son)，“兄弟节点”为B(Brother)，“叔叔节点”为U(Uncle)，“父节点”为F(Father)，祖父节点为G(Grand-Father)。   S和F都是红色，违背了红黑树的“特性(4)”，我们可以将F由“红色”变为“黑色”，就解决了“违背‘特性(4)’”的问题；但却引起了其它问题：违背特性(5)，因为将F由红色改为黑色之后，所有经过F的分支的黑色节点的个数增加了1。那我们如何解决“所有经过F的分支的黑色节点的个数增加了1”的问题呢？ 我们可以通过“将G由黑色变成红色”，同时“以G为支点进行右旋”来解决。

**2.3 示意图**

[![img](https://images0.cnblogs.com/i/497634/201404/170945094945387.jpg)](https://images0.cnblogs.com/i/497634/201404/170945094945387.jpg)

提示：上面的进行Case 3处理之后，再将节点"120"当作当前节点，就变成了Case 2的情况。

 

## **6.红黑树的基本操作(三) 删除**

将红黑树内的某一个节点删除。需要执行的操作依次是：首先，将红黑树当作一颗二叉查找树，将该节点从二叉查找树中删除；然后，通过"旋转和重新着色"等一系列来修正该树，使之重新成为一棵红黑树。详细描述如下：

**第一步：将红黑树当作一颗二叉查找树，将节点删除。**    这和"删除常规二叉查找树中删除节点的方法是一样的"。分3种情况：    ① 被删除节点没有儿子，即为叶节点。那么，直接将该节点删除就OK了。    ② 被删除节点只有一个儿子。那么，直接删除该节点，并用该节点的唯一子节点顶替它的位置。    ③ 被删除节点有两个儿子。那么，先找出它的后继节点；然后把“它的后继节点的内容”复制给“该节点的内容”；之后，删除“它的后继节点”。在这里，后继节点相当于替身，在将后继节点的内容复制给"被删除节点"之后，再将后继节点删除。这样就巧妙的将问题转换为"删除后继节点"的情况了，下面就考虑后继节点。 在"被删除节点"有两个非空子节点的情况下，它的后继节点不可能是双子非空。既然"的后继节点"不可能双子都非空，就意味着"该节点的后继节点"要么没有儿子，要么只有一个儿子。若没有儿子，则按"情况① "进行处理；若只有一个儿子，则按"情况② "进行处理。

**第二步：通过"旋转和重新着色"等一系列来修正该树，使之重新成为一棵红黑树。**    因为"第一步"中删除节点之后，可能会违背红黑树的特性。所以需要通过"旋转和重新着色"来修正该树，使之重新成为一棵红黑树。

 

删除操作的伪代码《算法导论》

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
RB-DELETE(T, z)
01 if left[z] = nil[T] or right[z] = nil[T]         
02    then y ← z                                  // 若“z的左孩子” 或 “z的右孩子”为空，则将“z”赋值给 “y”；
03    else y ← TREE-SUCCESSOR(z)                  // 否则，将“z的后继节点”赋值给 “y”。
04 if left[y] ≠ nil[T]
05    then x ← left[y]                            // 若“y的左孩子” 不为空，则将“y的左孩子” 赋值给 “x”；
06    else x ← right[y]                           // 否则，“y的右孩子” 赋值给 “x”。
07 p[x] ← p[y]                                    // 将“y的父节点” 设置为 “x的父节点”
08 if p[y] = nil[T]                               
09    then root[T] ← x                            // 情况1：若“y的父节点” 为空，则设置“x” 为 “根节点”。
10    else if y = left[p[y]]                    
11            then left[p[y]] ← x                 // 情况2：若“y是它父节点的左孩子”，则设置“x” 为 “y的父节点的左孩子”
12            else right[p[y]] ← x                // 情况3：若“y是它父节点的右孩子”，则设置“x” 为 “y的父节点的右孩子”
13 if y ≠ z                                    
14    then key[z] ← key[y]                        // 若“y的值” 赋值给 “z”。注意：这里只拷贝z的值给y，而没有拷贝z的颜色！！！
15         copy y's satellite data into z         
16 if color[y] = BLACK                            
17    then RB-DELETE-FIXUP(T, x)                  // 若“y为黑节点”，则调用
18 return y 
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

结合伪代码以及为代码上面的说明，先理解RB-DELETE。理解了RB-DELETE之后，接着对 RB-DELETE-FIXUP的伪代码进行说明

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
RB-DELETE-FIXUP(T, x)
01 while x ≠ root[T] and color[x] = BLACK  
02     do if x = left[p[x]]      
03           then w ← right[p[x]]                                             // 若 “x”是“它父节点的左孩子”，则设置 “w”为“x的叔叔”(即x为它父节点的右孩子)                                          
04                if color[w] = RED                                           // Case 1: x是“黑+黑”节点，x的兄弟节点是红色。(此时x的父节点和x的兄弟节点的子节点都是黑节点)。
05                   then color[w] ← BLACK                        ▹  Case 1   //   (01) 将x的兄弟节点设为“黑色”。
06                        color[p[x]] ← RED                       ▹  Case 1   //   (02) 将x的父节点设为“红色”。
07                        LEFT-ROTATE(T, p[x])                    ▹  Case 1   //   (03) 对x的父节点进行左旋。
08                        w ← right[p[x]]                         ▹  Case 1   //   (04) 左旋后，重新设置x的兄弟节点。
09                if color[left[w]] = BLACK and color[right[w]] = BLACK       // Case 2: x是“黑+黑”节点，x的兄弟节点是黑色，x的兄弟节点的两个孩子都是黑色。
10                   then color[w] ← RED                          ▹  Case 2   //   (01) 将x的兄弟节点设为“红色”。
11                        x ←  p[x]                               ▹  Case 2   //   (02) 设置“x的父节点”为“新的x节点”。
12                   else if color[right[w]] = BLACK                          // Case 3: x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的左孩子是红色，右孩子是黑色的。
13                           then color[left[w]] ← BLACK          ▹  Case 3   //   (01) 将x兄弟节点的左孩子设为“黑色”。
14                                color[w] ← RED                  ▹  Case 3   //   (02) 将x兄弟节点设为“红色”。
15                                RIGHT-ROTATE(T, w)              ▹  Case 3   //   (03) 对x的兄弟节点进行右旋。
16                                w ← right[p[x]]                 ▹  Case 3   //   (04) 右旋后，重新设置x的兄弟节点。
17                         color[w] ← color[p[x]]                 ▹  Case 4   // Case 4: x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的右孩子是红色的。(01) 将x父节点颜色 赋值给 x的兄弟节点。
18                         color[p[x]] ← BLACK                    ▹  Case 4   //   (02) 将x父节点设为“黑色”。
19                         color[right[w]] ← BLACK                ▹  Case 4   //   (03) 将x兄弟节点的右子节设为“黑色”。
20                         LEFT-ROTATE(T, p[x])                   ▹  Case 4   //   (04) 对x的父节点进行左旋。
21                         x ← root[T]                            ▹  Case 4   //   (05) 设置“x”为“根节点”。
22        else (same as then clause with "right" and "left" exchanged)        // 若 “x”是“它父节点的右孩子”，将上面的操作中“right”和“left”交换位置，然后依次执行。
23 color[x] ← BLACK   
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

下面对删除函数进行分析。在分析之前，我们再次温习一下红黑树的几个特性： (1) 每个节点或者是黑色，或者是红色。 (2) 根节点是黑色。 (3) 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！] (4) 如果一个节点是红色的，则它的子节点必须是黑色的。 (5) 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。

   前面我们将"删除红黑树中的节点"大致分为两步，在第一步中"将红黑树当作一颗二叉查找树，将节点删除"后，可能违反"特性(2)、(4)、(5)"三个特性。第二步需要解决上面的三个问题，进而保持红黑树的全部特性。   为了便于分析，我们假设"x包含一个额外的黑色"(x原本的颜色还存在)，这样就不会违反"特性(5)"。为什么呢？   通过RB-DELETE算法，我们知道：删除节点y之后，x占据了原来节点y的位置。 既然删除y(y是黑色)，意味着减少一个黑色节点；那么，再在该位置上增加一个黑色即可。这样，当我们假设"x包含一个额外的黑色"，就正好弥补了"删除y所丢失的黑色节点"，也就不会违反"特性(5)"。 因此，假设"x包含一个额外的黑色"(x原本的颜色还存在)，这样就不会违反"特性(5)"。   现在，x不仅包含它原本的颜色属性，x还包含一个额外的黑色。即x的颜色属性是"红+黑"或"黑+黑"，它违反了"特性(1)"。

   现在，我们面临的问题，由解决"违反了特性(2)、(4)、(5)三个特性"转换成了"解决违反特性(1)、(2)、(4)三个特性"。RB-DELETE-FIXUP需要做的就是通过算法恢复红黑树的特性(1)、(2)、(4)。RB-DELETE-FIXUP的思想是：将x所包含的额外的黑色不断沿树上移(向根方向移动)，直到出现下面的姿态： a) x指向一个"红+黑"节点。此时，将x设为一个"黑"节点即可。 b) x指向根。此时，将x设为一个"黑"节点即可。 c) 非前面两种姿态。

将上面的姿态，可以概括为3种情况。 ① 情况说明：x是“红+黑”节点。  处理方法：直接把x设为黑色，结束。此时红黑树性质全部恢复。 ② 情况说明：x是“黑+黑”节点，且x是根。  处理方法：什么都不做，结束。此时红黑树性质全部恢复。 ③ 情况说明：x是“黑+黑”节点，且x不是根。  处理方法：这种情况又可以划分为4种子情况。这4种子情况如下表所示：

|            | **现象说明**                                                 | **处理策略**                                                 |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **Case 1** | x是"黑+黑"节点，x的兄弟节点是红色。(此时x的父节点和x的兄弟节点的子节点都是黑节点)。 | (01) 将x的兄弟节点设为“黑色”。 (02) 将x的父节点设为“红色”。 (03) 对x的父节点进行左旋。 (04) 左旋后，重新设置x的兄弟节点。 |
| **Case 2** | x是“黑+黑”节点，x的兄弟节点是黑色，x的兄弟节点的两个孩子都是黑色。 | (01) 将x的兄弟节点设为“红色”。 (02) 设置“x的父节点”为“新的x节点”。 |
| **Case 3** | x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的左孩子是红色，右孩子是黑色的。 | (01) 将x兄弟节点的左孩子设为“黑色”。 (02) 将x兄弟节点设为“红色”。 (03) 对x的兄弟节点进行右旋。 (04) 右旋后，重新设置x的兄弟节点。 |
| **Case 4** | x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的右孩子是红色的，x的兄弟节点的左孩子任意颜色。 | (01) 将x父节点颜色 赋值给 x的兄弟节点。 (02) 将x父节点设为“黑色”。 (03) 将x兄弟节点的右子节设为“黑色”。 (04) 对x的父节点进行左旋。 (05) 设置“x”为“根节点”。 |

 

**1. (Case 1)x是"黑+黑"节点，x的兄弟节点是红色**

**1.1 现象说明** x是"黑+黑"节点，x的兄弟节点是红色。(此时x的父节点和x的兄弟节点的子节点都是黑节点)。

**1.2 处理策略** (01) 将x的兄弟节点设为“黑色”。 (02) 将x的父节点设为“红色”。 (03) 对x的父节点进行左旋。 (04) 左旋后，重新设置x的兄弟节点。

   **下面谈谈为什么要这样处理。**(建议理解的时候，通过下面的图进行对比)   这样做的目的是将“Case 1”转换为“Case 2”、“Case 3”或“Case 4”，从而进行进一步的处理。对x的父节点进行左旋；左旋后，为了保持红黑树特性，就需要在左旋前“将x的兄弟节点设为黑色”，同时“将x的父节点设为红色”；左旋后，由于x的兄弟节点发生了变化，需要更新x的兄弟节点，从而进行后续处理。

**1.3 示意图**

[![img](https://images0.cnblogs.com/i/497634/201403/251813539515702.jpg)](https://images0.cnblogs.com/i/497634/201403/251813539515702.jpg)

 

**2. (Case 2) x是"黑+黑"节点，x的兄弟节点是黑色，x的兄弟节点的两个孩子都是黑色**

**2.1 现象说明** x是“黑+黑”节点，x的兄弟节点是黑色，x的兄弟节点的两个孩子都是黑色。

**2.2 处理策略** (01) 将x的兄弟节点设为“红色”。 (02) 设置“x的父节点”为“新的x节点”。

   **下面谈谈为什么要这样处理。**(建议理解的时候，通过下面的图进行对比)   这个情况的处理思想：是将“x中多余的一个黑色属性上移(往根方向移动)”。 x是“黑+黑”节点，我们将x由“黑+黑”节点 变成 “黑”节点，多余的一个“黑”属性移到x的父节点中，即x的父节点多出了一个黑属性(若x的父节点原先是“黑”，则此时变成了“黑+黑”；若x的父节点原先时“红”，则此时变成了“红+黑”)。 此时，需要注意的是：所有经过x的分支中黑节点个数没变化；但是，所有经过x的兄弟节点的分支中黑色节点的个数增加了1(因为x的父节点多了一个黑色属性)！为了解决这个问题，我们需要将“所有经过x的兄弟节点的分支中黑色节点的个数减1”即可，那么就可以通过“将x的兄弟节点由黑色变成红色”来实现。   经过上面的步骤(将x的兄弟节点设为红色)，多余的一个颜色属性(黑色)已经跑到x的父节点中。我们需要将x的父节点设为“新的x节点”进行处理。若“新的x节点”是“黑+红”，直接将“新的x节点”设为黑色，即可完全解决该问题；若“新的x节点”是“黑+黑”，则需要对“新的x节点”进行进一步处理。

**2.3 示意图**

[**![img](https://images0.cnblogs.com/i/497634/201403/251814572322069.jpg)**](https://images0.cnblogs.com/i/497634/201403/251814572322069.jpg)

 

**3. (Case 3)x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的左孩子是红色，右孩子是黑色的**

**3.1 现象说明** x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的左孩子是红色，右孩子是黑色的。

**3.2 处理策略** (01) 将x兄弟节点的左孩子设为“黑色”。 (02) 将x兄弟节点设为“红色”。 (03) 对x的兄弟节点进行右旋。 (04) 右旋后，重新设置x的兄弟节点。

​    **下面谈谈为什么要这样处理。**(建议理解的时候，通过下面的图进行对比)     我们处理“Case 3”的目的是为了将“Case 3”进行转换，转换成“Case 4”,从而进行进一步的处理。转换的方式是对x的兄弟节点进行右旋；为了保证右旋后，它仍然是红黑树，就需要在右旋前“将x的兄弟节点的左孩子设为黑色”，同时“将x的兄弟节点设为红色”；右旋后，由于x的兄弟节点发生了变化，需要更新x的兄弟节点，从而进行后续处理。

**3.3 示意图**

[**![img](https://images0.cnblogs.com/i/497634/201403/251815496235531.jpg)**](https://images0.cnblogs.com/i/497634/201403/251815496235531.jpg)

 

**4. (Case 4)x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的右孩子是红色的，x的兄弟节点的左孩子任意颜色**

**4.1 现象说明** x是“黑+黑”节点，x的兄弟节点是黑色；x的兄弟节点的右孩子是红色的，x的兄弟节点的左孩子任意颜色。

**4.2 处理策略** (01) 将x父节点颜色 赋值给 x的兄弟节点。 (02) 将x父节点设为“黑色”。 (03) 将x兄弟节点的右子节设为“黑色”。 (04) 对x的父节点进行左旋。 (05) 设置“x”为“根节点”。

   **下面谈谈为什么要这样处理。**(建议理解的时候，通过下面的图进行对比)   我们处理“Case 4”的目的是：去掉x中额外的黑色，将x变成单独的黑色。处理的方式是“：进行颜色修改，然后对x的父节点进行左旋。下面，我们来分析是如何实现的。   为了便于说明，我们设置“当前节点”为S(Original Son)，“兄弟节点”为B(Brother)，“兄弟节点的左孩子”为BLS(Brother's Left Son)，“兄弟节点的右孩子”为BRS(Brother's Right Son)，“父节点”为F(Father)。   我们要对F进行左旋。但在左旋前，我们需要调换F和B的颜色，并设置BRS为黑色。为什么需要这里处理呢？因为左旋后，F和BLS是父子关系，而我们已知BL是红色，如果F是红色，则违背了“特性(4)”；为了解决这一问题，我们将“F设置为黑色”。 但是，F设置为黑色之后，为了保证满足“特性(5)”，即为了保证左旋之后：   第一，“同时经过根节点和S的分支的黑色节点个数不变”。       若满足“第一”，只需要S丢弃它多余的颜色即可。因为S的颜色是“黑+黑”，而左旋后“同时经过根节点和S的分支的黑色节点个数”增加了1；现在，只需将S由“黑+黑”变成单独的“黑”节点，即可满足“第一”。   第二，“同时经过根节点和BLS的分支的黑色节点数不变”。       若满足“第二”，只需要将“F的原始颜色”赋值给B即可。之前，我们已经将“F设置为黑色”(即，将B的颜色"黑色"，赋值给了F)。至此，我们算是调换了F和B的颜色。   第三，“同时经过根节点和BRS的分支的黑色节点数不变”。       在“第二”已经满足的情况下，若要满足“第三”，只需要将BRS设置为“黑色”即可。 经过，上面的处理之后。红黑树的特性全部得到的满足！接着，我们将x设为根节点，就可以跳出while循环(参考伪代码)；即完成了全部处理。

至此，我们就完成了Case 4的处理。理解Case 4的核心，是了解如何“去掉当前节点额外的黑色”。

**4.3 示意图**

[![img](https://images0.cnblogs.com/i/497634/201403/251817189983393.jpg)](https://images0.cnblogs.com/i/497634/201403/251817189983393.jpg)

  

OK！至此，红黑树的理论知识差不多讲完了。后续再更新红黑树的实现代码！

# 【NO.3】红黑树(二)之 C语言的实现

## **1.红黑树的介绍**

红黑树(Red-Black Tree，简称R-B Tree)，它一种特殊的二叉查找树。 红黑树是特殊的二叉查找树，意味着它满足二叉查找树的特征：任意一个节点所包含的键值，大于等于左孩子的键值，小于等于右孩子的键值。 除了具备该特性之外，红黑树还包括许多额外的信息。

红黑树的每个节点上都有存储位表示节点的颜色，颜色是红(Red)或黑(Black)。 红黑树的特性: (1) 每个节点或者是黑色，或者是红色。 (2) 根节点是黑色。 (3) 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！] (4) 如果一个节点是红色的，则它的子节点必须是黑色的。 (5) 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。

关于它的特性，需要注意的是： 第一，特性(3)中的叶子节点，是只为空(NIL或null)的节点。 第二，特性(5)，确保没有一条路径会比其他路径长出俩倍。因而，红黑树是相对是接近平衡的二叉树。

红黑树示意图如下：

[![img](https://images0.cnblogs.com/i/497634/201403/251730074203156.jpg)](https://images0.cnblogs.com/i/497634/201403/251730074203156.jpg)

 

## **2.红黑树的C实现(代码说明)**

红黑树的基本操作是**添加**、**删除**和**旋转**。在对红黑树进行添加或删除后，会用到旋转方法。为什么呢？道理很简单，添加或删除红黑树中的节点之后，红黑树就发生了变化，可能不满足红黑树的5条性质，也就不再是一颗红黑树了，而是一颗普通的树。而通过旋转，可以使这颗树重新成为红黑树。简单点说，旋转的目的是让树保持红黑树的特性。 旋转包括两种：**左旋** 和 **右旋**。下面分别对旋转(左旋和右旋)、添加、删除进行介绍。

**1. 基本定义**

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
#define RED        0    // 红色节点
#define BLACK    1    // 黑色节点

typedef int Type;

// 红黑树的节点
typedef struct RBTreeNode{
    unsigned char color;        // 颜色(RED 或 BLACK)
    Type   key;                    // 关键字(键值)
    struct RBTreeNode *left;    // 左孩子
    struct RBTreeNode *right;    // 右孩子
    struct RBTreeNode *parent;    // 父结点
}Node, *RBTree;

// 红黑树的根
typedef struct rb_root{
    Node *node;
}RBRoot;
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

RBTreeNode是红黑树的节点类，RBRoot是红黑树的根。

 

**2. 左旋**

[![img](https://images0.cnblogs.com/i/497634/201403/251733282013849.jpg)](https://images0.cnblogs.com/i/497634/201403/251733282013849.jpg)

对x进行左旋，意味着"将x变成一个左节点"。

左旋的实现代码(C语言)

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
/* 
 * 对红黑树的节点(x)进行左旋转
 *
 * 左旋示意图(对节点x进行左旋)：
 *      px                              px
 *     /                               /
 *    x                               y                
 *   /  \      --(左旋)-->           / \                #
 *  lx   y                          x  ry     
 *     /   \                       /  \
 *    ly   ry                     lx  ly  
 *
 *
 */
static void rbtree_left_rotate(RBRoot *root, Node *x)
{
    // 设置x的右孩子为y
    Node *y = x->right;

    // 将 “y的左孩子” 设为 “x的右孩子”；
    // 如果y的左孩子非空，将 “x” 设为 “y的左孩子的父亲”
    x->right = y->left;
    if (y->left != NULL)
        y->left->parent = x;

    // 将 “x的父亲” 设为 “y的父亲”
    y->parent = x->parent;

    if (x->parent == NULL)
    {
        //tree = y;            // 如果 “x的父亲” 是空节点，则将y设为根节点
        root->node = y;            // 如果 “x的父亲” 是空节点，则将y设为根节点
    }
    else
    {
        if (x->parent->left == x)
            x->parent->left = y;    // 如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子”
        else
            x->parent->right = y;    // 如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子”
    }
    
    // 将 “x” 设为 “y的左孩子”
    y->left = x;
    // 将 “x的父节点” 设为 “y”
    x->parent = y;
}
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 

**3. 右旋**

[![img](https://images0.cnblogs.com/i/497634/201403/251735527958942.jpg)](https://images0.cnblogs.com/i/497634/201403/251735527958942.jpg)

对y进行左旋，意味着"将y变成一个右节点"。

右旋的实现代码(C语言)

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
/* 
 * 对红黑树的节点(y)进行右旋转
 *
 * 右旋示意图(对节点y进行左旋)：
 *            py                               py
 *           /                                /
 *          y                                x                  
 *         /  \      --(右旋)-->            /  \                     #
 *        x   ry                           lx   y  
 *       / \                                   / \                   #
 *      lx  rx                                rx  ry
 * 
 */
static void rbtree_right_rotate(RBRoot *root, Node *y)
{
    // 设置x是当前节点的左孩子。
    Node *x = y->left;

    // 将 “x的右孩子” 设为 “y的左孩子”；
    // 如果"x的右孩子"不为空的话，将 “y” 设为 “x的右孩子的父亲”
    y->left = x->right;
    if (x->right != NULL)
        x->right->parent = y;

    // 将 “y的父亲” 设为 “x的父亲”
    x->parent = y->parent;

    if (y->parent == NULL) 
    {
        //tree = x;            // 如果 “y的父亲” 是空节点，则将x设为根节点
        root->node = x;            // 如果 “y的父亲” 是空节点，则将x设为根节点
    }
    else
    {
        if (y == y->parent->right)
            y->parent->right = x;    // 如果 y是它父节点的右孩子，则将x设为“y的父节点的右孩子”
        else
            y->parent->left = x;    // (y是它父节点的左孩子) 将x设为“x的父节点的左孩子”
    }

    // 将 “y” 设为 “x的右孩子”
    x->right = y;

    // 将 “y的父节点” 设为 “x”
    y->parent = x;
}
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 

**4. 添加**

将一个节点(z)插入到红黑树中，需要执行哪些步骤呢？首先，将红黑树当作一颗二叉查找树，将节点插入；然后，将节点着色为红色；最后，通过"旋转和重新着色"等一系列操作来修正该树，使之重新成为一颗红黑树。详细描述如下：

**第一步: 将红黑树当作一颗二叉查找树，将节点插入。**    红黑树本身就是一颗二叉查找树，将节点插入后，该树仍然是一颗二叉查找树。也就意味着，树的键值仍然是有序的。此外，无论是左旋还是右旋，若旋转之前这棵树是二叉查找树，旋转之后它一定还是二叉查找树。这也就意味着，任何的旋转和重新着色操作，都不会改变它仍然是一颗二叉查找树的事实。    好吧？那接下来，我们就来想方设法的旋转以及重新着色，使这颗树重新成为红黑树！

**第二步：将插入的节点着色为"红色"。**    为什么着色成红色，而不是黑色呢？为什么呢？在回答之前，我们需要重新温习一下红黑树的特性： (1) 每个节点或者是黑色，或者是红色。 (2) 根节点是黑色。 (3) 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！] (4) 如果一个节点是红色的，则它的子节点必须是黑色的。 (5) 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。    将插入的节点着色为红色，不会违背"特性(5)"！少违背一条特性，就意味着我们需要处理的情况越少。接下来，就要努力的让这棵树满足其它性质即可；满足了的话，它就又是一颗红黑树了。o(∩∩)o...哈哈

**第三步: 通过一系列的旋转或着色等操作，使之重新成为一颗红黑树。**    第二步中，将插入节点着色为"红色"之后，不会违背"特性(5)"。那它到底会违背哪些特性呢？    对于"特性(1)"，显然不会违背了。因为我们已经将它涂成红色了。    对于"特性(2)"，显然也不会违背。在第一步中，我们是将红黑树当作二叉查找树，然后执行的插入操作。而根据二叉查找数的特点，插入操作不会改变根节点。所以，根节点仍然是黑色。    对于"特性(3)"，显然不会违背了。这里的叶子节点是指的空叶子节点，插入非空节点并不会对它们造成影响。    对于"特性(4)"，是有可能违背的！ 那接下来，想办法使之"满足特性(4)"，就可以将树重新构造成红黑树了。

添加操作的实现代码(C语言)

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
/*
 * 添加节点：将节点(node)插入到红黑树中
 *
 * 参数说明：
 *     root 红黑树的根
 *     node 插入的结点        // 对应《算法导论》中的z
 */
static void rbtree_insert(RBRoot *root, Node *node)
{
    Node *y = NULL;
    Node *x = root->node;

    // 1. 将红黑树当作一颗二叉查找树，将节点添加到二叉查找树中。
    while (x != NULL)
    {
        y = x;
        if (node->key < x->key)
            x = x->left;
        else
            x = x->right;
    }
    rb_parent(node) = y;

    if (y != NULL)
    {
        if (node->key < y->key)
            y->left = node;                // 情况2：若“node所包含的值” < “y所包含的值”，则将node设为“y的左孩子”
        else
            y->right = node;            // 情况3：(“node所包含的值” >= “y所包含的值”)将node设为“y的右孩子” 
    }
    else
    {
        root->node = node;                // 情况1：若y是空节点，则将node设为根
    }

    // 2. 设置节点的颜色为红色
    node->color = RED;

    // 3. 将它重新修正为一颗二叉查找树
    rbtree_insert_fixup(root, node);
}
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

rbtree_insert(root, node)的作用是将"node"节点插入到红黑树中。其中，root是根，node是被插入节点。 rbtree_insert(root, node)是参考《算法导论》中红黑树的插入函数的伪代码进行实现的。

添加修正操作的实现代码(C语言)

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
/*
 * 红黑树插入修正函数
 *
 * 在向红黑树中插入节点之后(失去平衡)，再调用该函数；
 * 目的是将它重新塑造成一颗红黑树。
 *
 * 参数说明：
 *     root 红黑树的根
 *     node 插入的结点        // 对应《算法导论》中的z
 */
static void rbtree_insert_fixup(RBRoot *root, Node *node)
{
    Node *parent, *gparent;

    // 若“父节点存在，并且父节点的颜色是红色”
    while ((parent = rb_parent(node)) && rb_is_red(parent))
    {
        gparent = rb_parent(parent);

        //若“父节点”是“祖父节点的左孩子”
        if (parent == gparent->left)
        {
            // Case 1条件：叔叔节点是红色
            {
                Node *uncle = gparent->right;
                if (uncle && rb_is_red(uncle))
                {
                    rb_set_black(uncle);
                    rb_set_black(parent);
                    rb_set_red(gparent);
                    node = gparent;
                    continue;
                }
            }

            // Case 2条件：叔叔是黑色，且当前节点是右孩子
            if (parent->right == node)
            {
                Node *tmp;
                rbtree_left_rotate(root, parent);
                tmp = parent;
                parent = node;
                node = tmp;
            }

            // Case 3条件：叔叔是黑色，且当前节点是左孩子。
            rb_set_black(parent);
            rb_set_red(gparent);
            rbtree_right_rotate(root, gparent);
        } 
        else//若“z的父节点”是“z的祖父节点的右孩子”
        {
            // Case 1条件：叔叔节点是红色
            {
                Node *uncle = gparent->left;
                if (uncle && rb_is_red(uncle))
                {
                    rb_set_black(uncle);
                    rb_set_black(parent);
                    rb_set_red(gparent);
                    node = gparent;
                    continue;
                }
            }

            // Case 2条件：叔叔是黑色，且当前节点是左孩子
            if (parent->left == node)
            {
                Node *tmp;
                rbtree_right_rotate(root, parent);
                tmp = parent;
                parent = node;
                node = tmp;
            }

            // Case 3条件：叔叔是黑色，且当前节点是右孩子。
            rb_set_black(parent);
            rb_set_red(gparent);
            rbtree_left_rotate(root, gparent);
        }
    }

    // 将根节点设为黑色
    rb_set_black(root->node);
}
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

rbtree_insert_fixup(root, node)的作用是对应"上面所讲的第三步"。

 

**5. 删除操作**

将红黑树内的某一个节点删除。需要执行的操作依次是：首先，将红黑树当作一颗二叉查找树，将该节点从二叉查找树中删除；然后，通过"旋转和重新着色"等一系列来修正该树，使之重新成为一棵红黑树。详细描述如下：

**第一步：将红黑树当作一颗二叉查找树，将节点删除。**    这和"删除常规二叉查找树中删除节点的方法是一样的"。分3种情况： ① 被删除节点没有儿子，即为叶节点。那么，直接将该节点删除就OK了。 ② 被删除节点只有一个儿子。那么，直接删除该节点，并用该节点的唯一子节点顶替它的位置。 ③ 被删除节点有两个儿子。那么，先找出它的后继节点；然后把“它的后继节点的内容”复制给“该节点的内容”；之后，删除“它的后继节点”。在这里，后继节点相当于替身，在将后继节点的内容复制给"被删除节点"之后，再将后继节点删除。这样就巧妙的将问题转换为"删除后继节点"的情况了，下面就考虑后继节点。 在"被删除节点"有两个非空子节点的情况下，它的后继节点不可能是双子非空。既然"的后继节点"不可能双子都非空，就意味着"该节点的后继节点"要么没有儿子，要么只有一个儿子。若没有儿子，则按"情况① "进行处理；若只有一个儿子，则按"情况② "进行处理。

**第二步：通过"旋转和重新着色"等一系列来修正该树，使之重新成为一棵红黑树。** 因为"第一步"中删除节点之后，可能会违背红黑树的特性。所以需要通过"旋转和重新着色"来修正该树，使之重新成为一棵红黑树。

删除操作的实现代码(C语言)

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
/* 
 * 删除结点
 *
 * 参数说明：
 *     tree 红黑树的根结点
 *     node 删除的结点
 */
void rbtree_delete(RBRoot *root, Node *node)
{
    Node *child, *parent;
    int color;

    // 被删除节点的"左右孩子都不为空"的情况。
    if ( (node->left!=NULL) && (node->right!=NULL) ) 
    {
        // 被删节点的后继节点。(称为"取代节点")
        // 用它来取代"被删节点"的位置，然后再将"被删节点"去掉。
        Node *replace = node;

        // 获取后继节点
        replace = replace->right;
        while (replace->left != NULL)
            replace = replace->left;

        // "node节点"不是根节点(只有根节点不存在父节点)
        if (rb_parent(node))
        {
            if (rb_parent(node)->left == node)
                rb_parent(node)->left = replace;
            else
                rb_parent(node)->right = replace;
        } 
        else 
            // "node节点"是根节点，更新根节点。
            root->node = replace;

        // child是"取代节点"的右孩子，也是需要"调整的节点"。
        // "取代节点"肯定不存在左孩子！因为它是一个后继节点。
        child = replace->right;
        parent = rb_parent(replace);
        // 保存"取代节点"的颜色
        color = rb_color(replace);

        // "被删除节点"是"它的后继节点的父节点"
        if (parent == node)
        {
            parent = replace;
        } 
        else
        {
            // child不为空
            if (child)
                rb_set_parent(child, parent);
            parent->left = child;

            replace->right = node->right;
            rb_set_parent(node->right, replace);
        }

        replace->parent = node->parent;
        replace->color = node->color;
        replace->left = node->left;
        node->left->parent = replace;

        if (color == BLACK)
            rbtree_delete_fixup(root, child, parent);
        free(node);

        return ;
    }

    if (node->left !=NULL)
        child = node->left;
    else 
        child = node->right;

    parent = node->parent;
    // 保存"取代节点"的颜色
    color = node->color;

    if (child)
        child->parent = parent;

    // "node节点"不是根节点
    if (parent)
    {
        if (parent->left == node)
            parent->left = child;
        else
            parent->right = child;
    }
    else
        root->node = child;

    if (color == BLACK)
        rbtree_delete_fixup(root, child, parent);
    free(node);
}
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

rbtree_delete(root, node)的作用是将"node"节点插入到红黑树中。其中，root是根，node是被插入节点。

删除修正操作的实现代码(C语言)

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
/*
 * 红黑树删除修正函数
 *
 * 在从红黑树中删除插入节点之后(红黑树失去平衡)，再调用该函数；
 * 目的是将它重新塑造成一颗红黑树。
 *
 * 参数说明：
 *     root 红黑树的根
 *     node 待修正的节点
 */
static void rbtree_delete_fixup(RBRoot *root, Node *node, Node *parent)
{
    Node *other;

    while ((!node || rb_is_black(node)) && node != root->node)
    {
        if (parent->left == node)
        {
            other = parent->right;
            if (rb_is_red(other))
            {
                // Case 1: x的兄弟w是红色的  
                rb_set_black(other);
                rb_set_red(parent);
                rbtree_left_rotate(root, parent);
                other = parent->right;
            }
            if ((!other->left || rb_is_black(other->left)) &&
                (!other->right || rb_is_black(other->right)))
            {
                // Case 2: x的兄弟w是黑色，且w的俩个孩子也都是黑色的  
                rb_set_red(other);
                node = parent;
                parent = rb_parent(node);
            }
            else
            {
                if (!other->right || rb_is_black(other->right))
                {
                    // Case 3: x的兄弟w是黑色的，并且w的左孩子是红色，右孩子为黑色。  
                    rb_set_black(other->left);
                    rb_set_red(other);
                    rbtree_right_rotate(root, other);
                    other = parent->right;
                }
                // Case 4: x的兄弟w是黑色的；并且w的右孩子是红色的，左孩子任意颜色。
                rb_set_color(other, rb_color(parent));
                rb_set_black(parent);
                rb_set_black(other->right);
                rbtree_left_rotate(root, parent);
                node = root->node;
                break;
            }
        }
        else
        {
            other = parent->left;
            if (rb_is_red(other))
            {
                // Case 1: x的兄弟w是红色的  
                rb_set_black(other);
                rb_set_red(parent);
                rbtree_right_rotate(root, parent);
                other = parent->left;
            }
            if ((!other->left || rb_is_black(other->left)) &&
                (!other->right || rb_is_black(other->right)))
            {
                // Case 2: x的兄弟w是黑色，且w的俩个孩子也都是黑色的  
                rb_set_red(other);
                node = parent;
                parent = rb_parent(node);
            }
            else
            {
                if (!other->left || rb_is_black(other->left))
                {
                    // Case 3: x的兄弟w是黑色的，并且w的左孩子是红色，右孩子为黑色。  
                    rb_set_black(other->right);
                    rb_set_red(other);
                    rbtree_left_rotate(root, other);
                    other = parent->left;
                }
                // Case 4: x的兄弟w是黑色的；并且w的右孩子是红色的，左孩子任意颜色。
                rb_set_color(other, rb_color(parent));
                rb_set_black(parent);
                rb_set_black(other->left);
                rbtree_right_rotate(root, parent);
                node = root->node;
                break;
            }
        }
    }
    if (node)
        rb_set_black(node);
}
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

rbtree_delete_fixup(root, node, parent)是对应"上面所讲的第三步"。

 

## **3.红黑树的C实现(完整源码)**

下面是红黑数实现的完整代码和相应的测试程序。 (1) 除了上面所说的"左旋"、"右旋"、"添加"、"删除"等基本操作之后，还实现了"遍历"、"查找"、"打印"、"最小值"、"最大值"、"创建"、"销毁"等接口。 (2) 函数接口分为内部接口和外部接口。内部接口是static函数，外部接口则是非static函数，外部接口都在.h头文件中表明了。 (3) 测试代码中提供了"插入"和"删除"动作的检测开关。默认是关闭的，打开方法可以参考"代码中的说明"。建议在打开开关后，在草稿上自己动手绘制一下红黑树。

红黑树的实现文件(rbtree.h)

![img](https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif) View Code

红黑树的实现文件(rbtree.c)

![img](https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif) View Code

红黑树的测试文件(rbtree_test.c)

![img](https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif) View Code

 

## **4.红黑树的C测试程序**

前面已经给出了红黑树的测试程序(rbtree_test.c)，这里就不再重复说明。下面是测试程序的运行结果：

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
== 原始数据: 10 40 30 60 90 70 20 50 80 
== 前序遍历: 30 10 20 60 40 50 80 70 90 
== 中序遍历: 10 20 30 40 50 60 70 80 90 
== 后序遍历: 20 10 50 40 70 90 80 60 30 
== 最小值: 10
== 最大值: 90
== 树的详细信息: 
30(B) is root
10(B) is 30's   left child
20(R) is 10's  right child
60(R) is 30's  right child
40(B) is 60's   left child
50(R) is 40's  right child
80(B) is 60's  right child
70(R) is 80's   left child
90(R) is 80's  right child
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

# 【NO.4】红黑树(三)之 Linux内核中红黑树的经典实现

## **1.Linux内核中红黑树(完整源码)**

### 1.1红黑树的实现文件(rbtree.h)

![img](https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif)

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
  1 /*
  2   Red Black Trees
  3   (C) 1999  Andrea Arcangeli <andrea@suse.de>
  4   
  5   This program is free software; you can redistribute it and/or modify
  6   it under the terms of the GNU General Public License as published by
  7   the Free Software Foundation; either version 2 of the License, or
  8   (at your option) any later version.
  9 
 10   This program is distributed in the hope that it will be useful,
 11   but WITHOUT ANY WARRANTY; without even the implied warranty of
 12   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 13   GNU General Public License for more details.
 14 
 15   You should have received a copy of the GNU General Public License
 16   along with this program; if not, write to the Free Software
 17   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 18 
 19   linux/include/linux/rbtree.h
 20 
 21   To use rbtrees you'll have to implement your own insert and search cores.
 22   This will avoid us to use callbacks and to drop drammatically performances.
 23   I know it's not the cleaner way,  but in C (not in C++) to get
 24   performances and genericity...
 25 
 26   Some example of insert and search follows here. The search is a plain
 27   normal search over an ordered tree. The insert instead must be implemented
 28   in two steps: First, the code must insert the element in order as a red leaf
 29   in the tree, and then the support library function rb_insert_color() must
 30   be called. Such function will do the not trivial work to rebalance the
 31   rbtree, if necessary.
 32 
 33 -----------------------------------------------------------------------
 34 static inline struct page * rb_search_page_cache(struct inode * inode,
 35                          unsigned long offset)
 36 {
 37     struct rb_node * n = inode->i_rb_page_cache.rb_node;
 38     struct page * page;
 39 
 40     while (n)
 41     {
 42         page = rb_entry(n, struct page, rb_page_cache);
 43 
 44         if (offset < page->offset)
 45             n = n->rb_left;
 46         else if (offset > page->offset)
 47             n = n->rb_right;
 48         else
 49             return page;
 50     }
 51     return NULL;
 52 }
 53 
 54 static inline struct page * __rb_insert_page_cache(struct inode * inode,
 55                            unsigned long offset,
 56                            struct rb_node * node)
 57 {
 58     struct rb_node ** p = &inode->i_rb_page_cache.rb_node;
 59     struct rb_node * parent = NULL;
 60     struct page * page;
 61 
 62     while (*p)
 63     {
 64         parent = *p;
 65         page = rb_entry(parent, struct page, rb_page_cache);
 66 
 67         if (offset < page->offset)
 68             p = &(*p)->rb_left;
 69         else if (offset > page->offset)
 70             p = &(*p)->rb_right;
 71         else
 72             return page;
 73     }
 74 
 75     rb_link_node(node, parent, p);
 76 
 77     return NULL;
 78 }
 79 
 80 static inline struct page * rb_insert_page_cache(struct inode * inode,
 81                          unsigned long offset,
 82                          struct rb_node * node)
 83 {
 84     struct page * ret;
 85     if ((ret = __rb_insert_page_cache(inode, offset, node)))
 86         goto out;
 87     rb_insert_color(node, &inode->i_rb_page_cache);
 88  out:
 89     return ret;
 90 }
 91 -----------------------------------------------------------------------
 92 */
 93 
 94 #ifndef    _SLINUX_RBTREE_H
 95 #define    _SLINUX_RBTREE_H
 96 
 97 #include <stdio.h>
 98 //#include <linux/kernel.h>
 99 //#include <linux/stddef.h>
100 
101 struct rb_node
102 {
103     unsigned long  rb_parent_color;
104 #define    RB_RED        0
105 #define    RB_BLACK    1
106     struct rb_node *rb_right;
107     struct rb_node *rb_left;
108 } /*  __attribute__((aligned(sizeof(long))))*/;
109     /* The alignment might seem pointless, but allegedly CRIS needs it */
110 
111 struct rb_root
112 {
113     struct rb_node *rb_node;
114 };
115 
116 
117 #define rb_parent(r)   ((struct rb_node *)((r)->rb_parent_color & ~3))
118 #define rb_color(r)   ((r)->rb_parent_color & 1)
119 #define rb_is_red(r)   (!rb_color(r))
120 #define rb_is_black(r) rb_color(r)
121 #define rb_set_red(r)  do { (r)->rb_parent_color &= ~1; } while (0)
122 #define rb_set_black(r)  do { (r)->rb_parent_color |= 1; } while (0)
123 
124 static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
125 {
126     rb->rb_parent_color = (rb->rb_parent_color & 3) | (unsigned long)p;
127 }
128 static inline void rb_set_color(struct rb_node *rb, int color)
129 {
130     rb->rb_parent_color = (rb->rb_parent_color & ~1) | color;
131 }
132 
133 #define offsetof(TYPE, MEMBER) ((size_t) &((TYPE *)0)->MEMBER)
134 
135 #define container_of(ptr, type, member) ({          \
136     const typeof( ((type *)0)->member ) *__mptr = (ptr);    \
137     (type *)( (char *)__mptr - offsetof(type,member) );})
138 
139 #define RB_ROOT    (struct rb_root) { NULL, }
140 #define    rb_entry(ptr, type, member) container_of(ptr, type, member)
141 
142 #define RB_EMPTY_ROOT(root)    ((root)->rb_node == NULL)
143 #define RB_EMPTY_NODE(node)    (rb_parent(node) == node)
144 #define RB_CLEAR_NODE(node)    (rb_set_parent(node, node))
145 
146 static inline void rb_init_node(struct rb_node *rb)
147 {
148     rb->rb_parent_color = 0;
149     rb->rb_right = NULL;
150     rb->rb_left = NULL;
151     RB_CLEAR_NODE(rb);
152 }
153 
154 extern void rb_insert_color(struct rb_node *, struct rb_root *);
155 extern void rb_erase(struct rb_node *, struct rb_root *);
156 
157 typedef void (*rb_augment_f)(struct rb_node *node, void *data);
158 
159 extern void rb_augment_insert(struct rb_node *node,
160                   rb_augment_f func, void *data);
161 extern struct rb_node *rb_augment_erase_begin(struct rb_node *node);
162 extern void rb_augment_erase_end(struct rb_node *node,
163                  rb_augment_f func, void *data);
164 
165 /* Find logical next and previous nodes in a tree */
166 extern struct rb_node *rb_next(const struct rb_node *);
167 extern struct rb_node *rb_prev(const struct rb_node *);
168 extern struct rb_node *rb_first(const struct rb_root *);
169 extern struct rb_node *rb_last(const struct rb_root *);
170 
171 /* Fast replacement of a single node without remove/rebalance/add/rebalance */
172 extern void rb_replace_node(struct rb_node *victim, struct rb_node *new, 
173                 struct rb_root *root);
174 
175 static inline void rb_link_node(struct rb_node * node, struct rb_node * parent,
176                 struct rb_node ** rb_link)
177 {
178     node->rb_parent_color = (unsigned long )parent;
179     node->rb_left = node->rb_right = NULL;
180 
181     *rb_link = node;
182 }
183 
184 #endif    /* _LINUX_RBTREE_H */
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

### 1.2红黑树的实现文件(rbtree.c)

![img](https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif)

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
  1 /*
  2   Red Black Trees
  3   (C) 1999  Andrea Arcangeli <andrea@suse.de>
  4   (C) 2002  David Woodhouse <dwmw2@infradead.org>
  5   
  6   This program is free software; you can redistribute it and/or modify
  7   it under the terms of the GNU General Public License as published by
  8   the Free Software Foundation; either version 2 of the License, or
  9   (at your option) any later version.
 10 
 11   This program is distributed in the hope that it will be useful,
 12   but WITHOUT ANY WARRANTY; without even the implied warranty of
 13   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 14   GNU General Public License for more details.
 15 
 16   You should have received a copy of the GNU General Public License
 17   along with this program; if not, write to the Free Software
 18   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
 19 
 20   linux/lib/rbtree.c
 21 */
 22 
 23 #include "rbtree.h"
 24 
 25 static void __rb_rotate_left(struct rb_node *node, struct rb_root *root)
 26 {
 27     struct rb_node *right = node->rb_right;
 28     struct rb_node *parent = rb_parent(node);
 29 
 30     if ((node->rb_right = right->rb_left))
 31         rb_set_parent(right->rb_left, node);
 32     right->rb_left = node;
 33 
 34     rb_set_parent(right, parent);
 35 
 36     if (parent)
 37     {
 38         if (node == parent->rb_left)
 39             parent->rb_left = right;
 40         else
 41             parent->rb_right = right;
 42     }
 43     else
 44         root->rb_node = right;
 45     rb_set_parent(node, right);
 46 }
 47 
 48 static void __rb_rotate_right(struct rb_node *node, struct rb_root *root)
 49 {
 50     struct rb_node *left = node->rb_left;
 51     struct rb_node *parent = rb_parent(node);
 52 
 53     if ((node->rb_left = left->rb_right))
 54         rb_set_parent(left->rb_right, node);
 55     left->rb_right = node;
 56 
 57     rb_set_parent(left, parent);
 58 
 59     if (parent)
 60     {
 61         if (node == parent->rb_right)
 62             parent->rb_right = left;
 63         else
 64             parent->rb_left = left;
 65     }
 66     else
 67         root->rb_node = left;
 68     rb_set_parent(node, left);
 69 }
 70 
 71 void rb_insert_color(struct rb_node *node, struct rb_root *root)
 72 {
 73     struct rb_node *parent, *gparent;
 74 
 75     while ((parent = rb_parent(node)) && rb_is_red(parent))
 76     {
 77         gparent = rb_parent(parent);
 78 
 79         if (parent == gparent->rb_left)
 80         {
 81             {
 82                 register struct rb_node *uncle = gparent->rb_right;
 83                 if (uncle && rb_is_red(uncle))
 84                 {
 85                     rb_set_black(uncle);
 86                     rb_set_black(parent);
 87                     rb_set_red(gparent);
 88                     node = gparent;
 89                     continue;
 90                 }
 91             }
 92 
 93             if (parent->rb_right == node)
 94             {
 95                 register struct rb_node *tmp;
 96                 __rb_rotate_left(parent, root);
 97                 tmp = parent;
 98                 parent = node;
 99                 node = tmp;
100             }
101 
102             rb_set_black(parent);
103             rb_set_red(gparent);
104             __rb_rotate_right(gparent, root);
105         } else {
106             {
107                 register struct rb_node *uncle = gparent->rb_left;
108                 if (uncle && rb_is_red(uncle))
109                 {
110                     rb_set_black(uncle);
111                     rb_set_black(parent);
112                     rb_set_red(gparent);
113                     node = gparent;
114                     continue;
115                 }
116             }
117 
118             if (parent->rb_left == node)
119             {
120                 register struct rb_node *tmp;
121                 __rb_rotate_right(parent, root);
122                 tmp = parent;
123                 parent = node;
124                 node = tmp;
125             }
126 
127             rb_set_black(parent);
128             rb_set_red(gparent);
129             __rb_rotate_left(gparent, root);
130         }
131     }
132 
133     rb_set_black(root->rb_node);
134 }
135 
136 static void __rb_erase_color(struct rb_node *node, struct rb_node *parent,
137                  struct rb_root *root)
138 {
139     struct rb_node *other;
140 
141     while ((!node || rb_is_black(node)) && node != root->rb_node)
142     {
143         if (parent->rb_left == node)
144         {
145             other = parent->rb_right;
146             if (rb_is_red(other))
147             {
148                 rb_set_black(other);
149                 rb_set_red(parent);
150                 __rb_rotate_left(parent, root);
151                 other = parent->rb_right;
152             }
153             if ((!other->rb_left || rb_is_black(other->rb_left)) &&
154                 (!other->rb_right || rb_is_black(other->rb_right)))
155             {
156                 rb_set_red(other);
157                 node = parent;
158                 parent = rb_parent(node);
159             }
160             else
161             {
162                 if (!other->rb_right || rb_is_black(other->rb_right))
163                 {
164                     rb_set_black(other->rb_left);
165                     rb_set_red(other);
166                     __rb_rotate_right(other, root);
167                     other = parent->rb_right;
168                 }
169                 rb_set_color(other, rb_color(parent));
170                 rb_set_black(parent);
171                 rb_set_black(other->rb_right);
172                 __rb_rotate_left(parent, root);
173                 node = root->rb_node;
174                 break;
175             }
176         }
177         else
178         {
179             other = parent->rb_left;
180             if (rb_is_red(other))
181             {
182                 rb_set_black(other);
183                 rb_set_red(parent);
184                 __rb_rotate_right(parent, root);
185                 other = parent->rb_left;
186             }
187             if ((!other->rb_left || rb_is_black(other->rb_left)) &&
188                 (!other->rb_right || rb_is_black(other->rb_right)))
189             {
190                 rb_set_red(other);
191                 node = parent;
192                 parent = rb_parent(node);
193             }
194             else
195             {
196                 if (!other->rb_left || rb_is_black(other->rb_left))
197                 {
198                     rb_set_black(other->rb_right);
199                     rb_set_red(other);
200                     __rb_rotate_left(other, root);
201                     other = parent->rb_left;
202                 }
203                 rb_set_color(other, rb_color(parent));
204                 rb_set_black(parent);
205                 rb_set_black(other->rb_left);
206                 __rb_rotate_right(parent, root);
207                 node = root->rb_node;
208                 break;
209             }
210         }
211     }
212     if (node)
213         rb_set_black(node);
214 }
215 
216 void rb_erase(struct rb_node *node, struct rb_root *root)
217 {
218     struct rb_node *child, *parent;
219     int color;
220 
221     if (!node->rb_left)
222         child = node->rb_right;
223     else if (!node->rb_right)
224         child = node->rb_left;
225     else
226     {
227         struct rb_node *old = node, *left;
228 
229         node = node->rb_right;
230         while ((left = node->rb_left) != NULL)
231             node = left;
232 
233         if (rb_parent(old)) {
234             if (rb_parent(old)->rb_left == old)
235                 rb_parent(old)->rb_left = node;
236             else
237                 rb_parent(old)->rb_right = node;
238         } else
239             root->rb_node = node;
240 
241         child = node->rb_right;
242         parent = rb_parent(node);
243         color = rb_color(node);
244 
245         if (parent == old) {
246             parent = node;
247         } else {
248             if (child)
249                 rb_set_parent(child, parent);
250             parent->rb_left = child;
251 
252             node->rb_right = old->rb_right;
253             rb_set_parent(old->rb_right, node);
254         }
255 
256         node->rb_parent_color = old->rb_parent_color;
257         node->rb_left = old->rb_left;
258         rb_set_parent(old->rb_left, node);
259 
260         goto color;
261     }
262 
263     parent = rb_parent(node);
264     color = rb_color(node);
265 
266     if (child)
267         rb_set_parent(child, parent);
268     if (parent)
269     {
270         if (parent->rb_left == node)
271             parent->rb_left = child;
272         else
273             parent->rb_right = child;
274     }
275     else
276         root->rb_node = child;
277 
278  color:
279     if (color == RB_BLACK)
280         __rb_erase_color(child, parent, root);
281 }
282 
283 static void rb_augment_path(struct rb_node *node, rb_augment_f func, void *data)
284 {
285     struct rb_node *parent;
286 
287 up:
288     func(node, data);
289     parent = rb_parent(node);
290     if (!parent)
291         return;
292 
293     if (node == parent->rb_left && parent->rb_right)
294         func(parent->rb_right, data);
295     else if (parent->rb_left)
296         func(parent->rb_left, data);
297 
298     node = parent;
299     goto up;
300 }
301 
302 /*
303  * after inserting @node into the tree, update the tree to account for
304  * both the new entry and any damage done by rebalance
305  */
306 void rb_augment_insert(struct rb_node *node, rb_augment_f func, void *data)
307 {
308     if (node->rb_left)
309         node = node->rb_left;
310     else if (node->rb_right)
311         node = node->rb_right;
312 
313     rb_augment_path(node, func, data);
314 }
315 
316 /*
317  * before removing the node, find the deepest node on the rebalance path
318  * that will still be there after @node gets removed
319  */
320 struct rb_node *rb_augment_erase_begin(struct rb_node *node)
321 {
322     struct rb_node *deepest;
323 
324     if (!node->rb_right && !node->rb_left)
325         deepest = rb_parent(node);
326     else if (!node->rb_right)
327         deepest = node->rb_left;
328     else if (!node->rb_left)
329         deepest = node->rb_right;
330     else {
331         deepest = rb_next(node);
332         if (deepest->rb_right)
333             deepest = deepest->rb_right;
334         else if (rb_parent(deepest) != node)
335             deepest = rb_parent(deepest);
336     }
337 
338     return deepest;
339 }
340 
341 /*
342  * after removal, update the tree to account for the removed entry
343  * and any rebalance damage.
344  */
345 void rb_augment_erase_end(struct rb_node *node, rb_augment_f func, void *data)
346 {
347     if (node)
348         rb_augment_path(node, func, data);
349 }
350 
351 /*
352  * This function returns the first node (in sort order) of the tree.
353  */
354 struct rb_node *rb_first(const struct rb_root *root)
355 {
356     struct rb_node    *n;
357 
358     n = root->rb_node;
359     if (!n)
360         return NULL;
361     while (n->rb_left)
362         n = n->rb_left;
363     return n;
364 }
365 
366 struct rb_node *rb_last(const struct rb_root *root)
367 {
368     struct rb_node    *n;
369 
370     n = root->rb_node;
371     if (!n)
372         return NULL;
373     while (n->rb_right)
374         n = n->rb_right;
375     return n;
376 }
377 
378 struct rb_node *rb_next(const struct rb_node *node)
379 {
380     struct rb_node *parent;
381 
382     if (rb_parent(node) == node)
383         return NULL;
384 
385     /* If we have a right-hand child, go down and then left as far
386        as we can. */
387     if (node->rb_right) {
388         node = node->rb_right; 
389         while (node->rb_left)
390             node=node->rb_left;
391         return (struct rb_node *)node;
392     }
393 
394     /* No right-hand children.  Everything down and left is
395        smaller than us, so any 'next' node must be in the general
396        direction of our parent. Go up the tree; any time the
397        ancestor is a right-hand child of its parent, keep going
398        up. First time it's a left-hand child of its parent, said
399        parent is our 'next' node. */
400     while ((parent = rb_parent(node)) && node == parent->rb_right)
401         node = parent;
402 
403     return parent;
404 }
405 
406 struct rb_node *rb_prev(const struct rb_node *node)
407 {
408     struct rb_node *parent;
409 
410     if (rb_parent(node) == node)
411         return NULL;
412 
413     /* If we have a left-hand child, go down and then right as far
414        as we can. */
415     if (node->rb_left) {
416         node = node->rb_left; 
417         while (node->rb_right)
418             node=node->rb_right;
419         return (struct rb_node *)node;
420     }
421 
422     /* No left-hand children. Go up till we find an ancestor which
423        is a right-hand child of its parent */
424     while ((parent = rb_parent(node)) && node == parent->rb_left)
425         node = parent;
426 
427     return parent;
428 }
429 
430 void rb_replace_node(struct rb_node *victim, struct rb_node *new,
431              struct rb_root *root)
432 {
433     struct rb_node *parent = rb_parent(victim);
434 
435     /* Set the surrounding nodes to point to the replacement */
436     if (parent) {
437         if (victim == parent->rb_left)
438             parent->rb_left = new;
439         else
440             parent->rb_right = new;
441     } else {
442         root->rb_node = new;
443     }
444     if (victim->rb_left)
445         rb_set_parent(victim->rb_left, new);
446     if (victim->rb_right)
447         rb_set_parent(victim->rb_right, new);
448 
449     /* Copy the pointers/colour from the victim to the replacement */
450     *new = *victim;
451 }
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

### 1.3红黑树的测试文件(test.c)

![img](https://images.cnblogs.com/OutliningIndicators/ExpandedBlockStart.gif)

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
  1 /**
  2  * 根据Linux Kernel定义的红黑树(Red Black Tree)
  3  *
  4  * @author skywang
  5  * @date 2013/11/18
  6  */
  7 
  8 #include <stdio.h>
  9 #include <stdlib.h>
 10 #include "rbtree.h"
 11 
 12 #define CHECK_INSERT 0    // "插入"动作的检测开关(0，关闭；1，打开)
 13 #define CHECK_DELETE 0    // "删除"动作的检测开关(0，关闭；1，打开)
 14 #define LENGTH(a) ( (sizeof(a)) / (sizeof(a[0])) )
 15 
 16 typedef int Type;
 17 
 18 struct my_node {
 19     struct rb_node rb_node;    // 红黑树节点
 20     Type key;                // 键值
 21     // ... 用户自定义的数据
 22 };
 23 
 24 /*
 25  * 查找"红黑树"中键值为key的节点。没找到的话，返回NULL。
 26  */
 27 struct my_node *my_search(struct rb_root *root, Type key)
 28 {
 29     struct rb_node *rbnode = root->rb_node;
 30 
 31     while (rbnode!=NULL)
 32     {
 33         struct my_node *mynode = container_of(rbnode, struct my_node, rb_node);
 34 
 35         if (key < mynode->key)
 36             rbnode = rbnode->rb_left;
 37         else if (key > mynode->key)
 38             rbnode = rbnode->rb_right;
 39         else
 40             return mynode;
 41     }
 42     
 43     return NULL;
 44 }
 45 
 46 /*
 47  * 将key插入到红黑树中。插入成功，返回0；失败返回-1。
 48  */
 49 int my_insert(struct rb_root *root, Type key)
 50 {
 51     struct my_node *mynode; // 新建结点
 52     struct rb_node **tmp = &(root->rb_node), *parent = NULL;
 53 
 54     /* Figure out where to put new node */
 55     while (*tmp)
 56     {
 57         struct my_node *my = container_of(*tmp, struct my_node, rb_node);
 58 
 59         parent = *tmp;
 60         if (key < my->key)
 61             tmp = &((*tmp)->rb_left);
 62         else if (key > my->key)
 63             tmp = &((*tmp)->rb_right);
 64         else
 65             return -1;
 66     }
 67 
 68     // 如果新建结点失败，则返回。
 69     if ((mynode=malloc(sizeof(struct my_node))) == NULL)
 70         return -1; 
 71     mynode->key = key;
 72 
 73     /* Add new node and rebalance tree. */
 74     rb_link_node(&mynode->rb_node, parent, tmp);
 75     rb_insert_color(&mynode->rb_node, root);
 76 
 77     return 0;
 78 }
 79 
 80 /* 
 81  * 删除键值为key的结点
 82  */
 83 void my_delete(struct rb_root *root, Type key)
 84 {
 85     struct my_node *mynode;
 86 
 87     // 在红黑树中查找key对应的节点mynode
 88     if ((mynode = my_search(root, key)) == NULL)
 89         return ;
 90 
 91     // 从红黑树中删除节点mynode
 92     rb_erase(&mynode->rb_node, root);
 93     free(mynode);
 94 }
 95 
 96 /*
 97  * 打印"红黑树"
 98  */
 99 static void print_rbtree(struct rb_node *tree, Type key, int direction)
100 {
101     if(tree != NULL)
102     {   
103         if(direction==0)    // tree是根节点
104             printf("%2d(B) is root\n", key);
105         else                // tree是分支节点
106             printf("%2d(%s) is %2d's %6s child\n", key, rb_is_black(tree)?"B":"R", key, direction==1?"right" : "left");
107 
108         if (tree->rb_left)
109             print_rbtree(tree->rb_left, rb_entry(tree->rb_left, struct my_node, rb_node)->key, -1);
110         if (tree->rb_right)
111             print_rbtree(tree->rb_right,rb_entry(tree->rb_right, struct my_node, rb_node)->key,  1); 
112     }   
113 }
114 
115 void my_print(struct rb_root *root)
116 {
117     if (root!=NULL && root->rb_node!=NULL)
118         print_rbtree(root->rb_node, rb_entry(root->rb_node, struct my_node, rb_node)->key,  0); 
119 }
120 
121 
122 void main()
123 {
124     int a[] = {10, 40, 30, 60, 90, 70, 20, 50, 80};
125     int i, ilen=LENGTH(a);
126     struct rb_root mytree = RB_ROOT;
127 
128     printf("== 原始数据: ");
129     for(i=0; i<ilen; i++)
130         printf("%d ", a[i]);
131     printf("\n");
132 
133     for (i=0; i < ilen; i++) 
134     {
135         my_insert(&mytree, a[i]);
136 #if CHECK_INSERT
137         printf("== 添加节点: %d\n", a[i]);
138         printf("== 树的详细信息: \n");
139         my_print(&mytree);
140         printf("\n");
141 #endif
142 
143     }
144 
145 #if CHECK_DELETE
146     for (i=0; i<ilen; i++) {
147         my_delete(&mytree, a[i]);
148 
149         printf("== 删除节点: %d\n", a[i]);
150         printf("== 树的详细信息: \n");
151         my_print(&mytree);
152         printf("\n");
153     }
154 #endif
155 }
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

rbtree.h和rbtree.c基本上是从Linux 3.0的Kernel中移植出来的。仅仅只添加了offestof和container_of两个宏，这两个宏在文章"[Linux内核中双向链表的经典实现](http://www.cnblogs.com/skywang12345/p/3562146.html)"中已经介绍过了，这里就不再重复说明了。 test.c中包含了两部分内容：一是，基于内核红黑树接口，自定义的一个结构体，并提供了相应的接口(添加、删除、搜索、打印)。二是，包含了相应的测试程序。

# 【NO.5】B树详解

## 1.B树的定义

​        B-树，即为B树。因为B树的原英文名称为B树，而国内很多人喜欢把B树译作B-树，其实，这是个非常不好的直译，很容易让人产生误解。如人们可能会以为B-树是一种树，而B树又是一种树。而事实上是，B-树就是指的B树。

一棵简单的三阶B树如下图：

![image-20221206215241957](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206215241957.png?lastModify=1670851010)

**一棵m阶B树，或为空树，或为满足下列特性对的m叉树。**

1. 树中每个结点最多含有m棵子树。
2. 若根结点不是叶子结点，则至少有2个子树。
3. 除根结点之外的所有非终端结点至少有⌈m/2⌉棵子树。
4. 如果一个结点有n-1个关键字，则该结点有n个分支，且这n-1个关键字按照递增顺序排列。
5. 每个非终端结点中包含信息：（N，A0，K1，A1，K2，A2，...，KN，一）其中：

- Ki（1≤i≤n）为关键字，且关键字按升序排序。
- 指针Ai（0≤i≤n）指向子树的根结点，Ai-1指向子树中所有结点的关键字均小于Ki，且大于Ki-1;
- 关键字的个数n必须满足：⌈m/2⌉-1≤n≤m-1。
- 结点内关键字各不相等且按从小到大排列。
- 结点结构如下：

![image-20221206215333545](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206215333545.png?lastModify=1670851010)

- 所有的叶子节点都在同一层，子叶结点不包含任何信息。叶子结点处于同一层，可以用空指针表示，是查找失败到达的位置。

注：平衡m叉查找树是指每个关键字的左侧子树与右侧子树的高度差的绝对值不超过1的查找树，其结点结构与上面提到的B-树结点结构相同，由此可见，**B-树是平衡m叉查找树，但限制更强，要求所有叶结点都在同一层。**

## 2.B树的存储结构

```
#define  m  3       // B树的阶，此设为3
typedef int KeyType;

typedef struct {
  KeyType  key;
  char     data;
} Record;

typedef struct BTNode {
  int             keynum;        // 结点中关键字个数，即结点的大小
  struct BTNode  *parent;        // 指向双亲结点
  KeyType         key[m+1];      // 关键字向量，0号单元未用
  struct BTNode  *ptr[m+1];      // 子树指针向量
  Record         *recptr[m+1];   // 记录指针向量，0号单元未用
} BTNode, *BTree;                // B树结点和B树的类型

typedef struct {
  BTree    pt;      // 指向找到的结点
  int      i;       // 1..m，在结点中的关键字序号
  int      tag;     // 1:查找成功，0:查找失败
} Result;           // 在B树的查找结果类型
```



## 3.B树的查找操作

B-树的查找很简单，是二叉排序树的扩展，二叉排序树是二路查找，B-树是多路查找，因为B-树结点内的关键字是有序的，在结点内进行查找时除了顺序查找外，还可以用折半查找来提升效率.B-树的具体查找步骤如下（假设查找的关键字为key）：

- 先让key与根结点中的关键字比较，如果key等于K [i]（K []为结点内的关键字数组），则查找成功。
- 若key<K [1]，则到P [0]所指示的子树中进行继续查找（对p[]为结点内的指针数组），这里要注意B-树中每个结点的内部结构。
- 若key> K [n]的，则到P [n]所指示的子树中继续查找。
- 若k [i] <key <k [i + 1]，则沿着指针p [i]所指示的子树继续查找。
- 如果最后遇到空指针，则证明查找不成功。

拿上面的B树举例，假如我们想要查找关键字61，下面即显示了查找的路径

![image-20221206215519632](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206215519632.png?lastModify=1670851010)

**实现代码**

- ```
  /**
   
   * 在B-树t查找关键字key，用r返回（pt, i, tag）
   * 若查找成功，则tag=1，指针pt所指结点中第i个关键字等于key
   * 否则tag=0，若要插入关键字key，应位于pt结点中第i-1和第i个关键字之间
     *
   * @param t B-树
  ```

- @param key 待查找的关键字

- @param r B-树的查找结果类型 */ void SearchBTree(BTree t, KeyType key, Result &r) {  int i = 0;  int found = 0;  BTree p = t;    // 一开始指向根结点，之后指向待查结点  BTree q = NULL; // 指向待查结点的双亲  while (p != NULL && found == 0) {      i = Search(p, key);      if (i <= p->keynum && p->key[i] == key) {          found = 1;      } else {          q = p;          p = p->ptr[i - 1]; // 指针下移      }  }  if (1 == found) {  // 查找成功，返回key的位置p和i      r.pt = p;      r.i = i;      r.tag = 1;  } else {           // 查找失败，返回key的插入位置q和i      r.pt = q;      r.i = i;      r.tag = 0;  } }

 /**

- 在p->key[1 .. p->keynum]找key，并返回位序 *
- @param p B-树的结点p
- @param key 关键字
- @return key在p结点的位序 */ int Search(BTree p, KeyType key) {  int i = 1;  while (i <= p->keynum && key > p->key[i]) {      i++;  }  return i; }

```
## 4.B树的插入操作

与二叉排序树一样，B-树的创建过程也是将关键字逐个插入到树中的过程。

乙树的插入首先利用了B树的查找操作查找关键字ķ的插入位置。若该关键字存在于B树中，则不用插入直接返回，否则查找操作必定失败于某个最底层的非终端结点上，也就是要插入的位置。插入分两种情况讨论。

1. 插入关键字后该结点的关键字数小于等于m - 1，插入操作结束。

2. 插入关键字后该结点的关键字数等于m，则应该进行分裂操作，分裂操作如下：

- 生成一个新结点，从中间位置把结点（不包括中间位置的关键字）分为两部分。
- 前半部分留在旧结点中，后半部分复制到新结点中。
- 中间位置的关键字连同新结点的存储位置插入到父结点中，如果插入后父结点的关键字个数也超过了m-1，则继续分裂。

在上图B树中插入30，可以得到如下的结果：

![image-20221206215636134](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221206215636134.png)

再插入26，得到下图结果，插入后的终端结点中的关键字数大于m-1，需对结点进行分裂

![image-20221206215647098](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221206215647098.png)

左部26，中间值为30，右部为37，左部放在原来的结点，右部放入新的结点，中间值则插入到父结点中，并且父结点会产生一个新的指针，指向新的结点的位置，如下图所示： 

![image-20221206215655495](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221206215655495.png)

继续插入新的关键字85，如图所示

![image-20221206215704404](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221206215704404.png)

需对刚才插入的结点进行分裂操作，操作方式和之前一样得到结果如下:(分裂完后发现原来结点的父结点也需要分裂）

![image-20221206215713124](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221206215713124.png)

再进行分裂操作后的结果如下：

![image-20221206215725688](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221206215725688.png)

**实现代码**

 * ```
 /**
    
     * 在B-树t中q结点的key[i - 1]和key[i]之间插入关键字key
     * 若插入后结点关键字个数等于B-树的阶，则沿双亲指针链进行结点分裂
       *
     * @param t B-树
     * @param key 待插入的关键字
     * @param q 关键字插入的结点
     * @param i 插入位序
       */
       void InsertBTree(BTree &t, KeyType key, BTree q, int i) {
       KeyType x;
       int s;
       int finished = FALSE;
       int needNewRoot = FALSE;
       BTree ap;
       if (NULL == q) {
           newRoot(t, NULL, key, NULL);
       } else {
           x = key;
           ap = NULL;
           while (FALSE == needNewRoot && FALSE == finished) {
               Insert(q, i, x, ap);           // x和ap分别插入到q->key[i]和q->ptr[i]
               if (q->keynum < m) {
                   finished = TRUE;
               } else {
                   s = (m + 1) / 2;          // 得到中间结点位置
                   split(q, s, ap);          // 分裂q结点
                   x = q->key[s];
                   // 在双亲位置插入关键字x
                   if (q->parent != NULL) {
                       q = q->parent;
                       i = Search(q, x);    // 寻找插入的位置
                   } else {
                       needNewRoot = TRUE;
                   }
               }
           }
           if (TRUE == needNewRoot) {
               newRoot(t, q, x, ap);
           }
       }
       }
    
    /**
    
     * 将q结点分裂成两个结点，前一半保留在原结点，后一半移入ap所指新结点
       *
     * @param q B-树结点
     * @param s 中间位序
     * @param ap 新结点，用来存放原结点的后一半关键字
       */
       void split(BTree &q, int s, BTree &ap) {
       int i, j;
       int n = q->keynum;           // 关键字数量
       ap = (BTree)malloc(sizeof(BTNode));
       ap->ptr[0] = q->ptr[s];
       for (i = s + 1, j = 1; i <= n; i++, j++) {
           ap->key[j] = q->key[i];
           ap->ptr[j] = q->ptr[i];
       }
       ap->keynum = n - s;
       ap->parent = q->parent;
       for (i = 0; i <= n - s; i++) {
           // 修改新结点的子结点的parent域
           if (ap->ptr[i] != NULL) {
               ap->ptr[i]->parent = ap;
           }
       }
       q->keynum = s - 1;         // 修改q结点的关键字数量
       }
    
    /**
    
     * 生成新的根结点
       *
     * @param t B-树
     * @param p B-树结点
     * @param key 关键字
     * @param ap B-树结点
       */
       void newRoot(BTree &t, BTree p, KeyType key, BTree ap) {
       t = (BTree)malloc(sizeof(BTNode));
       t->keynum = 1;
       t->ptr[0] = p;
       t->ptr[1] = ap;
       t->key[1] = key;
       if (p != NULL) {
           p->parent = t;
       }
       if (ap != NULL) {
           ap->parent = t;
       }
       t->parent = NULL;
       }
    
    /**
    
     * 关键字key和新结点指针ap分别插入到q->key[i]和q->ptr[i]
       *
     * @param q 插入目标结点
     * @param i 插入位序
     * @param key 待插入的关键字
     * @param ap 新结点指针
       */
       void Insert(BTree &q, int i, KeyType key, BTree ap) {
       int j;
       int n = q->keynum;
       for (j = n; j >= i; j--) {
           q->key[j + 1] = q->key[j];          // 后移
           q->ptr[j + 1] = q->ptr[j];          // 后移
       }
       q->key[i] = key;
       q->ptr[i] = ap;
       if (ap != NULL) {
           ap->parent = q;
       }
       q->keynum++;
       }
## 五，B树的删除操作

      对于B-树关键字的删除，需要找到待删除的关键字，在结点中删除关键字的过程也有可能破坏B-树的特性，如旧关键字的删除可能使得结点中关键字的个数少于规定个数，的英文这可能需要向其兄弟结点借关键字或者其状语从句：结孩子点进行关键字的交换，可能也。需要进行结点的合并，其中，和当前结点的孩子进行关键字交换的操作可以保证删除操作总是发生在终端结点上。
```

**该结点为最下层非终端结点**

- 如果被删关键字所在结点的原关键字个数n≥⌈m/2⌉，则删去该关键字后结点仍满足B树的定义。

- 如果被删关键字所在结点的关键字个数n等于⌈m/2⌉-1，则删除该关键字后该结点将不满足B树的定义，需要调整：如果其左右兄弟结点中有富余的关键字，即与该结点相邻的右（或左）兄弟结点中的关键字数目大于⌈m/2⌉-1，则可将右（或左）兄弟结点中最小（大）关键字上移至双亲结点。而将双亲结点中小（大）于该上移关键字的关键字下移至被删关键字所在结点中。

- 如果左右兄弟结点都没有多余的关键字，则需要把要删除关键字的结点与其左（或右）兄弟结点以及双亲结点中分割两者的关键字合并成一个结点，即在删除关键字后，该结点中剩余的关键字和指针，加上双亲结点中的关键字的Ki一起，合并到Ai-1或（Ai）结点，即删除该关键字结点的左（右）兄弟结点。如果导致双亲结点中关键字个数小于⌈m/2⌉-1，则对此双亲结点做同样处理。如果知道根结点也做了合并，则整棵树减少一层。

  **该结点不是最下层非终端结点**

1.假设被删关键字为该结点中第我个关键字的Ki，则可从指针Ai所指的子树中找出位于最下层非终端结点的最小关键字替代Ki，并将其删除，即可转换为上面的情况进行操作。

下面给出删除叶子结点的几种情况：

第一种：关键字的数量不小于⌈m/2⌉，如下图删除关键字12

![image-20221206215910540](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206215910540.png?lastModify=1670851010)

删除12后的结果如下，只是简单的删除关键字和其对应的指针

![image-20221206220404743](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206220404743.png?lastModify=1670851010)

第二种：关键字个数等于⌈m/2⌉-1，而且该结点相邻的右兄弟（或左兄弟）结点中的关键字数目大于⌈m/2⌉-1

  在上图中删除50关键字

![image-20221206220412790](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206220412790.png?lastModify=1670851010)

我们需要把50的右兄弟中最小的关键字：61上移到其父结点，然后替换小于61的关键字53的位置，53则放至50的结点中然后，我们可以得到如下的结果： 

![image-20221206220422610](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206220422610.png?lastModify=1670851010)

第三种：关键字个数Ñ等于⌈m/2⌉-1，而且被删除关键字所在结点和其相邻的兄弟结点中的关键字数目均等于[M / 2] -1

![image-20221206220430339](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206220430339.png?lastModify=1670851010)

如上图我们删除53，需要将53外的关键字（空）和父亲结点的61关键字一起合并到70这个关键字的结点中，结果如下

![image-20221206220442171](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206220442171.png?lastModify=1670851010)

第四种：如下图所示

![image-20221206220450387](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206220450387.png?lastModify=1670851010)

删除关键字4，4在终端结点上，但是此时4所在的结点的关键字个数已经到下限，需要借关键字，可以看到其左右兄弟结点的关键字个数也不够借，因此需要进行关键字的合并，合并的方法有两种，如下图所示

![image-20221206220501794](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206220501794.png?lastModify=1670851010)

![image-20221206220511144](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206220511144.png?lastModify=1670851010)

**实现代码**

- ```
  /**
   
   * 删除B-树上p结点的第i个关键字
     *
   * @param t B-树
   * @param p 目标关键字所在结点
   * @param i 关键字位序
     */
     void DeleteBTree(BTree &t, BTree &p, int i) {
     if (p->ptr[i] != NULL) {         // 不是最下层非终端结点
         Successor(p, i);             // 找到后继最下层非终端结点的最小关键字代替它
         DeleteBTree(t, p, 1);        // 删除最下层非终端结点中的最小关键字
     } else {
         Remove(p, i);                // 从结点p中删除key[i]
         if (p->keynum < (m-1) / 2) {
             Restore(t, p);           // 调整B树
         }
     }
     }
  
  /**
  
   * 在Ai子树中找出最下层非终端结点的最小关键字代替Ki
     *
   * @param p B-树结点
   * @param i 关键字位序
     */
     void Successor(BTree &p, int i) {
     BTree leaf = p;
     if (NULL == p) {
         return;
     }
     leaf = leaf->ptr[i];             // 指向子树
     while (NULL != leaf->ptr[0]) {
         // 找到最下层非终端结点
         leaf = leaf->ptr[0];
     }
     p->key[i] = leaf->key[1];
     p = leaf;
     }
  
  /**
  
   * 从结点p移除关键字key[i]
     *
   * @param p B-树结点
   * @param i 关键字位序
     */
     void Remove(BTree &p, int i) {
     int k;
     // 指针与key都向左移
     for (k = i; k < p->keynum; k++) {
      p->key[k] = p->key[k + 1];
      p->ptr[k] = p->ptr[k + 1];
  }
  p->keynum--;
  }
  ```

 /**

- 调整B-树 *

- @param t B-树

- @param p B-树结点 */ void Restore(BTree &t, BTree &p) { BTree parent, leftBrother, rightBrother; // 被删结点的父结点、左右兄弟 parent = p->parent; if (parent != NULL) { // 父结点不为空    // 寻找左右兄弟    int i;    for (i = 0; i <= parent->keynum; i++) {        if (parent->ptr[i] == p) {            break;        }    }    if (i > 0) {        leftBrother = parent->ptr[i - 1];    } else {        leftBrother = NULL;    }    if (i < parent->keynum) {        rightBrother = parent->ptr[i + 1];    } else {        rightBrother = NULL;    }

  ```
  // 左兄弟或右兄弟有富余关键字
  if ((leftBrother != NULL && leftBrother->keynum >= (m + 1) / 2) ||
      (rightBrother != NULL && rightBrother->keynum >= (m + 1) / 2)) {
      BorrowFromBrother(p, leftBrother, rightBrother, parent, i);
  } else {
      // 左右兄弟都没富余关键字，需要合并
      if (leftBrother != NULL) {
          MegerWithLeftBrother(leftBrother, parent, p, t, i); // 与左兄弟合并
      } else if (rightBrother != NULL) {
          MegerWithRightBrother(rightBrother, parent, p, t, i);
      } else  {
          //当左右子树不存在时改变根结点
          for (int j = 0; j <= p->keynum + 1; j++) {
              if (p->ptr[j] != NULL) {
                  t = p->ptr[j];
                  break;
              }
          }
          t->parent = NULL;
      }
  }
  ```

  } else {    //根节点，去掉根节点，使树减一层    BTree a;    for (int j = 0; j <= p->keynum + 1; j++) {        if (p->ptr[j] != NULL) {            a = p;            p = p->ptr[j];            a->ptr[j] = NULL;            free(a);            break;        }    }    t = p;    t->parent = NULL; } }

 /**

- 向兄弟借关键字 *
- @param p B-树结点
- @param leftBrother p结点的左兄弟结点
- @param rightBrother p结点的右兄弟结点
- @param parent p结点的父亲结点
- @param i 位序 */ void BorrowFromBrother(BTree &p, BTree &leftBrother, BTree &rightBrother, BTree &parent, int &i) { // 左兄弟有富余关键字，向左兄弟借 if (leftBrother != NULL && leftBrother->keynum >= (m + 1) / 2) {    for (int j = p->keynum + 1; j > 0; j--) {        // 关键字与指针后移，腾出第一个位置        if (j > 1) {            p->key[j] = p->key[j - 1];        }        p->ptr[j] = p->ptr[j - 1];    }    p->ptr[0] = leftBrother->ptr[leftBrother->keynum];    if (p->ptr[0] != NULL) {        p->ptr[0]->parent = p;    }    leftBrother->ptr[leftBrother->keynum] = NULL;    p->key[1] = parent->key[i]; // 被删结点存父结点关键字    parent->key[i] = leftBrother->key[leftBrother->keynum]; // 父结点的key变为被删结点左兄弟的最大关键字    leftBrother->keynum--;    p->keynum++; } else if (rightBrother != NULL && rightBrother->keynum >= (m + 1) / 2) { // 右兄弟有富余关键字    p->key[p->keynum + 1] = parent->key[i + 1];    p->ptr[p->keynum + 1] = rightBrother->ptr[0];     // 子树指针指向右兄弟最左边的子树指针    if (p->ptr[p->keynum + 1] != NULL) {        p->ptr[p->keynum + 1]->parent = p;    }    p->keynum++;    parent->key[i + 1] = rightBrother->key[1];        // 父结点从右兄弟借关键字    for (int j = 0; j < rightBrother->keynum; j++) {        if (j > 0) {            rightBrother->key[j] = rightBrother->key[j + 1];        }        rightBrother->ptr[j] = rightBrother->ptr[j + 1];    }    rightBrother->ptr[rightBrother->keynum] = NULL;    rightBrother->keynum--; } }

 /**

- 与左兄弟合并 *
- @param leftBrother p结点的左兄弟结点
- @param parent p结点的父亲结点
- @param p B-树结点
- @param t B-树
- @param i 位序 */ void MegerWithLeftBrother(BTree &leftBrother, BTree &parent, BTree &p, BTree &t, int &i) { // 与左兄弟合并 leftBrother->key[leftBrother->keynum + 1] = parent->key[i];    // 从父结点拿下分割本节点与左兄弟的关键字 leftBrother->ptr[leftBrother->keynum + 1] = p->ptr[0]; if (leftBrother->ptr[leftBrother->keynum + 1] != NULL) {    leftBrother->ptr[leftBrother->keynum + 1]->parent = leftBrother;    // 给左兄弟的结点，当此结点存在时需要把其父亲指向指向左结点 } leftBrother->keynum++; //左兄弟关键数加1 for (int j = 1; j <= p->keynum; j++) {    // 把本结点的关键字和子树指针赋给左兄弟    leftBrother->key[leftBrother->keynum + j] = p->key[j];    leftBrother->ptr[leftBrother->keynum + j] = p->ptr[j];    if (leftBrother->ptr[leftBrother->keynum + j] != NULL) {        leftBrother->ptr[leftBrother->keynum + j]->parent = leftBrother;    } } leftBrother->keynum += p->keynum; parent->ptr[i] = NULL; free(p);    // 释放p结点 for (int j = i;j < parent->keynum; j++) {    // 左移    parent->key[j] = parent->key[j + 1];    parent->ptr[j] = parent->ptr[j + 1]; } parent->ptr[parent->keynum] = NULL; parent->keynum--;        // 父结点关键字个数减1 if (t == parent) {    // 如果此时父结点为根，则当父结点没有关键字时才调整    if (0 == parent->keynum) {        for (int j = 0;j <= parent->keynum + 1; j++) {            if (parent->ptr[j] != NULL) {                t = parent->ptr[j];                break;            }            t->parent = NULL;        }    } } else {    // 如果父结点不为根，则需要判断是否需要重新调整    if (parent->keynum < (m - 1) / 2) {        Restore(t, parent);    } } } /**
- 与右兄弟合并 *
- @param rightBrother p结点的右兄弟结点
- @param parent p结点的父亲结点
- @param p B-树结点
- @param t B-树
- @param i 位序 */ void MegerWithRightBrother(BTree &rightBrother, BTree &parent, BTree &p, BTree &t, int &i) { // 与右兄弟合并 for (int j = (rightBrother->keynum); j > 0; j--) {    if (j > 0) {        rightBrother->key[j + 1 + p->keynum] = rightBrother->key[j];    }    rightBrother->ptr[j + 1 + p->keynum] = rightBrother->ptr[j]; } rightBrother->key[p->keynum + 1] = parent->key[i + 1];    // 把父结点的分割两个本兄弟和右兄弟的关键字拿下来使用 for (int j = 0; j <= p->keynum; j++) {    // 把本结点的关键字及子树指针移动右兄弟中去    if (j > 0) {        rightBrother->key[j] = p->key[j];    }    rightBrother->ptr[j] = p->ptr[j];    if (rightBrother->ptr[j] != NULL) {        rightBrother->ptr[j]->parent = rightBrother;    // 给右兄弟的结点需要把其父结点指向右兄弟    } } rightBrother->keynum += (p->keynum + 1); parent->ptr[i] = NULL; free(p); // 释放p结点 for (int j = i;j < parent->keynum;j++) {    if (j > i) {        parent->key[j] = parent->key[j + 1];    }    parent->ptr[j] = parent->ptr[j + 1]; } if (1 == parent->keynum) {    // 如果父结点在关键字减少之前只有一个结点，那么需要把父结点的右孩子赋值给左孩子    parent->ptr[0] = parent->ptr[1]; } parent->ptr[parent->keynum] = NULL; parent->keynum--;                    // 父结点关键字数减1 if (t == parent) {    //如果此时父结点为根，则当父结点没有关键字时才调整    if (0 == parent->keynum) {        for (int j = 0; j <= parent->keynum + 1; j++) {            if (parent->ptr[j] != NULL) {                t = parent->ptr[j];                break;            }        }        t->parent = NULL;    } } else {    //如果父结点不为根，则需要判断是否需要重新调整    if (parent->keynum < (m - 1) / 2) {        Restore(t, parent);    } } }

```
————————————————
版权声明：本文为CSDN博主「_Kim」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/weixin_40862011/article/details/85047834

# ---------------------------------------

# 深入理解什么是B树？

## 1.前言

前面的文章，我们已经介绍过其他的几种高级的动态数据结构，典型如红黑树，跳跃表等，今天我们再来学习另外一种高级数据结构B树，我们知道树的查询时间复杂度和其树的高度有直接关系，当我们向红黑树里面插入大量的数据时，有两个问题：

（1）首先，内存是有限的不可能无止境的一直插入数据，而基于BST的平衡树如AVL树和红黑树，或者我们上一节学习的跳跃表本质上都是基于内存的数据结构。

（2）如果将AVL树，红黑树或者跳跃表直接存储到磁盘上，然后用来检索可以吗？ 答案是可以的，但问题是基于磁盘寻道查询比基于内存慢太多了，举个例子，假设现在有100万数据分布在红黑树里面，按照红黑树的平均查找性能O（logN）来计算，在N=100万时，检索任意数据平均需要20次查询，如果是在内存完全没问题，但此时红黑树是如果存在磁盘上，那就完全不一样了，按照普通硬盘或者文件系统，每次查询IO一次总耗时约10毫秒计算，那么在百万级别的数据量下，检索一次数据就需要10*20，大概就是0.2秒，假如现在数据量扩大100倍，数据总量是一亿，那么检索一次数据就需要20秒，这个性能是完全不能接受的，可以想象在这种情况下，使用二叉树是不合适的。

正是由于描述的问题，所以迫切需要一种面向磁盘或者文件系统友好的数据结构，而它就是B树，或者基于B树的扩展B+，B*树等。上面问题的根源在于树的高度太高，导致查询外存，也就是访问磁盘IO次数太多，从而大大拖慢了检索性能，那么思路就清晰了，只要想法子降低树的高度就可以了，没错B树就是这么一种m叉的多路平衡树，你可以想象它的孩子节点少则2个，多则数千个，所以就从二叉树的廋高，变成了多叉树的矮胖，正是这样才大大降低了树的高度，从而使得这种数据结构更适合存储在磁盘上，并且充分了利用了磁盘的块的预读机制（局部性访问原理），通过缓存的方式进一步提升性能，磁盘在读取某一个文件指针的时候，通常会把紧挨着的数据，也全部读到缓存，因为实践证明，当一个文件数据被访问的时候，它周边的数据很快也会被访问，这里简单说下磁盘扇区，磁盘块，磁盘也的区别：

扇区：磁盘的最小存储单位；

磁盘块：文件系统读写数据的最小单位；

页：内存的最小存储单位；

磁盘块和页的大小一般情况下为4KB，所以在B树中一个节点的最大存储数据个数的总大小一般不能超过4KB。

基于B树的结构充分利用了这一点，从而非常适合做文件系统或者[数据库](https://cloud.tencent.com/solution/database?from=10680)的索引。

## 2.B树的性质

假如当前有一颗m阶的B树（注意阶的意思是指每个节点的孩子节点的个数），那么其符合：

（1）每个节点最多有m个子节点

（2）除了根节点和叶子节点之外，其他的每个节点最少有m/2（向上取整）个孩子节点

（3）根节点至少有两个孩子节点，（除了第一次插入的时候，此时只有一个节点，根节点同时是叶子节点）

（4）所有的叶子节点都在同一层

（5）有k个子节点的父节点包含k-1个关键码

除了上面B树的性质外，B树还有几个特点：

1，树高平衡，所有的叶节点都在同一层

2，关键码没有重复，父节点中的关键码是其子节点的分解

3，B树把值接近的相关记录放在同一个磁盘页中，从而利用了访问的局部性原理。

4，B树保证树种至少有一部分比例的节点是满的。为什么这样说，在上面的性质2中，我们知道每个节点最少可以有 m/2个节点，注意这刚好是一半，没有太满，是因为可以给后续的添加，删除留有余地，这样以来节点不会频繁的触发不平衡，没有太空则意味着B树能够 保证降低树的高度。

如下图的一个2-3树的示例

![img](https://ask.qcloudimg.com/http-save/yehe-1903727/nb97ljn5cm.jpeg?imageView2/2/w/1620)

这个例子里面m=3，那么每个节点的孩子节点个数最多是3个节点，最少是2个节点（3/2向上取整），所以关键码的范围就是1-2，这一点需要注意，这是限制B树平衡的一个条件之一，另外一个是所有的叶节点都等高。注意在实际实现过程中，B树的每个关键码都会有两个指针，一个指针指向该关键码本身的记录在磁盘上的偏移量，另一个指针指向其子树的节点在磁盘上的位置，而B+树里面则没有存储该关键码本身的记录在磁盘上的偏移量，所以理论上来说同样的数量，B+树的节点能够比B树存储更多的关键码。

![img](https://ask.qcloudimg.com/http-save/yehe-1903727/kvzo2fhrhz.jpeg?imageView2/2/w/1620)

从上面我们看出来一个特点，包含i个关键码的节点，它的子节点个数（也叫子树）有i+1个，注意这里存的是指针，此外关键码的值规律是： k1 < k2 k3

## 3.B树的操作

### 3.1查询

B树的查询与二叉树的搜索基本类似，在高度为h的B树，最多的查询次数就该树的高度，B树查询分为交替执行的两步过程：

1. 把根节点从磁盘中读出来，在根节点所包含的关键码中进行查询给定的关键码值，这里注意如果关键码不多时，就用顺序检索，如果关键码数组数量较大时，可以采用二分法查询，如果找到则检索成功。否则走第二步。
2. 确定要查的关键码值是在某个ki和ki+1之间，然后取ki所指向的节点继续查找，如果最终仍然没有找到，就返回失败，成功则返回要检索的值。

### 3.2插入

（插入案例一） 插入找位置的过程与搜索类似，默认我们认为不支持存储重复的关键码，插入分几种情况，如下图所示，在这样一个2-3树（最大阶=3，最小阶=2，最大关键码=2，最小关键码是1）里面插入14，如下图： 

![img](https://ask.qcloudimg.com/http-save/yehe-1903727/pzk2kiznsn.jpeg?imageView2/2/w/1620)

我们可以直接将这个节点放在15左侧即可，其他不需要做任何改动，这就是前面我们说的，B树的节点默认只会使用50%的存储个数，这样不至于太满导致每插入一次就得维持平衡，同时也不至于太空，能够有效的降级树的高度。比如上的14插入后，插入节点没有违背B树的各个性质，所以本次效率比较高。

（插入案例二）

如下图，插入55，就破坏了B树的性质，2-3树的关键码只能是1或者2，但插入55后，就变成3了，这种情况解决方案是，对插入后序列取中位数，然后上升到父节点即可： 插入前： 

![img](https://ask.qcloudimg.com/http-save/yehe-1903727/j7kvzqv1s5.jpeg?imageView2/2/w/1620)

插入后：

![img](https://ask.qcloudimg.com/http-save/yehe-1903727/p0jiz4ideb.jpeg?imageView2/2/w/1620)

（插入案例三）

多个节点连续分裂的情况，如上图此时如果再插入19，就会形成级联分裂3级，如下： 

![img](https://ask.qcloudimg.com/http-save/yehe-1903727/l5p3eofiqh.jpeg?imageView2/2/w/1620)

注意如果根节点再分裂，就会新生成一个根节点，从而最终导致树的高度增加一级，如下： 

![img](https://ask.qcloudimg.com/http-save/yehe-1903727/robeo4si0s.jpeg?imageView2/2/w/1620)

注意这里可以思考一个问题，为什么B树的根节点阶是2到m，而不是其他非叶子节点的m/2 到 m，这其实从分裂过程就能看出来，因为在根节点在关键码多于2的时候分裂，分裂到最终的情况下就是从3个关键码中分裂，这样必然会新提取一个中间节点做根节点，然后剩下的两个节点充当孩子节点，所以B树的根节点是特殊的，它的限制和其他非叶子节点是不一样的。

下面我们总结下B树插入的流程：

首先最重要的是要保持B树的性质，特别是等高和阶的限制。

（1）查找插入节点的位置，找到最底层插入。

（2）若新增一个关键码后导致溢出，则节点分裂，中间关键码连同新指针插入父节点

（3）若父节点也溢出，则继续分裂，回到步骤2

（4）分裂过程中可能会传达到根节点，从而导致树升高一层

注意从磁盘已经加载过数据，默认我们认为已经缓存了，当再次读取的时候不会再访问磁盘。

### 3.3删除

删除相比插入要复杂的多，与二叉树的删除类似，如果删除的关键码不在叶节点层，先把此关键码与它在B树里的后继节点调换位置（这个与二叉树类似，可以选择左子树最大，或者右子树最小），然后再删除该关键码。

如果删除的关键码在叶节点层，删除后关键码个数不小于m/2-1，因为小于m/2，则意味着下溢出，在前面的插入过程中，会造成上溢出，注意这两种情况。如果不小于m/2，则直接删除。

如果关键码的个数小于m/2，如果兄弟节点的关键码个数不等于m/2-1，则执行借操作，从兄弟节点中移出若干个关键码到该节点中来（父节点中的一个关键码要做相应变化）

如果兄弟节点的关键码个数等于m/2-1,那么就执行合并。

如下图： 

![img](https://ask.qcloudimg.com/http-save/yehe-1903727/yz9rovgzp9.jpeg?imageView2/2/w/1620)

首先6阶B树子节点数限制是3-6，所以关键码的限制是2-5，在上图中删除45会出现下溢出，即破坏了性质，所以要从兄弟节点看看能不能借数据保持性质，这里的兄弟节点一般都是紧邻的左右，不会再远，上图中上面的节点也就是左兄弟的关键码刚好是2，所以不能借，而右边的兄弟关键码是3，所以可以借一个，但借是有规则的，不能直接拿过来，需要把父节点的中间码给拉下来，重新选举一个父节点，并基于父节点做分裂。如下图： 

![img](https://ask.qcloudimg.com/http-save/yehe-1903727/0c16m2po0x.jpeg?imageView2/2/w/1620)

接着我们继续删除，会发现左右兄弟都不能再借了，如下图： 

![img](https://ask.qcloudimg.com/http-save/yehe-1903727/jpqg5jtdwl.jpeg?imageView2/2/w/1620)

 这个时候，要考虑合并了，可以是跟左也可以是跟右，但注意合并同样需要把父节点的关键码拉下来合并，最终结果如下： 

![img](https://ask.qcloudimg.com/http-save/yehe-1903727/xmkb0h2im4.jpeg?imageView2/2/w/1620)

下面我们总结下B树删除的流程：

首先最重要的还是保持性质：叶子等高和阶的上下界。

总得来说有4步：交换，删除，借关键码，合并

（1）保证所删除的位置在最底层（否则与后继关键码交换）

（2）若删除后，节点中关键码数目不够最低限（下溢出），则先看左右兄弟有无多于的关键码可借，若有则拉下父节点的分界码，与兄弟节点进行重新分割，与插入类似。

（3）若其左右兄弟中的关键码都处在下限处，则把节点和它的一个兄弟，以及父节点他两的分界关键码一起合并成一个节点，注意这个时候父节点下拉的关键码，就从父节点里面删除了。

（4）此合并过程如果传递到根节点，则树的高度下降一层。

## 4.B树， B+ 和 B*区别

B树相信大家已经熟悉了，还有两种B树的延伸分别是B+树和B树，类B树系的结构主要是面向磁盘文件系统数据结构，但B树的不足也显而易见，在B树里面所有的节点都存储数据，这样导致在做范围查询的时候，性能比较低，所以才出现了B+树，B+树里面每个节点存储的关键码个数相比B树存的更多，另外B+树非叶子节点不存储数据，只存储索引，索引B+树可以很好的支持范围查询，而B树与B+树主要的区别在于B树更加饱满，前面说过B+树的孩子节点个数是保持在50%的饱满程度，而B树是保持在75%的饱满程度，当然这种设计会导致算法更加复杂，有利也有弊，在实际应用中并不常见。由于B树的不足，所以B+树才是在面向文件系统索引里面最流行的数据结构。同样的对应B+树在内存里面的数据结构就是跳跃表，但跳跃表的虽然支持范围查询，但平均时间复杂度是O（logN），对于磁盘访问来说还是比较耗时的。

## 5.总结

本篇文章主要介绍了B树相关内容，B树是面向磁盘的索引结构，B+树是基于B树的扩展，更好的支持了范围检索，常应用在主流的数据库中如[MySQL](https://cloud.tencent.com/product/cdb?from=10680)，Oracle等，对B树的学习和理解是掌握数据库索引原理必须不少的基础，感兴趣的同学可以自己再去研究下。
```

原文链接：https://cloud.tencent.com/developer/article/1425604

# 【NO.6】B树、B-树、B+树、B*树之间的关系

## **1.B树**

   **[B-tree](https://baike.baidu.com/item/B-tree/6606402)树即[B树](https://baike.baidu.com/item/B树/5411672)**，B即Balanced，平衡的意思。因为B树的原英文名称为B-tree，而国内很多人喜欢把B-tree译作B-树，其实，这是个非常不好的直译，很容易让人产生[误解](https://baike.baidu.com/item/误解/8094198)。如[人们](https://baike.baidu.com/item/人们/876144)可能会以为B-树是一种树，而B树又是另一种树。而事实上是，**B-tree就是指的B树**。特此说明。

先介绍下[二叉搜索树](https://so.csdn.net/so/search?q=二叉搜索树&spm=1001.2101.3001.7020)

​    1.所有非[叶子结点](https://so.csdn.net/so/search?q=叶子结点&spm=1001.2101.3001.7020)至多拥有两个儿子（Left和Right）；

​    2.所有结点存储一个关键字；

​    3.非叶子结点的左指针指向小于其关键字的子树，右指针指向大于其关键字的子树；

​    如：

​    

![img](https://p-blog.csdn.net/images/p_blog_csdn_net/manesking/1.JPG)

​    二叉搜索树的搜索，从根结点开始，如果查询的关键字与结点的关键字相等，那么就命中；

否则，如果查询关键字比结点关键字小，就进入左儿子；如果比结点关键字大，就进入

右儿子；如果左儿子或右儿子的指针为空，则报告找不到相应的关键字；

​    如果二叉搜索树的所有非叶子结点的左右子树的结点数目均保持差不多（平衡），那么B树

的搜索性能逼近二分查找；但它比连续内存空间的二分查找的优点是，改变二叉搜索树结构

（插入与删除结点）不需要移动大段的内存数据，甚至通常是常数开销；

​    如：

   

![img](https://p-blog.csdn.net/images/p_blog_csdn_net/manesking/2.JPG)

  但二叉搜索树在经过多次插入与删除后，有可能导致不同的结构：

![img](https://p-blog.csdn.net/images/p_blog_csdn_net/manesking/31.JPG)

 

  右边也是一个二叉搜索树，但它的搜索性能已经是线性的了；同样的关键字集合有可能导致不同的

树结构索引；所以，使用二叉搜索树还要考虑尽可能让B树保持左图的结构，和避免右图的结构，也就

是所谓的“平衡”问题；   

​    实际使用的二叉搜索树都是在原二叉搜索树的基础上加上平衡算法，即“平衡二叉树”；如何保持B树

结点分布均匀的平衡算法是平衡二叉树的关键；平衡算法是一种在二叉搜索树中插入和删除结点的

策略；

B树（B-树）

​    是一种多路搜索树（并不是二叉的）：

​    1.定义任意非叶子结点最多只有M个儿子；且M>2；

​    2.根结点的儿子数为[2, M]；

​    3.除根结点以外的非叶子结点的儿子数为[M/2, M]；

​    4.每个结点存放至少M/2-1（取上整）和至多M-1个关键字；（至少2个关键字）

​    5.非叶子结点的关键字个数=指向儿子的指针个数-1；

​    6.非叶子结点的关键字：K[1], K[2], …, K[M-1]；且K[i] < K[i+1]；

​    7.非叶子结点的指针：P[1], P[2], …, P[M]；其中P[1]指向关键字小于K[1]的

子树，P[M]指向关键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树；

​    8.所有叶子结点位于同一层；

​    如：（M=3）

![img](https://p-blog.csdn.net/images/p_blog_csdn_net/manesking/4.JPG)

​    B-树的搜索，从根结点开始，对结点内的关键字（有序）序列进行二分查找，如果

命中则结束，否则进入查询关键字所属范围的儿子结点；重复，直到所对应的儿子指针为

空，或已经是叶子结点；

B-树的特性：

​    1.关键字集合分布在整颗树中；

​    2.任何一个关键字出现且只出现在一个结点中；

​    3.搜索有可能在非叶子结点结束；

​    4.其搜索性能等价于在关键字全集内做一次二分查找；

​    5.自动层次控制；

​    由于限制了除根结点以外的非叶子结点，至少含有M/2个儿子，确保了结点的至少

利用率，其最底搜索性能为：

![img](https://p-blog.csdn.net/images/p_blog_csdn_net/manesking/0.JPG)

  

​    其中，M为设定的非叶子结点最多子树个数，N为关键字总数；

​    所以B-树的性能总是等价于二分查找（与M值无关），也就没有B树平衡的问题；

​    由于M/2的限制，在插入结点时，如果结点已满，需要将结点分裂为两个各占

M/2的结点；删除结点时，需将两个不足M/2的兄弟结点合并；

 

 

## 2.**B+树**

​    B+树是B-树的变体，也是一种多路搜索树：

​    1.其定义基本与B-树同，除了：

​    2.非叶子结点的子树指针与关键字个数相同；

​    3.非叶子结点的子树指针P[i]，指向关键字值属于[K[i], K[i+1])的子树

（B-树是开区间）；

​    5.为所有叶子结点增加一个链指针；

​    6.所有关键字都在叶子结点出现；

​    如：（M=3）

![img](https://p-blog.csdn.net/images/p_blog_csdn_net/manesking/5.JPG)

  B+的搜索与B-树也基本相同，区别是B+树只有达到叶子结点才命中（B-树可以在

非叶子结点命中），其性能也等价于在关键字全集做一次二分查找；

​    B+的特性：

​    1.所有关键字都出现在叶子结点的链表中（稠密索引），且链表中的关键字恰好

是有序的；

​    2.不可能在非叶子结点命中；

​    3.非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储

（关键字）数据的数据层；

​    4.更适合文件索引系统；

 

## 3.**B\*树**

​    是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针；

![img](https://p-blog.csdn.net/images/p_blog_csdn_net/manesking/6.JPG)

  B*树定义了非叶子结点关键字个数至少为(2/3)*M，即块的最低使用率为2/3

（代替B+树的1/2）；

​    B+树的分裂：当一个结点满时，分配一个新的结点，并将原结点中1/2的数据

复制到新结点，最后在父结点中增加新结点的指针；B+树的分裂只影响原结点和父

结点，而不会影响兄弟结点，所以它不需要指向兄弟的指针；

​    B*树的分裂：当一个结点满时，如果它的下一个兄弟结点未满，那么将一部分

数据移到兄弟结点中，再在原结点插入关键字，最后修改父结点中兄弟结点的关键字

（因为兄弟结点的关键字范围改变了）；如果兄弟也满了，则在原结点与兄弟结点之

间增加新结点，并各复制1/3的数据到新结点，最后在父结点增加新结点的指针；

​    所以，B*树分配新结点的概率比B+树要低，空间使用率更高；

 

## **4.小结**

​    二叉搜索树：二叉树，每个结点只存储一个关键字，等于则命中，小于走左结点，大于

走右结点；

​    B（B-）树：多路搜索树，每个结点存储M/2到M个关键字，非叶子结点存储指向关键

字范围的子结点；

​    所有关键字在整颗树中出现，且只出现一次，非叶子结点可以命中；

​    B+树：在B-树基础上，为叶子结点增加链表指针，所有关键字都在叶子结点

中出现，非叶子结点作为叶子结点的索引；B+树总是到叶子结点才命中；

​    B*树：在B+树基础上，为非叶子结点也增加链表指针，将结点的最低利用率

从1/2提高到2/3；

原文链接：[https://blog.csdn.net/u013411246/article/details/81088914?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-81088914-blog-85047834.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-81088914-blog-85047834.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=1](https://blog.csdn.net/u013411246/article/details/81088914?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-81088914-blog-85047834.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~CTRLIST~Rate-1-81088914-blog-85047834.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=1)

# 【NO.7】什么是B+树？（详细图解）

## 1.简介

### 1.1一个m阶的B+树具有如下几个特征：

1. 有k个子树的中间节点包含有k个元素（B树中是k-1个元素），每个元素不保存数据，只用来索引，所有数据都保存在叶子节点。
2. 所有的叶子结点中包含了全部元素的信息，及指向含这些元素记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接。
3. 所有的中间节点元素都同时存在于子节点，在子节点元素中是最大（或最小）元素。

**首先来看看B+树的结构：**

![image-20221206223121478](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206223121478.png?lastModify=1670851010)

上面的这颗树中，得出结论：

- 根节点元素8是子节点2,5,8 的最大元素，也是叶子节点6,8 的最大元素；
- 根节点元素15是子节点11,15 的最大元素，也是叶子节点13,15 的最大元素；
- 根节点的最大元素也就是整个B+树的最大元素，以后无论插入删除多少元素，始终要保持最大的元素在根节点当中。
- 由于父节点的元素都出现在子节点中，因此所有的叶子节点包含了全部元素信息，并且每一个叶子节点都带有指向下一个节点的指针，形成了一个有序链表。



## 2.卫星数据

指的的是索引元素所指向的数据记录，比如数据库中的某一行，在B-树中，无论中间节点还是叶子节点都带有卫星数据。

**B-树中的卫星数据（Satellite Information）**：

![image-20221206223220726](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206223220726.png?lastModify=1670851010)

**B+树中的卫星数据（Satellite Information）：**

只有叶子节点带有卫星数据，其余中间节点仅仅是索引，没有任何的数据关联。

![image-20221206223307007](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206223307007.png?lastModify=1670851010)

需要补充的是，在数据库的聚集索引（Clustered Index）中，叶子节点直接包含卫星数据。在非聚集索引（NonClustered Index）中，叶子节点带有指向卫星数据的指针。

## 3.B+ 树的查询

下面分别通过单行查询和范围查询来做分析：

### 3.1 单行查询：

比如我们要查找的元素是3：

B+树会自顶向下逐层查找节点，最终找到匹配的叶子节点。

第一次磁盘IO：

![image-20221206223403066](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206223403066.png?lastModify=1670851010)

第二次磁盘IO：

![image-20221206223410321](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206223410321.png?lastModify=1670851010)

第三次磁盘IO：

![image-20221206223418343](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206223418343.png?lastModify=1670851010)

### 3.2 B+树查询与B-数据对比：

- B+树的中间节点没有卫星数据，所以同样大小的磁盘可以容纳更多的节点元素；
- 在数据量相同的情况下，B+树的结构比B-树更加“矮胖”，因此查询的IO次数也更少；
- B+树的查询必须最终查找到叶子节点，而B-树只要查找匹配的元素即可，无论匹配的元素处于中间节点还是叶子节点。
- B-树的查找性能并不稳定（最好情况只查询根节点，最坏情况是查找叶子节点）。而B+树的每一次查找都是稳定的。

**范围查询：** 比如我们要查询3到11的范围数据：

**B-树的范围查询过程：** 自顶向下，查找到范围的下限（3）：

![image-20221206224008082](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206224008082.png?lastModify=1670851010)

中序遍历到元素6：

![image-20221206224018546](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206224018546.png?lastModify=1670851010)

中序遍历到元素8：

![image-20221206224028847](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206224028847.png?lastModify=1670851010)

中序遍历到元素9：

![image-20221206224037348](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206224037348.png?lastModify=1670851010)

中序遍历到元素11，遍历结束：

![image-20221206224045156](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206224045156.png?lastModify=1670851010)

**B+树的范围查找过程：** 自顶向下，查找到范围的下限（3）：

![image-20221206224057364](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206224057364.png?lastModify=1670851010)

通过链表指针，遍历到元素6, 8：

![image-20221206224111959](file://C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20221206224111959.png?lastModify=1670851010)



## 4.总结

B+树比B-树的优势三个：

单一节点存储更多的元素，使得查询的IO次数更少。 所有查询都要查找到叶子节点，查询性能稳定。 所有叶子节点形成有序链表，便于范围查询。 ———————————————— 版权声明：本文为CSDN博主「初念初恋」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/jiang_wang01/article/details/113739230

# 【NO.8】b+树详解

## 1.B树 与 B+树

我们今天要介绍的是工作开发中最常接触到的 InnoDB 存储引擎中的 B+ 树索引。要介绍 B+ 树索引，就不得不提[二叉查找树](https://so.csdn.net/so/search?q=二叉查找树&spm=1001.2101.3001.7020)，平衡二叉树和 B 树这三种数据结构。B+ 树就是从他们仨演化来的。

## 2.二叉查找树

首先，让我们先看一张图：

[![1](https://img-blog.csdnimg.cn/img_convert/48ddc31b33244e5403255e073879329c.png)](http://www.liuzk.com/wp-content/uploads/2019/11/1.jpg)

从图中可以看到，我们为 user 表（用户信息表）建立了一个二叉查找树的索引。

图中的圆为二叉查找树的节点，节点中存储了键（key）和数据（data）。键对应 user 表中的 id，数据对应 user 表中的行数据。

二叉查找树的特点就是任何节点的左子节点的键值都小于当前节点的键值，右子节点的键值都大于当前节点的键值。顶端的节点我们称为根节点，没有子节点的节点我们称之为叶节点。

如果我们需要查找 id=12 的用户信息，利用我们创建的二叉查找树索引，查找流程如下：

- 将根节点作为当前节点，把 12 与当前节点的键值 10 比较，12 大于 10，接下来我们把当前节点>的右子节点作为当前节点。
- 继续把 12 和当前节点的键值 13 比较，发现 12 小于 13，把当前节点的左子节点作为当前节点。
- 把 12 和当前节点的键值 12 对比，12 等于 12，满足条件，我们从当前节点中取出 data，即 id=12，name=xm。

利用二叉查找树我们只需要 3 次即可找到匹配的数据。如果在表中一条条的查找的话，我们需要 6 次才能找到。

## 3.平衡二叉树

上面我们讲解了利用二叉查找树可以快速的找到数据。但是，如果上面的二叉查找树是这样的构造：

![2](https://img-blog.csdnimg.cn/img_convert/845902799584cdaeab4860fede353710.png)

这个时候可以看到我们的二叉查找树变成了一个链表。如果我们需要查找 id=17 的用户信息，我们需要查找 7 次，也就相当于全表扫描了。

导致这个现象的原因其实是二叉查找树变得不平衡了，也就是高度太高了，从而导致查找效率的不稳定。

为了解决这个问题，我们需要保证二叉查找树一直保持平衡，就需要用到平衡二叉树了。

平衡二叉树又称 AVL 树，在满足二叉查找树特性的基础上，要求每个节点的左右子树的高度差不能超过 1。

下面是平衡二叉树和非平衡二叉树的对比：

![3](https://img-blog.csdnimg.cn/img_convert/9ffbed4615dd2a63031f4c6dc31942ef.png)

由平衡二叉树的构造我们可以发现第一张图中的二叉树其实就是一棵平衡二叉树。

平衡二叉树保证了树的构造是平衡的，当我们插入或删除数据导致不满足平衡二叉树不平衡时，平衡二叉树会进行调整树上的节点来保持平衡。具体的调整方式这里就不介绍了。

平衡二叉树相比于二叉查找树来说，查找效率更稳定，总体的查找速度也更快。

## 4.B 树

因为内存的易失性。一般情况下，我们都会选择将 user 表中的数据和索引存储在磁盘这种外围设备中。

但是和内存相比，从磁盘中读取数据的速度会慢上百倍千倍甚至万倍，所以，我们应当尽量减少从磁盘中读取数据的次数。

另外，从磁盘中读取数据时，都是按照磁盘块来读取的，并不是一条一条的读。

如果我们能把尽量多的数据放进磁盘块中，那一次磁盘读取操作就会读取更多数据，那我们查找数据的时间也会大幅度降低。

如果我们用树这种[数据结构](https://so.csdn.net/so/search?q=数据结构&spm=1001.2101.3001.7020)作为索引的数据结构，那我们每查找一次数据就需要从磁盘中读取一个节点，也就是我们说的一个磁盘块。

我们都知道平衡二叉树可是每个节点只存储一个键值和数据的。那说明什么？说明每个磁盘块仅仅存储一个键值和数据！那如果我们要存储海量的数据呢？

可以想象到二叉树的节点将会非常多，高度也会极其高，我们查找数据时也会进行很多次磁盘 IO，我们查找数据的效率将会极低！

![4](https://img-blog.csdnimg.cn/img_convert/7eaefd0ae8caffe30aa0f264b7f5e63d.png)

为了解决平衡二叉树的这个弊端，我们应该寻找一种单个节点可以存储多个键值和数据的平衡树。也就是我们接下来要说的 B 树。

B 树（Balance Tree）即为平衡树的意思，下图即是一棵 B 树：

![5](https://img-blog.csdnimg.cn/img_convert/ece9c1a3c7afee84d1fab445179faa63.png)

图中的 p 节点为指向子节点的指针，二叉查找树和平衡二叉树其实也有，因为图的美观性，被省略了。

图中的每个节点称为页，页就是我们上面说的磁盘块，在 MySQL 中数据读取的基本单位都是页，所以我们这里叫做页更符合 MySQL 中索引的底层数据结构。

从上图可以看出，B 树相对于平衡二叉树，每个节点存储了更多的键值（key）和数据（data），并且每个节点拥有更多的子节点，子节点的个数一般称为阶，上述图中的 B 树为 3 阶 B 树，高度也会很低。

基于这个特性，B 树查找数据读取磁盘的次数将会很少，数据的查找效率也会比平衡二叉树高很多。

假如我们要查找 id=28 的用户信息，那么我们在上图 B 树中查找的流程如下：

- 先找到根节点也就是页 1，判断 28 在键值 17 和 35 之间，那么我们根据页 1 中的指针 p2 找到页 3。
- 将 28 和页 3 中的键值相比较，28 在 26 和 30 之间，我们根据页 3 中的指针 p2 找到页 8。
- 将 28 和页 8 中的键值相比较，发现有匹配的键值 28，键值 28 对应的用户信息为（28，bv）。

## 5.B+ 树

B+ 树是对 B 树的进一步优化。让我们先来看下 B+ 树的结构图：

![6](https://img-blog.csdnimg.cn/img_convert/9ae2d6fb0ec7b2fb87a7c428e266acad.png)

根据上图我们来看下 B+ 树和 B 树有什么不同：

①B+ 树非叶子节点上是不存储数据的，仅存储键值，而 B 树节点中不仅存储键值，也会存储数据。

之所以这么做是因为在数据库中页的大小是固定的，InnoDB 中页的默认大小是 16KB。

如果不存储数据，那么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就会更矮更胖，如此一来我们查找数据进行磁盘的 IO 次数又会再次减少，数据查询的效率也会更快。

另外，B+ 树的阶数是等于键值的数量的，如果我们的 B+ 树一个节点可以存储 1000 个键值，那么 3 层 B+ 树可以存储 1000×1000×1000=10 亿个数据。

一般根节点是常驻内存的，所以一般我们查找 10 亿数据，只需要 2 次磁盘 IO。

②因为 B+ 树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的。

那么 B+ 树使得范围查找，排序查找，分组查找以及去重查找变得异常简单。而 B 树因为数据分散在各个节点，要实现这一点是很不容易的。

有心的读者可能还发现上图 B+ 树中各个页之间是通过双向链表连接的，叶子节点中的数据是通过单向链表连接的。

其实上面的 B 树我们也可以对各个节点加上链表。这些不是它们之前的区别，是因为在 MySQL 的 InnoDB 存储引擎中，索引就是这样存储的。

也就是说上图中的 B+ 树索引就是 InnoDB 中 B+ 树索引真正的实现方式，准确的说应该是聚集索引（聚集索引和非聚集索引下面会讲到）。

通过上图可以看到，在 InnoDB 中，我们通过数据页之间通过双向链表连接以及叶子节点中数据之间通过单向链表连接的方式可以找到表中所有的数据。

MyISAM 中的 B+ 树索引实现与 InnoDB 中的略有不同。在 MyISAM 中，B+ 树索引的叶子节点并不存储数据，而是存储数据的文件地址。

## 6.聚集索引 VS 非聚集索引

在上节介绍 B+ 树索引的时候，我们提到了图中的索引其实是聚集索引的实现方式。

那什么是聚集索引呢？在 MySQL 中，B+ 树索引按照存储方式的不同分为聚集索引和非聚集索引。

这里我们着重介绍 InnoDB 中的聚集索引和非聚集索引：

**①\****聚集索引（聚簇索引）：**以 InnoDB 作为存储引擎的表，表中的数据都会有一个主键，即使你不创建主键，系统也会帮你创建一个隐式的主键。

这是因为 InnoDB 是把数据存放在 B+ 树中的，而 B+ 树的键值就是主键，在 B+ 树的叶子节点中，存储了表中所有的数据。

这种以主键作为 B+ 树索引的键值而构建的 B+ 树索引，我们称之为聚集索引。

**②\****非聚集索引（非聚簇索引）：**以主键以外的列值作为键值构建的 B+ 树索引，我们称之为非聚集索引。

非聚集索引与聚集索引的区别在于非聚集索引的叶子节点不存储表中的数据，而是存储该列对应的主键，想要查找数据我们还需要根据主键再去聚集索引中进行查找，这个再根据聚集索引查找数据的过程，我们称为回表。

明白了聚集索引和非聚集索引的定义，我们应该明白这样一句话：数据即索引，索引即数据。

## 7.利用聚集索引和非聚集索引查找数据

前面我们讲解 B+ 树索引的时候并没有去说怎么在 B+ 树中进行数据的查找，主要就是因为还没有引出聚集索引和非聚集索引的概念。

下面我们通过讲解如何通过聚集索引以及非聚集索引查找数据表中数据的方式介绍一下 B+ 树索引查找数据方法。

### 7.1**利用聚集索引查找数据**

![7](https://img-blog.csdnimg.cn/img_convert/c082ca908566117969ce7bd79afb6f4b.png)

还是这张 B+ 树索引图，现在我们应该知道这就是聚集索引，表中的数据存储在其中。

现在假设我们要查找 id>=18 并且 id<40 的用户数据。对应的 sql 语句为：

MySQL

| 1    | **select** * **from** **user** **where** id>=18 **and** id <40 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

其中 id 为主键，具体的查找过程如下：

①一般根节点都是常驻内存的，也就是说页 1 已经在内存中了，此时不需要到磁盘中读取数据，直接从内存中读取即可。

从内存中读取到页 1，要查找这个 id>=18 and id <40 或者范围值，我们首先需要找到 id=18 的键值。

从页 1 中我们可以找到键值 18，此时我们需要根据指针 p2，定位到页 3。

②要从页 3 中查找数据，我们就需要拿着 p2 指针去磁盘中进行读取页 3。

从磁盘中读取页 3 后将页 3 放入内存中，然后进行查找，我们可以找到键值 18，然后再拿到页 3 中的指针 p1，定位到页 8。

③同样的页 8 页不在内存中，我们需要再去磁盘中将页 8 读取到内存中。

将页 8 读取到内存中后。因为页中的数据是链表进行连接的，而且键值是按照顺序存放的，此时可以根据二分查找法定位到键值 18。

此时因为已经到数据页了，此时我们已经找到一条满足条件的数据了，就是键值 18 对应的数据。

因为是范围查找，而且此时所有的数据又都存在叶子节点，并且是有序排列的，那么我们就可以对页 8 中的键值依次进行遍历查找并匹配满足条件的数据。

我们可以一直找到键值为 22 的数据，然后页 8 中就没有数据了，此时我们需要拿着页 8 中的 p 指针去读取页 9 中的数据。

④因为页 9 不在内存中，就又会加载页 9 到内存中，并通过和页 8 中一样的方式进行数据的查找，直到将页 12 加载到内存中，发现 41 大于 40，此时不满足条件。那么查找到此终止。

最终我们找到满足条件的所有数据，总共 12 条记录：

(18,kl), (19,kl), (22,hj), (24,io), (25,vg) , (29,jk), (31,jk) , (33,rt) , (34,ty) , (35,yu) , (37,rt) , (39,rt) 。

下面看下具体的查找流程图

![8](https://img-blog.csdnimg.cn/img_convert/091a48a642ce1c027ecae2ca66458058.png)

### 7.2**利用非聚集索引查找数据**

![9](https://img-blog.csdnimg.cn/img_convert/bf9089617c720d9d4b2c23c66e8f043f.png)

读者看到这张图的时候可能会蒙，这是啥东西啊？怎么都是数字。如果有这种感觉，请仔细看下图中红字的解释。

什么？还看不懂？那我再来解释下吧。首先，这个非聚集索引表示的是用户幸运数字的索引（为什么是幸运数字？一时兴起想起来的:-)），此时表结构是这样的。

![img](https://img-blog.csdnimg.cn/img_convert/8b3a2bcf77ab02a92cd699e000eea06a.png)[![91](https://img-blog.csdnimg.cn/img_convert/20c1c55933c978d24d3368df7e73cd76.png)](http://www.liuzk.com/wp-content/uploads/2019/11/91.jpg)

在叶子节点中，不再存储所有的数据了，存储的是键值和主键。对于叶子节点中的 x-y，比如 1-1。左边的 1 表示的是索引的键值，右边的 1 表示的是主键值。

如果我们要找到幸运数字为 33 的用户信息，对应的 sql 语句为：

MySQL

| 1    | **select** * **from** **user** **where** luckNum=33 |
| ---- | --------------------------------------------------- |
|      |                                                     |

查找的流程跟聚集索引一样，这里就不详细介绍了。我们最终会找到主键值 47，找到主键后我们需要再到聚集索引中查找具体对应的数据信息，此时又回到了聚集索引的查找流程。

下面看下具体的查找流程图：[  ](http://www.liuzk.com/wp-content/uploads/2019/11/92.jpg)

[![92](https://img-blog.csdnimg.cn/img_convert/63995dedc61332a15a4dd2daca873876.png)](http://www.liuzk.com/wp-content/uploads/2019/11/921.jpg)[  ](http://www.liuzk.com/wp-content/uploads/2019/11/92.jpg)

在 MyISAM 中，聚集索引和非聚集索引的叶子节点都会存储数据的文件地址。

## 8.总结

本篇文章从二叉查找树，详细说明了为什么 MySQL 用 B+ 树作为数据的索引，以及在 InnoDB 中数据库如何通过 B+ 树索引来存储数据以及查找数据。

我们一定要记住这句话：数据即索引，索引即数据。



原文链接：https://blog.csdn.net/qq_45814695/article/details/117171536

# 【NO.9】TCP/IP 介绍

TCP/IP 是用于因特网 (Internet) 的通信协议。

## 1.计算机通信协议（Computer Communication Protocol）

计算机通信协议是对那些计算机必须遵守以便彼此通信的的规则的描述。

## 2.什么是 TCP/IP？

TCP/IP 是供已连接因特网的计算机进行通信的通信协议。

TCP/IP 指传输控制协议/网际协议（***T\*ransmission \*C\*ontrol \*P\*rotocol** / ***I\*nternet \*P\*rotocol**）。

TCP/IP 定义了电子设备（比如计算机）如何连入因特网，以及数据如何在它们之间传输的标准。

## 3.在 TCP/IP 内部

在 TCP/IP 中包含一系列用于处理数据通信的协议：

- TCP (传输控制协议) - 应用程序之间通信
- UDP (用户数据报协议) - 应用程序之间的简单通信
- IP (网际协议) - 计算机之间的通信
- ICMP (因特网消息控制协议) - 针对错误和状态
- DHCP (动态主机配置协议) - 针对动态寻址

## 4.TCP 使用固定的连接

TCP 用于应用程序之间的通信。

当应用程序希望通过 TCP 与另一个应用程序通信时，它会发送一个通信请求。这个请求必须被送到一个确切的地址。在双方"握手"之后，TCP 将在两个应用程序之间建立一个全双工 (full-duplex) 的通信。

这个全双工的通信将占用两个计算机之间的通信线路，直到它被一方或双方关闭为止。

UDP 和 TCP 很相似，但是更简单，同时可靠性低于 TCP。

## 5.IP 是无连接的

IP 用于计算机之间的通信。

IP 是无连接的通信协议。它不会占用两个正在通信的计算机之间的通信线路。这样，IP 就降低了对网络线路的需求。每条线可以同时满足许多不同的计算机之间的通信需要。

通过 IP，消息（或者其他数据）被分割为小的独立的包，并通过因特网在计算机之间传送。

IP 负责将每个包路由至它的目的地。

## 6.IP 路由器

当一个 IP 包从一台计算机被发送，它会到达一个 IP 路由器。

IP 路由器负责将这个包路由至它的目的地，直接地或者通过其他的路由器。

在一个相同的通信中，一个包所经由的路径可能会和其他的包不同。而路由器负责根据通信量、网络中的错误或者其他参数来进行正确地寻址。

## 7.TCP/IP

TCP/IP 意味着 TCP 和 IP 在一起协同工作。

TCP 负责应用软件（比如您的浏览器）和网络软件之间的通信。

IP 负责计算机之间的通信。

TCP 负责将数据分割并装入 IP 包，然后在它们到达的时候重新组合它们。

IP 负责将包发送至接受者。

原文链接：https://www.runoob.com/tcpip/tcpip-intro.html

# 【NO.10】CP/IP 寻址

TCP/IP 使用 32 个比特或者 4 组 0 到 255 之间的数字来为计算机编址。

## 1.IP地址

每个计算机必须有一个 IP 地址才能够连入因特网。

每个 IP 包必须有一个地址才能够发送到另一台计算机。

在本教程下一节，您会学习到更多关于 IP 地址和 IP 名称的知识。

## 2.IP 地址包含 4 组数字：

TCP/IP 使用 4 组数字来为计算机编址。每个计算机必须有一个唯一的 4 组数字的地址。

每组数字必须在 0 到 255 之间，并由点号隔开，比如：192.168.1.60。

## 3.32 比特 = 4 字节

TCP/IP 使用 32 个比特来编址。一个计算机字节是 8 比特。所以 TCP/IP 使用了 4 个字节。

一个计算机字节可以包含 256 个不同的值：

00000000、00000001、00000010、00000011、00000100、00000101、00000110、00000111、00001000 ....... 直到 11111111。

现在，您应该知道了为什么 TCP/IP 地址是介于 0 到 255 之间的 4 组数字。

## 4.IP V6

IPv6 是 "Internet Protocol Version 6" 的缩写，也被称作下一代互联网协议，它是由 IETF 小组（Internet 工程任务组Internet Engineering Task Force）设计的用来替代现行的 IPv4（现行的）协议的一种新的 IP 协议。

我们知道，Internet 的主机都有一个唯一的 IP 地址，IP 地址用一个 32 位二进制的数表示一个主机号码，但 32 位地址资源有限，已经不能满足用户的需求了，因此 Internet 研究组织发布新的主机标识方法，即 IPv6。

在 RFC1884 中（RFC 是 Request for Comments document 的缩写。RFC 实际上就是 Internet 有关服务的一些标准），规定的标准语法建议把 IPv6 地址的 128 位（16 个字节）写成 8 个 16 位的无符号整数，每个整数用 4 个十六进制位表示，这些数之间用冒号（:）分开，例如：

```
686E：8C64：FFFF：FFFF：0：1180：96A：FFFF
```

冒号十六进制记法允许零压缩，即一串连续的0可以用一对冒号取代，例如：

```
FF05：0：0：0：0：0：0：B3可以定成：FF05：：B3
```

为了保证零压缩有一个清晰的解释，建议中规定，在任一地址中，只能使用一次零压缩。该技术对已建议的分配策略特别有用，因为会有许多地址包含连续的零串。

冒号十六进制记法结合有点十进制记法的后缀。这种结合在IPv4向IPv6换阶段特别有用。例如，下面的串是一个合法的冒号十六进制记法：

```
0：0：0：0：0：0：128.10.1.1
```

这种记法中，虽然冒号所分隔的每一个值是一个16位的量，但每个分点十进制部分的值则指明一个字节的值。再使用零压缩即可得出：

```
：：128.10.1.1
```

## 5.域名

12 个阿拉伯数字很难记忆。使用一个名称更容易。

用于 TCP/IP 地址的名字被称为域名。runoob.com 就是一个域名。

当你键入一个像 http://www.runoob.com 这样的域名，域名会被一种 DNS 程序翻译为数字。

在全世界，数量庞大的 DNS 服务器被连入因特网。DNS 服务器负责将域名翻译为 TCP/IP 地址，同时负责使用新的域名信息更新彼此的系统。

当一个新的域名连同其 TCP/IP 地址一起注册后，全世界的 DNS 服务器都会对此信息进行更新。

原文链接：https://www.runoob.com/tcpip/tcpip-addressing.html

# 【NO.11】CP/IP 协议

TCP/IP 是不同的通信协议的大集合。

## 1.协议族

TCP/IP 是基于 TCP 和 IP 这两个最初的协议之上的不同的通信协议的大集合。

## 2.TCP - 传输控制协议

TCP 用于从应用程序到网络的数据传输控制。

TCP 负责在数据传送之前将它们分割为 IP 包，然后在它们到达的时候将它们重组。

## 3.IP - 网际协议（Internet Protocol）

IP 负责计算机之间的通信。

IP 负责在因特网上发送和接收数据包。

## 4.HTTP - 超文本传输协议(Hyper Text Transfer Protocol)

HTTP 负责 web 服务器与 web 浏览器之间的通信。

HTTP 用于从 web 客户端（浏览器）向 web 服务器发送请求，并从 web 服务器向 web 客户端返回内容（网页）。

## 5.HTTPS - 安全的 HTTP（HTTP Secure）

HTTPS 负责在 web 服务器和 web 浏览器之间的安全通信。

作为有代表性的应用，HTTPS 会用于处理信用卡交易和其他的敏感数据。

## 6.SSL - 安全套接字层（Secure Sockets Layer）

SSL 协议用于为安全数据传输加密数据。

## 7.SMTP - 简易邮件传输协议（Simple Mail Transfer Protocol）

SMTP 用于电子邮件的传输。

## 8.MIME - 多用途因特网邮件扩展（Multi-purpose Internet Mail Extensions）

MIME 协议使 SMTP 有能力通过 TCP/IP 网络传输多媒体文件，包括声音、视频和二进制数据。

## 9.IMAP - 因特网消息访问协议（Internet Message Access Protocol）

IMAP 用于存储和取回电子邮件。

## 10.POP - 邮局协议（Post Office Protocol）

POP 用于从电子邮件服务器向个人电脑下载电子邮件。

## 11.FTP - 文件传输协议（File Transfer Protocol）

FTP 负责计算机之间的文件传输。

## 12.NTP - 网络时间协议（Network Time Protocol）

NTP 用于在计算机之间同步时间（钟）。

## 13.DHCP - 动态主机配置协议（Dynamic Host Configuration Protocol）

DHCP 用于向网络中的计算机分配动态 IP 地址。

## 14.SNMP - 简单网络管理协议（Simple Network Management Protocol）

SNMP 用于计算机网络的管理。

## 15.LDAP - 轻量级的目录访问协议（Lightweight Directory Access Protocol）

LDAP 用于从因特网搜集关于用户和电子邮件地址的信息。

## 16.ICMP - 因特网消息控制协议（Internet Control Message Protocol）

ICMP 负责网络中的错误处理。

## 17.ARP - 地址解析协议（Address Resolution Protocol）

ARP - 用于通过 IP 来查找基于 IP 地址的计算机网卡的硬件地址。

## 18.RARP - 反向地址转换协议（Reverse Address Resolution Protocol）

RARP 用于通过 IP 查找基于硬件地址的计算机网卡的 IP 地址。

## 19.BOOTP - 自举协议（Boot Protocol）

BOOTP 用于从网络启动计算机。

## 20.PPTP - 点对点隧道协议（Point to Point Tunneling Protocol）

PPTP 用于私人网络之间的连接（隧道）。

原文链接：https://www.runoob.com/tcpip/tcpip-protocols.html

# 【NO.12】TCP/IP 邮件

电子邮件是 TCP/IP 最重要的应用之一。

## 1.您不会用到...

当您写邮件时，您不会用到 TCP/IP。

当您写邮件时，您用到的是电子邮件程序，例如莲花软件的 Notes，微软公司出品的 Outlook，或者 Netscape Communicator 等等。

## 2.邮件程序会用到...

您的电子邮件程序使用不同的 TCP/IP 协议：

- 使用 SMTP 来发送邮件
- 使用 POP 从邮件服务器下载邮件
- 使用 IMAP 连接到邮件服务器

## 3.SMTP - 简单邮件传输协议

SMTP 协议用于传输电子邮件。SMTP 负责把邮件发送到另一台计算机。

通常情况下，邮件会被送到一台邮件服务器（SMTP 服务器），然后被送到另一台（或几台）服务器，然后最终被送到它的目的地。

SMTP 也可以传送纯文本，但是无法传输诸如图片、声音或者电影之类的二进制数据。

SMTP 使用 MIME 协议通过 TCP/IP 网络来发送二进制数据。MIME 协议会将二进制数据转换为纯文本。

## 4.POP - 邮局协议

POP 协议被邮件程序用来取回邮件服务器上面的邮件。

假如您的邮件程序使用 POP，那么一旦它连接上邮件服务器，您的所有的邮件都会被下载到邮件程序中（或者称之为邮件客户端）。

## 5.IMAP - 因特网消息访问协议

与 POP 类似，IMAP 协议同样被邮件程序使用。

IMAP 协议与 POP 协议之间的主要差异是：如果 IMAP 连上了邮件服务器，它不会自动地将邮件下载到邮件程序之中。

IMAP 使您有能力在下载邮件之前先通过邮件服务器端查看他们。通过 IMAP，您可以选择下载这些邮件或者仅仅是删除它们。比方说您需要从不同的位置访问邮件服务器，但是仅仅希望回到办公室的时候再下载邮件，IMAP 在这种情况下会很有用。

原文链接：https://www.runoob.com/tcpip/tcpip-email.html

# 【NO.13】Nginx负载均衡原理与实战经典案例

## **1.负载均衡的概念**

nginx应用场景之一就是负载均衡。在访问量较多的时候，可以通过负载均衡，将多个请求分摊到多台服务器上，相当于把一台服务器需要承担的负载量交给多台服务器处理，进而提高系统的吞吐率；另外如果其中某一台服务器挂掉，其他服务器还可以正常提供服务，以此来提高系统的可伸缩性与可靠性。

![img](https://pic1.zhimg.com/80/v2-0f0b3dc13daf7b304ba00eabea95d260_720w.webp)

上图为负载均衡示例图，当用户请求发送后，首先发送到负载均衡服务器，而后由负载均衡服务器根据配置规则将请求转发到不同的web服务器上。

## **2.nginx负载均衡策略**

nginx内置负载均衡策略主要分为三大类，分别是轮询、最少连接和ip hash

- 最少连接 请求分配给活动连接数最少的服务器，哪台服务器连接数最少，则把请求交给哪台服务器，由nginx统计服务器连接数
- ip hash 基于客户端ip的分配方式
- 轮询 以循环方式分发对应用服务器的请求，将请求平均分发到每台服务器上。

### **2-1轮询详解**

#### **2-1-1 普通轮询方式**

该方式是默认方式，轮询适合服务器配置相当，无状态且短平快的服务使用。另外在轮询中，如果服务器挂掉，会自动剔除该服务器。

```
http {
    # 定义转发分配规则
    upstream myapp1 {
        server srv1.com; # 要转发到的服务器，如ip、ip:端口号、域名、域名:端口号
        server srv2.com:8088;
        server 192.168.0.100:8088;
    }

    server {
        listen 80; # nginx监听的端口

        location / {
          # 使用myapp1分配规则，即刚自定义添加的upstream节点
          # 将所有请求转发到myapp1服务器组中配置的某一台服务器上
            proxy_pass http://myapp1; 
        }
    }
}
```

#### **2-1-2 权重轮询方式**

如果在 upstream 中配置的server参数后追加 weight 配置，则会根据配置的权重进行请求分发。此策略可以与least_conn和ip_hash结合使用，适合服务器的硬件配置差别比较大的情况。

```
# 定义转发分配规则
upstream myapp1 {
  server srv1.com weight=1; # 该台服务器接受1/6的请求量
  server srv2.com:8088 weight=2; # 该台服务器接受2/6的请求量
  server 192.168.0.100:8088 weight=3; # 该台服务器接受3/6的请求量;
}
```

### **2-2最少连接**

轮询算法是把请求平均的转发给各个后端，使它们的负载大致相同；但是，有些请求占用的时间很长，会导致其所在的后端负载较高。这种情况下，least_conn这种方式就可以达到更好的负载均衡效果，适合请求处理时间长短不一造成服务器过载的情况。

```
# 定义转发分配规则
upstream myapp1 {
  least_conn; # 把请求分派给连接数最少的服务器
  server srv1.com;
  server srv2.com:8088;
  server 192.168.0.100:8088;
}
```

### **2-3IP HASH**

这个方法确保了相同的客户端的请求一直发送到相同的服务器，这样每个访客都固定访问一个后端服务器。如用户需要分片上传文件到服务器下，然后再由服务器将分片合并，这时如果用户的请求到达了不同的服务器，那么分片将存储于不同的服务器目录中，导致无法将分片合并，该场景则需要使用ip hash策略。

需要注意的是，ip_hash不能与backup同时使用，另外当有服务器需要剔除，必须手动down掉，此模式适合有状态服务，比如session。

```
# 定义转发分配规则
upstream myapp1 {
  ip_hash; # #保证每个请求固定访问一个后端服务器
  server srv1.com;
  server srv2.com:8088;
  server 192.168.0.100:8088;
}
```



## **3.Nginx的负载均衡案例**

### **1.环境介绍**

该示例使用一台nginx作为负载均衡服务器，两台tomcat作为web服务器；可以把三个服务均在一台机器进行搭建，也可以使用虚拟机虚拟三台机器，然后进行测试。教程这里就只在一台机器进行搭建，采用默认的权重方式进行配置。

![img](https://pic2.zhimg.com/80/v2-16f961a829e95659ee6c5eb5d3493ec5_720w.webp)

如拓扑所示：**客户端要访问资源链接**如下：

```
http://www.ywflinux.com:8090/linux/linux.html
```

通过配置Nginx负载均衡服务器，它会将客户端请求平均转发到真实服务器站点1和真实服务器站点2上。

### **2.环境准备**

（1）客户端主机配置好域名解析，使得在客户端主机浏览器上通过访问[http://www.ywflinux.com](https://link.zhihu.com/?target=http%3A//www.ywflinux.com)，能够直接解析到192.168.13.199，我客户端在windows环境，现在通过修改相关配置文件，配置IP与域名的映射关系，配置如下：

```
192.168.13.199  www.ywflinux.com
```

![img](https://pic1.zhimg.com/80/v2-f479b2df0d5bbf7bab562fa085a72750_720w.webp)

（2）**真实服务器站点1和真实服务器站点2的tomcat环境准备好**，这里准备两个tomcat服务器，对应端口分别为8888和9999，主要是安装配置tomcat相关操作，如我配置好相关操作，能够访问到的真实服务器站点如下：

![img](https://pic3.zhimg.com/80/v2-ba44c92b5a228cfee09a1791b10bbe1a_720w.webp)

```
[root@www local]# cd tomcat2
[root@www tomcat2]# 
[root@www tomcat2]# ls
bin   lib      logs    RELEASE-NOTES  temp     work
conf  LICENSE  NOTICE  RUNNING.txt    webapps
[root@www tomcat2]# cd webapps/
[root@www webapps]# ls
docs  examples  host-manager  manager  ROOT  vod
[root@www webapps]# mkdir linux
[root@www webapps]# ls
docs  examples  host-manager  linux  manager  ROOT  vod
[root@www webapps]# cd vod/
[root@www vod]# ls
a.html
[root@www vod]# cp a.html /usr/local/tomcat2/webapps/linux/
[root@www vod]# cd /usr/local/tomcat2/webapps/linux/
[root@www linux]# ls
a.html
[root@www linux]# mv a.html linux.html
[root@www linux]# vi linux.html 
[root@www linux]# cat linux.html 
<h1>This is ywf 9999 Port</h1>
[root@www vod]#
```

![img](https://pic3.zhimg.com/80/v2-26ad476480c1282c448f14f695f0b75e_720w.webp)

![img](https://pic4.zhimg.com/80/v2-68ec5d498cfc92c03377581668bd5d8b_720w.webp)

![img](https://pic3.zhimg.com/80/v2-4290ef78641d9f2eb76a19919a338c6a_720w.webp)

（3）**负载均衡服务器环境准备**，这里主要是准备并且配置好负载均衡服务器，使得负载均衡服务器能够将客户端请求平均转发给真实服务器站点1和真实服务器站点2。具体配置如下所示：

修改nginx配置文件nginx.conf，在nginx.conf文件中的http块中添加、修改server{}标签相关配置。如下所示：

```
http {
    include       mime.types;
    default_type  application/octet-stream;

    #log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
    #                  '$status $body_bytes_sent "$http_referer" '
    #                  '"$http_user_agent" "$http_x_forwarded_for"';

    #access_log  logs/access.log  main;

    sendfile        on;
    #tcp_nopush     on;

    #keepalive_timeout  0;
    keepalive_timeout  65;

    #gzip  on;
    upstream ywfserver {
        server 192.168.13.199:8888;
        server 192.168.13.199:9999;
    }
    server {
        listen      8090;
        server_name  192.168.13.199;

        charset utf-8;

        #access_log  logs/host.access.log  main;

        location  / {
            proxy_pass http://ywfserver;
            root html;
            index index.html index.htm;
        }

        #error_page  404              /404.html;

        # redirect server error pages to the static page /50x.html
        #
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }

    }
```

![img](https://pic2.zhimg.com/80/v2-a3d99c666553056b012491fb42affc79_720w.webp)

### **3.检查nginx配置文件语法，确认语法无误，命令如下：**

```
[root@localhost conf]# nginx -t
```

![img](https://pic1.zhimg.com/80/v2-c8735b1943e7b5d403a3406b31638dc4_720w.webp)

![img](https://pic1.zhimg.com/80/v2-2d2c4e33544900244eea44a093ef5b50_720w.webp)

### **4.重启nginx服务：**

```
[root@localhost conf]# nginx -s stop[root@localhost conf]# nginx
```

### **5.验证实验配置结果**

客户端通过浏览器访问站点资源[http://www.ywflinux.com:8090/linux/linux.html](https://link.zhihu.com/?target=http%3A//www.ywflinux.com%3A8090/linux/linux.html)，返回结果如下图所示，说明负载均衡配置成功。

![img](https://pic2.zhimg.com/80/v2-91d7060fcd5e83e545f68d0133c1c1fd_720w.webp)

此时第一次访问成功的时候，只需要重新载入页面就可以发现自动切换到我们的真实站点目录2.

![img](https://pic3.zhimg.com/80/v2-6acfa5eed034603d648b46f021b1d6e6_720w.webp)

**由此可见，负载均衡服务器将客户端请求平均分摊转发给了两个不同端口号的tomcat服务器进行处理。**

## 4.Nginx的动静分离介绍

### **1. 动静分离的概念**

#### 1.1 动态页面与静态页面区别

- 静态资源：当用户多次访问这个资源，资源的源代码永远不会改变的资源。
- 动态资源：当用户多次访问这个资源，资源的源代码可能会发送改变。

#### 1.2 什么是动静分离

- 动静分离是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后，我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路
- 动静分离简单的概括是：动态文件与静态文件的分离。
- 伪静态：网站如果想被搜索引擎搜素到，动态页面静态技术freemarker等模版引擎技术
- 1.3 为什么要用动静分离
- 在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js等等文件），这些不需要经过后台处理的文件称为静态文件，否则动态文件。因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静态文件不就完了吗。当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决。
- 动静分离将网站静态资源（HTML，JavaScript，CSS，img等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问。这里我们将静态资源放到nginx中，动态资源转发到tomcat服务器中。
- 因此，动态资源转发到tomcat服务器我们就使用到了前面讲到的反向代理了。

### **2 .nginx实现动静分离**

**架构分析**

![img](https://pic3.zhimg.com/80/v2-5224f7575f16a287c60a976f7006b35e_720w.webp)

原文链接：https://zhuanlan.zhihu.com/p/584070725

# 【NO.14】TCP 的封包格式：TCP 为什么要粘包和拆包？

今天我们将**从稳定性角度深挖 TCP 协议的运作机制**。

如今，大半个互联网都建立在 TCP 协议之上，我们使用的 HTTP 协议、[消息队列](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/product/cmq%3Ffrom%3D10680)、存储、缓存，都需要用到 TCP 协议——**这是因为 TCP 协议提供了可靠性**。

简单来说，可靠性就是让数据无损送达。但若是考虑到成本，就会变得非常复杂——因为还需要尽可能地提升吞吐量、降低延迟、减少丢包率。

**TCP 协议具有很强的实用性，而可靠性又是 TCP 最核心的能力** 。具体来说，从一个终端有序地发出多个数据包，经过一个复杂的网络环境，到达目的地的时候会变得无序，而可靠性要求数据恢复到原始的顺序。这里先提出两个问题：

- TCP 协议是如何恢复数据的顺序的？
- 拆包和粘包的作用是什么？

那么带着这两个问题开始今天的学习。

## **1.TCP 的拆包和粘包**

### **1.1TCP数据发送**

**TCP 是一个传输层协议**

TCP 发送数据的时候，往往不会将数据一次性发送

![img](https://pic2.zhimg.com/80/v2-74671914df274e2f6b82f3f0648d8745_720w.webp)

而是将数据拆分成很多个部分，然后再逐个发送。像下图这样：

![img](https://pic4.zhimg.com/80/v2-332d128b061e2f30eced1b7da19d8be3_720w.webp)

**同样的，在目的地，TCP 协议又需要逐个接收数据。**

请 思考，TCP 为什么不一次发送完所有的数据？比如我们要传一个大小为 10M 的文件，对于应用层而言，就是一次传送完成的。而**传输层的协议为什么不选择将这个文件一次发送完呢？**

这里有很多原因，

- 比如为了稳定性，一次发送的数据越多，出错的概率越大。
- 再比如说为了效率，网络中有时候存在着并行的路径，拆分数据包就能更好地利用这些并行的路径。
- 再有，比如发送和接收数据的时候，都存在着缓冲区。如下图所示：

![img](https://pic4.zhimg.com/80/v2-c50f2772885ad7262d78a61e2f34651b_720w.webp)

**缓冲区是在内存中开辟的一块区域，目的是缓冲。因为大量的应用频繁地通过网卡收发数据，这个时候，网卡只能一个一个处理应用的请求。当网卡忙不过来的时候，数据就需要排队，也就是将数据放入缓冲区**。如果每个应用都随意发送很大的数据，可能导致其他应用实时性遭到破坏。

还有一些原因 比如内存的最小分配单位是页表，如果数据的大小超过一个页表，可能会存在页面置换问题，造成性能的损失。

总之，方方面面的原因：**在传输层封包不能太大**。

这种限制，往往是以缓冲区大小为单位的。也就是 **TCP 协议，会将数据拆分成不超过缓冲区大小的一个个部分**。每个部分有一个独特的名词，叫作 **TCP 段（TCP Segment）**。

在接收数据的时候，一个个 TCP 段又被重组成原来的数据。

像这样，**数据经过拆分，然后传输，然后在目的地重组，俗称拆包**。所以拆包是将数据拆分成多个 TCP 段传输。

那么粘包是什么呢？有时候，**如果发往一个目的地的多个数据太小了，为了防止多次发送占用资源，TCP 协议有可能将它们合并成一个 TCP 段发送，在目的地再还原成多个数据，这个过程俗称粘包。所以粘包是将多个数据合并成一个 TCP 段发送**。

### **1.2TCP Segment**

那么一个 TCP 段长什么样子呢？下图是一个 TCP 段的格式：

![img](https://pic1.zhimg.com/80/v2-5df0a9cbe0a5a0546c41ba83934a7dd0_720w.webp)

我们可以看到，TCP 的很多配置选项和数据粘在了一起，作为一个 TCP 段。

显然， 把每一部分都记住似乎不太现实，先把其中最主要的部分理解。



**TCP 协议就是依靠每一个 TCP 段工作的，所以你每认识一个 TCP 的能力，几乎都会找到在 TCP Segment 中与之对应的字段**。 接下来 认识它们。

- Source Port/Destination Port 描述的是发送端口号和目标端口号，代表发送数据的应用程序和接收数据的应用程序。比如 80 往往代表 HTTP 服务，22 往往是 SSH 服务……
- Sequence Number 和 Achnowledgment Number 是保证可靠性的两个关键
- Data Offset 是一个偏移量。这个量存在的原因是 TCP Header 部分的长度是可变的，因此需要一个数值来描述数据从哪个字节开始。
- Reserved 是很多协议设计会保留的一个区域，用于日后扩展能力。
- URG/ACK/PSH/RST/SYN/FIN 是几个标志位，用于描述 TCP 段的行为。也就是一个 TCP 封包到底是做什么用的

> 1）URG 代表这是一个紧急数据，比如远程操作的时候，用户按下了 Ctrl+C，要求终止程序，这种请求需要紧急处理。 2）ACK 代表响应， 所有的消息都必须有 ACK，这是 TCP 协议确保稳定性的一环。 3）PSH 代表数据推送，也就是在传输数据的意思。 4）SYN 同步请求，也就是申请握手。 5）FIN 终止请求，也就是挥手。

**特别说明一下：以上这 5 个标志位，每个占了一个比特，可以混合使用。比如 ACK 和 SYN 同时为 1，代表同步请求和响应被合并了。这也是 TCP 协议，为什么是三次握手的原因之一**。

- Window 也是 TCP 保证稳定性并进行流量控制的工具，后续会 TCP 的稳定性：滑动窗口和流速控制是中详细介绍。
- Checksum 是校验和，用于校验 TCP 段有没有损坏。
- Urgent Pointer 指向最后一个紧急数据的序号（Sequence Number）。它存在的原因是：有时候紧急数据是连续的很多个段，所以需要提前告诉接收方进行准备。
- Options 中存储了一些可选字段，比如接下来我们要讨论的 MSS（Maximun Segment Size）。
- Padding 存在的意义是因为 Options 的长度不固定，需要 Pading 进行对齐。

### **1.3Sequence Number 和 Acknowledgement Number**

在 TCP 协议的设计当中，数据被拆分成很多个部分，部分增加了协议头。合并成为一个 TCP 段，进行传输。这个过程，我们俗称拆包。这些 TCP 段经过复杂的网络结构，由底层的 IP 协议，负责传输到目的地，然后再进行重组。

这里请你思考一个问题：**稳定性要求数据无损地传输，也就是说拆包获得数据，又需要恢复到原来的样子。而在复杂的网络环境当中，即便所有的段是顺序发出的，也不能保证它们顺序到达，因此，发出的每一个 TCP 段都需要有序号。这个序号，就是 Sequence Number（Seq）**。

![img](https://pic1.zhimg.com/80/v2-facf4f565297f34847e4887beac638a0_720w.webp)

如上图所示。**发送数据的时候，为每一个 TCP 段分配一个自增的 Sequence Number。接收数据的时候，虽然得到的是乱序的 TCP 段，但是可以通过 Seq 进行排序。**

但是这样又会产生一个新的问题——**接收方如果要回复发送方，也需要这个 Seq。而网络的两个终端，去同步一个自增的序号是非常困难的**。因为任何两个网络主体间，时间都不能做到完全同步，又没有公共的存储空间，无法共享数据，更别说实现一个分布式的自增序号了。

其实这个问题的本质就好像两个人在说话一样，我们要确保他们说出去的话，和回答之间的顺序。因为 TCP 是一个双工的协议，两边可能会同时说话。所以聪明的**科学家想到了确定一句话的顺序，需要两个值去描述——也就是发送的字节数和接收的字节数**。

![img](https://pic2.zhimg.com/80/v2-e5d62634460b3dfbd17dc7a91604f955_720w.webp)

我们重新定义一下 Seq（如上图所示），对于任何一个接收方，如果知道了发送者发送某个 TCP 段时，已经发送了多少字节的数据，那么就可以确定发送者发送数据的顺序。

但是这里有一个问题。如果接收方也向发送者发送了数据请求（或者说双方在对话），接收方就不知道发送者发送的数据到底对应哪一条自己发送的数据了。

举个例子：下面 A 和 B 的对话中，我们可以确定他们彼此之间接收数据的顺序。但是无法确定数据之间的关联关系，所以只有 Sequence Number 是不够的。

A：今天天气好吗？ A：今天你开心吗？ B：开心 B：天气不好 复制

人类很容易理解这几句话的顺序，但是对于机器来说就需要特别的标注。因此我们还需要另一个数据，就是每个 TCP 段发送时，发送方已经接收了多少数据。用 Acknowledgement Number 表示，下面简写为 ACK。

下图中，终端发送了三条数据，并且接收到四条数据，通过观察，根据接收到的数据中的 Seq 和 ACK，将发送和接收的数据进行排序。

![img](https://pic4.zhimg.com/80/v2-1dc149546ddb7f0d25f6934f0ed974f7_720w.webp)

例如上图中，**发送方发送了 100 字节的数据，而接收到的（Seq = 0 和 Seq =100）的两个封包，都是针对发送方（Seq = 0）这个封包的。发送 100 个字节，所以接收到的 ACK 刚好是 100。说明（Seq= 0 和 Seq= 100）这两个封包是针对接收到第 100 个字节数据后，发送回来的。这样就确定了整体的顺序**。

**注意，无论 Seq 还是 ACK，都是针对“对方”而言的。是对方发送的数据和对方接收到的数据** 。 我们在实际的工作当中，可以通过 Whireshark 调试工具观察两个 TCP 连接的 Seq和 ACK。

![img](https://pic4.zhimg.com/80/v2-6137f6d93f3b6644cd4665431f805f0b_720w.webp)

### **1.4MSS（Maximun Segment Size）**

接下来，我们讨论下 MSS，它也是面试经常会问到的一个 TCP Header 中的可选项（Options），**这个可选项控制了 TCP 段的大小，它是一个协商字段（Negotiate）**。协议是双方都要遵循的标准，因此配置往往不能由单方决定，需要双方协商。

**TCP 段的大小（MSS）涉及发送、接收缓冲区的大小设置，双方实际发送接收封包的大小，对拆包和粘包的过程有指导作用，因此需要双方去协商**。

如果这个字段设置得非常大，就会带来一些影响。

- **首先对方可能会拒绝，作为服务的提供方，你可能不会愿意接收太大的 TCP 段。因为大的 TCP 段，会降低性能，比如内存使用的性能**。 还有就是资源的占用。一个用户占用[服务器](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/product/cvm%3Ffrom%3D10680)太多的资源，意味着其他的用户就需要等待或者降低他们的服务质量
- **其次，支持 TCP 协议工作的 IP 协议，工作效率会下降**

> TCP 协议不肯拆包，IP 协议就需要拆出大量的包。那么 IP 协议为什么需要拆包呢？这是因为在网络中，每次能够传输的数据不可能太大，这受限于具体的网络传输设备，也就是物理特性。但是 IP 协议，拆分太多的封包并没有意义。因为可能会导致属于同个 TCP 段的封包被不同的网络路线传输，这会加大延迟。同时，拆包，还需要消耗硬件和计算资源。 那是不是 MSS 越小越好呢？**MSS 太小的情况下，会浪费传输资源（降低吞吐量）。因为数据被拆分之后，每一份数据都要增加一个头部。如果 MSS 太小，那头部的数据占比会上升，这让吞吐量成为一个灾难。所以在使用的过程当中，MSS 的配置，往往都是一个折中的方案**。

不要去猜想什么样的方案是最合理的，而是要尝试去用实验证明它，一切都要用实验依据说话。

![img](https://pic2.zhimg.com/80/v2-80ad0f771f547c176bc32911f207cd71_720w.webp)

## **2.Question : TCP 协议是如何恢复数据的顺序的，TCP 拆包和粘包的作用是什么？**

Answer:

TCP 拆包的作用是将任务拆分处理，降低整体任务出错的概率，以及减小底层网络处理的压力。拆包过程需要保证数据经过网络的传输，又能恢复到原始的顺序。这中间，需要数学提供保证顺序的理论依据。 **TCP 利用（发送字节数、接收字节数）的唯一性来确定封包之间的顺序关系**。

粘包是为了防止数据量过小，导致大量的传输，而将多个 TCP 段合并成一个发送。

原文链接：https://zhuanlan.zhihu.com/p/583732173

# 【NO.15】网络收发与Nginx事件间的对应关系

## 1.**概述**

Nginx是一个事件驱动的框架， 所谓事件即网络事件。 Nginx每个连接自然对应两个网络事件，即 读事件和写事件。

要想理解Nginx的原理，以及Nginx再各种极端场景下的处理时，就必须要先了解网络事件。

## 2.**网络传输**

![img](https://pic4.zhimg.com/80/v2-5babcffb528891254b157864e53c4ca7_720w.webp)

假定主机 A 就是自己的电脑，主机 B 就是一台运行Nginx的[服务器](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/product/cvm%3Ffrom%3D10680)。

从主机 A 发送一个 HTTP 的 GET 请求到主机 B，这样的一个过程中主要经历了哪些事件？

通过上[图数据](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/product/konisgraph%3Ffrom%3D10680)流部分可以看出：

- 应用层里发送了一个 GET 请求 -> 到了传输层 ，这一步主要在做一件事，就是浏览器打开了一个端口，在 windows 的任务管理器中可以看到这一点，它会把这个端口记下来以及把 Nginx 打开的端口比如 80 或者 443 也记到传输层
- 然后在网络层会记下我们主机所在的 IP 和目标主机，也就是 Nginx 所在服务器公网 IP
- 到链路层以后 -> 经过以太网 -> 到达家里的路由器（网络层），家中的路由器会记录下所在运营商的一些下一段的 IP
- 通过广域网 -> 跳转到主机 B 所在的机器中 -> 报文会经过链路层 -> 网络层 -> 到传输层，在传输层操作系统就知道是给那个打开了 80 或者 443 的进程，这个进程自然就是 Nginx
- 那么 Nginx 在它的 HTTP 状态处理机里面（应用层）就会处理这个请求。

## 3.**TCP流与报文**

在上述过程中网络报文扮演了一个怎样的角色呢？

![img](https://pic3.zhimg.com/80/v2-cf52e6cd4a3ea2dfbd24be7f78a2595e_720w.webp)

- 数据链路层会在数据的前面 Header 部分和 Footer 部分添加上**源 MAC 地址和源目的地址**
- 到了网络层则是 Nginx 的公网地址（目的 IP 地址）和浏览器的公网地址（源 IP 地址）
- 到了 TCP 层（传输层），指定了 Nginx 打开的端口（目的端口）和浏览器打开的端口（源端口）
- 然后应用层就是 HTTP 协议了。

这就是一个报文，也就是说我们发送的 HTTP 协议会被切割成很多小的报文，在网络层会切割叫 MTU，以太网的每个 MTU 是 1500 字节；

> 在 TCP 层（传输层） 会考虑中间每个环节中最大的一个 MTU 值，这个时候往往每个报文只有几百字节，这个报文大小我们称为叫 MSS ，所以每收到一个 MSS 小于这么大小的一个报文时其实就是一个网络事件。 MTU： Maximum Transmit Unit，最大传输单元，即物理接口（数据链路层）提供给其上层（通常是网络层）最大一次传输数据的大小；每个以太网帧都有最小的大小64bytes，最大不能超过1518bytes，对于小于或者大于这个限制都视为错误的数据帧。一般的以太网转发设备会丢弃这些数据帧。 由于以太网帧的帧头的14字节和帧尾 CRC 校验4字节共占了18字节，剩下的承载上层协议的地方也就是 Data 域最大就只剩1500字节 MSS：Maximum Segment Size ，传输层概念，TCP 数据包每次能够传输的最大量。为了达到最佳的传输效能，TCP 协议在建立连接的时候通常要协商双方的 MSS 值，这个值TCP协议在实现的时候往往用 MTU 值代替（需要减去 IP 数据包包头的大小20Bytes和 TCP 数据段的包头20Bytes）所以往往 MSS 为1460。通讯双方会根据双方提供的 MSS 值得最小值确定为这次连接的最大 MSS 值。 MTU，最大传输单元是指一种通信协议在某一层上面所能通过的最[大数据](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/solution/bigdata%3Ffrom%3D10680)报大小（以字节为单位），它通常与链路层协议有密切的关系。 以太网传输电气方面的限制，每个以太网帧都有最小的大小64bytes，最大不能超过1518bytes，对于小于或者大于这个限制都视为错误的数据帧。一般的以太网转发设备会丢弃这些数据帧。 以太网EthernetII最大的数据帧是1518Bytes，除去以太网帧的帧头14Bytes和帧尾CRC校验部分4Bytes，那么剩下承载上层协议的地方也就是Data域最大就只能有1500Bytes，这个值我们就把它称之为MTU。 此MTU就是网络层协议非常关心的地方，因为网络层协议比如IP协议会根据这个值来决定是否把上层传下来的数据进行分片。就好比一个盒子没法装下一大块面包，我们需要把面包切成片，装在多个盒子里面一样的道理。当两台远程PC互联的时候，它们的数据需要穿过很多的路由器和各种各样的网络媒介才能到达对端，网络中不同媒介的MTU各不相同，就好比一长段的水管，由不同粗细的水管组成（MTU不同 ）通过这段水管最大水量就要由中间最细的水管决定。

![动图封面](https://pic3.zhimg.com/v2-29839370e8074f186e8dff0cc195cb06_b.jpg)





## **4.TCP 协议与非阻塞接口**

我们来看下 TCP 协议中许多事件是怎样和我们日常调用的一些接口（比如 Accept、Read、Write、Close）是怎样关联在一起的？

![img](https://pic3.zhimg.com/80/v2-5590a8a04b29e4d46ddd1a7b6aef4cee_720w.webp)

## 5.**读事件**

- 请求建立 TCP 连接事件实际上是发送了一个 TCP 报文，一个流程到达了 Nginx，对应的是读事件。因为对于 Nginx 来说，读取到了一个报文，所以就是 Accept 建立链接事件。
- 如果是 TCP 连接可读事件，就是发送了一个消息，对于 Nginx 也是一个读事件，就是 Read 读消息。
- 如果是对端（也就是浏览器）主动地关掉了，相当于 操作系统会去发送一个要求关闭链接的一个事件，对于 Nginx 来说还是一个读事件，因为它只是去读取一个报文。

## 6.**写事件**

那什么是写事件呢？

当需要向浏览器发送响应的时候，需要把消息写到操作系统中，要求操作系统发送到网络中，这就是一个写事件。

像这样的一些网络读写事件，通常在 Nginx 中或者任何一个异步事件的处理框架中，有个东西叫**事件收集、分发器**。

它会定义每类事件处理的消费者，也就是说事件是一个生产者，是通过网络中自动的生产到我们的 Nginx 中的，我们要对每种事件建立一个消费者。

> 比如连接建立事件消费者，就是对 Accept 调用，HTTP 模块就会去建立一个新的连接。还有很多读消息或者写消息，在 HTTP 状态机中不同的时间段会调用不同的方法也就是每个消费者处理。

以上就是一个事件分发、消费器，包括 AIO 像异步读写磁盘事件，还有定时器事件，比如是否超时（worker_shutdown_timeout）。

![动图封面](https://pic3.zhimg.com/v2-89e9068d77c7f035b0e184aefec4c452_b.jpg)



## 7.**WireShark抓包分析**

### **7.1WireShark设置**

首先下载一个WireShark ，设置好要抓取的 目标IP和端口，如下 。

![img](https://pic2.zhimg.com/80/v2-11aa7ac41f7b45d75f6b9118ebe06ca9_720w.webp)

点击开始

![img](https://pic1.zhimg.com/80/v2-4821c0a72a1f19d3ac8e7b953feb1564_720w.webp)

![img](https://pic2.zhimg.com/80/v2-dd62104afbdda595e9aef0db3bd2f415_720w.webp)

此时还没有流量，接下来我们访问下 nginx

![img](https://pic1.zhimg.com/80/v2-c2b8dcc221eba479722b00fb29e56cd8_720w.webp)

查看下wireshark

![img](https://pic1.zhimg.com/80/v2-4f022d05f2402e970251b6700b8570f4_720w.webp)

然后

![img](https://pic3.zhimg.com/80/v2-75e2b0a377fb0c2a95f6d586f563c2f2_720w.webp)

进行分析

### **7.2抓包分析**

本机IP

![img](https://pic1.zhimg.com/80/v2-dd624b499ce5e8d9124c99209e462f80_720w.webp)

- 浏览器首先会打开这个页面，本地打开了一个 50283端口，而 Nginx 启动的是 8888 端口。
- TCP 层主要做的是进程与进程之间通讯这件事。

![img](https://pic1.zhimg.com/80/v2-1cb85b2cf02d85f2a58d110ae6ea5090_720w.webp)

- IP 层主要解决机器与机器之间怎样互相找到的问题。

![img](https://pic2.zhimg.com/80/v2-5543d0fcf347175d0c264a6c44746d4d_720w.webp)

三次握手也就是 windows 先向 Nginx 发送了一次 [SYN]，那么相反的 Nginx 所在的服务器也会向 windows 发送一个 [SYN].

这个时候 Nginx 是没有感知到的，因为这个连接还是处于半打开的状态。直到这台 windows 服务器再次发送 [ACK] 到 Nginx 所在的服务器之上时，Nginx 所在的操作系统才会去通知 Nginx 我们收到了一个读事件，这个读事件对应是建立一个新连接，所以此时 Nginx 应该调用 Accept 方法去建立一个新的连接。

![img](https://pic3.zhimg.com/80/v2-cf7c3e23b69e32d0610beae89a6ff21e_720w.webp)

原文链接：https://zhuanlan.zhihu.com/p/583618780

# 【NO.16】Nginx源码分析与实践---编写一个简单的Http模块

本节通过亲自实践，写一个经典的Hello World模块来了解相应的流程是如何进行的。我们采用自上向下的方法，先让模块跑起来，再去向下分析模块的具体细节。

## 1.模块源码

准备工作：新建的模块名为ngx_http_mytest_module，于是在/home/zxin/nginx下新建个目录ngx_http_mytest_module。该目录下新建两个文件config，ngx_http_mytest_module.c。要将编写的模块加进nginx里，需要和nginx源码一起编译，因此，config就是告诉nginx如何将我们的模块编译进nginx。

ngx_http_mytest_module.c：

```
#include <ngx_core.h>
#include <ngx_http.h>
#include <ngx_config.h>
 
static char *ngx_http_mytest(ngx_conf_t *cf, ngx_command_t *cmd, void *conf);
static ngx_int_t ngx_http_mytest_handler(ngx_http_request_t *r);
 
/* 配置指令 */
static ngx_command_t ngx_http_mytest_commands[] = {
        {
             ngx_string("mytest"),
             NGX_HTTP_MAIN_CONF|NGX_HTTP_SRV_CONF|NGX_HTTP_LOC_CONF|NGX_HTTP_LMT_CONF|NGX_CONF_NOARGS,
             ngx_http_mytest,
             NGX_HTTP_LOC_CONF_OFFSET,
             0,
             NULL },
 
        ngx_null_command
};
 
/* 模块上下文 */
static ngx_http_module_t ngx_http_mytest_module_ctx = {
        NULL,
        NULL,
 
        NULL,
        NULL,
 
        NULL,
        NULL,
 
        NULL,
        NULL
};
 
/* 模块定义 */
ngx_module_t ngx_http_mytest_module = {
        NGX_MODULE_V1,
        &ngx_http_mytest_module_ctx,
        ngx_http_mytest_commands,
        NGX_HTTP_MODULE,
        NULL,
        NULL,
        NULL,
        NULL,
        NULL,
        NULL,
        NULL,
        NGX_MODULE_V1_PADDING
};
 
/* 挂载handler函数 */
static char *ngx_http_mytest(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
{
        ngx_http_core_loc_conf_t *clcf;
        clcf = ngx_http_conf_get_module_loc_conf(cf, ngx_http_core_module);
        clcf->handler = ngx_http_mytest_handler;
 
        return NGX_CONF_OK;
}
 
/* handler函数 */
static ngx_int_t ngx_http_mytest_handler(ngx_http_request_t *r)
{
        //必须是GET或HEAD方法
        if (!(r->method & (NGX_HTTP_GET|NGX_HTTP_HEAD)))
            return NGX_HTTP_NOT_ALLOWED;
 
        //丢弃请求包体
        ngx_int_t rc = ngx_http_discard_request_body(r);
        if (rc != NGX_OK) {
            return rc;
        }
 
        ngx_str_t type = ngx_string("text/plain");//响应的Content-Type
        ngx_str_t response = ngx_string("Hello World !");//响应的包体
 
        r->headers_out.status = NGX_HTTP_OK;//响应的状态码
        r->headers_out.content_length_n = response.len;//包体的长度
        r->headers_out.content_type = type;//响应的Content-Type
 
        rc = ngx_http_send_header(r);//发送响应头部
        if (rc == NGX_ERROR || rc > NGX_OK || r->header_only) {
            return rc;
        }
 
        ngx_buf_t *b;//构造ngx_buf_t结构，准备发送包体
        b = ngx_create_temp_buf(r->pool, response.len);
        if (b == NULL) {
            return NGX_HTTP_INTERNAL_SERVER_ERROR;
        }
 
        ngx_memcpy(b->pos, response.data, response.len);//将"Hello World !"拷贝到ngx_buf_t中
        b->last = b->pos + response.len;//设置last指针
        b->last_buf = 1;//声明是最后一块缓冲区
 
        ngx_chain_t out;//构造发送的ngx_chain_t结构
        out.buf = b;
        out.next = NULL;
 
        return ngx_http_output_filter(r, &out);//发送包体
}
```

config:

```
ngx_addon_name=ngx_http_mytest_module
HTTP_MODULES="$HTTP_MODULES ngx_http_mytest_module"
NGX_ADDON_SRCS="$NGX_ADDON_SRCS $ngx_addon_dir/ngx_http_mytest_module.c"
```

将模块编译进nginx内：

1.先进入目录：/usr/local/src/nginx-1.9.15

2.添加模块：./configure --add-module=/home/zxin/nginx/ngx_http_mytest_module

3.make

4.make install

然后修改我们的nginx.conf文件，在server{...}配置块内加入

```
location /test {
                mytest;
        }
```

重启nginx：

/usr/local/nginx/sbin/nginx -s reload

接着便可访问：[http://localhost/test](https://link.zhihu.com/?target=http%3A//localhost/test)

**相关视频推荐**

[手把手带你实现一个nginx模块，更加深入了解nginx（搭建好环境）](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1uU4y1j7i2/)

[16万行Nginx源码，就该这么读](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1hz411h7Go/)

[c/c++后台开发岗位，如何精进技术，8个维度来说清楚](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1gP411n7rJ/)

学习地址：**[c/c++ linux服务器开发/后台架构师](https://link.zhihu.com/?target=https%3A//ke.qq.com/course/417774%3FflowToken%3D1013300)**

需要C/C++ Linux服务器架构师学习资料加qun**[812855908](https://link.zhihu.com/?target=https%3A//jq.qq.com/%3F_wv%3D1027%26k%3DnVddKID0)**获取（资料包括**C/C++，Linux，golang技术，Nginx，ZeroMQ，MySQL，Redis，fastdfs，MongoDB，ZK，流媒体，CDN，P2P，K8S，Docker，TCP/IP，协程，DPDK，ffmpeg**等），免费分享

![img](https://pic3.zhimg.com/80/v2-0fb9145c6e395f48de7c1d3a1d500dc2_720w.webp)

## 2.实验结果

浏览器访问：

![img](https://pic4.zhimg.com/80/v2-62bedd02881737df53d74005b9f646ef_720w.webp)

linux curl 访问：

![img](https://pic1.zhimg.com/80/v2-5524fc1e9bcd9be8ec7992bc2fd40234_720w.webp)

## 3.模块分析

下面开始分析具体细节：

首先要知道nginx的配置是和模块相对应的，配置文件里有一个配置，则存在一个模块去处理这个配置。先看下面这幅流程图：

![img](https://pic3.zhimg.com/80/v2-77656ad2976f3bfc3a41bf8695f42d32_720w.webp)

1.在我们输入URL：http://localhost/test后，由HTTP框架去接收请求的HTTP包头，框架分析出包头的uri为/test，然后就去配置文件nginx.conf里去找相应的配置项/test。找到了我们的location块，和其/test相匹配。找到后则进入location，发现mytest配置项，则调用ngx_http_mytest_module模块去处理后面的请求。ngx_http_mytest_module模块处理完后，返回到HTTP框架，然后将响应返回给浏览器。下面看下模块内部是如何操作的。

2.模块和配置信息是紧密相关的。先看下配置指令相关的信息吧：

```
/* 配置指令 */
static ngx_command_t ngx_http_mytest_commands[] = {
        {
             ngx_string("mytest"),//配置项名称
             NGX_HTTP_MAIN_CONF|NGX_HTTP_SRV_CONF|NGX_HTTP_LOC_CONF|NGX_HTTP_LMT_CONF|NGX_CONF_NOARGS,//位置及参数
             ngx_http_mytest,//挂载handler函数
             NGX_HTTP_LOC_CONF_OFFSET,//配置项所在内存池
             0,//配置所在精确位置
             NULL },//一般为NULL
 
        ngx_null_command
};
```

我们用一个结构数组来存储配置信息，数组元素为ngx_command_t。

第一项：用函数ngx_string()来设置配置名称，这里的名称应和nginx.conf配置文件里自定义的配置项名称相同。模块就是通过这个名称检测到配置的。

第二项：说明了配置项可以出现在哪些配置块以及配置项后可以跟的参数的个数。

第三项：ngx_http_mytest是用来挂载handler函数的函数，框架在解析配置的时候，会将解析到的参数都传给ngx_http_mytest这个函数，在其内部调用真正的处理请求的函数。即在此函数内调用handler函数。

第四项：该字段指定当前配置项存储的内存位置。实际上是使用哪个内存池的问题。因为http模块对所有http模块所要保存的配置信息，划分了main, server和location三个地方进行存储，每个地方都有一个内存池用来分配存储这些信息的内存。

第五项：如果location配置块内有多个配置项，就可以利用此项来精确定位每一项在location内的具体位置。因为我们只有一个配置项，所以可设为0。

第六项：一般为NULL

3.下面看下在挂载函数内部是如何挂载handler函数的：

```
/* 挂载handler函数 */
static char *ngx_http_mytest(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
{
        ngx_http_core_loc_conf_t *clcf;
        clcf = ngx_http_conf_get_module_loc_conf(cf, ngx_http_core_module);
        clcf->handler = ngx_http_mytest_handler;
 
        return NGX_CONF_OK;
}
```

首先调用ngx_http_conf_get_module_loc_conf，找到mytest配置项所在的配置块。接下来去挂载我们的handler函数ngx_http_mytest_handler。HTTP框架在解析到请求与我们的配置块相匹配时，就会调用handler函数来处理请求。关于挂载函数的部分见： Nginx源码分析与实践---挂载函数

挂载函数的具体实现放最后再说，配置相关的内容说完了。接下来是模块相关的内容：

4.首先是模块上下文：

```
/* 模块上下文 */
static ngx_http_module_t ngx_http_mytest_module_ctx = {
        NULL,
        NULL,
 
        NULL,
        NULL,
 
        NULL,
        NULL,
 
        NULL,
        NULL
};
```

这是一个ngx_http_module_t类型的静态变量。这个变量实际上是提供一组回调函数指针，这些函数有在创建存储配置信息的对象的函数，也有在创建前和创建后会调用的函数。这些函数都将被nginx在合适的时间进行调用。

然后是最重要的模块本身：

```
/* 模块定义 */
ngx_module_t ngx_http_mytest_module = {
        NGX_MODULE_V1,
        &ngx_http_mytest_module_ctx,
        ngx_http_mytest_commands,
        NGX_HTTP_MODULE,
        NULL,
        NULL,
        NULL,
        NULL,
        NULL,
        NULL,
        NULL,
        NGX_MODULE_V1_PADDING
};
```

通过这个模块的定义，我们将模块本身、模块上下文以及配置指令就结合起来了。关于模块上下文和模块的详细介绍见：Nginx源码分析与实践---ngx_module_t/ngx_http_module_t

5.下面看下handler函数内部是如何处理请求的：

```
/* handler函数 */
static ngx_int_t ngx_http_mytest_handler(ngx_http_request_t *r)
{
        //必须是GET或HEAD方法
        if (!(r->method & (NGX_HTTP_GET|NGX_HTTP_HEAD)))
            return NGX_HTTP_NOT_ALLOWED;
 
        //丢弃请求包体
        ngx_int_t rc = ngx_http_discard_request_body(r);
        if (rc != NGX_OK) {
            return rc;
        }
 
        ngx_str_t type = ngx_string("text/plain");//响应的Content-Type
        ngx_str_t response = ngx_string("Hello World !");//响应的包体
 
        r->headers_out.status = NGX_HTTP_OK;//响应的状态码
        r->headers_out.content_length_n = response.len;//包体的长度
        r->headers_out.content_type = type;//响应的Content-Type
 
        rc = ngx_http_send_header(r);//发送响应头部
        if (rc == NGX_ERROR || rc > NGX_OK || r->header_only) {
            return rc;
        }
 
        ngx_buf_t *b;//构造ngx_buf_t结构，准备发送包体
        b = ngx_create_temp_buf(r->pool, response.len);
        if (b == NULL) {
            return NGX_HTTP_INTERNAL_SERVER_ERROR;
        }
 
        ngx_memcpy(b->pos, response.data, response.len);//将"Hello World !"拷贝到ngx_buf_t中
        b->last = b->pos + response.len;//设置last指针
        b->last_buf = 1;//声明是最后一块缓冲区
 
        ngx_chain_t out;//构造发送的ngx_chain_t结构
        out.buf = b;
        out.next = NULL;
 
        return ngx_http_output_filter(r, &out);//发送包体
}
```

请求的所有信息(如：方法、URI、协议版本号和头部等)都传给了结构体ngx_http_request_t，关于此结构体的详细信息见：Nginx源码分析与实践---ngx_http_request_t。该函数只处理GET和HEAD两种方法，其他方法不处理。请求由请求头和请求体组成，由于我们只需要请求头的信息，所以调用 ngx_http_discard_request_body(r)丢弃请求体。请求收到了，下面我们就可以构造响应的包头和包体了。响应的包头和包体要分别发送，r->headers_out成员是专门用于构造响应的包头的。该成员又是一个结构体，通过该结构体的成员，可设置包头的各项内容。包头构造结束后就调用ngx_http_send_header(r)来发回包头。对于包体的发送，我们是通过数据类型ngx_chain_t来完成的，而out是利用结构体ngx_buf_t来完成包体的存储的，我们将包体内容"Hello World !"存储到结构体b中，因此可以利用out来发送出去包体，构造好out后，便可调用函数ngx_http_output_filter(r, &out)来将包体发给浏览器。

## 4.总结

由上述讲解可知：模块开发最重要的就是配置信息和模块两部分的内容，它们是相对应的。正是因为Nginx将整个开发分为配置和模块两部分，使得我们在使用Nginx的时候，只用去简单修改下配置文件就可以得到我们想要的功能了。我们只需在配置文件中设置想要的配置项，剩下的Nginx全部会给你解决的很好的。当然前提是要有很好的模块支撑。看看源码src目录下的那些模块的实现，定义ngx_command_t、ngx_http_module_t、ngx_module_t这几个部分的内容肯定少不了的，这就是模块开发中的主要内容！

原文链接：https://zhuanlan.zhihu.com/p/583410850

# 【NO.17】三次握手时，客户端发送的 SYN 报文为什么会被丢弃？

## 1.**楔子**

我们知道当客户端和服务端建立连接时，会进行三次握手。客户端先向服务端发送 SYN 报文，表示想要建立连接，这是第一次握手；然后服务端收到 SYN 之后会给客户端回复 SYN + ACK，表示同意建立连接，也就是第二次握手。

![img](https://pic2.zhimg.com/80/v2-bcf301db24115a25c719013ac325946d_720w.webp)

但如果第二次握手的时候，服务端没有回复，那么说明客户端发送的 SYN 报文被服务端忽略了。

![img](https://pic3.zhimg.com/80/v2-fb0210147cdffa269e7644d2141b111a_720w.webp)

然后客户端在规定时间内，因收不到服务端的反馈就会触发超时，于是重传 SYN 报文，直到达到最大的重传次数。以上便是 SYN 报文被丢弃的过程，但问题是它为什么会被丢弃呢？主要有以下原因：

- 开启 tcp_tw_recycle 参数，并且处于 NAT 环境下；
- Accpet 队列满了；
- SYN 队列满了；

下面来解释一下。

## 2.**tcp_tw_recycle 参数**

TCP 四次挥手过程中，主动断开连接方会有一个 TIME_WAIT 状态，这个状态会持续 2 MSL 后才转变为 CLOSED 状态。

![img](https://pic4.zhimg.com/80/v2-4f3f8f1729f599e3db2e097e91a95627_720w.webp)

在 Linux 操作系统下，TIME_WAIT 状态的持续时间是 60 秒，你可以通过修改 Linux 源代码来改变这个值，但不推荐。

![img](https://pic3.zhimg.com/80/v2-e4d5ffe3a39b315917c682b5630e2006_720w.webp)

因此在 60 秒内，端口会一直被客户端占用。而端口资源是有限的，一般可开启的端口为 32768 ~ 60999。

![img](https://pic3.zhimg.com/80/v2-64d4cb416421f0cc97060ce0be29feb2_720w.webp)

当然你也可以修改这两个值，但总之如果主动断开连接方的 TIME_WAIT 状态过多，占满了所有端口资源，则会导致无法创建新连接。问题来了，既然 TIME_WAIT 有缺陷，那为什么还要保留这个特性呢？不用想，肯定是有着其它作用，而作用有两个：

- 防止旧的连接数据包；
- 保证连接正确关闭；

我们分别解释。

**原因一：防止旧的连接数据包**

假设 TIME-WAIT 没有等待时间或时间过短，被延迟的数据包抵达后会发生什么呢？

![img](https://pic1.zhimg.com/80/v2-cbf297f554c7c4d609d76cc639536d30_720w.webp)

如上图黄色框框显示的那样，服务端在关闭连接之前发送的 Seq = 301 报文，被网络延迟了。这时有相同端口的 TCP 连接被复用后，被延迟的 Seq = 301 抵达了客户端，那么客户端有可能正常接收这个过期的报文，这就会产生数据错乱等严重的问题。

所以 TCP 就设计出了这么一个机制，因为 1MSL 表示报文的最大生存时间，那么经过 2MSL 便可以让两个方向上的数据包在网络中都自然消失，那么再出现的数据包一定都是新建立连接所产生的。

**相关视频推荐**

[tcpip，accept，11个状态，细枝末节的秘密，还有哪些你不知道](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1gf4y1k78E/)

[通过10道经典网络面试题，搞懂tcp/ip协议栈所有知识点](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1oW4y1E7r3/)

[C/C++开发哪个方向更有前景，游戏，c++后端，网络处理，音视频开发，嵌入式开发，桌面开发](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1HV4y1578G/)

学习地址：**[c/c++ linux服务器开发/后台架构师](https://link.zhihu.com/?target=https%3A//jq.qq.com/%3F_wv%3D1027%26k%3DnzAMj4Qa)**

需要C/C++ Linux服务器架构师学习资料加qun**[812855908](https://link.zhihu.com/?target=https%3A//jq.qq.com/%3F_wv%3D1027%26k%3DT2C5XZV4)**获取（资料包括**C/C++，Linux，golang技术，Nginx，ZeroMQ，MySQL，Redis，fastdfs，MongoDB，ZK，流媒体，CDN，P2P，K8S，Docker，TCP/IP，协程，DPDK，ffmpeg**等），免费分享

![img](https://pic3.zhimg.com/80/v2-0fb9145c6e395f48de7c1d3a1d500dc2_720w.webp)

**原因二：保证连接正确关闭**

其实在四次挥手示意图中应该就能发现端倪，我们知道服务端在传输完数据之后会发送 FIN 表示正式关闭连接，然后处于 LAST_ACK 状态，等待客户端的最后一次确认。

如果客户端再发送一次 ACK 给服务端，那么服务端收到之后就会进入 CLOSED 状态，但问题是这最后一次 ACK 报文如果在网络中丢失了该怎么办？

如果没有 TIME_WAIT，那么客户端把 ACK 报文发送之后就进入 CLOSED 了，但 ACK 报文并没有到达服务端，这时服务端就会一直处于 LAST_ACK 状态。如果后续客户端再发起新的建立连接的 SYN 报文后，服务端就不会再返回 SYN + ACK 了，而是直接发送 RST 报文表示终止连接的建立。

因此客户端在发送完 ACK 之后不能直接 CLOSED，而是要等一段时间。如果服务端在发「FIN 关闭连接报文」之后的规定时间内没有收到来自客户端的 ACK 报文，那么服务端就知道这个 ACK 报文在网络中丢失了，此时会重新给客户端发送 FIN 报文。

所以客户端要等待（此时处于 TIME_WAIT 状态），因为它不知道自己最后发送的 ACK 报文是否成功抵达服务端，它只知道服务端收不到 ACK 报文时，会再度给自己发送 FIN 报文，因此只能默默等待 2MSL（发送 ACK 加上当服务端收不到时返回 FIN，整个过程最多 2MSL）。

如果再次收到服务端的 FIN，那么它要再次发送 ACK；但如果等了 2MSL 后，服务端没有再次发送 FIN，那么它就知道自己上一次发送的 ACK 被服务端成功接收了，此时也会进入 CLOSED 状态。至此，四次挥手完成，客户端和服务端之间连接断开。

以上便是 TIME_WAIT 状态的作用，之所以单独花些时间介绍它，是因为 TIME_WAIT 是导火索。由于 TIME_WAIT 状态的连接过多会造成内存资源和本地端口资源的占用，所以 Linux 内核提供了两个系统参数来快速回收处于 TIME_WAIT 状态的连接，而这两个参数默认都是关闭的：

- 参数一：net.ipv4.tcp_tw_reuse，如果开启该选项的话，客户端（连接发起方） 在调用 connect() 函数时，内核会随机找一个 time_wait 状态超过 1 秒的连接给新的连接复用，所以该选项只适用于连接发起方；
- 参数二：net.ipv4.tcp_tw_recycle，如果开启该选项的话，允许处于 TIME_WAIT 状态的连接被快速回收；
- 要使得上面两个选项在开启之后能够生效，还有一个前提条件，就是要打开 TCP 时间戳，也就是将 net.ipv4.tcp_timestamps 设置为 1（默认为 1）；

![img](https://pic1.zhimg.com/80/v2-ac1fe99e407cada3ed5f72df76c7dd70_720w.webp)

但是重点来了，tcp_tw_recycle 在使用了 NAT 的网络下是不安全的。因为对于服务器来说，如果同时开启了 tcp_tw_recycle 和 tcp_timestamps 选项，则会额外开启一种称之为 per-host 的 PAWS 机制，正是这个机制导致了 SYN 报文可能出现丢失。

目前的信息量估计稍微有点大，我们先简单回顾一下什么是 NAT，然后再来介绍一下什么是 PAWS，最后再来说 per-host 的 PAWS。

## 3.**什么是 NAT**

这里需要先解释一下什么是 NAT，NAT 指的是网络地址转换，简单来说就是将内部网络的私有 IP 转成公有 IP。估计很多人分不清 NAT 和桥接（Bridged）之间的区别，我们以 VMware 为例来解释一遍。

提个问题，我们在使用 VMware 虚拟出一个 CentOS 之后，这个 CentOS 要如何连接外网呢？

***第一种模式：桥接模式***

VMware 在安装之后会创建一个虚拟网桥以及一个虚拟交换机，所有以桥接模式设置的虚拟机都会连接到虚拟交换机的一个接口上，共处于一个二层网络中。所以桥接下的网卡与网卡都是交换模式的，相互可以直接访问而不干扰。

而虚拟机虚拟的网卡和主机网卡通信则是需要借助虚拟网桥，所以在桥接模式下，虚拟机 IP 地址需要与主机在同一个网段。如果需要联网，则网关与 DNS 需要与主机网卡一致。其网络结构如下图所示：

![img](https://pic4.zhimg.com/80/v2-223d583be8c11dd669904cd79d21e7c7_720w.webp)

桥接模式下的多个虚拟机之间可以直接通信，而虚拟机和主机之间则要借助于虚拟网桥，因此虚拟机是可以直接 ping 通外网 ip 的（前提是主机可以）。

似乎桥接模式还是蛮不错的，但它有一个缺点，就是它要和主机在同一个网段，并且要为其分配一个独立的 IP。如果你当前网段的可使用 IP 不多或者对 IP 管理比较严格的话，那么桥接模式就不适用了，因为 IP 地址占用严重（每一个虚机都要有一个独立的 IP）。

所以就有了 NAT。

***第二种模式：NAT 模式***

上面说道，如果你的网络 IP 资源紧缺，但又希望你的虚拟机能够联网，这时候 NAT 模式是最好的选择。NAT 模式借助虚拟 NAT 设备和虚拟 DHCP 服务器，首先主机网卡直接与虚拟 NAT 设备相连，然后虚拟 NAT 设备和虚拟 DHCP 服务器、以及虚机网卡一起连接在虚拟交换机上。

因此，通过 NAT 设备和 DHCP，虚拟机会共用主机网卡实现对外上网，对外暴露的都是主机 IP。就类似于局域网内的 IP 会共用一个公网 IP 一样，背后用的同样是 NAT 技术，每个局域网内的 IP 只需要保证在当前局域网内不冲突即可。至于对外上网，用的是同一个公网 IP，并且局域网内的 IP 和外网 IP 可以不在同一个网段。

不是很复杂，这里不画图了。

多提一句，Docker 也是类似的，Docker 在安装之后会创建一个 docker0 虚拟网桥，默认网络模式下每个容器也都有各自的 Network NameSpace、并且会设置 IP 等。这些容器也在一个二层网络中，并且通过 docker0 网桥以及 iptables nat 表连接至主机网卡，和外网进行通信。

## 4.**什么是 PAWS 机制**

说完了 NAT 之后再来看看什么是 PAWS？

当 tcp_timestamps 选项开启之后， PAWS 机制会自动开启，它的作用是防止 TCP 包中的序列号发生绕回。正常来说每个 TCP 包都会有自己唯一的 Seq 号（序列号），出现 TCP 数据包重传的时候会复用 Seq 号，这样接收方能通过 Seq 号来判断数据包的唯一性，也能在重复收到某个数据包的时候判断数据是不是重传的，最关键的是还可以保证数据包按序接收、不会错乱。

但问题是 TCP 的这个 Seq 号是有限的，一共 32 bit，随着传输对的不断进行，Seq 会不断递增，当溢出之后从 0 开始再次依次递增。

所以当 Seq 号出现溢出后单纯通过 Seq 号无法标识数据包的唯一性，某个数据包延迟或因重发而延迟时可能导致连接传递的数据被破坏，比如：

![img](https://pic1.zhimg.com/80/v2-80eb8fe3e17eeece31adb1769dcf7cdc_720w.webp)

上图 Seq=A 数据包出现了重传，并在 Seq 号耗尽再次从 A 递增时，第一次发的 A 数据包延迟到达了 server。由于 TCP 会通过序列号来去除重复数据，那么两个 Seq=A 的包肯定会丢弃一个。这种情况下如果没有别的机制来保证，server 会认为延迟到达的 A 数据包是正确的而接收，反而是将正常的第三次发的 Seq 为 A 的数据包丢弃，造成数据传输错误。

PAWS 就是为了避免这个问题而产生的，在开启了 tcp_timestamps 选项的情况下，一台机器发的所有 TCP 包都会带上发送时的时间戳（根据 CPU tick 计算得到，放在 TCP option 中）。

PAWS 要求连接双方维护最近一次收到的数据包的时间戳（Recent TSval），每收到一个新数据包都会读取数据包中的时间戳值跟 Recent TSval 值做比较。如果发现收到的数据包中时间戳不是递增的，则表示该数据包是过期的，就会直接丢弃这个数据包。

对于上面图中的例子，如果有了 PAWS 机制，就能做到在收到 Delay 到达的 A 号数据包时，识别出它是个过期的数据包而将其丢掉，因为时间戳不递增。

## **5.什么是 per-host 的 PAWS 机制**

当同时开启了 tcp_tw_recycle 和 tcp_timestamps 时，就会开启一种叫 per-host 的 PAWS 机制。而从名字上就可以得出，per-host 是针对每一个 IP 做 PAWS 检查，不同 IP 之间的 PAWS 检查是独立的。

但如果客户端网络环境使用了 NAT，那么客户端环境的每一台机器通过 NAT 网关后，都会是相同的 IP 地址，那么在服务端看来就好像跟一个客户端打交道一样，无法区分出来。

举个例子，当客户端 A 通过 NAT 网关和服务器建立 TCP 连接，然后服务器主动关闭并且快速回收 TIME_WAIT 状态的连接后，客户端 B 也通过 NAT 网关和服务器建立 TCP 连接。注意客户端 A 和 客户端 B 因为经过相同的 NAT 网关，所以使用相同的 IP 地址与服务端建立 TCP 连接。如果客户端 B 的 timestamp 比客户端 A 的 timestamp 小，那么由于服务端的 per-host 的 PAWS 机制的作用，服务端就会丢弃客户端主机 B 发来的 SYN 包。

因此 tcp_tw_recycle 在使用了 NAT 的网络下是存在问题的，因为使用 NAT 之后 IP 地址相同，都是同一个公网 IP。当然，如果不是对每一个 IP 做 PAWS 检查，而是对 IP 加端口组合起来做 PAWS 检查，那么就不会存在这个问题了。

以上就是 SYN 丢弃的原因之一，回顾整个过程，我们先是介绍了 TIME_WAIT，然后引出了 tcp_tw_recycle，接着引出了 NAT, PAWS，最后引出了 per-host PAWS。

> tcp_tw_recycle 在 Linux 4.12 版本后，直接取消了这一参数。所以不存在开启 tcp_tw_recycle 来优化 TCP 这一说。

## **6.Accept 队列满了**

在 TCP 三次握手的时候，Linux 内核会维护两个队列，分别是：

- 半连接队列，也称 SYN 队列；
- 全连接队列，也称 Accept 队列；

服务端收到客户端发起的 SYN 请求后，内核会把该连接存储到半连接队列，并向客户端响应 SYN+ACK。接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，内核会从半连接队列里面将连接取出，然后添加到全连接队列，等待进程调用 accept 函数时把连接取出来。

![img](https://pic1.zhimg.com/80/v2-38fea32a06ed5325f1751c2ce584afc0_720w.webp)

所以整个过程如下：

- \1. 客户端发送 SYN 报文；
- \2. 服务端将连接插入到半连接队列；
- \3. 服务端向客户端返回 SYN + ACK；
- \4. 客户端收到之后再向服务端返回 ACK；
- \5. 服务端将连接从半连接队列中取出，移入全连接队列；
- \6. 进程调用 accept 函数，从全连接队列中取出已完成连接建立的 socket连接；

因此半连接队列（SYN 队列）用来存储 SYN_RECV 状态、未完成建立的连接，全连接队列（Accept 队列）用来存储 ESTABLISH 状态、已完成建立的连接。

而我们也可以很容易得出结论，客户端返回成功是在第二次握手之后，服务端 accept 成功是在三次握手之后，因为调用 accept 就相当于从全连接队列中取出一个连接和客户端进行通信。

那么如何查看 SYN 队列和 Accept 队列的大小呢？

- net.ipv4.tcp_max_syn_backlog：查看半连接队列长度；
- net.core.somaxconn：查看全连接队列的长度；

![img](https://pic4.zhimg.com/80/v2-702a46bf1cd8946a1db5c1079d782aaf_720w.webp)

Linux 一切皆文件，如果想要修改队列大小的话，直接修改相应的文件即可。当然准确来说：

- max(64, tcp_max_syn_backlog) 才是半连接队列的长度；
- min(backlog, somaxconn) 才是全连接队列的长度，这里的 backlog 就是我们编写 socket 代码时，在 listen 方法里面指定的值；

但是在服务端并发处理大量请求时，如果 TCP Accpet 队列过小，或者应用程序调用 accept 方法不及时，就会造成 Accpet 队列已满。这时后续的连接就会被丢弃，这样就会出现服务端请求数量上不去的现象。

![img](https://pic2.zhimg.com/80/v2-1198ed32013d00b53fbcd51d4943dfb9_720w.webp)

所以 Accept 队列满了，也会造成 SYN 报文的丢失。那么如何查看 Accept 队列是否已满呢？可以使用命令 ss -lnt 来查看，其中 -l 表示显示正在监听的 socket，-n 表示不解析服务名称，-t 表示只显示 tcp socket，同理 -u 表示只显示 udp socket。

![img](https://pic4.zhimg.com/80/v2-403e3a86317bfedd12ec9983ea8a0477_720w.webp)

- Recv-Q：当前 Accpet 队列的大小，也就是当前已完成三次握手并等待服务端 accept() 的 TCP 连接个数；
- Send-Q：当前 Accpet 队列的最大长度，我们以监听 8088 端口的 TCP 服务进程为例，输出结果说明了 Accpet 队列的最大长度为 50，显然在 listen 的时候指定了 50；

如果 Recv-Q 的大小超过 Send-Q，就说明发生了 Accpet 队列满的情况。要解决这个问题，我们可以：

- 调大 Accpet 队列的最大长度，调大的方式是通过增大 backlog 以及 somaxconn 的值；
- 检查系统或者代码为什么调用 accept() 不及时；

注意：使用 ss 命令获取 Recv-Q 和 Send-Q 时，连接一定要在 LISTEN 状态（通过参数 -l 指定），如果不是 LISTEN的话，那么 Recv-Q 和 Send-Q 就不再表示队列大小了。

![img](https://pic3.zhimg.com/80/v2-b1d8b226338110c7b6fedaddbfdc85a2_720w.webp)

代码位于 net/ipv4/tcp_diag.c 中，可以看一下。如果我们使用 ss 命令的时候指定 -l 参数的话：

![img](https://pic3.zhimg.com/80/v2-eea828d94066cadb3b05728d0fb46c5a_720w.webp)

此时显示状态为 ESTABLISHED 的连接，连接既然都已经建立了，那么 Recv-Q 和 Send-Q 就不再表示队列大小了。

再来补充一下，当全连接队列满了之后，除了丢弃连接还有没有其它的做法呢？实际上，丢弃连接只是 Linux 的默认行为，我们还可以选择向客户端发送 RST 复位报文，告诉客户端连接已经建立失败，而这取决于 tcp_abort_on_overflow 的值。

- 如果为 0：表示当全连接队列满了，server 会扔掉 client 发过来的 ack；
- 如果为 1：表示当全连接队列满了，那么 server 发送一个 reset 包给 client，表示废掉这个握手过程和这个连接；

如果要想知道客户端连接不上服务端，是不是服务端 TCP 全连接队列满的原因，那么可以把 tcp_abort_on_overflow 设置为 1。这时如果在客户端异常中可以看到很多 connection reset by peer 的错误，那么就可以证明是由于服务端 TCP 全连接队列溢出的问题。但不管设置为 0 还是 1，最终 SYN 报文都不会得到正常的应答。

> connection reset by peer 这个错误应该有很多人遇见吧，我在连接消息队列的时候就遇到过。

但通常情况下，应当把 tcp_abort_on_overflow 设置为 0，因为这样更有利于应对突发流量。

举个例子，当 TCP 全连接队列满导致服务器丢掉了 ACK，与此同时，客户端的连接状态却是 ESTABLISHED（因为它发生在第二次握手之后），进程就在建立好的连接上发送请求。只要服务器没有为请求回复 ACK，请求就会被多次重发。

如果服务器上的进程只是短暂的繁忙造成 Accept 队列满，那么当 TCP 全连接队列有空位时，再次收到的请求报文由于含有 ACK，仍然会触发服务器端成功建立连接。

所以 tcp_abort_on_overflow 设为 0 可以提高连接建立的成功率，只有你非常肯定 TCP 全连接队列会长期溢出时，才能设置为 1 以尽快通知客户端。

## 7.**SYN 队列满了**

因为 SYN 发送之后连接会先进入半连接队列，之后再进入全连接队列。如果全连接队列满了会导致 SYN 报文丢弃，那么半连接队列满了应该也会导致 SYN 报文被丢弃吧。答案是肯定的，我们先来看看什么情况下半连接队列会满。

![img](https://pic4.zhimg.com/80/v2-793e3cb726a18198902e7aed35ccc113_720w.webp)

当服务端收到来自客户端的 SYN 报文时，就会进入 SYN_RECV 状态，此时连接会进入半连接队列。但服务端发出去的 SYN + ACK 报文却迟迟得不到客户端的应答，久而久之就会占满服务端的半连接队列。而所谓的 SYN 攻击就是采用这种方式，短时间伪造大量不同 IP 地址的 SYN 报文，但是故意不回复 ACK，直到服务端的半连接队列已满，使得其不能为正常用户服务。

![img](https://pic2.zhimg.com/80/v2-504ea77e3c6c932cc10e1a02c6352559_720w.webp)

我们知道当收到客户端的 ACK 报文之后会将连接从半连接队列移除并放入到全连接队列，但是 SYN 攻击的特点就是在发送完 SYN 报文之后故意不发 ACK 报文，因此最终半连接队列会被塞满，全连接队列会为空。咦，那出现这种情况除了丢弃连接之外还有什么解决办法呢？答案是启动 cookie。

![img](https://pic2.zhimg.com/80/v2-7a640d4bd97d264c9e24a240108fae2d_720w.webp)

通过设置 net.ipv4.tcp_syncookies = 1 实现。

![img](https://pic2.zhimg.com/80/v2-b1f8e7b4ca42fe830fc1d251f5454201_720w.webp)

默认值就是 1，那么它的含义是什么呢？

- 当 SYN 队列满之后，后续服务器收到 SYN 包，不进入半连接队列；
- 而是根据当前状态计算出一个 cookie 值，再以 SYN + ACK 中序列号的形式返回客户端；
- 服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包中 cookie 值的合法性。如果合法，直接放入到全连接队列；
- 最后应用通过调用 socket 接口 accpet()，从全连接队列取出连接；

![img](https://pic1.zhimg.com/80/v2-a96c182ace6eef8a80bff0e6b3cb4090_720w.webp)

tcp_syncookies 参数的取值有三种，值为 0 时表示关闭该功能，2 表示无条件开启功能，而 1 则表示仅当 SYN 半连接队列放不下时，再启用它。

由于 syncookie 仅用于应对 SYN 泛洪攻击（攻击者恶意构造大量的 SYN 报文发送给服务器，造成 SYN 半连接队列溢出，导致正常客户端的连接无法建立），毕竟这种方式建立的连接，许多 TCP 特性都无法使用。所以应当把 tcp_syncookies 设置为 1，仅在队列满时再启用。

因此 SYN 队列满了也会丢弃 SYN 报文，但连接是可以建立的。当然除了这种方式，我们还可以适当增加半连接队列的大小，但是不能单纯增大 tcp_max_syn_backlog 的值，还需要一同增大 somaxconn 和 backlog，也就是增大全连接队列，只增大 tcp_max_syn_backlog 没有意义。

```
# 增大 tcp_max_syn_backlog
echo 1024 > /proc/sys/net/ipv4/tcp_max_syn_backlog
# 增大 somaxconn
echo 1024 > /proc/sys/net/ipv4/somaxconn
```

至于 backlog 则是在应用程序中设置，比如 Python 是在调用 listen 方法的时候设置：

![img](https://pic1.zhimg.com/80/v2-63889b9226585201440af89a8de93400_720w.webp)

再比如 Nginx，是在配置文件中设置（配置完之后要重启 Nginx）：

```
server {
    listen 8088 default backlog=1024;
    server_name localhost;
    ......
}
```

另外当服务端受到 SYN 攻击时，就会有大量处于 SYN_RECV 状态的 TCP 连接，处于这个状态的 TCP 会重传 SYN+ACK ，当重传超过次数达到上限后，就会断开连接。而这个重传次数默认为 5：

![img](https://pic1.zhimg.com/80/v2-619b5808475fa10eaabef96a80ddee88_720w.webp)

那么针对 SYN 攻击的场景，我们可以减少 SYN+ACK 的重传次数，以加快处于 SYN_RECV 状态的 TCP 连接断开。

## 8.**小结**

以上就是关于 SYN 报文会在何时丢弃的相关内容，通过 SYN 报文丢弃我们引出了一些参数配置，以及半连接队列和全连接队列，并探讨了两个队列满了会引发什么后果、要如何处理等等。

当然私下里，建议没事也可以好好学习一下 TCP，该协议已经发展好几十年了，其性能已经做足了优化，对互联网的发展起到了举足轻重的作用，掌握它对我们的职业生涯是非常有帮助的。

原文链接：https://zhuanlan.zhihu.com/p/583279182

# 【NO.18】为什么要使用 TCP keepalive？C/C++代码实现TCP keepalive

为了理解 TCP keepalive的作用。我们需要清楚，当TCP的Peer A ，Peer B 两端建立了连接之后，如果一端突然拔掉网线或拔掉电源时，怎么检测到拔掉网线或者拔掉电源、链路不通？原因是在需要长连接的网络通信程序中，经常需要心跳检测机制，来实现检测对方是否在线或者维持网络连接的需要。

## **1.什么是 TCP 保活？**

当你建立一个 TCP 连接时，你关联了一组定时器。其中一些计时器处理保活过程。当保活计时器达到零时，向对等方发送一个保活探测数据包，其中没有数据并且 ACK 标志打开。

由于 TCP/IP 规范，可以这样做，作为一种重复的 ACK，并且远程端点将没有参数，因为 TCP 是面向流的协议。另一方面，将收到来自远程主机的回复，没有数据和ACK 集。

如果收到对 keepalive 探测的回复，则可以断言连接仍在运行。事实上，TCP 允许处理流，而不是数据包，因此零长度数据包对用户程序没有危险。

此过程很有用，因为如果其他对等方失去连接（例如通过重新启动），即使没有流量，也会注意到连接已断开。如果对等方未回复 keepalive 探测，可以断言连接不能被视为有效，然后采取正确的操作。

## **2.为什么要使用 TCP keepalive？**

> 1、检查死节点 2、 防止因网络不活动而断开连接

### **2.1检查死节点**

想一想 Peer A 和 Peer B 之间的简单 TCP 连接：初始的三次握手，从 A 到 B 的一个 SYN 段，从 B 到 A 的 SYN/ACK，以及从 A 到 B 的最终 ACK。

![img](https://pic1.zhimg.com/80/v2-867faba3a42f1bcdb407c4053cc63394_720w.webp)

此时，我们处于稳定状态：连接已建立，现在我们通常会等待有人通过通道发送数据。

那么问题来了：从 B 上拔下电源，它会立即断电，而不会通过网络发送任何信息来通知 A 连接将断开。

从它的角度来看，A 已准备好接收数据，并且不知道 B 已经崩溃。现在恢复B的电源，等待系统重启。A 和 B 现在又回来了，但是当 A 知道与 B 仍然处于活动状态的连接时，B 不知道。当 A 尝试通过死连接向 B 发送数据时，情况自行解决，B 回复 RST 数据包，导致 A 最终关闭连接。

```
    _____                                                     _____
   |     |                                                   |     |
   |  A  |                                                   |  B  |
   |_____|                                                   |_____|
      ^                                                         ^
      |--->--->--->-------------- SYN -------------->--->--->---|
      |---<---<---<------------ SYN/ACK ------------<---<---<---|
      |--->--->--->-------------- ACK -------------->--->--->---|
      |                                                         |
      |                                       system crash ---> X
      |
      |                                     system restart ---> ^
      |                                                         |
      |--->--->--->-------------- PSH -------------->--->--->---|
      |---<---<---<-------------- RST --------------<---<---<---|
      |                                                         |
```

Keepalive 可以告诉您何时无法访问另一个对等点，而不会出现误报的风险。

### **2.2防止因网络不活动而断开连接**

keepalive 的另一个有用目标是防止不活动断开通道。当你在 NAT 代理或防火墙后面时，无缘无故断开连接是一个非常常见的问题。这种行为是由代理和防火墙中实现的连接跟踪过程引起的，它们跟踪通过它们的所有连接。

它们跟踪通过它们的所有连接。由于这些机器的物理限制，它们只能在内存中保留有限数量的连接。最常见和合乎逻辑的策略是保持最新的连接并首先丢弃旧的和不活动的连接。

```
    _____           _____                                     _____
   |     |         |     |                                   |     |
   |  A  |         | NAT |                                   |  B  |
   |_____|         |_____|                                   |_____|
      ^               ^                                         ^
      |--->--->--->---|----------- SYN ------------->--->--->---|
      |---<---<---<---|--------- SYN/ACK -----------<---<---<---|
      |--->--->--->---|----------- ACK ------------->--->--->---|
      |               |                                         |
      |               | <--- connection deleted from table      |
      |               |                                         |
      |--->- PSH ->---| <--- invalid connection                 |
      |               |                                         |
```



## **3.Linux下使用TCP keepalive**

Linux 内置了对 keepalive 的支持。涉及 keepalive 的过程使用三个用户驱动的变量，可以使用 cat 查看参数值。

![img](https://pic1.zhimg.com/80/v2-228c38791ed25f830fb6c79418d8386c_720w.webp)

前两个参数以秒表示，最后一个是纯数字。这意味着keepalive 例程在发送第一个keepalive 探测之前等待两个小时（7200 秒），然后每75 秒重新发送一次。如果连续9次没有收到 ACK 响应，则连接被标记为断开。

修改这个值很简单，可以这样修改：

> echo 7000 > /proc/sys/net/ipv4/tcp_keepalive_time echo 40 > /proc/sys/net/ipv4/tcp_keepalive_intvl echo 10 > /proc/sys/net/ipv4/tcp_keepalive_probes

还有另一种访问内核变量的方法，使用 sysctl 命令

![img](https://pic3.zhimg.com/80/v2-87436a797e3363cf882e93c8773a56a2_720w.webp)

## **4.setsockopt 、getsockopt 函数调用**

在 Linux 操作系统中，我们可以通过代码启用一个 socket 的心跳检测，为特定套接字启用 keepalive 所需要做的就是在套接字本身上设置特定的套接字选项。函数原型如下：

```
int getsockopt(int sockfd, int level, int optname,
                      void *optval, socklen_t *optlen);

int setsockopt(int sockfd, int level, int optname,
                      const void *optval, socklen_t optlen);
                      
```

![img](https://pic1.zhimg.com/80/v2-2fd4c07d5c568c4f939dc66772e17f9c_720w.webp)

第一个参数是socket；第二个必须是 SOL_SOCKET，第三个必须是 SO_KEEPALIVE。第四个参数必须是布尔整数值，表示我们要启用该选项，而最后一个是之前传递的值的大小。

在编写应用程序时，还可以为 keepalive 设置其他三个套接字选项。它们都使用 SOL_TCP 级别而不是 SOL_SOCKET，并且它们仅针对当前套接字覆盖系统范围的变量。如果不先写入就读取，将返回当前系统范围的参数。

```
TCP_KEEPCNT：覆盖 tcp_keepalive_probes
TCP_KEEPIDLE：覆盖 tcp_keepalive_time
TCP_KEEPINTVL：覆盖 tcp_keepalive_intvl   
```

## **5.TCP keepalive 代码实现**

在写TCP keepalive 服务程序时，除了要处理SIGPIPE外，还要有客户端连接检测机制，用于及时发现崩溃的客户端连接。我们使用TCP的 keepalive 机制方式。

tcp_keepalive_client：

```
int main(int argc, char *argv[])
{
 kat_arg0 = basename(argv[0]);
 bzero(&cp, sizeof (cp));
 cp.cp_keepalive = 1;
 cp.cp_keepidle = -1;
 cp.cp_keepcnt = -1;
 cp.cp_keepintvl = -1;

 while ((c = getopt(argc, argv, ":c:d:i:")) != -1) {
  switch (c) {
  case 'c':
   cp.cp_keepcnt = parse_positive_int_option(
       optopt, optarg);
   break;

  case 'd':
   cp.cp_keepidle = parse_positive_int_option(
       optopt, optarg);
   break;

  case 'i':
   cp.cp_keepintvl = parse_positive_int_option(
       optopt, optarg);
   break;
  
  case ':':
   warnx("option requires an argument: -%c", optopt);
   usage();
   break;

  case '?':
   warnx("unrecognized option: -%c", optopt);
   usage();
   break;
  }
 }

 if (optind > argc - 1) {
  warnx("missing required arguments");
  usage();
 }

 ipport = argv[optind++];
 if (parse_ip4port(ipport, &cp.cp_ip) == -1) {
  warnx("invalid IP/port: \"%s\"", ipport);
  usage();
 }

 (void) fprintf(stderr, "going connect to: %s port %d\n",
     inet_ntoa(cp.cp_ip.sin_addr), ntohs(cp.cp_ip.sin_port));
 (void) fprintf(stderr, "set SO_KEEPALIVE  = %d\n", cp.cp_keepalive);
 (void) fprintf(stderr, "set TCP_KEEPIDLE  = %d\n", cp.cp_keepidle);
 (void) fprintf(stderr, "set TCP_KEEPCNT   = %d\n", cp.cp_keepcnt);
 (void) fprintf(stderr, "set TCP_KEEPINTVL = %d\n", cp.cp_keepintvl);
 rv = connectandwait(&cp);
 return (rv == 0 ? EXIT_SUCCESS : EXIT_FAILURE);
}
```

tcp_keepalive_server:

```
int main(int argc, char *argv[] )
{

   /* 创建套接字 */
   if((listen_sock = socket(PF_INET, SOCK_STREAM, IPPROTO_TCP)) < 0) {
      perror("socket()");
      exit(EXIT_FAILURE);
   }

   /* 检查 keepalive 选项的状态  */
   if(getsockopt(listen_sock, SOL_SOCKET, SO_KEEPALIVE, &optval, &optlen) < 0) {
      perror("getsockopt()");
      close(listen_sock);
      exit(EXIT_FAILURE);
   }
   printf("SO_KEEPALIVE default is %s\n", (optval ? "ON" : "OFF"));

   /* 将选项设置为活动  */
   optval = 1;
   optlen = sizeof(optval);
   
   if(setsockopt(listen_sock, SOL_SOCKET, SO_KEEPALIVE, &optval, optlen) < 0) {
      perror("setsockopt()");
      close(listen_sock);
      exit(EXIT_FAILURE);
   }
   printf("SO_KEEPALIVE set on socket\n");

   /* 再次检查状态  */
   if(getsockopt(listen_sock, IPPROTO_TCP, TCP_KEEPIDLE, &optval, &optlen) < 0) {
      perror("getsockopt()");
      close(listen_sock);
      exit(EXIT_FAILURE);
   }
   printf("TCP_KEEPIDLE is %d\n", optval );
      /* 再次检查状态  */
   if(getsockopt(listen_sock, IPPROTO_TCP, TCP_KEEPCNT, &optval, &optlen) < 0) {
      perror("getsockopt()");
      close(listen_sock);
      exit(EXIT_FAILURE);
   }
   printf("TCP_KEEPCNT is %d\n", optval);
      /* 再次检查状态  */
   if(getsockopt(listen_sock, IPPROTO_TCP, TCP_KEEPINTVL, &optval, &optlen) < 0) {
      perror("getsockopt()");
      close(listen_sock);
      exit(EXIT_FAILURE);
   }
   printf("TCP_KEEPINTVL is %d\n", optval );
  /* 初始化套接字结构 */
   bzero((char *) &serv_addr, sizeof(serv_addr));
   int portno = atoi(argv[1]);
   serv_addr.sin_family = AF_INET;
   serv_addr.sin_addr.s_addr = INADDR_ANY;
   serv_addr.sin_port = htons(portno);
 
  ...
 }  
```

![img](https://pic3.zhimg.com/80/v2-191350c3a091692dd7639a99c4f95a7e_720w.webp)

![img](https://pic4.zhimg.com/80/v2-7249343ab61572da61fba081b8a25c5f_720w.webp)

程序创建一个 TCP 套接字并将 SO_KEEPALIVE 套接字选项设置为 1。如果指定了“-c”、“-d”和“-i”选项中的任何一个，则设置 TCP_KEEPCNT、TCP_KEEPIDLE 和 TCP_KEEPINTVL 套接字选项 在相应选项参数的套接字上。

通过测试程序，我们可以使用tcpdump、或者tshark是命令行抓包工具，来分析KeepAlive。

> tshark -nn -i lo port 5050 tcpdump -nn -i lo port 5050

![img](https://pic4.zhimg.com/80/v2-1d3f9ab15ddbdb19ebd5cb17252265c3_720w.webp)

tcpdump -nn -i lo port 5050

![img](https://pic3.zhimg.com/80/v2-11cfeb52d1c4ee979f0128bbfa174bae_720w.webp)

整个keepalive过程很简单，就是client给server发送一个包，server返回给用户一个包。注意包内没有数据，只有ACK标识 被打开。

ps -aux | grep tcp_keepalive

![img](https://pic4.zhimg.com/80/v2-7ed343859db5b4becd9e1b6eaf6248bf_720w.webp)

## **6.总结**

keepalive 是一个设备向另一个设备发送的消息，用于检查两者之间的链路是否正在运行，或防止链路中断。

原文链接：https://zhuanlan.zhihu.com/p/582867947

# 【NO.19】C++中STL用法超详细总结（收藏级）

## 1. 什么是STL？

STL（Standard Template Library），即标准模板库，是一个具有工业强度的，高效的C++程序库。它被容纳于C++标准程序库（C++ Standard Library）中，是ANSI/ISO C++标准中最新的也是极具革命性的一部分。该库包含了诸多在计算机科学领域里所常用的基本数据结构和基本算法。为广大C++程序员们提供了一个可扩展的应用框架，高度体现了软件的可复用性。

STL的一个重要特点是数据结构和算法的分离。尽管这是个简单的概念，但这种分离确实使得STL变得非常通用。例如，由于STL的sort()函数是完全通用的，你可以用它来操作几乎任何数据集合，包括链表，容器和数组；

STL另一个重要特性是它不是面向对象的。为了具有足够通用性，STL主要依赖于模板而不是封装，继承和虚函数（多态性）——OOP的三个要素。你在STL中找不到任何明显的类继承关系。这好像是一种倒退，但这正好是使得STL的组件具有广泛通用性的底层特征。另外，由于STL是基于模板，内联函数的使用使得生成的代码短小高效；

从逻辑层次来看，在STL中体现了泛型化程序设计的思想，引入了诸多新的名词，比如像需求（requirements），概念（concept），模型（model），容器（container），算法（algorithmn），迭代子（iterator）等。与OOP（object-oriented programming）中的多态（polymorphism）一样，泛型也是一种软件的复用技术；

从实现层次看，整个STL是以一种类型参数化的方式实现的，这种方式基于一个在早先C++标准中没有出现的语言特性--模板（template）。

## 2. STL内容介绍

STL中六大组件：

容器（Container），是一种数据结构，如list，vector，和deques ，以模板类的方法提供。为了访问容器中的数据，可以使用由容器类输出的迭代器；

迭代器（Iterator），提供了访问容器中对象的方法。例如，可以使用一对迭代器指定list或vector中的一定范围的对象。迭代器就如同一个指针。事实上，C++的指针也是一种迭代器。但是，迭代器也可以是那些定义了operator*()以及其他类似于指针的操作符地方法的类对象；

算法（Algorithm），是用来操作容器中的数据的模板函数。例如，STL用sort()来对一个vector中的数据进行排序，用find()来搜索一个list中的对象，函数本身与他们操作的数据的结构和类型无关，因此他们可以在从简单数组到高度复杂容器的任何数据结构上使用；

仿函数（Functor）

适配器（Adaptor）

分配器（allocator）

### 2.1 容器

STL中的容器有队列容器和关联容器，容器适配器（congtainer adapters：stack,queue，priority queue），位集（bit_set），串包(string_package)等等。

（1）序列式容器（Sequence containers），每个元素都有固定位置－－取决于插入时机和地点，和元素值无关，vector、deque、list；

Vector：将元素置于一个动态数组中加以管理，可以随机存取元素（用索引直接存取），数组尾部添加或移除元素非常快速。但是在中部或头部安插元素比较费时；

Deque：是“double-ended queue”的缩写，可以随机存取元素（用索引直接存取），数组头部和尾部添加或移除元素都非常快速。但是在中部或头部安插元素比较费时；

List：双向链表，不提供随机存取（按顺序走到需存取的元素，O(n)），在任何位置上执行插入或删除动作都非常迅速，内部只需调整一下指针；

（2）关联式容器（Associated containers），元素位置取决于特定的排序准则，和插入顺序无关，set、multiset、map、multimap等。

Set/Multiset：内部的元素依据其值自动排序，Set内的相同数值的元素只能出现一次，Multisets内可包含多个数值相同的元素，内部由二叉树实现，便于查找；

Map/Multimap：Map的元素是成对的键值/实值，内部的元素依据其值自动排序，Map内的相同数值的元素只能出现一次，Multimaps内可包含多个数值相同的元素，内部由二叉树实现，便于查找；

容器类自动申请和释放内存，无需new和delete操作。

### 2.2 STL迭代器

Iterator（迭代器）模式又称Cursor（游标）模式，用于提供一种方法顺序访问一个聚合对象中各个元素, 而又不需暴露该对象的内部表示。或者这样说可能更容易理解：Iterator模式是运用于聚合对象的一种模式，通过运用该模式，使得我们可以在不知道对象内部表示的情况下，按照一定顺序（由iterator提供的方法）访问聚合对象中的各个元素。

迭代器的作用：能够让迭代器与算法不干扰的相互发展，最后又能无间隙的粘合起来，重载了*，＋＋，＝＝，！＝，＝运算符。用以操作复杂的数据结构，容器提供迭代器，算法使用迭代器；常见的一些迭代器类型：iterator、const_iterator、reverse_iterator和const_reverse_iterator.



### 2.3 算法

函数库对数据类型的选择对其可重用性起着至关重要的作用。举例来说，一个求方根的函数，在使用浮点数作为其参数类型的情况下的可重用性肯定比使用整型作为它的参数类性要高。而C++通过模板的机制允许推迟对某些类型的选择，直到真正想使用模板或者说对模板进行特化的时候，STL就利用了这一点提供了相当多的有用算法。它是在一个有效的框架中完成这些算法的——你可以将所有的类型划分为少数的几类，然后就可以在模版的参数中使用一种类型替换掉同一种类中的其他类型。

STL提供了大约100个实现算法的模版函数，比如算法for_each将为指定序列中的每一个元素调用指定的函数，stable_sort以你所指定的规则对序列进行稳定性排序等等。只要我们熟悉了STL之后，许多代码可以被大大的化简，只需要通过调用一两个算法模板，就可以完成所需要的功能并大大地提升效率。

算法部分主要由头文件<algorithm>，<numeric>和<functional>组成。

<algorithm>是所有STL头文件中最大的一个（尽管它很好理解），它是由一大堆模版函数组成的，可以认为每个函数在很大程度上都是独立的，其中常用到的功能范围涉及到比较、交换、查找、遍历操作、复制、修改、移除、反转、排序、合并等等。

<numeric>体积很小，只包括几个在序列上面进行简单数学运算的模板函数，包括加法和乘法在序列上的一些操作。

<functional>中则定义了一些模板类，用以声明函数对象。

STL中算法大致分为四类：

- 非可变序列算法：指不直接修改其所操作的容器内容的算法。
- 可变序列算法：指可以修改它们所操作的容器内容的算法。
- 排序算法：对序列进行排序和合并的算法、搜索算法以及有序序列上的集合操作。
- 数值算法：对容器内容进行数值计算。

以下对所有算法进行细致分类并标明功能：

**<一>查找算法(13个)：判断容器中是否包含某个值**

adjacent_find: 在iterator对标识元素范围内，查找一对相邻重复元素，找到则返回指向这对元素的第一个元素的 ForwardIterator。否则返回last。重载版本使用输入的二元操作符代替相等的判断。

binary_search: 在有序序列中查找value，找到返回true。重载的版本实用指定的比较函数对象或函数指针来判断相等。

count: 利用等于操作符，把标志范围内的元素与输入值比较，返回相等元素个数。

count_if: 利用输入的操作符，对标志范围内的元素进行操作，返回结果为true的个数。

equal_range: 功能类似equal，返回一对iterator，第一个表示lower_bound，第二个表示upper_bound。

find: 利用底层元素的等于操作符，对指定范围内的元素与输入值进行比较。当匹配时，结束搜索，返回该元素的 一个InputIterator。

find_end: 在指定范围内查找"由输入的另外一对iterator标志的第二个序列"的最后一次出现。找到则返回最后一对的第一 个ForwardIterator，否则返回输入的"另外一对"的第一个ForwardIterator。重载版本使用用户输入的操作符代 替等于操作。

find_first_of: 在指定范围内查找"由输入的另外一对iterator标志的第二个序列"中任意一个元素的第一次出现。重载版本中使 用了用户自定义操作符。

find_if: 使用输入的函数代替等于操作符执行find。

lower_bound: 返回一个ForwardIterator，指向在有序序列范围内的可以插入指定值而不破坏容器顺序的第一个位置。重载函 数使用自定义比较操作。

upper_bound: 返回一个ForwardIterator，指向在有序序列范围内插入value而不破坏容器顺序的最后一个位置，该位置标志 一个大于value的值。重载函数使用自定义比较操作。

search: 给出两个范围，返回一个ForwardIterator，查找成功指向第一个范围内第一次出现子序列(第二个范围)的位 置，查找失败指向last1。重载版本使用自定义的比较操作。

search_n: 在指定范围内查找val出现n次的子序列。重载版本使用自定义的比较操作。

**<二>排序和通用算法(14个)：提供元素排序策略**

inplace_merge: 合并两个有序序列，结果序列覆盖两端范围。重载版本使用输入的操作进行排序。

merge: 合并两个有序序列，存放到另一个序列。重载版本使用自定义的比较。

nth_element: 将范围内的序列重新排序，使所有小于第n个元素的元素都出现在它前面，而大于它的都出现在后面。重 载版本使用自定义的比较操作。

partial_sort: 对序列做部分排序，被排序元素个数正好可以被放到范围内。重载版本使用自定义的比较操作。

partial_sort_copy: 与partial_sort类似，不过将经过排序的序列复制到另一个容器。

partition: 对指定范围内元素重新排序，使用输入的函数，把结果为true的元素放在结果为false的元素之前。

random_shuffle: 对指定范围内的元素随机调整次序。重载版本输入一个随机数产生操作。

reverse: 将指定范围内元素重新反序排序。

reverse_copy: 与reverse类似，不过将结果写入另一个容器。

rotate: 将指定范围内元素移到容器末尾，由middle指向的元素成为容器第一个元素。

rotate_copy: 与rotate类似，不过将结果写入另一个容器。

sort: 以升序重新排列指定范围内的元素。重载版本使用自定义的比较操作。

stable_sort: 与sort类似，不过保留相等元素之间的顺序关系。

stable_partition: 与partition类似，不过不保证保留容器中的相对顺序。

**<三>删除和替换算法(15个)**

copy: 复制序列

copy_backward: 与copy相同，不过元素是以相反顺序被拷贝。

iter_swap: 交换两个ForwardIterator的值。

remove: 删除指定范围内所有等于指定元素的元素。注意，该函数不是真正删除函数。内置函数不适合使用remove和 remove_if函数。

remove_copy: 将所有不匹配元素复制到一个制定容器，返回OutputIterator指向被拷贝的末元素的下一个位置。

remove_if: 删除指定范围内输入操作结果为true的所有元素。

remove_copy_if: 将所有不匹配元素拷贝到一个指定容器。

replace: 将指定范围内所有等于vold的元素都用vnew代替。

replace_copy: 与replace类似，不过将结果写入另一个容器。

replace_if: 将指定范围内所有操作结果为true的元素用新值代替。

replace_copy_if: 与replace_if，不过将结果写入另一个容器。

swap: 交换存储在两个对象中的值。

swap_range: 将指定范围内的元素与另一个序列元素值进行交换。

unique: 清除序列中重复元素，和remove类似，它也不能真正删除元素。重载版本使用自定义比较操作。

unique_copy: 与unique类似，不过把结果输出到另一个容器。

**<四>排列组合算法(2个)：提供计算给定集合按一定顺序的所有可能排列组合**

next_permutation: 取出当前范围内的排列，并重新排序为下一个排列。重载版本使用自定义的比较操作。

prev_permutation: 取出指定范围内的序列并将它重新排序为上一个序列。如果不存在上一个序列则返回false。重载版本使用 自定义的比较操作。

**<五>算术算法(4个)**

accumulate: iterator对标识的序列段元素之和，加到一个由val指定的初始值上。重载版本不再做加法，而是传进来的 二元操作符被应用到元素上。

partial_sum: 创建一个新序列，其中每个元素值代表指定范围内该位置前所有元素之和。重载版本使用自定义操作代 替加法。

inner_product: 对两个序列做内积(对应元素相乘，再求和)并将内积加到一个输入的初始值上。重载版本使用用户定义 的操作。

adjacent_difference: 创建一个新序列，新序列中每个新值代表当前元素与上一个元素的差。重载版本用指定二元操作计算相 邻元素的差。

**<六>生成和异变算法(6个)**

fill: 将输入值赋给标志范围内的所有元素。

fill_n: 将输入值赋给first到first+n范围内的所有元素。

for_each: 用指定函数依次对指定范围内所有元素进行迭代访问，返回所指定的函数类型。该函数不得修改序列中的元素。

generate: 连续调用输入的函数来填充指定的范围。

generate_n: 与generate函数类似，填充从指定iterator开始的n个元素。

transform: 将输入的操作作用与指定范围内的每个元素，并产生一个新的序列。重载版本将操作作用在一对元素上，另外一 个元素来自输入的另外一个序列。结果输出到指定容器。

**<七>关系算法(8个)**

equal: 如果两个序列在标志范围内元素都相等，返回true。重载版本使用输入的操作符代替默认的等于操 作符。

includes: 判断第一个指定范围内的所有元素是否都被第二个范围包含，使用底层元素的<操作符，成功返回 true。重载版本使用用户输入的函数。

lexicographical_compare: 比较两个序列。重载版本使用用户自定义比较操作。

max: 返回两个元素中较大一个。重载版本使用自定义比较操作。

max_element: 返回一个ForwardIterator，指出序列中最大的元素。重载版本使用自定义比较操作。

min: 返回两个元素中较小一个。重载版本使用自定义比较操作。

min_element: 返回一个ForwardIterator，指出序列中最小的元素。重载版本使用自定义比较操作。

mismatch: 并行比较两个序列，指出第一个不匹配的位置，返回一对iterator，标志第一个不匹配元素位置。 如果都匹配，返回每个容器的last。重载版本使用自定义的比较操作。

<八>集合算法(4个)

set_union: 构造一个有序序列，包含两个序列中所有的不重复元素。重载版本使用自定义的比较操作。

set_intersection: 构造一个有序序列，其中元素在两个序列中都存在。重载版本使用自定义的比较操作。

set_difference: 构造一个有序序列，该序列仅保留第一个序列中存在的而第二个中不存在的元素。重载版本使用 自定义的比较操作。

set_symmetric_difference: 构造一个有序序列，该序列取两个序列的对称差集(并集-交集)。

<九>堆算法(4个)

make_heap: 把指定范围内的元素生成一个堆。重载版本使用自定义比较操作。

pop_heap: 并不真正把最大元素从堆中弹出，而是重新排序堆。它把first和last-1交换，然后重新生成一个堆。可使用容器的 back来访问被"弹出"的元素或者使用pop_back进行真正的删除。重载版本使用自定义的比较操作。

push_heap: 假设first到last-1是一个有效堆，要被加入到堆的元素存放在位置last-1，重新生成堆。在指向该函数前，必须先把 元素插入容器后。重载版本使用指定的比较操作。

sort_heap: 对指定范围内的序列重新排序，它假设该序列是个有序堆。重载版本使用自定义比较操作。

### 2.4 仿函数

**2.4.1 概述**

仿函数(functor)，就是使一个类的使用看上去象一个函数。其实现就是类中实现一个operator()，这个类就有了类似函数的行为，就是一个仿函数类了。

　　有些功能的的代码，会在不同的成员函数中用到，想复用这些代码。

1）公共的函数，可以，这是一个解决方法，不过函数用到的一些变量，就可能成为公共的全局变量，再说为了复用这么一片代码，就要单立出一个函数，也不是很好维护。

2）仿函数，写一个简单类，除了那些维护一个类的成员函数外，就只是实现一个operator()，在类实例化时，就将要用的，非参数的元素传入类中。

**2.4.2 仿函数(functor)在编程语言中的应用**

1）C语言使用函数指针和回调函数来实现仿函数，例如一个用来排序的函数可以这样使用仿函数

```
#include <stdio.h>
#include <stdlib.h>
//int sort_function( const void *a, const void *b);
int sort_function( const void *a, const void *b)
{   
	return *(int*)a-*(int*)b;
}
 
int main()
{
   
   int list[5] = { 54, 21, 11, 67, 22 };
   qsort((void *)list, 5, sizeof(list[0]), sort_function);//起始地址，个数，元素大小，回调函数 
   int  x;
   for (x = 0; x < 5; x++)
		  printf("%i\n", list[x]);
			      
   return 0;
}
```

2）在C++里，我们通过在一个类中重载括号运算符的方法使用一个函数对象而不是一个普通函数。

```
#include <iostream>
#include <algorithm>
 
using namespace std;
template<typename T>
class display
{
public:
	void operator()(const T &x)
	{
		cout << x << " ";
	}
};
int main()
{
	int ia[] = { 1,2,3,4,5 };
	for_each(ia, ia + 5, display<int>());
	system("pause");
	return 0;
}
```

**2.4.3 仿函数在STL中的定义**

要使用STL内建的仿函数，必须包含<functional>头文件。而头文件中包含的仿函数分类包括

1）算术类仿函数

加：plus<T>

减：minus<T>

乘：multiplies<T>

除：divides<T>

模取：modulus<T>

否定：negate<T>

例子：

```
#include <iostream>
#include <numeric>
#include <vector> 
#include <functional> 
using namespace std;
 
int main()
{
	int ia[] = { 1,2,3,4,5 };
	vector<int> iv(ia, ia + 5);
	//120
	cout << accumulate(iv.begin(), iv.end(), 1, multiplies<int>()) << endl;
	//15
	cout << multiplies<int>()(3, 5) << endl;
 
	modulus<int>  modulusObj;
	cout << modulusObj(3, 5) << endl; // 3 
	system("pause");
	return 0;
}
```

2）关系运算类仿函数

等于：equal_to<T>

不等于：not_equal_to<T>

大于：greater<T>

大于等于：greater_equal<T>

小于：less<T>

小于等于：less_equal<T>

从大到小排序：

```
#include <iostream>
#include <algorithm>
#include<functional>
#include <vector> 
 
using namespace std;
 
template <class T>
class display
{
public:
	void operator()(const T &x)
	{
		cout << x << " ";
	}
};
int main()
{
	int ia[] = { 1,5,4,3,2 };
	vector<int> iv(ia, ia + 5);
	sort(iv.begin(), iv.end(), greater<int>());
	for_each(iv.begin(), iv.end(), display<int>());
	system("pause");
	return 0;
}
```

3）逻辑运算仿函数

逻辑与：logical_and<T>

逻辑或：logical_or<T>

逻辑否：logical_no<T>

除了使用STL内建的仿函数，还可使用自定义的仿函数，具体实例见文章3.4.7.2小结

### 2.5 容器适配器

标准库提供了三种顺序容器适配器：queue(FIFO队列)、priority_queue(优先级队列)、stack(栈)

什么是容器适配器

适配器是使一种事物的行为类似于另外一种事物行为的一种机制”，适配器对容器进行包装，使其表现出另外一种行为。例 如，stack<int, vector<int> >实现了栈的功能，但其内部使用顺序容器vector<int>来存储数据。（相当于是vector<int>表现出 了栈的行为）。

容器适配器

要使用适配器，需要加入一下头文件：

\#include <stack> //stack

\#include<queue> //queue、priority_queue

![img](https://pic2.zhimg.com/80/v2-42e902c425fdcdb4d0490edb8a35fa45_720w.webp)

- 定义适配器

1、初始化

stack<int> stk(dep);

2、覆盖默认容器类型

stack<int,vector<int> > stk;

- 使用适配器

#### 2.5.1 stack

```
stack<int> s;
stack< int, vector<int> > stk;  //覆盖基础容器类型，使用vector实现stk
s.empty();  //判断stack是否为空，为空返回true，否则返回false
s.size();   //返回stack中元素的个数
s.pop();    //删除栈顶元素，但不返回其值
s.top();    //返回栈顶元素的值，但不删除此元素
s.push(item);   //在栈顶压入新元素item
```

实例：括号匹配

```
#include<iostream>
#include<cstdio>
#include<string>
#include<stack>
using namespace std;
int main()
{
	string s;
	stack<char> ss;
	while (cin >> s) 
	{
		bool flag = true;
		for (char c : s)  //C++11新标准，即遍历一次字符串s
		{
			if (c == '(' || c == '{' || c == '[')
			{
				ss.push(c);
				continue;
			}
			if (c == '}')
			{
				if (!ss.empty() && ss.top() == '{')
				{
					ss.pop();
					continue;
				}
				else
				{
					flag = false;
					break;
				}					
			}
			if (!ss.empty() && c == ']')
			{
				if (ss.top() == '[')
				{
					ss.pop();
					continue;
				}
				else
				{
					flag = false;
					break;
				}
			}
			if (!ss.empty() && c == ')')
			{
				if (ss.top() == '(')
				{
					ss.pop();
					continue;
				}
				else
				{
					flag = false;
					break;
				}
			}
		}
		if (flag)	cout << "Match!" << endl;
		else    cout << "Not Match!" << endl;
	}
}
```

**2.5.2 queue & priority_queue**

```
queue<int> q; //priority_queue<int> q;
q.empty();  //判断队列是否为空
q.size();   //返回队列长度
q.push(item);   //对于queue，在队尾压入一个新元素
               //对于priority_queue，在基于优先级的适当位置插入新元素
 
//queue only:
q.front();  //返回队首元素的值，但不删除该元素
q.back();   //返回队尾元素的值，但不删除该元素
 
//priority_queue only:
q.top();    //返回具有最高优先级的元素值，但不删除该元素
```

## 3 .常用容器用法介绍

### 3.1 vector

#### 3.1.1 基本函数实现

##### **1.构造函数**

- vector():创建一个空vector
- vector(int nSize):创建一个vector,元素个数为nSize
- vector(int nSize,const t& t):创建一个vector，元素个数为nSize,且值均为t
- vector(const vector&):复制构造函数
- vector(begin,end):复制[begin,end)区间内另一个数组的元素到vector中

##### **2.增加函数**

- void push_back(const T& x):向量尾部增加一个元素X
- iterator insert(iterator it,const T& x):向量中迭代器指向元素前增加一个元素x
- iterator insert(iterator it,int n,const T& x):向量中迭代器指向元素前增加n个相同的元素x
- iterator insert(iterator it,const_iterator first,const_iterator last):向量中迭代器指向元素前插入另一个相同类型向量的[first,last)间的数据

##### **3.删除函数**

- iterator erase(iterator it):删除向量中迭代器指向元素
- iterator erase(iterator first,iterator last):删除向量中[first,last)中元素
- void pop_back():删除向量中最后一个元素
- void clear():清空向量中所有元素

##### **4.遍历函数**

- reference at(int pos):返回pos位置元素的引用
- reference front():返回首元素的引用
- reference back():返回尾元素的引用
- iterator begin():返回向量头指针，指向第一个元素
- iterator end():返回向量尾指针，指向向量最后一个元素的下一个位置
- reverse_iterator rbegin():反向迭代器，指向最后一个元素
- reverse_iterator rend():反向迭代器，指向第一个元素之前的位置

##### **5.判断函数**

- bool empty() const:判断向量是否为空，若为空，则向量中无元素

##### **6.大小函数**

- int size() const:返回向量中元素的个数
- int capacity() const:返回当前向量张红所能容纳的最大元素值
- int max_size() const:返回最大可允许的vector元素数量值

##### **7.其他函数**

- void swap(vector&):交换两个同类型向量的数据
- void assign(int n,const T& x):设置向量中第n个元素的值为x
- void assign(const_iterator first,const_iterator last):向量中[first,last)中元素设置成当前向量元素

##### **8.看着清楚**

1.push_back 在数组的最后添加一个数据

2.pop_back 去掉数组的最后一个数据

[.at-Domain Parked](https://link.zhihu.com/?target=http%3A//3.at) 得到编号位置的数据

4.begin 得到数组头的指针

5.end 得到数组的最后一个单元+1的指针

6．front 得到数组头的引用

7.back 得到数组的最后一个单元的引用

8.max_size 得到vector最大可以是多大

9.capacity 当前vector分配的大小

10.size 当前使用数据的大小

11.resize 改变当前使用数据的大小，如果它比当前使用的大，者填充默认值

12.reserve 改变当前vecotr所分配空间的大小

13.erase 删除指针指向的数据项

14.clear 清空当前的vector

15.rbegin 将vector反转后的开始指针返回(其实就是原来的end-1)

16.rend 将vector反转构的结束指针返回(其实就是原来的begin-1)

17.empty 判断vector是否为空

18.swap 与另一个vector交换数据

#### **3.1.2 基本用法**

```
#include < vector> 
using namespace std;
```

#### **3.1.3 简单介绍**

- Vector<类型>标识符
- Vector<类型>标识符(最大容量)
- Vector<类型>标识符(最大容量,初始所有值)
- Int i[5]={1,2,3,4,5}
- Vector<类型>vi(I,i+2);//得到i索引值为3以后的值
- Vector< vector< int> >v; 二维向量//这里最外的<>要有空格。否则在比较旧的编译器下无法通过

#### **3.1.4 实例**

##### **3.1.4.1 pop_back()&push_back(elem)实例在容器最后移除和插入数据**

```
#include <string.h>
#include <vector>
#include <iostream>
using namespace std;
 
int main()
{
    vector<int>obj;//创建一个向量存储容器 int
    for(int i=0;i<10;i++) // push_back(elem)在数组最后添加数据 
    {
        obj.push_back(i);
        cout<<obj[i]<<",";    
    }
 
    for(int i=0;i<5;i++)//去掉数组最后一个数据 
    {
        obj.pop_back();
    }
 
    cout<<"\n"<<endl;
 
    for(int i=0;i<obj.size();i++)//size()容器中实际数据个数 
    {
        cout<<obj[i]<<",";
    }
 
    return 0;
}
```

输出结果为：

```
0,1,2,3,4,5,6,7,8,9,
 
0,1,2,3,4,
```

##### **3.1.4.2 clear()清除容器中所有数据**

```
#include <string.h>
#include <vector>
#include <iostream>
using namespace std;
 
int main()
{
    vector<int>obj;
    for(int i=0;i<10;i++)//push_back(elem)在数组最后添加数据 
    {
        obj.push_back(i);
        cout<<obj[i]<<",";
    }
 
    obj.clear();//清除容器中所以数据
    for(int i=0;i<obj.size();i++)
    {
        cout<<obj[i]<<endl;
    }
 
    return 0;
}
```

输出结果为：

```
0,1,2,3,4,5,6,7,8,9,
```

##### **3.1.4.3 排序**

```
#include <string.h>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;
 
int main()
{
    vector<int>obj;
 
    obj.push_back(1);
    obj.push_back(3);
    obj.push_back(0);
 
    sort(obj.begin(),obj.end());//从小到大
 
    cout<<"从小到大:"<<endl;
    for(int i=0;i<obj.size();i++)
    {
        cout<<obj[i]<<",";  
    } 
 
    cout<<"\n"<<endl;
 
    cout<<"从大到小:"<<endl;
    reverse(obj.begin(),obj.end());//从大到小 
    for(int i=0;i<obj.size();i++)
    {
        cout<<obj[i]<<",";
    }
    return 0;
}
```

输出结果为：

```
从小到大:
0,1,3,
 
从大到小:
3,1,0,
```

1.注意 sort 需要头文件 **#include <algorithm>**

2.如果想 sort 来降序，可重写 sort

```
bool compare(int a,int b) 
{ 
    return a< b; //升序排列，如果改为return a>b，则为降序 
} 
int a[20]={2,4,1,23,5,76,0,43,24,65},i; 
for(i=0;i<20;i++) 
    cout<< a[i]<< endl; 
sort(a,a+20,compare);
```

##### **3.1.4.4 访问（直接数组访问&迭代器访问）**

```
#include <string.h>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;
 
int main()
{
    //顺序访问
    vector<int>obj;
    for(int i=0;i<10;i++)
    {
        obj.push_back(i);   
    } 
 
    cout<<"直接利用数组："; 
    for(int i=0;i<10;i++)//方法一 
    {
        cout<<obj[i]<<" ";
    }
 
    cout<<endl; 
    cout<<"利用迭代器：" ;
    //方法二，使用迭代器将容器中数据输出 
    vector<int>::iterator it;//声明一个迭代器，来访问vector容器，作用：遍历或者指向vector容器的元素 
    for(it=obj.begin();it!=obj.end();it++)
    {
        cout<<*it<<" ";
    }
    return 0;
}
```

输出结果为：

```
直接利用数组：0 1 2 3 4 5 6 7 8 9 
利用迭代器：0 1 2 3 4 5 6 7 8 9
```

##### **3.1.4.5 二维数组两种定义方法（结果一样）**

**方法一**

```
#include <string.h>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;
 
 
int main()
{
    int N=5, M=6; 
    vector<vector<int> > obj(N); //定义二维动态数组大小5行 
    for(int i =0; i< obj.size(); i++)//动态二维数组为5行6列，值全为0 
    { 
        obj[i].resize(M); 
    } 
 
    for(int i=0; i< obj.size(); i++)//输出二维动态数组 
    {
        for(int j=0;j<obj[i].size();j++)
        {
            cout<<obj[i][j]<<" ";
        }
        cout<<"\n";
    }
    return 0;
}
```

**方法二**

```
#include <string.h>
#include <vector>
#include <iostream>
#include <algorithm>
using namespace std;
 
 
int main()
{
    int N=5, M=6; 
    vector<vector<int> > obj(N, vector<int>(M)); //定义二维动态数组5行6列 
 
    for(int i=0; i< obj.size(); i++)//输出二维动态数组 
    {
        for(int j=0;j<obj[i].size();j++)
        {
            cout<<obj[i][j]<<" ";
        }
        cout<<"\n";
    }
    return 0;
}
```

输出结果为：

```
0 0 0 0 0 0 
0 0 0 0 0 0 
0 0 0 0 0 0 
0 0 0 0 0 0 
0 0 0 0 0 0 
```

### 3.2 deque

所谓的deque是”double ended queue”的缩写，双端队列不论在尾部或头部插入元素，都十分迅速。而在中间插入元素则会比较费时，因为必须移动中间其他的元素。双端队列是一种随机访问的数据类型，提供了在序列两端快速插入和删除操作的功能，它可以在需要的时候改变自身大小，完成了标准的C++数据结构中队列的所有功能。

Vector是单向开口的连续线性空间，deque则是一种双向开口的连续线性空间。deque对象在队列的两端放置元素和删除元素是高效的，而向量vector只是在插入序列的末尾时操作才是高效的。deque和vector的最大差异，一在于deque允许于常数时间内对头端进行元素的插入或移除操作，二在于deque没有所谓的capacity观念，因为它是动态地以分段连续空间组合而成，随时可以增加一段新的空间并链接起来。换句话说，像vector那样“因旧空间不足而重新配置一块更大空间，然后复制元素，再释放旧空间”这样的事情在deque中是不会发生的。也因此，deque没有必要提供所谓的空间预留（reserved）功能。

虽然deque也提供Random Access Iterator，但它的迭代器并不是普通指针，其复杂度和vector不可同日而语，这当然涉及到各个运算层面。因此，除非必要，我们应尽可能选择使用vector而非deque。对deque进行的排序操作，为了最高效率，可将deque先完整复制到一个vector身上，将vector排序后（利用STL的sort算法），再复制回deque。

deque是一种优化了的对序列两端元素进行添加和删除操作的基本序列容器。通常由一些独立的区块组成，第一区块朝某方向扩展，最后一个区块朝另一方向扩展。它允许较为快速地随机访问但它不像vector一样把所有对象保存在一个连续的内存块，而是多个连续的内存块。并且在一个映射结构中保存对这些块以及顺序的跟踪。

#### **3.2.1 声明deque容器**

```
#include<deque>  // 头文件
deque<type> deq;  // 声明一个元素类型为type的双端队列que
deque<type> deq(size);  // 声明一个类型为type、含有size个默认值初始化元素的的双端队列que
deque<type> deq(size, value);  // 声明一个元素类型为type、含有size个value元素的双端队列que
deque<type> deq(mydeque);  // deq是mydeque的一个副本
deque<type> deq(first, last);  // 使用迭代器first、last范围内的元素初始化deq
```

#### **3.2.2 deque的常用成员函数**

```
deque<int> deq;
```

1. deq[ ]：用来访问双向队列中单个的元素。
2. deq.front()：返回第一个元素的引用。
3. deq.back()：返回最后一个元素的引用。
4. deq.push_front(x)：把元素x插入到双向队列的头部。
5. deq.pop_front()：弹出双向队列的第一个元素。
6. deq.push_back(x)：把元素x插入到双向队列的尾部。
7. deq.pop_back()：弹出双向队列的最后一个元素。

#### **3.2.3 deque的一些特点**

1. 支持随机访问，即支持[ ]以及at()，但是性能没有vector好。
2. 可以在内部进行插入和删除操作，但性能不及list。
3. deque两端都能够快速插入和删除元素，而vector只能在尾端进行。
4. deque的元素存取和迭代器操作会稍微慢一些，因为deque的内部结构会多一个间接过程。
5. deque迭代器是特殊的智能指针，而不是一般指针，它需要在不同的区块之间跳转。
6. deque可以包含更多的元素，其max_size可能更大，因为不止使用一块内存。
7. deque不支持对容量和内存分配时机的控制。
8. 在除了首尾两端的其他地方插入和删除元素，都将会导致指向deque元素的任何pointers、references、iterators失效。不过，deque的内存重分配优于vector，因为其内部结构显示不需要复制所有元素。
9. deque的内存区块不再被使用时，会被释放，deque的内存大小是可缩减的。不过，是不是这么做以及怎么做由实际操作版本定义。
10. deque不提供容量操作：capacity()和reverse()，但是vector可以。

#### **3.2.4 实例**

```
#include<iostream>
#include<stdio.h>
#include<deque>
using namespace std;
int main(void)
{
	int i;
	int a[10] = { 0,1,2,3,4,5,6,7,8,9 };
	deque<int> q;
	for (i = 0; i <= 9; i++)
	{
		if (i % 2 == 0)
			q.push_front(a[i]);
		else
			q.push_back(a[i]);
	}                                  /*此时队列里的内容是: {8,6,4,2,0,1,3,5,7,9}*/
	q.pop_front();
	printf("%d\n", q.front());    /*清除第一个元素后输出第一个(6)*/
	q.pop_back();
	printf("%d\n", q.back());     /*清除最后一个元素后输出最后一个(7)*/
	deque<int>::iterator it;
	for (it = q.begin(); it != q.end(); it++) {
		cout << *it << '\t';
	}
	cout << endl;
	system("pause");
	return 0;
}
```

输出结果：

![img](https://pic3.zhimg.com/80/v2-7847a25fd36a9ddd42746d894810839e_720w.webp)

### 3.3 list

#### 3.3.1 list定义

List是stl实现的双向链表，与向量(vectors)相比, 它允许快速的插入和删除，但是随机访问却比较慢。使用时需要添加头文件

\#include <list>

#### 3.3.2 list定义和初始化

list<int>lst1; //创建空list

list<int> lst2(5); //创建含有5个元素的list

list<int>lst3(3,2); //创建含有3个元素的list

list<int>lst4(lst2); //使用lst2初始化lst4

list<int>lst5(lst2.begin(),lst2.end()); //同lst4

#### **3.3.3 list常用操作函数**

Lst1.assign() 给list赋值

Lst1.back() 返回最后一个元素

Lst1.begin() 返回指向第一个元素的迭代器

Lst1.clear() 删除所有元素

Lst1.empty() 如果list是空的则返回true

Lst1.end() 返回末尾的迭代器

Lst1.erase() 删除一个元素

Lst1.front() 返回第一个元素

Lst1.get_allocator() 返回list的配置器

Lst1.insert() 插入一个元素到list中

Lst1.max_size() 返回list能容纳的最大元素数量

Lst1.merge() 合并两个list

Lst1.pop_back() 删除最后一个元素

Lst1.pop_front() 删除第一个元素

Lst1.push_back() 在list的末尾添加一个元素

Lst1.push_front() 在list的头部添加一个元素

Lst1.rbegin() 返回指向第一个元素的逆向迭代器

Lst1.remove() 从list删除元素

Lst1.remove_if() 按指定条件删除元素

Lst1.rend() 指向list末尾的逆向迭代器

Lst1.resize() 改变list的大小

Lst1.reverse() 把list的元素倒转

Lst1.size() 返回list中的元素个数

Lst1.sort() 给list排序

Lst1.splice() 合并两个list

Lst1.swap() 交换两个list

Lst1.unique() 删除list中相邻重复的元素

#### 3.3.4 List使用实例

##### **3.3.4.1 迭代器遍历list**

```
  for(list<int>::const_iteratoriter = lst1.begin();iter != lst1.end();iter++)
    {
      cout<<*iter;
    }
    cout<<endl;
```

##### **3.3.4.2 综合实例1**

```
#include <iostream>
#include <list>
#include <numeric>
#include <algorithm>
using namespace std;
 
typedef list<int> LISTINT;
typedef list<int> LISTCHAR;
 
void main()
{
	//用LISTINT创建一个list对象
	LISTINT listOne;
	//声明i为迭代器
	LISTINT::iterator i;
 
	listOne.push_front(3);
	listOne.push_front(2);
	listOne.push_front(1);
 
	listOne.push_back(4);
	listOne.push_back(5);
	listOne.push_back(6);
 
	cout << "listOne.begin()--- listOne.end():" << endl;
	for (i = listOne.begin(); i != listOne.end(); ++i)
		cout << *i << " ";
	cout << endl;
 
	LISTINT::reverse_iterator ir;
	cout << "listOne.rbegin()---listOne.rend():" << endl;
	for (ir = listOne.rbegin(); ir != listOne.rend(); ir++) {
		cout << *ir << " ";
	}
	cout << endl;
 
	int result = accumulate(listOne.begin(), listOne.end(), 0);
	cout << "Sum=" << result << endl;
	cout << "------------------" << endl;
 
	//用LISTCHAR创建一个list对象
	LISTCHAR listTwo;
	//声明i为迭代器
	LISTCHAR::iterator j;
 
	listTwo.push_front('C');
	listTwo.push_front('B');
	listTwo.push_front('A');
 
	listTwo.push_back('D');
	listTwo.push_back('E');
	listTwo.push_back('F');
 
	cout << "listTwo.begin()---listTwo.end():" << endl;
	for (j = listTwo.begin(); j != listTwo.end(); ++j)
		cout << char(*j) << " ";
	cout << endl;
 
	j = max_element(listTwo.begin(), listTwo.end());
	cout << "The maximum element in listTwo is: " << char(*j) << endl;
	system("pause");
}
```

输出结果

![img](https://pic1.zhimg.com/80/v2-ec3312189c315980dfce41de4b040a44_720w.webp)

##### **3.3.4.3 综合实例2**

```
#include <iostream> 
#include <list> 
 
using namespace std;
typedef list<int> INTLIST;
 
//从前向后显示list队列的全部元素 
void put_list(INTLIST list, char *name)
{
	INTLIST::iterator plist;
 
	cout << "The contents of " << name << " : ";
	for (plist = list.begin(); plist != list.end(); plist++)
		cout << *plist << " ";
	cout << endl;
}
 
//测试list容器的功能 
void main(void)
{
	//list1对象初始为空 
	INTLIST list1;
	INTLIST list2(5, 1);
	INTLIST list3(list2.begin(), --list2.end());
 
	//声明一个名为i的双向迭代器 
	INTLIST::iterator i;
 
	put_list(list1, "list1");
	put_list(list2, "list2");
	put_list(list3, "list3");
 
	list1.push_back(7);
	list1.push_back(8);
	cout << "list1.push_back(7) and list1.push_back(8):" << endl;
	put_list(list1, "list1");
 
	list1.push_front(6);
	list1.push_front(5);
	cout << "list1.push_front(6) and list1.push_front(5):" << endl;
	put_list(list1, "list1");
 
	list1.insert(++list1.begin(), 3, 9);
	cout << "list1.insert(list1.begin()+1,3,9):" << endl;
	put_list(list1, "list1");
 
	//测试引用类函数 
	cout << "list1.front()=" << list1.front() << endl;
	cout << "list1.back()=" << list1.back() << endl;
 
	list1.pop_front();
	list1.pop_back();
	cout << "list1.pop_front() and list1.pop_back():" << endl;
	put_list(list1, "list1");
 
	list1.erase(++list1.begin());
	cout << "list1.erase(++list1.begin()):" << endl;
	put_list(list1, "list1");
 
	list2.assign(8, 1);
	cout << "list2.assign(8,1):" << endl;
	put_list(list2, "list2");
 
	cout << "list1.max_size(): " << list1.max_size() << endl;
	cout << "list1.size(): " << list1.size() << endl;
	cout << "list1.empty(): " << list1.empty() << endl;
 
	put_list(list1, "list1");
	put_list(list3, "list3");
	cout << "list1>list3: " << (list1 > list3) << endl;
	cout << "list1<list3: " << (list1 < list3) << endl;
 
	list1.sort();
	put_list(list1, "list1");
 
	list1.splice(++list1.begin(), list3);
	put_list(list1, "list1");
	put_list(list3, "list3");
	system("pause");
}
```

输出结果：

![img](https://pic1.zhimg.com/80/v2-626fddfd1c2cebf70704eb75e0bff204_720w.webp)

### 3.4 map/multimap

map和multimap都需要#include<map>，唯一的不同是，map的键值key不可重复，而multimap可以，也正是由于这种区别，map支持[ ]运算符，multimap不支持[ ]运算符。在用法上没什么区别。

C++中map提供的是一种键值对容器，里面的数据都是成对出现的,如下图：每一对中的第一个值称之为关键字(key)，每个关键字只能在map中出现一次；第二个称之为该关键字的对应值。

![img](https://pic1.zhimg.com/80/v2-9364f2f50ec378029a8d0e8b6b160da0_720w.webp)

Map是STL的一个关联容器，它提供一对一（其中第一个可以称为关键字，每个关键字只能在map中出现一次，第二个可能称为该关键字的值）的数据 处理能力，由于这个特性，它完成有可能在我们处理一对一数据的时候，在编程上提供快速通道。这里说下map内部数据的组织，map内部自建一颗红黑树(一 种非严格意义上的平衡二叉树)，这颗树具有对数据自动排序的功能，所以在map内部所有的数据都是有序的。

#### **3.4.1 基本操作函数**

begin() 返回指向map头部的迭代器

clear(） 删除所有元素

count() 返回指定元素出现的次数

empty() 如果map为空则返回true

end() 返回指向map末尾的迭代器

equal_range() 返回特殊条目的迭代器对

erase() 删除一个元素

find() 查找一个元素

get_allocator() 返回map的配置器

insert() 插入元素

key_comp() 返回比较元素key的函数

lower_bound() 返回键值>=给定元素的第一个位置

max_size() 返回可以容纳的最大元素个数

rbegin() 返回一个指向map尾部的逆向迭代器

rend() 返回一个指向map头部的逆向迭代器

size() 返回map中元素的个数

swap() 交换两个map

upper_bound() 返回键值>给定元素的第一个位置

value_comp() 返回比较元素value的函数

#### **3.4.2 声明**

```
//头文件
#include<map>
 
map<int, string> ID_Name;
 
// 使用{}赋值是从c++11开始的，因此编译器版本过低时会报错，如visual studio 2012
map<int, string> ID_Name = {
                { 2015, "Jim" },
                { 2016, "Tom" },
                { 2017, "Bob" } };
```

#### **3.4.3 迭代器**

共有八个获取迭代器的函数：* begin, end, rbegin,rend* 以及对应的 * cbegin, cend, crbegin,crend*。

二者的区别在于，后者一定返回 const_iterator，而前者则根据map的类型返回iterator 或者 const_iterator。const情况下，不允许对值进行修改。如下面代码所示：

```
map<int,int>::iterator it;
map<int,int> mmap;
const map<int,int> const_mmap;
 
it = mmap.begin(); //iterator
mmap.cbegin(); //const_iterator
 
const_mmap.begin(); //const_iterator
const_mmap.cbegin(); //const_iterator
```

返回的迭代器可以进行加减操作，此外，如果map为空，则 begin = end。

![img](https://pic4.zhimg.com/80/v2-59f4861584ae30be38c45bbb3ea86a07_720w.webp)

#### **3.4.4 插入操作**

##### **3.4.4.1 用insert插入pair数据**

```
//数据的插入--第一种：用insert函数插入pair数据  
#include <map>    
#include <string>  
#include <iostream>  
using namespace std;  
  
int main()  
{  
    map<int, string> mapStudent;  
    mapStudent.insert(pair<int, string>(1, "student_one"));  
    mapStudent.insert(pair<int, string>(2, "student_two"));  
    mapStudent.insert(pair<int, string>(3, "student_three"));  
    map<int, string>::iterator iter;  
    for(iter = mapStudent.begin(); iter != mapStudent.end(); iter++)  
       cout<<iter->first<<' '<<iter->second<<endl;  
}  
```

##### **3.4.4.2 用insert函数插入value_type数据**

```
//第二种：用insert函数插入value_type数据，下面举例说明  
  
#include <map>    
#include <string>    
#include <iostream>    
using namespace std;  
  
int main()    
{    
    map<int, string> mapStudent;    
    mapStudent.insert(map<int, string>::value_type (1, "student_one"));    
    mapStudent.insert(map<int, string>::value_type (2, "student_two"));    
    mapStudent.insert(map<int, string>::value_type (3, "student_three"));    
    map<int, string>::iterator iter;    
    for(iter = mapStudent.begin(); iter != mapStudent.end(); iter++)  
         cout<<iter->first<<' '<<iter->second<<endl;    
}  
```

##### **3.4.4.3 用insert函数进行多个插入**

insert共有4个重载函数：

```
// 插入单个键值对，并返回插入位置和成功标志，插入位置已经存在值时，插入失败
pair<iterator,bool> insert (const value_type& val);
 
//在指定位置插入，在不同位置插入效率是不一样的，因为涉及到重排
iterator insert (const_iterator position, const value_type& val);
 
// 插入多个
void insert (InputIterator first, InputIterator last);
 
//c++11开始支持，使用列表插入多个   
void insert (initializer_list<value_type> il);
```

下面是具体使用示例：

```
#include <iostream>
#include <map>
 
int main()
{
    std::map<char, int> mymap;
 
    // 插入单个值
    mymap.insert(std::pair<char, int>('a', 100));
    mymap.insert(std::pair<char, int>('z', 200));
 
    //返回插入位置以及是否插入成功
    std::pair<std::map<char, int>::iterator, bool> ret;
    ret = mymap.insert(std::pair<char, int>('z', 500));
    if (ret.second == false) {
        std::cout << "element 'z' already existed";
        std::cout << " with a value of " << ret.first->second << '\n';
    }
 
    //指定位置插入
    std::map<char, int>::iterator it = mymap.begin();
    mymap.insert(it, std::pair<char, int>('b', 300));  //效率更高
    mymap.insert(it, std::pair<char, int>('c', 400));  //效率非最高
 
    //范围多值插入
    std::map<char, int> anothermap;
    anothermap.insert(mymap.begin(), mymap.find('c'));
 
    // 列表形式插入
    anothermap.insert({ { 'd', 100 }, {'e', 200} });
 
    return 0;
}
```

##### **3.4.4.4 用数组方式插入数据**

```
//第三种：用数组方式插入数据，下面举例说明  
  
#include <map>    
#include <string>    
#include <iostream>    
using namespace std;  
  
int main()    
{    
    map<int, string> mapStudent;    
    mapStudent[1] = "student_one";   
    mapStudent[2] = "student_two";    
    mapStudent[3] = "student_three";    
    map<int, string>::iterator iter;    
    for(iter = mapStudent.begin(); iter != mapStudent.end(); iter++)    
        cout<<iter->first<<' '<<iter->second<<endl;    
}  
```

以上三种用法，虽然都可以实现数据的插入，但是它们是有区别的，当然了第一种和第二种在效果上是完成一样的，用insert函数插入数据，在数据的 插入上涉及到集合的唯一性这个概念，即当map中有这个关键字时，insert操作是插入数据不了的，但是用数组方式就不同了，它可以覆盖以前该关键字对 应的值，用程序说明

mapStudent.insert(map<int, string>::value_type (1, "student_one"));

mapStudent.insert(map<int, string>::value_type (1, "student_two"));

上面这两条语句执行后，map中1这个关键字对应的值是“student_one”，第二条语句并没有生效，那么这就涉及到我们怎么知道insert语句是否插入成功的问题了，可以用pair来获得是否插入成功，程序如下

pair<map<int, string>::iterator, bool> Insert_Pair;

Insert_Pair = mapStudent.insert(map<int, string>::value_type (1, "student_one"));

我们通过pair的第二个变量来知道是否插入成功，它的第一个变量返回的是一个map的迭代器，如果插入成功的话Insert_Pair.second应该是true的，否则为false。

下面给出完成代码，演示插入成功与否问题

```
//验证插入函数的作用效果  
#include <map>    
#include <string>    
#include <iostream>    
using namespace std;  
  
int main()    
{  
    map<int, string> mapStudent;  
    pair<map<int, string>::iterator, bool> Insert_Pair;    
    Insert_Pair = mapStudent.insert(pair<int, string>(1, "student_one"));    
    if(Insert_Pair.second == true)  
        cout<<"Insert Successfully"<<endl;    
    else    
        cout<<"Insert Failure"<<endl;    
    Insert_Pair = mapStudent.insert(pair<int, string>(1, "student_two"));    
    if(Insert_Pair.second == true)    
        cout<<"Insert Successfully"<<endl;    
    else    
        cout<<"Insert Failure"<<endl;    
    map<int, string>::iterator iter;   
    for(iter = mapStudent.begin(); iter != mapStudent.end(); iter++)    
       cout<<iter->first<<' '<<iter->second<<endl;    
}  
```

大家可以用如下程序，看下用数组插入在数据覆盖上的效果

```
//验证数组形式插入数据的效果   
#include <map>    
#include <string>    
#include <iostream>    
using namespace std;  
  
int main()    
{    
    map<int, string> mapStudent;    
    mapStudent[1] = "student_one";    
    mapStudent[1] = "student_two";    
    mapStudent[2] = "student_three";    
    map<int, string>::iterator iter;    
    for(iter = mapStudent.begin(); iter != mapStudent.end(); iter++)    
       cout<<iter->first<<' '<<iter->second<<endl;  
}  
```

#### 3.4.5 查找、删除、交换

**查找**

```
// 关键字查询，找到则返回指向该关键字的迭代器，否则返回指向end的迭代器
// 根据map的类型，返回的迭代器为 iterator 或者 const_iterator
iterator find (const key_type& k);
const_iterator find (const key_type& k) const;
```

**删除**

```
// 删除迭代器指向位置的键值对，并返回一个指向下一元素的迭代器
iterator erase( iterator pos )
 
// 删除一定范围内的元素，并返回一个指向下一元素的迭代器
iterator erase( const_iterator first, const_iterator last );
 
// 根据Key来进行删除， 返回删除的元素数量，在map里结果非0即1
size_t erase( const key_type& key );
 
// 清空map，清空后的size为0
void clear();
```

**交换**

```
// 就是两个map的内容互换
void swap( map& other );
```

#### **3.4.6 容量**

```
// 查询map是否为空
bool empty();
 
// 查询map中键值对的数量
size_t size();
 
// 查询map所能包含的最大键值对数量，和系统和应用库有关。
// 此外，这并不意味着用户一定可以存这么多，很可能还没达到就已经开辟内存失败了
size_t max_size();
 
// 查询关键字为key的元素的个数，在map里结果非0即1
size_t count( const Key& key ) const; //
```

#### **3.4.7 排序**

map中的元素是自动按Key升序排序，所以不能对map用sort函数；

这里要讲的是一点比较高深的用法了,排序问题，STL中默认是采用小于号来排序的，以上代码在排序上是不存在任何问题的，因为上面的关键字是int 型，它本身支持小于号运算，在一些特殊情况，比如关键字是一个结构体或者自定义类，涉及到排序就会出现问题，因为它没有小于号操作，insert等函数在编译的时候过 不去，下面给出两个方法解决这个问题。

##### **3.4.7.1 小于号 < 重载**

```
#include <iostream>  
#include <string>  
#include <map>  
using namespace std;
 
typedef struct tagStudentinfo
{
	int      niD;
	string   strName;
	bool operator < (tagStudentinfo const& _A) const
	{     //这个函数指定排序策略，按niD排序，如果niD相等的话，按strName排序  
		if (niD < _A.niD) return true;
		if (niD == _A.niD)
			return strName.compare(_A.strName) < 0;
		return false;
	}
}Studentinfo, *PStudentinfo; //学生信息  
 
int main()
{
	int nSize;   //用学生信息映射分数  
	map<Studentinfo, int>mapStudent;
	map<Studentinfo, int>::iterator iter;
	Studentinfo studentinfo;
	studentinfo.niD = 1;
	studentinfo.strName = "student_one";
	mapStudent.insert(pair<Studentinfo, int>(studentinfo, 90));
	studentinfo.niD = 2;
	studentinfo.strName = "student_two";
	mapStudent.insert(pair<Studentinfo, int>(studentinfo, 80));
	for (iter = mapStudent.begin(); iter != mapStudent.end(); iter++)
		cout << iter->first.niD << ' ' << iter->first.strName << ' ' << iter->second << endl;
	return 0;
}
```

##### **3.4.7.2 仿函数的应用，这个时候结构体中没有直接的小于号重载**

```
//第二种：仿函数的应用，这个时候结构体中没有直接的小于号重载，程序说明  
 
#include <iostream>  
#include <map>  
#include <string>  
using namespace std;
 
typedef struct tagStudentinfo
{
	int      niD;
	string   strName;
}Studentinfo, *PStudentinfo; //学生信息  
 
class sort
{
public:
	bool operator() (Studentinfo const &_A, Studentinfo const &_B) const
	{
		if (_A.niD < _B.niD)
			return true;
		if (_A.niD == _B.niD)
			return _A.strName.compare(_B.strName) < 0;
		return false;
	}
};
 
int main()
{   
	//用学生信息映射分数  
	map<Studentinfo, int, sort>mapStudent;
	map<Studentinfo, int>::iterator iter;
	Studentinfo studentinfo;
	studentinfo.niD = 1;
	studentinfo.strName = "student_one";
	mapStudent.insert(pair<Studentinfo, int>(studentinfo, 90));
	studentinfo.niD = 2;
	studentinfo.strName = "student_two";
	mapStudent.insert(pair<Studentinfo, int>(studentinfo, 80));
	for (iter = mapStudent.begin(); iter != mapStudent.end(); iter++)
		cout << iter->first.niD << ' ' << iter->first.strName << ' ' << iter->second << endl;
	system("pause");
}
```

#### **3.4.8 unordered_map**

在c++11标准前，c++标准库中只有一种map，但是它的底层实现并不是适合所有的场景，如果我们需要其他适合的map实现就不得不使用比如boost库等三方的实现，在c++11中加了一种map unordered_map,unordered_set,他们的实现有什么不同呢？

map底层采用的是红黑树的实现查询的时间复杂度为O(logn),看起来并没有unordered_map快，但是也要看实际的数据量，虽然unordered_map的查询从算法上分析比map快，但是它有一些其它的消耗，比如哈希函数的构造和分析，还有如果出现哈希冲突解决哈希冲突等等都有一定的消耗，因此unordered_map的效率在很大的程度上由它的hash函数算法决定，而红黑树的效率是一个稳定值。

unordered_map的底层采用哈希表的实现，查询的时间复杂度为是O(1)。所以unordered_map内部就是无序的，数据是按散列函数插入到槽里面去的，数据之间无顺序可言，如果我们不需要内部有序，这种实现是没有问题的。unordered_map属于关联式容器，采用std::pair保存key-value形式的数据。用法与map一致。特别的是，STL中的map因为是有序的二叉树存储，所以对key值需要有大小的判断，当使用内置类型时，无需重载operator < ；但是用用户自定义类型的话，就需要重载operator < 。 unoredered_map全程使用不需要比较元素的key值的大小，但是，对于元素的==要有判断，又因为需要使用hash映射，所以，对于非内部类型，需要程序员为其定义这二者的内容，对于内部类型，就不需要了。unordered库使用“桶”来存储元素，散列值相同的被存储在一个桶里。当散列容器中有大量数据时，同一个桶里的数据也会增多，造成访问冲突，降低性能。为了提高散列容器的性能，unordered库会在插入元素是自动增加桶的数量，不需要用户指定。但是，用户也可以在构造函数或者rehash()函数中，指定最小的桶的数量。

还有另外一点从占用内存上来说因为unordered_map才用hash结构会有一定的内存损失，它的内存占用回高于map。

最后就是她们的场景了，首先如果你需要对map中的数据排序，就首选map，他会把你的数据按照key的自然排序排序（由于它的底层实现红黑树机制所以会排序），如果不需要排序，就看你对内存和cpu的选择了，不过一般都会选择unordered_map，它的查找效率会更高。

至于使用方法和函数，两者差不多，由于篇幅限制这里不再赘述，unordered_multimap用法亦可类推。

### 3.5 set/multiset

std::set 是关联容器，含有 Key 类型对象的已排序集。用比较函数compare进行排序。搜索、移除和插入拥有对数复杂度。 set 通常以红黑树实现。

set容器内的元素会被自动排序，set与map不同，set中的元素即是键值又是实值，set不允许两个元素有相同的键值。不能通过set的迭代器去修改set元素，原因是修改元素会破坏set组织。当对容器中的元素进行插入或者删除时，操作之前的所有迭代器在操作之后依然有效。

由于set元素是排好序的，且默认为升序，因此当set集合中的元素为结构体或自定义类时，该结构体或自定义类必须实现运算符‘<’的重载。

　　multiset特性及用法和set完全相同，唯一的差别在于它允许键值重复。

　　set和multiset的底层实现是一种高效的平衡二叉树，即红黑树（Red-Black Tree）。

#### **3.5.1 set常用成员函数**

\1. begin()--返回指向第一个元素的迭代器

\2. clear()--清除所有元素

\3. count()--返回某个值元素的个数

\4. empty()--如果集合为空，返回true

\5. end()--返回指向最后一个元素的迭代器

\6. equal_range()--返回集合中与给定值相等的上下限的两个迭代器

\7. erase()--删除集合中的元素

\8. find()--返回一个指向被查找到元素的迭代器

\9. get_allocator()--返回集合的分配器

\10. insert()--在集合中插入元素

\11. lower_bound()--返回指向大于（或等于）某值的第一个元素的迭代器

\12. key_comp()--返回一个用于元素间值比较的函数

\13. max_size()--返回集合能容纳的元素的最大限值

\14. rbegin()--返回指向集合中最后一个元素的反向迭代器

\15. rend()--返回指向集合中第一个元素的反向迭代器

\16. size()--集合中元素的数目

\17. swap()--交换两个集合变量

\18. upper_bound()--返回大于某个值元素的迭代器

\19. value_comp()--返回一个用于比较元素间的值的函数

#### **3.5.2 代码示例**

以下代码涉及的内容：

1、set容器中，元素类型为基本类型，如何让set按照用户意愿来排序？

2、set容器中，如何让元素类型为自定义类型？

3、set容器的insert函数的返回值为什么类型？

```
#include <iostream>
#include <string>
#include <set>
using namespace std;
 
/* 仿函数CompareSet，在test02使用 */
class CompareSet
{
public:
    //从大到小排序
    bool operator()(int v1, int v2)
    {
        return v1 > v2;
    }
    //从小到大排序
    //bool operator()(int v1, int v2)
    //{
    //    return v1 < v2;
    //}
};
 
/* Person类，用于test03 */
class Person
{
    friend ostream &operator<<(ostream &out, const Person &person);
public:
    Person(string name, int age)
    {
        mName = name;
        mAge = age;
    }
public:
    string mName;
    int mAge;
};
 
ostream &operator<<(ostream &out, const Person &person)
{
    out << "name:" << person.mName << " age:" << person.mAge << endl;
    return out;
}
 
/* 仿函数ComparePerson,用于test03 */
class ComparePerson
{
public:
    //名字大的在前面，如果名字相同，年龄大的排前面
    bool operator()(const Person &p1, const Person &p2)
    {
        if (p1.mName == p2.mName)
        {
            return p1.mAge > p2.mAge;
        }
        return p1.mName > p2.mName;
    }
};
 
/* 打印set类型的函数模板 */
template<typename T>
void PrintSet(T &s)
{
    for (T::iterator iter = s.begin(); iter != s.end(); ++iter)
        cout << *iter << " ";
    cout << endl;
}
 
void test01()
{
    //set容器默认从小到大排序
    set<int> s;
    s.insert(10);
    s.insert(20);
    s.insert(30);
 
    //输出set
    PrintSet(s);
    //结果为:10 20 30
 
    /* set的insert函数返回值为一个对组(pair)。
       对组的第一个值first为set类型的迭代器：
       1、若插入成功，迭代器指向该元素。
       2、若插入失败，迭代器指向之前已经存在的元素
       对组的第二个值seconde为bool类型：
       1、若插入成功，bool值为true
       2、若插入失败，bool值为false
    */
    pair<set<int>::iterator, bool> ret = s.insert(40);
    if (true == ret.second)
        cout << *ret.first << " 插入成功" << endl;
    else
        cout << *ret.first << " 插入失败" << endl;
}
 
void test02()
{
    /* 如果想让set容器从大到小排序，需要给set容
       器提供一个仿函数,本例的仿函数为CompareSet
    */
    set<int, CompareSet> s;
    s.insert(10);
    s.insert(20);
    s.insert(30);
    
    //打印set
    PrintSet(s);
    //结果为:30,20,10
}
 
void test03()
{
    /* set元素类型为Person，当set元素类型为自定义类型的时候
       必须给set提供一个仿函数，用于比较自定义类型的大小，
       否则无法通过编译 
    */
    set<Person,ComparePerson> s;
    s.insert(Person("John", 22));
    s.insert(Person("Peter", 25));
    s.insert(Person("Marry", 18));
    s.insert(Person("Peter", 36));
 
    //打印set
    PrintSet(s);
}
 
int main(void)
{
    //test01();
    //test02();
    //test03();
    return 0;
}
```

multiset容器的insert函数返回值为什么？

```
#include <iostream>
#include <set>
using namespace std;
 
/* 打印set类型的函数模板 */
template<typename T>
void PrintSet(T &s)
{
    for (T::iterator iter = s.begin(); iter != s.end(); ++iter)
        cout << *iter << " ";
    cout << endl;
}
 
void test(void)
{
    multiset<int> s;
    s.insert(10);
    s.insert(20);
    s.insert(30);
    
    //打印multiset
    PrintSet(s);
 
    /* multiset的insert函数返回值为multiset类型的迭代器，
       指向新插入的元素。multiset允许插入相同的值，因此
       插入一定成功，因此不需要返回bool类型。
    */
    multiset<int>::iterator iter = s.insert(10);
    
    cout << *iter << endl;    
}
 
int main(void)
{
    test();
    return 0;
}
```

#### **3.5.3 unordered_set**

C++ 11中出现了两种新的关联容器:unordered_set和unordered_map，其内部实现与set和map大有不同，set和map内部实现是基于RB-Tree，而unordered_set和unordered_map内部实现是基于哈希表(hashtable)，由于unordered_set和unordered_map内部实现的公共接口大致相同，所以本文以unordered_set为例。

unordered_set是基于哈希表，因此要了解unordered_set，就必须了解哈希表的机制。哈希表是根据关键码值而进行直接访问的数据结构，通过相应的哈希函数(也称散列函数)处理关键字得到相应的关键码值，关键码值对应着一个特定位置，用该位置来存取相应的信息，这样就能以较快的速度获取关键字的信息。比如：现有公司员工的个人信息（包括年龄），需要查询某个年龄的员工个数。由于人的年龄范围大约在[0，200]，所以可以开一个200大小的数组，然后通过哈希函数得到key对应的key-value，这样就能完成统计某个年龄的员工个数。而在这个例子中，也存在这样一个问题，两个员工的年龄相同，但其他信息（如：名字、身份证）不同，通过前面说的哈希函数，会发现其都位于数组的相同位置，这里，就涉及到“冲突”。准确来说，冲突是不可避免的，而解决冲突的方法常见的有：开发地址法、再散列法、链地址法(也称拉链法)。而unordered_set内部解决冲突采用的是----链地址法，当用冲突发生时把具有同一关键码的数据组成一个链表。下图展示了链地址法的使用:

![img](https://pic3.zhimg.com/80/v2-dc11de8cd200a29bc0c509f340cbf66e_720w.webp)

使用unordered_set需要包含#include<unordered_set>头文件，同unordered_map类似，用法没有什么太大的区别，参考set/multiset。

除此之外unordered_multiset也是一种可选的容器。

原文链接：https://zhuanlan.zhihu.com/p/582795495

# 【NO.20】一文掌握google开源单元测试框架Google Test

我们在开发的过程中，需要做一些验证测试，来保证我们的代码是按照设计要求工作的，这就需要单元测试了。单元测试（Unit Test），我们称为“UT测试”。对于一个复杂的系统来说，需要编写大量的单元测试用例，有人会觉得这么多的测试代码，将会花费大量的时间，影响开发的进度，会得不偿失。真的是这样吗？其实，对于越是复杂的系统就越是需要单元测试来保证我们的代码的开发质量，及时测试出代码的问题，在开发阶段发现问题总比在系统发布之后发现问题能够较少的节省资源或成本。

对于单元测试应该是每个开发工程师必备的技能，尤其是高阶的开发工程师会更加注重UT的重要性。同时，我们在开发功能模块之前会考虑到测试用例的实现，这样自然的就会考虑到功能模块的模块化便于UT的编写，从这一方面来说也能提高开发人员开发的代码质量。另外，单元测试用例还可以作为示例供开发人员参考，从而能够更轻松的掌握模块的使用。

今天就和大家一起学习一个开源的C++的单元测试框架Google test，大家看名字就知道它是由牛逼的Google公司出品。Google Test可以在多种平台上使用，它可以支持：

> Linux、Mac OS X、Windows、Cygwin、MinGW、Windows Mobile、Symbian、PlatformIO等。

## 1.安装和配置

我们可以从github获取Google Test的源码。

> github下载地址: [https://github.com/google/googletest](https://link.zhihu.com/?target=https%3A//github.com/google/googletest)

因为我们下载到的gTest是源代码，还需要将其编译成库文件再进行使用。下面将和大家一起学习如何在windows环境下生成gTest的库文件。在这之前我们需要安装CMake和MinGW。

将下载的gTest的源码进行解压，源码目录如下图所示。

![img](https://pic2.zhimg.com/80/v2-7b5580b3d8dc932c2d76c48b91c6fa8d_720w.webp)

打开命令行工具cmd，进入源码的工程目录，新建一个build目录用来存放构建文件，然后，进入build目录执行cmake命令生成Makefile文件。

```
mkdir build
cd build
cmake -G "MinGW Makefiles" ..
```

![img](https://pic3.zhimg.com/80/v2-e0b2d6fae1f4dfc369f7c5c268b81cba_720w.webp)

![img](https://pic1.zhimg.com/80/v2-16d08ea7934f23af7e679e0604f49cf4_720w.webp)

Makefile文件生成后，再执行下面的命令mingw32-make编译库文件。编译成功后就会发现有libgtest.a 和libgtest_main.a两个静态库生成。这里注意，Windows下mingw安装的make工具名称是mingw32-make而不是make。

```
mingw32-make
```

![img](https://pic3.zhimg.com/80/v2-6ca799fb7be17f7e6d7c433946a23a46_720w.webp)

接下来我们在VS Code写一个测试用例，使用生成的gTest静态库测试下。按下快捷键【Ctrl+Shift+p】，在弹出的搜索框中搜索【C/C++:Edit Configurations】，可以创建c_cpp_properties.json配置文件。

![img](https://pic3.zhimg.com/80/v2-98dccb609639e53d7b43a7eea1d0994e_720w.webp)

在c_cpp_properties.json配置文件添加gTest的头文件目录。

![img](https://pic2.zhimg.com/80/v2-cf8f7d5c0c7c9a16715bcda0dbf480ad_720w.webp)

在task.json配置文件中添加gTest头文件目录和库文件，task.json配置文件可以通过菜单栏中Terminal选项下的【Configure Default Build Task】选项创建。

![img](https://pic2.zhimg.com/80/v2-b8e5517b49abaeea7d07f8bc1922436d_720w.webp)

![img](https://pic3.zhimg.com/80/v2-f1c2ed2f310d64f7e1b3ed82b9593f72_720w.webp)

上面配置好之后，我们写个测试用例跑一下。

```
#include <iostream>
#include <gtest/gtest.h>

int add(int a, int b)
{
    return a + b;
}

int sub(int a, int b)
{
    return a - b;
}

TEST(testcase, test_add)
{
    EXPECT_EQ(add(1,2), 3);
    EXPECT_EQ(sub(1,2), -1);
}

int main(int argc, char **argv)
{  
    std::cout << "run google test --> " << std::endl << std::endl;
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

运行结果如下图所示，代码中的TEST是一个宏，用来创建测试用例，它有test_case_name和test_name两个参数。分别是测试用例名和测试名，在后面的文章中我们会对其有更深刻的理解，这里就不细说了。RUN_ALL_TESTS也是一个宏，它是测试用例的入口。EXPECT_EQ这个是一个断言相关的宏，用来检测两个数值是否相等。

![img](https://pic4.zhimg.com/80/v2-cd6a9066f65330ba7c79183434cab35f_720w.webp)



## 2.断言

除了上面示例里的EXPECT_EQ，在gTest里有很多断言相关的宏。断言可以检查出某些条件的真假，因此，我们可以通过它来判断被测试的函数的成功与否。这里断言我们主要可以分为两类：

- 以"ASSERT_"开头的断言，致命性断言（Fatal assertion）
- 以"EXPECT_"开头的断言 ，非致命性断言（Nonfatal assertion）

上面的两种断言会在断言条件不满足时会有区别，即当不满足条件时, "ASSERT*"断言会在当前函数终止，而不会继续执行下去；而"EXPECT*"则会继续执行。我们可以通过下面一个例子来理解下他们的区别。

```
#include <iostream>
#include <gtest/gtest.h>

int add(int a, int b)
{
    return a + b;
}

int sub(int a, int b)
{
    return a - b;
}

TEST(testcase, test_expect)
{
    std::cout << "------ test_expect start-----" << std::endl;

    std::cout << "add function start" << std::endl;
    EXPECT_EQ(add(1,2), 2);
    std::cout << "add function end" << std::endl;

    std::cout << "sub function start" << std::endl;
    EXPECT_EQ(sub(1,2), -1);
    std::cout << "sub function end" << std::endl;

    std::cout << "------ test_expect end-----" << std::endl;
}

TEST(testcase, test_assert)
{

    std::cout << "------ test_assert start-----" << std::endl;

    std::cout << "add function start" << std::endl;
    ASSERT_EQ(add(1,2), 2);
    std::cout << "add function end" << std::endl;

    std::cout << "sub function start" << std::endl;
    ASSERT_EQ(sub(1,2), -1);
    std::cout << "sub function end" << std::endl;

    std::cout << "------ test_assert end-----" << std::endl;
}

int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
}
```

从下面的运行结果上看，assert断言检查出被测函数add不满足条件，所以程序就没有继续执行下去；而expect虽然检查出被测试函数add不满足条件，但是程序还是继续去测试sub函数。

![img](https://pic2.zhimg.com/80/v2-1bb29da307c1538401cae497c54f03a9_720w.webp)

上面的示例用到的都是判断相等条件的断言，还有其他条件检查的断言。主要可以分为布尔检查，数值比较检查，字符串检查，浮点数检查，异常检查等等。下面我们逐一认识这些断言。

## 3.布尔检查

布尔检查主要用来检查布尔类型数据，检查其条件是真还是假。

![img](https://pic2.zhimg.com/80/v2-10bf9d8ab9ef57a35585b70f9c3431e5_720w.webp)

## 4.数值比较检查

数值比较检查主要用来比较两个数值之间的大小关系，这里有两个参数。

![img](https://pic1.zhimg.com/80/v2-579377a5007471a517c98107c2f101dc_720w.webp)

## 5.字符串检查

字符串检查主要用来比较字符串的内容。

![img](https://pic3.zhimg.com/80/v2-7c9703ad92123714ea41fc8c91e75512_720w.webp)

## 6.浮点数检查

对于浮点数来说，因为其精度原因，我们无法确定其是否完全相等，实际上对于浮点数我比较两个浮点数近似相等。

![img](https://pic1.zhimg.com/80/v2-c18380c78fa634142e3db9d98837f360_720w.webp)

## 7.异常检查

异常检查可以将异常转换成断言的形式。

![img](https://pic2.zhimg.com/80/v2-64d6bd623e2aa8ccafbfc4250ec6499d_720w.webp)

除了上面的一些类型的断言，还有一切其他的常用断言。

## 8.显示成功或失败

这一类断言会在测试运行中标记成功或失败。它主要有三个宏：

- SUCCED()：标记成功。
- FAIL() : 标记失败，类似ASSERT断言标记致命错误；
- ADD_FAILURE()：标记，类似EXPECT断言标记非致命错误。

```
#include <iostream>
#include <gtest/gtest.h>

int divison(int a, int b)
{
    return a / b;
}

TEST(testCaseTest, test0)
{
    std::cout << "start test 0" << std::endl;
    SUCCEED();
    std::cout << "test pass" << std::endl;
}

TEST(testCaseTest, test1)
{
    std::cout << "start test 1" << std::endl;
    FAIL();
    std::cout << "test fail" << std::endl;
}

TEST(testCaseTest, test2)
{
    std::cout << "start test 2" << std::endl;
    ADD_FAILURE();
    std::cout << "test fail" << std::endl;
}


int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

执行结果如下：

![img](https://pic4.zhimg.com/80/v2-c65dd0527dd2d1bb9b8b9a19456250c3_720w.webp)

## 9.死亡测试

死亡测试是用来检测测试程序是否按照预期的方式崩溃。

![img](https://pic3.zhimg.com/80/v2-767245c40ecd7fd113fcaabed8cc560a_720w.webp)

```
#include <iostream>
#include <gtest/gtest.h>

int divison(int a, int b)
{
    return a / b;
}

TEST(testCaseDeathTest, test_div)
{
    EXPECT_DEATH(divison(1, 0), "");
}
int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

上面这个例子就是死亡测试，其运行结果如下，这里需要注意的是test_case_name如果使用DeathTest为后缀，gTest会优先运行。

![img](https://pic4.zhimg.com/80/v2-6c91f252fa01982c02664fc7e69f5be7_720w.webp)

## 10.测试事件

在学习测试事件之前，我们先来了解下三个概念，它们分别是测试程序，测试套件，测试用例。

- 测试程序是一个可执行程序，它有一个测试程序的入口main函数。
- 测试用例是用来定义需要验证的内容。
- 测试套件是测试用例的集合，运行测试。

我们回过来看测试事件，在GTest中有了测试事件的这个机制，就能能够在测试之前或之后能够做一些准备/清理的操作。根据事件执行的位置不同，我们可将测试事件分为三种：

- TestCase级别测试事件：这个级别的事件会在TestCase之前与之后执行；
- TestSuite级别测试事件：这个级别的事件会在TestSuite中第一个TestCase之前与最后一个TestCase之后执行；
- 全局测试事件：这是级别的事件会在所有TestCase中第一个执行前，与最后一个之后执行。

这些测试事件都是基于类的，所以需要在类上实现。下面我们依次来学习这三种测试事件。

## 11.TestCase测试事件

TestCase测试事件，需要实现两个函数SetUp()和TearDown()。

- SetUp()函数是在TestCase之前执行。
- TearDown()函数是在TestCase之后执行。

这两个函数是不是有点像类的构造函数和析构函数，但是切记他们并不是构造函数和析构函数，只是打个比方才这么说而已。我们可以借助下面的代码示例来加深对它的理解。这两个函数是testing::Test的成员函数，我们在编写测试类时需要继承testing::Test。

```
#include <iostream>
#include <gtest/gtest.h>

class calcFunction
{
public:
    int add(int a, int b)
    {
        return a + b;
    }

    int sub(int a, int b)
    {
        return a - b;
    }
};

class calcFunctionTest : public testing::Test
{
protected:
    virtual void SetUp()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }
    virtual void TearDown()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }

    calcFunction calc;

};

TEST_F(calcFunctionTest, test_add)
{
    std::cout << "--> test_add start <--" << std::endl;
    EXPECT_EQ(calc.add(1,2), 3);
    std::cout << "--> test_add end <--" << std::endl;
}

TEST_F(calcFunctionTest, test_sub)
{
    std::cout << "--> test_sub start <--" << std::endl;
    EXPECT_EQ(calc.sub(1,2), -1);
    std::cout << "--> test_sub end <--" << std::endl;
}

int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

测试结果如下，两个函数都是是在每个TestCase（test_add和test_sub）之前和之后执行。

![img](https://pic3.zhimg.com/80/v2-a50073b96ec0cbfb70f47c477314731a_720w.webp)

## 12.TestSuite测试事件

TestSuite测试事件，同样的也需要实现的两个函数SetUpTestCase()和TearDownTestCase()，而这两个函数是静态函数。这两个静态函数同样也是testing::Test类的成员，我们直接改写下测试类calcFunctionTest，添加两个静态函数SetUpTestCase()和TearDownTestCase()到测试类中即可。

```
class calcFunctionTest : public testing::Test
{
protected:
    static void SetUpTestCase()
    {
        std::cout<< "--> " <<  __func__ << " <--" << std::endl;
    }

    static void TearDownTestCase()
    {
        std::cout<< "--> " << __func__ << " <--" << std::endl;
    }

    virtual void SetUp()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }
    virtual void TearDown()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }

    calcFunction calc;

};
```

改写好之后，我们再看一下运行结果。这两个函数分别是在本TestSuite中的第一个TestCase之前和最后一个TestCase之后执行。

![img](https://pic2.zhimg.com/80/v2-f9af1935b96f835960a4921fd2d07a2d_720w.webp)

## 13.全局测试事件

全局测试事件，也需要继承一个类，但是它需要继承testing::Environment类实现SetUp()和TearDown()两个函数。还需要在main函数中调用testing::AddGlobalTestEnvironment方法注册全局事件。我们直接上代码吧！

```
#include <iostream>
#include <gtest/gtest.h>

class calcFunction
{
public:
    int add(int a, int b)
    {
        return a + b;
    }

    int sub(int a, int b)
    {
        return a - b;
    }
};

class calcFunctionEnvironment : public testing::Environment
{
    public:
        virtual void SetUp()
        {
            val = 123;
            std::cout << "--> Environment " << __func__ << " <--" << std::endl;
        }
        virtual void TearDown()
        {
            std::cout << "--> Environment " << __func__ << " <--" << std::endl;
        }

        int val;
};

calcFunctionEnvironment* calc_env;

class calcFunctionTest : public testing::Test
{
protected:
    static void SetUpTestCase()
    {
        std::cout<< "--> " <<  __func__ << " <--" << std::endl;
    }

    static void TearDownTestCase()
    {
        std::cout<< "--> " << __func__ << " <--" << std::endl;
    }

    virtual void SetUp()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }
    virtual void TearDown()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }

    calcFunction calc;

};

TEST_F(calcFunctionTest, test_add)
{
    std::cout << "--> test_add start <--" << std::endl;
    EXPECT_EQ(calc.add(1,2), 3);
    std::cout << "Global Environment val = " << calc_env->val << std::endl;
    std::cout << "--> test_add end <--" << std::endl;
}

TEST_F(calcFunctionTest, test_sub)
{
    std::cout << "--> test_sub start <--" << std::endl;
    EXPECT_EQ(calc.sub(1,2), -1);
    std::cout << "Global Environment val = " << calc_env->val << std::endl;
    std::cout << "--> test_sub end <--" << std::endl;
}

int main(int argc, char **argv)
{  
    calc_env = new calcFunctionEnvironment;
    testing::AddGlobalTestEnvironment(calc_env);

    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

从测试结果上看，全局事件的这两个函数分别是在第一个TestSuite之前和最后一个TestSuite之后执行的。

![img](https://pic3.zhimg.com/80/v2-1bff7a98385fa963b4b01853d2c53766_720w.webp)

以上三种测试事件我们可以根据需要进行灵活使用。另外，细心的同学会发现，这里测试用例我们该用了TEST_F这个宏，这是因为继承了testing::Test,与之对应就需要使用TEST_F宏。

## 14.参数化

在学习gTest参数化之前我们先看一个测试例子。

```
#include <iostream>
#include <gtest/gtest.h>

class calcFunction
{
public:
    int add(int a, int b)
    {
        std::cout << a << " + " << b << " = " << a + b << std::endl;
        return a + b;
    }

    int sub(int a, int b)
    {
        std::cout << a << " - " << b << " = " << a - b << std::endl;
        return a - b;
    }
};

class calcFunctionTest : public testing::Test
{
protected:
    calcFunction calc;
};

TEST_F(calcFunctionTest, test_add0)
{
    EXPECT_EQ(calc.add(1,2), 3);
}

TEST_F(calcFunctionTest, test_add1)
{
    EXPECT_EQ(calc.add(1,3), 4);
}

TEST_F(calcFunctionTest, test_add2)
{
    EXPECT_EQ(calc.add(2,4), 6);
}

TEST_F(calcFunctionTest, test_add3)
{
    EXPECT_EQ(calc.add(-1,-2), -3);
}

int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

示例执行结果：

![img](https://pic4.zhimg.com/80/v2-4d528b76739ff7ef73f346f794d91103_720w.webp)

上面的测试用例中我们写了多个测试用例，但是其参数都是同样的，有的实际应用场景可能比这个程序写的测试检查还要多。写这么多重复的代码实在是太累了。gTest提供了一个非常友好的工具，将这些测试的值进行参数化，就不用写那么多重复的代码了。

如何对其进行参数化呢？直接上代码，我们再来看下面一个例子。

```
#include <iostream>
#include <gtest/gtest.h>

class calcFunction
{
public:
    int add(int a, int b)
    {
        std::cout << a << " + " << b << " = " << a + b << std::endl;
        return a + b;
    }

    int sub(int a, int b)
    {
        std::cout << a << " - " << b << " = " << a - b << std::endl;
        return a - b;
    }
};

struct TestParam
{
    int a;
    int b;
    int c;
};

class calcFunctionTest : public ::testing::TestWithParam<struct TestParam>
{
protected:
    calcFunction calc;
    TestParam param;

    virtual void SetUp()
    {
        param.a = GetParam().a;
        param.b = GetParam().b;
        param.c = GetParam().c;
    }

};

TEST_P(calcFunctionTest, test_add)
{
    EXPECT_EQ(calc.add(param.a, param.b), param.c);
}

INSTANTIATE_TEST_CASE_P(addTest, calcFunctionTest, ::testing::Values( TestParam{1, 2 , 3}, 
                                                                      TestParam{1, 3 , 4},
                                                                      TestParam{2, 4 , 6},
                                                                      TestParam{-1, -2 , -3}));

int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

执行结果和前面的例子一样。

![img](https://pic2.zhimg.com/80/v2-0c563bade7adeb8bc1a21c0eb4b347b5_720w.webp)

从这个例子中，我们不难发现和之前的测试程序有一些不同。这里继承了::testing::TestWithParam类，参数T就是需要参数化的数据类型，这个例子里参数化数据类型是TestParam结构体。这里还需要使用另外一个宏TEST_P而不是TEST_F这个宏，它的两个参数和TEST_F和TEST一致。另外，程序中还增加一个宏INSTANTIATE_TEST_CASE_P用来输入测试参数，它有三个参数（第一个参数大家可任意取名，第二个参数是test_case_name和TEST_P宏的名称一致，第三个参数是需要传递的参数）。

以上就是今天的所有内容，感谢大家耐心的阅读，希望大家都有所收获，愿大家代码无bug。

原文链接：https://zhuanlan.zhihu.com/p/582524287

# 【NO.21】高性能异步io机制：io_uring

io_uring 是 linux 内核 5.10 引入的异步 io 接口。相比起用户态的DPDK、SPDK，io_uring 作为内核的一部分，通过 mmap 的方式实现用户和内核共享内存，并基于 memory barrier 在这块内存上实现了两个无锁环形队列： submission queue ring(sq) 和 completion queue ring(cq)。 sq 用于用户程序向内核提交 IO 任务，内核执行完成的任务会放入cq，用户程序从 cq 获取结果。在提交任务和返回任务结果时，用户程序和内核共用环形队列中的数据，不再需要额外的数据拷贝。此外，io_uring 还提供了两种轮询 Polling 模式，可以避免提交任务时的系统调用，以及io完成后的中断通知。

## 1. 性能测试

### 1.1 FIO

iops 是指单位时间内系统能处理的I/O请求数量，用于存储设备性能测试。这里我们使用硬盘性能辅助测试工具 [FIO](https://link.zhihu.com/?target=https%3A//github.com/axboe/fio.git)，来直观感受异步 io: io_uring 的性能优势。

```
# 安装 fio
sudo apt install fio
# 运行方式
fio job_file
```

需要通过编写一个配置文件来预定义 FIO 将要以什么样的模式来执行任务。

FIO 的基本参数：

- rw readwrite：定义 IO 类型。随机读 randread、随机写 randwrite、顺序读 read、顺序写 write、顺序读写 rw readwrite ，随机混合读写 randrw
- bs, blocksize：IO 的块大小。默认 4k
- size: IO 传输的数据大小
- ioengine：IO 引擎。同步模式psync、异步模式io_uring
- iodepth：I/O 引擎若使用异步模式，保持队列深度
- direct: 是否使用非缓冲 io ，默认 false 缓冲 io

编写的 posix.fio 配置文件如下

```
[global]
thread=1
group_reporting=1
direct=1
verify=0
time_based=1
runtime=10
bs=16K
size=16384
iodepth=64
rw=randwrite
filename=Cat
ioengine=io_uring 

[test]
stonewall
description="variable bs"
```

实验结果：iops：psync 8k, io_uring 19.0k，由此可以看出异步 io 的性能优势。

### 1.2 rust_echo_benc

服务器性能测试方法

- 连接数
- 每个请求连接的大小
- 持续时间

epoll 与 io_uring 事件的区别

- epoll 设置完后，不更改。
- io_uring 设置一次，触发一次。

接下来，进行同步 epoll 与异步 io_uring 服务器的测试对比，代码见 liburing 测试代码

```
# 安装 rust_echo_benc
git clone https://github.com/haraldh/rust_echo_bench.git
cargo run --release

# 测试 
cargo run --release -- --address "127.0.0.1:9999" --number 1000 --duration 60 --length 512
```

实验结果：在网络 io 方面，io_uring并不明显。在磁盘 io 方面，io_uring 具有一定的优势。

## 2. io_uring

io_uring 提供了三个系统调用接口 `io_uring_setup`、`io_uring_enter`、`io_uring_register`

### 2.1 io_uring_setup

在 kernel 中创建：

- 提交队列 SQ：里面每一项是 sqe(submission queue event)，描述1个任务
- 完成队列 CQ：里面每一项是 cqe(completion queue event)，描述1个任务返回结果
- 提交队列项 SQEs 数组（Submission Queue Entries）

![img](https://pic1.zhimg.com/80/v2-9796a710eae4d235c00deac3663bf860_720w.webp)

SQ 和 CQ 采用 Ringbuffer 的结构，有 head 和 tail 两个成员，head = tail 时队列为空。每个节点保存的是 SQEs 数组的偏移量，实际的请求保存在 SQEs 数组中，这样就可以批量提交一组 SQEs 上不连续的请求。SQ 和 CQ 本身没有提供锁等同步机制，向 SQ中放入 sqe，从 CQ 中取出 cqe，都需要通过 memory barrier 来实现。

函数返回1个 fd 用于 io_uring 管理。用户将 fd 以 mmap 的方式映射到内存，实现了用户态和内核态的共享内存。

```
/*
- 参数1 entries：期望的 sq 长度。默认cq长度是sq的两倍
- 参数2 params: 配置io_uring，内核返回的 sq/cq 配置信息也通过它带回来
 */
int io_uring_setup(unsigned entries, struct io_uring_params *params)
    
struct io_uring_params {
    __u32 sq_entries;
    __u32 cq_entries;
    __u32 flags;
    __u32 sq_thread_cpu;
    __u32 sq_thread_idle;
    __u32 resv[5];
    struct io_sqring_offsets sq_off;
    struct io_cqring_offsets cq_off;
};
```



### 2.2 io_uring_enter

调用时，执行两个操作

- 提交 IO 请求：把 sqe 的索引尾插到 SQ 中，调用`io_uring_enter`提交到内核
- 等待 IO 完成：内核将完成的 IO 放到 CQ 中，用户轮询 CQ 来等待结果

![img](https://pic4.zhimg.com/80/v2-8f792738f40ff4fc2cb34003a211fcfb_720w.webp)

```
/*
- 参数1 fd：io_uring_setup返回的fd
- 参数2 to_submit: 一次提交多少个 sqe 到内核
- 参数3 min_complete: 要求内核至少等待min_complete个任务完成再返回
- 参数4 flags：接口控制行为，IORING_ENTER_GETEVENTS
 */
int io_uring_enter(unsigned int fd, u32 to_submit, u32 min_complete, u32 flags);
```

### 2.3 io_uring_register

注册用于异步 I/O 的文件或用户缓冲区

对于文件， 保持内核长时间持有该文件的索引。每次通过 sqe 向内核传递一个 fd，内核都需要通过 fd 找到对应的文件索引，完成该sqe 处理后，则将该索引释放。对于高 iops 的场景，这个开销会拖慢请求的速度。通过预先注册一组已经打开的文件。

对于缓冲区，保持内存的长期映射。内核在读写前进行page map，读写完成后，执行unmap。类似的，通过预注册，来避免多次的 map 和 unmap。

```
/*
- 参数1 fd：io_uring_setup返回的fd
- 参数2 opcode: 注册类型。
	文件类型: IORING_REGISTER_FILES；
	用户缓冲类型 buffer: IORING_REGISTER_BUFFERS
- 参数3 arg: 
	文件类型: 指向一个fd数组；
	用户缓冲类型：指向一个struct iovec的数组。
- 参数4 nr_args：arg数组的长度
 */
int io_uring_register(unsigned int fd, unsigned int opcode,
                      void *arg, unsigned int nr_args);
```

### 2.4 使用方法：cat 程序为例

接下来，基于 io_uring 的系统调用接口进行封装，实现自定义的 [uring_cat](https://link.zhihu.com/?target=https%3A//unixism.net/2020/04/io-uring-by-example-part-1-introduction/) 程序

```
// gcc -o uring_cat uring_cat.c
// ./uring_cat filename
#include <stdio.h>
#include <stdlib.h>
#include <sys/stat.h>
#include <sys/ioctl.h>
#include <sys/syscall.h>
#include <sys/mman.h>
#include <sys/uio.h>
#include <linux/fs.h>
#include <fcntl.h>
#include <unistd.h>
#include <string.h>

#include <linux/io_uring.h>


#define URING_QUEUE_DEPTH		1024
#define BLOCK_SZ    1024

// sqring
struct app_io_sq_ring {

	unsigned *head;
	unsigned *tail;

	unsigned *ring_mask;
	unsigned *ring_entries;

	unsigned *flags;
	unsigned *array;

};

// cqring
struct app_io_cq_ring {

	unsigned *head;
	unsigned *tail;

	unsigned *ring_mask;
	unsigned *ring_entries;

	struct io_uring_cqe *cqes;

};

// 提交器: cq, sq, sqe
struct submitter {

	int ring_fd;

	struct app_io_sq_ring sq_ring;
	struct app_io_cq_ring cq_ring;

	struct io_uring_sqe *sqes;

};

 -------------------

struct file_info {
	off_t file_sz;
	struct iovec iovecs[];
};

 -------------------
// 利用系统调用执行 io_uring_setup 流程
// 1、int 0x80 中断信号
// 2、mv arg1, eax
// 3、mv arg2, ebx
// 4、call sys_call_table: sys_call_table[__NR_io_uring_setup]

int io_uring_setup(unsigned entries, struct io_uring_params *p)
{
    return (int) syscall(__NR_io_uring_setup, entries, p);
}

int io_uring_enter(int ring_fd, unsigned int to_submit,
                          unsigned int min_complete, unsigned int flags)
{
    return (int) syscall(__NR_io_uring_enter, ring_fd, to_submit, min_complete,
                   flags, NULL, 0);
}

int app_setup_uring(struct submitter *s) {

	struct io_uring_params p;
	memset(&p, 0, sizeof(p));

	// 创建sq, cq, sqes
	s->ring_fd = io_uring_setup(URING_QUEUE_DEPTH, &p);
	if (s->ring_fd < 0) return -1;

	// 获取初始的sq,cq的大小，sq_off, cq_off起始偏移地址
	int sring_sz = p.sq_off.array + p.sq_entries * sizeof(unsigned);
	int cring_sz = p.cq_off.cqes + p.cq_entries * sizeof(struct io_uring_cqe);

	// io_uring特性：IORING_FEAT_SINGLE_MMAP：内核通过一次mmap完成sq, cq的映射
	// 即sq，cq共用1块内存，则两者大小必须设置相同
	if (p.features & IORING_FEAT_SINGLE_MMAP) {
		if (cring_sz > sring_sz) {
			sring_sz = cring_sz;
		}
		cring_sz = sring_sz;
	}

	// 1、将 sq 的映射到用户空间，sq_ptr 指向sq首地址
	void *sq_ptr = mmap(0, sring_sz, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_POPULATE,
						s->ring_fd, IORING_OFF_SQ_RING);
	if (sq_ptr == MAP_FAILED) return -1;

	// 2、将 cq 的映射到用户空间，cq_ptr 指向cq首地址
	void *cq_ptr;
	// 若共用一块内存，则两个指针指向相同
	if (p.features & IORING_FEAT_SINGLE_MMAP) {
		cq_ptr = sq_ptr;
	} else {
	// 若使用两块内存，则重新对cq进行mmap，
		cq_ptr = mmap(0, sring_sz, PROT_READ|PROT_WRITE, MAP_SHARED|MAP_POPULATE,
						s->ring_fd, IORING_OFF_CQ_RING);
		if (cq_ptr == MAP_FAILED) return -1;

	}

	struct app_io_sq_ring *sring = &s->sq_ring;
	struct app_io_cq_ring *cring = &s->cq_ring;

	sring->head = sq_ptr + p.sq_off.head;
	sring->tail = sq_ptr + p.sq_off.tail;

	sring->ring_mask = sq_ptr + p.sq_off.ring_mask;
	sring->ring_entries = sq_ptr + p.sq_off.ring_entries;

	sring->flags = sq_ptr + p.sq_off.flags;
	sring->array = sq_ptr + p.sq_off.array;

	// 3、将 seqs 映射到用户空间
	s->sqes = mmap(0, p.sq_entries * sizeof(struct io_uring_sqe), 
		PROT_READ | PROT_WRITE, MAP_SHARED | MAP_POPULATE, s->ring_fd, IORING_OFF_SQES);
	if (s->sqes == MAP_FAILED) {
		return 1;
	}

	cring->head = cq_ptr + p.cq_off.head;
	cring->tail = cq_ptr + p.cq_off.tail;
	cring->ring_mask = cq_ptr + p.cq_off.ring_mask;
	cring->ring_entries = cq_ptr + p.cq_off.ring_entries;
	cring->cqes = cq_ptr + p.cq_off.cqes;
	
	return 0;
}

off_t get_file_size(int fd) {
    struct stat st;
    if(fstat(fd, &st) < 0) {
        perror("fstat");
        return -1;
    }
    if (S_ISBLK(st.st_mode)) {
        unsigned long long bytes;
        if (ioctl(fd, BLKGETSIZE64, &bytes) != 0) {
            perror("ioctl");
            return -1;
        }
        return bytes;
    } else if (S_ISREG(st.st_mode))
        return st.st_size;
    return -1;
}


void output_to_console(char *buf, int len) {
    while (len--) {
        fputc(*buf++, stdout);
    }
}

void read_from_cq(struct submitter *s) {

	struct file_info *fi;

	struct app_io_cq_ring *cring = &s->cq_ring;
	struct io_uring_cqe *cqe;

	unsigned head = *cring->head;

	while (1) {

		//read_barrier();

		if (head == *cring->tail) break;

		cqe = &cring->cqes[head & *s->cq_ring.ring_mask];
		fi = (struct file_info*)cqe->user_data;

		if (cqe->res < 0) {
			fprintf(stderr, "Error: %d\n", cqe->res);
		}

		int blocks = fi->file_sz / BLOCK_SZ;
		if (fi->file_sz % BLOCK_SZ) blocks ++;

		
		int i = 0;
		while (++i < blocks) {
			output_to_console(fi->iovecs[i].iov_base, fi->iovecs[i].iov_len);
			printf("------------------------i : %d, blocks: %d\n", i, blocks);
		}
		head ++;

		printf("head: %d, tail: %d, blocks: %d\n", 
			head, *cring->tail, blocks);
	}

	*cring->head = head;

	printf("exit read_from_cq\n");
	//write_barrier();
	
}


int submit_to_sq(char *file_path, struct submitter *s) {

	int filefd = open(file_path, O_RDONLY);
	if (filefd < 0) {
		return -1;
	}

	struct app_io_sq_ring *sring = &s->sq_ring;

	off_t filesz = get_file_size(filefd);
	if (filesz < 0) return -1;

	off_t bytes_remaining = filesz;
	int blocks = filesz / BLOCK_SZ;

	if (filesz % BLOCK_SZ) blocks ++;

	struct file_info *fi = malloc(sizeof(struct file_info) + sizeof(struct iovec) * blocks);
	if (!fi) return -2;

	fi->file_sz = filesz;

	unsigned current_block;
	while (bytes_remaining) {

		off_t bytes_to_read = bytes_remaining;
		if (bytes_to_read > BLOCK_SZ) bytes_to_read = BLOCK_SZ;

		fi->iovecs[current_block].iov_len = bytes_to_read;


		void *buf;
		if (posix_memalign(&buf, BLOCK_SZ, BLOCK_SZ)) {
			return 1;
		}

		fi->iovecs[current_block].iov_base = buf;

		current_block ++;
		bytes_remaining -= bytes_to_read;

	}


	unsigned next_tail = 0, tail = 0, index = 0;

	next_tail = tail = *sring->tail;
	next_tail ++;

	index = tail & *s->sq_ring.ring_mask;

	struct io_uring_sqe *sqe = &s->sqes[index];
	sqe->fd = filefd;
	sqe->flags = 0;
	sqe->opcode = IORING_OP_READV;
	sqe->addr = (unsigned long)fi->iovecs;
	sqe->len = blocks;
	sqe->off = 0;

	sqe->user_data = (unsigned long long)fi;
	sring->array[index] = index;
	tail = next_tail;

	if (*sring->tail != tail) {
		*sring->tail = tail;
	}

	int ret = io_uring_enter(s->ring_fd, 1, 1, IORING_ENTER_GETEVENTS);
	if (ret < 0) {
		return 1;
	}
	
	return 0;
}


int main(int argc, char *argv[]) {

	struct submitter *s = malloc(sizeof(struct submitter));
	if (!s) {
		perror("malloc");
		return -1;
	}
	memset(s, 0, sizeof(struct submitter));

	// 1、setup
	if (app_setup_uring(s)) return 1;

	int i = 1;
	for (i = 1;i < argc;i ++) {
		// 2、submit
		if (submit_to_sq(argv[i], s)) {
			//fprintf(stderr, "Error reading file\n");
			return 1;
		}
		
		read_from_cq(s);

	}

	return 0;
}
```

## 3. liburing

由于 io_uring 使用起来比较麻烦，作者封装了 io_uring 接口，创作了 [liburing](https://link.zhihu.com/?target=https%3A//github.com/axboe/liburing) 库。

```
# 安装 liburing
git clone https://github.com/axboe/liburing.git
./configure 
make && make install
```

### **3.1 liburing api**

```
// 初始化io_uring，内部调用io_uring_setup
int io_uring_queue_init_params(unsigned entries, struct io_uring *ring,
				struct io_uring_params *p);

// 提交 sq 到内核，内核完成后移动到 cq，内部调用 io_uring_enter
// 1、提交io请求：将sqe的偏移信息加入到sq，提交sq到内核，不阻塞等待其完成
// 2、等待io完成：内核在io完成后，自动将sqe的偏移信息加入到cq
int io_uring_submit(struct io_uring *ring);

// 等待io完成，获取cqe
// 阻塞等待
unsigned io_uring_peek_batch_cqe(struct io_uring *ring,
	struct io_uring_cqe **cqes, unsigned count);
// 不阻塞等待
int io_uring_wait_cqes(struct io_uring *ring, struct io_uring_cqe **cqe_ptr,
		       unsigned wait_nr, struct __kernel_timespec *ts,
		       sigset_t *sigmask);

// 轮询 cq 队列，将 cq 队首后移动 nr 个
static inline void io_uring_cq_advance(struct io_uring *ring, unsigned nr)

// 和libaio封装的io_prep_writev一样
static inline void io_uring_prep_writev(struct io_uring_sqe *sqe, int fd,const struct iovec *iovecs, unsigned nr_vecs, off_t offset)

// 和libaio封装的io_prep_readv一样
static inline void io_uring_prep_readv(struct io_uring_sqe *sqe, int fd, const struct iovec *iovecs, unsigned nr_vecs, off_t offset)
    
// 销毁 io
void io_uring_queue_exit(struct io_uring *ring);
```

### 3.2 测试代码

利用 liburing 编写的简单测试 iouring_server

```
// gcc -o iouring_server iouring_server.c -luring
#include <liburing.h>

#include <stdio.h>
#include <string.h>

#include <sys/socket.h>
#include <netinet/in.h>

#include <unistd.h>

#define ENTRIES_LENGTH		4096

#define MAX_CONNECTIONS		1024
#define BUFFER_LENGTH		1024

char buf_table[MAX_CONNECTIONS][BUFFER_LENGTH] = {0};

// 传递的事件
enum {
	READ,
	WRITE,
	ACCEPT,
};

// 连接信息
struct conninfo {
	int connfd;	// fd
	int type;	// 事件类型
};

void set_read_event(struct io_uring *ring, int fd, void *buf, size_t len, int flags) {

	struct io_uring_sqe *sqe = io_uring_get_sqe(ring);

	// io_uring 读事件
	io_uring_prep_recv(sqe, fd, buf, len, flags);

	struct conninfo ci = {
		.connfd = fd,
		.type = READ
	};

	memcpy(&sqe->user_data, &ci, sizeof(struct conninfo));

}

void set_write_event(struct io_uring *ring, int fd, const void *buf, size_t len, int flags) {

	struct io_uring_sqe *sqe = io_uring_get_sqe(ring);

	// io_uring 写事件
	io_uring_prep_send(sqe, fd, buf, len, flags);

	struct conninfo ci = {
		.connfd = fd,
		.type = WRITE
	};

	memcpy(&sqe->user_data, &ci, sizeof(struct conninfo));
}

void set_accept_event(struct io_uring *ring, int fd,
	struct sockaddr *cliaddr, socklen_t *clilen, unsigned flags) {

	// 获取 sq 队列的空 sqe
	struct io_uring_sqe *sqe = io_uring_get_sqe(ring);

	// io_uring的accept事件：将fd放入到sqe里
	io_uring_prep_accept(sqe, fd, cliaddr, clilen, flags);

	// 用于回调函数
	struct conninfo ci = {
		.connfd = fd,
		.type = ACCEPT
	};

	memcpy(&sqe->user_data, &ci, sizeof(struct conninfo));
}

int main() {

	int listenfd = socket(AF_INET, SOCK_STREAM, 0);  
    if (listenfd == -1) return -1;

    struct sockaddr_in servaddr, clientaddr;
    servaddr.sin_family = AF_INET;
    servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
    servaddr.sin_port = htons(9999);

    if (-1 == bind(listenfd, (struct sockaddr*)&servaddr, sizeof(servaddr))) {
        return -2;
    }
	
	listen(listenfd, 10);

	struct io_uring_params params;
	memset(&params, 0, sizeof(params));


	// 初始化队列，内部调用io_uring_setup
	struct io_uring ring;
	io_uring_queue_init_params(ENTRIES_LENGTH, &ring, &params);

	socklen_t clilen = sizeof(clientaddr);
	set_accept_event(&ring, listenfd, (struct sockaddr*)&clientaddr, &clilen, 0);
	
	while (1) {

		// 封装 io_uring_enter
		// 1、提交io请求：将sqe的偏移信息加入到sq，提交sq到内核，不阻塞等待其完成
		// 2、等待io完成：内核在io完成后，自动将sqe的偏移信息加入到cq
		io_uring_submit(&ring);

		// 从获取 cqe 的两种方式
		// 1、阻塞等待io完成，获取 cqe
		struct io_uring_cqe *cqe;
		int ret = io_uring_wait_cqe(&ring, &cqe);

		// 2、不阻塞等待io完成，没有cqe返回错误，获取 cqe
		struct io_uring_cqe *cqes[10];
		int cqecount = io_uring_peek_batch_cqe(&ring, cqes, 10);

		int i = 0;
		unsigned count = 0;
		for (i = 0; i < cqecount; ++i) {

			cqe = cqes[i];
			count ++;

			struct conninfo ci;
			memcpy(&ci, &cqe->user_data, sizeof(ci));

			if (ci.type == ACCEPT) {

				int connfd = cqe->res;
				char *buffer = buf_table[connfd];
				
				set_read_event(&ring, connfd, buffer, 1024, 0);
				// io_uring 设置一次，触发一次
				set_accept_event(&ring, listenfd, (struct sockaddr*)&clientaddr, &clilen, 0);

			} else if (ci.type == READ) {

				int bytes_read = cqe->res;
				if (bytes_read == 0) {
					close(ci.connfd);
				} else if (bytes_read < 0) {

				} else {		
					char *buffer = buf_table[ci.connfd];
					set_write_event(&ring, ci.connfd, buffer, bytes_read, 0);
				}

			} else if (ci.type == WRITE) {
				char *buffer = buf_table[ci.connfd];
				set_read_event(&ring, ci.connfd, buffer, 1024, 0);
			}
		}
		
		// cq队列一次轮询完成后，因为cqe的取出，需要调整队首的位置，以便下次使用
		io_uring_cq_advance(&ring, count);
	}
}
```

原文链接：https://zhuanlan.zhihu.com/p/582318316

# 【NO.22】「查缺补漏」巩固你的Nginx知识体系

## 1.**基本介绍**

Nginx是一款轻量级的 Web服务器 / 反向代理服务器 / 电子邮件（IMAP/POP3）代理服务器，主要的优点是：

1. 支持高并发连接，尤其是静态界面，官方测试Nginx能够支撑5万并发连接
2. 内存占用极低
3. 配置简单，使用灵活，可以基于自身需要增强其功能，同时支持自定义模块的开发

使用灵活：可以根据需要，配置不同的负载均衡模式，URL地址重写等功能

1. 稳定性高，在进行反向代理时，宕机的概率很低
2. 支持热部署，应用启动重载非常迅速

## 2.**基础使用**

### **2.1 安装**

文件下载地址：[http://nginx.org/en/docs/windows.html](https://link.zhihu.com/?target=http%3A//nginx.org/en/docs/windows.html)

### 2.2 **基本命令**

```
# 启动
# 建议使用第一种，第二种会使窗口一直处于执行中，不能进行其他命令操作
C:\server\nginx-1.19.2> start nginx
C:\server\nginx-1.19.2> nginx.exe

# 停止
# stop是快速停止nginx，可能并不保存相关信息；quit是完整有序的停止nginx，并保存相关信息
C:\server\nginx-1.19.2> nginx.exe -s stop
C:\server\nginx-1.19.2> nginx.exe -s quit

# 重载Nginx
# 当配置信息修改，需要重新载入这些配置时使用此命令
C:\server\nginx-1.19.2> nginx.exe -s reload

# 重新打开日志文件
C:\server\nginx-1.19.2> nginx.exe -s reopen

# 查看Nginx版本
C:\server\nginx-1.19.2> nginx -v

# 查看配置文件是否正确
C:\server\nginx-1.19.2> nginx -t
```

### **2.3 简单Demo**

1. 利用`SwitchHost`软件编辑域名和IP的映射关系，或到目录`C:\Windows\System32\drivers\etc`下，编辑`hosts`文件，增加配置如下（Mac 同理）

```
   127.0.0.1  kerwin.demo.com
```

PS：推荐使用软件`SwitchHost`，工作时几乎是必用的

1. 修改配置，如图所示：

![img](https://pic1.zhimg.com/80/v2-b4b9567750fc4770edc6a0f91130b240_720w.webp)

效果如图所示：

![img](https://pic1.zhimg.com/80/v2-a3ce920a77c6f7ca65771b24bb5a2538_720w.webp)

## **3.Nginx在架构体系中的作用**

- 网关 （面向客户的总入口）
- 虚拟主机（为不同域名 / ip / 端口提供服务。如：VPS虚拟服务器）
- 路由（正向代理 / 反向代理）
- 静态服务器
- 负载集群（提供负载均衡）

### **3.1 网关**

网关：可以简单的理解为用户请求和服务器响应的关口，即面向用户的总入口

网关可以拦截客户端所有请求，对该请求进行权限控制、负载均衡、日志管理、接口调用监控等，因此无论使用什么架构体系，都可以使用`Nginx`作为最外层的网关

### 3.2**虚拟主机**

**虚拟主机的定义**：虚拟主机是一种特殊的软硬件技术，它可以将网络上的每一台计算机分成多个虚拟主机，每个虚拟主机可以独立对外提供 www 服务，这样就可以实现一台主机对外提供多个 web 服务，每个虚拟主机之间是独立的，互不影响的。

通过 Nginx 可以实现虚拟主机的配置，Nginx 支持三种类型的虚拟主机配置

- 基于 IP 的虚拟主机
- 基于域名的虚拟主机
- 基于端口的虚拟主机

表现形式其实大家多见过，即：

```
# 每个 server 就是一个虚拟主机
http {
    # ...
    server{
        # ...
    }
    
    # ...
    server{
        # ...
    }
}
```

### 3.3**路由**

在`Nginx`的配置文件中，我们经常可以看到这样的配置：

```
location / {
	#....
}
```

`location`在此处就起到了路由的作用，比如我们在同一个虚拟主机内定义两个不同的路由，如下：

```
location / {
	proxy_pass https://www.baidu.com/;
}
		
location /api {
	proxy_pass https://apinew.juejin.im/user_api/v1/user/get?aid=2608&user_id=1275089220013336&not_self=1;
 }
```

效果如下：

![img](https://pic2.zhimg.com/80/v2-301ac72979c8f9ca972a6944ff8019c9_720w.webp)

因为路由的存在，为我们后续解决`跨域问题`提供了一定的思路，同时配置内容和API接口等更加方便

PS：路由的功能非常强大，`支持正则匹配`

### 3.4**正向与反向代理**

此处额外解释一下`proxy_pass`的含义

在`Nginx`中配置`proxy_pass`代理转发时，如果在`proxy_pass`后面的url加 `/`，表示绝对根路径；

如果没有`/`，表示相对路径

**正向代理**

1. 代理客户;
2. 隐藏真实的客户，为客户端收发请求，使真实客户端对服务器不可见;
3. 一个局域网内的所有用户可能被一台服务器做了正向代理，由该台服务器负责 HTTP 请求;
4. 意味着同服务器做通信的是正向代理服务器;

**反向代理**

1. 代理服务器;
2. 隐藏了真实的服务器，为服务器收发请求，使真实服务器对客户端不可见;
3. 负载均衡服务器，将用户的请求分发到空闲的服务器上;
4. 意味着用户和负载均衡服务器直接通信，即用户解析服务器域名时得到的是负载均衡服务器的 IP ;

**共同点**

1. 都是做为服务器和客户端的中间层
2. 都可以加强内网的安全性，阻止 web 攻击
3. 都可以做缓存机制，提高访问速度

**区别**

1. 正向代理其实是客户端的代理,反向代理则是服务器的代理。
2. 正向代理中，服务器并不知道真正的客户端到底是谁；而在反向代理中，客户端也不知道真正的服务器是谁。
3. 作用不同。正向代理主要是用来解决访问限制问题；而反向代理则是提供负载均衡、安全防护等作用。

**相关视频推荐**

[8个nginx面试题，助你了解nginx的底层设计](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1eF411u71R/)

[6种epoll的做法，从redis，memcached到nginx的网络模型实现](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1fu411y78c/)

[16w行的nginx源码，如何才能读懂呢？全面分析nginx的机制](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1Ep4y1p7gn/)

学习地址：**[c/c++ linux服务器开发/后台架构师](https://link.zhihu.com/?target=https%3A//ke.qq.com/course/417774%3FflowToken%3D1013300)**

需要C/C++ Linux服务器架构师学习资料加qun**[812855908](https://link.zhihu.com/?target=https%3A//jq.qq.com/%3F_wv%3D1027%26k%3DkHaAoWMf)**获取（资料包括**C/C++，Linux，golang技术，Nginx，ZeroMQ，MySQL，Redis，fastdfs，MongoDB，ZK，流媒体，CDN，P2P，K8S，Docker，TCP/IP，协程，DPDK，ffmpeg**等），免费分享

![img](https://pic3.zhimg.com/80/v2-0fb9145c6e395f48de7c1d3a1d500dc2_720w.webp)

### 3.5**静态服务器**

静态服务器是`Nginx`的强项，使用非常容易，在默认配置下本身就是指向了静态的HTML界面，如：

```
location / {
	root   html;
	index  index.html index.htm;
}
```

所以前端同学们，如果构建好了界面，可以进行相应的配置，把界面指向目标文件夹中即可，`root`指的是`html`文件夹

### 3.6**负载均衡**

负载均衡功能是`Nginx`另一大杀手锏，一共有5种方式，着重介绍一下。

### 3.7**轮询**

每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除，配置如下：

```
upstream tomcatserver {
	server 192.168.0.1;
	server 192.168.0.2;
}
```

轮询策略是默认的负载均衡策略

### 3.7**指定权重**

即在轮询的基础之上，增加权重的概念，`weight`和访问比率成正比，用于后端服务器性能不均的情况，配置如下：

```
upstream tomcatserver {
	server 192.168.0.1 weight=1;
	server 192.168.0.2 weight=10;
}
```

### **3.8IP Hash**

每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题，配置如下：

```
upstream tomcatserver {
	ip_hash;
	server 192.168.0.14:88;
	server 192.168.0.15:80;
}
```

### 3.9**fair**

第三方提供的负载均衡策略，按后端服务器的响应时间来分配请求，响应时间短的优先分配，生产环境中有各种情况可能导致响应时间波动，需要慎用

```
upstream tomcatserver {
	server server1;
	server server2;
	fair;
}
```

### 3.10**url_hash**

第三方提供的负载均衡策略，按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器

```
upstream tomcatserver {
	server squid1:3128;
	server squid2:3128;
	hash $request_uri;
	hash_method crc32;
}
```

## 4.**Nginx的模块化设计**

先来看看`Nginx`模块架构图：

![img](https://pic2.zhimg.com/80/v2-f3bf04a705602a05f660ab7f6fac0149_720w.webp)

这5个模块由上到下重要性一次递减。

（1）核心模块；

核心模块是Nginx服务器正常运行必不可少的模块，如同操作系统的内核。它提供了Nginx最基本的核心服务。像进程管理、权限控制、错误日志记录等；

（2）标准HTTP模块；

标准HTTP模块支持标准的HTTP的功能，如：端口配置，网页编码设置，HTTP响应头设置等；

（3）可选HTTP模块；

可选HTTP模块主要用于扩展标准的HTTP功能，让Nginx能处理一些特殊的服务，如：解析GeoIP请求，SSL支持等；

（4）邮件服务模块；

邮件服务模块主要用于支持Nginx的邮件服务；

（5）第三方模块；

第三方模块是为了扩展Nginx服务器应用，完成开发者想要的功能，如：Lua支持，JSON支持等；

> 模块化设计使得Nginx方便开发和扩展，功能很强大

## **5.Nginx的请求处理流程**

基于上文中的`Nginx`模块化结构，我们很容易想到，在请求的处理阶段也会经历诸多的过程，`Nginx`将各功能模块组织成一条链，当有请求到达的时候，请求依次经过这条链上的部分或者全部模块，进行处理，每个模块实现特定的功能。

一个 HTTP Request 的处理过程：

- 初始化 HTTP Request
- 处理请求头、处理请求体
- 如果有的话，调用与此请求（URL 或者 Location）关联的 handler
- 依次调用各 phase handler 进行处理
- 输出内容依次经过 filter 模块处理

![img](https://pic1.zhimg.com/80/v2-bc3cd90e30dc7c1dadf5548c8c32fd7c_720w.webp)

## **6.Nginx的多进程模型**

Nginx 在启动后，会有一个 `master`进程和多个 `worker`进程。

`master`进程主要用来管理`worker`进程，包括接收来自外界的信号，向各 worker 进程发送信号，监控 worker 进程的运行状态以及启动 worker 进程。

`worker`进程是用来处理来自客户端的请求事件。多个 worker 进程之间是对等的，它们同等竞争来自客户端的请求，各进程互相独立，一个请求只能在一个 worker 进程中处理。worker 进程的个数是可以设置的，一般会设置与机器 CPU 核数一致，这里面的原因与事件处理模型有关

Nginx 的进程模型，可由下图来表示：

![img](https://pic4.zhimg.com/80/v2-1fe4b562a2abae1177cc82e4b543eb63_720w.webp)

这种设计带来以下优点：

**1） 利用多核系统的并发处理能力**

现代操作系统已经支持多核 CPU 架构，这使得多个进程可以分别占用不同的 CPU 核心来工作。Nginx 中所有的 worker 工作进程都是完全平等的。这提高了网络性能、降低了请求的时延。

**2） 负载均衡**

多个 worker 工作进程通过进程间通信来实现负载均衡，即一个请求到来时更容易被分配到负载较轻的 worker 工作进程中处理。这也在一定程度上提高了网络性能、降低了请求的时延。

**3） 管理进程会负责监控工作进程的状态，并负责管理其行为**

管理进程不会占用多少系统资源，它只是用来启动、停止、监控或使用其他行为来控制工作进程。首先，这提高了系统的可靠性，当 worker 进程出现问题时，管理进程可以启动新的工作进程来避免系统性能的下降。其次，管理进程支持 Nginx 服务运行中的程序升级、配置项修改等操作，这种设计使得动态可扩展性、动态定制性较容易实现。

## **7.Nginx如何解决惊群现象**

### **7.1什么是惊群现象？**

惊群效应（thundering herd）是指多进程（多线程）在同时阻塞等待同一个事件的时候（休眠状态），如果等待的这个事件发生，那么他就会唤醒等待的所有进程（或者线程），但是最终却只能有一个进程（线程）获得这个时间的“控制权”，对该事件进行处理，而其他进程（线程）获取“控制权”失败，只能重新进入休眠状态，这种现象和性能浪费就叫做惊群效应。

上文中介绍了Nginx的多进程模型，而典型的多进程模型正如文中所说，多个worker进程之间是对等的，因此当一个请求到来的时候，所有进程会同时开始竞争，最终执行的又只有一个，这样势必会造成资源的浪费。

Nginx解决该问题的思路是：**不让多个进程在同一时间监听接受连接的socket，而是让每个进程轮流监听**，这样当有连接过来的时候，就只有一个进程在监听那肯定就没有惊群的问题。

具体做法是：利用一把进程间锁，每个进程中都**尝试**获得这把锁，如果获取成功将监听socket加入wait集合中，并设置超时等待连接到来，没有获得锁的进程则将监听socket从wait集合去除。

## **8.事件驱动模型和异步非阻塞IO**

承接上文，我们知道了Nginx的多进程模型后了解到，其工作进程实际上只有几个，但为什么依然能获得如此高的并发性能，当然与其采用的事件驱动模型和异步非阻塞IO的方式来处理请求有关。

Nginx服务器响应和处理Web请求的过程，是基于事件驱动模型的，它包含事件收集器、事件发送器和事件处理器等三部分基本单元，着重关注`事件处理器`，而一般情况下事件处理器有这么几种办法：

- 事件发送器每传递过来一个请求，目标对象就创建一个新的进程
- 事件发送器每传递过来一个请求，目标对象就创建一个新的线程，来进行处理
- 事件发送器每传递过来一个请求，目标对象就将其放入一个待处理事件的列表，使用非阻塞I/O方式调用

第三种方式，在编写程序代码时，逻辑比前面两种都复杂。大多数网络服务器采用了第三种方式，逐渐形成了所谓的`事件驱动处理库`。

`事件驱动处理库`又被称为`多路IO复用方法`，最常见的包括以下三种：select模型，poll模型和epoll模型。

其中Nginx就默认使用的是`epoll`模型，同时也支持其他事件模型。

`epoll`的帮助就在于其提供了一种机制，可以让进程同时处理多个并发请求，不用关心IO调用的具体状态。IO调用完全由事件驱动模型来管理，这样一来，当某个工作进程接收到客户端的请求以后，调用IO进行处理，如果不能立即得到结果，就去处理其他的请求；而工作进程在此期间也无需等待响应，可以去处理其他事情；当IO返回时，`epoll`就会通知此工作进程；该进程得到通知后，会来继续处理未完的请求

## **9.Nginx配置的最佳实践**

在生产环境或者开发环境中Nginx一般会代理多个虚拟主机，如果把所有的配置文件写在默认的`nginx.conf`中，看起来会非常臃肿，因此建议将每一个虚拟文件单独放置一个文件夹，Nginx支持这样的配置，如下：

```
http {
	# 省略中间配置

	# 引用该目录下以 .conf 文件结尾的配置
    include /etc/nginx/conf.d/*.conf;
}
```

具体文件配置如：

```
# Demo
upstream web_pro_testin {
	server 10.42.46.70:6003 max_fails=3 fail_timeout=20s;
	ip_hash;
}
 
server {
	listen 80;
	server_name web.pro.testin.cn;
	location / {
		proxy_pass http://web_pro_testin;
		proxy_redirect off;
		proxy_set_header Host $host;
		proxy_set_header X-Real-IP $remote_addr;
    }
 
    location ~ ^/(WEB-INF)/ {
		deny all;
    }
}
```

## 10.**Nginx全量配置参数说明**

```
# 运行用户
user www-data;    

# 启动进程,通常设置成和cpu的数量相等
worker_processes  6;

# 全局错误日志定义类型，[debug | info | notice | warn | error | crit]
error_log  logs/error.log;
error_log  logs/error.log  notice;
error_log  logs/error.log  info;

# 进程pid文件
pid        /var/run/nginx.pid;

# 工作模式及连接数上限
events {
    # 仅用于linux2.6以上内核,可以大大提高nginx的性能
    use   epoll; 
    
    # 单个后台worker process进程的最大并发链接数
    worker_connections  1024;     
    
    # 客户端请求头部的缓冲区大小
    client_header_buffer_size 4k;
    
    # keepalive 超时时间
    keepalive_timeout 60;      
    
    # 告诉nginx收到一个新连接通知后接受尽可能多的连接
    # multi_accept on;            
}

#设定http服务器，利用它的反向代理功能提供负载均衡支持
http {
    # 文件扩展名与文件类型映射表义
    include       /etc/nginx/mime.types;
    
    # 默认文件类型
    default_type  application/octet-stream;
    
    # 默认编码
    charset utf-8;
    
    # 服务器名字的hash表大小
    server_names_hash_bucket_size 128;
    
    # 客户端请求头部的缓冲区大小
    client_header_buffer_size 32k;
    
    # 客户请求头缓冲大小
	large_client_header_buffers 4 64k;
	
	# 设定通过nginx上传文件的大小
    client_max_body_size 8m;
    
    # 开启目录列表访问，合适下载服务器，默认关闭。
    autoindex on;

    # sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy 方式）来输出文件，对于普通应用，
    # 必须设为 on,如果用来进行下载等应用磁盘IO重负载应用，可设置为 off，以平衡磁盘与网络I/O处理速度
    sendfile        on;
    
    # 此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用
    #tcp_nopush     on;

    # 连接超时时间（单秒为秒）
    keepalive_timeout  65;
    
    
    # gzip模块设置
    gzip on;               #开启gzip压缩输出
    gzip_min_length 1k;    #最小压缩文件大小
    gzip_buffers 4 16k;    #压缩缓冲区
    gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0）
    gzip_comp_level 2;     #压缩等级
    gzip_types text/plain application/x-javascript text/css application/xml;
    gzip_vary on;

    # 开启限制IP连接数的时候需要使用
    #limit_zone crawler $binary_remote_addr 10m;
   
	# 指定虚拟主机的配置文件，方便管理
    include /etc/nginx/conf.d/*.conf;


    # 负载均衡配置
    upstream mysvr {
        # 请见上文中的五种配置
    }


   # 虚拟主机的配置
    server {
        
        # 监听端口
        listen 80;

        # 域名可以有多个，用空格隔开
        server_name www.jd.com jd.com;
        
        # 默认入口文件名称
        index index.html index.htm index.php;
        root /data/www/jd;

        # 图片缓存时间设置
        location ~ .*.(gif|jpg|jpeg|png|bmp|swf)${
            expires 10d;
        }
         
        #JS和CSS缓存时间设置
        location ~ .*.(js|css)?${
            expires 1h;
        }
         
        # 日志格式设定
        #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址；
        #$remote_user：用来记录客户端用户名称；
        #$time_local： 用来记录访问时间与时区；
        #$request： 用来记录请求的url与http协议；
        #$status： 用来记录请求状态；成功是200，
        #$body_bytes_sent ：记录发送给客户端文件主体内容大小；
        #$http_referer：用来记录从那个页面链接访问过来的；
        log_format access '$remote_addr - $remote_user [$time_local] "$request" '
        '$status $body_bytes_sent "$http_referer" '
        '"$http_user_agent" $http_x_forwarded_for';
         
        # 定义本虚拟主机的访问日志
        access_log  /usr/local/nginx/logs/host.access.log  main;
        access_log  /usr/local/nginx/logs/host.access.404.log  log404;
         
        # 对具体路由进行反向代理
        location /connect-controller {
 
            proxy_pass http://127.0.0.1:88;
            proxy_redirect off;
            proxy_set_header X-Real-IP $remote_addr;
             
            # 后端的Web服务器可以通过X-Forwarded-For获取用户真实IP
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header Host $host;

            # 允许客户端请求的最大单文件字节数
            client_max_body_size 10m;

            # 缓冲区代理缓冲用户端请求的最大字节数，
            client_body_buffer_size 128k;

            # 表示使nginx阻止HTTP应答代码为400或者更高的应答。
            proxy_intercept_errors on;

            # nginx跟后端服务器连接超时时间(代理连接超时)
            proxy_connect_timeout 90;

            # 后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据
            proxy_send_timeout 90;

            # 连接成功后，后端服务器响应的超时时间
            proxy_read_timeout 90;

            # 设置代理服务器（nginx）保存用户头信息的缓冲区大小
            proxy_buffer_size 4k;

            # 设置用于读取应答的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k
            proxy_buffers 4 32k;

            # 高负荷下缓冲大小（proxy_buffers*2）
            proxy_busy_buffers_size 64k;

            # 设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长
            # 设定缓存文件夹大小，大于这个值，将从upstream服务器传
            proxy_temp_file_write_size 64k;
        }
        
        # 动静分离反向代理配置（多路由指向不同的服务端或界面）
        location ~ .(jsp|jspx|do)?$ {
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_pass http://127.0.0.1:8080;
        }
    }
}
```

## 11.**Nginx还能做什么**

### 11.1**解决CORS跨域问题**

思路有两个：

- 基于多路由，把跨域的两个请求发到各自的服务器，然后统一访问入口即可避免该问题
- 利用Nginx配置Headerd的功能，为其附上相应的请求头

### 11.2**适配 PC 或移动设备**

根据用户设备不同返回不同样式的站点，以前经常使用的是纯前端的自适应布局，但无论是复杂性和易用性上面还是不如分开编写的好，比如我们常见的淘宝、京东......这些大型网站就都没有采用自适应，而是用分开制作的方式，根据用户请求的 `user-agent` 来判断是返回 PC 还是 H5 站点

### 11.3**请求限流**

Nginx按请求速率限速模块使用的是漏桶算法，即能够强行保证请求的实时处理速度不会超过设置的阈值，如：

```
http {
    limit_req_zone $binary_remote_addr zone=one:10m rate=1r/s;
    server {
        location /search/ {
            limit_req zone=one burst=5 nodelay;
        }
    }
}
```

### 11.4**其他技巧**

如：图片防盗链，请求过滤，泛域名转发，配置HTTPS等等

## **12.常见问题**

### **Q1：Nginx一般用作什么？**

> 见上文中Nginx在架构体系中的作用，配合Nginx还能做什么作答即可

### **Q2：为什么要用Nginx？**

> 理解网关的必要性，以及Nginx保证高可用，负载均衡的能力

### **Q3：为什么Nginx这么快？**

如果一个server采用一个进程负责一个request的方式，那么进程数就是并发数。那么显而易见的，就是会有很多进程在等待中。等什么？最多的应该是等待网络传输。

而nginx 的异步非阻塞工作方式正是利用了这点等待的时间。在需要等待的时候，这些进程就空闲出来待命了。因此表现为少数几个进程就解决了大量的并发问题。

nginx是如何利用的呢，简单来说：同样的4个进程，如果采用一个进程负责一个request的方式，那么，同时进来4个request之后，每个进程就负责其中一个，直至会话关闭。期间，如果有第5个request进来了。就无法及时反应了，因为4个进程都没干完活呢，因此，一般有个调度进程，每当新进来了一个request，就新开个进程来处理。

nginx不这样，每进来一个request，会有一个worker进程去处理。但不是全程的处理，处理到什么程度呢？处理到可能发生阻塞的地方，比如向上游（后端）服务器转发request，并等待请求返回。那么，这个处理的worker不会这么傻等着，他会在发送完请求后，注册一个事件：“如果upstream返回了，告诉我一声，我再接着干”。于是他就休息去了。此时，如果再有request 进来，他就可以很快再按这种方式处理。而一旦上游服务器返回了，就会触发这个事件，worker才会来接手，这个request才会接着往下走。

由于web server的工作性质决定了每个request的大部份生命都是在网络传输中，实际上花费在server机器上的时间片不多。这是几个进程就解决高并发的秘密所在。

> 总结：事件模型，异步非阻塞，多进程模型加上细节优化的共同作用

### **Q4：什么是正向代理和反向代理**

> 见上文

### **Q5：Nginx负载均衡的算法有哪些？**

> 见上文

### **Q6：Nginx如何解决的惊群现象？**

> 见上文

### **Q7：Nginx为什么不用多线程模型？**

> 深入理解多进程模型加上异步非阻塞IO的好处以及多线程模型中上下文切换的劣势

### **Q8：Nginx压缩功能有什么坏处吗？**

> 非常耗费服务器的CPU

### **Q9：Nginx有几种进程模型?**

> 实际上有两种，多进程和单进程，但是实际工作中都是多进程的

原文链接：https://zhuanlan.zhihu.com/p/582188212

# 【NO.23】一篇文章彻底搞懂websocket协议的原理与应用（一）

## 0.前言

![img](https://pic4.zhimg.com/80/v2-47c47bee014ade2ff0f82e457bfb98c7_720w.webp)

WebSocket协议是基于TCP的一种新的网络协议。它实现了浏览器与服务器全双工(full-duplex)通信——允许服务器主动发送信息给客户端。

WebSocket通信协议于2011年被IETF定为标准RFC 6455，并被RFC7936所补充规范。

## 1.WebSocket简介

webSocket是什么:

1、WebSocket是一种在单个TCP连接上进行全双工通信的协议

2、WebSocket使得客户端和服务器之间的数据交换变得更加简单，允许服务端主动向客户端推送数据

3、在WebSocket API中，浏览器和服务器只需要完成一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输

4、需要安装第三方包：cmd中：go get -u -v [github.com/gorilla/websocket](https://link.zhihu.com/?target=http%3A//github.com/gorilla/websocket)

WebSocket 是一种标准协议，用于在客户端和服务端之间进行双向数据传输。但它跟 HTTP 没什么关系，它是一种基于 TCP 的一种独立实现。

以前客户端想知道服务端的处理进度，要不停地使用 Ajax 进行轮询，让浏览器隔个几秒就向服务器发一次请求，这对服务器压力较高。另外一种轮询就是采用 long poll 的方式，这就跟打电话差不多，没收到消息就一直不挂电话，也就是说，客户端发起连接后，如果没消息，就一直不返回 Response 给客户端，连接阶段一直是阻塞的。

而 WebSocket 解决了 HTTP 的这几个难题。首先，当服务器完成协议升级后（ HTTP -> WebSocket ），服务端可以主动推送信息给客户端，解决了轮询造成的同步延迟问题。由于 WebSocket 只需要一次 HTTP 握手，服务端就能一直与客户端保持通讯，直到关闭连接，这样就解决了服务器需要反复解析 HTTP 协议，减少了资源的开销。

WebSocket协议支持（在受控环境中运行不受信任的代码的）客户端与（选择加入该代码的通信的）远程主机之间进行全双工通信。用于此的安全模型是Web浏览器常用的基于原始的安全模式。 协议包括一个开放的握手以及随后的TCP层上的消息帧。 该技术的目标是为基于浏览器的、需要和服务器进行双向通信的（服务器不能依赖于打开多个HTTP连接（例如，使用XMLHttpRequest或和长轮询））应用程序提供一种通信机制。

![img](https://pic2.zhimg.com/80/v2-e84d51f6e63c249e9464cfed1cf57519_720w.webp)

websocket 是一个基于应用层的网络协议，建立在tcp 协议之上，和 http 协议可以说是兄弟的关系，但是这个兄弟有点依赖 http ，为什么这么说呢？我们都知道 HTTP 实现了三次握手来建立通信连接，实际上 websocket 的创始人很聪明，他不想重复的去造轮子，反正我兄弟已经实现了握手了，我干嘛还要重写一套呢？先让它去冲锋陷阵呢，我坐收渔翁之利不是更香 吗，所以一般来说，我们会先用 HTTP 先进行三次握手，再向服务器请求升级为websocket 协议，这就好比说，嘿兄弟你先去给我排个队占个坑位建个小房子，到时候我在把这房子改造成摩天大楼。而且一般来说 80 和 443 端口一般 web 服务端都会外放出去，这样可以有效的避免防火墙的限制。当然，你创建的 websocket 服务端进程的端口也需要外放出去。

很多人会想问，web开发 使用 HTTP 协议不是已经差不多够用了吗？为什么还要我再多学一种呢？这不是搞事情嘛，仔细想想，一门新技术的产生必然有原因的，如果没有需求，我们干嘛那么蛋疼去写那么多东西，就是因为 HTTP 这个协议有些业务需求支持太过于鸡肋了，从 HTTP 0.9 到现在的 HTTP3.0 ，HTTP协议可以说说是在普通的web开发领域已经是十分完善且高效的了，说这个协议养活了全球半数的公司也不为过吧，像 2.0 服务器推送技术，3.0 采用了 UDP 而放弃了原来的 TCP ,这些改动都是为了进一步提升协议的性能，然而大家现在还是基本使用的 HTTP 1.1 这个最为经典的协议, 也是让开发者挺尴尬的。

绝大多数的web开发都是应用层开发者,大多数都是基于已有的应用层去开发应用，可以说我们最熟悉、日常打交道最多的就是应用层协议了，底下 TCP/IP 协议我们基本很少会去处理，当然大厂可能就不一样了，自己弄一套协议也是正常的，这大概也是程序员和码农的区别吧，搬砖还是创新，差别还是很大的。网络这种分层协议的好处我在之前的文章也说过了，这种隔离性很方便就可以让我们基于原来的基础去拓展，具有较好的兼容性。

总的来说，它就是一种依赖HTTP协议的，支持全双工通信的一种应用层网络协议。

## 2.WebSocket产生背景

简单的说，WebSocket协议之前，双工通信是通过多个http链接来实现，这导致了效率低下。WebSocket解决了这个问题。下面是标准RFC6455中的产生背景概述。

长久以来, 创建实现客户端和用户端之间双工通讯的web app都会造成HTTP轮询的滥用: 客户端向主机不断发送不同的HTTP呼叫来进行询问。

**这会导致一系列的问题：**

- 1.服务器被迫为每个客户端使用许多不同的底层TCP连接：一个用于向客户端发送信息，其它用于接收每个传入消息。
- 2.有些协议有很高的开销，每一个客户端和服务器之间都有HTTP头。
- 3.客户端脚本被迫维护从传出连接到传入连接的映射来追踪回复。

一个更简单的解决方案是使用单个TCP连接双向通信。 这就是WebSocket协议所提供的功能。 结合WebSocket API ，WebSocket协议提供了一个用来替代HTTP轮询实现网页到远程主机的双向通信的方法。

WebSocket协议被设计来取代用HTTP作为传输层的双向通讯技术,这些技术只能牺牲效率和可依赖性其中一方来提高另一方，因为HTTP最初的目的不是为了双向通讯。

## 3.WebSocket实现原理

![img](https://pic3.zhimg.com/80/v2-cc90a0344d7e70f6215bb5d1a14d340e_720w.webp)

在实现websocket连线过程中，需要通过浏览器发出websocket连线请求，然后服务器发出回应，这个过程通常称为“握手” 。在 WebSocket API，浏览器和服务器只需要做一个握手的动作，然后，浏览器和服务器之间就形成了一条快速通道。两者之间就直接可以数据互相传送。在此WebSocket 协议中，为我们实现即时服务带来了两大好处：

\1. Header：互相沟通的Header是很小的-大概只有 2 Bytes。

\2. Server Push：服务器的推送，服务器不再被动的接收到浏览器的请求之后才返回数据，而是在有新数据时就主动推送给浏览器。

## 4.WebSocket协议举例

![img](https://pic1.zhimg.com/80/v2-7a2acc6bc32134ee998b50ff9f331b60_720w.webp)

浏览器请求:

- GET /webfin/websocket/ HTTP/1.1。
- Host: localhost。
- Upgrade: websocket。
- Connection: Upgrade。
- Sec-WebSocket-Key: xqBt3ImNzJbYqRINxEFlkg==。
- Origin: [http://服务器地址](https://link.zhihu.com/?target=http%3A//xn--zfru1gfr6bz63i/)。
- Sec-WebSocket-Version: 13。

服务器回应:

- HTTP/1.1 101 Switching Protocols。
- Upgrade: websocket。
- Connection: Upgrade。
- Sec-WebSocket-Accept: K7DJLdLooIwIG/MOpvWFB3y3FE8=。
- WebSocket借用http请求进行握手，相比正常的http请求，多了一些内容。其中：
- Upgrade: websocket。
- Connection: Upgrade。
- 表示希望将http协议升级到Websocket协议。Sec-WebSocket-Key是浏览器随机生成的base64 encode的值，用来询问服务器是否是支持WebSocket。

服务器返回:

- Upgrade: websocket。
- Connection: Upgrade。
- 告诉浏览器即将升级的是Websocket协议

Sec-WebSocket-Accept是将请求包“Sec-WebSocket-Key”的值，与”258EAFA5-E914-47DA-95CA-C5AB0DC85B11″这个字符串进行拼接，然后对拼接后的字符串进行sha-1运算，再进行base64编码得到的。用来说明自己是WebSocket助理服务器。

Sec-WebSocket-Version是WebSocket协议版本号。RFC6455要求使用的版本是13，之前草案的版本均应当被弃用。

## 5.WebSocket使用

### 5.1 WebSocket 介绍

WebSocket 发起单个请求，服务端不需要等待客服端，客户端在任何时候也能发消息到服务端，减少了轮询时候的延迟.经历一次连接后，服务器能给客户端发多次。下图是轮询与WebSocket的区别。

![img](https://pic1.zhimg.com/80/v2-460cb0b19bf881cc5fdd4d18b8e531c8_720w.webp)

基于http的实时消息是相当的复杂，在无状态的请求中维持回话的状态增加了复杂度，跨域也很麻烦，使用ajax处理请求有序请求需要考虑更多。通过ajax进行交流也不简单。每一个延伸http功能的目的不是增加他的复杂度。websocket 可以大大简化实时通信应用中的链接。

Websocket是一种底层网络协议，可以让你在这个基础上建立别的标准协议。比如在WebSocket的客户端的基础上使用XMPP登录不同的聊天服务器，因为所有的XMPP服务理解相同的标准协议。WebSocket是web应用的一种创新。

为了与其他平台竞争，WebSocket是H5应用提供的一部分先进功能。每个操作系统都需要网络功能，能够让应用使用Sockets与别的主机进行通信，是每个大平台的核心功能。在很多方面，让Web应用表现的像操作系统平台是html5的趋势。像socket这样底层的网络协议APIs不会符合原始的安全模型，也不会有web api那样的设计风格。WebSocket给H5应用提供TCP的方式不会消弱网络安全且有现代的Api。

WebSocket是Html5平台的一个重要组件也是开发者强有力的工具。简单的说，你需要WebSocket创建世界级的web应用。它弥补了http不适合实时通信的重大缺陷。异步、双向通信模式，通过传输层协议使WebSocket具有普遍灵活性。想象一下你能用WebSocket创建正真实实时应用的所有方式。比如聊天、协作文档编辑、大规模多人在线游戏（MMO）,股票交易应用等等。

WebSocket是一个协议，但也有一个WebSocket API，这让你的应用去控制WebSocket的协议去响应被服务端触发的事件。API是W3C开发，协议是IETE制定。现代浏览器支持WebSocket API，这包括使用全双工和双向链接的方法和特性。让你执行像打开关闭链接、发送接收消息、监听服务端事件等必要操作。

### 5.2 WebSocket API

WebSocket API其实就是一个使用WebSocket协议的接口，通过它来建立全双工通道来收发消息，简单易学，要连接远程服务器，只需要创建一个WebSocket对象实体，并传入一个服务端的URL。在客户端和服务端一开始握手的期间，http协议升级到WebSocket协议就建立了连接，底层都是TCP协议。一旦建立连接，通过WebSocket接口可以反复的发送消息。在你的代码里面，你可以使用异步事件监听连接生命周期的每个阶段。

WebSocket API是纯事件驱动，一旦建立全双工连接，当服务端给客户端发送数据或者资源，它能自动发送状态改变的数据和通知。所以你不需要为了状态的更新而去轮训Server，在客户端监听即可。

首先，我们需要通过调用WebSocket构造函数来创建一个WebSocket连接，构造函数会返回一个WebSocket实例，可以用来监听事件。这些事件会告诉你什么时候连接建立，什么时候消息到达，什么时候连接关闭了，以及什么时候发生了错误。WebSocket协议定义了两种URL方案，WS和WSS分别代表了客户端和服务端之间未加密和加密的通信。WS(WebSocket)类似于Http URL，而WSS（WebSocket Security）URL 表示连接是基于安全传输层（TLS/SSL）和https的连接是同样的安全机制。

WebSocket的构造函数需要一个URL参数和一个可选的协议参数（一个或者多个协议的名字），协议的参数例如XMPP（Extensible Messaging and Presence Protocol）、SOAP（Simple Object Access Protocol）或者自定义协议。而URL参数需要以WS://或者WSS://开头，例如：ws://[http://www.websocket.org](https://link.zhihu.com/?target=http%3A//www.websocket.org)，如果URL有语法错误，构造函数会抛出异常。

```
// Create new WebSocket connection
var ws = new WebSocket("ws://www.websocket.org");
//测试了下链接不上。
```

![img](https://pic2.zhimg.com/80/v2-52654c7090642d0302d7b06eb5d61a11_720w.webp)

第二个参数是协议名称，是可选的，服务端和客服端使用的协议必须一致，这样收发消息彼此才能理解，你可以定义一个或多个客户端使用的协议，服务端会选择一个来使用，一个客服端和一个服务端之间只能有一个协议。当然都得基于WebSocket，WebSocket的重大好处之一就是基于WebSocket协议的广泛使用，让你的Web能够拥有传统桌面程序那样的能力。

言归正传，我们回到构造函数，在第一次握手之后，和协议的名称一起，客户端会发送一个Sec-WebSocket-Protocol 头，服务端会选择0个或一个协议，响应会带上同样的Sec-WebSocket-Protocol 头，否则会关闭连接。通过协议协商（Protocol negotiation ），我们可以知道给定的WebSocket服务器所支持的协议和版本，然后应用选择协议使用。

```
// Connecting to the server with one protocol called myProtocol
var ws = new WebSocket("ws://echo.websocket.org", "myProtocol");
//myProtocol 是假设的一个定义好的且符合标准的协议。
```

你可以传递一个协议的数组。

```
var echoSocket = new WebSocket("ws://echo.websocket.org", ["com.kaazing.echo","example.imaginary.protocol"])
//服务端会选择其中一个使用
echoSocket.onopen = function(e) {
// Check the protocol chosen by the server
console.log(echoSocket.protocol);
}
```

输出：com.kaazing.ech

协议这个参数有三种。

1.注册协议：根据RFC6455（WebSocket 协议）和IANA被官方注册的标准协议。例如 微软的SOAP。

![img](https://pic4.zhimg.com/80/v2-d799e900f3493fee0b65c893ae1e4b83_720w.webp)

看到两个华为的：

2.开放协议：被广泛使用的标注协议，例如XMPP和STOMP。但没有被正式注册。

3.自定义协议：自己编写和使用的WebSocket的协议。 协议会再后续章节给出详细介绍，下面先看事件、对象和方法以及实例。

### 5.3 WebSocket事件

WebSocket API是纯事件驱动，通过监听事件可以处理到来的数据和改变的链接状态。客户端不需要为了更新数据而轮训服务器。服务端发送数据后，消息和事件会异步到达。WebSocket编程遵循一个异步编程模型，只需要对WebSocket对象增加回调函数就可以监听事件。你也可以使用addEventListener()方法来监听。而一个WebSocket对象分四类不同事件。

#### 5.3.1 open

一旦服务端响应WebSocket连接请求，就会触发open事件。响应的回调函数称为onopen。

```
// Event handler for the WebSocket connection opening
ws.onopen = function(e) {
console.log("Connection open...");
};
```

open事件触发的时候，意味着协议握手结束，WebSocket已经准备好收发数据。如果你的应用收到open事件，就可以确定服务端已经处理了建立连接的请求，且同意和你的应用通信。

#### **5.3.2 Message**

当消息被接受会触发消息事件，响应的回调函数叫做onmessage。如下：

```
// 接受文本消息的事件处理实例：
ws.onmessage = function(e) {
if(typeof e.data === "string"){
console.log("String message received", e, e.data);
} else {
console.log("Other message received", e, e.data);
}
};
```

除了文本消息，WebSocket消息机制还能处理二进制数据，有Blob和ArrayBuffer两种类型，在读取到数据之前需要决定好数据的类型。

```
// 设置二进制数据类型为blob（默认类型）
ws.binaryType = "blob";
// Event handler for receiving Blob messages
ws.onmessage = function(e) {
if(e.data instanceof Blob){
console.log("Blob message received", e.data);
var blob = new Blob(e.data);
}
};
```



```
//ArrayBuffer
ws.binaryType = "arraybuffer";
ws.onmessage = function(e) {
if(e.data instanceof ArrayBuffer){
console.log("ArrayBuffer Message Received", + e.data);
// e.data即ArrayBuffer类型
var a = new Uint8Array(e.data);
}
};
```

#### 5.3.**3 Error**

如果发生意外的失败会触发error事件，相应的函数称为onerror,错误会导致连接关闭。如果你收到一个错误事件，那么你很快会收到一个关闭事件，在关闭事件中也许会告诉你错误的原因。而对错误事件的处理比较适合做重连的逻辑。

```
//异常处理
ws.onerror = function(e) {
console.log("WebSocket Error: " , e);
//Custom function for handling errors
handleErrors(e);
};
```

#### **5.3.4 Close**

不言而喻，当连接关闭的时候回触发这个事件，对应onclose方法，连接关闭之后，服务端和客户端就不能再收发消息。

WebSocket的规范其实还定义了ping和pong 架构（frames），可以用来做keep-alive，心跳，网络状态查询，latency instrumentation（延迟仪表？），但是目前 WebSocket API还没有公布这些特性，尽管浏览器支持了ping，但不会触发ping事件，相反，浏览器会自动响应pong，第八章会将更多关于ping和pong的细节。

当然你可以调用close方法断开与服务端的链接来触发onclose事件：

```
ws.onclose = function(e) {
console.log("Connection closed", e);
};
```

连接失败和成功的关闭握手都会触发关闭事件，WebSocket的对象的readyState属性就代表连接的状态（2代表正在关闭，3代表已经关闭）。关闭事件有三个属性可以用来做异常处理和重获： wasClean,code和reason。wasClean是一个bool值，代表连接是否干净的关闭。 如果是响应服务端的close事件，这个值为true，如果是别的原因，比如因为是底层TCP连接关闭，wasClean为false。code和reason代表关闭连接时服务端发送的状态，这两个属性和给入close方法的code和reason参数是对应的，稍后会描述细节。

### 5.4 WebSocket 方法

**WebSocket 对象有两个方法：send()和close()。**

#### 5.4.1 send()

一旦在服务端和客户端建立了全双工的双向连接，可以使用send方法去发送消息。

```
//发送一个文本消息
ws.send("Hello WebSocket!");
```

当连接是open的时候send()方法传送数据，当连接关闭或获取不到的时候回抛出异常。一个通常的错误是人们喜欢在连接open之前发送消息。如下所示：

```
// 这将不会工作
var ws = new WebSocket("ws://echo.websocket.org")
ws.send("Initial data");
```

正确的姿势如下，应该等待open事件触发后再发送消息。

```
var ws = new WebSocket("ws://echo.websocket.org")
ws.onopen = function(e) {
ws.s
```

如果想通过响应别的事件去发送消息，可以检查readyState属性的值为open的时候来实现。

```
function myEventHandler(data) {
if (ws.readyState === WebSocket.OPEN) {
//open的时候即可发送
ws.send(data);
} else {
// Do something else in this case.
//Possibly ignore the data or enqueue it.
}
}
```

发送二进制数据：

```
// Send a Blob
var blob = new Blob("blob contents");
ws.send(blob);
// Send an ArrayBuffer
var a = new Uint8Array([8,6,7,5,3,0,9]);
ws.send(a.buffer);
```

Blob对象和JavaScript File API一起使用的时候相当有用，可以发送或接受文件，大部分的多媒体文件，图像，视频和音频文件。这一章末尾会结合File API提供读取文件内容来发送WebSocket消息的实例代码。

#### **5.4.2 close()**

使用close方法来关闭连接，如果连接以及关闭，这方法将什么也不做。调用close方法只后，将不能发送数据。

```
ws.close();
```

close方法可以传入两个可选的参数，code（numerical）和reason（string）,以告诉服务端为什么终止连接。第三章讲到关闭握手的时候再详细讨论这两个参数。

```
// 成功结束会话
ws.close(1000, "Closing normally");
//1000是状态码，代表正常结束。
```

### 5.5 WebSocket 属性

WebSocket对象有三个属性，readyState，bufferedAmount和Protocol。

#### **5.5.1 readyState**

WebSocket对象通过只读属性readyState来传达连接状态，它会更加连接状态自动改变。下表展示了readyState属性的四个不同的值。

![img](https://pic1.zhimg.com/80/v2-213323a5a774d488ab5b1df395344728_720w.webp)

了解当前连接的状态有助于我们调试。

#### **5.5.2 bufferedAmount**

有时候需要检查传输数据的大小，尤其是客户端传输大量数据的时候。虽然send()方法会马上执行，但数据并不是马上传输。浏览器会缓存应用流出的数据，你可以使用bufferedAmount属性检查已经进入队列但还未被传输的数据大小。这个值不包含协议框架、操作系统缓存和网络软件的开销。

下面这个例子展示了如何使用bufferedAmount属性每秒更新发送。如果网络不能处理这个频率，它会自适应。

```
// 10k
var THRESHOLD = 10240;
//建立连接
var ws = new WebSocket("ws://echo.websocket.org");
// Listen for the opening event
ws.onopen = function () {
setInterval( function() {
//缓存未满的时候发送
if (ws.bufferedAmount < THRESHOLD) {
ws.send(getApplicationState());
}
}, 1000);
};
//使用bufferedAmount属性发送数据可以避免网络饱和。
```

#### **5.5.3 protocol**

在构造函数中，protocol参数让服务端知道客户端使用的WebSocket协议。而WebSocket对象的这个属性就是指的最终服务端确定下来的协议名称，当服务端没有选择客户端提供的协议或者在连接握手结束之前，这个属性都是空的。

完整实例：

现在我们已经过了一遍WebSocket的构造函数、事件、属性和方法，接下来通过一个完整的实例来学习WebSocket API。实例使用“Echo”服务器：ws://[http://echo.websocket.org](https://link.zhihu.com/?target=http%3A//echo.websocket.org)，它能够接受和返回发过去的数据。这样有助于理解WebSocket API是如何和服务器交互的。

首先，我们先建立连接，让页面展示客户端连接服务端的信息，然后发送、接受消息，最后关闭连接。

```
<h2>Websocket Echo Client</h2>
<div id="output"></div>
```



```
// 初始化连接和事件
        function setup() {
            output = document.getElementById("output");
            ws = new WebSocket("ws://echo.websocket.org/echo");
            // 监听open
            ws.onopen = function (e) {
                log("Connected");
                sendMessage("Hello WebSocket!");
            }
            // 监听close
            ws.onclose = function (e) {
                log("Disconnected: " + e.reason);
            }
            //监听errors
            ws.onerror = function (e) {
                log("Error ");
            }
            // 监听 messages 
            ws.onmessage = function (e) {
                log("Message received: " + e.data);
                //收到消息后关闭
                ws.close();
            }
        }
        // 发送消息
        function sendMessage(msg) {
            ws.send(msg);
            log("Message sent");
        }
        // logging
        function log(s) {
            var p = document.createElement("p");
            p.style.wordWrap = "break-word";
            p.textContent = s;
            output.appendChild(p);
            // Also log information on the javascript console
            console.log(s);
        }
        // Start 
        setup();
```

![img](https://pic1.zhimg.com/80/v2-0e30d090faef031ed71d0302037639f4_720w.webp)

判断浏览器是否支持：

```
if (window.WebSocket){
console.log("This browser supports WebSocket!");
} else {
console.log("This browser does not support WebSocket.");
}
```

![img](https://pic1.zhimg.com/80/v2-cf0c9668301238ff351e3af9d9096400_720w.webp)

## 6.WebSocket语言支持

![img](https://pic3.zhimg.com/80/v2-a8625dcb12e58c913d33b3dda011328e_720w.webp)

- 所有主流浏览器都支持RFC6455。但是具体的WebSocket版本有区别。
- php jetty netty ruby Kaazing nginx python Tomcat Django erlang
- WebSocket浏览器支持
- WebSocket浏览器支持
- netty .net等语言均可以用来实现支持WebSocket的服务器。
- websocket api在浏览器端的广泛实现似乎只是一个时间问题了, 值得注意的是服务器端没有标准的api, 各个实现都有自己的一套api, 并且tcp也没有类似的提案, 所以使用websocket开发服务器端有一定的风险.可能会被锁定在某个平台上或者将来被迫升级。

WebSocket是HTML5出的东西（协议），也就是说HTTP协议没有变化，或者说没关系，但HTTP是不支持持久连接的（长连接，循环连接的不算）。

- 首先HTTP有1.1和1.0之说，也就是所谓的keep-alive，把多个HTTP请求合并为一个，但是Websocket其实是一个新协议，跟HTTP协议基本没有关系，只是为了兼容现有浏览器的握手规范而已，也就是说它是HTTP协议上的一种补充可以通过这样一张图理解：

![img](https://pic1.zhimg.com/80/v2-b7cd97b09822b27c8faf0381ca1aaf40_720w.webp)

- 有交集，但是并不是全部。另外Html5是指的一系列新的API，或- 者说新规范，新技术。Http协议本身只有1.0和1.1，而且跟Html本身没有直接关系。
- 通俗来说，你可以用HTTP 协议 传输非Html 数据 ，就是这样=。=
- 再简单来说， 层级不一样 。

## 7.WebSocket通信

### 7.1 连接握手

连接握手分为两个步骤：请求和应答。WebSocket利用了HTTP协议来建立连接，使用的是HTTP的协议升级机制。

#### 7.1.1 请求

一个标准的HTTP请求，格式如下：

![img](https://pic4.zhimg.com/80/v2-cbf2967bda07d89726555b1e8988f307_720w.webp)

请求头的具体格式定义参见**[Request-Line格式](https://link.zhihu.com/?target=https%3A//link.csdn.net/%3Ftarget%3Dhttps%3A//www.w3.org/Protocols/rfc2616/rfc2616-sec5.html)**。

**请求header中的字段解析:**

![img](https://pic2.zhimg.com/80/v2-f1b5808b3593ad7ec8767fac2bf32fb1_720w.webp)

协议升级机制

Origin

所有浏览器将会发送一个 Origin请求头。 你可以将这个请求头用于安全方面（检查是否是同一个域，白名单/ 黑名单等），如果你不喜欢这个请求发起源，你可以发送一个403 Forbidden。需要注意的是非浏览器只能发送一个模拟的 Origin。大多数应用会拒绝不含这个请求头的请求。

Sec-WebSocket-Key

由客户端随机生成的，提供基本的防护，防止恶意或者无意的连接。

#### 7.1.2 应答

**返回字段解析：**

![img](https://pic4.zhimg.com/80/v2-8b4c6f01a74a6f13da2b3e6b5e0c84b3_720w.webp)

Connection

可以参见 rfc7230 6.1 和 rfc7230 6.7。

Sec-WebSocket-Accept

它通过客户端发送的Sec-WebSocket-Key 计算出来。计算方法：

将 Sec-WebSocket-Key 跟 258EAFA5-E914-47DA-95CA-C5AB0DC85B11 拼接；

通过 SHA1 计算出摘要，并转成 base64 字符串。

Sec-WebSocket-Key / Sec-WebSocket-Accept 的主要作用还是为了避免一些网络通信过程中，一些非期待的数据包，”乱入“进来，导致一些错误的响应，并不能用于实现登录认证和数据安全，这些功能还需要应用层自己实现。

### 7.2 数据传输（双工）

WebSocket 以 frame 为单位传输数据, frame 是客户端和服务端数据传输的最小单元。当一条消息过长时, 通信方可以将该消息拆分成多个 frame 发送, 接收方收到以后重新拼接、解码从而还原出完整的消息, 在 WebSocket 中, frame 有多种类型, frame 的类型由 frame 头部的 Opcode 字段指示, WebSocket frame 的结构如下所示:

![img](https://pic1.zhimg.com/80/v2-b1c04497d138689f6b9e4e62a29c07d0_720w.webp)

该结构的字段语义如下:

FIN, 长度为 1 比特, 该标志位用于指示当前的 frame 是消息的最后一个分段, 因为 WebSocket 支持将长消息切分为若干个 frame 发送, 切分以后, 除了最后一个 frame, 前面的 frame 的 FIN 字段都为 0, 最后一个 frame 的 FIN 字段为 1, 当然, 若消息没有分段, 那么一个 frame 便包含了完成的消息, 此时其 FIN 字段值为 1。

RSV 1 ~ 3, 这三个字段为保留字段, 只有在 WebSocket 扩展时用, 若不启用扩展, 则该三个字段应置为 1, 若接收方收到 RSV 1 ~ 3 不全为 0 的 frame, 并且双方没有协商使用 WebSocket 协议扩展, 则接收方应立即终止 WebSocket 连接。

Opcode, 长度为 4 比特, 该字段将指示 frame 的类型, RFC 6455 定义的 Opcode 共有如下几种:

- 0x0, 代表当前是一个 continuation frame，既被切分的长消息的每个分片frame
- 0x1, 代表当前是一个 text frame
- 0x2, 代表当前是一个 binary frame
- 0x3 ~ 7, 目前保留, 以后将用作更多的非控制类 frame
- 0x8, 代表当前是一个 connection close, 用于关闭 WebSocket 连接
- 0x9, 代表当前是一个 ping frame
- 0xA, 代表当前是一个 pong frame
- 0xB ~ F, 目前保留, 以后将用作更多的控制类 frame

1. Mask, 长度为 1 比特, 该字段是一个标志位, 用于指示 frame 的数据 (Payload) 是否使用掩码掩盖, RFC 6455 规定当且仅当由客户端向服务端发送的 frame, 需要使用掩码覆盖, 掩码覆盖主要为了解决代理缓存污染攻击 (更多细节见 RFC 6455 Section 10.3)。
2. Payload Len, 以字节为单位指示 frame Payload 的长度, 该字段的长度可变, 可能为 7 比特, 也可能为 7 + 16 比特, 也可能为 7 + 64 比特. 具体来说, 当 Payload 的实际长度在 [0, 125] 时, 则 Payload Len 字段的长度为 7 比特, 它的值直接代表了 Payload 的实际长度; 当 Payload 的实际长度为 126 时, 则 Payload Len 后跟随的 16 位将被解释为 16-bit 的无符号整数, 该整数的值指示 Payload 的实际长度; 当 Payload 的实际长度为 127 时, 其后的 64 比特将被解释为 64-bit 的无符号整数, 该整数的值指示 Payload 的实际长度。
3. Masking-key, 该字段为可选字段, 当 Mask 标志位为 1 时, 代表这是一个掩码覆盖的 frame, 此时 Masking-key 字段存在, 其长度为 32 位, RFC 6455 规定所有由客户端发往服务端的 frame 都必须使用掩码覆盖, 即对于所有由客户端发往服务端的 frame, 该字段都必须存在, 该字段的值是由客户端使用熵值足够大的随机数发生器生成, 关于掩码覆盖, 将下面讨论, 若 Mask 标识位 0, 则 frame 中将设置该字段 (注意是不设置该字段, 而不仅仅是不给该字段赋值)。
4. Payload, 该字段的长度是任意的, 该字段即为 frame 的数据部分, 若通信双方协商使用了 WebSocket 扩展, 则该扩展数据 (Extension data) 也将存放在此处, 扩展数据 + 应用数据, 它们的长度和便为 Payload Len 字段指示的值。

以下是一个客户端和服务端相互传递文本消息的示例

![img](https://pic3.zhimg.com/80/v2-e82dd7828a0c15f5d3217768ad289f1a_720w.webp)

**其中模拟了长消息被切分为多个帧（continuation frame）的例子。**

### 7.3 关闭请求

**关闭相对简单，由客户端或服务端发送关闭帧，即可完成关闭。**

![img](https://pic4.zhimg.com/80/v2-845db9777fd53b43e20ad9a1b68860e7_720w.webp)



## 8.WebSocket协议进一步理解

WebSocket是一种在单个TCP连接上进行全双工通信的协议。WebSocket通信协议于2011年被IETF定为标准RFC 6455，并由RFC7936补充规范。WebSocket API也被W3C定为标准。

### **8.1 WebSocket是什么**

WebSocket 协议在2008年诞生，2011年成为国际标准。主流浏览器都已经支持。

WebSocket 是一种全新的协议。它将 TCP 的 Socket（套接字）应用在了网页上，从而使通信双方建立起一个保持在活动状态连接通道，并且属于全双工通信。

WebSocket 协议在2008年诞生，2011年成为国际标准。主流浏览器都已经支持。WebSocket 协议借用 HTTP协议 的 101 switch protocol 来达到协议转换，从HTTP协议切换WebSocket通信协议。它的最大特点就是，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息，是真正的双向平等对话。

### 8.2 WebSocket出现之前的实时技术

轮询：最早的一种实现实时 Web 应用的方案。客户端以一定的时间间隔向服务端发出请求，以频繁请求的方式来保持客户端和服务器端的通信。

长轮询：长轮询也采用轮询的方式，不过采取的是阻塞模型，客户端发起连接后，如果没消息，就一直不返回Response给客户端。直到有消息才返回，返回完之后，客户端再次建立连接，周而复始。

其他方式：如xhr-streaming、隐藏iframe、ActiveX控件、SSE。

![img](https://pic2.zhimg.com/80/v2-3bfabd9b252c79a407a2d2ae48bea36d_720w.webp)

轮询技术非真正实时技术。使用 Ajax 方式模拟实时效果，每次客户端和服务器端交互，都是一次 HTTP 的请求和应答过程，且每次的 HTTP 请求和应答都带有完整 HTTP 头信息，增加传输的数据量。需构建两个http连接。客户端和服务器端编程实现比较复杂，为模拟真实的实时效果，需构造两个 HTTP 连接来模拟客户端和服务器的双向通信，一个连接用来处理客户端到服务器端的数据传输，一个连接用来处理服务器端到客户端的数据传输，增加了编程实现的复杂度、服务器端的负载，制约了应用系统的扩展性。

### 8.3 WebSocket应用场景

BS架构下的即时通讯、游戏等应用需要客户端与服务端间的双向通信，而HTTP的请求/响应模型并不适合这种场景。会存在一定的问题：

- 服务器端被迫提供两类接口，一类提供给客户端轮询新消息，一类提供给客户端推送消息给服务器端。
- HTTP协议有较多的额外开销，每次发送消息都会有一个HTTP header信息，而且如果不用Keep-Alive每次还都要握手。
- 客户端的脚本比如JS可能还需要跟踪整个过程，发送一个消息后，我可能需要跟踪这个消息的返回。

Websocket出现使得浏览器提供socket的支持成为可能，从而在浏览器和服务器之间建立一条基于tcp的双向连接通道，web开发人员可以很方便的利用websocket构建实时web应用。**WebSocket适用于以下场景:**

- 在线聊天场景：例如qq聊天、淘宝与客服聊天、在线客服等等。这种场景都是需要实时的接收服务器推送的内容。
- 协同办公：例如腾讯在线文档，腾讯的在线文档是支持多人编辑的，在excel中，一旦有人修改就要立即同步给所有人。
- 直播弹幕：例如虎牙、斗鱼等各大直播平台，在直播时都是有弹幕的，遇到一些精彩片段时，往往会有弹幕刷屏。在这种情况下使用WebSocket会有一个更好的用户体验。
- 位置共享：例如微信里位置共享，这种场景需要用户实时的共享自己的位置给服务器，服务器收到位置信息后，要实时的推送给其它共享者的，实时性要求较高；百度地图导航系统，在自己位置到达某个地方之后，语音会立即播报前面道路情况，比如上高架、下地道、拐弯、直行、学校慢行等等。这种场景实时性特别高，汽车速度很快，延迟1秒钟，可能就错过了最佳提醒时机。
- 其他通过定义WebSocket子协议的扩展支持：例如sip、mqtt、xmpp、stomp等。

### 8.4 WebSocket协议栈

![img](https://pic4.zhimg.com/80/v2-aaf587560b040bb6e76c9dd2718fddeb_720w.webp)

**WebSocket是基于TCP的应用层协议。**需要特别注意的是：虽然WebSocket协议在建立连接时会使用HTTP协议，但这并意味着WebSocket协议是基于HTTP协议实现的。

### 8.5 WebSocket与HTTP的区别

![img](https://pic3.zhimg.com/80/v2-00ee437654f960e507d6809adfce415e_720w.webp)

- 通信方式不同。WebSocket是双向通信模式，客户端与服务器之间只有在握手阶段是使用HTTP协议的“请求-响应”模式交互，而一旦连接建立之后的通信则使用双向模式交互，不论是客户端还是服务端都可以随时将数据发送给对方；而HTTP协议则至始至终都采用“请求-响应”模式进行通信。也正因为如此，HTTP协议的通信效率没有WebSocket高。
- 协议格式不同。HTTP协议的一个数据包就是一条完整的消息；而WebSocket客户端与服务端通信的最小单位是帧，由1个或多个帧组成一条完整的消息。即：发送端将消息切割成多个帧，并发送给服务端；服务端接收消息帧，并将关联的帧重新组装成完整的消息。

### 8.6 WebSocket握手过程

**客户端到服务端:**

![img](https://pic4.zhimg.com/80/v2-bf7136dc1931af0645a6a70b6de9856b_720w.webp)

- GET ws://localhost…… HTTP/1.1 ：打开阶段握手，使用http1.1协议。
- Upgrade：websocket，表示请求为特殊http请求，请求的目的是要将客户端和服务端的通信协议从http升级为websocket。
- Sec-websocket-key：Base64 encode 的值，是浏览器随机生成的。客户端向服务端提供的握手信息。

**服务端到客户端:**

![img](https://pic2.zhimg.com/80/v2-326704f41a81df809c07fade43fc3e21_720w.webp)

- 101状态码：表示切换协议。服务器根据客户端的请求切换到Websocket协议。
- Sec-websocket-accept: 将请求头中的Set-websocket-key添加字符串并做SHA-1加密后做Base64编码，告知客户端服务器能够发起websocket连接。

**客户端发起连接的约定:**

- 如果请求为wss,则在TCP建立后，进行TLS连接建立。
- 请求的方式必须为GET，HTTP版本至少为HTTP1.1。
- 请求头中必须有Host。
- 请求头中必须有Upgrade，取值必须为websocket。
- 请求头中必须有Connection，取值必须为Upgrade。
- 请求头中必须有Sec-WebSocket-Key，取值为16字节随机数的Base64编码。
- 请求头中必须有Sec-WebSocket-Version，取值为13。
- 请求头中可选Sec-WebSocket-Protocol，取值为客户端期望的一个或多个子协议(多个以逗号分割)。
- 请求头中可选Sec-WebSocket-Extensitons，取值为子协议支持的扩展集(一般是压缩方式)。
- 可以包含cookie、Authorization等HTTP规范内合法的请求头。

**客户端检查服务端的响应:**

- 服务端返回状态码为101代表升级成功，否则判定连接失败。
- 响应头中缺少Upgrade或取值不是websocket，判定连接失败。
- 响应头中缺少Connection或取值不是Upgrade，判定连接失败。
- 响应头中缺少Sec-WebSocket-Accept或取值非法（其值为请求头中的Set-websocket-key添加字符串并做SHA-1加密后做Base64编码），判定连接失败。
- 响应头中有Sec-WebSocket-Extensions,但取值不是请求头中的子集，判定连接失败。
- 响应头中有Sec-WebSocket-Protocol,但取值不是请求头中的子集，判定连接失败。

**服务端处理客户端连接:**

- 服务端根据请求中的Sec-WebSocket-Protocol 字段，选择一个子协议返回，如果不返回，表示不同意请求的任何子协议。如果请求中未携带，也不返回。
- 如果建立连接成功，返回状态码为101。
- 响应头Connection设置为Upgrade。
- 响应头Upgrade设置为websocket。
- Sec-WebSocket-Accpet根据请求头Set-websocket-key计算得到，计算方式为：Set-websocket-key的值添加字符串： 258EAFA5-E914-47DA-95CA-C5AB0DC85B11并做SHA-1加密后得到16进制表示的字符串，将每两位当作一个字节进行分隔，得到字节数组，对字节数组做Base64编码。

### 8.7 WebSocket帧格式

**WebSocket通信流程如下:**

![img](https://pic4.zhimg.com/80/v2-7f140f2953205aae25737fdb5ad88f53_720w.webp)

**Websocket帧格式如下:**

![img](https://pic4.zhimg.com/80/v2-4faf01bacfaa0cf69e19f07e4507df4f_720w.webp)

**第一部分:**

![img](https://pic1.zhimg.com/80/v2-9f9dc9fedf00d66c215b027282fe0758_720w.webp)

FIN：1位，用于描述消息是否结束，如果为1则该消息为消息尾部，如果为零则还有后续数据包。

RSV1,RSV2,RSV3：各1位，用于扩展定义，如果没有扩展约定的情况则必须为0。

OPCODE：4位，用于表示消息接收类型，如果接收到未知的opcode，接收端必须关闭连接。

OPCODE说明:

- 0x0表示附加数据帧，当前数据帧为分片的数据帧。
- 0x1表示文本数据帧，采用UTF-8编码。
- 0x2表示二进制数据帧。
- 0x3-7暂时无定义，为以后的非控制帧保留。
- 0x8表示连接关闭。
- 0x9表示ping。
- 0xA表示pong。
- 0xB-F暂时无定义，为以后的控制帧保留。

**第二部分:**

![img](https://pic1.zhimg.com/80/v2-14d9365cdb2da722fec007d231022b30_720w.webp)

- MASK：1位，用于标识PayloadData是否经过掩码处理。服务端发送给客户端的数据帧不能使用掩码，客户端发送给服务端的数据帧必须使用掩码。如果一个帧的数据使用了掩码，那么在Maksing-key部分必须是一个32个bit位的掩码，用来给服务端解码数据。
- Payload len：数据的长度：默认位7个bit位。如果数据的长度小于125个字节（注意：是字节）则用默认的7个bit来标示数据的长度。如果数据的长度为126个字节，则用后面相邻的2个字节来保存一个16bit位的无符号整数作为数据的长度。如果数据的长度大于126个字节，则用后面相邻的8个字节来保存一个64bit位的无符号整数作为数据的长度。
- payload len本来是只能用7bit来表达的，也就是最多一个frame的payload只能有127个字节，为了表示更大的长度，给出的解决方案是添加扩展payload len字段。当payload实际长度超过126（包括），但在2^16-1长度内，则将payload len置为126，payload的实际长度由长为16bit的extended payload length来表达。当payload实际长度超过216（包括），但在264-1长度内，则将payload置为127，payload的实际长度由长为64bit的extended payload length来表达。

**第三部分:**

![img](https://pic2.zhimg.com/80/v2-fa5ad43529f61a048f8a782f4fefca6d_720w.webp)

数据掩码：如果MASK设置位0，则该部分可以省略，如果MASK设置位1，则Masking-key是一个32位的掩码。用来解码客户端发送给服务端的数据帧。

**第四部分:**

![img](https://pic2.zhimg.com/80/v2-8c6a2c34c41df996508fa31c7c0c2089_720w.webp)

数据：该部分，也是最后一部分，是帧真正要发送的数据，可以是任意长度。

### 8.8 WebSocket分片传输

![img](https://pic4.zhimg.com/80/v2-6ba4dd10e8d88f7edeb3d5c74a02a907_720w.webp)

控制帧可能插在一个Message的多个分片之间，但一个Message的分片不能交替传输(除非有扩展特别定义)。

控制帧不可分片。

分片需要按照分送方提交顺序传递给接收方，但由于IP路由特性，实际并不能保证顺序到达。

控制帧包括:

- Close：用于关闭连接，可以携带数据，表示关闭原因。
- Ping：可以携带数据。
- Pong：用于Keep-alive，返回最近一次Ping中的数据，可以只发送Pong帧，做单向心跳。

连接关闭时状态码说明:

![img](https://pic1.zhimg.com/80/v2-29bd022bc176f35566ade86b15e3d858_720w.webp)

### 8.9 WebSocket相关扩展

Stomp：

STOMP是基于帧的协议，它的前身是TTMP协议（一个简单的基于文本的协议），专为消息中间件设计。是属于消息队列的一种协议, 和AMQP, JMS平级。它的简单性恰巧可以用于定义websocket的消息体格式. STOMP协议很多MQ都已支持, 比如RabbitMq, ActiveMq。生产者（发送消息）、消息代理、消费者（订阅然后收到消息）。

![img](https://pic1.zhimg.com/80/v2-3f439d6a7256805b37c6ba5b8fb3a7cc_720w.webp)

\2. SockJs：

SockJS是一个浏览器JavaScript库，它提供了一个类似于网络的对象。SockJS提供了一个连贯的、跨浏览器的Javascript API，它在浏览器和web服务器之间创建了一个低延迟、全双工、跨域通信通道。

SockJS的一大好处在于提供了浏览器兼容性。优先使用原生WebSocket，如果在不支持websocket的浏览器中，会自动降为轮询的方式。 除此之外，spring也对socketJS提供了支持。

\3. [Socket.io](https://link.zhihu.com/?target=http%3A//Socket.io)：

[http://Socket.io](https://link.zhihu.com/?target=http%3A//Socket.io)实际上是WebSocket的父集，[http://Socket.io](https://link.zhihu.com/?target=http%3A//Socket.io)封装了WebSocket和轮询等方法，会根据情况选择方法来进行通讯。

[http://Sockei.io](https://link.zhihu.com/?target=http%3A//Sockei.io)最早由Node.js实现，Node.js提供了高效的服务端运行环境，但由于Browser对HTML5的支持不一，为了兼容所有浏览器，提供实时的用户体验，并为开发者提供客户端与服务端一致的编程体验，于是[http://Socket.io](https://link.zhihu.com/?target=http%3A//Socket.io)诞生了。Java模仿Node.js实现了Java版的[http://Netty-socket.io](https://link.zhihu.com/?target=http%3A//Netty-socket.io)库。

[http://Socket.io](https://link.zhihu.com/?target=http%3A//Socket.io)将WebSocket和Polling机制以及其它的实时通信方式封装成通用的接口，并在服务端实现了这些实时机制相应代码，包括：AJAX Long Polling、Adobe Flash Socket、AJAX multipart streaming、Forever Iframem、JSONP Polling。

## 9.WebSocket 能解决什么问题

工程师应该是以解决问题为主的，如果不会解决问题，只会伸手，必然不会长远，有思考，才会有突破，才能高效的处理事情，所以 websocket 到底解决了什么问题呢？它存在的价值是什么？

这还是得从HTTP说起，大家应该都很熟悉这门协议，我们简单说一下它的特点：

•三次握手、四次挥手 的方式建立连接和关闭连接

•支持长连接和短连接两种连接方式

•有同源策略的限制（端口，协议，域名）

•单次 请求-响应 机制，只支持单向通信

其中最鸡肋的就是最后一个特点，单向通信，什么意思呐？ 就是说只能由一方发起请求（客户端），另一方响应请求（服务端），而且每一次的请求都是一个单独的事件，请求之间还无法具有关联性，也就是说我上个请求和下个请求完全是隔离的，无法具有连续性。

也许你觉得这样的说法比较难懂，我们来举一个栗子：

每个人都打过电话吧，电话打通后可以一直聊天是不是觉得很舒服啊，这是一种全双工的通信方式，双方都可以主动传递信息。彼此的聊天也具有连续性。我们简单把这种方式理解为 websocket 协议支持的方式。

如果打电话变成了 HTTP 那种方式呢？ 那就不叫打电话了，而是联通爸爸的智能语音助手了，我们知道客户端和服务端本身的身份并不是固定的，只要你可以发起通信，就可以充当客户端，能响应请求，就可以当做服务端，但是在HTTP的世界里一般来说，客户端（大多数情况下是浏览器）和服务器一般是固定的，我们打电话 去查话费，会询问要人工服务还是智能助手，如果选了助手，你只要问她问题，她就会找对应的答案来回答你（响应你），一般都是简单的业务，你不问她也不会跟你闲聊，主动才有故事啊！

但是实际上有很多的业务是需要双方都有主动性的，半双工的模式肯定是不够用的，例如聊天室，跟机器人聊天没意思啊，又例如主动推送，我无聊的时候手都不想点屏幕，你能不能主动一点给我推一些好玩的信息过来。

只要做过前后端分离的同学应该都被跨域的问题折磨过。浏览器的这种同源策略，会导致 不同端口/不同域名/不同协议 的请求会有限制，当然这问题前后端都能处理，然而 websocket 就没有这种要求，他支持任何域名或者端口的访问（协议固定了只能是 ws/wss) ,所以它让人用的更加舒服

所以，上面 HTTP 存在的这些问题，websocket 都能解决！！！

## 10.WebSocket工作原理

主动是 websocket 的一大特点，像之前如果客户端想知道服务端对某个事件的处理进度，就只能通过轮训( Poll )的方式去询问，十分的耗费资源，会存在十分多的无效请求。下面我简单说推送技术的三种模型区别：

- •pull (主动获取) 即客户端主动发起请求，获取消息
- •poll (周期性主动获取) 即周期性的主动发起请求，获取消息
- •push (主动推送) 服务端主动推送消息给客户端

pull 和 poll 的唯一区别只在于周期性，但是很明显周期性的去询问，对业务来说清晰度很高，这也是为什么很多小公司都是基于轮训的方式去处理业务，因为简单嘛，能力不够机器来撑。这也是很多公司都会面临的问题，如果业务达到了瓶颈，使劲的堆机器，如果用新技术或者更高级的作法，开发成本和维护成本也会变高，还不如简单一点去增加机器配置。

如果两个人需要通话，首先需要建立一个连接，而且必须是一个长链接，大家都不希望讲几句话就得重新打吧，根据上面说的，websocket 会复用之前 HTTP 建立好的长链接，然后再进行升级，所以他和轮训的区别大致如下所示：

![img](https://pic1.zhimg.com/80/v2-306c1e8fb9159bed67e11d249bd9cf74_720w.webp)

图片省去了建立连接的过程，我们可以发现，基于轮训的方式，必须由客户端手动去请求，才会有响应，而基于 websocket 协议，不再需要你主动约妹子了，妹子也可以主动去约你，这才是公平的世界。

为了更好的阐述这个连接的原理，可以使用swoole 自带的 创建websocket 的功能进行测试，服务端代码如下，如果连接不上，可以看看是不是检查一下端口开放情况（iptables/filewall)和网络的连通性，代码如下：

```
//创建websocket服务器对象，监听0.0.0.0:9501端口
$ws = new Swoole\WebSocket\Server("0.0.0.0", 9501);

//监听WebSocket连接打开事件
$ws->on('open', function ($ws, $request) {
    var_dump($request->fd, $request->get, $request->server); //request 对象包含请求的相关信息
    //$ws->push($request->fd, "hello, welcome\n");
});

//监听WebSocket消息事件
$ws->on('message', function ($ws, $frame) {  // frame 是存储信息的变量，也就是传输帧
    echo "Message: {$frame->data}\n";
    $ws->push($frame->fd, "server: {$frame->data}");
});

//监听WebSocket连接关闭事件
$ws->on('close', function ($ws, $fd) { // fd 是客户端的标志
    echo "client-{$fd} is closed\n";
});

$ws->start(); // 启动这个进程
```

我们可以发现，相比于 HTTP 的头部，websocket 的数据结构十分的简单小巧，没有像 HTTP 协议一样老是带着笨重的头部，这一设计让websocket的报文可以在体量上更具优势，所以传输效率来说更高 。

当然，我们传输的文本也不能在大街上裸跑啊，既然 HTTP 都有衣服穿了（HTTPS），websocket（ws) 当然也有 （wss）。

在以前的文章我们也简单聊过 HTTPS 是个什么东西，大家不了解可以去翻一下之前的文章，总的来说就是使用了非对称加密算法进行了对称加密密钥的传输，后续采用对称加密解密的方式进行数据安全处理。

如果你的业务需要支撑双全工的通信，那么 websocket 便是一个很不错的选择。网上大多数关于 websocket 的文章，大多是基于前端学习者的角度，他们使用 Chrome 的console 的调试实验，本篇文章更多是基于后端开发者的角度。希望对你有所帮助。

## 11.进一步解析什么是WebSocket协议

### 11.1websocket特点

1.websocket优点

- 保持连接状态：websocket需要先创建连接，使其成为有状态的协议。
- 更好支持二进制：定义了二进制帧，增加安全性。
- 支持扩展：定义了扩展，可以自己实现部分部分自定义。
- 压缩效果好：可以沿用上下文的内容，有更好的压缩效果。

2.websocket缺点

- 开发要求高： 前端后端都增加了一定的难度。
- 推送消息相对复杂。
- HTTP协议已经很成熟，现今websocket则太新了一点。

### 11.2websocket协议通信过程

**协议有两个部分：handshake（握手）和 data transfer（数据传输）。**

1.handshake

#### 11.2.1 客户端

客户端握手报文是在HTTP的基础上发送一次HTTP协议升级请求。

```
GET /chat HTTP/1.1
Host: server.example.com
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==
Origin: http://example.com
Sec-WebSocket-Protocol: chat, superchat
Sec-WebSocket-Version: 13
```

Sec-WebSocket-Key 是由浏览器随机生成的，提供基本的防护，防止恶意或者无意的连接。

Sec-WebSocket-Version 表示 WebSocket 的版本，最初 WebSocket 协议太多，不同厂商都有自己的协议版本，不过现在已经定下来了。如果服务端不支持该版本，需要返回一个 Sec-WebSocket-Versionheader，里面包含服务端支持的版本号。

#### 11.2.2 服务端

服务端响应握手也是在HTTP协议基础上回应一个Switching Protocols。

```
HTTP/1.1 101 Switching Protocols
Upgrade: websocket
Connection: Upgrade
Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=
Sec-WebSocket-Protocol: chat
```

Linux下对应实现代码，注释在代码中。

```
int websocket_handshake(struct qsevent *ev)
{
    char linebuf[128];
    int index = 0;
    char sec_data[128] = {0};
    char sec_accept[32] = {0};
    do
    {
        memset(linebuf, 0, sizeof(linebuf));//清空以暂存一行报文
        index = readline(ev->buffer, index, linebuf);//获取一行报文
        if(strstr(linebuf, "Sec-WebSocket-Key"))//如果一行报文里面包括了Sec-WebSocket-Key
        {
            strcat(linebuf, GUID);//和GUID连接起来
            SHA1(linebuf+WEBSOCK_KEY_LENGTH, strlen(linebuf+WEBSOCK_KEY_LENGTH), sec_data);//SHA1
            base64_encode(sec_data, strlen(sec_data), sec_accept);//base64编码
            memset(ev->buffer, 0, MAX_BUFLEN);//清空服务端数据缓冲区

            ev->length = sprintf(ev->buffer,//组装握手响应报文到数据缓冲区，下一步有进行下发
                                 "HTTP/1.1 101 Switching Protocols\r\n"
                                 "Upgrade: websocket\r\n"
                                 "Connection: Upgrade\r\n"
                                 "Sec-websocket-Accept: %s\r\n\r\n", sec_accept);
            break;   
        }
    }while(index != -1 && (ev->buffer[index] != '\r') || (ev->buffer[index] != '\n'));//遇到空行之前
    return 0;
}
```

#### 11.2.3 data transfer

先看数据包格式。

![img](https://pic1.zhimg.com/80/v2-a5012dc34aa781ae2222dac5e080b34c_720w.webp)

FIN：指示这是消息中的最后一个片段。第一个片段也可能是最后的片段。

RSV1, RSV2, RSV3：一般情况下全为 0。当客户端、服务端协商采用 WebSocket 扩展时，这三个标志位可以非0，且值的含义由扩展进行定义。如果出现非零的值，且并没有采用 WebSocket 扩展，连接出错。

opcode：操作代码。

```
%x0：表示一个延续帧。当 Opcode 为 0 时，表示本次数据传输采用了数据分片，当前收到的数据帧为其中一个数据分片；
%x1：表示这是一个文本帧（frame）；
%x2：表示这是一个二进制帧（frame）；
%x3-7：保留的操作代码，用于后续定义的非控制帧；
%x8：表示连接断开；
%x9：表示这是一个 ping 操作；
%xA：表示这是一个 pong 操作；
%xB-F：保留的操作代码，用于后续定义的控制帧。
```

- mask：是否需要掩码。
- Payload length: 7bit or 7 + 16bit or 7 + 64bit

```
 表示数据载荷的长度
x 为 0~126：数据的长度为 x 字节；
x 为 126：后续 2 个字节代表一个 16 位的无符号整数，该无符号整数的值为数据的长度；
x 为 127：后续 8 个字节代表一个 64 位的无符号整数（最高位为 0），该无符号整数的值为数据的长度。
```

payload data：消息体。 下面是服务端的代码实现：

```
#define GUID            "258EAFA5-E914-47DA-95CA-C5AB0DC85B11"
enum			
{ 
    WS_HANDSHAKE = 0,		//握手
    WS_TANSMISSION = 1,		//通信
    WS_END = 2,				//end
};

typedef struct _ws_ophdr{
    unsigned char opcode:4,
    rsv3:1,
    rsv2:1,
    rsv1:1,
    fin:1;
    unsigned char pl_len:7,
    mask:1;
}ws_ophdr;//协议前两个字节

typedef struct _ws_head_126{
    unsigned short payload_lenght;
    char mask_key[4];
}ws_head_126;//协议mask和消息体长度


/*解码*/
void websocket_umask(char *payload, int length, char *mask_key)
{
    int i = 0;
    for( ; i<length; i++)
    payload[i] ^= mask_key[i%4];//异或
}

int websocket_transmission(struct qsevent *ev)
{
    ws_ophdr *ophdr = (ws_ophdr*)ev->buffer;//协议前两个自己
    printf("ws_recv_data length=%d\n", ophdr->pl_len);
    if(ophdr->pl_len <126)//如果消息体长度小于126
    {
        char * payload = ev->buffer + sizeof(ws_ophdr) + 4;//获取消息地址
        if(ophdr->mask)//如果消息是掩码
        {
            websocket_umask(payload, ophdr->pl_len, ev->buffer+2);//解码，异或
            printf("payload:%s\n", payload);
        }
        printf("payload : %s\n", payload);//消息回显
    }
    else if (hdr->pl_len == 126) {
		ws_head_126 *hdr126 = ev->buffer + sizeof(ws_ophdr);
	} else {
		ws_head_127 *hdr127 = ev->buffer + sizeof(ws_ophdr);
	}
    return 0;
}

int websocket_request(struct qsevent *ev)
{
    if(ev->status_machine == WS_HANDSHAKE)
    {
        websocket_handshake(ev);//握手
        ev->status_machine = WS_TANSMISSION;//设置标志位
    }else if(ev->status_machine == WS_TANSMISSION){
        websocket_transmission(ev);//通信
    }
    return 0;
}
```

**代码是基于reactor百万并发服务器框架实现的。**

### 11.3.epoll反应堆模型下实现http协议

![img](https://pic2.zhimg.com/80/v2-a39426bc462aa425c544f091ef5ad3f5_720w.webp)

1.客户端结构体

```
struct qsevent{
    int fd;				//clientfd
    int events;			//事件：读、写或异常
    int status;			//是否位于epfd红黑监听树上
    void *arg;			//参数
    long last_active;	//上次数据收发的事件

    int (*callback)(int fd, int event, void *arg);	//回调函数，单回调，后面修改成多回调
    unsigned char buffer[MAX_BUFLEN];				//数据缓冲区
    int length;										//数据长度

    /*http param*/
    int method;						//http协议请求头部
    char resource[MAX_BUFLEN];		//请求的资源
    int ret_code;					//响应状态码
};
```

[2.int](https://link.zhihu.com/?target=http%3A//2.int) http_response(struct qsevent *ev)

当客户端发送tcp连接时，服务端的listenfd会触发输入事件会调用ev->callback即accept_cb回调函数响应连接并获得clientfd，连接之后，http数据报文发送上来，服务端的clientfd触发输入事件会调用ev->callback即recv_cb回调函数进行数据接收，并解析http报文。

```
int http_request(struct qsevent *ev)
{
    char linebuf[1024] = {0};//用于从buffer中获取每一行的请求报文
    int idx = readline(ev->buffer, 0, linebuf);//读取第一行请求方法，readline函数，后面介绍
    if(strstr(linebuf, "GET"))//strstr判断是否存在GET请求方法
    {
        ev->method = HTTP_METHOD_GET;//GET方法表示客户端需要获取资源

        int i = 0;
        while(linebuf[sizeof("GET ") + i] != ' ')i++;//跳过空格
        linebuf[sizeof("GET ") + i] = '\0';
        sprintf(ev->resource, "./%s/%s", HTTP_METHOD_ROOT, linebuf+sizeof("GET "));//将资源的名字以文件路径形式存储在ev->resource中
        printf("resource:%s\n", ev->resource);//回显
    }
    else if(strstr(linebuf, "POST"))//POST的请求方法，暂时没写，方法差不多
    {}
    return 0;
}
```

[http://3.int](https://link.zhihu.com/?target=http%3A//3.int) http_response(struct qsevent *ev)

服务器对客户端的响应报文数据进行http封装储存在buffer中，事件触发时在send_cb回调函数发送给客户端。详细解释请看代码注释。

```
int http_response(struct qsevent *ev)
{
    if(ev == NULL)return -1;
    memset(ev->buffer, 0, MAX_BUFLEN);//清空缓冲区准备储存报文

    printf("resource:%s\n", ev->resource);//resource：客户端请求的资源文件，通过http_reques函数获取
    int filefd = open(ev->resource, O_RDONLY);//只读方式打开获得文件句柄
    if(filefd == -1)//获取失败则发送404 NOT FOUND
    {
        ev->ret_code = 404;//404状态码
        ev->length = sprintf(ev->buffer,//将下面数据传入ev->buffer
        					 /***状态行***/
        					 /*版本号 状态码 状态码描述 */
                             "HTTP/1.1 404 NOT FOUND\r\n"
                             /***消息报头***/
                             /*获取当前时间*/
                             "date: Thu, 11 Nov 2021 12:28:52 GMT\r\n"
                             /*响应正文类型；              编码方式*/
                             "Content-Type: text/html;charset=ISO-8859-1\r\n"
                             /*响应正文长度          空行*/
                             "Content-Length: 85\r\n\r\n"
                             /***响应正文***/
                             "<html><head><title>404 Not Found</title></head><body><H1>404</H1></body></html>\r\n\r\n");
    }
    else 
    {
        struct stat stat_buf;			//文件信息
        fstat(filefd, &stat_buf);		//fstat通过文件句柄获取文件信息
        if(S_ISDIR(stat_buf.st_mode))	//如果文件是一个目录
        {

            printf(ev->buffer, //同上，将404放入buffer中
                   "HTTP/1.1 404 Not Found\r\n"
                   "Date: Thu, 11 Nov 2021 12:28:52 GMT\r\n"
                   "Content-Type: text/html;charset=ISO-8859-1\r\n"
                   "Content-Length: 85\r\n\r\n"
                   "<html><head><title>404 Not Found</title></head><body><H1>404</H1></body></html>\r\n\r\n" );

        } 
        else if (S_ISREG(stat_buf.st_mode)) //如果文件是存在
        {

            ev->ret_code = 200;		//200状态码

            ev->length = sprintf(ev->buffer, //length记录长度，buffer储存响应报文
                                 "HTTP/1.1 200 OK\r\n"
                                 "Date: Thu, 11 Nov 2021 12:28:52 GMT\r\n"
                                 "Content-Type: text/html;charset=ISO-8859-1\r\n"
                                 "Content-Length: %ld\r\n\r\n", 
                                 stat_buf.st_size );//文件长度储存在stat_buf.st_size中

        }
        return ev->length;//返回报文长度
    }
}
```

4.总代码

```
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <arpa/inet.h>

#include <fcntl.h>
#include <unistd.h>
#include <errno.h>
#include <time.h>

#include <sys/stat.h>
#include <sys/sendfile.h>

#define HTTP_METHOD_ROOT    "html"

#define MAX_BUFLEN          4096
#define MAX_EPOLLSIZE       1024
#define MAX_EPOLL_EVENTS    1024

#define HTTP_METHOD_GET     0
#define HTTP_METHOD_POST    1

typedef int (*NCALLBACK)(int, int, void*);

struct qsevent{
    int fd;
    int events;
    int status;
    void *arg;
    long last_active;

    int (*callback)(int fd, int event, void *arg);
    unsigned char buffer[MAX_BUFLEN];
    int length;

    /*http param*/
    int method;
    char resource[MAX_BUFLEN];
    int ret_code;
};

struct qseventblock{
    struct qsevent *eventsarrry;
    struct qseventblock *next;
};

struct qsreactor{
    int epfd;
    int blkcnt;
    struct qseventblock *evblk;
};

int recv_cb(int fd, int events, void *arg);
int send_cb(int fd, int events, void *arg);
struct qsevent *qsreactor_idx(struct qsreactor *reactor, int sockfd);

int readline(char *allbuf, int idx, char *linebuf)
{
    int len = strlen(allbuf);    
    for( ; idx<len; idx++ )
    {
        if(allbuf[idx] == '\r' && allbuf[idx+1] == '\n')
        return idx+2;
        else
        *(linebuf++) = allbuf[idx];
    }
    return -1;
}

void qs_event_set(struct qsevent *ev, int fd, NCALLBACK callback, void *arg)
{
    ev->events = 0;
    ev->fd = fd;
    ev->arg = arg;
    ev->callback = callback;
    ev->last_active = time(NULL);
    return;
}

int qs_event_add(int epfd, int events, struct qsevent *ev)
{
    struct epoll_event epv = {0, {0}};;
    epv.events = ev->events = events;
    epv.data.ptr = ev;

    if(ev->status == 1)
    { 
        if(epoll_ctl(epfd, EPOLL_CTL_MOD, ev->fd, &epv) < 0)
        {
            perror("EPOLL_CTL_MOD error\n");
            return -1;
        }
    }
    else if(ev->status == 0)
    {
        if(epoll_ctl(epfd, EPOLL_CTL_ADD, ev->fd, &epv) < 0)
        {
            perror("EPOLL_CTL_ADD error\n");    
            return -2;
        }
        ev->status = 1;
    }
    return 0;
}

int qs_event_del(int epfd, struct qsevent *ev)
{
    struct epoll_event epv = {0, {0}};
    if(ev->status != 1)
    return -1;
    ev->status = 0;
    epv.data.ptr = ev;
    if((epoll_ctl(epfd, EPOLL_CTL_DEL, ev->fd, &epv)))
    {
        perror("EPOLL_CTL_DEL error\n");
        return -1;
    }
    return 0;
}

int sock(short port)
{
    int fd = socket(AF_INET, SOCK_STREAM, 0);
    fcntl(fd, F_SETFL, O_NONBLOCK);

    struct sockaddr_in ser_addr;
    memset(&ser_addr, 0, sizeof(ser_addr));
    ser_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    ser_addr.sin_family = AF_INET;
    ser_addr.sin_port = htons(port);

    bind(fd, (struct sockaddr*)&ser_addr, sizeof(ser_addr));

    if(listen(fd, 20) < 0)
    perror("listen error\n");

    printf("listener[%d] lstening..\n", fd);
    return fd;
}

int http_request(struct qsevent *ev)
{
    char linebuf[1024] = {0};
    int idx = readline(ev->buffer, 0, linebuf);
    if(strstr(linebuf, "GET"))
    {
        ev->method = HTTP_METHOD_GET;

        int i = 0;
        while(linebuf[sizeof("GET ") + i] != ' ')i++;
        linebuf[sizeof("GET ") + i] = '\0';
        sprintf(ev->resource, "./%s/%s", HTTP_METHOD_ROOT, linebuf+sizeof("GET "));
        printf("resource:%s\n", ev->resource);
    }
    else if(strstr(linebuf, "POST"))
    {}
    return 0;
}

int http_response(struct qsevent *ev)
{
    if(ev == NULL)return -1;
    memset(ev->buffer, 0, MAX_BUFLEN);

    printf("resource:%s\n", ev->resource);

    int filefd = open(ev->resource, O_RDONLY);
    if(filefd == -1)
    {
        ev->ret_code = 404;
        ev->length = sprintf(ev->buffer,
                             "HTTP/1.1 404 NOT FOUND\r\n"
                             "date: Thu, 11 Nov 2021 12:28:52 GMT\r\n"
                             "Content-Type: text/html;charset=ISO-8859-1\r\n"
                             "Content-Length: 85\r\n\r\n"
                             "<html><head><title>404 Not Found</title></head><body><H1>404</H1></body></html>\r\n\r\n");
    }
    else 
    {
        struct stat stat_buf;
        fstat(filefd, &stat_buf);
        if(S_ISDIR(stat_buf.st_mode))
        {

            printf(ev->buffer, 
                   "HTTP/1.1 404 Not Found\r\n"
                   "Date: Thu, 11 Nov 2021 12:28:52 GMT\r\n"
                   "Content-Type: text/html;charset=ISO-8859-1\r\n"
                   "Content-Length: 85\r\n\r\n"
                   "<html><head><title>404 Not Found</title></head><body><H1>404</H1></body></html>\r\n\r\n" );

        } 
        else if (S_ISREG(stat_buf.st_mode)) 
        {

            ev->ret_code = 200;

            ev->length = sprintf(ev->buffer, 
                                 "HTTP/1.1 200 OK\r\n"
                                 "Date: Thu, 11 Nov 2021 12:28:52 GMT\r\n"
                                 "Content-Type: text/html;charset=ISO-8859-1\r\n"
                                 "Content-Length: %ld\r\n\r\n", 
                                 stat_buf.st_size );

        }
        return ev->length;
    }
}

int qsreactor_init(struct qsreactor *reactor)
{
    if(reactor == NULL)
    return -1;
    memset(reactor, 0, sizeof(struct qsreactor));
    reactor->epfd = epoll_create(1);
    if(reactor->epfd <= 0)
    {
        perror("epoll_create error\n");
        return -1;
    }
    struct qseventblock *block = (struct qseventblock*)malloc(sizeof(struct qseventblock));
    if(block == NULL)
    {
        printf("blockinit malloc error\n");
        close(reactor->epfd);
        return -2;
    }
    memset(block, 0, sizeof(block));

    struct qsevent *evs = (struct qsevent*)malloc(MAX_EPOLLSIZE * sizeof(struct qsevent));
    if(evs == NULL)
    {
        printf("evsnit malloc error\n");
        close(reactor->epfd);
        return -3;
    }
    memset(evs, 0, sizeof(evs));

    block->next = NULL;
    block->eventsarrry = evs;

    reactor->blkcnt = 1;
    reactor->evblk = block;
    return 0;
}

int qsreactor_alloc(struct qsreactor *reactor)
{
    if(reactor == NULL)return -1;
    if(reactor->evblk == NULL)return -1;
    struct qseventblock *tailblock = reactor->evblk;
    while(tailblock->next != NULL)
    tailblock = tailblock->next;
    struct qseventblock *newblock = (struct qseventblock*)malloc(sizeof(struct qseventblock));
    if(newblock == NULL)
    {
        printf("newblock alloc error\n");
        return -1;
    }
    memset(newblock, 0, sizeof(newblock));

    struct qsevent *neweventarray = (struct qsevent*)malloc(sizeof(struct qsevent) * MAX_EPOLLSIZE);
    if(neweventarray == NULL)
    {
        printf("neweventarray malloc error\n");
        return -1;
    }
    memset(neweventarray, 0, sizeof(neweventarray));

    newblock->eventsarrry = neweventarray;
    newblock->next = NULL;

    tailblock->next = newblock;
    reactor->blkcnt++;

    return 0;
}

struct qsevent *qsreactor_idx(struct qsreactor *reactor, int sockfd)
{
    int index = sockfd / MAX_EPOLLSIZE;
    while(index >= reactor->blkcnt)qsreactor_alloc(reactor);
    int i=0;
    struct qseventblock *idxblock = reactor->evblk;
    while(i++<index && idxblock != NULL)
    idxblock = idxblock->next;

    return &idxblock->eventsarrry[sockfd%MAX_EPOLLSIZE];
}

int qsreactor_destory(struct qsreactor *reactor)
{
    close(reactor->epfd);
    free(reactor->evblk);
    reactor = NULL;
    return 0;
}

int qsreactor_addlistener(struct qsreactor *reactor, int sockfd, NCALLBACK acceptor)
{
    if(reactor == NULL)return -1;
    if(reactor->evblk == NULL)return -1;

    struct qsevent *event = qsreactor_idx(reactor, sockfd);
    qs_event_set(event, sockfd, acceptor, reactor);
    qs_event_add(reactor->epfd, EPOLLIN, event);

    return 0;
}

int send_cb(int fd, int events, void *arg)
{
    struct qsreactor *reactor = (struct qsreactor*)arg;
    struct qsevent   *ev = qsreactor_idx(reactor, fd);

    http_response(ev);
    int ret = send(fd, ev->buffer, ev->length, 0);
    if(ret < 0)
    {
        qs_event_del(reactor->epfd, ev);
        printf("clent[%d] ", fd);
        perror("send error\n");
        close(fd);
    }
    else if(ret > 0)
    {
        if(ev->ret_code == 200) 
        {
            int filefd = open(ev->resource, O_RDONLY);
            struct stat stat_buf;
            fstat(filefd, &stat_buf);

            sendfile(fd, filefd, NULL, stat_buf.st_size);
            close(filefd);   
        }
        printf("send to client[%d]:%s", fd, ev->buffer);
        qs_event_del(reactor->epfd, ev);
        qs_event_set(ev, fd, recv_cb, reactor);
        qs_event_add(reactor->epfd, EPOLLIN, ev);
    }
    return ret;
}

int recv_cb(int fd, int events, void *arg)
{
    struct qsreactor *reactor = (struct qsreactor*)arg;
    struct qsevent *ev = qsreactor_idx(reactor, fd);

    int len = recv(fd, ev->buffer, MAX_BUFLEN, 0);
    qs_event_del(reactor->epfd, ev);
    if(len > 0)
    {
        ev->length = len;
        ev->buffer[len] = '\0';

        printf("client[%d]:%s", fd, ev->buffer);

        http_request(ev);

        qs_event_del(reactor->epfd, ev);
        qs_event_set(ev, fd, send_cb, reactor);
        qs_event_add(reactor->epfd, EPOLLOUT, ev);
    }
    else if(len == 0)
    {
        qs_event_del(reactor->epfd, ev);
        close(fd);
        printf("client[%d] close\n", fd);
    }
    else
    {
        qs_event_del(reactor->epfd, ev);
        printf("client[%d]", fd);
        perror("reacv error,\n");
        close(fd);
    }
    return 0;
}

int accept_cb(int fd, int events, void *arg)
{
    struct qsreactor *reactor = (struct qsreactor*)arg;
    if(reactor == NULL)return -1;

    struct sockaddr_in client_addr;
    socklen_t len = sizeof(client_addr);

    int clientfd;


    if((clientfd = accept(fd, (struct sockaddr*)&client_addr, &len)) == -1)
    {
        if(errno != EAGAIN && errno != EINTR)
        {}
        perror("accept error\n");
        return -1;
    }

    int flag = 0;
    if((flag = fcntl(clientfd, F_SETFL, O_NONBLOCK)) < 0)
    {
        printf("fcntl noblock error, %d\n",MAX_BUFLEN);
        return -1;
    }
    struct qsevent *event = qsreactor_idx(reactor, clientfd);

    qs_event_set(event, clientfd, recv_cb, reactor);
    qs_event_add(reactor->epfd, EPOLLIN, event);

    printf("new connect [%s:%d], pos[%d]\n",
           inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), clientfd);

    return 0;
}

int qsreactor_run(struct qsreactor *reactor)
{
    if(reactor == NULL)
    return -1;
    if(reactor->evblk == NULL)
    return -1;
    if(reactor->epfd < 0)
    return -1;

    struct epoll_event events[MAX_EPOLL_EVENTS + 1];
    while(1)
    {
        int nready = epoll_wait(reactor->epfd, events, MAX_EPOLL_EVENTS, 1000);

        if(nready < 0)
        {
            printf("epoll_wait error\n");
            continue;
        }
        for(int i=0; i<nready; i++)
        {
            struct qsevent *ev = (struct qsevent*)events[i].data.ptr;
            if((events[i].events & EPOLLIN) && (ev->events & EPOLLIN))
            {
                ev->callback(ev->fd, events[i].events, ev->arg);    
            }
            if((events[i].events & EPOLLOUT) && (ev->events & EPOLLOUT))
            {
                ev->callback(ev->fd, events[i].events, ev->arg);
            }
        }
    }
}

int main(int argc, char **argv)
{
    unsigned short port = atoi(argv[1]);

    int sockfd = sock(port);


    struct qsreactor *reactor = (struct qsreactor*)malloc(sizeof(struct qsreactor));
    qsreactor_init(reactor);

    qsreactor_addlistener(reactor, sockfd, accept_cb);
    qsreactor_run(reactor);

    qsreactor_destory(reactor);
    close(sockfd);
}
```

5.epoll反应堆模型下实现websocket协议

```
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <arpa/inet.h>

#include <fcntl.h>
#include <unistd.h>
#include <errno.h>
#include <time.h>

#include <sys/stat.h>
#include <sys/sendfile.h>

#include <openssl/sha.h>
#include <openssl/pem.h>
#include <openssl/bio.h>
#include <openssl/evp.h>

#define MAX_BUFLEN          4096
#define MAX_EPOLLSIZE       1024
#define MAX_EPOLL_EVENTS    1024

#define GUID            "258EAFA5-E914-47DA-95CA-C5AB0DC85B11"

enum
{ 
    WS_HANDSHAKE = 0,
    WS_TANSMISSION = 1,
    WS_END = 2,
};

typedef struct _ws_ophdr{
    unsigned char opcode:4,
    rsv3:1,
    rsv2:1,
    rsv1:1,
    fin:1;
    unsigned char pl_len:7,
    mask:1;
}ws_ophdr;

typedef struct _ws_head_126{
    unsigned short payload_lenght;
    char mask_key[4];
}ws_head_126;

typedef struct _ws_head_127{
    long long payload_lenght;
    char mask_key[4];
}ws_head_127;

typedef int (*NCALLBACK)(int, int, void*);

struct qsevent{
    int fd;
    int events;
    int status;
    void *arg;
    long last_active;

    int (*callback)(int fd, int event, void *arg);
    unsigned char buffer[MAX_BUFLEN];
    int length;

    /*websocket param*/
    int status_machine;
};

struct qseventblock{
    struct qsevent *eventsarrry;
    struct qseventblock *next;
};

struct qsreactor{
    int epfd;
    int blkcnt;
    struct qseventblock *evblk;
};

int recv_cb(int fd, int events, void *arg);
int send_cb(int fd, int events, void *arg);
struct qsevent *qsreactor_idx(struct qsreactor *reactor, int sockfd);

int readline(char *allbuf, int idx, char *linebuf)
{
    int len = strlen(allbuf);

    for(;idx < len;idx ++) {
        if (allbuf[idx] == '\r' && allbuf[idx+1] == '\n') {
            return idx+2;
        } else {
            *(linebuf++) = allbuf[idx];
        }
    }
    return -1;
}

int base64_encode(char *in_str, int in_len, char *out_str)
{    
    BIO *b64, *bio;    
    BUF_MEM *bptr = NULL;    
    size_t size = 0;    

    if (in_str == NULL || out_str == NULL)        
    return -1;    

    b64 = BIO_new(BIO_f_base64());    
    bio = BIO_new(BIO_s_mem());    
    bio = BIO_push(b64, bio);

    BIO_write(bio, in_str, in_len);    
    BIO_flush(bio);    

    BIO_get_mem_ptr(bio, &bptr);    
    memcpy(out_str, bptr->data, bptr->length);    
    out_str[bptr->length-1] = '\0';    
    size = bptr->length;    

    BIO_free_all(bio);    
    return size;
}

#define WEBSOCK_KEY_LENGTH  19

int websocket_handshake(struct qsevent *ev)
{
    char linebuf[128];
    int index = 0;
    char sec_data[128] = {0};
    char sec_accept[32] = {0};
    do
    {
        memset(linebuf, 0, sizeof(linebuf));
        index = readline(ev->buffer, index, linebuf);
        if(strstr(linebuf, "Sec-WebSocket-Key"))
        {
            strcat(linebuf, GUID);
            SHA1(linebuf+WEBSOCK_KEY_LENGTH, strlen(linebuf+WEBSOCK_KEY_LENGTH), sec_data);
            base64_encode(sec_data, strlen(sec_data), sec_accept);
            memset(ev->buffer, 0, MAX_BUFLEN);

            ev->length = sprintf(ev->buffer,
                                 "HTTP/1.1 101 Switching Protocols\r\n"
                                 "Upgrade: websocket\r\n"
                                 "Connection: Upgrade\r\n"
                                 "Sec-websocket-Accept: %s\r\n\r\n", sec_accept);
            break;   
        }
    }while(index != -1 && (ev->buffer[index] != '\r') || (ev->buffer[index] != '\n'));
    return 0;
}

void websocket_umask(char *payload, int length, char *mask_key)
{
    int i = 0;
    for( ; i<length; i++)
    payload[i] ^= mask_key[i%4];
}

int websocket_transmission(struct qsevent *ev)
{
    ws_ophdr *ophdr = (ws_ophdr*)ev->buffer;
    printf("ws_recv_data length=%d\n", ophdr->pl_len);
    if(ophdr->pl_len <126)
    {
        char * payload = ev->buffer + sizeof(ws_ophdr) + 4;
        if(ophdr->mask)
        {
            websocket_umask(payload, ophdr->pl_len, ev->buffer+2);
            printf("payload:%s\n", payload);
        }
        memset(ev->buffer, 0, ev->length);
        strcpy(ev->buffer, "00ok");
    }
    return 0;
}

int websocket_request(struct qsevent *ev)
{
    if(ev->status_machine == WS_HANDSHAKE)
    {
        websocket_handshake(ev);
        ev->status_machine = WS_TANSMISSION;
    }else if(ev->status_machine == WS_TANSMISSION){
        websocket_transmission(ev);
    }
    return 0;
}
void qs_event_set(struct qsevent *ev, int fd, NCALLBACK callback, void *arg)
{
    ev->events = 0;
    ev->fd = fd;
    ev->arg = arg;
    ev->callback = callback;
    ev->last_active = time(NULL);
    return;
}

int qs_event_add(int epfd, int events, struct qsevent *ev)
{
    struct epoll_event epv = {0, {0}};;
    epv.events = ev->events = events;
    epv.data.ptr = ev;

    if(ev->status == 1)
    { 
        if(epoll_ctl(epfd, EPOLL_CTL_MOD, ev->fd, &epv) < 0)
        {
            perror("EPOLL_CTL_MOD error\n");
            return -1;
        }
    }
    else if(ev->status == 0)
    {
        if(epoll_ctl(epfd, EPOLL_CTL_ADD, ev->fd, &epv) < 0)
        {
            perror("EPOLL_CTL_ADD error\n");    
            return -2;
        }
        ev->status = 1;
    }
    return 0;
}

int qs_event_del(int epfd, struct qsevent *ev)
{
    struct epoll_event epv = {0, {0}};
    if(ev->status != 1)
    return -1;
    ev->status = 0;
    epv.data.ptr = ev;
    if((epoll_ctl(epfd, EPOLL_CTL_DEL, ev->fd, &epv)))
    {
        perror("EPOLL_CTL_DEL error\n");
        return -1;
    }
    return 0;
}

int sock(short port)
{
    int fd = socket(AF_INET, SOCK_STREAM, 0);
    fcntl(fd, F_SETFL, O_NONBLOCK);

    struct sockaddr_in ser_addr;
    memset(&ser_addr, 0, sizeof(ser_addr));
    ser_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    ser_addr.sin_family = AF_INET;
    ser_addr.sin_port = htons(port);

    bind(fd, (struct sockaddr*)&ser_addr, sizeof(ser_addr));

    if(listen(fd, 20) < 0)
    perror("listen error\n");

    printf("listener[%d] lstening..\n", fd);
    return fd;
}

int qsreactor_init(struct qsreactor *reactor)
{
    if(reactor == NULL)
    return -1;
    memset(reactor, 0, sizeof(struct qsreactor));
    reactor->epfd = epoll_create(1);
    if(reactor->epfd <= 0)
    {
        perror("epoll_create error\n");
        return -1;
    }
    struct qseventblock *block = (struct qseventblock*)malloc(sizeof(struct qseventblock));
    if(block == NULL)
    {
        printf("blockinit malloc error\n");
        close(reactor->epfd);
        return -2;
    }
    memset(block, 0, sizeof(block));

    struct qsevent *evs = (struct qsevent*)malloc(MAX_EPOLLSIZE * sizeof(struct qsevent));
    if(evs == NULL)
    {
        printf("evsnit malloc error\n");
        close(reactor->epfd);
        return -3;
    }
    memset(evs, 0, sizeof(evs));

    block->next = NULL;
    block->eventsarrry = evs;

    reactor->blkcnt = 1;
    reactor->evblk = block;
    return 0;
}

int qsreactor_alloc(struct qsreactor *reactor)
{
    if(reactor == NULL)return -1;
    if(reactor->evblk == NULL)return -1;
    struct qseventblock *tailblock = reactor->evblk;
    while(tailblock->next != NULL)
    tailblock = tailblock->next;
    struct qseventblock *newblock = (struct qseventblock*)malloc(sizeof(struct qseventblock));
    if(newblock == NULL)
    {
        printf("newblock alloc error\n");
        return -1;
    }
    memset(newblock, 0, sizeof(newblock));

    struct qsevent *neweventarray = (struct qsevent*)malloc(sizeof(struct qsevent) * MAX_EPOLLSIZE);
    if(neweventarray == NULL)
    {
        printf("neweventarray malloc error\n");
        return -1;
    }
    memset(neweventarray, 0, sizeof(neweventarray));

    newblock->eventsarrry = neweventarray;
    newblock->next = NULL;

    tailblock->next = newblock;
    reactor->blkcnt++;

    return 0;
}

struct qsevent *qsreactor_idx(struct qsreactor *reactor, int sockfd)
{
    int index = sockfd / MAX_EPOLLSIZE;
    while(index >= reactor->blkcnt)qsreactor_alloc(reactor);
    int i=0;
    struct qseventblock *idxblock = reactor->evblk;
    while(i++<index && idxblock != NULL)
    idxblock = idxblock->next;

    return &idxblock->eventsarrry[sockfd%MAX_EPOLLSIZE];
}

int qsreactor_destory(struct qsreactor *reactor)
{
    close(reactor->epfd);
    free(reactor->evblk);
    reactor = NULL;
    return 0;
}

int qsreactor_addlistener(struct qsreactor *reactor, int sockfd, NCALLBACK acceptor)
{
    if(reactor == NULL)return -1;
    if(reactor->evblk == NULL)return -1;

    struct qsevent *event = qsreactor_idx(reactor, sockfd);
    qs_event_set(event, sockfd, acceptor, reactor);
    qs_event_add(reactor->epfd, EPOLLIN, event);

    return 0;
}

int send_cb(int fd, int events, void *arg)
{
    struct qsreactor *reactor = (struct qsreactor*)arg;
    struct qsevent   *ev = qsreactor_idx(reactor, fd);

    int ret = send(fd, ev->buffer, ev->length, 0);
    if(ret < 0)
    {
        qs_event_del(reactor->epfd, ev);
        printf("clent[%d] ", fd);
        perror("send error\n");
        close(fd);
    }
    else if(ret > 0)
    {
        printf("send to client[%d]:\n%s\n", fd, ev->buffer);
        qs_event_del(reactor->epfd, ev);
        qs_event_set(ev, fd, recv_cb, reactor);
        qs_event_add(reactor->epfd, EPOLLIN, ev);
    }
    return ret;
}

int recv_cb(int fd, int events, void *arg)
{
    struct qsreactor *reactor = (struct qsreactor*)arg;
    struct qsevent *ev = qsreactor_idx(reactor, fd);

    int len = recv(fd, ev->buffer, MAX_BUFLEN, 0);
    qs_event_del(reactor->epfd, ev);
    if(len > 0)
    {
        ev->length = len;
        ev->buffer[len] = '\0';

        printf("client[%d]:\n%s\n", fd, ev->buffer);

        websocket_request(ev);

        qs_event_del(reactor->epfd, ev);
        qs_event_set(ev, fd, send_cb, reactor);
        qs_event_add(reactor->epfd, EPOLLOUT, ev);
    }
    else if(len == 0)
    {
        qs_event_del(reactor->epfd, ev);
        close(fd);
        printf("client[%d] close\n", fd);
    }
    else
    {
        qs_event_del(reactor->epfd, ev);
        printf("client[%d]", fd);
        perror("reacv error,\n");
        close(fd);
    }
    return 0;
}

int accept_cb(int fd, int events, void *arg)
{
    struct qsreactor *reactor = (struct qsreactor*)arg;
    if(reactor == NULL)return -1;

    struct sockaddr_in client_addr;
    socklen_t len = sizeof(client_addr);

    int clientfd;


    if((clientfd = accept(fd, (struct sockaddr*)&client_addr, &len)) == -1)
    {
        if(errno != EAGAIN && errno != EINTR)
        {}
        perror("accept error\n");
        return -1;
    }

    int flag = 0;
    if((flag = fcntl(clientfd, F_SETFL, O_NONBLOCK)) < 0)
    {
        printf("fcntl noblock error, %d\n",MAX_BUFLEN);
        return -1;
    }
    struct qsevent *event = qsreactor_idx(reactor, clientfd);

    event->status_machine = WS_HANDSHAKE;

    qs_event_set(event, clientfd, recv_cb, reactor);
    qs_event_add(reactor->epfd, EPOLLIN, event);

    printf("new connect [%s:%d], pos[%d]\n",
           inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), clientfd);

    return 0;
}

int qsreactor_run(struct qsreactor *reactor)
{
    if(reactor == NULL)
    return -1;
    if(reactor->evblk == NULL)
    return -1;
    if(reactor->epfd < 0)
    return -1;

    struct epoll_event events[MAX_EPOLL_EVENTS + 1];
    while(1)
    {
        int nready = epoll_wait(reactor->epfd, events, MAX_EPOLL_EVENTS, 1000);

        if(nready < 0)
        {
            printf("epoll_wait error\n");
            continue;
        }
        for(int i=0; i<nready; i++)
        {
            struct qsevent *ev = (struct qsevent*)events[i].data.ptr;
            if((events[i].events & EPOLLIN) && (ev->events & EPOLLIN))
            {
                ev->callback(ev->fd, events[i].events, ev->arg);    
            }
            if((events[i].events & EPOLLOUT) && (ev->events & EPOLLOUT))
            {
                ev->callback(ev->fd, events[i].events, ev->arg);
            }
        }
    }
}

int main(int argc, char **argv)
{
    unsigned short port = atoi(argv[1]);

    int sockfd = sock(port);


    struct qsreactor *reactor = (struct qsreactor*)malloc(sizeof(struct qsreactor));
    qsreactor_init(reactor);

    qsreactor_addlistener(reactor, sockfd, accept_cb);
    qsreactor_run(reactor);

    qsreactor_destory(reactor);
    close(sockfd);
}
```

6.C1000K reactor模型，epoll实现，连接并回发一段数据，测试正常

```
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <arpa/inet.h>

#include <fcntl.h>
#include <unistd.h>
#include <errno.h>
#include <time.h>

#include <sys/stat.h>
#include <sys/sendfile.h>

#define MAX_BUFLEN          4096
#define MAX_EPOLLSIZE       1024
#define MAX_EPOLL_EVENTS    1024

typedef int (*NCALLBACK)(int, int, void*);

struct qsevent{
    int fd;
    int events;
    int status;
    void *arg;
    long last_active;

    int (*callback)(int fd, int event, void *arg);
    unsigned char buffer[MAX_BUFLEN];
    int length;
};

struct qseventblock{
    struct qsevent *eventsarrry;
    struct qseventblock *next;
};

struct qsreactor{
    int epfd;
    int blkcnt;
    struct qseventblock *evblk;
};

int recv_cb(int fd, int events, void *arg);
int send_cb(int fd, int events, void *arg);
struct qsevent *qsreactor_idx(struct qsreactor *reactor, int sockfd);

void qs_event_set(struct qsevent *ev, int fd, NCALLBACK callback, void *arg)
{
    ev->events = 0;
    ev->fd = fd;
    ev->arg = arg;
    ev->callback = callback;
    ev->last_active = time(NULL);
    return;
}

int qs_event_add(int epfd, int events, struct qsevent *ev)
{
    struct epoll_event epv = {0, {0}};;
    epv.events = ev->events = events;
    epv.data.ptr = ev;

    if(ev->status == 1)
    { 
        if(epoll_ctl(epfd, EPOLL_CTL_MOD, ev->fd, &epv) < 0)
        {
            perror("EPOLL_CTL_MOD error\n");
            return -1;
        }
    }
    else if(ev->status == 0)
    {
        if(epoll_ctl(epfd, EPOLL_CTL_ADD, ev->fd, &epv) < 0)
        {
            perror("EPOLL_CTL_ADD error\n");    
            return -2;
        }
        ev->status = 1;
    }
    return 0;
}

int qs_event_del(int epfd, struct qsevent *ev)
{
    struct epoll_event epv = {0, {0}};
    if(ev->status != 1)
        return -1;
    ev->status = 0;
    epv.data.ptr = ev;
    if((epoll_ctl(epfd, EPOLL_CTL_DEL, ev->fd, &epv)))
    {
        perror("EPOLL_CTL_DEL error\n");
        return -1;
    }
    return 0;
}

int sock(short port)
{
    int fd = socket(AF_INET, SOCK_STREAM, 0);
    fcntl(fd, F_SETFL, O_NONBLOCK);

    struct sockaddr_in ser_addr;
    memset(&ser_addr, 0, sizeof(ser_addr));
    ser_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    ser_addr.sin_family = AF_INET;
    ser_addr.sin_port = htons(port);

    bind(fd, (struct sockaddr*)&ser_addr, sizeof(ser_addr));

    if(listen(fd, 20) < 0)
        perror("listen error\n");

    printf("listener[%d] lstening..\n", fd);
    return fd;
}

int qsreactor_init(struct qsreactor *reactor)
{
    if(reactor == NULL)
    return -1;
    memset(reactor, 0, sizeof(struct qsreactor));
    reactor->epfd = epoll_create(1);
    if(reactor->epfd <= 0)
    {
        perror("epoll_create error\n");
       return -1;
    }
    struct qseventblock *block = (struct qseventblock*)malloc(sizeof(struct qseventblock));
    if(block == NULL)
    {
        printf("blockinit malloc error\n");
        close(reactor->epfd);
        return -2;
    }
    memset(block, 0, sizeof(block));

    struct qsevent *evs = (struct qsevent*)malloc(MAX_EPOLLSIZE * sizeof(struct qsevent));
    if(evs == NULL)
    {
        printf("evsnit malloc error\n");
        close(reactor->epfd);
        return -3;
    }
    memset(evs, 0, sizeof(evs));

    block->next = NULL;
    block->eventsarrry = evs;

    reactor->blkcnt = 1;
    reactor->evblk = block;
    return 0;
}

int qsreactor_alloc(struct qsreactor *reactor)
{
    if(reactor == NULL)return -1;
    if(reactor->evblk == NULL)return -1;
    struct qseventblock *tailblock = reactor->evblk;
    while(tailblock->next != NULL)
    tailblock = tailblock->next;
    struct qseventblock *newblock = (struct qseventblock*)malloc(sizeof(struct qseventblock));
    if(newblock == NULL)
    {
        printf("newblock alloc error\n");
        return -1;
    }
    memset(newblock, 0, sizeof(newblock));

    struct qsevent *neweventarray = (struct qsevent*)malloc(sizeof(struct qsevent) * MAX_EPOLLSIZE);
    if(neweventarray == NULL)
    {
        printf("neweventarray malloc error\n");
        return -1;
    }
    memset(neweventarray, 0, sizeof(neweventarray));

    newblock->eventsarrry = neweventarray;
    newblock->next = NULL;

    tailblock->next = newblock;
    reactor->blkcnt++;

    return 0;
}

struct qsevent *qsreactor_idx(struct qsreactor *reactor, int sockfd)
{
    int index = sockfd / MAX_EPOLLSIZE;
    while(index >= reactor->blkcnt)qsreactor_alloc(reactor);
    int i=0;
    struct qseventblock *idxblock = reactor->evblk;
    while(i++<index && idxblock != NULL)
        idxblock = idxblock->next;
    
    return &idxblock->eventsarrry[sockfd%MAX_EPOLLSIZE];
}

int qsreactor_destory(struct qsreactor *reactor)
{
    close(reactor->epfd);
    free(reactor->evblk);
    reactor = NULL;
    return 0;
}

int qsreactor_addlistener(struct qsreactor *reactor, int sockfd, NCALLBACK acceptor)
{
    if(reactor == NULL)return -1;
    if(reactor->evblk == NULL)return -1;

    struct qsevent *event = qsreactor_idx(reactor, sockfd);
    qs_event_set(event, sockfd, acceptor, reactor);
    qs_event_add(reactor->epfd, EPOLLIN, event);

    return 0;
}

int send_cb(int fd, int events, void *arg)
{
    struct qsreactor *reactor = (struct qsreactor*)arg;
    struct qsevent   *ev = qsreactor_idx(reactor, fd);

    int ret = send(fd, ev->buffer, ev->length, 0);
    if(ret < 0)
    {
        qs_event_del(reactor->epfd, ev);
        printf("clent[%d] ", fd);
        perror("send error\n");
        close(fd);
    }
    else if(ret > 0)
    {
        printf("send to client[%d]:%s", fd, ev->buffer);
        qs_event_del(reactor->epfd, ev);
        qs_event_set(ev, fd, recv_cb, reactor);
        qs_event_add(reactor->epfd, EPOLLIN, ev);
    }
    return ret;
}

int recv_cb(int fd, int events, void *arg)
{
    struct qsreactor *reactor = (struct qsreactor*)arg;
    struct qsevent *ev = qsreactor_idx(reactor, fd);

    int len = recv(fd, ev->buffer, MAX_BUFLEN, 0);
    qs_event_del(reactor->epfd, ev);
    if(len > 0)
    {
        ev->length = len;
        ev->buffer[len] = '\0';

        printf("client[%d]:%s", fd, ev->buffer);
   
        qs_event_del(reactor->epfd, ev);
        qs_event_set(ev, fd, send_cb, reactor);
        qs_event_add(reactor->epfd, EPOLLOUT, ev);
    }
    else if(len == 0)
    {
        qs_event_del(reactor->epfd, ev);
        close(fd);
        printf("client[%d] close\n", fd);
    }
    else
    {
        qs_event_del(reactor->epfd, ev);
        printf("client[%d]", fd);
        perror("reacv error,\n");
        close(fd);
    }
    return 0;
}

int accept_cb(int fd, int events, void *arg)
{
    struct qsreactor *reactor = (struct qsreactor*)arg;
    if(reactor == NULL)return -1;

    struct sockaddr_in client_addr;
    socklen_t len = sizeof(client_addr);

    int clientfd;


    if((clientfd = accept(fd, (struct sockaddr*)&client_addr, &len)) == -1)
    {
        if(errno != EAGAIN && errno != EINTR)
        {}
        perror("accept error\n");
        return -1;
    }

    int flag = 0;
    if((flag = fcntl(clientfd, F_SETFL, O_NONBLOCK)) < 0)
    {
        printf("fcntl noblock error, %d\n",MAX_BUFLEN);
        return -1;
    }
    struct qsevent *event = qsreactor_idx(reactor, clientfd);

    qs_event_set(event, clientfd, recv_cb, reactor);
    qs_event_add(reactor->epfd, EPOLLIN, event);

    printf("new connect [%s:%d], pos[%d]\n",
           inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), clientfd);

    return 0;
}

int qsreactor_run(struct qsreactor *reactor)
{
    if(reactor == NULL)
       return -1;
    if(reactor->evblk == NULL)
       return -1;
    if(reactor->epfd < 0)
       return -1;

    struct epoll_event events[MAX_EPOLL_EVENTS + 1];
    while(1)
    {
        int nready = epoll_wait(reactor->epfd, events, MAX_EPOLL_EVENTS, 1000);

        if(nready < 0)
        {
            printf("epoll_wait error\n");
            continue;
        }
        for(int i=0; i<nready; i++)
        {
            struct qsevent *ev = (struct qsevent*)events[i].data.ptr;
            if((events[i].events & EPOLLIN) && (ev->events & EPOLLIN))
            {
                ev->callback(ev->fd, events[i].events, ev->arg);    
            }
            if((events[i].events & EPOLLOUT) && (ev->events & EPOLLOUT))
            {
                ev->callback(ev->fd, events[i].events, ev->arg);
            }
        }
    }
}

int main(int argc, char **argv)
{
    unsigned short port = atoi(argv[1]);

    int sockfd = sock(port);


    struct qsreactor *reactor = (struct qsreactor*)malloc(sizeof(struct qsreactor));
    qsreactor_init(reactor);
       
    qsreactor_addlistener(reactor, sockfd, accept_cb);
    qsreactor_run(reactor);

    qsreactor_destory(reactor);
    close(sockfd);
}
```

原文链接：https://zhuanlan.zhihu.com/p/581974844

# 【NO.24】Makefile入门(超详细一文读懂)

## 1. [Makefile](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3DMakefile%26spm%3D1001.2101.3001.7020)编译过程

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212122020445061817.png)

Makefile文件中的命令有一定规范，一旦该文件编写好以后在Linux命令行中执行一条make命令即可自动编译整个工程。不同厂家的make可能会稍有不同，并且语法上也有区别，不过基本思想都差不多，主要还是落在目标依赖上，最广泛使用的是GNUmake。

## 2. 语法规则

```
目标 ... : 依赖 ...    命令1    命令2    . . .
```

Makefile的核心规则，类似于一位厨神做菜，目标就是做好一道菜，那么所谓的依赖就是各种食材，各种厨具等等，然后需要厨师好的技术方法类似于命令，才能作出一道好菜。

同时这些依赖也有可能此时并不存在，需要现场制作，或者是由其他厨师做好，那么这个依赖就成为了其他规则的目标，该目标也会有他自己的依赖和命令。这样就形成了一层一层递归依赖组成了Makefile文件。

Makefile并不会关心命令是如何执行的，仅仅只是会去执行所有定义的命令，和我们平时直接输入命令行是一样的效果。

1、目标即要生成的文件。如果目标文件的更新时间晚于依赖文件更新时间，则说明依赖文件没有改动，目标文件不需要重新编译。否则会进行重新编译并更新目标文件。

2、默认情况下Makefile的第一个目标为终极目标。

3、依赖：即目标文件由哪些文件生成。

4、命令：即通过执行命令由依赖文件生成目标文件。注意每条命令之前必须有一个tab保持缩进，这是语法要求（会有一些编辑工具默认tab为4个空格，会造成Makefile语法错误）。

5、all：Makefile文件默认只生成第一个目标文件即完成编译，但是我们可以通过all 指定所需要生成的目标文件。

## 3. 变量

$符号表示取变量的值，当变量名多于一个字符时，使用”( )”
$符的其他用法

$^ 表示所有的依赖文件

$@ 表示生成的目标文件

$< 代表第一个依赖文件

```
SRC = $(wildcard *.c)OBJ = $(patsubst %.c, %.o, $(SRC))ALL: hello.outhello.out: $(OBJ)        gcc $< -o $@$(OBJ): $(SRC)        gcc -c $< -o $@
```

## 4. 变量赋值

1、”**=**“是最普通的等号，在Makefile中容易搞错赋值等号，使用 “=”进行赋值，变量的值是整个Makefile中最后被指定的值。

```
VIR_A = AVIR_B = $(VIR_A) BVIR_A = AA
```

经过上面的赋值后，最后VIR_B的值是AA B，而不是A B，在make时，会把整个Makefile展开，来决定变量的值

2、”**:=**“ 表示直接赋值，赋予当前位置的值。

```
VIR_A := AVIR_B := $(VIR_A) BVIR_A := AA
```

最后BIR_B的值是A B，即根据当前位置进行赋值。因此相当于“=”，“：=”才是真正意义上的直接赋值

3、”**?=**“ 表示如果该变量没有被赋值，赋值予等号后面的值。

```
VIR ?= new_value
```

如果VIR在之前没有被赋值，那么VIR的值就为new_value。

```
VIR := old_valueVIR ?= new_value
```

这种情况下，VIR的值就是old_value
4、”**+=**“和平时写代码的理解是一样的，表示将符号后面的值添加到前面的变量上

## 5. 预定义变量

CC：c编译器的名称，默认值为cc。cpp c预编译器的名称默认值为$(CC) -E

```
CC = gcc
```

回显问题，Makefile中的命令都会被打印出来。如果不想打印命令部分 可以使用@去除回显

```
@echo "clean done!"
```

## 6. 函数

通配符

```
SRC = $(wildcard ./*.c)
```

匹配目录下所有.c 文件，并将其赋值给SRC变量。

```
OBJ = $(patsubst %.c, %.o, $(SRC))
```

这个函数有三个参数，意思是取出SRC中的所有值，然后将.c 替换为.o 最后赋值给OBJ变量。

示例：如果目录下有很多个.c 源文件，就不需要写很多条规则语句了，而是可以像下面这样写

```
SRC = $(wildcard *.c)OBJ = $(patsubst %.c, %.o, $(SRC))ALL: hello.outhello.out: $(OBJ)        gcc $(OBJ) -o hello.out$(OBJ): $(SRC)        gcc -c $(SRC) -o $(OBJ)
```

这里先将所有.c 文件编译为 .o 文件，这样后面更改某个 .c 文件时，其他的 .c 文件将不在编译，而只是编译有更改的 .c 文件，可以大大提高大项目中的编译速度。

## 7. 伪目标 .PHONY

伪目标只是一个标签，clean是个伪目标没有依赖文件，只有用make来调用时才会执行

当目录下有与make 命令 同名的文件时 执行make 命令就会出现错误。

解决办法就是使用伪目标

```
SRC = $(wildcard *.c)OBJ = $(patsubst %.c, %.o, $(SRC))ALL: hello.outhello.out: $(OBJ)        gcc $< -o $@$(OBJ): $(SRC)        gcc -c $< -o $@clean:        rm -rf $(OBJ) hello.out.PHONY: clean ALL
```

通常也会把ALL设置成伪目标

## 8. 其他常用功能

代码清理clean
我们可以编译一条属于自己的clean语句，来清理make命令所产生的所有文件，列如

```
SRC = $(wildcard *.c)OBJ = $(patsubst %.c, %.o, $(SRC))ALL: hello.outhello.out: $(OBJ)        gcc $< -o $@$(OBJ): $(SRC)        gcc -c $< -o $@clean:        rm -rf $(OBJ) hello.out
```

## 9. 嵌套执行Makefile

在一些大工程中，会把不同模块或不同功能的源文件放在不同的目录中，我们可以在每个目录中都写一个该目录的Makefile这有利于让我们的Makefile变的更加简洁，不至于把所有东西全部写在一个Makefile中。

列如在子目录subdir目录下有个Makefile文件，来指明这个目录下文件的编译规则。外部总Makefile可以这样写

```
subsystem:            cd subdir && $(MAKE)其等价于：subsystem:            $(MAKE) -C subdir
```

定义$(MAKE)宏变量的意思是，也许我们的make需要一些参数，所以定义成一个变量比较有利于维护。两个例子意思都是先进入”subdir”目录，然后执行make命令

我们把这个Makefile叫做总控Makefile，总控Makefile的变量可以传递到下级的Makefile中，但是不会覆盖下层Makefile中所定义的变量，除非指定了 “-e”参数。

如果传递变量到下级Makefile中，那么可以使用这样的声明

export

如果不想让某些变量传递到下级Makefile，可以使用

unexport

```
export variable = value等价于variable = valueexport variable等价于export variable := value等价于variable := valueexport variable如果需要传递所有变量，那么只要一个export就行了。后面什么也不用跟，表示传递所有变量
```

## 10. 指定头文件路径

一般都是通过”**-I**“（大写i）来指定，假设头文件在：

```
/home/develop/include
```

则可以通过-I指定：

```
-I/home/develop/include
```

将该目录添加到头文件搜索路径中

在Makefile中则可以这样写：

```
CFLAGS=-I/home/develop/include
```

然后在编译的时候，引用CFLAGS即可，如下

```
yourapp:*.c    gcc $(CFLAGS) -o yourapp
```

## 11. 指定库文件路径

与上面指定头文件类似只不过使用的是”**-L**“来指定

```
LDFLAGS=-L/usr/lib -L/path/to/your/lib
```

告诉链接器要链接哪些库文件，使用”**-l**“（小写L）如下：

```
LIBS = -lpthread -liconv
```

## 12. 简单的Makefile实例

目录结构

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202218_25631.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202219_80074.png)

### 12.1 nclude

myinclude.h

```
#include <stdio.h>void print1() ;void print2() ;
```

### 12.2 f1

f1.c

```
#include "../include/myinclude.h"                                                                              void print1()  {      printf("Message f1.c\n");      return;  } 
```

Makefile

目标前面的路径，意思是将目标生成到指定的目录下

```
../$(OBJS_DIR)/f1.o:f1.c                                                                                           @$(CC) -c $^ -o $@  
```

### 12.3 main

main.c

```
#include "../include/myinclude.h"                                                                                            int main(int argc, char const *argv[]){    print1();      print2();      return 0;}
```

Makefile

```
../$(OBJS_DIR)/main.o:main.c                                                                                       @$(CC) -c $^ -o $@  
```

### 12.4 obj

```
此目录用来存放相关生成的目标文件
```

Makefile

```
../$(BIN_DIR)/$(BIN) : $(OBJS)    @$(CC) $^ -o $@ 
```

### 12.5 主Makefile

```
#预定义变量CC = gcc#预定义编译目录SUBDIRS = f1 \        f2 \        main \        obj#预定义目标OBJS = f1.o f2.o main.oBIN = myappOBJS_DIR = objBIN_DIR = bin#传递预定义参数export CC OBJS BIN OBJS_DIR BIN_DIRall:CHECK_DIR $(SUBDIRS)CHECK_DIR:    @mkdir -p $(BIN_DIR)$(SUBDIRS):ECHO    @make -C $@ ECHO:    @echo $(SUBDIRS)    @echo begin compileclean:    @$(RM) $(OBJS_DIR)/*.o    @rm -rf $(BIN_DIR)
```

### 12.6 bin

此文件用来存放生成的二进制文件

### 12.7 执行结果

主Makefile执行过程

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202220_84792.jpg)

生成.o文件

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202220_23459.png)

生成二进制文件执行结果

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202221_69485.jpg)

————————————————

版权声明：本文为CSDN博主「晨曦艾米」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。

原文链接：[Makefile入门(超详细一文读懂)*晨曦艾米的博客-CSDN博客*makefile](https://link.zhihu.com/?target=https%3A//blog.csdn.net/ZBraveHeart/article/details/123187908)

原文作者：零声Github整理库

# 【NO.25】从CPU架构开始，讲清楚Linux进程调度和管理

## 1. X86架构

**CPU包括3个部分**：运算单元，数据单元，控制单元

总线上主要有两类数据，一类是地址数据（要拿内存中哪个位置的数据），这类总线是**地址总线**；另一类是真正的数据，这类总线是**数据总线**

**32位CPU包含的寄存器**

- 通用寄存器：`EAX/EBX/ECX/EDX/ESI/EDI/ESP/EBP`
- 段寄存器：`CS/DS/ES/FS/GS/SS`
- 指令寄存器：`EIP`
- 标志寄存器：`EFLAGS`
- 控制寄存器：`CR0/CR2/CR3/CR4`
- 系统表指针寄存器：`IDTR/GDTR/LDTR`
- 任务寄存器：`TR`
- 调试寄存器：`DR0-DR7`

> **文章相关视频讲解：**
> **[免费订阅学习：c/c++Linux后台服务器开发高级架构师学习视频](https://link.zhihu.com/?target=https%3A//ke.qq.com/course/417774%3FflowToken%3D1033508)**
> **[Linux内核内存管理—内存映射](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1EQ4y1d76X/)**
> **[Linux内核内存管理(二)](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1Qy4y1g7mY/)**
> **[虚拟内存空间之VMA实战操作](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1i44y1r7yL/)**
> **[剖析Linux内核MMU机制](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1xy4y1W7a6/)**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212122028371156310.png)

- `CS`：存储代码段的基址
- `DS`：存储数据段的基址
- `SS`：存储运行栈的基址
- `EBP`：栈基指针寄存器
- `ESP`：栈顶指针寄存器
- `IP`：指令指针寄存器，**将`CS:IP`获取到的指令存储`EIP`（指令寄存器）中**

## 2. 操作系统启动流程

主板上的ROM固化了一段初始化程序**BIOS（Basic Input and Output System，基本输入输出系统）**

Grub2是一个linux启动管理器 ，Grub2把boot.img共512字节安装到启动盘的第一个扇区，这个扇区称为**MBR（Master Boot Record，主引导记录扇区）**

- 主机上电后，CS重置为0xFFFF，IP重置为0x0000，所以第一条指令指向0xFFFF0，正好在ROM范围内，从这里开始进行**硬件检查**
- **将磁盘引导扇区读入**0x7c00处，并从0x7c000开始执行
- boot模块：bootsect.s，继续读取其它扇区
- setup模块：setup.s
- **从实模式切换到保护模式**（CR0寄存器最后一位置1）
- **初始化GDT，IDT**
- system模块：许多文件（head.s，main.c …）
- 进入main函数，**进入各种初始化操作**
  `INIT_TASK(init_task)`：系统创建第一个进程，**0号进程**。这是唯一一个没有通过fork或者kernel_thread产生的进程，是进程列表的第一个
  `trap_init()`
  `mm_init()`
  `sched_init()`
  `rest_init()`
- - `kernel_thread(kernel_init, NULL, CLONE_FS)` 创建第二个进程，这个是**1号进程**。 1号进程即将运行一个用户进程
  - 从内核态到用户态
  - ramdisk
  - 创建2号进程

0号进程 -> 1号内核进程 -> 1号用户进程(init进程) -> getty进程 -> shell进程 -> 命令行执行进程。

1. 加载内核：打开电源后，加载BIOS（只读，在主板的ROM里），硬件自检，读取MBR（主引导记录），运行Boot Loader，内核就启动了，这里还有从实模式到保护模式的切换
2. 进程：内核启动后，先启动第0号进程init_task，然后在内核运行init()函数，1号进程进入用户态变为init进程，init进程是所有用户态进程的老祖宗，其配置文件是/etc/inittab，通过该文件设置运行等级：根据/etc/inittab文件设置运行等级，比如有无网络
3. 系统初始化：启动第一个用户层文件/etc/rc.d/rc.sysinit，然后激活交换分区、检查磁盘、加载硬件模块等
4. 建立终端：系统初始化后返回init，这时守护进程也启动了，init接下来打开终端以供用户登录

- **init_task是第0号进程，唯一一个没有通过fork或kernel_thread产生的进程，是进程列表的第一个**
- **init是第1号进程，会成为用户态，是所有用户态进程的祖先**
- **kthreadd是第2号进程，会成为内核态，是所有内核态进程的祖先**

## 3. 系统调用

**int指令将CS中的CPL改为0，从而进入内核，这是用户程序发起调用内核代码的唯一方式**

**系统调用核心**

1. 用户程序中包含一段包含int指令的代码
2. 操作系统写中断处理，获取调用程序的编号
3. 操作系统根据编号执行相应代码

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202938_75065.jpg)

1. 在`glibc`里，任何一个系统调用都会调用`DO_CALL`，在`DO_CALL`里会根据系统调用名称，得到**系统调用号**，放在**寄存器`EAX`**里，把请求参数放在其它寄存器里，然后**执行`int $0x80`**
2. `int $0x80`触发一个软中断，通过它陷入内核
   内核启动时`trap_init()`指定软中断陷入门的入口，即`entry_INT80_32`
3. 保存当前用户态的寄存器，保存在**`pt_regs`结构**里
4. 将系统调用号从寄存器`EAX`里取出来，然后根据系统调用号，在**系统调用表**中找到相应的函数进行调用
5. 系统调用结束后， 原来用户态保存的现场恢复回来

## 4. 进程状态切换

- 就绪状态（ready）：等待被调度
- 运行状态（running）
- 阻塞状态（waiting）：等待资源

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202939_59603.jpg)

- 只有就绪态和运行态可以相互转换，其它的都是单向转换。就绪状态的进程通过调度算法从而获得 CPU 时间，转为运行状态；而运行状态的进程，在分配给它的 CPU 时间片用完之后就会转为就绪状态，等待下一次调度。
- 阻塞状态是缺少需要的资源从而由运行状态转换而来，但是该资源不包括 CPU 时间，缺少 CPU 时间会从运行态转换为就绪态。

## 5.进程数据结构

**Linux的PCB是task_struct结构**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202939_30279.jpg)

### 5.1 进程状态

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202940_27637.jpg)

很多操作系统教科书将正在CPU上执行的进程定义为RUNNING状态，而将可执行但是尚未被调度执行的进程定义为READY状态，**这两种状态在linux下统一为TASK_RUNNING状态**

### 5.2 任务ID

在Linux系统中，一个线程组中的所有线程使用和该线程组的领头线程相同的pid，并被存放在tgid成员中。只有线程组的领头线程的pid成员才会被设置为与tgid相同的值

**注意，`getpid()`系统调用返回的是当前进程的tgid值而不是pid值**

### 5.3 用户栈和内核栈

内核在创建进程的时候，在创建`task_struct`的同时，会为进程创建相应的堆栈。**每一个进程都有两个栈，一个用户栈，存在于用户空间；一个内核栈，存在于内核空间**

当进程在用户空间运行时，CPU堆栈指针寄存器里面的内容都是用户栈地址，使用用户栈；当进程在内核空间运行时，CPU堆栈指针寄存器里面的内容是内核栈地址，使用内核栈

当进程因为中断或者系统调用陷入到内核态时，进程所使用的堆栈也要从用户栈转到内核栈。

进程陷入到内核态后，**先把用户态堆栈的地址保存在内核栈之中，然后设置堆栈指针寄存器的内容为内核栈的地址，**这样就完成了用户栈向内核栈的转换；当进程从内核态恢复到用户态时**，在内核态之后的最后将保存在内核栈里面的用户栈的地址恢复到堆栈指针寄存器即可**。这样就实现了内核栈向用户栈的转换

在进程从用户态转到内核态的时候，进程的内核栈总是空的。这是因为当进程在用户态运行时使用用户栈，当进程陷入到内核态时，内核保存进程在内核态运行的相关信息，但是一旦进程返回到用户态后，内核栈中保存的信息全部无效，因此每次进程从用户态陷入内核的时候得到的内核栈都是空的。**所以在进程陷入内核的时候，直接把内核栈的栈顶地址给堆栈指针寄存器就可以了**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202941_30547.jpg)

`task_struct->stack`指向进程的内核栈，**大小为8K**

```
union thread_union {    struct thread_info thread_info;    unsigned long stack[THREAD_SIZE/sizeof(long)];};
```

整个内核栈用union表示，thread_info和stack共用一段存储空间，thread_info占用低地址。在pt_regs和STACK_END_MAGIC之间，就是内核代码的运行栈。当内核栈增长超过STACK_END_MAGIC就会报内核栈溢出

**thread_info**：存储内核态运行的一些信息，如指向`task_struct`的task指针，使得陷入内核态之后仍然能够找到当前进程的`task_struct`，还包括是否允许内核中断的preemt_count开关等等

**pt_regs**：**存储用户态的硬件上下文**，用户态进入内核态后，由于使用的栈、内存地址空间、代码段等都不同，所以用户态的eip、esp、ebp等需要保存现场，内核态恢复到用户态时再将栈中的信息恢复到硬件。由于进程调度一定会在内核态的schedule函数，用户态的所有硬件信息都保存在pt_regs中了。SAVE_ALL指令就是将用户态的cpu寄存器值保存如内核栈，RESTORE_ALL就是将pt_regs中的值恢复到寄存器中，这两个指令在介绍中断的时候还会提到

TSS（task state segment）：这是intel为上层做进程切换提供的硬件支持，还有一个TR（task register）寄存器专门指向这个内存区域。当TR指针值变更时，intel会将当前所有寄存器值存放到当前进程的tss中，然后再讲切换进程的目标tss值加载后寄存器中

这里很多人都会有疑问，不是有内核栈的pt_regs存储硬件上下文了吗，为什么还要有tss？前文说过，**进程切换都是在内核态，而pt_regs是保存的用户态的硬件上下文，tss用于保存内核态的硬件上下文**

但是linux并没有买账使用tss，因为linux实现进程切换时并不需要所有寄存器都切换一次，如果使用tr去切换tss就必须切换全部寄存器，性能开销会很高。这也是intel设计的败笔，没有把这个功能做的更加的开放导致linux没有用。linux使用的是软切换，主要使用thread_struct，tss仅使用esp0这个值，用于进程在用户态 -> 内核态时，硬件会自动将该值填充到esp寄存器。在初始化时仅为每1个cpu仅绑定一个tss，然后tr指针一直指向这个tss，永不切换。

thread_struct：一个和硬件体系强相关的结构体，**用来存储内核态切换时的硬件上下文**

### 5.4 context_switch执行过程

内存空间切换

将curr_task设置为新进程的task

将cpu寄存器保存到旧进程的thread_struct结构

将新进程的thread_struct的寄存器的值写入cpu

切换栈顶指针

## 6. Linux调度算法

### 6.1 调度子系统

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202941_38524.jpg)

### 6.2 调度策略和调度类

- Linux把进程分成**实时进程**和**非实时进程**，非实时进程进一步分成**交互式进程**和**批处理进程**
- 实时进程要求执行时间控制在一定范围内，**基于优先级调度，可抢占低优先级进程**

```
task_struct {  unsigned int policy  // 调度策略  int prio, static_prio, normal_prio;   // 优先级  unsigned int rt_priority;  const struct sched_class* sched_class;   // 调度器类}
// 调度策略定义#define SCHED_NORMAL   0         // 普通进程#define SCHED_FIFO     1         // 相同优先级先来先服务#define SCHED_RR       2         // 相同优先级轮流调度#define SCHED_BATCH    3         // 后台进程#define SCHED_IDLE     5         // 空闲进程#define SCHED_DEADLINE 6         // 相同优先级电梯算法（deadline距离当前时间点最近）
```

- 实时进程的优先级范围是0-99，非实时进程优先级范围100-139，**数值越小，优先级越高**
- **实时进程的调度策略**：SCHED_FIFO、SCHED_RR、SCHED_DEADLINE
- **非实时进程的调度策略**：SCHED_NORMAL、SCHED_BATCH、SCHED_IDLE

```
// 调度器类定义stop_sched_class  // 优先级最高的任务会使用这种策略，中断所有其他线程，且不会被其他任务打断dl_sched_class    // 对应deadline调度策略rt_sched_class    // 对应RR算法或者FIFO算法的调度策略，具体调度策略由进程的task_struct->policy指定fair_sched_class  // 普通进程的调度策略idle_sched_class  // 空闲进程的调度策略
```

### 6.3 CFS调度算法（完全公平调度算法）

- CFS给每个进程安排一个**虚拟运行时间vruntime**，正在运行的进程vruntime随tick不断增大，没有运行的进程vruntime不变，**vruntime小的会被优先运行**
- 对于不同优先级的进程，换算vruntime时**优先级高的算少，优先级低的算多**，这样优先级高的进程实际运行时间就变多了
- 调度队列使用红黑树，红黑树的节点是调度实体

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202942_76856.jpg)

- CFS的队列是一棵红黑树，红黑树的节点是调度实体，每个调度实体都属于一个task_struct，task_struct里面有指针指向这个进程属于哪个调度类
- CPU需要找下一个任务执行时，会按照优先级依次调用调度类，不同的调度类操作不同的队列。当然rt_sched_class先被调用，它会在rt_rq上找下一个任务，只有找不到的时候，才轮到fair_sched_class被调用，它会在cfs_rq上找下一个任务。这样保证了实时任务的优先级永远大于普通任务

### 6.4 调度触发

- 调度触发有两种类型：**主动调度**和**被动调度**（抢占式调度）
- 进程的调度都最终会调用到内核态的**__schedule函数**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202943_48749.jpg)

**主动调度**

- 进程发生需要等待IO的系统调用
- 进程主动sleep
- 进程等待占用信号量或mutex（自旋锁不会触发调度）

**抢占式调度**（**先标记为应该被抢占，等到调用__schedule函数时才调度**）

- 时钟中断处理函数
- 当一个进程被唤醒的时候

### 6.5 抢占的时机

用户态的抢占时机

- 从系统调用返回用户态
- 从中断返回用户态

内核态的抢占时机

- 在内核态的执行中，有的操作是不能被中断的，在进行这些操作之前，先调用preempt_disable关闭抢占，当再次打开的时候，就是一次内核态被抢占的机会
- 从中断返内核态

## 7. fork过程

fork系统调用最终调用*do*fork函数

1. 复制结构copy_process
2. 唤醒新进程wake_up_new_task

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202943_53131.jpg)

## 8. 线程创建过程

线程不是一个完全由内核实现的机制，它是由内核态和用户态合作完成的

**调用clone系统调用**

创建进程调用的系统调用是fork，在copy_process函数里面，会将五大结构`files_struct`、`fs_struct`、`sighand_struct`、`signal_struct`、`mm_struct`都复制一遍，从此父进程和子进程各用各的数据结构

创建线程调用的系统调用是clone，在copy_process函数里面，**五大结构仅仅是引用计数加一**，即线程共享进程的数据结构

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212202944_59887.jpg)

## 9. Linux装载和运行程序

Shell进程在读取用户输入的命令之后会调用fork复制出一个新的Shell进程，然后新的Shell进程调用exec执行新的程序

## 10. 函数调用堆栈

3个重要的寄存器

- `EIP`指向CPU下次要执行指令
- `ESP`栈指针寄存器，永远指向系统栈最上面一个**栈帧的栈顶**
- `EBP`基址指针寄存器，永远指向系统栈最上面一个**栈帧的栈底**

栈帧

每个函数每次调用，都有自己独立的栈帧，`ESP`指向当前栈帧的栈顶，`EBP`指向当前栈帧的栈底

栈从高地址向低地址扩展

```
push eax; == esp=esp-4;eax->[esp];pop eax; == [esp]->eax;esp=esp+4;
```

在`main()`里调用`A(a, b)`

1. `push b;` 最右边参数先入栈
2. `push a;`
3. `push eip;` 将A函数的`EIP`压栈
4. `push ebp;` 将A函数的`EBP`压栈（把main函数的栈帧底部保存起来，不用保存栈帧顶部）
5. `mov ebp esp;` 将当前的`ESP`赋值给`EBP`，相当于`EBP`从指向main的栈基址指向了A的栈基址
   …
6. 当从A函数返回时，`ESP`移动到栈帧底部（释放局部变量），然后把main函数的栈帧底部弹出到`EBP`，再弹出返回地址到`EIP`上，`ESP`继续移动划过参数，这样，`EBP`，`ESP`就回到了调用函数前的状态，即恢复了原来的main的栈帧

## 11. CPU上下文切换

CPU上下文：**CPU相关寄存器**

CPU上下文切换：保存前一个任务的CPU上下文，加载新任务的CPU上下文，然后运行新任务

根据任务不同，CPU上下文切换分成不同场景

- 进程上下文切换
- 线程上下文切换
- 中断上下文切换

进程可以在用户空间运行，也可以在内核空间运行，用户态和内核态的转变通过系统调用完成；

系统调用过程发生了CPU上下文切换

- 一次系统调用，发生了两次CPU上下文切换

**进程的切换发生在内核态，进程的上下文不仅包括了虚拟内存，栈，全局变量等用户空间的资源，还包括内核堆栈，寄存器等内核空间的状态**

进程上下文切换比系统调用多了一步

当虚拟内存更新后，TLB也需要更新，内存访问会随之变慢，影响所有处理器的进程

线程上下文切换

- 线程是调度的基本单位
- 前后两个线程属于不同进程，切换过程同进程上下文切换
- 前后两个线程属于同一进程，切换时虚拟内存等共享资源不动，只切换线程私有数据等非共享资源

中断上下文只包括内核中断服务程序执行所必需的的状态，包括CPU寄存器，内核堆栈，硬件中断参数等

过多的上下文切换，会把CPU时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，从而缩短进程真正运行的时间，导致系统的整体性能大幅下降

```
# 每隔5s输出1组数据vmstat 5#    cs(context switch)是每秒上下文切换的次数#    in(interrupt)则是每秒中断的次数#    r(Running or Runnable)是就绪队列的长度，也就是正在运行和等待CPU的进程数#    b(Blocked)则是处于不可中断睡眠状态的进程数# 每隔5s输出1组数据pidstat -wt 5#    cswch 每秒自愿上下文切换的次数#    nvcswch 每秒非自愿上下文切换的次数
```

## 12. 内核线程、轻量级进程、用户进程

Linux下只有一种类型的进程，就是task_struct

一个进程由于其运行空间的不同, 从而有内核线程和用户进程的区分, 内核线程运行在内核空间, 之所以称之为线程是因为它没有虚拟地址空间，只能访问内核的代码和数据。而用户进程则运行在用户空间，但是可以通过中断, 系统调用等方式从用户态陷入内核态

用户进程运行在用户空间上, 而一些通过共享资源实现的一组进程我们称之为线程组。linux下内核其实本质上没有线程的概念，linux下线程其实上是与其他进程共享某些资源的进程而已。但是我们习惯上还是称他们为线程或者轻量级进程

## 13. 进程栈，线程栈，内核栈，中断栈

- 进程栈就是Linux内存结构的栈，向低地址增长
- **线程栈由mmap获得，和进程处于同一地址空间，线程栈是事先分配好的，不能动态增长**
- 内核栈在地址空间的最高处，当用户调用系统调用时就进入内核栈，进程间的内核栈是相互独立的
- 中断栈用于运行中断响应程序

## 14. 中断

**中断导致系统从用户态转为内核态**

- 用户程序调用系统调用引起的**软件中断**
- 由外部设备产生的**硬件中断**
- 代码执行出错导致的**异常**

**软件中断**

用户进程调用系统调用时，会触发软件中断（第128号中断），该中断使得系统转到内核态，并执行相应的中断处理程序，中断处理程序根据系统调用号调用对应的系统调用

**硬件中断**

CPU收到硬件中断后，就会通知操作系统，每个硬件中断有对应一个中断号，并对应一个中断处理程序（ISR）

中断处理程序一般分为两部分（top half，bottom half），执行前面部分时会禁止所有中断，做一些有严格时限的工作，在执行后面部分时则允许被打断

Linux2.6之后，每个处理器都拥有一个中断栈专门用来执行中断处理程序

**中断处理流程**

1. 硬件中断到达中断控制器，后者检查该中断是否被屏蔽
2. 中断到达CPU
3. 系统跳转到一个预定义的内存位置，执行预定义的指令
4. 屏蔽相同的中断，检查是否有对应的ISR
5. 如果有就执行对应的ISR
6. 检查是否需要调用schedule()，返回被中断处

**用户态和内核态**

系统调用、异常、外围设备中断会导致系统进入内核态

系统调用：malloc内存分配

异常：缺页异常

外围设备中断：鼠标键盘

从用户态转入内核态时，系统需要先保存进程上下文，切换到内核栈，更新寄存器，将权限修改为特权级，转而去执行相应指令

## 15. 进程、线程、协程

进程

- 进程是资源分配的基本单位，它是程序执行时的一个实例。程序运行时系统就会创建一个进程，并为它分配资源，然后把该进程放入进程就绪队列，进程调度器选中它的时候就会为它分配CPU时间，程序开始真正运行
- 一个进程收到来自客户端新的请求时，可以通过`fork()`复制出一个子进程来处理，父进程只需负责监控请求的到来，这样就能做到并发处理。根据写时拷贝（COW）机制，分为两个进程继续运行后面的代码。**fork分别在父进程和子进程中返回，在子进程永远返回0，在父进程返回的是子进程的pid**

线程

- 线程是资源调度的基本单位，一个进程可以由很多个线程组成，线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。同样多线程也可以实现并发操作，每个请求分配一个线程来处理

线程和进程的区别

- 进程是资源分配的最小单位，线程是资源调度的最小单位
- 一个进程的所有线程共享地址空间，但每个线程具有独立的程序计数器，寄存器，栈，CPU切换的代价和创建一个线程的开销都比进程小
- 进程间通信需要通过IPC进行，线程间通信更方便，但要处理好**同步与互斥**，
- 多进程程序更健壮，因为进程有自己独立的地址空间，多线程程序只要有一个线程死掉，整个进程就死掉了

协程

协程是用户态的轻量级线程，其调度由用户控制，共享进程的地址空间，协程有自己的栈和寄存器，在切换时需要保存/恢复状态，切换时减少了切换到内核态的开销. 一个进程和线程都可以有多个协程. **相比函数调用, 协程可以在遇到IO阻塞时交出控制权**

- 子程序，或者称为函数，在所有语言中都是层级调用，比如A调用B，B在执行过程中又调用了C，C执行完毕返回，B执行完毕返回，最后是A执行完毕。
- 所以子程序调用是通过栈实现的，一个线程就是执行一个子程序。
- 子程序调用总是一个入口，一次返回，调用顺序是明确的。
- 而协程的调用和子程序不同。协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。
- 在一个子程序中中断，去执行其他子程序，不是函数调用，有点类似CPU的中断
- 协程的特点在于是一个线程执行，那和多线程比，协程有何优势？
- - **最大的优势就是协程极高的执行效率**。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。
  - **第二大优势就是不需要多线程的锁机制**，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。
- 因为协程是一个线程执行，那怎么利用多核CPU呢？
- - 最简单的方法是多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能。
  - Python对协程的支持还非常有限，用在generator中的yield可以一定程度上实现协程。虽然支持不完全，但已经可以发挥相当大的威力了。

## 16. 多进程与多线程的使用场景

多进程适用于**CPU密集型**，或者**多机分布式**场景中, 易于多机扩展

**多线程模型的优势是线程间切换代价较小**，因此适用于**I/O密集型**的工作场景，因此I/O密集型的工作场景经常会由于I/O阻塞导致频繁的切换线程。同时，多线程模型也适用于**单机多核**分布式场景

## 17. 协程

协程（coroutine）又叫微线程、纤程，完全位于用户态，一个程序可以有多个协程

协程的执行过程类似于子例程，允许子例程在特定地方挂起和恢复

协程是一种伪多线程，在一个线程内执行，**由用户切换**，由**用户选择切换时机**，**没有进入内核态**，只涉及CPU上下文切换，所以切换效率很高

缺点：协程适用于IO密集型，不适用于CPU密集型

## 18. 进程调度算法

- RR 时间片轮转调度算法
- FCFS 先来先服务调度算法
- SJF 短作业优先调度算法
- 优先级调度算法
- 多级队列调度算法
- 多级反馈队列调度算法
- 高响应比优先调度算法

对于单处理器系统，每次只允许一个进程运行，任何其他进程必须等待，直到CPU空闲能被调度为止，**多道程序**的目的是在任何时候都有某些进程在运行，以使CPU使用率最大化。

- 先到先服务FCFS，用了FIFO队列，非抢占
- 轮转法调度Round Robin，就绪队列作为循环队列，按**时间片**切换，一般20ms~50ms
- 最短作业优先调度SJF，先处理短进程，平均等待时间最小，所以是**最佳**的，但是很难知道每个作业要多少实现，所以不太现实
- 优先级调度，会有**饥饿**现象，低优先级的进程永远得不到调度
- 多级队列调度，根据进程的属性（如内存大小、类型、优先级）分到特定队列，不同队列执行不同的调度算法
- 多级反馈队列调度，允许进程在队列之间移动
- - 如果进程使用过多的CPU时间，那么它会被移到更低的优先级队列。
  - 这种方案将I/O密集型和交互进程放在更高优先级队列上。
  - 此外，在较低优先级队列中等待过长的进程会被移到更高优先级队列。这种形式的老化可阻止饥饿的发生。

CPU密集转为IO密集

饥饿(starvation)是什么，如何解决

饥饿是指某进程因为优先级的关系一直得不到CPU的调度，可能永远处于等待/就绪状态；定期提升进程的优先级

原文链接：https://zhuanlan.zhihu.com/p/378155667

原文作者：零声Github整理库

# 【NO.26】【引路者】学习DPDK，须知多队列网卡的原理

网卡多队列，顾名思义，也就是传统网卡的`DMA`队列有多个，网卡有基于多个`DMA`队列的分配机制。多队列网卡已经是当前高速率网卡的主流。

## 1.RPS

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212122034487115657.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212203546_10420.jpg)

`Linux`内核中，`RPS`（`Receive Packet Steering`）在接收端提供了这样的机制。RPS主要是把软中断的负载均衡到`CPU`的各个`core`上，网卡驱动对每个流生成一个`hash`标识，这个`hash`值可以通过四元组（源IP地址`SIP`，源四层端口`SPOR`T，目的IP地址`DIP`，目的四层端口`DPORT`）来计算，然后由中断处理的地方根据这个`hash`标识分配到相应的`core`上去，这样就可以比较充分地发挥多核的能力了。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212203547_55361.jpg)

## 2. DPDK多队列支持

`DPDK Packet I/O`机制具有与生俱来的多队列支持功能，可以根据不同的平台或者需求，选择需要使用的队列数目，并可以很方便地使用队列，指定队列发送或接收报文。由于这样的特性，可以很容易实现`CPU`核、缓存与网卡队列之间的亲和性，从而达到很好的性能。从`DPDK`的典型应用`l3fwd`可以看出，在某个核上运行的程序从指定的队列上接收，往指定的队列上发送，可以达到很高的cache命中率，效率也就会高。

除了方便地做到对指定队列进行收发包操作外，DPDK的队列管理机制还可以避免多核处理器中的多个收发进程采用自旋锁产生的不必要等待。

以`run to completion`模型为例，可以从核、内存与网卡队列之间的关系来理解DPDK是如何利用网卡多队列技术带来性能的提升。

- 将网卡的某个接收队列分配给某个核，从该队列中收到的所有报文都应当在该指定的核上处理结束。
- 从核对应的本地存储中分配内存池，接收报文和对应的报文描述符都位于该内存池。
- 为每个核分配一个单独的发送队列，发送报文和对应的报文描述符都位于该核和发送队列对应的本地内存池中。

可以看出不同的核，操作的是不同的队列，从而避免了多个线程同时访问一个队列带来的锁的开销。但是，如果逻辑核的数目大于每个接口上所含的发送队列的数目，那么就需要有机制将队列分配给这些核。不论采用何种策略，都需要引入锁来保护这些队列的数据。

网卡是如何将网络中的报文分发到不同的队列呢？常用的方法有微软提出的RSS与英特尔提出的`Flow Director`技术，前者是根据哈希值希望均匀地将包分发到多个队列中。后者是基于查找的精确匹配，将包分发到指定的队列中。此外，网卡还可以根据优先级分配队列提供对`QoS`的支持。

## 3. DPDK重点学习内容如下：

### **3.1 DPDK环境与testpmd/l3fwd/skeleton**

1. DPDK环境参数讲解
2. **多队列网卡的工作原理**
3. CPU亲和性
4. Burst数据包的优缺点
5. DPDK轮询模式 异步中断，轮询模式，混合中断轮询模式
6. virtio与vhost

### **3.2 DPDK的用户态协议栈实现**

1. 内核网络接口 KNI的实现原理
2. 接收线程/发送线程/KNI线程
3. 内存数据结构 rte_mbuf, rte_mempool
4. 端口数据结构 rte_kni, rte_kni_conf, rte_kni_ops, rte_eth_conf
5. 协议数据结构 rte_ether_hdrrte_ipv4_hdr, rte_udp_hdr
6. 数据处理接口 rte_eth_rx_burst,rte_kni_tx_burst,rte_pktmbuf_mtod

### **3.3 千万级流量并发的DNS处理**

1. udp协议包处理
2. dns协议实现
3. 配置文件解析
4. 数据结构 rte_ring
5. trex数据包性能测试

### **3.4 高性能数据处理框架VPP**

1. vpp使用vmxnet3
2. DPDK ACL实现数据过滤
3. vpp web应用
4. vpp基础库 VPPInfra
5. 高速查找路由表，CAM表

### **3.5 DPDK的虚拟交换机框架 OvS**

1. OvS三大组件 ovs-vswitchd, ovsdb-server, openvswitch.ko
2. OvS报文处理机制
3. OvS 4种数据路径
4. VXLAN数据协议

PS：系统学习DPDK，**[我要进大厂](https://link.zhihu.com/?target=https%3A//jq.qq.com/%3F_wv%3D1027%26k%3DkV5PWhiO)**

## 4. 流分类

高级的网卡设备（比如`Intel XL710`）可以分析出包的类型，包的类型会携带在接收描述符中，应用程序可以根据描述符快速地确定包是哪种类型的包。`DPDK`的`Mbuf`结构中含有相应的字段来表示网卡分析出的包的类型。

### 4.1 RSS（Receive-Side Scaling，接收方扩展）

`RSS`就是根据关键字通过哈希函数计算出哈希值，再由哈希值确定队列。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212203547_95631.jpg)

关键字是如何确定的呢？

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212203548_92080.jpg)

哈希函数一般选取微软托普利兹算法（`Microsoft Toeplitz Based Hash`）或者对称哈希。

### 4.2 Flow Director

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212203549_83387.jpg)

`Flow Director`技术是`Intel`公司提出的根据包的字段精确匹配，将其分配到某个特定队列的技术：网卡上存储了一个`Flow Director`的表，表的大小受硬件资源限制，它记录了需要匹配字段的关键字及匹配后的动作；驱动负责操作这张表，包括初始化、增加表项、删除表项；网卡从线上收到数据包后根据关键字查`Flow Director`的这张表，匹配后按照表项中的动作处理，可以是分配队列、丢弃等。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212203550_78445.jpg)

相比`RSS`的负载分担功能，它更加强调特定性。比如，用户可以为某几个特定的TCP对话（`S-IP+D-IP+S-Port+D-Port`）预留某个队列，那么处理这些`TCP`对话的应用就可以只关心这个特定的队列，从而省去了`CPU`过滤数据包的开销，并且可以提高`cache`的命中率。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212203550_29281.jpg)

### 4.3 服务质量

多队列应用于服务质量（`QoS`）流量类别：把发送队列分配给不同的流量类别，可以让网卡在发送侧做调度；把收包队列分配给不同的流量类别，可以做到基于流的限速。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212203551_34380.jpg)

### 4.4 流过滤

来自外部的数据包哪些是本地的、可以被接收的，哪些是不可以被接收的？可以被接收的数据包会被网卡送到主机或者网卡内置的管理控制器，其过滤主要集中在以太网的二层功能，包括`VLAN`及`MAC`过滤。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212203552_74459.jpg)

## 5. 应用

针对`Intel®XL710`网卡，PF使用`i40e Linux Kernel`驱动，`VF`使用`DPDK i40e PMD`驱动。使用`Linux`的`Ethtool`工具，可以完成配置操作`cloud filter`，将大量的数据包直接分配到`VF`的队列中，交由运行在`VF`上的虚机应用来直接处理。

```
echo 1 > /sys/bus/pci/devices/0000:02:00.0/sriov_numvfsmodprobe pci-stubecho "8086 154c" > /sys/bus/pci/drivers/pci-stub/new_idecho 0000:02:02.0 > /sys/bus/pci/devices/0000:2:02.0/driver/unbindecho 0000:02:02.0 > /sys/bus/pci/drivers/pci-stub/bindqemu-system-x86_64 -name vm0 -enable-kvm -cpu host -m 2048 -smp 4 -drive file=dpdk-vm0.img -vnc :4 -device pci-assign,host=02:02.0ethtool -N ethx flow-type ip4 dst-ip 2.2.2.2 user-def 0xffffffff00000000 action 2 loc 1
```

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212203553_71886.jpg)

原文链接：https://zhuanlan.zhihu.com/p/384256684

原文作者：零声Github整理库

# 【NO.27】Netmap:一个用于快速数据包I/O的新框架

## 0. 摘要

许多应用程序(路由器、流量监视器、防火墙等)即使在非常快的链路上也需要以线路速率发送和接收数据包。在本文中，我们提出了一种新的框架netmap，它使普通操作系统能够每秒处理通过1..10的数百万个数据包传输速率为Gbit/s的链路，无需定制硬件或更改应用程序。

在构建netmap时，我们确定并成功地减少或消除了三个主要的包处理成本:每个包动态内存分配，通过预分配资源来消除;系统调用开销，在大批量上分摊;还有内存副本，通过在内核和用户空间之间共享缓冲区和元数据来消除，同时仍然保护对设备寄存器和其他内核内存区域的访问。另外，这些技术中的一些在过去已经被使用过。我们提议的优点不仅在于我们的性能超过了大多数以前的工作，而且还在于我们提供了一个与现有操作系统原语紧密集成的体系结构，不依赖于特定的硬件，并且易于使用和维护。

netmap已经在FreeBSD和Linux中实现了多个1 Gbit/s和10 Gbit/s网卡。在我们的原型中，运行在900 MHz的单个核心可以发送或接收14.88 Mpps (10gbit /s链路上的峰值包速率)。这比传统的api快20多倍。使用运行在netmap之上的libpcap仿真库，在用户空间的Click和其他包转发应用程序上也可以实现较大的加速(5倍或更多)。

> 本文为翻译论文，原论文链接：**[《netmap a novel framework for fast packet IO》](https://link.zhihu.com/?target=https%3A//github.com/0voice/computer_expert_paper/blob/main/网络编程那些事儿/《netmap a novel framework for fast packet IO》.pdf)**
> 更多计算机学术论文阅读：**[1000+份计算机paper，卡耐基梅隆大学，芝加哥大学，facebook，google，微软，twitter等大牛一作，持续更新中…](https://link.zhihu.com/?target=https%3A//github.com/0voice/computer_expert_paper)**

## 1. 简介

通用操作系统提供了丰富而灵活的运行环境，其中包括许多包处理和网络监控和测试任务。

这些应用程序所需的高速率原始数据包I/O不是通用操作系统的预期目标。

原始套接字、伯克利包过滤[14](https://linuxcpp.0voice.com/BPF)、AF套接字家族以及相应的api已经被用于构建各种网络监视器、流量生成器和通用路由系统

然而，对于1..10的每秒数百万包(pps)来说，性能是不够的Gbit / s链接。为了寻求更好的性能，一些系统(见第3节)要么完全运行在内核中，要么通过将NIC的数据结构暴露给用户空间应用程序而绕过设备驱动程序和整个网络堆栈。尽管它们可能很有效，但许多这些方法依赖于特定的硬件特性，对硬件提供不受保护的访问，或者与现有的OS原语集成得很差。

本文中提出的netmap框架结合并扩展了过去提出的一些想法，试图解决它们的缺点。除了极大的速度提升，netmap不依赖于特定的硬件1，已经完全集成在FreeBSD和Linux中，并且通过兼容性库支持未经修改的libpcap客户端。

评估我们的框架的一个指标是性能:在我们的实现中，在连接线和用户空间应用程序之间移动一个包的平摊成本不到70个CPU时钟周期，这至少比标准api快一个数量级。换句话说，运行在900 MHz的单个核心可以在10gbit /s链路上实现14.88 Mpps。运行在150mhz的同一核心远远超过1gbit /s链路的容量。

其他同样重要的指标是操作的安全性和易用性。Netmap客户机不可能使系统崩溃，因为设备寄存器和关键内核内存区域没有暴露给客户机，而且他们不能在内核中注入虚假的内存指针(这些通常是基于共享内存的其他方案的漏洞)。同时，netmap使用了非常简单的数据模型，非常适合零拷贝包转发;支持多队列适配器;并使用标准的系统调用(select()/poll())来进行事件通知。所有这些都使得将现有应用程序移植到新的机制和编写有效使用netmap API的新应用程序非常容易。

在本文中，我们将重点介绍netmap的体系结构和特性，以及它的核心性能。在Infocom的一篇相关论文[19]中，我们讨论了一个不同的问题:应用程序如何充分利用快速I/O子系统，比如netmap [19]表明，应用程序本身可能会出现显著的性能瓶颈，尽管在某些情况下，我们可以消除它们并充分利用新的基础设施。

在本文的其余部分，第2节给出了当前网络堆栈体系结构和性能的一些背景。第3节介绍了相关的工作，说明了netmap集成和扩展的一些技术。第4节详细介绍了netmap。性能数据将在第5节中介绍。最后，第6节讨论了未决问题和我们未来的工作计划。

## 2. 背景

人们一直对使用通用硬件和操作系统来运行软件交换机[15]、路由器[6,4,5]、防火墙、流量监视器、入侵检测系统或流量生成器等应用程序感兴趣。在提供方便的开发和运行时环境的同时，这样的操作系统通常不提供高效的机制来以高分组速率访问原始分组数据。本节介绍了通用操作系统中网络栈的组织结构，以及各个阶段的处理成本。

### **2.1 网卡数据结构和操作**

网络适配器(nic)通常通过缓冲区描述符的循环队列(环)管理进出数据包，如图1所示。环中的每个槽都包含缓冲区的长度和物理地址。NIC中的cpu可访问寄存器表示可用于传输或接收的环的部分。

在接收时，传入的数据包被存储在下一个可用的缓冲区(可能分成多个片段)，长度/状态信息被写回槽，以表明新数据的可用性。中断通知CPU这些事件。在传输端，NIC期望OS用要发送的数据填充缓冲区。发送新数据包的请求是通过写入网卡的寄存器发出的，相应地，网卡开始发送标记为TX环可用的数据包。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204253_80779.jpg)

图1:典型网卡的数据结构及其与操作系统数据结构的关系。

在高包速率下，中断处理可能代价昂贵，并可能导致所谓的“接收活锁”[16]，或在特定负载下无法执行任何有用的工作。轮询设备驱动程序[10,16,18]和最近网卡中实现的硬件中断缓解解决了这个问题。

一些高速网卡支持多个发送和接收环。这有助于将负载分散到多个CPU核上，减轻对网卡流量的过滤，并帮助解耦共享相同硬件的虚拟机。

### **2.2 内核和用户api**

操作系统维护NIC数据结构的影子副本。缓冲区被链接到os特定的，设备独立的容器(mbufs[22]或等价结构，如sk buffs和NdisPackets)。这些容器包括关于每个包的大量元数据:大小、源或目标接口、属性和标志，这些属性和标志指示NIC和操作系统应该如何处理缓冲区。

驱动程序/操作系统:设备驱动程序和操作系统之间的软件接口通常假定数据包在两个方向上可以被分割成任意数量的片段;设备驱动程序和主机堆栈都必须准备好处理碎片。同样的API也期望子系统可以保留数据包以进行延迟处理，因此缓冲区和元数据不能在函数调用期间简单地通过引用传递，而是必须复制或引用计数。这种灵活性在运行时带来了巨大的开销。

这些API合同在20-30年前被设计出来的时候可能是合适的，但对于今天的系统来说太昂贵了。通过缓冲区链分配、管理和导航的成本常常超过线性化内容的成本，即使生产者确实生成了分段包(例如TCP在从套接字缓冲区中添加数据头时)。

原始数据包I/O:为用户程序读写原始数据包的标准api需要至少一个内存副本来在内核和用户空间之间移动数据和元数据，每个数据包(或者，在最好的情况下，每个批数据包)需要一个系统调用。典型的方法包括打开套接字或Berkeley Packet Filter[14]设备，并使用send()/recv()或专用的ioctl()函数通过它执行I/O。

### **2.3 案例研究:FreeBSD sendto()**

为了评估处理数据包所花费的时间，我们在FreeBSD2中插入了sendto()系统调用，这样我们就可以在不同的深度强制系统调用的早期返回，并估算在网络堆栈的不同层所花费的时间。图2显示了测试程序在绑定的UDP套接字上循环sendto()时的结果。在表中,“time”是返回点在行中列出的函数开始处时每个包的平均时间;“delta”是相邻行之间的差值，表示在处理链的每个阶段所花费的时间。例如，用户空间代码每次迭代需要8 ns，进入内核需要额外的96 ns，以此类推。

正如我们所看到的，我们发现堆栈中所有级别的几个函数消耗了总执行时间的很大一部分。任何网络I/O(无论是通过TCP或原始套接字，还是BPF写入器)都必须经过几个昂贵的层。当然，我们不能避免系统调用;初始的mbuf构造/数据拷贝是昂贵的，路由和报头设置也是昂贵的，(令人惊讶的是)MAC报头设置。最后，将mbufs和元数据转换成NIC格式需要很长时间。本地优化(例如缓存路由和报头，而不是每次都重新构建它们)可以提供适度的改进，但我们需要在所有层进行彻底的改变，以获得10 Gbit/s接口的线速率所需的10倍加速。

我们在本文中展示的是，如果我们采取这样一种激进的方法，同时仍然通过系统调用对用户提供的数据进行安全检查，并提供兼容libpcap的API，那么我们的速度会有多快。

## 3. 相关(和不相关)工作

在这一点上，介绍一些文献中提出的或在商业系统中使用的技术来提高包处理速度是很有用的。这将有助于理解它们的优点和局限性，并展示我们的框架如何使用它们。

Socket api: Berkeley Packet Filter，简称BPF[14]，是最流行的直接访问数据包数据的系统之一。BPF进入网络设备驱动程序的数据路径，并将每个发送或接收的数据包的副本发送到一个文件描述符，用户空间进程可以从中读取或写入。Linux通过AF包套接字家族有类似的机制。BPF可以与系统之间的常规流量共存，尽管通常BPF客户端将卡置于混杂模式，导致大量流量被发送到主机堆栈(并立即丢弃)。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204253_27314.jpg)

图2:sendto()在最近的FreeBSD HEAD 64位i7-870上的路径和执行时间(2.93 GHz +Turboboost, Intel 10gbit NIC和ixgbe驱动。由单个进程发出sendto()调用完成的度量。值有5%的偏差，并且在多次5秒测试中取平均值。

**包过滤钩子:**Netgraph (FreeBSD)， Netfilter (Linux)和Ndis Miniport驱动(Windows)是在内核中使用的机制，当包复制是不必要的，而应用程序(例如防火墙)必须被插入到包处理链。这些钩子拦截来自/到驱动程序的流量，并将其传递给处理模块，而不需要额外的数据副本。包过滤钩子依赖于基于包表示的标准mbuf/sk buff。

**直接缓冲区访问:**删除内核-用户过渡中涉及的数据副本的一种简单方法是直接在内核中运行应用程序代码。内核模式单击[10]支持这种方式[4]。Click允许通过模块组成轻松构建包处理链，其中一些模块支持对NIC的快速访问(即使它们保留了基于sk buffo的包表示)。

然而，内核环境是非常有限和脆弱的，因此更好的选择是将包缓冲区暴露给用户空间。例如PF RING[2]和Linux PACKET MMAP，它们将包含多个预分配的包缓冲区的共享内存区域导出到用户空间客户端。内核负责在sk buffs和共享缓冲区之间复制数据，因此不需要自定义设备驱动程序。这将系统调用成本分摊到多个包上，但保留了数据拷贝和sk buff管理开销。可能(但我们没有详细的文档)这也是“Windows注册I/O API”(里约热内卢)[20]的工作方式。

更好的性能可以通过在用户空间中运行完整的堆栈(一直到网卡访问)来实现。这需要自定义设备驱动程序，并带来一些风险，因为NIC的DMA引擎可以写入任意内存地址(除非受到IOMMUs等硬件机制的限制)，因此行为不正常的客户机可能会在系统的任何地方垃圾数据。这类例子包括UIO-IXGBE[11]、PF RING-DNA[3]和商业解决方案，包括英特尔的DPDK[8]和SolarFlare的OpenOnload[21]。

Van Jacobson的NetChannels[9]和我们的工作有一些相似的特性，至少在用于加速性能的技术上:删除sk bufs，避免在中断处理程序中进行包处理，以及将缓冲区映射到合适的库实现整个协议处理的用户空间。唯一可用的文档able[9]显示了有趣的加速，尽管后来在Linux中实现相同想法的尝试(参见[13])被认为不令人满意，可能是因为试图保持与现有内核网络体系结构100%兼容引入了额外的约束。PacketShader [5] I/O引擎(PSIOE)是另一个与我们的提议密切相关的，特别是在每个的性能方面。PSIOE使用自定义设备驱动程序，该驱动程序re将基于sk buff的API与一个更简单的API放在一起，使用预分配的缓冲区。自定义ioctl()用于将内核与用户空间应用程序同步，并且mul多个包通过内核和应用程序之间共享的内存ory区域上下传递。内核负责在共享内存和包缓冲区之间复制包数据。与netmap不同，PSIOE只支持一个特定的NIC，不支持port select()/poll()，需要修改应用程序以允许它们使用新的API。

**硬件解决方案:**一些硬件被专门设计来支持高速数据包捕获，或者可能的数据包生成，以及一些特殊功能，如时间戳、过滤、转发。通常这些卡带有定制的设备驱动程序和用户库来访问硬件。例如，DAG[1,7]卡是基于fpga的设备，用于线速率包捕获和精确的时间戳，使用快速板载内存用于捕获缓冲区(在引入它们时，典型的I/O总线无法维持1和10gbit /s的线速率)。NetFPGA[12]是基于fpga的卡的另一个例子，卡的固件可以通过编程直接在NIC中实现特定功能，从CPU中卸载一些工作。

### **3.1 不相关的工作**

在高速网络中，很多商业利益都转向了TCP加速和硬件虚拟化，因此阐明netmap在这方面的地位是很重要的。Netmap是一个框架，用于降低在硬件和主机堆栈之间移动流量的成本。 与TCP加速相关的流行硬件特性，如硬件校验和甚至加密、Tx分段卸载、大型接收卸载，与我们的建议完全正交:它们减少了主机堆栈中的一些处理，但不处理与设备的通信。与虚拟化相关的特性也同样正交，例如支持多个硬件队列，以及将流量分配给特定队列(VMDq)和/或将队列分配给特定虚拟机(VMDc, SR-IOV)的能力。我们希望在虚拟机中运行netmap，尽管探索netmap中使用的思想如何在管理程序中用于帮助虚拟化网络硬件可能是值得的(但不是本文的重点)。

## 4. Netmap

之前的调查显示，大多数相关的建议已经确定并试图消除以下数据包处理中的高成本操作:数据复制、元数据管理和系统调用开销。我们的框架叫做netmap，它是一个系统，可以让用户空间应用程序非常快速地访问网络数据包，在接收端和发送端，包括来自/到主机堆栈的。效率不会以牺牲操作的安全性为代价:潜在的危险行为，如编程的网卡是由操作系统验证，这也加强了内存保护。另外，netmap的一个独特特性是试图设计和实现一个使用简单、与现有操作系统机制紧密集成、不绑定特定设备或硬件特性的API。

Netmap通过以下几种技术实现高性能:

- 一种轻量级元数据表示，它紧凑、易于使用，并且隐藏了特定于设备的特性。此外，该表示支持在每个系统调用中处理大量数据包，从而摊销其成本;
- 线性的、固定大小的包缓冲区在设备打开时预先分配，从而节省每个包分配和释放的成本;
- 通过授予应用程序对数据包缓冲区的直接、受保护的访问权来消除数据复制成本。同样的机制也支持接口之间的零拷贝传输数据包;
- 支持有用的硬件特性(例如多个硬件队列)。

总的来说，我们使用系统的每个部分来完成最适合的任务:网卡用于在网络和内存之间快速移动数据，操作系统用于加强保护并提供同步支持。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212122040232603928.png)

图3:在netmap模式下，网卡环与主机网络堆栈断开连接，并通过netmap API交换数据包。另外两个netmap环让应用程序与主机堆栈通信

在非常高的级别上，当程序请求将接口置于netmap模式时，NIC将部分地从主机协议堆栈断开连接(见图3)。通过在共享内存中实现的缓冲区(netmap环)的循环队列，该程序获得了与NIC和(单独)与主机堆栈交换数据包的能力。传统的OS原语，如select()/poll()用于同步。除了数据路径中的断开连接之外，操作系统并不知道更改，因此它仍然像在常规操作期间一样继续使用和管理接口。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204254_46624.jpg)

图4:netmap导出的共享内存区域的用户视图

### **4.1 数据结构**

netmap体系结构中的关键组件是图4所示的数据结构。它们的设计目的是提供以下特性:1)减少/摊销每个数据包的开销;2)接口间高效转发;3)网卡与主机栈之间的高效通信;4)支持多队列适配器和多核心系统。

netmap通过将三种用户可见的对象关联到每个接口来支持这些特性，如图4所示:包缓冲区、netmap环和netmap if描述符。系统中所有启用netmap的接口的所有对象都驻留在同一个内存区域中，由内核在一个非分页区域中分配，并由所有用户进程共享。使用单个区域可以方便地支持接口之间的零复制转发，但是修改代码使不同的接口或接口组使用单独的内存区域，从而在客户机之间获得更好的隔离是很简单的。

由于共享内存是由不同虚拟地址空间中的进程和内核线程映射的，因此包含在该区域中的任何内存引用都必须使用相对地址，这样指针就可以以位置无关的方式计算。这个问题的解决方案是将引用作为父数据结构和子数据结构之间的偏移量来实现。

包缓冲区有固定的大小(当前实现为2kbytes)，由网卡和用户进程共享。每个缓冲区都由唯一的索引标识，用户进程或内核可以很容易地将其转换为虚拟地址，并将其转换为NIC的DMA引擎所使用的物理地址。当接口进入netmap模式时，所有netmap环的缓冲区都被预先分配，这样在网络I/O期间就不需要分配它们了。描述缓冲区的元数据(索引、数据长度、一些标志)存储在下面描述的netmap环的插槽中。每个缓冲区被一个netmap环和相应的硬件环引用。

netmap环是由NIC实现的与设备无关的循环队列的副本，包括:

- 环的尺寸，环内槽的数量;
- 当前在环中的读或写位置;
- avail，可用缓冲区的数量(在RX环中接收的数据包，在TX环中空槽);
- Buf ofs，环与(固定大小的)包缓冲区数组开始之间的偏移量;
- Slots[]，一个包含环大小条目的数组。每个槽包含相应的数据包缓冲区的索引、数据包的长度和一些用于请求缓冲区上的特殊操作的标志。

最后，一个netmap if包含描述接口的只读信息，例如环的数量和一个数组，该数组包含netmap if和每个关联到接口的netmap环之间的内存偏移量(同样，偏移量用于使寻址位置独立)。

#### **4.1.1 数据所有权和访问规则**

netmap数据结构在内核和用户空间之间共享，但是不同数据区域的所有权定义得很好，因此不存在竞争。特别是，netmap环总是由用户空间应用程序拥有，除非在执行系统调用期间，内核代码仍然在用户进程的上下文中更新它。中断处理程序和其他内核线程永远不会触及netmap环。

当前和当前+ available -1之间的包缓冲区是由用户空间应用程序拥有的，而其余的缓冲区是由内核拥有的(实际上，只有NIC访问这些缓冲区)。这两个区域之间的边界在系统调用期间更新。

### **4.2 netmap API**

程序通过打开专用设备/dev/netmap并发出一个

```
ioctl(.., NIOCREG, arg) 
```

在文件描述符上。参数包含接口名称，并可选地指示我们想通过这个文件描述符控制哪个环(参见第4.2.2节)。成功,该函数返回所有数据结构所在的共享内存区域的大小，如果在该区域内，则返回netmap的偏移量。文件描述符上的后续mmap()使进程的地址空间中可以访问内存。

一旦文件描述符绑定到一个接口及其环上，另外两个ioctl()就支持数据包的传输和接收。特别是，传输要求程序填充TX环中的缓冲区，从槽cur(包长度写入槽的len字段)开始，然后发出一个

```
ioctl(.., NIOCTXSYNC) 
```

告诉操作系统要发送的新数据包。这个系统调用将信息传递给内核，并在返回时更新netmap环中的avail字段，报告由于之前的传输完成而变得可用的插槽。

在接收端，程序应该首先发出一个

```
ioctl(.., NIOCTXSYNC) 
```

询问OS有多少数据包可供读取;然后它们的长度和有效负载就可以通过netmap环中的槽(从cur开始)立即获得。

两个NIOC*SYNC ioctl()都是非阻塞的，不涉及数据复制(除了同步的插槽在netmap和硬件环)，并可以处理多个包一次。这些特性对于将每个包的开销减少到非常小的值非常重要。这些系统调用的内核部分做了以下工作:

- 验证cur/avail字段和涉及的槽的内容(长度和缓冲区索引，在netmap和硬件环);
- 同步netmap和硬件环之间的槽的内容，并向NIC发出命令通告新的数据包发送或新可用的接收缓冲区;
- 更新netmap环中的avail字段。

内核中的工作量很小，所执行的检查确保共享数据结构中用户提供的数据不会导致系统崩溃。

#### **4.2.1** 阻塞原语

阻塞I/O通过select()和poll()系统调用得到支持。Netmap文件描述符可以传递给这些函数，并在可用> 0时报告为就绪(唤醒调用者)。在从select()/poll()返回之前，系统更新环的状态，这与NIOC*SYNC ioctls中的状态相同。这样，在事件循环上旋转的应用程序每次迭代只需要一次系统调用。

#### **4.2.2 多队列接口**

对于具有多个环对的卡片，文件描述符(以及相关的ioctl()和poll())可以配置为两种模式之一，通过NIOCREG ioctl()参数中的环id字段进行选择。在默认模式下，文件描述符控制所有环，导致内核检查其中任何一个环上的可用缓冲区。在替代模式中，文件描述符与单个TX/RX环对相关联。通过这种方式，多个线程/进程可以创建单独的文件描述符，将它们绑定到不同的环对并可在卡上独立操作，不受干扰，不需要同步。将线程绑定到特定的核心只需要一个标准的OS系统调用setaffinity()，而不需要任何新的机制。

#### **4.2.3 使用示例**

下面的示例(第5节中使用的包生成器的核心)展示了netmap API的简单使用。除了一些用于在共享内存区域中导航数据结构的宏之外，netmap客户端不需要任何库来使用系统，并且代码非常紧凑和可读

```
fds.fd = open("/dev/netmap", O_RDWR);strcpy(nmr.nm_name, "ix0");ioctl(fds.fd, NIOCREG, &nmr);p = mmap(0, nmr.memsize, fds.fd);nifp = NETMAP_IF(p, nmr.offset);fds.events = POLLOUT;for (;;) {  poll(fds, 1, -1);  for (r = 0; r < nmr.num_queues; r++) {    ring = NETMAP_TXRING(nifp, r);    while (ring->avail-- > 0) {      i = ring->cur;      buf = NETMAP_BUF(ring, ring->slot[i].buf_index);      ... store the payload into buf ...      ring->slot[i].len = ... // set packet length      ring->cur = NETMAP_NEXT(ring, i);      }   }}
```

### **4.3 与主机堆栈通信**

即使在netmap模式下，操作系统中的网络堆栈仍然负责控制接口(通过ifconfig和其他功能)，并将生成(并期望)到接口/从接口的流量。此流量通过另外一对netmap环来处理，可以通过NIOCREG调用将其绑定到一个netmap文件描述符

其中一个环上的NIOCTXSYNC将缓冲区封装到mbufs中，然后将它们传递给主机堆栈，就像它们来自物理接口一样。相反，来自主机堆栈的数据包将排队进入“主机堆栈”netmap环，并在随后的niocrxsync上对netmap客户端可用。

netmap客户端负责确保数据包在连接到主机堆栈的环和那些连接到NIC的环之间正确地传递。实现这个特性很简单，甚至可以使用第4.5节中所示的零复制技术。这也是实现防火墙、流量整形器和NAT盒等功能的理想机会，这些功能通常附加在包过滤钩子上。

### **4.4 安全考虑**

内核和能够打开/dev/netmap的多个用户进程之间的内存共享提出了这样一个问题:在框架的使用中存在什么安全隐患。

使用netmap的进程，即使行为不正常，也不会导致内核崩溃，这与许多其他高性能数据包I/O系统(例如UIO-IXGBE, PF RING-DNA, in-kernel Click)不同。事实上，共享内存区域并不包含关键的内核内存区域，并且缓冲区索引和长度在使用之前总是由内核进行验证。

一个行为不当的进程可以破坏其他人的网络地图环或数据包缓冲区。解决这个问题的简单方法是为每个环实现一个单独的内存区域，这样客户端就不会干扰了。在硬件多队列的情况下，这很简单，或者可以在软件中简单地模拟，而不需要数据副本。这些解决方案将在今后的工作中进行探讨。

### **4.5 零拷贝数据包转发**

由于在同一个内存区域中有所有接口的所有缓冲区，所以接口之间的零拷贝包转发只需要在入接口的接收槽和出接口的发送槽之间交换缓冲区索引，并相应地更新长度和标志字段:

```
src = &src_nifp->slot[i]; /* locate src and dst slots */dst = &dst_nifp->slot[j];/* swap the buffers */tmp = dst->buf_index;dst->buf_index = src->buf_index;src->buf_index = tmp;/* update length and flags */dst->len = src->len;/* tell kernel to update addresses in the NIC rings */dst->flags = src->flags = BUF_CHANGED;
```

交换器在输出接口上对数据包进行排队，同时用一个空缓冲区填充输入环，而不需要涉及内存分配器。

### **4.6 libpcap兼容性**

如果没有应用程序使用API，那么API就没有什么价值，而部署新API的一个重大障碍是需要对现有代码进行调整以适应它们。

遵循一种解决兼容性问题的通用方法，我们在netmap上写的第一件事是一个小库，它将libpcap调用映射到netmap调用。由于netmap使用了标准的同步原语，这个任务被大大简化了，所以我们只需要将读/写函数(pcap dispatch()/pcap inject())映射到等效的netmap调用中——总共大约20行代码。

4.7 实现

在网络地图的设计和开发中，为了保证系统的可维护性和性能，做了大量的工作。FreeBSD中包含的当前版本包含了大约2000行用于系统调用(ioctl, select/poll)和驱动程序支持的代码。不需要用户空间库:一个小的C头文件(200行)定义了netmap客户端使用的所有结构、原型和宏。我们最近完成了一个Linux版本，它使用相同的代码加上一个小的包装器将某些FreeBSD内核函数映射到它们的Linux对等体中。

保持设备驱动程序修改小(必须,如果我们想要新的硬件上实现API),大多数通用代码实现的功能,并且每个司机只需要实现两个函数NIOC *同步程序的核心,一个戒指在netmap模式初始化函数,和一个函数导出设备驱动锁到公共代码。这将单个驱动程序的更改(主要是机械的)减少到每个大约500行，(一个典型的设备驱动程序有4k ..10k行代码)。netmap目前支持Intel 10gbit /s适配器(ixgbe驱动程序)和各种1gbit /s适配器(Intel、RealTek、nvidia)。

在netmap体系结构中，设备驱动程序在执行系统调用期间，在用户空间进程的上下文中完成它们的大部分工作(归结为同步网卡和netmap环)。这改进了缓存局域性，简化了资源管理(例如将进程绑定到特定的核心)，并使系统更加可控和健壮，因为我们不需要担心在不可中断的上下文中执行太多代码。我们通常修改网卡驱动程序，使中断服务程序除了唤醒任何休眠的进程之外不工作。这意味着中断缓解延迟将直接传递给用户进程。

一些微不足道的优化在性能方面也有巨大的回报。例如，如果使用avail > 0调用系统调用，我们不会回收传输的缓冲区或寻找更多的传入数据包。这有助于不必要地对每个包调用系统调用的应用程序。另外两种优化(即使没有指定POLLOUT，也会输出任何排队等待传输的数据包; 并且在poll()返回之前更新netmap环内的时间戳)将典型事件循环每次迭代中的系统调用数量从3个减少到1个——对于某些应用程序来说，这再次是一个显著的性能增强。

到目前为止，我们还没有尝试过与使用预取指令或数据放置相关的优化来改善缓存行为

## **5. 技术性能分析**

我们将通过分析简单I/O函数的行为来讨论框架的性能，然后研究运行在netmap上的更复杂的应用程序。在给出我们的结果之前，详细定义测试条件是很重要的。

### **5.1 性能标准**

数据包的处理涉及多个子系统:CPU管道，缓存，内存和I/O总线。有趣的应用程序是CPU绑定的，所以我们将关注CPU成本的度量。具体来说，我们将测量在应用程序和网卡之间移动数据包所执行的工作(系统成本)。这正是netmap或其他数据包i /O api负责的任务。我们可以将这些成本分成两部分:

i) 每字节成本是CPU周期从/到网卡的缓冲区移动数据(为读或写数据包)。在某些情况下，该组件可以等于零:例如，netmap将网卡缓冲区导出到应用程序，因此它没有每字节的系统成本。其他API,比如套接字API,征收数据复制到移动交通从/到用户空间,这有一个每字节CPU成本,考虑到内存总线的宽度和CPU和内存总线时钟之间的比例,可以在0.25到2时钟周期/字节的范围。

ii) 每个包的开销有多个来源。CPU至少必须为每个数据包更新一个网卡环插槽。此外，根据软件体系结构的不同，每个包可能需要额外的工作，如内存分配、系统调用、编程NIC的寄存器、更新统计数据等等。在某些情况下，第二个集合中的部分操作可以被删除或分摊到多个包上。

在大多数情况下(netmap当然是这样)，每个包的开销是主要的组成部分，就系统负载而言，最具挑战性的情况是由尽可能小的包遍历链路。因此，我们使用64字节数据包(60+4 CRC)运行我们的大多数测试。

当然，为了运行系统并测量其性能，我们需要运行一些测试代码，但我们希望它尽可能简单，以减少对测量的干扰。我们的初始化应用程序的成本几乎可以忽略不计:包生成器流预生成的包，包接收器只计算传入的包。

### **5.2 测试设备**

我们在配备了2.93 GHz的i7-870 4核CPU(带涡轮增压的3.2 GHz)、1.33 GHz的内存和基于Intel 82599 NIC的双端口10gbit /s卡的系统上运行了我们的大部分实验。本文中报道的数字指的是2012年4月FreeBSD HEAD/amd64中的netmap版本。在两个类似的系统上使用直接连接的卡进行了实验。结果是高度可重复性的(在2%以内或更少)，所以我们不报告表格和图表中的置信区间。

netmap非常高效，因此即使在最大包速率下，它也会使10gbit /s接口饱和，我们需要以较低的时钟速度运行系统，以确定性能限制和代码更改的影响。我们的系统可以从一组离散的值中获得不同频率的时钟。名义上，它们大多数是150 MHz的倍数，但我们不知道时钟速度有多精确，也不知道CPU和内存/总线时钟速度之间的关系。

传输速度(以数据包每秒为单位)已经用一个类似于4.2.3节中的数据包生成器来测量。可以在运行时配置包大小，以及用于发送/接收流量的队列和线程/核的数量。数据包是预先准备好的，这样我们就可以以接近于零的每字节成本运行测试。测试程序围绕poll()循环，每轮最多发送B个包(批处理大小)。在接收端，我们使用类似的程序，只不过这次我们轮询读事件，并且只对包进行计数。

### **5.3 传输速度与时钟速率**

作为第一个实验，我们使用可变时钟速度和核数运行生成器，使用大量的批处理，这样系统调用成本几乎可以忽略不计。通过降低时钟频率，我们可以确定系统受CPU限制的点，并估计每个包所花费的(平摊)周期数。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204255_79445.jpg)

图5:与pktgen (linux上专用的内核内生成器，峰值约为4 Mpps)和netsend相比，64字节包、可变时钟速率和内核数量的Netmap传输性能(FreeBSD用户空间，峰值为1.05 Mpps)。

图5显示了使用1..4核和相同数量的环，64字节数据包。吞吐量与时钟速度相当匹配，在1核的情况下达到接近900 MHz的最大线路速率。这对应于60-65 cycles/packet，这个值与我们的期望是合理的。实际上，在这个特定的测试中，每个包的工作仅限于验证netmap环中插槽的内容并更新NIC环中相应的插槽。缓存未命中的代价(确实存在，特别是在NIC环上)被摊销在所有符合缓存线的描述符中，而其他的代价(比如读/写NIC的寄存器)被摊销在整个批处理中。

一旦系统达到行速率，增加时钟速度会减少总的CPU使用，因为生成器会休眠，直到NIC的中断报告新缓冲区的可用性。这种现象不是线性的，它取决于中断缓解间隔的持续时间。对于一个核心，我们在900 MHz时测量了100%的CPU负载，在1.2 GHz时测量了80%，在全速时测量了55%。

多核的扩展是相当不错的，但数字不是特别有趣，因为在这种类型的实验中没有重要的争用点，而且我们只有少量的操作点(1..4核，150,300,450mhz)到达链路饱和。

仅供参考，图5还报告了两个数据包生成器的最大吞吐量，它们代表了使用标准api可以实现的性能。底部的一行表示netsend，一个运行在原始套接字之上的FreeBSD用户空间应用程序。在最高时钟速度下，netsend峰值为1.05 Mpps。图2详细说明了950 ns/pkt是如何使用的。

图中的另一条线是pktgen, Linux中可用的内核包生成器，在最高时钟速度下可以达到4 Mpps，在1.2 GHz(我们可以在Linux中设置的最低速度)下达到2 Mpps。在这里，我们没有详细描述时间是如何花费的，但是设备驱动程序和应用程序架构的相似性表明，大部分成本都在设备驱动程序本身。

接收的速度和时钟结果与发送的相似。netmap可以在900mhz的1核上实现行速率，至少对于64字节的包大小的倍数。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204255_93413.jpg)

图6:不同包大小的实际发送和接收速度(不包括以太网CRC)。上面的曲线是发送速率，下面的曲线是接收速率。看到第5.4节解释

### **5.4 速度与包大小**

前面的实验使用了最小大小的数据包，就每个数据包开销而言，这是最关键的情况。64字节包与系统中不同路径上的总线宽度匹配得非常好，这有助于提高系统的性能。然后我们检查了数据包大小的变化是否对系统的性能有影响，无论是在发送端还是接收端。

可变数据包大小的传输速度显示了预期的1/size行为，如图6中的上曲线所示。相反，接收端显示了一些令人惊讶的情况，如图6中底部曲线所示。最大速率(与CPU速度无关)仅在包大小为64倍时才能实现(或者足够大，从而使总数据速率较低)。在其他尺寸下，接收性能下降(例如，在英特尔cpu上，在65到127字节之间，平均约7.5 Mpps;在AMD cpu上，这个值略高)。调查表明，网卡和/或I/O桥为不是完整缓存线的写发出读-修改-写周期。改变操作模式，从接收的数据包中移除CRC，将“甜蜜点”移动4字节(即64+4,128+4等实现行率，其他没有)。我们发现，在一些网卡(包括1gbit /s的网卡)中，在某些包大小和发送或接收模式中无法实现线速率。

### **5.5 传输速度与批大小**

批量操作提高了系统的吞吐量，因为它摊平了系统调用和其他潜在昂贵操作的成本，比如访问NIC的寄存器。但并不是所有的应用程序都有这种特权，在某些情况下，它们被迫在每几个包上发出一个系统调用的情况下操作。然后，我们使用不同的批处理大小和最小大小的数据包(64字节，包括以太网CRC)运行了另一组实验，试图确定批处理大小如何影响吞吐量。在这个特定的测试中，我们只使用了一个核心和可变数量的队列。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204256_30346.jpg)

图7:1核2.93 GHz的发射性能64字节数据包，不同的批处理大小

结果如图7所示:吞吐量从约2.45 Mpps (408 ns/pkt)开始，batch size为1，并随着batch size快速增长，达到line rate (14.88 Mpps)， batch为8 packets。没有调用特定于netmap的轮询处理程序(netmap poll()和ixgbe txsync())的标准FreeBSD poll()的开销大约是250 ns，所以如果我们想要达到10gbit /s的线路速率和更快的接口，那么每次调用处理多个数据包是绝对必要的。

### **5.6 报文转发性能**

到目前为止，我们已经测量了在线路和应用程序之间移动数据包的成本。这包括操作系统开销，但不包括任何显著的应用程序成本，以及任何数据接触操作。然后，度量netmap API在被cpu密集型任务使用时的好处是很有趣的。分组转发是分组处理系统的主要应用之一，也是我们的框架的一个很好的测试用例。事实上，它涉及同时接收和传输(因此可能导致内存和总线争用)，可能涉及一些数据包处理，这会消耗CPU周期，并导致管道停滞和缓存冲突。与目前使用的简单应用程序相比，所有这些现象可能会降低使用快速数据包I/O机制的好处。

然后，我们研究了一些包转发应用程序在使用新API时的行为，直接使用或通过第4.6节中描述的libpcap兼容库。测试用例如下:

- netmap-fwd，一个简单的应用程序，使用第4.5节所示的零复制技术在接口之间转发数据包;
- Netmap-fwd + pcap，如上所述，但使用libpcap仿真代替零拷贝代码;
- 这个实验使用了系统的libpcap和netmap，并在netmap上使用了libpcap仿真库;
- click-EtherSwitch，如上所述，但将两个队列替换为一个EtherSwitch元素;
- openvswitch，使用用户空间转发的openvswitch软件，既使用系统的libpcap，也在netmap的顶部;
- bsd-bridge，内核中的FreeBSD桥接，使用基于mbuf的设备驱动程序。

图8报告了测量的性能。所有实验都是在具有两个10gbit /s接口的单个核心上运行的，除了第一种情况，即链路饱和仅为1.733 GHz。

从这个实验中我们可以得出一些有趣的观察结果:

- 本机网络地图转发，无数据触摸操作，容易达到线路速率。这很有趣，因为这意味着即使是单个核也可以实现全速率双向操作;
- libpcap仿真库给前面的情况增加了很大的开销(在完整时钟下为7.5 Mpps，而在1.733 GHz下为14.88 Mpps，这意味着每个包大约有80-100 ns的差异)。我们还没有研究是否/如何改进这一点(例如，使用预取);
- 使用基于netmap的libpcap模拟替换系统的libpcap, OpenvSwitch和Click将加速4到8倍，尽管pcap inject()确实使用了数据副本。这也是一个重要的结果，因为它意味着现实生活中的应用程序可以从我们的API中获益。

### **5.7 讨论**

存在巨大的性能提升那些呈现在图5和图8中,这表明netmap 4 - 40倍类似的应用程序使用标准的api,有人可能会怀疑我)有多公平比较,和2)的贡献是什么各种机制来提高性能。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204257_31149.jpg)

图8:使用各种软件配置转发测试硬件的性能

netmap比使用标准api的类似应用程序快4到40倍，人们可能想知道i)这种比较有多公平，ii)各种机制对性能改进的贡献是什么。

第一个问题的答案是，这种比较确实是公平的。图5中的所有流量生成器都执行完全相同的操作，并且每个生成器都试图以最有效的方式执行，只受其使用的底层api的限制。答案在图8中更加明显，在许多情况下，我们只是在两个不同的libpcap实现之上使用相同的未经修改的二进制文件。

在不同配置下测量的结果还让我们回答第二个问题——评估不同优化对netmap性能的影响。

如第5.6节所示，数据拷贝的开销比较大，但是它们不能阻止显著的加速(比如在libpcap+netmap之上实现7.5 Mpps转发数据包)。

每个包的系统调用当然起着重要的作用，netsend和pktgen之间的差异(尽管在不同的平台上)，或者使用小批量时包生成器的低性能都证明了这一点。

最后，一个关于基于skbuf/mbuf API成本的有趣观察来自pktgen(大约花费250 ns/pkt)和基于netmap的包生成器的比较，后者每个包只花费20-30 ns，用于编程NIC。这两个应用程序本质上的区别只是包缓冲区的管理方式不同，因为系统调用和内存副本的摊销成本在这两种情况下都可以忽略不计。5.8应用程序移植我们最后简要讨论了在将现有应用程序适应netmap时遇到的问题。我们的libpcap模拟库是标准模拟库的一种替代，但是应用程序中的其他性能瓶颈可能会阻止我们提供的更快的I/O子系统的利用。情况就是这样。

### **5.8 应用程序移植**

最后，我们简要讨论了在将现有应用程序适应netmap时遇到的问题。我们的libpcap模拟库是标准模拟库的一种替代，但是应用程序中的其他性能瓶颈可能会阻止我们提供的更快的I/O子系统的利用。这正是我们在两个应用程序中遇到的情况，OpenvSwitch和Click(详细信息在[19]中描述)。

在OpenvSwitch的例子中，原始代码(带有userspace/libpcap转发模块)有一个非常昂贵的事件循环，只能执行少于70 Kpps的任务。用基于netmap的版本替换本机libpcap几乎没有任何可测量的改进。在重新构造事件循环并将系统拆分为两个进程之后，本机性能提高到了780 Kpps，而基于netmap的libpcap进一步将转发性能提高到了近3 Mpps。

在Click的例子中，罪魁祸首是c++分配器，它比管理固定大小的包缓冲区的私有池要昂贵得多。在netmap上运行时，替换内存分配器将转发性能从1.3 Mpps提高到3.95 Mpps，在标准libpcap上运行时从0.40 Mpps提高到0.495 Mpps。点击用户空间现在实际上比内核版本要快，原因是昂贵的设备驱动程序和在2.3节中讨论的sk buff管理开销。

## **6. 结论和未来工作**

我们已经介绍了netmap，这是一个为用户空间应用程序提供一个非常快的通道来与网络适配器交换原始数据包的框架。netmap不依赖于特殊的硬件特性，它的设计对网卡的功能做了非常合理的假设。我们的测量表明，netmap可以给使用低/级数据包I/O(数据包捕获和生成工具，软件路由器，防火墙)的广泛应用程序带来巨大的性能改进。FreeBSD和Linux版本的存在，有限的操作系统依赖，以及libpcap仿真库的可用性，使我们有信心netmap可以成为一个有用的和流行的工具，用于开发低水平，高性能的网络软件。

一个有趣的开放式问题和未来工作的主题是，netmap体系结构的特性如何被主机传输/网络堆栈利用，以及它们如何帮助在虚拟化平台中构建高效的网络支持。

关于这项工作的更多信息，包括源代码和未来发展的更新，可以在该项目的页面[17]上找到。

## 7. 参考文献

[1] The dag project. Tech. rep., University of Waikato, 2001.

[2] DERI, L. Improving passive packet capture: Beyond device polling. In SANE 2004, Amsterdam.

[3] DERI, L. ncap: Wire-speed packet capture and transmission. In Workshop on End-to-End Monitoring Techniques and Services (2005), IEEE, pp. 47–55

[4] DOBRESCU, M., EGI, N., ARGYRAKI, K., CHUN, B., FALL, K., IANNACCONE, G., KNIES, A., MANESH, M., AND RATNASAMY, S. Routebricks: Exploiting parallelism to scale software routers. In ACM SOSP (2009), pp. 15–28.

[5] HAN, S., JANG, K., PARK, K., AND MOON, S. Packetshader: a gpu-accelerated software router. ACM SIGCOMM Computer Communication Review 40, 4 (2010), 195–206.

[6] HANDLEY, M., HODSON, O., AND KOHLER, E. Xorp: An open platform for network research. ACM SIGCOMM Computer Communication Review 33, 1 (2003), 53–57.

[7] HEYDE, A., AND STEWART, L. Using the Endace DAG 3.7 GF card with FreeBSD 7.0. CAIA, Tech. Rep. 080507A, May 2008.[Online]. Available: [http://caia](https://link.zhihu.com/?target=http%3A//caia). swin. edu. au/reports/080507A/CAIA-TR-080507A. pdf (2008).

[8] INTEL. Intel data plane development kit. [http://edc.intel.com/Link.aspx?id=5378](https://link.zhihu.com/?target=http%3A//edc.intel.com/Link.aspx%3Fid%3D5378) (2012).

[9] JACOBSON, V., AND FELDERMAN, B. Speeding up networking. Linux Conference Au, [http://www.lemis.com/grog/Documentation/vj/lca06vj.pdf](https://link.zhihu.com/?target=http%3A//www.lemis.com/grog/Documentation/vj/lca06vj.pdf) .

[10] KOHLER, E., MORRIS, R., CHEN, B., JANNOTTI, J., AND KAASHOEK, M. The click modular router. ACM Transactions on Computer Systems (TOCS) 18, 3 (2000), 263–297.

[11] KRASNYANSKY, M. Uio-ixgbe. Qualcomm, [https://opensource.qualcomm.com/wiki/UIO-IXGBE](https://link.zhihu.com/?target=https%3A//opensource.qualcomm.com/wiki/UIO-IXGBE).

[12] LOCKWOOD, J. W., MCKEOWN, N., WATSON, G., ET AL. Netfpga–an open platform for gigabit-rate network switching and routing. IEEE Conf. on Microelectronics Systems Education (2007).

[13] [http://LWN.NET](https://link.zhihu.com/?target=http%3A//LWN.NET) ARTICLE 192767. Reconsidering network channels. [http://lwn.net/Articles/192767/](https://link.zhihu.com/?target=http%3A//lwn.net/Articles/192767/).

[14] MCCANNE, S., AND JACOBSON, V. The bsd packet filter: A new architecture for user-level packet capture. In USENIX Winter Conference (1993), USENIX Association.

[15] MCKEOWN, N., ANDERSON, T., BALAKRISHNAN, H., PARULKAR, G., PETERSON, L., REXFORD, J., SHENKER, S., AND TURNER, J. Openflow: enabling innovation in campus networks. ACM SIGCOMM Computer Communication Review 38 (March 2008), 69–74.

[16] MOGUL, J., AND RAMAKRISHNAN, K. Eliminating receive livelock in an interrupt-driven kernel. ACM Transactions on Computer Systems (TOCS) 15, 3 (1997), 217–252.

[17] RIZZO, L. Netmap home page. Universit`a di Pisa, [http://info.iet.unipi.it/](https://link.zhihu.com/?target=http%3A//info.iet.unipi.it/)∼luigi/netmap/.

[18] RIZZO, L. Polling versus interrupts in network device drivers. BSDConEurope 2001 (2001).

[19] RIZZO, L., CARBONE, M., AND CATALLI, G. Transparent acceleration of software packet forwarding using netmap. INFOCOM’12, Orlando, FL, March 2012, [http://info.iet.unipi.it/](https://link.zhihu.com/?target=http%3A//info.iet.unipi.it/)∼luigi/netmap/.

[20] SCHUTZ, B., BRIGGS, E., AND ERTUGAY, O. New techniques to develop low-latency network apps. [http://channel9.msdn.com/Events/BUILD/BUILD2011/SAC593T](https://link.zhihu.com/?target=http%3A//channel9.msdn.com/Events/BUILD/BUILD2011/SAC593T).

[21] SOLARFLARE. Openonload. [http://www.openonload.org/](https://link.zhihu.com/?target=http%3A//www.openonload.org/) (2008).

[22] STEVENS, W., AND WRIGHT, G. TCP/IP illustrated (vol. 2): the implementation. Addison-Wesley Longman Publishing Co., Inc. Boston, MA, USA, 1995

原文链接：https://zhuanlan.zhihu.com/p/391083793
原文作者：零声Github整理库

# 【NO.28】【涨知识】腾讯、京东、爱奇艺都在用DPDK，看看用它做了什么？

## 1. 腾讯 F-Stack

### 1.1F-Stack 开发背景

随着网卡性能的飞速发展，10GE 网卡已经大规模普及，25GE/40GE/100GE 网卡也在逐步推广，linux 内核在网络数据包处理上的瓶颈也越发明显，在传统的内核协议栈中，网卡通过硬件中断通知协议栈有新的数据包到达，内核的网卡驱动程序负责处理这个硬件中断，将数据包从网卡队列拷贝到内核开辟的缓冲区中（DMA），然后数据包经过一系列的协议处理流程，最后送到用户程序指定的缓冲区中。在这个过程中中断处理、内存拷贝、系统调用（锁、软中断、上下文切换）等严重影响了网络数据包的处理能力。操作系统的对应用程序和数据包处理的调度可能跨 CPU 调度，局部性失效进一步影响网络性能。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212122048224695382.png)

而互联网的快速发展亟需高性能的网络处理能力，kernel bypass 方案也越来被人所接受，市场上也出现了多种类似技术，如 DPDK、NETMAP、PF_RING 等，其核心思想就是内核只用来处理控制流，所有数据流相关操作都在用户态进行处理，从而规避内核的包拷贝、线程调度、系统调用、中断等性能瓶颈，并辅以各种性能调优手段，从而达到更高的性能。其中 DPDK 因为更彻底的脱离内核调度以及活跃的社区支持从而得到了更广泛的使用。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204856_34609.jpg)

### **1.2 F-Stack 开发框架**

F-Stack 是一款兼顾高性能、易用性和通用性的网络开发框架，传统上 DPDK 大多用于 SDN、NFV、DNS 等简单的应用场景下，对于复杂的 TCP 协议栈上的七层应用很少，市面上已出现了部分用户态协议栈，如 mTCP、Mirage、lwIP、NUSE 等，也有用户态的编程框架，如 SeaStar 等，但统一的特点是应用程序接入门槛较高，不易于使用。

F-Stack 使用纯 C 实现，充当胶水粘合了 DPDK、FreeBSD 用户态协议栈、Posix API、微线程框架和上层应用（Nginx、Redis），使绝大部分的网络应用可以通过直接修改配置或替换系统的网络接口即可接入 F-Stack，从而获得更高的网络性能。

### **1.3 F-Stack 架构**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204857_84534.jpg)

F-Stack 总体架构如上图所示，具有以下特点:

- 使用多进程无共享架构。
- 各进程绑定独立的网卡队列和 CPU，请求通过设置网卡 RSS 散落到各进程进行处理。
- 各进程拥有独立的协议栈、PCB 表等资源。
- 每个 NUMA 节点使用独立的内存池。
- 进程间通信通过无锁环形队列（rte_ring）进行。
- 使用 DPDK 作为网络 I/O 模块，将数据包从网卡直接接收到用户态。
- 移植 FreeBSD Release 11.0.1 协议栈到用户态并与 DPDK 对接。

## 2. 京东 - SKYLB

随着京东业务的高速增长，作为应用入口的负载均衡，大流量大并发带来的挑战越来越严峻。本文主要介绍了京东商城设计和实践的一套高可靠，高性能的负载均衡器，名为SKYLB。是一个使用intel DPDK报文转发库，实现运行在通用X86服务器上自研的分布式负载均衡服务。配合网络路由器的多重等价路由协议（OSPF）或者边际网关协议（BGP），组成承担京东数据中心核心四层负载均衡的集群。最大限度的发挥普通X86服务器硬件资源的性能，实现一套适合于京东商城业务的低成本，分布式，高性能，可扩展的智能负载均衡系统。

负载均衡器一般介于网络上的路由器与后端服务器之间，负责将每个数据包通过一定的服务匹配，将其转发到后端服务器节点。充分考虑到京东商城数据中心全容器及全三层BGP组网的模型。以及基于DPDK的几乎达到网卡限速的性能，我们在设计负载均衡时，仅考虑实现了FULLNAT模式，即出向和入向的流量均通过负载均衡器，基本数据流程图如下图1所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204857_48899.jpg)

一般根据业务及流量的规模的不同阶段来选择使用不同的负载均衡，通常我们在负载均衡的选择上大致有以下两个方向：

- 硬件负载均衡，如F5。CitrixNetscaler等；
- 软件负载均衡，如基于LVS，Haproxy，Nginx等开源软件来实现的负载均衡。

### **2.1 对于上述两种负载均衡的选择，各有优缺点，如下：**

1） 硬件负载均衡

可扩展性受限，无法跟上业务流量增长的需求。以及如618、双十一大促等瞬间流量高峰。

虽然可以成对部署避免单点故障，但是一般只能提供1+1冗余。

缺乏互联网快速迭代的灵活性，升级成本昂贵。

一般只能根据网络的情况来设定负载均衡，无关乎实际系统和硬件的情况。

成本较高。

2） 基于开源软件的负载均衡

可以根据实际系统和应用的状态来合理的负载，迭代、扩容和部署相对方便。

单台负载均衡性能相对较差，需要用集群来支撑负载均衡的整体性能。

性价比较低。

### **2.2 项目目标**

1. 设计实现一套高可靠、高性能、易维护及性价比高的L4负载均衡系统,。
2. 基于通用X86_64服务器框架，以及支持DPDK网卡硬件，易开发和移植。
3. 方便部署、删除和维护，集成到京东软件定义数据中心（JDOS2.0）系统，作为京东下一代软件定义数据中心的基础组件。
4. 负载均衡下的服务器基于系统应用负载流量均摊，负载均衡器提供N+1 冗余，借助OSPF/BGP控制负载均衡器的流量负载。
5. 基于系统应用级别的探活，自动故障检测及流量快速恢复。

SKYLB一种基于DPDK平台实现的快速可靠的软件网络负载均衡系统。不仅可以快速的横向扩展，还可以最大限度的提升负载均衡单个NIC的处理转发速度，来实现L4的负载均衡。借助DPDK的优势，如便利的多核编程框架、巨页内存管理、无锁队列、无中断poll-mode 网卡驱动、CPU亲和性等等来实现快速的网卡收发及处理报文，后续考虑TCP/IP 用户态协议实现和优化，进而实现L7负载均衡。

### **2.3 系统概览**

- 工作场景

SKYLB部署在京东容器集群JDOS的前端，对于一个应用集群，发布一个或多个VIP到SKYLB服务上，当客户端需要访问应用节资源URL，首先通过域名访问JD智能分布式DNS服务（SKYDNS详见[https://github.com/ipdcode/skydns](https://link.zhihu.com/?target=https%3A//github.com/ipdcode/skydns)）, SkyDns会智能返回当前最近且状态正常且负载正常的VIP服务的IP,客户端就会向VIP去请求连接。

SKYLB节点上运行了一个路由发布服务agent，我们使用该agent与开启OSPF/BGP的路由器做路由交互，当SKYLB的上层路由器接收到请求VIP的数据包时，路由器通过（OSPF/BGP）的等价多路径转发协议选择一个可以使用的发布该VIP的SKYLB节点，将报文转发给一个SKYLB节点。通过这种策略来实现VIP的发布和横向容量负载能力扩展。

当报文通过上述步骤到达SKYLB负载均衡后，通过常用的负载均衡策略(round robin，一致性hash ，最小连接数)，将数据包送到相应的后端容器服务。

- 系统架构

JingdongDatacenter Operating System(JDOS) 是基于JDOS提供物理机/虚拟机/容器的统一管理系统、配备灵活的网络互连、可靠的分布式共享存储，实现软件定义数据中心的计算资源统一管理和集群管理。通过使用JDOS，可以直接迅速得到任意需要的计算、存储、网络、安全等方面的资源和能力。SKYLB作为整个JDOS系统的一个重要组成部分，负责提供集群的L4负载均衡能力，通过restful API等接口与JDOS系统交互。用户可以通过统一调度管理平台便捷的创建、删除、迁移负载均衡系统。同时多个应用服务进行流量分发的负载均衡服务，从而扩展应用系统对外的服务能力，并且通过消除单点故障提高应用系统的可用性。

系统的基本架构如下图2所示，每一个集群配备一组可容灾的负载均衡控制中心,主要通过restful api接口负责与JDOS调度中心交互，接收vip的配置请求。同时我们在每一个SKYLB的节点上运行一个代理进程，该代理进程通过gRPC与控制中心连接。接收控制中心下达的创建及删除vip,后端server endpoint服务等一系列指令，通过load balancer 提供的命令行执行相应的指令。接收load balancer 关于流量及报文的监控信息，对于流量及监控进行告警，并且通知控制中心和调度中心进行扩容等操作。

代理进程同时还负责后端服务 server endpoint基于服务可用性的健康检查，及时根据后端服务的状态通过命令行进行添加和删除的操作。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204858_18522.jpg)

- 优势

1）扩展性

支持动态添加和删除后端服务的容器，实现无缝的伸缩；在伸，缩过程中，对相关调用和访问者无影响。

2）高可用性

提供多活负载均衡，有多个VIP，它们对应一个域名，自研DNS服务SKYDNS会根据请求的客户端IP智能解析可用的VIP，返回给用户，从而实现更高的可用性；即使一个VIP不可用，也不会影响业务系统对外提供服务。同时借助OSPF/BGP等协议实现负载均衡的横向扩充

3）服务能力自动可调

SKYLB根据VIP实际接收流量的负载需要调整负载均衡的服务能力，比如流量、连接数的控制等指标。

- 功能特点

1）协议支持

负载均衡支持包含TCP、UDP协议的四层负载均衡，配备健全的链路跟踪机制，以及多种调度策略，用户可以根据服务的需要创建合适自己的负载均衡。

2）高可用性

支持容器的健康检查，除传统的IP+Port，并加入对URL检查，保证应用可用性： 健康检查频率可自定义；一旦探测到异常，则不会将流量再分配到这些异常实例，保证应用可用性。

3）集群部署，多层次容错机制： 负载均衡采用集群部署，支持热升级，机器故障和集群维护对用户完全透明，结合DNS使用还可支持全局负载均衡。

4）灵活性

支持多种流量调度算法，使得流量分配更均匀： 负载均衡支持加权轮询和最小连接数这两种调度算法，可根据自身需求选择相应的算法来分配用户访问流量，并支持设置后端容器权重，使得流量调度更均匀，提升负载均衡能力。

支持会话保持，满足用户个性化需求： 负载均衡通过IP地址实现会话保持，可将一定时间内来自同一用户的访问请求，转发到同一个后端容器上进行处理，从而实现用户访问的连续性。

5）易用性

提供多种管理途径，轻松操纵负载均衡： 用户可通过控制台轻松实现负载均衡器的配置、释放等功能。后续会开放标准的API或SDK提供给用户，从而自己开发对负载均衡的控制管理。

## 3. 爱奇艺 - DPVS

DPVS 是 iQiYi 采用 DPDK 技术开发的的高性能四层负载均衡器。与 Linux 内核的 LVS （Linux Virtual Server）相比，DPVS 具有如下特点：

- 更高的性能：DPVS 的包处理速度，1 个工作线程可以达到 2.3 Mpps，6 个工作线程可以达到万兆网卡小包的转发线速（约 12Mpps)。这是主要因为 DPVS 绕过了内核复杂的协议栈，并采用轮询的方式收发数据包，避免了锁、内核中断、上下文切换、内核态和用户态数据拷贝产生的性能开销。
- 更完善的功能：从转发转发模式看，DPVS 支持 Direct Routing（DR）、NAT、Tunnel、Full-NAT、SNAT 五种转发模式，可以灵活适配各种网络应用场景；从协议支持上看，DPVS 支持 IPv4 和 IPv6 协议、且最新版本增加了 NAT64 的转发功能，实现了用户从 IPv6 网络访问 IPv4 服务。
- 更好的维护性：DPVS 是一个用户态程序，与内核功能相比，功能开发周期更短、调试更方便、问题修复更及时。

### **3.1 DPVS 基本原理**

DPVS 的总体架构如下图所示，下面对相关的几个点着重解释说明一下。

- **Master/Worker 模型**

DPVS采用经典的 Master/Worker 模型。Master 处理控制平面，比如参数配置、统计获取等；Worker 实现核心负载均衡、调度、数据转发功能。另外，DPVS 使用多线程模型，每个线程绑定到一个 CPU 物理核心上，并且禁止这些 CPU 被调度。这些 CPU 只运行 DPVS 的 Master 或者某个 Worker，以此避免上下文切换，别的进程不会被调度到这些 CPU，Worker 也不会迁移到其他 CPU 造成缓存失效。

- **网卡队列 /CPU 绑定**

现代的网卡支持多个队列，队列可以和 CPU 绑定，让不同的 CPU 处理不同的网卡队列的流量，分摊工作量，实现并行处理和线性扩展。DPVS是由各个 Worker 使用 DPDK 的 API 处理不同的网卡队列，每个 Worker 处理某网卡的一个接收队列，一个发送队列，实现了处理能力随CPU核心、网卡队列数的增加而线性增长。

- **关键数据 per-cpu及无锁化**

内核性能问题的一大原因就是资源共享和锁。所以，被频繁访问的关键数据需要尽可能的实现无锁化，其中一个方法是将数据做到 per-cpu 化，每个 CPU 只处理自己本地的数据，不需要访问其他 CPU 的数据，这样就可以避免加锁。就 DPVS 而言，连接表，邻居表，路由表等，都是频繁修改或者频繁查找的数据，都做到了 per-cpu 化。
在具体 per-cpu 的实现上，连接表和邻居表、路由表并不相同。对于连接表，高并发的情况下，不光是查找，还会被频繁地添加、删除。我们让每个 CPU 维护的是不相同的连接表，不同的网络数据流（TCP/UDP/ICMP）按照 N 元组被定向到不同的 CPU，在此特定 CPU 上创建、查找、转发、销毁。同一个数据流的包，只会出现在某个 CPU 上，不会落到其他的 CPU 上。这样就可以做到不同的 CPU 只维护自己本地的表，无需加锁。另一方面，对于邻居和路由表，这种系统“全局”的数据，每个 CPU 都是要用到它们的。如果不采用”全局表+锁保护“的方式，而要做成 per-cpu，也需要让每个 CPU 有同样的视图，也就是每个 CPU 需要维护同样的表。对于这两个表，采用了跨 CPU 无锁同步的方式，虽然在具体实现上有小的差别，本质上都是通过跨 CPU 通信（路由是直接传递信息，邻居是克隆数数据并传递分组给别的 CPU），将表的变化同步到每个 CPU。不论用了什么方法，关键数据做到了 per-cpu 之后没有了锁的需求，性能也就能提升了。

- **用户态轻量级协议栈**

四层负载均衡并不需要完整的协议栈，但还是需要基本的网络组件，以便完成和周围设备的交互（ARP/NS/NA）、确定分组走向 （Route）、回应 Ping 请求、健全性检查（分组完整性，Checksum校验）、以及 IP 地址管理等基本工作。使用 DPDK 提高了收发包性能，但也绕过了内核协议栈，DPVS 依赖的协议栈需要自己实现。

- **跨 CPU 无锁消息**

之前已经提到过这点了。首先，虽然采用了关键数据 per-cpu等优化，但跨 CPU 还是需要通信的，比如:

- - Master 获取各个 Worker 的各种统计信息
  - Master 将路由、黑名单等配置同步到各个 Worker
  - Master 将来自 KNI 的数据发送到 Worker（只有 Worker 能操作 DPDK 接口发送数据）

- 既然需要通信，就不能存在互相影响、相互等待的情况，因为那会影响性能。为此，我们使用了 DPDK 提供的无锁 rte_ring 库，从底层保证通信是无锁的，并且我们在此之上封装一层消息机制来支持一对一，一对多，同步或异步的消息。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204859_92048.jpg)

下图给出了 DPVS 详细的功能模块，主要包含如下五大部分：

- 网络设备层
  负责数据包收发和设备管理，支持 vlan、bonding、tunnel 等设备，支持 KNI 通信、流量控制。

- 轻量级协议栈层
  轻量级的 IPv4 和 IPv6 三层协议栈，包括邻居、路由、地址管理等功能。

- IPVS 转发层
  五种数据转发模式的连接管理、业务管理、调度算法、转发处理等。特别地，Full-NAT 转发模式下支持了 IPv6-to-IPv4（NAT64） 转发、 SYN flood 攻击防御、 TCP/UDP 的源地址获取（toa/uoa）等功能。

- 基础模块
  包含定时器、CPU 消息、进程通信接口、配置文件等基础功能模块。

- 控制面和工具
  用于配置和管理 DPVS 服务的工具，包括 ipvsadm、keepalived、dpip，也支持使用进行 quagga 集群化部署。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212204859_33031.jpg)

原文链接：https://zhuanlan.zhihu.com/p/406806789

原文作者：零声Github整理库

# 【NO.29】一文让你看懂内存与CPU之间的关系

**内存：你跑慢点行不行？CPU：跑慢点你养我吗？内存：我不管！**

## **0. 本文的思维导图**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212122055112619937.png)

主存(RAM) 是一件非常重要的资源，必须要认真对待内存。虽然目前大多数内存的增长速度要比 IBM 7094 要快的多，但是，程序大小的增长要比内存的增长还快很多。不管存储器有多大，程序大小的增长速度比内存容量的增长速度要快的多。下面我们就来探讨一下操作系统是如何创建内存并管理他们的。

经过多年的研究发现，科学家提出了一种 分层存储器体系(memory hierarchy)，下面是分层体系的分类：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205629_87679.jpg)

位于顶层的存储器速度最快，但是相对容量最小，成本非常高。层级结构向下，其访问速度会变慢，但是容量会变大，相对造价也就越便宜（所以个人感觉相对存储容量来说，访问速度是更重要的）。

操作系统中管理内存层次结构的部分称为内存管理器(memory manager)，它的主要工作是有效的管理内存，记录哪些内存是正在使用的，在进程需要时分配内存以及在进程完成时回收内存。所有现代操作系统都提供内存管理。

下面我们会对不同的内存管理模型进行探讨，从简单到复杂，由于最低级别的缓存是由硬件进行管理的，所以我们主要探讨主存模型和如何对主存进行管理。

## **1. 无存储器抽象**

最简单的存储器抽象是无存储器。早期大型计算机（20 世纪 60 年代之前），小型计算机（20 世纪 70 年代之前）和个人计算机（20 世纪 80 年代之前）都没有存储器抽象。每一个程序都直接访问物理内存。当一个程序执行如下命令：

```
MOV REGISTER1, 1000
```

计算机会把位置为 1000 的物理内存中的内容移到 REGISTER1 中。因此呈现给程序员的内存模型就是物理内存，内存地址从 0 开始到内存地址的最大值中，每个地址中都会包含一个 8 位位数的内存单元。

所以这种情况下的计算机不可能会有两个应用程序同时在内存中。如果第一个程序向内存地址 2000 的这个位置写入了一个值，那么此值将会替换第二个程序 2000 位置上的值，所以，同时运行两个应用程序是行不通的，两个程序会立刻崩溃。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205629_53980.jpg)

不过即使存储器模型就是物理内存，还是存在一些可变体的。下面展示了三种变体：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205630_94184.jpg)

在上图 a 中，操作系统位于 RAM(Random Access Memory) 的底部，或像是图 b 一样位于 ROM(Read-Only Memory) 顶部；而在图 c 中，设备驱动程序位于顶端的 ROM 中，而操作系统位于底部的 RAM 中。图 a 的模型以前用在大型机和小型机上，但现在已经很少使用了；图 b 中的模型一般用于掌上电脑或者是嵌入式系统中。第三种模型就应用在早期个人计算机中了。ROM 系统中的一部分成为 BIOS (Basic Input Output System)。模型 a 和 c 的缺点是用户程序中的错误可能会破坏操作系统，可能会导致灾难性的后果。

按照这种方式组织系统时，通常同一个时刻只能有一个进程正在运行。一旦用户键入了一个命令，操作系统就把需要的程序从磁盘复制到内存中并执行；当进程运行结束后，操作系统在用户终端显示提示符并等待新的命令。收到新的命令后，它把新的程序装入内存，覆盖前一个程序。

在没有存储器抽象的系统中实现并行性一种方式是使用多线程来编程。由于同一进程中的多线程内部共享同一内存映像，那么实现并行也就不是问题了。但是这种方式却并没有被广泛采纳，因为人们通常希望能够在同一时间内运行没有关联的程序，而这正是线程抽象所不能提供的。

### **1.1 运行多个程序**

但是，即便没有存储器抽象，同时运行多个程序也是有可能的。操作系统只需要把当前内存中所有内容保存到磁盘文件中，然后再把程序读入内存即可。只要某一时刻内存只有一个程序在运行，就不会有冲突的情况发生。

在额外特殊硬件的帮助下，即使没有交换功能，也可以并行的运行多个程序。IBM 360 的早期模型就是这样解决的。

System/360是 IBM 在1964年4月7日，推出的划时代的大型电脑，这一系列是世界上首个指令集可兼容计算机。

在 IBM 360 中，内存被划分为 2KB 的区域块，每块区域被分配一个 4 位的保护键，保护键存储在 CPU 的特殊寄存器(SFR)中。一个内存为 1 MB 的机器只需要 512 个这样的 4 位寄存器，容量总共为 256 字节 (这个会算吧) PSW(Program Status Word, 程序状态字)中有一个 4 位码。一个运行中的进程如果访问键与其 PSW 中保存的码不同，360 硬件会捕获这种情况。因为只有操作系统可以修改保护键，这样就可以防止进程之间、用户进程和操作系统之间的干扰。

这种解决方式是有一个缺陷。如下所示，假设有两个程序，每个大小各为 16 KB。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205630_92996.jpg)

从图上可以看出，这是两个不同的 16KB 程序的装载过程，a 程序首先会跳转到地址 24，那里是一条 MOV 指令，然而 b 程序会首先跳转到地址 28，地址 28 是一条 CMP 指令。这是两个程序被先后加载到内存中的情况，假如这两个程序被同时加载到内存中并且从 0 地址处开始执行，内存的状态就如上面 c 图所示，程序装载完成开始运行，第一个程序首先从 0 地址处开始运行，执行 JMP 24 指令，然后依次执行后面的指令（许多指令没有画出），一段时间后第一个程序执行完毕，然后开始执行第二个程序。第二个程序的第一条指令是 28，这条指令会使程序跳转到第一个程序的 ADD 处，而不是事先设定好的跳转指令 CMP，由于这种不正确访问，可能会造成程序崩溃。

上面两个程序的执行过程中有一个核心问题，那就是都引用了绝对物理地址，这不是我们想要看到的。我们想要的是每一个程序都会引用一个私有的本地地址。IBM 360 在第二个程序装载到内存中的时候会使用一种称为 静态重定位(static relocation)的技术来修改它。它的工作流程如下：当一个程序被加载到 16384 地址时，常数 16384 被加到每一个程序地址上（所以 JMP 28会变为JMP 16412 ）。虽然这个机制在不出错误的情况下是可行的，但这不是一种通用的解决办法，同时会减慢装载速度。更近一步来讲，它需要所有可执行程序中的额外信息，以指示哪些包含（可重定位）地址，哪些不包含（可重定位）地址。毕竟，上图 b 中的 JMP 28 可以被重定向（被修改），而类似 MOV REGISTER1,28 会把数字 28 移到 REGISTER 中则不会重定向。所以，装载器(loader)需要一定的能力来辨别地址和常数。

## **2. 一种存储器抽象：地址空间**

把物理内存暴露给进程会有几个主要的缺点：第一个问题是，如果用户程序可以寻址内存的每个字节，它们就可以很容易的破坏操作系统，从而使系统停止运行（除非使用 IBM 360 那种 lock-and-key 模式或者特殊的硬件进行保护）。即使在只有一个用户进程运行的情况下，这个问题也存在。

第二点是，这种模型想要运行多个程序是很困难的（如果只有一个 CPU 那就是顺序执行）。在个人计算机上，一般会打开很多应用程序，比如输入法、电子邮件、浏览器，这些进程在不同时刻会有一个进程正在运行，其他应用程序可以通过鼠标来唤醒。在系统中没有物理内存的情况下很难实现。

### **2.1 地址空间的概念**

如果要使多个应用程序同时运行在内存中，必须要解决两个问题：保护和 重定位。我们来看 IBM 360 是如何解决的：第一种解决方式是用保护密钥标记内存块，并将执行过程的密钥与提取的每个存储字的密钥进行比较。这种方式只能解决第一种问题（破坏操作系统），但是不能解决多进程在内存中同时运行的问题。

还有一种更好的方式是创造一个存储器抽象：地址空间(the address space)。就像进程的概念创建了一种抽象的 CPU 来运行程序，地址空间也创建了一种抽象内存供程序使用。地址空间是进程可以用来寻址内存的地址集。每个进程都有它自己的地址空间，独立于其他进程的地址空间，但是某些进程会希望可以共享地址空间。

### **2.2 基址寄存器和变址寄存器**

最简单的办法是使用动态重定位(dynamic relocation)技术，它就是通过一种简单的方式将每个进程的地址空间映射到物理内存的不同区域。从 CDC 6600(世界上最早的超级计算机)到 Intel 8088(原始 IBM PC 的核心)所使用的经典办法是给每个 CPU 配置两个特殊硬件寄存器，通常叫做基址寄存器(basic register)和变址寄存器(limit register)。当使用基址寄存器和变址寄存器时，程序会装载到内存中的连续位置并且在装载期间无需重定位。当一个进程运行时，程序的起始物理地址装载到基址寄存器中，程序的长度则装载到变址寄存器中。在上图 c 中，当一个程序运行时，装载到这些硬件寄存器中的基址和变址寄存器的值分别是 0 和 16384。当第二个程序运行时，这些值分别是 16384 和 32768。如果第三个 16 KB 的程序直接装载到第二个程序的地址之上并且运行，这时基址寄存器和变址寄存器的值会是 32768 和 16384。那么我们可以总结下

- 基址寄存器：存储数据内存的起始位置
- 变址寄存器：存储应用程序的长度。

每当进程引用内存以获取指令或读取、写入数据时，CPU 都会自动将基址值添加到进程生成的地址中，然后再将其发送到内存总线上。同时，它检查程序提供的地址是否大于或等于变址寄存器 中的值。如果程序提供的地址要超过变址寄存器的范围，那么会产生错误并中止访问。这样，对上图 c 中执行 JMP 28 这条指令后，硬件会把它解释为 JMP 16412，所以程序能够跳到 CMP 指令，过程如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205631_91366.jpg)

使用基址寄存器和变址寄存器是给每个进程提供私有地址空间的一种非常好的方法，因为每个内存地址在送到内存之前，都会先加上基址寄存器的内容。在很多实际系统中，对基址寄存器和变址寄存器都会以一定的方式加以保护，使得只有操作系统可以修改它们。在 CDC 6600 中就提供了对这些寄存器的保护，但在 Intel 8088 中则没有，甚至没有变址寄存器。但是，Intel 8088 提供了许多基址寄存器，使程序的代码和数据可以被独立的重定位，但是对于超出范围的内存引用没有提供保护。

所以你可以知道使用基址寄存器和变址寄存器的缺点，在每次访问内存时，都会进行 ADD 和 CMP 运算。CMP 指令可以执行的很快，但是加法就会相对慢一些，除非使用特殊的加法电路，否则加法因进位传播时间而变慢。

### **2.3 交换技术**

如果计算机的物理内存足够大来容纳所有的进程，那么之前提及的方案或多或少是可行的。但是实际上，所有进程需要的 RAM 总容量要远远高于内存的容量。在 Windows、OS X、或者 Linux 系统中，在计算机完成启动（Boot）后，大约有 50 - 100 个进程随之启动。例如，当一个 Windows 应用程序被安装后，它通常会发出命令，以便在后续系统启动时，将启动一个进程，这个进程除了检查应用程序的更新外不做任何操作。一个简单的应用程序可能会占用 5 - 10MB 的内存。其他后台进程会检查电子邮件、网络连接以及许多其他诸如此类的任务。这一切都会发生在第一个用户启动之前。如今，像是 Photoshop 这样的重要用户应用程序仅仅需要 500 MB 来启动，但是一旦它们开始处理数据就需要许多 GB 来处理。从结果上来看，将所有进程始终保持在内存中需要大量内存，如果内存不足，则无法完成。

所以针对上面内存不足的问题，提出了两种处理方式：最简单的一种方式就是交换(swapping)技术，即把一个进程完整的调入内存，然后再内存中运行一段时间，再把它放回磁盘。空闲进程会存储在磁盘中，所以这些进程在没有运行时不会占用太多内存。另外一种策略叫做虚拟内存(virtual memory)，虚拟内存技术能够允许应用程序部分的运行在内存中。下面我们首先先探讨一下交换

### **2.4 交换过程**

下面是一个交换过程：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205632_10948.jpg)

刚开始的时候，只有进程 A 在内存中，然后从创建进程 B 和进程 C 或者从磁盘中把它们换入内存，然后在图 d 中，A 被换出内存到磁盘中，最后 A 重新进来。因为图 g 中的进程 A 现在到了不同的位置，所以在装载过程中需要被重新定位，或者在交换程序时通过软件来执行；或者在程序执行期间通过硬件来重定位。基址寄存器和变址寄存器就适用于这种情况。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205632_63876.jpg)

交换在内存创建了多个 空闲区(hole)，内存会把所有的空闲区尽可能向下移动合并成为一个大的空闲区。这项技术称为内存紧缩(memory compaction)。但是这项技术通常不会使用，因为这项技术回消耗很多 CPU 时间。例如，在一个 16GB 内存的机器上每 8ns 复制 8 字节，它紧缩全部的内存大约要花费 16s。

有一个值得注意的问题是，当进程被创建或者换入内存时应该为它分配多大的内存。如果进程被创建后它的大小是固定的并且不再改变，那么分配策略就比较简单：操作系统会准确的按其需要的大小进行分配。

但是如果进程的 data segment 能够自动增长，例如，通过动态分配堆中的内存，肯定会出现问题。这里还是再提一下什么是 data segment 吧。从逻辑层面操作系统把数据分成不同的段(不同的区域)来存储：

- 代码段（codesegment/textsegment）：

又称文本段，用来存放指令，运行代码的一块内存空间

此空间大小在代码运行前就已经确定

内存空间一般属于只读，某些架构的代码也允许可写

在代码段中，也有可能包含一些只读的常数变量，例如字符串常量等。

- 数据段（datasegment）：

可读可写

存储初始化的全局变量和初始化的 static 变量

数据段中数据的生存期是随程序持续性（随进程持续性） 随进程持续性：进程创建就存在，进程死亡就消失

- bss段（bsssegment）：

可读可写

存储未初始化的全局变量和未初始化的 static 变量

bss 段中数据的生存期随进程持续性

bss 段中的数据一般默认为0

- rodata段：

只读数据 比如 printf 语句中的格式字符串和开关语句的跳转表。也就是常量区。例如，全局作用域中的 const int ival = 10，ival 存放在 .rodata 段；再如，函数局部作用域中的 printf(“Hello world %d\n”, c); 语句中的格式字符串 “Hello world %d\n”，也存放在 .rodata 段。

- 栈（stack）：

可读可写

存储的是函数或代码中的局部变量(非 static 变量)

栈的生存期随代码块持续性，代码块运行就给你分配空间，代码块结束，就自动回收空间

- 堆（heap）：

可读可写

存储的是程序运行期间动态分配的 malloc/realloc 的空间

堆的生存期随进程持续性，从 malloc/realloc 到 free 一直存在

下面是我们用 Borland C++ 编译过后的结果

```
_TEXT  segment dword public use32 'CODE'_TEXT  ends_DATA  segment dword public use32 'DATA'_DATA  ends_BSS  segment dword public use32 'BSS'_BSS  ends
```

> 段定义( segment ) 是用来区分或者划分范围区域的意思。汇编语言的 segment 伪指令表示段定义的起始，ends 伪指令表示段定义的结束。段定义是一段连续的内存空间

所以内存针对自动增长的区域，会有三种处理方式

- 如果一个进程与空闲区相邻，那么可把该空闲区分配给进程以供其增大。
- 如果进程相邻的是另一个进程，就会有两种处理方式：要么把需要增长的进程移动到一个内存中空闲区足够大的区域，要么把一个或多个进程交换出去，已变成生成一个大的空闲区。
- 如果一个进程在内存中不能增长，而且磁盘上的交换区也满了，那么这个进程只有挂起一些空闲空间（或者可以结束该进程）

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205633_21884.jpg)

上面只针对单个或者一小部分需要增长的进程采用的方式，如果大部分进程都要在运行时增长，为了减少因内存区域不够而引起的进程交换和移动所产生的开销，一种可用的方法是，在换入或移动进程时为它分配一些额外的内存。然而，当进程被换出到磁盘上时，应该只交换实际上使用的内存，将额外的内存交换也是一种浪费，下面是一种为两个进程分配了增长空间的内存配置。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205634_68968.jpg)

如果进程有两个可增长的段，例如，供变量动态分配和释放的作为堆(全局变量)使用的一个数据段(data segment)，以及存放局部变量与返回地址的一个堆栈段(stack segment)，就如图 b 所示。在图中可以看到所示进程的堆栈段在进程所占内存的顶端向下增长，紧接着在程序段后的数据段向上增长。当增长预留的内存区域不够了，处理方式就如上面的流程图(data segment 自动增长的三种处理方式)一样了。

### **2.5 空闲内存管理**

在进行内存动态分配时，操作系统必须对其进行管理。大致上说，有两种监控内存使用的方式

- 位图(bitmap)
- 空闲列表(free lists)

下面我们就来探讨一下这两种使用方式

- 使用位图的存储管理

使用位图方法时，内存可能被划分为小到几个字或大到几千字节的分配单元。每个分配单元对应于位图中的一位，0 表示空闲， 1 表示占用（或者相反）。一块内存区域和其对应的位图如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205635_70530.jpg)

图 a 表示一段有 5 个进程和 3 个空闲区的内存，刻度为内存分配单元，阴影区表示空闲（在位图中用 0 表示）；图 b 表示对应的位图；图 c 表示用链表表示同样的信息。

分配单元的大小是一个重要的设计因素，分配单位越小，位图越大。然而，即使只有 4 字节的分配单元，32 位的内存也仅仅只需要位图中的 1 位。32n 位的内存需要 n 位的位图，所以1 个位图只占用了 1/32 的内存。如果选择更大的内存单元，位图应该要更小。如果进程的大小不是分配单元的整数倍，那么在最后一个分配单元中会有大量的内存被浪费。

位图提供了一种简单的方法在固定大小的内存中跟踪内存的使用情况，因为位图的大小取决于内存和分配单元的大小。这种方法有一个问题是，当决定为把具有 k 个分配单元的进程放入内存时，内容管理器(memory manager) 必须搜索位图，在位图中找出能够运行 k 个连续 0 位的串。在位图中找出制定长度的连续 0 串是一个很耗时的操作，这是位图的缺点。（可以简单理解为在杂乱无章的数组中，找出具有一大长串空闲的数组单元）

- 使用链表进行管理

另一种记录内存使用情况的方法是，维护一个记录已分配内存段和空闲内存段的链表，段会包含进程或者是两个进程的空闲区域。可用上面的图 c 来表示内存的使用情况。链表中的每一项都可以代表一个 空闲区(H) 或者是进程(P)的起始标志，长度和下一个链表项的位置。

在这个例子中，段链表(segment list)是按照地址排序的。这种方式的优点是，当进程终止或被交换时，更新列表很简单。一个终止进程通常有两个邻居（除了内存的顶部和底部外）。相邻的可能是进程也可能是空闲区，它们有四种组合方式。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205635_78033.jpg)

当按照地址顺序在链表中存放进程和空闲区时，有几种算法可以为创建的进程（或者从磁盘中换入的进程）分配内存。我们先假设内存管理器知道应该分配多少内存，最简单的算法是使用 首次适配(first fit)。内存管理器会沿着段列表进行扫描，直到找个一个足够大的空闲区为止。除非空闲区大小和要分配的空间大小一样，否则将空闲区分为两部分，一部分供进程使用；一部分生成新的空闲区。首次适配算法是一种速度很快的算法，因为它会尽可能的搜索链表。

首次适配的一个小的变体是 下次适配(next fit)。它和首次匹配的工作方式相同，只有一个不同之处那就是下次适配在每次找到合适的空闲区时就会记录当时的位置，以便下次寻找空闲区时从上次结束的地方开始搜索，而不是像首次匹配算法那样每次都会从头开始搜索。Bays(1997) 证明了下次算法的性能略低于首次匹配算法。

另外一个著名的并且广泛使用的算法是 最佳适配(best fit)。最佳适配会从头到尾寻找整个链表，找出能够容纳进程的最小空闲区。最佳适配算法会试图找出最接近实际需要的空闲区，以最好的匹配请求和可用空闲区，而不是先一次拆分一个以后可能会用到的大的空闲区。比如现在我们需要一个大小为 2 的块，那么首次匹配算法会把这个块分配在位置 5 的空闲区，而最佳适配算法会把该块分配在位置为 18 的空闲区，如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205636_38978.jpg)

那么最佳适配算法的性能如何呢？最佳适配会遍历整个链表，所以最佳适配算法的性能要比首次匹配算法差。但是令人想不到的是，最佳适配算法要比首次匹配和下次匹配算法浪费更多的内存，因为它会产生大量无用的小缓冲区，首次匹配算法生成的空闲区会更大一些。

最佳适配的空闲区会分裂出很多非常小的缓冲区，为了避免这一问题，可以考虑使用 最差适配(worst fit) 算法。即总是分配最大的内存区域（所以你现在明白为什么最佳适配算法会分裂出很多小缓冲区了吧），使新分配的空闲区比较大从而可以继续使用。仿真程序表明最差适配算法也不是一个好主意。

如果为进程和空闲区维护各自独立的链表，那么这四个算法的速度都能得到提高。这样，这四种算法的目标都是为了检查空闲区而不是进程。但这种分配速度的提高的一个不可避免的代价是增加复杂度和减慢内存释放速度，因为必须将一个回收的段从进程链表中删除并插入空闲链表区。

如果进程和空闲区使用不同的链表，那么可以按照大小对空闲区链表排序，以便提高最佳适配算法的速度。在使用最佳适配算法搜索由小到大排列的空闲区链表时，只要找到一个合适的空闲区，则这个空闲区就是能容纳这个作业的最小空闲区，因此是最佳匹配。因为空闲区链表以单链表形式组织，所以不需要进一步搜索。空闲区链表按大小排序时，首次适配算法与最佳适配算法一样快，而下次适配算法在这里毫无意义。

另一种分配算法是 快速适配(quick fit) 算法，它为那些常用大小的空闲区维护单独的链表。例如，有一个 n 项的表，该表的第一项是指向大小为 4 KB 的空闲区链表表头指针，第二项是指向大小为 8 KB 的空闲区链表表头指针，第三项是指向大小为 12 KB 的空闲区链表表头指针，以此类推。比如 21 KB 这样的空闲区既可以放在 20 KB 的链表中，也可以放在一个专门存放大小比较特别的空闲区链表中。

快速匹配算法寻找一个指定代销的空闲区也是十分快速的，但它和所有将空闲区按大小排序的方案一样，都有一个共同的缺点，即在一个进程终止或被换出时，寻找它的相邻块并查看是否可以合并的过程都是非常耗时的。如果不进行合并，内存将会很快分裂出大量进程无法利用的小空闲区。

## **3. 虚拟内存**

尽管基址寄存器和变址寄存器用来创建地址空间的抽象，但是这有一个其他的问题需要解决：管理软件的不断增大(managing bloatware)。虽然内存的大小增长迅速，但是软件的大小增长的要比内存还要快。在 1980 年的时候，许多大学用一台 4 MB 的 VAX 计算机运行分时操作系统，供十几个用户同时运行。现在微软公司推荐的 64 位 Windows 8 系统至少需要 2 GB 内存，而许多多媒体的潮流则进一步推动了对内存的需求。

这一发展的结果是，需要运行的程序往往大到内存无法容纳，而且必然需要系统能够支持多个程序同时运行，即使内存可以满足其中单独一个程序的需求，但是从总体上来看内存仍然满足不了日益增长的软件的需求（感觉和xxx和xxx 的矛盾很相似）。而交换技术并不是一个很有效的方案，在一些中小应用程序尚可使用交换，如果应用程序过大，难道还要每次交换几 GB 的内存？这显然是不合适的，一个典型的 SATA 磁盘的峰值传输速度高达几百兆/秒，这意味着需要好几秒才能换出或者换入一个 1 GB 的程序。

> SATA（Serial ATA）硬盘，又称串口硬盘，是未来 PC 机硬盘的趋势，已基本取代了传统的 PATA 硬盘。

那么还有没有一种有效的方式来应对呢？有，那就是使用 虚拟内存(virtual memory)，虚拟内存的基本思想是，每个程序都有自己的地址空间，这个地址空间被划分为多个称为页面(page)的块。每一页都是连续的地址范围。这些页被映射到物理内存，但并不是所有的页都必须在内存中才能运行程序。当程序引用到一部分在物理内存中的地址空间时，硬件会立刻执行必要的映射。当程序引用到一部分不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令。

在某种意义上来说，虚拟地址是对基址寄存器和变址寄存器的一种概述。8088 有分离的基址寄存器（但不是变址寄存器）用于放入 text 和 data 。

使用虚拟内存，可以将整个地址空间以很小的单位映射到物理内存中，而不是仅仅针对 text 和 data 区进行重定位。下面我们会探讨虚拟内存是如何实现的。

虚拟内存很适合在多道程序设计系统中使用，许多程序的片段同时保存在内存中，当一个程序等待它的一部分读入内存时，可以把 CPU 交给另一个进程使用。

### **3.1 分页**

大部分使用虚拟内存的系统中都会使用一种 分页(paging) 技术。在任何一台计算机上，程序会引用使用一组内存地址。当程序执行

```
MOV REG,1000
```

这条指令时，它会把内存地址为 1000 的内存单元的内容复制到 REG 中（或者相反，这取决于计算机）。地址可以通过索引、基址寄存器、段寄存器或其他方式产生。

这些程序生成的地址被称为 虚拟地址(virtual addresses) 并形成虚拟地址空间(virtual address space)，在没有虚拟内存的计算机上，系统直接将虚拟地址送到内存中线上，读写操作都使用同样地址的物理内存。在使用虚拟内存时，虚拟地址不会直接发送到内存总线上。相反，会使用 MMU(Memory Management Unit) 内存管理单元把虚拟地址映射为物理内存地址，像下图这样。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205636_37303.jpg)

下面这幅图展示了这种映射是如何工作的

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205637_16978.jpg)

页表给出虚拟地址与物理内存地址之间的映射关系。每一页起始于 4096 的倍数位置，结束于 4095 的位置，所以 4K 到 8K 实际为 4096 - 8191 ，8K - 12K 就是 8192 - 12287。

在这个例子中，我们可能有一个 16 位地址的计算机，地址从 0 - 64 K - 1，这些是虚拟地址。然而只有 32 KB 的物理地址。所以虽然可以编写 64 KB 的程序，但是程序无法全部调入内存运行，在磁盘上必须有一个最多 64 KB 的程序核心映像的完整副本，以保证程序片段在需要时被调入内存。

### **3.2 存在映射的页如何映射**

虚拟地址空间由固定大小的单元组成，这种固定大小的单元称为 页(pages)。而相对的，物理内存中也有固定大小的物理单元，称为 页框(page frames)。页和页框的大小一样。在上面这个例子中，页的大小为 4KB ，但是实际的使用过程中页的大小范围可能是 512 字节 - 1G 字节的大小。对应于 64 KB 的虚拟地址空间和 32 KB 的物理内存，可得到 16 个虚拟页面和 8 个页框。RAM 和磁盘之间的交换总是以整个页为单元进行交换的。

程序试图访问地址时，例如执行下面这条指令

```
MOV REG, 0会将虚拟地址 0 送到 MMU。MMU 看到虚拟地址落在页面 0 （0 - 4095），根据其映射结果，这一页面对应的页框 2 （8192 - 12287），因此 MMU 把地址变换为 8192 ，并把地址 8192 送到总线上。内存对 MMU 一无所知，它只看到一个对 8192 地址的读写请求并执行它。MMU 从而有效的把所有虚拟地址 0 - 4095 映射到了 8192 - 12287 的物理地址。同样的，指令MOV REG, 8192也被有效的转换为MOV REG, 24576
```

虚拟地址 8192（在虚拟页 2 中）被映射到物理地址 24576（在物理页框 6 中）上。

通过恰当的设置 MMU，可以把 16 个虚拟页面映射到 8 个页框中的任何一个。但是这并没有解决虚拟地址空间比物理内存大的问题。

上图中有 8 个物理页框，于是只有 8 个虚拟页被映射到了物理内存中，在上图中用 X 号表示的其他页面没有被映射。在实际的硬件中，会使用一个 在/不在(Present/absent bit)位记录页面在内存中的实际存在情况。

### **3.3 未映射的页如何映射**

当程序访问一个未映射的页面，如执行指令

```
MOV REG, 32780将会发生什么情况呢？虚拟页面 8 （从 32768 开始）的第 12 个字节所对应的物理地址是什么？MMU 注意到该页面没有被映射（在图中用 X 号表示），于是 CPU 会陷入(trap)到操作系统中。这个陷入称为 缺页中断(page fault) 或者是 缺页错误。操作系统会选择一个很少使用的页并把它的内容写入磁盘（如果它不在磁盘上）。随后把需要访问的页面读到刚才回收的页框中，修改映射关系，然后重新启动引起陷入的指令。有点不太好理解，举个例子来看一下。
```

例如，如果操作系统决定放弃页框 1，那么它将把虚拟机页面 8 装入物理地址 4096，并对 MMU 映射做两处修改。首先，它要将虚拟页中的 1 表项标记为未映射，使以后任何对虚拟地址 4096 - 8191 的访问都将导致陷入。随后把虚拟页面 8 的表项的叉号改为 1，因此在引起陷阱的指令重新启动时，它将把虚拟地址 32780 映射为物理地址（4096 + 12）。

下面查看一下 MMU 的内部构造以便了解它们是如何工作的，以及了解为什么我们选用的页大小都是 2 的整数次幂。下图我们可以看到一个虚拟地址的例子

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205638_94104.jpg)

虚拟地址 8196 （二进制 0010000000000100）用上面的页表映射图所示的 MMU 映射机制进行映射，输入的 16 位虚拟地址被分为 4 位的页号和 12 位的偏移量。4 位的页号可以表示 16 个页面，12 位的偏移可以为一页内的全部 4096 个字节。

可用页号作为页表(page table) 的索引，以得出对应于该虚拟页面的页框号。如果在/不在位则是 0 ，则引起一个操作系统陷入。如果该位是 1，则将在页表中查到的页框号复制到输出寄存器的高 3 位中，再加上输入虚拟地址中的低 12 位偏移量。如此就构成了 15 位的物理地址。输出寄存器的内容随即被作为物理地址送到总线。

### **3.4 页表**

在上面这个简单的例子中，虚拟地址到物理地址的映射可以总结如下：虚拟地址被分为虚拟页号（高位部分）和偏移量（低位部分）。例如，对于 16 位地址和 4 KB 的页面大小，高 4 位可以指定 16 个虚拟页面中的一页，而低 12 位接着确定了所选页面中的偏移量（0-4095）。

虚拟页号可作为页表的索引用来找到虚拟页中的内容。由页表项可以找到页框号（如果有的话）。然后把页框号拼接到偏移量的高位端，以替换掉虚拟页号，形成物理地址。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205639_38815.jpg)

因此，页表的目的是把虚拟页映射到页框中。从数学上说，页表是一个函数，它的参数是虚拟页号，结果是物理页框号。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205639_63823.jpg)

通过这个函数可以把虚拟地址中的虚拟页转换为页框，从而形成物理地址。

### 3.5 页表项的结构

下面我们探讨一下页表项的具体结构，上面你知道了页表项的大致构成，是由页框号和在/不在位构成的，现在我们来具体探讨一下页表项的构成

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205640_25523.jpg)

页表项的结构是与机器相关的，但是不同机器上的页表项大致相同。上面是一个页表项的构成，不同计算机的页表项可能不同，但是一般来说都是 32 位的。页表项中最重要的字段就是页框号(Page frame number)。毕竟，页表到页框最重要的一步操作就是要把此值映射过去。下一个比较重要的就是在/不在位，如果此位上的值是 1，那么页表项是有效的并且能够被使用。如果此值是 0 的话，则表示该页表项对应的虚拟页面不在内存中，访问该页面会引起一个缺页异常(page fault)。

保护位(Protection) 告诉我们哪一种访问是允许的，啥意思呢？最简单的表示形式是这个域只有一位，**0 表示可读可写，1 表示的是只读**。

修改位(Modified) 和 访问位(Referenced) 会跟踪页面的使用情况。当一个页面被写入时，硬件会自动的设置修改位。修改位在页面重新分配页框时很有用。如果一个页面已经被修改过（即它是 脏 的），则必须把它写回磁盘。如果一个页面没有被修改过（即它是 干净的），那么重新分配时这个页框会被直接丢弃，因为磁盘上的副本仍然是有效的。这个位有时也叫做 脏位(dirty bit)，因为它反映了页面的状态。

访问位(Referenced) 在页面被访问时被设置，不管是读还是写。这个值能够帮助操作系统在发生缺页中断时选择要淘汰的页。不再使用的页要比正在使用的页更适合被淘汰。这个位在后面要讨论的页面置换算法中作用很大。

最后一位用于禁止该页面被高速缓存，这个功能对于映射到设备寄存器还是内存中起到了关键作用。通过这一位可以禁用高速缓存。具有独立的 I/O 空间而不是用内存映射 I/O 的机器来说，并不需要这一位。

在深入讨论下面问题之前，需要强调一下：虚拟内存本质上是用来创造一个地址空间的抽象，可以把它理解成为进程是对 CPU 的抽象，虚拟内存的实现，本质是将虚拟地址空间分解成页，并将每一项映射到物理内存的某个页框。因为我们的重点是如何管理这个虚拟内存的抽象。

### **3.6 加速分页过程**

到现在我们已经虚拟内存(virtual memory) 和 分页(paging) 的基础，现在我们可以把目光放在具体的实现上面了。在任何带有分页的系统中，都会需要面临下面这两个主要问题：

- 虚拟地址到物理地址的映射速度必须要快
- 如果虚拟地址空间足够大，那么页表也会足够大

第一个问题是**由于每次访问内存都需要进行虚拟地址到物理地址的映射**，所有的指令最终都来自于内存，并且很多指令也会访问内存中的操作数。

> 操作数：操作数是计算机指令中的一个组成部分，它规定了指令中进行数字运算的量 。操作数指出指令执行的操作所需要数据的来源。操作数是汇编指令的一个字段。比如，MOV、ADD 等。

因此，每条指令可能会多次访问页表，如果执行一条指令需要 1 ns，那么页表查询需要在 0.2 ns 之内完成，以避免映射成为一个主要性能瓶颈。

第二个问题是所有的现代操作系统都会使用至少 32 位的虚拟地址，并且 64 位正在变得越来越普遍。假设页大小为 4 KB，32 位的地址空间将近有 100 万页，而 64 位地址空间简直多到无法想象。

对大而且快速的页映射的需要成为构建计算机的一个非常重要的约束。就像上面页表中的图一样，**每一个表项对应一个虚拟页面，虚拟页号作为索引。**在启动一个进程时，操作系统会把保存在内存中进程页表读副本放入寄存器中。

> 最后一句话是不是不好理解？还记得页表是什么吗？它是虚拟地址到内存地址的映射页表。页表是虚拟地址转换的关键组成部分，它是访问内存中数据所必需的。在进程启动时，执行很多次虚拟地址到物理地址的转换，会把物理地址的副本从内存中读入到寄存器中，再执行这一转换过程。

所以，在进程的运行过程中，不必再为页表而访问内存。使用这种方法的优势是简单而且映射过程中不需要访问内存。缺点是 页表太大时，代价高昂，而且每次上下文切换的时候都必须装载整个页表，这样会造成性能的降低。鉴于此，我们讨论一下加速分页机制和处理大的虚拟地址空间的实现方案

### 3.7 转换检测缓冲区

我们首先先来一起探讨一下加速分页的问题。大部分优化方案都是从内存中的页表开始的。这种设计对效率有着巨大的影响。考虑一下，例如，假设一条 1 字节的指令要把一个寄存器中的数据复制到另一个寄存器。在不分页的情况下，这条指令只访问一次内存，即从内存取出指令。有了分页机制后，会因为要访问页表而需要更多的内存访问。由于执行速度通常被 CPU 从内存中取指令和数据的速度所限制，这样的话，两次访问才能实现一次的访问效果，所以内存访问的性能会下降一半。在这种情况下，根本不会采用分页机制。

> 什么是 1 字节的指令？我们以 8085 微处理器为例来说明一下，在 8085 微处理中，一共有 3 种字节指令，它们分别是 1-byte(1 字节)、2-byte(2 字节)、3-byte(3 字节)，我们分别来说一下： 1-byte：1 字节的操作数和操作码共同以 1 字节表示；操作数是内部寄存器，并被编码到指令中；指令需要一个存储位置来将单个寄存器存储在存储位置中。没有操作数的指令也是 1-byte 指令。 例如：MOV B,C 、LDAX B、NOP、HLT（这块不明白的读者可以自行查阅） 2-byte: 2 字节包括：第一个字节指定的操作码；第二个字节指定操作数；指令需要两个存储器位置才能存储在存储器中。 例如 MVI B, 26 H、IN 56 H 3-byte: 在 3 字节指令中，第一个字节指定操作码；后面两个字节指定 16 位的地址；第二个字节保存低位地址；第三个字节保存 高位地址。指令需要三个存储器位置才能将单个字节存储在存储器中。 例如 LDA 2050 H、JMP 2085 H

大多数程序总是对少量页面进行多次访问，而不是对大量页面进行少量访问。因此，只有很少的页面能够被再次访问，而其他的页表项很少被访问。

> 页表项一般也被称为 Page Table Entry(PTE)。

基于这种设想，提出了一种方案，即从硬件方面来解决这个问题，为计算机设置一个小型的硬件设备，能够将虚拟地址直接映射到物理地址，而不必再访问页表。这种设备被称为转换检测缓冲区(Translation Lookaside Buffer, TLB)，有时又被称为 相联存储器(associate memory) 。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205641_44186.jpg)

TLB 通常位于 MMU 中，包含少量的表项，每个表项都记录了页面的相关信息，除了虚拟页号外，其他表项都和页表是一一对应的。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205641_25211.jpg)

是不是你到现在还是有点不理解什么是 TLB，TLB 其实就是一种内存缓存，用于减少访问内存所需要的时间，它就是 MMU 的一部分，TLB 会将虚拟地址到物理地址的转换存储起来，通常可以称为地址翻译缓存(address-translation cache)。TLB 通常位于 CPU 和 CPU 缓存之间，它与 CPU 缓存是不同的缓存级别。下面我们来看一下 TLB 是如何工作的。

当一个 MMU 中的虚拟地址需要进行转换时，硬件首先检查虚拟页号与 TLB 中所有表项进行并行匹配，判断虚拟页是否在 TLB 中。如果找到了有效匹配项，并且要进行的访问操作没有违反保护位的话，则将页框号直接从 TLB 中取出而不用再直接访问页表。如果虚拟页在 TLB 中但是违反了保护位的权限的话（比如只允许读但是是一个写指令），则会生成一个保护错误(protection fault) 返回。

上面探讨的是虚拟地址在 TLB 中的情况，那么如果虚拟地址不再 TLB 中该怎么办？如果 MMU 检测到没有有效的匹配项，就会进行正常的页表查找，然后从 TLB 中逐出一个表项然后把从页表中找到的项放在 TLB 中。当一个表项被从 TLB 中清除出，将修改位复制到内存中页表项，除了访问位之外，其他位保持不变。当页表项从页表装入 TLB 中时，所有的值都来自于内存。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205642_71756.jpg)

### 3.8 软件 TLB 管理

直到现在，我们假设每台电脑都有可以被硬件识别的页表，外加一个 TLB。在这个设计中，TLB 管理和处理 TLB 错误完全由硬件来完成。仅仅当页面不在内存中时，才会发生操作系统的陷入(trap)。

在以前，我们上面的假设通常是正确的。但是，许多现代的 RISC 机器，包括 SPARC、MIPS 和 HP PA，几乎所有的页面管理都是在软件中完成的。

> 精简指令集计算机或 RISC 是一种计算机指令集，它使计算机的微处理器的每条指令（CPI）周期比复杂指令集计算机（CISC）少。

在这些计算机上，TLB 条目由操作系统显示加载。当发生 TLB 访问丢失时，**不再是由 MMU 到页表中查找并取出需要的页表项，而是生成一个 TLB 失效并将问题交给操作系统解决。**操作系统必须找到该页，把它从 TLB 中移除（移除页表中的一项），然后把新找到的页放在 TLB 中，最后再执行先前出错的指令。然而，所有这些操作都必须通过少量指令完成，因为 TLB 丢失的发生率要比出错率高很多。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205642_62765.jpg)

无论是用硬件还是用软件来处理 TLB 失效，常见的方式都是找到页表并执行索引操作以定位到将要访问的页面，在软件中进行搜索的问题是保存页表的页可能不在 TLB 中，这将在处理过程中导致其他 TLB 错误。改善方法是可以在内存中的固定位置维护一个大的 TLB 表项的高速缓存来减少 TLB 失效。通过首先检查软件的高速缓存，操作系统 能够有效的减少 TLB 失效问题。

TLB 软件管理会有两种 TLB 失效问题，当一个页访问在内存中而不在 TLB 中时，将产生 软失效(soft miss)，那么此时要做的就是把页表更新到 TLB 中（我们上面探讨的过程），而不会产生磁盘 I/O，处理仅仅需要一些机器指令在几纳秒的时间内完成。然而，当页本身不在内存中时，将会产生硬失效(hard miss)，那么此时就需要从磁盘中进行页表提取，硬失效的处理时间通常是软失效的百万倍。在页表结构中查找映射的过程称为 页表遍历(page table walk)。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205643_14046.jpg)

上面的这两种情况都是理想情况下出现的现象，但是在实际应用过程中情况会更加复杂，未命中的情况可能既不是硬失效又不是软失效。一些未命中可能更软或更硬（偷笑）。比如，如果页表遍历的过程中没有找到所需要的页，那么此时会出现三种情况：

- 所需的页面就在内存中，但是却没有记录在进程的页表中，这种情况可能是由其他进程从磁盘掉入内存，这种情况只需要把页正确映射就可以了，而不需要在从硬盘调入，这是一种软失效，称为 次要缺页错误(minor page fault)。
- 基于上述情况，如果需要从硬盘直接调入页面，这就是严重缺页错误(major page falut)。
- 还有一种情况是，程序可能访问了一个非法地址，根本无需向 TLB 中增加映射。此时，操作系统会报告一个 段错误(segmentation fault) 来终止程序。只有第三种缺页属于程序错误，其他缺页情况都会被硬件或操作系统以降低程序性能为代价来修复

### 3.9 **针对大内存的页表**

还记得我们讨论的是什么问题吗？（捂脸），可能讨论的太多你有所不知道了，我再提醒你一下，上面加速分页过程讨论的是**虚拟地址到物理地址的映射速度必须要快的问题**，还有一个问题是 **如果虚拟地址空间足够大，那么页表也会足够大的问题，如何处理巨大的虚拟地址空间，**下面展开我们的讨论。

### 3.10 多级页表

第一种方案是使用多级页表(multi)，下面是一个例子

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205643_49153.jpg)

32 位的虚拟地址被划分为 10 位的 PT1 域，10 位的 PT2 域，还有 12 位的 Offset 域。因为偏移量是 12 位，所以页面大小是 4KB，公有 2^20 次方个页面。

**引入多级页表的原因是避免把全部页表一直保存在内存中。不需要的页表就不应该保留。**

多级页表是一种分页方案，它由两个或多个层次的分页表组成，也称为分层分页。级别1（level 1）页面表的条目是指向级别 2（level 2） 页面表的指针，级别2页面表的条目是指向级别 3（level 3） 页面表的指针，依此类推。最后一级页表存储的是实际的信息。

下面是一个二级页表的工作过程：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205644_45053.jpg)

在最左边是顶级页表，它有 1024 个表项，对应于 10 位的 PT1 域。当一个虚拟地址被送到 MMU 时，MMU 首先提取 PT1 域并把该值作为访问顶级页表的索引。因为整个 4 GB （即 32 位）虚拟地址已经按 4 KB 大小分块，所以顶级页表中的 1024 个表项的每一个都表示 4M 的块地址范围。

由索引顶级页表得到的表项中含有二级页表的地址或页框号。顶级页表的表项 0 指向程序正文的页表，表项 1 指向含有数据的页表，表项 1023 指向堆栈的页表，其他的项（用阴影表示）表示没有使用。现在把 PT2 域作为访问选定的二级页表的索引，以便找到虚拟页面的对应页框号。

### 3.11 倒排页表

针对分页层级结构中不断增加的替代方法是使用 倒排页表(inverted page tables)。采用这种解决方案的有 PowerPC、UltraSPARC 和 Itanium。在这种设计中，实际内存中的每个页框对应一个表项，而不是每个虚拟页面对应一个表项。

虽然倒排页表节省了大量的空间，但是它也有自己的缺陷：那就是从虚拟地址到物理地址的转换会变得很困难。当进程 n 访问虚拟页面 p 时，硬件不能再通过把 p 当作指向页表的一个索引来查找物理页。而是必须搜索整个倒排表来查找某个表项。另外，搜索必须对每一个内存访问操作都执行一次，而不是在发生缺页中断时执行。

解决这一问题的方式时使用 TLB。当发生 TLB 失效时，需要用软件搜索整个倒排页表。一个可行的方式是建立一个散列表，用虚拟地址来散列。当前所有内存中的具有相同散列值的虚拟页面被链接在一起。如下图所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205645_29209.jpg)

如果散列表中的槽数与机器中物理页面数一样多，那么散列表的冲突链的长度将会是 1 个表项的长度，这将会大大提高映射速度。一旦页框被找到，新的（虚拟页号，物理页框号）就会被装在到 TLB 中。

## **4. 页面置换算法**

当发生缺页异常时，操作系统会选择一个页面进行换出从而为新进来的页面腾出空间。如果要换出的页面在内存中已经被修改，那么必须将其写到磁盘中以使磁盘副本保持最新状态。如果页面没有被修改过，并且磁盘中的副本也已经是最新的，那么就不需要进行重写。那么就直接使用调入的页面覆盖需要移除的页面就可以了。

当发生缺页中断时，虽然可以随机的选择一个页面进行置换，但是如果每次都选择一个不常用的页面会提升系统的性能。如果一个经常使用的页面被换出，那么这个页面在短时间内又可能被重复使用，那么就可能会造成额外的性能开销。在关于页面的主题上有很多页面置换算法(page replacement algorithms)，这些已经从理论上和实践上得到了证明。

需要指出的是，页面置换问题在计算机的其他领域中也会出现。例如，多数计算机把最近使用过的 32 字节或者 64 字节的存储块保存在一个或多个高速缓存中。当缓存满的时候，一些块就被选择和移除。这些块的移除除了花费时间较短外，这个问题同页面置换问题完全一样。之所以花费时间较短，是因为丢掉的高速缓存可以从内存中获取，而内存没有寻找磁道的时间也不存在旋转延迟。

第二个例子是 Web 服务器。服务器会在内存中缓存一些经常使用到的 Web 页面。然而，当缓存满了并且已经引用了新的页面，那么必须决定退出哪个 Web 页面。在高速缓存中的 Web 页面不会被修改。因此磁盘中的 Web 页面经常是最新的，同样的考虑也适用在虚拟内存中。在虚拟系统中，内存中的页面可能会修改也可能不会修改。

下面我们就来探讨一下有哪些页面置换算法。

### **4.1 最优页面置换算法**

最优的页面置换算法很容易描述但在实际情况下很难实现。它的工作流程如下：在缺页中断发生时，这些页面之一将在下一条指令（包含该指令的页面）上被引用。其他页面则可能要到 10、100 或者 1000 条指令后才会被访问。每个页面都可以用在该页首次被访问前所要执行的指令数作为标记。

最优化的页面算法表明应该标记最大的页面。如果一个页面在 800 万条指令内不会被使用，另外一个页面在 600 万条指令内不会被使用，则置换前一个页面，从而把需要调入这个页面而发生的缺页中断推迟。计算机也像人类一样，会把不愿意做的事情尽可能的往后拖。

这个算法最大的问题时无法实现。当缺页中断发生时，操作系统无法知道各个页面的下一次将在什么时候被访问。这种算法在实际过程中根本不会使用。

### **4.2 最近未使用页面置换算法**

为了能够让操作系统收集页面使用信息，大部分使用虚拟地址的计算机都有两个状态位，R 和 M，来和每个页面进行关联。**每当引用页面（读入或写入）时都设置 R，写入（即修改）页面时设置 M，**这些位包含在每个页表项中，就像下面所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205645_18056.jpg)

因为每次访问时都会更新这些位，因此由硬件来设置它们非常重要。一旦某个位被设置为 1，就会一直保持 1 直到操作系统下次来修改此位。

如果硬件没有这些位，那么可以使用操作系统的缺页中断和时钟中断机制来进行模拟。当启动一个进程时，将其所有的页面都标记为不在内存；一旦访问任何一个页面就会引发一次缺页中断，此时操作系统就可以设置 R 位(在它的内部表中)，修改页表项使其指向正确的页面，并设置为 READ ONLY 模式，然后重新启动引起缺页中断的指令。如果页面随后被修改，就会发生另一个缺页异常。从而允许操作系统设置 M 位并把页面的模式设置为 READ/WRITE。

可以用 R 位和 M 位来构造一个简单的页面置换算法：当启动一个进程时，操作系统将其所有页面的两个位都设置为 0。R 位定期的被清零（在每个时钟中断）。用来将最近未引用的页面和已引用的页面分开。

当出现缺页中断后，操作系统会检查所有的页面，并根据它们的 R 位和 M 位将当前值分为四类：

- 第 0 类：没有引用 R，没有修改 M
- 第 1 类：没有引用 R，已修改 M
- 第 2 类：引用 R ，没有修改 M
- 第 3 类：已被访问 R，已被修改 M

尽管看起来好像无法实现第一类页面，但是当第三类页面的 R 位被时钟中断清除时，它们就会发生。时钟中断不会清除 M 位，因为需要这个信息才能知道是否写回磁盘中。清除 R 但不清除 M 会导致出现一类页面。

NRU(Not Recently Used) 算法从编号最小的非空类中随机删除一个页面。此算法隐含的思想是，在一个时钟内（约 20 ms）淘汰一个已修改但是没有被访问的页面要比一个大量引用的未修改页面好，**NRU 的主要优点是易于理解并且能够有效的实现。**

### **4.3 先进先出页面置换算法**

另一种开销较小的方式是使用 FIFO(First-In,First-Out) 算法，这种类型的数据结构也适用在页面置换算法中。由操作系统维护一个所有在当前内存中的页面的链表，最早进入的放在表头，最新进入的页面放在表尾。在发生缺页异常时，会把头部的页移除并且把新的页添加到表尾。

> 还记得缺页异常什么时候发生吗？我们知道应用程序访问内存会进行虚拟地址到物理地址的映射，缺页异常就发生在虚拟地址无法映射到物理地址的时候。因为实际的物理地址要比虚拟地址小很多（参考上面的虚拟地址和物理地址映射图），所以缺页经常会发生。

先进先出页面可能是最简单的页面替换算法了。在这种算法中，操作系统会跟踪链表中内存中的所有页。下面我们举个例子看一下（这个算法我刚开始看的时候有点懵逼，后来才看懂，我还是很菜）。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205646_74169.jpg)

- 初始化的时候，没有任何页面，所以第一次的时候会检查页面 1 是否位于链表中，没有在链表中，那么就是 MISS，页面1 进入链表，链表的先进先出的方向如图所示。
- 类似的，第二次会先检查页面 2 是否位于链表中，没有在链表中，那么页面 2 进入链表，状态为 MISS，依次类推。
- 我们来看第四次，此时的链表为 1 2 3，第四次会检查页面 2 是否位于链表中，经过检索后，发现 2 在链表中，那么状态就是 HIT，并不会再进行入队和出队操作，第五次也是一样的。
- 下面来看第六次，此时的链表还是 1 2 3，因为之前没有执行进入链表操作，页面 5 会首先进行检查，发现链表中没有页面 5 ，则执行页面 5 的进入链表操作，页面 2 执行出链表的操作，执行完成后的链表顺序为 2 3 5。

### **4.4 第二次机会页面置换算法**

我们上面学到的 FIFO 链表页面有个缺陷，那就是出链和入链并不会进行 check 检查，这样就会容易把经常使用的页面置换出去，为了避免这一问题，我们对该算法做一个简单的修改：我们检查最老页面的 R 位，如果是 0 ，那么这个页面就是最老的而且没有被使用，那么这个页面就会被立刻换出。如果 R 位是 1，那么就清除此位，此页面会被放在链表的尾部，修改它的装入时间就像刚放进来的一样。然后继续搜索。

这种算法叫做 第二次机会(second chance)算法，就像下面这样，我们看到页面 A 到 H 保留在链表中，并按到达内存的时间排序。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205647_33929.jpg)

假设缺页异常发生在时刻 20 处，这时最老的页面是 A ，它是在 0 时刻到达的。如果 A 的 R 位是 0，那么它将被淘汰出内存，或者把它写回磁盘（如果它已经被修改过），或者只是简单的放弃（如果它是未被修改过）。另一方面，如果它的 R 位已经设置了，则将 A 放到链表的尾部并且重新设置装入时间为当前时刻（20 处），然后清除 R 位。然后从 B 页面开始继续搜索合适的页面。

寻找第二次机会的是在最近的时钟间隔中未被访问过的页面。如果所有的页面都被访问过，该算法就会被简化为单纯的 FIFO 算法。具体来说，假设图 a 中所有页面都设置了 R 位。操作系统将页面依次移到链表末尾，每次都在添加到末尾时清除 R 位。最后，算法又会回到页面 A，此时的 R 位已经被清除，那么页面 A 就会被执行出链处理，因此算法能够正常结束。

### **4.5 时钟页面置换算法**

即使上面提到的第二次页面置换算法也是一种比较合理的算法，但它经常要在链表中移动页面，既降低了效率，而且这种算法也不是必须的。一种比较好的方式是把所有的页面都保存在一个类似钟面的环形链表中，一个表针指向最老的页面。如下图所示

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205647_73499.jpg)

当缺页错误出现时，算法首先检查表针指向的页面，如果它的 R 位是 0 就淘汰该页面，并把新的页面插入到这个位置，然后把表针向前移动一位；如果 R 位是 1 就清除 R 位并把表针前移一个位置。重复这个过程直到找到了一个 R 位为 0 的页面位置。了解这个算法的工作方式，就明白为什么它被称为 时钟(clokc)算法了。

### **4.6最近最少使用页面置换算法**

最近最少使用页面置换算法的一个解释会是下面这样：在前面几条指令中频繁使用的页面和可能在后面的几条指令中被使用。反过来说，已经很久没有使用的页面有可能在未来一段时间内仍不会被使用。这个思想揭示了一个可以实现的算法：在缺页中断时，置换未使用时间最长的页面。这个策略称为 LRU(Least Recently Used)，最近最少使用页面置换算法。

虽然 LRU 在理论上是可以实现的，但是从长远看来代价比较高。为了完全实现 LRU，会在内存中维护一个所有页面的链表，最频繁使用的页位于表头，最近最少使用的页位于表尾。困难的是在每次内存引用时更新整个链表。在链表中找到一个页面，删除它，然后把它移动到表头是一个非常耗时的操作，即使使用硬件来实现也是一样的费时。

然而，还有其他方法可以通过硬件实现 LRU。让我们首先考虑最简单的方式。这个方法要求硬件有一个 64 位的计数器，它在每条指令执行完成后自动加 1，每个页表必须有一个足够容纳这个计数器值的域。在每次访问内存后，将当前的值保存到被访问页面的页表项中。一旦发生缺页异常，操作系统就检查所有页表项中计数器的值，找到值最小的一个页面，这个页面就是最少使用的页面。

### **4.7 用软件模拟 LRU**

尽管上面的 LRU 算法在原则上是可以实现的，**但是很少有机器能够拥有那些特殊的硬件。**上面是硬件的实现方式，那么现在考虑要用软件来实现 LRU 。一种可以实现的方案是 NFU(Not Frequently Used，最不常用)算法。它需要一个软件计数器来和每个页面关联，初始化的时候是 0 。在每个时钟中断时，操作系统会浏览内存中的所有页，会将每个页面的 R 位（0 或 1）加到它的计数器上。这个计数器大体上跟踪了各个页面访问的频繁程度。当缺页异常出现时，则置换计数器值最小的页面。

NFU 最主要的问题是它不会忘记任何东西，想一下是不是这样？例如，在一个多次（扫描）的编译器中，在第一遍扫描中频繁使用的页面会在后续的扫描中也有较高的计数。事实上，如果第一次扫描的执行时间恰好是各次扫描中最长的，那么后续遍历的页面的统计次数总会比第一次页面的统计次数小。结果是操作系统将置换有用的页面而不是不再使用的页面。

幸运的是只需要对 NFU 做一个简单的修改就可以让它模拟 LRU，这个修改有两个步骤

- 首先，在 R 位被添加进来之前先把计数器右移一位；
- 第二步，R 位被添加到最左边的位而不是最右边的位。

修改以后的算法称为 老化(aging) 算法，下图解释了老化算法是如何工作的。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205648_55680.jpg)

我们假设在第一个时钟周期内页面 0 - 5 的 R 位依次是 1，0，1，0，1，1，（也就是页面 0 是 1，页面 1 是 0，页面 2 是 1 这样类推）。也就是说，**在 0 个时钟周期到 1 个时钟周期之间，0，2，4，5 都被引用了，**从而把它们的 R 位设置为 1，剩下的设置为 0 。在相关的六个计数器被右移之后 R 位被添加到 左侧 ，就像上图中的 a。剩下的四列显示了接下来的四个时钟周期内的六个计数器变化。

> CPU正在以某个频率前进，该频率的周期称为时钟滴答或时钟周期。一个 100Mhz 的处理器每秒将接收100,000,000个时钟滴答。

当缺页异常出现时，将置换（就是移除）计数器值最小的页面。如果一个页面在前面 4 个时钟周期内都没有被访问过，那么它的计数器应该会有四个连续的 0 ，因此它的值肯定要比前面 3 个时钟周期内都没有被访问过的页面的计数器小。

这个算法与 LRU 算法有两个重要的区别：看一下上图中的 e，第三列和第五列。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205649_55534.jpg)

它们在两个时钟周期内都没有被访问过，在此之前的时钟周期内都引用了两个页面。根据 LRU 算法，如果需要置换的话，那么应该在这两个页面中选择一个。那么问题来了，我萌应该选择哪个？现在的问题是我们不知道时钟周期 1 到时钟周期 2 内它们中哪个页面是后被访问到的。因为在每个时钟周期内只记录了一位，所以无法区分在一个时钟周期内哪个页面最早被引用，哪个页面是最后被引用的。因此，我们能做的就是置换页面3，因为**页面 3 在周期 0 - 1 内都没有被访问过，而页面 5 却被引用过。**

LRU 与老化之前的第 2 个区别是，在老化期间，计数器具有有限数量的位（这个例子中是 8 位），这就限制了以往的访问记录。如果两个页面的计数器都是 0 ，那么我们可以随便选择一个进行置换。实际上，有可能其中一个页面的访问次数实在 9 个时钟周期以前，而另外一个页面是在 1000 个时钟周期之前，但是我们却无法看到这些。在实际过程中，如果时钟周期是 20 ms，8 位一般是够用的。所以我们经常拿 20 ms 来举例。

### **4.8 工作集页面置换算法**

在最单纯的分页系统中，刚启动进程时，在内存中并没有页面。此时如果 CPU 尝试匹配第一条指令，就会得到一个缺页异常，使操作系统装入含有第一条指令的页面。其他的错误比如 全局变量和 堆栈 引起的缺页异常通常会紧接着发生。一段时间以后，进程需要的大部分页面都在内存中了，此时进程开始在较少的缺页异常环境中运行。这个策略称为 请求调页(demand paging)，因为页面是根据需要被调入的，而不是预先调入的。

在一个大的地址空间中系统的读所有的页面，将会造成很多缺页异常，因此会导致没有足够的内存来容纳这些页面。不过幸运的是，大部分进程不是这样工作的，它们都会以局部性方式(locality of reference) 来访问，这意味着在执行的任何阶段，程序只引用其中的一小部分。

一个进程当前正在使用的页面的集合称为它的 工作集(working set)，如果整个工作集都在内存中，那么进程在运行到下一运行阶段（例如，编译器的下一遍扫面）之前，不会产生很多缺页中断。**如果内存太小从而无法容纳整个工作集，那么进程的运行过程中会产生大量的缺页中断，会导致运行速度也会变得缓慢。**因为通常只需要几纳秒就能执行一条指令，而通常需要十毫秒才能从磁盘上读入一个页面。如果一个程序每 10 ms 只能执行一到两条指令，那么它将需要很长时间才能运行完。如果只是执行几条指令就会产生中断，那么就称作这个程序产生了 颠簸(thrashing)。

在多道程序的系统中，通常会把进程移到磁盘上（即从内存中移走所有的页面），这样可以让其他进程有机会占用 CPU 。有一个问题是，当进程想要再次把之前调回磁盘的页面调回内存怎么办？从技术的角度上来讲，并不需要做什么，此进程会一直产生缺页中断直到它的工作集 被调回内存。然后，每次装入一个进程需要 20、100 甚至 1000 次缺页中断，速度显然太慢了，并且由于 CPU 需要几毫秒时间处理一个缺页中断，因此由相当多的 CPU 时间也被浪费了。

因此，不少分页系统中都会设法跟踪进程的工作集，确保这些工作集在进程运行时被调入内存。这个方法叫做 工作集模式(working set model)。它被设计用来减少缺页中断的次数的。在进程运行前首先装入工作集页面的这一个过程被称为 预先调页(prepaging)，工作集是随着时间来变化的。

根据研究表明，大多数程序并不是均匀的访问地址空间的，而访问往往是集中于一小部分页面。一次内存访问可能会取出一条指令，也可能会取出数据，或者是存储数据。在任一时刻 t，都存在一个集合，它包含所哟欧最近 k 次内存访问所访问过的页面。这个集合 w(k,t) 就是工作集。因为最近 k = 1次访问肯定会访问最近 k > 1 次访问所访问过的页面，所以 w(k,t) 是 k 的单调递减函数。随着 k 的增大，w(k,t) 是不会无限变大的，因为程序不可能访问比所能容纳页面数量上限还多的页面。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205650_29019.jpg)

事实上大多数应用程序只会任意访问一小部分页面集合，但是这个集合会随着时间而缓慢变化，所以为什么一开始曲线会快速上升而 k 较大时上升缓慢。为了实现工作集模型，操作系统必须跟踪哪些页面在**工作集中**。一个进程从它开始执行到当前所实际使用的 CPU 时间总数通常称作 当前实际运行时间。进程的工作集可以被称为在过去的 t 秒实际运行时间中它所访问过的页面集合。

下面来简单描述一下工作集的页面置换算法，基本思路就是找出一个不在工作集中的页面并淘汰它。下面是一部分机器页表：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205650_59212.jpg)

因为只有那些在内存中的页面才可以作为候选者被淘汰，所以该算法忽略了那些不在内存中的页面。每个表项至少包含两条信息：上次使用该页面的近似时间和 R（访问）位。空白的矩形表示该算法不需要其他字段，例如页框数量、保护位、修改位。

算法的工作流程如下，假设硬件要设置 R 和 M 位。同样的，在每个时钟周期内，一个周期性的时钟中断会使软件清除 Referenced(引用)位。在每个缺页异常，页表会被扫描以找出一个合适的页面把它置换。

随着每个页表项的处理，都需要检查 R 位。如果 R 位是 1，那么就会将当前时间写入页表项的 上次使用时间域，表示的意思就是缺页异常发生时页面正在被使用。因为页面在当前时钟周期内被访问过，那么它应该出现在工作集中而不是被删除（假设 t 是横跨了多个时钟周期）。

如果 R 位是 0 ，那么在当前的时钟周期内这个页面没有被访问过，应该作为被删除的对象。为了查看是否应该将其删除，会计算其使用期限（当前虚拟时间 - 上次使用时间），来用这个时间和 t 进行对比。如果使用期限大于 t，那么这个页面就不再工作集中，而使用新的页面来替换它。然后继续扫描更新剩下的表项。

然而，如果 R 位是 0 但是使用期限小于等于 t，那么此页应该在工作集中。此时就会把页面临时保存起来，但是会记生存时间最长（即上次使用时间的最小值）的页面。如果扫描完整个页表却没有找到适合被置换的页面，也就意味着所有的页面都在工作集中。在这种情况下，如果找到了一个或者多个 R = 0 的页面，就淘汰生存时间最长的页面。最坏的情况下是，在当前时钟周期内，所有的页面都被访问过了（也就是都有 R = 1），因此就随机选择一个页面淘汰，如果有的话最好选一个未被访问的页面，也就是干净的页面。

### **4.9 工作集时钟页面置换算法**

当缺页异常发生后，需要扫描整个页表才能确定被淘汰的页面，因此基本工作集算法还是比较浪费时间的。一个对基本工作集算法的提升是基于时钟算法但是却使用工作集的信息，这种算法称为WSClock(工作集时钟)。由于它的实现简单并且具有高性能，因此在实践中被广泛应用。

与时钟算法一样，所需的数据结构是一个以页框为元素的循环列表，就像下面这样

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205651_28122.jpg)

工作集时钟页面置换算法的操作：a) 和 b) 给出 R = 1 时所发生的情形；c) 和 d) 给出 R = 0 的例子。

最初的时候，该表是空的。当装入第一个页面后，把它加载到该表中。随着更多的页面的加入，它们形成一个环形结构。每个表项包含来自基本工作集算法的上次使用时间，以及 R 位（已标明）和 M 位（未标明）。

与时钟算法一样，在每个缺页异常时，首先检查指针指向的页面。如果 R 位被是设置为 1，该页面在当前时钟周期内就被使用过，那么该页面就不适合被淘汰。然后把该页面的 R 位置为 0，指针指向下一个页面，并重复该算法。该事件序列化后的状态参见图 b。

现在考虑指针指向的页面 R = 0 时会发生什么，参见图 c，如果页面的使用期限大于 t 并且页面为被访问过，那么这个页面就不会在工作集中，并且在磁盘上会有一个此页面的副本。申请重新调入一个新的页面，并把新的页面放在其中，如图 d 所示。另一方面，如果页面被修改过，就不能重新申请页面，因为这个页面在磁盘上没有有效的副本。为了避免由于调度写磁盘操作引起的进程切换，指针继续向前走，算法继续对下一个页面进行操作。毕竟，有可能存在一个老的，没有被修改过的页面可以立即使用。

原则上来说，所有的页面都有可能因为磁盘I/O 在某个时钟周期内被调度。为了降低磁盘阻塞，需要设置一个限制，即最大只允许写回 n 个页面。一旦达到该限制，就不允许调度新的写操作。

那么就有个问题，指针会绕一圈回到原点的，如果回到原点，它的起始点会发生什么？这里有两种情况：

- 至少调度了一次写操作
- 没有调度过写操作

在第一种情况中，指针仅仅是不停的移动，寻找一个未被修改过的页面。由于已经调度了一个或者多个写操作，最终会有某个写操作完成，它的页面会被标记为未修改。置换遇到的第一个未被修改过的页面，这个页面不一定是第一个被调度写操作的页面，因为硬盘驱动程序为了优化性能可能会把写操作重排序。

对于第二种情况，所有的页面都在工作集中，否则将至少调度了一个写操作。由于缺乏额外的信息，最简单的方法就是置换一个未被修改的页面来使用，扫描中需要记录未被修改的页面的位置，如果不存在未被修改的页面，就选定当前页面并把它写回磁盘。

### **4.10页面置换算法小结**

我们到现在已经研究了各种页面置换算法，现在我们来一个简单的总结，算法的总结归纳如下

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/12/20221212205651_19059.jpg)

最优算法在当前页面中置换最后要访问的页面。不幸的是，没有办法来判定哪个页面是最后一个要访问的，因此实际上该算法不能使用。然而，它可以作为衡量其他算法的标准。

- NRU 算法根据 R 位和 M 位的状态将页面氛围四类。从编号最小的类别中随机选择一个页面。NRU 算法易于实现，但是性能不是很好。存在更好的算法。
- FIFO 会跟踪页面加载进入内存中的顺序，并把页面放入一个链表中。有可能删除存在时间最长但是还在使用的页面，因此这个算法也不是一个很好的选择。
- 第二次机会算法是对 FIFO 的一个修改，它会在删除页面之前检查这个页面是否仍在使用。如果页面正在使用，就会进行保留。这个改进大大提高了性能。
- 时钟 算法是第二次机会算法的另外一种实现形式，时钟算法和第二次算法的性能差不多，但是会花费更少的时间来执行算法。
- LRU 算法是一个非常优秀的算法，但是没有特殊的硬件(TLB)很难实现。如果没有硬件，就不能使用 LRU 算法。
- NFU 算法是一种近似于 LRU 的算法，它的性能不是非常好。
- 老化 算法是一种更接近 LRU 算法的实现，并且可以更好的实现，因此是一个很好的选择
- 最后两种算法都使用了工作集算法。工作集算法提供了合理的性能开销，但是它的实现比较复杂。WSClock 是另外一种变体，它不仅能够提供良好的性能，而且可以高效地实现。

总之，**最好的算法是老化算法和WSClock算法**。他们分别是基于 LRU 和工作集算法。他们都具有良好的性能并且能够被有效的实现。还存在其他一些好的算法，但实际上这两个可能是最重要的。

原文链接：https://zhuanlan.zhihu.com/p/450505454

原文作者：零声Github整理库



# 【NO.30】为什么要使用 TCP keepalive？C/C++代码实现TCP keepalive

为了理解 TCP keepalive的作用。我们需要清楚，当TCP的Peer A ，Peer B 两端建立了连接之后，如果一端突然拔掉网线或拔掉电源时，怎么检测到拔掉网线或者拔掉电源、链路不通？原因是在需要长连接的网络通信程序中，经常需要心跳检测机制，来实现检测对方是否在线或者维持网络连接的需要。

## 1.**什么是 TCP 保活？**

当你建立一个 TCP 连接时，你关联了一组定时器。其中一些计时器处理保活过程。当保活计时器达到零时，向对等方发送一个保活探测数据包，其中没有数据并且 ACK 标志打开。

由于 TCP/IP 规范，可以这样做，作为一种重复的 ACK，并且远程端点将没有参数，因为 TCP 是面向流的协议。另一方面，将收到来自远程主机的回复，没有数据和ACK 集。

如果收到对 keepalive 探测的回复，则可以断言连接仍在运行。事实上，TCP 允许处理流，而不是数据包，因此零长度数据包对用户程序没有危险。

此过程很有用，因为如果其他对等方失去连接（例如通过重新启动），即使没有流量，也会注意到连接已断开。如果对等方未回复 keepalive 探测，可以断言连接不能被视为有效，然后采取正确的操作。

## 2.**为什么要使用 TCP keepalive？**

> 1、检查死节点 2、 防止因网络不活动而断开连接

### 2.1**检查死节点**

想一想 Peer A 和 Peer B 之间的简单 TCP 连接：初始的三次握手，从 A 到 B 的一个 SYN 段，从 B 到 A 的 SYN/ACK，以及从 A 到 B 的最终 ACK。

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/01d38522da7a454ba99a5a295b5c68da~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671181843&x-signature=XWYb0%2B6rCZROBAO%2BihGnohVjRzE%3D)



此时，我们处于稳定状态：连接已建立，现在我们通常会等待有人通过通道发送数据。

那么问题来了：从 B 上拔下电源，它会立即断电，而不会通过网络发送任何信息来通知 A 连接将断开。

从它的角度来看，A 已准备好接收数据，并且不知道 B 已经崩溃。现在恢复B的电源，等待系统重启。A 和 B 现在又回来了，但是当 A 知道与 B 仍然处于活动状态的连接时，B 不知道。当 A 尝试通过死连接向 B 发送数据时，情况自行解决，B 回复 RST 数据包，导致 A 最终关闭连接。

```
    _____                                                     _____
   |     |                                                   |     |
   |  A  |                                                   |  B  |
   |_____|                                                   |_____|
      ^                                                         ^
      |--->--->--->-------------- SYN -------------->--->--->---|
      |---<---<---<------------ SYN/ACK ------------<---<---<---|
      |--->--->--->-------------- ACK -------------->--->--->---|
      |                                                         |
      |                                       system crash ---> X
      |
      |                                     system restart ---> ^
      |                                                         |
      |--->--->--->-------------- PSH -------------->--->--->---|
      |---<---<---<-------------- RST --------------<---<---<---|
      |                                                         |
```

Keepalive 可以告诉您何时无法访问另一个对等点，而不会出现误报的风险。

### 2.2**防止因网络不活动而断开连接**

keepalive 的另一个有用目标是防止不活动断开通道。当你在 NAT 代理或防火墙后面时，无缘无故断开连接是一个非常常见的问题。这种行为是由代理和防火墙中实现的连接跟踪过程引起的，它们跟踪通过它们的所有连接。

它们跟踪通过它们的所有连接。由于这些机器的物理限制，它们只能在内存中保留有限数量的连接。最常见和合乎逻辑的策略是保持最新的连接并首先丢弃旧的和不活动的连接。

```
    _____           _____                                     _____
   |     |         |     |                                   |     |
   |  A  |         | NAT |                                   |  B  |
   |_____|         |_____|                                   |_____|
      ^               ^                                         ^
      |--->--->--->---|----------- SYN ------------->--->--->---|
      |---<---<---<---|--------- SYN/ACK -----------<---<---<---|
      |--->--->--->---|----------- ACK ------------->--->--->---|
      |               |                                         |
      |               | <--- connection deleted from table      |
      |               |                                         |
      |--->- PSH ->---| <--- invalid connection                 |
      |               |                                         |
```





## 3.**Linux下使用TCP keepalive**

Linux 内置了对 keepalive 的支持。涉及 keepalive 的过程使用三个用户驱动的变量，可以使用 cat 查看参数值。

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/4f89ffad2bdd43b99f6b1eb43ec5436b~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671181843&x-signature=wqtAo4jq9FJggN%2B75LpzOWdlpSE%3D)



前两个参数以秒表示，最后一个是纯数字。这意味着keepalive 例程在发送第一个keepalive 探测之前等待两个小时（7200 秒），然后每75 秒重新发送一次。如果连续9次没有收到 ACK 响应，则连接被标记为断开。

修改这个值很简单，可以这样修改：

> echo 7000 > /proc/sys/net/ipv4/tcp_keepalive_time echo 40 > /proc/sys/net/ipv4/tcp_keepalive_intvl echo 10 > /proc/sys/net/ipv4/tcp_keepalive_probes

还有另一种访问内核变量的方法，使用 sysctl 命令

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/682d5b1025e64f3285928c13a7a9cce4~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671181843&x-signature=oOoxo9ZNXK9e1xqM0h6VWg%2FT1WA%3D)



## 4.**setsockopt 、getsockopt 函数调用**

在 Linux 操作系统中，我们可以通过代码启用一个 socket 的心跳检测，为特定套接字启用 keepalive 所需要做的就是在套接字本身上设置特定的套接字选项。函数原型如下：

```
int getsockopt(int sockfd, int level, int optname,
                      void *optval, socklen_t *optlen);

int setsockopt(int sockfd, int level, int optname,
                      const void *optval, socklen_t optlen);
                      
```

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/fa0cf9c6bdba47d281ba055f0336d9f6~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671181843&x-signature=Nxx5n9GnJdum1xWTBYc2bRlfdyg%3D)



第一个参数是socket；第二个必须是 SOL_SOCKET，第三个必须是 SO_KEEPALIVE。第四个参数必须是布尔整数值，表示我们要启用该选项，而最后一个是之前传递的值的大小。

在编写应用程序时，还可以为 keepalive 设置其他三个套接字选项。它们都使用 SOL_TCP 级别而不是 SOL_SOCKET，并且它们仅针对当前套接字覆盖系统范围的变量。如果不先写入就读取，将返回当前系统范围的参数。

```
TCP_KEEPCNT：覆盖 tcp_keepalive_probes
TCP_KEEPIDLE：覆盖 tcp_keepalive_time
TCP_KEEPINTVL：覆盖 tcp_keepalive_intvl   
```

## 5.**TCP keepalive 代码实现**

在写TCP keepalive 服务程序时，除了要处理SIGPIPE外，还要有客户端连接检测机制，用于及时发现崩溃的客户端连接。我们使用TCP的 keepalive 机制方式。

tcp_keepalive_client：

```
int main(int argc, char *argv[])
{
 kat_arg0 = basename(argv[0]);
 bzero(&cp, sizeof (cp));
 cp.cp_keepalive = 1;
 cp.cp_keepidle = -1;
 cp.cp_keepcnt = -1;
 cp.cp_keepintvl = -1;

 while ((c = getopt(argc, argv, ":c:d:i:")) != -1) {
  switch (c) {
  case 'c':
   cp.cp_keepcnt = parse_positive_int_option(
       optopt, optarg);
   break;

  case 'd':
   cp.cp_keepidle = parse_positive_int_option(
       optopt, optarg);
   break;

  case 'i':
   cp.cp_keepintvl = parse_positive_int_option(
       optopt, optarg);
   break;
  
  case ':':
   warnx("option requires an argument: -%c", optopt);
   usage();
   break;

  case '?':
   warnx("unrecognized option: -%c", optopt);
   usage();
   break;
  }
 }

 if (optind > argc - 1) {
  warnx("missing required arguments");
  usage();
 }

 ipport = argv[optind++];
 if (parse_ip4port(ipport, &cp.cp_ip) == -1) {
  warnx("invalid IP/port: \"%s\"", ipport);
  usage();
 }

 (void) fprintf(stderr, "going connect to: %s port %d\n",
     inet_ntoa(cp.cp_ip.sin_addr), ntohs(cp.cp_ip.sin_port));
 (void) fprintf(stderr, "set SO_KEEPALIVE  = %d\n", cp.cp_keepalive);
 (void) fprintf(stderr, "set TCP_KEEPIDLE  = %d\n", cp.cp_keepidle);
 (void) fprintf(stderr, "set TCP_KEEPCNT   = %d\n", cp.cp_keepcnt);
 (void) fprintf(stderr, "set TCP_KEEPINTVL = %d\n", cp.cp_keepintvl);
 rv = connectandwait(&cp);
 return (rv == 0 ? EXIT_SUCCESS : EXIT_FAILURE);
}
```

tcp_keepalive_server:

```
int main(int argc, char *argv[] )
{

   /* 创建套接字 */
   if((listen_sock = socket(PF_INET, SOCK_STREAM, IPPROTO_TCP)) < 0) {
      perror("socket()");
      exit(EXIT_FAILURE);
   }

   /* 检查 keepalive 选项的状态  */
   if(getsockopt(listen_sock, SOL_SOCKET, SO_KEEPALIVE, &optval, &optlen) < 0) {
      perror("getsockopt()");
      close(listen_sock);
      exit(EXIT_FAILURE);
   }
   printf("SO_KEEPALIVE default is %s\n", (optval ? "ON" : "OFF"));

   /* 将选项设置为活动  */
   optval = 1;
   optlen = sizeof(optval);
   
   if(setsockopt(listen_sock, SOL_SOCKET, SO_KEEPALIVE, &optval, optlen) < 0) {
      perror("setsockopt()");
      close(listen_sock);
      exit(EXIT_FAILURE);
   }
   printf("SO_KEEPALIVE set on socket\n");

   /* 再次检查状态  */
   if(getsockopt(listen_sock, IPPROTO_TCP, TCP_KEEPIDLE, &optval, &optlen) < 0) {
      perror("getsockopt()");
      close(listen_sock);
      exit(EXIT_FAILURE);
   }
   printf("TCP_KEEPIDLE is %d\n", optval );
      /* 再次检查状态  */
   if(getsockopt(listen_sock, IPPROTO_TCP, TCP_KEEPCNT, &optval, &optlen) < 0) {
      perror("getsockopt()");
      close(listen_sock);
      exit(EXIT_FAILURE);
   }
   printf("TCP_KEEPCNT is %d\n", optval);
      /* 再次检查状态  */
   if(getsockopt(listen_sock, IPPROTO_TCP, TCP_KEEPINTVL, &optval, &optlen) < 0) {
      perror("getsockopt()");
      close(listen_sock);
      exit(EXIT_FAILURE);
   }
   printf("TCP_KEEPINTVL is %d\n", optval );
  /* 初始化套接字结构 */
   bzero((char *) &serv_addr, sizeof(serv_addr));
   int portno = atoi(argv[1]);
   serv_addr.sin_family = AF_INET;
   serv_addr.sin_addr.s_addr = INADDR_ANY;
   serv_addr.sin_port = htons(portno);
 
  ...
 }  
```

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/50f37046a649496f9414e264c55deb46~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671181843&x-signature=Fddke5AElR3o1FQdWOgjFo%2BxBPA%3D)



![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/e90c0b550c4f4f46b2f080b0b1dae778~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671181843&x-signature=U%2BseS5xxifaZW5B3eHHT8qDfW1E%3D)



程序创建一个 TCP 套接字并将 SO_KEEPALIVE 套接字选项设置为 1。如果指定了“-c”、“-d”和“-i”选项中的任何一个，则设置 TCP_KEEPCNT、TCP_KEEPIDLE 和 TCP_KEEPINTVL 套接字选项 在相应选项参数的套接字上。

通过测试程序，我们可以使用tcpdump、或者tshark是命令行抓包工具，来分析KeepAlive。

> tshark -nn -i lo port 5050 tcpdump -nn -i lo port 5050

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/a5f21e0834ce41758072103b590492eb~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671181843&x-signature=IixWKqiIJ%2B5w%2BXnNxyf3xpwwywE%3D)

tcpdump -nn -i lo port 5050

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/005f8b716eb241d38f6caf6e3c268d38~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671181843&x-signature=ZhHaF%2B27r%2FmTlAEFVYEDwaiC2SQ%3D)

整个keepalive过程很简单，就是client给server发送一个包，server返回给用户一个包。注意包内没有数据，只有ACK标识 被打开。

ps -aux | grep tcp_keepalive

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/f74a2b1225c741e9940c08c8d576741e~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671181843&x-signature=AB3ae07MMFMuLjZ%2FGPOXiKR1ywg%3D)

## 6.**总结**

keepalive 是一个设备向另一个设备发送的消息，用于检查两者之间的链路是否正在运行，或防止链路中断。

原文链接：https://www.toutiao.com/article/7166152475840561704/    

作者：linux技术栈

# 【NO.31】最浅显易懂的 NAT 原理解析，看不懂来打我

## 1 概述

### 1.1 简介

#### 1.1.1 名词解释

公有IP地址：也叫全局地址，是指合法的IP地址，它是由NIC（网络信息中心）或者ISP(网络服务提供商)分配的地址，对外代表一个或多个内部局部地址，是全球统一的可寻 址的地址。

私有IP地址：也叫内部地址，属于非注册地址，专门为组织机构内部使用。因特网分配编号委员会（IANA）保留了3块IP地址做为私有IP地址：

10.0.0.0 ——— 10.255.255.255

172.16.0.0——— 172.16.255.255

192.168.0.0———192.168.255.255

地址池：地址池是有一些外部地址（全球唯一的IP地址）组合而成，我们称这样的一个地址集合为地址池。在内部网络的数据包通过地址转换到达外部网络时，将会在地址池中选择某个IP地址作为数据包的源IP地址，这样可以有效的利用用户的外部地址，提高访问外部网络的能力。

#### 1.1.2关于NAT

NAT英文全称是“Network Address Translation”，中文意思是“网络地址转换”，它是一个IETF(Internet Engineering Task Force, Internet工程任务组)标准，允许一个整体机构以一个公用IP（Internet Protocol）地址出现在Internet上。顾名思义，它是一种把内部私有网络地址（IP地址）翻译成合法网络IP地址的技术，如下图所示。因此我们可以认为，NAT在一定程度上，能够有效的解决公网地址不足的问题。

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/f1a2ef7832ce4fff88e7c094b33172a0~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671191078&x-signature=8zaNEjYETWnHh4a1Hj9RYCW3sXI%3D)



简单地说，NAT就是在局域网内部网络中使用内部地址，而当内部节点要与外部网络进行通讯时，就在网关（可以理解为出口，打个比方就像院子的门一样）处，将内部地址替换成公用地址，从而在外部公网（internet）上正常使用，NAT可以使多台计算机共享Internet连接，这一功能很好地解决了公共 IP地址紧缺的问题。通过这种方法，可以只申请一个合法IP地址，就把整个局域网中的计算机接入Internet中。这时，NAT屏蔽了内部网络，所有内部网计算机对于公共网络来说是不可见的，而内部网计算机用户通常不会意识到NAT的存在。如下图所示。这里提到的内部地址，是指在内部网络中分配给节点的私有IP地址，这个地址只能在内部网络中使用，不能被路由转发。

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/8c67178ae7f7464986ade76aa6aba348~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671191078&x-signature=K6cnyOnyA23PIikn037Je6Gd5Oc%3D)



NAT 功能通常被集成到路由器、防火墙、ISDN路由器或者单独的NAT设备中。比如Cisco路由器中已经加入这一功能，网络管理员只需在路由器的IOS中设置NAT功能，就可以实现对内部网络的屏蔽。再比如防火墙将WEB Server的内部地址192.168.1.1映射为外部地址202.96.23.11，外部访问202.96.23.11地址实际上就是访问访问 192.168.1.1。此外，对于资金有限的小型企业来说，现在通过软件也可以实现这一功能。Windows 98 SE、Windows 2000 都包含了这一功能。

### 1.2 分类

NAT有三种类型：静态NAT(Static NAT)、动态地址NAT(Pooled NAT)、网络地址端口转换NAPT（Port-Level NAT）。

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/0d50b5297e35456a9acfcb217a6154b4~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671191078&x-signature=9u6EXi1Ps5og1ix3j7%2Bon2jXLf4%3D)



#### 1.2.1静态NAT

通过手动设置，使 Internet 客户进行的通信能够映射到某个特定的私有网络地址和端口。如果想让连接在 Internet 上的计算机能够使用某个私有网络上的服务器（如网站服务器）以及应用程序（如游戏），那么静态映射是必需的。静态映射不会从 NAT 转换表中删除。 　　如果在 NAT 转换表中存在某个映射，那么 NAT 只是单向地从 Internet 向私有网络传送数据。这样，NAT 就为连接到私有网络部分的计算机提供了某种程度的保护。但是，如果考虑到 Internet 的安全性，NAT 就要配合全功能的防火墙一起使用。

对于以上网络拓扑图，当内网主机 10.1.1.1如果要与外网的主机201.0.0.11通信时，主机（IP：10.1.1.1）的数据包经过路由器时，路由器通过查找NAT table 将IP数据包的源IP地址（10.1.1.1）改成与之对应的全局IP地址（201.0.0.1），而目标IP地址201.0.0.11保持不变，这样，数据包就能到达201.0.0.11。而当主机HostB(IP:201.0.0.11) 响应的数据包到达与内网相连接的路由器时，路由器同样查找NAT table，将IP数据包的目的IP 地址改成10.1.1.1，这样内网主机就能接收到外网主机发过来的数据包。在静态NAT方式中，内部的IP地址与公有IP地址是一种一一对应的映射关系，所以，采用这种方式的前提是，机构能够申请到足够多的全局IP地址。

#### 1.2.2 动态NAT

动态地址NAT只是转换IP地址，它为每一个内部的IP地址分配一个临时的外部IP地址，主要应用于拨号，对于频繁的远程联接也可以采用动态NAT。当远程用户联接上之后，动态地址NAT就会分配给他一个IP地址，用户断开时，这个IP地址就会被释放而留待以后使用。

动态NAT方式适合于 当机构申请到的全局IP地址较少，而内部网络主机较多的情况。内网主机IP与全局IP地址是多对一的关系。当数据包进出内网时，具有NAT功能的设备对IP数据包的处理与静态NAT的一样，只是NAT table表中的记录是动态的，若内网主机在一定时间内没有和外部网络通信，有关它的IP地址映射关系将会被删除，并且会把该全局IP地址分配给新的IP数据包使用，形成新的NAT table映射记录。

#### 1.2.3网络地址端口转换NAPT

网络地址端口转换NAPT（Network Address Port Translation）则是把内部地址映射到外部网络的一个IP地址的不同端口上。它可以将中小型的网络隐藏在一个合法的IP地址后面。NAPT与 动态地址NAT不同，它将内部连接映射到外部网络中的一个单独的IP地址上，同时在该地址上加上一个由NAT设备选定的端口号。

NAPT是使用最普遍的一种转换方式，它又包含两种转换方式：SNAT和DNAT。

(1)源NAT（Source NAT，SNAT）：修改数据包的源地址。源NAT改变第一个数据包的来源地址，它永远会在数据包发送到网络之前完成，数据包伪装就是一具SNAT的例子。

(2)目的NAT（Destination NAT，DNAT）：修改数据包的目的地址。Destination NAT刚好与SNAT相反，它是改变第一个数据包的目的地地址，如平衡负载、端口转发和透明代理就是属于DNAT。

源NAT举例：对于以上网络拓扑图，内网的主机数量比较多，但是该组织只有一个合法的IP地址，当内网主机（10.1.1.3）往外发送数据包时，则需要修改数据包的IP地址和TCP/UDP端口号，例如将

源IP：10.1.1.3

源port：1493

改成

源IP：201.0.0.1

源port：1492（注意：源端口号可以与原来的一样也可以不一样）

当外网主机（201.0.0.11）响应内网主机（10.1.1.3）时，应将：

目的IP：201.0.0.1

目的port：1492

改成

目的IP：10.1.1.3

目的port：1493

这样，通过修改IP地址和端口的方法就可以使内网中所有的主机都能访问外网，此类NAT适用于组织或机构内只有一个合法的IP地址的情况，也是动态NAT的一种特例。

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/64f4e8ed8a524ca28eab6b50461d725a~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671191078&x-signature=ZIcoSk%2F3o3K%2BcV0wvG8wBWthE%2FU%3D)



要为外网提供某些服务的情况。例如以上拓扑结构，内网服务器群（ip地址分别为：10.1.1.1,10.1.1.2,10.1.1.3等）需要为外网提供WEB 服务，当外网主机HostB访问内网时，所发送的数据包的目的IP地址为10.1.1.127，端口号为：80，当该数据包到达内网连接的路由器时，路由器查找NAT table，路由器通过修改目的IP地址和端口号，将外网的数据包平均发送到不同的主机上（10.1.1.1,10.1.1.2,10.1.1.3等），这样就实现了负载均衡。



## 2 NAT原理

### 2.1 地址转换

NAT的基本工作原理是，当私有网主机和公共网主机通信的IP包经过NAT网关时，将IP包中的源IP或目的IP在私有IP和NAT的公共IP之间进行转换。

如下图所示，NAT网关有2个网络端口，其中公共网络端口的IP地址是统一分配的公共 IP，为202.20.65.5；私有网络端口的IP地址是保留地址为192.168.1.1。私有网中的主机192.168.1.2向公共网中的主机202.20.65.4发送了1个IP包(Dst=202.20.65.4,Src=192.168.1.2)。

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/4e91c23b5b2d408bb5e14fdf765b8e67~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671191078&x-signature=ca9B16BuYNiDW4FJNc0B32i1AEg%3D)



源IP转换为NAT Gateway的公共IP并转发到公共网，此时IP包（Dst=202.20.65.4，Src=202.20.65.5）中已经不含任何私有网IP的信息。由于IP包的源IP已经被转换成NAT Gateway的公共IP，Web Server发出的响应IP包（Dst= 202.20.65.5,Src=202.20.65.4）将被发送到NAT Gateway。

这时，NAT Gateway会将IP包的目的IP转换成私有网中主机的IP，然后将IP包（Des=192.168.1.2，Src=202.20.65.4）转发到私有网。对于通信双方而言，这种地址的转换过程是完全透明的。转换示意图如下。

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/e3e58bcdc1224827ba7b9c237895286c~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671191078&x-signature=Ie9cmC32xBWZ9CrisVdPh5Xww88%3D)



Web Server收到请求包，回复的响应包中的目的地址就是私有网络IP地址，在Internet上无法正确送达，导致连接失败。

### 2.2 连接跟踪

在上述过程中，NAT Gateway在收到响应包后，就需要判断将数据包转发给谁。此时如果子网内仅有少量客户机，可以用静态NAT手工指定；但如果内网有多台客户机，并且各自访问不同网站，这时候就需要连接跟踪（connection track）。如下图所示：

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/2aeca7be71d44c09a454d8017b35df58~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671191078&x-signature=0gCVR4EgOOpHikCPtC8w4RIeY%2FQ%3D)



在NAT Gateway收到客户机发来的请求包后，做源地址转换，并且将该连接记录保存下来，当NAT Gateway收到服务器来的响应包后，查找Track Table，确定转发目标，做目的地址转换，转发给客户机。

### 2.3 端口转换

以上述客户机访问服务器为例，当仅有一台客户机访问服务器时，NAT Gateway只须更改数据包的源IP或目的IP即可正常通讯。但是如果Client A和Client B同时访问Web Server，那么当NAT Gateway收到响应包的时候，就无法判断将数据包转发给哪台客户机，如下图所示。

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/bc5d73d46d1a491eb24e0f478d0b6644~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671191078&x-signature=O9T5dAyoSWRag8MGqnNPxSOfnU8%3D)



此时，NAT Gateway会在Connection Track中加入端口信息加以区分。如果两客户机访问同一服务器的源端口不同，那么在Track Table里加入端口信息即可区分，如果源端口正好相同，那么在实行SNAT和DNAT的同时对源端口也要做相应的转换，如下图所示。

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/9c6fde98a9574ad9beb0e1666629f61c~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671191078&x-signature=eQwVNddkLgnQYo5aJc1SjdeyVo8%3D)



NAT主要可以实现以下几个功能：数据包伪装、平衡负载、端口转发和透明代理。

- 数据伪装: 可以将内网数据包中的地址信息更改成统一的对外地址信息，不让内网主机直接暴露在因特网上，保证内网主机的安全。同时，该功能也常用来实现共享上网。例如，内网主机访问外网时，为了隐藏内网拓扑结构，使用全局地址替换私有地址。
- 端口转发: 当内网主机对外提供服务时，由于使用的是内部私有IP地址，外网无法直接访问。因此，需要在网关上进行端口转发，将特定服务的数据包转发给内网主机。例如公司小王在自己的服务器上架设了一个Web网站，他的IP地址为192.168.0.5，使用默认端口80，现在他想让局域网外的用户也能直接访问他的Web站点。利用NAT即可很轻松的解决这个问题，服务器的IP地址为210.59.120.89，那么为小王分配一个端口，例如81，即所有访问210.59.120.89:81的请求都自动转向192.168.0.5:80，而且这个过程对用户来说是透明的。
- 负载平衡:目的地址转换NAT可以重定向一些服务器的连接到其他随机选定的服务器。例如1.2.3所讲的目的NAT的例子。
- 失效终结:目的地址转换NAT可以用来提供高可靠性的服务。如果一个系统有一台通过路由器访问的关键服务器，一旦路由器检测到该服务器当机，它可以使用目的地址转换NAT透明的把连接转移到一个备份服务器上，提高系统的可靠性。
- 透明代理:例如自己架设的服务器空间不足，需要将某些链接指向存在另外一台服务器的空间；或者某台计算机上没有安装IIS服务，但是却想让网友访问该台计算机上的内容，这个时候利用IIS的Web站点重定向即可轻松的帮助我们搞定。

## 4 NAT的缺陷

NAT在最开始的时候是非常完美的，但随着网络的发展，各种新的应用层出不穷，此时NAT也暴露出了缺点。NAT的缺陷主要表现在以下几方面：

- (1) 不能处理嵌入式IP地址或端口

NAT设备不能翻译那些嵌入到应用数据部分的IP地址或端口信息，它只能翻译那种正常位于IP首部中的地址信息和位于TCP/UDP首部中的端口信息，如下图,由于对方会使用接收到的数据包中嵌入的地址和端口进行通信，这样就可能产生连接故障，如果通信双方都是使用的公网IP，这不会造成什么问题，但如果那个嵌入式地址和端口是内网的，显然连接就不可能成攻，原因就如开篇所说的一样。MSN Messenger的部分功能就使用了这种方式来传递IP和端口信息，这样就导致了NAT设备后的客户端网络应用程序出现连接故障。

![img](https://p3-sign.toutiaoimg.com/tos-cn-i-qvj2lq49k0/6b7996cb75c6413fa0487adf2786bed8~noop.image?_iz=58558&from=article.pc_detail&x-expires=1671191078&x-signature=%2Ft0fpvNNhxmL33bEC4zXUg1zCB4%3D)



- (2) 不能从公网访问内部网络服务

由于内网是私有IP，所以不能直接从公网访问内部网络服务，比如WEB服务，对于这个问题，我们可以采用建立静态映射的方法来解决。比如有一条静态映射，是把218.70.201.185:80与192.168.0.88:80映射起的，当公网用户要访问内部WEB服务器时，它就首先连接到218.70.201.185:80，然后NAT设备把请求传给192.168.0.88:80，192.168.0.88把响应返回NAT设备，再由NAT设备传给公网访问用户。

- (3) 有一些应用程序虽然是用A端口发送数据的，但却要用B端口进行接收，不过NAT设备翻译时却不知道这一点，它仍然建立一条针对A端口的映射，结果对方响应的数据要传给B端口时，NAT设备却找不到相关映射条目而会丢弃数据包。
- (4) 一些P2P应用在NAT后无法进行 对于那些没有中间服务器的纯P2P应用（如电视会议，娱乐等）来说，如果大家都位于NAT设备之后，双方是无法建立连接的。因为没有中间服务器的中转，NAT设备后的P2P程序在NAT设备上是不会有映射条目的，也就是说对方是不能向你发起一个连接的。现在已经有一种叫做P2P NAT穿越的技术来解决这个问题。

## 5.结语

NAT技术无可否认是在ipv4地址资源的短缺时候起到了缓解作用；在减少用户申请ISP服务的花费和提供比较完善的负载平衡功能等方面带来了不少好处。但是在ipv4地址在以后几年将会枯竭，NAT技术不能改变ip地址空间不足的本质。然而在安全机制上也潜在着威胁，在配置和管理上也是一个挑战。如果要从根本上解决ip地址资源的问题，ipv6才是最根本之路。在ipv4转换到ipv6的过程中，NAT技术确实是一个不错的选择，相对其他的方案优势也非常明显。

原文链接：https://zhuanlan.zhihu.com/p/577080876    

作者：linux

# 【NO.32】谈谈 C/C++ 和 JAVA 哪个更有前景！

## 1.**概述**

在学校或者已经踏入职场的朋友们，相信很多朋友们都会被这个问题所困扰，到底是选C++还是选JAVA？C++和JAVA哪个更有前途呢？作为一个学习计算机编程的人士来说，回头看，发现任何编程语言都是优美的。C，C++，JAVA从流行的程度来看，基本并驾齐驱，无论从从一年的排名，都基本处于前TOP5，至于说那个发展前景好，这个是仁者见仁，智者见智。可以说存在即符合逻辑。我们换个角度来看问题吧。

![img](https://pic4.zhimg.com/80/v2-af58d69165d3c408bc076529204e9997_720w.webp)

## 2.**C++编程语言的应用与发展**

从事嵌入式开发十几年了，从入行开始就使用C/C++写代码，现在市场整体感觉已经不如当初那么火爆了，编程语言现在向着两个大趋势发展。其一高度集成化，利用现成的类库实现一个基本功能只需要很短的代码就够了，充分挖掘这些年编程行业积累的经验和成功；其二对于一些性能要求比较高的行业，C/C++依然占据着非常大的空间，但相比第一种已经有明显的差距，像现在人工智能出现推动了python的发展，但在一些关键的算法模块，底层实现还是要依靠C/C++这种性能非常强的编程语言。

![img](https://pic1.zhimg.com/80/v2-735dcb83bf18d7172bc73446c68b094c_720w.webp)

### 2.1**C/C++的应用主要有以下几方面**

- 服务器端开发

很多游戏或者互联网公司的后台服务器程序都是基于C++开发的，而且大部分是linux操作系统，所以说，你如果想做这样的工作，需要熟悉linux操作系统及其在上面的开发，熟悉数据库开发，精通网络编程。

- 游戏开发

市面上相当多的游戏引擎都是基于C++开发的，比如Cocos2d、虚幻4等，这些游戏引擎的代表作有失落的方舟、绝地求生、地狱之刃、中国象棋、雷电传说、俄罗斯方块、保卫萝卜、捕鱼达人、开心消消乐等等。

- 虚拟现实

虚拟现实（VR）是一种可以创建和体验虚拟世界的计算机仿真系统，是利用计算机生成的一种实时动态的三维立体逼真图像，结合VR眼镜，可以在观影、游戏、旅游活动、教学等方面给人一种完美的沉浸体验。C++在这一技术中同样扮演着重要的角色。

- 数字图像处理

比如OpenCV视觉识别技术。

- 网络软件

C++拥有大量成熟的用于网络通信的库，ACE是其中最具有代表性的跨平台库，在许多重要的企业部门甚至是军方都有应用。

- 科学计算

在科学计算领域，FORTRAN是使用最多的语言之一。但是C++凭借先进的数值计算库、泛型编程等优势在这一领域也应用颇多。

- 操作系统

操作系统主要使用的编程语言是C，但是C++凭借其对C的兼容性，面向对象性质也开始在该领域有一席之地。等。

## 3.**JAVA编程语言的应用与发展**

如今，社会正处于互联网快速发展的阶段，不仅是Java程序，而且所有程序员都有很大的差距，尤其是优秀的程序。Java程序应用广泛，可以作为后台服务。它可以作为一个前端页面。安卓开发也是基于Java语言，Java作为一种跨平台语言在某些领域非常有优势。那么Java开发有哪些发展前景?

世界上并没有完美的程序，因为写程序本来就是一个不断追求完美的过程。同样没有一门语言在一诞生就是完美的，不变的唯有变化。Java诞生至今已经不仅仅是一门语言，背后所涵盖的是一个庞大的技术体系。

在过去二十年Java的发展是孤独求败的，在未来，Java也在迎接着各种挑战，这种挑战来自云原生、容器化、也来自其他设计更完善的语言，相信身怀各种绝技的Java会焕发出更加夺目的光彩，重新攀登另一个高峰。

![img](https://pic1.zhimg.com/80/v2-80ecf35e30ae1bbea24be30bb324c3ec_720w.webp)

发展至今，Java不仅是一门编程语言，还是一个由一系列计算机软件和规范组成的技术体系，Java 是几乎所有类型的网络应用程序的基础，也是开发和提供嵌入式和移动应用程序、游戏、基于 Web 的内容和企业软件的全球标准。

从笔记本电脑到数据中心，从游戏控制台到科学超级计算机，从手机到互联网，Java 无处不在！

- 97% 的企业桌面运行 Java
- 美国有 89% 的桌面（或计算机）运行 Java
- 全球有 900 万 Java 开发人员
- 开发人员的头号选择
- 排名第一的部署平台
- 有 30 亿部移动电话运行 Java
- 100% 的蓝光盘播放器附带了 Java
- 有 50 亿张 Java 卡在使用
- 1.25 亿台 TV 设备运行 Java
- 前 5 个原始设备制造商均提供了 Java ME

![img](https://pic4.zhimg.com/80/v2-f142635769a3b21ce8c46ae13f313a7f_720w.webp)

## 4.**综合分析Java开发的最初目的是Web，C++则主要是针对应用软件：**

- C++ 比较面向底层，速度快，而且windows开发非常友好。以后出来做桌面然见，游戏开发，嵌入式 C C++都使用。目前绝大数的大型的网络游戏都是C++开发的，3D游戏则更不用说了。但C++的学习曲线远远陡于JAVA，想擅长精通C++不是简单的事。
- 在系统编程和驱动编程中，更多的使用C/C++，与硬件打交道，C/C++充分利用硬件优势发挥其高效的性能，这方面JAVA就不行了，JAVA还有性能上的先天缺陷（不过现在很多好的JAVA解释器的优化策略非常好，不过就牺牲了可移植性）。
- 如果以后出来想走J2EE，Android App 或者 J2ME等方向，可以学JAVA。JAVA在网络编程中比C++更具有先天的优势，这主要考虑到了JAVA的高可移植性和易开发性。
- JAVA培训现在都模式工厂化了，然而C++从深度上是远超JAVA的。JAVA太过于依赖XML，C++是高风险高灵活高效率，JAVA主要是稳定，仅仅入门的话C++更简单。JAVA能做的C++都能做，C++能做的，JAVA不一定能做。如果一个人精通各种编程语言的话，显然C++给他的发挥空间更大。

总之，不管是学JAVA还是学C++，语言不是核心，但要有一两门非常精通，这样才有市场竞争力。

**至于薪资，术业有专攻，也不能说C++工程师工资就肯定比JAVA高。**

- 精通C/C++ 转换到其他语言是件很轻松的事情。
- 程序是种工具，软件为行业服务，行业的经验也很重要。再者程序员的工资决定于项目的收益，语言在其次。

## 5.最后，分享一个c/c++后端开发的学习知识图谱（摘自零声教育的大纲）

**c++后端开发是一个庞杂的技术栈，因为没有统一的开发框架并且应用行业非常广泛。所有涉猎广泛，这里就把c/c++后端开发的技术点进行整理总结，看完以后，不会让你失望的。**

1. 精进基石
2. 高性能网络设计
3. 基础组建设计
4. 中间件开发
5. 开源框架
6. 性能分析
7. 分布式架构
8. 上线实战



**1、精进基石，分为四个方面（数据结构，设计模式，c++新特性，Linux工程管理）**

数据结构部分

![img](https://pic4.zhimg.com/80/v2-a5e872e3f4ebff5f2e47f2839381a697_720w.webp)

设计模式

![img](https://pic3.zhimg.com/80/v2-03372ea2ce2feba7b108b66afeeb6e86_720w.webp)

C++新特性

![img](https://pic4.zhimg.com/80/v2-e50b7ad8735d347813040f9f90659aff_720w.webp)

linux工程管理

![img](https://pic4.zhimg.com/80/v2-176af3c57e8866f90356046dac8cc39f_720w.webp)

**2. 高性能网络设计（网络编程，网络原理，协程ntyco，用户态协议栈ntytcp）**

网络编程

![img](https://pic4.zhimg.com/80/v2-5426286b52a4957bc46d48b4b9768563_720w.webp)

网络原理

![img](https://pic3.zhimg.com/80/v2-30bacbf284fb94be950b8e3596d8d0aa_720w.webp)

自研框架： 纯c实现的协程（2000行代码）

![img](https://pic3.zhimg.com/80/v2-b6c40b94153b578adcb44fe354126bda_720w.webp)

自研tcp协议栈

![img](https://pic3.zhimg.com/80/v2-55103cd32fabf773893fe9e4ab525bd6_720w.webp)

高性能异步io机制io_uring

![img](https://pic2.zhimg.com/80/v2-b340e0b3ad0f7ceaab837839532e4f11_720w.webp)



**3. 基础组建设计，分为3部分， 池式组件，高性能组件，开源组件**

池式结构

![img](https://pic4.zhimg.com/80/v2-72e17934511f04b79d01387dd94b86af_720w.webp)

高性能组件

![img](https://pic1.zhimg.com/80/v2-21b403e920c42f632a0dee44c271c858_720w.webp)

开源组件

![img](https://pic4.zhimg.com/80/v2-449701cb30c276c67d2623835aa9c6bf_720w.webp)

4、中间件开发专栏

redis

![img](https://pic2.zhimg.com/80/v2-bbd008b47aaad5448750f93ceb95107d_720w.webp)

mysql

![img](https://pic2.zhimg.com/80/v2-411af5395fd23586dadb09a155430a45_720w.webp)

kafka

![img](https://pic3.zhimg.com/80/v2-fd1a3b402cdadbc1c9cfc56aa51f5ce6_720w.webp)

gRPC

![img](https://pic1.zhimg.com/80/v2-a2cea85a67a10a5e8563cf7cd0caef04_720w.webp)

nginx

![img](https://pic4.zhimg.com/80/v2-e8f9b4ca68300f7c797012869cfdfe2f_720w.webp)

**5. 开源框架**

游戏后端开源框架 skynet

![img](https://pic3.zhimg.com/80/v2-ef9162678cafae13eefb9bd94f85c53a_720w.webp)

分布式API网关

![img](https://pic1.zhimg.com/80/v2-70a66668b090577855763d4dbcac07a0_720w.webp)

DPDK

![img](https://pic1.zhimg.com/80/v2-16aa9606a881b6caf0b5765d571adfb0_720w.webp)

高性能计算CUDA

![img](https://pic2.zhimg.com/80/v2-19977aff14842f4756a6cb5d505f6f91_720w.webp)

**6、云原生专栏**

**docker**

![img](https://pic3.zhimg.com/80/v2-94e4caa56462fa1e22e31c92eb8137de_720w.webp)

kubernetes

![img](https://pic2.zhimg.com/80/v2-4f5ab6da7b798ebdde603942cab39481_720w.webp)

**7、性能分析专栏**

**性能与测试工具**

![img](https://pic3.zhimg.com/80/v2-021a78849d8efbf9e55e58e9581f4c6e_720w.webp)

观测技术bpf与ebpf

![img](https://pic3.zhimg.com/80/v2-16af55bef7b272f54e2a52f9f2983fda_720w.webp)

内核源码机制

![img](https://pic3.zhimg.com/80/v2-d7d1469ca1fc6e4bcf75247f47ab0842_720w.webp)



**8、分布式架构专栏**

**rocksdb**

![img](https://pic3.zhimg.com/80/v2-e6ce9f016c9bdf3aa775e593b43199fa_720w.webp)

TiDB

![img](https://pic4.zhimg.com/80/v2-cc8fddf5551aaa0b4aa5155ea529be5b_720w.webp)

分布式服务

![img](https://pic3.zhimg.com/80/v2-f22f724500b4f8e7b4d63bed73f92392_720w.webp)

**9、上线项目实战（可以写入简历的两个实战项目，让面试不再为没有项目发愁）**

**1、图床共享云存储**

**2、微服务即时通讯**

原文地址：https://zhuanlan.zhihu.com/p/590865083  

原文作者：linux

# 【NO.33】一文掌握google开源单元测试框架Google Test

我们在开发的过程中，需要做一些验证测试，来保证我们的代码是按照设计要求工作的，这就需要单元测试了。单元测试（Unit Test），我们称为“UT测试”。对于一个复杂的系统来说，需要编写大量的单元测试用例，有人会觉得这么多的测试代码，将会花费大量的时间，影响开发的进度，会得不偿失。真的是这样吗？其实，对于越是复杂的系统就越是需要单元测试来保证我们的代码的开发质量，及时测试出代码的问题，在开发阶段发现问题总比在系统发布之后发现问题能够较少的节省资源或成本。

对于单元测试应该是每个开发工程师必备的技能，尤其是高阶的开发工程师会更加注重UT的重要性。同时，我们在开发功能模块之前会考虑到测试用例的实现，这样自然的就会考虑到功能模块的模块化便于UT的编写，从这一方面来说也能提高开发人员开发的代码质量。另外，单元测试用例还可以作为示例供开发人员参考，从而能够更轻松的掌握模块的使用。

今天就和大家一起学习一个开源的C++的单元测试框架Google test，大家看名字就知道它是由牛逼的Google公司出品。Google Test可以在多种平台上使用，它可以支持：

> Linux、Mac OS X、Windows、Cygwin、MinGW、Windows Mobile、Symbian、PlatformIO等。

## 1.安装和配置

我们可以从github获取Google Test的源码。

> github下载地址: [https://github.com/google/googletest](https://link.zhihu.com/?target=https%3A//github.com/google/googletest)

因为我们下载到的gTest是源代码，还需要将其编译成库文件再进行使用。下面将和大家一起学习如何在windows环境下生成gTest的库文件。在这之前我们需要安装CMake和MinGW。

将下载的gTest的源码进行解压，源码目录如下图所示。

![img](https://pic2.zhimg.com/80/v2-7b5580b3d8dc932c2d76c48b91c6fa8d_720w.webp)

打开命令行工具cmd，进入源码的工程目录，新建一个build目录用来存放构建文件，然后，进入build目录执行cmake命令生成Makefile文件。

```text
mkdir build
cd build
cmake -G "MinGW Makefiles" ..
```

![img](https://pic3.zhimg.com/80/v2-e0b2d6fae1f4dfc369f7c5c268b81cba_720w.webp)

![img](https://pic1.zhimg.com/80/v2-16d08ea7934f23af7e679e0604f49cf4_720w.webp)

Makefile文件生成后，再执行下面的命令mingw32-make编译库文件。编译成功后就会发现有libgtest.a 和libgtest_main.a两个静态库生成。这里注意，Windows下mingw安装的make工具名称是mingw32-make而不是make。

```text
mingw32-make
```

![img](https://pic3.zhimg.com/80/v2-6ca799fb7be17f7e6d7c433946a23a46_720w.webp)

接下来我们在VS Code写一个测试用例，使用生成的gTest静态库测试下。按下快捷键【Ctrl+Shift+p】，在弹出的搜索框中搜索【C/C++:Edit Configurations】，可以创建c_cpp_properties.json配置文件。

![img](https://pic3.zhimg.com/80/v2-98dccb609639e53d7b43a7eea1d0994e_720w.webp)

在c_cpp_properties.json配置文件添加gTest的头文件目录。

![img](https://pic2.zhimg.com/80/v2-cf8f7d5c0c7c9a16715bcda0dbf480ad_720w.webp)

在task.json配置文件中添加gTest头文件目录和库文件，task.json配置文件可以通过菜单栏中Terminal选项下的【Configure Default Build Task】选项创建。

![img](https://pic2.zhimg.com/80/v2-b8e5517b49abaeea7d07f8bc1922436d_720w.webp)

![img](https://pic3.zhimg.com/80/v2-f1c2ed2f310d64f7e1b3ed82b9593f72_720w.webp)

上面配置好之后，我们写个测试用例跑一下。

```text
#include <iostream>
#include <gtest/gtest.h>

int add(int a, int b)
{
    return a + b;
}

int sub(int a, int b)
{
    return a - b;
}

TEST(testcase, test_add)
{
    EXPECT_EQ(add(1,2), 3);
    EXPECT_EQ(sub(1,2), -1);
}

int main(int argc, char **argv)
{  
    std::cout << "run google test --> " << std::endl << std::endl;
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

运行结果如下图所示，代码中的TEST是一个宏，用来创建测试用例，它有test_case_name和test_name两个参数。分别是测试用例名和测试名，在后面的文章中我们会对其有更深刻的理解，这里就不细说了。RUN_ALL_TESTS也是一个宏，它是测试用例的入口。EXPECT_EQ这个是一个断言相关的宏，用来检测两个数值是否相等。

![img](https://pic4.zhimg.com/80/v2-cd6a9066f65330ba7c79183434cab35f_720w.webp)

## 2.断言

除了上面示例里的EXPECT_EQ，在gTest里有很多断言相关的宏。断言可以检查出某些条件的真假，因此，我们可以通过它来判断被测试的函数的成功与否。这里断言我们主要可以分为两类：

- 以"ASSERT_"开头的断言，致命性断言（Fatal assertion）
- 以"EXPECT_"开头的断言 ，非致命性断言（Nonfatal assertion）

上面的两种断言会在断言条件不满足时会有区别，即当不满足条件时, "ASSERT_"断言会在当前函数终止，而不会继续执行下去；而"EXPECT_"则会继续执行。我们可以通过下面一个例子来理解下他们的区别。

```text
#include <iostream>
#include <gtest/gtest.h>

int add(int a, int b)
{
    return a + b;
}

int sub(int a, int b)
{
    return a - b;
}

TEST(testcase, test_expect)
{
    std::cout << "------ test_expect start-----" << std::endl;

    std::cout << "add function start" << std::endl;
    EXPECT_EQ(add(1,2), 2);
    std::cout << "add function end" << std::endl;

    std::cout << "sub function start" << std::endl;
    EXPECT_EQ(sub(1,2), -1);
    std::cout << "sub function end" << std::endl;

    std::cout << "------ test_expect end-----" << std::endl;
}

TEST(testcase, test_assert)
{

    std::cout << "------ test_assert start-----" << std::endl;

    std::cout << "add function start" << std::endl;
    ASSERT_EQ(add(1,2), 2);
    std::cout << "add function end" << std::endl;

    std::cout << "sub function start" << std::endl;
    ASSERT_EQ(sub(1,2), -1);
    std::cout << "sub function end" << std::endl;

    std::cout << "------ test_assert end-----" << std::endl;
}

int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
}
```

从下面的运行结果上看，assert断言检查出被测函数add不满足条件，所以程序就没有继续执行下去；而expect虽然检查出被测试函数add不满足条件，但是程序还是继续去测试sub函数。

![img](https://pic2.zhimg.com/80/v2-1bb29da307c1538401cae497c54f03a9_720w.webp)

上面的示例用到的都是判断相等条件的断言，还有其他条件检查的断言。主要可以分为布尔检查，数值比较检查，字符串检查，浮点数检查，异常检查等等。下面我们逐一认识这些断言。

## 3.布尔检查

布尔检查主要用来检查布尔类型数据，检查其条件是真还是假。

![img](https://pic2.zhimg.com/80/v2-10bf9d8ab9ef57a35585b70f9c3431e5_720w.webp)

## 4.数值比较检查

数值比较检查主要用来比较两个数值之间的大小关系，这里有两个参数。

![img](https://pic1.zhimg.com/80/v2-579377a5007471a517c98107c2f101dc_720w.webp)

## 5.字符串检查

字符串检查主要用来比较字符串的内容。

![img](https://pic3.zhimg.com/80/v2-7c9703ad92123714ea41fc8c91e75512_720w.webp)

## 6.浮点数检查

对于浮点数来说，因为其精度原因，我们无法确定其是否完全相等，实际上对于浮点数我比较两个浮点数近似相等。

![img](https://pic1.zhimg.com/80/v2-c18380c78fa634142e3db9d98837f360_720w.webp)

## 7.异常检查

异常检查可以将异常转换成断言的形式。

![img](https://pic2.zhimg.com/80/v2-64d6bd623e2aa8ccafbfc4250ec6499d_720w.webp)

除了上面的一些类型的断言，还有一切其他的常用断言。

## 8.显示成功或失败

这一类断言会在测试运行中标记成功或失败。它主要有三个宏：

- SUCCED()：标记成功。
- FAIL() : 标记失败，类似ASSERT断言标记致命错误；
- ADD_FAILURE()：标记，类似EXPECT断言标记非致命错误。

```text
#include <iostream>
#include <gtest/gtest.h>

int divison(int a, int b)
{
    return a / b;
}

TEST(testCaseTest, test0)
{
    std::cout << "start test 0" << std::endl;
    SUCCEED();
    std::cout << "test pass" << std::endl;
}

TEST(testCaseTest, test1)
{
    std::cout << "start test 1" << std::endl;
    FAIL();
    std::cout << "test fail" << std::endl;
}

TEST(testCaseTest, test2)
{
    std::cout << "start test 2" << std::endl;
    ADD_FAILURE();
    std::cout << "test fail" << std::endl;
}


int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

执行结果如下：

![img](https://pic4.zhimg.com/80/v2-c65dd0527dd2d1bb9b8b9a19456250c3_720w.webp)

## 9.死亡测试

死亡测试是用来检测测试程序是否按照预期的方式崩溃。

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='819' height='65'></svg>)

```text
#include <iostream>
#include <gtest/gtest.h>

int divison(int a, int b)
{
    return a / b;
}

TEST(testCaseDeathTest, test_div)
{
    EXPECT_DEATH(divison(1, 0), "");
}
int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

上面这个例子就是死亡测试，其运行结果如下，这里需要注意的是test_case_name如果使用DeathTest为后缀，gTest会优先运行。

![img](https://pic4.zhimg.com/80/v2-6c91f252fa01982c02664fc7e69f5be7_720w.webp)

## 10.测试事件

在学习测试事件之前，我们先来了解下三个概念，它们分别是测试程序，测试套件，测试用例。

- 测试程序是一个可执行程序，它有一个测试程序的入口main函数。
- 测试用例是用来定义需要验证的内容。
- 测试套件是测试用例的集合，运行测试。

我们回过来看测试事件，在GTest中有了测试事件的这个机制，就能能够在测试之前或之后能够做一些准备/清理的操作。根据事件执行的位置不同，我们可将测试事件分为三种：

- TestCase级别测试事件：这个级别的事件会在TestCase之前与之后执行；
- TestSuite级别测试事件：这个级别的事件会在TestSuite中第一个TestCase之前与最后一个TestCase之后执行；
- 全局测试事件：这是级别的事件会在所有TestCase中第一个执行前，与最后一个之后执行。

这些测试事件都是基于类的，所以需要在类上实现。下面我们依次来学习这三种测试事件。

## 11.TestCase测试事件

TestCase测试事件，需要实现两个函数SetUp()和TearDown()。

- SetUp()函数是在TestCase之前执行。
- TearDown()函数是在TestCase之后执行。

这两个函数是不是有点像类的构造函数和析构函数，但是切记他们并不是构造函数和析构函数，只是打个比方才这么说而已。我们可以借助下面的代码示例来加深对它的理解。这两个函数是testing::Test的成员函数，我们在编写测试类时需要继承testing::Test。

```text
#include <iostream>
#include <gtest/gtest.h>

class calcFunction
{
public:
    int add(int a, int b)
    {
        return a + b;
    }

    int sub(int a, int b)
    {
        return a - b;
    }
};

class calcFunctionTest : public testing::Test
{
protected:
    virtual void SetUp()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }
    virtual void TearDown()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }

    calcFunction calc;

};

TEST_F(calcFunctionTest, test_add)
{
    std::cout << "--> test_add start <--" << std::endl;
    EXPECT_EQ(calc.add(1,2), 3);
    std::cout << "--> test_add end <--" << std::endl;
}

TEST_F(calcFunctionTest, test_sub)
{
    std::cout << "--> test_sub start <--" << std::endl;
    EXPECT_EQ(calc.sub(1,2), -1);
    std::cout << "--> test_sub end <--" << std::endl;
}

int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

测试结果如下，两个函数都是是在每个TestCase（test_add和test_sub）之前和之后执行。

![img](https://pic3.zhimg.com/80/v2-a50073b96ec0cbfb70f47c477314731a_720w.webp)

## 12.TestSuite测试事件

TestSuite测试事件，同样的也需要实现的两个函数SetUpTestCase()和TearDownTestCase()，而这两个函数是静态函数。这两个静态函数同样也是testing::Test类的成员，我们直接改写下测试类calcFunctionTest，添加两个静态函数SetUpTestCase()和TearDownTestCase()到测试类中即可。

```text
class calcFunctionTest : public testing::Test
{
protected:
    static void SetUpTestCase()
    {
        std::cout<< "--> " <<  __func__ << " <--" << std::endl;
    }

    static void TearDownTestCase()
    {
        std::cout<< "--> " << __func__ << " <--" << std::endl;
    }

    virtual void SetUp()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }
    virtual void TearDown()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }

    calcFunction calc;

};
```

改写好之后，我们再看一下运行结果。这两个函数分别是在本TestSuite中的第一个TestCase之前和最后一个TestCase之后执行。

![img](https://pic2.zhimg.com/80/v2-f9af1935b96f835960a4921fd2d07a2d_720w.webp)

## 13.全局测试事件

全局测试事件，也需要继承一个类，但是它需要继承testing::Environment类实现SetUp()和TearDown()两个函数。还需要在main函数中调用testing::AddGlobalTestEnvironment方法注册全局事件。我们直接上代码吧！

```text
#include <iostream>
#include <gtest/gtest.h>

class calcFunction
{
public:
    int add(int a, int b)
    {
        return a + b;
    }

    int sub(int a, int b)
    {
        return a - b;
    }
};

class calcFunctionEnvironment : public testing::Environment
{
    public:
        virtual void SetUp()
        {
            val = 123;
            std::cout << "--> Environment " << __func__ << " <--" << std::endl;
        }
        virtual void TearDown()
        {
            std::cout << "--> Environment " << __func__ << " <--" << std::endl;
        }

        int val;
};

calcFunctionEnvironment* calc_env;

class calcFunctionTest : public testing::Test
{
protected:
    static void SetUpTestCase()
    {
        std::cout<< "--> " <<  __func__ << " <--" << std::endl;
    }

    static void TearDownTestCase()
    {
        std::cout<< "--> " << __func__ << " <--" << std::endl;
    }

    virtual void SetUp()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }
    virtual void TearDown()
    {
        std::cout << "--> " << __func__ << " <--" <<std::endl;
    }

    calcFunction calc;

};

TEST_F(calcFunctionTest, test_add)
{
    std::cout << "--> test_add start <--" << std::endl;
    EXPECT_EQ(calc.add(1,2), 3);
    std::cout << "Global Environment val = " << calc_env->val << std::endl;
    std::cout << "--> test_add end <--" << std::endl;
}

TEST_F(calcFunctionTest, test_sub)
{
    std::cout << "--> test_sub start <--" << std::endl;
    EXPECT_EQ(calc.sub(1,2), -1);
    std::cout << "Global Environment val = " << calc_env->val << std::endl;
    std::cout << "--> test_sub end <--" << std::endl;
}

int main(int argc, char **argv)
{  
    calc_env = new calcFunctionEnvironment;
    testing::AddGlobalTestEnvironment(calc_env);

    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

从测试结果上看，全局事件的这两个函数分别是在第一个TestSuite之前和最后一个TestSuite之后执行的。

![img](https://pic3.zhimg.com/80/v2-1bff7a98385fa963b4b01853d2c53766_720w.webp)

以上三种测试事件我们可以根据需要进行灵活使用。另外，细心的同学会发现，这里测试用例我们该用了TEST_F这个宏，这是因为继承了testing::Test,与之对应就需要使用TEST_F宏。

## 14.参数化

在学习gTest参数化之前我们先看一个测试例子。

```text
#include <iostream>
#include <gtest/gtest.h>

class calcFunction
{
public:
    int add(int a, int b)
    {
        std::cout << a << " + " << b << " = " << a + b << std::endl;
        return a + b;
    }

    int sub(int a, int b)
    {
        std::cout << a << " - " << b << " = " << a - b << std::endl;
        return a - b;
    }
};

class calcFunctionTest : public testing::Test
{
protected:
    calcFunction calc;
};

TEST_F(calcFunctionTest, test_add0)
{
    EXPECT_EQ(calc.add(1,2), 3);
}

TEST_F(calcFunctionTest, test_add1)
{
    EXPECT_EQ(calc.add(1,3), 4);
}

TEST_F(calcFunctionTest, test_add2)
{
    EXPECT_EQ(calc.add(2,4), 6);
}

TEST_F(calcFunctionTest, test_add3)
{
    EXPECT_EQ(calc.add(-1,-2), -3);
}

int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

示例执行结果：

![img](https://pic4.zhimg.com/80/v2-4d528b76739ff7ef73f346f794d91103_720w.webp)

上面的测试用例中我们写了多个测试用例，但是其参数都是同样的，有的实际应用场景可能比这个程序写的测试检查还要多。写这么多重复的代码实在是太累了。gTest提供了一个非常友好的工具，将这些测试的值进行参数化，就不用写那么多重复的代码了。

如何对其进行参数化呢？直接上代码，我们再来看下面一个例子。

```text
#include <iostream>
#include <gtest/gtest.h>

class calcFunction
{
public:
    int add(int a, int b)
    {
        std::cout << a << " + " << b << " = " << a + b << std::endl;
        return a + b;
    }

    int sub(int a, int b)
    {
        std::cout << a << " - " << b << " = " << a - b << std::endl;
        return a - b;
    }
};

struct TestParam
{
    int a;
    int b;
    int c;
};

class calcFunctionTest : public ::testing::TestWithParam<struct TestParam>
{
protected:
    calcFunction calc;
    TestParam param;

    virtual void SetUp()
    {
        param.a = GetParam().a;
        param.b = GetParam().b;
        param.c = GetParam().c;
    }

};

TEST_P(calcFunctionTest, test_add)
{
    EXPECT_EQ(calc.add(param.a, param.b), param.c);
}

INSTANTIATE_TEST_CASE_P(addTest, calcFunctionTest, ::testing::Values( TestParam{1, 2 , 3}, 
                                                                      TestParam{1, 3 , 4},
                                                                      TestParam{2, 4 , 6},
                                                                      TestParam{-1, -2 , -3}));

int main(int argc, char **argv)
{  
    testing::InitGoogleTest(&argc, argv);  
    return RUN_ALL_TESTS(); 
} 
```

执行结果和前面的例子一样。

![img](https://pic2.zhimg.com/80/v2-0c563bade7adeb8bc1a21c0eb4b347b5_720w.webp)

从这个例子中，我们不难发现和之前的测试程序有一些不同。这里继承了::testing::TestWithParam类，参数T就是需要参数化的数据类型，这个例子里参数化数据类型是TestParam结构体。这里还需要使用另外一个宏TEST_P而不是TEST_F这个宏，它的两个参数和TEST_F和TEST一致。另外，程序中还增加一个宏INSTANTIATE_TEST_CASE_P用来输入测试参数，它有三个参数（第一个参数大家可任意取名，第二个参数是test_case_name和TEST_P宏的名称一致，第三个参数是需要传递的参数）。

以上就是今天的所有内容，感谢大家耐心的阅读，希望大家都有所收获，愿大家代码无bug。

原文地址：https://zhuanlan.zhihu.com/p/582524287  

作者：Linux

# 【NO.34】深入linux操作系统-malloc到底如何分配内存？

我们知道malloc() 并不是系统调用，也不是运算符，而是 C 库里的函数，用于动态分配内存。

malloc 申请内存的时候，会有两种方式向操作系统申请堆内存：

- 方式一：通过 brk() 系统调用从堆分配内存
- 方式二：通过 mmap() 系统调用在文件映射区域分配内存；

## 1.brk()系统调用

### 1.1 brk()的申请方式

一般如果用户分配的内存小于 128 KB，则通过 brk() 申请内存。而brk()的实现的方式很简单，就是通过 brk() 函数将**堆顶**指针向高地址移动，获得新的内存空间。如下图：

![img](https://pic2.zhimg.com/80/v2-cd2da092c7eee36e4ffd70db6641d04d_720w.webp)

malloc 通过 brk() 方式申请的内存，free 释放内存的时候，并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用，这样就可以重复使用。

### 1.2 brk()系统调用的优缺点

所以使用brk()方式的点很明显：可以减少缺页异常的发生，提高内存访问效率。

但它的缺点也同样明显：由于申请的内存没有归还系统，在内存工作繁忙时，频繁的内存分配和释放会造成内存碎片。brk()方式之所以会产生内存碎片，是由于brk通过移动堆顶的位置来分配内存，并且使用完不会立即归还系统，重复使用，如果高地址的内存不释放，低地址的内存是得不到释放的。

正是由于使用brk()会出现内存碎片，所以在我们申请大块内存的时候才会使用mmap()方式，mmap()是以页为单位进行内存分配和管理的，释放后就直接归还系统了，所以不会出现这种小碎片的情况。

### 1.3 brk()系统调用的优化

**一、Ptmalloc ：**malloc采用的是内存池的管理方式，Ptmalloc 采用边界标记法将内存划分成很多块，从而对内存的分配与回收进行管理。为了内存分配函数malloc的高效性，ptmalloc会预先向操作系统申请一块内存供用户使用，当我们申请和释放内存的时候，ptmalloc会将这些内存管理起来，并通过一些策略来判断是否将其回收给操作系统。这样做的最大好处就是，使用户申请和释放内存的时候更加高效，避免产生过多的内存碎片。

**二、Tcmalloc：**Ptmalloc在性能上还是存在一些问题的，比如不同分配区（arena）的内存不能交替使用，比如每个内存块分配都要浪费8字节内存等等，所以一般倾向于使用第三方的malloc。

Tcmalloc是Google gperftools里的组件之一。全名是 thread cache malloc（线程缓存分配器）其内存管理分为线程内存和中央堆两部分。

**1.小块内部的分配：**对于小块内存分配，其内部维护了60个不同大小的分配器（实际源码中看到的是86个），和ptmalloc不同的是，它的每个分配器的大小差是不同的，依此按8字节、16字节、32字节等间隔开。在内存分配的时候，会找到最小符合条件的，比如833字节到1024字节的内存分配请求都会分配一个1024大小的内存块。如果这些分配器的剩余内存不够了，会向中央堆申请一些内存，打碎以后填入对应分配器中。同样，如果中央堆也没内存了，就向中央内存分配器申请内存。

在线程缓存内的60个分配器分别维护了一个大小固定的自由空间链表，直接由这些链表分配内存的时候是不加锁的。但是中央堆是所有线程共享的，在由其分配内存的时候会加自旋锁(spin lock)。

**2.大内存的分配：**对于大内存分配(大于8个分页, 即32K)，tcmalloc直接在中央堆里分配。中央堆的内存管理是以分页为单位的，同样按大小维护了256个空闲空间链表，前255个分别是1个分页、2个分页到255个分页的空闲空间，最后一个是更多分页的小的空间。这里的空间如果不够用，就会直接从系统申请了。

**3.ptmalloc与tcmalloc的不足：**都是针对小内存分配和管理；对大块内存还是直接用了系统调用。应该尽量避免大内存的malloc/new、free/delete操作。频繁分配小内存，例如：对bool、int、short进行new的时候，造成内存浪费。

**三、Jemalloc：** jemalloc 是由 Jason Evans 在 FreeBSD 项目中引入的新一代内存分配器。它是一个通用的malloc实现，侧重于减少内存碎片和提升高并发场景下内存的分配效率，其目标是能够替代 malloc。下面是Jemalloc的两个重要部分：

**1.arena:arena** 是 jemalloc 最重要的部分，内存由一定数量的 arenas 负责管理。每个用户线程都会被绑定到一个 arena 上，线程采用 round-robin 轮询的方式选择可用的 arena 进行内存分配，为了减少线程之间的锁竞争，默认每个 CPU 会分配 4 个 arena，各个 arena 所管理的内存相互独立。

```text
struct arena_s {
 
	atomic_u_t		nthreads[2];
	tsdn_t		*last_thd;
 
	arena_stats_t		stats;  // arena的状态
 
	ql_head(tcache_t)	tcache_ql;
	ql_head(cache_bin_array_descriptor_t)	cache_bin_array_descriptor_ql;
	malloc_mutex_t				tcache_ql_mtx;
 
	prof_accum_t		prof_accum;
	uint64_t		prof_accumbytes;
 
	atomic_zu_t		offset_state;
 
	atomic_zu_t		extent_sn_next;  // extent的序列号生成器状态
 
	atomic_u_t		dss_prec;   
 
	atomic_zu_t		nactive;    // 激活的extents的page数量
 
	extent_list_t	large;      // 存放 large extent 的 extents
 
	malloc_mutex_t	large_mtx;  // large extent的锁
 
	extents_t extents_dirty;    // 刚被释放后空闲 extent 位于的地方
 
	extents_t extents_muzzy;    // extents_dirty 进行 lazy purge 后位于的地方，dirty -> muzzy
 
	extents_t extents_retained; // extents_muzzy 进行 decommit 或 force purge 后 extent 位于的地方，muzzy -> retained
 
	arena_decay_t	decay_dirty; // dirty --> muzzy 
 
	arena_decay_t	decay_muzzy; // muzzy --> retained 
 
	pszind_t		extent_grow_next;
	pszind_t		retain_grow_limit;
	malloc_mutex_t		extent_grow_mtx;
 
	extent_tree_t		extent_avail;     // heap，存放可用的 extent 元数据
 
	malloc_mutex_t		extent_avail_mtx; // extent_avail的锁
 
	bin_t			bins[NBINS];      // 所有用于分配小内存的 bin
 
	base_t			*base;            // 用于分配元数据的 base
 
	nstime_t		create_time;      // 创建时间
};
```

**2.extent**:管理 jemalloc 内存块（即用于用户分配的内存）的结构，每一个内存块大小可以是 N * page_size(4KB)（N >= 1）。每个 extent 有一个序列号（serial number）。一个 extent 可以用来分配一次 large_class 的内存申请，但可以用来分配多次 small_class 的内存申请。

```text
struct extent_s {
    uint64_t		e_bits; // 8字节长，记录多种信息
 
    void			*e_addr; // 管理的内存块的起始地址
 
    union {
		size_t	e_size_esn; // extent和序列号的大小
		size_t	e_bsize;    // 基本extent的大小
	};
 
    union {
		/* 
         * S位图，当此 extent 用于分配 small_class 内存时，用来记录这个 extent 的分配情况，        
         * 此时每个 extent 的内的小内存称为 region 
         */
		arena_slab_data_t	e_slab_data; 
 
		atomic_p_t		e_prof_tctx; // 一个计数器，用于large object
	};
}
```

## 2.mmap()系统调用

### 2.1 mmap基础概念

mmap 是一种内存映射文件的方法，即将一个文件或者其他对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一映射关系。

实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必调用read,write等系统调用函数。相反，内核空间的这段区域的修改也直接反映用户空间，从而可以实现不同进程的文件共享。如下图所示：

![img](https://pic2.zhimg.com/80/v2-2273391a8f330c3d35588d452136f485_720w.webp)

由上图可以看出，进程的虚拟地址空间，由多个虚拟内存区域构成。虚拟内存区域是进程的虚拟地址空间中的一个同质区间，即具有同样特性的连续地址范围。上图中所示的text数据段、初始数据段、Bss数据段、堆、栈、内存映射，都是一个独立的虚拟内存区域。而为内存映射服务的地址空间处在堆栈之间的空余部分。

linux 内核使用的vm_area_struct 结构来表示一个独立的虚拟内存区域，由于每个不同质的虚拟内存区域功能和内部机制不同；因此同一个进程使用多个vm_area_struct 结构来分别表示不同类型的虚拟内存区域。各个vm_area_struct 结构使用链表或者树形结构链接，方便进程快速访问。如下图所示：

![img](https://pic4.zhimg.com/80/v2-eb6aaaeb166b35c4d9814a5e60ac9573_720w.webp)

vm_area_struct 结构中包含区域起始和终止地址以及其他相关信息，同时也包含一个vm_ops 指针，其内部可引出所有针对这个区域可以使用的系统调用函数。这样，进程对某一虚拟内存区域的任何操作都需要的信息，都可以从vm_area_struct 中获得。mmap函数就是要创建一个新的vm_area_struct结构 ，并将其与文件的物理磁盘地址相连。

### 2.2 mmap 内存映射原理

mmap 内存映射实现过程，总的来说可以分为三个阶段：

（一）进程启动映射过程，并在虚拟地址空间中为映射创建虚拟映射区域

1、进程在用户空间调用函数mmap ，原型：void *mmap(void *start, size_t length, int prot, int flags, int fd, off_t offset);

2、在当前进程虚拟地址空间中，寻找一段空闲的满足要求的连续的虚拟地址

3、为此虚拟区分配一个vm_area_struct 结构，接着对这个结构各个区域进行初始化

4、将新建的虚拟区结构（vm_area_struct）插入进程的虚拟地址区域链表或树中

（二）调用内核空间的系统调用函数mmap （不同于用户空间函数），实现文件物理地址和进程虚拟地址的一一映射关系

5、为映射分配新的虚拟地址区域后，通过待映射的文件指针，在文件描述符表中找到对应的文件描述符，通过文件描述符，链接到内核“已打开文集”中该文件结构体，每个文件结构体维护者和这个已经打开文件相关各项信息。

6、通过该文件的文件结构体，链接到file_operations模块，调用内核函数mmap，其原型为：int mmap(struct file *filp, struct vm_area_struct *vma)，不同于用户空间库函数。

7、内核mmap函数通过虚拟文件系统inode模块定位到文件磁盘物理地址。

8、通过remap_pfn_range函数建立页表，即实现了文件地址和虚拟地址区域的映射关系。此时，这片虚拟地址并没有任何数据关联到主存中。

（三）进程发起对这片映射空间的访问，引发缺页异常，实现文件内容到物理内存（主存）的拷贝。

前两个阶段仅在于创建虚拟区间并完成地址映射，但是并没有将任何文件数据拷贝至主存。真正的文件读取是当进程发起读或者写操作时。

9、进程的读写操作访问虚拟地址空间这一段映射地址后，通过查询页表，先这一段地址并不在物理页面。因为目前只建立了映射，真正的硬盘数据还没有拷贝到内存中，因此引发缺页异常。

10、缺页异常进行一系列判断，确定无法操作后，内核发起请求掉页过程。

11、调页过程先在交换缓存空间中寻找需要访问的内存页，，如果没有则调用nopage函数把所缺的页从磁盘装入到主存中。

12、之后进程即可对这片主存进行读或者写的操作了，如果写操作改变了内容，一定时间后系统自动回写脏页面到对应的磁盘地址，也即完成了写入到文件的过程。

注：修改过的脏页面并不会立即更新回文件，而是有一段时间延迟，可以调用msync() 来强制同步，这样所写的内容就能立即保存到文件里了。

### 2.3 mmap优点

1、对文件的读取操作跨过了页缓存，减少了数据的拷贝次数，用内存读写取代了I/O读写，提高了读取的效率。

2、实现了用户空间和内核空间的高校交互方式，两空间的各自修改操作可以直接反映在映射的区域内，从而被对方空间及时捕捉。

3、提供进程间共享内存及互相通信的方式。不管是父子进程还是无亲缘关系进程，都可以将自身空间用户映射到同一个文件或者匿名映射到同一片区域。从而通过各自映射区域的改动，打到进程间通信和进程间共享的目的。

同时，如果进程A和进程 B 都映射了区域C,当A第一次读取C时候，通过缺页从磁盘复制文件页到内存中，但当B再读C的相同页面时，虽然也会产生缺页异常，但是不会从磁盘中复制文件过来，而是直接使用已经保存再内存中的文件数据。

### 2.4 适用场景

可用于实现高效的大规模数据传输。内存空间不足，是制约大数据操作的一个方面，解决方案往往是借助于硬盘空间的协助，补充内存的不足。但是进一步造成大量的文件I/O操作，极大影响效率。这个问题可以通过mmap映射很好地解决。换句话说，但凡需要磁盘空间代替内存的时候，mmap都可以发挥功效。

原文地址：https://zhuanlan.zhihu.com/p/581863694   

作者：linux

# 【NO.35】TCP收发数据“丢失”问题的排查与解决

我们协议栈的某条业务线数据的收发基于TCP连接的，在测试过程中发现，TCP数据的接收与发送各有一次数据“丢失”的问题。我们知道TCP数据的收发是可靠的，不会发生数据丢失的情况，本文将讲的数据“丢失”是出问题时给人的一种假象。下面本文就来详细地讲述一下收发数据失败问题的排查过程。

![img](https://pic4.zhimg.com/80/v2-290f50dc6aa2952aef3292e1901b46e7_720w.webp)

## 1.TCP发送数据的“丢失”问题

### 1.1 问题描述

平台的同事向我们反馈，换了新版本的协议栈后，发送大的消息（发送的是1669个字节的数据）会有概率失败的情况。失败不是说，发送端调用我们协议栈的接口发送数据时接口就返回失败，而是返回了成功，但接收端却并没有收到此消息。

### 1.2 问题分析

老版本的协议栈和新版本的协议栈差异化还是很大的，所以不可能直接去对比所有协议栈的内容。所以只能一步步从这个问题的源头查起。

首先分析是发送端的问题还是接收端的问题，经过在给接口处加打印、数据收发处加打印以及抓包分析后发现，接口处传入的数据都是正确的，但是从包里看却发现是发送端出了数据“丢失”、“乱序”的问题。

因为数据流太大，协议栈会进行分包发送，每包最大512bytes。那么1699bytes的数据就应该分为4包发送，且按顺组成的内容应该跟原始数据一致。如图所示：

![img](https://pic1.zhimg.com/80/v2-0fb79e4477d39189a9f5c4a79d2190d8_720w.webp)

数据中包括一个TPKT头（03 00 06 a3），其中06 a3指示数据大小为1699bytes，4包数据为一个完整数据。

上图是正确情况下的内容，出问题时就会出现如下图所示的“乱序”问题：

![img](https://pic2.zhimg.com/80/v2-895efeccae78c324577e786a2e3dc2b5_720w.webp)

从上图中我们可以看到第四包数据的内容也出现了一个TPKT头（03 00 06 a3），实际上数据中有和头部相同的数据是没什么奇怪的，但是分析发现第四包数据内容和第一包数据一模一样。

对比打印中正确的数据，第四包的数据就是错的，而且打印中发现，业务是发送了两次非标数据，那么第四包的数据应该就是第二次发送的数据头，这样的话给我们的感觉就是TCP竟然出现了“乱序”！检查了包里的内容发现，后边缺失的数据也没看到有发送，那么分析下来就是出现“丢失”数据了，而不是“乱序”！

分析代码时经过高人指点，发现协议栈发送数据时是切分数据后直接循环调用send发送，会不会是发送太快又频繁而导致TCP的发送缓冲区不够用了呢。那么在每次发送后都sleep上几毫秒后测试，每次发送都是正确的了。

### 1.3 问题解决

我们知道，调用套接字的send函数后，返回>0的值并不代表tcp已经把这么多数据发送给对方了，而是拷贝给了发送缓冲区多少个字节（非阻塞模式）。Tcp从缓冲区中发送数据也是需要消耗时间的，那么使用sleep就可以多给tcp点时间去把缓冲区的数据发送出去从而移除掉。

sleep虽然可以解决这个问题，但是sleep多长时间合适呢，而老版本协议栈为什么没有问题呢？此时再去查看老版本协议栈的代码，发现它并没有用sleep，那么只有放大缓冲区大小可以解决这个问题了。经查看，4.0协议栈确实在创建socket的时候，都会把收发缓冲区的大小设置下。

新版本的协议栈也使用此方法后问题解决。设置方法就是使用setsockopt函数，套接字选项为SO_SNDBUF、SO_RCVBUF。

肯定有人会问，当发送缓冲区满时，send会返回-1的啊，或者要发送的数据大于缓冲区中剩余的数据也回返回实际放入的数据值啊。好吧，确实是我们使用的失误，对返回值的操作没有处理好，而是自以为的都发送成功了，从而导致了数据“丢失”。所以当时看协议栈打印并没有报错的地方，从而把人引入更晕乎的状态。赶紧修改之。



## 2.TCP接收数据的“丢失”问题

### 2.1 问题描述

测试人员反馈，在他的一台win7电脑上测试软件的某一项功能时会大概率性失败，而其他电脑则没有这个问题。

### 2.2 问题分析

首先分析失败原因，从打印中看，是因为平台侧认为超时没收到此win7电脑的MSD（主从决定）消息，从而导致MSDACK无法正确完成，从而导致问题。

但抓win7电脑和平台侧的包后发现，win7侧信令有发送出去，平台侧的抓包看是有收到此包数据的。但是看平台侧打印就是没有收到此信令的打印，以至于解析win7电脑发来的MSD消息的TPKT头的打印都没有。查看解析数据的代码，并没有看到有什么特殊处理的地方，为什么只针对此win7电脑有问题呢，而且还不是必现？！

在分析打印的时候，看到平台侧收到win7电脑致邻发送的TCS（能力集）数据流，数据的末尾和某类型的TPKT头格式相同，都是06 00 xx xx的格式。而其他电脑致邻的TCS数据却不是这样，会不会是因为数据和TPKT头相似而导致的解析错误呢？

协议栈的数据处理过程：首先它会读取数据的TPKT头部分，根据头部指示数据的大小再读入相应大小的数据。那么即使数据部分和头部相同应该也不会有把数据部分当做头部处理的情况。

在正确的情况下：

![img](https://pic1.zhimg.com/80/v2-9ffcd5a77ed24fcafb5fb2a043d0712c_720w.webp)

接收MSD消息打印分析收到win7发送的MSD消息过程：首先读取数据的前4个字节（TPKT头大小）的数据，TPKT头前两字节固定是03 00（某类型的TPKT头是06 00，会转化成03 00），后两个字节是数据大小，那么数据大小是00 0B=11bytes,减去TPKT头大小4bytes，剩下的就是7bytes，那么再读取7bytes的数据从而组成一个完整的MSD消息。

有问题时的抓包如下：

![img](https://pic4.zhimg.com/80/v2-754f751e0b198a8b6e42dc0fc01999cf_720w.webp)

对比上面两个图，01 00 32 80就是MSD的数据部分，后续再读取的3个字节总共7个字节就是MSD消息（传入的len=512，是因为发现之前读的数据不符合预期中的TPKT头，那么就一次读入512数据放到临时缓冲中，在从缓冲中逐个字节去找TPKT头，防止因为tcp数据流的传输方式导致一次的错误而把后续的有效消息也丢失的问题），但是为什么没有MSD消息的TPKT头部分呢？难道tcp数据出现了“丢失”？！但从wireshark抓包来看，其收到的数据是完整的！

在某特殊类型的产品中，把原来标准的TPKT头03 00 xx xx变为06 00 xx xx，并且数据部分插入4个bytes的0。这个处理被放到了调用send时转换为该特殊类型的产品数据，在recv收到数据后再恢复到标准数据。所以根据协议处理数据原理，先读TPKT头，然后根据头部读数据。在图4中的体现是：TPKT头“丢”了，而直接读取的是插入的4个0数据，发现不符合TPKT头格式再继续以4个字节大小尝试读取头部数据。

检查了tcp的socket接收缓冲区大小，已经被设置为挺大的值了，那么看来tcp数据的“丢失”不是对socket的设置导致，那就只能是我们代码哪里处理有问题了。因为win7电脑的TCS数据的“特殊化”，又继续分析了其TCS数据。对比成功和失败时候的打印，虽然测试人员说什么都没有改动，但是从打印中能看出来，成功和失败时候win7电脑致邻发送能力的大小是不一样的！

成功时接收TCS消息打印如下：

![img](https://pic2.zhimg.com/80/v2-9c83a7444a9edcb8d082f5baf7a8f00d_720w.webp)

发送失败时接收TCS消息打印如下：

![img](https://pic3.zhimg.com/80/v2-51a0ab96bfc473023749830f7aed55d6_720w.webp)

然后就注意到了，上面接收失败时读取数据时候出现了一个错误。虽然返回值是没有错误，而且打印也没有报错，但是从打印中看到我们要读取len=4字节长度的时候，实际received却是8！对于recv函数来说，除非接收buffer中数据长度小于你要读取的数据长度时，会返回和你要读取的数据不一样的值，否则你要读取多少字节就应该返回多少字节的数据。所以这个返回值肯定就是出现问题的元凶！

那么成功和失败时候的能力区别在哪里呢，对比包发现，呼叫时候的码率不同会导致能力字节大小不一样，所以在呼叫码率是8M的时候是必现，其他小点的码率就不出现。

### 2.3 问题解决

协议解析数据会先读取TPKT头数据的大小，为了解析某特殊类型产品的数据，在recv到数据后判断如果是该特殊类型产品的TPKT头，那么要做的就是改变头内容（把06变为03，指示数据长度的地方减去插入的4个bytes）、去掉后续是4个0的数据。

在错误情况下解析TCS数据的时，它刚好暴露了我们代码中的一个错误点。如图6所示，因为它的能力字节大小是520byets，去掉头部4bytes后还有516个。协议栈读数据时如果其大小超过512块大小就分块读取。所以先读512bytes，再读剩下的4bytes。而每次读取4bytes时，我们都会判断是否是某特殊类型产品的TPKT头，根据2.2.2中怀疑点，TCS的最后四个字节刚好和该特殊类型产品的TPKT头格式相同。而刚好因为其字节数的大小，与块大小的原因我们单独读取了这四个字节！

在判断是该特殊类型产品的TPKT头后，我们做了头部操作，然后再读取4个字节，判断是不是全0，如果是就去掉，否则就返回8（返回8是因为已经读了8个字节）。所以出现了接收失败时我们要读取4bytes但是返回8的现象。

所以最简单的解决方案就是实现“预读”。recv函数原型：

```text
int recv( _In_ SOCKET s, _Out_ char *buf, _In_ int len, _In_ int flags);
```

参数flags值：

MSG_DONTROUTE 绕过路由表查找。

MSG_DONTWAIT 仅本操作非阻塞。

MSG_OOB 发送或接收带外数据。

MSG_PEEK 窥看外来消息。

MSG_WAITALL 等待所有数据。

一般情况下，我们recv的第四个值都会写为0，那么我们调用了recv后，已经读过的数据就会从socket的缓冲区中移除。而MSG_PEEK值却可以为我们实现“预读”功能，即我们预读的数据就不会从socket接收缓冲区移除，在预读发现如果不是4个0字节，那么就直接返回已读的TPKT头大小数据，否则就使用从socket接收缓冲区中移除的方式读取完成删除插入的4个0字节的功能。

## 3.最后

在对分析问题的过程中，并没有像上述分析过程那样顺利，整个过程时艰难曲折的，中途查阅了不少资料。经历了这两个问题详细排查过程，使得我们对TCP的读写数据的原理及细节有了更为深刻的认识。

原文地址：https://zhuanlan.zhihu.com/p/580513576     

作者：linux

# 【NO.36】一条TCP连接时占用内存空间多少?

计算网络连接所占用的[内存](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%E5%86%85%E5%AD%98%26spm%3D1001.2101.3001.7020)，需要知道申请哪些内存，以及一条TCP连接需要占用多少内存空间。sock_inode_cache、tcp_sock、dentry、file 内存对象占用多少空间？

## 1.内存分配关系

### 1.1 物理内存分配关系

![img](https://pic3.zhimg.com/80/v2-ac9916dd2db25b1b6c623d9c4f11b87a_720w.webp)

### 1.2 TCP申请对象之间关系

![img](https://pic2.zhimg.com/80/v2-057e68e58cddac8511de9385b74e6361_720w.webp)

## 2.实测内存开销

### 2.1 客户端内核参数调整

```text
# vi /etc/sysctl.conf
#端口号
net.ipv4.ip_local_port_range = 5000     65000
net.ipv4.tcp_tw_reuse = 0
#net.ipv4.tcp_tw_recycle        = 0
#time_wait状态
net.ipv4.tcp_max_tw_buckets = 60000
fs.file-max=210000
fs.nr_open=210000
 
 
# sysctl -p
```



```text
#vim /etc/security/limits.conf
 
*       soft    nofile  1010485
*       hard    nofile  1011000
```

### 2.2 服务端内核参数调整

```text
# vi /etc/sysctl.conf
net.core.rmem_max = 8388608
net.ipv4.tcp_rmem = 4096        131072  8388608
 
net.core.wmem_max = 8388608
net.ipv4.tcp_wmem = 4096        16384   8388608
 
 
fs.file-max=2100000
fs.nr_open= 2100000
net.core.somaxconn = 1024
 
# sysctl -p
```



```text
#vim /etc/security/limits.conf
 
*       soft    nofile  1010485
*       hard    nofile  1011000
```

服务端与客户端的缓存清理

```text
echo "3" > /proc/sys/vm/drop_caches
```



## 3.服务端开销计算

### 3.1 slab内存在连接前后输出以及计算

```text
root@wy-virtual-machine:~# cat /proc/meminfo | grep Slab
Slab:             147724 kB
root@wy-virtual-machine:~# 
root@wy-virtual-machine:~# cat /proc/meminfo | grep Slab
Slab:             180136 kB
root@wy-virtual-machine:~# 
root@wy-virtual-machine:~# cat /proc/meminfo | grep Slab
Slab:             195816 kB
```

180136 - 147724 = 32412 / 10000 = 3.2412K ESTABLISH

195816 - 180136 = 15680 / 10000 = 1.568K CLOSE_WAIT

### 3.2 TCP对象占用内存

slabtop

```text
 Active / Total Objects (% used)    : 877387 / 896605 (97.9%)
 Active / Total Slabs (% used)      : 31483 / 31483 (100.0%)
 Active / Total Caches (% used)     : 119 / 167 (71.3%)
 Active / Total Size (% used)       : 298213.95K / 308366.00K (96.7%)
 Minimum / Average / Maximum Object : 0.01K / 0.34K / 8.00K
 
  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME                   
155210 155210 100%    0.02K    913      170      3652K lsm_file_cache
 99712  99712 100%    0.03K    779      128      3116K kmalloc-32
 90048  90048 100%    0.19K   4288       21     17152K dentry
 64480  64480 100%    0.12K   2015       32      8060K kernfs_node_cache
 60352  60352 100%    0.25K   3772       16     15088K filp
 56069  56069 100%    0.81K   2951       19     47216K sock_inode_cache
 50050  50050 100%    2.19K   3575       14    114400K TCP
 46949  45281  96%    0.20K   2471       19      9884K vm_area_struct
 34764  33034  95%    0.62K   2897       12     23176K inode_cache
```

2.19K（TCP）+ 0.25K（flip）+0.19K（dentry）+0.81K（sock_inode_cache） = 3.44K



TCP对象与slab内存比较 稍微差了一点点

CLOSE_WAIT状态开销也不小有1.56 k

### 3.3 TCP对象内存开销总结

| 缓存             | 大小  | 对象                |
| ---------------- | ----- | ------------------- |
| sock_inode_cache | 0.81K | struct socket_alloc |
| TCP              | 2.19K | struct tcp_sock     |
| dentry           | 0.19K | struct dentry       |
| file             | 0.25K | struct file         |

## 4.参数设置

### 4.1 linux打开文件限制配置

1）进程级别，单个进程打开个数 soft nofile 与 fs.nr_open

2）系统级别，fs.file-max，这个参数不限制root用户

- 如果设置soft nofile 那么 hard nofile也要同步设置
- 如果设置hard nofile ,fs.nr_open也要一起调整
- 如果hard nofile设置比fs.nr_open大，会导致用户无法登录

设置参考

```text
# vi /etc/sysctl.conf
fs.file-max=1100000
fs.nr_open= 1100000  #设置比hard大
 
#sysctl -p
 
# vi /etc/security/limits.conf
*       soft    nofile  1000000
*       hard    nofile  1000000
```

### 4.2 TCP内存设置

读写设置，每个TCP 建立连接大概3.44K，在设置总内存时乘上数量

```text
#sysctl -a 
 
net.core.rmem_max = 8388608
net.ipv4.tcp_rmem = 4096        131072  8388608
 
net.core.wmem_max = 8388608
net.ipv4.tcp_wmem = 4096        16384   8388608
```

### 4.3 查看连接状态

```text
# netstat -antp
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      689/systemd-resolve 
tcp        0      0 127.0.0.1:631           0.0.0.0:*               LISTEN      837/cupsd           
tcp6       0      0 ::1:631                 :::*                    LISTEN      837/cupsd
```

### 4.4 其他

- netstat -antp | grep EST |wc -l 查看单个状态并统计数量
- slabtop 查看slab对象内存的申请情况，以及使用量

原文地址：https://zhuanlan.zhihu.com/p/579154045   

作者：linux

# 【NO.37】300行代码带你实现一个Linux文件系统

Linux作为一个类UNIX系统，其文件系统保留了原始UNIX文件系统的表象形式，它看起来是这个样子：

```text
root@name-VirtualBox:/# ls
bin  boot  cdrom  dev  etc  home  lib  lib64  lost+found  media  mnt  opt  proc  root  run  sbin  snap  srv  sys  tmp  usr  var
root@name-VirtualBox:/#
root@name-VirtualBox:/# ls /home/
zy
root@name-VirtualBox:/#
root@name-VirtualBox:/# ls /usr/
bin  games  include  lib  lib64  local  sbin  share  src
root@name-VirtualBox:/# ls /mnt/
project  src
```

它其实是一棵目录树(没有画全)：

![img](https://pic3.zhimg.com/80/v2-c71f4e00e98cfbfc1e595b63e6c2625a_720w.webp)

然而，虽然所有的UNIX系统以及类UNIX系统的文件系统看起来一样，但是它们的实现却是不尽相同。

作为普通用户，了解文件系统的基本操作就够了；作为应用开发人员，了解文件系统的POSIX接口足矣，但是作为一个对操作系统有着浓厚兴趣的爱好者而言，自己可能就是一个新的文件系统的潜在实现者，所以必须一窥究竟，看看如此外观的文件系统到底是如何实现的。

网上已经有了很多关于UNIX/Linux文件系统实现的资源，但是无一例外，都太复杂了，除了整体的源码分析外，几乎就是针对某个特定文件系统的详解了，如此复杂的这些对于初涉该领域的满腔热情者无疑是一盆冷水，很多人因此望而却步。

- ...
- mount机制是如何实现的？
- inode是如何分配的？磁盘inode和内存inode有什么区别？
- dentry缓存是怎么回事？是如何管理的？
- pagecache是什么？radix树如何管理文件数据缓存？
- cache和buffer的区别又是什么？
- ...

几乎所有的关于Linux文件系统实现的资源都在用不同的语言解释上面的这些问题，这很容易陷入细节的泥潭。

本文以Linux内核为例，用一种稍微不同的方式去描述文件系统的实现。嗯，我会分3个部分来介绍Linux内核的文件系统：

1. Linux文件系统在不同视角下的样子
2. 实现一个很小但能跑的文件系统
3. 接下来要做什么

本文中，我会通过一个实实在在的文件系统实现的例子，试图阐述 **实现一个文件系统，哪些是必须的，哪些不是必须的。** 这是一个任务驱动的过程，从简单的例子开始。

读过本文之后，相信会对Linux文件系统的实现有一个总体上的宏观把握，然后再去反复推敲上述的细节问题，重读网上的那些经典资源，相信会事半功倍。

### 1.**Linux文件系统在不同视角下的样子**

当然，在给出最简单的tinyfs实现之前，还是会有一个总体的介绍。

如果我们把本文最初描述的那个在几乎所有UNIX/类UNIX系统中长的一模一样的文件系统表面刨开，在Linux内核中，文初的那棵树其实它长下面的样子(其实在大多数类UNIX系统中，它们长得都差不多)：

![img](https://pic1.zhimg.com/80/v2-09cda938279815479b145d89df318668_720w.webp)

我们看到，Linux系统的文件目录树就是靠上图中的这一系列的链表穿针引线给串在一起的，就像缝制一件衣服一样，最终的成衣就是我们看到的Linux系统目录树，而缝制这件成衣的线以及指导走线的规则便是VFS本身了。

现在只要记住两个重要链表：

1. 文件系统链表。
2. 每一个文件系统的mount挂载点链表。

然后读完本文之后再去结合代码深入分析它们是如何串起整个文件系统的。

VFS之所有可以将机制大相径庭的完全不同的文件系统对外统一成一个样子，完全就是依靠了它的统一的对POSIX文件调用的接口，该接口的结构看上去是下面的样子：

![img](https://pic3.zhimg.com/80/v2-4f1fbac91dcd317069e46fd78f8fc94e_720w.webp)

注意上图最下面的那个椭圆，如果要实现一个文件系统，这个椭圆里的东西是关键，它完成了穿针引线的大部分工作。

现在让我们纵向地看一下一个完整的文件系统实现都包括什么，我指的是从POSIX系统调用开始，一直到数据落盘。Linux内核关于文件系统IO，完整的视图如下所示：

![img](https://pic1.zhimg.com/80/v2-9b26ac61853112a182c8138de2d5f4a8_720w.webp)

注意VFS提供的三类接口：

- **和POSIX系统调用的接口** 即实现open/read/write的操作的接口。
- **和底层介质的接口** 即下接块设备层的接口。
- **如何管理自身** 即何时以及如何操作VFS数据结构inode，dentry，mount等对象。

一个文件系统如果能实现上面三类接口，那它就是个完整的文件系统了。

我们恰好可以从设计并实现一个最基本的这样的文件系统开始。一个基本的文件系统，其着重点在于上图中红色的部分，而其它部分则不是必不可少的，但是却是让该文件系统变得优秀(而不仅仅是可用)所必须的。



### 2.**实现一个很小但能跑的文件系统**

为什么要实现这么一个文件系统，难道没人已经做了这个工作吗？做这个工作的意义何在？

原因如下：

- **我没有找到现成的比较完整且炫酷的作品。** 当然有人写这种文件系统练手，但是看下来要么就是使用了libfs.c里封装好的接口，要么就是没有自己设计文件系统的底层存储格式。
- **下班的班车在路上堵了一个多小时，无聊撸会儿代码。**

然而确实，我没有找到简单的 **最小文件系统** 实现，也许你会说Linux内核自带的ramfs难道不就是一个现成的吗？的确算一个，但它有两个问题导致你无法领略实现一个文件系统的全过程，注意，我说的可是全过程：

1. ramfs无法让你自己设计底层模拟介质的格式，不完整。
2. ramfs调用了大量的fs/libfs.c中的内核库例程，不纯碎。

为了 **追求完整，** 如果你把如何组织一块内核作为ramfs的底层介质这部分代码全部看完，如果你把libfs.c里的库实现全部看完，我想ramfs也就不算一个 **足够简单** 的文件系统实例了。

看到了么？要想代码简单，你就不得不使用libfs.c里的现成的例程，这将损失你实现一个文件系统的完整性体验，反之，要想完整实现一个文件系统，你可能不得不自己写大量的代码，这却并不简单。

如何既完备，又足够简单呢？

对于我这种编程水平渣渣的内核爱好者而言，如何在堵车的一个多小时内完成一个可以编译通过的文件系统(我承认完全能跑是我回到家后又调试了一个多小时才完成的...)，这对于我而言，是一个挑战，但我要试一试，没想到就成功了。所以才有了今天的分享。

我从最底层的介质结构的设计开始。

我并没有真实的硬件介质，也并不打算编写专门的 **格式化程序** 去格式化一块内存区域，所以我直接用大数组定义一块内存，它便是我的模拟介质了，我的tinyfs的文件格式如下：

```text
// tinyfs.h
#define MAXLEN 8
#define MAX_FILES    32
#define MAX_BLOCKSIZE  512

// 定义每一个目录项的格式
struct dir_entry {
    char filename[MAXLEN];
    uint8_t idx;
};

// 定义每一个文件的格式。
struct file_blk {
    uint8_t busy;
    mode_t mode;
    uint8_t idx;

    union {
        uint8_t file_size;
        uint8_t dir_children;
    };
    char data[0];
};

// OK，下面的block数组所占据的连续内存就是我的tinyfs的介质，每一个元素代表一个文件。
// struct file_blk block[MAX_FILES+1];
```

这个文件系统的格式非常的Low：

1. **最多容纳512个文件(包括目录在内)。**
2. **每个文件包括元数据在内最多32个字节。**
3. **文件名最多8个字节。**

之所以这么Low是因为它只是一个开始， **当这个文件系统实现并且能跑之后，你会发现它因为Low而带来的不足和一些代价，而弥补这些不足正好是优化的动机，带着你逐步实现一个更加不Low的文件系统，在实现的过程中，你会窥见并掌握Linux内核文件系统的全貌和细节。** 完美的学习过程，OK！

下面是代码：

```text
// tinyfs.c
#include <linux/init.h>
#include <linux/module.h>
#include <linux/fs.h>
#include <linux/uaccess.h>

#include "tinyfs.h"

struct file_blk block[MAX_FILES+1];
int curr_count = 0; // 我勒个去，竟然使用了全局变量！

// 获得一个尚未使用的文件块，保存新创建的文件或者目录
static int get_block(void)
{
    int i;

    // 就是一个遍历，但实现快速。
    for (i = 2; i < MAX_FILES; i++) {
        if (!block[i].busy) {
            block[i].busy = 1;
            return i;
        }
    }
    return -1;
}

static struct inode_operations tinyfs_inode_ops;
// 读取目录的实现
static int tinyfs_readdir(struct file *filp, void *dirent, filldir_t filldir)
{
    loff_t pos;
    struct file_blk *blk;
    struct dir_entry *entry;
    int i;

    pos = filp->f_pos;
    if (pos)
        return 0;

    blk = (struct file_blk *)filp->f_dentry->d_inode->i_private;

    if (!S_ISDIR(blk->mode)) {
        return -ENOTDIR;
    }

    // 循环获取一个目录的所有文件的文件名
    entry = (struct dir_entry *)&blk->data[0];
    for (i = 0; i < blk->dir_children; i++) {
        filldir(dirent, entry[i].filename, MAXLEN, pos, entry[i].idx, DT_UNKNOWN);
        filp->f_pos += sizeof(struct dir_entry);
        pos += sizeof(struct dir_entry);
    }

    return 0;
}

// read实现
ssize_t tinyfs_read(struct file * filp, char __user * buf, size_t len, loff_t *ppos)
{
    struct file_blk *blk;
    char *buffer;

    blk = (struct file_blk *)filp->f_path.dentry->d_inode->i_private;
    if (*ppos >= blk->file_size)
        return 0;

    buffer = (char *)&blk->data[0];
    len = min((size_t) blk->file_size, len);

    if (copy_to_user(buf, buffer, len)) {
        return -EFAULT;
    }
    *ppos += len;

    return len;
}

// write实现
ssize_t tinyfs_write(struct file * filp, const char __user * buf, size_t len, loff_t * ppos)
{
    struct file_blk *blk;
    char *buffer;

    blk = filp->f_path.dentry->d_inode->i_private;

    buffer = (char *)&blk->data[0];
    buffer += *ppos;

    if (copy_from_user(buffer, buf, len)) {
        return -EFAULT;
    }
    *ppos += len;
    blk->file_size = *ppos;

    return len;
}

const struct file_operations tinyfs_file_operations = {
    .read = tinyfs_read,
    .write = tinyfs_write,
};

const struct file_operations tinyfs_dir_operations = {
    .owner = THIS_MODULE,
    .readdir = tinyfs_readdir,
};

// 创建文件的实现
static int tinyfs_do_create(struct inode *dir, struct dentry *dentry, umode_t mode)
{
    struct inode *inode;
    struct super_block *sb;
    struct dir_entry *entry;
    struct file_blk *blk, *pblk;
    int idx;

    sb = dir->i_sb;

    if (curr_count >= MAX_FILES) {
        return -ENOSPC;
    }

    if (!S_ISDIR(mode) && !S_ISREG(mode)) {
        return -EINVAL;
    }

    inode = new_inode(sb);
    if (!inode) {
        return -ENOMEM;
    }

    inode->i_sb = sb;
    inode->i_op = &tinyfs_inode_ops;
    inode->i_atime = inode->i_mtime = inode->i_ctime = CURRENT_TIME;

    idx = get_block(); // 获取一个空闲的文件块保存新文件

    blk = &block[idx];
    inode->i_ino = idx;
    blk->mode = mode;
    curr_count ++;

    if (S_ISDIR(mode)) {
        blk->dir_children = 0;
        inode->i_fop = &tinyfs_dir_operations;
    } else if (S_ISREG(mode)) {
        blk->file_size = 0;
        inode->i_fop = &tinyfs_file_operations;
    }

    inode->i_private = blk;
    pblk = (struct file_blk *)dir->i_private;

    entry = (struct dir_entry *)&pblk->data[0];
    entry += pblk->dir_children;
    pblk->dir_children ++;

    entry->idx = idx;
    strcpy(entry->filename, dentry->d_name.name);

    // VFS穿针引线的关键步骤，将VFS的inode链接到链表
    inode_init_owner(inode, dir, mode); 
    d_add(dentry, inode);

    return 0;
}

static int tinyfs_mkdir(struct inode *dir, struct dentry *dentry, umode_t mode)
{
    return tinyfs_do_create(dir, dentry, S_IFDIR | mode);
}

static int tinyfs_create(struct inode *dir, struct dentry *dentry, umode_t mode, bool excl)
{
    return tinyfs_do_create(dir, dentry, mode);
}

static struct inode *tinyfs_iget(struct super_block *sb, int idx)
{
    struct inode *inode;
    struct file_blk *blk;

    inode = new_inode(sb);
    inode->i_ino = idx;
    inode->i_sb = sb;
    inode->i_op = &tinyfs_inode_ops;

    blk = &block[idx];

    if (S_ISDIR(blk->mode))
        inode->i_fop = &tinyfs_dir_operations;
    else if (S_ISREG(blk->mode))
        inode->i_fop = &tinyfs_file_operations;

    inode->i_atime = inode->i_mtime = inode->i_ctime = CURRENT_TIME;
    inode->i_private = blk;

    return inode;
}

struct dentry *tinyfs_lookup(struct inode *parent_inode, struct dentry *child_dentry, unsigned int flags)
{
    struct super_block *sb = parent_inode->i_sb;
    struct file_blk *blk;
    struct dir_entry *entry;
    int i;

    blk = (struct file_blk *)parent_inode->i_private;
    entry = (struct dir_entry *)&blk->data[0];
    for (i = 0; i < blk->dir_children; i++) {
        if (!strcmp(entry[i].filename, child_dentry->d_name.name)) {
            struct inode *inode = tinyfs_iget(sb, entry[i].idx);
            struct file_blk *inner = (struct file_blk *)inode->i_private;
            inode_init_owner(inode, parent_inode, inner->mode);
            d_add(child_dentry, inode);
            return NULL;
        }
    }

    return NULL;
}

int tinyfs_rmdir(struct inode *dir, struct dentry *dentry)
{
    struct inode *inode = dentry->d_inode;
    struct file_blk *blk = (struct file_blk *)inode->i_private;

    blk->busy = 0;
    return simple_rmdir(dir, dentry);
}

int tinyfs_unlink(struct inode *dir, struct dentry *dentry)
{
    int i;
    struct inode *inode = dentry->d_inode;
    struct file_blk *blk = (struct file_blk *)inode->i_private;
    struct file_blk *pblk = (struct file_blk *)dir->i_private;
    struct dir_entry *entry;

    // 更新其上层目录
    entry = (struct dir_entry *)&pblk->data[0];
    for (i = 0; i < pblk->dir_children; i++) {
        if (!strcmp(entry[i].filename, dentry->d_name.name)) {
            int j;
            for (j = i; j < pblk->dir_children - 1; j++) {
                memcpy(&entry[j], &entry[j+1], sizeof(struct dir_entry));
            }
            pblk->dir_children --;
            break;
        }
    }

    blk->busy = 0;
    return simple_unlink(dir, dentry);
}

static struct inode_operations tinyfs_inode_ops = {
    .create = tinyfs_create,
    .lookup = tinyfs_lookup,
    .mkdir = tinyfs_mkdir,
    .rmdir = tinyfs_rmdir,
    .unlink = tinyfs_unlink,
};

int tinyfs_fill_super(struct super_block *sb, void *data, int silent)
{
    struct inode *root_inode;
    int mode = S_IFDIR;

    root_inode = new_inode(sb);
    root_inode->i_ino = 1;
    inode_init_owner(root_inode, NULL, mode);
    root_inode->i_sb = sb;
    root_inode->i_op = &tinyfs_inode_ops;
    root_inode->i_fop = &tinyfs_dir_operations;
    root_inode->i_atime = root_inode->i_mtime = root_inode->i_ctime = CURRENT_TIME;

    block[1].mode = mode;
    block[1].dir_children = 0;
    block[1].idx = 1;
    block[1].busy = 1;
    root_inode->i_private = &block[1];

    sb->s_root = d_make_root(root_inode);
    curr_count ++;

    return 0;
}

static struct dentry *tinyfs_mount(struct file_system_type *fs_type, int flags, const char *dev_name, void *data)
{
    return mount_nodev(fs_type, flags, data, tinyfs_fill_super);
}

static void tinyfs_kill_superblock(struct super_block *sb)
{
    kill_anon_super(sb);
}

struct file_system_type tinyfs_fs_type = {
    .owner = THIS_MODULE,
    .name = "tinyfs",
    .mount = tinyfs_mount,
    .kill_sb = tinyfs_kill_superblock,
};

static int tinyfs_init(void)
{
    int ret;

    memset(block, 0, sizeof(block));
    ret = register_filesystem(&tinyfs_fs_type);
    if (ret)
        printk("register tinyfs failed\n");

    return ret;
}

static void tinyfs_exit(void)
{
    unregister_filesystem(&tinyfs_fs_type);
}

module_init(tinyfs_init);
module_exit(tinyfs_exit);

MODULE_LICENSE("GPL");
```

review代码后，你可能已经发现了几个问题：

1. 这个tinyfs文件系统存储格式中怎么没有超级块？(竟然使用了全局变量)
2. 这个tinyfs文件系统存储格式中怎么没有inode表或者空闲位图？
3. 这个tinyfs文件系统怎么将元数据和文件数据放在了一起？
4. 这个文件系统怎么不支持并发？没见到一把锁啊！
5. ...

嗯，其实这些问题目前而言还都不是问题，它们并不阻碍这个文件系统的真实性，它用起来是那么的真实。

没有任何规范规定一个文件系统存储格式必须有什么或者必须没有什么，文件系统格式只是一个 **看上去还可以的信息持久化记录格式** ，只要下次能根据某些信息将文件读取出来，任何格式都是OK的。

之所以很多人会认为一个文件系统的格式必须要符合某种规范，完全是因为人们看的最多的那些文件系统ext3，ext4，ntfs等恰好是那样做的罢了。不过事实证明，那样做确实是很好的。在可用的玩具完成之后，就要考虑 **性能，健壮性，可扩展性** 等这些工程因素了。而ext3/4，ntfs，xfs则充分考虑了这些。

逐渐的，权威变成了规范，至少成了一种范式或者模式，这是计算机领域常有的事，见怪不怪。

我们来使用下这个文件系统：

```text
[root@localhost ~]# insmod ./tinyfs.ko
[root@localhost ~]# mount -t tinyfs none /mnt
```

OK，挂载成功。

这里又有疑问了，为什么是none挂载，而不是一个块设备，比如/dev/sda1之类的。

这是因为我根本没有将介质(其实是一块连续的内存)抽象成块设备，如果引入块设备抽象，势必要导出device层操作接口，这样做并不困难，但却太麻烦，且块设备抽象和文件系统的实现核心无关。本文不是讲块设备的，加之班车上的堵车时间有限，故不做抽象。

好了，现在让我们来折腾下/mnt目录，该目录就是我的tinyfs的挂载目录了，在其下读写文件，就是在tinyfs的内存介质上读写文件：

```text
[root@localhost ~]# cd /mnt/
[root@localhost mnt]# ls
[root@localhost mnt]#
[root@localhost mnt]# echo 11111 >./a
[root@localhost mnt]# cat ./a
11111
[root@localhost mnt]# mkdir dir1
[root@localhost mnt]# echo 22222 >./dir1/b
[root@localhost mnt]#
[root@localhost mnt]# cat ./dir1/b
22222
[root@localhost mnt]#
[root@localhost mnt]# tree
.
├── a
└── dir1
    └── b

1 directory, 2 files
[root@localhost mnt]# echo 333 >./dir1/c
[root@localhost mnt]# cat ./dir1/c
333
[root@localhost mnt]# tree
.
├── a
└── dir1
    ├── b
    └── c

1 directory, 3 files
[root@localhost mnt]# rm -f ./dir1/c
[root@localhost mnt]#
[root@localhost mnt]# tree
.
├── a
└── dir1
    └── b

1 directory, 2 files
[root@localhost mnt]# rm -rf dir1
rm: 无法删除"dir1": 不允许的操作
[root@localhost mnt]#
```

除了最后一个删除目录的操作，其它的都OK，这也是预期之中，毕竟删除目录是TODO嘛。

### 3.**接下来要做什么**

一共300来行的代码(省去了很多异常判断和处理，真实情况下，这些要占据80%的代码量)，非常容易读懂，你会发现这个文件系统实现是如此之low，然而却能看起来像真的一样。

**这意味着完成和完美真的是两回事！**

很多最终看起来很大型的东西，都是都这种刚刚完成可以用开始的。

很明显，这个代码没有使用块层来和底层介质通信，而是直接操作了底层介质，也就是那块连续的内存。因为我用内存模拟介质，尚且OK，如果底层真的有一个类似磁盘那样的慢速介质，每次操作直接读写block将是不可接受的。但如果你想获得性能上的提升，就必须使用块层的缓存机制，以及pagecache机制。

所以，方向很明确，我们有了一个todolist：

- **fix掉可能的bug，主要是引用计数的管理方面。**
- **设计更好的文件系统底层存储格式，至少让它看起来更像回事。** 比如引入超级块保存全局元数据，引入空闲索引位图，引入优雅的空闲块管理算法，类似内存管理那样，减少内外的存储碎片，引入一些启发式策略，比如预读，合并写，当然，这些属于调度的范畴，但无论如何， **良好的存储结构可以极大影响启发式策略的实施。**
- **使用块层标准接口进行IO，比如 \***sb_bread，mark_buffer_dirty **这样的接口。\***
- **使用pagecache进行数据cache。**

这些todo完成，意味着对Linux内核文件系统实现原理的彻底掌握，从一个简单的刚刚可用的tinyfs开始(这花不了多少时间)，到整理出一份todolist，到完成这些todo，这便是一个任务驱动的学习过程。

回过头来看Linux文件系统IO的纵向视图：

![img](https://pic2.zhimg.com/80/v2-5081b6b071ab9fa5086ffbdbf52b23f9_720w.webp)

这次注意蓝色部分，我们的TODO就是要补充这部分的实现。

好了，换一个视角看VFS。

我们把Linux内核内存中的VFS看作是磁盘等慢速介质中特定文件系统的缓存，这是一个典型的 **分级存储结构，** 就好像CPU cache和内存的关系一样。

在这个视角下，如何完成上图蓝色框框中的部分，可参考的现成范式就太多了。但无论如何都要解决的是：

- **哪些磁盘数据有资格进入到内存VFS的cache结构？**
- **这些被cache的磁盘文件系统数据cache在什么地方？**
- **cache的淘汰算法是怎样的？**

Linux内核已经给了我们一个现成的答案：

- **磁盘IO依然遵循局部性原则，无论是时间局部性还是空间局部性，磁盘IO的局部性原则和磁盘结构(磁道，扇区等)以及启发式预读算法结合在一起，共同决定哪些数据要cache在内存。**
- **磁盘数据以pagecache的形式保存在内存，当然你也可以DirectIO。**
- **cache淘汰根据LRU以及系统内存的吃紧程度来进行实施。具体过程涉及脏页如何回写。**

当然，如果你觉得这些不够好，你也可以设计你自己的。总之，这是一块非常独立的工作，正如我图中所示，这部分工作的目标是，在文件系统刚好可以工作后， **让事情变得更加完美**有趣！

原文地址：https://zhuanlan.zhihu.com/p/579011810    

作者： linux

# 【NO.38】一次解决Linux内核内存泄漏实战全过程

什么是内存泄漏：

程序向系统申请内存，使用完不需要之后,不释放内存还给系统回收，造成申请的内存被浪费.

发现系统中内存使用量随着时间的流逝，消耗的越来越多，例如下图所示：

![img](https://pic2.zhimg.com/80/v2-238d2a09f200f11e145b2bf263456de9_720w.webp)

接下来的排查思路是:

1.监控系统中每个用户进程消耗的PSS (使用pmap工具(pmap pid)).

PSS:按比例报告的物理内存，比如进程A占用20M物理内存，进程B和进程A共享5M物理内存，那么进程A的PSS就是(20 - 5) + 5/2 = 17.5M

2.监控/proc/meminfo输出,重点观察Slab使用量和slab对应的/proc/slabinfo信息

3.参考/proc/meminfo输出，计算系统中未被统计的内存变化，比如内核驱动代码

直接调用alloc_page()从buddy中拿走的内存不会被单独统计

以上排查思路分别对应下图中的1,2,3 :

![img](https://pic1.zhimg.com/80/v2-65fd3146cde9f3843d75b0e49ebf6c9c_720w.webp)

在排查的过程中发现系统非常空闲，都没有跑任何用户业务进程。

其中在使用slabtop监控slab的使用情况时发现size-4096 不停增长

![img](https://pic1.zhimg.com/80/v2-7d36f746609a6582b8a37a0ceb84a4d4_720w.webp)

通过监控/proc/slabinfo也发现SReclaimable 的使用量不停增长

```text
while true; 
do 
sleep 1 ; 
cat /proc/slabinfo >> /tmp/slabinfo.txt ; 
echo "===" >> /tmp/slabinfo.txt ; 
done
```

由此判断很可能是内核空间在使用size-4096 时发生了内存泄漏.

接下来使用trace event(tracepoint)功能来监控size-4096的使用和释放过程，

主要用来跟踪kmalloc()和kfree()函数对应的trace event, 因为他们的trace event被触发之后会打印kmalloc()和kfree()所申请和释放的内存地址,然后进一步只过滤申请4096字节的情况。

```text
#trace-cmd record -e kmalloc 
-f 'bytes_alloc==4096' -e kfree -T
```

(-T 打印堆栈)

等待几分钟之后…

\#ctrl ^c 中断trace-cmd

\#trace-cmd report

以上步骤相当于：

![img](https://pic1.zhimg.com/80/v2-112f1fb0265b037be2c84bce983cfaa4_720w.webp)

等待几分钟之后…

```text
#cp /sys/kernel/debug/tracing/trace_pipe  /tmp/kmalloc-trace
```

从trace-cmd report的输出结果来看，很多kmalloc 对应的ptr值都没有kfree与之对应的ptr值

![img](https://pic3.zhimg.com/80/v2-d7869c3e5f54a87033e1a79d41a7e3ea_720w.webp)

这就说明了cat进程在内核空间使用size-4096之后并没有释放，造成了内存泄漏。



为了进一步精确定位到是使用哪个内核函数造成的问题，此时手动触发vmcore

```text
#echo c > /proc/sysrq-trigger
```

然后使用crash工具分析vmcore：

```text
#crash ./vmcore ./vmlinux.debug
```

读出上面kmalloc申请的ptr内存信息

![img](https://pic3.zhimg.com/80/v2-6df11cad7524f9342bbb729e7240e2b6_720w.webp)

(读取0xffff880423744000内存开始的4096个字节，并以字符形式显示)

![img](https://pic3.zhimg.com/80/v2-bd6f9fe92a846e9c8bf66d9446ac80de_720w.webp)

发现从上面几个ptr内存中读出的内容都是非常相似，仔细看一下发现都是/proc/schedstat 的输出内容。

通过阅读相关代码发现，当读出/proc/schedstat内容之后，确实没有释放内存

![img](https://pic4.zhimg.com/80/v2-c6ea82ba477dcda95180e5f9a530e4c3_720w.webp)

然后发现kernel上游已经有patch解决了这个问题：

commit: 8e0bcc722289

fix a leak in /proc/schedstats



原文地址：https://zhuanlan.zhihu.com/p/577973486     

作者： linux

# 【NO.39】图解通用网络IO底层原理、Socket、epoll、用户态内核态······

## 1.LInux 操作系统中断

### 1.1 什么是系统中断

> 这个没啥可说的，大家都知道；

CPU 在执行任务途中接收到中断请求，需要保存现场后去处理中断请求！保存现场称为中断处理程序！处理中断请求也就是唤醒对应的任务进程来持有CPU进行需要的操作！

有了中断之后，提升了操作系统的性能！可以异步并行处理很多任务！

- **软中断（80中断）**

由CPU产生的；CPU检查到程序代码段发生异常会切换到内核态；

- **硬中断**

由硬件设备发起的中断称为硬中断！可以发生在任何时间；比方说网卡设备接收到一组报文；对应的报文会被[DMA](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3DDMA%26spm%3D1001.2101.3001.7020)设备进行拷贝到网卡缓冲区！然后网卡就会向CPU发起中断信号（IRQ）：

CPU收到信号后就会执行网卡对应的中断处理程序！

### 1.2 内核在系统中断时做了什么事

每种中断都有它对应的中断处理程序；

对应到内核的某一个代码段；

CPU接收到中断后；首先需要将寄存器中数据保存到进程描述符！PCB！

随后切换到内核态处理中断处理程序！执行网卡的程序；

执行完毕之后切换到用户态，根据PCB内容恢复现场！然后就可继续执行代码段了！

### 1.3 硬件中断触发的过程

![img](https://pic1.zhimg.com/80/v2-4f58845894548325a99a27258dba7dec_720w.webp)

**中断请求寄存器**： 保存需要发送中断请求的设备记录！

**优先级解析器**：中断请求是有优先级之分的，因为CPU不能同时执行多个中断请求！

**正在服务寄存器**：正在执行的请求！比方我正在打字，这里面记录的就是键盘IRQ1 ！

![img](https://pic2.zhimg.com/80/v2-30147da63326ce5299ff88fbf0258a51_720w.webp)

操作系统启动时需要将硬件向量值与处理程序地址进行映射！当硬件发送中断信息时只会发送向量值，通过匹配找到对应的处理程序！

## 2.[Socket](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3DSocket%26spm%3D1001.2101.3001.7020)基础

### 2.1Socket读写缓冲区机制

![img](https://pic1.zhimg.com/80/v2-cb1cbee2218d93c2e549ff61e275116c_720w.webp)

所谓socket,在底层也无非就是一个对象，通过对象绑定两个缓冲区，也就是数据队列，然后调用系统API对这两个缓冲区的数据进行操作罢了！

发数据；用户态转内核态，将数据拷贝到send缓存区，然后调用write系统调用将数据拷贝到网卡，再由网卡通过TCP/IP协议进行数据包的网络发送！

socket**两种工作模式**

- BIO

总结：读数据读不到就一直等，发数据发不了就一直等！

- NIO

读数据读不到就等一会再读，取数据取不到就等一会再取！

接受端缓冲区打满了，线程又抢占不到CPU去清理缓冲区，怎么办！

最后发送端的数据缓冲区也会被打满！

## 3.系统调用；用户态------内核态

### 3.1**系统调用**：

int 0X80对应的就是系统调用中断处理程序；向量值为128；system_call;

![img](https://pic2.zhimg.com/80/v2-b234bc234ccced64cf5c14fe51801881_720w.webp)

IRQ是有限的，不可能为每一个系统调用都分配一个向量值，所以统一使用80中断来进行系统调用的路由！



### 3.2 为什么要有这两种状态

指令的危险程度不一样；

对于不同的指令，为了保证系统安全，划分了用户空间和内核空间；

linux中：0表示内核态，3表示用户态！

所以：linux在创建进程的时候就会为进程分配两块空间；

用户栈：分配变量，创建对象

内核栈：分配变量！

### 3.3 什么时候进程进行切换至内核态

硬中断；

用户态中代码出现错误也要切换！

### 3.4 进程切换时都做了什么

CPU中存在很多寄存器

![img](https://pic1.zhimg.com/80/v2-f9411d13bbc8b163ad9c50165212c14c_720w.webp)

这些寄存器保存了进程在进行运算时的一些瞬时数据；如果现在要进行进程切换了；这些数据都需要找个地方保存起来；那么保存到哪里呢？

进程PCB：在OS创建进程的时候同时也会分配一段空间存放进程的一些信息；其中就有一个字段指向一个数据结构；叫做进程控制块PCB：

用来描述和控制进程的运行的一个数据结构——进程控制块PCB(Process Control Block)，是进程实体的一部分，是操作系统中最重要的记录型数据结构。

- PCB是进程存在的唯一标志
- 系统能且只能通过PCB对进程进行控制和调度
- PCB记录了操作系统所需的、用于描述进程的当前情况以及控制进程运行的全部信息

所以：在进程进行切换的时候CPU中的数据保存到了PCB中，供CPU回来时读取恢复！

## 4.Linux select 多路复用函数

select就是一个函数：只要传入相应的参数就能获得相应的数据：

1、们所关心的文件描述符fd;

2、描述符中我们关心的状态：读事件、写事件、等

3、等待时间

调用结束后内核会返回相关信息给我们！

做好准备的个数

哪些已经做好准备；有了这些返回信息，我们就可以调用合适的IO函数！这些函数就不会再被阻塞了；-

**函数详解**

```text
int select(int maxfdp1, fd_set *readset, fd_set *writeset, fd_set *exceptset, timeval *timeout)
    
- maxfdp1 readset 和 wirteset中的最大有数据位
- readset  bitmap结构的位信息；保存我们需要读取的socket序号；
- writeset 写数据信息
- exceptset 异常信息
```

![img](https://pic2.zhimg.com/80/v2-c2de5f86c6dec2894887b03ac2073865_720w.webp)

select函数这里不再细讲，可以翻看以前的文章

![img](https://pic1.zhimg.com/80/v2-431d325945c3906ccaddbf0951267e70_720w.webp)

将函数需要的参数准备好之后调用select；

select进行80中断；将rset数据拷贝到内核中；查询对应的状态之后设置rset对应的位置值，

完成后又拷贝到用户态中的rset；这样一来rset里面的位信息就代表了哪些socket是准备好了的！

随后遍历这些位信息就可以调用read或wirte进行缓冲区的操作了！

**缺点**

可以看到，while死循环中每次执行都将rset重新置位；然后循环重新SET位信息；随后才会发起请求！过程较为繁琐且重复！

## 5.select多路复用器底层原理分析

![img](https://pic2.zhimg.com/80/v2-aeec11d654478255ad0a51a6fb961d3d_720w.webp)

![img](https://pic4.zhimg.com/80/v2-638aaed8d22eb18eb77312de7314bf4f_720w.webp)

![img](https://pic1.zhimg.com/80/v2-b8822b3ecb523af8555dfe17912f0154_720w.webp)

![img](https://pic1.zhimg.com/80/v2-4e8b9c2203d29f3b4de412689fe0b84c_720w.webp)

![img](https://pic2.zhimg.com/80/v2-ba158e38135bc9ca5a094daccc15d0dd_720w.webp)

![img](https://pic3.zhimg.com/80/v2-0bbe7c7d184546082f32bc6e9bd52c92_720w.webp)

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1321' height='931'></svg>)

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1140' height='456'></svg>)

![img](https://pic2.zhimg.com/80/v2-1a3f5065af663ee69dbd10614c2a54e5_720w.webp)

## 6.epoll函数

了解到select的缺点后发现：select每次得到数据都要进行复位，然后又进行重复的步骤去内核中获取信息；感觉就是很多时间都花在重复的劳动上，为了解决这个问题，linux在2.6引入epoll模型，单独在内核区域开辟一块空间来做select主动去做的事，select是主动查，epoll则是准备数据，线程来了直接取就行了；大大提升了性能

既然是函数，看看相关的函数实现：

实现思路：

在内核创建一块空间；总所周知；linux下一切皆文件；所以所谓创建的空间也就是一个文件描述符fd,然后这个文件结构中有两个指针指向另外两个地址空间：事件队列、就绪队列

事件队列：存放已经建立所有socket连接

就绪队列：准备就绪的socket；也就是read或write的时候不用阻塞的socket；

其实epoll就像一个数据库；里面有两个数据表；一个放连接列表；一个放准备就绪的连接列表；

既然有这两个队列；就要涉及到增删查；这就是另外两个函数的来由；

```text
创建epoll空间
int epoll_create(int size);


int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);



对事件队列进行增删改：

epfd : epoll的文件描述符号：因为内核中可能有多个epoll

op : 参数op有以下几个值： EPOLL_CTL_ADD：注册新的fd到epfd中，并关联事件event； EPOLL_CTL_MOD：修改已经注册的fd的监听事件； EPOLL_CTL_DEL：从epfd中移除fd，并且忽略掉绑定的event，这时event可以为null；

fd : 表示socket对应的文件描述符。

![img](https://pic2.zhimg.com/80/v2-cae16b763e168bf16720da3e890eac99_720w.webp)

## 7.epoll底层原理解析

![img](https://pic3.zhimg.com/80/v2-3703c7d0d7a7ef6d8623ae8ee8dacbb2_720w.webp)

![img](https://pic1.zhimg.com/80/v2-5d3bbd408d81106f19ad52dfbcda964c_720w.webp)

![img](https://pic3.zhimg.com/80/v2-9bef7d9fde32f7e450f74a61bce2be56_720w.webp)

![img](https://pic3.zhimg.com/80/v2-50010b451e124c6c3ba699bd93a1fc46_720w.webp)

![img](https://pic1.zhimg.com/80/v2-150960e8b7a37e2745d1ce34a1db0614_720w.webp)

![img](https://pic1.zhimg.com/80/v2-8641d18bf33650a6a91bef5ea04bd360_720w.webp)

![img](https://pic2.zhimg.com/80/v2-ffed1f95b69d403ec04a9fb1f0efe5cd_720w.webp)

![img](https://pic4.zhimg.com/80/v2-4cfe93d9f02c4a5d46fa1843ea7d3ee3_720w.webp)

![img](https://pic3.zhimg.com/80/v2-68fd0316d0c532fe72ba3b6140cf57ca_720w.webp)

![img](https://pic2.zhimg.com/80/v2-68ec3366c33de62676ebeb3157feae91_720w.webp)

![img](https://pic1.zhimg.com/80/v2-ac3e968a8b0c69ed18277de06b6a2a14_720w.webp)

![img](https://pic2.zhimg.com/80/v2-a2b3a7c5c46f0a01afb065847191566d_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/579368540   

作者： linux

# 【NO.40】C++开发中使用协程需要注意的问题

![img](https://pic2.zhimg.com/80/v2-0690aa296b825c1e84e5580ac6dbb0f1_720w.webp)

在异步操作里，如异步连接、异步读写之类的协程，co_await这些协程时需要注意线程切换的细节。

以asio异步连接协程为例：

```text
class client {
public:
    client() {
        thd_ = std::thread([this]{
            io_ctx_.run();
        });
    }

    async_simple::coro::Lazy<bool> async_connect(auto host, auto port) {
        bool ret = co_await util::async_connect(host, port);  #1
        co_return ret;      #2                                 
    }

    ~client() {
        io_ctx_.stop();
        if(thd_.joinable()) {
            thd_.join();
        }
    }

private:
  asio::io_context io_ctx_;
  std::thread thd_;
};

int main() {
    client c;
    async_simple::coro::syncAwait(c.async_connect());
    std::cout<<"quit\n"; #3
}
```

这个例子很简单，client在连接之后就析构了，看起来没什么问题。但是运行之后就会发生线程join的错误，错误的意思是在线程里join自己了。这是怎么回事？co_await一个异步连接的协程，当连接成功后协程返回，这时候发生了线程切换。异步连接返回的时候是在io_context的线程里，代码中的#1在主线程，#2在io_context线程，之后就co_return 返回到main函数的#3，这时候#3仍然在io_context线程里，接着client就会析构了，这时候仍然在io_context线程里，析构的时候会调用thd_.join(); 然后就导致了在io_context的线程里join自己的错误。

这是使用协程时容易犯错的一个地方，解决方法就是避免co_await回来之后去析构client，或者co_await回来仍然回到主线程。这里可以考虑用协程条件变量，在异步连接的时候发起一个新的协程并传入协程条件变量并在连接返回后set_value，主线程去co_await这个条件变量，这样连接返回后就回到主线程了，就可以解决在io线程里join自己的问题了。

![img](https://pic2.zhimg.com/80/v2-0690aa296b825c1e84e5580ac6dbb0f1_720w.webp)

还是以上面的异步连接为例子，需要对之前的async_connect协程增加一个超时功能，代码稍作修改：

```text
class client {
public:
    client() : socket_(io_ctx_) {
        thd_ = std::thread([this]{
            io_ctx_.run();
        });
    }

    async_simple::coro::Lazy<bool> async_connect(auto host, auto port, auto duration) {
        coro_timer timer(io_ctx_);
        timeout(timer, duration).start([](auto&&){}); // #1 启动一个新协程做超时处理
        bool ret = co_await util::async_connect(host, port, socket_);//假设这里co_await返回后回到主线程
        co_return ret;                                      
    }

    ~client() {
        io_ctx_.stop();
        if(thd_.joinable()) {
            thd_.join();
        }
    }


private:
  async_simple::coro::Lazy<void> timeout(auto &timer, auto duration) {
    bool is_timeout = co_await timer.async_wait(duration);
    if(is_timeout) {
        asio::error_code ignored_ec;
        socket_.shutdown(tcp::socket::shutdown_both, ignored_ec);
        socket_.close(ignored_ec);
    }

    co_return;
  }

  asio::io_context io_ctx_;
  tcp::socket socket_;
  std::thread thd_;
  bool is_timeout_;
};

int main() {
    client c;
    async_simple::coro::syncAwait(c.async_connect("localhost", "9000", 5s));
    std::cout<<"quit\n"; #3
}
```

这个代码增加连接超时处理的协程，注意#1那里为什么需要新启动一个协程，而不能用co_await呢？因为co_await是阻塞语义，co_await会导致永远超时，启动一个新的协程不会阻塞当前协程从而可以去调用async_connect。

当timeout超时发生时就关闭socket，这时候async_connect就会返回错误然后返回到调用者，这看起来似乎可以对异步连接做超时处理了，但是这个代码是有问题的。假如异步连接没有超时会发生什么？没有超时的话就返回到main函数了，然后client就析构了，当timeout协程resume回来的时候client其实已经析构了，这时候再去调用成员变量socket_ close将会导致一个访问已经析构对象的错误。

也许有人会说，那就在co_return之前去取消timer不就好了吗？这个办法也不行，因为取消timer，timeout协程并不会立即返回，仍然会存在访问已经析构对象的问题。

正确的做法应该是对两个协程进行同步，timeout协程和async_connect协程需要同步，在async_connect协程返回之前需要确保timeout协程已经完成，这样就可以避免访问已经析构对象的问题了。

这个问题其实也是异步回调安全返回的一个经典问题，协程也同样会遇到这个问题，上面提到的对两个协程进行同步是解决方法之一，另外一个方法就是使用shared_from_this，就像异步安全回调那样处理。



还是以异步连接为例：

```text
async_simple::coro::Lazy<bool> async_connect(const std::string &host, const std::string& port) {
    co_return co_await util::async_connect(host, port);
}

async_simple::coro::Lazy<void> test_connect() {
    bool ok = co_await async_connect("localhost", "8000");
    if(!ok){
        std::cout<<"connect failed\n";
    }

    std::cout<<"connect ok\n";
}

int main() {
    async_simple::coro::syncAwait(test_connect());
}
```

这个代码简单明了，就是测试一下异步连接是否成功，运行也是正常的。如果稍微改一下test_connect：

```text
async_simple::coro::Lazy<void> test_connect() {
    auto lazy = async_connect("localhost", "8000");
    bool ok = co_await lazy;
    if(!ok){
        std::cout<<"connect failed\n";
    }

    std::cout<<"connect ok\n";
}
```

很遗憾，这个代码会导致连接总是失败，似乎很奇怪，后面发现原因是因为async_connect的两个参数失效了，但是写法和刚开始的写法几乎一样，为啥后面这种写法会导致参数失效呢？

原因是co_await一个协程函数时，其实做了两件事：

- 调用协程函数创建协程，这个步骤会创建协程帧，把参数和局部变量拷贝到协程帧里；
- co_await执行协程函数；

回过头来看auto lazy = async_connect("localhost", "8000"); 这个代码调用协程函数创建了协程，这时候拷贝到协程帧里面的是两个临时变量，在这一行结束的时候临时变量就析构了，在下一行去co_await执行这个协程的时候就会出现参数失效的问题了。

co_await async_connect("localhost", "8000"); 这样为什么没问题呢，因为协程创建和协程调用都在一行完成的，临时变量知道协程执行之后才会失效，因此不会有问题。

问题的本质其实是C++临时变量生命周期的问题。使用协程的时候稍微注意一下就好了，可以把const std::string&改成std::string，这样就不会临时变量生命周期的问题了，如果不想改参数类型就co_await 协程函数就好了，不分成两行去执行协程。

原文地址：https://zhuanlan.zhihu.com/p/575882816    

作者：linux

# 【NO.41】深入源码理解TCP建立连接过程（3次握手）

从TCP源码上深入了解三次握手过程

## 1.应用程序的基本框架

```text
//server
int main()
{
    int fd = socket(AF_INET,SOCK_STREAM,0);
    bind(fd，...);
    listen(fd,256);
    accept(fd,...);
 
}
 
//client
int main()
{
    fd = connect(AF_INET,SOCK_STREAM,0);
    connect(fd,...);
}
```

**三次握手概括**

![img](https://pic2.zhimg.com/80/v2-ab82f66854555f94d39a649815e181d1_720w.webp)

分析开始

## 2.客户端connect

```text
int tcp_v4_connect(struct sock *sk, struct sockaddr *uaddr, int addr_len)
{
    //设置socket状态TCP_SYN_SENT
	tcp_set_state(sk, TCP_SYN_SENT);
 
	//动态选择一个端口
	err = inet_hash_connect(tcp_death_row, sk);
    
	//根据sk中的信息，构建一个SYNC的报文，并将它发送出去
	err = tcp_connect(sk);
}
 
 
int tcp_connect(struct sock *sk)
{
	//申请skb
	buff = sk_stream_alloc_skb(sk, 0, sk->sk_allocation, true);
 
 
	//添加到发送队列sk_write_queue
	tcp_connect_queue_skb(sk, buff);
	tcp_ecn_send_syn(sk, buff);
	tcp_rbtree_insert(&sk->tcp_rtx_queue, buff);
 
	//发送syn  tcp_transmit_skb
	err = tp->fastopen_req ? tcp_send_syn_data(sk, buff) :
	      tcp_transmit_skb(sk, buff, 1, sk->sk_allocation);
 
	//重启定时器
	/* Timer for repeating the SYN until an answer. */
	inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,
				  inet_csk(sk)->icsk_rto, TCP_RTO_MAX);
	return 0;
}
```

connect 作用把本地socket状态设置成TCP_SYN_SENT；

选择一个可用的端口，发出SYN握手请求并重置定时器；

## 3.服务端响应SYN

```text
int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
{
	//服务端收到SYN或者第三步ACK都会走到这里
	if (sk->sk_state == TCP_LISTEN) {
		struct sock *nsk = tcp_v4_cookie_check(sk, skb);//查看半连接队列
 
 
	} else
		sock_rps_save_rxhash(sk, skb);
    //不同的状态处理
	if (tcp_rcv_state_process(sk, skb)) {
		rsk = sk;
		goto reset;
	}
}
```



```text
//服务端处理syn连接请求
int tcp_conn_request(struct request_sock_ops *rsk_ops,
		     const struct tcp_request_sock_ops *af_ops,
		     struct sock *sk, struct sk_buff *skb)
{
 
	//查看半连接队列是否满了，则直接丢弃
	if ((net->ipv4.sysctl_tcp_syncookies == 2 ||
	     inet_csk_reqsk_queue_is_full(sk)) && !isn) {
		want_cookie = tcp_syn_flood_action(sk, rsk_ops->slab_name);
		if (!want_cookie)
			goto drop;
	}
	//查看全连接队列，如果满了则直接丢弃
	if (sk_acceptq_is_full(sk)) {
		NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);
		goto drop;
	}
	//分配request_sock内核对象
	req = inet_reqsk_alloc(rsk_ops, sk, !want_cookie);
 
 
	if (fastopen_sk) {
		af_ops->send_synack(fastopen_sk, dst, &fl, req,
				    &foc, TCP_SYNACK_FASTOPEN);
		/* Add the child socket directly into the accept queue */
	} else {
		tcp_rsk(req)->tfo_listener = false;
		if (!want_cookie)//添加到半连接队列，并开启定时器
			inet_csk_reqsk_queue_hash_add(sk, req,
				tcp_timeout_init((struct sock *)req));
		//构造synack包
		af_ops->send_synack(sk, dst, &fl, req, &foc,
				    !want_cookie ? TCP_SYNACK_NORMAL :
						   TCP_SYNACK_COOKIE);
 
	}
```

.send_synack就是tcp_v4_send_synack

```text
static int tcp_v4_send_synack(const struct sock *sk, struct dst_entry *dst,
			      struct flowi *fl,
			      struct request_sock *req,
			      struct tcp_fastopen_cookie *foc,
			      enum tcp_synack_type synack_type)
{
 
	if (!dst && (dst = inet_csk_route_req(sk, &fl4, req)) == NULL)
		return -1;
	//构造syn+ack包
	skb = tcp_make_synack(sk, dst, req, foc, synack_type);
 
	if (skb) {
		__tcp_v4_send_check(skb, ireq->ir_loc_addr, ireq->ir_rmt_addr);
 
		rcu_read_lock();
		//将synack包发送出去
		err = ip_build_and_send_pkt(skb, sk, ireq->ir_loc_addr,
					    ireq->ir_rmt_addr,
					    rcu_dereference(ireq->ireq_opt));
		rcu_read_unlock();
		err = net_xmit_eval(err);
	}
 
	return err;
}
```

服务端响应SYN

1. 检查半连接与全连接队列是否满，满握手直接丢弃
2. 申请request_sock,并添加到半连接队列中
3. 构造syn+ack包，通过ip_build_and_send_pkt发送
4. 重启定时器tcp_timeout_init



## 4.客户端响应SYN+ACK

客户端收到服务端发来的syn+ack包的时候，进入tcp_rcv_state_process

```text
int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)
{
	switch (sk->sk_state) {
	case TCP_CLOSE:
		goto discard;
	//第一次握手 服务器收到
	case TCP_LISTEN:
 
		goto discard;
	//客户端第二次握手处理
	case TCP_SYN_SENT:
		//synack包
		queued = tcp_rcv_synsent_state_process(sk, skb, th);
		return 0;
	}
}
```



```text
static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,
					 const struct tcphdr *th)
{
        //step1:
		tcp_ack(sk, skb, FLAG_SLOWPATH);
 
		//step2:tcp 建立完成
		tcp_finish_connect(sk, skb);
 
 
		if (sk->sk_write_pending ||
		    icsk->icsk_accept_queue.rskq_defer_accept ||
		    inet_csk_in_pingpong_mode(sk)) {
 
			return 0;
		} else {
            //step3:发送确认
			tcp_send_ack(sk);
		}
}
```

step1:

```text
/* This routine deals with incoming acks, but not outgoing ones. */
static int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)
{
    //删除发送队列
   flag |= tcp_clean_rtx_queue(sk, prior_fack, prior_snd_una, &sack_state);
 
    //重置定时器
	if (flag & FLAG_SET_XMIT_TIMER)
		tcp_set_xmit_timer(sk);
}
```

step2

```text
void tcp_finish_connect(struct sock *sk, struct sk_buff *skb)
{
	//修改socket状态
	tcp_set_state(sk, TCP_ESTABLISHED);
	icsk->icsk_ack.lrcvtime = tcp_jiffies32;
 
	//拥塞控制
	tcp_init_transfer(sk, BPF_SOCK_OPS_ACTIVE_ESTABLISHED_CB);
	tp->lsndtime = tcp_jiffies32;
 
	//打开保活计时器
	if (sock_flag(sk, SOCK_KEEPOPEN))
		inet_csk_reset_keepalive_timer(sk, keepalive_time_when(tp));
 
}
```

step3：

```text
void __tcp_send_ack(struct sock *sk, u32 rcv_nxt)
{
	if (sk->sk_state == TCP_CLOSE)
		return;
 
	//申请和构造ack包
	buff = alloc_skb(MAX_TCP_HEADER,
			 sk_gfp_mask(sk, GFP_ATOMIC | __GFP_NOWARN));
 
	//发送出去
	__tcp_transmit_skb(sk, buff, 0, (__force gfp_t)0, rcv_nxt);
}
```

总结

1. 客户端响应synack，清除重传定时器
2. 设置当前状态为ESTABLISHED
3. 开启拥塞控制，保活机制
4. 发送ack包

## 5.服务器端响应ACK

```text
int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
{
 
    //服务端收到SYN或者第三步ACK都会走到这里
	if (sk->sk_state == TCP_LISTEN) {
		struct sock *nsk = tcp_v4_cookie_check(sk, skb);//step1:查看半连接队列,新创建线程
 
 
    //step2:
	if (tcp_rcv_state_process(sk, skb)) 
}
```

step1：创建子socket；添加全连接队列

```text
//tcp_v4_cookie_check->cookie_v4_check->tcp_get_cookie_sock
struct sock *tcp_get_cookie_sock(struct sock *sk, struct sk_buff *skb,
				 struct request_sock *req,
				 struct dst_entry *dst, u32 tsoff)
{
	struct inet_connection_sock *icsk = inet_csk(sk);
	struct sock *child;
	bool own_req;
 
	//创建子sock 回调函数在下面tcp_v4_syn_recv_sock
	child = icsk->icsk_af_ops->syn_recv_sock(sk, skb, req, dst,
						 NULL, &own_req);
	if (child) {
		//添加全连接队列
		if (inet_csk_reqsk_queue_add(sk, req, child))
			return child;
		bh_unlock_sock(child);
		sock_put(child);
	}
	return NULL;
}
 
 
//添加到全连接队列
struct sock *inet_csk_reqsk_queue_add(struct sock *sk,
				      struct request_sock *req,
				      struct sock *child)
{
	struct request_sock_queue *queue = &inet_csk(sk)->icsk_accept_queue;
		req->sk = child;
		req->dl_next = NULL;
		if (queue->rskq_accept_head == NULL)
			WRITE_ONCE(queue->rskq_accept_head, req);
}
 
 
 
const struct inet_connection_sock_af_ops ipv4_specific = {
	.syn_recv_sock	   = tcp_v4_syn_recv_sock,
}
//子socket的创建过程
struct sock *tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,
				  struct request_sock *req,
				  struct dst_entry *dst,
				  struct request_sock *req_unhash,
				  bool *own_req)
{
	struct ip_options_rcu *inet_opt;
 
	//判断队列是否满了
	if (sk_acceptq_is_full(sk))
		goto exit_overflow;
		
	//创建socket
	newsk = tcp_create_openreq_child(sk, req, skb);
	if (!newsk)
		goto exit_nonewsk;
}
```

step2:设置连接状态TCP_ESTABLISHED

```text
int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)
{
 
	switch (sk->sk_state) {
	case TCP_CLOSE:
		goto discard;
	//第一次握手
	case TCP_LISTEN:
	//客户端第二次握手处理
	case TCP_SYN_SENT:
	switch (sk->sk_state) {
	//服务器第三次握手
	case TCP_SYN_RECV:
 
		//改变连接状态
		tcp_set_state(sk, TCP_ESTABLISHED);
}
```

总结

1. 服务端将状态设置为ESTABLISHED;
2. 创建新sock加入全连接队列中

## 6.最后accept过程

```text
struct sock *inet_csk_accept(struct sock *sk, int flags, int *err, bool kern)
{
	struct inet_connection_sock *icsk = inet_csk(sk);
	//全连接队列获取元素
	struct request_sock_queue *queue = &icsk->icsk_accept_queue;
 
	//取出一个给用户使用
	req = reqsk_queue_remove(queue, sk);
	newsk = req->sk;
}
```

从全连接队列中取出一个给用户使用

原文地址：https://zhuanlan.zhihu.com/p/575657861    

作者： linux

# 【NO.42】这是我见过最详细的Nginx 内存池分析

# **1.为什么要使用内存池**

　　大多数的解释不外乎提升程序的处理性能及减小内存中的碎片，对于性能优化这点主要体现在：
　　（1）系统的malloc/free等内存申请函数涉及到较多的处理，如申请时合适空间的查找，释放时的空间合并。
　　（2）默认的内存管理函数还会考虑多线程的应用，加锁操作会增加开销。
　　（3）每次申请内存的系统态与用户态的切换也及为的消耗性能。
　　对于由于应用的频繁的在堆上分配及释放空间所带来的内存碎片化，其实主流的思想是认为存在的，不过也有人认为这种考虑其实是多余的，在“内存池到底为我们解决了什么问题”一文中则认为，大量的内存申请与释放仅会造成短暂的内存碎片化的产生，并不会引起大量内存的长久碎片化，从而导致最后申请大内存时的完全不可用性。文中认为对于确定的应用均是“有限对象需求”，即在任一程序中申请与释放的对象种类总是有限的，大小也总是有一定重复性的，这样在碎片产生一段时间后，会因为同样的对象申请而消除内存的临时碎片化。

　　不过，综上，内存池有利于提升程序在申请及释放内存时的处理性能这点是确定的。内存池的主要优点有：
　　（1）特殊情况的频繁的较小的内存空间的释放与申请不需要考虑复杂的分配释放方法，有较高的性能。
　　（2）初始申请时通常申请一块较大的连续空间的内存区域，因此进行管理及内存地址对齐时处理非常方便。
　　（3）小块内存的申请通常不用考虑实际的释放操作。

# **2.内存池的原理及实现方法**

内存池的原理基本是内存的提前申请，重复利用。其中主要需要关注的是内存池的初始化，内存分配及内存释放。
内存池的实现方法主要分两种：
一种是固定式，即提前申请的内存空间大小固定，空间划分成固定大小的内存单元以供使用如下图示：

![动图封面](https://pic1.zhimg.com/v2-86aaf64fd29706e5cbb5e02f875af482_720w.jpg?source=d16d100b)



另一种是动态式，即初始申请固定大小的单元，空间不做明确的大小划分，而是根据需要提供合适大小的空间以供使用。
不过，这两种方式在处理大空间的内存申请时的处理方法通常都是采用系统的内存申请调用。

# **3.Nginx的内存池实现**

nginx的内存池设计得非常精妙，它在满足小块内存申请的同时，也处理大块内存的申请请求，同时还允许挂载自己的数据区域
及对应的数据清楚操作。
nginx内存池实现主要是在core/ngx_palloc.{h,c}中，一些支持函数位于os/unix/ngx_alloc.{h,c}中，支持函数主要是对原有的
malloc/free/memalign等函数的封装，对就的函数为：
》ngx_alloc 完成malloc的封装
》ngx_calloc 使用malloc分配空间，同时使用memset完成初始化
》ngx_memalign 会根据系统不同而调用不一样的函数处理，如posix系列使用posix_memalign,windows则不考虑对齐。主要作用
　　　　　　　　　是申请指定的alignment对齐的起始地址的内存空间。
》ngx_free 完成free的封装

nginx内存池中有两个非常重要的结构，一个是ngx_pool_s,主要是作为整个内存池的头部，管理内存池结点链表，大内存链表，
cleanup链表等，具体结构如下：

```text
//该结构维护整个内存池的头部信息

struct ngx_pool_s {
ngx_pool_data_t d; //数据块
size_t max; //数据块大小，即小块内存的最大值
ngx_pool_t *current; //保存当前内存值
ngx_chain_t *chain; //可以挂一个chain结构
ngx_pool_large_t *large; //分配大块内存用，即超过max的内存请求
ngx_pool_cleanup_t *cleanup; //挂载一些内存池释放的时候，同时释放的资源
ngx_log_t *log;
};
```

另一重要的结构为ngx_pool_data_s,这个是用来连接具体的内存池结点的，具体如下：

```text
//该结构用来维护内存池的数据块，供用户分配之用
typedef struct {
u_char *last; //当前内存分配结束位置，即下一段可分配内存的起始位置
u_char *end; //内存池结束位置
ngx_pool_t *next; //链接到下一个内存池
ngx_uint_t failed;//统计该内存池不能满足分配请求的次数
} ngx_pool_data_t;
```

还有另两个结构ngx_pool_large_t,ngx_pool_cleanup_t，如下示：

```text
//大内存结构
struct ngx_pool_large_s {
ngx_pool_large_t *next; //下一个大块内存
void *alloc;//nginx分配的大块内存空间
};

struct ngx_pool_cleanup_s {
ngx_pool_cleanup_pt handler; //数据清理的函数句柄
void *data; //要清理的数据
ngx_pool_cleanup_t *next; //连接至下一个
};
```

然后我们具体看一下nginx内存池的组成结构，如下图示：

![img](https://pic3.zhimg.com/80/v2-14986487e035e3d72b43ffff116c313e_720w.webp)

上面的图中，current指针是指向的首结点，在具体的运行过程中是会根据failed值进行调整的。还有就是
ngx_pool_cleanup_s与ngx_pool_large_s的结构空间均来自内存池结点。

**然后看nginx相关的操作：** 

## **3.1创建内存池**

内存池的创建是在ngx_create_pool函数中完成的，实现如下：

```text
//创建内存池
ngx_pool_t *
ngx_create_pool(size_t size, ngx_log_t *log)
{
    ngx_pool_t  *p;
    //ngx_memalign实际上会依据os不用，分情况处理，在os不支持memalign情况的分配时，选择直接分配内存
    p = ngx_memalign(NGX_POOL_ALIGNMENT, size, log);  // 分配内存函数，uinx,windows分开走
    if (p == NULL) {
        return NULL;
    }

    p->d.last = (u_char *) p + sizeof(ngx_pool_t); //初始指向 ngx_pool_t 结构体后面
    p->d.end = (u_char *) p + size; //整个结构的结尾后面
    p->d.next = NULL;
    p->d.failed = 0;

    size = size - sizeof(ngx_pool_t);
    //实际上pool数据区的大小与系统页的大小有关的
    p->max = (size < NGX_MAX_ALLOC_FROM_POOL) ? size : NGX_MAX_ALLOC_FROM_POOL;
    //最大不超过 NGX_MAX_ALLOC_FROM_POOL,也就是getpagesize()-1 大小
    p->current = p;
    p->chain = NULL;
    p->large = NULL;
    p->cleanup = NULL;
    p->log = log;

    return p;
}
```

## **3.2销毁内存池**

内存池的销毁位于ngx_destroy_pool(ngx_pool_t *pool)中，此函数会清理所有的内存池结点，同时清理large链表的内存
并且对于注册的cleanup链表的清理操作也会进行。具体实现如下：

```text
void
ngx_destroy_pool(ngx_pool_t *pool)
{
    ngx_pool_t          *p, *n;
    ngx_pool_large_t    *l;
    ngx_pool_cleanup_t  *c;
    //会先调用cleanup函数进行清理操作,不过这儿是对自己指向的数据进行清理
    for (c = pool->cleanup; c; c = c->next) {
        if (c->handler) {
            ngx_log_debug1(NGX_LOG_DEBUG_ALLOC, pool->log, 0,
                           "run cleanup: %p", c);
            c->handler(c->data);
        }
    }
    //这儿是作大数据块的清除
    for (l = pool->large; l; l = l->next) {

        ngx_log_debug1(NGX_LOG_DEBUG_ALLOC, pool->log, 0, "free: %p", l->alloc);

        if (l->alloc) {
            ngx_free(l->alloc);
        }
    }

#if (NGX_DEBUG)

    /*
     * we could allocate the pool->log from this pool
     * so we cannot use this log while free()ing the pool
     */

    for (p = pool, n = pool->d.next; /* void */; p = n, n = n->d.next) {
        ngx_log_debug2(NGX_LOG_DEBUG_ALLOC, pool->log, 0,
                       "free: %p, unused: %uz", p, p->d.end - p->d.last);

        if (n == NULL) {
            break;
        }
    }

#endif

    for (p = pool, n = pool->d.next; /* void */; p = n, n = n->d.next) {
        ngx_free(p);

        if (n == NULL) {
            break;
        }
    }
}
```

## **3.3分配内存**

从内存池中分配内存涉及到几个函数，如下：

```text
void *ngx_palloc(ngx_pool_t *pool, size_t size); //palloc取得的内存是对齐的
void *ngx_pnalloc(ngx_pool_t *pool, size_t size); //pnalloc取得的内存是不对齐的
void *ngx_pcalloc(ngx_pool_t *pool, size_t size); //pcalloc直接调用palloc分配好内存，然后进行一次0初始化操作
void *ngx_pmemalign(ngx_pool_t *pool, size_t size, size_t alignment); //在分配size大小的内存，并按照alignment对齐，然后挂到large字段下
static void *ngx_palloc_block(ngx_pool_t *pool, size_t size); //申请新的内存池结点
static void *ngx_palloc_large(ngx_pool_t *pool, size_t size); //申请大的内存块
```

下面仅对部分函数进行源码的分析：
首先来看ngx_palloc()函数，其源码为：

```text
//有内存对齐的空间申请
void *
ngx_palloc(ngx_pool_t *pool, size_t size)
{
    u_char      *m;
    ngx_pool_t  *p;

    if (size <= pool->max) {
        //从current遍历到链表末尾,不找前面原因其实是因为failed的控制机制，保证前面的节点
        //基本处于満的状态。仅剩余部分小块区域。
        p = pool->current;

        do {
            m = ngx_align_ptr(p->d.last, NGX_ALIGNMENT); // 对齐内存指针，加快存取速度

            if ((size_t) (p->d.end - m) >= size) {
                p->d.last = m + size;

                return m;
            }

            p = p->d.next;

        } while (p);
        //遍历结束也不能找到合适的可以满足申请要求的结点则新建结点
        return ngx_palloc_block(pool, size);
    }
    //申请大内存时的处理
    return ngx_palloc_large(pool, size);
}
```

其中涉及到两个函数，分别为ngx_palloc_block，ngx_palloc_large先来看ngx_palloc_block，其源码如下：

```text
//申请新的内存池块
static void *
ngx_palloc_block(ngx_pool_t *pool, size_t size)
{
    u_char      *m;
    size_t       psize;
    ngx_pool_t  *p, *new, *current;

    psize = (size_t) (pool->d.end - (u_char *) pool);

    m = ngx_memalign(NGX_POOL_ALIGNMENT, psize, pool->log);
    if (m == NULL) {
        return NULL;
    }

    new = (ngx_pool_t *) m;

    new->d.end = m + psize;
    new->d.next = NULL;
    new->d.failed = 0;
    //这儿有个细节，新的节点可以用ngx_pool_t指针表示，但具体的数据存储则是ngx_pool_data_t.
    m += sizeof(ngx_pool_data_t);
    m = ngx_align_ptr(m, NGX_ALIGNMENT);
    new->d.last = m + size;
    //这儿是调整current指针，每一次空间申请失败都会导致current至内存池链表结尾的
    //结点的failed次数加1，这样在连续分配时，当前current其后的几个结点，其实也差不多
    //处于饱和状态，然后这时将current一次调至失败次数较小的结点是合理的，不过判断跳转时机
    //是依据经验值的。
    current = pool->current;

    for (p = current; p->d.next; p = p->d.next) {
        if (p->d.failed++ > 4) {
            current = p->d.next;
        }
    }

    p->d.next = new;

    pool->current = current ? current : new;

    return m;
}
```

然后是ngx_palloc_large,其源码如下：

```text
//控制大块内存的申请
static void *
ngx_palloc_large(ngx_pool_t *pool, size_t size)
{
    void              *p;
    ngx_uint_t         n;
    ngx_pool_large_t  *large;
    //ngx_alloc仅是对alloc的简单封装
    p = ngx_alloc(size, pool->log);
    if (p == NULL) {
        return NULL;
    }

    n = 0;

    for (large = pool->large; large; large = large->next) {
        if (large->alloc == NULL) {
            large->alloc = p;
            return p;
        }

        if (n++ > 3) {
            break;
        }
    }

    large = ngx_palloc(pool, sizeof(ngx_pool_large_t));
    if (large == NULL) {
        ngx_free(p);
        return NULL;
    }

    large->alloc = p;
    large->next = pool->large;
    pool->large = large;

    return p;
}
```

## **3.4其余的函数**

内存池中还一些其它支持函数，这里不细说了：

```text
ngx_pool_cleanup_t *ngx_pool_cleanup_add(ngx_pool_t *p, size_t size);
void ngx_pool_run_cleanup_file(ngx_pool_t *p, ngx_fd_t fd);
void ngx_pool_cleanup_file(void *data);
void ngx_pool_delete_file(void *data);
```

# 4.下面是一全例子

这个例子主要是演示下nginx内存池的使用，代码如下：

```text
/*
 * author:doop-ymc
 * date:2013-11-11
 * version:1.0
 */

#include <stdio.h>
#include "ngx_config.h"
#include "ngx_conf_file.h"
#include "nginx.h"
#include "ngx_core.h"
#include "ngx_string.h"
#include "ngx_palloc.h"

#define MY_POOL_SIZE 5000

volatile ngx_cycle_t  *ngx_cycle;

void 
ngx_log_error_core(ngx_uint_t level, ngx_log_t *log, ngx_err_t err,
    const char *fmt, ...)
{

}

void 
echo_pool(ngx_pool_t* pool)
{
    int                  n_index;
    ngx_pool_t          *p_pool;
    ngx_pool_large_t    *p_pool_large;

    n_index = 0;
    p_pool = pool;
    p_pool_large = pool->large;

    printf("------------------------------\n");
    printf("pool begin at: 0x%x\n", pool);

    do{
        printf("->d         :0x%x\n", p_pool);
        printf("        last = 0x%x\n", p_pool->d.last);
        printf("        end  = 0x%x\n", p_pool->d.end);
        printf("        next = 0x%x\n", p_pool->d.next);
        printf("      failed = %d\n", p_pool->d.failed);
        p_pool = p_pool->d.next;
    }while(p_pool);
    printf("->max       :%d\n", pool->max);
    printf("->current   :0x%x\n", pool->current);
    printf("->chain     :0x%x\n", pool->chain);
    
    if(NULL == p_pool_large){
        printf("->large     :0x%x\n", p_pool_large);
    }else{
        do{
            printf("->large     :0x%x\n", p_pool_large);
            printf("        next = 0x%x\n", p_pool_large->next);
            printf("       alloc = 0x%x\n", p_pool_large->alloc);
            p_pool_large = p_pool_large->next;
        }while(p_pool_large);
    }
    
    printf("->cleanup   :0x%x\n", pool->cleanup);
    printf("->log       :0x%x\n\n\n", pool->log);
    
}

int main()
{
    ngx_pool_t *my_pool;

    /*create pool size:5000*/
    my_pool = ngx_create_pool(MY_POOL_SIZE, NULL);
    if(NULL == my_pool){
        printf("create nginx pool error,size %d\n.", MY_POOL_SIZE);
        return 0;
    }
    printf("+++++++++++CREATE NEW POOL++++++++++++\n");
    echo_pool(my_pool);

    printf("+++++++++++ALLOC 2500+++++++++++++++++\n");
    ngx_palloc(my_pool, 2500);
    echo_pool(my_pool);

    printf("+++++++++++ALLOC 2500+++++++++++++++++\n");
    ngx_palloc(my_pool, 2500);
    echo_pool(my_pool);

    printf("+++++++++++ALLOC LARGE 5000+++++++++++\n");
    ngx_palloc(my_pool, 5000);
    echo_pool(my_pool);

    printf("+++++++++++ALLOC LARGE 5000+++++++++++\n");
    ngx_palloc(my_pool, 5000);
    echo_pool(my_pool);

    ngx_destroy_pool(my_pool);
    return 0;

}
```

Makefile文件：

```text
CC = gcc
CFLAGS += -W -Wall -g 

NGX_ROOT_PATH = /home/doop-ymc/nginx/nginx-1.0.14

TARGETS = pool_t
TARGETS_C_FILE = $(TARGETS).c

all: $(TARGETS)

.PHONY:clean

clean:
    rm -f $(TARGETS) *.o

INCLUDE_PATH = -I. \
            -I$(NGX_ROOT_PATH)/src/core \
            -I$(NGX_ROOT_PATH)/src/event \
            -I$(NGX_ROOT_PATH)/src/event/modules \
            -I$(NGX_ROOT_PATH)/src/os/unix \
            -I$(NGX_ROOT_PATH)/objs \

CORE_DEPS = $(NGX_ROOT_PATH)/src/core/nginx.h \
    $(NGX_ROOT_PATH)/src/core/ngx_config.h \
    $(NGX_ROOT_PATH)/src/core/ngx_core.h \
    $(NGX_ROOT_PATH)/src/core/ngx_log.h \
    $(NGX_ROOT_PATH)/src/core/ngx_palloc.h \
    $(NGX_ROOT_PATH)/src/core/ngx_array.h \
    $(NGX_ROOT_PATH)/src/core/ngx_list.h \
    $(NGX_ROOT_PATH)/src/core/ngx_hash.h \
    $(NGX_ROOT_PATH)/src/core/ngx_buf.h \
    $(NGX_ROOT_PATH)/src/core/ngx_queue.h \
    $(NGX_ROOT_PATH)/src/core/ngx_string.h \
    $(NGX_ROOT_PATH)/src/core/ngx_parse.h \
    $(NGX_ROOT_PATH)/src/core/ngx_inet.h \
    $(NGX_ROOT_PATH)/src/core/ngx_file.h \
    $(NGX_ROOT_PATH)/src/core/ngx_crc.h \
    $(NGX_ROOT_PATH)/src/core/ngx_crc32.h \
    $(NGX_ROOT_PATH)/src/core/ngx_murmurhash.h \
    $(NGX_ROOT_PATH)/src/core/ngx_md5.h \
    $(NGX_ROOT_PATH)/src/core/ngx_sha1.h \
    $(NGX_ROOT_PATH)/src/core/ngx_rbtree.h \
    $(NGX_ROOT_PATH)/src/core/ngx_radix_tree.h \
    $(NGX_ROOT_PATH)/src/core/ngx_slab.h \
    $(NGX_ROOT_PATH)/src/core/ngx_times.h \
    $(NGX_ROOT_PATH)/src/core/ngx_shmtx.h \
    $(NGX_ROOT_PATH)/src/core/ngx_connection.h \
    $(NGX_ROOT_PATH)/src/core/ngx_cycle.h \
    $(NGX_ROOT_PATH)/src/core/ngx_conf_file.h \
    $(NGX_ROOT_PATH)/src/core/ngx_resolver.h \
    $(NGX_ROOT_PATH)/src/core/ngx_open_file_cache.h \
    $(NGX_ROOT_PATH)/src/core/ngx_crypt.h \
    $(NGX_ROOT_PATH)/src/event/ngx_event.h \
    $(NGX_ROOT_PATH)/src/event/ngx_event_timer.h \
    $(NGX_ROOT_PATH)/src/event/ngx_event_posted.h \
    $(NGX_ROOT_PATH)/src/event/ngx_event_busy_lock.h \
    $(NGX_ROOT_PATH)/src/event/ngx_event_connect.h \
    $(NGX_ROOT_PATH)/src/event/ngx_event_pipe.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_time.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_errno.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_alloc.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_files.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_channel.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_shmem.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_process.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_setproctitle.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_atomic.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_gcc_atomic_x86.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_thread.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_socket.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_os.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_user.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_process_cycle.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_linux_config.h \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_linux.h \
    $(NGX_ROOT_PATH)/src/core/ngx_regex.h \
    $(NGX_ROOT_PATH)/objs/ngx_auto_config.h


NGX_PALLOC = $(NGX_ROOT_PATH)/objs/src/core/ngx_palloc.o
NGX_STRING = $(NGX_ROOT_PATH)/objs/src/core/ngx_string.o
NGX_ALLOC = $(NGX_ROOT_PATH)/objs/src/os/unix/ngx_alloc.o

$(TARGETS): $(TARGETS_C_FILE) $(NGX_PALLOC) $(NGX_STRING) $(NGX_ALLOC)
    $(CC) $(CFLAGS) $(INCLUDE_PATH)  $^ -o $@

$(NGX_PALLOC):$(CORE_DEPS) \
    $(NGX_ROOT_PATH)/src/core/ngx_palloc.c
    $(CC) -c -g -o0 $(INCLUDE_PATH) -o $(NGX_PALLOC) $(NGX_ROOT_PATH)/src/core/ngx_palloc.c

$(NGX_ALLOC):$(CORE_DEPS) \
    $(NGX_ROOT_PATH)/src/os/unix/ngx_alloc.c
    $(CC) -c -g -o0 $(INCLUDE_PATH) -o $(NGX_ALLOC) $(NGX_ROOT_PATH)/src/os/unix/ngx_alloc.c

$(NGX_STRING):$(CORE_DEPS) \
    $(NGX_ROOT_PATH)/src/core/ngx_string.c
    $(CC) -c -g -o0 $(INCLUDE_PATH) -o $(NGX_STRING) $(NGX_ROOT_PATH)/src/core/ngx_string.c
```

运行结果：

```text
[root@localhost pool]# ./pool_t 
+++++++++++CREATE NEW POOL++++++++++++
------------------------------
pool begin at: 0x8f33020
->d         :0x8f33020
        last = 0x8f33048
        end  = 0x8f343a8
        next = 0x0
      failed = 0
->max       :4960
->current   :0x8f33020
->chain     :0x0
->large     :0x0
->cleanup   :0x0
->log       :0x0


+++++++++++ALLOC 2500+++++++++++++++++
------------------------------
pool begin at: 0x8f33020
->d         :0x8f33020
        last = 0x8f33a0c
        end  = 0x8f343a8
        next = 0x0
      failed = 0
->max       :4960
->current   :0x8f33020
->chain     :0x0
->large     :0x0
->cleanup   :0x0
->log       :0x0


+++++++++++ALLOC 2500+++++++++++++++++
------------------------------
pool begin at: 0x8f33020
->d         :0x8f33020
        last = 0x8f33a0c
        end  = 0x8f343a8
        next = 0x8f343c0
      failed = 0
->d         :0x8f343c0
        last = 0x8f34d94
        end  = 0x8f35748
        next = 0x0
      failed = 0
->max       :4960
->current   :0x8f33020
->chain     :0x0
->large     :0x0
->cleanup   :0x0
->log       :0x0


+++++++++++ALLOC LARGE 5000+++++++++++
------------------------------
pool begin at: 0x8f33020
->d         :0x8f33020
        last = 0x8f33a14
        end  = 0x8f343a8
        next = 0x8f343c0
      failed = 0
->d         :0x8f343c0
        last = 0x8f34d94
        end  = 0x8f35748
        next = 0x0
      failed = 0
->max       :4960
->current   :0x8f33020
->chain     :0x0
->large     :0x8f33a0c
        next = 0x0
       alloc = 0x8f35750
->cleanup   :0x0
->log       :0x0


+++++++++++ALLOC LARGE 5000+++++++++++
------------------------------
pool begin at: 0x8f33020
->d         :0x8f33020
        last = 0x8f33a1c
        end  = 0x8f343a8
        next = 0x8f343c0
      failed = 0
->d         :0x8f343c0
        last = 0x8f34d94
        end  = 0x8f35748
        next = 0x0
      failed = 0
->max       :4960
->current   :0x8f33020
->chain     :0x0
->large     :0x8f33a14
        next = 0x8f33a0c
       alloc = 0x8f36ae0
->large     :0x8f33a0c
        next = 0x0
       alloc = 0x8f35750
->cleanup   :0x0
->log       :0x0
```

总结：从上面的例子初步能看出一些nginx pool使用的轮廓，不过这儿没有涉及到failed的处理。

# 5.内存池的释放

这部分在nginx中主要是利用其自己web server的特性来完成的；web server总是不停的接受连接及
请求，nginx中有不同等级的内存池，有进程级的，连接级的及请求级的，这样内存总会在对应的进程，
连接，或者请求终止时进行内存池的销毁。

原文链接：https://zhuanlan.zhihu.com/p/481302777

原文作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.43】红黑树的原理以及实现

**红黑树**

**红黑树基于二叉查找树的附加特性**

1. 节点是红色或黑色。
2. 根节点是黑色。
3. 每个叶子节点都是黑色的空节点（叶子结点指为空的叶子结点）。
4. 每个红色节点的两个子节点都是黑色的（从每个叶子到根的所有路径上不能有两个连续的红色节点）。
5. 从任意节点到其每个叶子的所有路径都包含相同数目的黑色节点。

## **1. 数据结构**

```text
class TreeNode{
    private Boolean color;
    private int val;
    private TreeNode left;
    private TreeNode right;
    private TreeNode parent;
    get,set...
}
class RBTree{
    public boolean add(int val){...}
    public boolean delete(int val){...}
    public void display(){...}
}
```

## **2. 左旋以及右旋**

### **2.1 左旋**

![img](https://pic2.zhimg.com/80/v2-ee7b35428f94dbbb1a0b77be93b9116d_720w.webp)

### **2.2 右旋**

![img](https://pic2.zhimg.com/80/v2-4822d2fb2cf7e522a4d19c8a72799511_720w.webp)

## **3. 插入**

- 新插入的节点（newNode）为红色。
- 按照二分查找树插入规则插入。
- **分情况讨论**（**以下情况基本都是为了保持上文所讲的的红黑树特性4和特5**）
  1、若newNode为根节点，则变为黑色，插入完毕，返回 true。
  2、若newNode父节点为黑色，则插入完毕，返回 true。
  3、如下图所示，若newNode父节点为红色，且叔叔节点存在且为红色，则父节点与叔叔节点变为黑色，祖父节点变为红色，newNode = 祖父节点。

![img](https://pic2.zhimg.com/80/v2-dbcbfb84ec49083414f92625b1d90bbd_720w.webp)


4、如下图所示，若newNode父节点为红色，叔叔节点不存在或为黑色，且newNode为父节点右孩子，父节点为祖父节点左孩子，则以父节点为轴左旋，进入情况6.

![img](https://pic1.zhimg.com/80/v2-3cba7f4e8bd4ca3a568f5450c961be54_720w.webp)

5、如下图所示，若newNode父节点为红色，叔叔节点不存在或为黑色，且newNode为父节点左孩子，父节点为祖父节点右孩子，则以父节点为轴右旋，进入情况7

![img](https://pic1.zhimg.com/80/v2-c111482e5099c1568ecef70e3de34db8_720w.webp)

6、如下图，此时以祖父节点为轴进行右旋，将祖父节点变为红色，newNode变为黑色。

![img](https://pic3.zhimg.com/80/v2-59fe561a9ce96ce46e04d5894216dc76_720w.webp)

7、如下图，此时以祖父节点为轴进行左旋，将祖父节点变为红色，newNode变为黑色。

![img](https://pic1.zhimg.com/80/v2-1030a853056be151c322ad68b31bc0f0_720w.webp)

## **4. 删除**

**分情况讨论**（和插入一样，以下情况基本都是为了保持上文所讲的的红黑树特性4和特5）

1. 如下图，如果待删除节点**B**有两个非空的孩子节点，转化成待删除节点只有右孩子（或没有孩子）的情况，习惯性选取待删除节点右子树最小节点**E**替换待删除节点（只是值替换，颜色不变），并将待删除节点变为E。

![img](https://pic3.zhimg.com/80/v2-ec68d96519294a04a60240be1bd32546_720w.webp)

1. 根据待删除节点和唯一子节点颜色，**分情况处理：**

2. 1. 自身O是红色，子节点N是黑色，直接删除。
   2. 自身O是黑色，子节点N是红色，直接删除**并**将子节点N变为黑色。
   3. 自身O是黑色，子节点N不存在(不存在即子节点为空黑色节点，也可以用来判断)**或**也是黑色，较为复杂，**先删除，再分情况讨论：**
      1、N是根节点，则不需要调整。
      2、如下图，N的父亲、兄弟、侄子都是黑色，则将兄弟变为红色，父亲视作N，进行递归处理。

![img](https://pic1.zhimg.com/80/v2-5a159ab516c5f352cdc39bd9e04fa238_720w.webp)

3、（**存在镜像**）N的兄弟节点是红色，且N为父亲节点左儿子，则以父亲节点为轴左旋（否则右旋），并将旋转后N的祖父节点变为黑色，N的父节点变为红色，进入情况4,5或6.

![img](https://pic3.zhimg.com/80/v2-8b2cff6a44151e7a702935f3729daf56_720w.webp)

4、N的父亲节点是红色，兄弟和侄子节点是黑色，父亲节点变为黑色，兄弟节点变为红色。

![img](https://pic3.zhimg.com/80/v2-8180d41932ed875f0e4ce984d316e43a_720w.webp)

 5、（**存在镜像**）N的父节点颜色随意，兄弟节点为父节点黑色右孩子，左侄子节点为红色，右侄子节点为黑色，以兄弟节点为轴进行右旋，将旋转后N的兄弟节点变为黑色，N的右侄子节点变为红色，进入情况6

![img](https://pic3.zhimg.com/80/v2-5a30f3fd2aac1134fa2e50031c31223a_720w.webp)

 6、（**存在镜像**）N的父节点随意，兄弟节点为父节点的黑色右儿子，右侄子节点为红色，以N的父节点为轴进行左旋，左旋后的N的祖父节点变为父节点颜色，父节点变黑，叔叔节点变黑。

![img](https://pic4.zhimg.com/80/v2-1cc5c141b027e59b1cd78362bef97c23_720w.webp)

## 5.**测试**

**原树（上右下左）**

![img](https://pic3.zhimg.com/80/v2-77246a8a5f27c41930382fc8c29a64c2_720w.webp)

**删除53**

![img](https://pic2.zhimg.com/80/v2-1f4bcf75365b8df10a0b5893c047b85d_720w.webp)

**删除23**

![img](https://pic1.zhimg.com/80/v2-79ad4c9fc33586bfe468113ea11affa0_720w.webp)

**删除54**

![img](https://pic2.zhimg.com/80/v2-270cd680a5c46cde236714a42f71e085_720w.webp)

**添加67**

![img](https://pic3.zhimg.com/80/v2-e906fa056465876568af351ce3f55c72_720w.webp)



原文链接：https://zhuanlan.zhihu.com/p/588861677

原文作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.44】浅谈NIO和Epoll实现原理

## **1 前置知识**

### **1.1 socket是什么？**

就是传输控制层tcp连接通信。

### **1.2 fd是什么？**

fd既file descriptor-文件描述符。Java中用对象代表输入输出流等...在Linux系统中不是面向对象的，是一切皆文件的，拿文件来代表输入输出流。其实是一个数值，例如1,2,3,4...0是标准输入，1是标准输出，2是错误输出...

任何进程在操作系统中都有自己IO对应的文件描述符，参考如下：

```text
[root@iZj6c1dib207c054itjn30Z ~]# ps -ef | grep redis
root     20688     1  0 Nov23 ?        00:01:01 /opt/redis/bin/redis-server 127.0.0.1:6379
root     20786     1  0 Nov23 ?        00:01:00 /opt/redis/bin/redis-server 127.0.0.1:6380
root     30741 30719  0 12:26 pts/1    00:00:00 grep --color=auto redis
[root@iZj6c1dib207c054itjn30Z ~]# cd /proc/20688/fd
[root@iZj6c1dib207c054itjn30Z fd]# ll
total 0
lrwx------ 1 root root 64 Nov 23 21:50 0 -> /dev/null
lrwx------ 1 root root 64 Nov 23 21:50 1 -> /dev/null
lrwx------ 1 root root 64 Nov 23 21:50 2 -> /dev/null
lr-x------ 1 root root 64 Nov 23 21:50 3 -> pipe:[27847377]
l-wx------ 1 root root 64 Nov 23 21:50 4 -> pipe:[27847377]
lrwx------ 1 root root 64 Nov 23 21:50 5 -> anon_inode:[eventpoll]
lrwx------ 1 root root 64 Nov 23 21:50 6 -> socket:[27847384]
```

## **2 从BIO到epoll**

### **2.1 BIO**

计算机有内核，客户端和内核连接产生fd，计算机的进程或线程会读取相应的fd。
因为socket在这个时期是blocking的，线程读取socket产生的文件描述符时，如果数据包没到，读取命令不能返回，会被阻塞。导致会有更多的线程被抛出，单位CPU在某一时间片只能处理一个线程，出现数据包到了的线程等着数据包没到的线程的情况，造成CPU资源浪费，CPU切换线程成本巨大。
例如早期Tomcat7.0版默认就是BIO。

![img](https://pic1.zhimg.com/80/v2-dbb0aa89b63d7608ed3e6ce233b2dd60_720w.webp)

### **2.2 早期NIO**

socket对应的fd是非阻塞的
单位CPU只用一个线程，就一颗CPU只跑一个线程，没有CPU切换损耗，在用户空间轮询遍历所有的文件描述符。从遍历到取数据都是自己完成，所以是同步非阻塞IO。
问题是如果有1000个fd，代表用户进程轮询调用1000次kernel，
用户空间查询一次文件描述符就得调用一次系统调用，让后内核态用户态来回切换成本很大。

![img](https://pic1.zhimg.com/80/v2-71a08825a2f4b3b9056b8e77d0389efc_720w.webp)

### **2.3 多路复用NIO**

内核向前发展，轮询放到内核里，内核增加一个系统调用select函数，统一把一千个fd传给select函数，内核操作select函数，fd准备好后返回给线程，拿着返回的文件描述符找出ready的再去调read。如果1000个fd只有1个有数据，以前要调1000次，现在只调1次，相对前面在系统调用上更加精准。还是同步非阻塞的，只是减少了用户态和内核态的切换。


注意Linux只能实现NIO，不能实现AIO。
问题：用户态和内核态沟通的fd相关数据要来回拷贝。

![img](https://pic3.zhimg.com/80/v2-75a2e0d3d06804b35b3bc49b1cd73ea6_720w.webp)

### **2.4 epoll**

内核通过mmap实现共享空间，用户态和内核态有一个空间是共享的，文件描述符fd存在共享空间实现用户态和内核态共享。epoll里面有三个调用，用户空间先epoll_create准备一个共享空间mmap，里面维护一个红黑树，内核态将连接注册进红黑树，epoll_ctl写入。当有数据准备好了，调用epoll_wait中断阻塞，取链表fd，再单独调用read。
mmap应用：kafka实现数据通过socket存到服务器文件上的过程也是mmap。


epoll应用：redis和nginx的worker进程等等。

![img](https://pic1.zhimg.com/80/v2-4939e2fc0d43fbdf3a3df57e7b36f068_720w.webp)

原文链接：https://zhuanlan.zhihu.com/p/482549633

原文作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.45】Linux性能优化—内存实战篇

## **1.Linux内存工作原理**

### 1.1内存映射

　　Linux内核给每个进程都提供了一个独立的虚拟空间，并且这个地址空间是连续的。这样，进程就可以很方便地访问内存，更确切地说是访问虚拟内存。

　　虚拟地址空间的内部又被分为**内核空间**和**用户空间**两部分，不同字长（也就是单个CPU指令可以处理数据的最大长度）的处理器，地址空间的范围也不同。比如常见的32位和64位系统

![img](https://pic4.zhimg.com/80/v2-4f0f7b568e2aca27835dc71d4720ba23_720w.webp)

- 32位系统的内核空间占用1G，位于最高处，剩下的3G是用户空间
- 64位系统的内核空间和用户空间都是128T，分别占据整个内存空间的最高处和最低处，剩下的中间部分是未定义的。

　　进程在用户态时，只能访问用户空间内存；只有进入内核态后，才可以访问内核空间内存。虽然每个进程的地址空间都包含了内核空间，但这些内核空间其实关联的都是相同的物理内存。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。既然每个进程都有一个这么大的地址空间，那么**所有的虚拟内存加起来，自然要比实际的物理内存大很多**。所以，并不是所有的虚拟内存都会分配物理内存，只有那些实际使用的虚拟内存才分配物理内存，并且分配后的物理内存，是通过**内存映射**来管理的。

　　内存映射，其实就是将虚拟内存地址映射到物理内存地址。为了完成内存映射，内核为每个进程都维护了一张页表，记录虚拟地址与物理地址的映射关系：

![img](https://pic2.zhimg.com/80/v2-e15fddafc54630799a1aaff3e4449ae1_720w.webp)

　　页表实际上存储在CPU的内存管理单元MMU中，正常情况下，处理器就可以直接通过硬件找出访问的内存。而当进程访问的内存地址在页表中查不到时，系统会产生一个缺页异常，进入内核空间分配物理内存、更新进程页表，最后在返回用户空间，恢复进程的运行。

　　TLB（Translation Lookaside Buffer，后备缓冲器）会影响CPU的内存访问性能：TLB其实就是MMU中页表的高速缓存。由于进程的虚拟地址空间是独立的，而TLB的访问速度又比MMU快很多，所以，通过减少上下文切换，减少TLB的刷新次数，就可以提高TLB缓存的使用率，进而提高CPU的内存访问性能。

　　MMU规定了一个内存映射的最小单位，也就是页，通常是4KB。每次内存映射，都需要关联4KB或者4KB整数倍的内存空间。由于页的大小只有4KB，导致的整个页表会变的非常大。例如：32位系统就需要100多万个页表项（4GB/4KB），才可以实现整个地址空间的映射。为了解决页表项过多的问题，Linux提供了两种机制：多级页表 和 大页（HugePage）

**多级页表**：把整个内存分成区块来管理，将原来的映射关系改成区块索引和区块内的偏移。由于虚拟内存空间通常只用了很少一部分，那么多级页表就只保存这些使用中的区块，这样就可以大大减少页表的选项。Linux用的正是四级页表来管理内存页，如图，虚拟地址被分为5个部分，前4个表项用于选择页，最后一个索引表示页内偏移。

![img](https://pic1.zhimg.com/80/v2-97039bffe2c96a9e03d297f4c9014870_720w.webp)

**大页**：顾名思义就是比普通也更大的内存块，常见的大小有2MB和1GB。大页通常用在使用大量内存的进程上，比如Oracle、DPDK等。

### **1.2虚拟内存空间分布**

　　粗略绘制32位系统的虚拟内存空间分布图

![img](https://pic2.zhimg.com/80/v2-24b9746ef1b977d712ed47148510f6e9_720w.webp)

　　从此图可以看出，用户空间内存从低到高分别是五种不同的内存段。

- **只读段**，包括代码和常量等
- **数据段**，包括全局变量等
- **堆**，包括动态分配的内存，从低地址开始向上增长
- **文件映射段**，包括动态库、共享空间，从高地址开始向下增长
- **栈**，包括局部变量和函数调用的上下文等。栈的大小固定，一般为8MB

　　在这五个内存段中，**堆和文件映射段的内存是动态分配的**。比如说，使用C标准库的malloc()或mmap()，就可以分别在堆和文件映射段动态分配内存。

### **1.3内存分配与回收**

**a>内存分配**

　　malloc()是C标准库提供的内存分配函数，对应到系统调用上，有两种实现方式，即**brk**()和**mmap**()。

- **对小块内存（小于128K）**，C标准库使用**brk**()来分配，也就是通过移动堆顶的位置来分配内存。这些**内存释放后并不会立即归还系统，而是被缓存起来，这样就可以重复使用**。

- - brk()方式的缓存，可以减少缺页异常的发生，提高内存访问效率。不过由于这些内存没有归还系统，在内存工作繁忙是，频繁的内存分配和释放会造成内存碎片



- **对大块内存（大于128K）**，则直接使用内存映射mmap()来分配，也就是在文件映射段找一块空闲内存分配出去。

- - mmap()方式分配的内存**，会在释放时直接归还系统**，所以每次mmap都会发生缺页异常。在内存工作繁忙时，频繁的内存分配会造成大量的缺页异常，使内核的管理负担增大。这也是malloc只对大块内存使用mmap的原因



　　当这两种调用发生后，其实并没有真正分配内存。这些**内存都只在首次访问时才分配**，也就是**通过缺页异常进入内核**中，再**由内核来分配内存**。Linux使用伙伴系统来管理内存分配，与MMU的页管理一样，伙伴系统也是以页为单位来管理内存的，并通过相邻页的合并，较少内存碎片化（比如brk方式造成的内存碎片）。

**b>内存回收**

　　对于内存来说，如果只分配而不释放，就会造成内存泄露，甚至会耗尽系统内存。所以，在应用程序用完内存后，需要调用 free() 或 unmap() ，来释放不用的内存。在发现内存紧张时，系统就会通过一系列机制来回收内存：

- **回收缓存**，比如使用 **LRU**(Least Recently Used)算法，回收最近最少使用的内存页面
- **回收不常访问的内存**，把不常用的内存**通过交换分区直接写到磁盘**中
- **杀死进程**，内存紧张时系统会通过 OOM（Out Of Memory），直接杀掉占用大量内存的进程

　　其中，第二种方式回收不常访问的内存时，会用到**交换分区（Swap）**。Swap其实就是吧一块磁盘空间当成内存来用。它可以吧进程暂时不用的数据存储到磁盘中（这个过程称为换出），当进程访问这些内存时，在从磁盘读取这些数据到内存（这个过程称为换入）。所以Swap把系统的可用内存变大了。不过通常只在内存不足时，才会发生Swap交换，并且由于**磁盘读写的速度远比内存慢，Swap会导致严重的内存性能问题**。

　　第三种方式提到的OOM，其实时内核的一种保护机制。它监控进程的内存使用情况，并且使用 oom_score 为每个进程的内存使用情况进行评分：

- 一个进程消耗的内存越大，oom_score就越大
- 一个进程运行占用的CPU越多，oom_score就越小

　　进程的o**om_score越大，代表消耗的内存越多，也就越容易被OOM杀死**，从而可以更好保护系统。结合实际需求，可以通过 /proc 文件系统，手动设置进程的 oom_adj，从而调整oom_score。oom_adj的范围是[-17,15]，数值越大表示进程越容易被OOM杀死；数值越小表示进程越不容易被OOM杀死，其中 -17 表示禁止OOM。例如：手动调整sshd进程的oom_adj 为 -16，保障sshd进程不容易被OOM杀死

echo -16 > /proc/$(pidof sshd)/oom_adj

### **1.4查看内存使用情况**

**a>free**

![img](https://pic4.zhimg.com/80/v2-4fc4b42f081ecb9818626c30b13f15ab_720w.webp)

- **total**：总内存大小
- **used**：已使用内存大小，包含了共享内存
- **free**：未使用内存大小
- **shared**：共享内存大小
- **buff/cache**：缓存和缓冲区大小
- **available**：新进程可用内存大小。不仅包含未使用内存，还包括了可回收的内存，一般比未使用内存更大（但并不是所有缓存都可以回收，因为有的缓存可能正在使用中）

**b>top**

**![img](https://pic3.zhimg.com/80/v2-dcc74390f455c7a33064231fc1cd8c72_720w.webp)**

- **VIRT**：进程的虚拟内存大小，只要是进程申请过的内存，即便还没有真正分配物理内存，也会计算在内。
- **RES**：常驻内存的大小，也就是进**程实际使用的物理内存大小**，但不包括Swap和共享内存。
- **SHR**：共享内存的大小，比如与其他进程共同使用的共享内存、加载的动态链接库以及程序的代码段等。
- **%MEM**：进程使用物理内存占系统总内存的百分比

## **2.Buffer/Cache**

### **2.1定义**

　　使用man free查看

```text
buffers     Memory used by kernel buffers (Buffers in /proc/meminfo)

cache       Memory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo)

buff/cache  Sum of buffers and cache
```

- **Buffers** 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的Buffers值
- **Cache** 是内核页缓存和 Slab 用到的内存，对应的是 /proc/meminfo 中的Cached 与 SReclaimable之和

　　使用man proc 查看

```text
Buffers %lu
     Relatively temporary storage for raw disk blocks that shouldn't get tremendously large (20MB or so).

Cached %lu
     In-memory cache for files read from the disk (the page cache).  Doesn't include SwapCached.

SReclaimable %lu (since Linux 2.6.19)
     Part of Slab, that might be reclaimed, such as caches.

SUnreclaim %lu (since Linux 2.6.19)
     Part of Slab, that cannot be reclaimed on memory pressure.
```

- Buffers 是对原始磁盘块的临时存储，也就是用来**缓存磁盘的数据**，通常不会特别大（20MB左右）。这样，内核就可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写等。
- Cached 是从磁盘读取文件的页缓存，也就是用来**缓存从文件读取的数据**。这样，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。
- SReclaimable 是Slab 的一部分。Slab 包括两个部分，其中的可回收部分用 SReclaimable 记录；不可回收部分用 SUnreclaim 记录

### **2.2案例**

清理系统缓存

\#清理文件页、目录项、Inodes 等各种缓存 echo 3 > /proc/sys/vm/drop_caches

**a>磁盘和文件写案例**

终端1：首先输出vmstat

![img](https://pic3.zhimg.com/80/v2-2f7783eccefce31bc80e4f17a8a3a01e_720w.webp)

- buff 和 cache就是前面说的Buffers 和 Cache，单位是KB
- bi 和 bo 则分别表示块设备读取和写入的大小，单位为 块/秒。因为Linux中块的大小是1KB，所以等价于KB/s

终端2：执行dd命令，通过读取随机设备，生成一个500MB大小的文件

dd if=/dev/urandom of=/tmp/file bs=1M count=500

终端1：继续观察vmstat中的Buffer 和 Cache。

![img](https://pic3.zhimg.com/80/v2-383edcbcc44b067cfe198092288d3eae_720w.webp)

　　发现在dd命令运行时，Cache 在不停地增长，而Buffer 基本保持不变：

- 在cache刚开始增长时，块设备I/O很少，bi值 只出现了一次 488KB/s，bo 则只有一次4KB，过一段时间后，才会出现大量的块设备写，bo甚至高达12880 KB/s
- 当dd命令结束后，Cache不在增长，但是块设备写还会持续一段时间，并且多次I/O写的结果加起来，才是dd要写的500M数据

终端2：清理缓存后，向磁盘/dev/sdb1 写入2GB的随机数据

```text
#清理文件页、目录项、Inodes 等各种缓存
echo 3 > /proc/sys/vm/drop_caches
#运行dd 命令向磁盘分区 /dev/sdb1 写入2G数据
dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048
```

终端1：观察内存和I/O变化

![img](https://pic1.zhimg.com/80/v2-2fd316f62bde98d3530d809a87d85e60_720w.webp)

　　此时可以看出，虽然都是写数据，但是写磁盘和写文件的现象不太一样。写磁盘时（也就是bo大于0时），Buffer和Cache都在增长，但是显然Buffer增长的快很多。这说明，写磁盘用到了大量的Buffer。

　　关于Cache，在写**文件时会用到Cache缓存数据，而写磁盘则会用到Buffer缓存数据**。所以，**Cache是文件读的缓存，实际Cache也会缓存写文件时的数据**。

**b>磁盘和文件读案例**

终端2：从文件/tmp/file中读取数据写入空设备

```text
#清理文件页、目录项、Inodes 等各种缓存
echo 3 > /proc/sys/vm/drop_caches
#运行dd 命令读取文件数据
dd if=/tmp/file of=/dev/null
```

终端1：vmstat观察内存和I/O变化情况

![img](https://pic1.zhimg.com/80/v2-8223dac466338ed087d857c8c635d99c_720w.webp)

　　观察vmstat输出，发现**读取文件时**（bi大于）），Buffer保持不变，而**Cache在不停增长**。

终端2：清理缓存，从磁盘分区 /dev/sda1中读取数据，写入空设备

```text
#清理文件页、目录项、Inodes 等各种缓存
echo 3 > /proc/sys/vm/drop_caches
#运行dd 命令读取文件数据
dd if=/dev/sda1 of=/dev/null bs=1M count=1024
```

终端1：vmstat观察内存和I/O变化情况

![img](https://pic2.zhimg.com/80/v2-c077749e79ee6dc0869e0489fc47b17d_720w.webp)

　　发现在读磁盘时（bi大于0），Buffer和Cache都在增长，但显然Buffer增长的快很多**。说明读磁盘时，数据缓存到了Buffer中**。

c>总结

- Buffer 既可以用作写入磁盘数据的缓存，也可以用作从磁盘读取数据的缓存
- Cache 既可以用作从文件读取数据的也缓存，也可以用作写文件的页缓存

**Buffer是对磁盘数据的缓存，而Cache是文件数据的缓存，它们既会用在读请求中，也会用在写请求中**。



原文链接：https://zhuanlan.zhihu.com/p/483177281

原文作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.46】一文读懂网关中间件-Nginx

## **1.Nginx介绍**

1.nginx是一个高性能HTTP服务器，反向代理服务器，邮件代理服务器，TCP/UDP反向代理服务器.

2.nginx处理请求是异步非阻塞的，在高并发下nginx 能保持低资源低消耗高性能,主要用在集群系统中用于支持负载均衡.

3.nginx对静态文件的处理速度也相当快，也可以用于前端站点的服务器.

## **2.为什么要使用Nginx?**

单个系统主要用于处理客户端请求，一个系统处理客户端的请求量是有限的，当客户端的并发量超过了系统的处理能力的时候，就会导致服务器性能降低，速度变慢，直接影响用户体验，所以为了提升性能，我们会创建多个服务实例，形成`集群系统`用于保证`高可用`

那么什么样的系统业务适合使用集群系统呢？我觉得主要从2个方面来看，第一请求人数多,导致次数多，第二请求量密集，例如我们近两年常用的`防疫健康码`查询，我们排除与其他的业务系统接入的因素，可以说他的99%针对用户的业务其实就是查询，而且并发量和请求数也是非常庞大的，所以就很适合使用集群系统。

## **3.查询分流Nginx原理**

### **3.1模块化设计**

高度模块化的设计是 Nginx的架构基础。在Nginx中，除了少量的核心代码，其他一切皆为模块，所有模块间是分层次、分类别的，Nginx 官方共有五大类型的模块：`核心模块`、`配置模块`、`事件模块`、`HTTP模块`、`mail模块`，5种模块中，`配置模块`和`核心模块`是与 Nginx 框架密切相关的。而事件模块则是 HTTP 模块和 mail 模块的基础。`HTTP 模块`和 `mail 模块`的“地位”类似，它们都是更关注于`应用层面`并且引用基础核心模块。

### **3.2多进程模型**

与Memcached的经典多线程模型相比，Nginx是经典的多进程模型，Nginx启动后在后台运行，后台进程包含一个master进程和多个worker进程，可以在配置中设置工作进程数，一般根据服务器的Cpu核心数，来决定工作进程数是多少，例如我的电脑核心数是12，那可以在配置文件中设置worker_processes为12，那么在进程中可以看到 一个13个nginx运行实例。

![img](https://pic3.zhimg.com/80/v2-485acc009963ea368519f27f78069d26_720w.webp)

### **3.3事件驱动架构**

![img](https://pic2.zhimg.com/80/v2-8190d1063be66d6acf0e5d7e489215f5_720w.webp)

处理请求事件时，Nginx 的事件消费者只是被事件分发者进程短期调用而已，这种设计使得网络性能、用户感知的请求时延都得到了提升，每个用户的请求所产生的事件会及时响应，整个服务器的网络吞吐量都会由于事件的及时响应而增大。当然，这也带来一定的要求，即每个事件消费者都不能有阻塞行为，否则将会由于长时间占用事件分发者进程而导致其他事件得不到及时响应，Nginx 的非阻塞特性就是由于它的模块都是满足这个要求，其实Nginx最佳的部署应该在linux ,linux的io及事件驱动优于windows，我们可以通过配置文件中设置events的数量表示当前的nginx能处理多少个请求，这个没有一个绝对的标准，可以基于服务的性能和本身业务需求而定。

### **3.4虚拟主机、反向代理、负载均衡**

1.虚拟主机就是为了对所有应用系统进行反向代理。
2.反向代理是指代理后端服务器，正向代理代表代理客户端。
3.负载均衡将流量均分到指定后端实例。

## **4.落地Nginx**

我们首先结合实际业务场景分析，然后对不同的业务用例进行落地实践的方案选择。

### **4.1负载均衡业务实践**

1.首先我们应该准备一个业务系统，在这就用上面说的“健康码查询”业务，模拟一个查询的服务，**`注意在这仅仅只是引用场景示例，不代表健康码真实场景如此， 因为我没有参与真正的防疫健康码的开发和设计,也不了解它业务和技术架构上真正的复杂度，单纯只是由此引入业务场景而已，如果您在阅读时觉得这样不合适，您可以把他当做你想当做的任何系统，或者忘记这件事，都是可以的`**
接下来我们应该下载Nginx作为我们的服务器，在这里我使用的是在Windows环境下的演示，其实不管在Linux或者Docker中部署都可以，但是开发在windows，所以基于Windows比较方便。

- 1.创建健康码服务

![img](https://pic3.zhimg.com/80/v2-f27abe32b4666e744f3c6025b4be520a_720w.webp)

2.我们应该创建服务集群，在这为了演示创建2个服务实例，然后使用nginx来进行负载均衡查询分流

- 1.命令行启动2个服务实例模拟集群，分别绑定端口8081和8082，其实真实环境不会只有2个实例或者在同一台服务器上部署。
- 2.配置nginx的反向代理和负载均衡

```text
worker_processes  1;
   error_log  logs/error.log  info;
   events {
       worker_connections  1024;
   }
   http {
    include       mime.types;
    default_type  application/octet-stream;
    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
    access_log  logs/access.log  main;
    sendfile        on;
    keepalive_timeout  65;
    
    #虚拟主机
    server{
      listen       8080;
      server_name  localhost;
      #配置反向代理
      location / {
               proxy_pass  http://HealthCode;
           }
      error_page   500 502 503 504  /50x.html;
     }
   }
   #负载均衡配置
   upstream HealthCode{
           server localhost:8081;
           server localhost:8082;
   }
```

- 3.启动nginx,并访问虚拟服务器

![img](https://pic1.zhimg.com/80/v2-2d60590547845212ea5a3fba6c9db920_720w.webp)

### **4.2负载均衡业务场景分析**

针对上面的业务，我们使用到nginx实现负载均衡，那对于我们来说，应该知道Nginx虚拟主机，是如何将我们的请求转发到各个不同的服务实例的，下面结合业务用例来介绍Nginx中的各种`负载均衡算法`，并设置对应的配置文件节点。

**1.轮训算法**

根据字面理解就是轮训处理 1 2 3 4 周而复始,它是upstream模块默认的负载均衡默认策略，适合服务器处理能力相同的实例，在轮询中，如果某个服务实例宕机了，会自动剔除该服务器。，也可以加入权重，指定轮询几率，weight和访问比率成正比，用于服务器性能不均的情况下，权重越高，在被访问的概率越大，如下面分别是20%，80%。

```text
#负载均衡配置
upstream HealthCode{
    server localhost:8081;
    server localhost:8082;

    #server localhost:8081 weight=2;
    #server localhost:8082 weight=8;
}
```

**2.最小连接数算法 least_conn**

当客户端给Nginx发送查询健康码的请求时，Nginx把请求转发给8081和8082 ,如果8082 处理请求比较慢，会导致请求堆积在8082,那我们就需要解决请求堆积的问题，在这种场景下，我们可以把请求转发给连接数较少的服务器处理，能够达到更好的负载均衡效果，使用least_conn算法,在nginx配置文件中，负载均衡节点加入配置least_conn

```text
#负载均衡配置
upstream HealthCode{
    #配置最小连接数算法
    least_conn;
    server localhost:8081;
    server localhost:8082;
}
```

**3. hash一致性算法 ip_hash**

由于查询压力过大，为了提升可用性，我们在服务端加入缓存，例如3分钟之内请求，就直接将缓存的信息丢出去，客户端给Nginx发送查询健康码的请求时，Nginx把请求转发给8081和8082，甚至更多实例，使用轮训或者最小连接数时，会导致在缓存的情况下命中率下降，基于这种缓存`状态丢失`的情况，请求依然会给到没有缓存的服务实例，并去数据库中去查询数据，导致性能下降。

**（当第一次请求发送到8081去查询了数据库，但是在8082 或者其他的节点没有缓存，如果使用轮训算法及其他算法，会导致下次请求时，并不会访问缓存，所以叫`缓存命中率下降`）**
在这种场景下我们应该使用`Hash一致性算法`,将某一个请求客户端的ip地址与nginx的负载均衡中的某一个实例绑定。

```text
#负载均衡配置
upstream HealthCode{
  #配置iphash算法
    ip_hash;
    server localhost:8081;
    server localhost:8082;
}
```

**4.容灾策略**

```
重试机制
```

1.当客户端给Nginx发送查询请求时，Nginx把请求转发给8081和8082 ,如果转发到8081的时候，8081服务器被人拉闸，临时宕机了，会导致请求失败。如何保证请求成功？
在这种场景下我们应该使用nginx的`失败重试`机制,将某一个请求客户端的ip地址与nginx的负载均衡中的某一个实例绑定。

```text
#动态负载均衡配置
upstream HealthCode{
     ip_hash;
     #设置最大失败次数2次，超时时间10s钟
     server localhost:8081 max_fails=2 fail_timeout=10s;
     server localhost:8082 max_fails=2 fail_timeout=10s;
}
```

`主机备份 backup`
1.查询时请求转发给8081和8082 ，假设此时两个实例同时宕机了，会导致系统不可用，在这种异常业务情况下，我们可以使用`主机备份`来解决，注意在正常节点在运行时 ，备份节点是不工作的，如果使用`ip_hash`将不会生效，因为ip和主机已经绑定。

```text
#动态负载均衡配置
upstream HealthCode{
    ip_hash;
    server localhost:8081 max_fails=2 fail_timeout=10s;
    server localhost:8082 max_fails=2 fail_timeout=10s;
    #主机备份
    server localhost:8083 backup;
}
```

## **5.配置HTTPS**

我们要保证我们的请求的安全，所以需要使用Https通信,同样需要对我们的虚拟主机设置https，设置https的前提需要证书，一个是秘钥（server-key.pem），一个是证书（server-cert.pem）。

### **5.1基本设置**

1.下载openSSL,然后使用openSSL工具生成，教程连接
2.找到证书生成路径
3.然后在nginx配置文件添加对应的虚拟主机节点，然后配置Https

```text
# https 虚拟主机
    server {
        listen       4435 ssl;
        server_name  localhost;

        ssl_certificate      D:/cert/server-cert.pem;
        ssl_certificate_key  D:/cert/server-key.pem;

        ssl_session_cache    shared:SSL:1m;
        ssl_session_timeout  5m;

    #    ssl_ciphers  HIGH:!aNULL:!MD5;
    #    ssl_prefer_server_ciphers  on;

        location / {
            proxy_pass  http://HealthCode;
        }
    }
```

### **5.2http转https**

系统当中总是有很多默认的Http请求，我们需要使用nginx的ngx_http_rewrite_module模块来将http请求转换成https

```text
worker_processes  1;
    error_log  logs/error.log  info;
    events {
        worker_connections  1024;
    }
    http {
     include       mime.types;
     default_type  application/octet-stream;
     log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
     access_log  logs/access.log  main;
     sendfile        on;
     keepalive_timeout  65;
     
     #虚拟主机
     server{
       listen       8080;
       server_name  localhost;

        #默认重定向到https
        if($scheme = http)
        {
            return 301 https://$host:4435$request_url
        }

       #配置反向代理
       location / {
                proxy_pass  http://HealthCode;
            }
       error_page   500 502 503 504  /50x.html;
      }
    }
#负载均衡配置
upstream HealthCode{
            server localhost:8081;
            server localhost:8082;
    }
```



原文链接：https://zhuanlan.zhihu.com/p/483693498

原文作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.47】Redis面试题：基本数据类型与底层存储结构

最近在整理有关redis的相关知识，对于redis的基本数据类型以及其底层的存储结构简要的进行汇总和备注（**主要为面试用** ）

Redis对外提供的基本数据类型主要为五类，分别是

- STRING：可以存储字符串、数字
- LIST：列表，链表的每个节点存储一个字符串对象
- HASH：包含键值对的无需散列表
- SET：无序集合，集合中包含的是不重复的集合对象
- ZSET：有序集合，是有一对一对字符串成员-浮点数分值所构成的有序映射，排序规则由分值大小所决定

以上是我们在使用Redis的时候经常见到的五种数数据结构，这五种数据结构在底层存储上又有着千丝万缕的联系；例如字符串对象作为一个最基本的存储对象，其在上午五种数据结构中均有应用，那么具体在Redis底层数据结构中是如何构造这五种数据结构的，本文将对底层存储做深入解析。

文章中所描述的数据结构大都基于2.9版本，如有描述不对还请留言指正，万分感谢

## **1.STRING**

字符串对象根据保存值的类型、长度不同，可以分为三种存储结构

- 如果存储的是整数值（**可以用long表示**），则底层通过如下结构进行存储，其中type代表当前对象为STRING对象，encoding表示当前对象的编码格式，ptr的属性保存是真实的值；

![img](https://pic3.zhimg.com/80/v2-2d6c6901f93cc1f474835b298bd65ba6_720w.webp)

举例：

![img](https://pic3.zhimg.com/80/v2-5f6172bf811d65520d7a0d175d858682_720w.webp)

- 如果存储的是字符串且字符串长度超过39字节，则底层通过如下结构进行存储，其中type代表当前对象为STRING对象，encoding表示当前对象的编码格式，ptr为指针指向一个SDS（shshdr：简单动态字符串对象）来保存具体的值；

![img](https://pic2.zhimg.com/80/v2-34ca721fe96e2db43b5baf098a104bd5_720w.webp)

举例：

![img](https://pic1.zhimg.com/80/v2-76d4408155ac7229b796481ce060afb0_720w.webp)

- 如果存储的是字符串且字符串长度未超过39字节，则底层通过如下结构进行存储（需要一块连续的内存空间），其中type代表当前对象为STRING对象，encoding表示当前对象的编码格式，ptr为指针指向一个SDS（shshdr：简单动态字符串对象）来保存具体的值；

![img](https://pic2.zhimg.com/80/v2-fbd9c7732d196b6f3788efe2d9c9351d_720w.webp)

举例：

![img](https://pic4.zhimg.com/80/v2-bd70743a0677db4173c852a736252283_720w.webp)

- 存储结构差异

- - embstr需要一块连续的内存空间，因此其效率上比raw方式要高
  - emstr在内存分配以及内存释放时只需要一次接口，而raw方式需要两次（因为存在redisObject和shshdr两个对象）
  - embstr为只读对象，任何对embstr编码对象的修改都会导致对象的编码格式变为raw
  - int/embstr编码格式的字符串对象在满足一定条件后会自动转为raw编码格式

- 字符串对象常用的命令

| 命令        | 作用                               | 备注                                                |
| ----------- | ---------------------------------- | --------------------------------------------------- |
| set         | 设置key的值                        | 根据值不同底层会采用三种不同的编码进行存储          |
| get         | 获取字符串对象值                   | 对于int编码格式有个值拷贝-转换的过程                |
| append      | 在现有字符串值后面追加新的值       | int/embstr编码格式对象会先转换为raw后在执行追加操作 |
| incrbyfloat | 对浮点型数值进行加法操作           |                                                     |
| incrby      | 对整数型数值进行加法操作           | embstr/raw不能执行此命令                            |
| decrby      | 对整数型数值进行减法操作           | embstr/raw不能执行此命令                            |
| strlen      | 返回字符串长度                     | int编码格式需要拷贝对象并转换为raw格式后在执行操作  |
| setrange    | 在字符串指定索引上的值设置为给定值 | int/embstr需要转换为raw后在执行操作                 |
| getrange    | 返回字符串指定索引的值             | int编码格式需要拷贝对象并转换为raw格式后在执行操作  |

## **2.LIST**

列表对象根据存储数据的长度以及存储数据元素个数的不同，可以分为两种存储结构：

- 如果列表对象保存的所有字符串对象值的长度均未超过64字节且列表对象保存元素数量小于512个的时候，就采用ziplist（压缩列表）格式存储

![img](https://pic1.zhimg.com/80/v2-670d051cf9d97d96f224c6d5663bf9cc_720w.webp)

- 如果列表对象保存的所有字符串对象值的长度有超过64字节或者列表对象保存元素数量大于等于512个的时候，就采用linkedlist（双端链表）格式存储

![img](https://pic4.zhimg.com/80/v2-ea60b02968c5d8e2661a7556ecc5b86b_720w.webp)

- 存储结构差异

- - 压缩列表是有一系列特殊编码的连续内存块组成的顺序型数据结构，而双端链表则不需要连续的内存块
  - 压缩列表的每个节点是由三部分组成（previous_entry_length/encoding/content）,其中previous_entry_length是记录前一个节点的长度，以便程序可以通过任意节点的指针计算出前一个节点的起始位置；而双端链表则必须通过头尾节点进行遍历获取
  - 由于previous_entry_length属性会随着前一个节点的字节长度不同而存储1或者5字节，如果新增的头结点长度大于254字节，会导致当前头结点的previous_entry_length（假设当前节点的previous_entry_length为1）的长度无法保存新节点的长度，此时程序会对原头节点进行内存空间重新分配，最坏的情况是新增的头结点导致原列表中的所有元素全部重新分配；而双端链表则不会存在该问题；因此在使用列表对象时要考虑连锁更新的问题；

## **3.HASH**

哈希对象根据存储键值对数据长度以及键值对数量，可以分为两种存储结构：

- 如果hash对象保存的所有键值对的字符串长度小于64直接且键值对数量小于512个，采用ziplist（压缩列表）编码存储

![img](https://pic1.zhimg.com/80/v2-313d8222ac32d6b4361c2f28307cd548_720w.webp)

key-value总是以成对的方式存在，存储顺序类似于栈，先添加的键值对会存在列表的前面，后添加的会在列表的尾部；

- 如果hash对象保存的键值对的字符串长度有超过64字节或者键值对数量大于等于512个，将采用hashtable编码存储

![img](https://pic4.zhimg.com/80/v2-9392836324d5cfcf7fb9b21f17e668cb_720w.webp)

- 上述存储格式会随着存储的内容变化进行编码格式转变，转变只能从ziplist转换为hashtable

## **4.SET**

集合对象根据存储数据的长度以及存储数据元素类型的不同，可以分为两种存储结构：

- 如果列表对象保存的所有元素均是整数型且保存元素数量不超过512个的时候，就采用intset编码存储

![img](https://pic4.zhimg.com/80/v2-bce742d880e177b35d6f5b85ce6769b7_720w.webp)

- 如果列表对象保存的元素有不是整数型或者保存元素数量超过512个的时候，就采用hashtable编码存储

![img](https://pic4.zhimg.com/80/v2-b9d9158884c502a37da0baee758b8177_720w.webp)

- 上述存储格式会随着存储的内容变化进行编码格式转变，转变只能从intset转换为hashtable
- 在实际使用过程中，需要提前规划好存储数据内容，尽量不要出现编码格式转换

## **5.ZSET**

有序集合对象根据数据元素数量以元素成员长度，可以分为两种存储结构：

- 如果有序集合对象保存的元素数量小于128个且有序结合对象保存的元素成员的长度都小于64字节，就采用ziplist（压缩列表）编码存储

![img](https://pic3.zhimg.com/80/v2-f53f28e3dbf682952690005166bd144a_720w.webp)

使用压缩列表的有序结合中每个对象由两个节点构成，第一个节点保存元素的具体值，第二个节点保存元素的分值；默认情况元素是按照分值从小到大排序；

- 如果有序集合对象保存的元素数量>=128个或有序结合对象保存的元素成员的长度存在>=64字节，就采用skplist（跳跃表【详细请参见：算法-跳跃表原理与实现】）和dict编码存储

![img](https://pic3.zhimg.com/80/v2-42593600cfca50c1466af1f233985e56_720w.webp)

为了兼顾查找与范围查找，有序集合需要同时使用跳跃表与字典

1. 字典可以保证O(1)复杂度直接根据指定key查找成员值
2. 跳跃表可以在O(logN)的复杂度下完成范围查找，远远优于字典的O(NlogN)

在底层存储上，针对相同的数据对象以及分值，跳跃表与字典会通过指针进行共享，因此不会产生较多的内存浪费

## **6.总结**

Redis对外提供的是上述五种数据类型，但是在底层构造这五种数据类型时，底层实际上使用了包括“简单动态字符串”、“链表”、“字典”、“跳跃表”、“整数集合”、“压缩列表”来构造

根据存储数据的类型、长度以及数量，不同存储格式之间可以进行动态转换，有的存储结构体现是查询速度，有的存储接口体现的空间占用

![img](https://pic2.zhimg.com/80/v2-420b42aad7501a40e723d14a610c07e1_720w.webp)

原文链接：https://zhuanlan.zhihu.com/p/484776036

原文作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.48】一文来了解关于分布式锁的那些事儿

### 1.**什么是分布式锁**

> 通过互斥性质，来保证线程对分布式系统中共享资源的有序访问
> 说人话：**一把锁，挨个进**

### 2.**分布式锁的特性**

- **互斥**（线程独享）：即同一时刻只有一个线程能够获取锁
- **避免死锁**：获得锁的线程崩溃后，不会影响后续线程获取锁，操作共享资源
- **隔离性**：A获取的锁，不能让B去解锁（解铃还须系铃人）
- **原子性**：加锁和解锁必须保证为原子操作

### **3.分布式锁的实现方式**

1. **基于Redis**
   演变过程：

> V-1.0:

- SETNX：Redis提供了SETNE（SET if Not eXists）命令，表示当Key不存在时，才能设置Value,否则设置失败（获取锁失败）
- DEL KEY:第一步获取锁成功，对共享资源操作完后，释放锁

![img](https://pic2.zhimg.com/80/v2-0d2d2136365dd306ee66e193927465c9_720w.webp)

> 问题：**`如果业务代码出现异常，阻塞或者报错了，那么该线程就一直持有锁，不释放，其他线程也永远获取不到`**————我王霸天得不到的谁也别想得到！
> V-2.0:

- SETNX+EXPIRE：给锁上过期时间，假如持有锁线程崩溃了，达到设置的过期时间后，会自动释放锁，避免后续线程获取不到锁！

问题：仍旧会死锁！SETNX和EXPIRE是两条命令，Redis单命令是原子操作，但多条命令为非原子操作！**`SETNX执行成功，EXPIRE失败时就会发生死锁`**
v-3.0:

- SET(NX+EX)（2.6.12版本之后）：获取锁，并设置锁过期时间（**`原子操作`**）

如此，可以说是**`彻底解决了死锁问题`**！

那么还问存在其他问题吗？
分析分布式锁的特征：互斥、死锁、原子等特性，我们都算是解决了！
但还未考虑隔离性的问题！

### 4.**场景**

1. 线程A加锁成功后，去操作共享资源
2. 但是因为发生了意外，线程A操作的时间超过了锁过期时间，锁被释放了
3. 线程B进来了，枷锁成功，去操作共享资源了
4. 此时，线程A操作完成了，回来释放锁，线程B的锁被A释放（**`动了别人的老婆！`**）

隔离性带来的问题：

1. 锁的过期时间设置不合理，导致线程A锁过期，被释放
2. 线程A释放了线程B的锁

### 5.**分析：**

- 线程A的过期时间设置不合理，那就换一个合理的时间————对应到现实工作中，就是根据程序员的工作经验，对改值进行较为合理的设置，实在不行，杀了祭天！（**不是很可靠**）
- 其实很简单，锁过期就像去麦当劳喝咖啡喝完了呗，还想喝怎么办？续杯！————获取锁时，先设置一个过期时间，同时，开启一个**守护线程**，**定时**去查看锁的剩余存活时间，假如锁的存活时间快过期了，但业务代码还没执行完，赶紧去给大爷**续杯**,即重新设置过期时间（看门狗）
- 至于第二个问题，还是那句老话————解铃还须系铃人，加一个业务唯一标识，每个线程只能根据业务唯一去释放自己的锁，同时，需要注意：**判断是否为自己的锁和删除锁`应为原子操作`**！不然仍旧会删错锁！

![img](https://pic2.zhimg.com/80/v2-b4c8dd579d62d83ff40d7c9c08b99755_720w.webp)

### 6.**实现**

- Redission的看门狗（基于Netty时间轮算法实现）：

```text
private long lockWatchdogTimeout = 30 * 1000;


public RedissonLock(CommandAsyncExecutor commandExecutor, String name) {
    super(commandExecutor, name);
    this.commandExecutor = commandExecutor;
    //会获取看门狗设置的时间，默认为10s检查一次，锁过快过期，且业务代码还没执行完，就会给锁续上这个时间，默认30s
    this.internalLockLeaseTime = commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout();
    this.pubSub = commandExecutor.getConnectionManager().getSubscribeService().getLockPubSub();
}


private RFuture<Boolean> tryAcquireOnceAsync(long waitTime, long leaseTime, TimeUnit unit, long threadId) {

    //如果锁是永不过期，那么就按常规方式索取锁
    if (leaseTime != -1) {
        return tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_NULL_BOOLEAN);
    }

    //否则，会在获取锁之后，加一个定时任务，在锁执行完业务代码自行释放之前，不断的给所续上过期时间（默认10s检查一次，每次给锁续期30s）
    RFuture<Boolean> ttlRemainingFuture = tryLockInnerAsync(waitTime, internalLockLeaseTime,TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_NULL_BOOLEAN);

    ttlRemainingFuture.onComplete((ttlRemaining, e) -> {
        if (e != null) {
            return;
        }

        // lock acquired
        if (ttlRemaining) {
            scheduleExpirationRenewal(threadId);
        }
    });
    return ttlRemainingFuture;
}
```

> 实现具体细节，参见Redission源码

- 线程隔离的问题：

> 考虑到获取锁判断后，再删除锁，这两个操作必须是原子性的，那么就需要查看一下Redis的API有没有提供这两个操作的原子性操作了
> 结果发现，没有！那么叫考虑第二种方案，在Redis中除了单条命令是原子性的，还有执行**`Lua脚本`**也是原子性操作！

```text
//如果是自己的锁，则进行删除，否则返回
if redis.call("GET",KEY[1]) == ARGV[1] then           
  return redis.call("DEL",KEY[1])
else
  return 0
end
```

### 7.**总览**

![img](https://pic3.zhimg.com/80/v2-9797d65406443ff0cd06600b743abfbe_720w.webp)

### **8.小总结**

> 经过了这几波优化之后，基于Redis的分布式锁（**`Redis单实例`**），可算是安全放心大胆的使用了！悄悄的告诉你，其实我们这些优化过程Redis作者早就想到了，同时，他也提供了较为完善的解决方案，在工作中**`Redission`**可以实现以上所有！

作为技术宅男，要有极客精神（其实就是闲了无聊），有心的人，可能会发现，以上粉色标粗的**`Redis单实例`**字样,确实！以上分析的分布式锁适合单节点的Redis实例，如果遇到**主从+哨兵**的模式基本**`凉凉`**！

### 9.**凉凉场景：**

1. 线程A在遇到主从架构时，先在Master上加锁成功
2. 此时，还未等加锁命令SET同步到Slave上，Master就出现问题，宕机了！
3. 通过哨兵过半原则，重新选出新的主节点，那么此时这把锁在新的主库上是找不到的！出现新问题了！

### 10.**为之奈何？**

> 遇到这种情况是不是就完了！芭比Q了！准备提桶跑路了！

![img](https://pic2.zhimg.com/80/v2-3908ccbcc248743da69320e9e982b215_720w.webp)

### 11.亲妈解法！

![img](https://pic3.zhimg.com/80/v2-8af5b4844cb5734efb88d8b345781862_720w.webp)

如果一遇到这种问题，就要程序员提桶跑路，那么Redis的作者恐怕在大佬圈是混不下去了！于是，他苦心钻研，誓死捍卫Redis尊严！于是乎它就出世了！————***`RedLock`***

### 12.**要求：**

> **1. 主节点至少5个实例多主部署**
> **2. 不再需要从节点和哨兵**

### 13.**原理：**

> **1. 加锁线程带着Expire时间进入，在加锁前记录一个开始加锁时间T1**
> **2. 轮流用相同的key和value在不同的节点上进行加锁操作，并且必须保证大多数（N/2+1）节点加锁成功，才算成功**
> **3. 最少(N/2+1)个节点加锁成功后，记录当前时间T2**
> **4. 如果T2-T1 < Expire，则加锁成功，反之失败**
> **5. 释放锁时，要向`所有节点`（不管是否在该节点加锁成功）发送解锁请求！**
> **6. 此时，锁的Key真正有效时间为：Expire - (T2-T1)**
> **7. 部署的节点数最好是奇数，以更好的满足过半原则**

### 14.**疑问：**

1. **为什么是N/2+1个节点加锁？**
2. **加锁成功后，计算加锁耗时的意义？**
3. **为什么释放锁时，要给所有节点（包括没有加锁成功的节点）发送解锁请求?**

### **15.分析：**

1. **N/2+1公式为过半原则，这里的本质时为了容错，CAP中的P说到，当分布式系统中，如果存在部分故障节点，但大多数节点仍旧正常时，可以认为整个系统仍旧可用**
2. **假如T2-T1 > Expire 就意味着一定会存在，最早加锁的节点过期自动解锁的情况，那么此时的加锁节点计数就不再正确！那么此次加锁就毫无意义了！（T2-T1为加锁时间，Expire为过期时间）**
3. **假设某节点加锁成功了，但是后续因为其他原因（网络）导致无法从该节点上获取响应结果，而被判断为未成功加锁，如果只给加锁成功的节点发起解锁请求，那么此时该节点是收不到解锁请求的，就会一直持有，影响后续无法使用**

### **16.理性看待**

其实，Redis作者研究出来的RedLock，在一些极端的情况下是存在风险的，比如：

- N节点的时钟存在较大偏差时，T2-T1 < Expire的讨论就是毫无意义的，依然存在琐失效的问题，想要解决这个问题，就得需要人工的去维护N节点之间的时钟趋于一致
- RedLock仍旧解决不了获得锁的线程客户端发生长时间GC，导致锁过期，如果再出现第二个线程仍旧可以获取锁，此时，就会出现同一时刻两个线程对共享资源同时获得锁的矛盾情况，严重违反分布式锁特性中的互斥性
- 因为RedLock无法提供类似fencing token的设计方案，从而推导出RedLock无法保证分布式的正确性



1. **基于Zookeeper**

- 利用节点名称唯一性
  **原理**

1. **加锁时,所有线程均在相同的目录下创建一个文件，谁先创建成功，就代表获得锁，否则就代表失败,只能等待下次**
2. **当获取得锁的线程操作完业务代码后，会将该文件删除，同时通知其余客户端再次进入竞争**
3. **在一个路径下只能创建一个唯一的文件（文件名唯一），但容易引起“`惊群`”效应**

- 利用临时顺序节点
  **原理**

1. **所有线程刚开始都会在ZK中创建自己的临时节点，由ZK去保证这些节点的顺序**
2. **加锁时，线程会`判断ZK下的第一个节点是不是自己创建`的，如果是，则加锁成功，如果不是，加锁失败，同时，给自己的上一个节点加一个\**\**`节点监听器`**
3. **当节点监听器被通知上一个节点被删除时，当前节点会重新判断ZK下第一个节点是否是自己创建的，循环2的判断操作**
4. **用完锁后，每个线程只能删除自己创建的临时节点**

![img](https://pic4.zhimg.com/80/v2-802a1b8a88a3b771d607dc884fbfbe73_720w.webp)

1. **二者对比**

- **效率：ZK锁远不如Redis锁**
- **失败处理：**
  **ZK锁只需要维护`Watch监听器`，等待锁被释放**
  **Redis锁则是自旋重试，高并发时耗性能**
- **宕机处理：**
  **ZK是根据客户端上报心跳（长连接），判断客户端是否存在（持有锁），无心跳上报时，会删除节点（释放锁）**————**`（客户端长GC时，锁会被ZK释放）`**
  **Redis则是需要等到过期时间，才会释放锁**

原文链接：https://zhuanlan.zhihu.com/p/485419860

原文作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.49】2022年作为一个中年程序员写给35岁的自己

### 1.问时间都去哪了？


一晃神自己都已经35了，虽然不愿意承认，但是时间就这么一点一滴地过去了。


记得09年毕业刚工作的时候，听着同事们谈论买房、炒股、养娃的事情，当时感觉这个事情离自己是多么的遥远，但现在看来真的就好像是昨天发生的事情，当时的情景还很清晰的印在脑海里，甚至他们谈论时候的表情、语气、肢体语言都还清晰的记得。


自从过了30岁以后，每一年过生日都会比较感慨，总是会问一下自己，这一年的时间都去哪了呢，如果这一年可以重新循环一次，自己又会怎样去选择。

###  2.回想


2022年，一个充满2的一年，自己的第14个工作年，回想我这14年的工作经历，有得有失，有太多的心酸。
第一个5年可以说是比较顺利的，从一个小兵，通过自己的**全身心地投入**，逐渐成长为一个小组的负责人。
前面为第二个5年奠定了一个新的起点，并在新的角色上继续沉淀自己的技术和团队管理。用最近的话说，也是一路“卷”过来的。


第3个5年比较坎坷，自己并不满意，未能达成自己期望的目标，时间也耗费过去了，**有遗憾、有决策失误、有不甘心**，但不能就此认命，不能后悔，因为未来的路上仍需要全力以赴。

###  3.地下室


09年7月，一毕业就自己拖着一个行李箱，坐上了开往北京的火车，开启了北漂之旅。


大学期间就听说北京的程序员很好找工作，一个月能够拿到1万5的高薪，当时北京的房价估计也就1万不到，当时就想自己抓紧学习，也能早点拿到1万5的薪水，所以坚定地选择了北漂。


初生牛犊不怕虎，拿着家里最后一次给的3千块钱，工作和住宿都还没有着落，还好当时的北京地下室很流行，都是给很多想要来北京打拼，但是又支付不起高昂的房租的人准备的，里面很多像我这样的年轻人，还有起早贪黑做路边早餐生意的摊贩，所以我很有幸成为了其中的一员。


之后就踏实的开启了一年的地下室-公司两点一线的生活，早晨从负三层走上地面，每天从黑暗走向光明，出门挤公交到公司，晚上接着回去地下三层，只有2-4平米的地下室，没有网络，没有娱乐，偶尔去网吧和当时的女朋友（现在的老婆）视频，这一年非常充实，2000块钱工资仅够生活，周末加一天班还能多拿几百，但是精力充沛，学习为主，非常充实。

###   4.养成强大的内心


为什么要拥有一颗强大的内心？因为无论是工作中，还是生活中，肯定会遇到各式各样的困难与挫折，如果轻而易举的就被压垮，轻而易举的说放弃，那如何能够在职业道路上走的更远，又怎样担负起自己的家庭的责任。
头一两年工作的时候，其实很多不懂的东西，只能靠自己不断的百度和Google，所以经常会晚上经常梦到自己程序写不下去的情况，压力很大，特别是接到一个比较大的项目的时候，晚上还会睡不着。


但这些年通过项目的历练，和与更优秀的人共事，逐渐让自己养成了强大的内心。当你需要同时肩负家庭与工作的双重压力时，如果内心不够强大的话，崩溃或者抑郁就会找上来，每个人都是一步步走过来的，放眼我们这代人，我接触的人当中都是非常皮实，都是在一次次被压垮崩溃之后，自我调节又振作起来，让自己的内心逐渐的强大起来。


我在给团队招人的时候，优先会考察候选人皮实、乐观这两个方面。


“皮实”的反义词是“玻璃心”，是人都会犯错，都会有迷茫的时候，皮实的人能够听得进去别人的建议，并且把压力化解为动力，从而逐渐承担更多的职责。


乐观的人不仅能够带动组内的气氛，还是个积极向上的人，不会轻易否定自己，经过适当的培养和引导，很快就能成为一个独当一面的角色。

###  5.个人成长


我最精力旺盛最能拼搏的阶段是在第二家公司，快7年的时间。在这段时间，买房、结婚、生娃，可以说是把最重要的事情都做了，这段时间里从一个小兵，踏实的完成好领导安排的每一项任务，把自己当成owner的角色要求自己，到后来晋升为技术leader，也正好经历了互联网最辉煌的那几年。
非常喜欢公司的文化氛围，很怀念和当时的同事们一起共事的经历。
这7年里收获了许多做事情的方式和方法，跟对了靠谱的领导，也结识了很多优秀的同事。总结下来就是，**强大的内心，真诚待人，踏实稳重，厚积薄发**。

###  6.创业


有创业的念头是在第二个5年的后半段，当时事业上进入了舒适区，也可以说是瓶颈期，感觉安逸下来就会落后。所以在挣扎了比较长的一段时间后，放弃了留在阿里拿着股票修福报的机会，裸辞寻找创业机会（这个决定其实是有点头脑发热的，在选择的话，估计会骑驴找马），同时照顾也想参与一下娃的成长阶段。


我所说的创业，并不是大家想象中的，自己当老板那种创业，我期望的是能跟着一个早期的业务，经历一步步的做大做强的过程，也就是0-1的过程，从而能够更加的皮实自己。


所以后来创业经历过短视频、区块链、民宿这些业务的，项目从0-1的过程是我最大的收获，投入度和抗压能力，这个是创业必备的，只要能看着数据库的用户数据一天比一天成倍的增长，能验证项目的价值，这就是创业能坚持下去的动力，虽然最想要的“钱”没到位。


创业都会有风险，周期长，充满不确定性，会受政策和资金的影响，有成功有失败，这一点在从阿里出来的时候，就已经做好了心理准备，这也是经历了多个项目的原因，1-10 这个阶段不是努力就能达到的，天时地利人和一个都少不了。


未来的世界总是在不断的变化，谁也没能想到2019年“新冠”这个病毒就这样默默的流行起来了，这是所有人人生计划当中的一个很大的变数，疫情下面一切都变得不再那么重要，活着最重要，家人健康最重要。所以创业的计划也就没法继续下去了，还是选择先回归到了大厂继续“卷”吧，毕竟还有家庭的责任需要担起来，希望人类战胜疫情的那一天早日到来。

###  7.技术


技术是程序员吃饭的工具，也可以说是本能，任何时候都应该保持这种本能，所以不断更新自己的技术储备，是作为一个程序员的必修课。

### 8.危机感


活到老学到老，这个应该不是只有程序员这个工种需要做的事情，任何岗位都应该如此（机关单位除外），在这个竞争如此激烈的环境里，不进则退，危机感是每个人都应该时刻保持的状态，除非你真的可以做到躺平。
一线编程是时刻都要保持的能力，再忙也还是需要抽出时间去练习巩固，架构能力固然重要，落地能力也很重要。


希望等我50、60岁的时候，也还能够保持技术的能力，虽然不知道未来的世界会是怎样，但秉承内心即可。
学会正确的做事


工作到了一定阶段，只要是技术能够搞定的事情，那都是最简单的工作。想要把技术做好，真正实施之前，先要透过现象看清事物的本质，**谋定而后动**，**厚积而薄发**。这个也是和之前的领导那里学到的，所以**“和优秀的人共事，你也会变得优秀”**。


培养做事的方式方法，列提纲，想清楚，写明白，凡事都要写下来，然后印在脑海里。
以下是我习惯的流程提纲，能完善好其中的每个点，我认为是能够在工作中做到应对自如的：
事件分析 背景调查 人物关系 痛点梳理 方向确立 制定短期、中期、长期目标 详细方案 时间线 过程跟踪 阶段复盘，调整方向 复制代码

###  9.正确看待技术的更新迭代


我现在虽然是做着最前端的工作，但是我并不仅仅把自己定义为前端工程师这个角色，我不想把自己局限在某个设定好的框架里。同样头几年做后端的时候，到后来让我带领H5团队的时候，我并没有觉得这个是一个不好的选择，相反正因为有了后端的经历，才让我在前端这个领域上持续深耕，更加的从容和淡定。
时代在变，人也在变，技术也在变，技术总是不断地更新迭代，作为研发，需要有一颗进取的心，努力跟上时代的潮流，而不是被后浪拍死在沙滩上。


就拿前端来说，我最早接触的框架是雅虎的Yui，后来到jQuery，再到后来nodejs的出现，使得前端技术有了爆炸式的增长和变化，所以才有了Angular、React、Vue等这些框架的出现。
这几年前端领域就出现了很多新名词新框架，比如跨端、severless、lowcode、D2C、P2C、esbuild、Flutter等等，这些名词背后的核心，其实还是围绕着如何提升研发效能，如何最大化研发效率，如何优化运行性能展开的。所以无论技术如何的变化，事物的本质并没有变，而是我们通过不断的研究和探索，不断的找到了更优的解决办法，从而替代之前的架构更好更快的达成目标。
摆正姿态，正确对待技术的更新迭代，同时也要不断的把自身的基建打牢，打通任督二脉，这样无论来什么武功都能融会贯通。

###  10.某一方面的技术专家


“技术专家”这个词，我个人理解的是：在某个技术领域里，不仅有业界影响力，而且技术的深度也是非常厉害，能够利用所掌握的技能，完美解决工作中的各种疑难杂症，带着团队其他人一起进步。
不得不承认，我自己并没有达到自己所想的预期，虽然顶着技术专家的头衔，但技术深度远远不够的，而且业界也确实没啥影响力啊，有点惭愧，未来任重道远啊。


这么多年技术语言接触了不少，比如C++、PHP、Python、Java、Javascript、go，但是似乎每一项都是点到为止，都是为了解决当下业务痛点，临时上手学习，并且运用到实际的业务中，之后再没有更深入的去研究了。除了前端，这个是因为真的感兴趣，所以一直也在持续的关注和投入，所以未来应该还是会在这个方向上一直走下去，结合实际的业务更加的深入，争取达到自己内心的期望。

###  11.焦虑


说一下“焦虑”，也可以说是“中年危机”，带来的焦虑，最近正好在看一本书《认知觉醒：开启自我改变的原动力》，作者写下这段话的时候，正好36岁：
在很长一段时间内，我就像一个没有睡醒的人，对自己不了解，对生活没主张，对命运无选择。那时的我，虽然对本职工作非常投入，但业余时间几乎被不需要动脑筋的事情占据：有空就找朋友们聚会，时常喝到烂醉；经常熬夜，从不主动看书、运动；打发时间的方式就是看搞笑视频、读八卦新闻、玩手机游戏；实在没事可做，就裹起被子睡大觉……下意识中，我觉得这种无忧的生活会继续下去。

人之所以会焦虑，是因为每个人都会有自己的期望，当担心这个期望无法达成，亦或者短时间内无法达成的时候，人就会产生一种焦虑的情绪。


作为一个35岁的码农，很自然地会把自己和“中年危机”关联到一起。
因为担心自己精力不够，无法胜任高强度的工作。
因为担心自己思维不再灵活，无法做到全面地思考。
因为担心自己家庭原因，无法平衡家庭与工作。
因为担心自己期望过高，无法得到很好的上升空间。
因为担心自己失业，无法再承担起家庭的责任。


非常害怕迷迷糊糊地到了某个年纪，突然发现自己对这个世界已经无能为力了：梦想与现实落差巨大，生活和工作压力缠身，而优秀的同龄人已绝尘而去。一时间，焦虑急躁又如梦初醒


“为什么没有早点知道这个世界的真相？”


“为什么没有在最好的年纪及时觉醒？”


但即使含泪拷问，也似乎错过了最佳时机，毕竟人生是个单行道，无法从头再来。
最后不得不敲碎那颗高傲的心，在无奈和叹息中默默接受平庸的人生。
我接下来要做的，就是努力找到一个平衡，学会与焦虑共存，有欲望就会有焦虑，比如看书、写文章、利用好业余时间充实自己，当自己足够强大的时候，或许就能进入另外一种心境。

###  12.自我认知，清醒的认识自己


如果你觉得自己已经错过所谓的最好年纪，其实也没有关系，因为“现在”永远都是开始的最好时机——这不是什么安慰人的话，这是事实。“摩西奶奶”76岁开始学画、80岁举办个人展，王德顺79岁走上T台，褚时健74岁开始创业种橙子……就算你今年60岁，他们仍可以对你说：“孩子，别着急，你至少还有20年可以随时重来……”

我一直担惊受怕，过去，可能是因为我年轻，但现在，我已经不是那么年轻了，我仍然发现有很多事情让我害怕。

当年纪越来越大后，我开始变得不能加班。我开始用更多的时间和家人在一起，而不是坐在计算机前(尽管这样，她们仍是抱怨)。我在本地教育委员会社区里提供一些帮助，还组织开源兴趣小组参加活动。

我在思考，为什么以前会把如此多的时间全部用在编程上。大量的编程。那是我渴望深入研究一个类库，一个框架或一门技术。

现在的技术的学习曲线的增加，让我的忍耐性越来越低。各种新技术，因为新奇让人兴奋，但最终变成一场场争论。我越来越无法忍受这些充满市场宣传气息的喧嚣。我对技术看重的是稳定，清晰。

原文链接：https://zhuanlan.zhihu.com/p/486097118

原文作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.50】让人秒懂的Redis的事件处理机制

redis是单进程，单线程模型，与nginx的多进程不同，与golang的多协程也不同，“工作的工人”那么少，可那么为什么redis能这么快呢？

## 1.**epoll多路复用**

这里重点要说的就是redis的IO编程模型，首先了解下

为什么要有多路复用呢？

如果没有多路复用，一个线程只能监听一个端口的一个连接，这样这个效率比较低。当然我们有几种办法可以破除这个，一个是使用多线程模型，我们还是监听一个端口，但是一个请求进来，我们为其创建一个线程。但是这种消耗是比较大的。所以我们一直想办法，有没有办法一个线程监听多个端口，或者多个一个端口的多个连接（fd）。

这里再说说fd， 文件描述符（file descriptor）是内核为了高效管理已被打开的文件所创建的索引，其是一个非负整数（通常是小整数），用于指代被打开的文件，所有执行I/O操作(包括网络socket操作)的系统调用都通过文件描述符。每个连接请求上来，都会创建一个连接套接字，一个连接使用一个连接套接字。

对于监听端口，我们会有一个监听套接字，对应监听fd。我们所有的监听业务都是从监听这个套接字开始的。

那么如果我一个程序能同时监听多个连接套接字，是不是就很赞了。是的，这就是linux的io多路复用逻辑。但是这么多连接套接字，传递数据等是断断续续的，A连接接收一个包，B连接再接收一个包，A连接再接收一个包，B连接再接收一个包....如果我等着A连接把包都接收完再处理B，那效率是非常慢的。所以，这里我们就需要有一个通知机制，让有收到包的时候通知下处理线程。

linux的IO多路复用逻辑主要有三种：select, poll, epoll。

### **1.1select**

select模型监听的三个事件：读数据事件，写数据事件，异常事件。

使用select模型的步骤如下：

- 我们确定要监听的监听fd列表

- 调用select监听所有监听fd，阻塞线程。

- select只有当有事件出现并且有事件的fd已经等待完毕

- 如果是创建一个连接事件：

- - 创建一个连接套接字，连接fd
  - 将连接fd和监听fd集合放在一起



- 如果是一个读写事件：

- - 遍历所有fd，判断是否是准备好的fd
  - 如果是准备好的fd，进行业务读写逻辑



- 循环进入select。

select一次可以监听1024个文件描述符。

### 1.2**poll模型**

poll传递给内核的是：

- 监听的fd集合
- 需要监听的事件类型
- 实际发生的事件类型

poll的模型逻辑是：

- 我们确定要监听的监听fd列表

- 调用poll监听所有监听fd，阻塞线程。

- poll只有当有事件出现才解除阻塞

- 如果是创建一个连接事件：

- - 创建一个连接套接字，连接fd
  - 将连接fd和监听fd集合放在一起



- 如果是一个读写事件：

- - 遍历所有fd，判断是否是有读写事件的fd
  - 如果fd有读写事件，进行业务读写逻辑



- 循环进入poll。

poll比select优秀在它没有了1024的限制了。但是还是有一些缺陷，就是必须要遍历所有fd。

### 1.3**epoll**

epoll的数据结构类似poll，但是在调用epoll的时候，它不是返回发生了事件的fd个数，而是返回了所有发生的事件，这个事件中可以查出发生事件的fd。

所以epoll的逻辑模型是：

- 我们确定要监听的监听fd列表

- 调用epoll监听所有监听fd，阻塞线程。

- epoll只有当有事件出现才解除阻塞，并且返回事件列表

- 遍历事件列表：

- 如果是创建一个连接事件：

- - 创建一个连接套接字，连接fd
  - 将连接fd和监听fd集合放在一起，继续epoll



- 如果是一个读写事件：

- - 处理这个事件



- 循环进入epoll。

说白了，epoll就是我们逻辑上能想到的最优的通知机制。一群人去排队，有多个事件发生，警察来了，那么就告诉警察有哪几个列发生了什么事件，警察一个个处理就行了。

## 2.**Reactor模型**

有了IO多路复用的机制，我们就可以实现一种模型，叫做Reactor模型了。

最经典的一张图就是这个

![img](https://pic1.zhimg.com/80/v2-620435931dcba05592e271fc38df11e8_720w.webp)

reactor的五大角色：

- Handle（句柄或者是描述符）
- Synchronous Event Demultiplexer(同步事件分离器)
- Event Handler(事件处理器)
- Concrete Event Handler(具体事件处理器)
- Initiation Dispatcher(初始分发器)

简要来说，Reactor就是我们现在最正常理解的“事件驱动”，对，就是字面理解的那种。比如订阅一个kafka，我们会创建一个监听程序，监听kafka的某个topic，然后在监听程序中挂载几个处理不同消息的处理程序，每当有一个事件从topic进入的时候，我们就会有通过这个监听程序，通知我们的处理程序。处理程序来处理不同的消息。

这种所谓的通知机制，就叫做reactor。

这个kafka的例子，里面有一个监听事件的程序，它一定是一个同步的，一条消息来了，投递一个消息，就叫做 Synchronous Event Demultiplexer(同步事件分离器)。而这个消息，就是Handle（句柄或者是描述符）。我们需要将某个具体的事件处理函数，也就是上图的Concrete Event Handler(具体事件处理器) 挂载到监听的处理程序中。当然这里的每个Concrete Event Handler(具体事件处理器) 都必须遵照某种格式，比如定义了handle_event和get_handle接口。这种格式我们统称为Event Handler(事件处理器)。再回到监听事件，监听事件一定有一个挂载的具体map之类的结构，即哪个事件对应哪个处理程序，这个挂载的核心我们叫它Initiation Dispatcher(初始分发器)。

标准的处理流程描述如下：

- 当应用向Initiation Dispatcher注册Concrete Event Handler时，应用会标识出该事件处理器希望Initiation Dispatcher在某种类型的事件发生发生时向其通知，事件与handle关联
- Initiation Dispatcher要求注册在其上面的Concrete Event Handler传递内部关联的handle，该handle会向操作系统标识
- 当所有的Concrete Event Handler都注册到 Initiation Dispatcher上后，应用会调用handle_events方法来启动Initiation Dispatcher的事件循环，这时Initiation Dispatcher会将每个Concrete Event Handler关联的handle合并，并使用Synchronous Event Demultiplexer来等待这些handle上事件的发生
- 当与某个事件源对应的handle变为ready时，Synchronous Event Demultiplexer便会通知 Initiation Dispatcher。比如tcp的socket变为ready for reading
- Initiation Dispatcher会触发事件处理器的回调方法。当事件发生时， Initiation Dispatcher会将被一个“key”（表示一个激活的handle）定位和分发给特定的Event Handler的回调方法
- Initiation Dispatcher调用特定的Concrete Event Handler的回调方法来响应其关联的handle上发生的事件

在这五种角色中，

其中的 Initiation Dispatcher(初始分发器) 是最重要的，我们也称其为Reactor。它本身定义了一些规范，同时提供了Handler的一些注册机制。

而Synchronous Event Demultiplexer(同步事件分离器) 在IO场景下，一般是由操作系统底层实现的，就是说操作系统底层必须能有这个能力，才能基于这个能力实现Reactor模型。在我们这个场景下，就是前面提到的linux的多路复用机制。

Handle（句柄或者是描述符）在IO场景下就是IO网络连接的fd。

而Event Handler(事件处理器) 和 Concrete Event Handler(具体事件处理器) 在IO场景下分为三种处理事件：连接事件，写事件，读事件。对于连接事件的处理器，我们称之为acceptor，读/写事件的处理器，我们统称为handler。

所以在IO场景下，Reactor 我们需要实现的三个关键角色为：reactor、acceptor、handler。

## 3.**Redis的实现**

在redis中，下面一张图就能说明其实现逻辑。

![img](https://pic1.zhimg.com/80/v2-1ced02d894a1107c68e301a7e18da870_720w.webp)

在redis中，有个reactor（叫做aeMain）接收客户端的redis请求。而在这个reactor中除了监听连接事件acceptor之外，还可以动态注册各种handler （aeCreateFileEvent）。当一个客户端请求进入的时候，调用 aeProcessEvents 来分发事件。

这个逻辑就很清晰了吧。整个就是redis的事件处理机制。

原文链接：https://zhuanlan.zhihu.com/p/486698266

原文作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.51】一文弄懂Linux下五种IO模型

Linux下主要的IO主要分为：阻塞IO(Blocking IO)，非阻塞IO(Non-blocking IO)，同步IO(Sync IO)和异步IO(Async IO)。 同步：调用端会一直等待服务端响应，直到返回结果。 异步：调用端发起调用之后不会立刻返回，不会等待服务端响应。服务端通过通知机制或者回调函数来通知客户端。 阻塞：服务端返回结果之前，客户端线程会被挂起，此时线程不可被CPU调度，线程暂停运行。 非阻塞：在服务端返回前，函数不会阻塞调用端线程，而会立刻返回。
同步异步的区别在于：服务端在拷贝数据时是否阻塞调用端线程；阻塞和非阻塞的区别在于：调用端线程在调用function后是否立刻返回。要理解这些I/O，需要先理解一些基本的概念。

###  1.用户态和核心态


Linux系统中分为核心态(Kernel model)和用户态(User model)，CPU会在两个model之间切换。

1. 核心态代码拥有完全的底层资源控制权限，可以执行任何CPU指令，访问任何内存地址，其占有的处理机是不允许被抢占的。内核态的指令包括：启动I/O，内存清零，修改程序状态字，设置时钟，允许/终止中断和停机。内核态的程序崩溃会导致PC停机。
2. 用户态是用户程序能够使用的指令，不能直接访问底层硬件和内存地址。用户态运行的程序必须委托系统调用来访问硬件和内存。用户态的指令包括：控制转移，算数运算，取数指令，访管指令（使用户程序从用户态陷入内核态）。

### 2.**用户态和核心态的切换**


用户态切换到核心态有三种方式： a.系统调用
这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作，比如前例中fork()实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。 b.异常
当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。 c.外围设备的中断
当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。

###  3.进程切换


为了控制进程的执行，内核必须有能力挂起正在CPU上运行的进程，并恢复以前挂起的某个进程的执行。这种行为被称为进程切换。因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化：

1. 保存处理机上下文，包括程序计数器和其他寄存器。
2. 更新PCB信息。
3. 把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。
4. 选择另一个进程执行，并更新其PCB。
5. 更新内存管理的数据结构。
6. 恢复处理机上下文。

### 4.进程阻塞


正在执行的进程由于一些事情发生，如请求资源失败、等待某种操作完成、新数据尚未达到或者没有新工作做等，由系统自动执行阻塞原语，使进程状态变为阻塞状态。因此，进程阻塞是进程自身的一种主动行为，只有处于运行中的进程才可以将自身转化为阻塞状态。**当进程被阻塞，它是不占用CPU资源的。**

###  5.文件描述符(fd, File Descriptor)


FD用于描述指向文件的引用的抽象化概念。文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于UNIX、Linux这样的操作系统。

###  6.缓存I/O


缓存IO又被称作标准IO，大多数文件系统的默认IO 操作都是缓存IO。在Linux的缓存IO 机制中，操作系统会将 IO 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。

###  7.缓存IO的缺点：


数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。

### 8. Linux下的五种I/O模型


Linux下主要有以下五种I/O模型：

1. 阻塞I/O（blocking IO）
2. 非阻塞I/O (nonblocking I/O)
3. I/O 复用 (I/O multiplexing)
4. 信号驱动I/O (signal driven I/O (SIGIO))
5. 异步I/O (asynchronous I/O)

### 9.阻塞IO模型


进程会一直阻塞，直到数据拷贝完成 应用程序调用一个IO函数，导致应用程序阻塞，等待数据准备好。数据准备好后，从内核拷贝到用户空间，IO函数返回成功指示。阻塞IO模型图如下所示：

![img](https://pic3.zhimg.com/80/v2-90291cc3560d4038c8010b77a38c9566_720w.webp)

### 10.非阻塞IO模型


通过进程反复调用IO函数，在数据拷贝过程中，进程是阻塞的。模型图如下所示:

![img](https://pic3.zhimg.com/80/v2-0a1b4b7f6fa5deba709bea7eb73a849a_720w.webp)

### 11.IO复用模型


主要是select和epoll。一个线程可以对多个IO端口进行监听，当socket有读写事件时分发到具体的线程进行处理。模型如下所示：

![img](https://pic1.zhimg.com/80/v2-3ae09faf2d633b0343c339ca107e71f0_720w.webp)

### 12.信号驱动IO模型


信号驱动式I/O：首先我们允许Socket进行信号驱动IO,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个SIGIO信号，可以在信号处理函数中调用I/O操作函数处理数据。过程如下图所示：

![img](https://pic3.zhimg.com/80/v2-377b78d443b3519dce79a0de29fc08da_720w.webp)

### 13.异步IO模型


相对于同步IO，异步IO不是顺序执行。用户进程进行aio_read系统调用之后，无论内核数据是否准备好，都会直接返回给用户进程，然后用户态进程可以去做别的事情。等到socket数据准备好了，内核直接复制数据给进程，然后从内核向进程发送通知。IO两个阶段，进程都是非阻塞的。异步过程如下图所示：

![img](https://pic4.zhimg.com/80/v2-74cd8c6386be65dc597c657ab87d9b6b_720w.webp)

### 14.五种IO模型比较


**阻塞IO和非阻塞IO的区别**
调用阻塞IO后进程会一直等待对应的进程完成，而非阻塞IO不会等待对应的进程完成，在kernel还在准备数据的情况下直接返回。 **同步IO和异步IO的区别**
首先看一下POSIX中对这两个IO的定义：

```text
A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;
An asynchronous I/O operation does not cause the requesting process to be blocked;
```

**两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。**按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。注意到non-blocking IO会一直轮询(polling)，这个过程是没有阻塞的，但是recvfrom阶段blocking IO,non-blocking IO和IO multiplexing都是阻塞的。 而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。

![img](https://pic1.zhimg.com/80/v2-044036bf2551ada39157b487f79e9b80_720w.webp)

### 15.IO复用之select、poll、epoll简介


epoll是linux所特有，而select是POSIX所规定，一般操作系统均有实现。

###  16.select


select本质是通过设置或检查存放fd标志位的数据结构来进行下一步处理。缺点是：

1. 单个进程可监视的fd数量被限制，即能监听端口的大小有限。一般来说和系统内存有关，具体数目可以cat /proc/sys/fs/file-max察看。32位默认是1024个，64位默认为2048个
2. 对socket进行扫描时是线性扫描，即采用轮询方法，效率低。当套接字比较多的时候，每次select()都要遍历FD_SETSIZE个socket来完成调度，不管socket是否活跃都遍历一遍。会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，就避免了轮询，这正是epoll与kqueue做的
3. 需要维护一个用来存放大量fd的数据结构，会使得用户空间和内核空间在传递该结构时复制开销大

### 17.poll


poll本质和select相同，将用户传入的数据拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或主动超时，被唤醒后又要再次遍历fd。它没有最大连接数的限制，原因是它是基于链表来存储的，但缺点是：

1. 大量的fd的数组被整体复制到用户态和内核空间之间，不管有无意义。
2. poll还有一个特点“水平触发”，如果报告了fd后，没有被处理，那么下次poll时再次报告该ffd。

### 18.epoll


epoll支持水平触发和边缘触发，最大特点在于边缘触发，只告诉哪些fd刚刚变为就绪态，并且只通知一次。还有一特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一量该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。epoll的优点：

1. 没有最大并发连接的限制。
2. 效率提升，只有活跃可用的FD才会调用callback函数。
3. 内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递。

### 19.select、poll、epoll区别总结：

![img](https://pic2.zhimg.com/80/v2-d93ef0b2ec588d2b8a7cd5a9918de571_720w.webp)

参考资料：
《Unix网络编程》

原文链接：https://zhuanlan.zhihu.com/p/487395852

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.52】微信终端自研 C++协程框架的设计与实现

## **1. 背景**

基于跨平台考虑，微信终端很多基础组件使用 C++ 编写，随着业务越来越复杂，传统异步编程模型已经无法满足业务需要。Modern C++ 虽然一直在改进，但一直没有统一编程模型，为了提升开发效率，改善代码质量，我们自研了一套 `C++ 协程框架 owl`，用于为所有基础组件提供统一的编程模型。

owl 协程框架目前主要应用于 `C++ 跨平台微信客户端内核（Alita）`，Alita 的业务逻辑部分全部用协程实现，相比传统异步编程模型，**至少减少了 50% 代码量**。Alita 目前已经应用于儿童手表微信、Linux 车机微信、Android 车机微信等多个业务，其中 **Linux 车机微信的所有 UI 逻辑也全部用协程实现**。

## **2.为什么要造轮子？**

那么问题来了，既然 C++20 已经支持了协程，业界也有不少开源方案（如 libco、libgo 等），为什么不直接使用？

原因：

- owl 基础库需要支持尽量多的操作系统和架构，操作系统包括：Android、iOS、macOS、Windows、Linux、`RTOS`；架构包括：x86、x86_64、arm、arm64、`loongarch64`，目前并没有任何一个方案能直接支持。
- owl 协程自 2019 年初就推出了，而当时 C++20 还未成熟，实际上到目前为止 C++20 普及程度依然不高，公司内部和外部合作伙伴的编译器版本普遍较低，导致目前 owl 最多只能用到 C++14 的特性
- 业界已有方案有不少缺点：
- 1. 大多为后台开发设计，不适用终端开发场景
  2. 基本只支持 Linux 系统和 x86/x86_64 架构
  3. 封装层次较低，大多是玩具或 API 级别，并没有达到框架级别
  4. 在 C++ 终端开发没有看到大规模应用案例

## **3.Show me the code**

那么协程比传统异步编程到底好在哪里？下面我们结合代码来展示一下协程的优势，同时也回顾一下异步编程模型的演化过程：

假设有一个异步方法 `AsyncAddOne`，用于将一个 int 值加 1，为了简单起见，这里直接开一个线程 sleep 100ms 后再回调新的值：

```
void AsyncAddOne(int value, std::function<void (int)> callback) {    std::thread t([value, callback = std::move(callback)] {        std::this_thread::sleep_for(100ms);        callback(value + 1);    });    t.detach();}
```

要调用 `AsyncAddOne` 将一个 int 值加 3，有三种主流写法：

### 3.1 Callback

传统回调方式，代码写起来是这样：

```
AsyncAddOne(100, [] (int result) {    AsyncAddOne(result, [] (int result) {        AsyncAddOne(result, [] (int result) {            printf("result %d\n", result);        });    });});
```

回调有一些众所周知的痛点，如回调地狱、信任问题、错误处理困难、生命周期管理困难等，在此不再赘述。

### 3.2 Promise

Promise 解决了 Callback 的痛点，使用 owl::promise 库的代码写起来是这样：

```
// 将回调风格的 AsyncAddOne 转成 Promise 风格owl::promise AsyncAddOnePromise(int value) {    return owl::make_promise([=] (auto d) {        AsyncAddOne(value, [=] (int result) {            d.resolve(result);        });    });}// Promise 方式AsyncAddOnePromise(100).then([] (int result) {    return AsyncAddOnePromise(result);}).then([] (int result) {    return AsyncAddOnePromise(result);}).then([] (int result) {    printf("result %d\n", result);});
```

很显然，由于消除了回调地狱，代码漂亮多了。实际上 owl::promise 解决了 Callback 的所有痛点，通过使用模版元编程和类型擦除技术，甚至连语法都接近 JavaScript Promise。

但实践发现，Promise 只适合线性异步逻辑，复杂一点的异步逻辑用 Promise 写起来也很乱（如循环调用某个异步接口），因此我们废弃了 owl::promise，最终将方案转向了协程。

### 3.3 Coroutine

使用 owl 协程写起来是这样：

```
// 将回调风格的 AsyncAddOne 转成 Promise 风格// 注：// owl::promise 擦除了类型，owl::promise2 是类型安全版本// owl 协程需要配合 owl::promise2 使用owl::promise2<int> AsyncAddOnePromise2(int value) {    return owl::make_promise2<int>([=] (auto d) {        AsyncAddOne(value, [=] (int result) {            d.resolve(result);        });    });}// Coroutine 方式// 使用 co_launch 启动一个协程// 在协程中即可使用 co_await 将异步调用转成同步方式owl::co_launch([] {    auto value = 100;    for (auto i = 0; i < 3; i++) {        value = co_await AsyncAddOnePromise2(value);    }    printf("result %d\n", value);});
```

使用协程可以用同步方式写异步代码，大大减轻了异步编程的心智负担。`co_await` 语法糖让 owl 协程写起来跟很多语言内置的协程并无差别。

## **4.回调转协程**

要在实际业务中使用协程，必须通过某种方式让回调代码转换为协程支持的形式。通过上面的例子可以看出，回调风格接口要支持在协程中同步调用非常简单，只需短短几行代码将回调接口先转成 Promise 接口，在协程中即可直接通过 `co_await` 调用：

```
// 回调接口void AsyncAddOne(int value, std::function<void (int)> callback);// Promise 接口owl::promise2<int> AsyncAddOnePromise2(int value);// 协程中调用auto value = co_await AsyncAddOnePromise2(100);
```

实际项目中通常会省略掉上述中间步骤，直接一步到位：

```
// 在协程中可以像调用普通函数一样调用此函数int AsyncAddOneCoroutine(int value) {    return co_await owl::make_promise2<int>([=] (auto d) {        AsyncAddOne(value, [=] (int result) {            d.resolve(result);        });    });}
```

> 后台开发使用协程，通常会 hook socket 相关的 I/O API，而终端开发很少需要在协程中使用底层 I/O 能力，通常已经封装好了高层次的异步 I/O 接口，因此 owl 协程并没有 hook I/O API，而是提供一种方便的将回调转协程的方式。

### **4.1 一个完整的例子**

上述代码片段很难体现出协程的实际用法，这个例子使用协程实现了一个 tcp-echo-server，只有 40 多行代码：

```
int main(int argc, char* argv[]) {    // 使用 co_thread_scope() 创建一个协程作用域，并启动一个线程作为协程调度器    co_thread_scope() {        owl::tcp_server server;        int error = server.listen(3090);        if (error < 0) {            zerror("tcp server listen failed!");            return;        }        zinfo("tcp server listen OK, local %_", server.local_address());        while (true) {            auto client = server.accept();            if (!client) {                zerror("tcp server accept failed!");                break;            }            zinfo("accept OK, local %_, peer %_", client->local_address(), client->peer_address());            // 当有新 client 连接时，使用 co_launch 启动一个协程专门处理            owl::co_launch([client] {                char buf[1024] = { 0 };                while (true) {                    auto num_recv = client->recv_some(buf, sizeof(buf), 0);                    if (num_recv <= 0) {                        break;                    }                    buf[num_recv] = '\0';                    zinfo("[fd=%_] RECV %_ bytes: %_", client->fd(), num_recv, buf);                    if (strcmp(buf, "exit") == 0) {                        break;                    }                    auto num_send = client->send(buf, num_recv, 0);                    if (num_send < 0) {                        break;                    }                    zinfo("[fd=%_] SENT %_ bytes back", client->fd(), num_send);                }            });        }    };    return 0;}
```

### **4.2 框架分层**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131433182674890.png)

为了便于扩展和复用，owl 协程采用分层设计，开发者可以直接使用最上层的 API，也可以基于 Context API 或 Core API 搭建自己的协程框架。

### **4.3 协程设计**

#### 4.3.1 协程栈

协程按有无调用栈分为两类：

- **有栈协程**（stackful）：每个协程都有自己的调用栈，类似于线程的调用栈
- **无栈协程**（stackless）：协程没有调用栈，协程的状态通过状态机或闭包来实现

很显然，无栈协程比有栈协程占用更少的内存，但无栈协程通常需要手动管理状态，如果自研协程采用无栈方式会非常难用。因此语言级别的协程通常使用无栈协程，将复杂的状态管理交给编译器处理；自研方案通常使用有栈协程，**owl 也不例外是有栈协程**。

有栈协程按栈的管理方式又可以分为两类：

- **独立栈**：每个协程都有独立的调用栈
- **共享栈**：每个协程都有独立的状态栈，一个线程中的多个协程共享一个调用栈。由于这些协程中同时只会有一个协程处于活跃状态，当前活跃的协程可以临时使用调用栈。当此协程被挂起时，将调用栈中的状态保存到自身的状态栈；当协程恢复运行时，将状态栈再拷贝到调用栈。实践中通常设置较大的调用栈和较小的状态栈，来达到节省内存的目的。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131433375111499.png)

共享栈本质上是一种时间换空间的做法，但共享栈有一个比较明显的缺点，看代码：

```
owl::co_launch("co1", [] {    char buf[1024] = { 0 };    auto job = owl::co_launch("co2", [&buf] {        // oops!!!        buf[0] = 'a';    });    job->join();});
```

上面的代码在共享栈模式下会出问题，协程 `co1` 在栈上分配的 `buf`，在协程 `co2` 访问的时候已经失效了。要规避共享栈的这个缺点，可能需要对协程的使用做一些限制或检查，无疑会加重使用者的负担。

对于终端开发，由于同时运行的协程数量并不多，性能问题并不明显，为了使用上的便捷性，**owl 协程使用独立栈**。

选择独立栈之后，协程栈应该如何分配又是另外的问题，有如下几种方案：

- **Split Stacks**：简单来说是一个支持自动增长的非连续栈，由于只有 gcc 支持且有兼容性问题，实践中比较少用
- **malloc/mmap**：直接使用 malloc 或 mmap 分配内存，业界主流方案
- **Thread Stack**：在线程中预先分配一大段栈内存作为协程栈，业界比较少用

> 后两种方案通常还会采用内存池来优化性能，采用 `mprotect` 来进行栈保护

owl 协程同时使用了后两种方案，那么什么场景下会使用到 `Thread Stack` 方案呢？因为 `Android JNI` 和部分 `RTOS 系统调用` 会检查 `sp` 寄存器是否在线程栈空间内，如果不在则认为栈被破坏，程序会直接挂掉。独立栈协程在执行时 `sp` 寄存器会被修改为指向协程栈，而通过 `malloc/mmap` 分配的协程栈空间不属于任何线程栈，一定无法通过 `sp` 检查。为了解决这个问题，我们在 `Android` 和部分 `RTOS` 上默认使用 `Thread Stack`。

#### 4.3.2 协程调度

协程按控制传递机制分为两类：

- **对称协程**（Symmetric Coroutine）：和线程类似，协程之间是对等关系，多个协程之间可以任意跳转
- **非对称协程**（Asymmetric Coroutine）：协程之间存在调用和被调用关系，如协程 A 调用/恢复协程 B，协程 B 挂起/返回时只能回到协程 A

非对称协程与函数调用类似，比较容易理解，主流编程语言对协程的支持大都是非对称协程。从实现的角度，非对称协程的实现也比较简单，实际上我们很容易用非对称协程实现对称协程。**owl 协程使用非对称协程**。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131433533483278.png)
上图展示了非对称协程调用和函数调用的相似性，详细的时序如下：

1. 调用者调用 `co_create()` 创建协程，这一步会分配一个单独的协程栈，并为 `func` 设置好执行环境
2. 调用者调用 `co_resume()` 启动协程，`func` 函数开始运行
3. 协程运行到 `co_yield()`，协程挂起自己并返回到调用者
4. 调用者调用 `co_resume()` 恢复协程，协程从 `co_yield()` 后续代码继续执行
5. 协程执行完毕，返回到调用者

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131434152330518.png)

如上图所示，有意思的是，如果一个协程没用调用 `co_yield()`，这个协程的调用流程其实跟函数一模一样，因此我们经常会说：**函数就是协程的一种特例**。

#### 4.3.3 单线程调度器

协程和线程很像，不同的是线程多是抢占式调度，而协程多是协作式调度。多个线程之间共享资源时通常需要锁和信号量等同步原语，而协程可以不需要。

通过上面的示例可以看出，使用 `co_create()` 创建协程后，可以通过不断调用 `co_resume()` 来驱动协程的运行，而协程函数可以随时调用 `co_yield()` 来挂起自己并将控制权转移给调用者。

很显然，当协程数量较多时，通过手工调用 `co_resume()` 来驱动协程不太现实，因此需要实现协程调度器。

协程调度器分为两类：

- **1:N 调度**（单线程调度）：使用 1 个线程调度 N 个协程，由于多个协程都在同一个线程中运行，因此协程之间访问共享资源无需加锁
- **M:N 调度**（多线程调度）：使用 M 个线程调度 N 个协程，由于多个协程可能不在同一个线程运行，甚至同一个协程每次调度都有可能运行在不同线程，因此协程之间访问共享资源需要加锁，且协程中使用 TLS（Thread Local Storage） 会有问题

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131434266203938.png)

单线程调度通常使用 `RunLoop` 之类的消息循环来作为调度器，虽然调度性能低于多线程调度，但单线程调度器可以免加锁的特性，能极大降低编码复杂度，因此 **owl 协程使用单线程调度**。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131434395310811.png)

使用 `RunLoop` 作为调度器的原理其实很简单，将所有 `co_resume()` 调用都 Post 到 `RunLoop` 中执行即可。原理如图所示，要想象一个协程是如何在 `RunLoop` 中执行的，大概可以认为是：**协程函数中的代码被 co_yield() 分隔成多个部分，每一部分代码都被 Post 到 RunLoop 中执行**。

使用 `RunLoop` 作为调度器除了协程不用加锁，还有一些额外的好处：

- 协程中的代码可以和 `RunLoop` 中的传统异步代码和谐共处
- 若使用 UI 框架的 `RunLoop` 作为调度器，从协程中可以直接访问 UI

为了方便扩展，owl 协程将调度器抽象成一个单独的接口类，开发者可以很容易实现自己的调度器，或和项目已有的 `RunLoop` 机制结合：

```
class executor {public:    virtual ~executor() {}    virtual uint64_t post(std::function<void ()> closure) = 0;    virtual uint64_t post_delayed(unsigned delay, std::function<void ()> closure) = 0;    virtual void cancel(uint64_t id) {}};
```

在 Linux 车机微信客户端，我们通过实现自定义调度器让协程运行在 UI 框架的消息循环中，得以方便地在协程中访问 UI。

#### 4.3.4 协程间通信

通过使用单线程调度器，多个协程之间访问共享资源不再需要多线程的锁机制了。

那么用协程写代码是否就完全不需要加锁呢？看代码：

```
static int value = 0;for (auto i = 0; i < 4; ++i) {    owl::co_launch([] {        value++;        owl::co_delay(1000);        value--;        printf("value %d\n", value);    });}
```

假设协程中要先将 `value++`，做完一些事情，再将 `value--`，我们期望最终 4 个协程的输出都是 0。但由于 `owl::co_delay(1000)` 这一行导致了协程调度，最终输出结果必然不符合预期。

一些协程库为了解决这种问题，提供了和多线程锁类似的协程锁机制。好不容易避免了线程锁，又要引入协程锁，难道没有更好的办法了吗？

实际上目前主流的并发模型除了共享内存模型，还有 Actor 模型与 CSP（Communicating Sequential Processes）模型，对比如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131438366480941.png)

> Do not communicate by sharing memory; instead, share memory by communicating. 不要通过共享内存来通信，而应该通过通信来共享内存

相信这句 Go 语言的哲学大家已经不陌生了，如何理解这句话？本质上看，多个线程或协程之间同步信息最终都是通过`共享内存`来进行的，因为无论是用哪种通信模型，最终都是从内存中获取数据，因此这句话我们可以理解为 `尽量使用消息来通信，而不要直接共享内存`。

Actor 模型和 CSP 模型采用的都是消息机制，区别在于 Actor 模型里协程与消息队列（mailbox）是绑定关系；而 CSP 模型里协程与消息队列（channel）是独立的。从耦合性的角度，CSP 模型比 Actor 模型更松耦合，因此 **owl 协程使用 channel 作为协程间通信机制**。

由于我们在实际业务开发中并没有遇到一定需要协程锁的场景，因此 owl 协程暂没有提供协程锁机制。

#### 4.3.5 结构化并发

想象这样一个场景：我们写一个 UI 界面，在这个界面会启动若干协程通过网络去拉取和更新数据，当用户退出 UI 时，为了不泄露资源，我们希望协程以及协程发起的异步操作都能取消。当然，我们可以通过手动保存每一个协程的句柄，在 UI 退出时通知每一个协程退出，并等待所有协程都结束后再退出 UI。然而，手动进行上述操作非常繁琐，而且很难保证正确性。

不止是使用协程才会遇到上述问题，把协程换成线程，问题依然存在。传统并发主要有两类问题：

- **生命周期问题**：如何保证协程引用的资源不被突然释放？
- **协程取消问题**：1）如何打断正在挂起的协程？2）结束协程时，如何同时结束协程中创建的子协程？3）如何等待所有子协程都结束后再结束父协程？

这里的主要矛盾在于：**协程是独立的，但业务是结构化的**。

为了解决这个问题，owl 协程引入了**结构化并发**：

结构化并发的概念是：

- 作用域中的并发操作，必须在作用域退出前结束
- 作用域可以嵌套

作用域是一个抽象概念，有明确生命周期的实体都是作用域，如：

- 一个代码块
- 一个对象
- 一个 UI 页面

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131438481255578.png)

如上图所示，代码由上而下执行，在进入外部 scope 后，从 scope 中启动了两个协程，并进入了内部 scope，当执行流最终从外部 scope 出来时，结构化并发机制必须保证这两个协程已经结束。同样的，若内部 scope 中启动了协程，执行流从内部 scope 出来时，也必须保证其中的协程全部结束。

结构化并发在 owl 协程的实现其实并不复杂，本质上是一个树形结构：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131439133560592.png)

核心理念是：

- 协程也是一个作用域
- 协程有父子关系
- 父协程取消，子协程也自动取消
- 父协程结束前，必须等待子协程结束

光说概念有点抽象，最后来看一个 owl 协程结构化并发的例子：

```
class SimpleActivity {public:    SimpleActivity() {        // 为 scope_ 设置调度器，后续通过 scope_ 启动的协程        // 默认使用 UI 的消息循环作为调度器        scope_.set_exec(GetUiExecutor());    }    ~SimpleActivity() {        // UI 销毁的时候取消所有子协程        scope_.cancel();        // scope_ 析构时会等待所有子协程结束    }    void OnButtonClicked() {        // 在 UI 事件中通过 scope_ 启动协程        scope_.co_launch([=] {            // 启动子协程下载图片            auto p1 = owl::co_async([] { return DownloadImage(...); });            auto p2 = owl::co_async([] { return DownloadImage(...); });            // 等待图片下载完毕            auto image1 = co_await p1;            auto image2 = co_await p2;            // 合并图片            auto new_image = co_await AsyncCombineImage(image1, image2);            // 更新图片，由于协程运行在消息循环中，可以直接访问 UI            image_->SetImage(new_image);        });        // 可以通过 scope_ 启动任意多个协程        scope_.co_launch([=] {            ...        });    }private:    owl::co_scope scope_;    ImageLabel* image_;};
```

#### 4.3.6 性能测试

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131439288478733.png)

说明：

- 上下文切换：使用 Context API 进行上下文切换的性能，耗时在 `20~30ns` 级别
- 协程切换：使用单线程调度器进行协程切换的性能，耗时在 `0.5~3us` 级别
- 线程切换：pthread 线程切换的性能，耗时在 `2~8us` 级别

owl 协程受限于单线程调度器性能，切换速度和上下文切换比并不算快，但在终端使用也足够了。

## 5.**总结**

总的来说，自 owl 协程在实际项目中应用以来，开发效率和代码质量都有很大提升。owl 协程虽然已经得到广泛应用，但还存在很多不完善的地方，未来会继续迭代打磨。owl 现阶段在腾讯内部开源，待框架更完善且 API 稳定后，再进行对外开源。

作者：peterfan，腾讯 WXG 客户端开发工程师

原文链接：https://mp.weixin.qq.com/s/TNkvze6YtdTaLtwUDHfwHg

# 【NO.53】高并发系统建设经验总结

## **0.前言**

早期从事运单系统的开发和维护工作，从最早的日均百万单，到日均千万单，业务的快速发展再加上外卖业务的特点是，业务量集中在午高峰和晚高峰两个高峰期，所以高峰期并发请求量也是水涨船高，每天都要面对高并发的挑战。拿运单系统来举例，日常午高峰核心查询服务的 QPS 在 20 万以上，Redis 集群的 QPS 更是在百万级，数据库 QPS 也在 10 万级以上，TPS 在 2 万以上。

在这么大的流量下，主要的工作也是以围绕如何建设系统的稳定性和提升容量展开，下面主要从基础设施、数据库、架构、应用、规范这几方面谈谈如何建设高并发的系统。以下都是我个人这几年的一些经验总结，架构没有银弹，因此也称不上是最佳实践，**仅供参考**。

## 1.**基础设施**

在分层架构中，最底层的就是基础设施。基础设置一般来说包含了物理服务器、IDC、部署方式等等。就像一个金字塔，基础设施就是金字塔的底座，只有底座稳定了，上层才能稳定。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131451263847897.png)

### 1.1 异地多活

多活可以分为同城多活、异地多活等等，实现方式也有多种，比如阿里使用的单元化方案，饿了么使用的是多中心的方案，关于多活的实现可以参考：[饿了么多活实现分享](https://zhuanlan.zhihu.com/p/32009822)。当时做多活的主要出发点是保证系统的高可用性，避免单 IDC 的单点故障问题，同时由于每个机房的流量都变成了总流量的 1/N，也变相提升了系统容量，在高并发的场景下可以抗更多的流量。下图是活的整体架构，来源于上面多活实现的分享文章中。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131451436240470.png)

### **1.2 数据库**

数据库是整个系统最重要的组成部分之一，在高并发的场景下很大一部分工作是围绕数据库展开的，主要需要解决的问题是如何提升数据库容量。

#### 1.2.1 读写分离

互联网的大部分业务特点是读多写少，因此使用读写分离架构可以有效降低数据库的负载，提升系统容量和稳定性。核心思路是由主库承担写流量，从库承担读流量，且在读写分离架构中一般都是 1 主多从的配置，通过多个从库来分担高并发的查询流量。比如现在有 1 万 QPS 的以及 1K 的 TPS，假设在 1 主 5 从的配置下，主库只承担 1K 的 TPS，每个从库承担 2K 的 QPS，这种量级对 DB 来说是完全可接受的，相比读写分离改造前，DB 的压力明显小了许多。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131451563971678.png)

这种模式的好处是简单，几乎没有代码改造成本或只有少量的代码改造成本，只需要配置数据库主从即可。缺点也是同样明显的：

##### **1.2.1.1 主从延迟**

MySQL 默认的主从复制是异步的，如果在主库插入数据后马上去从库查询，可能会发生查不到的情况。正常情况下主从复制会存在毫秒级的延迟，在 DB 负载较高的情况下可能存在秒级延迟甚至更久，但即使是毫秒级的延迟，对于实时性要求较高的业务来说也是不可忽视的。所以在一些关键的查询场景，我们会将查询请求绑定到主库来避免主从延迟的问题。关于主从延迟的优化网上也有不少的文章分享，这里就不再赘述。

##### 1.2.1.2**从库的数量是有限的**

一个主库能挂载的从库数量是很有限的，没办法做到无限的水平扩展。从库越多，虽然理论上能承受的 QPS 就越高，但是从库过多会导致主库主从复制 IO 压力更大，造成更高的延迟，从而影响业务，所以一般来说只会在主库后挂载有限的几个从库。

##### 1.2.1.3 无法解决 TPS 高的问题

从库虽然能解决 QPS 高的问题，但没办法解决 TPS 高的问题，所有的写请求只有主库能处理，一旦 TPS 过高，DB 依然有宕机的风险。

#### 1.2.2 分库分表

当读写分离不能满足业务需要时，就需要考虑使用分库分表模式了。当确定要对数据库做优化时，应该优先考虑使用读写分离的模式，只有在读写分离的模式已经没办法承受业务的流量时，我们才考虑分库分表的模式。分库分表模式的最终效果是把单库单表变成多库多表，如下图。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131452174173978.png)

首先来说下分表，分表可以分为垂直拆分和水平拆分。垂直拆分就是按业务维度拆，假设原来有张订单表有 100 个字段，可以按不同的业务纬度拆成多张表，比如用户信息一张表，支付信息一张表等等，这样每张表的字段相对来说都不会特别多。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131452318784895.png)

水平拆分是把一张表拆分成 N 张表，比如把 1 张订单表，拆成 512 张订单子表。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131452411450793.png)

在实践中可以只做水平拆分或垂直拆分，也可以同时做水平及垂直拆分。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131452511750209.png)
说完了分表，那分库是什么呢？分库就是把原来都在一个 DB 实例中的表，按一定的规则拆分到 N 个 DB 实例中，每个 DB 实例都会有一个 master，相当于是多 mater 的架构，同时为了保证高可用性，每个 master 至少要有 1 个 slave，来保证 master 宕机时 slave 能及时顶上，同时也能保证数据不丢失。拆分完后每个 DB 实例中只会有部分表。

由于是多 master 的架构，分库分表除了包含读写分离模式的所有优点外，还可以解决读写分离架构中无法解决的 TPS 过高的问题，同时分库分表理论上是可以无限横向扩展的，也解决了读写分离架构下从库数量有限的问题。当然在实际的工程实践中一般需要提前预估好容量，因为数据库是有状态的，如果发现容量不足再扩容是非常麻烦的，应该尽量避免。

在分库分表的模式下可以通过不启用查询从库的方式来避免主从延迟的问题，也就是说读写都在主库，因为在分库后，每个 master 上的流量只占总流量的 1/N，大部分情况下能扛住业务的流量，从库只作为 master 的备份，在主库宕机时执行主从切换顶替 master 提供服务使用。说完了好处，再来说说分库分表会带来的问题，主要有以下几点：

##### **1.2.2.1 改造成本高**

分库分表一般需要中间件的支持，常见的模式有两种：客户端模式和代理模式。客户端模式会通过在服务上引用 client 包的方式来实现分库分表的逻辑，比较有代表的是开源的 sharding JDBC。代理模式是指所有的服务不是直接连接 MySQL，而是通过连接代理，代理再连接到 MySQL 的方式，代理需要实现 MySQL 相关的协议。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131453047242241.png)

两种模式各有优劣势，代理模式相对来说会更复杂，但是因为多了一层代理，在代理这层能做更多的事情，也比较方便升级，而且通过代理连接数据库，也能保证数据库的连接数稳定。使用客户端模式好处是相对来说实现比较简单，无中间代理，理论上性能也会更好，但是在升级的时候需要业务方改造代码，因此升级会比代理模式更困难。

##### **1.2.2.2 事务问题**

在业务中我们会使用事务来处理多个数据库操作，通过事务的 4 个特性——一致性、原子性、持久性、隔离性来保证业务流程的正确性。在分库分表后，会将一张表拆分成 N 张子表，这 N 张子表可能又在不同的 DB 实例中，因此虽然逻辑上看起来还是一张表，但其实已经不在一个 DB 实例中了，这就造成了无法使用事务的问题。

最常见的就是在批量操作中，在分库分表前我们可以同时把对多个订单的操作放在一个事务中，但在分库分表后就不能这么干了，因为不同的订单可能属于不同用户，假设我们按用户来分库分表，那么不同用户的订单表位于不同的 DB 实例中，多个 DB 实例显然没办法使用一个事务来处理，这就需要借助一些其他的手段来解决这个问题。在分库分表后应该要尽量避免这种跨 DB 实例的操作，如果一定要这么使用，优先考虑使用补偿等方式保证数据最终一致性，如果一定要强一致性，常用的方案是通过分布式事务的方式。

##### **1.2.2.3无法支持多维度查询**

分库分表一般只能按 1-2 个纬度来分，这个维度就是所谓的`sharding key`。常用的维度有用户、商户等维度，如果按用户的维度来分表，最简单的实现方式就是按用户 ID 来取模定位到在哪个分库哪个分表，这也就意味着之后所有的读写请求都必须带上用户 ID，但在实际业务中不可避免的会存在多个维度的查询，不一定所有的查询都会有用户 ID，这就需要我们对系统进行改造。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131453143091299.png)

为了能在分库分表后也支持多维度查询，常用的解决方案有两种，第一种是引入一张索引表，这张索引表是没有分库分表的，还是以按用户 ID 分库分表为例，索引表上记录各种维度与用户 ID 之间的映射关系，请求需要先通过其他维度查询索引表得到用户 ID，再通过用户 ID 查询分库分表后的表。这样，一来需要多一次 IO，二来索引表由于是没有分库分表的，很容易成为系统瓶颈。

第二种方案是通过引入`NoSQL`的方式，比较常见的组合是`ES+MySQL`，或者`HBase+MySQL`的组合等，这种方案本质上还是通过 NoSQL 来充当第一种方案中的索引表的角色，但是相对于直接使用索引表来说，`NoSQL`具有更好的水平扩展性和伸缩性，只要设计得当，一般不容易成为系统的瓶颈。

##### **1.2.2.4数据迁移**

分库分表一般是需要进行数据迁移的，通过数据迁移将原有的单表数据迁移到分库分表后的库表中。数据迁移的方案常见的有两种，第一种是停机迁移，顾名思义，这种方式简单粗暴，好处是能一步到位，迁移周期短，且能保证数据一致性，坏处是对业务有损，某些关键业务可能无法接受几分钟或更久的停机迁移带来的业务损失。

另外一种方案是双写，这主要是针对新增的增量数据，存量数据可以直接进行数据同步，关于如何进行双写迁移网上已经有很多分享了，这里也就不赘述，核心思想是同时写老库和新库。双写的好处是对业务的影响小，但也更复杂，迁移周期更长，容易出现数据不一致问题，需要有完整的数据一致性保证方案支持。

### 1.3 小结

读写分离模式和分库分表模式推荐优先使用读写分离模式，只有在不满业务需求的情况才才考虑使用分库分表模式。原因是分库分表模式虽然能显著提升数据库的容量，但会增加系统复杂性，而且由于只能支持少数的几个维度读写，从某种意义上来说对业务系统也是一种限制，因此在设计分库分表方案的时候需要结合具体业务场景，更全面的考虑。

## 2. **架构**

在高并发系统建设中，架构同样也是非常重要的，这里分享缓存、消息队列、资源隔离等等模式的一些经验。

### 2.1 缓存

在高并发的系统架构中缓存是最有效的利器，可以说没有之一。缓存的最大作用是可以提升系统性能，保护后端存储不被大流量打垮，增加系统的伸缩性。缓存的概念最早来源于 CPU 中，为了提高 CPU 的处理速度，引入了 L1、L2、L3 三级高速缓存来加速访问，现在系统中使用的缓存也是借鉴了 CPU 中缓存的做法。

缓存是个非常大的话题，可以单独写一本书也毫不夸张，在这里总结一下我个人在运单系统设计和实现缓存的时候遇到的一些问题和解决方案。缓存主要分为本地缓存和分布式缓存，本地缓存如`Guava Cache`、`EHCache`等，分布式缓存如`Redis`、`Memcached`等，在运单系统中使用的主要以分布式缓存为主。

#### **2.1.1 如何保证缓存与数据库的数据一致性**

首先是如何保证缓存与数据库的数据一致性问题，基本在使用缓存的时候都会遇到这个问题，同时这也是个高频的面试题。在我负责的运单系统中使用缓存这个问题就更突出了，首先运单是会频繁更新的，并且运单系统对数据一致性的要求是非常高的，基本不太能接受数据不一致，所以不能简单的通过设置一个过期时间的方式来失效缓存。

关于缓存读写的模式推荐阅读耗子叔的文章：[缓存更新的套路](https://coolshell.cn/articles/17416.html)，里面总结了几种常用的读写缓存的套路，我在运单系统中的缓存读写模式也是参考了文章中的`Write through`模式，通过伪代码的方式大概是这样的：

```
lock(运单ID) { //...    // 删除缓存   deleteCache();    // 更新DB   updateDB();    // 重建缓存   reloadCache()}
```

既然是`Write through`模式，那对缓存的更新就是在写请求中进行的。首先为了防止并发问题，写请求都需要加分布式锁，锁的粒度是以运单 ID 为 key，在执行完业务逻辑后，先删除缓存，再更新 DB，最后再重建缓存，这些操作都是同步进行的，在读请求中先查询缓存，如果缓存命中则直接返回，如果缓存不命中则查询 DB，然后直接返回，也就是说在读请求中不会操作缓存，这种方式把缓存操作都收敛在写请求中，且写请求是加锁的，有效防止了读写并发导致的写入脏缓存数据的问题。

#### **2.1.2 缓存数据结构的设计**

缓存要避免大 key 和热 key 的问题。举个例子，如果使用`redis`中的`hash`数据结构，那就比普通字符串类型的 key 更容易有大 key 和热 key 问题，所以如果不是非要使用`hash`的某些特定操作，可以考虑把`hash`拆散成一个一个单独的 key/value 对，使用普通的`string`类型的 key 存储，这样可以防止`hash`元素过多造成的大 key 问题，同时也可以避免单`hash key`过热的问题。

#### **2.1.3 读写性能**

关于读写性能主要有两点需要考虑，首先是写性能，影响写性能的主要因素是 key/value 的数据大小，比较简单的场景可以使用`JSON`的序列化方式存储，但是在高并发场景下使用 JSON 不能很好的满足性能要求，而且也比较占存储空间，比较常见的替代方案有`protobuf`、`thrift`等等，关于这些序列化/反序列化方案网上也有一些性能对比，参考[thrift-protobuf-compare - Benchmarking.wiki](https://code.google.com/p/thrift-protobuf-compare/wiki/Benchmarking)。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131453327534287.png)

读性能的主要影响因素是每次读取的数据包的大小。在实践中推荐使用`redis pipeline`+批量操作的方式，比如说如果是字符串类型的 key，那就是`pipeline+mget`的方式，假设一次`mget`10 个 key，100 个`mget`为一批 pipeline，那一次网络 IO 就可以查询 1000 个缓存 key，当然这里具体一批的数量要看缓存 key 的数据包大小，没有统一的值。

#### **2.1.4 适当冗余**

适当冗余的意思是说我们在设计对外的业务查询接口的时候，可以适当的做一些冗余。这个经验是来自于当时我们在设计运单系统对外查询接口的时候，为了追求通用性，将接口的返回值设计成一个大对象，把运单上的所有字段都放在了这个大对象里面直接对外暴露了，这样的好处是不需要针对不同的查询方开发不同的接口了，反正字段就在接口里了，要什么就自己取。

这么做一开始是没问题的，但到我们需要对查询接口增加缓存的时候发现，由于所有业务方都通过这一个接口查询运单数据，我们没办法知道他们的业务场景，也就不知道他们对接口数据一致性的要求是怎么样的，比如能否接受短暂的数据一致性，而且我们也不知道他们具体使用了接口中的哪些字段，接口中有些字段是不会变的，有些字段是会频繁变更的，针对不同的更新频率其实可以采用不同的缓存设计方案，但很可惜，因为我们设计接口的时候过于追求通用性，在做缓存优化的时候就非常麻烦，只能按最坏的情况打算，也就是所有业务方都对数据一致性要求很高来设计方案，导致最后的方案在数据一致性这块花了大量的精力。

如果我们一开始设计对外查询接口的时候能做一些适当的冗余，区分不同的业务场景，虽然这样势必会造成有些接口的功能是类似的，但在加缓存的时候就能有的放矢，针对不同的业务场景设计不同的方案，比如关键的流程要注重数据一种的保证，而非关键场景则允许数据短暂的不一致来降低缓存实现的成本。同时在接口中最好也能将会更新的字段和不会更新的字段做一定的区分，这样在设计缓存方案的时候，针对不会更新的字段，可以设置一个较长的过期时间，而会更新的字段，则只能设置较短的过期时间，并且需要做好缓存更新的方案设计来保证数据一致性。

### 2.2 消息队列

在高并发系统的架构中，消息队列（MQ）是必不可少的，当大流量来临时，我们通过消息队列的异步处理和削峰填谷的特性来增加系统的伸缩性，防止大流量打垮系统，此外，使用消息队列还能使系统间达到充分解耦的目的。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131453455747891.png)

消息队列的核心模型由生产者（Producer）、消费者（Consumer）和消息中间件（Broker）组成。目前业界常用的开源解决方案有`ActiveMQ`、`RabbitMQ`、`Kafka`、`RocketMQ`和近年比较火的`Pulsar`，关于各种消息中间件的对比可以参考文章：[消息队列背后的设计思想](https://zhuanlan.zhihu.com/p/401416746)。

使用消息队列后，可以将原本同步处理的请求，改为通过消费 MQ 消息异步消费，这样可以减少系统处理的压力，增加系统吞吐量，关于如何使用消息队列有许多的分享的文章，这里我的经验是在考虑使用消息队列时要结合具体的业务场景来决定是否引入消息队列，因为使用消息队列后其实是增加了系统的复杂性的，原来通过一个同步请求就能搞定的事情，需要引入额外的依赖，并且消费消息是异步的，异步天生要比同步更复杂，还需要额外考虑消息乱序、延迟、丢失等问题，如何解决这些问题又是一个很大话题，天下没有免费的午餐，做任何架构设计是一个取舍的过程，需要仔细考虑得失后再做决定。

### 2.3 服务治理

服务治理是个很大的话题，可以单独拿出来说，在这里我也把它归到架构中。服务治理的定义是

> 一般指独立于业务逻辑之外，给系统提供一些可靠运行的系统保障措施。

常见的保障措施包括服务的注册发现、可观测性（监控）、限流、超时、熔断等等，在微服务架构中一般通过服务治理框架来完成服务治理，开源的解决方案包括`Spring Cloud`、`Dubbo`等。

在高并发的系统中，服务治理是非常重要的一块内容，相比于缓存、数据库这些大块的内容，服务治理更多的是细节，比如对接口的超时设置到底是 1 秒还是 3 秒，怎么样做监控等等，有句话叫细节决定成败，有时候就是因为一个接口的超时设置不合理而导致大面积故障的事情，我曾经也是见识过的，特别是在高并发的系统中，一定要注意这些细节。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131453581939339.png)

#### **2.3.1 超时**

对于超时的原则是：**一切皆有超时**。不管是 RPC 调用、Redis 操作、消费消息/发送消息、DB 操作等等，都要有超时。之前就遇到过依赖了外部组件，但是没有设置合理的超时，当外部依赖出现故障时，把服务所有的线程全部阻塞导致资源耗尽，无法响应外部请求，从而引发故障，这些都是“血”的教训。

除了要设置超时，还要设置合理的超时也同样重要，像上面提到的故障即使设置了超时，但是超时太久的话依然会因为外部依赖故障而把服务拖垮。如何设置一个合理的超时是很有讲究的，可以从是否关键业务场景、是否强依赖等方面去考虑，没有什么通用的规则，需要结合具体的业务场景来看。比如在一些 C 端展示接口中，设置 1 秒的超时似乎没什么问题，但在一些对性能非常敏感的场景下 1 秒可能就太久了，总之，需要结合具体的业务场景去设置，但无论怎么样，原则还是那句话：一切皆有超时。

#### **2.3.2 监控**

监控就是系统的眼睛，没有监控的系统就像一个黑盒，从外部完全不知道里面的运行情况，我们就无法管理和运维这个系统。所以，监控系统是非常重要的。系统的可观测性主要包含三个部分——`logging`、`tracing`、`metrics`。主要是使用的自研的监控系统，不得不说真的是非常的好用，具体的介绍可以参考：[饿了么 EMonitor 演进史](https://mp.weixin.qq.com/s?__biz=MzU4NzU0MDIzOQ==&mid=2247494222&idx=1&sn=143a8ad9e4da4bdf9e7a6c4c738e3bf2&scene=21#wechat_redirect)。在建设高并发系统时，我们一定要有完善的监控体系，包括系统层面的监控（CPU、内存、网络等）、应用层面的监控（JVM、性能等）、业务层面的监控（各种业务曲线等）等，除了监控还要有完善的报警，因为不可能有人 24 小时盯着监控，一旦有什么风险一定要报警出来，及时介入，防范风险于未然。

#### **2.3.3 熔断**

在微服务框架中一般都会内置熔断的特性，熔断的目的是为了在下游服务出故障时保护自身服务。熔断的实现一般会有一个断路器（`Crit Breaker`），断路器会根据接口成功率/次数等规则来判断是否触发熔断，断路器会控制熔断的状态在关闭-打开-半打开中流转。熔断的恢复会通过时间窗口的机制，先经历半打开状态，如果成功率达到阈值则关闭熔断状态。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131454117451056.png)

如果没有什么特殊需求的话在业务系统中一般是不需要针对熔断做什么的，框架会自动打开和关闭熔断开关。可能需要注意的点是要避免**无效的熔断**，什么是无效的熔断呢？在以前碰到过一个故障，是服务的提供方在一些正常的业务校验中抛出了不合理的异常（比如系统异常），导致接口熔断影响正常业务。所以我们在接口中抛出异常或者返回异常码的时候一定要区分业务和系统异常，一般来说业务异常是不需要熔断的，如果是业务异常而抛出了系统异常，会导致被熔断，正常的业务流程就会受到影响。

#### **2.3.4 降级**

降级不是一种具体的技术，更像是一种架构设计的方法论，是一种丢卒保帅的策略，核心思想就是在异常的情况下限制自身的一些能力，来保证核心功能的可用性。降级的实现方式有许多，比如通过配置、开关、限流等等方式。降级分为主动降级和被动降级。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131454545800964.png)

在电商系统大促的时候会把一些非核心的功能暂时关闭，来保证核心功能的稳定性，或者当下游服务出现故障且短时间内无法恢复时，为了保证自身服务的稳定性而把下游服务降级，这些都是主动降级。

被动降级指的是，比如调用了下游一个接口，但是接口超时了，这个时候为了让业务流程能继续执行下去，一般会选择在代码中`catch`异常，打印一条错误日志，然后继续执行业务逻辑，这种降级是被动的。

在高并发的系统中做好降级是非常重要的。举个例子来说，当请求量很大的时候难免有超时，如果每次超时业务流程都中断了，那么会大大影响正常业务，合理的做法是我们应该仔细区分强弱依赖，对于弱依赖采用被动降级的降级方式，而对于强依赖是不能进行降级的。降级与熔断类似，也是对自身服务的保护，避免当外部依赖故障时拖垮自身服务，所以，我们要做好充分的降级预案。

#### **2.3.5 限流**

关于限流的文章和介绍网上也有许多，具体的技术实现可以参考网上文章。关于限流我个人的经验是在设置限流前一定要通过压测等方式充分做好系统容量的预估，不要拍脑袋，限流一般来说是有损用户体验的，应该作为一种兜底手段，而不是常规手段。

### 2.4 资源隔离

资源隔离有各种类型，物理层面的服务器资源、中间件资源，代码层面的线程池、连接池，这些都可以做隔离。这里介绍的资源隔离主要是应用部署层面的，比如`Set化`等等。上文提到的异地多活也算是 Set 化的一种。

负责运单系统的期间也做过一些类似的资源隔离上的优化。背景是当时出遇到过一个线上故障，原因是某服务部署的服务器都在一个集群，没有按流量划分各自单独的集群，导致关键业务和非关键业务流量互相影响而导致的故障。因此，在这个故障后我也是决定对服务器做按集群隔离部署，隔离的维度主要是按业务场景区分，分为关键集群、次关键集群和非关键集群三类，这样能避免关键和非关键业务互相影响。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131455043589993.png)

### 2.5 小结

在架构方面，我个人也不是专业的架构师，也是一直在学习相关技术和方法论，上面介绍的很多技术和架构设计模式都是在工作中边学习边实践。如果说非要总结一点经验心得的话，我觉得是注重细节。个人认为架构不止高大上的方法论，技术细节也是同样重要的，正所谓细节决定成败，有时候忘记设置一个小小的超时，可能导致整个系统的崩溃。

## **3. 应用**

在高并发的系统中，在应用层面能做的优化也是非常多的，这部分主要分享关于补偿、幂等、异步化、预热等这几方面的优化。

### 3.1 补偿

在微服务架构下，会按各业务领域拆分不同的服务，服务与服务之前通过 RPC 请求或 MQ 消息的方式来交互，在分布式环境下必然会存在调用失败的情况，特别是在高并发的系统中，由于服务器负载更高，发生失败的概率会更大，因此补偿就更为重要。常用的补偿模式有两种：**定时任务模式**或者**消息队列模式**。

#### **3.1.1 定时任务模式**

定时任务补偿的模式一般是需要配合数据库的，补偿时会起一个定时任务，定时任务执行的时候会扫描数据库中是否有需要补偿的数据，如果有则执行补偿逻辑，这种方案的好处是由于数据都持久化在数据库中了，相对来说比较稳定，不容易出问题，不足的地方是因为依赖了数据库，在数据量较大的时候，会对数据库造成一定的压力，而且定时任务是周期性执行的，因此一般补偿会有一定的延迟。

#### **3.1.2 消息队列模式**

消息队列补偿的模式一般会使用消息队列中延迟消息的特性。如果处理失败，则发送一个延迟消息，延迟 N 分钟/秒/小时后再重试，这种方案的好处是比较轻量级，除了 MQ 外没有外部依赖，实现也比较简单，相对来说也更实时，不足的地方是由于没有持久化到数据库中，有丢失数据的风险，不够稳定。因此，我个人的经验是在关键链路的补偿中使用定时任务的模式，非关键链路中的补偿可以使用消息队列的模式。除此之外，在补偿的时候还有一个特别重要的点就是**幂等性**设计。

#### 3.1.3 幂等

幂等操作的特点是**其任意多次执行所产生的影响均与一次执行的影响相同**，体现在业务上就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为发起多次而产生副作用。在分布式系统中发生系统错误是在所难免的，当发生错误时，会使用重试、补偿等手段来提高容错性，在高并发的系统中发生系统错误的概率就更高了，所以这时候接口幂等就非常重要了，可以防止多次请求而引起的副作用。

幂等的实现需要通过一个唯一的业务 ID 或者 Token 来实现，一般的流程是先在 DB 或者缓存中查询唯一的业务 ID 或者 token 是否存在，且状态是否为已处理，如果是则表示是重复请求，那么我们需要幂等处理，即不做任何操作，直接返回即可。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131455233450688.png)

在做幂等性设计的时候需要注意的是并不是所有的场景都要做幂等，比如用户重复转账、提现等等，因为幂等会让外部系统的感知是调用成功了，并没有阻塞后续流程，但其实我们系统内部是没有做任何操作的，类似上面提到的场景，会让用户误以为操作已成功。所以说要仔细区分需要幂等的业务场景和不能幂等的业务场景，对于不能幂等的业务场景还是需要抛出业务异常或者返回特定的异常码来阻塞后续流程，防止引发业务问题。

#### 3.1.4 异步化

上文提到的消息队列也是一种异步化，除了依赖外部中间件，在应用内我们也可以通过线程池、协程的方式做异步化。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131455393067902.png)

关于线程池的实现原理，拿 Java 中线程池的模型来举例，核心是通过任务队列和复用线程的方式相配合来实现的，网上关于这些分享的文章也很多。在使用线程池或者协程等类似技术的时候，我个人的经验是有以下两点是需要特别注意的：

**关键业务场景需要配合补偿**

我们都知道，不管是线程池也好，协程也好，都是基于内存的，如果服务器意外宕机或者重启，内存中的数据是会丢失的，而且线程池在资源不足的时候也会拒绝任务，所以在一些关键的业务场景中如果使用了线程池等类似的技术，需要配合补偿一块使用，避免内存中数据丢失造成的业务影响。在我维护的运单系统中有一个关键的业务场景是入单，简单来说就是接收上游请求，在系统中生成运单，这是整个物流履约流量的入口，是特别关键的一个业务场景。

因为生成运单的整个流程比较长，依赖外部接口有 10 几个，所以当时为了追求高性能和吞吐率，设计成了异步的模式，也就是在线程池中处理，同时为了防止数据丢失，也做了完善的补偿措施，这几年时间入单这块基本没有出过问题，并且由于采用了异步的设计，性能非常好，那我们具体是怎么做的呢。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131455507094180.png)

总的流程是在接收到上游的请求后，第一步是将所有的请求参数落库，这一步是非常关键的，如果这一步失败，那整个请求就失败了。在成功落库后，封装一个 Task 提交到线程池中，然后直接对上游返回成功。后续的所有处理都是在线程池中进行的，此外，还有一个定时任务会定时补偿，补偿的数据源就是在第一步中落库的数据，每一条落库的记录会有一个 flag 字段来表示处理状态，如果发现是未处理或者处理失败，则通过定时任务再触发补偿逻辑，补偿成功后再将 flag 字段更新为处理成功。

**做好监控**

在微服务中像 RPC 接口调用、MQ 消息消费，包括中间件、基础设施等的监控，这些基本都会针对性的做完善的监控，但是类似像线程池一般是没有现成监控的，需要使用方自行实现上报打点监控，这点很容易被遗漏。我们知道线程池的实现是会有内存队列的，而我们也一般会对内存队列设置一个最大值，如果超出了最大值可能会丢弃任务，这时候如果没有监控是发现不了类似的问题的，所以，使用线程池一定要做好监控。那么线程池有哪些可以监控的指标呢，按我的经验来说，一般会上报线程池的**活跃线程数**以及**工作队列的任务个数**，这两个指标我认为是最重要的，其他的指标就见仁见智了，可以结合具体业务场景来选择性上报。

### 3.2 预热

> Warm Up。当系统长期处于低水位的情况下，流量突然增加时，直接把系统拉升到高水位可能瞬间把系统压垮。通过”冷启动”，让通过的流量缓慢增加，在一定时间内逐渐增加到阈值上限，给冷系统一个预热的时间，避免冷系统被压垮。

参考网上的定义，说白了，就是如果服务一直在低水位，这时候突然来一波高并发的流量，可能会一下子把系统打垮。系统的预热一般有 JVM 预热、缓存预热、DB 预热等，通过预热的方式让系统先“热”起来，为高并发流量的到来做好准备。预热实际应用的场景有很多，比如在电商的大促到来前，我们可以把一些热点的商品提前加载到缓存中，防止大流量冲击 DB，再比如 Java 服务由于 JVM 的动态类加载机制，可以在启动后对服务做一波压测，把类提前加载到内存中，同时还有可以提前触发 JIT 编译、Code cache 等等好处。

还有一种预热的思路是利用业务的特性做一些**预加载**，比如我们在维护运单系统的时候做过这样一个优化，在一个正常的外卖业务流程中是用户下单后到用户交易系统生成订单，然后经历支付->商家接单->请求配送这样一个流程，所以说从用户下单到请求配送这之间有秒级到分钟级的时间差，我们可以通过感知用户下单的动作，利用这时间差来提前加载一些数据。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131456025403670.png)

这样在实际请求到来的时候只需要到缓存中获取即可，这对于一些比较耗时的操作提升是非常大的，之前我们利用这种方式能提升接口性能 50%以上。当然有个点需要注意的就是如果对于一些可能会变更的数据，可能就不适合预热，因为预热后数据存在缓存中，后面就不会再去请求接口了，这样会导致数据不一致，这是需要特别注意的。

### 3.3 小结

在做高并发系统设计的时候我们总是会特别关注架构、基础设施等等，这些的确非常重要，但其实在应用层面能做的优化也是非常多的，而且成本会比架构、基础设施的架构优化低很多。很多时候在应用层面做的优化需要结合具体的业务场景，利用特定的业务场景去做出合理的设计，比如缓存、异步化，我们就需要思考哪些业务场景能缓存，能异步化，哪些就是需要同步或者查询 DB，一定要结合业务才能做出更好的设计和优化。

**规范**

这是关于建设高并发系统经验分享的最后一个部分了，但我认为规范的重要性一点都不比基础设施、架构、数据库、应用低，可能还比这些都更重要。根据二八定律，在软件的整个生命周期中，我们花了 20%时间创造了系统，但要花 80%的时间来维护系统，这也让我想起来一句话，有人说代码主要是给人读的，顺便给机器运行，其实都是体现了可维护性的重要性。

在我们使用了高大上的架构、做了各种优化之后，系统确实有了一个比较好的设计，但问题是怎么在后续的维护过程中防止架构腐化呢，这时候就需要规范了。

规范包括代码规范、变更规范、设计规范等等，当然这里我不会介绍如何去设计这些规范，我更想说的是我们一定要重视规范，只有在有了规范之后，系统的可维护性才能有保证。根据破窗理论，通过各种规范我们尽量不让系统有第一扇破窗产生。

## **4.总结**

说了这么多关于设计、优化的方法，最后想再分享两点。

第一点就是有句著名的话——“**过早优化是万恶之源**”，个人非常认同，我做的所有这些设计和优化，都是在系统遇到实际的问题或瓶颈的时候才做的，切忌不要脱离实际场景过早优化，不然很可能做无用功甚至得不偿失。

第二点是在设计的时候要遵循**KISS 原则**，也就是 Keep it simple, stupid。简单意味着维护性更高，更不容易出问题，正所谓大道至简，或许就是这个道理。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131456131232848.png)

以上这些都是我在工作期间维护高并发系统的一些经验总结，鉴于篇幅和个人技术水平原因，可能有些部分没有介绍的特别详细和深入，算是抛砖引玉吧。如果有什么说的不对的地方也欢迎指出，同时也欢迎交流和探讨。

作者：listenzhang，腾讯 PCG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/TTn3YNwKKWn5IS8F6HJHIg

# 【NO.54】15 年腾讯"老兵"谈技术人成长之路

每个职场人都会经历从职场新人到骨干、专家亦或是管理者的蜕变过程。作为技术职业人，大家常会碰到一些困惑，在不同职业发展阶段所需要具备的认知和专业能力差异在哪里？除了勤奋地敲代码，还有哪些方法可以加速成长？腾讯云架平总监 alexguo 以自己经历为蓝本，分享从毕业生新人到有经验者、骨干、专家一步步的历程，每一阶段的成长和烦恼，更与大家共同探讨内卷、35 岁之后的职业规划等当下内部热点，为所有可能看到这个分享的技术鹅们带来一些成长道路的启示亦或是感悟。

作为毕业后就一直在 TEG 的老鹅，这里结合我个人在公司成长晋升过程的思考，跟大家交流下，不一定都对，也不一定适合每个人，但还是希望带给各位的成长或者晋升有些帮助指引或启发。

## 1.技术人员升级之路

### 1.1 升级成长之路全景图

一个新人在 TEG 的成长过程我画了一幅非常简单的图，对应当前职级，通常 5~~6 级的阶段为新人，7~~8 级为有经验者，9~11 级是通常的骨干；12 级即为专家，专家向上可以继续 13/14 级…..发展，也可以向 TechLead 的路径上发展，TechLead 目前是部门甚至公司去提报且任命的，一般 TechLead 一个方向只有一个，是专家中的专家，所以全过程就是从新人-专家-TechLead，当然从骨干级别开始如果你足够的管理能力和有合适的机遇也可以往管理路线即 leader 上走也就是新人-骨干-leader。以下结合个人经历以及同事间的交流就大家遇到的一些普适性、共性的问题谈一下我的思考建议。

## 2.新人的成长与烦恼

### 2.1 新人的第一印象

好的第一印象融入是关键，一个从外部加入腾讯的体系或文化的新人，能够快速给别人留下好的第一印象是非常重要，这就需要快速融入，那如何做到呢？

首先，是能力。举个自己的例子。06 年我们一起的是 20 几个新人，全部被派到一个平台部工作（平台部里都是专家）培训和练习，最终部门仅留 10 个人。但我被留下来了，其实当时编程水平有限.记得当时题目，固定给你一个 IP 地址的二进制文件，里面 IP 地址会对应一个实际地址（哪个省哪个地方），要求写个程序即输入一个 IP 地址就显示出其真实地址，是比较简单的题目，当时的我也不能立即写出来，但完成后，部门检测时我发现其实题目考察的不是一个简单功能的实现，因为测试输入了很多非标准类 IP 的数据格式内容来测试，看系统是否有做出错处理等等；而当时我恰好设计了一块，针对各种条件做了很多限制，可能正是因为这样全面系统化设计让部门印象深刻，所以我被留下了。

这里想告诉大家，或者你能力足够强学东西很快；或你的细节做得好，或者你是一个非常积极主动的人，突出自己的优点给别人留下深刻的第一印象非常重要是你快速融入团队的第一点。

其次，让别人更容易地记住你。比如有些人取了很容易识别的名字，或者有共同的爱好，比如说打羽毛球、篮球等等。

最后，让自己忙起来。一个新人是做后台开发的，但进来发现没有后台开发工作可以做，怎么办，那不挑活先忙起来。我加入公司后曾在某业务系统做后台开发，做了一段时间发现没人写前端主页面，上面就派我去，Web 前端开发我基本没做过，但我接受了，而且还有一个很牛的前端同事来带我，还提供一些前端文档，所以当下我更觉得是个非常好的机会，虽然没有经验且与自己的长期发展不符，但是：

1）我了解前端开发；

2）让大家看到了我的学习能力和适应力，什么地方都能顶上。

也因此我收获了第一次 5 星。所以新人加入先不要挑活，而是先应该让自己忙起来。

再比如后台开发的代码有很多告警，先去看一下 warning 的情况怎么样，能不能去解决？如果是运营日报数据不准确，能不能做的更准确？如果有很多中间件的工具、脚本的建设工作，可能没有人指挥或安排你做，但如果你有精力，就用心做起来，这些都能够体现出你的价值，也能让你更快速成长，扩充知识体系，融入团队，所以让自己忙起来，去主动承担更多事情。

#### 2.2 新人的烦恼

**做杂事，不受重视**

有人说新人来了都是做杂事，写个运营系统或是做日报，或是就做周边的一些杂工作，总之新人似乎都不受重视。

但是我看来不是不受重视，而是在你不太了解研发团队情况下，安排一些相关的事情，这是建立信任的一个必要过程，当你逐步把每一件事情做好后，才能承担更多更重要的事情。所以能把杂事做好了，其实更能反映出你的能力。但如果你做运营系统一年了，同时已做得很好了，还在分配你做杂事，那你可以尝试去主动沟通能不能承担更多更重要的项目。我相信我们所有的团队都是希望每个人能更快地成长，所以如果你要求，一定会分配给你更多更难的项目去历练的。

**原来的代码是垃圾，坑多**

当看到原来的代码差、垃圾、坑多，但却不知道是否要重新做一套，怎么办？其实你认为的垃圾代码目前也在现网运行着，在后台服务着，所以先不要想着全部推翻，而是该思考如何用更好的代码逐步去替代。

首先要把自己想补在上面的东西做好，其次要把自己相关的文档逻辑理清楚，最后有选择地去做，同时保证好心态。目前互联网公司的整个研发体系和过程，包括做云市场和 To C 产品，都必须是先解决问题，把产品需求尽快先完成后，再去优化和完善质量等细节。

**考核不好，晋级延期**

低考核会经常给新人吗？从历史统计数据看，新人差考核相对较少，但往往由于新人比较脆弱，所以触动和反应就较大。如果你是新人考核成绩不理想需要先沉下气，分析判断，再主动找 leader，通过沟通真正认识到实际问题并找出不足。过往很多情况都是因为自己的认知和 leader 的认知不匹配导致的，员工认为，事情完成了，自己也有努力加班、学习；导师认为，员工工作交付延期，遇到问题不主动不积极请教，这种间隙认知导致考核结果低，所以月度改善计划能够弥补员工和导师之间的间隙，员工应主动跟导师进行月度沟通，对齐事务目标、完成时间以及具体工作内容，工作期间的沟通对双方都非常重要。

## 3.成为一个有经验者

### 3.1 有经验者-能负责部分模块/功能

新人大概经过一两年时间，就成了有经验者。如何定义有经验者？简单讲，就是能负责部分功能/模块，甚至整个体系。

如何算能负责？3 个指标：

1、快速定位问题，有短期和长期解决方案；

2、能够熟悉底层组件，推动运维和资源；

3、能够理解业务，在需求上与业务 PK，甚至引导业务发展。

怎样做到？还是 3 点：

1、多练：熟悉代码+运营；

2、多想：善于总结，不要等着被指点；

3、多学：类似的问题别人怎么做的。

### 3.2 有经验者-像海绵，快速吸收

新人们可能前一两年看不到太大的差距，但两年之后差距就会逐渐拉大，有的人一两年之后就能承担越来越重的角色，成长快速；有的人却在原地踏步。为什么？最大的区别就是个人的成长速度差异。刚毕业时都差不多，但在一两年工作后，会发现许多人在技术上面快速积累很多，比如已经可以研究整个系统架构方法，熟悉很多运营业务知识，在工作的成熟度上，能够主动去沟通分析，定时做汇报总结，同时个人的产出更加高效，心态也更加正能量，这样的同学 3~~4 年就可能到达 9 级，甚至 4~~5 年就可能到达 10 级。所以，在工作过程中要善于总结，积极吸收。

### 3.3 有经验者-对外沟通能力

有经验者都需要负责某些模块，所以沟通能力就非常重要。对外沟通最核心的是，在沟通中能否站在对方的立场思考，创造双赢。许多人总是站在自身立场去想问题，但其实更重要是应该站在对方立场思考，能帮对方解决什么问题，同时在沟通中需要注意语气和方式，可使用企业微信/邮件/电话/白板/会议等方式，尽量避免在企业微信中争吵，尽可能线下面对面对齐。

在腾讯 TEG 多年前就明确定义了“专业·服务·伙伴”价值观。如何理解“服务”？“服务”更多是用专业的能力，用乙方的心态服务伙伴，思考如何去配合支持甲方，然后和客户形成良好的伙伴关系，像我们做技术团队也是更多要服务业务部门或业务 BG 的，强大的专业加上良好的服务，才能够取得更大的成功。

### 3.4 有经验者的烦恼

**没挑战，重复工作多**

有经验者从事一块方向或模块，做久了都会遇到一些新问题，比如没挑战，重复工作多，没有成长。比如图片业务组做的用于用户上传图片时压缩保存的图片压缩模块，这项工作中常规需求是不同的业务配置不同参数和水印，使用的压缩库需要日常维护、扩缩容，维护这个模块 1 年或久了，就会感到没挑战，烦恼自然就来了，那么：

1）首先保证好心态，同时挖掘更简单的工作方法。比如做图像压缩模块，要追求更高的压缩率，原来使用 CPU 追求加速度，尝试用 GPU 或者 AI 去解决，去调研去推动，扩大自身的运作能力和视野；

2）思考在日常维护中精简人力，让自己有精力做其它事情。例如在需求配置中，页面、版本可以共同协作；在整个质量体系建设中，多跟团队沟通，分析成功率、失败率，建设计算资源池化、结合多个弹性资源做扩缩容，做到自动弹性扩缩容。

有经验者需要能够做更深入的思考。

**工作太多，没时间学习**

有经验者另一个烦恼是工作太多，没有时间学习想学的东西。但最好的学习方法是在工作和实践中学习，比如独自看书或小组结队学习；明确目标，有空就学，时间都是挤出来。

**故障和出错**

做互联网的后台开发团队或多或少都会遇到故障，大的项目都会出错，所以要学会在出错中成长，遇到故障，首先要有好的心态，不慌乱。在我负责在图片业务组时，也出现过一次蛮严重的故障，把整个传图系统搞挂了半个小时左右。出错似乎是必然的，但未来我们需要尽量减少出错，在出错中增长经验总结原因，把整个体系和能力建设好，得到成长。

如果所负责的业务遇到现网大故障怎么办？即时止损，判断等级，解决问题。第一件时间成立应急小组，纳入所有涉及的角色定位解决，缩短故障阶段和理清解决步骤时间，这就是处理故障的经验积累。

## 4.骨干的能力和烦恼

### 4.1 如何成为骨干？

**主导中大型项目**

从有经验者到 9 级后，这人就逐渐成长为骨干了。怎么判断骨干？就一句话：能够主导中大型项目。

什么叫主导？主导意味着技术方面你能够一切尽在掌握。其次，在自己的领域有一定程度上跨团队的影响力，比如做一个图片程序系统，该业务系统里出现各种问题故障我能去解决，知道该怎么做，包括未来如何发展都胸有成竹，同时对业务足够了解，能够影响到业务，或者是面对需求知道应该怎么做。

如何定义中大型项目？有两个维度，

1)项目的规模，

2)重要性。

一般来说做过 2-3 个中大的项目就已经不错了。所谓重要项目指该项目能不能在整个的中心或部门层面去总结提炼，对公司或业务都能产生重要影响意义的，这都是 9~11 级的同学应该做的事情。

**业务思维、敢于创新**

第一、必须要理解业务，

理解业务贴合业务就是深入地去做，比如为了做存储系统，我们甚至去了解微信客户端的后台传输过程到底做了哪些优化和措施，比如手机到基站的传输。比如如何建立站点，比如还做了很多并发传输和客户端通信的协议，思考如何在节假日期间降低视频的长度或者提高清晰度等等。只有真正深入业务，从用户体验出发，才能把系统做得更有竞争力，并且基于这个系统进一步成长。

第二、作为骨干要敢于舍弃包袱。比如之前做过因为解决痛点做了一些新技术，但是越做越发现这变成一个包袱，都在讲资源和内部的定义，后面大家快速转变最终做了其他。作为骨干，你要想有没有新的方向可以做？要不要更结合业务？方案上追求更加极致，我认为骨干必须要从这个层面去理解和思考。

第三，带领小团队的能力。作为 11 级，可能会去带一些毕业生或者级别低的同学，不管是 leader 或者 TechLead，一定是有这样的要求，你需要负责小团队，很多事不是一个人能够搞定的。怎么样把小团队带起来，也是这几点：你要有技术影响力，平时除了自己工作，还要多去想一想别人怎么一起做，要有工作规划能力，要写一些总结，要去了解别人的困难，要向你的 leader 或者总监甚至 GM 去寻求一些资源，这些都是你作为一个能够带团队的骨干要去面对和解决的问题。

### 4.2 骨干的烦恼

**杂事多，如何保持激情**

骨干可能面临一种问题，比如干扰太多，leader 老板总找你，或是常常被拉到很多跟你实际工作不太相关的群里等等；同时你还要考虑团队人员配合，工作进展，对外沟通，所以杂事变多，而写代码从事技术研究的时间会变少，那如何去保持激情？

我建议大家掌握这些小技巧，第一、合理安排时间；第二、抓住核心部分，不太核心的学会去放，哪些事情别人可以去做，那就把这个能做的人培养起来，作为骨干就是要让大家一起完成工作。

**空间不大，心累**

有些骨干觉得自己空间不大，向上晋升为专家好像也很难，遥遥无期……这可能是很多老鹅都遇到的问题。第一，是心态要放稳，你的工作不能因为这个想法出问题，只有工作做好的基础上才有发展，但如果因为心态影响工作和绩效，其实就更没有机会。第二，就是要等待机会，希望晋升为 leader 的需要时机。管理团队和技术骨干不太一样，管理需要机会，而且目前公司管理干部要求能上能下，只要你有才能就一定能够有机会晋升上去的。第三，公司职级也在逐步打开，所以公司在解决这个问题。最重要是心态要好，这是长跑的一个过程。

**诱惑多**

骨干会遇到很多诱惑。第一、要不要去创业？现在各种公司各种方向创业很多；第二、在要不要去业务 BG 试试？或者像外面有些大公司也挺好的……不可否认这个阶段大家都会或多或少面临这样的诱惑，怎么办？

我觉得这些事情没有好坏，只有是否适合自己，但我建议在做选择时，一定要非常谨慎的态度去问自己几个问题：你适不适合创业？你的能力是公司赋予的，还是你已经有足够能力带领团队？如果创业，你的家庭和个人能否应对创业风险？团队融合问题？你过去后能够给团队什么？….一定要想得非常清楚。我看到过往的创业公司，其实绝大部分人是过得很苦并且是失败的，其实成功的机会还是少的，所以要谨慎。

去业务 BG 怎么样？可能是一个选择，但我觉得也还是要谨慎。大家看到的都是很牛逼的业务。业务 BG 有好也有不好，但去到好的 BG 是否可以做的好，那真的因人而异，但很多时候大家都很难知道自己这个点能不能踩对。

还有业务 BG 多变化，其实压力是非常大的。因为毕竟业务要看收入，对技术的长期投入和太长远规划也会受限，如果想要去搏一把可以试试，但如果你是希望长期在技术上面有耕耘，一步一步精进，我觉得 TEG 还是非常合适的，无论从技术长期投入还是对个人成长的回报，我觉得是一个不错的地方。

也许这几年你加入了那些目前风头很劲的公司，但也可能入职即巅峰，进去之后能做的成、做的好么——不知道（当然某明星团队除外）。可能隔一段时间这个团队就解散了，要么就内部自己找，要么就离职，当然这里我并不是要求大家面对诱惑不为所动，而是在不同的选择下要好好考虑自己能力到底适合什么。

## **5.专家的影响力和烦恼**

### **5.1 专家的领域影响力**

成为骨干几年后可能有两条路：或者晋升为专家，或者晋升 Leader

晋升专家不是个人自己申请就可以，目前需要部门推荐，即部门评估出个人能力达到后，会提名申报专家。那如何获得部门提名呢？

1、有实际效果产出，即在部门有一定有影响力的项目或者主导或是重点参与部门长期的重要项目；

2、你的项目对公司产生了一定收益，增加收入、节省成本或提升效率，项目对内对外都有技术影响力，同时你的能力获得了团队的认可；

3、你影响到了公司或者行业，最重要一点是你的所在 BG 老板知不知道你，不知道你没关系，知不知道你所负责或者所做的这块工作内容，如果事情和人都不被人知道，那这里就有一些问题。

### **5.2 专家的烦恼**

第一、如何开辟新战场，专家需要有新想法，新创意，还要告诉 leader 你的新想法，技术上应该怎么做等等；第二、新想法如何获得总监或者 Leader 的支持，新创意我想的很详细了，很想做也觉得必须要做，但好像总监并不支持或者当前并不在乎，苦恼中。

比如设备编译器要去寻求资源，怎么去沟通呢？大部分的技术专家会觉得编译器很牛，这个技术很难……但其实从一个商业计划书的角度去谈会更合适。首先不是说技术难就应该去做，而是它的背景是什么、解决什么问题；其次，解决这个问题能产生多大的收益？再次，实现路径是怎样的；为什么我能做而不是别人做或者别的条件下不能做；最后，如果有别的团队或公司已经做成，目前结果如何，是否可以能够印证我做的这些规划可行性，当然以上这些内容可能会受技术专家的视野限制，所以专家们这里除了技术需要好好切磋交流外，可能日常也要从更高、更直接地引入一些商业计划书内容作为补充和拓展视野，解决这样受限的问题。

还有一点，很多专家的问题就是他宁愿去写代码，也不愿意分享，不愿意出去讲，去建设自己的影响力。而我们要求专家是需要有自己的影响力，所以心态要到位，也需要走出去发挥专家的影响力，进而带动团队整体的影响力的进一步提升。现在部门 HR、PR 的团队也提供了很多渠道和方法帮助大家提升和拓展自己的影响力。

## 6.当你成为 Leader

### **6.1 Leader 能做什么**

Leader 一是能搞定事情，二是能搞定人。

怎么搞定事？第一你必须有能力，能耕田，技术能力被认可的，组内的技术是精通的，有什么问题困难找你能解决；第二有前瞻性，能打猎；除了自己这摊事之外，还能看到周围的事，怎么样去打、获得更好的效果。

怎么搞定人?第一你对内团队管理能力要强，大家跟着你做事是服气的。第二对外你能搞定客户的。所以我觉得一个好的 leader 标准是说团队都是子弟兵，事情不要他操心。这里子弟兵的概念意思是说团队跟你是一条心的，你受到欺负，团队会一起帮你；不操心的理解是指你不在的情况下团队能不能帮你搞定，比如你休一周假团队工作是否照常，我觉得这也是一个行业的标准。

### **6.2 Leader 的烦恼**

人不好搞，团队激情不够，能力不匹配。我觉得核心就是讲真话，敢担当，不折腾，打胜仗、建口径、强信心，要锻炼团队打胜仗，自然大家就好搞。

事情不好搞，老是有别的团队想做我们的事情怎么办，还有不知道团队朝哪个方向走会更好？我觉得这里要做减法，做深入，仅仅是招募更多的专家和 TechLead 还不够，还要经常开一些务虚会，把组内的群智释放，帮助团队整体成长。

最后总结一个技术人员的成长，其实就是能力和意愿：能力就是通过不断地学习沟通，加强执行力和提升影响力；意愿是我们要有平常心，要有创业者的激情，去投入去深入，一步一步打胜仗，从新手逐渐完成到专家的蜕变。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131504092976917.png)

## 7.晋级难、35 岁退休、内卷？

### 7.1 晋级难，不知道怎么写 PPT。

在我们每年做晋级预评审时，发现一些共性的问题：1）晋级 PPT 不等于项目汇报。很多人都当作写项目汇报来准备，这是不对的。项目汇报是你向老板汇报，需要项目背景、收益，是一个大而全的概念，而且重点是成绩和收益。晋级 PPT 的核心是整个项目分析过程，有数据有分析，技术难度的量化体现，两者有差别。晋级 PPT 更多是我们怎么样去解决问题，难题是什么，解决问题中我有什么创新思路和高效方法，这才是最重要的。2）晋级更多是对前段工作的持续的沉淀和总结，功夫更多在平时，很多同学其实是晋级前专心致志写一个月的 PPT，但更多应该平时就要沉淀积累，靠后面一个月去集中或者突击，往往达不到很好的效果。

### 7.2 如何增加影响力？如何跨部门沟通？

首先，和其他团队的私交非常重要。记得之前一个团队的 leader 经常和外部团队开会，而他们的会议时间总定在上午 11 点或 11 点半，开完就会跟外团队一起吃饭再深度聊聊。这样挺好的，多和别的团队除了工作顺畅沟通外，一起吃饭这种实打实交流，也能起到非常好的效果。

其次，人脉很重要，在公司建立人脉越往后越重要。从骨干到专家到 leader，一个事情要去推动很需要寻求各种信息，这么大的公司是否能结识一些人非常重要。建立人脉第一点是争做子弟兵。比如你的 leader 现在面临的困难问题是什么？你怎么样能帮他们？我估计大部分人常常可能会忽略这个问题，都是有事情分配下来就把它完成做好，但你可以再多想一步，除此之外我还能做些什么？这点在对外合作，为合作伙伴着想也非常重要。还有积极创造沟通交流的机会，结识不同的人，比如和不同部门的同事约饭或喝咖啡聊聊新技术新点子，互通有无，也是建立人脉关系很好的方法。

### 7.3 怎么保持激情和冲劲？

所谓“剩”者为王，如果你一直抱有激情的坚持做一件事情，在你坚持的过程中可能浮躁的人熬不住就走了，而你不仅在坚持并且还不断有积累有沉淀有成长，那么坚持到最后你就是王者，剩下的才是真正厉害的。

### 7.4 内卷是一个最近非常流行的词

内卷怎么办？首先做减法。比如我们做新技术领域拓展，比如编码技术，在这一块我们是不是足够强，足够深入，业务了解的足够透，如果要做到这样就要做减法，该舍去的一定要舍。

其次是创新。举个计算加速的例子，彼时 AI 那么火，我们觉得是不是做点 AI 的事情，AI 这边的工程能力比较弱，能不能帮下忙，做了这件事让我们发现原来 AI 在工程方面是有很多优化并可以逐渐实现的。比如我们通过一些竞赛打榜、做训练加速提升技能。当然这里是基于你原有的一些积累，或者是一些机遇能够结合，逐渐成长。再比如 CDN 原来只做静态下载，然后视频发展起来了，也用了 CDN，再到逐步发展的编解码技术，你会发现它是一环一环去成长扩大的一个过程。现在公司在推开源协同也是一个很好的例子，参与主导一些事情，也能够防止内卷的情况。

最后谈下，共性或热点即大家都说的 35 岁退休。退休后是开滴滴还是干嘛？确实有很多人在担心这个问题，但大家不用太担心，因为从职业规划方向来看，有几个方向可以去做：

1、做专家。目前公司最牛的 15 级专家也是 40 多岁，如果你往专家方向发展，不用太担心你的年龄，只要提升技术能力，逐步往上做。之前团队招了一位专家快 60 岁了，应该快接近到中国退休的年龄了，但是人家还是扎根这个领域，对于美国来说 67 岁才算退休，我们 40 多岁就在担心这个问题，是不是有点早。

2、往行业专家团队的管理者方向发展，或者在内部升总监，或者去外部公司做 CTO。

3、深度去理解产品和用户的需求，一方面可以往产品业务方向发展，另一方面即使出去创业，也能做一番事情，所以出路还是蛮多的，其实互联网从业者也可以干到退休的。

可能十几年前互联网是一个新兴的行业，大家都是年轻人，现在这些人都年纪大了，怎样保持好的心态和健康的身体才是重要的。所以我每天早上都跑步锻炼身体，保持自己有好的精力，虽然也是工作 10 多年，但我觉得自己在技术能力上拼劲还是蛮强的。无论是谁自己不去提升能力深耕或拓展工作内容都可能会被淘汰。

最后，不管你是往专家还是去外部做 CTO 还是创业，都要把当下核心事情做好，先做短期计划，比如今年要达到什么目标，长期要相信整个大趋势，整个公司是在往前走的，互联网在往前走，再到未来面临各行各业的融合。你只要在这个行业里，把每一块事情做好，自然而然可以干下去。把每一块被安排的事情都做好，自然而然就成长起来，所以我觉得大家不用太担心职业生涯。

作者：alexguo，腾讯TEG技术总监&技术专家

原文地址：https://mp.weixin.qq.com/s/TB0GlR-3KevIZ5hQAWqiNA

# 【NO.55】分布式消息队列

## 1.消息队列的演进

分布式消息队列中间件是是大型分布式系统中常见的中间件。消息队列主要解决应用耦合、异步消息、流量削锋等问题，具有高性能、高可用、可伸缩和最终一致性等特点。消息队列已经逐渐成为企业应用系统内部通信的核心手段，使用较多的消息队列有 RabbitMQ、RocketMQ、ActiveMQ、Kafka、ZeroMQ、Pulsar 等，此外，利用数据库（如 Redis、MySQL 等）也可实现消息队列的部分基本功能。

### 1.1 基于 OS 的 MQ

单机消息队列可以通过操作系统原生的进程间通信机制来实现，如消息队列、共享内存等。比如我们可以在共享内存中维护一个双端队列：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131532255732025.png)

消息产出进程不停地往队列里添加消息，同时消息消费进程不断地从队尾有序地取出这些消息。添加消息的任务我们称为 producer，而取出并使用消息的任务，我们称之为 consumer。这种模式在早期单机多进程模式中比较常见， 比如 IO 进程把收到的网络请求存入本机 MQ，任务处理进程从本机 MQ 中读取任务并进行处理。

单机 MQ 易于实现，但是缺点也很明显：因为依赖于单机 OS 的 IPC 机制，所以**无法实现分布式**的消息传递，并且**消息队列的容量**也受限于单机资源。

### 1.2 基于 DB 的 MQ

即使用存储组件（如 Mysql 、 Redis 等）存储消息， 然后在消息的生产侧和消费侧实现消息的生产消费逻辑，从而实现 MQ 功能。以 Redis 为例， 可以使用 Redis 自带的 list 实现。Redis list 使用 lpush 命令，从队列左边插入数据；使用 rpop 命令，从队列右边取出数据。与单机 MQ 相比， 该方案至少满足了分布式， 但是仍然带有很多无法接受的缺陷。

- 热 key 性能问题：不论是用 codis 还是 twemproxy 这种集群方案，对某个队列的读写请求最终都会落到同一台 redis 实例上，并且无法通过扩容来解决问题。如果对某个 list 的并发读写非常高，就产生了无法解决的热 key，严重可能导致系统崩溃
- 没有消费确认机制：每当执行 rpop 消费一条数据，那条消息就被从 list 中永久删除了。如果消费者消费失败，这条消息也没法找回了。
- 不支持多订阅者：一条消息只能被一个消费者消费，rpop 之后就没了。如果队列中存储的是应用的日志，对于同一条消息，监控系统需要消费它来进行可能的报警，BI 系统需要消费它来绘制报表，链路追踪需要消费它来绘制调用关系……这种场景 redis list 就没办法支持了
- 不支持二次消费：一条消息 rpop 之后就没了。如果消费者程序运行到一半发现代码有 bug，修复之后想从头再消费一次就不行了。

针对上述缺点，redis 5.0 开始引入 stream 数据类型，它是专门设计成为消息队列的数据结构，借鉴了很多 kafka 的设计，但是随着很多分布式 MQ 组件的出现，仍然显得不够友好， 毕竟 Redis 天生就不是用来做消息转发的。

### **1.3** **专用分布式 MQ 中间件**

随着时代的发展，一个真正的消息队列，已经不仅仅是一个队列那么简单了，业务对 MQ 的吞吐量、扩展性、稳定性、可靠性等都提出了严苛的要求。因此，专用的分布式消息中间件开始大量出现。常见的有 RabbitMQ、RocketMQ、ActiveMQ、Kafka、ZeroMQ、Pulsar 等等。

## 2.消息队列设计要点

消息队列本质上是一个消息的转发系统， 把一次 RPC 就可以直接完成的消息投递，转换成多次 RPC 间接完成，这其中包含两个关键环节：

1.消息转储；

2.消息投递：时机和对象；

基于此，消息队列的整体设计思路是：

- 确定整体的数据流向：如 producer 发送给 MQ，MQ 转发给 consumer，consumer 回复消费确认，消息删除、消息备份等。
- 利用 RPC 将数据流串起来，最好基于现有的 RPC 框架，尽量做到无状态，方便水平扩展。
- 存储选型，综合考虑性能、可靠性和开发维护成本等诸多因素。
- 消息投递，消费模式 push、pull。
- 消费关系维护，单播、多播等，可以利用 zk、config server 等保存消费关系。
- 高级特性，如可靠投递，重复消息，顺序消息等， 很多高级特性之间是相互制约的关系，这里要充分结合应用场景做出取舍。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131532374105743.png)

### 2.1 MQ 基本特性

#### **2.1.1 RPC 通信**

MQ 组件要实现和生产者以及消费者进行通信功能， 这里涉及到 RPC 通信问题。消息队列的 RPC，和普通的 RPC 没有本质区别。对于负载均衡、服务发现、序列化协议等等问题都可以借助现有 RPC 框架来实现，避免重复造轮子。

#### 2.1.2 存储系统

存储可以做成很多方式。比如存储在内存里，存储在分布式 KV 里，存储在磁盘里，存储在数据库里等等。但归结起来，主要有持久化和非持久化两种。

持久化的形式能更大程度地保证消息的可靠性（如断电等不可抗外力），并且理论上能承载更大限度的消息堆积（外存的空间远大于内存）。但并不是每种消息都需要持久化存储。很多消息对于投递性能的要求大于可靠性的要求，且数量极大（如日志）。这时候，消息不落地直接暂存内存，尝试几次 failover，最终投递出去也未尝不可。常见的消息队列普遍两种形式都支持。

从速度来看，理论上，文件系统>分布式 KV（持久化）>分布式文件系统>数据库，而可靠性却相反。还是要从支持的业务场景出发作出最合理的选择。

#### 2.1.3 高可用

MQ 的高可用，依赖于 RPC 和存储的高可用。通常 RPC 服务自身都具有服务自动发现，负载均衡等功能，保证了其高可用。存储的高可用， 例如 Kafka，使用分区加主备模式，保证每一个分区内的高可用性，也就是每一个分区至少要有一个备份且需要做数据的同步。

#### 2.1.4 推拉模型

push 和 pull 模型各有利弊，两种模式也都有被市面上成熟的消息中间件选用。

1.慢消费

慢消费是 push 模型最大的致命伤，如果消费者的速度比发送者的速度慢很多，会出现两种恶劣的情况：

1.消息在 broker 的堆积。假设这些消息都是有用的无法丢弃的，消息就要一直在 broker 端保存。

2.broker 推送给 consumer 的消息 consumer 无法处理，此时 consumer 只能拒绝或者返回错误。

而 pull 模式下，consumer 可以按需消费，不用担心自己处理不了的消息来骚扰自己，而 broker 堆积消息也会相对简单，无需记录每一个要发送消息的状态，只需要维护所有消息的队列和偏移量就可以了。所以对于慢消费，消息量有限且到来的速度不均匀的情况，pull 模式比较合适。

2.消息延迟与忙等

这是 pull 模式最大的短板。由于主动权在消费方，消费方无法准确地决定何时去拉取最新的消息。如果一次 pull 取到消息了还可以继续去 pull，如果没有 pull 取到则需要等待一段时间重新 pull。

#### 2.1.5 消息投放时机

即消费者应该在什么时机消费消息。一般有以下三种方式：

1. 攒够了一定数量才投放。
2. 到达了一定时间就投放。
3. 有新的数据到来就投放。

至于如何选择，也要结合具体的业务场景来决定。比如，对及时性要求高的数据，可用采用方式 3 来完成。

#### 2.1.6 消息投放对象

不管是 JMS 规范中的 Topic/Queue，Kafka 里面的 Topic/Partition/ConsumerGroup，还是 AMQP（如 RabbitMQ）的 Exchange 等等， 都是为了维护消息的消费关系而抽象出来的概念。本质上，消息的消费无外乎点到点的一对一单播，或一对多广播。另外比较特殊的情况是组间广播、组内单播。比较通用的设计是，不同的组注册不同的订阅，支持组间广播。组内不同的机器，如果注册一个相同的 ID，则单播；如果注册不同的 ID(如 IP 地址+端口)，则广播。

例如 pulsar 支持的订阅模型有：

- Exclusive：独占型，一个订阅只能有一个消息者消费消息。
- Failover：灾备型，一个订阅同时只有一个消费者，可以有多个备份消费者。一旦主消费者故障则备份消费者接管。不会出现同时有两个活跃的消费者。
- Shared：共享型，一个订阅中同时可以有多个消费者，多个消费者共享 Topic 中的消息。
- Key_Shared：键共享型，多个消费者各取一部分消息。

通常会在公共存储上维护广播关系，如 config server、zookeeper 等。

### 2.2 队列高级特性

常见的高级特性有可靠投递、消息丢失、消息重复、事务等等，他们并非是 MQ 必备的特性。由于这些特性可能是相互制约的，所以不可能完全兼顾。所以要依照业务的需求，来仔细衡量各种特性实现的成本、利弊，最终做出最为合理的设计。

#### 2.2.1 可靠投递

如何保证消息完全不丢失？

直观的方案是，在任何不可靠操作之前，先将消息落地，然后操作。当失败或者不知道结果（比如超时）时，消息状态是待发送，定时任务不停轮询所有待发送消息，最终一定可以送达。但是，这样必然导致消息可能会重复，并且在异常情况下，消息延迟较大。

例如：

- producer 往 broker 发送消息之前，需要做一次落地。
- 请求到 server 后，server 确保数据落地后再告诉客户端发送成功。
- 支持广播的消息队列需要对每个接收者，持久化一个发送状态，直到所有接收者都确认收到，才可删除消息。

即对于任何不能确认消息已送达的情况，都要重推消息。但是，随着而来的问题就是消息重复。在消息重复和消息丢失之间，无法兼顾，要结合应用场景做出取舍。

#### 2.2.2 消费确认

当 broker 把消息投递给消费者后，消费者可以立即确认收到了消息。但是，有些情况消费者可能需要再次接收该消息（比如收到消息、但是处理失败），即消费者主动要求重发消息。所以，要允许消费者主动进行消费确认。

#### 2.2.3 顺序消息

对于 push 模式，要求支持分区且单分区只支持一个消费者消费，并且消费者只有确认一个消息消费后才能 push 另外一个消息，还要发送者保证发送顺序唯一。

对于 pull 模式，比如 kafka 的做法：

1. producer 对应 partition，并且单线程。
2. consumer 对应 partition，消费确认（或批量确认），单线程消费。

但是这样也只是实现了消息的分区有序性，并不一定全局有序。总体而言，要求消息有序的 MQ 场景还是比较少的。

## 3.Kafka

Kafka 是一个分布式发布订阅消息系统。它以高吞吐、可持久化、可水平扩展、支持流数据处理等多种特性而被广泛使用（如 Storm、Spark、Flink）。在大数据系统中，数据需要在各个子系统中高性能、低延迟的不停流转。传统的企业消息系统并不是非常适合大规模的数据处理，但 Kafka 出现了，它可以高效的处理实时消息和离线消息，降低编程复杂度，使得各个子系统可以快速高效的进行数据流转，Kafka 承担高速数据总线的作用。

### **3.1 kafka** **基础概念**

- **Broker**Kafka 集群包含一个或多个服务器，这种服务器被称为 broker。
- **Topic**Topic 在逻辑上可以被认为是一个 queue，每条消费都必须指定它的 Topic，可以简单理解为必须指明把这条消息放进哪个 queue 里。为了使得 Kafka 的吞吐率可以线性提高，物理上把 Topic 分成一个或多个 Partition，每个 Partition 在物理上对应一个文件夹，该文件夹下存储这个 Partition 的所有消息和索引文件。
- **Partition**Parition 是物理上的概念，每个 Topic 包含一个或多个 Partition。
- **Producer**负责发布消息到 Kafka broker。
- **Consumer**消息消费者，向 Kafka broker 读取消息的客户端。
- **Consumer Group**每个 Consumer 属于一个特定的 Consumer Group（可为每个 Consumer 指定 group name，若不指定 group name 则属于默认的 group）。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131532559881730.png)

一个典型的 kafka 集群包含若干 Producer，若干个 Broker（kafka 支持水平扩展）、若干个 Consumer Group，以及一个 zookeeper 集群。Producer 使用 push 模式将消息发布到 broker。consumer 使用 pull 模式从 broker 订阅并消费消息。多个 broker 协同工作，producer 和 consumer 部署在各个业务逻辑中。kafka 通过 zookeeper 管理集群配置及服务协同。

这样就组成了一个高性能的分布式消息发布和订阅系统。Kafka 有一个细节是和其他 mq 中间件不同的点，producer 发送消息到 broker 的过程是 push，而 consumer 从 broker 消费消息的过程是 pull，主动去拉数据。而不是 broker 把数据主动发送给 consumer。

Producer 发送消息到 broker 时，会根据 Paritition 机制选择将其存储到哪一个 Partition。如果 Partition 机制设置合理，所有消息可以均匀分布到不同的 Partition 里，这样就实现了负载均衡。如果一个 Topic 对应一个文件，那这个文件所在的机器 I/O 将会成为这个 Topic 的性能瓶颈，而有了 Partition 后，不同的消息可以并行写入不同 broker 的不同 Partition 里，极大的提高了吞吐率。

**Kafka 特点**

**优点：**

- 高性能：单机测试能达到 100w tps
- 低延时：生产和消费的延时都很低，e2e 的延时在正常的 cluster 中也很低
- 可用性高：replicate+ isr + 选举 机制保证
- 工具链成熟：监控 运维 管理 方案齐全
- 生态成熟：大数据场景必不可少 kafka stream

**不足：**

- 无法弹性扩容：对 partition 的读写都在 partition leader 所在的 broker，如果该 broker 压力过大，也无法通过新增 broker 来解决问题
- 扩容成本高：集群中新增的 broker 只会处理新 topic，如果要分担老 topic-partition 的压力，需要手动迁移 partition，这时会占用大量集群带宽
- 消费者新加入和退出会造成整个消费组 rebalance：导致数据重复消费，影响消费速度，增加延迟
- partition 过多会使得性能显著下降：ZK 压力大，broker 上 partition 过多让磁盘顺序写几乎退化成随机写

### **3.2 高吞吐机制**

#### 3.2.1 顺序存取

如果把消息以随机的方式写入到磁盘，那么磁盘首先要做的就是寻址，也就是定位到数据所在的物理地址，在磁盘上就要找到对应柱面、磁头以及对应的扇区；这个过程相对内存来说会消耗大量时间，为了规避随机读写带来的时间消耗，kafka 采用顺序写的方式存储数据。

#### 3.2.2 页缓存

即使是顺序存取，但是频繁的 I/O 操作仍然会造成磁盘的性能瓶颈，所以 kafka 使用了**页缓存**和**零拷贝**技术。当进程准备读取磁盘上的文件内容时， 操作系统会先查看待读取的数据是否在页缓存中，如果存在则直接返回数据， 从而避免了对物理磁盘的 I/O 操作；

如果没有命中， 则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存， 之后再将数据返回给进程。一个进程需要将数据写入磁盘， 那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在， 则会先在页缓存中添加相应的页， 最后将数据写入对应的页。被修改过后的页也就变成了脏页， 操作系统会在**合适的时间**把脏页中的数据写入磁盘， 以保持数据的 一 致性。

Kafka 中大量使用了页缓存， 这是 Kafka 实现高吞吐的重要因素之 一 。虽然消息都是先被写入页缓存，然后由操作系统负责具体的刷盘任务的， 但在 Kafka 中同样提供了同步刷盘及间断性强制刷盘(fsync),可以通过参数来控制。

**同步刷盘**能够保证消息的可靠性，避免因为宕机导致页缓存数据还未完成同步时造成的数据丢失。但是实际使用上，我们没必要去考虑这样的因素以及这种问题带来的损失，消息可靠性可以由多副本来解决，同步刷盘会带来性能的影响。

**页缓存的好处：**

- I/O Scheduler 会将连续的小块写组装成大块的物理写从而提高性能；
- I/O Scheduler 会尝试将一些写操作重新按顺序排好，从而减少磁头移动时间；
- 充分利用所有空闲内存（非 JVM 内存）；
- 读操作可以直接在 Page Cache 内进行，如果消费和生产速度相当，甚至不需要通过物理磁盘交换数据；
- 如果进程重启，JVM 内的 Cache 会失效，但 Page Cache 仍然可用。

#### 3.2.3 零拷贝

零拷贝技术可以减少 CPU 的上下文切换和数据拷贝次数。

**常规方式**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131533081506151.png)

应用程序一次常规的数据请求过程，发生了 4 次拷贝，2 次 DMA 和 2 次 CPU，而 CPU 发生了 4 次的切换。（DMA 简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情）

**零拷贝的方式**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131533196461277.png)

通过零拷贝优化，CPU 只发生了 2 次的上下文切换和 3 次数据拷贝。

#### 3.2.4 批量发送

Kafka 允许进行批量发送消息，先将消息缓存在内存中，然后一次请求批量发送出去，这种策略将大大减少服务端的 I/O 次数。

#### 3.2.5 数据压缩

Kafka 还支持对消息集合进行压缩，Producer 可以通过 GZIP 或 Snappy 格式对消息集合进行压缩，Producer 压缩之后，在 Consumer 需进行解压，虽然增加了 CPU 的工作，但在对大数据处理上，瓶颈在网络上而不是 CPU，所以这个成本很值得。

### **3.3 高可用机制**

#### 3.3.1 副本

Producer 在发布消息到某个 Partition 时，先通过 ZooKeeper 找到该 Partition 的 Leader，然后无论该 Topic 的 Replication Factor 为多少，Producer 只将该消息发送到该 Partition 的 Leader。Leader 会将该消息写入其本地 Log。

每个 Follower 都从 Leader pull 数据。这种方式上，Follower 存储的数据顺序与 Leader 保持一致。Follower 在收到该消息后，向 Leader 发送 ACK， 并把消息写入其 Log。一旦 Leader 收到了 ISR 中的所有 Replica 的 ACK，该消息就被认为已经 commit 了，Leader 将增加 HW 并且向 Producer 发送 ACK。

为了提高性能，每个 Follower 在接收到数据后就立马向 Leader 发送 ACK，而**非等到数据写入 Log 中**。因此，对于已经 commit 的消息，Kafka 只能保证它被存于多个 Replica 的内存中，而不能保证它们被持久化到磁盘中，也就不能完全保证异常发生后该条消息一定能被 Consumer 消费。Consumer 读消息也是从 Leader 读取，只有被 commit 过的消息才会暴露给 Consumer。Kafka Replication 的数据流如下图所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131533379600301.png)

对于 Kafka 而言，定义一个 Broker 是否“活着”包含两个条件：

- 一是它必须维护与 ZooKeeper 的 session（这个通过 ZooKeeper 的 Heartbeat 机制来实现）。
- 二是 Follower 必须能够及时将 Leader 的消息复制过来，不能“落后太多”。

Leader 会跟踪与其保持同步的 Replica 列表，该列表称为 ISR（即 in-sync Replica）。如果一个 Follower 宕机，或者落后太多，Leader 将把它从 ISR 中移除。这里所描述的“落后太多”指 Follower 复制的消息落后于 Leader 后的条数超过预定值或者 Follower 超过一定时间未向 Leader 发送 fetch 请求。Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。

完全同步复制要求所有能工作的 Follower 都复制完，这条消息才会被认为 commit，这种复制方式极大的影响了吞吐率（高吞吐率是 Kafka 非常重要的一个特性）。异步复制方式下，Follower 异步的从 Leader 复制数据，数据只要被 Leader 写入 log 就被认为已经 commit，这种情况下如果 Follower 都复制完都落后于 Leader，而如果 Leader 突然宕机，则会丢失数据。而 Kafka 的这种使用 ISR 的方式则很好的均衡了确保数据不丢失以及吞吐率。Follower 可以批量的从 Leader 复制数据，这样极大的提高复制性能（批量写磁盘），极大减少了 Follower 与 Leader 的差距。

一条消息只有被 ISR 里的所有 Follower 都从 Leader 复制过去才会被认为已提交。这样就避免了部分数据被写进了 Leader，还没来得及被任何 Follower 复制就宕机了，而造成数据丢失（Consumer 无法消费这些数据）。而对于 Producer 而言，它可以选择是否等待消息 commit。这种机制确保了只要 ISR 有一个或以上的 Follower，一条被 commit 的消息就不会丢失。

#### 3.3.2 故障恢复

**Leader** **故障**

leader 发生故障后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader 同步数据。**注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。**

Kafka 在 ZooKeeper 中动态维护了一个 ISR（in-sync replicas），这个 ISR 里的所有 Replica 都跟上了 leader，只有 ISR 里的成员才有被选为 Leader 的可能。在这种模式下，对于 f+1 个 Replica，一个 Partition 能在保证不丢失已经 commit 的消息的前提下容忍 f 个 Replica 的失败。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131533494685835.png)
LEO：每个副本最大的 offset。

HW：消费者能见到的最大的 offset，ISR 队列中最小的 LEO。

#### 3.3.3 故障

follower 发生故障后会被临时踢出 ISR 集合，待该 follower 恢复后，follower 会 读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步数据操作。等该 follower 的 LEO 大于等于该 partition 的 HW，即 follower 追上 leader 后，就可以重新加入 ISR 了。

### **3.4 扩展性**

由于 Broker 存储着特定分区的数据， 因此，不管是 Broker 还是分区的扩缩容，都是比较复杂的，属于典型的“有状态服务”扩缩容问题。接下来，我们看一下 Pulsar 是怎么针对 kafka 的不足进行优化的。

## 4.Pulsar

Apache Pulsar 是 Apache 软件基金会顶级项目，是下一代云原生分布式消息流平台，集消息、存储、轻量化函数式计算为一体。采用计算与存储分离架构设计，支持多租户、持久化存储、多机房跨区域数据复制，具有强一致性、高吞吐、低延时及高可扩展性等流数据存储特性。在消息领域，Pulsar 是第一个将存储计算分离云原生架构落地的开源项目。

### 4.1 **服务和存储分离**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131534018326616.png)

在 kafka 的基础上，把数据存储功能从 Broker 中分离出来，Broker 仅面向生产者、消费者提供数据读写能力，但其自身并不存储数据。而在 Broker 层下面使用 Bookie 作为存储层，承担具体的数据存储职责。在 Pulsar 中，broker 的含义和 kafka 中的 broker 是一致的，就是一个运行的 Pulsar 实例， 提供多个分区的读写服务。由于 broker 层不在承担数据存储职责，使得 Broker 层成为无状态服务。这样一来，Broker 的扩缩容就变得非常简单。

相比之下，服务存储集于一体的 Kafka 就非常难以扩容。

- Broker 和 Bookie 相互独立，方便实现独立的扩展以及独立的容错

- Broker 无状态，便于快速上、下线，更加适合于云原生场景

- 分区存储不受限于单个节点存储容量

- Bookie 数据分布均匀

  

  ### 4.2 **分片存储**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131534514179484.png)

1.在 Kafka 分区（Partition）概念的基础上，按照时间或大小，把分区切分成分片（Segment）。

2.同一个分区的分片，分散存储在集群中所有的 Bookie 节点上。

3.同一个分片，拥有多个副本，副本数量可以指定，存储于不同的 Bookie 节点。

### 4.3 Pulsar 性能

和 Kafka 一样，Pulsar 也使用了**顺序读写**和**零拷贝**等技术来提高系统的性能。

此外，Pulsar 还设计了分层缓存机制，在服务层和存储层都做了**分层缓存**，来提高性能。

- 生产者发送消息时，调用 Bookie 层写入消息时，同时将消息写入 broker 缓存中。
- 实时消费时（追尾读），首先从 broker 缓存中读取数据，避免从持久层 bookie 中读取，从而降低投递延迟。
- 读取历史消息（追赶读）场景中，bookie 会将磁盘消息读入 bookie 读缓存中，从而避免每次都读取磁盘数据，降低读取延时。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131535005431551.png)

### 4.4 Pulsar 扩展性

分片存储解决了分区容量受单节点存储空间限制的问题，当容量不够时，可以通过扩容 Bookie 节点的方式支撑更多的分区数据，也解决了分区数据倾斜问题，数据可以均匀的分配在 Bookie 节点上。

Broker 和 Bookie 灵活的容错以及无缝的扩容能力让 Apache Pulsar 具备非常高的可用性，实现了无限制的分区存储。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131535124682535.png)

#### 4.4.1 Broker扩展

在 Pulsar 中 Broker 是无状态的，可以通过增加节点的方式实现快速扩容。当需要支持更多的消费者或生产者时，可以简单地添加更多的 Broker 节点来满足业务需求。Pulsar 支持自动的分区负载均衡，在 Broker 节点的资源使用率达到阈值时，会将负载迁移到负载较低的 Broker 节点。新增 Broker 节点时，分区也将在 Brokers 中做平衡迁移，一些分区的所有权会转移到新的 Broker 节点。

#### 4.4.2 Bookie 扩展

存储层的扩容，通过增加 Bookie 节点来实现。通过资源感知和数据放置策略，流量将自动切换到新的 Apache Bookie 中，整个过程不会涉及到不必要的数据搬迁。即扩容时，不会将旧数据从现有存储节点重新复制到新存储节点。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131535212472705.png)

如图所示，起始状态有四个存储节点，Bookie1, Bookie2, Bookie3, Bookie4，以 Topic1-Part2 为例，当这个分区的最新的存储分片是 SegmentX 时，对存储层扩容，添加了新的 Bookie 节点，BookieX,BookieY，那么当存储分片滚动之后，新生成的存储分片， SegmentX+1,SegmentX+2，会优先选择新的 Bookie 节点（BookieX,BookieY）来保存数据。

### 4.5 Pulsar 可用性

#### 4.5.1 Broker容错

如下图，假设当存储分片滚动到 SegmentX 时，Broker2 节点失败。此时生产者和消费者向其他的 Broker 发起请求，这个过程会触发分区的所有权转移，即将 Broker2 拥有的分区 Topic1-Part2 的所有权转移到其他的 Broker(Broker3)。

由于数据存储和数据服务分离，所以新 Broker 接管分区的所有权时，它不需要复制 Partiton 的数据。新的分区 Owner（Broker3）会产生一个新的分片 SegmentX+1, 如果有新数据到来，会存储在新的分片 Segment x+1 上，不会影响分区的可用性。

**即当某个 Broker 实例故障时，整个集群的消息存储能力仍然完好。此时，集群只是丧失了特定分区的消息服务，只需要把这些分区的服务权限分配给其他 Broker 即可。**

注意，和 Kafka 一样， Pulsar 的一个分区仍然只能由一个 Broker 提供服务，否则无法保证消息的分区有序性。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131535375408643.png)

#### 4.5.2 **Bookie 容错**

如下图，假设 Bookie 2 上的 Segment 4 损坏。Bookie Auditor 会检测到这个错误并进行复制修复。Bookie 中的副本修复是 Segment 级别的多对多快速修复，BookKeeper 可以从 Bookie 3 和 Bookie 4 读取 Segment 4 中的消息，并在 Bookie 1 处修复 Segment 4。如果是 Bookie 节点故障，这个 Bookie 节点上所有的 Segment 会按照上述方式复制到其他的 Bookie 节点。

所有的副本修复都在后台进行，对 Broker 和应用透明，**Broker 会产生新的 Segment 来处理写入请求**，不会影响分区的可用性。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131537011754420.png)

### 4.6 Pulsar 其他特性

基于上述的设计特点，Pulsar 提供了很多特性。

#### 4.6.1 读写分离

Pulsar 另外一个有吸引力的特性是提供了读写分离的能力，读写分离保证了在有大量滞后消费（磁盘 IO 会增加）时，不会影响服务的正常运行，尤其是不会影响到数据的写入。读写分离的能力由 Bookie 提供，简单说一下 Bookie 存储涉及到的概念：

- Journals：Journal 文件包含了 Bookie 事务日志，在 Ledger (可以认为是分片的一部分) 更新之前，Journal 保证描述更新的事务写入到 Non-volatile 的存储介质上；
- Entry logger：Entry 日志文件管理写入的 Entry，来自不同 ledger 的 entry 会被聚合然后顺序写入；
- Index files：每个 Ledger 都有一个对应的索引文件，记录数据在 Entry 日志文件中的 Offset 信息。

Entry 的读写入过程下图所示，数据的写入流程：

- 数据首先会写入 Journal，写入 Journal 的数据会实时落到磁盘；
- 然后，数据写入到 Memtable ，Memtable 是读写缓存；
- 写入 Memtable 之后，对写入请求进行响应；
- Memtable 写满之后，会 Flush 到 Entry Logger 和 Index cache，Entry Logger 中保存了数据，Index cache 保存了数据的索引信息，然后由后台线程将 Entry Logger 和 Index cache 数据落到磁盘。

数据的读取流程：

- 如果是 Tailing read 请求，直接从 Memtable 中读取 Entry；
- 如果是 Catch-up read（滞后消费）请求，先读取 Index 信息，然后索引从 Entry Logger 文件读取 Entry。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131538123193289.png)
一般在进行 Bookie 的配置时，会将 Journal 和 Ledger 存储磁盘进行隔离，减少 Ledger 对于 Journal 写入的影响，并且推荐 Journal 使用性能较好的 SSD 磁盘，读写分离主要体现在：

- 写入 Entry 时，Journal 中的数据需要实时写到磁盘，Ledger 的数据不需要实时落盘，通过后台线程批量落盘，因此写入的性能主要受到 Journal 磁盘的影响；
- 读取 Entry 时，首先从 Memtable 读取，命中则返回；如果不命中，再从 Ledger 磁盘中读取，所以对于 Catch-up read 的场景，读取数据会影响 Ledger 磁盘的 IO，对 Journal 磁盘没有影响，也就不会影响到数据的写入。

所以，数据写入是主要是受 Journal 磁盘的负载影响，不会受 Ledger 磁盘的影响。另外，Segment 存储的多个副本都可以提供读取服务，相比于主从副本的设计，Apache Pulsar 可以提供更好的数据读取能力。

通过以上分析，Apache Pulsar 使用 Apache BookKeeper 作为数据存储，可以带来下列的收益：

- **支持将多个 Ledger 的数据写入到同一个 Entry logger 文件，可以避免分区膨胀带来的性能下降问题**
- **支持读写分离，可以在滞后消费场景导致磁盘 IO 上升时，保证数据写入的不受影响**
- **支持全副本读取，可以充分利用存储副本的数据读取能力**

#### 4.6.2 多种消费模型

Pulsar 提供了多种订阅方式来消费消息，分为三种类型：独占（Exclusive），故障切换（Failover）或共享（Share）。

- **Exclusive** **独占订阅** ：在任何时间，一个消费者组（订阅）中有且只有一个消费者来消费 Topic 中的消息。
- **Failover** **故障切换**：多个消费者（Consumer）可以附加到同一订阅。但是，一个订阅中的所有消费者，只会有一个消费者被选为该订阅的主消费者。其他消费者将被指定为故障转移消费者。当主消费者断开连接时，分区将被重新分配给其中一个故障转移消费者，而新分配的消费者将成为新的主消费者。发生这种情况时，所有未确认（ack）的消息都将传递给新的主消费者。
- **Share** **共享订阅**：使用共享订阅，在同一个订阅背后，用户按照应用的需求挂载任意多的消费者。订阅中的所有消息以循环分发形式发送给订阅背后的多个消费者，并且一个消息仅传递给一个消费者。

当消费者断开连接时，所有传递给它但是未被确认（ack）的消息将被重新分配和组织，以便发送给该订阅上剩余的剩余消费者。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131538312503417.png)

#### 4.6.3 多种 ACK 模型

消息确认（ACK）的目的就是保证当发生故障后，消费者能够从上一次停止的地方恢复消费，保证既不会丢失消息，也不会重复处理已经确认（ACK）的消息。在 Pulsar 中，每个订阅中都使用一个专门的数据结构–游标（Cursor）来跟踪订阅中的每条消息的确认（ACK）状态。每当消费者在分区上确认消息时，游标都会更新。

Pulsar 提供两种消息确认方法：

- 单条确认（Individual Ack），单独确认一条消息。被确认后的消息将不会被重新传递
- 累积确认（Cumulative Ack），通过累积确认，消费者只需要确认它收到的最后一条消息

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131612343684943.png)

上图说明了单条确认和累积确认的差异（灰色框中的消息被确认并且不会被重新传递）。对于累计确认，M12 之前的消息被标记为 Acked。对于单独进行 ACK，仅确认消息 M7 和 M12， 在消费者失败的情况下，除了 M7 和 M12 之外，其他所有消息将被重新传送。

作者：vincentchma，腾讯 IEG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/-MXA4T-ei_U5ewXUNZ0QdQ

# 【NO.56】MongoDB 基础浅谈

MongoDB 作为一款优秀的基于分布式文件存储的 NoSQL 数据库，在业界有着广泛的应用。下文对 MongoDB 的一些基础概念进行简单介绍。

### **1. MongoDB 特点**

- 面向集合存储：MongoDB 是面向集合的，数据以 collection 分组存储。每个 collection 在数据库中都有唯一的名称。
- 模式自由：集合的概念类似 MySQL 里的表，但它不需要定义任何模式。
- 结构松散：对于存储在数据库中的文档，不需要设置相同的字段，并且相同的字段不需要相同的数据类型，不同结构的文档可以存在同一个 collection 里。
- 高效的二进制存储：存储在集合中的文档，是以键值对的形式存在的。键用于唯一标识一个文档，一般是 ObjectId 类型，值是以 BSON 形式存在的。BSON = Binary JSON， 是在 JSON 基础上加了一些类型及元数据描述的格式。
- 支持索引：可以在任意属性上建立索引，包含内部对象。MongoDB 的索引和 MySQL 的索引基本一样，可以在指定属性上创建索引以提高查询的速度。除此之外，MongoDB 还提供创建基于地理空间的索引的能力。

- 支持 mapreduce：通过分治的方式完成复杂的聚合任务。
- 支持 failover：通过主从复制机制，可以实现数据备份、故障恢复、读扩展等功能。基于复制集的复制机制提供了自动故障恢复的功能，确保了集群数据不会丢失。
- 支持分片：MongoDB 支持集群自动切分数据，可以使集群存储更多的数据，实现更大的负载，在数据插入和更新时，能够自动路由和存储。
- 支持存储大文件：MongoDB 中 BSON 对象最大不能超过 16 MB。对于大文件的存储，BSON 格式无法满足。GridFS 机制提供了一个存储大文件的机制，可以将一个大文件分割成为多个较小的文档进行存储。

### **2. MongoDB 要素**

- database: 数据库。
- collection: 数据集合，相当于 MySQL 的 table。
- document: 数据记录行，相当于 MySQL 的 row。
- field: 数据域，相当于 MySQL 的 column。
- index: 索引。
- primary key: 主键。

### **3. MongoDB 数据库**

一个 MongoDB 实例可以创建多个 database。连接时如果没开启免认证模式的话，需要连接到 admin 库进行认证。如果开启免认证模式，若不指定 database 进行连接，默认连接一个叫 db 的数据库，该数据库存储在 data 目录中。通过 show dbs 命令可以查看所有的数据库。数据库名不能包含空字符。数据库名不能为空并且必须小于 64 个字符。

MongoDB 预留了几个特殊的 database。

- admin: admin 数据库主要是保存 root 用户和角色。例如，system.users 表存储用户，system.roles 表存储角色。一般不建议用户直接操作这个数据库。将一个用户添加到这个数据库，且使它拥有 admin 库上的名为 dbAdminAnyDatabase 的角色权限，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如关闭服务器。
- local: local 数据库是不会被复制到其他分片的，因此可以用来存储本地单台服务器的任意 collection。一般不建议用户直接使用 local 库存储任何数据，也不建议进行 CRUD 操作，因为数据无法被正常备份与恢复。
- config: 当 MongoDB 使用分片设置时，config 数据库可用来保存分片的相关信息。

一个 MongoDB 实例的数据结构如下图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131627505493955.png)

### **4. MongoDB 集合**

MongoDB 集合存在于数据库中，没有固定的结构，可以往集合插入不同格式和类型的数据。集合不需要事先创建。当第一个文档插入，或者第一个索引创建时，集合就会被创建。集合名必须以下划线或者字母符号开始，并且不能包含 $，不能为空字符串（比如 “”），不能包含空字符，且不能以 system. 为前缀。

capped collection 是固定大小的集合，支持高吞吐的插入操作和查询操作。它的工作方式与循环缓冲区类似，当一个集合填满了被分配的空间，则通过覆盖最早的文档来为新的文档腾出空间。和标准的 collection 不同，capped collection 需要显式创建，指定大小，单位是字节。capped collection 可以按照文档的插入顺序保存到集合中，而且这些文档在磁盘上存放位置也是按照插入顺序来保存的，所以更新 capped collection 中的文档，不可以超过之前文档的大小，以便确保所有文档在磁盘上的位置一直保持不变。

### **5. MongoDB 视图**

视图基于已有的集合进行创建，是只读的，不实际存储硬盘，通过视图进行写操作会报错。视图使用其上游集合的索引。由于索引是基于集合的，所以你不能基于视图创建、删除或重建索引，也不能获取视图的索引列表。如果视图依赖的集合是分片的, 那么视图也视为分片的。视图是实时计算并读取的。

### **6. MongoDB 索引**

MongoDB 支持丰富的索引方式。如果没有索引，读操作就必须扫描集合中的每个文档并筛选符合查询条件的记录。索引能够在很大程度上提高查询速度。

- 单字段索引：有三种方式，（1）在单个字段上创建索引；（2）在嵌入式字段上创建索引；（3）在内嵌文档上创建索引。
- 复合索引：支持在多个字段上匹配的查询。对任何复合索引施加 32 个字段的限制。对于复合索引，MongoDB 可以使用索引来支持对索引前缀的查询。
- 多键索引：为了索引包含数组值的字段，MongoDB 为数组中的每个元素创建一个索引键。这些多键索引支持对数组字段的高效查询。
- 文本索引：支持对字符串内容的文本搜索查询。文本索引可以包含任何值为字符串或字符串元素数组的字段。一个集合最多可以有一个文本索引。
- 通配符索引：支持针对未知或任意字段的查询。例如：db.collection.createIndex( {“a.$**” : 1 } ) 可支持诸如 db.collection.find({ “a.b” : 1 })、db.collection.find({ “a.c” : { $lt : 2 } }) 等查询，提高查询效率。不能使用通配符索引来分片集合。不能为通配符创建复合索引。
- 通配符文本索引：通配符文本索引不同于通配符索引。通配符索引不支持使用 $text操作符的查询。通配符文本索引为集合中每个文档中包含字符串数据的每个字段建立索引。索引的创建方式示例：db.collection.createIndex( { “$**”: “text” } )。
- 2dsphere 索引：支持球体上的地理空间查询：包含、相交和邻近度查询。
- hashed 索引：支持使用哈希的分片键进行分片。基于哈希的分片使用字段的散列索引作为分片键，以便跨分片集群对数据进行分区。MongoDB 支持任何单个字段的哈希索引，但不支持创建具有多个哈希字段的复合索引，也不能在索引上指定唯一哈希索引。
- ttl 索引：一种特殊的单字段索引，支持在一定的时间或特定的期限后自动从集合中删除文档。TTL 索引不能保证过期数据在过期时立即删除。默认每 60 秒运行一次删除过期文档的后台进程。capped collection 不支持 ttl 索引。
- 唯一索引：确保索引字段不会存储重复值。如果集合已经存在了违反索引的唯一约束的文档，则后台创建唯一索引会失败。
- 部分索引：只索引集合中满足指定筛选器表达式的文档。例如：db.collection.createIndex({ a:1 },{ partialFilterExpression: { b: { $lt: 100 } } }) 表示只对集合中 b 字段小于 100 的文进行索引，大于等于 100 的文档不会被索引。这可以有效提高存储效率。
- 稀疏索引：只包含有索引字段的文档的条目，即使索引字段包含空值。索引会跳过任何缺少索引字段的文档。非稀疏索引包含集合中的所有文档，为那些不包含索引字段的文档存储空值。

### **7. MongoDB ObjectId**

ObjectId 可以快速生成并排序，长度为 12 个字节，包括：

- 一个 4 字节的时间戳，表示 unix 时间戳
- 5 字节随机值
- 3 字节递增计数器，初始化为随机值

在 MongoDB 中，存储在集合中的每个文档都需要一个唯一的 _id 字段作为主键。如果插入的文档省略了 _id 字段，则自动为文档生成一个 _id。

### **8. MongoDB 复制集**

MongoDB 的复制集又称为副本集（Replica Set），是一组维护相同数据集合的 mongod 进程。复制集包含多个数据节点和一个可选的仲裁节点（arbiter）。在数据节点中，有且仅有一个成员为主节点（primary），其他节点为从节点（secondary）。

一个典型的复制集架构图如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131628061275259.png)

#### 8.1 复制集节点类型

- 主节点：接收所有的写操作，并将集合所有的变化记录到操作日志中，即 oplog。
- 从节点：通过复制主节点的操作来维护一个相同的数据集。从节点有几个选配项：v 参数决定是否具有投票权；priority 参数决定节点选主过程时的优先级；hidden 参数 决定是否对客户端可见；slaveDelay 参数表示复制 n 秒之前的数据，保持与主节点的时间差。从节点可以配置成 0 优先级，阻止它在选举中成为主节点，适用于将该节点部署在备用数据中心，或者将它作为一个冷节点；可以配置为隐藏复制集，防止应用程序从它读取数据，适用于在该节点上运行需要与正常流量分离的程序；可以配置为延迟复制集，保持一个历史快照，以便做按特定时间的故障恢复。
- 仲裁节点：如果将一个 mongod 实例作为仲裁节点添加到一个复制集中，该节点可以参与主节点选举，但不保存数据。仲裁节点永远只能是仲裁节点。

#### 8.2 复制集选主

MongoDB 的副本集协议（又称为 pv1)，是一种 raft-like 协议，即基于 raft 协议的理论思想实现，并且对之进行了一些扩展。当往复制集添加一个节点，或当主节点无法和集群中其他节点通信的时间超过参数 electionTimeoutMillis 配置的期限时，从节点会尝试通过 pv1 协议发起选举来推荐自己成为新主节点。

在选举前具有投票权的节点之间两两互相发送心跳，以侦测节点是否存活。复制集节点每两秒向彼此发送心跳。如果心跳未在 10 秒内返回，则发送心跳的一方将被发送方标记为不可访问，也就是说，默认当 5 次心跳未收到时判断为节点失联。如果失联的是主节点，从节点会发起选举，选出新的主节点；如果失联的是从节点则不会产生新的选举。选举基于 raft 一致性算法实现，在大多数投票节点存活下选举出主节点。只有能够与多数节点建立连接且具有较新的 oplog 的节点才可能被选举为主节点，如果集群里的节点配置了优先级，那么具有较高的优先级的节点更可能被选举为主节点。

复制集中最多可以有 50 个节点，但具有投票权的节点最多 7 个。

#### 8.3 复制集作用

- 主节点发生故障时自动选举出一个新的主节点，以实现 failover。
- 将数据从一个数据中心复制到另一个数据中心，减少另一个数据中心的读延迟。
- 实现读写分离。
- 实现容灾，可以在数据中心故障时快速切换到同城或异地的数据中心。

### **9. MongoDB 分片集**

MongoDB 支持通过分片技术来支持海量数据存储。解决数据增长的扩展方式有两种：垂直扩展和水平扩展。垂直扩展通过增加单个服务器的能力来实现，比如磁盘空间、内存容量、CPU 数量等；水平扩展则通过将数据存储到多个服务器上来实现。MongoDB 通过分片实现水平扩展。

一个典型的分片集群架构如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131628236472979.png)

#### 9.1 分片集组件

- shard：每个分片上可以保存一个集合的子集，所有分片上的子集的数据互不相交，构成完整的集合。每个分片可以被部署为复制集架构。最大为 1024 个分片。
- mongos：充当查询路由器，在客户端和分片集之间提供读写接口。mongos 提供集群单一入口，转发应用端请求，选择合适的数据节点进行读写，合并多个数据节点的返回。mongos 是无状态的，分片集群一般需要配置至少 2 个 mongos。
- config server：存储分片集的相关配置信息。

#### 9.2 分片键

MongoDB 集合若要采用分片，必须要指定分片键（shard key）。分片键由文档中的一个或多个字段组成。分片集合必须具有支持分片键的索引，索引可以是分片键的索引，也可以是以分片键是索引前缀的复合索引。要对已填充的集合进行分片，该集合必须具有以分片键开头的索引；分片一个空集合时，如果该集合还没有包含指定分片键的索引，则 MongoDB 会默认给分片键创建索引。

对于一个即将要分片的集合，如果该集合具有其他唯一索引，则无法分片该集合。

对于已分片的集合，不能在其他字段上创建唯一索引。

4.2 版本开始可以更改文档的分片键值，除非分片键字段为不可变的 _id 字段。更新分片键时必须在事务中或以可重试写入的方式在 mongos 上运行，不能直接在分片上执行操作。在此之前文档的分片键字段值是不可变的。

4.4 版本开始，可以向现有片键中添加一个或多个后缀字段以优化集合的片键。

5.0 版本开始，实现了实时重新分片（live resharding），可以实现分片键的完全重新选择。live resharding 机制下，数据将根据新的分片规则进行迁移，不过有一些限制，比如一个实例中有且只能有一个集合在相同的时间下 resharding 等。

数据库可以混合使用分片和未分片集合。分片集合被分区并分布在集群中的各个分片中。而未分片集合仅存储在主分片中。

设置 shard key 时应该充分考虑取值基数和取值分布。分片键应被尽可能多的业务场景用到。尽可能避免使用单调递增或递减的字段作为分片键。

#### 9.3 分片策略

MongoDB 将分片数据拆分成块。每个分块都有一个基于分片键的上下限范围 。分片策略包括哈希分片、范围分片和自定义 zone 分片。

- 哈希分片会计算分片键字段的哈希值，这个值被用作片键，然后根据哈希值的散列为每个块分配一个范围。
- 范围分片根据分片键的值将数据划分为多个连续范围。，然后基于分片键的值分配每个块的范围。当片键的基数大、频率低且值非单调变更时，范围分片更高效。
- 自定义 zone 分片基于 shard key 创建。每个 zone 与集群中的一个或者更多分片关联。一个分片可以和任意数目的非冲突 zone 相关联。

### **10. MongoDB 聚合**

MongoDB 聚合框架（Aggregation Framework）是一个计算框架，功能是：

- 作用在一个或几个集合上。
- 对集合中的数据进行的一系列运算。
- 将这些数据转化为期望的形式。

MongoDB 提供了三种执行聚合的方法：聚合管道，map-reduce 和单一目的聚合方法（如 count、distinct 等方法）。

#### 10.1 聚合管道

在聚合管道中，整个聚合运算过程称为管道（pipeline），它是由多个步骤（stage）组成的， 每个管道的工作流程是：

1. 接受一系列原始数据文档
2. 对这些文档进行一系列运算
3. 结果文档输出给下一个 stage

聚合计算基本格式如下：

```
pipeline = [$stage1, $stage2, ...$stageN];     db.collection.aggregate( pipeline, { options } )
```

#### 10.2 map-reduce

map-reduce 操作包括两个阶段：map 阶段处理每个文档并将 key 与 value 传递给 reduce 函数进行处理，reduce 阶段将 map 操作的输出组合在一起。map-reduce 可使用自定义 JavaScript 函数来执行 map 和 reduce 操作，以及可选的 finalize 操作。通常情况下效率比聚合管道低。

#### 10.3 单一目的聚合方法

主要包括以下三个：

- db.collection.estimatedDocumentCount()
- db.collection.count()
- db.collection.distinct()

### **11. MongoDB 一致性**

分布式系统有个 PACELC 理论。根据 CAP，在一个存在网络分区（P）的分布式系统中，要面临在可用性（A）和一致性（C）之间的权衡，除此之外（E），即使没有网络分区的存在，在实际系统中，我们也要面临在访问延迟（L）和一致性（C）之间的权衡。MongoDB 的一致性模型对读写操作 L 和 C 的选择提供了丰富的选项。

#### 11.1 因果一致性

单节点的数据库由于为读写操作提供了顺序保证，因此实现了因果一致性。分布式系统同样可以提供这些保证，但必须对所有节点上的相关事件进行协调和排序。

以下是一个不遵循因果一致性的例子：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131628407472316.png)

为了保持因果一致性，必须有以下保证：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131628485896142.png)

实现因果一致性的单号读写应遵循以下流程：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131629017847117.png)

为了建立复制集和分片集事件的全局偏序关系，MongoDB 实现了一个逻辑时钟，称为 lamport logical clock。每个写操作在应用于主节点时都会被分配一个时间值。这个值可以在副本和分片之间进行比较。从驱动到查询路由器再到数据承载节点，分片集群中的每个成员都必须在每条消息中跟踪和发送其最新时间值，从而允许分片之间的每个节点在最新时间保持一致。主节点将最新的时间值赋值给后续的写入，这为任何一系列相关操作创建了一个因果顺序。节点可以使用这个因果顺序在执行所需的读或写之前等待，以确保它在另一个操作之后发生。

从 MongoDB 3.6 开始，在客户端会话中开启因果一致性，保证 read concern 为 majority 的读操作和 write concern 为 majority 的写操作的关联序列具有因果关系。应用程序必须确保一次只有一个线程在客户端会话中执行这些操作。

对于因果相关的操作：

1. 客户端开启客户端会话，需满足以下条件：read concern 为 majority，数据已被大多数复制集成员确认并且是持久化的；write concern 为 majority，确认该操作已应用于复制集中大多数可投票成员。
2. 当客户端发出 read concern 为 majority 的读操作和 write concern 为 majority 的写操作的序列时，客户端将会话信息包含在每个操作中。
3. 对于与会话相关联的每个 read concern 为 majority 的读操作和 write concern 为 majority 的写操作，即使操作出错，MongoDB 也会返回操作时间和集群时间。
4. 相关的客户端会话会跟踪这两个时间字段。

#### 11.2 线性一致性

线性一致性又被称为强一致性。CAP 中的 C 指的就是线性一致性。顺序一致性中进程只关心各自的顺序一样就行，不需要与全局时钟一致。线性一致性是顺序一致性的进化版，要求顺序一致性的这种偏序（partial order）要达到全序（total order）。

在实现了线性一致性的系统中，任何操作在该系统生效的时刻都对应时间轴上的一个点。把这些时刻连接成一条线，则这条线会一直沿时间轴向前，不会反向。任何操作都需要互相比较决定发生的顺序。

以下是一个线性一致性的系统示例：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131629131655327.png)

在以上系统中，写操作生效之前的任何时刻，读取值均为 1，生效后均为 2。也就是说，任何读操作都能读到某个数据的最近一次写的数据。系统中的所有进程看到的操作顺序，都遵循全局时钟的顺序。

#### 11.3 read concern

read concern 是针对读操作的配置。它控制读取数据的新近度和持久性。read concern 选项控制数据读取的一致性，分为 local、available、majority、linearizable 四种，它们对一致性的承诺依次由弱到强。其中 linearizable 表示线性一致性，另外 3 种级别代表了 MongoDB 在实现最终一致性时，对访问延迟和一致性的取舍。

- local/available: 语义基本一致，都是读操作直接读取本地最新的数据，但不保证该数据已被写入大多数复制集成员。数据可能会被回滚。默认是针对主节点读。如果读取操作与因果一致的会话相关联，则针对副节点读。唯一的区别在于，avaliable 在分片集群场景下，为了保证性能，可能返回孤儿文档。
- majority：读取 majority committed 的数据，可以保证读取的数据不会被回滚，但是并不能保证读到本地最新的数据。受限于不同节点的复制进度，可能会读取到更旧的值。当写操作对应的 write concern 配置中 w 的值越大，则写操作在扩散到更多的复制集节点上之后才返回写成功，这时通过 read concern 被配置为 majority 的读操作进行读取数据，就有更大的概率读取到最新的数据。
- linearizable：读取 majority committed 的数据，但会等待在读之前所有的 majority committed 确认。它承诺线性一致性，要求读写顺序和操作真实发生的时间完全一致，既保证能读取到最新的数据，也保证读到数据不会被回滚。只对读取单个文档时有效，且可能导致非常慢的读，因此总是建议配合使用 maxTimeMS 使用。linearizable 只能用在主节点的读操作上，考虑到写操作也只能发生在主节点上，相当于说 MongoDB 的线性一致性被限定在单机环境下实现。实现 linearizable，读取的数据应该是被 write concern 为 majority 的写操作写入到 MongoDB 集群中的、且持久化到日志中的数据。如果数据写入到多数节点后，没有在日志中持久化，当这些节点发生重启恢复，那么之前通过配置 read concern 为 linearizable 的读操作读取到的数据就可能丢失。可以通过 writeConcernMajorityJournalDefault 选项保证指定 write concern 为 majority 的写操作在日志中是否持久化。如果写操作持久化到了日志中，但是没有复制到多数节点，在重新选主后，同样可能会发生数据丢失，违背一致性承诺。
- snapshot: 与关系型数据库中的快照隔离级别语义一致。最高隔离级别，接近于 serializable。是伴随着 MongoDB 4.0 版本中新出现的多文档事务而设计的，只能用在显式开启的多文档事务中。如果事务是因果一致会话的一部分，且 write concern 为 majority，则在事务提交后，读操作可以保证已从多数提交数据的快照中读取，该快照提供与该事务开始之前的操作的因果一致性。它读取 majority committed 的数据，但可能读不到最新的已提交数据。snapshot 保证在事务中的读不出现脏读、不可重复读和幻读。因为所有的读都将使用同一个快照，直到事务提交为止该快照才被释放。

下面借用一张图展示 majority 和 linearizable 的区别：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131629274488014.png)

#### 11.4 write concern

write concern 是针对写操作的配置，表示写请求对独立 mongod 实例或复制集或分片集进行写操作的确认级别。它主要是控制数据写入的持久性。包含三个选项：

- w：指定了写操作需要复制并应用到多少个复制集成员才能返回成功，可以为数字或 majority。

- - w:0 表示客户端不需要收到任何有关写操作是否执行成功的确认，就直接返回成功，具有最高性能。
  - w:1 表示写主成功则返回。
  - w: majority 需要收到多数节点（含主节点）关于操作执行成功的确认，具体个数由 MongoDB 根据复制集配置自动得出。w 值越大，对客户端来说，数据的持久性保证越强，写操作的延迟越大。w:1 要求事务只要在本地成功提交即可，而 w: majority 要求事务在复制集的多数派节点提交成功。
  - w:all 表示全部节点确认才返回成功。

- j：表示写操作对应的修改是否要被持久化到存储引擎日志中，只能选填 true 或 false。

- - j:false 表示写操作到达内存即算作成功。
  - j:true 表示写操作落到 journal 文件中才算成功。w:0 如果指定 j:true，则优先使用 j:true 来请求独立或复制集主副本的确认。j:true 本身并不能保证不会因复制集主故障转移而回滚写操作。
- wtimeout：主节点在等待足够数量的确认时的超时时间，单位为毫秒。超时返回错误，但并不代表写操作已经执行失败。跟 w 有关，比如：w 是 1，则是带主节点确认的超时时间；w 为 0，则永不返回错误；w 为 majority，表示多数节点确认的超时时间。

### **12. MongoDB WiredTiger 引擎**

从 3.2 版本开始，默认使用 WiredTiger 存储引擎，每个被创建的表和索引，都对应各自独立的 WiredTiger 表。为了保证 MongoDB 中数据的持久性，使用 WiredTiger 的写操作会先写入 cache，并持久化到 WAL（write ahead log），每 60s 或日志文件达到 2 GB，就会做一次 checkpoint，定期将缓存数据刷到磁盘，将当前的数据持久化产生一个新的快照。

#### 12.1 WiredTiger 数据结构

MongoDB 采用插件式存储引擎架构，实现了服务层和存储引擎层的解耦，可支持使用多种存储引擎。除此之外，底层的 WiredTiger 引擎还支持使用 B+ 树和 LSM 两种数据结构进行数据管理和存储，默认使用 B+ 树结构做存储。使用 B+ 树时，WiredTiger 以 page 为单位往磁盘读写数据，B+ 树的每个节点为一个 page，包含三种类型的 page，即 root page、internal page 和 leaf page。

以下是 B+ 树的结构示意图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131629374432906.png)

- root page 是 B+ 树的根节点。
- internal page 是不实际存储数据的中间索引节点。
- leaf page 是真正存储数据的叶子节点，包含页头（page header）、块头（block header）和真正的数据（key-value 对）。page header 定义了页的类型、页存储的记录条数等信息；块头定义了页的校验和 checksum、块在磁盘上的寻址位置等信息。真正的数据由一个 WT_ROW 结构的数组变量进行存储，每一条记录还有一个 cell_offset 变量，表示这条记录在 page 上的偏移量。WiredTiger 有一个用来为 page 分配 block 的块设备管理模块。定位文档位置时，先计算 block 的位置，通过 block 的位置找到它对应的 page，再通过 page 找到文档行数据的相对位置。leaf page 为了实现 MVCC，还会维护一个 WT_UPDATE 结构的数组变量，每条记录对应一个数组元素，每个元素是一个链表，将所有修改值以链表形式保存。

#### 12.2 WiredTiger 压缩

WiredTiger 支持在内存和磁盘上对索引进行压缩，通过前缀压缩的方式减少 RAM 的使用。

#### 12.3 WiredTiger 一致性原理

WiredTiger 使用了二级缓存 WiredTiger Cache 和 File System Cache 来保证 Disk 上 Database File 数据的最终一致性。

- WiredTiger Cache：通过 B+ 树缓存未压缩的数据，并通过淘汰算法确保内存占用在合理范围内。
- File System Cache：由操作系统管理，缓存压缩后的数据。
- Database File：存储压缩后的数据。每个 WiredTiger 表对应一个独立的磁盘文件。磁盘文件划分成多个按 4 KB 对齐的 extent，并通过 3 个链表来管理：available list（可分配的 extent 列表) ，discard list（废弃的 extent 列表）和 allocate list（当前已分配的 extent 列表）

#### 12.4 WiredTiger MVCC

WiredTiger 使用 MVCC 进行写操作，多个客户端可以并发同时修改集合的不同文档。事务开始时，WiredTiger 为操作提供反映内存数据的一致视图的时间点快照。MVCC 通过非锁机制进行读写操作，是一种乐观并发控制模式。WiredTiger 仅在全局、数据库和集合级别使用意向锁。当存储引擎检测到两个操作之间存在冲突时，将引发写冲突，从而导致 MongoDB 自动重试该操作。

使用 WiredTiger，如果没有 journal 记录，MongoDB 能且仅能从最后一个检查点恢复。如果需要恢复最后一次 checkpoint 之后所做的更改，那么开启日志是必要的。

### **13. MongoDB 数据读写**

#### 13.1 读偏好 ReadPerference

默认情况下，客户端读取复制集主节点上的数据。但客户端可以指定一个 read perference 改变读取行为，以便对复制集上的其他节点进行直接读操作。可选值包括：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131629487154583.png)

#### 13.2 在复制集上进行读写操作

读操作由客户端指定的 read prefenence 选项决定。

所有的写操作都在集合的主节点上执行。主节点执行写操作并将操作记录在操作日志或 oplog 上。oplog 是 local 数据库的一个集合，叫 local.oplog.rs。这是一个 capped collection，是固定大小，循环使用的。oplog 是对数据集的可重复操作序列，其记录的每个操作都是幂等的，也就是说，对目标数据集应用一次或多次 oplog 操作都会产生相同的结果。从节点从上一次结束时间点建立 tailable cursor，不断的从同步源拉取 oplog 并重放应用到自身，且严格按照原始的写顺序对给定的文档执行写操作。mongodb 使用多线程批量执行写操作来提高并发，根据文档 id 进行分批执行。MongoDB 为了提升同步效率，将拉取 oplog 以及重放 oplog 分到了不同的线程来执行。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131629594002586.png)

大致的写流程如下：

- producer thread 不断的从主节点上拉取 oplog，并把它加入到一个 blockQueue 里，blockQueue 不是无限容量的，当超过最大存储容量，producer thread 就必须等到 oplog 被 replBatcher thread 从队列里取出后才能继续拉取 oplog。
- replBatcher thread 不断从 producer thread 对应的 blockQueue 里取出 oplog，放到自己的内存队列里，内存队列也不是无限容量，一旦满了，就需要等待被 oplogApplication thread 消费。
- oplogApplication thread 不断取出 replBatch thread 内存队列里的所有元素，分散到不同的 replWriter thread，由 replWriter thread 根据 oplog 进行写操作。等待所有 oplog 都应用完毕，oplogApplication hread 将所有的 oplog 顺序写入到 local.oplog.rs 集合。

#### 13.3 在分片集群上进行读写操作

对于分片集群，需要一个 mongos 实例提供客户端应用程序和分片集群之间的接口。在客户端看来，该 mongos 实例的行为与其他 MongoDB 实例是相同的。客户端向路由节点 mongos 发送请求，由该节点决定往哪个分片进行读写。对于读取操作，若能定向到特定分片时，效率最高。一般而言，分片集合的查询应包含集合的分片键，以避免低效的全分片查询。在这种情况下，mongos 可以使用配置数据库 config 中的集群元数据信息，将查询路由到分片。如果查询不包含分片键，则 mongos 节点必须将查询定向到集群中的所有分片，然后在 mongos 上聚合所有分片的查询结果，返回给客户端。

对于写操作， mongos 定向到负责数据集特定部分的分片，config 数据库上有集合相关的分片键信息，mongos 从中读取配置，并路由写操作到适当的分片。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212131630085228763.png)

### **14. MongoDB 事务**

#### 14.1 ACID 特性

MongoDB 在一定程度上支持了事务的 ACID 特性。MongoDB 4.0 版本开始支持复制集上的多文档事务，4.2 版本引入了分布式事务，它增加了对分片群集上多文档事务的支持。

- 原子性：成功提交事务时，事务中所有数据更新将完全进行成功，并在事务外部可见。在提交事务之前，事务外部看不到在事务中进行的任何数据更新。当事务被打断或终止时，事务中进行的所有数据更新都将被丢弃，对事务外部完全不可见。但是当事务写入多个分片时，并非所有事务外的读操作都需要等待事务提交后所有分片上数据完全可见。
- 隔离性：MongoDB 提供 snapshot 隔离级别，在事务开始创建一个 WiredTiger snapshot，然后在整个事务过程中，便可以使用这个快照提供事务读。
- 持久性：事务使用 write concern 指定 {j: true} 时，MongoDB 会保证事务日志提交才返回，即使发生 crash，也能根据事务日志来恢复；而如果没有指定 {j: true} 级别，即使事务提交成功了，在故障恢复之后，事务的也可能被回滚掉。
- 一致性：参考前文提到的 MongoDB 一致性。

#### 14.2 事务的使用限制

- 仅 WiredTiger 引擎支持事务。
- 对集合的创建和删除操作，不能出现在事务中。
- 对索引的创建和删除操作，不能出现在事务中。
- 不能对系统级别的数据库和集合进行操作。
- 默认情况下，事务大小的限制在 16 MB。
- 默认情况下，事务操作整体不允许超过 60 秒。
- 事务不能在 session 外运行。
- 一个 session 只能运行一个事务，多个 session 可以并行运行事务。
- 不能对 capped collection 进行操作。
- 不能使用 explain 操作做查询分析。

#### 14.3 事务与 read concern

事务中的操作使用事务级别的 read concern。事务内部忽略在集合和数据库级别设置的任何 read concern。事务支持设置 read concern 为 local、majority 和 snapshot 其中之一。

- 当 read concern 为 local 时，可读取节点可用的最新数据，但数据可能回滚。对于分片群集上的事务，local 不能保证数据是从整个分片的同一快照视图获取。
- 当 read concern 为 majority 时，如果在提交事务时指定了 write concern 为 majority 级别，则返回大多数副本成员已确认的数据（即无法回滚数据）。如果事务未指定 write concern 为 majority 级别，则不保证读操作可以读取多数提交的数据。对于分片群集上的事务，不能保证数据是从整个分片的同一快照视图中获取。
- 当 read concern 为 snapshot 时，如果在提交事务时指定了 write concern 为 majority 级别，则从大多数已提交数据的快照中返回数据。如果事务未指定 write concern 为 majority 级别，则不保证读操作使用了 majority commited 的数据的快照。对于分片群集上的事务，snapshot 跨分片同步。

#### 14.4 事务与 write concern

事务使用事务级别的 write concern 来进行写操作提交，可以通过配置 w 选项设置节点个数，来决定事务写入是否成功，默认情况下为 1。

- w:0 表示事务写入不关注是否成功，默认为成功。
- w:1 表示事务写入到主节点就开始往客户端发送确认写入成功。
- w:majority 表示大多数节点成功原则，例如一个复制集 3 个节点，2 个节点成功就认为本次事务写入成功。
- w:all 表示所有节点都写入成功，才认为事务提交成功。
- j:false 表示写操作到达内存就算事务成功。
- j:true 表示写操作只有记录到日志文件才算事务成功。
- wtimeout: 写入超时时间，过期表示事务失败。

### **15. MongoDB Change Stream**

#### 15.1 变更流使用场景

MongoDB 3.6 引入了 change stream（变更流）。它的使用场景包括：

- 数据同步：多个 MongoDB 集群之间的增量数据同步。
- 审计：对 MongoDB 操作进行审计、监控。
- 数据订阅：外部程序订阅 MongoDB 的数据变更，可离线数据同步、计算或分析等。

#### 15.2 变更流特点

change stream 允许外部程序访问实时数据更改，而不会增加 MongoDB 基础操作的复杂性，也不会导致 oplog 延迟的风险。应用程序可以使用 change stream 来订阅单个集合、数据库或整个集群中的所有数据变更。若要开启 change stream，必须使用 WiredTiger 存储引擎。

change stream 可应用于复制集和分片集。应用于复制集时，可以在复制集中任意一个节点上开启监听；应用于分片集时，则只能在 mongos 上开启监听。在 mongos 上发起监听，是利用全局逻辑时钟提供了整个分片上变更的总体排序，确保监听事件可以按接收到的顺序安全地解释。mongos 会一直检查每个分片，查看每个分片是否存在最新的变更。如果多个分片上一直很少出现变更，则可能会对 change stream 的响应时间产生负面影响，因为 mongos 仍必须检查这些冷分片保持总体有序。

#### 15.3 变更流监听事件类型

从 change stream 中能监听到的变更事件包括：insert、update、replace、delete、drop、rename、dropDatabase 和 invalidate。

#### 15.4 变更流故障恢复

MongoDB 4.0 之后，可以通过指定 startAtOperationTime 来控制从某个特定的时间点开启监听，但该时间点必须在所选择节点的有效 oplog 时间范围内。change stream 监听返回的字段中有个 _id 字段，表示的是 resume token，这是唯一标志 change stream 流中的位置的字段。

如果 change stream 监听比中止后需要继续监听，那么可指定 resumeAfter 恢复订阅。指定 resumeAfter 为 change stream 中断处的 _id 字段即可。

当监听的集合发生 rename、drop 或 dropDatabase 事件，就会导致 invalidate 事件；当监听的数据库出现 dropDatabase 事件，也会导致无效事件。invalidate 事件后 change stream 的游标会被关闭，这时就需要使用 resumeAfter 选项来恢复 change stream 的监听，在 4.2 版本后也可以通过 startAfter 选项创建新的更改流来恢复监听。

#### 15.5 变更流使用限制

- change stream 无法配置到系统库或者 system.xxx 表上。
- change stream 依赖于 oplog，因此中断时间不可超过 oplog 回收的最大时间窗。

### **16. MongoDB 性能问题定位方式**

- 可以为 mongod 实例启用数据库分析。数据库分析器既可以在实例上启用，也可以在单个数据库层面上启用。它收集在实例上执行的 CRUD 操作、游标、命令、配置等详细信息，并将它收集的所有数据写到 system.profile 集合。这是一个 capped collection，默认情况下，system.profile 容量大小为 4M。开启实时数据库分析往往伴随着副作用，请谨慎使用。
- 使用 db.currentOp() 操作。它返回一个文档，其中包含有关数据库实例正在进行的操作的信息。
- 使用 db.serverStatus() 命令。它返回一个文档，提供数据库状态的概述，通过它可以收集有关该实例的统计信息。
- 使用 explain 来评估查询性能，例如 cursor.explain() 或 db.collection.explain() 方法可以用来返回关于查询执行的信息。
- 借用一些商业工具，比如 MongoDB Ops Manager、Percona 等。

作者：hazenweng，腾讯 QQ 音乐后台开发工程师

原文链接：https://mp.weixin.qq.com/s/CWjPigYjnREPXTiIRXI6MA

# 【NO.57】仅5天注册用户超百万的爆火ChatGPT是什么

## **0.导读**

OpenAI 近期发布聊天[机器人](https://wallstreetcn.com/markets/codes/300024.SZ)模型 ChatGPT，迅速出圈全网。它以对话方式进行交互。以更贴近人的对话方式与使用者互动，可以回答问题、承认错误、挑战不正确的前提、拒绝不适当的请求。高质量的回答、上瘾式的交互体验，圈内外都纷纷惊呼。

为什么有如此高的评价？理论支撑是什么？背后的技术原理是什么？待解决的问题和方案有哪些？资本怎么看待这件事？本文的目标是将这些问题详细的给大家讲清楚。

## **1. ChatGPT 是什么？**

ChatGPT 本质是一个应用在对话场景的语言模型，基于 GPT3.5 通过人类反馈的强化学习微调而来，能够回答后续问题、承认错误、质疑不正确的前提以及拒绝不适当的请求。首先让我们今天的主角 ChatGPT 来亲自介绍自己。

### **1.1 让 ChatGPT 介绍自己**

ChatGPT 是什么？既然 ChatGPT 是语言模型，语言模型是什么？通过强化学习训练，强化学习又是什么？

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141427382589228.png)

### **1.2 全球范围的兴起和爆发**

OpenAI 11 月 30 号发布，首先在北美、欧洲等已经引发了热烈的讨论。随后在国内开始火起来。全球用户争相晒出自己极具创意的与 ChatGPT 交流的成果。ChatGPT 在大量网友的疯狂测试中表现出各种惊人的能力，如流畅对答、写代码、写剧本、纠错等，甚至让记者编辑、程序员等从业者都感受到了威胁，更不乏其将取代谷歌搜索引擎之说。继 AlphaGo 击败李世石、AI 绘画大火之后，ChatGPT 成为又一新晋网红。下面是谷歌全球指数，我们可以看到火爆的程度。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141427489691879.png)

国内对比各大平台，最先火起来是在微信上，通过微信指数我们可以看到，97.48%来自于公众号，开始于科技圈，迅速拓展到投资圈等。我最先了解到 ChatGPT 相关信息的也是在关注的科技公众号上，随后看到各大公众号出现关于介绍 ChatGPT 各种震惊体关键词地震、杀疯了、毁灭人类等。随后各行各业都参与进来有趣的整活，问数学题，问历史，还有写小说，写日报，写代码找 BUG……

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141428154034805.png)

### **1.3 背后的金主 OpenAI**

OpenAI 是一个人工智能研究实验室，目的是促进和发展友好的人工智能，使人类整体受益。OpenAI 原是非营利机构，但为了更好地实现产研结合，2019 年 3 月成立 OpenAI LP 子公司，目的为营利所用。

2019 年 7 月[微软](https://zh.wikipedia.org/wiki/微軟)投资双方将携手合作，2020 年 6 月宣布了[GPT-3](https://zh.wikipedia.org/wiki/GPT-3)[语言模型](https://zh.wikipedia.org/wiki/語言模型)，刷新了人们对 AI 的认知。GPT 系列语言模型让我们不断对通用人工智能（AGI）充满了期待。

OpenAI 目标之初就很远大，解决通用人工智能问题，主要涉及强化学习和生成模型。

强化学习最早被认为是实现人类通用智能重要手段，2016 年 DeepMind 开发的 AlphaGo Zero 使用强化学习训练，让人类围棋的历史经验成为了「Zero」，标志着人类向通用型的人工智能迈出了重要一步。2019 年 OpenAI 在《Dota2》的比赛中战胜了人类世界冠军。OpenAI 在强化学习有很多深入的研究，Dactyl 也是一款 OpenAI 通过强化强化学习训练能够高精度操纵物体的机器人手，OpenAI Gym 是一款用于研发和比较强化学习算法的工具包，所以 ChatGPT 中使用强化学习也是顺理成章。

生成模型方面，为我们熟知的是 GPT-3，这是一个强大的语言模型能够生成人类一样流畅的语言。DellE 2 是最近很火的 AI 绘画根据文本描述生成图片。Codex 是和微软合作通过 github 代码训练而来，可以生成代码和 Debug，已经商业化。

作为 OpenAI 曾经的创始人[伊隆·马斯克](https://zh.wikipedia.org/wiki/伊隆·马斯克)也发表了对 ChatGPT 的评价！

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141428299782764.png)

看看 ChatGPT 是怎么介绍自家公司的？

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141428377750608.png)

## **2. ChatGPT 一些有趣的体验**

作为一个聊天机器人，我们体验发现相比传统的机器人在连贯性问答中更加流畅自然。微信上已经有很多的小程序可以体验，或者直接讲 ChatGPT 接入了微信中，下面体验一下已经接入到企业微信的 ChatGPT。

1）公司一向注重价值观，第一道题已经不符合公司价值观了，公司规定应第一时间退回，特殊情况无论价值多少都需要进行申报，所以 ChatGPT 应该入职不了我司。第二道经典问题的回答还蛮符合公司正直的价值观的，哈哈。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141428515250147.png)

2）公司协会活动总又人放鸽子，我来问问 ChatGPT 这些人啥心态。看到帮忙想的理由和放鸽子的人说的一样，我有点怀疑他们的心态了。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141429198637200.png)
3）职场工具人看看能不能通过 ChatGPT 代劳，可以看到文案写作还是不错的。回答的这个提纲有一点小问题但不影响大局。讲故事编剧本也是不错的，先帮中国足球写好参加世界杯的文案，就是不知道能不能用的上了。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141429338967354.png)

4）身边同事很重视娃的教育，那么从娃娃抓起先看看 ChatGPT 能不能带娃学习。文化常识题回答正确，数学题这推理能力，我担心娃考不上初中，可以用但是家长给把把关啊！同时也考察了一下他脑筋急转弯怎么样，这个傻瓜没答对。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141430097535495.png)

5）号称编程神器可写代码、修 bug，考察一下 Leetcode 中等难度的都没问题。虽然它自谦不会编程，但根据测试和网友的验证能力确实强。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141429594124050.png)

6）考察一下互联网知识储备，挑战失败！如 ChatGPT 自己所述，他还有很多局限性比如给出看起来没问题其实挺离谱的答案，例如回答自己公司的成果还夹杂私人感情，把竞争对手 DeepMind 的 AlphaGo 功劳都据为己有。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141430247288684.png)

做一个小结，其实网上有特别多有趣的案例，这里篇幅有限只是简单了列举几个。通过体验结合网友的反馈，ChatGPT 的确掌握了一些知识体系和回答技巧。我们看到相比传统的聊天机器人，ChatGPT 在连贯性问答中更加流畅自然，什么话都能接住。除了好玩的聊天神器外还有很多实用的价值，比如解答专业概念、编程类问题、从日常邮件、写请假条、广告文案等等，都可以通过 ChatGPT 代劳。看完这些有趣的案例，那么 ChatGPT 究竟如何实现的，我们接下来将讲解关于 ChatGPT 的哪些技术原理。

## **3. ChatGPT 之前技术沿袭**

ChatGPT 是基于 GPT3.5 语言模型，人类反馈的强化学习微调而来。本节将对涉及语言模型和强化学习两个重要技术做一个科普，已经熟悉的可直接跳过本节。

### **3.1 语言模型的技术演进**

语言模型通俗讲是判断这句话是否通顺、正确。数学函数表达为给定前 N 个词，预测第 N+1 个词概率，将概率序列分解成条件概率乘积的形式，这个函数就可以实现语言模型去生成句子。那么是什么样的语言模型如此强大，本小节梳理了深度学习开始的语言模型演技过程，如下图所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141430388858506.png)

第一次开始用神经网络做语言模型是 2003 年 Bengio 提出的 NNLM 的网络结构，随着图像领域预训练的取得的突破迅速迁移到 NLP 领域，有了我们熟知的 word2vec，通常做 NLP 任务句子中每个单词 Onehot 形式输入，使用预训练好的 word embedding 初始化网络的第一层，进行下游任务。word2vec 的弊端是 word embedding 静态的，后续代表性工作中 ELMo 通过采用双层双向 LSTM 实现了根据当前上下文对 Word Embedding 动态调整。

ELMo 非常明显的缺点在特征抽取器 LSTM 结构带来的，17 年 Google 在机器翻译 Transformer 取得了效果的突破，NLP 各种任务开始验证 Transformer 特征提取的能力比 LSTM 强很多。自此 NLP 开启了 Transformer 时代。

2018 年 OpenAI 采用 Transformer Decoder 结构在大规模语料上训练 GPT1 模型横扫了各项 NLP 任务，自此迈入大规模预训练时代 NLP 任务标准的预训练+微调范式。由于 GPT 采用 Decoder 的单向结构天然缺陷是无法感知上下文，Google 很快提出了 Encoder 结构的 Bert 模型可以感知上下文效果上也明显有提升。随后 2019 年 OpenAI 提出了 GPT2，GPT2 拥有和 GPT1 一样的模型结构，但得益于更高的数据质量和更大的数据规模有了惊人的生成能力。同年 Google 采用了 Encoder-Decoder 结构，提出了 T5 模型。从此大规模预训练语言模型兵分三路，开始了一系列延续的工作。

2020 年 OpenAI 提出 GPT3 将 GPT 模型提升到全新的高度，其训练参数达到了 1750 亿，自此超大模型时代开启。技术路线上摒弃了之前预训练+微调的范式，通过输入自然语言当作指示生成答案，开始了 NLP 任务新的范式预训练+提示学习。由于 GPT3 可以产生通顺的句子但是准确性等问题一直存在，出现 WebGPT、InstructGPT、ChatGPT 等后续优化的工作，实现了模型可以理解人类指令的含义，会甄别高水准答案，质疑错误问题和拒绝不适当的请求。

### **3.2 深度强化学习技术演进**

深度强化学习（deep reinforcement learning，DRL）是强化学习一个分支，基于深度学习强大的感知能力来处理复杂的、高维的环境特征，并结合强化学习的思想与环境进行交互，完成决策过程。DRL 在游戏场景这种封闭、静态和确定性环境可以达到甚至超越人类的决策水平。比较著名的事件是 2017 年 DeepMind 根据深度学习和策略搜索的 AlphaGo 击败了围棋世界冠军李世石。2018 年 OpenAI 团队基于多智能体 DRL 推出的 OpenAI Five 在 Dota2 游戏中击败了人类玩家。DRL 算法主要分为以下两类：

值函数算法：值函数算法通过迭代更新值函数来间接得到智能体的策略，智能体的最优策略通过最优值函数得到。基于值函数的 DRL 算法采用深度神经网络对值函数或者动作值函数进行近似，通过时间差分学习或者 Q 学习的方式分别对值函数或者动作值函数进行更新。代表性的是 2015 年 DeepMind 团队提出深度 Q 网络（DQN），及其后的各种变种 DDQN、Dueling DQN、分布式 DQN 等。

策略梯度算法：策略梯度算法直接采用函数近似的方法建立策略网络，通过策略网络选取动作得到奖励值，并沿梯度方向对策略网络参数进行优化，得到优化的策略最大化奖励值。可以用来处理连续动作。在实际应用中流行的做法是将值函数算法和策略梯度算法结合得到的执行器‒评价器（AC）结构。代表性工作有策略梯度算法、AC 算法以及各种变种 DDPG、A3C、PPO 等。ChatGPT 使用的就是策略梯度算法 PPO。

## **4. ChatGPT 背后的技术原理**

ChatGPT 整体技术方案是基于 GPT-3.5 大规模语言模型通过人工反馈强化学习来微调模型，让模型一方面学习人的指令，另一方面学习回答的好不好。

本节首先阐述 ChatGPT 提升的效果及背后对应的技术，然后介绍 ChatGPT 的整体训练流程，其次介绍提升涉及几个技术细节。

### **4.1 核心提升了什么？**

ChatGPT 在对话场景核心提升了以下三方面：

1）更好的理解用户的提问，提升模型和人类意图的一致性，同时具备连续多轮对话能力。

2）大幅提升结果的准确性，主要表现在回答的更加的全面，同时可以承认错误、发现无法回答的问题。

3）具备识别非法和偏见的机制，针对不合理提问提示并拒绝回答。

ChatGPT 的提升主要涉及以下三方面技术：

1）性能强大的预训练语言模型 GPT3.5，使得模型具备了博学的基础。

2）webGPT 等工作验证了监督学习信号可大幅提升模型准确性。

3）InstructGPT 等工作引入强化学习验证了对齐模型和用户意图的能力。

### **4.2 整体技术流程**

ChatGPT 的训练过程分为微调 GPT3.5 模型、训练回报模型、强化学习来增强微调模型三步：
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141430535690780.png)

第一步：微调 GPT3.5 模型。让 GPT 3.5 在对话场景初步具备理解人类的的意图，从用户的 prompt 集合中采样，人工标注 prompt 对应的答案，然后将标注好的 prompt 和对应的答案去 Fine-tune GPT3.5，经过微调的模型具备了一定理解人类意图的能力。

第二步：训练回报模型。第一步微调的模型显然不够好，至少他不知道自己答的好不好，这一步通过人工标注数据训练一个回报模型，让回报模型来帮助评估回答的好不好。具体做法是采样用户提交的 prompt，先通过第一步微调的模型生成 n 个不同的答案，比如 A、B、C、D。接下来人工对 A、B、C、D 按照相关性、有害性等标准标准并进行综合打分。有了这个人工标准数据，采取 pair-wise 损失函数来训练回报模型 RM。这一步实现了模型判别答案的好坏。

第三步：强化学习来增强微调模型。使用第一步微调 GPT3.5 模型初始化 PPO 模型，采样一批和前面用户提交 prompt 不同的集合，使用 PPO 模型生成答案，使用第二步回报模型对答案打分。通过产生的策略梯度去更新 PPO 模型。这一步利用强化学习来鼓励 PPO 模型生成更符合 RM 模型判别高质量的答案。

通过第二和第三步的迭代训练并相互促进，使得 PPO 模型能力越来越强。

### **4.3 主要涉及的技术细节**

#### 4.3.1 GPT3.5 理解能力提升

ChatGPT 是在 GPT3.5 模型技术上进行微调的，这里对 GPT-3.5 在 GPT3 基础上做的工作进行梳理，官方列举了以下 GPT-3.5 系列几个型号：

code-davinci-002 是一个基础模型,对于纯代码补全任务。这也是 ChatGPT 具备超强代码生成能力的原因。

text-davinci-002 是在 code-davinci-002 基础上训练的 InstructGPT 模型，训练策略是 instructGPT+FeedRM。

text-davinci-003 是基于 text-davinci-002 模型的增强版本，训练策略是 instructGPT+PPO。

根据如下图官方发布的模型时间线和文档，我们可以了解到 ChatGPT 是在 text-davinci-003 基础上微调而来，这也是 ChatGPT 模型性能如此强大的核心要素。因为 GPT-3.5 系列模型是在 2021 年第四季度之前的文本和代码样本上训练，所以我们体验 ChatGPT 时候同样无法回答训练样本日期之后的问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141431044271116.png)

#### 4.3.2 监督信号提升效果显著

GPT3 之前在预训练+微调已经是 NLP 任务中标准范式，GPT3 模型的训练是纯自监督学习并以 API 的形式发布，用户不具备微调的能力，官方也是主打预训练+提示学习的能力。Prompt 方法本质是挖掘语言模型本身具备的知识，恰当的提示去激发语言模型的补全能力。监督信号微调可以理解为改变了语言模型的理解能力，InstructGPT 的工作可以理解为对 GPT3-SFT 做了数据增强提升，使得模型在理解人类指令方面更出色。但这并不影响监督信号对最终效果的价值。

在 InstructGPT 的工作中，我们可以看到 GPT3-SFT 和 InstructGPT 在毒性、幻觉、理解客户能力上，监督学习微调已经和强化学习对比有很大的竞争力，甚至在幻觉角度比基于强化学习的 InstructGPT 提升很明显。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141431141683990.png)

#### 4.3.3 人类反馈强化微调效果

ChatGPT 通过人类反馈强化学习（RLHF）来让模型理解人类的指令。人类反馈强化学习（RLHF）是 DeepMind 早期提出的，使用少量的人类反馈来解决现代 RL 任务。RLHF 的思想在很多工作中都有体现，例如 OpenAI 的 webGPT、DeepMind 中 Sparrow 等都通过人类的反馈进一步提升大模型的效果。

RLHF 整个训练过程如下图所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141431256092487.png)

目标是实现后空翻的任务，智能体 Agent 在环境中随机行动，每隔一段时间，两个行为的视频片段给一个人，人判断两个视频哪个更接近目标。通过人的反馈数据，学习一个最能解释人类判断的奖励模型 Reward Model，然后使用 RL 来学习如何实现目标。随着人类继续提供模型无法判断时候的反馈，实现了进一步完善它对目标的理解。智能体 Agent 从人类反馈中学习最终在许多环境中有时甚至是超过人类的表现。

### **4.4 行动驱动的大语言模型**

尽管学术界一直无法真正定义 AGI，今年大型语言模型（LLM）的表现让我们对通用人工智能有了期待，通过 OpenAI 的 ChatGPT、Google 的 PaLM、DeepMind 的 Sparrow 取得的成功，人工智能的未来应该是行动驱动的，一个行动驱动的 LLM 看起来很像 AGI，如下图所示。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141431407043101.png)

模型的行为就像一个智能体 Agent 选择行动。在中间，我们有开箱即用的基础模型 LLM。用户通过 Prompt 询问模型结果。

左边是外部可利用的资源，这些可以是任何将文本作为输入并提供文本作为输出的函数，包括搜索、数据库、代码解释器和与人聊天等，它可以增强模型的能力。

右边是我们有任务导向的训练，如 instruction tuning、RLHF 等。instruction tuning 相对好实现，RLHF 需要调整 PPO 算法相对较难。整体上 RL 利用使用日志等专有数据，通过创建强大的反馈回路，训练模型使其更加符合任务需求并迭代优化。

## **5 总结与展望**

### **5.1 技术创新：待解决问题和改进**

ChatGPT 一个问题是只能回答 2021 年前的问题。模型无法获取近期的知识，将 ChatGPT+webGPT 结合是一个可以想到的方案。DeepMind 提出的 Sparrow 就是一个参考，Sparrow model 针对对话内容模型生成结果并判断是否搜索互联网,以提供更多的正确参考答案，用强化学习算法去优化 Sparrow 的输出结果。整体流程如下图所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141431517741178.png)

### **5.2 技术应用：能否取代搜索引擎**

应该不会取代，根据目前体验的效果，距离搜索引擎还有很长的路要走，主要基于几个方面。

首先 ChatGPT 本质是语言模型，当前的训练技术模型不具备或者说很弱的推理能力，一些推理问题比如小学生问题完败。根据当前体验看擅长创作类文案，其他问题经常出现一些事实错误情况。而搜索引擎技术的核心索引、检索和排序是给到用户 Top 相关性内容，用户自主多了一层推理、对比筛选、总结。

其次目前的 ChatGPT 不能够回答 21 年之后的问题，新知识的获取是通过增加标注数据实现。如果要支持获取社会热点新闻等，就需要改变底层技术方案。尽管这个问题 WebGPT、Sparrow 通过搜索引擎解决，能否替代自己就有了答案。

最后就是成本问题，ChatGPT 火的原因之一就是免费体验，之前超大模型 GPT3 收费模式根本没有产生这么大的反响。商业化一直是大模型的痛，模型效果和模型参数成正比。搜索引擎索引、检索、排序的成本和 ChatGPT 这种模型计算成本不在一个量级上。

### **5.3 未来预期：资本市场怎么看**

和负责投资和战略的同学聊，近期都在讨论 AI。AI 赛道无疑是投资界“今年最大的热点之一”。ChatGPT 和今年大火的 AI 绘画都属于泛 AIGC 领域，AIGC 是继 PGC、UGC 后的新内容生产形态。AI 投资人看来，从语音、文字、图像的内容生成都将出现增长，而对话可能是其中最重要的杀手级应用。根据 Gartner 预计，到 2025 年，生成式人工智能将占所有生成数据的 10%，而当前占比小于 1%。

回顾一下 OpenAI，作为 AIGC 顶级技术公司已经做了不少商业化的尝试，通过 API 方式来推动 GPT-3 的技术商业化，将 GPT3 作为一项付费服务来推广。Codex 也是已经商业化的产品。GPT-3 历经两年商业化尝试，如今并未取代记者编辑或码农的职业生涯，OpenAI 也从中发现，将 GPT 系列作为辅助生产力工具对商业化更为合适。此次 ChatGPT 采取免费试用可能是 OpenAI 准备继续打磨这款产品，根据用户的反馈帮助模型改进从而作出更恰当的反应。等产品打磨好可能为 GPT-4 商业化铺路。

2022 年以来 AIGC 应用多点开花，伴随着深度学习模型不断完善、开源模式的推动、大模型探索商业化的可能，AIGC 有望加速发展，让人们对通用人工智能有了更多的期待。

**参考材料**

[**ChatGPT: Optimizing Language Models for Dialogue**](https://openai.com/blog/chatgpt/)

[**Aligning Language Models to Follow Instructions**](https://openai.com/blog/instruction-following/)

[**WebGPT: Improving the Factual Accuracy of Language Models through Web Browsing**](https://openai.com/blog/webgpt/)

[**Aligning Language Models to Follow Instructions**](https://openai.com/blog/instruction-following/#moon)

[**Learning from Human Preferences**](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/)

[**Proximal Policy Optimization**](https://openai.com/blog/openai-baselines-ppo/)

[**https://gpt.Chatapi.art/?**](https://gpt.chatapi.art/?)

[**Building safer dialogue agents**](https://www.deepmind.com/blog/building-safer-dialogue-agents)

**https://jmcdonnell.substack.com/p/the-near-future-of-ai-is-action-driven**

**https://mp.weixin.qq.com/s/OO03dHMStOV8tVdRwzEkLA**

**https://36kr.com/p/2033972476849410**

**https://easyai.tech/ai-definition/reinforcement-learning/**

原文作者：作者：qizailiu，腾讯 IEG 应用研究员

原文链接：https://mp.weixin.qq.com/s/LjxNpyX4_UPnZQtPGMs73g

# 【NO.58】从Linux零拷贝深入了解Linux-I/O

> 本文将从文件传输场景以及零拷贝技术深究 Linux I/O 的发展过程、优化手段以及实际应用。

## **0.前言**

存储器是计算机的核心部件之一，在完全理想的状态下，存储器应该要同时具备以下三种特性：

- 速度足够快：存储器的存取速度应当快于 CPU 执行一条指令，这样 CPU 的效率才不会受限于存储器；
- 容量足够大：容量能够存储计算机所需的全部数据；
- 价格足够便宜：价格低廉，所有类型的计算机都能配备。

但是现实往往是残酷的，我们目前的计算机技术无法同时满足上述的三个条件，于是现代计算机的存储器设计采用了一种分层次的结构：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141447122216817.png)

从顶至底，现代计算机里的存储器类型分别有：**寄存器、高速缓存、主存和磁盘**，这些存储器的速度逐级递减而容量逐级递增。

存取速度最快的是**寄存器**，因为寄存器的制作材料和 CPU 是相同的，所以速度和 CPU 一样快，CPU 访问寄存器是没有时延的，然而因为价格昂贵，因此容量也极小，一般 32 位的 CPU 配备的寄存器容量是 32✖️32 Bit，64 位的 CPU 则是 64✖️64 Bit，不管是 32 位还是 64 位，寄存器容量都小于 1 KB，且寄存器也必须通过软件自行管理。

第二层是**高速缓存**，也即我们平时了解的 CPU 高速缓存 **L1、L2、L3**，一般 L1 是每个 CPU 独享，L3 是全部 CPU 共享，而 L2 则根据不同的架构设计会被设计成独享或者共享两种模式之一，比如 Intel 的多核芯片采用的是共享 L2 模式而 AMD 的多核芯片则采用的是独享 L2 模式。

第三层则是**主存**，也即主内存，通常称作随机访问存储器（Random Access Memory, RAM）。是与 CPU 直接交换数据的内部存储器。它可以随时读写（刷新时除外），而且速度很快，通常作为操作系统或其他正在运行中的程序的临时资料存储介质。

至于**磁盘**则是图中离用户最远的一层了，读写速度相差内存上百倍；另一方面自然针对磁盘操作的优化也非常多，如`零拷贝`**、**`direct I/O`**、**`异步 I/O` 等等，这些优化的目的都是为了提高系统的吞吐量；另外操作系统内核中也有`磁盘高速缓存区`、`PageCache`、`TLB`等，可以有效的减少磁盘的访问次数。

现实情况中，大部分系统在由小变大的过程中，最先出现瓶颈的就是`I/O`，尤其是在现代网络应用从 CPU 密集型转向了 `I/O` 密集型的大背景下，`I/O`越发成为大多数应用的性能瓶颈。

传统的 Linux 操作系统的标准 I/O 接口是基于数据拷贝操作的，即 I/O 操作会导致数据在操作系统内核地址空间的缓冲区和用户进程地址空间定义的缓冲区之间进行传输。**设置缓冲区最大的好处是可以减少磁盘 I/O 的操作**，如果所请求的数据已经存放在操作系统的高速缓冲存储器中，那么就不需要再进行实际的物理磁盘 I/O 操作；然而传统的 Linux I/O 在数据传输过程中的数据拷贝操作**深度依赖 CPU**，也就是说 I/O 过程需要 CPU 去执行数据拷贝的操作，因此导致了极大的系统开销，限制了操作系统有效进行数据传输操作的能力。

这篇文章就从文件传输场景以及**零拷贝**技术深究 `Linux I/O`的发展过程、优化手段以及实际应用。

## 1.**需要了解的词**

- **DMA**DMA，全称 Direct Memory Access，即直接存储器访问，是为了避免 CPU 在磁盘操作时承担过多的中断负载而设计的；在磁盘操作中，CPU 可将总线控制权交给 DMA 控制器，由 DMA 输出读写命令，直接控制 RAM 与 I/O 接口进行 DMA 传输，无需 CPU 直接控制传输，也没有中断处理方式那样保留现场和恢复现场过程，使得 CPU 的效率大大提高。
- **MMU**Memory Management Unit—内存管理单元，主要实现：
- - **竞争访问保护管理需求**：需要严格的访问保护，动态管理哪些内存页/段或区，为哪些应用程序所用。这属于资源的竞争访问管理需求；
  - **高效的翻译转换管理需求**：需要实现快速高效的映射翻译转换，否则系统的运行效率将会低下；
  - **高效的虚实内存交换需求**：需要在实际的虚拟内存与物理内存进行内存页/段交换过程中快速高效。
- **Page Cache**为了避免每次读写文件时，都需要对硬盘进行读写操作，Linux 内核使用 `页缓存（Page Cache）` 机制来对文件中的数据进行缓存。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141447449057933.png)

此外，由于读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是非常耗时的，为了降低它的影响，**PageCache 使用了「预读功能」**。

比如，假设 read 方法每次只会读 `32 KB` 的字节，虽然 read 刚开始只会读 0 ～ 32 KB 的字节，但内核会把其后面的 32 ～ 64 KB 也读取到 PageCache，这样后面读取 32 ～ 64 KB 的成本就很低，如果在 32 ～ 64 KB 淘汰出 PageCache 前，有进程读取到它了，收益就非常大。

- **虚拟内存**

  在计算机领域有一句如同摩西十诫般神圣的哲言：”**计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决**“，从内存管理、网络模型、并发调度甚至是硬件架构，都能看到这句哲言在闪烁着光芒，而虚拟内存则是这一哲言的完美实践之一。

  虚拟内存为每个进程提供了一个**一致的、私有且连续完整的内存空间**；所有现代操作系统都使用虚拟内存，使用虚拟地址取代物理地址，主要有以下几点好处：

  利用上述的第一条特性可以优化，可以把**内核空间和用户空间的虚拟地址映射到同一个物理地址**，这样在 I/O 操作时就不需要来回复制了。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141447575839835.png)

- - 多个虚拟内存可以指向同一个物理地址；
  - 虚拟内存空间可以远远大于物理内存空间；
  - 应用层面可管理连续的内存空间，减少出错。
- **NFS**文件系统 网络文件系统是 FreeBSD 支持的文件系统中的一种，也被称为 NFS；NFS 允许一个系统在网络上与它人共享目录和文件，通过使用 NFS，用户和程序可以象访问本地文件 一样访问远端系统上的文件。
- **Copy-on-write**写入时复制（Copy-on-write，COW）是一种计算机程序设计领域的优化策略。其核心思想是，如果有多个调用者（callers）同时请求相同资源（如内存或磁盘上的数据存储），他们会共同获取相同的指针指向相同的资源，直到某个调用者试图修改资源的内容时，系统才会真正复制一份专用副本（private copy）给该调用者，而其他调用者所见到的最初的资源仍然保持不变。这过程对其他的调用者都是透明的。此作法主要的优点是**如果调用者没有修改该资源，就不会有副本（private copy）被创建**，因此多个调用者只是读取操作时可以共享同一份资源。

## 2.**为什么要有 DMA**

在没有 DMA 技术前，I/O 的过程是这样的：

- CPU 发出对应的指令给磁盘控制器，然后返回；
- 磁盘控制器收到指令后，于是就开始准备数据，会把数据放入到磁盘控制器的内部缓冲区中，然后产生一个**中断**；
- CPU 收到中断信号后，停下手头的工作，接着把磁盘控制器的缓冲区的数据一次一个字节地读进自己的寄存器，然后再把寄存器里的数据写入到内存，而在数据传输的期间 CPU 是被阻塞的状态，无法执行其他任务。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141448103728019.png)

整个数据的传输过程，都要需要 CPU 亲自参与拷贝数据，而且这时 CPU 是被阻塞的；简单的搬运几个字符数据那没问题，但是如果我们用千兆网卡或者硬盘传输大量数据的时候，都用 CPU 来搬运的话，肯定忙不过来。

计算机科学家们发现了事情的严重性后，于是就发明了 DMA 技术，也就是**直接内存访问（Direct Memory Access）** 技术。

简单理解就是，**在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情，这样 CPU 就可以去处理别的事务**。

具体流程如下图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141448261050327.png)

- 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；
- 操作系统收到请求后，进一步将 I/O 请求发送 DMA，释放 CPU；
- DMA 进一步将 I/O 请求发送给磁盘；
- 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；
- **DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 依然可以执行其它事务**；
- 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；
- CPU 收到 中断信号，将数据从内核拷贝到用户空间，系统调用返回。

在有了 DMA 后，整个数据传输的过程，CPU 不再参与与磁盘交互的数据搬运工作，而是全程由 DMA 完成，但是 CPU 在这个过程中也是必不可少的，因为传输什么数据，从哪里传输到哪里，都需要 CPU 来告诉 DMA 控制器。

早期 DMA 只存在在主板上，如今由于 I/O 设备越来越多，数据传输的需求也不尽相同，所以每个 I/O 设备里面都有自己的 DMA 控制器。

## 3.**传统文件传输的缺陷**

有了 DMA 后，我们的磁盘 I/O 就一劳永逸了吗？并不是的；拿我们比较熟悉的下载文件举例，服务端要提供此功能，比较直观的方式就是：将磁盘中的文件读出到内存，再通过网络协议发送给客户端。

具体的 I/O 工作方式是，数据读取和写入是从用户空间到内核空间来回复制，而内核空间的数据是通过操作系统层面的 I/O 接口从磁盘读取或写入。

代码通常如下，一般会需要两个系统调用：

```
read(file, tmp_buf, len)write(socket, tmp_buf, len)
```

代码很简单，虽然就两行代码，但是这里面发生了不少的事情：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141448448175898.png)

这其中有：

- **4 次用户态与内核态的上下文切换**两次系统调用 `read()` 和 `write()`中，每次系统调用都得先从**用户态切换到内核态**，等内核完成任务后，再从内核态切换回用户态；**上下文切换**的成本并不小，一次切换需要耗时几十纳秒到几微秒，在高并发场景下很容易成为性能瓶颈（参考**线程切换和协程切换的成本差别**）。
- **4 次数据拷贝**两次由 DMA 完成拷贝，另外两次则是由 CPU 完成拷贝；我们只是搬运一份数据，结果却搬运了 4 次，过多的数据拷贝无疑会消耗 额外的资源，大大降低了系统性能。

所以，要想提高文件传输的性能，就需要减少**用户态与内核态的上下文切换**和**内存拷贝**的次数。

### 3.1 如何优化传统文件传输

#### **3.1.1 减少「用户态与内核态的上下文切换」**

读取磁盘数据的时候，之所以要发生上下文切换，这是因为用户空间没有权限操作磁盘或网卡，内核的权限最高，这些操作设备的过程都需要交由操作系统内核来完成，所以一般要通过内核去完成某些任务的时候，就需要使用操作系统提供的系统调用函数。

而一次系统调用必然会发生 2 次上下文切换：首先从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行。

#### **3.1.2 减少「数据拷贝」次数**

前面提到，传统的文件传输方式会历经 4 次数据拷贝；但很明显的可以看到：**从内核的读缓冲区拷贝到用户的缓冲区**和**从用户的缓冲区里拷贝到 socket 的缓冲区**」这两步是没有必要的。

因为在下载文件，或者说广义的文件传输场景中，我们并不需要在用户空间对数据进行**再加工**，所以数据并不需要回到用户空间中。

## 4.**零拷贝**

那么**零拷贝**技术就应运而生了，它就是为了解决我们在上面提到的场景——跨过与用户态交互的过程，直接将数据从文件系统移动到网络接口而产生的技术。

### 4.1 零拷贝实现原理

零拷贝技术实现的方式通常有 3 种：

- mmap + write
- sendfile
- splice

### 4.2 **mmap + write**

在前面我们知道，`read()` 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了省去这一步，我们可以用 `mmap()` 替换 `read()` 系统调用函数，伪代码如下：

```
buf = mmap(file, len)write(sockfd, buf, len)
```

`mmap`的函数原型如下：

```
void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);
```

`mmap()` 系统调用函数会在调用进程的虚拟地址空间中创建一个新映射，直接把内核缓冲区里的数据「**映射**」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141448578263658.png)

具体过程如下：

- 应用进程调用了 `mmap()` 后，DMA 会把磁盘的数据拷贝到内核的缓冲区里，应用进程跟操作系统内核「共享」这个缓冲区；
- 应用进程再调用 `write()`，操作系统直接将内核缓冲区的数据拷贝到 socket 缓冲区中，这一切都发生在内核态，由 CPU 来搬运数据；
- 最后，把内核的 socket 缓冲区里的数据，拷贝到网卡的缓冲区里，这个过程是由 DMA 搬运的。

我们可以看到，通过使用 `mmap()` 来代替 `read()`， 可以减少一次数据拷贝的过程。

但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，且仍然需要 4 次上下文切换，因为系统调用还是 2 次。

### **4.3 sendfile**

在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 `sendfile()`如下：

```
#include <sys/socket.h>ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。

首先，它可以替代前面的 `read()` 和 `write()` 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。

其次，该系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，这样就只有 2 次上下文切换，和 3 次数据拷贝。如下图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141449115898424.png)

### 4.4 带有 scatter/gather 的 sendfile 方式

Linux 2.4 内核进行了优化，提供了带有 `scatter/gather` 的 sendfile 操作，这个操作可以把最后一次 `CPU COPY` 去除。其原理就是在内核空间 Read BUffer 和 Socket Buffer 不做数据复制，而是将 Read Buffer 的内存地址、偏移量记录到相应的 Socket Buffer 中，这样就不需要复制。其本质和虚拟内存的解决方法思路一致，就是内存地址的记录。

你可以在你的 Linux 系统通过下面这个命令，查看网卡是否支持 scatter-gather 特性：

```
$ ethtool -k eth0 | grep scatter-gatherscatter-gather: on
```

于是，从 Linux 内核 `2.4` 版本开始起，对于支持网卡支持 SG-DMA 技术的情况下， `sendfile()` 系统调用的过程发生了点变化，具体过程如下：

- 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；
- 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝；

所以，这个过程之中，只进行了 2 次数据拷贝，如下图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141449211627298.png)

### 4.5 splice 方式

`splice` 调用和`sendfile` 非常相似，用户应用程序必须拥有两个已经打开的文件描述符，一个表示输入设备，一个表示输出设备。与`sendfile`不同的是，`splice`允许任意两个文件互相连接，而并不只是文件与`socket`进行数据传输。对于从一个文件描述符发送数据到`socket`这种特例来说，一直都是使用`sendfile`系统调用，而`splice`一直以来就只是一种机制，它并不仅限于`sendfile`的功能。也就是说 `sendfile` 是 `splice` 的一个子集。

`splice()` 是基于 `Linux` 的管道缓冲区 (`pipe buffer`) 机制实现的，所以`splice()`的两个入参文件描述符要求必须有一个是管道设备。

使用 `splice()` 完成一次磁盘文件到网卡的读写过程如下：

- 用户进程调用 `pipe()`，从用户态陷入内核态；创建匿名单向管道，`pipe()` 返回，上下文从内核态切换回用户态；
- 用户进程调用 `splice()`，从用户态陷入内核态；
- `DMA` 控制器将数据从硬盘拷贝到内核缓冲区，从管道的写入端”拷贝”进管道，`splice()`返回，上下文从内核态回到用户态；
- 用户进程再次调用 `splice()`，从用户态陷入内核态；
- 内核把数据从管道的读取端拷贝到`socket`缓冲区，`DMA` 控制器将数据从`socket`缓冲区拷贝到网卡；
- `splice()` 返回，上下文从内核态切换回用户态。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141449344922921.png)
在 Linux 2.6.17 版本引入了 `splice`，而在 Linux 2.6.23 版本中， `sendfile` 机制的实现已经没有了，但是其 API 及相应的功能还在，只不过 API 及相应的功能是利用了 `splice` 机制来实现的。

和 `sendfile` 不同的是，`splice` 不需要硬件支持。

## 5.**零拷贝的实际应用**

### 5.1 Kafka

事实上，Kafka 这个开源项目，就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一。

如果你追溯 Kafka 文件传输的代码，你会发现，最终它调用了 Java NIO 库里的 `transferTo` 方法：

```
@Overridepubliclong transferFrom(FileChannel fileChannel, long position, long count) throws IOException {    return fileChannel.transferTo(position, count, socketChannel);}
```

如果 Linux 系统支持 `sendfile()` 系统调用，那么 `transferTo()` 实际上最后就会使用到 `sendfile()` 系统调用函数。

### 5.2 Nginx

Nginx 也支持零拷贝技术，一般默认是开启零拷贝技术，这样有利于提高文件传输的效率，是否开启零拷贝技术的配置如下：

```
http {...    sendfile on...}
```

## 6.**大文件传输场景**

### 6.1 零拷贝还是最优选吗

在大文件传输的场景下，零拷贝技术并不是最优选择；因为在零拷贝的任何一种实现中，都会有「DMA 将数据从磁盘拷贝到内核缓存区——Page Cache」这一步，但是，在传输大文件（GB 级别的文件）的时候，PageCache 会不起作用，那就白白浪费 DMA 多做的一次数据拷贝，造成性能的降低，即使使用了 PageCache 的零拷贝也会损失性能。

这是因为在大文件传输场景下，每当用户访问这些大文件的时候，内核就会把它们载入 PageCache 中，PageCache 空间很快被这些大文件占满；且由于文件太大，可能某些部分的文件数据被再次访问的概率比较低，这样就会带来 2 个问题：

- PageCache 由于长时间被大文件占据，其他「热点」的小文件可能就无法充分使用到 PageCache，于是这样磁盘读写的性能就会下降了；
- PageCache 中的大文件数据，由于没有享受到缓存带来的好处，但却耗费 DMA 多拷贝到 PageCache 一次。

### 6.2 异步 I/O + `direct I/O`

那么大文件传输场景下我们该选择什么方案呢？让我们先来回顾一下我们在文章开头介绍 DMA 时最早提到过的同步 I/O：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141449499586057.png)

这里的**同步**体现在当进程调用 read 方法读取文件时，进程实际上会阻塞在 read 方法调用，因为要等待磁盘数据的返回，并且我们当然不希望进程在读取大文件时被阻塞，对于阻塞的问题，可以用异步 `I/O` 来解决，即：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141449592934905.png)

它把读操作分为两部分：

- 前半部分，内核向磁盘发起读请求，但是可以**不等待数据就位就返回**，于是进程此时可以处理其他任务；
- 后半部分，当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的**通知**，再去处理数据；

而且，我们可以发现，异步 I/O 并没有涉及到 PageCache；使用异步 I/O 就意味着要绕开 PageCache，因为填充 PageCache 的过程在内核中必须阻塞。

所以异步 I/O 中使用的是`direct I/O`（对比使用 PageCache 的`buffer I/O`），这样才能不阻塞进程，立即返回。

`direct I/O` 应用场景常见的两种：

- 应用程序已经实现了磁盘数据的缓存，那么可以不需要 PageCache 再次缓存，减少额外的性能损耗。在 MySQL 数据库中，可以通过参数设置开启`direct I/O`，默认是不开启；
- 传输大文件的时候，由于大文件难以命中 PageCache 缓存，而且会占满 PageCache 导致「热点」文件无法充分利用缓存，从而增大了性能开销，因此，这时应该使用`direct I/O；；

当然，由于`direct I/O` 绕过了 PageCache，就无法享受内核的这两点的优化：

- 内核的 I/O 调度算法会缓存尽可能多的 I/O 请求在 PageCache 中，最后「**合并**」成一个更大的 I/O 请求再发给磁盘，这样做是为了减少磁盘的寻址操作；
- 内核也会「**预读**」后续的 I/O 请求放在 PageCache 中，一样是为了减少对磁盘的操作；

实际应用中也有类似的配置，在 **nginx** 中，我们可以用如下配置，来根据文件的大小来使用不同的方式传输：

```
location /video/ {    sendfile on;    aio on;    directio 1024m;}
```

当文件大小大于 `directio` 值后，使用「异步 I/O + 直接 I/O」，否则使用「零拷贝技术」。

### 6.3 使用 direct I/O 需要注意的点

首先，贴一下我们的`Linus(Linus Torvalds)`**对**O_DIRECT的评价：

> “The thing that has always disturbed me about O_DIRECT is that the whole interface is just stupid, and was probably designed by a deranged monkey on some serious mind-controlling substances.” —Linus

一般来说能引得`Linus`开骂的东西，那是一定有很多坑的。

在 Linux 的[`man page`](https://man7.org/linux/man-pages/man2/open.2.html)中我们可以看到**O_DIRECT**下有一个 Note，还挺长的，这里我就不贴出来了。

总结一下其中需要注意的点如下：

#### **6.3.1 地址对齐限制**

**O_DIRECT**会带来强制的地址对齐限制，这个对齐的大小也跟**文件系统/存储介质**相关，并且当前没有不依赖文件系统自身的接口提供指定文件/文件系统是否有这些限制的信息

- Linux 2.6 以前 总传输大小、用户的对齐缓冲区起始地址、文件偏移量必须都是逻辑**文件系统的数据块**大小的倍数，这里说的数据块(block)是一个逻辑概念，是文件系统捆绑一定数量的连续扇区而来，因此通常称为 “文件系统逻辑块”，可通过以下命令获取：

  ```
  blockdev --getss
  ```

- Linux2.6以后对齐的基数变为物理上的存储介质的`sector size`扇区大小，对应物理存储介质的最小存储粒度，可通过以下命令获取：

  ```
  blockdev --getpbsz
  ```

带来这个限制的原因也很简单，内存对齐这件小事通常是内核来处理的，而**O_DIRECT**绕过了内核空间，那么内核处理的所有事情都需要用户自己来处理，这里贴一篇[详细解释](https://www.quora.com/Why-does-O_DIRECT-require-I-O-to-be-512-byte-aligned)。

#### 6.3.2 **O_DIRECT 平台不兼容**

这应该是大部分跨平台应用需要注意到的点，**O_DIRECT**本身就是`Linux`中才有的东西，在语言层面 / 应用层面需要考虑这里的兼容性保证，比如在`Windows`下其实也有类似的机制**FILE_FLAG_NO_BUFFERIN**用法类似，参考微软的[官方文档](https://learn.microsoft.com/en-us/windows/win32/fileio/file-buffering)；再比如`macOS`下的**F_NOCACHE**虽然类似**O_DIRECT**，但实际使用中也有差距（参考这个[issue](https://github.com/axboe/fio/issues/48)）。

#### **6.3.3 不要并发地运行 fork 和 O_DIRECT I/O**

如果`O_DIRECT I/O`中使用到的内存`buffer`是一段私有的映射（虚拟内存），如任何使用上文中提到过的`mmap`并以**MAP_PRIVATE** flag 声明的虚拟内存，那么相关的`O_DIRECT I/O`（不管是异步 I/O / 其它子线程中的 I/O）都必须在调用`fork`系统调用前执行完毕；否则会造成数据污染或产生未定义的行为（实例可参考这个[Page](https://www.ibm.com/support/pages/ibm-spectrum-scale-using-odirect-and-fork2-same-process-linux)）。

以下情况这个限制不存在：

- 相关的内存`buffer`是使用`shmat`分配或是使用`mmap`以**MAP_SHARED** flag 声明的；
- 相关的内存`buffer`是使用`madvise`以**MADV_DONTFORK**声明的（注意这种方式下该内存`buffer`在子进程中不可用）。

#### **6.3.4 避免对同一文件混合使用 O_DIRECT 和普通 I/O**

在应用层需要避免对同一文件（**尤其是对同一文件的相同偏移区间内**）混合使用`O_DIRECT`和普通`I/O`；即使我们的文件系统能够帮我们处理和保证这里的**一致性问题**，总体来说整个`I/O`吞吐量也会比单独使用某一种`I/O`方式要小。

同样的，应用层也要避免对同一文件混合使用`direct I/O`和`mmap。`

#### **6.3.5 NFS 协议下的 O_DIRECT**

虽然`NFS`文件系统就是为了让用户像访问本地文件一样去访问网络文件，但`O_DIRECT`在`NFS`文件系统中的表现和本地文件系统不同，比较老版本的内核或是魔改过的内核可能并不支持这种组合。

这是因为在`NFS`协议中并不支持传递**flag 参数**到服务器，所以`O_DIRECT I/O`实际上只绕过了本地客户端的`Page Cache`，但服务端/同步客户端仍然会对这些`I/O`进行`cache`。

当客户端请求服务端进行`I/O`同步来保证`O_DIRECT`的同步语义时，一些服务器的性能表现不佳（尤其是当这些`I/O`很小时）；还有一些服务器干脆设置为**欺骗客户端**，直接返回客户端「**数据已写入存储介质**」，这样就可以一定程度上避免`I/O`同步带来的性能损失，但另一方面，当服务端断电时就无法保证未完成`I/O`同步的数据的**数据完整性**了。

`Linux`的`NFS`客户端也没有上面说过的地址对齐的限制。

### 6.4 在 Golang 中使用 direct I/O

direct io 必须要满足 3 种对齐规则：io 偏移扇区对齐，长度扇区对齐，内存 buffer 地址扇区对齐；前两个还比较好满足，但是分配的内存地址仅凭原生的手段是无法直接达成的。

先对比一下 c 语言，libc 库是调用 `posix_memalign` 直接分配出符合要求的内存块，但`Golang`中要怎么实现呢？

在`Golang`中，io 的 buffer 其实就是字节数组，自然是用 make 来分配，如下：

```
buffer := make([]byte, 4096)
```

但`buffer`中的`data`字节数组首地址并不一定是对齐的。

方法也很简单，就是**先分配一个比预期要大的内存块，然后在这个内存块里找对齐位置** ；这是一个任何语言皆通用的方法，在 Go 里也是可用的。

比如，我现在需要一个 4096 大小的内存块，要求地址按照 512 对齐，可以这样做：

- 先分配 4096 + 512 大小的内存块，假设得到的内存块首地址是 p1；
- 然后在 [ p1, p1+512 ] 这个地址范围找，一定能找到 512 对齐的地址 p2；
- 返回 p2 ，用户能正常使用 [ p2, p2 + 4096 ] 这个范围的内存块而不越界。

**以上就是基本原理了**，具体实现如下：

```
// 从 block 首地址往后找到符合 AlignSize 对齐的地址并返回// 这里很巧妙的使用了位运算，性能upupfunc alignment(block []byte, AlignSize int) int {   return int(uintptr(unsafe.Pointer(&block[0])) & uintptr(AlignSize-1))}// 分配 BlockSize 大小的内存块// 地址按 AlignSize 对齐func AlignedBlock(BlockSize int) []byte {   // 分配一个大小比实际需要的稍大   block := make([]byte, BlockSize+AlignSize)   // 计算到下一个地址对齐点的偏移量   a := alignment(block, AlignSize)   offset := 0   if a != 0 {      offset = AlignSize - a   }   // 偏移指定位置，生成一个新的 block，这个 block 就满足地址对齐了   block = block[offset : offset+BlockSize]   if BlockSize != 0 {      // 最后做一次地址对齐校验      a = alignment(block, AlignSize)      if a != 0 {         log.Fatal("Failed to align block")      }   }   return block}
```

所以，通过以上 AlignedBlock 函数分配出来的内存一定是 512 地址对齐的，唯一的一点点缺点就是在分配较小内存块时对齐的额外开销显得比较大。

##### **开源实现**

Github 上就有开源的`Golang direct I/O`实现：[ncw/directio](https://github.com/ncw/directio)

使用也很简单：

- O_DIRECT 模式打开文件：

  ```
  // 创建句柄fp, err := directio.OpenFile(file, os.O_RDONLY, 0666)
  ```

- 读数据

  ```
  // 创建地址按照 4k 对齐的内存块buffer := directio.AlignedBlock(directio.BlockSize)// 把文件数据读到内存块中_, err := io.ReadFull(fp, buffer)
  ```

## 7.**内核缓冲区和用户缓冲区之间的传输优化**

到目前为止，我们讨论的 `zero-copy`技术都是基于减少甚至是避免用户空间和内核空间之间的 `CPU` 数据拷贝的，虽然有一些技术非常高效，但是大多都有适用性很窄的问题，比如 `sendfile()`、`splice()` 这些，效率很高，但是都只适用于那些用户进程**不需要再处理数据**的场景，比如静态文件服务器或者是直接转发数据的代理服务器。

前面提到过的虚拟内存机制和`mmap`等都表明，通过在不同的虚拟地址上重新映射页面可以实现在用户进程和内核之间虚拟复制和共享内存；因此如果要在实现在用户进程内处理数据（这种场景比直接转发数据更加常见）之后再发送出去的话，**用户空间和内核空间的数据传输就是不可避免的**，既然避无可避，那就只能选择优化了。

两种优化用户空间和内核空间数据传输的技术：

- 动态重映射与写时拷贝 (`Copy-on-Write`)
- 缓冲区共享 (`Buffer Sharing`)

### 7.1 写时拷贝 (Copy-on-Write)

前面提到过过利用内存映射(`mmap`)来减少数据在用户空间和内核空间之间的复制，通常用户进程是对共享的缓冲区进行同步阻塞读写的，这样不会有**线程安全**问题，但是很明显这种模式下效率并不高，而提升效率的一种方法就是**异步地对共享缓冲区进行读写**，而这样的话就必须引入保护机制来避免**数据冲突**问题，`COW (Copy on Write)` 就是这样的一种技术。

`COW` 是一种建立在虚拟内存重映射技术之上的技术，因此它需要 `MMU` 的硬件支持，`MMU` 会记录当前哪些内存页被标记成只读，当有进程尝试往这些内存页中写数据的时候，`MMU` 就会抛一个异常给操作系统内核，内核处理该异常时为该进程分配一份物理内存并复制数据到此内存地址，重新向 `MMU` 发出执行该进程的写操作。

下图为`COW`在`Linux`中的应用之一: `fork / clone`，`fork`出的子进程共享父进程的物理空间，当父子进程**有内存写入操作时**，`read-only`内存页发生中断，**将触发的异常的内存页复制一份**(其余的页还是共享父进程的)。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141450233452281.png)

#### **7.1.1 局限性**

`COW` 这种零拷贝技术比较适用于那种**多读少写从而使得 COW 事件发生较少的场景**，而在其它场景下反而可能造成负优化，因为 `COW`事件所带来的系统开销要远远高于一次 `CPU` 拷贝所产生的。

此外，在实际应用的过程中，为了避免频繁的内存映射，可以重复使用同一段内存缓冲区，因此，你不需要在只用过一次共享缓冲区之后就解除掉内存页的映射关系，而是重复循环使用，从而提升性能。

但这种内存页映射的持久化并不会减少由于页表往返移动/换页和[ TLB flush](https://zhuanlan.zhihu.com/p/108425561)所带来的系统开销，因为每次接收到 COW 事件之后对内存页而进行加锁或者解锁的时候，内存页的只读标志 (`read-ony`) 都要被更改为 (`write-only`)。

#### 7.1.2 **COW 的实际应用**

**Redis 的持久化机制**

`Redis` 作为典型的内存型应用，一定是有内核缓冲区和用户缓冲区之间的传输优化的。

`Redis` 的持久化机制中，如果采用 `bgsave` 或者 `bgrewriteaof` 命令，那么会 fork 一个子进程来将数据存到磁盘中；总体来说`Redis` 的读操作是比写操作多的（在正确的使用场景下），因此这种情况下使用 `COW` 可以减少 `fork()` 操作的阻塞时间。

**语言层面的应用**

写时复制的思想在很多语言中也有应用，相比于传统的深层复制，能带来很大性能提升；比如 `C++ 98` 标准下的 `std::string` 就采用了写时复制的实现：

```
std::string x("Hello");std::string y = x;  // x、y 共享相同的 buffery += ", World!";    // 写时复制，此时 y 使用一个新的 buffer                    // x 依然使用旧的 buffer
```

`Golang`中的`string, slice`也使用了类似的思想，在复制 / 切片等操作时都不会改变底层数组的指向，变量共享同一个底层数组，仅当进行`append` / 修改等操作时才可能进行真正的`copy`（`append`时如果超过了当前切片的容量，就需要分配新的内存）。

### 7.2 缓冲区共享 (Buffer Sharing)

从前面的介绍可以看出，传统的 `Linux I/O`接口，都是基于复制/拷贝的：数据需要在操作系统内核空间和用户空间的缓冲区之间进行拷贝。在进行 `I/O` 操作之前，用户进程需要预先分配好一个内存缓冲区，使用 `read()` 系统调用时，内核会将从存储器或者网卡等设备读入的数据拷贝到这个用户缓冲区里；而使用 `write()` 系统调用时，则是把用户内存缓冲区的数据拷贝至内核缓冲区。

为了实现这种传统的 `I/O` 模式，`Linux` 必须要在每一个 `I/O` 操作时都进行内存虚拟映射和解除。这种内存页重映射的机制的效率严重受限于缓存体系结构、`MMU` 地址转换速度和 `TLB` 命中率。如果能够避免处理 `I/O` 请求的虚拟地址转换和 `TLB` 刷新所带来的开销，则有可能极大地提升 `I/O` 性能。而缓冲区共享就是用来解决上述问题的一种技术（说实话我觉得有些套娃的味道了）。

操作系统内核开发者们实现了一种叫 **fbufs** 的缓冲区共享的框架，也即快速缓冲区（ `Fast Buffers` ），使用一个 `fbuf` 缓冲区作为数据传输的最小单位，使用这种技术需要调用新的操作系统 API，用户区和内核区、内核区之间的数据都必须严格地在 `fbufs` 这个体系下进行通信。`fbufs` 为每一个用户进程分配一个 `buffer pool`，里面会储存预分配 (也可以使用的时候再分配) 好的 buffers，这些 buffers 会被同时映射到用户内存空间和内核内存空间。`fbufs` 只需通过一次虚拟内存映射操作即可创建缓冲区，有效地消除那些由存储一致性维护所引发的大多数性能损耗。

共享缓冲区技术的实现需要**依赖于用户进程、操作系统内核、以及 I/O 子系统 (设备驱动程序，文件系统等)之间协同工作**。比如，设计得不好的用户进程容易就会修改已经发送出去的 fbuf 从而污染数据，更要命的是这种问题很难 debug。虽然这个技术的设计方案非常精彩，但是它的门槛和限制却不比前面介绍的其他技术少：首先会对操作系统 API 造成变动，需要使用新的一些 API 调用，其次还需要设备驱动程序配合改动，还有由于是内存共享，内核需要很小心谨慎地实现对这部分共享的内存进行数据保护和同步的机制，而这种并发的同步机制是非常容易出 bug 的从而又增加了内核的代码复杂度，等等。因此这一类的技术还远远没有到发展成熟和广泛应用的阶段，**目前大多数的实现都还处于实验阶段**。

## **8.总结**

从早期的`I/O`到`DMA`，解决了阻塞`CPU`的问题；而为了省去`I/O`过程中不必要的上下文切换和数据拷贝过程，零拷贝技术就出现了。

所谓的零拷贝(`Zero-copy`)技术，就是完完全全不需要在内存层面拷贝数据，省去`CPU`搬运数据的过程。

零拷贝技术的文件传输方式相比传统文件传输的方式，减少了 2 次上下文切换和数据拷贝次数，**只需要 2 次上下文切换和数据拷贝次数，就可以完成文件的传输，而且 2 次的数据拷贝过程，都不需要通过 CPU，2 次都是由 DMA 来搬运**。

总体来看，**零拷贝技术至少可以把文件传输的性能提高一倍以上**，以下是各方案详细的成本对比：

|                               | CPU 拷贝 | DMA 拷贝 | 系统调用   | 上下文切换 | 硬件依赖 | 支持任意类型输入/输出描述符 |
| :---------------------------- | :------- | :------- | :--------- | :--------- | :------- | :-------------------------- |
| 传统方法                      | 2        | 2        | read/write | 4          | 否       | 是                          |
| 内存映射                      | 1        | 2        | mmap/write | 4          | 否       | 是                          |
| sendfile                      | 1        | 2        | sendfile   | 2          | 否       | 否                          |
| sendfile(scatter/gather copy) | 0        | 2        | sendfile   | 2          | 是       | 否                          |
| splice                        | 0        | 2        | splice     | 2          | 否       | 是                          |

零拷贝技术是基于 `PageCache` 的，`PageCache` 会缓存最近访问的数据，提升了访问缓存数据的性能，同时，为了解决机械硬盘寻址慢的问题，它还协助 `I/O` 调度算法实现了 `I/O` 合并与预读，这也是顺序读比随机读性能好的原因之一；这些优势，进一步提升了零拷贝的性能。

但当面对大文件传输时，不能使用零拷贝，因为可能由于 `PageCache` 被大文件占据，而导致「热点」小文件无法利用到 `PageCache`的问题，并且大文件的缓存命中率不高，这时就需要使用「异步 `I/O` + `direct I/O` 」的方式；在**使用`direct I/O`时也需要注意许多的坑点**，毕竟连`Linus`也会被 **O_DIRECT** ‘disturbed’ 到。

而在更广泛的场景下，我们还需要注意到**内核缓冲区和用户缓冲区之间的传输优化**，这种方式侧重于在用户进程的缓冲区和操作系统的页缓存之间的 CPU 拷贝的优化，延续了以往那种传统的通信方式，但更灵活。

`I/O`相关的各类优化自然也已经深入到了日常我们接触到的语言、中间件以及数据库的方方面面，通过了解和学习这些技术和思想，也能对日后自己的程序设计以及性能优化上有所启发。

原文作者：kevineluo，腾讯 CSIG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/j1KjR5bRQV6oGrU5tsyHgg

# 【NO.59】深入学习IO多路复用 select/poll/epoll 实现原理

> select/poll/epoll 是 Linux 服务器提供的三种处理高并发网络请求的 IO 多路复用技术，是个老生常谈又不容易弄清楚其底层原理的知识点，本文打算深入学习下其实现机制。

Linux 服务器处理网络请求有三种机制，select、poll、epoll，本文打算深入学习下其实现原理。

吃水不忘挖井人，最近两周花了些时间学习了张彦飞大佬的文章 [图解 | 深入揭秘 epoll 是如何实现 IO 多路复用的](https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247484905&idx=1&sn=a74ed5d7551c4fb80a8abe057405ea5e&scene=21#wechat_redirect) 和[其他文章](https://github.com/yanfeizhang/coder-kung-fu) ，及出版的书籍《深入理解 Linux 网络》，对阻塞 IO、多路复用、epoll 等的实现原理有了一定的了解；飞哥的文章描述底层源码逻辑比较清晰，就是有时候归纳总结事情本质的抽象程度不够，涉及内核源码细节的讲述较多，会让读者产生一定的学习成本，本文希望在这方面改进一下。

## **0. 结论**

本文其他的内容主要是得出了下面几个结论：

1. 服务器要接收客户端的数据，要建立 socket 内核结构，主要包含两个重要的数据结构，**（进程）等待队列**，和**（数据）接收队列**，socket 在进程中作为一个文件，可以用文件描述符 fd 来表示，为了方便理解，本文中， socket 内核对象 ≈ fd 文件描述符 ≈ TCP 连接；
2. 阻塞 IO 的主要逻辑是：服务端和客户端建立了连接 socket 后，服务端的用户进程通过 recv 函数接收数据时，如果数据没有到达，则当前的用户进程的进程描述符和回调函数会封装到一个进程等待项中，加入到 socket 的进程等待队列中；如果连接上有数据到达网卡，由网卡将数据通过 DMA 控制器拷贝到内核内存的 RingBuffer 中，并向 CPU 发出硬中断，然后，CPU 向内核中断进程 ksoftirqd 发出软中断信号，内核中断进程 ksoftirqd 将内核内存的 RingBuffer 中的数据根据数据报文的 IP 和端口号，将其拷贝到对应 socket 的数据接收队列中，然后通过 socket 的进程等待队列中的回调函数，唤醒要处理该数据的用户进程；
3. 阻塞 IO 的问题是：一次数据到达会进行**两次进程切换，**一次数据读取有**两处阻塞，单进程对单连接；**
4. 非阻塞 IO 模型解决了“**两次进程切换，两处阻塞，单进程对单连接**”中的“**两处阻塞**”问题，将“**两处阻塞**”变成了“**一处阻塞**”，但依然存在“**两次进程切换，一处阻塞，单进程对单连接**”的问题；
5. 用一个进程监听多个连接的 IO 多路复用技术解决了“**两次进程切换，一处阻塞，单进程对单连接**” 中的“**两次进程切换，单进程对单连接**”，剩下了“**一处阻塞**”，这是 Linux 中同步 IO 都会有的问题，因为 Linux 没有提供异步 IO 实现；
6. Linux 的 IO 多路复用用三种实现：select、poll、epoll。select 的问题是：

a）调用 select 时会陷入内核，这时需要将参数中的 fd_set 从用户空间拷贝到内核空间，高并发场景下这样的拷贝会消耗极大资源；（epoll 优化为不拷贝）

b）进程被唤醒后，不知道哪些连接已就绪即收到了数据，需要遍历传递进来的所有 fd_set 的每一位，不管它们是否就绪；（epoll 优化为异步事件通知）

c）select 只返回就绪文件的个数，具体哪个文件可读还需要遍历；（epoll 优化为只返回就绪的文件描述符，无需做无效的遍历）

d）同时能够监听的文件描述符数量太少，是 1024 或 2048；（poll 基于链表结构解决了长度限制）

1. poll 只是基于链表的结构解决了最大文件描述符限制的问题，其他 select 性能差的问题依然没有解决；终极的解决方案是 epoll，解决了 select 的前三个缺点；
2. epoll 的实现原理看起来很复杂，其实很简单，注意两个回调函数的使用：数据到达 socket 的等待队列时，通过**回调函数 ep_poll_callback** 找到 eventpoll 对象中红黑树的 epitem 节点，并将其加入就绪列队 rdllist，然后通过**回调函数 default_wake_function** 唤醒用户进程 ，并将 rdllist 传递给用户进程，让用户进程准确读取就绪的 socket 的数据。这种回调机制能够定向准确的通知程序要处理的事件，而不需要每次都循环遍历检查数据是否到达以及数据该由哪个进程处理，日常开发中可以学习借鉴下这种思想。

## **1. Linux 怎样处理网络请求**

### 1.1 阻塞 IO

要讲 IO 多路复用，最好先把传统的同步阻塞的网络 IO 的交互方式剖析清楚。

如果客户端想向 Linux 服务器发送一段数据 ，C 语言的实现方式是：

```
int main(){     int fd = socket();      // 创建一个网络通信的socket结构体     connect(fd, ...);       // 通过三次握手跟服务器建立TCP连接     send(fd, ...);          // 写入数据到TCP连接     close(fd);              // 关闭TCP连接}
```

服务端通过如下 C 代码接收客户端的连接和发送的数据：

```
int main(){     fd = socket(...);        // 创建一个网络通信的socket结构体     bind(fd, ...);           // 绑定通信端口     listen(fd, 128);         // 监听通信端口，判断TCP连接是否可以建立     while(1) {         connfd = accept(fd, ...);              // 阻塞建立连接         int n = recv(connfd, buf, ...);        // 阻塞读数据         doSomeThing(buf);                      // 利用读到的数据做些什么         close(connfd);                         // 关闭连接，循环等待下一个连接    }}
```

把服务端处理请求的细节展开，得到如下图所示的同步阻塞网络 IO 的数据接收流程：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141455044571769.png)

主要步骤是：

1）服务端通过 **socket() 函数**陷入内核态进行 socket 系统调用，该内核函数会创建 **socket 内核对象**，主要有两个重要的结构体，（**进程）等待队列**，和（**数据）接收队列，**为了方便理解，等待队列前可以加上进程二字，其实不加更准确，接收队列同样；进程等待队列，存放了服务端的用户进程 A 的进程描述符和回调函数；socket 的数据接收队列，存放网卡接收到的该 socket 要处理的数据；

2）进程 A 调用 recv() 函数接收数据，会进入到 **recvfrom() 系统调用函数**，发现 socket 的数据等待队列没有它要接收的数据到达时，进程 A 会让出 CPU，进入阻塞状态，进程 A 的进程描述符和它被唤醒用到的回调函数 callback func 会组成一个结构体叫等待队列项，放入 socket 的进程等待队列；

3）客户端的发送数据到达服务端的网卡；

4）网卡首先会将网络传输过来的数据通过 DMA 控制程序复制到内存环形缓冲区 RingBuffer 中；

5）网卡向 CPU 发出**硬中断**；

6）CPU 收到了硬中断后，为了避免过度占用 CPU 处理网络设备请求导致其他设备如鼠标和键盘的消息无法被处理，会调用网络驱动注册的中断处理函数，进行简单快速处理后向内核中断进程 ksoftirqd 发出**软中断**，就释放 CPU，由软中断进程处理复杂耗时的网络设备请求逻辑；

7）**内核中断进程 ksoftirqd** 收到软中断信号后，会将网卡复制到内存的数据，根据数据报文的 IP 和端口号，将其拷贝到对应 socket 的接收队列；

8）内核中断进程 ksoftirqd 根据 socket 的数据接收队列的数据，通过进程等待队列中的回调函数，唤醒要处理该数据的进程 A，进程 A 会进入 CPU 的运行队列，等待获取 CPU 执行数据处理逻辑；

9）进程 A 获取 CPU 后，会回到之前调用 **recvfrom() 函数**时阻塞的位置继续执行，这时发现 socket 内核空间的等待队列上有数据，会在内核态将内核空间的 socket 等待队列的数据拷贝到用户空间，然后才会回到用户态执行进程的用户程序，从而真的解除阻塞**；**

用户进程 A 在调用 recvfrom() 系统函数时，有两个阶段都是等待的：在数据没有准备好的时候，进程 A 等待内核 socket 准备好数据；内核准备好数据后，进程 A 继续等待内核将 socket 等待队列的数据拷贝到自己的用户缓冲区；在内核完成数据拷贝到用户缓冲区后，进程 A 才会从 recvfrom() 系统调用中返回，并解除阻塞状态。整体流程如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141455152262102.png)

在 IO 阻塞逻辑中，存在下面三个问题：

1. 进程在 recv 的时候大概率会被阻塞掉，导致一次进程切换；
2. 当 TCP 连接上的数据到达服务端的网卡、并从网卡复制到内核空间 socket 的数据等待队列时，进程会被唤醒，又是一次进程切换；并且，在用户进程继续执行完 recvfrom() 函数系统调用，将内核空间的数据拷贝到了用户缓冲区后，用户进程才会真正拿到所需的数据进行处理；
3. 一个进程同时只能等待一条连接，如果有很多并发，则需要很多进程；

总结：一次数据到达会进行**两次进程切换，**一次数据读取有**两处阻塞，单进程对单连接**。

### 1.2 非阻塞 IO

为了解决同步阻塞 IO 的问题，操作系统提供了非阻塞的 recv() 函数，这个函数的效果是：如果没有数据从网卡到达内核 socket 的等待队列时，系统调用会直接返回，而不是阻塞的等待。

如果我们要产生一个非阻塞的 socket，在 C 语言中如下代码所示：

```
// 创建socketint sock_fd = socket(AF_INET, SOCK_STREAM, 0);...// 更改socket为nonblockfcntl(sock_fd, F_SETFL, fdflags | O_NONBLOCK);// connect....while(1)  {    int recvlen = recv(sock_fd, recvbuf, RECV_BUF_SIZE) ;    ......}...
```

非阻塞 IO 模型如下图所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141455262925271.png)

从上图中，我们知道，非阻塞 IO，是将等待数据从网卡到达 socket 内核空间这一部分变成了非阻塞的，用户进程调用 recvfrom() 会重复发送请求检查数据是否到达内核空间，如果没有到，则立即返回，不会阻塞。不过，当数据已经到达内核空间的 socket 的等待队列后，用户进程依然要等待 recvfrom() 函数将数据从内核空间拷贝到用户空间，才会从 recvfrom() 系统调用函数中返回。

非阻塞 IO 模型解决了“**两次进程切换，两处阻塞，单进程对单连接**”中的“**两处阻塞**”问题，将“**两处阻塞**”变成了“**一处阻塞**”，但依然存在“**两次进程切换，一处阻塞，单进程对单连接**”的问题。

### 1.3 IO 多路复用

要解决“**两次进程切换，单进程对单连接**”的问题，服务器引入了 IO 多路复用技术，通过一个进程处理多个 TCP 连接，不仅降低了服务器处理网络请求的进程数，而且不用在每个连接的数据到达时就进行进程切换，进程可以一直运行并只处理有数据到达的连接，当然，如果要监听的所有连接都没有数据到达，进程还是会进入阻塞状态，直到某个连接有数据到达时被回调函数唤醒。

IO 多路复用模型如下图所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141455355885332.png)

从上图可知，系统调用 select 函数阻塞执行并返回数据就绪的连接个数，然后调用 recvfrom() 函数将到达内核空间的数据拷贝到用户空间，尽管这两个阶段都是阻塞的，但是由于只会处理有数据到达的连接，整体效率会有极大的提升。

到这里，阻塞 IO 模型的“**两次进程切换，两处阻塞，单进程对单连接**”问题，通过非阻塞 IO 和多路复用技术，就只剩下了“**一处阻塞**”这个问题，即 Linux 服务器上用户进程一定要等待数据从内核空间拷贝到用户空间，如果这个步骤也变成非阻塞的，也就是进程调用 recvfrom 后立刻返回，内核自行去准备好数据并将数据从内核空间拷贝到用户空间、再 notify 通知用户进程去读取数据，那就是 **IO 异步调用**，不过，Linux 没有提供异步 IO 的实现，真正意义上的网络异步 IO 是 Windows 下的 IOCP（IO 完成端口）模型，这里就不探讨了。

## **2. 详解 select、poll、epoll 实现原理**

### 2.1 select 实现原理

select 函数定义

Linux 提供的 select 函数的定义如下：

```
int select(    int nfds,                     // 监控的文件描述符集里最大文件描述符加1    fd_set *readfds,              // 监控有读数据到达文件描述符集合，引用类型的参数    fd_set *writefds,             // 监控写数据到达文件描述符集合，引用类型的参数    fd_set *exceptfds,            // 监控异常发生达文件描述符集合，引用类型的参数    struct timeval *timeout);     // 定时阻塞监控时间
```

readfds、writefds、errorfds 是三个文件描述符集合。select 会遍历每个集合的前 nfds 个描述符，分别找到可以读取、可以写入、发生错误的描述符，统称为“就绪”的描述符。然后用找到的子集替换这三个引用参数中的对应集合，返回所有就绪描述符的数量。

timeout 参数表示调用 select 时的阻塞时长。如果所有 fd 文件描述符都未就绪，就阻塞调用进程，直到某个描述符就绪，或者阻塞超过设置的 timeout 后，返回。如果 timeout 参数设为 NULL，会无限阻塞直到某个描述符就绪；如果 timeout 参数设为 0，会立即返回，不阻塞。

**文件描述符 fd**

文件描述符（file descriptor）是一个非负整数，从 0 开始。进程使用文件描述符来标识一个打开的文件。Linux 中一切皆文件。

系统为每一个进程维护了一个文件描述符表，表示该进程打开文件的记录表，而**文件描述符实际上就是这张表的索引**。每个进程默认都有 3 个文件描述符：0 (stdin)、1 (stdout)、2 (stderr)。

**socket**

socket 可以用于同一台主机的不同进程间的通信，也可以用于不同主机间的通信。操作系统将 socket 映射到进程的一个文件描述符上，进程就可以通过读写这个文件描述符来和远程主机通信。

socket 是进程间通信规则的高层抽象，而 fd 提供的是底层的具体实现。socket 与 fd 是一一对应的。通过 socket 通信，实际上就是通过文件描述符 fd 读写文件。

本文中，为了方便理解，可以认为 socket 内核对象 ≈ fd 文件描述符 ≈ TCP 连接。

**fd_set 文件描述符集合**

select 函数参数中的 fd_set 类型表示文件描述符的集合。

由于文件描述符 fd 是一个从 0 开始的无符号整数，所以可以使用 fd_set 的二进制每一位来表示一个文件描述符。某一位为 1，表示对应的文件描述符已就绪。比如比如设 fd_set 长度为 1 字节，则一个 fd_set 变量最大可以表示 8 个文件描述符。当 select 返回 fd_set = 00010011 时，表示文件描述符 1、2、5 已经就绪。

**fd_set 的 API**

fd_set 的使用涉及以下几个 API：

```
#include <sys/select.h>int FD_ZERO(int fd, fd_set *fdset);  // 将 fd_set 所有位置 0int FD_CLR(int fd, fd_set *fdset);   // 将 fd_set 某一位置 0int FD_SET(int fd, fd_set *fd_set);  // 将 fd_set 某一位置 1int FD_ISSET(int fd, fd_set *fdset); // 检测 fd_set 某一位是否为 1
```

**select 监听多个连接的用法**

服务端使用 select 监控多个连接的 C 代码是：

```
#define MAXCLINE 5       // 连接队列中的个数int fd[MAXCLINE];        // 连接的文件描述符队列int main(void){      sock_fd = socket(AF_INET,SOCK_STREAM,0)          // 建立主机间通信的 socket 结构体      .....      bind(sock_fd, (struct sockaddr *)&server_addr, sizeof(server_addr);         // 绑定socket到当前服务器      listen(sock_fd, 5);  // 监听 5 个TCP连接      fd_set fdsr;         // bitmap类型的文件描述符集合，01100 表示第1、2位有数据到达      int max;      for(i = 0; i < 5; i++)      {          .....          fd[i] = accept(sock_fd, (struct sockaddr *)&client_addr, &sin_size);   // 跟 5 个客户端依次建立 TCP 连接，并将连接放入 fd 文件描述符队列      }      while(1)               // 循环监听连接上的数据是否到达      {        FD_ZERO(&fdsr);      // 对 fd_set 即 bitmap 类型进行复位，即全部重置为0        for(i = 0; i < 5; i++)        {             FD_SET(fd[i], &fdsr);      // 将要监听的TCP连接对应的文件描述符所在的bitmap的位置置1，比如 0110010110 表示需要监听第 1、2、5、7、8个文件描述符对应的 TCP 连接        }        ret = select(max + 1, &fdsr, NULL, NULL, NULL);  // 调用select系统函数进入内核检查哪个连接的数据到达        for(i=0;i<5;i++)        {            if(FD_ISSET(fd[i], &fdsr))      // fd_set中为1的位置表示的连接，意味着有数据到达，可以让用户进程读取            {                ret = recv(fd[i], buf,sizeof(buf), 0);                ......            }        }  }
```

从注释中，我们可以看到，在一个进程中使用 select 监控多个连接的主要步骤是：

1）调用 socket() 函数建立主机间通信的 socket 结构体，bind() 绑定 socket 到当前服务器，listen() 监听五个 TCP 连接；

2）调用 accept() 函数建立和 5 个客户端的 TCP 连接，并把连接的文件描述符放入 fd 文件描述符队列；

3） 定义一个 fd_set 类型的变量 fdsr；

4）调用 FD_ZERO，将 fdsr 所有位置 0；

5）调用 FD_SET，将 fdsr 要监听的几个文件描述符的位置 1，表示要监听这几个文件描述符指向的连接；

6）调用 select() 函数，并将 fdsr 参数传递给 select；

7）select 会将 fdsr 中就绪的位置 1，未就绪的位置 0，返回就绪的文件描述符的数量；

8）当 select 返回后，调用 FD_ISSET 检测哪些位为 1，表示对应文件描述符对应的连接的数据已经就绪，可以调用 recv 函数读取该连接的数据了。

**select 的执行过程**

在服务器进程 A 启动的时候，要监听的连接的 socket 文件描述符是 3、4、5，如果这三个连接均没有数据到达网卡，则进程 A 会让出 CPU，进入阻塞状态，同时会将进程 A 的进程描述符和被唤醒时用到的回调函数组成等待队列项加入到 socket 对象 3、4、5 的进程等待队列中，注意，这时 select 调用时，fdsr 文件描述符集会从用户空间拷贝到内核空间，如下图所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141455568949167.png)

当网卡接收到数据，然后网卡通过中断信号通知 CPU 有数据到达，执行中断程序，中断程序主要做了两件事：

1）将网络数据写入到对应 socket 的数据接收队列里面；

2）唤醒队列中的等待进程 A，重新将进程 A 放入 CPU 的运行队列中；

假设连接 3、5 有数据到达网卡，注意，这时 select 调用结束时，fdsr 文件描述符集会从内核空间拷贝到用户空间：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141456087271156.png)

**select 的缺点**

从上面两图描述的执行过程，可以发现 select 实现多路复用有以下缺点：

1.性能开销大

1）调用 select 时会陷入内核，这时需要将参数中的 fd_set 从用户空间拷贝到内核空间，select 执行完后，还需要将 fd_set 从内核空间拷贝回用户空间，高并发场景下这样的拷贝会消耗极大资源；（epoll 优化为不拷贝）

2）进程被唤醒后，不知道哪些连接已就绪即收到了数据，需要遍历传递进来的所有 fd_set 的每一位，不管它们是否就绪；（epoll 优化为异步事件通知）

3）select 只返回就绪文件的个数，具体哪个文件可读还需要遍历；（epoll 优化为只返回就绪的文件描述符，无需做无效的遍历）

2.同时能够监听的文件描述符数量太少。受限于 sizeof(fd_set) 的大小，在编译内核时就确定了且无法更改。一般是 32 位操作系统是 1024，64 位是 2048。（poll、epoll 优化为适应链表方式）

第 2 个缺点被 poll 解决，第 1 个性能差的缺点被 epoll 解决。

### 2.2 poll 实现原理

和 select 类似，只是描述 fd 集合的方式不同，poll 使用 pollfd 结构而非 select 的 fd_set 结构。

```
struct pollfd {    int fd;           // 要监听的文件描述符    short events;     // 要监听的事件    short revents;    // 文件描述符fd上实际发生的事件};
```

管理多个描述符也是进行轮询，根据描述符的状态进行处理，但 **poll 无最大文件描述符数量的限制**，**因其基于链表存储**。

select 和 poll 在内部机制方面并没有太大的差异。相比于 select 机制，poll 只是取消了最大监控文件描述符数限制，并没有从根本上解决 select 存在的问题。

### 2.3 epoll 实现原理

epoll 是对 select 和 poll 的改进，解决了“性能开销大”和“文件描述符数量少”这两个缺点，是性能最高的多路复用实现方式，能支持的并发量也是最大。

epoll 的特点是：

1）使用**红黑树**存储**一份**文件描述符集合，每个文件描述符只需在添加时传入一次，无需用户每次都重新传入；—— 解决了 select 中 fd_set 重复拷贝到内核的问题

2）通过异步 IO 事件找到就绪的文件描述符，而不是通过轮询的方式；

3）使用队列存储就绪的文件描述符，且会按需返回就绪的文件描述符，无须再次遍历；

epoll 的基本用法是：

```
int main(void){      struct epoll_event events[5];      int epfd = epoll_create(10);         // 创建一个 epoll 对象      ......      for(i = 0; i < 5; i++)      {          static struct epoll_event ev;          .....          ev.data.fd = accept(sock_fd, (struct sockaddr *)&client_addr, &sin_size);          ev.events = EPOLLIN;          epoll_ctl(epfd, EPOLL_CTL_ADD, ev.data.fd, &ev);  // 向 epoll 对象中添加要管理的连接      }      while(1)      {         nfds = epoll_wait(epfd, events, 5, 10000);   // 等待其管理的连接上的 IO 事件         for(i=0; i<nfds; i++)         {             ......             read(events[i].data.fd, buff, MAXBUF)         }  }
```

主要涉及到三个函数：

```
int epoll_create(int size);   // 创建一个 eventpoll 内核对象int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);   // 将连接到socket对象添加到 eventpoll 对象上，epoll_event是要监听的事件int epoll_wait(int epfd, struct epoll_event *events,               int maxevents, int timeout);      // 等待连接 socket 的数据是否到达
```

**epoll_create**

epoll_create 函数会创建一个 struct eventpoll 的内核对象，类似 socket，把它关联到当前进程的已打开文件列表中。

eventpoll 主要包含三个字段：

```
struct eventpoll { wait_queue_head_t wq;      // 等待队列链表，存放阻塞的进程 struct list_head rdllist;  // 数据就绪的文件描述符都会放到这里 struct rb_root rbr;        // 红黑树，管理用户进程下添加进来的所有 socket 连接        ......}
```

wq：等待队列，如果当前进程没有数据需要处理，会把当前进程描述符和回调函数 default_wake_functon 构造一个等待队列项，放入当前 wq 对待队列，软中断数据就绪的时候会通过 wq 来找到阻塞在 epoll 对象上的用户进程。

rbr：一棵红黑树，管理用户进程下添加进来的所有 socket 连接。

rdllist：就绪的描述符的链表。当有的连接数据就绪的时候，内核会把就绪的连接放到 rdllist 链表里。这样应用进程只需要判断链表就能找出就绪进程，而不用去遍历整棵树。

eventpoll 的结构如图 2.3 所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141456261978920.png)

**epoll_ctl**

epoll_ctl 函数主要负责把服务端和客户端建立的 socket 连接注册到 eventpoll 对象里，会做三件事：

1）创建一个 epitem 对象，主要包含两个字段，分别存放 socket fd 即连接的文件描述符，和所属的 eventpoll 对象的指针；

2）将一个数据到达时用到的回调函数添加到 socket 的进程等待队列中，注意，跟第 1.1 节的阻塞 IO 模式不同的是，这里添加的 socket 的进程等待队列结构中，只有回调函数，没有设置进程描述符，因为**在 epoll 中，进程是放在 eventpoll 的等待队列中**，等待被 epoll_wait 函数唤醒，而不是放在 socket 的进程等待队列中；

3）将第 1）步创建的 epitem 对象插入红黑树；

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141456385278412.png)

**epoll_wait**

epoll_wait 函数的动作比较简单，检查 eventpoll 对象的就绪的连接 rdllist 上是否有数据到达，如果没有就把当前的进程描述符添加到一个等待队列项里，加入到 eventpoll 的进程等待队列里，然后阻塞当前进程，等待数据到达时通过回调函数被唤醒。

当 eventpoll 监控的连接上有数据到达时，通过下面几个步骤唤醒对应的进程处理数据：

1）socket 的数据接收队列有数据到达，会通过进程等待队列的回调函数 ep_poll_callback 唤醒红黑树中的节点 epitem；

2）ep_poll_callback 函数将有数据到达的 epitem 添加到 eventpoll 对象的就绪队列 rdllist 中；

3）ep_poll_callback 函数检查 eventpoll 对象的进程等待队列上是否有等待项，通过回调函数 default_wake_func 唤醒这个进程，进行数据的处理；

4）当进程醒来后，继续从 epoll_wait 时暂停的代码继续执行，把 rdlist 中就绪的事件返回给用户进程，让用户进程调用 recv 把已经到达内核 socket 等待队列的数据拷贝到用户空间使用。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141456489832038.png)

## **3. 总结**

从阻塞 IO 到 epoll 的实现中，我们可以看到 **wake up 回调函数机制**被频繁的使用，至少有三处地方：一是阻塞 IO 中数据到达 socket 的等待队列时，通过回调函数唤醒进程，二是 epoll 中数据到达 socket 的等待队列时，通过回调函数 ep_poll_callback 找到 eventpoll 中红黑树的 epitem 节点，并将其加入就绪列队 rdllist，三是通过回调函数 default_wake_func 唤醒用户进程 ，并将 rdllist 传递给用户进程，让用户进程准确读取数据 。从中可知，这种回调机制能够定向准确的通知程序要处理的事件，而不需要每次都循环遍历检查数据是否到达以及数据该由哪个进程处理，提高了程序效率，在日常的业务开发中，我们也可以借鉴下这一机制。

**References**

[图解 | 深入揭秘 epoll 是如何实现 IO 多路复用的！](https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247484905&idx=1&sn=a74ed5d7551c4fb80a8abe057405ea5e&scene=21#wechat_redirect)

[图解 Linux 网络包接收过程](https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247484058&idx=1&sn=a2621bc27c74b313528eefbc81ee8c0f&scene=21#wechat_redirect)

[图解 | 深入理解高性能网络开发路上的绊脚石 - 同步阻塞网络 IO](https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247484834&idx=1&sn=b8620f402b68ce878d32df2f2bcd4e2e&scene=21#wechat_redirect)

[从 linux 源码看 socket 的阻塞和非阻塞](https://developer.aliyun.com/article/626844)

[Select、Poll、Epoll 详解](https://www.jianshu.com/p/722819425dbd/)

[你管这破玩意叫 IO 多路复用？](https://mp.weixin.qq.com/s?__biz=Mzk0MjE3NDE0Ng==&mid=2247494866&idx=1&sn=0ebeb60dbc1fd7f9473943df7ce5fd95&scene=21#wechat_redirect)

[I/O 多路复用，select / poll / epoll 详解](https://imageslr.com/2020/02/27/select-poll-epoll.html)

[大话 Select、Poll、Epoll](https://cloud.tencent.com/developer/article/1005481)

[IO 多路复用底层原理全解，select，poll，epoll，socket，系统中断，进程调度，系统调用](https://www.bilibili.com/video/BV1Ka4y177gs/?p=2&spm_id_from=pageDriver&vd_source=0c7b3e851a63cba48e0c6c9ad2c57629)

原文作者：mingguangtu，腾讯 IEG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/5xj42JPKG8o5T7hjXIKywg

# 【NO.60】十多年前祖传代码重构——从25万到5万行

> 近期，我们接管并重构了十多年前的 Query 理解祖传代码，代码量减少80%，性能、稳定性、可观测性都得到大幅度提升。本文将介绍重构过程中系统实现、DIFF修复、coredump 修复等方面的优化经验。

## **1. 背景**

### 1.1 接手

7 月份组织架构调整后，我们组接手了搜索链路中的 Query 理解基础模块，包括本次重构对象 Query Optimizer，负责 query 的分词、词权、紧密度、意图识别。

### 1.2 为什么重构

面对一份10年+历史包袱较重的代码，部分开发者认为“老项目和人有一个能跑就行”，不愿意对其做较大的改动，而我们选择重构，主要有这些原因：

- 生产工具落后，无法使用现代 C++，多项监控和 TRACE 能力缺失
- 单进程内存消耗巨大——114G
- 服务不定期出现耗时毛刺
- 进程启动需要 18 分钟
- 研效低下，一个简单的功能需要开发 3 人天

基于上述原因，也缘于我们热爱挑战、勇于折腾，我们决定进行拆迁式的重构。

## **2. 编码实现**

### 2.1 重写与复用

我们对老 QO 的代码做分析，综合考虑三个因素：是否在使用、是否Query理解功能、是否高频迭代，将代码拆分为四种处理类型：1、删除；2、lib库引入；3、子仓库引入；4、重写引入。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141503083109843.png)

### 2.2 整体架构

老服务代码架构堪称灾难，整体遵守“想到哪就写到哪，需要啥就拷贝啥”的设计原则，完全不考虑单一职责、接口隔离、最少知识、模块化、封装复用等。下图介绍老服务的抽象架构：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141503188492895.png)

请求进来先后执行 3 次分词：

1. 不带标点符号的分词结果，用于后续紧密度词权算子的计算输
2. 带标点符号的分词结果，用于后续基于规则的意图算子的计算输入
3. 不带标点符号的分词结果，用于最终结果 XML queryTokens 字段的输出

1 和 3 的唯一区别，就是调用内核分词的代码位置不同。

下一个环节，请求 Query 分词时，分词接口中竟然包含了 RPC 请求下游 GPU 模型服务获取意图。这是此服务迭代最频繁的功能块，当想要实验模型调整、增减意图时，需要在 QO 仓库进行实验参数解析，将参数万里长征传递到 word_segmentor 仓库的分词接口里，再根据参数修改 RPC 意图调用逻辑。一个简单参数实验，要修改 2个仓库中的多个模块。设计上不符合模块内聚的设计原理，会造成霰弹式代码修改，影响迭代效率，又因为 Query 分词是处理链路中的耗时最长步骤，不必要的串行增加了服务耗时，可谓一举三失。

除此之外，老服务还有其他各类问题：多个函数超过一千行，圈复杂度破百，接口定义 50 多个参数并且毫无注释，代码满地随意拷贝，从以下 CodeCC 扫描结果可见一斑：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141503324938396.png)
新的服务求追架构合理性，确保：

1. 类和函数实现遵守单一职责原则，功能内聚；
2. 接口设计符合最少知识原则，只传入所需数据；
3. 每个类、接口都附上功能注释，可读性高。

项目架构如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141503425028469.png)
CodeCC 扫描结果：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141503539770526.png)

### 2.3 核心实现

老服务的请求处理流程：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141504018010555.png)

老服务采用的是原始的线程池模型。服务启动时初始化 20 条线程，每条线程分别持有自身的分词和意图对象，监听任务池中的任务。服务接口收到请求则投入任务池，等待任意一条线程处理。单个请求的处理基本是串行执行，只少量并行处理了几类意图计算。

新服务中，我们实现了一套基于 tRPC Fiber 的简单 DAG 控制器：

1. 用算子数初始化 FiberLatch，初始化算子任务间的依赖关系
2. StartFiberDetached 启动无依赖的算子任务，FiberLatch Wait 等待全部算子完成
3. 算子任务完成时，FiberLatch -1 并更新此算子的后置算子的前置依赖数
4. 计算前置依赖数规 0 的任务，StartFiberDetached 启动任务

通过 DAG 调度，新服务的请求处理流程如下，最大化的提升了算子并行度，优化服务耗时：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141504133495172.png)

## **3. DIFF 抹平**

完成功能模块迁移开发后，我们进入 DIFF 测试修复期，确保新老模块产出的结果一致。原本预计一周的 DIFF 修复，实际花费三周。解决掉逻辑错误、功能缺失、字典遗漏、依赖版本不一致等问题。如何才能更快的修复 DIFF，我们总结了几个方面：DIFF 对比工具、DIFF 定位方法、常见 DIFF 原因。

### 3.1 DIFF 比对工具

工欲善其事必先利其器，通过比对工具找出存在 DIFF 的字段，再针对性地解决。由于老服务对外接口使用 XML 协议，我们开发基于 XML 比对的 DIFF 工具，并根据排查时遇到的问题，为工具增加了一些个性选项：基于XML解析的DIFF工具

我们根据排查时遇到的问题为工具增加了一些个性选项：

1. 支持线程数量与 qps 设置（一些 DIFF 问题可能在多线程下才能复现）
2. 支持单个 query 多轮比对（某些模块结果存在一定波动，譬如下游超时了或者每次计算浮点数都有一定差值，初期排查对每个query可重复请求 3-5 轮，任意一轮对上则认为无 DIFF ，待大块 DIFF 收敛后再执行单轮对比测试）
3. 支持忽略浮点数漂移误差
4. 在统计结果中打印出存在 DIFF 的字段名、字段值、原始 query 以便排查、手动跟踪复现

### 3.2 DIFF 定位方法

获取 DIFF 工具输出的统计结果后，接下来就是定位每个字段的 DIFF 原因。

#### **3.2.1 逻辑流梳理确认**

梳理计算该字段的处理流，确认是否有缺少处理步骤。对流程的梳理也有利于下面的排查。

#### **3.2.2 对处理流的多阶段查看输入输出**

一个字段的计算在处理流中一定是由多个阶段组成，检查各阶段的输入输出是否一致，以缩小排查范围，再针对性地到不一致的阶段排查细节。

例如原始的分词结果在 QO 上是调用分词库获得的，当发现最后返回的分词结果不一致时，首先查看该接口的输入与输出是否一致，如果输入输出都有 DIFF，那说明是请求处理逻辑有误，排查请求处理阶段；如果输出无 DIFF，但是最终结果有DIFF，那说明对结果的后处理中存在问题，再去排查后处理阶段。以此类推，采用二分法思想缩小排查范围，然后再到存在 DIFF 的阶段细致排查、检查代码。

查看 DIFF 常见有两种方式：日志打印比对， GDB 断点跟踪。采用日志打印的话，需要在新老服务同时加日志，发版启动服务，而老服务启动需要 18 分钟，排查效率较低。因此我们在排查过程中主要使用 GDB 深入到 so 库中打断点，对比变量值。

### 3.3 常见 DIFF 原因

#### **3.3.1 外部库的请求一致，输出不一致**

这是很头疼的 case，明明调用外部库接口输入的请求与老模块是完全一致的，但是从接口获取到的结果却是不一致，这种情况可能有以下原因：

1. 初始化问题：遗漏关键变量初始化、遗漏字典加载、加载的字典有误，都有可能会造成该类DIFF，因为外部库不一定会因为遗漏初始化而返回错误，甚至外部库的初始化函数加载错字典都不一定会返回 false，所以对于依赖文件数据这块需要细致检查，保证需要的初始化函数及对应字典都是正确的。

有时可能知道是初始化有问题，但找不到是哪里初始化有误，此时可以用 DIFF 的 query，深入到外部库的代码中去，新老两模块一起单步调试，看看结果从哪里开始出现偏差，再根据那附近的代码推测出可能原因。

1. 环境依赖：外部库往往也会有很多依赖库，如果这些依赖库版本有 DIFF，也有可能会造成计算结果 DIFF。

#### **3.3.2 外部库的输出一致，处理后结果不一致**

这种情况即是对结果的后处理存在问题，如果确认已有逻辑无误，那可能原因是老模块本地会有一些调整逻辑 或 屏蔽逻辑，把从外部库拿出来原始结果结合其他算子结果进行本地调整。例如老 QO 中的百科词权，它的原始值是分词库出的词权，结合老 QO 本地的老紧密度算子进行了 3 次结果调整才得到最终值。

#### **3.3.3 将老模块代码重写后输出不一致**

重构过程中对大量的过时写法做重写，如果怀疑是重写导致的 DIFF，可以将原始函数替代掉重写的函数测一下，确认是重写函数带来的 DIFF 后，再细致排查，实在看不出可以在原始函数上一小块一小块的重写。

#### **3.3.4 请求输入不一致**

可能原因包括：

1. 缺少 query 预处理逻辑：例如 QO 输入分词库的 query 是将原始 query 的各短语经过空格分隔的，且去除了引号
2. query 编码有误：例如 QO 输入分词库的 query 的编码流程经过了：utf16le → gb13080 → gchar_t (内部自定义类型) → utf16le → char16_t
3. 缺少接口请求参数

#### **3.3.5 预期内的随机 DIFF**

某些库/业务逻辑自身存在预期内的不稳定，譬如排序时未使用 stable_sort，数组元素分数一致时，不能保证两次计算得出的 Top1 是同一个元素。遇到 DIFF 率较低的字段，需根据最终结果的输入值，结果计算逻辑排除业务逻辑预期内的 DIFF。

## **4. coredump 问题修复**

在进行 DIFF 抹平测试时，我们的测试工具支持多线程并发请求测试，等于同时也在进行小规模稳定性测试。在这段期间，我们基本每天都能发现新的 coredump 问题，其中部分问题较为罕见。下面介绍我们遇到的一些典型 CASE。

### 4.1 栈内存被破坏，变量值随机异常

如第 2 章所述，分词库属于不涉及 RPC 且未来不迭代的模块，我们将其在 GCC 8.3.1 下编译成 so 引入。在稳定性测试时，进程会在此库的多个不同代码位置崩溃。没有修改一行代码挂载的 so，为什么老 QO 能稳定运行，而我们会花式 coredump？本质上是因为此代码历史上未重视编译告警，代码存在潜藏漏洞，升级 GCC 后才暴露出来，主要是如下两种漏洞：

1. 定义了返回值的函数实际没有 return，栈内存数据异常
2. sprintf 越界，栈内存数据异常

排查这类问题时，需要综合上下文检查。以下图老 QO 代码为例：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141504306383710.png)

sprintf 将数字以 16 进制形式输出到 buf_1 ，输出内容占 8 个字节，加上 ‘\0’ 实际需 9 个字节，但 buf_1 和 buf_2 都只申请了 8 个字节的空间，此处将栈内存破坏，栈上的变量 query_words 值就异常了。

异常的表现形式为，while 循环的第一轮，query_words 的数组大小是 x，下一轮 while 循环时，还没有 push 元素，数组大小就变成了 y，因内存被写坏，导致异常新增了 y - x 个不明物体。在后续逻辑中，只要访问到这几个异常元素，就会发生崩溃。

光盯着 query_words 数组，发现不了问题，因为数组的变幻直接不符合基本法。解决此类问题，需联系上下文分析，最好是将代码单独提取出来，在单元测试/本地客户端测试复现，缩小代码范围，可以更快定位问题。而当代码量较少，编译器的 warning 提示也会更加明显，辅助我们定位问题。

上段代码的编译器提示信息如下：(开启了 -Werror 编译选项）

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141504399774003.png)

### 4.2 请求处理中使用了线程不安全的对象

在代码接手时，我们看到了老的分词模块“怪异”的初始化姿势：一部分数据模型的初始化函数定义为 static 接口，在服务启动时全局调用一次；另一部分则定义为类的 public 接口，每个处理线程中构造一个对象去初始化，为什么不统一定义为 static，在服务启动时进行初始化？每个线程都持有一个对象，不是会浪费内存吗？没有深究这些问题，我们也就错过了问题的答案：因为老的分词模块是线程不安全的，一个分词对象只能同时处理一个请求。

新服务的请求处理实现是，定义全局管理器，管理器内挂载一个唯一分词对象；请求进来后统一调用此分词对象执行分词接口。当 QPS 稍高，两个请求同时进入到线程不安全的函数内部时，就可能把内存数据写坏，进而发生 coredump。

为解决此问题，我们引入了 tRPC 内支持任务窃取的 MQ 线程池，利用 c++11 的 thread_local 特性，为线程池中的每个线程都创建线程私有的分词对象。请求进入后，往线程池内抛入分词任务，单个线程同时只处理一个请求，解决了线程安全问题。

### 4.3 tRPC 框架使用问题

#### **4.3.1 函数内局部变量较大 && v0.13.3 版 tRPC 无法正确设置栈大小**

稳定性测试过程中，我们发现服务会概率性的 coredump 在老朋友分词 so 里，20 个字以内的 Query 可以稳定运行，超过 20 个字则有可能会崩溃，但老服务的 Query 最大长度是 40 个字。从代码来看，函数中根据 Query 长度定义了不同长度的字节数组，Query 越长，临时变量占据内存越大，那么可能是栈空间不足，引发的 coredump。

根据这个分析，我们首先尝试使用 ulimit -s 命令调整系统栈大小限制，毫无效果。经过在码客上搜寻，了解到 tRPC Fiber 模型有独立的 stack size 参数，我们又满怀希望的给框架配置加上了 fiber stack size 属性，然而还是毫无效果。

无计可施之下，我们将崩溃处相关的函数提取到本地，分别用纯粹客户端（不使用 tRPC）, tRPC Future 模型， tRPC Fiber 模型承载这段代码逻辑，循环测试。结果只有 Fiber 模型的测试程序会崩溃，而 Future / 本地客户端的都可以稳定运行。

最后通过在码客咨询，得知我们选用的框架版本 Fiber Stack Size 设置功能恰好有问题，无法正确设置为业务配置值，升级版本后，问题解决。

#### **4.3.2 Redis 连接池模式，不能同时使用一应一答和单向调用的接口**

我们尝试打开结果缓存开关后，“惊喜”的发现新的 coredump，并且是 core 在了 tRPC 框架层。与 tRPC 框架开发同事协作排查，发现原因是 Redis 采取连接池模式连接时，不可同时使用一应一答接口和单向调用接口。而我们为了极致性能，在读取缓存执行 Get 命令时使用的是一应一答接口，在缓存更新执行 Set 命令时，采用的是单向调用方式，引发了 coredump。

快速解决此问题，我们将缓存更新执行 Set 命令也改为了应答调用，后续调优再改为异步 Detach 任务方式。

## **5. 重构效果**

最终，我们的成果如下：

【DIFF】

\- 算子功能结果无 DIFF

【性能】

\- 平均耗时：优化 28.4% （13.01 ms -> 9.31 ms）

\- P99 耗时：优化 16.7%（30ms -> 25ms）

\- 吞吐率：优化 12%（728qps—>832qps）

【稳定性】

\- 上游主调成功率从 99.7% 提升至 99.99% ，消除不定期的 P99 毛刺问题

\- 服务启动速度从 18 分钟 优化至 5 分钟

\- 可观察可跟踪性提升：建设服务主调监控，缓存命中率监控，支持 trace

\- 规范研发流程：单元测试覆盖率从 0% 提升至 60%+，建设完整的 CICD 流程

【成本】

\- 内存使用下降 40 G（114 GB -> 76 GB）

\- CPU 使用率：基本持平

\- 代码量：减少 80%（25 万行—> 5万行）

【研发效率】

\- 需求 LeadTime 由 3 天降低至 1 天内

附-性能压测：

（1）不带cache：新 QO 优化平均耗时 26%（13.199ms->9.71ms），优化内存 32%（114.47G->76.7G），提高吞吐率 10%（695qps->775qps）

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141504536918407.png)

（2）带cache：新 QO 优化平均耗时 28%（11.15ms->8.03ms），优化内存 33%（114G->76G），提高吞吐率 12%（728qps->832qps）

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212141505011217374.png)

## **6. 总结**

重构过程中遇到的各类编码问题及解决方案的分享就到这里。如果觉得本文对您有帮助，记得收藏点赞。

原文作者：gillyang，腾讯PCG后台开发工程师

原文链接：https://mp.weixin.qq.com/s/XFWX2hm_V952XYt7nd1HXA

# 【NO.61】字节跳动面试题汇总 -- C++后端（含答案）

**malloc和new的区别**

> new/delete 是 C++关键字，需要编译器支持。malloc/free 是库函数，需要头文件支持 使用 new 操作符申请内存分配时无须指定内存块的大小，编译器会根据类型信息自行计算。而 malloc 则需要显式地指出所需内存的尺寸 new 操作符内存分配成功时，返回的是对象类型的指针，类型严格与对象匹配，无须进行类型转换，故 new 是符合类型安全性的操作符。而 malloc 内存分配成功则是返回 void * ，需要通过强制类型转换将 void*指针转换成我们需要的类型 new 内存分配失败时，会抛出 bad_alloc 异常。malloc 分配内存失败时返回 NULL new 会先调用 operator new 函数，申请足够的内存（通常底层使用 malloc实现）。然后调用类型的构造函数，初始化成员变量，最后返回自定义类型指针。delete 先调用析构函数，然后调用 operator delete 函数释放内存（通常底层使用 free 实现）。malloc/free 是库函数，只能动态的申请和释放内存，无法强制要求其做自定义类型对象构造和析构工作

**排序算法时间复杂度**

![img](https://pic1.zhimg.com/80/v2-da40499c23c297a37f5906dfa4225544_720w.webp)

**如何用Linux shell命令统计一个文本中各个单词的个数**

more log.txt | tr ' ' '\n' | sort | uniq -c

**Linux下需要打开或者查看大文件**

> 查看文件的几行到几行 sed -n '10,10000p' log # 查看第10到10000行的数据

**linux 怎么查看进程，怎么结束进程？原理是什么？**

> top, kill 
> ps -e 查看进程详细信息

**Linux怎么查看当前的负载情况**

> uptime命令主要用于获取主机运行时间和查询linux系统负载等信息 
> cat /proc/loadavg 
> tload 
> top

**Http Code**

**MySQL的底层索引结构，InnoDB里面的B+Tree**

**不同引擎索引的区别**

**i++是否原子操作**

**锁的底层实现**

**B Tree 和 B+ Tree的区别**

**b树和b+树**

> B树（B-tree）是一种树状数据结构，它能够存储数据、对其进行排序并允许以O(log n)的时间复杂度运行进行查找、顺序读取、插入和删除的数据结构。B树，概括来说是一个节点可以拥有多于2个子节点的二叉查找树。与自平衡二叉查找树不同，B-树为系统最优化大块数据的读和写操作。B-tree算法减少定位记录时所经历的中间过程，从而加快存取速度。普遍运用在[数据库](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/solution/database%3Ffrom%3D10680)和文件系统。 B 树可以看作是对2-3查找树的一种扩展，即他允许每个节点有M-1个子节点。M阶B树具有以下特征：

1. 根节点至少有两个子节点
2. 每个节点有M-1个key，并且以升序排列
3. 位于M-1和M key的子节点的值位于M-1 和M key对应的Value之间
4. 其它节点至少有M/2个子节点

![img](https://pic4.zhimg.com/80/v2-9ba3e51bebf2cadc17fc56e10830699f_720w.webp)

> B+树 B+树是对B树的一种变形树，它与B树的差异在于：

- 有k个子结点的结点必然有k个关键码
- 非叶结点仅具有索引作用，跟记录有关的信息均存放在叶结点中
- 树的所有叶结点构成一个有序链表，可以按照关键码排序的次序遍历全部记录

![img](https://pic1.zhimg.com/80/v2-1cca15bc1a6c034144bc3bca58ab4a04_720w.webp)

> B树与B+树的区别：

- B树每个节点都存储数据，所有节点组成这棵树。B+树只有叶子节点存储数据（B+数中有两个头指针：一个指向根节点，另一个指向关键字最小的叶节点），叶子节点包含了这棵树的所有数据，所有的叶子结点使用链表相连，便于区间查找和遍历，所有非叶节点起到索引作用。
- B树中叶节点包含的关键字和其他节点包含的关键字是不重复的，B+树的索引项只包含对应子树的最大关键字和指向该子树的指针，不含有该关键字对应记录的存储地址。
- B树中每个节点（非根节点）关键字个数的范围为[m/2(向上取整)-1,m-1](https://link.zhihu.com/?target=https%3A//www.cnblogs.xn--com%5B1%2Cm-1%5D-1f5sr520auw0amq3d/)，并且具有n个关键字的节点包含（n+1）棵子树。B+树中每个节点（非根节点）关键字个数的范围为[m/2(向上取整),m](https://link.zhihu.com/?target=https%3A//www.cnblogs.xn--com%5B1%2Cm%5D-0x2pt90widxagg9c/)，具有n个关键字的节点包含（n）棵子树。
- B+树中查找，无论查找是否成功，每次都是一条从根节点到叶节点的路径。

> B树的优点:

- B树的每一个节点都包含key和value，因此经常访问的元素可能离根节点更近，因此访问也更迅速。

> B+树的优点：

- 所有的叶子结点使用链表相连，便于区间查找和遍历。B树则需要进行每一层的递归遍历。相邻的元素可能在内存中不相邻，所以缓存命中性没有B+树好。
- b+树的中间节点不保存数据，能容纳更多节点元素。

> B树与B+树的共同优点：

- 考虑磁盘IO的影响，它相对于内存来说是很慢的。数据库索引是存储在磁盘上的，当数据量大时，就不能把整个索引全部加载到内存了，只能逐一加载每一个磁盘页（对应索引树的节点）。所以我们要减少IO次数，对于树来说，IO次数就是树的高度，而“矮胖”就是b树的特征之一，m的大小取决于磁盘页的大小。

**MySQL索引的发展过程？是一来就是B+Tree的么？从 没有索引、hash、二叉排序树、AVL树、B树、B+树**

**MySQL里面的事务，说说什么是事务**

> 数据库事务(Database Transaction) ，是指作为单个逻辑工作单元执行的一系列操作，要么完全地执行，要么完全地不执行。 事务处理可以确保除非事务性单元内的所有操作都成功完成，否则不会永久更新面向数据的资源。通过将一组相关操作组合为一个要么全部成功要么全部失败的单元，可以简化错误恢复并使应用程序更加可靠。一个逻辑工作单元要成为事务，必须满足所谓的 ACID（原子性、一致性、隔离性和持久性）属性。事务是数据库运行中的逻辑工作单位，由 DBMS 中的事务管理子系统负责事务的处理。

**MySQL里面有那些事务级别，并且不同的事务级别会出现什么问题**

> 读未提交 (脏读)：最低的隔离级别，什么都不需要做，一个事务可以读到另一个事务未提交的结果。所有的并发事务问题都会发生 读提交 (读旧数据，不可重复读问题)：只有在事务提交后，其更新结果才会被其他事务看见。可以解决脏读问题。 可重复读 (解决了脏读但是有幻影读)：在一个事务中，对于同一份数据的读取结果总是相同的，无论是否有其他事务对这份数据进行操作，以及这个事务是否提交。可以解决脏读、不可重复读。 串行化：事务串行化执行，隔离级别最高，牺牲了系统的并发性。可以解决并发事务的所有问题。

**不可重复读和幻读的区别**

1. 不可重复读是读异常，但幻读则是写异常
2. 不可重复读是读异常的意思是，如果你不多select几次，你是发现不了你曾经select过的数据行已经被其他人update过了。避免不可重复读主要靠一致性快照
3. 幻读是写异常的意思是，如果不自己insert一下，你是发现不了其他人已经偷偷insert过相同的数据了。解决幻读主要靠间隙锁

**数据库持久性是怎么实现的？**

> [详细](https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_25448409/article/details/105376450) 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 MySQL采用了一种叫WAL（Write Ahead Logging）提前写日志的技术。意思就是说，发生了数据修改操作先写日志记录下来，等不忙的时候再持久化到磁盘。这里提到的日志就是redo log。 redo log称为重做日志，当有一条记录需要修改的时候，InnoDB引擎会先把这条记录写到redo log里面。redo log是物理格式日志，它记录的是对于每个页的修改。 redo log是由两部分组成的：一是内存中的重做日志缓冲（redo log buffer）;二是用来持久化的重做日志文件（redo log file）。为了消耗不必要的IO操作，事务再执行过程中产生的redo log首先会redo log buffer中，之后再统一存入redo log file刷盘进行持久化，这个动作称为fsync binlog记录了mysql执行更改了所有操作，但不包含select和show这类本对数据本身没有更改的操作。但是不是说对数据本身没有修改就不会记录binlog日志。

- binlog是mysql自带的，他会记录所有存储引擎的日志文件。而redo log是InnoDB特有的，他只记录该存储引擎产生的日志文件
- binlog是逻辑日志，记录这个语句具体操作了什么内容。Redo log是物理日志，记录的是每个页的更改情况
- redo log是循环写，只有那么大的空间。binlog采用追加写入，当一个binlog文件写到一定大小后会切换到下一个文件

**更新一条语句的流程**

1. 首先执行器调用引擎获取数据，如果数据在内存中就直接返回；否则先从磁盘中读取数据，写入内存后再返回。
2. 修改数据后再调用引擎接口写入这行数据
3. 引擎层将这行数据更新到内存中，然后将更新操作写入redo log，这时候redo log标记为prepare状态。然后告诉执行器我处理完了，可以提交事务了。
4. 执行器生成这个操作的binlog，并把binlog写入磁盘，然后调用引擎提交事务
5. 引擎收到commit命令后，把刚才写入的redo log改成commit状态 这里使用了两阶段提交prepare阶段和commit阶段



**数据库回表是什么？**

> [详细](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/hnzkljq/p/12620971.html) Innodb的索引存在两类，一类是聚簇索引一类是非聚簇索引，Innodb有且仅有一个聚簇索引。1. 如果表定义了PK，则PK就是聚集索引；2. 如果表没有定义PK，则第一个not NULL unique列是聚集索引；否则，InnoDB会创建一个隐藏的row-id作为聚集索引； InnoDB 聚集索引 的叶子节点存储行记录，而普通索引的叶子节点存储主键值 在使用聚簇索引时，可以一步直接获取到记录值，而使用普通索引时，会首先获取记录的PK，然后再从聚簇索引中查找对应的记录，这个过程叫做回表

**索引覆盖**

> 理解方式一：就是select的数据列只用从索引中就能够取得，不必读取数据行，换句话说查询列要被所建的索引覆盖 理解方式二：索引是高效找到行的一个方法，但是一般数据库也能使用索引找到一个列的数据，因此它不必读取整个行。毕竟索引叶子节点存储了它们索引的数据；当能通过读取索引就可以得到想要的数据，那就不需要读取行了。一个索引包含了（或覆盖了）满足查询结果的数据就叫做覆盖索引 理解方式三：是非聚集复合索引的一种形式，它包括在查询里的Select、Join和Where子句用到的所有列（即建索引的字段正好是覆盖查询条件中所涉及的字段，也即，索引包含了查询正在查找的数据）。 总结：要查找的数据可以都在索引中出现，而不需要再去查表获取完整记录 为了实现索引覆盖可以将被查询的字段，建立到联合索引里去

**数据库读写锁发生死锁的情景**

> [详细](https://link.zhihu.com/?target=https%3A//blog.csdn.net/Hpsyche/article/details/102076870)

- MyISAM中不会出现死锁 在MyISAM中只用到表锁，不会有死锁的问题，锁的开销也很小，但是相应的并发能力很差。 解析：MyISAM不支持事务，即每次的读写都会隐性的加上读写锁，而我们知道读锁是共享的，写锁是独占的，意味着当一个Session在写时，另一个Session必须等待。
- InnoDB中会出现死锁 InnoDB中实用了行锁和表锁，当未命中索引时，会自动退化为表锁。 解决方法为InnoDB中的MVCC机制

**为什么推荐主键使用自增的整型，MySQL为什么主键自增好**

> 为什么推荐主键：Innodb底层是B+树，数据和索引放在一起，因此需要一个主键作为索引，从而存储数据 为什么要自增：当新存储一条数据时，只需要向B+树后面的叶子节点插入即可，而不需要B+ 树为保持有序而进行旋转 为什么要整型：整形作为索引，容易直接判断大小而保持有序，使用String，相对于整数而言，不易判断大小

**vector的底层实现，扩容机制**

> [详细](https://link.zhihu.com/?target=http%3A//c.biancheng.net/view/6901.html) 使用一段连续的内存来存储数据，同时在数据结构中保存了三个指针来标记内存地址，首先是指向vector中数据的起始位置的_Myfirst指针，最后一个数据的位置的_Mylst指针，以及一个指向连续内存空间末尾的_Myend指针 当 vector 的大小和容量相等（size==capacity）也就是满载时，如果再向其添加元素，那么 vector 就需要扩容。vector 容器扩容的过程需要经历以下 3 步：

1. 完全弃用现有的内存空间，重新申请更大的内存空间；
2. 将旧内存空间中的数据，按原有顺序移动到新的内存空间中；
3. 最后将旧的内存空间释放 不同的编译器，vector 有不同的扩容大小。在 vs 下是 1.5 倍，在 GCC 下是 2 倍 采用成倍方式扩容，可以保证常数的时间复杂度，而增加指定大小的容量只能达到O(n)的时间复杂度，因此，使用成倍的方式扩容 过大的倍数将导致大量空间的浪费 [为什么扩容二倍](https://link.zhihu.com/?target=https%3A//blog.csdn.net/bryant_xw/article/details/89524910)

**MySQL中如果使用like进行模糊匹配的时候，是否会使用索引**

> mysql在使用like查询的时候只有使用后面的%时，才会使用到索引

**Volatile的作用，Volatile如何保证可见性的？以及如何实现可见性的机制**

> volatile 关键字是一种类型修饰符，用它声明的类型变量表示可以被某些编译器未知的因素更改，比如：操作系统、硬件或者其它线程等。遇到这个关键字声明的变量，编译器对访问该变量的代码就不再进行优化，从而可以提供对特殊地址的稳定访问。声明时语法：int volatile vInt; 当要求使用 volatile 声明的变量的值的时候，系统总是重新从它所在的内存读取数据，即使它前面的指令刚刚从该处读取过数据。而且读取的数据立刻被保存 volatile 用在如下的几个地方：

1. 中断服务程序中修改的供其它程序检测的变量需要加 volatile；
2. 多任务环境下各任务间共享的标志应该加 volatile；
3. 存储器映射的硬件寄存器通常也要加 volatile 说明，因为每次对它的读写都可能有不同意义 [详细](https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_44363885/article/details/92838607). volatile作用： 锁总线，其它CPU对内存的读写请求都会被阻塞，直到锁释放，不过实际后来的处理器都采用锁缓存替代锁总线，因为锁总线的开销比较大，锁总线期间其他CPU没法访问内存 lock后的写操作会回写已修改的数据，同时让其它CPU相关缓存行失效，从而重新从主存中加载最新的数据 不是内存屏障却能完成类似内存屏障的功能，阻止屏障两遍的指令重排序

**如果大量的使用Volatile存在什么问题**

**操作系统的线程，以及它的状态**

> 线程的基本状态： 1.新建 new语句创建的线程对象处于新建状态，此时它和其他java对象一样，仅被分配了内存。 2.等待 当线程在new之后，并且在调用start方法前，线程处于等待状态。 3.就绪 当一个线程对象创建后，其他线程调用它的start()方法，该线程就进入就绪状态。处于这个状态的线程位于Java虚拟机的可运行池中，等待cpu的使用权。 4.运行状态 处于这个状态的线程占用CPU，执行程序代码。在并发运行环境中，如果计算机只有一个CPU，那么任何时刻只会有一个线程处于这个状态。只有处于就绪状态的线程才有机会转到运行状态。 5. 阻塞状态 阻塞状态是指线程因为某些原因放弃CPU，暂时停止运行。当线程处于阻塞状态时，Java虚拟机不会给线程分配CPU，直到线程重新进入就绪状态，它才会有机会获得运行状态。 6.死亡状态 当线程执行完run()方法中的代码，或者遇到了未捕获的异常，就会退出run()方法，此时就进入死亡状态，该线程结束生命周期。

**进程和线程(进程与线程)的区别以及使用场景**

> 线程产生的原因：进程可以使多个程序能并发执行，以提高资源的利用率和系统的吞吐量；但是其具有一些缺点：

- 进程在同一时间只能干一件事
- 进程在执行的过程中如果阻塞，整个进程就会挂起，即使进程中有些工作不依赖于等待的资源，仍然不会执行。
- 因此，操作系统引入了比进程粒度更小的线程，作为并发执行的基本单位，从而减少程序在并发执行时所付出的时空开销，提高并发性

> 进程是资源分配的最小单位，线程是操作系统进行执行和调度的最小单位

1. 同一进程的线程共享本进程的地址空间，而进程之间则是独立的地址空间；
2. 同一进程内的线程共享本进程的资源，但是进程之间的资源是独立的；
3. 一个进程崩溃后，在保护模式下不会对其他进程产生影响，但是一个线程崩溃整个进程崩溃，所以多进程比多线程健壮；
4. 进程切换，消耗的资源大。所以涉及到频繁的切换，使用线程要好于进程；
5. 两者均可并发执行；
6. 每个独立的进程有一个程序的入口、程序出口。但是线程不能独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制

使用场景

- 需要频繁创建销毁的优先用线程 最常见的应用就是 Web 服务器了，来一个连接建立一个线程，断了就销毁线程
- 需要进行大量计算的优先使用线程 所谓大量计算，当然就是要耗费很多 CPU，切换频繁了，这种情况下线程是最合适的 最常见的是图像处理、算法处理
- 强相关的处理用线程，弱相关的处理用进程
- 可能要扩展到多机分布的用进程，多核分布的用线程

> 线程私有：线程栈，寄存器，程序寄存器 共享：堆，地址空间，全局变量，静态变量 进程私有：地址空间，堆，全局变量，栈，寄存器 共享：代码段，公共数据，进程目录，进程 ID

**为什么线程创建和撤销开销大**

> 当从一个线程切换到另一个线程时，不仅会发生线程上下文切换，还会发生特权模式切换。 既然是线程切换，那么一定涉及线程状态的保存和恢复，包括寄存器、栈等私有数据。另外，线程的调度是需要内核级别的权限的（操作CPU和内存），也就是说线程的调度工作是在内核态完成的，因此会有一个从用户态到内核态的切换。而且，不管是线程本身的切换还是特权模式的切换，都要进行CPU的上下文切换

**线程和协程的由来和作用**

> 协程，又称微线程。协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。 和多线程比，协程最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显 第二大优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多 在协程上利用多核 CPU —— 多进程+协程，既充分利用多核，又充分发挥协程的高效率， 可获得极高的性能

**多线程的通信和同步，多线程访问同一个对象怎么办**

- 互斥锁
- 条件变量
- 读写锁
- 信号

**查看端口号或者进程号，使用什么命令**

> 查看程序对应的进程号： ps -ef | grep 进程名字 查看进程号所占用的端口号： netstat -nltp | grep 进程号 查看端口号所使用的进程号: lsof -i:端口号

**信号量与mutex和自旋锁的区别**

> 信号量（semaphore）用在多线程多任务同步的，一个线程完成了某一个动作就通过信号量告诉别的线程，别的线程再进行某些动作。而互斥锁（Mutual exclusion，缩写 Mutex）是用在多线程多任务互斥的，一个线程占用了某一个资源，那么别的线程就无法访问，直到这个线程unlock，其他的线程才开始可以利用这个资源 自旋锁与前两者的区别是自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁

**非对称加密与对称加密**

> 对称加密算法：加密效率高，速度快，适合大数据量加密。DES/AES 非对称加密算法：算法复杂，加密速度慢，安全性更高。结合对称加密使用。RSA、DH

**在两列属性上分别建索引，则某个查询语句使用 where att1 = \* and att2 = \* 会怎么使用索引**

> 一次查询只使用一个索引，因为每个索引都代表了一颗树，使用多个索引所带来的增益远小于增加的性能消耗。 对于联合索引来说会使用最左匹配 -- 在MySQL的user表中，对a,b,c三个字段建立联合索引，根据查询字段的位置不同来决定，如查询a, a,b a,b,c a,c 都可以走索引，其他条件的查询不能走索引 对于多个单列索引来说，MySQL会试图选择一个限制最严格的索引。但是，即使是限制最严格的单列索引，它的限制能力也肯定远远低于在多列上的多列索引

**通过两个索引查询出来的结果，会进行什么要的操作？交集，并集**

**MySQL中遇到一些慢查询，有什么解决方法**

- 定位慢查询

> 根据慢查询日志定位慢查询sql

1. slow_query_log 默认是off关闭的，使用时，需要改为on 打开
2. slow_query_log_file 记录的是慢日志的记录文件
3. long_query_time 默认是10S，每次执行的sql达到这个时长，就会被记录

- 优化方案

> 优化数据库结构 分解关联查询: 很多高性能的应用都会对关联查询进行分解，就是可以对每一个表进行一次单表查询，然后将查询结果在应用程序中进行关联，很多场景下这样会更高效。 增加索引 建立视图 优化查询语句 添加存储过程 冗余保存数据

**有了解过IO多路复用技术是个什么样的原理**

> I/O多路复用，I/O就是指的我们网络I/O,多路指多个TCP连接(或多个Channel)，复用指复用一个或少量线程。串起来理解就是很多个网络I/O复用一个或少量的线程来处理这些连接。

**通过一个线程，同时连接多个线程会不会存在多个线程切换**

**在操作系统中，有高速缓存，主存，虚拟内存，外存，有知道它们之间有什么样的关系，以及它们的作用是啥**

> 缓存： 在CPU同时处理很多数据，而又不可能同时进行所有数据的传输的情况，把优先级低的数据暂时放入缓存中，等优先级高的数据处理完毕后再把它们从缓存中拿出来进行处理 主存：主存就是内存，是直接与CPU交换信息的存储器，指CPU能够通过指令中的地址码直接访问的存储器，常用于存放处于活动状态的程序和数据 虚拟内存：当运行数据超过内存限度，部分数据自动“溢出”，这时系统会将硬盘上的部分空间模拟成内存——虚拟内存，并且将暂时不运行的程序或不使用的数据存放到虚拟内存中等待需要时调用 辅存就是外存： 硬盘与磁盘、光盘、软盘、U盘等

**缺页的产生和换页算法**

> 缺页中断：进程线性地址空间里的页面不必常驻内存，在执行一条指令时，如果发现他要访问的页没有在内存中（存在位为0），那么停止该指令的执行，并产生一个页不存在异常，对应的故障处理程序可通过从外存加载该页到内存的方法来排除故障，之后，原先引起的异常的指令就可以继续执行，而不再产生异常 页面置换算法：将新页面调入内存时，如果内存中所有的物理页都已经分配出去，就要按某种策略来废弃某个页面，将其所占据的物理页释放出来，好的算法，让缺页率降低。

1. 先进先出调度算法（FIFO）
2. 最近最少调度算法（LFU，根据时间判断）：利用局部性原理，根据一个作业在执行过程中过去的页面访问历史来推测未来的行为。它认为过去一段时间里不曾被访问过的页面，在最近的将来可能也不会再被访问。所以，这种算法的实质是：当需要淘汰一个页面时，总是选择在最近一段时间内最久不用的页面予以淘汰。
3. 最近最不常用调度算法（LRU，根据使用频率判断

```text
struct DLinkedNode {
    int key, value;
    DLinkedNode* prev;
    DLinkedNode* next;
    DLinkedNode(): key(0), value(0), prev(nullptr), next(nullptr) {}
    DLinkedNode(int _key, int _value): key(_key), value(_value), prev(nullptr), next(nullptr) {}
};

class LRUCache {
private:
    unordered_map<int, DLinkedNode*> cache;
    DLinkedNode* head;
    DLinkedNode* tail;
    int size;
    int capacity;

public:
    LRUCache(int _capacity): capacity(_capacity), size(0) {
        // 使用伪头部和伪尾部节点
        head = new DLinkedNode();
        tail = new DLinkedNode();
        head->next = tail;
        tail->prev = head;
    }
    
    int get(int key) {
        if (!cache.count(key)) {
            return -1;
        }
        // 如果 key 存在，先通过哈希表定位，再移到头部
        DLinkedNode* node = cache[key];
        moveToHead(node);
        return node->value;
    }
    
    void put(int key, int value) {
        if (!cache.count(key)) {
            // 如果 key 不存在，创建一个新的节点
            DLinkedNode* node = new DLinkedNode(key, value);
            // 添加进哈希表
            cache[key] = node;
            // 添加至双向链表的头部
            addToHead(node);
            ++size;
            if (size > capacity) {
                // 如果超出容量，删除双向链表的尾部节点
                DLinkedNode* removed = removeTail();
                // 删除哈希表中对应的项
                cache.erase(removed->key);
                // 防止内存泄漏
                delete removed;
                --size;
            }
        }
        else {
            // 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部
            DLinkedNode* node = cache[key];
            node->value = value;
            moveToHead(node);
        }
    }

    void addToHead(DLinkedNode* node) {
        node->prev = head;
        node->next = head->next;
        head->next->prev = node;
        head->next = node;
    }
    
    void removeNode(DLinkedNode* node) {
        node->prev->next = node->next;
        node->next->prev = node->prev;
    }

    void moveToHead(DLinkedNode* node) {
        removeNode(node);
        addToHead(node);
    }

    DLinkedNode* removeTail() {
        DLinkedNode* node = tail->prev;
        removeNode(node);
        return node;
    }
};
```

1. 最佳置换算法（OPT）:从主存中移出永远不再需要的页面；如无这样的页面存在，则选择最长时间不需要访问的页面。于所选择的被淘汰页面将是以后永不使用的，或者是在最长时间内不再被访问的页面，这样可以保证获得最低的缺页率

**面向对象采用的设计模式有哪些**

- 在软件工程中，软件设计模式是通用的，可重用的在给定上下文中解决软件设计中常见问题的解决方案

> 单例模式(有的叫单元素模式,单态模式） 工厂模式 观察者模式 命令链模式 策略模式

**设计模式的六大原则**

**为什么要有补码？** （为了更方便的实现减法运算）

**一致性哈希**

**面向对象有哪些设计原则**

> OCP原则（也叫开闭原则）: 开闭原则就是说对扩展开放，对修改关闭。 SRP原则（职责单一原则）: 一个类只负责一项职责，可以降低类的复杂度，提高类的可读性，提高系统的可维护性，当修改一个功能时，可以显著降低对其他功能的影响。 OCP原则(里氏替换原则)：任何基类可以出现的地方，子类一定可以出现。通俗的理解即为子类可以扩展父类的功能，但不能改变父类原有的功能。 DIP原则（依赖倒置原则）：高层模块不应该依赖低层模块，二者都应该依赖其抽象；抽象不应该依赖细节；细节应该依赖抽象。通俗点说：要求对抽象进行编程，不要对实现进行编程，这样就降低了客户与实现模块间的耦合。 LoD法则（迪米特法则）：一个对象应该对其他对象保持最少的了解。通俗的来讲，就是一个类对自己依赖的类知道的越少越好。也就是说，对于被依赖的类来说，无论逻辑多么复杂，都尽量地的将逻辑封装在类的内部，对外除了提供的public方法，不对外泄漏任何信息。 接口隔离原则：建立单一接口，不要建立庞大臃肿的接口，尽量细化接口，接口中的方法尽量少。也就是说，我们要为各个类建立专用的接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。

**说说你对于 “不要用共享内存来通信，而应该用通信来共享内存” 的理解**

> 在锁模式中，一块内存可以被多个线程同时看到，所以叫共享内存。线程之间通过改变内存中的数据来通知其他线程发生了什么，所以是通过共享内存来通信。锁是为了保护一个线程对内存操作的逻辑完整性而引入的一种约定，注意是一种约定而不是规则（一个线程可以不获取锁就操作内存，也可以解锁其他线程加的锁从而破坏保护，这种错误很难发现）。这种约定要每个线程的编写人员自觉遵守，否则就会出现多线程问题，如数据被破坏，死锁，饥饿等。 在go模式中，一块内存同一时间只能被一个线程看到，另外一个线程要操作这块内存，需要当前线程让渡所有权，这个所有权的让渡过程是“通信”。通信的原子性由channel封装好了，内存同一时间只能被同一线程使用，所以这种模式下不需要显示的锁。然而go模式也有约定，如果传递的是内存的指针，或者是控制消息，还是等于共享了内存，还是要保证将所有权让渡后, 不能再操作这块内存。

**什么是双向链表**

> 双向链表也叫双链表，是链表的一种，它的每个数据结点中都有两个指针，分别指向直接后继和直接前驱。所以，从双向链表中的任意一个结点开始，都可以很方便地访问它的前驱结点和后继结点。

**操作系统内存管理（分页、分段、段页式）**

> 页式内存管理，内存分成固定长度的一个个页片。操作系统为每一个进程维护了一个从虚拟地址到物理地址的映射关系的数据结构，叫页表，页表的内容就是该进程的虚拟地址到物理地址的一个映射。页表中的每一项都记录了这个页的基地址。通过页表，由逻辑地址的高位部分先找到逻辑地址对应的页基地址，再由页基地址偏移一定长度就得到最后的物理地址，偏移的长度由逻辑地址的低位部分决定。一般情况下，这个过程都可以由硬件完成，所以效率还是比较高的。 页式内存管理的优点就是比较灵活，内存管理以较小的页为单位，方便内存换入换出和扩充地址空间。 分段存储管理方式的目的，主要是为了满足用户（程序员）在编程和使用上多方面的要求，其中有些要求是其他几种存储管理方式所难以满足的。因此，这种存储管理方式已成为当今所有存储管理方式的基础

- 分页与分段的区别 页是信息的物理单位，分页是为实现离散分配方式，以消减内存的外零头，提高内存的利用率。或者说，分页仅仅是由于系统管理的需要而不是用户的需要。段则是信息的逻辑单位，它含有一组其意义相对完整的信息。分段的目的是为了能更好地满足用户的需要 页的大小固定且由系统决定，而段的长度却不固定 分页的作业地址空间是一维的，即单一的线性地址空间；而分段的作业地址空间则是二维的 分页系统能有效地提高内存利用率，而分段系统则能很好地满足用户需求。

> 段页式系统的基本原理，是分段和分页原理的结合，即先将用户程序分成若干个段，再把每个段分成若干个页，并为每一个段赋予一个段名。在段页式系统中，地址结构由段号、段内页号和页内地址三部分所组成。

**浏览器输入网址到渲染的过程**

> [详细](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/xiaohuochai/p/9193083.html)

- 浏览器构建HTTP Request请求
- 网络传输
- 服务器构建HTTP Response 响应
- 网络传输
- 浏览器渲染页面

> DNS解析URL地址、生成HTTP请求报文、构建TCP连接、使用IP协议选择传输路线、数据链路层保证数据的可靠传输、物理层将数据转换成电子、光学或微波信号进行传输

**DNS解析过程**

- DNS 协议运行在 UDP 协议之上，使用端口号 53，用于将域名转换为IP地址

1. 浏览器先检查自身缓存中有没有被解析过这个域名对应的 ip 地址
2. 如果浏览器缓存没有命中，浏览器会检查操作系统缓存中有没有对应的已解析过的结果。在 windows 中可通过 c 盘里 hosts 文件来设置
3. 还没命中，请求本地域名服务器来解析这个域名，一般都会在本地域名服务器找到
4. 本地域名服务器没有命中，则去根域名服务器请求解析
5. 根域名服务器返回给本地域名服务器一个所查询域的主域名服务器
6. 本地域名服务器向主域名服务器发送请求
7. 接受请求的主域名服务器查找并返回这个域名对应的域名服务器的地址
8. 域名服务器根据映射关系找到 ip 地址，返回给本地域名服务器
9. 本地域名服务器缓存这个结果
10. 本地域名服务器将该结果返回给用户

**https讲一下？密钥怎么交换的？私钥存储在哪里**

> HTTPs 是以安全为目标的 HTTP 通道，简单讲是 HTTP 的安全版，即HTTP 下加入 SSL 层，HTTPS 的安全基础是 SSL，因此加密的详细内容就需要 SSL HTTPs的握手过程包含五步

1. 浏览器请求连接
2. 服务器返回证书：证书里面包含了网站地址，加密公钥，以及证书的颁发机构等信息，服务器采用非对称加密算法（RSA）生成两个秘钥，私钥自己保留
3. 浏览器收到证书后作以下工作： 3.1 验证证书的合法性 3.2 生成随机（对称）密码，取出证书中提供的公钥对随机密码加密；浏览器即客户端使用非对称加密来加密对称加密规则，对称加密用于加密后续传输的信息 3.3 将之前生成的加密随机密码等信息发送给网站
4. 服务器收到消息后作以下的操作 4.1 使用自己的私钥解密浏览器用公钥加密后的消息，并验证 HASH 是否与浏览器发来的一致；获得浏览器发过来的对称秘钥 4.2 使用加密的随机对称密码加密一段消息，发送给浏览器
5. 浏览器解密并计算握手消息的 HASH：如果与服务端发来的 HASH 一致，此时握手过程结束，之后进行通信

**http的流程**

> 每个万维网的网点都有一个服务器进程，它不断的监听TCP端口80，以便发现是否有浏览器向它发出连接请求，一旦监听到连接建立请求，就通过三次握手建立TCP连接，然后浏览器会向服务器发出浏览某个页面的请求，服务器接着返回所请求的页面作为响应，然后TCP连接就被释放了。 这些响应和请求报文都遵循一定的格式，这就是HTTP协议所规定的。

**SSL加密**

1. 客户端向服务器端索要并验证公钥
2. 双方协商生成”对话密钥”。客户端用公钥对对话秘钥进行加密
3. 服务器通过私钥解密出对话秘钥
4. 双方采用”对话密钥”进行加密通信

- 对称加密算法： 加密效率高，速度快，适合大数据量加密。DES/AES
- 非对称加密算法：算法复杂，加密速度慢，安全性更高。结合对称加密使用。RSA、DH
- 私钥存储在服务器上

**session和cookie**

- cookie 是一种发送到客户浏览器的文本串句柄，并保存在客户机硬盘上，可以用来在某个WEB站点会话间持久的保持数据。
- Session的本质上也是cookie，但是不同点是存在服务器上的。这就导致，你如果使用cookie，你关闭浏览器之后，就丢掉Cookie了，但是如果关掉浏览器，重新打开之后，发现还有相应的信息，那就说明用的是Session。因为cookie是存在本地的，所以也会有相应的安全问题，攻击者可以去伪造他，填写相应的字段名，就能登录你的账户，还有如果cookie的有效期很长的话，也不安全。
- session 由服务器产生，对于客户端，只存储session id在cookie中

**http和https的区别**

- https 协议需要到 ca 申请证书，一般免费证书较少，因而需要一定费用
- http 是超文本传输协议，信息是明文传输，https 则是具有安全性的 ssl 加密传输协议
- http 和 https 使用的是完全不同的连接方式，用的端口也不一样，前者是 80，后者是 443
- http 的连接很简单，是无状态的；HTTPS 协议是由 SSL+HTTP 协议构建的可进行加密传输、身份认证的网络协议，比 http 协议安全

**https的安全外壳是怎么实现的**

> HTTPS就是在原HTTP的基础上加上一层用于数据加密、解密、校验、身份认证的安全层SSL/TSL，用于解决HTTP存在的安全隐患 信息加密：所有信息都是加密传播，第三方无法窃听；内容经过对称加密，每个连接生成一个唯一的加密密钥； 身份认证：配备了身份认证，第三方无法伪造服务端（客户端）的身份 数据完整性校验：内容传输经过完整性校验，一旦报文被篡改，通信双方会立刻发现

**TCP TIMEWAIT讲一下？为啥需要这个？**

> 当断开连接时，客户端发送完ACK将处于TIME WAIT状态，保持2MSL，之后完全断开意义在于：

1. 保证最后一次握手报文能到服务端，能进行超时重传
2. 2MSL 后，这次连接的所有报文都会消失，不会影响下一次连接

**说一下TCP/IP**

> TCP/IP协议是包含TCP协议和IP协议，UDP(User Datagram Protocol)协议、ICMP(Internet Control Message Protocol) 协议和其他一些的协议的协议组 TCP/IP定义了电子设备（如计算机）如何连入因特网，以及数据如何在它们之间传输的标准.它是互联网中的基本通信语言或协议，在私网中它也被用作通信协议，当用户直接网络连接时，计算机应提供一个TCP/IP程序的标准实现，而且接受所发送的信息的计算机也应只有一个TCP/IP程序的标准实现 TCP/IP协议并不完全符合OSI 标准定制的七层参考模型，它采取了四层的层级结构 网络接口层：接收IP数据包并进行传输，从网络上接收物理帧，抽取IP 转交给下一层，对实际网络的网络媒体的管理，定义如何使用物理网络 ，如以太网。 网际层IP: 负责提供基本的数据封包传送功能，让每一块数据包都能打到目的主机，但不检查是否被正确接收，主要表现为IP协议 传输层：在此层中，它提供了节点的数据传送，应用程序之间的通信服务，主要是数据格式化，数据确认和丢失重传等。主要协议包括TCP和UDP 应用层：应用程序间沟通单层，如万维网（WWW）、简单电子邮件传输(SMTP)、文件传输协议（FTP）、网络远程访问协议（Telnet）等

![img](https://pic2.zhimg.com/80/v2-8602b3c99cb2dc34725b422638ac44c9_720w.webp)

![img](https://pic2.zhimg.com/80/v2-d816bc6a66705d2e3b4ab8f254ba7cd9_720w.webp)

**fread和read的区别**

> read/write 操作文件描述符 (int型) fread/fwrite 操作文件流 (FILE*型) fread/fwrite 调用 read/write read/write是系统调用，要自己分配缓存，也就是说效率要自己根据实际情况来控制。 fread/fwrite是标准输入/输出函数，不需要自己分配缓存，对于一般情况具有较高的效率。

**什么是内存栅栏**

> 内存栅栏（Memory Barrier）就是从本地或工作内存到主存之间的拷贝动作。 仅当写操作线程先跨越内存栅栏而读线程后跨越内存栅栏的情况下，写操作线程所做的变更才对其他线程可见。关键字 synchronized 和 volatile 都强制规定了所有的变更必须全局可见，该特性有助于跨越内存边界动作的发生，无论是有意为之还是无心插柳。 在程序运行过程中，所有的变更会先在寄存器或本地 cache 中完成，然后才会被拷贝到主存以跨越内存栅栏。此种跨越序列或顺序称为 happens-before。 写操作必须要 happens-before 读操作，即写线程需要在所有读线程跨越内存栅栏之前完成自己的跨越动作，其所做的变更才能对其他线程可见

**TCP拥塞控制**

> 拥塞控制的最终受控变量是发送端向网络一次连续写入的数据量（收到其中第一个数据报的确认之前），称之为发送窗口（SWND），SWND 受接收方接受窗口（RWND）的影响。同时也受控于发送方的拥塞窗口（CWND）。SWND = min(RWND, CWND )

- 当 cwnd < 慢开始门限(ssthresh) 时，使用慢开始算法
- 当 cwnd > ssthresh 时，改用拥塞避免算法
- 快重传要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时捎带确认。快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。快重传配合使用的还有快恢复算法，是将 ssthresh减半，然后将 cwnd 设置为 ssthresh 的大小，之后执行拥塞避免算法

**TCP为什么要三次握手、但是要四次挥手，握手第二步拆开行不行，挥手2 3步合并行不行，第三次握手确认的是什么能力**

> 三次握手是为了避免僵尸连接，四次挥手是为了确保被断开方的数据能够全部完成传输，握手的第二部可以分开，不过需要增加一下状态。如果服务端没有数据要发送，挥手的2，3步可以合并，因为TCP是全双工的。第三次握手确认的是客户端时真实IP

**TCP和UDP的区别**

> TCP 是面向连接的传输层协议，即传输数据之前必须先建立好连接, UDP 无连接 TCP 是点对点的两点间服务，即一条 TCP 连接只能有两个端点；UDP 支持一对一，一对多，多对一，多对多的交互通信 TCP 是可靠交付：无差错，不丢失，不重复，按序到达；UDP 是尽最大努力交付，不保证可靠交付 TCP 有拥塞控制和流量控制保证数据传输的安全性；UDP 没有拥塞控制，网络拥塞不会影响源主机的发送效率 TCP 是动态报文长度，即 TCP 报文长度是根据接收方的窗口大小和当前网络拥塞情况决定的。UDP 面向报文，不合并，不拆分，保留上面传下来报文的边界 TCP 首部开销大，首部 20 个字节；UDP 首部开销小，8 字节 如果数据完整性更重要，如文件传输、重要状态的更新等，应该选用 TCP 协议。如果通信的实时性较重要，如视频传输、实时通信等，则使用 UDP 协议

**用udp会有什么问题**

> 不可靠，不稳定 因为本身没有重传的控制机制，所以丢包率的可能是其最主要的问题

**TCP是可靠的，为什么UDP还要去实现可靠连接**

**Linux查看网络连接的命令**

> 使用netstat查看存在的网络连接 使用ping判断主机间联通情况

**tcp可靠性传输怎么实现**

> 序列号、确认应答、超时重传 窗口控制与高速重发控制/快速重传（重复确认应答） 拥塞控制 流量控制

**虚函数的实现原理，继承的时候怎么实现的**

> 在有虚函数的类中，类的最开始部分是一个虚函数表的指针，这个指针指向一个虚函数表，表中放了虚函数的地址，实际的虚函数在代码段(.text)中。当子类继承了父类的时候也会继承其虚函数表，当子类重写父类中虚函数时候，会将其继承到的虚函数表中的地址替换为重新写的函数地址。使用了虚函数，会增加访问内存开销，降低效率。

**局部变量分配在哪**

> 分配在栈区

**进程的栈有多大**

> 32位Windows，一个进程栈的默认大小是1M，在vs的编译属性可以修改程序运行时进程的栈大小 inux下进程栈的默认大小是10M，可以通过 ulimit -s查看并修改默认栈大小 默认一个线程要预留1M左右的栈大小，所以进程中有N个线程时，Windows下大概有N M的栈大小 堆的大小理论上大概等于进程虚拟空间大小-内核虚拟内存大小。windows下，进程的高位2G留给内核，低位2G留给用户，所以进程堆的大小小于2G。Linux下，进程的高位1G留给内核，低位3G留给用户，所以进程堆大小小于3G

**进程的最大线程数**

> 32位windows下，一个进程空间4G，内核占2G，留给用户只有2G，一个线程默认栈是1M，所以一个进程最大开2048个线程。当然内存不会完全拿来做线程的栈，所以最大线程数实际值要小于2048，大概2000个 32位Linux下，一个进程空间4G，内核占1G，用户留3G，一个线程默认8M，所以最多380个左右线程（ps：ulimit -a 查看电脑的最大进程数，大概7000多个）

**怎么快速把进程的栈用完**

> 对函数进行递归调用 在函数中定义大对象

**C++内存对齐**

> 为什么要内存对齐：

1. 平台原因（移植原因）：不是所有的硬件平台都能访问任意地址上的任意数据的；某些硬件平台只能在某些地址处取某些特定类型的数据，否则抛出硬件异常
2. 性能原因：数据结构（尤其是栈）应该尽可能地在自然边界上对齐。原因在于，为了访问未对齐的内存，处理器需要作两次内存访问；而对齐的内存访问仅需要一次访问

> 如何进行内存对齐

- 分配内存的顺序是按照声明的顺序
- 每个变量相对于起始位置的偏移量必须是该变量类型大小的整数倍，不是整数倍空出内存，直到偏移量是整数倍为止
- 最后整个结构体的大小必须是里面变量类型最大值的整数倍

```text
class A {
	int a, b;
	char c;
};

class B {
	int a, b;
	double c;
};

class C {
	char a;
	int b;
	double c;
};

class D {
	int a;
	double b;
	int c;
};

class E {
	int a;
	double b;
	int c;
	char d;
};
int main() {
	cout << sizeof(A) << " " << sizeof(B) << " " 
        << sizeof(C) << " " << sizeof(D) << " " << sizeof(E) << endl;
	return 0;
}
// output
// 12 16 16 24 24
```

**引用和指针的区别？对const型常量可以取引用吗**

> 首先引用可以视作对象的别名，指针拥有自己的地址空间，其中保存着所指对象的地址 区别：

1. 指针有自己的一块空间，而引用只是一个别名
2. 使用 sizeof 看一个指针的大小是 4，而引用则是被引用对象的大小
3. 指针可以被初始化为 NULL，而引用必须被初始化且必须是一个已有对象的引用
4. 作为参数传递时，指针需要被解引用才可以对对象进行操作，而直接对引用的修改都会改变引用所指向的对象
5. 指针在使用中可以指向其它对象，但是引用只能是一个对象的引用，不能被改变
6. 指针可以有多级指针（**p），而引用只有一级
7. 指针和引用使用++运算符的意义不一样
8. 如果返回动态内存分配的对象或者内存，必须使用指针，引用可能引起内存泄露

**http请求格式**

> HTTP 请求报文由请求行、请求头部、空行 和 请求包体 4 个部分组成

![img](https://pic1.zhimg.com/80/v2-1b52a5464ebf5902a574ece0ed9914c4_720w.webp)

**get post的区别 put delete 知道吗 put和post**

**http 1.X 2.0区别 （ 帧 流 推送 头部压缩 安全性等等**

**联合索引：b+树是什么状态**

**HTTP头部字段**

> [详细](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/soldierback/p/11714052.html)

**http头部可以包含二进制吗**

> http2支持二进制，http1.x不支持

- http 2 和 http 1 的区别

1. HTTP2使用的是二进制传送，HTTP1.X是文本（字符串）传送。 二进制传送的单位是帧和流。帧组成了流，同时流还有流ID标示
2. HTTP2支持多路复用 因为有流ID，所以通过同一个http请求实现多个http请求传输变成了可能，可以通过流ID来标示究竟是哪个流从而定位到是哪个http请求
3. HTTP2头部压缩 HTTP2通过gzip和compress压缩头部然后再发送，同时客户端和服务器端同时维护一张头信息表，所有字段都记录在这张表中，这样后面每次传输只需要传输表里面的索引Id就行，通过索引ID查询表头的值
4. HTTP2支持服务器推送 HTTP2支持在未经客户端许可的情况下，主动向客户端推送内容

**MYSQL的事务**

> ACID特性，原子性、一致性、隔离性、持久性

**多级缓存的由来和使用**

> 计算机结构中CPU和内存之间一般都配有一级缓存、二级缓存来增加交换速度，这样当CPU调用大量数据时，就可避开内存直接从CPU缓存中调用，加快读取速度。 根据CPU缓存得出多级缓存的特点：

- 每一级缓存中储存的是下一级缓存的一部分
- 读取速度按级别依次递减，成本也依次递减，容量依次递增
- 当前级别未命中时，才会去下一级寻找

**项目文件传输时怎么限速**

> 在客户端进行文件传输时，每当上传限制大小数据，就sleep一下

**用户态和内核态，为啥这样做，好处是什么**

> 用户态和内核态是操作系统的两种运行级别，两者最大的区别就是特权级不同。用户态拥有最低的特权级，内核态拥有较高的特权级。运行在用户态的程序不能直接访问操作系统内核数据结构和程序。内核态和用户态之间的转换方式主要包括：系统调用，异常和中断进程

**堆和栈的区别**

> 堆是由低地址向高地址扩展；栈是由高地址向低地址扩展 堆中的内存需要手动申请和手动释放；栈中内存是由 OS 自动申请和自动释放，存放着参数、局部变量等内存 堆中频繁调用 malloc 和 free,会产生内存碎片，降低程序效率；而栈由于其先进后出的特性，不会产生内存碎片 堆的分配效率较低，而栈的分配效率较高 栈是操作系统提供的数据结构，计算机底层对栈提供了一系列支持：分配专门的寄存器存储栈的地址，压栈和入栈有专门的指令执行；而堆是由 C/C++函数库提供的，机制复杂，需要一些列分配内存、合并内存和释放内存的算法，因此效率较低

**五种IO模型**

1. 阻塞IO：调用者调用了某个函数，等待这个函数返回，期间什么也不做，不停的去检查这个函数有没有返回，必须等这个函数返回才能进行下一步动作
2. 非阻塞IO：非阻塞等待，每隔一段时间就去检测 IO 事件是否就绪。没有就绪就可以做其他事
3. 异步IO：
4. 信号驱动IO：linux 用套接口进行信号驱动 IO，安装一个信号处理函数，进程继续运行并不阻塞，当 IO 时间就绪，进程收到 SIGIO 信号。然后处理 IO 事件
5. 多路复用IO：linux 用 select/poll 函数实现 IO 复用模型，这两个函数也会使进程阻塞，但是和阻塞 IO 所不同的是这两个函数可以同时阻塞多个 IO 操作。而且可以同时对多个读操作、写操作的 IO 函数进行检测。知道有数据可读或可写时，才真正调用 IO 操作函数

**bio nio aio 区别**

> [详细](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/blackjoyful/p/11534985.html) BIO （Blocking I/O）：同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。这里使用那个经典的烧开水例子，这里假设一个烧开水的场景，有一排水壶在烧开水，BIO的工作模式就是， 叫一个线程停留在一个水壶那，直到这个水壶烧开，才去处理下一个水壶。但是实际上线程在等待水壶烧开的时间段什么都没有做。 NIO （New I/O）：同时支持阻塞与非阻塞模式，但这里我们以其同步非阻塞I/O模式来说明，那么什么叫做同步非阻塞？如果还拿烧开水来说，NIO的做法是叫一个线程不断的轮询每个水壶的状态，看看是否有水壶的状态发生了改变，从而进行下一步的操作。 AIO （ Asynchronous I/O）：异步非阻塞I/O模型。异步非阻塞与同步非阻塞的区别在哪里？异步非阻塞无需一个线程去轮询所有IO操作的状态改变，在相应的状态改变后，系统会通知对应的线程来处理。对应到烧开水中就是，为每个水壶上面装了一个开关，水烧开之后，水壶会自动通知我水烧开了。

**Select poll epoll**

> I/O 多路复用的特点是通过一种机制一个进程能同时等待多个文件描述符，而这些文件描述符（套接字描述符）其中的任意一个进入读就绪状态，select()函数就可以返回 I/O 多路复用和阻塞 I/O 其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个 system call (select 和 recvfrom)，而 blocking IO 只调用了一个 system call (recvfrom)。但是，用 select 的优势在于它可以同时处理多个 connection。 所以，如果处理的连接数不是很高的话，使用 select/epoll 的 web server 不一定比使用 multi-threading + blocking IO 的 web server 性能更好，可能延迟还更大。select/epoll 的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。
> **select** 是最初解决 IO 阻塞问题的方法。用结构体 fd_set 来告诉内核监听多个文件描述符，该结构体被称为描述符集。由数组来维持哪些描述符被置位了。对结构体的操作封装在三个宏定义中。通过轮寻来查找是否有描述符要被处理 存在的问题：

1. 内置数组的形式使得 select 的最大文件数受限于 FD_SIZE；
2. 每次调用 select 前都要重新初始化描述符集，将 fd 从用户态拷贝到内核态，每次调用select 后，都需要将 fd 从内核态拷贝到用户态
3. 轮寻排查当文件描述符个数很多时，效率很低
4. select 会修改传入的参数数组，这个对于一个需要调用很多次的函数，是非常不友好的
5. select 不是线程安全的，如果你把一个sock加入到select, 然后突然另外一个线程发现这个sock不用，要收回。对不起，这个select 不支持的，如果你要关掉这个sock, select的标准行为是不可预测的

> **poll** 与 select 相比，poll 使用链表保存文件描述符，一没有了监视文件数量的限制，但其他三个缺点依然存在
> **epoll** epoll 使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的 copy 只需一次。Epoll 是事件触发的，不是轮询查询的。没有最大的并发连接限制，内存拷贝，利用 mmap() 文件映射内存加速与内核空间的消息传递。 epoll 对文件描述符的操作有两种模式：LT（level trigger）和 ET（edge trigger LT 模式是默认模式 LT(level triggered)是缺省的工作方式，并且同时支持 block 和 no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的 fd 进行 IO 操作。如果你不作任何操作，内核还是会继续通知你的 ET 模式 ET(edge-triggered)是高速工作方式，只支持 no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过 epoll 告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个 EWOULDBLOCK 错误）。但是请注意，如果一直不对这个 fd 作 IO 操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once) ET 模式在很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。epoll 工作在 ET 模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死 LT 模式与 ET 模式的区别如下： LT 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用 epoll_wait 时，会再次响应应用程序并通知此事件。 ET 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用 epoll_wait 时，不会再次响应应用程序并通知此事件。

**epoll的底层实现**

> [epoll发展于介绍](https://link.zhihu.com/?target=https%3A//blog.csdn.net/sunxianghuang/article/details/105028062) epoll中就绪列表引用着就绪的socket，所以它应能够快速的插入数据。程序可能随时调用epoll_ctl添加监视socket，也可能随时删除。当删除时，若该socket已经存放在就绪列表中，它也应该被移除。所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列。 既然epoll将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的socket。至少要方便的添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好。epoll使用了红黑树作为索引结构

**Linux的阻塞和非阻塞怎么体现**

> 阻塞（休眠）调用是没有获得资源则挂起进程，被挂起的进程进入休眠状态，调用的函数只有在得到结果之后才返回，进程继续。 非阻塞（休眠）是不能进行设备操作时不挂起，或返回，或反复查询，直到可以进行操作为止，被调用的函数不会阻塞当前进程，而会立刻返回

**进程间通信**

> 管道 具名管道 消息队列 信号 信号量 共享内存 socket

**进程上下文切换**

> 1 保存当前进程的上下文 2 恢复某个先前被抢占的进程的上下文 3 将控制传递给这个新恢复的进程 
> **详细** 保存处理器上下文环境即 PSW、PC 等寄存器和堆栈内容，保存到内核堆栈中 调整被中断进程的 PCB 进程控制块信息，改变进程状态和其它信息 将进程控制块移到相应队列即阻塞队列或就绪队列 选择另一个进程执行 更新所选择进程的 PCB 进程控制块 更新内存管理的数据结构 恢复第二个进程的上下文环境

**中断是什么**

中断定义：指当出现需要时，CPU暂时停止当前程序的执行转而执行处理新情况的程序和执行过程。 硬件中断是由外设引发的, 软中断是执行中断指令产生的。 硬件中断的中断号是由中断控制器提供的, 软中断的中断号由指令直接指出, 无需使用中断控制器。 硬件中断是可屏蔽的, 软中断不可屏蔽。

中断处理过程

1. 中断响应的事前准备
2. CPU检查是否有中断/异常信号
3. 根据中断向量到IDT表中取得处理这个向量的中断程序的段选择符
4. 根据取得的段选择符到GDT中找相应的段描述符
5. CPU根据特权级的判断设定即将运行的中断服务程序要使用的栈的地址
6. 保护当前程序的现场
7. 跳转到中断服务程序的第一条指令开始执行
8. 中断服务程序处理完毕，恢复执行先前中断的程序

**线程的上下文是什么、进程的上下文是什么**

- 线程上下文 线程在切换的过程中需要保存当前线程 Id、线程状态、堆栈、寄存器状态等信息。其中寄存器主要包括 SP PC EAX 等寄存器，其主要功能如下 SP:堆栈指针，指向当前栈的栈顶地址 PC:程序计数器，存储下一条将要执行的指令 EAX:累加寄存器，用于加法乘法的缺省寄存器
- 进程上下文 进程上下文包括三个，用户级上下文，寄存器上下文和系统级上下文 用户级上下文：指令，数据，共享内存、用户栈 寄存器上下文：程序计数器，通用寄存器，控制寄存器，状态字寄存器，栈指针（用来指向用户栈或者内存栈） 系统级上下文：pcb，主存管理信息（页表&段表）、核心栈

**Linux文件系统，inode讲一讲？inode里存文件名称吗？**

> inode，中文名为索引结点，引进索引结点是为了在物理内存上找到文件块，所以 inode 中包含文件的相关基本信息，比如文件位置、文件创建者、创建日期、文件大小等，输入 stat 指令可以查看某个文件的 inode 信息 硬盘格式化的时候，操作系统自动将硬盘分成两个区域，一个是数据区，一个是 inode 区，存放 inode 所包含的信息，查看每个硬盘分区的 inode 总数和已经使用的数量，可以用 df 命令 在 linux 系统中，系统内部并不是采用文件名查找文件，而是使用 inode 编号来识别文件。查找文件分为三个过程：系统找到这个文件名对应的inode 号码，通过 inode 号码获得 inode 信息，根据 inode 信息找到文件数据所在的 block 读取数据 除了文件名之外的所有文件信息，都存储在 inode 之中

**Linux chmod讲一讲，为啥有9位，分别对应什么**

> [详细](https://link.zhihu.com/?target=https%3A//www.runoob.com/linux/linux-comm-chmod.html) Linux chmod（英文全拼：change mode）命令是控制用户对文件的权限的命令 Linux/Unix 的文件调用权限分为三级 : 文件所有者（Owner）、用户组（Group）、其它用户（Other Users） 9为代表三种用户的权限： rwxrwxrwx，前三位rwx表示文件所有者的权限，中间三位表示用户组的权限，最后三位表示其它用户的权限

**STL中的map和unordered_map的实现**

> map实现使用红黑树，unordered_map使用hash表 [简述](https://link.zhihu.com/?target=https%3A//blog.csdn.net/tenchu/article/details/106502871)

**红黑树，B+树，跳表**

- 红黑树。一种二叉查找树，但在每个节点增加一个存储位表示节点的颜色，可以是红或黑（非红即黑）。通过对任何一条从根到叶子的路径上各个节点着色的方式的限制，红黑树确保没有一条路径会比其它路径长出两倍，因此，红黑树是一种弱平衡二叉树（由于是弱平衡，可以看到，在相同的节点情况下，AVL树的高度低于红黑树），相对于要求严格的AVL树来说，它的旋转次数少，所以对于搜索，插入，删除操作较多的情况下，我们就用红黑树。 性质：1. 每个节点非红即黑 2. 根节点是黑的; 3. 每个叶节点（叶节点即树尾端NULL指针或NULL节点）都是黑的; 4. 如果一个节点是红的，那么它的两儿子都是黑的; 5. 对于任意节点而言，其到叶子点树NULL指针的每条路径都包含相同数目的黑节点; 6. 每条路径都包含相同的黑节点。
- B+ 树

1. 有n棵子树的非叶子结点中含有n个关键字（b树是n-1个），这些关键字不保存数据，只用来索引，所有数据都保存在叶子节点（b树是每个关键字都保存数据）。
2. 所有的叶子结点中包含了全部关键字的信息，及指向含这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大顺序链接（叶子节点组成一个链表）。
3. 所有的非叶子结点可以看成是索引部分，结点中仅含其子树中的最大（或最小）关键字。
4. 通常在b+树上有两个头指针，一个指向根结点，一个指向关键字最小的叶子结点。
5. 同一个数字会在不同节点中重复出现，根节点的最大元素就是b+树的最大元素。

**怎么查看linux下哪个进程打开了哪些文件**

> [详细](https://link.zhihu.com/?target=https%3A//blog.csdn.net/sinat_34241861/article/details/109186770) lsof

**虚拟地址怎么转换成物理地址**

> 对于段页式系统来说，首先是查找段号，在对应段内找到页号，在页内找到页内偏移，从而 程序地址：段号+页号+页内偏移

**进程的调度算法**

- 先来先服务调度算法（FCFS） 在进程调度中采用 FCFS 算法时，则每次调度是从就绪队列中选择一个最先进入该队列的进程，为之分配处理机，使之投入运行，该进程一直运行到完或发生某事件而阻塞后才放弃处理机。–非抢占式 有利于长作业，不利于短作业；有利于 CPU 繁忙的作业，不利于 I/O 繁忙的作业
- 短作业（进程）优先调度算法（SJF、SPF） 短进程（SPF）调度算法则是从就绪队列中选出一个估计运行时间最短的进程，将处理机分配给它，使它立即执行并一直执行到完成，或发生某事件被阻塞放弃处理机。—非抢占式 比 FCFS 改善平均周转时间和平均带权周转时间，提高系统吞吐量；对长作业不利，没能根据紧迫程度来划分执行的优先级，难以准确估计 作业或进程的执行时间。
- 优先权调度算法 当该算法用于进程调度时，将把处理机分配给就绪进程队列中优先级最高的进程投入运行。分为非抢占式优先级算法和抢占式优先级算法。
- 时间片轮转调度算法 系统将就绪进程按到达的顺序排成一个队列，按 FCFS 原则，进程调度程序总是选择就绪队列中的第一个进程执行，且只运行一个时间片。时间用完后，即使此进程并未完成，仍然将处理机分配给下一个就绪的进程，将此进程返回到就绪队列的末尾，等候重新运行
- 多级反馈队列调度算法 设置 n 个就绪队列，优先级从 1 到 n 依次递减，即第 1 级队列优先级最高每个队列的时间也不相同，优先级高的队列，时间片越短，即从 1 到 n 时间片越来越多。一个新进程进入内存后，先插入第一级队列的末尾，按照FCFS的原则等待调度。如果某个进程可在一个时间片内完成，那么结束此进程；如果某进程在一个时间片内无法完成，就把此进程转入下一级队列的末尾，按照 FCFS 原则等待调度，一直到第 n-1 级队列。当一个很长的进程从第 1 级一直到第 n 级队列，那么它在第 n 级队列按照时间片轮转的方式等待调度。仅当第 1 级队列为空时，调度程序才调度第 2 级队列中的进程执行，依次类推。如果处理机正在处理第 i 级的队列的某进程，又有新的进程进入优先级更高的队列（第 1——i -1），则此时新的进程抢占处理机，原本正在执行第 i 级此进程停止运行，放到第 i 级就绪队列的末尾，把处理机分配给更高优先级的进程 短作业优先、短批处理作业周转时间较短、长批处理作业不会长期得不到执行

**怎么解锁死锁**

> 死锁四条件：

1. 互斥
2. 不可抢夺
3. 占有与等待
4. 循环等待

解决死锁的方案：

- 允许进程强行从占有者那里夺取某些资源，破坏不可抢占条件
- 进程在运行前一次性地向系统申请它所需要的全部资源，破坏了保持与等待条件
- 把资源事先分类编号，按号分配，使进程在申请，占用资源时不会形成环路，破坏了循环等待条件

死锁避免：银行家算法

**OSI七层模型**

- 物理层：规定通信设备的机械的、电气的、功能的和过程的特性，用以建立、维护和拆除物理链路连接。在这一层，数据的单位称为比特(bit)。属于物理层定义的典型规范代表包括：EIA/TIA RS-232、EIA/TIA RS-449、V.35、RJ-45 等
- 数据链路层：在物理层提供比特流服务的基础上，建立相邻结点之间的数据链路，通过差 错控制提供数据帧(Frame)在信道上无差错的传输，并进行各电路上的动作系列。数据链路层在不可靠的物理介质上提供可靠的传输。该层的作用包括：物理地址寻址、数据的成帧、流量控制、数据的检错、重发等。在这一层，数据的单位称为帧(frame)。数据链路层协议的代表包括：SDLC、HDLC、PPP、STP、帧中继等。
- 网络层：在 计算机网络中进行通信的两个计算机之间可能会经过很多个数据链路，也可能还要经过很多通信子网。网络层的任务就是选择合适的网间路由和交换结点，确保数据及时传送。网络层将数据链路层提供的帧组成数据包，包中封装有网络层包头，其中含有逻辑地址信息- -源站点和目的站点地址的网络地址。IP 是第 3 层问题的一部分，此外还有一些路由协议和地址解析协议(ARP)。有关路由的一切事情都在这第 3 层处理。地址解析和路由是 3 层的重要目的。网络层还可以实现拥塞控制、网际互连等功能。在这一层，数据的单位称为数据包(packet)。网络层协议的代表包括：IP、IPX、RIP、OSPF 等。
- 传输层：第 4 层的数据单元也称作数据包(packets)。但是，当你谈论 TCP 等具体的协议时又有特殊的叫法，TCP 的数据单元称为段 (segments)而 UDP 协议的数据单元称为“数据报(datagrams)”。这个层负责获取全部信息，因此，它必须跟踪数据单元碎片、乱序到达的数据包和其它在传输过程中可能发生的危险。第 4 层为上层提供端到端(最终用户到最终用户)的透明的、可靠的[数据传输服务](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/product/dts%3Ffrom%3D10680)。所谓透明的传输是指在通信过程中 传输层对上层屏蔽了通信传输系统的具体细节。传输层协议的代表包括：TCP、UDP、SPX 等。
- 会话层：在会话层及以上的高层次中，数据传送的单位不再另外命名，而是统称为报文。会话层不参与具体的传输，它提供包括访问验证和会话管理在内的建立和维护应用之间通信的机制。如服务器验证用户登录便是由会话层完成的。
- 表示层：这一层主要解决拥护信息的语法表示问题。它将欲交换的数据从适合于某一用户的抽象语法，转换为适合于 OSI 系统内部使用的传送语法。即提供格式化的表示和转换[数据服务](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/solution/data-collect-and-label-service%3Ffrom%3D10680)。数据的压缩和解压缩， 加密和解密等工作都由表示层负责。
- 应用层：为操作系统或网络应用程序提供访问网络服务的接口。应用层协议的代表包括：Telnet、FTP、HTTP、SNMP 等

**ARP协议**

> ARP（地址解析）协议是一种解析协议，本来主机是完全不知道这个 IP 对应的是哪个主机的哪个接口，当主机要发送一个 IP 包的时候，会首先查一下自己的 ARP 高速缓存表（最近数据传递更新的 IP-MAC 地址对应表），如果查询的 IP－MAC 值对不存在，那么主机就向网络广播一个 ARP 请求包，这个包里面就有待查询的 IP 地址，而直接收到这份广播的包的所有主机都会查询自己的 IP 地址，如果收到广播包的某一个主机发现自己符合条件，那么就回应一个 ARP 应答包（将自己对应的 IP-MAC 对应地址发回主机），源主机拿到 ARP 应答包后会更新自己的 ARP 缓存表。源主机根据新的 ARP 缓存表准备好数据链路层的的数据包发送工作

**malloc底层的实现**

> Malloc 函数用于动态分配内存。为了减少内存碎片和系统调用的开销，malloc 其采用内存池的方式，先申请大块内存作为堆区，然后将堆区分为多个内存块，以块作为内存管理的基本单位。当用户申请内存时，直接从堆区分配一块合适的空闲块。Malloc 采用隐式链表结构将堆区分成连续的、大小不一的块，包含已分配块和未分配块；同时 malloc 采用显示链表结构来管理所有的空闲块，即使用一个双向链表将空闲块连接起来，每一个空闲块记录了一个连续的、未分配的地址 当进行内存分配时，Malloc 会通过隐式链表遍历所有的空闲块，选择满足要求的块进行分配；当进行内存合并时，malloc 采用边界标记法，根据每个块的前后块是否已经分配来决定是否进行块合并 Malloc 在申请内存时，一般会通过 brk 或者 mmap 系统调用进行申请。其中当申请内存小于128K 时，会使用系统函数 brk 在堆区中分配；而当申请内存大于 128K 时，会使用系统函数 mmap在映射区分配

**逻辑地址---（分段硬件）>>> 线型地址 --- （分页硬件)>>> 物理地址 的过程，虚拟内存的实现**

**为什么引入虚拟内存**

为了防止不同进程同一时刻在物理内存中运行而对物理内存的争夺和践踏，采用了虚拟内存。

虚拟内存技术使得不同进程在运行过程中，它所看到的是自己独自占有了当前系统的 4G 内存。所有进程共享同一物理内存，每个进程只把自己目前需要的虚拟内存空间映射并存储到物理内存上。

虚拟内存的好处：

1. 扩大地址空间；
2. 内存保护：每个进程运行在各自的虚拟内存地址空间，互相不能干扰对方。虚存还对特定的内存地址提供写保护，可以防止代码或数据被恶意篡改。
3. 公平内存分配。采用了虚存之后，每个进程都相当于有同样大小的虚存空间。
4. 当进程通信时，可采用虚存共享的方式实现。
5. 当不同的进程使用同样的代码时，比如库文件中的代码，物理内存中可以只存储一份这样的代码，不同的进程只需要把自己的虚拟内存映射过去就可以了，节省内存
6. 虚拟内存很适合在多道程序设计系统中使用，许多程序的片段同时保存在内存中。当一个程序等待它的一部分读入内存时，可以把 CPU 交给另一个进程使用。在内存中可以保留多个进程，系统并发度提高
7. 在程序需要分配连续的内存空间的时候，只需要在虚拟内存空间分配连续空间，而不需要实际物理内存的连续空间，可以利用碎片

虚拟内存的代价：

1. 虚存的管理需要建立很多数据结构，这些数据结构要占用额外的内存
2. 虚拟地址到物理地址的转换，增加了指令的执行时间。
3. 页面的换入换出需要磁盘 I/O，这是很耗时的
4. 如果一页中只有一部分数据，会浪费内存。

**64位操作系统下，实现一个链接式的hash map，保存n组(key, value) 对，假设key, value各占8字节，问一共需要占多少字节**

> 64位操作系统，指针8字节，假设hash函数有m个值，则8*m + (8+8+8)*n = 8m + 24n 8m表示开始的m个指针大小，(8+8+8)分别表示指针、key和value

**延时队列怎么实现**

> [详细](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/tiancai/p/13161165.html) 什么是延时队列？顾名思义：首先它要具有队列的特性，再给它附加一个延迟消费队列消息的功能，也就是说可以指定队列中的消息在哪个时间点被消费。 对于C++来说，可以直接使用优先队列，如在队列中存储消息id以及过期时间，队列自动按照时间排序，每次从队列中拿第一个消息进行消费。

**怎么解决缓存击穿？怎么解决缓存雪崩？**

- 缓存穿透

> 缓存穿透是指缓存和数据库中都没有的数据，而用户不断发起请求，如发起为id为“-1”的数据或id为特别大且不存在的数据。这时的用户很可能是攻击者，攻击会导致数据库压力过大。 解决方案：

1. 接口层增加校验，如用户鉴权校验，id做基础校验，id<=0的直接拦截； 从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击

- 缓存击穿

> 缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力 解决方案： 设置热点数据永远不过期 加互斥锁降低从数据库中读取数据频率

- 缓存雪崩

> 缓存雪崩是指缓存中数据大批量到过期时间，而查询数据量巨大，引起数据库压力过大甚至down机。和缓存击穿不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库 解决方案：

1. 缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。
2. 如果缓存数据库是分布式部署，将热点数据均匀分布在不同搞得缓存数据库中。
3. 设置热点数据永远不过期。

**10M带宽，下载速度大约有多少**

> 10 M 带宽单位是bps，而我们常使用的是Byte，因此约为10/8=1.25M。上行/上传速度会更慢，大约只有256K

**客户端和服务端建立socket连接的过程，相关的方法**

- 服务端

1. 创建一个socket，用函数socket()；
2. 设置socket属性，用函数setsockopt(); * 可选
3. 绑定IP地址、端口等信息到socket上，用函数bind();
4. 开启监听，用函数listen()；
5. 接收客户端上来的连接，用函数accept()；
6. 收发数据，用函数send()和recv()，或者read()和write();
7. 关闭网络连接；
8. 关闭监听； closesocket

- 客户端

1. 创建一个socket，用函数socket()；
2. 设置socket属性，用函数setsockopt();* 可选
3. 绑定IP地址、端口等信息到socket上，用函数bind();* 可选
4. 设置要连接的对方的IP地址和端口等属性；
5. 连接服务器，用函数connect()；
6. 收发数据，用函数send()和recv()，或者read()和write();
7. 关闭网络连接；

**路由器和交换机有什么区别，分别工作在哪一层**

> 交换机，工作在 OSI 第二层（数据链路层），根据 MAC 地址进行数据转发。 路由器，工作在 OSI 第三次（网络层），根据 IP 进行寻址转发数据包

**fork 与 vfork**

> fork与vfork都是创建进程，vfork创建的子进程是与父进程共享地址空间，而fork创建的子进程是父进程的副本，它们的区别如下

1. fork：子进程拷贝父进程的数据段，代码段
2. vfork：子进程与父进程共享数据段
3. fork：父子进程的执行次序不确定
4. vfork 保证子进程先运行，在调用exec 或exit 之前与父进程数据是共享的,在它调用exec或exit 之后父进程才可能被调度运行。如果在调用这两个函数之前子进程依赖于父进程的进一步动作，则会导致死锁。

**vfork为什么需要exit()而不用return**

> 如果你在vfork中return了，那么，这就意味main()函数return了，注意因为函数栈父子进程共享，所以整个程序的栈就跪了。

\#include<unistd.h> #include<iostream> #include<stdlib.h> using namespace std; int gdata = 1; int main(){ pid_t pid; pid = fork(); //更改为vfork int tmp = 10; if(pid == -1) cout << "fork error"<< endl; else if(pid == 0){ cout << "son process, my parent is: " << getppid() << endl; gdata = 11; cout << "in son: " << gdata << endl; tmp = 100; cout << "son tmp: " << tmp << endl; exit(0); }else { cout << "parent process, my pid is " << getpid() << endl; cout << "in parent: " << gdata << endl; cout << "in parent: " << tmp << endl; } return 0; } /* 当前输出： son process, my parent is: 9743 in son: 11 son tmp: 100 parent process, my pid is 9743 in parent: 1 in parent: 10 将fork改为vfork son process, my parent is: 9735 in son: 11 son tmp: 100 parent process, my pid is 9735 in parent: 11 in parent: 10 */
复制

**客户端怎么校验https的证书是否合法**

> [详细](https://link.zhihu.com/?target=https%3A//blog.csdn.net/love_hot_girl/article/details/81164279) 数字证书包含以下信息：申请者公钥、申请者的组织信息和个人信息、签发机构 CA 的信息、有效时间、证书序列号等信息的明文，同时包含一个签名 客户端在对服务器say hello之后，服务器将公开密钥证书发送给客户端，注意这个证书里面包含了公钥+各种信息+签名（私钥对各种信息加密后生成签名），客户端收到公开密钥证书后，相当于收到了一个包裹里面有公钥+各种信息+签名，怎么样使用这三个数据来校验呢，很简单，公钥加密，私钥解，私钥加密公钥也可以解，只要利用公钥对签名进行解密，然后最和各种信息做比较就可以校验出证书的合法性。

**两个进程某变量用gdb调试打印出的地址是否会一样**

**如果使用指针访问一个区域，指针+1 、指针-1可能会访问到什么？为什么**

**一致性哈希**

[详细](https://link.zhihu.com/?target=https%3A//blog.csdn.net/u010006625/article/details/103739974)

> 通过hash环来实现负载均衡，将不同的服务器hash映射到一致性hash环上，当服务请求到来时，使用hash将其映射到hash环上，然后可以采用如顺时针寻找的方法选择距其最近的服务器进行服务。 当服务器较少或hash公式不够好时，可能出现大多数请求都会落在同一个服务器上，这就是数据倾斜，可以采用添加服务器、虚拟节点、更换一致性hash的方法进行解决。

**Tcp: 拔网线之后连接是否存在 为什么 （记得tcp的长连接是有一个类似心跳检测的机制,忘了叫啥了，面试官问我心跳检测是在传输层吗还是应用层 ，我说应用层有心跳检测，但tcp那层也有类似的，后来回来看了下tcp的保活，**

**操作系统如何识别tcp连接**

**C++锁**

> 互斥锁（Mutex） -- 互斥锁用于控制多个线程对他们之间共享资源互斥访问的一个信号量。 条件锁 -- 条件锁就是所谓的条件变量 自旋锁 读写锁

**vector、stack、queue这些容器是怎么实现的**

> stack 底层一般用 deque 实现，封闭头部即可，不用 vector 的原因应该是容量大小有限制，扩容耗时 queue 底层一般用 deque 实现，封闭头部的出口和前端的入口即可 priority_queue 的底层数据结构一般为 vector 为底层容器，堆 heap 为处理规则来管理底层容器实现

**并发编程三要素**

> 原子性:即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行 可见性:指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 有序性:即程序执行的顺序按照代码的先后顺序执行，首先什么是指令重排序，一般来说，处理器为了提高程序运行效率，可能会对输入代码进行优化，它不保证程序中各个语句的执行先后顺序同代码中的顺序一致，但是它会保证程序最终执行结果和代码顺序执行的结果是一致的。处理器在进行重排序时也是会考虑指令之间的数据依赖性。指令重排序不会影响单个线程的执行，但是会影响到线程并发执行的正确性。

**短网址服务 -- 将很长的网址连接设计成短网址**

- 使用分布式 ID 生成器
- 使用mysql数据库存储id与网址的对应关系
- 使用302重定向

> 如果用了 301，Google、百度等搜索引擎，搜索的时候会直接展示真实地址，那我们就无法统计到短地址被点击的次数了，也无法收集用户的 Cookie、User Agent 等信息，这些信息可以用来做很多有意思的大数据分析，也是短网址服务商的主要盈利来源 [详细](https://link.zhihu.com/?target=https%3A//blog.csdn.net/a419240016/article/details/114683919)

**零拷贝**

> “零拷贝”：在整个发送数据过程中，数据的复制是必不可少的，这里数据复制分两种类型，一种是CPU参与的一个字节一个字节处理的数据复制，一个是CPU不用参与，通过专有硬件DMA参与的，批量数据复制。自然，不用CPU参与的数据复制性能高。而“零拷贝”所说的拷贝，其实指的是，减少CPU参与的数据拷贝，最好减少到零次，但是各种实现方式里，很多种都只是减少一次两次，并没有直接让CPU参与的数据复制数归零

**通过mmap实现的零拷贝I/O**

> 发出mmap系统调用，导致用户空间到内核空间的上下文切换(第一次上下文切换)。通过DMA引擎将磁盘文件中的内容拷贝到内核空间缓冲区中(第一次拷贝: hard drive ——> kernel buffer)。 mmap系统调用返回，导致内核空间到用户空间的上下文切换(第二次上下文切换)。接着用户空间和内核空间共享这个缓冲区，而不需要将数据从内核空间拷贝到用户空间。因为用户空间和内核空间共享了这个缓冲区数据，所以用户空间就可以像在操作自己缓冲区中数据一般操作这个由内核空间共享的缓冲区数据 发出write系统调用，导致用户空间到内核空间的上下文切换(第三次上下文切换)。将数据从内核空间缓冲区拷贝到内核空间socket相关联的缓冲区(第二次拷贝: kernel buffer ——> socket buffer)。 write系统调用返回，导致内核空间到用户空间的上下文切换(第四次上下文切换)。通过DMA引擎将内核空间socket缓冲区中的数据传递到协议引擎(第三次拷贝: socket buffer ——> protocol engine 通过mmap实现的零拷贝I/O进行了4次用户空间与内核空间的上下文切换，以及3次数据拷贝。其中3次数据拷贝中包括了2次DMA拷贝和1次CPU拷贝

**快照读在读提交和可重复读(RR和RC)模式下的问题**

> 事务总能够读取到，自己写入(update /insert /delete)的行记录 RC下，快照读总是能读到最新的行数据快照，当然，必须是已提交事务写入的 RR下，某个事务首次read记录的时间为T，未来不会读取到T时间之后已提交事务写入的记录，以保证连续相同的read读到相同的结果集 在RC级别下每次都是读取最新的快照版本，在RR级别下是事务开启时生成一个全局快照，后续的快照读都读取这个快照

**索引失效**

1.有or必全有索引; 2.复合索引未用左列字段; 3.like以%开头; 4.需要类型转换; 5.where中索引列有运算; 6.where中索引列使用了函数; 7.如果mysql觉得全表扫描更快时（数据少）;

**内存泄漏**

内存泄漏是指由于疏忽或错误造成了程序未能释放掉不再使用的内存的情况。内存泄漏并非指内存在物理上消失，而是应用程序分配某段内存后，由于设计错误，失去了对该段内存的控制。

检查、定位内存泄漏：

检查方法： 在 main 函数最后面一行，加上一句_CrtDumpMemoryLeaks()。调试程序，自然关闭程序让其退出，查看输出： 被{}包围的 数字x 就是我们需要的内存泄漏定位值 定位代码位置： 在 main 函数第一行加上_CrtSetBreakAlloc(x);意思就是在申请 x这块内存的位置中断。然后调试程序，程序中断了，查看调用堆栈。加上头文件#include <crtdbg.h>

**算法题**

由于篇幅有限，后续再更新

原文地址：https://zhuanlan.zhihu.com/p/560967750
作者：linux

# 【NO.62】百度C++研发工程师面试题（最新整理）

**C++ 内存分为几部分？介绍堆和栈的区别**

> text(代码段)： 用来存放程序执行代码，同时也可能会包含一些常量(如一些字符串常量等）。该段内存为静态分配，只读(某些架构可能允许修改) data(数据段)：用来存放程序中已经初始化的非零全局变量，静态分配。data又可分为读写（RW）区域和只读（RO）区域，RO段保存常量所以也被称为.constdata，RW段则是普通非常全局变量，静态变量就在其中 bss：存放程序中未初始化的和零值全局变量。静态分配，在程序开始时通常会被清零。 堆：在内存开辟另一块存储区域，般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收 栈：程序运行时由编译器自动分配，存放函数的参数值，局部变量的值等 堆和栈的区别：

- 栈由系统自动分配和管理，堆由程序员手动分配和管理
- 栈由系统分配，速度快，不会有内存碎片
- 堆由程序员分配，速度较慢，可能由于操作不当产生内存碎片
- 栈从高地址向低地址进行扩展，堆由低地址向高地址进行扩展
- 程序局部变量是使用的栈空间，new/malloc 动态申请的内存是堆空间，函数调用时会进行形参和返回值的压栈出栈，也是用的栈空间

**程序结束后如何回收内存？（析构函数）**

> 在 main()函数中的显示代码执行之前，会调用一个由编译器生成的_main()函数，而_main()函数会进行所有全局对象的的构造及初始化工作。而在main()函数结束之前，会调用由编译器生成的exit函数，来释放所有的全局对象 假设我们要在main()函数执行之前做某些准备工作，那么我们可以将这些准备工作写到一个自定义的全局对象的构造函数中，这样，在main()函数的显式代码执行之前，这个全局对象的构造函数会被调用，执行预期的动作，这样就达到了我们的目的。 垃圾回收的时候，只需要扫描 bss 段, data 段以及当前被使用着的栈空间，找到可能是动态内存指针的量，把引用到的内存递归扫描就可以得到当前正在使用的所有动态内存了。

**析构函数是否可以为虚函数？构造函数是否可以是虚函数**

> 析构函数可以，构造函数不行 虚函数的调用需要虚函数表指针，而该指针存放在对象的内容空间中；若构造函数声明为虚函数，那么由于对象还未创建，还没有内存空间，更没有虚函数表地址用来调用虚函数——构造函数了。

**New和malloc，delete和free**

> new 分配内存按照数据类型进行分配，malloc 分配内存按照指定的大小分配 new 返回的是指定对象的指针，而 malloc 返回的是 void*，因此 malloc 的返回值一般都需要进行类型转化 new 不仅分配一段内存，而且会调用构造函数，malloc 不会 new 分配的内存要用 delete 销毁，malloc 要用 free 来销毁；delete 销毁的时候会调用对象的析构函数，而 free 则不会 new 是一个操作符可以重载，malloc 是一个库函数 malloc 分配的内存不够的时候，可以用 realloc 扩容。new 没用这样操作 new 如果分配失败了会抛出 bad_malloc 的异常，而 malloc 失败了会返回 NULL 申请数组时： new[]一次分配所有内存，多次调用构造函数，搭配使用 `delete[],delete[]` 多次调用析构函数，销毁数组中的每个对象。而 malloc 则只能 sizeof(int) * n

**C++四种类型转换**

1. const_cast: 用于将 const 变量转为非 const
2. static_cast: 用于各种隐式转换
3. dynamic_cast: 用于动态类型转换。只能用于含有虚函数的类
4. reinterpret_cast: 几乎什么都可以转

**用过数据库中的哪些事务**

**数据库的索引底层的实现**

> B+树(多路平衡树)

**查询时命中主键和普通值有什么区别**

> 主键是聚簇索引

**HTTP错误码有哪些了解么**

**事务回滚有什么实现机制？除了日志之外呢？**

> ublog 代码回滚

**进程和线程区别？进程与线程共享什么资源？线程独占什么资源？**

**介绍 UDP 协议**

**TCP三次握手四次挥手**

**TCP重传**

**指针和引用的区别**

- 指针是一个实体，需要分配内存空间。引用只是变量的别名，不需要分配内存空间
- 引用在定义的时候必须进行初始化，并且不能够改变。指针在定义的时候不一定要初始化，并且指向的空间可变
- 有多级指针，但是没有多级引用，只能有一级引用
- 指针和引用的自增运算结果不一样。（指针是指向下一个空间，引用时引用的变量值加 1）
- sizeof 引用得到的是所指向的变量（对象）的大小，而 sizeof 指针得到的是指针本身的大小
- 引用访问一个变量是直接访问，而指针访问一个变量是间接访问
- 使用指针前最好做类型检查，防止野指针的出现
- 使用指针前最好做类型检查，防止野指针的出现
- 作为参数时也不同，传指针的实质是传值，传递的值是指针的地址；传引用的实质是传地址，传递的是变量的地址

**一致性hash及其用途和数据倾斜如何解决** [详细](https://link.zhihu.com/?target=https%3A//blog.csdn.net/u010006625/article/details/103739974)

> 通过hash环来实现负载均衡，将不同的[服务器](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/product/cvm%3Ffrom%3D10680)hash映射到一致性hash环上，当服务请求到来时，使用hash将其映射到hash环上，然后可以采用如顺时针寻找的方法选择距其最近的服务器进行服务。 当服务器较少或hash公式不够好时，可能出现大多数请求都会落在同一个服务器上，这就是数据倾斜，可以采用添加服务器、虚拟节点、更换一致性hash的方法进行解决。

**list 和 vector 的区别， 用什么方法可以提升链表的查询效率**

- vector 和数组类似，拥有一段连续的内存空间，并且起始地址不变。因此能高效的进行随机存取
- list 是由双向链表实现的，因此内存空间是不连续的。只能通过指针访问数据，所以 list 的随机存取非常没有效率，时间复杂度为 o(n);但由于链表的特点，能高效地进行插入和删除。由于涉及对额外指针的维护，所以开销比较大
- 提升查询效率的方法： [详细](https://zhuanlan.zhihu.com/p/74018419) 1. 跳表 2. 散列表+链表

**有两张表：student（id,name），score（id,sid,score,course）假设所有学生都参与各科考试，没有任何缺考，查询出所有科目分数都大于80分的学生姓名**

**有2个大文件分布在两台机器上，每个文件1T，两个文件的diff大小1m，用最小的网络流量交换两个文件**



**Hash的实现方式有哪些**

> 直接地址法 平方取中法 除留余数

**浏览器打开一个网页经历了怎样的过程**

> dns解析，tcp链接，https连接中的ssl加密过程，https链接 [详细](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/xiaohuochai/p/9193083.html)

- 浏览器构建HTTP Request请求
- 网络传输
- 服务器构建HTTP Response 响应
- 网络传输
- 浏览器渲染页面

**Hash冲突的解决方式**

> 开放定址法: 当发生地址冲突时，按照某种方法继续探测哈希表中的其他存储单元，直到找到空位置为止 再哈希法：当发生哈希冲突时使用另一个哈希函数计算地址值，直到冲突不再发生 链地址法：将所有哈希值相同的 Key 通过链表存储。key 按顺序插入到链表中 建立公共溢出区：采用一个溢出表存储产生冲突的关键字。如果公共溢出区还产生冲突，再采用处理冲突方法处理

**Stl map是怎么实现的，为什么用红黑树不用hash**

> 红黑树，有序需求

**Linux查看一个100G的文件该用什么命令**

**cookie和session的区别？session是存在服务器的什么地方，怎么存储的？**

> 当用户首次与Web服务器建立连接的时候，服务器会给用户分发一个 SessionID作为标识。SessionID是一个由24个字符组成的随机字符串。用户每次提交页面，浏览器都会把这个SessionID包含在 HTTP头中提交给Web服务器，这样Web服务器就能区分当前请求页面的是哪一个客户端。这个SessionID就是保存在客户端的，属于客户端Session。 Cookie分为内存中Cookie（也可以说是进程中Cookie）和硬盘中Cookie。大部分的Session机制都使用进程中Cookie来保存Session id的，关闭浏览器后这个进程也就自动消失了，进程中的Cookie自然就消失了，那么Session id也跟着消失了，再次连接到服务器时也就无法找到原来的Session了。当然，我们可以在登陆时点击下次自动登录，比如说CSDN的“记住我一周”，或者我们的购物车信息可以在切换不同浏览器时依然可用。这就要用到我们上文提到的另一种Cookie了——硬盘中Cookie，这时Session id将长期保存在硬盘上的Cookie中，直到失效为止。 [详细](https://link.zhihu.com/?target=https%3A//blog.csdn.net/java_faep/article/details/78082802)

**两个1T文件如何找到公共部分**

- 将两个文件的数据分别通过hash映射到小文件中，然后依次比较每个小文件数据是否相同

**进程间有哪些通信方式？**

- 管道
- FIFO 有名管道
- 消息队列
- 信号量
- 信号
- 共享内存
- socket

**线程间通信方式**

- 临界区
- 互斥对象
- 信号量
- 事件对象

**C++ 智能指针**

> shared_ptr,unique_ptr,weak_ptr,auto_ptr

**红黑树查询、插入和删除的时间复杂度分别是多少？**

> O(log n)

**redis有几种数据类型**

**TCP和UDP的区别？能举一些常见的使用场景吗？有哪些使用他们的应用层协议？**

**https和http的区别是什么？**

**redis的缓存击穿是怎么出现的？有什么解决办法**

**https请求的完整过程**

- 浏览器请求连接
- 服务器返回证书：证书里面包含了网站地址，加密公钥，以及证书的颁发机构等信息
- 浏览器收到证书后作以下工作
- 验证证书的合法性
- 生成随机（对称）密码，取出证书中提供的公钥对随机密码加密
- 将之前生成的加密随机密码等信息发送给网站
- 服务器收到消息后作以下的操作
- 使用自己的私钥解密浏览器用公钥加密后的消息，并验证 HASH 是否与浏览器发来的一致；获得浏览器发过来的对称秘钥
- 使用加密的随机对称密码加密一段消息，发送给浏览器
- 浏览器解密并计算握手消息的 HASH：如果与服务端发来的 HASH 一致，此时握手过程结束，之后进行通信

**Linux用户态占用过高怎么排查**

> top, ps

**给你一个包含100亿个url的文件，请你找出使用频率最高的10个url，应该怎么做？**

> 用哈希分成多个文件，再把这些小文件一次性加到内存用map，取前10。最后从每个文件的前10

**如果用户在检索的时候，会出现返回的链接点进去是无效的或非相关的内容，如何发现和解决这个问题？**

**网络中有上亿个url，每个url都有一个id唯一标识，现在给你100台机器，你会怎么去设计他们的缓存系统?**

> 用哈希对流量分组，每台机器承接一定的流量，再搞点负载均衡的策略

**我的本地机器只有2.5G，但我想申请4G的内存空间，可以做到吗？**

> 取决于你的系统总线数量，如果是32位的，则最大的可寻址内存空间为4G，而如linux系统还要保留1G，则不能申请，如果是64位的则可以申请。

**具体谈谈地址空间存在的意义以及实现的方法？**

**http 和 https 区别，https 在请求时额外的过程，https 是如何保证数据安全的**

> [详细](https://link.zhihu.com/?target=https%3A//blog.csdn.net/ydonghao2/article/details/106975965)

**IP 地址子网划分**

> [详细](https://link.zhihu.com/?target=https%3A//blog.51cto.com/u_6930123/2113151)

**POST 和 GET 区别**

1. 发送机制不同，GET 一般用于查询/获取资源信息，而 POST 一般用于更新资源信息。
2. GET 请求的数据会附在 URL 之后，POST 把提交的数据放置在 HTTP 请求体中
3. GET 方式提交的数据最多只能是 1024 字节（取决于操作系统的支持），POST 理论上没有数据量的限制（取决于服务器的处理能力）
4. GET 方式提交的数据最多只能是 1024 字节（取决于操作系统的支持），POST 理论上没有数据量的限制（取决于服务器的处理能力）
5. GET 请求会被浏览器自动缓存，而 POST 不会，除非手动设置。在浏览器回退时，GET 是无害的，POST 会再次提交请求。GET 请求参数会被完整保留在浏览历史记录中，而 POST 中的参数不会被保留
6. 在发送请求时，GET 产生一个 TCP 数据包，服务器响应 200。POST 产生两个 TCP 数据包，浏览器先发送 header，响应 100，再发送 data，响应 200
7. GET 请求只能进行 url 编码，而 POST 支持多种编码方式

**DNS 解析过程**

1. 浏览器先检查自身缓存中有没有被解析过这个[域名](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/act/pro/domain-sales%3Ffrom%3D10680)对应的 ip 地址
2. 如果浏览器缓存没有命中，浏览器会检查操作系统缓存中有没有对应的已解析过的结果。在 windows 中可通过 c 盘里 hosts 文件来设置
3. 还没命中，请求本地域名服务器来解析这个域名，一般都会在本地域名服务器找到
4. 本地域名服务器没有命中，则去根域名服务器请求解析
5. 根域名服务器返回给本地域名服务器一个所查询域的主域名服务器
6. 本地域名服务器向主域名服务器发送请求
7. 接受请求的主域名服务器查找并返回这个域名对应的域名服务器的地址
8. 域名服务器根据映射关系找到 ip 地址，返回给本地域名服务器
9. 本地域名服务器缓存这个结果
10. 本地域名服务器将该结果返回给用户

**硬链接与软链接的区别**

> 软链接可以看作是 Windows 中的快捷方式，可以让你快速链接到目标档案或目录。硬链接则透过文件系统的 inode 来产生新档名，而不是产生新档案 硬链接(hard link)：A 是 B 的硬链接（A 和 B 都是文件名），则 A 的目录项中的 inode 节点号与 B 的目录项中的 inode 节点号相同，即一个 inode 节点对应两个不同的文件名，两个文件名指向同一个文件，A 和 B 对文件系统来说是完全平等的。如果删除了其中一个，对另外一个没有影响。每增加一个文件名，inode 节点上的链接数增加一，每删除一个对应的文件名，inode 节点上的链接数减一，直到为 0，inode 节点和对应的数据块被回收 软链接(soft link)：A 是 B 的软链接（A 和 B 都是文件名），A 的目录项中的 inode 节点号与 B 的目录项中的 inode 节点号不相同，A 和 B 指向的是两个不同的 inode，继而指向两块不同的数据块。但是 A 的数据块中存放的只是 B 的路径名（可以根据这个找到 B 的目录项）。A 和 B 之间是“主从”关系，如果 B 被删除了，A 仍然存在（因为两个是不同的文件），但指向的是一个无效的链接 不能对目录创建硬链接；不能对不同的文件系统创建硬链接；不能对不存在的文件创建硬链接 可以对目录创建软连接；可以跨文件系统；可以对不存在的文件创建软连接

**Kill用法，某个进程杀不掉的原因（进入内核态，忽略kill信号)**

- SIGNKILL（9） 的效果是立即杀死进程. 该信号不能被阻塞, 处理和忽略。
- SIGNTERM（15） 的效果是正常退出进程，退出前可以被阻塞或回调处理。并且它是Linux缺省的程序中断信号(默认是15)。
- kill - 9 表示强制杀死该进程；与SIGTERM相比，这个信号不能被捕获或忽略，同时接收这个信号的进程在收到这个信号时不能执行任何清理
- 处于内核态的进程会屏蔽所有信号
- 僵死进程也不能被kill

**系统管理命令（查看内存，网络情况）**

- free查看内存

> free -m --查看内存，不带单位 free -h --查看内存使用情况，带单位，显示查看结果 total：总计物理内存的大小 used：已使用内存 free：可用内存 Shared：多个进程共享的内存总额

- netstat查看网络情况

**管道的使用**

**消息队列，怎么实现的**

**grep的使用**

**find和awk的使用**

Linux 下的一些指令，（进程id），?（上一条命令退出时状态）

**怎么查看进程，按照内存大小，CPU 占用排序等等。（大写 M 和大写 P）** [详细](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/fwmlmx/p/5673986.html)

**介绍epoll**

**b 树和 b+ 树**

**进程间的通信，共享内存方式的优缺点**

> 共享内存是进程间通信中最简单的方式之一。共享内存允许两个或更多进程访问同一块内存，就如同 malloc() 函数向不同进程返回了指向同一个物理内存区域的指针。当一个进程改变了这块地址中的内容的时候，其它进程都会察觉到这个更改。 因为所有进程共享同一块内存，共享内存在各种进程间通信方式中具有最高的效率。访问共享内存区域和访问进程独有的内存区域一样快，并不需要通过系统调用或者其它需要切入内核的过程来完成。同时它也避免了对数据的各种不必要的复制。 因为系统内核没有对访问共享内存进行同步，您必须提供自己的同步措施。例如，在数据被写入之前不允许进程从共享内存中读取信息、不允许两个进程同时向同一个共享内存地址写入数据等。解决这些问题的常用方法是通过使用信号量进行同步。 共享内存块提供了在任意数量的进程之间进行高效双向通信的机制。每个使用者都可以读取写入数据，但是所有程序之间必须达成并遵守一定的协议，以防止诸如在读取信息之前覆写内存空间等竞争状态的出现。不幸的是，Linux无法严格保证提供对共享内存块的独占访问，甚至是在您通过使用IPC_PRIVATE创建新的共享内存块的时候也不能保证访问的独占性。 同时，多个使用共享内存块的进程之间必须协调使用同一个键值。

**给你一个系统，后台的逻辑已经实现了，但是前端加载很慢，怎么检测**

**分布式缓存的一致性，服务器如何扩容（哈希环）**

> 一致性hash

**项目中遇到的困难（提前想好，并且把实现或者优化方法说清楚）**

**用过哪些git命令**

**事务的特性**

> acid

**mysql索引，innodb和myisam的差别**

**唯一性索引或者开发分布式锁**

> redis

**了解哪些设计模式**

**多个线程达到同一个状态然后再一起执行，达到某一个状态之后再继续并发执行，这种怎么实现？**

**读写锁中加读锁后如何避免写线程饿死？**

**如何实现控制线程在某段时间内完成，不完成就撤销**

**程序装载进内存后分别在哪里**

> 代码在.init .text .rodata，初始化的全局变量在.data，未初始化的在.bss，还有堆和用户栈

**程序在调用中发生了什么？返回时又发生了什么？**

> 大概就是在用户栈中压入参数，返回地址，%ebp，接着跳转，开辟栈空间，保存需要保存的寄存器。返回时反过来从栈中进行恢复。

**同步/异步 阻塞/非阻塞**

**多态有哪些？分别怎么实现的？**

> 静态函数多态通过编译时不同的函数名来实现，不同的函数名是怎么组合出来的？ 动态多态通过虚函数实现 虚函数表头指针属于类还是对象？ 对象 虚函数表属于类还是对象？ 类 虚函数表存在哪里？ 静态数据区

**静态变量和非静态变量有什么区别？分别存在什么地方**

> 要分为静态全局变量，静态局部变量，非静态全局变量，非静态局部变量来答

**异步处理的幂等性**

> 幂等需要通过唯一的业务单号来保证。也就是说相同的业务单号，认为是同一笔业务。使用这个唯一的业务单号来确保，后面多次的相同的业务单号的处理逻辑和执行效果是一致的。 下面以支付为例，在不考虑并发的情况下，实现幂等很简单：1. 先查询一下订单是否已经支付过，2. 如果已经支付过，则返回支付成功；如果没有支付，进行支付流程，修改订单状态为‘已支付’。

**volatile**

**加密算法** MD5 -- 对称加密算法 RSA -- 非对称加密算法

**索引数据结构**

**B+树、hash索引，bitMap位图索引**

**可以用两个epoll监听同一个描述符吗，有事件发生时，怎么工作**

**segment fault怎么用gdb调试，两个函数循环调用会怎样**

> [详细](https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_34205076/article/details/91607309) g++ -g 指明在编译的时候，产生调试信息 编译完成后输入使用gdb进入调试模式 run编译的程序会产生段错误，输入bt跟踪段错误 使用frame *参看具体出错的代码

**listen的两个参数分别是什么**

> 连接的socket以及队列长度

**send、recv都是阻塞的，怎么控制在一定时间内完成send、recv，如果超时就不执行，怎么跟select联系起来**

**socket关闭时，两端的状态变化**

> 四次挥手

**epoll底层使用了红黑树，说一下红黑树的特点和优点**

> epoll内核中维护了一个内核事件表，它是将所有的文件描述符全部都存放在内核中，系统去检测有事件发生的时候触发回调，当你要添加新的文件描述符的时候也是调用epoll_ctl函数使用。 现在要在内核中长久的维护一个数据结构来存放文件描述符，并且时常会有插入，查找和删除的操作发生，这对内核的效率会产生不小的影响，因此需要一种插入，查找和删除效率都不错的数据结构来存放这些文件描述符，那么红黑树当然是不二的人选

**printf的可变参数是怎么实现的，如果参数个数不匹配会发生什么，比如字符串需要3个参数，但是只传了2个或者4个分别会发生什么**

**函数调用栈里面存储的是什么**

> 函数调用时： 主函数的下一条指令的地址入栈 函数的参数入栈，从右往左入栈 函数的局部变量入栈。注意：静态变量不入栈。 free怎么释放 使用指针，怎么尽量避免segment fault token口令的实现原理，产生口令的客户端和服务端是否需要通信，具体实现接口（输入和输出）

**gdb调试core文件**

**5种I/O模型介绍一下**

1. 阻塞IO
2. 非阻塞IO
3. IO复用
4. 异步IO
5. 信号驱动IO

**shared_ptr的实现**

**UDP数据包长度多少**

> 数据链路中数据帧内容最大长度为1500，IP头部需要20字节，UDP头部8字节，所以MTU为1472，TCP首部20字节

**进程的状态有几种**

**数据库少了某些字段，现在要让你加，你怎么办？设计上有没有考虑可扩展性？**

**cookie有存什么东西吗**

**Linux静态库和动态库有什么区别？动态库的加载器是哪个**

**glibc是干什么的**

> glibc是GNU发布的libc库，即c运行库。glibc是linux系统中最底层的api

**select什么时候比epoll好**

> 少连接，高并发。连接少意味着不会超过select 1024的上限，高并发意味着一次wait每一个连接都会来数据

**线程池怎么实现**

**平面里画出9个点10条边怎么画**

**分布式数据如何保证一致性**

**TCP四次挥手断开连接，为什么主动断开连接的那一方要有TIME_WAIT状态**

**Linux中查看端口、查找某个进程ID分别使用哪个命令**

> netstat, top

**SQL语句中，order by 会用到索引吗**

**数据库索引覆盖问题，如果在修改数据时不按照索引的顺序，会怎样**

**Linux如何去查找某个文件，找出文件中的第10至20行**

> find head -n 20 file | tail -n 10

**百度有post请求吗**

**算法题**

循环队列实现

旋转数组找最小值

**跳台阶的题，每次跳一步或两步，有n级台阶，有多少种跳法**

```text
class Solution {
public:
    int numWays(int n) {
        if(n <= 1) return 1;
        if(n == 2) return 2;
        vector<long long> vec(n+1, 1);
        vec[2] = 2;
        for(int i = 3;i <= n;++i) vec[i] = (vec[i-1] + vec[i-2])%1000000007;
        return vec[n];
    }
};
```

**给一个字符串，找出最长不重复子串**

```text
class Solution {
public:
    int lengthOfLongestSubstring(string s) {
        if(s.size() <= 1) return s.size();
        unordered_map<char, bool> m;
        int max_len = 1, i = 0, j = 1;
        while(j < s.size()){
            m[s[i]] = true;
            if(!m[s[j]]){
                m[s[j]] = true;
                ++j;
                if(j-i > max_len) max_len = j-i;
            }else{
                m[s[i]] = false;
                ++i;
                j = max(j, i+1);
            }            
        }
        return max_len;
    }
};
```

**给一个n\*m的矩阵，0表示是水，1表示是陆地，求整个空间内有几块分离的岛屿**

> dfs

**前序、中序和后序遍历**

**用加减乘除四种运算求根号二的值，保留小数点后10位**

**两个链表找公共节点**

**给一个函数，返回 0 和 1，概率为 p 和 1-p，请你实现一个函数，使得返回 0 1 概率一样**

```text
#include<stdlib.h>
#include<iostream>
#include<time.h>
int main(){
    srand(time(NULL));
    int one_num = 0, zero_num = 0;
    for(int i = 0;i < 10000;++i){
        if(rand() % 10 >= 5) ++one_num;
        else ++zero_num;
    }
    cout << "num of one: " << one_num << endl;
    cout << "num of zero: " << zero_num << endl;
    return 0;
}
```

**如何理解IO复用？poll，select，epoll有什么区别**

**把一个 bst 转化成一个双向链表**

手写一个全排列

写个 strcpy 函数

英文语句倒序输出

1亿个数中找1000个最大的

八个字母共有多少组合

一个有序数组，找出两个值加起来等于key值

找出1到n中重复的数字

合并两个排序链表

100G大文件排序，桶排序？

如何判断链表是否有环

最长公共子串

僵尸吃人问题，僵尸吃人后变成人，一个人能够被两个僵尸吃，人被吃后牺牲。问最后多少人存活

1-100以内的素数

原文地址：https://zhuanlan.zhihu.com/p/561562441

作者：linux

# 【NO.63】linux 零拷贝（ zero-copy ）技术原理详解

## 1.引言

传统的 Linux 操作系统的标准 I/O 接口是基于数据拷贝操作的，即 I/O 操作会导致数据在操作系统内核地址空间的缓冲区和应用程序地址空间定义的缓冲区之间进行传输。这样做最大的好处是可以减少磁盘 I/O 的操作，因为如果所请求的数据已经存放在操作系统的高速缓冲存储器中，那么就不需要再进行实际的物理磁盘 I/O 操作。但是数据传输过程中的数据拷贝操作却导致了极大的 CPU 开销，限制了操作系统有效进行数据传输操作的能力。

零拷贝（ zero-copy ）技术可以有效地改善数据传输的性能，在内核驱动程序（比如网络堆栈或者磁盘存储驱动程序）处理 I/O 数据的时候，零拷贝技术可以在某种程度上减少甚至完全避免不必要 CPU 数据拷贝操作。

## 2.什么是零拷贝？

零拷贝就是一种避免 CPU 将数据从一块存储拷贝到另外一块存储的技术。针对操作系统中的设备驱动程序、文件系统以及网络协议堆栈而出现的各种零拷贝技术极大地提升了特定应用程序的性能，并且使得这些应用程序可以更加有效地利用系统资源。这种性能的提升就是通过在数据拷贝进行的同时，允许 CPU 执行其他的任务来实现的。

零拷贝技术可以减少数据拷贝和共享总线操作的次数，消除传输数据在存储器之间不必要的中间拷贝次数，从而有效地提高数据传输效率。而且，零拷贝技术减少了用户应用程序地址空间和操作系统内核地址空间之间因为上下文切换而带来的开销。进行大量的数据拷贝操作其实是一件简单的任务，从操作系统的角度来说，如果 CPU 一直被占用着去执行这项简单的任务，那么这将会是很浪费资源的；如果有其他比较简单的系统部件可以代劳这件事情，从而使得 CPU 解脱出来可以做别的事情，那么系统资源的利用则会更加有效。综上所述，零拷贝技术的目标可以概括如下：

避免数据拷贝

①避免操作系统内核缓冲区之间进行数据拷贝操作。
②避免操作系统内核和用户应用程序地址空间这两者之间进行数据拷贝操作。
③用户应用程序可以避开操作系统直接访问硬件存储。
④数据传输尽量让 DMA 来做。

将多种操作结合在一起

①避免不必要的系统调用和上下文切换。
②需要拷贝的数据可以先被缓存起来。
③对数据进行处理尽量让硬件来做。

接下来就探讨Linux中**主要的几种零拷贝技术**以及零拷贝技术**适用的场景**。为了迅速建立起零拷贝的概念，我们拿一个常用的场景进行引入：

## 3.引文

在写一个服务端程序时（Web Server或者文件服务器），文件下载是一个基本功能。这时候服务端的任务是：**将服务端主机磁盘中的文件不做修改地从已连接的socket发出去**，我们通常用下面的代码完成：

```text
while((n = read(diskfd, buf, BUF_SIZE)) > 0)
    write(sockfd, buf , n);
```

基本操作就是循环的从磁盘读入文件内容到缓冲区，再将缓冲区的内容发送到`socket`。但是由于Linux的`I/O`操作默认是缓冲`I/O`。这里面主要使用的也就是`read`和`write`两个系统调用，我们并不知道操作系统在其中做了什么。实际上在以上`I/O`操作中，发生了多次的数据拷贝。

当应用程序访问某块数据时，操作系统首先会检查，是不是最近访问过此文件，文件内容是否缓存在内核缓冲区，如果是，操作系统则直接根据`read`系统调用提供的`buf`地址，将内核缓冲区的内容拷贝到`buf`所指定的用户空间缓冲区中去。如果不是，操作系统则首先将磁盘上的数据拷贝的内核缓冲区，这一步目前主要依靠`DMA`来传输，然后再把内核缓冲区上的内容拷贝到用户缓冲区中。

接下来，`write`系统调用再把用户缓冲区的内容拷贝到网络堆栈相关的内核缓冲区中，最后`socket`再把内核缓冲区的内容发送到网卡上。

说了这么多，不如看图清楚：

![img](https://pic2.zhimg.com/80/v2-636202ea28493cb1790f08fd7d14a1c1_720w.webp)

数据拷贝

从上图中可以看出，共产生了四次数据拷贝，即使使用了`DMA`来处理了与硬件的通讯，CPU仍然需要处理两次数据拷贝，与此同时，在用户态与内核态也发生了多次上下文切换，无疑也加重了CPU负担。

在此过程中，我们没有对文件内容做任何修改，那么在内核空间和用户空间来回拷贝数据无疑就是一种浪费，而零拷贝主要就是为了解决这种低效性。

## 4.什么是零拷贝技术（zero-copy）？

零拷贝主要的任务就是**避免**CPU将数据从一块存储拷贝到另外一块存储，主要就是利用各种零拷贝技术，避免让CPU做大量的数据拷贝任务，减少不必要的拷贝，或者让别的组件来做这一类简单的数据传输任务，让CPU解脱出来专注于别的任务。这样就可以让系统资源的利用更加有效。

我们继续回到引文中的例子，我们如何减少数据拷贝的次数呢？一个很明显的着力点就是减少数据在内核空间和用户空间来回拷贝，这也引入了零拷贝的一个类型：

让数据传输不需要经过user space

### 4.1使用mmap

我们减少拷贝次数的一种方法是调用mmap()来代替read调用：

```text
buf = mmap(diskfd, len);
write(sockfd, buf, len);
```

应用程序调用`mmap()`，磁盘上的数据会通过`DMA`被拷贝的内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用`write()`,操作系统直接将内核缓冲区的内容拷贝到`socket`缓冲区中，这一切都发生在内核态，最后，`socket`缓冲区再把数据发到网卡去。

同样的，看图很简单：

![img](https://pic3.zhimg.com/80/v2-fc408a7d628df3a5ef0d29a4b97316ee_720w.webp)



### 4.2mmap

使用mmap替代read很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。但是使用`mmap`是有代价的。当你使用`mmap`时，你可能会遇到一些隐藏的陷阱。例如，当你的程序`map`了一个文件，但是当这个文件被另一个进程截断(truncate)时, write系统调用会因为访问非法地址而被`SIGBUS`信号终止。`SIGBUS`信号默认会杀死你的进程并产生一个`coredump`,如果你的服务器这样被中止了，那会产生一笔损失。

通常我们使用以下解决方案避免这种问题：

1. **为SIGBUS信号建立信号处理程序**
   当遇到`SIGBUS`信号时，信号处理程序简单地返回，`write`系统调用在被中断之前会返回已经写入的字节数，并且`errno`会被设置成success,但是这是一种糟糕的处理办法，因为你并没有解决问题的实质核心。
2. **使用文件租借锁**
   通常我们使用这种方法，在文件描述符上使用租借锁，我们为文件向内核申请一个租借锁，当其它进程想要截断这个文件时，内核会向我们发送一个实时的`RT_SIGNAL_LEASE`信号，告诉我们内核正在破坏你加持在文件上的读写锁。这样在程序访问非法内存并且被`SIGBUS`杀死之前，你的`write`系统调用会被中断。`write`会返回已经写入的字节数，并且置`errno`为success。

我们应该在`mmap`文件之前加锁，并且在操作完文件后解锁：

```text
if(fcntl(diskfd, F_SETSIG, RT_SIGNAL_LEASE) == -1) {
    perror("kernel lease set signal");
    return -1;
}
/* l_type can be F_RDLCK F_WRLCK  加锁*/
/* l_type can be  F_UNLCK 解锁*/
if(fcntl(diskfd, F_SETLEASE, l_type)){
    perror("kernel lease set type");
    return -1;
}
```

### 4.3使用sendfile

从2.1版内核开始，Linux引入了`sendfile`来简化操作:

```text
#include<sys/sendfile.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

系统调用`sendfile()`在代表输入文件的描述符`in_fd`和代表输出文件的描述符`out_fd`之间传送文件内容（字节）。描述符`out_fd`必须指向一个套接字，而`in_fd`指向的文件必须是可以`mmap`的。这些局限限制了`sendfile`的使用，使`sendfile`只能将数据从文件传递到套接字上，反之则不行。

使用`sendfile`不仅减少了数据拷贝的次数，还减少了上下文切换，数据传送始终只发生在`kernel space`。

![img](https://pic4.zhimg.com/80/v2-303b963b10d4e907c82ed5e0203310af_720w.webp)

### 4.4sendfile系统调用过程

在我们调用`sendfile`时，如果有其它进程截断了文件会发生什么呢？假设我们没有设置任何信号处理程序，`sendfile`调用仅仅返回它在被中断之前已经传输的字节数，`errno`会被置为success。如果我们在调用sendfile之前给文件加了锁，`sendfile`的行为仍然和之前相同，我们还会收到RT_SIGNAL_LEASE的信号。

目前为止，我们已经减少了数据拷贝的次数了，但是仍然存在一次拷贝，就是页缓存到socket缓存的拷贝。那么能不能把这个拷贝也省略呢？

借助于硬件上的帮助，我们是可以办到的。之前我们是把页缓存的数据拷贝到socket缓存中，实际上，我们仅仅需要把缓冲区描述符传到`socket`缓冲区，再把数据长度传过去，这样`DMA`控制器直接将页缓存中的数据打包发送到网络中就可以了。

总结一下，`sendfile`系统调用利用`DMA`引擎将文件内容拷贝到内核缓冲区去，然后将带有文件位置和长度信息的缓冲区描述符添加socket缓冲区去，这一步不会将内核中的数据拷贝到socket缓冲区中，`DMA`引擎会将内核缓冲区的数据拷贝到协议引擎中去，避免了最后一次拷贝。

![img](https://pic1.zhimg.com/80/v2-6cf7009e5788af6d9b80ecc15faf5cbc_720w.webp)

**带DMA的sendfile**

不过这一种收集拷贝功能是需要硬件以及驱动程序支持的。

### 4.5使用splice

sendfile只适用于将数据从文件拷贝到套接字上，限定了它的使用范围。Linux在`2.6.17`版本引入`splice`系统调用，用于在两个文件描述符中移动数据：

```text
#define _GNU_SOURCE         /* See feature_test_macros(7) */
#include <fcntl.h>
ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);
```

splice调用在两个文件描述符之间移动数据，而不需要数据在内核空间和用户空间来回拷贝。他从`fd_in`拷贝`len`长度的数据到`fd_out`，但是有一方必须是管道设备，这也是目前`splice`的一些局限性。`flags`参数有以下几种取值：

- **SPLICE_F_MOVE** ：尝试去移动数据而不是拷贝数据。这仅仅是对内核的一个小提示：如果内核不能从`pipe`移动数据或者`pipe`的缓存不是一个整页面，仍然需要拷贝数据。Linux最初的实现有些问题，所以从`2.6.21`开始这个选项不起作用，后面的Linux版本应该会实现。
- ** SPLICE_F_NONBLOCK** ：`splice` 操作不会被阻塞。然而，如果文件描述符没有被设置为不可被阻塞方式的 I/O ，那么调用 splice 有可能仍然被阻塞。
- ** SPLICE_F_MORE**： 后面的`splice`调用会有更多的数据。

splice调用利用了Linux提出的管道缓冲区机制， 所以至少一个描述符要为管道。

以上几种零拷贝技术都是减少数据在用户空间和内核空间拷贝技术实现的，但是有些时候，数据必须在用户空间和内核空间之间拷贝。这时候，我们只能针对数据在用户空间和内核空间拷贝的时机上下功夫了。Linux通常利用**写时复制(copy on write)**来减少系统开销，这个技术又时常称作`COW`。

由于篇幅原因，本文不详细介绍写时复制。大概描述下就是：如果多个程序同时访问同一块数据，那么每个程序都拥有指向这块数据的指针，在每个程序看来，自己都是独立拥有这块数据的，只有当程序需要对数据内容进行修改时，才会把数据内容拷贝到程序自己的应用空间里去，这时候，数据才成为该程序的私有数据。如果程序不需要对数据进行修改，那么永远都不需要拷贝数据到自己的应用空间里。这样就减少了数据的拷贝。写时复制的内容可以再写一篇文章了。。。

除此之外，还有一些零拷贝技术，比如传统的Linux I/O中加上`O_DIRECT`标记可以直接`I/O`，避免了自动缓存，还有尚未成熟的`fbufs`技术，本文尚未覆盖所有零拷贝技术，只是介绍常见的一些，如有兴趣，可以自行研究，一般成熟的服务端项目也会自己改造内核中有关I/O的部分，提高自己的数据传输速率。

## 5.零拷贝原理

### 5.1 io读写的方式

#### 5.1.1 中断

#### 5.12 DMA

### 5.2 中断方式

#### 5.2.1 中断方式的流程图如下：

![img](https://pic2.zhimg.com/80/v2-68f188488d6583878afcb220591cdd9d_720w.webp)

①用户进程发起数据读取请求

②系统调度为该进程分配cpu

③cpu向io控制器(ide,scsi)发送io请求

④用户进程等待io完成，让出cpu

⑤系统调度cpu执行其他任务

⑥数据写入至io控制器的缓冲寄存器

⑦缓冲寄存器满了向cpu发出中断信号

⑧cpu读取数据至内存

#### 5.2.2 缺点：中断次数取决于缓冲寄存器的大小

### 5.3 DMA ： 直接内存存取

#### 5.3.1 DMA方式的流程图如下：

![img](https://pic3.zhimg.com/80/v2-bfb79b2982aa6e491d71e582f578f79e_720w.webp)

①用户进程发起数据读取请求

②系统调度为该进程分配cpu

③cpu向DMA发送io请求

④用户进程等待io完成，让出cpu

⑤系统调度cpu执行其他任务

⑥数据写入至io控制器的缓冲寄存器

⑦DMA不断获取缓冲寄存器中的数据（需要cpu时钟）

⑧传输至内存（需要cpu时钟）

⑨所需的全部数据获取完毕后向cpu发出中断信号

#### 5.3.2 优点：减少cpu中断次数，不用cpu拷贝数据

### 5.4 数据拷贝

#### 5.4.1 下面展示了 传统方式读取数据后并通过网络发送 所发生的数据拷贝：

![img](https://pic4.zhimg.com/80/v2-a6e56cdfb0ff9444301fbbc96d131b13_720w.webp)

①一个read系统调用后，DMA执行了一次数据拷贝，从磁盘到内核空间

②read结束后，发生第二次数据拷贝，由cpu将数据从内核空间拷贝至用户空间

③send系统调用，cpu发生第三次数据拷贝，由cpu将数据从用户空间拷贝至内核空间(socket缓冲区)

④send系统调用结束后，DMA执行第四次数据拷贝，将数据从内核拷贝至协议引擎

⑤另外，这四个过程中，每个过程都发生一次上下文切换

#### 5.4.2 内存缓冲数据，主要是为了提高性能，内核可以预读部分数据，当所需数据小于内存缓冲区大小时，将极大的提高性能。

#### 5.4.3 零拷贝是为了消除这个过程中冗余的拷贝

## 6.零拷贝-sendfile 对应到java中

FileChannel.transferTo(long position, long count, WritableByteChannel target)//将数据从文件通道传输到了给定的可写字节通道

### 6.1避免了第2，3步的数据拷贝，参考下图：

![img](https://pic1.zhimg.com/80/v2-0fb110b4e3d2fa1637bc231683d46b70_720w.webp)

①DMA从拷贝至内核缓冲区

②cpu将数据从内核缓冲区拷贝至内核空间(socket缓冲区)

③DMA将数据从内核拷贝至协议引擎

④这三个过程中共发生2次上下文切换，分别为发起读取文件和发送数据

### 6.2以上过程发生了三次数据拷贝，其中有一次为cpu完成

### 6.3linux内核2.4以后，socket缓冲区做了调整，DMA带收集功能，如下图：

![img](https://pic2.zhimg.com/80/v2-a041c34c1f094d2911bf09556a060d45_720w.webp)

①DMA从拷贝至内核缓冲区

②将数据的位置和长度的信息的描述符增加至内核空间(socket缓冲区)

③DMA将数据从内核拷贝至协议引擎

## 7.零拷贝-mmap 对应到java中

MappedByteBuffer//文件内存映射

#### 7.1数据不会复制到用户空间，只在内核空间，与sendfile类似，但是应用程序可以直接操作该内存。

原文地址：https://zhuanlan.zhihu.com/p/558465064

作者：linux

# 【NO.64】C++数据结构与算法：布隆过滤器（Bloom Filter）原理与实现

文本代码下载地址：Github：[https://github.com/dongyusheng/csdn-code/tree/master/BloomFilter](https://link.zhihu.com/?target=https%3A//github.com/dongyusheng/csdn-code/tree/master/BloomFilter)

## 1.什么是布隆过滤器

布隆过滤器（Bloom Filter）是1970年由布隆提出的

它实际上是一个**很长的二进制向量和一系列随机映射函数**。布隆过滤器可以用于检索一个元素是否在一个集合中

**优点：**

- 可以高效地进行查询，可以用来告诉你“某样东西一定不存在或者可能存在”
- 可以高效的进行插入
- 相比于传统的List、Set、Map等数据结构，它占用空间更少，因为其本身并不存储任何数据（重点）

**缺点：**

- 其返回的结果是概率性（存在误差）的
- 一般不提供删除操作

布隆过滤器一般使用在数据量特别大的场景下，一般不会使用

用的使用场景：

- 使用word文档时，判断某个单词是否拼写正确。例如我们在编写word时，某个单词错误那么就会在单词下面显示红色波浪线
- 网络爬虫程序，不去爬相同的url页面
- 垃圾邮件的过滤算法
- 缓存崩溃后造成的缓存击穿
- 集合重复元素的判别
- 查询加速（比如基于key-value的存储系统，如redis等）

## 2.什么时候选择布隆过滤器，而不使用其他数据结构

如果想要判断一个元素是不是在一个集合里，一般想到的是将所有元素保存起来，然后通过比较确定。链表，树、哈希表等数据结构都是这种思路（如下图所示）

![img](https://pic3.zhimg.com/80/v2-fd393023f85161cf69394abfa297357e_720w.webp)

上面这些数据结构面对数据量特别大的时候显现的缺点：

- 存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的
- 当数据量特别大时，会占用大量的内存空间。如果存储了类似于URL这样的key，那么内存消费太严重
- 如果使用hashmap，如果已有元素超过了总容量的一半之后，一般就需要考虑扩容了，因为元素多了之后哈希冲突就会增加，退化为链表存储的效率了

**下面是两个测试程序，分别测试hashmap和红黑树**，当元素特别多时，其查询和占用的内存会非常大

**测试map（内部使用红黑树）**

```text
#include <iostream>
#include <map>
#include <string>
#include <sys/time.h>
#include <utility>
#include <iomanip>
 
#define MAP_ITEMS 100000
 
using namespace std;
 
int main()
{
	std::map<std::string, bool> mp;
	
	timeval startTime, endTime;
	
	//1.插入MAP_ITEMS个元素到map中
	gettimeofday(&startTime, NULL);
	std::string key = "https://blog.csdn.net/qq_41453285";
	for(int i = 0; i < MAP_ITEMS; ++i){
		string sub_key = to_string(i);
		mp.insert(std::make_pair(key + sub_key, 1));
	}
	
	gettimeofday(&endTime, NULL);
	long insert_time = (endTime.tv_sec - startTime.tv_sec)*1000 + (endTime.tv_usec-startTime.tv_usec)/1000;
	
	//2.在map中查找一个元素
	gettimeofday(&startTime, NULL);
	if( mp.find(key + "10000") == mp.end())
		std::cout << "not found!" << std::endl;
	
	gettimeofday(&endTime, NULL);
	long find_time = endTime.tv_usec - startTime.tv_usec;
	
	//3.估算当前key的平均大小
	double key_size = key.size() + to_string(MAP_ITEMS).size()/2;
	
	//4.打印相关信息
	std::cout << "Number of members  " << "key size  " << "insert time(ms)  " << "find time(us)  " << std::endl;
	std::cout << left << setw(19) << MAP_ITEMS;
	std::cout << left << setw(10) << key_size;
	std::cout << left << setw(17) << insert_time;
	std::cout << left << setw(15) << find_time << std::endl;
}
```

**代码中的MAP_ITEMS常量代表当前map中存储的元素的个数**

当MAP_ITEMS为100000时，结果如下：

![img](https://pic2.zhimg.com/80/v2-a5926459485bfcd78e034acb212ce005_720w.webp)

当MAP_ITEMS为1000000时，结果如下：

![img](https://pic1.zhimg.com/80/v2-cd276da5ad69a134e5d0cb6b8dbc2048_720w.webp)

当MAP_ITEMS为10000000时，结果如下：

![img](https://pic3.zhimg.com/80/v2-c0ba50d58bbea8f735933a6355b6e776_720w.webp)



**测试unordered_map（内部使用hashmap）**

```text
#include <iostream>
#include <unordered_map>
#include <string>
#include <sys/time.h>
#include <utility>
#include <iomanip>
 
#define MAP_ITEMS 100000
 
using namespace std;
 
int main()
{
	unordered_map<string, bool> unordermp;
	
	timeval startTime, endTime;
	
	//1.插入MAP_ITEMS个元素到map中
	gettimeofday(&startTime, NULL);
	std::string key = "https://blog.csdn.net/qq_41453285";
	for(int i = 0; i < MAP_ITEMS; ++i){
		string sub_key = to_string(i);
		unordermp.insert(std::make_pair(key + sub_key, 1));
	}
	
	gettimeofday(&endTime, NULL);
	long insert_time = (endTime.tv_sec - startTime.tv_sec)*1000 + (endTime.tv_usec-startTime.tv_usec)/1000;
	
	//2.在map中查找一个元素
	gettimeofday(&startTime, NULL);
	if( unordermp.find(key + "10000") == unordermp.end())
		std::cout << "not found!" << std::endl;
	
	gettimeofday(&endTime, NULL);
	long find_time = endTime.tv_usec - startTime.tv_usec;
	
	//3.估算当前key的平均大小
	double key_size = key.size() + to_string(MAP_ITEMS).size()/2;
	
	//4.打印相关信息
	std::cout << "Number of members  " << "key size  " << "insert time(ms)  " << "find time(us)  " << std::endl;
	std::cout << left << setw(19) << MAP_ITEMS;
	std::cout << left << setw(10) << key_size;
	std::cout << left << setw(17) << insert_time;
	std::cout << left << setw(15) << find_time << std::endl;
}
```

**代码中的MAP_ITEMS常量代表当前unordered_map中存储的元素的个数**

当MAP_ITEMS为100000时，结果如下：

![img](https://pic1.zhimg.com/80/v2-b4d98c4bd16631555829f927933dc8f0_720w.webp)

当MAP_ITEMS为1000000时，结果如下：

![img](https://pic2.zhimg.com/80/v2-34e57284cb7efe4d3cc977a2a4c218a1_720w.webp)

当MAP_ITEMS为10000000时，结果如下：

![img](https://pic2.zhimg.com/80/v2-175057b0fdd6c5dab4c9083eab9c7acd_720w.webp)

## 3.布隆过滤器的数据结构与实现原理

### 3.1数据结构

**布隆过滤器是一个bit向量或者说是一个bit数组（下面的数字为索引）**。如下所示：

![img](https://pic2.zhimg.com/80/v2-4edd9b19517d3af7cbfb6bcdf81a3bd1_720w.webp)

**其最小单位为bit，初始化时全部置为0**

添加、查询原理

布隆过滤器添加原理：利用K个Hash函数，将元素传入到这K个Hash函数中，并且映射到bit向量的K个点中，并且将映射到的K个点置为1

布隆过滤器查询原理：

- 利用K个Hash函数，将元素传入到这K个Hash函数中，并且映射到bit向量的K个点中
- 如果这些点中有任何一个为0，则被检测的元素一定不存在
- 如果这些点都返回1，则被检测的元素很可能（因为布隆过滤器存在误差）存在，但是不一定百分百存在

上面添加、查询使用的Hash函数一般都是相同的，实现设计好的

为什么布隆过滤器要使用多个Hash函数？

- Hash面临的问题就是冲突。假设Hash函数是良好的，如果我们的位阵列长度为m个点，那么如果我们想将冲突率降低到例如 1%，这个散列表就只能容纳 m/100个元素
- 解决方法较简单，使用K>1的布隆过滤器，即K个函数将每个元素改为对应于K个bits，因为误判度会降低很多，并且如果参数k和m选取得好，一半的m可被置为1

**一个重要的概念：**针对于一个特定的哈希函数和一个特定的值，那么该哈希函数返回的值每次都是固定的，不可能出现多次调用之间出现哈希函数返回值不同的情况

### 3.2演示说明

假设我们的布隆过滤器有三个哈希函数，分别名为hash1、hash2、hash3

**①添加元素：**针对于“baidu”这个元素，我们调用三个哈希函数，将其映射到bit向量的三个位置（分别为1、4、7），并且将对应的位置置为1

![img](https://pic4.zhimg.com/80/v2-65dafedd27d1744c9d27a00297648dff_720w.webp)

**②添加元素：**现在针对于“tencent”这个元素，我们也调用三个哈希函数，将其映射到bit向量的三个位置（分别为3、4、8），并且将对应的位置置为1

![img](https://pic2.zhimg.com/80/v2-33e7aa1cebfeff849a6a67f16faebd11_720w.webp)

③此时，整个bit向量的1、3、4、7、8这几个位置被置为1了。其中4这个索引被覆盖了，因为“baidu”和“tencent”都将其置为1，覆盖的索引与误判率有关，详情见下面的介绍

④去查询一个不存在的元素，并且确定其肯定不存在：例如现在我们去查询“dongshao”这个元素，假设调用上面的三个哈希函数返回的索引是1、5、8，通过上图我们知道5这个索引处为0，因此“dongshao”这个元素一定不存在，因为如果存在的话，那么5这个位置应该被置为1才对（见上面的“一个重要概念”）

⑤去查询“baidu”这个元素，不能判断其百分百存在：我们将“baidu”传入上面的三个哈希函数中，哈希返回的对应索引值为1、4、7，发现1、4、7这几个索引处都为1，因此我们判断“baidu”这个元素可能存在。为什么不是百分百确定呢？见下面的误判率介绍

### 3.3误判率

布隆过滤器允许存在一定的误判断，误判率也称为“假阳”

误判率一般是出现在查询的时候

例如上面我们去查询“baidu”的时候，由于“baidu”之前被我们插入过，为什么还不能百分百确定它一定存在呢？

- 因为“tencent”这个元素在插入的时候，将4这个索引置为1了
- 假设我们查询“baidu”的时候实际返回的是1、7索引为1，4索引为0。而4索引又被tencent覆盖为1，所以最终“baidu”最终看到的是1、4、7索引都为1，我们不能百分百确定“baidu”这个元素存在

因此，当随着增加的值越来越多时，bit向量被置为1的数量也就会越来越多，因此误判率会越来越大。例如，当查询“taobao”时，万一所有的哈希函数返回的对应bit都为1，那么布隆过滤器可能也认为“taobao”这个元素存在

### 3.4布隆过滤器一般不拥有删除的功能

我们一般不能从布隆过滤器中删除元素。考虑下面几种情况：

- 因为要删除该元素，我们必须百分百确保该元素存在于布隆过滤器中，而布隆过滤器由于存在误判率，无法确定该元素百分百存在于布隆过滤器内
- 另外计数器回绕也会造成问题
- 如果我们因为某一个元素而将其对应的bit位删除变为0，那么如果这些bit位也是其他元素正在使用的，那么其他元素在查询时就会返回0，从而认为元素不存在而造成误判

## 4.误判概率的相关证明和计算

### 4.1 证明①（哈希函数越多、插入元素越少，误判率越低）

假设布隆过滤器中的hash函数满足simple uniform hashing(简单一致散列)假设：每个元素都等概率地hash到m个slot中的任何一个，与其它元素被hash到哪个slot无关

若m为bit数（向量表的长度）， 则对某一特定bit位在一个元素由某特定hash函数插入时没有被置位为1的概率为：

![img](https://pic1.zhimg.com/80/v2-ce557415a3d9b72aea102c6c61448d5c_720w.webp)

则k个hash函数中没有一个对其置位的概率为，随着k的增加，概率会变小：

![img](https://pic4.zhimg.com/80/v2-b11ecd2ae4acc82001f71525e6c0790b_720w.webp)

如果插入了n个元素，但都没有将其置位的概率为：

![img](https://pic3.zhimg.com/80/v2-9c6d65ceec6429c9a23b9e53ead31f8e_720w.webp)

现在考虑查询阶段，若对应某个待查询元素的k bits全部置位为1，则可判定其在集合中。 因此将某元素误判的概率p为：

![img](https://pic4.zhimg.com/80/v2-26b24706ea5cd3423718f07f90d92ec3_720w.webp)

现在考虑查询阶段，若对应某个待query元素的k bits全部置位为1，则可判定其在集合中。 因此将某元素误判的概率p为：

![img](https://pic3.zhimg.com/80/v2-95ca5d633d777673905ee4e682d68b96_720w.webp)

由于

![img](https://pic2.zhimg.com/80/v2-32ae1714c379b872c4596b2bb86f33f5_720w.webp)

当x→0时，并且

![img](https://pic4.zhimg.com/80/v2-88118a3e86be84a225dcd50a7bc11337_720w.webp)

当m很大时趋近于0，所以：

![img](https://pic4.zhimg.com/80/v2-3fe256b2518a3ffedea8dbd90bcec71f_720w.webp)

从上式中可以看出，当m增大或n减小时，都会使得误判率减小

### 4.2 证明②（何时误判率最低？）

现在计算对于给定的m和n，k为何值时可以使得误判率最低。设误判率为k的函数为：

![img](https://pic2.zhimg.com/80/v2-0059ace8559a63813b887480094472b9_720w.webp)

下面求最值，即是误差趋近于0

![img](https://pic3.zhimg.com/80/v2-d190387f891fa67ed8e3a5ea71ab712e_720w.webp)

- 因此，即当

![img](https://pic1.zhimg.com/80/v2-195d7838a78207b9b354eab77289a6dc_720w.webp)

时误判率最低，此时误判率为：

![img](https://pic2.zhimg.com/80/v2-f6fb2ec070e6077474c5559acb963cc1_720w.webp)

可以看出若要使得误判率≤1/2，则：

![img](https://pic2.zhimg.com/80/v2-c3256ebcc421b286ef031d10aa66515d_720w.webp)

这说明了若想保持某固定误判率不变，布隆过滤器的bit数m与被增加的元素数n应该是线性同步增加的

## 5.Hash函数的选择

**常见的应用比较广的hash函数有MD5， SHA1， SHA256，**一般用于信息安全方面，比如签名认证和加密等。比如我们传输文件时习惯用对原文件内容计算它的MD5值，生成128 bit的整数，通 常我们说的32位MD5值，是转换为HEX格式后的32个字符

**MurmurHash：**

- MurmurHash是2008年发明的，相比较MD5， MurMurhash不太安全（当然MD5也被破译了， sha也可以被破译），但是性能是MD5的几十倍
- MurmurHash有很多个版本， MurmurHash3修复了MurmurHash2的一些缺陷同时速度还要快一些，因此很多开源项目有用，比如nginx、 redis、 memcashed、 Hadoop等，比如用于计算一致性hash等
- MurmurHash被比较好的测试过了，测试方法见[https://github.com/aappleby/smhasher](https://link.zhihu.com/?target=https%3A//github.com/aappleby/smhasher)
- MurMurhash的实现也可以参考smhasher，或者参考[https://sites.google.com/site/murmurhash](https://link.zhihu.com/?target=https%3A//sites.google.com/site/murmurhash)
- 我们演示的布隆过滤器中的hash函数选择MurmurHash2算法

**补充：双重散列**

双重散列是线性开型寻址散列（开放寻址法）中的冲突解决技术。双重散列使用在发生冲突时将第二个散列函数应用于键的想法

此算法使用下面的公式来进行双哈希处理。hash1() 和 hash2() 是哈希函数，而 TABLE_SIZE 是哈希表的大小。 当发生碰撞时，我们通过重复增加步长i 来寻找键

![img](https://pic1.zhimg.com/80/v2-0f6e2dad0677d50d3ce33721c14e0638_720w.webp)

## 6.布隆过滤器的实现

**布隆过滤器在实现时一般设计考虑下面几样东西：**

- n：布隆过滤器最大处理的元素的个数
- P：希望的误差率
- m：布隆过滤器的bit位数目
- k：哈希函数的个数

应用时首先要先由用户决定要增加的最多元素个数n和希望的误差率P。这也是一个设计完整的布隆过滤器需要用户输入的仅有的两个参数（加入hash种子则为3个），之后的所有参数将由系统计算，并由此建立布隆过滤器

**①首先根据传入的n和p计算需要的内存大小m bits:**

![img](https://pic4.zhimg.com/80/v2-42c635e06efdf792fc95f5c35b7cc8df_720w.webp)

**②再由m，n得到hash function的个数：**

![img](https://pic2.zhimg.com/80/v2-df9999dd891b87d003e296de6882b1e5_720w.webp)

至此系统所需的参数已经备齐，后面就可以添加n个元素到布隆过滤器中，进行查询

### 6.1 布隆过滤器空间利用率问题

根据公式，当k最优时：

![img](https://pic4.zhimg.com/80/v2-15022fd076028771b9dfb4160b6fc917_720w.webp)

因此可验证当P=1%时，存储每个元素需要9.6 bits：

![img](https://pic4.zhimg.com/80/v2-1759583c141698a7de75f9f83cae257f_720w.webp)

而每当想将误判率降低为原来的1/10，则存储每个元素需要增加4.8 bits：

![img](https://pic3.zhimg.com/80/v2-081d15e3fd6d01b6d19806040915eb7a_720w.webp)

### 6.2布隆过滤器误判率对比表

如果方便知道需要使用多少位才能降低错误概率，可以从下表所示的存储项目和位数 比率估计布隆过滤器的误判率

![img](https://pic1.zhimg.com/80/v2-9037055a3dd167e47ae0d48a59d523c8_720w.webp)

为每个URL分配两个字节就可以达到千分之几的冲突。比较保守的实现是，为每个URL 分配4个字节，项目和位数比是1∶32，误判率是0.00000021167340。对于5000万数量级的URL，布隆过滤器只占用200MB的空间

## 7.在线验证公式

**测试网址：**[https://hur.st/bloomfilter/](https://link.zhihu.com/?target=https%3A//hur.st/bloomfilter/)

**下面是一个测试网址，可以根据你输入的数值返回对应的数据：**

- n：布隆过滤器最大处理的元素的个数
- P：希望的误差率
- m：布隆过滤器的bit位数目
- k：哈希函数的个数

![img](https://pic3.zhimg.com/80/v2-ac94c9562fed3ee4845265bcedb3a6e6_720w.webp)

## 8.编码实现

### 8.1 bloomfilter.h

这个代码是布隆过滤器的实现代码

```text
#ifndef __MICRO_BLOOMFILTER_H__
#define __MICRO_BLOOMFILTER_H__
 
/**
 *
 *  仿照Cassandra中的BloomFilter实现，Hash选用MurmurHash2，通过双重散列公式生成散列函数，参考：http://hur.st/bloomfilter
 *    Hash(key, i) = (H1(key) + i * H2(key)) % m
 *
**/
 
#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <string.h>
#include <math.h>
 
#define __BLOOMFILTER_VERSION__ "1.1"
#define __MGAIC_CODE__          (0x01464C42)
 
/**
 *  BloomFilter使用例子：
 *  static BaseBloomFilter stBloomFilter = {0};
 *
 *  初始化BloomFilter(最大100000元素，不超过0.00001的错误率)：
 *      InitBloomFilter(&stBloomFilter, 0, 100000, 0.00001);
 *  重置BloomFilter：
 *      ResetBloomFilter(&stBloomFilter);
 *  释放BloomFilter:
 *      FreeBloomFilter(&stBloomFilter);
 *
 *  向BloomFilter中新增一个数值（0-正常，1-加入数值过多）：
 *      uint32_t dwValue;
 *      iRet = BloomFilter_Add(&stBloomFilter, &dwValue, sizeof(uint32_t));
 *  检查数值是否在BloomFilter内（0-存在，1-不存在）：
 *      iRet = BloomFilter_Check(&stBloomFilter, &dwValue, sizeof(uint32_t));
 *
 *  (1.1新增) 将生成好的BloomFilter写入文件:
 *      iRet = SaveBloomFilterToFile(&stBloomFilter, "dump.bin")
 *  (1.1新增) 从文件读取生成好的BloomFilter:
 *      iRet = LoadBloomFilterFromFile(&stBloomFilter, "dump.bin")
**/
 
// 注意，要让Add/Check函数内联，必须使用 -O2 或以上的优化等级
#define FORCE_INLINE __attribute__((always_inline))
 
#define BYTE_BITS           (8)
#define MIX_UINT64(v)       ((uint32_t)((v>>32)^(v)))
 
#define SETBIT(filter, n)   (filter->pstFilter[n/BYTE_BITS] |= (1 << (n%BYTE_BITS)))
#define GETBIT(filter, n)   (filter->pstFilter[n/BYTE_BITS] & (1 << (n%BYTE_BITS)))
 
#pragma pack(1)
 
// BloomFilter结构定义
typedef struct
{
    uint8_t cInitFlag;                              // 初始化标志，为0时的第一次Add()会对stFilter[]做初始化
    uint8_t cResv[3];
 
    uint32_t dwMaxItems;                            // n - BloomFilter中最大元素个数 (输入量)
    double dProbFalse;                              // p - 假阳概率(误判率) (输入量，比如万分之一：0.00001)
    uint32_t dwFilterBits;                          // m =  ; - BloomFilter的比特数
    uint32_t dwHashFuncs;                           // k = round(log(2.0) * m / n); - 哈希函数个数
 
    uint32_t dwSeed;                                // MurmurHash的种子偏移量
    uint32_t dwCount;                               // Add()的计数，超过MAX_BLOOMFILTER_N则返回失败
 
    uint32_t dwFilterSize;                          // dwFilterBits / BYTE_BITS
    unsigned char *pstFilter;                       // BloomFilter存储指针，使用malloc分配
    uint32_t *pdwHashPos;                           // 存储上次hash得到的K个bit位置数组(由bloom_hash填充)
} BaseBloomFilter;
 
// BloomFilter文件头部定义
typedef struct
{
    uint32_t dwMagicCode;                           // 文件头部标识，填充 __MGAIC_CODE__
    uint32_t dwSeed;
    uint32_t dwCount;
 
    uint32_t dwMaxItems;                            // n - BloomFilter中最大元素个数 (输入量)
    double dProbFalse;                              // p - 假阳概率 (输入量，比如万分之一：0.00001)
    uint32_t dwFilterBits;                          // m = ceil((n * log(p)) / log(1.0 / (pow(2.0, log(2.0))))); - BloomFilter的比特数
    uint32_t dwHashFuncs;                           // k = round(log(2.0) * m / n); - 哈希函数个数
 
    uint32_t dwResv[6];
    uint32_t dwFileCrc;                             // (未使用)整个文件的校验和
    uint32_t dwFilterSize;                          // 后面Filter的Buffer长度
} BloomFileHead;
 
#pragma pack()
 
 
// 计算BloomFilter的参数m,k
static inline void _CalcBloomFilterParam(uint32_t n, double p, uint32_t *pm, uint32_t *pk)
{
    /**
     *  n - Number of items in the filter
     *  p - Probability of false positives, float between 0 and 1 or a number indicating 1-in-p
     *  m - Number of bits in the filter
     *  k - Number of hash functions
     *
     *  f = ln(2) × ln(1/2) × m / n = (0.6185) ^ (m/n)
     *  m = -1 * ln(p) × n / 0.6185 , 这里有错误
     *  k = ln(2) × m / n = 0.6931 * m / n
     * darren修正：
     * m = -1*n*ln(p)/((ln(2))^2) = -1*n*ln(p)/(ln(2)*ln(2)) = -1*n*ln(p)/(0.69314718055995*0.69314718055995))
     *   = -1*n*ln(p)/0.4804530139182079271955440025
     * k = ln(2)*m/n
    **/
 
    uint32_t m, k, m2;
 
    //    printf("ln(2):%lf, ln(p):%lf\n", log(2), log(p)); // 用来验证函数正确性
 
    // 计算指定假阳(误差)概率下需要的比特数
    m =(uint32_t) ceil(-1.0 * n * log(p) / 0.480453); //darren 修正
	//m2 =(uint32_t) ceil(-1 * n * log(p) / 0.480453); //错误写法
    
	m = (m - m % 64) + 64;                              // 8字节对齐
 
    // 计算哈希函数个数
    double double_k = (0.69314 * m / n); // ln(2)*m/n // 这里只是为了debug出来看看具体的浮点数值
    k = round(double_k);    // 返回x的四舍五入整数值。
    printf("orig_k:%lf, k:%u\n", double_k, k);
 
    *pm = m;
    *pk = k;
    return;
}
 
 
// 根据目标精度和数据个数，初始化BloomFilter结构
/**
 * @brief 初始化布隆过滤器
 * @param pstBloomfilter 布隆过滤器实例
 * @param dwSeed    hash种子
 * @param dwMaxItems 存储容量
 * @param dProbFalse 允许的误判率
 * @return 返回值
 *      -1 传入的布隆过滤器为空
 *      -2 hash种子错误或误差>=1
 */
inline int InitBloomFilter(BaseBloomFilter *pstBloomfilter,
                           uint32_t dwSeed,
                           uint32_t dwMaxItems,  double dProbFalse)
{
    if (pstBloomfilter == NULL)
        return -1;
    if ((dProbFalse <= 0) || (dProbFalse >= 1))
        return -2;
 
    // 先检查是否重复Init，释放内存
    if (pstBloomfilter->pstFilter != NULL)
        free(pstBloomfilter->pstFilter);
    if (pstBloomfilter->pdwHashPos != NULL)
        free(pstBloomfilter->pdwHashPos);
 
    memset(pstBloomfilter, 0, sizeof(BaseBloomFilter));
 
    // 初始化内存结构，并计算BloomFilter需要的空间
    pstBloomfilter->dwMaxItems = dwMaxItems;    // 最大存储
    pstBloomfilter->dProbFalse = dProbFalse;    // 误差
    pstBloomfilter->dwSeed = dwSeed;            // hash种子
 
    // 计算 m, k
    _CalcBloomFilterParam(pstBloomfilter->dwMaxItems, pstBloomfilter->dProbFalse,
                          &pstBloomfilter->dwFilterBits, &pstBloomfilter->dwHashFuncs);
 
    // 分配BloomFilter的存储空间
    pstBloomfilter->dwFilterSize = pstBloomfilter->dwFilterBits / BYTE_BITS;
    pstBloomfilter->pstFilter = (unsigned char *) malloc(pstBloomfilter->dwFilterSize);
    if (NULL == pstBloomfilter->pstFilter)
        return -100;
 
    // 哈希结果数组，每个哈希函数一个
    pstBloomfilter->pdwHashPos = (uint32_t*) malloc(pstBloomfilter->dwHashFuncs * sizeof(uint32_t));
    if (NULL == pstBloomfilter->pdwHashPos)
        return -200;
 
    printf(">>> Init BloomFilter(n=%u, p=%e, m=%u, k=%d), malloc() size=%.6fMB, items:bits=1:%0.1lf\n",
           pstBloomfilter->dwMaxItems, pstBloomfilter->dProbFalse, pstBloomfilter->dwFilterBits,
           pstBloomfilter->dwHashFuncs, (double)pstBloomfilter->dwFilterSize/1024/1024,
           pstBloomfilter->dwFilterBits*1.0/pstBloomfilter->dwMaxItems);
 
    // 初始化BloomFilter的内存
    memset(pstBloomfilter->pstFilter, 0, pstBloomfilter->dwFilterSize);
    pstBloomfilter->cInitFlag = 1;
    return 0;
}
 
// 释放BloomFilter
inline int FreeBloomFilter(BaseBloomFilter *pstBloomfilter)
{
    if (pstBloomfilter == NULL)
        return -1;
 
    pstBloomfilter->cInitFlag = 0;
    pstBloomfilter->dwCount = 0;
 
    free(pstBloomfilter->pstFilter);
    pstBloomfilter->pstFilter = NULL;
    free(pstBloomfilter->pdwHashPos);
    pstBloomfilter->pdwHashPos = NULL;
    return 0;
}
 
// 重置BloomFilter
// 注意: Reset()函数不会立即初始化stFilter，而是当一次Add()时去memset
inline int ResetBloomFilter(BaseBloomFilter *pstBloomfilter)
{
    if (pstBloomfilter == NULL)
        return -1;
 
    pstBloomfilter->cInitFlag = 0;
    pstBloomfilter->dwCount = 0;
    return 0;
}
 
// 和ResetBloomFilter不同，调用后立即memset内存
inline int RealResetBloomFilter(BaseBloomFilter *pstBloomfilter)
{
    if (pstBloomfilter == NULL)
        return -1;
 
    memset(pstBloomfilter->pstFilter, 0, pstBloomfilter->dwFilterSize);
    pstBloomfilter->cInitFlag = 1;
    pstBloomfilter->dwCount = 0;
    return 0;
}
 
///
///  函数FORCE_INLINE，加速执行
///
// MurmurHash2, 64-bit versions, by Austin Appleby
// https://sites.google.com/site/murmurhash/
FORCE_INLINE uint64_t MurmurHash2_x64 ( const void * key, int len, uint32_t seed )
{
    const uint64_t m = 0xc6a4a7935bd1e995;
    const int r = 47;
 
    uint64_t h = seed ^ (len * m);
 
    const uint64_t * data = (const uint64_t *)key;
    const uint64_t * end = data + (len/8);
 
    while(data != end)
    {
        uint64_t k = *data++;
 
        k *= m;
        k ^= k >> r;
        k *= m;
 
        h ^= k;
        h *= m;
    }
 
    const uint8_t * data2 = (const uint8_t*)data;
 
    switch(len & 7)
    {
    case 7: h ^= ((uint64_t)data2[6]) << 48;
    case 6: h ^= ((uint64_t)data2[5]) << 40;
    case 5: h ^= ((uint64_t)data2[4]) << 32;
    case 4: h ^= ((uint64_t)data2[3]) << 24;
    case 3: h ^= ((uint64_t)data2[2]) << 16;
    case 2: h ^= ((uint64_t)data2[1]) << 8;
    case 1: h ^= ((uint64_t)data2[0]);
        h *= m;
    };
 
    h ^= h >> r;
    h *= m;
    h ^= h >> r;
 
    return h;
}
 
// 双重散列封装，k个函数函数, 比如要20个
FORCE_INLINE void bloom_hash(BaseBloomFilter *pstBloomfilter, const void * key, int len)
{
    //if (pstBloomfilter == NULL) return;
    int i;
    uint32_t dwFilterBits = pstBloomfilter->dwFilterBits;
    uint64_t hash1 = MurmurHash2_x64(key, len, pstBloomfilter->dwSeed);
    uint64_t hash2 = MurmurHash2_x64(key, len, MIX_UINT64(hash1));
 
    for (i = 0; i < (int)pstBloomfilter->dwHashFuncs; i++)
    {
        // k0 = (hash1 + 0*hash2) % dwFilterBits; // dwFilterBits bit向量的长度
        // k1 = (hash1 + 1*hash2) % dwFilterBits;
        pstBloomfilter->pdwHashPos[i] = (hash1 + i*hash2) % dwFilterBits;
    }
 
    return;
}
 
// 向BloomFilter中新增一个元素
// 成功返回0，当添加数据超过限制值时返回1提示用户
FORCE_INLINE int BloomFilter_Add(BaseBloomFilter *pstBloomfilter, const void * key, int len)
{
    if ((pstBloomfilter == NULL) || (key == NULL) || (len <= 0))
        return -1;
 
    int i;
 
    if (pstBloomfilter->cInitFlag != 1)
    {
        // Reset后没有初始化，使用前需要memset
        memset(pstBloomfilter->pstFilter, 0, pstBloomfilter->dwFilterSize);
        pstBloomfilter->cInitFlag = 1;
    }
 
    // hash key到bloomfilter中, 为了计算不同hash命中的位置，保存pdwHashPos数组
    bloom_hash(pstBloomfilter, key, len);
    for (i = 0; i < (int)pstBloomfilter->dwHashFuncs; i++)
    {
        // dwHashFuncs[0] = hash0(key)
        // dwHashFuncs[1] = hash1(key)
        // dwHashFuncs[k-1] = hashk-1(key)
        SETBIT(pstBloomfilter, pstBloomfilter->pdwHashPos[i]);
    }
 
    // 增加count数
    pstBloomfilter->dwCount++;
    if (pstBloomfilter->dwCount <= pstBloomfilter->dwMaxItems)
        return 0;
    else
        return 1;       // 超过N最大值，可能出现准确率下降等情况
}
 
// 检查一个元素是否在bloomfilter中
// 返回：0-存在，1-不存在，负数表示失败
FORCE_INLINE int BloomFilter_Check(BaseBloomFilter *pstBloomfilter, const void * key, int len)
{
    if ((pstBloomfilter == NULL) || (key == NULL) || (len <= 0))
        return -1;
 
    int i;
 
    bloom_hash(pstBloomfilter, key, len);
    for (i = 0; i < (int)pstBloomfilter->dwHashFuncs; i++)
    {
        // 如果有任意bit不为1，说明key不在bloomfilter中
        // 注意: GETBIT()返回不是0|1，高位可能出现128之类的情况
        if (GETBIT(pstBloomfilter, pstBloomfilter->pdwHashPos[i]) == 0)
            return 1;
    }
 
    return 0;
}
 
 
/* 文件相关封装 */
// 将生成好的BloomFilter写入文件
inline int SaveBloomFilterToFile(BaseBloomFilter *pstBloomfilter, char *szFileName)
{
    if ((pstBloomfilter == NULL) || (szFileName == NULL))
        return -1;
 
    int iRet;
    FILE *pFile;
    static BloomFileHead stFileHeader = {0};
 
    pFile = fopen(szFileName, "wb");
    if (pFile == NULL)
    {
        perror("fopen");
        return -11;
    }
 
    // 先写入文件头
    stFileHeader.dwMagicCode = __MGAIC_CODE__;
    stFileHeader.dwSeed = pstBloomfilter->dwSeed;
    stFileHeader.dwCount = pstBloomfilter->dwCount;
    stFileHeader.dwMaxItems = pstBloomfilter->dwMaxItems;
    stFileHeader.dProbFalse = pstBloomfilter->dProbFalse;
    stFileHeader.dwFilterBits = pstBloomfilter->dwFilterBits;
    stFileHeader.dwHashFuncs = pstBloomfilter->dwHashFuncs;
    stFileHeader.dwFilterSize = pstBloomfilter->dwFilterSize;
 
    iRet = fwrite((const void*)&stFileHeader, sizeof(stFileHeader), 1, pFile);
    if (iRet != 1)
    {
        perror("fwrite(head)");
        return -21;
    }
 
    // 接着写入BloomFilter的内容
    iRet = fwrite(pstBloomfilter->pstFilter, 1, pstBloomfilter->dwFilterSize, pFile);
    if ((uint32_t)iRet != pstBloomfilter->dwFilterSize)
    {
        perror("fwrite(data)");
        return -31;
    }
 
    fclose(pFile);
    return 0;
}
 
// 从文件读取生成好的BloomFilter
inline int LoadBloomFilterFromFile(BaseBloomFilter *pstBloomfilter, char *szFileName)
{
    if ((pstBloomfilter == NULL) || (szFileName == NULL))
        return -1;
 
    int iRet;
    FILE *pFile;
    static BloomFileHead stFileHeader = {0};
 
    if (pstBloomfilter->pstFilter != NULL)
        free(pstBloomfilter->pstFilter);
    if (pstBloomfilter->pdwHashPos != NULL)
        free(pstBloomfilter->pdwHashPos);
 
    //
    pFile = fopen(szFileName, "rb");
    if (pFile == NULL)
    {
        perror("fopen");
        return -11;
    }
 
    // 读取并检查文件头
    iRet = fread((void*)&stFileHeader, sizeof(stFileHeader), 1, pFile);
    if (iRet != 1)
    {
        perror("fread(head)");
        return -21;
    }
 
    if ((stFileHeader.dwMagicCode != __MGAIC_CODE__)
            || (stFileHeader.dwFilterBits != stFileHeader.dwFilterSize*BYTE_BITS))
        return -50;
 
    // 初始化传入的 BaseBloomFilter 结构
    pstBloomfilter->dwMaxItems = stFileHeader.dwMaxItems;
    pstBloomfilter->dProbFalse = stFileHeader.dProbFalse;
    pstBloomfilter->dwFilterBits = stFileHeader.dwFilterBits;
    pstBloomfilter->dwHashFuncs = stFileHeader.dwHashFuncs;
    pstBloomfilter->dwSeed = stFileHeader.dwSeed;
    pstBloomfilter->dwCount = stFileHeader.dwCount;
    pstBloomfilter->dwFilterSize = stFileHeader.dwFilterSize;
 
    pstBloomfilter->pstFilter = (unsigned char *) malloc(pstBloomfilter->dwFilterSize);
    if (NULL == pstBloomfilter->pstFilter)
        return -100;
    pstBloomfilter->pdwHashPos = (uint32_t*) malloc(pstBloomfilter->dwHashFuncs * sizeof(uint32_t));
    if (NULL == pstBloomfilter->pdwHashPos)
        return -200;
 
 
    // 将后面的Data部分读入 pstFilter
    iRet = fread((void*)(pstBloomfilter->pstFilter), 1, pstBloomfilter->dwFilterSize, pFile);
    if ((uint32_t)iRet != pstBloomfilter->dwFilterSize)
    {
        perror("fread(data)");
        return -31;
    }
    pstBloomfilter->cInitFlag = 1;
 
    printf(">>> Load BloomFilter(n=%u, p=%f, m=%u, k=%d), malloc() size=%.2fMB\n",
           pstBloomfilter->dwMaxItems, pstBloomfilter->dProbFalse, pstBloomfilter->dwFilterBits,
           pstBloomfilter->dwHashFuncs, (double)pstBloomfilter->dwFilterSize/1024/1024);
 
    fclose(pFile);
    return 0;
}
 
#endif
```

### 8.2 bloomfilter.cpp

这个是布隆过滤器的测试代码

```text
#include "bloomfilter.h"
#include <stdio.h>
 
#define MAX_ITEMS 6000000      // 设置最大元素个数
#define ADD_ITEMS 1000      // 添加测试元素
#define P_ERROR 0.0001// 设置误差
 
//
int main(int argc, char** argv)
{
 
    printf(" test bloomfilter\n");
 
    // 1. 定义BaseBloomFilter
    static BaseBloomFilter stBloomFilter = {0};
 
    // 2. 初始化stBloomFilter，调用时传入hash种子，存储容量，以及允许的误判率
    InitBloomFilter(&stBloomFilter, 0, MAX_ITEMS, P_ERROR);
 
    // 3. 向BloomFilter中新增数值
    char url[128] = {0};
    for(int i = 0; i < ADD_ITEMS; i++){
        sprintf(url, "https://blog.csdn.net/qq_41453285/%d.html", i);
        if(0 == BloomFilter_Add(&stBloomFilter, (const void*)url, strlen(url))){
            // printf("add %s success", url);
        }else{
            printf("add %s failed", url);
        }
        memset(url, 0, sizeof(url));
    }
 
    // 4. check url exist or not
    char* str = "https://blog.csdn.net/qq_41453285/0.html";
    if (0 == BloomFilter_Check(&stBloomFilter, (const void*)str, strlen(str)) ){
        printf("https://blog.csdn.net/qq_41453285/0.html exist\n");
    }
 
    char* str2 = "https://blog.csdn.net/qq_41453285/10001.html";
    if (0 != BloomFilter_Check(&stBloomFilter, (const void*)str2, strlen(str2)) ){
          printf("https://blog.csdn.net/qq_41453285/10001.html not exist\n");
    }
 
    // 5. free bloomfilter
    FreeBloomFilter(&stBloomFilter);
    getchar();
    return 0;
}
```

**结果图下图所示：**

- n：布隆过滤器最大处理的元素的个数
- P：希望的误差率
- m：布隆过滤器的bit位数目
- k：哈希函数的个数

![img](https://pic3.zhimg.com/80/v2-46bc8f881346482402baf60c261d2e26_720w.webp)

![img](https://pic3.zhimg.com/80/v2-bea13254fcb5d65149c2ff44d60f7d4e_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/557308262

作者：linux

# 【NO.65】底层原理：epoll源码分析，还搞不懂epoll的看过来

## 1.前言

Linux内核提供了3个关键函数供用户来操作epoll，分别是：

- epoll_create(), 创建eventpoll对象
- epoll_ctl(), 操作eventpoll对象
- epoll_wait(), 从eventpoll对象中返回活跃的事件

而操作系统内部会用到一个名叫epoll_event_callback()的回调函数来调度epoll对象中的事件，这个函数非常重要，故本文将会对上述4个函数进行源码分析。

## 2.源码来源

由于epoll的实现内嵌在内核中，直接查看内核源码的话会有一些无关代码影响阅读。为此在GitHub上写的简化版TCP/IP协议栈，里面实现了epoll逻辑。链接为：[https://github.com/wangbojing/NtyTcp](https://link.zhihu.com/?target=https%3A//github.com/wangbojing/NtyTcp)

存放着以上4个关键函数的文件是[src\nty_epoll_rb.c]，本文接下来通过分析该程序的代码来探索epoll能支持高并发连接的秘密。

## 3.两个核心数据结构

### 3.1 epitem

![img](https://pic3.zhimg.com/80/v2-81634d6162b04a52392571e608a62252_720w.webp)

如图所示，epitem是中包含了两个主要的成员变量，分别是rbn和rdlink，前者是红黑树的节点，而后者是双链表的节点，也就是说一个epitem对象即可作为红黑树中的一个节点又可作为双链表中的一个节点。并且每个epitem中存放着一个event，对event的查询也就转换成了对epitem的查询。

```text
struct epitem {
	RB_ENTRY(epitem) rbn;
	/*  RB_ENTRY(epitem) rbn等价于
	struct {											
		struct type *rbe_left;		//指向左子树
		struct type *rbe_right;		//指向右子树
		struct type *rbe_parent;	//指向父节点
		int rbe_color;			    //该节点的颜色
	} rbn
	*/
 
	LIST_ENTRY(epitem) rdlink;
	/* LIST_ENTRY(epitem) rdlink等价于
	struct {									
		struct type *le_next;	//指向下个元素
		struct type **le_prev;	//前一个元素的地址
	}*/
 
	int rdy; //判断该节点是否同时存在与红黑树和双向链表中
	
	int sockfd; //socket句柄
	struct epoll_event event;  //存放用户填充的事件
};
```

### 3.2 eventpoll

![img](https://pic2.zhimg.com/80/v2-2844a72b6efbb90ab0de45e122c12365_720w.webp)

如图所示，eventpoll中包含了两个主要的成员变量，分别是rbr和rdlist，前者指向红黑树的根节点，后者指向双链表的头结点。即一个eventpoll对象对应二个epitem的容器。对epitem的检索，将发生在这两个容器上（红黑树和双链表）。

```text
struct eventpoll {
	/*
	struct ep_rb_tree {
		struct epitem *rbh_root; 			
	}
	*/
	ep_rb_tree rbr;      //rbr指向红黑树的根节点
	
	int rbcnt; //红黑树中节点的数量（也就是添加了多少个TCP连接事件）
	
	LIST_HEAD( ,epitem) rdlist;    //rdlist指向双向链表的头节点；
	/*	这个LIST_HEAD等价于 
		struct {
			struct epitem *lh_first;
		}rdlist;
	*/
	
	int rdnum; //双向链表中节点的数量（也就是有多少个TCP连接来事件了）
 
	// ...略...
	
};
```

## 4.四个关键函数

### 4.1 epoll_create()

```text
//创建epoll对象，包含一颗空红黑树和一个空双向链表
int epoll_create(int size) {
	//与很多内核版本一样，size参数没有作用，只要保证大于0即可
	if (size <= 0) return -1;
	
	nty_tcp_manager *tcp = nty_get_tcp_manager(); //获取tcp对象
	if (!tcp) return -1;
	
	struct _nty_socket *epsocket = nty_socket_allocate(NTY_TCP_SOCK_EPOLL);
	if (epsocket == NULL) {
		nty_trace_epoll("malloc failed\n");
		return -1;
	}
 
	// 1° 开辟了一块内存用于填充eventpoll对象
	struct eventpoll *ep = (struct eventpoll*)calloc(1, sizeof(struct eventpoll));
	if (!ep) {
		nty_free_socket(epsocket->id, 0);
		return -1;
	}
 
	ep->rbcnt = 0;
 
	// 2° 让红黑树根指向空
	RB_INIT(&ep->rbr);       //等价于ep->rbr.rbh_root = NULL;
 
	// 3° 让双向链表的头指向空
	LIST_INIT(&ep->rdlist);  //等价于ep->rdlist.lh_first = NULL;
 
	// 4° 并发环境下进行互斥
	// ...该部分代码与主线逻辑无关，可自行查看...
 
	//5° 保存epoll对象
	tcp->ep = (void*)ep;
	epsocket->ep = (void*)ep;
 
	return epsocket->id;
}
```

对以上代码的逻辑进行梳理，可以总结为以下6步：

1. 创建eventpoll对象
2. 让eventpoll中的rbr指向空
3. 让eventpoll中的rdlist指向空
4. 在并发环境下进行互斥
5. 保存eventpoll对象
6. 返回eventpoll对象的句柄(id)



### 4.2 epoll_ctl()

该函数的逻辑其实很简单，无非就是将用户传入的参数封装为一个epitem对象，然后根据传入的op是①EPOLL_CTL_ADD、②EPOLL_CTL_MOD还是③EPOLL_CTL_DEL，来决定是①将epitem对象插入红黑树中，②更新红黑树中的epitem对象，还是③移除红黑树中的epitem对象。

```text
//往红黑树中加每个tcp连接以及相关的事件
int epoll_ctl(int epid, int op, int sockid, struct epoll_event *event) {
 
	nty_tcp_manager *tcp = nty_get_tcp_manager();
	if (!tcp) return -1;
 
	nty_trace_epoll(" epoll_ctl --> 1111111:%d, sockid:%d\n", epid, sockid);
	struct _nty_socket *epsocket = tcp->fdtable->sockfds[epid];
 
	if (epsocket->socktype == NTY_TCP_SOCK_UNUSED) {
		errno = -EBADF;
		return -1;
	}
 
	if (epsocket->socktype != NTY_TCP_SOCK_EPOLL) {
		errno = -EINVAL;
		return -1;
	}
 
	nty_trace_epoll(" epoll_ctl --> eventpoll\n");
 
	struct eventpoll *ep = (struct eventpoll*)epsocket->ep;
	if (!ep || (!event && op != EPOLL_CTL_DEL)) {
		errno = -EINVAL;
		return -1;
	}
 
	if (op == EPOLL_CTL_ADD) {
		//添加sockfd上关联的事件
		pthread_mutex_lock(&ep->mtx);
 
		struct epitem tmp;
		tmp.sockfd = sockid;
		struct epitem *epi = RB_FIND(_epoll_rb_socket, &ep->rbr, &tmp); //先在红黑树上找，根据key来找，也就是这个sockid，找的速度会非常快
		if (epi) {
			//原来有这个节点，不能再次插入
			nty_trace_epoll("rbtree is exist\n");
			pthread_mutex_unlock(&ep->mtx);
			return -1;
		}
 
		//只有红黑树上没有该节点【没有用过EPOLL_CTL_ADD的tcp连接才能走到这里】；
 
		//(1)生成了一个epitem对象，这个结构对象，其实就是红黑的一个节点；
		epi = (struct epitem*)calloc(1, sizeof(struct epitem));
		if (!epi) {
			pthread_mutex_unlock(&ep->mtx);
			errno = -ENOMEM;
			return -1;
		}
		
		//(2)把socket(TCP连接)保存到节点中；
		epi->sockfd = sockid;  //作为红黑树节点的key，保存在红黑树中
 
		//(3)我们要增加的事件也保存到节点中；
		memcpy(&epi->event, event, sizeof(struct epoll_event));
 
		//(4)把这个节点插入到红黑树中去
		epi = RB_INSERT(_epoll_rb_socket, &ep->rbr, epi); //实际上这个时候epi的rbn成员就会发挥作用，如果这个红黑树中有多个节点，那么RB_INSERT就会epi->rbi相应的值：可以参考图来理解
		assert(epi == NULL);
		ep->rbcnt ++;
		
		pthread_mutex_unlock(&ep->mtx);
 
	} else if (op == EPOLL_CTL_DEL) {
		pthread_mutex_lock(&ep->mtx);
 
		struct epitem tmp;
		tmp.sockfd = sockid;
		
		struct epitem *epi = RB_FIND(_epoll_rb_socket, &ep->rbr, &tmp);//先在红黑树上找，根据key来找，也就是这个sockid，找的速度会非常快
		if (!epi) {
			nty_trace_epoll("rbtree no exist\n");
			pthread_mutex_unlock(&ep->mtx);
			return -1;
		}
		
		//只有在红黑树上找到该节点【用过EPOLL_CTL_ADD的tcp连接才能走到这里】；
 
		//从红黑树上把这个节点移除
		epi = RB_REMOVE(_epoll_rb_socket, &ep->rbr, epi);
		if (!epi) {
			nty_trace_epoll("rbtree is no exist\n");
			pthread_mutex_unlock(&ep->mtx);
			return -1;
		}
 
		ep->rbcnt --;
		free(epi);
		
		pthread_mutex_unlock(&ep->mtx);
 
	} else if (op == EPOLL_CTL_MOD) {
		struct epitem tmp;
		tmp.sockfd = sockid;
		struct epitem *epi = RB_FIND(_epoll_rb_socket, &ep->rbr, &tmp); //先在红黑树上找，根据key来找，也就是这个sockid，找的速度会非常快
		if (epi) {
			//红黑树上有该节点，则修改对应的事件
			epi->event.events = event->events;
			epi->event.events |= EPOLLERR | EPOLLHUP;
		} else {
			errno = -ENOENT;
			return -1;
		}
 
	} else {
		nty_trace_epoll("op is no exist\n");
		assert(0);
	}
 
	return 0;
}
```

### 4.3 epoll_wait()

```text
//到双向链表中去取相关的事件通知
int epoll_wait(int epid, struct epoll_event *events, int maxevents, int timeout) {
 
	nty_tcp_manager *tcp = nty_get_tcp_manager();
	if (!tcp) return -1;
 
	struct _nty_socket *epsocket = tcp->fdtable->sockfds[epid];
 
	struct eventpoll *ep = (struct eventpoll*)epsocket->ep;
	
    // ...此处主要是一些负责验证性工作的代码...
 
	//(1)当eventpoll对象的双向链表为空时，程序会在这个while中等待一定时间，
	//直到有事件被触发，操作系统将epitem插入到双向链表上使得rdnum>0时，程序才会跳出while循环
	while (ep->rdnum == 0 && timeout != 0) {
		// ...此处主要是一些与等待时间相关的代码...
	}
 
 
	pthread_spin_lock(&ep->lock);
 
	int cnt = 0;
 
	//(1)取得事件的数量
	//ep->rdnum：代表双向链表里边的节点数量（也就是有多少个TCP连接来事件了）
	//maxevents：此次调用最多可以收集到maxevents个已经就绪【已经准备好】的读写事件
	int num = (ep->rdnum > maxevents ? maxevents : ep->rdnum); //哪个数量少，就取得少的数字作为要取的事件数量
	int i = 0;
	
	while (num != 0 && !LIST_EMPTY(&ep->rdlist)) { //EPOLLET
 
		//(2)每次都从双向链表头取得 一个一个的节点
		struct epitem *epi = LIST_FIRST(&ep->rdlist);
 
		//(3)把这个节点从双向链表中删除【但这并不影响这个节点依旧在红黑树中】
		LIST_REMOVE(epi, rdlink); 
 
		//(4)这是个标记，标记这个节点【这个节点本身是已经在红黑树中】已经不在双向链表中；
		epi->rdy = 0;  //当这个节点被操作系统 加入到 双向链表中时，这个标记会设置为1。
 
		//(5)把事件标记信息拷贝出来；拷贝到提供的events参数中
		memcpy(&events[i++], &epi->event, sizeof(struct epoll_event));
		
		num --;
		cnt ++;       //拷贝 出来的 双向链表 中节点数目累加
		ep->rdnum --; //双向链表里边的节点数量减1
	}
	
	pthread_spin_unlock(&ep->lock);
 
	//(5)返回 实际 发生事件的 tcp连接的数目；
	return cnt; 
}
```

该函数的逻辑也十分简单，就是让先看一下eventpoll对象的双链表中是否有节点。如果有节点的话则取出节点中的事件填充到用户传入的指针所指向的内存中。如果没有节点的话，则在while循环中等待一定时间，直到有事件被触发后操作系统会将epitem插入到双向链表上使得rdnum>0时(这个过程是由操作系统调用epoll_event_callback函数完成的)，程序才会跳出while循环，去双向链表中取数据。

### 4.4 epoll_event_callback()

通过跟踪epoll_event_callback在内核中被调用的位置。可知，当服务器在以下5种情况会调用epoll_event_callback：

1. 客户端connect()连入，服务器处于SYN_RCVD状态时
2. 三路握手完成，服务器处于ESTABLISHED状态时
3. 客户端close()断开连接，服务器处于FIN_WAIT_1和FIN_WAIT_2状态时
4. 客户端send/write()数据，服务器可读时
5. 服务器可以发送数据时

接下来，我们来看一下epoll_event_callback的源码：

```text
//当发生客户端三路握手连入、可读、可写、客户端断开等情况时，操作系统会调用这个函数，用以往双向链表中增加一个节点【该节点同时 也在红黑树中】
int epoll_event_callback(struct eventpoll *ep, int sockid, uint32_t event) {
	struct epitem tmp;
	tmp.sockfd = sockid;
 
	//(1)根据给定的key【这个TCP连接的socket】从红黑树中找到这个节点
	struct epitem *epi = RB_FIND(_epoll_rb_socket, &ep->rbr, &tmp);
	if (!epi) {
		nty_trace_epoll("rbtree not exist\n");
		assert(0);
	}
 
	//(2)从红黑树中找到这个节点后，判断这个节点是否已经被连入到双向链表里【判断的是rdy标志】
	if (epi->rdy) {
		//这个节点已经在双向链表里，那无非是把新发生的事件标志增加到现有的事件标志中
		epi->event.events |= event;
		return 1;
	} 
 
	//走到这里，表示 双向链表中并没有这个节点，那要做的就是把这个节点连入到双向链表中
 
	nty_trace_epoll("epoll_event_callback --> %d\n", epi->sockfd);
	
	pthread_spin_lock(&ep->lock);
 
	//(3)标记这个节点已经被放入双向链表中，我们刚才研究epoll_wait()的时候，从双向链表中把这个节点取走的时候，这个标志被设置回了0
	epi->rdy = 1;  
 
	//(4)把这个节点链入到双向链表的表头位置
	LIST_INSERT_HEAD(&ep->rdlist, epi, rdlink);
 
	//(5)双向链表中的节点数量加1，刚才研究epoll_wait()的时候，从双向链表中把这个节点取走的时候，这个数量减了1
	ep->rdnum ++;
 
	pthread_spin_unlock(&ep->lock);
	pthread_mutex_lock(&ep->cdmtx);
	pthread_cond_signal(&ep->cond);
	pthread_mutex_unlock(&ep->cdmtx);
 
	return 0;
}
```

以上代码的逻辑也十分简单，就是将eventpoll所指向的红黑树的节点插入到双向链表中。

## 5.总结

epoll底层实现中有两个关键的数据结构，一个是eventpoll另一个是epitem，其中eventpoll中有两个成员变量分别是rbr和rdlist,前者指向一颗红黑树的根，后者指向双向链表的头。而epitem则是红黑树节点和双向链表节点的综合体，也就是说epitem即可作为树的节点，又可以作为链表的节点，并且epitem中包含着用户注册的事件。

- 当用户调用epoll_create()时，会创建eventpoll对象（包含一个红黑树和一个双链表）；
- 而用户调用epoll_ctl(ADD)时，会在红黑树上增加节点（epitem对象）；
- 接下来，操作系统会默默地在通过epoll_event_callback()来管理eventpoll对象。当有事件被触发时，操作系统则会调用epoll_event_callback函数，将含有该事件的epitem添加到双向链表中。
- 当用户需要管理连接时，只需通过epoll_wait()从eventpoll对象中的双链表下"摘取"epitem并取出其包含的事件即可。

原文地址：https://zhuanlan.zhihu.com/p/552580039

作者：linux

# 【NO.66】游戏服务器框架-skynet的设计原理和使用

## 1.多核并发编程方式

（1）多线程。

在一个进程中开启多线程，为了充分利用多核，一般设置工作线程的个数为 cpu 的核心数。memcached 就是采用这种方式。

多线程在一个进程当中，所以数据共享来自进程当中的虚拟内存；这里会涉及到很多临界资源的访问，所以需要考虑加锁。

（2）多进程。

在一台机器当中，开启多个进程充分利用多核，一般设置工作进程的个数为 cpu 的核心数。nginx 就是采用这种方式，nginx 当中的 worker 进程，通过共享内存来进行共享数据；也需要考虑使用锁。

（3）CSP。

以 go 语言为代表，并发实体是协程（用户态线程、轻量级线程）；内部也是采用多少个核心开启多少个线程来充分利用多核。

（4）Actor。

erlang 从语言层面支持 actor 并发模型，并发实体是 actor（在skynet 中称之为服务）；skynet 采用 c + lua来实现 actor 并发模型；底层也是通过采用多少个核心开启多少个内核线程来充分利用多核。

## 2.skynet

### 2.1 skynet简介

它是一个轻量级游戏服务器框架，但也不仅仅用于游戏。

轻量级体现在：

1. 实现了 actor 模型，以及相关的脚手架（工具集）：actor 间数据共享机制以及c 服务扩展机制。
2. 实现了服务器框架的基础组件。实现了 reactor 并发网络库；并提供了大量连接的接入方案；基于自身网络库，实现了常用的数据库驱动（异步连接方案），并融合了 lua 数据结构；实现了网关服务；时间轮用于处理定时消息。

skynet抽象了actor并发模型，用户层抽象进程；sknet通过消息的方式共享内存；通过消息驱动actor运行。

skynet的actor模型使用lua虚拟，lua虚拟机非常小（只有几十kb），代价不高；每个actor对应一个lua虚拟机；系统中不能启动过多的进程就是因为资源受限，lua虚拟机占用的资源很少，可以开启很多个，这就能抽象出很多个用户层的进程。lua虚拟机可以共享一些数据，比如常量，达到资源复用。

抽象进程而不抽象线程的原因在于进程有独立的工作空间，隔离的运行环境。

sknet的所有actor都是对等的，通过公平调度实现。

### 2.2 环境准备

ubuntu：

```text
sudo apt-get install git build-essential readline-dev autoconf
# 或者
sudo apt-get install git build-essential libreadline-dev autoconf
```

centos：

```text
yum install -y git gcc readline-devel autoconf
```

mac：

```text
yum install -y git gcc readline-devel autoconf
```

### 2.3 编译安装

```text
git clone https://github.com/cloudwu/skynet.git
cd skynet
# centos or ubuntu
make linux
# mac
make macosx
```

### 2.4 Actor 模型

有消息的 actor 为活跃的 actor，没有消息为非活跃的 actor。

**定义：**

1. 用于并行计算；
2. Actor 是最基本的计算单元；
3. 基于消息计算；
4. Actor 通过消息进行沟通。

**组成：**

1. 隔离的环境。主要通过 lua 虚拟机来实现。
2. 消息队列。用来存放有序（先后到达）的消息。
3. 回调函数。用来运行 Actor；从 Actor 的消息队列中取出消息，并作为该回调函数的参数来运行 Actor。

### 2.5 消息队列

Actor 模型基于消息计算，在 skynet 框架中，消息包含 Actor（之间）消息、网络消息以及定时消息。

生产者生产消息，消息放入消息队列中，由线程池去消费消息。



### 2.6 actor公平调度

所有的actor都是对等的，每个actor都有自己的消息队列；skynet的并发实体是actor，因此需要公平的调度actor。

skynet会有非常的actor，公平调度就非常重要；采用两级队列。

首先，查找活跃的消息队列（有消息的消息队列）；

然后，通过队列组织所有活跃的消息队列；

最后，调度队列。

![img](https://pic1.zhimg.com/80/v2-a62db5cbff2f41b94a0e207e66c73f4c_720w.webp)

线程池从一级队列取出一个消息，消费消息，消费完消息后如果还有消息，则添加到一级队列的末尾。

因为用户问题造成消息不均匀，在应用上不一定公平；skynet在工作线程赋予权重来解决这个问题。

```text
// 工作线程权重图   32个核心
static int weight[] = {
    -1, -1, -1, -1, 0, 0, 0, 0,
    1, 1, 1, 1, 1, 1, 1, 1,  // 1/2
    2, 2, 2, 2, 2, 2, 2, 2,  // 1/4
    3, 3, 3, 3, 3, 3, 3, 3, }; // 1/8
```

当工作线程的权重为 -1 时，该工作线程每次只 pop 一条消息；当工作线程的权重为 0 时，该工作线程每次消费完所有的消息；当工作线程的权重为 1 时，每次消费消息队列中二分之一的消息；当工作线程的权重为 2 时，每次消费消息队列中四分之一 的消息；以此类推；通过这种方式，完成消息队列梯度消费，从而不至于让某些队列过长。

## 3.skynet的使用

github 上打开 skynet 点击 [wiki](https://link.zhihu.com/?target=https%3A//github.com/cloudwu/skynet/wiki) 里面有所有函数的详细说明。

### 3.1 第一个skynet程序

（1）在sknet源码的父目录下创建一个config文件，用于定制skynet的行为。
内容可参考如下：

```text
thread=4
logger=nil
harbor=0
start="main"
lua_path="./skynet/lualib/?.lua;./skynet/lualib/?/init.lua;"
luaservice="./skynet/service/?.lua;./app/?.lua"
lualoader="./skynet/lualib/loader.lua"
cpath="./skynet/cservice/?.so"
lua_cpath="./skynet/luaclib/?.so"
```

参数说明：

![img](https://pic4.zhimg.com/80/v2-d2f5d3953971737865686f33a04ba51f_720w.webp)

（2）在app文件下面创建main.lua。每次启动时都是从skynet.start(…)中运行，相当于入口函数。

```text
local skynet = require("skynet")

skynet.start(function()
    -- body
    print("hello skynet")
end)
```

（3）编写一个自己项目的makefile。首先编译skynet的源码，如果有自己的c代码直接加入makefile文件。

```text
SKYNET_PATH?=./skynet

all:
	cd $(SKYNET_PATH) && $(MAKE) PLAT='linux'

clean:
	cd $(SKYNET_PATH) && $(MAKE) clean
```

（4）编译，执行自己的makefile。

```text
make
```

（5）执行skynet，指定config。

```text
./skynet/skynet config
```



```text
[:00000002] LAUNCH snlua bootstrap
[:00000003] LAUNCH snlua launcher
[:00000004] LAUNCH snlua cdummy
[:00000005] LAUNCH harbor 0 4
[:00000006] LAUNCH snlua datacenterd
[:00000007] LAUNCH snlua service_mgr
[:00000008] LAUNCH snlua main
hello skynet
[:00000002] KILL self
```

可以看到打印了hello skynet。

### 3.2 skynet网络消息

skynet 当中采用一个 socket 线程来处理网络信息；skynet 基于 reactor 网络模型。

网络消息驱动actor运行。需要skynet.socket模块。

首先绑定一个监听 listenfd，然后调用socket.start(listenfd, function)绑定fd到进程函数function中处理。

示例：

main.lua

```text
local skynet = require "skynet"
local socket=require "skynet.socket"


skynet.start(function()
    -- body
    print("hello skynet")

    local listenfd=socket.listen("0.0.0.0",8888);
    socket.start(listenfd,function(clientfd,addr)
        -- body
        print("receive a client: ",clientfd,addr);
    end)

end)
```



```text
[:00000002] LAUNCH snlua bootstrap
[:00000003] LAUNCH snlua launcher
[:00000004] LAUNCH snlua cdummy
[:00000005] LAUNCH harbor 0 4
[:00000006] LAUNCH snlua datacenterd
[:00000007] LAUNCH snlua service_mgr
[:00000008] LAUNCH snlua main
hello skynet
[:00000002] KILL self
receive a client: 	2	192.168.0.105:50024
```

### 3.3 skynet定时消息

定时任务推送给定时线程，定时线程检测完时间后往消息队列推送一个消息，然后actor开始执行callback函数。

使用示例：
main.lua

```text
local skynet = require "skynet"


skynet.start(function()
    -- body
    print("hello skynet")
    skynet.timeout(100,function()
        -- body
        print("timer 1s");
    end)

end)
```

### 3.4 skynet actor间消息

所有的服务都是通过消息来交换数据。actor间消息通信通过将消息放到对方的消息队列实现。

示例，创建两个lua文件。

main发送“ping”给slave，协程挂起；slave回复“pong”给main，协程唤醒。整个过程使用协程实现异步操作，消除异步中的回调。

main.lua

```text
local skynet = require("skynet")

skynet.start(function()
    -- body
    print("hello skynet")

    local slave=skynet.newservice("slave")
    local response=skynet.call(slave,"lua","ping")
    print("main ",response)
end)
```

slave.lua

```text
local skynet = require "skynet"

local CMD={}

function CMD.ping()
    -- body
    skynet.retpack("pong")
end

skynet.start(function()
    skynet.dispatch("lua",function(session,source,cmd,...)
        local func=assert(CMD[cmd])
        func(...)
    end)
end)
```

执行效果：

```text
[:00000002] LAUNCH snlua bootstrap
[:00000003] LAUNCH snlua launcher
[:00000004] LAUNCH snlua cdummy
[:00000005] LAUNCH harbor 0 4
[:00000006] LAUNCH snlua datacenterd
[:00000007] LAUNCH snlua service_mgr
[:00000008] LAUNCH snlua main
hello skynet
[:00000009] LAUNCH snlua slave
main    pong
[:00000002] KILL self
```

## 4.vscode调试skynet

首先要知道整个系统/框架 是怎么应用的。skynet会自己产生一个skynet执行程序，需要指定配置文件来启动服务。

项目下的.vscode文件夹需要如下三个文件。

launch.json

```text
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "启动 app",
            "type": "cppdbg",
            "request": "launch",
            "program": "${workspaceFolder}/skynet/skynet",
            "args": ["config"],
            "stopAtEntry": false,
            "cwd": "${workspaceFolder}",
            "environment": [],
            "externalConsole": false,
            "MIMode": "gdb",
            "setupCommands": [
                {
                    "description": "为 gdb 启用整齐打印",
                    "text": "-enable-pretty-printing",
                    "ignoreFailures": true
                },
                {
                    "description": "将反汇编风格设置为 Intel",
                    "text": "-gdb-set disassembly-flavor intel",
                    "ignoreFailures": true
                }
            ],
            "preLaunchTask": "build-skynet",
            "miDebuggerPath": "/usr/bin/gdb"
        }
    ]
}
```

settings.json

```text
{
    "C_Cpp.default.configurationProvider": "ms-vscode.makefile-tools",
    "files.associations": {
        "string.h": "c",
        "stdlib.h": "c",
        "pthread.h": "c"
    },
}
```

tasks.json

```text
{
    "tasks": [
        {
            "type": "cppbuild",
            "label": "build-skynet",
            "command": "/usr/bin/make",
            "args": [],
            "options": {
                "cwd": "${workspaceFolder}"
            },
            "problemMatcher": [
                "$gcc"
            ],
            "group": {
                "kind": "build",
                "isDefault": true
            },
            "detail": "..."
        }
    ],
    "version": "2.0.0"
}
```

实现这三个文件就可以开始调试了。

skynet是一个网络服务，需要处理网络连接，需要知道数据是怎么流向，理解框架的运行机制（多线程并发）；所有的actor的运行都是由线程池来驱动，actor的运行又是由消息队列的消息驱动（网络消息、定时消息、actor间消息）。

线程池从消息队列中取出消息，找到回调函数，驱动actor运行。

## 5.总结

不要通过共享内存来通信，而应该通过通信来共享内存。CSP 和 Actor 都符合这一哲学；通过通信来共享数据，其实是一种解耦合的过程。并lua发实体之间可以分别开发并单独优化，而它们唯一的耦合在于消息；这能让我们快速地进行开发；同时也符合我们开发的思路，将一个大的问题拆分成若干个小问题。

actor模型是在用户层抽象了进程。

actor调度：将活跃的 actor 通过全局队列组织起来；actor 当中的消息队列有消息就是活跃的 actor； 线程池去全局队列中取出 actor 的消息队列，接着运行actor。

原文地址：https://zhuanlan.zhihu.com/p/591754195

作者：linux

# 【NO.67】面试题:Linux网络编程中可靠UDP，KCP协议快在哪

## 1.前言

云真机已经支持手机端的画面投影。云真机实时操作，对延迟的要求比远程视频对话的要求更高（100ms以内）。在无线网络下，如何更实时、更可靠的传输视频流就成了一个挑战。**通过websocket、RTMP、UDP的比较，最后选择了可靠的UDP协议KCP来进行实时音视频的传输。**

## 2.KCP简介

KCP是一个快速可靠协议，能以比 TCP浪费10%-20%的带宽的代价，换取平均延迟降低 30%-40%，且最大延迟降低三倍的传输效果。纯算法实现，并不负责底层协议（如UDP）的收发，需要使用者自己定义下层数据包的发送方式，以 callback的方式提供给 KCP。 连时钟都需要外部传递进来，内部不会有任何一次系统调用。本文传输协议之考虑UDP的情况。

名词说明（源码字段）：

- 用户数据：应用层发送的数据，如一张图片2Kb的数据。
- MTU：最大传输单元。即每次发送的最大数据。
- RTO：Retransmission TimeOut，重传超时时间。
- cwnd:congestion window，拥塞窗口，表示发送方可发送多少个KCP数据包。与接收方窗口有关，与网络状况（拥塞控制）有关，与发送窗口大小有关。
- rwnd:receiver window,接收方窗口大小，表示接收方还可接收多少个KCP数据包。
- snd_queue:待发送KCP数据包队列。
- snd_nxt:下一个即将发送的kcp数据包序列号。
- snd_una:下一个待确认的序列号。

## 3.KCP使用方式

### 3.1 创建 KCP对象

```text
// 初始化 kcp对象，conv为一个表示会话编号的整数，和tcp的 conv一样，通信双

// 方需保证 conv相同，相互的数据包才能够被认可，user是一个给回调函数的指针

ikcpcb *kcp = ikcp_create(conv, user);
```

### 3.2 设置传输[回调](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%E5%9B%9E%E8%B0%83%26spm%3D1001.2101.3001.7020)函数（如UDP的send函数）

```text
// KCP的下层协议输出函数，KCP需要发送数据时会调用它

// buf/len 表示缓存和长度

// user指针为 kcp对象创建时传入的值，用于区别多个 KCP对象

int udp_output(const char *buf, int len, ikcpcb *kcp, void *user)

{

  .... 

}

// 设置回调函数

kcp->output = udp_output;
```

### 3.3 循环调用 update

```text
// 以一定频率调用 ikcp_update来更新 kcp状态，并且传入当前时钟（毫秒单位）

// 如 10ms调用一次，或用 ikcp_check确定下次调用 update的时间不必每次调用

ikcp_update(kcp, millisec);
```

### 3.4 输入一个[应用层](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%E5%BA%94%E7%94%A8%E5%B1%82%26spm%3D1001.2101.3001.7020)数据包（如UDP收到的数据包）

```text
// 收到一个下层数据包（比如UDP包）时需要调用：ikcp_input(kcp,received_udp_packet,received_udp_size);
```

处理了下层协议的输出/输入后 KCP协议就可以正常工作了，使用 ikcp_send 来向远端发送数据。而另一端使用 ikcp_recv(kcp, ptr, size)来接收数据。

![img](https://pic2.zhimg.com/80/v2-5fc605ca1b452033cf61821cb98cbfe5_720w.webp)

总结：UDP收到的包，不断通过kcp_input喂给KCP，KCP会对这部分数据（KCP协议数据）进行解包，重新封装成应用层用户数据，应用层通过kcp_recv获取。应用层通过kcp_send发送数据，KCP会把用户数据拆分kcp数据包，通过kcp_output，以UDP（send）的方式发送。

## 4.KCP使用方式

**这部分KCP文档有介绍，理解KCP协议无需过于关注。协议默认模式是一个标准的 ARQ，需要通过配置打开各项加速开关：**

### 4.1 工作模式

```text
int ikcp_nodelay(ikcpcb *kcp, int nodelay, int interval, int resend, int nc)
```

- nodelay ：是否启用 nodelay模式，0不启用；1启用。
- interval ：协议内部工作的 interval，单位毫秒，比如 10ms或者 20ms。
- resend ：快速重传模式，默认0关闭，可以设置2（2次ACK跨越将会直接重传）。
- nc ：是否关闭流控，默认是0代表不关闭，1代表关闭。

**普通模式： ikcp_nodelay(kcp, 0, 40, 0, 0);**

**极速模式： ikcp_nodelay(kcp, 1, 10, 2, 1);**

### 4.2 最大窗口

```text
int ikcp_wndsize(ikcpcb *kcp, int sndwnd, int rcvwnd);
```

该调用将会设置协议的最大发送窗口和最大接收窗口大小，默认为32. 这个可以理解为 TCP的 SND_BUF 和 RCV_BUF，只不过单位不一样 SND/RCV_BUF 单位是字节，这个单位是包。

### 4.3 最大传输单元

纯算法协议并不负责探测 MTU，默认 mtu是1400字节，可以使用ikcp_setmtu来设置该值。该值将会影响数据包归并及分片时候的最大传输单元。

### 4.4 最小RTO

不管是 TCP还是 KCP计算 RTO时都有最小 RTO的限制，即便计算出来RTO为40ms，由于默认的 RTO是100ms，协议只有在100ms后才能检测到丢包，快速模式下为30ms，可以手动更改该值：
**kcp->rx_minrto = 10;**



## 5.KCP为什么存在

首先要看TCP与UDP的区别，TCP与UDP都是传输层的协议，比较两者的区别主要应该是说TCP比UDP多了什么:

- 面向连接：TCP接收方与发送方维持了一个状态（建立连接，断开连接），双方知道对方还在。
- 可靠的：发送出去的数据对方一定能够接收到，而且是按照发送的顺序收到的。
- 流量控制与拥塞控制：TCP靠谱通过滑动窗口确保，发送的数据接收方来得及收。TCP无私，发生数据包丢失的时候认为整个网络比较堵，自己放慢数据发送速度。

TCP协议的可靠与无私让使用TCP开发更为简单，同时它的这种设计也导致了慢的特点。UDP协议简单，所以它更快。但是，UDP毕竟是不可靠的，应用层收到的数据可能是缺失、乱序的。KCP协议就是在保留UDP快的基础上，提供可靠的传输，应用层使用更加简单。

其他差别，TCP以字节流的形式，UDP以数据包的形式。很多人以为，UDP是不可靠的，所以sendto(1000),接收端recvfrom(1000)可能会收到900。这个是错误的。所谓数据包，就是说UDP是有界的，sendto(300),sendto(500)；接收到，recvfrom(1000),recvfrom(1000)那么可能会收到300，500或者其中一个或者都没收到。UDP应用层发送的数据，在接收缓存足够的情况下，要么收到全的，要么收不到。

总结：TCP可靠简单，但是复杂无私，所以速度慢。KCP尽可能保留UDP快的特点下，保证可靠。

## 6.KCP原理

### 6.1 协议简介

KCP是一个可靠的传输协议，UDP本身是不可靠的，所以需要额外信息来保证传输数据的可靠性。**因此，我们需要在传输的数据上增加一个包头。用于确保数据的可靠、有序。**

![img](https://pic2.zhimg.com/80/v2-fe57c61fd733e26bf1ac9fa419cf2c31_720w.webp)

- conv:连接号。UDP是无连接的，conv用于表示来自于哪个客户端。对连接的一种替代。
- cmd:命令字。如，IKCP_CMD_ACK确认命令，IKCP_CMD_WASK接收窗口大小询问命令，IKCP_CMD_WINS接收窗口大小告知命令。
- frg:分片，用户数据可能会被分成多个KCP包，发送出去。
- wnd:接收窗口大小，发送方的发送窗口不能超过接收方给出的数值
- ts:时间序列。
- sn:序列号。
- una:下一个可接收的序列号。其实就是确认号，收到sn=10的包，una为11。
- len：数据长度。
- data:用户数据。

后面的讲解，主要以极速模式：**ikcp_nodelay(kcp, 1, 10, 2, 1)为主，启用nodelay设置，刷新间隔控制在10ms，开启快速重传模式，关闭流量控制。**

### 6.2 数据发送过程

1.数据发送准备

用户发送数据的函数为ikcp_send。

ikcp_send(ikcpcb kcp, const char buffer, int len)

该函数的功能非常简单，把用户发送的数据根据MSS进行分片。如上图，用户发送1900字节的数据，MTU为1400byte。因此，该函数会把1900byte的用户数据分成两个包，一个数据大小为1400，头frg设置为1，len设置为1400；第二个包，头frg设置为0，len设置为500。切好KCP包之后，放入到名为snd_queue的待发送队列中。

注：流模式情况下，kcp会把两次发送的数据衔接为一个完整的kcp包。非流模式下，用户数据%MSS的包，也会作为一个包发送出去。

MTU，数据链路层规定的每一帧的最大长度，超过这个长度数据会被分片。通常MTU的长度为1500字节，IP协议规定所有的路由器均应该能够转发（512数据+60IP首部+4预留=576字节）的数据。MSS，最大输出大小（双方的约定），KCP的大小为MTU-kcp头24字节。IP数据报越短，路由器转发越快，但是资源利用率越低。传输链路上的所有MTU都一至的情况下效率最高，应该尽可能的避免数据传输的工程中，再次被分。UDP再次被分的后（通常1分为2），只要丢失其中的任意一份，两份都要重新传输。因此，合理的MTU应该是保证数据不被再分的前提下，尽可能的大。

以太网的MTU通常为1500字节-IP头（20字节固定+40字节可选）-UDP头8个字节=1472字节。KCP会考虑多传输协议，但是在UDP的情况下，设置为1472字节更为合理。

2.实际发送

KCP会不停的进行update更新最新情况，数据的实际发送在update时进行。发送过程如下图所示：

![img](https://pic1.zhimg.com/80/v2-2e50a43b3ed2cfd22bdc865e31ec5974_720w.webp)

**步骤一：待发送队列移至发送队列：**

- KCP会把snd_queue待发送队列中的kcp包，移至snd_buf发送队列。移动的包的数量不会超过snd_una+cwnd-snd_nxt，确保发送的数据不会让接收方的接收队列溢出。该功能类似于TCP协议中的滑动窗口。cwnd=min(snd_wnd,rmt_wnd,kcp->cwnd)的最小值决定，snd_wnd，rmt_wnd比较好理解可发送的数据，可发送的数据最大值，应该是发送方可以发送的数据和接收方可以接收的数据的最小值。kcp->cwnd是拥塞控制的一个值，跟网络状况相关，网络状况差的时候，KCP认为应该降低发送的数据，后面会有详细的介绍。
- 如上图中，snd_queue待发送队列中有4个KCP包等待发送，这个时候snd_nxt下一个发送的kcp包序列号为11，snd_una下一个确认的KCP包为9（8已经确认，9，10已经发送但是还没得到接收方的确认）。因为cwnd=5，发送队列中还有2个发送了但是还未得到确认，所以可以从待发送队列中取前面的3个KCP包放入到发送队列中，序列号分别设置为11,12,13。

**步骤二：发送发送队列的数据：**

发送队列中包含两种类型的数据，已发送但是尚未被接收方确认的数据，没被发送过的数据。没发送过的数据比较好处理，直接发送即可。重点在于已经发送了但是还没被接收方确认的数据，该部分的策略直接决定着协议快速、高效与否。 KCP主要使用两种策略来决定是否需要重传KCP数据包，超时重传、快速重传、选择重传。

超时重传：

TCP超时计算是RTOx2，这样连续丢三次包就变成RTOx8了，而KCP非快速模式下每次+RTO，急速模式下+0.5RTO（实验证明1.5这个值相对比较好），提高了传输速度。

![img](https://pic4.zhimg.com/80/v2-851b5a3f238bcb96b0d173f3d5a3e893_720w.webp)

快速重传:

发送端发送了1,2,3,4,5几个包，然后收到远端的ACK: 1, 3, 4, 5，当收到ACK3时，KCP知道2被跳过1次，收到ACK4时，知道2被跳过了2次，此时可以认为2号丢失，不用等超时，直接重传2号包，大大改善了丢包时的传输速度。TCP有快速重传算法，TCP包被跳过3次之后会进行重传。注：可以通过统计错误重传（重传的包实际没丢，仅乱序），优化该设置。

选择重传:

老的TCP丢包时会全部重传从丢的那个包开始以后的数据，KCP是选择性重传，只重传真正丢失的数据包。但是，目前大部分的操作系统，linux与android手机均是支持SACK选择重传的。

**步骤三：数据发送：**

通过步骤2判定，kcp包是否需要发送，如果需要发送的kcp包则通过，kcp_setoutput设置的发送接口进行发送，UDP通常为sendto。步骤3，会对较小的kcp包进行合并，一次性发送提高效率。

3.数据接收过程

KCP的接收过程是将UDP收到的数据进行解包，重新组装顺序的、可靠的数据后交付给用户。

1.KCP数据包接收

kcp_input输入UDP收到的数据包。kcp包对前面的24个字节进行解压，包括conv、 frg、 cmd、 wnd、 ts、 sn、 una、 len。根据una，会删除snd_buf中，所有una之前的kcp数据包，因为这些数据包接收者已经确认。根据wnd更新接收端接收窗口大小。根据不同的命令字进行分别处理。数据接收后，更新流程如下所示：

![img](https://pic3.zhimg.com/80/v2-82471fc151b9e2993925e39d51a717da_720w.webp)

1、IKCP_CMD_PUSH数据发送命令：

a、KCP会把收到的数据包的sn及ts放置在acklist中，两个相邻的节点为一组，分别存储sn和ts。update时会读取acklist，并以IKCP_CMD_ACK的命令返回确认包。如下图中，收到了两个kpc包，acklist中会分别存放10,123,11,124。

b、kcp数据包放置rcv_buf队列。丢弃接收窗口之外的和重复的包。然后将rcv_buf中的包，移至rcv_queue。原来的rcv_buf中已经有sn=10和sn=13的包了，sn=10的kcp包已经在rcv_buf中了，因此新收到的包会直接丢弃掉，sn=11的包放置至rcv_buf中。

c、把rcv_buf中前面连续的数据sn=11，12，13全部移动至rcv_queue，rcv_nxt也变成14。

rcv_queue的数据是连续的，rcv_buf可能是间隔的。d、kcp_recv函数，用户获取接收到数据（去除kcp头的用户数据）。该函数根据frg，把kcp包数据进行组合返回给用户。

![img](https://pic1.zhimg.com/80/v2-08cef05642e908ea4f200481ce2c345c_720w.webp)

2、IKCP_CMD_ACK数据确认包：

两个使命：1、RTO更新，2、确认发送包接收方已接收到。

正常情况：收到的sn为11,una为12。表示sn为11的已经确认，下一个等待接收的为12。发送队列中，待确认的一个包为11，这个时候snd_una向后移动一位，序列号为11的包从发送队列中删除。

![img](https://pic3.zhimg.com/80/v2-e30f58e256e1f0750878c68adba30f3a_720w.webp)

**异常情况：**如下图所示，sn!=11的情况均为异常情况。sn<11表示，收到重复确认的包，如本来以为丢失的包重新又收到了，所以产生重复确认的包；sn>17，收到没发送过的序列号，概率极低，可能是conv没变重启程序导致的；112，则启动快速重传。

![img](https://pic2.zhimg.com/80/v2-1159a3751e5a359e8b1a7f24dc19ceb5_720w.webp)

**确认包发送，接收到的包会全部放在acklist中，以IKCP_CMD_ACK包发送出去。**

## 7.[流量控制](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6%26spm%3D1001.2101.3001.7020)与拥塞控制

### 7.1 RTO计算（与TCP完全一样）

- RTT：一个报文段发送出去，到收到对应确认包的时间差。
- SRTT(kcp->rx_srtt)：RTT的一个加权RTT平均值，平滑值。
- RTTVAR(kcp->rx_rttval)：RTT的平均偏差，用来衡量RTT的抖动。

### 7.2 流量控制

流量控制是点对点的通信量的控制，是一个端到端的问题。总结起来，就是发送方的速度要匹配接收方接收（处理）数据的速度。发送方要抑制自身的发送速率，以便使接收端来得及接收。

KCP的发送机制采用TCP的滑动窗口方式，可以非常容易的控制流量。KCP的头中包含wnd，即接收方目前可以接收的大小。能够发送的数据即为snd_una与snd_una+wnd之间的数据。接收方每次都会告诉发送方我还能接收多少，发送方就控制下，确保自己发送的数据不多于接收端可以接收的大小。

KCP默认为32，即可以接收最大为32*MTU=43.75kB。KCP采用update的方式，更新间隔为10ms，那么KCP限定了你最大传输速率为4375kB/s，在高网速传输大内容的情况下需要调用ikcp_wndsize调整接收与发送窗口。

KCP的主要特色在于实时性高，对于实时性高的应用，如果发生数据堆积会造成延迟的持续增大。建议从应用侧更好的控制发送流量与网络速度持平，避免缓存堆积延迟。

![img](https://pic3.zhimg.com/80/v2-8a920482e817c3752df001419dd22f8e_720w.webp)

### 7.3 拥塞控制（KCP可关闭）

KCP的优势在于可以完全关闭拥塞控制，非常自私的进行发送。KCP采用的拥塞控制策略为TCP最古老的策略，无任何优势。完全关闭拥塞控制，也不是一个最优策略，它全是会造成更为拥堵的情况。

网络中链路的带宽，与整条网络中的交换节点（路由器、交换机、基站等）有关。如果，所有使用该链路的流量超出了，该链路所能提供的能力，就会发生拥塞。车多路窄，就会堵车，车越多堵的越厉害。因此，TCP作为一个大公无私的协议，当网络上发送拥堵的时候会降低自身发送数据的速度。拥塞控制是整个网络的事情，流量控制是发送和接收两方的事情。

当发送方没有按时接收到确认包，就认为网络发生了拥堵行为。TCP拥塞控制的方式，归结为慢开始、拥塞避免，如下图所示:

![img](https://pic4.zhimg.com/80/v2-eb3f5c3cd927110c78bded7e6dde0d13_720w.webp)

KCP发生丢包的情况下的拥塞控制策略与TCP Tahoe版本的策略一致。TCP Reno版本已经使用快恢复策略。因此，丢包的情况下，其实KCP拥塞控制策略比TCP更为苛刻。

KCP在发生快速重传，数据包乱序时，采用的是TCP快恢复的策略。控制窗口调整为已经发送没有接收到ack的数据包数目的一半+resent。

注：目前kernel 3.2以上的linux，默认采用google改进的拥塞控制算法，Proportional Rate Reduction for TCP。该算法的主要特点为，的cwnd如下图所示：

![img](https://pic4.zhimg.com/80/v2-24645bc476b807060ad280a8e20db2ef_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/581526921

作者：linux

# 【NO.68】Socket 面对的挑战？

在软件中最普遍和生命力最强的接口之一就是是Socket API。Socket API最早是由由加州大学伯克利分校计算机系统研究小组开发的，在1982年作为 BSD 4.1c操作系统的一部分首次发布。虽然有一些使用时间更长的 API ，例如那些处理 Unix 文件 I/O 的 API ，但是一个 API 能够保持使用并且近40年来基本上没有变化，这是及其令人印象深刻的事情了。对Socket API 的主要更新是扩展了辅助程序，以适应 IPv6的大地址空间。

互联网和整个网络世界自 socket API 诞生以来已经发生了非常重大的变化， API 已经改变了开发者思考和编写网络应用程序的方式，但是，网络世界和各种服务的不断变化，给socket API 带来了哪些挑战呢？

![img](https://pic4.zhimg.com/80/v2-7d6520bcfcf6f641869d7e867c4eb247_720w.webp)

## 1.Socket 的历史

1982年到如今，关于网络的两个最大区别是拓扑和速度。人们注意到的是速度的提高，而不是拓扑结构的变化。在1982年，商用长途网络链路的最大带宽是1.5 Mbps。而所部署的以太局域网速度为10mbps。局域网上两台计算机之间的往返时间以几十毫秒计算，互联网上各系统之间的往返时间以几百毫秒计算，这当然取决于位置和一个数据包在计算机之间传送时的跳数。一个家庭用户能够通过电话线连接到任何计算设备都是幸福的事，1995年，自己在当时的电报局申请了BTA的邮箱，并兴奋了很久。

当时的网络拓扑结构相对简单，大多数计算机只连接到一个局域网; 局域网连接到一个原始路由器，这个路由器可能有一些到其他局域网的连接或者到互联网的一个连接。对于一个应用程序到另一个应用程序，连接要么跨越局域网，要么传输一个或多个路由器。

分布式编程的模型中最普及的是基于socket API 的客户端/服务器模型，其中有一个服务器和一组客户端。客户端向服务器发送消息，要求服务器代表它们完成工作，等待服务器完成请求的工作，然后在稍后的某个时刻收到答复。这种计算模型已经无处不在，它通常是许多软件工程师所熟悉的唯一模型。然而，在设计socket的时候，它被看作是在计算机网络上扩展 Unix 文件 I/O 模型的一种方法。另一个原因是它支持最流行的 TCP协议，本质上具有点对点的通信模型。

Socket API 使客户机/服务器模型易于实现，程序员只需要将少量的系统调用添加到非联网代码中，就可以利用其他计算资源，这使得Socket API的客户机/服务器模式已经成为主导网络计算的模式。

Socket 中的以下五个函数是 API 的核心，并且是与常规文件 I/O 的区别所在:

| socket()  | 创建通信端点                   |
| --------- | ------------------------------ |
| bind()    | 将端点绑定到一组网络层参数     |
| connect() | 连接服务器提交请求             |
| listen()  | 监听链路并设置请求的数量限制   |
| accept()  | 接受来自客户端的一个或多个请求 |

实际上，socket ()调用可以替换为 open ()的一个变体，但是当时还没有这样做。Socket ()和 open ()实际上都是将相同的东西返回给程序: 一个进程唯一的文件描述符，并用于用于该 API 的所有后续操作。socket API 的简单性导致了它的无处不在，但无处不在阻碍了替代或增强API 的开发，而那些 API 可以帮助程序员开发其他类型的分布式应用程序。



## 2.socket 面临的挑战

客户机/服务器的计算模式在开发时具有许多优点。它允许许多用户共享资源，有了这种共享模式，就有可能提高资源的利用率。

然而，Socket AP在以下三个不同的网络区域表现不佳:

- 低延迟或实时应用程序
- 高带宽应用程序
- 多宿主系统(即具有多个网络接口的系统)。

许多人混淆了增加网络带宽和提高性能，因为增加带宽并不一定会减少延迟。Socket API 面临的主要是性能挑战，即如何让应用程序更快地访问网络数据。

任何使用socket API 的程序发送和接收数据的方式都是通过对操作系统的调用。所有这些调用都有一个共同点: 调用程序必须不断地请求要传递的数据，因为服务器不能在没有客户机请求的情况下做任何事情。然而，如果服务是音乐或视频呢，那该怎么办？在媒体分发服务中，可能有一个或多个数据源和多个监听器。只要用户在收听或查看媒体，最有可能的情况是应用程序需要任何已经到达的数据。不断地请求新数据是对应用程序的时间和资源的浪费。Socket API 没有向程序员提供这样一种方式: “无论何时有数据需要处理，都直接调用socket来处理它。”

Socket 程序是从数据缺乏而不是数据丰富的角度编写的。网络程序非常习惯于等待数据，因此使用一个单独的系统调用例如 select () ，这样就可以侦听多个数据源，而不会阻塞单个请求。基于 socket 的程序的典型处理循环不是简单地 read ()、 process ()、 read () ，而是 select ()、 read ()、 process ()、 select ()。虽然将单个系统调用添加到循环中似乎不会增加太多负担，但情况并非如此。每个系统调用都需要将参数封送并复制到内核中，同时导致系统阻塞调用进程并调度另一个进程。如果调用者在调用 select ()时可以获得数据，那么跨越用户/内核边界的所有工作都将被浪费，因为 read ()会立即返回数据。除非连续请求之间的间隔时间相当长，否则常规的检查/读取/检查是一种浪费。

要克服这个问题，需要反转应用程序和操作系统之间的通信模型，提供一个允许内核直接调用程序的 API 。但各种尝试中没有一个获得广泛接受。在开发sockket API 时存在的操作系统，在一般情况下，都是在单处理器计算机上执行单线程的。如果内核反调 API，就会有调用在哪个上下文中执行的问题。这种软件架构唯一流行的地方是没有用户和虚拟内存的嵌入式系统和网络路由器。

虚拟内存的问题使得实现内核上行调用机制的问题更加复杂。分配给用户进程的内存是虚拟内存，但网络接口等设备使用的内存是物理内存。让内核将物理内存从设备映射到用户空间，打破了虚拟内存系统提供的基本保护。

![img](https://pic2.zhimg.com/80/v2-490690bb358c444bc47760c9c0ea6ebd_720w.webp)

## 3.面对挑战的尝试与猜想

为了克服socket API 中存在的性能问题，有几种不同的机制，有时在不同的操作系统上实现了这些机制。

### 3.1 低延迟的网络应用

对于那些更关心延迟的程序而言，所做的工作很少。对于正在等待网络事件的程序来说，唯一重要的改进是添加了一组程序可以等待的内核事件，实现异步通知机制。例如 kevents () 是 select ()机制的扩展，它包含了内核可能告诉程序的任何可能的事件。在 kevents ()出现之前，用户程序可以在任何文件描述符上调用 select () ，这样程序就可以知道一组文件描述符中的任何一个是可读的、可写的，或者有错误。当程序被写入一个循环并等待一组文件描述符时，例如从网络读取并写入磁盘ー select ()调用就足够了，但是一旦程序想检查其他事件，例如计时器和信号，select ()就无能为力了。低延迟应用程序的问题在于 kevents ()不传递数据，只传递数据就绪的信号。下一个逻辑步骤是使用基于事件的 API 来传递数据。为了获得内核知道应用程序需要的数据，让应用程序两次跨越用户/内核边界是没有道理的。

### 3.2 高带宽的网络应用

因为复制数据会降低网络协议的性能，其中一种机制是零拷贝socket，为了提高对高带宽更感兴趣的网络应用程序速度，对操作系统进行了修改，以避免更多的数据副本。

传统上，操作系统对系统接收到的每个数据包执行两个副本。第一个拷贝由网络驱动程序从网络设备的内存中执行到内核的内存中，第二个拷贝由内核中的socket层在用户程序读取数据时执行。系统接收到的每个消息都要执行拷贝，导致这些复制操作的成本都较高。同理，当程序想要发送一条消息时，必须将发送的每条消息的数据从用户程序复制到内核; 然后再被复制到设备用来在网络上传输的缓冲区中。

数据复制对系统性能是一种诅咒，可以努力在内核中最小化这种复制。内核避免数据拷贝的最简单方法是让设备驱动程序将数据直接复制到内核内存中或从内核内存中复制出来。在现代网络的设备上，这是如何构建内存的结果。驱动程序和内核共享两个分组描述符环(一个用于发送，一个用于接收) ，其中每个描述符都有一个指向内存的指针。网络设备驱动程序最初用内核的内存填充这些发送/接收环。当接收到数据时，设备在正确的接收描述符中设置一个标志，通常通过中断告诉内核有数据等待。然后，内核从接收描述符环中删除已填充的缓冲区，并将其替换为新的缓冲区，以便设备填充。数据包以缓冲区的形式在网络堆栈中移动，直到到达套接字层，当用户的程序调用 read ()时，数据包从内核中复制出来。程序发送的数据由内核以类似的方式处理，内核缓冲区最终被添加到传输描述符环中，然后设置一个标志来告诉设备它可以将数据放在网络上的缓冲区中。

内核中的所有这些工作都没有解决最后那个拷贝的问题，仍然是跨用户/内核边界安全地共享内存。内核无法将其内存提供给用户程序，因为这时它将失去对内存的控制。崩溃的用户程序可能会使内核失去大量可用内存，从而导致系统性能下降。跨内核/用户边界共享内存缓冲区也存在固有的安全问题。此时，对于如何使用 sockets API 实现更高的带宽，暂时没有单一的答案。

![img](https://pic2.zhimg.com/80/v2-cae1ab53ef685a48b1c78d48f672e715_720w.webp)

### 3.3多宿主的网络应用

socket API 不仅在应用程序编写上存在性能问题，而且还减少了可能发生的通信类型。客户机/服务器模式本质上是点对点的通信类型。虽然服务器可以处理来自不同客户机组的请求，但是每个客户机对于一个请求或一组请求只有一个到单个服务器的连接。在一个每台计算机只有一个网络接口的世界里，这种模式非常合理。客户机和服务器之间的连接由 < 源 IP，源端口，目标 IP，目标端口 > 来标识。由于服务通常有一个众所周知的目标端口(例如，HTTP 的目标端口为80) ， IP 地址是固定的，所以唯一可以容易改变的值是源端口。

在socket API诞生的年代，每台不是路由器的计算机只有一个网络接口，这意味着为了识别一个服务，客户端计算机需要一个目的地址和端口，而它本身只有一个源地址和端口。一台计算机用多种方式获得服务的想法过于复杂，而且实现起来成本太高。考虑到这些限制，sockets API 没有理由向程序员展示编写多宿主程序的能力，这样的呈现可以管理对其重要的接口或连接。这些特性在实现时是操作系统中路由软件的一部分。程序最终能够访问它们的唯一途径是通过一组名为路由套接字(routing socket)的非标准内核 API。

在具有多个网络接口的系统上，使用标准的Socket API 编写一个可以轻松地多网址址的应用程序是不可能的。如果那样的话，在利用这两个接口时，如果其中一个出现故障，或者如果数据包流经的主要路由出现故障，应用程序不会失去与服务器的连接。

尽管SCTP 在协议级别集成了对多宿主的支持，但是不可能通过socket API 导出这种支持。最初提供了几个临时系统调用，这是访问这一功能的唯一方法。到目前为止，这可能是唯一一个同时具有这个特性的能力和用户需求的协议，但这个 API 还没有在多个操作系统中标准化。下表列出了 SCTP 添加的API：

![img](https://pic2.zhimg.com/80/v2-c1c06ba9addeca1ea807f8f6a4f186a9_720w.webp)

虽然这个函数列表超过了API必需的数量，但需要注意的是，许多函数都是socket api 的衍生品，例如 send () ，需要扩展才能在一个多宿主的世界中工作。现在的问题是Socket API无处不在，以至于很难改变现有的 API 集合，害怕混淆用户或者已有的应用程序。

随着系统内置了越来越多的网络接口，编写利用多宿主应用程序的能力将是必要的。很容易地想象这种技术在智能手机中的应用，智能手机有三个显然的网络接口: 通过蜂窝网络的接口，WiFi 接口，通常还有一个蓝牙接口。如果哪怕只有一个网络接口正常工作，应用程序也不应该失去连接性。应用程序设计者的问题在于，希望自己的代码能够在很少或没有任何变化的情况下，通过大量的设备工作，从手机到笔记本电脑，再到台式机等等。有了正确定义的API，就可以移除阻止这种情况发生。只是由于 socket API “足够好”的事实，这种需求尚未得到满足。

![img](https://pic2.zhimg.com/80/v2-cc43c2c658863728e82229a86a88caa1_720w.webp)

## 4.小结

对高带宽、低延迟和多宿主的支持是socket API 需要面对的挑战。局域网现在已经达到10 Gbps，对于许多应用程序来说，客户机/服务器风格的通信效率太低，可能无法高效使用可用的带宽。扩展socket API 支持的通信范例，以允许跨内核边界共享内存，允许将数据传送到应用程序的低延迟机制。另外，因为具有多个主动接口的设备正在成为网络系统的标准，多宿主的支持也应该成为socket API 的一个特性。

原文地址：https://zhuanlan.zhihu.com/p/580665411

作者：linux

# 【NO.69】TCP收发数据“丢失”问题的排查与解决

我们协议栈的某条业务线数据的收发基于TCP连接的，在测试过程中发现，TCP数据的接收与发送各有一次数据“丢失”的问题。我们知道TCP数据的收发是可靠的，不会发生数据丢失的情况，本文将讲的数据“丢失”是出问题时给人的一种假象。下面本文就来详细地讲述一下收发数据失败问题的排查过程。

![img](https://pic4.zhimg.com/80/v2-290f50dc6aa2952aef3292e1901b46e7_720w.webp)

## 1.TCP发送数据的“丢失”问题

### 1.1 问题描述

平台的同事向我们反馈，换了新版本的协议栈后，发送大的消息（发送的是1669个字节的数据）会有概率失败的情况。失败不是说，发送端调用我们协议栈的接口发送数据时接口就返回失败，而是返回了成功，但接收端却并没有收到此消息。

### 1.2 问题分析

老版本的协议栈和新版本的协议栈差异化还是很大的，所以不可能直接去对比所有协议栈的内容。所以只能一步步从这个问题的源头查起。

首先分析是发送端的问题还是接收端的问题，经过在给接口处加打印、数据收发处加打印以及抓包分析后发现，接口处传入的数据都是正确的，但是从包里看却发现是发送端出了数据“丢失”、“乱序”的问题。

因为数据流太大，协议栈会进行分包发送，每包最大512bytes。那么1699bytes的数据就应该分为4包发送，且按顺组成的内容应该跟原始数据一致。如图所示：

![img](https://pic1.zhimg.com/80/v2-0fb79e4477d39189a9f5c4a79d2190d8_720w.webp)

数据中包括一个TPKT头（03 00 06 a3），其中06 a3指示数据大小为1699bytes，4包数据为一个完整数据。

上图是正确情况下的内容，出问题时就会出现如下图所示的“乱序”问题：

![img](https://pic2.zhimg.com/80/v2-895efeccae78c324577e786a2e3dc2b5_720w.webp)

从上图中我们可以看到第四包数据的内容也出现了一个TPKT头（03 00 06 a3），实际上数据中有和头部相同的数据是没什么奇怪的，但是分析发现第四包数据内容和第一包数据一模一样。

对比打印中正确的数据，第四包的数据就是错的，而且打印中发现，业务是发送了两次非标数据，那么第四包的数据应该就是第二次发送的数据头，这样的话给我们的感觉就是TCP竟然出现了“乱序”！检查了包里的内容发现，后边缺失的数据也没看到有发送，那么分析下来就是出现“丢失”数据了，而不是“乱序”！

分析代码时经过高人指点，发现协议栈发送数据时是切分数据后直接循环调用send发送，会不会是发送太快又频繁而导致TCP的发送缓冲区不够用了呢。那么在每次发送后都sleep上几毫秒后测试，每次发送都是正确的了。

### 1.3 问题解决

我们知道，调用套接字的send函数后，返回>0的值并不代表tcp已经把这么多数据发送给对方了，而是拷贝给了发送缓冲区多少个字节（非阻塞模式）。Tcp从缓冲区中发送数据也是需要消耗时间的，那么使用sleep就可以多给tcp点时间去把缓冲区的数据发送出去从而移除掉。

sleep虽然可以解决这个问题，但是sleep多长时间合适呢，而老版本协议栈为什么没有问题呢？此时再去查看老版本协议栈的代码，发现它并没有用sleep，那么只有放大缓冲区大小可以解决这个问题了。经查看，4.0协议栈确实在创建socket的时候，都会把收发缓冲区的大小设置下。

新版本的协议栈也使用此方法后问题解决。设置方法就是使用setsockopt函数，套接字选项为SO_SNDBUF、SO_RCVBUF。

肯定有人会问，当发送缓冲区满时，send会返回-1的啊，或者要发送的数据大于缓冲区中剩余的数据也回返回实际放入的数据值啊。好吧，确实是我们使用的失误，对返回值的操作没有处理好，而是自以为的都发送成功了，从而导致了数据“丢失”。所以当时看协议栈打印并没有报错的地方，从而把人引入更晕乎的状态。赶紧修改之。



## 2.TCP接收数据的“丢失”问题

### 2.1 问题描述

测试人员反馈，在他的一台win7电脑上测试软件的某一项功能时会大概率性失败，而其他电脑则没有这个问题。

### 2.2 问题分析

首先分析失败原因，从打印中看，是因为平台侧认为超时没收到此win7电脑的MSD（主从决定）消息，从而导致MSDACK无法正确完成，从而导致问题。

但抓win7电脑和平台侧的包后发现，win7侧信令有发送出去，平台侧的抓包看是有收到此包数据的。但是看平台侧打印就是没有收到此信令的打印，以至于解析win7电脑发来的MSD消息的TPKT头的打印都没有。查看解析数据的代码，并没有看到有什么特殊处理的地方，为什么只针对此win7电脑有问题呢，而且还不是必现？！

在分析打印的时候，看到平台侧收到win7电脑致邻发送的TCS（能力集）数据流，数据的末尾和某类型的TPKT头格式相同，都是06 00 xx xx的格式。而其他电脑致邻的TCS数据却不是这样，会不会是因为数据和TPKT头相似而导致的解析错误呢？

协议栈的数据处理过程：首先它会读取数据的TPKT头部分，根据头部指示数据的大小再读入相应大小的数据。那么即使数据部分和头部相同应该也不会有把数据部分当做头部处理的情况。

在正确的情况下：

![img](https://pic1.zhimg.com/80/v2-9ffcd5a77ed24fcafb5fb2a043d0712c_720w.webp)

接收MSD消息打印分析收到win7发送的MSD消息过程：首先读取数据的前4个字节（TPKT头大小）的数据，TPKT头前两字节固定是03 00（某类型的TPKT头是06 00，会转化成03 00），后两个字节是数据大小，那么数据大小是00 0B=11bytes,减去TPKT头大小4bytes，剩下的就是7bytes，那么再读取7bytes的数据从而组成一个完整的MSD消息。

有问题时的抓包如下：

![img](https://pic4.zhimg.com/80/v2-754f751e0b198a8b6e42dc0fc01999cf_720w.webp)

对比上面两个图，01 00 32 80就是MSD的数据部分，后续再读取的3个字节总共7个字节就是MSD消息（传入的len=512，是因为发现之前读的数据不符合预期中的TPKT头，那么就一次读入512数据放到临时缓冲中，在从缓冲中逐个字节去找TPKT头，防止因为tcp数据流的传输方式导致一次的错误而把后续的有效消息也丢失的问题），但是为什么没有MSD消息的TPKT头部分呢？难道tcp数据出现了“丢失”？！但从wireshark抓包来看，其收到的数据是完整的！

在某特殊类型的产品中，把原来标准的TPKT头03 00 xx xx变为06 00 xx xx，并且数据部分插入4个bytes的0。这个处理被放到了调用send时转换为该特殊类型的产品数据，在recv收到数据后再恢复到标准数据。所以根据协议处理数据原理，先读TPKT头，然后根据头部读数据。在图4中的体现是：TPKT头“丢”了，而直接读取的是插入的4个0数据，发现不符合TPKT头格式再继续以4个字节大小尝试读取头部数据。

检查了tcp的socket接收缓冲区大小，已经被设置为挺大的值了，那么看来tcp数据的“丢失”不是对socket的设置导致，那就只能是我们代码哪里处理有问题了。因为win7电脑的TCS数据的“特殊化”，又继续分析了其TCS数据。对比成功和失败时候的打印，虽然测试人员说什么都没有改动，但是从打印中能看出来，成功和失败时候win7电脑致邻发送能力的大小是不一样的！

成功时接收TCS消息打印如下：

![img](https://pic2.zhimg.com/80/v2-9c83a7444a9edcb8d082f5baf7a8f00d_720w.webp)

发送失败时接收TCS消息打印如下：

![img](https://pic3.zhimg.com/80/v2-51a0ab96bfc473023749830f7aed55d6_720w.webp)

然后就注意到了，上面接收失败时读取数据时候出现了一个错误。虽然返回值是没有错误，而且打印也没有报错，但是从打印中看到我们要读取len=4字节长度的时候，实际received却是8！对于recv函数来说，除非接收buffer中数据长度小于你要读取的数据长度时，会返回和你要读取的数据不一样的值，否则你要读取多少字节就应该返回多少字节的数据。所以这个返回值肯定就是出现问题的元凶！

那么成功和失败时候的能力区别在哪里呢，对比包发现，呼叫时候的码率不同会导致能力字节大小不一样，所以在呼叫码率是8M的时候是必现，其他小点的码率就不出现。

### 2.3 问题解决

协议解析数据会先读取TPKT头数据的大小，为了解析某特殊类型产品的数据，在recv到数据后判断如果是该特殊类型产品的TPKT头，那么要做的就是改变头内容（把06变为03，指示数据长度的地方减去插入的4个bytes）、去掉后续是4个0的数据。

在错误情况下解析TCS数据的时，它刚好暴露了我们代码中的一个错误点。如图6所示，因为它的能力字节大小是520byets，去掉头部4bytes后还有516个。协议栈读数据时如果其大小超过512块大小就分块读取。所以先读512bytes，再读剩下的4bytes。而每次读取4bytes时，我们都会判断是否是某特殊类型产品的TPKT头，根据2.2.2中怀疑点，TCS的最后四个字节刚好和该特殊类型产品的TPKT头格式相同。而刚好因为其字节数的大小，与块大小的原因我们单独读取了这四个字节！

在判断是该特殊类型产品的TPKT头后，我们做了头部操作，然后再读取4个字节，判断是不是全0，如果是就去掉，否则就返回8（返回8是因为已经读了8个字节）。所以出现了接收失败时我们要读取4bytes但是返回8的现象。

所以最简单的解决方案就是实现“预读”。recv函数原型：

```text
int recv( _In_ SOCKET s, _Out_ char *buf, _In_ int len, _In_ int flags);
```

参数flags值：

MSG_DONTROUTE 绕过路由表查找。

MSG_DONTWAIT 仅本操作非阻塞。

MSG_OOB 发送或接收带外数据。

MSG_PEEK 窥看外来消息。

MSG_WAITALL 等待所有数据。

一般情况下，我们recv的第四个值都会写为0，那么我们调用了recv后，已经读过的数据就会从socket的缓冲区中移除。而MSG_PEEK值却可以为我们实现“预读”功能，即我们预读的数据就不会从socket接收缓冲区移除，在预读发现如果不是4个0字节，那么就直接返回已读的TPKT头大小数据，否则就使用从socket接收缓冲区中移除的方式读取完成删除插入的4个0字节的功能。

## 3.最后

在对分析问题的过程中，并没有像上述分析过程那样顺利，整个过程时艰难曲折的，中途查阅了不少资料。经历了这两个问题详细排查过程，使得我们对TCP的读写数据的原理及细节有了更为深刻的认识。

原文地址：https://zhuanlan.zhihu.com/p/580513576

作者：linux

# 【NO.70】用红黑树封装map和set

## 1.红黑树模拟实现完整代码

如下是红黑树kv模型的模拟实现完整代码，现在我们需要基于此代码封装出map和set基本的接口实现。

```text
#pragma once
#include<iostream>
using namespace std;

enum Color
{
	RED,
	BLACK,
};

template<class K, class V>
struct RBTreeNode
{
	RBTreeNode<K, V>* _left;
	RBTreeNode<K, V>* _right;
	RBTreeNode<K, V>* _parent;

	// 存储数据的键值对
	pair<K, V> _kv;
	// 存储节点颜色
	Color _col;

	RBTreeNode(const pair<K, V>& kv)
		:_left(nullptr)
		, _right(nullptr)
		, _parent(nullptr)
		, _kv(kv)
		, _col(RED)
	{}
};

template<class K, class V>
class RBTree
{
	typedef RBTreeNode<K, V> Node;

	void RotateL(Node* parent)
	{
		Node* subR = parent->_right;
		Node* subRL = subR->_left;
		Node* parentParent = parent->_parent;

		// 让parent的右指针指向subRL,判断一下subRL是否为空
		parent->_right = subRL;
		if (subRL)
			subRL->_parent = parent;

		// subR的左指针链接parent
		subR->_left = parent;
		parent->_parent = subR;

		// parent为根的情况，更新根节点，让根节点指向空
		if (_root == parent)
		{
			_root = subR;
			_root->_parent = nullptr;
		}
		else //若parent为一棵子树，则链接与parentParent的关系
		{
			if (parentParent->_right == parent)
				parentParent->_right = subR;
			else
				parentParent->_left = subR;

			subR->_parent = parentParent;
		}
	}

	void RotateR(Node* parent)
	{
		Node* subL = parent->_left;
		Node* subLR = subL->_right;
		Node* parentParent = parent->_parent;

		// 将subLR链接到parent的左边，这里注意subLR可能为空的情况，需要判断一下
		parent->_left = subLR;
		if (subLR)
		{
			subLR->_parent = parent;
		}

		// 将parent这棵子树链接到subL的右指针
		subL->_right = parent;
		parent->_parent = subL;

		// 若parent为根，则更新新的根节点
		if (parent == _root)
		{
			_root = subL;
			_root->_parent = nullptr;
		}
		else //若parent为一棵子树，则链接与parentParent的关系
		{
			if (parentParent->_right == parent)
				parentParent->_right = subL;
			else
				parentParent->_left = subL;

			subL->_parent = parentParent;
		}
	}

	void _InOrder(Node* root)
	{
		if (root == nullptr)
			return;

		_InOrder(root->_left);
		cout << root->_kv.first << " -> " << root->_kv.second << endl;
		_InOrder(root->_right);
	}

	void _Destroy(Node* root)
	{
		if (root == nullptr)
			return;

		_Destroy(root->_left);
		_Destroy(root->_right);
		delete root;
	}
public:
	void InOrder()
	{
		_InOrder(_root);
	}

	RBTree()
		:_root(nullptr)
	{}

	~RBTree()
	{
		_Destroy(_root);
		_root = nullptr;
	}

	Node* find(const K& key)
	{
		Node* cur = _root;
		while (cur)
		{
			if (cur->_kv.first < key)
				cur = cur->_right;
			else if (cur->_kv.first > key)
				cur = cur->_left;
			else
				return cur;
		}
		return nullptr;
	}

	pair<Node*, bool> insert(const pair<K, V>& kv)
	{
		// 一开始插入新节点时树为空，则直接让新节点作为根节点
		if (_root == nullptr)
		{
			_root = new Node(kv);
			_root->_col = BLACK;
			return make_pair(_root, true);
		}

		Node* cur = _root;
		Node* parent = _root;
		while (cur)
		{
			if (cur->_kv.first < kv.first)
			{
				parent = cur;
				cur = cur->_right;
			}
			else if (cur->_kv.first > kv.first)
			{
				parent = cur;
				cur = cur->_left;
			}
			else
				// 待插入的节点已经存在，则返回该节点的指针，插入失败
				return make_pair(cur, false);
		}

		// 走到这里就已经找到了将要插入的位置
		Node* newnode = new Node(kv);
		newnode->_col = RED;
		// 判断待插入节点与parent指向值的大小，将待插入节点插入到正确的位置
		if (parent->_kv.first < kv.first)
		{
			parent->_right = newnode;
			newnode->_parent = parent;
		}
		else
		{
			parent->_left = newnode;
			newnode->_parent = parent;
		}
		cur = newnode;

		// 若父亲存在且为红色就需要进行处理
		while (parent && parent->_col == RED)
		{
			// 若父节点为红色，则祖父节点一定存在，不需要进行判断
			Node* grandfather = parent->_parent;
			if (parent == grandfather->_left)
			{
				Node* uncle = grandfather->_right;
				//情况1：uncle存在且为红
				if (uncle && uncle->_col == RED)
				{
					parent->_col = uncle->_col = BLACK;
					grandfather->_col = RED;

					// 继续向上进行处理
					cur = grandfather;
					parent = cur->_parent;
				}
				else // 情况2+3：uncle不存在或者uncle存在且为黑
				{
					// 情况2：右单旋
					if (cur == parent->_left)
					{
						RotateR(grandfather);
						grandfather->_col = RED;
						parent->_col = BLACK;
					}
					else // 左右双旋
					{
						RotateL(parent);
						RotateR(grandfather);
						cur->_col = BLACK;
						grandfather->_col = RED;
					}
					break;
				}
			}
			else // grandfather->_right == parent;
			{
				Node* uncle = grandfather->_left;
				// 情况1
				if (uncle && uncle->_col == RED)
				{
					parent->_col = uncle->_col = BLACK;
					grandfather->_col = RED;

					// 继续向上进行处理
					cur = grandfather;
					parent = cur->_parent;
				}
				else // 情况2+3
				{
					if (cur == parent->_right)
					{
						RotateL(grandfather);
						parent->_col = BLACK;
						grandfather->_col = RED;
					}
					else // cur == parent->_left
					{
						RotateR(parent);
						RotateL(grandfather);
						cur->_col = BLACK;
						grandfather->_col = RED;
					}
					break;
				}
			}
		}

		// 将根节点的颜色处理为黑色
		_root->_col = BLACK;
		return make_pair(newnode, true);
	}

	bool erase(const K& key)
	{
		Node* parent = nullptr;
		Node* cur = _root;
		//用于标记实际的待删除结点及其父结点
		Node* deleteParent = nullptr;
		Node* deleteNode = nullptr;
		while (cur)
		{
			if (key < cur->_kv.first)
			{
				parent = cur;
				cur = cur->_left;
			}
			else if (key > cur->_kv.first)
			{
				parent = cur;
				cur = cur->_right;
			}
			else
			{
				if (cur->_left == nullptr) //待删除结点的左子树为空
				{
					if (cur == _root) //待删除结点是根结点
					{
						_root = _root->_right; //让根结点的右子树作为新的根结点
						if (_root)
						{
							_root->_parent = nullptr;
							_root->_col = BLACK; //根结点为黑色
						}
						delete cur; //删除原根结点
						return true;
					}
					else
					{
						deleteParent = parent; //标记实际删除结点的父结点
						deleteNode = cur; //标记实际删除的结点
					}
					break;
				}
				else if (cur->_right == nullptr) //待删除结点的右子树为空
				{
					if (cur == _root) //待删除结点是根结点
					{
						_root = _root->_left; //让根结点的左子树作为新的根结点
						if (_root)
						{
							_root->_parent = nullptr;
							_root->_col = BLACK; //根结点为黑色
						}
						delete cur; //删除原根结点
						return true;
					}
					else
					{
						deleteParent = parent; //标记实际删除结点的父结点
						deleteNode = cur; //标记实际删除的结点
					}
					break;
				}
				else //待删除结点的左右子树均不为空
				{
					//替换法删除
					//寻找待删除结点右子树当中key值最小的结点作为实际删除结点
					Node* minParent = cur;
					Node* minRight = cur->_right;
					while (minRight->_left)
					{
						minParent = minRight;
						minRight = minRight->_left;
					}
					cur->_kv.first = minRight->_kv.first;
					cur->_kv.second = minRight->_kv.second;
					deleteParent = minParent;
					deleteNode = minRight;
					break;
				}
			}
		}
		if (deleteNode == nullptr)
		{
			return false;
		}

		//记录待删除结点及其父结点
		Node* del = deleteNode;
		Node* delP = deleteParent;

		//调整红黑树
		if (deleteNode->_col == BLACK) //删除的是黑色结点
		{
			if (deleteNode->_left)
				deleteNode->_left->_col = BLACK;

			else if (deleteNode->_right) //待删除结点有一个红色的右孩子（不可能是黑色）
			{
				deleteNode->_right->_col = BLACK;
			}
			else //待删除结点的左右均为空
			{
				while (deleteNode != _root)
				{
					if (deleteNode == deleteParent->_left)
					{
						Node* brother = deleteParent->_right;
						//情况一：brother为红色
						if (brother->_col == RED)
						{
							deleteParent->_col = RED;
							brother->_col = BLACK;
							RotateL(deleteParent);

							brother = deleteParent->_right; //更新brother
						}
						//情况二：brother为黑色，且其左右孩子都是黑色结点或为空
						if (((brother->_left == nullptr) || (brother->_left->_col == BLACK))
							&& ((brother->_right == nullptr) || (brother->_right->_col == BLACK)))
						{
							brother->_col = RED;
							if (deleteParent->_col == RED)
							{
								deleteParent->_col = BLACK;
								break;
							}

							deleteNode = deleteParent;
							deleteParent = deleteNode->_parent;
						}
						else
						{
							//情况三：brother为黑色，且其左孩子是红色结点，右孩子是黑色结点或为空
							if ((brother->_right == nullptr) || (brother->_right->_col == BLACK))
							{
								brother->_left->_col = BLACK;
								brother->_col = RED;
								RotateR(brother);

								brother = deleteParent->_right; //更新brother
							}
							//情况四：brother为黑色，且其右孩子是红色结点
							brother->_col = deleteParent->_col;
							deleteParent->_col = BLACK;
							brother->_right->_col = BLACK;
							RotateL(deleteParent);
							break;
						}
					}
					else  //待删除结点是其父结点的左孩子
					{
						Node* brother = deleteParent->_left;
						//情况一：brother为红色
						if (brother->_col == RED)
						{
							deleteParent->_col = RED;
							brother->_col = BLACK;
							RotateR(deleteParent);
							//需要继续处理
							brother = deleteParent->_left;
						}
						//情况二：brother为黑色，且其左右孩子都是黑色结点或为空
						if (((brother->_left == nullptr) || (brother->_left->_col == BLACK))
							&& ((brother->_right == nullptr) || (brother->_right->_col == BLACK)))
						{
							brother->_col = RED;
							if (deleteParent->_col == RED)
							{
								deleteParent->_col = BLACK;
								break;
							}

							deleteNode = deleteParent;
							deleteParent = deleteNode->_parent;
						}
						else
						{
							//情况三：brother为黑色，且其右孩子是红色结点，左孩子是黑色结点或为空
							if ((brother->_left == nullptr) || (brother->_left->_col == BLACK))
							{
								brother->_right->_col = BLACK;
								brother->_col = RED;
								RotateL(brother);

								brother = deleteParent->_left;
							}
							//情况四：brother为黑色，且其左孩子是红色结点
							brother->_col = deleteParent->_col;
							deleteParent->_col = BLACK;
							brother->_left->_col = BLACK;
							RotateR(deleteParent);
							break;
						}
					}
				}
			}
		}
		//进行实际删除
		if (del->_left == nullptr)
		{
			if (del == delP->_left)
			{
				delP->_left = del->_right;
				if (del->_right)
					del->_right->_parent = delP;
			}
			else
			{
				delP->_right = del->_right;
				if (del->_right)
					del->_right->_parent = delP;
			}
		}
		else
		{
			if (del == delP->_left)
			{
				delP->_left = del->_left;
				if (del->_left)
					del->_left->_parent = delP;
			}
			else
			{
				delP->_right = del->_left;
				if (del->_left)
					del->_left->_parent = delP;
			}
		}
		delete del;
		return true;
	}
private:
	Node* _root;
};
```

## 2.红黑树参数适配改造

这里我们需要用同一棵红黑树来实现map和set，但是map是kv模型，而set是key模型。我们实现的红黑树是kv模型，为了同时适配k的模型，我们需要对红黑树的模板参数做一些处理。

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1007' height='531'></svg>)

1. 这里需要控制 map 和 set 传入红黑树底层结构的模板参数，为了与之前的 kv 模型参数进行区分，将红黑树的第二个模板参数改为T。则T参数可能存储键值 key，也有可能存储键值对 key-value。
2. map 传递给红黑树的T模板参数是 pair<K,V> , set 传递给红黑树的T模板参数是 K 。

```text
template<class K>
class set
{
private:
	RBTree<K, K> _t;
};

template<class K,class V>
class map
{
private:
	RBTree<K, pair<const K,V>> _t;
};
```

❓为什么不去掉红黑树的第一个模板参数，只保留第二个模板参数呢？

不能！红黑树中的第二个模板参数是一个键值对，对于 set 容器来说，省略红黑树第一个模板参数是没有任何问题的，但是对于 map 容器而言，若去掉红黑树的第二个模板参数，那么就无法得到 key 的类型，像 find、erase 这样的接口中是需要键值 key 的，所以第一个模板参数不能够省略。

**相关视频推荐**

[5种红黑树的场景，从Linux内核谈到Nginx源码，听完醍醐灌顶](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1KA411L72x/)

[源码阅读：STL 红黑树、散列表的实现](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1pY4y1n7Ew/)

[C/C++开发哪个方向更有前景，游戏，c++后端，网络处理，音视频开发，嵌入式开发，桌面开发](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1HV4y1578G/)

学习地址：**[c/c++ linux服务器开发/后台架构师](https://link.zhihu.com/?target=https%3A//ke.qq.com/course/417774%3FflowToken%3D1013300)**

需要C/C++ Linux服务器架构师学习资料加qun**[812855908](https://link.zhihu.com/?target=https%3A//jq.qq.com/%3F_wv%3D1027%26k%3DhpbIICU6)**获取（资料包括**C/C++，Linux，golang技术，Nginx，ZeroMQ，MySQL，Redis，fastdfs，MongoDB，ZK，流媒体，CDN，P2P，K8S，Docker，TCP/IP，协程，DPDK，ffmpeg**等），免费分享

![img](https://pic3.zhimg.com/80/v2-0fb9145c6e395f48de7c1d3a1d500dc2_720w.webp)

## 3.仿函数

红黑树中的模板参数T可能是 key ，也可能是 <key,value>键值对。在红黑树的实现中需要多次用 key 值进行比较，那我们应该怎样拿到节点的 key 呢？为了解决这个问题，上层容器 map 和 set 需要向底层结构红黑树提供一个仿函数去获取T模板的键值 key 。

![img](https://pic4.zhimg.com/80/v2-7a5c60b0c4a4622715a753a8c3a8d7db_720w.webp)

当上层容器是 set 时，红黑树中的 T 就是键值 key ，因此 set 的仿函数直接返回 K 值就可以了。

```text
template<class K>
class set
{
	struct SetKeyOfT
	{
		const K& operator()(const K& key)
		{
			return key;
		}
	};

private:
	RBTree<K, K, SetKeyOfT> _t;
};
```

当上层容器是 map 时，红黑树中的 T 储存的就是键值对 pair<key,value> ，因此 map 的仿函数需要取出键值对中的第一个参数 key 。

```text
template<class K,class V>
class map
{
	struct MapKeyOfT
	{
		const K& operator()(const pair<K, V>& kv)
		{
			return kv.first;
		}
	};

private:
	RBTree<K, pair<const K, V>, MapKeyOfT> _t;
};
```

✅仿函数应该如何使用呢？以下我们以红黑树的查找为例来看看仿函数的使用方法：

```text
iterator find(const K& key)
{
	KeyOfT kot; // 定义一个仿函数对象
	Node* cur = _root;
	while (cur)
	{
		if (kot(cur->_data) < key) // 利用仿函数对象去取_data中用来进行比较的参数
			cur = cur->_right;
		else if (kot(cur->_data) > key)
			cur = cur->_left;
		else
			return iterator(cur);
	}
	return end();
}
```

## 3.正向迭代器

✔️红黑树的迭代器即对红黑树中节点指针进行封装，便于使用。

```text
// 正向迭代器
template<class T,class Ref,class Ptr>
struct __TreeIterator
{
	typedef Ref reference;
	typedef Ptr pointer;

	typedef RBTreeNode<T> Node; // 重定义节点的类型
	typedef __TreeIterator<T, Ref, Ptr> Self; // 迭代器类型

	Node* _node; // 迭代器封装节点指针

	__TreeIterator(Node* node) // 迭代器的构造函数
		:_node(node)
	{}
}
```

✔️对正向迭代器进行解引用操作，只需要返回对应节点的数据即可。使用 -> 操作时，只需返回对应节点数据指针即可。

```text
Ref operator*()
{
	return _node->_data; // 返回节点数据的引用
}

Ptr operator->()
{
	return &(_node->_data); // 返回节点数据的指针
}
```

✔️在迭代器中我们还要使用 != 这个操作符，让迭代器指针在不越界的情况下依次向后遍历。

```text
bool operator!=(const Self& s)const
{
	return _node != s._node;
}

bool operator==(const Self& s)const
{
	return _node == s._node;
}
```

✔️红黑树正向迭代器找到下一个节点需要进行++操作，应该根据红黑树中序遍历的序列找到当前节点的下一个节点。

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='819' height='351'></svg>)

实现红黑树正向迭代器时，当前节点进行 ++ 操作后，应该根据红黑树中序遍历的序列找到当前节点的下一个节点。

++ 操作的具体步骤：

- 若当前节点的右子树不为空，则 ++ 之后应是其右子树中的最左节点。
- 若当前节点的右子树为空，则 ++ 之后应是该节点的祖先节点中，孩子不在父节点右的祖先。

实现代码：

```text
Self operator++()
{
	if (_node->_right) // 当前节点的右子树不为空
	{
		// 找到右子树中的最左节点 - 即右子树中中序的第一个节点
		Node* left = _node->_right;
		while (left->_left)
		{
			left = left->_left;
		}
		_node = left;
	}
	else // 当前节点右子树为空
	{
		Node* cur = _node;
		Node* parent = cur->_parent;
		while (parent && parent->_right == cur)
		{
			cur = cur->_parent;
			parent = parent->_parent;
		}
		_node = parent;
	}
	return *this;
}
```

✔️红黑树迭代器中 - - 的实现恰好与 ++ 相反，根据红黑树中序遍历序列找到当前节点的前一个节点。

\- - 操作的具体步骤：

1. 若当前节点的左子树不为空，则 - - 之后应是其左子树中的最右节点。
2. 若当前节点的左子树为空，则 - - 之后应是该节点的祖先节点中，孩子不在父节点左的祖先。

实现代码：

```text
Self& operator--()
{
	if (_node->_left)
	{
		// 左子树的最右节点
		Node* right = _node->_left;
		while (right->_right)
		{
			right = right->_right;
		}
		_node = right;
	}
	else
	{
		Node* cur = _node;
		Node* parent = cur->_parent;
		while (parent && cur == parent->_left)
		{
			cur = parent;
			parent = parent->_parent;
		}
		_node = parent;
	}
	return *this;
}
```

**注意：** 此处的 ++ 操作和 - - 操作和STL中的实现是不一样的，相对库里面的实现是有差距的，这里只是一个简单的模拟。想要看STL库里面的实现可以去查一下。

**正向迭代器完整代码：**

```text
template<class T,class Ref,class Ptr>
struct __TreeIterator
{
	typedef Ref reference;
	typedef Ptr pointer;

	typedef RBTreeNode<T> Node;
	typedef __TreeIterator<T, Ref, Ptr> Self;

	Node* _node;

	__TreeIterator(Node* node)
		:_node(node)
	{}

	Ref operator*()
	{
		return _node->_data;
	}

	Ptr operator->()
	{
		return &(_node->_data);
	}

	bool operator!=(const Self& s)const
	{
		return _node != s._node;
	}

	bool operator==(const Self& s)const
	{
		return _node == s._node;
	}

	Self operator++()
	{
		if (_node->_right) // 当前节点的右子树不为空
		{
			// 找到右子树中的最左节点 - 即右子树中中序的第一个节点
			Node* left = _node->_right;
			while (left->_left)
			{
				left = left->_left;
			}
			_node = left;
		}
		else // 当前节点右子树为空
		{
			Node* cur = _node;
			Node* parent = cur->_parent;
			while (parent && parent->_right == cur)
			{
				cur = cur->_parent;
				parent = parent->_parent;
			}
			_node = parent;
		}
		return *this;
	}

	Self& operator--()
	{
		if (_node->_left)
		{
			// 左子树的最右节点
			Node* right = _node->_left;
			while (right->_right)
			{
				right = right->_right;
			}
			_node = right;
		}
		else
		{
			Node* cur = _node;
			Node* parent = cur->_parent;
			while (parent && cur == parent->_left)
			{
				cur = parent;
				parent = parent->_parent;
			}
			_node = parent;
		}
		return *this;
	}
};
```

## 4.反向迭代器

红黑树的反向迭代器是由正向迭代器封装的，因此红黑树的反向迭代器也是一个迭代器适配器。和之前学习的 priority_queue、stack、queue 都是属于适配器。

```text
// 反向迭代器 - 迭代器适配器
template<class Iterator>
struct ReverseIterator
{
	typedef typename Iterator::reference Ref; // 节点指针的引用
	typedef typename Iterator::pointer Ptr;  // 节点指针
	typedef typename ReverseIterator<Iterator> Self; // 反向迭代器类型

	ReverseIterator(Iterator it)
		:_it(it) // 用正向迭代器封装一个反向迭代器
	{}

	Ref operator*()
	{
		return *_it; // 调用正向迭代器的operator* 返回节点数据的引用
	}

	Ptr operator->()
	{
		return _it.operator->(); // 调用正向迭代器的operator-> 返回节点数据指针
	}

	Self& operator++()
	{
		--_it; // 调用正向迭代器的 --
		return *this;
	}

	Self& operator--()
	{
		++_it; // 调用正向迭代器的 ++
		return *this;
	}

	bool operator==(const Self& s)const
	{
		return _it == s._it;
	}

	bool operator!=(const Self& s)const
	{
		return _it != s._it;
	}

	Iterator _it; 
};
```

☃️反向迭代器只有一个模板参数，即正向迭代器的类型。因此反向迭代器是不知道节点的引用类型和节点的指针类型的，这时我们需要对正向迭代器中这两个类型进行 typedef ，则反向迭代器可通过正向迭代器获取节点引用类型和节点指针类型。

```text
template<class T,class Ref,class Ptr>
struct __TreeIterator
{
	typedef Ref reference; // 节点指针引用
	typedef Ptr pointer; // 节点指针
}
```

## 5.红黑树封装后的代码

```text
#pragma once
#include<iostream>
using namespace std;

enum Color
{
	RED,
	BLACK,
};

template<class T>
struct RBTreeNode
{
	RBTreeNode<T>* _left;
	RBTreeNode<T>* _right;
	RBTreeNode<T>* _parent;

	// 存储数据的键值对
	T _data;
	// 存储节点颜色
	Color _col;

	RBTreeNode(const T& data)
		:_left(nullptr)
		,_right(nullptr)
		,_parent(nullptr)
		,_data(data)
		,_col(RED)
	{}
};

template<class T,class Ref,class Ptr>
struct __TreeIterator
{
	typedef Ref reference;
	typedef Ptr pointer;

	typedef RBTreeNode<T> Node;
	typedef __TreeIterator<T, Ref, Ptr> Self;

	Node* _node;

	__TreeIterator(Node* node)
		:_node(node)
	{}

	Ref operator*()
	{
		return _node->_data;
	}

	Ptr operator->()
	{
		return &(_node->_data);
	}

	bool operator!=(const Self& s)const
	{
		return _node != s._node;
	}

	bool operator==(const Self& s)const
	{
		return _node == s._node;
	}

	Self operator++()
	{
		if (_node->_right) // 当前节点的右子树不为空
		{
			// 找到右子树中的最左节点 - 即右子树中中序的第一个节点
			Node* left = _node->_right;
			while (left->_left)
			{
				left = left->_left;
			}
			_node = left;
		}
		else // 当前节点右子树为空
		{
			Node* cur = _node;
			Node* parent = cur->_parent;
			while (parent && parent->_right == cur)
			{
				cur = cur->_parent;
				parent = parent->_parent;
			}
			_node = parent;
		}
		return *this;
	}

	Self& operator--()
	{
		if (_node->_left)
		{
			// 左子树的最右节点
			Node* right = _node->_left;
			while (right->_right)
			{
				right = right->_right;
			}
			_node = right;
		}
		else
		{
			Node* cur = _node;
			Node* parent = cur->_parent;
			while (parent && cur == parent->_left)
			{
				cur = parent;
				parent = parent->_parent;
			}
			_node = parent;
		}
		return *this;
	}
};

template<class K,class T,class KeyOfT>
class RBTree
{
	typedef RBTreeNode<T> Node;
public:
	typedef __TreeIterator<T, T&, T*> iterator;
	typedef __TreeIterator<T, const T&, const T*> const_iterator;
	typedef ReverseIterator<iterator> reverse_iterator;

	iterator begin()
	{
		// 返回红黑树的最左节点
		Node* left = _root;
		while (left && left->_left)
		{
			left = left->_left;
		}
		return iterator(left);
	}

	iterator end()
	{
		return iterator(nullptr); // 用nullptr去标志红黑树的结束位置
	}

	reverse_iterator rbegin()
	{
		// 返回红黑树的最右节点
		Node* right = _root;
		while (right && right->_right)
		{
			right = right->_right;
		}
		return reverse_iterator(iterator(right));
	}

	reverse_iterator rend()
	{
		return reverse_iterator(iterator(nullptr));
	}

private:
	void RotateL(Node* parent)
	{
		Node* subR = parent->_right;
		Node* subRL = subR->_left;
		Node* parentParent = parent->_parent;

		// 让parent的右指针指向subRL,判断一下subRL是否为空
		parent->_right = subRL;
		if (subRL)
			subRL->_parent = parent;

		// subR的左指针链接parent
		subR->_left = parent;
		parent->_parent = subR;

		// parent为根的情况，更新根节点，让根节点指向空
		if (_root == parent)
		{
			_root = subR;
			_root->_parent = nullptr;
		}
		else //若parent为一棵子树，则链接与parentParent的关系
		{
			if (parentParent->_right == parent)
				parentParent->_right = subR;
			else
				parentParent->_left = subR;

			subR->_parent = parentParent;
		}
	}


	void RotateR(Node* parent)
	{
		Node* subL = parent->_left;
		Node* subLR = subL->_right;
		Node* parentParent = parent->_parent;

		// 将subLR链接到parent的左边，这里注意subLR可能为空的情况，需要判断一下
		parent->_left = subLR;
		if (subLR)
		{
			subLR->_parent = parent;
		}

		// 将parent这棵子树链接到subL的右指针
		subL->_right = parent;
		parent->_parent = subL;

		// 若parent为根，则更新新的根节点
		if (parent == _root)
		{
			_root = subL;
			_root->_parent = nullptr;
		}
		else //若parent为一棵子树，则链接与parentParent的关系
		{
			if (parentParent->_right == parent)
				parentParent->_right = subL;
			else
				parentParent->_left = subL;

			subL->_parent = parentParent;
		}
	}

	void _Destroy(Node* root)
	{
		if (root == nullptr)
			return;

		_Destroy(root->_left);
		_Destroy(root->_right);
		delete root;
	}
public:
	RBTree()
		:_root(nullptr)
	{}

	~RBTree()
	{
		_Destroy(_root);
		_root = nullptr;
	}

	iterator find(const K& key)
	{
		KeyOfT kot;
		Node* cur = _root;
		while (cur)
		{
			if (kot(cur->_data) < key)
				cur = cur->_right;
			else if (kot(cur->_data) > key)
				cur = cur->_left;
			else
				return iterator(cur);
		}
		return end();
	}

	pair<iterator, bool> insert(const T& data)
	{
		// 一开始插入新节点时树为空，则直接让新节点作为根节点
		if (_root == nullptr)
		{
			_root = new Node(data);
			_root->_col = BLACK;
			return make_pair(iterator(_root), true);
		}

		KeyOfT kot;
		Node* cur = _root;
		Node* parent = _root;
		while (cur)
		{
			if (kot(cur->_data) < kot(data))
			{
				parent = cur;
				cur = cur->_right;
			}
			else if (kot(cur->_data) > kot(data))
			{
				parent = cur;
				cur = cur->_left;
			}
			else
				// 待插入的节点已经存在，则返回该节点的指针，插入失败
				return make_pair(iterator(cur), false);
		}

		// 走到这里就已经找到了将要插入的位置
		Node* newnode = new Node(data);
		newnode->_col = RED;
		// 判断待插入节点与parent指向值的大小，将待插入节点插入到正确的位置
		if (kot(parent->_data) < kot(data))
		{c
			parent->_right = newnode;
			newnode->_parent = parent;
		}
		else
		{
			parent->_left = newnode;
			newnode->_parent = parent;
		}
		cur = newnode;

		// 若父亲存在且为红色就需要进行处理
		while (parent&& parent->_col == RED)
		{
			// 若父节点为红色，则祖父节点一定存在，不需要进行判断
			Node* grandfather = parent->_parent;
			if (parent == grandfather->_left)
			{
				Node* uncle = grandfather->_right;
				//情况1：uncle存在且为红
				if (uncle && uncle->_col == RED)
				{
					parent->_col = uncle->_col = BLACK;
					grandfather->_col = RED;

					// 继续向上进行处理
					cur = grandfather;
					parent = cur->_parent;
				}
				else // 情况2+3：uncle不存在或者uncle存在且为黑
				{
					// 情况2：右单旋
					if (cur == parent->_left)
					{
						RotateR(grandfather);
						grandfather->_col = RED;
						parent->_col = BLACK;
					}
					else // 左右双旋
					{
						RotateL(parent);
						RotateR(grandfather);
						cur->_col = BLACK;
						grandfather->_col = RED;
					}
					break;
				}
			}
			else // grandfather->_right == parent;
			{
				Node* uncle = grandfather->_left;
				// 情况1
				if (uncle&& uncle->_col == RED)
				{
					parent->_col = uncle->_col = BLACK;
					grandfather->_col = RED;

					// 继续向上进行处理
					cur = grandfather;
					parent = cur->_parent;
				}
				else // 情况2+3
				{
					if (cur == parent->_right)
					{
						RotateL(grandfather);
						parent->_col = BLACK;
						grandfather->_col = RED;
					}
					else // cur == parent->_left
					{
						RotateR(parent);
						RotateL(grandfather);
						cur->_col = BLACK;
						grandfather->_col = RED;
					}
					break;
				}
			}
		}

		// 将根节点的颜色处理为黑色
		_root->_col = BLACK;
		return make_pair(iterator(newnode), true);
	}

	bool erase(const K& key)
	{
		Node* parent = nullptr;
		Node* cur = _root;
		//用于标记实际的待删除结点及其父结点
		Node* deleteParent = nullptr;
		Node* deleteNode = nullptr;
		while (cur)
		{
			if (key < cur->_kv.first)
			{
				parent = cur;
				cur = cur->_left;
			}
			else if (key > cur->_kv.first)
			{
				parent = cur;
				cur = cur->_right;
			}
			else
			{
				if (cur->_left == nullptr) //待删除结点的左子树为空
				{
					if (cur == _root) //待删除结点是根结点
					{
						_root = _root->_right; //让根结点的右子树作为新的根结点
						if (_root)
						{
							_root->_parent = nullptr;
							_root->_col = BLACK; //根结点为黑色
						}
						delete cur; //删除原根结点
						return true;
					}
					else
					{
						deleteParent = parent; //标记实际删除结点的父结点
						deleteNode = cur; //标记实际删除的结点
					}
					break;
				}
				else if (cur->_right == nullptr) //待删除结点的右子树为空
				{
					if (cur == _root) //待删除结点是根结点
					{
						_root = _root->_left; //让根结点的左子树作为新的根结点
						if (_root)
						{
							_root->_parent = nullptr;
							_root->_col = BLACK; //根结点为黑色
						}
						delete cur; //删除原根结点
						return true;
					}
					else
					{
						deleteParent = parent; //标记实际删除结点的父结点
						deleteNode = cur; //标记实际删除的结点
					}
					break;
				}
				else //待删除结点的左右子树均不为空
				{
					//替换法删除
					//寻找待删除结点右子树当中key值最小的结点作为实际删除结点
					Node* minParent = cur;
					Node* minRight = cur->_right;
					while (minRight->_left)
					{
						minParent = minRight;
						minRight = minRight->_left;
					}
					cur->_kv.first = minRight->_kv.first;
					cur->_kv.second = minRight->_kv.second;
					deleteParent = minParent;
					deleteNode = minRight;
					break;
				}
			}
		}
		if (deleteNode == nullptr)
		{
			return false;
		}

		//记录待删除结点及其父结点
		Node* del = deleteNode;
		Node* delP = deleteParent;

		//调整红黑树
		if (deleteNode->_col == BLACK) //删除的是黑色结点
		{
			if (deleteNode->_left)
				deleteNode->_left->_col = BLACK;

			else if (deleteNode->_right) //待删除结点有一个红色的右孩子（不可能是黑色）
			{
				deleteNode->_right->_col = BLACK;
			}
			else //待删除结点的左右均为空
			{
				while (deleteNode != _root)
				{
					if (deleteNode == deleteParent->_left)
					{
						Node* brother = deleteParent->_right;
						//情况一：brother为红色
						if (brother->_col == RED)
						{
							deleteParent->_col = RED;
							brother->_col = BLACK;
							RotateL(deleteParent);

							brother = deleteParent->_right; //更新brother
						}
						//情况二：brother为黑色，且其左右孩子都是黑色结点或为空
						if (((brother->_left == nullptr) || (brother->_left->_col == BLACK))
							&& ((brother->_right == nullptr) || (brother->_right->_col == BLACK)))
						{
							brother->_col = RED;
							if (deleteParent->_col == RED)
							{
								deleteParent->_col = BLACK;
								break;
							}

							deleteNode = deleteParent;
							deleteParent = deleteNode->_parent;
						}
						else
						{
							//情况三：brother为黑色，且其左孩子是红色结点，右孩子是黑色结点或为空
							if ((brother->_right == nullptr) || (brother->_right->_col == BLACK))
							{
								brother->_left->_col = BLACK;
								brother->_col = RED;
								RotateR(brother);

								brother = deleteParent->_right; //更新brother
							}
							//情况四：brother为黑色，且其右孩子是红色结点
							brother->_col = deleteParent->_col;
							deleteParent->_col = BLACK;
							brother->_right->_col = BLACK;
							RotateL(deleteParent);
							break;
						}
					}
					else  //待删除结点是其父结点的左孩子
					{
						Node* brother = deleteParent->_left;
						//情况一：brother为红色
						if (brother->_col == RED)
						{
							deleteParent->_col = RED;
							brother->_col = BLACK;
							RotateR(deleteParent);
							//需要继续处理
							brother = deleteParent->_left;
						}
						//情况二：brother为黑色，且其左右孩子都是黑色结点或为空
						if (((brother->_left == nullptr) || (brother->_left->_col == BLACK))
							&& ((brother->_right == nullptr) || (brother->_right->_col == BLACK)))
						{
							brother->_col = RED;
							if (deleteParent->_col == RED)
							{
								deleteParent->_col = BLACK;
								break;
							}

							deleteNode = deleteParent;
							deleteParent = deleteNode->_parent;
						}
						else
						{
							//情况三：brother为黑色，且其右孩子是红色结点，左孩子是黑色结点或为空
							if ((brother->_left == nullptr) || (brother->_left->_col == BLACK))
							{
								brother->_right->_col = BLACK;
								brother->_col = RED;
								RotateL(brother);

								brother = deleteParent->_left;
							}
							//情况四：brother为黑色，且其左孩子是红色结点
							brother->_col = deleteParent->_col;
							deleteParent->_col = BLACK;
							brother->_left->_col = BLACK;
							RotateR(deleteParent);
							break;
						}
					}
				}
			}
		}
		//进行实际删除
		if (del->_left == nullptr)
		{
			if (del == delP->_left)
			{
				delP->_left = del->_right;
				if (del->_right)
					del->_right->_parent = delP;
			}
			else
			{
				delP->_right = del->_right;
				if (del->_right)
					del->_right->_parent = delP;
			}
		}
		else
		{
			if (del == delP->_left)
			{
				delP->_left = del->_left;
				if (del->_left)
					del->_left->_parent = delP;
			}
			else
			{
				delP->_right = del->_left;
				if (del->_left)
					del->_left->_parent = delP;
			}
		}
		delete del;
		return true;
	}

private:
	Node* _root;
};
```

## 6.map完整代码

```text
template<class K,class V>
class map
{
	struct MapKeyOfT
	{
		const K& operator()(const pair<const K, V>& kv)
		{
			return kv.first;
		}
	};

public:
	typedef typename RBTree<K, pair<const K, V>, MapKeyOfT>::iterator iterator;
	typedef typename RBTree<K, pair<const K, V>, MapKeyOfT>::reverse_iterator reverse_iterator;

	reverse_iterator rbegin()
	{
		return _t.rbegin();
	}

	reverse_iterator rend()
	{
		return _t.rend();
	}

	iterator begin()
	{
		return _t.begin();
	}

	iterator end()
	{
		return _t.end();
	}

	pair<iterator, bool> insert(const pair<const K, V>& kv)
	{
		return _t.insert(kv);
	}

	// []的运算符重载
	V& operator[](const K& key)
	{
		pair<iterator, bool> ret = insert(make_pair(key, V()));
		return ret.first->second;
	}

	void erase(const K& k)
	{
		return _t.erase();
	}

	iterator find(const K& k)
	{
		return _t.find();
	}
private:
	RBTree<K, pair<const K, V>, MapKeyOfT> _t;
};
```

## 7.set完整代码

```text
template<class K>
class set
{
	struct SetKeyOfT
	{
		const K& operator()(const K& key)
		{
			return key;
		}
	};
public:
	typedef typename RBTree<K, K, SetKeyOfT>::iterator iterator;
	typedef typename RBTree<K, K, SetKeyOfT>::reverse_iterator reverse_iterator;

	iterator begin()
	{
		return _t.begin();
	}

	iterator end()
	{
		return _t.end();
	}

	reverse_iterator rbegin()
	{
		return _t.rbegin();
	}

	reverse_iterator rend()
	{
		return _t.rend();
	}

	pair<iterator, bool> insert(const K& k)
	{
		return _t.insert(k);
	}

	void erase(const K& k)
	{
		return _t.erase(k);
	}

	iterator find(const K& k)
	{
		return _t.find(k);
	}

private:
	RBTree<K, K, SetKeyOfT> _t;
};
```

**说明：**我们对 map 和 set 进行模拟实现只是为了让我们更加了解它们的框架和红黑树的底层实现原理。

原文地址：https://zhuanlan.zhihu.com/p/579730274

作者：linux

# 【NO.71】linux操作系统中线程是怎么切换的（用户态的内部切换）

多线程，会减少内核态，用户态的切换，从而提高性能。

## 1.简述

为什么要有线程，大概是这样的？

因为一个进程，要进行多个指令，如果访问地址，需要有地址，要回到映射表中。

执行不下去了，进程要切换，那么一个执行指令序列，要切换到另外一个执行指令序列，

比如A函数 从b函数执行， 那么我们只切换指令，不切换映射表，那么就是线程了。

所以，期望是操作系统减少切换映射表，避免了进程切换的代价（切换映射表比较消耗资源）

大概，面试打到这里，很浅可能也够了。

但是求知的角度，一定要好好学习一下。

**（tips：进程切换其实是一样的。只不过从pcb变成了tcb）**

## 2.用户级线程-user threads

![img](https://pic1.zhimg.com/80/v2-c0475a2ae93b625169c271f5c5fac40c_720w.webp)

![img](https://pic3.zhimg.com/80/v2-f0b71a689bb5b614dc197f4abb6dc9c2_720w.webp)

什么是线程：

一个执行指令序列，切换到另外一个指令序列，只切指令。

映射表对应的是资源，也就是内存。

我们期望，资源和指令分开，资源不变，内存不用跟着切，多个指令多个程序交替进行，切起来还快。

并发的特点，还保证了切换特别快。

在一个资源下面，启动了多个轻巧的指令序列，可以来回切，也是程序执行起来的，就是函数呗~Thread

线程保留并发有点，还不用切换消耗特别大。

但是依然要一段程序切换到另一端程序。（线程切换）

**进程切换：包括指令切换，和内存的切换**。



## 3.指令是怎么切换的？（线程的切换）

我们要切换线程，并发，也就是交替执行，cpu的利用率才会上去。

如果读写磁盘，我们就释放cpu去。

![img](https://pic1.zhimg.com/80/v2-846455226351c296f0c87ec189e05e20_720w.webp)

yield的目的就是能切换出去，也能切换回来。

注意这边，跟接下来要讲到的细节指令有关系。

第一遍我都看懵了，我们期望就是下图，100的指令，切到300，让它去show，之后我们再回到GetData函数，进行往下走。

![img](https://pic2.zhimg.com/80/v2-b073b43f8fd7bc91160b87fb93bae795_720w.webp)

![img](https://pic2.zhimg.com/80/v2-9ee06221f72c848164ccfdc98ad9d76d_720w.webp)

（注意上图： 主要是两个执行序列，一个栈，但是这样有问题的，后面会说 ）

执行的切换，操作系统的核心！！

A到B执行的话，我们需要跳到b执行，还需要跳回来的，

所以需要把函数返回的地址压栈，用于跳回A方法！

当b执行的时候，yield是一个用户函数，我们自己定义的。

![img](https://pic4.zhimg.com/80/v2-d4cbf3ffd15e834e49f1c6e639b18d33_720w.webp)

所以我们需要跳到300执行，304压栈，继续往下执行跳转到了D，执行yield，把404压栈。

我们下一个指令序列的话，会进行pc指针，因为400处的yield执行完之后，要回到204

我觉得因为这边Dyield，让出了cpu，所以回到了200；

右大括号} 这个会有汇编指令，弹栈，此时栈顶的地址是404，B函数应该是返回到104处，而不是404处；

这样就会出现问题，

因为ret 结束要返回一个线程内部的函数调用，只能在一个线程内部绕来绕去，

所以应该在左边的框框折腾，为什么会跑到右边去呢，因为两个线程共用了一个栈，

ret不允许到另一个栈里面去，一个函数调用是在一个指令序列内部发生的事，所以每一个指令序列，函数调用应该用自己的栈。所以应该把共用的栈拆开来。

总结：上部分 就是用yield ，进行跳转的手段，然后强行让204之后的返回结果，绕到了另外一个指令序列里面去了，也就是到404弹出栈了，但是这样是不对滴。

那么两个栈：

![img](https://pic1.zhimg.com/80/v2-d4042bc8f966a188d6a2d23997b3c284_720w.webp)

所以栈是这样滴：

![img](https://pic3.zhimg.com/80/v2-5f3df287b39a30cc9b689571509e049a_720w.webp)

两个tcb对应两个栈。

所以yield，就会栈切回去，所以栈要存放起来，栈的指针，要存放在tcb，全局数据结构

将来切回去，要从tcb找到指针，tcb是cpu里面的寄存器，有点像大脑里面寄了一下，存放了一下地址。

（tips：进程切换其实是一样的。只不过从pcb变成了tcb）

用户级线程，也可以实现切换，由用户主动做的。（完成了一个栈到两个栈，支持了线程的切换，没有进入到内核！）

![img](https://pic3.zhimg.com/80/v2-163aaa9d9377b3eae5f83350cd135be2_720w.webp)

注意这个东西，大概就是thread 创建的一个过程，

过程：（用户自己的线程，在用户态上来回切来切去，操作系统感知不到它的存在。）

\1. 申请内存做tcb

2.申请内存作为栈 图上是1000

\3. 内存的起止地址放进去

\4. 栈和tcb关联

所以切换的时候首先找到tcb 找到esp，取出值，赋值给真的esp，所以这样弹栈就ok了~

问题：要是一个线程卡了，那么久进程阻塞了。内核需要切换到别的进程，那么这时候，

假如浏览器加载东西的时候，等待网卡IO ，一旦一个用户级的线程卡了，那么就切换到了别的进程，所以有的时候浏览器，切换到了别的标签了，那段程序就不知行了，所以浏览器可能就一块一块的。如果卡了，没有别的进程，那么cpu就会空转了。

核心级的话，需要进入到内核，tcb在内核里面。如果获取数据在内核阻塞，那么缓冲区已经下载了一部分数据，那部分就可以进行执行。

内核级线程就是schedule！调度的特性！内核接管线程，我要调度。

听起来还蛮酷的！

## 4.cpu中的MMU的作用

虚拟内存与物理内存之间的映射

用户空间映射到物理内存是独立的，提高安全性

修改内存访问级别 （0是最高级）

MMU是个硬件，每当cpu访问一个地址的时候，MMU从内存里面查table，把cpu想访问的那个虚拟地址转换成物理地址。因为MMU每次查table都要读内存，比较慢，就在内存和MMU之间弄了个缓存，这个缓存就是tlb，里面存着MMU最近访问过的table的内容，如果下次还访问table的这些项的话，就不用读内存里面的table，而是从tlb里面读，这样比较快。

TLB - translation lookaside buffer

快表，直译为旁路快表缓冲，也可以理解为页表缓冲，地址变换高速缓存。

其实就是个缓存；

## 5.TCB是什么？

**TCB(transmission control block)传输控制块**

**可以认为是寄存器吧~如果有不对的可以指正。**

当任务重新得到CPU使用权时，TCB能确保任务从被中断的点丝毫不差地继续执行

具体的就不细细研究了

原文地址：https://zhuanlan.zhihu.com/p/576178810

作者：linux

# 【NO.72】Linux下系统 I/O 性能分析的套路

## 1.如何快速分析定位 I/O 性能问题

### 1.1  文件系统 I/O性能指标

首先，想到是存储空间的使用情况，包括容量、使用量、以及剩余空间等。我们通常也称这些为磁盘空间的用量，但是这只是文件系统向外展示的空间使用，而非在磁盘空间的真实用量，因为文件系统的元数据也会占用磁盘空间。而且，如果你配置了RAID，从文件系统看到的使用量跟实际磁盘的占用空间，也会因为RAID级别不同而不一样。

除了数据本身的存储空间，还有一个容易忽略的是索引节点的使用情况，包括容量、使用量、剩余量。如果文件系统小文件数过多，可能会碰到索引节点容量已满的问题。

其次，缓存使用情况，包括页缓存、索引节点缓存、目录项缓存以及各个具体文件系统的缓存。通过使用内存，来临时缓存文件数据或者文件系统元数据，从而减少磁盘访问次数。·

最后，文件 I/O的性能指标，包括IOPS（r/s、w/s）、响应时间（延迟）、吞吐量（B/s）等。考察这类指标时，还要结合实际文件读写情况，文件大小、数量、I/O类型等，综合分析文件 I/O 的性能。

### 1.2  磁盘 I/O性能指标

磁盘 I/O的性能指标，主要由四个核心指标：使用率、IOPS、响应时间、吞吐量，还有一个前面提到过，缓冲区。

考察这些指标时，一定要注意综合 I/O的具体场景来分析，比如读写类型（顺序读写还是随机读写）、读写比例、读写大小、存储类型（有无RAID、RAID级别、本地存储还是网络存储）等。

不过考察这些指标时，有一个大忌，就是把不同场景的 I/O指标拿过来作对比。

![img](https://pic3.zhimg.com/80/v2-2ccf624bca12533c0e2df0d10c69807e_720w.webp)

### **1.3 性能工具**

一类：df、top、iostat、pidstat；

二类：/proc/meminfo、/proc/slabinfo、slabtop；

三类：strace、lsof、filetop、opensnoop

### **1.4 性能指标及性能工具之间的关系**

![img](https://pic2.zhimg.com/80/v2-13737e2711c3999167399f9bd876be09_720w.webp)

### 1.5 如何迅速分析 I/O性能瓶颈

简单来说，就是找关联。多种性能指标间，都是存在一定的关联性。想弄清楚指标之间的关联性，就要知晓各种指标的工作原理。出现性能问题，基本的分析思路是：

先用 iostat发现磁盘 I/O的性能瓶颈；

再借助 pidstat，定位导致性能瓶颈的进程；

随后分析进程 I/O的行为；

最后，结合应用程序的原理，分析这些 I/O的来源。

![img](https://pic3.zhimg.com/80/v2-44a1f3599d95380928a8088b6f59ceb2_720w.webp)

图中列出最常用的几个文件系统和磁盘 I/O的性能分析工具，及相应的分析流程。

## 2.磁盘 I/O性能优化的几个思路

### 2.1 I/O基准测试

在优化之前，我们要清楚 I/O性能优化的目标是什么？也就是说，我们观察的这些 I/O指标（IOPS、吞吐量、响应时间等），要达到多少才合适？为了更客观合理地评估优化效果，首先应该对磁盘和文件系统进行基准测试，得到它们的极限性能。

fio（Flexible I/O Tester），它是常用的文件系统和磁盘 I/O的性能基准测试工具。它提供了大量的可定制化的选项，可以用来测试，裸盘或者文件系统在各种场景下的 I/O性能，包括不同块大小、不同 I/O引擎以及是否使用缓存等场景。

fio的选项非常多，这里介绍几个常用的：

```text
# 随机读
fio -name=randread -direct=1 -iodepth=64 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb
# 随机写
fio -name=randwrite -direct=1 -iodepth=64 -rw=randwrite -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb
# 顺序读
fio -name=read -direct=1 -iodepth=64 -rw=read -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb
# 顺序写
fio -name=write -direct=1 -iodepth=64 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sdb
```

重点解释几个参数：

- direct，是否跳过系统缓存，iodepth1 是跳过。
- iodepth，使用异步 I/O（AIO）时，同时发出的请求上限。
- rw，I/O模式，顺序读 / 写、随机读 / 写。
- ioengine，I/O引擎，支持同步（sync）、异步（libaio）、内存映射（mmap）、网络等各种 I/O引擎。
- bs，I/O大小。 4k，默认值。
- filename，文件路径，可以是磁盘路径，也可以是文件路径。不过要注意，用磁盘路径测试写，会破坏这个磁盘的文件系统，所以测试前，要注意备份。

下面展示， fio测试顺序读的示例：

```text
read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64
fio-3.1
Starting 1 process
Jobs: 1 (f=1): [R(1)][100.0%][r=16.7MiB/s,w=0KiB/s][r=4280,w=0 IOPS][eta 00m:00s]
read: (groupid=0, jobs=1): err= 0: pid=17966: Sun Dec 30 08:31:48 2018
   read: IOPS=4257, BW=16.6MiB/s (17.4MB/s)(1024MiB/61568msec)
    slat (usec): min=2, max=2566, avg= 4.29, stdev=21.76
    clat (usec): min=228, max=407360, avg=15024.30, stdev=20524.39
     lat (usec): min=243, max=407363, avg=15029.12, stdev=20524.26
    clat percentiles (usec):
     |  1.00th=[   498],  5.00th=[  1020], 10.00th=[  1319], 20.00th=[  1713],
     | 30.00th=[  1991], 40.00th=[  2212], 50.00th=[  2540], 60.00th=[  2933],
     | 70.00th=[  5407], 80.00th=[ 44303], 90.00th=[ 45351], 95.00th=[ 45876],
     | 99.00th=[ 46924], 99.50th=[ 46924], 99.90th=[ 48497], 99.95th=[ 49021],
     | 99.99th=[404751]
   bw (  KiB/s): min= 8208, max=18832, per=99.85%, avg=17005.35, stdev=998.94, samples=123
   iops        : min= 2052, max= 4708, avg=4251.30, stdev=249.74, samples=123
  lat (usec)   : 250=0.01%, 500=1.03%, 750=1.69%, 1000=2.07%
  lat (msec)   : 2=25.64%, 4=37.58%, 10=2.08%, 20=0.02%, 50=29.86%
  lat (msec)   : 100=0.01%, 500=0.02%
  cpu          : usr=1.02%, sys=2.97%, ctx=33312, majf=0, minf=75
  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, >=64=100.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, >=64=0.0%
     issued rwt: total=262144,0,0, short=0,0,0, dropped=0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=64
 
Run status group 0 (all jobs):
   READ: bw=16.6MiB/s (17.4MB/s), 16.6MiB/s-16.6MiB/s (17.4MB/s-17.4MB/s), io=1024MiB (1074MB), run=61568-61568msec
 
Disk stats (read/write):
  sdb: ios=261897/0, merge=0/0, ticks=3912108/0, in_queue=3474336, util=90.09% 
```

这个示例中，重点关注几行，slat、clat、lat，以及 bw和 iops。前三者，都是指 I/O延迟，但是有不同之处：

slat，是指从 I/O提交到实际执行 I/O的时长；

clat，是指从 I/O提交到 I/O完成的时长；

lat，是指从 fio创建 I/O 到 I/O完成的时长。

这里需要注意的是，对同步 I/O来说，提交和完成是一个动作，slat就是 I/O完成的时间，clat是0；使用异步 I/O时，lat 约等于 slat + clat。

再来看bw，他表示吞吐量，上面的输出中，平均吞吐量是16MB（17005/1024）。

最后的IOPS，其实是每秒 I/O的次数，上面输出的平均 IOPS是 4250.

通常情况下，应用程序的IO 读写是并行的，每次的 I/O大小也不相同。所以上面的几个场景并不能精确模拟应用程序的 I/O模式。幸运的是，fio支持 I/O 的重放，需要先用 blktrace，记录磁盘设备的 I/O访问情况，再使用 fio，重放 blktrace的记录。

```text
# 使用blktrace跟踪磁盘I/O，注意指定应用程序正在操作的磁盘
$ blktrace /dev/sdb
# 查看blktrace记录的结果
# ls
sdb.blktrace.0  sdb.blktrace.1
# 将结果转化为二进制文件
$ blkparse sdb -d sdb.bin
# 使用fio重放日志
$ fio --name=replay --filename=/dev/sdb --direct=1 --read_iolog=sdb.bin
```



### **2.2 I/O性能优化思路**

**应用程序优化**

应用程序处于 I/O栈的最上端，可以通过系统调用，来调整 I/O模式（顺序还是随机、同步还是异步），同时也是数据的最终来源。下面总结了几个方面来优化应用程序性能：

第一，可以用追加写代替随机写，减少寻址开销，加快 I/O写的速度。

第二，借助缓存 I/O，充分利用系统缓存，降低实际 I/O的次数。

第三，在应用程序内部构建自己缓存，或者使用Redis这种的外部缓存系统。这样不仅可以在内部控制缓存的数据和生命周期，而且降低其他应用程序使用缓存对自身的影响。比如，C标准库，提供的fopen、fread等库函数，都会利用标准库缓存，减少磁盘的操作。而如果直接使用open、read等系统调用时，就只能利用操作系统的页缓存和缓冲区等。

第四，在需要频繁读写同一块磁盘空间时，可以使用 mmap 代替 read/write，减少内存的拷贝次数。

第五，在需要同步写的场景中，尽量将写请求合并，而不是让每个请求都同步写磁盘，即可用fsync() 代替 O_SYNC。

第六，在多个应用程序共享相同磁盘时，为了保证 I/O不被某个应用完全占用，推荐使用 cgroups 的 I/O子系统，来限制进程/进程组的 IOPS 以及吞吐量。

最后，在使用CFQ 调度器时，可以用 ionice来调整进程的 I/O调度优先级，特别是提高核心应用的 I/O优先级，他支持三个优先级类：Idle、Best-effort 和 Realtime。其中，后两者还支持 0-7的级别，数值越小，优先级越高。

**文件系统优化**

应用程序在访问普通文件时，是通过文件系统间接负责，文件在磁盘中的读写。所以跟文件系统相关的也有很多优化方式。

第一，可以根据实际负载场景的不同，选择合适的文件系统。比如，Ubuntu默认使用ext4，Centos默认使用 xfs。相比于ext4，xfs支持更大的磁盘分区和更大的文件数量。xfs支持大于 16TB的磁盘，但它的缺点在于无法收缩，而ext4可以。

第二，在选好文件系统后，可以优化文件系统得配置选项。包括文件系统的特性（如 ext_attr、dir_index）、日志模式（如 journal、ordered、writeback等）、挂载选项（如 noatime）等等。比如在使用 tune2fs这个工具，可以调整文件系统的特性，也常用来查看文件系统超级块的内容。而通过 /etc/fstab，或者mount，来调整文件系统的日志模式和挂载选项等。

第三，优化文件系统的缓存。比如，可以优化 pdflush的脏页刷新频率（设置dirty_expire_centisecs 和 dirty_writeback_centisecs）以及脏页限额（调整 dirty_background_ratio 和 dirty_ratio）。再如，还可以优化内核回收目录项缓存和索引节点缓存的倾向，及调整 vfs_cache_pressure（/proc/sys/vm/vfs_cache_pressure，默认值100）,数值越大，表示越容易回收。

最后，在不需要持久化时，可以用内存文件系统 tmpfs 以获得更好的 I/O性能。tmpfs直接把数据保存在内存中，而不是磁盘中。比如 /dev/shm，就是大多数Linux默认配置的一个内存文件系统，它的大小默认为系统总内存的一半。

**磁盘优化**

数据的持久化，最终要落到物理磁盘上，同时磁盘也是整个 I/O栈的最底层。从磁盘角度出发，也有很多优化方法：

第一，最简单的就是SSD代替 HDD。

第二，使用 RAID把多块磁盘组合成一个逻辑磁盘，构成冗余独立的磁盘阵列，即可以提高数据的可靠性，也可以提升数据的访问性能。

第三，针对磁盘和应用程序的 I/O模式的特征，可选择最适合的 I/O调度算法。

第四，可以针对应用程序的数据，进行磁盘级别的隔离。比如，可以为日志、数据库等 I/O压力比较重的应用，配置单独的磁盘。

第五，在顺序读比较多的场景中，可以增大磁盘的预读数据，可以通过两种方法，调整 /dev/sdb的预读大小。一种，调整内核选项，/sys/block/sdb/queue/read_ahead_kb，默认大小128KB。另一种，blockdev工具，比如，blockdev --setra 8192 /dev/sdb ，注意这里的单位是 512B，所以它的数值总是 read_ahead_kb的两倍。

第六，优化内核块设备 I/O的选项。比如，调整磁盘队列的长度，/sys/block/sdb/queue/nr_requests，适当增大队列长度，可以增大磁盘的吞吐量，当然也会增大 I/O延迟。

最后，磁盘本身的硬件错误，也会导致 I/O性能急剧下降。比如，查看 dmesg中是否有硬件 I/O故障的日志，还可以使用badblocks、smartctl等工具，检测磁盘的硬件问题，或用 e2fsck等来检测文件系统错误。如果发现问题，可使用fsck 等工具修复。

原文地址：https://zhuanlan.zhihu.com/p/572855548

作者：linux

# 【NO.73】linux性能优化之网络篇

网络通讯的链路为：机器A-----》路由器----〉机器B

如果我们站在本机机器作为参考物的话，应该拆分成下面三个阶段：

1.消息入口流量部分的处理流程

2.流量在本机上面的处理流程

3.流量出口流量部分的处理流程

在上面的这三个阶段，分别涉及到了不同的基础知识、工具和命令来进行诊断，笔者本篇文章按照下面的内容来整理：

1.基础知识

2.常用工具和命令

3.测试工具

## **1.基础知识**

**衡量网络性能的主要指标为：**

**带宽：**表示链路的最大传输速率，单位是 b/s（比特 / 秒），常用的带宽有 1000M、10G、40G、100G 等。

**吞吐量：**表示没有丢包时的最大数据传输速率，单位通常为 b/s （比特 / 秒）或者 B/s（字节 / 秒），吞吐量受带宽的限制，吞吐量 / 带宽也就是该网络链路的使用率。

**延时：**表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟，表示建立连接需要的时间（比如 TCP 握手延时），或者一个数据包往返所需时间（比如 RTT）。

通常，我们更常用的是双向的往返通信延迟，比如 ping 测试的结果，就是往返延时 RTT（Round-Trip Time）。除了网络延迟外，另一个常用的指标是应用程序延迟，它是指，从应用程序接收到请求，再到发回响应，全程所用的时间。应用程序延迟也指的是往返延迟，是网络数据传输时间加上数据处理时间的和。

**PPS：**Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速率。PPS 通常用来评估网络的转发能力，而基于 Linux 服务器的转发，很容易受到网络包大小的影响（交换机通常不会受到太大影响，即交换机可以线性转发）。PPS，是以网络包为单位的网络传输速率，通常用在需要大量转发的场景中。对 TCP 或者 Web 服务来说，更多会用并发连接数和每秒请求数（QPS，Query per Second）等指标，它们更能反应实际应用程序的性能。

网络的可用性（网络能否正常通信）、并发连接数（TCP 连接数量）、丢包率（丢包百分比）、重传率（重新传输的网络包比例）等也是常用的性能指标。

（**备注**：网络接口层和网络层，它们主要负责网络包的封装、寻址、路由以及发送和接收。在这两个网络协议层中，每秒可处理的网络包数 PPS，就是最重要的性能指标，特别是 64B 小包的处理能力，值得我们特别关注。）

### **1.1 本机内部的流量处理知识整理：**

Linux 内核中的网络栈，类似于 TCP/IP 的四层结构，如下所示：

![img](https://pic3.zhimg.com/80/v2-9455f474f20e70478fd7b16a2de37a76_720w.webp)

对于操作系统中的数据包收发流程，如下所示：

![img](https://pic2.zhimg.com/80/v2-61eddcc318ca695b6239ac19f370bfc5_720w.webp)

### **1.2 本机入口和出口流量的处理知识点整理：**

1）对于机器的入口流量来说，主要涉及到的知识便是C10K、C1000K、C10M的场景处理。

C10K 、C1000K、 C10M的首字母 C 是 Client 的缩写，C10K 是单机同时处理 1 万个请求（并发连接 1 万）的问题，C1000K 是单机支持处理 100 万个请求（并发连接 100 万）的问题，C10M是1000万个请求（并发连接1000万）的问题。

**1>I/O 模型的优化**，特别是Linux 2.6 中引入的 epoll完美解决了 C10K 的问题。

I/O 模型相关知识略

**2>从 C10K 到 C100K，我们只需要增加系统的物理资源**，就可以满足要求。

**3>**从 C100K 到 C1000K ，光增加物理资源就不够了。这时，**就要对系统的软硬件进行统一优化**，从硬件的中断处理，到网络协议栈的文件描述符数量、连接状态跟踪、缓存队列，再到应用程序的工作模型等的整个网络链路，都需要深入优化。**4>**要实现 C10M，就不是增加物理资源、调优内核和应用程序可以解决的问题了，这时内核中冗长的网络协议栈就成了最大的负担。需要用 **XDP 方式**，在内核协议栈之前，先处理网络包；或基于 **DPDK** ，直接跳过网络协议栈，在用户空间通过轮询的方式处理。

**DPDK：**是用户态网络的标准，它跳过内核协议栈，直接由用户态进程通过轮询的方式，来处理网络接收。对于C10M场景，基本上每时每刻都有新的网络包需要处理，轮询的优势就很明显了。

```text
1. 在 PPS 非常高的场景中，查询时间比实际工作时间少了很多，绝大部分时间都在处理网络包；
2. 跳过内核协议栈后，就省去了繁杂的硬中断、软中断再到 Linux 网络协议栈逐层处理的过程，
应用程序可以针对应用的实际场景，有针对性地优化网络包的处理逻辑，而不需要关注所有的细节。
3. DPDK 还通过大页、CPU 绑定、内存对齐、流水线并发等多种机制，优化网络包的处理效率。
```

![img](https://pic1.zhimg.com/80/v2-d6d8ed06a38f8e24c7892668d74bf114_720w.webp)

（**备注**：DPDK 是目前最主流的高性能网络方案，不过，这需要能支持 DPDK 的网卡配合使用。）

**XDP**（eXpress Data Path）：则是 Linux 内核提供的一种高性能网络数据路径，它允许网络包，在进入内核协议栈之前，就进行处理，也可以带来更高的性能，XDP 底层都是基于 Linux 内核的 eBPF 机制实现的。

![img](https://pic3.zhimg.com/80/v2-576738195188839d373f5cacd2f0f382_720w.webp)

（**备注**：XDP 对内核的要求比较高，需要的是 Linux 4.8 以上版本，并且它也不提供缓存队列，基于 XDP 的应用程序通常是专用的网络应用，常见的有 IDS（入侵检测系统）、DDoS 防御、 cilium 容器网络插件等。）



**2) iptables 与 NAT：**

NAT 技术可以重写 IP 数据包的源 IP 或者目的 IP，被普遍地用来解决公网 IP 地址短缺的问题。它的主要原理就是，网络中的多台主机，通过共享同一个公网 IP 地址，来访问外网资源，同时，由于 NAT 屏蔽了内网网络，自然也就为局域网中的机器提供了安全隔离。

NAT 的主要目的，是实现地址转换，根据实现方式的不同，NAT 可以分为三类：

```text
静态 NAT，即内网 IP 与公网 IP 是一对一的永久映射关系；
动态 NAT，即内网 IP 从公网 IP 池中，动态选择一个进行映射；
网络地址端口转换 NAPT（Network Address and Port Translation），即把内网 IP 映射到公网 IP 的不同端口上，让多个内网 IP 可以共享同一个公网 IP 地址。
```

**iptables**:
Linux 内核提供的 Netfilter 框架，允许对网络数据包进行修改（比如 NAT）和过滤（比如防火墙）。在这个基础上，iptables、ip6tables、ebtables 等工具，又提供了更易用的命令行接口，以便系统管理员配置和管理 NAT、防火墙的规则，其中，iptables 就是最常用的一种配置工具。

### **1.3 非本机相关的网络知识：**

DNS（Domain Name System），即域名系统，是互联网中最基础的一项服务，主要提供域名和 IP 地址之间映射关系的查询服务。

## **2.工具和命令介绍：**

### 2.1 查看网络配置 ifconfig、ip

```text
# RUNNING,LOWER_UP: 表示物理网络是连通的
# RX:receive,表示的是接收数据，从开启到现在接收封包的情况，是下行流量（Downlink）
# TX:Transmit,表示的是发送数据，从开启到现在发送封包的情况，是上行流量（Uplink）。
# errors 表示发生错误的数据包数，比如校验错误、帧同步错误等；
# dropped 表示丢弃的数据包数，即数据包已经收到了 Ring Buffer，但因为内存不足等原因丢包；
# overruns 表示超限数据包数，即网络 I/O 速度过快，导致 Ring Buffer 中的数据包来不及处理（队列满）而导致的丢包；
# carrier 表示发生 carrirer 错误的数据包数，比如双工模式不匹配、物理电缆出现问题等；
# collisions 表示碰撞数据包数。
$ ifconfig eth0
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500
      inet 10.240.0.30 netmask 255.240.0.0 broadcast 10.255.255.255
      inet6 fe80::20d:3aff:fe07:cf2a prefixlen 64 scopeid 0x20<link>
      ether 78:0d:3a:07:cf:3a txqueuelen 1000 (Ethernet)
      RX packets 40809142 bytes 9542369803 (9.5 GB)
      RX errors 0 dropped 0 overruns 0 frame 0
      TX packets 32637401 bytes 4815573306 (4.8 GB)
      TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0

$ ip -s addr show dev eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
  link/ether 78:0d:3a:07:cf:3a brd ff:ff:ff:ff:ff:ff
  inet 10.240.0.30/12 brd 10.255.255.255 scope global eth0
      valid_lft forever preferred_lft forever
  inet6 fe80::20d:3aff:fe07:cf2a/64 scope link
      valid_lft forever preferred_lft forever
  RX: bytes packets errors dropped overrun mcast
   9542432350 40809397 0       0       0       193
  TX: bytes packets errors dropped carrier collsns
   4815625265 32637658 0       0       0       0
```

### 2.2 查看套接字、网络栈、网络接口以及路由表的信息 netstat、ss

1）当套接字处于连接状态（Established）：

Recv-Q 表示套接字缓冲还没有被应用程序取走的字节数（即接收队列长度）。

Send-Q 表示还没有被远端主机确认的字节数（即发送队列长度）。

2）当套接字处于监听状态（Listening）：

Recv-Q 表示全连接队列的长度。

Send-Q 表示全连接队列的最大长度。

```text
# head -n 3 表示只显示前面3行
# -l 表示只显示监听套接字
# -n 表示显示数字地址和端口(而不是名字)
# -p 表示显示进程信息
$ netstat -nlp | head -n 3
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      840/systemd-resolve

# -l 表示只显示监听套接字
# -t 表示只显示 TCP 套接字
# -n 表示显示数字地址和端口(而不是名字)
# -p 表示显示进程信息
$ ss -ltnp | head -n 3
State    Recv-Q    Send-Q        Local Address:Port        Peer Address:Port
LISTEN   0         128           127.0.0.53%lo:53               0.0.0.0:*        users:(("systemd-resolve",pid=840,fd=13))
LISTEN   0         128                 0.0.0.0:22    
           0.0.0.0:*        users:(("sshd",pid=1459,fd=3))
           
 # 协议栈统计信息
$ netstat -s
#主动连接、被动连接、失败重试、发送和接收的分段数量等各种信息
...
Tcp:
    3244906 active connection openings
    23143 passive connection openings
    115732 failed connection attempts
    2964 connection resets received
    1 connections established
    13025010 segments received
    17606946 segments sent out
    44438 segments retransmitted
    42 bad segments received
    5315 resets sent
    InCsumErrors: 42
...

$ ss -s
Total: 186 (kernel 1446)
TCP:   4 (estab 1, closed 0, orphaned 0, synrecv 0, timewait 0/0), ports 0

Transport Total     IP        IPv6
*    1446      -         -
RAW    2         1         1
UDP    2         2         0
TCP    4         3         1
...
```

### 2.3 查看系统当前的网络吞吐量和 PPS：sar

```text
# 数字1表示每隔1秒输出一组数据
$ sar -n DEV 1 // -n 参数就可以查看网络的统计信息，DEV是接口
#rxpck/s 和 txpck/s 分别是接收和发送的 PPS，单位为包 / 秒。
# rxkB/s 和 txkB/s 分别是接收和发送的吞吐量，单位是 KB/ 秒。
# rxcmp/s 和 txcmp/s 分别是接收和发送的压缩数据包数，单位是包 / 秒。
# %ifutil 是网络接口的使用率，即半双工模式下为 (rxkB/s+txkB/s)/Bandwidth，而全双工模式下为 max(rxkB/s, txkB/s)/Bandwidth。
Linux 4.15.0-1035 (ubuntu)   01/06/19   _x86_64_  (2 CPU)

13:21:40        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
13:21:41         eth0     18.00     20.00      5.79      4.25      0.00      0.00      0.00      0.00
13:21:41      docker0      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
13:21:41           lo      0.00      0.00      0.00      0.00 
```

Bandwidth 可以用 ethtool 来查询，它的单位通常是 Gb/s 或者 Mb/s，这里小写字母 b ，表示比特而不是字节。

```text
$ ethtool eth0 | grep Speed
  Speed: 1000Mb/s
```

### 2.4 连通性和延迟 ping

```text
# -c3表示发送三次ICMP包后停止
# 第一部分，是每个 ICMP 请求的信息，包括 ICMP 序列号（icmp_seq）、TTL（生存时间，或者跳数）以及往返延时。
$ ping -c3 114.114.114.114
PING 114.114.114.114 (114.114.114.114) 56(84) bytes of data.
64 bytes from 114.114.114.114: icmp_seq=1 ttl=54 time=244 ms
64 bytes from 114.114.114.114: icmp_seq=2 ttl=47 time=244 ms
64 bytes from 114.114.114.114: icmp_seq=3 ttl=67 time=244 ms
# 第二部分，则是三次 ICMP 请求的汇总。
--- 114.114.114.114 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 2001ms
rtt min/avg/max/mdev = 244.023/244.070/244.105/0.034 ms
```

### 2.5 抓包工具tcpdump

安装：

```text
# Ubuntu
apt-get install tcpdump wireshark
# CentOS
yum install -y tcpdump wireshark
```

使用介绍：

```text
# -nn ，表示不解析抓包中的域名（即不反向解析）、协议以及端口号。
# udp port 53 ，表示只显示 UDP 协议的端口号（包括源端口和目的端口）为 53 的包。
# host 35.190.27.188 ，表示只显示 IP 地址（包括源地址和目的地址）为 35.190.27.188 的包。
# 这两个过滤条件中间的“ or ”，表示或的关系，也就是说，只要满足上面两个条件中的任一个，就可以展示出来。
# -w ping.pcap 表示输出到本地的ping.pcap存储起来
$ tcpdump -nn udp port 53 or host 35.190.27.188 -w ping.pcap
```

下载到本地机器，使用wireshark进行分析

```text
$ scp host-ip/path/ping.pcap .
```

### 2.6 路由相关工具：mtr、route、traceroute

```text
# --tcp表示使用TCP协议，-p表示端口号，-n表示不对结果中的IP地址执行反向域名解析
$ traceroute --tcp -p 80 -n baidu.com
traceroute to baidu.com (123.125.115.110), 30 hops max, 60 byte packets
 1  * * *
 2  * * *
 3  * * *
 4  * * *
 5  * * *
 6  * * *
 7  * * *
 8  * * *
 9  * * *
10  * * *
11  * * *
12  * * *
13  * * *
14  123.125.115.110  20.684 ms *  20.798 ms
```

## **3.压测工具介绍**

### **3.1 转发性能：hping3**

网络接口层和网络层，它们主要负责网络包的封装、寻址、路由以及发送和接收，在这两个网络协议层中，每秒可处理的网络包数 PPS，就是最重要的性能指标。

hping3 可以作为一个测试网络包处理能力的性能工具。

```text
# -S参数表示设置TCP协议的SYN（同步序列号），-p表示目的端口为80
# -i u10表示每隔10微秒发送一个网络帧
$ hping3 -S -p 80 -i u10 192.168.0.30
```

**（备注：**Linux 内核自带的高性能网络测试工具 pktgen**）**

```text
# -c表示发送3次请求，-S表示设置TCP SYN，-p表示端口号为80
$ hping3 -c 3 -S -p 80 baidu.com
HPING baidu.com (eth0 123.125.115.110): S set, 40 headers + 0 data bytes
len=46 ip=123.125.115.110 ttl=51 id=47908 sport=80 flags=SA seq=0 win=8192 rtt=20.9 ms
len=46 ip=123.125.115.110 ttl=51 id=6788  sport=80 flags=SA seq=1 win=8192 rtt=20.9 ms
len=46 ip=123.125.115.110 ttl=51 id=37699 sport=80 flags=SA seq=2 win=8192 rtt=20.9 ms

--- baidu.com hping statistic ---
3 packets transmitted, 3 packets received, 0% packet loss
round-trip min/avg/max = 20.9/20.9/20.9 ms
```

### **3.2 TCP/UDP 性能：iperf 或者 netperf**

iperf 和 netperf 是最常用的网络性能测试工具，测试 TCP 和 UDP 的吞吐量，它们都以客户端和服务器通信的方式，测试一段时间内的平均吞吐量。

```text
# iperf 安装方式
# Ubuntu
apt-get install iperf3
# CentOS
yum install iperf3
```

iperf测试方式：

```text
# 被测试机器上启动服务端进行
# -s表示启动服务端，-i表示汇报间隔，-p表示监听端口
$ iperf3 -s -i 1 -p 10000
```

测试的客户端

```text
# -c表示启动客户端，192.168.0.30为目标服务器的IP
# -b表示目标带宽(单位是bits/s)
# -t表示测试时间，15 表示15秒
# -P表示并发数，-p表示目标服务器监听端口
$ iperf3 -c 192.168.0.30 -b 1G -t 15 -P 2 -p 10000
```

测试结果报告分析：

```text
[ ID] Interval           Transfer     Bandwidth
...
[SUM]   0.00-15.04  sec  0.00 Bytes  0.00 bits/sec                  sender
[SUM]   0.00-15.04  sec  1.51 GBytes   860 Mbits/sec                  receiver
# 吞吐量是：860 Mb/s，传递的消息量是：1.51 GBytes
```

### **3.3 HTTP 的性能： ab、webbench**

ab 是 Apache 自带的 HTTP 压测工具，主要测试 HTTP 服务的每秒请求数、请求延迟、吞吐量以及请求延迟的分布情况等。

```text
# ab的安装
# Ubuntu
$ apt-get install -y apache2-utils
# CentOS
$ yum install -y httpd-tools
```

目标机器上面运行被测试的服务。

客户端运行ab进行测试：

```text
# -c表示并发请求数为1000，-n表示总的请求数为10000，http://192.168.0.30/ 测试的测试服务的接口
$ab -c 1000 -n 10000 http://192.168.0.30/
...
Server Software:        nginx/1.15.8
Server Hostname:        192.168.0.30
Server Port:            80

...
# 请求汇总、
Requests per second:    1078.54 [#/sec] (mean)
Time per request:       927.183 [ms] (mean)
Time per request:       0.927 [ms] (mean, across all concurrent requests)
Transfer rate:          890.00 [Kbytes/sec] received
# 连接时间汇总
Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0   27 152.1      1    1038
Processing:     9  207 843.0     22    9242
Waiting:        8  207 843.0     22    9242
Total:         15  233 857.7     23    9268
# 还有请求延迟汇总
Percentage of the requests served within a certain time (ms)
  50%     23
  66%     24
  75%     24
  80%     26
  90%    274
  95%   1195
  98%   2335
  99%   4663
 100%   9268 (longest request)
```

### **3.4 应用负载性能：wrk**

wrk是一个 HTTP 性能测试工具，内置了 LuaJIT，方便你根据实际需求，生成所需的请求负载payload，或者自定义响应的处理方法。

安装：

```text
$ https://github.com/wg/wrk
$ cd wrk
$ apt-get install build-essential -y
$ make
$ sudo cp wrk /usr/local/bin/
```

启动被测试服务之后，客户端运行下面的命令进行测试：

```text
# -c表示并发连接数1000，-t表示线程数为2，被测试服务的接口路径：http://192.168.0.30/
$wrk -c 1000 -t 2 http://192.168.0.30/
Running 10s test @ http://192.168.0.30/
  2 threads and 1000 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    65.83ms  174.06ms   1.99s    95.85%
    Req/Sec     4.87k   628.73     6.78k    69.00%
  96954 requests in 10.06s, 78.59MB read
  Socket errors: connect 0, read 0, write 0, timeout 179
Requests/sec:   9641.31
Transfer/sec:      7.82MB
```

tcpdump 和 Wireshark 就是最常用的网络抓包和分析工具，更是分析网络性能必不可少的利器。tcpdump 仅支持命令行格式使用，常用在服务器中抓取和分析网络包。Wireshark 除了可以抓包外，还提供了强大的图形界面和汇总分析工具，在分析复杂的网络情景时，尤为简单和实用。因而，在实际分析网络性能时，先用 tcpdump 抓包，后用 Wireshark 分析，也是一种常用的方法。

原文地址：https://zhuanlan.zhihu.com/p/572183950

作者：linux

# 【NO.74】linux操作系统是如何管理tcp连接的？

首先，在linux内核的网络模块里维护着一个全局实例，用来存储所有和tcp相关的socket：

```text
// net/ipv4/tcp_ipv4.c
struct inet_hashinfo tcp_hashinfo;
```

其次，在该实例的内部，又根据socket类型的不同，划分成四个hashtable：

```text
// include/net/inet_hashtables.h
struct inet_hashinfo {
        // key是由本地地址、本地端口、远程地址、远程端口组成的四元组
        // value是正在建立连接或已经建立连接的socket
        // 比如，当内核收到一个tcp消息时，它先从消息头里读出地址和端口等信息
        // 然后用该信息到ehash里获取对应的socket
        // 最后把剩余的tcp数据添加到该socket的recv buf中供用户程序读取
        struct inet_ehash_bucket        *ehash;

        // key是本地端口
        // value是使用这个端口的所有socket
        // 比如，当我们用socket监听一个端口时，该socket就在bhash里
        // 同理，由该监听端口建立的连接对应的那些socket也在这里
        // 因为它们也都是使用同样的本地端口
        struct inet_bind_hashbucket     *bhash;

        // key是本地地址和端口组成的二元组
        // value是对应的处于listen状态的socket
        struct inet_listen_hashbucket   *lhash2;

        // key是本地端口
        // value是对应的处于listen状态的socket
        struct inet_listen_hashbucket   listening_hash[INET_LHTABLE_SIZE];
};
```

在系统启动时，这个全局的tcp_hashinfo实例会在下面的方法中被初始化：

```text
// net/ipv4/tcp.c
void __init tcp_init(void)
{
        // 初始化tcp_hashinfo里的四个hashtable等信息
}
```

该tcp_hashinfo实例还会被赋值给下面tcp_prot实例中的对应字段：

```text
// net/ipv4/tcp_ipv4.c
struct proto tcp_prot = {
        // 在struct sock里会通过sk_prot字段引用该tcp_prot实例
        // 也就是说，如果拿到任一个struct sock实例
        // 就可以通过它的sk_prot字段获取tcp_prot实例
        // 进而也就可以获取tcp_hashinfo实例
        .h.hashinfo             = &tcp_hashinfo,
};
EXPORT_SYMBOL(tcp_prot);
```

好，以上就是操作系统管理tcp连接用到的全局的数据结构，接下来我们看一些具体操作。

在tcp编程中一般都分为客户端和服务端，我们先来看下服务端对应的操作。

首先，一个socket想要监听一个端口，必须要先bind一个地址，然后再执行listen操作。

其中bind操作就用到了上面的tcp_hashinfo实例里的bhash这个字段，用来判断该端口是否被占用。

来看下代码：

```text
// net/ipv4/inet_connection_sock.c
int inet_csk_get_port(struct sock *sk, unsigned short snum)
{
        // 该方法的调用栈：
        // SYSCALL_DEFINE3(bind)
        // __sys_bind
        // inet_bind
        // inet_csk_get_port
        
        // 下面的hinfo就是全局实例tcp_hashinfo
        struct inet_hashinfo *hinfo = sk->sk_prot->h.hashinfo;

        // 根据端口算出hash值，然后根据这个值找到bhash中对应的slot
        head = &hinfo->bhash[inet_bhashfn(net, port,
                                          hinfo->bhash_size)];

        // 遍历slot指向的链表，找到port对应的值
        inet_bind_bucket_for_each(tb, &head->chain)
                if (net_eq(ib_net(tb), net) && tb->l3mdev == l3mdev &&
                    tb->port == port)
                        goto tb_found;

        // 如果没找到，说明现在还没有人使用这个端口，就新创建一个
        // 新创建的实例就会放到bhash中，表明这个端口我在使用了
        tb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,
                                     net, head, port, l3mdev);

tb_found:
        // 如果tb的owners字段不为空，则说明有人在使用这个端口
        if (!hlist_empty(&tb->owners)) {
                // 如果该端口被别人占用了，且不能共享使用，就返回错误给用户
                if (inet_csk_bind_conflict(sk, tb, true, true))
                        goto fail_unlock;
        }

        // 省略很多无关代码

        // 在该方法的最后，会调用inet_bind_hash方法
        // 方法内容会在下面描述
        if (!inet_csk(sk)->icsk_bind_hash)
                inet_bind_hash(sk, tb, port);
}
EXPORT_SYMBOL_GPL(inet_csk_get_port);
```

再来看下inet_bind_hash方法：

```text
// net/ipv4/inet_hashtables.c
void inet_bind_hash(struct sock *sk, struct inet_bind_bucket *tb,
                    const unsigned short snum)
{
         // 保存绑定端口
        inet_sk(sk)->inet_num = snum;
        
        // tb是上面方法中获取的或创建的bhash中的一个值
        // 它的owners字段存放的是所有使用该端口的sock
        // 下面语句的意思是，把这个sock也加入到owner里
        // 这样在其他人拿到tb时，就能知道哪些sock在使用这个tb对应的端口了
        sk_add_bind_node(sk, &tb->owners);

        // 将tb地址存放到sock的icsk_bind_hash字段里
        // 这样以后想知道该sock对应的bhash里的值时（比如在移除owners时）
        // 就可以通过下面的字段获取了
        inet_csk(sk)->icsk_bind_hash = tb;
}
```

好，bind方法涉及到tcp_hashinfo的地方，到这里就都已经讲完了，我们再看下listen方法：

```text
// net/ipv4/inet_hashtables.c
int __inet_hash(struct sock *sk, struct sock *osk)
{
        // 该方法的调用栈：
        // SYSCALL_DEFINE2(listen)
        // __sys_listen
        // inet_listen
        // inet_csk_listen_start
        // inet_hash
        // __inet_hash

        // hashinfo就是全局实例tcp_hashinfo
        struct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;

        // 根据本地端口，找到该sock在listening_hash中的slot
        ilb = &hashinfo->listening_hash[inet_sk_listen_hashfn(sk)];

        // 将该sock添加到slot对应的链表中
        if (IS_ENABLED(CONFIG_IPV6) && sk->sk_reuseport &&
                sk->sk_family == AF_INET6)
                hlist_add_tail_rcu(&sk->sk_node, &ilb->head);
        else
                hlist_add_head_rcu(&sk->sk_node, &ilb->head);

        // 以本地端口和地址作为key，将该sock加入到tcp_hashinfo里的lhash2里
        inet_hash2(hashinfo, sk);
}
EXPORT_SYMBOL(__inet_hash);
```

listen方法涉及到tcp_hashinfo的地方就是这一点点。

服务端的相关操作就是这些，我们再来看下客户端。

客户端第一步要做的事就是连接服务器，所以我们看下对应的connect方法：

```text
// net/ipv4/inet_hashtables.c
int __inet_hash_connect(struct inet_timewait_death_row *death_row,
                struct sock *sk, u32 port_offset,
                int (*check_established)(struct inet_timewait_death_row *,
                        struct sock *, __u16, struct inet_timewait_sock **))
{
        // 该方法的调用栈
        // SYSCALL_DEFINE3(connect)
        // __sys_connect
        // inet_stream_connect
        // __inet_stream_connect
        // tcp_v4_connect
        // inet_hash_connect
        // __inet_hash_connect

        // hinfo是全局的tcp_hashinfo实例
        struct inet_hashinfo *hinfo = death_row->hashinfo;

        // 一般来说，connect操作我们都不会主动指定本地端口
        // 而是让操作系统帮我们自由挑选
        // 下面的方法就是用于获取操作系统自由挑选的本地端口的范围
        // 该范围默认是 [32768-60999]
        // 当前范围可由以下命令查看：
        // $ cat /proc/sys/net/ipv4/ip_local_port_range
        inet_get_local_port_range(net, &low, &high);

        // 依次检测范围内的端口，找到第一个可以使用的
        // 第一个要检测的端口
        port = low + offset;
        for (i = 0; i < remaining; i += 2, port += 2) {
                // 找到该端口对应的bhash中的slot
                head = &hinfo->bhash[inet_bhashfn(net, port, hinfo->bhash_size)];

                // 遍历该slot指向的链表，查看是否有人已经在使用该端口
                inet_bind_bucket_for_each(tb, &head->chain) {
                        if (net_eq(ib_net(tb), net) && tb->l3mdev == l3mdev && tb->port == port) {
                                // 如果该端口已经被人使用
                                // 那就检查一下使用者中是否有处于连接状态的socket
                                // 且该socket的tcp四元组和我们的socket的tcp四元组完全一致（tcp四元组唯一确定一个tcp连接）
                                // 如果有，则该端口不可用
                                // 如果没有，则可用
                                if (!check_established(death_row, sk,
                                                       port, &tw))
                                        goto ok;
                                goto next_port;
                        }
                }

                // 如果该端口没人用，我们就在bhash中新创建一个对象，表示我们要用
                tb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,
                                             net, head, port, l3mdev);
                goto ok;
next_port:
        }
ok:
        // 该方法上面有讲过，主要就是将该socket与tb实例联系起来
        // 详情可参考上面
        inet_bind_hash(sk, tb, port);
        if (sk_unhashed(sk)) {
                // 该方法下面会详细看
                inet_ehash_nolisten(sk, (struct sock *)tw);
        }
}
```

再来看下上面提到的inet_ehash_nolisten方法：

```text
// net/ipv4/inet_hashtables.c
bool inet_ehash_insert(struct sock *sk, struct sock *osk)
{
        // hashinfo就是全局的tcp_hashinfo实例
        struct inet_hashinfo *hashinfo = sk->sk_prot->h.hashinfo;

        // 根据本地和远程的地址端口信息算出该socket的hash值
        // 并保存到sk的sk_hash字段里，供后续使用
        sk->sk_hash = sk_ehashfn(sk);

        // 根据该hash值找到ehash中对应的slot
        head = inet_ehash_bucket(hashinfo, sk->sk_hash);

        // 把该socket加入到该slot指向的链表中
        if (ret)
                __sk_nulls_add_node_rcu(sk, list);
}

bool inet_ehash_nolisten(struct sock *sk, struct sock *osk)
{
        bool ok = inet_ehash_insert(sk, osk);
}
```

由上可见，tcp_hashinfo在connect操作里的作用是，先根据bhash和ehash里的信息，为该次connect操作挑选出一个合适的本地端口（该端口的使用也会被记录在bhash里），然后在syn消息发送给服务器之前，将该socket放入到ehash中，这样当内核收到服务器的应答消息时，就可以找到对应的socket了。

connect操作最终会发syn消息给服务器，所以下面我们就来看下服务器在收到这个syn消息时是如何处理的。

在此之前，我们先讲一些铺垫性的内容。

当操作系统收到任意tcp的消息时，都会调用下面的方法，找到该tcp消息所属的socket，然后再根据该socket的当前状态和tcp消息的内容做后续处理：

```text
// net/ipv4/tcp_ipv4.c
int tcp_v4_rcv(struct sk_buff *skb)
{
        struct sock *sk;

        // 该方法会从tcp_hashinfo中的各种hashtable中尝试找到对应的socket
        // th->source是发送方的本地端口
        // th->dest是接收方的本地端口
        sk = __inet_lookup_skb(&tcp_hashinfo, skb, __tcp_hdrlen(th), th->source,
                               th->dest, sdif, &refcounted);
}
```

再看下__inet_lookup_skb方法：

```text
// include/net/inet_hashtables.h
static inline struct sock *__inet_lookup_skb(struct inet_hashinfo *hashinfo,
                                             struct sk_buff *skb,
                                             int doff,
                                             const __be16 sport,
                                             const __be16 dport,
                                             const int sdif,
                                             bool *refcounted)
{
        const struct iphdr *iph = ip_hdr(skb);
        return __inet_lookup(dev_net(skb_dst(skb)->dev), hashinfo, skb,
                             doff, iph->saddr, sport,
                             iph->daddr, dport, inet_iif(skb), sdif,
                             refcounted);
}
```

该方法又调用了__inet_lookup方法：

```text
static inline struct sock *__inet_lookup(struct net *net,
                                         struct inet_hashinfo *hashinfo,
                                         struct sk_buff *skb, int doff,
                                         const __be32 saddr, const __be16 sport,
                                         const __be32 daddr, const __be16 dport,
                                         const int dif, const int sdif,
                                         bool *refcounted)
{
        u16 hnum = ntohs(dport);
        struct sock *sk;

        // 该方法会根据本地和远程的地址端口信息
        // 从tcp_hashinfo的ehash中找对应的socket
        sk = __inet_lookup_established(net, hashinfo, saddr, sport,
                                       daddr, hnum, dif, sdif);

       // 如果在ehash中没有找到对应的socket，则调用下面的方法
       // 从tcp_hashinfo的lhash2中找对应的处于listen状态的socket
       return __inet_lookup_listener(net, hashinfo, skb, doff, saddr,
                                      sport, daddr, hnum, dif, sdif);
}
```

好，铺垫性内容结束。

当服务端收到客户端发来的syn包后，会先通过上述方法，在lhash2中找到对应的listen状态的socket（listen方法把这个socket放入到lhash2中的），然后执行下面的逻辑：

```text
// net/ipv4/tcp_input.c
int tcp_conn_request(struct request_sock_ops *rsk_ops,
                     const struct tcp_request_sock_ops *af_ops,
                     struct sock *sk, struct sk_buff *skb)
{
        // 该方法的调用栈
        // tcp_v4_rcv
        // tcp_v4_do_rcv
        // tcp_rcv_state_process
        // tcp_v4_conn_request

        // 该方法的参数sk就是上面找到的处于listen状态的socket

        // 服务端在收到syn消息后，并不是直接创建一个struct sock
        // 而是创建一个struct request_sock，表示该socket还处于tcp三次握手过程中
        struct request_sock *req;
        req = inet_reqsk_alloc(rsk_ops, sk, !want_cookie);

        if (fastopen_sk) {
        } else {
                if (!want_cookie)
                        // 该方法会根据本地和远程的地址端口信息
                        // 将request_sock放到tcp_hashinfo里的ehash里
                        // 这样后续的消息就可以找到这个request_sock了
                        inet_csk_reqsk_queue_hash_add(sk, req,
                                tcp_timeout_init((struct sock *)req));
        }
}
```

服务端在处理完syn消息后，会发synack给客户端，客户端收到synack消息后，会再次发ack给服务端，同时将客户端的socket状态设置为TCP_ESTABLISHED。

由于客户端处理synack消息的逻辑不涉及到tcp_hashinfo里的内容，所以这里就不详细说了。

再看下服务端在收到ack消息之后的逻辑。

服务端在收到ack消息后，会先通过上面介绍过的__inet_lookup_skb方法，找到刚刚创建的request_sock，然后执行如下逻辑：

```text
// net/ipv4/tcp_ipv4.c
struct sock *tcp_v4_syn_recv_sock(const struct sock *sk, struct sk_buff *skb,
                                  struct request_sock *req,
                                  struct dst_entry *dst,
                                  struct request_sock *req_unhash,
                                  bool *own_req)
{
        // 该方法的调用栈
        // tcp_v4_rcv
        // tcp_check_req
        // tcp_v4_syn_recv_sock

        // 收到客户端的ack消息表示该tcp连接建立成功
        // 下面的方法会根据request_sock创建一个真正的struct sock
        struct sock *newsk;
        newsk = tcp_create_openreq_child(sk, req, skb);

        // 该方法会把新创建的socket放到tcp_hashinfo的bhash中
        // 对应的端口就是监听socket所使用的端口
        // 此时该端口对应的bhash中的value中的owners字段里包含监听socket和这个新创建的socket
        if (__inet_inherit_port(sk, newsk) < 0)
                goto put_and_exit;

        // 在上面创建request_sock时，把它放到tcp_hashinfo的ehash里
        // 到这里request_sock的任务已经完成
        // 所以下面的方法会把request_sock从ehash中移除
        // 而把新创建的socket放到ehash里
        *own_req = inet_ehash_nolisten(newsk, req_to_sk(req_unhash));
}
```

到现在一个完整的tcp连接已经建立好了，我们再重新梳理下整个思路。

首先，服务端的socket先执行了bind操作，把它自己放到了tcp_hashinfo的bhash中，然后执行了listen操作，把它自己放到了tcp_hashinfo的lhash2中。

然后，客户端执行connect方法，把对应的socket放到了bhash和ehash中，然后发了syn消息给服务端。

服务端收到syn后，先从lhash2中找到对应的listen状态的socket，然后又根据该socket和syn消息创建了request_sock，并放入ehash中，最后发synack给客户端。

客户端收到synack后，先从ehash中找到对应的socket，然后把其状态设置为TCP_ESTABLISHED，最后又返回ack给服务端。

服务端收到ack后，会先从ehash中找到之前创建的request_sock，然后根据该request_sock，创建真正的sock，最后将request_sock从ehash中移除，将新创建的sock放到bhash和ehash中。

至此，一个tcp连接就建立成功。

再之后，就是tcp连接的数据传输过程了，当操作系统收到对方发来的数据时，先根据tcp消息头里的地址端口等信息，从ehash中找到对应的socket，然后将该数据添加到这个socket的接受缓冲区里，这样用户就可以通过read等方法获取这些数据了。

这就是在tcp连接建立成功之后，tcp内的逻辑对tcp_hashinfo的使用。

下面我们再来看下在tcp的关闭流程中，tcp_hashinfo是如何被使用的。

假设客户端先调用了close方法，主动关闭了连接，看下对应代码：

```text
// net/ipv4/tcp.c
void tcp_close(struct sock *sk, long timeout)
{
        // 该方法的调用栈
        // SYSCALL_DEFINE1(close)
        // __close_fd
        // filp_close
        // fput
        // fput_many
        // ____fput
        // __fput
        // sock_close
        // __sock_release
        // inet_release
        // tcp_close

        // 下面的方法会将socket状态设置为TCP_FIN_WAIT1
        } else if (tcp_close_state(sk)) {
                // 发fin消息给对方
                tcp_send_fin(sk);
        }
}
```

服务端在收到fin包的处理逻辑为：

```text
// net/ipv4/tcp_input.c
void tcp_fin(struct sock *sk)
{
        // 该方法的调用栈
        // tcp_v4_rcv
        // tcp_v4_do_rcv
        // tcp_rcv_established
        // tcp_data_queue
        // tcp_fin

        switch (sk->sk_state) {
        case TCP_ESTABLISHED:
                tcp_set_state(sk, TCP_CLOSE_WAIT);
}
```

该方法将服务端socket的状态设置为TCP_CLOSE_WAIT，然后返回ack给客户端。

客户端收到ack后的处理逻辑：

```text
// net/ipv4/tcp_input.c
int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)
{
        // 该方法的调用栈
        // tcp_v4_rcv
        // tcp_v4_do_rcv
        // tcp_rcv_state_process

        switch (sk->sk_state) {
        case TCP_FIN_WAIT1: {
                tcp_set_state(sk, TCP_FIN_WAIT2);
                break;
        }       
}
```

该方法收到ack后，将客户端对应的socket的状态设置为TCP_FIN_WAIT2。

此时，假设服务器的应用层也调用了socket的close方法，该方法会执行以下逻辑：

```text
// net/ipv4/tcp.c
void tcp_close(struct sock *sk, long timeout)
{
        // 该方法的调用栈
        // SYSCALL_DEFINE1(close)
        // __close_fd
        // filp_close
        // fput
        // fput_many
        // ____fput
        // __fput
        // sock_close
        // __sock_release
        // inet_release
        // tcp_close

        // 下面的方法会将socket状态设置为TCP_LAST_ACK
        } else if (tcp_close_state(sk)) {
                // 发fin消息给对方
                tcp_send_fin(sk);
        }
}
```

客户端收到fin消息的处理逻辑：

```text
// net/ipv4/tcp_input.c
void tcp_fin(struct sock *sk)
{
        // 该方法调用栈
        // tcp_v4_rcv
        // tcp_v4_do_rcv
        // tcp_rcv_state_process
        // tcp_data_queue
        // tcp_fin

        switch (sk->sk_state) {
        case TCP_FIN_WAIT2:
                // 发送ack给对方
                tcp_send_ack(sk);
                // 进入time wait逻辑处理
                tcp_time_wait(sk, TCP_TIME_WAIT, 0);
        }
}
```

继续看下tcp_time_wait方法：

```text
// net/ipv4/tcp_minisocks.c
void tcp_time_wait(struct sock *sk, int state, int timeo)
{
        // 类似于三次握手时服务端创建了request_sock
        // 这里也根据当前sock创建了一个inet_timewait_sock
        // 对应于处于time wait状态时的socket
        struct inet_timewait_sock *tw;
        tw = inet_twsk_alloc(sk, tcp_death_row, state);

        if (tw) {
                // 从sock拷贝各种必要数据到inet_timewait_sock

                // 进行time wait定时，超时后会调用inet_twsk_kill方法
                // 将inet_timewait_sock从ehash和bhash中移除
                inet_twsk_schedule(tw, timeo);

                // 该方法会将sock从ehash中移除
                // 将inet_timewait_sock加入到ehash和bhash中
                inet_twsk_hashdance(tw, sk, &tcp_hashinfo);     
        }

        // 该方法会将sock从bhash中移除，并将其销毁
        tcp_done(sk);
}
```

好，客户端的逻辑就全部完成了，我们再看下服务端逻辑。

当服务器处于TCP_LAST_ACK状态时，收到客户端的ack消息，进行下面的处理：

```text
// net/ipv4/tcp_input.c
int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb)
{
        // 该方法调用栈
        // tcp_v4_rcv
        // tcp_v4_do_rcv
        // tcp_rcv_state_process

        switch (sk->sk_state) {
        case TCP_LAST_ACK:
                if (tp->snd_una == tp->write_seq) {
                        // 该方法及其底层方法会将该sock从ebash和bhash中移除
                        tcp_done(sk);
                }       
        }
}
```

到这里，一个tcp连接就完全关闭了，并且前后端的socket都已经从tcp_hashinfo的ehash和bhash中移除。

现在系统又回到tcp连接之前的状态，即只有一个服务端的socket处于listen状态，该socket同时被存放于tcp_hashinfo的bhash、lhash2及listening_hash里。

我们看下当该listen状态的socket关闭的时候，对应的有关tcp_hashinfo的处理：

```text
// net/ipv4/tcp.c
void tcp_set_state(struct sock *sk, int state)
{
        // 该方法的调用栈
        // SYSCALL_DEFINE1(close)
        // __close_fd
        // filp_close
        // fput
        // fput_many
        // ____fput
        // __fput
        // sock_close
        // __sock_release
        // inet_release
        // tcp_close
        // tcp_set_state

        switch (state) {
        case TCP_CLOSE:
                // 下面的方法会将socket从lhash2及listening_hash里移除
                sk->sk_prot->unhash(sk);

                // 下面方法会将socket从bhash中移除
                if (inet_csk(sk)->icsk_bind_hash &&
                    !(sk->sk_userlocks & SOCK_BINDPORT_LOCK))
                        inet_put_port(sk);
        }
}
```

至此，所有socket都已关闭，且tcp_hashinfo又回到了最原始的空状态。

上文用了大量的篇幅讲述在tcp的各种操作中，tcp_hashinfo是如何被使用的。其实回过头来看一下，tcp_hashinfo在这其中的作用还是非常简单的，其主要目的就是辅助操作系统在各种情况下找到对应的socket。

比如，在syn消息来时，要找到对应的listen状态的socket，用了tcp_hashinfo中的lhash2。

比如，在syn消息之后的所有后续消息来时，要找到其对应的消息处理socket，用了tcp_hashinfo中的ehash。

比如，在绑定端口或挑选端口时，要用到bhash来查询端口是否被占用。

好，就这么多吧，文章到此就结束了。

原文地址：https://zhuanlan.zhihu.com/p/571947344

作者：linux

# 【NO.75】资深工程师带你探秘C++内存管理（理论篇）

在互联网的服务中，C++常用于搭建高性能、高并发、大流量、低延时的后端服务。如何合理的分配内存满足系统高性能需求是一个高频且重要的话题，而且因为内存自身的特点和实际问题的复杂，组合出了诸多难题。

我们可以对内存进行多种类型的划分，**从内存申请大小来看**：

1. 小对象分配：小于4倍内存页大小的内存分配，在4KiB页大小情况下，<16KiB算作小对象分配；
2. 大对象分配：大于等于4倍内存页大小的内存分配，在4KiB页大小情况下，>=16KiB算作大对象分配。

**从一块内存的被持有时长来看**：

1. 后端一次请求内甚至更短时间申请和释放
2. 任意时间窗口内内存持有和更新
3. 几乎与应用进程等长的内存持有和更新
4. 某个进程消亡后一段时间内，由该进程申请的仍具有意义的内存持有和释放

当然还可以按照内存申请释放频率、读写频率进行进一步的分类。

内存管理服务于应用系统，目的是协助系统更好的解决瓶颈问题，比如对于『如何降低后端响应的延迟和提高稳定性』内存管理可能要考虑的是：

1. 处理内存读写并发（读频繁or写频繁）降低响应时间和CPU消耗
2. 应用层的内存的池化复用
3. 底层内存向系统申请的内存块大小及内存碎片化

每一个问题展开可能都是一个比较大的话题，本文作为系列文章《探秘C++内存管理》的开篇，先介绍Linux C++程序内存管理的理论基础。后续会继续解密C++程序常用的内存管理库的实现原理，包括ptmalloc，jemalloc，tcmalloc等，介绍当前业界流行的内存分配器如何管理C++程序的内存。了解内存分配器原理，更有助于工程师在实践中降低处理内存使用问题的成本，根据系统量身打造应用层的内存管理体系。

## **1.Linux内存管理**

GEEK TALK

Linux自底向上大致可以被划分为：

- 硬件(Physical Hardware)
- 内核层(Kernel Space)
- 用户层(User Space)

![img](https://pic1.zhimg.com/80/v2-42b224a85f079dfcd2dabb8c184c7cf8_720w.webp)

*△图1：Linux结构*

内核模块在内核空间中运行，应用程序在用户空间中运行，二者的内存地址空间不重叠。这种方法确保在用户空间中运行的应用程序具有一致的硬件视图，而与硬件平台无关。用户空间通过使用系统调用以可控的方式使内核服务，如：陷入内核态，处理缺页中断。

Linux的内存管理系统自底向上大致可以被划分为：

- 内核层内存管理 : 在 Linux 内核中 , 通过内存分配函数管理内存：

- - kmalloc()/__get_free_pages()：申请较小内存（kmalloc()以字节为单位，__get_free_pages()以一页128K为单位），申请的内存位于物理内存的映射区域，而且在物理上也是连续的，它们与真实的物理地址只有一个固定的偏移。
  - vmalloc()：申请较大内存，虚拟内存空间给出一块连续的内存区，但不保证物理内存连续，开销远大于__get_free_pages()，需要建立新的页表。

- 用户层内存管理：通过调用系统调用函数（brk、mmap等），实现常用的内存管理接口（malloc, free, realloc, calloc）管理内存；经典内存管理库ptmalloc2、tcmalloc、jemalloc。

- 应用程序通过内存管理库或直接调用系统内存管理函数分配内存，根据应用程序本身的程序特性进行使用，如：单个变量内存申请和释放、内存池化复用等。

至此单个进程可以使用Linux提供的内存划分顺利的运行，从用户程序来看Linux进程的内存模型大致如下所示：

![img](https://pic3.zhimg.com/80/v2-905f8d551f38a8fb401bb24e97d2fb2e_720w.webp)

*△图2：Linux进程的内存模型*

- 栈区(Stack)：存储程序执行期间的本地变量和函数的参数，从高地址向低地址生长
- 堆区(Heap): 动态内存分配区域，通过malloc、new、free和delete等函数管理

在标准C库中，提供了malloc/free函数分配释放内存，这些函数的底层是基于brk/mmap这些系统调用实现的，对照图2来看：

- brk(): 用于申请和释放小内存。数据段的末尾，堆内存的开始，叫做brk(program break)。通过设置heap的结束地址，将该地址向高或低移动实现堆内存的扩张或收缩。低地址内存必须在高地址内存的释放之后才能得到的释放，被标记为空闲区的低地址，无法被合并，如果后续再来内存空间的请求大于此空闲区，这部分将成为内存空洞。默认情况下，当最高地址空间的空闲内存超过128K（可由M_TRIM_THRESHOLD选项调节）时，执行内存紧缩操作（trim）。
- mmap()：用于申请大内存。mmap（memory map）是一种内存映射文件的方法，即将一个文件或者其它对象映射到进程的虚拟地址空间中（堆和栈中间的文件映射区域 Memory Mapping Segment），实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。大于 128 K 的内存，使用系统调用mmap()分配内存。与 brk() 分配内存不同的是，mmap() 分配的内存可以单独释放。
- munmp()：释放有mmap()创建的这段内存空间。

但在对于多个同时运行的进程，系统仍需处理有限的物理内存和增长的内存地址等问题。那么当Linux存在多个同时运行的进程时，一次内存的分配过程具体都经过哪些过程呢？现代Linux系统上内存的分配主要过程如下[1] ：

1. 应用程序通过调用内存分配函数，系统调用brk或者mmap进行内存分配，申请虚拟内存地址空间。
2. 虚拟内存至物理内存映射处理过程，通过请求MMU分配单元，根据虚拟地址计算出该地址所属的页面，再根据页面映射表的起始地址计算出该页面映射表(PageTable)项所在的物理地址，根据物理地址在高速缓存的TLB中寻找该表项的内容，如果该表项不在TLB中，就从内存将其内容装载到TLB中。

![img](https://pic4.zhimg.com/80/v2-688a881aaf95d3ce3df2fe5b3c007b7b_720w.webp)

*△图3：Linux内存分配机制（虚拟+物理映射）*

对于内存分配过程中涉及到工具进一步剖析：

- 虚拟内存(Virtual Memory)：现代操作系统普遍使用的一种技术，每个进程有用独立的逻辑地址空间，内存被分为大小相等的多个块，称为页(Page)。每个页都是一段连续的地址，对应物理内存上的一块称为页框，通常页和页框大小相等。虚拟内存使得多个虚拟页面共享同一个物理页面，而内核和用户进程、不同用户进程隔离。
- MMU(Memory-Management Unit)：内存管理单元，负责管理虚拟地址到物理地址的内存映射，实现各个用户进程都拥有自己的独立的地址空间，提供硬件机制的内存访问权限检查，保护每个进程所用的内存不会被其他的进程所破坏。
- PageTable：虚拟内存至物理内存页面映射关系存储单元。
- TLB(Translation Lookaside Buffer)：高速虚拟地址映射缓存， 主要为了提升MMU地址映射处理效率，加了缓存机制，如果存在即可直接取出映射地址供使用。

这里要提到一个很重要的概念，内存的延迟分配，只有在真正访问一个地址的时候才建立这个地址的物理映射，这是 Linux 内存管理的基本思想之一。Linux 内核在用户申请内存的时候，只是分配了虚拟内存，并没有分配实际物理内存；当用户第一次使用这块内存的时候，内核会发生缺页中断，分配物理内存，建立虚拟内存和物理内存之间的映射关系。当一个进程发生缺页中断的时候，进程会陷入内核态，执行以下操作：

- 检查要访问的虚拟地址是否合法
- 查找/分配一个物理页
- 填充物理页内容
- 建立映射关系（虚拟地址到物理地址）
- 重新执行触发缺页中断的指令

如果填充物理页的过程需要读取磁盘，那这次缺页中断是majflt，否则是minflt。我们需要重点关注majflt的值，因为majflt对于性能的损害是致命的，随机读一次磁盘的耗时数量级在几个毫秒，而minflt只有在大量的时候才会对性能产生影响。

## **2.总结**

GEEK TALK

通过对Linux内存管理的介绍，我们可以看到内存管理需要解决的问题：

- 调用系统提供的有限接口操作虚存读写
- 权衡单次分配较大内存和多次分配较少内存带来成本：控制缺页中断（尤其是majflt）vs 进程占用过多内存
- 降低内存碎片
- 降低内存管理库自身带来的额外损耗

原文地址：https://zhuanlan.zhihu.com/p/568776770

作者：linux

# 【NO.76】杂谈代码整洁

> Programs are meant to be read by humans and only **incidentally** for computers to execute. ——[Donald Knuth](https://en.wikipedia.org/wiki/Donald_Knuth)
> “代码始终是写给人看的，只是恰好能被计算机执行。”

什么是好的代码？局部干净，核心逻辑简洁。

本文是一篇总结笔记，是以往工作学习中关于如何实现“局部干净”的一些见闻、教训、团队实践和一些思考。写出整洁代码不仅需在函数、类级别上用功，也应该理解一些其他主题，如项目架构、设计原则等，软件工程是复杂(complex)的，只有各个方面都处理得干干净净，才能在整体上做到代码整洁。特别感谢旭哥，授我思想与技术。

## 1.**指导原则：消除重复，分离关注点，统一抽象层次**

程序员终其一生所做得事大抵不超过这几个**层次**

- 函数与类
- 包与模块(依赖)
- 服务(系统)与服务域
- 产品

在各个层面，这十五个字都足以作一些指导或参考。

### 1.1 消除重复

重复的代码会让系统臃肿，难以维护，增加程序员的心智负担。消除重复的手段不外乎封装，抽取函数、类等。

1. 代码重复

完完全全重复的代码，应该抽取出公共的函数。同一段代码出现两次及以上，就应该抽取出函数。

1. 结构重复

代码虽然不一样，但结构类似，也应该抽取。结构重复可以推导出一些高级技术，如

- 继承体系
- 泛型
- 模板方法(template method，四人帮 23 种设计模式之一)
- 高阶函数，lambda

可惜的是，这些在 golang 里支持不够，各有喜忧。

1. 过程重复

如果总是重复做同一件事，应该使其自动化。

### 1.2 分离关注点

物以类聚，人以群分，代码也是一样。关注点相同的代码应该在一起，天然具有亲和性，这句话的另一个含义，对关注点不同的代码天然具有隔离性，相互之间不应该太深入了解。

1. 分离主线和支线

这是最应该注意的，特别是在业务代码开发中。主要业务逻辑是主线，应该突出主线，淡化支线，按照人的思维，这样才是好理解的。例如旋律音和伴奏音，应该突出旋律，而淡化伴奏。假使伴奏音和旋律音差不多强，喧宾夺主，这样的音乐一定是难听的，因为我们听不出旋律。代码也是这样，应该突出主线，使核心逻辑一目了然。

例如在下单的逻辑中，可能的主线是：检查库存、检查余额、生成订单。那这个下单方法里就应该只有 3 行代码，而不应该有诸如权限判断、性能记录等，如果出现就会有 2 行代码是跟主线无关的，造成不必要的干扰，不要造成无谓的心智负担，应该解放心智去完成更复杂的事情。

分离主线和支线的技术如：

- AOP
- interceptor、filter 等

1. 分离技术和业务

技术型代码常常是公用的，如日期计算、日志记录、性能测量、数据库链接、基础工具类。这些应该和业务逻辑分开，相信这点大家都没有疑问。

1. 按业务性质分离

对业务开发来说，业务知识永远都是第一位的。一个技术水平很高的程序员，但是对业务不理解，他也发挥不了全部水平，就像杀鸡用牛刀，施展不了全部功力。不同业务应该分开，在模块级、服务级甚至更高的产品级，这也应该是共识。但是在一个系统内部，推荐也应该按业务分成不同的包，同一业务下的对象是天然亲和的，同样也是对不同业务的对象是隔离的。

1. 分离变化快慢的代码

变化快的代码和长年不变的代码分开。

1. 分离性能高低的代码

重 I/O 的代码和重 CPU 的代码理应分开，方便合理分配资源，其他诸如此类的代码应该注意分开。

### 1.3 统一抽象层次

> 将有关认识与那些在实际中和他们同在的所有其他认识隔离开，这就是抽象，所有具有普遍性的认识都是这样得到的。——John Locke 《关于人类理解的随笔》

怎么理解抽象？抽象的反面是具体，具体是细节，可见抽象是细节的反面，抽象刻画了统一的画像，描述能力，是对事物在某些方面的特征的提取总结。总之，抽象表达的是意图，另一个理解就是，它不表达细节。“Tom 要成为世界首富”，这句话的抽象层次就很高，意图很明显，但是关于 Tom 如何成为世界首富、用什么货币衡量等细节，一概不知。抽象层次高，偏意图，语义(代码在上下文中表达的语义)清晰，信息量小；抽象层次低，偏实现，语义模糊，信息量大。

两个原则：

- 同一抽象层次上的对象才能直接对话；
- 同一抽象层次上的对象之间存在着紧密合作；

#### **1.3.1 典型的函数结构**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151349413285195.png)

一个好的函数结构，应该这样像一棵树一样层次分明。一方面，每一个层次都只有 2~~5 个步骤，一般而言我们做一件事也就 2~~5 个步骤，分解太多太少都不好，太少没必要分解，太多记不住，增加心智负担。实际上，更多的情况，我们都喜欢 3 这个数字，例如在会议总结时，总结 3 点足够了，更多估计不会有太多人愿意继续集中注意力听超过 3 点的总结。所以**一个好的函数，不应该超过 5 行**，我们之所以做不到，除了抽象层次划分不准确之外，还有很大一部分原因是表达能力不足，毕竟英语不是我们的母语。(函数 10 行代码，是我在过去工作中合代码的及格线，20 行是红线。)另一方面，只有叶子节点才表达实现，非叶节点都应该表达意图。

以“把大象装进冰箱”为例，不外乎三步：

1. 打开冰箱门
2. 放进大象
3. 关闭冰箱门

所以关于如何把大象切成碎片，不应该出现在上面，应该在步骤 2 的后续调用中。

由这个函数结构还可以得见，好的程序读起来应该像自然语言，极少部分像数学语言(偏算法)，不好的程序读起来就像是程序。当我们读一段程序，一眼看去它就像是程序，那不是它太好，它就是不够好的。一直认为，写作能力才是成为优秀程序员最重要的能力。

### 1.4 隔离与隐藏

信息隐藏，是抽象的一种手段。通过信息隐藏，来暴露只想让外界知道的东西，表达意图。隔离是实现信息隐藏的重要手段。隐藏与隔离有一个天然的好处，例如我们有一个包，我们只提供数个 public 的方法，包内的其他对象、方法都只是包可见的，这样，我们可以随意修改内部实现，只要保证那些 public 方法的行为不变。特别是对于复杂系统，如果做不好隔离与隐藏，到处都是 public 方法，到后面谁都不敢随意改动代码，谁也不知道哪位大哥在方法上加了一个 if-else 分支。

## 2.**编码 tips**

以下都是一些简单实用的技术，以如何写出整洁代码，很多是出自《代码整洁之道》，一些是出自过去团队的经验。

### 2.1 类

1. 类应该足够小

最初级的程序员可能会在一个 Controller 里做完所有的业务逻辑，最终会使这个类成为 God Class。一个类太大，代码太多，会使类的结构不清晰，职责混乱，维护代码时花费很多时间去寻找修改位置。譬如我们所见的世界，由分子、原子甚至更小的粒子排列组合而成，所以才有缤纷多彩的各色物质(对象)，但如果构成物质的最小粒子就是人，那还能组合出什么其他物质呢？代码也是如此，类应该足够小，才能发挥排列组合的威力。

1. 单一职责

类的职责应该单一，即“SOLID”五大原则的 S，职责单一意味着，“只有一个理由可以修改它”。另外，类名一般而言应该是名词，且描述其职责。

> 如果无法为一个类名以精确的名称，这个类大概就太长了。类名越含混，该类越有可能拥有过多的权责。
>
> ——《Clean Code》

1. 内聚

内聚的含义是，类的每一个字段都应该被某个(些)方法所使用到。如果不能达到这个结果，应该考虑是否类的字段应该拆分出去成为新的类。

1. 严格控制访问权限，注意信息隐藏，OCP

访问权限应该能小则小。能 private 就不要 package，能 package 就不要 protected。这样做能使我们更好的遵循 OCP 原则。最稳定的系统，是从不修改的系统。

### 2.2 函数

1. 尽可能小

> 经过漫长的试错，经验告诉我，函数就应该小 ——《Clean Code》

应该控制在 10 行以内，至多 20 行，除非是细节代码。这是完全可以做到的，做不到的原因可能有：函数功能太多，职责不单一；函数抽象层次划分不清；语言支持不够等。前面已经说过，做一件事大概也就 2~5 步，每一步一个函数，加上可能的条件判断，10 行是一个比较合理的数字。而且，函数越小，功能越集中，越便于取一个好名字。

1. 单一职责

一个函数只做一件事。这一点很容易理解，难的是我们如何确定函数做的那件事是什么。一千个读者就有一千个哈姆雷特，同样的，不同的人对一个函数的理解也有所不同，对于做一件事的步骤拆分也可能有所不同。对此，一个可靠的判断准则是：函数的内容(函数体内的代码)只是做了函数所在抽象层级的步骤，那这个函数就是只做了一件事。函数所在抽象层级，根据对业务的理解，应该用良好的函数名加以示意。

1. 单一抽象层次

一个函数应该只在一个抽象层次上。计算机世界都是**层层叠加**的，例如：寄存器 -> 高速缓存 -> 主存 -> 硬盘 -> 网络(可参见《CSAPP》第六章)，再如硬件 -> 机器指令 -> 汇编 -> C -> C++ -> JVM -> Java -> Servlet -> Spring -> SpringBoot。严格禁止跨层次搞事。我们应该熟悉业务，根据业务上的一次用例，划分抽象层次，使每一个函数都只在某一个抽象层次上，不要跨层次。还是以把大象装进冰箱为例：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151350053634077.png)

最顶层的函数是 f，f 里就只应该有 s1, s2, s3 三个函数。s2a, s2b 里的实现代码则不应该出现在 f 里。同理在 s2 函数里，只应该有 s2a, s2b 函数，而不应该有抽象层次更低(更具体)的 s2aα, s2aβ 的实现代码等。

绿色部分是最低抽象层次的具体实现，这部分是无法拆分，且难以控制代码行数的，因为有些情况下做一件事就是有很多细节实现步骤。

1. 参数尽量少

> 最理想是 0 个，其次是 1 个，2 个，最多 3 个参数，不要超过 3 个参数，除非你有非常特殊的理由。——《Clean Code》

参数带了极大的语义干扰，而且也难于测试。一个典型的不好的设计，就是用 bool 作为公开函数的参数，因为 bool 变量天然地会使人想到这个函数不会只做一件事，它分情况处理，bool 入参的命名稍有歧义就会使人困惑。例如

```
func GoToWork(raining bool) {    if raining {        // 开车去    } else {        // 走路去    }}
```

更推荐的做法是，将 bool 参数的函数私有，另外公开两个语义清晰的函数。

```
func WalkToWork() {    goToWork(false)}func DriveToWork() {    goToWork(true)}// 私有func goToWork(raining bool) {    if raining {        // 开车去    } else {        // 走路去    }}
```

任何时候，我们维护代码，最关心的都是对外可访问的函数，这些函数应该尽我们所能使其整洁。另一个例子，在 JUnit 里曾有这样的方法，不知给多少初学者带来困扰

```
assertEquals(expected, actual)
```

对使用者来说，完全没有必要去记忆两个参数的相对位置。相较而言，assertJ 里的连贯式接口就要友好得多

```
assertThat(actual).isEqualTo(expected)
```

golang 里能够返回多个返回值，但这绝不可以滥用。试看

```
func func1(/* params */) (string, string, string, string, string) {    // 函数职责不单一，功能太多}func func2(/* 此处多达6个参数 */) {    // 函数职责不单一，功能太多}
```

这样多入参、多返回值，给调用方造成很大困扰，调用方需要反复分辨每个参数、返回值的对应关系。不能因为眼前就只有自己调用自己写的函数而这样放纵，我们写的代码，终究是会由别人接手的。

1. 无副作用

一般而言，函数应该是无副作用的，对于调用方来说，它就是一个黑盒：给定输入，产生输出。仅此而已。不要让调用方去思考我这次调用会不会产生输出以外的其他结果。例如应该尽量避免这种情况：一个函数，以指针作为参数，返回一个结果的同时，还修改了指针所指向的内容。一个函数的作用，要么是 get，要么是 post，即要么函数无修改的 get 一个结果，要么就是单纯修改而不返回修改以外的结果。jdk 里有一个典型的反例，各种集合的 add/set 总返回了一个 bool 值，就会出现这样的代码

```
// numbers is a listif (numbers.add(1)) {    //}
```

对于新手这可能就是一个让人迷惑的地方，可见，无副作用也不是绝对的，强如 JDK 也有不得已的折衷处理。

1. if 嵌套不应超过 2 层

if 不要嵌套超过 2 层，这初听起来有些强人所难，仿佛要求每个职业篮球运动员都应该以乔丹的能力作为基准。可人的天性就是不喜欢思考的，喜欢简单。在此再一次强调统一抽象层次，if 嵌套太多，一定要思考，是不是函数做的事情太多，跨层次在搞事情。我们应该用一些高标准去检验自己的代码，想办法去满足，这个过程才会有所成长，否则除了收获经验以外，不会有进阶的成长(其实人生又何尝不是如此)。

消除多层 if 嵌套的一些手段

- 提前返回，将嵌套 if 铺陈开来，使不满足条件的分支提前返回；
- 碰到第三个 if，直接将其抽取为函数(简单粗暴)；
- lambda，在 Java 里利用 stream 的扁平化处理，使 filter、map 等语法元素都可以接收简单的函数，从而避免在 for 里加 if 判断。对于集合的遍历处理，都应该尽量先采用 stream 的做法，这种流水线的思想，在一个步骤里就剔除了不满足条件的对象，然后流转到下一个步骤。

1. 语义和实现距离不为 0 时应该抽取函数

好的代码读起来就应该像自然语言，而不是像程序，这就要求在高抽象层次时，函数应该表达意图，而只有在叶子结点——抽象层次最低的实现部分才表达实现，这个地方的代码更像是程序。所以，在代码中的某个位置，我们本应该表达意图，却写了细节实现代码，这就应该抽取出函数。以下面这段代码为例。

```
tom := &Person{}if tom.Age >= 18 {    // do your bussiness}
```

一般认为这是表达 tom 是否成年，但实际的业务含义中却是判断 tom 是否可以申领 C1 驾照。即使是想表达是否成年，这样也要使大脑经过一层转换，由`Age >= 18`推理一次，才能得出结论这是表达是否成年，这是典型的“代码 prase 语义”，不要小看这层 parse 对人脑的开销，特别是所见之处都是这样的代码会让我们的大脑长期忙于“线程切换”活动，造成的思维停顿让人非常沮丧；此外，如果一个日本人看到这段代码，一定不会想是表达是否成年这个语义，因为他们的法定成年年龄是 20 岁(2022 年 4 月 1 日起改为 18 岁)，这是代码不灵活的体现。推荐的做法是

```
if tom.isAdult() {    // do your bussiness}func (p *Person) isAdult() bool {    return p.Age >= 18}
```

这样，在`isAdult`方法里还可以更改实现，也更灵活，很多时候，如果我们程序写得好，实现比较灵活，就能够从容的应对经常变化的需求；如果需求稍微变化一下，现有代码就顶不住了，就应该思量实现是否足够好。代码应该表达意图，特别是 if 条件分支里，不要让人再去推理，直接表达语义。就像人走路，相比于一马平川，我们不会更喜欢岔路；但凡岔路，就应该明确指明路线，而不是在路口打个机锋，才让你思考十年然后顿悟才选择出了某一条路。

1. 童子军军规

走的时候，比来的时候干净一点。代码中如果我们能经常注意这一点，那我们每时每刻都在改善代码。世界是朝着熵增的方向发展的，譬如一个房间，即使我们完全不去干扰它，久而久之它也会变得更加混乱，代码也是这样，它终究会变得越来越混乱、难以修改、难以维护。如果我们不注意这一点，反而每次来都扔一点垃圾，久而久之就会成为“破窗”直至“破楼”。

1. hardcode

任何时候都不应该在代码中直接出现 hardcode，hardcode 难以表达语义，且难以管理。

### 2.3 命名与注释

命名是一个哲学问题，我们所知的一切，都是命名，存在、宗教、知识、伦理…没有命名，我们所知的一切所谓知识都将崩塌。

> There are only two hard things in Computer Science: cache invalidation and naming things. ——Phil Karlson

“计算机世界只有两个难题：缓存失效和命名。”(可读一读《CSAPP》关于存储层次结构的描述，对此会深有体会。)

坊间流传着一句话，给变量命名犹如给自己亲女儿命名一般，只因如此，就不会随意命名了。命名的一般原则无外乎完整、简洁、准确等。

1. 顾名思义、望文知义、无歧义

清楚明白无歧义地表达含义，不要让别人猜你的意思。在 API 设计里，有一条原则即是“Don’t Let Me Think”，命名也应该如此，乃至日常工作沟通中也应当如此。

1. 名副其实

`cat := &Dog{}`？

1. 表达语义，避免误导

`userList`实际实现是一个 `Set`，`users` 这个命名会更好，语义更清晰，`userList` 有一些语义干扰。命名不应该表达实现(如 List 实现，数据结构等)，而应该表达语义。

1. 使用读得出来的名字，谨慎使用缩写

人看代码，实际是在默读代码，包括你现在看到这句话的时候，心里也是在默念出来的。如`xxCmd`这样的命名，一定会在脑海中多了一次 parse，对于一些更不常见的缩写，这种情况更严重。前面提过，这种脑内 parse 会使大脑忙于“线程切换”，思维停顿更是让人沮丧。

1. 团队统一业务术语

DDD 的一个重要理念就是同一术语，在一个团队内部就应该统一术语，从运营产品到开发测试等，都应该对某一个业务专有词不产生任何歧义。我见过太多因为产品和开发对某一个词的理解不同而“大打出手”的事。

1. 注释

好的代码是自注释的。

命名虽然重要，但也无需发展成为圣战。

### 2.4 单元测试

应该重视单元测试。单元测试，**保证软件质量和代码质量**。单元测试是我们所写函数的第一个调用者，如果发现单元测试很难写，那不用说，函数实现绝对是有问题的，或者抽象层次划分不清，或者依赖复杂等。如果连我们自己调自己的方法都用得这么不爽，那可想而知其他调用者，特别是网络接口。这是为什么单元测试可以保证代码质量，它可以检验我们的代码是否写得足够好。

**单元测试对于修改代码或重构的重要性无可替代**，对于拥有一组完善单测的函数，我们可以随意更改，只要让修改后的函数通过单测，就几乎是安全修改的，单元测试铺了一张安全网，让我们像走钢丝一样地写代码不至于失足跌入深渊万劫不复。

关于单元测试有很多实践，最著名的可能莫过于 TDD，我们虽不至于按 TDD 的实践来开发，但我们应该善用单元测试，来检验我们的函数实现是否合理，实现得好的函数，单测一定是好写的，逆否亦然。

一些 tips：

- 不能依赖真实依赖，这是大忌。如依赖真实数据库且数据库出错，并不能检验单测所测函数逻辑失败，而是外部造成的，应该 mock，且对一般对象也应该尽量使用 mock 对象；否则即为集成测试；
- 路径应该尽可能全；
- 不能有条件分支，任何条件分支都应该新开单测；
- 单测也应该像业务代码一样，干净整洁；
- realBug 测试是必要的，发生过一次的事情很有可能会反复发生，我们选择题第一次选错了，第二次还是很可能选择上次的那个错误答案；
- …

## 3.**其他话题**

以下这些话题，单独拎出来都是一个很大的主题，这里只是抛砖引玉，简单谈谈一些和整洁代码相关的感悟和实践，实是整洁代码需要各个方面的努力，而非仅代码一途用功。

### 3.1 心智负担与复杂

> Complexity is caused by two things: dependencies and obscurity.

软件开发的复杂性由两样东西带来：依赖和晦涩。这两者都会加重心智负担。消除心智负担一定程度上意味着增加可读性和可维护性。

其实我们所做的一切，都是在驯服复杂度。人脑终究是有限的，我们眼所能见、脑所能别的资源几乎都是有限的。驯服复杂度，代码写好了，升职加薪，业余时间没有 bug 找上门，提高生活质量，我们所做的一切不就是为了这个吗？

**复杂是我们软件生涯的一生之敌**。

### 3.2 分层分包

分层是除“模块化”之外最古老的架构模式，冯诺依曼计算机模型是模块化的架构，但同时计算机世界也是层层叠加的。**分层分包的本质就是隔离**，人处理难题的能力是有限的，无法同时处理很多复杂的事情，所以不把所有东西都放在同一层次，譬如行政体系也是分层的。隔离使得各个层次职责更清晰，更容易管理。

**分层的原则是只能上层调用下层，而不能反过来，反之容易导致循环依赖。分包的原则是，同一个包中的对象天然是亲和的，同时对包外的对象是不亲和(隔离)的**。

从分层的理念理解，则 controller/api 层 的 request 不应该一直传递到 service 层甚至是 dao 层，然而这种现象却是非常常见。业务层不应该对界面层有所了解，而是相反，界面层调用业务层来完成一次用户用例。凡是进入业务层，就不应该有界面层的对象，而应该在界面层转换成业务对象，进而使业务层只处理它所能知的业务对象。这种跨层次的信息传递，无异于乡长直接向省长汇报工作。

传统 MVC 的分层对于简单业务而言，是简单实用的。但是其对于复杂业务系统的架构能力十分有限，一个 service 包里有上百个 xxxService 类，业务表达能力有限，如果所有对外服务都可以叫做 service，那为何要区分餐厅、医院、商场，统一叫服务不就好了？而且很多时候，往往就是一些无法准确划分职责的类干脆就合并到 Service 类里，这让 Service 类成了一个大杂烩直至成为 God Class，最终退化成过程式代码，只是机械的代码堆积，没有层次分明、职责分明的对象，没有设计感。

对于业务复杂的系统，DDD 微服务经典四层分层是一个更好的实践，重视业务、重视 OO，整个系统设计感十足，对象林立，可以做一些了解。但是对于业务简单的系统，则不应该为了炫技而使用技术。因地制宜，学会取舍。

此外，关于 dao，业务复杂情况下应该避免使用。dao 的表达能力同样很弱，dao 里的方法很难表达意图，语义表达能力很弱，findByXXX 实际是没有业务语义的，例如 findByAge 接受参数 18，还是上面的例子，并不是选择成年的业务意义。此外 dao 难以管理。例如一个 dao 里有上百个 findByXXX 方法，如果业务需要新增方法，一般最省事的做法就是直接又加一个 findByXXX 方法，这样下去 dao 会越来越膨胀并趋于崩坏。业务复杂情况应该使用 repository，repository 通过组合规格(specification)来表达查询语义，repository 是仓储的概念，类似一个 ADT，只有有限几个经过仔细设计的方法，类比一个 map 就理解了。关于更多为何不使用 dao 而应该使用 repository 的知识，可参考 https://thinkinginobjects.com/2012/08/26/dont-use-dao-use-repository/

### 3.3 设计原则

遵循良好的设计原则，能使代码更整洁，当然意义不仅于此。有关设计原则的资料很多，我们也应该对此有所了解。常见设计原则如：

- SOLID
- ADP
- REP
- CCP
- CRP
- SDP
- SAP
- DRY
- KISS
- YAGNI
- SLAP
- POLA
- LoD

### 3.4 代码的非功能特性

只完成功能的代码，是最基础的代码。好的代码还应该尽量完成代码的非功能特性，有兴趣可以了解下，不外乎：

- 可操作性
- 健壮性
- 可测试性
- 可维护性
- 易用性
- 可重用性

其实还有些主题是无法避而不谈的，如错误处理，但限于篇幅和能力，只能推荐读两遍《Clean Code》。

最后，人生不过是“看山是山，看山不是山，看山仍是山”，代码也是如此，不要着相。

原文作者：russhe

原文链接：https://mp.weixin.qq.com/s/s_2dfOnMqND1qKjTfnmg5A

# 【NO.77】微信 libco 协程库原理剖析

> 同 Go 语言一样，libco 也是提供了同步风格编程模式，同时还能保证系统的高并发能力，本文主要剖析 libco 中的协程原理。

## **0.简介**

- libco 是微信后台大规模使用的 c/c++协程库，2013 年至今稳定运行在微信后台的数万台机器上。
- libco 通过仅有的几个函数接口 co_create/co_resume/co_yield 再配合 co_poll，可以支持同步或者异步的写法，如线程库一样轻松。同时库里面提供了 socket 族函数的 hook，使得后台逻辑服务几乎不用修改逻辑代码就可以完成异步化改造。
- 开源地址：https://github.com/Tencent/libco

## 1.**准备知识**

### 1.1 协程是什么

- 协程本质上就是用户态线程，又名纤程，将调度的代码在用户态重新实现。有极高的执行效率，因为子程序切换不是线程切换而是由程序自身控制，没有线程切换的开销。协程通常是纯软件实现的多任务，与 CPU 和操作系统通常没有关系，跨平台，跨体系架构。
- 协程在执行过程中，可以调用别的协程自己则中途退出执行，之后又从调用别的协程的地方恢复执行。这有点像操作系统的线程，执行过程中可能被挂起，让位于别的线程执行，稍后又从挂起的地方恢复执行。
- 对于线程而言，其上下文切换流程如下，需要两次权限等级切换和三次栈切换。上下文存储在内核栈上。线程的上下文切换必须先进入内核态并切换上下文, 这就造成了严重的调度开销。线程的结构体存在于内核中，在 pthread_create 时需要进入内核态，频繁创建开销大。

### 1.2 Linux 程序内存布局

Linux 使用虚拟地址空间，大大增加了进程的寻址空间，由低地址到高地址分别为：

- 只读段/代码段：只能读，不可写；可执行代码、字符串字面值、只读变量
- 数据段：已初始化且初值非 0 全局变量、静态变量的空间
- BSS 段：未初始化或初值为 0 的全局变量和静态局部变量
- 堆 ：就是平时所说的动态内存， malloc/new 大部分都来源于此。
- 文件映射区域 ：如动态库、共享内存等映射物理空间的内存，一般是 mmap 函数所分配的虚拟地址空间。
- 栈：用于维护函数调用的上下文空间；局部变量、函数参数、返回地址等
- 内核虚拟空间：用户代码不可见的内存区域，由内核管理(页表就存放在内核虚拟空间)。

其中需要注意的是：栈和堆的这两种不同的地址增长方向，栈从高到低地址增长。堆从低到高增长，后面协程切换中就涉及到该布局的不同。

### 1.3 栈帧

栈帧是从栈上分配的一段内存，每次函数调用时，用于存储自动变量。从物理介质角度看，栈帧是位于 esp（栈指针）及 ebp（基指针）之间的一块区域。每个栈帧对应着一个未运行完的函数。栈帧中保存了该函数的函数参数、返回地址和局部变量等数据。局部变量等分配均在栈帧上分配，函数结束自动释放。

- ESP：栈指针寄存器，指向当前栈帧的栈顶。
- EBP：基址指针寄存器，指向当前栈帧的底部。

C 函数调用，调用者将一些参数放在栈上，调用函数，然后弹出栈上存放的参数。这里涉及调用约定，调用约定涉及参数的入栈顺序（从左到右还是从右到左）、参数入栈和清理的是调用者(caller)还是被调用者(callee)，函数名的处理。

- 采用**cdecl 调用约定的调用者会将参数从右到左的入栈，最后将返回地址入栈。这个返回地址是指，函数调用结束后的下一行执行的代码地址。（**cdecl is the default calling convention for C and C++ programs. Because the stack is cleaned up by the caller, it can do vararg functions. The __cdecl calling convention creates larger executables than __stdcall, because it requires each function call to include stack cleanup code. The following list shows the implementation of this calling convention. The __cdecl modifier is Microsoft-specific.）

## **2.关键数据结构**

libco 的协程控制块 stCoRoutine_t：

```
struct stCoRoutine_t{ stCoRoutineEnv_t *env; pfn_co_routine_t pfn; void *arg; coctx_t ctx; char cStart; char cEnd; char cIsMain; char cEnableSysHook; char cIsShareStack; void *pvEnv; //char sRunStack[ 1024 * 128 ]; stStackMem_t* stack_mem; //save stack buffer while confilct on same stack_buffer; char* stack_sp; unsigned int save_size; char* save_buffer; stCoSpec_t aSpec[1024];};
```

- env：即协程执行的环境，libco 协程一旦创建便跟对应线程绑定了，不支持在不同线程间迁移，这里 env 即同属于一个线程所有协程的执行环境，包括了当前运行协程、嵌套调用的协程栈，和一个 epoll 的封装结构。这个结构是跟运行的线程绑定了的，运行在同一个线程上的各协程是共享该结构的，是个全局性的资源。

```
struct stCoRoutineEnv_t{ stCoRoutine_t *pCallStack[ 128 ]; int iCallStackSize; stCoEpoll_t *pEpoll; //for copy stack log lastco and nextco stCoRoutine_t* pending_co; stCoRoutine_t* occupy_co;};
```

- pfn：实际等待执行的协程函数
- arg：上面协程函数的参数
- ctx：上下文，即 ESP、EBP、EIP 和其他通用寄存器的值

```
struct coctx_t{#if defined(__i386__) void *regs[ 8 ];#else void *regs[ 14 ];#endif size_t ss_size; char *ss_sp;};
```

- cStart、cEnd、cIsMain、cEnableSysHook、cIsShareStack：一些状态和标志变量，后面会细说
- pvEnv：保存程序系统环境变量的指针
- stack_mem：协程运行时的栈内存，这个栈内存是固定的 128KB 的大小。

```
struct stStackMem_t{ stCoRoutine_t* occupy_co; int stack_size; char* stack_bp; //stack_buffer + stack_size char* stack_buffer;};
```

stack_sp、save_size、save_buffer：这里要提到实现 stackful 协程（与之相对的还有一种 stackless 协程）的两种技术：Separate coroutine stacks 和 Copying the stack（又叫共享栈）。这三个变量就是用来实现这两种技术的。

实现细节上，前者为每一个协程分配一个单独的、固定大小的栈；而后者则仅为正在运行的协程分配栈内存，当协程被调度切换出去时，就把它实际占用的栈内存 copy 保存到一个单独分配的缓冲区；当被切出去的协程再次调度执行时，再一次 copy 将原来保存的栈内存恢复到那个共享的、固定大小的栈内存空间。

如果是独享栈模式，分配在堆中的一块作为当前协程栈帧的内存 stack_mem，这块内存的默认大小为 128K。

如果是共享栈模式，协程切换的时候，用来拷贝存储当前共享栈内容的 save_buffer，长度为实际的共享栈使用长度。

通常情况下，一个协程实际占用的（从 esp 到栈底）栈空间，相比预分配的这个栈大小（比如 libco 的 128KB）会小得多；这样一来， copying stack 的实现方案所占用的内存便会少很多。当然，协程切换时拷贝内存的开销有些场景下也是很大的。因此两种方案各有利弊，而 libco 则同时实现了两种方案，默认使用前者，也允许用户在创建协程时指定使用共享栈。

## **3.生命周期**

### 3.1 创建协程 Create coroutine

调用 co_create 将协程创建出来后，这时候它还没有启动，也即是说我们传递的 routine 函数还没有被调用。实质上，这个函数内部仅仅是分配并初始化 stCoRoutine_t 结构体、设置任务函数指针、分配一段“栈”内存，以及分配和初始化 coctx_t。

- ppco：输出参数，co_create 内部为新协程分配一个协程控制块，ppco 将指向这个分配的协程控制块。
- attr：指定要创建协程的属性（栈大小、指向共享栈的指针（使用共享栈模式））
- pfn：协程的任务（业务逻辑）函数
- arg：传递给任务函数的参数

```
int co_create( stCoRoutine_t **ppco,const stCoRoutineAttr_t *attr,pfn_co_routine_t pfn,void *arg ){ if( !co_get_curr_thread_env() ) {  co_init_curr_thread_env(); } stCoRoutine_t *co = co_create_env( co_get_curr_thread_env(), attr, pfn,arg ); *ppco = co; return 0;}
```

### 3.2 启动协程 Resume coroutine

在调用 co_create 创建协程返回成功后，便可以调用 co_resume 函数将它启动了。

取当前协程控制块指针，将待启动的协程压入 pCallStack 栈，然后 co_swap 切换到指向的新协程上取执行，co_swap 不会就此返回，而是要等当前执行的协程主动让出 cpu 时才会让新的协程切换上下文来执行自己的内容。

```
void co_resume( stCoRoutine_t *co ){ stCoRoutineEnv_t *env = co->env; stCoRoutine_t *lpCurrRoutine = env->pCallStack[ env->iCallStackSize - 1 ]; if( !co->cStart ) {  coctx_make( &co->ctx,(coctx_pfn_t)CoRoutineFunc,co,0 );  co->cStart = 1; } env->pCallStack[ env->iCallStackSize++ ] = co; co_swap( lpCurrRoutine, co );}
```

### 3.3 挂起协程 Yield coroutine

在非对称协程理论，yield 与 resume 是个相对的操作。A 协程 resume 启动了 B 协程，那么只有当 B 协程执行 yield 操作时才会返回到 A 协程。在上一节剖析协程启动函数 co_resume() 时，也提到了该函数内部 co_swap() 会执行被调协程的代码。只有被调协程 yield 让出 CPU，调用者协程的 co_swap() 函数才能返回到原点，即返回到原来 co_resume() 内的位置。

在被调协程要让出 CPU 时，会将它的 stCoRoutine_t 从 pCallStack 弹出，“栈指针” iCallStackSize 减 1，然后 co_swap() 切换 CPU 上下文到原来被挂起的调用者协程恢复执行。这里“被挂起的调用者协程”，即是调用者 co_resume() 中切换 CPU 上下文被挂起的那个协程。

```
void co_yield_env( stCoRoutineEnv_t *env ){ stCoRoutine_t *last = env->pCallStack[ env->iCallStackSize - 2 ]; stCoRoutine_t *curr = env->pCallStack[ env->iCallStackSize - 1 ]; env->iCallStackSize--; co_swap( curr, last);}void co_yield_ct(){ co_yield_env( co_get_curr_thread_env() );}void co_yield( stCoRoutine_t *co ){ co_yield_env( co->env );}
```

- 同一个线程上所有协程是共享一个 stCoRoutineEnv_t 结构的，因此任意协程的 co->env 指向的结构都相同。

### 3.4 切换协程 Switch coroutine

上面的启动协程和挂起协程都设计协程的切换，本质是上下文的切换，发生在 co_swap()中。

- 如果是独享栈模式：将当前协程的上下文存好，读取下一协程的上下文。
- 如果是共享栈模式：libco 对共享栈做了个优化，可以申请多个共享栈循环使用，当目标协程所记录的共享栈没有被其它协程占用的时候，整个切换过程和独享栈模式一致。否则就是：将协程的栈空间内容从共享栈拷贝到自己的 save_buffer 中，将下一协程的 save_buffer 中的栈内容拷贝到共享栈中，将当前协程的上下文存好，读取下一协程上下文。

协程的本质是，使用 ContextSwap，来代替汇编中函数 call 调用，在保存寄存器上下文后，把需要执行的协程入口 push 到栈上。

```
void co_swap(stCoRoutine_t* curr, stCoRoutine_t* pending_co){  stCoRoutineEnv_t* env = co_get_curr_thread_env(); //get curr stack sp char c; curr->stack_sp= &c; if (!pending_co->cIsShareStack) {  env->pending_co = NULL;  env->occupy_co = NULL; } else {  env->pending_co = pending_co;  //get last occupy co on the same stack mem  stCoRoutine_t* occupy_co = pending_co->stack_mem->occupy_co;  //set pending co to occupy thest stack mem;  pending_co->stack_mem->occupy_co = pending_co;  env->occupy_co = occupy_co;  if (occupy_co && occupy_co != pending_co)  {   save_stack_buffer(occupy_co);  } } //swap context coctx_swap(&(curr->ctx),&(pending_co->ctx) ); //stack buffer may be overwrite, so get again; stCoRoutineEnv_t* curr_env = co_get_curr_thread_env(); stCoRoutine_t* update_occupy_co =  curr_env->occupy_co; stCoRoutine_t* update_pending_co = curr_env->pending_co; if (update_occupy_co && update_pending_co && update_occupy_co != update_pending_co) {  //resume stack buffer  if (update_pending_co->save_buffer && update_pending_co->save_size > 0)  {   memcpy(update_pending_co->stack_sp, update_pending_co->save_buffer, update_pending_co->save_size);  } }}
```

这里起寄存器拷贝切换作用的 coctx_swap 函数，是用汇编来实现的。

coctx_swap 接受两个参数，第一个是当前协程的 coctx_t 指针，第二个参数是待切入的协程的 coctx_t 指针。该函数调用前还处于第一个协程的环境，调用之后就变成另一个协程的环境了。

```
extern "C"{ extern void coctx_swap( coctx_t *,coctx_t* ) asm("coctx_swap");};.globl coctx_swap#if !defined( __APPLE__ ).type  coctx_swap, @function#endifcoctx_swap:#if defined(__i386__)    movl 4(%esp), %eax    movl %esp,  28(%eax)    movl %ebp, 24(%eax)    movl %esi, 20(%eax)    movl %edi, 16(%eax)    movl %edx, 12(%eax)    movl %ecx, 8(%eax)    movl %ebx, 4(%eax)    movl 8(%esp), %eax    movl 4(%eax), %ebx    movl 8(%eax), %ecx    movl 12(%eax), %edx    movl 16(%eax), %edi    movl 20(%eax), %esi    movl 24(%eax), %ebp    movl 28(%eax), %esp ret#elif defined(__x86_64__) leaq (%rsp),%rax    movq %rax, 104(%rdi)    movq %rbx, 96(%rdi)    movq %rcx, 88(%rdi)    movq %rdx, 80(%rdi) movq 0(%rax), %rax movq %rax, 72(%rdi)    movq %rsi, 64(%rdi) movq %rdi, 56(%rdi)    movq %rbp, 48(%rdi)    movq %r8, 40(%rdi)    movq %r9, 32(%rdi)    movq %r12, 24(%rdi)    movq %r13, 16(%rdi)    movq %r14, 8(%rdi)    movq %r15, (%rdi) xorq %rax, %rax    movq 48(%rsi), %rbp    movq 104(%rsi), %rsp    movq (%rsi), %r15    movq 8(%rsi), %r14    movq 16(%rsi), %r13    movq 24(%rsi), %r12    movq 32(%rsi), %r9    movq 40(%rsi), %r8    movq 56(%rsi), %rdi    movq 80(%rsi), %rdx    movq 88(%rsi), %rcx    movq 96(%rsi), %rbx leaq 8(%rsp), %rsp pushq 72(%rsi)    movq 64(%rsi), %rsi ret#endif
```

### 3.5 退出协程

同协程挂起一样，协程退出时也应将 CPU 控制权交给它的调用者，这也是调用 co_yield_env() 函数来完成的。

我们调用 co_create()、co_resume() 启动协程执行一次性任务，当任务结束后要记得调用 co_free()或 co_release() 销毁这个临时性的协程，否则将引起内存泄漏。

```
void co_free( stCoRoutine_t *co ){    if (!co->cIsShareStack)    {        free(co->stack_mem->stack_buffer);        free(co->stack_mem);    }    //walkerdu fix at 2018-01-20    //存在内存泄漏    else    {        if(co->save_buffer)            free(co->save_buffer);        if(co->stack_mem->occupy_co == co)            co->stack_mem->occupy_co = NULL;    }    free( co );}void co_release( stCoRoutine_t *co ){    co_free( co );}
```

## 4.**补充**

### 4.1 协程的调度

co_eventloop() 即“调度器”的核心所在。这里讲的“调度器”，严格意义上算不上真正的调度器，只是为了表述的方便。libco 的协程机制是非对称的，没有什么调度算法。在执行 yield 时，当前协程只能将控制权交给调用者协程，没有任何可调度的余地。Resume 灵活性稍强一点，不过也还算不得调度。如果非要说有什么“调度算法”的话，那就只能说是“基于 epoll/kqueue 事件驱动”的调度算法。“调度器”就是 epoll/kqueue 的事件循环。

```
void co_eventloop( stCoEpoll_t *ctx,pfn_co_eventloop_t pfn,void *arg ){ if( !ctx->result ) {  ctx->result =  co_epoll_res_alloc( stCoEpoll_t::_EPOLL_SIZE ); } co_epoll_res *result = ctx->result; for(;;) {  int ret = co_epoll_wait( ctx->iEpollFd,result,stCoEpoll_t::_EPOLL_SIZE, 1 );  stTimeoutItemLink_t *active = (ctx->pstActiveList);  stTimeoutItemLink_t *timeout = (ctx->pstTimeoutList);  memset( timeout,0,sizeof(stTimeoutItemLink_t) );  for(int i=0;i<ret;i++)  {   stTimeoutItem_t *item = (stTimeoutItem_t*)result->events[i].data.ptr;   if( item->pfnPrepare )   {    item->pfnPrepare( item,result->events[i],active );   }   else   {    AddTail( active,item );   }  }  unsigned long long now = GetTickMS();  TakeAllTimeout( ctx->pTimeout,now,timeout );  stTimeoutItem_t *lp = timeout->head;  while( lp )  {   //printf("raise timeout %p\n",lp);   lp->bTimeout = true;   lp = lp->pNext;  }  Join<stTimeoutItem_t,stTimeoutItemLink_t>( active,timeout );  lp = active->head;  while( lp )  {   PopHead<stTimeoutItem_t,stTimeoutItemLink_t>( active );            if (lp->bTimeout && now < lp->ullExpireTime)   {    int ret = AddTimeout(ctx->pTimeout, lp, now);    if (!ret)    {     lp->bTimeout = false;     lp = active->head;     continue;    }   }   if( lp->pfnProcess )   {    lp->pfnProcess( lp );   }   lp = active->head;  }  if( pfn )  {   if( -1 == pfn( arg ) )   {    break;   }  } }}
```

在关键数据结构 stCoRoutineEnv_t 中，有一个变量 stCoEpoll_t 类型的指针，即与 epoll 事件循环相关。

- iEpollFd：epoll 实例的文件描述符
- *EPOLL*SIZE：一次 epoll_wait 最多返回的就绪事件个数
- pTimeout：时间轮定时器
- pstTimeoutList：存放超时事件
- pstActiveList：存放就绪事件/超时事件
- result：epoll_wait 得到的结果集

```
struct stCoEpoll_t{ int iEpollFd; static const int _EPOLL_SIZE = 1024 * 10; struct stTimeout_t *pTimeout; struct stTimeoutItemLink_t *pstTimeoutList; struct stTimeoutItemLink_t *pstActiveList; co_epoll_res *result;};
```

一般而言，使用定时功能时，我们首先向定时器中注册一个定时事件（Timer Event），在注册定时事件时需要指定这个事件在未来的触发时间。在到了触发时间点后，我们会收到定时器的通知。

网络框架里的定时器可以看做由两部分组成：

- 第一部分是保存已注册 timer events 的数据结构，第二部分则是定时通知机制。保存已注册的 timer events ，一般选用红黑树，比如 nginx；另外一种常见的数据结构便是时间轮，libco 就使用了这种结构。
- 第二部分是高精度的定时（精确到微秒级）通知机制，一般使用 getitimer/setitimer 这类接口，用 epoll/kqueue 这样的系统调用来完成定时通知。

### 4.2 何时挂起何时恢复

libco 中有 3 种调用 yield 的场景：

- 用户程序中主动调用 co_yield_ct()；
- 程序调用了 epoll() 或 co_cond_timedwait() 陷入“阻塞”等待；
- 程序调用了 connect(), read(), write(), recv(), send() 等系统调用陷入“阻塞”等待。

resume 启动一个协程有 3 种情况：

- 对应用户程序主动 yield 的情况，这种情况也有赖于用户程序主动将协程 co_resume() 起来；
- epoll() 的目标文件描述符事件就绪或超时，co_cond_timedwait() 等到了其他协程的 co_cond_signal() 通知信号或等待超时；
- read(), write() 等 I/O 接口成功读到或写入数据，或者读写超时。

原文作者：alexzmzheng

原文链接：https://mp.weixin.qq.com/s/sy26w9XVvQsQoVhbQoN3vQ

# 【NO.78】如何做一款面向企业客户的商用级 SDK

## 0.**导读**

我在 2008 年进入公司后，做的一直是面向 C 端用户的客户端产品—QQ，产品的可测性是很强的，虽然功能很多，但我们测试团队总是能成为产品质量的坚强后盾。2016 年我们团队加入腾讯云之后，依然在客户端方向，但所做的产品已经不再是一款软件，而是一套音视频通信领域的 PaaS SDK，即 [TRTC SDK](https://cloud.tencent.com/product/trtc) 和 [IM SDK](https://cloud.tencent.com/product/im)。

相比于 QQ 只需要做好一款 App，我们要面对的是服务好几千个客户的 App，而于此同时，测试资源又是有限的。在这种情况下，如何确保产品质量呢？

## **1.从一个小故事开始讲起**

在几年前我们刚开始做 ToB 的 SDK 的时候，曾经对接过一家做社交 App 的公司，对方的技术负责人很年轻也很务实。在商务大哥给力的努力下，客户成功完成了产品的接入，并进入线上灰度阶段。

然而，在开始灰度的那天晚上，线上用户出现了很多消息延迟大的投诉，用户的一条消息需要很长时间才能发出去。客户虽然对我们很失望，但依然很努力地在配合我们排查和修复问题。

在两天的时间里，我们给客户改进了多个版本，每次给版本的时候我们都说“已经找到问题了，这个版本肯定可以”，但每次效果都不理想。两天之后，客户的技术负责人很严肃地询问了我们一个问题：“从这两天的排障过程和修复过程来看，我想确认一下你们这是一款商用级的产品吗？”

在那个晚上，我们开始冷静地思考一个问题：**一款优秀的商用级 SDK 应该怎么做？**

## 2.**一年的努力功亏一篑**

最近教育行业被政策打压地非常厉害，但在两年前，这是个 PaaS 服务的兵家必争之地。我们有一家做在线英语教学的客户，一直在对接我们的 TRTC。这个客户对质量要求非常苛刻，他们很早便引入了赛马机制，将多家 PaaS 服务商拉入到自己的供应商集群，互为灾备，并进行质量评估，看谁的质量好就用谁的产品。

在最开始对接的时候，我们的产品质量还不是很优秀，几个关键指标跟竞品都有差距。这倒不是问题，优化总要有一个过程，于是我们一个迭代一个迭代地去跟进。因为客户的版本发布速度非常慢，所以我们需要在两个版本之间都做好问题分析和优化落地，稳抓稳打地慢慢降低工单率。就这样，经过了将近一年的时间，产品各项指标都已经很不错了，我们非常有信心在下一个版本超过友商获得质量上的领先地位。

但就在我们信心满满地等待客户上线的灰度反馈时，客户突然抛出一个问题：“你们的 SDK 有一个对音频模块的自动重启逻辑，这个逻辑会在切换线路时影响到其他供应商的音频模块， 这是绝对不能容忍的”。因为引入多家供应商赛马的意义就在于保证可以有灾备，如果一家供应商影响了其他供应商的稳定性，这个灾备便没有了意义，因此客户对我们异常失望，放量计划无限期搁置。

面对一年的努力功亏一篑，我们开始接受一个现实：**每位同事都可能会因为手滑引入缺陷，但对团队而言代价却是难以承受的，怎么办？**

回到正题，接下来我会介绍一下过去的这些日子里，我们怎么去应对这个问题。不过在此之前，我需要先介绍一下我们的产品：

## **3.我们是做什么的？**

我们团队所开发的是一套面向企业级客户的 SDK，包括用于实时音视频通讯的 [TRTC SDK](https://cloud.tencent.com/product/trtc)；用于消息通信的 [IM SDK](https://cloud.tencent.com/product/im)；用于直播推流和播放的 [LIVE SDK](https://cloud.tencent.com/product/mlvb) ；以及用于短视频录制和编辑的 [UGC SDK](https://cloud.tencent.com/product/ugsv)。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151408332763584.png)

产品面向的客户群很多：有做泛互联网行业的，比如在社交领域长期霸榜苹果应用商店的某知名 App；也有在线教育领域的很多知名机构，教学模式包括 1V1、小班课、大班课等等；也有金融和保险领域的巨无霸，他们会使用我们的产品将现有的业务尽快地跟互联网融合；还有各行各业的中小型企业，他们虽然可能并不出名，但确实支撑我们国家互联网经济持续繁荣的基石；对了，还有做毕业设计的学生，虽然他们不会付费，但保不齐人家会在毕业后给自己的老板推荐我们的产品呢。

面对这么多行业领域的客户，有喜有忧，喜的是这是一桩很好的生意，忧的是这里有着车载斗量的压力：因为 SDK 这种形态的技术产品，如果要面向企业客户去服务，那真是**打从娘胎里一出来就注定了坎坷的一生**。

首先是**客户群体**：

- 客户所属行业分布广，教育、泛互、金融，不同的客户对产品的需求差异性大。
- 客户接入周期长，大客户在接入过程中会不断追加新需求和新特性，与此同时，客户对交付周期要求又很苛刻。

然后是**产品形态**：

- SDK 对专业性要求是比较高的，别人家的客户都只需要理解 http 的 get 和 post 就行了，俺们家的客户就得知道多线程安全、内存泄漏、前后台切换，苹果隐私合规要求，还有 android 的 gradle 配置方法和 windows 的 stl 兼容问题…
- 涉及平台众多，iOS、Android、Windows、Mac、Web，每个方向都需要很长时间的积累和沉淀。同时，在微信的强大的影响力面前，我们又增加了一个新的平台——微信小程序。

最后是**交付成本**：

- SDK 完成接入后，成不成要依赖客户的最终反馈，但往往客户的反馈周期很长，迭代周期也很长。
- SDK 版本多，平台多，这也就意味着测试工作是海量的。就说一个细节，这么多平台和版本，全量编译都需要两个小时，转测和发版就更不用说了。

面对这个问题，我们的友商做法是：**加人**。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151408501987182.png)

当然，我们不能这么简单粗暴，毕竟粗放型经济是走不远的，我们还是得从研发体系上用集约的思想去解决问题，这就是接下来要说的重点：从研发、产品、数据和排障等四个方向去认认真真做好一款面向企业服务的 SDK 产品。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151408583309952.png)

## **4.研发体系的优化**

在研发体系方面，我们依然遵循腾讯倡导的需求评审=>技术评审=>开发=>测试的流程。但每个环节，我们都结合自身的特点进行了改进。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151409061184639.png)

### 4.1 怎么做需求评审？

首先是需求评审，我们团队经过这几年的打拼，总结出来最关键的一个点，就是看需求一定要看客户背后的意图。有时候客户会跟你说：“我想要你给我增加一个设置视频分辨率和码率的接口”。这个时候你要不要加呢？如果我们只是看客户的需求，那是要加的。但如果我们再问问，“您为什么需要我们加这个接口呢？” 那客户可能就会跟你说：“我觉得你们的画质不行，不够清楚，我要自己调，我要调清楚一点。” 这个时候我们就明白了，我们的需求不是“去增加一个可以设置视频分辨率和码率的接口”，而是去“提升我们的画质以满足客户的需求”。

这两者是不对等的，因为前者客户可能认为只要分辨率调到 4K 就是清晰的，但客户可能误以为“清晰度”就等同于“分辨率”，所以往往会指定一个 4K 的分辨率，却配置了一个 40Kbps 的视频码率。懂音视频的朋友都知道，这样的画面是模糊地没法看得。所以我们在简单版的 API 接口中，都不开分辨率设置接口，而仅仅是提供一个画质等级的接口，以避免客户的错误配置。但我们在得知客户的意图之后，会去了解客户为什么觉得我们画质不行，是跟哪款产品比有差距。进而分析是提升颜色矩阵转换的精度，还是在前处理的最后增加一道锐化，还是视频分辨率不匹配显示分辨率导致的问题，还是 OpenGL 的线性变换和就近变化的差异问题。

### 4.2 怎么做技术评审？

这个部分，我们一般会鼓励大家提供两个以上的方案，然后进入“左右互搏”的模式。因为很多可爱的同事本身也是可爱的急性子，只要能早点写代码，什么都是不重要的。毕竟咱们做研发的成就感，不就来自于把功能做出来看到自己的成果吗。

但我们也不断地告诫自己，我们究竟是做“一票子买卖”还是是“百年老店的生意”。如果是前者，那大可以想到哪里代码就写到哪里；但如果是后者，则需要我们综合考虑多个方案，选择更能可持续发展的方案。

要知道在 ToB 这个领域，我们一不注意就会把自己陷入到做定制需求的套路里。面对业务压力，一开始这样是很解渴的，但随后的维护成本就让自己彻底吃不消了，每天除了救火什么都干不了的团队，也就失去了创造新价值的能力。

### 4.3 怎么做代码合入？

在代码合入方面，我们团队在很早的时候就引入了一套非常严格的代码评审流程，即三级评审：

- CR 一级：模块的维护者来 review，这一级的目的是让模块的稳定性能够得到保障。毕竟在别人家的田间地头种自己的庄稼，你总不能背着这块地的主人搞小动作不是。
- CR 二级：自己的 leader 来 review，这是我们整个 CR 的核心基石。很幸运团队有像 taopu 一样负责和认真的 leader，会非常细致的 review 大家的每一行代码，并积极提出意见和建议，在 CR 中提升大家的技术水平。
- CR 三级：总监负责 review 和 代码合入，这一步更多是抽检，看看哪些同事是真正地爱这款产品，哪些同事则是不那么负责的架构破坏者。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151409182850117.png)

### 4.4 怎么做功能测试？

最近半年在测试团队，尤其是 svein 和俊哥的大力支持下，我们的自动化测试进步极大。不管是 native sdk 还是 webrtc sdk，自动化测试都能覆盖掉很多刁钻和难以手工覆盖的部分。比如一次通话过程中几十次的“进进出出”，或者是频繁的切换某个状态，这些都是以往手工测试很容易把人逼疯的部分。益于测试团队的持续投入，目前我们的自动化测试系统已经小有成绩。要知道，构建一个面向音视频功能的自动化测试体系，那难度可是非常高的。仔细想想就知道这里面有多少破事儿要解决：

- 怎么确认画面出来了？
- 怎么确认声音是正常的?
- 怎么构造复杂的测试流程和测试序列?
- 怎么保证测试环境的稳定和不被干扰?
- 还有最艰苦的：怎么找到足够多又耐操的手机，尤其是水果牌的。

通过需求评审、技术评审、代码审查和自动化测试的多重保护，我们最近已经很久没有再发生前面说的第二个故事里的事情了。即使有些同事一时手滑引入了一些问题，也大都能在 SDK 交付前得到暴露，只是目前我们并不能将这个概率降低到 0% 而已。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151409286401567.png)

## **5.产品体系的优化**

作为一款自身不带界面的 SDK，要做到产品体系的优化，就只能去优化技术本身，但这是枯燥且不好度量的。俗话说得好，“文无第一，武无第二”，说的就是评判标准的问题。这就好比你画一幅画，如果没有老师指点你怎么才算好，那就会很难度量自己这段时间是水平提高了还是退步了。不然人家丢勒一个德国人，干嘛两次跑到意大利的威尼斯去学画画；又不然怎么会出现很多画家都是在那啥之后才有人开始欣赏他们的作品的呢？

所以说，作为一款面向 ToB 客户的 SDK 产品，要提升产品质量，就得有一些手段和方法，我们是这么做的：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151409386555801.png)

### 5.1 方法一：通过场景落地来验证产品质量

我们做 TRTC SDK，我们的客户拿 TRTC SDK 的能力去做合唱，去做 K歌，去做语音聊天室，去做视频直播。那我们就只是做好自己的一亩三分地吗？

当然不行，所以我们自己也实打实地开发了一些面向行业场景的 App（也可说是 Demo），比如合唱、语聊、教育、直播等等。并在这些场景的开发过程中，不停地寻找产品的问题和不足，并持续打磨，以确保在产品交付客户之前，在产品体验上就已经达到了一个很好的水平。

比如我们在开发在线合唱的场景时，就经常有人找我问：“rex，我跟你确认一下哈，咱们团队里的同学，究竟都是写代码的程序员，还是想要通过《我是歌手》来改变人生的麦霸？”

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151409558692327.png)

### 5.2 方法二：通过数据体系来评估产品质量

构建一套靠谱的数据体系很重要，这就是把“文无第一”的事情变成“武无第二”。通过数据体系，让所有的指标都变成可以比较的数字，并且依托数据分析系统，不断地提升产品质量。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151410046803486.png)

虽然这个思路大家都很清楚，也都在各自的产品中有所落地，毕竟咱们腾讯的产品团队，谁家还没有一个负责数据运营的同事呢？当然有些比较大的业务，都是有自己的数据运营团队的。

但还是得说，这个事情在 toB 的方向上不好做，难在两点：

- **不同的客户关注的点是不一样**：比如教育客户关注的是稳定性，电商直播关注的是清晰度，秀场直播关注的则是音质。如果我们给一个在线教育客户去过度地优化画质，客户不仅可能不买账，还可能因为我们的优化影响了其他指标而弃用我们的产品。
- **音视频的表现不是简单地靠 DAU、成功率来衡量的**。比如“切课率”这个指标，影响因素非常多，比如网络波动呀，硬件发热呀，麦克风阻抗大呀，显卡驱动不匹配呀，还有可能是用户心情不好砸键盘呀。就说我们有个客户，发现上课的声音效果不好，结果客户很负责，亲自到了学生家去确认，最后发现是 iPad 的保护套把麦克风给遮住了，你说这找谁说理去？

## 6.**数据体系建设**

面对上述挑战，我们还是得从技术角度去解决问题，毕竟靠堆人是不行的，这生意得做出毛利率才能长久地坚持下去。

庆幸的是这方面我们还是做得不错的，尤其是我们团队一向比较在意数据，团队里还有一等一的聪明脑袋负责数据体系的建构。比如我们在自己的引擎内部的各个关键模块都做了数据“挂节点”。这些模块会每时每刻将近百个技术指标以一秒一次的频率反馈给统计模块，在统计模块进行汇总之后，再实时上传到服务器上。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151410141341597.png)

基于这些海量的数据信息，仅仅靠 group by 和 count 、where 等 SQL 语句做简单的统计分析是肯定没用的，因为这样的分析得不出任何有价值的信息。

比如一次糟糕的通话体验，可能出现过一次 2s 的卡顿，但是这些数值如果仅仅是用来做大盘平均分析，那这次 2s 的卡顿就“淹没”在了海量的通话数据里，你拿到的最终的平均值甚至不会有小数点上的一个波动。

针对这个问题，团队中的 xuanyi 和 yuting 两位同事，基于对以往 badcase 的经验综合分析，构建了一套“根因分析系统”，并用了将近半年的时间，不断地打磨其准确性。到目前为止，这套系统对于 badcase 的分析已经接近人工挨个 case 分析的准确性，为团队节省了不知道多少人力。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151410239684953.png)

## 7.**排障体系**

回到最初的小故事，客户之所以怀疑我们的产品不是一款商用级的产品，最大的问题就在排障体系上。因为客户也不是最终用户，客户在面对自己用户的反馈和投诉时，往往也是很难拿到第一手信息的。如果我们将排障过程演化成了：我们 <=> 客户<=>客户的用户，之间的复杂关系，这个事情就很容易引发矛盾和冲突。

所以我们在接受了早期的失败教训之后，就励精图治建设了一套商业级的排障系统。经过这几年的努力，这套系统已经越来越强大了，也承载了越来越多的能力。目前已经能够做到分析过去两周内任何一个用户的任何一次体验问题，并能够定位到技术层面的缺陷或者环境方面的问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151410342950122.png)

于此同时，在线日志和离线日志系统的双重保障，也让排障的信息变得更加容易获取。以往比较困难的线上死锁问题和调用时序问题，也开始不再那么可怕和束手无措。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151410438997892.png)
当然，面对这么多的客户，靠一个团队的人力是不可能搞定数千个客户的技术支持和售后服务的，靠两个也不行。不过作为一款腾讯云上的老产品，我们的 TRTC 和 IM 很早就接入了腾讯云的安灯系统。借助安灯的问题跟踪和信息流转能力，中小客户的问题也得到及时的处理和沉淀。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151410539361168.png)

## 8.**总结**

从 2016 年加入腾讯云，团队到今天已经走过了五年，我们用了五年时间去学习如何做一款商用级的 SDK。虽然现在来说，我们做得还不够好，但至少可以回答几年前客户的质问，我们现在还是有信心并且有能力做好一款商用级的 SDK 产品的，而且不止一个。

原文作者：rexchang，腾讯 CSIG 客户端开发工程师

原文链接：https://mp.weixin.qq.com/s/DcDZad4UP4VUSWRnfVy9MQ

# 【NO.79】消息队列背后的设计思想

消息队列也通常称为消息中间件，提到消息队列，大部分互联网人或多或少都听过该名词。对于后端工程师而言，更是日常开发中必备的一项技能。随着大数据时代的到来，apache 旗下的 kafka 一度成为消息队列的代名词，提起消息队列大家自然而然就想到了 kafka。近而网上有太多太多介绍消息队列 kafka 功能或者内部实现的文章。

然而消息队列本身是工程领域内一种解决问题的通用方案。它的背后有着一些通用的设计思想和经典模型，这些是消息队列的精髓和灵魂。它们独立于任何一种消息队列的具体实现(例如 kafka)，但每种消息队列(除了 kafka 外，还有 rocketMQ、pulsar 等)的实现中到处体现着这些设计思想。本文主要从抽象层面来简单谈谈消息队列背后的一些设计思想，辅助理解消息队列这一类组件。

本文主要解决三个问题：

1. 消息队列适合什么场景？
2. 消息队列有哪些主流产品、各自的优缺点？
3. 消息队列背后的设计思想(整体核心模型、数据存储考量、数据获取方案对比、消费者消费模型)

### **1.消息队列适合哪些场景？**

**消息队列：它主要用来暂存生产者生产的消息，供后续其他消费者来消费。它的功能主要有两个：a.暂存(存储)、b.队列(有序：先进先出)。其他大部分场景对数据的消费没有顺序要求，主要用它的暂存能力** 。从目前互联网应用中使用消息队列的场景来看，主要有以下三个：
**1. 异步处理数据**
**2. 系统应用解耦**
**3. 业务流量削峰**

下面对上述每一种场景进行简单描述。

#### 1.1 异步处理数据

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151424281393776.png)

第一个例子我们以现实生活中**送快递**来类比，在该例子中我们把**暂存快递的快递柜**比作**暂存数据的消息队列**。我们来看一下在现实生活中，没有快递柜时，快递员把快递送到目的地后，一般需要把联系收货人来签收快递，如果收货人此时有空，那一切都很顺利。但如果收货人此时不方便(开会、正在吃午饭、外出出差)。那对于快递员而言，就很尴尬，需要一直等待(开会 or 吃午饭)或者将快递拿回去(外出出差)，导致白跑一趟。这对于快递员而言简直太不友好。

从这儿可以看出，当快递员送货时，是一个同步状态，即需要等待收货人签收后才能去送下一趟单子，对快递员而言效率太低。上述例子虽然有点牵强，大家凑合理解，意思能大概理解到位就 ok。

接着我们再来看一下，当有了快递柜后，对于快递员而言，每次需要送快递时，只需要将快递投掷到快递柜，然后再通过短信或者电话通知收货人具体的快递信息即可。他就可以继续去派送下一单。而对于收获人而言，也可以根据具体方便的时间来取件。这样一来，二者完全异步了，不用相互等待了。

在这个例子中，如果把快递员比作生产者，收货人比作是消费者，则快递柜就类似于消息队列。我们可以通过采用消息队列来实现异步数据的处理。

#### 1.2 系统应用解耦

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151424396770166.png)

案例二我们以目前最主流的推荐系统中内容的流转来举例。在推荐系统中当创作者发布了一条内容后，该内容会首先经过安全部分的相关审核，通过审核后的内容，通常需要进行内容入库存储、送入模型进行特征的计算和生成。

假如后期我们想提升推荐的效果，需要单独构建一份冷启动的推荐池，此时也需要用到这部分内容，那问题来了，在没有使用消息队列时，对于上游服务而言，需要通过扩展新的逻辑来实现该功能。同时在该场景里，会存在依赖三个下游服务，如果其中一个下游服务失败后，该如何处理，是重试还是返回失败等这些细节的处理。如果后期这部分数据还想在其他渠道分发，那又该如何对接。明显这种场景下面临着系统紧耦合的问题。

我们再来看一下，如果我们一开始就引入了消息队列，那问题又会变成怎样的呢？当内容审核通过后，就直接将数据生产出来丢到消息队列中，下游的多个服务再从消息队列消费数据。当后续这一份数据需要扩展供其他系统使用时，也只要通过新的消费者来接入到消息队列消费就 ok。上游生产消息的模块不要做任何的改动。这样我们就通过消息队列进行了系统应用之间的解耦。这是消息队列的第二个用途。

#### 1.3 业务流量削峰

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151424498148740.png)

消息对应的第三个使用场景便是**削峰**。在现如今的互联网世界中，电商场景中每年的 618 秒杀活动、双 11 抢购便是最典型的案例。这种场景中系统的峰值流量往往集中于一小段时间内，平常的流量比较可控，所以为了防止系统在短时间内的峰值流量冲垮，往往采用消息队列来削弱峰值流量，高峰值期间产生的订单消息等数据首先送入到消息队列中暂存，然后供下游系统根据自己的消费能力来逐步处理。同时这类消息往往对时延的要求不是很高，比较适合采用消息队列暂存。

#### 1.4 小结

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151424596751835.png)

最后我们在对本节的内容做一个简单的总结，上面通过三个简单的实例介绍了消息队列的典型的三个使用场景：**异步**、**解耦**、**削峰**。换个角度来理解可以看到，消息队列主要适用于处理**对消息要求不是很实时，同时一份数据可能会多处使用的场景，不同的使用方处理速率不同。**更多的消息队列的使用场景读者可以自行找资料阅读和总结。

### **2.有哪些消息队列(解决方案)？**

#### 2.1 消息队列主流产品

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151425096063234.png)

上图根据时间线展示了不同时间点产生的消息列队产品，主要的产品有：ActiveMQ(2003)、RabbitMQ(2006)、Kafka(2010)、RocketMQ(2011)、Pulsar(2012)。这些消息队列中或多或少我们都听过一些，部分也在项目中真实使用过。下面对上述几个消息队列做一个简单的介绍。

**ActiveMQ：** ActiveMQ 由 Apache 软件基金会基于 Java 语言开发的一个开源的消息代理。能够支持多个客户机或服务器。计算机集群等属性支持 ActiveMQ 来管理通信系统。

**RabbitMQ：** RabbitMQ 是实现了高级消息队列协议（AMQP）的开源消息代理软件（亦称面向消息的中间件）。RabbitMQ 服务器是用 Erlang 语言编写的，而集群和故障转移是构建在开放电信平台框架上的。所有主要的编程语言均有与代理接口通讯的客户端库。RabbitMQ 支持多种消息传递协议、传递确认等特性。

**Kafka：** Apache Kafka 是由 Apache 软件基金会开发的一个开源消息系统项目，由 Scala 写成。Kafka 最初是由 LinkedIn 开发，并于 2011 年初开源。2012 年 10 月从 Apache Incubator 毕业。该项目的目标是为处理实时数据提供一个统一、高通量、低等待的平台。Kafka 是一个分布式的、分区的、多复本的日志提交服务。它通过一种独一无二的设计提供了一个消息系统的功能。

**RocketMQ：** Apache RocketMQ 是一个分布式消息和流媒体平台，具有低延迟、**强一致**、高性能和可靠性、万亿级容量和灵活的可扩展性。它有借鉴 Kafka 的设计思想，但不是 kafka 的拷贝。

**Pulsar：** Apache Pulsar 是 Apache 软件基金会顶级项目，是下一代云原生分布式消息流平台，集消息、存储、轻量化函数式计算为一体，采用计算与存储分离架构设计，支持多租户、持久化存储、多机房跨区域数据复制，具有**强一致性**、高吞吐、低延时及高可扩展性等流数据存储特性，被看作是云原生时代实时消息流传输、存储和计算最佳解决方案。

#### 2.2 不同消息队列对比

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151425209549929.png)

上图详细的展示了几种消息队列的各自功能及优缺点，首先，ActiveMQ 和 RabbitMQ 二者属于同一量级，在吞吐量上比后面三者差一个量级；其次，支持强一致性的有 RocketMQ 和 Pulsar，在对消息一致性要求比较高的场景可以采用这些产品。同时 kafka 虽然会有数据丢失的风险，但其吞吐量比较高同时社区非常活跃，在大数据的绝大部分场景里，都可以采用 kafka；最后 kafka、RocketMQ、Pulsar 这几款是基于磁盘存储数据的，内存加速访问。而 ActiveMQ、RabbitMQ 采用内存存储数据，也支持数据持久化到磁盘。

### **3. 消息队列背后的设计思想**

在前面，第一节内容中，主要介绍了为什么要使用消息队列，消息队列适合解决哪些问题？在第二节内容中，又介绍了有哪些可选择的消息队列，以及他们之间各自的优缺点。这一节是最重要的内容，主要会介绍一下上述消息队列背后的通用的一些设计思想。部分思想可以扩展到其他的业务模型或者领域内。后面讲到对应内容也会有所提及。

#### 3.1 消息队列核心模型

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151425328494917.png)

上图是几乎所有消息队列设计的一个核心模型。对于一个消息队列而言，从数据流向的维度，可以拆解为三大部分：**生产者**、**消息队列集群**、**消费者**，数据是从生产者流向消息队列集群，最终再从消息队列集群流向消费者，下面对这几个概念进行一一阐述。

**生产者：** 生产数据的服务，通常也称为数据的输入提供方，这里的数据通常指我们的业务数据，例如推荐场景中用户对内容的点击数据、内容曝光数据、电商中的订单数据等等。生产者通常是作为客户端的方式存在，但在支持事务消息的消息队列中，生产者也被设计为服务端，实现事务消息这一特性。其次生产者通常会有多个，消息队列集群内部也会有多个分区队列，所以在生产者发送数据时，通常会存在负载均衡的一些策略，常见的有**按 key hash**、**轮询**、**随机**等方式。其本质是**一条数据，被消息队列封装后也被称为一条消息，该条消息只能发送到其消息队列集群内部的一个分区队列中。因此只需按照一定的策略从多个队列中选择一个队列即可**。

**消息队列集群：** 消息队列集群是消息队列这种组件实现中的核心中的核心，它的主要功能是**存储消息**、**过滤消息**、**分发消息**。

其中存储消息主要指生产者生产的数据需要存储到消息队列内部；存储消息可以说是消息队列的核心，一个消息队列吞吐量的高低、性能优劣都和它的存储模型脱不开关系。这部分内容会在 3.2 节进行介绍。

过滤消息只指消息队列可以通过一定的规则或者策略进行消息的过滤，该项能力通常也被称为消息路由；过滤消息属于高阶的特性功能，AMQP 协议对这些能力抽象的比较完备，部分消息队列可以选择性的实现该协议来达到该功能，关于 AMQP 协议内容读者可以自行搜索资料阅读，此处不再展开。

分发消息是指消息队列通常需要将消息分发给处理同一逻辑的多个消费者处理或者处理不同逻辑的不同消费者处理。分发消息可以说和消费者模型想挂钩，这块会涉及到不同的数据获取方式，也会涉及到消费者消费消息的模型。这两部分内容会在 3.3 节和 3.4 节进行重点介绍。

此外绝大部分的消息队列也都支持对消息进行分类，分类的标签称为**topic(主题)**，一个 topic 中存放的是同一类消息。

**消费者：** 最终消息队列存储的消息会被消费者消费使用，消费者也可以看做消息队列中数据的输出方。消费者通常有两种方式从消息队列中获取数据：**推送(push)数据**、**拉取(pull)数据**，3.3 节中会对这两种方案进行详细对比说明。其次消费者也经常是作为客户端的角色出现在在消息队列这种组件中。

#### 3.2 消息队列数据组织方式

在这一节中，我们详细看看消息队列**存储消息**这个环节的一些权衡考量，通常数据的存储无外乎就是两种，一种是存储在**非易失性存储**中，例如磁盘这种介质；另一种是选择存储在**易失性存储**中，典型的就是内存。关于二者的对比大家可以参考下表，此处就不再赘述。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151425452332884.png)

通常在大部分组件设计时，往往会选择一种主要介质来存储、另一种介质作为辅助使用。就拿 redis 来说，它主要采用内存存储数据，磁盘用来做辅助的持久化。拿 RabbitMQ 举例，它也是主要采用内存存储消息，但也支持将消息持久化到磁盘。而 RocketMQ、Kafka、Pulsar 这种，则是数据主要存储在磁盘，通过内存来主力提升系统的性能。关系型数据库例如 mysql 这种组件也是主要采用磁盘组织数据，合理利用内存提升性能。

针对采用内存存储数据的方案而言，难点一方面在于如何在不降低访问效率的情况下，充分利用有限的内存空间来存储尽可能多的数据，这个过程中少不了对数据结构的选型、优化；另一方面在于如何保证数据尽可能少的丢失，我们可以看到针对此问题的解决方案通常是快照+广泛意义的 wal 文件来解决。此类典型的代表就是 redis 啦。

针对采用磁盘存储数据的方案而言，难点一方面在于如何根据系统所要解决的特点场景进行合理的对磁盘布局。读多写少情况下采用 b+树方式存储数据；写多读少情况下采用 lsm tree 这类方案处理。另一方面在于如何尽可能减少对磁盘的频繁访问，一些做法是采用 mmap 进行内存映射，提升读性能；还有一些则是采用缓存机制缓存频繁访问的数据。还有一些则是采用巧妙的数据结构布局，充分利用磁盘预读特性保证系统性能。

**总的来说，针对写磁盘的优化，要不采用顺序写提升性能、要不采用异步写磁盘提升性能(异步写磁盘时需要结合 wal 保证数据的持久化，事实上 wal 也主要采用顺序写的特性)；针对读磁盘的优化，一方面是缓存、另一方面是 mmap 内存映射加速读**。

上述这些存储方案上权衡的选择在 kafka、RocketMQ、Pulsar 中都可以看到。其实抛开消息队列而言，这些存储方案的选择上无论是关系型数据库还是 kv 型组件都是通用的。

下图列举了几种磁盘上的数据组织方式，仅供大家参考。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151425552772473.png)

#### 3.3 获取数据的推、拉两种方案对比

在前面 3.1 节中提到，消费者在从消息队列中获取数据时，主要有两种方案：
**1. 等待推送数据**
**2. 主动拉取数据**

目前的消息队列实现时，都会选择支持两种的至少一种，关于这两种方案的对比，大家可以参考下表。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151426081211594.png)

在此处，个人想抛开消息队列谈一点关于这两种方案的理解，其实推拉模型不仅仅只用于消息队列这种组件中，更一般意义上，它解决的其实是数据传送双方的一个问题。本质是**数据需要从一方流向另一方**。顺着这个思路来看，下面这三个例子都是遵循这个原则。

**网络中传输的数据：** 在 IO 多路复用中，以 epoll 为例，当内核检测到监听的描述符有数据到来时，epoll_wait()阻塞会返回，上层用户态程序就知道有数据就绪了，然后可以通过 read()系统调用函数读取数据。这个过程就绪通知，类似于推送，但推送的不是数据，而是数据就绪的信号。具体的数据获取方式还是需要通过拉取的方式来主动读。

**feeds 流系统用户时间线后台实现方案(读扩散、写扩散)：** 读扩散和写扩散更是这样一个 case。对于读扩散而言，主要采用拉取的方式获取数据。而对于写扩散而言，它是典型的数据推送的方式。当然在系统实现中，更复杂的场景往往会选择**读写结合**的思路来实现。

**生活中的点外卖例子：** 当下单点外卖时，通常也会有两种方式可以选择，**外卖派送**、**到店自取**。不过通常**外卖派送**比较实时，我们通常就选择这种方式了而已。可以看出外卖派送其实就是一种推的方式，而到店自取，则是拉的方式。

#### 3.4 消息队列消费模型

本节我们来介绍最后一部分内容，消息队列中**消费者的消费模型**。下图中上半部分展示了最简单的一种消费模型。一个生产者、一个消费者。但往往我们的一份数据通常会被不同场景所使用。那这个时候，首先就会存在每种场景需要使用全量的数据、而且不同场景之间不会相关影响，彼此独立。方便理解起见，我们假设有 N 个场景需要使用这同一份数据，每个场景需要消费全量的数据。而在 N 个场景中的一种场景里，又会有多个消费者一起分摊消费这些数据。我们假设一个场景里有 M 个消费者。由于每个场景中包含 M 个消费者，我们也将其采用**消费者组**来描述。通过上面的介绍，我们可以用下面一句话总结消息队列中的消费模型：

**消费者消费者模型其实是一个 1:N:M 的关系，一份数据被 N 个消费者组独立使用，每个消费者组中有 M 个消费者进行分摊消费**

其实这种模型也称为**发布订阅模型**，对于一条消息而言，组间广播、组内单播。一条消息只能被一个消费者组中的一个消费者使用。在消费者组内部也存在一些负载均衡的策略。常用的有：**轮询**、**随机**、**hash**、**一致性 hash**等方案。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151426413270920.png)

#### 3.5 小结

第三部分内容我们重点介绍了关于消息队列背后的一些设计思想，其中包括：**消息队列的核心模型**、**数据存储模型**、**推拉方案获取数据对比**、**消费者消费模型**。其中数据存储模型不仅仅适用于消息队列，也同样适用于其他数据存储组件的方案选择。同理数据获取的推拉两种方案也不仅仅局限于消息队列。我们可以在很多业务场景里看到同类思想的影子。

### **4. 总结**

到此，本文也就告一段落了。本文主要从理论、抽象层面泛泛的谈了下关于消息队列的一些思想和理念。主要介绍了消息队列的使用场景，主流的消息队列可选方案以及他们之间的优缺点。最后介绍了一些关于消息队列背后的设计理念。本文只是抛砖引玉，希望上述内容能辅助大家一起重新认识消息队列。后面会逐步挑选上述的几种消息队列(kafka、RocketMQ、Pulsar)，重点分析其内部实现机制，敬请期待。限于个人水平有限，理解有误之处欢迎大家批评指正。

### **5. 参考资料**

1. [ActiveMQ 与 RabbitMQ 的区别](https://javakk.com/1721.html)
2. [Kafka、ActiveMQ、RabbitMQ、RocketMQ 区别以及高可用原理](https://cloud.tencent.com/developer/article/1408126)
3. [Kafka、RabbitMQ、RocketMQ 等消息中间件的对比](https://blog.csdn.net/belvine/article/details/80842240)
4. [Apache Pulsar 在腾讯计费场景下的应用](https://mp.weixin.qq.com/s?__biz=MzUxOTc4NDc2MQ==&mid=2247485123&idx=1&sn=d79f5f085646d75280455753ea7bc6fa&source=41&scene=21#wechat_redirect)
5. [kafka Push vs. pull](https://kafka.apache.org/documentation/#design_pull)
6. [消息队列-推/拉模式学习 & ActiveMQ 及 JMS 学习](https://blog.csdn.net/u014469692/article/details/79866330?utm_medium=distribute.pc_relevant.none-task-blog-baidujs_title-9&spm=1001.2101.3001.4242)

原文作者：jaydenwen，腾讯 PCG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/k8sA6XPrp80JiNbuwKaVfg

# 【NO.80】一文读懂 @Decorator 装饰器——理解 VS Code 源码的基础

## **1. 装饰器的样子**

我们先来看看 `Decorator` 装饰器长什么样子，大家可能没在项目中用过 `Decorator` 装饰器，但多多少少会看过下面装饰器的写法：

```
/* Nest.Js cats.controller.ts */import { Controller, Get } from '@nestjs/common';@Controller('cats')export class CatsController {  @Get()  findAll(): string {    return 'This action returns all cats';  }}
```

> 摘自[《Nest.Js》官方文档](https://docs.nestjs.cn/8/controllers)

上述代码大家可以不着急去理解，主要是让大家对装饰器有一个初步了解，后面我们会逐一分析 `Decorator` 装饰器的实现原理以及具体用法。

## **2. 为什么要理解装饰器**

### 2.1 浅一点来说，理解才能读懂 VS Code 源码

Decorator 装饰器是 **ECMAScript** 的语言提案，目前还处于 [stage-2](https://github.com/tc39/proposal-decorators) 阶段，但是借助 TypeScript 或者 Babel，已经有大量的优秀开源项目深度用上它了，比如：VS Code, Angular, Nest.Js(后端 Node.js 框架), TypeORM, Mobx(5) 等等

举个例子：https://github.com/microsoft/vscode/blob/main/src/vs/workbench/services/editor/browser/codeEditorService.ts#L22

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151433393286593.png)

作为一个有追求的程序员，你可能会问：上面代码的装饰器代表什么含义？去掉装饰器后能不能正常运行？

如果没弄懂装饰器，很难读懂 VS Code 这些优秀项目源码的核心思想。所以说你不需要熟练使用装饰器，但一定要理解装饰器的用法。

### 2.2 深一点来说，理解才能弄懂 AOP , IoC, DI 等优秀编程思想

***\*1.AOP 即面向切面编程 (Aspect Oriented Programming)\****

AOP 主要意图是将日志记录，性能统计，安全控制，异常处理等代码从业务逻辑代码中划分出来，将它们独立到非指导业务逻辑的方法中，进而改变这些行为的时候不影响业务逻辑的代码。

简而言之，就是“优雅”的把“辅助功能逻辑”从“业务逻辑”中分离，解耦出来。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151433501200461.png)

> 图摘自[《简谈前端开发中的 AOP(一) – 前端 AOP 的实现思路》](https://zhuanlan.zhihu.com/p/269504590)

***\*2.IoC 即 控制反转 (Inversion of Control)，是解耦的一种设计理念\****

***\*3.DI 即 依赖注入 (Dependency Injection)，是 IoC 的一种具体实现\****

使用 IoC 前：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151434005214608.png)

使用 IoC 后：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151434129000501.png)

> 图摘自[《两张图让你理解 IoC (控制反转)》](https://learnku.com/laravel/t/3845/the-two-picture-lets-you-understand-ioc-inversion-of-control)

IoC 控制反转的设计模式可以大幅度地降低了程序的耦合性。而 Decorator 装饰器在 VS Code 的控制反转设计模式里，其主要作用是实现 DI 依赖注入的功能和精简部分重复的写法。由于该步骤实现较为复杂，我们先从简单的例子为切入点去了解装饰器的基本原理。

## **3. 装饰器的概念区分**

在理解装饰器之前，有必要先对装饰器的 3 个概念进行区分。

### 3.1 Decorator Pattern (装饰器模式)

是一种**抽象的设计理念**，核心思想是**在不修改原有代码情况下，对功能进行扩展。**

### 3.2 Decorator (装饰器)

是一种**特殊的装饰类函数**，是一种对装饰器模式理念的具体实现。

### 3.3 [@Decorator](https://github.com/Decorator) (装饰器语法)

是一种**便捷的语法糖**(写法)，通过 `@` 来引用，需要编译后才能运行。理解了概念之后可以知道：**装饰器的存在就是希望实现装饰器模式的设计理念。**

说法 1：**在不修改原有代码情况下，对功能进行扩展**。也就是对扩展开放，对修改关闭。

说法 2：**优雅地把“辅助性功能逻辑”从“业务逻辑”中分离，解耦出来。**（AOP 面向切面编程的设计理念）

# **4. 装饰器的实战：记录函数耗时**

现在有一个 `关羽(GuanYu)` 类，它有两个函数方法：`attack(攻击)` 和 `run(奔跑)`

```
class GuanYu {  attack() {    console.log('挥了一次大刀')  }  run() {    console.log('跑了一段距离')  }}
```

而我们都是优秀的程序员，时时刻刻都有着经营思维 (性能优化)，因此想给 `关羽(GuanYu)` 的函数方法提前做好准备：记录关羽的每一次 `attack(攻击)` 和 `run(奔跑)` 的执行时间，以便于后期做性能优化。

### 4.1 做法一：复制粘贴，不用思考一把梭就是干

拿到需求，不用多想，立刻在函数前后，添加记录函数耗时的逻辑代码，并复制粘贴到其他地方：

```
class GuanYu {  attack() {+   const start = +new Date()    console.log('挥了一次大刀')+   const end = +new Date()+   console.log(`耗时: ${end - start}ms`)  }  run() {+   const start = +new Date()    console.log('跑了一段距离')+   const end = +new Date()+   console.log(`耗时: ${end - start}ms`)  }}
```

但是这样直接修改原函数代码有以下几个问题：

1. **理解成本高**

统计耗时的相关代码与函数本身逻辑并无关系，对函数结构造成了破坏性的修改，影响到了对原函数本身的理解

1. **维护成本高**

如果后期还有更多类似的函数需要添加统计耗时的代码，在每个函数中都添加这样的代码非常低效，也大大提高了维护成本

### 4.2 做法二：装饰器模式，不修改原代码扩展功能

#### **4.2.1 装饰器前置基础知识**

在开始用装饰器实现之前必须掌握以下基础：

1. [Object.getOwnPropertyDescriptor()](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Object/getOwnPropertyDescriptor)

返回指定对象上一个自有属性对应的**属性描述符**

```
var a = { b: () => {} }var descriptor = Object.getOwnPropertyDescriptor(a, 'b')console.log(descriptor)/** * { *   configurable: true,  // 可配置的 *   enumerable: true,    // 可枚举的 *   value: () => {},     // 该属性对应的值（数值，对象，函数等） *   writable: true,      // 可写入的 * } */
```

这里要注意一个点是：value 可以是 JavaScript 的任意值，比如函数方法，正则，日期等

1. [Object.defineProperty()](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Object/defineProperty)

在一个对象上定义或修改一个属性的描述符：

```
const object1 = {};Object.defineProperty(object1, 'property1', {  value: 'ThisIsNotWritable',  writable: false});object1.property1 = 'newValue';// throws an error in strict modeconsole.log(object1.property1);// expected output: 'ThisIsNotWritable'
```

#### **4.2.2 【重点】手写一个装饰器函数**

有了上面的两个基础后，我们开始利用装饰器模式的设计理念，用纯函数的形式写一个装饰器，实现记录函数耗时功能。为了让大家更深刻理解装饰器的原理，我们先不用 `@Decorator` 这个语法糖。

下面代码是本文的重点，大家可以放慢阅读速度，理解后再继续往下看：

```
// 装饰器函数function decoratorLogTime(target, key) {  const targetPrototype = target.prototype  // Step1 备份原来类构造器上的属性描述符 Descriptor  const oldDescriptor = Object.getOwnPropertyDescriptor(targetPrototype, key)  // Step2 编写装饰器函数业务逻辑代码  const logTime = function (...arg) {    // Before 钩子    let start = +new Date()    try {      // 执行原来函数      return oldDescriptor.value.apply(this, arg) // 调用之前的函数    } finally {      // After 钩子      let end = +new Date()      console.log(`耗时: ${end - start}ms`)    }  }  // Step3 将装饰器覆盖原来的属性描述符的 value  Object.defineProperty(targetPrototype, key, {    ...oldDescriptor,    value: logTime  })}class GuanYu {  attack() {    console.log('挥了一次大刀')  }  run() {    console.log('跑了一段距离')  }}// Step4 手动执行装饰器函数，装饰 GuanYu 的 attack 函数decoratorLogTime(GuanYu, 'attack')// Step4 手动执行装饰器函数，装饰 GuanYu 的 run 函数decoratorLogTime(GuanYu, 'run')const guanYu = new GuanYu()guanYu.attack()// 挥了一次大刀// 耗时: 0msguanYu.run()// 跑了一段距离// 耗时: 0ms
```

以上就是装饰器的具体实现方法，其核心思路是：

1. **Step1 备份原来类构造器 (Class.prototype) 的属性描述符 (Descriptor)**

利用 `Object.getOwnPropertyDescriptor` 获取

1. **Step2 编写装饰器函数业务逻辑代码**

利用执行原函数前后钩子，添加耗时统计逻辑

1. **Step3 用装饰器函数覆盖原来属性描述符的 value**

利用 `Object.defineProperty` 代理

1. **Step4 手动执行装饰器函数，装饰 Class(类) 指定属性**

从而实现在不修改原代码的前提下，执行额外逻辑代码

## **5. [@Decorator](https://github.com/Decorator) 装饰器语法糖**

但上一步 4.2.2 手写的装饰器函数存在两个可优化的点：

1. **是否可以让装饰器函数更关注业务逻辑？**

Step1, Step2 是通用逻辑的，每个装饰器都需要实现，简单来说就是可复用的。

1. **是否可以让装饰器写法更简单？**

纯函数实现的装饰器，每装饰一个属性都要手动执行装饰器函数，详见 Step4 步骤。针对上述优化点，装饰器草案中有一颗特别甜的语法糖，也就是 `@Decorator` ，它能够帮你省去很多繁琐的步骤来用上装饰器。

只需要在想使用的装饰器前加上`@`符号，装饰器就会被应用到目标上。

### 5.1 [@Decorator](https://github.com/Decorator) 语法糖的便捷性

下面我们用 `@Decorator` 的写法，来实现同样的功能，看看代码量可以精简多少：

```
// Step2 编写装饰器函数业务逻辑代码function logTime(target, key, descriptor) {  const oldMethed = descriptor.value  const logTime = function (...arg) {    let start = +new Date()    try {      return oldMethed.apply(this, arg) // 调用之前的函数    } finally {      let end = +new Date()      console.log(`耗时: ${end - start}ms`)    }  }  descriptor.value = logTime  return descriptor}class GuanYu {  // Step4 利用 @ 语法糖装饰指定属性  @logTime  attack() {    console.log('挥了一次大刀')  }  // Step4 利用 @ 语法糖装饰指定属性  @logTime  run() {    console.log('跑了一段距离')  }}const guanYu = new GuanYu()guanYu.attack()// [LOG]: 挥了一次大刀// [LOG]: 耗时: 3msguanYu.run()// [LOG]: 跑了一段距离// [LOG]: 耗时: 3ms
```

为了让更直观了解上述代码是否可以编译后正常执行，

我们可以从 [**TypeScript Playground**](https://www.typescriptlang.org/play) 直接看到编译后的代码以及运行结果，

> 注意！为了方便理解，记得关闭配置 `emitDecoratorMetadata` 禁止输出元数据，
>
> 元数据是另一个比较复杂的知识点，我们本篇文章先跳过
>
> 关闭后编译的代码会更简单

我们打开[上面代码的在线 Playground 链接](https://www.typescriptlang.org/play?emitDecoratorMetadata=false&target=99&jsx=0#code/GYVwdgxgLglg9mABAGzgcwCowLYFMAUUAhgE5q5QA0iA1rgJ7UAmuAzhCTAA5RwkCUiAN4AoRIggJWURHGRMAshQAWuJogC8iFu048+AOgBuRZCFxiJUmakw5cmxKEiwEifAc+k0g0ePHIFIjSpDJaANRguADuiAAiRFAE-JbiUCT0wqn+JBQgJEhyiipqBkRcXMj0hMowrNTeggD0TYiAwDGAFK6A0nKAskqAIW6Av4qADqbZAL5OMGCmVVn+-oEyuGDqEVGxCUn4KXPikmCscrgGtvgABoDoAYBvpgBciAAkQkvqALTBxCRQo9isp9v+o5YAeIdBxuLwSMZTOZHLYsHhLLkoPkkCC9OCRACRBBkERWKxEABxEBEMAATRAs0QAAFYfZLIliBAaFtKbspIdjuh8AByQCkxoAwuUAAHKAQmtAOSagAAlbl-IH+Sw09BwiziEjgFl+fx7A6BTloHmARdihYBW60Au7GAbs8pYCMSIsdZEGhiWSKVo1oSHeStiJ7STyWUoIzmSkWogBSKJSIg1dbgBmb6et0gAwqsAeoMGwUm03h1qRxAx1hAA)，点击 `Run` 运行按钮，即可看到其正常运行和输出结果：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151434425794402.png)

对比纯手写的装饰器，用 `@Decorator` 语法糖可以省去 2 个重复的步骤：

- **Step1 备份原来类构造器 (Class.prototype) 的属性描述符 (Descriptor)**

  ```
  const oldDescriptor = Object.getOwnPropertyDescriptor(targetPrototype, key)
  ```

- **Step3 用装饰器函数覆盖原来属性描述符的 value**

  ```
  Object.defineProperty(targetPrototype, key, {  ...oldDescriptor,  value: logTime})
  ```

开发者仅需两步即可实现装饰器的功能，可以更专注于装饰器本身的业务逻辑：

- **Step2 编写装饰器函数业务逻辑代码**

  ```
  function logTime(target, key, descriptor) {  const oldMethed = descriptor.value  const logTime = function (...arg) {    let start = +new Date()    try {      return oldMethed.apply(this, arg) // 调用之前的函数    } finally {      let end = +new Date()      console.log(`耗时: ${end - start}ms`)    }  }  descriptor.value = logTime  return descriptor}
  ```

- **Step4 利用 @ 语法糖装饰指定属性**

  ```
  @logTimeattack() {  console.log('挥了一次大刀')}
  ```

### 5.2 【重点】分析 [@Decorator](https://github.com/Decorator) 语法糖编译后的代码

[@Decorator](https://github.com/Decorator) 语法糖很甜，但却不能直接食用。因为装饰器目前仅仅是 **ECMAScript** 的语言提案，还处于 [stage-2](https://github.com/tc39/proposal-decorators) 阶段，无论是最新版的 Chrome 浏览器还是 Node.js 都不能直接运行带有 [@Decorator](https://github.com/Decorator) 语法糖的代码。我们需要借助 TypeScript 或者 Babel 的能力，将源码编译后才能正常运行。而在 [TypeSciprt Playground](https://www.typescriptlang.org/play?emitDecoratorMetadata=false&target=99&jsx=0#code/GYVwdgxgLglg9mABAGzgcwCowLYFMAUUAhgE5q5QA0iA1rgJ7UAmuAzhCTAA5RwkCUiAN4AoRIggJWURHGRMAshQAWuJogC8iFu048+AOgBuRZCFxiJUmakw5cmxKEiwEifAc+k0g0ePHIFIjSpDJaANRguADuiAAiRFAE-JbiUCT0wqn+JBQgJEhyiipqBkRcXMj0hMowrNTeggD0TYiAwDGAFK6A0nKAskqAIW6Av4qADqbZAL5OMGCmVVn+-oEyuGDqEVGxCUn4KXPikmCscrgGtvgABoDoAYBvpgBciAAkQkvqALTBxCRQo9isp9v+o5YAeIdBxuLwSMZTOZHLYsHhLLkoPkkCC9OCRACRBBkERWKxEABxEBEMAATRAs0QAAFYfZLIliBAaFtKbspIdjuh8AByQCkxoAwuUAAHKAQmtAOSagAAlbl-IH+Sw09BwiziEjgFl+fx7A6BTloHmARdihYBW60Au7GAbs8pYCMSIsdZEGhiWSKVo1oSHeStiJ7STyWUoIzmSkWogBSKJSIg1dbgBmb6et0gAwqsAeoMGwUm03h1qRxAx1hAA) 上，我们可以直接看到编译后代码。

为了更清晰容易理解，我们把编译后的业务代码先注释掉，只看装饰器实现的相关代码：

```
"use strict";// Part1 装饰器工具函数(__decorate)的定义var __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;    if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;    return c > 3 && r && Object.defineProperty(target, key, r), r;};function logTime(target, key, descriptor) {    // ...}class GuanYu {    // ...}// Part2 装饰器工具函数(__decorate)的执行__decorate([logTime], GuanYu.prototype, "attack", null);__decorate([logTime], GuanYu.prototype, "run", null);// ...
```

上述代码核心点是两个部分，一个是定义，一个是执行。定义部分较为复杂，我们先从执行入手：Part2 装饰器工具函数(__decorate)的执行会传入以下 4 个参数：

1. **装饰器业务逻辑函数**
2. **类的构造器**
3. **类的构造器属性名**
4. **属性描述符(可以为 null)**

为了方便理解 Part1 装饰器工具函数 `__decorate` 的定义，我们需要精简 `__decorate` 的函数代码，让它变成最简单的样子，而精简代码的前提是收集条件：

- **条件 1 `(this && this.__decorate)` 可删除**

这里的 this 是指 window 对象，这一步的含义是避免重复定义 __decorate 函数，属于辅助代码，可删掉。

- **条件 2 `c < 3 === false`**

Part1 的 c = arguments.length 代表参数的个数，由 Part2 我们知道工具函数会传入 4 个参数，因此在本次案例中 c < 3 参数个数小于 3 的情况不存在，即 c < 3 === false，

- **条件 3 `c > 3 === true`**

本次案例中 c > 3 参数大于 3 的情况存在，即 c > 3 === true 。

- **条件 4 `desc === null`**

同时在 Part1 我们知道第四个参数 desc 传入的值就是 null ，即 desc === null

- **条件 5 `typeof Reflect !== "object"`**

Reflect 反射是 ES6 的语法，本文为了更容易理解，暂不引入新的 ES6 特性和语法，让环境默认为 ES5，即不存在 Reflect 对象，即 typeof Reflect !== “object”，有了上述条件后，我们可以进一步精简 `__decorate` 的方法

- 代码片段 1：

```
r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc// 根据 c < 3 === false , desc === null 条件// 精简后r = desc = Object.getOwnPropertyDescriptor(target, key)// r 和 desc 此时代表的是属性的描述符 Descriptor
```

- 代码片段 2：

```
if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;// 根据 c < 3 === false , c > 3 === true 条件// 精简后if (d = decorators[i]) r = d(target, key, r) || r;
```

- 代码片段 3：

```
if (typeof Reflect === "object" && typeof Reflect.decorate === "function") r = Reflect.decorate(decorators, target, key, desc);// 为了方便理解，本案例暂认为 Reflect 不存在// 精简后// 空
```

- 代码片段 4：

```
return c > 3 && r && Object.defineProperty(target, key, r), r;// 根据 c > 3 === true, r 是属性描述符，必定存在// 精简后Object.defineProperty(target, key, r)return r;
```

- **精简后整体代码：**

```
var __decorate = function (decorators, target, key, desc) {    var c = arguments.length;    // Step1 备份原来类构造器 (Class.prototype) 的属性描述符 (Descriptor)    var r = desc = Object.getOwnPropertyDescriptor(target, key),    var d;    for (var i = decorators.length - 1; i >= 0; i--) {      // d 为装饰器业务逻辑函数      if (d = decorators[i]) {        // 执行 d，并传入 target 类构造器，key 属性名，r 属性描述符        r = d(target, key, r) || r;      }    }    // Step3 用装饰器函数覆盖原来属性描述符    Object.defineProperty(target, key, r)    return r;};
```

代码经过精简之后核心原理还是和我们 **4.2.2 手写一个装饰器函数**的原理是一样的。

1. **Step1 备份原来类构造器 (Class.prototype) 的属性描述符 (Descriptor)**

利用 `Object.getOwnPropertyDescriptor` 获取

1. **Step3 用装饰器函数覆盖原来属性描述符的 value **

利用 `Object.defineProperty` 代理

TypeScript 对装饰器编译后的代码，只不过是把装饰器可复用的逻辑抽离成一个工具函数，方便复用而已。分析到这里，是不是对 `@Decorator` 装饰器最根本的实现有了更深入的了解？从上面的例子，我们也进一步验证了：

1. `Decorator Pattern` 装饰器模式的设计理念：**在不修改原有代码情况下，对功能进行扩展**
2. `Decorator` 装饰器的具体实现，本质是函数，参数有 target, key, descriptor
3. `@Decoretor` 是装饰器的一种语法糖，只是一种便捷写法，编译后本质还是一个函数

## **6. 带参数的装饰器：装饰器工厂函数**

在上面的「记录函数耗时」例子中，如果我们希望在日志前面加个可变的标签，如何实现？

答案是使用带参数的装饰器

重点：**logTime 是个高阶函数，可以理解成装饰器工厂函数，其接收参数执行后，返回一个装饰器函数**

```
function logTime(tag) { // 这是一个装饰器工厂函数  return function(target, key, descriptor) {  // 这是装饰器    const oldMethed = descriptor.value    const logTime = function (...arg) {      let start = +new Date()      try {        return oldMethed.apply(this, arg)      } finally {        let end = +new Date()        console.log(`【${tag}】耗时: ${end - start}ms`)      }    }    descriptor.value = logTime    return descriptor  }}class GuanYu {  @logTime('攻击')  attack() {    console.log('挥了一次大刀')  },  @logTime('奔跑')  run() {    console.log('跑了一段距离')  }}// ...
```

编译后：

```
// ...__decorate([logTime('攻击')], GuanYu.prototype, "attack", null);__decorate([logTime('奔跑')], GuanYu.prototype, "run", null);// ...
```

看了编译后的代码，我们就很容易知道带参数装饰器的具体实现原理，无非是直接先执行装饰器工厂函数，此时传入对应参数，然后返回一个新的装饰器业务逻辑的函数。

## **7. 五种装饰器：类、属性、方法、参数、访问器**

我们上面学了那么多装饰器的内容，其实只学了一种装饰器：方法装饰器，而装饰器一共有 5 种类型可被我们使用：

1. 类装饰器
2. 属性装饰器
3. 方法装饰器
4. 访问器装饰器
5. 参数装饰器

先来个全家福，然后我们逐一攻破

```
// 类装饰器@classDecoratorclass GuanYu {  // 属性装饰器  @propertyDecorator  name: string;  // 方法装饰器  @methodDecorator  attack (    // 参数装饰器    @parameterDecorator      meters: number  ) {}  // 访问器装饰器  @accessorDecorator  get horse() {}}
```

### 7.1 类装饰器

类型声明：

```
// 类装饰器function classDecorator(target: any) {  return // ...};
```

- @参数：只接受一个参数

`target`: 类的构造器

- @返回：如果类装饰器返回了一个值，她将会被用来代替原有的类构造器的声明

  因此，类装饰器适合用于继承一个现有类并添加一些属性和方法。例如我们可以添加一个 `addToJsonString` 方法给所有的类来新增一个 `toString` 方法

```
function addToJSONString(target) {  return class extends target {    toJSONString() {      return JSON.stringify(this);    }  };}@addToJSONStringclass C {  public foo = "foo";  public num = 24;}console.log(new C().toJSONString())// [LOG]: "{"foo":"foo","num":24}"
```

### 7.2 方法装饰器

已经在上面章节介绍过利用方法装饰器来实现「记录函数耗时」功能，现在我们重新复习下

类型声明：

```
// 方法装饰器function methodDecorator(target: any, propertyKey: string, descriptor: PropertyDescriptor) {  return // ...};
```

- @参数：
- 1. `target`: 对于静态成员来说是类的构造器，对于实例成员来说是类的原型链
  2. `propertyKey`: 属性的名称
  3. `descriptor`: 属性的[描述器](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Object/getOwnPropertyDescriptor)
- @返回：如果返回了值，它会被用于替代属性的描述器。

利用方法装饰器我们可以实现更多的具体场景，比如「打印 Request 的请求参数和结果」功能：

```
function loggerParamsResult(target, propertyKey, descriptor) {  const original = descriptor.value;  descriptor.value = async function (...args) {    let result    let error    try {      result = await original.call(this, ...args);    } catch(e) {      error = new Error(e)    }    if (error) {      console.error('请求失败！')      console.error('请求参数: ', ...args)      console.error('失败原因: ', error)    } else {      console.log('请求成功！')      console.log('请求参数', ...args)      console.log('请求结果: ', result)    }    return result;  }}class App {  @loggerParamsResult  request(data) {    return new Promise((resolve, reject) => {      const random = Math.random() > 0.5      if (random) {        resolve(random)      } else {        reject(random)      }    })  }}const app = new App();app.request({ url: 'https://www.tencent.com'});// [LOG]: "请求成功！"// [LOG]: "请求参数",  {//   "url": "https://www.tencent.com"// }// [LOG]: "请求结果: ",  true// [ERR]: "请求失败！"// [ERR]: "请求参数: ",  {//   "url": "https://www.tencent.com"// }// [ERR]: "失败原因: ",  false
```

**总结：**

无论是「记录函数耗时」还是「打印 Request 的请求参数和结果」，本质都是在实现 **Before / After 钩子**，因此我们只需要记住**方法装饰器**可以实现与 **Before / After 钩子** 相关的场景功能。

**课后题：**

除了上述两个例子，大家还能想到方法装饰器有什么好的应用场景吗？

### 7.3 属性装饰器

类型声明：

```
// 属性装饰器function propertyDecorator(target: any, propertyKey: string) {}
```

- @参数: 只接受两个参数，少了 `descriptor` [描述器](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Object/getOwnPropertyDescriptor)
- 1. `target`: 对于静态成员来说是类的构造器，对于实例成员来说是类的原型链
  2. `propertyKey`: 属性的名称
- @返回: 返回的结果将被忽略

利用属性装饰器，我们可以实现一个非常简单的属性监听功能 ，当属性改变时触发指定函数：

```
function observable(fnName) {  // 装饰器工厂函数  return function (target: any, key: string): any {  // 装饰器    let prev = target[key];    Object.defineProperty(target, key, {      set(next) {        target[fnName](prev, next);        prev = next;      }    })  }}class Store {  @observable('onCountChange')  count = -1;  onCountChange(prev, next) {    console.log('>>> count has changed!')    console.log('>>> prev: ', prev)    console.log('>>> next: ', next)  }}const store = new Store();store.count = 10// [LOG]: ">>> count has changed!"// [LOG]: ">>> prev: ",  undefined// [LOG]: ">>> next: ",  -1// [LOG]: ">>> count has changed!"// [LOG]: ">>> prev: ",  -1// [LOG]: ">>> next: ",  10
```

### 7.4 访问器装饰器

访问器装饰器总体上讲和方法装饰器很接近，唯一的区别在于第三个参数 `descriptor` [描述器](https://developer.mozilla.org/zh-CN/docs/Web/JavaScript/Reference/Global_Objects/Object/getOwnPropertyDescriptor)中有的 key 不同：

方法装饰器的描述器的 key 为：

- `value`
- `writable`
- `enumerable`
- `configurable`

访问器装饰器的描述器的 key 为：

- `get`
- `set`
- `enumerable`
- `configurable`

类型声明：

```
// 访问器装饰器function methodDecorator(target: any, propertyKey: string, descriptor: PropertyDescriptor) {  return // ...};
```

例如，我们可以将某个属性在赋值的时候做一层代理，额外相加一个值：

```
function addExtraNumber(num) {  // 装饰器工厂函数  return function (target, propertyKey, descriptor) { // 装饰器    const original = descriptor.set;    descriptor.set = function (value) {      const newObj = {}      Object.keys(value).forEach(key => {        newObj[key] = value[key] + num      })      return original.call(this, newObj)    }  }}class C {  private _point = { x: 0, y: 0 }  @addExtraNumber(2)  set point(value: { x: number, y: number }) {    this._point = value;  }  get point() {    return this._point;  }}const c = new C();c.point = { x: 1, y: 1 };console.log(c.point)// [LOG]: {//   "x": 3,//   "y": 3// }
```

### 7.5 参数装饰器

类型声明：

```
// 参数装饰器function parameterDecorator(target: any, methedKey: string, parameterIndex: number) {}
```

- @参数：接收三个参数
- 1. `target`: 对于静态成员来说是类的构造器，对于实例成员来说是类的原型链
  2. `methedKey`: 方法的名称，**注意！是方法的名称，而不是参数的名称**
  3. `parameterIndex`: 参数在方法中所处的位置的下标
- @返回：返回的值将会被忽略

单独的参数装饰器能做的事情很有限，它一般都被用于记录可被其它装饰器使用的信息。

```
function Log(target, methedKey, parameterIndex) {  console.log(`方法名称 ${methedKey}`);  console.log(`参数顺序 ${parameterIndex}`);}class GuanYu {  attack(@Log person, @Log dog) {    console.log(`向 ${person} 挥了一次大刀`)  }}// [LOG]: "方法名称 attack"// [LOG]: "参数顺序 0"
```

### 7.6 装饰器参数总结

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151435546888201.png)

## **8. 装饰器顺序**

### 8.1 同种装饰器组合顺序：洋葱模型

如果同一个方法有多个装饰器，其执行顺序是怎样的？

**答案：**

以方法装饰器为例，同种装饰器组合后，其顺序会像剥洋葱一样，

先从外到内进入，然后由内向外执行。和 Koa 的中间件顺序类似。

```
function dec(id){  console.log('装饰器初始化', id);  return function (target, property, descriptor) {    console.log('装饰器执行', id);  }}class Example {  @dec(1)  @dec(2)  method(){}}// 装饰器初始化 1// 装饰器初始化 2// 装饰器执行 2// 装饰器执行 1
```

![图片](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215143617_88938.jpg)I2b3cL.png

其原理，看编译后的代码就非常清楚：

重点：

1. **dec(1), dec(2) 初始化时就执行**
2. **for (var i = decorators.length - 1; i >= 0; i–) 是从右向左，倒叙执行**

```
// 由于本段代码不存在 c < 3 (参数少于3个) 的情况，为了方便理解已精简了部分不可能执行的代码var __decorate = function (decorators, target, key, desc) {    var c = arguments.length,        r = desc = Object.getOwnPropertyDescriptor(target, key),        d;    for (var i = decorators.length - 1; i >= 0; i--)      if (d = decorators[i]) r = d(target, key, r) || r;    Object.defineProperty(target, key, r)    return r;};function dec(id) {    console.log('装饰器初始化', id);    return function (target, property, descriptor) {        console.log('装饰器执行', id);    };}class Example {    method() { }}__decorate([    dec(1),    dec(2)], Example.prototype, "method", null);// 装饰器初始化 1// 装饰器初始化 2// 装饰器执行 2// 装饰器执行 1
```

### 8.2 不同类型装饰器顺序：有规则有规律

1. 实例成员：(参数 > 方法) / 访问器 / 属性 装饰器 (按顺序)
2. 静态成员：(参数 > 方法) / 访问器 / 属性 装饰器 (按顺序)
3. 构造器：参数装饰器
4. 类装饰器

多种装饰器优先级为：

实例成员最高，内部成员里面的装饰器则按定义顺序执行，

依次排下来，类装饰器最低

```
function f(key: string): any {  // console.log("初始化: ", key);  return function () {    console.log("执行: ", key);  };}@f("8. 类")class C {  @f("4. 静态属性")  static prop?: number;  @f("5. 静态方法")  static method(@f("6. 静态方法参数") foo) {}  constructor(@f("7. 构造器参数") foo) {}  @f("2. 实例方法")  method(@f("1. 实例方法参数") foo) {}  @f("3. 实例属性")  prop?: number;}// "执行: ",  "1. 实例方法参数"// "执行: ",  "2. 实例方法"// "执行: ",  "3. 实例属性"// "执行: ",  "4. 静态属性"// "执行: ",  "6. 静态方法参数"// "执行: ",  "5. 静态方法"// "执行: ",  "7. 构造器参数"// "执行: ",  "8. 类"
```

## **9. 装饰器总结**

### 9.1 应用场景

装饰器很像是组合一系列函数，类似于高阶函数和类。

合理利用装饰器对一些非内部逻辑相关的代码进行封装提炼，

能够帮助我们快速完成重复性的工作，节省时间，极大提高开发效率。

1. 类装饰器

可添加额外的方法和属性，比如：扩展 toJSONString 方法

1. 方法装饰器

可实现 Before / After 钩子功能，比如：记录函数耗时，打印 request 参数结果，节流防抖

1. 属性装饰器

可监听属性改变触发其他事件，比如：实现 count 监听器

1. 访问器装饰器
2. 参数装饰器

当然，还有更多可以使用装饰器的场景等着我们去发现

- 运行时类型检查
- 依赖注入

### 9.2 优点

- **在不修改原有代码情况下，对功能进行扩展**。也就是对扩展开放，对修改关闭。
- **优雅地把“辅助性功能逻辑”从“业务逻辑”中分离，解耦出来。**（AOP 面向切面编程的设计理念）
- 装饰类和被装饰类可以独立发展，不会相互耦合
- 装饰模式是 Class 继承的一个替代模式，可以理解成组合

### 9.3 缺点

但是糖再好吃，也不要吃太多，容易坏牙齿的，滥用过多装饰器会导致很多问题：

- 理解成本：过多带业务功能的装饰器会使代码本身逻辑变得扑朔迷离
- 调试成本：装饰器层次增多，会增加调试成本，很难追溯到一个 Bug 是在哪一层包装导致的

### 9.4 注意事项

1. **装饰器的功能逻辑代码一定是辅助性的**

比如日志记录，性能统计等，这样才符合 AOP 面向切面编程的思想，如果把过多的业务逻辑写在了装饰器上，效果会适得其反。

1. **装饰器语法尚未定案以及未被纳入 ES 标准，标准化的过程还需要很长时间**

由于装饰器语法未来制定的标准可能与当前的装饰器实现方案有所不同，Mobx 6 出于兼容性的考虑，放弃了装饰器的用法，并建议使用 [`makeObservable` / `makeAutoObservable`](https://zh.mobx.js.org/observable-state.html) 代替。

详情请查看：https://zh.mobx.js.org/enabling-decorators.html

装饰器提案目前进度：https://github.com/tc39/proposal-decorators

原文作者：easonruan，腾讯 CSIG 前端开发工程师

原文链接：https://mp.weixin.qq.com/s/jzLO37bKYwLlPlVXTnzPfQ

# 【NO.81】学C++的以后都能从事哪些岗位？

相信很多人对编程的接触都来自于大学时的C++语言编程，但这门课只告诉你什么是编程语言，并没有告诉你如何熟练地掌握编程。所以，很多人在毕业前夕发现，虽然学了C++，但似乎不知道自己能做什么，能找到什么样的工作，能去什么样的公司，这是必然的。所以这篇文章从C++的角度谈谈我对C++的理解。

这次我不会讲什么复杂的应用场景，也不会讲语言底层的优化特性。只说我大学学完C++能做什么吧。当然，由于C语言和C++的密切关系，我们会混淆在一起说。道理也差不多。

## **1.实验室开发**

不敢说`C++`是世界上最好的语言，但`C++`基本是工科生中的大语言。暂且不说计算机专业，很多工科专业，比如电信、电气、通信等专业，基本上正常的学习中都会使用到`C++`。

都说`C++`兼具面向过程以及面向对象的特性，既拥有比较优秀的运行速度，又有良好的大型项目开发能力，那简直可以制霸高校实验室了。当然我们知道，另外一门同样在高校实验室里与`C++`不相上下的语言，就是`Matlab`了。

像我研究生期间就是左手`Matlab`试探，右手`C++`优化。论文专利什么的就靠这两种语言双管齐下。还记得在实验室里，基本所有的算法都是用`C/C++`写的。

最开始就学`C++`的基本都是老理工男了。就算指针再怎么难理解，虚函数表再怎么无情，也磨不灭我们这些工科生对编程的热情。

## **2.后端开发**

不过等到真正要找工作的时候，就不是那么回事了。听说现在由于算法岗已经是诸神黄昏了，导致一大批人冲向了后端开发。

而主流的后端开发语言就那么几种，以`Java`、`C++`领衔，`Python`和`Go`紧跟其后。作为写了这么多年的老`C++ coder`了，哪能不去凑个热闹。

所以呀，学了`C++`之后，第一选择当然就是去面试后台开发的岗位啦。只不过我们都常说一句话，语言都不是最重要的。那对于学了`C++`想找后端开发的同学而言，什么最重要呢？

其实软件开发工程需要掌握的技能不外乎那么几点，各种经验分享和面经都已经说烂了。但是那些只是大的方向，具体涉及到的概念和技术点非常多，在这里也没办法全部罗列。总的来说，想通过`C++`找一份合适的后端开发工作，我建议可以从以下几个方面来提升自己：

### **2.1.扎实的编程基础。**

这个基础扎实首先意味着你比较熟悉`C++`这门语言了，知道不同版本的语言特性（比如`C++11`以及新出的`C++20`的区别），对常见的语言机制（比如多态、虚函数表、模板等）的底层有自己理解。

虽然说不看重你用什么语言，但是最起码你得比较熟练的使用一门语言吧。怎么才算熟练呢，大概`C++`底层机制、多线程编程、跨平台编程这些你都需要有所了解吧。

除此之外，编程能力的素养还体现在算法思想和数据结构的理解上。毕竟程序的基础就是算法和数据结构嘛。所以一些基本的数据结构，比如链表、数组、二叉树、队列之类的，还有一些基本的算法思想，比如贪心、分治、动态规划等都最好需要好好掌握一下。

### **2.2.系统的理论知识。**

这个就是科班与非科班比较明显的差距所在了。**很多非科班的同学其实编程能力很强，但是由于没有接受过系统的理论学习，所以在理论学习上会比较吃亏。而后端开发涉及的东西比较多，从网络到性能再到架构，都需要系统的计算机理论来支撑的。**

所以如果连计算机网络、操作系统、计算机组成原理、编译原理这些书都没见过的同学，需要赶紧买几本回来压压惊了。

### **2.3.后端基础。**

想去做后端开发，起码得知道后端是什么吧。这就意味着你得知道一些基本的软件工程、软件架构、设计模式等知识。同时，搞个后端哪能不懂数据库呢，那你不得再学些诸如`MySQL`、`Redis`等数据库的基本操作吗？

当然到这，可能也就算是个入门水平。真正上手后端开发的时候，像微服务、中间件（`Kafka`/`Zookeeper`/`Hadoop`等）等相关概念与技术的学习不也得提上日程了嘛。

当然对于校招生来说，**由于的确是缺少大型的上线项目经验，所以对后端基础的要求不会太高，更多的是看编程能力和计算机理论基础。不**过倘若是已经工作想转行的同学来说，后端基础还是会比较看重的。

这三点是在我看来目前互联网公司比较看重的地方，大家可以对照着反思自己是否具备这样的能力了。如果还没有，可以抓紧时间好好准备准备。

## **3 .客户端开发**

毕竟`C++`已经是一个很成熟的语言了，所以除了后端开发其实它还有很多其它的开发岗位可以选择。自然而然的，就是客户端开发了。

相信大多数学`C++`的同学都用过`C++`写桌面软件吧。那时候估计还是用MFC，照着代码书上敲一遍就能写一个比较简单的界面了。所以如果不想做后端，完全可以靠`C++`找一份客户端开发的工作。

只不过，客户端也并不容易呀。做客户端同样也需要扎实的编程基础和计算机理论基础，同时可能还要熟悉`Windows`/`C++`编译链接机制、`QT`客户端开发技术体系、`Windows`消息机制等技术。

所以，假如你学的是`C++`，同时也不在乎业内莫名其名的岗位歧视链的话，找一个客户端开发的工作也是非常不错的。只不过相较于后端开发，你可能需要重新审视客户端这个角色。

在我看来，客户端与后端在职业发展上的区别主要有以下几点：

### **3.1.技术天花板**

这就是岗位歧视链中最经常拿来讨论的一个点。普遍认为客户端的天花板较低，长期在市场上的竞争力不高。甚至还有人拿`CTO`基本全是后端开发出身的例子来验证这个歧视链。

怎么说呢，我觉得对于大多数人来说，客户端的天花板的确比后端要低。因为客户端是一个两级分化比较严重的技术，厉害的人天花板可以突破天际，而一般的人真的就很一般。而后端相较之下，的确职业高度的分布稍微均匀一些。

但是，当客户端开发达到一定深度后，难度绝对不比后端开发难度低。**反而有时候为了获得更好的端上体验以及更快的用户响应，涉及到的技术会更加底层，更加硬核。** 记住一点，客户端开发绝不是改改`UI`，改改文案这么简单。

所以我特别不喜欢这种歧视链，从非常片面的角度去论证某个职业或者技术的好与坏，甚至还上升到歧视的层面。**我比较认同的是兴趣爱好论，而不是这种天花板论。**

### **3.2.发展方向**

客户端客户端，很显然是面向用户的。这就意味着客户端的开发是直接接触到用户，这也是与后端开发最主要的区别。

说简单一点，客户端开发就是为用户服务，优化用户体验，让产品变得更好用。所以客户端有一个天然的优势，就是离用户很近，知道用户想要的是什么。

问问自己，**你觉得自己干技术能干几年，写代码还能写到多少岁。身体熬得住吗，头发还剩多少？干技术这行，还是需要天赋和意识的，特别是真正顶尖的技术大牛。** 不是所有人能够在技术这条道上一直走下去。

那走不下去怎么办？转型。

所以客户端的发展从来都不只是技术，而是业务技术两手抓，当然会有所侧重。业务能力意味着产品思维，放大点说就是互联网思维。这就是客户端的一大优势。

### **3.3就业选择**

这的确是目前客户端发展的一个痛点。放开到二三四线城市，后端的需求的确要比客户端要大。因为在小公司里，往往不需要那么复杂的客户端支撑。

很多时候都是把后端开发程序员当作全栈来用。很多人都会考虑说，假如干了客户端，之后跳回老家就找不到合适的公司了。虽然这的确很有可能，但也没这么夸张。

只要你的基础扎实，并且善于学习，其实到哪儿都能找到差不多的工作的。不过在一线城市，后端和客户端并没有明显的差别，互联网大厂对这两种岗位的需求都是很大的。

## **4 .音视频开发/多媒体开发**

我们知道现在基本各个大平台都在做短视频，同时也不断接入各种直播模式。在这个趋势中，对于音视频/多媒体方向的开发需求越来越广。

比较成熟的互联网大厂通常会把音视频/多媒体技术方向独立出来，作为为全公司所有产品线提供中台的支撑。然后相关的处理算法会封装成SDK给各个业务中调用。

这说明音视频/多媒体开发是很重要的一份工作，而在对音视频或流媒体进行处理的时候，算法效率就是很重要的评价标准。这不就是`C++`所擅长的场景嘛。

所以呢，学了`C++`还可以选择去做音视频开发的方向。这个方向更偏向于具体的场景，比如播放器，比如直播平台，比如音视频特效等。当然了，`C++`更多的还是偏算法实现的方向，许多多媒体开发本身还是立足于平台，比如基于`Android`、`iOS`或者`Windows`，来进行满足平台特性的开发。

不过想要找这个方向的岗位，基本的音视频开发的理论知识得掌握吧。**这里同样也包括了很多理论层面的知识，比如图像、音频、视频的采集、渲染、处理、传输等方面的问题，图像和音视频的加工、裁剪、编解码等处理。**

一些基本的处理库，用于图像处理的像`OpenGL`、`OpenCV`等，用于视频编解码的`x264`、`OpenH264`、`ffmpeg`，用于音视频处理的`speexdsp`、`libfaac`、`opus`等库不也得熟悉熟悉吧。

还有音视频传输的协议、直播推/拉流的协议，如`RTMP`、`RTSP`等也可以掌握掌握。哈哈不要被吓到，毕竟会`C++`只是基本的编程能力，但是具体的岗位肯定有业务能力上的要求。

## **5. 游戏开发**

游戏开发也是`C++`的可以进军领域之一，像很多游戏引擎都是基于`C++`开发的。只不过游戏开发真的需要兴趣和热情。如果不是因为喜欢游戏，建议还是不要选择游戏开发的岗位。

不是说游戏开发前景不好，而是说这件事如果没有足够的热情很难走得很远，并且因为方向不是很大众，到时候想换方向了，可选择性就不是很多。

国内比较出名的游戏厂商就那么几家，但是投递的人却很多。这也导致这个行业的竞争很大，所以在招聘的时候会更看中你的游戏开发经历。

如果是校招生，很少会有比较多的游戏开发经验，**但是可以通过许多途径去弥补这一缺陷，比如参加游戏开发比赛，比如自己开发**`demo`。**起码也得了解过或者用过一些游戏引擎，这样简历也会更加饱满一点。**

对于游戏开发的业务能力要求刚刚也提到过一点，大概包括游戏引擎的使用和了解，比如常见的`Cocosa2d`、`Unity3D`等，游戏引擎的实现原理和机制，还有面试时候基本都会问到的计算机图形学理论。计算图形学是游戏开发领域及其重要的理论基础，毕竟游戏离不开各种图像模型的渲染和处理。

所以呢，如果你在大学期间有基础过一些图像学的工作，比如一些模型绘制、三维渲染的工作就可以去尝试一下游戏开发的岗位。我之前有个师兄在研究生阶段是做医学图像处理的，主要的工作就是对人体脏器和血管进行三维显示的，然后就去做了游戏开发。美滋滋。

## **6 .嵌入式开发**

上面几种岗位选择都是偏软件方向想的，而`C/C++`还有一类非常大的就业方向，就是去做嵌入式。嵌入式本身可能是偏硬件一点，但是现在做个什么事，都是软硬不分家的。

嵌入式的开发同样也会涉及到网络编程、并发编程等方面。通俗点解释可能就是，嵌入式代码是运行在别的小系统上，而不是传统意义的计算机上。

所以嵌入式开发更适合原来专业是电信、微电子之类的专业，**毕竟这个方向除了需要编程能力，还需要基本的电路理论素养。**

一般招聘嵌入式开发岗的公司大多是以硬件产品为主的互联网+公司，比如小米、华为等。进去公司很多都是在做物联网相关的工作，毕竟这是热点。当然也会有其它的业务方向，例如芯片、机器人等。

## **7. 总结**

说了这么多，有没有发现好像只会`C++`是找不到工作的。不管是哪个岗位，在对`C++`基础的要求上，还会有额外的东西。这其实很好理解，毕竟`C++`只是一门语言，只是一个工具。

就好比你光有一把弓箭，是没办法射大雕的。**你还需要许多额外的能力和知识储备，甚至还需要一点预判的意识，才能够在合适的时机以合适的力度射出那一箭。**

而`C++`，只是那支射出的箭罢了。你越熟悉它，箭头便愈发锃亮，破坏力也就越强。并且同时，空中盘旋的也不只有一只大雕，你还需要选择机会最大的那一只。

原文链接：https://zhuanlan.zhihu.com/p/579042527

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.82】耗时1个月，万字干货，详解腾讯面试（T1-T9）核心技术点，面试题整理

本文以非腾讯在职人的身份，来聊腾讯面试的流程、攻略和建议，但愿能助有缘人。

标题涉及的范围很广，对多数面试腾讯的人而言，都有参考价值，看完本文之后，必有所得。退一步来讲，即便是准备面试其他公司，也有很多问题是相通的，亦可借鉴，有所广益。从毕业到现在，被别人面试过，也面试过别人，大大小小的面试，一两百次，也算久经沙场。

## 1.**Part1: 面试流程**

### **1.1. 整体流程**

腾讯的部门很多，岗位类型有别，职级差异较大，因此，每个面试者的面试流程肯定不尽相同，然而，很多流程基本通用，故值得一看。

郭靖(化名)是社招入职腾讯的，面试岗位是后台开发，最近跟郭靖吃了顿饭，聊了两个小时，获得许可后，便以郭靖的面试为例进行介绍。面试流程如下：

![img](https://pic2.zhimg.com/80/v2-799cc8dd07778660d0324cff1b853d65_720w.webp)

一图胜千言，要进腾讯并不那么容易。跟游戏升级一样，只有通过前一关，才有机会进入下一关。微信部门的面试，要求更高，基本是2轮面委面，还有可能是3轮。当然，微信部门的钱也更多。

有的朋友问：组长是啥岗位？总监和总经理又是啥岗位？我们来简单介绍一下公开信息中的腾讯管理层级(副级别不单独列出)：

![img](https://pic4.zhimg.com/80/v2-7f4e28bc1c46c55e58e004b662649e7f_720w.webp)

看似层级不多，但要从下一层跨越到上一层，通常是五年十年之功，并不容易。可以算算，接近小马哥要多少年？有的朋友还会问：面委是什么？面委就是面试委员会，会对面试者的能力进行再次考核，以便进行定岗定级。通常来说，面试9级或9级以上的岗位，才需要面委。这里的9级是什么意思呢？且看公开信息中的腾讯技术通道职级体系(左旧右新)：

![img](https://pic4.zhimg.com/80/v2-7bc793e5d3fe1a6aa21c1888ec791253_720w.webp)

一般来说，应届毕业生级别最低，组内骨干至少是9级工程师，组长至少是10、11级工程师，总监至少是12、13级工程师，依此类推。接下来，我们来具体聊聊面试流程中的每个环节，仍以郭靖的社招面试流程为例，岗位是后台开发。

### **1.2. 简历筛选流程**

郭靖第一步是要投递简历。筛选简历的是hr和业务部门。如果hr觉得郭靖简历不错，就会把简历给到业务部门。当然，业务部门也可以去人才库中找匹配的简历。

总之，简历的最终评判会落在业务部门身上。如果简历不通过，就没有下文了。如果简历通过，郭靖就有希望了。

### **1.3. 电话面流程**

简历通过后，业务部门的组员或组长，会打电话给郭靖，简要了解基本情况，也可能初步问一下技术和岗位方面的问题。

极端情况下，如果电话中发现郭靖技术确实太水，啥都不会，尽是扯淡，那么就会让郭靖等后续通知，然后就没有然后了。

然而，通常情况下，既然简历通过筛选，那么还是值得给予面试机会，这个电话的主要目的，是了解基本情况，并约郭靖到腾讯面试。

随后，郭靖收到一条短信，大概内容就是何时何地参加面试。就这样，郭靖需要亲自跑到腾讯去参加面试。聪明的郭靖，选择了不聪明的出行方式：开车去腾讯面试。

要知道，腾讯那地方实在是太堵了，而且还不好找停车位，郭靖急得大汗淋漓，嘴里骂骂咧咧。好在郭靖做事靠谱，提前预留了很多时间。

### **1.4. 组员面流程**

组员面试，不是100%必须的环节，然而有时候组长太忙，就会让组员先面试，探探路。

郭靖屁颠屁颠地到了腾讯，参加面试，遇到的就是组员面试，这个组员，一般是组内的骨干员工，职级通常是9级，很可能就是之前电话约面试的那个人。

组员简单地接待了郭靖，顺便客气地倒上一杯水，然后递给郭靖一张试卷。咦？还有笔试环节呢！还要在纸上手写代码。

20分钟答完试卷后，那个组员问了郭靖一些基本的技术问题，组员面大概耗时30分钟。只要郭靖的技术不是太水，就有组长面的机会。

果然，在30分钟的组员面后，郭靖被告知即将进行组长面。

### **1.5. 组长面流程**

前面说了，组员面不是100%存在的环节，如果没有组员面，那么组员面的内容会在组长面时体现。

组长拿着郭靖的笔试试卷，皱起了眉头，就笔试题目进行发问，问完之后，还问了别的技术问题。这个过程又持续了30分钟左右。

总共已经1个小时了，如果郭靖的技术不行，就不会去浪费总监的时间了。然后，会告诉郭靖回家等消息，那么，这种情况基本就表明面试不通过。

如果水平不错，组长就会让郭靖稍微等一下，去请总监来面试。通常来说，组长会提前跟总监打招呼，说今天有个叫郭靖的来面试，让总监预留一些时间。

果然，组长让郭靖在会议室等了几分钟，自己去跟总监沟通了一下。郭靖还是有几把刷子的，经历了1个小时左右的切磋较量后，顺利进入总监面。

### **1.6. 总监面流程**

一般来说，总监差不多工作十年以上。根据面貌，就可以大概判断出来。郭靖，混迹IT江湖数年，显然是察言观色的高手。

总监对郭靖继续进行深度发问，聊了很多技术和项目问题。郭靖感觉难度明显加大了，不过还是能正常应对。最后，还要求在会议室的白板上，手写了两个题目的代码，并进行讲解。

整个总监面的时间接近1个小时，这说明总监觉得郭靖还不错，不然就会在20分钟内草草结束面试，互不耽误时间，终生不见。

无论是否通过总监面，当天的面试通常都会结束。不会进行总经理面，因为总经理时间很难约。

总监不可能跟总经理提前说：今天有个叫郭靖的人来面试，如果通过，我再给您面试。

这显然不合理，总经理那么忙，他需要知道的是确定的消息，而不是等待郭靖是否通过的消息。

总之，总监面后，无论是否通过，郭靖都要回家等消息。如果不通过，那就没啥好说的。

回家后，啥消息都没有。一天、两天、三天都是如此。郭靖有点着急了，看来是没戏了。

然而，一周后，一个电话打破了宁静。郭靖接到电话后，是个MM的声音，声音很甜。郭靖有点紧张和激动，心想，终于到hr面了，offer有希望了。

做梦！还差得远呢！这个MM可不是腾讯hr, 而是部门的秘书, 她会根据总经理的时间，跟郭靖预约面试时间。

随后，郭靖收到一条短信，大概内容就是何时何地参加GM面试。GM就是General Manager，翻译过来就是总经理。

郭靖的心情，由紧张激动，变成了一点点落寞，原来面试还没有结束。所以，郭靖还得再跑一次腾讯，去现场参加总经理面。

### **1.7. 总经理面流程**

郭靖上次是开车去的，结果到处找停车位，还差点迟到。这次郭靖变聪明了，直接打车去。

总经理面，是最重要的一轮面试，在很大程度上，决定了郭靖能否拿到最终的offer.

秘书MM带着郭靖来到总经理办公室旁，郭靖觉得跟以前不一样了。嗯，单独的一间办公室。

正在思索间，里面出来一个人，我们姑且称他为杨康。郭靖算是明白了，今天是大家一起PK来了。

总经理问了很多有深度的问题，也问了一些宏观的问题，还要求在草稿纸上写了两个题目的程序。这有点出乎郭靖的预料，没想到总经理也这么懂技术。

随后，总经理问了郭靖的当前基本工资和年终奖情况，就让郭靖回家等待消息。带着不安的情绪，郭靖回家了。

一天、两天、三天，没有任何消息，郭靖等得有点着急了。终于，在一个星期后，郭靖接到了电话。

电话的那头是另一个MM的声音，声音没上次甜。郭靖内心有点激动，以为到了hr面试的环节，终于快要大功告成了。

天真！有点天真！对方MM确实是腾讯hr, 但郭靖被告知还要参加面委会面试。郭靖已经两次亲自跑到腾讯去了，这次的面委面，采用视频面的方式，这倒让郭靖松了一口气。

### **1.8. 面委面流程**

郭靖按要求接入视频，参加面试。面委面试官继续对技术和项目进行深度发问，在系统方案设计方面的考查力度明显加大。面委面的目的，是对人才进行再次把关，顺便进行职级定级。一般来说，面试9级以下的岗位，是不需要面委面的。

面委面试完毕后，断掉视频，郭靖长叹了一口气。一路走来，并不容易。面试的严格程度，让人窒息。

郭靖不知道这次面试的结果，也没有最初那份兴奋了，因为，有点累了，真的有点累了。一天、两天、三天还是没有消息，郭靖也不着急了，反正不期待，就不会失望，自然也不会着急。

一个星期后，电话再次响起，对方还是MM, 郭靖一向会察言观色，一下就听出了是上次那个hr MM.

郭靖知道，这应该就是正式的hr面试了。这次，郭靖对了。终于等到你，还好没放弃，郭靖牢记着张靓颖的鼓励。

### **1.9. hr面流程**

腾讯招人的成本是非常大的，前面那么多流程，郭靖都成功突围。所以，只要郭靖情商不是太差，通过hr面试是没有问题的。

hr面试，问的就是一些基本信息和常规问题。说白了，hr面试，就是一个情商面试。关于hr面试的具体详情，会在后面的攻略部分详细阐述。

谈钱不伤感情，hr自然是要跟郭靖谈钱的。基本的一番了解和沟通之后，郭靖获得了hr的口头offer，也知道自己能拿多少钱了。hr补充说，后续会有电子offer发到邮箱。

hr为了吸引郭靖加入腾讯，开始机关枪式地介绍腾讯的各种福利待遇，大概持续了3分钟，比如年假、年终奖、社保、五险一金、商业险、团建、文娱活动、免费餐券、晋升、股票、无息贷款购房等众多福利。

郭靖听得有点晕乎，倒也不觉得烦，只是不停助谈地说：嗯，好的，明白，了解，谢谢，听说过，嗯，好的，知道，嗯，OK，了解。

hr面试结束后，郭靖心无所恋，下班后就开始玩王者荣耀，始终选择阿轲，向敌方发起攻击，然后一觉睡到大清早。

### **1.10. 电子offer流程**

第二天，郭靖如期收到邮件，是正式电子offer，内容包括钱和岗位，具体信息，就不说了。此刻，郭靖异常平静。

为了更方便地沟通，hr加了郭靖的微信。接下来，hr让郭靖提供薪资证明和背景调查的基本信息。

郭靖需要提供三个人的电话：郭靖的直接领导(洪七公)，郭靖的同事(黄蓉)，郭靖部门的hr(华筝)。

腾讯拿到这些这些信息后，会委托第三方专门负责背景调查的公司，对郭靖进行背景调查。

同时，郭靖也跟领导洪七公进行了真诚沟通，表明自己要离职，希望去腾讯发展。洪七公是舍不得郭靖走的，但没办法，郭靖要走。

这里要强调一下，只有郭靖愿意并同意加入腾讯，且同意开始接受背景调查，腾讯和背景调查公司才能发起对郭靖进行调查。

如果郭靖还没有拿到电子offer, 或者郭靖最终不愿意加入腾讯，那么腾讯和背景调查公司就无权调查郭靖。

这很好理解，如果郭靖最终不想离职，还被调查，还打电话给他的领导洪七公，那洪七公就知道郭靖至少有想离开的想法，岂不是害了郭靖吗？

腾讯，当然是有节操的，流程也是规范的，不会犯这种低级、愚蠢且恶意的错误。大家可以放心地相信腾讯。

可是，其他某些公司，就不一定了。在面试流程还在进行时，在还没有发正式offer的情况下，用各种方式私下偷偷无耻地打听面试者的情况和表现。这种垃圾公司，不去也罢。

**十一. 背景调查流程**

背景调查，是现在很多公司采取的策略，主要是为了保证人才质量，避免弄虚作假，降低企业招人风险。

背景调查，主要考察人品，那些还想在简历中写虚假信息的人，那些虚报当前工资的人，要注意啦。

不要有侥幸心理，背景调查公司，是干这个吃饭的，要调查郭靖的背景资料，方法多得很。串通造假，难度太大，也不正直，自己想想。

背景调查公司调查完基本信息后，开始给郭靖的直接领导(洪七公)、郭靖的同事(黄蓉)和郭靖所在部门的hr(华筝)打电话，询问郭靖的基本情况，了解郭靖的人品和平时表现。

当然，郭靖身正不怕影子斜，一切信息真实，不担心背景调查。对郭靖而言，背景调查无非就是个流程而已。

关于背景调查的具体内容和攻略，我们会在后面详细介绍。

## **2.Part2: 面试攻略**

### **2.1. 简历攻略**

简历的投递，可以通过腾讯招聘网站，也可以找腾讯的朋友内推。要说明的是，内推不会增加通过几率，更不会减少面试流程。

简历的基本原则是信息真实，少数求职者可能会动歪心思，比如故意延长或缩短某段工作经历，或是对学历进行美容。真别这样，一旦被发现，不只是尴尬。

简历的内容要展示自己才能，让面试官看到郭靖的厉害之处，要多花点心思来写，突出亮点，吸引面试官。

对郭靖来说，简历所写内容，必须了如指掌，因为简历上的任何东西，都可能被问到。有些简历上写研究生期间发表了几篇论文，结果面试时被问论文的内容和原理，答不出来，真让人怀疑。

简历要好好写，用心写，真实地写，而且要对简历上的每个内容都了如指掌。郭靖自己的经历，郭靖怎么能不清楚呢?

如果学校不错，建议简历文件名采用如下格式：郭靖*XX大学*本科_应聘腾讯后台开发岗位.pdf，看到这简历名，就有想看看的冲动。

如果有github账号且写得很好，或者有开源作品，也可以放到简历中，让人看到郭靖对技术的热爱和追求，这是加分项。

简历的其他注意点也很多，如下4点，不接受反驳：

a. 简历只能是PDF格式。

b. 简历不能是压缩文件。

c. 简历中不能有错别字。

d. 简历最多不超过2页。

总之，自己写完简历后，要好好检查和修改，多站在他人的角度审视自己的简历，也可以让自己信赖的朋友帮修改。

### **2.2. 技术面攻略**

鸡汤大师说，如果看到地上有纸屑，要去捡起来，然后会被录用。这有点反智。真的，别去捡，没用的！要搞清楚，这是技术面，需要真刀真枪的本领。电话面、组员面、组长面、总监面、总经理面、面委面，我们不逐一介绍攻略。这些流程，都是以考查技术为主，故统一介绍。技术面涉及的内容实在太多，无法完全穷尽。所以，本文只聊面试前的准备方向，以及面试现场的注意事项。

还是以社招后台开发为例，在面试之前，郭靖去腾讯招聘网站看了相关岗位的要求，就知道要准备的技术内容和方向了。要准备的技术内容，纷繁驳杂，大致如下：

\1. 数据结构和算法。

\2. 操作系统和Linux.

\3. 计算机网络和网络编程。

\4. C++和STL.

\5. 存储和数据库。

\6. 海量数据处理。

\7. 分布式组件和原理。

\8. 后台安全原理。

\9. 设计模式和原则。

\10. 系统方案设计。

\11. 项目介绍和讲解。

\12. 其他相关问题。

在面试现场，基本的事情还是要注意。少数面试者匆匆赶来，满头大汗，一身汗味，显然不好。少数面试者，觉得自己牛气冲天，在现场和面试官争论不休，自然也不可取。

一些面试者在现场遇到难题后，唉声叹气，变得急躁，这是非常不成熟的表现。也有一些面试者遇到困难后，不加思考，非常轻率，直接放弃，这怎么能打动和说服面试官呢？

没有万能人，大家都有知识盲区，遇到困难太正常了。遇到困难还能现场解决，才能体现出自身价值。即便无法解决问题，也可尝试说出自己的思路，而一个人的思路，往往能反映出这个人的心智模式和未来潜力。

总之，技术准备，功在平时。还是应该修炼基本功，有了这份基本功，就不怕变化，毕竟万变不离其宗。至于该怎么提升基本功，建议多学习、多思考、多实践、多总结。

世上没有银弹，也没有万能捷径，不要浮躁地以为，临时抱佛脚，刷几个题目，就能通过腾讯面试，没那么简单的。功夫修炼，千朝万夕。用兵一时，养兵千日。

## **3. hr面试攻略**

hr面试考察的重点是情商，包括性格和沟通能力。掌握三个原则，万问可破：原则一：强调自己是积极进取的人，乐于挑战自己，有更高的追求，不安于现状，对未来有清晰的思考和规划，并在付诸行动。

原则二：要以腾讯利益为目标，强调自己能为腾讯产品做出什么贡献，而不是强调自身利益和感受。

原则三：面对问题，不要直接用yes或no一棍子打死，而应该从多角度看问题，体现出严谨的逻辑性和条理性，辩证思维。

郭靖去面试，本质上是和腾讯做利益交换。郭靖是来向腾讯要钱的，自然要多体现自己能给腾讯带来什么利益。在牢记上述三个原则的前提下，我们来看hr常问的20个问题。

### **3.1.问题一：你为什么离职？**

错误答案1：现在领导傻逼。

错误答案2：现在加班太多。

错误答案3：现在钱太少了。

错误答案4：现在部门斗争严重。

错误答案5：腾讯的福利待遇好。

错误答案6：在腾讯工作有光环。

错误答案7：想工作生活平衡一下。

错误答案8：师兄在腾讯，说腾讯好。

错误答案9：老婆在腾讯，我也想来。

错误答案10：我是一个孝子，要养父母，需要更多的钱。

点评：这个问题，绝对不是给机会让你吐槽现状或者谈主观感受。离职的原因，要强调自己有进取心，想去更大的平台，表达出看好腾讯的发展，想加入腾讯，共同成长。

### **3.2.问题二：我们为什么要招你？**

错误答案1：因为你们缺人。

错误答案2：你们自己都不知道为什么要招我吗？

错误答案3：我之前干过类似工作，一定可以胜任。

错误答案4：因为我很优秀，你们需要我这样优秀的人。

错误答案5：因为你们业务发展很快，要更多的人力投入。

点评：这个问题，部分人读不懂题意。腾讯要招聘你的原因，一定是因为你能给腾讯带来价值。价值怎么体现和证明？不是吹嘘自己有多牛逼，而是要通过以往的事例，来说明自己有价值。

### 3.3.**问题三：你怎么看待加班？**

错误答案1：我讨厌加班。

错误答案2：我喜欢加班。

错误答案3：看心情来确定。

错误答案3：有加班费吗？

错误答案4：有钱就加，没钱不加。

错误答案5：有事就加，没事不加。

点评：看问题要看到本质，别直接回答yes或no. 也别傻乎乎太强调自己的感受，要牢记腾讯的利益。这个问题很好回答。

### **3.4.问题四：你对腾讯有什么了解？**

错误答案1：没了解过。

错误答案2：了解一些。

错误答案3：了解不多。

错误答案4：腾讯挺好的，所以我来面试。

错误答案5：腾讯是个大公司，福利待遇好。

点评：如果对目标公司和产品不了解，那就显得面试动机很不真诚。平时多了解一下腾讯公司和腾讯产品，不至于面试时把天聊死。

### 3.5.**问题五：你职业发展规划是怎样的？**

错误答案1：嗯，啊，哦…

错误答案2：这个问题我没有考虑过。

错误答案3：我是脚踏实地的人，走一步，看一步。

错误答案4：我不关心这些虚无的问题，我只想把当前的事情做好，一步一个脚印。

错误答案5：我想好了，十年后要做市级专家, 二十年后要做省级专家，三十年后要做国家级专家。

点评：如果没有思考过职业规划，说明不够用心，对自己不够负责。也别跟某某一样，海阔天空，畅想未来三十年。

### 3.6.**问题六：最打动你的一件事是什么？**

点评：别扯那些潸然泪下的感人故事。可以举一个完成了不可能完成任务的事例，深受打动，从中学到什么，对后续工作有什么帮助。

### 3.7.**问题七：你遇到的最大困难是什么？**

点评：不要诉苦和煽情，要强调遇到困难，并且千方百计解决了，学到了什么，对后续工作有什么帮助。

### 3.8.**问题八：工作中遇到不喜欢的人怎么办？**

点评：千万别说不喜欢就远离，还是要以工作和项目为重，合作完成共同目标，时刻考虑腾讯利益。

### 3.9.**问题九：你有在应聘其他公司吗？**

点评：如实陈述，如果同时在面试阿里、头条，如实说出来，也让腾讯hr知道，你是有很多选择余地的。

### 3.10.**问题十：你喜欢怎样的上级？**

点评：上级岂要看你喜欢与不喜欢？任何上级都要喜欢。一起工作，共同目的是做好产品，自己要去适应上级。回答时可以说喜欢有责任心、有目标感且为团队着想的上级。

### 3.11.**问题十一：你期望在工作中得到什么？**

点评：千万别说钱。要说自己期望得到展示能力的平台和提升自己的机会，并和公司一起成长。要牢记自己能给腾讯带来什么利益，别傻乎乎地强调自己要获得什么回报，尤其不要提钱。

### **3.12.问题十二：你觉得腾讯哪个产品有什么可以改进的地方？**

点评：别说一切都好无需改进，这显得很没有思考且不认真。也别瞎抱怨一通，只能轻微地说有哪些地方不足，并且一定要想出改进措施。任何没有改进措施的意见，都是添乱。

### 3.13.**问题十三：你有什么缺点？**

点评：不能说自己没有缺点 ，也不能把自己的缺点无限放大，如果说自己懒惰、有拖延症，这对面试有什么好处？还有人说自己有完美主义情结，这太俗套了。可以考虑说一些人性通病，比如自己工作有时比较着急，但要强调都是为了工作项目考虑，并说自己在逐渐改进。

### 3.14.**问题十四：你的短期目标是什么？**

点评：千万别说短期目标是为了找一份工作，多么没意思的回答啊。可以说自己短期目标是学习一门新知识，希望尽快学会，并且应用到工作之中。

### **3.15.问题十五：如果成功应聘，你打算在腾讯做多久？**

点评：如果回答1年，说明不稳定，何必要招你？如果回答20年，那就是乱扯。可以考虑一种循序渐进的方式来回答。方式多得很，也很好回答。

### 3.16.**问题十六：谈一次失败的经历。**

点评：别只说失败如何惨痛，要强调从失败中学到什么。而且，在挑选失败案例时，不要是那种损失很大的事例。假如，你让公司损失了1亿美金，也好意思说？可以选择那些损失不大但意义重大的失败经历。

### 3.17.**问题十七：说一下你的家庭吧。**

点评：别陷入温柔的陷阱，尽说家里几口人并如何和睦恩爱。这对面试没有帮助。可以考虑强调家庭教育，比如从小爸妈就教育自己要正直、积极乐观、团结进取，有责任心。你看，这和腾讯价值观又暗中结合起来了。

### 3.18.**问题十八：你最欣赏谁？**

点评：别说一些乌七八糟的明星。低俗！也别仅仅说一个名字就结束。要尽量选真正能打动自己的人物，并简要说明为什么欣赏他。

### 3.19.**问题十九：你喜欢怎样的工作氛围？**

点评：这不是你妈妈问你，所以别随心所欲敞开心扉。这是hr问你，是在面试，容不得你挑选那么多。所以，不要太强调自己的内心喜好。可以说自己喜欢有活力、有目标感、有责任感、有创造力的氛围。这种回答，没人能反驳，况且也和腾讯的氛围吻合呢。

### 3.20.**问题二十：你为什要这么高的工资？能不能向下调整？**

点评：千万别扯什么家里开支大，或者买房了，所以需要钱。也别说自己之前工资高，跳槽更要涨一波。搞清楚，你是要强调自己有什么能力和价值，配得起这么高的工资，配得起这个涨幅。至于预期工资是否可以向下调整，这是进入了讨价还价的环节。直接说可以就是在自降身价，直接说不可以就是在自断后路。前面点评那么多了，这个问题其实很好作答。

## **4. 接受电子offer攻略**

电子offer，就是正式的书面offer，这还要什么攻略呢？确实不需要，但也还是有些事情需要注意。

郭靖只有收到offer邮件才算数，才能开始跟自己的领导洪七公提离职，所以，这个时间节点要把控好。

收到电子offer，记得低调，千万别在公司炫耀，也别在网上忘乎所以。当然，郭靖这种职场老手，是不会犯这种低级错误的。接下来，就是背景调查了。

## **5. 背景调查攻略**

背景调查，其实谈不上什么攻略，姑且称之为注意事项吧。首先，基本信息不能造假，这个很好理解，毋庸置疑，无需强调。毕竟，正直是腾讯的最重要价值观。其次，郭靖在找同事帮做背调时，要尽量选择关系好且会说话的，比如黄蓉。如果郭靖选择欧阳克帮背调，可能招黑，百口难辩。最后，在提离职后，进行工作交接务必尽心尽力，不能跟领导和同事把关系闹僵，背景调查会问这些东西。基本的职业道德，还是要有。

背景调查公司会通过很多方式，调查郭靖的基本信息。然后，也会打电话给郭靖的直接领导(洪七公)、郭靖的同事(黄蓉)和郭靖所在部门的hr(华筝).

那么，背景调查公司打电话给洪七公、黄蓉、华筝，都会问一些什么问题呢？其实，无非就是问郭靖的基本情况和表现，比如问郭靖所在部门，比如问郭靖的入职时间和离职时间，比如问郭靖的人品、平时表现和工作交接等问题。

洪七公作为领导，肯定不是第一次做背调，一般不会故意为难郭靖，所以基本会正常回答。华筝作为hr, 也会如实回答问题。黄蓉聪明伶俐，夸人的本领，不显山漏水，但句句话都是在用真实案例夸郭靖。

就这样，背景调查公司拿到了真实信息，很满意，并把调查结果反馈给腾讯。还是那句话，身正不怕影子歪，对郭靖而言，背景调查无非就是个流程。

有的朋友可能觉得，自己平时跟领导的关系不好，怕领导说坏话，想提供一个假领导的电话供背调，行不行呢？

我的个人看法是，不建议。很容易穿帮，而且也确实不够诚实和正直。即使跟领导关系不好，也要想办法。

大家都是打工的，平时就要跟同事和领导进行良好合作，为了共同的目标而合作，弄僵关系只能说明情商不高。

背景调查这段时间，是郭靖在公司的最后一段时间了，要好好珍惜。坚持做好工作，做好交接，多写文档，好好表现，贡献最后一份热量。既然爱过，就和平分手，给彼此留下最美的回忆。

切记保持低调，不要跟无关人透漏即将离职，更不要透露去腾讯后的福利待遇，免得人心浮动，惹怒领导。有时，做孙子，甚至是装孙子，是一门学问。互联网圈子很小，口碑和人品很重要。

万事留一线，江湖好相见。轻轻地离开，不带走一片云彩。

## 6.**Part3: 实用建议**

终于，快接近尾声了，感谢你有耐心看到这里。最后，聊一下自己的个人看法，也许不成熟，姑且称之为建议吧。

**充分准备**

鲁迅先生曾经说过，如果准备得不成功，那就准备失败吧。鲁迅先生，是不会骗人的。

如果轰炸一个地方，需要投放10个炸弹，那你就准备投放20个，这叫饱和攻击，做好双倍的准备。

如果想去腾讯，那么建议珍惜面试机会。可以提前找关系比较好的朋友帮模拟面试演练，查缺补漏。也可以先找一些小公司练练手，找找面试感觉，何尝不可？

**不抛弃 不放弃**

如果面试不顺利，简历会被放回到人才库，其他部门仍可以让你去继续面试，只是还得从头再面。总之，机会多多，别灰心丧气。铩羽而归，总结反思，查缺补漏，继续尝试。

在这一点上，腾讯做得挺好的，是才就用，不管你是第几次面试。而其他公司，很可能是如果第一次面试不通过，要等到第二年才有资格再次面试。

退一步来讲，万一最终还是没有面试成功，也正常，并欣然接受。腾讯不应该是你唯一的目标，路子多得很，好公司多得很，失之东隅，收之桑榆。

**务实前行 持续努力**

如果最终顺利进入腾讯，开心一下，可以理解，但也不能太得意忘形。凡尔赛文学，不能太沉迷。

路还很长，要提升的地方也很多。站在一个新的平台上，高度更高了，但要区分自身价值和平台溢价，而不是盲目自嗨，错把平台价值当做自身价值。

谦卑心态，务实前行，低调发育，别浪。更厉害且更低调的人，腾讯有的是！得多向他们学习。

原文链接：[万字攻略，详解腾讯面试](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzIzNzg5MDg0OA%3D%3D%26mid%3D2247487030%26idx%3D2%26sn%3D32c0f951664e5eb675665cdff198071f%26chksm%3De8c0f279dfb77b6f21e2099390f2abefb82b57feebd77450300e6ce1690a3b9095b8ec612dba%26mpshare%3D1%26scene%3D23%26srcid%3D0621aSM7JscTxCEu1hKISFpV%26sharer_sharetime%3D1655793388356%26sharer_shareid%3Dfef2e677b8dd3654b49985aaa66fad0e%23rd)

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.83】关于【零声教育】2022年12代C/C++Linux服务器开发高级架构师课程学习心得，资源分享

## 1.前言

对于零声教育的C/C++Linux服务器高级架构师的课程到2022目前已经迭代到12代了，像之前小编也总结过，但是课程每期都有做一定的更新，也是为了更好的完善课程跟上目前互联网大厂的岗位技术需求，之前课程里面也包含了一些小的分支，其中就有音视频开发、Linux内核开发、DPDK、golang等等一些程序员所需要的硬核技术。今天总结分析是2022年最新的课程体系。

## 2.课程定位为中高级课程，学习这个课程也有一定的要求，以下是适应学习的一些人群：

1.从事业务开发多年，对底层原理理解不够深入的在职工程师

2.从事嵌入式方向开发，想转入互联网开发的在职工程师

3.从事Qt/MFC等桌面开发的，薪资多年涨幅不大的在职工程师

4.从事非开发岗位（算法岗，运维岗，测试岗），想转后台开发岗位的在职工程师

5.工作中技术没有挑战，工作中接触不到新技术的在职工程师

6.自己研究学习速度较慢，不能系统构建知识体系的开发人员

7.了解很多技米名词，但是深入细问又不理解的工程师

8.自己研究学习速度较慢，不能系统构建知识体系的开发人员了解很多技米名词，但是深入细问又不理解的工程师

## 3.课程方式介绍：课程是在腾讯课堂以直播的形式教学

1.98次直播课，持续8个半月，直播每周二四六晚8点到10点

2.课前预习资料课后思考实践作业

3.班主任督学作业统计博客统计

4.老师答疑工作问题课程问题

5.课程涉及编程语言45%的c，25%的c++，20%的go，5%的lua，5%的其他语言(Rust, shell, java, awk, python)

6.奖学金机制最高1000元，公开透明（一-期评选一-次)

## 4.课程优势：

1.简历梳理技术点凸显项目技术梳理

2.模拟面试技术表述

3.薪资谈判福利争取

4.offer选择职业规划技术前景

## **5.往期学员学习过程的心得总结：**

**1.学习要有主动性**。无论是开始的自学，还是后面的培训学习，学习的主观能动性一定要有，特别是报班学习之后，不要觉得万事有老师，外部的辅导条件能够让你有更好的学习效率和氛围，但是最终需要掌握技能的还是你自己的，所以学习的过程不要懈怠。

**2.学完技术内容之后，要形成自己的技术栈体系。**我在学完之后，就根据我自己的技术内容花了三天时间整理一份c/c++后端开发需要掌握的技术体系路线图，来帮助自己梳理自己所学的技术点。

**3.善于总结自己的学习过程。**每当自己学完一个小块的知识点之后，最好是将自己对它的理解整理成博客文章，这样既能自我梳理自己的学习成果，又能作为自己在面试工作时向面试官展现的一个亮点。

**4.一定要复盘自己的面试过程。**在我学习之后的面试过程，并不是一帆风顺。但是我在老师的建议下，不管成功的还是失败的面试过程，场场复盘！找出自己回答的不好的地方做备注修改，这样一次次下来，对于面试，我也是越来越胸有成竹。

**5.学习方式，不管黑猫白猫，抓住老鼠的就是好猫。**对于也想从事或是转行到c/c++后端开发岗的兄弟，如果考虑报班培训的话，可以推荐大家了解一下我之前学习过的课程，整个课程体系对标的是腾讯的T9级别。

## 6.课程内容总结：

### **6.1.精进基是专栏**

![img](https://pic3.zhimg.com/80/v2-b13527702b26f0a455e8e7cbb8db5852_720w.webp)

### 6.2.高性能网络专栏

![img](https://pic4.zhimg.com/80/v2-343424c17b3551c1e8e2f77e91789433_720w.webp)

### 6.3.基础组件设计专栏

![img](https://pic2.zhimg.com/80/v2-3b3396eae08403a2c21827ca57d57b5d_720w.webp)

### 6.4.中间件开发专栏

![img](https://pic2.zhimg.com/80/v2-9e9d4876f12cab678596bd4c9406bf35_720w.webp)

### 6.5.开源框架专栏

![img](https://pic3.zhimg.com/80/v2-0618943057dfe8eac2cc924c94222246_720w.webp)

### 6.6.云原生专栏

![img](https://pic4.zhimg.com/80/v2-3d26e2d0d8e6ed7003b0dc579e7c8c7f_720w.webp)

### 6.7.性能分析专栏

![img](https://pic2.zhimg.com/80/v2-ca3fbd03b4e1b68350ac071b5905e465_720w.webp)

### 6.8.分布式架构专栏

![img](https://pic4.zhimg.com/80/v2-4c995b9acf2deddfeda5f6f3ecf1b9bb_720w.webp)

### 6.9.上线项目实战

![img](https://pic3.zhimg.com/80/v2-d3d788b71c3ce34f58fb27a331249a8a_720w.webp)

### 7.**课程总结：**

现在的技术的学习曲线的增加，让我的忍耐性越来越低。各种新技术，因为新奇让人兴奋，但最终变成一场场争论。我越来越无法忍受这些充满市场宣传气息的喧嚣。我对技术看重的是稳定，清晰。

据不完全统计，截至目前(2018.07)为止，中国C++程序员的数量已经超过了100万。而且，随着IT培训业的持续发展和大量的应届毕业生进入社会，C++程序员面临的竞争压力越来越大。那么，作为一名C++程序员，怎样努力才能快速成长为一名高级的程序员或者架构师，或者说一名优秀的高级工程师或架构师应该有怎样的技术知识体系，这不仅是一个刚刚踏入职场的初级程序员，也是工作三五年之后开始迷茫的老程序员，都必须要面对和想明白的问题。

技术的瓶颈是认知的问题， 认知不是知其名，还需要知其因，更需要知其原。

最后祝大家都学有所成。

原文链接：https://zhuanlan.zhihu.com/p/519780422

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.84】2022全网最详细的音视频开发学习路线，零基础到项目实战，从小白到音视频专家



## 1.**前言**

音视频的历史与前景在这里就不介绍了，小编之前的文章里面都有讲到。

## 2.行业现状分析

- 核心竞争力：定义音视频是程序届的皇冠，掌握音视频意味着拿到通往未来的船票，不用担心会被其他人替代。音视频是有门槛的。是与其他人拉开差距的分水岭
- 高端人才相关缺乏：Boss直聘中，北上广深很多年限上50w-70w的音视频岗位，常年还招不到人，月薪2-3万大多是刚从事音视频入门级开发者
- 技术迭代慢：就H264编码从95年成为标准至今，都在使用。比较偏底层技术，底层技术几十年不会有太大的改变

## 3.**招聘的公司**

招聘的公司行业分布广泛（以下列举大部分是互联网公司）

**互联网**

掌门科技 哔哩哔哩 字节跳动 腾讯 欢聚时代 快手 阿里巴巴 虎牙

即构科技 网易 小米 商汤科技 融云 蚂蚁金服 爱奇艺 …等等

**计算机软件**

华为 海康威视 浙江大华 …等等

**移动互联网**

常相伴(互动娱乐) 声网 …等等

**电子商务**

京东 拼多多 美团 …等等

**人力资源服务**

Boss直聘

## 4.音视频开发岗位

说道薪资待遇什么的，想必是你们比较关心的一件事。我就随便在Boss直聘搜索的音视频开发岗位的薪资。

从上图可以看出音视频开发岗位的薪资平均都是在20K以上，但是他们对学历还是有一定的要求的最低基本都是本科学历。

**为什么音视频的开发薪资这么高呢？**

因为是音视频岗位人才稀缺，很少有人会完整的音视频开发。

## **5.音视频开发总体学习知识点：**

**1.首先需要学习的是音视频基础知识**

有些朋友对音视频学习有些误区，以为需要很多的理论基础才开始去实践，实际上大家只需要懂：

视频：

- YUV格式
- RGB格式
- 帧率
- 分辨率
- H264 I P B帧原理

音频：

- PCM
- 采样率
- 采样格式
- 码率

等这些基础知识掌握了之后即可开启音视频学习的大门。

### **5.1.音视频基础开发知识：**

| 音视频基础知识 | o 音频基础知识，PCM格式、编码原理等  |
| :------------: | :----------------------------------: |
|                | o 视频基础知识，YUV格式、IPB帧原理等 |
|                |                                      |
| Ffmpeg环境搭建 |            o Windows平台             |
|                |             o Linux平台              |
|                |              o MAC平台               |
|                |                                      |
|    常用工具    |             o MediaInfo              |
|                |        o VLC播放器 o EasyICE         |

### **5.2.FFmpeg命令实战**

|  命令帮助  |  o 如何查询Ffmpeg命令帮助   |
| :--------: | :-------------------------: |
|            |      o Ffmpeg命令分类       |
|            |                             |
| ffplay命令 |      o ffplay播放控制       |
|            |       o ffplay播放pcm       |
|            |       o ffplay播放yuv       |
|            |   o ffplay使用filter播放    |
|            |                             |
| ffmpeg命令 |          o pcm提取          |
|            |          o yuv提取          |
|            |          o aac提取          |
|            |         o h264提取          |
|            |       o 视频录制命令        |
|            | o 多媒体文件的分解/复用命令 |
|            |      o 裁剪与合并命令       |
|            |     o 图片/视频互转命令     |
|            |       o 直播相关命令        |
|            |       o 各种滤镜命令        |

### **5.3.FFmpeg编程实战**

| 音视频渲染实战 |     o SDL环境搭建      |
| :------------: | :--------------------: |
|                |     o SDL事件处理      |
|                |     o SDL线程处理      |
|                |   o YUV视频播放实战    |
|                |   o PCM声音播放实战    |
|                |                        |
| FFmpeg API精讲 |    o FFmpeg框架分析    |
|                |  o FFmpeg内存模型分析  |
|                | o FFmpeg常用结构体精讲 |
|                |                        |
|  音视频编解码  |  o FFmpeg解码流程分析  |
|                |  o FFmpeg编码流程分析  |
|                |    o AAC编解码原理     |
|                |    o H264编解码原理    |
|                |     o AAC解码实战      |
|                |     o AAC编码实战      |
|                |     o H264解码实战     |
|                |     o H264编码实战     |
|                |     o MP3转AAC实战     |
|                |                        |
| 音视频封装格式 |   o FLV封装格式分析    |
|                |   o MP4封装格式分析    |
|                |   o 多媒体解复用实战   |
|                |    o 多媒体复用实战    |
|                | o 多媒体转封装格式实战 |
|                |      o 音频重采样      |
|                |     o 视频尺寸变换     |
|                |                        |
|  音视频过滤器  |     o 音视频过滤器     |
|                |      o 视频过滤器      |
|                |  o 过滤器实际项目实现  |
|                |                        |
|  ffplay播放器  |  o 掌握ffplay.c的意义  |
|                |    o ffplay框架分析    |
|                |      o 音视频解码      |
|                |      o 音视频控制      |
|                |      o 音视频同步      |
|                |       o 参数机制       |
|                |                        |
| 播放器开发实战 |    o 播放器框架分析    |
|                |       o 模块划分       |
|                |      o 音视频解码      |
|                |      o 播放器控制      |
|                |      o 音视频同步      |
|                |                        |
| ffmpeg录制转码 |  o 掌握ffmpeg.c的意义  |
|                |    o ffmpeg框架分析    |
|                |      o 音视频编码      |
|                |     o 封装格式转换     |
|                |       o 提取音频       |
|                |       o 提取视频       |
|                |       o logo叠加       |
|                |    o 音视频文件拼接    |
|                |      o filter机制      |

### **5.4.流媒体客户端实战**

|   RTMP流媒体   |      o RTMP协议分析       |
| :------------: | :-----------------------: |
|                |    o wireshark抓包分析    |
|                |      o H264 RTMP封装      |
|                |      o AAC RTMP封装       |
|                |      o RTMP拉流实战       |
|                |      o H264 RTMP解析      |
|                |      o AAC RTMP解析       |
|                |      o RTMP推流实战       |
|                |                           |
|   HLS流媒体    |       o HLS协议分析       |
|                |      o HTTP协议分析       |
|                |       o TS格式分析        |
|                |    o wireshark抓包分析    |
|                |       o HLS拉流实战       |
|                |   o FFmpeg HLS源码分析    |
|                |      o HLS多码率机制      |
|                |                           |
| HTTP-FLV流媒体 |    o HTTP-FLV协议分析     |
|                |    o wireshark抓包分析    |
|                |   o http chunk机制分析    |
|                |    o HTTP-FLV拉流实战     |
|                | o FFmpeg HTTP-FLV源码分析 |
|                |                           |
| RTSP流媒体实战 |      o RTSP协议分析       |
|                |       o RTP协议分析       |
|                |      o H264 RTP封装       |
|                |      o H264 RTP解析       |
|                |       o AAC RTP封装       |
|                |       o AAC RTP解析       |
|                |      o RTCP协议分析       |
|                |  o RTSP流媒体服务器搭建   |
|                |      o RTSP推流实战       |
|                |      o RTSP拉流实战       |
|                |    o wireshark抓包分析    |

### **5.5.SRS流媒体服务器**

| SRS 3.0源码剖析 |    o 整体框架分析     |
| :-------------: | :-------------------: |
|                 |    o RTMP推流分析     |
|                 |    o RTMP拉流分析     |
|                 |     o HLS拉流分析     |
|                 |  o HTTP-FLV拉流分析   |
|                 |   o FFmpeg转码分析    |
|                 |  o 首屏秒开技术分析   |
|                 | o forward集群源码分析 |
|                 |  o edge集群源码分析   |
|                 |  o 负载均衡部署方式   |

### **5.6.WebRTC实战**

|   WebRTC中级开发    |     o WebRTC通话原理分析      |
| :-----------------: | :---------------------------: |
|                     |     o WebRTC开发环境搭建      |
|                     |     o coturn最佳搭建方法      |
|                     |     o 如何采集音视频数据      |
|                     |     o 一对一通话时序分析      |
|                     |       o 信令服务器设计        |
|                     |           o SDP分析           |
|                     |      o Candidate类型分析      |
|                     |        o Web一对一通话        |
|                     |      o Web和Android通话       |
|                     |       o AppRTC快速演示        |
|                     |                               |
|   WebRTC高级开发    |     o 自定义摄像头分辨率      |
|                     |          o 码率限制           |
|                     |       o 调整编码器顺序        |
|                     |      o Mesh模型多方通话       |
|                     |        o Janus框架分析        |
|                     |   o Janus Web客户端源码分析   |
|                     | o Janus Android客户端源码分析 |
|                     | o Janus Windows客户端源码分析 |
|                     |        o Janus信令设计        |
|                     |    o 基于Janus实现会议系统    |
|                     |       o WebRTC源码编译        |
|                     |                               |
| Janus服务器源码分析 |          o 源码结构           |
|                     |          o 插件机制           |
|                     |          o 线程分析           |
|                     |        o 信令交互过程         |
|                     |        o videoroom分析        |
|                     |                               |
|   SRS4.x源码分析    |     o RTMP转发WebRTC逻辑      |
|                     |     o WebRTC转发RTMP逻辑      |
|                     |   o WebRTC音视频一对一通话    |
|                     |       o WebRTC多人通话        |
|                     |     o WebRTC SFU模型分析      |
|                     |          o stun分析           |
|                     |          o turn分析           |
|                     |           o sdp分析           |
|                     |           o rtp分析           |
|                     |          o srtp分析           |
|                     |        o 拥塞控制算法         |
|                     |             o FEC             |
|                     |        o jitter buffer        |

### **5.7.Android NDK开发**

|       音视频开发       |        o So库适配总结        |
| :--------------------: | :--------------------------: |
|                        | o JNI Native层构建 Java 对象 |
|                        |        o JNI异常处理         |
|                        |         o 编译FFmpeg         |
|                        |       o 编译ijkplayer        |
|                        |   o 基于ijkplayer二次开发    |
|                        |   o OpenSL ES播放音频数据    |
|                        |     o MediaCodec硬件解码     |
|                        |  o OpenGL ES Shader显示视频  |
|                        |        o RTMP推流直播        |
| GSYVideoPlayer源码分析 |       o 多视频同时播放       |
|                        |          o gif截图           |
|                        |        o 边播放边缓存        |
|                        |            o 水印            |
|                        |            o 弹幕            |

### 6.**总结：**

**以上就是音视频开发学习知识点，相比音视频开发自学非常困难的，网上通俗易懂的难找到，网上部分资源是对应的开源版本较低，比如雷霄骅(雷神)博客的FFmpeg版本较低，音视频涉及大量的开源库和协议，比如ffmpeg的编译，如果不熟悉各种编译报错，Janus编译涉及各种开源组件编译不通过，比如WebRTC涉及的RTP RTCP SDP STUN等协议，部分嵌入式音视频的朋友技术面窄，技术不深入。**

原文链接：https://zhuanlan.zhihu.com/p/478164918

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.85】高性能库DPDK精简理解

## 1.前言

才开始接触到DPDK，发现概念很多，很难以下了解，在这文章中记录下关键的内容，做到对dpdk的基本东西真正了解了。
这样后面用它来写程序才可能顺利，不能赶进度啊，越赶进度反而可能越慢，慢慢来比较快。
本文主要是自己理解，参考很多文章，有哪里不理解的就查，做不到精深，只了解含义。
文章算是汇编，参考多篇文章，如有侵权，请告知，谢谢！

## 2. 整体理解

历史：
随着计算机核数的增加，网络带宽的增加，对主机进行网络包的处理性能要求越来越高，但是现在的操作系统对网络包处理的方式很低效。
低效表现在：
1）网络数据包来了之后通过中断模式进行通知，而cpu处理中断的能力是一定的，如果网络中有大量的小数据包，造成了网络的拥堵，cpu处理不及时。
【以前cpu的频率远高于网络设备，所以中断很有效】
2）操作系统的协议栈是单核处理，没办法利用现在操作系统的多核。
3）网络数据包从网卡到内核空间，再到用户空间，进行了多次数据拷贝，性能比较差。
DPDK 全称 Data Plane Development Kit 专注于数据面的软件开发套件，是专为Intel的网络芯片开发，运行于Linux和FreeBsd上。
DPDK改变了传统的网络数据包的处理方式，在用户空间直接处理，图示如下：

![img](https://pic4.zhimg.com/80/v2-457c406c6f5f666235ba0aec07e35613_720w.webp)

传统VSDPDK抓包方式

## 3.重要概念理解

这里面说明DPDK文档里面的主要概念，另外如何将概念与实际的我们自己的机器上参数对应起来。

### 3.1 .PPS：包转发率

即1s可以发送多个frame、在以太网里面为以太帧，我们常说的接口带宽为1Gbits/s 、10Gbits/s 代表以太接口能够传输的最高速率，单位为（bit per second 位/秒）
实际上，传输过程中，帧之间有间距（12个字节），每个帧前面还有前导（7个字节）、帧首界定符（1个字节）。
帧理论转发率= BitRate/8 / (帧前导+帧间距+帧首界定符+报文长度）

![img](https://pic3.zhimg.com/80/v2-102a26372bd203e0d55c9f27927d5d1a_720w.webp)

以太帧传输中结构

按照10Gbits/s （没记错的话是万兆光纤）来计算下64个字节下的包的转发率。

![img](https://pic3.zhimg.com/80/v2-89783a27999f5558ee9d4bd78554c9b2_720w.webp)

最短帧大小

10*1024*1024*1024*1024/(12+7+1+64) *8 约等于 1000M*10 /(12+7+1+64) *8 = 14.880952380952381 M/PPS （百万数据包）
也就是1s可以发送 1千400万个数据包。
注意，这里面的Data长度是在46-1500个字节之间，所以最小的帧的长度为 ： 6+6+2+46+4 = 64个字节。
线速：网卡或网络支持的最极限速度。
汇总数据：

![img](https://pic1.zhimg.com/80/v2-b59633a751fbb182d04f050d831e4504_720w.webp)

网卡的限速

arrival为每个数据包之间的时间间隔。
rte：runtime environment 即运行环境。
eal： environment abstraction layer 即抽象环境层。

### **3.2. UIO：用户空间IO**

小的内核模块，用于将设备内存映射到用户空间，并且注册中断。
uio_pci_generic 为linux 内核模块，提供此功能，可以通过 modprobe uio_pci_generic 加载。
但是其不支持虚拟功能，DPDK，提供一个替代模块 igb_uio模块，通过
sudo modprobe uio
sudo insmod kmod/igb_uio.ko
命令加载。

### **3.3. VFIO**

VFIO是一个可以安全的把设备IO、中断、DMA等暴露到用户空间（usespace），从而在用户空间完成设备驱动的框架。用户空间直接访问设备，虚拟设备的分配可以获得更高的IO性能。
参考（[https://blog.csdn.net/wentyoon/article/details/60144824](https://link.zhihu.com/?target=https%3A//blog.csdn.net/wentyoon/article/details/60144824)）
sudo modprobe vfio-pci
命令加载vfio驱动。
1.将两个82599以太网绑定到VFIO ./tools/dpdk_nic_bind.py -b vfio-pci 03：00.0 03：00.1
3.将82599 ehter绑定到IGB_UIO ./tools/dpdk_nic_bind.py -b igb_uio 03：00.0 03：00.1
可参看：[http://www.cnblogs.com/vancasola/p/9378970.html](https://link.zhihu.com/?target=http%3A//www.cnblogs.com/vancasola/p/9378970.html) 进行配置vfio驱动模式。
两者都是用户空间的网卡驱动模块，只是据说UIO依赖IOMMU，VFIO性能更好，更安全，不过必须系统和BSIO支持
通过工具查看现在的绑定情况：

![img](https://pic3.zhimg.com/80/v2-a6518a53b7d2ea0eed9a02c3f991a86a_720w.webp)

### 3.4网卡的限速

说明： 以上driv谁说明在使用的网卡驱动，后面unused为未使用可以兼容的网卡驱动。
绑定命令：
./dpdk-devbind.py –bind=ixgbe 01:00.0

![img](https://pic3.zhimg.com/80/v2-9c45e2976857201a4c376ee462476cde_720w.webp)

绑定网卡和驱动

注意在DPDK的驱动情况下，用ifconfig是看不到网卡的。

### 3.5. PMD

PMD, Poll Mode Driver 即轮询驱动模式 ，DPDK用这种轮询的模式替换中断模式

### 3.6 .RSS

RSS(Receive Side Scaling)是一种能够在多处理器系统下使接收报文在多个CPU之间高效分发的网卡驱动技术。

网卡对接收到的报文进行解析，获取IP地址、协议和端口五元组信息
网卡通过配置的HASH函数根据五元组信息计算出HASH值,也可以根据二、三或四元组进行计算。
取HASH值的低几位(这个具体网卡可能不同)作为RETA(redirection table)的索引
根据RETA中存储的值分发到对应的CPU
DPDK支持设置静态hash值和配置RETA。 不过DPDK中RSS是基于端口的，并根据端口的接收队列进行报文分发的。 例如我们在一个端口上配置了3个接收队列(0,1,2)并开启了RSS，那么 中就是这样的:

{0,1,2,0,1,2,0………}

运行在不同CPU的应用程序就从不同的接收队列接收报文，这样就达到了报文分发的效果。
在DPDK中通过设置rte_eth_conf中的mq_mode字段来开启RSS功能， rx_mode.mq_mode = ETH_MQ_RX_RSS。
当RSS功能开启后，报文对应的rte_pktmbuf中就会存有RSS计算的hash值，可以通过pktmbuf.hash.rss来访问。 这个值可以直接用在后续报文处理过程中而不需要重新计算hash值，如快速转发，标识报文流等。

### 3.7 .对称RSS

在网络应用中，如果同一个连接的双向报文在开启RSS之后被分发到同一个CPU上处理，这种RSS就称为对称RSS。 DPDK的hash算法没办法做到这一点，
对我们需要解析http报文，那么请求和访问如果采用普通的rss就造成了发送和返回报文无法匹配的问题，如果dpdk要支持需要替换其Hash算法。

### 3.8. NUMA架构

NUMA(Non-Uniform Memory Architecture 非一致性内存架构）系统。
特点是每个处理器都有本地内存、访问本地的内存块，访问其他处理器对应的内存需要通过总线，慢。

![img](https://pic1.zhimg.com/80/v2-163afc1a9a00a063721a026a018abddc_720w.webp)

NUMA架构

![img](https://pic1.zhimg.com/80/v2-077e36e59ec9e177af1a1a569e05fc50_720w.webp)

经典计算机架构

### 3.9. Hugepages大页内存

操作系统中，内存分配是按照页为单位分配的，页面的大小一般为4kB，如果页面大小固定内存越大，对应的页项越多，通过多级内存访问越慢，TLB方式访问内存更快，
但是TLB存储的页项不多，所以需要减少页面的个数，那么就通过增加页面大小的办法，增大内存页大小到2MB或1GB等。
DPDK主要分为2M和1G两种页面，具体支持要靠CPU，可以从cpu的flags里面看出来，举个例子：
如果flags里面有pse标识，标识支持2M的大内存页面；
如果有pdge1gb 标识，说明支持1G的大内存页。

![img](https://pic4.zhimg.com/80/v2-3a645123eecb54c2d7075d2a12a6b76f_720w.webp)

cpu的大页支持

![img](https://pic2.zhimg.com/80/v2-cc08954d47b1d8900434b0c5f2a71a65_720w.webp)

查看内存大页信息

## 4.重要模块划分

以下为重要的内核模块划分。

![img](https://pic4.zhimg.com/80/v2-02b6c344a450c75431734e7af571906f_720w.webp)

原文链接：https://zhuanlan.zhihu.com/p/281250848

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.86】带你快速了解 Docker 和 Kubernetes

> 从单机容器化技术 Docker 到分布式容器化架构方案 Kubernetes，当今容器化技术发展盛行。本文面向小白读者，旨在快速带领读者了解 Docker、Kubernetes 的架构、原理、组件及相关使用场景。

## 1.**Docker**

### 1.1 什么是 Docker

Docker 是一个开源的应用容器引擎，是一种资源虚拟化技术，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上。虚拟化技术演历路径可分为三个时代：

1. 物理机时代，多个应用程序可能跑在一台物理机器上
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151450107586854.png)
2. 虚拟机时代，一台物理机器启动多个虚拟机实例，一个虚拟机跑多个应用程序
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151450403594316.png)
3. 容器化时代，一台物理机上启动多个容器实例，一个容器跑多个应用程序
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151450571828054.png)

在没有 Docker 的时代，我们会使用硬件虚拟化（虚拟机）以提供隔离。这里，虚拟机通过在操作系统上建立了一个中间虚拟软件层 Hypervisor ，并利用物理机器的资源虚拟出多个虚拟硬件环境来共享宿主机的资源，其中的应用运行在虚拟机内核上。但是，虚拟机对硬件的利用率存在瓶颈，因为虚拟机很难根据当前业务量动态调整其占用的硬件资源，加之容器化技术蓬勃发展使其得以流行。

Docker、虚拟机对比：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151451137564461.png)

另外开发人员在实际的工作中，经常会遇到测试环境或生产环境与本地开发环境不一致的问题，轻则修复保持环境一致，重则可能需要返工。但 Docker 恰好解决了这一问题，它将软件程序和运行的基础环境分开。开发人员编码完成后将程序整合环境通过 DockerFile 打包到一个容器镜像中，从根本上解决了环境不一致的问题。

### 1.2 Docker 的构成

Docker 由镜像、镜像仓库、容器三个部分组成

- 镜像: 跨平台、可移植的程序+环境包
- 镜像仓库: 镜像的存储位置，有云端仓库和本地仓库之分，官方镜像仓库地址（https://hub.docker.com/）
- 容器: 进行了资源隔离的镜像运行时环境

### 1.3 Docker 的实现原理

到此读者们肯定很好奇 Docker 是如何进行资源虚拟化的，并且如何实现资源隔离的，其核心技术原理主要有(内容部分参考自 Docker 核心技术与实现原理)：
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151451317631587.png)

#### **1.3.1 Namespace**

> 在日常使用 Linux 或者 macOS 时，我们并没有运行多个完全分离的服务器的需要，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，这是很多时候我们都不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。

命名空间 (Namespaces) 是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。Linux 的命名空间机制提供了以下七种不同的命名空间，通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。

1. CLONE_NEWCGROUP
2. CLONE_NEWIPC
3. CLONE_NEWNET
4. CLONE_NEWNS
5. CLONE_NEWPID
6. CLONE_NEWUSER
7. CLONE_NEWUTS

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151451544165176.png)

在 Linux 系统中，有两个特殊的进程，一个是 pid 为 1 的 /sbin/init 进程，另一个是 pid 为 2 的 kthreadd 进程，这两个进程都是被 Linux 中的上帝进程 idle 创建出来的，其中前者负责执行内核的一部分初始化工作和系统配置，也会创建一些类似 getty 的注册进程，而后者负责管理和调度其他的内核进程。

当在宿主机运行 Docker，通过`docker run`或`docker start`创建新容器进程时，会传入 CLONE_NEWPID 实现进程上的隔离。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151452065487278.png)

接着，在方法`createSpec`的`setNamespaces`中，完成除进程命名空间之外与用户、网络、IPC 以及 UTS 相关的命名空间的设置。

```
func (daemon *Daemon) createSpec(c *container.Container) (*specs.Spec, error) { s := oci.DefaultSpec() // ... if err := setNamespaces(daemon, &s, c); err != nil {  return nil, fmt.Errorf("linux spec namespaces: %v", err) } return &s, nil}func setNamespaces(daemon *Daemon, s *specs.Spec, c *container.Container) error { // user // network // ipc // uts // pid if c.HostConfig.PidMode.IsContainer() {  ns := specs.LinuxNamespace{Type: "pid"}  pc, err := daemon.getPidContainer(c)  if err != nil {   return err  }  ns.Path = fmt.Sprintf("/proc/%d/ns/pid", pc.State.GetPID())  setNamespace(s, ns) } else if c.HostConfig.PidMode.IsHost() {  oci.RemoveNamespace(s, specs.LinuxNamespaceType("pid")) } else {  ns := specs.LinuxNamespace{Type: "pid"}  setNamespace(s, ns) } return nil}
```

##### **网络**

当 Docker 容器完成命名空间的设置，其网络也变成了独立的命名空间，与宿主机的网络互联便产生了限制，这就导致外部很难访问到容器内的应用程序服务。Docker 提供了 4 种网络模式，通过`--net`指定。

1. host
2. container
3. none
4. bridge

由于后续介绍 Kubernetes 利用了 Docker 的 bridge 网络模式，所以仅介绍该模式。Linux 中为了方便各网络命名空间的网络互相访问，设置了 Veth Pair 和网桥来实现，Docker 也是基于此方式实现了网络通信。

下图中 `eth0` 与 `veth9953b75` 是一个 Veth Pair，`eth0` 与 `veth3e84d4f` 为另一个 Veth Pair。Veth Pair 在容器内一侧会被设置为 `eth0` 模拟网卡，另一侧连接 Docker0 网桥，这样就实现了不同容器间网络的互通。加之 Docker0 为每个容器配置的 iptables 规则，又实现了与宿主机外部网络的互通。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151452231388903.png)

挂载点

> 解决了进程和网络隔离的问题，但是 Docker 容器中的进程仍然能够访问或者修改宿主机器上的其他目录，这是我们不希望看到的。

在新的进程中创建隔离的挂载点命名空间需要在 clone 函数中传入 CLONE_NEWNS，这样子进程就能得到父进程挂载点的拷贝，如果不传入这个参数子进程对文件系统的读写都会同步回父进程以及整个主机的文件系统。当一个容器需要启动时，它一定需要提供一个根文件系统（rootfs），容器需要使用这个文件系统来创建一个新的进程，所有二进制的执行都必须在这个根文件系统中，并建立一些符号链接来保证 IO 不会出现问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151452363533407.png)

另外，通过 Linux 的`chroot`命令能够改变当前的系统根目录结构，通过改变当前系统的根目录，我们能够限制用户的权利，在新的根目录下并不能够访问旧系统根目录的结构个文件，也就建立了一个与原系统完全隔离的目录结构。

#### **1.3.2 Control Groups(CGroups)**

Control Groups(CGroups) 提供了宿主机上物理资源的隔离，例如 CPU、内存、磁盘 I/O 和网络带宽。主要由这几个组件构成：

1. **控制组（CGroup）** 一个 CGroup 包含一组进程，并可以在这个 CGroup 上增加 Linux Subsystem 的各种参数配置，将一组进程和一组 Subsystem 关联起来。
2. **Subsystem** 子系统 是一组资源控制模块，比如 CPU 子系统可以控制 CPU 时间分配，内存子系统可以限制 CGroup 内存使用量。可以通过`lssubsys -a`命令查看当前内核支持哪些 Subsystem。
3. **Hierarchy** 层级树 主要功能是把 CGroup 串成一个树型结构，使 CGruop 可以做到继承，每个 Hierarchy 通过绑定对应的 Subsystem 进行资源调度。
4. **Task** 在 CGroups 中，task 就是系统的一个进程。一个任务可以加入某个 CGroup，也可以从某个 CGroup 迁移到另外一个 CGroup。

在 Linux 的 Docker 安装目录下有一个 docker 目录，当启动一个容器时，就会创建一个与容器标识符相同的 CGroup，举例来说当前的主机就会有以下层级关系：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151452499860071.png)
每一个 CGroup 下面都有一个 tasks 文件，其中存储着属于当前控制组的所有进程的 pid，作为负责 cpu 的子系统，cpu.cfs_quota_us 文件中的内容能够对 CPU 的使用作出限制，如果当前文件的内容为 50000，那么当前控制组中的全部进程的 CPU 占用率不能超过 50%。

当我们使用 Docker 关闭掉正在运行的容器时，Docker 的子控制组对应的文件夹也会被 Docker 进程移除。

#### **1.3.3 UnionFS**

> 联合文件系统(Union File System)，它可以把多个目录内容联合挂载到同一个目录下，而目录的物理位置是分开的。UnionFS 可以把只读和可读写文件系统合并在一起，具有写时复制功能，允许只读文件系统的修改可以保存到可写文件系统当中。Docker 之前使用的为 AUFS(Advanced UnionFS)，现为 Overlay2。

Docker 中的每一个镜像都是由一系列只读的层组成的，Dockerfile 中的每一个命令都会在已有的只读层上创建一个新的层：

```
FROM ubuntu:15.04COPY . /appRUN make /appCMD python /app/app.py
```

容器中的每一层都只对当前容器进行了非常小的修改，上述的 Dockerfile 文件会构建一个拥有四层 layer 的镜像：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151453055881176.png)
当镜像被 命令创建时就会在镜像的最上层添加一个可写的层，也就是容器层，所有对于运行时容器的修改其实都是对这个容器读写层的修改。容器和镜像的区别就在于，所有的镜像都是只读的，而每一个容器其实等于镜像加上一个可读写的层，也就是同一个镜像可以对应多个容器。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151453177329176.png)

## **2.Kubernetes**

> Kubernetes，简称 K8s，其中 8 代指中间的 8 个字符。Kubernetes 项目庞大复杂，文章不能面面俱到，因此这个部分将向读者提供一种主线学习思路：
>
> - 什么是 Kubernetes？
> - Kubernetes 提供的组件及适用场景
> - Kubernetes 的架构
> - Kubernetes 架构模块实现原理
>
> 有更多未交代或浅尝辄止的地方读者可以查阅文章或书籍深入研究。

### 2.1 为什么要 Kubernetes

尽管 Docker 为容器化的应用程序提供了开放标准，但随着容器越来越多出现了一系列新问题：

- 单机不足以支持更多的容器
- 分布式环境下容器如何通信？
- 如何协调和调度这些容器？
- 如何在升级应用程序时不会中断服务？
- 如何监视应用程序的运行状况？
- 如何批量重新启动容器里的程序？
- …

Kubernetes 应运而生。

### 2.2 什么是 Kubernetes

Kubernetes 是一个全新的基于容器技术的分布式架构方案，这个方案虽然还很新，但却是 Google 十几年来大规模应用容器技术的经验积累和升华的重要成果，确切的说是 Google 一个久负盛名的内部使用的大规模集群管理系统——Borg 的开源版本，其目的是实现资源管理的自动化以及跨数据中心的资源利用率最大化。

Kubernetes 具有完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建的智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多力度的资源配额管理能力。同时，Kubernetes 提供了完善的管理工具，这些工具涵盖了包括开发、部署测试、运维监控在内的各个环节，不仅是一个全新的基于容器技术的分布式架构解决方案，还是一个一站式的完备分布式系统开发和支撑平台。

### 2.3 Kubernetes 术语

#### **2.3.1 Pod**

Pod 是 Kubernetes 最重要的基本概念，可由多个容器（一般而言一个容器一个进程，不建议一个容器多个进程）组成，它是系统中资源分配和调度的最小单位。下图是 Pod 的组成示意图，其中有一个特殊的 Pause 容器:

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151453301682228.png)
Pause 容器的状态标识了一个 Pod 的状态，也就是代表了 Pod 的生命周期。另外 Pod 中其余容器共享 Pause 容器的命名空间，使得 Pod 内的容器能够共享 Pause 容器的 IP，以及实现文件共享。以下是一个 Pod 的定义：

```
apiVersion: v1  # 分组和版本kind: Pod       # 资源类型metadata:  name: myWeb   # Pod名  labels:    app: myWeb # Pod的标签spec:  containers:  - name: myWeb # 容器名    image: kubeguide/tomcat-app:v1  # 容器使用的镜像    ports:    - containerPort: 8080 # 容器监听的端口    env:  # 容器内环境变量    - name: MYSQL_SERVICE_HOST      value: 'mysql'    - name: MYSQL_SERVICE_PORT      value: '3306'    resources:   # 容器资源配置      requests:  # 资源下限，m表示cpu配额的最小单位，为1/1000核        memory: "64Mi"        cpu: "250m"      limits:    # 资源上限        memory: "128Mi"        cpu: "500m"
```

> EndPoint : PodIP + containerPort，代表一个服务进程的对外通信地址。一个 Pod 也存在具有多个 Endpoint 的情 况，比如当我们把 Tomcat 定义为一个 Pod 时，可以对外暴露管理端口与服务端口这两个 Endpoint。

#### **2.3.2 Label**

Label 是 Kubernetes 系统中的一个核心概念，一个 Label 表示一个 key=value 的键值对，key、value 的值由用户指定。Label 可以被附加到各种资源对象上，例如 Node、Pod、Service、RC 等，一个资源对 象可以定义任意数量的 Label，同一个 Label 也可以被添加到任意数量的资源对象上。Label 通常在资源对象定义时确定，也可以在对象创建后动态添加或者删除。给一个资源对象定义了 Label 后，我们随后可以通过 Label Selector 查询和筛选拥有这个 Label 的资源对象，来实现多维度的资源分组管理功能，以便灵活、方便地进行资源分配、调 度、配置、部署等管理工作。

Label Selector 当前有两种表达式，基于等式的和基于集合的:

- `name=redis-slave`: 匹配所有具有标签`name=redis-slave`的资源对象。
- `env!=production`: 匹配所有不具有标签`env=production`的资源对象。
- `name in(redis-master, redis-slave)`:`name=redis-master`或者`name=redis-slave`的资源对象。
- `name not in(php-frontend)`:匹配所有不具有标签`name=php-frontend`的资源对象。

以 myWeb Pod 为例:

```
apiVersion: v1  # 分组和版本kind: Pod       # 资源类型metadata:  name: myWeb   # Pod名  labels:    app: myWeb # Pod的标签
```

当一个 Service 的 selector 中指明了这个 Pod 时，该 Pod 就会与该 Service 绑定

```
apiVersion: v1kind: Servicemetadata:  name: myWebspec:  selector:    app: myWeb  ports:  - port: 8080
```

#### **2.3.3 Replication Controller**

Replication Controller，简称 RC，简单来说，它其实定义了一个期望的场景，即声明某种 Pod 的副本数量在任意时刻都符合某个预期值。

RC 的定义包括如下几个部分：

- Pod 期待的副本数量
- 用于筛选目标 Pod 的 Label Selector
- 当 Pod 的副本数小于预期数量时，用于创建新 Pod 的模版(template)

```
apiVersion: v1kind: ReplicationControllermetadata:  name: frontendspec:  replicas: 3  # Pod 副本数量  selector:    app: frontend  template:   # Pod 模版    metadata:      labels:        app: frontend    spec:      containers:      - name: tomcat_demp        image: tomcat        ports:        - containerPort: 8080
```

当提交这个 RC 在集群中后，Controller Manager 会定期巡检，确保目标 Pod 实例的数量等于 RC 的预期值，过多的数量会被停掉，少了则会创建补充。通过`kubectl scale`可以动态指定 RC 的预期副本数量。

> 目前，RC 已升级为新概念——Replica Set(RS)，两者当前唯一区别是，RS 支持了基于集合的 Label Selector，而 RC 只支持基于等式的 Label Selector。RS 很少单独使用，更多是被 Deployment 这个更高层的资源对象所使用，所以可以视作 RS+Deployment 将逐渐取代 RC 的作用。

#### **2.3.4 Deployment**

Deployment 和 RC 相似度超过 90%，无论是作用、目的、Yaml 定义还是具体命令行操作，所以可以将其看作是 RC 的升级。而 Deployment 相对于 RC 的一个最大区别是我们可以随时知道当前 Pod“部署”的进度。实际上由于一个 Pod 的创建、调度、绑定节点及在目 标 Node 上启动对应的容器这一完整过程需要一定的时间，所以我们期待系统启动 N 个 Pod 副本的目标状态，实际上是一个连续变化的“部署过程”导致的最终状态。

```
apiVersion: v1kind: Deploymentmetadata:  name: frontendspec:  replicas: 3  selector:    matchLabels:      app: frontend    matchExpressions:      - {key: app, operator: In, values [frontend]}  template:    metadata:      labels:        app: frontend    spec:      containers:      - name: tomcat_demp        image: tomcat        ports:        - containerPort: 8080
```

#### **2.3.5 Horizontal Pod Autoscaler**

除了手动执行`kubectl scale`完成 Pod 的扩缩容之外，还可以通过 Horizontal Pod Autoscaling(HPA)横向自动扩容来进行自动扩缩容。其原理是追踪分析目标 Pod 的负载变化情况，来确定是否需要针对性地调整目标 Pod 数量。当前，HPA 有一下两种方式作为 Pod 负载的度量指标：

- CPUUtilizationPercentage，目标 Pod 所有副本自身的 CPU 利用率的平均值。
- 应用程序自定义的度量指标，比如服务在每秒内的相应请求数(TPS 或 QPS)

```
apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata:  name: php-apache  namespace: defaultspec:  maxReplicas: 3  minReplicas: 1  scaletargetRef:    kind: Deployment    name: php-apache  targetCPUUtilizationPercentage: 90
```

根据上边定义，当 Pod 副本的 CPUUtilizationPercentage 超过 90%时就会出发自动扩容行为，数量约束为 1 ～ 3 个。

#### **2.3.6 StatefulSet**

在 Kubernetes 系统中，Pod 的管理对象 RC、Deployment、DaemonSet 和 Job 都面向无状态的服务。但现实中有很多服务是有状态的，特别是 一些复杂的中间件集群，例如 MySQL 集群、MongoDB 集群、Akka 集 群、ZooKeeper 集群等，这些应用集群有 4 个共同点。

1. 每个节点都有固定的身份 ID，通过这个 ID，集群中的成员可 以相互发现并通信。
2. 集群的规模是比较固定的，集群规模不能随意变动。
3. 集群中的每个节点都是有状态的，通常会持久化数据到永久 存储中。
4. 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功 能受损。

因此，StatefulSet 具有以下特点：

- StatefulSet 里的每个 Pod 都有稳定、唯一的网络标识，可以用来 发现集群内的其他成员。假设 StatefulSet 的名称为 kafka，那么第 1 个 Pod 叫 kafka-0，第 2 个叫 kafka-1，以此类推。

- StatefulSet 控制的 Pod 副本的启停顺序是受控的，操作第 n 个 Pod 时，前 n-1 个 Pod 已经是运行且准备好的状态。

- StatefulSet 里的 Pod 采用稳定的持久化存储卷，通过 PV 或 PVC 来 实现，删除 Pod 时默认不会删除与 StatefulSet 相关的存储卷(为了保证数 据的安全)。

- StatefulSet 除了要与 PV 卷捆绑使用以存储 Pod 的状态数据，还要与 Headless Service 配合使用。

  > Headless Service : Headless Service 与普通 Service 的关键区别在于， 它没有 Cluster IP，如果解析 Headless Service 的 DNS 域名，则返回的是该 Service 对应的全部 Pod 的 Endpoint 列表。

#### **2.3.7 Service**

Service 在 Kubernetes 中定义了一个服务的访问入口地址，前段的应用（Pod）通过这个入口地址访问其背后的一组由 Pod 副本组成的集群实例，Service 与其后端 Pod 副本集群之间则是通过 Label Selector 来实现无缝对接的。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151453558761925.png)

```
apiVersion: v1kind: servicemetadata:  name: tomcat_servicespec:  ports:  - port: 8080   name: service_port  - port: 8005   name: shutdown_port  selector:    app: backend
```

**Service 的负载均衡**

在 Kubernetes 集群中，每个 Node 上会运行着 kube-proxy 组件，这其实就是一个负载均衡器，负责把对 Service 的请求转发到后端的某个 Pod 实例上，并在内部实现服务的负载均衡和绘画保持机制。其主要的实现就是每个 Service 在集群中都被分配了一个全局唯一的 Cluster IP，因此我们对 Service 的网络通信根据内部的负载均衡算法和会话机制，便能与 Pod 副本集群通信。

**Service 的服务发现**

因为 Cluster IP 在 Service 的整个声明周期内是固定的，所以在 Kubernetes 中，只需将 Service 的 Name 和 其 Cluster IP 做一个 DNS 域名映射即可解决。

#### **2.3.8 Volume**

Volume 是 Pod 中能够被多个容器访问的共享目录，Kubernetes 中的 Volume 概念、用途、目的与 Docker 中的 Volumn 比较类似，但不等价。首先，其可被定义在 Pod 上，然后被 一个 Pod 里的多个容器挂载到具体的文件目录下；其次，Kubernetes 中的 Volume 与 Pod 的生命周期相同，但与容器的生命周期不相关，当容器终止或者重启时，Volume 中的数据也不会丢失。

```
template:  metadata:    labels:      app: frontend  spec:    volumes:  # 声明可挂载的volume      - name: dataVol       emptyDir: {}    containers:    - name: tomcat_demo      image: tomcat      ports:      - containerPort: 8080      volumeMounts:  # 将volume通过name挂载到容器内的/mydata-data目录        - mountPath: /mydata-data         name: dataVol
```

Kubernetes 提供了非常丰富的 Volume 类型:

- emptyDir，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为这是 Kubernetes 自动分配的一个目录，当 Pod 从 Node 上移除 emptyDir 中的数据也会被永久删除，适用于临时数据。
- hostPath，hostPath 为在 Pod 上挂载宿主机上的文件或目录，适用于持久化保存的数据，比如容器应用程序生成的日志文件。
- NFS，可使用 NFS 网络文件系统提供的共享目录存储数据。
- 其他云持久化盘等

#### **2.3.9 Persistent Volume**

在使用虚拟机的情况下，我们通常会先定义一个网络存储，然后从中 划出一个“网盘”并挂接到虚拟机上。Persistent Volume(PV) 和与之相关联的 Persistent Volume Claim(PVC) 也起到了类似的作用。PV 可以被理解成 Kubernetes 集群中的某个网络存储对应的一块存储，它与 Volume 类似，但有以下区别:

- PV 只能是网络存储，不属于任何 Node，但可以在每个 Node 上访问。
- PV 并不是被定义在 Pod 上的，而是独立于 Pod 之外定义的。

```
apiVersion: v1kind: PersistentVolumemetadata:  name: pv001  spec:    capacity:      storage: 5Gi    accessMods:      - ReadWriteOnce    nfs:      path: /somePath      server: xxx.xx.xx.x
```

> accessModes，有几种类型，1.ReadWriteOnce:读写权限，并且只能被单个 Node 挂载。2. ReadOnlyMany:只读权限，允许被多个 Node 挂载。3.ReadWriteMany:读写权限，允许被多个 Node 挂载。

如果 Pod 想申请某种类型的 PV，首先需要定义一个 PersistentVolumeClaim 对象，

```
apiVersion: v1kind: PersistentVolumeClaim  # 声明pvcmetadata:  name: pvc001  spec:    resources:      requests:        storage: 5Gi    accessMods:      - ReadWriteOnce
```

然后在 Pod 的 Volume 中引用 PVC 即可。

```
volumes:  - name: mypd   persistentVolumeClaim:     claimName: pvc001
```

PV 有以下几种状态：

- Available：空闲
- Bound：已绑定到 PVC
- Relead：对应 PVC 被删除，但 PV 还没被回收
- Faild：PV 自动回收失败

#### **2.3.10 Namespace**

Namespace 在很多情况下用于实现多租户的资源隔离。分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。Kubernetes 集群在启动后会创建一个名为 default 的 Namespace，通过 kubectl 可以查看:

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151454149230502.png)

#### **2.3.11 ConfigMap**

我们知道，Docker 通过将程序、依赖库、数据及 配置文件“打包固化”到一个不变的镜像文件中的做法，解决了应用的部署的难题，但这同时带来了棘手的问题，即配置文件中的参数在运行期如何修改的问题。我们不可能在启动 Docker 容器后再修改容器里的配置 文件，然后用新的配置文件重启容器里的用户主进程。为了解决这个问题，Docker 提供了两种方式:

- 在运行时通过容器的环境变量来传递参数;
- 通过 Docker Volume 将容器外的配置文件映射到容器内。

在大多数情况下，后一种方式更合 适我们的系统，因为大多数应用通常从一个或多个配置文件中读取参数。但这种方式也有明显的缺陷:我们必须在目标主机上先创建好对应 配置文件，然后才能映射到容器里。上述缺陷在分布式情况下变得更为严重，因为无论采用哪种方式， 写入(修改)多台服务器上的某个指定文件，并确保这些文件保持一致，都是一个很难完成的目标。针对上述问题， Kubernetes 给出了一个很巧妙的设计实现。

首先，把所有的配置项都当作 key-value 字符串，这些配置项可以 作为 Map 表中的一个项，整个 Map 的数据可以被持久化存储在 Kubernetes 的 Etcd 数据库中，然后提供 API 以方便 Kubernetes 相关组件或 客户应用 CRUD 操作这些数据，上述专门用来保存配置参数的 Map 就是 Kubernetes ConfigMap 资源对象。Kubernetes 提供了一种内建机制，将存储在 etcd 中的 ConfigMap 通过 Volume 映射的方式变成目标 Pod 内的配置文件，不管目标 Pod 被调度到哪台服务器上，都会完成自动映射。进一步地，如果 ConfigMap 中的 key-value 数据被修改，则映射到 Pod 中的“配置文件”也会随之自动更新。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151454277097919.png)

### 2.4 Kubernetes 的架构

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151454478600733.png)

Kubernetes 由 Master 节点、 Node 节点以及外部的 ETCD 集群组成，集群的状态、资源对象、网络等信息存储在 ETCD 中，Mater 节点管控整个集群，包括通信、调度等，Node 节点为工作真正执行的节点，并向主节点报告。Master 节点由以下组件构成：

#### **2.4.1 Master 组件：**

1. API Server —— 提供 HTTP Rest 接口，是所有资源增删改查和集群控制的唯一入口。（在集群中表现为名称是 kubernetes 的 service）。可以通过 Dashboard 的 UI 或 kubectl 工具来与其交互。（1）集群管理的 API 入口；
   （2）资源配额控制入口；
   （3）提供完备的集群安全机制。
2. Controller Manager —— 资源对象的控制自动化中心。即监控 Node，当故障时转移资源对象，自动修复集群到期望状态。
3. Scheduler —— 负责 Pod 的调度，调度到最优的 Node。

#### **2.4.2 Node 组件：**

1. kubelet —— 负责 Pod 内容器的创建、启停，并与 Master 密切协作实现集群管理（注册自己，汇报 Node 状态）。
2. kube-proxy —— 实现 k8s Service 的通信与负载均衡。
3. Docker Engine —— Docker 引擎，负责本机容器的创建和管理。

### 2.5 Kubernetes 架构模块实现原理

#### **2.5.1 API Server**

Kubernetes API Server 通过一个名为 kube-apiserver 的进程提供服务，该进程运行在 Master 上。在默认情况下，kube-apiserver 进程在本机的 8080 端口(对应参数–insecure-port)提供 REST 服务。我们可以同时启动 HTTPS 安全端口(–secure-port=6443)来启动安全机制，加强 REST API 访问的安全性。

由于 API Server 是 Kubernetes 集群数据的唯一访问入口，因此安全性与高性能就成为 API Server 设计和实现的两大核心目标。通过采用 HTTPS 安全传输通道与 CA 签名数字证书强制双向认证的方式，API Server 的安全性得以保障。此外，为了更细粒度地控制用户或应用对 Kubernetes 资源对象的访问权限，Kubernetes 启用了 RBAC 访问控制策略。Kubernetes 的设计者综合运用以下方式来最大程度地保证 API Server 的性 能。

1. API Server 拥有大量高性能的底层代码。在 API Server 源码中 使用协程(Coroutine)+队列(Queue)这种轻量级的高性能并发代码， 使得单进程的 API Server 具备了超强的多核处理能力，从而以很快的速 度并发处理大量的请求。
2. 普通 List 接口结合异步 Watch 接口，不但完美解决了 Kubernetes 中各种资源对象的高性能同步问题，也极大提升了 Kubernetes 集群实时响应各种事件的灵敏度。
3. 采用了高性能的 etcd 数据库而非传统的关系数据库，不仅解决 了数据的可靠性问题，也极大提升了 API Server 数据访问层的性能。在 常见的公有云环境中，一个 3 节点的 etcd 集群在轻负载环境中处理一个请 求的时间可以低于 1ms，在重负载环境中可以每秒处理超过 30000 个请求。

#### **2.5.2 安全认证**

**RBAC**

Role-Based Access Control(RBAC)，基于角色的访问控制。

**4 种资源对象**

1. Role
2. RoleBinding
3. ClusterRole
4. ClusterRoleBinding

**Role 与 ClusterRole**

一个角色就是一组权限的集合，都是以许可形式，不存在拒绝的规则。Role 作用于一个命名空间中，ClusterRole 作用于整个集群。

```
apiVersion:rbac.authorization.k8s.io/v1beta1kind:Rolemetadata:  namespace: default #ClusterRole可以省略，毕竟是作用于整个集群  name: pod-readerrules:- apiGroups: [""]  resources: ["pod"]  verbs: ["get","watch","list"]
```

RoleBinding 和 ClusterRoleBinding 是把 Role 和 ClusterRole 的权限绑定到 ServiceAccount 上。

```
kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:    namespace: default    name: app-adminsubjects:-   kind: ServiceAccount    name: app    apiGroup: ""    namespace: defaultroleRef:    kind: ClusterRole    name: cluster-admin    apiGroup: rbac.authorization.k8s.io
```

**ServiceAccount**

Service Account 也是一种账号，但它并不是给 Kubernetes 集群的用户 (系统管理员、运维人员、租户用户等)用的，而是给运行在 Pod 里的进程用的，它为 Pod 里的进程提供了必要的身份证明。在每个 Namespace 下都有一个名为 default 的默认 Service Account 对象，在这个 Service Account 里面有一个名为 Tokens 的可以当作 Volume 被挂载到 Pod 里的 Secret，当 Pod 启动时，这个 Secret 会自动被挂载到 Pod 的指定目录下，用来协助完成 Pod 中的进程访问 API Server 时的身份鉴权。

#### **2.5.3 Controller Manager**

下边介绍几种 Controller Manager 的实现组件

**ResourceQuota Controller**

kubernetes 的配额管理使用过 Admission Control 来控制的，提供了两种约束，LimitRanger 和 ResourceQuota。LimitRanger 作用于 Pod 和 Container 之上(limit ,request)，ResourceQuota 则作用于 Namespace。资源配额，分三个层次：

> 1. **容器级别**，对容器的 CPU、memory 做限制
> 2. **Pod 级别**，对一个 Pod 内所有容器的可用资源做限制
> 3. **Namespace 级别**，为 namespace 做限制，包括：

```
    + pod数量    + RC数量    + Service数量    + ResourceQuota数量    + Secrete数量    + PV数量
```

**Namespace Controller**

管理 Namesoace 创建删除.

**Endpoints Controller**

Endpoints 表示一个 service 对应的所有 Pod 副本的访问地址，而 Endpoints Controller 就是负责生成和维护所有 Endpoints 对象的控制器。

> - 负责监听 Service 和对应 Pod 副本的变化，若 Service 被创建、更新、删除，则相应创建、更新、删除与 Service 同名的 Endpoints 对象。
> - EndPoints 对象被 Node 上的 kube-proxy 使用。

#### **2.5.4 Scheduler**

Kubernetes Scheduler 的作用是将待调度的 Pod(API 新创 建的 Pod、Controller Manager 为补足副本而创建的 Pod 等)按照特定的调 度算法和调度策略绑定(Binding)到集群中某个合适的 Node 上，并将绑定信息写入 etcd 中。Kubernetes Scheduler 当前提供的默认调度流程分为以下两步。

1. 预选调度过程，即遍历所有目标 Node，筛选出符合要求的候 选节点。为此，Kubernetes 内置了多种预选策略(xxx Predicates)供用户选择。
2. 确定最优节点，在第 1 步的基础上，采用优选策略(xxx Priority)计算出每个候选节点的积分，积分最高者胜出。

#### **2.5.5 网络**

Kubernetes 的网络利用了 Docker 的网络原理，并在此基础上实现了跨 Node 容器间的网络通信。

1. 同一个 Node 下 Pod 间通信模型：
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151455076877270.png)
2. 不同 Node 下 Pod 间的通信模型（CNI 模型实现）：
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151455201189114.png)

CNI 提供了一种应用容器的插件化网络解决方案，定义对容器网络 进行操作和配置的规范，通过插件的形式对 CNI 接口进行实现，以 Flannel 举例，完成了 Node 间容器的通信模型。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151455294724795.png)

可以看到，Flannel 首先创建了一个名为 flannel0 的网桥，而且这个 网桥的一端连接 docker0 网桥，另一端连接一个叫作 flanneld 的服务进程。flanneld 进程并不简单，它上连 etcd，利用 etcd 来管理可分配的 IP 地 址段资源，同时监控 etcd 中每个 Pod 的实际地址，并在内存中建立了一 个 Pod 节点路由表;它下连 docker0 和物理网络，使用内存中的 Pod 节点 路由表，将 docker0 发给它的数据包包装起来，利用物理网络的连接将 数据包投递到目标 flanneld 上，从而完成 Pod 到 Pod 之间的直接地址通信。

#### **2.5.6 服务发现**

从 Kubernetes 1.11 版本开始，Kubernetes 集群的 DNS 服务由 CoreDNS 提供。CoreDNS 是 CNCF 基金会的一个项目，是用 Go 语言实现的高性能、插件式、易扩展的 DNS 服务端。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151455389563234.png)

## **3.结语**

文章包含的内容说多不多，说少不少，但对于 Docker、Kubernetes 知识原理的小白来说是足够的，笔者按照自己的学习经验，以介绍为出发点，让大家更能了解相关技术原理，所以实操的部分较少。Kubernetes 技术组件还是十分丰富的，文章有选择性地进行了介绍，感兴趣的读者可以再自行从官方或者书籍中学习了解。（附《Kubernetes 权威指南——从 Docker 到 Kubernetes 实践全接触》第四版）

## 4.**相关链接**

- [Docker 核心技术与实现原理](https://draveness.me/docker/)
- [官方镜像仓库地址](https://hub.docker.com/)
- [Kubernetes 官网](https://kubernetes.io/)
- [Kubernetes 中文社区](https://www.kubernetes.org.cn/)

原文作者：honghaohu，腾讯 PCG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/ji0Pj00xeHOeispNhsPKZw



# 【NO.87】浅谈 Protobuf 编码

> 近日简单学习了 Protobuf 中的编码实现，总结并整理成文。本文结构总体与 Protobuf 官方文档相似，不少内容也来自官方文档，并在官方文档的基础上添加作者理解的内容，如有出入请以官方文档为准。作者水平有限，难免有疏漏之处，欢迎指正并分享您的意见。

## 1.**0x00 Before you start**

简单来说，Protobuf 的编码是基于变种的 Base128。在学习 Protobuf 编码或是 Base128 之前，先来了解下 Base64 编码。

## **2.0x01 Base 64**

当我们在计算机之间传输数据时，数据本质上是一串字节流。TCP 协议可以保证被发送的字节流正确地达到目的地（至少在出错时有一定的纠错机制），所以本文不讨论因网络因素造成的数据损坏。但数据到达目标机器之后，由于不同机器采用的字符集不同等原因，我们并不能保证目标机器能够正确地“理解”字节流。

Base 64 最初被设计是用于在邮件中嵌入文件（作为 MIME 的一部分）。它可以将任何形式的字节流编码为“安全”的字节流。何为“安全“的字节？先来看看 Base 64 是如何工作的。

假设这里有四个字节,代表你要传输的二进制数据。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151501523865652.png)

首先将这个字节流按每 6 个 bit 为一组进行分组,剩下少于 6 bits 的低位补 0。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502001648745.png)

然后在每一组 6 bits 的高位补两个 0。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502115959855.png)

对照 Base 64 table，字节流可以用 `ognC0w` 来表示。另外，Base64 编码是按照 6 bits 为一组进行编码，每 3 个字节的原始数据要用 4 个字节来储存，编码后的长度要为 4 的整数倍，不足 4 字节的部分要使用 pad 补齐，所以最终的编码结果为`ognC0w==`。

任意的字节流均可以使用 Base 64 进行编码，编码之后所有字节均可以用数字、字母和 + / = 号进行表示，这些都是可以被正常显示的 ascii 字符，即“安全”的字节。绝大部分的计算机和操作系统都对 ascii 有着良好的支持，保证了编码之后的字节流能被正确地复制、传播、解析。

注：下文关于字节顺序内容均基于机器采用小端模式的前提进行讨论。

## 3.**0x02 Base 128?**

Base 64 存在的问题就是，编码后的每一个字节的最高两位总是 0，在不考虑 pad 的情况下，有效 bit 只占 bit 总数的 75%，造成大量的空间浪费。是否可以进一步提高信息密度呢？

意识到这一点，你就很自然能想象出 Base 128 的大致实现思路了，将字节流按 7 bits 进行分组，然后低位补 0。

但问题来了，Base 64 实际上用了 64+1 个 ascii 字符，按照这个思路 Base 128 需要使用 128+1 个 ascii 个字符，但是 ascii 字符一共只有 128 个。另外，即使不考虑 pad，ascii 中包含了一些不可以正常打印的控制字符，编码之后的字符还可能包含会被不同操作系统转换的换行符号（10 和 13）。因此，Base 64 至今依然没有被 Base 128 替代。

Base 64 的规则因为上述限制不能完美地扩展到 Base 128，所以现有基于 Base 64 扩展而来的编码方式大部分都属于变种。如 LEB128（Little-Endian Base 128）， Base 85 （Ascii 85），以及本文的主角：Base 128 Varints。

注：下文关于字节顺序内容均基于机器采用小端模式的前提进行讨论。

## **4.0x03 Base 128 Varints**

Base 128 Varints 是 Google 开发的序列化库 Protocol Buffers 所用的编码方式。以下为 Protobuf 官方文档中对于 Varints 的解释：

> Varints are a method of serializing integers using one or more bytes. Smaller numbers take a smaller number of bytes.

使用一个或多个字节对整数进行序列化。小的数字占用更少的字节。简单来说，就是尽量只储存整数的有效位，高位的 0 尽可能抛弃。

有两个需要注意的细节：

- Base 128 Varints 只能对一部分数据结构进行编码，不适用于所有字节流（当然你可以把任意字节流转换为 string，但不是所有语言都支持这个 trick）。否则无法识别哪部分是无效的 bits。
- Base 128 Varints 编码后的字节可以不存在于 Ascii 表中，因为和 Base 64 使用场景不同，不用考虑是否能正常打印。

下面以例子进行说明 Base 128 Varints 的编码实现。

对于编码后的每个字节，低 7 位用于储存数据，最高位用来标识当前字节是否是当前整数的最后一个字节，称为最高有效位（most significant bit, msb）。msb 为 1 时，代表着后面还有数据；msb 为 0 时代表着当前字节是当前整数的最后一个字节。

举个例子，下面是编码后的整数`1`。1 只需要用一个字节就能表示完全，所以 msb 为 0。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502239941675.png)

对于需要多个字节来储存的数据，如 300 (0b100101100)，有效位数为 9，编码后需要两个字节储存。下面是编码后的整数`300`。第一个字节的 msb 为 1，最后一个字节的 msb 为 0。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502356867932.png)

要将这两个字节解码成整数,需要三个步骤：

- 去除 msb
- 第二步，将字节流逆序（msb 为 0 的字节储存原始数据的高位部分，小端模式）
- 最后拼接所有的 bits。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502469788623.png)

下面一个例子展示如何将使用 Base 128 Varints 对整数进行编码。

- 将数据按每 7 bits 一组拆分
- 逆序每一个组
- 添加 msb

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502589907756.png)

需要注意的是，无论是编码还是解码，逆序字节流这一步在机器处理中实际是不存在的，机器采用小端模式处理数据，此处逆序仅是为了符合人的阅读习惯而写出。下面展示 Go 版本的 protobuf 中关于 Base 128 Varints 的实现：

```
// google.golang.org/protobuf@v1.25.0/encoding/protowire/wire.go// AppendVarint appends v to b as a varint-encoded uint64.func AppendVarint(b []byte, v uint64) []byte { switch { case v < 1<<7:  b = append(b, byte(v)) case v < 1<<14:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte(v>>7)) case v < 1<<21:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte(v>>14)) case v < 1<<28:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte(v>>21)) case v < 1<<35:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte(v>>28)) case v < 1<<42:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte((v>>28)&0x7f|0x80),   byte(v>>35)) case v < 1<<49:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte((v>>28)&0x7f|0x80),   byte((v>>35)&0x7f|0x80),   byte(v>>42)) case v < 1<<56:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte((v>>28)&0x7f|0x80),   byte((v>>35)&0x7f|0x80),   byte((v>>42)&0x7f|0x80),   byte(v>>49)) case v < 1<<63:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte((v>>28)&0x7f|0x80),   byte((v>>35)&0x7f|0x80),   byte((v>>42)&0x7f|0x80),   byte((v>>49)&0x7f|0x80),   byte(v>>56)) default:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte((v>>28)&0x7f|0x80),   byte((v>>35)&0x7f|0x80),   byte((v>>42)&0x7f|0x80),   byte((v>>49)&0x7f|0x80),   byte((v>>56)&0x7f|0x80),   1) } return b}
```

从源码中可以看出，protobuf 的 varints 最多可以编码 8 字节的数据，这是因为绝大部分的现代计算机最高支持处理 64 位的整型。

## **5.0x04 其他类型**

Protobuf 不仅支持整数类型，下图列出 protobuf 支持的数据类型（wire type）。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151503114423539.png)

在上一小节中展示的编码与解码的例子中的“整数”并不是我们一般理解的整数（编程语言中的 int32，uint32 等），而是对应着上图中的 Varint。

当实际使用编程语言使用 protobuf 进行编码时经过了两步处理：

- 将编程语言的数据结构转化为 wire type。
- 根据不同的 wire type 使用对应的方法编码。前文所提到的 Base 128 Varints 用来编码 varint 类型的数据，其他 wire type 则使用其他编码方式。

```
    {obj}  -> {wire type} -> {encoded byte stream}    uint32 -> wire type 0 -> varint    int32  -> wire type 0 -> varint    bool   -> wire type 0 -> varint    string -> wire type 2 -> length-delimited    ...
```

不同语言中 wire type 实际上也可能采用了语言中的某种类型来储存 wire type 的数据。例如，Go 中使用了 uint64 来储存 wire type 0。一般来说，大多数语言中的无符号整型被转换为 varints 之后，有效位上的内容并没有改变。

下面说明部分其他数据类型到 wire type 的转换规则：

- ### **有符号整型**

采用 ZigZag 编码来将 sint32 和 sint64 转换为 wire type 0。下面是 ZigZag 编码的规则（注意是算术位移）：

```
    (n << 1) ^ (n >> 31)  // for 32-bit signed integer    (n << 1) ^ (n >> 63)  // for 64-bit signed integer
```

或者从数学意义来理解：

```
    n * 2       // when n >= 0    -n * 2 - 1  // when n < 0
```

下图展示了部分 ZigZag 编码的例子：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151503251275844.png)

如果不先采用 ZigZag 编码成 wire type，负值 sint64 直接使用 Base 128 Varints 编码之后的长度始终为`ceil(64/7)=10bytes`，浪费大量空间。使用 ZigZag 编码后，绝对值较小的负数的长度能够被显著压缩。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151503364873212.png)
对于 -234(sint32) 这个例子，编码成 varints 之前采用 ZigZag 编码，比直接编码成 varints 少用了 60%的空间。当然，ZigZag 编码也不是完美的方法。当你尝试把 sint32 或 sint64 范围内所有的整数都编码成 varints 字节流,使用 ZigZag 已经不能压缩字节数量了。ZigZag 虽然能压缩部分负数的空间，但同时正数变得需要更多的空间来储存。因此，建议在业务场景允许的场景下尽量用无符号整型，有助于进一步压缩编码后的空间。

- ### **定长数据（64-bit）**

直接采用小端模式储存，不作转换。

- ### **字符串**

以字符串`"testing"`为例

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151503531303965.png)
编码后的 value 分为两部分：

- 蓝色，表示字符串采用 UTF-8 编码后字节流的长度（bytes），采用 Base 128 Varints 进行编码。
- 白色，字符串用 UTF-8 编码后的字节流。

## **6.0x05 消息结构**

Protobuf 采用 proto3 作为 DSL 来描述其支持的消息结构。

```
syntax = "proto3";message SearchRequest {  string query = 1;  int32 page_number = 2;  int32 result_per_page = 3;}
```

设想一下这样一个场景：数据的发送方在业务迭代之后需要在消息内携带更多的字段，而有的接收方并没有更新自己的 proto 文件。要保持较好的兼容性，接收方分辨出哪些字段是自己可以识别的，哪些是不能识别的新增字段。要做到这一点，发送方在编码消息时还必须附带每个字段的 key，客户端读取到未知的 key 时，可以直接跳过对应的 value。

proto3 中每一个字段后面都有一个 `= x`，比如：

```
  string query = 1;
```

这里的等号并不是用于赋值，而是给每一个字段指定一个 ID，称为 field number。消息内同一层次字段的 field number 必须各不相同。

上面所说的 key，在 protobuf 源码中被称为 tag。tag 由 field number 和 type 两部分组成：

- field number 左移 3 bits
- 在最低 3 bits 写入 wire type

下面展示一个生成 tag 例子：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151504038447723.png)

Go 版本 Protobuf 中生成 tag 的源码：

```
// google.golang.org/protobuf@v1.25.0/encoding/protowire/wire.go// EncodeTag encodes the field Number and wire Type into its unified form.func EncodeTag(num Number, typ Type) uint64 {    return uint64(num)<<3 | uint64(typ&7)}
```

源码中生成的 tag 是 uint64，代表着 field number 可以使用 61 个 bit 吗？并非如此。事实上，tag 的长度不能超过 32 bits，意味着 field number 的最大取值为 2^29-1 (536870911)。而且在这个范围内，有一些数是不能被使用的：

- 0 ，protobuf 规定 field number 必须为正整数。
- 19000 到 19999， protobuf 仅供内部使用的保留位。

理解了生成 tag 的规则之后，不难得出以下结论：

- field number 不必从 1 开始，可以从合法范围内的任意数字开始。
- 不同字段间的 field number 不必连续，只要合法且不同即可。

但是实际上，大多数人分配 field number 还是会从 1 开始，因为 tag 最终要经过 Base 128 Varints 编码，较小的 field number 有助于压缩空间，field number 为 1 到 15 的 tag 最终仅需占用一个字节。当你的 message 有超过 15 个字段时，Google 也不建议你将 1 到 15 立马用完。如果你的业务日后有新增字段的可能，并且新增的字段使用比较频繁，你应该在 1 到 15 内预留一部分供新增的字段使用。

当你修改的 proto 文件需要注意：

- field number 一旦被分配了就不应该被更改，除非你能保证所有的接收方都能更新到最新的 proto 文件。
- 由于 tag 中不携带 field name 信息，更改 field name 并不会改变消息的结构。发送方认为的 apple 到接受方可能会被识别成 pear。双方把字段读取成哪个名字完全由双方自己的 proto 文件决定，只要字段的 wire type 和 field number 相同即可。

由于 tag 中携带的类型是 wire type，不是语言中具体的某个数据结构，而同一个 wire type 可以被解码成多种数据结构，具体解码成哪一种是根据接收方自己的 proto 文件定义的。修改 proto 文件中的类型，有可能导致错误。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151504159043275.png)

最后用一个比前面复杂一点的例子来结束本节内容：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151504234942704.png)

## **7.0x06 嵌套消息**

嵌套消息的实现并不复杂。在上一节展示的 protobuf 的 wire type 中，wire type2 （length-delimited）不仅支持 string，也支持 embedded messages。

对于嵌套消息，首先你要将被嵌套的消息进行编码成字节流，然后你就可以像处理 UTF-8 编码的字符串一样处理这些字节流：在字节流前面加入使用 Base 128 Varints 编码的长度即可。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151504322045900.png)

## **8.0x07 重复消息**

假设接收方的 proto3 中定义了某个字段（假设 field number=1），当接收方从字节流中读取到多个 field number=1 的字段时，会执行 merge 操作。merge 的规则如下：

- 如果字段为不可分割的类型，则直接覆盖
- 如果字段为 repeated，则 append 到已有字段
- 如果字段为嵌套消息，则递归执行 merge

如果字段的 field number 相同但是结构不同，则出现 error。以下为 Go 版本 Protobuf 中 merge 的部分源码：

```
// google.golang.org/protobuf@v1.25.0/proto/merge.go// Merge merges src into dst, which must be a message with the same descriptor.//// Populated scalar fields in src are copied to dst, while populated// singular messages in src are merged into dst by recursively calling Merge.// The elements of every list field in src is appended to the corresponded// list fields in dst. The entries of every map field in src is copied into// the corresponding map field in dst, possibly replacing existing entries.// The unknown fields of src are appended to the unknown fields of dst.//// It is semantically equivalent to unmarshaling the encoded form of src// into dst with the UnmarshalOptions.Merge option specified.func Merge(dst, src Message) { // TODO: Should nil src be treated as semantically equivalent to a // untyped, read-only, empty message? What about a nil dst? dstMsg, srcMsg := dst.ProtoReflect(), src.ProtoReflect() if dstMsg.Descriptor() != srcMsg.Descriptor() {  if got, want := dstMsg.Descriptor().FullName(), srcMsg.Descriptor().FullName(); got != want {   panic(fmt.Sprintf("descriptor mismatch: %v != %v", got, want))  }  panic("descriptor mismatch") } mergeOptions{}.mergeMessage(dstMsg, srcMsg)}func (o mergeOptions) mergeMessage(dst, src protoreflect.Message) { methods := protoMethods(dst) if methods != nil && methods.Merge != nil {  in := protoiface.MergeInput{   Destination: dst,   Source:      src,  }  out := methods.Merge(in)  if out.Flags&protoiface.MergeComplete != 0 {   return  } } if !dst.IsValid() {  panic(fmt.Sprintf("cannot merge into invalid %v message", dst.Descriptor().FullName())) } src.Range(func(fd protoreflect.FieldDescriptor, v protoreflect.Value) bool {  switch {  case fd.IsList():   o.mergeList(dst.Mutable(fd).List(), v.List(), fd)  case fd.IsMap():   o.mergeMap(dst.Mutable(fd).Map(), v.Map(), fd.MapValue())  case fd.Message() != nil:   o.mergeMessage(dst.Mutable(fd).Message(), v.Message())  case fd.Kind() == protoreflect.BytesKind:   dst.Set(fd, o.cloneBytes(v))  default:   dst.Set(fd, v)  }  return true }) if len(src.GetUnknown()) > 0 {  dst.SetUnknown(append(dst.GetUnknown(), src.GetUnknown()...)) }}
```

## **9.0x08 字段顺序**

Proto 文件中定义字段的顺序与最终编码结果的字段顺序无关，两者有可能相同也可能不同。当消息被编码时，Protobuf 无法保证消息的顺序，消息的顺序可能随着版本或者不同的实现而变化。任何 Protobuf 的实现都应该保证字段以任意顺序编码的结果都能被读取。

- 序列化后的消息字段顺序是不稳定的。
- 对同一段字节流进行解码，不同实现或版本的 Protobuf 解码得到的结果不一定完全相同（bytes 层面）。只能保证相同版本相同实现的 Protobuf 对同一段字节流多次解码得到的结果相同。
- 假设有一条消息`foo`，以下关系可能不成立：

```
foo.SerializeAsString() == foo.SerializeAsString()Hash(foo.SerializeAsString()) == Hash(foo.SerializeAsString())CRC(foo.SerializeAsString()) == CRC(foo.SerializeAsString())FingerPrint(foo.SerializeAsString()) == FingerPrint(foo.SerializeAsString())
```

- 假设有两条逻辑上相等消息，但是序列化之后的内容（bytes 层面）不相同，部分可能的原因有：
- - 其中一条消息可能使用了较老版本的 protobuf，不能处理某些类型的字段，设为 unknwon。+ 使用了不同语言实现的 Protobuf，并且以不同的顺序编码字段。+ 消息中的字段使用了不稳定的算法进行序列化。+ 某条消息中有 bytes 类型的字段，用于储存另一条消息使用 Protobuf 序列化的结果，而这个 bytes 使用了不同的 Protobuf 进行序列化。+ 使用了新版本的 Protobuf，序列化实现不同。+ 消息字段顺序不同。

**References**

https://datatracker.ietf.org/doc/html/rfc4648
https://developers.google.com/protocol-buffers/docs/encodinghttps://developers.google.com/protocol-buffers/docs/proto3https://stackoverflow.com/questions/3538021/why-do-we-use-base64

原文作者：SG4YK，腾讯 PCG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/enDUynhZ1Pnzg_4xEjR21A

# 【NO.88】gRPC 基础概念详解

**gRPC** (gRPC Remote Procedure Calls) 是 Google 发起的一个开源远程过程调用系统，该系统基于 HTTP/2 协议传输，本文介绍 gRPC 的基础概念，首先通过关系图直观展示这些基础概念之间关联，介绍异步 gRPC 的 Server 和 Client 的逻辑；然后介绍 RPC 的类型，阅读和抓包分析 gRPC 的通信过程协议，gRPC 上下文；最后分析 `grpc.pb.h` 文件的内容，包括 Stub 的能力、Service 的种类以及与核心库的关系。

之所以谓之基础，是这些内容基本不涉及 gRPC Core 的内容。

## **1.基本概念概览**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151513016353805.png)

上图中列出了 gRPC 基础概念及其关系图。其中包括：**Service(定义)、RPC、API、Client、Stub、Channel、Server、Service(实现)、ServiceBuilder** 等。

接下来，以官方提供的 `example/helloworld` 为例进行说明。

`.proto` 文件定义了**服务** `Greeter` 和 **API** `SayHello`：

```
// helloworld.proto// The greeting service definition.service Greeter {  // Sends a greeting  rpc SayHello (HelloRequest) returns (HelloReply) {}}
```

`class GreeterClient` 是 **Client**，是对 **Stub** 封装；通过 **Stub** 可以真正的调用 RPC 请求。

```
class GreeterClient { public:  GreeterClient(std::shared_ptr<Channel> channel)      : stub_(Greeter::NewStub(channel)) {}  std::string SayHello(const std::string& user) {...private:  std::unique_ptr<Greeter::Stub> stub_;};
```

**Channel** 提供一个与特定 gRPC server 的主机和端口建立的连接。

**Stub** 就是在 **Channel** 的基础上创建而成的。

```
target_str = "localhost:50051";auto channel =    grpc::CreateChannel(target_str, grpc::InsecureChannelCredentials());GreeterClient greeter(channel);std::string user("world");std::string reply = greeter.SayHello(user);
```

Server 端需要实现对应的 RPC，所有的 RPC 组成了 **Service**：

```
class GreeterServiceImpl final : public Greeter::Service {  Status SayHello(ServerContext* context, const HelloRequest* request,                  HelloReply* reply) override {    std::string prefix("Hello ");    reply->set_message(prefix + request->name());    return Status::OK;  }};
```

**Server** 的创建需要一个 **Builder**，添加上监听的地址和端口，**注册**上该端口上绑定的服务，最后构建出 Server 并启动：

```
ServerBuilder builder;builder.AddListeningPort(server_address, grpc::InsecureServerCredentials());builder.RegisterService(&service);std::unique_ptr<Server> server(builder.BuildAndStart());
```

**RPC 和 API 的区别**：RPC (Remote Procedure Call) 是一次远程过程调用的整个动作，而 API (Application Programming Interface) 是不同语言在实现 RPC 中的具体接口。一个 RPC 可能对应多种 API，比如同步的、异步的、回调的。一次 RPC 是对某个 API 的一次调用，比如：

```
std::unique_ptr<ClientAsyncResponseReader<HelloReply> > rpc(    stub_->PrepareAsyncSayHello(&context, request, &cq));
```

不管是哪种类型 RPC，都是由 Client 发起请求。

## **2.异步相关概念**

不管是 Client 还是 Server，异步 gRPC 都是利用 [CompletionQueue](https://grpc.io/grpc/cpp/classgrpc_1_1_completion_queue.html) API 进行异步操作。基本的流程：

- 绑定一个 `CompletionQueue` 到一个 RPC 调用
- 利用唯一的 `void*` Tag 进行读写
- 调用 `CompletionQueue::Next()` 等待操作完成，完成后通过唯一的 Tag 来判断对应什么请求/返回进行后续操作

官方文档 [Asynchronous-API tutorial](https://grpc.io/docs/languages/cpp/async/) 中有上边的介绍，并介绍了异步 client 和 server 的解释，对应这 `greeter_async_client.cc` 和 `greeter_async_server.cc` 两个文件。

Client 看文档可以理解，但 Server 的代码复杂，文档和注释中的解释并不是很好理解，接下来会多做一些解释。

### 2.1 异步 Client

`greeter_async_client.cc` 中是异步 Client 的 Demo，其中只有一次请求，逻辑简单。

- 创建 CompletionQueue
- 创建 RPC (既 `ClientAsyncResponseReader<HelloReply>`)，这里有两种方式：
- - `stub_->PrepareAsyncSayHello()` + `rpc->StartCall()`
  - `stub_->AsyncSayHello()`
- 调用 `rpc->Finish()` 设置请求消息 reply 和唯一的 tag 关联，将请求发送出去
- 使用 `cq.Next()` 等待 Completion Queue 返回响应消息体，通过 tag 关联对应的请求

[TODO] **ClientAsyncResponseReader 在 `Finish()` 完之后就没有用了？**

### 2.2 异步 Server

`RequestSayHello()` 这个函数没有任何的说明。只说是：”we *request* that the system start processing SayHello requests.” 也没有说跟 `cq_->Next(&tag, &ok);` 的关系。我这里通过加上一些日志打印，来更清晰的展示 Server 的逻辑：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151513209083141.png)

上边绿色的部分为创建的第一个 CallData 对象地址，橙色的为第二个 CallData 的地址。

- 创建一个 CallData，初始构造列表中将状态设置为 **CREATE**
- 构造函数中，调用 Process()成员函数，调用 `service_->RequestSayHello()`后，状态变更为 **PROCESS**：
- - 传入 `ServerContext ctx_`
  - 传入 `HelloRequest request_`
  - 传入 `ServerAsyncResponseWriter<HelloReply> responder_`
  - 传入 `ServerCompletionQueue* cq_`
  - 将对象自身的地址作为 `tag` 传入
  - 该动作，能将**事件加入事件循环，可以在 CompletionQueue 中等待**
- 收到请求，`cq->Next()`的阻塞结束并返回，**得到 tag**，既上次传入的 CallData 对象地址
- 调用 tag 对应 CallData 对象的 `Proceed()`，此时状态为 **Process**
- - 创建新的 CallData 对象以接收新请求
  - 处理消息体并设置 `reply_`
  - 将状态设置为 **FINISH**
  - 调用 `responder_.Finish()` 将返回发送给客户端
  - 该动作，能将**事件加入到事件循环，可以在 CompletionQueue 中等待**
- 发送完毕，`cq->Next()`的阻塞结束并返回，**得到 tag**。现实中，如果发送有异常应当有其他相关的处理
- 调用 tag 对应 CallData 对象的 `Proceed()`，此时状态为 **FINISH**，`delete this` 清理自己，一条消息处理完成

### 2.3 关系图

将上边的异步 Client 和异步 Server 的逻辑通过关系图进行展示。右侧 RPC 为创建的对象中的内存容，左侧使用相同颜色的小块进行代替。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151513336865444.png)

以下 CallData 并非 gRPC 中的概念，而是异步 Server 在实现过程中为了方便进行的封装，其中的 Status 也是在异步调用过程中自定义的、用于转移的状态。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151513437399143.png)

### 2.4 异步 Client 2

在 `example/cpp/helloworld` 中还有另外一个异步 Client，对应文件名为 `greeter_async_client2.cc`。这个例子中使用了**两个线程去分别进行发送请求和处理返回**，一个线程批量发出 100 个 SayHello 的请求，另外一个不断的通过 `cq_.Next()` 来等待返回。

无论是 Client 还是 Server，**在以异步方式进行处理时，都要预先分配好一定的内存/对象，以存储异步的请求或返回。**

### 2.5 在 `example/cpp/helloworld` 中，还提供了 callback 相关的 Client 和 Server。

使用回调方式简介明了，结构上与同步方式相差不多，但是并发有本质的区别。可以通过文件对比，来查看其中的差异。

```
cd examples/cpp/helloworld/vimdiff greeter_callback_client.cc greeter_client.ccvimdiff greeter_callback_server.cc greeter_server.cc
```

其实，回调方式的异步调用属于实验性质的，不建议直接在生产环境使用，这里也只做简单的介绍：

> Notice: This API is EXPERIMENTAL and may be changed or removed at any time.

#### **2.5.1 回调 Client**

发送单个请求，在调用 `SayHello` 时，除了传入 Request、 Reply 的地址之外，还需要传入一个接收 Status 的回调函数。

例子中只有一个请求，因此在 `SayHello` 之后，就直接通过 `condition_variable` 的 wait 函数等待回调结束，然后进行后续处理。这样其实不能进行并发，跟同步请求差别不大。如果要进行大规模的并发，还是需要使用额外的对象进行封装一下。

```
stub_->async()->SayHello(&context, &request, &reply,                         [&mu, &cv, &done, &status](Status s) {                           status = std::move(s);                           std::lock_guard<std::mutex> lock(mu);                           done = true;                           cv.notify_one();                         });
```

上边函数调用函数声明如下，很明显这是实验性（experimental）的接口：

```
void Greeter::Stub::experimental_async::SayHello(    ::grpc::ClientContext* context, const ::helloworld::HelloRequest* request,    ::helloworld::HelloReply* response, std::function<void(::grpc::Status)> f);
```

#### **2.5.2 回调 Server**

与同步 Server 不同的是：

- 服务的实现是继承 `Greeter::CallbackService`
- `SayHello` 返回的不是状态，而是 `ServerUnaryReactor` 指针
- 通过 `CallbackServerContext` 获得 `reactor`
- 调用 `reactor` 的 `Finish` 函数处理返回状态

## **3.流相关概念**

可以按照 Client 和 Server 一次发送/返回的是单个消息还是多个消息，将 gRPC 分为：

- Unary RPC
- Server streaming RPC
- Client streaming RPC
- Bidirectional streaming RPC

### 3.1 Server 对 RPC 的实现

Server 需要实现 proto 中定义的 RPC，每种 RPC 的实现都需要将 ServerContext 作为参数输入。

如果是一元 (Unary) RPC 调用，则像调用普通函数一样。将 Request 和 Reply 的对象地址作为参数传入，函数中将根据 Request 的内容，在 Reply 的地址上写上对应的返回内容。

```
// rpc GetFeature(Point) returns (Feature) {}Status GetFeature(ServerContext* context, const Point* point, Feature* feature);
```

如果涉及到流，则会用 Reader 或/和 Writer 作为参数，读取流内容。如 ServerStream 模式下，只有 Server 端产生流，这时对应的 Server 返回内容，需要使用作为参数传入的 `ServerWriter`。这类似于以 `'w'` 打开一个文件，持续的往里写内容，直到没有内容可写关闭。

```
// rpc ListFeatures(Rectangle) returns (stream Feature) {}Status ListFeatures(ServerContext* context,                    const routeguide::Rectangle* rectangle,                    ServerWriter<Feature>* writer);
```

另一方面，Client 来的流，Server 需要使用一个 ServerReader 来接收。这类似于打开一个文件，读其中的内容，直到读到 `EOF` 为止类似。

```
// rpc RecordRoute(stream Point) returns (RouteSummary) {}Status RecordRoute(ServerContext* context, ServerReader<Point>* reader,                   RouteSummary* summary);
```

如果 Client 和 Server 都使用流，也就是 Bidirectional-Stream 模式下，输入参数除了 ServerContext 之外，只有一个 `ServerReaderWriter` 指针。通过该指针，既能读 Client 来的流，又能写 Server 产生的流。

例子中，Server 不断地从 stream 中读，读到了就将对应的写过写到 stream 中，直到客户端告知结束；Server 处理完所有数据之后，直接返回状态码即可。

```
// rpc RouteChat(stream RouteNote) returns (stream RouteNote) {}Status RouteChat(ServerContext* context,                 ServerReaderWriter<RouteNote, RouteNote>* stream);
```

### 3.2 Client 对 RPC 的调用

Client 在调用一元 (Unary) RPC 时，像调用普通函数一样，除了传入 ClientContext 之外，将 Request 和 Response 的地址，返回的是 RPC 状态：

```
// rpc GetFeature(Point) returns (Feature) {}Status GetFeature(ClientContext* context, const Point& request,                  Feature* response);
```

Client 在调用 ServerStream RPC 时，不会得到状态，而是返回一个 ClientReader 的指针：

```
// rpc ListFeatures(Rectangle) returns (stream Feature) {}unique_ptr<ClientReader<Feature>> ListFeatures(ClientContext* context,                                               const Rectangle& request);
```

Reader 通过不断的 `Read()`，来不断的读取流，结束时 `Read()` 会返回 `false`；通过调用 `Finish()` 来读取返回状态。

调用 ClientStream RPC 时，则会返回一个 ClientWriter 指针：

```
// rpc RecordRoute(stream Point) returns (RouteSummary) {}unique_ptr<ClientWriter<Point>> RecordRoute(ClientContext* context,                                            Route Summary* response);
```

Writer 会不断的调用 `Write()` 函数将流中的消息发出；发送完成后调用 `WriteDone()` 来说明发送完毕；调用 `Finish()` 来等待对端发送状态。

而双向流的 RPC 时，会返回 ClientReaderWriter，：

```
// rpc RouteChat(stream RouteNote) returns (stream RouteNote) {}unique_ptr<ClientReaderWriter<RouteNote, RouteNote>> RouteChat(    ClientContext* context);
```

前面说明了 Reader 和 Writer 读取和发送完成的函数调用。因为 RPC 都是 Client 请求而后 Server 响应，双向流也是要 Client 先发送完自己流，才有 Server 才可能结束 RPC。所以对于双向流的结束过程是：

- `stream->WriteDone()`
- `stream->Finish()`

示例中创建了单独的一个线程去发送请求流，在主线程中读返回流，实现了一定程度上的并发。

### 3.3 流是会结束的

并不似长连接，建立上之后就一直保持，有消息的时候发送。（是否有通过建立一个流 RPC 建立推送机制？）

- Client 发送流，是通过 `Writer->WritesDone()` 函数结束流
- Server 发送流，是通过结束 RPC 函数并返回状态码的方式来结束流
- 流接受者，都是通过 `Reader->Read()` 返回的 bool 型状态，来判断流是否结束

Server 并没有像 Client 一样调用 `WriteDone()`，而是在消息之后，将 status code、可选的 status message、可选的 trailing metadata 追加进行发送，这就意味着流结束了。

## **4.通信协议**

本节通过介绍 gRPC 协议文档描述和对 helloworld 的抓包，来说明 gRPC 到底是如何传输的。

官方文档《[gRPC over HTTP2](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md)》中有描述 gRPC 基于 HTTP2 的具体实现，主要介绍的就是协议，也就是 gRPC 的请求和返回是如何基于 HTTP 协议构造的。如果不熟悉 HTTP2 可以阅读一下 [RFC 7540](https://httpwg.org/specs/rfc7540.html)。

### 4.1 ABNF 语法

ABNF 语法是一种描述协议的标准，gRPC 协议也是使用 ABNF 语法描述，几种常见的运算符在[第三节](https://datatracker.ietf.org/doc/html/rfc5234#section-3)中有介绍：

```
3.  Operators 3.1.  Concatenation:  Rule1 Rule2 3.2.  Alternatives:  Rule1 / Rule2 3.3.  Incremental Alternatives: Rule1 =/ Rule2 3.4.  Value Range Alternatives:  %c##-## 3.5.  Sequence Group:  (Rule1 Rule2) 3.6.  Variable Repetition:  *Rule 3.7.  Specific Repetition:  nRule 3.8.  Optional Sequence:  [RULE] 3.9.  Comment:  ; Comment 3.10. Operator Precedence
```

### 4.2 请求协议

`*<element>` 表示 element 会重复多次（最少 0 次）。知道这个就能理解概况里的描述了：

```
Request → Request-Headers *Length-Prefixed-Message EOSRequest-Headers → Call-Definition *Custom-Metadata
```

这表示 **Request 是由 3 部分组成**，首先是 `Request-Headers`，接下来是可能多次出现的 `Length-Prefixed-Message`，最后以一个 `EOS` 结尾（EOS 表示 End-Of-Stream）。

#### **4.2.1 Request-Headers**

根据上边的协议描述， `Request-Headers` 是由一个 `Call-Definition` 和若干 `Custom-Metadata` 组成。

`[]` 表示最多出现一次，比如 `Call-Definition` 有很多组成部分，其中 `Message-Type` 等是选填的：

```
Call-Definition → Method Scheme Path TE [Authority] [Timeout] Content-Type [Message-Type] [Message-Encoding] [Message-Accept-Encoding] [User-Agent]
```

通过 Wireshark 抓包可以看到请求的 Call-Definition 中共有所有要求的 Header，还有额外可选的，比如 user-agent：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151514217645328.png)

因为 helloworld 的示例比较简单，请求中没有填写自定义的元数据（Custom-Metadata）

#### **4.2.2 Length-Prefixed-Message**

传输的 Length-Prefixed-Message 也分为三部分：

```
Length-Prefixed-Message → Compressed-Flag Message-Length Message
```

同样的，Wireshark 抓到的请求中也有这部分信息，并且设置 `.proto` 文件的搜索路径之后可以自动解析 PB：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151514343488279.png)

其中第一个红框（Compressed-Flag）表示不进行压缩，第二个红框（Message-Length）表示消息长度为 7，蓝色反选部分则是 Protobuf 序列化的二进制内容，也就是 Message。

在 gRPC 的[核心概念](https://grpc.io/docs/what-is-grpc/core-concepts/#service-definition)介绍时提到，gRPC 默认使用 Protobuf 作为接口定义语言（IDL），也可以使用其他的 IDL 替代 Protobuf：

> By default, gRPC uses [protocol buffers](https://developers.google.com/protocol-buffers) as the Interface Definition Language (IDL) for describing both the service interface and the structure of the payload messages. It is possible to use other alternatives if desired.

这里 Length-Prefixed-Message 中传输的可以是 PB 也可以是 JSON，须通过 `Content-Type` 头中描述告知。

#### **4.2.3 EOS**

End-Of-Stream 并没有单独的数据去描述，而是通过 HTTP2 的数据帧上带一个 END_STREAM 的 flag 来标识的。比如 helloworld 中请求的数据帧，也携带了 END_STREAM 的标签：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151514519376577.png)

### 4.3 返回协议

`()` 表示括号中的内容作为单个元素对待，`/` 表示前后两个元素可选其一。Response 的定义说明，可以有两种返回形式，一种是消息头、消息体、Trailer，另外一种是只带 Trailer：

```
Response → (Response-Headers *Length-Prefixed-Message Trailers) / Trailers-Only
```

这里需要区分 gRPC 的 Status 和 HTTP 的 Status 两种状态。

```
Response-Headers → HTTP-Status [Message-Encoding] [Message-Accept-Encoding] Content-Type *Custom-MetadataTrailers-Only → HTTP-Status Content-Type TrailersTrailers → Status [Status-Message] *Custom-Metadata
```

不管是哪种形式，最后一部分都是`Trailers`，其中包含了 gRPC 的状态码、状态信息和额外的自定义元数据。

同样地，**使用 END_STREAM 的 flag 标识最后 Trailer 的结束。**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151515032682015.png)

### 4.4 与 HTTP/2 的关系

> The libraries in this repository provide a concrete implemnetation of the gRPC protocol, layered over HTTP/2.

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151515155159188.png)

## **5.上下文**

gRPC 支持上下文的传递，其主要用途有：

- 添加自定义的 metadata，能够通过 gRPC 调用传递
- 控制调用配置，如压缩、鉴权、超时
- 从对端获取 metadata
- 用于性能测量，比如使用 opencensus 等

客户端添加自定义的 metadata key-value 对没有特别的区分，而服务端添加的，则有 inital 和 trailing 两种 metadata 的区分。这也分别对应这 ClientContext 只有一个添加 Metadata 的函数：

```
void AddMetadata (const std::string &meta_key, const std::string &meta_value)
```

而 ServerContext 则有两个：

```
void AddInitialMetadata (const std::string &key, const std::string &value)void AddTrailingMetadata (const std::string &key, const std::string &value)
```

还有一种 Callback Server 对应的上下文叫做 `CallbackServerContext`，它与 `ServerContext` 继承自同一个基类，功能基本上相同。区别在于：

- ServerContext 被 Sync Server 和基于 CQ 的 Async Server 所使用，后者需要用到 `AsyncNotifyWhenDone`
- CallbackServerContext 因为在 `CallOnDone` 的时候，需要释放 context，因此需要知道 context_allocator，因此对应设置和获取 context_allocator 的两个函数

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151515261688750.png)

## **6.Generated Code**

通过 `protoc` 生成 gRPC 相关的文件，除了用于消息体定义的 `xxx.pb.h` 和 `xxx.pb.cc` 文件之外，就是定义 RPC 过程的 `xxx.grpc.pb.h` 和 `xxx.grpc.pb.cc`。本节以 `helloworld.proto` 生成的文件为例，看看 `.grpc.pb` 相关文件具体定义了些什么。

`helloworld.grpc.pb.h` 文件中有命名空间 `helloworld`，其中就仅包含一个类 `Greeter`，所有的 RPC 相关定义都在 `Greeter` 当中，这其中又主要分为两部分：

- Client 用于调用 RPC 的媒介 `Stub` 相关类
- Server 端用于实现不同服务的 Service 相关类和类模板

### 6.1 Stub

`.proto` 中的一个 `service` 只有一个 `Stub`，该类中会提供对应每个 RPC 所有的同步、异步、回调等方式的函数都包含在该类中，而该类继承自接口类 `StubInterface`。

为什么需要一个 StubInterface 来让 Stub 继承，而不是直接产生 Stub？别的复杂的 proto 会有多个 Stub 继承同一个 StubInterface 的情况？不会，因为每个 RPC 对应的函数名是不同。

Greeter 中唯一一个函数是用于创建 Stub 的静态函数 `NewStub`：

```
static std::unique_ptr<Stub> NewStub(...)
```

Stub 中同步、异步方式的函数是直接作为 Stub 的成员函数提供，比如针对一元调用：

- SayHello
- AsyncSayHello
- PrepareAsyncSayHello

[TODO] 为什么同步函数`SayHello`的实现是放在源代码中，而异步函数`AsyncSayHello`的实现是放在头文件中（两者都是直接 `return` 的）？

```
return ::grpc::internal::BlockingUnaryCall< ::helloworld::HelloRequest, ::helloworld::HelloReply, ::grpc::protobuf::MessageLite, ::grpc::protobuf::MessageLite>(channel_.get(), rpcmethod_SayHello_, context, request, response);return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::helloworld::HelloReply>>(AsyncSayHelloRaw(context, request, cq));
```

回调方式的 RPC 调用是通过一个 `experimental_async` 的类进行了封装（有个 `async_stub_` 的成员变量），所以回调 Client 中提到，回调的调用方式用法是 `stub_->async()->SayHello(...)`。

`experimental_async` 类定义中将 `Stub` 类作为自己的友元，自己的成员可以被 `Stub` 直接访问，而在 `StubInterface` 中也对应有一个 `experimental_async_interface` 的接口类，规定了要实现哪些接口。

### 6.2 Service

有几个概念都叫 Service：proto 文件中 RPC 的集合、proto 文件中 service 产生源文件中的 `Greeter::Service` 类、gRPC 框架中的 `::grpc::Service` 类。本小节说的 Service 就是 `helloworld.grpc.pb.h` 中的 `Greeter::Service`。

#### **6.2.1 Service 是如何定义的**

`helloworld.grpc.pb.h` 文件中共定义了 **7 种 Service**，拿出最常用的 `Service` 和 `AsyncService` 两个定义来说明下 Service 的定义过程：通过类模板链式继承。

**`Service` 跟其他几种 Service 不同，直接继承自 `grpc::Service`，而其他的 Service 都是由类模板构造出来的，而且使用类模板进行嵌套，最基础的类就是这里的 `Service`。**

`Service` 有以下特点：

- 构造函数利用其父类 `grpc::Service` 的 `AddMethod()` 函数，将 `.proto` 文件中定义的 RPC API，添加到成员变量 `methods_` 中（`methods_` 是个向量）
- `AddMethod()` 时会创建 `RpcServiceMethod` 对象，而该对象有一个属性叫做 `api_type_`，构造时默认填的 `ApiType::SYNC`
- `SayHello` 函数不直接声明为纯虚函数，而是以返回 `UNIMPLEMENTED` 状态，因为这个类可能被多次、多级继承

**所以 `Service` 类中的所有 RPC API 都是同步的。**

再看 `AsyncService` 的具体定义：

```
template <class BaseClass>  class WithAsyncMethod_SayHello : public BaseClass { ... };typedef WithAsyncMethod_SayHello<Service > AsyncService;
```

所以 **`AsyncService` 的含义就是继承自 `Service`，加上了 `WithAsyncMethod_SayHello` 的新功能**：

- 构造时，将 SayHello (RPC) 对应的 `api_type_` 设置为 `ApiType::ASYNC`
- 将 `SayHello` 函数直接禁用掉， `abort()` + 返回 `UNIMPLEMENTED` 状态码
- 添加 `RequestSayHello()` 函数， 异步 Server 小节中有介绍过这个函数用法

通过 gRPC 提供的 `route_guide.proto` 例子能更明显的理解这点：

```
typedef WithAsyncMethod_GetFeature< \    WithAsyncMethod_ListFeatures< \    WithAsyncMethod_RecordRoute< \    WithAsyncMethod_RouteChat<Service> > > >    AsyncService;
```

这里 RouteGuide 服务中有 4 个 RPC，`GetFeature`、`ListFeatures`、`RecordRoute`、`RouteChat`，通过 4 个`WithAsyncMethod_{RPC_name}` 的类模板嵌套，能将 4 个 API 都设置成 `ApiType::ASYNC`、添加上对应的 `RequestXXX()` 函数、禁用同步函数。

[TODO] **通过类模板嵌套继承的方式，有什么好处？** 为什么不直接实现 `AsyncService` 这个类呢？

#### **6.2.2 Service 的种类**

`helloworld.grpc.pb.h` 文件中 7 种 Service 中，有 3 对 Service 的真正含义都相同（出于什么目的使用不同的名称？），实际只剩下 4 种 Service。前三种在前边的同步、异步、回调 Server 的介绍中都有涉及。

- Service
- AsyncService
- CallbackService
- ExperimentalCallbackService – 等价于 CallbackService
- StreamedUnaryService
- SplitStreamedService – 等价于 Service
- StreamedService – 等价于 StreamedUnaryService

其实这些不同类型的 Service 是跟前边提到的 `api_type_` 有关。使用不同的 `::grpc::Service::MarkMethodXXX` 设置**不同的 ApiType** 会产生**不同的 API 模板类**，所有 API 模板类级联起来，就得到了**不同的 Service**。这三者的关系简单列举如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151515452378680.png)

另外还有两种模板是通过设置其他属性产生的，这里暂时不做介绍：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151515538628129.png)

[TODO] **头文件中没有用到的类模板在什么场景中会用到？**

### 6.3 与 `::grpc` 核心库的关系

`Stub` 类中主要是用到 gRPC Channel 和不同类型 RPC 对应的方法实现:

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151516064044761.png)

`Service` 类则继承自 `::grpc::Service`，具备其父类的能力，需要自己实现一些 RPC 方法具体的处理逻辑。其它 Service 涉及到 gRPC 核心库的联系有：

- `AsyncService::RequestSayHello()` 调用 `::grpc::Service::RequestAsyncUnary`。
- `CallbackService::SayHello()` 函数返回的是 `::grpc::ServerUnaryReactor` 指针。
- `CallbackService::SetMessageAllocatorFor_SayHello()` 函数中调用 `::grpc::internal::CallbackUnaryHandler::SetMessageAllocator()` 函数设置 RPC 方法的回调的消息分配器。

[TODO] `SetMessageAllocatorFor_SayHello()` 函数并没有被调用到，默认该分配器指针初始值为空，表示用户预先自己分配好而无需回调时分配？

**参考资料**

- https://grpc.io/docs/what-is-grpc/core-concepts/
- https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md
- https://grpc.io/blog/wireshark/

原文作者：jasonzxpan，腾讯 IEG 运营开发工程师

原文链接：https://mp.weixin.qq.com/s/I2QHEBO26nGqhGwIw281Pg



# 【NO.89】深入浅出 Linux 惊群：现象、原因和解决方案

> “惊群”简单地来讲，就是多个进程(线程)阻塞睡眠在某个系统调用上，在等待某个 fd(socket)的事件的到来。当这个 fd(socket)的事件发生的时候，这些睡眠的进程(线程)就会被同时唤醒，多个进程(线程)从阻塞的系统调用上返回，这就是”惊群”现象。”惊群”被人诟病的是效率低下，大量的 CPU 时间浪费在被唤醒发现无事可做，然后又继续睡眠的反复切换上。本文谈谈 linux socket 中的一些”惊群”现象、原因以及解决方案。

## **1. Accept”惊群”现象**

我们知道，在网络分组通信中，网络数据包的接收是异步进行的，因为你不知道什么时候会有数据包到来。因此，网络收包大体分为两个过程：

```
[1] 数据包到来后的事件通知[2] 收到事件通知的Task执行流，响应事件并从队列中取出数据包
```

数据包到来的通知分为两部分：

(1)网卡通知数据包到来，中断协议栈收包；

(2)协议栈将数据包填充 socket 的接收队列，通知应用程序有数据可读，这里仅讨论数据到达协议栈之后的事情。

应用程序是通过 socket 和协议栈交互的，socket 隔离了应用程序和协议栈，socket 是两者之间的接口，对于应用程序，它代表协议栈；而对于协议栈，它又代表应用程序，当数据包到达协议栈的时候，发生下面两个过程：

```
[1] 协议栈将数据包放入socket的接收缓冲区队列，并通知持有该socket的应用程序；[2] 持有该socket的应用程序响应通知事件，将数据包从socket的接收缓冲区队列中取出
```

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151528404233757.png)

对于高性能的服务器而言，为了利用多 CPU 核的优势，大多采用多个进程(线程)同时在一个 listen socket 上进行 accept 请求。多个进程阻塞在 Accept 调用上，那么在协议栈将 Client 的请求 socket 放入 listen socket 的 accept 队列的时候，是要唤醒一个进程还是全部进程来处理呢？

linux 内核通过睡眠队列来组织所有等待某个事件的 task，而 wakeup 机制则可以异步唤醒整个睡眠队列上的 task，wakeup 逻辑在唤醒睡眠队列时，会遍历该队列链表上的每一个节点，调用每一个节点的 callback，从而唤醒睡眠队列上的每个 task。这样，在一个 connect 到达这个 lisent socket 的时候，内核会唤醒所有睡眠在 accept 队列上的 task。N 个 task 进程(线程)同时从 accept 返回，但是，只有一个 task 返回这个 connect 的 fd，其他 task 都返回-1(EAGAIN)。这是典型的 accept”惊群”现象。这个是 linux 上困扰了大家很长时间的一个经典问题，在 linux2.6(似乎在 2.4.1 以后就已经解决，有兴趣的同学可以去验证一下)以后的内核中得到彻底的解决，通过添加了一个 WQ_FLAG_EXCLUSIVE 标记告诉内核进行排他性的唤醒，即唤醒一个进程后即退出唤醒的过程，具体如下：

```
/* * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just * wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve * number) then we wake all the non-exclusive tasks and one exclusive task. * * There are circumstances in which we can try to wake a task which has already * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns * zero in this (rare) case, and we handle it by continuing to scan the queue. */static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,            int nr_exclusive, int wake_flags, void *key){    wait_queue_t *curr, *next;    list_for_each_entry_safe(curr, next, &q->task_list, task_list) {        unsigned flags = curr->flags;        if (curr->func(curr, mode, wake_flags, key) &&                (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)            break;    }}
```

这样，在 linux 2.6 以后的内核，用户进程 task 对 listen socket 进行 accept 操作，如果这个时候如果没有新的 connect 请求过来，用户进程 task 会阻塞睡眠在 listent fd 的睡眠队列上。这个时候，用户进程 Task 会被设置 WQ_FLAG_EXCLUSIVE 标志位，并加入到 listen socket 的睡眠队列尾部(这里要确保所有不带 WQ_FLAG_EXCLUSIVE 标志位的 non-exclusive waiters 排在带 WQ_FLAG_EXCLUSIVE 标志位的 exclusive waiters 前面)。根据前面的唤醒逻辑，一个新的 connect 到来，内核只会唤醒一个用户进程 task 就会退出唤醒过程，从而不存在了”惊群”现象。

## **2. select/poll/Epoll “惊群”现象**

尽管 accept 系统调用已经不再存在”惊群”现象，但是我们的”惊群”场景还没结束。通常一个 server 有很多其他网络 IO 事件要处理，我们并不希望 server 阻塞在 accept 调用上，为提高服务器的并发处理能力，我们一般会使用 select/poll/epoll I/O 多路复用技术，同时为了充分利用多核 CPU，服务器上会起多个进程(线程)同时提供服务。于是，在某一时刻多个进程(线程)阻塞在 select/poll/epoll_wait 系统调用上，当一个请求上来的时候，多个进程都会被 select/poll/epoll_wait 唤醒去 accept，然而只有一个进程(线程 accept 成功，其他进程(线程 accept 失败，然后重新阻塞在 select/poll/epoll_wait 系统调用上。可见，尽管 accept 不存在”惊群”，但是我们还是没能摆脱”惊群”的命运。难道真的没办法了么？我只让一个进程去监听 listen socket 的可读事件，这样不就可以避免”惊群”了么？

没错，就是这个思路，我们来看看 Nginx 是怎么避免由于 listen fd 可读造成的 epoll_wait”惊群”。这里简单说下具体流程，不进行具体的源码分析。

### 2.1 Nginx 的 epoll”惊群”避免

Nginx 中有个标志 ngx_use_accept_mutex，当 ngx_use_accept_mutex 为 1 的时候（当 nginx worker 进程数>1 时且配置文件中打开 accept_mutex 时，这个标志置为 1），表示要进行 listen fdt”惊群”避免。

Nginx 的 worker 进程在进行 event 模块的初始化的时候，在 core event 模块的 process_init 函数中(ngx_event_process_init)将 listen fd 加入到 epoll 中并监听其 READ 事件。Nginx 在进行相关初始化完成后，进入事件循环(ngx_process_events_and_timers 函数)，在 ngx_process_events_and_timers 中判断，如果 ngx_use_accept_mutex 为 0，那就直接进入 ngx_process_events(ngx_epoll_process_events)，在 ngx_epoll_process_events 将调用 epoll_wait 等待相关事件到来或超时，epoll_wait 返回的时候该干嘛就干嘛。这里不讲 ngx_use_accept_mutex 为 0 的流程，下面讲下 ngx_use_accept_mutex 为 1 的流程。

```
[1] 进入ngx_trylock_accept_mutex，加锁抢夺accept权限（ngx_shmtx_trylock(&ngx_accept_mutex)），加锁成功，则调用ngx_enable_accept_events(cycle) 来将一个或多个listen fd加入epoll监听READ事件(设置事件的回调函数ngx_event_accept)，并设置ngx_accept_mutex_held = 1;标识自己持有锁。[2] 如果ngx_shmtx_trylock(&ngx_accept_mutex)失败，则调用ngx_disable_accept_events(cycle, 0)来将listen fd从epoll中delete掉。[3] 如果ngx_accept_mutex_held = 1(也就是抢到accept权)，则设置延迟处理事件标志位flags |= NGX_POST_EVENTS; 如果ngx_accept_mutex_held = 0(没抢到accept权)，则调整一下自己的epoll_wait超时，让自己下次能早点去抢夺accept权。[4] 进入ngx_process_events(ngx_epoll_process_events)，在ngx_epoll_process_events将调用epoll_wait等待相关事件到来或超时。[5] epoll_wait返回，循环遍历返回的事件，如果标志位flags被设置了NGX_POST_EVENTS，则将事件挂载到相应的队列中(Nginx有两个延迟处理队列，(1)ngx_posted_accept_events：listen fd返回的事件被挂载到的队列。(2)ngx_posted_events：其他socket fd返回的事件挂载到的队列)，延迟处理事件，否则直接调用事件的回调函数。[6] ngx_epoll_process_events返回后，则开始处理ngx_posted_accept_events队列上的事件，于是进入的回调函数是ngx_event_accept，在ngx_event_accept中accept客户端的请求，进行一些初始化工作，将accept到的socket fd放入epoll中。[7] ngx_epoll_process_events处理完成后，如果本进程持有accept锁ngx_accept_mutex_held = 1，那么就将锁释放。[8] 接着开始处理ngx_posted_events队列上的事件。
```

Nginx 通过一次仅允许一个进程将 listen fd 放入自己的 epoll 来监听其 READ 事件的方式来达到 listen fd”惊群”避免。然而做好这一点并不容易，作为一个高性能 web 服务器，需要尽量避免阻塞，并且要很好平衡各个工作 worker 的请求，避免饿死情况，下面有几个点需要大家留意：

```
[1] 避免新请求不能及时得到处理的饿死现象    工作worker在抢夺到accept权限，加锁成功的时候，要将事件的处理delay到释放锁后在处理(为什么ngx_posted_accept_events队列上的事件处理不需要延迟呢？ 因为ngx_posted_accept_events上的事件就是listen fd的可读事件，本来就是我抢到的accept权限，我还没accept就释放锁，这个时候被别人抢走了怎么办呢？)。否则，获得锁的工作worker由于在处理一个耗时事件，这个时候大量请求过来，其他工作worker空闲，然而没有处理权限在干着急。[2] 避免总是某个worker进程抢到锁，大量请求被同一个进程抢到，而其他worker进程却很清闲。    Nginx有个简单的负载均衡，ngx_accept_disabled表示此时满负荷程度，没必要再处理新连接了，我们在nginx.conf曾经配置了每一个nginx worker进程能够处理的最大连接数，当达到最大数的7/8时，ngx_accept_disabled为正，说明本nginx worker进程非常繁忙，将不再去处理新连接。每次要进行抢夺accept权限的时候，如果ngx_accept_disabled大于0，则递减1，不进行抢夺逻辑。
```

Nginx 采用在同一时刻仅允许一个 worker 进程监听 listen fd 的可读事件的方式，来避免 listen fd 的”惊群”现象。然而这种方式编程实现起来比较难，难道不能像 accept 一样解决 epoll 的”惊群”问题么？答案是可以的。要说明 epoll 的”惊群”问题以及解决方案，不能不从 epoll 的两种触发模式说起。

## **3. Epoll”惊群”之 LT(水平触发模式)、ET(边沿触发模式)**

我们先来看下 LT、ET 的语意：

```
[1] LT 水平触发模式只要仍然有未处理的事件，epoll就会通知你，调用epoll_wait就会立即返回。[2] ET 边沿触发模式只有事件列表发生变化了，epoll才会通知你。也就是，epoll_wait返回通知你去处理事件，如果没处理完，epoll不会再通知你了，调用epoll_wait会睡眠等待，直到下一个事件到来或者超时。
```

LT(水平触发模式)、ET(边沿触发模式)在”惊群”问题上，有什么不一样的表现么？要说明这个，就不能不来谈谈 Linux 内核的 sleep/wakeup 机制以及 epoll 的实现核心机制了。

### 3.1 epoll 的核心机制

在了解 epoll 的核心机制前，先了解一下内核 sleep/wakeup 机制的几个核心概念：

```
[1] 等待队列 waitqueue队列头(wait_queue_head_t)往往是资源生产者队列成员(wait_queue_t)往往是资源消费者当头的资源ready后, 会逐个执行每个成员指定的回调函数，来通知它们资源已经ready了[2] 内核的poll机制被Poll的fd, 必须在实现上支持内核的Poll技术，比如fd是某个字符设备,或者是个socket, 它必须实现file_operations中的poll操作, 给自己分配有一个等待队列头wait_queue_head_t，主动poll fd的某个进程task必须分配一个等待队列成员, 添加到fd的等待队列里面去, 并指定资源ready时的回调函数，用socket做例子, 它必须有实现一个poll操作, 这个Poll是发起轮询的代码必须主动调用的, 该函数中必须调用poll_wait(),poll_wait会将发起者作为等待队列成员加入到socket的等待队列中去，这样socket发生事件时可以通过队列头逐个通知所有关心它的进程。[3] epollfd本身也是个fd, 所以它本身也可以被epoll
```

epoll 作为中间层，为多个进程 task，监听多个 fd 的多个事件提供了一个便利的高效机制，我们来看下 epoll 的机制图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151528572399723.png)

从图中，可以看到 epoll 可以监控多个 fd 的事件，它通过一颗红黑树来组织所有被 epoll_ctl 加入到 epoll 监听列表中的 fd，每个被监听的 fd 在 epoll 用一个 epoll item(epi)来标识。

根据内核的 poll 机制，epoll 需要为每个监听的 fd 构造一个 epoll entry(设置关心的事件以及注册回调函数)作为等待队列成员睡眠在每个 fd 的等待队列，以便 fd 上的事件 ready 了，可以通过 epoll 注册的回调函数通知到 epoll。

epoll 作为进程 task 的中间层，它需要有一个等待队列 wq 给 task 在没事件来 epoll_wait 的时候来睡眠等待(epoll fd 本身也是一个 fd，它和其他 fd 一样还有另外一个等待队列 poll_wait，作为 poll 机制被 poll 的时候睡眠等待的地方)。

epoll 可能同时监听成千上万的 fd，这样在少量 fd 有事件 ready 的时候，它需要一个 ready list 队列来组织所有已经 ready 的就绪 fd，以便能够高效通知给进程 task，而不需要遍历所有监听的 fd。图中的一个 epoll 的 sleep/wakeup 流程如下：

```
无事件的时候，多个进程task调用epoll_wait睡眠在epoll的wq睡眠队列上。[1] 这个时候一个请求RQ_1上来，listen fd这个时候ready了，开始唤醒其睡眠队列上的epoll entry，并执行之前epoll注册的回调函数ep_poll_callback。[2] ep_poll_callback主要做两件事情，(1)发生的event事件是epoll entry关心的，则将epi挂载到epoll的就绪队列ready list并进入(2)，否则结束。(2)如果当前wq不为空，则唤醒睡眠在epoll等待队列上睡眠的task(这里唤醒一个还是多个，是区分epoll的ET模式还是LT模式，下面在细讲)。[3] epoll_wait被唤醒继续前行，在ep_poll中调用ep_send_events将fd相关的event事件和数据copy到用户空间，这个时候就需要遍历epoll的ready list以便收集task需要监控的多个fd的event事件和数据上报给用户进程task，这个在ep_scan_ready_list中完成，这里会将ready list清空。
```

通过上图的 epoll 事件通知机制，epoll 的 LT 模式、ET 模式在事件通知行为上的差别，也只能是在[2]上 task 唤醒逻辑上的差别了。我们先来看下，在 epoll_wait 中调用的导致用户进程 task 睡眠的 ep_poll 函数的核心逻辑：

```
static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout){int res = 0, eavail, timed_out = 0;bool waiter = false;...eavail = ep_events_available(ep);//是否有fd就绪if (eavail)goto send_events;//有fd就绪，则直接跳过去上报事件给用户if (!waiter) {          waiter = true;          init_waitqueue_entry(&wait, current);//为当前进程task构造一个睡眠entry          spin_lock_irq(&ep->wq.lock);          //插入到epoll的wq后面，注意这里是排他插入的，就是带WQ_FLAG_EXCLUSIVE flag          __add_wait_queue_exclusive(&ep->wq, &wait);          spin_unlock_irq(&ep->wq.lock);    }for (;;) {  //将当前进程设置位睡眠, 但是可以被信号唤醒的状态, 注意这个设置是"将来时", 我们此刻还没睡  set_current_state(TASK_INTERRUPTIBLE);  // 检查是否真的要睡了  if (fatal_signal_pending(current)) {              res = -EINTR;              break;          }          eavail = ep_events_available(ep);          if (eavail)              break;          if (signal_pending(current)) {              res = -EINTR;              break;          }          // 检查是否真的要睡了 end  //使得当前进程休眠指定的时间范围，  if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) {  timed_out = 1;  break;  }}__set_current_state(TASK_RUNNING);send_events:      /*       * Try to transfer events to user space. In case we get 0 events and       * there's still timeout left over, we go trying again in search of       * more luck.       */      // ep_send_events往用户态上报事件，即那些epoll_wait返回后能获取的事件      if (!res && eavail &&          !(res = ep_send_events(ep, events, maxevents)) && !timed_out)          goto fetch_events;      if (waiter) {          spin_lock_irq(&ep->wq.lock);          __remove_wait_queue(&ep->wq, &wait);          spin_unlock_irq(&ep->wq.lock);      }   return res;}
```

接着，我们看下监控的 fd 有事件发生的回调函数 ep_poll_callback 的核心逻辑：

```
#define wake_up(x)__wake_up(x, TASK_NORMAL, 1, NULL)static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key){      int pwake = 0;      struct epitem *epi = ep_item_from_wait(wait);      struct eventpoll *ep = epi->ep;      __poll_t pollflags = key_to_poll(key);      unsigned long flags;      int ewake = 0;  ....  //判断是否有我们关心的event      if (pollflags && !(pollflags & epi->event.events))          goto out_unlock;  //将当前的epitem放入epoll的ready list      if (!ep_is_linked(epi) &&          list_add_tail_lockless(&epi->rdllink, &ep->rdllist)) {          ep_pm_stay_awake_rcu(epi);      }      //如果有task睡眠在epoll的等待队列，唤醒它  if (waitqueue_active(&ep->wq)) {  ....  wake_up(&ep->wq);//  }  ....}
```

wake_up 函数最终会调用到 wake_up_common，通过前面的 wake_up_common 我们知道，唤醒过程在唤醒一个带 WQ_FLAG_EXCLUSIVE 标记的 task 后，即退出唤醒过程。通过上面的 ep_poll，task 是排他(带 WQ_FLAG_EXCLUSIVE 标记)加入到 epoll 的等待队列 wq 的。也就是，在 ep_poll_callback 回调中，只会唤醒一个 task。这就有问题，根据 LT 的语义：只要仍然有未处理的事件，epoll 就会通知你。例如有两个进程 A、B 睡眠在 epoll 的睡眠队列，fd 的可读事件到来唤醒进程 A，但是 A 可能很久才会去处理 fd 的事件，或者它根本就不去处理。根据 LT 的语义，应该要唤醒进程 B 的。

我们来看下 epoll 怎么在 ep_send_events 中实现满足 LT 语义的：

```
  static int ep_send_events(struct eventpoll *ep,                struct epoll_event __user *events, int maxevents)  {      struct ep_send_events_data esed;      esed.maxevents = maxevents;      esed.events = events;      ep_scan_ready_list(ep, ep_send_events_proc, &esed, 0, false);      return esed.res;  }  static __poll_t ep_scan_ready_list(struct eventpoll *ep,                    __poll_t (*sproc)(struct eventpoll *,                         struct list_head *, void *),                    void *priv, int depth, bool ep_locked)  {  ...  // 所有的epitem都转移到了txlist上, 而rdllist被清空了      list_splice_init(&ep->rdllist, &txlist);      ...      //sproc 就是 ep_send_events_proc      res = (*sproc)(ep, &txlist, priv);      ...      //没有处理完的epitem, 重新插入到ready list      list_splice(&txlist, &ep->rdllist);  /* ready list不为空, 直接唤醒... */ // 保证(2)      if (!list_empty(&ep->rdllist)) {          if (waitqueue_active(&ep->wq))              wake_up(&ep->wq);  ...      }  }    static __poll_t ep_send_events_proc(struct eventpoll *ep, struct list_head *head,                     void *priv)  {    ...    //遍历就绪fd列表        list_for_each_entry_safe(epi, tmp, head, rdllink) {        ...        //然后从链表里面移除当前就绪的epi        list_del_init(&epi->rdllink);        //读取当前epi的事件            revents = ep_item_poll(epi, &pt, 1);            if (!revents)              continue;        //将当前的事件和用户传入的数据都copy给用户空间            if (__put_user(revents, &uevent->events) ||              __put_user(epi->event.data, &uevent->data)) {              //如果发生错误了， 则终止遍历过程，将当前epi重新返回就绪队列，剩下的也会在ep_scan_ready_list中重新放回就绪队列              list_add(&epi->rdllink, head);              ep_pm_stay_awake(epi);              if (!esed->res)                  esed->res = -EFAULT;              return 0;           }        }          if (epi->event.events & EPOLLONESHOT)              epi->event.events &= EP_PRIVATE_BITS;          else if (!(epi->event.events & EPOLLET)) { // 保证(1)          //如果是非ET模式(即LT模式)，当前epi会被重新放到epoll的ready list。              list_add_tail(&epi->rdllink, &ep->rdllist);              ep_pm_stay_awake(epi);          }  }
```

上面处理逻辑的核心流程就 2 点：

```
[1] 遍历并清空epoll的ready list，遍历过程中，对于每个epi收集其返回的events，如果没收集到event，则continue去处理其他epi，否则将当前epi的事件和用户传入的数据都copy给用户空间，并判断，如果是在LT模式下，则将当前epi重新放回epoll的ready list[2] 遍历epoll的ready list完成后，如果ready list不为空，则继续唤醒epoll睡眠队列wq上的其他task B。task B从epoll_wait醒来继续前行，重复上面的流程，继续唤醒wq上的其他task C，这样链式唤醒下去。
```

通过上面的流程，在一个 epoll 上睡眠的多个 task，如果在一个 LT 模式下的 fd 的事件上来，会唤醒 epoll 睡眠队列上的所有 task，而 ET 模式下，仅仅唤醒一个 task，这是 epoll”惊群”的根源。等等，这样在 LT 模式下就必然”惊群”，epoll 在 LT 模式下的”惊群”没办法解决么？

### 3.2 epoll_create& fork

相信大家在多进程服务中使用 epoll 的时候，都会有这样一个疑问，是先 epoll_create 得到 epoll fd 后在 fork 子进程，还是先 fork 子进程，然后每个子进程在 epoll_create 自己独立的 epoll fd 呢？有什么异同？

#### **3.2.1 先 epoll_create 后 fork**

这样，多个进程公用一个 epoll 实例(父子进程的 epoll fd 指向同一个内核 epoll 对象)，上面介绍的 epoll 核心机制流程，都是在同一个 epoll 对象上的，这种情况下，epoll 有以下这些特性：

```
[1] epoll在ET模式下不存在“惊群”现象，LT模式是epoll“惊群”的根源，并且LT模式下的“惊群”没办法避免。[2] LT的“惊群”是链式唤醒的，唤醒过程直到当前epi的事件被处理了，无法获得到新的事件才会终止唤醒过程。例如有A、B、C、D...等多个进程task睡眠在epoll的睡眠队列上，并且都监控同一个listen fd的可读事件。一个请求上来，会首先唤醒A进程，A在epoll_wait的处理过程中会唤醒进程B，这样进程B在epoll_wait的处理过程中会唤醒C，这个时候A的epoll_wait处理完成返回，进程A调用accept读取了当前这个请求，进程C在自己的epoll_wait处理过程中，从epi中获取不到事件了，于是终止了整个链式唤醒过程。[3] 多个进程的epoll fd由于指向同一个epoll内核对象，他们对epoll fd的相关epoll_ctl操作会相互影响。一不小心可能会出现一些比较诡异的行为。想象这样一个场景(实际上应该不是这样用)，有一个服务在1234，1235，1236这3个端口上提供服务，于是它epoll_create得到epoll fd后，fork出3个工作的子进程A、B、C，它们分别在这3个端口创建listen fd，然后加入到epoll中监听其可读事件。这个时候端口1234上来一个请求，A、B、C同时被唤醒，A在epoll_wait返回后，在进行accept前由于种种原因卡住了，没能及时accept。B、C在epoll_wait返回后去accept又不能accept到请求，这样B、C重新回到epoll_wait，这个时候又被唤醒，这样只要A没有去处理这个请求之前，B、C就一直被唤醒，然而B、C又无法处理该请求。[4] ET模式下，一个fd上的同事多个事件上来，只会唤醒一个睡眠在epoll上的task，如果该task没有处理完这些事件，在没有新的事件上来前，epoll不会在通知task去处理。
```

由于 ET 的事件通知模式，通常在 ET 模式下的 epoll_wait 返回，我们会循环 accept 来处理所有未处理的请求，直到 accept 返回 EAGAIN 才退出 accept 流程。否则，没处理遗留下来的请求，这个时候如果没有新的请求过来触发 epoll_wait 返回，这样遗留下来的请求就得不到及时处理。这种处理模式，会带来一种类”惊群”现象。考虑，下面的一个处理过程：

```
A、B、C三个进程在监听listen fd的EPOLLIN事件，都睡眠在epoll_wait上，都是ET模式。[1] listen fd上一个请求C_1上来，该请求唤醒了A进程，A进程从epoll_wait返回准备去accept该请求来处理。[2] 这个时候，第二个请求C_2上来，由于睡眠队列上是B、C，于是epoll唤醒B进程，B进程从epoll_wait返回准备去accept该请求来处理。[3] A进程在自己的accept循环中，首选accept得到C_1，接着A进程在第二个循环继续accept，继续得到C_2。[4] B进程在自己的accept循环中，调用accept，由于C_2已经被A拿走了，于是B进程accept返回EAGAIN错误，于是B进程退出accept流程重新睡眠在epoll_wait上。[5] A进程继续第三个循环，这个时候已经没有请求了， accept返回EAGAIN错误，于是A进程也退出accept处理流程，进入请求的处理流程。
```

可以看到，B 进程被唤醒了，但是并没有事情可以做，同时，epoll 的 ET 这样的处理模式，负载容易出现不均衡。

#### **3.2.2 先 fork 后 epoll_create**

用法上，通常是在父进程创建了 listen fd 后，fork 多个 worker 子进程来共同处理同一个 listen fd 上的请求。这个时候，A、B、C…等多个子进程分别创建自己独立的 epoll fd，然后将同一个 listen fd 加入到 epoll 中，监听其可读事件。这种情况下，epoll 有以下这些特性：

```
[1] 由于相对同一个listen fd而言， 多个进程之间的epoll是平等的，于是，listen fd上的一个请求上来，会唤醒所有睡眠在listen fd睡眠队列上的epoll，epoll又唤醒对应的进程task，从而唤醒所有的进程(这里不管listen fd是以LT还是ET模式加入到epoll)。[2] 多个进程间的epoll是独立的，对epoll fd的相关epoll_ctl操作相互独立不影响。
```

可以看出，在使用友好度方面，多进程独立 epoll 实例要比共用 epoll 实例的模式要好很多。独立 epoll 模式要解决 fd 的排他唤醒 epoll 即可。

## **4.EPOLLEXCLUSIVE 排他唤醒 Epoll**

linux4.5 以后的内核版本中，增加了 EPOLLEXCLUSIVE， 该选项只能通过 EPOLL_CTL_ADD 对需要监控的 fd(例如 listen fd)设置 EPOLLEXCLUSIVE 标记。这样 epoll entry 是通过排他方式挂载到 listen fd 等待队列的尾部的，睡眠在 listen fd 的等待队列上的 epoll entry 会加上 WQ_FLAG_EXCLUSIVE 标记。根据前面介绍的内核 wake up 机制，listen fd 上的事件上来，在遍历并唤醒等待队列上的 entry 的时候，遇到并唤醒第一个带 WQ_FLAG_EXCLUSIVE 标记的 entry 后，就结束遍历唤醒过程。于是，多进程独立 epoll 的”惊群”问题得到解决。

## **5.”惊群”之 SO_REUSEPORT**

“惊群”浪费资源的本质在于很多处理进程在别惊醒后，发现根本无事可做，造成白白被唤醒，做了无用功。但是，简单的避免”惊群”会造成同时并发上来的请求得不到及时处理(降低了效率)，为了避免这种情况，NGINX 允许配置成获得 Accept 权限的进程一次性循环 Accept 所有同时到达的全部请求，但是，这会造成短时间 worker 进程的负载不均衡。为此，我们希望的是均衡唤醒，也就是，假设有 4 个 worker 进程睡眠在 epoll_wait 上，那么此时同时并发过来 3 个请求，我们希望 3 个 worker 进程被唤醒去处理，而不是仅仅唤醒一个进程或全部唤醒。

然而要实现这样不是件容易的事情，其根本原因在于，对于大多采用 MPM 机制(multi processing module)TCP 服务而言，基本上都是多个进程或者线程同时在一个 Listen socket 上进行监听请求。根据前面介绍的 Linux 睡眠队列的唤醒方式，基本睡眠在这个 listen socket 上的 Task 只能要么全部被唤醒，要么被唤醒一个。

于是，基本的解决方案是起多个 listen socket，好在我们有 SO_REUSEPORT(linux 3.9 以上内核支持)，它支持多个进程或线程 bind 相同的 ip 和端口，支持以下特性：

```
[1] 允许多个socket bind/listen在相同的IP，相同的TCP/UDP端口[2] 目的是同一个IP、PORT的请求在多个listen socket间负载均衡[3] 安全上，监听相同IP、PORT的socket只能位于同一个用户下
```

于是，在一个多核 CPU 的服务器上，我们通过 SO_REUSEPORT 来创建多个监听相同 IP、PORT 的 listen socket，每个进程监听不同的 listen socket。这样，在只有 1 个新请求到达监听的端口的时候，内核只会唤醒一个进程去 accept，而在同时并发多个请求来到的时候，内核会唤醒多个进程去 accept，并且在一定程度上保证唤醒的均衡性。SO_REUSEPORT 在一定程度上解决了”惊群”问题，但是，由于 SO_REUSEPORT 根据数据包的四元组和当前服务器上绑定同一个 IP、PORT 的 listen socket 数量，根据固定的 hash 算法来路由数据包的，其存在如下问题：

```
[1] Listen Socket数量发生变化的时候，会造成握手数据包的前一个数据包路由到A listen socket，而后一个握手数据包路由到B listen socket，这样会造成client的连接请求失败。[2] 短时间内各个listen socket间的负载不均衡
```

## **6.惊不”惊群”其实是个问题**

很多时候，我们并不是害怕”惊群”，我们怕的”惊群”之后，做了很多无用功。相反在一个异常繁忙，并发请求很多的服务器上，为了能够及时处理到来的请求，我们希望能有多”惊群”就多”惊群”，因为根本做不了无用功，请求多到都来不及处理。于是出现下面的情形：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151529318010740.png)

从上可以看到各个 CPU 都很忙，但是实际有用的 CPU 时间却很少，大部分的 CPU 消耗在*spin*lock 自旋锁上了，并且服务器并发吞吐量并没有随着 CPU 核数增加呈现线性增长，相反出现下降的情况。这是为什么呢？怎么解决？

### 6.1 问题原因

我们知道，一般一个 TCP 服务只有一个 listen socket、一个 accept 队列，而一个 TCP 服务一般有多个服务进程(一个核一个)来处理请求。于是并发请求到达 listen socket 处，那么多个服务进程势必存在竞争，竞争一存在，那么就需要用排队来解决竞态问题，于是似乎锁就无法避免了。在这里，有两类竞争主体，一类是内核协议栈(不可睡眠类)、一类是用户进程(可睡眠类)，这两类主体对 listen socket 发生三种类型的竞争：

```
[1] 协议栈内部之间的竞争[2] 用户进程内部之间的竞争[3] 协议栈和用户之间的竞争
```

由于内核协议栈是不可睡眠的，为此 linux 中采用两层锁定的 lock 结构，一把 listen_socket.lock 自旋锁，一把 listen_socket.own 排他标记锁。其中，listen_socket.lock 用于协议栈内部之间的竞争、协议栈和用户之间的竞争，而 listen_socket.own 用于用户进程内部之间的竞争，listen_socket.lock 作为 listen_socket.own 的排他保护(要获取 listen_socket.own 首先要获取到 listen_socket.lock)。对于处理 TCP 请求而言，一个 SYN 包 syn_skb 到来，这个时候内核 Lock(RCU 锁)住全局的 listeners Table，查找 syn_skb 对应的 listen_socket，没找到则返回错误。否则，就需要进入三次握手处理，首先内核协议栈需要自旋获得 listen_socket.lock 锁，初始化一些数据结构，回复 syn_ack，然后释放 listen_socket.lock 锁。

接着，client 端的 ack 包到来，协议栈这个时候，需要自旋获得 listen_socket.lock 锁，构造 client 端的 socket 等数据结构，如果 accept 队列没有被用户进程占用，那么就将连接排入 accept 队列等待用户进程来 accept，否则就排入 backlog 队列(职责转移，连接排入 accept 队列的事情交给占有 accept 队列的用户进程)。可见，处理一个请求，协议栈需要竞争两次 listen_socket 的自旋锁。由于内核协议栈不能睡眠，于是它只能自旋不断地去尝试获取 listen_socket.lock 自旋锁，直到获取到自旋锁成功为止，中间不能停下来。自旋锁这种暴力、打架的抢锁方式，在一个高并发请求到来的服务器上，就有可能出现上面这种 80%多的 CPU 时间被内核占用，应用程序只能够分配到较少的 CPU 时钟周期的资源的情况。

### 6.2 问题的解决

解决这个问题无非两个方向：(1) 多队列化，减少竞争者 (2) listen_socket 无锁化 。

#### **6.2.1 多队列化 - SO_REUSEPORT**

通过上面的介绍，在 Linux kernel 3.9 以上，可以通过 SO_REUSEPORT 来创建多个 bind 相同 IP、PORT 的 listen_socket。我们可以每一个 CPU 核创建一个 listen_socket 来监听处理请求，这样就是每个 CPU 一个处理进程、一个 listen_socket、一个 accept 队列，多个进程同时并发处理请求，进程之间不再相互竞争 listen_socket。SO_REUSEPORT 可以做到多个 listen_socket 间的负载均衡，然而其负载均衡效果是取决于 hash 算法，可能会出现短时间内的负载极端不均衡。

SO_REUSEPORT 是在将一对多的问题变成多对多的问题，将 Listen Socket 无序暴力争抢 CPU 的现状变成更为有序的争抢。多队列化的优化必须要面对和解决的四个问题是：队列比 CPU 多，队列与 CPU 相等，队列比 CPU 少，根本就没有队列，于是，他们要解决队列发生变化的情况。

如果仅仅把 TCP 的 Listener 看作一个被协议栈处理的 Socket，它和 Client Socket 一起都在相互拼命抢 CPU 资源，那么就可能出现上面的，短时间大量并发请求过来的时候，大量的 CPU 时间被消耗在自旋锁的争抢上了。我们可以换个角度，如果把 TCP Listener 看作一个基础设施服务呢？Listener 为新来的连接请求提供连接服务，并产生 Client Socket 给用户进程，它可以通过一个或多个两类 Accept 队列提供一个服务窗口给用户进程来 accept Client Socket 来处理。仅仅在 Client Socket 需要排入 Accept 队列的是，细粒度锁住队列即可，多个有多个 Accept 队列(每 CPU 一个，那么连锁队列的操作都可以省了)。这样 Listener 就与用户进程无关了，用户进程的产生、退出、CPU 间跳跃、绑定，解除绑定等等都不会影响 TCP Listener 基础设施服务，受影响的是仅仅他们自己该从那个 Accept 队列获取 Client Socket 来处理。于是一个解决思路是连接处理无锁化。

#### **6.2.2 listen socket 无锁化- 旁门左道之 SYN Cookie**

SYN Cookie 原理由 D.J. Bernstain 和 Eric Schenk 提出，专门用来防范 SYN Flood 攻击的一种手段。它的原理是，在 TCP 服务器接收到 SYN 包并返回 SYN ACK 包时，不分配一个专门的数据结构(避免浪费服务器资源)，而是根据这个 SYN 包计算出一个 cookie 值。这个 cookie 作为 SYN ACK 包的初始序列号。当客户端返回一个 ACK 包时，根据包头信息计算 cookie，与返回的确认序列号(初始序列号 + 1)进行对比，如果相同，则是一个正常连接，然后，分配资源，创建 Client Socket 排入 Accept 队列等等用户进程取出处理。于是，整个 TCP 连接处理过程实现了无状态的三次握手。SYN Cookie 机制实现了一定程度上的 listen socket 无锁化，但是它有以下几个缺点。

- **（1）丢失 TCP 选项信息**在建立连接的过程中，不在服务器端保存任何信息，它会丢失很多选项协商信息，这些信息对 TCP 的性能至关重要，比如超时重传等。但是，如果使用时间戳选项，则会把 TCP 选项信息保存在 SYN ACK 段中 tsval 的低 6 位。
- **（2）cookie 不能随地开启**Linux 采用动态资源分配机制，当分配了一定的资源后再采用 cookie 技术。同时为了避免另一种拒绝服务攻击方式，攻击者发送大量的 ACK 报文，服务器忙于计算验证 SYN Cookie。服务器对收到的 ACK 进行 Cookie 合法性验证前，需要确定最近确实发生了半连接队列溢出，不然攻击者只要随便发送一些 ACK，服务器便要忙于计算了。

#### **6.2.3 listen socket 无锁化- Linux 4.4 内核给出的 Lockless TCP listener**

SYN cookie 给出了 Lockless TCP listener 的一些思路，但是我们不想是无状态的三次握手，又不想请求的处理和 Listener 强相关，避免每次进行握手处理都需要 lock 住 listen socket，带来性能瓶颈。4.4 内核前的握手处理是以 listen socket 为主体，listen socket 管理着所有属于它的请求，于是进行三次握手的每个数据包的处理都需要操作这个 listener 本身，而一般情况下，一个 TCP 服务器只有一个 listener，于是在多核环境下，就需要加锁 listen socket 来安全处理握手过程了。我们可以换个角度，握手的处理不再以 listen socket 为主体，而是以连接本身为主体，需要记住的是该连接所属的 listen socket 即可。4.4 内核握手处理流程如下：

[1] TCP 数据包 skb 到达本机，内核协议栈从全局 socket 表中查找 skb 的目的 socket(sk)，如果是 SYN 包，当然查找到的是 listen_socket 了，于是，协议栈根据 skb 构造出一个新的 socket(tmp_sk)，并将 tmp_sk 的 listener 标记为 listen_socket，并将 tmp_sk 的状态设置为 SYNRECV，同时将构造好的 tmp_sk 排入全局 socket 表中，并回复 syn_ack 给 client。

[2] 如果到达本机的 skb 是 syn_ack 的 ack 数据包，那么查找到的将是 tmp_sk，并且 tmp_sk 的 state 是 SYNRECV，于是内核知道该数据包 skb 是 syn_ack 的 ack 包了，于是在 new_sk 中拿出连接所属的 listen_socket，并且根据 tmp_sk 和到来的 skb 构造出 client_socket，然后将 tmp_sk 从全局 socket 表中删除(它的使命结束了)，最后根据所属的 listen_socket 将 client_socket 排如 listen_socket 的 accept 队列中，整个握手过程结束。

4.4 内核一改之前的以 listener 为主体，listener 管理所有 request 的方式，在 SYN 包到来的时候，进行控制反转，以 Request 为主体，构造出一个临时的 tmp_sk 并标记好其所属的 listener，然后平行插入到所有 socket 公共的 socket 哈希表中，从而解放掉 listener，实现 Lockless TCP listener。

## **7.参考文献**

https://blog.csdn.net/dog250/article/details/50528426 https://zhuanlan.zhihu.com/p/51251700 https://blog.csdn.net/dog250/article/details/80837278

原文作者：morganhuang，腾讯 IEG 后台工程师

原文链接：https://mp.weixin.qq.com/s/dQWKBujtPcazzw7zacP1lg

# 【NO.90】Nginx 最全操作总结

> 本文将会从：安装 -> 全局配置 -> 常用的各种配置 来书写，其中常用配置写的炒鸡详细，需要的童鞋可以直接滑倒相应的位置查看。

## 1.**安装 nginx**

**下载 nginx 的压缩包文件到根目录，官网下载地址：nginx.org/download/nginx-x.xx.xx.tar.gz**

```
yum update #更新系统软件cd /wget nginx.org/download/nginx-1.17.2.tar.gz
```

**解压 tar.gz 压缩包文件，进去 nginx-1.17.2**

```
tar -xzvf nginx-1.17.2.tar.gzcd nginx-1.17.2
```

**进入文件夹后进行配置检查**

```
./configure
```

**通过安装前的配置检查，发现有报错。检查中发现一些依赖库没有找到，这时候需要先安装 nginx 的一些依赖库**

```
yum -y install pcre* #安装使nginx支持rewriteyum -y install gcc-c++yum -y install zlib*yum -y install openssl openssl-devel
```

**再次进行检查操作 ./configure 没发现报错显示，接下来进行编译并安装的操作**

```
 // 检查模块支持  ./configure  --prefix=/usr/local/nginx  --with-http_ssl_module --with-http_v2_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_slice_module --with-http_stub_status_module --with-mail --with-mail_ssl_module --with-stream --with-stream_ssl_module --with-stream_realip_module --with-stream_ssl_preread_module --with-threads --user=www --group=www
```

这里得特别注意下，你以后需要用到的功能模块是否存在，不然以后添加新的包会比较麻烦。

**查看默认安装的模块支持**

命令 `ls nginx-1.17.2` 查看 nginx 的文件列表，可以发现里面有一个 auto 的目录。

在这个 auto 目录中有一个 options 文件，这个文件里面保存的就是 nginx 编译过程中的所有选项配置。

通过命令：`cat nginx-1.17.2/auto/options | grep YES`就可以查看

[nginx 编译安装时，怎么查看安装模块](https://jingyan.baidu.com/article/454316ab354edcf7a7c03a81.html)

**编译并安装**

```
make && make install
```

这里需要注意，模块的支持跟后续的 nginx 配置有关，比如 SSL，gzip 压缩等等，编译安装前最好检查需要配置的模块存不存在。

**查看 nginx 安装后在的目录，可以看到已经安装到 /usr/local/nginx 目录了**

```
whereis nginx$nginx: /usr/local/nginx
```

**启动 nginx 服务**

```
cd /usr/local/nginx/sbin/./nginx
```

服务启动的时候报错了：`nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)` ，通过命令查看本机网络地址和端口等一些信息，找到被占用的 80 端口 `netstat -ntpl` 的 tcp 连接，并杀死进程(kill 进程 pid)

```
netstat -ntplkill 进程PID
```

继续启动 nginx 服务，启动成功

```
./nginx
```

在浏览器直接访问 ip 地址，页面出现 Welcome to Nginx! 则安装成功。

## **2.nginx 配置**

### 2.1 基本结构

```
main        # 全局配置，对全局生效├── events  # 配置影响 nginx 服务器或与用户的网络连接├── http    # 配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置│   ├── upstream # 配置后端服务器具体地址，负载均衡配置不可或缺的部分│   ├── server   # 配置虚拟主机的相关参数，一个 http 块中可以有多个 server 块│   ├── server│   │   ├── location  # server 块可以包含多个 location 块，location 指令用于匹配 uri│   │   ├── location│   │   └── ...│   └── ...└── ...
```

### 2.2 主要配置含义

- main:nginx 的全局配置，对全局生效。
- events:配置影响 nginx 服务器或与用户的网络连接。
- http：可以嵌套多个 server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。
- server：配置虚拟主机的相关参数，一个 http 中可以有多个 server。
- location：配置请求的路由，以及各种页面的处理情况。
- upstream：配置后端服务器具体地址，负载均衡配置不可或缺的部分。

### 2.3 nginx.conf 配置文件的语法规则

1. 配置文件由指令与指令块构成
2. 每条指令以 “;” 分号结尾，指令与参数间以空格符号分隔
3. 指令块以 {} 大括号将多条指令组织在一起
4. include 语句允许组合多个配置文件以提升可维护性
5. 通过 # 符号添加注释，提高可读性
6. 通过 $ 符号使用变量
7. 部分指令的参数支持正则表达式，例如常用的 location 指令

### 2.4 内置变量

nginx 常用的内置全局变量，你可以在配置中随意使用：
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151537491057483.png)

### 2.5 常用命令

这里列举几个常用的命令：

```
nginx -s reload  # 向主进程发送信号，重新加载配置文件，热重启nginx -s reopen  # 重启 Nginxnginx -s stop    # 快速关闭nginx -s quit    # 等待工作进程处理完成后关闭nginx -T         # 查看当前 Nginx 最终的配置nginx -t -c <配置路径>  # 检查配置是否有问题，如果已经在配置目录，则不需要 -c
```

以上命令通过 `nginx -h` 就可以查看到，还有其它不常用这里未列出。

Linux 系统应用管理工具 systemd 关于 nginx 的常用命令：

```
systemctl start nginx    # 启动 Nginxsystemctl stop nginx     # 停止 Nginxsystemctl restart nginx  # 重启 Nginxsystemctl reload nginx   # 重新加载 Nginx，用于修改配置后systemctl enable nginx   # 设置开机启动 Nginxsystemctl disable nginx  # 关闭开机启动 Nginxsystemctl status nginx   # 查看 Nginx 运行状态
```

### 2.6 配置 nginx 开机自启

**利用 systemctl 命令**：

如果用 yum install 命令安装的 nginx，yum 命令会自动创建 nginx.service 文件，直接用命令:

```
systemctl enable nginx   # 设置开机启动 Nginxsystemctl disable nginx  # 关闭开机启动 Nginx
```

就可以设置开机自启，否则需要在系统服务目录里创建 nginx.service 文件。

创建并打开 nginx.service 文件：

```
vi /lib/systemd/system/nginx.service
```

内容如下：

```
[Unit]Description=nginxAfter=network.target[Service]Type=forkingExecStart=/usr/local/nginx/sbin/nginxExecReload=/usr/local/nginx/sbin/nginx -s reloadExecStop=/usr/local/nginx/sbin/nginx -s quitPrivateTmp=true[Install]WantedBy=multi-user.target
```

`:wq` 保存退出，运行 `systemctl daemon-reload` 使文件生效。

这样便可以通过以下命令操作 nginx 了：

```
systemctl start nginx.service # 启动nginx服务systemctl enable nginx.service # 设置开机启动systemctl disable nginx.service # 停止开机自启动systemctl status nginx.service # 查看服务当前状态systemctl restart nginx.service # 重新启动服务systemctl is-enabled nginx.service #查询服务是否开机启动
```

**通过开机启动命令脚本实现开机自启**

创建开机启动命令脚本文件：

```
vi /etc/init.d/nginx
```

在这个 nginx 文件中插入一下启动脚本代码，启动脚本代码来源网络复制，实测有效：

```
#! /bin/bash# chkconfig: - 85 15PATH=/usr/local/nginxDESC="nginx daemon"NAME=nginxDAEMON=$PATH/sbin/$NAMECONFIGFILE=$PATH/conf/$NAME.confPIDFILE=$PATH/logs/$NAME.pidscriptNAME=/etc/init.d/$NAMEset -e[ -x "$DAEMON" ] || exit 0do_start() {$DAEMON -c $CONFIGFILE || echo -n "nginx already running"}do_stop() {$DAEMON -s stop || echo -n "nginx not running"}do_reload() {$DAEMON -s reload || echo -n "nginx can't reload"}case "$1" instart)echo -n "Starting $DESC: $NAME"do_startecho ".";;stop)echo -n "Stopping $DESC: $NAME"do_stopecho ".";;reload|graceful)echo -n "Reloading $DESC configuration..."do_reloadecho ".";;restart)echo -n "Restarting $DESC: $NAME"do_stopdo_startecho ".";;*)echo "Usage: $scriptNAME {start|stop|reload|restart}" >&2exit 3;;esacexit 0
```

设置所有人都有对这个启动脚本 nginx 文件的执行权限：

```
chmod a+x /etc/init.d/nginx
```

把 nginx 加入系统服务中：

```
chkconfig --add nginx
```

把服务设置为开机启动：

```
chkconfig nginx on
```

reboot 重启系统生效，可以使用上面 systemctl 方法相同的命令：

```
systemctl start nginx.service # 启动nginx服务systemctl enable nginx.service # 设置开机启动systemctl disable nginx.service # 停止开机自启动systemctl status nginx.service # 查看服务当前状态systemctl restart nginx.service # 重新启动服务systemctl is-enabled nginx.service #查询服务是否开机启动
```

如果服务启动的时候出现 `Restarting nginx daemon: nginxnginx: [error] open() "/usr/local/nginx/logs/nginx.pid" failed (2: No such file or directory) nginx not running` 的错误，通过 nginx -c 参数指定配置文件即可解决

```
/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf
```

如果服务启动中出现 `nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)` 的错误，可以先通过 `service nginx stop` 停止服务，再启动就好。

### 2.7 配置 nginx 全局可用

当你每次改了 `nginx.conf` 配置文件的内容都需要重新到 nginx 启动目录去执行命令，或者通过 -p 参数指向特定目录，会不会感觉很麻烦？

例如：直接执行 `nginx -s reload` 会报错 `-bash: nginx: command not found`，需要到 `/usr/local/nginx/sbin` 目录下面去执行，并且是执行 `./nginx -s reload`。

这里有两种方式可以解决，一种是通过脚本对 nginx 命令包装，这里介绍另外一种比较简单：通过把 nginx 配置到环境变量里，用 nginx 执行指令即可。步骤如下：

1、编辑 /etc/profile

```
vi /etc/profile
```

2、在最后一行添加配置，:wq 保存

```
export PATH=$PATH:/usr/local/nginx/sbin
```

3、使配置立即生效

```
source /etc/profile
```

这样就可以愉快的直接在全局使用 nginx 命令了。

## 3.**nginx 常用功能**

### 3.1 反向代理

我们最常说的反向代理的是通过反向代理解决跨域问题。

其实反向代理还可以用来控制缓存（代理缓存 proxy cache），进行访问控制等等，以及后面说的负载均衡其实都是通过反向代理来实现的。

```
server {    listen    8080;        # 用户访问 ip:8080/test 下的所有路径代理到 github        location /test {         proxy_pass   https://github.com;        }        # 所有 /api 下的接口访问都代理到本地的 8888 端口        # 例如你本地运行的 java 服务的端口是 8888，接口都是以 /api 开头        location /api {            proxy_pass   http://127.0.0.1:8888;        }}
```

### 3.2 访问控制

```
server {   location ~ ^/index.html {       # 匹配 index.html 页面 除了 127.0.0.1 以外都可以访问       deny 192.168.1.1;       deny 192.168.1.2;       allow all; }}
```

上面的命令表示禁止 192.168.1.1 和 192.168.1.2 两个 ip 访问，其它全部允许。从上到下的顺序，匹配到了便跳出，可以按你的需求设置。

### 3.3 负载均衡

通过负载均衡充利用服务器资源，nginx 目前支持自带 4 种负载均衡策略，还有 2 种常用的第三方策略。

**轮询策略（默认）**

每个请求按时间顺序逐一分配到不同的后端服务器，如果有后端服务器挂掉，能自动剔除。但是如果其中某一台服务器压力太大，出现延迟，会影响所有分配在这台服务器下的用户。

```
http {    upstream test.com {        server 192.168.1.12:8887;        server 192.168.1.13:8888;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

**根据服务器权重**

例如要配置：10 次请求中大概 1 次访问到 8888 端口，9 次访问到 8887 端口：

```
http {    upstream test.com {        server 192.168.1.12:8887 weight=9;        server 192.168.1.13:8888 weight=1;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

**客户端 ip 绑定（ip_hash）**

来自同一个 ip 的请求永远只分配一台服务器，有效解决了动态网页存在的 session 共享问题。例如：比如把登录信息保存到了 session 中，那么跳转到另外一台服务器的时候就需要重新登录了。

所以很多时候我们需要一个客户只访问一个服务器，那么就需要用 ip_hash 了。

```
http {    upstream test.com {     ip_hash;        server 192.168.1.12:8887;        server 192.168.1.13:8888;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

**最小连接数策略**

将请求优先分配给压力较小的服务器，它可以平衡每个队列的长度，并避免向压力大的服务器添加更多的请求。

```
http {    upstream test.com {     least_conn;        server 192.168.1.12:8887;        server 192.168.1.13:8888;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

**最快响应时间策略（依赖于第三方 NGINX Plus）**

依赖于 NGINX Plus，优先分配给响应时间最短的服务器。

```
http {    upstream test.com {     fair;        server 192.168.1.12:8887;        server 192.168.1.13:8888;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

**按访问 url 的 hash 结果（第三方）**

按访问 url 的 hash 结果来分配请求，使每个 url 定向到同一个后端服务器，后端服务器为缓存时比较有效。在 upstream 中加入 hash 语句，server 语句中不能写入 weight 等其他的参数，hash_method 是使用的 hash 算法

```
http {    upstream test.com {     hash $request_uri;     hash_method crc32;     server 192.168.1.12:8887;     server 192.168.1.13:8888;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

采用 HAproxy 的 loadbalance uri 或者 nginx 的 upstream_hash 模块，都可以做到针对 url 进行哈希算法式的负载均衡转发。

### 3.4 gzip 压缩

开启 gzip 压缩可以大幅减少 http 传输过程中文件的大小，可以极大的提高网站的访问速度，基本是必不可少的优化操作：

```
gzip  on; # 开启gzip 压缩# gzip_types# gzip_static on;# gzip_proxied expired no-cache no-store private auth;# gzip_buffers 16 8k;gzip_min_length 1k;gzip_comp_level 4;gzip_http_version 1.0;gzip_vary off;gzip_disable "MSIE [1-6]\.";
```

解释一下：

1. gzip_types：要采用 gzip 压缩的 MIME 文件类型，其中 text/html 被系统强制启用；
2. gzip_static：默认 off，该模块启用后，Nginx 首先检查是否存在请求静态文件的 gz 结尾的文件，如果有则直接返回该 .gz 文件内容；
3. gzip_proxied：默认 off，nginx 做为反向代理时启用，用于设置启用或禁用从代理服务器上收到相应内容 gzip 压缩；
4. gzip_buffers：获取多少内存用于缓存压缩结果，16 8k 表示以 8k*16 为单位获得；
5. gzip_min_length：允许压缩的页面最小字节数，页面字节数从 header 头中的 Content-Length 中进行获取。默认值是 0，不管页面多大都压缩。建议设置成大于 1k 的字节数，小于 1k 可能会越压越大；
6. gzip_comp_level：gzip 压缩比，压缩级别是 1-9，1 压缩级别最低，9 最高，级别越高压缩率越大，压缩时间越长，建议 4-6；
7. gzip_http_version：默认 1.1，启用 gzip 所需的 HTTP 最低版本；
8. gzip_vary：用于在响应消息头中添加 Vary：Accept-Encoding，使代理服务器根据请求头中的 Accept-Encoding 识别是否启用 gzip 压缩；
9. gzip_disable 指定哪些不需要 gzip 压缩的浏览器

其中第 2 点，普遍是结合前端打包的时候打包成 gzip 文件后部署到服务器上，这样服务器就可以直接使用 gzip 的文件了，并且可以把压缩比例提高，这样 nginx 就不用压缩，也就不会影响速度。一般不追求极致的情况下，前端不用做任何配置就可以使用啦~

附前端 webpack 开启 gzip 压缩配置，在 vue-cli3 的 vue.config.js 配置文件中：

```
const CompressionWebpackPlugin = require('compression-webpack-plugin')module.exports = {  // gzip 配置  configureWebpack: config => {    if (process.env.NODE_ENV === 'production') {      // 生产环境      return {        plugins: [new CompressionWebpackPlugin({          test: /\.js$|\.html$|\.css/,    // 匹配文件名          threshold: 1024,               // 文件压缩阈值，对超过 1k 的进行压缩          deleteOriginalAssets: false     // 是否删除源文件        })]      }    }  },  ...}
```

### 3.5 HTTP 服务器

nginx 本身也是一个静态资源的服务器，当只有静态资源的时候，就可以使用 nginx 来做服务器：

```
server {  listen       80;  server_name  localhost;  location / {      root   /usr/local/app;      index  index.html;  }}
```

这样如果访问 [http://ip](http://ip/) 就会默认访问到 /usr/local/app 目录下面的 index.html，如果一个网站只是静态页面的话，那么就可以通过这种方式来实现部署，比如一个静态官网。

### 3.6 动静分离

就是把动态和静态的请求分开。方式主要有两种：

- 一种是纯粹把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案
- 一种方法就是动态跟静态文件混合在一起发布， 通过 nginx 配置来分开

```
# 所有静态请求都由nginx处理，存放目录为 htmllocation ~ \.(gif|jpg|jpeg|png|bmp|swf|css|js)$ {    root    /usr/local/resource;    expires     10h; # 设置过期时间为10小时}# 所有动态请求都转发给 tomcat 处理location ~ \.(jsp|do)$ {    proxy_pass  127.0.0.1:8888;}
```

注意上面设置了 expires，当 nginx 设置了 expires 后，例如设置为：expires 10d; 那么，所在的 location 或 if 的内容，用户在 10 天内请求的时候，都只会访问浏览器中的缓存，而不会去请求 nginx 。

### 3.7 请求限制

对于大流量恶意的访问，会造成带宽的浪费，给服务器增加压力。可以通过 nginx 对于同一 IP 的连接数以及并发数进行限制。合理的控制还可以用来防止 DDos 和 CC 攻击。

关于请求限制主要使用 nginx 默认集成的 2 个模块：

- limit_conn_module 连接频率限制模块
- limit_req_module 请求频率限制模块

涉及到的配置主要是：

- limit_req_zone 限制请求数
- limit_conn_zone 限制并发连接数

**通过 limit_req_zone 限制请求数**

```
http{    limit_conn_zone $binary_remote_addrzone=limit:10m; // 设置共享内存空间大    server{     location /{            limit_conn addr 5; # 同一用户地址同一时间只允许有5个连接。        }    }}
```

如果共享内存空间被耗尽，服务器将会对后续所有的请求返回 503 (Service Temporarily Unavailable) 错误。

当多个 limit_conn_zone 指令被配置时，所有的连接数限制都会生效。比如，下面配置不仅会限制单一 IP 来源的连接数，同时也会限制单一虚拟服务器的总连接数：

```
limit_conn_zone $binary_remote_addr zone=perip:10m;limit_conn_zone $server_name zone=perserver:10m;server {    limit_conn perip 10; # 限制每个 ip 连接到服务器的数量    limit_conn perserver 2000; # 限制连接到服务器的总数}
```

**通过 limit_conn_zone 限制并发连接数**

```
limit_req_zone $binary_remote_addr zone=creq:10 mrate=10r/s;server{    location /{        limit_req zone=creq burst=5;    }}
```

限制平均每秒不超过一个请求，同时允许超过频率限制的请求数不多于 5 个。如果不希望超过的请求被延迟，可以用 nodelay 参数,如：

```
limit_req zone=creq burst=5 nodelay;
```

这里只是简单讲讲，让大家有这个概念，配置的时候可以深入去找找资料。

### 3.8 正向代理

正向代理，意思是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理，比如我们使用的 VPN 服务就是正向代理，直观区别：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151538333925963.png)

配置正向代理：

```
resolver 8.8.8.8 # 谷歌的域名解析地址server {    resolver_timeout 5s; // 设超时时间    location / {        # 当客户端请求我的时候，我会把请求转发给它        # $host 要访问的主机名 $request_uri 请求路径        proxy_pass http://$host$request_uri;    }}
```

正向代理的对象是客户端，服务器端看不到真正的客户端。

### 3.9 图片防盗链

```
server {    listen       80;    server_name  *.test;    # 图片防盗链    location ~* \.(gif|jpg|jpeg|png|bmp|swf)$ {        valid_referers none blocked server_names ~\.google\. ~\.baidu\. *.qq.com;  # 只允许本机 IP 外链引用，将百度和谷歌也加入白名单有利于 SEO        if ($invalid_referer){            return 403;        }    }}
```

以上设置就能防止其它网站利用外链访问我们的图片，有利于节省流量

### 3.10 适配 PC 或移动设备

根据用户设备不同返回不同样式的站点，以前经常使用的是纯前端的自适应布局，但是复杂的网站并不适合响应式，无论是复杂性和易用性上面还是不如分开编写的好，比如我们常见的淘宝、京东。

根据用户请求的 user-agent 来判断是返回 PC 还是 H5 站点：

```
server {    listen 80;    server_name test.com;    location / {     root  /usr/local/app/pc; # pc 的 html 路径        if ($http_user_agent ~* '(Android|webOS|iPhone|iPod|BlackBerry)') {            root /usr/local/app/mobile; # mobile 的 html 路径        }        index index.html;    }}
```

### 3.11 设置二级域名

新建一个 server 即可：

```
server {    listen 80;    server_name admin.test.com; // 二级域名    location / {        root  /usr/local/app/admin; # 二级域名的 html 路径        index index.html;    }}
```

### 3.12 配置 HTTPS

这里我使用的是 certbot 免费证书，但申请一次有效期只有 3 个月（好像可以用 crontab 尝试配置自动续期，我暂时没试过）：

先安装 certbot

```
wget https://dl.eff.org/certbot-autochmod a+x certbot-auto
```

申请证书（注意：需要把要申请证书的域名先解析到这台服务器上，才能申请）:

```
sudo ./certbot-auto certonly --standalone --email admin@abc.com -d test.com -d www.test.com
```

执行上面指令，按提示操作。

Certbot 会启动一个临时服务器来完成验证（会占用 80 端口或 443 端口，因此需要暂时关闭 Web 服务器），然后 Certbot 会把证书以文件的形式保存，包括完整的证书链文件和私钥文件。

文件保存在 /etc/letsencrypt/live/ 下面的域名目录下。

修改 nginx 配置：

```
server{    listen 443 ssl http2; // 这里还启用了 http/2.0    ssl_certificate /etc/letsencrypt/live/test.com/fullchain.pem; # 证书文件地址    ssl_certificate_key /etc/letsencrypt/live/test.com/privkey.pem; # 私钥文件地址    server_name test.com www.test.com; // 证书绑定的域名}
```

### 3.13 配置 HTTP 转 HTTPS

```
server {    listen      80;    server_name test.com www.test.com;    # 单域名重定向    if ($host = 'www.sherlocked93.club'){        return 301 https://www.sherlocked93.club$request_uri;    }    # 全局非 https 协议时重定向    if ($scheme != 'https') {        return 301 https://$server_name$request_uri;    }    # 或者全部重定向    return 301 https://$server_name$request_uri;}
```

以上配置选择自己需要的一条即可，不用全部加。

### 3.14 单页面项目 history 路由配置

```
server {    listen       80;    server_name  fe.sherlocked93.club;    location / {        root       /usr/local/app/dist;  # vue 打包后的文件夹        index      index.html index.htm;        try_files  $uri $uri/ /index.html @rewrites; # 默认目录下的 index.html，如果都不存在则重定向        expires -1;                          # 首页一般没有强制缓存        add_header Cache-Control no-cache;    }    location @rewrites { // 重定向设置        rewrite ^(.+)$ /index.html break;    }}
```

[vue-router](https://router.vuejs.org/zh/guide/essentials/history-mode.html#后端配置例子) 官网只有一句话 `try_files $uri $uri/ /index.html;`，而上面做了一些重定向处理。

### 3.15 配置高可用集群（双机热备）

当主 nginx 服务器宕机之后，切换到备份的 nginx 服务器

首先安装 keepalived:

```
yum install keepalived -y
```

然后编辑 `/etc/keepalived/keepalived.conf` 配置文件，并在配置文件中增加 `vrrp_script` 定义一个外围检测机制，并在 `vrrp_instance` 中通过定义 `track_script` 来追踪脚本执行过程，实现节点转移：

```
global_defs{   notification_email {        cchroot@gmail.com   }   notification_email_from test@firewall.loc   smtp_server 127.0.0.1   smtp_connect_timeout 30 // 上面都是邮件配置   router_id LVS_DEVEL     // 当前服务器名字，用 hostname 命令来查看}vrrp_script chk_maintainace { // 检测机制的脚本名称为chk_maintainace    script "[[ -e/etc/keepalived/down ]] && exit 1 || exit 0" // 可以是脚本路径或脚本命令    // script "/etc/keepalived/nginx_check.sh"    // 比如这样的脚本路径    interval 2  // 每隔2秒检测一次    weight -20  // 当脚本执行成立，那么把当前服务器优先级改为-20}vrrp_instanceVI_1 {   // 每一个vrrp_instance就是定义一个虚拟路由器    state MASTER      // 主机为MASTER，备用机为BACKUP    interface eth0    // 网卡名字，可以从ifconfig中查找    virtual_router_id 51 // 虚拟路由的id号，一般小于255，主备机id需要一样    priority 100      // 优先级，master的优先级比backup的大    advert_int 1      // 默认心跳间隔    authentication {  // 认证机制        auth_type PASS        auth_pass 1111   // 密码    }    virtual_ipaddress {  // 虚拟地址vip       172.16.2.8    }}
```

其中检测脚本 `nginx_check.sh`，这里提供一个：

```
#!/bin/bashA=`ps -C nginx --no-header | wc -l`if [ $A -eq 0 ];then    /usr/sbin/nginx # 尝试重新启动nginx    sleep 2         # 睡眠2秒    if [ `ps -C nginx --no-header | wc -l` -eq 0 ];then        killall keepalived # 启动失败，将keepalived服务杀死。将vip漂移到其它备份节点    fifi
```

复制一份到备份服务器，备份 nginx 的配置要将 `state` 后改为 `BACKUP`，`priority` 改为比主机小。设置完毕后各自 `service keepalived start` 启动，经过访问成功之后，可以把 Master 机的 keepalived 停掉，此时 Master 机就不再是主机了 `service keepalived stop`，看访问虚拟 IP 时是否能够自动切换到备机 ip addr。

再次启动 Master 的 keepalived，此时 vip 又变到了主机上。

配置高可用集群的内容来源于：[Nginx 从入门到实践，万字详解！](https://juejin.im/post/6844904144235413512#heading-11)

## 4. **其它功能和技巧**

### 4.1 代理缓存

nginx 的 http_proxy 模块，提供类似于 Squid 的缓存功能，使用 proxy_cache_path 来配置。

nginx 可以对访问过的内容在 nginx 服务器本地建立副本，这样在一段时间内再次访问该数据，就不需要通过 nginx 服务器再次向后端服务器发出请求，减小数据传输延迟，提高访问速度：

```
proxy_cache_path usr/local/cache levels=1:2 keys_zone=my_cache:10m;server {  listen       80;  server_name  test.com;  location / {      proxy_cache my_cache;      proxy_pass http://127.0.0.1:8888;      proxy_set_header Host $host;  }}
```

上面的配置表示：nginx 提供一块 10 M 的内存用于缓存，名字为 my_cache, levels 等级为 1:2，缓存存放的路径为 `usr/local/cache`。

### 4.2 访问日志

访问日志默认是注释的状态，需要可以打开和进行更详细的配置，一下是 nginx 的默认配置：

```
http {    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '                      '$status $body_bytes_sent "$http_referer" '                      '"$http_user_agent" "$http_x_forwarded_for"';    access_log  logs/access.log  main;}
```

### 4.3 错误日志

错误日志放在 main 全局区块中，童鞋们打开 nginx.conf 就可以看见在配置文件中和下面一样的代码了：

```
#error_log  logs/error.log;#error_log  logs/error.log  notice;#error_log  logs/error.log  info;
```

nginx 错误日志默认配置为：

```
error_log logs/error.log error;
```

### 4.4 静态资源服务器

```
server {    listen       80;    server_name  static.bin;    charset utf-8;    # 防止中文文件名乱码    location /download {        alias           /usr/share/nginx/static;  # 静态资源目录        autoindex               on;    # 开启静态资源列目录，浏览目录权限        autoindex_exact_size    off;   # on(默认)显示文件的确切大小，单位是byte；off显示文件大概大小，单位KB、MB、GB        autoindex_localtime     off;   # off(默认)时显示的文件时间为GMT时间；on显示的文件时间为服务器时间    }}
```

### 4.5 禁止指定 user_agent

nginx 可以禁止指定的浏览器和爬虫框架访问：

```
# http_user_agent 为浏览器标识# 禁止 user_agent 为baidu、360和sohu，~*表示不区分大小写匹配if ($http_user_agent ~* 'baidu|360|sohu') {    return 404;}# 禁止 Scrapy 等工具的抓取if ($http_user_agent ~* (Scrapy|Curl|HttpClient)) {    return 403;
```

### 4.6 请求过滤

**根据请求类型过滤**

```
# 非指定请求全返回 403if ( $request_method !~ ^(GET|POST|HEAD)$ ) {    return 403;}
```

**根据状态码过滤**

```
error_page 502 503 /50x.html;location = /50x.html {    root /usr/share/nginx/html;}
```

这样实际上是一个内部跳转，当访问出现 502、503 的时候就能返回 50x.html 中的内容，这里需要注意是否可以找到 50x.html 页面，所以加了个 location 保证找到你自定义的 50x 页面。

**根据 URL 名称过滤**

```
if ($host = zy.com' ) {     #其中 $1是取自regex部分()里的内容,匹配成功后跳转到的URL。     rewrite ^/(.*)$  http://www.zy.com/$1  permanent；}location /test {    // /test 全部重定向到首页    rewrite  ^(.*)$ /index.html  redirect;}
```

### 4.7 ab 命令

ab 命令全称为：Apache bench，是 Apache 自带的压力测试工具，也可以测试 Nginx、IIS 等其他 Web 服务器:

- -n 总共的请求数
- -c 并发的请求数
- -t 测试所进行的最大秒数，默认值 为 50000
- -p 包含了需要的 POST 的数据文件
- -T POST 数据所使用的 Content-type 头信息

```
ab -n 1000 -c 5000 http://127.0.0.1/ # 每次发送1000并发的请求数，请求数总数为5000。
```

测试前需要安装 httpd-tools：`yum install httpd-tools`

### 4.8 泛域名路径分离

这是一个非常实用的技能，经常有时候我们可能需要配置一些二级或者三级域名，希望通过 nginx 自动指向对应目录，比如：

1. test1.doc.test.club 自动指向 /usr/local/html/doc/test1 服务器地址；
2. test2.doc.test.club 自动指向 /usr/local/html/doc/test2 服务器地址；

```
server {    listen       80;    server_name  ~^([\w-]+)\.doc\.test\.club$;    root /usr/local/html/doc/$1;}
```

### 4.9 泛域名转发

和之前的功能类似，有时候我们希望把二级或者三级域名链接重写到我们希望的路径，让后端就可以根据路由解析不同的规则：

1. test1.serv.test.club/api?name=a 自动转发到 127.0.0.1:8080/test1/api?name=a
2. test2.serv.test.club/api?name=a 自动转发到 127.0.0.1:8080/test2/api?name=a

```
server {    listen       80;    server_name ~^([\w-]+)\.serv\.test\.club$;    location / {        proxy_set_header        X-Real-IP $remote_addr;        proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;        proxy_set_header        Host $http_host;        proxy_set_header        X-NginX-Proxy true;        proxy_pass              http://127.0.0.1:8080/$1$request_uri;    }}
```

## 5.**常见问题**

### 5.1 nginx 中怎么设置变量

或许你不知道，nginx 的配置文件使用的是一门微型的编程语言。既然是编程语言，一般也就少不了“变量”这种东西，但是在 nginx 配置中，变量只能存放一种类型的值，因为也只存在一种类型的值，那就是字符串。

例如我们在 nginx.conf 中有这样一行配置：

```
set $name "chroot";
```

上面使用了 set 配置指令对变量 `$name`进行了赋值操作，把 “chroot” 赋值给了 `$name`。nginx 变量名前面有一个 `$` 符号，这是记法上的要求。所有的 Nginx 变量在 Nginx 配置文件中引用时都须带上 `$` 前缀。这种表示方法和 Perl、PHP 这些语言是相似的。

这种表示方法的用处在哪里呢，那就是可以直接把变量嵌入到字符串常量中以构造出新的字符串，例如你需要进行一个字符串拼接：

```
server {  listen       80;  server_name  test.com;  location / {     set $temp hello;     return "$temp world";  }}
```

以上当匹配成功的时候就会返回字符串 “hello world” 了。需要注意的是，当引用的变量名之后紧跟着变量名的构成字符时（比如后跟字母、数字以及下划线），我们就需要使用特别的记法来消除歧义，例如：

```
server {  listen       80;  server_name  test.com;  location / {     set $temp "hello ";     return "${temp}world";  }}
```

这里，我们在配置指令的参数值中引用变量 `$temp` 的时候，后面紧跟着 `world` 这个单词，所以如果直接写作 `"$tempworld"` 则 nginx 的计算引擎会将之识别为引用了变量 `$tempworld`. 为了解决这个问题，nginx 的字符串支持使用花括号在 `$` 之后把变量名围起来，比如这里的 `${temp}`，所以 上面这个例子返回的还是 “hello world”：

```
$ curl 'http://test.com/'    hello world
```

还需要注意的是，若是想输出 `$` 符号本身，可以这样做：

```
geo $dollar {    default "$";}server {    listen       80;    server_name  test.com;    location / {        set $temp "hello ";        return "${temp}world: $dollar";    }}
```

上面用到了标准模块 ngx_geo 提供的配置指令 geo 来为变量 `$dollar` 赋予字符串 `"$"` ，这样，这里的返回值就是 “hello world: $” 了。

## 6.**附 nginx 内置预定义变量**

按字母顺序，变量名与对应定义：

- `$arg_PARAMETER` #GET 请求中变量名 PARAMETER 参数的值
- `$args` #这个变量等于 GET 请求中的参数，例如，foo=123&bar=blahblah;这个变量可以被修改
- `$binary_remote_addr` #二进制码形式的客户端地址
- `$body_bytes_sent` #传送页面的字节数
- `$content_length` #请求头中的 Content-length 字段
- `$content_type` #请求头中的 Content-Type 字段
- `$cookie_COOKIE` #cookie COOKIE 的值
- `$document_root` #当前请求在 root 指令中指定的值
- `$document_uri` #与 $uri 相同
- `$host` #请求中的主机头(Host)字段，如果请求中的主机头不可用或者空，则为处理请求的 server 名称(处理请求的 server 的 server_name 指令的值)。值为小写，不包含端口
- `$hostname` #机器名使用 gethostname 系统调用的值
- `$http_HEADER` #HTTP 请求头中的内容，HEADER 为 HTTP 请求中的内容转为小写，-变为*(破折号变为下划线)，例如：`$http*user_agent`(Uaer-Agent 的值)
- `$sent_http_HEADER` #HTTP 响应头中的内容，HEADER 为 HTTP 响应中的内容转为小写，-变为*(破折号变为下划线)，例如：`$sent*http_cache_control`、`$sent_http_content_type`…
- `$is_args` #如果 $args 设置，值为”?”，否则为””
- `$limit_rate` #这个变量可以限制连接速率
- `$nginx_version` #当前运行的 nginx 版本号
- `$query_string` #与 $args 相同
- `$remote_addr` #客户端的 IP 地址
- `$remote_port` #客户端的端口
- `$remote_port` #已经经过 Auth Basic Module 验证的用户名
- `$request_filename` #当前连接请求的文件路径，由 root 或 alias 指令与 URI 请求生成
- `$request_body` #这个变量（0.7.58+）包含请求的主要信息。在使用 proxy_pass 或 fastcgi_pass 指令的 location 中比较有意义
- `$request_body_file` #客户端请求主体信息的临时文件名
- `$request_completion` #如果请求成功，设为”OK”；如果请求未完成或者不是一系列请求中最后一部分则设为空
- `$request_method` #这个变量是客户端请求的动作，通常为 GET 或 POST。包括 0.8.20 及之前的版本中，这个变量总为 main request 中的动作，如果当前请求是一个子请求，并不使用这个当前请求的动作
- `$request_uri` #这个变量等于包含一些客户端请求参数的原始 URI，它无法修改，请查看 $uri 更改或重写 URI
- `$scheme` #所用的协议，例如 http 或者是 https，例如 `rewrite ^(.+)$$scheme://example.com$1 redirect`
- `$server_addr` #服务器地址，在完成一次系统调用后可以确定这个值，如果要绕开系统调用，则必须在 listen 中指定地址并且使用 bind 参数
- `$server_name` #服务器名称
- `$server_port` #请求到达服务器的端口号
- `$server_protocol` #请求使用的协议，通常是 HTTP/1.0、HTTP/1.1 或 HTTP/2
- `$uri` #请求中的当前 URI(不带请求参数，参数位于 args ) ， 不 同 于 浏 览 器 传 递 的 args)，不同于浏览器传递的 args)，不同于浏览器传递的 request_uri 的值，它可以通过内部重定向，或者使用 index 指令进行修改。不包括协议和主机名，例如 /foo/bar.html

## 7.**附 nginx 模块**

### 7.1 nginx 模块分类

- 核心模块：nginx 最基本最核心的服务，如进程管理、权限控制、日志记录；
- 标准 HTTP 模块：nginx 服务器的标准 HTTP 功能；
- 可选 HTTP 模块：处理特殊的 HTTP 请求
- 邮件服务模块：邮件服务
- 第三方模块：作为扩展，完成特殊功能

### 7.2 模块清单

**核心模块**：

- ngx_core
- ngx_errlog
- ngx_conf
- ngx_events
- ngx_event_core
- ngx_epll
- ngx_regex

**标准 HTTP 模块**：

- ngx_http
- ngx_http_core #配置端口，URI 分析，服务器相应错误处理，别名控制 (alias) 等
- ngx_http_log #自定义 access 日志
- ngx_http_upstream #定义一组服务器，可以接受来自 proxy, Fastcgi,Memcache 的重定向；主要用作负载均衡
- ngx_http_static
- ngx_http_autoindex #自动生成目录列表
- ngx_http_index #处理以/结尾的请求，如果没有找到 index 页，则看是否开启了 random_index；如开启，则用之，否则用 autoindex
- ngx_http_auth_basic #基于 http 的身份认证 (auth_basic)
- ngx_http_access #基于 IP 地址的访问控制 (deny,allow)
- ngx_http_limit_conn #限制来自客户端的连接的响应和处理速率
- ngx_http_limit_req #限制来自客户端的请求的响应和处理速率
- ngx_http_geo
- ngx_http_map #创建任意的键值对变量
- ngx_http_split_clients
- ngx_http_referer #过滤 HTTP 头中 Referer 为空的对象
- ngx_http_rewrite #通过正则表达式重定向请求
- ngx_http_proxy
- ngx_http_fastcgi #支持 fastcgi
- ngx_http_uwsgi
- ngx_http_scgi
- ngx_http_memcached
- ngx_http_empty_gif #从内存创建一个 1×1 的透明 gif 图片，可以快速调用
- ngx_http_browser #解析 http 请求头部的 User-Agent 值
- ngx_http_charset #指定网页编码
- ngx_http_upstream_ip_hash
- ngx_http_upstream_least_conn
- ngx_http_upstream_keepalive
- ngx_http_write_filter
- ngx_http_header_filter
- ngx_http_chunked_filter
- ngx_http_range_header
- ngx_http_gzip_filter
- ngx_http_postpone_filter
- ngx_http_ssi_filter
- ngx_http_charset_filter
- ngx_http_userid_filter
- ngx_http_headers_filter #设置 http 响应头
- ngx_http_copy_filter
- ngx_http_range_body_filter
- ngx_http_not_modified_filter

**可选 HTTP 模块**:

- ngx_http_addition #在响应请求的页面开始或者结尾添加文本信息
- ngx_http_degradation #在低内存的情况下允许服务器返回 444 或者 204 错误
- ngx_http_perl
- ngx_http_flv #支持将 Flash 多媒体信息按照流文件传输，可以根据客户端指定的开始位置返回 Flash
- ngx_http_geoip #支持解析基于 GeoIP 数据库的客户端请求
- ngx_google_perftools
- ngx_http_gzip #gzip 压缩请求的响应
- ngx_http_gzip_static #搜索并使用预压缩的以.gz 为后缀的文件代替一般文件响应客户端请求
- ngx_http_image_filter #支持改变 png，jpeg，gif 图片的尺寸和旋转方向
- ngx_http_mp4 #支持.mp4,.m4v,.m4a 等多媒体信息按照流文件传输，常与 ngx_http_flv 一起使用
- ngx_http_random_index #当收到 / 结尾的请求时，在指定目录下随机选择一个文件作为 index
- ngx_http_secure_link #支持对请求链接的有效性检查
- ngx_http_ssl #支持 https
- ngx_http_stub_status
- ngx_http_sub_module #使用指定的字符串替换响应中的信息
- ngx_http_dav #支持 HTTP 和 WebDAV 协议中的 PUT/DELETE/MKCOL/COPY/MOVE 方法
- ngx_http_xslt #将 XML 响应信息使用 XSLT 进行转换

**邮件服务模块**:

- ngx_mail_core
- ngx_mail_pop3
- ngx_mail_imap
- ngx_mail_smtp
- ngx_mail_auth_http
- ngx_mail_proxy
- ngx_mail_ssl

**第三方模块**：

- echo-nginx-module #支持在 nginx 配置文件中使用 echo/sleep/time/exec 等类 Shell 命令
- memc-nginx-module
- rds-json-nginx-module #使 nginx 支持 json 数据的处理
- lua-nginx-module

原文作者：chrootliu，腾讯 QQ 音乐前端开发工程师

原文链接：https://mp.weixin.qq.com/s/LmtHTOVOvdcnMBuxv7a9_A
