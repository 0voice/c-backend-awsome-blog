![image-20221226222234483](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20221226222234483.png)

文章素材来源于微信公众号/知乎/csdn/头条/掘金/51cto/博客园等技术网站的优秀文章，版权属于原创作者（文章标注以及原文链接）
如有侵权，联系wangbojing@0voice.com 或留下的qq群联系，第一时间删除。由衷感谢各位优秀文章的作者，在互联网分享自己思想

# 【NO.91】带你快速了解 Docker 和 Kubernetes

> 从单机容器化技术 Docker 到分布式容器化架构方案 Kubernetes，当今容器化技术发展盛行。本文面向小白读者，旨在快速带领读者了解 Docker、Kubernetes 的架构、原理、组件及相关使用场景。

## 1.**Docker**

### 1.1 什么是 Docker

Docker 是一个开源的应用容器引擎，是一种资源虚拟化技术，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上。虚拟化技术演历路径可分为三个时代：

1. 物理机时代，多个应用程序可能跑在一台物理机器上
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151450107586854.png)
2. 虚拟机时代，一台物理机器启动多个虚拟机实例，一个虚拟机跑多个应用程序
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151450403594316.png)
3. 容器化时代，一台物理机上启动多个容器实例，一个容器跑多个应用程序
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151450571828054.png)

在没有 Docker 的时代，我们会使用硬件虚拟化（虚拟机）以提供隔离。这里，虚拟机通过在操作系统上建立了一个中间虚拟软件层 Hypervisor ，并利用物理机器的资源虚拟出多个虚拟硬件环境来共享宿主机的资源，其中的应用运行在虚拟机内核上。但是，虚拟机对硬件的利用率存在瓶颈，因为虚拟机很难根据当前业务量动态调整其占用的硬件资源，加之容器化技术蓬勃发展使其得以流行。

Docker、虚拟机对比：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151451137564461.png)

另外开发人员在实际的工作中，经常会遇到测试环境或生产环境与本地开发环境不一致的问题，轻则修复保持环境一致，重则可能需要返工。但 Docker 恰好解决了这一问题，它将软件程序和运行的基础环境分开。开发人员编码完成后将程序整合环境通过 DockerFile 打包到一个容器镜像中，从根本上解决了环境不一致的问题。

### 1.2 Docker 的构成

Docker 由镜像、镜像仓库、容器三个部分组成

- 镜像: 跨平台、可移植的程序+环境包
- 镜像仓库: 镜像的存储位置，有云端仓库和本地仓库之分，官方镜像仓库地址（https://hub.docker.com/）
- 容器: 进行了资源隔离的镜像运行时环境

### 1.3 Docker 的实现原理

到此读者们肯定很好奇 Docker 是如何进行资源虚拟化的，并且如何实现资源隔离的，其核心技术原理主要有(内容部分参考自 Docker 核心技术与实现原理)：
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151451317631587.png)

#### **1.3.1 Namespace**

> 在日常使用 Linux 或者 macOS 时，我们并没有运行多个完全分离的服务器的需要，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，这是很多时候我们都不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。

命名空间 (Namespaces) 是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。Linux 的命名空间机制提供了以下七种不同的命名空间，通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。

1. CLONE_NEWCGROUP
2. CLONE_NEWIPC
3. CLONE_NEWNET
4. CLONE_NEWNS
5. CLONE_NEWPID
6. CLONE_NEWUSER
7. CLONE_NEWUTS

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151451544165176.png)

在 Linux 系统中，有两个特殊的进程，一个是 pid 为 1 的 /sbin/init 进程，另一个是 pid 为 2 的 kthreadd 进程，这两个进程都是被 Linux 中的上帝进程 idle 创建出来的，其中前者负责执行内核的一部分初始化工作和系统配置，也会创建一些类似 getty 的注册进程，而后者负责管理和调度其他的内核进程。

当在宿主机运行 Docker，通过`docker run`或`docker start`创建新容器进程时，会传入 CLONE_NEWPID 实现进程上的隔离。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151452065487278.png)

接着，在方法`createSpec`的`setNamespaces`中，完成除进程命名空间之外与用户、网络、IPC 以及 UTS 相关的命名空间的设置。

```
func (daemon *Daemon) createSpec(c *container.Container) (*specs.Spec, error) { s := oci.DefaultSpec() // ... if err := setNamespaces(daemon, &s, c); err != nil {  return nil, fmt.Errorf("linux spec namespaces: %v", err) } return &s, nil}func setNamespaces(daemon *Daemon, s *specs.Spec, c *container.Container) error { // user // network // ipc // uts // pid if c.HostConfig.PidMode.IsContainer() {  ns := specs.LinuxNamespace{Type: "pid"}  pc, err := daemon.getPidContainer(c)  if err != nil {   return err  }  ns.Path = fmt.Sprintf("/proc/%d/ns/pid", pc.State.GetPID())  setNamespace(s, ns) } else if c.HostConfig.PidMode.IsHost() {  oci.RemoveNamespace(s, specs.LinuxNamespaceType("pid")) } else {  ns := specs.LinuxNamespace{Type: "pid"}  setNamespace(s, ns) } return nil}
```

##### **网络**

当 Docker 容器完成命名空间的设置，其网络也变成了独立的命名空间，与宿主机的网络互联便产生了限制，这就导致外部很难访问到容器内的应用程序服务。Docker 提供了 4 种网络模式，通过`--net`指定。

1. host
2. container
3. none
4. bridge

由于后续介绍 Kubernetes 利用了 Docker 的 bridge 网络模式，所以仅介绍该模式。Linux 中为了方便各网络命名空间的网络互相访问，设置了 Veth Pair 和网桥来实现，Docker 也是基于此方式实现了网络通信。

下图中 `eth0` 与 `veth9953b75` 是一个 Veth Pair，`eth0` 与 `veth3e84d4f` 为另一个 Veth Pair。Veth Pair 在容器内一侧会被设置为 `eth0` 模拟网卡，另一侧连接 Docker0 网桥，这样就实现了不同容器间网络的互通。加之 Docker0 为每个容器配置的 iptables 规则，又实现了与宿主机外部网络的互通。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151452231388903.png)

挂载点

> 解决了进程和网络隔离的问题，但是 Docker 容器中的进程仍然能够访问或者修改宿主机器上的其他目录，这是我们不希望看到的。

在新的进程中创建隔离的挂载点命名空间需要在 clone 函数中传入 CLONE_NEWNS，这样子进程就能得到父进程挂载点的拷贝，如果不传入这个参数子进程对文件系统的读写都会同步回父进程以及整个主机的文件系统。当一个容器需要启动时，它一定需要提供一个根文件系统（rootfs），容器需要使用这个文件系统来创建一个新的进程，所有二进制的执行都必须在这个根文件系统中，并建立一些符号链接来保证 IO 不会出现问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151452363533407.png)

另外，通过 Linux 的`chroot`命令能够改变当前的系统根目录结构，通过改变当前系统的根目录，我们能够限制用户的权利，在新的根目录下并不能够访问旧系统根目录的结构个文件，也就建立了一个与原系统完全隔离的目录结构。

#### **1.3.2 Control Groups(CGroups)**

Control Groups(CGroups) 提供了宿主机上物理资源的隔离，例如 CPU、内存、磁盘 I/O 和网络带宽。主要由这几个组件构成：

1. **控制组（CGroup）** 一个 CGroup 包含一组进程，并可以在这个 CGroup 上增加 Linux Subsystem 的各种参数配置，将一组进程和一组 Subsystem 关联起来。
2. **Subsystem** 子系统 是一组资源控制模块，比如 CPU 子系统可以控制 CPU 时间分配，内存子系统可以限制 CGroup 内存使用量。可以通过`lssubsys -a`命令查看当前内核支持哪些 Subsystem。
3. **Hierarchy** 层级树 主要功能是把 CGroup 串成一个树型结构，使 CGruop 可以做到继承，每个 Hierarchy 通过绑定对应的 Subsystem 进行资源调度。
4. **Task** 在 CGroups 中，task 就是系统的一个进程。一个任务可以加入某个 CGroup，也可以从某个 CGroup 迁移到另外一个 CGroup。

在 Linux 的 Docker 安装目录下有一个 docker 目录，当启动一个容器时，就会创建一个与容器标识符相同的 CGroup，举例来说当前的主机就会有以下层级关系：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151452499860071.png)
每一个 CGroup 下面都有一个 tasks 文件，其中存储着属于当前控制组的所有进程的 pid，作为负责 cpu 的子系统，cpu.cfs_quota_us 文件中的内容能够对 CPU 的使用作出限制，如果当前文件的内容为 50000，那么当前控制组中的全部进程的 CPU 占用率不能超过 50%。

当我们使用 Docker 关闭掉正在运行的容器时，Docker 的子控制组对应的文件夹也会被 Docker 进程移除。

#### **1.3.3 UnionFS**

> 联合文件系统(Union File System)，它可以把多个目录内容联合挂载到同一个目录下，而目录的物理位置是分开的。UnionFS 可以把只读和可读写文件系统合并在一起，具有写时复制功能，允许只读文件系统的修改可以保存到可写文件系统当中。Docker 之前使用的为 AUFS(Advanced UnionFS)，现为 Overlay2。

Docker 中的每一个镜像都是由一系列只读的层组成的，Dockerfile 中的每一个命令都会在已有的只读层上创建一个新的层：

```
FROM ubuntu:15.04COPY . /appRUN make /appCMD python /app/app.py
```

容器中的每一层都只对当前容器进行了非常小的修改，上述的 Dockerfile 文件会构建一个拥有四层 layer 的镜像：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151453055881176.png)
当镜像被 命令创建时就会在镜像的最上层添加一个可写的层，也就是容器层，所有对于运行时容器的修改其实都是对这个容器读写层的修改。容器和镜像的区别就在于，所有的镜像都是只读的，而每一个容器其实等于镜像加上一个可读写的层，也就是同一个镜像可以对应多个容器。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151453177329176.png)

## **2.Kubernetes**

> Kubernetes，简称 K8s，其中 8 代指中间的 8 个字符。Kubernetes 项目庞大复杂，文章不能面面俱到，因此这个部分将向读者提供一种主线学习思路：
>
> - 什么是 Kubernetes？
> - Kubernetes 提供的组件及适用场景
> - Kubernetes 的架构
> - Kubernetes 架构模块实现原理
>
> 有更多未交代或浅尝辄止的地方读者可以查阅文章或书籍深入研究。

### 2.1 为什么要 Kubernetes

尽管 Docker 为容器化的应用程序提供了开放标准，但随着容器越来越多出现了一系列新问题：

- 单机不足以支持更多的容器
- 分布式环境下容器如何通信？
- 如何协调和调度这些容器？
- 如何在升级应用程序时不会中断服务？
- 如何监视应用程序的运行状况？
- 如何批量重新启动容器里的程序？
- …

Kubernetes 应运而生。

### 2.2 什么是 Kubernetes

Kubernetes 是一个全新的基于容器技术的分布式架构方案，这个方案虽然还很新，但却是 Google 十几年来大规模应用容器技术的经验积累和升华的重要成果，确切的说是 Google 一个久负盛名的内部使用的大规模集群管理系统——Borg 的开源版本，其目的是实现资源管理的自动化以及跨数据中心的资源利用率最大化。

Kubernetes 具有完备的集群管理能力，包括多层次的安全防护和准入机制、多租户应用支撑能力、透明的服务注册和服务发现机制、内建的智能负载均衡器、强大的故障发现和自我修复能力、服务滚动升级和在线扩容能力、可扩展的资源自动调度机制，以及多力度的资源配额管理能力。同时，Kubernetes 提供了完善的管理工具，这些工具涵盖了包括开发、部署测试、运维监控在内的各个环节，不仅是一个全新的基于容器技术的分布式架构解决方案，还是一个一站式的完备分布式系统开发和支撑平台。

### 2.3 Kubernetes 术语

#### **2.3.1 Pod**

Pod 是 Kubernetes 最重要的基本概念，可由多个容器（一般而言一个容器一个进程，不建议一个容器多个进程）组成，它是系统中资源分配和调度的最小单位。下图是 Pod 的组成示意图，其中有一个特殊的 Pause 容器:

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151453301682228.png)
Pause 容器的状态标识了一个 Pod 的状态，也就是代表了 Pod 的生命周期。另外 Pod 中其余容器共享 Pause 容器的命名空间，使得 Pod 内的容器能够共享 Pause 容器的 IP，以及实现文件共享。以下是一个 Pod 的定义：

```
apiVersion: v1  # 分组和版本kind: Pod       # 资源类型metadata:  name: myWeb   # Pod名  labels:    app: myWeb # Pod的标签spec:  containers:  - name: myWeb # 容器名    image: kubeguide/tomcat-app:v1  # 容器使用的镜像    ports:    - containerPort: 8080 # 容器监听的端口    env:  # 容器内环境变量    - name: MYSQL_SERVICE_HOST      value: 'mysql'    - name: MYSQL_SERVICE_PORT      value: '3306'    resources:   # 容器资源配置      requests:  # 资源下限，m表示cpu配额的最小单位，为1/1000核        memory: "64Mi"        cpu: "250m"      limits:    # 资源上限        memory: "128Mi"        cpu: "500m"
```

> EndPoint : PodIP + containerPort，代表一个服务进程的对外通信地址。一个 Pod 也存在具有多个 Endpoint 的情 况，比如当我们把 Tomcat 定义为一个 Pod 时，可以对外暴露管理端口与服务端口这两个 Endpoint。

#### **2.3.2 Label**

Label 是 Kubernetes 系统中的一个核心概念，一个 Label 表示一个 key=value 的键值对，key、value 的值由用户指定。Label 可以被附加到各种资源对象上，例如 Node、Pod、Service、RC 等，一个资源对 象可以定义任意数量的 Label，同一个 Label 也可以被添加到任意数量的资源对象上。Label 通常在资源对象定义时确定，也可以在对象创建后动态添加或者删除。给一个资源对象定义了 Label 后，我们随后可以通过 Label Selector 查询和筛选拥有这个 Label 的资源对象，来实现多维度的资源分组管理功能，以便灵活、方便地进行资源分配、调 度、配置、部署等管理工作。

Label Selector 当前有两种表达式，基于等式的和基于集合的:

- `name=redis-slave`: 匹配所有具有标签`name=redis-slave`的资源对象。
- `env!=production`: 匹配所有不具有标签`env=production`的资源对象。
- `name in(redis-master, redis-slave)`:`name=redis-master`或者`name=redis-slave`的资源对象。
- `name not in(php-frontend)`:匹配所有不具有标签`name=php-frontend`的资源对象。

以 myWeb Pod 为例:

```
apiVersion: v1  # 分组和版本kind: Pod       # 资源类型metadata:  name: myWeb   # Pod名  labels:    app: myWeb # Pod的标签
```

当一个 Service 的 selector 中指明了这个 Pod 时，该 Pod 就会与该 Service 绑定

```
apiVersion: v1kind: Servicemetadata:  name: myWebspec:  selector:    app: myWeb  ports:  - port: 8080
```

#### **2.3.3 Replication Controller**

Replication Controller，简称 RC，简单来说，它其实定义了一个期望的场景，即声明某种 Pod 的副本数量在任意时刻都符合某个预期值。

RC 的定义包括如下几个部分：

- Pod 期待的副本数量
- 用于筛选目标 Pod 的 Label Selector
- 当 Pod 的副本数小于预期数量时，用于创建新 Pod 的模版(template)

```
apiVersion: v1kind: ReplicationControllermetadata:  name: frontendspec:  replicas: 3  # Pod 副本数量  selector:    app: frontend  template:   # Pod 模版    metadata:      labels:        app: frontend    spec:      containers:      - name: tomcat_demp        image: tomcat        ports:        - containerPort: 8080
```

当提交这个 RC 在集群中后，Controller Manager 会定期巡检，确保目标 Pod 实例的数量等于 RC 的预期值，过多的数量会被停掉，少了则会创建补充。通过`kubectl scale`可以动态指定 RC 的预期副本数量。

> 目前，RC 已升级为新概念——Replica Set(RS)，两者当前唯一区别是，RS 支持了基于集合的 Label Selector，而 RC 只支持基于等式的 Label Selector。RS 很少单独使用，更多是被 Deployment 这个更高层的资源对象所使用，所以可以视作 RS+Deployment 将逐渐取代 RC 的作用。

#### **2.3.4 Deployment**

Deployment 和 RC 相似度超过 90%，无论是作用、目的、Yaml 定义还是具体命令行操作，所以可以将其看作是 RC 的升级。而 Deployment 相对于 RC 的一个最大区别是我们可以随时知道当前 Pod“部署”的进度。实际上由于一个 Pod 的创建、调度、绑定节点及在目 标 Node 上启动对应的容器这一完整过程需要一定的时间，所以我们期待系统启动 N 个 Pod 副本的目标状态，实际上是一个连续变化的“部署过程”导致的最终状态。

```
apiVersion: v1kind: Deploymentmetadata:  name: frontendspec:  replicas: 3  selector:    matchLabels:      app: frontend    matchExpressions:      - {key: app, operator: In, values [frontend]}  template:    metadata:      labels:        app: frontend    spec:      containers:      - name: tomcat_demp        image: tomcat        ports:        - containerPort: 8080
```

#### **2.3.5 Horizontal Pod Autoscaler**

除了手动执行`kubectl scale`完成 Pod 的扩缩容之外，还可以通过 Horizontal Pod Autoscaling(HPA)横向自动扩容来进行自动扩缩容。其原理是追踪分析目标 Pod 的负载变化情况，来确定是否需要针对性地调整目标 Pod 数量。当前，HPA 有一下两种方式作为 Pod 负载的度量指标：

- CPUUtilizationPercentage，目标 Pod 所有副本自身的 CPU 利用率的平均值。
- 应用程序自定义的度量指标，比如服务在每秒内的相应请求数(TPS 或 QPS)

```
apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata:  name: php-apache  namespace: defaultspec:  maxReplicas: 3  minReplicas: 1  scaletargetRef:    kind: Deployment    name: php-apache  targetCPUUtilizationPercentage: 90
```

根据上边定义，当 Pod 副本的 CPUUtilizationPercentage 超过 90%时就会出发自动扩容行为，数量约束为 1 ～ 3 个。

#### **2.3.6 StatefulSet**

在 Kubernetes 系统中，Pod 的管理对象 RC、Deployment、DaemonSet 和 Job 都面向无状态的服务。但现实中有很多服务是有状态的，特别是 一些复杂的中间件集群，例如 MySQL 集群、MongoDB 集群、Akka 集 群、ZooKeeper 集群等，这些应用集群有 4 个共同点。

1. 每个节点都有固定的身份 ID，通过这个 ID，集群中的成员可 以相互发现并通信。
2. 集群的规模是比较固定的，集群规模不能随意变动。
3. 集群中的每个节点都是有状态的，通常会持久化数据到永久 存储中。
4. 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功 能受损。

因此，StatefulSet 具有以下特点：

- StatefulSet 里的每个 Pod 都有稳定、唯一的网络标识，可以用来 发现集群内的其他成员。假设 StatefulSet 的名称为 kafka，那么第 1 个 Pod 叫 kafka-0，第 2 个叫 kafka-1，以此类推。

- StatefulSet 控制的 Pod 副本的启停顺序是受控的，操作第 n 个 Pod 时，前 n-1 个 Pod 已经是运行且准备好的状态。

- StatefulSet 里的 Pod 采用稳定的持久化存储卷，通过 PV 或 PVC 来 实现，删除 Pod 时默认不会删除与 StatefulSet 相关的存储卷(为了保证数 据的安全)。

- StatefulSet 除了要与 PV 卷捆绑使用以存储 Pod 的状态数据，还要与 Headless Service 配合使用。

  > Headless Service : Headless Service 与普通 Service 的关键区别在于， 它没有 Cluster IP，如果解析 Headless Service 的 DNS 域名，则返回的是该 Service 对应的全部 Pod 的 Endpoint 列表。

#### **2.3.7 Service**

Service 在 Kubernetes 中定义了一个服务的访问入口地址，前段的应用（Pod）通过这个入口地址访问其背后的一组由 Pod 副本组成的集群实例，Service 与其后端 Pod 副本集群之间则是通过 Label Selector 来实现无缝对接的。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151453558761925.png)

```
apiVersion: v1kind: servicemetadata:  name: tomcat_servicespec:  ports:  - port: 8080   name: service_port  - port: 8005   name: shutdown_port  selector:    app: backend
```

**Service 的负载均衡**

在 Kubernetes 集群中，每个 Node 上会运行着 kube-proxy 组件，这其实就是一个负载均衡器，负责把对 Service 的请求转发到后端的某个 Pod 实例上，并在内部实现服务的负载均衡和绘画保持机制。其主要的实现就是每个 Service 在集群中都被分配了一个全局唯一的 Cluster IP，因此我们对 Service 的网络通信根据内部的负载均衡算法和会话机制，便能与 Pod 副本集群通信。

**Service 的服务发现**

因为 Cluster IP 在 Service 的整个声明周期内是固定的，所以在 Kubernetes 中，只需将 Service 的 Name 和 其 Cluster IP 做一个 DNS 域名映射即可解决。

#### **2.3.8 Volume**

Volume 是 Pod 中能够被多个容器访问的共享目录，Kubernetes 中的 Volume 概念、用途、目的与 Docker 中的 Volumn 比较类似，但不等价。首先，其可被定义在 Pod 上，然后被 一个 Pod 里的多个容器挂载到具体的文件目录下；其次，Kubernetes 中的 Volume 与 Pod 的生命周期相同，但与容器的生命周期不相关，当容器终止或者重启时，Volume 中的数据也不会丢失。

```
template:  metadata:    labels:      app: frontend  spec:    volumes:  # 声明可挂载的volume      - name: dataVol       emptyDir: {}    containers:    - name: tomcat_demo      image: tomcat      ports:      - containerPort: 8080      volumeMounts:  # 将volume通过name挂载到容器内的/mydata-data目录        - mountPath: /mydata-data         name: dataVol
```

Kubernetes 提供了非常丰富的 Volume 类型:

- emptyDir，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为这是 Kubernetes 自动分配的一个目录，当 Pod 从 Node 上移除 emptyDir 中的数据也会被永久删除，适用于临时数据。
- hostPath，hostPath 为在 Pod 上挂载宿主机上的文件或目录，适用于持久化保存的数据，比如容器应用程序生成的日志文件。
- NFS，可使用 NFS 网络文件系统提供的共享目录存储数据。
- 其他云持久化盘等

#### **2.3.9 Persistent Volume**

在使用虚拟机的情况下，我们通常会先定义一个网络存储，然后从中 划出一个“网盘”并挂接到虚拟机上。Persistent Volume(PV) 和与之相关联的 Persistent Volume Claim(PVC) 也起到了类似的作用。PV 可以被理解成 Kubernetes 集群中的某个网络存储对应的一块存储，它与 Volume 类似，但有以下区别:

- PV 只能是网络存储，不属于任何 Node，但可以在每个 Node 上访问。
- PV 并不是被定义在 Pod 上的，而是独立于 Pod 之外定义的。

```
apiVersion: v1kind: PersistentVolumemetadata:  name: pv001  spec:    capacity:      storage: 5Gi    accessMods:      - ReadWriteOnce    nfs:      path: /somePath      server: xxx.xx.xx.x
```

> accessModes，有几种类型，1.ReadWriteOnce:读写权限，并且只能被单个 Node 挂载。2. ReadOnlyMany:只读权限，允许被多个 Node 挂载。3.ReadWriteMany:读写权限，允许被多个 Node 挂载。

如果 Pod 想申请某种类型的 PV，首先需要定义一个 PersistentVolumeClaim 对象，

```
apiVersion: v1kind: PersistentVolumeClaim  # 声明pvcmetadata:  name: pvc001  spec:    resources:      requests:        storage: 5Gi    accessMods:      - ReadWriteOnce
```

然后在 Pod 的 Volume 中引用 PVC 即可。

```
volumes:  - name: mypd   persistentVolumeClaim:     claimName: pvc001
```

PV 有以下几种状态：

- Available：空闲
- Bound：已绑定到 PVC
- Relead：对应 PVC 被删除，但 PV 还没被回收
- Faild：PV 自动回收失败

#### **2.3.10 Namespace**

Namespace 在很多情况下用于实现多租户的资源隔离。分组的不同项目、小组或用户组，便于不同的分组在共享使用整个集群的资源的同时还能被分别管理。Kubernetes 集群在启动后会创建一个名为 default 的 Namespace，通过 kubectl 可以查看:

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151454149230502.png)

#### **2.3.11 ConfigMap**

我们知道，Docker 通过将程序、依赖库、数据及 配置文件“打包固化”到一个不变的镜像文件中的做法，解决了应用的部署的难题，但这同时带来了棘手的问题，即配置文件中的参数在运行期如何修改的问题。我们不可能在启动 Docker 容器后再修改容器里的配置 文件，然后用新的配置文件重启容器里的用户主进程。为了解决这个问题，Docker 提供了两种方式:

- 在运行时通过容器的环境变量来传递参数;
- 通过 Docker Volume 将容器外的配置文件映射到容器内。

在大多数情况下，后一种方式更合 适我们的系统，因为大多数应用通常从一个或多个配置文件中读取参数。但这种方式也有明显的缺陷:我们必须在目标主机上先创建好对应 配置文件，然后才能映射到容器里。上述缺陷在分布式情况下变得更为严重，因为无论采用哪种方式， 写入(修改)多台服务器上的某个指定文件，并确保这些文件保持一致，都是一个很难完成的目标。针对上述问题， Kubernetes 给出了一个很巧妙的设计实现。

首先，把所有的配置项都当作 key-value 字符串，这些配置项可以 作为 Map 表中的一个项，整个 Map 的数据可以被持久化存储在 Kubernetes 的 Etcd 数据库中，然后提供 API 以方便 Kubernetes 相关组件或 客户应用 CRUD 操作这些数据，上述专门用来保存配置参数的 Map 就是 Kubernetes ConfigMap 资源对象。Kubernetes 提供了一种内建机制，将存储在 etcd 中的 ConfigMap 通过 Volume 映射的方式变成目标 Pod 内的配置文件，不管目标 Pod 被调度到哪台服务器上，都会完成自动映射。进一步地，如果 ConfigMap 中的 key-value 数据被修改，则映射到 Pod 中的“配置文件”也会随之自动更新。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151454277097919.png)

### 2.4 Kubernetes 的架构

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151454478600733.png)

Kubernetes 由 Master 节点、 Node 节点以及外部的 ETCD 集群组成，集群的状态、资源对象、网络等信息存储在 ETCD 中，Mater 节点管控整个集群，包括通信、调度等，Node 节点为工作真正执行的节点，并向主节点报告。Master 节点由以下组件构成：

#### **2.4.1 Master 组件：**

1. API Server —— 提供 HTTP Rest 接口，是所有资源增删改查和集群控制的唯一入口。（在集群中表现为名称是 kubernetes 的 service）。可以通过 Dashboard 的 UI 或 kubectl 工具来与其交互。（1）集群管理的 API 入口；
   （2）资源配额控制入口；
   （3）提供完备的集群安全机制。
2. Controller Manager —— 资源对象的控制自动化中心。即监控 Node，当故障时转移资源对象，自动修复集群到期望状态。
3. Scheduler —— 负责 Pod 的调度，调度到最优的 Node。

#### **2.4.2 Node 组件：**

1. kubelet —— 负责 Pod 内容器的创建、启停，并与 Master 密切协作实现集群管理（注册自己，汇报 Node 状态）。
2. kube-proxy —— 实现 k8s Service 的通信与负载均衡。
3. Docker Engine —— Docker 引擎，负责本机容器的创建和管理。

### 2.5 Kubernetes 架构模块实现原理

#### **2.5.1 API Server**

Kubernetes API Server 通过一个名为 kube-apiserver 的进程提供服务，该进程运行在 Master 上。在默认情况下，kube-apiserver 进程在本机的 8080 端口(对应参数–insecure-port)提供 REST 服务。我们可以同时启动 HTTPS 安全端口(–secure-port=6443)来启动安全机制，加强 REST API 访问的安全性。

由于 API Server 是 Kubernetes 集群数据的唯一访问入口，因此安全性与高性能就成为 API Server 设计和实现的两大核心目标。通过采用 HTTPS 安全传输通道与 CA 签名数字证书强制双向认证的方式，API Server 的安全性得以保障。此外，为了更细粒度地控制用户或应用对 Kubernetes 资源对象的访问权限，Kubernetes 启用了 RBAC 访问控制策略。Kubernetes 的设计者综合运用以下方式来最大程度地保证 API Server 的性 能。

1. API Server 拥有大量高性能的底层代码。在 API Server 源码中 使用协程(Coroutine)+队列(Queue)这种轻量级的高性能并发代码， 使得单进程的 API Server 具备了超强的多核处理能力，从而以很快的速 度并发处理大量的请求。
2. 普通 List 接口结合异步 Watch 接口，不但完美解决了 Kubernetes 中各种资源对象的高性能同步问题，也极大提升了 Kubernetes 集群实时响应各种事件的灵敏度。
3. 采用了高性能的 etcd 数据库而非传统的关系数据库，不仅解决 了数据的可靠性问题，也极大提升了 API Server 数据访问层的性能。在 常见的公有云环境中，一个 3 节点的 etcd 集群在轻负载环境中处理一个请 求的时间可以低于 1ms，在重负载环境中可以每秒处理超过 30000 个请求。

#### **2.5.2 安全认证**

**RBAC**

Role-Based Access Control(RBAC)，基于角色的访问控制。

**4 种资源对象**

1. Role
2. RoleBinding
3. ClusterRole
4. ClusterRoleBinding

**Role 与 ClusterRole**

一个角色就是一组权限的集合，都是以许可形式，不存在拒绝的规则。Role 作用于一个命名空间中，ClusterRole 作用于整个集群。

```
apiVersion:rbac.authorization.k8s.io/v1beta1kind:Rolemetadata:  namespace: default #ClusterRole可以省略，毕竟是作用于整个集群  name: pod-readerrules:- apiGroups: [""]  resources: ["pod"]  verbs: ["get","watch","list"]
```

RoleBinding 和 ClusterRoleBinding 是把 Role 和 ClusterRole 的权限绑定到 ServiceAccount 上。

```
kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata:    namespace: default    name: app-adminsubjects:-   kind: ServiceAccount    name: app    apiGroup: ""    namespace: defaultroleRef:    kind: ClusterRole    name: cluster-admin    apiGroup: rbac.authorization.k8s.io
```

**ServiceAccount**

Service Account 也是一种账号，但它并不是给 Kubernetes 集群的用户 (系统管理员、运维人员、租户用户等)用的，而是给运行在 Pod 里的进程用的，它为 Pod 里的进程提供了必要的身份证明。在每个 Namespace 下都有一个名为 default 的默认 Service Account 对象，在这个 Service Account 里面有一个名为 Tokens 的可以当作 Volume 被挂载到 Pod 里的 Secret，当 Pod 启动时，这个 Secret 会自动被挂载到 Pod 的指定目录下，用来协助完成 Pod 中的进程访问 API Server 时的身份鉴权。

#### **2.5.3 Controller Manager**

下边介绍几种 Controller Manager 的实现组件

**ResourceQuota Controller**

kubernetes 的配额管理使用过 Admission Control 来控制的，提供了两种约束，LimitRanger 和 ResourceQuota。LimitRanger 作用于 Pod 和 Container 之上(limit ,request)，ResourceQuota 则作用于 Namespace。资源配额，分三个层次：

> 1. **容器级别**，对容器的 CPU、memory 做限制
> 2. **Pod 级别**，对一个 Pod 内所有容器的可用资源做限制
> 3. **Namespace 级别**，为 namespace 做限制，包括：

```
    + pod数量    + RC数量    + Service数量    + ResourceQuota数量    + Secrete数量    + PV数量
```

**Namespace Controller**

管理 Namesoace 创建删除.

**Endpoints Controller**

Endpoints 表示一个 service 对应的所有 Pod 副本的访问地址，而 Endpoints Controller 就是负责生成和维护所有 Endpoints 对象的控制器。

> - 负责监听 Service 和对应 Pod 副本的变化，若 Service 被创建、更新、删除，则相应创建、更新、删除与 Service 同名的 Endpoints 对象。
> - EndPoints 对象被 Node 上的 kube-proxy 使用。

#### **2.5.4 Scheduler**

Kubernetes Scheduler 的作用是将待调度的 Pod(API 新创 建的 Pod、Controller Manager 为补足副本而创建的 Pod 等)按照特定的调 度算法和调度策略绑定(Binding)到集群中某个合适的 Node 上，并将绑定信息写入 etcd 中。Kubernetes Scheduler 当前提供的默认调度流程分为以下两步。

1. 预选调度过程，即遍历所有目标 Node，筛选出符合要求的候 选节点。为此，Kubernetes 内置了多种预选策略(xxx Predicates)供用户选择。
2. 确定最优节点，在第 1 步的基础上，采用优选策略(xxx Priority)计算出每个候选节点的积分，积分最高者胜出。

#### **2.5.5 网络**

Kubernetes 的网络利用了 Docker 的网络原理，并在此基础上实现了跨 Node 容器间的网络通信。

1. 同一个 Node 下 Pod 间通信模型：
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151455076877270.png)
2. 不同 Node 下 Pod 间的通信模型（CNI 模型实现）：
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151455201189114.png)

CNI 提供了一种应用容器的插件化网络解决方案，定义对容器网络 进行操作和配置的规范，通过插件的形式对 CNI 接口进行实现，以 Flannel 举例，完成了 Node 间容器的通信模型。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151455294724795.png)

可以看到，Flannel 首先创建了一个名为 flannel0 的网桥，而且这个 网桥的一端连接 docker0 网桥，另一端连接一个叫作 flanneld 的服务进程。flanneld 进程并不简单，它上连 etcd，利用 etcd 来管理可分配的 IP 地 址段资源，同时监控 etcd 中每个 Pod 的实际地址，并在内存中建立了一 个 Pod 节点路由表;它下连 docker0 和物理网络，使用内存中的 Pod 节点 路由表，将 docker0 发给它的数据包包装起来，利用物理网络的连接将 数据包投递到目标 flanneld 上，从而完成 Pod 到 Pod 之间的直接地址通信。

#### **2.5.6 服务发现**

从 Kubernetes 1.11 版本开始，Kubernetes 集群的 DNS 服务由 CoreDNS 提供。CoreDNS 是 CNCF 基金会的一个项目，是用 Go 语言实现的高性能、插件式、易扩展的 DNS 服务端。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151455389563234.png)

## **3.结语**

文章包含的内容说多不多，说少不少，但对于 Docker、Kubernetes 知识原理的小白来说是足够的，笔者按照自己的学习经验，以介绍为出发点，让大家更能了解相关技术原理，所以实操的部分较少。Kubernetes 技术组件还是十分丰富的，文章有选择性地进行了介绍，感兴趣的读者可以再自行从官方或者书籍中学习了解。（附《Kubernetes 权威指南——从 Docker 到 Kubernetes 实践全接触》第四版）

## 4.**相关链接**

- [Docker 核心技术与实现原理](https://draveness.me/docker/)
- [官方镜像仓库地址](https://hub.docker.com/)
- [Kubernetes 官网](https://kubernetes.io/)
- [Kubernetes 中文社区](https://www.kubernetes.org.cn/)

原文作者：honghaohu，腾讯 PCG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/ji0Pj00xeHOeispNhsPKZw



# 【NO.92】浅谈 Protobuf 编码

> 近日简单学习了 Protobuf 中的编码实现，总结并整理成文。本文结构总体与 Protobuf 官方文档相似，不少内容也来自官方文档，并在官方文档的基础上添加作者理解的内容，如有出入请以官方文档为准。作者水平有限，难免有疏漏之处，欢迎指正并分享您的意见。

## 1.**0x00 Before you start**

简单来说，Protobuf 的编码是基于变种的 Base128。在学习 Protobuf 编码或是 Base128 之前，先来了解下 Base64 编码。

## **2.0x01 Base 64**

当我们在计算机之间传输数据时，数据本质上是一串字节流。TCP 协议可以保证被发送的字节流正确地达到目的地（至少在出错时有一定的纠错机制），所以本文不讨论因网络因素造成的数据损坏。但数据到达目标机器之后，由于不同机器采用的字符集不同等原因，我们并不能保证目标机器能够正确地“理解”字节流。

Base 64 最初被设计是用于在邮件中嵌入文件（作为 MIME 的一部分）。它可以将任何形式的字节流编码为“安全”的字节流。何为“安全“的字节？先来看看 Base 64 是如何工作的。

假设这里有四个字节,代表你要传输的二进制数据。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151501523865652.png)

首先将这个字节流按每 6 个 bit 为一组进行分组,剩下少于 6 bits 的低位补 0。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502001648745.png)

然后在每一组 6 bits 的高位补两个 0。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502115959855.png)

对照 Base 64 table，字节流可以用 `ognC0w` 来表示。另外，Base64 编码是按照 6 bits 为一组进行编码，每 3 个字节的原始数据要用 4 个字节来储存，编码后的长度要为 4 的整数倍，不足 4 字节的部分要使用 pad 补齐，所以最终的编码结果为`ognC0w==`。

任意的字节流均可以使用 Base 64 进行编码，编码之后所有字节均可以用数字、字母和 + / = 号进行表示，这些都是可以被正常显示的 ascii 字符，即“安全”的字节。绝大部分的计算机和操作系统都对 ascii 有着良好的支持，保证了编码之后的字节流能被正确地复制、传播、解析。

注：下文关于字节顺序内容均基于机器采用小端模式的前提进行讨论。

## 3.**0x02 Base 128?**

Base 64 存在的问题就是，编码后的每一个字节的最高两位总是 0，在不考虑 pad 的情况下，有效 bit 只占 bit 总数的 75%，造成大量的空间浪费。是否可以进一步提高信息密度呢？

意识到这一点，你就很自然能想象出 Base 128 的大致实现思路了，将字节流按 7 bits 进行分组，然后低位补 0。

但问题来了，Base 64 实际上用了 64+1 个 ascii 字符，按照这个思路 Base 128 需要使用 128+1 个 ascii 个字符，但是 ascii 字符一共只有 128 个。另外，即使不考虑 pad，ascii 中包含了一些不可以正常打印的控制字符，编码之后的字符还可能包含会被不同操作系统转换的换行符号（10 和 13）。因此，Base 64 至今依然没有被 Base 128 替代。

Base 64 的规则因为上述限制不能完美地扩展到 Base 128，所以现有基于 Base 64 扩展而来的编码方式大部分都属于变种。如 LEB128（Little-Endian Base 128）， Base 85 （Ascii 85），以及本文的主角：Base 128 Varints。

注：下文关于字节顺序内容均基于机器采用小端模式的前提进行讨论。

## **4.0x03 Base 128 Varints**

Base 128 Varints 是 Google 开发的序列化库 Protocol Buffers 所用的编码方式。以下为 Protobuf 官方文档中对于 Varints 的解释：

> Varints are a method of serializing integers using one or more bytes. Smaller numbers take a smaller number of bytes.

使用一个或多个字节对整数进行序列化。小的数字占用更少的字节。简单来说，就是尽量只储存整数的有效位，高位的 0 尽可能抛弃。

有两个需要注意的细节：

- Base 128 Varints 只能对一部分数据结构进行编码，不适用于所有字节流（当然你可以把任意字节流转换为 string，但不是所有语言都支持这个 trick）。否则无法识别哪部分是无效的 bits。
- Base 128 Varints 编码后的字节可以不存在于 Ascii 表中，因为和 Base 64 使用场景不同，不用考虑是否能正常打印。

下面以例子进行说明 Base 128 Varints 的编码实现。

对于编码后的每个字节，低 7 位用于储存数据，最高位用来标识当前字节是否是当前整数的最后一个字节，称为最高有效位（most significant bit, msb）。msb 为 1 时，代表着后面还有数据；msb 为 0 时代表着当前字节是当前整数的最后一个字节。

举个例子，下面是编码后的整数`1`。1 只需要用一个字节就能表示完全，所以 msb 为 0。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502239941675.png)

对于需要多个字节来储存的数据，如 300 (0b100101100)，有效位数为 9，编码后需要两个字节储存。下面是编码后的整数`300`。第一个字节的 msb 为 1，最后一个字节的 msb 为 0。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502356867932.png)

要将这两个字节解码成整数,需要三个步骤：

- 去除 msb
- 第二步，将字节流逆序（msb 为 0 的字节储存原始数据的高位部分，小端模式）
- 最后拼接所有的 bits。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502469788623.png)

下面一个例子展示如何将使用 Base 128 Varints 对整数进行编码。

- 将数据按每 7 bits 一组拆分
- 逆序每一个组
- 添加 msb

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151502589907756.png)

需要注意的是，无论是编码还是解码，逆序字节流这一步在机器处理中实际是不存在的，机器采用小端模式处理数据，此处逆序仅是为了符合人的阅读习惯而写出。下面展示 Go 版本的 protobuf 中关于 Base 128 Varints 的实现：

```
// google.golang.org/protobuf@v1.25.0/encoding/protowire/wire.go// AppendVarint appends v to b as a varint-encoded uint64.func AppendVarint(b []byte, v uint64) []byte { switch { case v < 1<<7:  b = append(b, byte(v)) case v < 1<<14:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte(v>>7)) case v < 1<<21:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte(v>>14)) case v < 1<<28:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte(v>>21)) case v < 1<<35:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte(v>>28)) case v < 1<<42:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte((v>>28)&0x7f|0x80),   byte(v>>35)) case v < 1<<49:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte((v>>28)&0x7f|0x80),   byte((v>>35)&0x7f|0x80),   byte(v>>42)) case v < 1<<56:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte((v>>28)&0x7f|0x80),   byte((v>>35)&0x7f|0x80),   byte((v>>42)&0x7f|0x80),   byte(v>>49)) case v < 1<<63:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte((v>>28)&0x7f|0x80),   byte((v>>35)&0x7f|0x80),   byte((v>>42)&0x7f|0x80),   byte((v>>49)&0x7f|0x80),   byte(v>>56)) default:  b = append(b,   byte((v>>0)&0x7f|0x80),   byte((v>>7)&0x7f|0x80),   byte((v>>14)&0x7f|0x80),   byte((v>>21)&0x7f|0x80),   byte((v>>28)&0x7f|0x80),   byte((v>>35)&0x7f|0x80),   byte((v>>42)&0x7f|0x80),   byte((v>>49)&0x7f|0x80),   byte((v>>56)&0x7f|0x80),   1) } return b}
```

从源码中可以看出，protobuf 的 varints 最多可以编码 8 字节的数据，这是因为绝大部分的现代计算机最高支持处理 64 位的整型。

## **5.0x04 其他类型**

Protobuf 不仅支持整数类型，下图列出 protobuf 支持的数据类型（wire type）。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151503114423539.png)

在上一小节中展示的编码与解码的例子中的“整数”并不是我们一般理解的整数（编程语言中的 int32，uint32 等），而是对应着上图中的 Varint。

当实际使用编程语言使用 protobuf 进行编码时经过了两步处理：

- 将编程语言的数据结构转化为 wire type。
- 根据不同的 wire type 使用对应的方法编码。前文所提到的 Base 128 Varints 用来编码 varint 类型的数据，其他 wire type 则使用其他编码方式。

```
    {obj}  -> {wire type} -> {encoded byte stream}    uint32 -> wire type 0 -> varint    int32  -> wire type 0 -> varint    bool   -> wire type 0 -> varint    string -> wire type 2 -> length-delimited    ...
```

不同语言中 wire type 实际上也可能采用了语言中的某种类型来储存 wire type 的数据。例如，Go 中使用了 uint64 来储存 wire type 0。一般来说，大多数语言中的无符号整型被转换为 varints 之后，有效位上的内容并没有改变。

下面说明部分其他数据类型到 wire type 的转换规则：

- ### **有符号整型**

采用 ZigZag 编码来将 sint32 和 sint64 转换为 wire type 0。下面是 ZigZag 编码的规则（注意是算术位移）：

```
    (n << 1) ^ (n >> 31)  // for 32-bit signed integer    (n << 1) ^ (n >> 63)  // for 64-bit signed integer
```

或者从数学意义来理解：

```
    n * 2       // when n >= 0    -n * 2 - 1  // when n < 0
```

下图展示了部分 ZigZag 编码的例子：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151503251275844.png)

如果不先采用 ZigZag 编码成 wire type，负值 sint64 直接使用 Base 128 Varints 编码之后的长度始终为`ceil(64/7)=10bytes`，浪费大量空间。使用 ZigZag 编码后，绝对值较小的负数的长度能够被显著压缩。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151503364873212.png)
对于 -234(sint32) 这个例子，编码成 varints 之前采用 ZigZag 编码，比直接编码成 varints 少用了 60%的空间。当然，ZigZag 编码也不是完美的方法。当你尝试把 sint32 或 sint64 范围内所有的整数都编码成 varints 字节流,使用 ZigZag 已经不能压缩字节数量了。ZigZag 虽然能压缩部分负数的空间，但同时正数变得需要更多的空间来储存。因此，建议在业务场景允许的场景下尽量用无符号整型，有助于进一步压缩编码后的空间。

- ### **定长数据（64-bit）**

直接采用小端模式储存，不作转换。

- ### **字符串**

以字符串`"testing"`为例

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151503531303965.png)
编码后的 value 分为两部分：

- 蓝色，表示字符串采用 UTF-8 编码后字节流的长度（bytes），采用 Base 128 Varints 进行编码。
- 白色，字符串用 UTF-8 编码后的字节流。

## **6.0x05 消息结构**

Protobuf 采用 proto3 作为 DSL 来描述其支持的消息结构。

```
syntax = "proto3";message SearchRequest {  string query = 1;  int32 page_number = 2;  int32 result_per_page = 3;}
```

设想一下这样一个场景：数据的发送方在业务迭代之后需要在消息内携带更多的字段，而有的接收方并没有更新自己的 proto 文件。要保持较好的兼容性，接收方分辨出哪些字段是自己可以识别的，哪些是不能识别的新增字段。要做到这一点，发送方在编码消息时还必须附带每个字段的 key，客户端读取到未知的 key 时，可以直接跳过对应的 value。

proto3 中每一个字段后面都有一个 `= x`，比如：

```
  string query = 1;
```

这里的等号并不是用于赋值，而是给每一个字段指定一个 ID，称为 field number。消息内同一层次字段的 field number 必须各不相同。

上面所说的 key，在 protobuf 源码中被称为 tag。tag 由 field number 和 type 两部分组成：

- field number 左移 3 bits
- 在最低 3 bits 写入 wire type

下面展示一个生成 tag 例子：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151504038447723.png)

Go 版本 Protobuf 中生成 tag 的源码：

```
// google.golang.org/protobuf@v1.25.0/encoding/protowire/wire.go// EncodeTag encodes the field Number and wire Type into its unified form.func EncodeTag(num Number, typ Type) uint64 {    return uint64(num)<<3 | uint64(typ&7)}
```

源码中生成的 tag 是 uint64，代表着 field number 可以使用 61 个 bit 吗？并非如此。事实上，tag 的长度不能超过 32 bits，意味着 field number 的最大取值为 2^29-1 (536870911)。而且在这个范围内，有一些数是不能被使用的：

- 0 ，protobuf 规定 field number 必须为正整数。
- 19000 到 19999， protobuf 仅供内部使用的保留位。

理解了生成 tag 的规则之后，不难得出以下结论：

- field number 不必从 1 开始，可以从合法范围内的任意数字开始。
- 不同字段间的 field number 不必连续，只要合法且不同即可。

但是实际上，大多数人分配 field number 还是会从 1 开始，因为 tag 最终要经过 Base 128 Varints 编码，较小的 field number 有助于压缩空间，field number 为 1 到 15 的 tag 最终仅需占用一个字节。当你的 message 有超过 15 个字段时，Google 也不建议你将 1 到 15 立马用完。如果你的业务日后有新增字段的可能，并且新增的字段使用比较频繁，你应该在 1 到 15 内预留一部分供新增的字段使用。

当你修改的 proto 文件需要注意：

- field number 一旦被分配了就不应该被更改，除非你能保证所有的接收方都能更新到最新的 proto 文件。
- 由于 tag 中不携带 field name 信息，更改 field name 并不会改变消息的结构。发送方认为的 apple 到接受方可能会被识别成 pear。双方把字段读取成哪个名字完全由双方自己的 proto 文件决定，只要字段的 wire type 和 field number 相同即可。

由于 tag 中携带的类型是 wire type，不是语言中具体的某个数据结构，而同一个 wire type 可以被解码成多种数据结构，具体解码成哪一种是根据接收方自己的 proto 文件定义的。修改 proto 文件中的类型，有可能导致错误。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151504159043275.png)

最后用一个比前面复杂一点的例子来结束本节内容：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151504234942704.png)

## **7.0x06 嵌套消息**

嵌套消息的实现并不复杂。在上一节展示的 protobuf 的 wire type 中，wire type2 （length-delimited）不仅支持 string，也支持 embedded messages。

对于嵌套消息，首先你要将被嵌套的消息进行编码成字节流，然后你就可以像处理 UTF-8 编码的字符串一样处理这些字节流：在字节流前面加入使用 Base 128 Varints 编码的长度即可。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151504322045900.png)

## **8.0x07 重复消息**

假设接收方的 proto3 中定义了某个字段（假设 field number=1），当接收方从字节流中读取到多个 field number=1 的字段时，会执行 merge 操作。merge 的规则如下：

- 如果字段为不可分割的类型，则直接覆盖
- 如果字段为 repeated，则 append 到已有字段
- 如果字段为嵌套消息，则递归执行 merge

如果字段的 field number 相同但是结构不同，则出现 error。以下为 Go 版本 Protobuf 中 merge 的部分源码：

```
// google.golang.org/protobuf@v1.25.0/proto/merge.go// Merge merges src into dst, which must be a message with the same descriptor.//// Populated scalar fields in src are copied to dst, while populated// singular messages in src are merged into dst by recursively calling Merge.// The elements of every list field in src is appended to the corresponded// list fields in dst. The entries of every map field in src is copied into// the corresponding map field in dst, possibly replacing existing entries.// The unknown fields of src are appended to the unknown fields of dst.//// It is semantically equivalent to unmarshaling the encoded form of src// into dst with the UnmarshalOptions.Merge option specified.func Merge(dst, src Message) { // TODO: Should nil src be treated as semantically equivalent to a // untyped, read-only, empty message? What about a nil dst? dstMsg, srcMsg := dst.ProtoReflect(), src.ProtoReflect() if dstMsg.Descriptor() != srcMsg.Descriptor() {  if got, want := dstMsg.Descriptor().FullName(), srcMsg.Descriptor().FullName(); got != want {   panic(fmt.Sprintf("descriptor mismatch: %v != %v", got, want))  }  panic("descriptor mismatch") } mergeOptions{}.mergeMessage(dstMsg, srcMsg)}func (o mergeOptions) mergeMessage(dst, src protoreflect.Message) { methods := protoMethods(dst) if methods != nil && methods.Merge != nil {  in := protoiface.MergeInput{   Destination: dst,   Source:      src,  }  out := methods.Merge(in)  if out.Flags&protoiface.MergeComplete != 0 {   return  } } if !dst.IsValid() {  panic(fmt.Sprintf("cannot merge into invalid %v message", dst.Descriptor().FullName())) } src.Range(func(fd protoreflect.FieldDescriptor, v protoreflect.Value) bool {  switch {  case fd.IsList():   o.mergeList(dst.Mutable(fd).List(), v.List(), fd)  case fd.IsMap():   o.mergeMap(dst.Mutable(fd).Map(), v.Map(), fd.MapValue())  case fd.Message() != nil:   o.mergeMessage(dst.Mutable(fd).Message(), v.Message())  case fd.Kind() == protoreflect.BytesKind:   dst.Set(fd, o.cloneBytes(v))  default:   dst.Set(fd, v)  }  return true }) if len(src.GetUnknown()) > 0 {  dst.SetUnknown(append(dst.GetUnknown(), src.GetUnknown()...)) }}
```

## **9.0x08 字段顺序**

Proto 文件中定义字段的顺序与最终编码结果的字段顺序无关，两者有可能相同也可能不同。当消息被编码时，Protobuf 无法保证消息的顺序，消息的顺序可能随着版本或者不同的实现而变化。任何 Protobuf 的实现都应该保证字段以任意顺序编码的结果都能被读取。

- 序列化后的消息字段顺序是不稳定的。
- 对同一段字节流进行解码，不同实现或版本的 Protobuf 解码得到的结果不一定完全相同（bytes 层面）。只能保证相同版本相同实现的 Protobuf 对同一段字节流多次解码得到的结果相同。
- 假设有一条消息`foo`，以下关系可能不成立：

```
foo.SerializeAsString() == foo.SerializeAsString()Hash(foo.SerializeAsString()) == Hash(foo.SerializeAsString())CRC(foo.SerializeAsString()) == CRC(foo.SerializeAsString())FingerPrint(foo.SerializeAsString()) == FingerPrint(foo.SerializeAsString())
```

- 假设有两条逻辑上相等消息，但是序列化之后的内容（bytes 层面）不相同，部分可能的原因有：
- - 其中一条消息可能使用了较老版本的 protobuf，不能处理某些类型的字段，设为 unknwon。+ 使用了不同语言实现的 Protobuf，并且以不同的顺序编码字段。+ 消息中的字段使用了不稳定的算法进行序列化。+ 某条消息中有 bytes 类型的字段，用于储存另一条消息使用 Protobuf 序列化的结果，而这个 bytes 使用了不同的 Protobuf 进行序列化。+ 使用了新版本的 Protobuf，序列化实现不同。+ 消息字段顺序不同。

**References**

https://datatracker.ietf.org/doc/html/rfc4648
https://developers.google.com/protocol-buffers/docs/encodinghttps://developers.google.com/protocol-buffers/docs/proto3https://stackoverflow.com/questions/3538021/why-do-we-use-base64

原文作者：SG4YK，腾讯 PCG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/enDUynhZ1Pnzg_4xEjR21A

# 【NO.93】gRPC 基础概念详解

**gRPC** (gRPC Remote Procedure Calls) 是 Google 发起的一个开源远程过程调用系统，该系统基于 HTTP/2 协议传输，本文介绍 gRPC 的基础概念，首先通过关系图直观展示这些基础概念之间关联，介绍异步 gRPC 的 Server 和 Client 的逻辑；然后介绍 RPC 的类型，阅读和抓包分析 gRPC 的通信过程协议，gRPC 上下文；最后分析 `grpc.pb.h` 文件的内容，包括 Stub 的能力、Service 的种类以及与核心库的关系。

之所以谓之基础，是这些内容基本不涉及 gRPC Core 的内容。

## **1.基本概念概览**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151513016353805.png)

上图中列出了 gRPC 基础概念及其关系图。其中包括：**Service(定义)、RPC、API、Client、Stub、Channel、Server、Service(实现)、ServiceBuilder** 等。

接下来，以官方提供的 `example/helloworld` 为例进行说明。

`.proto` 文件定义了**服务** `Greeter` 和 **API** `SayHello`：

```
// helloworld.proto// The greeting service definition.service Greeter {  // Sends a greeting  rpc SayHello (HelloRequest) returns (HelloReply) {}}
```

`class GreeterClient` 是 **Client**，是对 **Stub** 封装；通过 **Stub** 可以真正的调用 RPC 请求。

```
class GreeterClient { public:  GreeterClient(std::shared_ptr<Channel> channel)      : stub_(Greeter::NewStub(channel)) {}  std::string SayHello(const std::string& user) {...private:  std::unique_ptr<Greeter::Stub> stub_;};
```

**Channel** 提供一个与特定 gRPC server 的主机和端口建立的连接。

**Stub** 就是在 **Channel** 的基础上创建而成的。

```
target_str = "localhost:50051";auto channel =    grpc::CreateChannel(target_str, grpc::InsecureChannelCredentials());GreeterClient greeter(channel);std::string user("world");std::string reply = greeter.SayHello(user);
```

Server 端需要实现对应的 RPC，所有的 RPC 组成了 **Service**：

```
class GreeterServiceImpl final : public Greeter::Service {  Status SayHello(ServerContext* context, const HelloRequest* request,                  HelloReply* reply) override {    std::string prefix("Hello ");    reply->set_message(prefix + request->name());    return Status::OK;  }};
```

**Server** 的创建需要一个 **Builder**，添加上监听的地址和端口，**注册**上该端口上绑定的服务，最后构建出 Server 并启动：

```
ServerBuilder builder;builder.AddListeningPort(server_address, grpc::InsecureServerCredentials());builder.RegisterService(&service);std::unique_ptr<Server> server(builder.BuildAndStart());
```

**RPC 和 API 的区别**：RPC (Remote Procedure Call) 是一次远程过程调用的整个动作，而 API (Application Programming Interface) 是不同语言在实现 RPC 中的具体接口。一个 RPC 可能对应多种 API，比如同步的、异步的、回调的。一次 RPC 是对某个 API 的一次调用，比如：

```
std::unique_ptr<ClientAsyncResponseReader<HelloReply> > rpc(    stub_->PrepareAsyncSayHello(&context, request, &cq));
```

不管是哪种类型 RPC，都是由 Client 发起请求。

## **2.异步相关概念**

不管是 Client 还是 Server，异步 gRPC 都是利用 [CompletionQueue](https://grpc.io/grpc/cpp/classgrpc_1_1_completion_queue.html) API 进行异步操作。基本的流程：

- 绑定一个 `CompletionQueue` 到一个 RPC 调用
- 利用唯一的 `void*` Tag 进行读写
- 调用 `CompletionQueue::Next()` 等待操作完成，完成后通过唯一的 Tag 来判断对应什么请求/返回进行后续操作

官方文档 [Asynchronous-API tutorial](https://grpc.io/docs/languages/cpp/async/) 中有上边的介绍，并介绍了异步 client 和 server 的解释，对应这 `greeter_async_client.cc` 和 `greeter_async_server.cc` 两个文件。

Client 看文档可以理解，但 Server 的代码复杂，文档和注释中的解释并不是很好理解，接下来会多做一些解释。

### 2.1 异步 Client

`greeter_async_client.cc` 中是异步 Client 的 Demo，其中只有一次请求，逻辑简单。

- 创建 CompletionQueue
- 创建 RPC (既 `ClientAsyncResponseReader<HelloReply>`)，这里有两种方式：
- - `stub_->PrepareAsyncSayHello()` + `rpc->StartCall()`
  - `stub_->AsyncSayHello()`
- 调用 `rpc->Finish()` 设置请求消息 reply 和唯一的 tag 关联，将请求发送出去
- 使用 `cq.Next()` 等待 Completion Queue 返回响应消息体，通过 tag 关联对应的请求

[TODO] **ClientAsyncResponseReader 在 `Finish()` 完之后就没有用了？**

### 2.2 异步 Server

`RequestSayHello()` 这个函数没有任何的说明。只说是：”we *request* that the system start processing SayHello requests.” 也没有说跟 `cq_->Next(&tag, &ok);` 的关系。我这里通过加上一些日志打印，来更清晰的展示 Server 的逻辑：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151513209083141.png)

上边绿色的部分为创建的第一个 CallData 对象地址，橙色的为第二个 CallData 的地址。

- 创建一个 CallData，初始构造列表中将状态设置为 **CREATE**
- 构造函数中，调用 Process()成员函数，调用 `service_->RequestSayHello()`后，状态变更为 **PROCESS**：
- - 传入 `ServerContext ctx_`
  - 传入 `HelloRequest request_`
  - 传入 `ServerAsyncResponseWriter<HelloReply> responder_`
  - 传入 `ServerCompletionQueue* cq_`
  - 将对象自身的地址作为 `tag` 传入
  - 该动作，能将**事件加入事件循环，可以在 CompletionQueue 中等待**
- 收到请求，`cq->Next()`的阻塞结束并返回，**得到 tag**，既上次传入的 CallData 对象地址
- 调用 tag 对应 CallData 对象的 `Proceed()`，此时状态为 **Process**
- - 创建新的 CallData 对象以接收新请求
  - 处理消息体并设置 `reply_`
  - 将状态设置为 **FINISH**
  - 调用 `responder_.Finish()` 将返回发送给客户端
  - 该动作，能将**事件加入到事件循环，可以在 CompletionQueue 中等待**
- 发送完毕，`cq->Next()`的阻塞结束并返回，**得到 tag**。现实中，如果发送有异常应当有其他相关的处理
- 调用 tag 对应 CallData 对象的 `Proceed()`，此时状态为 **FINISH**，`delete this` 清理自己，一条消息处理完成

### 2.3 关系图

将上边的异步 Client 和异步 Server 的逻辑通过关系图进行展示。右侧 RPC 为创建的对象中的内存容，左侧使用相同颜色的小块进行代替。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151513336865444.png)

以下 CallData 并非 gRPC 中的概念，而是异步 Server 在实现过程中为了方便进行的封装，其中的 Status 也是在异步调用过程中自定义的、用于转移的状态。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151513437399143.png)

### 2.4 异步 Client 2

在 `example/cpp/helloworld` 中还有另外一个异步 Client，对应文件名为 `greeter_async_client2.cc`。这个例子中使用了**两个线程去分别进行发送请求和处理返回**，一个线程批量发出 100 个 SayHello 的请求，另外一个不断的通过 `cq_.Next()` 来等待返回。

无论是 Client 还是 Server，**在以异步方式进行处理时，都要预先分配好一定的内存/对象，以存储异步的请求或返回。**

### 2.5 在 `example/cpp/helloworld` 中，还提供了 callback 相关的 Client 和 Server。

使用回调方式简介明了，结构上与同步方式相差不多，但是并发有本质的区别。可以通过文件对比，来查看其中的差异。

```
cd examples/cpp/helloworld/vimdiff greeter_callback_client.cc greeter_client.ccvimdiff greeter_callback_server.cc greeter_server.cc
```

其实，回调方式的异步调用属于实验性质的，不建议直接在生产环境使用，这里也只做简单的介绍：

> Notice: This API is EXPERIMENTAL and may be changed or removed at any time.

#### **2.5.1 回调 Client**

发送单个请求，在调用 `SayHello` 时，除了传入 Request、 Reply 的地址之外，还需要传入一个接收 Status 的回调函数。

例子中只有一个请求，因此在 `SayHello` 之后，就直接通过 `condition_variable` 的 wait 函数等待回调结束，然后进行后续处理。这样其实不能进行并发，跟同步请求差别不大。如果要进行大规模的并发，还是需要使用额外的对象进行封装一下。

```
stub_->async()->SayHello(&context, &request, &reply,                         [&mu, &cv, &done, &status](Status s) {                           status = std::move(s);                           std::lock_guard<std::mutex> lock(mu);                           done = true;                           cv.notify_one();                         });
```

上边函数调用函数声明如下，很明显这是实验性（experimental）的接口：

```
void Greeter::Stub::experimental_async::SayHello(    ::grpc::ClientContext* context, const ::helloworld::HelloRequest* request,    ::helloworld::HelloReply* response, std::function<void(::grpc::Status)> f);
```

#### **2.5.2 回调 Server**

与同步 Server 不同的是：

- 服务的实现是继承 `Greeter::CallbackService`
- `SayHello` 返回的不是状态，而是 `ServerUnaryReactor` 指针
- 通过 `CallbackServerContext` 获得 `reactor`
- 调用 `reactor` 的 `Finish` 函数处理返回状态

## **3.流相关概念**

可以按照 Client 和 Server 一次发送/返回的是单个消息还是多个消息，将 gRPC 分为：

- Unary RPC
- Server streaming RPC
- Client streaming RPC
- Bidirectional streaming RPC

### 3.1 Server 对 RPC 的实现

Server 需要实现 proto 中定义的 RPC，每种 RPC 的实现都需要将 ServerContext 作为参数输入。

如果是一元 (Unary) RPC 调用，则像调用普通函数一样。将 Request 和 Reply 的对象地址作为参数传入，函数中将根据 Request 的内容，在 Reply 的地址上写上对应的返回内容。

```
// rpc GetFeature(Point) returns (Feature) {}Status GetFeature(ServerContext* context, const Point* point, Feature* feature);
```

如果涉及到流，则会用 Reader 或/和 Writer 作为参数，读取流内容。如 ServerStream 模式下，只有 Server 端产生流，这时对应的 Server 返回内容，需要使用作为参数传入的 `ServerWriter`。这类似于以 `'w'` 打开一个文件，持续的往里写内容，直到没有内容可写关闭。

```
// rpc ListFeatures(Rectangle) returns (stream Feature) {}Status ListFeatures(ServerContext* context,                    const routeguide::Rectangle* rectangle,                    ServerWriter<Feature>* writer);
```

另一方面，Client 来的流，Server 需要使用一个 ServerReader 来接收。这类似于打开一个文件，读其中的内容，直到读到 `EOF` 为止类似。

```
// rpc RecordRoute(stream Point) returns (RouteSummary) {}Status RecordRoute(ServerContext* context, ServerReader<Point>* reader,                   RouteSummary* summary);
```

如果 Client 和 Server 都使用流，也就是 Bidirectional-Stream 模式下，输入参数除了 ServerContext 之外，只有一个 `ServerReaderWriter` 指针。通过该指针，既能读 Client 来的流，又能写 Server 产生的流。

例子中，Server 不断地从 stream 中读，读到了就将对应的写过写到 stream 中，直到客户端告知结束；Server 处理完所有数据之后，直接返回状态码即可。

```
// rpc RouteChat(stream RouteNote) returns (stream RouteNote) {}Status RouteChat(ServerContext* context,                 ServerReaderWriter<RouteNote, RouteNote>* stream);
```

### 3.2 Client 对 RPC 的调用

Client 在调用一元 (Unary) RPC 时，像调用普通函数一样，除了传入 ClientContext 之外，将 Request 和 Response 的地址，返回的是 RPC 状态：

```
// rpc GetFeature(Point) returns (Feature) {}Status GetFeature(ClientContext* context, const Point& request,                  Feature* response);
```

Client 在调用 ServerStream RPC 时，不会得到状态，而是返回一个 ClientReader 的指针：

```
// rpc ListFeatures(Rectangle) returns (stream Feature) {}unique_ptr<ClientReader<Feature>> ListFeatures(ClientContext* context,                                               const Rectangle& request);
```

Reader 通过不断的 `Read()`，来不断的读取流，结束时 `Read()` 会返回 `false`；通过调用 `Finish()` 来读取返回状态。

调用 ClientStream RPC 时，则会返回一个 ClientWriter 指针：

```
// rpc RecordRoute(stream Point) returns (RouteSummary) {}unique_ptr<ClientWriter<Point>> RecordRoute(ClientContext* context,                                            Route Summary* response);
```

Writer 会不断的调用 `Write()` 函数将流中的消息发出；发送完成后调用 `WriteDone()` 来说明发送完毕；调用 `Finish()` 来等待对端发送状态。

而双向流的 RPC 时，会返回 ClientReaderWriter，：

```
// rpc RouteChat(stream RouteNote) returns (stream RouteNote) {}unique_ptr<ClientReaderWriter<RouteNote, RouteNote>> RouteChat(    ClientContext* context);
```

前面说明了 Reader 和 Writer 读取和发送完成的函数调用。因为 RPC 都是 Client 请求而后 Server 响应，双向流也是要 Client 先发送完自己流，才有 Server 才可能结束 RPC。所以对于双向流的结束过程是：

- `stream->WriteDone()`
- `stream->Finish()`

示例中创建了单独的一个线程去发送请求流，在主线程中读返回流，实现了一定程度上的并发。

### 3.3 流是会结束的

并不似长连接，建立上之后就一直保持，有消息的时候发送。（是否有通过建立一个流 RPC 建立推送机制？）

- Client 发送流，是通过 `Writer->WritesDone()` 函数结束流
- Server 发送流，是通过结束 RPC 函数并返回状态码的方式来结束流
- 流接受者，都是通过 `Reader->Read()` 返回的 bool 型状态，来判断流是否结束

Server 并没有像 Client 一样调用 `WriteDone()`，而是在消息之后，将 status code、可选的 status message、可选的 trailing metadata 追加进行发送，这就意味着流结束了。

## **4.通信协议**

本节通过介绍 gRPC 协议文档描述和对 helloworld 的抓包，来说明 gRPC 到底是如何传输的。

官方文档《[gRPC over HTTP2](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md)》中有描述 gRPC 基于 HTTP2 的具体实现，主要介绍的就是协议，也就是 gRPC 的请求和返回是如何基于 HTTP 协议构造的。如果不熟悉 HTTP2 可以阅读一下 [RFC 7540](https://httpwg.org/specs/rfc7540.html)。

### 4.1 ABNF 语法

ABNF 语法是一种描述协议的标准，gRPC 协议也是使用 ABNF 语法描述，几种常见的运算符在[第三节](https://datatracker.ietf.org/doc/html/rfc5234#section-3)中有介绍：

```
3.  Operators 3.1.  Concatenation:  Rule1 Rule2 3.2.  Alternatives:  Rule1 / Rule2 3.3.  Incremental Alternatives: Rule1 =/ Rule2 3.4.  Value Range Alternatives:  %c##-## 3.5.  Sequence Group:  (Rule1 Rule2) 3.6.  Variable Repetition:  *Rule 3.7.  Specific Repetition:  nRule 3.8.  Optional Sequence:  [RULE] 3.9.  Comment:  ; Comment 3.10. Operator Precedence
```

### 4.2 请求协议

`*<element>` 表示 element 会重复多次（最少 0 次）。知道这个就能理解概况里的描述了：

```
Request → Request-Headers *Length-Prefixed-Message EOSRequest-Headers → Call-Definition *Custom-Metadata
```

这表示 **Request 是由 3 部分组成**，首先是 `Request-Headers`，接下来是可能多次出现的 `Length-Prefixed-Message`，最后以一个 `EOS` 结尾（EOS 表示 End-Of-Stream）。

#### **4.2.1 Request-Headers**

根据上边的协议描述， `Request-Headers` 是由一个 `Call-Definition` 和若干 `Custom-Metadata` 组成。

`[]` 表示最多出现一次，比如 `Call-Definition` 有很多组成部分，其中 `Message-Type` 等是选填的：

```
Call-Definition → Method Scheme Path TE [Authority] [Timeout] Content-Type [Message-Type] [Message-Encoding] [Message-Accept-Encoding] [User-Agent]
```

通过 Wireshark 抓包可以看到请求的 Call-Definition 中共有所有要求的 Header，还有额外可选的，比如 user-agent：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151514217645328.png)

因为 helloworld 的示例比较简单，请求中没有填写自定义的元数据（Custom-Metadata）

#### **4.2.2 Length-Prefixed-Message**

传输的 Length-Prefixed-Message 也分为三部分：

```
Length-Prefixed-Message → Compressed-Flag Message-Length Message
```

同样的，Wireshark 抓到的请求中也有这部分信息，并且设置 `.proto` 文件的搜索路径之后可以自动解析 PB：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151514343488279.png)

其中第一个红框（Compressed-Flag）表示不进行压缩，第二个红框（Message-Length）表示消息长度为 7，蓝色反选部分则是 Protobuf 序列化的二进制内容，也就是 Message。

在 gRPC 的[核心概念](https://grpc.io/docs/what-is-grpc/core-concepts/#service-definition)介绍时提到，gRPC 默认使用 Protobuf 作为接口定义语言（IDL），也可以使用其他的 IDL 替代 Protobuf：

> By default, gRPC uses [protocol buffers](https://developers.google.com/protocol-buffers) as the Interface Definition Language (IDL) for describing both the service interface and the structure of the payload messages. It is possible to use other alternatives if desired.

这里 Length-Prefixed-Message 中传输的可以是 PB 也可以是 JSON，须通过 `Content-Type` 头中描述告知。

#### **4.2.3 EOS**

End-Of-Stream 并没有单独的数据去描述，而是通过 HTTP2 的数据帧上带一个 END_STREAM 的 flag 来标识的。比如 helloworld 中请求的数据帧，也携带了 END_STREAM 的标签：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151514519376577.png)

### 4.3 返回协议

`()` 表示括号中的内容作为单个元素对待，`/` 表示前后两个元素可选其一。Response 的定义说明，可以有两种返回形式，一种是消息头、消息体、Trailer，另外一种是只带 Trailer：

```
Response → (Response-Headers *Length-Prefixed-Message Trailers) / Trailers-Only
```

这里需要区分 gRPC 的 Status 和 HTTP 的 Status 两种状态。

```
Response-Headers → HTTP-Status [Message-Encoding] [Message-Accept-Encoding] Content-Type *Custom-MetadataTrailers-Only → HTTP-Status Content-Type TrailersTrailers → Status [Status-Message] *Custom-Metadata
```

不管是哪种形式，最后一部分都是`Trailers`，其中包含了 gRPC 的状态码、状态信息和额外的自定义元数据。

同样地，**使用 END_STREAM 的 flag 标识最后 Trailer 的结束。**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151515032682015.png)

### 4.4 与 HTTP/2 的关系

> The libraries in this repository provide a concrete implemnetation of the gRPC protocol, layered over HTTP/2.

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151515155159188.png)

## **5.上下文**

gRPC 支持上下文的传递，其主要用途有：

- 添加自定义的 metadata，能够通过 gRPC 调用传递
- 控制调用配置，如压缩、鉴权、超时
- 从对端获取 metadata
- 用于性能测量，比如使用 opencensus 等

客户端添加自定义的 metadata key-value 对没有特别的区分，而服务端添加的，则有 inital 和 trailing 两种 metadata 的区分。这也分别对应这 ClientContext 只有一个添加 Metadata 的函数：

```
void AddMetadata (const std::string &meta_key, const std::string &meta_value)
```

而 ServerContext 则有两个：

```
void AddInitialMetadata (const std::string &key, const std::string &value)void AddTrailingMetadata (const std::string &key, const std::string &value)
```

还有一种 Callback Server 对应的上下文叫做 `CallbackServerContext`，它与 `ServerContext` 继承自同一个基类，功能基本上相同。区别在于：

- ServerContext 被 Sync Server 和基于 CQ 的 Async Server 所使用，后者需要用到 `AsyncNotifyWhenDone`
- CallbackServerContext 因为在 `CallOnDone` 的时候，需要释放 context，因此需要知道 context_allocator，因此对应设置和获取 context_allocator 的两个函数

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151515261688750.png)

## **6.Generated Code**

通过 `protoc` 生成 gRPC 相关的文件，除了用于消息体定义的 `xxx.pb.h` 和 `xxx.pb.cc` 文件之外，就是定义 RPC 过程的 `xxx.grpc.pb.h` 和 `xxx.grpc.pb.cc`。本节以 `helloworld.proto` 生成的文件为例，看看 `.grpc.pb` 相关文件具体定义了些什么。

`helloworld.grpc.pb.h` 文件中有命名空间 `helloworld`，其中就仅包含一个类 `Greeter`，所有的 RPC 相关定义都在 `Greeter` 当中，这其中又主要分为两部分：

- Client 用于调用 RPC 的媒介 `Stub` 相关类
- Server 端用于实现不同服务的 Service 相关类和类模板

### 6.1 Stub

`.proto` 中的一个 `service` 只有一个 `Stub`，该类中会提供对应每个 RPC 所有的同步、异步、回调等方式的函数都包含在该类中，而该类继承自接口类 `StubInterface`。

为什么需要一个 StubInterface 来让 Stub 继承，而不是直接产生 Stub？别的复杂的 proto 会有多个 Stub 继承同一个 StubInterface 的情况？不会，因为每个 RPC 对应的函数名是不同。

Greeter 中唯一一个函数是用于创建 Stub 的静态函数 `NewStub`：

```
static std::unique_ptr<Stub> NewStub(...)
```

Stub 中同步、异步方式的函数是直接作为 Stub 的成员函数提供，比如针对一元调用：

- SayHello
- AsyncSayHello
- PrepareAsyncSayHello

[TODO] 为什么同步函数`SayHello`的实现是放在源代码中，而异步函数`AsyncSayHello`的实现是放在头文件中（两者都是直接 `return` 的）？

```
return ::grpc::internal::BlockingUnaryCall< ::helloworld::HelloRequest, ::helloworld::HelloReply, ::grpc::protobuf::MessageLite, ::grpc::protobuf::MessageLite>(channel_.get(), rpcmethod_SayHello_, context, request, response);return std::unique_ptr< ::grpc::ClientAsyncResponseReader< ::helloworld::HelloReply>>(AsyncSayHelloRaw(context, request, cq));
```

回调方式的 RPC 调用是通过一个 `experimental_async` 的类进行了封装（有个 `async_stub_` 的成员变量），所以回调 Client 中提到，回调的调用方式用法是 `stub_->async()->SayHello(...)`。

`experimental_async` 类定义中将 `Stub` 类作为自己的友元，自己的成员可以被 `Stub` 直接访问，而在 `StubInterface` 中也对应有一个 `experimental_async_interface` 的接口类，规定了要实现哪些接口。

### 6.2 Service

有几个概念都叫 Service：proto 文件中 RPC 的集合、proto 文件中 service 产生源文件中的 `Greeter::Service` 类、gRPC 框架中的 `::grpc::Service` 类。本小节说的 Service 就是 `helloworld.grpc.pb.h` 中的 `Greeter::Service`。

#### **6.2.1 Service 是如何定义的**

`helloworld.grpc.pb.h` 文件中共定义了 **7 种 Service**，拿出最常用的 `Service` 和 `AsyncService` 两个定义来说明下 Service 的定义过程：通过类模板链式继承。

**`Service` 跟其他几种 Service 不同，直接继承自 `grpc::Service`，而其他的 Service 都是由类模板构造出来的，而且使用类模板进行嵌套，最基础的类就是这里的 `Service`。**

`Service` 有以下特点：

- 构造函数利用其父类 `grpc::Service` 的 `AddMethod()` 函数，将 `.proto` 文件中定义的 RPC API，添加到成员变量 `methods_` 中（`methods_` 是个向量）
- `AddMethod()` 时会创建 `RpcServiceMethod` 对象，而该对象有一个属性叫做 `api_type_`，构造时默认填的 `ApiType::SYNC`
- `SayHello` 函数不直接声明为纯虚函数，而是以返回 `UNIMPLEMENTED` 状态，因为这个类可能被多次、多级继承

**所以 `Service` 类中的所有 RPC API 都是同步的。**

再看 `AsyncService` 的具体定义：

```
template <class BaseClass>  class WithAsyncMethod_SayHello : public BaseClass { ... };typedef WithAsyncMethod_SayHello<Service > AsyncService;
```

所以 **`AsyncService` 的含义就是继承自 `Service`，加上了 `WithAsyncMethod_SayHello` 的新功能**：

- 构造时，将 SayHello (RPC) 对应的 `api_type_` 设置为 `ApiType::ASYNC`
- 将 `SayHello` 函数直接禁用掉， `abort()` + 返回 `UNIMPLEMENTED` 状态码
- 添加 `RequestSayHello()` 函数， 异步 Server 小节中有介绍过这个函数用法

通过 gRPC 提供的 `route_guide.proto` 例子能更明显的理解这点：

```
typedef WithAsyncMethod_GetFeature< \    WithAsyncMethod_ListFeatures< \    WithAsyncMethod_RecordRoute< \    WithAsyncMethod_RouteChat<Service> > > >    AsyncService;
```

这里 RouteGuide 服务中有 4 个 RPC，`GetFeature`、`ListFeatures`、`RecordRoute`、`RouteChat`，通过 4 个`WithAsyncMethod_{RPC_name}` 的类模板嵌套，能将 4 个 API 都设置成 `ApiType::ASYNC`、添加上对应的 `RequestXXX()` 函数、禁用同步函数。

[TODO] **通过类模板嵌套继承的方式，有什么好处？** 为什么不直接实现 `AsyncService` 这个类呢？

#### **6.2.2 Service 的种类**

`helloworld.grpc.pb.h` 文件中 7 种 Service 中，有 3 对 Service 的真正含义都相同（出于什么目的使用不同的名称？），实际只剩下 4 种 Service。前三种在前边的同步、异步、回调 Server 的介绍中都有涉及。

- Service
- AsyncService
- CallbackService
- ExperimentalCallbackService – 等价于 CallbackService
- StreamedUnaryService
- SplitStreamedService – 等价于 Service
- StreamedService – 等价于 StreamedUnaryService

其实这些不同类型的 Service 是跟前边提到的 `api_type_` 有关。使用不同的 `::grpc::Service::MarkMethodXXX` 设置**不同的 ApiType** 会产生**不同的 API 模板类**，所有 API 模板类级联起来，就得到了**不同的 Service**。这三者的关系简单列举如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151515452378680.png)

另外还有两种模板是通过设置其他属性产生的，这里暂时不做介绍：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151515538628129.png)

[TODO] **头文件中没有用到的类模板在什么场景中会用到？**

### 6.3 与 `::grpc` 核心库的关系

`Stub` 类中主要是用到 gRPC Channel 和不同类型 RPC 对应的方法实现:

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151516064044761.png)

`Service` 类则继承自 `::grpc::Service`，具备其父类的能力，需要自己实现一些 RPC 方法具体的处理逻辑。其它 Service 涉及到 gRPC 核心库的联系有：

- `AsyncService::RequestSayHello()` 调用 `::grpc::Service::RequestAsyncUnary`。
- `CallbackService::SayHello()` 函数返回的是 `::grpc::ServerUnaryReactor` 指针。
- `CallbackService::SetMessageAllocatorFor_SayHello()` 函数中调用 `::grpc::internal::CallbackUnaryHandler::SetMessageAllocator()` 函数设置 RPC 方法的回调的消息分配器。

[TODO] `SetMessageAllocatorFor_SayHello()` 函数并没有被调用到，默认该分配器指针初始值为空，表示用户预先自己分配好而无需回调时分配？

**参考资料**

- https://grpc.io/docs/what-is-grpc/core-concepts/
- https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md
- https://grpc.io/blog/wireshark/

原文作者：jasonzxpan，腾讯 IEG 运营开发工程师

原文链接：https://mp.weixin.qq.com/s/I2QHEBO26nGqhGwIw281Pg



# 【NO.94】深入浅出 Linux 惊群：现象、原因和解决方案

> “惊群”简单地来讲，就是多个进程(线程)阻塞睡眠在某个系统调用上，在等待某个 fd(socket)的事件的到来。当这个 fd(socket)的事件发生的时候，这些睡眠的进程(线程)就会被同时唤醒，多个进程(线程)从阻塞的系统调用上返回，这就是”惊群”现象。”惊群”被人诟病的是效率低下，大量的 CPU 时间浪费在被唤醒发现无事可做，然后又继续睡眠的反复切换上。本文谈谈 linux socket 中的一些”惊群”现象、原因以及解决方案。

## **1. Accept”惊群”现象**

我们知道，在网络分组通信中，网络数据包的接收是异步进行的，因为你不知道什么时候会有数据包到来。因此，网络收包大体分为两个过程：

```
[1] 数据包到来后的事件通知[2] 收到事件通知的Task执行流，响应事件并从队列中取出数据包
```

数据包到来的通知分为两部分：

(1)网卡通知数据包到来，中断协议栈收包；

(2)协议栈将数据包填充 socket 的接收队列，通知应用程序有数据可读，这里仅讨论数据到达协议栈之后的事情。

应用程序是通过 socket 和协议栈交互的，socket 隔离了应用程序和协议栈，socket 是两者之间的接口，对于应用程序，它代表协议栈；而对于协议栈，它又代表应用程序，当数据包到达协议栈的时候，发生下面两个过程：

```
[1] 协议栈将数据包放入socket的接收缓冲区队列，并通知持有该socket的应用程序；[2] 持有该socket的应用程序响应通知事件，将数据包从socket的接收缓冲区队列中取出
```

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151528404233757.png)

对于高性能的服务器而言，为了利用多 CPU 核的优势，大多采用多个进程(线程)同时在一个 listen socket 上进行 accept 请求。多个进程阻塞在 Accept 调用上，那么在协议栈将 Client 的请求 socket 放入 listen socket 的 accept 队列的时候，是要唤醒一个进程还是全部进程来处理呢？

linux 内核通过睡眠队列来组织所有等待某个事件的 task，而 wakeup 机制则可以异步唤醒整个睡眠队列上的 task，wakeup 逻辑在唤醒睡眠队列时，会遍历该队列链表上的每一个节点，调用每一个节点的 callback，从而唤醒睡眠队列上的每个 task。这样，在一个 connect 到达这个 lisent socket 的时候，内核会唤醒所有睡眠在 accept 队列上的 task。N 个 task 进程(线程)同时从 accept 返回，但是，只有一个 task 返回这个 connect 的 fd，其他 task 都返回-1(EAGAIN)。这是典型的 accept”惊群”现象。这个是 linux 上困扰了大家很长时间的一个经典问题，在 linux2.6(似乎在 2.4.1 以后就已经解决，有兴趣的同学可以去验证一下)以后的内核中得到彻底的解决，通过添加了一个 WQ_FLAG_EXCLUSIVE 标记告诉内核进行排他性的唤醒，即唤醒一个进程后即退出唤醒的过程，具体如下：

```
/* * The core wakeup function. Non-exclusive wakeups (nr_exclusive == 0) just * wake everything up. If it's an exclusive wakeup (nr_exclusive == small +ve * number) then we wake all the non-exclusive tasks and one exclusive task. * * There are circumstances in which we can try to wake a task which has already * started to run but is not in state TASK_RUNNING. try_to_wake_up() returns * zero in this (rare) case, and we handle it by continuing to scan the queue. */static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,            int nr_exclusive, int wake_flags, void *key){    wait_queue_t *curr, *next;    list_for_each_entry_safe(curr, next, &q->task_list, task_list) {        unsigned flags = curr->flags;        if (curr->func(curr, mode, wake_flags, key) &&                (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)            break;    }}
```

这样，在 linux 2.6 以后的内核，用户进程 task 对 listen socket 进行 accept 操作，如果这个时候如果没有新的 connect 请求过来，用户进程 task 会阻塞睡眠在 listent fd 的睡眠队列上。这个时候，用户进程 Task 会被设置 WQ_FLAG_EXCLUSIVE 标志位，并加入到 listen socket 的睡眠队列尾部(这里要确保所有不带 WQ_FLAG_EXCLUSIVE 标志位的 non-exclusive waiters 排在带 WQ_FLAG_EXCLUSIVE 标志位的 exclusive waiters 前面)。根据前面的唤醒逻辑，一个新的 connect 到来，内核只会唤醒一个用户进程 task 就会退出唤醒过程，从而不存在了”惊群”现象。

## **2. select/poll/Epoll “惊群”现象**

尽管 accept 系统调用已经不再存在”惊群”现象，但是我们的”惊群”场景还没结束。通常一个 server 有很多其他网络 IO 事件要处理，我们并不希望 server 阻塞在 accept 调用上，为提高服务器的并发处理能力，我们一般会使用 select/poll/epoll I/O 多路复用技术，同时为了充分利用多核 CPU，服务器上会起多个进程(线程)同时提供服务。于是，在某一时刻多个进程(线程)阻塞在 select/poll/epoll_wait 系统调用上，当一个请求上来的时候，多个进程都会被 select/poll/epoll_wait 唤醒去 accept，然而只有一个进程(线程 accept 成功，其他进程(线程 accept 失败，然后重新阻塞在 select/poll/epoll_wait 系统调用上。可见，尽管 accept 不存在”惊群”，但是我们还是没能摆脱”惊群”的命运。难道真的没办法了么？我只让一个进程去监听 listen socket 的可读事件，这样不就可以避免”惊群”了么？

没错，就是这个思路，我们来看看 Nginx 是怎么避免由于 listen fd 可读造成的 epoll_wait”惊群”。这里简单说下具体流程，不进行具体的源码分析。

### 2.1 Nginx 的 epoll”惊群”避免

Nginx 中有个标志 ngx_use_accept_mutex，当 ngx_use_accept_mutex 为 1 的时候（当 nginx worker 进程数>1 时且配置文件中打开 accept_mutex 时，这个标志置为 1），表示要进行 listen fdt”惊群”避免。

Nginx 的 worker 进程在进行 event 模块的初始化的时候，在 core event 模块的 process_init 函数中(ngx_event_process_init)将 listen fd 加入到 epoll 中并监听其 READ 事件。Nginx 在进行相关初始化完成后，进入事件循环(ngx_process_events_and_timers 函数)，在 ngx_process_events_and_timers 中判断，如果 ngx_use_accept_mutex 为 0，那就直接进入 ngx_process_events(ngx_epoll_process_events)，在 ngx_epoll_process_events 将调用 epoll_wait 等待相关事件到来或超时，epoll_wait 返回的时候该干嘛就干嘛。这里不讲 ngx_use_accept_mutex 为 0 的流程，下面讲下 ngx_use_accept_mutex 为 1 的流程。

```
[1] 进入ngx_trylock_accept_mutex，加锁抢夺accept权限（ngx_shmtx_trylock(&ngx_accept_mutex)），加锁成功，则调用ngx_enable_accept_events(cycle) 来将一个或多个listen fd加入epoll监听READ事件(设置事件的回调函数ngx_event_accept)，并设置ngx_accept_mutex_held = 1;标识自己持有锁。[2] 如果ngx_shmtx_trylock(&ngx_accept_mutex)失败，则调用ngx_disable_accept_events(cycle, 0)来将listen fd从epoll中delete掉。[3] 如果ngx_accept_mutex_held = 1(也就是抢到accept权)，则设置延迟处理事件标志位flags |= NGX_POST_EVENTS; 如果ngx_accept_mutex_held = 0(没抢到accept权)，则调整一下自己的epoll_wait超时，让自己下次能早点去抢夺accept权。[4] 进入ngx_process_events(ngx_epoll_process_events)，在ngx_epoll_process_events将调用epoll_wait等待相关事件到来或超时。[5] epoll_wait返回，循环遍历返回的事件，如果标志位flags被设置了NGX_POST_EVENTS，则将事件挂载到相应的队列中(Nginx有两个延迟处理队列，(1)ngx_posted_accept_events：listen fd返回的事件被挂载到的队列。(2)ngx_posted_events：其他socket fd返回的事件挂载到的队列)，延迟处理事件，否则直接调用事件的回调函数。[6] ngx_epoll_process_events返回后，则开始处理ngx_posted_accept_events队列上的事件，于是进入的回调函数是ngx_event_accept，在ngx_event_accept中accept客户端的请求，进行一些初始化工作，将accept到的socket fd放入epoll中。[7] ngx_epoll_process_events处理完成后，如果本进程持有accept锁ngx_accept_mutex_held = 1，那么就将锁释放。[8] 接着开始处理ngx_posted_events队列上的事件。
```

Nginx 通过一次仅允许一个进程将 listen fd 放入自己的 epoll 来监听其 READ 事件的方式来达到 listen fd”惊群”避免。然而做好这一点并不容易，作为一个高性能 web 服务器，需要尽量避免阻塞，并且要很好平衡各个工作 worker 的请求，避免饿死情况，下面有几个点需要大家留意：

```
[1] 避免新请求不能及时得到处理的饿死现象    工作worker在抢夺到accept权限，加锁成功的时候，要将事件的处理delay到释放锁后在处理(为什么ngx_posted_accept_events队列上的事件处理不需要延迟呢？ 因为ngx_posted_accept_events上的事件就是listen fd的可读事件，本来就是我抢到的accept权限，我还没accept就释放锁，这个时候被别人抢走了怎么办呢？)。否则，获得锁的工作worker由于在处理一个耗时事件，这个时候大量请求过来，其他工作worker空闲，然而没有处理权限在干着急。[2] 避免总是某个worker进程抢到锁，大量请求被同一个进程抢到，而其他worker进程却很清闲。    Nginx有个简单的负载均衡，ngx_accept_disabled表示此时满负荷程度，没必要再处理新连接了，我们在nginx.conf曾经配置了每一个nginx worker进程能够处理的最大连接数，当达到最大数的7/8时，ngx_accept_disabled为正，说明本nginx worker进程非常繁忙，将不再去处理新连接。每次要进行抢夺accept权限的时候，如果ngx_accept_disabled大于0，则递减1，不进行抢夺逻辑。
```

Nginx 采用在同一时刻仅允许一个 worker 进程监听 listen fd 的可读事件的方式，来避免 listen fd 的”惊群”现象。然而这种方式编程实现起来比较难，难道不能像 accept 一样解决 epoll 的”惊群”问题么？答案是可以的。要说明 epoll 的”惊群”问题以及解决方案，不能不从 epoll 的两种触发模式说起。

## **3. Epoll”惊群”之 LT(水平触发模式)、ET(边沿触发模式)**

我们先来看下 LT、ET 的语意：

```
[1] LT 水平触发模式只要仍然有未处理的事件，epoll就会通知你，调用epoll_wait就会立即返回。[2] ET 边沿触发模式只有事件列表发生变化了，epoll才会通知你。也就是，epoll_wait返回通知你去处理事件，如果没处理完，epoll不会再通知你了，调用epoll_wait会睡眠等待，直到下一个事件到来或者超时。
```

LT(水平触发模式)、ET(边沿触发模式)在”惊群”问题上，有什么不一样的表现么？要说明这个，就不能不来谈谈 Linux 内核的 sleep/wakeup 机制以及 epoll 的实现核心机制了。

### 3.1 epoll 的核心机制

在了解 epoll 的核心机制前，先了解一下内核 sleep/wakeup 机制的几个核心概念：

```
[1] 等待队列 waitqueue队列头(wait_queue_head_t)往往是资源生产者队列成员(wait_queue_t)往往是资源消费者当头的资源ready后, 会逐个执行每个成员指定的回调函数，来通知它们资源已经ready了[2] 内核的poll机制被Poll的fd, 必须在实现上支持内核的Poll技术，比如fd是某个字符设备,或者是个socket, 它必须实现file_operations中的poll操作, 给自己分配有一个等待队列头wait_queue_head_t，主动poll fd的某个进程task必须分配一个等待队列成员, 添加到fd的等待队列里面去, 并指定资源ready时的回调函数，用socket做例子, 它必须有实现一个poll操作, 这个Poll是发起轮询的代码必须主动调用的, 该函数中必须调用poll_wait(),poll_wait会将发起者作为等待队列成员加入到socket的等待队列中去，这样socket发生事件时可以通过队列头逐个通知所有关心它的进程。[3] epollfd本身也是个fd, 所以它本身也可以被epoll
```

epoll 作为中间层，为多个进程 task，监听多个 fd 的多个事件提供了一个便利的高效机制，我们来看下 epoll 的机制图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151528572399723.png)

从图中，可以看到 epoll 可以监控多个 fd 的事件，它通过一颗红黑树来组织所有被 epoll_ctl 加入到 epoll 监听列表中的 fd，每个被监听的 fd 在 epoll 用一个 epoll item(epi)来标识。

根据内核的 poll 机制，epoll 需要为每个监听的 fd 构造一个 epoll entry(设置关心的事件以及注册回调函数)作为等待队列成员睡眠在每个 fd 的等待队列，以便 fd 上的事件 ready 了，可以通过 epoll 注册的回调函数通知到 epoll。

epoll 作为进程 task 的中间层，它需要有一个等待队列 wq 给 task 在没事件来 epoll_wait 的时候来睡眠等待(epoll fd 本身也是一个 fd，它和其他 fd 一样还有另外一个等待队列 poll_wait，作为 poll 机制被 poll 的时候睡眠等待的地方)。

epoll 可能同时监听成千上万的 fd，这样在少量 fd 有事件 ready 的时候，它需要一个 ready list 队列来组织所有已经 ready 的就绪 fd，以便能够高效通知给进程 task，而不需要遍历所有监听的 fd。图中的一个 epoll 的 sleep/wakeup 流程如下：

```
无事件的时候，多个进程task调用epoll_wait睡眠在epoll的wq睡眠队列上。[1] 这个时候一个请求RQ_1上来，listen fd这个时候ready了，开始唤醒其睡眠队列上的epoll entry，并执行之前epoll注册的回调函数ep_poll_callback。[2] ep_poll_callback主要做两件事情，(1)发生的event事件是epoll entry关心的，则将epi挂载到epoll的就绪队列ready list并进入(2)，否则结束。(2)如果当前wq不为空，则唤醒睡眠在epoll等待队列上睡眠的task(这里唤醒一个还是多个，是区分epoll的ET模式还是LT模式，下面在细讲)。[3] epoll_wait被唤醒继续前行，在ep_poll中调用ep_send_events将fd相关的event事件和数据copy到用户空间，这个时候就需要遍历epoll的ready list以便收集task需要监控的多个fd的event事件和数据上报给用户进程task，这个在ep_scan_ready_list中完成，这里会将ready list清空。
```

通过上图的 epoll 事件通知机制，epoll 的 LT 模式、ET 模式在事件通知行为上的差别，也只能是在[2]上 task 唤醒逻辑上的差别了。我们先来看下，在 epoll_wait 中调用的导致用户进程 task 睡眠的 ep_poll 函数的核心逻辑：

```
static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events, int maxevents, long timeout){int res = 0, eavail, timed_out = 0;bool waiter = false;...eavail = ep_events_available(ep);//是否有fd就绪if (eavail)goto send_events;//有fd就绪，则直接跳过去上报事件给用户if (!waiter) {          waiter = true;          init_waitqueue_entry(&wait, current);//为当前进程task构造一个睡眠entry          spin_lock_irq(&ep->wq.lock);          //插入到epoll的wq后面，注意这里是排他插入的，就是带WQ_FLAG_EXCLUSIVE flag          __add_wait_queue_exclusive(&ep->wq, &wait);          spin_unlock_irq(&ep->wq.lock);    }for (;;) {  //将当前进程设置位睡眠, 但是可以被信号唤醒的状态, 注意这个设置是"将来时", 我们此刻还没睡  set_current_state(TASK_INTERRUPTIBLE);  // 检查是否真的要睡了  if (fatal_signal_pending(current)) {              res = -EINTR;              break;          }          eavail = ep_events_available(ep);          if (eavail)              break;          if (signal_pending(current)) {              res = -EINTR;              break;          }          // 检查是否真的要睡了 end  //使得当前进程休眠指定的时间范围，  if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS)) {  timed_out = 1;  break;  }}__set_current_state(TASK_RUNNING);send_events:      /*       * Try to transfer events to user space. In case we get 0 events and       * there's still timeout left over, we go trying again in search of       * more luck.       */      // ep_send_events往用户态上报事件，即那些epoll_wait返回后能获取的事件      if (!res && eavail &&          !(res = ep_send_events(ep, events, maxevents)) && !timed_out)          goto fetch_events;      if (waiter) {          spin_lock_irq(&ep->wq.lock);          __remove_wait_queue(&ep->wq, &wait);          spin_unlock_irq(&ep->wq.lock);      }   return res;}
```

接着，我们看下监控的 fd 有事件发生的回调函数 ep_poll_callback 的核心逻辑：

```
#define wake_up(x)__wake_up(x, TASK_NORMAL, 1, NULL)static int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key){      int pwake = 0;      struct epitem *epi = ep_item_from_wait(wait);      struct eventpoll *ep = epi->ep;      __poll_t pollflags = key_to_poll(key);      unsigned long flags;      int ewake = 0;  ....  //判断是否有我们关心的event      if (pollflags && !(pollflags & epi->event.events))          goto out_unlock;  //将当前的epitem放入epoll的ready list      if (!ep_is_linked(epi) &&          list_add_tail_lockless(&epi->rdllink, &ep->rdllist)) {          ep_pm_stay_awake_rcu(epi);      }      //如果有task睡眠在epoll的等待队列，唤醒它  if (waitqueue_active(&ep->wq)) {  ....  wake_up(&ep->wq);//  }  ....}
```

wake_up 函数最终会调用到 wake_up_common，通过前面的 wake_up_common 我们知道，唤醒过程在唤醒一个带 WQ_FLAG_EXCLUSIVE 标记的 task 后，即退出唤醒过程。通过上面的 ep_poll，task 是排他(带 WQ_FLAG_EXCLUSIVE 标记)加入到 epoll 的等待队列 wq 的。也就是，在 ep_poll_callback 回调中，只会唤醒一个 task。这就有问题，根据 LT 的语义：只要仍然有未处理的事件，epoll 就会通知你。例如有两个进程 A、B 睡眠在 epoll 的睡眠队列，fd 的可读事件到来唤醒进程 A，但是 A 可能很久才会去处理 fd 的事件，或者它根本就不去处理。根据 LT 的语义，应该要唤醒进程 B 的。

我们来看下 epoll 怎么在 ep_send_events 中实现满足 LT 语义的：

```
  static int ep_send_events(struct eventpoll *ep,                struct epoll_event __user *events, int maxevents)  {      struct ep_send_events_data esed;      esed.maxevents = maxevents;      esed.events = events;      ep_scan_ready_list(ep, ep_send_events_proc, &esed, 0, false);      return esed.res;  }  static __poll_t ep_scan_ready_list(struct eventpoll *ep,                    __poll_t (*sproc)(struct eventpoll *,                         struct list_head *, void *),                    void *priv, int depth, bool ep_locked)  {  ...  // 所有的epitem都转移到了txlist上, 而rdllist被清空了      list_splice_init(&ep->rdllist, &txlist);      ...      //sproc 就是 ep_send_events_proc      res = (*sproc)(ep, &txlist, priv);      ...      //没有处理完的epitem, 重新插入到ready list      list_splice(&txlist, &ep->rdllist);  /* ready list不为空, 直接唤醒... */ // 保证(2)      if (!list_empty(&ep->rdllist)) {          if (waitqueue_active(&ep->wq))              wake_up(&ep->wq);  ...      }  }    static __poll_t ep_send_events_proc(struct eventpoll *ep, struct list_head *head,                     void *priv)  {    ...    //遍历就绪fd列表        list_for_each_entry_safe(epi, tmp, head, rdllink) {        ...        //然后从链表里面移除当前就绪的epi        list_del_init(&epi->rdllink);        //读取当前epi的事件            revents = ep_item_poll(epi, &pt, 1);            if (!revents)              continue;        //将当前的事件和用户传入的数据都copy给用户空间            if (__put_user(revents, &uevent->events) ||              __put_user(epi->event.data, &uevent->data)) {              //如果发生错误了， 则终止遍历过程，将当前epi重新返回就绪队列，剩下的也会在ep_scan_ready_list中重新放回就绪队列              list_add(&epi->rdllink, head);              ep_pm_stay_awake(epi);              if (!esed->res)                  esed->res = -EFAULT;              return 0;           }        }          if (epi->event.events & EPOLLONESHOT)              epi->event.events &= EP_PRIVATE_BITS;          else if (!(epi->event.events & EPOLLET)) { // 保证(1)          //如果是非ET模式(即LT模式)，当前epi会被重新放到epoll的ready list。              list_add_tail(&epi->rdllink, &ep->rdllist);              ep_pm_stay_awake(epi);          }  }
```

上面处理逻辑的核心流程就 2 点：

```
[1] 遍历并清空epoll的ready list，遍历过程中，对于每个epi收集其返回的events，如果没收集到event，则continue去处理其他epi，否则将当前epi的事件和用户传入的数据都copy给用户空间，并判断，如果是在LT模式下，则将当前epi重新放回epoll的ready list[2] 遍历epoll的ready list完成后，如果ready list不为空，则继续唤醒epoll睡眠队列wq上的其他task B。task B从epoll_wait醒来继续前行，重复上面的流程，继续唤醒wq上的其他task C，这样链式唤醒下去。
```

通过上面的流程，在一个 epoll 上睡眠的多个 task，如果在一个 LT 模式下的 fd 的事件上来，会唤醒 epoll 睡眠队列上的所有 task，而 ET 模式下，仅仅唤醒一个 task，这是 epoll”惊群”的根源。等等，这样在 LT 模式下就必然”惊群”，epoll 在 LT 模式下的”惊群”没办法解决么？

### 3.2 epoll_create& fork

相信大家在多进程服务中使用 epoll 的时候，都会有这样一个疑问，是先 epoll_create 得到 epoll fd 后在 fork 子进程，还是先 fork 子进程，然后每个子进程在 epoll_create 自己独立的 epoll fd 呢？有什么异同？

#### **3.2.1 先 epoll_create 后 fork**

这样，多个进程公用一个 epoll 实例(父子进程的 epoll fd 指向同一个内核 epoll 对象)，上面介绍的 epoll 核心机制流程，都是在同一个 epoll 对象上的，这种情况下，epoll 有以下这些特性：

```
[1] epoll在ET模式下不存在“惊群”现象，LT模式是epoll“惊群”的根源，并且LT模式下的“惊群”没办法避免。[2] LT的“惊群”是链式唤醒的，唤醒过程直到当前epi的事件被处理了，无法获得到新的事件才会终止唤醒过程。例如有A、B、C、D...等多个进程task睡眠在epoll的睡眠队列上，并且都监控同一个listen fd的可读事件。一个请求上来，会首先唤醒A进程，A在epoll_wait的处理过程中会唤醒进程B，这样进程B在epoll_wait的处理过程中会唤醒C，这个时候A的epoll_wait处理完成返回，进程A调用accept读取了当前这个请求，进程C在自己的epoll_wait处理过程中，从epi中获取不到事件了，于是终止了整个链式唤醒过程。[3] 多个进程的epoll fd由于指向同一个epoll内核对象，他们对epoll fd的相关epoll_ctl操作会相互影响。一不小心可能会出现一些比较诡异的行为。想象这样一个场景(实际上应该不是这样用)，有一个服务在1234，1235，1236这3个端口上提供服务，于是它epoll_create得到epoll fd后，fork出3个工作的子进程A、B、C，它们分别在这3个端口创建listen fd，然后加入到epoll中监听其可读事件。这个时候端口1234上来一个请求，A、B、C同时被唤醒，A在epoll_wait返回后，在进行accept前由于种种原因卡住了，没能及时accept。B、C在epoll_wait返回后去accept又不能accept到请求，这样B、C重新回到epoll_wait，这个时候又被唤醒，这样只要A没有去处理这个请求之前，B、C就一直被唤醒，然而B、C又无法处理该请求。[4] ET模式下，一个fd上的同事多个事件上来，只会唤醒一个睡眠在epoll上的task，如果该task没有处理完这些事件，在没有新的事件上来前，epoll不会在通知task去处理。
```

由于 ET 的事件通知模式，通常在 ET 模式下的 epoll_wait 返回，我们会循环 accept 来处理所有未处理的请求，直到 accept 返回 EAGAIN 才退出 accept 流程。否则，没处理遗留下来的请求，这个时候如果没有新的请求过来触发 epoll_wait 返回，这样遗留下来的请求就得不到及时处理。这种处理模式，会带来一种类”惊群”现象。考虑，下面的一个处理过程：

```
A、B、C三个进程在监听listen fd的EPOLLIN事件，都睡眠在epoll_wait上，都是ET模式。[1] listen fd上一个请求C_1上来，该请求唤醒了A进程，A进程从epoll_wait返回准备去accept该请求来处理。[2] 这个时候，第二个请求C_2上来，由于睡眠队列上是B、C，于是epoll唤醒B进程，B进程从epoll_wait返回准备去accept该请求来处理。[3] A进程在自己的accept循环中，首选accept得到C_1，接着A进程在第二个循环继续accept，继续得到C_2。[4] B进程在自己的accept循环中，调用accept，由于C_2已经被A拿走了，于是B进程accept返回EAGAIN错误，于是B进程退出accept流程重新睡眠在epoll_wait上。[5] A进程继续第三个循环，这个时候已经没有请求了， accept返回EAGAIN错误，于是A进程也退出accept处理流程，进入请求的处理流程。
```

可以看到，B 进程被唤醒了，但是并没有事情可以做，同时，epoll 的 ET 这样的处理模式，负载容易出现不均衡。

#### **3.2.2 先 fork 后 epoll_create**

用法上，通常是在父进程创建了 listen fd 后，fork 多个 worker 子进程来共同处理同一个 listen fd 上的请求。这个时候，A、B、C…等多个子进程分别创建自己独立的 epoll fd，然后将同一个 listen fd 加入到 epoll 中，监听其可读事件。这种情况下，epoll 有以下这些特性：

```
[1] 由于相对同一个listen fd而言， 多个进程之间的epoll是平等的，于是，listen fd上的一个请求上来，会唤醒所有睡眠在listen fd睡眠队列上的epoll，epoll又唤醒对应的进程task，从而唤醒所有的进程(这里不管listen fd是以LT还是ET模式加入到epoll)。[2] 多个进程间的epoll是独立的，对epoll fd的相关epoll_ctl操作相互独立不影响。
```

可以看出，在使用友好度方面，多进程独立 epoll 实例要比共用 epoll 实例的模式要好很多。独立 epoll 模式要解决 fd 的排他唤醒 epoll 即可。

## **4.EPOLLEXCLUSIVE 排他唤醒 Epoll**

linux4.5 以后的内核版本中，增加了 EPOLLEXCLUSIVE， 该选项只能通过 EPOLL_CTL_ADD 对需要监控的 fd(例如 listen fd)设置 EPOLLEXCLUSIVE 标记。这样 epoll entry 是通过排他方式挂载到 listen fd 等待队列的尾部的，睡眠在 listen fd 的等待队列上的 epoll entry 会加上 WQ_FLAG_EXCLUSIVE 标记。根据前面介绍的内核 wake up 机制，listen fd 上的事件上来，在遍历并唤醒等待队列上的 entry 的时候，遇到并唤醒第一个带 WQ_FLAG_EXCLUSIVE 标记的 entry 后，就结束遍历唤醒过程。于是，多进程独立 epoll 的”惊群”问题得到解决。

## **5.”惊群”之 SO_REUSEPORT**

“惊群”浪费资源的本质在于很多处理进程在别惊醒后，发现根本无事可做，造成白白被唤醒，做了无用功。但是，简单的避免”惊群”会造成同时并发上来的请求得不到及时处理(降低了效率)，为了避免这种情况，NGINX 允许配置成获得 Accept 权限的进程一次性循环 Accept 所有同时到达的全部请求，但是，这会造成短时间 worker 进程的负载不均衡。为此，我们希望的是均衡唤醒，也就是，假设有 4 个 worker 进程睡眠在 epoll_wait 上，那么此时同时并发过来 3 个请求，我们希望 3 个 worker 进程被唤醒去处理，而不是仅仅唤醒一个进程或全部唤醒。

然而要实现这样不是件容易的事情，其根本原因在于，对于大多采用 MPM 机制(multi processing module)TCP 服务而言，基本上都是多个进程或者线程同时在一个 Listen socket 上进行监听请求。根据前面介绍的 Linux 睡眠队列的唤醒方式，基本睡眠在这个 listen socket 上的 Task 只能要么全部被唤醒，要么被唤醒一个。

于是，基本的解决方案是起多个 listen socket，好在我们有 SO_REUSEPORT(linux 3.9 以上内核支持)，它支持多个进程或线程 bind 相同的 ip 和端口，支持以下特性：

```
[1] 允许多个socket bind/listen在相同的IP，相同的TCP/UDP端口[2] 目的是同一个IP、PORT的请求在多个listen socket间负载均衡[3] 安全上，监听相同IP、PORT的socket只能位于同一个用户下
```

于是，在一个多核 CPU 的服务器上，我们通过 SO_REUSEPORT 来创建多个监听相同 IP、PORT 的 listen socket，每个进程监听不同的 listen socket。这样，在只有 1 个新请求到达监听的端口的时候，内核只会唤醒一个进程去 accept，而在同时并发多个请求来到的时候，内核会唤醒多个进程去 accept，并且在一定程度上保证唤醒的均衡性。SO_REUSEPORT 在一定程度上解决了”惊群”问题，但是，由于 SO_REUSEPORT 根据数据包的四元组和当前服务器上绑定同一个 IP、PORT 的 listen socket 数量，根据固定的 hash 算法来路由数据包的，其存在如下问题：

```
[1] Listen Socket数量发生变化的时候，会造成握手数据包的前一个数据包路由到A listen socket，而后一个握手数据包路由到B listen socket，这样会造成client的连接请求失败。[2] 短时间内各个listen socket间的负载不均衡
```

## **6.惊不”惊群”其实是个问题**

很多时候，我们并不是害怕”惊群”，我们怕的”惊群”之后，做了很多无用功。相反在一个异常繁忙，并发请求很多的服务器上，为了能够及时处理到来的请求，我们希望能有多”惊群”就多”惊群”，因为根本做不了无用功，请求多到都来不及处理。于是出现下面的情形：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151529318010740.png)

从上可以看到各个 CPU 都很忙，但是实际有用的 CPU 时间却很少，大部分的 CPU 消耗在*spin*lock 自旋锁上了，并且服务器并发吞吐量并没有随着 CPU 核数增加呈现线性增长，相反出现下降的情况。这是为什么呢？怎么解决？

### 6.1 问题原因

我们知道，一般一个 TCP 服务只有一个 listen socket、一个 accept 队列，而一个 TCP 服务一般有多个服务进程(一个核一个)来处理请求。于是并发请求到达 listen socket 处，那么多个服务进程势必存在竞争，竞争一存在，那么就需要用排队来解决竞态问题，于是似乎锁就无法避免了。在这里，有两类竞争主体，一类是内核协议栈(不可睡眠类)、一类是用户进程(可睡眠类)，这两类主体对 listen socket 发生三种类型的竞争：

```
[1] 协议栈内部之间的竞争[2] 用户进程内部之间的竞争[3] 协议栈和用户之间的竞争
```

由于内核协议栈是不可睡眠的，为此 linux 中采用两层锁定的 lock 结构，一把 listen_socket.lock 自旋锁，一把 listen_socket.own 排他标记锁。其中，listen_socket.lock 用于协议栈内部之间的竞争、协议栈和用户之间的竞争，而 listen_socket.own 用于用户进程内部之间的竞争，listen_socket.lock 作为 listen_socket.own 的排他保护(要获取 listen_socket.own 首先要获取到 listen_socket.lock)。对于处理 TCP 请求而言，一个 SYN 包 syn_skb 到来，这个时候内核 Lock(RCU 锁)住全局的 listeners Table，查找 syn_skb 对应的 listen_socket，没找到则返回错误。否则，就需要进入三次握手处理，首先内核协议栈需要自旋获得 listen_socket.lock 锁，初始化一些数据结构，回复 syn_ack，然后释放 listen_socket.lock 锁。

接着，client 端的 ack 包到来，协议栈这个时候，需要自旋获得 listen_socket.lock 锁，构造 client 端的 socket 等数据结构，如果 accept 队列没有被用户进程占用，那么就将连接排入 accept 队列等待用户进程来 accept，否则就排入 backlog 队列(职责转移，连接排入 accept 队列的事情交给占有 accept 队列的用户进程)。可见，处理一个请求，协议栈需要竞争两次 listen_socket 的自旋锁。由于内核协议栈不能睡眠，于是它只能自旋不断地去尝试获取 listen_socket.lock 自旋锁，直到获取到自旋锁成功为止，中间不能停下来。自旋锁这种暴力、打架的抢锁方式，在一个高并发请求到来的服务器上，就有可能出现上面这种 80%多的 CPU 时间被内核占用，应用程序只能够分配到较少的 CPU 时钟周期的资源的情况。

### 6.2 问题的解决

解决这个问题无非两个方向：(1) 多队列化，减少竞争者 (2) listen_socket 无锁化 。

#### **6.2.1 多队列化 - SO_REUSEPORT**

通过上面的介绍，在 Linux kernel 3.9 以上，可以通过 SO_REUSEPORT 来创建多个 bind 相同 IP、PORT 的 listen_socket。我们可以每一个 CPU 核创建一个 listen_socket 来监听处理请求，这样就是每个 CPU 一个处理进程、一个 listen_socket、一个 accept 队列，多个进程同时并发处理请求，进程之间不再相互竞争 listen_socket。SO_REUSEPORT 可以做到多个 listen_socket 间的负载均衡，然而其负载均衡效果是取决于 hash 算法，可能会出现短时间内的负载极端不均衡。

SO_REUSEPORT 是在将一对多的问题变成多对多的问题，将 Listen Socket 无序暴力争抢 CPU 的现状变成更为有序的争抢。多队列化的优化必须要面对和解决的四个问题是：队列比 CPU 多，队列与 CPU 相等，队列比 CPU 少，根本就没有队列，于是，他们要解决队列发生变化的情况。

如果仅仅把 TCP 的 Listener 看作一个被协议栈处理的 Socket，它和 Client Socket 一起都在相互拼命抢 CPU 资源，那么就可能出现上面的，短时间大量并发请求过来的时候，大量的 CPU 时间被消耗在自旋锁的争抢上了。我们可以换个角度，如果把 TCP Listener 看作一个基础设施服务呢？Listener 为新来的连接请求提供连接服务，并产生 Client Socket 给用户进程，它可以通过一个或多个两类 Accept 队列提供一个服务窗口给用户进程来 accept Client Socket 来处理。仅仅在 Client Socket 需要排入 Accept 队列的是，细粒度锁住队列即可，多个有多个 Accept 队列(每 CPU 一个，那么连锁队列的操作都可以省了)。这样 Listener 就与用户进程无关了，用户进程的产生、退出、CPU 间跳跃、绑定，解除绑定等等都不会影响 TCP Listener 基础设施服务，受影响的是仅仅他们自己该从那个 Accept 队列获取 Client Socket 来处理。于是一个解决思路是连接处理无锁化。

#### **6.2.2 listen socket 无锁化- 旁门左道之 SYN Cookie**

SYN Cookie 原理由 D.J. Bernstain 和 Eric Schenk 提出，专门用来防范 SYN Flood 攻击的一种手段。它的原理是，在 TCP 服务器接收到 SYN 包并返回 SYN ACK 包时，不分配一个专门的数据结构(避免浪费服务器资源)，而是根据这个 SYN 包计算出一个 cookie 值。这个 cookie 作为 SYN ACK 包的初始序列号。当客户端返回一个 ACK 包时，根据包头信息计算 cookie，与返回的确认序列号(初始序列号 + 1)进行对比，如果相同，则是一个正常连接，然后，分配资源，创建 Client Socket 排入 Accept 队列等等用户进程取出处理。于是，整个 TCP 连接处理过程实现了无状态的三次握手。SYN Cookie 机制实现了一定程度上的 listen socket 无锁化，但是它有以下几个缺点。

- **（1）丢失 TCP 选项信息**在建立连接的过程中，不在服务器端保存任何信息，它会丢失很多选项协商信息，这些信息对 TCP 的性能至关重要，比如超时重传等。但是，如果使用时间戳选项，则会把 TCP 选项信息保存在 SYN ACK 段中 tsval 的低 6 位。
- **（2）cookie 不能随地开启**Linux 采用动态资源分配机制，当分配了一定的资源后再采用 cookie 技术。同时为了避免另一种拒绝服务攻击方式，攻击者发送大量的 ACK 报文，服务器忙于计算验证 SYN Cookie。服务器对收到的 ACK 进行 Cookie 合法性验证前，需要确定最近确实发生了半连接队列溢出，不然攻击者只要随便发送一些 ACK，服务器便要忙于计算了。

#### **6.2.3 listen socket 无锁化- Linux 4.4 内核给出的 Lockless TCP listener**

SYN cookie 给出了 Lockless TCP listener 的一些思路，但是我们不想是无状态的三次握手，又不想请求的处理和 Listener 强相关，避免每次进行握手处理都需要 lock 住 listen socket，带来性能瓶颈。4.4 内核前的握手处理是以 listen socket 为主体，listen socket 管理着所有属于它的请求，于是进行三次握手的每个数据包的处理都需要操作这个 listener 本身，而一般情况下，一个 TCP 服务器只有一个 listener，于是在多核环境下，就需要加锁 listen socket 来安全处理握手过程了。我们可以换个角度，握手的处理不再以 listen socket 为主体，而是以连接本身为主体，需要记住的是该连接所属的 listen socket 即可。4.4 内核握手处理流程如下：

[1] TCP 数据包 skb 到达本机，内核协议栈从全局 socket 表中查找 skb 的目的 socket(sk)，如果是 SYN 包，当然查找到的是 listen_socket 了，于是，协议栈根据 skb 构造出一个新的 socket(tmp_sk)，并将 tmp_sk 的 listener 标记为 listen_socket，并将 tmp_sk 的状态设置为 SYNRECV，同时将构造好的 tmp_sk 排入全局 socket 表中，并回复 syn_ack 给 client。

[2] 如果到达本机的 skb 是 syn_ack 的 ack 数据包，那么查找到的将是 tmp_sk，并且 tmp_sk 的 state 是 SYNRECV，于是内核知道该数据包 skb 是 syn_ack 的 ack 包了，于是在 new_sk 中拿出连接所属的 listen_socket，并且根据 tmp_sk 和到来的 skb 构造出 client_socket，然后将 tmp_sk 从全局 socket 表中删除(它的使命结束了)，最后根据所属的 listen_socket 将 client_socket 排如 listen_socket 的 accept 队列中，整个握手过程结束。

4.4 内核一改之前的以 listener 为主体，listener 管理所有 request 的方式，在 SYN 包到来的时候，进行控制反转，以 Request 为主体，构造出一个临时的 tmp_sk 并标记好其所属的 listener，然后平行插入到所有 socket 公共的 socket 哈希表中，从而解放掉 listener，实现 Lockless TCP listener。

## **7.参考文献**

https://blog.csdn.net/dog250/article/details/50528426 https://zhuanlan.zhihu.com/p/51251700 https://blog.csdn.net/dog250/article/details/80837278

原文作者：morganhuang，腾讯 IEG 后台工程师

原文链接：https://mp.weixin.qq.com/s/dQWKBujtPcazzw7zacP1lg

# 【NO.95】Nginx 最全操作总结

> 本文将会从：安装 -> 全局配置 -> 常用的各种配置 来书写，其中常用配置写的炒鸡详细，需要的童鞋可以直接滑倒相应的位置查看。

## 1.**安装 nginx**

**下载 nginx 的压缩包文件到根目录，官网下载地址：nginx.org/download/nginx-x.xx.xx.tar.gz**

```
yum update #更新系统软件cd /wget nginx.org/download/nginx-1.17.2.tar.gz
```

**解压 tar.gz 压缩包文件，进去 nginx-1.17.2**

```
tar -xzvf nginx-1.17.2.tar.gzcd nginx-1.17.2
```

**进入文件夹后进行配置检查**

```
./configure
```

**通过安装前的配置检查，发现有报错。检查中发现一些依赖库没有找到，这时候需要先安装 nginx 的一些依赖库**

```
yum -y install pcre* #安装使nginx支持rewriteyum -y install gcc-c++yum -y install zlib*yum -y install openssl openssl-devel
```

**再次进行检查操作 ./configure 没发现报错显示，接下来进行编译并安装的操作**

```
 // 检查模块支持  ./configure  --prefix=/usr/local/nginx  --with-http_ssl_module --with-http_v2_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_dav_module --with-http_flv_module --with-http_mp4_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_auth_request_module --with-http_random_index_module --with-http_secure_link_module --with-http_degradation_module --with-http_slice_module --with-http_stub_status_module --with-mail --with-mail_ssl_module --with-stream --with-stream_ssl_module --with-stream_realip_module --with-stream_ssl_preread_module --with-threads --user=www --group=www
```

这里得特别注意下，你以后需要用到的功能模块是否存在，不然以后添加新的包会比较麻烦。

**查看默认安装的模块支持**

命令 `ls nginx-1.17.2` 查看 nginx 的文件列表，可以发现里面有一个 auto 的目录。

在这个 auto 目录中有一个 options 文件，这个文件里面保存的就是 nginx 编译过程中的所有选项配置。

通过命令：`cat nginx-1.17.2/auto/options | grep YES`就可以查看

[nginx 编译安装时，怎么查看安装模块](https://jingyan.baidu.com/article/454316ab354edcf7a7c03a81.html)

**编译并安装**

```
make && make install
```

这里需要注意，模块的支持跟后续的 nginx 配置有关，比如 SSL，gzip 压缩等等，编译安装前最好检查需要配置的模块存不存在。

**查看 nginx 安装后在的目录，可以看到已经安装到 /usr/local/nginx 目录了**

```
whereis nginx$nginx: /usr/local/nginx
```

**启动 nginx 服务**

```
cd /usr/local/nginx/sbin/./nginx
```

服务启动的时候报错了：`nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)` ，通过命令查看本机网络地址和端口等一些信息，找到被占用的 80 端口 `netstat -ntpl` 的 tcp 连接，并杀死进程(kill 进程 pid)

```
netstat -ntplkill 进程PID
```

继续启动 nginx 服务，启动成功

```
./nginx
```

在浏览器直接访问 ip 地址，页面出现 Welcome to Nginx! 则安装成功。

## **2.nginx 配置**

### 2.1 基本结构

```
main        # 全局配置，对全局生效├── events  # 配置影响 nginx 服务器或与用户的网络连接├── http    # 配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置│   ├── upstream # 配置后端服务器具体地址，负载均衡配置不可或缺的部分│   ├── server   # 配置虚拟主机的相关参数，一个 http 块中可以有多个 server 块│   ├── server│   │   ├── location  # server 块可以包含多个 location 块，location 指令用于匹配 uri│   │   ├── location│   │   └── ...│   └── ...└── ...
```

### 2.2 主要配置含义

- main:nginx 的全局配置，对全局生效。
- events:配置影响 nginx 服务器或与用户的网络连接。
- http：可以嵌套多个 server，配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置。
- server：配置虚拟主机的相关参数，一个 http 中可以有多个 server。
- location：配置请求的路由，以及各种页面的处理情况。
- upstream：配置后端服务器具体地址，负载均衡配置不可或缺的部分。

### 2.3 nginx.conf 配置文件的语法规则

1. 配置文件由指令与指令块构成
2. 每条指令以 “;” 分号结尾，指令与参数间以空格符号分隔
3. 指令块以 {} 大括号将多条指令组织在一起
4. include 语句允许组合多个配置文件以提升可维护性
5. 通过 # 符号添加注释，提高可读性
6. 通过 $ 符号使用变量
7. 部分指令的参数支持正则表达式，例如常用的 location 指令

### 2.4 内置变量

nginx 常用的内置全局变量，你可以在配置中随意使用：
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151537491057483.png)

### 2.5 常用命令

这里列举几个常用的命令：

```
nginx -s reload  # 向主进程发送信号，重新加载配置文件，热重启nginx -s reopen  # 重启 Nginxnginx -s stop    # 快速关闭nginx -s quit    # 等待工作进程处理完成后关闭nginx -T         # 查看当前 Nginx 最终的配置nginx -t -c <配置路径>  # 检查配置是否有问题，如果已经在配置目录，则不需要 -c
```

以上命令通过 `nginx -h` 就可以查看到，还有其它不常用这里未列出。

Linux 系统应用管理工具 systemd 关于 nginx 的常用命令：

```
systemctl start nginx    # 启动 Nginxsystemctl stop nginx     # 停止 Nginxsystemctl restart nginx  # 重启 Nginxsystemctl reload nginx   # 重新加载 Nginx，用于修改配置后systemctl enable nginx   # 设置开机启动 Nginxsystemctl disable nginx  # 关闭开机启动 Nginxsystemctl status nginx   # 查看 Nginx 运行状态
```

### 2.6 配置 nginx 开机自启

**利用 systemctl 命令**：

如果用 yum install 命令安装的 nginx，yum 命令会自动创建 nginx.service 文件，直接用命令:

```
systemctl enable nginx   # 设置开机启动 Nginxsystemctl disable nginx  # 关闭开机启动 Nginx
```

就可以设置开机自启，否则需要在系统服务目录里创建 nginx.service 文件。

创建并打开 nginx.service 文件：

```
vi /lib/systemd/system/nginx.service
```

内容如下：

```
[Unit]Description=nginxAfter=network.target[Service]Type=forkingExecStart=/usr/local/nginx/sbin/nginxExecReload=/usr/local/nginx/sbin/nginx -s reloadExecStop=/usr/local/nginx/sbin/nginx -s quitPrivateTmp=true[Install]WantedBy=multi-user.target
```

`:wq` 保存退出，运行 `systemctl daemon-reload` 使文件生效。

这样便可以通过以下命令操作 nginx 了：

```
systemctl start nginx.service # 启动nginx服务systemctl enable nginx.service # 设置开机启动systemctl disable nginx.service # 停止开机自启动systemctl status nginx.service # 查看服务当前状态systemctl restart nginx.service # 重新启动服务systemctl is-enabled nginx.service #查询服务是否开机启动
```

**通过开机启动命令脚本实现开机自启**

创建开机启动命令脚本文件：

```
vi /etc/init.d/nginx
```

在这个 nginx 文件中插入一下启动脚本代码，启动脚本代码来源网络复制，实测有效：

```
#! /bin/bash# chkconfig: - 85 15PATH=/usr/local/nginxDESC="nginx daemon"NAME=nginxDAEMON=$PATH/sbin/$NAMECONFIGFILE=$PATH/conf/$NAME.confPIDFILE=$PATH/logs/$NAME.pidscriptNAME=/etc/init.d/$NAMEset -e[ -x "$DAEMON" ] || exit 0do_start() {$DAEMON -c $CONFIGFILE || echo -n "nginx already running"}do_stop() {$DAEMON -s stop || echo -n "nginx not running"}do_reload() {$DAEMON -s reload || echo -n "nginx can't reload"}case "$1" instart)echo -n "Starting $DESC: $NAME"do_startecho ".";;stop)echo -n "Stopping $DESC: $NAME"do_stopecho ".";;reload|graceful)echo -n "Reloading $DESC configuration..."do_reloadecho ".";;restart)echo -n "Restarting $DESC: $NAME"do_stopdo_startecho ".";;*)echo "Usage: $scriptNAME {start|stop|reload|restart}" >&2exit 3;;esacexit 0
```

设置所有人都有对这个启动脚本 nginx 文件的执行权限：

```
chmod a+x /etc/init.d/nginx
```

把 nginx 加入系统服务中：

```
chkconfig --add nginx
```

把服务设置为开机启动：

```
chkconfig nginx on
```

reboot 重启系统生效，可以使用上面 systemctl 方法相同的命令：

```
systemctl start nginx.service # 启动nginx服务systemctl enable nginx.service # 设置开机启动systemctl disable nginx.service # 停止开机自启动systemctl status nginx.service # 查看服务当前状态systemctl restart nginx.service # 重新启动服务systemctl is-enabled nginx.service #查询服务是否开机启动
```

如果服务启动的时候出现 `Restarting nginx daemon: nginxnginx: [error] open() "/usr/local/nginx/logs/nginx.pid" failed (2: No such file or directory) nginx not running` 的错误，通过 nginx -c 参数指定配置文件即可解决

```
/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf
```

如果服务启动中出现 `nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)` 的错误，可以先通过 `service nginx stop` 停止服务，再启动就好。

### 2.7 配置 nginx 全局可用

当你每次改了 `nginx.conf` 配置文件的内容都需要重新到 nginx 启动目录去执行命令，或者通过 -p 参数指向特定目录，会不会感觉很麻烦？

例如：直接执行 `nginx -s reload` 会报错 `-bash: nginx: command not found`，需要到 `/usr/local/nginx/sbin` 目录下面去执行，并且是执行 `./nginx -s reload`。

这里有两种方式可以解决，一种是通过脚本对 nginx 命令包装，这里介绍另外一种比较简单：通过把 nginx 配置到环境变量里，用 nginx 执行指令即可。步骤如下：

1、编辑 /etc/profile

```
vi /etc/profile
```

2、在最后一行添加配置，:wq 保存

```
export PATH=$PATH:/usr/local/nginx/sbin
```

3、使配置立即生效

```
source /etc/profile
```

这样就可以愉快的直接在全局使用 nginx 命令了。

## 3.**nginx 常用功能**

### 3.1 反向代理

我们最常说的反向代理的是通过反向代理解决跨域问题。

其实反向代理还可以用来控制缓存（代理缓存 proxy cache），进行访问控制等等，以及后面说的负载均衡其实都是通过反向代理来实现的。

```
server {    listen    8080;        # 用户访问 ip:8080/test 下的所有路径代理到 github        location /test {         proxy_pass   https://github.com;        }        # 所有 /api 下的接口访问都代理到本地的 8888 端口        # 例如你本地运行的 java 服务的端口是 8888，接口都是以 /api 开头        location /api {            proxy_pass   http://127.0.0.1:8888;        }}
```

### 3.2 访问控制

```
server {   location ~ ^/index.html {       # 匹配 index.html 页面 除了 127.0.0.1 以外都可以访问       deny 192.168.1.1;       deny 192.168.1.2;       allow all; }}
```

上面的命令表示禁止 192.168.1.1 和 192.168.1.2 两个 ip 访问，其它全部允许。从上到下的顺序，匹配到了便跳出，可以按你的需求设置。

### 3.3 负载均衡

通过负载均衡充利用服务器资源，nginx 目前支持自带 4 种负载均衡策略，还有 2 种常用的第三方策略。

**轮询策略（默认）**

每个请求按时间顺序逐一分配到不同的后端服务器，如果有后端服务器挂掉，能自动剔除。但是如果其中某一台服务器压力太大，出现延迟，会影响所有分配在这台服务器下的用户。

```
http {    upstream test.com {        server 192.168.1.12:8887;        server 192.168.1.13:8888;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

**根据服务器权重**

例如要配置：10 次请求中大概 1 次访问到 8888 端口，9 次访问到 8887 端口：

```
http {    upstream test.com {        server 192.168.1.12:8887 weight=9;        server 192.168.1.13:8888 weight=1;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

**客户端 ip 绑定（ip_hash）**

来自同一个 ip 的请求永远只分配一台服务器，有效解决了动态网页存在的 session 共享问题。例如：比如把登录信息保存到了 session 中，那么跳转到另外一台服务器的时候就需要重新登录了。

所以很多时候我们需要一个客户只访问一个服务器，那么就需要用 ip_hash 了。

```
http {    upstream test.com {     ip_hash;        server 192.168.1.12:8887;        server 192.168.1.13:8888;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

**最小连接数策略**

将请求优先分配给压力较小的服务器，它可以平衡每个队列的长度，并避免向压力大的服务器添加更多的请求。

```
http {    upstream test.com {     least_conn;        server 192.168.1.12:8887;        server 192.168.1.13:8888;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

**最快响应时间策略（依赖于第三方 NGINX Plus）**

依赖于 NGINX Plus，优先分配给响应时间最短的服务器。

```
http {    upstream test.com {     fair;        server 192.168.1.12:8887;        server 192.168.1.13:8888;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

**按访问 url 的 hash 结果（第三方）**

按访问 url 的 hash 结果来分配请求，使每个 url 定向到同一个后端服务器，后端服务器为缓存时比较有效。在 upstream 中加入 hash 语句，server 语句中不能写入 weight 等其他的参数，hash_method 是使用的 hash 算法

```
http {    upstream test.com {     hash $request_uri;     hash_method crc32;     server 192.168.1.12:8887;     server 192.168.1.13:8888;    }    server {        location /api {            proxy_pass  http://test.com;        }    }}
```

采用 HAproxy 的 loadbalance uri 或者 nginx 的 upstream_hash 模块，都可以做到针对 url 进行哈希算法式的负载均衡转发。

### 3.4 gzip 压缩

开启 gzip 压缩可以大幅减少 http 传输过程中文件的大小，可以极大的提高网站的访问速度，基本是必不可少的优化操作：

```
gzip  on; # 开启gzip 压缩# gzip_types# gzip_static on;# gzip_proxied expired no-cache no-store private auth;# gzip_buffers 16 8k;gzip_min_length 1k;gzip_comp_level 4;gzip_http_version 1.0;gzip_vary off;gzip_disable "MSIE [1-6]\.";
```

解释一下：

1. gzip_types：要采用 gzip 压缩的 MIME 文件类型，其中 text/html 被系统强制启用；
2. gzip_static：默认 off，该模块启用后，Nginx 首先检查是否存在请求静态文件的 gz 结尾的文件，如果有则直接返回该 .gz 文件内容；
3. gzip_proxied：默认 off，nginx 做为反向代理时启用，用于设置启用或禁用从代理服务器上收到相应内容 gzip 压缩；
4. gzip_buffers：获取多少内存用于缓存压缩结果，16 8k 表示以 8k*16 为单位获得；
5. gzip_min_length：允许压缩的页面最小字节数，页面字节数从 header 头中的 Content-Length 中进行获取。默认值是 0，不管页面多大都压缩。建议设置成大于 1k 的字节数，小于 1k 可能会越压越大；
6. gzip_comp_level：gzip 压缩比，压缩级别是 1-9，1 压缩级别最低，9 最高，级别越高压缩率越大，压缩时间越长，建议 4-6；
7. gzip_http_version：默认 1.1，启用 gzip 所需的 HTTP 最低版本；
8. gzip_vary：用于在响应消息头中添加 Vary：Accept-Encoding，使代理服务器根据请求头中的 Accept-Encoding 识别是否启用 gzip 压缩；
9. gzip_disable 指定哪些不需要 gzip 压缩的浏览器

其中第 2 点，普遍是结合前端打包的时候打包成 gzip 文件后部署到服务器上，这样服务器就可以直接使用 gzip 的文件了，并且可以把压缩比例提高，这样 nginx 就不用压缩，也就不会影响速度。一般不追求极致的情况下，前端不用做任何配置就可以使用啦~

附前端 webpack 开启 gzip 压缩配置，在 vue-cli3 的 vue.config.js 配置文件中：

```
const CompressionWebpackPlugin = require('compression-webpack-plugin')module.exports = {  // gzip 配置  configureWebpack: config => {    if (process.env.NODE_ENV === 'production') {      // 生产环境      return {        plugins: [new CompressionWebpackPlugin({          test: /\.js$|\.html$|\.css/,    // 匹配文件名          threshold: 1024,               // 文件压缩阈值，对超过 1k 的进行压缩          deleteOriginalAssets: false     // 是否删除源文件        })]      }    }  },  ...}
```

### 3.5 HTTP 服务器

nginx 本身也是一个静态资源的服务器，当只有静态资源的时候，就可以使用 nginx 来做服务器：

```
server {  listen       80;  server_name  localhost;  location / {      root   /usr/local/app;      index  index.html;  }}
```

这样如果访问 [http://ip](http://ip/) 就会默认访问到 /usr/local/app 目录下面的 index.html，如果一个网站只是静态页面的话，那么就可以通过这种方式来实现部署，比如一个静态官网。

### 3.6 动静分离

就是把动态和静态的请求分开。方式主要有两种：

- 一种是纯粹把静态文件独立成单独的域名，放在独立的服务器上，也是目前主流推崇的方案
- 一种方法就是动态跟静态文件混合在一起发布， 通过 nginx 配置来分开

```
# 所有静态请求都由nginx处理，存放目录为 htmllocation ~ \.(gif|jpg|jpeg|png|bmp|swf|css|js)$ {    root    /usr/local/resource;    expires     10h; # 设置过期时间为10小时}# 所有动态请求都转发给 tomcat 处理location ~ \.(jsp|do)$ {    proxy_pass  127.0.0.1:8888;}
```

注意上面设置了 expires，当 nginx 设置了 expires 后，例如设置为：expires 10d; 那么，所在的 location 或 if 的内容，用户在 10 天内请求的时候，都只会访问浏览器中的缓存，而不会去请求 nginx 。

### 3.7 请求限制

对于大流量恶意的访问，会造成带宽的浪费，给服务器增加压力。可以通过 nginx 对于同一 IP 的连接数以及并发数进行限制。合理的控制还可以用来防止 DDos 和 CC 攻击。

关于请求限制主要使用 nginx 默认集成的 2 个模块：

- limit_conn_module 连接频率限制模块
- limit_req_module 请求频率限制模块

涉及到的配置主要是：

- limit_req_zone 限制请求数
- limit_conn_zone 限制并发连接数

**通过 limit_req_zone 限制请求数**

```
http{    limit_conn_zone $binary_remote_addrzone=limit:10m; // 设置共享内存空间大    server{     location /{            limit_conn addr 5; # 同一用户地址同一时间只允许有5个连接。        }    }}
```

如果共享内存空间被耗尽，服务器将会对后续所有的请求返回 503 (Service Temporarily Unavailable) 错误。

当多个 limit_conn_zone 指令被配置时，所有的连接数限制都会生效。比如，下面配置不仅会限制单一 IP 来源的连接数，同时也会限制单一虚拟服务器的总连接数：

```
limit_conn_zone $binary_remote_addr zone=perip:10m;limit_conn_zone $server_name zone=perserver:10m;server {    limit_conn perip 10; # 限制每个 ip 连接到服务器的数量    limit_conn perserver 2000; # 限制连接到服务器的总数}
```

**通过 limit_conn_zone 限制并发连接数**

```
limit_req_zone $binary_remote_addr zone=creq:10 mrate=10r/s;server{    location /{        limit_req zone=creq burst=5;    }}
```

限制平均每秒不超过一个请求，同时允许超过频率限制的请求数不多于 5 个。如果不希望超过的请求被延迟，可以用 nodelay 参数,如：

```
limit_req zone=creq burst=5 nodelay;
```

这里只是简单讲讲，让大家有这个概念，配置的时候可以深入去找找资料。

### 3.8 正向代理

正向代理，意思是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理，比如我们使用的 VPN 服务就是正向代理，直观区别：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212151538333925963.png)

配置正向代理：

```
resolver 8.8.8.8 # 谷歌的域名解析地址server {    resolver_timeout 5s; // 设超时时间    location / {        # 当客户端请求我的时候，我会把请求转发给它        # $host 要访问的主机名 $request_uri 请求路径        proxy_pass http://$host$request_uri;    }}
```

正向代理的对象是客户端，服务器端看不到真正的客户端。

### 3.9 图片防盗链

```
server {    listen       80;    server_name  *.test;    # 图片防盗链    location ~* \.(gif|jpg|jpeg|png|bmp|swf)$ {        valid_referers none blocked server_names ~\.google\. ~\.baidu\. *.qq.com;  # 只允许本机 IP 外链引用，将百度和谷歌也加入白名单有利于 SEO        if ($invalid_referer){            return 403;        }    }}
```

以上设置就能防止其它网站利用外链访问我们的图片，有利于节省流量

### 3.10 适配 PC 或移动设备

根据用户设备不同返回不同样式的站点，以前经常使用的是纯前端的自适应布局，但是复杂的网站并不适合响应式，无论是复杂性和易用性上面还是不如分开编写的好，比如我们常见的淘宝、京东。

根据用户请求的 user-agent 来判断是返回 PC 还是 H5 站点：

```
server {    listen 80;    server_name test.com;    location / {     root  /usr/local/app/pc; # pc 的 html 路径        if ($http_user_agent ~* '(Android|webOS|iPhone|iPod|BlackBerry)') {            root /usr/local/app/mobile; # mobile 的 html 路径        }        index index.html;    }}
```

### 3.11 设置二级域名

新建一个 server 即可：

```
server {    listen 80;    server_name admin.test.com; // 二级域名    location / {        root  /usr/local/app/admin; # 二级域名的 html 路径        index index.html;    }}
```

### 3.12 配置 HTTPS

这里我使用的是 certbot 免费证书，但申请一次有效期只有 3 个月（好像可以用 crontab 尝试配置自动续期，我暂时没试过）：

先安装 certbot

```
wget https://dl.eff.org/certbot-autochmod a+x certbot-auto
```

申请证书（注意：需要把要申请证书的域名先解析到这台服务器上，才能申请）:

```
sudo ./certbot-auto certonly --standalone --email admin@abc.com -d test.com -d www.test.com
```

执行上面指令，按提示操作。

Certbot 会启动一个临时服务器来完成验证（会占用 80 端口或 443 端口，因此需要暂时关闭 Web 服务器），然后 Certbot 会把证书以文件的形式保存，包括完整的证书链文件和私钥文件。

文件保存在 /etc/letsencrypt/live/ 下面的域名目录下。

修改 nginx 配置：

```
server{    listen 443 ssl http2; // 这里还启用了 http/2.0    ssl_certificate /etc/letsencrypt/live/test.com/fullchain.pem; # 证书文件地址    ssl_certificate_key /etc/letsencrypt/live/test.com/privkey.pem; # 私钥文件地址    server_name test.com www.test.com; // 证书绑定的域名}
```

### 3.13 配置 HTTP 转 HTTPS

```
server {    listen      80;    server_name test.com www.test.com;    # 单域名重定向    if ($host = 'www.sherlocked93.club'){        return 301 https://www.sherlocked93.club$request_uri;    }    # 全局非 https 协议时重定向    if ($scheme != 'https') {        return 301 https://$server_name$request_uri;    }    # 或者全部重定向    return 301 https://$server_name$request_uri;}
```

以上配置选择自己需要的一条即可，不用全部加。

### 3.14 单页面项目 history 路由配置

```
server {    listen       80;    server_name  fe.sherlocked93.club;    location / {        root       /usr/local/app/dist;  # vue 打包后的文件夹        index      index.html index.htm;        try_files  $uri $uri/ /index.html @rewrites; # 默认目录下的 index.html，如果都不存在则重定向        expires -1;                          # 首页一般没有强制缓存        add_header Cache-Control no-cache;    }    location @rewrites { // 重定向设置        rewrite ^(.+)$ /index.html break;    }}
```

[vue-router](https://router.vuejs.org/zh/guide/essentials/history-mode.html#后端配置例子) 官网只有一句话 `try_files $uri $uri/ /index.html;`，而上面做了一些重定向处理。

### 3.15 配置高可用集群（双机热备）

当主 nginx 服务器宕机之后，切换到备份的 nginx 服务器

首先安装 keepalived:

```
yum install keepalived -y
```

然后编辑 `/etc/keepalived/keepalived.conf` 配置文件，并在配置文件中增加 `vrrp_script` 定义一个外围检测机制，并在 `vrrp_instance` 中通过定义 `track_script` 来追踪脚本执行过程，实现节点转移：

```
global_defs{   notification_email {        cchroot@gmail.com   }   notification_email_from test@firewall.loc   smtp_server 127.0.0.1   smtp_connect_timeout 30 // 上面都是邮件配置   router_id LVS_DEVEL     // 当前服务器名字，用 hostname 命令来查看}vrrp_script chk_maintainace { // 检测机制的脚本名称为chk_maintainace    script "[[ -e/etc/keepalived/down ]] && exit 1 || exit 0" // 可以是脚本路径或脚本命令    // script "/etc/keepalived/nginx_check.sh"    // 比如这样的脚本路径    interval 2  // 每隔2秒检测一次    weight -20  // 当脚本执行成立，那么把当前服务器优先级改为-20}vrrp_instanceVI_1 {   // 每一个vrrp_instance就是定义一个虚拟路由器    state MASTER      // 主机为MASTER，备用机为BACKUP    interface eth0    // 网卡名字，可以从ifconfig中查找    virtual_router_id 51 // 虚拟路由的id号，一般小于255，主备机id需要一样    priority 100      // 优先级，master的优先级比backup的大    advert_int 1      // 默认心跳间隔    authentication {  // 认证机制        auth_type PASS        auth_pass 1111   // 密码    }    virtual_ipaddress {  // 虚拟地址vip       172.16.2.8    }}
```

其中检测脚本 `nginx_check.sh`，这里提供一个：

```
#!/bin/bashA=`ps -C nginx --no-header | wc -l`if [ $A -eq 0 ];then    /usr/sbin/nginx # 尝试重新启动nginx    sleep 2         # 睡眠2秒    if [ `ps -C nginx --no-header | wc -l` -eq 0 ];then        killall keepalived # 启动失败，将keepalived服务杀死。将vip漂移到其它备份节点    fifi
```

复制一份到备份服务器，备份 nginx 的配置要将 `state` 后改为 `BACKUP`，`priority` 改为比主机小。设置完毕后各自 `service keepalived start` 启动，经过访问成功之后，可以把 Master 机的 keepalived 停掉，此时 Master 机就不再是主机了 `service keepalived stop`，看访问虚拟 IP 时是否能够自动切换到备机 ip addr。

再次启动 Master 的 keepalived，此时 vip 又变到了主机上。

配置高可用集群的内容来源于：[Nginx 从入门到实践，万字详解！](https://juejin.im/post/6844904144235413512#heading-11)

## 4. **其它功能和技巧**

### 4.1 代理缓存

nginx 的 http_proxy 模块，提供类似于 Squid 的缓存功能，使用 proxy_cache_path 来配置。

nginx 可以对访问过的内容在 nginx 服务器本地建立副本，这样在一段时间内再次访问该数据，就不需要通过 nginx 服务器再次向后端服务器发出请求，减小数据传输延迟，提高访问速度：

```
proxy_cache_path usr/local/cache levels=1:2 keys_zone=my_cache:10m;server {  listen       80;  server_name  test.com;  location / {      proxy_cache my_cache;      proxy_pass http://127.0.0.1:8888;      proxy_set_header Host $host;  }}
```

上面的配置表示：nginx 提供一块 10 M 的内存用于缓存，名字为 my_cache, levels 等级为 1:2，缓存存放的路径为 `usr/local/cache`。

### 4.2 访问日志

访问日志默认是注释的状态，需要可以打开和进行更详细的配置，一下是 nginx 的默认配置：

```
http {    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '                      '$status $body_bytes_sent "$http_referer" '                      '"$http_user_agent" "$http_x_forwarded_for"';    access_log  logs/access.log  main;}
```

### 4.3 错误日志

错误日志放在 main 全局区块中，童鞋们打开 nginx.conf 就可以看见在配置文件中和下面一样的代码了：

```
#error_log  logs/error.log;#error_log  logs/error.log  notice;#error_log  logs/error.log  info;
```

nginx 错误日志默认配置为：

```
error_log logs/error.log error;
```

### 4.4 静态资源服务器

```
server {    listen       80;    server_name  static.bin;    charset utf-8;    # 防止中文文件名乱码    location /download {        alias           /usr/share/nginx/static;  # 静态资源目录        autoindex               on;    # 开启静态资源列目录，浏览目录权限        autoindex_exact_size    off;   # on(默认)显示文件的确切大小，单位是byte；off显示文件大概大小，单位KB、MB、GB        autoindex_localtime     off;   # off(默认)时显示的文件时间为GMT时间；on显示的文件时间为服务器时间    }}
```

### 4.5 禁止指定 user_agent

nginx 可以禁止指定的浏览器和爬虫框架访问：

```
# http_user_agent 为浏览器标识# 禁止 user_agent 为baidu、360和sohu，~*表示不区分大小写匹配if ($http_user_agent ~* 'baidu|360|sohu') {    return 404;}# 禁止 Scrapy 等工具的抓取if ($http_user_agent ~* (Scrapy|Curl|HttpClient)) {    return 403;
```

### 4.6 请求过滤

**根据请求类型过滤**

```
# 非指定请求全返回 403if ( $request_method !~ ^(GET|POST|HEAD)$ ) {    return 403;}
```

**根据状态码过滤**

```
error_page 502 503 /50x.html;location = /50x.html {    root /usr/share/nginx/html;}
```

这样实际上是一个内部跳转，当访问出现 502、503 的时候就能返回 50x.html 中的内容，这里需要注意是否可以找到 50x.html 页面，所以加了个 location 保证找到你自定义的 50x 页面。

**根据 URL 名称过滤**

```
if ($host = zy.com' ) {     #其中 $1是取自regex部分()里的内容,匹配成功后跳转到的URL。     rewrite ^/(.*)$  http://www.zy.com/$1  permanent；}location /test {    // /test 全部重定向到首页    rewrite  ^(.*)$ /index.html  redirect;}
```

### 4.7 ab 命令

ab 命令全称为：Apache bench，是 Apache 自带的压力测试工具，也可以测试 Nginx、IIS 等其他 Web 服务器:

- -n 总共的请求数
- -c 并发的请求数
- -t 测试所进行的最大秒数，默认值 为 50000
- -p 包含了需要的 POST 的数据文件
- -T POST 数据所使用的 Content-type 头信息

```
ab -n 1000 -c 5000 http://127.0.0.1/ # 每次发送1000并发的请求数，请求数总数为5000。
```

测试前需要安装 httpd-tools：`yum install httpd-tools`

### 4.8 泛域名路径分离

这是一个非常实用的技能，经常有时候我们可能需要配置一些二级或者三级域名，希望通过 nginx 自动指向对应目录，比如：

1. test1.doc.test.club 自动指向 /usr/local/html/doc/test1 服务器地址；
2. test2.doc.test.club 自动指向 /usr/local/html/doc/test2 服务器地址；

```
server {    listen       80;    server_name  ~^([\w-]+)\.doc\.test\.club$;    root /usr/local/html/doc/$1;}
```

### 4.9 泛域名转发

和之前的功能类似，有时候我们希望把二级或者三级域名链接重写到我们希望的路径，让后端就可以根据路由解析不同的规则：

1. test1.serv.test.club/api?name=a 自动转发到 127.0.0.1:8080/test1/api?name=a
2. test2.serv.test.club/api?name=a 自动转发到 127.0.0.1:8080/test2/api?name=a

```
server {    listen       80;    server_name ~^([\w-]+)\.serv\.test\.club$;    location / {        proxy_set_header        X-Real-IP $remote_addr;        proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;        proxy_set_header        Host $http_host;        proxy_set_header        X-NginX-Proxy true;        proxy_pass              http://127.0.0.1:8080/$1$request_uri;    }}
```

## 5.**常见问题**

### 5.1 nginx 中怎么设置变量

或许你不知道，nginx 的配置文件使用的是一门微型的编程语言。既然是编程语言，一般也就少不了“变量”这种东西，但是在 nginx 配置中，变量只能存放一种类型的值，因为也只存在一种类型的值，那就是字符串。

例如我们在 nginx.conf 中有这样一行配置：

```
set $name "chroot";
```

上面使用了 set 配置指令对变量 `$name`进行了赋值操作，把 “chroot” 赋值给了 `$name`。nginx 变量名前面有一个 `$` 符号，这是记法上的要求。所有的 Nginx 变量在 Nginx 配置文件中引用时都须带上 `$` 前缀。这种表示方法和 Perl、PHP 这些语言是相似的。

这种表示方法的用处在哪里呢，那就是可以直接把变量嵌入到字符串常量中以构造出新的字符串，例如你需要进行一个字符串拼接：

```
server {  listen       80;  server_name  test.com;  location / {     set $temp hello;     return "$temp world";  }}
```

以上当匹配成功的时候就会返回字符串 “hello world” 了。需要注意的是，当引用的变量名之后紧跟着变量名的构成字符时（比如后跟字母、数字以及下划线），我们就需要使用特别的记法来消除歧义，例如：

```
server {  listen       80;  server_name  test.com;  location / {     set $temp "hello ";     return "${temp}world";  }}
```

这里，我们在配置指令的参数值中引用变量 `$temp` 的时候，后面紧跟着 `world` 这个单词，所以如果直接写作 `"$tempworld"` 则 nginx 的计算引擎会将之识别为引用了变量 `$tempworld`. 为了解决这个问题，nginx 的字符串支持使用花括号在 `$` 之后把变量名围起来，比如这里的 `${temp}`，所以 上面这个例子返回的还是 “hello world”：

```
$ curl 'http://test.com/'    hello world
```

还需要注意的是，若是想输出 `$` 符号本身，可以这样做：

```
geo $dollar {    default "$";}server {    listen       80;    server_name  test.com;    location / {        set $temp "hello ";        return "${temp}world: $dollar";    }}
```

上面用到了标准模块 ngx_geo 提供的配置指令 geo 来为变量 `$dollar` 赋予字符串 `"$"` ，这样，这里的返回值就是 “hello world: $” 了。

## 6.**附 nginx 内置预定义变量**

按字母顺序，变量名与对应定义：

- `$arg_PARAMETER` #GET 请求中变量名 PARAMETER 参数的值
- `$args` #这个变量等于 GET 请求中的参数，例如，foo=123&bar=blahblah;这个变量可以被修改
- `$binary_remote_addr` #二进制码形式的客户端地址
- `$body_bytes_sent` #传送页面的字节数
- `$content_length` #请求头中的 Content-length 字段
- `$content_type` #请求头中的 Content-Type 字段
- `$cookie_COOKIE` #cookie COOKIE 的值
- `$document_root` #当前请求在 root 指令中指定的值
- `$document_uri` #与 $uri 相同
- `$host` #请求中的主机头(Host)字段，如果请求中的主机头不可用或者空，则为处理请求的 server 名称(处理请求的 server 的 server_name 指令的值)。值为小写，不包含端口
- `$hostname` #机器名使用 gethostname 系统调用的值
- `$http_HEADER` #HTTP 请求头中的内容，HEADER 为 HTTP 请求中的内容转为小写，-变为*(破折号变为下划线)，例如：`$http*user_agent`(Uaer-Agent 的值)
- `$sent_http_HEADER` #HTTP 响应头中的内容，HEADER 为 HTTP 响应中的内容转为小写，-变为*(破折号变为下划线)，例如：`$sent*http_cache_control`、`$sent_http_content_type`…
- `$is_args` #如果 $args 设置，值为”?”，否则为””
- `$limit_rate` #这个变量可以限制连接速率
- `$nginx_version` #当前运行的 nginx 版本号
- `$query_string` #与 $args 相同
- `$remote_addr` #客户端的 IP 地址
- `$remote_port` #客户端的端口
- `$remote_port` #已经经过 Auth Basic Module 验证的用户名
- `$request_filename` #当前连接请求的文件路径，由 root 或 alias 指令与 URI 请求生成
- `$request_body` #这个变量（0.7.58+）包含请求的主要信息。在使用 proxy_pass 或 fastcgi_pass 指令的 location 中比较有意义
- `$request_body_file` #客户端请求主体信息的临时文件名
- `$request_completion` #如果请求成功，设为”OK”；如果请求未完成或者不是一系列请求中最后一部分则设为空
- `$request_method` #这个变量是客户端请求的动作，通常为 GET 或 POST。包括 0.8.20 及之前的版本中，这个变量总为 main request 中的动作，如果当前请求是一个子请求，并不使用这个当前请求的动作
- `$request_uri` #这个变量等于包含一些客户端请求参数的原始 URI，它无法修改，请查看 $uri 更改或重写 URI
- `$scheme` #所用的协议，例如 http 或者是 https，例如 `rewrite ^(.+)$$scheme://example.com$1 redirect`
- `$server_addr` #服务器地址，在完成一次系统调用后可以确定这个值，如果要绕开系统调用，则必须在 listen 中指定地址并且使用 bind 参数
- `$server_name` #服务器名称
- `$server_port` #请求到达服务器的端口号
- `$server_protocol` #请求使用的协议，通常是 HTTP/1.0、HTTP/1.1 或 HTTP/2
- `$uri` #请求中的当前 URI(不带请求参数，参数位于 args ) ， 不 同 于 浏 览 器 传 递 的 args)，不同于浏览器传递的 args)，不同于浏览器传递的 request_uri 的值，它可以通过内部重定向，或者使用 index 指令进行修改。不包括协议和主机名，例如 /foo/bar.html

## 7.**附 nginx 模块**

### 7.1 nginx 模块分类

- 核心模块：nginx 最基本最核心的服务，如进程管理、权限控制、日志记录；
- 标准 HTTP 模块：nginx 服务器的标准 HTTP 功能；
- 可选 HTTP 模块：处理特殊的 HTTP 请求
- 邮件服务模块：邮件服务
- 第三方模块：作为扩展，完成特殊功能

### 7.2 模块清单

**核心模块**：

- ngx_core
- ngx_errlog
- ngx_conf
- ngx_events
- ngx_event_core
- ngx_epll
- ngx_regex

**标准 HTTP 模块**：

- ngx_http
- ngx_http_core #配置端口，URI 分析，服务器相应错误处理，别名控制 (alias) 等
- ngx_http_log #自定义 access 日志
- ngx_http_upstream #定义一组服务器，可以接受来自 proxy, Fastcgi,Memcache 的重定向；主要用作负载均衡
- ngx_http_static
- ngx_http_autoindex #自动生成目录列表
- ngx_http_index #处理以/结尾的请求，如果没有找到 index 页，则看是否开启了 random_index；如开启，则用之，否则用 autoindex
- ngx_http_auth_basic #基于 http 的身份认证 (auth_basic)
- ngx_http_access #基于 IP 地址的访问控制 (deny,allow)
- ngx_http_limit_conn #限制来自客户端的连接的响应和处理速率
- ngx_http_limit_req #限制来自客户端的请求的响应和处理速率
- ngx_http_geo
- ngx_http_map #创建任意的键值对变量
- ngx_http_split_clients
- ngx_http_referer #过滤 HTTP 头中 Referer 为空的对象
- ngx_http_rewrite #通过正则表达式重定向请求
- ngx_http_proxy
- ngx_http_fastcgi #支持 fastcgi
- ngx_http_uwsgi
- ngx_http_scgi
- ngx_http_memcached
- ngx_http_empty_gif #从内存创建一个 1×1 的透明 gif 图片，可以快速调用
- ngx_http_browser #解析 http 请求头部的 User-Agent 值
- ngx_http_charset #指定网页编码
- ngx_http_upstream_ip_hash
- ngx_http_upstream_least_conn
- ngx_http_upstream_keepalive
- ngx_http_write_filter
- ngx_http_header_filter
- ngx_http_chunked_filter
- ngx_http_range_header
- ngx_http_gzip_filter
- ngx_http_postpone_filter
- ngx_http_ssi_filter
- ngx_http_charset_filter
- ngx_http_userid_filter
- ngx_http_headers_filter #设置 http 响应头
- ngx_http_copy_filter
- ngx_http_range_body_filter
- ngx_http_not_modified_filter

**可选 HTTP 模块**:

- ngx_http_addition #在响应请求的页面开始或者结尾添加文本信息
- ngx_http_degradation #在低内存的情况下允许服务器返回 444 或者 204 错误
- ngx_http_perl
- ngx_http_flv #支持将 Flash 多媒体信息按照流文件传输，可以根据客户端指定的开始位置返回 Flash
- ngx_http_geoip #支持解析基于 GeoIP 数据库的客户端请求
- ngx_google_perftools
- ngx_http_gzip #gzip 压缩请求的响应
- ngx_http_gzip_static #搜索并使用预压缩的以.gz 为后缀的文件代替一般文件响应客户端请求
- ngx_http_image_filter #支持改变 png，jpeg，gif 图片的尺寸和旋转方向
- ngx_http_mp4 #支持.mp4,.m4v,.m4a 等多媒体信息按照流文件传输，常与 ngx_http_flv 一起使用
- ngx_http_random_index #当收到 / 结尾的请求时，在指定目录下随机选择一个文件作为 index
- ngx_http_secure_link #支持对请求链接的有效性检查
- ngx_http_ssl #支持 https
- ngx_http_stub_status
- ngx_http_sub_module #使用指定的字符串替换响应中的信息
- ngx_http_dav #支持 HTTP 和 WebDAV 协议中的 PUT/DELETE/MKCOL/COPY/MOVE 方法
- ngx_http_xslt #将 XML 响应信息使用 XSLT 进行转换

**邮件服务模块**:

- ngx_mail_core
- ngx_mail_pop3
- ngx_mail_imap
- ngx_mail_smtp
- ngx_mail_auth_http
- ngx_mail_proxy
- ngx_mail_ssl

**第三方模块**：

- echo-nginx-module #支持在 nginx 配置文件中使用 echo/sleep/time/exec 等类 Shell 命令
- memc-nginx-module
- rds-json-nginx-module #使 nginx 支持 json 数据的处理
- lua-nginx-module

原文作者：chrootliu，腾讯 QQ 音乐前端开发工程师

原文链接：https://mp.weixin.qq.com/s/LmtHTOVOvdcnMBuxv7a9_A

# 【NO.96】基于libco的c++协程实现（时间轮定时器）

## 1.**在后端的开发中，定时器有很广泛的应用。**

比如：

心跳检测

倒计时

游戏开发的技能冷却

redis的键值的有效期等等，都会使用到定时器。

## 2.定时器的实现数据结构选择

红黑树

对于增删查，时间复杂度为O(logn)，对于红黑树最⼩节点为最左侧节点，时间复杂度O(logn)

最小堆

对于增查，时间复杂度为O(logn)，对于删时间复杂度为O(n)，但是可以通过辅助数据结构（ map 或者hashtable来快速索引节点）来加快删除操作；对于最⼩节点为根节点，时间复杂度为O(1)

跳表

对于增删查，时间复杂度为O(logn)，对于跳表最⼩节点为最左侧节点，时间复杂度为O(1)，但是空间复杂度⽐较⾼，为O(1.5n)

时间轮

对于增删查，时间复杂度为O(1)，查找最⼩节点也为O(1)

## 3.libco的使用了时间轮的实现

首先，时间轮有几个结构，必须理清他们的关系。

```text
struct stTimeoutItem_t
{
	enum { eMaxTimeout = 40 * 1000 };	// 40s
	stTimeoutItem_t* pPrev;				// 前
	stTimeoutItem_t* pNext;				// 后
	stTimeoutItemLink_t* pLink;			// 链表，没有用到，写这里有毛用
 
	OnPreparePfn_t pfnPrepare;			// 不是超时的事件的处理函数
	OnProcessPfn_t pfnProcess;			// resume协程回调函数
 
	void* pArg;							// routine 协程对象指针
	bool bTimeout;						// 是否超时
	unsigned long long ullExpireTime;	// 到期时间
};
 
struct stPoll_t;
struct stPollItem_t : public stTimeoutItem_t
{
	struct pollfd* pSelf;			// 对应的poll结构
	stPoll_t* pPoll;				// 所属的stPoll_t
	struct epoll_event stEvent;		// epoll事件，poll转换过来的
};
 
// co_poll_inner 创建，管理这多个stPollItem_t
struct stPoll_t : public stTimeoutItem_t
{
	struct pollfd* fds;				// poll 的fd集合
	nfds_t nfds;					// poll 事件个数
	stPollItem_t* pPollItems;		// 要加入epoll 事件
	int iAllEventDetach;			// 如果处理过该对象的子项目pPollItems，赋值为1
	int iEpollFd;					// epoll fd句柄
	int iRaiseCnt;					// 此次触发的事件数
};
```

我把这几个结构拉一起了，

![img](https://pic3.zhimg.com/80/v2-6b637f3ab664dc6a78d39d72e0b887f2_720w.webp)

其中，能看出，stCoEpool_t管理了这一切

```text
// TimeoutItem的链表
struct stTimeoutItemLink_t
{
	stTimeoutItem_t* head;
	stTimeoutItem_t* tail;
};
 
// TimeOut 
struct stTimeout_t	// 时间伦
{
	stTimeoutItemLink_t* pItems;	// 时间轮链表，开始初始化分配只一圈的长度，后续直接使用
	int iItemSize;					// 超时链表中一圈的tick 60*1000
	unsigned long long ullStart;	// 时间轮开始时间，会一直变化
	long long llStartIdx;			// 时间轮开始的下标，会一直变化
};
 
// epoll 结构
struct stCoEpoll_t
{
	int iEpollFd;
	static const int _EPOLL_SIZE = 1024 * 10;
	struct stTimeout_t* pTimeout;					// epoll 存着时间轮
	struct stTimeoutItemLink_t* pstTimeoutList;		// 超时事件链表
	struct stTimeoutItemLink_t* pstActiveList;		// 用于signal时会插入
	co_epoll_res* result;
};
```

也就是说，一个[协程](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3D%E5%8D%8F%E7%A8%8B%26spm%3D1001.2101.3001.7020)，就有一个，在co_init_curr_thread_env 中创建

它管理着超时链表，信号事件链表

其中的pTimeout，就是时间轮，也就是一个数组，这个数组的大小位60*1000

![img](https://pic3.zhimg.com/80/v2-2f7dd50629e3a224c0953a2e5d82ed22_720w.webp)

```text
stTimeout_t *AllocTimeout( int iSize )
{
	stTimeout_t *lp = (stTimeout_t*)calloc( 1,sizeof(stTimeout_t) );
	lp->iItemSize = iSize;
	// 注意这里先把item分配好了，后续直接使用
	lp->pItems = (stTimeoutItemLink_t*)calloc( 1, sizeof(stTimeoutItemLink_t) * lp->iItemSize );
	lp->ullStart = GetTickMS();
	lp->llStartIdx = 0;
	return lp;
}
```

这就是分配的时间轮的方法，首先指定了下标时间等信息，根据结构注释应该不难懂

有了这些后，再来看看时怎么添加超时事件的

```text
// apTimeout：时间轮
// apItem: 某一个定时item
// allNow：当前的时间
// 函数目的，将超时项apItem加入到apTimeout
int AddTimeout( stTimeout_t *apTimeout, stTimeoutItem_t *apItem ,unsigned long long allNow )
{
	// 这个判断有点多余，start正常已经分配了
	if( apTimeout->ullStart == 0 )
	{
		apTimeout->ullStart = allNow;
		apTimeout->llStartIdx = 0;
	}
	// 当前时间也不大可能比前面的时间大
	if( allNow < apTimeout->ullStart )
	{
		co_log_err("CO_ERR: AddTimeout line %d allNow %llu apTimeout->ullStart %llu",
					__LINE__,allNow,apTimeout->ullStart);
 
		return __LINE__;
	}
	if( apItem->ullExpireTime < allNow )
	{
		co_log_err("CO_ERR: AddTimeout line %d apItem->ullExpireTime %llu allNow %llu apTimeout->ullStart %llu",
					__LINE__,apItem->ullExpireTime,allNow,apTimeout->ullStart);
 
		return __LINE__;
	}
	// 到期时间到start的时间差
	unsigned long long diff = apItem->ullExpireTime - apTimeout->ullStart;
	// itemsize 实际上是毫秒数，如果超出了，说明设置的超时时间过长
	if( diff >= (unsigned long long)apTimeout->iItemSize )
	{
		diff = apTimeout->iItemSize - 1;
		co_log_err("CO_ERR: AddTimeout line %d diff %d",
					__LINE__,diff);
 
		//return __LINE__;
	}
	// 将apItem加到末尾
	AddTail( apTimeout->pItems + ( apTimeout->llStartIdx + diff ) % apTimeout->iItemSize , apItem );
 
	return 0;
}
```

其实，这里有个概念，stTimeoutItemLink_t 与stTimeoutItem_t，也就是说，stTimeout_t里面管理的时60*1000个链表，而每个链表有一个或者多个stTimeoutItem_t，下面这个函数，就是把节点Item加入到链表的方法。

```text
template <class TNode,class TLink>
void inline AddTail(TLink*apLink, TNode *ap)
{
	if( ap->pLink )
	{
		return ;
	}
	if(apLink->tail)
	{
		apLink->tail->pNext = (TNode*)ap;
		ap->pNext = NULL;
		ap->pPrev = apLink->tail;
		apLink->tail = ap;
	}
	else
	{
		apLink->head = apLink->tail = ap;
		ap->pNext = ap->pPrev = NULL;
	}
	ap->pLink = apLink;
}
```

![img](https://pic1.zhimg.com/80/v2-2e3b70df0f91eb6f537309ecea28977c_720w.webp)

到这里，基本把一个超时事件添加到时间轮中了，这时就应该切换协程了co_yield_env

```text
	int ret = AddTimeout( ctx->pTimeout, &arg, now );
	int iRaiseCnt = 0;
	if( ret != 0 )
	{
		co_log_err("CO_ERR: AddTimeout ret %d now %lld timeout %d arg.ullExpireTime %lld",
				ret,now,timeout,arg.ullExpireTime);
		errno = EINVAL;
		iRaiseCnt = -1;
	}
    else
	{
		co_yield_env( co_get_curr_thread_env() );
		iRaiseCnt = arg.iRaiseCnt;
	}
```

接下来，看怎么检测超时事件co_eventloop

```text
    for(;;)
	{
		// 等待事件或超时1ms
		int ret = co_epoll_wait( ctx->iEpollFd,result,stCoEpoll_t::_EPOLL_SIZE, 1 );
		
        //  遍历所有ret事件处理
		for(int i=0;i<ret;i++)
		{
			pfnPrepare(xxx)
		}
 
		// 取出所有的超时时间item，设置为超时
		TakeAllTimeout( ctx->pTimeout, now, plsTimeout );
		stTimeoutItem_t *lp = plsTimeout->head;
		while( lp )
		{
			lp->bTimeout = true;
			lp = lp->pNext;
		}
 
		// 将超时链表plsTimeout加入到plsActive
		Join<stTimeoutItem_t, stTimeoutItemLink_t>( plsActive, plsTimeout );
		lp = plsActive->head;
		while( lp )
		{
            // 弹出链表头，处理超时事件
			PopHead<stTimeoutItem_t,stTimeoutItemLink_t>( plsActive );
            if (lp->bTimeout && now < lp->ullExpireTime) 
			{
				int ret = AddTimeout(ctx->pTimeout, lp, now);
				if (!ret) 
				{
					lp->bTimeout = false;
					lp = plsActive->head;
					continue;
				}
			}
            // 只有stPool_t 才需要切协程，要切回去了
			if( lp->pfnProcess )
			{
				lp->pfnProcess( lp );
			}
			lp = plsActive->head;
		}
 
		// 如果传入该函数指针，则可以控制event_loop 退出
		if( pfn )
		{
			if( -1 == pfn( arg ) )
			{
				break;
			}
		}
	}
```

其中包括了定时事件处理，协程切换，主协程退出等操作。如果设置了主协程退出函数，则主协程可以正常的退出。

原文地址：https://zhuanlan.zhihu.com/p/573575861

作者：linux

# 【NO.97】Linux文件系统、磁盘I/O是怎么工作的

同前面讲述的CPU、内存一样，文件系统和磁盘I/O，也是Linux操作系统最核心的功能。

- 磁盘为系统提供了最基本的持久化存储。
- 文件系统则在磁盘基础上，提供了一个用来管理文件的树状结构。

## 1.文件系统

### 1.1 索引节点和目录项

Linux中的一切都由统一的文件系统来管理，包括普通的文件和目录，以及块设备、套接字、管道等。Linux文件系统为每个文件都分配了两个数据结构，索引节点（index node）和目录项（directory entry），主要用来记录文件的元信息和目录结构。

- 索引节点，简称为 inode，用来记录文件的元数据，比如inode编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样会被持久化到磁盘，所以，索引节点同样占磁盘。
- 目录项，简称为dentry，用来记录的文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构，它是由内核维护的一个内存数据结构，通常也被称为目录项缓存。

换句话说，索引节点是每个文件的唯一标志，目录项维护的是文件系统的树状结构。目录项和索引节点的关系是多对一，或者可理解为一个文件多个别名。举个例子，通过硬链接为文件创建的别名，就会对应不同目录项，这些目录项本质上是连接同一个文件，所以索引节点相同。

更具体地说，文件数据是怎么存储的，是直接保存到磁盘的？实际上磁盘读写的最小单位是扇区，扇区只有512B大小，如果每次读写这么小的单位，效率一定很低。所以，文件系统又把连续的扇区组成逻辑块，再以逻辑块为最小单元去管理数据。常见的逻辑块大小是4KB，即连续的8个扇区。下面展示一张示意图：

![img](https://pic3.zhimg.com/80/v2-a0d60e148b7231c48dea987b98ea0e2a_720w.webp)

这里需要注意两点：

第一，目录项本身在内存中，索引节点在磁盘中。前面的 Buffer 和 Cache 原理中提到，为了协调慢速磁盘和快速CPU之间的性能差异，文件内容会缓存到页缓存 Cache中。索引节点自然也会缓存到内存中，增加速文件访问。

第二，磁盘在执行文件系统格式化时，会被分成三个存储区域，超级块、索引节点区 和 数据块区。其中，超级块存储整个文件系统状态；索引节点区存储索引节点；数据块区，存储文件数据。

### 1.2 虚拟文件系统

目录项、索引节点、超级块、逻辑块构成Linux文件系统四大基本要素。不过，为了支持各种不同的文件系统，Linux内核在用户进程和文件系统中间，引入了一个抽象层，即虚拟文件系统VFS。VFS定义了一套所有文件系统都支持的数据结构和标准接口。这样，用户层和内核其他子系统都只需要跟 VFS 提供的统一接口交互就可以了，不需要关心底层各种文件系统的实现细节。下图很好展示了Linux文件系统的架构图，能更好的帮助理解系统调用、VFS、缓存、文件系统以及块存储之间的关系：

![img](https://pic1.zhimg.com/80/v2-12186438990ed22398df8cc5867ef630_720w.webp)

从图中可以看到，在VFS下面Linux可以支持各种文件系统，按照存储位置的不同，可以分为三类：

- 基于磁盘的文件系统，也就是把数据直接存储到计算机本地挂载的磁盘中。如 EXT4、XFS、OverlayFS等。
- 基于内存的文件系统，也就是虚拟文件系统，不需要磁盘分配任何存储空间，只占用内存。如 /proc 文件系统、/sys 文件系统（主要向用户空间导出层次化的内核对象）。
- 网络文件系统，用来访问其他计算机数据的文件系统，如 NFS、SMB、iSCSI等。

这些文件系统，要先挂载到 VFS 目录树中的子目录（挂载点），然后才能访问其中文件。比如安装系统时，要先挂在一个根目录（ / ），在根目录下，再把其他文件系统挂在进来。

### 1.3 文件系统I/O

把文件挂到挂载点后，就能通过它去访问它管理的文件了。VFS提供的访问文件的标准接口，以系统调用的方式提供给应用程序使用。比如，cat命令，相继调用 open()、read()、write()。文件读写方式的各种差异，也导致I/O 的分类多种多样。常见的有，缓冲与非缓冲I/O、直接与非直接I/O、阻塞与非阻塞I/O、同步与异步I/O等。下面详细解释下这四种 I/O分类：

第一种，根据是否利用标准库缓存，可以把文件I/O 分为 缓冲I/O 和 非缓冲I/O。这里的“缓冲”，其实指的是标准库内部实现的缓存。例如，很多程序遇到换行时才真正输出，换行前的内容，就是被标准库暂时缓存起来。因此，缓冲I/O 指的是利用标准库缓存来加速文件的访问，在标准库内部再通过系统调用访问文件；非缓冲I/O 指的是直接通过系统调用访问文件，而不通过标准库缓存。无论是缓冲还是非缓冲 I/O，最后都是通过系统调用访问文件。而根据前面内容，系统调用后，还通过页缓存，来减少磁盘I/O操作。

第二种，根据是否利用操作系统的页缓存，可以把文件I/O 分为直接I/O 和 非直接I/O。想要实现直接I/O，需要在系统调用中指定标志 O_DIRECT，如果不指定，默认是非直接I/O。不过注意，这里的直接、非直接I/O，其实最终还是和文件系统交互。如果实在数据库等场景中，还会看到，跳过文件系统读写磁盘的情况，即裸I/O。

第三种，根据应用程序是否阻塞自身运行，可以把文件I/O 分为阻塞I/O 和 非阻塞I/O。在应用程序执行I/O 操作后，如果没获得响应，就阻塞当前线程，自然不能执行其他任务，这是阻塞I/O；如果没获得响应，却不阻塞当前线程，继续执行其他任务，随后通过轮询或者时间通知的形式，获得之前调用的结果。比如，访问管道或者网络套接字时，设置 O_NONBLOCK 标志，表示非阻塞方式访问，若不做任何设置，默认就是阻塞方式访问。

第四种，根据是否等待响应结果，可以把文件I/O 分为同步I/O 和 异步I/O。在应用程序执行I/O 操作后，如果一直等到 整个 I/O完成后才获得响应，就是同步I/O；如果不等待 I/O 完成以及完成后的响应，继续往下执行，等到 I/O 完成后，响应会用事件通知的方式，告诉应用程序。比如，在操作文件时，如果设置了 O_SYNC 或 O_DSYNC标志，就代表同步I/O，后者是等待文件数据写入磁盘后才返回，而前者是在后者基础上，要求文件元数据也要写入磁盘后才能返回。再比如，在访问管道或者网络套接字时，设置选项 O_ASYNC后，就是异步 I/O内核会通过 SIGIO 或者 SIGPOLL，来通知进程，文件是否可读写。

总之，无论是普通文件和块设备、还是网络套接字和管道等，都通过统一的VFS接口来被访问。

### **1.4 文件系统性能观测**

```text
$ df /dev/sda1 
Filesystem     1K-blocks    Used Available Use% Mounted on 
/dev/sda1       30308240 3167020  27124836  11% /
 
$ df -h /dev/sda1 
Filesystem      Size  Used Avail Use% Mounted on 
/dev/sda1        29G  3.1G   26G  11% / 
 
$ df -i /dev/sda1 
Filesystem      Inodes  IUsed   IFree IUse% Mounted on 
/dev/sda1      3870720 157460 3713260    5% /
```

加上-i 参数查看索引节点的使用情况，索引节点的容量，（也就是 Inode个数）是在格式化磁盘时设定好的，由格式化工具自动生成。当你发现索引节点空间不足时，但磁盘空间充足时，很可能是过多的小文件导致的，一般的删除它们或者移到其他的索引节点充足的磁盘上，就能解决问题。

接下来，文件系统的目录项和索引节点的缓存，如何查看呢？

实际上，内核使用 Slab 机制，管理目录项和索引节点的缓存。/proc/meminfo 只给出了Slab整体大小，具体到每一种Slab缓存，就要查看 /proc/slabinfo。运行下面命令可以得到，所有目录项和各种文件系统的索引节点的缓存情况：

```text
$ cat /proc/slabinfo | grep -E '^#|dentry|inode' 
# name            <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab> : tunables <limit> <batchcount> <sharedfactor> : slabdata <active_slabs> <num_slabs> <sharedavail> 
xfs_inode              0      0    960   17    4 : tunables    0    0    0 : slabdata      0      0      0 
... 
ext4_inode_cache   32104  34590   1088   15    4 : tunables    0    0    0 : slabdata   2306   2306      0hugetlbfs_inode_cache     13     13    624   13    2 : tunables    0    0    0 : slabdata      1      1      0 
sock_inode_cache    1190   1242    704   23    4 : tunables    0    0    0 : slabdata     54     54      0 
shmem_inode_cache   1622   2139    712   23    4 : tunables    0    0    0 : slabdata     93     93      0 
proc_inode_cache    3560   4080    680   12    2 : tunables    0    0    0 : slabdata    340    340      0 
inode_cache        25172  25818    608   13    2 : tunables    0    0    0 : slabdata   1986   1986      0 
dentry             76050 121296    192   21    1 : tunables    0    0    0 : slabdata   5776   5776      0 
```

dentry 行表示目录项缓存，inode_cache 行，表示VFS 索引节点缓存，其余的则是各种文件系统的缓存。这里列比较多，可查询man slabinfo。实际性能分析时，更多使用 slabtop，来找到占用内存最多的缓存类型：

```text
# 按下c按照缓存大小排序，按下a按照活跃对象数排序 
$ slabtop 
Active / Total Objects (% used)    : 277970 / 358914 (77.4%) 
Active / Total Slabs (% used)      : 12414 / 12414 (100.0%) 
Active / Total Caches (% used)     : 83 / 135 (61.5%) 
Active / Total Size (% used)       : 57816.88K / 73307.70K (78.9%) 
Minimum / Average / Maximum Object : 0.01K / 0.20K / 22.88K 
 
  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME 
69804  23094   0%    0.19K   3324       21     13296K dentry 
16380  15854   0%    0.59K   1260       13     10080K inode_cache 
58260  55397   0%    0.13K   1942       30      7768K kernfs_node_cache 
   485    413   0%    5.69K     97        5      3104K task_struct 
  1472   1397   0%    2.00K     92       16      2944K kmalloc-2048 
```

从这个结果可以看到，目录项和索引节点占用最多的 Slab缓存，但其实并不大，约23MB。

思考：find / -name file-name 命令导致会不会导致缓存升高，如果会，导致哪类缓存升高呢？

find / -name 命令是全盘扫描（包括内存文件系统、磁盘文件系统等），所以这里会导致 xfs_inode 、proc_inode_cache、dentry、 inode_cache这几类缓存的升高，而且在下次执行 find 命令时，就会快很多，因为它大部分会直接在缓存中查找结果。这里你可以在执行find命令前后，比较slabtop、free、vmstat输出结果，又会有更深的理解。

## 2.**磁盘 I/O**

### 2.**1 磁盘**

首先，**根据存储介质的不同**，可以分为两类，**机械磁盘 和 固态磁盘**。

机械磁盘：也称为硬盘驱动器（Hard Disk Driver，缩写HDD），机械磁盘由盘片和读写磁头组成，数据存储在盘片的环状磁道中，最小读写单位 扇区，一般大小为512B。在读写数据时，需要移动磁头，定位到数据所在的盘片磁道中，然后才访问数据。如果 I/O 请求刚好连续，那就不需要磁道寻址，可获得最佳性能，这就是顺序I/O 的工作原理。随机 I/O，需要不停地移动磁头，来定位数据位置，读写速度比较慢。

固态磁盘：Solid State Driver，缩写SSD，由固态电子元器件组成，最小读写单位 页，一般大小4KB、8KB等。固态磁盘不需要磁道寻址，不管是连续I/O，还是随机I/O的性能，都比机械磁盘好得多。

另外，相同磁盘的顺序I/O 都要比 随机I/O 快得多，原因如下：

- 对于机械磁盘来说，随机 I/O需要更多的磁头寻道和盘片旋转，性能比顺序I/O 慢。
- 对于固态盘来说，虽然随机I/O 性能比机械盘好很多，但是它也会有“先擦除、再写入”的限制。随机读写也有大量的垃圾回收，所以还是会比顺序I/O 慢很多。
- 另外，顺序I/O 可以通过预读的方式，来减少 I/O请求的次数，这也是其性能优异的原因之一。

在上一节提到过，如果每次都读写 512B 数据，效率会很低。文件系统会把连续的扇区或页组成逻辑块，作为最小单元管理数据，常见的逻辑块是 4KB，即连续的8个扇区，或者一个页。

其次，还可以按照接口来分类，可以把硬盘分为 IDE、SCSI、SAS、SATA、FC等。不同的接口，分配不同的设备名称。比如 IDE的会分配一个前缀为 hd 的设备名，SCSI 和 SATA会分配一个 sd 前缀的设备名。如果是多块同类型的磁盘，会按照a、b、c等字母顺序编号。

第三，还可以根据使用方式，将磁盘划分为不同架构。最简单的就是，作为独立磁盘来使用。然后再根据需要，将磁盘划分成多个逻辑分区，再给分区编号。比如前面多次用到的 /dev/sda，还可以分成两个分区 /dev/sda1 和 /dev/sda2。另一个比较常用的架构是，将多块磁盘组成一个逻辑磁盘，构成冗余独立 的磁盘阵列，RAID，提高数据访问性能，增强数据存储的可靠性。

根据容量、性能、可靠性的不同，RAID可以分为多个级别，如RAID0、RAID1、RAID5、RAID10等。RAID0有最优的读写性能，但不提供数据冗余的功能，其他级别的 RAID，在数据冗余的基础上，对读写性能有一定的优化。

最后一种架构，把磁盘组合成网络存储集群，再通过NFS、SMB、iSCSI等网络存储协议，暴露给服务器使用。

其实，在Linux下，磁盘是作为块设备来管理的，也就是以块为单位来读写数据，且支持随机读写。每个块设备都被赋予主、次两个设备号，主设备号用在驱动程序中区别设备类型，次设备号用来给多个同类设备编号。

### 2.2 通用块层

为了减少不同块设备的差异带来的影响，Linux通过一个统一的通用块层，来管理各种不同的块设备。通用块层其实是处在文件系统和磁盘驱动中间的一个块设备抽象层。有两个功能：

第一个跟虚拟文件系统的功能类似。向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象成统一的块设备，并提供统一框架来管理这些设备的驱动程序。

第二个功能，通用块层还给文件系统和应用程序发来的I/O请求排队，并通过请求排队、合并等，提高磁盘读写的效率。

对 I/O请求排序也是 I/O调度。事实上，Linux内核支持四种 I/O调度算法，NONE、NOOP、CFQ、DeadLine。

NONE：确切的说并不能算调度，因为它完全不使用任何调度器，对文件系统和应用程序的 I/O不作任何处理，常用在虚拟机中（此时磁盘 I/O调度完全由物理机支持）。

NOOP：最简单的一种调度算法，是一个先进先出的队列，只做一些最基本的请求合并，常用于SSD盘。

CFQ：完全公平调度器，是现在很多发行版的默认 I/O调度器。它为每个进程维护了一个 I/O调度队列，并按时间片来均匀分布每个进程的 I/O请求。类似于进程的CPU调度，CFQ调度还支持进程 I/O的优先级调度，所以适用运行着大量进程的系统，像桌面环境、多媒体应用等。

DeadLine：分别为读、写请求创建不同的 I/O 队列，可以提高机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理。这种调度算法 多用在 I/O 压力比较大的场合，如数据库等。

### 2.3** I/O栈**

结合上面讲的文件系统、磁盘和通用块层的工作原理，我们可以整体来看 Linux存储系统的 I/O原理了。事实上，我们可以把 Linux存储系统的 I/O栈，由上至下分为三层：文件系统层、通用块层、设备层。看图：

![img](https://pic3.zhimg.com/80/v2-b1ce5b9157ef3c9ea0e53552a26593ca_720w.webp)

根据这张全景图，我们可以更清楚理解，存储系统的 I/O的工作原理：

- 文件系统层，包括虚拟文件系统和其他各种文件系统的具体实现。首先为上层的应用程序提供标准的文件访问接口，对下会通过通用块层，来存储和管理磁盘数据。
- 通用块层，是Linux磁盘 I/O的核心，包括设备 I/O队列和 I/O调度器。会对文件系统的 I/O请求进行排队，再通过重新排序和请求合并，再发给下一级设备层。
- 设备层，包括存储设备和相应的驱动程序，负责最终物理设备的 I/O操作。

存储系统的 I/O，通常是整个Linux系统中最慢的一环。所以，Linux通过多种缓存机制来优化 I/O 效率。比如，为了优化文件访问性能，会使用页缓存、索引节点缓存、目录项缓存等多种缓存机制，减少对下层块设备的直接调用。同样，为了优化块设备的访问性能，会使用缓冲区，来缓存块设备的数据。

### **2.4 磁盘性能指标以及观测**

这里说一下常见的五个指标，使用率、饱和度、IOPS、吞吐量以及响应时间等，这五个指标是衡量磁盘性能的基本指标。

- 使用率，是指磁盘处理 I/O的时间百分比。过高的使用率（如超过80%），通常意味着磁盘 I/O的性能瓶颈。
- 饱和度，磁盘处理 I/O的繁忙程度，过高的饱和度，意味着磁盘存在严重的性能瓶颈。当达到100%时，磁盘就无法接受新的 I/O请求。
- IOPS，每秒的 I/O请求数。
- 吞吐量，每秒的 I/O请求大小。
- 响应时间，从发出请求到收到响应的时间间隔。

注意，使用率只考虑有没有 I/O，而不考虑 I/O大小，即使达到100%，也有可能接受新的 I/O请求。在数据库、大量小文件等这类随机读写比较多的场景中，IOPS更能反应系统整体性能。在多媒体等顺序读写较多的场景中，吞吐量更能反应系统整体性能。

一般来说，我们在为应用程序的服务器选型时，要先对磁盘 I/O的性能进行基准测试，推荐的性能测试工具 fio，来测试磁盘的 IOPS，吞吐量以及响应时间等核心指标。用性能工具得到的指标，作为后续分析应用程序的性能依据。一旦发生性能问题，就可以把它们作为磁盘性能的极限值，进而评估磁盘 I/O的使用情况。

接下来看看怎么观测磁盘 I/O？首推的工具 iostat，它提供每个磁盘的使用率、IOPS、吞吐量等各种常见的性能指标，当然这些指标来自 /proc/diskstats。iostats 的输出界面如下：

```text
# -d -x表示显示所有磁盘I/O的指标
$ iostat -d -x 1 
Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util 
loop0            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 
loop1            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 
sda              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     0.00   0.00   0.00 
sdb              0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00
```

下图说明了这些列的具体含义：

![img](https://pic4.zhimg.com/80/v2-28ee554ae0fce5df93dd3e0f8c0cf867_720w.webp)

这些指标，你要注意，%util 磁盘使用率，r/s + w/s IOPS，rkB/s + wkB/s 吞吐量， r_await + w_await 响应时间。另外从 iostat 并不能直接得到磁盘的饱和度，但是可以把观测到的，平均请求队列长度 或者 读写请求完成的等待时间，跟基准测试的结果进行对比，综合来评估。

我们再来看看，每个进程的 I/O情况。 iostat只能看到磁盘整体的 I/O性能数据，并不能知道具体哪些进程 在进行磁盘读写，推荐两个工具： pidstat 和 iotop。具体使用这里略过。

原文地址：https://zhuanlan.zhihu.com/p/572541668

作者：linux

# 【NO.98】linux性能优化实战之cpu篇

对于性能来说，cpu的调度逻辑是影响性能的主要来源，本文主要来介绍下cpu跟性能相关的调度逻辑和排障工具。

## **1.cpu调度逻辑介绍：**

Linux 是一个多任务操作系统，支持远大于 CPU 数量的任务同时运行，而这里的同时运行采用的是分时逻辑的思路，只是看起来多个进程是同时运行的。

任务运行前，CPU通过系统事先帮它设置好的 CPU 寄存器和程序计数器（Program Counter，PC）来完成加载。

**CPU 上下文：**CPU 在运行任何任务前，必须的依赖环境，也就是 CPU 寄存器和程序计数器。
**CPU 寄存器：**是 CPU 内置的容量小、但速度极快的内存。
**程序计数器：**则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。

**CPU 上下文切换**：就是先把前一个任务的 CPU 上下文保存到操作系统的内核中，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。

![img](https://pic4.zhimg.com/80/v2-998ae29ce74c2216e3df201b16c274e7_720w.webp)

在操作系统中CPU调度的任务有三类，分别是进程、线程和中断程序，所以上下文切换也可以分为这三类：

**系统调用：**
在开始介绍这三类上下文切换之前，我们先来看下系统调用，系统调用指的是用户态到内核态的转变，例如：open(),read()等。

一次系统调用的过程，只会发生在同一个进程中，其实是发生了两次 CPU 上下文切换，过程如下所示：

```text
1.CPU 寄存器里原来用户态的指令位置，需要先保存起来。
2.接着，为了执行内核态代码，CPU 寄存器需要更新为内核态指令的新位置。
3.最后才是跳转到内核态运行内核任务。
4.系统调用结束后，CPU 寄存器需要恢复原来保存的用户态，
5.然后再切换到用户空间，
6.继续运行进程。
```

**1.进程上下文切换：**

进程的上下文不仅包括了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的状态，进程的上下文切换步骤如下所示：

```text
1.在保存当前进程的内核状态和 CPU 寄存器之前，需要先把该进程的虚拟内存、栈等保存下来；
2.而加载了下一进程的内核态后，还需要刷新进程的虚拟内存和用户栈。
备注：每次上下文切换都需要几十纳秒到数微秒的 CPU 时间。
```

这种切换的次数变多，就会导致上下文切换时间变久，运行进程的时间占比变少，进而导致平均负载升高。

**进程调度原理：**

Linux 为每个 CPU 都维护了一个就绪队列，将活跃进程（即正在运行和正在等待 CPU 的进程）按照**优先级**和**等待 CPU 的时间**排序，然后选择最需要 CPU 的进程，也就是优先级最高和等待 CPU 时间最长的进程来运行。触发进程调度的场景如下所示：

```text
1.某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。
2.进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。
3.进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。
4.当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行。
5.发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。
```

**2.线程上下文切换：**

线程是**调度的基本单位**，而进程则是**资源拥有的基本单位**，内核中的任务调度，实际上的调度对象是线程，而进程只是给线程提供了虚拟内存、全局变量等资源。线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。

对于**不同进程里面的线程之间的切换**，与进程间的上下文切换是一致的。
对于**同一进程里面的不同线程之间的切换**，区别较大，如下所示：

```text
因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，
只需要切换线程的私有数据、寄存器等不共享的数据。
```

**3.中断上下文切换：**

为了快速响应硬件的事件，中断处理会打断进程的正常调度和执行，转而调用中断处理程序，响应设备事件。而在打断其他进程时，就需要将进程当前的状态保存下来，这样在中断结束后，进程仍然可以从原来的状态恢复运行，这种行为发生在内核态。

```text
1.中断上下文，只包括内核态中断服务程序执行所必需的状态，
包括 CPU 寄存器、内核堆栈、硬件中断参数等。
2.对同一个 CPU 来说，中断处理比进程拥有更高的优先级，
所以中断上下文切换并不会与进程上下文切换同时发生。
3.中断上下文切换也需要消耗 CPU，切换次数过多也会耗费大量的 CPU，
甚至严重降低系统的整体性能。
```



## **2.常用工具介绍：**

vmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。

```text
# 每隔5秒输出1组数据
$ vmstat 5
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0      0 7005360  91564 818900    0    0     0     0   25   33  0  0 100  0  0

# 指标介绍：
# cs（context switch）：是每秒上下文切换的次数。
# in（interrupt）：则是每秒中断的次数。
# r（Running or Runnable）：是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。
# b（Blocked）：则是处于不可中断睡眠状态的进程数。
# us 用户态使用的cpu占比
# sy 内核态使用的cpu占比
```

pidstat用来查看每个进程的详细情况：

**自愿上下文切换**（voluntary context switches）：是指进程无法获取所需资源，导致的上下文切换，比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。

**非自愿上下文切换（**non voluntary context switches）：则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换，比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。

```text
# 每隔1秒输出一组数据（需要 Ctrl+C 才结束）
# -wt 参数表示输出线程的上下文切换指标，t指的是线程
$ pidstat -wt 1
08:14:05      UID      TGID       TID   cswch/s nvcswch/s  Command
...
08:14:05        0     10551         -      6.00      0.00  sysbench
08:14:05        0         -     10551      6.00      0.00  |__sysbench
08:14:05        0         -     10552  18911.00 103740.00  |__sysbench
08:14:05        0         -     10553  18915.00 100955.00  |__sysbench
08:14:05        0         -     10554  18827.00 103954.00  |__sysbench
...
#指标解释：
# cswch：表示每秒自愿上下文切换的次数，
# nvcswch：表示每秒非自愿上下文切换的次数。
```

查看中断发生情况：

```text
# -d 参数表示高亮显示变化的区域
$ watch -d cat /proc/interrupts
           CPU0       CPU1
...
RES:    2450431    5279697   Rescheduling interrupts
...
#指标说明：
#重调度中断（RES），这个中断类型表示，唤醒空闲状态的 CPU 来调度新的任务运行。
```

场景总结：

1.**自愿上下文切换变多**：说明进程都在等待资源，有可能发生了 I/O 等其他问题。

2.**非自愿上下文切换变多**：说明进程都在被强制调度，也就是都在争抢 CPU，说明 CPU 的确成了瓶颈。

3.**中断次数变多**：说明 CPU 被中断处理程序占用，还需要通过查看 /proc/interrupts 文件来分析具体的中断类型。

**备注，两类压测工具介绍：**

stress基于多进程的，会fork多个进程，导致进程上下文切换，导致us开销很高。sysbench基于多线程的，会创建多个线程，单一进程基于内核线程切换，导致sy的内核开销很高。

原文地址：https://zhuanlan.zhihu.com/p/571624940

作者：linux

# 【NO.99】理解Redis的反应堆模式

## **1. Redis的网络模型**

Redis基于Reactor模式(**反应堆模式**)开发了自己的网络模型，形成了一个完备的基于IO复用的事件驱动服务器，但是不由得浮现几个问题:

- 为什么要使用Reactor模式呢？
- Redis如何实现自己的Reactor模式？

## **2. Reactor模式的背景**

单纯的epoll/kqueue可以单机支持数万并发，单纯从性能的角度而言毫无问题，但是技术实现和软件设计仍然存在一些差异。

**设想这样一种场景**:

- epoll/kqueue将收集到的可读写事件全部放入队列中等待业务线程的处理，此时线程池的工作线程拿到任务进行处理，实际场景中可能有很多种请求类型，工作线程每拿到一种任务就进行相应的处理，处理完成之后继续处理其他类型的任务;
- 工作线程需要关注各种不同类型的请求，对于不同的请求选择不同的处理方法，因此请求类型的增加会让工作线程复杂度增加，维护起来也变得越来越困难;

作为吃货的你，如果还是没听懂，不用着急，想象一下吃饭的场景:

城南大熊饭店在来客高峰期服务员快速地接待了很多顾客，有的顾客点凉菜，有的点热菜，有的点主食，有的点饮料等等，在后厨如果人员没有分工，那么厨师A一会儿弄凉菜一会弄主食，厨师B一会儿弄热菜一会儿弄饮料，厨师C一会儿弄主食一会儿弄火锅.....虽然最终也响应了顾客的需求，但是把这帮厨师累够呛并且厨师都是全栈厨师(FullStack)，对饭店来说招个这样的厨师非常不容易而且薪水也高，老板孙大熊很苦恼。

当局者迷旁观者清，孙大熊找到了他MBA的同学李明诉说这个苦恼，李明说专业的人做专业的事情才是王道，凉菜的有凉菜厨师、热菜有热菜厨师、主食有主食厨师，如果有新增的菜系菜品，那就再找个专门做这个菜品的厨师就可以了，这样解决了厨师业务能力要求高、饭店扩张慢的问题，孙大熊频频点头，感觉自己花钱买的这个MBA算是瞎了，回去之后进行改进，果然有很大的改善，孙大熊又开始嘚瑟了。

上面的场景其实和高并发网络模型很相似，如果我们在epoll/kqueue的基础上进行业务区分，并且对每一种业务设置相应的处理函数，每次来任务之后对任务进行识别和分发，每种处理函数只处理一种业务，这种模型更加符合OO的设计理念，这也是Reactor反应堆模式的设计思路。

## **3. Reactor模式**

基于Reactor的组件阵营非常强大：

- Java NIO
- Netty
- libevent/libuv
- Redis

反应堆模式是一种对象行为的设计模式，主要同于同步IO，异步IO有Proactor模式，这里不详细讲述Proactor模式，二者的主要区别就是Reactor是同步IO,Proactor是异步IO，理论上Proactor效率更高，但是Proactor模式需要操作系统在内核层面对异步IO进行支持，Linux的Boost.asio就是Proactor模式的代表，Windows有IOCP。
**网上比较经典的一张Reactor模式的类图:**

![img](https://pic3.zhimg.com/80/v2-642f05baff9fb6165507ace84fbec4be_720w.webp)


**图中给出了5个部件分别为：**

- handle 可以理解为读写事件 可以注册到Reactor进行监控
- Sync event demultiplexer 可以理解为epoll/kqueue/select等作为IO事件的采集器
- Dispatcher 提供注册/删除事件并进行分发，作为事件分发器
- Event Handler 事件处理器 完成具体事件的回调 供Dispatcher调用
- Concrete Event Handler 具体请求处理函数

**更简洁的流程如下：**

![img](https://pic3.zhimg.com/80/v2-3a1f27184b1d6723ea110e9a4cf5b35e_720w.webp)


**以网络场景为例：**

循环前先将待监控的事件进行注册，当监控中的Socket读写事件到来时，事件采集器epoll等IO复用工具检测到并且将事件返回给事件分发器Dispatcher，分发器根据读、写、异常等情况进行分发给事件处理器，事件处理器进而根据事件具体类型来调度相应的实现函数来完成任务。

## **4. Redis的Reactor实现**

Redis处理客户端业务(文件事件)的基本流程：

![img](https://pic4.zhimg.com/80/v2-8a5cfd82a7f8f9613bd78e4ebe085fab_720w.webp)


**Redis的IO复用的选择**

\#ifdef HAVE_EVPORT #include "ae_evport.c" #else #ifdef HAVE_EPOLL #include "ae_epoll.c" #else #ifdef HAVE_KQUEUE #include "ae_kqueue.c" #else #include "ae_select.c" #endif #endif #endif

Redis中支持多种IO复用，源码中使用相应的宏定义进行选择，编译时就可以获取当前系统支持的最优的IO复用函数来使用，从而实现了Redis的优秀的可移植特性。

**Redis的任务事件队列**

由于Redis的是单线程处理业务的，因此IO复用程序将读写事件同步的逐一放入队列中，如果当前队列已经满了，那么只能出一个入一个，但是由于Redis正常情况下处理得很快，不太会出现队列满迟迟无法放任务的情况，但是当执行某些阻塞操作时将导致长时间的阻塞，无法处理新任务。


**Redis事件分派器**

事件的可读写是从服务器角度看的，分派看到的事件类型包括：

- AE_READABLE 客户端写数据、关闭连接、新连接到达
- AE_WRITEABLE 客户端读数据

特别地，当一个套接字连接同时可读可写时，服务器会优先处理读事件再处理写事件，也就是读优先。

**Redis事件处理器**

Redis将文件事件进行归类，编写了多个事件处理器函数，其中包括：

- 连接应答处理器：实现新连接的建立
- 命令请求处理器：处理客户端的新命令
- 命令回复处理器：返回客户端的请求结果
- 复制处理器：实现主从服务器的数据复制

**Redis C/S一次完整的交互**

Redis服务器的主线程处于循环中，此时一个Client向Redis服务器发起连接请求，假如是6379端口，监听端口在IO复用工具下检测到AE_READABLE事件，并将该事件放入TaskQueue中，等待被处理，事件分派器获取这个读事件，进一步确定是新连接请求，就将该事件交给了连接应答处理器建立连接；

建立连接之后Client继续向服务器发送了一个get命令，仍然被IO复用检测处理放入队列，被事件分派器处理指派给命令请求处理器，调用相应程序进行执行；

服务器将套接字的AE_WRITEABLE事件与命令回复处理器相关联，当客户端尝试读取结果时产生可写事件，此时服务器端触发命令回复响应，并将数据结果写入套接字，完成之后服务端接触该套接字与命令回复处理器之间的关联；

![img](https://pic1.zhimg.com/80/v2-745cabc911b46ae96c23445eed64f9f8_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/569141858

作者：linux

# 【NO.99】Linux服务器检查性能瓶颈

## 1.**概述**

如果Linux服务器突然访问卡顿变慢，负载暴增，如何在最短时间内找出Linux性能问题所在？

通过执行以下命令，可以在1分钟内对系统资源使用情况有个大致的了解。

- uptime
- dmesg | tail
- vmstat 1
- mpstat -P ALL 1
- pidstat 1
- iostat -xz 1
- free -m
- sar -n DEV 1
- sar -n TCP,ETCP 1
- top

其中一些命令需要安装sysstat包，有一些由procps包提供。这些命令的输出，有助于快速定位性能瓶颈，检查出所有资源（CPU、内存、磁盘IO等）的利用率（utilization）、饱和度（saturation）和错误（error）度量，也就是所谓的USE方法。

下面我们来逐一介绍下这些命令，有关这些命令更多的参数和说明，请参照命令的手册。

## 2.**平均负载**

```text
uptime
```

结果

![img](https://pic4.zhimg.com/80/v2-765adaf45d13f67cde79adbc913baacf_720w.webp)

这个命令可以快速查看机器的负载情况。在Linux系统中，这些数据表示等待CPU资源的进程和阻塞在不可中断IO进程（进程状态为D）的数量。这些数据可以让我们对系统资源使用有一个宏观的了解。

命令的输出分别表示1分钟、5分钟、15分钟的平均负载情况。通过这三个数据，可以了解服务器负载是在趋于紧张还是区域缓解。如果1分钟平均负载很 高，而15分钟平均负载很低，说明服务器正在命令高负载情况，需要进一步排查CPU资源都消耗在了哪里。反之，如果15分钟平均负载很高，1分钟平均负载 较低，则有可能是CPU资源紧张时刻已经过去。

上面例子中的输出，可以看见最近1分钟的平均负载非常高，且远高于最近15分钟负载，因此我们需要继续排查当前系统中有什么进程消耗了大量的资源。可以通过下文将会介绍的vmstat、mpstat等命令进一步排查。

## 3.**系统核心指标**

```text
vmstat 1
```

结果

![img](https://pic2.zhimg.com/80/v2-55685bfcb32dd0db63144f7a62e63985_720w.webp)

vmstat 命令，每行会输出一些系统核心指标，这些指标可以让我们更详细的了解系统状态。后面跟的参数1，表示每秒输出一次统计信息，表头提示了每一列的含义，这几介绍一些和性能调优相关的列：

- r：等待在CPU资源的进程数。这个数据比平均负载更加能够体现CPU负载情况，数据中不包含等待IO的进程。如果这个数值大于机器CPU核数，那么机器的CPU资源已经饱和。
- free：系统可用内存数（以千字节为单位），如果剩余内存不足，也会导致系统性能问题。下文介绍到的free命令，可以更详细的了解系统内存的使用情况。
- si, so：交换区写入和读取的数量。如果这个数据不为0，说明系统已经在使用交换区（swap），机器物理内存已经不足。
- us, sy, id, wa, st：这些都代表了CPU时间的消耗，它们分别表示用户时间（user）、系统（内核）时间（sys）、空闲时间（idle）、IO等待时间（wait）和被偷走的时间（stolen，一般被其他虚拟机消耗）。

上述这些CPU时间，可以让我们很快了解CPU是否出于繁忙状态。

一般情况下，如果用户时间和系统时间相加非常大，CPU出于忙于执行指令。

如果IO等待时间很长，那么系统的瓶颈可能在磁盘IO。

如果大量CPU时间消耗在用户态，也就是用户应用程序消耗了CPU时间。这不一定是性能问题，需要结合r队列，一起分析。

## 4.**CPU占用情况-每个核心**

```text
mpstat -P ALL 1
```

结果

![img](https://pic1.zhimg.com/80/v2-1352123f501c3bcba8aefc1956022a5c_720w.webp)

该命令可以显示每个CPU的占用情况，如果有一个CPU占用率特别高，那么有可能是一个单线程应用程序引起的。

## 5.**CPU占用情况-每个进程**

```text
pidstat 1
```

结果

![img](https://pic1.zhimg.com/80/v2-c34442a841f5278222d85f8b3093a234_720w.webp)

pidstat命令输出进程的CPU占用率，该命令会持续输出，并且不会覆盖之前的数据，可以方便观察系统动态。

如上的输出，可以看见两个JAVA进程占用了将近1600%的CPU时间，既消耗了大约16个CPU核心的运算资源。

## 6.**磁盘IO情况**

```text
iostat -xz 1
```

结果

![img](https://pic1.zhimg.com/80/v2-74131a6753a089832902e17904f49bdc_720w.webp)

iostat命令主要用于查看机器磁盘IO情况。该命令输出的列，主要含义是：

- r/s, w/s, rkB/s, wkB/s：分别表示每秒读写次数和每秒读写数据量（千字节）。读写量过大，可能会引起性能问题。
- await：IO操作的平均等待时间，单位是毫秒。这是应用程序在和磁盘交互时，需要消耗的时间，包括IO等待和实际操作的耗时。如果这个数值过大，可能是硬件设备遇到了瓶颈或者出现故障。
- avgqu-sz：向设备发出的请求平均数量。如果这个数值大于1，可能是硬件设备已经饱和（部分前端硬件设备支持并行写入）。
- %util：设备利用率。这个数值表示设备的繁忙程度，经验值是如果超过60，可能会影响IO性能（可以参照IO操作平均等待时间）。如果到达100%，说明硬件设备已经饱和。

如果显示的是逻辑设备的数据，那么设备利用率不代表后端实际的硬件设备已经饱和。值得注意的是，即使IO性能不理想，也不一定意味这应用程序性能会不好，可以利用诸如预读取、写缓存等策略提升应用性能。

## 7.**内存情况**

```text
free -m
```

结果

![img](https://pic4.zhimg.com/80/v2-c696ca705ffe07c561315b563263c17b_720w.webp)

free命令可以查看系统内存的使用情况，-m参数表示按照兆字节展示。最后两列分别表示用于IO缓存的内存数，和用于文件系统页缓存的内存数。需 要注意的是，第二行-/+ buffers/cache，看上去缓存占用了大量内存空间。这是Linux系统的内存使用策略，尽可能的利用内存，如果应用程序需要内存，这部分内存会 立即被回收并分配给应用程序。因此，这部分内存一般也被当成是可用内存。

如果可用内存非常少，系统可能会动用交换区（如果配置了的话），这样会增加IO开销（可以在iostat命令中提现），降低系统性能。

## **8.网络设备的吞吐率**

```text
sar -n DEV 1
```

结果

![img](https://pic1.zhimg.com/80/v2-497adddb31f7b761f37053205fe060b4_720w.webp)

sar命令在这里可以查看网络设备的吞吐率。在排查性能问题时，可以通过网络设备的吞吐量，判断网络设备是否已经饱和。

如示例输出中，eth0网卡设备，吞吐率大概在22 Mbytes/s，既176 Mbits/sec，没有达到1Gbit/sec的硬件上限。

## 9.**TCP连接数**

```text
sar -n TCP,ETCP 1
```

结果

![img](https://pic2.zhimg.com/80/v2-a04ae32ac3edcd7f7c56a443aa3ba255_720w.webp)

sar命令在这里用于查看TCP连接状态，其中包括：

- active/s：每秒本地发起的TCP连接数，既通过connect调用创建的TCP连接；
- passive/s：每秒远程发起的TCP连接数，即通过accept调用创建的TCP连接；
- retrans/s：每秒TCP重传数量；

TCP连接数可以用来判断性能问题是否由于建立了过多的连接，进一步可以判断是主动发起的连接，还是被动接受的连接。TCP重传可能是因为网络环境恶劣，或者服务器压力过大导致丢包。

## 10.**整体情况-TOP**

```text
top
```

结果

![img](https://pic3.zhimg.com/80/v2-fefd9a5f74294a1ead68fd19eed97e4e_720w.webp)

top命令包含了前面好几个命令的检查的内容。

比如系统负载情况（uptime）、系统内存使用情况（free）、系统CPU使用情况 （vmstat）等。因此通过这个命令，可以相对全面的查看系统负载的来源。

同时，top命令支持排序，可以按照不同的列排序，方便查找出诸如内存占用最 多的进程、CPU占用率最高的进程等。

但是，top命令相对于下面的一些命令，输出是一个瞬间值，如果不持续盯着，可能会错过一些线索。这时可能需要暂停top命令刷新，来记录和比对数据。

## 11.**查看系统日志**

```text
dmesg | tail
```

该命令会输出系统日志的最后10行。

原文地址：https://zhuanlan.zhihu.com/p/567441428

作者：linux

# 【NO.100】熬夜肝了这一份C++开发详细学习路线

一般开发岗主流的就是 Java 后台开发，前端开发以及 C++ 后台开发，现在 Go 开发也是越来越多了，今天把 C++ 后台开发学习路线补上。

写之前先来回答几个问题

### **1.C++ 后台开发有哪些岗位？**

C++ 后台开发的岗位还是很多的，例如游戏引擎开发，游戏服务端开发，音视频服务端/客户端开发，数据库内核开发等等，而且 C++ 也能用来写深度学习，做硬件底层这些。

总之，C++ 后台开发的岗位，还是很丰富的，大家不用担心找不到合适的岗位。

### **2.C++ 后台开发岗位需求量大吗？**

一般大公司大需求量会多一些，小公司需求量较少。

说到岗位需求量，那肯定是 Java 的岗位需求量是最大的，当然，学 Java 的人也是最多的，假如你要学习 C++，那我觉得你要定位大公司可能会好一点，进大公司反而会比 Java 容易。
假如你觉得自己实力很一般，够不着大公司，那我觉得你可以考虑学习 Java，因为大部分小公司，Java 岗位多一些。

但是呢，假如你是应届生，那么语言其实也不是特别重要，只要你 把计算机基础和算法学好，就算你是学 Java 的，也可以去面 C++；学 C++ 的也可以去面 Java。

我当时是学 Java 的，不过秋招那会还面了几个 C++ 岗位，直接和面试官说我不会 C++ 就可以了，他会问你其他的知识。

下面跟大家说一说 C++ 后台开发学习路线，为了方便大家做规划，每一个模块的学习，我都会说下大致的学习时间

C/C++后台开发学习路线总结图

![img](https://pic3.zhimg.com/80/v2-8c5a0c0d084a216d21b300724d87c306_720w.webp)

#### 2.1.C++ 基础

假如你有 C 语言基础，那么这块感觉花个三四个月就能拿下了，假如你是零基础的，估计还得学两三个月的 C 语言，也就是说，得花半年时间才行，没有 C 语言基础的看这个 C 语言教程：[一份评价超高的 C 语言入门教程]
C++ 这块，重点需要学习的就是一些**关键字**、**面向对象**以及 **STL 容器**的知识，特别是 STL，还得研究下他们的一些源码，下面我总结一下一些比较重要的知识（其实是根据面试结果来挑选）。

1. 指针与引用的区别，C 与 C++ 的区别，struct 与 class 的区别
2. struct 内存对齐问题，sizeof 与 strlen 区别
3. 面向对象的三大特性：封装、继承、多态
4. 类的访问权限：private、protected、public
5. 类的构造函数、析构函数、赋值函数、拷贝函数
6. 移动构造函数与拷贝构造函数对比
7. 内存分区：全局区、堆区、栈区、常量区、代码区
8. 虚函数实现动态多态的原理、虚函数与纯虚函数的区别
9. 深拷贝与浅拷贝的区别
10. 一些关键字：static, const, extern, volatile 等
11. 四种类型转换：static_cast、dynamic_cast、const_cast、reinterpret_cast
12. 静态与多态：重写、重载、模板
13. 四种智能指针及底层实现：auto_ptr、unique_ptr、shared_ptr、weak_ptr
14. 右值引用
15. std::move函数
16. 迭代器原理与迭代器失效问题
17. 一些重要的 STL：vector, list, map, set 等。
18. 容器对比，如 map 与 unordered_map 对比，set 与 unordered_set 对比，vector 与 list 比较等。
19. STL容器空间配置器

等等。
根据书来学就可以了，然后学到一些重点，可以重点关注一下。
书籍推荐：

1、《C++Primer》，这本书内容很多的，把前面基础的十几章先看一看，不用从头到尾全啃，后面可以**字典**来使用。

2、《STL 源码剖析》，必看书籍，得知道常见 STL 的原理，建议看个两三遍。

3、《深度探索C++对象模型》，这本主要讲解**面向对象**的相关知识，可以帮你扫清各种迷雾。

#### 2.2.计算机网络

无论你是从事啥岗位，无论是校招还是社招，计算机网络基本都会问，特特是腾讯，字节，shopee，小米等这些非 Java 系的公司，问的更多。这块认真学，**一个半月**就可以搞定了。

计算机网络就是一堆协议的构成，下面是一些比较重要的知识点，学的时候可以重点关注下。

**物理层、链路层**：

1. MTU，MAC地址，以太网协议。
2. 广播与 ARP 协议

**网络层**

1. ip 地址分类
2. IP 地址与 MAC 地址区别
3. 子网划分，子网掩码
4. ICMP 协议及其应用
5. 路由寻址
6. 局域网，广域网区别

**传输层**（主要就是 TCP）

1. TCP首部报文格式（SYN、ACK、FIN、RST必须知道）
2. TCP滑动窗口原理，TCP 超时重传时间选择
3. TCP 拥塞控制，TCP 流量控制
4. TCP 三次握手与四次挥手以及状态码的变化
5. TCP连接释放中TIME_WAIT状态的作用
6. SYN 泛洪攻击
7. TCP 粘包，心跳包
8. UDP 如何实现可靠传输
9. UDP 与 TCP 的区别
10. UDP 以及 TCP 的应用场景

**应用层**

1. DNS 原理以及应用
2. HTTP 报文格式，HTTP1.0、HTTP1.1、HTTP2.0 之间的区别
3. HTTP 请求方法的区别：GET、HEAD、POST、PUT、DELETE
4. HTTP 状态码
5. HTTP 与 HTTPS 的区别
6. 数字证书，对称加密与非对称加密
7. cookie与session区别
8. 输入一个URL到显示页面的流程（越详细越好，搞明白这个，网络这块就差不多了）

书籍推荐：零基础可以先看《图解HTTP》，当然，也可以直接看《计算机网网络：自顶向下》这本书，这本书建议看两遍以及以上，还有时间的可以看《TCP/IP详解卷1：协议》。

### 3.操作系统

操作系统和计算机网络差不多，不过计算机网络会问的多一些，操作系统会少一些，学到时候如果可以带着问题去学是最好的，例如

咋就还有进程和线程之分？为什么要有挂起、运行、阻塞等这么多种状态？怎么就还有悲观锁和乐观锁，他们的本质区别？

进程咋还会出现死锁，都有哪些处理策略？进程都有哪些调度算法？

虚拟内存解决了什么问题？为啥每个进程的内存地址就是独立的呢？

为啥 cpu 很快而内存很慢？磁盘怎么就更慢了？

总结起来大致：
1、进程与线程区别
2、线程同步的方式：互斥锁、自旋锁、读写锁、条件变量
3、互斥锁与自旋锁的底层区别
4、孤儿进程与僵尸进程
5、死锁及避免
6、多线程与多进程比较
7、进程间通信：PIPE、FIFO、消息队列、信号量、共享内存、socket
8、管道与消息队列对比
9、fork进程的底层：读时共享，写时复制
10、线程上下文切换的流程
11、进程上下文切换的流程
12、进程的调度算法
13、阻塞IO与非阻塞IO
14、同步与异步的概念
15、静态链接与动态链接的过程
16、虚拟内存概念（非常重要）
17、MMU地址翻译的具体流程
18、缺页处理过程
19、缺页置换算法：最久未使用算法、先进先出算法、最佳置换算法

书籍推荐：《现代操作系统》

这里也有一门合并的视频：[C/C++后台开发学习视频](https://link.zhihu.com/?target=https%3A//ke.qq.com/course/417774%3FflowToken%3D1013189)

### 4.MySQL(一个月左右)

数据库一般主流的有 MySQL 和 Oracle，不过建议大家学习 MySQL 了，因为大部分公司都是使用 MySQL，也是属于面试必问，而且工作中 MySQL 也是接触的最多的，毕竟工作 crud 才是常态。

下面这些是我认为比较重要的知识点：

1、一条 sql 语句是如何执行的？也就是说，从客户端执行了一条 sql 命令，服务端会进行哪些处理？（例如验证身份，是否启用缓存啥的）。

2、索引相关：索引是如何实现的？多种引擎的实现区别？聚族索引，非聚族索引，二级索引，唯一索引、最左匹配原则等等（非常重要）。

3、事务相关：例如事务的隔离是如何实现的？事务是如何保证原子性？不同的事务看到的数据怎么就不一样了？难道每个事务都拷贝一份视图？MVCC 的实现原理（重要）等等。

4、各种锁相关：例如表锁，行锁，间隙锁，共享锁，排他锁。这些锁的出现主要是用来解决哪些问题？（重要）

5、日志相关：redolog，binlog，undolog，这些日志的实现原理，为了解决怎么问题？日志也是非常重要的吧，面试也问的挺多。

6、数据库的主从备份、如何保证数据不丢失、如何保证高可用等等。

7、一些故障排查的命令，例如慢查询，sql 的执行计划，索引统计的刷新等等。

对于 2-4 这四个相关知识，面试被问到的频率是最高的，有时候面试会让你说一说索引，如果你知道的多的话就可以疯狂扯一波了，记得我当时总结了一套扯的模版：

先说从 B 树角度说为啥索引会快-》趁机说一下索引的其他实现方式-〉不同引擎在索引实现上的不同-》系统是如果判断是否要使用索引的-〉明明加了索引却不走索引？

只有你对各种数据结构和索引原理都懂，你才能扯的起来，对于事物和锁也是，当时面试官问了我事务是如何保证一致性的，刚好我研究过 ，redolog，binlog，undolog 这些日志，然后和面试官扯了好久。

书籍：《MySQL必知必会》和《MySQL技术内幕》

### 5.网络编程

网络编程这块，有些公司还是问的挺多的，特别是 IO 多路复用，同步非同步 IO，阻塞非阻塞啥的，当时面腾讯基本每次都问，，，，学习 C++ 这块还是要重视一下，下面我说一下比较重要的吧。

1、IO多路复用：select、poll、epoll的区别（非常重要，几乎必问，回答得越底层越好，要会使用）
2、手撕一个最简单的server端服务器（socket、bind、listen、accept这四个API一定要非常熟练）
3、线程池
4、基于事件驱动的reactor模式
5、边沿触发与水平触发的区别
6、非阻塞IO与阻塞IO区别

书籍：可以看一看《Unix网络编程》

### 6.数据结构与算法

数据结构与算法，我觉得是需要花最多时间的，因为算法这块，很难快速突击，从基础数据结构与各种算法思想到 leetcode 刷题，如果你零基础，那真的需要挺久的，不过你有一些基础，可能会快一点，看你想掌握到什么程度了。

我这里大致说一下学习流程吧

1、先跟着书学**基础数据结构与算法**：链表，队列，栈，哈希表，二叉树，图，十大排序，二分查找。

2、之后了解一下算法思想：递归，深度与广度搜索，枚举，动态规划这些。

入门数据结构推荐《数据结构与算法分析：c语言描述版》这本书，学的过程中，也可以配合刷题，一般刷《剑指 offer》 + LeetCode 刷个两三百就差不多了，没时间到就先刷 《剑指 offer》吧。

### 7.项目

项目是必须要做的了，Java 的项目教程满天飞，不过 C++ 的会少一些，不过大家可以跟着书，或者 github 上找或者自己花点钱买一个付费视频吧。

推荐自学项目：实现 http服务器（ github 一堆源码、音视频服务器）、实现一个聊天系统(这块有些书就有附带)

### 8.学习顺序

我建议有时间的，可以先入门下 C++ ，然后就是开始学习数据结构与算法，算法这块长期保持刷题，然后一边深入学习 C++，之后学习计算机网络，操作系统，在之后学习网络编程，项目这块放到最后面。

如果时间比较紧的，算法这块可以放松一点，C++ 和项目可以优先，计算机基础可以突击学习，通过视频或者别人总结的笔记突击。

总之，这一套学下来，感觉需要大概8个月的时间，当然，这个不好衡量，还得看你自己掌握了哪些基础。

### 9.总结

学了之后要验证自己学得如何，可以来小编的网站看看这些面试题，通过面试题查漏补缺

小破站网址：后续会越来越完善，包括各种算法也都会更新，建议大家收藏。

总之，关于校招，学习路线，面试题等等，很多我在网站都更新了，包括个人经历，大家迷茫没事做时，可以多打开看看。

最后，大家加油，努力学两年，争取日后那个好的 offer.

原文链接：[https://juejin.cn/post/69972481](https://link.zhihu.com/?target=https%3A//juejin.cn/post/6997248187413037070)

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.101】什么是DPDK？DPDK的原理及学习路线总结

## 1.什么是DPDK

　　对于用户来说，它可能是一个性能出色的包数据处 理加速软件库；对于开发者来说，它可能是一个实践包处理新想法的创 新工场；对于性能调优者来说，它可能又是一个绝佳的成果分享平台。　

　　DPDK用软件的方式在通用多核处理器上演绎着数据包处理的新篇 章，而对于数据包处理，多核处理器显然不是唯一的平台。支撑包处理 的主流硬件平台大致可分为三个方向。
　　·硬件加速器
　　·网络处理器
　　·多核处理器

　　在类似 IA（Intel Architecture）多核处理器为目标的平台上，网络数据包处理远早于DPDK而存在。从商业版的 Windows到开源的Linux操作系统，所有跨主机通信几乎都会涉及网络 协议栈以及底层网卡驱动对于数据包的处理。然而，低速网络与高速网 络处理对系统的要求完全不一样。

## 2.DPDK原理

网络设备（路由器、交换机、媒体网关、SBC、PS网关等）需要在瞬间进行大量的报文收发，因此在传统的网络设备上，往往能够看到专门的NP（Network Process）处理器，有的用FPGA，有的用ASIC。这些专用器件通过内置的硬件电路（或通过编程形成的硬件电路）高效转发报文，只有需要对报文进行深度处理的时候才需要CPU干涉。

但在公有云、NFV等应用场景下，基础设施以CPU为运算核心，往往不具备专用的NP处理器，操作系统也以通用Linux为主，网络数据包的收发处理路径如下图所示：

![img](https://pic4.zhimg.com/80/v2-39d9c5742815718ffc3e616342c75c9b_720w.webp)

在虚拟化环境中，路径则会更长：

![img](https://pic3.zhimg.com/80/v2-ee0175b746bbf5eed76de9f29bb4bbda_720w.webp)

由于包处理任务存在内核态与用户态的切换，以及多次的内存拷贝，系统消耗变大，以CPU为核心的系统存在很大的处理瓶颈。为了提升在通用服务器（COTS）的数据包处理效能，Intel推出了服务于IA（Intel Architecture）系统的DPDK技术。

DPDK是Data Plane Development Kit的缩写。简单说，DPDK应用程序运行在操作系统的User Space，利用自身提供的数据面库进行收发包处理，绕过了Linux内核态协议栈，以提升报文处理效率。

## 3.DPDK源码目录结构　

　　lib/ : DPDK的库源代码
　　drivers/ : DPDK轮询模式驱动程序源代码
　　app/ : DPDK应用程序源代码
　　examples/ : DPDK的一些应用程序例子源代码
　　config/ : DPDK关于arm和x86平台的一些编译配置
　　buildtools/ : DPDK一些编译配置的脚本
　　mk/ : DPDK的Makefile
　　usertools/ : DPDK提供给用户的一些实用工具

## 4.常用术语及缩写

　　ACL：Access Control List，访问控制列表，是路由器和交换机接口的指令列表，用来控制端口进出的数据包；简而言之就是用来控制数据流。
　　SSL：Secure Sockets Layer，安全套接层，是为网络通信提供安全及数据完整性的一种安全协议，在传输层对网络连接进行加密。
　　RSS：Receive Side Scaling，是一种能够在多处理器系统下使接收报文在多个CPU之间高效分发的网卡驱动技术。
　　NUMA：Non Uniform Memory Access Architecture，非统一内存访问架构；
　　QOS：Quality of Service，服务质量，指一个网络能够利用各种基础技术，为指定的网络通信提供更好的服务能力, 是网络的一种安全机制， 是用来解决网络延迟和阻塞等问题的一种技术。
　　NIC：Network Interface Card，网卡，网卡是局域网中最基本的部件之一，它是连接计算机与网络的硬件设备。
　　PCI：Peripheral Component Interconnect，计算机一种标准总线，NIC就是使用的这种总线方式。
　　PMD：Poll Mode Drive，轮询模式驱动，DPDK就是采用的这种模式。
　　RTE：Run Time Environment，通过PMD实现快速分组处理数据的一个框架。
　　MPLS：Multi-Protocol Label Switching，多协议标签交换，是一种用于快速数据包交换和路由的体系，它为网络数据流量提供了目标、路由地址、转发和交换等能力。更特殊的是，它具有管理各种不同形式通信流的机制。

## 5.DPDK框架简介

　　DPDK为IA上的高速包处理而设计。

　　图1-6所示的DPDK主要模块分 解展示了以基础软件库的形式，为上层应用的开发提供一个高性能的基 础I/O开发包。它大量利用了有助于包处理的软硬件特性，如大页、缓 存行对齐、线程绑定、预取、NUMA、IA最新指令的利用、Intel DDIO、内存交叉访问等。
　　核心库Core Libs，提供系统抽象、大页内存、缓存池、定时器及无 锁环等基础组件。
　　PMD库，提供全用户态的驱动，以便通过轮询和线程绑定得到极高 的网络吞吐，支持各种本地和虚拟的网卡。
　　Classify库，支持精确匹配（Exact Match）、最长匹配（LPM）和 通配符匹配（ACL），提供常用包处理的查表操作。
　　QoS库，提供网络服务质量相关组件，如限速（Meter）和调度 （Sched）。

![img](https://pic1.zhimg.com/80/v2-617431c4136778d6ccc9462016a76f10_720w.webp)

## 6.DPDK的轮询模式

　　DPDK采用了轮询或者轮询混杂中断的模式来进行收包和发包，此 前主流运行在操作系统内核态的网卡驱动程序基本都是基于异步中断处 理模式。

## 　　1、异步中断模式

　　当有包进入网卡收包队列后，网卡会产生硬件 （MSIX/MSI/INTX）中断，进而触发CPU中断，进入中断服务程序，在 中断服务程序（包含下半部）来完成收包的处理。当然为了改善包处理 性能，也可以在中断处理过程中加入轮询，来避免过多的中断响应次 数。总体而言，基于异步中断信号模式的收包，是不断地在做中断处 理，上下文切换，每次处理这种开销是固定的，累加带来的负荷显而易 见。在CPU比I/O速率高很多时，这个负荷可以被相对忽略，问题不 大，但如果连接的是高速网卡且I/O频繁，大量数据进出系统，开销累 加就被充分放大。中断是异步方式，因此CPU无需阻塞等待，有效利用 率较高，特别是在收包吞吐率比较低或者没有包进入收包队列的时候， CPU可以用于其他任务处理。
当有包需要发送出去的时候，基于异步中断信号的驱动程序会准备 好要发送的包，配置好发送队列的各个描述符。在包被真正发送完成 时，网卡同样会产生硬件中断信号，进而触发CPU中断，进入中断服务 程序，来完成发包后的处理，例如释放缓存等。与收包一样，发送过程 也会包含不断地做中断处理，上下文切换，每次中断都带来CPU开销； 同上，CPU有效利用率高，特别是在发包吞吐率比较低或者完全没有发 包的情况。

## 　　2、轮询模式

　　DPDK起初的纯轮询模式是指收发包完全不使用任何中断，集中所 有运算资源用于报文处理。但这不是意味着DPDK不可以支持任何中 断。根据应用场景需要，中断可以被支持，最典型的就是链路层状态发 生变化的中断触发与处理。

原文链接：https://zhuanlan.zhihu.com/p/397919872

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.102】腾讯同事内推的那位Linux C/C++后端开发同学面试没过......

最近同事内推了一位 Linux C/C++ 后端开发的同学到我们公司面试，很遗憾这位工作了两年的同学面试表现不是很好。我问了如下一些问题：

> “redis持久化机制，redis销毁方式机制，mq实现原理，c++虚函数，hash冲突的解决，memcached一致性哈希，socket函数select的缺陷，epoll模型，同步互斥，异步非阻塞，回调的概念，innodb索引原理，单向图最短路径，动态规划算法。”

为了帮助更多的同学拿到满意的 offer，我整理了一下发出来，那么 Linux C/C++ 岗位一般会问哪些知识点呢？

## 1.思路分析

除了关于 c++ 虚函数这个问题以外，其他的大多数问题都与哪种编程语言关系不大，大多数是原理性和基础性的问题，少数是工作经验问题，我试着给大家分析分析。

## 2.语言基础

C++ 虚函数这是面试初、中级 C ++ 职位一个概率95%以上的面试题。一般有以下几种问法：

1. 在有继承关系的父子类中，构建和析构一个子类对象时，父子构造函数和析构函数的执行顺序分别是怎样的？
2. 在有继承关系的类体系中，父类的构造函数和析构函数一定要申明为 virtual 吗？如果不申明为 virtual 会怎样？
3. 什么是 C++ 多态？C++ 多态的实现原理是什么？
4. 什么是虚函数？虚函数的实现原理是什么？
5. 什么是虚表？虚表的内存结构布局如何？虚表的第一项（或第二项）是什么？
6. 菱形继承（类D同时继承B和C，B和C又继承自A）体系下，虚表在各个类中的布局如何？如果类B和类C同时有一个成员变了m，m如何在D对象的内存地址上分布的？是否会相互覆盖？

另外，时至今日，你一定要熟悉 C++11/14/17 常用的语言特性和类库，这里简单地列一下：

- 统一的类成员初始化语法与 std::initializer_list
- 注解标签（attributes）
- final/override/=default/=delete 语法
- auto 关键字
- Range-based 循环语法
- 结构化绑定
- stl 容器新增的实用方法
- std::thread
- 线程局部存储 thread_local
- 线程同步原理 std::mutex、std::condition_variable 等
- 原子操作类
- 智能指针类
- std::bind/std::function

C++11/14 网上的资料已经很多了，C++17 的资料不多，重头戏还是 C++11 引入的各种实用特性，这就给读者推荐一本我读过的：

- 《深入理解 C++11：C++11 新特性解析与应用》
- 《深入应用 C++11：代码优化与工程级应用》
- 《C++17 完全指南》
- 《Cpp 17 in Detail》

## 3.算法与数据结构基础

说到算法和数据结构，对于社招人士和对于应届生一般是不一样的，对于大的互联网公司和一般的小的企业也是不一样的。下面根据我当面试官面试别人和找工作被别人面试经验来谈一谈。

先说考察的内容，除了一些特殊的岗位，常见的算法和数据结构面试问题有如下：

### 3.1.排序（常考的排序按频率考排序为：快速排序 > 冒泡排序 > 归并排序 > 桶排序）

一般对于对算法基础有要求的公司，如果你是应届生或者工作经验在一至三年内，以上算法如果写不出来，给面试官的影响会非常不好，甚至直接被 pass 掉。对于工作三年以上的社会人士，如果写不出来，但是能分析出其算法复杂度、最好和最坏的情况下的复杂度，说出算法大致原理，在多数面试官面前也可以过的。注意，如果你是学生，写不出来或者写的不对，基本上面试过不了。

### 3.2.二分查找

二分查找的算法尽量要求写出来。当然，大多数面试官并不会直接问你二分查找，而是结合具体的场景，例如如何求一个数的平方根，这个时候你要能想到是二分查找。我在2017年年底，面试agora时，面试官问了一个问题：如何从所有很多的ip地址中快速找个某个ip地址。

### 3.3.链表

无论是应届生还是工作年限不长的社会人士，琏表常见的操作一定要熟练写出来，如链表的查找、定位、反转、连接等等。还有一些经典的问题也经常被问到，如两个链表如何判断有环（我在2017年面试饿了么二面、上海黄金交易所一面被问过）。链表的问题一般不难，但是链表的问题存在非常多的“坑”，如很多人不注意边界检查、空链表、返回一个链表的函数应该返回链表的头指针等等。

### 3.4.队列与栈

对于应届生来说一般这一类问的比较少，但是对于社会人士尤其是中高级岗位开发，会结合相关的问题问的比较多，例如让面试者利用队列写一个多线程下的生产者和消费者程序，全面考察的多线程的资源同步与竞态问题（下文介绍多线程面试题时详细地介绍）。
栈一般对于基础要求高的面试，会结合函数调用实现来问。即函数如何实现的，包括函数的调用的几种常见调用方式、参数的入栈顺序、内存栈在地址从高向低扩展、栈帧指针和栈顶指针的位置、函数内局部变量在栈中的内存分布、函数调用结束后，调用者和被调用者谁和如何清理栈等等。某年面试京东一基础部门，面试官让写从0加到100这样一个求和算法，然后写其汇编代码。

### 3.5.哈希表

哈希表是考察最多的数据结构之一。常见的问题有哈希冲突的检测、让面试者写一个哈希插入函数等等。基本上一场面试下来不考察红黑树基本上就会问哈希表，而且问题可浅可深。我印象比较深刻的是，当年面试百度广告推荐部门时，二面问的一些关于哈希表的问题。当时面试官时先问的链表，接着问的哈希冲突的解决方案，后来让写一个哈希插入算法，这里需要注意的是，你的算法中插入的元素一定要是通用元素，所以对于 C++ 或者 Java 语言，一定要使用模板这一类参数作为哈希插入算法的对象。然后，就是哈希表中多个元素冲突时，某个位置的元素使用链表往后穿成一串的方案。最终考察 linux 下 malloc（下面的ptmalloc） 函数在频繁调用造成的内存碎片问题，以及开源方案解决方案 tcmalloc 和 jemalloc。总体下来，面试官是一步步引导你深入。（有兴趣的读者可以自行搜索，网上有很多相关资料）

### 3.6.树

面试高频的树是红黑树，也有一部分是B树（B+树）。
红黑树一般的问的深浅不一，大多数面试官只要能说出红黑树的概念、左旋右旋的方式、分析出查找和插入的平均算法复杂度和最好最坏时的算法复杂度，并不要写面试者写出具体代码实现。一般 C++ 面试问 stl 的map，java 面试问 TreeMap 基本上就等于开始问你红黑树了，要有心里准备。笔者曾经面试爱奇艺被问过红黑树。
B树一般不会直接问，问的最多的形式是通过问 MySQL 索引实现原理（数据库知识点将在下文中讨论）。笔者面试腾讯看点部门二面被问到过。

### 3.7.图

图的问题就我个人面试从来没遇到过，不过据我某位哥哥所说，他在进三星电子之前有一道面试题就是深度优先和广度优先问题。

### 3.8.其他的一些算法

如A*寻路、霍夫曼编码也偶尔会在某一个领域的公司的面试中被问到，如宝开（《植物大战僵尸》的母公司， 在上海人民广场附近有分公司）。

## 4.编码基本功

还有一类面试题不好分类，笔者姑且将其当作是考察编码基本功，这类问题既可以考察算法也可以考察你写代码基本素养，这些素养不仅包括编码风格、计算机英语水平、调试能力等，还包括你对细节的掌握和易错点理解，如有意识地对边界条件的检查和非法值的过滤。请读者看以下的代码执行结果是什么？

```
for(char i = 0; i < 256; ++i) {   printf("%d\n", i);}
```

Copy

下面再列举几个常见的编码题：

1. 实现一个 memmov 函数
   这个题目考查点在于 memmov 函数与 memcpy 函数的区别，这两者对于源地址与目标地址内存有重叠的这一情况的处理方式是不一样的。

2. 实现strcpy或strcpy函数
   这个函数写出来没啥难度，但是除了边界条件需要检查以外，还有一个容易被忽视的地方即其返回值一定要是目标内存地址，以支持所谓的链式拷贝。即：
   strcpy(dest3, strcpy(dest2, strcpy(dest1, src1)));

3. 实现atoi函数
   这个函数的签名如下：
   int atoi(const char* p);

   容易疏忽的地方有如下几点：

- 小数点问题，如数字0.123和.123都是合法的；
- 正负号问题，如+123和-123；
- 考虑如何识别第一个非法字符问题，如123Z89，则应转换成应该123。

## 5.多线程开发基础

现如今的多核CPU早已经是司空见惯，而多线程编程早已经是“飞入寻常百姓家”。对于大多数桌面应用（与 Web 开发相对），尤其是像后台开发这样的岗位，且面试者是社会人员（有一定的工作经验），如果面试者不熟悉多线程编程，那么一般会被直接 pass 掉。

这里说的“熟悉多线程编程”到底熟悉到什么程度呢？一般包括：知道何种场合下需要新建新的线程、线程如何创建和等待、线程与进程的关系、线程局部存储（TLS 或者叫 thread local）、多线程访问资源产生静态的原因和解决方案等等、熟练使用所在操作系统平台提供的线程同步的各种原语。

对于 C++ 开发者，你需要：

- 对于 Windows 开发者，你需要熟练使用 Interlock系列函数、CriticalSection、Event、Mutex、Semphore等API 函数和两个重要的函数 WaitForSingleObject、WaitForMultipleObjects。
- 对于linux 开发者，你需要熟练使用 mutex、semphore、condition_variable、read-write-lock 等操作系统API。

对于 Java，你需要熟悉使用 synchronized关键字、CountDownLatch、CyclicBarrier、Semaphore以及java.util.concurrent 等包中的大多数线程同步对象。

## 6.数据库

数据库知识一般在大的互联网企业对应届生不做硬性要求，对于小的互联网企业或社会人士一般有一定的要求。其要求一般包括：

1. 熟悉基本 SQL 操作
   包括增删改查（insert、delete、update、select语句），排序 order，条件查询（where 子语句），限制查询结果数量（LIMIT语句）等
2. 稍微高级一点的 SQL 操作（如Group by，in，join，left join，多表联合查询，别名的使用，select 子语句等）
3. 索引的概念、索引的原理、索引的创建技巧
4. 数据库本身的操作，建库建表，数据的导入导出
5. 数据库用户权限控制（权限机制）
6. MySQL的两种数据库引擎的区别
7. SQL 优化技巧

以上属于对开发的基本的数据库知识要求，你可以找一本相关入门级的数据库图书学习即可。

高级开发除了以上要求还要熟悉高可用 MySQL、主从同步、读写分离、分表分库等技术，这些技术的细节一定要清楚，它们是你成为技术专家或者高级架构的必备知识。我们在实际面试时，在讨论高可用服务服务方案时，很多面试者也会和我们讨论到这些技术，但是不少面试者只知道这些技术的大致思想，细节往往说不清楚，细节不会就意味着你的高可用方案无法落地，企业需要可以落地的方案。

这些技术我首推《高性能 MySQL》这本书，这本书高级开发者一定要通读的，另外还有 2 本非常好的图书也推荐一下：一本是《MySQL 排错指南》，读完这本书以后，你会对整个“数据库世界”充满了清晰的认识；另外一本是《数据库索引设计与优化》，这本书读起来非常舒服，尤其是对于喜欢算法和数据结构的同学来说。

## 7.网络编程

网络编程这一块，对于应届生或者初级岗位一般只会问一些基础网络通信原理（如三次握手和四次挥手）的socket 基础 API 的使用，客户端与服务器端网络通信的流程（回答 【客户端创建socket -> 连接server ->收发数据；服务器端创建socket -> 绑定ip和端口号 -> 启动侦听 ->接受客户端连接 ->与客户端通信收发数据】即可）、TCP 与 UDP的区别等等。

对于工作经验三年以内的社会人士或者一些中级面试者一般会问一些稍微重难点问题，如 select 函数的用法，非阻塞 connect 函数的写法，epoll 的水平和边缘模式、阻塞socket与非阻塞socket的区别、send/recv函数的返回值情形、reuse_addr选项等等。Windows 平台可能还会问 WSAEventSelect 和 WSAAsyncSelect 函数的用法、完成端口（IOCP模型）。

对于三年以上尤其是“号称”自己设计过服务器、看过开源网络通信库代码的面试者，面试官一般会深入问一些问题，这类问题要么是实际项目中常见的难题或者网络通信细节，根据我的经验，一般有这样一些问题：

1. nagle算法；
2. keepalive选项；
3. Linger选项；
4. 对于某一端出现大量CLOSE_WAIT 或者 TIME_WAIT如何解决；
5. 通讯协议如何设计或如何解决数据包的粘包与分片问题；
6. 心跳机制如何设计；（可能不会直接问问题本身，如问如何检查死链）
7. 断线重连机制如何设计；
8. 对 IO Multiplexing 技术的理解；
9. 收发数据包正确的方式，收发缓冲区如何设计；
10. 优雅关闭；
11. 定时器如何设计；
12. epoll 的实现原理。

举个例子，让读者感受一下，笔者曾去BiliBili被问过这样一个问题：如果A机器与B机器网络 connect 成功后从未互发过数据，此时其中一机器突然断电，则另外一台机器与断电的机器之间的网络连接处于哪种状态？

不知道读者是否能答出来。

网络编程对于已经工作了的或者时间不是很充裕的同学来说，如果想入门或者上手，不建议去读一些大部头的图书，容易坚持不下，最后放弃。

建议找一些通俗易懂又可快速实践的书，这里推荐韩国人尹圣雨写的《TCP/IP 网络编程》这本书，这本书尤其适合非科班出身或者网络编程小白的同学，常见的 socket API 以及网络通信模式都有介绍，且同时包括 Linux 和 Windows 两个操作系统平台。

我刚工作那会儿，做股票行情服务器的底层服务开发，需要熟悉网络编程，那会儿天天下班抱着这本书看，建议小白把书中的网络通信代码都自己敲一遍。

我们面试一些同学时，发现很多同学写的网络通信程序在本机测试没问题，一拿到局域网或者测试环境就不能正常工作，这本书会告诉你答案。

## 8.内存数据库技术

时下以NoSql key-value为思想的内存数据库大行其道，广泛地用于各种后台项目开发。所以熟悉一种或几种内存数据库程序已经是面试后台开发的基本要求，而这当中以 redis 和 memcached 为最典型代表，这里以 redis 为例。

- 第一层面一般是对 redis 的基础用法的考察
  如考察 redis 支持的基础数据类型、redis的数据持久化、事务等。
- 第二层面不仅考察 redis 的基础用法，还会深入到 redis 源码层面上，如 redis 的网络通信模型、redis 各种数据结构的实现等等。
- redis高可用、cluster、哨兵策略等。

笔者以为，无论是从找工作应付面试还是从提高技术的角度，redis 是一个非常值得学习的开源软件，希望广大读者有意识地去了解、学习它。

## 9.项目经验

除了社会招聘和一些小型的企业，一般的大型互联网公司对应届生不会做过多的项目经验要求，而是希望他们算法与数据结构等基础扎实、动手实践能力强即可。对于一般的小公司，对于应届生会要求其至少熟练使用一门编程语言以及相应的开发工具，号称熟悉linux C++ 开发的面试者，不熟悉 GDB 调试基本上不是真正的熟悉 linux C++ 开发；号称熟悉汇编或者反汇编，不熟悉 IDA 或者 OllyDbg，基本上也是名不符实的；号称熟悉 VC++ 开发，连F8、F9、F10、F11、F12等快捷键不熟悉也是难以经得住面试官的提问的；号称熟悉 Java 开发的却对 IDEA 或 eclipse 陌生，这也是说不过去的。

这里给一些学历不算好，学校不是非常有名，尤其是二本以下的广大想进入 IT 行业的同学一个建议，在大学期间除了要学好计算机专业基础知识以外，一定要熟练使用一门编程语言以及相应的开发工具。

关于项目经验，许多面试者认为一定要是自己参与的项目，其实也可以来源于你学习和阅读他人源码或开源软件的源码，如果你能理解并掌握这些开源软件中的思想和技术，在面试的时候能够与面试官侃侃而谈，面试官也会非常满意的。

很多同学可能纠结大学或者研究生期间要不要跟着导师做一些项目。当然，如果这些项目是课程要求，那么你必须得参加；如果这些项目是可以选择性的，尤其是一些仅仅拿着第三方的库进行所谓的包装和加工，那么建议可以少参加一些。

## 10.思路总结

不知道通过我上面的技术分析，聪明的读者是否已经明确本文开头“成都-go-戒炸鸡”同学提出的面试题中，哪些是技术面试重难点，哪些又是技术开发的重难点呢？

## 11.技术比重与薪资

这里根据我自己招人的经验来谈一谈技术水平与薪资，就上面的面试题来看：

- 第一层次：如果面试者能答出上面面试题中的C++基础问题和算法与数据结构题目（如 C++ 函数与hash冲突的解决、innodb索引原理，单向图最短路径，动态规划算法等），可以认为面试者是一个合格的初、中级开发者，薪资范围一般在6 ～ 12k（注意：这里上海为参考标准）。
- 第二层次：在第一层次基础之上，如果面试者还能答出上述面试题中网络编程相关的或者多线程相关的问题（如socket函数select的缺陷，epoll模型，同步互斥，异步非阻塞，回调的概念等），可以认为面试者是个基础不错的中级开发者，薪资范围一般在14～22k之间。
- 第三层次：在前两个层次之间，如果面试者还能回答出上述问题中关于redis、memcached和mq实现原理，说明面试者是一个有着不错项目经验并且对一些常用开源项目也有一定的理解，薪资可以给到22k +。

## 12.总结

工资收入是每个人的秘密，一般不轻易对外人道也。这里笔者冒天下之大不韪，只想说明一点——对于普通开发人员，提高薪资最好的捷径就是提高自己的技术，无论是“面向搜索引擎编程”还是“面向工资编程”终将得不偿失，聪明的你一定会深谋远虑的。

原文链接：https://zhuanlan.zhihu.com/p/383586929

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.103】从四线城市程序员，到深圳大厂架构师，我只用了半年

从苦逼的程序员，到现在的**Linux高级互联网架构师，**要问身份的转变给我带来了什么实质上的利益，那肯定是**薪水**了。除此之外就是**面子**，毕竟在大厂比在不知名小公司要长脸的多。

主要还是去年在家上班那段时间，感觉到了小公司的种种不便，最让人难以忍受的就是在家996随时待命，还只发底薪，真是令人窒息的操作，让我只想赶紧逃离这个公司。

![img](https://pic1.zhimg.com/80/v2-510e73918b0cd1069f60d0b2f1f7639c_720w.webp)

但成年人的世界从来就不是可以任性的，我也自知我的水平没办法胜任更好的岗位，于是我决定**工作之余提升自己**。

边上班边学习其实挺苦的，幸好时间不长，我也熬了过来。现在每个月看到工资卡上比之前涨了几倍的数字，觉得**当时的努力是很值**的。

咳咳，扯远了啊，言归正传，就跟大家分享一下，我是**怎样进阶**的。

## **1.** **学好C语言**

作为一名程序员，C++的难度在我看来是top级的，多少次被这玩意折腾的怀疑人生。但是！不入虎穴焉得虎子，它的确很难，但是它的**含金量和竞争力**同样也是没话说的。

会与不会，很多时候就是薪资高低的决定因素。

![img](https://pic2.zhimg.com/80/v2-a6a9b09b56496a588782fb33806eb029_720w.webp)

要学习C++，那就一定要先打牢C语言的基础，这是至关重要的前提条件。

## **2.** **看书**

有了扎实的C语言基础之后，可以开始学习C++了。

给大伙推荐一些我觉得不错的书。

▪ **《C++ Primer》 及习题册**

如果只想看一本教材，那我强烈推荐这本。《C++ Primer》 非常全面，方方面面都考虑到了，可读性也很强，很适合初学者。它的习题册也一定要买，搭配使用事半功倍。

![img](https://pic1.zhimg.com/80/v2-4f9573d37434595aef464cd0a4b5627c_720w.webp)

**▪ 《21天学通C++》**

听这个名字就知道，这也是一本适合初学者的书，难度没有上一本那样大，但基本知识都交代了，适合作为学习C++的第一本书。

![img](https://pic3.zhimg.com/80/v2-580d5740d933ea60dda9f01e9320ea4e_720w.webp)

▪ **《Effective C++》、《More Effective C++》**

这两本是初学者看完、练完《C++ Primer》后，用来提升的教材。告诉程序员在使用C++时应该怎么解决问题、要注意什么、避免什么，进阶之路必备好书。

![img](https://pic4.zhimg.com/80/v2-b1faf8d048e7b5c9662049e917f69fb3_720w.webp)

![img](https://pic1.zhimg.com/80/v2-35ccf2c850b39c5d2370a378021783c8_720w.webp)

## **3.** **看教程**

根据我自己的学习经验，其实单纯看书挺枯燥的，很多时候就是看不下去，所以我会**结合一些教程来**看。

![img](https://pic3.zhimg.com/80/v2-c71979939e6b6a7f6d608098bf103062_720w.webp)

也有很多一样在学习C++的同仁，可以**相互交流每天打卡**，有队友学起来才更有激情嘛。而且会在群里聊聊**行情**什么的，也有**项目实操**，是锻炼的好机会。

## **4.** **学习资料**

来点干货，这是群里大牛整理的**腾讯核心技术学习路线（T1-T9）**

![img](https://pic1.zhimg.com/80/v2-806d7ae9d425ebf381e284802f7f1118_720w.webp)

**腾讯职级技术学习提升路线详情版**：

![img](https://pic1.zhimg.com/80/v2-806d7ae9d425ebf381e284802f7f1118_720w.webp)

相比很多人学C++学的怀疑人生，我学习的过程其实**没有走多少弯路**，毕竟一开始就找到了优质的教程和学习资料，而且大牛带飞嘛，结果自然不同凡响。

我是感觉学习任何一门技术都不能闭门造车，因为学习过程中很多问题不是你一个人遇到过，**多和同仁交流**，钻牛角尖的概率低很多。

另外，作为一个过来人，也想提醒大家：想要学习C++，一定要努力且有耐心，不可能一天就能走到罗马，唯一可以做的，就是**立刻出发**。

原文链接：https://zhuanlan.zhihu.com/p/356387701

作者：Linux服务器研究

# 【NO.104】详解 epoll 原理【Redis，Netty，Nginx实现高性能IO的核心原理】

## 1.【Redis，Netty，Nginx 等实现高性能IO的核心原理】

### 1.1.I/O

![img](https://pic1.zhimg.com/80/v2-f2d47755f938a2b5a7d4d2e7c75458a8_720w.webp)
输入输出(input/output)的对象可以是文件(file)， 网络(socket)，进程之间的管道(pipe)。在linux系统中，都用文件描述符(fd)来表示。

### 1.2.I/O 多路复用（multiplexing）

![img](https://pic4.zhimg.com/80/v2-c2417f7394fdc1bef0b34623dee68923_720w.webp)

I/O 多路复用的本质，是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。

select、poll 和 epoll 都是 Linux API 提供的 IO 复用方式。
Linux中提供的epoll相关函数如下：

```
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

## 2.Unix五种IO模型

我们在进行编程开发的时候，经常会涉及到同步，异步，阻塞，非阻塞，IO多路复用等概念，这里简单总结一下。

Unix网络编程中的五种IO模型：

```
Blocking IO - 阻塞IO
NoneBlocking IO - 非阻塞IO
IO multiplexing - IO多路复用
signal driven IO - 信号驱动IO
asynchronous IO - 异步IO
```

对于一个network IO，它会涉及到两个系统对象：

1. Application 调用这个IO的进程
2. kernel 系统内核

那他们经历的两个交互过程是：

阶段1： wait for data 等待数据准备；
阶段2： copy data from kernel to user 将数据从内核拷贝到用户进程中。

之所以会有同步、异步、阻塞和非阻塞这几种说法就是根据程序在这两个阶段的处理方式不同而产生的。

## 3.事件

可读事件，当文件描述符关联的内核读缓冲区可读，则触发可读事件。
(可读：内核缓冲区非空，有数据可以读取)

可写事件，当文件描述符关联的内核写缓冲区可写，则触发可写事件。
(可写：内核缓冲区不满，有空闲空间可以写入）

## 4.通知机制

通知机制，就是当事件发生的时候，则主动通知。通知机制的反面，就是轮询机制。

## 5.epoll 的通俗解释

结合以上三条，epoll的通俗解释是：

> 当文件描述符的内核缓冲区非空的时候，发出可读信号进行通知，当写缓冲区不满的时候，发出可写信号通知的机制。

## 6.epoll 数据结构 + 算法

epoll 的核心数据结构是：1个红黑树和1个链表。还有3个核心API。如下图所示：

![img](https://pic1.zhimg.com/80/v2-6c7a55178a7b2f5454aa36a9ba4a12e0_720w.webp)

## 7.就绪列表的数据结构

就绪列表引用着就绪的socket，所以它应能够快速的插入数据。

程序可能随时调用epoll_ctl添加监视socket，也可能随时删除。当删除时，若该socket已经存放在就绪列表中，它也应该被移除。（事实上，每个epoll_item既是红黑树节点，也是链表节点，删除红黑树节点，自然删除了链表节点）

所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列（对应上图的rdllist）。

## 8.epoll 索引结构

既然epoll将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的socket。至少要方便的添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好。epoll 使用了红黑树作为索引结构。

Epoll在linux内核中源码主要为 eventpoll.c 和 eventpoll.h 主要位于fs/eventpoll.c 和 include/linux/eventpool.h, 具体可以参考linux3.16。

下述为部分关键数据结构摘要, 主要介绍epitem 红黑树节点 和eventpoll 关键入口数据结构，维护着链表头节点ready list header和红黑树根节点RB-Tree root。

```
/*
 * Each file descriptor added to the eventpoll interface will
 * have an entry of this type linked to the "rbr" RB tree.
 * Avoid increasing the size of this struct, there can be many thousands
 * of these on a server and we do not want this to take another cache line.
 */
struct epitem {
    union {
        /* RB tree node links this structure to the eventpoll RB tree */
        struct rb_node rbn;
        /* Used to free the struct epitem */
        struct rcu_head rcu;
    };

    /* List header used to link this structure to the eventpoll ready list */
    struct list_head rdllink;

    /*
     * Works together "struct eventpoll"->ovflist in keeping the
     * single linked chain of items.
     */
    struct epitem *next;

    /* The file descriptor information this item refers to */
    struct epoll_filefd ffd;

    /* Number of active wait queue attached to poll operations */
    int nwait;

    /* List containing poll wait queues */
    struct list_head pwqlist;

    /* The "container" of this item */
    struct eventpoll *ep;

    /* List header used to link this item to the "struct file" items list */
    struct list_head fllink;

    /* wakeup_source used when EPOLLWAKEUP is set */
    struct wakeup_source __rcu *ws;

    /* The structure that describe the interested events and the source fd */
    struct epoll_event event;
};

/*
 * This structure is stored inside the "private_data" member of the file
 * structure and represents the main data structure for the eventpoll
 * interface.
 */
struct eventpoll {
    /* Protect the access to this structure */
    spinlock_t lock;

    /*
     * This mutex is used to ensure that files are not removed
     * while epoll is using them. This is held during the event
     * collection loop, the file cleanup path, the epoll file exit
     * code and the ctl operations.
     */
    struct mutex mtx;

    /* Wait queue used by sys_epoll_wait() */
    wait_queue_head_t wq;

    /* Wait queue used by file->poll() */
    wait_queue_head_t poll_wait;

    /* List of ready file descriptors */
    struct list_head rdllist;

    /* RB tree root used to store monitored fd structs */
    struct rb_root rbr;

    /*
     * This is a single linked list that chains all the "struct epitem" that
     * happened while transferring ready events to userspace w/out
     * holding ->lock.
     */
    struct epitem *ovflist;

    /* wakeup_source used when ep_scan_ready_list is running */
    struct wakeup_source *ws;

    /* The user that created the eventpoll descriptor */
    struct user_struct *user;

    struct file *file;

    /* used to optimize loop detection check */
    int visited;
    struct list_head visited_list_link;
};
```

epoll使用RB-Tree红黑树去监听并维护所有文件描述符，RB-Tree的根节点。

调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个 红黑树 用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件.

当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已.

## 9.epoll API

### 9.1. int epoll_create(int size)

功能：

内核会产生一个epoll 实例数据结构并返回一个文件描述符，这个特殊的描述符就是epoll实例的句柄，后面的两个接口都以它为中心（即epfd形参）。size参数表示所要监视文件描述符的最大值，不过在后来的Linux版本中已经被弃用（同时，size不要传0，会报invalid argument错误）

### 9.2. int epoll_ctl(int epfd， int op， int fd， struct epoll_event *event)

功能：

将被监听的描述符添加到红黑树或从红黑树中删除或者对监听事件进行修改

```
typedef union epoll_data {
void *ptr; /* 指向用户自定义数据 */
int fd; /* 注册的文件描述符 */
uint32_t u32; /* 32-bit integer */
uint64_t u64; /* 64-bit integer */
} epoll_data_t;
struct epoll_event {
uint32_t events; /* 描述epoll事件 */
epoll_data_t data; /* 见上面的结构体 */
};
```

对于需要监视的文件描述符集合，epoll_ctl对红黑树进行管理，红黑树中每个成员由描述符值和所要监控的文件描述符指向的文件表项的引用等组成。

op参数说明操作类型：

EPOLL_CTL_ADD：向interest list添加一个需要监视的描述符EPOLL_CTL_DEL：从interest list中删除一个描述符EPOLL_CTL_MOD：修改interest list中一个描述符struct epoll_event结构描述一个文件描述符的epoll行为。在使用epoll_wait函数返回处于ready状态的描述符列表时，

data域是唯一能给出描述符信息的字段，所以在调用epoll_ctl加入一个需要监测的描述符时，一定要在此域写入描述符相关信息events域是bit mask，描述一组epoll事件，在epoll_ctl调用中解释为：描述符所期望的epoll事件，可多选。常用的epoll事件描述如下：

EPOLLIN：描述符处于可读状态EPOLLOUT：描述符处于可写状态EPOLLET：将epoll event通知模式设置成edge triggeredEPOLLONESHOT：第一次进行通知，之后不再监测EPOLLHUP：本端描述符产生一个挂断事件，默认监测事件EPOLLRDHUP：对端描述符产生一个挂断事件EPOLLPRI：由带外数据触发EPOLLERR：描述符产生错误时触发，默认检测事件

### 9.3. int epoll_wait(int epfd， struct epoll_event *events， int maxevents， int timeout);

功能：

阻塞等待注册的事件发生，返回事件的数目，并将触发的事件写入events数组中。

events: 用来记录被触发的events，其大小应该和maxevents一致

maxevents: 返回的events的最大个数处于ready状态的那些文件描述符会被复制进ready list中，epoll_wait用于向用户进程返回ready list。

events和maxevents两个参数描述一个由用户分配的struct epoll event数组，调用返回时，内核将ready list复制到这个数组中，并将实际复制的个数作为返回值。

注意，如果ready list比maxevents长，则只能复制前maxevents个成员；反之，则能够完全复制ready list。

另外，struct epoll event结构中的events域在这里的解释是：

> 在被监测的文件描述符上实际发生的事件。

参数timeout描述在函数调用中阻塞时间上限，单位是ms：

timeout = -1表示调用将一直阻塞，直到有文件描述符进入ready状态或者捕获到信号才返回；timeout = 0用于非阻塞检测是否有描述符处于ready状态，不管结果怎么样，调用都立即返回；timeout > 0表示调用将最多持续timeout时间，如果期间有检测对象变为ready状态或者捕获到信号则返回，否则直到超时。epoll的两种触发方式

epoll监控多个文件描述符的I/O事件。epoll支持边缘触发(edge trigger，ET)或水平触发（level trigger，LT)，通过epoll_wait等待I/O事件，如果当前没有可用的事件则阻塞调用线程。

select和poll只支持LT工作模式，epoll的默认的工作模式是LT模式。

## 10.epoll更高效的原因

select和poll的动作基本一致，只是poll采用链表来进行文件描述符的存储，而select采用fd标注位来存放，所以select会受到最大连接数的限制，而poll不会。

select、poll、epoll虽然都会返回就绪的文件描述符数量, 但是select和poll并不会明确指出是哪些文件描述符就绪，而epoll会。

造成的区别就是，系统调用返回后，调用select和poll的程序需要遍历监听的整个文件描述符找到是谁处于就绪，而epoll则直接处理即可（直接监听到了哪些文件描述符就绪）。

select、poll都需要将有关文件描述符的数据结构拷贝进内核，最后再拷贝出来。**而epoll创建的有关文件描述符的数据结构本身就存于内核态中，系统调用返回时利用 mmap() 文件映射内存加速与内核空间的消息传递：即 epoll 使用 mmap() 减少复制开销。**

select、poll采用 **轮询** 的方式来检查文件描述符是否处于就绪态，而epoll采用回调机制。造成的结果就是，随着fd的增加，select和poll的效率会线性降低，而epoll不会受到太大影响，除非活跃的socket很多。

epoll的边缘触发模式效率高，系统不会充斥大量不关心的就绪文件描述符。

虽然epoll的性能最好，但是在**连接数少并且连接都十分活跃**的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。

## 11.epoll与select、poll的对比

### 11.1. 用户态将文件描述符传入内核的方式

select：创建3个文件描述符集并拷贝到内核中，分别监听读、写、异常动作。这里受到单个进程可以打开的fd数量限制，默认是1024。

poll：将传入的struct pollfd结构体数组拷贝到内核中进行监听。

epoll：执行epoll_create，会在**内核**的高速cache区中，建立一颗红黑树以及就绪链表(该链表存储已经就绪的文件描述符)。接着用户执行的epoll_ctl函数，添加文件描述符会在红黑树上增加相应的结点。

### 11.2. 内核态检测文件描述符读写状态的方式

select：采用**轮询**方式，遍历所有fd，最后返回一个描述符读写操作是否就绪的mask掩码，根据这个掩码给fd_set赋值。

poll：同样采用**轮询**方式，查询每个fd的状态，如果就绪则在等待队列中加入一项并继续遍历。

epoll：采用**事件回调**机制。在执行 epoll_ctl 的add操作时，不仅将文件描述符放到红黑树上，而且也注册了回调函数;内核在检测到某文件描述符可读/可写时会调用回调函数，该回调函数将文件描述符放在就绪链表中。

### 11.3. 找到就绪的文件描述符并传递给用户态的方式

select：将之前传入的fd_set拷贝传出到**用户态**并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。

poll：将之前传入的 fd 数组拷贝传出**用户态**，并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。

epoll：epoll_wait 只用观察**就绪链表**中有无数据即可，最后将链表的数据返回给数组, 并返回就绪的数量。**内核**，将就绪的文件描述符，放在传入的数组中。然后，依次遍历，处理即可。这里返回的文件描述符，是通过 mmap() ，让内核和用户空间，共享同一块内存实现传递的，减少了不必要的拷贝。

### 11.4. 重复监听的处理方式

select：将新的监听文件描述符集合拷贝传入内核中，继续以上步骤。
poll：将新的struct pollfd结构体数组拷贝传入内核中，继续以上步骤。
epoll：无需重新构建红黑树，直接沿用已存在的即可。

## 12.epoll 水平触发与边缘触发

epoll事件有两种模型：

边沿触发：edge-triggered (ET)
水平触发：level-triggered (LT)

## 13.水平触发(level-triggered)

socket接收缓冲区不为空， 有数据可读， 读事件一直触发。
socket发送缓冲区不满， 可以继续写入数据， 写事件一直触发。

## 14.边沿触发(edge-triggered)

socket的接收缓冲区状态变化时，触发读事件，即空的接收缓冲区刚接收到数据时触发读事件。
socket的发送缓冲区状态变化时，触发写事件，即满的缓冲区刚空出空间时触发读事件。

边沿触发仅触发一次，水平触发会一直触发。

## 15.事件宏

EPOLLIN ： 表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
EPOLLOUT： 表示对应的文件描述符可以写；
EPOLLPRI： 表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
EPOLLERR： 表示对应的文件描述符发生错误；
EPOLLHUP： 表示对应的文件描述符被挂断；
EPOLLET： 将 EPOLL设为边缘触发(Edge Triggered)模式（默认为水平触发），这是相对于水平触发(Level Triggered)来说的。
EPOLLONESHOT： 只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里
libevent 采用水平触发， nginx 采用边沿触发

JDK并没有实现边缘触发，Netty重新实现了epoll机制，采用边缘触发方式；另外像Nginx也采用边缘触发。

JDK在Linux已经默认使用epoll方式，但是JDK的epoll采用的是水平触发，而Netty重新实现了epoll机制，采用边缘触发方式，netty epoll transport 暴露了更多的nio没有的配置参数，如 TCP_CORK, SO_REUSEADDR等等；另外像Nginx也采用边缘触发。

## 16.mmap() 文件映射内存

mmap是一种内存映射文件（文件映射内存，建立映射关系）的方法，即将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。

实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上（参考：Linux文件读写与缓存），即完成了对文件的操作而不必再调用read,write等系统调用函数。

> Dirty Page： 页缓存对应文件中的一块区域，如果页缓存和对应的文件区域内容不一致，则该页缓存叫做脏页（Dirty Page）。对页缓存进行修改或者新建页缓存，只要没有刷磁盘，都会产生脏页。Linux支持以下5种缓冲区类型：
> Clean 未使用、新创建的缓冲区
> Locked 被锁住、等待被回写
> Dirty 包含最新的有效数据，但还没有被回写
> Shared 共享的缓冲区
> Unshared 原来被共享但现在不共享

相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。如下图所示：

![img](https://pic4.zhimg.com/80/v2-88d705451c9b9f6d21102813786bc8ef_720w.webp)

由上图可以看出，进程的虚拟地址空间，由多个虚拟内存区域构成。虚拟内存区域是进程的虚拟地址空间中的一个同质区间，即具有同样特性的连续地址范围。上图中所示的text数据段（代码段）、初始数据段、BSS数据段、堆、栈和内存映射，都是一个独立的虚拟内存区域。而为内存映射服务的地址空间处在堆栈之间的空余部分。

linux内核使用vm_area_struct结构来表示一个独立的虚拟内存区域，由于每个不同质的虚拟内存区域功能和内部机制都不同，因此一个进程使用多个vm_area_struct结构来分别表示不同类型的虚拟内存区域。各个vm_area_struct结构使用链表或者树形结构链接，方便进程快速访问，如下图所示：

![img](https://pic1.zhimg.com/80/v2-2765fa33d5a4553c76041f0925b8ec9c_720w.webp)

vm_area_struct结构中包含区域起始和终止地址以及其他相关信息，同时也包含一个vm_ops指针，其内部可引出所有针对这个区域可以使用的系统调用函数。这样，进程对某一虚拟内存区域的任何操作需要用要的信息，都可以从vm_area_struct中获得。mmap函数就是要创建一个新的vm_area_struct结构，并将其与文件的物理磁盘地址相连。

## 17.mmap在write和read时会发生什么

### **17.1.write**

- 1.进程(用户态)将需要写入的数据直接copy到对应的mmap地址(内存copy)
- 2.若mmap地址未对应物理内存，则产生缺页异常，由内核处理
- 3.若已对应，则直接copy到对应的物理内存
- 4.由操作系统调用，将脏页回写到磁盘（通常是异步的）

因为物理内存是有限的，mmap在写入数据超过物理内存时，操作系统会进行页置换，根据淘汰算法，将需要淘汰的页置换成所需的新页，所以mmap对应的内存是可以被淘汰的（若内存页是”脏”的，则操作系统会先将数据回写磁盘再淘汰）。这样，就算mmap的数据远大于物理内存，操作系统也能很好地处理，不会产生功能上的问题。

### **17.2.read**

![img](https://pic3.zhimg.com/80/v2-ee003c210c5d77d97df12bf4c2b3b666_720w.webp)

从图中可以看出，mmap要比普通的read系统调用少了一次copy的过程。因为read调用，进程是无法直接访问kernel space的，所以在read系统调用返回前，内核需要将数据从内核复制到进程指定的buffer。但mmap之后，进程可以直接访问mmap的数据(page cache)。

## 18.总结

一张图总结一下select,poll,epoll的区别：

![img](https://pic1.zhimg.com/80/v2-5a2a074550dda2645bc3f0a65d66f108_720w.webp)

epoll是Linux目前大规模网络并发程序开发的首选模型。在绝大多数情况下性能远超select和poll。目前流行的高性能web服务器Nginx正式依赖于epoll提供的高效网络套接字轮询服务。但是，在并发连接不高的情况下，多线程+阻塞I/O方式可能性能更好。

既然select，poll，epoll都是I/O多路复用的具体的实现，之所以现在同时存在，其实他们也是不同历史时期的产物：

- select出现是1984年在BSD里面实现的

- 14年之后也就是1997年才实现了poll，其实拖那么久也不是效率问题， 而是那个时代的硬件实在太弱，一台服务器处理1千多个链接简直就是神一样的存在了，select很长段时间已经满足需求

- 2002, 大神 Davide Libenzi 实现了epoll。

  

  原文链接：https://zhuanlan.zhihu.com/p/364832778

  作者：[Linux服务器研究](https://www.zhihu.com/people/shao-nian-bu-nian-shao-zhu-80)


# 【NO.105】这篇Redis文章，图灵看了都说好

> 2007 年，他和朋友一起创建了一个网站。为了解决这个网站的负载问题，他自己定制了一个数据库。2009 年开发的，这个是 Redis。这位意大利程序员是萨尔瓦托勒·桑菲利波(Salvatore Sanfilippo)，他被称为Redis之父，更广为人知的名字是Antirez。

## **1.Redis简介**

REmote DIctionary Server(Redis) 是一个开源的使用 ANSI C 语言编写、遵守 BSD 协议、支持网络、可基于内存、分布式、可选持久性的键值对(Key-Value)存储数据库，并提供多种语言的 API。

Redis 通常被称为数据结构服务器，因为值（value）可以是字符串(String)、哈希(Hash)、列表(list)、集合(sets)和有序集合(sorted sets)等类型。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171421193031639.png)

## **2.内存模型**

首先可以进行Redis的内存模型学习，对Redis的使用有很大帮助，例如OOM时定位、内存使用量评估等。

通过info memory命令查看内存的使用情况。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171421366449791.png)

主要参数：

1. used_memory：从Redis角度使用了多少内存，即Redis分配器分配的内存总量（单位是字节），**包括使用的虚拟内存**（即swap）；
2. used_memory_rss：从操作系统角度实际使用量，即Redis进程占据操作系统的内存（单位是字节），**包括进程运行本身需要的内存、内存碎片**等，**不包括虚拟内存**。一般情况下used_memory_rss都要比used_memory大，因为Redis频繁删除读写等操作使得内存碎片较多，而虚拟内存的使用一般是非极端情况下是不怎么使用的；
3. mem_fragmentation_ratio：即内存碎片比率，该值是used_memory_rss / used_memory的比值；mem_fragmentation_ratio一般大于1，且该值越大，内存碎片比例越大。如果mem_fragmentation_ratio<1，说明Redis使用了虚拟内存，由于虚拟内存的媒介是磁盘，比内存速度要慢很多，当这种情况出现时，应该及时排查，如果内存不足应该及时处理，如增加Redis节点、增加Redis服务器的内存、优化应用等。一般来说，mem_fragmentation_ratio在1.03左右是比较健康的状态（对于jemalloc来说）；
4. mem_allocator：即Redis使用的内存分配器，一般默认是jemalloc。

### 2.1 Redis内存划分

1. 数据

作为数据库，数据是最主要的部分，这部分占用的内存会统计在used_memory中。

1. 进程本身运行需要的内存

这部分内存不是由jemalloc分配，因此不会统计在used_memory中。

1. 缓冲内存

缓冲内存包括：

- 客户端缓冲区：存储客户端连接的输入输出缓冲；
- 复制积压缓冲区：用于部分复制功能；
- aof_buf：用于在进行AOF重写时，保存最近的写入命令。

这部分内存由jemalloc分配，因此会统计在used_memory中。

1. 内存碎片

内存碎片是Redis在数据更改频繁分配、回收物理内存过程中产生的。

内存碎片不会统计在used_memory中。

### 2.2 Redis数据存储的细节

下面看一张经典的图。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171421485421259.png)

1. jemalloc

无论是DictEntry对象，还是RedisObject、SDS对象，都需要内存分配器（如jemalloc）分配内存进行存储。Redis在编译时便会指定内存分配器；内存分配器可以是 libc 、jemalloc或者tcmalloc，默认是jemalloc。当Redis存储数据时，会选择大小最合适的内存块进行存储。

jemalloc划分的内存单元如下图所示：
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171422022754159.png)

1. dictEntry

每个dictEntry都保存着一个键值对，key值保存一个sds结构体，value值保存一个redisObject结构体。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171422142621840.png)

1. redisObject

前面说到，Redis对象有5种类型；无论是哪种类型，Redis都不会直接存储，而是通过RedisObject对象进行存储。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171422259288942.png)

RedisObject的每个字段的含义和作用如下：

- type

type字段表示对象的类型，占4个比特；目前包括REDIS_STRING(字符串)、REDIS_LIST (列表)、REDIS_HASH(哈希)、REDIS_SET(集合)、REDIS_ZSET(有序集合)。

- encoding

encoding表示对象的内部编码，占4个比特(redis-3.0)。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171422366477395.png)

- lru

lru记录的是对象最后一次被命令程序访问的时间，占据的比特数不同的版本有所不同（如4.0版本占24比特，2.6版本占22比特）。

- refcount

refcount记录的是该对象被引用的次数，类型为整型。refcount的作用，主要在于对象的引用计数和内存回收：

- 当创建新对象时，refcount初始化为1；
- 当有新程序使用该对象时，refcount加1；
- 当对象不再被一个新程序使用时，refcount减1；
- 当refcount变为0时，对象占用的内存会被释放。

Redis中被多次使用的对象(refcount>1)称为共享对象。Redis为了节省内存，当有一些对象重复出现时，新的程序不会创建新的对象，而是仍然使用原来的对象。这个被重复使用的对象，就是共享对象。目前共享对象仅支持整数值的字符串对象。

**共享对象的具体实现**

Redis的共享对象目前只支持整数值的字符串对象。之所以如此，实际上是对内存和CPU（时间）的平衡：共享对象虽然会降低内存消耗，但是判断两个对象是否相等却需要消耗额外的时间。

对于整数值，判断操作复杂度为O(1)；

对于普通字符串，判断复杂度为O(n)；

而对于哈希、列表、集合和有序集合，判断的复杂度为O(n^2)。

虽然共享对象只能是整数值的字符串对象，但是5种类型都可能使用共享对象（如哈希、列表等的元素可以使用）。

就目前的实现来说，Redis服务器在初始化时，会创建10000个字符串对象，值分别是0~~9999的整数值；当Redis需要使用值为0~~9999的字符串对象时，可以直接使用这些共享对象。10000这个数字可以通过调整参数REDIS_SHARED_INTEGERS（4.0中是OBJ_SHARED_INTEGERS）的值进行改变。

- ptr

ptr指针指向具体的数据，如前面的例子中，set hello world，ptr指向包含字符串world的SDS。

1. SDS

Redis没有直接使用C字符串(即以空字符‘\0’结尾的字符数组)作为默认的字符串表示，而是使用了SDS。SDS是简单动态字符串(Simple Dynamic String)的缩写。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171422569473144.png))

通过SDS的结构可以看出，buf数组的长度=free+len+1（其中1表示字符串结尾的空字符）；所以，一个SDS结构占据的空间为：free所占长度+len所占长度+ buf数组的长度=4+4+free+len+1=free+len+9。

**为什么使用SDS而不直接使用C字符串？**

SDS在C字符串的基础上加入了free和len字段，带来了很多好处：

- 获取字符串长度：SDS是O(1)，C字符串是O(n)。
- 缓冲区溢出：使用C字符串的API时，如果字符串长度增加（如strcat操作）而忘记重新分配内存，很容易造成缓冲区的溢出；而SDS由于记录了长度，相应的API在可能造成缓冲区溢出时会自动重新分配内存，杜绝了缓冲区溢出。
- 修改字符串时内存的重分配：对于C字符串，如果要修改字符串，必须要重新分配内存（先释放再申请），因为如果没有重新分配，字符串长度增大时会造成内存缓冲区溢出，字符串长度减小时会造成内存泄露。而对于SDS，由于可以记录len和free，因此解除了字符串长度和空间数组长度之间的关联，可以在此基础上进行优化——空间预分配策略（即分配内存时比实际需要的多）使得字符串长度增大时重新分配内存的概率大大减小；惰性空间释放策略使得字符串长度减小时重新分配内存的概率大大减小。
- 存取二进制数据：SDS可以，C字符串不可以。因为C字符串以空字符作为字符串结束的标识，而对于一些二进制文件（如图片等），内容可能包括空字符串，因此C字符串无法正确存取；而SDS以字符串长度len来作为字符串结束标识，因此没有这个问题。

## **3.持久化Persistence**

持久化的功能：Redis是内存数据库，数据都是存储在内存中，为了避免进程退出导致数据的永久丢失，需要定期将Redis中的数据以某种形式（数据或命令）从内存保存到硬盘。当下次Redis重启时，利用持久化文件实现数据恢复。除此之外，为了进行灾难备份，可以将持久化文件拷贝到一个远程位置。

Redis持久化分为RDB持久化和AOF持久化，前者将当前数据保存到硬盘，后者则是将每次执行的写命令保存到硬盘（类似于MySQL的Binlog）。由于AOF持久化的实时性更好，即当进程意外退出时丢失的数据更少，因此AOF是目前主流的持久化方式，不过RDB持久化仍然有其用武之地。

### 3.1 RDB持久化

RDB(Redis Database)持久化方式能够在指定的时间间隔能对你的数据进行快照存储。一般通过bgsave命令会创建一个子进程，由子进程来负责创建RDB文件，父进程(即Redis主进程)则继续处理请求。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171423116278070.png)

图片中的5个步骤所进行的操作如下：

1. Redis父进程首先判断：当前是否在执行save，或bgsave/bgrewriteaof（后面会详细介绍该命令）的子进程，如果在执行则bgsave命令直接返回。bgsave/bgrewriteaof 的子进程不能同时执行，主要是基于性能方面的考虑：两个并发的子进程同时执行大量的磁盘写操作，可能引起严重的性能问题。
2. 父进程执行fork操作创建子进程，这个过程中父进程是阻塞的，Redis不能执行来自客户端的任何命令；
3. 父进程fork后，bgsave命令返回”Background saving started”信息并不再阻塞父进程，并可以响应其他命令；
4. 子进程进程对内存数据生成快照文件；
5. 子进程发送信号给父进程表示完成，父进程更新统计信息。

**这里补充一下第4点是如何生成RDB文件的。一定有读者也有疑问：在同步到磁盘和持续写入这个过程是如何处理数据不一致的情况呢？生成快照RDB文件时是否会对业务产生影响？**

1. 通过 fork 创建的子进程能够获得和父进程完全相同的内存空间，父进程对内存的修改对于子进程是不可见的，两者不会相互影响；
2. 通过 fork 创建子进程时不会立刻触发大量内存的拷贝，采用的是写时拷贝COW (Copy On Write)。内核只为新生成的子进程创建虚拟空间结构，它们来复制于父进程的虚拟究竟结构，但是不为这些段分配物理内存，它们共享父进程的物理空间，当父子进程中有更改相应段的行为发生时，再为子进程相应的段分配物理空间；

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171423239627221.png)

### 3.2 AOF持久化

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171423335467197.png)

AOF(Append Only File)持久化方式记录每次对服务器写的操作，当服务器重启的时候会重新执行这些命令来恢复原始的数据，AOF命令以redis协议追加保存每次写的操作到文件末尾。Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大。

AOF的执行流程包括：

### 3.3 命令追加(append)

Redis先将写命令追加到缓冲区aof_buf，而不是直接写入文件，主要是为了避免每次有写命令都直接写入硬盘，导致硬盘IO成为Redis负载的瓶颈。

### 3.4 文件写入(write)和文件同步(sync)

根据不同的同步策略将aof_buf中的内容同步到硬盘；

Linux 操作系统中为了提升性能，使用了页缓存（page cache）。当我们将 aof_buf 的内容写到磁盘上时，此时数据并没有真正的落盘，而是在 page cache 中，为了将 page cache 中的数据真正落盘，需要执行 fsync / fdatasync 命令来强制刷盘。这边的文件同步做的就是刷盘操作，或者叫文件刷盘可能更容易理解一些。

AOF缓存区的同步文件策略由参数appendfsync控制，各个值的含义如下：

- always：命令写入aof_buf后立即调用系统write操作和系统fsync操作同步到AOF文件，fsync完成后线程返回。这种情况下，每次有写命令都要同步到AOF文件，硬盘IO成为性能瓶颈，Redis只能支持大约几百TPS写入，严重降低了Redis的性能；即便是使用固态硬盘（SSD），每秒大约也只能处理几万个命令，而且会大大降低SSD的寿命。**可靠性较高，数据基本不丢失。**
- no：命令写入aof_buf后调用系统write操作，不对AOF文件做fsync同步；同步由操作系统负责，通常同步周期为30秒。这种情况下，文件同步的时间不可控，且缓冲区中堆积的数据会很多，数据安全性无法保证。
- everysec：命令写入aof_buf后调用系统write操作，write完成后线程返回；fsync同步文件操作由专门的线程每秒调用一次。everysec是前述两种策略的折中，是性能和数据安全性的平衡，因此是Redis的默认配置，也是我们推荐的配置。

**有同学可能会疑问为什么always策略还是不能100%保障数据不丢失，例如在开启AOF的情况下，有一条写命令，Redis在写命令执行完，写aof_buf未成功的情况下宕机了?**

不能，Redis就不能100%保证数据不丢失。

```
void flushAppendOnlyFile(int force) {    ssize_t nwritten;    int sync_in_progress = 0;    mstime_t latency;    if (sdslen(server.aof_buf) == 0) return;    if (server.aof_fsync == AOF_FSYNC_EVERYSEC)        sync_in_progress = bioPendingJobsOfType(REDIS_BIO_AOF_FSYNC) != 0;    if (server.aof_fsync == AOF_FSYNC_EVERYSEC && !force) {        /* With this append fsync policy we do background fsyncing.         * If the fsync is still in progress we can try to delay         * the write for a couple of seconds. */        if (sync_in_progress) {            if (server.aof_flush_postponed_start == 0) {                /* No previous write postponing, remember that we are                 * postponing the flush and return. */                server.aof_flush_postponed_start = server.unixtime;                return;            } else if (server.unixtime - server.aof_flush_postponed_start < 2) {                /* We were already waiting for fsync to finish, but for less                 * than two seconds this is still ok. Postpone again. */                return;            }            /* Otherwise fall trough, and go write since we can't wait             * over two seconds. */            server.aof_delayed_fsync++;            redisLog(REDIS_NOTICE,"Asynchronous AOF fsync is taking too long (disk is busy?). Writing the AOF buffer without waiting for fsync to complete, this may slow down Redis.");        }    }    /* We want to perform a single write. This should be guaranteed atomic     * at least if the filesystem we are writing is a real physical one.     * While this will save us against the server being killed I don't think     * there is much to do about the whole server stopping for power problems     * or alike */    latencyStartMonitor(latency);    nwritten = write(server.aof_fd,server.aof_buf,sdslen(server.aof_buf));    latencyEndMonitor(latency);    /* We want to capture different events for delayed writes:     * when the delay happens with a pending fsync, or with a saving child     * active, and when the above two conditions are missing.     * We also use an additional event name to save all samples which is     * useful for graphing / monitoring purposes. */    if (sync_in_progress) {        latencyAddSampleIfNeeded("aof-write-pending-fsync",latency);    } else if (server.aof_child_pid != -1 || server.rdb_child_pid != -1) {        latencyAddSampleIfNeeded("aof-write-active-child",latency);    } else {        latencyAddSampleIfNeeded("aof-write-alone",latency);    }    latencyAddSampleIfNeeded("aof-write",latency);    /* We performed the write so reset the postponed flush sentinel to zero. */    server.aof_flush_postponed_start = 0;    if (nwritten != (signed)sdslen(server.aof_buf)) {        static time_t last_write_error_log = 0;        int can_log = 0;        /* Limit logging rate to 1 line per AOF_WRITE_LOG_ERROR_RATE seconds. */        if ((server.unixtime - last_write_error_log) > AOF_WRITE_LOG_ERROR_RATE) {            can_log = 1;            last_write_error_log = server.unixtime;        }        /* Log the AOF write error and record the error code. */        if (nwritten == -1) {            if (can_log) {                redisLog(REDIS_WARNING,"Error writing to the AOF file: %s",                    strerror(errno));                server.aof_last_write_errno = errno;            }        } else {            if (can_log) {                redisLog(REDIS_WARNING,"Short write while writing to "                                       "the AOF file: (nwritten=%lld, "                                       "expected=%lld)",                                       (long long)nwritten,                                       (long long)sdslen(server.aof_buf));            }            if (ftruncate(server.aof_fd, server.aof_current_size) == -1) {                if (can_log) {                    redisLog(REDIS_WARNING, "Could not remove short write "                             "from the append-only file.  Redis may refuse "                             "to load the AOF the next time it starts.  "                             "ftruncate: %s", strerror(errno));                }            } else {                /* If the ftruncate() succeeded we can set nwritten to                 * -1 since there is no longer partial data into the AOF. */                nwritten = -1;            }            server.aof_last_write_errno = ENOSPC;        }        /* Handle the AOF write error. */        if (server.aof_fsync == AOF_FSYNC_ALWAYS) {            /* We can't recover when the fsync policy is ALWAYS since the             * reply for the client is already in the output buffers, and we             * have the contract with the user that on acknowledged write data             * is synced on disk. */            redisLog(REDIS_WARNING,"Can't recover from AOF write error when the AOF fsync policy is 'always'. Exiting...");            exit(1);        } else {            /* Recover from failed write leaving data into the buffer. However             * set an error to stop accepting writes as long as the error             * condition is not cleared. */            server.aof_last_write_status = REDIS_ERR;            /* Trim the sds buffer if there was a partial write, and there             * was no way to undo it with ftruncate(2). */            if (nwritten > 0) {                server.aof_current_size += nwritten;                sdsrange(server.aof_buf,nwritten,-1);            }            return; /* We'll try again on the next call... */        }    } else {        /* Successful write(2). If AOF was in error state, restore the         * OK state and log the event. */        if (server.aof_last_write_status == REDIS_ERR) {            redisLog(REDIS_WARNING,                "AOF write error looks solved, Redis can write again.");            server.aof_last_write_status = REDIS_OK;        }    }    server.aof_current_size += nwritten;    /* Re-use AOF buffer when it is small enough. The maximum comes from the     * arena size of 4k minus some overhead (but is otherwise arbitrary). */    if ((sdslen(server.aof_buf)+sdsavail(server.aof_buf)) < 4000) {        sdsclear(server.aof_buf);    } else {        sdsfree(server.aof_buf);        server.aof_buf = sdsempty();    }    /* Don't fsync if no-appendfsync-on-rewrite is set to yes and there are     * children doing I/O in the background. */    if (server.aof_no_fsync_on_rewrite &&        (server.aof_child_pid != -1 || server.rdb_child_pid != -1))            return;    /* Perform the fsync if needed. */    if (server.aof_fsync == AOF_FSYNC_ALWAYS) {        /* aof_fsync is defined as fdatasync() for Linux in order to avoid         * flushing metadata. */        latencyStartMonitor(latency);        aof_fsync(server.aof_fd); /* Let's try to get this data on the disk */        latencyEndMonitor(latency);        latencyAddSampleIfNeeded("aof-fsync-always",latency);        server.aof_last_fsync = server.unixtime;    } else if ((server.aof_fsync == AOF_FSYNC_EVERYSEC &&                server.unixtime > server.aof_last_fsync)) {        if (!sync_in_progress) aof_background_fsync(server.aof_fd);        server.aof_last_fsync = server.unixtime;    }}
```

那么从上面redis-3.0的源码及上下文

```
if (server.aof_fsync == AOF_FSYNC_ALWAYS)
```

分析得出，其实我们每次执行客户端命令的时候操作并没有写到aof文件中，只是写到了aof_buf内存当中，只有当下一个事件来临时，才会去fsync到disk中，从redis的这种策略上我们也可以看出，redis和mysql在数据持久化之间的区别，redis的数据持久化仅仅就是一个附带功能，并不是其主要功能。

结论：**Redis即使在配制appendfsync=always的策略下，还是会丢失一个事件循环的数据。**

### 3.5 文件重写(rewrite)

定期重写AOF文件，达到压缩的目的。

AOF重写是AOF持久化的一个机制，用来压缩AOF文件，通过fork一个子进程，重新写一个新的AOF文件，该次重写不是读取旧的AOF文件进行复制，而是读取内存中的Redis数据库，重写一份AOF文件，有点类似于RDB的快照方式。

文件重写之所以能够压缩AOF文件，原因在于：

- 过期的数据不再写入文件
- 无效的命令不再写入文件：如有些数据被重复设值(set mykey v1, set mykey v2)、有些数据被删除了(sadd myset v1, del myset)等等
- 多条命令可以合并为一个：如sadd myset v1, sadd myset v2, sadd myset v3可以合并为sadd myset v1 v2 v3。不过为了防止单条命令过大造成客户端缓冲区溢出，对于list、set、hash、zset类型的key，并不一定只使用一条命令；而是以某个常量为界将命令拆分为多条。这个常量在redis.h/REDIS_AOF_REWRITE_ITEMS_PER_CMD中定义，不可更改，3.0版本中值是64。

#### 3.5.1 **文件重写时机**

相关参数：

- aof_current_size：表示当前 AOF 文件空间
- aof_base_size：表示上一次重写后 AOF 文件空间
- auto-aof-rewrite-min-size: 表示运行 AOF 重写时文件的最小体积，默认为64MB
- auto-aof-rewrite-percentage: 表示当前 AOF 重写时文件空间（aof_current_size）超过上一次重写后 AOF 文件空间（aof_base_size）的比值多少后会重写。

同时满足下面两个条件，则触发 AOF 重写机制：

- aof_current_size 大于 auto-aof-rewrite-min-size
- 当前 AOF 相比上一次 AOF 的增长率:(aof_current_size - aof_base_size)/aof_base_size 大于或等于 auto-aof-rewrite-percentage

#### 3.5.2 **文件重写流程**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171423556907666.png)
流程说明：

1. 执行AOF重写请求。

如果当前进程正在执行bgrewriteaof重写，请求不执行。

如果当前进程正在执行bgsave操作，重写命令延迟到bgsave完成之后再执行。

1. 父进程执行fork创建子进程，开销等同于bgsave过程。

3.1 主进程fork操作完成后，继续响应其它命令。所有修改命令依然写入AOF文件缓冲区并根据appendfsync策略同步到磁盘，保证原有AOF机制正确性。

3.2 由于fork操作运用写时复制技术，子进程只能共享fork操作时的内存数据由于父进程依然响应命令，Redis使用“AOF”重写缓冲区保存这部分新数据，防止新的AOF文件生成期间丢失这部分数据。

1. 子进程依据内存快照，按照命令合并规则写入到新的AOF文件。每次批量写入硬盘数据量由配置aof-rewrite-incremental-fsync控制，默认为32MB，防止单次刷盘数据过多造成硬盘阻塞。

5.1 新AOF文件写入完成后，子进程发送信号给父进程，父进程更新统计信息。

5.2 父进程把AOF重写缓冲区的数据写入到新的AOF文件。

5.3 使用新的AOF文件替换老的AOF文件，完成AOF重写。

**Redis 为什么考虑使用 AOF 而不是 WAL 呢?**

很多数据库都是采用的 Write Ahead Log（WAL）写前日志，其特点就是先把修改的数据记录到日志中，再进行写数据的提交，可以方便通过日志进行数据恢复。

但是 Redis 采用的却是 AOF（Append Only File）写后日志，特点就是先执行写命令，把数据写入内存中，再记录日志。

如果先让系统执行命令，只有命令能执行成功，才会被记录到日志中。因此，Redis 使用写后日志这种形式，可以避免出现记录错误命令的情况。

另外还有一个原因就是：AOF 是在命令执行后才记录日志，所以不会阻塞当前的写操作。

## **4.复制Replication**

主从复制过程大体可以分为3个阶段：连接建立阶段（即准备阶段）、数据同步阶段、命令传播阶段；下面分别进行介绍。

### 4.1 连接建立阶段

1. 保存主节点信息
2. 建立socket连接
3. 发送ping命令
4. 身份验证
5. 发送从节点端口信息

### 4.2 数据同步阶段

执行流程：
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171424139528314.png)

1. 全量复制（完整重同步）

   Redis通过psync命令进行全量复制的过程如下：

2. - 从节点判断无法进行部分复制，向主节点发送全量复制的请求；或从节点发送部分复制的请求，但主节点判断无法进行部分复制；具体判断过程需要在讲述了部分复制原理后再介绍；
   - 主节点收到全量复制的命令后，执行bgsave，在后台生成RDB文件，并使用一个缓冲区（称为复制缓冲区）记录从现在开始执行的所有写命令；
   - 主节点的bgsave执行完成后，将RDB文件发送给从节点；**从节点首先清除自己的旧数据，然后载入接收的**RDB文件，将数据库状态更新至主节点执行bgsave时的数据库状态；
   - 主节点将前述复制缓冲区中的所有写命令发送给从节点，从节点执行这些写命令，将数据库状态更新至主节点的最新状态；
   - 如果从节点开启了AOF，则会触发bgrewriteaof的执行，从而保证AOF文件更新至主节点的最新状态。

3. 部分复制（部分重同步）

- 复制偏移量

主节点和从节点分别维护一个复制偏移量（offset），代表的是**主节点向从节点传递的字节数**；主节点每次向从节点传播N个字节数据时，主节点的offset增加N；从节点每次收到主节点传来的N个字节数据时，从节点的offset增加N。

- 复制积压缓冲区

复制积压缓冲区是由主节点维护的、固定长度的、先进先出(FIFO)队列，默认大小1MB；当主节点开始有从节点时创建，其作用是备份主节点最近发送给从节点的数据。注意，无论主节点有一个还是多个从节点，都只需要一个复制积压缓冲区。

- 服务器运行ID(runid)

每个Redis节点(无论主从)，在启动时都会自动生成一个随机ID(每次启动都不一样)，由40个随机的十六进制字符组成；runid用来唯一识别一个Redis节点。

### 4.3 命令传播阶段

主->从：PING。

每隔指定的时间，**主节点会向从节点发送**PING命令，这个PING命令的作用，主要是为了让从节点进行超时判断。

从->主：REPLCONF ACK

在命令传播阶段，**从节点会向主节点发送**REPLCONF ACK命令，频率是每秒1次；命令格式为：REPLCONF ACK {offset}，其中offset指从节点保存的复制偏移量。

## **5.架构模式**

### 5.1 哨兵模式

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171424263286533.png)

#### 5.1.1 **哨兵模式工作原理**

1. 每个 Sentinel 以每秒一次的频率向它所知的 Master，Slave 以及其他 Sentinel 节点发送一个 `PING` 命令；
2. 如果一个实例（instance）距离最后一次有效回复 PING 命令的时间超过配置文件 `own-after-milliseconds` 选项所指定的值，则这个实例会被 Sentinel 标记为**主观下线**；
3. 如果一个 Master 被标记为主观下线，那么正在监视这个 Master 的所有 Sentinel 要以每秒一次的频率确认 Master 是否真的进入主观下线状态；
4. 当有**足够数量的 Sentinel**（大于等于配置文件指定的值）在**指定的时间范围内确认** Master 的确进入了主观下线状态，则 Master 会被标记为**客观下线**；
5. 如果 Master 处于 **ODOWN 状态**，则投票自动选出新的主节点。将剩余的从节点指向新的主节点继续进行数据复制；
6. 在正常情况下，每个 Sentinel 会以每 10 秒一次的频率向它已知的所有 Master，Slave 发送 `INFO` 命令；当 Master 被 Sentinel 标记为客观下线时，Sentinel 向已下线的 Master 的所有 Slave 发送 INFO 命令的频率会从 10 秒一次改为每秒一次；
7. 若没有足够数量的 Sentinel 同意 Master 已经下线，Master 的客观下线状态就会被移除。若 Master 重新向 Sentinel 的 PING 命令返回有效回复，Master 的主观下线状态就会被移除。

**主要缺陷**：单个节点的写能力，存储能力受到单机的限制，动态扩容困难复杂。

### 5.2 集群模式

为了解决哨兵模式存储受单机的限制，这里引入分片概念。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171424399389547.png)

#### 5.2.1 **分片**

Redis Cluster 采用**虚拟哈希槽分区**，所有的键根据哈希函数映射到 0 ~ 16383 整数槽内，计算公式：`HASH_SLOT = CRC16(key) % 16384`。每一个节点负责维护一部分槽以及槽所映射的键值数据。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171424508816218.png)

Redis Cluster 提供了灵活的节点扩容和缩容方案。在不影响集群对外服务的情况下，可以为集群添加节点进行扩容也可以下线部分节点进行缩容。可以说，槽是 Redis Cluster 管理数据的基本单位，集群伸缩就是槽和数据在节点之间的移动。

**参考文献**

- https://redis.io/topics/replication
- https://redis.io/topics/persistence
- https://www.cnblogs.com/mrhelloworld/p/redis-architecture.html
- [https://mp.weixin.qq.com/s/V2MKyMKtCO1skgWWXBnrgg](https://mp.weixin.qq.com/s?__biz=MzkwOTIxNDQ3OA==&mid=2247533213&idx=1&sn=96abbf1f568b88c2408cbfbd50b41f95&scene=21#wechat_redirect)
- https://www.cnblogs.com/biyeymyhjob/archive/2012/07/20/2601655.html
- https://www.cnblogs.com/kismetv/p/8654978.html
- https://zhuanlan.zhihu.com/p/187596888
- https://cloud.tencent.com/developer/article/1633077
- [**https://www.cnblogs.com/yangmingxianshen/p/8373205.html**](https://www.cnblogs.com/yangmingxianshen/p/8373205.html)

原文作者：lunnzhang，腾讯 CDG 后台开发工程师。

原文链接：https://mp.weixin.qq.com/s/k8agEub4qmhm3kX_TpETrA

# 【NO.106】一文深入理解 Kubernetes

> 这篇文章，你要翻很久，建议收藏。

Kubernetes，简称 K8s，是用 8 代替 8 个字符“ubernete”而成的缩写。是一个开源的，用于管理云平台中多个主机上的容器化的应用。k8s 作为学习云原生的入门技术，熟练运用 k8s 就相当于打开了云原生的大门。本文通过笔者阅读书籍整理完成，希望能帮助想学习云原生、以及正在学习云原生的童鞋快速掌握核心要点。学习 k8s 和大家学习 linux 差不多，看似复杂，但掌握了日常熟悉的指令和运行机理就能愉快的使用了，本文的重点和难点是服务、kubernetes 机理部分。

## 1.**要点**

Kubemetes 采用的是指令式模型， 你不必判断出部署的资源的当前状态， 然后向它们发送命令来将资源状态切换到你期望的那样。你需要做的就是**告诉 Kuberetes 你希望的状态**， 然后 Kubemetes 会采取相关的必要措施来将集群的状态 切换到你期望的样子。

1：资源：定义了一份资源，意味着将创建一个对象(如 Pod) 或 新加了某种规则(类似于打补丁，NetworkPolicy)；

2：每个种类的 资源都对应一个 **控制器**，负责资源的管理；

3：pod 可以看成远行单个应用的 虚拟机，但可能被频繁地自动迁移而无需人工介入；

集群管理和部署的最小单位。

- 无状态服务：新的 IP 名和主机地址
- 有状态服务: StatefulSet, 一致的主机名 和 持久化状态

pod 中应用写入磁盘的数据**随时** 会丢失 【包括运行时， 容器重启，会在新的写入层写入】

> 记住，pod 是随时可能会被重启

4：容器重启原因：

- 进程崩溃了
- 存活探针返回失败
- 节点内存耗尽，进程被 OOM

> 需要 Pod 级别的存储卷， 和 Pod 同生命周期， 防止容器重启丢失数据， 例如挂载 emptyDir 卷，看容器启动日志。
>
> 容器重启以指数时间避退，直到满 5 分钟。特别注意，容器重启并不影响 Pod 数量变化【理论上所有 Pod 都是一样，即使换新的 Pod，容器还是会重启】。

5：控制 Pod 的启动顺序；

- Init 容器：Pod 中可以有任意多个 initContainers，初始化容器用来控制 Pod 与 Pod 之间 启动 的先后顺序；

- 就绪探针: 一个应用依赖于另一个应用，若依赖无法工作，则 阻止 当前应用成为服务 端点群内的 pod 访问。

- - Deployment 滚动升级中会应用 就绪探针， 避免错误版本的出现。

6：localhost 一般指代节点，而非 pod；

7：当 Pod 关闭的时候，可能工作节点上 kube-proxy 还没来得及修改 Iptables，还是会将通信转移到 关闭的 Pod 上去；

> 推荐是 Pod 等一段时间再关闭【等多长时间是个问题， 和 TCP 断开连接等 2MSL 再关闭差不多】，直到 kube-proxy 已修改 路由表。

8：服务目录可以在 kubernetes 中轻松配置和暴露服务；

9：Kubernetes 可以通过单个 JSON 或 YAML 清单部署 一 组资源；

10：Endpoint， 有站点的意思（URL）， REST endpoint, 就是一个 http 请求而已。

Endpoint 资源指 Service 下覆盖的 pod 的 ip:端口列表。

## 2.**整理**

### 2.1 字段

1：在定义 manifest 时， 常用的一些字段罗列如下：

> 备注：常用字段，非全部。
>
> [在线文档](https://share.mubu.com/doc/2n0iYCbQNal)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgw1HHEbGdI129xicYJkOAsIKSaxIwChZs0OVm9qvwqtcibFZo49oZHhOQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)k8s-字段

### 2.2 标注和必知

1：常见的注解整列

> [在线文档](https://share.mubu.com/doc/w-VRMG2Eal)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgzm3oTpyAKEibUKb8MhnHru37WtiatY1qNVAghyNGA9x6h2ZYX18fhghw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)k8s-标注和必知

### 2.3 命名空间和资源

1: k8s 中整理的命名空间和常用资源如下：

> [在线文档](https://share.mubu.com/doc/b_CYIA-IGl)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgXWgvdFTWQxzSWNGL4P3ic2LmEBZyccraE8dRmZDo7YDDvGEibfiacwF6A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)k8s-命名空间和资源

### 2.4 常用指令

1：梳理常用指令

> [在线文档](https://share.mubu.com/doc/180hhorFtWl)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhggCsje0o8POdiccHo39IoZC8FVlRAXMKysVMymKtsGHdXlWDECBFyycA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)kubernetes 指令汇总

## 3.**环境**

### 3.1 集群安装

1：单节点集群，minikube

2: 多节点集群，虚拟机 + kubeadm

## 4.**k8s 介绍**

### 4.1 微服务

1：微服务：大量的单体应用 被拆成独立的、小的 组件

2：配置、管理 需要自动化；

3：监控应用，变成了监控 kubernets

> 传统的应用 由 kubernets 自己去监控

4：拆成微服务的好处：

- 1: 改动单个服务的 API 成本更小；
- 2：服务之间可通过 HTTP（同步协议）、AMQP（异步协议）通信；
- 3：新服务可用不同语言开发；

### 4.2 虚拟机和容器

1：虚拟机多出来的三个部分：

- 虚拟化 CPU；
- 用户操作系统；
- 管理程序：**透传**虚拟机上应用的 操作指令 到 宿主机上的 物理 CPU 来执行；

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgJhIib4cRaWHRBFbXKg8B3iaI95foR4L5CXw4ORDmegHFPSibpGcxvQs3A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

2：容器的隔离机制

- Linux 命名空间， 每个进程只能看到自己的系统视图（文件、进程、网络接口、主机名等）

- - Mount: 挂载卷，存储；
  - PID：proceess ID , 进程树；
  - Network：网络接口；
  - Inter-process communication: IPC, 进程间通信；
  - UTS：本地主机名；
  - User ID: 用户。

- Linux 控制组 (cgroups), 限制进程能使用的资源量（CPU、内存、网络带宽等）

3：容器限制了 只能使用 母机的 Linux 内核；

- X86 上编译的应用 容器化后，不能运行在 ARM 架构的母机上；

### 4.3 Docker

1：Docker：打包、分发、运行应用程序的 ==平台==。

> 运行容器镜像的 软件，类似于 VMware
>
> 简化了 Linux 命名空间隔离 和 cgroups 之内的 系统管理；

- 镜像：经过 Docker 打包的 环境（包含应用程序的依赖，配置文件，运行 app）
- 镜像仓库：云端存储；
- 容器：基于 Docker 创建的运行时环境，是一个 运行在 Docker 主机上的进程， 和其他进程隔离且能使用的资源受限；

2：类似的容器运行时，还有 rock-it;

### 4.4 k8s 组成

1：一个 k8s 分成两类：

- master node (主节点)：主节点上的组件可以组成一个集群，负责集群的控制和调度
- work node （工作节点）：工作节点一般是多个，实际部署应用的 节点

2：组件

- 1：调度器：选择资源足够的 node 分配 pod

  节点不够，Cluster Autoscaller 横向扩容节点控制 pod 选在哪个节点上【亲缘性 affinity】pod 要和 哪个 pod 在一块。

- 2：控制器：跟踪节点状态，复制 pod，持续跟踪 pod， 处理节点失败， 大部分资源都会对应一个 控制器

例如 Endpoint Controller 通知工作节点上的 kube-proxy 修改 iptables 。

本质是一个 死循环，监听 API 服务器的状态；

- 3：etcd 分布式数据存储，API 服务器将集群配置存入。比如每次提交一个 yaml 文件，校验通过后会存入；
- 4：kubelete: 接收 API 服务器通知 和 Docker 交互，控制容器的启动 上报 node 的资源总量
- 5：Docker 容器运行时， 拉取镜像、启动容器
- 6：pod:

1：独立的 IP 和 端口空间 ；

pod.hostNetwork: true, 使用宿主节点的网络接口和端口；containers.ports.hostPort: 仅把容器端口绑定在节点上。

2：独立的进程树，自己的 PID 命名空间

- pod.hostPID: true, 使用宿主节点的 进程空间
- pod.hostIPC: true, 使用宿主节点的 IPC 命名空间

3：自己的 IPC 命名空间，同 pod 可以通过进程间通信 IPC

4：同 pod 两个容器，可以共享存储卷

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgw9lxZRV8l38ap1siblp2jIRlDbynegBUibvWTVqVz6HEfwMLlVwokyLQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)k8s总体架构图-第 2 页

3: k8s 功能

1：自动处理失败 容器：重启， 可自动选择 新的 工作节点。【自修复】

2：自动调整 副本数：根据 CPU 负载、内存消耗、每秒查询 应用程序的其它指标 等；【==自动扩缩容==】

3：容器 移除或移动， 对外暴露的 静态 IP 不变（环境变量 or client 通过 DNS 查询 IP） 【==可实现复杂的 集群 leader 选举==】

4：可以限制某些容器镜像 运行 在指定硬件的机器**群** 【在 HDDS 系列的机器上选择一个】上：

```
- HDDS；
- SSD；
```

## 5.**Docker 和 k8s**

### 5.1使用 Docker

#### 5.1.1 **安装 Docker**

1：安装：略

2：`docker run <image>` ：现在本地查找镜像， 若无， 前往 [http://docker.io](http://docker.io/) 中 pull 镜像。

> 其它可用镜像：http://hub.docker.com

```
# 运行一个镜像，并传入参数
docker run busybox echo "Hello World"
Unable to find image 'busybox:latest' locally
latest: Pulling from library/busybox
b71f96345d44: Pull complete
Digest: sha256:930490f97e5b921535c153e0e7110d251134cc4b72bbb8133c6a5065cc68580d
Status: Downloaded newer image for busybox:latest
Hello World
```

3：==docker 查看镜像本地是否已存在 --下载镜像-- 创建容器 -- 运行 echo 命令--- 进程终止 --- 容器停止运行；==

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg1zwswdZRRskV7snKHcbce7973K1IwIEp1mibLhiae3tMibZ6ySzmCIXKw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 5.1.2 **创建应用**

1：创建一个简单的 `node.js` 应用， 输出主机名:

```
const http = require('http');
const os = require('os');

console.log("Kubia server starting...");

var handler = function(request, response) {
  console.log("Received request from " + request.connection.remoteAddress);
  response.writeHead(200);
  response.end("You've hit " + os.hostname() + "\n");
};

var www = http.createServer(handler);
www.listen(8080);
```

2：构建镜像，需要一个 Dockerfile 文件

> 和 app.js 同一目录

```
# From 定义了基础镜像，可以是 Ubuntu 等系统，但应尽量遵从精简
FROM node:7
# 本地文件添加到 镜像的根目录
ADD app.js /app.js
# 镜像被运行时 需要执行的命令
ENTRYPOINT ["node", "app.js"]
```

#### **5.1.3 构建镜像**

1：Dockerfile + app.js 就能创建镜像包：

```
# 基于当前目录 创建 kubia 镜像
$ docker build -t kubia .
Sending build context to Docker daemon  3.072kB
Step 1/3 : FROM node:7
7: Pulling from library/node
ad74af05f5a2: Pull complete
2b032b8bbe8b: Pull complete
a9a5b35f6ead: Pull complete
3245b5a1c52c: Pull complete
afa075743392: Pull complete
9fb9f21641cd: Pull complete
3f40ad2666bc: Pull complete
49c0ed396b49: Pull complete
Digest: sha256:af5c2c6ac8bc3fa372ac031ef60c45a285eeba7bce9ee9ed66dad3a01e29ab8d
Status: Downloaded newer image for node:7
 ---> d9aed20b68a4
Step 2/3 : ADD app.js /app.js
 ---> 43461e2e8cef
Step 3/3 : ENTRYPOINT ["node", "app.js"]
 ---> Running in 56bc0e5982ce
Removing intermediate container 56bc0e5982ce
 ---> 0d2c12c8cc80
Successfully built 0d2c12c8cc80
Successfully tagged kubia:latest
```

2: 镜像构建过程

- Docker 客户端：在宿主机上的；
- Docker 守护进程：运行在一个虚拟机内；【可以在远端机器】

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgmRmlDQE20ClMadZwh1Kb8DkWvDfk3pjrtDU7ibashJ2W4feW6ibAmVlg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 5.1.4 **镜像分层**

1：Dockerfile 中的的**每一行** 都会 形成一个 镜像层

最后一层 是 `kubia:latest` 镜像

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgQtgDTvURGiamdlyWojsLj57oic65eGe9ODemep4jwo0NGkSE0LdfUNyA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

2：镜像分层的好处：节省下载，

3：查看本地新镜像 `docker image`

#### 5.1.5 **运行镜像**

1：端口映射后，可用 [http://localhost:8080](http://localhost:8080/) 访问；

```
                 # 容器名称    # 本机8080 映射到容器的 8080 端口      # 镜像文件
docker run --name kubia-container -p 8080:8080                  -d kubia
4099df6236c5d4905a268b213ab986949f6522122454de41f56293ce3508e958 # 容器 ID


# test，主机名就是 分配的 容器 ID
$ curl localhost:8080
You've hit 4099df6236c5
```

2: 查看运行中的容器：

```
$ docker ps
CONTAINER ID   IMAGE                                 COMMAND                  CREATED          STATUS          PORTS                                                                                                                                  NAMES
4099df6236c5   kubia                                 "node app.js"            40 seconds ago   Up 38 seconds   0.0.0.0:8080->8080/tcp, :::8080->8080/tcp                                                                                              kubia-container
2744c31527b9   gcr.io/k8s-minikube/kicbase:v0.0.22   "/usr/local/bin/entr…"   3 days ago       Up 3 days       127.0.0.1:49172->22/tcp, 127.0.0.1:49171->2376/tcp, 127.0.0.1:49170->5000/tcp, 127.0.0.1:49169->8443/tcp, 127.0.0.1:49168->32443/tcp   minikube

# 查看容器 详细信息
docker inspect kubia-container
```

#### 5.1.6 **查看容器内部**

1：一个容器可以运行多个进程；

2：Node.js 镜像中 包含了 bash shell , 可在容器 内运行 shell :

```
# kubia-container 容器内 运行 bash 进程
docker exec -it kubia-container bash

# 退出
exit
```

- `-i`，确保标准输入流保待开放。需要在 shell 中输入命令。
- `-t`, 分配一个伪终端(TTY)。

3: 查看容器内的进程；

```
$ docker exec -it kubia-container bash
root@4099df6236c5:/# ps axuf
USER         PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root          12  0.6  0.0  20244  2984 pts/0    Ss   21:44   0:00 bash
root          18  0.0  0.0  17496  2020 pts/0    R+   21:44   0:00  \_ ps axuf
root           1  0.0  0.6 614432 26320 ?        Ssl  21:41   0:00 node app.js
```

> 只能在 docker 的守护进程内查看

4：在母机上查看进程

```
user00@ubuntu:~$ ps axuf | grep  app.js
app.js
root     3224898  0.0  0.6 614432 26320 ?        Ssl  14:41   0:00  \_ node app.js
```

> 容器和主机上的进程 ID 是独立、不同的；

5：容器 是独立的：

- PID Linux 命名空间
- 文件系统 都是独立的
- 进程
- 用户
- 主机名
- 网络接口

#### 5.1.7 **停止和删除容器**

1：停止容器，会停止容器内的主进程。

容器本身依然存在，通过 `docker ps -a`

```
docker stop kubia-container

# 打印所有容器, 包括 运行中的和已停止的
$ docker ps -a
CONTAINER ID  IMAGE  COMMAND         CREATED                STATUS    PORTS      NAMES
4099df6236c5  kubia  "node app.js"   6 minutes ago    Exited (137) 4 seconds ago

# 真正的 删除容器
docker rm kubia-container
```

#### 5.1.8 **向仓库推送镜像**

1：仓库 `http://hub.docker.com` 镜像中心；

2：按照仓库要求，创建额外的 tag

```
     # 个人 ID
docker tag kubia luksa/kubia

# 查看镜像
docker images | head

# 自己的 ID 登录
docker login

# 推送镜像
docker push luksa/kubia
```

3: 可在任意机器上运行 云端 docker 镜像：

```
docker run -p 8080:8080 -d luksa/kubia
```

### 5.2 使用 kubernetes 集群

#### 5.2.1 **安装集群的方式**

1：安装集群通常有以下四种方式：

- 本地 单点；
- Google Kubernetes Engine(GKE) 上托管的集群；
- kubeadm 工具安装；
- 亚马逊的 AWS 上安装 kubernetes

#### 5.2.2 **Minikue 启动 k8s 集群**

1：安装略

2：启动 Minikube 虚拟机

```
minikube start
```

3: 安装 k8s 客户端(kubectl)

#### **5.2.3 GKE 创建三节点集群**

1：创建 3 个工作节点的示例：

```
gcloud container clusters create kubia --num-node 3 --machine-type f1-micro
```

2：本地发起请求 到 k8s 的 master 节点；master 节点负责 调度 pod, 以下创建了 3 个工作节点；

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgZbaXIAD4Lt0OvM9LSib8dibwL9zjdWicWaXiawsibKGOSXjBMw7TwLg6q2w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 5.2.4 **部署 Node.js 应用**

```
kubectl run kubia --image=luksa/kubia --port=8080 --generator=run/v1
```

#### **5.2.5 pod**

1: 多个容器运行在一起：

一个 pod 可以包含任意数量的容器；

2：pod 拥有单独的 **私有** IP、主机名、单独的 Linux 命名空间；

==**Pod 是扩缩容的基本单位；**==

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgXW4m6ibmF7xR6aNCRyg01BUKuPPuKVEF2CxpRibibwUMyjgsQblib5nIgg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3：k8s 中 运行 容器镜像需要经历两个步骤：

- 1：推送 docker 镜像 到云端 【不同工作节点上的 Docker 能访问到 该镜像】；

- 2：运行 kubectl ，==创建一个 ReplicationController 对象 【运行指定数量的 pod 副本】；==

- - 调度器 创建 pod，并 选择一个 工作节点，分配给 pod;

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgNgwao5YlAicMDLiaVAAicZn57gOEFoa8mL1hMXBJsBJJZH80HfzFib2AzQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 5.2.6 **创建外部访问的服务**

1：常规服务：ClusterIP 服务， 比如 pod， 只能从 集群内部访问；

2：创建 LoadBalancer 类型的服务，负载均衡，对外提供 公共 IP 访问 pod;

> 内部删减 pod， 移动 pod，外部 IP 不变， 一个外部 IP 可对应 多个 pod。

```
kubectl expose rc kubia --type=LoadBalancer --name kubia-http

kubectl get svc # 查看服务是否分配 EXTERNAL-IP（外部IP）
```

> `minikube service kubia-http` 可查看 IP：端口

#### 5.2.7 **系统逻辑部分**

1：服务、pod、对象：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgMibbecicYx7TOgeSAxCArjiawSb0ibzGnxncWMiaDicHNXYM3TWxW4Wibc6nw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 5.3 水平伸缩 pod 节点

```
kubectl get replicationcontrollers  # 获取 ReplicationControllers 状态
```

告诉期望数量即可：

```
kubectl scale rc kubia --replicas=3
```

多次访问，服务作为负载均衡，会随机选择一个 pod：

> 和上面对比，这里有三个 pod 实例。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgJrZYFI0TW5j8S9Mz0ePtREF9psAG1B79BDoibJTBQDUbITrsJmicmgog/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 5.4 pod 运行在哪个节点上

1: pod 的 IP 和运行的工作节点

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgdmHXaZ5FeJ10aYgfmzGeKnAb6ZeuGI4smNTCfGlfEFsrp9qiaJsJBBw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617220307330

```
$ kubectl  describe  pod kubia-jppwd
Name:         kubia-jppwd
Namespace:    default
Priority:     0
Node:         minikube/192.168.49.2   # 具体的节点
```

### 5.5 Dashboard

1：若是 GKE 集群， 可通过 `kubectl cluster-info | grep dashboard` 获取 dashboard 的 URL。

2：终端输入 `minikube dashboard` 会自动打开浏览器：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgDKKibNhZIWXb770lUxQCpic0xNST25XAACPliavyN4ajShc2IhNzjNDLw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## 6.**pod**

### 6.1 Pod

1: 容器被设计为每个容器只运行一个进程， 保证轻量和一定的隔离性；

2：有些容器又有一些紧密的联系，例如常说的 side-car 容器，负责网络代理，日志采集的容器， 这些容器最好放一起。这就出现了 上层的 pod 。

> pod 是 k8s 中引入的概念，docker 是可以直接运行容器的。

3：k8s 通过配置 Docker 让一个 pod 内的所有容器 共享 相同的 Linux 命名空间 【有些容器放到一个 pod 的好处】：

- 相同的 network 和 UTS 命名空间；

- 共享相同的主机名和网络接口；pod 中的端口，不能绑定多次；

- - 两个 pod 之间可以实现 两个 IP 相互访问

> 不管两个 pod 是否在同一节点， 可以想 无 NAT 的平坦网络之间通信（类似局域网 LAN）

- 相同的 IPC 命名空间下运行；能通过 IPC 进行通信；
- 共享相同的 PID 命名空间

> 注意：文件系统来自容器镜像，默认容器的文件系统彼此隔离。

4：pod 是逻辑主机， 其行为与非容器世界中的物理主机或虚拟机非常相似。此外， 运行在同一个 pod 中的进程与运行在同一物理机或虚拟机上的进程相似， 只是每个进程都封装在一个容器之中。

5: pod 可以当做独立的机器，非常轻量， 可同时有大量的 pod；

6: pod 是扩缩容的基本单位；

7: pod 的定义包含三个部分：

- metadata 包括名称、命名空间、标签和关于该容器的其他信息 。
- spec 包含 pod 内容的实际说明 ， 例如 pod 的容器、卷和其他数据 。
- status 包含运行中的 pod 的当前信息，例如 pod 所处的条件 、 每个容器的描述和状态，以及内部 IP 和其他基本信息 。

一个简单 pod 包含的三部分：

```
apiVersion: v1  # 分组和版本
kind: Pod       # 资源类型
metadata:
  name: kubia-manual  # Pod 名称
spec:
  containers:
  - image: luksa/kubia  # 容器使用的镜像
    name: kubia
    ports:
    - containerPort: 8080 # 应用监听的端口
      protocol: TCP
```

8：可使用 `kubectl explain` 查看指定资源信息

```
kubectl explain pods
```

9：创建资源：

```
# 所有定义的 资源 manifest 都通过该指令来创建，非常重要
kubectl create -f kubia-manual.yaml
```

10：查看日志：

```
kubectl logs <pod-name>
```

11：若不想通过 Service 与 pod 通信，可通过端口转发：

```
# 将 8888 端口 转发到 该 pod的 8080 端口
kubectl port-forward <pod-name> 8888:8080

curl localhost:8888
```

> 端口转发是 `kubectl` 内部实现的。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgCarl2G8nzWo2qnjRNKfjJetwu4hyrR8x0Eu88lvhxLibib1ibibFiaffH4A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 6.2 标签

1：可使用 标签组织管理 pod

> 标签也能组织其他 k8s 资源

2：例如定义两组标签，可不同维度管理 pod

- `label_key = app`: 按应用分类
- `label_key = rel`: 按版本分类

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgrUuPdw24ffKGG104m4Bf6KBzj7ub2N25OvLvnvvpk23ZCtcK25dLMg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3：pod 中使用 labels 定义标签：

```
kind: Pod
metadata:
  name: kubia-manual-v2
  labels:
    # 定义的两组标签
    creation_method: manual
    env: prod
```

4：可通过 `-l` 指定标签选择列出 pod 子集：

```
$  kubectl get po -l  creation_method=manual
NAME              READY   STATUS    RESTARTS   AGE
kubia-manual-v2   1/1     Running   0          31s
```

> 使用多个标签， 中间用 逗号分隔即可。

#### 6.2.1 **pod 调度到特定节点**

1：默认下，pod 调度到哪个 节点是不确定的，这由调度器决定。

2：有些情况，需要将 pod 调度到特定的节点上（比如偏计算的 pod 需要调度到 gpu 集群上）

3：给节点打标签

```
kubectl label node <node-name> <label-key>=<label_value>
```

4：通过 `nodeSelector` 调度到含有 `gpu=true` 标签的节点上。

```
apiVersion: v1
kind: Pod
metadata:
  name: kubia-gpu
spec:
  # 调度到含有 gpu=true 的节点上
  nodeSelector:
    gpu: "true"
  containers:
  - image: luksa/kubia
    name: kubia
```

> 注意：含有该标签的节点可能有多个，届时将选择其中一个。
>
> 候选节点最好是一个集合，避免单个节点故障会造成服务不可用。

### 6.3 注解

1：注解（Annotation）和 标签类似，也是键值对。

> 有些注解是 k8s 自动添加的。
>
> 注解不能超过 256K

### 6.4 命名空间

1：命名空间（namespace, 简称 ns）可对对象分组。

> 资源名称只需要在命名空间内保证唯一即可， 跨命名空间可以重。

2：列出某个命名空间下的 pod

```
kubectl get po --namespace kube-system
```

3：命名空间，可控制用户在该命名空间的访问权限， 限制单个用户可用的 资源数量；

4：创建命名空间：

```
apiVersion: v1
kind: Namespace
metadata:
  name: custom-namespace
```

5：尽管命名空间对 对象进行了分组， 但 **==并不提供实质上的隔离==** ，例如不同命名空间的 pod 能否通信取决于网络策略。

6：删除 pod 有多种方式：

```
kubectl delete po <pod-name>  # 按名称删除
kubectl delete po --all  # 删除当前命名空间的所有 pod, 若 ReplicationController 未删除，将重新创建 pods
kubectl delete po -l <label-key>=<label-val>  # 按标签删除

kubectl delete ns <ns-anme>  # 删除整个命名空间

kubectl delete all --all     # 删除当前命名空间内的所有资源，包括托管的 ReplicationController， Service，  但 Secret 会保留
```

## 7.**托管集群**

### 7.1 保持进程健康

1：进程异常的几种情形：

- 主进程 崩溃->kubelet 将 重启 容器；
- 内存泄漏， JVM 会一直运行，但会抛出 OutofMemoryErrors, 让程序 向 k8s 发出信号 触发 重启；
- 外部检查：应用死循环 or 死锁

#### 7.1.1 **存活探针**

1：定期检查**容器**

2：三种探测机制：

- HTTP Get 向容器发送请求；
- TCP 套接字，与容器建立 TCP 连接；
- Exec 探针，在容器内执行任意指令，查看退出状态码；

3：HTTP 探针，定期发送 http Get 请求；

> /heath HTTP 站点不需要认证，否则会一直认为失败，容器 无限重启；

```
apiVersion: v1
kind: Pod
metadata:
  name: kubia-liveness
spec:
  containers:
  # 镜像内有坏掉的应用
  - image: luksa/kubia-unhealthy
    name: kubia
    # 存活探针
    livenessProbe:
      httpGet:
        path: /
        port: 8080
```

4：返回的状态码 137 和 143：

```
$ kubectl describe  pod kubia-liveness
 State:          Running
      Started:      Thu, 17 Jun 2021 11:04:53 -0700
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137   # 有错误码返回

 Warning  Unhealthy  10s    kubelet            Liveness probe failed: HTTP probe failed with statuscode: 500
```

5：探针的附加信息：

查看状态时，可看到 存活探针信息：

```
 Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3
```

- `delay=0s`: 容器启动后立即检测；
- `timeout=1s`：限制容器在 1s 内响应，否则失败；
- `period=10s`：每隔 10s 探测一次；
- `failure=3`：连续三次失败后，重启容器；

6：具有初始延迟的 存活探针：【程序还未 启动稳定】

```
# 第一次探针前，等15s，防止容器没准备好
      initialDelaySeconds: 15
```

7：若无探针， k8s 认为进程还在运行，容器就是健康的。

8：探针的注意事项：

- 1：探针应该轻量，不能占用太多 cpu 【应 计入 容器的 CPU 配额】， 一般 1s 内执行完；
- 2：java 程序应该用 http get 探针，而非启动全新 JVM 获取存活信息的 Exec 探针（太耗时）
- 3：无需设置 探针的失败重试次数， k8s 为了确认一次探测的失败，==默认就会尝试若干次==；

9：重启容器由 kubelet 执行；主服务器上的 k8s Control Plane 组件不会参与；

- ==若整个节点崩溃， 则无法重启== 【kubelet 依赖于节点】；
- 若要保证节点挂了，pod 能重启，应该使用 RC 或 RS；

### 7.2 ReplicationController

1: 用于管理 pod 的多个副本；

2：会自动调整 pod 数量为 指定数量：

多余副本在以下几种情况下会出现：

- 有人会手动创建相同类型的 pod。
- 有人更改现有的 pod 的 ” 类型” 。
- 有人减少了所需的 pod 的数量， 等等。

3：ReplicationController 的功能：

- 确保一 个 pod (或多个 pod 副本）持续运行， 方法是在现有 pod 丢失时启动一 个新 pod。
- 集群节点发生故障时， 它将为故障节 点 上运 行的所有 pod (即受 ReplicationController 控制的节点上的那些 pod) 创建替代副本。
- 它能轻松实现 pod 的水平伸缩：手动和自动都可以

4: 根据 pod 是否匹配 标签选择器 来调整：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgKNEibTqalVcOmVIYejxAfTZwULkIzAh0Dic5NfFiaibQLWUuFOwTibk4MrA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

5: 更改标签选择器 和 pod 模板，对当前的 pod 没有影响；

- 也不关心 容器镜像、 环境变量和 其它；
- 只影响 创建新的 pod （新的 曲奇 切模 cookie cutter ）

修改 pod 模板：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhge9XnoW0kXBCMrkicauxiclysEUZ3dTATaUr5QkuiboScgr65iauGEKaFuA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

更改副本个数，就能实现动态扩缩容：

```
kubectl scale rc kubia  --replicas=3 # 调整副本数为3
```

6: 创建对象：

上传到 API 服务器， 会创建 kubia 的 ReplicationController 【简称 RC】。

- 模板中的 pod 标签 必须 与 RC 一致，否则会无休止创建容器（达不到期望数量的 pod）
- API 服务会校验 RC 的定义，不会接受错误配置；
- 可以不指定 RC 的选择器，会自动根据 pod 模板中的标签自动设置；

```
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3  # pod 副本数量

  # 标签选择器，选择 标签 app = kubia的pod进行管理
  selector:
    app: kubia

  # pod 模板
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080
```

7：删除 pod 标签（或者 移入另一个 RC 的掌控）， 脱离 RC 掌控，RC 会自动起一个 新的 pod;

- 原 pod 可用于调试，完成后，手动删除 该 pod 即可；
- 实际 pod 数量， RC 通过就绪探针；
- 删除 pod， 允许 客户端监听到 API 服务器的通知，通过检查实际的 pod 数量 采取适当的措施；

```
kubectl delete pod <pod-name>
```

添加 pod 另外的标签， RC 并不 care；

8：通过 pod 的 `metadata.ownerReferences` 可以知道 该 pod 属于哪个 RC；

9：节点故障：例如网络断开；

- RC 一段时间后检测到 pod 关闭（旧节点变为 `unknown`）， 会启动新的 pod 代替原来的 pod；
- 当旧节点恢复时， 节点状态变为 `ready`, `unknown` pod 会被删除；

10：更改 RC 的 标签选择器：

- 原有 pod 都会脱离管控；
- RC 会创建（若无）新的指定数量、指定标签 的 pod

> **==RC 的标签选择器可以修改，但其他的 控制器对象 不能。==**

11：删除 RC，默认会删除 RC 管理的 pod

> 可以使用 选项 `--cascade=false` 保留 pod.

```
kubectl delete rc <rc-name> --cascade=false
```

### 7.3 使用 ReplicaSet 替换 ReplicationController

1：ReplicationController 最初是 用于赋值和异常时重新调度节点 的**唯一** 组件。

2：一般不会直接创建 ReplicaSet ， 而是 创建 更高层级的 Deployment 资源时（第 9 章） 自动创建他们。

3：ReplicaSet 功能和 ReplicationController 一样， pod 选择器的 表达能力更强：

- ReplicationController 只允许 `k和v` **同时** 匹配的标签；
- ReplicationController 只能匹配单个 kv;
- ReplicaSet 基于 标签名 k 匹配；

4：若已经有了 3 个 pod，不会创建任何新的 pod，会将 旧 pod 纳入自己的管辖范围；

> 基础使用 和 RC 一样简单。

```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
```

5：`matchExpressions` 更强大的选择器；

```
selector:
    # 标签 需要包含的 key 和 val
    matchExpressions:
      - key: app
        operator: In
        values:
         - kubia
```

6：四个有效的运算符 `operator`：

- `In` : Label 的值 必须与其中 一个指定的 values 匹配。
- `Notln` : Label 的值与任何指定的 values 不匹配。
- `Exists` : pod 必须包含一个指定名称的标签（值不重要）。使用此运算符时，不应指定 values 字段。
- `DoesNotExist` : pod 不得包含有指定名称的标签。values 属性不得指定 。

7: `matchLabels` 和 `matchExpressions` 可以同时指定，条件是**与**的关系。

8：删除 ReplicaSet 也会删除 pod:

```
kubectl delete rs <rs-name>
```

### 7.4 DaemonSet: 在每个节点上运行一个 pod

1：需在每个节点上运行日志收集 or 监控：如 k8s 的`kube-proxy` 进程

2：通过 系统初始化脚本 or systemd 守护进程启动；

3：无期望副本数概念，在 节点选择器下， 运行一个 pod；

- 和节点绑定在一起：节点下线，并不会再创建 新 pod；
- ==新节点加入 【添加 节点 label 后】， 匹配节点选择器， 自动创建一个 新的 pod ；==
- 无意中删除了 该 pod， 会自动创建一个 pod；

4：从 DaemonSet 的 pod 模板 创建 pod

5：通过 节点选择器 `nodeSelector` 选中 部分节点创建 pod;

```
apiVersion: apps/v1beta2
kind: DaemonSet
metadata:
  name: ssd-monitor
spec:
  selector:
    matchLabels:
      app: ssd-monitor
  template:
    metadata:
      labels:
        app: ssd-monitor
    spec:
      # 节点选择器，选择 标签为 disk=ssd 的节点
      nodeSelector:
        disk: ssd
      containers:
      - name: main
        image: luksa/ssd-monitor
```

6: 节点可以设置为 不可调度 【通过调度器控制】， 防止 pod 部署到 该节点；

但 DaemonSet 管理的 pod 作为==系统服务，完全绕过调度器，== 即使 节点是 不可调度的，仍然可以运行系统服务；

7：从节点删除 节点标签， DaemonSet 管理的 Pod 也会被删除：

```
kubectl label node <node-name> disk=hdd --overwrite
```

### 7.5 Job：运行单个任务的 pod

1：Job: 一旦任务完成，不重启 容器；

两个地方会重启：

- 1：job 异常；
- 2：pod 在执行任务时，被从节点逐出；

2：会重启的资源

> job 只有在执行失败的时候才会被重启；

被托管的 ReplicaSet 会重启， Job 若未完成，也会重启。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgHg0GouTYdu2r9bOSwOaQh72SpIerD6OhNaHMibLIstticpTQIhY6WIZg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617182356951

3：job 资源：

`restartPolicy` 配置为 `Onfailure` or `Never`：完成后 不需要 一直重启

```
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        # Job 未指定 pod 选择器，默认根据 pod 模板中的标签创建
        app: batch-job
    spec:
      # job 不能使用默认的 Always 作为重启策略
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job
```

4：job 中串行运行多个 pod:

```
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5 # 顺序执行 5 次
```

5：job 中并行运行多个 pod：

```
spec:
  completions: 5  # 需要完成 5 次
  parallelism: 2  # 最多同时 2 个并行
```

6：Job 在运行时，可调整 Job 数量：

```
kubectl scale job <job-name> --replicas 3
```

7：限制 Job 完成时间和失败重试次数：

- activeDeadlineSeconds: 限制 Job 运行的时间
- spec.backoffLimit: 默认 6，Job 失败前 可重试的次数

### 7.6 CronJob: 定期执行

1：时间格式：cron

2: 每隔 15 分钟运行一次：

```
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: batch-job-every-fifteen-minutes
spec:
  # 每15分钟运行一次
  schedule: "0,15,30,45 * * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: periodic-batch-job
        spec:
          restartPolicy: OnFailure
          containers:
          - name: main
            image: luksa/batch-job
```

3：时间格式：

- 分钟
- 小时
- 每月中的第几天
- 月
- 星期几

4：注意事项

- CronJob 根据 `jobTemplate` 创建 job 对象;
- `startingDeadlineSeconds` 超时未执行 视为 Failed;
- Job 能被重复执行，可能会被创建多个；
- Job 应该是串行的， 中间不能有 遗漏的任务；

## 8.**服务**

1：服务可以说是 k8s 中最复杂的一环。

k8s 集群和普通集群不同的是，

- pod 是临时的，随时会被创建和关闭（动态扩缩容）
- pod 重启，，会分配新的 IP 地址；

2：Service 资源：外部访问后端 pod， 可提供一个统一可供外部访问的 IP 地址和端口

> 这里更多的是针对无状态服务，所有 pod 都是对等的， API 服务器只需 随机分配一个 pod

3：应用服务的两种情形：

- 外部集群 可通过 服务 连接 pod
- 内部 pod 之间也可通过服务连接

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgImhiaT4AOEd0IO79cjoSsY4Z4DWUibHNrCTUsy1LnmbictS9F5a8psW6Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617141428940

### 8.1 连接集群内部的 Service

4：服可通过标签选择器，选择 需要 连接的 pod

> 控制 Service 服务的范围。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhggDhRIicaGJgfcicyahBx4HIR7Yx0g0uv3QZSjm7Prib5icBt2lz4xqMA9w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617141644761

5：服务可通过 `kubectl expose` 创建，也可通过 yaml 创建。

```
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  # 服务对外的端口
  - port: 80
    # 服务转发到容器的端口
    targetPort: 8080

  # 连接pod 集合是带有 app: kubia 标签的 pods
  selector:
    app: kubia
```

创建 svc 后，会分配一个 **集群 IP**，==该 IP 对外不可用==，仅限于 集群内的 pod 访问 【pod 和 pod 之间也可通过 服务连接】。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgTWraGlibuf6UEAp3ZSZLvDLic6PcVia6hsTtISw84v0258WJdTNiaQKDuA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210621205022413

集群内部测试服务，有三种方法向 Service 发送请求：

- 创建一个 pod 应用，并向 集群 IP 发送 requests；
- ssh 登录到节点上，使用 curl
- 登录到其中的一个 pod 运行 curl 指令

6：可用 `kubectl exec` 在远程容器里执行：

```
user00@ubuntu:~$ kubectl get po
NAME          READY   STATUS    RESTARTS   AGE
kubia-jppwd   1/1     Running   0          5m13s
kubia-sxkrr   1/1     Running   0          5m13s
kubia-xvkpg   1/1     Running   0          5m13s

# --  表示 Kubectl 执行命令的结束
# -s 告诉 kubectl 需要连接不同的 API服务器，而非默认的
user00@ubuntu:~$ kubectl exec kubia-jppwd -- curl -s  http://10.105.237.81
```

curl 的过程如下， Service 选择 pod 是随机选择一个。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgXjG6tickZ9fHSe3FYuETEsB1Go4xdKicDHCaE6UMWYiaVvknuiaM4gnDBg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617144543726

7：Service 可以通过设置 `sessionAffinity: ClientIP` 来让同一个客户端的请求每次指向同一个 pod.

> 默认值是 None

8：Service 可同时暴露多个端口， 例如 http 请求时， 80 端口映射到 8080， https 请求时， 443 端口映射到 8443

9：在 Service 的生命周期内， 服务 ip 不变。

- 当 pod 创建时，k8s 会初始环境变量指向现在的 集群 IP

```
user00@ubuntu:~$ kubectl exec kubia-jppwd  env | grep -in  service
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
3:KUBERNETES_SERVICE_HOST=10.96.0.1
4:KUBERNETES_SERVICE_PORT_HTTPS=443

# 集群IP 和 端口
7:KUBIA_SERVICE_HOST=10.105.237.81
8:KUBIA_SERVICE_PORT=80
12:KUBERNETES_SERVICE_PORT=443
```

对应了两个服务：

```
$ kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   2d21h
kubia        ClusterIP   10.105.237.81   <none>        80/TCP    50m
```

10：每个 pod 默认使用的 dns:

```
$kubectl exec kubia-jppwd  -- cat /etc/resolv.conf
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local localdomain
options ndots:5
```

pod 是否适用 dns， 由 `dnsPolicy` 属性决定。

系统命名空间，通常有个 pod 运行 DNS 服务；

```
$ kubectl get po -n kube-system
NAME                               READY   STATUS    RESTARTS   AGE
coredns-74ff55c5b-pvqxv            1/1     Running   0          2d21h
```

11：在 pod 中，可使用全限定域名（FQDN） 来访问 Service。

格式如下：

```
kubia.default.svc.cluster.local

# kubia: Service 名称
# default: namespace
# svc.cluster.local： 所有集群本地服务中使用的可配置集群域后缀
```

若在同命名空间下， `svc.cluster.local` 和 `default` 可省略。

```
user00@ubuntu:~$  kubectl exec -it  kubia-jppwd  bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@kubia-jppwd:/# curl http://kubia.default.svc.cluster.local
You've hit kubia-sxkrr
root@kubia-jppwd:/# curl http://kubia.default
You've hit kubia-xvkpg
root@kubia-jppwd:/# curl http://kubia
You've hit kubia-xvkpg
```

12：集群 IP 是一个 虚拟的 IP， 只有配合服务端口才有意义。

```
root@kubia-jppwd:/# ping kubia
PING kubia.default.svc.cluster.local (10.105.237.81): 56 data bytes
^C--- kubia.default.svc.cluster.local ping statistics ---
51 packets transmitted, 0 packets received, 100% packet loss
```

### 8.2 连接集群外部的 Service

1：让 pod 连接到集群外部。

2：服务并不是和 pod 直接相连，中间有 Endpoint 资源

```
user00@ubuntu:~$ kubectl get svc kubia
Selector:          app=kubia # 创建 endpoint 资源，选择的 pod 标签

TargetPort:        8080/TCP
Endpoints:         172.17.0.3:8080,172.17.0.4:8080,172.17.0.5:8080
```

> Service 通过选择器构建 IP 和 端口 列表，然后存储在 endpoint 资源中
>
> 连接时，随机选择其中一个

3：当 Service 无节点选择器时，不会自动创建 Endpoint 资源。

```
apiVersion: v1
kind: Service
metadata:
  # Service 名称，后面会用
  name: external-service
spec:
  ports:
  - port: 80

  # 无 选择器
```

手动指定 Endpoint 注意需要**==和 Service 同名称==**。

```
apiVersion: v1
kind: Endpoints
metadata:
  # 和 Service 同名称
  name: external-service
subsets:
  # Service 重定向的 IP 地址
  - addresses:
    - ip: 11.11.11.11
    - ip: 22.22.22.22
    ports:
    - port: 80
```

通过 endpoint 列表， 可连向其它地址。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg7r516pUlia5kTTVJJmGD5UJ7jicmcH3BHMMEtn6WibCVcjzRR5vjoeLtA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617154439324

4：也可创建具有别名的外部服务：

```
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  # 别名
  type: ExternalName
  externalName: api.somecompany.com
  ports:
  - port: 80
```

创建该服务后，内部 pod 可通过 `external-service.default.svc.cluster.local` 访问外部域名

### 8.3 将服务暴露给外部客户端

1：pod 向外部公开的服务，如 web 服务。

2：有以下几种方式，使外部可访问服务：

- 服务类型为 NodePort: 每个节点上开放一个端口，访问内部服务 (可被 Service 访问)。
- 服务类型 为 LoadBalance：NodePort 的一种扩展， 通过负载均衡器访问， 将流量重定向到所有节点的 NodePort;
- 创建 Ingress 资源：通过一个 IP 地址公开多个 服务 （运行在 HTTP 层）

#### 8.3.1 **NodePort**

1：创建一个 NodePort 类型的 Service。

```
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  # 服务类型为 NodePort
  type: NodePort
  ports:
   # 集群IP的端口
  - port: 80
    # pod 开放的端口
    targetPort: 8080

    # 不指定端口，将随机选择一个端口
    nodePort: 30123
  selector:
    app: kubia
```

2：现在可通过以下几种方式访问服务：

```
user00@ubuntu:~$ kubectl get svc
NAME               TYPE           CLUSTER-IP
kubia-nodeport     NodePort       10.104.119.122   <none>          80:30123/TCP   5m21s
```

- 1: <集群 IP>:80

- 2: 多节点集群

- - 节点 1IP:30123
  - 节点 2IP:30123

> 注意：即使连到节点 1 的端口， 也可能分配到 节点 2 上执行。
>
> 设置 `externalTrafficPolicy:local` 属性，可将 流量路由到当前节点所在的 pod . 若当前节点 pod 不存在，连接挂起。

`externalTrafficPolicy:local` 不利于负载均衡， 当节点上的 pod 分散不均时。

> 有个好处是，转发到本地 pod，不用进行 SNAT（源网路地址转换），这会将 改变 源 IP 记录。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgtk7Py6ObxHxQf4U4SGzLQ91KeIL8TeZG4Rk7bqRoAJoTeG2sTpFHgQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617170835886

开放 NodePort 端口。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg75ehHmOb6EacB81TCh7fwYVXX4MQiadM9NiaQHA2fuGdJ1P7YX7VHvVQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617162126942

3：获取 节点 IP 【特意观察了下，节点 IP 和 机器 IP 不是一个，但在同一网关下】

```
user00@ubuntu:~$ kubectl get nodes -o json | grep address
                "addresses": [
                        "address": "192.168.49.2",
```

可在本地机器上，向 `节点IP:nodePort` 发送请求：

```
$ curl http://192.168.49.2:30123
You've hit kubia-jppwd
```

minikue 可通过网页访问 `minikube service <service-name>`：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg23tO69PlTFW0n5Srxf6Hm7MoGZd8EnlpcRCqsIqfwgibCFEHAeWgWkQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617164756909

#### 8.3.2 **LoadBalancer**

负载均衡器在 NodePort 外面包了一层。

1：负载均衡器**放在节点前面**，将请求传播到 健康的节点。

负载均衡器拥有 自己独一无二的 可公开访问的**IP 地址**， 并将连接重定向到服务。

```
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  # 负载均衡器类型
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

> 若没有指定特定端口，负载均衡器会随机选择一个端口。

```
$ kubectl get svc | grep load
              # <EXTERNAL IP>
kubia-loadbalancer   LoadBalancer   10.96.24.178     <pending>       80:30600/TCP   11m
```

2：创建服务后，要等待很长时间， 才能将 外部 IP 地址写入对象。

可通过外部 ip 直接访问：

```
curl http://<EXTERNAL IP>
```

3：可以通过 浏览器访问，但每次访问都是一个 pod， 即使 `Session Affinity` 设为 None

因为浏览器采用 keep-alive 连接，而 curl 每次开新连接。

4：请求连的时候， 会连接到负载均衡器的 80 端口， 并路由到某个节点上分配的 NodePort 上，随后转发到 一个 pod 实例上。

本质还是打开了 NodePort，仍能继续使用 `节点IP:隐式 NodePort` 端口访问：

```
$ curl http://192.168.49.2:30600
You've hit kubia-sxkrr
```

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg1j3TlHnMtysCcmJruSicB3NgVfZ1HBYOS7rpNld1opR5Y3VlO9icBxCw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617170233160

#### 8.3.3 **Ingress**

1：Ingress 也可对外暴露服务，准入流量控制。

2：Ingress 工作在 HTTP 层，通过一个 主机名 + 路径 就能转到不同的服务

而 每个 LoadBalancer 服务需要自己的负载均衡器和独有的 公有 IP 地址。

> 工作在更高层次的协议层，可以提供更丰富的服务。

ingress 可以绑定 **==多主机、同主机多路径。==**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgOtJanHnu6APXtkfbxLW72UfnFiav2TFgRRS7iaichn79CrJoVXiaNR8aTQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617171436236

3：启用 Ingress 资源需要 Ingress 控制器。

> 通过 `minikube addons list` 确认。

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          # 将 kubia.example.com/ 请求转发到 nodeport 服务
          serviceName: kubia-nodeport
          servicePort: 80
```

4：`kubia.example.com` 访问服务的前提是 域名能正确解析为 ingress 的 IP。

```
$ kubectl get ingress
NAME    CLASS    HOSTS               ADDRESS        PORTS   AGE
kubia   <none>   kubia.example.com   192.168.49.2   80      70s
```

然后再 `/etc/hosts` 加入映射：

```
root@ubuntu:~# cat /etc/hosts
127.0.0.1       localhost
127.0.1.1       ubuntu

192.168.49.2 kubia.example.com
```

通过地址访问：

```
# 必须 配置 hosts，直接通过 ingress IP 访问是不行的，无法知道访问的是哪个服务
$ curl http://kubia.example.com
You've hit kubia-xvkpg
```

5：ingress 访问流程如下：

- 通过 DNS 查找 `http://kubia.example.com` 对应的 ingress IP
- 向 Ingress 控制器发送 请求，并在头部中包含 需要访问的 服务 `kubia.example.com`
- 在 Endpoints 中查看该服务对应的 pod 的 IP 表，选择其中一个 pod 进行处理；

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgv8RRfHhJ5AZGgL1cyjJRWZKPaKl4ya1pqzFfaLUzJQvqficxjfMmLhQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617172908817

6：若要采用 https 访问，需要 配置 tls.secrete

> 针对每个主机域名 配置一个。

```
name: kubia
spec:
  tls:
  - hosts:
    - kubia.example.com
    secretName: tls-secret
```

#### 8.3.4 **就绪探针**

1：确认服务启动后，对外提供服务。

#### 8.3.5 **headless 服务**

有时候在集群内部/或者集群外部 需要知道 其它节点的 IP 列表，创建一个 headless 服务包裹一层， 去查询该服务的 DNS，会打印 该服务下的（标签选择器选中的）所有 pod。

1：`clusterIP:None`, DNS 服务器返回的是 pod 的 IP，而非集群 IP

2：创建 headless Service：

该 headless 后端，包含标签选择器选择的所有 pod

```
apiVersion: v1
kind: Service
metadata:
  name: kubia-headless
spec:
  # headless 服务
  clusterIP: None
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

headless 服务无集群 IP

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgt48ml3mrYicVlCQOZg39L6dCwh74vPAABuYNQ4UgCh8SMm4fFANkbbA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617174322816

运行临时 pod

```
kubectl run dnsutils --image=tutum/dnsutils   --command -- sleep infinitypod/dnsutils created
```

在该临时 pod 查看 headless 服务标签选择器选择的 pod

> 注意，只会展示 就绪的 pod ip

```
$ kubectl exec dnsutils nslookup kubia-headless
Name:   kubia-headless.default.svc.cluster.local
Address: 172.17.0.5
Name:   kubia-headless.default.svc.cluster.local
Address: 172.17.0.4
Name:   kubia-headless.default.svc.cluster.local
Address: 172.17.0.3
```

普通 service 返回的是集群 IP：

```
$ kubectl exec dnsutils nslookup kubia

Name:   kubia.default.svc.cluster.local
Address: 10.105.237.81
```

## 9.**卷**

1：卷的作用是将 磁盘挂载到容器。

> 这个 和 linux 将指定目录挂载到盘 很类似。

2：每个 pod 都有独立的文件系统，文件系统来自于 容器镜像。

> 默认， 容器重启后并不能识别 之前容器写入文件系统的内容。
>
> 这是因为 新的容器拥有 新的 写入层。

3：pod 中的所有容器都能使用卷，但是需要提前挂载。

4：emptyDir 卷是挂载一个空的目录。

- 卷的装载在容器启动之前执行；
- emptyDir 卷 的生命周期 和 pod 相同；

5：可用的卷类型：

- emptyDir —— 用于存储临时数据的简单空目录。
- hostPath —— 用于将目录从工作节点的文件系统挂载到 pod 中。
- gitRepo —— 通过检出 Git 仓库的内容来初始化的卷。
- nfs —— 挂载到 pod 中的 NFS 共享卷。
- gcePersistentDisk (Google 高效能型存储磁盘卷）、 awsElastic BlockStore (AmazonWeb 服务弹性块存储卷）、 azureDisk (Microsoft Azure 磁盘卷）一一用于挂载云服务商提供的特定存储类型。
- cinder 、 cephf 、 iscsi 、 flocker 、 glusterfs 、 quobyte 、 rbd 、 flexVolume 、 vsphere-Volume 、 photonPersistentDis k、 scaleIO 用于挂载其他类型的网络存储。
- configMap 、 secret 、 downwardAPI 一一用于将 Kubemetes 部分资源和集群信息公开给 pod 的特殊类型的卷 。
- persistentVolumeClaim 一一一种使用预置或者动态配置的持久存储类型（我们将在本章 的最后一节对此展开讨论） 。

> 单个容器可同时使用不同类型的多个卷

### 9.1 emptyDir

1：emptyDir: 在 pod 中的多个容器间共享存储：

```
apiVersion: v1
kind: Pod
metadata:
  name: fortune
spec:
  containers:
  - image: luksa/fortune
    name: html-generator
    volumeMounts:
    - name: html
      # 挂载的目录
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      # 只读
      readOnly: true
    ports:
    # Nginx 监听80端口
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html # 卷名称
    emptyDir: {}
```

可在内存上创建 emptyDir

```
volumes:
  - name: html # 卷名称
    # emptyDir: {}
    emptyDir:
      medium: Memory
```

### 9.2 gitRepo

1：gitRepo 卷：gitRepo 卷基本上也是 一 个 emptyDir 卷，它通过克隆 Git 仓库并**在 pod 启\****动时（但在创建容器之前 ） 检出特定版本**来填充数据

> 创建 pod 时，会 checkout 指定版本。

```
apiVersion: v1
kind: Pod
metadata:
  name: gitrepo-volume-pod
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    # 创建一个 gitRepo 卷
    gitRepo:
      repository: https://github.com/luksa/kubia-website-example.git
      revision: master
      # checkout 到当前目录， 可通过路径 /usr/share/nginx/html 访问
      directory: .
```

8：可创建一个 sidecar 容器，实时同步 git， 如 Docker Hub 上的 gitsync

### 9.3 hostPath

1：hostPath 卷，指向节点文件系统上特定的文件或目录。

> 注意是一些 系统级别的 pod （通常由 DaemonSet 管理）需要访问。

2：多个 pod hostPath 卷中使用相同的路径，可看到相同的文件。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgq46knWarpyqILq6qs3zQCGwZ24fGqL3TrCIuiclhEibGcicjKHsNJX8uA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617125426036

3：hostPath 是第一种持久性存储的卷。【pod 删除后依然还在】

emptyDir 和 gitRepod 随 pod 删除而删除。

4：若将数据存储到 节点上，则 pod 不能随机调度，需要调度到指定节点才行。

> 特别是 pod 重启时。

### 9.4 nfs

1：nfs 服务器可共享路径

```
apiVersion: v1
kind: Pod
metadata:
  name: mongodb-nfs
spec:
  volumes:
  - name: mongodb-data
    nfs:
      # 指定 nfs 服务
      server: 1.2.3.4
      # 共享路径
      path: /some/path
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
```

### 9.5 PV 和 PVC

1：将卷这种持久性信息 和 pod 解耦，可避免处理基础设施细节。

2：PersistentVlume (持久卷， 简称 PV) ，由集群管理员设置的 底层存储。

3：PersistentVlumeClaim （持久卷声明，简称 PVC），用户声明需要申请的（存储容量和访问模式）

API 服务器 负责找到满足要求的 持久卷并绑定到 持久卷声明。

> 持久卷声明可当做 pod 中的一个卷来使用；

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgk9DzE4Sujqt71PJCFnmhtD5h9BRtDOrECkPGyFoGzeRBqgUammeRIg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617130652486

4：创建 PV

```
apiVersion: v1

# 创建持久卷
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  # 持久卷大小
  capacity:
    storage: 1Gi
  # 读写模式
  accessModes:
    # 单个客户端挂载是 读写模式
    - ReadWriteOnce
    # 多个客户端挂载是 只读模式
    - ReadOnlyMany

  # 设置策略， PVC 释放后，PB将保留
  persistentVolumeReclaimPolicy: Retain

  # 在Google的 gce 磁盘上分配
  gcePersistentDisk:
    pdName: mongodb
    fsType: ext4
```

`persistentVolumeReclaimPolicy` 可指定回收策略：

- Retain: 删除 PVC 后， PV 保留；
- Recycle：删除内容，可再次被 PVC 绑定
- Deleet: 删除底层存储

5: 特别注意的是：

PV：属于集群层面的资源；

PVC 和 Pod: 属于命名空间内的资源；

PVC 声明：

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
spec:
  resources:
    # 申请 容量
    requests:
      storage: 1Gi
  # 单个客户端，支持读写
  accessModes:
  - ReadWriteOnce
  # 动态配置
  storageClassName: ""
```

> k8s 根据声明的访问权限、容量大小， 寻找满足要求的 PV

PV 和 PVC 的创建相对独立。

6：使用 PVC，只需要在 volumes 中引用即可。

```
apiVersion: v1
kind: Pod
metadata:
  name: mongodb
spec:
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
  volumes:
  - name: mongodb-data
  # 引用 PVC
    persistentVolumeClaim:
      claimName: mongodb-pvc
```

7: PV 和 PVC 的解耦，存储这块，方便管理。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg4hP67KwtmpDVaY0LQQbwKLwOt2e5ibg7fVtmiaYBv1XcT6R5lq1UedIw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



### 9.6 StorageClass

1: 创建存储类 StorageClass，可动态创建 PV。

> 方便集群管理员管理。

2：不指定 StorgeClass ，会使用集群默认的存储类分配。

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc2
spec:
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce

  # 未指定 StorageClass，则使用默认的 StorageClass 分配
```

3：手动配置 的 PV 和 StorageClass 可同时存在，若不想用 StorageClass 分配时，可将 `StorageClass:""` 配置为空，则将使用 预先配置的 PV 持久卷。

4：创建一个 StorageClass 对象：

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast

# 创建 PV的预制程序
provisioner: k8s.io/minikube-hostpath
# 参数
parameters:
  type: pd-ssd
```

在 PVC 中使用 StorageClass:

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
spec:
  # 引用 StorageClass 即可
  storageClassName: fast
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce
```

5：整体关系如图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgUskHp9ibXoxFbHiaGibfiaBH81NDcN5roplKVib60O6hkvGHHs3sJ9kdjLA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617140107905

## 10.**ConfigMap 和 Secret**

### 10.1 ConfigMap

1：给应用传递参数，有以下几种方式：

- 命令行参数；

```
docker run <image> <arguments>
```

在容器中启动有两种方式：

-- shell 形式一如 ENTRYPOINT node app.js。

-- exec 形式一如 ENTRYPOINT ["node", "app. js"]

> 前者是通过 shell 启动的， 后者是 进程直接运行。

例如 在 Dockerfile 镜像中传入：

```
FROM ubuntu:latest

RUN apt-get update ; apt-get -y install fortune
ADD fortuneloop.sh /bin/fortuneloop.sh

ENTRYPOINT ["/bin/fortuneloop.sh"]
# 参数列表
CMD ["10"]
```

在 pod 中定义容器时，可覆盖 ENTRYPOINT 和 CMD，只需要设置 command 和 args

```
kind:pod
spec:
 containers:
 - image: some/image
   command: ["/bin/command"]
   args: ["arg1", "arg2", "arg3"]

   # 多个参数，可用下面格式
   args:
   - foo
   - bar

   # 数值需要用引号
   - "15"
```

- 环境变量；

在容器中设置环境变量如下， 可以在 shell 中直接引用该变量 `$INTERVAL`

```
kind: Pod
metadata:
  name: fortune-env
spec:
  containers:
  - image: luksa/fortune:env
    env:
    - name: INTERVAL
      value: "30"

    - name: SECOND_VAR
      # 引用第一个参数 30test
      value: "${INTERVAL}test"
```

- 通过特殊类型的卷挂载到容器

卷挂载如下：

```
apiVersion: v1
kind: Pod
metadata:
  name: fortune-env
spec:
  containers:
  - image: luksa/fortune:env
    env:
    - name: INTERVAL
      value: "30"

    - name: SECOND_VAR
      # 引用第一个参数 30test
      value: "${INTERVAL}test"
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    emptyDir: {}
```

- ConfigMap 是 k8s 中存储配置数据的资源。
- 证书和私钥相关的配置数据，使用 Secret 资源存储。

2：ConfigMap 主要是将配置 从 pod 中解耦出来， 这样生产环境和非生产环境下的 pod 定义文件可以保持不变。

> 多种环境下，只要保证 ConfigMap 不同即可。

```
kubectl create configmap <configmap-name> --from-literal=foo=bar  # kv 结构，key 是 foo，value 是bar

kubectl create configmap <configmap-name> --from-file=customkey=config.conf  # 从文件读取 或者 文件夹读取
```

如：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgfhfCrlaXiad08bibu6BTT1fGa2YdDuicjpfCAj0Bs8IrM9e8P0UhosKfQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617103038594

对应关系如下：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgamjxKtWYJDYOdKibeyC2IAZlRicXXNapuFzcsfoF6HSB6RG65E5ED6RA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210617103121948

3：将 configMap 条目作为环境变量：

```
kind: Pod
metadata:
  name: fortune-env-from-configmap
spec:
  containers:
  - image: luksa/fortune:env
    env:
    - name: INTERVAL # 设置环境变量
      valueFrom:
        configMapKeyRef: # 来自于 ConfigMap
          # 引用 config-map 的名称
          name: fortune-config
          # config-map 下的 key
          key: sleep-interval
    args: ["${INTERVAL}"]  # 作为命令行参数
```

> 引用不存在的 configMap, 容器会启动失败，从而一直重启。

4：`envFrom` 字段可暴露所有来自 ConfigMap 的变量，并可在变量名引入后加入 前缀。

5：可将 configMap 中的条目挂载到指定目录下 【每个 key 作为 文件存在】

```
kind: Pod
metadata:
  name: fortune-configmap-volume
spec:
  containers:
  - image: luksa/fortune:env
    env:
    - name: INTERVAL
      valueFrom:
        configMapKeyRef:
          name: fortune-config
          key: sleep-interval
    name: html-generator
    volumeMounts:
    - name: html
    # 将卷中的条目，挂载至该目录下，条目的 名称就是该目录下的文件名
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    - name: config
      mountPath: /etc/nginx/conf.d
      readOnly: true
    - name: config
      mountPath: /tmp/whole-fortune-config-volume
      readOnly: true
    ports:
      - containerPort: 80
        name: http
        protocol: TCP
  volumes:
  - name: html
    emptyDir: {}
  - name: config
    #  configMap 卷
    configMap:
      name: fortune-config
```

6：注意：configMap 挂载到已存在的文件夹，会隐藏所有已有的条目。

可以选择挂载部分卷，避免隐藏整个文件夹。

```
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      # 只挂载 configmap 中的指定条目
      subPath: myconfig.conf
```

> configMap 卷中的文件权限默认设置为 644 (-rw-r-r--)，
>
> 可通过 `defaultMode` 修改

7：ConfigMap 通过暴露卷，可以达到配置热更的效果，无需新建 pod 或 重启容器。

### 10.2 Secret

1：和 configMap 一样， secret 也是 key-val 存储。

2：使用 和 ConfigMap 相同：

- 将 Secret 条目作为环境变量传递给容器
- 将 Secret 条目暴露为卷中的文件

> Secret 只会存储在节点的内存中， 永不写入物理存储，

3：使用场景：

- 采用 ConfgMap 存储非敏感的文本配置数据。
- 采用 Secret 存储天生敏感的数据， 通过键来引用。如果一 个配置文件同时包 含敏感与非敏感数据， 该文件应该被存储在 Secret 中。

4：Secretes 一般包含三种文件：

- ca.crt
- namespace
- token

```
user00@ubuntu:~$ kubectl describe secrets
Name:         default-token-c2v26
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: default
              kubernetes.io/service-account.uid: 1fe33279-b738-4a3b-a012-5c5a709cdcb8

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1111 bytes
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IjVVWWRld2FCX0tLbGJFZFdBd3JBcncxRmxQRnBGMTFaRDZfM2FJWTVra0kifQ.
```

> Secret 条目的内容会以 Base64 编码

5：挂载 Secrete 卷：

```
kind: Pod
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: certs

      # 挂载证书的路径
      mountPath: /etc/nginx/certs/
      readOnly: true
    ports:
    - containerPort: 80
    - containerPort: 443
  volumes:

  # 引用 secret 卷
  - name: certs
    secret:
      secretName: fortune-https
```

6: k8s 允许通过环境变量暴露 Secret，但是不安全。

```
env:
 -name: FOO_SECRET
 valueFrom:
  secretKeyRef:
      # secret 资源名称
   name: fortune-https
   # 引用的条目
   key:foo
```

7：从 Docker Hub 网站拉取私有 Docker 镜像仓库时，需用 Secret 鉴权

```
$ kubectl create secret docker-registry mydockerhubsecret \
--docker-username=myusername --docker-password=mypassword \
--docker-email=my.email@provider.com
```

8：Pod 中使用 Secret。

```
kind: Pod
metadata:
  name: private-pod
spec:
  # 引用  Secret
  imagePullSecrets:
  - name: mydockerhubsecret
  containers:
  - image: username/private:tag
    name: main
```

若运行大量 pod，可通过将 Secret 添加到 ServiceAccout， 所有 pod 只要使用了 sa，都能自动添加上镜像拉取的 Secret.

## 11.**从应用访问 pod 元数据及其它资源**

### 11.1 Downward API

1：传递不能提前知道的数据，如 pod 的 IP、主机名

或者在别处预定义的数据，如 pod 标签和注解。

2：Downward API 不像 REST Endpoint 那样需要通过访问的方式获取数据；

通过将 pod 中取得的数据作为环境变量 或 文件的值（Downward API 卷）对外暴露。

> 注意：标签和注解只能通过卷暴露

3：Downward API 可以给容器传递的 元数据有：

- pod 的名称
- pod 的 IP
- pod 所在的命名空间
- pod 运行节点的名称
- pod 运行所归属的服务账户的名称
- 每个容器请求的 CPU 和内存的使用量
- 每个容器可以使用的 CPU 和内存的限制
- pod 的标签
- pod 的注解

4：使用元数据作为环境变量：

```
apiVersion: v1
kind: Pod
metadata:
  name: downward
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi

    # 设置环境变量
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          fieldPath: metadata.name  # 应用 pod 中的元数据名称字段
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    - name: SERVICE_ACCOUNT
      valueFrom:
        fieldRef:
          fieldPath: spec.serviceAccountName
    - name: CONTAINER_CPU_REQUEST_MILLICORES
      valueFrom:
        # 注意: CPU 和 内存的引用使用 resourceFieldRef， 而非 fieldRef
        resourceFieldRef:
          resource: requests.cpu
          # 基数单位是 1 毫核，1/1000 核
          divisor: 1m
    - name: CONTAINER_MEMORY_LIMIT_KIBIBYTES
      valueFrom:
        resourceFieldRef:
          resource: limits.memory
          divisor: 1Ki
```

对应关系如下：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgibnL47zzCkWjQNc3CRJKWO26y6UEdsqSPTmmyIicJ6cuGQlK7a6KsKgw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616211558165

5：挂载 downwardAPI 卷暴露 【推荐】

```
apiVersion: v1
kind: Pod
metadata:
  name: downward

  # 将会在 Downward  卷暴露的元数据
  labels:
    foo: bar
  annotations:
    key1: value1
    key2: |
      multi
      line
      value
spec:
  containers:
  - name: main
    image: busybox
    command: ["sleep", "9999999"]
    resources:
      requests:
        cpu: 15m
        memory: 100Ki
      limits:
        cpu: 100m
        memory: 4Mi
    volumeMounts:
    - name: downward
      # downwardAPI 挂载的目录
      mountPath: /etc/downward
  volumes:
  - name: downward
    downwardAPI:
      items:
      # pod 的名称会写入文件 /etc/downward/podName
      - path: "podName"
        fieldRef:
          fieldPath: metadata.name

      # 写入文件 /etc/downward/podNamespace
      - path: "podNamespace"
        fieldRef:
          fieldPath: metadata.namespace

      # 写入文件 /etc/downward/labels
      - path: "labels"
        fieldRef:
          fieldPath: metadata.labels

      # 写入文件 /etc/downward/annotations
      - path: "annotations"
        fieldRef:
          fieldPath: metadata.annotations

      # 写入文件 /etc/downward/containerCpuRequestMilliCores
      - path: "containerCpuRequestMilliCores"
        resourceFieldRef:
          # 资源相关的必须指定容器名称，因为卷 是 pod 级别的
          containerName: main
          resource: requests.cpu
          divisor: 1m

      # 写入文件 /etc/downward/containerMemoryLimitBytes
      - path: "containerMemoryLimitBytes"
        resourceFieldRef:
          containerName: main
          resource: limits.memory
          divisor: 1
```

文件列表的对应关系：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgILEyKnAjtkbljHh9sTGHNjpp4yYiaKXf2Q7h4V3E2NlIJJ8r7PpLl1g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616212130889

6：推荐用卷的方式暴露，在运行时修改 注解 或 标签， k8s 会更新相关的文件；

且卷能在 同 pod 的多容器间传递；

但环境变量一旦设置，修改后无法暴露新值。

7：downward API 只能对 pod 内的容器暴露数据，有一定的局限性；

### 11.2 与 k8s API 服务器交互

1：k8s API 可获取其它 pod 或集群中其它资源信息；

2：可以通过 `kube proxy` 启动一个代理服务接收本地的 http 连接并转发至 API 服务器。

同时处理**身份认证**， 所以不需要每次请求都上传认证凭证。它也可以确保我们直接与真实的 API 服务器交互 ， 而不是一个中间入（通过每次验证服务器证书的方式）

> kube proxy 会捎带 API 服务器的 URL 、认证凭证等。

代理服务器 在本地的 8001 端口接收请求。

```
$ curl localhost:8001
{
"paths": [
"/api",
"/api/vl",
```

3：可以通过 URL 查看对应资源的 API， 例如 Job 资源的 API，分组在 `/apis/batch`

例如：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg6muNHjnBA4ibBSNNm5uZ4sPQkiazVIialSjsyVY0ib2BvHd5djicicGXowmQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616213620233

4：从 pod 内部访问 API 服务器，需要带上 凭证和授权：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg29MMckhHeenVXPDpGh04ibzBV0Y9cZpDae6esskVGA7mh3cO4Cgq3pw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616213916792

> 后续可通过 ServiceAccount 和 RBAC 解决账户和授权的问题。

5：Pod 与 API 服务器的交互如下：

- 应用应该验证 API 服务器的证书是否是证书机构所签发， 这个证书是在 ca.crt 文件中。
- 应用应该将它在 token 文件中持有的凭证通过 Authorization 标头来获得 API 服务器的授权。
- 当对 pod 所 在 命 名空间的 API 对 象进行 CRUD 操作时， 应 该使 用 namespace 文件来传递命名空间信息到 API 服务器

> CRUD 代表创建、 读取、 修改和删除操作， 与之对应的 HTTP 方法分别是 POST、 GET、 PATCH/PUT 以及 DELETE。

可在 pod 中运行一个 sidecar 容器（代理服务器）；

- 主容器 通过 端口访问 代理 容器 【同一网络命名空间】
- 代理服务器 运行 `kubectl-proxy` 命令，实现和 API 服务器通信；

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgRmUcLjag8TLQbicqKqRGST9ZweQpVfOQuDXZqpKJWnMyEqbwDztfaaA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616214346169

代理容器如下，

```
apiVersion: v1
kind: Pod
metadata:
  name: curl-with-ambassador
spec:
  containers:
  - name: main
    image: curlimages/curl:7.77.0
    command: ["sleep", "9999999"]
  - name: ambassador
    # 代理容器，在该容器中, 可 通过 curl localhost:8001  访问 API 服务器
    image: luksa/kubectl-proxy:1.6.2
```

由于主容器和 sidecar 容器共享网络命名空间， 直接可以访问 代理服务器的 8001 端口。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhguYsge3Bdqic029AibSQWDEWHWIDmWjTGYlGM8dFfiaq21UlAvvJeDY1Ug/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616214821893

6：也可使用标准的 客户端库与 API 服务器交互。

## 12.**Deployment**

1：Deployment 是基于 ReplicaSet 资源， 声明式的升级应用 【滚动更新】

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgJ7fmUEQ7qNhNEQLWjwQhknzPwdZKLSDwNND73TnzzpibeOgurOYsh0A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616202200622

> Deployment 在上层 控制期望的状态，更新时，会创建 新的 ReplicaSet 资源

2：通常有两种更新方式：

- 直接删除所有现有的 pod, 然后创建新的 pod。

- - 停机更新，短时间不可用

使用 ReplicationController 管理，修改 replicas 副本数即可。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgY79kTrpunTnvDQJ1N13S2AkR9Ws50GLwd59cK9ESfnfadxIXngzQ0A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616201348186

- 也可以先创建新的 pod, 并等待它们成功运行之后， 再删除旧的 pod。可以先创建所有新的 pod, 然后一 次性删除所有旧的 pod, 或者按顺序创建新的 pod, 然后逐渐删除旧的 pod。

- - 需要支持两个版本同时对外提供服务

第一种方法：先创建新的，可用后，一次性删除旧的 【蓝绿部署】：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgjEKeewC76CTh7MBJHyias1MhDa2B2ZW118UIpdkhSEI8EfekIGPAT6g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616201709906

第二种方法：手动执行滚动升级 【旧的副本数在减少，新的副本数在增加。】：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgMattTG7aXxVXmpjJ3Kes0fU4PFovLBpfiaRibZ6suOewGqpOIvzKSOmQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616201824859

3：创建一个 Deployment 示例：

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  # 目标副本数
  replicas: 3

  # 新的 pod 模板
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs

  # 需要更新的 pod
  selector:
    matchLabels:
      app: kubia
```

4：Deployment 提供两种升级策略：

- 1：滚动更新， RollingUpdate;

升级过程中 速度可控：`minReadySeconds` 可控制滚动更新的速度

> minReadySeconds 要求 pod 至少运行多久才算可用。在 新 pod 可用之前， maxUnavailable 会卡主更新。
>
> 若在 minReadySeconds 时间内，就绪探针失败， 新版本滚动会停止。

pod 未就绪时，新的请求也不会 分发到上面。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgsVUr0JVqe0A1U72yvAweDSHwSzYicIkENgCCEaF3rOJQicISgDK3alEw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616204948017

触发更新如下， 直接设置新的镜像版本即可：

```
kubectl set image deployment/replicationcontroller/replicaset <资源名> <container-name>=<ID>/<image>:<tag>: 修改 资源下容器里的 镜像，会修改 pod 模板; 并 触发 Deployment 的 滚动升级
```

- 2：一次性删除所有旧的，然后创建新的, Recreate

5：Deployment 升级完后，旧的 ReplicaSet 还会保留，方便出错之后 回滚。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgDBu2k02OWUAuAXmg7p7oiaY2icuQibSYERTdDzLmGn82BEwwYh5a9W5vg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616203617762

可以回滚到指定的版本，保留的历史版本数 通过 `revisionHistoryLimit` 属性控制：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgfuPsUOicyxFQCoUxic5ISy41mkIPUWawqYibRM6zwPPt9DD71VGRZ2bCg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616203753923

6：控制滚动 更新速度的两个参数：

- maxSurge: 某一时刻，最多运行的 pod 数量（包括 新版本下的 pod 和旧版本下的 pod 数总和）， 控制 每次**新版本** 增加的 pod 数
- maxUnavaliable: 滚动期间，最大不可用 pod 数量，控制必须得保留一定量 **旧版本**的 pod 树；

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgTvzDPbTmdZZmc0Bwbiccb0qGfRnLgS9oWZe0oazr9dPm667KE40iaJvw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616204222968

假设 集群目标数量是 3， maxSurge 允许最多 pod 数量达到 4， 同时 maxUnavailable = 0（任意时刻， 必须至少有 3 个可用）

3 个副本数下的更新过程：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgCiaOiaOhzb2iae5fcMq8BocCmzAjUicvfQKtaDibWnZTo6OUpLQgufM9otw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616204359032

7：升级过程中，可以暂停滚动升级（发布金丝雀版本进行测试），但在哪个时刻暂停无法控制。

> 功能验证后，可恢复滚动升级。

8：可以通过 `progressDeadlineSeconds` 属性指定滚动升级必须在 多长时间 **内** 完成，否则视为失败。

## 13.**部署有状态多副本**

1：有状态服务通常需要考虑：

- 有状态服务需要有独立的存储：
- 不变的主机名和 ip 地址

### 13.4 StatefulSet

1：StatefulSet 可以保证重启一个 pod 实例，拥有和之前一样的 名称 和 主机名。

2：StatefulSet 可以保证每个 pod 都有稳定的名字和状态；

- RS 分配的主机名都是随机的（默认平等）
- StatefulSet 分配主机名是按顺序递增的

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgaswNF4BFJM8IFiaxClnHwiboMddSTmzQkyUHcHUY1ahjXczpZ5TKzv0Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3：pod 重启后，并不保证在原来的节点上：

> 但 主机名和 ip 保持不变

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgLNfghm3TFHsVZGm8Q8pqeltLcGQC5fShLV50WzMm3F7LOC2afGeVRg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

4：StatefulSet 缩容时，会优先删除 高索引主机名的 实例 （如下，第一次缩容，Pod A-2 最先被删除。）

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgRAuwgcsPOycAj5SMOxDmtJfIEsm1icfs83DdGicNOuVvovfSgedHCH5A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

5：为每个 pod 声明单独的 PVC，提供独立的存储

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgoMN8jVGZlI4jUibex8jicqCaXgSP0HjBpjlqzuLpxadVgER8K6WljCCw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

缩容时，只会删除 pod， PVC 默认不会删除（除非手动），当下次重新扩容时， 会绑定到之前的 PVC

> 可以保证扩容出来的 pod 还是写相同的文件。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgf64St1LdxPYS02ITHJ7trnyoaK7xzs5vLgvzHYNQ7m1boiaGgBYLPew/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

6：StatefulSet 实例，创建三个对象

- 创建 PV

```
kind: List
apiVersion: v1
items:
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-a
  spec:
    capacity:
      storage: 1Mi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    gcePersistentDisk:
      pdName: pv-a
      fsType: ext4
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-b
  spec:
    capacity:
      storage: 1Mi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    gcePersistentDisk:
      pdName: pv-b
      fsType: ext4
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-c
  spec:
    capacity:
      storage: 1Mi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    gcePersistentDisk:
      pdName: pv-c
      fsType: ext4
```

- 创建一个 控制 Service

创建一个 headless Service, 可以让 pod 之间彼此发现。

通过这个 Service， 每个 pod 都有 独立的 DNS 记录， 可以通过主机名方便的找到它。

```
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  # StatefulSet 的控制 Service 必须是 headless 模式
  clusterIP: None

  # 所有 带有 app: kubia 标签的pod 都属于这个 service
  selector:
    app: kubia
  ports:
  - name: http
    port: 80
```

> 可以通过在 pod 中触发一次 SRV DNS 查询，获取其它 pod 列表

> 有时不需要或不想要负载均衡，以及单独的 Service IP。遇到这种情况，可以通过指定 Cluster IP（`spec.clusterIP`）的值为 `"None"` 来创建 `Headless` Service。
>
> 你可以使用无头 Service 与其他服务发现机制进行接口，而不必与 Kubernetes 的实现捆绑在一起。
>
> 对这无头 Service 并 **不会分配 Cluster IP，kube-proxy 不会处理它们， 而且平台也不会为它们进行负载均衡和路由**。DNS 如何实现自动配置，依赖于 Service 是否定义了选择算符。
>
> **带选择算符的服务**
>
> 对定义了选择算符的无头服务，Endpoint 控制器在 API 中创建了 Endpoints 记录， 并且修改 DNS 配置返回 A 记录（IP 地址），通过**这个地址直接到达 `Service` 的后端 Pod** 上。
>
> **无选择算符的服务**
>
> 对没有定义选择算符的无头服务，Endpoint 控制器不会创建 `Endpoints` 记录。然而 DNS 系统会查找和配置，无论是：
>
> - 对于 [`ExternalName`](https://kubernetes.io/zh/docs/concepts/services-networking/service/#external-name) 类型的服务，查找其 CNAME 记录
> - 对所有其他类型的服务，查找与 Service 名称相同的任何 `Endpoints` 的记录

- StatefulSet 本身

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kubia
spec:
  serviceName: kubia

  # 创建2个副本，pod 上带有 标签 app: kubia
  replicas: 2
  selector:
    matchLabels:
      app: kubia # has to match .spec.template.metadata.labels
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia-pet
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - name: data
          mountPath: /var/data

  # 创建 pvc 模板， 生成的pvc  data-<主机名>
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      resources:
        requests:
          storage: 1Mi
      accessModes:
      - ReadWriteOnce
```

7：StatefulSet 对集群同时启动两个 pod 非常敏感，可能存在竞争。

依次启动 是比较安全可靠的：所以后面的 pod 会等待前面的 pod 成为就绪状态后 创建。

8：StatefulSet 修改模板文件后，不会自动触发运行的 pod 更新。

> 和 ReplicaSet 一样，重启更新。

9：当 pod 突然失效时（NotReady）, StatefulSet 不会立刻驱逐. 需要等足够多的时间，或者显示的删除 该 pod，才能触发 重新调度。

简言之， StatefulSet 会避免同时运行两个一样的 pod.

## 14.**kubernetes 机理**

### 14.1 架构

1：k8s 集群分两部分：

- master node (The k8s Control Plane, 控制面板)：存储和管理集群的状态

- - etcd 分布式持久化存储
  - API 服务器
  - 调度器
  - 控制器管理器

- work node

- - Kubelet
  - Kubelet 服务代理( kube-proxy)
  - 容器运行时(Docker、rkt 或者其他)

- 附加组件

- - KubemetesDNS 服务器：通过 IP 对外暴露 HTTP 服务；
  - 仪表板
  - Ingress 控制器：对客户端 ip 保存，后续多次连接路由到 同一个 pod
  - Heapster(容器集群监控）
  - 容器网络接口插件

整体组件如下：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgBAImWSWF7Ph0Ct88tUBha0pFoMqTibwYpEM0ibeql0F1mNLLBVWM0XrQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

```
kubectl get componentstatus ：显示控制面板各个组件的状态
```

2：系统组件只能通过 API 服务器进行通信， 相互之间不同通信。

3：etcd 和 API 服务器可以有多个实例 同时并行工作。【分布式集群】

> 为了保证集群一致性，采用 RAFT 算法。

但 调度器 和 控制器 在某一个时刻，只能有一个 实例起作用，其它实例处于 待命状态。

4：kubelet 是唯一一直作为常规系统运行的组件。

5: **API 服务器**， 可查询、修改集群状态的 CRUD (Create、Read、Update、Delete)，并最终存入 etcd .

对请求的 Rest 进行校验。

- Authentication plugin: 认证插件，获取用户、用户组 等信息；
- Authorization plugin：授权插件，是否有权限对指定资源进行 指定的操作；
- Admission Control plugin: 准入插件控制，例如 资源限制 ResourceQuota 等

准入控制插件包括：• AlwaysPullImages：重写 pod 的 imagePullPolicy 为 Always, 强制每次部署 pod 时拉取镜像。• ServiceAccount：未明确定义服务账户的使用默认账户。• NamespaceLifecycle：防止在命名空间中创建正在被删除的 pod, 或在不存在的命名空间中创建 pod。• ResourceQuota：保证特定命名空间中的 pod 只能使用该命名空间分配数量的资源， 如 CPU 和内存。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgJqJptS9NrurmicoPoQBQh9WXDY0v2uJIwRPwpGdfoXNVGjYEUI6v9aw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

控制器可通过定期的去拉取 API 服务器信息，监听资源的变化。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgUbJHs1aJw6ybdqzwbLyqEglkgY51D3YiaCqKyrMkNdbtaSBOzpK51Mg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

6：调度器：为 pod 选择合适的节点

- 最简单的是随机选择一个；
- 以更优的方式选择一个， 对所有节点按优先级排序，找出最优节点（若分数一致，则循环分配）

筛选哪些节点可用：

- 节点是否能满足 pod 对硬件资源的请求。
- 节点是否耗尽资源（是否报告过内存／硬盘压力参数） ？
- pod 是否要求被调度到指定节点（通过名字）， 是否是当前节点？
- 节点是否有和 pod 规格定义里的节点选择器一致的标签（如果定义了的话） ？
- 如果 pod 要求绑定指定的主机端口（第 13 章中讨论）那么这个节点上的这个端口是否已经 被占用？
- 如果 pod 要求有特定类型的卷， 该节点是否能为此 pod 加载此卷， 或者说该节点上是否已经有 pod 在使用该卷了？
- pod 是否能够容忍节点的污点，涉及污点和容忍度。
- pod 是否定义了节点、pod 的亲缘性以及非亲缘性规则？如果是， 那么调度节点给该 pod 是否会违反规则？

> 集群中可运行 多个调度器 而非单个， 设置 `schedulerName` 来调度特定的 pod

7: 控制器：控制集群服务器的状态 朝 API 服务器定义的期望状态 收敛。

控制器包括

- Replication 管理器 (ReplicationController 资源的管理器）：监听 pod 的数量
- ReplicaSet、 DaemonSet 以及 Job 控制器：部署和维护 pod
- Deployment 控制器：控制滚动更新，每个版本，都会创建一个 ReplicaSet
- StatefulSet 控制器: 有状态服务的管理，挂载到相同的 PV，相同的 IP 和主机名
- Node 控制器：管理 Node 资源，监控 每个 Node 的健康状态；
- Serice 控制器：网络管理相关组件，创建或删除 LoadBalancer 类型服务；
- Endpoints 控制器：从 Service 的 pod 选择器中选出指定的 pod，并将 IP 和端口更新到 Endpoint 资源中；
- Namespace 控制器：创建 或 删除 Namespace 对象；
- PersistentVolume 控制器：创建一个 PVC 后，由 该控制器找到一个 合适的 PV 绑定。【存储量大于 PVC 声明的 最小 PV】
- 其他

> 控制器就是活跃的 Kuberetes 组件， 去做具体工作部署资源。

控制器通过监听 API 资源，作出相应的调整，如 更新、删除已有对象。

控制器之间不会直接通信， 每个控制器都会连接到 API 服务器。

8：kubelet: 监控 API 服务器 是否在当前节点 新分配了 pod，告知 容器运行时(如 Docker) 运行容器。

9：kube-proxy: 通过 修改 iptables ，将请求重定向到 服务器。

userspace 代理模式如下， 对每个进来的连接，代理到一个 pod

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgJFaKfgN2uAAuUBz8oLrF3hhdOxPBsskia730myHSYzf7zEJD4sFZBEA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

现在是 默认的 iptables 模式。

10：控制器之间是相互协作的，通过监听 API 服务器来判断 是否要创建 / 删除 资源。

如下 是创建一个 Deployment 资源的事件链：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgE6jcZmOMm1CkYjicicvGicriamI8Yj38MZ4EdTP7HeTzxibH1gN4YtEFU6Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

11：在每个 pod 中会有一个 基础容器（处于 pending 状态），用于保存 Linux 命名空间，当容器被重启时，需要保持和之前的命名空间一样， 这个 pod 就发挥了作用。

#### **14.1.1 网络通信**

1：同节点的 pod 之间通信。

基础容器启动前，会为容器创建一个 虚拟 Ethernet 接口对 （veth pair）：

- 一端在 node 节点的命名空间中：vethXXXX
- 一端在容器网络命名空间中：eth0

只要连接到 同一 网桥，相互之间就能通信。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgX5LUDnbvt2f4XY0u4gOyJmW9BBxgZriarNKa36ibTEMvbUWEyzJXJ5gQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616165429183

2：不同节点的 pod 之间通信，两个节点之间需要连接网桥

连接网桥的方式有：

- overlay
- underlay 网络
- 常规的三层路由

> 跨节点网桥必须使用 非重叠地址段， 保证不同的 pod 有不同的 IP

以下节点需要配置路由 或者 连接到相同网关【中间无路由】，否则会因为 pod IP 是局域私有的，会丢包。

> 使用 SDN （软件定义网络） 技术，可以忽略底层网络拓扑，所有节点就像连接到同一个网关；
>
> pod 发出的报文会被封装，到达其它节点，会被解封装；

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgPjQkvTveh291LntOGHkevHr2mhzibtE3w2cibQnyIPiawU1MdkrWcGicsg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 14.2 服务

1：和 Service 相关的操作 都是由 每个 节点上的 kube-proxy 进行处理的。

2：每个 service 都有一个稳定的 IP 地址 和端口 对；【针对多端口 Service 有多个 IP：端口对】

> 单独的服务 IP 无任何意义，不能 ping

3：创建一个 Service，会发生以下事件链：

- 当创建一个服务时， 虚拟 IP 地址会分配给它

- API 服务器 会通知所有 节点上的 kube-proxy, 有个新的服务创建了， 修改 iptables, 让服务在字节所在的节点可寻址；【修改目的地址 ，重定向到其中的一个 pod】

- kube - proxy 还会监听 Endpoint 对象的更改

- - Endpoint 保存了所有 pod 的 ip:端口 对 【每次创建 或 删除 pod，都会影响 Endpoint】

下图中 pod A 请求 pod B 时， 会随机选择一个 pod（假设 pod B2 选中）， 根据 iptables 规则，目标地址被修改为 pod B2 的 ip:端口

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgK5LadFC3jtG2aZKOeJVKAwTcxf2XBLae7cJaZSEWNF78aHbBW8GtFw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 14.3 高可用集群

1：应用高可用：分布式集群；

2：主节点高可用：部署成集群

- API server 通过负载均衡器选择，同时并行工作；
- 控制器和调度器：领导者选举一个运行；

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhggib0qephEXYXO449BaiaDvl3glnFbCITibDaA4OEWu4Z6OB4ITOENMHKQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616172020019

## 15.**kubernetes API 服务器的安全防护**

### 15.1**认证机制**

1：pod 与 API 服务器进行通信时，会经过 API 服务器的 **认证插件**

该插件会根据 证书+token 或 HTTP 用户验证 提取 客户端的 用户名、用户 ID 和 组信息。

2：连接 API 服务器有两种客户端：

- 真实的用户；
- 运行在 pod 中的应用；

3：系统内置的组 有特殊含义

- `system:unauthenticated` 组用于所有认证插件都不会认证客户端身份的 请求。
- `system:authenticated` 组会自动分配给一个成功通过认证的用户。
- `system:serviceaccounts` 组包含所有在系统中的 ServiceAccount 。
- `system:serviceaccounts:<namespace>`组包含了所有在特定命名空间中的 ServiceAccount。

#### 15.1.1 **ServiceAccount**

1：ServiceAccount 也是一种资源(简称 sa), 会为每个命名空间自动创建一个 默认的 ServiceAccount

> 主要用于 客户端身份认证，省去 手动传 token

2：pod 只能使用 同一命名空间的 ServiceAccount

- 可单独使用一个 ServiceAccount
- 也可和同命名空间的 其它 pod 共用 ServiceAccount

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgia0cjWr6R2WPEGJORmAfwGdxttG7Qnm4ib06LY4NJU1cib34btUlQz6JQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3：可在 pod 中显示指定使用的 ServiceAccount， 否则使用该命名 空间下 默认的 sa.

> 添加注解 `kubernetes.io/enforce-mountable-secrets= "true"`， 可强制 pod 只允许挂载 ServiceAccount 中的秘钥

4：ServiceAccount 的镜像拉取秘钥， 用于拉取容器镜像的凭证。

> 注意：所有使用 该 ServiceAccount 的 pod 都会拥有这个秘钥，而不必每个单独添加。

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-service-account
imagePullSecrets:
  # 所有使用 该 ServiceAccount的 pod 都会拥有这个秘钥
- name: my-dockerhub-secret
```

5：在 pod 中使用 serviceAccount

```
apiVersion: v1
kind: Pod
metadata:
  name: curl-custom-sa
spec:
  # 若不显示声明，则使用 默认的 serviceAccount
  serviceAccountName: foo
  containers:
  - name: main
    image: tutum/curl
    command: ["sleep", "9999999"]
  - name: ambassador
    image: luksa/kubectl-proxy:1.6.2
```

> ServiceAccount 账户 产生的 Token 在 `/var/run/secrets/kubernetes.io/serviceaccount/` 目录。
>
> 未使用 RBAC 授权插件， **==默认的 serviceaccount 和 显示创建的 serviceaccount 都允许 执行任何操作。==**

#### 15.1.2 RBAC

1：RBAC：基于 角色的权限访问控制插件；

> 开启 RBAC 后，未经授权的 serviceAccount 或 默认的 serviceAccount 禁止查看集群状态。

2：RBAC 用来设置一个用户、ServiceAccount 或者一组用户，控制该角色在特定资源上 能否执行动作的权限，比如以下动作：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgUfMMwbjSCfqngYKotOgAUTVyQickmv1ia4R5hqzHrSlcylg5jtvjAA8g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> use 动词用于 PodSecurityPolicy 资源。

3：RBAC 授权规则是通过四种资源来进行配置的， 它们可以分为两个组

- Role( 角色）和 ClusterRole (集群角色）， 它们指定了在资源上可以执行哪些动词。

- - **命名空间** 范围内的资源

- RoleBinding (角色绑定） 和 ClusterRoleBinding (集群角色绑定）， 它们将上述角色绑定到特定的用户、 组或 ServiceAccounts 上。

- - **集群级别** 的资源

> 角色定义了可以做什么操作，而绑定定义了谁可以做这些操作

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgCnINdiboZNZnejZfgNYngkRy0TAuZI1S7bJZofH1AqHxMfJVZJDEl9g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

需要注意的是，RoleBinding 也可以引用 **不在命名空间中的集群角色。**

> Role 和 RoleBinding 都在命名空间中， ClusterRole 和 ClusterRoleBinding 不在命名空间中。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgaib2NnnstrtzOxWtEv0ZI6EkZwGOrqYNFicZKVViccB58JBWicicoLnSarA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

4：启用 RBAC，一旦开启，禁止未授权的 serviceAccount 查看/修改 资源。

> `kubectl delete clusterrolebinding permissive-binding`: 重新启用 RBAC

5：Role: 哪些操作可以在 哪些资源上执行。【授权某些资源的 操作权限。】

```
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  # Role 所在的命名空间，若没指定，默认是当前命名空间
  namespace: foo
  name: service-reader
rules:
# service 是核心 apiGroup 资源，无 apiGroup 就是 ""
- apiGroups: [""]
  # 允许执行的 操作
  verbs: ["get", "list"]

  # 该规则和服务相关 (复数)
  resources: ["services"]
```

> 在本例中，你允许访 问所有服 务资原，但是也可以通过额外的 `resourceNames` 字段指定服务实例的名称来限制对服务实例的访问。

Role 中的声明，表明 能 get 和 list 中的 service 资源。【默认是禁止的】

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgcuR5VoVc6lmTfFBvicia9D3XVBArrxme4u1jRJnQW2liaXVFwZqoVS95A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210616143601754

6：rolebinding， 将 Role 中允许的权限 绑定到指定的账户上。

```
kubectl create rolebinding <rolebinding-name> --role=<role-name> serviceaccount=foo:deault -n foo: 创建 rolebinding 资源，绑定到 foo 命名空间中， default  的 ServiceAccount 账号上
```

> --user ：绑定到用户
>
> --group: 绑定 Role 到组

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgAnicjichBpmqAN4sA8TSwYG7z9rCXzZCJCRY9icTibtN4XNd91u7u7EZZg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

RoleBinding 也能绑定到其它 命名空间 的 serviceAccount。总之，绑定到哪个账户，就赋予哪个账户权限。

如下，RoleBinding 绑定到 bar 命名空间的 sa 账户时，也能查看 services。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgez62l3eSsBpIBw205yqzHhKibibn9wdcMMuPYqHmZ1TxFicBibkFLvtJfA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

7：ClusterRole 和 ClusterRoleBinding 属于集群级别的资源管理

以下两种情况，需要使用 集群级别的授权：

- 1：当允许跨命名空间访问资源时， 每次扩展，都需要为新的命名空间 添加 Role 和 RoleBinding 【命名空间与命名空间之间会相互绑定】

- 2：有些资源不在命名空间中: Node、PersistentVolume、Namespace 等

- - 非资源的 URL 路径 (`/healthz`)

> 默认的 ClusterRole 都以 `system:` 为前缀

注意：sa 账户 可以通过 RoleBinding 绑定 到 ClusterRole 上，但是**无法 访问集群级别的资源**。

> 只能访问指定命名空间中的资源。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgaQXIQEV9HKDl22uA9Gsl1zK4I0gpc7mySlPzciaiaGcTT6140ibH3Nicqw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

只有 通过 ClusterRoleBinding 才能访问集群级别的资源。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgx0XAXW3lh3KouNkPEbicmibcOr5J2SAoX5vjSZkf3Eau4SnEQf4SabXw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

非资源型的 URL 在 ClusterRole 中 使用 的是 URL 路径 而非资源。

通过 ClusterRoleBinding 绑定后，也能访问。

以下是一个 ClusterRole 示例。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgCMCAj6MOuKHQiaGvFMuNsbv5ibm3OBA2GqibttjM7phffFvgRy3TgWNXA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

8：使用 ClusterRole 授权访问指定命名空间中的资源。

- ClusterRoleBinding --- 绑定 --- ClusterRole: 可以发查看所有命名空间、集群中的资源；

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgB4SIdna00L5s6MYNez8Lh3QRVibdkTzd5BsnibibOmB9BU49eTeOjnNfg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

- RoleBinding --- 绑定 --- ClusterRole: 只能查看 绑定的 RoleBinding 命名空间中的资源

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgutA2diaplewD9IAgleO8iaWHkrxENCib1aT9j57DF4OOoWPeBvwiadNUicw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

9：Role 和 Binding 的组合，特别 注意 倒数第二条

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgOBPiaQ18pJBkF7QiaoNgW7bYnXRZnYIZU6ib2M9XkZNUm6gYviaxiaVIicUw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 15.2 **节点和网络安全**

1：pod 可以访问 宿主 node 的资源。

#### 15.2.1 使用节点的 Linux 命名空间

**使用节点的网络命名空间和端口**

1：默认 每个 pod 拥有自己的 IP 和 端口空间。

设置 `pod.spec.hostNetwork:true` , pod 使用节点的网络接口，而无自己的 IP 地址。

端口也会绑定到主节点

2：如下， pod B 和 节点公用 ip: 端口空间，

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgfHyicV6BW8EcTFyPRLdY98IAJPrJbC9DjmrHfrfalgMsP1cGWyk4QRw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3：一般 master 节点上 部署的 node 经常会开启 `pod.spec.hostNetwork:true`

**仅绑定节点的端口**

1：仅仅绑定节点的端口，而让 pod 继续有自己的网络命名空间。

2：设置 `pod.spec.hostPort:true`

使用 hostPort 和 NodePort 有两点不同：

- 到达节点端口的连接

- - hostPort: 会直接转发 到 pod 对应的端口上；
  - NodePort: 随机选择一个 pod

- 作用范围

- - hostPort: 仅有 运行了 hostPort 配置的 pod 才会绑定对应的端口，未运行则不绑定(Node 3)
  - NodePort: 集群中的所有节点 都会绑定。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgYxtbw2I8eogWjg81kicWAickcAUpS7OibD6eBBGwr6og2K15REbDviawtQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3：若采用 hostPort 公用主机端口，则在一个节点上，一个端口只允许绑定一次。

若在调度的时候，多个 pod 需要绑定到同一端口， 则控制器会分散到不同的 节点。

> 若无足够多的节点，pod 会保持 pending 状态

4：同样的，设置 `hostPID`: 可共用 Node 的 进程树空间。

`hostIPC`: 可通过 IPC 进行进程间通信

#### 15.2.2 配置节点安全上下文

1: 配置节点的安全上下文，可通过 `securityContext` 设置。

配置安全上下文可以允许你做很多事 ：

- 指定容器中运行进程的用户（用户 ID ）。

- 阻止容器使用 root 用户运行（容器的默认运行用户通常在其镜像中指定，所以可能需要阻止容器 以 root 用户运行〉。

- 使用特权模式运行容器，使其对宿主节点的内核具有完全的访问权限 。

- 与以上相反，通过添加或禁用内核功能，配置细粒度的内核访问权限。

- - 修改系统时间

- 设置 SELinux （Security Enhaced Linux ， 安全增强型 Linux ）边项，加强对容器的限制。

- 阻止进程写入容器的根文件系统

- 同 pod 多容器下，多用户共享存储卷。

- - fsGroup 属性， 在创建文件时起作用
  - supplementalGroups 属性定义了某个用户所关联的额外的用户组。

#### 15.2.3 PodSecurityPolicy

1: PodSecurityPolicy 是集群级别的资源， 限制用户 在 pod 中能否使用安全相关的特性。

2：PodSecurityPolicy 可以做的事项：

- 是否允许 pod 使用宿主节点的 PID、 IPC、 网络命名空间
- pod 允许绑定的宿主节点端口
- 容器运行时允许使用的用户 ID
- 是否允许拥有特权模式容器的 pod
- 允许添加哪些内核功能， 默认添加哪些内核功能， 总是禁用哪些内核功能
- 允许容器使用哪些 SELinux 选项
- 容器是否允许使用可写的根文件系统
- 允许容器在哪些文件系统组下运行
- 允许 pod 使用哪些类型的存储卷

#### 15.2.4 NetworkPolicy

1: NetworkPolicy 用于限制 pod 与 pod 之间的通信。网络 **隔离** 组件。

- ingress: 允许访问这些 pod 的源地址；【入向规则，**能被** 哪些 pod/命名空间下的 pod/IP 段 访问】
- egress: 这些 pod 可以访问的 目标地址；【出向规则， 该 pod **只能与 哪些 pod 通信**】

2：可选定 pod 的范围：

- 标签选择器 选出的 pod；
- 一个 namespace 中的所有 pod;
- 无类别域间路由 (Classes Inter-Domain Routing, CIDR) 指定的 IP 段；

3：由于 NetworkPolicy 是网络隔离组件， 该命名空间下，pod 无法访问。

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  # 空的标签选择器 匹配 命名空间内的 所有pod
  podSelector:
```

4：即使其他 pod 通过 service 访问，依然会被 NetworkPolicy 隔离。

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: postgres-netpolicy
spec:
  podSelector:
    # 标签为 app=database 的 pod 设置了访问权限
    matchLabels:
      app: database
  ingress:
  - from:
    # 只对 app=webserver 的 pod 开放了 5432 端口
    - podSelector:
        matchLabels:
          app: webserver
    ports:
    - port: 5432
```

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgntibSNw1wic0gGptHdR1KpIzePGKuApLQBgfXZgZl4oxUXypmg6vD8cw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

5：在不同的命名空间之间隔离。

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: shoppingcart-netpolicy
spec:
  podSelector:
    # 限定 pod 的范围
    matchLabels:
      app: shopping-cart
  ingress:
  - from:
    # 只对以下命名空间 开放了 80 端口
    - namespaceSelector:
        matchLabels:
          tenant: manning
    ports:
    - port: 80
```

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgkgWVtX0uZHW9mkGPSgzafJlw9IESXOo5iaVvkHInD602DK3uXqSz1Xw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

6: CIDR 表示法：设置一个 IP 段

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ipblock-netpolicy
spec:
  podSelector:
    matchLabels:
      app: shopping-cart
  ingress:
  - from:
    # 限定 只有  192.168.1.1 ~ 192.168.1.255 的 pod 可以访问
    - ipBlock:
        cidr: 192.168.1.0/24
```

7：出向规则 egress: 指定 pod 只能对外 访问 哪些 pod。

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: egress-net-policy
spec:
  podSelector:
    matchLabels:
      app: webserver
  egress:
  - to:
    # webserver 只能访问 带有 app: database 标签的 pod
    - podSelector:
        matchLabels:
          app: database
    ports:
    - port: 5432
```

## 16.**资源管理**

1：配置 pod 资源的 预期使用量和最大使用量，可保证 pod 公平使用 集群内的资源。

### 16.1 容器申请资源

1：`requests` 中可申请 CPU 和内存使用量。

- 若不申请 CPU，极端情形，会被挂起

```
containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      requests:
        # 200 毫核，单核的 1/5，1200m，则是 1.2 个核【多核CPU】
        cpu: 200m
        memory: 10Mi
```

> top 中 CPU 的使用量率 占所有核的百分比

2：调度器在调度 pod 时，判断 pod 是否能调度到该节点的依据是根据 资源的**申请量**之和，而非资源的实际使用量。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgE1Ootiawvk5rjUFPT14OvPfDXH5vGoPenQsVpE3H0kSZKrpMf2zrW7Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3：调度器利用 pod requests 为其选择最佳节点 有两种策略：

- LeastRequestedPriority: pod 调度到 资源使用量少的节点上
- MostRequestedPriority: pod 调度到资源使用量高的节点上 【按节点付费时，可选择】

4：当没有合适的 节点分配 给待调度的 pod 时， pod 状态会一直卡在 Pending 状态。

> 但此时并未放弃，一旦有 pod 删除，调度器将收到通知， 有可能重新 将 pod 部署在上面。

5：CPU requests 会影响时间片的分配。

假设一个节点上运行两个 pod， 一个 pod 请求 200 毫核 CPU，另一个是 1000 毫核 CPU，则时间片将按照 1：5 分配。

- 若一个 pod 空闲，另一个 pod 跑满，则另一个 pod 将占用 全部 CPU；
- 当空闲的 pod 重新运转， 另一个 pod 的 CPU 会立刻压缩到之前的比例 【动态伸缩，提高 CPU 使用率】

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgROYKS8TW6MibFicHT04FCyUKwWlET5AZibhRnia6xgdicA4WvCYsLgmMMGA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

6：允许自定义资源，例如 GPU。

### 16.2 限制容器资源

1：限制容器可以消耗资源的最大量；

> 特别是内存，不可压缩资源，会影响后来 pod 的内存分配。

2：未设置 requests, 则将指定与资源 limits 相同的值.

```
containers:
  - image: busybox
    command: ["dd", "if=/dev/zero", "of=/dev/null"]
    name: main
    resources:
      limits:
        cpu: 1
        memory: 20Mi
```

3: 节点中 所有 pod 的 limits 总量允许超过 节点总量 100%

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgRBaKzJibg1SicAJsRp5ULDcmfjAarVicVv3gyxKKqk7e5xCmxaibHnT4AA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

4: 若内存超过物理上限，容器会被 OOM killed

若 Pod 的重启策略 `restartPolicy` 设置为 Always 或 OnFailuer, 容器会立刻重启。

若 pod 连续重启 `CrashLoopBackOff`, 下次重启时间呈 指数级避退: 10、20、40、80、160 秒，最终收敛到 300s。一旦达到 300s 间隔，后续将以 5 分钟为间隔 无限重启。

5：注意：在容器内看的内存(`free -g`)始终是**节点的内存**，而非容器的内存

无论有没有配置 CPU limits ， 容器内也会看到**节点所有的 CPU**。将 CPU 限额配置为 1，并不会神奇地只为容器暴露一个核。CPU limits 做的只是限制容器使用的 CPU 时间 。

> 因此如果一个拥有 i 核 CPU 限额的容器运行在 64 核 CPU 上，只能获得 1/64 的全部 CPU 时间 。而且即使限额设置为 1 核， 容器进程也不会只运行在一个核上，不同时刻，代码还是**会在多个核上**执行 。

一些程序通过查询系统 CPU 核数来决定启动工作线程的数量。同样在开发环境的笔记本电脑上运行良好，但是部署在拥有更多数量 CPU 的节点上，程序将快速启动大量线程，**所有线程都会争夺（可能极其）有限的 CPU 时间**。同时每个线程通常都需要额外的内存资源，导致应用的内存用量急剧增加 。

### 16.3 pod QoS 等级

1：当资源 limits 超出节点上限时， 可**指定哪些 pod 从优先级更高**，当资源不足时，首先 杀掉的是 低优先级的 pod.

可给 pod 分配三种 QoS 等级：

- BestEffort （优先级最低）

- - 没有设置任何 requests 和 limits 的 pod；
  - 无任何资源保证；
  - 最坏情况下，分不到 CPU 时间片，同时为其它 pod 释放内存时，第一批被杀死；
  - 由于无 limits，内存充足时，可使用 任意多的 内存；

- Burstable

- - 容器的 requests 和 limits 不等
  - 只定义了 requests 的 pod
  - 部分容器 requests 和 limits 相等，部分不等

> Burstable 获得它们所申请的等额资源，并可以使用额外的资源（不超过 imits ） 。

- Guaranteed （优先级最高）

- - requests 和 limits 相等的 pod
  - CPU 和 内存都要设置 requests 和 limits
  - **每个**容器都要设置 资源量 【注意是每个都要设置】
  - 每个容器的每种资源的 requests 和 limits 必须相等

> 这些 pod 的 容器可以使用 它所申请的等额资源，但是无法消耗更多的资源（因为它们的 limits 和 requests 相等） 。

若没显示设置 requests, 则默认和 Limits 相同。所以设置了 limits 的 pod，QoS 就是 Guaranteed

三种等级的分类：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgiczIKrQQyvWJD7TgLUTV53aibkSew4aicib4iaYuzhVhpcGy3B7YvmQANRg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

单容器 pod 的 QoS 等级

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgmZ5DjC8bEQXcXOuoPdQriaxFASdTfbDvqh4iavagefddDK8iccibbOLK6Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

对千**多容器 pod**, 如果所有的容器的 QoS 等级相同， 那么这个等级就是 pod 的 QoS 等级。如果至少有一个容器的 QoS 等级与其他不同，无论这个容器是什么等级，这个 pod 的 QoS 等级都是 Burstable 等级。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgtQnrjtxxBZr6icJv4WX802icibZFBgyTgE0pOmZkSXP9CZVVQapENv5MA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

2：内存不足时， BestEffort 等级的 pod 首先被杀掉。

其次是 Burstable

最后 是 Guaranteed，只有系统进程需要内存时， 才会被杀掉。

对于 QoS 等级相同的 pod，最先被杀掉的是 **实际内存占内存申请量比例更高的 pod.**

> 如下图， pod B 虽然 requests 比 pod C 少，但使用 率高达 90%, 所以先杀掉的是 pod B.

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgNPQmKdeIwrZomGT37qw5cRzvRk0yEvJejr0Cpkib8o3aQtj1MHcGyWQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 16.4 LimitRange

1：LimitRange 给命名空间中的 pod 设置默认的 requests 和 limits

LimitRange 资源中的 limit 应用于同一 个命名空间中每个独立的 pod、 容器，或者其他类型的对象。

> 当为显示指定 资源 requests 时，设置默认值。

它并不会限制这个命名空间中所有 pod 可用资源的总量， 总量是通过 ResourceQuota 对象指定的，

2：LimitRange 资源被 LimitRanger 准入插件控制。

LimitRange 示例如下：

```
apiVersion: v1
kind: LimitRange
metadata:
  name: example
spec:
  limits:
  # 整个 pod 的资源限制
  - type: Pod
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi

  # 单个容器的资源限制
  - type: Container
    defaultRequest:
      cpu: 100m
      memory: 10Mi

    # limits 默认值
    default:
      cpu: 200m
      memory: 100Mi
    min:
      cpu: 50m
      memory: 5Mi
    max:
      cpu: 1
      memory: 1Gi
    # 每种资源 requests / limits 的最大比值
    maxLimitRequestRatio:
      cpu: 4
      memory: 10

  # PVC 限制
  - type: PersistentVolumeClaim
    min:
      storage: 1Gi
    max:
      storage: 10Gi
```

3：LimitRange 中配置的 limits**只能应用于单独的 pod 或容器** 。用户仍然可以创建大量的 pod 吃掉集群所有可用资源。

### 16.5 ResourceQuota

1: 限制命名空间中的 可用资源总量。

> LimitRange 是针对单个的实体
>
> ResourceQuota 是针对命名空间下的总量

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgG6SgtJS3NXz1NlZLeiakWazl7GeueO9k3kGSLDDib92FzEUF6tGc1XJg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

2：对象个数配额目前可以为以下对象配置 ：• pod • ReplicationController • Secret • ConfigMap • Persistent Volume Claim • Service （通 用） ， 以 及两种特定类型的 Service，比如 LoadBalancer Service (services . loadbalancers ）和 NodePort Service ( services.nodeports)

3: 可通过 以下四种类型 控制 Quota 作用的范围

- BestEffort : Quota 是否应用于 BestEffort QoS 等级的 pod

注意：BestEffort **只能限制 pod 的个数**。不能限制 CPU/内存的 requests 和 limits

下面的都能限制：

- NotBestEffort ：Quota 是否应用于 Burstable 和 Guaranteed QoS 等级的 pod
- Termination：设置了 activeDeadlineSeconds 的 pod （pod 被标记为 Failed 到 真正停止前还能运行的事件）
- NotTerminating：未 设置 activeDeadlineSeconds 的 pod

```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-notterminating-pods
spec:
  # 这个 Quota 值作用于 拥有 BestEffort QoS, 以及未设置 最大可存活时间的 pod
  scopes:
  - BestEffort
  - NotTerminating

  # 这种的pod 只允许存在 4 个
  hard:
    pods: 4
```

### 16.6 监控 pod 资源使用量

1：集群中，每个 Node 的 Kubelet 中 cAdvisor 代理负责收集 本节点数据；

最终汇总到 Heapster 上（也运行在一个 pod）

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgjQuYpa3V16OX4eqfd98OEha3T1MYDG0I6ZHBFpcbSlpUXpNWT6uX3A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

2: 存储历史监控数据，可用 InfluxDB

可视化套件使用 Grafana



## 17.**自动横向伸缩**

1：根据 CPU 使用率或其它度量指标，自动横向扩缩容。

- pod 的横向扩缩容；
- node 的横向扩缩容；

### 17.1 pod 的横向伸缩

1：横向 pod 自动伸缩是指由控制器管理的 **pod 副本数量** 的自动伸缩。它由 Horizontal 控制器执行， 我们通过创建 一个**HorizontalpodAutoscaler** (HPA)资源来启用和配置 Horizontal 控制器。

该控制器**周期性检查 pod 度量**， 计算满足 HPA 资源所配置的目标数值所需的副本数量， 进而调整目标资源（如 Deployment、ReplicaSet、 ReplicationController、 StateflSet 等）的 replicas 字段。

> AutoScale 能避免 抖动情况下的 自动扩缩容。

2: Pod 的度量数据 通过 kubelet 上的 cAdvisor agent 采集，并汇总到 Heapster.

流程如下：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg5wIXKfxemPibH4GFQe1ziasJscnYT7D0VhEpEI4GP9jtZE68jYZ8s3fw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3：HPA 获得 度量值后（可以有多个度量），取每个度量下 计算的副本数 **最大值**作为 最终 pod 数量

度量的目标值 是一个平均值。

例如，同时按照 CPU 使用率和 QPS 指标计算。

```
计算公式：
目标 Replicas = (所有Pod 度量值总和) / 目标度量值 # 向上取整
```

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgYkXIpkoZX0Yrk2nEjtnLibcqzyTbgc9N7I4RAicPlZjGicPTOgvN0m6EA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

4: HPA 控制器 通过 Scale 子资源 修改 部署(Deployment、ReplicaSet、ReplicationController、StatefulSet) 的 `replicas` 字段，由部署相关的 控制器 实现 pod 的增减。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgImDXgw2AibSG1Ru3E2A5QE6lrOqiaNwk6ibPFXBKYnDmJ9OyFgfu9ZXSw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

5: 拿 CPU 使用率举例， 已经使用了 60% 的 CPU 是相对于 pod 请求量的 CPU 【并非节点 CPU 的 60% 或者 资源上限 Quota 的 60%】

> pod 请求量的基准可在 pod 模板的 requests 或者 LimitRange 间接设置

- 容器的 CPU 使用率是它实际的 CPU 使用除以它的 CPU 请求 。
- 请求量是 **==最低标准， 相当于有下限，没上限。==**

例如：

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
        resources:
          requests:
            # 100 毫核，1/10 的CPU
            cpu: 100m
```

6: 创建一个 HPA 对象，并指向 该 Deployment。

```
kubactl autoscale deployment <deployment-name> --cpu-percent=30 -min=1 --max=5: 对指定 deployment 自动伸缩pod，使CPU使用量达到 30%, 最少1个 pod，最多5个
```

使用 yaml 文件定义：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgBLgtqH14nUjbwaFpyxvP3N6Mniadq6qYUlrqTYNpZNa5kjaP5lZaicrw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> 注意：HPA 的目标是 Deployment， 在副本数量 replicas 更新后， Deployment 会给每个应用版本 创建一个 新的 ReplicaSet.

7：基于内存的自动伸缩比基于 CPU 的 更复杂， 因为无法强制 Pod 释放内存，除非杀死并重启应用。

> k8s 1.8 开始支持基于内存的自动伸缩

8：HPA 不支持 `minReplicas:0`, 即不允许缩容到 0 个副本， 总得保持一个 空载(idling)。

#### 17.1.1 **扩缩容速度**

1：HPA 单次扩容操作，至多使副本数翻倍， 如果 副本数只有 1 或 2，则最多扩容到 4 个副本。

两次扩容之间也有时间限制，只有 当 3 分钟 内没有任何伸缩操作，才会继续触发扩容。

缩容频率更低，需要 5 分钟。

#### **17.1.2 Resource 度量类型**

1：基于 **Pod** 的度量类型，例如 pod 的 QPS， 运行在 Pod 中消息队列的消息数量。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgoH1x7V1g7YQqhdiaoGp9FfnIyic1Fb6fTHOztHNPwlPKPbib9KAUZ0wng/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> 注意，此时会 从 **所有** Pod 中取度量值后，看平均值 与 目标 度量对比。

2：基于 Object 的度量类型， 间接基于另一个集群对象，例如 Ingress 来伸缩 pod

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgDoPATFG8g8EsXxSHQyNTsYvo4mryook8Rc5Jn8Osroeaoe2Ajdbpfw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210615171232588

> 注意， HPA 只会从这个 特定的 对象中 获取**单个的**度量数据。【并非 基于 Pod 的从所有 对象中获取】

3：一个好的度量类型，应该是 增加副本数时，可以使度量平均值 下降。例如 CPU、QPS

> 内存占用 并非好的度量类型。

#### **17.1.3 自动配置资源请求**

1：如果新创建的 pod 的容器没有明确设置 CPU 与内存请求， 该特性即会代为设置。

这一特性由一 个叫作**lnitialResources**的准入控制(AdmissionControl)插件提供 。当 一个没有资源请求的 pod 被 创建时， 该插件 会**根据 pod 容器的历史资源使用数据**（随容器镜像、tag 而变）来设置资源请求。

> 如果一个容器总是内存不足， 下次创建一 个包含该容器镜像的 pod 的时候， 它的内存 资源请求就会被自动调高了

### 17.2 集群节点的横向伸缩

1：集群节点的横向伸缩， 解决所有节点都满了，放不下 Pod。

2：当在云服务厂商上运行集群时， Cluster Autoscaler 负责请求/释放 节点。

- 节点资源不足， 申请节点 ；

- - 会先检查新节点有没有可能容纳这个 pod，若无法容纳，则不用启动该 node
  - 当有不同规格的节点类型时，会挑选一个最合适的节点（最差是随机选择一个）

- 节点长时间使用率底下，下线节点；

下图显示，当没有节点分配 pod 时，Cluster Autoscaler 会申请一个新的节点。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgOibNLr5DPFa9T7ZpArRpGZBkd9b9ibUzMalic4IBVE1QjCIrDGIYjjK4g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3：归还节点：Cluster Autoscale 通过监控所有节点请求的 CPU 与 内存实现。

> 若某个 node 上，所有 pod 请求的 CPU、内存均达不到 50%， 该节点认定为不再需要。

以下几种情形，节点不会被归还：【如果回收，会导致服务中断】

- 1：有系统 pod 在运行（DameonSet 部署的服务）；
- 2：非托管 pod
- 3：本地存储的 pod

只有当节点上运行的所有 pod 能被重新调度到其它节点时， Cluster Autoscale 才会触发缩容。

缩容时，该节点 首先会被标记为不可调度 【拒绝 pod 重新调度回来】，然后 该节点上的 pod 会疏散到其它节点。

4：节点下线时，可以通过 `podDisruptionBudget` 资源的指定 下线等操作时需要保待的最少 pod 数量【逐步迁移】，避免服务受影响。

```
# 标签为 app=kubia 的 pod，至少要保证3个在运行。
kubectl create pd kubia-pdb --selector=app=kubia --min-available=3
```

## 18.**高级调度**

### 18.1 污点和容忍度

1：Pod 可通过 `nodeSelector` 和 节点亲缘性 选择在 哪些节点上部署；

2：从 node 侧，也能 限制 哪些 pod 可以部署在上面；

**污点（Taints）：** 节点添加污点，拒绝 pod 在该节点上部署；

> 污点在 master-node 上用的比较多，可以限制普通 pod 部署，只有系统 pod 才能部署在上面。

除非 pod 能容忍(Toleration) 这个污点, 否则不能部署在该 node.

> 某些硬件只能运行特殊的 pod， 可采用这种形式

3: 通过 kubeadm 部署的集群上的 主节点， 查看其 污点：

污点的格式

```
<key>=<value>:<effect>

# 若value 为空
<key>:<effect>
```

每个污点的效果(effect) 包含三种：

- NoSchedule 表示如果 pod 没有容忍这些污点， pod 则不能被**调度**到包含 这些污点的节点上。
- PreferNoSchedule 是 NoSchedule 的一个宽松的版本， 表示**尽量阻**止 pod 被调度到这个节点上， 但是如果没有其他节点可以调度， pod 依然会被调度到这个节点上。
- NoExecute 不同于 NoSchedule 以及 PreferNoSchedule, 后两者只在调度期间起作用， 而 NoExecute 也会**影响正在节点上**运行着的 pod。如果在一个节点上添加了 NoExecute 污点， 那些在该节点上运行着的 pod, 如果没有容忍这个 NoExecute 污点， 将会从这个节点去除。

4：查看已有的污点：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhglkJCdpuvft40e9UgEnL9tibVIJBaDqWbZodz5ib8yvnzVL26uiaJtOrZw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

如下，

- System pod 可以同时部署在 主节点和普通节点；
- 但未添加容忍度(默认) 的 pod 只能部署在 普通节点；

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgPVpqHceUed2Fia5GPB1yGzJpOOdaVYtP8w8U70JQibe88GGibDnibalzUg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

5：节点上添加自定义污点：

```
kubectl taint node node1.k8s node-type=production:NoSchedule: 在节点上添加 key = node-type, value = production 的污点
```

6：给 pod 添加容忍度示例：

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: prod
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: prod
    spec:
      containers:
      - args:
        - sleep
        - "99999"
        image: busybox
        name: main
      tolerations:
        # 和 paint 配对使用
      - key: node-type
        # 使用 Equal 或 Exists
        operator: Equal
        value: production
        effect: NoSchedule
```

7: 配置 node 不可用后，pod 最长等待时间：

> 可用于网络抖动的情形，若 超时后，pod 将被调度到其它 node.

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgV2ia540QDX4xic57BAb13ZOWhbkIicWJPOZibUnQGtJbGz7S8Uvk43NbXA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 18.2 节点亲缘性

1：亲缘性(node affinity): 允许你通知 Kubemetes 将 pod 只调度到**某个几点子集**上面。

通过给 **节点** 打标签，然后 在 pod 中使用 `pod.spec.affinity.nodeaffinity` 强制选择(require) 或者 推荐选择(prefer) 节点。

一个典型的 gke 标准节点，常常会打以下三种标签：

- `failure-domain.beta.kubernetes.io/region` 表示该节点所在的地 理地域 。
- `failure-domain.beta.kubernetes.io/zone` 表示该节点所在的可用 性区域（ availability zone ） 。
- `kubernetes.io/hostname` 很显然是该节点的主机名 。

> 已知的，租户常常会选择 机型、区、地理位置来确保服务的可用和高可用，通过打标签，相当于划分了 机群。

2：节点亲缘性 比 `nodeSelector` 表达能力更强

- `requiredDuringScheduling...`: 调度的时候，强制要求。若找不到合适的 node, 则 pod 无法部署

- `preferredDuringScheduling...`: 调度的时候，推荐

- - 通常和 权重 weight 搭配使用，用于控制优先级；

- `...IgnoredDuringExecution`: 忽略已经在 node 上运行的 pod，否则会把不符合条件的 pod 踢出

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 5
  template:
    metadata:
      labels:
        app: frontend
    spec:
      affinity:
        podAffinity:
          # 强制要求，忽略已经执行的pod
          requiredDuringSchedulingIgnoredDuringExecution:
          # 选择节点的范围
          - topologyKey: rack
            labelSelector:
              matchLabels:
                app: backend
      containers:
      - name: main
        image: busybox
        args:
        - sleep
        - "99999"
```

3：节点亲缘性为 requried 的 ,只会在符合条件的 node 中部署：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg3De1UaxPpDkDfdCzibIB8L0ok2WyM5gHSSrjXrvJzILsVicNOzY42iagA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

4：亲缘性为 preferred ， 会优先选择 都符合的，其次是 一个符合的（按 weight 排列），最后是都不符合的 【一般会有一个 pod 部署在上面】

> 因为调度器还会使用其他优先级函数：`Selector SpreadPriority` 函数. 确保了属于同一个 ReplicaSet 或者 Serice 的 pod, 将分散部署在不同节点上，以**避免单个节点失效**导致这个服务也宅机。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgRwS7hntuQj4n4GSw2PgjVNhyJp7wPmE97FIz3JjQ42IV8pictB8uNQQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

```
spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          # 配置第一个权重
          - weight: 80
            preference:
              matchExpressions:
              - key: availability-zone
                operator: In
                values:
                - zone1
          # 配置第2个的权重
          - weight: 20
            preference:
              matchExpressions:
              - key: share-type
                operator: In
                values:
                - dedicated
```

### 18.3 pod 间亲缘性

1: pod 间亲缘性 可以控制 多个 pod 部署在同一节点、同一机架、同一数据中心。

> 通常是 联系比较紧密的 pod 需要部署在一块，降低延时等。

```
kind: Deployment
# ....
spec:
  replicas: 5
  template:
    #....
    spec:
      affinity:
        podAffinity:
          # 强制限定
          requiredDuringSchedulingIgnoredDuringExecution:
          # 节点选择的范围， 含有 label-key=kubernetes.io/hostname 的节点集合
          - topologyKey: kubernetes.io/hostname

           # 依赖的 pod 的标签
            labelSelector:
              matchLabels:
                app: backend
```

2：若被依赖的 pod 被删除了， 重新调度时， 还是会调用到原来的节点（调度器根据被依赖度，有一套反向打分机制）

### 18.4 pod 间非亲缘性

1：有些 pod 部署在一起会影响彼此的性能，需要分开部署

```
spec:
      affinity:
        # 非亲缘性
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          # 在指定的节点集中，决定 pod 不能被调度的范围
          - topologyKey: kubernetes.io/hostname
            # pod 标签
            labelSelector:
              matchLabels:
                app: frontend
```

> preferredDuringSchedulingIgnoredDuringExecution: 可指定软性要求

## 19.**开发应用**

1：典型应用中使用的 k8s 组件如下：

- Manifest

- - 引用两种 Secret
  - 拉取私有镜像
  - pod 中的运行进程 访问其他 pod 或 k8s API 服务器
  - CronJobs
  - DaemonSet: 集群管理员 在每个 pod 上运行的系统服务
  - HorizontalpodAutoscaler: 水平 pod 扩容器
  - Deployment、StatefulSet 对象
  - Jobs
  - pod 模板
  - 每个容器包含 存活探针 和 就绪探针

- 集群外访问服务

- - LoadBalancer
  - NodePort
  - Ingress 资源

- 存储

- - pod 中以 configmap 卷挂载
  - ConfigMap: 初始化环境变量
  - emptyDir 卷
  - gitRepo 卷
  - PVC 卷
  - StorageClass 资源

- 资源管理

- - LimitRange
  - ResourceQuota

- 运行时创建的对象

- - 端点控制器(Endpoint controller) 创建的 Endpoint 对象；
  - Deployment Controller 创建的 ReplicaSet 对象
  - ReplicaSet 创建的 pod 对象

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgoUXLMeOoUHvg9NLejaibeHFKRvRcwV0m4Abficc94fuV1nms5w4r4aJA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 19.1 pod 生命周期

1：一个 pod 可以比作只运行单个应用的虚拟机。

> pod 随时会重启，主机名和 ip 会变化（除非使用 StatefulSet）
>
> 容器重启，会有新的写入层，磁盘数据会丢失。应该使用 pod 级的存储卷，和 pod 同生命周期

2：容器重启， 并不会影响 ReplicaSet 重新调度 pod， RS 只关注 pod 的数量是否符合预期。

3：以固定顺序启动 pod

- 一个 pod 可有任意数量的 initContainer 容器
- init 容器结束后，才会启动主容器
- 有依赖启动顺序的应用，最好是添加 Readliness 探针，特别是 Deployment，避免在滚动升级时，出现错误版本

4：生命周期的钩子

- Post-start: 启动后钩子，容器启动后， 和主进程并行执行；【钩子运行失败或返回非零错误码，会杀死主容器】
- Pre-Stop: 停止前钩子， 容器停止前执行；执行后给容器发送 SIGTERM 【不管执行成功与否，都会使容器终止】

> 容器崩溃不会执行

5：Pod 的关闭

Pod 的关闭是通过 API 服务器 删除 pod 对象来触发的。

6: Kubelet 关闭 pod 时，会给每个容器一定的时间期限进行优雅的终止，这个时间叫做 **终止宽限期**（Termination Grace Period）。

> pod 的 `spec.terminationGracePeriod` 参数，默认 30

- \1. 执行停止前钩子（如果配置了的话）， 然后等待它执行完毕
- \2. 向容器的主进程发送 SIGTERM 信号
- \3. 等待容器优雅地关闭或者等待终止宽限期超时
- \4. 如果容器主进程没有优雅地关闭， 使用 SIGKILL 信号强制终止进程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg9vnQk8sU8XTlTBujTABofebEXY8A7CDYb4rKia7N2pZ1Ob6wUgDm2og/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

7：若是有状态的 Pod 关闭，关闭前，需要将数据迁移到 其它 存活的 pod。

不建议在收到关闭信号的时候，触发数据迁移：

- 容器终止不一定代表整个 Pod 终止了 （会有其它容器）
- 无法保证 迁移流程在进程被杀死前执行完毕；（宽限期不够 或 关闭过程中 pod 发生故障）

若 pod 重启是不会触发迁移流程的。

推荐做法：

- 应用终止前， 创建 Job 资源，运行一个单独的 pod 迁移数据；【前提是创建 Job 不会出故障】
- 创建一个 CronJob, 周期性的 检测是否有孤立数据
- 若使用 StatefulSet, 缩容，会使 PVC 处于孤立状态，数据搁浅。【若扩容永远不发生的时候】， 可在终止前拉起一个数据迁移的 pod

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg0Zu7dCviaIctnz0sMEUosKsXpx5L0ia4LZ2ypUr6BD0OaRRwc9aAt5eg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 19.2 妥善处理 client 请求

1：pod 需要设置就绪探针， 确保服务可用，才将 pod 加入可对外服务的集合中。

> 若不设置就绪探针，则默认 pod 启动后，服务立马可用。

2：k8s API 在收到 关闭 Pod 时，要做两件事：

- 修改 etcd 的状态，
- 删除通知给观察者 Kubelet 和 端点控制器(Endpoint Controller)

如下图所示，这里的问题是 容器停止的事件 和 kube-proxy 修改 iptables 的前后是不确定的。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgodGX7ickypdbCbSe4iaBYyKJZKGJtbTUiaicFH5TqtCOA8VFqfalHDs0bg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)image-20210615141054178

删除 pod 的时序如下：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhge4xYJvqXssJgv7INdJSsUCibQBkC4nia1jC3ZxqibI9pBPyibL3ibNTnJbQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

分两种情形：

- 1：容器先停止， iptables 后被修改，这时候 API 服务器会接着分配请求 【不合理】
- 2：iptables 先修改，容器后停止 【合理】

对于第一种情形，一般是 将容器 延后停止，尽量满足第二种情形。停止时间需要取一个合理的值， 若时间太长，会导致容器无法正常关闭，太短可能无法处理 request。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhg5gwttHkicjwZPklaPkDkM3lcWG7RBWJnZaaZyfHserl9zxek4VviaItg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 19.3 应用在 k8s 中合理管理

1：打包镜像时：包含最小工具集即可，避免更新版本时间过长。

2：合理给镜像打标签

- 若一直是 latest 镜像，则 imagePullPolicy 需要设置为 Always，会有两个问题

- - 1：无法回退到旧镜像；
  - 2：每次创建新 pod，都要去检查 镜像是否被修改了；

3：使用多维度标签

标签可以包含如下的内容 ·资源所属的应用（或者微服务） 的名称 ·应用层级（前端、后端，等等） ·运行环境（开发、测试、预发布、生产，等等） · 版本号 ·发布类型（稳定版 、 金丝雀、蓝绿开发中的绿色或者蓝色，等等） ·租户 （如果你在每个租户中运行不同的 pod 而不是使用命名空间） ．分片（带分片的系统）

> 分组管理资源

4：添加注解

- 包含作者；
- 应用必须的依赖；

5：更完善的进程终止信息

- 将终止消息 写入 `/dev/termination-log` 【可通过 `terminationMessagePath` 修改 】 , 当 `kubectl describe pod` 可看到终止日志

- - 若未设置，容器终止的最后几行日志会当做终止消息

- 将日志打印标准中断， `kubectl logs` 可见

- 或者写到 pod 中 【挂载 pod 级别的卷】， 通过 `kubectl cp` 可拷贝出来；

6：集中式日志记录， 由 ElasticSearch、Logstash 和 Kibana 组成的 ELK 栈。【可通过 Helm 部署】

- FluentD 通过 DaemonSet 部署 Pod 收集日志
- Kibana 可视化 ElasticSearch 的 web 工具， 也是单独作为 Pod 运行

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgYricG4xY0nP4SzVRA6NIEvcTiaKBWOxXIXM3bXv42ibNWKsrzCbNqJXpw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## 20.**k8s 应用扩展**

### 20.1 自定义 API 对象

1：开发者需要向 k8s API 提交 CustomResourceDefinitions (CRD) 对象，即可提交 Json 或 YAML 清单的方式创建新的资源类型。

2：例如创建一个 静态网站，自动拉取 Git， 创建 pod，并通过 Service 对外公开。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgoolL8sDsszaksnHWFYJOMFW5o5euqN3utIUvITyva4YAxtmX5YPDKQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3：先创建 CRD 对象，让 k8s API 服务器识别该类型。

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  # 资源名称
  name: websites.extensions.example.com
spec:
  # Website 这种资源属于哪种命名空间
  scope: Namespaced

  # 定义API 集群和所属版本
  group: extensions.example.com
  version: v1
  names:
    # 缩短资源的名称
    kind: Website
    singular: website
    #  最后 url 的链接
    #  http://localhost:8001/apis/extensions.example.com/v1/websites?watch=true
    plural: websites
```

4：提交自定义资源请求：

```
kind: Website
metadata:
  name: kubia
spec:
  gitRepo: https://github.com/luksa/kubia-website-example.git
```

5: create 上述两种资源后，可用 `kubectl get` 查看资源实例

```
kubectl get websites
```

6: 自定义控制器，通过 HTTP 监听 API 服务器 Add/Delete 接口，创建 Deployment 资源和 Service 资源。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgWadAnuCWibHLGURWDWv4w5FZD7uD3g4H1bpaEyK3zuKsLiaoJEygdeibg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

7: 控制器部署成一个 pod

```
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: website-controller
spec:
  replicas: 1
  template:
    metadata:
      name: website-controller
      labels:
        app: website-controller
    spec:
      serviceAccountName: website-controller
      containers:
      - name: main
        image: luksa/website-controller
      - name: proxy
        image: luksa/kubectl-proxy:1.6.2
```

自定义控制器 先和 proxy 通信，然后由 proxy 连接 k8s API.

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgeviaJsEQ1B7QUmAcSPj203wKVLKp2fc4FA4T3xRNPNwDDsSywbzkdzw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

运行这个控制器 pod，和 API 服务器进行通信， 需要创建 特殊的 ServiceAccount，

若启用了角色访问控制(RBAC), 则需要 clusterrolebinding 到 `clusterrole=cluster-admin`

控制器运行的核心逻辑如下：

```
func main() {
 log.Println("website-controller started.")
 for {
  resp, err := http.Get("http://localhost:8001/apis/extensions.example.com/v1/websites?watch=true")
  if err != nil {
   panic(err)
  }
  defer resp.Body.Close()

  decoder := json.NewDecoder(resp.Body)
  for {
   var event v1.WebsiteWatchEvent
   if err := decoder.Decode(&event); err == io.EOF {
    break
   } else if err != nil {
    log.Fatal(err)
   }

   log.Printf("Received watch event: %s: %s: %s\n", event.Type, event.Object.Metadata.Name, event.Object.Spec.GitRepo)

   if event.Type == "ADDED" {
    createWebsite(event.Object)
   } else if event.Type == "DELETED" {
    deleteWebsite(event.Object)
   }
  }
 }

}

func createWebsite(website v1.Website) {
 createResource(website, "api/v1", "services", "service-template.json")
 createResource(website, "apis/extensions/v1beta1", "deployments", "deployment-template.json")
}

func deleteWebsite(website v1.Website) {
 deleteResource(website, "api/v1", "services", getName(website));
 deleteResource(website, "apis/extensions/v1beta1", "deployments", getName(website));
}

func createResource(webserver v1.Website, apiGroup string, kind string, filename string) {
 log.Printf("Creating %s with name %s in namespace %s", kind, getName(webserver), webserver.Metadata.Namespace)
 templateBytes, err := ioutil.ReadFile(filename)
 if err != nil {
  log.Fatal(err)
 }
 template := strings.Replace(string(templateBytes), "[NAME]", getName(webserver), -1)
 template = strings.Replace(template, "[GIT-REPO]", webserver.Spec.GitRepo, -1)

 resp, err := http.Post(fmt.Sprintf("http://localhost:8001/%s/namespaces/%s/%s/", apiGroup, webserver.Metadata.Namespace, kind), "application/json", strings.NewReader(template))
 if err != nil {
  log.Fatal(err)
 }
 log.Println("response Status:", resp.Status)
}

func deleteResource(webserver v1.Website, apiGroup string, kind string, name string) {
 log.Printf("Deleting %s with name %s in namespace %s", kind, name, webserver.Metadata.Namespace)
 req, err := http.NewRequest(http.MethodDelete, fmt.Sprintf("http://localhost:8001/%s/namespaces/%s/%s/%s", apiGroup, webserver.Metadata.Namespace, kind, name), nil)
 if err != nil {
  log.Fatal(err)
  return
 }
 resp, err := http.DefaultClient.Do(req)
 if err != nil {
  log.Fatal(err)
  return
 }
 log.Println("response Status:", resp.Status)

}

func getName(website v1.Website) string {
 return website.Metadata.Name + "-website";
}
```

8：通过 模板 创建所需要的 部署：

请求创建的 `deployment-template.json` pod 模板如下：

- 创建 nginx 容器，提供服务
- 创建 git-sync 拉取仓库， 并通过 emptyDir 进行容器间共享

```
{
    "apiVersion": "extensions/v1beta1",
    "kind": "Deployment",
    "metadata": {
        "name": "[NAME]",
        "labels": {
            "webserver": "[NAME]"
        }
    },
    "spec": {
        "replicas": 1,
        "template": {
            "metadata": {
                "name": "[NAME]",
                "labels": {
                    "webserver": "[NAME]"
                }
            },
            "spec": {
                "containers": [
                    {
                        "image": "nginx:alpine",
                        "name": "main",
                        "volumeMounts": [
                            {
                                "name": "html",
                                "mountPath": "/usr/share/nginx/html",
                                "readOnly": true
                            }
                        ],
                        "ports": [
                            {
                                "containerPort": 80,
                                "protocol": "TCP"
                            }
                        ]
                    },
                    {
                        "image": "openweb/git-sync",
                        "name": "git-sync",
                        "env": [
                            {
                                "name": "GIT_SYNC_REPO",
                                "value": "[GIT-REPO]"
                            },
                            {
                                "name": "GIT_SYNC_DEST",
                                "value": "/gitrepo"
                            },
                            {
                                "name": "GIT_SYNC_BRANCH",
                                "value": "master"
                            },
                            {
                                "name": "GIT_SYNC_REV",
                                "value": "FETCH_HEAD"
                            },
                            {
                                "name": "GIT_SYNC_WAIT",
                                "value": "10"
                            }
                        ],
                        "volumeMounts": [
                            {
                                "name": "html",
                                "mountPath": "/gitrepo"
                            }
                        ]
                    }
                ],
                "volumes": [
                    {
                        "name": "html",
                        "emptyDir": {
                            "medium": ""
                        }
                    }
                ]
            }
        }
    }
}
```

service 模板 `service-template.json` 如下

```
{
    "apiVersion": "v1",
    "kind": "Service",
    "metadata": {
        "labels": {
            "webserver": "[NAME]"
        },
        "name": "[NAME]"
    },
    "spec": {
        "type": "NodePort",
        "ports": [
            {
                "port": 80,
                "protocol": "TCP",
                "targetPort": 80
            }
        ],
        "selector": {
            "webserver": "[NAME]"
        }
    }
}
```

### 20.2 自定义 API 服务器

1：k8s 1.7 后， 可自定义 API 服务器 集成到 主 API 服务器上

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgjX7w4muKsRoSA8z6ajm8g7gsLuJB9GvSDXSYJXCCfKt8vvD8EQdvibw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 20.3 使用 k8s 服务目录扩展

1：服务目录可以快速创建服务实例，而无需处理一个个的 Pod、service、configMap 和其它资源。

2：服务目录使用四种通用 API 资源：

- 一个 ClusterServiceBroker, 描述一个可以提供服务的（外部）系统

- - 集群管理员 为每个 服务代理创建 Broker 资源

- 一个 ClusterServiceClass, 描述一个可供应的服务类型

- - k8s 从服务代理获取到的 服务列表

- 一个 Servicelnstance, 已配置服务的一个实例

- - 用户调配服务时，创建实例

- 一个 ServiceBinding, 表示 一 组客户端(pod) 和 Servicelnstance 之间的绑定。

- - 绑定到具体的 pod (`instanceRef` 引入具体的实例)，并注入一个 Secret

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgg3FEicuVpBd1SytF7GJ5ibMia4C72aiaibMgmg5icR5aoAPmdaFFDr3qyhrg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3：服务目录是作为一种外部系统提供服务， 向 k8s 集群中注册代理， 暴露服务。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvasf2bKlI7ShtJ2pmImicobhgNVOqhV7rYTh06SI84cRiaUG8bicxUPJO4Q7sH3GTiaqBZpdu0Nia6tkC1A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 20.4 PaaS

1: platform-as-a-Service, 平台即服务

2：红帽(Red Hat) 的 OpenShift

> 提供多用户环境

- OpenShift PaaS 服务

- - Minishift，与 Minikube 等效
  - https://manage.openshift.com

3：

- Deis Workflow 【微软】

- - https://deis.com/workflow

- Helm ：部署现有应用的标准方式， 包管理器，类似于 yum,apt,homebrew

- - 寻找现有的图表: https://github.com/kubernetes/charts
  - helm install --name my-database stable/mysql: 自动部署所需的 Deployment、Service、Secret 和 PersistentVolumeClaim
  - OpenVPN: 最有用的图表，通过 VPN 和访问服务来输入 pod 网络，类似本地计算机是集群中的一个容器， 对开发应用程序并在本地运行时很有用
  - helm CLI 客户端
  - Tiller

**致谢**

感谢读者阅读，欢迎指正错误，谢谢。

**参考及扩展**

- Luksa M. Kubernetes in action[M]. Shelter Island: Manning Publications, 2018.

- https://skyao.io/

- https://jimmysong.io/

- 搜索镜像：https://hub.docker.com/

- 社区兴趣小组：https://github.com/kubernetes/community/blob/master/sig-list.md

- Swagger 创建 访问 API 服务器的客户端 : https://swagger.io/

- 控制器相关 源代码：https://github.com/kubernetes/kubernetes/tree/master/pkg/controller

- k8s 中领导者选举的例子：https://github.com/kubernetes-retired/contrib/blob/master/election/

- k8s 集群管理员指南：http://kubernetes.io/docs/admin

- minikube 文档：https://minikube.sigs.k8s.io/docs/commands/ip/

- minikube mount 将本地文件系统挂载到 Minikube Vm 中， 然后通过 一个 hostPath 卷 挂载到容器

- - https://github.com/kuberetes/minikube/tree/master/docs

- kube-applier 工具：可以定时从 版本控制中检出资源，提交更改

- Ksonnet + jsonnet: 自定义高级片段，快速转换成完整的 Deployment 配置文件

- - https://github.com/ksonnet/ksonnet-lib

- Fabric8 持续集成方案: http://fabric8.io

- - Google Cloud 在线实验室：https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubenetes

- k8s 最新的 代码仓库: http://github.com/kubernetes

- OpenShift PaaS 服务

- - Minishift，与 Minikube 等效
  - https://manage.openshift.com

- Deis Workflow 【微软】

- - https://deis.com/workflow

- Helm ：部署现有应用的标准方式， 包管理器，类似于 yum,apt,homebrew

- - 寻找现有的图表: https://github.com/kubernetes/charts
  - helm install --name my-database stable/mysql: 自动部署所需的 Deployment、Service、Secret 和 PersistentVolumeClaim
  - OpenVPN: 最有用的图表，通过 VPN 和访问服务来输入 pod 网络，类似本地计算机是集群中的一个容器， 对开发应用程序并在本地运行时很有用
  - helm CLI 客户端
  - Tiller

- kubernetes 中文文档：https://kubernetes.io/zh/docs/home/

原文作者：xixie，腾讯 IEG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/OXiqHvWJkmqz7FbclDd1jA

# 【NO.107】HTTP 请求之合并与拆分技术详解

导语：HTTP/2 中，是否还需要减少请求数？来看看实验数据吧。

## **1. 背景**

随着网站升级 HTTP/2 协议，在浏览页面时常常会发现页面的请求数量很大，尤其是小图片请求，经典的雅虎前端性能优化军规中的第 1 条就是减少请求数，在 HTTP/1.1 时代合并雪碧图是这种场景减少请求数的一大途径，但是现在这些图片是使用 HTTP/2 协议传输的，这种方式是否也适用？另外，在都使用 HTTP/2 的情况，在浏览器并发这么多小图片请求时，是否会影响其他静态资源的拉取速度（例如页面 js 文件的请求耗时)？

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171542054725654.png)

基于上面问题的思考，本文进行了一个简单的实验，尝试通过数据来分析 HTTP 中的合并与拆分，以及并发请求是否影响其他请求。通过这次的实验我们对比了以下几个不同 HTTP 场景的耗时数据:

- HTTP/1.1 合并 VS 拆分
- HTTP/1.1 VS HTTP/2 并发请求
- HTTP/2 合并 VS 拆分
- 浏览器并发 HTTP/2 请求数（大量 VS 少量）时，其他请求的耗时

## **2. 实验准备**

理论：合并与拆分都是 HTTP 请求优化的常用方法，合并主要为了减少请求数，可以减少多次建立 TCP 连接耗时，不过相对的，缓存命中率会受到影响；拆分主要为了利用并发能力，浏览器可以并发多个 TCP 连接，还可以结合 HTTP/1.1 中的长链接，不过受 HTTP 队头阻塞影响，并发能力并不强，于是 HTTP/2 协议出现，使用多路复用、头部压缩等技术很好的解决了 HTTP 队头阻塞问题，实现了较强的并发能力。而 HTTP/2 由于基于 TCP，依然无法绕过 TCP 队头阻塞问题，于是又出现了 HTTP/3，不过本文并不讨论 HTTP/3，有兴趣的同学可以自行 Google。实验环境：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171542251896603.png)
为了避免自己搭服务器可能存在的性能影响，实验中的图片资源数据使用腾讯云的 COS 存储，并开启了 CDN 加速。

## **3. 实验分析**

第一个实验：有 2 个 HTML。1 个 HTML 中并发加载 361 张小图片，记录所有图片加载完成时的耗时；另 1 个 HTML 加载合并图并记录其耗时。并分别记录基于 HTTP/1.1 和 HTTP/2 协议的不同限速情况的请求耗时情况。每个场景测试 5 次，每次都间隔一段时间避免某一时间段网络不好造成的数据偏差，最后计算平均耗时。实验数据：
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171542367883789.png)

### 3.1 HTTP/1.1 合并 VS 拆分

根据上面实验数据，抽出其中 HTTP/1.1 的合并和拆分的数据来看，很明显拆分的多个小请求耗时远大于合并的请求，且网速较低时差距更大。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171543402585101.png)

### **3.2 HTTP/1.1 合并请求的优化原理**

简单看下 HTTP 请求的主要过程：DNS 解析(T1) -> 建立 TCP 连接(T2) -> 发送请求(T3) -> 等待服务器返回首字节（TTFB）(T4) -> 接收数据(T5)。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171544051519645.png)

从上面请求过程中，可以看出当多个请求时，请求中的 DNS 解析、建立 TCP 连接等步骤都会重复执行多遍。那么如果合并 N 个 HTTP 请求为 1 个，理论上可以节省（N-1)* (T1+T2+T3+T4) 的时间。当然实际场景并没有这么理想，比如浏览器会缓存 DNS 信息，因此不是每次请求都需要 DNS 解析；比如 HTTP/1.1 keep-alive 的特性，使 HTTP 请求可以复用已有 TCP 连接，所以并不是每个 HTTP 请求都需要建立新的 TCP 连接；再比如浏览器可以并行发送多个 HTTP 请求，同样可能影响到资源的下载时间，而上面的分析显然只是基于同一时刻只有 1 个 HTTP 请求的场景。

感兴趣深入了解的可以参考网上一篇[HTTP/1.1 详细实验数据](https://segmentfault.com/a/1190000015665465)，其结论是：当文件体积较小的时候，（网络延迟低的场景下）合并后的文件的加载耗时明显小于加载多个文件的总耗时；当文件体积较大的时候，合并请求对于加载耗时没有明显的影响，且拆分资源可以提高缓存命中率。但是注意有特殊的场景，由于合并资源后可能导致网络往返次数的增加，当网络延迟很大时，是会增大耗时的（参考 TCP 拥塞控制）。

【扩展：TCP 拥塞控制】 TCP 中包含一种称为拥塞控制的机制，拥塞控制的主要工作是确保网络不会同时被过多的数据传输导致过载。当前拥塞控制的方法有许多，主要原理是慢启动，例如，开始阶段只发送一点数据，观察是否能通过，如果能接收方将确认发送回发送方，只要所有数据都得到确认，发送方就在下次 RTT 时将发送数据量加倍，直到观察到丢包事件（丢包意味着过载，需要后退），每次发送的数据量即拥塞窗口，就是这样动态调整拥塞窗口来避免拥塞。拥塞控制机制对每个 TCP 连接都是独立的。

### 3.3 HTTP/1.1 VS HTTP/2 并发请求

抽出实验数据中的 HTTP/1.1 和 HTTP/2 并发请求来对比分析，可以看出 HTTP/2 的并发总耗时明显优于 HTTP/1.1，且网速越差，差距越大。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171545464234968.png)

### **3.4 HTTP/2 多路复用和头部压缩的原理**

**多路复用** ：在一个 TCP 链接中可以并行处理多个 HTTP 请求，主要是通过流和帧实现，一个流代表一个 HTTP 请求，每个 HTTP 资源拆分成一个个的帧按顺序进行传输，不同流的帧可以穿插传输，最终依然能根据流 ID 组合成完整资源，以此实现多路复用。帧的类型有 11 种，例如 headers 帧（请求头/响应头），data 帧（body），settings 帧（控制传输过程的配置信息，例如流的并发上限数、缓冲容量、每帧大小上限）等等。

**头部压缩** ：为了节约传输消耗，通过压缩的方式传输同一个 TCP 链接中不同 HTTP 请求/响应的头部数据，主要利用了静态表和动态表来实现，静态表规定了常用的一些头部，只用传输一个索引即可表示，动态表用于管理一些头部数据的缓存，第一次出现的头部添加至动态表中，下次传输同样的头部时就只用传输一个索引即可。由于基于 TCP，头部帧的发送和接收后的处理顺序是保持一致的，因此两端维护的动态表也就保证一致。

多路复用允许一次 TCP 链接处理多次 HTTP 请求，头部压缩又大大减少了多个 HTTP 请求可能产生的重复头部数据消耗。因此 HTTP/2 可以很好的支持并发请求，感兴趣可以[深入 HTTP/2 浏览器源码分析](https://zhuanlan.zhihu.com/p/34662800)。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171546037946496.png)

【扩展：队头阻塞】 HTTP/2 解决了 HTTP/1.1 中 HTTP 层面（应用层）队头阻塞的问题，但是由于 HTTP/2 仍然基于 TCP，因此 TCP 层面的队头阻塞依然存在。HTTP/3 使用 QUIC 解决了 TCP 队头阻塞的问题。感兴趣可以看看[队头阻塞](https://zhuanlan.zhihu.com/p/330300133)这篇文章。

HTTP 层面的队头阻塞在于，HTTP/1.1 协议中同一个 TCP 连接中的多个 HTTP 请求只能按顺序处理，方式有两种标准，非管道化和管道化两种，非管道化方式：即串行执行，请求 1 发送并响应完成后才会发送请求 2，一但前面的请求卡住，后面的请求就被阻塞了；管道化方式：即请求可以并行发出，但是响应也必须串行返回（只提出过标准，没有真正应用过）。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171546123896844.png)

TCP 层面的队头阻塞在于，TCP 本身不知道传输的是 HTTP 请求，TCP 只负责传递数据，传递数据的过程中会将数据分包，由于网络本身是不可靠的，TCP 传输过程中，当存在数据包丢失的情况时，顺序排在丢失的数据包之后的数据包即使先被接收也不会进行处理，只会将其保存在接收缓冲区中，为了保证分包数据最终能完整拼接成可用数据，所丢失的数据包会被重新发送，待重传副本被接收之后再按照正确的顺序处理它以及它后面的数据包。

But，由于 TCP 的握手协议存在，TCP 相对比较可靠，TCP 层面的丢包现象比较少见，需要明确的是，TCP 队头阻塞是真实存在的，但是对 Web 性能的影响比 HTTP 层面队头阻塞小得多，因此 HTTP/2 的性能提升还是很有作用的。

HTTP/2 中存在 TCP 的队头阻塞问题主要由于 TCP 无法记录到流 id，因为如果 TCP 数据包携带流 id，所丢失的数据包就只会影响数据包中相关流的数据，不会影响其他流，所以顺序在后的其他流数据包被接收到后仍可处理。出于各种原因，无法改造 TCP 本身，因此为了解决 HTTP/2 中存在的 TCP 对头阻塞问题，HTTP/3 在传输层不再基于 TCP，改为基于 UDP，在 UDP 数据帧中加入了流 id 信息。

### 3.5 HTTP/2 合并 VS 拆分

由于 HTTP/2 支持多路复用和头部压缩，是不是原来 HTTP/1.1 中的合并请求的优化方式就没用了，在 HTTP/2 中合并雪碧图有优化效果吗？

抽出 HTTP/2 的合并和拆分的数据来看，拆分的多个小请求耗时仍大于合并的请求，不过差距明显缩小了很多。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171546296426561.png))那么为什么差距还是挺大呢？理论上 HTTP/2 的场景下，带宽固定，总大小相同的话，拆分的多个请求最好的情况应该是接近合并的总耗时的才对吧。

分析一下，因为是复用一个 TCP 连接，所以首先排除重复 DNS 查询、建立 TCP 连接这些影响因素。那么再分析一下资源大小的影响：

1. 本身合并的图片（516kB）就比拆分的 361 张小图片总大小（总 646kB）要小。
2. 拆分的很多个小请求时，虽然有头部压缩，但是请求和响应中的头部数据以及一些 settings 帧数据还是会多一些。通过查看 chrome 的 transferred 可以知道小图片最终总传输数据 741kB，说明除 body 外多传输了将近 100kB 的数据。

结合上面两点，理论上拆分的小图片总耗时应该是合并图片的耗时的（741/516=）1.44 倍。但是很明显测试中各网速场景下拆分的小图片总耗时与合并图片耗时的比值都大于 1.44 这个理论值（2.62、2.96、1.84）。其中的原因这里有两点推测：

1. 并发多个请求的总耗时计算的是所有请求加载完的耗时，每个请求都有发送和响应过程，其中分为一个个帧的传输过程，只要其中某小部分发生阻塞，就会拖累总耗时情况。
2. 浏览器在处理并发请求过程存在一定的调度策略而导致。推测的依据来自 Chrome 开发者工具中的 Waterfall，可以看到很多并发请求的 Queueing Time、Stalled Time 很高，说明浏览器不会在一开始就并行发送所有请求。
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171546433126427.png)

不过这里只属于猜测，还未深入探究。

### 3.6 浏览器并发 HTTP/2 请求数（大量 VS 少量）时，其他请求的耗时

第二个实验：在 HTML 中分别基于 HTTP/2 加载 360+张小图片、130+张小图片、20+张小图片、0 张小图片，以及 1 张大图片和 1 个 js 文件，大图片在 DOM 中放在所有小图片的后面，图片都是同域名的，js 文件是不同域名的，然后记录大图片和脚本的耗时，同样也是利用 Chrome 限速工具在不同的网络限速下测试（不过这个连的 WIFI 与第一个实验中不同，无限速时的网速略微不同）。这个实验主要用于分析并发请求过多时是否会影响其他请求的访问速度。实验数据：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171546554859125.png)

从实验数据中可以看出，

1. 图片并发数量不会影响 js 的加载速度，无限速时无论并发图片请求有多少，脚本加载都只要 0.12s 左右。
2. 很明显对大图片的加载速度有影响，可以看到并发量从大到小时，大图片的耗时明显一次减少。

但是其中也有几个反常的数据：Fast3G 和 Slow3G 的网速限制下，无小图片时的 js 加载耗时明显高于有并发小图片请求的 js 加载耗时。这是为啥？我们推测这里的原因是，由于图片和 js 不同域名，分别在两个 TCP 连接中传输，两个 TCP 是分享总网络带宽的，当有多个小图片时，小图片在 DOM 前优先级高，js 和小图片分享网络带宽，js 体积较大占用带宽较多，而无小图片时，js 是和大图片分享网络带宽，js 占用带宽比率变小，因此在限速时带宽不够的情况下表现出这样的反常数据。

## **4. 实验结论**

1. HTTP/1.1 中合并请求带来的优化效果还是明显的。
2. 对于多并发请求的场景 HTTP/2 比 HTTP/1.1 的优势也是挺明显的。不过也要结合具体环境，HTTP/2 中由于复用 1 个 TCP 链接，如果并发中某一个大请求资源丢包率严重，可能影响导致整个 TCP 链路的流量窗口一直很小，而这时 HTTP/1.1 中可以开启多个 TCP 链接可能其他资源的加载速度更快？当然这也只是个人猜测，没有具体实验过。
3. HTTP/2 中合并请求耗时依然会比拆分的请求总耗时低一些，但是相对来说效果没有 HTTP/1.1 那么明显，可以多结合其他因素，例如拆分的必要性、缓存命中率需求等，综合决策是否合并或拆分。
4. 网速较好的情况下，非同域名下的请求相互间不受影响，同域名的并发请求，随着并发量增大，优先级低的请求耗时也会增大。

不过，本文中的实验环境较为有限，说不定换了一个环境会得到不同的数据和结论？比如不同的浏览器（Firefox、IE 等）、不同的操作系统（Windows、Linux 等）、不同的服务端能力以及不同测试资源等等，大家感兴趣也可以抽点时间试一试。

**5. 其他思考**

以上讨论主要针对低计算量的静态资源，那么高计算量的动态资源的请求呢，（例如涉及鉴权、数据库查询之类的），合并 vs. 拆分？

**关于我们**

我们团队主要致力于前端相关技术的研究和在腾讯业务的应用，团队内部每周有内部分享会，有兴趣的读者可以加入我们或者参与一起讨论，微信 : darminzhou; camdyzeng。

原文作者：darminzhou，腾讯 CSIG 前端开发工程师

原文链接：https://mp.weixin.qq.com/s/gx5UCRcxG00YZq2Go_xsxQ

# 【NO.108】一文带你理解云原生

> 本文是一篇云原生的关键知识科普，希望给大家提供一扇云原生的“**窗户**”，传达三个目标：1、透过窗户看到一棵**大树代表：云原生的蓝图全貌**；2、树上会有很多核心**树干代表：云原生的关键技术**；3、希望树干上能摘到**果实代表：云原生对我的启发**。

开始阅读文章前，请角色切换：设想你作为一位中小型 IT 公司 CTO，面对云原生技术决策，你需要回答两个问题：

**1、为什么需要上云？**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171552066521840.png)
**2、上云有何弊端？**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171552154867181.png)

作为一家公司的技术决策者，**必须理解上云的利与弊**，并结合**公司各阶段发展目标**给出**最适合的技术方案**。

### **1 云原生-概述**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171552251198304.png)

#### 1.1 云原生-定义

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171552484193578.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171552574981814.png)

云原生的定义，业界也是“百家争鸣”各持观点，从技术视角理解云原生会相对清晰。云原生的关键技术包括：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171553207910796.png)

• **微服务架构**：服务与服务之间通过高内聚低耦合的方式交互；

• **容器**：作为微服务的最佳载体，提供了一个自包含的打包方式；

• **容器编排**：解决了微服务在生产环境的部署问题；

• **服务网络**：作为基础设施，解决了服务之间的通信；

• **不可变基础**：设施提升发布效率，方便快速扩展；

• **声明式 API**:让系统更加健壮；

- 命令式 API：可以直接发出让服务器执行的命令，例如：“运行容器”、”停止容器”等；
- 声明式 API：可以声明期望的状态，系统将不断地调整实际状态，直到与期望状态保持一致。

• **DevOps**：缩短研发周期，增加部署频率，更安全地方便：

- Culture ：达成共识
- Automation：基础设施自动化
- Measurement：可度量
- Sharing：你中有我，我中有你

**【私人观点】**

- 云原生的定义：**应用因云而生，即云原生**。
- 应用原生被**设计为在云上以最佳方式运行**，充分发挥**云的优势**，是**上云的最短路径**。

#### 1.2 云原生-技术生态

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171553401093866.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171553521302989.png)

#### 1.3 云原生-关键技术

云原生关键技术包括：**微服务，容器，容器编排，服务网络，不可变基础，声明式 API**。

##### **1.3.1 微服务**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171554029262038.png)

**微服务是一种用于构建应用的架构方案**。

将一个复杂的应用拆分成多个独立自治的服务，服务与服务间通过“高内聚低耦合”的形式交互。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171554129909395.png)

微服务典型架构包括：

- **服务重构**：单体改造成符合业务的微服务架构；
- **服务注册与发现**：微服务模块间的服务生命周期管理；
- **服务网关**：身份认证、路由服务、限流防刷、日志统计；
- **服务通信**：通信技术方案如，RPC vs REST vs 异步消息；
- **可靠性**：服务优雅降级，容灾，熔断，多副本。

##### **1.3.2 容器**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171554212025921.png)

**容器**是一种打包应用的方式，可以打包应用中的所有软件和软件所依赖的环境，并可实现跨平台部署。

**容器关键技术**：namespac 视图隔离，cgroups 资源隔离 ，Union File System 联合文件系统。

**容器优势**：

- 更高效的利用资源；
- 更快速的启动时间；
- 一致性的运行环境。

##### **1.3.3 容器编排**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171555103959291.png)

**容器编排包括**：自动化管理和协调容器的系统，专注于容器的生命周期管理和调度。

**核心功能**：

1. **容器调度**：依据策略完成容器与母机绑定；
2. **资源管理**：CPU、MEM、GPU、Ports、Device；
3. **服务管理**：负载均衡、健康检查。

##### **1.3.4 服务网格**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171555201783483.png)

**服务网格（Service Mesh）是致力于解决服务间通讯的基础设施层**。

- Service Mesh 应对云原生应用的复杂服务拓扑，提供可靠的通信传递；
- 通过一组**轻量级网络代理**（Sidecar proxy），与应用程序代码部署在一起来实现，且对应用程序透明。

**Service Mesh 特点**：

1. 应用程序间通讯的中间层；
2. 轻量级网络代理，应用程序无感知；
3. 解耦应用的重试、监控、追踪、服务发现。

**Service Mesh 主流组件**：Istio、MOSN（Modular Open Smart Network）Linkerd。

##### **1.3.5 不可变基础设施**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171555288163251.png)

**不可变基础设施（Immutable Infrastructure）（宠物 VS 牲畜）**

- 任何基础设施实例（服务器、容器等各种软硬件）一旦创建之后便成为一种只读状态，不可对其进行任何更改；
- 如果需要修改或升级实例，唯一方式是创建一批新实例以替换。

**不可变基础设施的优势**

1. 提升发布应用效率；
2. 没有雪花服务器；
3. 快速水平扩展。

##### **1.3.6 声明式 API**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171555379395206.png)

- **命令式 API**：可直接发出让服务器执行的命令，例如：“运行容器”、“停止容器”等；
- **声明式 API**：可声明期望的状态，系统将不断地调整实际状态，直到与期望状态保持一致。

为什么声明式使系统更加健壮？

可以类比理解成自动化工程学的**闭环自适应模型**。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171558321078357.png)

##### **1.3.7 DevOps**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171558421955796.png)

**DevOps 目标** **：缩短开发周期，增加部署频率，更可靠地发布**。

- 从历史上开发和运维相对孤立到开发和运维之间建立合作，可以增加信任，更快速地发布新版本。

**DevOps 是一组过程，方法和系统的统称**包括：

**Culture**：

**文化是 DevOps 中的第一成功要素**。

- 由于目标不同，开发和运维形成一堵墙，DevOps 通过建立开发和运维之间合作和沟通的文化来消除墙。

**Automation**：

- 自动化软件的开发和交付，通常包含持续集成，持续交付和持续部署，云原生时代还包括基础架构的自动化，即 IaC(Infrastructureas code)。

**Measurement**：

- 度量尤其重要，通过客观的测量来确定正在发生的事情的真实性，验证是否按预期进行改变。并为不同职能部门达成一致建立客观基础。

**Sharing**：

- 开发和运维团队之间长期存在摩擦的主要原因是缺乏共同的基础。
- 开发参与运维值班，参与软件的部署和发布，运维参与架构设计。

### **2 容器-Docker**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171558549358529.png)

#### 2.1 Docker 概述

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171559048245115.png)

**为什么学习容器技术?**

云时代从业者：**Docker 已成云平台运行分布式、微服务化应用的行业标准**。

作为有技术追求的程序员，有必要理解云原生的关键技术：容器。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171559338428803.png)

Docker 核心概念：**镜像、容器、仓库**。

**镜像(Image)**：

1. 一个只读模板；
2. 由一堆只读层（read-only layer）重叠；
3. 统一文件系统（UnionFileSystem）整合成统一视角。

**容器(Container)**：

1. 通过镜像创建的相互隔离的运行实例；
2. 容器与镜像区别：最上面那一层可读可写层；
3. 运行态容器定义：一个可读写的统一文件系统，加上隔离的进程空间，以及包含在其中的应用进程。

**仓库（Repository）**：

1. 集中存放镜像文件的地方；
2. Docker Registry 可包含多个仓库（Repository），每个仓库可包含多个标签（Tag），每个标签对应一个镜像。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171559517521782.png)

#### 2.2 Docker 关键技术

##### **2.2.1 Namespace 视图隔离**

- **Linux namespace 是一种内核级别的环境隔离机制**，使得其中的进程好像拥有独立的系统环境。
  ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171600078923517.png)

Network namespace 在 Linux 中创建相互隔离的网络视图，每个网络名字空间都有自己独立的网络配置，包括：**网络设备、路由表、IPTables 规则，路由表、网络协议栈等**。（默认操作是主机默认网络名字空间）

##### **2.2.2 control groups（资源隔离）**

Linux Control Group 是**内核用于限制进程组资源使用**的功能。资源包括：CPU，内存，磁盘 IO 等。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171600184012441.png)

##### **2.2.3 Union File System（联合文件系统）**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171600274383952.png)

**Union File System， 联合文件系统**：将多个不同位置的目录联合挂载(union mount)到同一个目录下。

1. Docker 利用联合挂载能力，**将容器镜像里的多层内容呈现为统一的 rootfs**(根文件系统)；
2. Rootfs 打包整个操作系统的文件和目录，是应用运行所需要的**最完整的“依赖库”**。
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171600421875756.png)
   ![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171600529931705.png)

#### 2.3 Docker-网络技术

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171601027491198.png)

**Bridge 模式：Docker0 充当网桥**，在默认情况下，被限制在 Network Namespace 里的容器进程，是通过 Veth Pair 设备 +宿主机网桥的方式，实现跟同其他容器的数据交换。

**一旦一张虚拟网卡被“插”在网桥上，它就会变成该网桥的“从设备”**。

从设备会被“剥夺”调用网络协议栈处理数据包的资格，从而“降级”成为网桥上的一个端口。

而这个端口唯一的作用，就是接收流入的数据包，然后把这些数据包全部交给对应的网桥，由网桥完成转发或者丢弃。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171601168812485.png)

Veth 提供一种连接两个 network namespace 的方法。Veth 是 Linux 中一种虚拟以太设备，总是成对出现常被称为 Veth pair。

可以实现点对点的虚拟连接，可看成一条连接两张网卡的网线。一端网卡在容器的 Network Namespace 上，另一端网卡在宿主机 Network Namespace 上。任何一张网卡发送的数据包，都可以对端的网卡上收到。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171601275644896.png)
在物理网络中，如果需要连接多个主机，会用交换机。在 Linux 中，能够起到虚拟交换机作用的网络设备，是**网桥（Bridge）**。它是一个工作在数据链路层的设备，主要功能是根据 MAC 地址学习来将数据包转发到网桥的不同端口（Port）上。

Bridge 网桥类似交换机，两个以上 namespace 接入同一个二层网络。veth pair 一端虚拟网卡加入到 namespace，另一端到网桥上。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171601363882987.png)

- **路由 routing**是通过互联的网络把信息从源地址传输到目的地址的活动，发生在 OSI 模型的第三层（网络层）。Linux 内核提供 IPForwarding 功能，实现不同子网接口间转发 IP 数据包。

路由器工作原理：

- 路由器上有多个网络接口，每个网络接口处于不同的三层子网上。
- 根据内部路由转发表将从一个网络接口中收到的数据包转发到另一个网络接口，实现不同三层子网间互通。

### **3 容器编排-Kubernetes**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171601475717305.png)

#### 3.1 概述&架构&核心组件

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171602018445444.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171602115567497.png)

**我认为 Kubernetes 最大成功：让容器应用进入大规模工业生产**。

- Kubernetes 的提供特性，几乎覆盖一个分布式系统在生产环境运行的所有关键事项。包括：

**Automated rollouts and rollbacks(自动化上线和回滚)**

- 使用 Kubernetes 描述已部署容器的所需状态，受控的速率将实际状态更改为期望状态。

**Self-healing(自我修复)**

- Kubernetes 重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器，并且在准备好服务之前不将其通告给客户端。

**Service discovery and load balancing(服务发现与负载均衡)**

- Kubernetes 可以使用 DNS 名称或自己的 IP 地址公开容器，如果进入容器的流量很大，Kubernetes 可以负载均衡并分配网络流量，从而实现部署稳定。

**Storage orchestration(存储编排)**

- Kubernetes 允许你自动挂载选择的存储系统，例如本地存储、公厂商等。

**Automatic bin packing(自动装箱)**

- Kubernetes 允许指定每个容器所需 CPU 和内存（RAM）。当容器指定资源请求时，Kubernetes 可以做出更好的决策来管理容器的资源。

**Secret and configuration management(安全和配置管理)**

- Kubernetes 允许存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。你可在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171602239806834.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171602316518877.png)

**API Service**: **Kubernetes 各组件通信中枢**。

- 资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制；
- 为 Pod， Deployment， Service 等各种对象提供 Restful 接口；
- 与 etcd 交互的唯一组件。

**Scheduler**：负责资源调度，按照预定调度策略将 Pod 调度到相应的机器。

- **Predicates(断言)**：淘汰制
- **Priorities（优先级）**：权重计算总分。

**Controller manager**：负责维护集群的状态，比如故障检测、自动扩展、滚动更新等。

**etcd**：分布式的 K-V 存储，**独立于 Kubernetes 的开源组件**。

- 主要存储关键的**原数据**，支持**水平扩容**保障元数据的高可用性；
- 基于**Raft 算法**实现强一致性，独特的**watch 机制**是 Kubernetes 设计的关键。

**kubelet** ：负责维护 Pod 的生命周期，同时负责 Volume（CVI）和网络（CNI）的管理。

**kube-proxy**：负责为 Service 提供 cluster 内部的服务发现和负载均衡

- kube-proxy 通过在节点上添加 iptables 规则以及从中移除这些规则来管理此端口重新映射过程。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171602422277047.png)
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171602546430683.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171603041175350.png)

**控制器模式的设计思想**：

> 容器类比集装箱，集装箱固然好用，但是如果它各面光秃秃的，吊车还怎么把它吊起来摆放好呢？

Pod 对象其实就是容器的升级版，对容器进行组合，添加更多属性和字段。就好比在集装箱上安装了吊环，Kubernetes 这台“吊车”可以更轻松操作容器。

> 然而**Kubernetes 操作这些“集装箱”的逻辑都是由控制器完成的**。
>
> **Kubernetes 通过“控制器模式” 的设计思想，来统一编排各种对象和资源**。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171603165673485.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171603315308630.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171603438662040.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171603536566757.png)

#### 3.2 部署&资源控制&存储

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171604059833444.png)

**Kubernetes-集群部署架构**

1. 所有组件通过 kubelet staticpod 的方式启动保证宿主机各组件的高可用，systemd 提供 kubelet 的高可用；
2. 每个 Master 的使用 hostNetwork 网络，controller-manager 和 scheduler 通过 localhost 连接到本节点 apiserver；
3. controller-manager 和 scheduler 的高可用通过自身提供的 leader 选举功能（–leader-elect=true）；
4. apiserver 高可用，可通过经典的 haporxy+keepalived 保证，集群对外暴露 VIP;
5. 外部访问通过 TLS 证书，在 LB 节点做 TLS Termination，LB 出来 http 请求到对应 apiserver 实例。apiserver 到 kubelet、kube-proxy 类似。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171604176889156.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171604258633691.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171604364296514.png)
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171604487947443.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171605006153592.png)

#### 3.3 Kubernetes-网络技术

##### **3.3.1 对外服务**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171605091488032.png))

**Service**是一个**逻辑概念**，**一组提供相同功能 Pods 的逻辑集合，并提供四层负载统一入口和定义访问策略**。

**交互流程**：Service 可通过标签选取后端服务。匹配标签的 Pod IP 和端口列表组成 endpoints，有 kube-proxy 负责均衡到对应 endpoint。

**为什么需要 service？**

1. 对外提供入口（容器如何被外部访问）；
2. 克服 Pod 动态性（Pod IP 不一定可以稳定依赖）；
3. 服务发现和稳定的服务（ Pod 服务发现、负载、高可用）。

**Service Type 四种方式**

1. **Cluster IP**：配置 endpoint 列表；
2. **NodePort**：默认端口范围：30000-32767，通过 nodeIP：nodePort 访问 ；
3. **LoadBalancer**：适用于公有云，云厂商实现负载，配置 LoadBalance 到 podIP ；
4. **ExternalName**：服务通过 DNS CNAME 记录方式转发到指定的域名。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171605195766416.png)

**Service Type 为 Cluster IP**：

1. Kubernetes 的默认服务，配置 endpoint 列表，可以通过 proxy 模式来访问该对应服务；
2. 类似通过 Nginx 实现集群的 VIP。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171605274450932.png)

**Service Type 为 Node Port**：

在集群所有节点上开放特定端口，任何发送到该端口流量借助 Service 的 Iptables 规则链发送到后端 Pod。

**注意事项**：

1. 每个服务对应一个端口，端口范围只有 30000–32767；
2. 需要感知和发现节点变化，流量转发增加 SNAT 流程，Iptables 规则会成倍增长。

**适用场景**：服务高可用性要求不高或成本不敏感，例如：样例服务或临时服务。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171605383779591.png)

**Service Type 为 Load Balancer**：

对公网暴露服务建议采用此方式，Service 没有对其行为做任何规范，依赖云厂商 LB 具体实现（云厂商收费服务）如：腾讯公有云：**CLB**。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171605466618254.png)

**Service Type 为 External Name** ：

DNS 作为服务发现机制，在集群内提供服务名到 Cluster IP 的解析。

**CoreDNS ：DNS 服务，CNCF 第 04 个毕业项目，KUBERNETES 的 1.11 版本已支持**。

CoreDNS 实现的高性能、插件式、易扩展的 DNS 服务端，支持自定义 DNS 记录及配置 upstream DNS Server，可以统一管理 Kubernetes 基于服务的内部 DNS。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171605553228925.png)

**Ingress Controller**：定义入流量规则，可做七层 HTTP 负载君合。

**Egress Controller**：定义出流量规则。

**交互流程**：通过与 Kubernetes API 交互，动态感知集群 Ingress 规则，按照自定义的规则生成（负载均衡器）配置文件，并通过 reload 来重新加载。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171606063433359.png)

##### **3.3.2 Underlay 与 Overlay 网络**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171606195341815.png)

**Underlay 网络模式:** **底层承载网络，是网络通信的基础**。

1. **优势**：复用基建，网络扁平，性能优势；
2. **劣势**：协作复杂，安全问题，管理成本。

很多场景下业务方希望容器、物理机和虚拟机可以在同一个扁平面中直接通过 IP 进行通信，通过 Floating-IP 网络实现。

**Floating-IP 模式**将宿主机网络同一网段的 IP 直接配置到容器中。

**这种模式为了保证容器与宿主机的交换机二层连通，需要在物理机上搭一个虚拟网桥**。具体选择哪种网桥，主流有：Linux bridge、MacVlan、SRIOV 三种模式。

1. **BridgeBridge**：设备内核最早实现的网桥，性能与 OVS 相当，可以使用到所有场景；

2. **MacVlan**：一个简化版的 bridge 设备，为了隔离需要内核，实现时不允许 MacVlan 容器访问其宿主机 IP 和 ServiceCluster IP；

3. **SR-IOV** 技术：一种基于硬件的虚拟化解决方案，可提高性能和可伸缩性；

   SR-IOV 标准允许在虚拟机之间高效共享 PCIe（快速外设组件互连）设备，并且它是在硬件中实现的，可以获得能够与本机性能媲美的 I/O 性能。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171606304074489.png)

**Overlay 网络**：是一种建立在另一网络之上的计算机网络。

1. **优势**：独立自治，快速扩展，网络策略；
2. **劣势**：复杂层级，性能损失，定制成本。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171606396981256.png)

Kubernetes 相当于**云原生的操作系统**。

有人会问，凭什么云原生的操作系统这杆大旗？

**主要原因是：Kubernetes 解决了一个分布式操作系统最核心的计算、存储、网络三种资源**。

**CNI 容器网络统一标准**：

1. CNCF 项目，为 Linux 容器提供配置网络接口的标准和以该标准扩展插件提供基础函数库；
2. CNI 命令行调用规范，其插件在主机上直接切换进入容器网络命名空间，为容器创建网络设备，配置 IP，路由信息。

**CNI 规范内容**：

1. 输入：ADD/DEL 控制指令，CNI 目录，容器 ID，网络命名空间，网卡名称。
2. 配置文件：标准部分：cniVersion，Name，Type，IPAM。
3. 输出：设备列表、IP 资源列表、DNS 信息。

**插件应用如**：

1. **Bridge**：Linux 网桥 CNI 实现，采用网卡对链接网桥和容器；
2. **Host-device**：将主机设备直接移动到容器命名空间中；
3. **PTP**：创建虚拟网卡对，采用路由方式实现对外互联；
4. **MacVlan**：网卡多 Mac 地址虚拟技术完整支持 vlan；
5. **Vlan**：Vlan 设备 CNI 实现，允许容器和主机分属不同 LAN；
6. **IPVlan**：网卡上基于 IP 实现流量转发。

##### **3.3.3 Overlay 网络-Flannel 方案**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171606493410062.png)

CoreOS（被 Red Hat 收购）为 Kubernetes 专门定制设计的 overlay 网络方案。

03 层网络方案实现：在每台主机部署 flanneld 进程实现网段分配，路由控制，采用多种转发机制实现流量跨机交互。

**Flannel 职责**

1. 子网管理：每个主机分配唯一的子网；
2. 互联方式：为同 Overlay 平面容器分配唯一 IP。

- **Etcd 存储**：容器之间路由映射；
- **SubNetManager**：子网资源划分、IP 资源申请释放的接口定义；
- **Backend**：针对网络互联方式的接口定义。

1. UDP，UDP 封包转发，建议仅调试使用；
2. VxLAN(建议)，使用内核 vxlan 特性实现封包转发；
3. Host-GW，主机 2 层互联情况下较高性能互联方式；
4. IPIP，使用 IPIP 隧道完成封包和转发；
5. IPSec，使用 IPSecurity 实现加密封包转发；
6. AliVPC，对接阿里云 VPC 路由表实现网络互联；
7. AWSVPC，对接 Amazon VPC 路由表实现网络互联。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171606597496744.png))

**Flannel 的单机互联方案**：

1. **子网分配**：充当虚拟交换机/网关角色，连接所有本机容器，完成虚拟子网构建；
2. **Bridge**：通过 NAT 借助主机网络实现外部服务访问；
3. **Veth pair**：一端设置到容器网络 namespace，一端链接 bridge 实现容器接入网络；
4. **对外访问**：为每个节点分配独立不冲突的 24 位子网。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171607092603129.png)

**Overlay 解决方案**：跨 Node 的 Pod 之间通信通过 Node 之间的 Overlay 隧道。

**职责**：路由控制，数据转发。

**主要流程**：

1. **本节点设置**：设备创建、本地路由创建、回写本地信息；
2. **监听其他节点信息**：更新 ARP 信息、更新 FDB、更新路由信息。

##### **3.3.4 Overlay 网络-Calico 方案**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171607237044826.png)

**Calico 项目**：是纯三层的虚拟网络解决方案，旨在简化、扩展和保护云网络的容器方案。

**Calico 优势**：

1. **可扩展性**：采用 IP 路由支持大规模网络。扩展便利，容错性高；
2. **网络安全**：支持 Kubernetes 网络策略，支持自定义网络策略；
3. **广泛集成**：集成 Kubernetes ，Mesos，支持 OpenStack，AWS，GCE，Azure。

**Calico 不足**：

1. **BGP 支持问题**：需要网路设备支持 BGP 协议，否则需要追加 IPIP 隧道；
2. **规划 2 层直连**：需要节点做良好的规划实现 2 层网络直接互联；
3. **大规模配置复杂**：网络规划，手动部署 Route Reflector，增加 API 代理。

**关键组件**：

1. **BGP Client**：和其他节点互联，发布当前节点路由并学习其他节点路由；
2. **Confd**：同步节点配置信息启动 BGPClient；
3. **Felix**：负责虚拟设备监控，ACL 控制、状态同步的 agent；
4. **Calico**：CNI 插件，负责容器设备创建；
5. **Calico-IPAM**：CNI 插件，负责容器网段管理与 IP 地址管理；
6. **RouteReflector**：对接 BGPclient 实现路由中转；
7. **Etcd/Kube-apiserver**：Calico 数据存储；
8. **typha**：应对大规模节点接入时作为数据缓存 proxy；
9. **RouteReflector 安装**：集群超过**100 个节点**时强烈建议启用，通过 RR 中转全局路由信息。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171607339630501.png)

**Calico 单机互联方案**：

1. **Veth-pair**：一端设置到容器，一端放置在主机上，为容器提供网络出入口；
2. **路由策略**：针对 IP 和设备设置路由条目，在主机上实现互联。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171607416349831.png)

**Calico 跨机互联方案**：

1. **同网段/BGP 支持**：主机之间通过 2 层直连或者网络支持路由转发至目标主机；
2. **跨网段 IPIP 互联**：网络设备不支持 BGP 协议情况下，采用 IPIP 隧道实现跨网段互联；
3. **跨网段 VxLAN 互联（Cannel）**：集成 flannel，底层通过 VxLAN 实现跨机转发。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171607527498334.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171608031462732.png)

### **4 服务网格 Istio**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171608153015969.png)

#### 4.1 服务网格概述

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171608251792471.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171608343749528.png)
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171608442373111.png)
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171608541641819.png)

#### 4.2 Istio 控制面

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171609026554039.png)
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171609082150774.png)

#### 4.3 Istio 数据面

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171609187874997.png)
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171609298388682.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171609384861715.png)
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171609454291651.png)

### **5 云原生-主流组件**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171609532903403.png)

#### 5.1 Prometheus

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171610023890060.png)

#### 5.2 Grafana

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171610115322822.png)

#### 5.3 Elasticsearch + Fluentd + Kibana

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171610184427537.png)

#### 5.4 Jaeger

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171610252131898.png)

#### 5.5 Chaos Engineering

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171610391257351.png)
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171610441384271.png)

### **6 云原生-常用网络技术**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171610544894009.png)

#### 6.1 主机网络

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171611043728717.png)
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171611315102831.png)
**iptables**是运行在**用户空间**的应用软件，通过控制**Linux 内核 netfilter**，来管理**网络数据包的处理和转发**。

存在“**表（tables）”、“链（chain）”和“规则（rules）**”三个层面：

1. 每个“**表**”指的是不同类型的数据包处理流程，例如 filter 表表示进行数据包过滤，而 NAT 表针对连接进行地址转换操作；
2. 每个表中又可以存在多个“**链**”，系统按照预订的规则将数据包通过某个内建链，例如将从本机发出的数据通过 OUTPUT 链；
3. 在“链”中可以存在若干“**规则**”，这些规则会被逐一进行匹配，如果匹配，可以执行相应的动作，例如修改数据包，或者跳转。

#### 6.2 Underlay 网络技术

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171611399827229.png)

**VLAN 虚拟局域网**：是将一个物理 LAN 在逻辑上划分成多个广播域的通信技术。每个 VLAN 是一个广播域，VLAN 内的主机间通信就和在一个 LAN 内一样。

**没有划分 VLAN：LAN 局域网**：

1. **优势**：简单，静态，IP 地址与交换机关联；
2. **劣势**：迁移域受限，不能机房内随意迁移。交换机下 IP 需要提前规划好，约束虚拟比。

**划分 VLAN：虚拟局域网**：

1. **优势**：IP 地址与交换机无关，虚拟机可以在机房范围内迁移。VLAN 间则不能直接互通，这样广播报文就被限制在一个 VLAN 内。

**有人会问：交换如何区分不同 VLAN？**

- 交换机能够分辨不同 VLAN 的报文，需要在报文中添加标识 VLAN 信息的字段。数据帧中的**VID（VLAN ID**）字段标识了该数据帧所属的 VLAN，数据帧只能在其所属 VLAN 内进行传输。

#### 6.3 Overlay 网络技术

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171611461394679.png)

**VXLAN 虚拟扩展局域网**：

1. 是对传统 VLAN 协议的一种扩展；
2. 是一种网络虚拟化技术，试图改善云计算部署的可扩展性问题。

**解决哪些问题？**

1. vlan 的数量限制（12bit->24bit），VLAN 报文 Header 预留长度只有 12bit，只支持 4096 个终端；
2. VNI（VXLAN Network Index）标识某条指定隧道；
3. 不改变 IP 实现服务器迁移。

**传统二三层网络架构限制了虚拟机的动态迁移范围**。

VXLAN 在两台 TOR 交换机之间建立一条隧道，将服务器发出的原始数据帧加以“包装”，好让原始报文可以在承载网络（比如 IP 网络）上传输。

当到达目的服务器所连接的 TOR 交换机后，离开 VXLAN 隧道，并将原始数据帧恢复出来，继续转发给目的服务器。VXLAN 将整个数据中心基础网络虚拟成了一台巨大的“二层交换机”

**VXLAN 网络模型**

1. **UDP 封装**（L2 over L4）：将 L2 的以太帧封装到 UDP 报文即（L2overL4）中，并在 L3 网络中传输；
2. **VTEP**，VXLAN 隧道端点，对 VXLAN 报文进行封装和解封装；
3. **VNI**，VXLAN 隧道标识，用于区分不同 VXLAN 隧道。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171612008820234.png)

1. **矢量性协议**：使用基于路径、网络策略或规则集来决定路由；

2. **AS（自治域）**：AS 是指在一个实体管辖下的拥有相同选路策略的 IP 网络；

   BGP 网络中的每个 AS 都被分配一个唯一的 AS 号，用于区分不同的 AS；

3. **eBGP（域外 BGP）**：运行于不同 AS 之间的 BGP，为了防止 AS 间产生环路；

   为了防止 AS 间产生环路，当 BGP 设备接收 EBGP 对等体发送的路由时，会将带有本地 AS 号的路由丢弃；

4. **iBGP（域内 BGP）**：运行于同一 AS 内部的 BGP，为了防止 AS 内产生环路；

5. **RR（路由反射器）**：通过集中反射同步，解决全连通的网状网格结构路由同步问题。

EBGP+IBGP 实现 AS 间的路由传递：一个常见的 IP 骨干网的拓扑结构，骨干层和汇聚层分别是两个自治系统，AS100 有两个出口设备 SwitchC 和 SwitchD，两个 AS 之间需要进行路由互通。

### **7 总结-云原生**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171612104515470.png)

**云原生应用：docker 应用打包、发布、运行，Kubernetes 服务部署和集群管理，Istio 构建服务治理能力**。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171612213584661.png)

云计算以“**资源**”为中心，关键技术：

1. **虚拟化**：SDX，NFV；

2. **资源池化**：弹性快速扩缩容；

3. **多租化**：提升云厂商的资源利用率；

   典型代表：计算、网络、存储三大基础设施的云化。

**云计算以“应用”为中心**，关键导向：

1. 设计之初，关注更好适应云，充分发挥云优势；
2. 云原生已成为企业数字创新的最短路径；
3. 云原生一系列 IAAS、PAAS、SAAS 层技术，支撑产品高效交付、稳定运维、持续运营。

**【私人观点**】

1. **以“资源”为中心的云，将成为“底层基础设施”，利用云原生以“应用”为中心赋能自身业务**；
2. 云的时代，已经来临。作为云的使用者、从业者，**更多思考如何利用云赋能业务产品**；
3. 商业市场模式从“大鱼吃小鱼”靠信息不对称，**向“快鱼吃慢鱼”转变。我们必须利用趋势，拥抱云原生**。

### **8 鸣谢**

以下小伙伴给出宝贵建议，非常感谢。

- 鸣谢 CDG-FiT 线：**Hawkliu、Cafeeqiu**
- 鸣谢腾讯 OTeam：**Kubernetes 开源协同技术讲师**

### **9 学习资料**

- 《SRE Google 运维解密》
- 《Kubernetes 权威指南》
- 《Kubernetes in Action》
- 《深入剖析 Kubernetes》
- 《Docker 容器与容器云》-浙大
- 《云原生服务网格 Istio》华为丛书
- 《Kubernetes 开源协同技术课程》
- CNCF 官网：https：//[www.cncf.io/](http://www.cncf.io/)
- Huawei-Cloud Native : https：//github.com/huawei-cloudnative
- Docker 官方文档：https：//docs.docker.com/
- Kubernetes 官网：https：//kubernetes.io/
- Istio 官网：https：//istio.io/

### **10 英雄帖**

欢迎对 **云原生或金融科技** 感兴趣的小伙伴随时交流，**更欢迎加入我们团队**。

邮箱地址：**[williammeng@tencent.com](mailto:williammeng@tencent.com)**

原文作者：William 孟祥龙，腾讯 CDG 系统架构师，从事云原生技术赋能金融科技。

原文链接：https://mp.weixin.qq.com/s/yX0hgIOLuaKsAcrWfOfcUQ

# 【NO.109】为什么磁盘存储引擎用 b+树来作为索引结构？

> 在数据库或者存储的世界里，存储引擎的角色一直处于核心位置。往简单了说，存储引擎主要负责数据如何读写。往复杂了说，怎么快速、高效的完成数据的读写，一直是存储引擎要解决的关键问题。在绝大部分介绍、讲解存储引擎的书籍或者文章里，大家都默认了读多写少的磁盘存储引擎采用的就是 b+树，而极少有人来剖析选择 b+树作为索引结构的背后，到底有着怎样的思考和权衡？为了解答上述问题，本文尝试从一个新的视角和大家讨论：
> **在处理读多写少的场景下，为什么基于磁盘的存储引擎会选择用 b+树来作为索引结构？
>
> **希望在看完本文后，读者能对该问题有一个全新的认识和属于自己的答案。限于个人能力有限，有表述、理解不正当之处希望批评指正。

**本文的内容主要以问答方式展开，层层递进分析、解决问题，本文涉及内容会围绕下面三个问题展开。在开始阅读本文内容前，大家不妨先尝试自己回答下面三个问题！**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171616537350011.png)

**为了减少读者的疑惑，在开始本文的正式内容之前，先和读者做一些简要的关键名词的解释和说明：**

**1.存储引擎：** 存储引擎是一个很广的范畴，有处理读多写少的基于页结构的 b+树存储引擎，也有后起之秀基于日志结构(lsm 树)的存储引擎。在本文中提到的存储引擎，如没有特殊说明，都指的是**针对处理读多写少场景的基于磁盘的 b+树存储引擎**，这类存储引擎在关系型数据库中出现的频率较高，经典代表就是 mysql 中的 innodb，此外 golang 编写的 boltdb 也属于本文讨论的范畴。

**2.扩展内容：** 文中标识为扩展内容的章节，对于基础相对较好的读者这些内容可以选择性阅读，不读也不会造成本文核心内容理解困难；对于基础相对一般的小伙伴，可以选择性的进行阅读。

下面我们先尝试回答前两个问题，因为前两个问题可以算作是一大类问题。第一个问题主要在于数据结构的选型。第二个问题主要在于因果关系的区分。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171616596629390.png)

### **1.背景**

这个问题的答案，我们从哪里开始说起呢？想之又想，只有搞清楚了整体的一个背景，我们才能知道当时的工程师面临的怎样的一个问题。近而，我们才能尝试从根上解释这个问题。从整体的大的环境来看，他们要解决的问题主要面临的以下四个主要特点：

**1. 处理读多写少的场景**
**2. 关系型数据库按照行组织**
**3. 存储千万级量级数据**
**4. 采用性价比高的存储**

接下来我们一一对其进行解释，因为如果上述四个背景如果不成立的话，说明我们一开始的出发点就错了，后面的回答都是无稽之谈。

#### 1.1 处理读多写少的场景

提起这个话题，我们就不得不说，在互联网发展起来的早期，大部分的系统主要处理的是**读多写少**的场景。例如早期的 bbs 内容、博客、门户网站、电商的商品入库等等，这些场景都可以划分为读多写少。他们通过有限次的写操作将数据写入到数据库中，然后供用户多次浏览、阅读。发展到今天的互联网，面向用户的很多系统仍然是属于读多写少的范畴。所以**读多写少**这是一个早期存储引擎在数据读写时面临的最大的背景。

#### 1.2 关系型数据库按照行组织

早期的时候存储引擎这个概念主要还是出现在关系型数据库中，大部分人接触这个概念估计也是因为 mysql，mysql 中经常提到存储引擎这个词。在关系型数据库中，数据往往通过**数据库->表(多列)–>行** 的方式来组织。最终落到存储引擎这一层时，数据已经按照行的格式来组织了。广义来看其实也就是类似于 key-value 的存储了，只不过在关系型数据库中，到达存储引擎这层的 value 是一行记录按照指定的格式来扁平化组织而成，具体的格式大同小异。这儿不再展开赘述。大家可以自行搜索资料查阅，此处主要是抛出来在关系型数据库中**数据按照行格式来存储**这个背景。

为了方便介绍，在后续的内容中，存储引擎存储的数据我们就统一按照 key-value 来讨论了。此处的 key 大家暂且可以直观的理解为主键。

#### 1.3 存储千万级量级数据

随着互联网的迅速发展，数据存储的量级日益增长，很快就达到了**存储千万级量级数据**这个量级。很明显这个背景从需求的角度看，其实是属于不断迭代的过程。不可能初期互联网一起来，马上就面临这个量级。但是我们也知道在计算机软件的世界中，**可扩展性**是大家耳熟能详的词语。所以在设计存储引擎时，自然而然肯定会考虑这个问题。所以此处，我们将**存储千万级数据量级**这个作为第三个背景。

#### 1.4 采用性价比高的存储

接着第三个背景，自然而然就引出了数据存储在哪里的问题。回答这个问题，必须就得引出一个成本问题了。如果不考虑成本来存储，那自然问题就简化了。但是千万级量级的数据存储时，有了成本的限制，就得思考了。

**我们的目标是要找到一个成本相对廉价，但又能支持千万级数据量级的存储介质。**

对于计算机中，存储数据的媒介整体可以分为两大类：

**1.易失性存储：** 典型代表内存
**2.非易失性存储：** 典型代表硬盘(磁盘)，尤其是早期的机械硬盘

关于二者的详细对比，大家可以参考下图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171617099451970.png)

**首先，** 通过上图的对比，我们大致可以确定了，我们期望的存储介质就是硬盘(主要是机械硬盘)了。因为它很符合我们所寻找的几个特点。但我们都知道硬盘虽然符合我们的要求，但硬盘有着它先天结构的限制。**访问磁盘的速度要比访问内存慢的多。**

到这儿也就不得不提一下，关于机械硬盘它的构成了。关于机械硬盘的简单介绍，我们在下面的扩展内容中进行简要介绍，大家感兴趣可以进行阅读，已经掌握的读者可以直接跳过这部分虚线框中的内容。

------

**扩展内容**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171617174745843.png)

上图关于磁盘介绍的图片摘自[本篇文章](https://blog.csdn.net/weixin_37641832/article/details/103217311)。

普通的机械硬盘读写数据主要是通过移动磁头到对应的磁道，然后再旋转磁头到对应的扇区。最后进行移动磁头进行读写数据。

简单说：一次硬盘数据读写主要包括三大部分耗时：**寻道时间**、**旋转时间**、**传输时间**。而在这其中**寻道时间**主要占大头，主要是因为磁头的移动主要是马达通过驱动磁臂近而移动磁头，这个运动属于机械运动，所以速度相对较慢。

对磁盘而言，磁盘的访问肯定是要比内存慢的。但是在磁盘访问时，又有两种访问方式：

**1. 随机 IO**
**2. 顺序 IO**

顺序 IO 的访问速度要远远快于随机 IO，其根本原因是：顺序 IO 主要减少了磁头的移动频率，也就是减少了**寻道时间**。所以它的性能要比随机 IO 要快很多。

由于篇幅有限，关于硬盘的介绍我们就不过多展开了，不然就跑题了。有了上述的知识，我们就足以开展我们后续的内容了。关于硬盘的详细内容介绍，大家可以自行找其他资料学习或者[点击本篇文章](https://blog.csdn.net/weixin_37641832/article/details/103217311)进行阅读。下面我们继续介绍我们的主要内容。

------

**其次，\**我们既然选择了硬盘做存储媒介，**那就必须想办法克服这个问题**。下面这张图详细描述了**内存访问速度和磁盘访问速度**的对比结果。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171617256203700.png)

下面我们简单总结下，抛出我们在这块得出的**结论**：

> 结论 1 可以参考扩展内容详细了解。

**1.磁盘访问时间：寻道时间+旋转时间+传输时间：**

> 寻道时间：8ms~12ms
> 旋转时间：7200 转/min：半周 4ms
> 传输时间：50M/s，约 0.3ms

**2.磁盘随机 IO ≪ 磁盘顺序 IO ≈ 内存随机 IO ≪ 内存顺序 IO**

**3.机械磁盘和固态硬盘构成：**

> **机械硬盘：** 电磁存储，通过电磁信号转变来控制读写，磁头机械臂移动
> **固态硬盘：** 半导体存储，用固态电子存储芯片阵列、由控制单元和存储单元组成，内部由 闪存颗粒组成。速度较

#### 1.5 总结

本节主要交代了 4 个大的背景，下面再和大家回顾一下。

**1. 处理读多写少的场景**
**2. 关系型数据库按照行组织**
**3. 存储千万级量级数据**
**4. 采用性价比高的存储**

最后我们结合实际的场景选择以**硬盘**这种介质来存储数据，同时在存储引擎层，数据按照抽象层面的 key-value 来组织读取和写入。了解了大的背景，下面得了解我们的目标是啥了。没有目标就没有方向，搞清楚目标对我们至关重要。

### **2.目标**

在第一节中，我们介绍了四大基本背景。并分析出来了我们最终需要采取硬盘来存储数据。在本节中，我们将重点关注我们的要**达到的目标**，只有明确了目标，我们才能更好的进行自顶向下分解任务并解决问题。

在介绍我们的目标前，我们先来看看我们在**基于磁盘存储数据的条件下，一次常规的用户请求大概是怎样的？**

#### 2.1 常规的一次用户请求响应过程

我们都知道，我们的数据存储在硬盘上，因此当用户的请求(读/写)进来后，首先会到操作系统管理的内存中，在内存中进行一些逻辑处理，然后 cpu 会发送指令从硬盘读取数据到内存中，最后就会响应上层用户(读：读取的数据、写：写数据是否成功等)。

上面描述的一个大概的流程。从中我们可以看到整个过程经历三个阶段：

**请求过程：**
用户请求->内存->硬盘

**响应过程:**
响应用户<-内存<-硬盘

理清楚了整个用户的请求响应流程后，我们就来看看，我们最终的目标是啥呢？

#### 2.2 目标

其实互联网的应用，有个潜移默化的潜规则，那就是**高效、快速**，对存储引擎而言也不例外，我们的目标就是要在上述背景下进行**快速、高效的数据读写请求**。

问题来了！**快速、高效**这都属于定性分析的一类描述用语，怎么样算快速呢？怎么样又算高效呢？怎么定量分析这个需求呢？

到这儿，大伙儿可以想想如果这个需求是你来负责的，那么你会从哪些角度出发来思考这个问题呢？

这个问题应该难不倒聪明的你，还记得数据结构与算法里有一个指标吗！**时间复杂度**，这就是一个很重要的核心指标呀，衡量数据结构或者算法处理效率的一个关键指标。我们想想，我们的数据最终要存储，怎么存储，怎么组织，这不就涉及到选择哪种数据结构了吗！**那上述问题我们也就进一步延伸到了，采用哪种数据结构来组织我们的数据，并尽可能使得其读写比较快速、高效。具体的快速、高效通过时间复杂度来判定。**

#### 2.3 总结

本小节我们对前面介绍的两部分内容通过一个框图来进行总结回顾。具体的选择哪种数据结构这个问题我们放到下一节来进行介绍。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171617367662586.png)

### **3.数据结构选型**

在 2.2 节提到，我们最终将**快速、高效读写**这个问题转化成了**选择采用哪种数据结构来组织数据、并通过其时间复杂度来定量的判定我们的目标**。下面我们就从数据结构这个方面着手看看。

#### 3.1 数据结构方案对比

我们详细的分析下，**快速、高效**那也就意味着读写的平均时间复杂度，要尽可能的低。在数据结构中我们学习过很多的数据结构，例如：平均时间复杂度是 O(1)的数据结构，典型代表是哈希表(hash table)。哈希表主要在点对点的映射读写上冲突比较低时效率很高，但其原生的数据结构在面对范围查找、排序等操作时时间复杂度会相对较高，这也算是哈希表的一大缺陷。

其次平均时间复杂度比 O(1)稍慢的是平均时间复杂度为 O(logn)，这类数据结构有二叉查找/排序树(bst tree)、平衡二叉树(avl tree)、红黑树(rb tree)、b 树(b/b- tree)、b+树(b+ tree)、跳表(skiplist)等。他们天然就支持排序、范围查找操作；再其次比 O(logn)还慢的时间复杂度为 O(n)、O(n^2)等等。O(n)的平均时间复杂度的典型代表有数组。其他类型我们这儿就不过多展开了。

下图是我们根据平均时间复杂度依次从**O(1)->O(logn)->O(n)->…** 由快到慢的一个对比结果。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171617466312396.png)

我们都知道互联网的应用中，排序、范围查找是一个再普遍不过的需求了。例如在电商网站购物时，大部分用户都会下意识的点击按照销量从高到低排序；再比如在门户新闻网站上，我们会关注过去一周某个新闻被阅读的次数，按照时间来排序；再比如推荐系统中，我们会按照内容的一类或者多类属性对物品进行排序，还有很多很多例子。所以我们在选择数据结构时，必须考虑**支持范围查找、排序等操作**。

基于这点的话，看来哈希表就不太符合我们的需求了。那我们退而求其次，再来看看 O(logn)的时间复杂度中，我们选择哪种数据结构呢？这几种数据结构粗略一看貌似都能满足我们的需求，同时上述几种数据结构:**二叉查找/排序树(bst tree)、平衡二叉树(avl tree)、红黑树(rb tree)、b 树(b/b- tree)、b+树(b+ tree)、跳表(skiplist)** 在内存中都可以实现，我们如何选择呢？直观来看我们选哪种好像都可以，但我们别忘了，我们的数据最终要落到硬盘，既然这儿得不出结论，那我们就从硬盘的维度来看看，硬盘上哪种数据结构好维护呢？

#### 3.2 目光转向磁盘

根据前面的介绍，我们的数据流向分为三个阶段，以读取操作举例：**磁盘->内存->用户**。既然这样的话，我们的**直观想法是，如果能在内存和硬盘这两种介质上维护同一种数据结构**，那就最完美了，这样当用户请求进来后，从磁盘加载数据后，可以直接加载到内存中，然后做一些处理就返回用户了。如果直观想法走不通的话(**找不到这样一种数据结构**)。那我们就只能按照**间接思路来出发了，硬盘上维护一种数据结构存储我们的数据，内存中选择另外一种数据结构保存数据。当从硬盘读取数据到内存中时，中间做一层转换**。间接思路这种做法是下策，因为中间数据的转换避免不了会引入一些性能的损耗。

那就先按照直观想法出发来找找看，是否存在这样一类数据结构呢？

#### 3.3 直观思路出发

我们先想想，既然我们的目标仍然是**快速、高效读写**，那对应到磁盘上，怎么做到对磁盘快速、高效读写呢？

根据前面的铺垫介绍，大伙应该都知道了那就尽可能的利用磁盘的**顺序 IO**呗。提到顺序 IO，脑子里第一时间蹦出来的自然就是**追加写**，因为这种方式就是一种典型的顺序写、顺序 IO，性能挺高的。我们假设用户每个写请求进来，都采用追加写的方式来保存数据。在这种方式下写操作是快了、高效了。**那怎么来读呢？**

根据前面的介绍，数据是按照 key-value 来扁平化存储的。每条记录长度各不相同，既然要保证读，那就得额外保存一些信息来辅助处理用户的读请求。这些额外保存的数据，我们暂且称为**索引**。我们思索一下，在这种追加写的场景下，我们需要保存哪些信息才可以完成正常的读请求呢？其实**每条记录我们只要知道了它写在磁盘的哪个位置(偏移量)offset、占了多长 size 这两个信息。我们就可以对其进行读了**。简而言之，一条记录对应一个这样的二元组索引信息。简单示意图如下所以：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171617577894834.png)

到这儿，高效写可以了，维护了索引后，单个记录的读也可以了；但是有个问题我们得想想怎么办？**那就是我们前面提到的排序、范围查找操作**。

在这种场景下，每来一条记录我们都是直接追加的，所以数据在磁盘上本身就是乱序存储的，既然需要**排序、范围查找**的话。那就得把磁盘上的所有记录都加载到内存中，然后再挨个挨个遍历判断，最后过滤出来满足条件的记录返回用户。这种方式能实现功能，但显然效率太低了。同时磁盘上存储的数据可能都远远超过内存大小了，所以这种方式根本就不可取。**那有没有办法解决该问题呢？**

我们做一点假设：**假设我们写磁盘的时候能保证顺序写的同时，写入的数据是有序的**。比如，我们写入了三条数据，这三条数据本身写入的时候是排好序的，那么此时范围查找时，我们只要定位到第一条数据，后面的数据是有序的，就可以很快进行按序读取了。如果假设条件成立的话，那排序、范围查找这个问题就从根本上得到简化了。我们也就不用这么大费周折了。我们先基于这个简单假设来看一下，在假设条件成立的情况下，我们还需要解决哪些问题呢？

在这种模式下，我们访问每条记录同时还是需要保留之前的结论：**每条数据都维护一个索引项：offset、size**。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171618076741503.png)

我们要存储的是千万级量级的数据，每一条记录都需要一个索引项，那么千万条的记录就需要维护千万条索引项。问题就接着产生了，这千万条的索引项怎么组织？选哪种数据结构组织? 存哪里？…

针对千万条索引项这个问题，我们来看看这个问题有没有解。直观的想法可能就分为两大类：

1. 能否减少索引项的个数？索引项个数减少，自然问题就好解决了
2. 不能减少索引项个数的情况下，是否可以找到**合理的数据结构**来组织。这儿的“合理”可以理解成：空间压缩、优化等等。

我们先从按照第一个思路来看看吧！

Q:为什么会产生千万条索引项呢？

W:因为每一条记录都需要维护一个索引项，我们需要保存千万条记录，所以就得存储千万条索引项。

Q:为什么每一条记录需要维护一个索引项呢？

W:因为每一条记录都是从用户请求传递进来的，每条记录在按照行格式扁平化存储时，长度是不固定的，所以需要每一条记录维护一个索引项。

到这儿我们知道了问题的核心原因了。

到这儿我们将上面层层推导的内容用一张图来总结一下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171618171337530.png)

#### 3.4 索引矛盾点

**索引核心矛盾点:** 根据前面的分析，每条记录是**变长**的，所以需要每条记录都维护一个索引项。**变长、变长、变长**，我们能从变长这个维度切入做一些改进或者优化吗？既然每条记录的长度我们无法控制，那是否可以将磁盘转化一下呢？

我们如果将磁盘划分成一段一段的固定大小的连续块，**对于每一块的话，我们记录一个块编号 no 来区分不同的块**，假设我们的块大小是 100 个字节，那么第 0 块对应的范围是 0~~99，第 1 块对应的是 100~~199，依次类推。做了这样的改变后会发生什么呢？我们详细分析一下。

将磁盘划分成一个一个的固定大小连续块后，每个块内仍然保留原先过程中的两大特性：**数据有序并且顺序写**。大致的结果如下图所以：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171618252669556.png)

这样改造以后，关键我们看看怎么保证读写呢？

我们先假设我们的块空间足够大，这样的话就能避免出现一个块存不下一条记录的问题。

正常情况下，我们的一个块会保存多条记录，并且块内的记录是有序存储的。我们在读取一条记录的时候，一条记录肯定是位于其中一块上，首先我们得解决这个定位问题。当定位到具体的块后，将当前块的数据从磁盘加载到内存中，块内部的数据是有序存储的，那自然就可以通过二分的方式来找到我们的具体数据对应的索引项了。最后再根据索引项来读取数据即可。同理写的过程虽然对外来看是对单条记录进行写，但内部是按照块作为单位来写磁盘。

那问题就转化成了**如何确定一条记录保存在哪一块上了？**

针对这个问题，我们就需要**确定一块上具体存储的多条记录的具体范围**。例如第 0 块保存的是 id 从 0~~10 的记录；第 1 块保存的是 id 从 11~~23 的记录。等等

这样的话，当查询 id 为 7 的记录时，我们就很快可以知道该条记录存储在第 0 块上，然后再从第 0 块内部查找具体 id 为 7 的记录的索引项，最后再读取数据。

怎么确定呢？**自然就需要在原先只保存一个块编号 no 的基础上，再多保存两个信息：该块保存的记录最小值 min、该块保存的记录的最大值 max**。

**每一块都对应这样一个三元组 block->(no,min,max)。**
**这个三元组表达的含义是：第 no 块保存的记录范围是 min~max**

我们仔细再回想一下，其实这个三元组还是有改进空间的。因为我们写入的时候，每个块都是顺序写的并且块内数据是有序的，块间也是有序的。那也就是说：**对于第 i 块而言，第 i 块存储的记录范围就是第 i 块的最小值拼接上第 i+1 块的最小值。其根本原因也就是存储的时候块间是有序的。那进一步我们就可以将上述三元组简化成一个二元组了 block->(no,min)**。同时附加保证每块之间保存的数据是逻辑有序的。

前面啰里啰嗦说了一大堆，我们最后总结一下：

1. 引入了将磁盘划分成一个一个**固定大小连续块**的概念
2. **块内数据仍然按照有序、顺序写存储**：块内仍然对每条记录保存两个信息：该记录写到磁盘的哪个位置 offset、该条记录占多长 size
3. **块间数据有序，每块需要保存两个信息**：块编号 no、该块保存的最小记录值 min

在引入这个块的概念后，我们看看当执行范围查找、排序等操作时，大部分情况下可以减少 IO 的次数，因为一次读取的一块数据，而一块中的数据包含多条记录。如果所涉及的数据在一块内的话，多条数据就只需要读取一次磁盘。所以在这种场景下，性能也会有所改善。

整体大致的结构如下图所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171618364689891.png)

同时，我们还有两个遗留问题需要解决:

**1. 块的大小定多大呢？**

**2. 块索引存不存？怎么存？**

针对第一个问题:**块大小定多大？**，我们可以辩证的来看。

**如果块大小越大**，那么一块能保存的数据就越多，因此同等数据量级的条件下我们需要的块就越少，近而需要维护的**块索引也就越少**。但读写每条记录的时候额外读写的空间会越大(按照块读写)，**因此读写效率越低**。
**如果块大小越小**，那么一块能保存的数据就越少，因此同等数据量级的条件下我们需要的块就越多，近而需要维护的**块索引也就越多**。但读写每条记录的时候额外读写的空间会越小(按照块读写)，**因此读写效率越高**。

到这儿总算看出来了，其实**块大小定多大**就是一个折中问题。那我们怎么来选择呢？

别忘了，我们的数据存储在磁盘，同时我们的应用时跑在操作系统上层，我们在这儿就想**怎么让操作系统为我们的应用服务的更好呢？\**简而言之就是更好的利用操作系统的特性，让其发挥出最大的价值。我们每次读写数据都会涉及到磁盘操作，例如读写磁盘、刷盘等，而在数据写到磁盘前，数据会先写到内存中，在操作系统中管理内存时，有一个**页**的概念。操作系统读写磁盘、刷盘时机都和**管理内存的页**有不可分割的关系。因此那我们这块要不为了更好利用操作系统，**就将操作系统页做为基本单位来确定块的大小**，最简单也就是一块大小等于一页大小(默认 4k)。更大也可以是 8k、16k、32k 等等。

其实到这儿，我们也就回想起来了，innodb 中默认的页大小就是 16k；而在 boltdb 中，默认的页大小就是操作系统的页大小 4k。

既然选择的操作系统页作为块大小基本单位，那我们也不引入一个新的概念**块**了，我们也**称块为页**呗。减少大家对新名词的理解成本。

第一个问题，到这儿我们也就解答完了。接下来我们看第二个问题。

**块索引存不存？怎么存？**

我们的答案是**存**，因为不存的话，当我们的应用重启时，就需要重新建块索引，当存储的数据量很大时，重建块索引是相当耗时的一件事情，在重建索引期间，可能会导致服务对外不可用。**所以我们需要存块索引。**那具体怎么存储呢？

**第一种：最简单划分独立的块来保存快索引**
该种方式在 mysql 中也被称为**聚簇索引**,索引和记录数据存储在一起，存储在一个文件中。**第二种：将快索引采用单独的文件来保存**
该种方式在 mysql 中也被称为**非聚簇索引**,索引和记录数据分开存储，存储在不同的文件中。

#### 3.5 b 树还是 b+树

到此，我们的整体推导已经差不多接近尾声了，我们将上述推导做一个汇总，最终得到的结果如下图所示。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171618479043428.png)

上图中每个虚线框表示一页，其中每一页都保存数据(数据项或者索引项)，每一页之间也可以有指针指向确保页之间是逻辑有序的。其次每个页内部都包含多个数据项或者索引项，而且数据是有序存储的。如果我们把其中的黑线去掉后，剩余的这种结构是一种啥结构呢？

答案是:**多叉树，其中每页可以看做一个节点，该节点内部有多项，同时一个节点可以多有个孩子节点**

接下来我们再来回想个问题。现在我们可以基于这样的结构进行读写了。那我们来看看，当我们读取的时候，如果读取的数据正好是其中某一页保存的最小记录，那这时候如果我们的最小记录保存了数据的话，就可以直接返回了。而不用再往下遍历了。如果只保存一个最小记录关键字的话，那就还需要一直往下遍历。那我们就来看看**每页中的索引项存或者不存该条记录的原始数据会有哪些差异点呢？**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171618589623564.png)

根据上图的分析，我们可以看到，如果对应的页索引项中保存了原始数据，则它对应的就是**b 树**的数据结构；如果不存储原始数据，则它对应的就是**b+树**的数据结构。分析清楚了存和不存的区别，那我们到底选择存还是不存呢？

答案是:**不存**，因为同样大小的一页，如果页索引项存储了额外的原始数据的话，必然一页中存储的页索引个数就会减少。同时进一步会导致存储同等数据量级的情况下，存储时的树的高度会比不存时高太多。而树的高度在我们这个场景里其实对应的就是磁盘 IO 的次数。显然我们要**快速、高效**，那就要尽可能减少不必要的磁盘 IO 次数。所以不存。近而，**我们也就确定了我们选择的数据结构就是 b+树了**。

到此，我们就从最初的直观思路出发，找到了在磁盘上容易维护的数据结构：**b+树**。

在我们的抽象和改进中，引入了页的概念，磁盘中按照页的单位来组织数据，页内部保存的数据有序存储，页间数据也是有序存储。同时在磁盘上的 b+树中，非叶子节点保存页索引信息。其中包括(页编号 no、该页保存的数据最小记录 min)；叶子节点保存具体的记录数据。

既然磁盘上选择了 b+树存储，那自然内存中也就选择 b+树实现咯。我们来看看二者之间如何相互转化。

内存中 b+树的节点对应磁盘上的一页。内存中一个节点内部的多项对应磁盘上每一页中保存每一个元素(每条记录数据 or 每个页索引项)。

**这儿再强调下：我们选择用 b+树作为索引而不是 b 树作为索引的核心点在于，在存储同等数据量级的情况下，选择用 b+树做索引时，要比用 b 树做索引。平均的磁盘 IO 次数要少。同时对 b+树而言，不同请求的时间复杂度都比较平均。因为每条记录的数据都保存在叶子节点上。**

#### 3.6 总结

到此我们尝试回答**为什么选择 b+树作为存储引擎索引结构**这个问题就回答完毕了。我们用一张图来总结下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171619086393481.png)
最后我们看一下数据结构的 b+树长啥样，我们磁盘+内存中的 b+树又长啥样。

下图是数据结构中的 b+树，此处我们就不再过多解释其 b+树的特性了。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171619162088872.png)

下图是磁盘+内存中最后对应的 b+树示意图。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171619248639018.png)

最后，我们在接下来的一节内容中尝试通过回答第三个问题来我们来佐证一下我们选择的这个方案。

### **4.反向论证**

既然选择了用 b+树来存储千万级数据量级的索引结构，那对于一个指定页大小的存储引擎，3 层或者 4 层的 b+树能存储多少条数据呢？通过这个问题，我们再来证明下，我们选择的方案是否能解决我们当初提到的**存储千万级数据量级的数据**这个问题。

#### 4.1 3 层、4 层 b+树(innodb 为例)各能存储多少数据?

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171619338976517.png)

针对这个问题，我们如何做一个粗略的估算呢？

我们都知道 innodb 中，默认的一页大小是 16k，此处我们也就以 16k 来做近似的估算。

在估算前，我们需要事先假设两个数据:

1. 假设非叶子节点中保存的页索引项，每一项的大小占 16 个字节。对于 16k 的一页而言，大约可以存储 1000(16k/16)条索引项。
2. 假设叶子节点中保存的每条记录数据的平均大小为 160 个字节。对于 16k 的一页而言，大约可以存储 100(16k/160)条记录。

**综上：**

对于 3 层的 b+树而言，其所能存储的数据量级：1000 *1000 * 100，大概 10^8 条

对于 4 层的 b+树而言，其所能存储的数据量级：1000 * 1000 * 1000 * 100，大概 10^11 条

也就是说，一个 3 层的 b+树，在此种场景下，大约可以存储的数据量级就在千万级。因此该解决方案是可以解决我们最初提出来的问题的。下图是一个简单的总结。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171619425913877.png)

#### 4.2 总结

到此，我们也就回答完了三个问题。并通过回答这三个问题，探讨了面对读多写少场景时选择的 b+树存储引擎背后的一些选型过程。值得说明的是本文纯属个人学习过程中的一点思考和琢磨。有理解或表达不正确之处还请各位指正。

原文作者：jaydenwen，腾讯 PCG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/8gDVqlywLBl-MZa6XrtXug

# 【NO.110】一致性哈希算法及其在分布式系统中的应用

## 1.摘要

本文将会从实际应用场景出发，介绍一致性哈希算法（Consistent Hashing）及其在分布式系统中的应用。首先本文会描述一个在日常开发中经常会遇到的问题场景，借此介绍一致性哈希算法以及这个算法如何解决此问题；接下来会对这个算法进行相对详细的描述，并讨论一些如虚拟节点等与此算法应用相关的话题。

## 2.分布式缓存问题

假设我们有一个网站，最近发现随着流量增加，服务器压力越来越大，之前直接读写数据库的方式不太给力了，于是我们想引入Memcached作为缓存机制。现在我们一共有三台机器可以作为Memcached服务器，如下图所示。

![img](https://pic3.zhimg.com/80/v2-84a49b3095bc9498af19c29eb0117d06_720w.webp)

很显然，最简单的策略是将每一次Memcached请求随机发送到一台Memcached服务器，但是这种策略可能会带来两个问题：一是同一份数据可能被存在不同的机器上而造成数据冗余，二是有可能某数据已经被缓存但是访问却没有命中，因为无法保证对相同key的所有访问都被发送到相同的服务器。因此，随机策略无论是时间效率还是空间效率都非常不好。

要解决上述问题只需做到如下一点：保证对相同key的访问会被发送到相同的服务器。很多方法可以实现这一点，最常用的方法是计算哈希。例如对于每次访问，可以按如下算法计算其哈希值：

h = Hash(key) % 3

其中Hash是一个从字符串到正整数的哈希映射函数。这样，如果我们将Memcached Server分别编号为0、1、2，那么就可以根据上式和key计算出服务器编号h，然后去访问。

这个方法虽然解决了上面提到的两个问题，但是存在一些其它的问题。如果将上述方法抽象，可以认为通过：

h = Hash(key) % N

这个算式计算每个key的请求应该被发送到哪台服务器，其中N为服务器的台数，并且服务器按照0 – (N-1)编号。

这个算法的问题在于容错性和扩展性不好。所谓容错性是指当系统中某一个或几个服务器变得不可用时，整个系统是否可以正确高效运行；而扩展性是指当加入新的服务器后，整个系统是否可以正确高效运行。

现假设有一台服务器宕机了，那么为了填补空缺，要将宕机的服务器从编号列表中移除，后面的服务器按顺序前移一位并将其编号值减一，此时每个key就要按h = Hash(key) % (N-1)重新计算；同样，如果新增了一台服务器，虽然原有服务器编号不用改变，但是要按h = Hash(key) % (N+1)重新计算哈希值。因此系统中一旦有服务器变更，大量的key会被重定位到不同的服务器从而造成大量的缓存不命中。而这种情况在分布式系统中是非常糟糕的。

一个设计良好的分布式哈希方案应该具有良好的单调性，即服务节点的增减不会造成大量哈希重定位。一致性哈希算法就是这样一种哈希方案。

## 3.一致性哈希算法

### 3.1算法简述

一致性哈希算法（Consistent Hashing）最早在论文《[Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web](https://link.zhihu.com/?target=http%3A//www.akamai.com/dl/technical_publications/ConsistenHashingandRandomTreesDistributedCachingprotocolsforrelievingHotSpotsontheworldwideweb.pdf)》中被提出。简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0 - 232-1（即哈希值是一个32位无符号整形），整个哈希空间环如下：

![img](https://pic4.zhimg.com/80/v2-405f0a2eab8b49e24b83a21c75695c57_720w.webp)

整个空间按顺时针方向组织。0和232-1在零点中方向重合。

下一步将各个服务器使用H进行一个哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中三台服务器使用ip地址哈希后在环空间的位置如下：

![img](https://pic2.zhimg.com/80/v2-dbc599469147a9ec7881b6a5fc19c361_720w.webp)

接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数H计算出哈希值h，通根据h确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。

例如我们有A、B、C、D四个数据对象，经过哈希计算后，在环空间上的位置如下：

![img](https://pic3.zhimg.com/80/v2-2e5b64d7c60a521670c1a3308ee41f0a_720w.webp)

根据一致性哈希算法，数据A会被定为到Server 1上，D被定为到Server 3上，而B、C分别被定为到Server 2上。

### 3.2容错性与可扩展性分析

下面分析一致性哈希算法的容错性和可扩展性。现假设Server 3宕机了：

![img](https://pic3.zhimg.com/80/v2-869803fe1377a9ddf0a050607f43e3c2_720w.webp)

可以看到此时A、C、B不会受到影响，只有D节点被重定位到Server 2。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即顺着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。

下面考虑另外一种情况，如果我们在系统中增加一台服务器Memcached Server 4：

![img](https://pic3.zhimg.com/80/v2-330fbba0d628ce5cfbc6c3c4c6491076_720w.webp)

此时A、D、C不受影响，只有B需要重定位到新的Server 4。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即顺着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。

综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。

### 3.3虚拟节点

一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如我们的系统中有两台服务器，其环分布如下：

![img](https://pic3.zhimg.com/80/v2-0eee2ab0fe933a1d827c4c66e3205a26_720w.webp)

此时必然造成大量数据集中到Server 1上，而只有极少量会定位到Server 2上。为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。例如上面的情况，我们决定为每台服务器计算三个虚拟节点，于是可以分别计算“Memcached Server 1#1”、“Memcached Server 1#2”、“Memcached Server 1#3”、“Memcached Server 2#1”、“Memcached Server 2#2”、“Memcached Server 2#3”的哈希值，于是形成六个虚拟节点：

![img](https://pic3.zhimg.com/80/v2-451e8e3ceb25c5e43d1766c1627cf16a_720w.webp)

同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Memcached Server 1#1”、“Memcached Server 1#2”、“Memcached Server 1#3”三个虚拟节点的数据均定位到Server 1上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。

## 4.总结

目前一致性哈希基本成为了分布式系统组件的标准配置，例如Memcached的各种客户端都提供内置的一致性哈希支持。本文只是简要介绍了这个算法

原文地址：https://zhuanlan.zhihu.com/p/565477967

作者：linux

# 【NO.111】如何实现一个malloc

任何一个用过或学过C的人对malloc都不会陌生。大家都知道malloc可以分配一段连续的内存空间，并且在不再使用时可以通过free释放掉。但是，许多程序员对malloc背后的事情并不熟悉，许多人甚至把malloc当做操作系统所提供的系统调用或C的关键字。实际上，malloc只是C的标准库中提供的一个普通函数，而且实现malloc的**基本**思想并不复杂，任何一个对C和操作系统有些许了解的程序员都可以很容易理解。

这篇文章通过实现一个简单的malloc来描述malloc背后的机制。当然与现有C的标准库实现（例如glibc）相比，我们实现的malloc并不是特别高效，但是这个实现比目前真实的malloc实现要简单很多，因此易于理解。重要的是，这个实现和真实实现在基本原理上是一致的。

这篇文章将首先介绍一些所需的基本知识，如操作系统对进程的内存管理以及相关的系统调用，然后逐步实现一个简单的malloc。为了简单起见，这篇文章将只考虑x86_64体系结构，操作系统为Linux。

## 1. 什么是malloc

在实现malloc之前，先要相对正式地对malloc做一个定义。

根据标准C库函数的定义，malloc具有如下原型：

```text
void* malloc(size_t size);
```

这个函数要实现的功能是在系统中分配一段连续的可用的内存，具体有如下要求：

- malloc分配的内存大小**至少**为size参数所指定的字节数
- malloc的返回值是一个指针，指向一段可用内存的起始地址
- 多次调用malloc所分配的地址不能有重叠部分，除非某次malloc所分配的地址被释放掉
- malloc应该尽快完成内存分配并返回（不能使用[NP-hard](https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/NP-hard)的内存分配算法）
- 实现malloc时应同时实现内存大小调整和内存释放函数（即realloc和free）

对于malloc更多的说明可以在命令行中键入以下命令查看：

```text
man malloc
```

## 2. 预备知识

在实现malloc之前，需要先解释一些Linux系统内存相关的知识。

### 2.1 Linux内存管理

#### 2.1.1 虚拟内存地址与物理内存地址

为了简单，现代操作系统在处理内存地址时，普遍采用虚拟内存地址技术。即在汇编程序（或机器语言）层面，当涉及内存地址时，都是使用虚拟内存地址。采用这种技术时，每个进程仿佛自己独享一片2（N次方）字节的内存，其中N是机器位数。例如在64位CPU和64位操作系统下，每个进程的虚拟地址空间为2（64次方） Byte。

这种虚拟地址空间的作用主要是简化程序的编写及方便操作系统对进程间内存的隔离管理，真实中的进程不太可能（也用不到）如此大的内存空间，实际能用到的内存取决于物理内存大小。

由于在机器语言层面都是采用虚拟地址，当实际的机器码程序涉及到内存操作时，需要根据当前进程运行的实际上下文将虚拟地址转换为物理内存地址，才能实现对真实内存数据的操作。这个转换一般由一个叫[MMU](https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Memory_management_unit)（Memory Management Unit）的硬件完成。

#### 2.1.2 页与地址构成

在现代操作系统中，不论是虚拟内存还是物理内存，都不是以字节为单位进行管理的，而是以页（Page）为单位。一个内存页是一段固定大小的连续内存地址的总称，具体到Linux中，典型的内存页大小为4096Byte（4K）。

所以内存地址可以分为页号和页内偏移量。下面以64位机器，4G物理内存，4K页大小为例，虚拟内存地址和物理内存地址的组成如下：

![img](https://pic4.zhimg.com/80/v2-98967865130cdd11edd719af7e0d568b_720w.webp)

上面是虚拟内存地址，下面是物理内存地址。由于页大小都是4K，所以页内便宜都是用低12位表示，而剩下的高地址表示页号。

MMU映射单位并不是字节，而是页，这个映射通过查一个常驻内存的数据结构页表来实现。现在计算机具体的内存地址映射比较复杂，为了加快速度会引入一系列缓存和优化，例如TLB等机制。下面给出一个经过简化的内存地址翻译示意图，虽然经过了简化，但是基本原理与现代计算机真实的情况的一致的。

![img](https://pic1.zhimg.com/80/v2-b25366bf0e3b0cee1ff5894ccb2ad9cc_720w.webp)

#### 2.1.3 内存页与磁盘页

我们知道一般将内存看做磁盘的的缓存，有时MMU在工作时，会发现页表表明某个内存页不在物理内存中，此时会触发一个缺页异常（Page Fault），此时系统会到磁盘中相应的地方将磁盘页载入到内存中，然后重新执行由于缺页而失败的机器指令。关于这部分，因为可以看做对malloc实现是透明的，所以不再详细讲述，有兴趣的可以参考《深入理解计算机系统》相关章节。

最后附上一张在维基百科找到的更加符合真实地址翻译的流程供大家参考，这张图加入了TLB和缺页异常的流程（[图片来源页](https://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Page_table)）。

![img](https://pic1.zhimg.com/80/v2-a0d5d6923f0b78c9cde0e881eccb2948_720w.webp)

### 2.2 Linux进程级内存管理

#### 2.2.1 内存排布

明白了虚拟内存和物理内存的关系及相关的映射机制，下面看一下具体在一个进程内是如何排布内存的。

以Linux 64位系统为例。理论上，64bit内存地址可用空间为0x0000000000000000 ~ 0xFFFFFFFFFFFFFFFF，这是个相当庞大的空间，Linux实际上只用了其中一小部分（256T）。

根据[Linux内核相关文档](https://link.zhihu.com/?target=https%3A//www.kernel.org/doc/Documentation/x86/x86_64/mm.txt)描述，Linux64位操作系统仅使用低47位，高17位做扩展（只能是全0或全1）。所以，实际用到的地址为空间为0x0000000000000000 ~ 0x00007FFFFFFFFFFF和0xFFFF800000000000 ~ 0xFFFFFFFFFFFFFFFF，其中前面为用户空间（User Space），后者为内核空间（Kernel Space）。图示如下：

![img](https://pic1.zhimg.com/80/v2-5670af9ca284004bd6b7b793501642a0_720w.webp)

对用户来说，主要关注的空间是User Space。将User Space放大后，可以看到里面主要分为如下几段：

- Code：这是整个用户空间的最低地址部分，存放的是指令（也就是程序所编译成的可执行机器码）
- Data：这里存放的是初始化过的全局变量
- BSS：这里存放的是未初始化的全局变量
- Heap：堆，这是我们本文重点关注的地方，堆自低地址向高地址增长，后面要讲到的brk相关的系统调用就是从这里分配内存
- Mapping Area：这里是与mmap系统调用相关的区域。大多数实际的malloc实现会考虑通过mmap分配较大块的内存区域，本文不讨论这种情况。这个区域自高地址向低地址增长
- Stack：这是栈区域，自高地址向低地址增长

下面我们主要关注Heap区域的操作。对整个Linux内存排布有兴趣的同学可以参考其它资料。

#### 2.2.2 Heap内存模型

一般来说，malloc所申请的内存主要从Heap区域分配（本文不考虑通过mmap申请大块内存的情况）。

由上文知道，进程所面对的虚拟内存地址空间，只有按页映射到物理内存地址，才能真正使用。受物理存储容量限制，整个堆虚拟内存空间不可能全部映射到实际的物理内存。Linux对堆的管理示意如下：

![img](https://pic4.zhimg.com/80/v2-9fcd1025b068825975fbc0bff07acf43_720w.webp)

Linux维护一个break指针，这个指针指向堆空间的某个地址。从堆起始地址到break之间的地址空间为映射好的，可以供进程访问；而从break往上，是未映射的地址空间，如果访问这段空间则程序会报错。

#### 2.2.3 brk与sbrk

由上文知道，要增加一个进程实际的可用堆大小，就需要将break指针向高地址移动。Linux通过brk和sbrk系统调用操作break指针。两个系统调用的原型如下：

```text
int brk(void *addr);
void *sbrk(intptr_t increment);
```

brk将break指针直接设置为某个地址，而sbrk将break从当前位置移动increment所指定的增量。brk在执行成功时返回0，否则返回-1并设置errno为ENOMEM；sbrk成功时返回break移动之前所指向的地址，否则返回(void *)-1。

一个小技巧是，如果将increment设置为0，则可以获得当前break的地址。

另外需要注意的是，由于Linux是按页进行内存映射的，所以如果break被设置为没有按页大小对齐，则系统实际上会在最后映射一个完整的页，从而实际已映射的内存空间比break指向的地方要大一些。但是使用break之后的地址是很危险的（尽管也许break之后确实有一小块可用内存地址）。

#### 2.2.4 资源限制与rlimit

系统对每一个进程所分配的资源不是无限的，包括可映射的内存空间，因此每个进程有一个rlimit表示当前进程可用的资源上限。这个限制可以通过getrlimit系统调用得到，下面代码获取当前进程虚拟内存空间的rlimit：

```text
int main() {
    struct rlimit *limit = (struct rlimit *)malloc(sizeof(struct rlimit));
    getrlimit(RLIMIT_AS, limit);
    printf("soft limit: %ld, hard limit: %ld\n", limit->rlim_cur, limit->rlim_max);
}
```

其中rlimit是一个结构体：

```text
struct rlimit {
    rlim_t rlim_cur;  /* Soft limit */
    rlim_t rlim_max;  /* Hard limit (ceiling for rlim_cur) */
};
```

每种资源有软限制和硬限制，并且可以通过setrlimit对rlimit进行有条件设置。其中硬限制作为软限制的上限，非特权进程只能设置软限制，且不能超过硬限制。

## 3. 实现malloc

### 3.1 玩具实现

在正式开始讨论malloc的实现前，我们可以利用上述知识实现一个简单但几乎没法用于真实的玩具malloc，权当对上面知识的复习：

```text
/* 一个玩具malloc */
#include <sys/types.h>
#include <unistd.h>
void *malloc(size_t size)
{
    void *p;
    p = sbrk(0);
    if (sbrk(size) == (void *)-1)
        return NULL;
    re
```

这个malloc每次都在当前break的基础上增加size所指定的字节数，并将之前break的地址返回。这个malloc由于对所分配的内存缺乏记录，不便于内存释放，所以无法用于真实场景。

### 3.2 正式实现

下面严肃点讨论malloc的实现方案。

#### 3.2.1 数据结构

首先我们要确定所采用的数据结构。一个简单可行方案是将堆内存空间以块（Block）的形式组织起来，每个块由meta区和数据区组成，meta区记录数据块的元信息（数据区大小、空闲标志位、指针等等），数据区是真实分配的内存区域，并且数据区的第一个字节地址即为malloc返回的地址。

可以用如下结构体定义一个block：

```text
typedef struct s_block *t_block;
struct s_block {
    size_t size;  /* 数据区大小 */
    t_block next; /* 指向下个块的指针 */
    int free;     /* 是否是空闲块 */
    int padding;  /* 填充4字节，保证meta块长度为8的倍数 */
    char data[1]  /* 这是一个虚拟字段，表示数据块的第一个字节，长度不应计入meta */
};
```

由于我们只考虑64位机器，为了方便，我们在结构体最后填充一个int，使得结构体本身的长度为8的倍数，以便内存对齐。示意图如下：

![img](https://pic2.zhimg.com/80/v2-2bcd994a18ed2f4bc7c0cb940233e54d_720w.webp)

#### 3.2.2 寻找合适的block

现在考虑如何在block链中查找合适的block。一般来说有两种查找算法：

- **First fit**：从头开始，使用第一个数据区大小大于要求size的块所谓此次分配的块
- **Best fit**：从头开始，遍历所有块，使用数据区大小大于size且差值最小的块作为此次分配的块

两种方法各有千秋，best fit具有较高的内存使用率（payload较高），而first fit具有更好的运行效率。这里我们采用first fit算法。

```text
/* First fit */
t_block find_block(t_block *last, size_t size) {
    t_block b = first_block;
    while(b && !(b->free && b->size >= size)) {
        *last = b;
        b = b->next;
    }
    return b;
}
```

find_block从frist_block开始，查找第一个符合要求的block并返回block起始地址，如果找不到这返回NULL。这里在遍历时会更新一个叫last的指针，这个指针始终指向当前遍历的block。这是为了如果找不到合适的block而开辟新block使用的，具体会在接下来的一节用到。

#### 3.2.3 开辟新的block

如果现有block都不能满足size的要求，则需要在链表最后开辟一个新的block。这里关键是如何只使用sbrk创建一个struct：

```text
#define BLOCK_SIZE 24 /* 由于存在虚拟的data字段，sizeof不能正确计算meta长度，这里手工设置 */
 
t_block extend_heap(t_block last, size_t s) {
    t_block b;
    b = sbrk(0);
    if(sbrk(BLOCK_SIZE + s) == (void *)-1)
        return NULL;
    b->size = s;
    b->next = NULL;
    if(last)
        last->next = b;
    b->free = 0;
    return b;
}
```

#### 3.2.4 分裂block

First fit有一个比较致命的缺点，就是可能会让很小的size占据很大的一块block，此时，为了提高payload，应该在剩余数据区足够大的情况下，将其分裂为一个新的block，示意如下：

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='750' height='486'></svg>)

实现代码：

```text
void split_block(t_block b, size_t s) {
    t_block new;
    new = b->data + s;
    new->size = b->size - s - BLOCK_SIZE ;
    new->next = b->next;
    new->free = 1;
    b->size = s;
    b->next = new;
}
```

#### 3.2.5 malloc的实现

有了上面的代码，我们可以利用它们整合成一个简单但初步可用的malloc。注意首先我们要定义个block链表的头first_block，初始化为NULL；另外，我们需要剩余空间至少有BLOCK_SIZE + 8才执行分裂操作。

由于我们希望malloc分配的数据区是按8字节对齐，所以在size不为8的倍数时，我们需要将size调整为大于size的最小的8的倍数：

```text
size_t align8(size_t s) {
    if(s & 0x7 == 0)
        return s;
    return ((s >> 3) + 1) << 3;
}
```



```text
#define BLOCK_SIZE 24
void *first_block=NULL;
 
/* other functions... */
 
void *malloc(size_t size) {
    t_block b, last;
    size_t s;
    /* 对齐地址 */
    s = align8(size);
    if(first_block) {
        /* 查找合适的block */
        last = first_block;
        b = find_block(&last, s);
        if(b) {
            /* 如果可以，则分裂 */
            if ((b->size - s) >= ( BLOCK_SIZE + 8))
                split_block(b, s);
            b->free = 0;
        } else {
            /* 没有合适的block，开辟一个新的 */
            b = extend_heap(last, s);
            if(!b)
                return NULL;
        }
    } else {
        b = extend_heap(NULL, s);
        if(!b)
            return NULL;
        first_block = b;
    }
    return b->data;
}
```

#### 3.2.6 calloc的实现

有了malloc，实现calloc只要两步：

1. malloc一段内存
2. 将数据区内容置为0

由于我们的数据区是按8字节对齐的，所以为了提高效率，我们可以每8字节一组置0，而不是一个一个字节设置。我们可以通过新建一个size_t指针，将内存区域强制看做size_t类型来实现。

```text
void *calloc(size_t number, size_t size) {
    size_t *new;
    size_t s8, i;
    new = malloc(number * size);
    if(new) {
        s8 = align8(number * size) >> 3;
        for(i = 0; i < s8; i++)
            new[i] = 0;
    }
    return new;
}
```

#### 3.2.7 free的实现

free的实现并不像看上去那么简单，这里我们要解决两个关键问题：

1. 如何验证所传入的地址是有效地址，即确实是通过malloc方式分配的数据区首地址
2. 如何解决碎片问题

首先我们要保证传入free的地址是有效的，这个有效包括两方面：

- 地址应该在之前malloc所分配的区域内，即在first_block和当前break指针范围内
- 这个地址确实是之前通过我们自己的malloc分配的

第一个问题比较好解决，只要进行地址比较就可以了，关键是第二个问题。这里有两种解决方案：一是在结构体内埋一个magic number字段，free之前通过相对偏移检查特定位置的值是否为我们设置的magic number，另一种方法是在结构体内增加一个magic pointer，这个指针指向数据区的第一个字节（也就是在合法时free时传入的地址），我们在free前检查magic pointer是否指向参数所指地址。这里我们采用第二种方案：

首先我们在结构体中增加magic pointer（同时要修改BLOCK_SIZE）：

```text
typedef struct s_block *t_block;
struct s_block {
    size_t size;  /* 数据区大小 */
    t_block next; /* 指向下个块的指针 */
    int free;     /* 是否是空闲块 */
    int padding;  /* 填充4字节，保证meta块长度为8的倍数 */
    void *ptr;    /* Magic pointer，指向data */
    char data[1]  /* 这是一个虚拟字段，表示数据块的第一个字节，长度不应计入meta */
};
```

然后我们定义检查地址合法性的函数：

```text
t_block get_block(void *p) {
    char *tmp;  
    tmp = p;
    return (p = tmp -= BLOCK_SIZE);
}
 
int valid_addr(void *p) {
    if(first_block) {
        if(p > first_block && p < sbrk(0)) {
            return p == (get_block(p))->ptr;
        }
    }
    return 0;
}
```

当多次malloc和free后，整个内存池可能会产生很多碎片block，这些block很小，经常无法使用，甚至出现许多碎片连在一起，虽然总体能满足某此malloc要求，但是由于分割成了多个小block而无法fit，这就是碎片问题。

一个简单的解决方式时当free某个block时，如果发现它相邻的block也是free的，则将block和相邻block合并。为了满足这个实现，需要将s_block改为双向链表。修改后的block结构如下：

```text
typedef struct s_block *t_block;
struct s_block {
    size_t size;  /* 数据区大小 */
    t_block prev; /* 指向上个块的指针 */
    t_block next; /* 指向下个块的指针 */
    int free;     /* 是否是空闲块 */
    int padding;  /* 填充4字节，保证meta块长度为8的倍数 */
    void *ptr;    /* Magic pointer，指向data */
    char data[1]  /* 这是一个虚拟字段，表示数据块的第一个字节，长度不应计入meta */
};
```

合并方法如下：

```text
t_block fusion(t_block b) {
    if (b->next && b->next->free) {
        b->size += BLOCK_SIZE + b->next->size;
        b->next = b->next->next;
        if(b->next)
            b->next->prev = b;
    }
    return b;
}
```

有了上述方法，free的实现思路就比较清晰了：首先检查参数地址的合法性，如果不合法则不做任何事；否则，将此block的free标为1，并且在可以的情况下与后面的block进行合并。如果当前是最后一个block，则回退break指针释放进程内存，如果当前block是最后一个block，则回退break指针并设置first_block为NULL。实现如下：

```text
void free(void *p) {
    t_block b;
    if(valid_addr(p)) {
        b = get_block(p);
        b->free = 1;
        if(b->prev && b->prev->free)
            b = fusion(b->prev);
        if(b->next)
            fusion(b);
        else {
            if(b->prev)
                b->prev->prev = NULL;
            else
                first_block = NULL;
            brk(b);
        }
    }
}
```

#### 3.2.8 realloc的实现

为了实现realloc，我们首先要实现一个内存复制方法。如同calloc一样，为了效率，我们以8字节为单位进行复制：

```text
void copy_block(t_block src, t_block dst) {
    size_t *sdata, *ddata;
    size_t i;
    sdata = src->ptr;
    ddata = dst->ptr;
    for(i = 0; (i * 8) < src->size && (i * 8) < dst->size; i++)
        ddata[i] = sdata[i];
}
```

然后我们开始实现realloc。一个简单（但是低效）的方法是malloc一段内存，然后将数据复制过去。但是我们可以做的更高效，具体可以考虑以下几个方面：

- 如果当前block的数据区大于等于realloc所要求的size，则不做任何操作
- 如果新的size变小了，考虑split
- 如果当前block的数据区不能满足size，但是其后继block是free的，并且合并后可以满足，则考虑做合并

下面是realloc的实现：

```text
void *realloc(void *p, size_t size) {
    size_t s;
    t_block b, new;
    void *newp;
    if (!p)
        /* 根据标准库文档，当p传入NULL时，相当于调用malloc */
        return malloc(size);
    if(valid_addr(p)) {
        s = align8(size);
        b = get_block(p);
        if(b->size >= s) {
            if(b->size - s >= (BLOCK_SIZE + 8))
                split_block(b,s);
        } else {
            /* 看是否可进行合并 */
            if(b->next && b->next->free
                    && (b->size + BLOCK_SIZE + b->next->size) >= s) {
                fusion(b);
                if(b->size - s >= (BLOCK_SIZE + 8))
                    split_block(b, s);
            } else {
                /* 新malloc */
                newp = malloc (s);
                if (!newp)
                    return NULL;
                new = get_block(newp);
                copy_block(b, new);
                free(p);
                return(newp);
            }
        }
        return (p);
    }
    return NU
｝
```

### 3.3 遗留问题和优化

以上是一个较为简陋，但是初步可用的malloc实现。还有很多遗留的可能优化点，例如：

- 同时兼容32位和64位系统
- 在分配较大快内存时，考虑使用mmap而非sbrk，这通常更高效
- 可以考虑维护多个链表而非单个，每个链表中的block大小均为一个范围内，例如8字节链表、16字节链表、24-32字节链表等等。此时可以根据size到对应链表中做分配，可以有效减少碎片，并提高查询block的速度
- 可以考虑链表中只存放free的block，而不存放已分配的block，可以减少查找block的次数，提高效率

还有很多可能的优化，这里不一一赘述。有兴趣的同学可以更深入研究。

原文地址：https://zhuanlan.zhihu.com/p/565379503

作者：linux

# 【NO.112】Linux网络设计之异步IO机制与io_uring

## 1.同步与异步

用于形容两者的关系，是同时存在的参考物。

同步： 所谓同步，就是发起一个请求时，在返回结果前，该调用不会返回。类似串行的概念。
异步： 异步的概念和同步相对，当发起一个请求时，该调用立刻返回，不等待结果，实际返回的结果由另外的线程 / 进程处理。类似并行的概念。

![img](https://pic3.zhimg.com/80/v2-9dc8313e96b46bcc4962197d49322726_720w.webp)

## 2.io_uring系统调用

io_uring从linix 5.1内核开始支持，但是到linix5.10后才达到比较好的支持，所以使用io_uring编程时，最好使用linix 5.10版本之后。升级linux内核可以[参考这里](https://link.zhihu.com/?target=https%3A//blog.csdn.net/Long_xu/article/details/126710992%3Fspm%3D1001.2014.3001.5501)。

内核提供三个接口，函数原型：

```text
#include <linux/io_uring.h>

int io_uring_setup(u32 entries,struct io_uring_params *p);

int io_uring_register(unsigned int fd,unsigned int opcode,void *arg,unsigned int nr_args);

int io_uring_entry(unsigned int fd,unsigned int to_submit,
	unsigned int min_complete,unsigned int flags,sigset_t *sig);
```

## 3.io_uring_setup

函数原型：

```text
#include <linux/io_uring.h>

int io_uring_setup(u32 entries,struct io_uring_params *params);
```

系统调用，设置提交队列（SQ）和完成队列（CQ），其中至少包含entries条目，并返回一个文件描述符，可用于对io_urine实例执行后续操作。SQ和CQ在用户空间和内核之间共享，这减少了在启动和完成I/O时复制数据的消耗。

| 参数    | 含义                                                         |
| ------- | ------------------------------------------------------------ |
| entries | 队列元素个数                                                 |
| params  | 配置io_uring，向内核传递选项，内核使用params传递有关环形缓冲区的信息。 |

成功时返回新的文件描述符。然后，应用程序可以在随后的mmap系统调用中提供文件描述符，以映射提交队列（submission queues）和完成队列（completion queues），或者传给io_uring_register() / io_uring_enter()系统调用。

出现错误时，返回负错误代码。调用方不应依赖errno变量。

![img](https://pic3.zhimg.com/80/v2-0e691a060beaf716169bd7f7ff7ffab2_720w.webp)

## 4.io_uring_register

函数原型：

```text
#include <linux/io_uring.h>

int io_uring_register(unsigned int fd,unsigned int opcode,void *arg,unsigned int nr_args);
```

注册用于异步 I/O 的文件或用户缓冲区，使内核能长时间持有对该文件在内核内部的数据结构引用， 或创建应用内存的长期映射， 这个操作只会在注册时执行一次，而不是每个 I/O 请求都会处理，因此大大减少了每个IO的开销。

![img](https://pic4.zhimg.com/80/v2-40087b1df6b16642e501476304e3efeb_720w.webp)

成功时返回0。
出现错误时，返回负错误代码。调用方不应依赖errno变量。

## 5.io_uring_enter

```text
#include <linux/io_uring.h>

int io_uring_enter(unsigned int fd,unsigned int to_submit,
	unsigned int min_complete,unsigned int flags,sigset_t *sig);
```

这个系统调用使用共享的 SQ 和 CQ初始化和完成（initiate and complete）I/O。
单次调用同时执行：
提交新的 I/O 请求；
等待 I/O 完成。

成功返回使用的I/O数量。如果to_submit为零或提交队列为空，则该值可以为零。注意，如果创建环时指定了IORING_SETUP_SQPOLL，则返回值通常与to_submit相同，因为提交发生在系统调用的上下文之外。

与提交队列条目相关的错误将通过完成队列条目返回，而不是通过系统调用本身返回。
不代表提交队列条目发生的错误将通过系统调用直接返回。在出现这种错误时，返回负错误代码。调用方不应依赖errno变量。
更多信息可以执行 **man io_uring_enter** 查看

## 6.struct io_uring_params 结构体

```text
struct io_uring_params {
               __u32 sq_entries;
               __u32 cq_entries;
               __u32 flags;
               __u32 sq_thread_cpu;
               __u32 sq_thread_idle;
               __u32 features;
               __u32 wq_fd;
               __u32 resv[3];
               struct io_sqring_offsets sq_off;
               struct io_cqring_offsets cq_off;
           };
```

flags、sq_thread_cpu和sq_ thread_idle字段用于配置io_uring实例。flags是一个位掩码，其中0个或多个以下值一起或：

![img](https://pic4.zhimg.com/80/v2-ab82f5c407f0e564b13367769fecbaf7_720w.webp)



## 7.struct io_cqring_offsets 结构体

```text
struct io_cqring_offsets {
               __u32 head;
               __u32 tail;
               __u32 ring_mask;
               __u32 ring_entries;
               __u32 overflow;
               __u32 cqes;
               __u32 flags;
               __u32 resv[3];
           };
```

## 8.struct io_uring_sqe 结构体

```text
struct io_uring_sqe {
               __u8    opcode;         /* type of operation for this sqe */
               __u8    flags;          /* IOSQE_ flags */
               __u16   ioprio;         /* ioprio for the request */
               __s32   fd;             /* file descriptor to do IO on */
               union {
                   __u64   off;            /* offset into file */
                   __u64   addr2;
               };
               union {
                   __u64   addr;       /* pointer to buffer or iovecs */
                   __u64   splice_off_in;
               }
               __u32   len;            /* buffer size or number of iovecs */
               union {
                   __kernel_rwf_t  rw_flags;
                   __u32    fsync_flags;
                   __u16    poll_events;   /* compatibility */
                   __u32    poll32_events; /* word-reversed for BE */
                   __u32    sync_range_flags;
                   __u32    msg_flags;
                   __u32    timeout_flags;
                   __u32    accept_flags;
                   __u32    cancel_flags;
                   __u32    open_flags;
                   __u32    statx_flags;
                   __u32    fadvise_advice;
                   __u32    splice_flags;
                   __u32    rename_flags;
                   __u32    unlink_flags;
                   __u32    hardlink_flags;
               };
				__u64    user_data;     /* data to be passed back at completion time */
               union {
               struct {
                   /* index into fixed buffers, if used */
                       union {
                           /* index into fixed buffers, if used */
                           __u16    buf_index;
                           /* for grouped buffer selection */
                           __u16    buf_group;
                       }
                   /* personality to use, if used */
                   __u16    personality;
                   union {
                       __s32    splice_fd_in;
                       __u32    file_index;
                };
               };
               __u64    __pad2[3];
               };
           };
```

## 9.struct io_uring_cqe 结构体

```text
struct io_uring_cqe {
               __u64    user_data; /* sqe->data submission passed back */
               __s32    res;       /* result code for this event */
               __u32    flags;
           };
```

## 10.执行流程

![img](https://pic4.zhimg.com/80/v2-c4a0984f6ddfba7ab41bd70b572b9f13_720w.webp)

## 11.liburing库

### 11.1 安装

（1）下载源码

```text
git clone https://github.com/axboe/liburing.git
```

（2）进入liburing

```text
cd liburing
```

（3）配置

```text
./configure
```

（4）编译和安装

```text
make && sudo make install
```

（5）编译应用程序
一定要指定库 -luring -D_GUN_SOURCE

```text
gcc -o io_uring_test io_uring_test.c -luring -D_GUN_SOURCE
```

### 11.2 liburing提供的接口

**io_uring_queue_init_params**

函数原型：

```text
#include <liburing.h>

int io_uring_queue_init(unsigned entries,
                       struct io_uring *ring,
                       unsigned flags);

int io_uring_queue_init_params(unsigned entries,
                              struct io_uring *ring,
                              struct io_uring_params *params);
```

io_uring_queue_init()函数执行io_uring_setup()系统调用来初始化内核中的提交队列和完成队列，其中至少包含提交队列中的条目，然后将生成的文件描述符映射到应用程序和内核之间共享的内存中。

默认情况下，CQ环的条目数将是SQ环条目指定的条目数的两倍。这对于常规文件或存储工作负载足够，但对于网络工作负载可能太小。SQ环条目并没有限制环可以支持的进程中请求的数量，它只是限制了一次（批）提交给内核的数量。如果CQ环溢出，例如，在应用程序可以获取它们之前，生成的条目比环中的条目多，则环进入CQ环溢流状态。这通过在SQ环标志中设置IORING_SQ_CQ_OVERFLOW来指示。除非内核耗尽可用内存，否则不会删除条目，但这是一条慢得多的完成路径，会减慢请求处理速度。因此，应该避免这种情况，并且CQ环的大小适合于工作负载。在struct io_uring_params中设置cq_entries将告诉内核为cq环分配这么多条目，与给定条目中的SQ环大小无关。如果该值不是2的幂，则将四舍五入到最接近的2的幂。

成功时，io_uring_queue_init返回0，ring将指向包含io_RUING队列的共享内存。失败时返回-errno。flags将传递给io_uring_setup系统调用。

如果使用io_uring_queue_init_params()，则params指示的参数将直接传递到io_uring_setup系统调用。成功后返回0，应通过对io_uring_queue_exit的相应调用释放ring持有的资源。失败时返回-errno。

**io_uring_get_sqe**

函数原型：

```text
#include <liburing.h>

struct io_uring_sqe *io_uring_get_sqe(struct io_uring *ring);
```

从属于ring参数的提交队列中获取下一个可用的提交队列条目。
成功时返回一个指向提交队列条目的指针。
失败时返回NULL。

**如果返回了提交队列条目，则应通过io_uring_prep_read()等准备函数之一填写该条目，并通过io_ uring_submit()提交。**
**如果返回NULL，则SQ环当前已满，必须提交条目进行处理，然后才能分配新条目。**

**io_uring_prep_accept**

函数原型：

```text
#include <sys/socket.h>
#include <liburing.h>

void io_uring_prep_accept(struct io_uring_sqe *sqe,
                          int sockfd,
                          struct sockaddr *addr,
                          socklen_t *addrlen,
                          int flags);

void io_uring_prep_accept_direct(struct io_uring_sqe *sqe,
                                 int sockfd,
                                 struct sockaddr *addr,
                                 socklen_t *addrlen,
                                 int flags,
                                 unsigned int file_index);

void io_uring_prep_multishot_accept(struct io_uring_sqe *sqe,
                                    int sockfd,
                                    struct sockaddr *addr,
                                    socklen_t *addrlen,
                                    int flags);

void io_uring_prep_multishot_accept_direct(struct io_uring_sqe *sqe,
                                           int sockfd,
                                           struct sockaddr *addr,
                                           socklen_t *addrlen,
                                           int flags);
```

这些函数准备一个异步accept()请求。
io_uring_prep_accept()函数准备接受请求。提交队列条目sqe被设置为使用文件描述符sockfd开始接受由addr处的套接字地址和结构长度addrlen描述的连接请求，并在标志中使用修饰符标志。

注意：io_uring_prep_accept()与在结构中传递数据的任何请求一样，在成功提交请求之前，该数据必须保持有效。它不需要在完成之前保持有效。一旦请求被提交，内核状态就稳定了。

**io_uring_prep_recv**

函数原型：

```text
#include <liburing.h>

void io_uring_prep_recv(struct io_uring_sqe *sqe,
                        int sockfd,
                        void *buf,
                        size_t len,
                        int flags);

void io_uring_prep_recv_multishot(struct io_uring_sqe *sqe,
                                  int sockfd,
                                  void *buf,
                                  size_t len,
                                  int flags);
```

描述：
io_uring_prep_recv()函数准备recv请求。提交队列条目sqe被设置为使用文件描述符sockfd来开始将数据接收到大小为size且具有修改标志flags的缓冲区目的地buf中。
此函数用于准备异步recv()请求。
multishot版本允许应用程序发出单个接收请求，当数据可用时，该请求会重复发布CQI。

**io_uring_prep_send**

函数原型：

```text
#include <liburing.h>

void io_uring_prep_send(struct io_uring_sqe *sqe,
                        int sockfd,
                        const void *buf,
                        size_t len,
                        int flags);
```

描述：
io_uring_prep_send()函数准备发送请求。提交队列条目sqe被设置为使用文件描述符sockfd开始从buf发送大小为size的数据，并带有修改标志flags。
此函数用于准备异步send()请求。

**io_uring_submit (重要)**

函数原型：

```text
#include <liburing.h>

int io_uring_submit(struct io_uring *ring);
```

描述：
将下一个事件提交到属于ring的提交队列。
调用者使用io_uring_get_sqe(）检索提交队列条目（SQE）并使用提供的帮助程序之一准备SQE后，可以使用io_ uring_ submit()提交。

返回值：
成功时返回提交的提交队列条目数。
失败时返回-errno。

**io_uring_submit_and_wait (重要)**

函数原型：

```text
#include <liburing.h>

int io_uring_submit_and_wait(struct io_uring *ring,unsigned wait_nr);
```

描述：
将下一个事件提交到属于环的提交队列，并等待wait_nr完成事件。
在调用方使用io_uring_get_sqe(）检索提交队列条目（SQE）并准备SQE之后，可以使用io_ uring_ submit_and_wait()提交它。

返回值：
成功时返回提交的提交队列条目数。
失败时返回-errno。

**io_uring_wait_cqe**

函数原型：

```text
#include <liburing.h>

int io_uring_wait_cqe(struct io_uring *ring,
                      struct io_uring_cqe **cqe_ptr);
```

描述：
等待来自属于环参数的队列的io完成，必要时等待。如果调用时环中已有事件可用，则不会发生等待。成功时填写cqe_ptr参数。
在呼叫者提交了具有io_uring_submit()的请求之后，应用程序可以使用io_uring_wait_cqe检索完成。

返回值：
成功时返回0，并填写cqe_ptr参数。失败时返回-errno。返回值指示等待CQE的结果，并且与CQE结果本身无关。

**io_uring_peek_batch_cqe**

```text
#include <liburing.h>

int io_uring_peek_cqe(struct io_uring *ring,
                      struct io_uring_cqe **cqe_ptr);
                      
int io_uring_peek_batch_cqe(struct io_uring *ring,
                      struct io_uring_cqe **cqe_ptr,
                      int count);
```

描述：
io_uring_peek_cqe()函数从属于ring参数的队列返回IO完成（如果有）。成功返回后，cqe_ptr参数将填充有效的cqe条目。
此函数不进入内核等待事件，只有在CQ环中已经可用时才返回事件。

io_uring_peek_batch_cqe是对io_uring_peek_cqe的封装，表示一次最多从ring参数获取count个IO完成。
返回值：
成功时io_uring_peek_cqe()返回0，并填写cqe_ptr参数。失败时返回-EAGAIN。
成功时io_uring_peek_cqe()返回IO完成数量，并填写cqe_ptr参数。失败时返回-EAGAIN。

**io_uring_cq_advance**

```text
#include <liburing.h>

void io_uring_cq_advance(struct io_uring *ring,
                          unsigned nr);
```

描述：
io_uring_cq_advance()函数将属于ring参数的nr个io完成标记为消耗。

在呼叫者已经使用io_uring_submit()提交请求之后，应用程序可以使用io_ uring_ wait_cqe()、io_uring_peek_cqe()或任何其他cqe检索帮助器检索完成，并使用io_uring_cqe_seen()将其标记为。
函数io_uring_cqe_seen()调用io_ uring_cq_advance()函数。

**完成必须标记为可见，以便可以重用它们的插槽。否则将导致在下一次调用时返回相同的完成。**

### 11.3 更多接口

除了bind没有异步接口，其他基本都有。比如io_uring_prep_connect()、io_uring_prep_close()等等。

## 12.基于liburing的TCP服务器实现

在应用层使用io_uring，主要使用liburing库，它提供丰富的用户接口，底层调用的是三个内核io_uring系统调用。

基于liburing的TCP服务器实现示例代码：

```text
#include <stdio.h>
#include <unistd.h>

#include <sys/socket.h>
#include <netinet/in.h>

#include <string.h>
#include <liburing.h>

#define ENTRIES_LENGTH	4096
#define RING_CQE_NUMBER	10

#define BUFFER_SIZE		1024

struct conninfo
{
	int connfd;
	int type;
};

enum 
{
	READ,
	WRITE,
	ACCPT,
};


void set_accept_event(struct io_uring *ring,int fd, 
struct sockaddr* clientaddr, socklen_t *len,unsigned flags)
{
	struct io_uring_sqe *sqe = io_uring_get_sqe(ring);//·µ»Ø¶ÓÁÐÊ×µØÖ·
	

	io_uring_prep_accept(sqe, fd, clientaddr, len, flags);

	struct conninfo ci = {
		.connfd = fd,
		.type = ACCPT
	};

	memcpy(&sqe->user_data, &ci, sizeof(struct conninfo));
}

void set_read_event(struct io_uring *ring, int fd, void *buf, size_t len, int flags)
{
	struct io_uring_sqe *sqe = io_uring_get_sqe(ring);//·µ»Ø¶ÓÁÐÊ×µØÖ·


	io_uring_prep_recv(sqe, fd, buf, len, flags);

	struct conninfo ci = {
		.connfd = fd,
		.type = READ
	};

	memcpy(&sqe->user_data, &ci, sizeof(struct conninfo));
}

void set_write_event(struct io_uring *ring, int fd, void *buf, size_t len, int flags)
{
	struct io_uring_sqe *sqe = io_uring_get_sqe(ring);//·µ»Ø¶ÓÁÐÊ×µØÖ·


	io_uring_prep_send(sqe, fd, buf, len, flags);

	struct conninfo ci = {
		.connfd = fd,
		.type = WRITE
	};

	memcpy(&sqe->user_data, &ci, sizeof(struct conninfo));
}

int main()
{
	int listenfd = socket(AF_INET, SOCK_STREAM, 0);
	if (listenfd == -1)
		return -1;

	struct sockaddr_in serveraddr,clientaddr;
	serveraddr.sin_family = AF_INET;
	serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);
	serveraddr.sin_port = htons(9999);

	if (-1 == bind(listenfd, (struct sockaddr*)&serveraddr, sizeof(serveraddr)))
		return -2;

	listen(listenfd, 10);

	struct io_uring_params params;
	memset(&params,0,sizeof(params));
	struct io_uring ring;
	io_uring_queue_init_params(ENTRIES_LENGTH,&ring,&params);

	socklen_t clientlen = sizeof(clientaddr);
	
	set_accept_event(&ring, listenfd, (struct sockaddr*)&clientaddr,&clientlen, 0);

	char buffer[BUFFER_SIZE] = { 0 };

	while (1)
	{
		struct io_uring_cqe *cqe;

		io_uring_submit(&ring);
		int ret = io_uring_wait_cqe(&ring,&cqe);

		struct io_uring_cqe *cqes[RING_CQE_NUMBER];
		int cqecount=io_uring_peek_batch_cqe(&ring, cqes, RING_CQE_NUMBER);

		int i = 0;
		unsigned count =0;
		for (i = 0; i < cqecount; i++)
		{
			count++;
			cqe = cqes[i];
			struct conninfo ci;
			memcpy(&ci, &cqe->user_data, sizeof(struct conninfo));
			if (ci.type == ACCPT)
			{
				int connfd = cqe->res;
				
				set_read_event(&ring, connfd, buffer, BUFFER_SIZE, 0);
				
				set_accept_event(&ring, listenfd, (struct sockaddr*)&clientaddr,&clientlen, 0);
			}
			else if (ci.type == READ)
			{
				int bufsize = cqe->res;
				if(bufsize==0)
				{
					close(ci.connfd);
				}
				else if(bufsize<0)
				{
				
				}
				else{
				//set_read_event(&ring, ci.connfd, buffer, 1024, 0);
					printf("buff: %s\n",buffer);
					set_write_event(&ring, ci.connfd, buffer, bufsize, 0);
				}
			}
			else if(ci.type == WRITE)
			{
				set_read_event(&ring, ci.connfd, buffer, BUFFER_SIZE, 0);
			}
			
		}
		io_uring_cq_advance(&ring,count);
	}

	return 0;
}
```

## 13.总结

io_uring主要由三部分构成：内核提供的三个系统调用接口（io_uring_setup、io_uring_register、io_uring_enter），内核实现的系统调用，应用层提供的liburing库。

io_uring_submite会在底层协议栈执行accept、recv、send等功能。从fio测试磁盘IO的测试结果来看，io_uring的IOPS与libaio相同，是psync的两倍。

io_uring的异步操作在内核下完成，用户态调用api是感觉不到异步操作的。

io_uring实现TCP服务器的函数调用框图：

![img](https://pic3.zhimg.com/80/v2-416fecfe9f6c04883f139436978ecaea_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/565095977

作者：linux

# 【NO.113】glibc malloc源码分析

## 1.**malloc**

本文梳理了一下malloc跟free的源码。malloc()函数在源代码中使用宏定义为public_mALLOc()。public_mALLOc()函数只是简单的封装_int_malloc()函数，_int_malloc()函数才是内存分配的核心实现。

### 1.1**public_mALLOc()**

```text
Void_t* public_mALLOc(size_t bytes) 
{   
    mstate ar_ptr;   
    Void_t *victim; 
    __malloc_ptr_t (*hook) (size_t, __const __malloc_ptr_t)    
    = force_reg (__malloc_hook);   
    if (__builtin_expect (hook != NULL, 0))
    return (*hook)(bytes, RETURN_ADDRESS (0));
```

首先检查是否存在__malloc_hook。如果存在，则调用hook函数。注意hook函数的传参为请求分配的内存大小。

```text
 arena_lookup(ar_ptr);   
    arena_lock(ar_ptr, bytes);   
    if(!ar_ptr)     
        return 0;   
    victim = _int_malloc(ar_ptr, bytes);
```

获取分配区指针，如果获取分配区失败，返回退出，否则，调用_int_malloc()函数分配内存。

```text
    if(!victim) 
    {     
        /* Maybe the failure is due to running out of mmapped areas. */
        if(ar_ptr != &main_arena)
        {       
            (void)mutex_unlock(&ar_ptr->mutex);       
            ar_ptr = &main_arena;       
            (void)mutex_lock(&ar_ptr->mutex);       
            victim = _int_malloc(ar_ptr, bytes);                      
            (void)mutex_unlock(&ar_ptr->mutex);
        }
```

如果_int_malloc()函数分配内存失败，并且使用的分配区不是主分配区，这种情况可能是mmap区域的内存被用光了。如果目前主分配区还可以从堆中分配内存，则需要再尝试从主分配区中分配内存。首先释放所使用分配区的锁，然后获得主分配区的锁，并调用_int_malloc()函数分配内存，最后释放主分配区的锁。

```text
        else 
        { 
#if USE_ARENAS       
            /* ... or sbrk() has failed and there is still a chance to mmap() */
            ar_ptr = arena_get2(ar_ptr->next ? ar_ptr : 0, bytes);                   
            (void)mutex_unlock(&main_arena.mutex);       
            if(ar_ptr) 
            {         
                victim = _int_malloc(ar_ptr, bytes);                     
                (void)mutex_unlock(&ar_ptr->mutex);       
            }
#endif
        }
    }
```

如果_int_malloc()函数分配内存失败，并且使用的分配区是主分配区，查看是否有非主分配区，如果有，调用arena_get2()获取分配区，然后对主分配区解锁，如果arena_get2()返回一个非分配区，尝试调用_int_malloc()函数从该非主分配区分配内存，最后释放该非主分配区的锁。

```text
    else 
        (void)mutex_unlock(&ar_ptr->mutex);
    assert(!victim || chunk_is_mmapped(mem2chunk(victim)) || ar_ptr ==      
    arena_for_chunk(mem2chunk(victim)));   
    return victim; 
} 
```

如果_int_malloc()函数分配内存成功，释放所使用的分配区的锁。返回分配的chunk。

### 1.2**_int_malloc**

_int_malloc函数是内存分配的核心，根据分配的内存块的大小，该函数中实现了了四种分配内存的路径。先给出_int_malloc()函数的定义及临时变量的定义：

```text
static Void_t* _int_malloc(mstate av, size_t bytes) 
{   
    INTERNAL_SIZE_T nb;               /* normalized request size */
    unsigned int    idx;              /* associated bin index */
    mbinptr         bin;              /* associated bin */

    mchunkptr       victim;           /* inspected/selected chunk */
    INTERNAL_SIZE_T size;             /* its size */
    int             victim_index;     /* its bin index */
 
    mchunkptr       remainder;        /* remainder from a split */
    unsigned long   remainder_size;   /* its size */

    unsigned int    block;            /* bit map traverser */
    unsigned int    bit;              /* bit map traverser */
    unsigned int    map;              /* current word of binmap */
 
    mchunkptr       fwd;              /* misc temp for linking */
    mchunkptr       bck;              /* misc temp for linking */
 
 
  const char *errstr = NULL; 
 
  /*
     Convert request size to internal form by adding SIZE_SZ bytes
     overhead plus possibly more to obtain necessary alignment and/or
     to obtain a size of at least MINSIZE, the smallest allocatable
     size. Also, checked_request 2size traps (returning 0) request sizes
     that are so large that they wrap around zero when padded and
     aligned.
   */
```

checked_request2size()函数将需要分配的内存大小bytes转换为需要分配的chunk大小nb，Ptmalloc内部分配都是以chunk为单位，根据chunk的大小，决定如何获得满足条件的chunk。

### 1.3**分配fast bin chunk**

```text
 /*
     If the size qualifies as a fastbin, first check corresponding bin.
     This code is safe to execute even if av is not yet initialized, so we
     can try it without checking, which saves some time on this fast path.
   */
    if ((unsigned long)(nb) <= (unsigned long)(get_max_fast ())) 
    {     
        idx = fastbin_index(nb);     
        mfastbinptr* fb = &fastbin (av, idx); 
#ifdef ATOMIC_FASTBINS     
        mchunkptr pp = *fb;     
        do       
        {         
            victim = pp;         
            if (victim == NULL)           
            break;       
        } while ((pp = catomic_compare_and_exchange_val_acq (fb, victim->fd, victim))            != victim); 
#else     
        victim = *fb; 
#endif     
        if (victim != 0) 
        {       
            if (__builtin_expect (fastbin_index (chunksize (victim)) != idx, 0))                 
            {           
                errstr = "malloc(): memory corruption (fast)";         
            errout:           
                malloc_printerr (check_action, errstr, chunk2mem (victim));                   
                return NULL;         
            } 
#ifndef ATOMIC_FASTBINS       
            *fb = victim->fd; 
#endif       
            check_remalloced_chunk(av, victim, nb);       
            void *p = chunk2mem(victim);       
            if (__builtin_expect (perturb_byte, 0))         
                alloc_perturb (p, bytes);       
            return p;     
        }       
    } 
```

如果所需的chunk大小小于等于fast bins中的最大chunk大小，首先尝试从fast bin中分配chunk。
1.根据所需的chunk大小获得该chunk所属的fast bin的index，根据该index获得所属fast bin的空闲chunk链表头指针。
如果没有开启ATOMIC_FASTBINS优化，则按以下步骤：
2.将头指针的下一个chunk作为空闲chunk链表的头部。
3.取出第一个chunk，并调用chunk2mem()函数返回用户所需的内存块。
如果开启了ATOMIC_FASTBINS优化，则步骤与上述类似，只是在删除fastbin头节点的时候使用了lock-free技术，加快了分配速度。

**check**

fastbin分配对size做了检查，如果分配chunk的size不等于分配时的idx，就会报错。使用chunksize()和fastbin_index函数计算chunk的size大小，所以我们无需管size的后三位(size_sz=8的情况下无需管后四位)，只需保证前几位与idx相同即可。

### 1.4**分配small bin chunk**

```text
 /*
     If a small request, check regular bin.  Since these "smallbins"
     hold one size each, no search ing within bins is necessary.
     (For a large request, we need to wait until unsorted chunks are
     processed to find best fit. But for small ones, fits are exact
     anyway, so we can check now, which is faster.)
   */
    if (in_smallbin_range(nb)) 
    {     
        idx = smallbin_index(nb);     
        bin = bin_at(av,idx); 
 
        if ( (victim = last(bin)) != bin) 
        {       
            if (victim == 0) /* initialization check */
                malloc_consolidate(av);       
            else 
            {         
                bck = victim->bk;         
                if (__builtin_expect (bck->fd != victim, 0))           
                {             
                    errstr = "malloc(): smallbin double linked list corrupted";                     
                    goto errout;           
                }         
                set_inuse_bit_at_offset(victim, nb);         
                bin->bk = bck;         
                bck->fd = bin; 
 
                if (av != &main_arena)           
                    victim->size |= NON_MAIN_ARENA; 
                check_malloced_chunk(av, victim, nb);         
                void *p = chunk2mem(victim);         
                if (__builtin_expect (perturb_byte, 0))           
                    alloc_perturb (p, bytes);         
                return p;       
            }     
        }   
    } 
```

如果所需的chunk大小属于small bin，则会执行如下步骤：
1.查找chunk对应small bins数组的index，根据index获得某个small bin的空闲chunk双向循环链表表头。
2.将最后一个chunk赋值给victim，如果victim与表头相同，表示该链表为空，不能从small bin的空闲chunk链表中分配，这里不做处理，等后面的步骤来处理。
3.victim与表头不同有两种情况。

- victim为0
  1.表示small bin还没有初始化为双向循环链表，调用malloc_consolidete()函数将fast bins中的chunk合并。
- victim不为0
  1.设置victim chunk的inuse标志，该标志处于vimctim chunk的下一个相邻chunk的size字段的第一个bit。
  2.做与unlink类似的操作将chunk从small bin中脱链。

4.判断当前分配区是否属于非主分配区，如果是，将victim chunk的size字段中的标志非主分配区的标志bit清零。
5.调用chunk2mem()函数获得chunk的实际可用的内存指针，将该内存指针返回给应用层。

**check**

申请的chunk需满足chunk->bk->fd = chunk

### 1.6**分配large bin chunk**

```text
 /*
      If this is a large request, consolidate fastbins before continuing.
      While it might look excessive to kill all fastbins before
      even seeing if there is space available, this avoids
      fragmentation problems normally associated with fastbins.
      Also, in practice, programs tend to have runs of either small or
      large requests, but less often mixtures, so consolidation is not
      invoked all that often in most programs. And the programs that
      it is called frequently in otherwise tend to fragment.
   */
 
    else 
    {     
        idx = largebin_index(nb);     
        if (have_fastchunks(av))       
        malloc_consolidate(av);   
    } 
```

所需chunk不属于small bins，那么就在large bins的范围内，则

1.根据chunk的大小获得对应large bin的index

2.判断当前分配区的fast bins中是否包含chunk，如果存在，调用malloc_consolidate()函数合并fast bins中的chunk，并将这些空闲chunk加入unsorted bin中。

下面的源代码实现从last remainder chunk，large bins和top chunk中分配所需的chunk，这里包含了多个多层循环，在这些循环中，主要工作是分配前两步都未分配成功的small bin chunk、large bin chunk和large chunk。最外层的循环用于重新尝试分配small bin chunk，因为如果在前一步分配smallbin chunk不成功，并没有调用malloc_consolidate()函数合并fast bins中的chunk，将空闲chunk加入unsorted bin中，如果第一尝试从last remainder chunk、top chunk中分配small bin chunk都失败以后，如果fast bins中存在空闲chunk，会调用malloc_consolidate()函数，那么在usorted bin中就可能存在合适的small bin chunk供分配，所以需要再次尝试。

```text
 /*
     Process recently freed or remaindered chunks, taking one only if
     it is exact fit, or, if this a small request, the chunk is remainder from
     the most recent non - exact fit.  Place other traversed chunks in
     bins.  Note that this step is the onl y place in any routine where
     chunks are placed in bins.
 
 
    The outer loop here is needed because we might not realize until
     near the end of malloc that we should have consolidated, so must
     do so and retry. This happens at most once, and only when we would
     otherwise need to expand memory to service a "small" request.
   */
    for(;;) 
    {     
        int iters = 0; 
        while ( (victim = unsorted_chunks(av)->bk) != unsorted_chunks(av)) 
        { 
```

反向遍历unsorted bin的双向链表，遍历结束的条件是循环链表中只剩一个头节点。

```text
            bck = victim->bk;       
            if (__builtin_expect (victim->size <= 2 * SIZE_SZ, 0)           
            || __builtin_expect (victim->size > av->system_mem, 0))             
                malloc_printerr (check_action, "malloc(): memory corruption",                         
                chunk2mem (victim));       
            size = chunksize(victim);
```

1.检查当前遍历的chunk是否合法，chunk的大小不能小于等于2*SIZE_SZ，也不能超过该分配区总的内存分配量。

2.获取chunk的大小并赋值给size。

```text
          if (in_smallbin_range(nb) &&           
                bck == unsorted_chunks(av) &&           
                victim == av->last_remainder &&           
                (unsigned long)(size) > (unsigned long)(nb + MINSIZE)) 
            { 
```

如果需要分配一个small bin chunk，并且unsorted bin中只有一个chunk，并且这个chunk为last remainder chunk，并且这个chunk的大小大于所需chunk的大小加上MINSIZE，在满足这些条件的情况下，可以使用这个chunk切分出需要的small bin chunk。这是唯一的从unsorted bin中分配small bin chunk的情况。

```text
                /* split and reattach remainder */
                remainder_size = size - nb;         
                remainder = chunk_at_offset(victim, nb);         
                unsorted_chunks(av)->bk = unsorted_chunks(av)->fd = remainder;             
                av->last_remainder = remainder;         
                remainder->bk = remainder->fd = unsorted_chunks(av);         
                if (!in_smallbin_range(remainder_size))           
                {             
                    remainder->fd_nextsize = NULL;             
                    remainder->bk_nextsize = NULL;           
                } 
```

1.从chunk中切分出所需大小的chunk。

2.计算切分后剩下chunk的大小，将剩下的chunk加入unsorted bin的链表中，并将剩下的chunk作为分配区的last remainder chunk。

3.如果剩下的chunk属于large bin chunk，将该chunk的fd_nextsize和bk_nextsize设置为NULL，因为这个chunk仅仅存在于unsorted bin中，并且unsorted bin中有且仅有这一个chunk。

```text
                set_head(victim, nb | PREV_INUSE |                  
                (av != &main_arena ? NON_MAIN_ARENA : 0));         
                set_head(remainder, remainder_size | PREV_INUSE);         
                set_foot(remainder, remainder_size); 
                check_malloced_chunk(av, victim, nb);         
                void *p = chunk2mem(victim);         
                if (__builtin_expect (perturb_byte, 0))           
                    alloc_perturb (p, bytes);         
                return p；
            }
```

设置分配出的chunk和last remainder chunk的相关信息。，调用chunk2mem()获得chunk中可用的内存指针，返回给应用层，退出。

```text
        /* remove from unsorted list */
            unsorted_chunks(av)->bk = bck;       
            bck->fd = unsorted_chunks(av); 
```

将双向循环链表中的最后一个chunk移除。

```text
if (size == nb) 
            {         
                set_inuse_bit_at_offset(victim, size);         
                if (av != &main_arena)           
                    victim->size |= NON_MAIN_ARENA;         
                check_malloced_chunk(av, victim, nb);         
                void *p = chunk2mem(victim);         
                if (__builtin_expect (perturb_byte, 0))           
                    alloc_perturb (p, bytes);         
                return p;       
             }
```

如果当前chunk与所需的chunk大小一致
1.设置当前chunk处于inuse状态
2.设置分配区标志位
3.调用chunk2mem()获得chunk中的可用内存指针，返回给应用层。

```text
/* place chunk in bin */
            if (in_smallbin_range(size)) 
            {         
                victim_index = smallbin_index(size);         
                bck = bin_at(av, victim_index);         
                fwd = bck->fd;
            }
```

如果当前chunk属于small bins，获得当前chunk所属small bin的index，并将该small bin的链表表头复制给bck，第一个chunk赋值给fwd，也就是当前的chunk会插入到bck和fwd之间，作为small bin比链表的第一个chunk。

```text
else 
             {         
                victim_index = largebin_index(size);         
                bck = bin_at(av, victim_index);         
                fwd = bck->fd;
```

如果当前chunk属于large bins，获得当前chunk所属large bin的index，并将该large bin的链表表头赋值给bck，第一个chunk赋值给fwd，也就是当前的chunk会插入到bck和fwd之间，作为large bin链表的第一个chunk。

```text
/* maintain large bins in sorted order */
                 if (fwd != bck) 
                 {           
                    /* Or with inuse bit to speed comparisons */
                     size |= PREV_INUSE;           
                    /* if smalle r than smallest, bypass loop below */
                     assert((bck->bk->size & NON_MAIN_ARENA) == 0);
```

如果fwd不等于bck，意味着large bin中有空闲chunk存在，由于large bin中的空闲chunk是按照大小排序的，需要将当前从unsorted bin中取出的chunk插入到large bin中合适的位置。将当前的chunk的size的inuse标志bit置位，相当于加1，便于加快chunk大小的比较，找到合适的地方插入当前chunk。

```text
if ((unsigned long)(size) < (unsigned long)(bck->bk->size)) 
                    {             
                        fwd = bck;             
                        bck = bck->bk;             
                        victim->fd_nextsize = fwd->fd;             
                        victim->bk_nextsize = fwd->fd->bk_nextsize;             
                        fwd->fd->bk_nextsize = victim->bk_nextsize->fd_nextsize = victim;
                    }
```

如果当前的chunk比large bin的最后一个chunk的大小还小，那么当前chunk1就插入到large bin的链表的最后，作为最后一个chunk。可以看出large bin中的chunk是按照从大到小的顺序排序的，同时一个chunk存在于两个双向循环链表中，一个链表包含了large bin中所有的chunk，另一个链表为chunk size链表，该链表从每个相同大小的chunk中取除第一个chunk按照大小顺序链接在一起，便于一次跨域多个相同大小的chunk遍历下一个不同大小的chunk，这样可以加快在large bin链表中的遍历速度。

```text
else
                    {
                        assert((fwd->size & NON_MAIN_ARENA) == 0);            
                        while ((unsigned long) size < fwd->size)               
                        {                 
                            fwd = fwd->fd_nextsize;                 
                            assert((fwd->size & NON_MAIN_ARENA) == 0);               
                        }
```

正向遍历chunk size链表，直到找到第一个chunk大小小于等于当前chunk大小的chunk退出循环。

```text
if ((unsigned long) size == (unsigned long) fwd->size)               
                            /* Always insert in the second position.  */
                            fwd = fwd->fd;
```

如果从large bin链表中找到了与当前chunk大小相同的chunk，则统一大小的chunk已经存在，那么chunk size一定包含了fwd指向的chunk，为了不久改chunk size链表，当前chunk只能插入fwd之后。

```text
else               
                        {                 
                            victim->fd_nextsize = fwd; 
                            victim->bk_nextsize = fwd->bk_nextsize;                 
                            fwd->bk_nextsize = victim;                                 
                            victim->bk_nextsize->fd_nextsize = victim;
                        }
```

如果chunk size链表中还没有包含当前chunk大小的chunk，也就是说当前chunk的大小大于fwd的大小，则将当前chunk作为该chunk size的代表加入chunk size链表，chunk size链表也是按照由大到小的顺序排序。

```text
bck = fwd->bk;           
                    }         
                }
                else           
                    victim->fd_nextsize = victim->bk_nextsize = victim;
            }
```

如果large bin链表中没有chunk，直接将当前chunk加入chunk size链表。

```text
mark_bin(av, victim_index);       
            victim->bk = bck;       
            victim->fd = fwd;       
            fwd->bk = victim;       
            bck->fd = victim;
```

将当前chunk插入到对应的空闲的chunk链表中，并将large bin所对应binmap的相应bit置位。

```text
#define MAX_ITERS    10000       
            if (++iters >= MAX_ITERS)         
                break；
        }
```

如果unsorted bin中的chunk超过了10000个，最多遍历一万个就退出，避免长时间处理unsorted bin影响内存分配的效率。
当unsorted bin中的空闲chunk加入到相应的small bins和large bins后，将使用最佳匹配法分配large bin chunk。

```text
/*
           If a large request, scan through the chunks of current bin in
           sorted order to find smallest that fits.  Use the skip list for this.
         */
         if (!in_smallbin_range(nb)) 
        {       
            bin = bin_at(av, idx); 
 
           /* skip scan if empty or largest chunk is too small */
            if ((victim = first(bin)) != bin &&           
                (unsigned long)(victim->size) >= (unsigned long)(nb)) 
            {
```

如果所需分配的chunk为large bin chunk，查询对应的large bin链表，如果large bin链表为空，或者链表中最大的chunk也不能满足要求，则不能从large bin中分配。否则，遍历large bin链表，找到合适的chunk。

```text
victim = victim->bk_nextsize;         
                while (((unsigned long)(size = chunksize(victim)) <                 
                        (unsigned long)(nb)))           
                    victim = victim->bk_nextsize;
```

反向遍历chunk size链表，直到找到第一个大于等于所需chunk大小的chunk退出循环。

```text
/* Avoid removing the first entry for a size so that the skip
                list does not have to be rerouted.  */
                if (victim != last(bin) && victim->size == victim->fd->size)             
                    victim = victim->fd;
```

如果large bin链表中选取的chunk civtim不是链表中的最后一个chunk，并且与victim大小相同的chunk不止一个，那么意味着victim为chunk size链表中的节点，为了不调整chunk size链表，需要避免将chunk size链表中取出，所以取victim->fd节点对应的chunk作为候选chunk。由于large bin链表中的chunk也是按照大小排序，同一大小的chunk有多个时，这些chunk必定排在一起，所以victim->fd节点对应的chunk的大小必定与victim的大小一样。

```text
remainder_size = size - nb;         
                unlink(victim, bck, fwd);
```

计算将victim切分后剩余大小，并调用unlink()宏函数将victim从large bin链表中取出。

```text
if (remainder_size < MINSIZE)  
                {           
                    set_inuse_bit_at_offset(victim, size);           
                    if (av != &main_arena)             
                        victim->size |= NON_MAIN_ARENA; 
                }
```

如果将victim切分后剩余大小小于MINSIZE，则将整个victim分配给应用层，设置victim的inuse标志，inuse标志位于下一个相邻的chunk的size字段中。如果当前分配区不是主分配区，将victim的size字段中的非主分配区标志置位。

```text
/* Split */
                else 
                {           
                    remainder = chunk_at_offset(victim, nb);           
                    /* We cannot assume the unsorted list is empty and therefore
                    have to perform a complete insert here.  */
                    bck = unsorted_chunks(av);           
                    fwd = bck->fd;           
                    if (__builtin_expect (fwd->bk != bck, 0)) 
                    {               
                        errstr = "malloc(): corrupted unsorted chunks";               
                        goto errout;             
                    }           
                    remainder->bk = bck;           
                    remainder->fd = fwd;           
                    bck->fd = remainder;           
                    fwd->bk = remainder;           
                    if (!in_smallbin_range(remainder_size))             
                    {               
                        remainder->fd_nextsize = NULL;               
                        remainder->bk_nextsize = NULL;             
                    }
```

从victim中切分出所需的chunk，剩余部分作为一个新的chunk加入到unsorted bin中。 如果剩余部分chunk属于large bins，将剩余部分chunk的chunk size链表指针设置为NULL，因为unsorted bin中的chunk是不排序的，这两个指针无用，必须清零。

```text
set_head(victim, nb | PREV_INUSE |                    
                            (av != &main_arena ? NON_MAIN_ARENA : 0));                   
                   set_head(remainder, remainder_size | PREV_INUSE);                   
                   set_foot(remainder, remainder_size);
                }
```

设置victim和remainder的状态，由于remainder为空闲chunk，所以需要设置该chunk的foot。

```text
check_malloced_chunk(av, victim, nb);         
                void *p = chunk2mem(victim);         
                if (__builtin_expect (perturb_byte, 0))           
                    alloc_perturb (p, bytes);         
                return p;
            }
        }
```

从large bin中使用最佳匹配法找到了合适的chunk，调用chunk2mem()获得chunk中可用的内存指针，返回给应用层，退出。
如果通过上面的方式从最合适的small bin或large bin中都没有分配到需要的chunk，则查看比当前bin的index大的small bin或large bin是否有空闲chunk可利用来分配所需的chunk。源代码实现如下：

```text
++idx;     
        bin = bin_at(av,idx);     
        block = idx2block(idx);     
        map = av->binmap[block];     
        bit = idx2bit(idx);
```

获取下一个相邻bin的空闲chunk链表，并获取该bin对于binmap中的bit位的值。Binmap中的标识了相应的bin中是否有空闲 chunk 存在。Binmap按block管理，每个block为一个int，共32个bit，可以表示32个bin中是否有空闲chunk存在。使用binmap可以加快查找bin是否包含空闲chunk。这里只查询比所需chunk大的bin中是否有空闲chunk 可用。

```text
for (;;) 
        {       
            /* Skip rest of block if there are no more set bits in this block.  */
            if (bit > map || bit == 0) 
            {         
                do 
                    {           
                        if (++block >= BINMAPSIZE)  /* out of bins */
                        goto use_top;         
                    }while ( (map = av->binmap[block]) == 0); 
                bin = bin_at(av, (block << BINMAPSHIFT));         
                bit = 1;       
            }
```

Idx2bit()宏将idx指定的位设置为1，其它位清零，map表示一个block值，如果bit大于map，意味着map为0，该block所对应的所有bins中都没有空闲chunk，于是遍历binmap的下一个block，直到找到一个不为0的block或者遍历完所有的 block。 退出循环遍历后，设置 bin 指向 block 的第一个 bit 对应的 bin，并将 bit 置为 1，表示该 block 中 bit 1 对应的 bin，这个 bin 中如果有空闲 chunk，该 chunk 的大小一定满足要求。

```text
/* Advance to bin with set bit. There must be one. */
            while ((bit & map) == 0) 
            {         
                bin = next_bin(bin);         
                bit <<= 1;         
                assert(bit != 0);       
            }
            /* Inspect the bin. It is likely to be non - empty */
            victim = last(bin); 
```

在一个block遍历对应的bin，直到找到一个bit不为0退出遍历，则该bit对于的bin中有空闲chunk存在。将bin链表中的最后一个chunk赋值为victim。

```text
/*  If a false alarm (empty bin), clear the bit. */
            if (victim == bin) 
            {         
                av->binmap[block] = map &= ~bit; 
                /* Write through */
                bin = next_bin(bin);         
                bit <<= 1;
            }
```

如果victim与bin链表头指针相同，表示该bin中没有空闲chunk，binmap中的相应位设置不准确，将binmap的相应bit位清零，获取当前bin下一个bin，将bit移到下一个bit位，即乘以2。

```text
else 
            {         
                size = chunksize(victim);         
                /*  We know the first chunk in this bin is big enough to use. */
                assert((unsigned long)(size) >= (unsigned long)(nb));         
                remainder_size = size - nb;         
                /* unlink */
                unlink(victim, bck, fwd);
```

当前bin中的最后一个chunk满足要求，获取该chunk的大小，计算切分出所需chunk后剩余部分的大小，然后将victim从bin的链表中取出。

```text
/* Exhaust */
                if (remainder_size < MINSIZE) 
                {           
                    set_inuse_bit_at_offset(victim, size);           
                    if (av != &main_arena)             
                        victim->size |= NON_MAIN_ARENA
                }
```

如果剩余部分的大小小于MINSIZE，将整个chunk分配给应用层(代码在后面)，设置victim的状态为inuse，如果当前分配区为非分配区，设置victim的非主分配区标志位。

```text
/* Split */
                else 
                {           
                    remainder = chunk_at_offset(victim, nb); 
                    /* We cannot assume the unsorted list is empty and therefore
                    have to perform a complete insert here.  */
                    bck = unsorted_chunks(av);           
                    fwd = bck->fd;           
                    if (__builtin_expect (fwd->bk != bck, 0))             
                    {               
                        errstr = "malloc(): corrupted unsorted chunks 2";               
                        goto errout;             
                    }           
                    remainder->bk = bck;           
                    remainder->fd = fwd;
                    bck->fd = remainder;           
                    fwd->bk = remainder; 
                    /* advertise as last remainder */
                    if (in_smallbin_range(nb))             
                    av->last_remainder = remainder;           
                    if (!in_smallbin_range(remainder_size))             
                    {               
                        remainder->fd_nextsize = NULL;               
                        remainder->bk_nextsize = NULL;
                    }
```

从victim中切分出所需的chunk，剩余部分作为一个新的chunk加入到unsorted bin中。如果剩余部分chunk属于large bins，将剩余部分chunk的chunk size链表指针设置为NULL，因为unsorted bin中的chunk是不排序的，这两个指针无用，必须清零。

```text
set_head(victim, nb | PREV_INUSE |                    
                             (av != &main_arena ? NON_MAIN_ARENA : 0));                       
                     set_head(remainder, remainder_size | PREV_INUSE);                      
                     set_foot(remainder, remainder_size);
                }
```

设置victim和remainder的状态，由于remainder为空闲chunk，所以需要设置该chunk的foot。

```text
check_malloced_chunk(av, victim, nb);         
                void *p = chunk2mem(victim);         
                if (__builtin_expect (perturb_byte, 0))           
                    alloc_perturb (p, bytes);         
                return p;
            }
        }
```

调用chunk2mem()获得chunk中可用的内存指针，返回给应用层，退出。
如果从所有的bins中都没有获得所需的chunk，可能的情况为bins中没有空闲chunk，或者所需的chunk大小很大，下一步将尝试从top chunk中分配所需chunk。源代码实现如下：

```text
use_top:
        victim = av->top;     
        size = chunksize(victim);
```

将当前分配区的top chunk赋值给victim，并获得victim的大小。

```text
if ((unsigned long)(size) >= (unsigned long)(nb + MINSIZE)) 
        {       
            remainder_size = size - nb;       
            remainder = chunk_at_offset(victim, nb);       
            av->top = remainder;       
            set_head(victim, nb | PREV_INUSE |                
                    (av != &main_arena ? NON_MAIN_ARENA : 0));       
            set_head(remainder, remainder_size | PREV_INUSE); 
            check_malloced_chunk(av, victim, nb);       
            void *p = chunk2mem(victim);       
            if (__builtin_expect (perturb_byte, 0))         
                alloc_perturb (p, bytes);       
            return p;     
        }
```

由于top chunk切分出所需chunk后，还需要MINSIZE的空间来作为fencepost，所需必须满足top chunk的大小大于所需chunk的大小加上MINSIZE这个条件，才能从top chunk中分配所需chunk。从top chunk切分出所需chunk的处理过程跟前面的chunk切分类似，不同的是，原top chunk切分后的剩余部分将作为新的top chunk，原top chunk的fencepost仍然作为新的top chunk的fencepost，所以切分之后剩余的chunk不用set_foot。

```text
#ifdef ATOMIC_FASTBINS     
        /* When we are using atomic ops to free fast chunks we can get
        here for all block sizes.  */
        else if (have_fastchunks(av)) 
        {       
            malloc_consolidate(av);       
            /* restore original bin index */
            if (in_smallbin_range(nb))         
                idx = smallbin_index(nb);       
            else         
                idx = largebin_index(nb); 
        }
```

如果top chunk也不能满足要求，查看fast bins中是否有空闲chunk存在，由于开启了ATOMIC_FASTBINS优化情况下，free属于fast bins的chunk时不需要获得分配区的锁，所以在调用_int_malloc()函数时，有可能有其它线程已经向fast bins中加入了新的空闲chunk，也有可能是所需的chunk属于small bins，但通过前面的步骤都没有分配到所需的chunk，由于分配small bin chunk时在前面的步骤都不会调用malloc_consolidate()函数将fast bins中的 chunk合并加入到unsorted bin中。所在这里如果fast bin中有chunk存在，调用malloc_consolidate()函数，并重新设置当前bin的index。并转到最外层的循环，尝试重新分配small bin chunk或是large bin chunk。如果开启了ATOMIC_FASTBINS优化，有可能在由其它线程加入到fast bins中的chunk被合并后加入unsorted bin中，从unsorted bin中就可以分配出所需的large bin chunk了，所以对没有成功分配的large bin chunk也需要重试。

```text
#else
        else if (have_fastchunks(av)) 
        {       
            assert(in_smallbin_range(nb));       
            malloc_consolidate(av);       
            idx = smallbin_index(nb); /* restore original bin index */
        }
```

如果top chunk也不能满足要求，查看fast bins中是否有空闲chunk存在，如果fast bins中有空闲chunk存在，在没有开启ATOMIC_FASTBINS优化的情况下，只有一种可能，那就是所需的chunk属于small bins，但通过前面的步骤都没有分配到所需的small bin chunk，由于分配small bin chunk时在前面的步骤都不会调用malloc_consolidate()函数将fast bins中的空闲chunk合并加入到unsorted bin中。所在这里如果fast bins中有空闲chunk存在，调用 malloc_consolidate()函数，并重新设置当前bin的index。并转到最外层的循环，尝试重新分配small bin chunk。

```text
else 
       {       
            void *p = sYSMALLOc(nb, av);       
            if (p != NULL && __builtin_expect (perturb_byte, 0))         
            alloc_perturb (p, bytes);       
            return p; 
        }
    }
}
```

山穷水尽了，只能想系统申请内存了。sYSMALLOc()函数可能分配的chun 包括small bin chunk，large bin chunk 和large chunk。

**check**

1.fastbin头部的chunk的idx与fastbin的idx要一致。
2.unsorted bin中的chunk1大小不能小于等于2*SIZE_SZ，也不能超过该分配区总的内存分配量。
3.将chunk从unsorted bin取出放入small bin和large bin时用到了unlink()宏，注意绕过unlink()宏中的检测。
4.切出的remainder在重新放入unsorted bin时需要满足 unsorted_chunks(av)->fd->bk = unsorted_chunks(av)。

## 2.**总结**

malloc分配步骤大致如下：
1.检查有没有_malloc_hook，有则调用hook函数。
2.获得分配区的锁，调用函数_int_malloc()分配内存。
3.如果申请大小在fast bin范围内，则从fast bin分配chunk，成功则返回用户指针，否则进行下一步。(当对应的bin为空时，就会跳过第5步操作)
4.如果申请大小在small bin范围内，则从small bin中分配chunk，成功则返回用户指针，否则进行下一步。
5.调用malloc_consolidate()函数合并fast bin，并链接进unsorted bin中。
6.如果申请大小在small bin范围内，且此时unsorted bin只有一个chunk，并且这个chunk为last remainder chunk且大小够大，则从这个chunk中切分出需要的大小，成功则返回用户指针，否则进行下一步。
7.反向遍历unsorted bin，如果当前chunk与所需chunk大小一致，则分配，成功则返回用户指针，否则将当前chunk放入small bin或者large bin中合适的位置。
8.使用最佳匹配算法在large bin中找到合适的chunk进行分配，成功则返回用户指针，否则进行下一步。
9.到了这一步，说明没有大小正好合适的chunk，则看看比当前bin的index大的small bin或者large bin中有没有空闲chunk可用来分配。成功则返回用户指针，否则进行下一步。
10.尝试从top chunk中分配，成功则返回用户指针，否则进行下一步。
11.如果fast bin中还有chunk，调用malloc_consolidate()回到第6步(因为第3步对应bin为空时会跳过第五步，而fast bin合并之后有可能出现能够分配的small bin)。
12.到了这步还不行，则调用sYSMALLOc()函数向系统申请内存。

原文地址：https://zhuanlan.zhihu.com/p/564448120

作者：linux

# 【NO.114】C++内存管理及内存问题的分析

写服务端的，内存是一个绕不过的问题，而用C++写的，这个问题就显得更严重。进程的内存持续上涨，有可能是正常的内存占用，也有可能是内存碎片，而C++写的，还有可能是内存泄漏，那就需要一些方法来检测到底是哪些问题引起的

## **1. 内存占用**

首先从top这个指令说起

```text
Tasks:  80 total,   1 running,  79 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.3 us,  0.7 sy,  0.0 ni, 92.7 id,  6.3 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  2052544 total,  1453600 free,   162408 used,   436536 buff/cache
KiB Swap:   782332 total,   782332 free,        0 used.  1708652 avail Mem 

  PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND      
  179 root      20   0       0      0      0 S  0.3  0.0   0:00.27 [jbd2/dm-0-+ 
  493 mongodb   20   0 1102144  78548  36688 S  0.3  3.8   0:26.07 /usr/bin/mo+ 
  636 mysql     20   0  653808  75932  15548 S  0.3  3.7   0:03.55 /usr/sbin/m+ 
```

与进程内存相关的两个指标：VIRT Virtual Memory，虚拟内存、RES Resident Memory，常驻内存，通常叫物理内存。虚拟内存，是指整个进程申请的内存，包括程序本身的占内存、new或者malloc分配的内存等等。物理内存，就是这个进程在主板上内存条那里占用了多少内存。那为什么会有虚拟内存这个东西，C++不是可以操作硬件么，为什么不直接使用物理内存？这得简单了解一下操作系统的内存管理。

现代的计算机都会同时运行N个程序，有N多个进程，这些进程都是独立在运行。如果直接使用物理内存，那就会产生一个问题，进程A申请了内存，进程B也要申请一块内存，但进程B并不知道进程A的存在，就没法保证进程B使用的内存进程A没在用。因此linux下使用内核来管理这些资源，所有进程都只是向内核申请，由内核管理物理内存。而一个进程，可能多次申请、释放内存，或者程序直接当掉没有释放内存，内核为了解决这些复杂的问题，用一个列表维护了进程分配的内存，这就叫虚拟内存，然后把虚拟内存映射到物理内存，这就完成了整个内存的管理。而且，内核对内存的映射做了优化，用到时才映射，如下面的图中，进程A的new2这块内存分配了以后，一直没使用，也就不会映射到物理内存。有很多程序，利用了这个特性。例如，在socket收发时，我们可以分配很大的一块内存（比如16M），避免频繁分配缓冲区，但实际这个socket可能收到的数据块最大只有16k，那内核是不会直接映射16M物理内存的，这样既方便了我们写程序，但又没浪费物理内存。

![img](https://pic2.zhimg.com/80/v2-d519e76e268d0ee4a73ed28f8fd25b7d_720w.webp)

下面写个程序来验证这个问题

```text
#include <cstring>
#include <iostream>

int main()
{
#define PAUSE(msg) std::cout << msg << std::endl; std::cin >> p

        char p;

        size_t size = 1024 * 1024 *100;
        char *l = new char[size];

        PAUSE("new");

        memset(l, 1, size / 2);
        PAUSE("using half large");

        memset(l, 1, size);
        PAUSE("using whole large");

        delete []l;
        PAUSE("del");

        return 0;
}
```

在每次暂停时，top的输出结果(RES 1588 54328 105600 3348)，说明memset的时候，内核才会映射物理内存。

```text
new
 进程号 USER      PR  NI    VIRT    RES    SHR    %CPU  %MEM     TIME+ COMMAND  
  25295 root     20   0  108280   1588   1436 S   0.0   0.0   0:00.00 ./a.out


using half large
  25295 root     20   0  108280  54328   3096 S   0.0   0.7   0:00.05 ./a.out


using whole large
  25295 root     20   0  108280 105600   3156 S   0.0   1.4   0:00.12 ./a.out


del
  25295 root     20   0    5876   3348   3156 S   0.0   0.0   0:00.13 ./a.out
```

所以，通过top查看进程内存时，如果发现VIRT占用很大，说明这个程序用new或者malloc等分配了很多内存，但如果RES不是很大，那就不要慌，可能这只是程序的一个缓存优化（当然也有可能是写这个程序的人用new分配内存时很不合理，分配的值远大于使用值），实际程序运行占用的物理内存并不大。但如果RES也很高，那可能就有点慌了。

## **2. 内存泄漏**

内存泄漏是导致进程内存持续上涨最常见的原因，而这是C++中常见但不好处理的问题，一个维护多年的大项目，代码不知道由多少个人写的，想找出哪个指针的内存没释放，谈何容易。解决这个问题没有什么通用快捷的办法，只能根据实际业务处理。

第一，从业务上，能不能重现内存泄漏。例如我们做游戏的，假如玩家不停地登录，就会导致内存不断上涨，那说明问题就在登录流程，把整个流程拆分，一个个屏蔽测试，最终找出问题。

第二，从部署上，能不能定位内存泄漏。例如，最近更新了一个版本，发现内存占用变得很高，那就可以确定，是这个版本的修改出了问题。一个版本的代码量终究是有限的，查找起来也比较容易。

第三，使用valgrind memcheck。如果能够复现内存泄漏，但无法定位是哪个逻辑，那可以用valgrind memcheck。复现内存泄漏，这个通常比较难实现，一般是线下测试无法复现，线上用户量大，运行久了才会复现，而valgrind会导致程序运行很慢，无法支撑线上测试，因此这个选项通常不太适用于线上。

第四，使用Visual Leak Detector。valgrind是linux下的，如果程序可以跨平台，或者只在win下，那么可以试试这个，这个和valgrind一样，需要复现泄漏才能得到堆栈，因此也是用于线下调试比较多。

第五，重载new、delete。像我[之前的博客](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/coding-my-life/p/10125538.html)里写的，可以简单地加个计数，用于平时预防泄漏，也可更深入一些，记录内存的分配，得到内存漏泄的堆栈，但是这个是否能支撑线上debug，我持怀疑态度。

第六，使用自己的内存分配函数，每一个内存分配，都使用自己的函数，每一个STL的容器，都传入自己的分配器，然后分别记录这些内存分配的大小。这个方法看起来很不现实，但我确实见过在实际的项目中使用，对内存统计、查找有很大的帮助，而且支持在线上debug。查找内存，只需要打印下每个分配器分配的内存大小基本上可以得到结论是哪个分配器出问题。唯一的问题是它增加了开发难度，而且不能像valgrind那样不需要修改原程序即可使用。

第七，使用valgrind massif。valgrind memcheck需要复现内存泄漏，所以不容易找出问题。它会定时记录分配内存的各个堆栈以及分配内存的量，当出现内存泄漏时，根据分配内存的量检查下各个堆栈，应该是可以找到问题的。massif也会导致程序运行慢，但比memcheck要快，能不能在线上debug，这个依然得看具体情况

第八，使用第三方内存分配器，如jemalloc。并不是说使用第三方内存分配器就解决问题了，而是jemalloc自带了一大堆工具，其中jeprof可以得到内存的大小以及堆栈等信息，对查找内存泄漏有很大帮助。不过开启prof后，效率如何，能不能在线上使用，我倒是没测试过。

## **3. 内存碎片**

假如找不到内存泄漏，也许本来就没有内存泄漏，这时不妨考虑下内存碎片的问题。这里以linux下的ptmalloc为例（其他的分配器我就不懂了），说下内存分配。假如一个进程，依次分配了内存块m1(1k)、m2(10b)、

![img](https://pic2.zhimg.com/80/v2-7fb767e2bb0f024de78cf2fffe62bf51_720w.webp)

m3(1k)，然后释放了m2，那整个内存看起来是这样子的：

我们可以看到，m1、m2、m3是按顺序分配的，当m2被释放时，那中间就空了一块了。那空的这一块怎么办，是把它还给系统了吗？这个问题就很复杂了，涉及到ptmalloc的整个分配机制，这里不打算详细说，建议看[华庭(庄明强) - ptmalloc2源代码分析](https://link.zhihu.com/?target=https%3A//paper.seebug.org/papers/Archive/refs/heap/glibc%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86ptmalloc%E6%BA%90%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90.pdf)。简单来讲，就是ptmalloc会暂把释放的内存按大小用链表存起来，比如10b的，放到fast bin那个链表，大一点的，放small bin的第一个链表，再大一点，放small bin的第二个链表，... 放进去的内存，直到第再次用到时取出。

随着程序运行，放进链表的内存可能会越来越多，但是却很少取出（可能是程序释放后没有再申请，也可能是申请的大小和链表里的大小不合适，比如链表里有个10b的，但是程序申请了1k），那这些小内存就会越来越多，进程占用的内存也会越来越多，但实际使用的内存不多。那如何检测这种情况呢？

方法一，使用

malloc_stats。malloc_stats是一个glibc的函数，因此可以在gdb调用

```text
gdb -p 16021
call malloc_stats()

Arena 0:
system bytes     =    1359872
in use bytes     =     954224
Arena 1:
system bytes     =     135168
in use bytes     =       3488
Arena 2:
system bytes     =     135168
in use bytes     =      20784
Arena 3:
system bytes     =     139264
in use bytes     =     120080
Total (incl. mmap):
system bytes     =    1769472
in use bytes     =    1098576
max mmap regions =          0
max mmap bytes   =          0
```

1. Arena N表示多个分配域，一般一个线程一个
2. system bytes 当前申请的内存总数
3. in use bytes 当前使用的内存总数
4. max mmap regions 使用mmap分配了多少块内存(大内存用mmap分配，大于128K，可由M_MMAP_THRESHOLD选项调节)
5. max mmap bytes 使用mmap分配了多少内存

这里，system bytes减去in use bytes就可以得到当前进程缓存了多少内存。不过malloc_stats是一个很老的接口了，里面的变量都是用的int，如果你的程序占用内存比较大，这里可能会溢出。

方法二，使用使用malloc_info

```text
gdb -p 16021
call malloc_info(0, stdout)

<malloc version="1">
<heap nr="0">
<sizes>
<size from="17" to="32" total="3104" count="97"/>
<size from="33" to="48" total="11136" count="232"/>
<size from="49" to="64" total="12288" count="192"/>
<size from="65" to="80" total="14640" count="183"/>
<size from="81" to="96" total="4896" count="51"/>
<size from="97" to="112" total="1232" count="11"/>
<size from="113" to="128" total="7296" count="57"/>
<size from="33" to="33" total="13299" count="403"/>
<size from="97" to="97" total="97" count="1"/>
<size from="7281" to="7281" total="7281" count="1"/>
<size from="32833" to="32833" total="32833" count="1"/>
<unsorted from="145" to="8753" total="166107" count="155"/>
</sizes>
<total type="fast" count="823" size="54592"/>
<total type="rest" count="561" size="219617"/>
<system type="current" size="1359872"/>
<system type="max" size="1376256"/>
<aspace type="total" size="1359872"/>
<aspace type="mprotect" size="1359872"/>
</heap>
<heap nr="1">
<sizes>
<size from="33" to="48" total="48" count="1"/>
<unsorted from="4673" to="4705" total="9378" count="2"/>
</sizes>
<total type="fast" count="1" size="48"/>
<total type="rest" count="2" size="9378"/>
<system type="current" size="135168"/>
<system type="max" size="135168"/>
<aspace type="total" size="135168"/>
<aspace type="mprotect" size="135168"/>
</heap>
<heap nr="2">
<sizes>
<size from="33" to="48" total="48" count="1"/>
<size from="113" to="128" total="128" count="1"/>
<size from="65" to="65" total="65" count="1"/>
<unsorted from="81" to="3233" total="10054" count="6"/>
</sizes>
<total type="fast" count="2" size="176"/>
<total type="rest" count="7" size="10119"/>
<system type="current" size="135168"/>
<system type="max" size="135168"/>
<aspace type="total" size="135168"/>
<aspace type="mprotect" size="135168"/>
</heap>
<heap nr="3">
<sizes>
<size from="65" to="80" total="80" count="1"/>
</sizes>
<total type="fast" count="1" size="80"/>
<total type="rest" count="0" size="0"/>
<system type="current" size="139264"/>
<system type="max" size="139264"/>
<aspace type="total" size="139264"/>
<aspace type="mprotect" size="139264"/>
</heap>
<total type="fast" count="827" size="54896"/>
<total type="rest" count="570" size="239114"/>
<total type="mmap" count="0" size="0"/>
<system type="current" size="1769472"/>
<system type="max" size="1785856"/>
<aspace type="total" size="1769472"/>
<aspace type="mprotect" size="1769472"/>
</malloc>
```

\1. nr即arena，通常一个线程一个

\2. <size from="17" to="32" total="3104" count="97"/>上面说了，大小在一定范围内的内存，会放到一个链表里，这就是其中一个链表。from是内存下限，to是上限，上面的意思是内存分配在 [17,32]范围内的空闲内存总共有97个，占3104字节内存。在这个区间内的内存申请都会被对齐为32，故total = to * count

\3. <total type="fast" count="2" size="176"/> 即fastbin这链表当前有2个空闲内存块，大小为176

<total type="rest" count="7" size="10119"/> 除fastbin以外，所有链表空闲的内存数量，以及内存大小。因此fast和rest加起来，应该和当前arena里所有的size一致，如

```text
<heap nr="2">
<sizes>
<size from="33" to="48" total="48" count="1"/>
<size from="113" to="128" total="128" count="1"/>
<size from="65" to="65" total="65" count="1"/>
<unsorted from="81" to="3233" total="10054" count="6"/>
</sizes>
<total type="fast" count="2" size="176"/>
<total type="rest" count="7" size="10119"/>
 
```

前两个to大小为48和128为fast bin，数量为2，剩下的都为rest，与下面的fast和reset对应。

\5. <total type="mmap" count="0" size="0"/> 使用mmap分配的当前在使用块数(count)和当前在用的内存大小(size)(低版本glibc无此字段，如centos6上的glibc 2.12)

\6. <system type="current" size="1769472"/> 当前已经申请的内存大小

\7. <system type="max" size="1785856"/> 历史上申请的内存大小（包括已经归还给操作系统的）

\8. total和mprotect看源码没看出是什么东西

到这里可以看到，假如一个进程fast和reset里的数量很多，那么说明这个进程其实缓存了很多内存。另外这里都是直接用gdb attach到一个进程直接调用函数，打印到stdout。如果需要查看的程序被关掉了stdout或者重定向了stdout（很多服务器进程都这么做），那可能看不见了，或者信息不是打印到当前终端。

## **4. 内存利用率**

如果一个进程占用的内存远高于预期，但没有持续上涨，还需要考虑下是不是内存使用率的问题。当使用new分配一块内存时，系统需要为这次分配记录大小、地址，分配的内存也需要对齐，假如分配的内存很小（比如说1b），那系统最终需要消耗的内存是远大于1b的。比如

```text
#include <cstring>
#include <iostream>

int main()
{
#define PAUSE(msg) std::cout << msg << std::endl; std::cin >> p

    char p = NULL;

    size_t total = 0;
    while (total < 1024 * 1024 * 1024)
    {
        size_t size = rand() % 16;

        total += size;
        char *p = new char[size];
    }

    PAUSE("pause");
```

这个程序每次分配小于16字节的内存，直到总分配量到1G，然而，在我的系统里（ubuntu 20.04），这个程序跑起来占用的内存就多得多

```text
进程号 USER      PR  NI    VIRT    RES    SHR    %CPU  %MEM     TIME+ COMMAND  
   4174 root      20   0 4479488   4.3g   1616 S   0.0  59.0   0:15.97 ./a.out 
```

已经达到了4.3G，显然内存利用率只有1/4不到。你也许会说这种分配小内存的情况不多，但其实不是的。举个例子，做关键字搜索时，会用到二叉搜索树，每一个树的节点对应一个字符，比如"abcd“就需要分配4个节点，但是每个节点其实很小。假如关键字很多（上百万还是很常见的），那这个问题就比较严重。这时候就应该使用valgrind massif来看下，到底是哪个地方分配的内存，然后根据逻辑优化即可。

原文地址：https://zhuanlan.zhihu.com/p/564309039

作者：linux

# 【NO.115】一大波C++进阶知识干货分享，请接收！

2020年是全球IT科技版图震荡和转折之年，系统级软件作为数字世界的核心基础设施，被视为“卡脖子”技术的关键，成为IT产学研“兵家必争之地”。C++语言一直被誉为系统级编程“皇冠上的明珠”。

在疫情放假那段时间，我就专心钻研C++进阶知识，终于功夫不负有心人，在不断地学习以及练习中，学会了很多知识，也算是学有所成，现在是一名月收入上万的Linux高级互联网架构师，觉得这么长时间的努力还是没有白费。

![img](https://pic1.zhimg.com/80/v2-9b95855affa78bfede6b505f254443e4_720w.webp)

## 1.源码分析专栏

关于这一部分的内容还是非常广阔的，主要讲一下关于nginx方面的知识。

### 1.1.Nginx基础架构

core ：Nginx的核心源代码，包括常用数据结构的以及Nginx 内核实现的核心代码；

event：Nginx事件驱动模型，以及定时器的实现相关代码；

http ：Nginx 实现http 服务器相关的代码；

mail ：Nginx 实现邮件代理服务器相关的代码；

misc ：辅助代码，测试C++头 的兼容性，以及对Google_PerfTools 的支持；

os ：不同体系统结构所提供的系统函数的封装，提供对外统一的系统调用接口；

stream：nginx（tcp/udp）反向代理及与上游通信的基础模块

### 1.2.HTTP架构

它的架构主要有七层，包括Controller、View、Application、Business、Component、Datadriver、Systemdriver。

这方面知识非常广阔，我主要拿两个重点讲，可能下面也有些内容讲得不够全面，大家都可以进交流群详细了解，免费领取相关学习资料。

1、Application

应用层在最上面，其针对实际中的单个页面或则单个接口, Controller通过HTTP请求地址中的参数找到对应的Application,然后执行中指定的公共方法，比如main(), 然后应用就开始启动。

应用层的职责包括接受HTTP参数(- 般是间接接受，比如从request对象中获取) ，调用Business层的特定业务，保存业务执行结果，这些结果最终会由View显示出来，当然是通过Controller协调。应用层是M层分解成五层之后最高的层，Controller会 与此层直接通信。

2、Business

业务层在应用层之下，通常一个应用实例对应一个业务实例， 而一个业务有可能为多个应用服务,业务是一个执行流， 它通过执行一系列的操作来完成应用的需求。

这些操作来自下层的组件层Component,可能一个业务需 要一个或则多 个组件来完成-个完整的需求。因为一个业务实例通常只对应-个功能，所以只有-个固定的方法会被上层的应用调用，比如flow()。 业务层的职责是帮应用层执行业务流并且有必要的时候返回数据给应用层，它会调用下层Component的方法。

### 1.3.进程间的通信机制

进程通常被定义为一个正在运行的程序的实例，它由两个部分组成：
一个是操作系统用来管理进程的内核对象，它是系统用来存放关于进程的统计信息的地方。
另一个是地址空间，它包含所有的可执行模块或DLL模块的代码和数据，还包含动态分配的空间，如线程堆栈和堆分配空间。

每个进程被赋予它自己的虚拟地址空间，当进程中的一个线程正在运行时，该线程可以访问只属于它的进程的内存。属于其它进程的内存则是隐藏的，并不能被正在运行的线程访问。

### 1.4.Nginx高级数据结构

![img](https://pic2.zhimg.com/80/v2-c21d86c61bb6a3d5756d945a3fde65ed_720w.webp)

### 1.5.Slab共享共存

使用共享内存，需要在配置文件里加上该共享内存的相关配置信息，而 Nginx 在进行配置解析的过程中，根据这些配置信息就会创建对应的共享内存，不过此时的创建仅仅只是代表共享内存的结构体 ngx_shm_zone_t 变量的创建。具体实现在函数 ngx_shared_memory_add 内。

### 1.6.upstream机制设计

1、轮询(weight)

指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。默认当weight不指定时，各服务器weight相同，每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。

2、ip_hash

每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session不能跨服务器的问题。如果后端服务器down掉，要手工down掉。

3、fair（第三方插件）

按后端服务器的响应时间来分配请求，响应时间短的优先分配。

4、url_hash（第三方插件）

按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存服务器时比较有效。在upstream中加入hash语句，hash_method是使用的hash算法。

设备的状态有:

（1）down：表示单前的server暂时不参与负载

（2）weight：权重，默认为1， weight越大，负载的权重就越大。

（3）max_fails：允许请求失败的次数默认为1。当超过最大次数时，返回proxy_next_upstream 模块定义的错误。

（4）fail_timeout：max_fails次失败后，暂停的时间。

（5）backup：备用服务器, 其它所有的非backup机器down或者忙的时候，请求backup机器，所以这台机器压力会最轻。



## 2.对标腾讯T1-T9学习技术路线图谱

![img](https://pic4.zhimg.com/80/v2-0136174b9a47c53154bd90972b8b93b3_720w.webp)

## **3. 数十位大佬大厂面经视频及2021面试题资源分享**

![img](https://pic1.zhimg.com/80/v2-238077f1b5caa91c38999aac3cd24bfc_720w.webp)

## **4.金三银四”程序员26套求职简历模板**

![img](https://pic4.zhimg.com/80/v2-8eea469345276a43b1f78991b5d4e8f3_720w.webp)

作为一个过来人给大家提个醒，C++的学习是一个漫长的过程，特别是进阶需要花很多时间，学习过程也困难重重，所以最好是找学习的小伙伴，相互帮助一起学，学起来会感觉轻松一些，学习效果也不言而喻。

原文链接：https://linuxcpp.0voice.com/?id=107

作者：[HG](https://linuxcpp.0voice.com/?auth=10)

# 【NO.116】线程池的优点及其原理，简单、明了。

## 1.使用线程池的好处

池化技术应用：线程池、数据库连接池、http连接池等等。

池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。

**线程池提供了一种限制、管理资源的策略。** 每个**线程池**还维护一些基本统计信息，例如已完成任务的数量。

**使用线程池的好处：**

- **降低资源消耗**：通过重复利用已创建的线程降低线程创建和销毁造成的消耗。
- **提高响应速度**：当任务到达时，可以不需要等待线程创建就能立即执行。
- **提高线程的可管理性**：线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，监控和调优。

## 2.Executor框架

Executor框架不仅包括了线程池的管理，还提供了线程工厂、队列以及拒绝策略等，让并发编程变得更加简单。

**Executor框架的使用示意图**

![img](https://pic2.zhimg.com/80/v2-dff549e683cdef4a7025c42b9f431e5d_720w.webp)

1. **主线程首先要创建实现Runnable或Callable接口的任务对象。**
2. **把创建完成的实现Runnable/Callable接口的对象直接交给ExecutorService执行：**

```
ExecutorService.execute(Runnable command)或者ExecutorService.sumbit(Runnable command)或ExecutorService.sumbit(Callable <T> task).
```

1. **如果执行ExecutorService.submit(…)，ExecutorService将返回一个实现Future接口的对象。最后，主线程可以执行FutureTask.get()方法来等待任务执行完成。主线程也可以执行FutureTask.cancel()来取消次任务的执行。**

```
import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class ThreadPoolExecutorDemo {    private static final int CORE_POOL_SIZE = 5;    private static final int MAX_POOL_SIZE = 10;    private static final int QUEUE_CAPACITY = 100;    private static final Long KEEP_ALIVE_TIME = 1L;    public static void main(String[] args) {        ThreadPoolExecutor executor = new ThreadPoolExecutor(                CORE_POOL_SIZE,                MAX_POOL_SIZE,                KEEP_ALIVE_TIME,                TimeUnit.SECONDS,                new ArrayBlockingQueue<>(QUEUE_CAPACITY),                new ThreadPoolExecutor.CallerRunsPolicy());        //执行线程代码        executor.shutdown();    }} 
```

**CORE_POOL_SIZE:**核心线程数定义了最小可以同时运行的线程数量。

**MAX_POOL_SIZE：**当队列中存放的任务到达队列容量的时候，当前可以同时运行的线程数量变为最大线程数。

**QUEUE_CAPACITY：**当新任务加入是会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，任务就会被存放到队列中。

**KEEP_ALIVE_TIME：**当线程池中的线程数量大于核心线程数时，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过**KEEP_ALIVE_TIME**才会被回收销毁。

**ThreadPoolExecutor.CallerRunsPolicy()：**调用执行自己的线程运行任务，也就是直接在调用execute方法的线程运行（run）被拒绝的任务，如果执行程序已关闭，则会丢弃任务。因此这种策略会降低新任务的提交速度，影响程序的整体性能。另外，这个策略喜欢增加队列容量。如果应用程序可以承受此延迟并且不能任务丢弃一个任务请求的话，可以选择这个策略。

**线程池分析原理**

![img](https://pic1.zhimg.com/80/v2-965f8856f55a687ef34664b2c1a7086c_720w.webp)

## 3.线程池大小确定

有一个简单且使用面比较广的公式：

- **CPU密集型任务（N+1）：**这种任务消耗的主要是CPU资源，可以将线程数设置为N（CPU核心数）+1，比CPU核心数多出来一个线程是为了防止线程偶发的缺页中断，或者其他原因导致的任务暂停而带来的影响。一旦任务停止，CPU就会处于空闲状态，而这种情况下多出来一个线程就可以充分利用CPU的空闲时间。
- **I/O密集型（2N）：**这种任务应用起来，系统大部分时间用来处理I/O交互，而线程在处理I/O的是时间段内不会占用CPU来处理，这时就可以将CPU交出给其他线程使用。因此在I/O密集型任务的应用中，可以配置多一些线程，具体计算方是2N。

原文链接：https://zhuanlan.zhihu.com/p/330504183

作者：Linux服务器研究 

# 【NO.117】2022年腾讯C++研发类笔试面试试题及答案

题很多，先上题后上答案，便于大家思考

![img](https://pic2.zhimg.com/80/v2-177d94dea223cfe8874b5ae1671c5865_720w.webp)

问题点：

> **1、C和C++的特点与区别？**
> **2、C++的多态**
> **3、虚函数实现**
> **4、C和C++内存分配问题**
> **5、协程**
> **6、CGI的了解**
> **7、进程间通信方式和线程间通信方式**
> **8、TCP握手与释放**
> **9、http和https的区别？**
> **10、虚拟内存的概念与介绍**
> **11、单链表的反转算法**
> **12、红黑树以及其查找复杂度**
> **13、KPM字符串匹配**
> **14、TCP超时等待、重传以及流量控制**
> **15、数据库引擎**
> **16、数据库索引**

## 1.C和C++的特点与区别？

答：（1）C语言特点：

1.作为一种面向过程的结构化语言，易于调试和维护；

2.表现能力和处理能力极强，可以直接访问内存的物理地址；

3.C语言实现了对硬件的编程操作，也适合于应用软件的开发；

4.C语言还具有效率高，可移植性强等特点。

（2）C++语言特点：

1.在C语言的基础上进行扩充和完善，使C++兼容了C语言的面向过程特点，又成为了一种面向对象的程序设计语言；

2.可以使用抽象数据类型进行基于对象的编程；

3.可以使用多继承、多态进行面向对象的编程；

4.可以担负起以模版为特征的泛型化编程。

C++与C语言的本质差别：在于C++是面向对象的，而C语言是面向过程的。或者说C++是在C语言的基础上增加了面向对象程序设

计的新内容，是对C语言的一次更重要的改革，使得C++成为软件开发的重要工具。

## 2.C++的多态

答：C++的多态性用一句话概括：在基类的函数前加上virtual关键字，在派生类中重写该函数，运行时将会根据对象的实际类型来

调用相应的函数。如果对象类型是派生类，就调用派生类的函数；如果对象类型是基类，就调用基类的函数。

1）：用virtual关键字申明的函数叫做虚函数，虚函数肯定是类的成员函数；

2）：存在虚函数的类都有一个一维的虚函数表叫做虚表，类的对象有一个指向虚表开始的虚指针。虚表是和类对应的，虚表指针是

和对象对应的；

3）：多态性是一个接口多种实现，是面向对象的核心，分为类的多态性和函数的多态性。；

4）：多态用虚函数来实现，结合动态绑定.；

5）：纯虚函数是虚函数再加上 = 0；

6）：抽象类是指包括至少一个纯虚函数的类；

纯虚函数：virtual void fun()=0;即抽象类，必须在子类实现这个函数，即先有名称，没有内容，在派生类实现内容。

## 3.虚函数实现

答：简单地说，每一个含有虚函数（无论是其本身的，还是继承而来的）的类都至少有一个与之对应的虚函数表，其中存放着该类

所有的虚函数对应的函数指针。例：

![img](https://pic2.zhimg.com/80/v2-dbf21ae180ebc3b3299248424ce9ff29_720w.webp)

其中：

B的虚函数表中存放着B::foo和B::bar两个函数指针。

D的虚函数表中存放的既有继承自B的虚函数B::foo，又有重写（override）了基类虚函数B::bar的D::bar，还有新增的虚函数D::quz。

虚函数表构造过程：

从编译器的角度来说，B的虚函数表很好构造，D的虚函数表构造过程相对复杂。下面给出了构造D的虚函数表的一种方式（仅供参考）：

![img](https://pic4.zhimg.com/80/v2-2f934c8c3421a2539aea6963db78c61b_720w.webp)

虚函数调用过程

以下面的程序为例：

![img](https://pic4.zhimg.com/80/v2-037d3b0d8ca26903005157c35ede5a37_720w.webp)

## 4.C和C++内存分配问题

答：（1）C语言编程中的内存基本构成

C的内存基本上分为4部分：静态存储区、堆区、栈区以及常量区。他们的功能不同，对他们使用方式也就不同。

1.栈 ——由编译器自动分配释放；

2.堆 ——一般由程序员分配释放，若程序员不释放，程序结束时可能由OS回收；

3.全局区（静态区）——全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域，未初始化的全局变量

和未初始化的静态变量在相邻的另一块区域（C++中已经不再这样划分），程序结束释放；

4.另外还有一个专门放常量的地方，程序结束释放；

(a)函数体中定义的变量通常是在栈上；

(b)用malloc, calloc, realloc等分配内存的函数分配得到的就是在堆上；

(c)在所有函数体外定义的是全局量；

(d)加了static修饰符后不管在哪里都存放在全局区（静态区）；

(e)在所有函数体外定义的static变量表示在该文件中有效，不能extern到别的文件用；

(f)在函数体内定义的static表示只在该函数体内有效；

(g)另外，函数中的”adgfdf”这样的字符串存放在常量区。

（2）C++编程中的内存基本构造

在C++中内存分成5个区，分别是堆、栈、全局/静态存储区、常量存储区和代码区；

1、栈，就是那些由编译器在需要的时候分配，在不需要的时候自动清楚的变量的存储区，里面的变量通常是局部变量、函数参数等。

2、堆，就是那些由new分配的内存块，他们的释放编译器不去管，由我们的应用程序去控制，一般一个new就要对应一个delete。如

果程序员没有释放掉，那么在程序结束后，操作系统会自动回收。

3、全局/静态存储区，全局变量和静态变量被分配到同一块内存中，在以前的C语言中，全局变量又分为初始化的和未初始化的，在

C++里面没有这个区分了，他们共同占用同一块内存区。

4、常量存储区，这是一块比较特殊的存储区，他们里面存放的是常量，不允许修改（当然，你要通过非正当手段也可以修改）。

5、代码区 （.text段），存放代码（如函数），不允许修改（类似常量存储区），但可以执行（不同于常量存储区）。

内存模型组成部分：自由存储区，动态区、静态区；

根据c/c++对象生命周期不同，c/c++的内存模型有三种不同的内存区域，即：自由存储区，动态区、静态区。

自由存储区：局部非静态变量的存储区域，即平常所说的栈；

动态区： 用new ，malloc分配的内存，即平常所说的堆；

静态区：全局变量，静态变量，字符串常量存在的位置；

注：代码虽然占内存，但不属于c/c++内存模型的一部分；

一个正在运行着的C编译程序占用的内存分为5个部分：代码区、初始化数据区、未初始化数据区、堆区 和栈区；

（1）代码区（text segment）：代码区指令根据程序设计流程依次执行，对于顺序指令，则只会执行一次（每个进程），如果反复，则需要使用跳转指令，如果进行递归，则需要借助栈来实现。注意：代码区的指令中包括操作码和要操作的对象（或对象地址引用）。如果是立即数（即具体的数值，如5），将直接包含在代码中；

（2）全局初始化数据区/静态数据区（Data Segment）：只初始化一次。

（3）未初始化数据区（BSS）：在运行时改变其值。

（4）栈区（stack）：由编译器自动分配释放，存放函数的参数值、局部变量的值等，其操作方式类似于数据结构中的栈。

（5）堆区（heap）：用于动态内存分配。

为什么分成这么多个区域？

主要基于以下考虑：

\#代码是根据流程依次执行的，一般只需要访问一次，而数据一般都需要访问多次，因此单独开辟空间以方便访问和节约空间。

\#未初始化数据区在运行时放入栈区中，生命周期短。

\#全局数据和静态数据有可能在整个程序执行过程中都需要访问，因此单独存储管理。

\#堆区由用户自由分配，以便管理。

## 5.协程

答：定义：协程是一种用户态的轻量级线程。

协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。因此：协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置；

线程是抢占式，而协程是协作式；

协程的优点：

跨平台

跨体系架构

无需线程上下文切换的开销

无需原子操作锁定及同步的开销

方便切换控制流，简化编程模型

高并发+高扩展性+低成本：一个CPU支持上万的协程都不是问题。所以很适合用于高并发处理。

协程的缺点：

无法利用多核资源：协程的本质是个单线程,它不能同时将 单个CPU 的多个核用上,协程需要和进程配合才能运行在多CPU；

进行阻塞（Blocking）操作（如IO时）会阻塞掉整个程序：这一点和事件驱动一样，可以使用异步IO操作来解决。

## 6.CGI的了解

答：CGI：通用网关接口（Common Gateway Interface）是一个Web服务器主机提供信息服务的标准接口。通过CGI接口，Web服务

器就能够获取客户端提交的信息，转交给服务器端的CGI程序进行处理，最后返回结果给客户端。

CGI通信系统的组成是两部分：一部分是html页面，就是在用户端浏览器上显示的页面。另一部分则是运行在服务器上的Cgi程序。

## 7.进程间通信方式和线程间通信方式

答：（1）进程间通信方式：

\# 管道( pipe )：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。

\# 信号量( semophore ) ： 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。

\# 消息队列( message queue ) ： 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。

\# 共享内存( shared memory ) ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。

\# 套接字( socket ) ： 套解口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同及其间的进程通信。

（2）线程间通信方式：

\#全局变量；

\#Messages消息机制；

\#CEvent对象（MFC中的一种线程通信对象，通过其触发状态的改变实现同步与通信）。

## 8.TCP握手与释放

答：（1）握手

\#第一次握手：主机A发送握手信号syn＝1和seq=x（随机产生的序列号）的数据包到服务器，主机B由SYN=1知道，A要求建立联机；

\#第二次握手：主机B收到请求后要确认联机信息，向A发送syn=1，ack=x（x是主机A的Seq）+1，以及随机产生的确认端序列号

seq=y的包；

\#第三次握手：主机A收到后检查ack是否正确（ack=x+1），即第一次发送的seq+1，若正确，主机A会再发送ack=y+1，以及随机序

列号seq=z，主机B收到后确认ack值则连接建立成功；

\#完成三次握手，主机A与主机B开始传送数据。

注：上述步骤中，第二和第三次确认包中都还包含一个标志位未予以说明，该标志位为1表示正常应答；

具体可见图片：

![img](https://pic3.zhimg.com/80/v2-08a675dbc65a471de270da54e3bdaa4e_720w.webp)

为什么需要“三次握手”？

“三次握手”的目的是“为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误”。具体例如：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。主要目的防止server端一直等待，浪费资源。

（2）挥手

由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个 FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。

(1) TCP客户端发送一个FIN，用来关闭客户到服务器的数据传送(报文段4)；

(2) 服务器收到这个FIN，发回一个ACK，确认序号为收到的序号加1(报文段5)。和SYN一样，一个FIN将占用一个序号；

(3) 服务器关闭客户端的连接后，再发送一个FIN给客户端(报文段6)；

(4) 客户段收到服务端的FIN后，发回ACK报文确认，并将确认序号设置为收到序号加1(报文段7)；

注意：TCP连接的任何一方都可以发起挥手操作，上述步骤只是两种之一；

具体过程见图：

![img](https://pic2.zhimg.com/80/v2-38876cec3db96818b6de4207c7274ba5_720w.webp)

为什么是“四次挥手”？

因为当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了，所以你可能还需要发送一些数据给对方，再发送FIN报文给对方来表示你同意现在可以关闭连接了，故这里的ACK报文和FIN报文多数情况下都是分开发送的，也就造成了4次挥手。

握手，挥手过程中各状态介绍：

（1）3次握手过程状态：

\#LISTEN: 这个也是非常容易理解的一个状态，表示服务器端的某个SOCKET处于监听状态，可以接受连接了。

\#SYN_SENT: 当客户端SOCKET执行CONNECT连接时，它首先发送SYN报文，因此也随即它会进入到了SYN_SENT状态，并等待服务端的发送三次握手中的第2个报文。SYN_SENT状态表示客户端已发送SYN报文。(发送端)

\#SYN_RCVD: 这个状态与SYN_SENT遥想呼应这个状态表示接受到了SYN报文，在正常情况下，这个状态是服务器端的SOCKET在建立TCP连接时的三次握手会话过程中的一个中间状态，很短暂，基本上用netstat你是很难看到这种状态的，除非你特意写了一个客户端测试程序，故意将三次TCP握手过程中最后一个ACK报文不予发送。因此这种状态时，当收到客户端的ACK报文后，它会进入到ESTABLISHED状态。(服务器端)

\#ESTABLISHED：这个容易理解了，表示连接已经建立了。

（2）4次挥手过程状态：

\#FIN_WAIT_1: 这个状态要好好解释一下，其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。(主动方)

\#FIN_WAIT_2：上面已经详细解释了这种状态，实际上FIN_WAIT_2状态下的SOCKET，表示半连接，也即有一方要求close连接，但另外还告诉对方，我暂时还有点数据需要传送给你(ACK信息)，稍后再关闭连接。(主动方)

\#TIME_WAIT: 表示收到了对方的FIN报文，并发送出了ACK报文，就等2MSL后即可回到CLOSED可用状态了。如果FIN_WAIT_1状态下，收到了对方同时带FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。(主动方)

\#CLOSING(比较少见): 这种状态比较特殊，实际情况中应该是很少见，属于一种比较罕见的例外状态。正常情况下，当你发送FIN报文后，按理来说是应该先收到(或同时收到)对方的ACK报文，再收到对方的FIN报文。但是CLOSING状态表示你发送FIN报文后，并没有收到对方的ACK报文，反而却也收到了对方的FIN报文。什么情况下会出现此种情况呢?其实细想一下，也不难得出结论：那就是如果双方几乎在同时close一个SOCKET的话，那么就出现了双方同时发送FIN报文的情况，也即会出现CLOSING状态，表示双方都正在关闭SOCKET连接。

\#CLOSE_WAIT: 这种状态的含义其实是表示在等待关闭。怎么理解呢?当对方close一个SOCKET后发送FIN报文给自己，你系统毫无疑问地会回应一个ACK报文给对方，此时则进入到CLOSE_WAIT状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。(被动方)

\#LAST_ACK: 这个状态还是比较容易好理解的，它是被动关闭一方在发送FIN报文后，最后等待对方的ACK报文。当收到ACK报文后，也即可以进入到CLOSED可用状态了。(被动方)

## 9.http和https的区别？

答：HTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比http协议安全。

HTTPS（Secure Hypertext Transfer Protocol）安全超文本传输协议，与http主要区别在于：

\#http是超文本传输协议，信息是明文传输，https 则是具有安全性的ssl加密传输协议；

\#http和https使用的是完全不同的连接方式用的端口也不一样,前者是80,后者是443；

下面具体介绍一下HTTP和HTTPS协议：

首先说明一下：HTTP和HTTPS协议是应用层协议；

![img](https://pic1.zhimg.com/80/v2-8db1567ef946013c47b43130c687f96c_720w.webp)

上图充分表明：HTTP是应用层协议，并且HTTPS是在HTTP协议基础上添加SSL等加密策略后的协议；

TLS/SSL中使用了非对称加密，对称加密以及HASH算法。

（1）Http协议

1）HTTP协议和TCP协议之间的区别联系

①TPC/IP协议是传输层协议，主要解决数据如何在网络中传输，而HTTP是应用层协议，主要解决如何包装数据；

②HTTP的默认端口号是80，TCP/IP协议通信编程时端口号需要自己指定（例如socket编程）；

③HTTP协议是在TCP/IP协议基础上实现的，即HTTP数据包是经过TCP/IP协议实现传输的；

④HTTP是无状态的短连接协议，TCP是有状态的长连接协议；

HTTP是在有状态长连接TCP/IP协议的基础上实现的，为什么却是无状态短连接协议？

答：因为HTTP协议每次请求结束就会自动关闭连接，这样就变成了短连接；

短连接又导致了该次请求相关信息的丢失，也就造成了HTTP协议对于前期事务处理没有记忆能力，故为无状态协议。

2）HTTP协议其完整的工作过程可分为四步：

①连接：首先客户机与服务器需要建立连接（由TCP/IP握手连接实现）。只要单击某个超级链接，HTTP的工作开始；

②请求：建立连接后，客户机发送一个请求给服务器，请求方式的格式为：统一资源标识符（URL）、协议版本号，后边是MIME信息包括请求修饰符、客户机信息和可能的内容；

③应答：服务器接到请求后，给予相应的响应信息，其格式为一个状态行，包括信息的协议版本号、一个成功或错误的代码，后边是MIME信息包括服务器信息、实体信息和可能的内容。客户端接收服务器所返回的信息通过浏览器显示在用户的显示屏上；

④关闭：当应答结束后，浏览器和服务器关闭连接，以保证其他浏览器可以与服务器进行连接。

更完整的过程可能如下：

域名解析 –> 发起TCP的3次握手 –> 建立TCP连接后发起http请求 –> 服务器响应http请求，浏览器得到html代码 –> 浏览器解析html代码，并请求html代码中的资源（如js、css、图片等） –> 浏览器对页面进行渲染呈现给用户。

如果在以上过程中的某一步出现错误，那么产生错误的信息将返回到客户端，有显示屏输出。对于用户来说，这些过程是由HTTP自己完成的，用户只要用鼠标点击，等待信息显示就可以了。

（2）Https协议

HTTPS握手过程包括五步：

1）浏览器请求连接；

2）服务器返回证书：证书里面包含了网站地址，加密公钥，以及证书的颁发机构等信息。

3）浏览器收到证书后作以下工作：

a) 验证证书的合法性；

b) 生成随机（对称）密码，取出证书中提供的公钥对随机密码加密；

c) 将之前生成的加密随机密码等信息发送给网站；

4）服务器收到消息后作以下的操作：

a) 使用自己的私钥解密浏览器用公钥加密后的消息，并验证HASH是否与浏览器发来的一致；

b) 使用加密的随机对称密码加密一段消息，发送给浏览器；

5）浏览器解密并计算握手消息的HASH：如果与服务端发来的HASH一致，此时握手过程结束，之后所有的通信数据将由之前浏览

器生成的随机密码并利用对称加密算法进行加密。

注意：服务器有两个密钥，一个公钥、一个私钥，只有私钥才可以解密公钥加密的消息；

如图：

![img](https://pic4.zhimg.com/80/v2-9093cfc39b478d279cd1bb9a070cecdf_720w.webp)

或者如下图：

![img](https://pic1.zhimg.com/80/v2-9a90334d890d97860e19f75b8abcb9e4_720w.webp)

HTTPS协议、SSL、和数字证书的关系介绍：

概述：对于HTTPS协议，所有的消息都是经过SSL协议方式加密，而支持加密的文件正是数字证书；

（1）SSL

SSL常用的加密算法：对称密码算法、非对称密码算法、散列算法；

SSL的加密过程：需要注意的是非对称加解密算法的效率要比对称加解密要低的多。所以SSL在握手过程中使用非对称密码算法来

协商密钥，实际使用对称加解密的方法对http内容加密传输；

（2）数字证书

数字证书是用于在INTERNET上标识个人或者机构身份的一种技术手段，它通过由一些公认的权威机构所认证，从而可以保证其

安全地被应用在各种场合。证书里面包含了网站地址，加密公钥，以及证书的颁发机构等信息。

## 10.虚拟内存的概念与介绍

答：虚拟内存中，允许将一个作业分多次调入内存，需要时就调入，不需要的就先放在外存。因此，虚拟内存的实需要建立在离散

分配的内存管理方式的基础上。虚拟内存的实现有以下三种方式：

\#请求分页存储管理

\#请求分段存储管理

\#请求段页式存储管理

虚拟内存的意义：

一，虚拟内存可以使得物理内存更加高效。虚拟内存使用置换方式，需要的页就置换进来，不需要的置换出去，使得内存中只保存了需要的页，提高了利用率，也避免了不必要的写入与擦除；

二，使用虚拟地址可以使内存的管理更加便捷。在程序编译的时候就会生成虚拟地址，该虚拟地址并不是对应一个物理地址，使得也就极大地减少了地址被占用的冲突，减少管理难度；

三，为了安全性的考虑。在使用虚拟地址的时候，暴露给程序员永远都是虚拟地址，而具体的物理地址在哪里，这个只有系统才了解。这样就提

高了系统的封装性。

## 11.单链表的反转算法

答：思想：创建3个指针，分别指向上一个节点、当前节点、下一个节点，遍历整个链表的同时，将正在访问的节点指向上一个节点，当遍历结束后，就同时完成了链表的反转。

实现代码：

> ListNode* ReverseList(ListNode* pHead) {
> ListNode *p,*q,*r;
> if(pHead==NULL || pHead->next==NULL){
> return pHead;
> }else{
> p=pHead;
> q=p->next;
> pHead->next=NULL;
> while(q!=NULL){
> r=q->next;
> q->next=p;
> p=q;
> q=r;
> }
> return p;
> }
> }

## 12.红黑树以及其查找复杂度

答：（1）红黑树来源于二叉搜索树，其在关联容器如map中应用广泛，主要优势在于其查找、删除、插入时间复杂度小，但其也有缺点，就是容易偏向一边而变成一个链表。

红黑树是一种二叉查找树，但在每个结点上增加一个存储位表示结点的颜色，可以是Red或Black。也就是说，红黑树是在二叉

查找树基础上进一步实现的；

红黑树的五个性质：

性质1. 节点是红色或黑色；

性质2. 根节点是黑色；

性质3 每个叶节点（指树的末端的NIL指针节点或者空节点）是黑色的；

性质4 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点)；

性质5. 从任一节点到其每个尾端NIL节点或者NULL节点的所有路径都包含相同数目的黑色节点。

（注：上述第3、5点性质中所说的NIL或者NULL结点，并不包含数据，只充当树的路径结束的标志，即此叶结点非常见的叶子结点）。

![img](https://pic4.zhimg.com/80/v2-2a866176ed6cdd3621af1e48db5f0f8b_720w.webp)

因为一棵由n个结点随机构造的二叉查找树的高度为lgn，所以顺理成章，二叉查找树的一般操作的执行时间为O(lgn)。但二叉查

找树若退化成了一棵具有n个结点的线性链后，则这些操作最坏情况运行时间为O(n)；

红黑树虽然本质上是一棵二叉查找树，但它在二叉查找树的基础上增加以上五个性质使得红黑树相对平衡，从而保证了

红黑树的查找、插入、删除的时间复杂度最坏为O(log n)。

（2）左旋右旋

红黑树插入或删除后，一般就会改变红黑树的特性，要恢复红黑树上述5个性质，一般都要那就要做2方面的工作：

1、部分结点颜色，重新着色

2、调整部分指针的指向，即左旋、右旋。

左选右旋如图所示：

![img](https://pic2.zhimg.com/80/v2-ea33c58eefced070b3d592ddba5d6eb5_720w.webp)

左旋，如图所示（左->右），以x->y之间的链为“支轴”进行，使y成为该新子树的根，x成为y的左孩子，而y的左孩子则成为x的右孩

子。算法很简单，旋转后各个结点从左往右，仍然都是从小到大。

左旋代码实现，分三步:

（1） 开始变化，y的左孩子成为x的右孩子；

（2） y成为x的父结点；

（3） x成为y的左孩子；

右旋类似，不再累述；

## 13.KMP字符串匹配

（1）KMP匹配算法代码实现：

```
int KmpSearch(char* s, char* p){int i = 0;int j = 0;int sLen = strlen(s);int pLen = strlen(p);while (i < sLen && j < pLen){//①如果j = -1，或者当前字符匹配成功（即S[i] == P[j]），都令i++，j++if (j == -1 || s[i] == p[j]){i++;j++;}else{//②如果j != -1，且当前字符匹配失败（即S[i] != P[j]），则令 i 不变，j = next[j]//next[j]即为j所对应的next值j = next[j];}}if (j == pLen)return i - j;elsereturn -1;}
```

（2）next数组求取

上述（1）中最重要的就是：一旦不匹配，模式串不是向后移动一位，而是根据前面匹配信息移动多位。而这个多位获得就是根据next数组，下面有next数组的求取方式：

Next数组是根据模式串的前缀后缀获取的，如下：

①寻找前缀后缀最长公共元素长度

举个例子，如果给定的模式串为“abab”，那么它的各个子串的前缀后缀的公共元素的最大长度如下表格所示：

![img](https://pic3.zhimg.com/80/v2-ec6ce3c7e05c746b68f7e03e20c6591a_720w.webp)

比如对于字符串aba来说，它有长度为1的相同前缀后缀a；而对于字符串abab来说，它有长度为2的相同前缀后缀ab（相同前缀后缀的长度为k + 1，k + 1 = 2）。

②求next数组

next 数组考虑的是除当前字符外的最长相同前缀后缀，所以通过第①步骤求得各个前缀后缀的公共元素的最大长度后，只要稍作变形即可：将第①步骤中求得的数组整体右移一位，然后第一个元素赋为-1即可（注意：字符串下标需要从0开始），如下表格所示：

![img](https://pic4.zhimg.com/80/v2-82a05f98a309f4dd4678a746136abe93_720w.webp)

比如对于aba来说，第3个字符a之前的字符串ab中有长度为0的相同前缀后缀，所以第3个字符a对应的next值为0；而对于abab来说，第4个字符b之前的字符串aba中有长度为1的相同前缀后缀a，所以第4个字符b对应的next值为1（相同前缀后缀的长度为k，k = 1）。

KMP的next 数组相当于告诉我们：当模式串中的某个字符跟文本串中的某个字符匹配失配时，模式串下一步应该跳到哪个位置（具体：保持测试串的下标i不变，使得匹配串的下标j=next[j]）。

前缀后缀长度求取以及next数组获取：

如果给定的模式串是：“ABCDABD”，从左至右遍历整个模式串，其各个子串的前缀后缀分别如下表格所示：

![img](https://pic2.zhimg.com/80/v2-a01046ffd1cb906c421bea3f7f9f2d6d_720w.webp)

也就是说，原模式串子串对应的各个前缀后缀的公共元素的最大长度表为：

0 0 0 0 1 2 0；

故对应的next数组为：-1 0 0 0 0 1 2；

（注意：这里的字符串下标是从0开始的，若从1开始，next数组所有元素都对应要加1。）

求取next的实现代码：

```
string T; //T为模式串cin>>T;int len=T.size();queue<int> MaxLen;vector<int> next;MaxLen.push(0); //第一个元素都设为0for(int i=1;i<len;i++){int k=1,maxLen=0;while(k<=i){if(T.substr(0,k)==T.substr(i-k+1,k)){maxLen=k;}k++;}MaxLen.push(maxLen);}cout<<endl;next.push_back(-1); //第一个元素都设为-1while(MaxLen.size()>1){int temp=MaxLen.front();next.push_back(temp);MaxLen.pop();cout<<temp<<' ';}
```

## 14.TCP超时等待、重传以及流量控制

答：TCP等待时间需要设定，超过了就认为丢包，需要重传；

为了防止拥塞情况，一般会采用流量控制，其实现手段是用滑动窗口限制客户端发送分组数量；

## 15.数据库引擎

答：数据库引擎是用于存储、处理和保护数据的核心服务。利用数据库引擎可控制访问权限并快速处理事务，从而满足企业内大多

数需要处理大量数据的应用程序的要求。

简言之，数据库引擎就是一段用于支撑所有数据库操作的核心程序，就如名称一样，是一个车的引擎功能；

常见的数据库引擎有：

（1）Microsoft JET (Joint Engineering Technologe) 用于Access和VB的内嵌数据库功能的核心元素；

（2）ODBC（Open DataBase Connectivity，开放数据库互连）是由Microsoft定义的一种数据库访问标准，它提供一种标准的数据

库访问方法以访问不同平台的数据库。一个ODBC应用程序既可以访问在本地PC机上的数据库，也可以访问多种异构平台上的数据

库，例如SQL Server、Oracle或者DB2；

（3）OLE DB是Microsoft开发的最新数据库访问接口，Microsoft将其定义为ODBC接班人；

（4）MYSQL支持三个引擎：ISAM、MYISAM和HEAP。另外两种类型INNODB和BERKLEY（BDB）也常常可以使用；

①ISAM执行读取操作的速度很快，而且不占用大量的内存和存储资源。ISAM的两个主要不足之处在于，它不 支持事务处理，也不能够容错；

②MyISAM是MySQL的ISAM扩展格式和缺省的数据库引擎MYISAM。除了提供ISAM里所没有的索引和字段管理的大量功能，

MyISAM还使用一种表格锁定的机制，来优化多个并发的读写操作，其代价是你需要经常运行OPTIMIZE TABLE命令，来恢复被更新

机制所浪费的空间；

③HEAP允许只驻留在内存里的临时表格。驻留在内存里让HEAP要比ISAM和MYISAM都快，但是它所管理的数据是不稳定的，

而且如果在关机之前没有进行保存，那么所有的数据都会丢失。

## **16.数据库索引**

答：定义：数据库索引是对数据库表中一列或多列的值进行排序的一种结构，使用索引可快速访问数据库表中的特定信息；

举例：employee 表的人员编号列（id）就是数据库索引，select * from employee where id=10000即可查找编号10000的人员信息。如果没有索引，必须遍历整个表直到id=10000；

数据库索引作用：

一，大大加快 数据的检索速度，这也是创建索引的最主要的原因；

二，保证数据库表中每一行数据的唯一性；

三，可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义；

四，在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间；

五，通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。

数据库索引缺陷：

一，表的增删改查、创建索引和维护索引要耗费时间；

二，索引需要占物理空间；

数据库索引的两个特征：索引有两个特征，即唯一性索引和复合索引；

①唯一 性索引保证在索引列中的全部数据是唯一的，不会包含冗余数据；

②复合索引就是一个索引创建在两个列或者多个列上，搜索时需要两个或者多个索引列作为一个关键值；

数据库索引好比是一本书前面的目录，索引分为聚簇索引和非聚簇索引两类：

1）聚簇索引是按照数据存放的物理位置为顺序的，其多个连续行的访问速度更快；

2）非聚簇索引是按照数据存放的逻辑位置为顺序的，其单行访问速度更快；

局部性原理与磁盘预读

局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用。程序运行期间所需要的数据通常比较集中；

磁盘预读：正是由于局部性原理以及数据存储磁盘的读写速度慢的原因，每次对数据库进行读取都不是按需读取，而是读取多

于需求数据区域内的数据到内存，用于后续使用，提高写读取数据速度；

注：磁盘预读一般都是每次读取逻辑上的一页，或物理上的一块，不管实际需求是多少；

数据库索引的实现通常使用B树及其变种B+树，下面进行B-/+Tree结构的数据库索引的性能分析：

（1）B树索引结构：

数据库系统的设计者巧妙利用了磁盘预读原理，将B树的一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以

完全载入。为了达到这个目的，在实际实现B-Tree还需要使用如下技巧：

——每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页；

B-Tree中一次检索最多需要h-1次I/O（磁盘IO不包括根节点，因为根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一

般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。

而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进

复杂度也为O(h)，效率明显比B-Tree差很多。

所以，B树结构的数据库索引，在元素查找上效率很高；

（2）B+树的索引结构：

B+树则适当牺牲检索的时间复杂度（都必须检索到叶子结点），但改善了节点插入和删除的时间复杂度（类似用链表改善数组的效

果），所以B+树属于一种折中选择。

原文链接：https://zhuanlan.zhihu.com/p/274473971

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.118】C++后台开发，以我之见

![img](https://pic3.zhimg.com/80/v2-7694bcb9180778650c4c8ab710df2562_720w.webp)

今天趁着过完春节快要回公司工作之际，也马上进入金三银四的时期，谈谈我个人对后台开发的一些个人见解，希望能够对在校的学生或者刚刚接触C++后台开发的同学有点帮助。

还记得自己在学校的时候，一直都比较注重的是：编程语言+数据结构与算法。没错，对于一个在校的计算机专业的学生，这是很重要的方面。但是，这往往不够，或许是因为毕业前一直没有进入企业实习，以至于自己在毕业之前，对自己未来的职业规划做得很不够，不知道自己以后会做什么方向，那时候比较宽泛且迷茫的定位是，只要是软件开发的工作，我都OK。毕业后，主要是从事C++后台开发，工作一段时间后，才知道自己擅长什么，对什么感兴趣。

## 1.**前端和后端，你喜欢什么？**

一提到前端，大家都会想到html+javascript+css，或许这是web前端的最最基本的东西了吧。我个人会将与用户直接打交道的端称为前端，除了前面所提到的传统意义上的前端，我还会把android和ios开发的app称为前端。现在前端各种框架的迭代速度相当的快，要跟上各种比较NB的框架的步伐，也不是那么简单的事情。虽然工作之后，没有做过前端方面的项目，更多的是与前端工程师FE合作，但是我知道，前端领域也有很多东西要学，而且前端的东西由于能自己直接看到开发结果，或许在工作中会很有成就感，所以永远不要觉得前端工程师做的事情没技术含量，萝卜青菜，各有所爱，任何一个领域，只要深入了，都很有技术含量，关键在于自己喜不喜欢，擅长不擅长。就我个人而言，更喜欢的是后端开发，主要原因是在学校的时候一直学的是C++，工作之后一直做的是后端的项目，没有直接参与前端的项目，既来之，则安之，既然上天给我分配了一个方向，我就应该在这个方向上做深入研究。

## 2.**后台开发是什么？**

我第一次听说过后台开发这个岗位是在腾讯的招聘网站上，有一个岗位叫后台开发。个人觉得，后台开发也很广，开发语言也很多，如：php，node.js，java，C/C++，go ，每一个公司都有自己主打的语言，如腾讯和百度的后端开发中，C++用的比较多，当然php也用得比较多，阿里和美团，java用得比较多。当然，语言只是一种实现工具而已，不能单一地认为那种语言好那种语言不好，没有最好，只有最适合。后台开发，是相对前端开发而言，个人觉得，所有跟前端直接交互的开发都可以认为是后台开发。企业里面，除了前端开发的岗位，就是后台开发了吗？当然不是。这也是我要说的，希望能够给在校的学生一点思考。在互联网公司里面，有美学功底非常好的UE工程师，他们常常会站在用户的角度进行审美，提高用户体验，能够在产品真正落地之前，做出各种demo；有市场调研和需求分析的产品经理PM，具有严密的逻辑思维和良好的沟通能力；有前面所提到的前端工程师FE，负责向后端发送用户提交的请求，并接收后端返回的结果，进行展示；有软件研发工程师RD，需要具备一定的研发能力和bug定位和修复，系统性能优化等能力；有测试开发工程师QA，上线前的最后把关；又做运维的OP，负责维护和监控线上的稳定；有做运营的，像双十一等大型的购物节，一般都需要强大的运营支持；有做大数据的，hadoop+spark+storm各种大数据框架；有做基础架构的；有做算法分析的。。。还有更多的职位。

## 3.**C++后台开发需要掌握什么？**

这个话题有点大，而且像我这种小菜，只能抛砖引玉。语言只是基础，不能一味地去研究语法糖。记得我在学校的时候，特别喜欢去研究语法糖，现在想想，浪费了很多时间。当然，作为C++后端的研发工程师，**你首先需要掌握C++的基础语法，需要掌握STL里面常用的库和算法**，如果你觉得这还不够，你可以去系统地学习下**boost库**，里面多STL里面所不具有很备的，看看C++11就知道了，里面很多新增的东西都是来自boost库。当然，仅仅掌握语言还远远不够，C++做后台开发时，模块跟模块直接除了通过lib库或so库的方式相互调用外，还有更多的是采用网络交互，这个时候，你就需要**掌握多线程编程和网络编程的基础知识，**当然，由于开发效率的需要，现在你不需要从零搭建一个网络服务框架，比如：ACE、boost的asio和libevent。当然现在已经有各种开源的RPC框架了，比如google-rpc，你可以通过调用本地函数来完成网络包的发送与接收，so easy！那么网络通信包的格式如何定义呢？客户端和服务端需要提前约定？数据交互格式，常用的包括：js**on、xml和protobuffer**，通常前端后后端交互会采用json，而后端各个模块的交互，你可以随便选择；对于HTTP协议的交互，我用的比较多的是json，而 tcp协议，我用的比较多的是protobuffer。当然，服务端的平台有很重要，国内后台开发，基本都是运行在Linux系统上，**所以你需要掌握Linux系统的常用的命令**，这样你才可以在Linux系统上运用自如，所以，如果你想从事或者即将从事C++后台开发，请暂时抛下VS下的C++学习，从现在开始，转向Linux平台下的C++开发，那里有**你要编译器GCC/G++，调试时用到的gdb，**如果你想依次性一个命令编译所有的文件，**请学习下如何编写makefile。**好了，有了编程语言，有了编译和调试方法，你就可以将你的应用程序放在你的Linux系统上监听客户端的请求了。如果某一天，你的程序出core了怎么办？你必须要学会如果找出bug，除了前面提到的gdb，在大型的应用里面，你必须要学会掌握如何追bug，这个时候，**你就要学会打日志**，并且分等级打印日志，这样**一出问题了你就能够快速定位问题的所在**。日志有了，程序也能正常跑了，那你怎么算你程序的性能或者收益呢？所以，**你需要学会编写脚本语言**，我个人**推荐你去掌握shell脚本和python脚本**，脚本语言能够一边执行一边编译，具有比较高的开发效率，不用你每次执行前编译，掌握了脚本，你不用再那么忙了，哈哈。

**提高自己的技术硬实力**。**技术的瓶颈是认知的问题，认知不是知其名，还需要知其因，更需要知其原。**这个话题更大，但是适合很多技术岗位。在工作中，你不能只跟项目中的业务逻辑打交道，那样你会觉得自己做的事情越来越没意思，越来越没技术含量。你应该**有一种开源的情怀**，你要找一个比较NB的开源软件，如 redis， zookeeper，nginx等，去阅读其中的源码，当然，你也可以将你写的一些库上传到gitlab上，让大家给你提建议，**相信开源让人进步**；你可以去gitlab上下载和学习各种有意思的开源库，这会给你带来更多的成就感。同时你要**学会利用各种资源来解决你所遇到的各种问题**，如segmentfault，stackoverflow等国外著名的网站。

原文链接：https://zhuanlan.zhihu.com/p/352365043

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.119】ETCD介绍—etcd概念及原理方面分析

etcd作为一个受到ZooKeeper与doozer启发而催生的项目，除了拥有与之类似的功能外，更专注于以下四点。

1. 简单：基于HTTP+JSON的API让你用curl就可以轻松使用。
2. 安全：可选SSL客户认证机制。
3. 快速：每个实例每秒支持一千次写操作。
4. 可信：使用Raft算法充分实现了分布式。

**分布式系统中的数据分为控制数据和应用数据。etcd的使用场景默认处理的数据都是控制数据，对于应用数据，只推荐数据量很小，但是更新访问频繁的情况。应用场景有如下几类：场景一：服务发现（Service Discovery）场景二：消息发布与订阅场景三：负载均衡场景四：分布式通知与协调场景五：分布式锁、分布式队列场景六：集群监控与Leader竞选举个最简单的例子，如果你需要一个分布式存储仓库来存储配置信息，并且希望这个仓库读写速度快、支持高可用、部署简单、支持http接口，那么就可以使用etcd。目前，cloudfoundry使用etcd作为hm9000的应用状态信息存储，kubernetes用etcd来存储docker集群的配置信息等。**

## 1. ETCD是什么

ETCD是用于共享配置和服务发现的分布式，一致性的KV存储系统。该项目目前最新稳定版本为2.3.0. 具体信息请参考[项目首页]和[Github]。ETCD是CoreOS公司发起的一个开源项目，授权协议为Apache。

![img](https://pic3.zhimg.com/80/v2-438b2c4de1230e391107b5b7d9ac980a_720w.webp)

提供配置共享和服务发现的系统比较多，其中最为大家熟知的是[Zookeeper]（后文简称ZK），而ETCD可以算得上是后起之秀了。在项目实现，一致性协议易理解性，运维，安全等多个维度上，ETCD相比Zookeeper都占据优势。

### 1.2. ETCD vs ZK

本文选取ZK作为典型代表与ETCD进行比较，而不考虑[Consul]项目作为比较对象，原因为Consul的可靠性和稳定性还需要时间来验证（项目发起方自身服务并未使用Consul, 自己都不用)。

- 一致性协议： ETCD使用[Raft]协议， ZK使用ZAB（类PAXOS协议），前者容易理解，方便工程实现；
- 运维方面：ETCD方便运维，ZK难以运维；
- 项目活跃度：ETCD社区与开发活跃，ZK已经快死了；
- API：ETCD提供HTTP+JSON, gRPC接口，跨平台跨语言，ZK需要使用其客户端；
- 访问安全方面：ETCD支持HTTPS访问，ZK在这方面缺失；

### 1.3. ETCD的使用场景

和ZK类似，ETCD有很多使用场景，包括：

- 配置管理
- 服务注册于发现
- 选主
- 应用调度
- 分布式队列
- 分布式锁

### 1.4. ETCD读写性能

按照官网给出的[Benchmark], 在2CPU，1.8G内存，SSD磁盘这样的配置下，单节点的写性能可以达到16K QPS, 而先写后读也能达到12K QPS。这个性能还是相当可观的。

### 1.5. ETCD工作原理

ETCD使用Raft协议来维护集群内各个节点状态的一致性。简单说，ETCD集群是一个分布式系统，由多个节点相互通信构成整体对外服务，每个节点都存储了完整的数据，并且通过Raft协议保证每个节点维护的数据是一致的。

![img](https://pic1.zhimg.com/80/v2-b253d80ffd29f87eaba4e22b4521bbf4_720w.webp)

如图所示，每个ETCD节点都维护了一个状态机，并且，任意时刻至多存在一个有效的主节点。主节点处理所有来自客户端写操作，通过Raft协议保证写操作对状态机的改动会可靠的同步到其他节点。

ETCD工作原理核心部分在于Raft协议。本节接下来将简要介绍Raft协议，具体细节请参考其[论文]。
Raft协议正如论文所述，确实方便理解。主要分为三个部分：选主，日志复制，安全性。

#### 1.5.1.选主

Raft协议是用于维护一组服务节点数据一致性的协议。这一组服务节点构成一个集群，并且有一个主节点来对外提供服务。当集群初始化，或者主节点挂掉后，面临一个选主问题。集群中每个节点，任意时刻处于Leader, Follower, Candidate这三个角色之一。选举特点如下：

- 当集群初始化时候，每个节点都是Follower角色；
- 集群中存在至多1个有效的主节点，通过心跳与其他节点同步数据；
- 当Follower在一定时间内没有收到来自主节点的心跳，会将自己角色改变为Candidate，并发起一次选主投票；当收到包括自己在内超过半数节点赞成后，选举成功；当收到票数不足半数选举失败，或者选举超时。若本轮未选出主节点，将进行下一轮选举（出现这种情况，是由于多个节点同时选举，所有节点均为获得过半选票）。
- Candidate节点收到来自主节点的信息后，会立即终止选举过程，进入Follower角色。为了避免陷入选主失败循环，每个节点未收到心跳发起选举的时间是一定范围内的随机值，这样能够避免2个节点同时发起选主。

#### 1.5.2.日志复制

所谓日志复制，是指主节点将每次操作形成日志条目，并持久化到本地磁盘，然后通过网络IO发送给其他节点。其他节点根据日志的逻辑时钟(TERM)和日志编号(INDEX)来判断是否将该日志记录持久化到本地。当主节点收到包括自己在内超过半数节点成功返回，那么认为该日志是可提交的(committed），并将日志输入到状态机，将结果返回给客户端。

这里需要注意的是，每次选主都会形成一个唯一的TERM编号，相当于逻辑时钟。每一条日志都有全局唯一的编号。

![img](https://pic4.zhimg.com/80/v2-c1dc8b61c482eb26124c2c7cc0fd290f_720w.webp)

主节点通过网络IO向其他节点追加日志。若某节点收到日志追加的消息，首先判断该日志的TERM是否过期，以及该日志条目的INDEX是否比当前以及提交的日志的INDEX跟早。若已过期，或者比提交的日志更早，那么就拒绝追加，并返回该节点当前的已提交的日志的编号。否则，将日志追加，并返回成功。

当主节点收到其他节点关于日志追加的回复后，若发现有拒绝，则根据该节点返回的已提交日志编号，发生其编号下一条日志。

主节点像其他节点同步日志，还作了拥塞控制。具体地说，主节点发现日志复制的目标节点拒绝了某次日志追加消息，将进入日志探测阶段，一条一条发送日志，直到目标节点接受日志，然后进入快速复制阶段，可进行批量日志追加。

按照日志复制的逻辑，我们可以看到，集群中慢节点不影响整个集群的性能。另外一个特点是，数据只从主节点复制到Follower节点，这样大大简化了逻辑流程。

#### 1.5.3.安全性

截止此刻，选主以及日志复制并不能保证节点间数据一致。试想，当一个某个节点挂掉了，一段时间后再次重启，并当选为主节点。而在其挂掉这段时间内，集群若有超过半数节点存活，集群会正常工作，那么会有日志提交。这些提交的日志无法传递给挂掉的节点。当挂掉的节点再次当选主节点，它将缺失部分已提交的日志。在这样场景下，按Raft协议，它将自己日志复制给其他节点，会将集群已经提交的日志给覆盖掉。

这显然是不可接受的。

其他协议解决这个问题的办法是，新当选的主节点会询问其他节点，和自己数据对比，确定出集群已提交数据，然后将缺失的数据同步过来。这个方案有明显缺陷，增加了集群恢复服务的时间（集群在选举阶段不可服务），并且增加了协议的复杂度。

Raft解决的办法是，在选主逻辑中，对能够成为主的节点加以限制，确保选出的节点已定包含了集群已经提交的所有日志。如果新选出的主节点已经包含了集群所有提交的日志，那就不需要从和其他节点比对数据了。简化了流程，缩短了集群恢复服务的时间。

这里存在一个问题，加以这样限制之后，还能否选出主呢？答案是：只要仍然有超过半数节点存活，这样的主一定能够选出。因为已经提交的日志必然被集群中超过半数节点持久化，显然前一个主节点提交的最后一条日志也被集群中大部分节点持久化。当主节点挂掉后，集群中仍有大部分节点存活，那这存活的节点中一定存在一个节点包含了已经提交的日志了。

至此，关于Raft协议的简介就全部结束了。

### 1.6. ETCD使用案例

据公开资料显示，至少有CoreOS, Google Kubernetes, Cloud Foundry, 以及在Github上超过500个项目在使用ETCD。

### 1.7. ETCD接口

ETCD提供HTTP协议，在最新版本中支持Google gRPC方式访问。具体支持接口情况如下：

- ETCD是一个高可靠的KV存储系统，支持PUT/GET/DELETE接口；
- 为了支持服务注册与发现，支持WATCH接口（通过http long poll实现）；
- 支持KEY持有TTL属性；
- CAS（compare and swap)操作;
- 支持多key的事务操作；
- 支持目录操作

## 2.ETCD系列之二：部署集群

### 2.1. 概述

想必很多人都知道ZooKeeper，通常用作配置共享和服务发现。和它类似，ETCD算是一个非常优秀的后起之秀了。本文重点不在描述他们之间的不同点。首先，看看其官网关于ETCD的描述1:

> A distributed, reliable key-value store for the most critical data of a distributed system.

在云计算大行其道的今天，ETCD有很多典型的使用场景。常言道，熟悉一个系统先从部署开始。本文接下来将描述，如何部署ETCD集群。

安装官网说明文档，提供了3种集群启动方式，实际上按照其实现原理分为2类：

- 通过静态配置方式启动
- 通过服务发现方式启动

在部署集群之前，我们需要考虑集群需要配置多少个节点。这是一个重要的考量，不得忽略。

### 2.2. 集群节点数量与网络分割

ETCD使用RAFT协议保证各个节点之间的状态一致。根据RAFT算法原理，节点数目越多，会降低集群的写性能。这是因为每一次写操作，需要集群中大多数节点将日志落盘成功后，Leader节点才能将修改内部状态机，并返回将结果返回给客户端。

也就是说在等同配置下，节点数越少，集群性能越好。显然，只部署1个节点是没什么意义的。通常，按照需求将集群节点部署为3，5，7，9个节点。

这里能选择偶数个节点吗？ 最好不要这样。原因有二：

- 偶数个节点集群不可用风险更高，表现在选主过程中，有较大概率或等额选票，从而触发下一轮选举。
- 偶数个节点集群在某些网络分割的场景下无法正常工作。试想，当网络分割发生后，将集群节点对半分割开。此时集群将无法工作。按照RAFT协议，此时集群写操作无法使得大多数节点同意，从而导致写失败，集群无法正常工作。

当网络分割后，ETCD集群如何处理的呢?

- 当集群的Leader在多数节点这一侧时，集群仍可以正常工作。少数节点那一侧无法收到Leader心跳，也无法完成选举。
- 当集群的Leader在少数节点这一侧时，集群仍可以正常工作，多数派的节点能够选出新的Leader, 集群服务正常进行。

当网络分割恢复后，少数派的节点会接受集群Leader的日志，直到和其他节点状态一致。

### 2.3. ETCD参数说明

这里只列举一些重要的参数，以及其用途。

- —data-dir 指定节点的数据存储目录，这些数据包括节点ID，集群ID，集群初始化配置，Snapshot文件，若未指定—wal-dir，还会存储WAL文件；
- —wal-dir 指定节点的was文件的存储目录，若指定了该参数，wal文件会和其他数据文件分开存储。
- —name 节点名称
- —initial-advertise-peer-urls 告知集群其他节点url.
- — listen-peer-urls 监听URL，用于与其他节点通讯
- — advertise-client-urls 告知客户端url, 也就是服务的url
- — initial-cluster-token 集群的ID
- — initial-cluster 集群中所有节点

### 2.4. 通过静态配置方式启动ETCD集群

按照官网中的文档，即可完成集群启动。这里略。

### 2.5. 通过服务发现方式启动ETCD集群

ETCD还提供了另外一种启动方式，即通过服务发现的方式启动。这种启动方式，依赖另外一个ETCD集群，在该集群中创建一个目录，并在该目录中创建一个_config的子目录，并且在该子目录中增加一个size节点，指定集群的节点数目。

在这种情况下，将该目录在ETCD中的URL作为节点的启动参数，即可完成集群启动。使用
–discovery [https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83](https://link.zhihu.com/?target=https%3A//myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83) 配置项取代静态配置方式中的–initial-cluster 和inital-cluster-state参数。其中[https://myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83](https://link.zhihu.com/?target=https%3A//myetcd.local/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83)是在依赖etcd中创建好的目录url。

### 2.6. 节点迁移

在生产环境中，不可避免遇到机器硬件故障。当遇到硬件故障发生的时候，我们需要快速恢复节点。ETCD集群可以做到在不丢失数据的，并且不改变节点ID的情况下，迁移节点。
具体办法是：

- 1）停止待迁移节点上的etc进程；
- 2）将数据目录打包复制到新的节点；
- 3）更新该节点对应集群中peer url，让其指向新的节点；
- 4）使用相同的配置，在新的节点上启动etcd进程；

## 3. 结束

本文记录了ETCD集群启动的一些注意事项，希望对你有帮助。

[https://yq.aliyun.com/articles/29897?spm=5176.100239.blogcont11035.15.7bihps](https://link.zhihu.com/?target=https%3A//yq.aliyun.com/articles/29897%3Fspm%3D5176.100239.blogcont11035.15.7bihps)

## 4.ETCD系列之三：网络层实现

### 4.1. 概述

在理清ETCD的各个模块的实现细节后，方便线上运维，理解各种参数组合的意义。本文先从网络层入手，后续文章会依次介绍各个模块的实现。

本文将着重介绍ETCD服务的网络层实现细节。在目前的实现中，ETCD通过HTTP协议对外提供服务，同样通过HTTP协议实现集群节点间数据交互。

网络层的主要功能是实现了服务器与客户端(能发出HTTP请求的各种程序)消息交互，以及集群内部各节点之间的消息交互。

### 4.2. ETCD-SERVER整体架构

ETCD-SERVER 大体上可以分为网络层，Raft模块，复制状态机，存储模块，架构图如图1所示。

![img](https://pic3.zhimg.com/80/v2-83aa7f6abeef9008e0e94cb528f574e2_720w.webp)

图1 ETCD-SERVER架构图

- 网络层：提供网络数据读写功能，监听服务端口，完成集群节点之间数据通信，收发客户端数据；
- Raft模块：完整实现了Raft协议；
- 存储模块：KV存储，WAL文件，SNAPSHOT管理
- 复制状态机：这个是一个抽象的模块，状态机的数据维护在内存中，定期持久化到磁盘，每次写请求会持久化到WAL文件，并根据写请求的内容修改状态机数据。 ## 3. 节点之间网络拓扑结构 ETCD集群的各个节点之间需要通过HTTP协议来传递数据，表现在：
- Leader 向Follower发送心跳包, Follower向Leader回复消息；
- Leader向Follower发送日志追加信息；
- Leader向Follower发送Snapshot数据；
- Candidate节点发起选举，向其他节点发起投票请求；
- Follower将收的写操作转发给Leader;

各个节点在任何时候都有可能变成Leader, Follower, Candidate等角色，同时为了减少创建链接开销，ETCD节点在启动之初就创建了和集群其他节点之间的链接。

因此，ETCD集群节点之间的网络拓扑是一个任意2个节点之间均有长链接相互连接的网状结构。如图2所示。

![img](https://pic2.zhimg.com/80/v2-4a4357b24660e77bb0588149645b15f9_720w.webp)

图2 ETCD集群节点网络拓扑图

需要注意的是，每一个节点都会创建到其他各个节点之间的长链接。每个节点会向其他节点宣告自己监听的端口，该端口只接受来自其他节点创建链接的请求。

### 4.3. 节点之间消息交互

在ETCD实现中，根据不同用途，定义了各种不同的消息类型。各种不同的消息，最终都通过google protocol buffer协议进行封装。这些消息携带的数据大小可能不尽相同。例如 传输SNAPSHOT数据的消息数据量就比较大，甚至超过1GB, 而leader到follower节点之间的心跳消息可能只有几十个字节。

因此，网络层必须能够高效地处理不同数据量的消息。ETCD在实现中，对这些消息采取了分类处理，抽象出了2种类型消息传输通道：Stream类型通道和Pipeline类型通道。这两种消息传输通道都使用HTTP协议传输数据。

![img](https://pic3.zhimg.com/80/v2-c27e543cf27cbb769a53e0d1674c1c06_720w.webp)

图3 节点之间建立消息传输通道

集群启动之初，就创建了这两种传输通道，各自特点：

- Stream类型通道：点到点之间维护HTTP长链接，主要用于传输数据量较小的消息，例如追加日志，心跳等；
- Pipeline类型通道：点到点之间不维护HTTP长链接，短链接传输数据，用完即关闭。用于传输数据量大的消息，例如snapshot数据。

如果非要做做一个类别的话，Stream就向点与点之间维护了双向传输带，消息打包后，放到传输带上，传到对方，对方将回复消息打包放到反向传输带上；而Pipeline就像拥有N辆汽车，大消息打包放到汽车上，开到对端，然后在回来，最多可以同时发送N个消息。

Stream类型通道
Stream类型通道处理数据量少的消息，例如心跳，日志追加消息。点到点之间只维护1个HTTP长链接，交替向链接中写入数据，读取数据。

Stream 类型通道是节点启动后主动与其他每一个节点建立。Stream类型通道通过Channel 与Raft模块传递消息。每一个Stream类型通道关联2个Goroutines, 其中一个用于建立HTTP链接，并从链接上读取数据, decode成message, 通过Channel传给Raft模块中，另外一个通过Channel 从Raft模块中收取消息，然后写入通道。

具体点，ETCD使用golang的http包实现Stream类型通道：

- 1）被动发起方监听端口, 并在对应的url上挂载相应的handler（当前请求来领时，handler的ServeHTTP方法会被调用）
- 2）主动发起方发送HTTP GET请求；
- 3）监听方的Handler的ServeHTTP访问被调用(框架层传入http.ResponseWriter和http.Request对象），其中http.ResponseWriter对象作为参数传入Writter-Goroutine（就这么称呼吧），该Goroutine的主循环就是将Raft模块传出的message写入到这个responseWriter对象里；http.Request的成员变量Body传入到Reader-Gorouting(就这么称呼吧），该Gorutine的主循环就是不断读取Body上的数据，decode成message 通过Channel传给Raft模块。

Pipeline类型通道
Pipeline类型通道处理数量大消息，例如SNAPSHOT消息。这种类型消息需要和心跳等消息分开处理，否则会阻塞心跳。

Pipeline类型通道也可以传输小数据量的消息，当且仅当Stream类型链接不可用时。

Pipeline类型通道可用并行发出多个消息，维护一组Goroutines, 每一个Goroutines都可向对端发出POST请求（携带数据），收到回复后，链接关闭。

具体地，ETCD使用golang的http包实现的：

- 1）根据参数配置，启动N个Goroutines；
- 2）每一个Goroutines的主循环阻塞在消息Channel上，当收到消息后，通过POST请求发出数据，并等待回复。

### 4.4. 网络层与Raft模块之间的交互

在ETCD中，Raft协议被抽象为Raft模块。按照Raft协议，节点之间需要交互数据。在ETCD中，通过Raft模块中抽象的RaftNode拥有一个message box, RaftNode将各种类型消息放入到messagebox中，有专门Goroutine将box里的消息写入管道，而管道的另外一端就链接在网络层的不同类型的传输通道上，有专门的Goroutine在等待(select）。

而网络层收到的消息，也通过管道传给RaftNode。RaftNode中有专门的Goroutine在等待消息。

也就是说，网络层与Raft模块之间通过Golang Channel完成数据通信。这个比较容易理解。

## 5.ETCD-SERVER处理请求(与客户端的信息交互)

在ETCD-SERVER启动之初，会监听服务端口，当服务端口收到请求后，解析出message后，通过管道传入给Raft模块，当Raft模块按照Raft协议完成操作后，回复该请求（或者请求超时关闭了）。

## 6.主要数据结构

网络层抽象为Transport类，该类完成网络数据收发。对Raft模块提供Send/SendSnapshot接口，提供数据读写的Channel，对外监听指定端口。

![img](https://pic1.zhimg.com/80/v2-e713818a34a2d5726985daf9ad65baf8_720w.webp)

## 7.结束

本文整理了ETCD节点网络层的实现，为分析其他模块打下基础。

原文链接：https://zhuanlan.zhihu.com/p/405811320

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.120】十个问题理解Linux epoll工作原理

epoll 是 linux 特有的一个 I/O 事件通知机制。很久以来对 epoll 如何能够高效处理数以百万记的文件描述符很有兴趣。近期学习、研究了 epoll 源码，在这个过程中关于 epoll 数据结构和作者的实现思路产生出不少疑惑，在此总结为了 10 个问题并逐个加以解答和分析。本文基于的内核源码版本是[2.6.39 版本](https://github.com/torvalds/linux/releases/tag/v2.6.39) 。

### **1.Question 1：是否所有的文件类型都可以被 epoll 监视？**

答案：不是。看下面这个实验代码：

```
#include <stdio.h>#include <unistd.h>#include <sys/epoll.h>#include <stdlib.h>#include <sys/types.h>#include <sys/stat.h>#include <fcntl.h>#include <errno.h>#define MAX_EVENTS 1int main (void){    int epfd;    epfd = epoll_create(100); /* 创建epoll实例，预计监听100个fd */    if (epfd < 0) {        perror ("epoll_create");    }    struct epoll_event *events;    int nr_events, i;    events = malloc (sizeof (struct epoll_event) * MAX_EVENTS);    if (!events) {        perror("malloc");        return 1;    }    /* 打开一个普通文本文件 */    int target_fd = open ("./11.txt", O_RDONLY);    printf("target_fd %d\n", target_fd);    int target_listen_type = EPOLLIN;    for (i = 0; i < 1; i++) {        int ret;        events[i].data.fd = target_fd; /* epoll调用返回后，返回给应用进程的fd号 */        events[i].events = target_listen_type; /* 需要监听的事件类型 */        ret = epoll_ctl (epfd, EPOLL_CTL_ADD, target_fd, &events[i]); /* 注册fd到epoll实例上 */        if (ret) {     printf("ret %d, errno %d\n", ret, errno);            perror ("epoll_ctl");        }    }    /* 应用进程阻塞在epoll上，超时时长置为-1表示一直等到有目标事件才会返回 */    nr_events = epoll_wait(epfd, events, MAX_EVENTS, -1);    if (nr_events < 0) {        perror ("epoll_wait");        free(events);        return 1;    }    for (i = 0; i < nr_events; i++) {        /* 打印出处于就绪状态的fd及其事件 */        printf("event=%d on fd=%d\n", events[i].events, events[i].data.fd);    }    free (events);    close(epfd);    return 0;}
```

编译、运行上面的代码，会打印出下列信息：

```
gcc epoll_test.c -o epdemo./epdemotarget_fd 4ret -1, errno 1epoll_ctl: Operation not permitted
```

正常打开了”txt”文件 fd=4, 但调用 epoll_ctl 监视这个 fd 时却 ret=-1 失败了, 并且错误码为 1，错误信息为”Operation not permitted”。错误码指明这个 fd 不能够被 epoll 监视。

**那什么样的 fd 才可以被 epoll 监视呢？**

只有底层驱动实现了 file_operations 中 poll 函数的文件类型才可以被 epoll 监视！**socket 类型的文件驱动是实现了 poll 函数的，因此才可以被 epoll 监视**。struct file_operations 声明位置是在 include/linux/fs.h 中。

### 2.**Question 2：ep->wq 的作用是什么？**

答案：wq 是一个等待队列，用来保存对某一个 epoll 实例调用 epoll_wait()的所有进程。

一个进程调用 epoll_wait()后，如果当前还没有任何事件发生，需要让当前进程挂起等待（放到 ep->wq 里）；当 epoll 实例监视的文件上有事件发生后，需要唤醒 ep->wq 上的进程去继续执行用户态的业务逻辑。之所以要用一个等待队列来维护关注这个 epoll 的进程，是因为有时候调用 epoll_wait()的不只一个进程，当多个进程都在关注同一个 epoll 实例时，休眠的进程们通过这个等待队列就可以逐个被唤醒了。

多个进程关注同一个 epoll 实例，那么有事件发生后先唤醒谁？后唤醒谁？还是一起全唤醒？这涉及到一个称为“**惊群效应**”的问题。

### **3.Question 3：什么是 epoll 惊群？**

答案：多个进程等待在 ep->wq 上，事件触发后所有进程都被唤醒，但只有其中 1 个进程能够成功继续执行的现象。其他被白白唤起的进程等于做了无用功，可能会造成系统负载过高的问题。下面这段代码能够直观感受什么是 epoll 惊群：

```
#include <sys/types.h>#include <sys/socket.h>#include <sys/epoll.h>#include <netdb.h>#include <string.h>#include <stdio.h>#include <unistd.h>#include <fcntl.h>#include <stdlib.h>#include <errno.h>#include <sys/wait.h>#define PROCESS_NUM 10static int create_and_bind (char *port){    int fd = socket(PF_INET, SOCK_STREAM, 0);    struct sockaddr_in serveraddr;    serveraddr.sin_family = AF_INET;    serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);    serveraddr.sin_port = htons(atoi(port));    bind(fd, (struct sockaddr*)&serveraddr, sizeof(serveraddr));    return fd;}static int make_socket_non_blocking (int sfd){    int flags, s;    flags = fcntl (sfd, F_GETFL, 0);    if (flags == -1)    {        perror ("fcntl");        return -1;    }    flags |= O_NONBLOCK;    s = fcntl (sfd, F_SETFL, flags);    if (s == -1)    {        perror ("fcntl");        return -1;    }    return 0;}#define MAXEVENTS 64int main (int argc, char *argv[]){    int sfd, s;    int efd;    struct epoll_event event;    struct epoll_event *events;    sfd = create_and_bind("8001");    if (sfd == -1)        abort ();    s = make_socket_non_blocking (sfd);    if (s == -1)        abort ();    s = listen(sfd, SOMAXCONN);    if (s == -1)    {        perror ("listen");        abort ();    }    efd = epoll_create(MAXEVENTS);    if (efd == -1)    {        perror("epoll_create");        abort();    }    event.data.fd = sfd;    //event.events = EPOLLIN | EPOLLET;    event.events = EPOLLIN;    s = epoll_ctl(efd, EPOLL_CTL_ADD, sfd, &event);    if (s == -1)    {        perror("epoll_ctl");        abort();    }    /* Buffer where events are returned */    events = calloc(MAXEVENTS, sizeof event);    int k;    for(k = 0; k < PROCESS_NUM; k++)    {        int pid = fork();        if(pid == 0)        {            /* The event loop */            while (1)            {                int n, i;                n = epoll_wait(efd, events, MAXEVENTS, -1);                printf("process %d return from epoll_wait!\n", getpid());             for (i = 0; i < n; i++)                {                    if ((events[i].events & EPOLLERR) || (events[i].events & EPOLLHUP) || (!(events[i].events & EPOLLIN)))                    {                        /* An error has occured on this fd, or the socket is not ready for reading (why were we notified then?) */                        fprintf (stderr, "epoll error\n");                        close (events[i].data.fd);                        continue;                    }                    else if (sfd == events[i].data.fd)                    {                        /* We have a notification on the listening socket, which means one or more incoming connections. */                        struct sockaddr in_addr;                        socklen_t in_len;                        int infd;                        char hbuf[NI_MAXHOST], sbuf[NI_MAXSERV];                        in_len = sizeof in_addr;                        infd = accept(sfd, &in_addr, &in_len);                        if (infd == -1)                        {                            printf("process %d accept failed!\n", getpid());                            break;                        }                        printf("process %d accept successed!\n", getpid());                        /* Make the incoming socket non-blocking and add it to the list of fds to monitor. */                        close(infd);                    }                }            }        }    }    int status;    wait(&status);    free (events);    close (sfd);    return EXIT_SUCCESS;}
```

将服务端的监听 socket fd 加入到 epoll_wait 的监视集合中，这样当有客户端想要建立连接，就会事件触发 epoll_wait 返回。此时如果 10 个进程同时在 epoll_wait 同一个 epoll 实例就出现了惊群效应。所有 10 个进程都被唤起，但只有一个能成功 accept。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171624181622724.png)

为了解决 epoll 惊群，内核后续的高版本又提供了 EPOLLEXCLUSIVE 选项和 SO_REUSEPORT 选项，我个人理解两种解决方案思路上的不同点在于：EPOLLEXCLUSIVE 是在唤起进程阶段起作用，只唤起排在队列最前面的 1 个进程；而 SO_REUSEPORT 是在分配连接时起作用，相当于每个进程自己都有一个独立的 epoll 实例，内核来决策把连接分配给哪个 epoll。

### 4.**Question 4：ep->poll_wait 的作用是什么？**

答案：ep->poll_wait 是 epoll 实例中另一个等待队列。当被监视的文件是一个 epoll 类型时，需要用这个等待队列来处理递归唤醒。

在阅读内核代码过程中，ep->wq 还算挺好理解，但我发现伴随着 ep->wq 唤醒， 还有一个 ep->poll_wait 的唤醒过程。比如下面这段代码，在 eventpoll.c 中出现了很多次：

```
/* If the file is already "ready" we drop it inside the ready list */    if ((revents & event->events) && !ep_is_linked(&epi->rdllink)) {        list_add_tail(&epi->rdllink, &ep->rdllist);        /* Notify waiting tasks that events are available */        if (waitqueue_active(&ep->wq))            wake_up_locked(&ep->wq);        if (waitqueue_active(&ep->poll_wait))            pwake++;    }    spin_unlock_irqrestore(&ep->lock, flags);    atomic_long_inc(&ep->user->epoll_watches);    /* We have to call this outside the lock */    if (pwake)        ep_poll_safewake(&ep->poll_wait);
```

查阅很多资料后才搞明白其实 epoll 也是一种文件类型，其底层驱动也**实现了 file_operations 中的 poll 函数**，因此一个 epoll 类型的 fd 可以被其他 epoll 实例监视。而 epoll 类型的 fd 只会有“读就绪”的事件。当 epoll 所监视的非 epoll 类型文件有“读就绪”事件时，当前 epoll 也会进入“读就绪”状态。

因此如果一个 epoll 实例监视了另一个 epoll 就会出现递归。举个例子，如图所示：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171624345489147.png)

1. epollfd1 监视了 2 个“非 epoll”类型的 fd
2. epollfd2 监视了 epollfd1 和 2 个“非 epoll”类型的 fd

如果 epollfd1 所监视的 2 个 fd 中有可读事件触发，fd 的 ep_poll_callback 回调函数会触发将 fd 放到 epollfd1 的 rdllist 中。此时 epollfd1 本身的可读事件也会触发，就需要从 epollfd1 的 poll_wait 等待队列中找到 epollfd2，调用 epollfd1 的 ep_poll_callback(将 epollfd1 放到 epollfd2 的 rdllist 中)。**因此 ep->poll_wait 是用来处理 epoll 间嵌套监视的情况的。**

**
**

### 5.**Question 5：ep->rdllist 的作用是什么？**

答案：epoll 实例中包含就绪事件的 fd 组成的链表。

通过扫描 ep->rdllist 链表，内核可以轻松获取当前有事件触发的 fd。而不是像 select()/poll() 那样全量扫描所有被监视的 fd，再从中找出有事件就绪的。因此可以说这一点决定了 epoll 的性能是远高于 select/poll 的。

看到这里你可能又产生了一个小小的疑问：**为什么 epoll 中事件就绪的 fd 会“主动”跑到 rdllist 中去，而不用全量扫描就能找到它们呢？** 这是因为每当调用 epoll_ctl 新增一个被监视的 fd 时，都会注册一下这个 fd 的回调函数 ep_poll_callback， 当网卡收到数据包会触发一个中断，中断处理函数再回调 ep_poll_callback 将这个 fd 所属的“epitem”添加至 epoll 实例中的 rdllist 中。

### 6.**Question 6：ep->ovflist 的作用是什么？**

答案：在 rdllist 被占用时，用来在不持有 ep->lock 的情况下收集有就绪事件的 fd。

当 epoll 上已经有了一些就绪事件的时候，内核需要扫描 rdllist 将就绪的 fd 返回给用户态。这一步通过 ep_scan_ready_list 函数来实现。其中 sproc 是一个回调函数(也就是 ep_send_events_proc 函数)，来处理数据从内核态到用户态的复制。

```
/** * ep_scan_ready_list - Scans the ready list in a way that makes possible for the scan code, to call f_op->poll(). Also allows for O(NumReady) performance. * @ep: Pointer to the epoll private data structure. * @sproc: Pointer to the scan callback. * @priv: Private opaque data passed to the @sproc callback. * Returns: The same integer error code returned by the @sproc callback. */static int ep_scan_ready_list(struct eventpoll *ep,                  int (*sproc)(struct eventpoll *,                       struct list_head *, void *),                  void *priv)
```

由于 rdllist 链表业务非常繁忙（epoll 增加监视文件、修改监视文件、有事件触发…等情况都需要操作 rdllist)，所以在复制数据到用户空间时，加了一个 ep->mtx 互斥锁来保护 epoll 自身数据结构线程安全，此时其他执行流程里有争抢 ep->mtx 的操作都会因命中 ep->mtx 进入休眠。

但加锁期间很可能有新事件源源不断地产生，进而调用 ep_poll_callback(ep_poll_callback 不用争抢 ep->mtx 所以不会休眠)，新触发的事件需要一个地方来收集，不然就丢事件了。这个用来临时收集新事件的链表就是 ovflist。我的理解是：引入 ovflist 后新产生的事件就不用因为想向 rdllist 里写而去和 ep_send_events_proc 争抢自旋锁(ep->lock), 同时 ep_send_events_proc 也可以放心大胆地在无锁(不持有 ep->lock)的情况下修改 rdllist。

看代码时会发现，还有一个 txlist 链表，这个链表用来最后向用户态复制数据，rdllist 要先把自己的数据全部转移到 txlist，然后 rdllist 自己被清空。ep_send_events_proc 遍历 txlist 处理向用户空间复制，复制成功后如果是水平触发(LT)还要把这个事件还回 rdllist，等待下一次 epoll_wait 来获取它。

ovflist 上的 fd 会合入 rdllist 上等待下一次扫描；如果 txlist 上的 fd 没有处理完，最后也会合入 rdllist。这 3 个链表的关系是这样：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171627484871493.png)

### 7.**Question 7：epitem->pwqlist 队列的作用是什么？**

答案：用来保存这个 epitem 的 poll 等待队列。

首先介绍下什么是 epitem。epitem 是 epoll 中很重要的一种数据结构， 是红黑树和 rdllist 的基本组成元素。需要监听的文件和事件信息，都被包装在 epitem 结构里。

```
struct epitem {    struct rb_node rbn;  // 用于加入红黑树    struct list_head rdllink; // 用于加入rdllist    struct epoll_filefd ffd; // 包含被监视文件的文件指针和fd信息    struct list_head pwqlist; // poll等待队列    struct eventpoll *ep; // 所属的epoll实例    struct epoll_event event;  // 关注的事件    /* 其他成员省略 */};
```

回忆一下上文说到，每当用户调用 epoll_ctl()新增一个监视文件，都要给这个文件注册一个回调函数 ep_poll_callback, 当网卡收到数据后软中断会调用这个 ep_poll_callback 把这个 epitem 加入到 ep->rdllist 中。

**pwdlist 就是跟 ep_poll_callback 注册相关的**。

当调用 epoll_ctl()新增一个监视文件后，内核会为这个 epitem 创建一个 eppoll_entry 对象，通过 eppoll_entry->wait_queue_t->wait_queue_func_t 来设置 ep_poll_callback。pwdlist 为什么要做成一个队列呢，直接设置成 eppoll_entry 对象不就行了吗？实际上不同文件类型实现 file_operations->poll 用到等待队列数量可能不同。虽然大多数都是 1 个，但也有例外。比如“scullpipe”类型的文件就用到了 2 个等待队列。

pwqlist、epitem、fd、epoll_entry、ep_poll_callback 间的关系是这样：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171627586820957.png)

### 8.**Question 8：epmutex、ep->mtx、ep->lock 3 把锁的区别是？**

答案：锁的粒度和使用目的不同。

1. epmutex 是一个全局互斥锁，epoll 中一共只有 3 个地方用到这把锁。分别是 ep_free() 销毁一个 epoll 实例时、eventpoll_release_file() 清理从 epoll 中已经关闭的文件时、epoll_ctl() 时避免 epoll 间嵌套调用时形成死锁。我的理解是 epmutex 的锁粒度最大，用来处理跨 epoll 实例级别的同步操作。
2. ep->mtx 是一个 epoll 内部的互斥锁，在 ep_scan_ready_list() 扫描就绪列表、eventpoll_release_file() 中执行 ep_remove()删除一个被监视文件、ep_loop_check_proc()检查 epoll 是否有循环嵌套或过深嵌套、还有 epoll_ctl() 操作被监视文件增删改等处有使用。可以看出上述的函数里都会涉及对 epoll 实例中 rdllist 或红黑树的访问，因此我的理解是 ep->mtx 是一个 epoll 实例内的互斥锁，用来保护 epoll 实例内部的数据结构的线程安全。
3. ep->lock 是一个 epoll 实例内部的自旋锁，用来保护 ep->rdllist 的线程安全。自旋锁的特点是得不到锁时不会引起进程休眠，所以在 ep_poll_callback 中只能使用 ep->lock，否则就会丢事件。

### 9.**Question 9：epoll 使用红黑树的目的是什么？**

答案：用来维护一个 epoll 实例中所有的 epitem。

用户态调用 epoll_ctl()来操作 epoll 的监视文件时，需要增、删、改、查等动作有着比较高的效率。尤其是当 epoll 监视的文件数量达到百万级的时候，选用不同的数据结构带来的效率差异可能非常大。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171628079043314.png)

从时间(增、删、改、查、按序遍历)、空间(存储空间大小、扩展性)等方面考量，红黑树都是非常优秀的数据结构(当然这以红黑树比较高的实现复杂度作为代价)。epoll 红黑树中的 epitem 是按什么顺序组织的。阅读代码可以发现是先比较 2 个文件指针的地址大小，如果相同再比较文件 fd 的大小。

```
/* Compare RB tree keys */static inline int ep_cmp_ffd(struct epoll_filefd *p1, struct epoll_filefd *p2){    return (p1->file > p2->file ? +1 : (p1->file < p2->file ? -1 : p1->fd - p2->fd));}
```

epoll、epitem、和红黑树间的组织关系是这样：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171628165972322.png)

### 10.**Question 10：什么是水平触发、边缘触发？**

答案：水平触发(LT)和边缘触发(ET)是 epoll_wait 的 2 种工作模式。水平触发：关注点是数据（读操作缓冲区不为空，写操作缓冲区不为满），epoll_wait 总会返回就绪。LT 是 epoll 的默认工作模式。

边缘触发：关注点是变化，只有监视的文件上有数据变化发生（读操作关注有数据写进缓冲区，写操作关注数据从缓冲区取走），epoll_wait 才会返回。

看一个[实验](https://github.com/cheerfuldustin/test_epoll_lt_and_et) ,直观感受下 2 种模式的区别, 客户端都是输入“abcdefgh” 8 个字符，服务端每次接收 2 个字符。

水平触发时，客户端输入 8 个字符触发了一次读就绪事件，由于被监视文件上还有数据可读故一直返回读就绪，服务端 4 次循环每次都能取到 2 个字符，直到 8 个字符全部读完。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171628283365857.png)

边缘触发时，客户端同样输入 8 个字符但服务端一次循环读到 2 个字符后这个读就绪事件就没有了。等客户端再输入一个字符串后，服务端关注到了数据的“变化”继续从缓冲区读接下来的 2 个字符“c”和”d”。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171628595662381.png)

### **11.小结**

本文通过 10 个问题，其实也是从 10 个不同的视角去观察 epoll 这间宏伟的殿堂。至此也基本介绍完了 epoll 从监视事件，到内部数据结构组织、事件处理，最后到 epoll_wait 返回的整体工作过程。最后附上一张 epoll 相关数据结构间的关系图，在学习 epoll 过程中它曾解答了我心中不少的疑惑，我愿称之为灯塔~
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171629098386883.png)

**参考资料**

1. [Implementation of Epoll](https://fd3kyt.github.io/posts/implementation-of-epoll/)
2. [Red-black Trees (rbtree) in Linux](https://www.kernel.org/doc/html/latest/core-api/rbtree.html)
3. [What is the purpose of epoll’s edge triggered option?](https://stackoverflow.com/questions/9162712/what-is-the-purpose-of-epolls-edge-triggered-option)
4. [epoll 源码分析(基于 linux-5.1.4)](https://icoty.github.io/2019/06/03/epoll-source/)
5. [epoll 实现原理](https://www.jianshu.com/p/81610605e623)
6. [epoll (2) source code analysis](https://www.programmersought.com/article/73753380894/)
7. [epoll 的内核实现](https://blog.csdn.net/wallwind/article/details/8981641)
8. [Linux Kernel Notes: epoll Implementation Principle](https://programming.vip/docs/linux-kernel-notes-epoll-implementation-principle.html)
9. [accept 与 epoll 惊群](https://pureage.info/2015/12/22/thundering-herd.html)

原文作者：dustinzhou，腾讯 IEG 运营开发工程师

原文链接：https://mp.weixin.qq.com/s/h3CBZt2KEA-ScXFSKHaRBg

# 【NO.121】GPU虚拟化，算力隔离，和qGPU

### **0.本文写作背景**

大约 2 年前，在腾讯内网，笔者和很多同事讨论了 GPU 虚拟化的现状和问题。从那以后，出现了一些新的研究方向，并且，有些业界变化，可能会彻底颠覆掉原来的一些论断。

但这里并不是要重新介绍完整的 GPU 虚拟化的方案谱系。而是，我们将聚焦在英伟达 GPU + CUDA 计算领域，介绍下我们最新的技术突破 qGPU，以及它的意义究竟是什么。关于 GPU 虚拟化的历史性介绍，我将直接摘抄当时的讨论。

这也不是一篇介绍 TKE qGPU 产品特性的文章。而是，我们将潜入到前所未有的深度，去探索 GPU 调度和 QoS 的本质。本文也不是巨细靡遗的系统性探索，但你可以在这里看到别处不曾出现过的知识。

本文涉及对一些厂商的推测性技术介绍，不保证准确性。

### 1.术语介绍**

**GPU —————** Graphics Processing Unit，显卡

**CUDA ————** Compute Unified Device Architecture，英伟达 2006 年推出的计算 API

**VT/VT-x/VT-d —** Intel Virtualization Technology。-x 表示 x86 CPU，-d 表示 Device。

**SVM —————** AMD Secure Virtual Machine。AMD 的等价于 Intel VT-x 的技术。

**EPT —————** Extended Page Table，Intel 的 CPU 虚拟化中的页表虚拟化硬件支持。

**NPT —————** Nested Page Table，AMD 的等价于 Intel EPT 的技术。

**SR-IOV ———** Single Root I/O Virtualization。PCI-SIG 2007 年推出的 PCIe 虚拟化技术。

**PF —————** Physical Function，亦即物理卡

**VF —————** Virtual Function，亦即 SR-IOV 的虚拟 PCIe 设备

**MMIO ———** Memory Mapped I/O。设备上的寄存器或存储，CPU 以内存读写指令来访问。

**CSR ————** Control & Status Register，设备上的用于控制、或反映状态的寄存器。CSR 通常以 MMIO 的方式访问。

**UMD ————** User Mode Driver。GPU 的用户态驱动程序，例如 CUDA 的 UMD 是 libcuda.so

**KMD ————** Kernel Mode Driver。GPU 的 PCIe 驱动，例如英伟达 GPU 的 KMD 是 nvidia.ko

**GVA ————** Guest Virtual Address，VM 中的 CPU 虚拟地址

**GPA ————** Guest Physical Address，VM 中的物理地址

**HPA ————** Host Physical Address，Host 看到的物理地址

**IOVA ————** I/O Virtual Address，设备发出去的 DMA 地址

**PCIe TLP ——** PCIe Transaction Layer Packet

**BDF ————** Bus/Device/Function，一个 PCIe/PCI 功能的 ID

**MPT ————** Mediated Pass-Through，受控直通，一种设备虚拟化的实现方式

**MDEV ———** Mediated Device，Linux 中的 MPT 实现

**PRM ————** Programming Reference Manual，硬件的编程手册

**MIG ————** Multi-Instance GPU，Ampere 架构高端 GPU 如 A100 支持的一种 hardware partition 方案

### 2.GPU 虚拟化的历史和谱系**

#### 2.1 GPU 能做什么

GPU 天然适合向量计算。常用场景及 API：
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171635106223937.png)
此外还有加解密、哈希等场景，例如近些年来的挖矿。渲染是 GPU 诞生之初的应用: GPU 的 G 就是 Graphics —— 图形。

桌面、服务器级别的 GPU，长期以来仅有三家厂商:

1. **英伟达**：GPU 的王者。主要研发力量在美国和印度。
2. **AMD/ATI**：ATI 于 2006 年被 AMD 收购。渲染稍逊英伟达，计算的差距更大。
3. **Intel**：长期只有集成显卡，近年来开始推独立显卡。

2006 这一年，GPU 工业界发生了三件大事: ATI 被 AMD 收购；nVidia 黄仁勋提出了 CUDA 计算；Intel 宣布要研发独立显卡。

日光之下并无新事。如同经常发生的，这些事有成功有失败: Intel 很快就放弃了它的独立显卡，直到 2018 才终于明白过来自己到底放弃了什么，开始决心生产独立显卡；AMD 整合 ATI 不太成功，整个公司差点被拖死，危急时公司股票跌到不足 2 美元；而当时不被看好的 CUDA，则在几年后取得了不可思议的成功。

从 2012 年开始，人工智能领域的深度学习方法开始崛起，此时 CUDA 受到青睐，并很快统治了这个领域。

#### 2.2 系统虚拟化和 OS 虚拟化

系统虚拟化演化之路，起初是和 GPU 的演化完全正交的：

- 1998 年，VMWare 公司成立，采用 Binary Translation 方式，实现了系统虚拟化。
- 2001 年，剑桥大学 Xen Source，提出了 PV 虚拟化(Para-Virtualization)，亦即 Guest-Host 的主动协作来实现虚拟化。
- 2005 年，Intel 提出了 VT，最初实现是安腾 CPU 上的 VT-i (VT for Itanium)，很快就有了 x86 上的 VT-x。
- 2007 年，Intel 提出了 VT-d (VT for Device)，亦即 x86 上的 IOMMU。
- 2008 年，Intel 提出了 EPT，支持了内存虚拟化。
- 2010 年，Linux 中的 PV Hypervisor lguest 的作者，Rusty Russell（他更有名的作品是 iptables/netfilter），提出了 VirtIO，一种 Guest-Host 的 PV 设备虚拟化方案。

应该可以说，在 PV 时代和 Binary Translation 时代，虚拟化是危险的。只有当 VT 在硬件层面解决了 CPU 的隔离、保证了安全性之后，公有云才成为可能。VT-x 于 2005 ～ 2006 年出现，亚马逊 AWS 于 2006 年就提出云计算，这是非常有远见的。

系统的三个要素: CPU，内存，设备。CPU 虚拟化由 VT-x/SVM 解决，内存虚拟化由 EPT/NPT 解决，这些都是非常确定的。但设备虚拟化呢？它的情况要复杂的多，不管是 VirtIO，还是 VT-d，都不能彻底解决设备虚拟化的问题，这些我们稍后还会谈到。

除了这种完整的系统虚拟化，还有一种也往往被称作「虚拟化」的方式: 从 OS 级别，把一系列的 library 和 process 捆绑在一个环境中，但所有的环境共享同一个 OS Kernel。

严格来说，这种容器技术，和以 KVM 为代表的系统虚拟化，有着本质的区别。随着容器的流行，「虚拟化」这个术语，也被用来指称这种 OS 级别的容器技术。因此我们也从众，把它也算作虚拟化的一种 —— 只不过为了区分，称之为「OS 虚拟化」。

这种 OS 虚拟化最初于 2005 年，由 Sun 公司在 Solaris 10 上实现，名为「Solaris Zone」。Linux 在 2007 ～ 2008 开始跟进，接下来有了 LXC 容器等；到了 2013 年，Docker 横空出世，彻底改变了软件分发的生态，成为事实上的标准。

#### 2.3 GPU 虚拟化的谱系

##### **2.3.1 作为 PCIe 设备的 GPU**

不考虑嵌入式平台的话，那么，GPU 首先是一个 PCIe 设备。GPU 的虚拟化，还是要首先从 PCIe 设备虚拟化角度来考虑。

那么一个 PCIe 设备，有什么资源？有什么能力？

2 种资源:

- 配置空间
- MMIO
- (有的还有 PIO 和 Option ROM，此略)

2 种能力:

- 中断能力
- DMA 能力

一个典型的 GPU 设备的工作流程是:

1. 应用层调用 GPU 支持的某个 API，如 OpenGL 或 CUDA
2. OpenGL 或 CUDA 库，通过 UMD (User Mode Driver)，提交 workload 到 KMD (Kernel Mode Driver)
3. KMD 写 CSR MMIO，把它提交给 GPU 硬件
4. GPU 硬件开始工作… 完成后，DMA 到内存，发出中断给 CPU
5. CPU 找到中断处理程序 —— KMD 此前向 OS Kernel 注册过的 —— 调用它
6. 中断处理程序找到是哪个 workload 被执行完毕了，…最终驱动唤醒相关的应用

##### **2.3.2 PCIe 直通**

我们首先来到 GPU 虚拟化的最保守的实现: PCIe 设备直通。

如前述，一个 PCIe 设备拥有 2 种资源、2 种能力。你把这 2 种资源都（直接或间接地）交给 VM、针对这 2 种能力都把设备和 VM 接通，那么，VM 就能完整使用这个 PCIe 设备，就像在物理机上一样。这种方案，我们称之为 PCIe 直通（PCIe Pass-Through）。它只能 1:1，不支持 1:N。其实并不能算真正的虚拟化，也没有超卖的可能性。

VM 中，使用的是原生的 GPU 驱动。它向 VM 内核分配内存，把 GPA 填入到 GPU 的 CSR 寄存器，GPU 用它作为 IOVA 来发起 DMA 访问，VT-d 保证把 GPA 翻译为正确的 HPA，从而 DMA 到达正确的物理内存。

PCIe 协议，在事务层(Transaction Layer)，有多种 TLP，DMA 即是其中的一种: MRd/MWr。在这种 TLP 中，必须携带发起者的 Routing ID，而在 IOMMU 中，就根据这样的 Routing ID，可以使用不同的 IOMMU 页表进行翻译。

很显然，PCIe 直通只能支持 1:1 的场景，无法满足 1:N 的需求。

##### **2.3.3 SR-IOV**

那么，业界对 1:N 的 PCIe 虚拟化是如何实现的呢？我们首先就会想到 SR-IOV。SR-IOV 是 PCI-SIG 在 2007 年推出的规范，目的就是 PCIe 设备的虚拟化。SR-IOV 的本质是什么？考虑我们说过的 2 种资源和 2 种能力，来看看一个 VF 有什么:

- 配置空间是虚拟的（特权资源）
- MMIO 是物理的
- 中断和 DMA，因为 VF 有自己的 PCIe 协议层的标识（Routing ID，就是 BDF），从而拥有独立的地址空间。

那么，什么设备适合实现 SR-IOV？其实无非是要满足两点:

- 硬件资源要容易 partition
- 无状态（至少要接近无状态）

常见 PCIe 设备中，最适合 SR-IOV 的就是网卡了: 一或多对 TX/RX queue + 一或多个中断，结合上一个 Routing ID，就可以抽象为一个 VF。而且它是近乎无状态的。

试考虑 NVMe 设备，它的资源也很容易 partition，但是它有存储数据，因此在实现 SR-IOV 方面，就会有更多的顾虑。

回到 GPU 虚拟化：为什么 2007 年就出现 SR-IOV 规范、直到 2015 业界才出现第一个「表面上的」SRIOV-capable GPU【1】？这是因为，虽然 GPU 基本也是无状态的，但是它的硬件复杂度极高，远远超出 NIC、NVMe 这些，导致硬件资源的 partition 很难实现。

**注释**

【1】 AMD S7150 GPU。腾讯云 GA2 机型使用。

表面上它支持 SR-IOV，但事实上硬件只是做了 VF 在 PCIe 层的抽象。Host 上还需要一个 Virtualization-Aware 的 pGPU 驱动，负责 VF 的模拟和调度。

##### **2.3.4 API 转发**

因此，在业界长期缺乏 SRIOV-capable GPU、又有强烈的 1:N 需求的情形下，就有更 high-level 的方案出现了。我们首先回到 GPU 应用的场景:

1. 渲染（OpenGL、DirectX，etc.）
2. 计算（CUDA，OpenCL）
3. 媒体编解码（VAAPI…)

业界就从这些 API 入手，在软件层面实现了「GPU 虚拟化」。以 AWS Elastic GPU 为例:

- VM 中看不到真的或假的 GPU，但可以调用 OpenGL API 进行渲染
- 在 OpenGL API 层，软件捕捉到该调用，转发给 Host
- Host 请求 GPU 进行渲染
- Host 把渲染的结果，转发给 VM

API 层的 GPU 虚拟化是目前业界应用最广泛的 GPU 虚拟化方案。它的好处是:

- **灵活**。1:N 的 N，想定为多少，软件可自行决定；哪个 VM 的优先级高，哪个 VM 的优先级低，同理。
- **不依赖于 GPU 硬件厂商**。微软、VMWare、Citrix、华为……都可以实现。这些 API 总归是公开的。
- **不限于系统虚拟化环境**。容器也好，普通的物理机也好，都可以 API 转发到远端。

缺点呢？

- **复杂度极高**。同一功能有多套 API（渲染的 DirectX 和 OpenGL），同一套 API 还有不同版本（如 DirectX 9 和 DirectX 11），兼容性就复杂的要命。
- **功能不完整**。计算渲染媒体都支持的 API 转发方案，还没听说过。并且，编解码甚至还不存在业界公用的 API！【1】

**注释**

【1】 Vulkan 的编解码支持，spec 刚刚添加，有望被所有 GPU 厂商支持。见下「未来展望」部分。

##### **2.3.5 MPT/MDEV/vGPU**

鉴于这些困难，业界就出现了 SR-IOV、API 转发之外的第三种方案。我们称之为 MPT（Mediated Pass-Through，受控的直通）。MPT 本质上是一种通用的 PCIe 设备虚拟化方案，甚至也可以用于 PCIe 之外的设备。它的基本思路是：

- 敏感资源如配置空间，是虚拟的
- 关键资源如 MMIO（CSR 部分），是虚拟的，以便 trap-and-emulate
- 性能关键资源如 MMIO（GPU 显存、NVMe CMB 等），硬件 partition 后直接赋给 VM
- Host 上必须存在一个 Virtualization-Aware 的驱动程序，以负责模拟和调度，它实际上是 vGPU 的 device-model

这样，VM 中就能看到一个「看似」完整的 GPU PCIe 设备，它也可以 attach 原生的 GPU 驱动。以渲染为例，vGPU 的基本工作流程是:

1. VM 中的 GPU 驱动，准备好一块内存，保存的是渲染 workload
2. VM 中的 GPU 驱动，把这块内存的物理地址(GPA)，写入到 MMIO CSR 中
3. Host/Hypervisor/驱动: 捕捉到这次的 MMIO CSR 写操作，拿到了 GPA
4. Host/Hypervisor/驱动: 把 GPA 转换成 HPA，并 pin 住相应的内存页
5. Host/Hypervisor/驱动: 把 HPA（而不是 GPA），写入到 pGPU 的真实的 MMIO CSR 中
6. pGPU 工作，完成这个渲染 workload，并发送中断给驱动
7. 驱动找到该中断对应哪个 workload —— 当初我是为哪个 vGPU 提交的这个 workload？—— 并注入一个虚拟的中断到相应的 VM 中
8. VM 中的 GPU 驱动，收到中断，知道该 workload 已完成、结果在内存中

这就是 nVidia GRID vGPU、Intel GVT-g（KVMGT、XenGT）的基本实现思路。一般认为 graphics stack 是 OS 中最复杂的，加上虚拟化之后复杂度更是暴增，任何地方出现一个编程错误，调试起来都是无比痛苦。但只要稳定下来，这种 MPT 方案，就能兼顾 1:N 灵活性、高性能、渲染计算媒体的功能完整性…是不是很完美？

其实也不是。

**该方案最大的缺陷，是必须有一个 pGPU 驱动，负责 vGPU 的模拟和调度工作。**逻辑上它相当于一个实现在内核态的 device-model。而且，由于 GPU 硬件通常并不公开其 PRM，所以事实上就只有 GPU 厂商才有能力提供这样的 Virtualization-Aware pGPU 驱动。使用了厂商提供的 MPT 方案，事实上就形成了对厂商的依赖。

##### **2.3.6 SR-IOV: revisited**

重新回到 GPU 的 SR-IOV。AMD 从 S7150 开始、英伟达从 Turing 架构开始，数据中心 GPU 都支持了 SR-IOV。但是 again，它不是 NIC 那样的 SR-IOV，它需要 Host 上存在一个 vGPU 的 device-model，来模拟从 VM 来的 VF 访问。

所以事实上，到目前为止，GPU 的 SR-IOV 仅仅是封装了 PCIe TLP 层的 VF 路由标识、从而规避了 runtime 时的软件 DMA 翻译，除此之外，和基于 MDEV 的 MPT 方案并无本质的不同。

##### **2.3.7 谱系表**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171635358324389.png)

在介绍完了上述的这些方案后，我们重新看下 CUDA 计算、OpenGL 渲染两种场景的软件栈，看看能发现什么:

CUDA 计算 stack：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171635437716340.png)

OpenGL 渲染 Stack：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171635544557737.png)

可以看出，**从 API 库开始，直到 GPU 硬件，Stack 中的每一个阶段，都有被截获、转发的可能性**。甚至，一概称之为「API 转发」是不合适的 —— 以 GRID vGPU、GVT-g 为例的 DEV 转发，事实上就是 MPT，和任何 API 都没有关系。

### 3.容器 GPU 虚拟化**

首先，我们这里谈到的，都是 nVidia 生产的 GPU、都只考虑 CUDA 计算场景。其次，这里的虚拟化指的是 OS 虚拟化的容器技术，不适用于 KATA 这样的、基于系统虚拟化的安全容器。

#### 3.1 CUDA 的生态

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171636056710441.png)

CUDA 开发者使用的，通常是 CUDA Runtime API，它是 high-level 的；而 CUDA Driver API 则是 low-level 的，它对程序和 GPU 硬件有更精细的控制。Runtime API 是对 Driver API 的封装。

CUDA Driver 即是 UMD，它直接和 KMD 打交道。两者都属于 NVIDIA Driver package，它们之间的 ABI，是 NVIDIA Driver package 内部的，不对外公开。

英伟达软件生态封闭：

- 无论是 nvidia.ko，还是 libcuda.so，还是 libcudart，都是被剥离了符号表的
- 大多数函数名是加密替换了的
- 其它的反调试、反逆向手段

以 nvidia.ko 为例，为了兼容不同版本的 Linux 内核 API，它提供了相当丰富的兼容层，于是也就开源了部分代码:

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171636164044672.png)

其中这个 26M 大小的、被剥离了符号表的 nv-kernel.o_binary，就是 GPU 驱动的核心代码，所有的 GPU 硬件细节都藏在其中。

#### 3.2 vCUDA 和友商 cGPU

为了让多个容器可以共享同一个 GPU，为了限定每个容器能使用的 GPU 份额，业界出现了不同的方案，典型的如 vCUDA 和 cGPU：

vCUDA 架构：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171636278647536.png)

cGPU 架构：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171636371046407.png)

两者的实现策略不同，cGPU 比 vCUDA 更底层，从而实现了不侵入用户环境。

#### 3.3 GPU 池化简介

从截获的位置，看 GPU 池化的谱系：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171636469983200.png)

以 CUDA API 转发的池化方案、业界某产品为例，它到了 GPU 所在的后端机器上，由于一个 GPU 卡可能运行多个 GPU 任务，这些任务之间，依然需要有算力隔离。它为了实现这一点，在后端默认启用了 nVidia MPS —— 也就是故障隔离最差的方案。这会导致什么？**一个 VM 里的 CUDA 程序越界访问了显存，一堆风马牛不相及的 VM 里的 CUDA 应用就会被杀死**。

所以，很显然，GPU 池化也必须以同时满足故障隔离和算力隔离的方案作为基础。

#### 3.4 算力隔离的本质

从上述介绍中，我们可以看出：算力隔离、故障隔离都是 GPU 虚拟化、GPU 池化的关键，缺一不可。如果没有算力隔离，不管虚拟化损耗有多低，都会导致其方案价值变低；而如果缺少实例间的故障隔离，则基本无法在生产环境使用了。

事实上，英伟达 GPU 提供了丰富的硬件特性，支持 Hardware Partition，支持 Time Sharing。

**1. Hardware Partition，亦即: 空分**

Ampere 架构的 A100 GPU 所支持的 MIG，即是一种 Hardware Partition。硬件资源隔离、故障隔离都是硬件实现的 —— 这是无可争议的隔离性最好的方案。它的问题是不灵活: 只有高端 GPU 支持；只支持 CUDA 计算；A100 只支持 7 个 MIG 实例。

**2. nVidia MPS**

除了 MIG，算力隔离表现最优秀的，是 MPS —— 它通过将多个进程的 CUDA Context，合并到一个 CUDA Context 中，省去了 Context Switch 的开销，也在 Context 内部实现了算力隔离。如前所述，MPS 的致命缺陷，是把许多进程的 CUDA Context 合并成一个，从而导致了额外的故障传播。所以尽管它的算力隔离效果极好，但长期以来工业界使用不多，多租户场景尤其如此。

**3. Time Sharing，亦即: 时分**

nVidia GPU 支持基于 Engine 的 Context Switch。不管是哪一代的 GPU，其 Engine 都是支持多任务调度的。一个 OS 中同时运行多个 CUDA 任务，这些任务就是在以 Time Sharing 的方式共享 GPU。

鉴于 MIG 的高成本和不灵活、MPS 故障隔离方面的致命缺陷，事实上就只剩下一种可能：Time Sharing。唯一的问题是，如何在原厂不支持的情况下，利用 Time Sharing 支持好算力隔离、以保证 QoS。这也是学术界、工业界面临的最大难题。

##### **3.4.1 GPU microarchitecture 和 chip**

真正决定 GPU 硬件以何种方式工作的，是 chip 型号。不管是 GRID Driver 还是 Tesla Driver，要指挥 GPU 硬件工作，就要首先判断 GPU 属于哪种 chip，从而决定用什么样的软硬件接口来驱动它。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171636589030641.png)

##### **3.4.2 PFIFO: GPU Scheduling Internals**

PFIFO 架构：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171637066759023.png)

概念解释：

> **PFIFO**

GPU 的调度硬件，整体上叫 PFIFO。

> **Engine**

执行某种类型的 GPU 硬件单元。常见 Engine 有：

- PGRAPH —— CUDA/Graphics
- PCOPY ——— Copy Engine
- PVENC ——— Video Encoding
- PVDEC ——— Video Decoding
- …

最重要的就是 PGRAPH Engine，它是 CUDA 和渲染的硬件执行单元。

> **Channel**

GPU 暴露给软件的，对 Engine 的抽象。一个 app 可以对应一或多个 channels，执行时由 GPU 硬件把一个一个的 channel，放在一个一个的 engine 上执行。

channel 是软件层的让 GPU 去执行的最小调度单位。

> **TSG**

Timeslice Group。

由一或多个 channel(s)组成。一个 TSG 共享一个 context，且作为一个调度单位被 GPU 执行。

> **runlist**

GPU 调度的最大单位。调度时，GPU 通常是从当前 runlist 的头部摘取 TSG 或 channel 来运行。因此，切换 runlist 也意味着切换 active TSG/channel。

> **PBDMA**

pushbuffer DMA。GPU 上的硬件，用于从 Memory 中获取 pushbuffer。

> **Host**

GPU 上和 SYSMEM 打交道的部分(通过 PCIe 系统)。PBDMA 是 Host 的一部分。注意，Host 是 Engine 和 SYSMEM 之间的唯一桥梁。

> **Instance Block**

每个 Channel 对应一个 Instance Block，它包含各个 Engine 的状态，用于 Context Switch 时的 Save/Restore；包含 GMMU pagetable；包含 RAMFC —— 其中包括 UMD 控制的 USERD。

##### **3.4.3 runlist/TSG/channel 的关系**

1. Tesla 驱动为每个 GPU，维护一或多个 runlist，runlist 或位于 GPU 显存，或位于系统内存
2. runlist 中有很多的 entry，每个 entry 是一个 TSG 或一个 channel

- 一个 TSG 是 multi-channel 或 single-channel 的
- 一个 channel 必定隶属于某个 TSG

1. 硬件执行 TSG 或 channel，当遇到以下情景之一时，进行 Context Switch：

- 执行完毕
- timeslice 到了
- 发生了 preemption

##### **3.4.4 pending channel notification**

pending channel notification 是 USERD 提供的机制。UMD 可以利用它通知 GPU：某个 channel 有了新的任务了【1】。这样，GPU 硬件在当前 channel 被切换后(执行完毕、或 timeslice 到了)，就会执行相应的 channel。

**注释**

【1】 不同 chip，实现有所不同。

##### **3.4.5 从硬件调度看 GRID vGPU**

GRID vGPU 支持 3 种 scheduler：

**1. Best Effort: 所有 vGPU 的任务随意提交，GPU 尽力执行。**

**现象：** 如果启动了 N 个 vGPU，它们的负载足够高，那么结果就是均分算力。

**原理：** 所有的 vGPU 使用同一个 runlist。runlist 内，还是按照 channel 为粒度进行调度。如同在 native 机器上运行多个 CUDA 任务一样。

**2. Equal Share: 所有在用的 vGPU 严格拥有同样的 GPU 配额**

**现象：** 如果启动了 N 个 vGPU，它们严格拥有相同的算力，不管是否需要这么多。

**原理：** 为每个 vGPU 维护一个 runlist。当它的 timeslice 过了，GRID Host Driver 会写 GPU 寄存器，触发当前 runlist 被抢占、下一个 runlist 被调度。

**3. Fixed Share: 每个 vGPU 有自己固定的 GPU 配额**

**现象：** 每个 vGPU 严格按照创建时的规格来分配算力。

**原理：** Ditto.

#### 3.5 腾讯云 qGPU 简介

qGPU == QoS GPU。它是目前业界唯一真正实现了故障隔离、显存隔离、算力隔离、且不入侵生态的容器 GPU 共享的技术。

##### **3.5.1 qGPU 基本架构**

qGPU 基本架构：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171637212944225.png)

##### **3.5.2 qGPU QoS 效果**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171637294820103.png)

**注释**

【1】 测试数据来自 T4(chip: TU104)。其它 chip 上，正确性、功能性和性能都待验证，虽然原理上是相通的。

【2】两个 PoD 的算力配比为 2：1。横坐标为 batch 值，纵坐标为运行时两个 PoD 的实际算力比例。可以看到，batch 较小时，负载较小，无法反映算力配比；随着 batch 增大，qGPU 和 MPS 都趋近理论值 2，vCUDA 也偏离不远，但缺乏算力隔离的业界某产品则逐渐趋近 1。

### **4.GPU 虚拟化: 未来展望**

2021 以来，GPU 工业界发生了一些变化，e.g.：

**1. 英伟达 GPU 的 QoS 突破**

英伟达在 CUDA 计算领域占据压倒性的优势，但 QoS 表现不尽如人意。

长期以来，学术界和工业界付出了大量的努力，尝试在英伟达不支持 QoS 的前提下，实现某种程度的算力隔离。遗憾的是，这些努力，要么集中在 CUDA API 层，能够做到一定的算力隔离，但同样会带来副作用；要么尝试在 low-level 层面突破 —— 但不幸全都失败了。

**qGPU 是十几年来在英伟达 GPU 上实现 QoS 的最大突破**。基于它：

- 腾讯云 TKE 的多容器共享 GPU，将无悬念地领先整个工业界
- 在线推理 + 离线训练的混布，成为可能
- GPU 池化的后端实现，不管采用哪种方案，都有了坚实的基础
- Linux/Android 场景的渲染虚拟化，也有了坚实的基础

**2. Vulkan Spec 支持了 video encode/decode**

很可能，编解码 API 不统一的乱象即将终结，这对 API 转发方案有很大的意义。不远的将来，或许某种 API 方案的 vGPU 会成为主流。Google 在社区的一些活动标明，很可能它就有这样的计划。

### **5.参考资料和项目简介**

**1. nVidia MPS**

官方。部分文档公开。

**2. nouveau driver in Linux Kernel**

开源社区版的英伟达 GPU 驱动，基于 DRM，硬件细节基本靠逆向工程。不支持 CUDA、只支持 OpenGL 渲染。代码庞大，包含很多有用信息。

**3. envytools 及其 nVidia Hardware Documentation**

nouveau 的配套项目。除了提供各种 profiling GPU 硬件细节的工具，还维护了一个文档仓库，记录所有已经被成功逆向了的信息。

**4. GDEV project, an opensource implementation of CUDA**

基于 nouveau 实现了 CUDA driver 和 CUDA runtime，代码较旧，已不维护。大神级作品。

**5. libcudest, a partially RE-ed CUDA driver**

英伟达实习生实现的 CUDA Driver 逆向工程。只逆向了一小部分 UMD 和 KMD 之间的接口。已不维护。

**6. vCUDA**

开源项目。

**7. nVidia official: nvidia-uvm driver for Tesla**

官方，开源。Telsa Driver 配套的 UVM 驱动，代码开源。和 Tesla Driver 有很多 low-level 交互，可以从中窥见很多 GPU 硬件细节。

**8. Tesla Driver**

官方。细节全藏在 nv-kernel.o_binary 文件中。

**9. GRID vGPU**

官方。细节也是全在 nv-kernel.o_binary 文件中。与 Tesla Driver 不同的是，它为 vGPU 的 Fixed Share 和 Equal Share 两种调度策略，实现了 per-vGPU runlist。因此有很高的参考价值。

### **6.思考 & 致谢**

> “Lasciate ogne speranza, voi ch’intrate.” – Dante Alighieri
>
> “Καιρὸν γνῶθι.” – Πιττακός ο Μυτιληναίος

使用 nVidia GPU 进行计算，有两种场景：1) 推理业务，往往是在线业务；2) 训练业务，往往是离线业务。这两种业务之间，很难混布到同一个 GPU 上。甚至两种在线推理业务之间，也很难进行这样的混布。因为没有 QoS 隔离，你不知道哪一个业务会流量突发，影响另一个业务。所以，长期以来，关键的在线业务，GPU 利用率都不高，据我们了解，大多在 50%以下，甚至个别 BG 的推理业务只能到～ 20%。即使 GPU 很昂贵，即使一个业务占不满 GPU，也只能如此。

我们很自然要问：是 nVidia 做不好 QoS 吗？显然不是。MPS 也好，GRID vGPU 也好，其 QoS 表现都很优秀。但是，为什么 MPS 会画蛇添足地引入 CUDA Context Merging 呢？真的是因为这样会带来些许性能上的收益吗？我是持怀疑态度的。在我看来，更可信的解释是，英伟达公司在拥有市场支配地位的情况下，并不希望提升 GPU 利用率。卓越的硬件加上封闭的软件生态，当然能带来丰厚的利润。

学术界、工业界在 CUDA 算力隔离上的努力，这里不再一一列举【1】。这其中既有 GDEV 这样的以一人之力做出的大神级作品，也有毫无营养的灌水式 paper。有意思的是，几乎所有的努力都在上层，很少人有勇气下潜到 GPU 硬件的细节中。我们下潜了，也很幸运地成功了。

- 感谢腾讯云虚拟化团队的各位同事，一起加班到深夜，分析搜罗到的各种靠谱和不靠谱的项目和 paper，脑补各种可能的软硬件细节，讨论技术上的各种可能性；
- 感谢腾讯云 TKE 团队的各位同事，协调客户收集需求、协同产品化开发；
- 感谢 WXG 的同事，和我们一起梳理 GPU 利用率的痛点；
- 感谢友商的同类产品，它的 idea 无疑是优秀的；

**注释**

【1】 部分列表可参考阎姝含的文章: [针对深度学习的 GPU 共享](https://zhuanlan.zhihu.com/p/285994980)

原文作者：jikesong，腾讯 CSIG 腾讯云异构计算研发副总监

原文链接：https://mp.weixin.qq.com/s/3VjGpyXZSkJhy6sFPUsZzw

# 【NO.122】一文入门 Kafka

温故而知新，反复学习优秀的框架，定有所获。因为工作原因，需要用到 Kafka 的特殊场景，周末再次阅读了 kafka 的资料，收获不少。

kafka 由 LinkedIn 公司推出的一个高吞吐的分布式消息系统，通俗的说就是一个基于发布和订阅的消息队列，官网地址：

https://kafka.apache.org/intro

### **1.应用场景**

- 异步解构：在上下游没有强依赖的业务关系或针对单次请求不需要立刻处理的业务；
- 系统缓冲：有利于解决服务系统的吞吐量不一致的情况，尤其对处理速度较慢的服务来说起到缓冲作用；
- 消峰作用：对于短时间偶现的极端流量，对后端的服务可以启动保护作用；
- 数据流处理：集成 spark 做实事数据流处理。

### 2.**Kafka 拓扑图（多副本机制）**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171648445247353.png)
由上图我们可以发现 Kafka 是分布式，同时对于每一个分区都存在多副本，同时整个集群的管理都通过 zookeeper 管理。

### 3.**Kafka 核心组件**

#### 3.1 broker

Kafka 服务器，负责消息存储和转发；一 broker 就代表一个 kafka 节点。**一个 broker 可以包含多个 topic**

#### 3.2 topic

消息类别，Kafka 按照 topic 来分类消息

#### 3.3 partition

- topic 的分区，一个 topic 可以包含多个 partition，topic 消息保存在各个 partition 上；由于一个 topic 能被分到多个分区上，给 kafka 提供给了并行的处理能力，这也正是 kafka 高吞吐的原因之一。
- partition 物理上由多个 segment 文件组成，每个 segment 大小相等，**顺序读写（这也是 kafka 比较快的原因之一，不需要随机写）**。每个 Segment 数据文件以该段中最小的 offset ，文件扩展名为.log。当查找 offset 的 Message 的时候，通过二分查找快找到 Message 所处于的 Segment 中。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171648582128534.png)

#### 3.4 offset

- 消息在日志中的位置，可以理解是消息在 partition 上的偏移量，也是代表该消息的**唯一序号**。
- 同时也是主从之间的需要同步的信息。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171649072331678.png)

#### 3.5 Producer

生产者，负责向 Kafka Broker 发消息的客户端

#### 3.6 Consumer

消息消者，负责消费 Kafka Broker 中的消息

#### 3.7 Consumer Group

消费者组，每个 Consumer 必须属于一个 group；（**注意的是 一个分区只能由组内一个消费者消费，消费者组之间互不影响。**）

#### 3.8 Zookeeper

管理 kafka 集群，负责存储了集群 broker、topic、partition 等 meta 数据存储，同时也负责 broker 故障发现，partition leader 选举，负载均衡等功能。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171649156807965.png)

### 4.**服务治理**

既然 Kafka 是分布式的发布/订阅系统，这样如果做的集群之间数据同步和一致性，kafka 是不是肯定不会丢消息呢？以及宕机的时候如果进行 Leader 选举呢？

#### 4.1 数据同步

在 Kafka 中的 Partition 有一个 leader 与多个 follower，producer 往某个 Partition 中写入数据是，只会往 leader 中写入数据，然后数据才会被复制进其他的 Replica 中。而每一个 follower 可以理解成一个消费者，定期去 leader 去拉去消息。而只有数据同步了后，kafka 才会给生产者返回一个 ACK 告知消息已经存储落地了。

##### **4.1.1 ISR**

在 Kafka 中，为了保证性能，Kafka 不会采用强一致性的方式来同步主从的数据。而是维护了一个：in-sync Replica 的列表，Leader 不需要等待所有 Follower 都完成同步，只要在 ISR 中的 Follower 完成数据同步就可以发送 ack 给生产者即可认为消息同步完成。同时如果发现 ISR 里面某一个 follower 落后太多的话，就会把它剔除。

具体流程如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171649282813272.png)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171649371612276.png)
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171649459622059.png)
**上述的做法并无法保证 kafka 一定不丢消息。** 虽然 Kafka 通过多副本机制中最大限度保证消息不会丢失，但是如果数据已经写入系统 page cache 中但是还没来得及刷入磁盘，此时突然机器宕机或者掉电，那消息自然而然的就会丢失。

##### 4.1.2 **Kafka 故障恢复**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171649539330755.png)

Kafka 通过 Zookeeper 连坐集群的管理，所以这里的选举机制采用的是 Zab(zookeeper 使用)。

- 生产者发生消息给 leader，这个时候 leader 完成数据存储，突然发生故障，没有给 producer 返回 ack；
- 通过 ZK 选举，其中一个 follower 成为 leader，这个时候 producer 重新请求新的 leader，并存储数据。

### 5.**Kafka 为什么这么快**

#### 5.1 顺序写磁盘

Kafka 采用了顺序写磁盘，而由于顺序写磁盘相对随机写，减少了寻地址的耗费时间。（在 Kafka 的每一个分区里面消息是有序的。

#### 5.2 Page Cache

Kafka 在 OS 系统方面使用了 Page Cache 而不是我们平常所用的 Buffer。Page Cache 其实不陌生，也不是什么新鲜事物。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171650036810572.png)

我们在linux上查看内存的时候，经常可以看到buff/cache，两者都是用来加速IO读写用的，而cache是作用于读，也就是说，磁盘的内容可以读到cache里面这样，应用程序读磁盘就非常快；而buff是作用于写，我们开发写磁盘都是，一般如果写入一个buff里面再flush就非常快。而kafka正是把这两者发挥了极致：Kafka虽然是scala写的，但是依旧在Java的虚拟机上运行，尽管如此，kafka它还是尽量避开了JVM的限制，它利用了Page cache来存储，这样躲开了数据在JVM因为GC而发生的STW。另一方面也是Page Cache使得它实现了零拷贝，具体下面会讲。

#### 5.3 零拷贝

无论是优秀的 Netty 还是其他优秀的 Java 框架，基本都在零拷贝减少了 CPU 的上下文切换和磁盘的 IO。当然 Kafka 也不例外。零拷贝的概念具体这里不作太详细的复述，大致的给大家讲一下这个概念。

##### 5.3.1 **传统的一次应用程请求数据的过程**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171650156340532.png)

这里大致可以发传统的方式发生了 4 次拷贝，2 次 DMA 和 2 次 CPU，而 CPU 发生了 4 次的切换。_（DMA 简单理解就是，在进行 I/O 设备和内存的数据传输的时候，数据搬运的工作全部交给 DMA 控制器，而 CPU 不再参与任何与数据搬运相关的事情）。

##### 5.3.2 零拷贝的方式

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171650241724176.png)

通过优化我们可以发现，CPU 只发生了 2 次的上下文切换和 3 次数据拷贝。（linux 系统提供了系统事故调用函数“ sendfile()”，这样系统调用，可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态）。

##### **5.3.3 分区分段**

我们上面也介绍过了，kafka 采取了分区的模式，而每一个分区又对应到一个物理分段，而查找的时候可以根据二分查找快速定位。这样不仅提供了数据读的查询效率，也提供了并行操作的方式。

##### 5.3.4 **数据压缩**

Kafka 对数据提供了：Gzip 和 Snappy 压缩协议等压缩协议，对消息结构体进行了压缩，一方面减少了带宽，也减少了数据传输的消耗。

### **6.Kafka 安装**

#### 6.1 安装 JDK

由于使用压缩包还需要自己配置环境变量，所以这里推荐直接用 yum 安装，熟悉查看目前 Java 的版本：

```
yum -y list Java*
```

安装你想要的版本，这里我是 1.8

```
yum install java-1.8.0-openjdk-devel.x86_64
```

查看是否安装成功

```
Java -version
```

#### 6.2 安装 Zookeeper

首先需要去官网下载安装包，然后解压

```
tar -zxvf zookeeper-3.4.9.tar.gz
```

**要做的就是将这个文件复制一份，并命名为：zoo.cfg，然后在 zoo.cfg 中修改自己的配置即可**

```
cp zoo_sample.cfg zoo.cfgvim zoo.cfg
```

主要配置解释如下

```
# zookeeper内部的基本单位，单位是毫秒，这个表示一个tickTime为2000毫秒，在zookeeper的其他配置中，都是基于tickTime来做换算的tickTime=2000# 集群中的follower服务器(F)与leader服务器(L)之间 初始连接 时能容忍的最多心跳数（tickTime的数量）。initLimit=10#syncLimit：集群中的follower服务器(F)与leader服务器(L)之间 请求和应答 之间能容忍的最多心跳数（tickTime的数量）syncLimit=5# 数据存放文件夹，zookeeper运行过程中有两个数据需要存储，一个是快照数据（持久化数据）另一个是事务日志dataDir=/tmp/zookeeper## 客户端访问端口clientPort=2181
```

配置环境变量

```
vim ~/.bash_profileexport ZK=/usr/local/src/apache-zookeeper-3.7.0-binexport PATH=$PATH:$ZK/binexport PATH// 启动zkServer.sh start
```

下面能看启动成功

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171650384057267.png)

#### 6.3 安装 Kafka

下载 kafka

https://www.apache.org/dyn/closer.cgi?path=/kafka/2.8.0/kafka-2.8.0-src.tgz

安装 kafka

```
 tar -xzvf kafka_2.12-2.0.0.tgz
```

配置环境变量

```
 export ZK=/usr/local/src/apache-zookeeper-3.7.0-bin export PATH=$PATH:$ZK/bin export KAFKA=/usr/local/src/kafka export PATH=$PATH:$KAFKA/bin
```

启动 Kafka

```
 nohup kafka-server-start.sh 自己的配置文件路径/server.properties &
```

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171650491178332.png)

大功告成！

**参考资料**

《深入理解 Kafka：核心设计实践原理》

原文作者：ninetyhe，腾讯 CDG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/vzvmOXGcsX7rwY4J_--onw

# 【NO.123】浏览器性能优化实战

当我们在做性能优化的时候，我们究竟在优化什么？浏览器底层是一个什么架构？浏览器渲染的本质究竟是什么？哪些方面对用户的体验影响才是最大的？有没有业内一些通用的标准或标杆参考？都 1202 年了，雅虎军规还有没有用？性能分析工具都有哪些？我们怎么进行打点分析才是合适的？

本文为你一一讲解这些。了解了这些问题，可能你在做性能优化的时候才能更加得心应手。

### **1. 性能优化的本质**

#### 1.1 展示更快，响应更快

性能优化的目的，就是为了提供给用户更好的体验，这些体验包含这几个方面：展示更快、交互响应快、页面无卡顿情况。

更详细的说，就是指，在用户输入 url 到站点完整把整个页面展示出来的过程中，通过各种优化策略和方法，让页面加载更快；在用户使用过程中，让用户的操作响应更及时，有更好的用户体验。

对于前端工程师来说，要做好性能优化，需要理解浏览器加载和渲染的本质。理解了本质原理，才能更好的去做优化。所以我们先来看看浏览器架构是怎样的。

#### 1.2 理解浏览器多进程架构

从大的方面来说，浏览器是一个多进程架构。

它可以是一个进程包含多个线程，也可以是多个进程中，每个进程有多个线程，线程之间通过 IPC 通讯。每个浏览器有不同的实现细节，并没有标准规定浏览器必须如何去实现。

这里我们只谈论 chrome 架构。

下面这张图是目前 chrome 的多进程架构图。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171654506051927.png)

我们来看看这些进程分别对应浏览器窗口中的哪一部分：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171654592066886.png)

那么，怎么看浏览器对应启动了什么进程呢？

chrome 中，我们可以通过更多->More Tools->Task Manager 看到启动的进程。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171655087997842.png)

从 chrome 官网和源码，我们也可以得知，多进程架构中包含这些进程：

- Browser 进程：打开浏览器后，始终只有一个。该进程有 UI 线程、Network 线程、Storage 线程等。用户输入 url 后，首先是 Browser 进程进行响应和请求服务器获取数据。然后传递给 Renderer 进程。
- Renderer 进程：每一个 tab 一个，负责 html、css、js 执行的整个过程。**前端性能优化也与这个进程有关**。
- Plugin 进程：与浏览器插件相关，例如 flash 等。
- GPU 进程：浏览器共用一个。主要负责把 Renderer 进程中绘制好的 tile 位图作为纹理上传到 GPU，并调用 GPU 相关方法把纹理 draw 到屏幕上。

这里的话只是简单介绍一下浏览器的多进程架构，让大家对浏览器整体架构有个初步认识，其实背后的细节还有很多，这里就不一一展开。有兴趣可以细看[这一系列文章](https://developers.google.com/web/updates/2018/09/inside-browser-part1)和[chrome 官网](https://www.chromium.org/developers/how-tos/getting-around-the-chrome-source-code)介绍。

#### 1.3 理解页面渲染相关进程

##### **1.3.1 Renderer Process & GPU Process**

从以上的多架构，我们了解到，与前端渲染、性能优化相关的，其实主要是 Renderer 进程和 GPU 进程。那么，它们又是什么架构呢？

来看一下这张我们再熟悉不过的图。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171655204746486.png)

- Renderer 进程：包括 3 个线程。合成线程（Compositor Thread）、主线程（Main Thread）、Compositor Tile Worker。
- GPU 进程：只有 GPU 线程，负责接收从 Renderer 进程中的 Compositor Thread 传过来的纹理，显示到屏幕上。

##### **1.3.2 Renderer Process 详解**

Renderer 进程中 3 个线程的作用为：

- Compositor Thread：首先接收 vsync 信号(vsync 信号是指操作系统指示浏览器去绘制新的帧)，任何事件都会先到达 Compositor 线程。如果主线程没有绑定事件，那么 Compositor 线程将避免进入主线程，并尝试将输入转换为屏幕上的移动。它将更新的图层位置信息作为帧通过 GPU 线程传递给 GPU 进行绘制。

**当用户在快速滑动过程中，如果主线程没有绑定事件，Compositor 线程是可以快速响应并绘制的**，这是浏览器做的一个优化。

- Main Thread：**主线程就是我们前端工程师熟知的线程，这里会执行解析 Html、样式计算、布局、绘制、合成等动作。所以关于性能的问题，都发生在了这里。所以应该重点关注这里**。
- Compositor Tile Worker：由合成线程产生一个或多个 worker 来处理光栅化的工作。

Service Workers 和 Web Workers 可以暂时理解也在 Renderer 进程中，这里不展开讨论。

###### **1.3.2.1 Main Thread**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171655307930369.png)

主线程需要重点讲下。因为这是我们的代码真实存在的环境。

从上一小节 Render 进程和 GPU 进程的图中，我们可以看到有个红色的箭头，从 Recal Styles 和 Layout 指向了 requestAnimationFrame，这意味着有 Forced Synchronous Layout (or Styles)(强制回流和重绘)发生，这一点在性能方面特别要注意。

在 Main Thread 中，有这几个需要注意一下：

- requestAnimationFrame：因为布局和样式计算是在 rAF 之后，所以在 rAF 是进行元素变更的理想时机。如果在这里对一个元素变更 100 个类，不会进行 100 次计算，它们会分批以后处理。需要注意的是，不能在 rAF 中查询任何计算样式和布局的属性（例如：el.style.backgroundImage 或 el.style.offsetWidth），因为这样会导致重绘和回流。
- Layout：布局的计算通常是针对整个文档的，并且与 DOM 元素的大小成正比！（这点特别要注意，如果一个页面 DOM 元素太多，也会导致性能问题）

主线程的顺序始终都是：

```
Input Event Handler->requestAnimationFrame->ParseHtml->ReculateStyles->Layout- >Update Layer Tree->Paint->Composite->commit->requestIdleCallback
```

只能从前往后，例如，必须先是 ReculateStyles，然后 Layout、然后 Paint。但是，如果它只需要做最后一步 Paint，那么这就是它全部要做的事情，不会再发生前面的 ReculateStyles 和 Layout。

这里其实给了我们一个启示：**如果要让 fps 保持 60，即每帧的 js 执行时间少于 16.66ms，那么让这个主线程执行的过程尽可能地少，是我们的性能优化目标**。

根据主线程的这些步骤，理想的情况下，我们只希望浏览器只发生最后一个步骤：Composite(合成)。

CSS 的属性是我们需要关注一下的模块。这里有描述了哪些[CSS 属性](https://csstriggers.com/)会引起重绘、回流和合成。例如，让我们给一个元素进行移动位置时：`transform`和`opacity`可以直接触发合成，但是`left`和`top`却会触发 Layout、Paint、Composite3 个动作。所以显然用 transform 时更好的方案。

**但这并不是说我们不应该用 left 和 top 这些可能引起重绘回流的属性，而是应该关注每个属性在浏览器性能中引起的效果**。

### **2. 看看经典：雅虎军规**

多年前雅虎的 Nicolas C. Zakas 提出 7 个类别 35 条军规，至今为止很多前端优化准则都是围绕着这个展开。如果严格按照这些规则去做，其实我们有很多优化工作可以做，只要认真践行，性能提升不是问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171655417084264.png)

我们来看看它 7 个分类都是围绕哪些方面展开：

- Server：与页面发起请求的相关；
- Cookie：与页面发起请求相关；
- Mobile：与页面请求相关；
- Content：与页面渲染相关；
- Image：与页面渲染相关；
- CSS：与页面渲染相关；
- Javascript：与页面渲染和交互相关。

从上面的描述可以看到，其实雅虎军规，是围绕页面发起请求那一刻，到页面渲染完成，页面开始交互这几个方面来展开，提出的一些原则。

很多原则大家也都耳熟能详，就不全部展开了，有兴趣的同学可以去查看[原文](https://developer.yahoo.com/performance/rules.html)。这里主要想提一些忽略但是又值得注意的点：

**减少 DOM 节点数量**

为什么要减少 DOM 节点的数量？

当遍历查询 500 和 5000 个 DOM 节点，进行事件绑定时，会有所差别。

当一个页面 DOM 节点过多，应该考虑使用无限滚动方案来使视窗节点可控。可以看看[google 提的方案](https://developers.google.com/web/updates/2016/07/infinite-scroller)。

**减少 cookie 大小**

cookie 传输会造成带宽浪费，影响响应时间，可以这样做：

消除不必要的 cookies；

静态资源不需要 cookie，可以采用其他的域名，不会主动带上 cookie。

**避免图片 src 为空**

图片 src 为空时，不同浏览器会有不同的副作用，会重新发起一起请求。

### **3. 性能指标**

#### 3.1 什么样的性能指标才能真正代表用户体验？

要衡量性能，我们必须有一些客观的、可衡量的指标来进行监控。**但是客观且定量可衡量的指标不一定能反映用户的真实体验**。

以前，我们会用 load 事件的触发来衡量一个页面是否加载或显示完成。但是设想会不会有这样的情况：一个页面的 load 事件已经被触发，但是却在 load 事件之后几秒才开始加载内容和渲染页面，所以这个时候，load 事件并不能真实反映用户看到内容的时刻。

在过去几年，google 团队和[W3C 性能工作组](https://www.w3.org/webperf/)致力于提供标准的性能 API 来真正衡量用户的体验。主要是从这 4 个方面思考：

| 思考点            | 详细内容                                         |
| :---------------- | :----------------------------------------------- |
| Is it happening?  | 导航是否成功，服务器是否响应了                   |
| Is it useful?     | 是否已经渲染了足够的内容，让用户可以开始参与其中 |
| Is it usable?     | 用户是否可以与页面交互，页面是否处于繁忙状态     |
| Is it delightful? | 交互是否流畅、自然、没有滞后反映或卡顿           |

通常有 2 种途径来衡量性能。

1. 本地实验衡量：本地模拟用户的网络、设备等情况进行测试。通常在开发新功能的时候，实验测量是很重要的，因为我们不知道这个功能发布到线上会有什么性能问题，所以提前进行性能测试，可以进行预防。
2. 线上衡量：实验测量固然可以反映一些问题，但无法反映在用户那里真实的情况。同样的，在用户那里，性能问题会和用户的设备、网络情况有关，而且还跟用户如何与页面进行交互有关。

有这几个类型与用户感知性能相关。

- 页面加载时间：页面以多快的速度加载和渲染元素到页面上。
- 加载后响应时间：页面加载和执行 js 代码后多久能响应用户交互。
- 运行时响应：页面加载完成后，对用户的交互响应时间。
- 视觉稳定性：页面元素是否会以用户不期望的方式移动，并干扰用户的交互。
- 流畅度：过渡和动画是否以一致的帧率渲染，并从一种状态流畅地过渡到另一种状态。

对应上面几种分类，Google 和 W3C 性能工作组提供了对应这几种性能指标：

- **[First contentful paint (FCP)](https://web.dev/fcp/):** 测量页面开始加载到某一块内容显示在页面上的时间。
- **[Largest contentful paint (LCP)](https://web.dev/lcp/):** 测量页面开始加载到最大文本块内容或图片显示在页面中的时间。
- **[First input delay (FID)](https://web.dev/fid/):** 测量用户首次与网站进行交互(例如点击一个链接、按钮、js 自定义控件)到浏览器真正进行响应的时间。
- **[Time to Interactive (TTI)](https://web.dev/tti/):** 测量从页面加载到可视化呈现、页面初始化脚本已经加载，并且可以可靠地快速响应用户的时间。
- **[Total blocking time (TBT)](https://web.dev/tbt/):** 测量从 FCP 到 TTI 之间的时间，这个时间内主线程被阻塞无法响应用户输入。
- **[Cumulative layout shift (CLS)](https://web.dev/cls/):** 测量从页面开始加载到状态变为隐藏过程中，发生不可预期的 layout shifts 的累积分数。

这些指标能从一定程度上衡量页面性能，但不一定都是有效的。举个例子。LCP 指标主要用户衡量页面的主要内容是否完成加载，但会有这样的情况，最大的元素并不是主要内容，那么这个时候 LCP 指标并不是那么重要。

每个不同的站点有自己的特殊性，可以参考以上角度进行衡量，也需要因地制宜。

#### 3.2 Core Web Vitals

在以上列出的指标中，Google 定义了 3 个最核心的指标，作为 Core Web Vitals。它们分别代表着：加载、交互、视觉稳定性。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171656409012206.png)

- **[Largest Contentful Paint (LCP)](https://web.dev/lcp/)**: 测量加载性能。为了能提供较好的用户体验，LCP 指标建议页面首次加载要在 2.5s 内完成。
- **[First Input Delay (FID)](https://web.dev/fid/)**: 测量交互性能。为了提供较好用户体验，交互时间建议在 100ms 或以内。
- **[Cumulative Layout Shift (CLS)](https://web.dev/cls/)**: 测量视觉稳定性。为了提供较好用户体验，页面应该维持 CLS 在 0.1 或以内。

当页面访问量有 75%的数据达到了以上以上 Good 的标准，则认为性能是不错的了。

Core Web Vitals 是作为核心性能指标，但是其他指标也同样在重要，是做为核心指标的一个辅助。例如，TTFB 和 FCP 都可以用来衡量加载性能(服务器响应时间和渲染时间)，它们作为 LCP 的一个问题手段辅助。同样的，TBT 和 TTI 对于衡量交互性能也很重要，是 FID 的一个辅助，但是它们无法在线上进行测量，也无法反映以用户为中心的结果。

Google 官方提供了一个[web-vitals](https://github.com/GoogleChrome/web-vitals)库，线上或本地都可以测量上面提到的 3 个指标：

```
import {getCLS, getFID, getLCP} from 'web-vitals';function sendToAnalytics(metric) {  const body = JSON.stringify(metric);  // Use `navigator.sendBeacon()` if available, falling back to `fetch()`.  (navigator.sendBeacon && navigator.sendBeacon('/analytics', body)) ||      fetch('/analytics', {body, method: 'POST', keepalive: true});}getCLS(sendToAnalytics);getFID(sendToAnalytics);getLCP(sendToAnalytics);
```

下面，分别讲讲这 3 个指标定义的原因、如何测量、如何优化。

##### **3.2.1 Largest Contentful Paint (LCP)**

###### **3.2.1.1 LCP 如何定义**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171656528808285.png)

LCP 是指页面开始加载到最大文本块内容或图片显示在页面中的时间。那么哪些元素可以被定义为最大元素呢？

- `<img>`标签
- `<image>` 在 svg 中的 image 标签
- `<video>` video 标签
- CSS background url()加载的图片
- 包含内联或文本的块级元素

###### **3.2.1.2 如何测量 LCP**

**线上测量工具**

- [Chrome User Experience Report](https://developers.google.com/web/tools/chrome-user-experience-report)
- [PageSpeed Insights](https://developers.google.com/speed/pagespeed/insights/)
- [Search Console (Core Web Vitals report)](https://support.google.com/webmasters/answer/9205520)
- [`web-vitals` JavaScript library](https://github.com/GoogleChrome/web-vitals)

**实验室工具**

- [Chrome DevTools](https://developers.google.com/web/tools/chrome-devtools/)
- [Lighthouse](https://developers.google.com/web/tools/lighthouse/)
- [WebPageTest](https://webpagetest.org/)

**原生的 JS API 测量**

LCP 还可以用 JS API 进行测量，主要使用 PerformanceObserver 接口，目前除了 IE 不支持，其他浏览器基本都支持了。

```
new PerformanceObserver((entryList) => {  for (const entry of entryList.getEntries()) {    console.log('LCP candidate:', entry.startTime, entry);  }}).observe({type: 'largest-contentful-paint', buffered: true});
```

我们看一下结果是怎样的：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171657055420242.png)

**Google 官方 web-vitals 库**

Google 官方也提供了一个[web-vitals](https://github.com/GoogleChrome/web-vitals)库，底层还是使用这个 API，只是帮我们处理了一些需要测量和不需测量的场景、以及一些细节问题。

###### **3.2.1.3 如何优化 LCP**

LCP 可能被这四个因素影响：

- 服务端响应时间
- Javascript 和 CSS 引起的渲染卡顿
- 资源加载时间
- 客户端渲染

更加详细的优化建议就不展开了，可以参考[这里](https://web.dev/optimize-lcp/)。

##### **3.2.2 First Input Delay (FID)**

###### **3.2.2.1 FID 如何定义**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171657167766061.png)

我们都知道第一印象的重要性，比如初次遇到某人形成的印象，会在后续交往中起重要的影响。对于一个网站也是如此。

网站以多快的速度加载完成是其中一项指标，加载后以多快的速度对用户进行响应也同样重要。FID 就是指后者。

可以通过下面的图来更详细了解 FID 处于哪个位置：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171657278557042.png)

从上图可以看出，当主线程处于繁忙的时候，FID 是指从浏览器接收到了用户输入，到浏览器对用户的输入进行响应的延迟时间。

通常，**当我们在写代码的时候，会认为只要用户输入信息，我们的事件回调就会立刻响应，但实际上并不是这样。这是主线程可能处于繁忙，浏览器正忙着解析和执行其他 js。**如上图所示的 FID 时间，主线程正在处理其他任务。

当 FID 的时间为 100ms 或以内，则为 Good。

上面的例子中，用户刚好在主线程最繁忙的时刻进行了交互，但是如果用户在主线程空闲的时候交互，那么浏览器可以立刻响应。所以 FID 的值需要重点查看它的分布情况。

FID 实际上测量的是输入事件被感知到到主线程空闲的这段时间。这意味着即使没有输入事件被注册，FID 也可以测量。因为用户的输入相应并不一定需要事件被执行，但一定需要主线程是空闲的。例如，下面这些 HTML 元素都需要在交互响应之前等待主线程上的正在执行的任务完成：

- 输入框，例如`<input>`、`<textarea>`、`<radio>`、`<checkbox>`
- 下拉框，例如`<select>`
- 链接，例如`<a>`

为什么要考虑测量第一次的输入延迟？有如下原因：

- 因为第一次输入延迟是用户对你的网站形成的第一个印象，网站是否有质量且可靠；
- 在今天，web 中最大的交互问题第一次加载之后；
- 对于网站应该如何解决较高的首次输入延迟(例如代码分割、减少 JavaScript 的预加载)的建议解决方案(TTI 是指衡量这一块)，不一定与在页面加载后解决输入延迟(FID 是指衡量这一块)的解决方案相同。所以 FID 是在 TTI 的基础上更精确的细分。

为什么 FID 只是包含从用户输入到主线程开始相应的时间？而没有包含事件处理到浏览器绘制 UI 的时间？

尽管主线程处理和绘制的这段时间也很重要，但是如果 FID 把这段时间也包含进来，开发者可能会使用异步 API(例如`setTimeout`、`requestAnimationFrame`)来把这个 task 拆分到下一帧，以较少 FID 的时间，这样不仅没有提高用户体验，反而使用户体验降低。

#### 3.2.2.2 如何测量 FID

FID 可以在实验环境也可以在线上环境测量。

**线上测量工具**

- [Chrome User Experience Report](https://developers.google.com/web/tools/chrome-user-experience-report)
- [PageSpeed Insights](https://developers.google.com/speed/pagespeed/insights/)
- [Search Console (Core Web Vitals report)](https://support.google.com/webmasters/answer/9205520)
- [`web-vitals` JavaScript library](https://github.com/GoogleChrome/web-vitals)

**原生的 JS API 测量**

```
new PerformanceObserver((entryList) => {  for (const entry of entryList.getEntries()) {    const delay = entry.processingStart - entry.startTime;    console.log('FID candidate:', delay, entry);  }}).observe({type: 'first-input', buffered: true});
```

PerformanceObserver 目前除了在 IE 上没有兼容，其他浏览器基本都兼容了。

我们看一下结果是怎样的：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171657505359328.png)
**Google 官方 web-vitals 库**

Google 官方也提供了一个[web-vitals](https://github.com/GoogleChrome/web-vitals)库，底层还是使用这个 API，只是帮我们处理了一些需要测量和不需测量的场景、以及一些细节问题。

#### 3.2.2.3 如何优化 FID

FID 可能被这四个因素影响：

- 减少第三方代码的影响
- 减少 Javascript 的执行时间
- 最小化主线程工作
- 减小请求数量和请求文件大小

更加详细的优化建议可以参考[这里](https://web.dev/optimize-fid/)。

##### **3.2.3 Cumulative Layout Shift (CLS)**

###### **3.2.3.1 CLS 如何定义**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171658011068863.png)

CLS 是一个非常重要的、以用户为中心的测量指标。它能衡量页面是否排版稳定。

页面移动会经常发生在资源异步加载、或者 DOM 元素动态添加到已存在的页面元素上面。这些元素有可能是图片、视频、第三方广告或小图标等。

但是我们开发过程中可能不会察觉到这些问题，因为调试过程中刷新页面，图片都已经缓存在本地。调试接口的时候我们使用的是 mock 或者在局域网，接口速度都很快，这些延迟都可能被我们忽略。

CLS 就是帮我们去发现这些真实发生在用户端的问题的指标。

CLS 是测量页面生命周期中，每个发生意外布局移动的分数。当一个可视元素在下一帧移动到另外一个位置，就是指布局移动。

CLS 的分数在 0.1 或以下，则为 Good。

那么意外布局移动的分数如何计算？

浏览器会监控两桢之间发生移动的不稳定元素。布局移动分数由 2 个元素决定：impact fraction 和 distance fraction。

```
layout shift score = impact fraction * distance fraction
```

可视区域内，在前一帧到下一帧之间所有不稳定的元素的并集，会影响当前帧的布局移动分数。

举个例子，下面这张图中，左边是当前帧的一个元素，下一帧中，元素下移了可视区域内 25%的高度。红色虚线框标出了两桢中当前元素的并集，占适口的 75%，所以这个时候，impact faction 是 0.75。

另外一个影响布局移动分数的是 distance fraction，指这个元素相对视口移动的距离。不管是横向还是竖向，取最大值。

下面例子中，竖向距离更大，该元素相对适口移动了 25%的距离，所以 distance fraction 是 0.25。所以布局移动分数是 `0.75 * 0.25 = 0.1875`.

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171658129451951.png)

**但是要注意的是，并不是所有的布局移动都是不好的，很多 web 网站都会改变元素的开始位置。只有当布局移动是非用户预期的，才是不好的**。

换句话说，当用户点击了按钮，布局进行了改动，这是 ok 的，CLS 的 JS API 中有一个字段`hadRecentInput`，用来标识 500ms 内是否有用户数据，视情况而定，可以忽略这个计算。

###### **3.2.3.2 如何测量 CLS**

**线上测量工具**

- [Chrome User Experience Report](https://developers.google.com/web/tools/chrome-user-experience-report)
- [PageSpeed Insights](https://developers.google.com/speed/pagespeed/insights/)
- [Search Console (Core Web Vitals report)](https://support.google.com/webmasters/answer/9205520)
- [`web-vitals` JavaScript library](https://github.com/GoogleChrome/web-vitals)

**实验室工具**

- [Chrome DevTools](https://developers.google.com/web/tools/chrome-devtools/)
- [Lighthouse](https://developers.google.com/web/tools/lighthouse/)
- [WebPageTest](https://webpagetest.org/)

**原生的 JS API 测量**

```
let cls = 0;new PerformanceObserver((entryList) => {  for (const entry of entryList.getEntries()) {    if (!entry.hadRecentInput) {      cls += entry.value;      console.log('Current CLS value:', cls, entry);    }  }}).observe({type: 'layout-shift', buffered: true});
```

我们看一下结果是怎样的：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171658214521799.png)

**Google 官方 web-vitals 库**

Google 官方也提供了一个[web-vitals](https://github.com/GoogleChrome/web-vitals)库，底层还是使用这个 API，只是帮我们处理了一些需要测量和不需测量的场景、以及一些细节问题。

###### **3.2.3.3 如何优化 CLS**

我们可以根据这些原则来避免非预期布局移动：

- 图片或视屏元素有大小属性，或者给他们保留一个空间大小，设置 width、height，或者使用[unsized-media feature policy](https://github.com/w3c/webappsec-feature-policy/blob/master/policies/unsized-media.md)。
- 不要在一个已存在的元素上面插入内容，除了相应用户输入。
- 使用 animation 或 transition 而不是直接触发布局改变。

更详细的内容可以看[这里](https://web.dev/optimize-cls/)。

### **4. 性能工具：工欲善其事，必先利其器**

Google 开发的[所有工具](https://web.dev/vitals-tools/)都支持 Core Web Vitals 的测量。工具如下：

- [Lighthouse](https://github.com/GoogleChrome/lighthouse)
- [PageSpeed Insights](https://developers.google.com/speed/pagespeed/insights/)
- [Chrome DevTools](https://developers.google.com/web/tools/chrome-devtools)
- [Search Console](https://search.google.com/search-console/about)
- [web.dev’s 提供的测量工具](https://web.dev/measure/)
- [Web Vitals 扩展](https://chrome.google.com/webstore/detail/web-vitals/ahfhijdlegdabablpippeagghigmibma)
- [Chrome UX Report](https://developers.google.com/web/tools/chrome-user-experience-report) API

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171658302259657.png)

这些工具对 Core Web Vitals 的支持如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171658404080209.png)

#### 4.1 Lighthouse

打开 F12，就可以看到 Lighthouse，点击 Generate Report，即可生成报告。当然也可以添加 chrome 插件使用。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171658511977119.png)

Lighthhouse 是一个实验室工具，本地模拟移动端和 PC 端对这几个方面进行测试。同时 lighthouse 还会针对这几个方面提出建议，在产品上线前值得一测。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171658591915966.png)

Lighthouse 还提供了[Lighthouse CI](https://github.com/GoogleChrome/lighthouse-ci)，把 Lighthouse 集成到 CI 流水线中。举个例子，每次在上线之前，跑 50 次流水线对 Lighthouse 的各项指标进行测试取平均值，一旦发现异常，立刻进行排查。把性能问题排查提前到发布之前。这块后面会细讲。

#### 4.2 PageSpeed Insights

PageSpeed Insights(PSI)是一个可以分析线上和实验室数据的工具。它是根据线上环境用户真实的数据(在 Chrome UX 报告中)和 Lighthouse 结合出一份报告。和 Lighthouse 类似，它也会给出一些分析建议，可以知道页面的 Core Web Vitals 是否达标。
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171659097738804.png)

PageSpeed 只是提供对单个页面的性能测试，而 Search Console 是正对整个网站的性能测试。

PageSpeed Insights 也提供了[API](https://developers.google.com/speed/docs/insights/v5/get-started)供我们使用。同样的，我们也可以把它集成到 CI 中。

#### 4.3 CrUX

Chrome UX Report (CrUX)是指汇聚了成千上万条用户体验数据的数据报告集，它是经过用户同意才进行上报的，目前存储在 Google BigQuery 上，可以使用账号登陆进行查询。它测量了所有的 Core Web Vitals 指标。

上面提到的 PageSpeed Insights 工具就是结合 CrUX 的数据进行分析给出的结论。

当然 CrUX 现在也提供了 API 共我们进行查询，可以查询的数据包括：

- Largest Contentful Paint
- Cumulative Layout Shift
- First Input Delay
- First Contentful Paint

原理如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171659191368083.png)

通过 API 的查询的数据每日都更新，并汇集了过去 28 天的数据。

具体的使用方式可以参考官方给出的[demo](https://developers.google.com/web/tools/chrome-user-experience-report/api/guides/getting-started)。

#### 4.4 Chrome DevTools Performance 面板

Performance 是我们最常用的本地性能分析工具。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171659272795468.png)

这里像提几点可以关注下的功能：Frame、Timings、Main、Layers、FPS。下面一一讲解。

##### **4.4.1 Frame**

点击 Frame 展开后，会看到有一个一个红色或绿色小块，这些代表着每帧的消耗时间。目前大多数设备的屏幕刷新率为 60 次/秒，浏览器渲染页面的每一帧的速率如果与设备屏幕的刷新率保持一致，即 60fps 时，我们是不会感知到页面卡的情况的。

我们把鼠标移上去看看：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171659361527158.png)

这种是体验顺畅的情况。

再比如：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171659446344336.png)

提示这一帧耗时了 30.9ms，当前是 32fps 并且是掉帧状态。

##### **4.4.2 Timings**

这里可以看到几个关键指标的时间点。

FP：First Paint；

FCP：First Contentful Paint；

LCP：Largest Contenful Paint；

DCL：DOMContentLoaded Event

L：OnLoad Event。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171659514165792.png)

##### **4.4.3 Main**

Main 是 DevTools 中最常用也是最重要的功能。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171700021394158.png)

通过 record，我们可以查看页面上所有操作在主线程中的执行过程。也就是我们常说的流程：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171700113604346.png)

一旦有任何一个流程时间过长或频繁发生，比如 Update Layer Tree 时间过长、频繁出现 RecalcStyles、Layout(重绘回流)，那么需要引起注意。后面会举一个例子。

##### **4.4.4 Layers**

Layers 是浏览器在绘制过程中生成的一个层。因为浏览器底层渲染的本质是纵向分层、横向分块。这一块的知识点是发生在 Renderer Process 进程中。后面会以一个例子展开讲。

这里想提 Layers 的原因是，**Layer 的渲染也会影响性能问题**，而且有时候还不容易被发现！

Layers 面板一般不会默认展示出来，点击更多->more tools->Layers 即可打开。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171700214127503.png)

点击 Layers 面板，点击左边下三角展开按钮，可以看见页面最终生成的合成层。右边左上角可以选择不同纬度进行查看。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171700294507568.png)
选中某个层，可以查看该层生成的原因。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171700387103475.png)

Chrome 的 Blink 内核给出了 54 种会生成合成层的原因：

```
constexpr CompositingReasonStringMap kCompositingReasonsStringMap[] = {    {CompositingReason::k3DTransform, "transform3D", "Has a 3d transform"},    {CompositingReason::kTrivial3DTransform, "trivialTransform3D",     "Has a trivial 3d transform"},    {CompositingReason::kVideo, "video", "Is an accelerated video"},    {CompositingReason::kCanvas, "canvas",     "Is an accelerated canvas, or is a display list backed canvas that was "     "promoted to a layer based on a performance heuristic."},    {CompositingReason::kPlugin, "plugin", "Is an accelerated plugin"},    {CompositingReason::kIFrame, "iFrame", "Is an accelerated iFrame"},    {CompositingReason::kSVGRoot, "SVGRoot", "Is an accelerated SVG root"},    {CompositingReason::kBackfaceVisibilityHidden, "backfaceVisibilityHidden",     "Has backface-visibility: hidden"},    {CompositingReason::kActiveTransformAnimation, "activeTransformAnimation",     "Has an active accelerated transform animation or transition"},    {CompositingReason::kActiveOpacityAnimation, "activeOpacityAnimation",     "Has an active accelerated opacity animation or transition"},    {CompositingReason::kActiveFilterAnimation, "activeFilterAnimation",     "Has an active accelerated filter animation or transition"},    {CompositingReason::kActiveBackdropFilterAnimation,     "activeBackdropFilterAnimation",     "Has an active accelerated backdrop filter animation or transition"},    {CompositingReason::kXrOverlay, "xrOverlay",     "Is DOM overlay for WebXR immersive-ar mode"},    {CompositingReason::kScrollDependentPosition, "scrollDependentPosition",     "Is fixed or sticky position"},    {CompositingReason::kOverflowScrolling, "overflowScrolling",     "Is a scrollable overflow element"},    {CompositingReason::kOverflowScrollingParent, "overflowScrollingParent",     "Scroll parent is not an ancestor"},    {CompositingReason::kOutOfFlowClipping, "outOfFlowClipping",     "Has clipping ancestor"},    {CompositingReason::kVideoOverlay, "videoOverlay",     "Is overlay controls for video"},    {CompositingReason::kWillChangeTransform, "willChangeTransform",     "Has a will-change: transform compositing hint"},    {CompositingReason::kWillChangeOpacity, "willChangeOpacity",     "Has a will-change: opacity compositing hint"},    {CompositingReason::kWillChangeFilter, "willChangeFilter",     "Has a will-change: filter compositing hint"},    {CompositingReason::kWillChangeBackdropFilter, "willChangeBackdropFilter",     "Has a will-change: backdrop-filter compositing hint"},    {CompositingReason::kWillChangeOther, "willChangeOther",     "Has a will-change compositing hint other than transform and opacity"},    {CompositingReason::kBackdropFilter, "backdropFilter",     "Has a backdrop filter"},    {CompositingReason::kBackdropFilterMask, "backdropFilterMask",     "Is a mask for backdrop filter"},    {CompositingReason::kRootScroller, "rootScroller",     "Is the document.rootScroller"},    {CompositingReason::kAssumedOverlap, "assumedOverlap",     "Might overlap other composited content"},    {CompositingReason::kOverlap, "overlap",     "Overlaps other composited content"},    {CompositingReason::kNegativeZIndexChildren, "negativeZIndexChildren",     "Parent with composited negative z-index content"},    {CompositingReason::kSquashingDisallowed, "squashingDisallowed",     "Layer was separately composited because it could not be squashed."},    {CompositingReason::kOpacityWithCompositedDescendants,     "opacityWithCompositedDescendants",     "Has opacity that needs to be applied by compositor because of composited "     "descendants"},    {CompositingReason::kMaskWithCompositedDescendants,     "maskWithCompositedDescendants",     "Has a mask that needs to be known by compositor because of composited "     "descendants"},    {CompositingReason::kReflectionWithCompositedDescendants,     "reflectionWithCompositedDescendants",     "Has a reflection that needs to be known by compositor because of "     "composited descendants"},    {CompositingReason::kFilterWithCompositedDescendants,     "filterWithCompositedDescendants",     "Has a filter effect that needs to be known by compositor because of "     "composited descendants"},    {CompositingReason::kBlendingWithCompositedDescendants,     "blendingWithCompositedDescendants",     "Has a blending effect that needs to be known by compositor because of "     "composited descendants"},    {CompositingReason::kPerspectiveWith3DDescendants,     "perspectiveWith3DDescendants",     "Has a perspective transform that needs to be known by compositor because "     "of 3d descendants"},    {CompositingReason::kPreserve3DWith3DDescendants,     "preserve3DWith3DDescendants",     "Has a preserves-3d property that needs to be known by compositor because "     "of 3d descendants"},    {CompositingReason::kIsolateCompositedDescendants,     "isolateCompositedDescendants",     "Should isolate descendants to apply a blend effect"},    {CompositingReason::kFullscreenVideoWithCompositedDescendants,     "fullscreenVideoWithCompositedDescendants",     "Is a fullscreen video element with composited descendants"},    {CompositingReason::kRoot, "root", "Is the root layer"},    {CompositingReason::kLayerForHorizontalScrollbar,     "layerForHorizontalScrollbar",     "Secondary layer, the horizontal scrollbar layer"},    {CompositingReason::kLayerForVerticalScrollbar, "layerForVerticalScrollbar",     "Secondary layer, the vertical scrollbar layer"},    {CompositingReason::kLayerForScrollCorner, "layerForScrollCorner",     "Secondary layer, the scroll corner layer"},    {CompositingReason::kLayerForScrollingContents, "layerForScrollingContents",     "Secondary layer, to house contents that can be scrolled"},    {CompositingReason::kLayerForSquashingContents, "layerForSquashingContents",     "Secondary layer, home for a group of squashable content"},    {CompositingReason::kLayerForForeground, "layerForForeground",     "Secondary layer, to contain any normal flow and positive z-index "     "contents on top of a negative z-index layer"},    {CompositingReason::kLayerForMask, "layerForMask",     "Secondary layer, to contain the mask contents"},    {CompositingReason::kLayerForDecoration, "layerForDecoration",     "Layer painted on top of other layers as decoration"},    {CompositingReason::kLayerForOther, "layerForOther",     "Layer for link highlight, frame overlay, etc."},    {CompositingReason::kBackfaceInvisibility3DAncestor,     "BackfaceInvisibility3DAncestor",     "Ancestor in same 3D rendering context has a hidden backface"},};
```

##### **4.4.5 Rendering**

Rendering 面板也隐藏了很多好用的功能。

###### **4.4.5.1 Paint flashing**

勾选了 Paint flashing 后，我们就会看到页面上有哪些内容被重绘了：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171700554576876.png)

###### **4.4.5.2 Layout Shift6 Regions**

勾选了 Layout Shift Regions 后，进行交互，就可以看到哪些元素进行了布局移动：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171701032304771.png)

###### **4.4.5.3 Frame Rendering Stats**

这个工具有一个小插曲。

Frame Rendering Stats 的前身是 FPS meter，在 Google 版本[85.0.4181.0](https://chromium.googlesource.com/chromium/src/+log/85.0.4180.0..85.0.4181.0?pretty=fuller&n=10000)改成了 Frame Rendering Stats，但是迫于[用户抱怨](https://www.bleepingcomputer.com/news/google/google-chrome-rolls-back-fps-meter-changes-after-user-complaints/?__cf_chl_jschl_tk__=cc8bc924a9cecaaeac51306be72fd84e9e9497e4-1620787727-0-AcZbEMncBYzkJEfBKoagUU_Lx_usv9QYDmr9LcL011Asjxa2HprCsaE1dwQcSl-OQQ82hMfZEpmBKtrfSDn1AXmdocNGLn3WqQePxYnFpYr5xPlLRYCzEDejvJYmGWRW09ceBNb4LKnAqh-5uKGnE5DccFkd1CY_t_Omo-l8SaAQqrEcOboHnNIehm6-UOHfwHJ6nvyrOUTDAHtzi0x1izcrT73ECtRUNkvg3l48cNkm2NNZVcq7mIpnWqTfJEEdZ9oSmZOU9AjWkNETVkiUdZ7m3ylBJ_tXVptpa01quzWE9xhdRYyLM6dbeHGpxcqaNHKdbkPBki7jZYpZiKza1hFELUG5RXceHwQPqAjZLuY0StG7nQnIcoPdYh4339dB4883flK-OXEB4O1xykIF_HEQ6Kz3cnvLTrpaxuDVwxUhV3S7rdhXxj4wiNPju2t6gsXiHgGfuNUxu2c9sMbM6888k9jSIMBUMhLHgDYn6lnBxPcQO-MoIzUf0NalXSZqyw)，在 90 版本的时候又改回来了。

Frame Rendering Stats 主要显示不掉帧率。而 FPS 侧重于显示每秒的刷新率 fps。

Chrome 为什么要改成不掉帧率，是因为认为不掉帧率更能反映页面的顺畅度。而 FPS 显示每一秒渲染的帧数虽然能一定程度反映页面顺畅度，但是在一些特殊情况例如没有激活或空闲的页面，fps 会比较低，这样并不能反映真实情况。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171701147681846.png)

##### **4.4.6 Memory**

在大型项目中，内存问题也是有发生。DevTools 也提供了内存分析工具供我们使用。

点击 Memory 面板，点击录制按钮。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171701211749102.png)
点击录制后，会看到当前状态下内存的占用情况，根据大小排序，我们可以定位到内存占用过多的地方。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171701583562951.png)

#### 4.5 Search Console

Google Search Console 其实就是监控和维护网站在 Google 搜索结果中的展示情况以及排查问题的平台。数据来源是 CrUX。

它会展示 3 个 Core Web Vitals metrics: LCP, FID, CLS。如果发现有问题，可以配合 PageSpeed 一起使用，分析问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171701447244702.png)

#### 4.6 web.dev

[web.dev/measure](https://web.dev/measure/)是 google 官方提供的测量性能工具，也会提供类似 PageSpeed Insight 的指标，还会提供一些具体代码更改建议。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171702095330832.png)

#### 4.7 Web Vitals extension

Google 也提供了扩展工具去测量 Core Web Vitals。可以从[Store](https://chrome.google.com/webstore/detail/web-vitals/ahfhijdlegdabablpippeagghigmibma?hl=en)中进行安装。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171702202480424.png)

#### 4.8 工具：思考与总结

当我们了解了这么多工具之后，琳琅满目，我们该如何选择？如何使用好这些工具进行分析？

- 首先我们可以使用 Lighthouse，在本地进行测量，根据报告给出的一些建议进行优化；
- 发布之后，我们可以使用 PageSpeed Insights 去看下线上的性能情况；
- 接着，我们可以使用 Chrome User Experience Report API 去捞取线上过去 28 天的数据；
- 发现数据有异常，我们可以使用 DevTools 工具进行具体代码定位分析；
- 使用 Search Console’s Core Web Vitals report 查看网站功能整体情况；
- 使用 Web Vitals 扩展方便的看页面核心指标情况；

### **5. 谈谈监控**

最后一个章节想来谈谈监控。

我们在做性能优化的时候，常常会通过各种线上打点，来收集用户数据，进行性能分析。没错，这是一种监控手段，更精确的说，这是一种”事后”监控手段。

“事后”监控固然重要，但我们也应该考虑”事前”监控，否则，每次发布一个需求后，去线上看数据。咦，发现数据下降了，然后我们去查代码，去查数据，去查原因。这样性能优化的同学永远处于”追赶者”的角色，永远跟在屁股后面查问题。

举个例子，我们可以这样去做”事前”监控。

建立流水线机制。流水线上如何做呢？

- [Lighthouse CI](https://github.com/GoogleChrome/lighthouse-ci) 或 [PageSpeed Insights API](https://developers.google.com/speed/docs/insights/v5/get-started)：把 Lighthouse 或 PageSpeed Insights API 集成到 CI 流水线中，输出报告分析。
- [Puppeteer](https://github.com/puppeteer/puppeteer/blob/main/docs/api.md#class-tracing) 或 [Playwright](https://playwright.dev/docs/api/class-browser?_highlight=tracing#browserstarttracingpage-options)：使用 E2E 自动化测试工具集成到流水线模拟用户操作，得到 Chrome Trace Files，也就是我们平常录制 Performance 后，点击左上角下载的文件。Puppeteer 和 Playwright 底层都是基于[Chrome DevTools Protocol](https://chromedevtools.github.io/devtools-protocol/)。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171702388250820.png)

- Chrome Trace Files：根据[规则](https://docs.google.com/document/d/1CvAClvFfyA5R-PhYUmn5OOQtYMH4h6I0nSsKchNAySU/edit)分析 Trace 文件，可以得到每个函数执行的时间。如果函数执行时间超过了一个临界值，可以抛出异常。如果一个函数每次的执行时间都超过了临界值，那么就值得注意了。但是还有一点需要思考的是：函数执行的时间是否超过临界值固然重要，但更重要的是这是不是用户的输入响应函数，与用户体验是否有关。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171702481831930.png)

- 输出报告。定义异常临界值。如果异常过多，考虑是否卡发布流程。

### **6. 总结**

我们来回顾一下前面的内容：

**第一部分，讲了浏览器整体架构和渲染相关进程**.为什么把这个章节也放到这篇性能优化的文章中？浏览器对于我们前端开发来说，是一个 sandbox 或者 darkbox。我们知道 js、html、css 结合起来就能实现我们的需求，但如果知道它是如何去渲染、执行、处理我们的代码，不管是对做需求还是性能优化，都能更知其然和所以然。

**第二部分，雅虎军规是多年前提出的非常经典的优化建议**。至今对于我们异常有很强的指导作用。你会发现它是从页面加载、页面渲染、到页面交互全面的一个指导建议。与今天 Chrome 和 W3c 提出的 Web Vitals 思路依然类似。

**第三部分，性能指标**。参考标准与业内标杆的建议，能更好地指导我们进行优化。

**第四部分，性能工具**。工欲善其事，必先利其器。这个道理大家都懂，运用好工具，才能让我们更加事半功倍。

**第五部分，监控在性能优化中占很重要的部分，”事前”监控更重要，防患于未然。让性能优化成为一个预防者而不是追赶者。**

罗里吧嗦说了很多，当然还有很多性能优化的细节没有讲到，如果有错误的地方欢迎指正。或者有什么好方法好建议也强烈欢迎私聊交流一下。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171702583609362.png)

参考文章：

- https://web.dev/learn-web-vitals/
- https://developers.google.com/web/updates/2018/09/inside-browser-part1
- https://aerotwist.com/blog/the-anatomy-of-a-frame/
- https://www.chromium.org/developers/how-tos/getting-around-the-chrome-source-code
- https://medium.com/punching-performance/jank-you-can-measure-what-your-users-can-feel-e5713df2845f

原文作者：rosefang，腾讯 PCG 前端开发工程师

原文链接：https://mp.weixin.qq.com/s/RCJftzmhQbc-b89pU5d32w

# 【NO.124】一文掌握 Linux 内存管理

以下源代码来自 linux-5.10.3 内核代码，主要以 x86-32 为例。

Linux 内存管理是一个很复杂的“工程”，它不仅仅是对物理内存的管理，也涉及到虚拟内存管理、内存交换和内存回收等

## 1.物理内存的探测

Linux 内核通过 detect_memory()函数实现对物理内存的探测

```
void detect_memory(void){ detect_memory_e820(); detect_memory_e801(); detect_memory_88();}
```

这里主要介绍一下 detect_memory_e820()，detect_memory_e801()和 detect_memory_88()是针对较老的电脑进行兼容而保留的

```
static void detect_memory_e820(void){ int count = 0; struct biosregs ireg, oreg; struct boot_e820_entry *desc = boot_params.e820_table; static struct boot_e820_entry buf; /* static so it is zeroed */ initregs(&ireg); ireg.ax  = 0xe820; ireg.cx  = sizeof(buf); ireg.edx = SMAP; ireg.di  = (size_t)&buf; do {  intcall(0x15, &ireg, &oreg);  ireg.ebx = oreg.ebx; /* for next iteration... */  if (oreg.eflags & X86_EFLAGS_CF)   break;  if (oreg.eax != SMAP) {   count = 0;   break;  }  *desc++ = buf;  count++; } while (ireg.ebx && count < ARRAY_SIZE(boot_params.e820_table)); boot_params.e820_entries = count;}
```

**detect_memory_e820()实现内核从 BIOS 那里获取到内存的基础布局**，之所以叫 e820 是因为内核是通过 0x15 中断向量，并在 AX 寄存器中指定 0xE820，中断调用后将会返回被 BIOS 保留的内存地址范围以及系统可以使用的内存地址范围，所有通过中断获取的数据将会填充在 boot_params.e820_table 中，具体 0xE820 的详细用法感兴趣的话可以上网查……这里获取到的 e820_table 里的数据是未经过整理，linux 会通过 setup_memory_map 去整理这些数据

```
start_kernel() -> setup_arch() -> setup_memory_map()void __init e820__memory_setup(void){ char *who; BUILD_BUG_ON(sizeof(struct boot_e820_entry) != 20); who = x86_init.resources.memory_setup(); memcpy(e820_table_kexec, e820_table, sizeof(*e820_table_kexec)); memcpy(e820_table_firmware, e820_table, sizeof(*e820_table_firmware)); pr_info("BIOS-provided physical RAM map:\n"); e820__print_table(who);}
```

x86_init.resources.memory_setup()指向了 e820__memory_setup_default()，会将 boot_params.e820_table 转换为内核自己使用的 e820_table，转换之后的**e820 表记录着所有物理内存的起始地址、长度以及类型**，然后通过 memcpy 将 e820_table 复制到 e820_table_kexec、e820_table_firmware

```
struct x86_init_ops x86_init __initdata = { .resources = {  .probe_roms  = probe_roms,  .reserve_resources = reserve_standard_io_resources,  .memory_setup  = e820__memory_setup_default, }, ......}char *__init e820__memory_setup_default(void){ char *who = "BIOS-e820"; /*  * Try to copy the BIOS-supplied E820-map.  *  * Otherwise fake a memory map; one section from 0k->640k,  * the next section from 1mb->appropriate_mem_k  */ if (append_e820_table(boot_params.e820_table, boot_params.e820_entries) < 0) {  u64 mem_size;  /* Compare results from other methods and take the one that gives more RAM: */  if (boot_params.alt_mem_k < boot_params.screen_info.ext_mem_k) {   mem_size = boot_params.screen_info.ext_mem_k;   who = "BIOS-88";  } else {   mem_size = boot_params.alt_mem_k;   who = "BIOS-e801";  }  e820_table->nr_entries = 0;  e820__range_add(0, LOWMEMSIZE(), E820_TYPE_RAM);  e820__range_add(HIGH_MEMORY, mem_size << 10, E820_TYPE_RAM); } /* We just appended a lot of ranges, sanitize the table: */ e820__update_table(e820_table); return who;}
```

内核使用的**e820_table 结构描述**如下：

```
enum e820_type { E820_TYPE_RAM  = 1, E820_TYPE_RESERVED = 2, E820_TYPE_ACPI  = 3, E820_TYPE_NVS  = 4, E820_TYPE_UNUSABLE = 5, E820_TYPE_PMEM  = 7, E820_TYPE_PRAM  = 12, E820_TYPE_SOFT_RESERVED = 0xefffffff, E820_TYPE_RESERVED_KERN = 128,};struct e820_entry { u64   addr; u64   size; enum e820_type  type;} __attribute__((packed));struct e820_table { __u32 nr_entries; struct e820_entry entries[E820_MAX_ENTRIES];};
```

### 1.1 memblock 内存分配器

linux x86 内存映射主要存在两种方式：**段式映射和页式映射**。linux 首次进入保护模式时会用到段式映射（加电时，运行在实模式，任意内存地址都能执行代码，可以被读写，这非常不安全，CPU 为了提供限制/禁止的手段，提出了保护模式），根据段寄存器（以 8086 为例，段寄存器有 CS（Code Segment）：代码段寄存器；DS（Data Segment）：数据段寄存器；SS（Stack Segment）：堆栈段寄存器；ES（Extra Segment）：附加段寄存器）查找到对应的**段描述符**（这里其实就是用到了段描述符表，即段表），段描述符指明了此时的环境可以通过段访问到内存基地址、空间大小和访问权限。访问权限则点明了哪些内存可读、哪些内存可写。

```
typedef struct Descriptor{    unsigned int base;  // 段基址    unsigned int limit; // 段大小    unsigned short attribute;   // 段属性、权限}
```

linux 在段描述符表准备完成之后会通过汇编跳转到保护模式

事实上，在上面这个过程中，linux 并没有明显地去区分每个段，所以这里并没有很好地起到保护作用，linux 最终使用的还是内存分页管理（开启页式映射可以参考/arch/x86/kernel/head_32.S）

#### **1.1.1 memblock 算法**

memblock 是 linux 内核初始化阶段使用的一个内存分配器，实现较为简单，负责**页分配器初始化之前的内存管理和分配请求**，memblock 的结构如下

```
struct memblock_region { phys_addr_t base; phys_addr_t size; enum memblock_flags flags;#ifdef CONFIG_NEED_MULTIPLE_NODES int nid;#endif};struct memblock_type { unsigned long cnt; unsigned long max; phys_addr_t total_size; struct memblock_region *regions; char *name;};struct memblock { bool bottom_up;  /* is bottom up direction? */ phys_addr_t current_limit; struct memblock_type memory; struct memblock_type reserved;};
```

**bottom_up**：用来表示分配器分配内存是自低地址向高地址还是自高地址向低地址

**current_limit**：用来表示用来限制 memblock_alloc()和 memblock_alloc_base()的内存申请

**memory**：用于指向系统**可用物理内存区**，这个内存区维护着系统所有可用的物理内存，即系统 DRAM 对应的物理内存

**reserved**：用于指向系统预留区，也就是这个内存区的内存**已经分配**，在释放之前不能再次分配这个区内的内存区块

**memblock_type**中的**cnt**用于描述该类型内存区中的**内存区块数**，这有利于 MEMBLOCK 内存分配器动态地知道某种类型的内存区还有多少个内存区块

memblock_type 中的**max**用于描述该类型内存区**最大可以含有多少个内存区块**，当往某种类型的内存区添加 内存区块的时候，如果内存区的内存区块数超过 max 成员，那么 memblock 内存分配器就会增加内存区的容量，以此维护更多的内存区块

memblock_type 中的**total_size**用于统计内存区总共含有的**物理内存数**

memblock_type 中的**regions**是一个**内存区块链表**，用于维护属于这类型的所有内存区块（包括基址、大小和内存块标记等），

**name** ：用于指明这个**内存区的名字**，MEMBLOCK 分配器目前支持的内存区名字有：**“memory”, “reserved”, “physmem”**

具体关系可以参考下图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171713176389130.png)
内核启动后会为 MEMBLOCK 内存分配器创建了一些私有的 section，这些 section 用于存放于 MEMBLOCK 分配器有关的函数和数据，即 **init_memblock** 和 **initdata_memblock**。在创建完 init_memblock section 和 initdata_memblock section 之后，memblock 分配器会开始创建 struct memblock 实例，这个实例此时作为最原始的 MEMBLOCK 分配器，描述了系统的物理内存的初始值

```
#define MEMBLOCK_ALLOC_ANYWHERE (~(phys_addr_t)0) //即0xFFFFFFFF#define INIT_MEMBLOCK_REGIONS   128#ifndef INIT_MEMBLOCK_RESERVED_REGIONS# define INIT_MEMBLOCK_RESERVED_REGIONS  INIT_MEMBLOCK_REGIONS#endifstatic struct memblock_region memblock_memory_init_regions[INIT_MEMBLOCK_REGIONS] __initdata_memblock;static struct memblock_region memblock_reserved_init_regions[INIT_MEMBLOCK_RESERVED_REGIONS] __initdata_memblock;struct memblock memblock __initdata_memblock = { .memory.regions  = memblock_memory_init_regions, .memory.cnt  = 1, /* empty dummy entry */ .memory.max  = INIT_MEMBLOCK_REGIONS, .memory.name  = "memory", .reserved.regions = memblock_reserved_init_regions, .reserved.cnt  = 1, /* empty dummy entry */ .reserved.max  = INIT_MEMBLOCK_RESERVED_REGIONS, .reserved.name  = "reserved", .bottom_up  = false, .current_limit  = MEMBLOCK_ALLOC_ANYWHERE,};
```

内核在 setup_arch(char **cmdline_p)中会调用**e820__memblock_setup()**对 MEMBLOCK 内存分配器进行**初始化**启动

```
void __init e820__memblock_setup(void){ int i; u64 end; memblock_allow_resize(); for (i = 0; i < e820_table->nr_entries; i++) {  struct e820_entry *entry = &e820_table->entries[i];  end = entry->addr + entry->size;  if (end != (resource_size_t)end)   continue;  if (entry->type == E820_TYPE_SOFT_RESERVED)   memblock_reserve(entry->addr, entry->size);  if (entry->type != E820_TYPE_RAM && entry->type != E820_TYPE_RESERVED_KERN)   continue;  memblock_add(entry->addr, entry->size); } /* Throw away partial pages: */ memblock_trim_memory(PAGE_SIZE); memblock_dump_all();}
```

memblock_allow_resize() 仅是用于置 memblock_can_resize 的值，里面的 for 则是用于循环遍历 e820 的内存布局信息，然后进行 memblock_add 的操作

```
int __init_memblock memblock_add(phys_addr_t base, phys_addr_t size){ phys_addr_t end = base + size - 1; memblock_dbg("%s: [%pa-%pa] %pS\n", __func__,       &base, &end, (void *)_RET_IP_); return memblock_add_range(&memblock.memory, base, size, MAX_NUMNODES, 0);}
```

memblock_add()主要封装了 memblock_add_region()，且它操作对象是 memblock.memory，即系统可用物理内存区。

```
static int __init_memblock memblock_add_range(struct memblock_type *type,    phys_addr_t base, phys_addr_t size,    int nid, enum memblock_flags flags){ bool insert = false; phys_addr_t obase = base;    //调整size大小，确保不会越过边界 phys_addr_t end = base + memblock_cap_size(base, &size); int idx, nr_new; struct memblock_region *rgn; if (!size)  return 0; /* special case for empty array */ if (type->regions[0].size == 0) {  WARN_ON(type->cnt != 1 || type->total_size);  type->regions[0].base = base;  type->regions[0].size = size;  type->regions[0].flags = flags;  memblock_set_region_node(&type->regions[0], nid);  type->total_size = size;  return 0; }repeat: /*  * The following is executed twice.  Once with %false @insert and  * then with %true.  The first counts the number of regions needed  * to accommodate the new area.  The second actually inserts them.  */ base = obase; nr_new = 0; for_each_memblock_type(idx, type, rgn) {  phys_addr_t rbase = rgn->base;  phys_addr_t rend = rbase + rgn->size;  if (rbase >= end)   break;  if (rend <= base)   continue;  /*   * @rgn overlaps.  If it separates the lower part of new   * area, insert that portion.   */  if (rbase > base) {#ifdef CONFIG_NEED_MULTIPLE_NODES   WARN_ON(nid != memblock_get_region_node(rgn));#endif   WARN_ON(flags != rgn->flags);   nr_new++;   if (insert)    memblock_insert_region(type, idx++, base,             rbase - base, nid,             flags);  }  /* area below @rend is dealt with, forget about it */  base = min(rend, end); } /* insert the remaining portion */ if (base < end) {  nr_new++;  if (insert)   memblock_insert_region(type, idx, base, end - base,            nid, flags); } if (!nr_new)  return 0; /*  * If this was the first round, resize array and repeat for actual  * insertions; otherwise, merge and return.  */ if (!insert) {  while (type->cnt + nr_new > type->max)   if (memblock_double_array(type, obase, size) < 0)    return -ENOMEM;  insert = true;  goto repeat; } else {  memblock_merge_regions(type);  return 0; }}
```

**如果 memblock 算法管理的内存为空时，将当前空间添加进去**

不为空的情况下，for_each_memblock_type(idx, type, rgn)这个循环会检查是否存在内存重叠的情况，如果有的话，则剔除重叠部分，然后将其余非重叠的部分插入 memblock。内存块的添加主要分为四种情况（其余情况与这几种类似），可以参考下图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171713323044606.png)

如果 region 空间不够，则通过 memblock_double_array()添加新的空间，然后重试。

最后 memblock_merge_regions()会将紧挨着的内存进行合并（节点号、flag 等必须一致，节点号后面再进行介绍）。

##### **1.1.2 memblock 内存分配与回收**

到这里，memblock 内存管理的初始化基本完成，后面还有一些关于 memblock.memory 的修正，这里就不做介绍了，最后也简单介绍一下**memblock 的内存分配和回收**，即 memblock_alloc()和 memblock_free()。

```
//size即分配区块的大小，align用于字节对齐,表示分配区块的对齐大小//这里NUMA_NO_NODE指任何节点（没有节点），关于节点后面会介绍，这里节点还没初始化//MEMBLOCK_ALLOC_ACCESSIBLE指分配内存块时仅受memblock.current_limit的限制#define NUMA_NO_NODE (-1)#define MEMBLOCK_LOW_LIMIT 0#define MEMBLOCK_ALLOC_ACCESSIBLE 0static inline void * __init memblock_alloc(phys_addr_t size,  phys_addr_t align){ return memblock_alloc_try_nid(size, align, MEMBLOCK_LOW_LIMIT,          MEMBLOCK_ALLOC_ACCESSIBLE, NUMA_NO_NODE);}void * __init memblock_alloc_try_nid(   phys_addr_t size, phys_addr_t align,   phys_addr_t min_addr, phys_addr_t max_addr,   int nid){ void *ptr; memblock_dbg("%s: %llu bytes align=0x%llx nid=%d from=%pa max_addr=%pa %pS\n",       __func__, (u64)size, (u64)align, nid, &min_addr,       &max_addr, (void *)_RET_IP_); ptr = memblock_alloc_internal(size, align,        min_addr, max_addr, nid, false); if (ptr)  memset(ptr, 0, size); return ptr;}static void * __init memblock_alloc_internal(    phys_addr_t size, phys_addr_t align,    phys_addr_t min_addr, phys_addr_t max_addr,    int nid, bool exact_nid){ phys_addr_t alloc; /*  * Detect any accidental use of these APIs after slab is ready, as at  * this moment memblock may be deinitialized already and its  * internal data may be destroyed (after execution of memblock_free_all)  */ if (WARN_ON_ONCE(slab_is_available()))  return kzalloc_node(size, GFP_NOWAIT, nid); if (max_addr > memblock.current_limit)  max_addr = memblock.current_limit; alloc = memblock_alloc_range_nid(size, align, min_addr, max_addr, nid,     exact_nid); /* retry allocation without lower limit */ if (!alloc && min_addr)  alloc = memblock_alloc_range_nid(size, align, 0, max_addr, nid,      exact_nid); if (!alloc)  return NULL; return phys_to_virt(alloc);}
```

memblock_alloc_internal 返回的是分配到的内存块的虚拟地址，为 NULL 表示分配失败，关于 phys_to_virt 的实现后面再介绍，这里主要看 memblock_alloc_range_nid 的实现。

```
phys_addr_t __init memblock_alloc_range_nid(phys_addr_t size,     phys_addr_t align, phys_addr_t start,     phys_addr_t end, int nid,     bool exact_nid){ enum memblock_flags flags = choose_memblock_flags(); phys_addr_t found; if (WARN_ONCE(nid == MAX_NUMNODES, "Usage of MAX_NUMNODES is deprecated. Use NUMA_NO_NODE instead\n"))  nid = NUMA_NO_NODE; if (!align) {  /* Can't use WARNs this early in boot on powerpc */  dump_stack();  align = SMP_CACHE_BYTES; }again: found = memblock_find_in_range_node(size, align, start, end, nid,         flags); if (found && !memblock_reserve(found, size))  goto done; if (nid != NUMA_NO_NODE && !exact_nid) {  found = memblock_find_in_range_node(size, align, start,          end, NUMA_NO_NODE,          flags);  if (found && !memblock_reserve(found, size))   goto done; } if (flags & MEMBLOCK_MIRROR) {  flags &= ~MEMBLOCK_MIRROR;  pr_warn("Could not allocate %pap bytes of mirrored memory\n",   &size);  goto again; } return 0;done: if (end != MEMBLOCK_ALLOC_KASAN)  kmemleak_alloc_phys(found, size, 0, 0); return found;}
```

kmemleak 是一个检查内存泄漏的工具，这里就不做介绍了。

memblock_alloc_range_nid()首先对 align 参数进行检测，如果为零，则警告。接着函数调用 memblock_find_in_range_node() 函数**从可用内存区中找一块大小为 size 的物理内存区块**， 然后调用 memblock_reseve() 函数在找到的情况下，将这块物理内存区块加入到预留区内。

```
static phys_addr_t __init_memblock memblock_find_in_range_node(phys_addr_t size,     phys_addr_t align, phys_addr_t start,     phys_addr_t end, int nid,     enum memblock_flags flags){ phys_addr_t kernel_end, ret; /* pump up @end */ if (end == MEMBLOCK_ALLOC_ACCESSIBLE ||     end == MEMBLOCK_ALLOC_KASAN)  end = memblock.current_limit; /* avoid allocating the first page */ start = max_t(phys_addr_t, start, PAGE_SIZE); end = max(start, end); kernel_end = __pa_symbol(_end); if (memblock_bottom_up() && end > kernel_end) {  phys_addr_t bottom_up_start;  /* make sure we will allocate above the kernel */  bottom_up_start = max(start, kernel_end);  /* ok, try bottom-up allocation first */  ret = __memblock_find_range_bottom_up(bottom_up_start, end,            size, align, nid, flags);  if (ret)   return ret;  WARN_ONCE(IS_ENABLED(CONFIG_MEMORY_HOTREMOVE),     "memblock: bottom-up allocation failed, memory hotremove may be affected\n"); } return __memblock_find_range_top_down(start, end, size, align, nid,           flags);}
```

memblock 分配器分配内存是支持自低地址向高地址和自高地址向低地址的，如果 memblock_bottom_up() 函数返回 true，表示 MEMBLOCK 从低向上分配，而前面初始化的时候这个返回值其实是 false（某些情况下不一定），所以这里主要看**memblock_find_range_top_down 的实现（**memblock_find_range_bottom_up 的实现是类似的）。

```
static phys_addr_t __init_memblock__memblock_find_range_top_down(phys_addr_t start, phys_addr_t end,          phys_addr_t size, phys_addr_t align, int nid,          enum memblock_flags flags){ phys_addr_t this_start, this_end, cand; u64 i; for_each_free_mem_range_reverse(i, nid, flags, &this_start, &this_end,     NULL) {  this_start = clamp(this_start, start, end);  this_end = clamp(this_end, start, end);  if (this_end < size)   continue;  cand = round_down(this_end - size, align);  if (cand >= this_start)   return cand; } return 0;}
```

Clamp 函数可以将随机变化的数值限制在一个给定的区间[min, max]内。

***_memblock\*find_range_top_down()**通过使用 for_each_free_mem_range_reverse 宏封装调用*_next*free_mem_range_rev()函数，该函数逐一将 memblock.memory 里面的内存块信息提取出来与 memblock.reserved 的各项信息进行检验，确保返回的 this_start 和 this_end 不会与 reserved 的内存存在交叉重叠的情况。判断大小是否满足，满足的情况下，将自末端向前（因为这是 top-down 申请方式）的 size 大小的空间的起始地址（前提该地址不会超出 this_start）返回回去，至此满足要求的内存块找到了。

最后看一下**memblock_free**的实现：

```
int __init_memblock memblock_free(phys_addr_t base, phys_addr_t size){ phys_addr_t end = base + size - 1; memblock_dbg("%s: [%pa-%pa] %pS\n", __func__,       &base, &end, (void *)_RET_IP_); kmemleak_free_part_phys(base, size); return memblock_remove_range(&memblock.reserved, base, size);}
```

这里直接看最核心的部分：

```
static int __init_memblock memblock_remove_range(struct memblock_type *type,       phys_addr_t base, phys_addr_t size){ int start_rgn, end_rgn; int i, ret; ret = memblock_isolate_range(type, base, size, &start_rgn, &end_rgn); if (ret)  return ret; for (i = end_rgn - 1; i >= start_rgn; i--)  memblock_remove_region(type, i); return 0;}static void __init_memblock memblock_remove_region(struct memblock_type *type, unsigned long r){ type->total_size -= type->regions[r].size; memmove(&type->regions[r], &type->regions[r + 1],  (type->cnt - (r + 1)) * sizeof(type->regions[r])); type->cnt--; /* Special case for empty arrays */ if (type->cnt == 0) {  WARN_ON(type->total_size != 0);  type->cnt = 1;  type->regions[0].base = 0;  type->regions[0].size = 0;  type->regions[0].flags = 0;  memblock_set_region_node(&type->regions[0], MAX_NUMNODES); }}
```

其主要功能是**将指定下标索引的内存项从 memblock.reserved 中移除**。

在*_memblock*remove()里面，**memblock_isolate_range()主要作用是将要移除的物理内存区从 reserved 内存区中分离出来**，将 start_rgn 和 end_rgn（该内存区块的起始、结束索引号）返回回去。

memblock_isolate_range()返回后，接着调用 memblock_remove_region() 函数将这些索引对应的内存区块从内存区中移除，这里具体做法为调用 memmove 函数将 r 索引之后的内存区块全部往前挪一个位置，这样 r 索引对应的内存区块就被移除了，如果移除之后，内存区不含有任何内存区块，那么就初始化该内存区。

##### **1.1.3 memblock 内存管理总结**

memblock 内存区管理算法将可用可分配的内存用 memblock.memory 进行管理，已分配的内存用 memblock.reserved 进行管理，只要内存块加入到 memblock.reserved 里面就表示该内存被申请占用了，另外，在内存申请的时候，memblock 仅是把被申请到的内存加入到 memblock.reserved 中，而没有对 memblock.memory 进行任何删除或改动操作，因此申请和释放的操作都集中在 memblock.reserved。这个算法效率不高，但是却是合理的，因为在内核初始化阶段并没有太多复杂的内存操作场景，而且很多地方都是申请的内存都是永久使用的。

------

## 2.内核页表

上面有提到，内核会通过/arch/x86/kernel/head_32.S 开启页式映射，不过这里建立的页表只是临时页表，而内核真正需要建立的是内核页表。

### 2.1 **内核虚拟地址空间与用户虚拟地址空间**

为了合理地利用 4G 的内存空间，Linux 采用了 3：1 的策略，即内核占用 1G 的线性地址空间，用户占用 3G 的线性地址空间，由于历史原因，用户进程的地址范围从 0~~3G，内核地址范围从 3G~~4G，而内核的那 1GB 地址空间又称为内核虚拟地址（逻辑地址）空间，虽然内核在虚拟地址中是在高地址的，但是在物理地址中是从 0 开始的。

内核虚拟地址与用户虚拟地址，这两者都是虚拟地址，都需要经过 MMU 的翻译转换为物理地址，从硬件层面上来看，所谓的内核虚拟地址和用户虚拟地址只是权限不一样而已，但在软件层面上看，就大不相同了，当进程需要知道一个用户空间虚拟地址对应的物理地址时，linux 内核需要通过页表来得到它的物理地址，而内核空间虚拟地址是所有进程共享的，也就是说，内核在初始化时，就可以创建内核虚拟地址空间的映射，并且这里的映射就是线性映射，它基本等同于物理地址，只是它们之间有一个固定的偏移，当内核需要获取该物理地址时，可以绕开页表翻译直接通过偏移计算拿到，当然这是从软件层面上来看的，当内核去访问该页时, 硬件层面仍然走的是 MMU 翻译的全过程。

至于为什么用户虚拟地址空间不能也像内核虚拟地址空间这么做，原因是用户地址空间是随进程创建才产生的，无法事先给它分配一块连续的内存

内核通过内核虚拟地址可以直接访问到对应的物理地址，那内核如何使用其它的用户虚拟地址（0~3G）？

Linux 采用的一种折中方案是只对 1G 内核空间的前 896 MB 按**线性映射**, 剩下的 128 MB 采用动态映射，即走多级页表翻译，这样，内核态能访问空间就更多了。这里 linux 内核把这 896M 的空间称为 NORMAL 内存，剩下的 128M 称为高端内存，即 highmem。在 64 位处理器上，内核空间大大增加，所以也就不需要高端内存了，但是仍然保留了动态映射。

**动态映射**不全是为了内核空间可以访问更多的物理内存，还有一个重要原因：如果内核空间全线性映射，那么很可能就会出现内核空间碎片化而满足不了很多连续页面分配的需求（这类似于内存分段与内存分页）。因此内核空间也必须有一部分是非线性映射，从而在这碎片化物理地址空间上，用页表构造出连续虚拟地址空间（虚拟地址连续、物理地址不连续），这就是所谓的 vmalloc 空间。

到这里，可以大致知道**linux 虚拟内存的构造**：

### 2.2 **linux 内存分页**

linux 内核主要是通过内存分页来管理内存的，这里先介绍两个重要的变量：max_pfn 和 max_low_pfn。**max_pfn 为最大物理内存页面帧号，max_low_pfn 为低端内存区的最大可用页帧号**，它们的初始化如下：

```
void __init setup_arch(char **cmdline_p){    ...... max_pfn = e820__end_of_ram_pfn(); .....#ifdef CONFIG_X86_32 /* max_low_pfn get updated here */ find_low_pfn_range();#else check_x2apic(); /* How many end-of-memory variables you have, grandma! */ /* need this before calling reserve_initrd */ if (max_pfn > (1UL<<(32 - PAGE_SHIFT)))  max_low_pfn = e820__end_of_low_ram_pfn(); else  max_low_pfn = max_pfn; high_memory = (void *)__va(max_pfn * PAGE_SIZE - 1) + 1;#endif ......    e820__memblock_setup();    ......}
```

其中 e820__end_of_ram_pfn 的实现如下，其中 E820_TYPE_RAM 代表可用物理内存类型

```
#define PAGE_SHIFT  12#ifdef CONFIG_X86_32# ifdef CONFIG_X86_PAE#  define MAX_ARCH_PFN  (1ULL<<(36-PAGE_SHIFT))# else//32位系统，1<<20，即0x100000，代表4G物理内存的最大页面帧号#  define MAX_ARCH_PFN  (1ULL<<(32-PAGE_SHIFT))# endif#else /* CONFIG_X86_32 */# define MAX_ARCH_PFN MAXMEM>>PAGE_SHIFT#endifunsigned long __init e820__end_of_ram_pfn(void){ return e820_end_pfn(MAX_ARCH_PFN, E820_TYPE_RAM);}/* * Find the highest page frame number we have available */static unsigned long __init e820_end_pfn(unsigned long limit_pfn, enum e820_type type){ int i; unsigned long last_pfn = 0; unsigned long max_arch_pfn = MAX_ARCH_PFN; for (i = 0; i < e820_table->nr_entries; i++) {  struct e820_entry *entry = &e820_table->entries[i];  unsigned long start_pfn;  unsigned long end_pfn;  if (entry->type != type)   continue;  start_pfn = entry->addr >> PAGE_SHIFT;  end_pfn = (entry->addr + entry->size) >> PAGE_SHIFT;  if (start_pfn >= limit_pfn)   continue;  if (end_pfn > limit_pfn) {   last_pfn = limit_pfn;   break;  }  if (end_pfn > last_pfn)   last_pfn = end_pfn; } if (last_pfn > max_arch_pfn)  last_pfn = max_arch_pfn; pr_info("last_pfn = %#lx max_arch_pfn = %#lx\n",  last_pfn, max_arch_pfn); return last_pfn;}
```

**e820__end_of_ram_pfn**其实就是遍历**e820_table**，得到内存块的起始地址以及内存块大小，将起始地址右移 PAGE_SHIFT，算出其起始地址对应的页面帧号，同时根据内存块大小可以算出结束地址的页号，如果结束页号大于 limit_pfn，则设置该页号为为 limit_pfn，然后通过比较得到一个 last_pfn，即系统真正的最大物理页号。

**max_low_pfn**的计算则调用到了**find_low_pfn_range**：

```
#define PFN_UP(x) (((x) + PAGE_SIZE-1) >> PAGE_SHIFT)#define PFN_DOWN(x) ((x) >> PAGE_SHIFT)#define PFN_PHYS(x) ((phys_addr_t)(x) << PAGE_SHIFT)#ifndef __pa#define __pa(x)  __phys_addr((unsigned long)(x))#endif#define VMALLOC_RESERVE  SZ_128M#define VMALLOC_END  (CONSISTENT_BASE - PAGE_SIZE)#define VMALLOC_START  ((VMALLOC_END) - VMALLOC_RESERVE)#define VMALLOC_VMADDR(x) ((unsigned long)(x))#define MAXMEM   __pa(VMALLOC_START)#define MAXMEM_PFN  PFN_DOWN(MAXMEM)void __init find_low_pfn_range(void){ /* it could update max_pfn */ if (max_pfn <= MAXMEM_PFN)  lowmem_pfn_init(); else  highmem_pfn_init();}
```

**PFN_DOWN(x)是用来返回小于 x 的最后一个物理页号，PFN_UP(x)是用来返回大于 x 的第一个物理页号，这里 x 即物理地址，而 PFN_PHYS(x)返回的是物理页号 x 对应的物理地址**。

**__pa 其实就是通过虚拟地址计算出物理地址**，这一块后面再做讲解。

将 MAXMEM 展开一下可得

```
#ifdef CONFIG_HIGHMEM#define CONSISTENT_BASE  ((PKMAP_BASE) - (SZ_2M))#define CONSISTENT_END  (PKMAP_BASE)#else#define CONSISTENT_BASE  (FIXADDR_START - SZ_2M)#define CONSISTENT_END  (FIXADDR_START)#endif#define SZ_2M    0x00200000#define SZ_128M    0x08000000#define MAXMEM              __pa(VMALLOC_END – PAGE_OFFSET – __VMALLOC_RESERVE)//进一步展开#define MAXMEM              __pa(CONSISTENT_BASE - PAGE_SIZE – PAGE_OFFSET – SZ_128M)//再进一步展开#define MAXMEM              __pa((PKMAP_BASE) - (SZ_2M) - PAGE_SIZE – PAGE_OFFSET – SZ_128M)
```

下面这一部分就涉及到高端内存的构成了，其中**PKMAP_BASE 是持久映射空间（KMAP 空间，持久映射区）的起始地址，LAST_PKMAP 则是持久映射空间的映射页面数，而 FIXADDR_TOP 是固定映射区（临时内核映射区）的末尾，FIXADDR_START 是固定映射区起始地址**，其中的*_end*of_permanent_fixed_addresses 是固定映射的一个标志（一个枚举值，具体可以参考\arch\x86\include\asm\fixmap.h 里的 enum fixed_addresses）。最后的 VMALLOC_END 即为动态映射区的末尾。

```
//临时映射//-4096(4KB) -> 0xfffff000#define __FIXADDR_TOP (-PAGE_SIZE)#define FIXADDR_TOP ((unsigned long)__FIXADDR_TOP)#define FIXADDR_SIZE  (__end_of_permanent_fixed_addresses << PAGE_SHIFT)#define FIXADDR_START  (FIXADDR_TOP - FIXADDR_SIZE)#define FIXADDR_TOT_SIZE (__end_of_fixed_addresses << PAGE_SHIFT)#define FIXADDR_TOT_START (FIXADDR_TOP - FIXADDR_TOT_SIZE)//持久内核映射#ifdef CONFIG_X86_PAE#define LAST_PKMAP 512#else#define LAST_PKMAP 1024#endif#define CPU_ENTRY_AREA_BASE \ ((FIXADDR_TOT_START - PAGE_SIZE*(CPU_ENTRY_AREA_PAGES+1)) & PMD_MASK)#define LDT_BASE_ADDR  \ ((CPU_ENTRY_AREA_BASE - PAGE_SIZE) & PMD_MASK)#define LDT_END_ADDR  (LDT_BASE_ADDR + PMD_SIZE)#define PKMAP_BASE  \ ((LDT_BASE_ADDR - PAGE_SIZE) & PMD_MASK)//动态映射//0xffffff80<<20 -> 0xf8000000 -> 4,160,749,568 -> 3948MB -> 3GB+896MB 与上述一致#define high_memory (-128UL << 20)//8MB#define VMALLOC_OFFSET (8 * 1024 * 1024)#define VMALLOC_START ((unsigned long)high_memory + VMALLOC_OFFSET)#ifdef CONFIG_HIGHMEM# define VMALLOC_END (PKMAP_BASE - 2 * PAGE_SIZE)#else# define VMALLOC_END (LDT_BASE_ADDR - 2 * PAGE_SIZE)#endif
```

直接看图~

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171714069763974.png)

PAGE_OFFSET 代表的是内核空间和用户空间对虚拟地址空间的划分，对不同的体系结构不同。比如在 32 位系统中 3G-4G 属于内核使用的内存空间，所以 PAGE_OFFSET = 0xC0000000

内核空间如上图，可分为直接内存映射区和高端内存映射区，其中直接内存映射区是指 3G 到 3G+896M 的线性空间，直接对应物理地址就是 0 到 896M（前提是有超过 896M 的物理内存），其中 896M 是 high_memory 值，使用 kmalloc()/kfree()接口申请释放内存；而高端内存映射区则是超过 896M 物理内存的空间，**它又分为动态映射区、持久映射区和固定映射区**。

动态内存映射区，又称之为 vmalloc 映射区或非连续映射区，是指 VMALLOC_START 到 VMALLOC_END 的地址空间，申请释放内存的接口是 vmalloc()/vfree()，通常用于将非连续的物理内存映射为连续的线性地址内存空间；

而持久映射区，又称之为 KMAP 区或永久映射区，是指自 PKMAP_BASE 开始共 LAST_PKMAP 个页面大小的空间，操作接口是 kmap()/kunmap()，用于将高端内存长久映射到内存虚拟地址空间中；

最后的固定映射区，有时候也称为临时映射区，是指 FIXADDR_START 到 FIXADDR_TOP 的地址空间，操作接口是 kmap_atomic()/kummap_atomic()，用于解决持久映射不能用于中断处理程序而增加的临时内核映射。

上面的**MAXMEM_PFN 其实就是用来判断是否初始化（启用）高端内存，当内存物理页数本来就小于低端内存的最大物理页数时，就没有高端地址映射**。

这里接着看 max_low_pfn 的初始化，进入 highmem_pfn_init(void)。

```
static void __init highmem_pfn_init(void){ max_low_pfn = MAXMEM_PFN; if (highmem_pages == -1)  highmem_pages = max_pfn - MAXMEM_PFN; if (highmem_pages + MAXMEM_PFN < max_pfn)  max_pfn = MAXMEM_PFN + highmem_pages; if (highmem_pages + MAXMEM_PFN > max_pfn) {  printk(KERN_WARNING MSG_HIGHMEM_TOO_SMALL,   pages_to_mb(max_pfn - MAXMEM_PFN),   pages_to_mb(highmem_pages));  highmem_pages = 0; }#ifndef CONFIG_HIGHMEM /* Maximum memory usable is what is directly addressable */ printk(KERN_WARNING "Warning only %ldMB will be used.\n", MAXMEM>>20); if (max_pfn > MAX_NONPAE_PFN)  printk(KERN_WARNING "Use a HIGHMEM64G enabled kernel.\n"); else  printk(KERN_WARNING "Use a HIGHMEM enabled kernel.\n"); max_pfn = MAXMEM_PFN;#else /* !CONFIG_HIGHMEM */#ifndef CONFIG_HIGHMEM64G if (max_pfn > MAX_NONPAE_PFN) {  max_pfn = MAX_NONPAE_PFN;  printk(KERN_WARNING MSG_HIGHMEM_TRIMMED); }#endif /* !CONFIG_HIGHMEM64G */#endif /* !CONFIG_HIGHMEM */}
```

highmem_pfn_init 的主要工作其实就是**把 max_low_pfn 设置为 MAXMEM_PFN，将 highmem_pages 设置为 max_pfn – MAXMEM_PFN**，至此，max_pfn 和 max_low_pfn 初始化完毕。

### 2.3 **低端内存初始化**

回到 setup_arch 函数：

```
void __init setup_arch(char **cmdline_p){    ...... max_pfn = e820__end_of_ram_pfn(); .....#ifdef CONFIG_X86_32 /* max_low_pfn get updated here */ find_low_pfn_range();#else check_x2apic(); /* How many end-of-memory variables you have, grandma! */ /* need this before calling reserve_initrd */ if (max_pfn > (1UL<<(32 - PAGE_SHIFT)))  max_low_pfn = e820__end_of_low_ram_pfn(); else  max_low_pfn = max_pfn; high_memory = (void *)__va(max_pfn * PAGE_SIZE - 1) + 1;#endif ...... early_alloc_pgt_buf(); //<-------------------------------------    /*  * Need to conclude brk, before e820__memblock_setup()  *  it could use memblock_find_in_range, could overlap with  *  brk area.  */ reserve_brk(); //<-------------------------------------------    ......    e820__memblock_setup(); ......}
```

**early_alloc_pgt_buf()即申请页表缓冲区**

```
#define INIT_PGD_PAGE_COUNT      6#define INIT_PGT_BUF_SIZE (INIT_PGD_PAGE_COUNT * PAGE_SIZE)RESERVE_BRK(early_pgt_alloc, INIT_PGT_BUF_SIZE);void  __init early_alloc_pgt_buf(void){ unsigned long tables = INIT_PGT_BUF_SIZE; phys_addr_t base; base = __pa(extend_brk(tables, PAGE_SIZE)); pgt_buf_start = base >> PAGE_SHIFT; pgt_buf_end = pgt_buf_start; pgt_buf_top = pgt_buf_start + (tables >> PAGE_SHIFT);}
```

pgt_buf_start：标识该缓冲空间的起始物理内存页框号；

pgt_buf_end：初始化时和 pgt_buf_start 是同一个值，但是它是用于表示该空间未被申请使用的空间起始页框号；

pgt_buf_top：则是用来表示缓冲空间的末尾，存放的是该末尾的页框号

INIT_PGT_BUF_SIZE 即 24KB，这里直接看最关键的部分：**extend_brk**

```
unsigned long _brk_start = (unsigned long)__brk_base;unsigned long _brk_end   = (unsigned long)__brk_base;void * __init extend_brk(size_t size, size_t align){ size_t mask = align - 1; void *ret; BUG_ON(_brk_start == 0); BUG_ON(align & mask); _brk_end = (_brk_end + mask) & ~mask; BUG_ON((char *)(_brk_end + size) > __brk_limit); ret = (void *)_brk_end; _brk_end += size; memset(ret, 0, size); return ret;}
```

BUG_ON()函数是内核标记 bug、提供断言并输出信息的常用手段

*_brk*base 相关的初始化可以参考 arch\x86\kernel\vmlinux.lds.S

在 setup_arch()中，紧接着 early_alloc_pgt_buf()还有 reserve_brk()函数

```
static void __init reserve_brk(void){ if (_brk_end > _brk_start)  memblock_reserve(__pa_symbol(_brk_start),     _brk_end - _brk_start); /* Mark brk area as locked down and no longer taking any    new allocations */ _brk_start = 0;}
```

这个地方主要是**将 early_alloc_pgt_buf()申请的空间在 membloc 中做 reserved 保留操作**，避免被其它地方申请使用而引发异常

回到 setup_arch 函数：

```
void __init setup_arch(char **cmdline_p){    ...... max_pfn = e820__end_of_ram_pfn(); //max_pfn初始化 ...... find_low_pfn_range(); //max_low_pfn、高端内存初始化 ...... ...... early_alloc_pgt_buf(); //页表缓冲区分配 reserve_brk(); //缓冲区加入memblock.reserve    ......    e820__memblock_setup(); //memblock.memory空间初始化 启动 ......    init_mem_mapping(); //<-----------------------------    ......}
```

**init_mem_mapping()即低端内存内核页表初始化的关键函数**

```
#define ISA_END_ADDRESS  0x00100000 //1MBvoid __init init_mem_mapping(void){ unsigned long end; pti_check_boottime_disable(); probe_page_size_mask(); setup_pcid();#ifdef CONFIG_X86_64 end = max_pfn << PAGE_SHIFT;#else end = max_low_pfn << PAGE_SHIFT;#endif /* the ISA range is always mapped regardless of memory holes */ init_memory_mapping(0, ISA_END_ADDRESS, PAGE_KERNEL); /* Init the trampoline, possibly with KASLR memory offset */ init_trampoline(); /*  * If the allocation is in bottom-up direction, we setup direct mapping  * in bottom-up, otherwise we setup direct mapping in top-down.  */ if (memblock_bottom_up()) {  unsigned long kernel_end = __pa_symbol(_end);  memory_map_bottom_up(kernel_end, end);  memory_map_bottom_up(ISA_END_ADDRESS, kernel_end); } else {  memory_map_top_down(ISA_END_ADDRESS, end); }#ifdef CONFIG_X86_64 if (max_pfn > max_low_pfn) {  /* can we preseve max_low_pfn ?*/  max_low_pfn = max_pfn; }#else early_ioremap_page_table_range_init();#endif load_cr3(swapper_pg_dir); __flush_tlb_all(); x86_init.hyper.init_mem_mapping(); early_memtest(0, max_pfn_mapped << PAGE_SHIFT);}
```

probe_page_size_mask()主要作用是初始化直接映射变量（直接映射区相关）以及根据配置来控制 CR4 寄存器的置位，用于后面分页时页面大小的判定。

上面 init_memory_mapping 的参数 ISA_END_ADDRESS 表示 ISA 总线上设备的末尾地址。

**init_memory_mapping(0, ISA_END_ADDRESS, PAGE_KERNEL)初始化 0 ~ 1MB 的物理地址**，一般内核启动时被安装在 1MB 开始处，这里初始化完成之后会调用到 memory_map_bottom_up 或者 memory_map_top_down，后面就是初始化 1MB ~ 内核结束地址 这块物理地址区域 ，最后也会回归到 init_memory_mapping 的调用，因此这里不做过多的介绍，直接看 init_memory_mapping()：

```
#ifdef CONFIG_X86_32#define NR_RANGE_MR 3#else /* CONFIG_X86_64 */#define NR_RANGE_MR 5#endifstruct map_range { unsigned long start; unsigned long end; unsigned page_size_mask;};unsigned long __ref init_memory_mapping(unsigned long start,     unsigned long end, pgprot_t prot){ struct map_range mr[NR_RANGE_MR]; unsigned long ret = 0; int nr_range, i; pr_debug("init_memory_mapping: [mem %#010lx-%#010lx]\n",        start, end - 1); memset(mr, 0, sizeof(mr)); nr_range = split_mem_range(mr, 0, start, end); for (i = 0; i < nr_range; i++)  ret = kernel_physical_mapping_init(mr[i].start, mr[i].end,         mr[i].page_size_mask,         prot); add_pfn_range_mapped(start >> PAGE_SHIFT, ret >> PAGE_SHIFT); return ret >> PAGE_SHIFT;}
```

struct map_range，该结构是用来保存内存段信息，其包含了一个段的起始地址、结束地址，以及该段是按多大的页面进行分页(4K、2M、1G，1G 是 64 位的，所以这里不提及)。

```
static int __meminit split_mem_range(struct map_range *mr, int nr_range,         unsigned long start,         unsigned long end){ unsigned long start_pfn, end_pfn, limit_pfn; unsigned long pfn; int i; //返回小于...的最后一个物理页号 limit_pfn = PFN_DOWN(end); pfn = start_pfn = PFN_DOWN(start); if (pfn == 0)  end_pfn = PFN_DOWN(PMD_SIZE); else  end_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE)); if (end_pfn > limit_pfn)  end_pfn = limit_pfn; if (start_pfn < end_pfn) {  nr_range = save_mr(mr, nr_range, start_pfn, end_pfn, 0);  pfn = end_pfn; } /* big page (2M) range */ start_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE)); end_pfn = round_down(limit_pfn, PFN_DOWN(PMD_SIZE)); if (start_pfn < end_pfn) {  nr_range = save_mr(mr, nr_range, start_pfn, end_pfn,    page_size_mask & (1<<PG_LEVEL_2M));  pfn = end_pfn; } /* tail is not big page (2M) alignment */ start_pfn = pfn; end_pfn = limit_pfn; nr_range = save_mr(mr, nr_range, start_pfn, end_pfn, 0); if (!after_bootmem)  adjust_range_page_size_mask(mr, nr_range); /* try to merge same page size and continuous */ for (i = 0; nr_range > 1 && i < nr_range - 1; i++) {  unsigned long old_start;  if (mr[i].end != mr[i+1].start ||      mr[i].page_size_mask != mr[i+1].page_size_mask)   continue;  /* move it */  old_start = mr[i].start;  memmove(&mr[i], &mr[i+1],   (nr_range - 1 - i) * sizeof(struct map_range));  mr[i--].start = old_start;  nr_range--; } for (i = 0; i < nr_range; i++)  pr_debug(" [mem %#010lx-%#010lx] page %s\n",    mr[i].start, mr[i].end - 1,    page_size_string(&mr[i])); return nr_range;}
```

PMD_SIZE 用于计算由页中间目录的一个单独表项所映射的区域大小，也就是一个页表的大小。

split_mem_range()根据传入的内存 start 和 end 做四舍五入的对齐操作（即 round_up 和 round_down）

```
#define __round_mask(x, y) ((__typeof__(x))((y)-1))#define round_up(x, y) ((((x)-1) | __round_mask(x, y))+1)//可以理解为：#define round_up(x, y) (((x)+(y) - 1)/(y))*(y))#define round_down(x, y) ((x) & ~__round_mask(x, y))//可以理解为：#define round_down(x, y) ((x/y) * y)
```

round_up 宏依靠整数除法来完成这项工作，仅当两个参数均为整数时，它才有效，x 是需要四舍五入的数字，y 是应该四舍五入的间隔，也就是说，round_up(12,5)应返回 15，因为 15 是大于 12 的 5 的第一个间隔，而 round_down(12,5)应返回 10。

split_mem_range()会**根据对齐的情况，把开始、末尾的不对齐部分及中间部分分成了三段**，使用 save_mr()将其存放在 init_mem_mapping()的局部变量数组 mr 中。**划分开来主要是为了让各部分可以映射到不同大小的页面，最后如果相邻两部分映射页面的大小是一致的，则将其合并。**

可以通过 dmesg 得到划分的情况（以下是我私服的划分情况，不过是 64 位的……）

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171714388654695.png)

初始化完内存段信息 mr 之后，就行了进入到**kernel_physical_mapping_init**，这个函数是建立内核页表的最为关键的一步，负责处理**物理内存的映射**。

在 2.6.11 后，Linux 采用**四级分页模型**，这四级页目录分别为：

- 页全局目录(Page Global Directory)
- 页上级目录(Page Upper Directory)
- 页中间目录(Page Middle Directory)
- 页表(Page Table)

对于没有启动 PAE（物理地址扩展）的 32 位系统，Linux 虽然也采用四级分页模型，但本质上只用到了两级分页，Linux 通过将”页上级目录”位域和“页中间目录”位域全为 0 来达到使用两级分页的目的，但为了保证程序能 32 位和 64 系统上都能运行，内核保留了页上级目录和页中间目录在指针序列中的位置，它们的页目录数都被内核置为 1，并把这 2 个页目录项映射到适合的全局目录项。

开启 PAE 后，32 位系统寻址方式将大大改变，这时候使用的是三级页表，即页上级目录其实没有真正用到。

这里不考虑 PAE

**PAGE_OFFSET 代表的是内核空间和用户空间对虚拟地址空间的划分**，对不同的体系结构不同。比如在 32 位系统中 3G-4G 属于内核使用的内存空间，所以 PAGE_OFFSET = 0xC0000000

```
unsigned long __initkernel_physical_mapping_init(unsigned long start,        unsigned long end,        unsigned long page_size_mask,        pgprot_t prot){ int use_pse = page_size_mask == (1<<PG_LEVEL_2M); unsigned long last_map_addr = end; unsigned long start_pfn, end_pfn; pgd_t *pgd_base = swapper_pg_dir; int pgd_idx, pmd_idx, pte_ofs; unsigned long pfn; pgd_t *pgd; pmd_t *pmd; pte_t *pte; unsigned pages_2m, pages_4k; int mapping_iter; start_pfn = start >> PAGE_SHIFT; end_pfn = end >> PAGE_SHIFT; mapping_iter = 1; if (!boot_cpu_has(X86_FEATURE_PSE))  use_pse = 0;repeat: pages_2m = pages_4k = 0; pfn = start_pfn; //pfn保存起始页框号 pgd_idx = pgd_index((pfn<<PAGE_SHIFT) + PAGE_OFFSET); //低端内存的起始地址对应的pgd的偏移 pgd = pgd_base + pgd_idx; //得到起始页框对应的pgd //由pgd开始遍历    for (; pgd_idx < PTRS_PER_PGD; pgd++, pgd_idx++) {  pmd = one_md_table_init(pgd);//申请得到一个pmd表  if (pfn >= end_pfn)   continue;#ifdef CONFIG_X86_PAE  pmd_idx = pmd_index((pfn<<PAGE_SHIFT) + PAGE_OFFSET);  pmd += pmd_idx;#else  pmd_idx = 0;#endif        //遍历pmd表,对于未激活PAE的32位系统，PTRS_PER_PMD为1，激活PAE则为512  for (; pmd_idx < PTRS_PER_PMD && pfn < end_pfn;       pmd++, pmd_idx++) {   unsigned int addr = pfn * PAGE_SIZE + PAGE_OFFSET;   /*    * Map with big pages if possible, otherwise    * create normal page tables:    */   if (use_pse) {    unsigned int addr2;    pgprot_t prot = PAGE_KERNEL_LARGE;    /*     * first pass will use the same initial     * identity mapping attribute + _PAGE_PSE.     */    pgprot_t init_prot =     __pgprot(PTE_IDENT_ATTR |       _PAGE_PSE);    pfn &= PMD_MASK >> PAGE_SHIFT;    addr2 = (pfn + PTRS_PER_PTE-1) * PAGE_SIZE +     PAGE_OFFSET + PAGE_SIZE-1;    if (__is_kernel_text(addr) ||        __is_kernel_text(addr2))     prot = PAGE_KERNEL_LARGE_EXEC;    pages_2m++;    if (mapping_iter == 1)     set_pmd(pmd, pfn_pmd(pfn, init_prot));    else     set_pmd(pmd, pfn_pmd(pfn, prot));    pfn += PTRS_PER_PTE;    continue;   }   pte = one_page_table_init(pmd); //创建一个页表   //得到pfn在page table中的偏移并定位到具体的pte   pte_ofs = pte_index((pfn<<PAGE_SHIFT) + PAGE_OFFSET);   pte += pte_ofs;            //由pte开始遍历page table   for (; pte_ofs < PTRS_PER_PTE && pfn < end_pfn;        pte++, pfn++, pte_ofs++, addr += PAGE_SIZE) {    pgprot_t prot = PAGE_KERNEL;    /*     * first pass will use the same initial     * identity mapping attribute.     */    pgprot_t init_prot = __pgprot(PTE_IDENT_ATTR);    if (__is_kernel_text(addr)) //如果处于内核代码段，权限设为可执行     prot = PAGE_KERNEL_EXEC;    pages_4k++;                //设置pte与pfn关联    if (mapping_iter == 1) {     set_pte(pte, pfn_pte(pfn, init_prot)); //第一次执行将权限位设为init_prot     last_map_addr = (pfn << PAGE_SHIFT) + PAGE_SIZE;    } else     set_pte(pte, pfn_pte(pfn, prot)); //之后的执行将权限位置为prot   }  } } if (mapping_iter == 1) {  /*   * update direct mapping page count only in the first   * iteration.   */  update_page_count(PG_LEVEL_2M, pages_2m);  update_page_count(PG_LEVEL_4K, pages_4k);  /*   * local global flush tlb, which will flush the previous   * mappings present in both small and large page TLB's.   */  __flush_tlb_all(); //TLB全部刷新  /*   * Second iteration will set the actual desired PTE attributes.   */  mapping_iter = 2;  goto repeat; } return last_map_addr;}
```

**内核的内核页全局目录的基地址保存在 swapper_pg_dir 全局变量中**，但需要使用主内核页表时系统会把这个变量的值放入 cr3 寄存器，详细可参考/arch/x86/kernel/head_32.s。

**Linux 分别采用 pgd_t、pud_t、pmd_t 和 pte_t 四种数据结构来表示页全局目录项、页上级目录项、页中间目录项和页表项**。这四种数据结构本质上都是无符号长整型，Linux 为了更严格数据类型检查，将无符号长整型分别封装成四种不同的页表项。如果不采用这种方法，那么一个无符号长整型数据可以传入任何一个与四种页表相关的函数或宏中，这将大大降低程序的健壮性。下面仅列出 pgd_t 类型的内核源码实现，其他类型与此类似

```
typedef unsigned long   pgdval_t;typedef struct { pgdval_t pgd; } pgd_t;#define pgd_val(x)      native_pgd_val(x)static inline pgdval_t native_pgd_val(pgd_t pgd){      return pgd.pgd;}
```

这里需要区别指向页表项的指针和页表项所代表的数据，如果已知一个 pgd_t 类型的指针 pgd，那么通过 pgd_val(*pgd)即可获得该页表项(也就是一个无符号长整型数据)

**PAGE_SHIFT，PMD_SHIFT，PUD_SHIFT，PGDIR_SHIFT，对应相应的页目录所能映射的区域大小的位数**，如 PAGE_SHIFT 为 12，即页面大小为 4k。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171714575379377.png)

**PTRS_PER_PTE, PTRS_PER_PMD, PTRS_PER_PUD, PTRS_PER_PGD，对应相应页目录中的表项数**。32 位系统下，当 PAE 被禁止时，他们的值分别为 1024,，1，1 和 1024，也就是说只使用两级分页

pgd_index(addr)，pud_index,(addr)，pmd_index(addr)，pte_index(addr)，取 addr 在该目录中的索引

pud_offset(pgd,addr), pmd_offset(pud,addr), pte_offset(pmd,addr)，以 pmd_offset 为例，线性地址 addr 对应的 pmd 索引在在 pud 指定的 pmd 表的偏移地址。在两级或三级分页系统中，pmd_offset 和 pud_offset 都返回页全局目录的地址

至此，低端内存的物理地址和虚拟地址之间的映射关系已全部建立起来了

回到前面的 init_memory_mapping()函数，它的最后一个函数调用为 add_pfn_range_mapped()

```
struct range pfn_mapped[E820_MAX_ENTRIES];int nr_pfn_mapped;static void add_pfn_range_mapped(unsigned long start_pfn, unsigned long end_pfn){ nr_pfn_mapped = add_range_with_merge(pfn_mapped, E820_MAX_ENTRIES,          nr_pfn_mapped, start_pfn, end_pfn); nr_pfn_mapped = clean_sort_range(pfn_mapped, E820_MAX_ENTRIES); max_pfn_mapped = max(max_pfn_mapped, end_pfn); if (start_pfn < (1UL<<(32-PAGE_SHIFT)))  max_low_pfn_mapped = max(max_low_pfn_mapped,      min(end_pfn, 1UL<<(32-PAGE_SHIFT)));}
```

该函数主要是将前面完成内存映射的物理页框范围加入到全局数组**pfn_mapped**中，其中 nr_pfn_mapped 用于表示数组中的有效项数量，之后可以通过内核函数 pfn_range_is_mapped 来判断指定的物理内存是否被映射，避免重复映射的情况

### 2.4 **固定映射区初始化**

再回到更前面的 init_mem_mapping()函数，**early_ioremap_page_table_range_init**()用来**建立高端内存的固定映射区页表，与低端内存的页表初始化不同的是，固定映射区的页表只是被分配，相应的 PTE 项并未初始化**，这个工作交由后面的**set_fixmap**()函数将相关的固定映射区页表与物理内存进行关联

```
# define PMD_MASK (~(PMD_SIZE - 1))void __init early_ioremap_page_table_range_init(void){ pgd_t *pgd_base = swapper_pg_dir; unsigned long vaddr, end; /*  * Fixed mappings, only the page table structure has to be  * created - mappings will be set by set_fixmap():  */ vaddr = __fix_to_virt(__end_of_fixed_addresses - 1) & PMD_MASK; end = (FIXADDR_TOP + PMD_SIZE - 1) & PMD_MASK; page_table_range_init(vaddr, end, pgd_base); early_ioremap_reset();}
```

PUD_MASK、PMD_MASK、PGDIR_MASK，这些 MASK 的作用是：从给定地址中提取某些分量，用给定地址与对应的 MASK 位与操作之后即可获得各个分量，上面的操作为屏蔽低位

这里可以先具体看一下固定映射区的组成

每个固定映射区索引都以枚举类型的形式定义在 enum fixed_addresses 中

```
enum fixed_addresses {#ifdef CONFIG_X86_32 FIX_HOLE,#else#ifdef CONFIG_X86_VSYSCALL_EMULATION VSYSCALL_PAGE = (FIXADDR_TOP - VSYSCALL_ADDR) >> PAGE_SHIFT,#endif#endif FIX_DBGP_BASE, FIX_EARLYCON_MEM_BASE,#ifdef CONFIG_PROVIDE_OHCI1394_DMA_INIT FIX_OHCI1394_BASE,#endif#ifdef CONFIG_X86_LOCAL_APIC FIX_APIC_BASE, /* local (CPU) APIC) -- required for SMP or not */#endif#ifdef CONFIG_X86_IO_APIC FIX_IO_APIC_BASE_0, FIX_IO_APIC_BASE_END = FIX_IO_APIC_BASE_0 + MAX_IO_APICS - 1,#endif#ifdef CONFIG_X86_32    //这里即为固定映射区 FIX_KMAP_BEGIN, /* reserved pte's for temporary kernel mappings */ FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1,#ifdef CONFIG_PCI_MMCONFIG FIX_PCIE_MCFG,#endif#endif#ifdef CONFIG_PARAVIRT_XXL FIX_PARAVIRT_BOOTMAP,#endif#ifdef CONFIG_X86_INTEL_MID FIX_LNW_VRTC,#endif#ifdef CONFIG_ACPI_APEI_GHES /* Used for GHES mapping from assorted contexts */ FIX_APEI_GHES_IRQ, FIX_APEI_GHES_NMI,#endif __end_of_permanent_fixed_addresses, /*  * 512 temporary boot-time mappings, used by early_ioremap(),  * before ioremap() is functional.  *  * If necessary we round it up to the next 512 pages boundary so  * that we can have a single pmd entry and a single pte table:  */#define NR_FIX_BTMAPS  64#define FIX_BTMAPS_SLOTS 8#define TOTAL_FIX_BTMAPS (NR_FIX_BTMAPS * FIX_BTMAPS_SLOTS) FIX_BTMAP_END =  (__end_of_permanent_fixed_addresses ^   (__end_of_permanent_fixed_addresses + TOTAL_FIX_BTMAPS - 1)) &  -PTRS_PER_PTE  ? __end_of_permanent_fixed_addresses + TOTAL_FIX_BTMAPS -    (__end_of_permanent_fixed_addresses & (TOTAL_FIX_BTMAPS - 1))  : __end_of_permanent_fixed_addresses, FIX_BTMAP_BEGIN = FIX_BTMAP_END + TOTAL_FIX_BTMAPS - 1,#ifdef CONFIG_X86_32 FIX_WP_TEST,#endif#ifdef CONFIG_INTEL_TXT FIX_TBOOT_BASE,#endif __end_of_fixed_addresses};#define __fix_to_virt(x) (FIXADDR_TOP - ((x) << PAGE_SHIFT))
```

一个索引对应一个 4KB 的页框，**固定映射区的结束地址为 FIXADDR_TOP**，即 0xfffff000(4G-4K)，固定映射区是反向生长的，也就是说第一个索引对应的地址离 FIXADDR_TOP 最近。另外宏*_fix*to_virt(idx)可以通过索引来计算相应的固定映射区域的线性地址

```
//初始化pgd_base指向的页全局目录中start到end这个范围的线性地址，整个函数结束后只是初始化好了页中间目录项对应的页表，但是页表中的页表项并没有初始化static void __initpage_table_range_init(unsigned long start, unsigned long end, pgd_t *pgd_base){ int pgd_idx, pmd_idx; unsigned long vaddr; pgd_t *pgd; pmd_t *pmd; pte_t *pte = NULL; unsigned long count = page_table_range_init_count(start, end); void *adr = NULL; if (count)  adr = alloc_low_pages(count); vaddr = start; pgd_idx = pgd_index(vaddr); //得到vaddr对应的pgd索引 pmd_idx = pmd_index(vaddr); //得到vaddr对应的pmd索引 pgd = pgd_base + pgd_idx;   //得到pgd项 for ( ; (pgd_idx < PTRS_PER_PGD) && (vaddr != end); pgd++, pgd_idx++) {  pmd = one_md_table_init(pgd); //得到pmd起始项  pmd = pmd + pmd_index(vaddr); //得到偏移后的pmd  for (; (pmd_idx < PTRS_PER_PMD) && (vaddr != end);       pmd++, pmd_idx++) {            //建立pte表并检查vaddr是否对应内核临时映射区,若是则重新申请一个页表来保存pte表   pte = page_table_kmap_check(one_page_table_init(pmd),          pmd, vaddr, pte, &adr);   vaddr += PMD_SIZE;  }  pmd_idx = 0; }}
```

先看 page_table_range_init_count 函数

```
static unsigned long __initpage_table_range_init_count(unsigned long start, unsigned long end){ unsigned long count = 0;#ifdef CONFIG_HIGHMEM int pmd_idx_kmap_begin = fix_to_virt(FIX_KMAP_END) >> PMD_SHIFT; int pmd_idx_kmap_end = fix_to_virt(FIX_KMAP_BEGIN) >> PMD_SHIFT; int pgd_idx, pmd_idx; unsigned long vaddr; if (pmd_idx_kmap_begin == pmd_idx_kmap_end)  return 0; vaddr = start; pgd_idx = pgd_index(vaddr); pmd_idx = pmd_index(vaddr); for ( ; (pgd_idx < PTRS_PER_PGD) && (vaddr != end); pgd_idx++) {  for (; (pmd_idx < PTRS_PER_PMD) && (vaddr != end);       pmd_idx++) {   if ((vaddr >> PMD_SHIFT) >= pmd_idx_kmap_begin &&       (vaddr >> PMD_SHIFT) <= pmd_idx_kmap_end)    count++;   vaddr += PMD_SIZE;  }  pmd_idx = 0; }#endif return count;}
```

**page_table_range_init_count**()用来计算临时映射区间的页表数量。FIXADDR_START 到 FIXADDR_TOP 即整个固定映射区，就如上面所提到的里面有多个索引标识的不同功能的映射区间，而其中的一个区间**FIX_KMAP_BEGIN 到 FIX_KMAP_END 是临时映射区间**。这里再看一下两者的定义：

```
FIX_KMAP_BEGIN, /* reserved pte's for temporary kernel mappings */FIX_KMAP_END = FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1,
```

固定映射是在编译时就确定的地址空间，但是它对应的物理页可以是任意的，每一个固定地址中的项都是一页范围,这些地址的用途是固定的，这是该区间被设计的最主要的用途。临时映射区间也叫原子映射，它可以用在不能睡眠的地方，如中断处理程序中，因为临时映射获取映射时是绝对不会阻塞的，上面的 KM_TYPE_NR 可以理解为临时内核映射的最大数量，NR_CPUS 则表示 CPU 数量。

如果返回的页表数量不为 0，则 page_table_range_init()函数还会调用 alloc_low_pages()：

```
__ref void *alloc_low_pages(unsigned int num){ unsigned long pfn; int i; if (after_bootmem) {  unsigned int order;  order = get_order((unsigned long)num << PAGE_SHIFT);  return (void *)__get_free_pages(GFP_ATOMIC | __GFP_ZERO, order); } if ((pgt_buf_end + num) > pgt_buf_top || !can_use_brk_pgt) {  unsigned long ret = 0;  if (min_pfn_mapped < max_pfn_mapped) {   ret = memblock_find_in_range(     min_pfn_mapped << PAGE_SHIFT,     max_pfn_mapped << PAGE_SHIFT,     PAGE_SIZE * num , PAGE_SIZE);  }  if (ret)   memblock_reserve(ret, PAGE_SIZE * num);  else if (can_use_brk_pgt)   ret = __pa(extend_brk(PAGE_SIZE * num, PAGE_SIZE));  if (!ret)   panic("alloc_low_pages: can not alloc memory");  pfn = ret >> PAGE_SHIFT; } else {  pfn = pgt_buf_end;  pgt_buf_end += num; } for (i = 0; i < num; i++) {  void *adr;  adr = __va((pfn + i) << PAGE_SHIFT);  clear_page(adr); } return __va(pfn << PAGE_SHIFT);}
```

**alloc_low_pages**函数根据前面 early_alloc_pgt_buf()申请保留的页表缓冲空间使用情况来判断是从页表缓冲空间中申请内存还是通过 memblock 算法申请页表内存，页表缓冲空间空间足够的话就在页表缓冲空间中分配。

回到 page_table_range_init()，其中**one_md_table_init**()是用于申请新物理页作为页中间目录的，不过这里分析的是 x86 非 PAE 环境，不存在页中间目录，因此实际上返回的仍是入参，代码如下：

```
static pmd_t * __init one_md_table_init(pgd_t *pgd){ p4d_t *p4d; pud_t *pud; pmd_t *pmd_table;#ifdef CONFIG_X86_PAE if (!(pgd_val(*pgd) & _PAGE_PRESENT)) {  pmd_table = (pmd_t *)alloc_low_page();  paravirt_alloc_pmd(&init_mm, __pa(pmd_table) >> PAGE_SHIFT);  set_pgd(pgd, __pgd(__pa(pmd_table) | _PAGE_PRESENT));  p4d = p4d_offset(pgd, 0);  pud = pud_offset(p4d, 0);  BUG_ON(pmd_table != pmd_offset(pud, 0));  return pmd_table; }#endif p4d = p4d_offset(pgd, 0); pud = pud_offset(p4d, 0); pmd_table = pmd_offset(pud, 0); return pmd_table;}
```

接着的是**page_table_kmap_check**()，其入参调用的**one_page_table_init**()是用于当入参 pmd 没有页表指向时，创建页表并使其指向被创建的页表。

```
static pte_t * __init one_page_table_init(pmd_t *pmd){ if (!(pmd_val(*pmd) & _PAGE_PRESENT)) {  pte_t *page_table = (pte_t *)alloc_low_page();  paravirt_alloc_pte(&init_mm, __pa(page_table) >> PAGE_SHIFT);  set_pmd(pmd, __pmd(__pa(page_table) | _PAGE_TABLE));  BUG_ON(page_table != pte_offset_kernel(pmd, 0)); } return pte_offset_kernel(pmd, 0);}
```

page_table_kmap_check()的实现如下：

```
#define PTRS_PER_PTE 512static pte_t *__init page_table_kmap_check(pte_t *pte, pmd_t *pmd,        unsigned long vaddr, pte_t *lastpte,        void **adr){#ifdef CONFIG_HIGHMEM /*  * Something (early fixmap) may already have put a pte  * page here, which causes the page table allocation  * to become nonlinear. Attempt to fix it, and if it  * is still nonlinear then we have to bug.  */    //得到内核固定映射区的临时映射区的起始和结束虚拟页框号 int pmd_idx_kmap_begin = fix_to_virt(FIX_KMAP_END) >> PMD_SHIFT; int pmd_idx_kmap_end = fix_to_virt(FIX_KMAP_BEGIN) >> PMD_SHIFT; if (pmd_idx_kmap_begin != pmd_idx_kmap_end     && (vaddr >> PMD_SHIFT) >= pmd_idx_kmap_begin     && (vaddr >> PMD_SHIFT) <= pmd_idx_kmap_end) {  pte_t *newpte;  int i;  BUG_ON(after_bootmem);  newpte = *adr;        //拷贝操作  for (i = 0; i < PTRS_PER_PTE; i++)   set_pte(newpte + i, pte[i]);  *adr = (void *)(((unsigned long)(*adr)) + PAGE_SIZE);  paravirt_alloc_pte(&init_mm, __pa(newpte) >> PAGE_SHIFT);  set_pmd(pmd, __pmd(__pa(newpte)|_PAGE_TABLE)); //pmd与newpte表进行关联  BUG_ON(newpte != pte_offset_kernel(pmd, 0));  __flush_tlb_all();  paravirt_release_pte(__pa(pte) >> PAGE_SHIFT);  pte = newpte; } BUG_ON(vaddr < fix_to_virt(FIX_KMAP_BEGIN - 1)        && vaddr > fix_to_virt(FIX_KMAP_END)        && lastpte && lastpte + PTRS_PER_PTE != pte);#endif return pte;}
```

检查当前页表初始化的地址是否处于该区间范围，如果是，则把其 pte 页表的内容拷贝到 page_table_range_init()的 alloc_low_pages 申请的页表空间中（这里的拷贝主要是为了保证连续性），并将 newpte 新页表的地址设置到 pmd 中（32bit 系统实际上就是页全局目录），然后调用*_flush*tlb_all()刷新 TLB 缓存；如果不是该区间，则仅是由入参中调用的 one_page_table_init()被分配到了页表空间。

至此高端内存的固定映射区的页表分配完成，后面的 paging_init()会负责完成剩下的页表建立工作。

## 3.Linux 内存管理框架

传统的多核运算是使用 SMP(Symmetric Multi-Processor )模式：将多个处理器与一个集中的存储器和 I/O 总线相连，所有处理器访问同一个物理存储器，因此 SMP 系统有时也被称为一致存储器访问（**UMA**）结构体系，即无论在什么时候，处理器只能为内存的每个数据保持或共享唯一一个数值。

而**NUMA**模式是一种分布式存储器访问方式，处理器可以同时访问不同的存储器地址，大幅度提高并行性。NUMA 模式下系统的每个 CPU 都有本地内存，可支持快速访问，各个处理器之间通过总线连接起来，以支持对其它 CPU 本地内存的访问，但是这些访问要比处理器本地内存的慢.

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171715275451885.png)

Linux 内核通过插入一些兼容层，使两个不同体系结构的差异被隐藏，两种模式都使用了同一个数据结构。

在 NUMA 模式下，处理器和内存块被划分成多个”节点”(node)，比如机器上有 2 个处理器、4 个内存块，我们可以把 1 个处理器和 2 个内存块合起来（GNU/Linux 根据物理 CPU 的数量分配 node，一个物理 CPU 对应一个 node），共同组成一个 NUMA 的节点。每个节点被分配有本地存储器空间，所有节点中的处理器都可以访问系统全部的物理存储器，但是访问本节点内的存储器所需要的时间比访问某些远程节点内的存储器所花的时间要少得多。

与 CPU 类似，内存被分割成多个区域 BANK，也叫”簇”，依据簇与 CPU 的”距离”的不同，访问不同簇的方式也会有所不同，CPU 被划分为多个节点，每个 CPU 对应一个本地物理内存, 一般一个 CPU-node 对应一个内存簇，也就是说每个内存簇被认为是一个节点。

而在 UMA 系统中, 内存就相当于一个只使用一个 NUMA 节点来管理整个系统的内存，这样在内存管理的其它地方可以认为他们就是在处理一个(伪)NUMA 系统。

### **3.1 内存管理框架初始化**

在 linux 中，每个物理内存节点 node 都被划分为多个内存管理区域用于表示不同范围的内存，比如上面提到的 NORMAL 内存、高端内存，内核可以使用不同的映射方式映射物理内存。zone 只是内核为了管理方便而做的一种逻辑上的划分，并不存在这种物理硬件单元。

综上，**linux 的物理内存管理机制将物理内存划分为三个层次来管理**，依次是：Node（存储节点）、Zone（管理区）和 Page（页面），它们之间的关系如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171715407686398.png)

其中，zone 的类型如下：

```
include/linux/mmzone.henum zone_type {#ifdef CONFIG_ZONE_DMA ZONE_DMA, //通常为内存首部16MB,某些工业标准体系结构设备需要用到ZONE_DMA(较旧的外设，只能寻址24位内存)#endif#ifdef CONFIG_ZONE_DMA32 ZONE_DMA32, //在64位Linux操作系统上,DMA32为16M~4G（可访问32位），高于4G的内存为Normal ZONE#endif ZONE_NORMAL, //通常为16MB~896MB，该部分的内存由内核直接映射到物理地址空间的较高部分#ifdef CONFIG_HIGHMEM ZONE_HIGHMEM, //通常为896MB~末尾，将保留给系统使用，是系统中预留的可用内存空间，动态映射#endif ZONE_MOVABLE, //用于减少内存的碎片化，这个区域的页都是可迁移的#ifdef CONFIG_ZONE_DEVICE ZONE_DEVICE, //为支持热插拔设备而分配的非易失性内存#endif __MAX_NR_ZONES};
```

回到之前的 setup_arch()函数，接着往下走，来到内存管理框架初始化的地方

```
void __init setup_arch(char **cmdline_p){    ...... max_pfn = e820__end_of_ram_pfn(); //max_pfn初始化 ...... find_low_pfn_range(); //max_low_pfn、高端内存初始化 ...... ...... early_alloc_pgt_buf(); //页表缓冲区分配 reserve_brk(); //缓冲区加入memblock.reserve    ......    e820__memblock_setup(); //memblock.memory空间初始化 启动 ......    init_mem_mapping(); //低端内存内核页表初始化 高端内存固定映射区中临时映射区页表初始化  ......    initmem_init(); // <----------------------------------------    ......}
```

**initmem_init**的实现如下：

```
#define PHYS_ADDR_MAX (~(phys_addr_t)0)#ifndef CONFIG_NEED_MULTIPLE_NODESvoid __init initmem_init(void){#ifdef CONFIG_HIGHMEM highstart_pfn = highend_pfn = max_pfn; if (max_pfn > max_low_pfn)  highstart_pfn = max_low_pfn; printk(KERN_NOTICE "%ldMB HIGHMEM available.\n",  pages_to_mb(highend_pfn - highstart_pfn)); high_memory = (void *) __va(highstart_pfn * PAGE_SIZE - 1) + 1;#else high_memory = (void *) __va(max_low_pfn * PAGE_SIZE - 1) + 1;#endif memblock_set_node(0, PHYS_ADDR_MAX, &memblock.memory, 0);#ifdef CONFIG_FLATMEM max_mapnr = IS_ENABLED(CONFIG_HIGHMEM) ? highend_pfn : max_low_pfn;#endif __vmalloc_start_set = true; printk(KERN_NOTICE "%ldMB LOWMEM available.\n",   pages_to_mb(max_low_pfn)); setup_bootmem_allocator();}#endif /* !CONFIG_NEED_MULTIPLE_NODES */
```

这个函数将**high_memory**初始化为低端内存最大页框**max_low_pfn**对应的地址大小，接着调用 memblock_set_node，通过 memblock 内存管理器**设置 node 节点信息**。

```
int __init_memblock memblock_set_node(phys_addr_t base, phys_addr_t size,          struct memblock_type *type, int nid){#ifdef CONFIG_NEED_MULTIPLE_NODES int start_rgn, end_rgn; int i, ret; ret = memblock_isolate_range(type, base, size, &start_rgn, &end_rgn); if (ret)  return ret; for (i = start_rgn; i < end_rgn; i++)  memblock_set_region_node(&type->regions[i], nid); memblock_merge_regions(type);#endif return 0;}
```

memblock_set_node 主要调用了三个函数：memblock_isolate_range、memblock_set_region_node 和 memblock_merge_regions，首先看**memblock_isolate_range**()函数：

```
/* adjust *@size so that (@base + *@size) doesn't overflow, return new size */static inline phys_addr_t memblock_cap_size(phys_addr_t base, phys_addr_t *size){ return *size = min(*size, PHYS_ADDR_MAX - base);}static int __init_memblock memblock_isolate_range(struct memblock_type *type,     phys_addr_t base, phys_addr_t size,     int *start_rgn, int *end_rgn){ phys_addr_t end = base + memblock_cap_size(base, &size); int idx; struct memblock_region *rgn; *start_rgn = *end_rgn = 0; if (!size)  return 0; /* we'll create at most two more regions */ while (type->cnt + 2 > type->max)  if (memblock_double_array(type, base, size) < 0)   return -ENOMEM; for_each_memblock_type(idx, type, rgn) {  phys_addr_t rbase = rgn->base;  phys_addr_t rend = rbase + rgn->size;  if (rbase >= end)   break;  if (rend <= base)   continue;  if (rbase < base) {   /*    * @rgn intersects from below.  Split and continue    * to process the next region - the new top half.    */   rgn->base = base;   rgn->size -= base - rbase;   type->total_size -= base - rbase;   memblock_insert_region(type, idx, rbase, base - rbase,            memblock_get_region_node(rgn),            rgn->flags);  } else if (rend > end) {   /*    * @rgn intersects from above.  Split and redo the    * current region - the new bottom half.    */   rgn->base = end;   rgn->size -= end - rbase;   type->total_size -= end - rbase;   memblock_insert_region(type, idx--, rbase, end - rbase,            memblock_get_region_node(rgn),            rgn->flags);  } else {   /* @rgn is fully contained, record it */   if (!*end_rgn)    *start_rgn = idx;   *end_rgn = idx + 1;  } } return 0;}
```

在*_memblock*remove()中有提到，**memblock_isolate_range()主要作用是将要移除的物理内存区从 reserved 内存区中分离出来**，将 start_rgn 和 end_rgn（该内存区块的起始、结束索引号）返回回去，而这里，由于我们传入的**type 是 memblock.memory**，该函数会根据入参 base 和 size 标记节点内存范围，将**该内存从 memory 中划分开来，同时返回对应的 start_rgn 和 end_rgn。**

1）如果 memblock 中的 region 恰好以在该节点内存范围内的话，那么再未赋值 end_rgn 时将当前 region 的索引记录至 start_rgn，end_rgn 在此基础上加 1；

2）如果 memblock 中的 region 跨越了该节点内存末尾分界，那么将会把当前的 region 边界调整为 node 节点内存范围边界，然后通过 memblock_insert_region()函数将剩下的部分（即越出内存范围的那一块内存）重新插入 memblock 管理 regions 当中，实现拆分；

```
static inline void memblock_set_region_node(struct memblock_region *r, int nid){ r->nid = nid;}static inline int memblock_get_region_node(const struct memblock_region *r){ return r->nid;}static void __init_memblock memblock_insert_region(struct memblock_type *type,         int idx, phys_addr_t base,         phys_addr_t size,         int nid, unsigned long flags){ struct memblock_region *rgn = &type->regions[idx]; BUG_ON(type->cnt >= type->max); memmove(rgn + 1, rgn, (type->cnt - idx) * sizeof(*rgn)); rgn->base = base; rgn->size = size; rgn->flags = flags; memblock_set_region_node(rgn, nid); type->cnt++; type->total_size += size;}
```

上面的 memmove()将后面的 region 信息往后移，另外调用 memblock_set_region_node()将原 region 的 node 节点号保留在被拆分出来的 region 当中。

回到前面的 memblock_set_node()函数，紧接着 memblock_isolate_range()被调用的是**memblock_set_region_node**()，通过这个函数**把划分出来的 region 进行 node 节点号设置**，而后面的**memblock_merge_regions**()前面 MEMBLOCK 内存分配器初始化时已经分析过了，**是用于将相邻的 region 进行合并的**（节点号、flag 等一致才会合并）。

最后回到 initmem_init()函数中，memblock_set_node()返回后，接着调用的函数为 setup_bootmem_allocator()。

```
void __init setup_bootmem_allocator(void){ printk(KERN_INFO "  mapped low ram: 0 - %08lx\n",   max_pfn_mapped<<PAGE_SHIFT); printk(KERN_INFO "  low ram: 0 - %08lx\n", max_low_pfn<<PAGE_SHIFT);}
```

原来该函数是用来初始化 bootmem 管理算法的，但现在 x86 的环境已经使用了 memblock 管理算法，所以这里仅作保留，打印部分信息。

**bootmem 分配器使用一个 bitmap 来标记物理页是否被占用**，分配的时候按照第一适应的原则，从 bitmap 中进行查找，如果这位为 1，表示已经被占用，否则表示未被占用。bootmem 分配器每次分配内存都会在 bitmap 中进行线性搜索，效率非常低，而且容易在内存中留下许多小的空闲碎片，在需要非常大的内存块的时候，检查位图这一过程就显得代价很高。bootmem 分配器是用于在启动阶段分配内存的，对该分配器的需求集中于简单性方面，而不是性能和通用性（和 memblock 管理器一致）。

至此，已完成对内存的节点 node 设置。

回到 setup_arch()函数：

```
void __init setup_arch(char **cmdline_p){    ...... max_pfn = e820__end_of_raCm_pfn(); //max_pfn初始化 ...... find_low_pfn_range(); //max_low_pfn、高端内存初始化 ...... ...... early_alloc_pgt_buf(); //页表缓冲区分配 reserve_brk(); //缓冲区加入memblock.reserve    ......    e820__memblock_setup(); //memblock.memory空间初始化 启动 ......    init_mem_mapping(); //低端内存内核页表初始化 高端内存固定映射区中临时映射区页表初始化  ......    initmem_init(); //high_memory（高端内存起始pfn）初始化 通过memblock内存管理器设置node节点信息    ......    x86_init.paging.pagetable_init(); // <-----------------------------    ......}
```

x86_init 结构体内**pagetable_init**实际上挂接的是**native_pagetable_init**()函数：

```
struct x86_init_ops x86_init __initdata = { ...... .paging = {  .pagetable_init  = native_pagetable_init, }, ......}
```

native_pagetable_init()函数内容如下：

```
void __init native_pagetable_init(void){ unsigned long pfn, va; pgd_t *pgd, *base = swapper_pg_dir; p4d_t *p4d; pud_t *pud; pmd_t *pmd; pte_t *pte;    //循环 低端内存最大物理页号~最大物理页号 for (pfn = max_low_pfn; pfn < 1<<(32-PAGE_SHIFT); pfn++) {  va = PAGE_OFFSET + (pfn<<PAGE_SHIFT);  pgd = base + pgd_index(va);  if (!pgd_present(*pgd))   break;  p4d = p4d_offset(pgd, va);  pud = pud_offset(p4d, va);  pmd = pmd_offset(pud, va);  if (!pmd_present(*pmd))   break;  /* should not be large page here */  if (pmd_large(*pmd)) {   pr_warn("try to clear pte for ram above max_low_pfn: pfn: %lx pmd: %p pmd phys: %lx, but pmd is big page and is not using pte !\n",    pfn, pmd, __pa(pmd));   BUG_ON(1);  }  pte = pte_offset_kernel(pmd, va);  if (!pte_present(*pte))   break;  printk(KERN_DEBUG "clearing pte for ram above max_low_pfn: pfn: %lx pmd: %p pmd phys: %lx pte: %p pte phys: %lx\n",    pfn, pmd, __pa(pmd), pte, __pa(pte));  pte_clear(NULL, va, pte); } paravirt_alloc_pmd(&init_mm, __pa(base) >> PAGE_SHIFT); paging_init();}
```

**PAGE_OFFSET 代表的是内核空间和用户空间对虚拟地址空间的划分，对不同的体系结构不同。比如在 32 位系统中 3G-4G 属于内核使用的内存空间，所以 PAGE_OFFSET = 0xC0000000**。

该函数的 for 循环主要是用于检测 max_low_pfn 后面的内核空间内存直接映射空间后面的物理内存**是否存在系统启动引导时创建的页表**（pfn 通过直接映射的方法得到虚拟地址，然后通过内核页表得到 pgd、pmd、pte），如果存在，则使用 pte_clear()将其清除。

接着看 native_pagetable_init()调用的最后一个函数：**paging_init**()。

```
void __init paging_init(void){ pagetable_init(); __flush_tlb_all(); kmap_init(); /*  * NOTE: at this point the bootmem allocator is fully available.  */ olpc_dt_build_devicetree(); sparse_init(); zone_sizes_init();}
```

可以对着这个图看：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171716098339686.png)

前面已经分析过低端内存、固定映射区中临时映射区的内核页表的建立，这里 paging_init 将会完成剩下的工作，首先看**pagetable_init**()

```
static void __init pagetable_init(void){ pgd_t *pgd_base = swapper_pg_dir; permanent_kmaps_init(pgd_base);}static void __init permanent_kmaps_init(pgd_t *pgd_base){ unsigned long vaddr = PKMAP_BASE; page_table_range_init(vaddr, vaddr + PAGE_SIZE*LAST_PKMAP, pgd_base); pkmap_page_table = virt_to_kpte(vaddr);}
```

**该函数为建立持久映射区（KMAP 区）的页表**，page_table_range_init 函数前面固定映射区页表初始化时已经分析过了（初始化 pgd_base 指向的页全局目录中 start 到 end 这个范围的线性地址，整个函数结束后只是初始化好了页中间目录项对应的页表，但是页表中的页表项并没有初始化），这里建立页表范围为 PKMAP_BASE 到 PKMAP_BASE + PAGE_SIZE*LAST_PKMAP，**建好页表后将页表地址赋值给给持久映射区页表变量 pkmap_page_table**。

*_flush*tlb_all()为刷新全部 TLB，这里不做介绍，接着看 paging_init()调用的下一个函数 kmap_init()。

```
static void __init kmap_init(void){ unsigned long kmap_vstart; /*  * Cache the first kmap pte:  */ kmap_vstart = __fix_to_virt(FIX_KMAP_BEGIN); kmap_pte = virt_to_kpte(kmap_vstart);}
```

可以很容易看到 kmap_init()主要是**获取到临时映射区间的起始页表并往临时映射页表变量 kmap_pte 置值**。

回到 paging_init()，olpc_dt_build_devicetree 这里就不做介绍了，而**sparse_init**()则涉及到了 Linux 的内存模型，这里介绍一下 Linux 的三种内存模型，注意，以下都是从 CPU 的角度来看的。

从系统中任意一个 CPU 的角度来看，当它访问物理内存的时候，物理地址空间是一个**连续的、没有空洞**的地址空间，那么这种计算机系统的内存模型就是**Flat memory**。在这种内存模型下，物理内存的管理比较简单，每一个物理页帧都会有一个 page 数据结构来抽象，因此系统中存在一个 struct page 的数组（位于直接映射区，mem_map，在节点 node 里，后面就会介绍到），每一个数组条目指向一个实际的物理页帧（page frame）。在 flat memory 的情况下，PFN 和 mem_map 数组 index 的关系是线性的（即位于直接映射区，有一个固定偏移），因此从 PFN 到对应的 page 数据结构是非常容易的，反之亦然。

pfn_to_page/page_to_pfn 的作用是 struct page* 和 pfn 页帧号之间的转换，flat memory 内存模型的相关代码如下：

```
#if defined(CONFIG_FLATMEM)#define __pfn_to_page(pfn) (mem_map + ((pfn) - ARCH_PFN_OFFSET))#define __page_to_pfn(page) ((unsigned long)((page) - mem_map) + \     ARCH_PFN_OFFSET)
```

**PFN 和 struct page 数组（mem_map）的 index 是线性关系，有一个固定的偏移就是 ARCH_PFN_OFFSET（跟架构相关的物理起始地址的 PFN）**。

如果 cpu 在访问物理内存的时候，**其地址空间有一些空洞，是不连续的**，那么这种计算机系统的内存模型就是**Discontiguous memory**。一般而言，NUMA 架构的计算机系统的 memory model 都是选择 Discontiguous Memory，不过 NUMA 强调的是 memory 和 processor 的位置关系，和内存模型其实是没有关系的（NUMA 并没有规定其内存的连续性，而 Discontiguous memory 系统也并非一定是 NUMA 系统，但是这两种都是多节点的），只不过，由于同一 node 上的 memory 和 processor 有更紧密的耦合关系（访问更快），因此需要多个 node 来管理。**Discontiguous memory 本质上是 flat memory 内存模型的扩展，整个物理内存的内存空间大部分是成片的大块内存，中间会有一些空洞，每一个成片的内存地址空间属于一个 node（如果局限在一个 node 内部，其内存模型是 flat memory）**。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171716257917012.png)

在这种模型下，从 PFN 转换到具体的 struct page 会稍微复杂一点，首先要从 PFN 得到 node ID，然后根据这个 ID 找到对于的节点 node 数据结构，也就找到了对应的 page 数组，之后的方法就类似 flat memory 了。

```
#elif defined(CONFIG_DISCONTIGMEM)#define __pfn_to_page(pfn)   \({ unsigned long __pfn = (pfn);  \ unsigned long __nid = arch_pfn_to_nid(__pfn);  \ NODE_DATA(__nid)->node_mem_map + arch_local_page_offset(__pfn, __nid);\})#define __page_to_pfn(pg)      \({ const struct page *__pg = (pg);     \ struct pglist_data *__pgdat = NODE_DATA(page_to_nid(__pg)); \ (unsigned long)(__pg - __pgdat->node_mem_map) +   \  __pgdat->node_start_pfn;     \})
```

Discontiguous memory 模型需要获取 node id，只要找到 node id，一切都好办了，后面类比 flat memory model 进行就 OK 了。对于**pfn_to_page 的定义，首先通过 arch_pfn_to_nid 将 PFN 转换成 node id，通过 NODE_DATA 宏定义可以找到该 node 对应的 pglist_data 数据结构，该数据结构的 node_start_pfn 记录了该 node 的第一个 pfn，因此，也就可以得到其对应 struct page 在 node_mem_map 的偏移，**page_to_pfn 类似与上述基本类似(pglist_data 数据结构后面再进行介绍)。

内存模型也是一个演进过程，刚开始的时候，使用 flat memory 去抽象一个连续的内存地址空间，出现 NUMA 之后，整个不连续的内存空间被分成若干个 node，每个 node 上是连续的内存地址空间，也就是从原来单一的一个 mem_maps[]变成了若干个 mem_maps[]。而现在 memory hotplug 的出现让原来完美的设计变得不完美了（热插拔，即带电插拔，热插拔功能就是允许用户在不关闭系统，不切断电源的情况下取出和更换损坏的硬盘、电源或板卡等部件。Linux 内核支持热插拔的部件有 USB 设备、PCI 设备甚至 CPU），因为即便是一个 node 中的 mem_maps[]也有可能是不连续了，因此目前 Discontiguous memory 也逐渐在被 sparse memory 替代。

在**sparse memory**内存模型下，连续的地址空间按照 SECTION（例如 1G）被分成了一段一段的，其中每一 section 都是 hotplug 的，**因此 sparse memory 下，内存地址空间可以被切分的更细**。整个连续的物理地址空间是按照一个个**section**来切断的，在每一个 section 内部，其 memory 是连续的（即符合 flat memory 的特点），因此，**mem_map 的 page 数组依附于 section 结构（struct mem_section），而不是 node 结构（struct pglist_data）。在这个模型下，PFN 转 struct page 变为了转换变成了 PFN<—>Section<—>page。**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171716353283542.png)

linux 内核中静态定义了一个 mem_section 的指针数组，一个 section 中往往包括多个 page，因此需要通过 PFN 得到 section 号，用 section 号做为 index 在 mem_section 指针数组可以找到该 PFN 对应的 section 数据结构。实际上**PFN 分成两个部分：一部分是 section index，另外一个部分是 page 在该 section 的偏移**，找到 section 之后，沿着其 mem_map 就可以找到对应的 page 数据结构。

**对于 page 到 section index 的转换，sparse memory 有 2 种方案**，先看看经典的方案，也就是把 section 号保存在 page->flags 中（page 的结构同样在后面再进行介绍），这种方法的最大的问题是 page->flags 中的 bit 数不一定够用，因为这个 flag 中承载了太多的信息，各种 page flag、node id、zone id，现在又增加一个 section id，在不同的处理器架构中无法实现一致性的算法（上面的图即为采用经典算法的 sparse memory）。

```
#elif defined(CONFIG_SPARSEMEM)/* * Note: section's mem_map is encoded to reflect its start_pfn. * section[i].section_mem_map == mem_map's address - start_pfn; */#define __page_to_pfn(pg)     \({ const struct page *__pg = (pg);    \ int __sec = page_to_section(__pg);   \ (unsigned long)(__pg - __section_mem_map_addr(__nr_to_section(__sec))); \})#define __pfn_to_page(pfn)    \({ unsigned long __pfn = (pfn);   \ struct mem_section *__sec = __pfn_to_section(__pfn); \ __section_mem_map_addr(__sec) + __pfn;  \})#endif /* CONFIG_FLATMEM/DISCONTIGMEM/SPARSEMEM */
```

对于经典的 sparse memory 模型，一个 section 的 struct page 数组所占用的内存来自直接映射区，页表在初始化的时候就建立好了。但是，对于 SPARSEMEM_VMEMMAP 而言，虚拟地址一开始就分配好了，是 vmemmap 开始的一段连续的虚拟地址空间，但是没有物理地址。因此，当一个 section 被发现后，可以立刻找到对应的 struct page 的虚拟地址，而这时候还需要分配一个物理的 page frame，对于这种 sparse memory，开销会稍微大一些，需要建立页表，关联 page 跟物理地址。

```
#elif defined(CONFIG_SPARSEMEM_VMEMMAP)/* memmap is virtually contiguous.  */#define __pfn_to_page(pfn) (vmemmap + (pfn))#define __page_to_pfn(page) (unsigned long)((page) - vmemmap)
```

### 3.2 **内存管理区 Zone 初始化**

回到 paging_init()的最后一个函数调用：**zone_sizes_init**()。

```
void __init zone_sizes_init(void){ unsigned long max_zone_pfns[MAX_NR_ZONES]; memset(max_zone_pfns, 0, sizeof(max_zone_pfns));#ifdef CONFIG_ZONE_DMA max_zone_pfns[ZONE_DMA]  = min(MAX_DMA_PFN, max_low_pfn);#endif#ifdef CONFIG_ZONE_DMA32 max_zone_pfns[ZONE_DMA32] = min(MAX_DMA32_PFN, max_low_pfn);#endif max_zone_pfns[ZONE_NORMAL] = max_low_pfn;#ifdef CONFIG_HIGHMEM max_zone_pfns[ZONE_HIGHMEM] = max_pfn;#endif free_area_init(max_zone_pfns);}
```

这个函数**为 zone 的各个内存管理区域最大物理页号进行初始化**，并作为参数调用 free_area_init_nodes()，其中**free_area_init_nodes**()函数实现如下：

```
void __init free_area_init(unsigned long *max_zone_pfn){ unsigned long start_pfn, end_pfn; int i, nid, zone; bool descending; /* Record where the zone boundaries are */    //全局数组arch_zone_lowest_possible_pfn用来存储各个内存域可使用的最低内存页帧编号    //全局数组arch_zone_highest_possible_pfn用来存储各个内存域可使用的最高内存页帧编号 memset(arch_zone_lowest_possible_pfn, 0,    sizeof(arch_zone_lowest_possible_pfn)); memset(arch_zone_highest_possible_pfn, 0,    sizeof(arch_zone_highest_possible_pfn));    //用于最低内存区域中可用的编号最小的页帧，即memblock.memory.regions[0].base start_pfn = find_min_pfn_with_active_regions();    //return false descending = arch_has_descending_max_zone_pfns();    //根据max_zone_pfn和start_pfn初始化arch_zone_lowest_possible_pfn和arch_zone_highest_possible_pfn for (i = 0; i < MAX_NR_ZONES; i++) {  if (descending)   zone = MAX_NR_ZONES - i - 1;  else   zone = i;  //由于ZONE_MOVABLE是一个虚拟内存域，不与真正的硬件内存域关联，该内存域的边界总是设置为0  if (zone == ZONE_MOVABLE)   continue;  end_pfn = max(max_zone_pfn[zone], start_pfn);  arch_zone_lowest_possible_pfn[zone] = start_pfn;  arch_zone_highest_possible_pfn[zone] = end_pfn;  start_pfn = end_pfn; } /* Find the PFNs that ZONE_MOVABLE begins at in each node */ memset(zone_movable_pfn, 0, sizeof(zone_movable_pfn));    //用于计算ZONE_MOVABLE的内存数量    //在内存较多的32位系统上, 这通常会是ZONE_HIGHMEM, 但是对于64位系统，将使用ZONE_NORMAL或ZONE_DMA32，这里计算也比较复杂，感兴趣的话可以去看一下源码，这里便不做介绍了 find_zone_movable_pfns_for_nodes(); ......    //各种打印    ...... /* Initialise every node */    //为系统中的每个节点调用free_area_init_node() mminit_verify_pageflags_layout(); setup_nr_node_ids(); init_unavailable_mem(); for_each_online_node(nid) {  pg_data_t *pgdat = NODE_DATA(nid);  free_area_init_node(nid);  /* Any memory on that node */  if (pgdat->node_present_pages)   node_set_state(nid, N_MEMORY);  check_for_memory(pgdat, nid); }}
```

**free_area_init_node()函数会计算每个节点中每个区域的大小及其空洞的大小**，如果两个相邻区域之间的最大 PFN 匹配，则可以假定后面的区域为空。例如 arch_max_dma_pfn == arch_max_dma32_pfn，则假定 arch_max_dma32_pfn 该区域为空。

```
pg_data_t node_data[MAX_NUMNODES];EXPORT_SYMBOL(node_data);#ifdef CONFIG_NUMAextern struct pglist_data *node_data[];#define NODE_DATA(nid) (node_data[nid])#endif /* CONFIG_NUMA */static void __init free_area_init_node(int nid){    //从全局节点数组中获取一个节点 pg_data_t *pgdat = NODE_DATA(nid); unsigned long start_pfn = 0; unsigned long end_pfn = 0; /* pg_data_t should be reset to zero when it's allocated */ WARN_ON(pgdat->nr_zones || pgdat->kswapd_highest_zoneidx); //根据节点id获取起始pfn和结束pfn，前面node初始化时，memblock处已经设置好节点ID了 get_pfn_range_for_nid(nid, &start_pfn, &end_pfn); //设置节点ID以及起始pfn pgdat->node_id = nid; pgdat->node_start_pfn = start_pfn; pgdat->per_cpu_nodestats = NULL; pr_info("Initmem setup node %d [mem %#018Lx-%#018Lx]\n", nid,  (u64)start_pfn << PAGE_SHIFT,  end_pfn ? ((u64)end_pfn << PAGE_SHIFT) - 1 : 0); //初始化节点中每个zone的大小和空洞，同时计算节点的spanned_pages和present_pages    calculate_node_totalpages(pgdat, start_pfn, end_pfn); alloc_node_mem_map(pgdat); pgdat_set_deferred_range(pgdat); free_area_init_core(pgdat);}
```

在进入 calculate_node_totalpages 之前，这里还是先简单介绍一下**node**的数据结构。

```
struct zoneref { struct zone *zone; /* Pointer to actual zone */ int zone_idx;  /* zone_idx(zoneref->zone) */};struct zonelist { struct zoneref _zonerefs[MAX_ZONES_PER_ZONELIST + 1];};typedef struct pglist_data { //本节点的所有zone内存管理区 struct zone node_zones[MAX_NR_ZONES];    //包含了对所有node的zone的引用，通常第一个node_zonelists为本节点自己的zones struct zonelist node_zonelists[MAX_ZONELISTS]; //本节点zone管理区数目 int nr_zones; /* number of populated zones in this node */#ifdef CONFIG_FLAT_NODE_MEM_MAP /* means !SPARSEMEM */    //Discontiguous Memory内存模型，可以找到节点下的所有page struct page *node_mem_map;#ifdef CONFIG_PAGE_EXTENSION struct page_ext *node_page_ext;#endif#endif    ......    //节点第一个页的页号 unsigned long node_start_pfn;    //节点总共的物理页数，不含空洞 unsigned long node_present_pages; /* total number of physical pages */    //节点物理页的范围大小，含空洞 unsigned long node_spanned_pages; /* total size of physical page          range, including holes */ int node_id;    ......    //内存回收相关的数据结构 ...... ZONE_PADDING(_pad1_)    ...... unsigned long  flags; ZONE_PADDING(_pad2_)    ......} pg_data_t;
```

ZONE_PADDING 的作用，通过添加大量的填充把经常被访问的“热门”数据分到了单独的 cache line 上，可以理解为空间换时间。

**calculate_node_totalpages**()实现：

```
static void __init calculate_node_totalpages(struct pglist_data *pgdat,      unsigned long node_start_pfn,      unsigned long node_end_pfn){ unsigned long realtotalpages = 0, totalpages = 0; enum zone_type i; for (i = 0; i < MAX_NR_ZONES; i++) {  struct zone *zone = pgdat->node_zones + i;  unsigned long zone_start_pfn, zone_end_pfn;  unsigned long spanned, absent;  unsigned long size, real_size;  spanned = zone_spanned_pages_in_node(pgdat->node_id, i,           node_start_pfn,           node_end_pfn,           &zone_start_pfn,           &zone_end_pfn);  absent = zone_absent_pages_in_node(pgdat->node_id, i,         node_start_pfn,         node_end_pfn);  size = spanned;  real_size = size - absent;  if (size)   zone->zone_start_pfn = zone_start_pfn;  else   zone->zone_start_pfn = 0;  zone->spanned_pages = size;  zone->present_pages = real_size;  totalpages += size;  realtotalpages += real_size; } pgdat->node_spanned_pages = totalpages; pgdat->node_present_pages = realtotalpages; printk(KERN_DEBUG "On node %d totalpages: %lu\n", pgdat->node_id,       realtotalpages);}
```

其中**zone_spanned_pages_in_node**()：

```
static unsigned long __init zone_spanned_pages_in_node(int nid,     unsigned long zone_type,     unsigned long node_start_pfn,     unsigned long node_end_pfn,     unsigned long *zone_start_pfn,     unsigned long *zone_end_pfn){ unsigned long zone_low = arch_zone_lowest_possible_pfn[zone_type]; unsigned long zone_high = arch_zone_highest_possible_pfn[zone_type]; /* When hotadd a new node from cpu_up(), the node should be empty */ if (!node_start_pfn && !node_end_pfn)  return 0; /* Get the start and end of the zone */    //限制zone_start_pfn和zone_end_pfn所在区间 *zone_start_pfn = clamp(node_start_pfn, zone_low, zone_high); *zone_end_pfn = clamp(node_end_pfn, zone_low, zone_high); adjust_zone_range_for_zone_movable(nid, zone_type,    node_start_pfn, node_end_pfn,    zone_start_pfn, zone_end_pfn); /* Check that this node has pages within the zone's required range */ if (*zone_end_pfn < node_start_pfn || *zone_start_pfn > node_end_pfn)  return 0; /* Move the zone boundaries inside the node if necessary */ *zone_end_pfn = min(*zone_end_pfn, node_end_pfn); *zone_start_pfn = max(*zone_start_pfn, node_start_pfn); /* Return the spanned pages */ return *zone_end_pfn - *zone_start_pfn;}
```

该函数**主要是统计 node 管理节点的内存跨度**，该跨度不包括 movable 管理区的（因为 movable 就是在其它内存管理区里分配出来的），里面调用的 adjust_zone_range_for_zone_movable()则是用于剔除 movable 管理区的部分。

另外的**zone_absent_pages_in_node**()函数：

```
unsigned long __init __absent_pages_in_range(int nid,    unsigned long range_start_pfn,    unsigned long range_end_pfn){ unsigned long nr_absent = range_end_pfn - range_start_pfn; unsigned long start_pfn, end_pfn; int i; for_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {  start_pfn = clamp(start_pfn, range_start_pfn, range_end_pfn);  end_pfn = clamp(end_pfn, range_start_pfn, range_end_pfn);  nr_absent -= end_pfn - start_pfn; } return nr_absent;}static unsigned long __init zone_absent_pages_in_node(int nid,     unsigned long zone_type,     unsigned long node_start_pfn,     unsigned long node_end_pfn){ unsigned long zone_low = arch_zone_lowest_possible_pfn[zone_type]; unsigned long zone_high = arch_zone_highest_possible_pfn[zone_type]; unsigned long zone_start_pfn, zone_end_pfn; unsigned long nr_absent; /* When hotadd a new node from cpu_up(), the node should be empty */ if (!node_start_pfn && !node_end_pfn)  return 0; zone_start_pfn = clamp(node_start_pfn, zone_low, zone_high); zone_end_pfn = clamp(node_end_pfn, zone_low, zone_high); adjust_zone_range_for_zone_movable(nid, zone_type,   node_start_pfn, node_end_pfn,   &zone_start_pfn, &zone_end_pfn); nr_absent = __absent_pages_in_range(nid, zone_start_pfn, zone_end_pfn); /*  * ZONE_MOVABLE handling.  * Treat pages to be ZONE_MOVABLE in ZONE_NORMAL as absent pages  * and vice versa.  */ if (mirrored_kernelcore && zone_movable_pfn[nid]) {  unsigned long start_pfn, end_pfn;  struct memblock_region *r;  for_each_mem_region(r) {   start_pfn = clamp(memblock_region_memory_base_pfn(r),       zone_start_pfn, zone_end_pfn);   end_pfn = clamp(memblock_region_memory_end_pfn(r),     zone_start_pfn, zone_end_pfn);   if (zone_type == ZONE_MOVABLE &&       memblock_is_mirror(r))    nr_absent += end_pfn - start_pfn;   if (zone_type == ZONE_NORMAL &&       !memblock_is_mirror(r))    nr_absent += end_pfn - start_pfn;  } } return nr_absent;}
```

**该函数主要用于计算内存空洞页面数的**，计算方法大致为在 zone 区域范围内，遍历所有 memblock 的内存块，将这些内存块的大小累加，之后两者做差，zone_absent_pages_in_node 后面是对 ZONE_MOVABLE 的特殊处理了，方法是类似的，这里也不做介绍了。

calculate_node_totalpages()后面就是各种简单的赋值操作了，这里也简单介绍一下**zone**的结构：

其中 MAX_NR_ZONES 是一个节点中所能包容纳的 Zones 的最大数。

```
struct zone { ......    //保留页框池，记录每个管理区中必须保留的物理页面数，以用于紧急状况下的内存分配 long lowmem_reserve[MAX_NR_ZONES];    //保持对UMA的兼容(当做一个节点)，NUMA模式下的节点数#ifdef CONFIG_NUMA int node;#endif //该zone的父节点    struct pglist_data *zone_pgdat; ...... //该zone的第一个页的页号 unsigned long  zone_start_pfn;    //伙伴系统管理的page数，这是除去了在初始化阶段被申请的页面（比如memblock） atomic_long_t  managed_pages;    //zone大小，含空洞，即zone_end_pfn - zone_start_pfn unsigned long  spanned_pages;    //zone实际大小，不含空洞 unsigned long  present_pages; //zone的名称，如“DMA”“Normal”“Highmem”，这些名称定义于page_alloc.c的zone_names[MAX_NR_ZONES] const char  *name; ZONE_PADDING(_pad1_) //包含所有空闲页面，伙伴系统使用，里面有数量为MIGRATE_TYPES个的free_list链表，分别用于管理不同迁移类型的内存页面 struct free_area free_area[MAX_ORDER];    //描述zone的当前状态 unsigned long  flags;    /* Primarily protects free_area */    //与伙伴算法的碎片迁移算法有关 spinlock_t  lock; ZONE_PADDING(_pad2_) ...... ZONE_PADDING(_pad3_) ......} ____cacheline_internodealigned_in_smp;
```

回到 free_area_init_node()函数，紧接在 calculate_node_totalpages()后的函数调用的为**alloc_node_mem_map()，这个函数是用于申请 node 节点的 node_mem_map 相应的内存空间**，如果是 sparse memory 内存模型，则该函数实现为空，这里便不做过多介绍了，直接看最后的初始化工作：**free_area_init_core**()。

```
static void __init free_area_init_core(struct pglist_data *pgdat){ enum zone_type j; int nid = pgdat->node_id; //对节点的一些锁和队列进行初始化 pgdat_init_internals(pgdat); pgdat->per_cpu_nodestats = &boot_nodestats; for (j = 0; j < MAX_NR_ZONES; j++) {  struct zone *zone = pgdat->node_zones + j;  unsigned long size, freesize, memmap_pages;  unsigned long zone_start_pfn = zone->zone_start_pfn;  size = zone->spanned_pages;  freesize = zone->present_pages;  //memmap_pages,每一个4k物理页都对应一个mem_map_t来管理  memmap_pages = calc_memmap_size(size, freesize);  if (!is_highmem_idx(j)) {   if (freesize >= memmap_pages) {    freesize -= memmap_pages;    if (memmap_pages)     printk(KERN_DEBUG            "  %s zone: %lu pages used for memmap\n",            zone_names[j], memmap_pages);   } else    pr_warn("  %s zone: %lu pages exceeds freesize %lu\n",     zone_names[j], memmap_pages, freesize);  }  //dma保留页  if (j == 0 && freesize > dma_reserve) {   freesize -= dma_reserve;   printk(KERN_DEBUG "  %s zone: %lu pages reserved\n",     zone_names[0], dma_reserve);  }  //计算nr_kernel_pages（低端内存的页数）和nr_all_pages的数量  if (!is_highmem_idx(j))   nr_kernel_pages += freesize;  /* Charge for highmem memmap if there are enough kernel pages */        //如果有足够的页，则也为高端内存提供memmap_pages  else if (nr_kernel_pages > memmap_pages * 2)   nr_kernel_pages -= memmap_pages;  nr_all_pages += freesize;  /*   * Set an approximate value for lowmem here, it will be adjusted   * when the bootmem allocator frees pages into the buddy system.   * And all highmem pages will be managed by the buddy system.   */        //初始化zone使用的各类锁  zone_init_internals(zone, j, nid, freesize);  if (!size)   continue;  set_pageblock_order();  setup_usemap(pgdat, zone, zone_start_pfn, size);  init_currently_empty_zone(zone, zone_start_pfn, size);  memmap_init(size, nid, j, zone_start_pfn); }}
```

该函数主要用于向节点下的每个 zone 填充相关信息，在 for 循环内，循环遍历统计各个管理区最大跨度间相差的页面数 size 以及除去内存“空洞”后的实际页面数 freesize，然后通过 calc_memmap_size()计算出该管理区所需的页面管理结构占用的页面数 memmap_pages，最后可以计算得出高端内存外的系统内存共有的内存页面数（freesize-memmap_pages）。

**nr_kernel_pages 用于统计低端内存的页数**，此外循环体内的操作则是初始化内存管理区的管理结构，例如各类锁的初始化、队列初始化。其中 set_pageblock_order()用于在 CONFIG_HUGETLB_PAGE_SIZE_VARIABLE 下设置 pageblock_order 的值；setup_usemap()函数则是为了给 zone 管理结构体中的 pageblock_flags 申请内存空间的，pageblock_flags 与伙伴系统的碎片迁移算法有关。init_currently_empty_zone()则主要是初始化管理区的等待队列哈希表和等待队列，同时还初始化了与伙伴系统相关的 free_area 列表

nr_kernel_pages、nr_all_pages 和页面的关系可以参考下图：
![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171717111820985.png)

这里以我自己的私服为例，看一下我私服 node 和 zone 的情况

首先是 node 的个数，GNU/Linux 根据物理 CPU 的数量分配 node，因此可以直接查物理 CPU 的数量：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171717186670264.png)
当然，用 numactl 会更加直观：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171717242420066.png)

我的机器上只有一个 node，接下来可以用 cat /proc/zoneinfo 查看这个 node 下各个 zone 的情况：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171717346972692.png)
回到 free_area_init_core()函数的最后，memmap_init()->memmap_init_zone()，该函数主要是根据 PFN，然后通过 pfn_to_page 找到对应的 struct page 结构，并将该结构进行初始化处理，并设置 MIGRATE_MOVABLE 标志，表明可移动。

```
//遍历memblock，找到节点的内存地址范围，zone的范围不能大于这个，使用memmap_init_zone对该zone进行处理void __meminit __weak memmap_init(unsigned long size, int nid,      unsigned long zone,      unsigned long range_start_pfn){ unsigned long start_pfn, end_pfn; unsigned long range_end_pfn = range_start_pfn + size; int i; for_each_mem_pfn_range(i, nid, &start_pfn, &end_pfn, NULL) {  start_pfn = clamp(start_pfn, range_start_pfn, range_end_pfn);  end_pfn = clamp(end_pfn, range_start_pfn, range_end_pfn);  if (end_pfn > start_pfn) {   size = end_pfn - start_pfn;   memmap_init_zone(size, nid, zone, start_pfn,      MEMINIT_EARLY, NULL, MIGRATE_MOVABLE);  } }}void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,  unsigned long start_pfn,  enum meminit_context context,  struct vmem_altmap *altmap, int migratetype){ unsigned long pfn, end_pfn = start_pfn + size; struct page *page; if (highest_memmap_pfn < end_pfn - 1)  highest_memmap_pfn = end_pfn - 1;#ifdef CONFIG_ZONE_DEVICE /*  * Honor reservation requested by the driver for this ZONE_DEVICE  * memory. We limit the total number of pages to initialize to just  * those that might contain the memory mapping. We will defer the  * ZONE_DEVICE page initialization until after we have released  * the hotplug lock.  */ if (zone == ZONE_DEVICE) {  if (!altmap)   return;  if (start_pfn == altmap->base_pfn)   start_pfn += altmap->reserve;  end_pfn = altmap->base_pfn + vmem_altmap_offset(altmap); }#endif for (pfn = start_pfn; pfn < end_pfn; ) {  /*   * There can be holes in boot-time mem_map[]s handed to this   * function.  They do not exist on hotplugged memory.   */  if (context == MEMINIT_EARLY) {   if (overlap_memmap_init(zone, &pfn))    continue;   if (defer_init(nid, pfn, end_pfn))    break;  }  page = pfn_to_page(pfn);  __init_single_page(page, pfn, zone, nid);  if (context == MEMINIT_HOTPLUG)   __SetPageReserved(page);  /*   * Usually, we want to mark the pageblock MIGRATE_MOVABLE,   * such that unmovable allocations won't be scattered all   * over the place during system boot.   */  if (IS_ALIGNED(pfn, pageblock_nr_pages)) {   set_pageblock_migratetype(page, migratetype);   cond_resched();  }  pfn++; }}
```

### 3.3 **struct page**

最后这里简单介绍一下 struct page，内核会为每一个物理页帧创建一个 struct page 的结构体，因此要保证 page 结构体足够的小，否则仅 struct page 就要占用大量的内存，该结构有很多 union 结构，主要是用于各种算法不同数据的空间复用。

struct page 这个结构相当复杂，这里我放上网上找到的一个全局参考，可以比源码更清晰地了解整个结构体，这里我也只简单介绍里面的几个字段。

```
    struct page (include/linux/mm_types.h)                         page               +--------------------------------------------------------------+               |flags                                                         |               |   (unsigned long)                                            |   --+--       +==============================================================+     |         |..............................................................|     |         |page cache and anonymous pages                                |     |         |    +---------------------------------------------------------+     |         |    |lru                                                      |     |         |    |    (struct list_head)                                   |               |    |mapping                                                  |               |    |    (struct address_space*)                              |               |    |index                                                    |               |    |    (pgoff_t)                                            |  5 words      |    |private                                                  |   union       |    |    (unsigned long)                                      |               |    +---------------------------------------------------------+               |..............................................................|    has        |slab, slob, slub                                              |               |    +---------------------------------------------------------+  7 usage      |    |.........................................................|               |    |                            .+---------------------------|               |    |slab_list                   .|next                       |               |    |    (struct list_head)      .|   (struct page*)          |               |    |                            .|pages                      |               |    |                            .|pobjects                   |               |    |                            .|   (int)                   |               |    |                            .+---------------------------|               |    |.........................................................|               |    +---------------------------------------------------------+               |    |slab_cache                                               |               |    |    (struct kmem_cache*)                                 |               |    |freelist                                                 |               |    |    (void*)                                              |               |    +---------------------------------------------------------+               |    |.........................................................|               |    |s_mem    .counters          .+---------------------------|               |    | (void*) . (unsigned long)  .|inuse                      |               |    |         .                  .|objects                    |               |    |         .                  .|frozen                     |               |    |         .                  .|    (unsigned)             |               |    |         .                  .+---------------------------|               |    |.........................................................|               |    +---------------------------------------------------------+               |..............................................................|               |Tail pages of compound page                                   |               |    +---------------------------------------------------------+               |    |compound_head                                            |               |    |    (unsigned long)                                      |               |    |compound_dtor                                            |               |    |compound_order                                           |               |    |    (unsigned char)                                      |               |    |compound_mapcount                                        |               |    |    (atomic_t)                                           |               |    +---------------------------------------------------------+               |..............................................................|               |Second tail page of compound page                             |               |    +---------------------------------------------------------+               |    |_compound_pad_1                                          |               |    |_compound_pad_2                                          |               |    |    (unsigned long)                                      |               |    |deferred_list                                            |               |    |    (struct list_head)                                   |               |    +---------------------------------------------------------+               |..............................................................|               |Page table pages                                              |               |    +---------------------------------------------------------+               |    |_pt_pad_1                                                |               |    |    (unsigned long)                                      |               |    |pmd_huge_pte                                             |               |    |    (pgtable_t)                                          |               |    |_pt_pad_2                                                |               |    |    (unsigned long)                                      |               |    |.........................................................|               |    |pt_mm                         .pt_frag_refcount          |               |    |    (struct mm_struct*)       .    (atomic_t)            |               |    |.........................................................|               |    |ptl                                                      |               |    |    (spinlock_t/spinlock_t *)                            |               |    +---------------------------------------------------------+               |..............................................................|               |ZONE_DEVICE pages                                             |               |    +---------------------------------------------------------+               |    |pgmap                                                    |               |    |    (struct dev_pagemap*)                                |               |    |hmm_data                                                 |               |    |_zd_pad_1                                                |     |         |    |    (unsigned long)                                      |     |         |    +---------------------------------------------------------+     |         |..............................................................|     |         |rcu_head                                                      |     |         |    (struct rcu_head)                                         |     |         |..............................................................|   --+--       +==============================================================+     |         |..............................................................|               |            .                 .                .              |   4 bytes     |_mapcount   .page_type        .active          .units         |    union      |  (atomic_t).   (unsigned int).  (unsigned int).   (int)      |               |            .                 .                .              |     |         |..............................................................|   --+--       +==============================================================+               |_refcount                                                     |               |     (atomic_t)                                               |               |mem_cgroup                                                    |               |     (struct mem_cgroup)                                      |               |virtual                                                       |               |     (void *)                                                 |               |_last_cpupid                                                  |               |     (int)                                                    |               +--------------------------------------------------------------+
```

首先介绍一下**flags，它描述 page 的状态和其它的一些信息**，如下图。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/202212171717523047065.png)

主要分为 4 部分，其中标志位 flag 向高位增长，其余位字段向低位增长，中间存在空闲位。

- section：主要用于 sparse memory 内存模型，即 section 号。
- node：NUMA 节点号，标识该 page 属于哪一个节点。
- zone：内存域标志，标识该 page 属于哪一个 zone。
- flag：page 的状态标识，常用的有：

```
page-flags.henum pageflags { PG_locked,  /* 表示页面已上锁，不要访问 */ PG_error,     /* 表示页面发生IO错误 */ PG_referenced, /* 用于RCU算法 */ PG_uptodate,   /* 表示页面内容有效，当该页面上读操作完成后，设置该标志位 */ PG_dirty,      /* 表示页面是脏页，内容被修改过 */ PG_lru,        /* 表示该页面在lru链表中 */ PG_active,     /* 表示该页面在活跃lru链表中 */ PG_slab,       /* 表示该页面是属于slab分配器创建的slab */ PG_owner_priv_1, /* 页面的所有者使用，如果是pagecache页面，文件系统可能使用*/ PG_arch_1,       /* 与体系架构相关的页面状态位 */ PG_reserved,     /* 表示该页面不可被换出，防止该page被交换到swap */ PG_private,  /* 如果page中的private成员非空，则需要设置该标志，如果是pagecache, 包含fs-private data */ PG_writeback,  /* 页面正在回写 */ PG_head,  /* A head page */ PG_swapcache,  /* 表示该page处于swap cache中 */ PG_reclaim,  /* 表示该page要被回收，决定要回收某个page后，需要设置该标志 */ PG_swapbacked,  /* 该page的后备存储器是swap/ram，一般匿名页才可以回写swap分区 */ PG_unevictable,  /* 该page被锁住，不能回收，并会出现在LRU_UNEVICTABLE链表中 */ ......}
```

内核定义了一些**标准宏，用于检查页面是否设置了某个特定的标志位或者用于操作某些特定的标志位**，比如

```
PageXXX()（检查是否设置）SetPageXXX()ClearPageXXX()
```

**伙伴系统**，内存被分成含有很多页面的大块，每一块都是 2 个页面大小的方幂。如果找不到想要的块, 一个大块会被分成两部分，这两部分彼此就成为伙伴。其中一半被用来分配，而另一半则空闲。这些块在以后分配的过程中会继续被二分直至产生一个所需大小的块。当一个块被最终释放时, 其伙伴将被检测出来, 如果伙伴也空闲则合并两者

**slab**，Slab 对小对象进行分配，不用为每个小对象去分配页，节省了空间。内核中一些小对象在创建析构时很频繁，Slab 对这些小对象做缓存，可以重复利用一些相同的对象，减少内存分配次数

接着看**struct list_head lru**，链表头，具体作用得看 page 处于什么用途中，如果是伙伴系统则用于连接相同的伙伴，通过第一个 page 可以找到伙伴中所有的 page；如果是 slab，page->lru.next 指向 page 驻留的的缓存管理结构，page->lru.prec 指向保存该 page 的 slab 管理结构；而当 page 被用户态使用或被当做页缓存使用时，lru 则用于将该 page 连入 zone 中相应的 lru 链表，供内存回收时使用

**struct address_space \*mapping**，当 mapping 为 NULL 时，该 page 为交换缓存（swap）；当 mapping 不为 NULL 且第 0 位为 0，该 page 为页缓存或文件映射，mapping 指向文件的地址空间；当 mapping 不为 NULL 且第 0 位为 1，该 page 为匿名页（匿名映射），mapping 指向 struct anon_vma 对象

**pgoff_t index**，映射虚拟内存空间里的地址偏移，一个文件可能只映射其中的一部分，假设映射了 1M 的空间，index 指的是在 1M 空间内的偏移，而不是在整个文件内的偏移

**unsigned long private**，私有数据指针

**atomic_t _mapcount**，该 page 被页表映射的次数，即这个 page 被多少个进程共享，初始值为-1（非伙伴系统，如果是伙伴系统则为 PAGE_BUDDY_MAPCOUNT_VALUE），例如只被一个进程的页表映射的话，值为 0

**atomic_t _refcount**，页表引用计数，内核要操作该 page 时，引用计数会+1，操作完成后则-1，当引用计数为 0 时，表示该 page 没有被引用到，这时候就可以解除该 page 的映射（虚拟页-物理页，该物理页是占用内存的）（用于内存回收）

更详细的内容可以参考源代码~

ok，回到 memmap_init_zone()，直接看关键函数***_init\*single_page**()

```
static void __meminit __init_single_page(struct page *page, unsigned long pfn,    unsigned long zone, int nid){ mm_zero_struct_page(page); //page初始化，根据page大小还有一些特殊操作 set_page_links(page, zone, nid, pfn); //flags初始化，将页面映射到zone和node init_page_count(page); //page的_refcount设置为1 page_mapcount_reset(page); //page的_mapcount设置为-1 INIT_LIST_HEAD(&page->lru); //初始化lru，指向自身 ......}
```

至此，free_area_init_node()的初始化操作执行完毕，据前面分析可以知道该函数**主要是将整个 linux 物理内存管理框架进行初始化，包括内存管理节点 node、管理区 zone 以及页面管理 page 等数据的初始化**。

回到前面的 free_area_init()函数的循环体内的最后两个函数 node_set_state()和 check_for_memory()，node_set_state()主要是对 node 节点进行状态设置，而 check_for_memory()则是做内存检查。

到这里，内存管理框架的构建基本完毕。

```
void __init setup_arch(char **cmdline_p){    ...... max_pfn = e820__end_of_raCm_pfn(); //max_pfn初始化 ...... find_low_pfn_range(); //max_low_pfn、高端内存初始化 ...... ...... early_alloc_pgt_buf(); //页表缓冲区分配 reserve_brk(); //缓冲区加入memblock.reserve    ......    e820__memblock_setup(); //memblock.memory空间初始化 启动 ......    init_mem_mapping(); //低端内存内核页表初始化 高端内存固定映射区中临时映射区页表初始化  ......    initmem_init(); //high_memory初始化 通过memblock内存管理器设置node节点信息    ......    x86_init.paging.pagetable_init(); // 节点node、内存管理区zone、page初始化    ......}
```

最后补充一下 pfn 和物理地址以及 pfn 和虚拟地址的转换。

```
//物理地址->物理页#define PAGE_SHIFT _PAGE_SHIFT#define _PAGE_SHIFT 12#define phys_to_pfn(phys) ((phys) >> PAGE_SHIFT)#define pfn_to_phys(pfn) ((pfn) << PAGE_SHIFT)#define phys_to_page(phys) pfn_to_page(phys_to_pfn(phys))#define page_to_phys(page) pfn_to_phys(page_to_pfn(page))
```

pfn_to_page、page_to_pfn 可以参考上面的内存模型，不同的模型实现的细节不一样。

这里可以看出物理页的大小是 4096，即 4kb，虽然内核在虚拟地址中是在高地址的，但是在物理地址中是从 0 开始的。

在 linux 内核直接映射区里内核逻辑地址与物理页的转换关系如下：

```
#define pfn_to_virt(pfn) __va(pfn_to_phys(pfn))#define virt_to_pfn(kaddr) (phys_to_pfn(__pa(kaddr)))#define virt_to_page(kaddr) pfn_to_page(virt_to_pfn(kaddr))#define page_to_virt(page) pfn_to_virt(page_to_pfn(page))#define __pa(x)         ((unsigned long) (x) - PAGE_OFFSET)#define __va(x)         ((void *)((unsigned long) (x) + PAGE_OFFSET))
```

PAGE_OFFSET 与具体的架构有关，在 x86_32 中，PAGE_OFFSET 是 0xC0000000，即 32 位系统中，内核的逻辑地址只有高位的 1GB

## 4. 总结

Linux 内存管理是一个很复杂的“工程”，Linux 会通过中断调用获取被 BIOS 保留的内存地址范围以及系统可以使用的内存地址范围。在内核初始化阶段，通过 memblock 内存分配器，实现页分配器初始化之前的内存管理和分配请求，memblock 内存区管理算法将可用可分配的内存用 memblock.memory 进行管理，已分配的内存用 memblock.reserved 进行管理，只要内存块加入到 memblock.reserved 里面就表示该内存被申请占用了，申请和释放的操作都集中在 memblock.reserved，这个算法效率不高，但是却是合理的，因为在内核初始化阶段并没有太多复杂的内存操作场景，而且很多地方都是申请的内存都是永久使用的。为了合理地利用 4G 的内存空间，Linux 采用了 3：1 的策略，即内核占用 1G 的线性地址空间，用户占用 3G 的线性地址空间，且 Linux 采用了一种折中方案是只对 1G 内核空间的前 896 MB 按线性映射, 剩下的 128 MB 采用动态映射，即走多级页表翻译，这样，内核态能访问空间就更多了。

传统的多核运算是使用 SMP(Symmetric Multi-Processor )模式，将多个处理器与一个集中的存储器和 I/O 总线相连，所有处理器访问同一个物理存储器，因此 SMP 系统有时也被称为一致存储器访问（UMA）结构体系。而 NUMA 模式是一种分布式存储器访问方式，处理器可以同时访问不同的存储器地址，大幅度提高并行性。NUMA 模式下系统的每个 CPU 都有本地内存，可支持快速访问，各个处理器之间通过总线连接起来，以支持对其它 CPU 本地内存的访问，但是这些访问要比处理器本地内存的慢。Linux 内核通过插入一些兼容层，使两个不同体系结构的差异被隐藏，两种模式都使用了同一个数据结构，另外 linux 的物理内存管理机制将物理内存划分为三个层次来管理，依次是：Node（存储节点）、Zone（管理区）和 Page（页面）。

Linux 内存管理的内容十分多且复杂，上面介绍到的也只是其中的一部分，如果感兴趣的话可以下载一份源代码，然后细细品味。

## 5.参考文献

Linux 内存管理-Zone：https://blog.csdn.net/wyy4045/article/details/81776277

Linux 内存管理-Node：https://blog.csdn.net/wyy4045/article/details/81708895

内存管理框架：https://www.jeanleo.com/

memblock：https://biscuitos.github.io/blog/MMU-ARM32-MEMBLOCK-index/

Zone_sizes_init：http://www.soolco.com/post/19152_1_1.html

内核页表：https://www.daimajiaoliu.com/daima/47db35735100402

linux 内存模型：http://www.wowotech.net/memory_management/memory_model.html

linux 中的分页机制：http://edsionte.com/techblog/archives/3435

linux 内核介绍：https://richardweiyang-2.gitbook.io/kernel-exploring

struct page：https://blog.csdn.net/gatieme/article/details/52384636

页表初始化：https://www.cnblogs.com/tolimit/p/4585803.html

原文作者：dengxuanshi，腾讯 IEG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/fPPlzx8yfNvC7U6_y8RKPQ

# 【NO.125】Linux网络编程 | 零拷贝 ：sendfile、mmap、splice、tee

## 1.传统文件传输的问题

在网络编程中，如果我们想要提供文件传输的功能，最简单的方法就是用read将数据从磁盘上的文件中读取出来，再将其用write写入到socket中，通过网络协议发送给客户端。

```text
ssize_t read(int fd, void *buf, size_t count);
ssize_t write(int fd, const void *buf, size_t count);
```

但是就是这两个简单的操作，却带来了大量的性能丢失

例如我们的服务器需要为客户端提供一个下载操作，此时的操作如下

![img](https://pic2.zhimg.com/80/v2-d5bcafefde2ebf48d0b0c3efbb3c6f51_720w.webp)

从上图可以看出，虽然仅仅只有这两行代码，但是却在发生了**四次用户态和内核态的上下文切换**，以及**四次数据拷贝**，也就是在这个地方产生了大量不必要的损耗。

那么为什么会发生这些操作呢？

**上下文切换**

由于read和recv是系统调用，所以每次调用该函数我们都需要从用户态切换至内核态，等待内核完成任务后再从内核态切换回用户态。

**数据拷贝**

上面也说了，由于数据的读取与写入都是由系统进行的，那么我们就得将数据从用户的缓冲区中拷贝到内核，

- 第一次拷贝：将磁盘中的数据拷贝到内核的缓冲区中
- 第二次拷贝：内核将数据处理完，接着拷贝到用户缓冲区中
- 第三次拷贝：此时需要通过socket将数据发送出去，将用户缓冲区中的数据拷贝至内核中socket的缓冲区中
- 第四次拷贝：把内核中socket缓冲区的数据拷贝到网卡的缓冲区中，通过网卡将数据发送出去。

所以要想优化传输性能，就要从**减少数据拷贝和用户态内核态的上下文切换**下手，这也就是**零拷贝**技术的由来。

## 2.**什么是零拷贝呢？**

**零拷贝的主要任务就是避免CPU将数据从一块存储中拷贝到另一块存储**，主要就是利用各种技术，避免让CPU做大量的数据拷贝任务，以此减少不必要的拷贝。或者借助其他的一些组件来完成简单的数据传输任务，让CPU解脱出来专注别的任务，使得系统资源的利用更加有效

Linux中实现零拷贝的方法主要有以下几种，下面一一对其进行介绍

1. sendfile
2. mmap
3. splice
4. tee

## 3.sendfile

sendfile函数的作用是直接在两个文件描述符之间传递数据。由于整个操作完全在内核中（直接从内核缓冲区拷贝到socket缓冲区），从而避免了内核缓冲区和用户缓冲区之间的数据拷贝。

需要注意的是，in_fd必须是一个支持类似mmap函数的文件描述符，不能是socket或者管道，而out_fd必须是一个socket，由此可见sendfile是专门为了在网络上传输文件而实现的函数。

![img](https://pic3.zhimg.com/80/v2-0e9a420edd62f49ddcc0241a984a04f6_720w.webp)

```text
#include <sys/sendfile.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

> 参数：
> out_fd : 待写入内容的文件描述符
> in_fd : 待读出内容的文件描述符
> offset : 文件的偏移量
> count : 需要传输的字节数
> 返回值：
> 成功：返回传输的字节数
> 失败：返回-1并设置errno

## 4.**mmap**

mmap用于申请一段内存空间，也就是我们在进程间通信中提到过的**共享内存**，通过将内核缓冲区的数据映射到用户空间中，两者通过共享缓冲区直接访问统一资源，此时内核与用户空间就不需要再进行任何的数据拷贝操作了

![img](https://pic4.zhimg.com/80/v2-13655c6ca2a0154f7d75fd348251e287_720w.webp)

其中mmap用于申请空间，额munmap用于释放这段空间。

```text
#include <sys/mman.h>
void *mmap(void *addr, size_t length, int prot, int flags,
                  int fd, off_t offset);
int munmap(void *addr, size_t length);
```

参数：

addr : 内存的起始地址，如果设置为空则系统会自动分配

length : 指定内存段的长度

prot : 内存段的访问权限，通过按位与或可以取以下几种值

![img](https://pic3.zhimg.com/80/v2-40c76a8d03204163345541e5b166640a_720w.webp)

flag : 选项

![img](https://pic3.zhimg.com/80/v2-92610c23cd71caf6cee15fdceea1a356_720w.webp)

fd : 被映射文件对应的文件描述符

offset : 文件的偏移量

返回值：

成功：成功时返回指向内存区域的指针

失败：返回MAP_FAILED并设置errno

## 5.**splice**

splice函数用于**在两个文件描述符之间移动数据**，而不需要数据在内核空间和用户空间中来回拷贝

需要注意的是，使用splice函数时fd_in和fd_out**至少有一个是管道文件描述符，即**

```text
#include <fcntl.h>
ssize_t splice(int fd_in, loff_t *off_in, int fd_out,
               loff_t *off_out, size_t len, unsigned int flags);
```

参数：

out_fd : 待写入内容的文件描述符

off_out : 待写入文件描述符的偏移量，如果文件描述符为管道则必须为空

in_fd : 待读出内容的文件描述符

off_in : 待读出文件描述符的偏移量，如果文件描述符为管道则必须为空

len : 需要复制的字节数

flags : 选项

![img](https://pic1.zhimg.com/80/v2-27e32c19c51d98b2392b064a5821c450_720w.webp)

返回值：

成功：返回在两个文件描述符之间复制的字节数

没有数据：返回0

失败：返回-1并设置errno

可能产生的errno

![img](https://pic1.zhimg.com/80/v2-f31d8051fb9e4ce9374caf76b9e64f64_720w.webp)

## 6.**tee**

tee函数用于**在两个管道文件描述符之间复制数据**，并且它是直接复制，不会将数据读出，所以源文件上的数据仍可以用于后面的读操作

```text
#include <fcntl.h>
ssize_t tee(int fd_in, int fd_out, size_t len, unsigned int flags);
```

参数：

out_fd : 待写入内容的文件描述符

in_fd : 待读出内容的文件描述符

len : 需要复制的字节数

flags : 选项

返回值：

成功：返回在两个文件描述符之间复制的字节数

没有数据：返回0

原文地址：https://zhuanlan.zhihu.com/p/592397046

作者：linux

# 【NO.126】TCP将成为历史？看看谷歌的QUIC协议都做了些什么你就知道了

在过去几年中，QUIC已经成为谷歌服务网络通信的默认协议。正如这里所说的，QUIC现在被从Chrome浏览器到谷歌服务器的所有连接中的一半以上使用。它还得到了Microsoft Edge、Firefox和Opera的官方支持。

![img](https://pic2.zhimg.com/80/v2-5367efdcc38a220f8b239c1a7d46e3dd_720w.webp)

谷歌开发它时考虑到了网络安全，并用更先进和最新的技术取代了一些过时的标准。换句话说，QUIC可能代表着互联网的未来，这就是为什么理解它如此重要。

因此，让我们深入了解谷歌的QUIC协议，并介绍您需要了解的一切。

## **1.QUIC的历史**

> QUIC最初代表“快速UDP Internet连接”，尽管该术语不再用作首字母缩略词。现在，“QUIC”被用来描述谷歌设计的通用传输层网络协议。
> 2012年，最初开始实施和部署。
> 2013年，在IETF会议上，随着实验范围的扩大，它被公开宣布。
> 2015年6月，针对其规范的互联网草案提交给IETF进行标准化。
> 2016年，QUIC工作组成立。
> 2018年10月，QUIC上的HTTP映射开始被称为“HTTP/3”，使QUIC必然成为全球标准。
> 2021 5月，IETF最终在RFC 9000中对其进行了标准化。

## **2.什么是QUIC？**

> 谷歌的QUIC是一种基于UDP的低延迟互联网传输协议，该协议通常用于游戏、流媒体和VoIP服务。
> UDP比TCP轻得多，但反过来，它的纠错服务比TCP少得多。
> 通过QUIC，谷歌的目标是将UDP和TCP的一些最佳功能与现代安全工具相结合。-谷歌希望通过其QUIC协议加速网络

首先我们先来看看http协议，再带入到tcp，udp，quic，逐步探索。

## **3.HTTP 1：**

我们从HTTP/1.0开始，其中每个请求-响应对前面都是打开一个tcp连接，然后关闭同一个连接。这引入了大量延迟，并导致人们想出了一个名为keep-alive的解决方案，该解决方案在HTTP请求之间重用TCP连接，基本上延迟了连接的关闭。下面是有和没有keep alive的两个图表。

![img](https://pic1.zhimg.com/80/v2-06319b737a3b3a2e5ee9ae4748c4dfa0_720w.webp)

此示例还演示了在发出HTTP请求之前需要建立的TCP 3路握手。基本上，双方都使用序列号（SYN数据包）跟踪他们发送的内容。这样，如果他们丢失了任何数据包，就可以从最后一个序列号重新发送。请注意，作为每个连接建立的一部分，需要进行3路握手（SYN、SYN-ACK、ACK）。由于TCP是双向通信模式，因此需要在每个方向上发送ACK。FIN用于关闭TCP连接。

显而易见借助Keep-alive是一个很好的手段来保持连接。

仔细看，每个资源请求仍然需要在激发之前完成其先前的资源请求。例如，我不能发出索引请求。css在索引之前。html已返回。

嗯。他们是怎么解决的？

![img](https://pic1.zhimg.com/80/v2-631b5ce16c3b0ce1359371290608261c_720w.webp)

他们在HTTP/1.1中引入了一个名为pipeline流水线的特性。通过该特性，可以立即通过TCP连接发出每个HTTP请求，而无需等待前一个请求的响应返回。该图显示了HTTP/1.1.和pipeline。

报文的回复将以相同的顺序返回。这引入了一个称为HTTP线路头（HOL）阻塞的问题

## 4.什么是HOL？

> 假设您正在请求一个猫的图像和一个javascript文件。如果猫的图像太大，服务器在完成发送图像之前不会开始发送javascript文件。

是不是听起来很可怕，那么如何解决这一问题？

最初的方法是让浏览器打开最多6个到同一服务器的连接，作为性能优化，开发人员开始在多个域之间共享资源，以支持服务器上超过6个资源的情况。此外，这并没有解决为每个单独的连接设置TCP（和TLS）握手的开销。在第2部分之前，我不会讨论TLS。

这也引入了一些创新，如css ，减少了要通过网络传输的单个资源的数量。

很快人们意识到这是不可扩展的，于是引入了一种新的方法，即HTTP/2和多路复用。

本质上，它声明TCP连接上的每个HTTP请求都可以立即发出，而无需等待上一个响应返回。响应可以按任何顺序返回。下图再次说明了使用流的HTTP/1.1流水线和HTTP/2复用。注意在HTTP/2中使用多路复用流的视图。css在cat.png之前返回

![img](https://pic4.zhimg.com/80/v2-355a531bcabba89f9939f763089f62cf_720w.webp)

此外，与HTTP/1.1不同，在HTTP/1.1中，作为HTTP头的一部分的资源标识仅在一组tcp包的第一个tcp包中，在HTTP/2中，每个tcp包都包含资源的标识。这使得被拆分为多个TCP数据包的HTTP响应可以很容易地重新组合，消除了HTTP/1.1的串行性

是的，看起来我们已经最大限度地改进了HTTP/2多路复用

不完全是这样，这里还有一个缺陷，但您必须更深入地研究TCP堆栈。

## 5.**TCP，现在有什么缺陷？**

TCP是一种面向连接的协议。它会跟踪客户端和服务器之间传输的所有数据包，如果数据包在从服务器传输过程中丢失，它会将所有数据包保存在该数据包的序列号之后，直到丢失的数据包被重新发送，才将其传递给应用层。这里有一个图表来说明这一点。

![img](https://pic4.zhimg.com/80/v2-4fec9dd0ac0df8780db332c7539a31b7_720w.webp)

例如，在图中，如果数据包2来自视图。css丢失时，它会导致所有从3到20的数据包存储在接收缓冲区中，直到数据包2被重新发送后才传递到应用程序堆栈。尽管所有的包裹都来自猫。png将被接收，但底层TCP协议无法知道这一点，因此它强制重传。

（注意：这个图不是一个非常准确的表示，因为我已经从HTTP响应切换到了TCP响应来说明这个问题）。

如果你仔细观察它，潜在的问题是因为TCP的面向连接的本质。例如，如果TCP知道数据包1和数据包2是view.css的一部分，则只会导致数据包2重新传输，而不会阻止数据包3–20。标识HTTP/2层中底层多路复用流的流ID与底层TCP数据包ID断开连接。

## 6.QUIC与TCP

与TCP不同，QUIC协议只允许以加密形式进行通信。由于QUIC中未加密的通信形式被设计为禁止，因此隐私和安全是QUIC数据传输的固有部分。在网络安全方面，这无疑是一个优势，但在不严格要求加密的情况下，这也可能是一个无用的开销。

但与TCP+TLS相比，QUIC建立安全连接所需的时间代表了真正的突破。换句话说，QUIC的主要目标是大大减少连接设置期间的开销。

这得益于QUIC的设计。事实上，QUIC使交换配置密钥和支持的协议成为初始握手过程的一部分更快。具体而言，当发送方打开连接时，响应包还包括使用加密所需的未来数据包所需的数据。这一步骤不需要建立TCP连接，然后通过其他数据包协商安全协议。这会导致更高的连接速度和显著的响应降低，甚至在主机间重新连接期间降低到0ms，这被称为“零RTT连接建立”。

![img](https://pic1.zhimg.com/80/v2-03de8426d4a2736e8beb370d960e73f0_720w.webp)

正如您所看到的，典型的安全TCP连接需要两到三次往返，发送方才能真正开始接收数据。这可能需要300毫秒。而通过使用QUIC，发送者可以立即开始与之前已经交互过的接收者进行交互。

与UDP相比，QUIC是赢家，因为它具有UDP所不具备的拥塞控制和自动重传等TCP功能。这使得它本质上比纯UDP更可靠。详细地说，虽然QUIC使用UDP作为基础，但它涉及丢失恢复。这是因为QUIC的行为类似于TCP，它分别检查每个流，并在数据丢失时重新传输数据。

此外，如果一个流中发生错误，QUIC可以继续独立地为其他流提供服务。这一特性对于提高易出错链路的性能非常有用，因为在TCP注意到丢失或丢失的数据包之前，可能会收到大量额外的数据。在QUIC中，在修复流时，可以自由处理这些数据。

QUIC还提高了网络切换事件期间的性能，例如当移动设备用户从Wi-Fi网络移动到移动网络时。当在TCP上发生同样的事情时，将执行一个长过程，其中每个现有连接一次断开一个，然后按需重新建立。为了解决这个问题及其在性能方面的后果，QUIC包括到接收器的连接ID，而不考虑源。这允许简单地通过重新发送单个数据包来重新建立连接，该数据包始终包含该ID，即使发送方的IP地址发生了更改，接收方也会认为该ID有效。

## **7.那么，这是否足以让QUIC取代TCP？**

我知道一种叫做UDP的替代方案，它是无连接的。

但我们仍然需要联系，不是吗？我们如何在没有连接的情况下以可靠的方式请求资源（重新传输、确认、排序和整个沙邦）。我们需要一种方法来识别跨多个TCP数据包的资源。

如果我们删除TCP并使用IP上的流协议并将其与HTTP分层，该怎么办？该协议将像以前一样保持单个连接，但流将有自己的重传。实际上，每个流在一个更大的连接上都具有tcp特性。

这正是HTTP/3 到 QUIC所做的。QUIC作为TCP的替代品，在底层UDP数据包本身和HTTP/3之上维护流，而HTTP/2对流一无所知。将流视为小型TCP连接，但处于资源级别。换句话说，与HTTP相比，QUIC保留了单个资源流中的排序，但不再跨越单个流。这也意味着QUIC不再按照请求的顺序向应用程序层传递资源。

QUIC系统的另一个目标是提高网络切换事件期间的性能，例如当移动设备的用户从本地wifi热点移动到移动网络时发生的情况。当这种情况发生在TCP上时，会开始一个漫长的过程，每个现有连接都会逐一超时，然后根据需要重新建立。为了解决这个问题，QUIC包括一个连接标识符，该标识符唯一地标识到服务器的连接，而不考虑源。这允许通过发送始终包含此ID的数据包来重新建立连接，因为即使用户的IP地址发生更改，原始连接ID仍然有效。

指示客户端和服务器的速度的流控制保持在流级别。这是通过发布/订阅一个窗口来实现的，该窗口指示每一方可以发送多少数据，并在每次成功传输后更新该窗口。

为了在不等待重传的情况下从丢失的数据包中恢复，QUIC可以用FEC数据包补充一组数据包。与RAID-4非常类似，FEC数据包包含FEC组中数据包的奇偶校验。如果组中的一个分组丢失，则可以从FEC分组和组中的剩余分组中恢复该分组的内容。发送者可以决定是否发送FEC分组以优化特定场景（例如，请求的开始和结束）。

嗯。为什么我们需要UDP？为什么不通过IP实现QUIC？

这是因为企业和互联网上的大多数防火墙仍然支持UDP协议。如果我们要在IP上分层QUIC，我们必须重新配置所有这些。UDP无论如何都是一个小开销（8字节报头）。

QUIC的安全性问题呢？下次我们再来讨论

原文地址：https://zhuanlan.zhihu.com/p/592578434

作者：linux

# 【NO.127】MySQL 的锁机制，那么多的锁，该怎么区分？

## 1.楔子

本篇文章来聊一下 MySQL 的锁，首先不光是数据库，任何的一门高级语言也都内置了锁机制。从本质上讲，锁是一种协调多个进程或多个线程对某一资源进行访问的机制。

而之所以要存在锁，是因为在并发编程中，程序的某一部分在并发访问的时候会导致意想不到的结果。所以这部分程序就需要用锁保护起来，而保护起来的部分就叫做临界区。

在 MySQL 中，按照不同的角度，可以将锁分为如下几种：

![img](https://pic3.zhimg.com/80/v2-d7efe6e913b506ae84d2910728f50e06_720w.webp)

这么多的锁，我们该怎么区分呢？下面就来逐一回答。

## 2.**脏写是如何避免的**

在区分锁之前，先来回顾一个问题，前面我们说四种隔离级别，无论哪一种都可以避免脏写的问题。但怎么避免的当时却没有解释，原因就是涉及到了锁，下面来解释一下。

再来回顾一下什么是脏写，假设事务 A 和 事务 B 同时对张三的账户余额进行更新，初始值为 100，那么两个事务拿到的也都是 100。然后事务 A 给余额增加 100，事务 B 给余额增加 200。理论上最终应该是 400 才对，但如果 A 先提交 B 后提交，最终的结果却是 300；B 先提交 A 后提交，最终的结果就是 200。

以上这种现象就是脏写，具体表现为：两个事务更新同一条数据，后提交的事务将先提交的事务所做的更新给覆盖了。

那么如何避免呢？显然要依赖锁，多个事务在更新同一条数据的时候要串行更新。

所以当事务在更新数据的时候，会先看这条数据有没有人加锁。如果没有，那么该事务就会创建一个锁，里面包含了事务 ID（trx_id） 和等待状态，然后将锁和这条数据关联在一起。

![img](https://pic1.zhimg.com/80/v2-2463872e216709834532b272911e8814_720w.webp)

事务 A 更新数据的时候，会给数据加锁，然后别的事务就不能再更新了。但假设这个时候又来个事务 B 也要更新这条数据，它会怎么做呢？

首先还是判断数据有没有人加锁，结果发现被事务 A 加锁了，就知道自己不能修改这条数据。但事务 B 仍会对这条数据加锁，只不过它处于等待状态。

![img](https://pic1.zhimg.com/80/v2-c2c8e02fad7c0a6110ba28e11ccce120_720w.webp)

当事务 A 更新完数据，就会将自己的锁释放掉，并且还会去找，有没有别人也对这条数据加了锁。显然它会发现该数据也被事务 B 加锁了，于是会把事务 B 锁里的等待状态修改为 false，然后唤醒事务 B 开始执行，此时事务 B 就获取到锁了。

![img](https://pic3.zhimg.com/80/v2-cb9ce999bf20b2752b8e47e1c4181c42_720w.webp)

以上就是 MySQL 锁机制的一个最基本的原理，其实就和 Python 里面的互斥锁是一样的，但是基于此我们又引申出了很多不同种类的锁。

## 3.**MySQL 的读锁和写锁**

先来聊聊读锁和写锁，读锁也被称为共享锁、S 锁，写锁也被称为独占锁、排它锁、X 锁。而上面多个事务在更新数据时加的锁，就是写锁。

那么问题来了，如果一个事务在读数据的时候，发现这条数据被加锁了，那么该事务需要继续加锁吗？如果是更新数据，那么需要加锁，但读数据是不需要的。因为默认情况下，如果是读数据，会走 MVCC 机制。

因为读数据可以根据 ReadView 在 undo log 版本链里找一个能读取的版本，完全不用考虑是否有别的事务在更新，ReadView 机制不允许当前事务读取别的事务已经更新的值。所以默认情况下读数据完全不需要加锁，更不需要关心别的事务是否在更新数据，直接基于 MVCC 机制读某个快照即可。

但如果就是想在读数据的时候加锁呢？答案是使用读锁，也叫 S 锁、共享锁。

```text
SELECT * FROM girl
WHERE age > 16 IN SHARE MODE;
```

在查询语句后面加上 IN SHARE MODE 就代表查询数据的时候施加读锁。

注意：读锁和写锁是互斥的，只能有一把写锁或者任意多把读锁，也就是说如果先施加了写锁，就不能再施加读锁，因为两者互斥，当然更不能施加写锁，因为写锁只能有一把。如果先施加了读锁，那么不能再施加写锁，但是可以继续施加读锁，因为读锁可以有任意把。

所以可以得到如下结论：

- 更新数据的时候必然加写锁（MySQL 自动加），写锁和写锁是互斥的，此时别人不能更新；并且也不能加读锁，因为写锁和读锁也是互斥的；但可以查询，因为查询默认是不加锁的，它走的是 MVCC 机制，会读取快照版本；
- 查询数据的时候可以加读锁，但需要手动加，默认不加锁。并且读锁和写锁是互斥的，施加了读锁就不能再加写锁，但读锁和读锁之间是不互斥的，可以有任意把读锁；

![img](https://pic3.zhimg.com/80/v2-4e1e2fd8278bcb8f8bd29de6a91d3756_720w.webp)

不过说实话，一般开发业务系统的时候，主动给查询加读锁是很少见的。另外，我们说查询的时候默认没有锁，走的是 MVCC，但可以手动加读锁。其实除了读锁，查询的时候还可以手动加写锁。

```text
SELECT * FROM girl
WHERE age > 16 FOR UPDATE;
```

在查询语句后面加上 FOR UPDATE 则表示给该查询语句施加写锁，一般主要出现在事务查询完毕之后还要更新数据的时候。比如该数据非常重要，事务在处理的时候不希望受到干扰。而一旦查询的时候加了写锁，那么在事务提交之前，任何人都不能更新数据了，只能在当前事务里更新数据。而等该事务提交之后，别人才能继续更新。

> 另外，读锁也被称为共享锁和 S 锁，写锁也被称为排它锁、独占锁和 X 锁。这里我们一直说的是读锁和写锁，但在 MySQL 中更常说共享锁和独占锁（排它锁），当然意思都是一样的，我们理解就好。

## 4.**MySQL 的行锁、表锁和页面锁**

基于操作类型，我们将锁分为读锁和写锁，如果基于操作的数据粒度划分的话，还可以将锁分为行锁、表锁和页面锁。

像 IN SHARE MODE 和 FOR UPDATE 施加的都属于行锁，因此也可以说行级读锁和行级写锁。行锁是针对指定行进行加锁，比如：

```text
-- 更新数据，MySQL会自动施加写锁
-- 并且只对 id = 1 的行施加写锁
-- 其它行不受影响
UPDATE * FROM girl
SET age = age + 1 
WHERE id = 1;
```

行锁的特点是开销比较大，加锁速度慢，可能会出现死锁，但锁定的粒度最小，发生锁冲突的概率最小，并发度最高。

而表锁则是在整个数据表上对数据进行加锁和释放锁，特点是开销比较小，加锁速度快，一般不会出现死锁，但锁定的粒度比较大，发生锁冲突的概率最高，并发度最低。

在 MySQL 中可以通过以下方式手动添加表锁：

```text
-- 为 account 表增加表级读锁
lock table account read;

-- 为 account 表增加表级写锁
lock table account write;

-- 查看数据表上增加的锁
show open tables;

-- 删除添加的表锁
unlock tables;
```

但说实话，在工作中我们几乎不会使用表锁，好端端的锁整张表干什么。

最后是页面锁，也称为页级锁，就是在页级别对数据进行加锁和解锁。锁定的粒度介于表锁和行锁之间，并发度一般。

![img](https://pic1.zhimg.com/80/v2-03cea0dc0ddae375a85288657800de74_720w.webp)

工作中最常用的是行锁，表锁和页面锁基本不用，MySQL 也不会自动添加。但使用行锁的时候，有以下几点需要注意：

- 行锁主要加在索引上，如果以非索引字段作为条件进行更新，行锁可能会变成表锁；
- InnoDB 的行锁是针对索引加锁，不是针对记录加锁，并且加锁的索引不能失效，否则行锁可能会变成表锁；

另外行锁、表锁和页面锁都是 InnoDB 存储引擎的特性，可能有人觉得执行 ALTER TABLE 之类的 DDL 语句施加的也是表锁，虽然 DDL 语句和普通的增删改语句之间也是互斥的。但其实 DDL 语句执行时施加的不是表锁，而是元数据锁（metadata locks），这一点要注意。

## 5.**死锁的产生和预防**

虽然锁在一定程度上能够解决并发问题，但稍有不慎，就可能造成死锁。发生死锁的必要条件有 4 个，分别为互斥条件、不可剥夺条件、请求与保持条件和循环等待条件，如下图所示。

![img](https://pic1.zhimg.com/80/v2-11e21d50cc72e963a42a7f74a2970c68_720w.webp)

**1）互斥条件**

在一段时间内，计算机中的某个资源只能被一个进程占用，此时如果其他进程请求该资源，则只能等待。

**2）不可剥夺条件**

某个进程获得的资源在使用完毕之前，不能被其他进程强行夺走，只能由获得资源的进程主动释放。

**3）请求与保持条件**

进程已经获得了至少一个资源，又要请求其他资源，但请求的资源已经被其他进程占有，此时请求的进程就会被阻塞，并且不会释放自己已获得的资源。

**4）循环等待条件**

系统中的进程之间相互等待，同时各自占用的资源又会被下一个进程所请求。例如有进程 A、进程 B 和进程 C 三个进程，进程 A 请求的资源被进程 B 占用，进程 B 请求的资源被进程 C 占用，进程 C 请求的资源被进程 A 占用，于是形成了循环等待条件。

但需要注意的是，只有 4 个必要条件都满足时，才会发生死锁。而处理死锁有 4 种方法，分别为预防死锁、避免死锁、检测死锁和解除死锁。

![img](https://pic1.zhimg.com/80/v2-3df547aaa1243602ab01b9db89868de8_720w.webp)

- 预防死锁：处理死锁最直接的方法就是破坏造成死锁的 4 个必要条件中的一个或多个，以防止死锁的发生。
- 避免死锁：在系统资源的分配过程中，使用某种策略或者方法防止系统进入不安全状态，从而避免死锁的发生。
- 检测死锁：这种方法允许系统在运行过程中发生死锁，但是能够检测死锁的发生，并采取适当的措施清除死锁。
- 解除死锁：当检测出死锁后，采用适当的策略和方法将进程从死锁状态解脱出来。

在实际工作中，通常采用有序资源分配法和银行家算法这两种方式来避免死锁，有兴趣可自行了解一下。

## 6.**MySQL 的死锁问题**

在 MySQL 5.5.5 及以上版本中，默认存储引擎是 InnoDB。该存储引擎使用的是行级锁，在某种情况下会产生死锁问题，所以 InnoDB 存储引擎采用了一种叫作等待图（wait-for graph）的方法来自动检测死锁，如果发现死锁，就会自动回滚一个事务。我们举例说明：

第一步：在终端 1 中将事务隔离级别设置为可重复读，开启事务后为 account 数据表中 id 为 1 的数据添加排他锁。

![img](https://pic1.zhimg.com/80/v2-730be8e334304a2c3486604947f2c170_720w.webp)

第二步：在终端 2 中将事务隔离级别设置为可重复读，开启事务后为 account 数据表中 id 为 2 的数据添加排他锁。

![img](https://pic4.zhimg.com/80/v2-6cf4457fb42a48a5bf8a1a60b21c2c83_720w.webp)

第三步：在终端 1 中为 account 数据表中 id 为 2 的数据添加排他锁。

```text
select * from account 
where id = 2 for update;
```

此时事务 1 会阻塞住，因为它在等待事务 2 释放 id = 2 的排他锁。

第四步：在终端 2 中为 account 数据表中 id 为 1 的数据添加排他锁。

![img](https://pic3.zhimg.com/80/v2-5dda8518183b0d08fbf887ea8a7bdd26_720w.webp)

我们看到死锁了，事务 1 因事务 2 已经处于阻塞了，但此时事务 2 又因事务 1 陷入阻塞，因此出现了循环等待，所以事务 2 直接报错、并且终止。而一旦事务 2 终止，那么它施加的行锁就会失效，然后事务 1 就会给 id = 2 施加行锁成功，不再阻塞。

我们可以通过如下命令查看死锁的日志信息：show engine innodb status \G，或者通过配置 innodb_print_all_deadlocks（MySQL 5.6.2 版本开始提供）参数为 ON，将死锁相关信息打印到 MySQL 错误日志中。

原文地址：https://zhuanlan.zhihu.com/p/560890679

作者：linux

# 【NO.128】详解 MySQL 的事务以及隔离级别

## 1.楔子

本次来聊一聊事务，首先事务一般指的是逻辑上的一组操作，或者作为单个逻辑单元执行的一系列操作。同属于一个事务的操作会作为一个整体提交给系统，这些操作要么全部执行成功，要么全部执行失败。

下面就简单地介绍一下事务的特性。

## 2.**事务的特性**

总体来说，事务存在四大特性，分别是原子性（Atomic）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability），因此事务的四大特性又被称为 ACID。

![img](https://pic2.zhimg.com/80/v2-22853dca2119338e4f9602aa972f95cd_720w.webp)

**原子性：**

事务的原子性指的是构成事务的所有操作要么全部执行成功，要么全部执行失败，不会出现部分执行成功，部分执行失败的情况。

例如在转账业务中，张三向李四转账 100 元，于是张三的账户余额减少 100 元，李四的账户余额增加 100 元。在开启事务的情况下，这两个操作要么全部执行成功，要么全部执行失败，不可能出现只将张三的账户余额减少 100 元的操作，也不可能出现只将李四的账户余额增加 100 元的操作。

**一致性：**

事务的一致性指的是事务在执行前和执行后，数据库中已存在的约束不会被打破。

比如余额必须大于等于 0 就是一个约束，而张三余额只有 90 元，这个时候如果转账 100 元给李四，那么之后它的余额就变成了 -10，此时就破坏了数据库的约束。所以数据库认为这个事务是不合法的，因此执行失败。

**隔离性：**

事务的隔离性指的是并发执行的两个事务之间互不干扰，也就是说，一个事务在执行过程中不会影响其它事务运行。

**持久性：**

事务的持久性指的是事务提交完成后，对数据的更改操作会被持久化到数据库中，并且不会被回滚。

例如张三向李四转账，在同一事务中执行扣减张三账户余额和增加李四账户余额操作。事务提交完成后，这种对数据的修改操作就会被持久化到数据库中，且不会被回滚，因为已经被提交了，而回滚是在事务执行之后、事务提交之前发生的。

所以数据库的事务在实现时，会将一次事务中包含的所有操作全部封装成一个不可分割的执行单元，这个单元中的所有操作必须全部执行成功，事务才算成功。只要其中任意一个操作执行失败，整个事务就会执行回滚操作，即自动回滚（当然也可以手动回滚）。但执行成功之后，就无法再回滚了，因为事务已经结束了。

## 3.**如何在 MySQL 中开启事务**

那么在 MySQL 中如何开启一个事务呢？语法如下：

```text
-- 开启事务
-- begin 也可以写成 start transaction 
begin
-- 执行一系列操作，这些操作是一个整体
insert into table(id, name) values (1, 'satori')
update table set name = 'koishi' where id = 1
delete from table where id = 2
-- 执行 commit 表示提交事务
-- 执行 rollback 表示回滚事务
commit / rollback
```

以上这种事务也被称为本地事务，它具有如下特征：

- 一次事务过程中只能连接一个支持事务的数据库，这里的数据库一般指的是关系型数据库；
- 事务的执行结果必须满足 ACID 特性；
- 事务的执行过程会用到数据库本身的锁机制；

本地事务的执行流程如下图所示：

![img](https://pic2.zhimg.com/80/v2-5b783279ce84bab7015de1005f25d395_720w.webp)

- 1）客户端开始事务操作之前，需要开启一个连接会话；
- 2）开始会话后，客户端发起开启事务的指令；
- 3）事务开启后，客户端发送各种 SQL 语句处理数据；
- 4）正常情况下，客户端会发起提交事务的指令，如果发生异常情况，客户端会发起回滚事务的指令；
- 5）上述流程完成后，关闭会话；

本地事务是由资源管理器在本地进行管理的，它的优缺点如下。

**优点：**

- 支持严格的ACID特性，这也是本地事务得以实现的基础；
- 事务可靠，一般不会出现异常情况；
- 本地事务的执行效率比较高；
- 事务的状态可以只在数据库中进行维护，上层的应用不必理会事务的具体状态；
- 应用的编程模型比较简单，不会涉及复杂的网络通信。因为在同一个节点，所以一个 transaction 里面怼几张表都行，很方便。如果是分布式事务，那么还要引入 2PC、3PC、TCC 等模型；

**缺点：**

- 不具备分布式事务的处理能力；
- 一次事务过程中只能连接一个支持事务的数据库，即不能用于多个事务性数据库。比如一个事务里面有两条 SQL 语句，这两条语句操作的一定是同一个数据库；

另外 MySQL 的事务还可以带有保存点，因为 MySQL 的事务一旦回滚，就会回滚到事务执行之前的状态。那么问题来了，我们能不能指定回滚的位置呢？答案是可以的，只需要设置 savepoint 即可。

```text
-- 设置一个名为 point1 的保存点
-- 在 MySQL 中通过如下方式
savepoint point1;

-- 通过如下命令将当前事务回滚到定义的保存点位置
rollback to point1;

-- 通过如下命令删除保存点
release savepoint point1
```

从本质上讲，普通的本地事务也是有保存点的，只不过它只有一个隐式的保存点，并且会在事务启动的时候，自动设置为当前事务的开始位置。也就是说，普通的本地事务具有保存点，而且默认是事务的开始位置。

> 像 MySQL 的这种本地事务，一般也叫做扁平化事务。

## **4.并发事务带来的问题**

数据库一般会并发执行多个事务，而多个事务可能会并发地对相同的数据进行增删改查操作，进而导致并发事务问题。并发事务带来的问题包括更新丢失（脏写）、脏读、不可重复读和幻读，如下图所示。

![img](https://pic4.zhimg.com/80/v2-2bccbed3f11efa380958e374781e1c93_720w.webp)

我们来解释一下这些问题是怎么表现的。

**更新丢失（脏写）**

当两个或两个以上的事务选择数据库中的同一行数据，并基于最初选定的值更新该行数据时，因为多个事务之间都无法感知彼此的存在，所以会出现最后的更新操作覆盖之前由其它事务完成的更新操作的情况。也就是说，对于同一行数据，一个事务对该行数据的更新操作覆盖了其它事务对该行数据的更新操作。

例如张三的账户余额是 100 元，当前有事务 A 和事务 B 两个事务，事务 A 负责将张三的账户余额增加 100 元，事务 B 负责将张三的账户余额增加 200 元。

起初事务 A 和事务 B 同时读取到张三的账户余额为 100 元，然后事务 A 和事务 B 分别更新张三的银行账户余额。假设事务 A 先于事务 B 提交，但最后的结果是张三的账户余额为 300 元。本来应该有 400 元的，因为 A 增加 100、B 增加 200，加上原本的 100。因此这个现象就是脏写，因为后提交的事务 B 覆盖了事务 A 的更新操作，A 的更新操作无效了。

更新丢失（脏写）本质上是写操作的冲突，解决办法是让每个事务按照串行的方式执行，按照一定的顺序依次进行写操作。

**脏读**

一个事务正在对数据库中的一条记录进行修改操作，但在这个事务完成并提交之前，有另一个事务来读取正在修改的这条数据记录。如果没有对这两个事务进行控制，则第二个事务就会读取到没有被提交的脏数据，并根据这些脏数据做进一步的处理，此时就会产生未提交的数据依赖。我们通常把这种现象称为脏读，也就是一个事务读取了另一个事务未提交的数据。

例如当前有事务 A 和事务 B 两个事务，事务 A 向张三的银行账户转账 100 元，事务 B 查询张三的账户余额。事务 A 执行完转账操作、但还没有提交时，事务 B 就查询到张三的银行账户多了 100 元。

后来事务 A 由于某些原因，例如服务超时、系统异常等因素进行回滚操作，张三的余额又变回去了。但事务 B 并不知道，此时我们就说事务 B 发生了脏读，因为事务 B 使用的还是在事务 A 回滚之前就读到的脏数据。

脏读本质上是读写操作的冲突，解决办法是先写后读，也就是写完之后再读。

**不可重复读**

一个事务读取了某些数据，在一段时间后，这个事务再次读取之前读过的数据，此时发现读取的数据发生了变化，这种现象就叫作不可重复读。即同一个事务，使用相同的查询语句，在不同时刻读取到的结果不一致。

例如当前有事务 A 和事务 B 两个事务，事务 A 是向张三的银行账户转账 100 元，事务 B 是查询张三的账户余额。事务 B 第一次查询时，事务 A 还没有转账，第二次查询时，事务 A 已经转账成功，此时就会导致事务 B 两次查询的结果不一致。

不可重复读本质上也是读写操作的冲突，解决办法是先读后写，也就是读完之后再写。

**幻读**

一个事务按照相同的查询条件重新读取数据，发现读到了其它事务插入的满足当前查询条件的新数据，这种现象叫作幻读。即一个事务两次读取一个范围的数据记录，两次读取到的结果不同。

比如一个事务要查询年龄大于 35 的员工数量，第一次查询的时候假设有 100 人，然后别的事务执行了 insert 操作，第二次查询的时候变成了 101 人。此时我们说发生了幻读，这多出来的 1 个就像幻影一样。

幻读本质上是读写操作的冲突，解决办法是先读后写，也就是读完之后再写。

话说很多人不清楚不可重复读和幻读到底有何区别。这里，我们简单介绍一下。

- 不可重复读的重点在于更新和删除操作，而幻读的重点在于插入操作；
- 使用锁机制实现事务隔离级别（一会说）时，在可重复读隔离级别中，SQL 语句第一次读取到数据后，会将相应的数据加锁，使得其他事务无法修改和删除这些数据，此时可以实现可重复读。但这种方法无法对新插入的数据加锁，如果事务 A 读取了数据，或者修改和删除了数据，此时事务 B 还可以进行插入操作，导致事务 A 莫名其妙地多了一条之前没有的数据，这就是幻读；
- 幻读无法通过行级锁来避免，需要使用串行化的事务隔离级别，但是这种事务隔离级别会极大降低数据库的并发能力；
- 从本质上讲，不可重复读和幻读最大的区别在于如何通过锁机制解决问题；

另外，除了可以使用悲观锁（锁的内容后续说）来避免不可重复读和幻读的问题外，我们也可以使用乐观锁来处理，例如，MySQL、Oracle 和 PostgreSQL 等数据库为了提高整体性能，就使用了基于乐观锁的MVCC（多版本并发控制）机制来避免不可重复读和幻读。

## 5.**MySQL 的事务隔离级别**

MySQL 中的 InnoDB 储存引擎提供 SQL 标准所描述的 4 种事务隔离级别，分别为：读未提交（Read Uncommitted）、读已提交（Read Committed）、可重复读（Repeatable Read）和串行化（Serializable）。

MySQL 默认的隔离级别为可重复读，当然我们也可以手动指定隔离级别，通过在 my.cnf 或者 my.ini 文件中的 mysqld 节点下面配置如下选项。

```text
-- 读未提交
transaction-isolation = READ-UNCOMMITTED
-- 读已提交
transaction-isolation = READ-COMMITTED 
-- 可重复读
transaction-isolation = REPEATABLE-READ 
-- 串行化
transaction-isolation = SERIALIZABLE
```

也可以使用 SET TRANSACTION 命令改变单个或者所有新连接的事务隔离级别，基本语法如下所示。

```text
-- 读未提交
SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL READ UNCOMMITTED
-- 读已提交
SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL READ COMMITTED 
-- 可重复读
SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL REPEATABLE READ 
-- 串行化
SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL SERIALIZABLE
```

如果使用 SET TRANSACTION 命令来设置事务隔离级别，需要注意以下几点。

- 不带 SESSION 或 GLOBAL 关键字设置事务隔离级别，指的是为下一个（还未开始的）事务设置隔离级别；
- 使用 GLOBAL 关键字指的是对全局设置事务隔离级别，也就是设置后的事务隔离级别对所有新产生的数据库连接生效；
- 用 SESSION 关键字指的是对当前的数据库连接设置事务隔离级别，此时的事务隔离级别只对当前连接的后续事务生效；
- 任何客户端都能自由改变当前会话的事务隔离级别，可以在事务中间改变，也可以改变下一个事务的隔离级别；

使用如下命令可以查询全局级别和会话级别的事务隔离级别。

```text
SELECT @@global.tx_isolation;
SELECT @@session.tx_isolation;
SELECT @@tx_isolation;
```

而这些隔离级别主要是为了解决并发事务带来的问题，并且不同的隔离级别所解决的问题范围也不同。

![img](https://pic2.zhimg.com/80/v2-91682a70ae705dbc44bc82962f316115_720w.webp)

- 读未提交允许脏读，即在读未提交的隔离级别下，可能读取到其它会话未提交事务修改的数据。这种事务隔离级别下存在脏读、不可重复读和幻读的问题；
- 读已提交只能读取到已提交事务修改的数据，Oracle 等数据库使用的默认事务隔离级别就是读已提交。这种事务隔离级别存在不可重复读和幻读的问题；
- 可重复读就是在一个事务内，同一查询无论何时执行，得到的数据都是一致的，这是 MySQL 中 InnoDB 存储引擎默认的事务隔离级别。这种事务隔离级别下存在幻读的问题；
- 串行化是指完全串行读写，每次操作数据库中的数据时，都需要获得表级别的共享锁，造成读和写都会阻塞。这种事务隔离级别解决了并发事务带来的问题，但完全的串行化操作使得数据库失去了并发特性，所以这种隔离级别往往在互联网行业中不太常用；

接下来，为了更好地理解 MySQL 的事务隔离级别，我们实际说明一下。

## 6.**隔离级别代码演示**

先在 MySQL 中创建一个 test 数据库，在 test 数据库中创建一个 account 表作为测试使用的账户数据表，然后写入几条数据，如下所示。

![img](https://pic1.zhimg.com/80/v2-3fbf3aba30961e962a97c9d8440de798_720w.webp)

此时 account 数据表中有张三、李四和王五的账户信息，账户余额分别为 300 元、350 元和 500 元。准备工作完成了，接下来我们一起来看 MySQL 中每种事务隔离级别下数据的处理情况。

### **6.1 读未提交**

第一步：打开第一个服务器终端，登录 MySQL，将当前终端的事务隔离级别设置为 read uncommitted，也就是读未提交，然后查询数据表。

![img](https://pic1.zhimg.com/80/v2-66e5ea371c526be1fb453651286c7738_720w.webp)

第二步：打开服务器的另一个终端（后续就简化为终端 1、终端 2），连接 MySQL，将当前事务模式设置为 read uncommitted 并更新 account 表的数据，将张三的账户余额加 100 元。

![img](https://pic2.zhimg.com/80/v2-a0ebf8a5c5aad16246c66a9fd74fcc71_720w.webp)

可以看到，在终端 2 中，当前事务未提交时，张三的账户余额变为更新后的值，即 400 元。

第三步：在终端 1 查看 account 数据表的数据。

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='747' height='624'></svg>)

可以看到，虽然终端 2 的事务并未提交，但是终端 1 可以查询到终端 2 已经更新的数据。

第四步：如果终端 2 的事务由于某种原因执行了回滚操作，那么终端 2 中执行的所有操作都会被撤销。也就是说，终端 1 查询到的数据其实就是脏数据，下面我们执行回滚。

![img](https://pic2.zhimg.com/80/v2-8a8040409f379369ad90351df8191639_720w.webp)

在终端 2 执行了事务的回滚操作后，张三的账户余额重新变为 300 元，那么终端 1 查询返回的是不是也是 300 元呢？

第五步：在终端 1 查询张三的账户余额，发现也变成了 300 元（图就不贴了）。

以上便是脏读的问题，因为你不知道另一个事务的操作最终是提交还是回滚。就好比原来张三的余额有 300，但是事务 1 发现变成了 400，因为事务 2 给它加了 100，于是准备减去 100 只留 300。

但问题是事务 2 的操作还没提交呢？如果事务 2 回滚了自己的操作，那么事务 1 再减去 100 的话，张三的余额就变成 200 了。所以万恶之源还是「读未提交」这个隔离级别允许一个事务读取另一个事务在执行过程中所做的变更，因此「读未提交」基本不会在生产环境上使用。

### **6.2 读已提交**

第一步：在终端 1 中将事务隔离级别设置为 read committed，也就是读已提交。

![img](https://pic2.zhimg.com/80/v2-2626c42bba117cfbb61541aaff247841_720w.webp)

和之前一样，张三、李四和王五的账户余额分别为 300 元、350 元和 500 元。

第二步：打开终端 2，将事务隔离级别设置为 read committed，开启事务并更新 account 数据表中的数据，将张三的账户余额增加 100 元。

![img](https://pic2.zhimg.com/80/v2-3a41af30cf7a9350f5c506b15dce7bf1_720w.webp)

可以看到，在终端 2 的查询结果中，张三的账户余额已经由原来的 300 元变成 400 元。

第三步：在终端 2 的事务提交之前，在终端 1 中查询 account 数据表中的数据，如下所示。

![img](https://pic1.zhimg.com/80/v2-39f1f26ecb437525d4544c6b687ee978_720w.webp)

可以看到，在终端 1 查询出来的张三的账户余额仍为 300 元，说明此时已经解决了脏读的问题。

第四步：在终端 2 提交事务，如下所示。

```text
mysql> commit;
Query OK, 0 rows affected (0.03 sec)
```

第五步：在终端 2 提交事务后，在终端 1 再次查询 account 数据表中的数据，如下所示。

![img](https://pic4.zhimg.com/80/v2-7c0923916828cbfd76888d3bd7642967_720w.webp)

可以看到，此时就不会出现脏读的问题了，在「读已提交」隔离级别下，一个事务必须提交之后，所做的修改才能被另一个事务读取。

但此时又产生了一个问题，终端 1 在终端 2 的事务提交前和提交后读取到的数据不一致，产生了不可重复读的问题。而我们希望在一个事务内，不管什么时候读取，读到的数据是不变的，要想解决这个问题，就需要使用可重复读的事务隔离级别。

### **6.3 可重复读**

第一步：在终端 1 中将事务隔离级别设置为 repeatable read，也就是可重复读。

![img](https://pic1.zhimg.com/80/v2-a8fc5a33d4516c3fcc16ceece8d12ef8_720w.webp)

可以看到，此时张三、李四、王五的账户余额分别为 400 元、350 元、500 元。

第二步：打开终端 2，将当前终端的事务隔离级别设置为可重复读。开启事务，将张三的账户余额增加 100 元，随后提交事务，如下所示。

![img](https://pic4.zhimg.com/80/v2-5dbf9b56cb862ceea407e6503e4af85f_720w.webp)

可以看到，在终端 2 查询的结果中，张三的账户余额已经由原来的 400 元变成 500 元。

第三步：在终端 1 查询 account 数据表中的数据，如下所示。

![img](https://pic2.zhimg.com/80/v2-1cad87f30eed2969b03e6f54a99e26e9_720w.webp)

可以看到在终端 1 查询的结果中，张三的账户余额仍为 400 元，并没有出现不可重复读的问题，说明可重复读的事务隔离级别解决了不可重复读的问题。

第四步：在终端 1 为张三的账户增加 100元，如下所示。

![img](https://pic1.zhimg.com/80/v2-d7c53a1f304b1c3cee11f24df0da7de8_720w.webp)

事务 1 查询的结果是 400 元，然后增加 100 元，但此时张三的账户余额却变成 600 元，而不是 500 元，说明数据的一致性没有遭到破坏。这是因为在终端 1 为张三的账户余额增加 100 元之前，终端 2 已经为张三的账户余额增加了 100 元，共计增加了 200 元，所以最终张三的账户余额是 600 元。

> 可重复读的隔离级别使用了MVCC（Multi-Version Concurrency Control，多版本并发控制）机制，数据库中的查询（select）操作不会更新版本号，是快照读，而操作数据表中的数据（insert、update、delete）则会更新版本号，是当前读。关于 MVCC 后续详细说。

第五步：在终端 2 开启事务，插入一条数据后提交事务，如下所示。

![img](https://pic2.zhimg.com/80/v2-0c9a05a72dde7b4608043eb48e44b831_720w.webp)

在终端 2 查询的结果中，已经显示出新插入的赵六的账户信息了。

第六步：在终端 1 查询 account 数据表的数据，如下所示。

![img](https://pic4.zhimg.com/80/v2-ab575b7f672aee07743b426f15d130c3_720w.webp)

在终端 1 查询的数据中，并没有赵六的账户信息，说明没有出现幻读。

第七步：在终端 1 为 id = 4 的账户增加 100 元，按理说由于该事务没有查到 id = 4 的记录，所以应该什么也不会发生。

![img](https://pic2.zhimg.com/80/v2-68fe00f01de61cf2d4a12385d0ef0299_720w.webp)

可以看到，在终端 1 执行完数据更新操作后，查询到赵六的账户信息，出现了幻读的问题。如何解决该问题呢？答案是使用串行化的事务隔离级别或者间隙锁和临键锁。

> 目前出现了很多的锁的概念，关于锁我们后续会说。

### **6.4 串行化**

第一步：在终端 1 中将事务隔离级别设置为 serializable，也就是串行化。

![img](https://pic3.zhimg.com/80/v2-b1e900055cdb3abb5dd9b838dfe4383a_720w.webp)

第二步：打开终端 2，将当前终端的事务隔离级别设置为 serializable，开启事务，修改 account 数据表中 id 为 1 的数据，如下所示。

![img](https://pic3.zhimg.com/80/v2-780c59ace5b8fff4377fd87b0db333f2_720w.webp)

可以看到，在终端 2 中对 id 为 1 的数据执行更新操作时，会发生阻塞。因为所有事务操作都是串行的，终端 1 的事务执行完毕之前，终端 2 的事务是无法执行的。

MySQL 背后的做法是通过锁来保证串行的，因此终端 2 的事务想要执行必须获取锁，但锁此时被终端 1 的事务持有，因此终端 2 的事务只能陷入等待。如果锁超时，会抛出 "ERROR 1205(HY000): Lock wait timeout exceeded: try restarting transaction" 错误。因此采用串行化的方式可以避免幻读，但它是最高的隔离级别，此时完全丧失了并发性，生产环境也很少使用。

另外，在可重复读的事务隔离级别下，如果终端 1 执行的是一个范围查询，那么该范围内的所有行都会被加锁。比如查询 id 为 2 到 10 的数据，那么整个范围都会被加锁，即使存在间隙（比如不存在 id = 4 的记录，那么也会被加锁，这就是间隙锁，后续会详细讲解）。此时终端 2 在此范围内插入数据（比如 id = 4），就会被阻塞，从而也可以避免幻读。

## **7.小结**

MySQL 在并发处理事务时，会面临一系列问题，为了解决这些问题，MySQL 也提供了一系列隔离级别。不同的隔离级别，解决问题的程度不同，MySQL 的默认隔离级别是可重复读。

原文地址：https://zhuanlan.zhihu.com/p/560641722

作者：linux

# 【NO.129】glibc内存管理那些事儿

## 1.**Linux内存空间简介**

32位Linux平台下进程虚拟地址空间分布如下图:

![img](https://pic3.zhimg.com/80/v2-7a3c89176f43705baf5db809e469422a_720w.webp)

进程虚拟地址空间分布

图中，0xC0000000开始的最高1G空间是内核地址空间，剩下3G空间是用户态空间。用户态空间从上到下依次为stack栈(向下增长)、mmap(匿名文件映射区)、Heap堆(向上增长)、bss数据段、数据段、只读代码段。

其中，Heap区是程序的动态内存区，同时也是C++内存泄漏的温床。`malloc`、`free`均发生在这个区域。本文将简单介绍下glibc在动态内存管理方面的机制，抛砖引玉，希望能和大家多多交流。

Linux提供了如下几个系统调用，用于内存分配：

```text
brk()/sbrk() // 通过移动Heap堆顶指针brk，达到增加内存目的 
mmap()/munmap() // 通过文件影射的方式，把文件映射到mmap区
```

> 这两种方式分配的都是虚拟内存，没有分配物理内存。在第一次访问已分配的虚拟地址空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系。

那么，既然`brk、mmap`提供了内存分配的功能，直接使用`brk、mmap`进行内存管理不是更简单吗，为什么需要glibc呢？ 我们知道，系统调用本身会产生软中断，导致程序从用户态陷入内核态，比较消耗资源。试想，如果频繁分配回收小块内存区，那么将有很大的性能耗费在系统调用中。因此，为了减少系统调用带来的性能损耗，glibc采用了内存池的设计，增加了一个代理层，每次内存分配，都优先从内存池中寻找，如果内存池中无法提供，再向操作系统申请。

> 一切计算机的问题都可以通过加**层**的方式解决。

## 2.**glibc的内存分配回收策略**

glibc中`malloc`内存分配逻辑如下是:

![img](https://pic3.zhimg.com/80/v2-f67f16bd2cd5c9e6aa45f455c2f894b2_720w.webp)

malloc

- 分配内存 < `DEFAULT_MMAP_THRESHOLD`，走__brk，从内存池获取，失败的话走brk系统调用
- 分配内存 > `DEFAULT_MMAP_THRESHOLD`，走__mmap，直接调用mmap系统调用

其中，`DEFAULT_MMAP_THRESHOLD`默认为128k，可通过`mallopt`进行设置。 重点看下小块内存(`size > DEFAULT_MMAP_THRESHOLD`)的分配，glibc使用的内存池如下图示：

![img](https://pic1.zhimg.com/80/v2-ad0b92494a7745abac1d29e417993bdc_720w.webp)

**内存池**

内存池保存在bins这个长128的数组中，每个元素都是一双向个链表。其中：

- bins[0]目前没有使用
- bins[1]的链表称为`unsorted_list`，用于维护free释放的chunk。
- bins[2,63)的区间称为`small_bins`，用于维护＜512字节的内存块，其中每个元素对应的链表中的chunk大小相同，均为index*8。
- bins[64,127)称为`large_bins`，用于维护>512字节的内存块，每个元素对应的链表中的chunk大小不同，index越大，链表中chunk的内存大小相差越大，例如: 下标为64的chunk大小介于[512, 512+64)，下标为95的chunk大小介于[2k+1,2k+512)。同一条链表上的chunk，按照从小到大的顺序排列。

> chunk数据结构

![img](https://pic4.zhimg.com/80/v2-fbbc9af5f78e1d93afa5a65cdebec44b_720w.webp)

chunk结构

glibc在内存池中查找合适的chunk时，采用了**最佳适应**的伙伴算法。举例如下：

1、如果分配内存<512字节，则通过内存大小定位到smallbins对应的index上(`floor(size/8)`)

- - 如果smallbins[index]为空，进入步骤3
  - 如果smallbins[index]非空，直接返回第一个chunk

2、如果分配内存>512字节，则定位到largebins对应的index上

- - 如果largebins[index]为空，进入步骤3
  - 如果largebins[index]非空，扫描链表，找到第一个大小最合适的chunk，如size=12.5K，则使用chunk B，剩下的0.5k放入unsorted_list中

3、遍历unsorted_list，查找合适size的chunk，如果找到则返回；否则，将这些chunk都归类放到smallbins和largebins里面

4、index++从更大的链表中查找，直到找到合适大小的chunk为止，找到后将chunk拆分，并将剩余的加入到unsorted_list中

5、如果还没有找到，那么使用top chunk

6、或者，内存<128k，使用brk；内存>128k，使用mmap获取新内存

> **top chunk** 如下图示: `top chunk`是堆顶的chunk，堆顶指针brk位于top chunk的顶部。移动brk指针，即可扩充top chunk的大小。**当`top chunk`大小超过128k(可配置)时，会触发`malloc_trim`操作，调用`sbrk(-size)`将内存归还操作系统**。

![img](https://pic2.zhimg.com/80/v2-79597291f54a731e558346d18d2b8429_720w.webp)

chunk分布图

`free`释放内存时，有两种情况：

1. chunk和top chunk相邻，则和top chunk合并
2. chunk和top chunk不相邻，则直接插入到`unsorted_list`中

## 3.**内存碎片**

以上图chunk分布图为例，按照glibc的内存分配策略，我们考虑下如下场景(假设brk其实地址是512k)：

1. malloc 40k内存，即chunkA，brk = 512k + 40k = 552k
2. malloc 50k内存，即chunkB，brk = 552k + 50k = 602k
3. malloc 60k内存，即chunkC，brk = 602k + 60k = 662k
4. free chunkA。

此时，由于brk = 662k，而释放的内存是位于[512k, 552k]之间，无法通过移动brk指针，将区域内内存交还操作系统，因此，在[512k, 552k]的区域内便形成了一个内存空洞 ---- 内存碎片。 按照glibc的策略，free后的chunkA区域由于不和top chunk相邻，因此，无法和top chunk 合并，应该挂在`unsorted_list`链表上。

## 4.**glibc实现的一些重要结构**

glibc中用于维护空闲内存的结构体是`malloc_state`，其主要定义如下:

```text
struct malloc_state {
    mutex_t mutex; // 并发编程下锁的竞争
    mchunkptr        top; // top chunk
    unsigned int     binmap[BINMAPSIZE]; // bitmap，加快bins中chunk判定
    mchunkptr        bins[NBINS * 2 - 2]; // bins，上文所述
    mfastbinptr      fastbinsY[NFASTBINS]; // fastbins，类似bins，维护的chunk更小(80字节的chunk链表)
...
}
static struct malloc_state main_arena; // 主arena
```

## 5.**多线程下的竞争抢锁**

并发条件下，`main_arena`引发的竞争将会成为限制程序性能的瓶颈所在，因此glibc采用了多arena机制，线程A分配内存时获取`main_arena`锁成功，将在`main_arena`所管理的内存中分配；此时线程B获取`main_arena`失败，glibc会新建一个arena1，此次内存分配从arena1中进行。 这种策略，一定程度上解决了多线程下竞争的问题；但是随着arena的增多，内存碎片出现的可能性也变大了。例如，main_arena中有10k、20k的空闲内存，线程B要获取20k的空闲内存，但是获取main_arena锁失败，导致留下20k的碎片，降低了内存使用率。

普通arena的内部结构:

![img](https://pic2.zhimg.com/80/v2-fdb1a7864a322cb5d0c1eefec32ea871_720w.webp)

普通arena结构

1. 一个arena由多个Heap构成
2. 每个Heap通过mmap获得，最大为1M，多个Heap间可能不相邻
3. Heap之间有prev指针指向前一个Heap
4. 最上面的Heap，也有top chunk

每个Heap里面也是由chunk组成，使用和main_arena完全相同的管理方式管理空闲chunk。 多个arena之间是通过链表连接的。如下图:

![img](https://pic4.zhimg.com/80/v2-6397a080bbd3172c28f59fcd8e6d9d4f_720w.webp)

arena链表

> **main arena和普通arena的区别** `main_arena`是为一个使用brk指针的arena，由于brk是堆顶指针，一个进程中只可能有一个，因此普通arena无法使用brk进行内存分配。普通arena建立在mmap的机制上，内存管理方式和`main_arena`类似，只有一点区别，普通arena只有在整个arena都空闲时，才会调用`munmap`把内存还给操作系统。

## 6.**一些特殊情况的分析**

根据上文所述，glibc在调用`malloc_trim`时，需要满足如下2个条件：

```text
1. size(top chunk) > 128K
2. brk = top chunk->base + size(top chunk)
```

假设，brk指针上面的空间已经被占用，无法通过移动brk指针获得新的地址空间，此时main_arena就无法扩容了吗？ glibc的设计考虑了这样的特殊情况，此时，glibc会换用mmap操作来获取新空间(每次最少MMAP_AS_MORECORE_SIZE<1M>)。这样，main_arena和普通arena一样，由非连续的Heap块构成，不过这种情况下，glibc并未将这种mmap空间表示为Heap，因此，main_arena多个块之间是没有联系的，这就导致了**main_arena从此无法归还给操作系统，永远保留在空闲内存中了**。如下图示：

![img](https://pic2.zhimg.com/80/v2-af13315ffcb11f753debfed6c663d4d9_720w.webp)

main_arena无法回收

显而易见，此时根本不可能满足调用malloc_trim的条件2，即:`brk !== top chunk->base + size(top chunk)`，因为此时brk处于堆顶，而`top chunk->base > brk`.

```text
#include <stdlib.h>
#include <stdio.h>
#include <string.h>
#include <unistd.h>
#include <sys/mman.h>
#include <malloc.h>

#define ARRAY_SIZE 127
char cmd[1024];

void print_info()
{
    struct mallinfo mi = mallinfo();           
    system(cmd);
    printf("\theap_malloc_total=%lu heap_free_total=%lu heap_in_use=%lu\n\
            \tmmap_total=%lu mmap_count=%lu\n", mi.arena, mi.fordblks, mi.uordblks, mi.hblkhd, mi.hblks);
}

int main(int argc, char** argv)
{
    char** ptr_arr[ARRAY_SIZE];
    int i;
    char*  mmap_var;
    pid_t  pid;
    pid = getpid();
    sprintf(cmd, "ps aux | grep %lu | grep -v grep", pid);
    /* mmap占据堆顶后1M的地址空间 */
    mmap_var = mmap((void*)sbrk(0) + 1024*1024, 127*1024, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0); 
    printf("before malloc\n");
    print_info();

    /* 分配内存，总大小超过1M，导致main_arena被拆分 */
    for( i = 0; i < ARRAY_SIZE; i++) {
        ptr_arr[i] = malloc(i * 1024);
    }   
    printf("\nafter malloc\n");
    print_info();
    /* 释放所有内存，观察内存使用是否改变 */
    for( i = 0; i < ARRAY_SIZE; i++) {
        free(ptr_arr[i]);
    }
    printf("\nafter free\n");
    print_info();
    munmap(mmap_var, 127*1024);
    return 1;
}
```

![img](https://pic2.zhimg.com/80/v2-9edf3371b2e413f0c8eb3d952e72c011_720w.webp)

异常运行

作为对比，去除掉brk上面的mmap区再次运行后结果如下：

![img](https://pic4.zhimg.com/80/v2-322302901aa8b62db6ed9aa1b51243ff_720w.webp)

正常运行

可以看出，异常情况下(brk无法扩展)，free的内存没有归还操作系统，而是留在了main_arena的unsorted_list了；而正常情况下，由于满足执行`malloc_trim`的条件，因此，free后，调用了`sbrk(-size)`把内存归还了操作系统，main_arena内存响应减少。

原文地址：https://zhuanlan.zhihu.com/p/560341532

作者：linux



# 【NO.130】红黑树 与 B+树区别和应用场景

## **1.红黑树**

红黑树是每个节点都带有颜色属性的二叉查找树，颜色或红色或黑色。再二叉查找树强制一般要求以外，对于任何有效的红黑树我们增加了如下的额外要求:

\1. 节点是红色或黑色

\2. 根节点是黑色。

3 每个叶节点（NIL节点，空节点）是黑色的。

4 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点)

\5. 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。

## **2.红黑树和avl(二叉平衡树)的比较**

\1. 如果插入一个node引起了树的不平衡，AVL和RB-Tree(红黑树)都是最多只需要2次旋转操作，即两者都是O(1)；但是在删除node引起树的不平衡时，最坏情况下，AVL需要维护从被删node到root这条路径上所有node的平衡性，因此需要旋转的量级O(logN)，而RB-Tree最多只需3次(因为不需要严格的平衡，从根到叶子的最长的可能路径不多于最短的可能路径的两倍长)旋转以及修改节点的颜色，只需要O(1)的复杂度。

\2. 其次，AVL的结构相较RB-Tree来说更为平衡，在插入和删除node更容易引起Tree的unbalance，因此在大量数据需要插入或者删除时，AVL需要rebalance的频率会更高。因此，RB-Tree在需要大量插入和删除node的场景下，效率更高。自然，由于AVL高度平衡，因此AVL的search效率更高。

## 3.**红黑树实际应用**：

IO多路复用epoll的实现采用红黑树组织管理sockfd，以支持快速的增删改查.
ngnix中,用红黑树管理timer,因为红黑树是有序的,可以很快的得到距离当前最小的定时器.
java中TreeMap，jdk1.8的hashmap的实现.

### 3.1.**B+树**

B+ 树是一种树数据结构，是一个n叉排序树，每个节点通常有多个孩子，一棵B+树包含根节点、内部节点和叶子节点。根节点可能是一个叶子节点，也可能是一个包含两个或两个以上孩子节点的节点。

（ ps:举例说明3阶B-树指的是每个结点最多2个关键字，3个孩子)

**B+**树是对B树的一种变形树，它与B树的差异在于：

- 有k个子结点的结点必然有k个关键码；
- 非叶结点仅具有索引作用，跟记录有关的信息均存放在叶结点中。
- 树的所有叶结点构成一个有序链表，可以按照关键码排序的次序遍历全部记录，便于区间查找和遍历。
- B+ 树的优点在于：由于B+树在内部节点上不包含数据信息，因此在内存页中能够存放更多的key。 数据存放的更加紧密，具有更好的空间局部性。因此访问叶子节点上关联的数据也具有更好的缓存命中率。B+树的叶子结点都是相连的，因此对整棵树的便利只需要一次线性遍历叶子结点即可。而且由于数据顺序排列并且相连，所以便于区间查找和搜索。而B树则需要进行每一层的递归遍历。相邻的元素可能在内存中不相邻，所以缓存命中性没有B+树好。但是B树也有优点，其优点在于，由于B树的每一个节点都包含key和value，因此经常访问的元素可能离根节点更近，因此访问也更迅速。下面是B 树和B+树的区别图：

![img](https://pic2.zhimg.com/80/v2-0b69ce0bbe57d6c35db33c5eb03ecff1_720w.webp)

### **3.2.b+树的应用场景:**

B/B+树是为了磁盘或其它存储设备而设计的一种平衡多路查找树(相对于二叉,B树每个内节点有多个分支),与红黑树相比,在相同的的节点的情况下,一颗B/B+树的高度远远小于红黑树的高度(在下面B/B+树的性能分析中会提到).B/B+树上操作的时间通常由存取磁盘的时间和CPU计算时间这两部分构成,而CPU的速度非常快,所以B树的操作效率取决于访问磁盘的次数,关键字总数相同的情况下B树的高度越小，磁盘I/O所花的时间越少.
二叉查找树的结构不适合数据库，因为它的查找效率与层数相关。越处在下层的数据，就需要越多次比较。对于数据库来说，每进入一层，就要从硬盘读取一次数据，这非常致命，因为硬盘的读取时间远远大于数据处理时间，数据库读取硬盘的次数越少越好。这种数据结构，非常有利于减少读取硬盘的次数。假定一个节点可以容纳100个值，那么3层的B树可以容纳100万个数据，如果换成二叉查找树，则需要20层！假定操作系统一次读取一个节点，并且根节点保留在内存中，那么B树在100万个数据中查找目标值，只需要读取两次硬盘。

原文链接：https://zhuanlan.zhihu.com/p/217875063

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.131】C/C++ Linux 后台服务器开发高级架构师学习知识路线总结

前言：

小编也是从事c方面10多年的工作经验、今天跟大家分享一下我总结出来的一系列 C/C Linux后台服务器开发的学习路线。从Linux开发工程师-Linux后台开发工程师-Linux高级互联网架构师。

想必大家都知道从事后台开发首先就是要选择一种语言，小编今天跟大家分享是用C/C++ 做的后台开发。所以想从事这方面的朋友得有C/C++的基础。

![img](https://pic2.zhimg.com/80/v2-6586beb7e3fe0b7335f722ee264504c5_720w.jpg)

首先跟大家说的是从学习步骤：（Linux入门到精通篇）

## 1.Linux开发环境

1.了解Linux环境搭建，了解LinuxC编程

2.了解Linux安装，命令使用，shell编程

3.shell脚本实现检测局域网内哪些ip地址机器宕机

## 2.Linux C编程

1.Linux C编程 统计文件单词数量

包括：文件操作、文件指针

2.Linux C编程 实现通讯录

包括：结构体

## 3.Linux环境编程

1.并发下的计数方案

包括：互斥锁、自旋锁、原子操作

2.实现线程池

包括：线程队列，任务队列，条件变量

3.CPU与进程的关系

包括：进程操作，进程与CPU粘合，进程通信

4.数据库操作

包括：数据库封装，sql语句封装，网络连接封装

## 4.网络编程

1.DNS请求器

包括：UDP通信，DNS协议，协议解析

2.实现http请求器 TCP客户端

包括：TCP编程，HTTP请求协议

3.百万级并发服务器 TCP服务器

包括：tcp，网络io，Linux系统

**总结：把以上知识点内容掌握之后你的Linux就已经比较成熟了，达到了一个Linux开发工程师的水平了。**

![img](https://pic1.zhimg.com/80/v2-bf8e7ff84a673171a747eaf82d44afc0_720w.webp)

熟练掌握上面的知识点后就可以来了解一下后面的知识点了：（Linux后台开发篇）

## 5.算法于设计

千里之行，始于足下。不积跬步，无以致千里。既能仰望星空又能脚踏实地

1.排序与查找

包括：插入排序、快速排序、希尔排序、桶排序、基数排序、归并排序

2.常用算法

包括：布隆过滤器、字符串匹配KMP算法、回溯算法、贪心算法、推荐算法、深度 广度优先

3.常用的数据结构

包括：平衡二叉树、红黑树、B-树、KMP算法、栈/队列

4.常用设计模式

包括：单列模式、责任链模式、过滤器模式、发布订阅模式、代理模式、工厂模式

## 6.后台组件编程

工欲善其事，必先利其器。后台组件是开发的入门石

\1. 持久化 MySQL

包括：MySQL安装配置与远程连接、数据操作源于SQL语句、存储过程与事务处理、SQL函数，运算，临时表、防数据丢失 备份与恢复、MySQL建库建表建索引

2.消息队列 ZeroMQ

包括：ZMQ编译安装与开发环境搭建、publisher-subscriber模式实现、request-response模式实现、Router-Dealer模式实现、消息队列—性能分析

3.缓存 Redis

包括： Redis编译安装配置、客户端全局唯一ID保存机制、Redis消息队列机制 发布订阅、Redis事务实战、Redis安全性能，数据备份与恢复、Redis分布式锁详解

\4. 反向代理 Nginx

包括： Nginx开发介绍、反向代理负载均衡配置详解、自定义协议upstream开发、子域名映射、服务器后台攻击预防、nginx双虚拟主机

\5. Restful Http

包括：Http第三方接口实现、异步Http请求、ngrok与Restlet、长连接与短链接

\6. 协调服务 ZooKeeper

包括：ZK编译安装与C API开发环境、集群管理与服务注册、节点创建与监控、分布式锁的实现、ZK伪集群部署与服务管理

7.NoSQL MongoDB

包括：MongDB安装与开发介绍、MongoDB备份与恢复、MongoDB文档操作、全文检索与正则表达式、MongoDB建库建集合

## 7.代码工程化

优秀的工程师有优秀的代码组织能力与代码迭代能力。

1.架构工程

包括：工程参数配置与编译 cmake、代码规范与命名规则、文件命名与变量命名规则、脚本配置工具 autoconf、代码工程组织架构 Makefile

\2. 管理代码

包括： 分布式版本控制系统 git、远程仓库，标签管理、 github与码云、创建仓库，导入，checkout、svn环境搭建与原理、 分支管理 冲突解决、产品代码版本管理 SVN

## 8.网络服务

网络IO是网络通信的血管，数据是血液。血液的流动是不能离开血管的。

1.源码实现

包括：服务器IO核心— epoll编程实战、客户端多网络连接机制poll、文件IO管理select

2.框架

包括：高性能的时间循环 libev、跨平台异步I/O libuv、跨平台的C++库 Boost.Asio、事件通知库 libevent

3.理论

包括：阻塞型 BIO、异步IO AIO、非阻塞型IO NIO

## 9.开源框架

欲穷千里目，更上一层楼。站在巨人的肩膀上，看到窗外的景色。

1.TCP协议栈

包括：基于DPDK的高性能用户态协议栈 f-stack、基于Netmap单线程协议栈 NtyTcp、精简版tcp协议栈 LWIP

2.并发性

包括：用OpenCL的C++ GPU计算库 Boost.Compute、Intel线程构件块 Intel TBB、并行编程的异构系统的开放标准 OpenCL、C++11的反应性编程库 C++ React

\3. 数据库

包括：Redis数据库的C客户端库 hiredis、Facebook的嵌入键值的快速存储 RocksDB、用于Sqlite3的C++对象关系映射 hiberlite

\4. 国际化

包括：Unicode 和全球化支持的C、C++ 和Java库 IBM ICU、不同字符编码之间的编码转换库 libiconv、GNU gettext

5.压缩

包括：非常紧凑的数据流压缩库 Zlib、快速压缩和解压缩 Snappy、非常快速的压缩算法 LZ4、单一的C源文件，紧缩/膨胀压缩库 Miniz

6.日志

包括：设计非常模块化，并且具有扩展性 Boost.Log、灵活添加日志到文件，系统日志 Log4cpp、添加日志到你的C++应用程序 templog、C++日志库，只包含单一的头文件 easyloggingpp

7.多媒体库

包括：开源音频库—跨平台的音频API OpenAL、网络实时流媒体通信 WebRTC、音频和音乐数字信号处理库 Maximilian、C++易用和高效的音频合成 Tonic

\8. 序列化

包括：快速数据交换格式和RPC系统 Cap’n Proto、协议缓冲，谷歌的数据交换格式 ProtoBuf、高效的跨语言IPC/RPC Thrift、内存高效的序列化库 FlatBuffers

9.XML库

包括：Gnome的xml C解析器和工具包 LibXml2、单快速的C++CML解析器 TinyXML2、简单快速的XML解析器 PugiXML、C++的xml解析器 LibXml++

10.脚本

包括：小型快速脚本引擎 Lua、谷歌的快速JavaScript引擎 V8、嵌入式脚本语言 ChaiScript、

11.Json库

包括：进行编解码和处理Jason数据的C语言库 Jansson、C语言中的JSON解析和打印库 ibjson、轻量级的JSON库 libjson、C/C++的Jason解析生成器 Frozen

12.数学库

包括：高质量的C++线性代数库 Armadillo、数学图形模板库 GMTL、用于个高精度计算的C/C++库 GMP、高级C++模板头文件库 Eigen

13.安全

包括：SSL，TLS和DTLS协议的安全通信库 GnuTLS、功能齐全的，开源加密库 Openssl、有关加密方案的免费的C++库 Cryto++

14.Web应用框架

包括：安全快速开源Web服务器 Lighttpd、于Qt库的web框架 QDjango、高性能的HTTP和反向代理web服务器 Nginx

15.网络库

包括：C异步网络开发库 Dyad.c、多协议文件传输库 Curl、高速模块化的异步通信库 ZeroMQ、C++面向对象网络工具包 ACE

16.异步事件

包括：事件通知库 libevent、 跨平台异步I/O libuv、功能齐全，高性能的时间循环 libev、网络和底层I/O编程的跨平台的C++库 Boost.Asio

17.协程

包括：纯c版的协程框架 ntyco、C++11实现协程库, golang风格 libgo、微信支持8亿用户同时在线的底层IO库 libco

## 10.性能测试

学而不思则罔，思而不学则殆。从技术反馈中理解知识的原理。

1.调试库

包括：Boost测试库 Boost.Test、内存调试性能分析工具 Valgrind、谷歌C++测试框架 GoogleTest、内存分配跟踪库 MemTrack

2.测试库

包括：单元测试框架 minUnit、测试用例编写 libtap、轻量级的C++单元测试框架 UnitTest++、自动化测试用例 gtest和luatest

3.性能工具

包括：高性能代码构建系统 tundra、Http压测工具 WRK、 网站压测工具 webbench、高性能构建系统 FASTBuild

## 11.Linux系统

上帝关闭一扇门，就会打开一扇窗，Linux是程序员世界的另一扇窗。

1.系统命令工具

包括：进程间通信设施状态 ipcs、Linux系统运行时长 uptime、CPU平均负载和磁盘活动 iostat、监控，收集和汇报系统活动 sar、监控多处理器使用情况 mpstat、监控进程的内存使用情况 pmap、系统管理员调优和基准测量工具 nmon、密切关注Linux系统 glances、查看系统调用 strace

\2. 基础命令工具

包括：系统进程状态 ps、虚拟内存统计工具 vmstat、控制台的流量监控工具 vnstat、 进程监控工具 atop，htop、内存使用状态 free

3.网络参数工具

包括：Linux网络统计监控工具 netstat、显示和修改网络接口控制器 ethtool、网络数据包分析利刃 tcpdump、远程登陆服务的标准协议 telnet、获取实时网络统计信息 iptraf、显示主机上网络接口带宽使用情况 iftop

4.磁盘参数工具

包括：磁盘卸载 umount、读取、转换并输出数据 dd、文件系统系统 df、磁盘挂载 mount

5.日志监控工具

包括：实时网络日志分析器 GoAccess、多窗口之下日志监控 MultiTail、日志分析系统 LogWatch/Swatch

6.参数监控工具

包括：监控apache网络服务器整体性能 apachetop、ftp 服务器基本信息 ftptop、IO监控 iotop、电量消耗和电源管理 powertop、监控 mysql 的线程和性能 mytop、系统运行参数分析 htop/top/atop

**总结：以上知识点比较多、但是想要真正了解后台开发就必需要了解跟熟悉的掌握这些知识点内容。在你以后的工作中看的是会要用到。熟练掌握以上知识点内容你的水平就达到了后台开发工程师了。**

![img](https://pic3.zhimg.com/80/v2-3421a8b795b5a401287c7a8e250dc5a6_720w.webp)

原文链接：https://zhuanlan.zhihu.com/p/96792003

作者：Hu先生的Linux

# 【NO.132】C++后台服务器开发必备技能——数据库连接池

## 1.数据库连接的

前言：在后端开发过程中，有很多技能是必备的，比如我们今天说到的数据库；后端人员与数据库打交道是最多的；当然，数据库连接并不是我们所想的那么简单，只是单纯的连接一下就OK了，其实在这其中还有很多的坑等着我们去踩，我这里提出来几个点，大家可以思考一下：
1.数据库连接后，怎么防止断开，断开后又是怎么重连的
2.数据库带宽打满后，数据库超时，我们需要怎么处理
3.数据库异常，我们又该怎么处理？
4.数据库连接池连接数越大越好吗？
这些都是我在做项目中遇到的问题，这些问题大家可以留言一起讨论，这边文章就不作回答了。

## **2.数据库连接池**

什么是数据库连接池？其实数据库池就是好比我们有很多的玩具放在一个储物柜中，其中玩具就是我们的数据库连接，储物柜就是我们池子；我们需要玩具时就存储物柜中，使用完后会将玩具放入储物柜；这样，我们就会有源源不断的玩具供我们使用，也就是我们会有源源不断的连接供我们使用。

## **3.使用连接池的原因**

数据库连接是一种关键的有限的昂贵的资源， 一个数据库连接对象均对应一个物理数据库连接，每次操作都打开一个物理连接，使用完都关闭连接，这样造成系统的 性能低下。 数据库连接池的解决方案是在应用程序启动时建立足够的数据库连接，并将这些连接组成一个连接池(简单说：在一个“池”里放了好多半成品的数据库连接对象)，由应用程序动态地对池中的连接进行申请、使用和释放。对于多于连接池中连接数的并发请求，应该在请求队列中排队等待。并且应用程序可以根据池中连接的使用率，动态增加或减少池中的连接数。 连接池技术尽可能多地重用了消耗内存地资源，大大节省了内存，提高了服务器地服务效率，能够支持更多的客户服务。通过使用连接池，将大大提高程序运行效率，同时，我们可以通过其自身的管理机制来监视数据库连接的数量、使用情况。

## **4.连接池使用的优点**

### 1.数据库连接过程

执行数据库语句流程

![img](https://pic4.zhimg.com/80/v2-b1fcd220075e4b8617bccf9977f79e87_720w.webp)

1.TCP建立连接的三次握手
2.MySQL认证的三次握手
3.真正的SQL执行
4.MySQL的关闭
5.TCP的四次握手关闭

缺点：
网络IO较多
数据库的负载较高
响应时间较长及QPS较低
应用频繁的创建连接和关闭连接，导致临时对象较多
在关闭连接后，会出现大量TIME_WAIT 的TCP状态（在2个MSL之后关闭）

### 2.数据库的连接

![img](https://pic2.zhimg.com/80/v2-a8cf302ed0c53994c0fe44ee8fb0ab75_720w.webp)

使用数据库连接池的步骤：

第一次访问的时候，需要建立连接。 但是之后的访问，均会复用之前创建的连接，直接执行SQL语句。

优点：

减少了网络开销
系统的性能会有一个实质的提升
没了麻烦的TIME_WAIT状态

## **5.连接池的工作原理**

连接池的工作原理主要由三部分组成，分别为

连接池的建立
连接池中连接的使用管理
连接池的关闭
第一、连接池的建立。一般在系统初始化时，连接池会根据系统配置建立，并在池中创建了几个连接对象，以便使用时能从连接池中获取。连接池中的连接不能随意创建和关闭，这样避免了连接随意建立和关闭造成的系统开销。Java中提供了很多容器类可以方便的构建连接池，例如Vector、Stack等
第二、连接池的管理。连接池管理策略是连接池机制的核心，连接池内连接的分配和释放对系统的性能有很大的影响。其管理策略是：
当客户请求数据库连接时，首先查看连接池中是否有空闲连接，如果存在空闲连接，则将连接分配给客户使用；如果没有空闲连接，则查看当前所开的连接数是否已经达到最大连接数，如果没达到就重新创建一个连接给请求的客户；如果达到就按设定的最大等待时间进行等待，如果超出最大等待时间，则抛出异常给客户。
当客户释放数据库连接时，先判断该连接的引用次数是否超过了规定值，如果超过就从连接池中删除该连接，否则保留为其他客户服务。
该策略保证了数据库连接的有效复用，避免频繁的建立、释放连接所带来的系统资源开销。
第三、连接池的关闭。当应用程序退出时，关闭连接池中所有的连接，释放连接池相关的资源，该过程正好与创建相反。

## **6.案例**

最近在github看到一个数据库连接池，顺便按照这个做一个解析：

![img](https://pic2.zhimg.com/80/v2-46ae81bb8e0c21653574b5d5b11cfc81_720w.webp)

这个数据库连接池做的比较简单，但是中间是有比较多的问题的：
1.正常项目中，数据库连接一般会放在配置文件中，避免硬编码
2.数据库连接应该放在初始化中，建立好固定连接数，如果在程序执行过程中执行的话，速度会比较慢。
3.对于连接池的维护，更合适的做法是使用线程定时器去不断检测，并且要做好异常措施。

原文链接：https://zhuanlan.zhihu.com/p/250542768

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.133】深入理解异步I/O+epoll+协程

## 1.前言

**同步和异步**的概念描述的是用户线程与内核的交互方式：同步是指用户线程发起IO请求后需要等待或者轮询内核IO操作完成后才能继续执行；而异步是指用户线程发起IO请求后仍继续执行，当内核IO操作完成后会通知用户线程，或者调用用户线程注册的回调函数。
**阻塞和非阻塞**的概念描述的是用户线程调用内核IO操作的方式：阻塞是指IO操作需要彻底完成后才返回到用户空间；而非阻塞是指IO操作被调用后立即返回给用户一个状态值，无需等到IO操作彻底完成。

## 2.异步I/O

在理解异步I/O之前，我们先要知道什么是**同步I/O**
在**阻塞同步I/O**模型下，用户线程向内核发起 recvfrom 系统调用，当数据没有准备好的时候，用户线程阻塞。
此外还有一种**非阻塞同步I/O**，此时用户线程不阻塞于 recvfrom，而是反复向系统查询数据状态。当数据准备好了，就对数据进行后续处理。

![img](https://pic2.zhimg.com/80/v2-b555a5fbc20f5471f20e87f7d34f9e41_720w.webp)

Paste_Image.png

而在**异步I/O**模式下，用户线程在数据还没有准备好的时候**既不阻塞也不反复查询**，而是**继续干自己该干的事情**。内核会开启一个**内核线程**去读取数据，等到数据准备好了，内核给用户线程一个信号，用户线程中断去执行信号处理函数。

![img](https://pic1.zhimg.com/80/v2-369b5305eb19bdb26a628aad2926b72c_720w.webp)

Paste_Image.png

**node.js**中的异步回调也是采用开线程的方式实现的

## 3.epoll

epoll/select 是一种**I/O多路复用模型**
用户线程可以先通过 epoll **注册多个I/O事件**
然后用户线程反复执行调用 epoll/select 查询是否有准备好的事件
如果有准备好的I/O事件则进行处理
关键点是**一个用户线程处理多个I/O事件**

**epoll/select 的区别在于**
select 的底层原理是遍历所有注册的I/O事件，找出准备好的的I/O事件。
而 epoll 则是由内核主动通知哪些I/O事件需要处理，不需要用户线程主动去反复查询，因此大大提高了事件处理的效率。

![img](https://pic3.zhimg.com/80/v2-e071e80242c1d2d5ade493e9cb7ba532_720w.webp)

Paste_Image.png

## 4.协程

协程是一种轻量级的线程
**本质上协程就是用户空间下的线程**
如果把线程/进程当作虚拟“CPU”，协程即跑在这个“CPU”上的线程。

**协程的特点**

1. 占用的资源更少。
2. 所有的切换和调度都发生在用户态。

不管是进程还是线程，每次阻塞、切换都需要陷入系统调用，先让CPU跑操作系统的调度程序，然后再由调度程序决定该跑哪一个线程。而且由于**抢占式调度执行顺序无法确定**的特点，使用线程时需要非常小心地处理同步问题，而协程完全不存在这个问题。
因为**协程可以在用户态显示控制切换**

**例子**
传统的生产者-消费者模型是一个线程写消息，一个线程取消息，通过锁机制控制队列和等待，但一不小心就可能死锁。

如果改用协程，生产者生产消息后，直接通过yield跳转到消费者开始执行，待消费者执行完毕后，切换回生产者继续生产，效率极高：

```
import timedef consumer():    r = ''    while True:        n = yield r        if not n:            return        print('[CONSUMER] Consuming %s...' % n)        time.sleep(1)        r = '200 OK'def produce(c):    c.next()    n = 0    while n < 5:        n = n + 1        print('[PRODUCER] Producing %s...' % n)        r = c.send(n)        print('[PRODUCER] Consumer return: %s' % r)    c.close()if __name__=='__main__':    c = consumer()    produce(c)
```

协程的优点是可以用同步的处理方式实现异步回调的性能

原文链接：https://zhuanlan.zhihu.com/p/254990291

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.134】redis 数据类型详解 以及 redis适用场景场合（详细）

## 1. MySql+Memcached架构的问题

实际MySQL是适合进行海量数据存储的，通过Memcached将热点数据加载到cache，加速访问，很多公司都曾经使用过这样的架构，但随着业务数据量的不断增加，和访问量的持续增长，我们遇到了很多问题：
　　1.MySQL需要不断进行拆库拆表，Memcached也需不断跟着扩容，扩容和维护工作占据大量开发时间。
　　2.Memcached与MySQL数据库数据一致性问题。
　　3.Memcached数据命中率低或down机，大量访问直接穿透到DB，MySQL无法支撑。
　　4.跨机房cache同步问题。
　　众多NoSQL百花齐放，如何选择
　　最近几年，业界不断涌现出很多各种各样的NoSQL产品，那么如何才能正确地使用好这些产品，最大化地发挥其长处，是我们需要深入研究和思考的问题，实际归根结底最重要的是了解这些产品的定位，并且了解到每款产品的tradeoffs，在实际应用中做到扬长避短，总体上这些NoSQL主要用于解决以下几种问题
　　1.少量数据存储，高速读写访问。此类产品通过数据全部in-momery 的方式来保证高速访问，同时提供数据落地的功能，实际这正是Redis最主要的适用场景。
　　2.海量数据存储，分布式系统支持，数据一致性保证，方便的集群节点添加/删除。
　　3.这方面最具代表性的是dynamo和bigtable 2篇论文所阐述的思路。前者是一个完全无中心的设计，节点之间通过gossip方式传递集群信息，数据保证最终一致性，后者是一个中心化的方案设计，通过类似一个分布式锁服务来保证将一致性,数据写入先写内存和redo log，然后定期compat归并到磁盘上，将随机写优化为顺序写，提高写入性能。
　　4.Schema free，auto-sharding等。比如目前常见的一些文档数据库都是支持schema-free的，直接存储json格式数据，并且支持auto-sharding等功能，比如mongodb。
　　面对这些不同类型的NoSQL产品,我们需要根据我们的业务场景选择最合适的产品。
Redis最适合所有数据in-momory的场景，虽然Redis也提供持久化功能，但实际更多的是一个disk-backed的功能，跟传统意义上的持久化有比较大的差别，那么可能大家就会有疑问，似乎Redis更像一个加强版的Memcached，那么何时使用Memcached,何时使用Redis呢?
如果简单地比较Redis与Memcached的区别，大多数都会得到以下观点：
1 、Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，zset，hash等数据结构的存储。 2 、Redis支持数据的备份，即master-slave模式的数据备份。 3 、Redis支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用。

## 2.Redis常用数据类型

Redis最为常用的数据类型主要有以下：
String

Hash

List

Set

Sorted set

pub/sub

Transactions

在具体描述这几种数据类型之前，我们先通过一张图了解下Redis内部内存管理中是如何描述这些不同数据类型的：

![img](https://pic4.zhimg.com/80/v2-e5b20d3b3aa51a2174c8712c0de12e47_720w.webp)

首先Redis内部使用一个redisObject对象来表示所有的key和value,redisObject最主要的信息如上图所示：
type代表一个value对象具体是何种数据类型，
encoding是不同数据类型在redis内部的存储方式，
比如：type=string代表value存储的是一个普通字符串，那么对应的encoding可以是raw或者是int,如果是int则代表实际redis内部是按数值型类存储和表示这个字符串的，当然前提是这个字符串本身可以用数值表示，比如:”123” “456”这样的字符串。
这里需要特殊说明一下vm字段，只有打开了Redis的虚拟内存功能，此字段才会真正的分配内存，该功能默认是关闭状态的，该功能会在后面具体描述。通过上图我们可以发现Redis使用redisObject来表示所有的key/value数据是比较浪费内存的，当然这些内存管理成本的付出主要也是为了给Redis不同数据类型提供一个统一的管理接口，实际作者也提供了多种方法帮助我们尽量节省内存使用，我们随后会具体讨论。

## 3.各种数据类型应用和实现方式

下面我们先来逐一的分析下这7种数据类型的使用和内部实现方式:

String:

Strings 数据结构是简单的key-value类型，value其实不仅是String，也可以是数字.
常用命令: set,get,decr,incr,mget 等。

**应用场景：**String是最常用的一种数据类型，普通的key/ value 存储都可以归为此类.即可以完全实现目前 Memcached 的功能，并且效率更高。还可以享受Redis的定时持久化，操作日志及 Replication等功能。除了提供与 Memcached 一样的get、set、incr、decr 等操作外，Redis还提供了下面一些操作：

获取字符串长度

往字符串append内容

设置和获取字符串的某一段内容

设置及获取字符串的某一位（bit）

批量设置一系列字符串的内容

**实现方式：**String在redis内部存储默认就是一个字符串，被redisObject所引用，当遇到incr,decr等操作时会转成数值型进行计算，此时redisObject的encoding字段为int。

Hash

**常用命令：**hget,hset,hgetall 等。
**应用场景：**在Memcached中，我们经常将一些结构化的信息打包成HashMap，在客户端序列化后存储为一个字符串的值，比如用户的昵称、年龄、性别、积分等，这时候在需要修改其中某一项时，通常需要将所有值取出反序列化后，修改某一项的值，再序列化存储回去。**这样不仅增大了开销，也不适用于一些可能并发操作的场合**（比如两个并发的操作都需要修改积分）。而Redis的Hash结构可以使你像在数据库中Update一个属性一样只修改某一项属性值。
我们简单举个实例来描述下Hash的应用场景，比如我们要存储一个用户信息对象数据，包含以下信息：
用户ID为查找的key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key/value结构来存储，主要有以下2种存储方式：

![img](https://pic1.zhimg.com/80/v2-d5d94adbd3221c0a2dd3d3165bb7a0c4_720w.webp)

第一种方式将用户ID作为查找key,把其他信息封装成一个对象以序列化的方式存储，这种方式的缺点是，增加了序列化/反序列化的开销，并且在需要修改其中一项信息时，需要把整个对象取回，并且修改操作需要对并发进行保护，引入CAS等复杂问题。

![img](https://pic2.zhimg.com/80/v2-2399821dc1a6d192a5c7ed93991493f5_720w.webp)

第二种方法是这个用户信息对象有多少成员就存成多少个key-value对象，用用户ID+对应属性的名称作为唯一标识来取得对应属性的值，虽然省去了序列化开销和并发问题，但是用户ID为重复存储，如果存在大量这样的数据，内存浪费还是非常可观的。
那么Redis提供的Hash很好的解决了这个问题，Redis的Hash实际是内部存储的Value为一个HashMap，并提供了直接存取这个Map成员的接口，如下图：

![img](https://pic3.zhimg.com/80/v2-f13483762cc7117170b2e5dd4557c30a_720w.webp)

也就是说，Key仍然是用户ID, value是一个Map，这个Map的key是成员的属性名，value是属性值，这样对数据的修改和存取都可以直接通过其内部Map的Key(Redis里称内部Map的key为field), 也就是通过 key(用户ID) + field(属性标签) 就可以操作对应属性数据了，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题。很好的解决了问题。
这里同时需要注意，Redis提供了接口(hgetall)可以直接取到全部的属性数据,但是如果内部Map的成员很多，那么涉及到遍历整个内部Map的操作，由于Redis单线程模型的缘故，这个遍历操作可能会比较耗时，而另其它客户端的请求完全不响应，这点需要格外注意。
**实现方式：**
上面已经说到Redis Hash对应Value内部实际就是一个HashMap，实际这里会有2种不同实现，这个Hash的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，对应的value redisObject的encoding为zipmap,当成员数量增大时会自动转成真正的HashMap,此时encoding为ht。

List

**常用命令：**lpush,rpush,lpop,rpop,lrange等。
**应用场景：**
Redis list的应用场景非常多，也是Redis最重要的数据结构之一，比如twitter的关注列表，粉丝列表等都可以用Redis的list结构来实现。
Lists 就是链表，相信略有数据结构知识的人都应该能理解其结构。使用Lists结构，我们可以轻松地实现最新消息排行等功能。Lists的另一个应用就是消息队列，可以利用Lists的PUSH操作，将任务存在Lists中，然后工作线程再用POP操作将任务取出进行执行。Redis还提供了操作Lists中某一段的api，你可以直接查询，删除Lists中某一段的元素。
**实现方式：**
Redis list的实现为一个双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销，Redis内部的很多实现，包括发送缓冲队列等也都是用的这个数据结构。

Set

**常用命令：**
sadd,spop,smembers,sunion 等。
**应用场景：**
Redis set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。
Sets 集合的概念就是一堆不重复值的组合。利用Redis提供的Sets数据结构，可以存储一些集合性的数据，比如在微博应用中，可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis还为集合提供了求交集、并集、差集等操作，可以非常方便的实现如共同关注、共同喜好、二度好友等功能，对上面的所有集合操作，你还可以使用不同的命令选择将结果返回给客户端还是存集到一个新的集合中。
**实现方式：**
set 的内部实现是一个 value永远为null的HashMap，实际就是通过计算hash的方式来快速排重的，这也是set能提供判断一个成员是否在集合内的原因。

Sorted Set

**常用命令：**
zadd,zrange,zrem,zcard等
**使用场景：**
Redis sorted set的使用场景与set类似，区别是set不是自动有序的，而sorted set可以通过用户额外提供一个优先级(score)的参数来为成员排序，并且是插入有序的，即自动排序。当你需要一个有序的并且不重复的集合列表，那么可以选择sorted set数据结构，比如twitter 的public timeline可以以发表时间作为score来存储，这样获取时就是自动按时间排好序的。
另外还可以用Sorted Sets来做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行。
**实现方式：**
Redis sorted set的内部使用HashMap和跳跃表(SkipList)来保证数据的存储和有序，HashMap里放的是成员到score的映射，而跳跃表里存放的是所有的成员，排序依据是HashMap里存的score,使用跳跃表的结构可以获得比较高的查找效率，并且在实现上比较简单。

Pub/Sub

Pub/Sub 从字面上理解就是发布（Publish）与订阅（Subscribe），在Redis中，你可以设定对某一个key值进行消息发布及消息订阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应的消息。这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能。

Transactions

谁说NoSQL都不支持事务，虽然Redis的Transactions提供的并不是严格的ACID的事务（比如一串用EXEC提交执行的命令，在执行中服务器宕机，那么会有一部分命令执行了，剩下的没执行），但是这个Transactions还是提供了基本的命令打包执行的功能（在服务器不出问题的情况下，可以保证一连串的命令是顺序在一起执行的，中间有会有其它客户端命令插进来执行）。Redis还提供了一个Watch功能，你可以对一个key进行Watch，然后再执行Transactions，在这过程中，如果这个Watched的值进行了修改，那么这个Transactions会发现并拒绝执行。

## 4.Redis实际应用场景

```
Redis在很多方面与其他数据库解决方案不同：它使用内存提供主存储支持，而仅使用硬盘做持久性的存储；它的数据模型非常独特，用的是单线程。另一个大区别在于，你可以在开发环境中使用Redis的功能，但却不需要转到Redis。
```

转向Redis当然也是可取的，许多开发者从一开始就把Redis作为首选数据库；但设想如果你的开发环境已经搭建好，应用已经在上面运行了，那么更换数据库框架显然不那么容易。另外在一些需要大容量数据集的应用，Redis也并不适合，因为它的数据集不会超过系统可用的内存。所以如果你有大数据应用，而且主要是读取访问模式，那么Redis并不是正确的选择。
然而我喜欢Redis的一点就是你可以把它融入到你的系统中来，这就能够解决很多问题，比如那些你现有的数据库处理起来感到缓慢的任务。这些你就可以通过Redis来进行优化，或者为应用创建些新的功能。在本文中，我就想探讨一些怎样将Redis加入到现有的环境中，并利用它的原语命令等功能来解决 传统环境中碰到的一些常见问题。在这些例子中，Redis都不是作为首选数据库。

### **4.1.显示最新的项目列表**

下面这个语句常用来显示最新项目，随着数据多了，查询毫无疑问会越来越慢。

SELECT * FROM foo WHERE … ORDER BY time DESC LIMIT 10

```
    在Web应用中，“列出最新的回复”之类的查询非常普遍，这通常会带来可扩展性问题。这令人沮丧，因为项目本来就是按这个顺序被创建的，但要输出这个顺序却不得不进行排序操作。    类似的问题就可以用Redis来解决。比如说，我们的一个Web应用想要列出用户贴出的最新20条评论。在最新的评论边上我们有一个“显示全部”的链接，点击后就可以获得更多的评论。    我们假设数据库中的每条评论都有一个唯一的递增的ID字段。    我们可以使用分页来制作主页和评论页，使用Redis的模板，每次新评论发表时，我们会将它的ID添加到一个Redis列表：
```

LPUSH latest.comments

```
   我们将列表裁剪为指定长度，因此Redis只需要保存最新的5000条评论：   LTRIM latest.comments 0 5000   每次我们需要获取最新评论的项目范围时，我们调用一个函数来完成（使用伪代码）：
```

FUNCTION get_latest_comments(start, num_items):

```
id_list = redis.lrange("latest.comments",start,start+num_items - 1)  IF id_list.length < num_items      id_list = SQL_DB("SELECT ... ORDER BY time LIMIT ...")  END  RETURN id_list  
```

END

```
  这里我们做的很简单。在Redis中我们的最新ID使用了常驻缓存，这是一直更新的。但是我们做了限制不能超过5000个ID，因此我们的获取ID函数会一直询问Redis。只有在start/count参数超出了这个范围的时候，才需要去访问数据库。    我们的系统不会像传统方式那样“刷新”缓存，Redis实例中的信息永远是一致的。SQL数据库（或是硬盘上的其他类型数据库）只是在用户需要获取“很远”的数据时才会被触发，而主页或第一个评论页是不会麻烦到硬盘上的数据库了。
```

### **4.2.删除与过滤**

我们可以使用LREM来删除评论。如果删除操作非常少，另一个选择是直接跳过评论条目的入口，报告说该评论已经不存在。
有些时候你想要给不同的列表附加上不同的过滤器。如果过滤器的数量受到限制，你可以简单的为每个不同的过滤器使用不同的Redis列表。毕竟每个列表只有5000条项目，但Redis却能够使用非常少的内存来处理几百万条项目。

### **4.3.排行榜相关**

另一个很普遍的需求是各种数据库的数据并非存储在内存中，因此在按得分排序以及实时更新这些几乎每秒钟都需要更新的功能上数据库的性能不够理想。
典型的比如那些在线游戏的排行榜，比如一个Facebook的游戏，根据得分你通常想要：
\- 列出前100名高分选手
\- 列出某用户当前的全球排名
这些操作对于Redis来说小菜一碟，即使你有几百万个用户，每分钟都会有几百万个新的得分。
模式是这样的，每次获得新得分时，我们用这样的代码：
ZADD leaderboard
你可能用userID来取代username，这取决于你是怎么设计的。
得到前100名高分用户很简单：ZREVRANGE leaderboard 0 99。
用户的全球排名也相似，只需要：ZRANK leaderboard 。

### **4.4.按照用户投票和时间排序**

排行榜的一种常见变体模式就像Reddit或Hacker News用的那样，新闻按照类似下面的公式根据得分来排序：
score = points / time^alpha
因此用户的投票会相应的把新闻挖出来，但时间会按照一定的指数将新闻埋下去。下面是我们的模式，当然算法由你决定。
模式是这样的，开始时先观察那些可能是最新的项目，例如首页上的1000条新闻都是候选者，因此我们先忽视掉其他的，这实现起来很简单。
每次新的新闻贴上来后，我们将ID添加到列表中，使用LPUSH + LTRIM，确保只取出最新的1000条项目。
有一项后台任务获取这个列表，并且持续的计算这1000条新闻中每条新闻的最终得分。计算结果由ZADD命令按照新的顺序填充生成列表，老新闻则被清除。这里的关键思路是排序工作是由后台任务来完成的。

### **4.5.处理过期项目**

另一种常用的项目排序是按照时间排序。我们使用unix时间作为得分即可。
模式如下：
\- 每次有新项目添加到我们的非Redis数据库时，我们把它加入到排序集合中。这时我们用的是时间属性，current_time和time_to_live。
\- 另一项后台任务使用ZRANGE…SCORES查询排序集合，取出最新的10个项目。如果发现unix时间已经过期，则在数据库中删除条目。

### **4.6.计数**

Redis是一个很好的计数器，这要感谢INCRBY和其他相似命令。
我相信你曾许多次想要给数据库加上新的计数器，用来获取统计或显示新信息，但是最后却由于写入敏感而不得不放弃它们。
好了，现在使用Redis就不需要再担心了。有了原子递增（atomic increment），你可以放心的加上各种计数，用GETSET重置，或者是让它们过期。
例如这样操作：
INCR user: EXPIRE
user: 60
你可以计算出最近用户在页面间停顿不超过60秒的页面浏览量，当计数达到比如20时，就可以显示出某些条幅提示，或是其它你想显示的东西。

### **4.7.特定时间内的特定项目**

另一项对于其他数据库很难，但Redis做起来却轻而易举的事就是统计在某段特点时间里有多少特定用户访问了某个特定资源。比如我想要知道某些特定的注册用户或IP地址，他们到底有多少访问了某篇文章。
每次我获得一次新的页面浏览时我只需要这样做：
SADD page:day1:
当然你可能想用unix时间替换day1，比如time()-(time()%3600*24)等等。
想知道特定用户的数量吗？只需要使用SCARD page:day1:。
需要测试某个特定用户是否访问了这个页面？SISMEMBER page:day1:。

------

### **4.8.实时分析正在发生的情况，用于数据统计与防止垃圾邮件等**

我们只做了几个例子，但如果你研究Redis的命令集，并且组合一下，就能获得大量的实时分析方法，有效而且非常省力。使用Redis原语命令，更容易实施垃圾邮件过滤系统或其他实时跟踪系统。

### **4.9.Pub/Sub**

Redis的Pub/Sub非常非常简单，运行稳定并且快速。支持模式匹配，能够实时订阅与取消频道。

### **4.10.队列**

你应该已经注意到像list push和list pop这样的Redis命令能够很方便的执行队列操作了，但能做的可不止这些：比如Redis还有list pop的变体命令，能够在列表为空时阻塞队列。
现代的互联网应用大量地使用了消息队列（Messaging）。消息队列不仅被用于系统内部组件之间的通信，同时也被用于系统跟其它服务之间的交互。消息队列的使用可以增加系统的可扩展性、灵活性和用户体验。非基于消息队列的系统，其运行速度取决于系统中最慢的组件的速度（注：短板效应）。而基于消息队列可以将系统中各组件解除耦合，这样系统就不再受最慢组件的束缚，各组件可以异步运行从而得以更快的速度完成各自的工作。
此外，当服务器处在高并发操作的时候，比如频繁地写入日志文件。可以利用消息队列实现异步处理。从而实现高性能的并发操作。

### **4.11.缓存**

Redis的缓存部分值得写一篇新文章，我这里只是简单的说一下。Redis能够替代memcached，让你的缓存从只能存储数据变得能够更新数据，因此你不再需要每次都重新生成数据了。

原文链接：https://zhuanlan.zhihu.com/p/266211754

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)



# 【NO.136】OSI 七层模型和TCP/IP模型及对应协议（详解）

![动图封面](https://pic3.zhimg.com/v2-2af488004591cbc12cd82c44518523de_b.jpg)

![img](https://pic3.zhimg.com/80/v2-4ce0a8a1b77a348597fde346a1400872_720w.webp)

![img](https://pic1.zhimg.com/80/v2-0df113c819c74fbee70987bdc1699bd0_720w.webp)

完成中继功能的节点通常称为中继系统。在OSI七层模型中，处于不同层的中继系统具有不同的名称。

一个设备工作在哪一层，关键看它工作时利用哪一层的数据头部信息。网桥工作时，是以MAC头部来决定转发端口的，因此显然它是数据链路层的设备。
具体说:
物理层：网卡，网线，集线器，中继器，调制解调器

数据链路层：网桥，交换机

网络层：路由器

网关工作在第四层传输层及其以上

集线器是物理层设备,采用广播的形式来传输信息。

交换机就是用来进行报文交换的机器。多为链路层设备(二层交换机)，能够进行地址学习，采用存储转发的形式来交换报文.。

路由器的一个作用是连通不同的网络，另一个作用是选择信息传送的线路。选择通畅快捷的近路，能大大提高通信速度，减轻网络系统通信负荷，节约网络系统资源，提高网络系统畅通率。

## **1.交换机和路由器的区别**

交换机拥有一条很高带宽的背部总线和内部交换矩阵。交换机的所有的端口都挂接在这条总线上，控制电路收到数据包以后，处理端口会查找内存中的地址对照表以确定目的MAC（网卡的硬件地址）的NIC（网卡）挂接在哪个端口上，通过内部交换矩阵迅速将数据包传送到目的端口，目的MAC若不存在则广播到所有的端口，接收端口回应后交换机会“学习”新的地址，并把它添加入内部MAC地址表中。
使用交换机也可以把网络“分段”，通过对照MAC地址表，交换机只允许必要的网络流量通过交换机。通过交换机的过滤和转发，可以有效的隔离广播风暴，减少误包和错包的出现，避免共享冲突。
交换机在同一时刻可进行多个端口对之间的数据传输。每一端口都可视为独立的网段，连接在其上的网络设备独自享有全部的带宽，无须同其他设备竞争使用。当节点A向节点D发送数据时，节点B可同时向节点C发送数据，而且这两个传输都享有网络的全部带宽，都有着自己的虚拟连接。假使这里使用的是10Mbps的以太网交换机，那么该交换机这时的总流通量就等于2×10Mbps＝20Mbps，而使用10Mbps的共享式HUB时，一个HUB的总流通量也不会超出10Mbps。
总之，交换机是一种基于MAC地址识别，能完成封装转发数据包功能的网络设备。交换机可以“学习”MAC地址，并把其存放在内部地址表中，通过在数据帧的始发者和目标接收者之间建立临时的交换路径，使数据帧直接由源地址到达目的地址。

从过滤网络流量的角度来看，路由器的作用与交换机和网桥非常相似。但是与工作在网络物理层，从物理上划分网段的交换机不同，路由器使用专门的软件协议从逻辑上对整个网络进行划分。例如，一台支持IP协议的路由器可以把网络划分成多个子网段，只有指向特殊IP地址的网络流量才可以通过路由器。对于每一个接收到的数据包，路由器都会重新计算其校验值，并写入新的物理地址。因此，使用路由器转发和过滤数据的速度往往要比只查看数据包物理地址的交换机慢。但是，对于那些结构复杂的网络，使用路由器可以提高网络的整体效率。路由器的另外一个明显优势就是可以自动过滤网络广播。

## 2.**集线器与路由器在功能上有什么不同?**

首先说HUB,也就是集线器。它的作用可以简单的理解为将一些机器连接起来组成一个局域网。而交换机（又名交换式集线器）作用与集线器大体相同。但是两者在性能上有区别：集线器采用的是共享带宽的工作方式，而交换机是独享带宽。这样在机器很多或数据量很大时，两者将会有比较明显的。而路由器与以上两者有明显区别，它的作用在于连接不同的网段并且找到网络中数据传输最合适的路径。路由器是产生于交换机之后，就像交换机产生于集线器之后，所以路由器与交换机也有一定联系，不是完全独立的两种设备。路由器主要克服了交换机不能路由转发数据包的不足。
总的来说，路由器与交换机的主要区别体现在以下几个方面：
（1）工作层次不同
最初的的交换机是工作在数据链路层，而路由器一开始就设计工作在网络层。由于交换机工作在数据链路层，所以它的工作原理比较简单，而路由器工作在网络层，可以得到更多的协议信息，路由器可以做出更加智能的转发决策。
（2）数据转发所依据的对象不同
交换机是利用物理地址或者说MAC地址来确定转发数据的目的地址。而路由器则是利用IP地址来确定数据转发的地址。IP地址是在软件中实现的，描述的是设备所在的网络。MAC地址通常是硬件自带的，由网卡生产商来分配的，而且已经固化到了网卡中去，一般来说是不可更改的。而IP地址则通常由网络管理员或系统自动分配。
（3）传统的交换机只能分割冲突域，不能分割广播域；而路由器可以分割广播域
由交换机连接的网段仍属于同一个广播域，广播数据包会在交换机连接的所有网段上传播，在某些情况下会导致通信拥挤和安全漏洞。连接到路由器上的网段会被分配成不同的广播域，广播数据不会穿过路由器。虽然第三层以上交换机具有VLAN功能，也可以分割广播域，但是各子广播域之间是不能通信交流的，它们之间的交流仍然需要路由器。
（4）路由器提供了防火墙的服务
路由器仅仅转发特定地址的数据包，不传送不支持路由协议的数据包传送和未知目标网络数据包的传送，从而可以防止广播风暴。

## **3.物理层**

在OSI参考模型中，物理层（Physical Layer）是参考模型的最低层，也是OSI模型的第一层。
物理层的主要功能是：利用传输介质为数据链路层提供物理连接，实现比特流的透明传输。
物理层的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。

## 4.**数据链路层**

数据链路层（Data Link Layer）是OSI模型的第二层，负责建立和管理节点间的链路。该层的主要功能是：通过各种控制协议，将有差错的物理信道变为无差错的、能可靠传输数据帧的数据链路。
在计算机网络中由于各种干扰的存在，物理链路是不可靠的。因此，这一层的主要功能是在物理层提供的比特流的基础上，通过差错控制、流量控制方法，使有差错的物理线路变为无差错的数据链路，即提供可靠的通过物理介质传输数据的方法。
该层通常又被分为介质访问控制（MAC）和逻辑链路控制（LLC）两个子层。

MAC子层的主要任务是解决共享型网络中多用户对信道竞争的问题，完成网络介质的访问控制；

LLC子层的主要任务是建立和维护网络连接，执行差错校验、流量控制和链路控制。
数据链路层的具体工作是接收来自物理层的位流形式的数据，并封装成帧，传送到上一层；同样，也将来自上层的数据帧，拆装为位流形式的数据转发到物理层；并且，还负责处理接收端发回的确认帧的信息，以便提供可靠的数据传输。

## 5.**网络层**

网络层（Network Layer）是OSI模型的第三层，它是OSI参考模型中最复杂的一层，也是通信子网的最高一层。它在下两层的基础上向资源子网提供服务。其主要任务是：通过路由选择算法，为报文或分组通过通信子网选择最适当的路径。该层控制数据链路层与传输层之间的信息转发，建立、维持和终止网络的连接。具体地说，数据链路层的数据在这一层被转换为数据包，然后通过路径选择、分段组合、顺序、进/出路由等控制，将信息从一个网络设备传送到另一个网络设备。
一般地，数据链路层是解决同一网络内节点之间的通信，而网络层主要解决不同子网间的通信。例如在广域网之间通信时，必然会遇到路由（即两节点间可能有多条路径）选择问题。

在实现网络层功能时，需要解决的主要问题如下：
寻址：数据链路层中使用的物理地址（如MAC地址）仅解决网络内部的寻址问题。在不同子网之间通信时，为了识别和找到网络中的设备，每个子网中的设备都会被分配一个唯一的地址。由于各子网使用的物理技术可能不同，因此这个地址应当是逻辑地址（如IP地址）。
交换：规定不同的信息交换方式。常见的交换技术有：线路交换技术和存储转发技术，后者又包括报文交换技术和分组交换技术。
路由算法：当源节点和目的节点之间存在多条路径时，本层可以根据路由算法，通过网络为数据分组选择最佳路径，并将信息从最合适的路径由发送端传送到接收端。
连接服务：与数据链路层流量控制不同的是，前者控制的是网络相邻节点间的流量，后者控制的是从源节点到目的节点间的流量。其目的在于防止阻塞，并进行差错检测。

## **6.传输层**

OSI下3层的主要任务是数据通信，上3层的任务是数据处理。而传输层（Transport Layer）是OSI模型的第4层。因此该层是通信子网和资源子网的接口和桥梁，起到承上启下的作用。
该层的主要任务是：向用户提供可靠的端到端的差错和流量控制，保证报文的正确传输。传输层的作用是向高层屏蔽下层数据通信的细节，即向用户透明地传送报文。该层常见的协议：TCP/IP中的TCP协议、Novell网络中的SPX协议和微软的NetBIOS/NetBEUI协议。
传输层提供会话层和网络层之间的传输服务，这种服务从会话层获得数据，并在必要时，对数据进行分割。然后，传输层将数据传递到网络层，并确保数据能正确无误地传送到网络层。因此，传输层负责提供两节点之间数据的可靠传送，当两节点的联系确定之后，传输层则负责监督工作。综上，传输层的主要功能如下：
传输连接管理：提供建立、维护和拆除传输连接的功能。传输层在网络层的基础上为高层提供“面向连接”和“面向无接连”的两种服务。
处理传输差错：提供可靠的“面向连接”和不太可靠的“面向无连接”的数据传输服务、差错控制和流量控制。在提供“面向连接”服务时，通过这一层传输的数据将由目标设备确认，如果在指定的时间内未收到确认信息，数据将被重发。
监控服务质量。

## 7.**会话层**

会话层（Session Layer）是OSI模型的第5层，是用户应用程序和网络之间的接口，主要任务是：向两个实体的表示层提供建立和使用连接的方法。将不同实体之间的表示层的连接称为会话。因此会话层的任务就是组织和协调两个会话进程之间的通信，并对数据交换进行管理。
用户可以按照半双工、单工和全双工的方式建立会话。当建立会话时，用户必须提供他们想要连接的远程地址。而这些地址与MAC（介质访问控制子层）地址或网络层的逻辑地址不同，它们是为用户专门设计的，更便于用户记忆。域名（DN）就是一种网络上使用的远程地址例如：[http://www.3721.com](https://link.zhihu.com/?target=http%3A//www.3721.com)就是一个域名。会话层的具体功能如下：
会话管理：允许用户在两个实体设备之间建立、维持和终止会话，并支持它们之间的数据交换。例如提供单方向会话或双向同时会话，并管理会话中的发送顺序，以及会话所占用时间的长短。
会话流量控制：提供会话流量控制和交叉会话功能。
寻址：使用远程地址建立会话连接。l
出错控制：从逻辑上讲会话层主要负责数据交换的建立、保持和终止，但实际的工作却是接收来自传输层的数据，并负责纠正错误。会话控制和远程过程调用均属于这一层的功能。但应注意，此层检查的错误不是通信介质的错误，而是磁盘空间、打印机缺纸等类型的高级错误。

## 8.**表示层**

表示层（Presentation Layer）是OSI模型的第六层，它对来自应用层的命令和数据进行解释，对各种语法赋予相应的含义，并按照一定的格式传送给会话层。其主要功能是“处理用户信息的表示问题，如编码、数据格式转换和加密解密”等。表示层的具体功能如下：
数据格式处理：协商和建立数据交换的格式，解决各应用程序之间在数据格式表示上的差异。
数据的编码：处理字符集和数字的转换。例如由于用户程序中的数据类型（整型或实型、有符号或无符号等）、用户标识等都可以有不同的表示方式，因此，在设备之间需要具有在不同字符集或格式之间转换的功能。
压缩和解压缩：为了减少数据的传输量，这一层还负责数据的压缩与恢复。
数据的加密和解密：可以提高网络的安全性。

## **9.应用层**

应用层（Application Layer）是OSI参考模型的最高层，它是计算机用户，以及各种应用程序和网络之间的接口，其功能是直接向用户提供服务，完成用户希望在网络上完成的各种工作。它在其他6层工作的基础上，负责完成网络中应用程序与网络操作系统之间的联系，建立与结束使用者之间的联系，并完成网络用户提出的各种网络服务及应用所需的监督、管理和服务等各种协议。此外，该层还负责协调各个应用程序间的工作。
应用层为用户提供的服务和协议有：文件服务、目录服务、文件传输服务（FTP）、远程登录服务（Telnet）、电子邮件服务（E-mail）、打印服务、安全服务、网络管理服务、数据库服务等。上述的各种网络服务由该层的不同应用协议和程序完成，不同的网络操作系统之间在功能、界面、实现技术、对硬件的支持、安全可靠性以及具有的各种应用程序接口等各个方面的差异是很大的。应用层的主要功能如下：
用户接口：应用层是用户与网络，以及应用程序与网络间的直接接口，使得用户能够与网络进行交互式联系。
实现各种服务：该层具有的各种应用程序可以完成和实现用户请求的各种服务。

## 10.OSI7层模型的小结

由于OSI是一个理想的模型，因此一般网络系统只涉及其中的几层，很少有系统能够具有所有的7层，并完全遵循它的规定。
在7层模型中，每一层都提供一个特殊的网络功能。从网络功能的角度观察：下面4层（物理层、数据链路层、网络层和传输层）主要提供数据传输和交换功能，即以节点到节点之间的通信为主；第4层作为上下两部分的桥梁，是整个网络体系结构中最关键的部分；而上3层（会话层、表示层和应用层）则以提供用户与应用程序之间的信息和数据处理功能为主。简言之，下4层主要完成通信子网的功能，上3层主要完成资源子网的功能。

以下是TCP/IP分层模型

![img](https://pic1.zhimg.com/80/v2-fcc62be57088f09d84d0992f158c9da8_720w.webp)

　　　　　　 TCP/IP四层参考模型

![img](https://pic3.zhimg.com/80/v2-a6ef9beba7981b0d35379c0f1ff0c36a_720w.webp)

在这里插入图片描述

![img](https://pic4.zhimg.com/80/v2-0761b5a38c911e56ca26e9a556053f1f_720w.webp)

　　TCP/IP协议被组织成四个概念层，其中有三层对应于ISO参考模型中的相应层。ICP/IP协议族并不包含物理层和数据链路层，因此它不能独立完成整个计算机网络系统的功能，必须与许多其他的协议协同工作。
　　TCP/IP分层模型的四个协议层分别完成以下的功能：
　　第一层:网络接口层
　　包括用于协作IP数据在已有网络介质上传输的协议。实际上TCP/IP标准并不定义与ISO数据链路层和物理层相对应的功能。相反，它定义像地址解析协议(Address Resolution Protocol,ARP)这样的协议，提供TCP/IP协议的数据结构和实际物理硬件之间的接口。
　　第二层:网间层
　　对应于OSI七层参考模型的网络层。本层包含IP协议、RIP协议(Routing Information Protocol，路由信息协议)，负责数据的包装、寻址和路由。同时还包含网间控制报文协议(Internet Control Message Protocol,ICMP)用来提供网络诊断信息。
　　第三层:传输层
　　对应于OSI七层参考模型的传输层，它提供两种端到端的通信服务。其中TCP协议(Transmission Control Protocol)提供可靠的数据流运输服务，UDP协议(Use Datagram Protocol)提供不可靠的用户数据报服务。
　　第四层:应用层
　　对应于OSI七层参考模型的应用层和表达层。因特网的应用层协议包括Finger、Whois、FTP(文件传输协议)、Gopher、HTTP(超文本传输协议)、Telent(远程终端协议)、SMTP(简单邮件传送协议)、IRC(因特网中继会话)、NNTP（网络新闻传输协议）等，这也是本书将要讨论的重点。

原文链接：https://zhuanlan.zhihu.com/p/213981143

作者：Hu先生的Linux

# 【NO.137】腾讯T9/T3.1级别的后台服务器开发技术大佬是怎样炼成的？

今天给大家分享一下腾讯T9/T3.1级别的技术大佬的学习路线，希望对在自学提升的朋友有一些帮助，学习途径总结在下面这张思维导图里面了，觉得还不错的请点赞收藏支持一下

思维导图：

![img](https://pic2.zhimg.com/80/v2-6c7e07b9bed3742db0c0fb2981440cd5_720w.webp)

## 1.精进基石

![img](https://pic4.zhimg.com/80/v2-bd0019385cfcfd1db5b6ff08702029af_720w.webp)

### **1.1.数据算法与结构**

1.1 排序（11种）与KMP
1.2 红黑树证明
1.3 B树与B+树
1.4 Hash与布隆过滤器

### **1.2.设计模式（23种）**

2.1 责任链模式
2.2 过滤器墨海
2.3 发布订阅模式
2.4 工厂模式

### **1.3.工程管理**

3.1 Makefile/cmake/configure
3.2 git /svn与持续集成
3.3 Linux系统运行时命令

## 2.高性能网络设计

![img](https://pic3.zhimg.com/80/v2-6a25fadbc85acbd4be4631b447f5dcf6_720w.webp)

### **2.1.代码实现**

1.1 网络io与select/poll/epoll
1.2 reactor的原理与实现
1.3 http/https web服务器的实现
1.4 websocket协议与服务器实现

### **2.2.方案分析**

2.1 服务器百万并发的实现（c10K，c1000k， C10M）
2.2 redis/memcached/Nginx网络组件
2.3 Posix API与网络协议栈
2.4 UDP可靠协议 QUIC/KCP

## 3.基础组件实现

![img](https://pic3.zhimg.com/80/v2-d97cb962371caf08e136514bb2ceba52_720w.webp)

### **3.1.池式结构**

1.1 手写线程池
1.2 内存池 ringbuffer
1.3 异步请求池 性能优化，异步mysql 异步dns 异步redis
1.4 mysql连接池
1.5 redis连接池

### **3.2.高性能组件**

2.1 原子操作 CAS
2.2 消息队列与无锁队列
2.3 定时器的方案 红黑树 时间轮 最小堆
2.4 锁的实现原理 互斥锁，自旋锁 ，乐观锁，悲观锁，分布式锁
2.5 服务器连接保活 keepalived
2.6 try/catch的实现

### **3.3.开源组件**

3.1 libevent/libev框架
3.2 异步日志方案 log4cpp
3.3 应用层协议 protobuf/thrift
3.4 openssl加密
3.5 json与xml解析器
3.6 字符编码unicode/gbk/utf-8

## 4.自主自研框架

![img](https://pic2.zhimg.com/80/v2-75f29501141f73fc63913b35f4dced25_720w.webp)

### **4.1.协程框架的实现 NtyCo**

1.1 协程的原理与工程案例
1.2 协程的调度器实现

### **4.2.用户态协议栈NtyTcp(tcp/ip)**

2.1 滑动窗口 拥塞控制 满启动
2.2 tcp定时器的实现
2.3 epoll的源码实现

## 5.基础开源框架

![img](https://pic1.zhimg.com/80/v2-9d94f6a8b5363ee429efa2cda7c12914_720w.webp)

### **5.1.skynet**

1.1 skynet高性能网关
1.2 actor实现与cluster/负载均衡
1.3 skynet网络与热更新 数据共享**

### 5.2.ZeroMQ

2.1 ZeroMQ Router-Dealter模式
2.2 源码分析：消息模型与工程案例
2.3 源码分析：网络机制**

### **5.3.DPDK**

3.1 dpdk PCI原理与 testpmd/l3fwd/skeletion
3.2 kni数据流程
3.3 dpdk实现dns
3.4 dpdk的高性能网关的实现
3.5 半虚拟化 virtio/vhost的加速**

## 6.中间件开发

![img](https://pic1.zhimg.com/80/v2-6b633e9c908268602c2446707a0ff5b4_720w.webp)

### **6.1.MySQL**

1.1 SQL语句 索引 存储过程 触发器
1.2 数据库连接池与sql解析剖析
1.3 存储引擎原理 MyISAM与Innodb 事务隔离
1.4 自己实现一个存储引擎 MySQL源码
1.5 MySQL集群与分布式 高可用高并发

### **6.2.Redis**

2.1 Redis相关命令与持久化
2.2 Redis连接池与异步操作
2.3 源码分析：存储原理与数据模型
2.4 源码分析：主从 原子模型
2.5 redis的集群方案

### **6.3.NGINX**

3.1 Nginx使用conf配置
3.2 nginx模块开发 过滤器模块
3.3 Nginx模块开发 handler模块
3.4 源码分析： Nginx Http状态机
3.5 源码分析：进程间通信与Slab共享机制

### **6.4.mongodb**

4.1 Mongo接口编程与MongoDB命令使用
4.2 MongoDB的集群方案**

### **6.5.dfs**

5.1 ceph
5.2 fastdfs

## 7.Linux内核

![img](https://pic1.zhimg.com/80/v2-f932bd93349874895089a871f98f8594_720w.webp)

### **7.1.进程管理**

1.1 进程管理与调度
1.2 锁与进程间通信
1.3 系统调用 如何自己实现一个syscall

### **7.2.内存管理**

2.1 物理内存 伙伴算法
2.2 进程虚拟内存 mm_struct
2.3 页的回收与页交换

### **7.3.文件系统**

3.1 虚拟文件系统
3.2 Ext2/3/4 文件系统
3.3 无持久的存储

### **7.4.设备驱动**

4.1 内核编译与升级
4.2 进程通信组件的实现
4.3 网卡的实现吧

## 8.性能分析

![img](https://pic4.zhimg.com/80/v2-c59a00c4ae87cff81c61613cd9dd885b_720w.webp)

1、工具 wrk/ webbench/ loadbalance/valgrind

2、Google gTest/Memtrack

3、火焰图/热图

## 9.分布式架构

![img](https://pic2.zhimg.com/80/v2-7c6d0935583d01c34a1a1ac48218c709_720w.webp)

1、腾讯的Tars

2、虚拟化的docker

3、分布式注册中心etcd

4、P2P 网络穿透 打洞 去中心化的网络

原文链接：https://linuxcpp.0voice.com/?id=245

作者：[HG](https://linuxcpp.0voice.com/?auth=10)

# 【NO.138】TCP和UDP详解

本篇文章主要是从运输层协议概述、UDP、TCP、可靠传输的工作原理、TCP首部格式、TCP可靠传输的实现、TCP流量控制、TCP的拥塞控制、TCP的连接管理这几个方面进行解析。不对之处还望指出，喜欢的可以点赞关注一下，谢谢。

## **1.运输层协议概述**

### **1.1.进程之间的通信**

- 从通信和信息处理的角度看，运输层向它上面的应用层提供通信服务，它属于面向通信部分的最高层，同时也是用户功能中的最低层。
- 当两台主机使用网络的核心部分的功能进行点对点通信的时候，只有位于边缘部分的主机的协议栈才有运输层，而网络核心的路由器在转发的时候只有用到下三层的功能。

![img](https://pic2.zhimg.com/80/v2-d8dbe065477acbfbcaab3cf273aef1bd_720w.webp)

应用进程之间的通信：

- 两个主机进行通信实际上就是两个主机中的应用进程互相通信。
- 应用进程之间的通信又称为端到端的通信。 输层的一个很重要的功能就是复用和分用。应用层不同进程的报文通过不同的端口向下交到运输层，再往下就共用网络层提供的服务。
- “运输层提供应用进程间的逻辑通信”。“逻辑通信”的意思是：运输层之间的通信好像是沿水平方向传送数据。但事实上这两个运输层之间并没有一条水平方向的物理连接。

![img](https://pic4.zhimg.com/80/v2-799d09715965f31bd471873adc0acf2f_720w.webp)

运输层的主要功能：

- 运输层为应用进程之间提供端到端的逻辑通信（但网络层是为主机之间提供逻辑通信）。
- 运输层还要对收到的报文进行差错检测。
- 运输层需要有两种不同的运输协议，即面向连接的 TCP 和无连接的 UDP。

两种不同的运输协议：

- 运输层向高层用户屏蔽了下面网络核心的细节（如网络拓扑、所采用的路由选择协议等），它使应用进程看见的就是好像在两个运- 输层实体之间有一条端到端的逻辑通信信道。
- 当运输层采用面向连接的 TCP 协议时，尽管下面的网络是不可靠的（只提供尽最大努力服务），但这种逻辑通信信道就相当于一条全双工的可靠信道。
- 当运输层采用无连接的 UDP 协议时，这种逻辑通信信道是一条不可靠信道。

### 1.2.运输层的两个主要协议

TCP/IP 的运输层有两个不同的协议：

- (1) 用户数据报协议 UDP(User Datagram Protocol)
- (2) 传输控制协议 TCP(Transmission Control Protocol)

TCP与UDP：

- 两个对等运输实体在通信时传送的数据单位叫作运输协议数据单元 TPDU (Transport Protocol Data Unit)。
- TCP 传送的数据单位协议是 TCP 报文段(segment)
- UDP 传送的数据单位协议是 UDP 报文或用户数据报。

![img](https://pic1.zhimg.com/80/v2-27a84d97bfd38770e090cffabd26e598_720w.webp)

- UDP 在传送数据之前不需要先建立连接。对方的运输层在收到 UDP 报文后，不需要给出任何确认。虽然 UDP 不提供可靠交付，但在某些情况下 UDP 是一种最有效的工作方式。
- TCP 则提供面向连接的服务。TCP 不提供广播或多播服务。由于 TCP 要提供可靠的、面向连接的运输服务，因此不可避免地增加了许多的开销。这不仅使协议数据单元的首部增大很多，还要占用许多的处理机资源。

### 1.3.运输层的端口

- 运行在计算机中的进程是用进程标识符来标志的。
- 运行在应用层的各种应用进程却不应当让计算机操作系统指派它的进程标识符。这是因为在因特网上使用的计算机的操作系统种类很多，而不同的操作系统又使用不同格式的进程标识符。
- 为了使运行不同操作系统的计算机的应用进程能够互相通信，就必须用统一的方法对 TCP/IP 体系的应用进程进行标志。

端口号(protocol port number)，简称为端口(port)：

- 解决这个问题的方法就是在运输层使用协议端口号(protocol port number)，或通常简称为端口(port)。
- 虽然通信的终点是应用进程，但我们可以把端口想象是通信的终点，因为我们只要把要传送的报文交到目的主机的某一个合适的目的端口，剩下的工作（即最后交付目的进程）就由 TCP 来完成。

软件端口与硬件端口:

- 在协议栈层间的抽象的协议端口是软件端口。
- 路由器或交换机上的端口是硬件端口。
- 硬件端口是不同硬件设备进行交互的接口，而软件端口是应用层的各种协议进程与运输实体进行层间交互的一种地址。

TCP 的端口 :

- 端口用一个 16 位端口号进行标志。
- 端口号只具有本地意义，即端口号只是为了标志本计算机应用层中的各进程。在因特网中不同计算机的相同端口号是没有联系的。

三类端口号：

- 熟知端口，数值一般为 0~1023。
- 登记端口号，数值为1024~49151，为没有熟知端口号的应用程序使用的。使用这个范围的端口号必须在 IANA 登记，以防止重复。
- 客户端口号或短暂端口号，数值为49152~65535，留给客户进程选择暂时使用。当服务器进程收到客户进程的报文时，就知道了客户进程所使用的动态端口号。通信结束后，这个端口号可供其他客户进程以后使用。

## 2.用户数据报协议UDP

### **2.1.UDP概述**

- UDP 只在 IP 的数据报服务之上增加了很少一点的功能，即端口的功能和差错检测的功能。
- 虽然 UDP 用户数据报只能提供不可靠的交付，但 UDP 在某些方面有其特殊的优点。

UDP的主要特点：

- UDP 是无连接的，即发送数据之前不需要建立连接。
- UDP 使用尽最大努力交付，即不保证可靠交付，同时也不使用拥塞控制。
- UDP 是面向报文的。UDP 没有拥塞控制，很适合多媒体通信的要求。
- UDP 支持一对一、一对多、多对一和多对多的交互通信。
- UDP 的首部开销小，只有 8 个字节。

面向报文的UDP：

- 发送方 UDP 对应用程序交下来的报文，在添加首部后就向下交付 IP 层。UDP 对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。
- 应用层交给 UDP 多长的报文，UDP 就照样发送，即一次发送一个报文。
- 接收方 UDP 对 IP 层交上来的 UDP 用户数据报，在去除首部后就原封不动地交付上层的应用进程，一次交付一个完整的报文。
- 应用程序必须选择合适大小的报文

![img](https://pic2.zhimg.com/80/v2-665630da201e46769efdee226f80fb49_720w.webp)

![img](https://pic4.zhimg.com/80/v2-01adba799539170ad6792a6348095467_720w.webp)

- 用户数据报 UDP 有两个字段：数据字段和首部字段。首部字段有 8 个字节，由 4 个字段组成，每个字段都是两个字节。
- 在计算检验和时，临时把“伪首部”和 UDP 用户数据报连接在一起。伪首部仅仅是为了计算检验和。

![img](https://pic4.zhimg.com/80/v2-5be649b92499b30a5d86e18ce4acf377_720w.webp)

## 3.传输控制协议 TCP 概述

### **3.1.TCP 最主要的特点**

- TCP 是面向连接的运输层协议。
- 每一条 TCP 连接只能有两个端点(endpoint)，每一条 TCP 连接只能是点对点的（一对一）。
- TCP 提供可靠交付的服务。
- TCP 提供全双工通信。
- 面向字节流。

![img](https://pic2.zhimg.com/80/v2-0534ecffd29ca2ed612c03c6163e15bd_720w.webp)

注意：

- TCP 连接是一条虚连接而不是一条真正的物理连接。
- TCP 对应用进程一次把多长的报文发送到TCP 的缓存中是不关心的。
- TCP 根据对方给出的窗口值和当前网络拥塞的程度来决定一个报文段应包含多少个字节（UDP 发送的报文长度是应用进程给出的）。
- TCP 可把太长的数据块划分短一些再传送。TCP 也可等待积累有足够多的字节后再构成报文段发送出去。

### 3.2.TCP的连接

TCP 把连接作为最基本的抽象。

- 每一条 TCP 连接有两个端点。
- TCP 连接的端点不是主机，不是主机的IP 地址，不是应用进程，也不是运输层的协议端口。TCP 连接的端点叫做套接字(socket)或插口。
- 端口号拼接到(contatenated with) IP 地址即构成了套接字。
- 每一条 TCP 连接唯一地被通信两端的两个端点（即两个套接字）所确定

## 4.可靠传输工作原理

### **4.1.停止等待协议**

![img](https://pic4.zhimg.com/80/v2-42bc4969152d4e561c47a0075748948b_720w.webp)

- 在发送完一个分组后，必须暂时保留已发送的分组的副本。
- 分组和确认分组都必须进行编号。
- 超时计时器的重传时间应当比数据在分组传输的平均往返时间更长一些。

![img](https://pic4.zhimg.com/80/v2-ed1b3cdf43ae272c7000aa74db0623bf_720w.webp)

- 使用上述的确认和重传机制，我们就可以在不可靠的传输网络上实现可靠的通信。
- 这种可靠传输协议常称为自动重传请求ARQ (Automatic Repeat reQuest)。
- ARQ 表明重传的请求是自动进行的。接收方不需要请求发送方重传某个出错的分组 。

缺点：

- 停止等待协议的优点是简单，但缺点是信道利用率太低。

流水线传输：

- 发送方可连续发送多个分组，不必每发完一个分组就停顿下来等待对方的确认。
- 由于信道上一直有数据不间断地传送，这种传输方式可获得很高的信道利用率。

![img](https://pic3.zhimg.com/80/v2-b8ae2eba87d72485f593afa6c3b85756_720w.webp)

### 4.2.连续ARQ协议

![img](https://pic2.zhimg.com/80/v2-db4a23c3d715af266cebcf868a153405_720w.webp)

累积确认:

- 接收方一般采用累积确认的方式。即不必对收到的分组逐个发送确认，而是对按序到达的最后一个分组发送确认，这样就表示：到这个分组为止的所有分组都已正确收到了。
- 累积确认有的优点是：容易实现，即使确认丢失也不必重传。缺点是：不能向发送方反映出接收方已经正确收到的所有分组的信息。

Go-back-N(回退 N):

- 如果发送方发送了前 5 个分组，而中间的第 3 个分组丢失了。这时接收方只能对前两个分组发出确认。发送方无法知道后面三个分组的下落，而只好把后面的三个分组都再重传一次。
- 这就叫做 Go-back-N（回退 N），表示需要再退回来重传已发送过的 N 个分组。
- 可见当通信线路质量不好时，连续 ARQ 协议会带来负面的影响。

TCP 可靠通信的具体实现:

- TCP 连接的每一端都必须设有两个窗口——一个发送窗口和一个接收窗口。
- TCP 的可靠传输机制用字节的序号进行控制。TCP 所有的确认都是基于序号而不是基于报文段。
- TCP 两端的四个窗口经常处于动态变化之中。
- TCP连接的往返时间 RTT 也不是固定不变的。需要使用特定的算法估算较为合理的重传时间。

## 5.TCP报文段的首部格式

![img](https://pic4.zhimg.com/80/v2-569b592580af7a114fbe9aeae30711ef_720w.webp)

- 源端口和目的端口字段——各占 2 字节。端口是运输层与应用层的服务接口。运输层的复用和分用功能都要通过端口才能实现。
- 序号字段——占 4 字节。TCP 连接中传送的数据流中的每一个字节都编上一个序号。序号字段的值则指的是本报文段所发送的数据的第一个字节的序号。
- 确认号字段——占 4 字节，是期望收到对方的下一个报文段的数据的第一个字节的序号。
- 数据偏移（即首部长度）——占 4 位，它指出 TCP 报文段的数据起始处距离 TCP 报文段的起始处有多远。“数据偏移”的单位是 32 位字（以 4 字节为计算单位）。
- 保留字段——占 6 位，保留为今后使用，但目前应置为 0。
- 紧急 URG —— 当 URG  1 时，表明紧急指针字段有效。它告诉系统此报文段中有紧急数据，应尽快传送(相当于高优先级的数据)。
- 确认 ACK —— 只有当 ACK  1 时确认号字段才有效。当 ACK  0 时，确认号无效。
- 推送 PSH (PuSH) —— 接收 TCP 收到 PSH = 1 的报文段，就尽快地交付接收应用进程，而不再等到整个缓存都填满了后再向上交付。
- 复位 RST (ReSeT) —— 当 RST  1 时，表明 TCP 连接中出现严重差错（如由于主机崩溃或其他原因），必须释放连接，然后再重新建立运输连接。
- 同步 SYN —— 同步 SYN = 1 表示这是一个连接请求或连接接受报文。
- 终止 FIN (FINis) —— 用来释放一个连接。FIN  1 表明此报文段的发送端的数据已发送完毕，并要求释放运输连接。
- 窗口字段 —— 占 2 字节，用来让对方设置发送窗口的依据，单位为字节。
- 检验和 —— 占 2 字节。检验和字段检验的范围包括首部和数据这两部分。在计算检验和时，要在 TCP 报文段的前面加上 12 字节的伪首部。
- 紧急指针字段 —— 占 16 位，指出在本报文段中紧急数据共有多少个字节（紧急数据放在本报文段数据的最前面）。
- 选项字段 —— 长度可变。TCP 最初只规定了一种选项，即最大报文段长度 MSS。MSS (Maximum Segment Size)是 TCP 报文段中的数据字段的最大长度。数据字段加上 TCP 首部才等于整个的 TCP 报文段。后面Tcp又增加了其他的选项：
- - 窗口扩大选项 ——占 3 字节，其中有一个字节表示移位值 S。新的窗口值等于TCP 首部中的窗口位数增大到(16 + S)，相当于把窗口值向左移动 S 位后获得实际的窗口大小。
  - 时间戳选项——占10 字节，其中最主要的字段时间戳值字段（4 字节）和时间戳回送回答字段（4 字节）。
  - 选择确认选项——在后面的 5.6.3 节介绍。
- 填充字段 —— 这是为了使整个首部长度是 4 字节的整数倍。

## 6.TCP可靠传输的实现

### **6.1.以字节为单位的滑动窗口**

![img](https://pic2.zhimg.com/80/v2-e22fa019532ba9fc36c47448281ae319_720w.webp)

![img](https://pic1.zhimg.com/80/v2-e855ede9ce84e0c6bce22a8262133dd4_720w.webp)

![img](https://pic2.zhimg.com/80/v2-79097b9b3f94ab19416fdba9ac6928b1_720w.webp)

![img](https://pic1.zhimg.com/80/v2-64db1f036c69c77a67e7ac067a9ecfbc_720w.webp)

发送缓存与接收缓存的作用：

- 发送缓存用来暂时存放：
- - 发送应用程序传送给发送方 TCP 准备发送的数据；
  - TCP 已发送出但尚未收到确认的数据。

![img](https://pic2.zhimg.com/80/v2-89711cce051f15f523077a7c83ccd345_720w.webp)

- 接收缓存用来暂时存放：
- - 按序到达的、但尚未被接收应用程序读取的数据；
  - 不按序到达的数据。

![img](https://pic2.zhimg.com/80/v2-e481f9a5d115defb9b58312763690ac5_720w.webp)

需要强调三点：

- A 的发送窗口并不总是和 B 的接收窗口一样大（因为有一定的时间滞后）。
- TCP 标准没有规定对不按序到达的数据应如何处理。通常是先临时存放在接收窗口中，等到字节流中所缺少的字节收到后，再按序交付上层的应用进程。
- TCP 要求接收方必须有累积确认的功能，这样可以减小传输开销。

### 6.2.超时重传时间的选择

- 重传机制是 TCP 中最重要和最复杂的问题之一。
- TCP 每发送一个报文段，就对这个报文段设置一次计时器。只要计时器设置的重传时间到但还没有收到确认，就要重传这一报文段。

加权平均往返时间：

- TCP 保留了 RTT 的一个加权平均往返时间 RTTS（这又称为平滑的往返时间）。
- 第一次测量到 RTT 样本时，RTTS 值就取为所测量到的 RTT 样本值。以后每测量到一个新的 RTT 样本，就按下式重新计算一次 RTTS：
  新的 RTTS = (1 -a) * (旧的 RTTS)+ a* (新的 RTT 样本) (5-4)式中，0 < a < 1。若  很接近于零，表示 RTT 值更新较慢。若选择a 接近于 1，则表示 RTT 值更新较快。
- R- FC 2988 推荐的 a 值为 1/8，即 0.125。

超时重传时间RTO:

- RTO 应略大于上面得出的加权平均往返时间 RTTS。
- RTO = RTTS + 4 * RTTD
- RTTD 是 RTT 的偏差的加权平均值。
- RFC 2988 建议这样计算 RTTD。第一次测量时，RTTD 值取为测量到的 RTT 样本值的一半。在以后的测量中，则使用下式计算加权平均的 RTTD：
- - 新的 RTTD = (1 - b) * (旧的RTTD)
    \+ b * |RTTS - 新的 RTT 样本|
- 
- b 是个小于 1 的系数，其推荐值是 1/4，即 0.25。

Karn算法：
在计算平均往返时间 RTT 时，只要报文段重传了，就不采用其往返时间样本。
这样得出的加权平均平均往返时间 RTTS 和超时重传时间 RTO 就较准确。
修正的 Karn 算法：

- 报文段每重传一次，就把 RTO 增大一些：
- - 新的 RTO = y * (旧的 RTO)
- 
- 系数 y 的典型值是 2 。
- 当不再发生报文段的重传时，才根据报文段的往返时延更新平均往返时延 RTT 和超时重传时间 RTO 的数值。
- 实践证明，这种策略较为合理。

### **6.3.选择确认SACK**

- 接收方收到了和前面的字节流不连续的两个字节块。
- 如果这些字节的序号都在接收窗口之内，那么接收方就先收下这些数据，但要把这些信息准确地告诉发送方，使发送方不要再重复发送这些已收到的数据。

## 7.TCP的流量控制

### **7.1.利用滑动窗口实现流量控制**

- 一般说来，我们总是希望数据传输得更快一些。但如果发送方把数据发送得过快，接收方就可能来不及接收，这就会造成数据的丢失。
- 流量控制(flow control)就是让发送方的发送速率不要太快，既要让接收方来得及接收，也不要使网络发生拥塞。
- 利用滑动窗口机制可以很方便地在 TCP 连接上实现流量控制。

![img](https://pic3.zhimg.com/80/v2-436e42cf04269bb6cc143fdb1363114e_720w.webp)

持续计时器：

- TCP 为每一个连接设有一个持续计时器。
- 只要 TCP 连接的一方收到对方的零窗口通知，就启动持续计时器。
- 若持续计时器设置的时间到期，就发送一个零窗口探测报文段（仅携带 1 字节的数据），而对方就在确认这个探测报文段时给出了现在的窗口值。
- 若窗口仍然是零，则收到这个报文段的一方就重新设置持续计时器。
- 若窗口不是零，则死锁的僵局就可以打破了。

### 7.2.必须考虑传输效率

可以用不同的机制来控制 TCP 报文段的发送时机:

- 第一种机制是 TCP 维持一个变量，它等于最大报文段长度 MSS。只要缓存中存放的数据达到 MSS 字节时，就组装成一个 TCP 报文段发送出去。
- 第二种机制是由发送方的应用进程指明要求发送报文段，即 TCP 支持的推送(push)操作。
- 第三种机制是发送方的一个计时器期限到了，这时就把当前已有的缓存数据装入报文段（但长度不能超过 MSS）发送出去。

## 8.TCP的拥塞控制

### **8.1.拥塞控制的一般原理**

- 在某段时间，若对网络中某资源的需求超过了该资源所能提供的可用部分，网络的性能就要变坏——产生拥塞(congestion)。
- 出现资源拥塞的条件：
- - 对资源需求的总和 > 可用资源
- 
- 若网络中有许多资源同时产生拥塞，网络的性能就要明显变坏，整个网络的吞吐量将随输入负荷的增大而下降。

拥塞控制与流量控制的关系：

- 拥塞控制所要做的都有一个前提，就是网络能够承受现有的网络负荷。
- 拥塞控制是一个全局性的过程，涉及到所有的主机、所有的路由器，以及与降低网络传输性能有关的所有因素。
- 流量控制往往指在给定的发送端和接收端之间的点对点通信量的控制。
- 流量控制所要做的就是抑制发送端发送数据的速率，以便使接收端来得及接收。

拥塞控制的一般原理：

- 拥塞控制是很难设计的，因为它是一个动态的（而不是静态的）问题。
- 当前网络正朝着高速化的方向发展，这很容易出现缓存不够大而造成分组的丢失。但分组的丢失是网络发生拥塞的征兆而不是原因。
- 在许多情况下，甚至正是拥塞控制本身成为引起网络性能恶化甚至发生死锁的原因。这点应特别引起重视。

开环控制和闭环控制：

- 开环控制方法就是在设计网络时事先将有关发生拥塞的因素考虑周到，力求网络在工作时不产生拥塞。
- 闭环控制是基于反馈环路的概念。属于闭环控制的有以下几种措施：
- - 监测网络系统以便检测到拥塞在何时、何处发生。
  - 将拥塞发生的信息传送到可采取行动的地方。
  - 调整网络系统的运行以解决出现的问题。

### 8.2.几种拥塞控制方法

1.慢开始和拥塞避免

- 发送方维持一个叫做拥塞窗口 cwnd (congestion window)的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞窗口。如再考虑到接收方的接收能力，则发送窗口还可能小于拥塞窗口。
- 发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数。

慢开始算法的原理：

- 在主机刚刚开始发送报文段时可先设置拥塞窗口 cwnd = 1，即设置为一个最大报文段 MSS 的数值。
- 在每收到一个对新的报文段的确认后，将拥塞窗口加 1，即增加一个 MSS 的数值。
- 用这样的方法逐步增大发送端的拥塞窗口 cwnd，可以使分组注入到网络的速率更加合理。

![img](https://pic1.zhimg.com/80/v2-d2772f048ce6bfd2721ba2c84310ab2c_720w.webp)

传输轮次：

- 使用慢开始算法后，每经过一个传输轮次，拥塞窗口 cwnd 就加倍。
- 一个传输轮次所经历的时间其实就是往返时间 RTT。
- “传输轮次”更加强调：把拥塞窗口 cwnd 所允许发送的报文段都连续发送出去，并收到了对已发送的最后一个字节的确认。
- 例如，拥塞窗口 cwnd = 4，这时的往返时间 RTT 就是发送方连续发送 4 个报文段，并收到这 4 个报文段的确认，总共经历的时间。

设置慢开始门限状态变量ssthresh：

- 慢开始门限 ssthresh 的用法如下：
- 当 cwnd < ssthresh 时，使用慢开始算法。
- 当 cwnd > ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。
- 当 cwnd = ssthresh 时，既可使用慢开始算法，也可使用拥塞避免算法。
- 拥塞避免算法的思路是让拥塞窗口 cwnd 缓慢地增大，即每经过一个往返时间 RTT 就把发送方的拥塞窗口 cwnd 加 1，而不是加倍，使拥塞窗口 cwnd 按线性规律缓慢增长。

当网络出现拥塞时：

- 无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有按时收到确认），就要把慢开始门限 ssthresh 设置为出现拥塞时的发送方窗口值的一半（但不能小于2）。
- 然后把拥塞窗口 cwnd 重新设置为 1，执行慢开始算法。
- 这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。

![img](https://pic2.zhimg.com/80/v2-d2fcad84c742acf85d09efb9fb43a935_720w.webp)

加法增大：
“加法增大”是指执行拥塞避免算法后，在收到对所有报文段的确认后（即经过一个往返时间），就把拥塞窗口 cwnd增加一个 MSS 大小，使拥塞窗口缓慢增大，以防止网络过早出现拥塞。
“拥塞避免”并非指完全能够避免了拥塞。利用以上的措施要完全避免网络拥塞还是不可能的。
“拥塞避免”是说在拥塞避免阶段把拥塞窗口控制为按线性规律增长，使网络比较不容易出现拥塞。
2.快重传和快恢复

- 快重传算法首先要求接收方每收到一个失序的报文段后就立即发出重复确认。这样做可以让发送方及早知道有报文段没有到达接收方。
- 发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段。
- 不难看出，快重传并非取消重传计时器，而是在某些情况下可更早地重传丢失的报文段。

![img](https://pic2.zhimg.com/80/v2-ea3b5996a9f687fb7bcb9a1a2252469d_720w.webp)

快恢复算法 ：

- (1) 当发送端收到连续三个重复的确认时，就执行“乘法减小”算法，把慢开始门限 ssthresh 减半。但接下去不执行慢开始算法。
- (2)由于发送方现在认为网络很可能没有发生拥塞，因此现在不执行慢开始算法，即拥塞窗口 cwnd 现在不设置为 1，而是设置为慢开始门限 ssthresh 减半后的数值，然后开始执行拥塞避免算法（“加法增大”），使拥塞窗口缓慢地线性增大。

![img](https://pic2.zhimg.com/80/v2-5b64e2b97867ed74fcd240ba7476e085_720w.webp)

发送窗口的上限值：

- 发送方的发送窗口的上限值应当取为接收方窗口 rwnd 和拥塞窗口 cwnd 这两个变量中较小的一个，即应按以下公式确定：
  发送窗口的上限值=Min [rwnd, cwnd] (5-8)
- 当 rwnd < cwnd 时，是接收方的接收能力限制发送窗口的最大值。
- 当 cwnd < rwnd 时，则是网络的拥塞限制发送窗口的最大值。

## 9.TCP 的运输连接管理

### 9.**1.运输连接的三个阶段**

- 运输连接就有三个阶段，即：连接建立、数据传送和连接释放。运输连接的管理就是使运输连接的建立和释放都能正常地进行。
- 连接建立过程中要解决以下三个问题：
- - 要使每一方能够确知对方的存在。
  - 要允许双方协商一些参数（如最大报文段长度，最大窗口大小，服务质量等）。
  - 能够对运输实体资源（如缓存大小，连接表中的项目等）进行分配。

客户-服务器方式：

- TCP 连接的建立都是采用客户服务器方式。
- 主动发起连接建立的应用进程叫做客户(client)。
- 被动等待连接建立的应用进程叫做服务器(server)。

![img](https://pic1.zhimg.com/80/v2-a774eb919bbc24129dde69ddf1ff9f1c_720w.webp)

- A 的 TCP 向 B 发出连接请求报文段，其首部中的同步位 SYN = 1，并选择序号 seq = x，表明传送数据时的第一个数据字节的序号是 x。
- B 的 TCP 收到连接请求报文段后，如同意，则发回确认。B 在确认报文段中应使 SYN = 1，使 ACK = 1，其确认号ack = x  1，自己选择的序号 seq = y。
- A 收到此报文段后向 B 给出确认，其 ACK = 1，确认号 ack = y  1。A 的 TCP 通知上层应用进程，连接已经建立。
- B 的 TCP 收到主机 A 的确认后，也通知其上层应用进程：TCP 连接已经建立。

### 9.**2.TCP的释放**

![img](https://pic3.zhimg.com/80/v2-e5df041bc82a8942b24ab690961e6172_720w.webp)

- 数据传输结束后，通信的双方都可释放连接。现在 A 的应用进程先向其 TCP 发出连接释放报文段，并停止再发送数据，主动关闭 TCP 连接。A 把连接释放报文段首部的 FIN = 1，其序号seq = u，等待 B 的确认。
- B 发出确认，确认号 ack = u  1，而这个报文段自己的序号 seq = v。TCP 服务器进程通知高层应用进程。从 A 到 B 这个方向的连接就释放了，TCP 连接处于半关闭状态。B 若发送数据，A 仍要接收。
- 若 B 已经没有要向 A 发送的数据，其应用进程就通知 TCP 释放连接。
- A 收到连接释放报文段后，必须发出确认。
- 在确认报文段中 ACK = 1，确认号 ack  w  1，自己的序号 seq = u + 1。TCP 连接必须经过时间 2MSL 后才真正释放掉。

A 必须等待 2MSL 的时间：

- 第一，为了保证 A 发送的最后一个 ACK 报文段能够到达 B。
- 第二，防止 “已失效的连接请求报文段”出现在本连接中。A 在发送完最后一个 ACK 报文段后，再经过时间 2MSL，就可以使本连接持续的时间内所产生的所有报文段，都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求报文段。



原文链接：https://zhuanlan.zhihu.com/p/275936585

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.139】网络IO模型的介绍引出nginx的网络IO模型

## 1. 什么是IO？

  简单来说就是输入输出

## 2. 网络IO经历步骤

- 1. 用户在获取网络资源是在进入网卡，经过网络七层模型将请求交给nginx用户进程
  2. 用户进程无法直接获取磁盘上的资源，会将请求获取什么资源翻译并转发给内核，内核驱动磁盘寻道找到文件（最耗时间的环节）
  3. 文件同样也不能直接交给用户进程，首先磁盘将文件放至内核缓冲区，然后内核告知用户进程请求的资源结果已准备好（耗时比较短）
  4. 内核缓冲区将文件拷贝一份至用户进程缓冲区，用户进程拿到文件 构建响应报文，通过http回传给用户

- ![img](https://pic4.zhimg.com/80/v2-74c2c051ead8d5e7575b4d620c838c63_720w.webp)

- 总结：
- IO总共要经历两个阶段
- 1.磁盘读取数据，将数据拷贝至内核缓冲区
- 2.将内核缓冲区数据拷贝至用户进程缓冲区

------

## 3. 网络IO模型种类

### 3.1.同步阻塞

- 比如你要去完成这样一件事情，把水烧开，灌入暖瓶
- 现在给你一口普通的锅，你也不知道水什么时候可以烧开，你决定守在旁边等待水开。
- 这个时候的你是什么也做不了的，是不是觉得很浪费时间
- 当水开了就需要你把水灌入暖瓶，你还是做不了其他的事情

- ![img](https://pic1.zhimg.com/80/v2-8d30ffd63c1dd5e2c539bfec2ada014c_720w.webp)

### 3.2.同步非阻塞

比如还是烧一锅开水，你还是不知道水什么时候会开，你选择了另外一个策略，去忙其它的事情但是要时不时看下谁开没开

### 3.3.改进了什么呢？

你可以在空挡时间忙其它的事情

### 3.4.增加了什么麻烦呢？

你得时不时的确认水开没开(就意味着得不停的调用cpu，占用cpu资源）

可能你上一次刚看的时候水没开，等你刚看完水开了，那就得等下一次查看的时候才能发现，增加了延迟，等你知道了水开了，自己去把水灌好

![img](https://pic2.zhimg.com/80/v2-d1cbb03b4c24e49aec6527482bc27f39_720w.webp)

### 3.5.异步阻塞

- 这个就比较二了
- 假如给你一个智能热水壶，水开了会自动的提醒你的，但是你还是要傻傻的等
- 完全没有利用好智能热水壶的优势（不做解释）

- ![img](https://pic2.zhimg.com/80/v2-8409cfe5e921afadeb490dbea39e6221_720w.webp)

同步异步：关注的是消息的通知机制*

阻塞非阻塞：关注的是在收到结果之前调用者的状态是等待还是忙自己的事情*

### 3.6.IO多路复用

- 还拿烧水的例子
- 不要你来管水开没开，直接交给一个代理，假如代理名叫select，select在那帮你监控水开没开，捎带他还会看饭有没有煮好，你把所有的事情都交代给它，可以去忙自己的事情了在饭还没熟，水还没开的时候select就处于一个等待的状态，假如这个时候水开了select会水开了的结果写到一个小黑板你可以看到并知道水开了，不需要你去查看水开没开，接下来由你将烧开的水灌进暖瓶（用户进程前半部分不阻塞，后半部份阻塞）

- ![img](https://pic3.zhimg.com/80/v2-3e3c6249511eb75bf6b026f8e5f52386_720w.webp)

### 3.7.select、poll、epoll的区别

无论是select、poll、epoll都可以面对多个用户进程的请求，它相当于一个代理人，收集很多用户进程的请求，他帮你从磁盘上获取数据，复制到内核中，那么这个数据准备没准备好它的实现机制是不一样的

通知方式：假设有一个用户数据准备好了，那么还有很多用户数据还没有准备好，那么如何通知？

select和poll是遍历扫描，效率低下

epoll采用回调机制，epoll会主动通知，效率更高（nginx支持）

### 3.8.信号驱动IO

- 举例：比如我们的智能热水壶我么设定好烧水以后，会给你一个反馈结果为设定成功，然后你就可以去忙其他的事情了，等到水烧开了会发给你一条短息，这个时候你再回去把水灌入暖瓶

- ![img](https://pic1.zhimg.com/80/v2-190efdab6bda949111ca47cec8a527bc_720w.webp)

- 通知机制：
  水平触发：多次通知
  边缘触发：只通知一次

- ![img](https://pic3.zhimg.com/80/v2-e94c61f777d1a25e4871637cb10332ce_720w.webp)

原文链接：https://zhuanlan.zhihu.com/p/283073309

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.140】Nginx底层原理：解析Nginx为什么并发数可以达到3w!

**Nginx 以其高性能，稳定性，丰富的功能，简单的配置和低资源消耗而闻名。本文从底层原理分析 Nginx 为什么这么快！**

## 1.**Nginx 的进程模型**

![img](https://pic2.zhimg.com/80/v2-590a9968f53238523488d908ca96f861_720w.webp)

**Nginx 服务器，正常运行过程中：**

**多进程：**一个 Master 进程、多个 Worker 进程。

**Master 进程：**管理 Worker 进程。对外接口：接收外部的操作（信号）；对内转发：根据外部的操作的不同，通过信号管理 Worker；**监控：**监控 Worker 进程的运行状态，Worker 进程异常终止后，自动重启 Worker 进程。

**Worker 进程：**所有 Worker 进程都是平等的。实际处理：网络请求，由 Worker 进程处理。Worker 进程数量**：**在 nginx.conf 中配置，一般设置为核心数，充分利用 CPU 资源，同时，避免进程数量过多，避免进程竞争 CPU 资源，增加上下文切换的损耗。

**思考：**

请求是连接到 Nginx，Master 进程负责处理和转发？

如何选定哪个 Worker 进程处理请求？请求的处理结果，是否还要经过 Master 进程？

![img](https://pic4.zhimg.com/80/v2-009ce38b1845f40a1ab8a897063158df_720w.webp)

## **2.HTTP 连接建立和请求处理过程**

**HTTP 连接建立和请求处理过程如下：**

Nginx 启动时，Master 进程，加载配置文件。

Master 进程，初始化监听的 Socket。

Master 进程，Fork 出多个 Worker 进程。

Worker 进程，竞争新的连接，获胜方通过三次握手，建立 Socket 连接，并处理请求。

## **3.Nginx 高性能、高并发**

**Nginx 为什么拥有高性能并且能够支撑高并发？**

Nginx 采用多进程+异步非阻塞方式（IO 多路复用 Epoll）。

请求的完整过程：建立连接→读取请求→解析请求→处理请求→响应请求。

请求的完整过程对应到底层就是：读写 Socket 事件。

## **4.Nginx 的事件处理模型**

Request：Nginx 中 HTTP 请求。

**基本的 HTTP Web Server 工作模式：**

**接收请求：**逐行读取请求行和请求头，判断段有请求体后，读取请求体。

**处理请求。**

**返回响应：**根据处理结果，生成相应的 HTTP 请求（响应行、响应头、响应体）。

**Nginx 也是这个套路，整体流程一致：**

![img](https://pic3.zhimg.com/80/v2-077101b70520a60dfa8d5f86ffb4b5c6_720w.webp)

**模块化体系结构**

![img](https://pic2.zhimg.com/80/v2-82d8eb85e2dfb40fa2131c1f42afa8c9_720w.webp)

## 5.**Nginx 的模块根据其功能基本上可以分为以下几种类型：**

**①event module：**搭建了独立于操作系统的事件处理机制的框架，及提供了各具体事件的处理。包括 ngx_events_module，ngx_event_core_module 和 ngx_epoll_module 等。

Nginx 具体使用何种事件处理模块，这依赖于具体的操作系统和编译选项。

**②phase handler：**此类型的模块也被直接称为 handler 模块。主要负责处理客户端请求并产生待响应内容，比如 ngx_http_static_module 模块，负责客户端的静态页面请求处理并将对应的磁盘文件准备为响应内容输出。

**③output filter：**也称为 filter 模块，主要是负责对输出的内容进行处理，可以对输出进行修改。

例如，可以实现对输出的所有 html 页面增加预定义的 footbar 一类的工作，或者对输出的图片的 URL 进行替换之类的工作。

**④upstream：**upstream 模块实现反向代理的功能，将真正的请求转发到后端服务器上，并从后端服务器上读取响应，发回客户端。

upstream 模块是一种特殊的 handler，只不过响应内容不是真正由自己产生的，而是从后端服务器上读取的。

**⑤load-balancer：**负载均衡模块，实现特定的算法，在众多的后端服务器中，选择一个服务器出来作为某个请求的转发服务器。

## 6.**常见问题剖析**

**Nginx vs Apache**

Nginx：

IO 多路复用，Epoll（freebsd 上是 kqueue）

高性能

高并发

占用系统资源少

**Apache：**

阻塞+多进程/多线程

更稳定，Bug 少

模块更丰富

**Nginx 最大连接数**

基础背景：

Nginx 是多进程模型，Worker 进程用于处理请求。

单个进程的连接数（文件描述符 fd），有上限（nofile）：ulimit -n。

Nginx 上配置单个 Worker 进程的最大连接数：worker_connections 上限为 nofile。

Nginx 上配置 Worker 进程的数量：worker_processes。

因此，Nginx 的最大连接数：

Nginx 的最大连接数：Worker 进程数量 x 单个 Worker 进程的最大连接数。

上面是 Nginx 作为通用服务器时，最大的连接数。

Nginx 作为反向代理服务器时，能够服务的最大连接数：（Worker 进程数量 x 单个 Worker 进程的最大连接数）/ 2。

Nginx 反向代理时，会建立 Client 的连接和后端 Web Server 的连接，占用 2 个连接。

## 7.**思考：**

每打开一个 Socket 占用一个 fd？

为什么，一个进程能够打开的 fd 数量有限制？

**HTTP 请求和响应**

HTTP 请求：

请求行：method、uri、http version

请求头

请求体

HTTP 响应：

响应行：http version、status code

响应头

响应体

**IO 模型**

处理多个请求时，可以采用：IO 多路复用或者阻塞 IO+多线程：

**IO 多路复用：**一个线程，跟踪多个 Socket 状态，哪个就绪，就读写哪个。

**阻塞 IO+多线程：**每一个请求，新建一个服务线程。

IO 多路复用和多线程的适用场景？

IO 多路复用：单个连接的请求处理速度没有优势。

大并发量：只使用一个线程，处理大量的并发请求，降低上下文环境切换损耗，也不需要考虑并发问题，相对可以处理更多的请求。

消耗更少的系统资源（不需要线程调度开销）。

适用于长连接的情况（多线程模式长连接容易造成线程过多，造成频繁调度）。

阻塞 IO +多线程：实现简单，可以不依赖系统调用。

每个线程，都需要时间和空间。

线程数量增长时，线程调度开销指数增长。

select/poll 和 epoll 比较如下：

## **8.select/poll 系统调用：**

// select 系统调用

int select(int maxfdp,fd_set *readfds,fd_set *writefds,fd_set *errorfds,struct timeval *timeout);

// poll 系统调用

int poll(struct pollfd fds[], nfds_t nfds, int timeout)；

**select：**

查询 fd_set 中，是否有就绪的 fd，可以设定一个超时时间，当有 fd (File descripter) 就绪或超时返回。

fd_set 是一个位集合，大小是在编译内核时的常量，默认大小为 1024。

特点：连接数限制，fd_set 可表示的 fd 数量太小了；线性扫描：判断 fd 是否就绪，需要遍历一边 fd_set；数据复制：用户空间和内核空间，复制连接就绪状态信息。

**poll：**

**解决了连接数限制：**poll 中将 select 中的 fd_set 替换成了一个 pollfd 数组，解决 fd 数量过小的问题。

**数据复制：**用户空间和内核空间，复制连接就绪状态信息。

## **9.epoll，event 事件驱动：**

**事件机制：**避免线性扫描，为每个 fd，注册一个监听事件，fd 变更为就绪时，将 fd 添加到就绪链表。

**fd 数量：**无限制（OS 级别的限制，单个进程能打开多少个 fd）。

**select，poll，epoll：**

I/O 多路复用的机制。

I/O 多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作；监视多个文件描述符。

但 select，poll，epoll 本质上都是同步 I/O：用户进程负责读写（从内核空间拷贝到用户空间），读写过程中，用户进程是阻塞的；异步 IO，无需用户进程负责读写，异步 IO，会负责从内核空间拷贝到用户空间。

## **10.Nginx 的并发处理能力**

关于 Nginx 的并发处理能力：并发连接数，一般优化后，峰值能保持在 1~3w 左右。（内存和 CPU 核心数不同，会有进一步优化空间）。



原文链接：https://zhuanlan.zhihu.com/p/288404156

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.141】常见C++面试题及基本知识点总结

**【转载请注明出处】：http://www.cnblogs.com/LUO77/p/5771237.html** 

## **1. 结构体和共同体的区别。**

定义：

结构体struct：把不同类型的数据组合成一个整体，自定义类型。

共同体union：使几个不同类型的变量共同占用一段内存。

地址：

struct和union都有内存对齐，结构体的内存布局依赖于CPU、操作系统、编译器及编译时的对齐选项。

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
关于内存对齐，先让我们看四个重要的基本概念：
1.数据类型自身的对齐值：
对于char型数据，其自身对齐值为1，对于short型为2，对于int,float,double类型，其自身对齐值为4，单位字节。
2.结构体或者类的自身对齐值：其成员中自身对齐值最大的那个值。
3.指定对齐值：#pragma pack(n)，n=1,2,4,8,16改变系统的对齐系数
4.数据成员、结构体和类的有效对齐值：自身对齐值和指定对齐值中小的那个值。
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 常见数据类型及其长度：

注意long int和int一样是4byte，long double和double一样是8byte。（关于long double，ANSI C标准规定了double变量存储为 IEEE 64 位（8 个字节）浮点数值，但并未规定long double的确切精度。所以对于不同平台可能有不同的实现。有的是8字节，有的是10字节，有的是12字节或16字节。）

在标准c++中，int的定义长度要依靠你的机器的字长，也就是说，如果你的机器是32位的，int的长度为32位，如果你的机器是64位的，那么int的标准长度就是64位。经测试，在64位操作系统下，int的长度还是32位的。

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160814201505156-1567273898.jpg)

从上面的一段文字中，我们可以看出，首先根据结构体内部成员的自身对齐值得到结构体的自身对齐值（**内部成员最大的长度**），如果没有修改系统设定的默认补齐长度4的话，取较小的进行内存补齐。

结构体struct：不同之处，stuct里每个成员都有自己独立的地址。sizeof(struct)是内存对齐后所有成员长度的加和。

共同体union：当共同体中存入新的数据后，原有的成员就失去了作用，新的数据被写到union的地址中。sizeof(union)是最长的数据成员的长度。

总结： struct和union都是由多个不同的数据类型成员组成, 但在任何同一时刻, union中只存放了一个被选中的成员, 而struct的所有成员都存在。在struct中，各成员都占有自己的内存空间，它们是同时存在的。一个struct变量的总长度等于所有成员长度之和。在Union中，所有成员不能同时占用它的内存空间，它们不能同时存在。Union变量的长度等于最长的成员的长度。对于union的不同成员赋值, 将会对其它成员重写, 原来成员的值就不存在了, 而对于struct的不同成员赋值是互不影响的。

 

------

 

##  **2.static 和const分别怎么用，类里面static和const可以同时修饰成员函数吗。**

 static的作用：

对变量：

1.局部变量：

在局部变量之前加上关键字static，局部变量就被定义成为一个局部静态变量。

 1）内存中的位置：静态存储区

 2）初始化：局部的静态变量只能被初始化一次，且C中不可以用变量对其初始化，而C++可以用变量对其初始化。（详见：http://www.cnblogs.com/novice-dxx/p/7094690.html）

 3）作用域：作用域仍为局部作用域，当定义它的函数或者语句块结束的时候，作用域随之结束。

 注：当static用来修饰局部变量的时候，它就**改变了局部变量的存储位置（从原来的栈中存放改为静态存储区）及其生命周期（局部静态变量在离开作用域之后，并没有被销毁，而是仍然驻留在内存当中，直到程序结束，只不过我们不能再对他进行访问），但未改变其作用域。**

2.全局变量

在全局变量之前加上关键字static，全局变量就被定义成为一个全局静态变量。

1）内存中的位置：静态存储区（静态存储区在整个程序运行期间都存在）

2）初始化：未经初始化的全局静态变量会被程序自动初始化为0（自动对象的值是任意的，除非他被显示初始化）

3）作用域：全局静态变量在声明他的文件之外是不可见的。准确地讲从定义之处开始到文件结尾。

注：static修饰全局变量，**并未改变其存储位置及生命周期，而是改变了其作用域，使当前文件外的源文件无法访问该变量**，好处如下：（1）不会被其他文件所访问，修改（2）其他文件中可以使用相同名字的变量，不会发生冲突。**对全局函数也是有隐藏作用。**而普通全局变量只要定义了，任何地方都能使用，使用前需要声明所有的.c文件，只能定义一次普通全局变量，但是可以声明多次（外部链接）。注意：全局变量的作用域是全局范围，但是在某个文件中使用时，必须先声明。

对类中的：

　　　　1.成员变量

　　　　用static修饰类的数据成员实际使其成为类的全局变量，会被类的所有对象共享，包括派生类的对象。因此，**static成员必须在类外进行初始化(\**初始化格式： int base::var=10;)\**，而不能在构造函数内进行初始化，不过也可以用const修饰static数据成员在类内初始化 。**

　　　　特点：

1. 1. 不要试图在头文件中定义(初始化)静态数据成员。在大多数的情况下，这样做会引起重复定义这样的错误。即使加上#ifndef #define #endif或者#pragma once也不行。 
   2. 静态数据成员可以成为成员函数的可选参数，而普通数据成员则不可以。
   3. 静态数据成员的类型可以是所属类的类型，而普通数据成员则不可以。普通数据成员的只能声明为 所属类类型的指针或引用。

2.成员函数

1. 1. 用static修饰成员函数，使这个类只存在这一份函数，所有对象共享该函数，不含this指针。
   2. 静态成员是可以独立访问的，也就是说，无须创建任何对象实例就可以访问。base::func(5,3);当static成员函数在类外定义时不需要加static修饰符。
   3. 在静态成员函数的实现中不能直接引用类中说明的非静态成员，可以引用类中说明的静态成员。因为静态成员函数不含this指针。 

**不可以同时用const和static修饰成员函数。**

C++编译器在实现const的成员函数的时候为了确保该函数不能修改类的实例的状态，会在函数中添加一个隐式的参数const this*。但当一个成员为static的时候，该函数是没有this指针的。也就是说此时const的用法和static是冲突的。

我们也可以这样理解：两者的语意是矛盾的。**static的作用是表示该函数只作用在类型的静态变量上，与类的实例没有关系；而const的作用是确保函数不能修改类的实例的状态**，与类型的静态变量没有关系。因此不能同时用它们。

const的作用：

 1.限定变量为不可修改。

2.限定成员函数不可以修改任何数据成员。

3.const与指针：

const char *p 表示 指向的内容不能改变。

char * const p，就是将P声明为常指针，它的地址不能改变，是固定的，但是它的内容可以改变。

------

 

##  **3.指针和引用的区别，引用可以用常指针实现吗。**

本质上的区别是，指针是一个新的变量，只是这个变量存储的是另一个变量的地址，我们通过访问这个地址来修改变量。

而引用只是一个别名，还是变量本身。对引用进行的任何操作就是对变量本身进行操作，因此以达到修改变量的目的。

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
(1)指针：指针是一个变量，只不过这个变量存储的是一个地址，指向内存的一个存储单元；而引用跟原来的变量实质上是同一个东西，只不过是原变量的一个别名而已。如：
int a=1;int *p=&a;
int a=1;int &b=a;
上面定义了一个整形变量和一个指针变量p，该指针变量指向a的存储单元，即p的值是a存储单元的地址。
而下面2句定义了一个整形变量a和这个整形a的引用b，事实上a和b是同一个东西，在内存占有同一个存储单元。
(2)可以有const指针，但是没有const引用（const引用可读不可改，与绑定对象是否为const无关）
(3)指针可以有多级，但是引用只能是一级（int **p；合法 而 int &&a是不合法的）
(4)指针的值可以为空，但是引用的值不能为NULL，并且引用在定义的时候必须初始化；
(5)指针的值在初始化后可以改变，即指向其它的存储单元，而引用在进行初始化后就不会再改变了。
(6)"sizeof引用"得到的是所指向的变量(对象)的大小，而"sizeof指针"得到的是指针本身的大小；
(7)指针和引用的自增(++)运算意义不一样；
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
指针传参的时候，还是值传递，试图修改传进来的指针的值是不可以的。只能修改地址所保存变量的值。
引用传参的时候，传进来的就是变量本身，因此可以被修改。
```

------

## **4.什么是多态，多态有什么用途。**

1. 定义：“一个接口，多种方法”，程序在运行时才决定调用的函数。
2. 实现：C++多态性主要是通过虚函数实现的，虚函数允许子类重写override(注意和overload的区别，overload是重载，是允许同名函数的表现，这些函数参数列表/类型不同）。

```
多态与非多态的实质区别就是函数地址是早绑定还是晚绑定。如果函数的调用，在编译器编译期间就可以确定函数的调用地址，并生产代码，是静态的，就是说地址是早绑定的。而如果函数调用的地址不能在编译器期间确定，需要在运行时才确定，这就属于晚绑定。
```

3.目的：**接口重用。**封装可以使得代码模块化，继承可以扩展已存在的代码，他们的目的都是为了代码重用。而多态的目的则是为了接口重用。

4.用法：声明基类的指针，利用该指针指向任意一个子类对象，调用相应的虚函数，可以根据指向的子类的不同而实现不同的方法。

补充一下关于重载、重写、隐藏（总是不记得）的区别：

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
Overload(重载)：在C++程序中，可以将语义、功能相似的几个函数用同一个名字表示，但参数或返回值不同（包括类型、顺序不同），即函数重载。
（1）相同的范围（在同一个类中）；
（2）函数名字相同；
（3）参数不同；
（4）virtual 关键字可有可无。
Override(重写)：是指派生类函数覆盖基类函数，特征是：
（1）不同的范围（分别位于派生类与基类）；
（2）函数名字相同；
（3）参数相同；
（4）基类函数必须有virtual 关键字。注：重写基类虚函数的时候，会自动转换这个函数为virtual函数，不管有没有加virtual，因此重写的时候不加virtual也是可以的，不过为了易读性，还是加上比较好。
Overwrite(隐藏)：隐藏，是指派生类的函数屏蔽了与其同名的基类函数，规则如下：
（1）如果派生类的函数与基类的函数同名，但是参数不同。此时，不论有无virtual关键字，基类的函数将被隐藏（注意别与重载混淆）。
（2）如果派生类的函数与基类的函数同名，并且参数也相同，但是基类函数没有virtual关键字。此时，基类的函数被隐藏（注意别与覆盖混淆）。
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

补充一下虚函数表：

多态是由虚函数实现的，而虚函数主要是通过**虚函数表（V-Table）**来实现的。

如果一个类中包含虚函数（virtual修饰的函数），那么这个类就会包含一张虚函数表，虚函数表存储的每一项是一个虚函数的地址。如下图：

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815122317593-1444517318.jpg)

这个类的每一个对象都会包含一个**虚指针**（虚指针存在于对象实例地址的最前面，保证虚函数表有最高的性能），这个虚指针指向虚函数表。

**注：对象不包含虚函数表，只有虚指针，类才包含虚函数表，派生类会生成一个兼容基类的虚函数表。**

- 原始基类的虚函数表

　　下图是原始基类的对象，可以看到虚指针在地址的最前面，指向基类的虚函数表（假设基类定义了3个虚函数）

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815123956531-397793609.png)

- 单继承时的虚函数（**无重写基类虚函数**）

假设现在派生类继承基类，并且重新定义了3个虚函数，派生类会自己产生一个兼容基类虚函数表的**属于自己的虚函数表**。

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815124419156-189096231.png)

　　Derive class 继承了 Base class 中的三个虚函数，准确的说，是该函数实体的地址被拷贝到 Derive类的虚函数表，派生类新增的虚函数置于虚函数表的后面，并**按声明顺序存放**。

- 单继承时的虚函数（**重写基类虚函数**）

现在派生类重写基类的x函数，可以看到这个派生类构建自己的虚函数表的时候，修改了base::x()这一项，指向了自己的虚函数。

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815124615562-420906064.png)

- 多重继承时的虚函数（Derived ::public Base1,public Base2）

这个派生类多重继承了两个基类base1，base2，因此它有两个虚函数表。

　　![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815125139921-1422683315.png)

　　它的对象会有**多个虚指针（据说和编译器相关）**，指向不同的虚函数表。

　　多重继承时指针的调整：

```
Derive b;
Base1* ptr1 = &b;   // 指向 b 的初始地址
Base2* ptr2 = &b;   // 指向 b 的第二个子对象
```

因为 Base1 是第一个基类，所以 ptr1 指向的是 Derive 对象的起始地址，不需要调整指针（偏移）。

因为 Base2 是第二个基类，所以必须对指针进行调整，即加上一个 offset，让 ptr2 指向 Base2 子对象。

当然，上述过程是由编译器完成的。

```
Base1* b1 = (Base1*)ptr2;
b1->y();                   // 输出 Base2::y()
Base2* b2 = (Base2*)ptr1;
b2->y();                   // 输出 Base1::y()
```

其实，**通过某个类型的指针访问某个成员时，编译器只是根据类型的定义查找这个成员所在偏移量，用这个偏移量获取成员**。由于 ptr2 本来就指向 Base2 子对象的起始地址，所以`b1->y()`调用到的是`Base2::y()`，而 ptr1 本来就指向 Base1 子对象的起始地址（即 Derive对象的起始地址），所以`b2->y()`调用到的是`Base1::y()`。

- 虚继承时的虚函数表

　　虚继承的引入把对象的模型变得十分复杂，除了每个基类（MyClassA和MyClassB）和公共基类（MyClass）的虚函数表指针需要记录外，每个虚拟继承了MyClass的父类还需要记录一个虚基类表vbtable的指针vbptr。MyClassC的对象模型如图4所示。

　　![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815131033578-1325553756.png)

　　　虚基类表每项记录了被继承的虚基类子对象相对于**虚基类表指针的偏移量**。比如MyClassA的虚基类表第二项记录值为24，正是MyClass::vfptr相对于MyClassA::vbptr的偏移量，同理MyClassB的虚基类表第二项记录值12也正是MyClass::vfptr相对于MyClassA::vbptr的偏移量。（[虚函数与虚继承深入探讨](http://www.cnblogs.com/fanzhidongyzby/archive/2013/01/14/2859064.html)）

对象模型探讨：

 

1.没有继承情况,vptr存放在对象的开始位置,以下是Base1的内存布局

m_iData :100


 2.单继承的情况下,对象只有一个vptr,它存放在对象的开始位置,派生类子对象在父类子对象的最后面,以下是D1的内存布局

| B1:: m_iData : 100 |
| ------------------ |
| B1::vptr : 4294800 |
| B2::vptr : 4294776 |
| D::m_iData :300    |


\4. 虚拟继承情况下,虚父类子对象会放在派生类子对象之后,派生类子对象的第一个位置存放着一个vptr,虚拟子类子对象也会保存一个vptr,以下是VD1的内存布局

 

| Unknown : 4294888      |
| ---------------------- |
| B1::vptr :4294864      |
| VD1::vptr :    4294944 |
| VD1::m_iData : 200     |
| VD2::Unknown : 4294952 |
| VD::m_iData : 500      |
| B1::m_iData : 100      |

\5. 棱形继承的情况下,非虚基类子对象在派生类子对象前面,并按照声明顺序排列,虚基类子对象在派生类子对象后面

| VD1::Unknown : 4294968 |
| ---------------------- |
| VD2::vptr :  4  294932 |
| VD2::m_iData : 300     |
| B1::vptr :    4294920  |
| B1::m_iData : 100      |

 

补充一下纯虚函数：

- 定义： 在很多情况下，基类本身生成对象是不合情理的。为了解决这个问题，方便使用类的多态性，引入了纯虚函数的概念，将函数定义为纯虚函数（方法：virtual ReturnType Function()= **0**;）纯虚函数不能再在基类中实现，编译器要求在派生类中必须予以重写以实现多态性。同时含有纯虚拟函数的类称为抽象类，它不能生成对象。
- 特点：

1，当想在基类中抽象出一个方法，且该基类只做能被继承，而不能被实例化；（避免类被实例化且在编译时候被发现，可以采用此方法）

2，这个方法必须在派生类(derived class)中被实现；

- 目的：使派生类仅仅只是继承函数的接口。

补充一下纯虚函数：

- 定义：称带有纯虚函数的类为抽象类。

- 作用：为一个继承体系提供一个公共的根，为派生类提供操作接口的通用语义。

- 特点：1.抽象类只能作为基类来使用，而继承了抽象类的派生类如果没有实现纯虚函数，而只是继承纯虚函数，那么该类仍旧是一个抽象类，如果实现了纯虚函数，就不再是抽象类。

　　　　　　2.抽象类不可以定义对象。

补充一下多重继承和虚继承：

多重继承：

定义：派生类继承多个基类，派生类为每个基类（显式或隐式地）指定了访问级别——`public`、`protected` 或 `private`。

```
    class Panda : public Bear, public Endangered {
    }
```

构造：

1. 1. 派生类的对象包含每个基类的基类子对象。
   2. 派生类构造函数初始化所有基类（多重继承中若没有显式调用某个基类的构造函数，则编译器会调用该基类默认构造函数），派生类只能初始化自己的基类，并不需要考虑基类的基类怎么初始化。
   3. 多重继承时，基类构造函数按照基类构造函数在类派生列表中的出现次序调用。

析构：总是按构造函数运行的**逆序**调用析构函数。（基类的析构函数最好写成virtual，否则再子类对象销毁的时候，无法销毁子类对象部分资源。）假定所有根基类都将它们的析构函数适当定义为虚函数，那么，无论通过哪种指针类型删除对象，虚析构函数的处理都是一致的。

 

拷贝构造/赋值：如果要为派生类编写拷贝构造函数，则需要为调用基类相应拷贝构造函数并为其传递参数，否则只会拷贝派生类部分。

```
深拷贝与浅拷贝：
浅拷贝：默认的复制构造函数只是完成了对象之间的位拷贝，也就是把对象里的值完全复制给另一个对象，如A=B。这时，如果B中有一个成员变量指针已经申请了内存，那A中的那个成员变量也指向同一块内存。　　　　这就出现了问题：当B把内存释放了（如：析构），这时A内的指针就是野指针了，出现运行错误。
深拷贝：自定义复制构造函数需要注意，对象之间发生复制，资源重新分配，即A有5个空间，B也应该有5个空间，而不是指向A的5个空间。
```

虚继承与虚基类：

定义：在多重继承下，一个基类可以在派生层次中出现多次。（派生类对象中可能出现多个基类对象）在 C++ 中，通过使用**虚继承**解决这类问题。虚继承是一种机制，类通过虚继承指出它希望共享其虚基类的状态。在虚继承下，对给定虚基类，无论该类在派生层次中作为虚基类出现多少次，只继承一个共享的基类子对象。共享的基类子对象称为**虚基类**。

用法：`istream` 和 `ostream` 类对它们的基类进行虚继承。通过使基类成为虚基类，`istream` 和 `ostream` 指定，如果其他类（如 `iostream` 同时继承它们两个，则派生类中只出现它们的公共基类ios的一个副本。通过在派生列表中包含关键字 `virtual` 设置虚基类：

```
    class istream : public virtual ios { ... };
    class ostream : virtual public ios { ... };
    // iostream inherits only one copy of its ios base class
    class iostream: public istream, public ostream { ... };
```

## **5.各个排序算法的时间复杂度和稳定性，快排的原理。**

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815163155765-181431145.png)

[排序深入探讨](http://blog.csdn.net/morewindows/article/details/7961256)

- 插入排序

　　每次将一个待排序的数据，跟前面已经有序的序列的数字一一比较找到自己合适的位置，插入到序列中，直到全部数据插入完成。

- 希尔排序

　　先将整个待排元素序列分割成若干个子序列（由相隔某个“增量”的元素组成的）分别进行直接插入排序，然后依次缩减增量再进行排序，待整个序列中的元素基本有序（增量足够小）时，再对全体元素进行一次直接插入排序。由于希尔排序是对**相隔若干距离**的数据进行**直接插入排序**，因此可以形象的称希尔排序为“**跳着插**”

- 冒泡排序

通过交换使相邻的两个数变成小数在前大数在后，这样每次遍历后，最大的数就“沉”到最后面了。重复N次即可以使数组有序。

冒泡排序改进1：在某次遍历中如果没有数据交换，说明整个数组已经有序。因此通过设置标志位来记录此次遍历有无数据交换就可以判断是否要继续循环。

冒泡排序改进2：记录某次遍历时最后发生数据交换的位置，这个位置之后的数据显然已经有序了。因此通过记录最后发生数据交换的位置就可以确定下次循环的范围了。

- 快速排序

**“挖坑填数+分治法”**，首先令i =L; j = R; 将a[i]挖出形成第一个坑，称a[i]为基准数。然后j--由后向前找比基准数小的数，找到后挖出此数填入前一个坑a[i]中，再i++由前向后找比基准数大的数，找到后也挖出此数填到前一个坑a[j]中。重复进行这种“挖坑填数”直到i==j。再将基准数填入a[i]中，这样i之前的数都比基准数小，i之后的数都比基准数大。因此将数组分成二部分再分别重复上述步骤就完成了排序。

- 选择排序

数组分成有序区和无序区，初始时整个数组都是无序区，然后每次从无序区选一个最小的元素直接放到有序区的最后，直到整个数组变有序区。

- 堆排序

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815164725671-1307064177.png)

堆的插入就是——每次插入都是将新数据放在数组最后，而从这个新数据的父结点到根结点必定是一个有序的数列，因此只要将这个新数据插入到这个有序数列中即可。

堆的删除就是——堆的删除就是将最后一个数据的值赋给根结点，然后再从根结点开始进行一次从上向下的调整。调整时先在左右儿子结点中找最小的，如果父结点比这个最小的子结点还小说明不需要调整了，反之将父结点和它交换后再考虑后面的结点。相当于从根结点开始将一个数据在有序数列中进行“下沉”。

因此，堆的插入和删除非常类似**直接插入排序**，只不是在**二叉树**上进行插入过程。所以可以将堆排序形容为“**树上插**”

- 归并排序

归并排序主要分为两步：分数列（divide），每次把数列一分为二，然后分到只有两个元素的小数列；合数列（Merge），合并两个已经内部有序的子序列，直至所有数字有序。用递归可以实现。

- 基数排序（桶排序）

基数排序，第一步根据数字的个位分配到每个桶里，在桶内部排序，然后将数字再输出（串起来）；然后根据十位分桶，继续排序，再串起来。直至所有位被比较完，所有数字已经有序。

 　![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815165252312-182869324.png)

------

 

## **6.vector中size()和capacity()的区别。**

 

size()指容器当前拥有的元素个数（对应的resize(size_type)会在容器尾添加或删除一些元素，来调整容器中实际的内容，使容器达到指定的大小。）；capacity（）指容器在必须分配存储空间之前可以存储的元素总数。

 

size表示的这个vector里容纳了多少个元素，capacity表示vector能够容纳多少元素，它们的不同是在于vector的size是2倍增长的。如果vector的大小不够了，比如现在的capacity是4，插入到第五个元素的时候，发现不够了，此时会给他重新分配8个空间，把原来的数据及新的数据复制到这个新分配的空间里。（会有迭代器失效的问题）

各容器的特点：

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815190552968-787753341.png)

 

------

## **7.map和set的原理。**

([map和set的四个问题](http://www.cnblogs.com/LUO77/p/5774476.html))

map和set的底层实现主要是由红黑树实现的。

红黑树：

性质1 节点是**红色**或**黑色**。

性质2 根节点是**黑色**。

性质3 每个叶节点（NIL节点，空节点）是**黑色**的。

性质4 每个**红色**节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点)

性质5 从任一节点到其每个叶子的所有路径都包含相同数目的**黑色**节点。

这些约束的好处是：保持了树的相对平衡，同时又比AVL的插入删除操作的复杂性要低许多。

（[深入探讨红黑树](http://daoluan.net/数据结构/算法/2013/09/25/rbtree-is-not-difficult.html)）

------

 

## **8.tcp为什么要三次握手，tcp为什么可靠。**

为什么不能两次握手：（防止已失效的连接请求又传送到服务器端，因而产生错误）

假设改为两次握手，client端发送的一个连接请求在服务器滞留了，这个连接请求是无效的，client已经是closed的状态了，而服务器认为client想要建立

一个新的连接，于是向client发送确认报文段，而client端是closed状态，无论收到什么报文都会丢弃。而如果是两次握手的话，此时就已经建立连接了。

服务器此时会一直等到client端发来数据，这样就浪费掉很多server端的资源。

```
（校注：此时因为client没有发起建立连接请求，所以client处于CLOSED状态，接受到任何包都会丢弃，谢希仁举的例子就是这种场景。但是如果服务器发送对这个延误的旧连接报文的确认的同时，客户端调用connect函数发起了连接，就会使客户端进入SYN_SEND状态，当服务器那个对延误旧连接报文的确认传到客户端时，因为客户端已经处于SYN_SEND状态，所以就会使客户端进入ESTABLISHED状态，此时服务器端反而丢弃了这个重复的通过connect函数发送的SYN包，见第三个图。而连接建立之后，发送包由于SEQ是以被丢弃的SYN包的序号为准，而服务器接收序号是以那个延误旧连接SYN报文序号为准，导致服务器丢弃后续发送的数据包）
```

**三次握手的最主要目的是保证连接是双工的，可靠更多的是通过重传机制来保证的。** 

TCP可靠传输的实现：

TCP 连接的每一端都必须设有两个窗口——一个发送窗口和一个接收窗口。TCP 的可靠传输机制用字节的序号进行控制。TCP 所有的确认都是基于序号而不是基于报文段。
发送过的数据未收到确认之前必须保留，以便超时重传时使用。发送窗口没收到确认不动，和收到新的确认后前移。

**发送缓存**用来暂时存放： 发送应用程序传送给发送方 TCP 准备发送的数据；TCP 已发送出但尚未收到确认的数据。

**接收缓存**用来暂时存放：按序到达的、但尚未被接收应用程序读取的数据； 不按序到达的数据。

```
必须强调三点：
    1>   A 的发送窗口并不总是和 B 的接收窗口一样大（因为有一定的时间滞后）。
    2>   TCP 标准没有规定对不按序到达的数据应如何处理。通常是先临时存放在接收窗口中，等到字节流中所缺少的字节收到后，再按序交付上层的应用进程。
    3>   TCP 要求接收方必须有累积确认的功能，这样可以减小传输开销（累积确认：一般地讲，如果发送方发了包1，包2，包3，包4；接受方成功收到包1，包2，包3。那么接受方可以发回一个确认包，序号为4(4表示期望下一个收到的包的序号；当然你约定好用3表示也可以)，那么发送方就知道包1到包3都发送接收成功，必要时重发包4。一个确认包确认了累积到某一序号的所有包。而不是对没个序号都发确认包。）
```

- TCP报文格式

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815220215171-1900260802.png)

 （1）序号：Seq序号，占32位，用来标识从TCP源端向目的端发送的字节流，发起方发送数据时对此进行标记。
 （2）确认序号：Ack序号，占32位，只有ACK标志位为1时，确认序号字段才有效，Ack=Seq+1。
 （3）标志位：共6个，即URG、ACK、PSH、RST、SYN、FIN等，具体含义如下：
 　　（A）URG：紧急指针（urgent pointer）有效。
 　　（B）ACK：确认序号有效。
 　　（C）PSH：接收方应该尽快将这个报文交给应用层。
 　　（D）RST：重置连接。
 　　（E）SYN：发起一个新连接。
 　　（F）FIN：释放一个连接。

 需要注意的是：
 （A）不要将确认序号Ack与标志位中的ACK搞混了。
 （B）确认方Ack=发起方Req+1，两端配对。

- 三次握手

TCP三次即建立TCP连接，指建立一个TCP连接时，需要客户端服务端总共发送3 个包以确认连接的建立。在socket编程中，这一过程中由客户端执行connect来触发，流程如下：

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815220601750-1518158934.png)

（1）第一次握手：Client将标志位SYN置为1（表示要发起一个连接），随机产生一个值seq=J，并将该数据包发送给Server，Client进入**SYN_SENT**状态，等待Server确认。
（2）第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入**SYN_RCVD**状态。
（3）第三次握手：Client收到确认后，**检查ack是否为J+1，ACK是否为1**，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。

 

```
SYN攻击：
  在三次握手过程中，Server发送SYN-ACK之后，收到Client的ACK之前的TCP连接称为半连接（half-open connect），此时Server处于SYN_RCVD状态，当收到ACK后，Server转入ESTABLISHED状态。SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server回复确认包，并等待Client的确认，由于源地址是不存在的，因此，Server需要不断重发直至超时，这些伪造的SYN包将产时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络堵塞甚至系统瘫痪。SYN攻击时一种典型的DDOS攻击，检测SYN攻击的方式非常简单，即当Server上有大量半连接状态且源IP地址是随机的，则可以断定遭到SYN攻击了，使用如下命令可以让之现行：
  #netstat -nap | grep SYN_RECV
ddos攻击：
分布式拒绝服务(DDoS:Distributed Denial of Service)攻击指借助于客户/服务器技术，将多个计算机联合起来作为攻击平台，对一个或多个目标发动DDoS攻击，从而成倍地提高拒绝服务攻击的威力。通常，攻击者使用一个偷窃帐号将DDoS主控程序安装在一个计算机上，在一个设定的时间主控程序将与大量代理程序通讯，代理程序已经被安装在网络上的许多计算机上。代理程序收到指令时就发动攻击。利用客户/服务器技术，主控程序能在几秒钟内激活成百上千次代理程序的运行。
```

 

- 四次挥手

所谓四次挥手（Four-Way Wavehand）即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发，整个流程如下图所示：

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815222034328-563037178.png)

由于TCP连接时全双工的，因此，每个方向都必须要单独进行关闭，这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接，收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭，上图描述的即是如此。
 （1）第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。
 （2）第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。
 （3）第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。
 （4）第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。

 为什么需要TIME_WAIT

TIMEWAIT状态也称为**2MSL等待状态**。

 1）为实现TCP这种全双工（full-duplex）连接的可靠释放

这样可让TCP再次发送最后的ACK以防这个**ACK丢失**(另一端超时并重发最后的FIN)。这种2MSL等待的另一个结果是这个TCP连接在2MSL等待期间，定义这个连接的插口(客户的IP地址和端口号，服务器的IP地址和端口号)不能再被使用。这个连接只能在2MSL结束后才能再被使用。

2）为使旧的数据包在网络因过期而消失

每个具体TCP实现必须选择一个报文段最大生存时间MSL(Maximum Segment Lifetime)。它是任何报文段被丢弃前在网络内的最长时间。

为什么建立连接是三次握手，而关闭连接却是四次挥手呢？

 这是因为服务端在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，我们也未必全部数据都发送给对方了，所以我们不可以立即close，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，我们的ACK和FIN一般都会分开发送。

------

##  **9.函数调用和系统调用的区别。**

什么是系统调用？（[常见Linux及其分类表](http://www.cnblogs.com/LUO77/p/5823241.html)）

所谓系统调用就是用户在程序中调用操作系统所提供的一个子功能，也就是系统API，系统调用可以被看做特殊的公共子程序。系统中的各种共享资源都由操作系统统一掌管，因此在用户程序中，凡是与资源有关的操作（如存储分配、进行I/O传输及管理文件等），都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。通常，一个操作系统提供的系统调用命令有几十个乃至上百个之多。这些系统调用按照功能大致可以分为以下几类：

- 设备管理：完成设备的请求或释放，以及设备启动等功能。
- 文件管理：完成文件的读、写、创建及删除等功能
- 进程控制：完成进程的创建、撤销、阻塞、及唤醒的功能
- 进程通信：完成进程之间的消息传递或信号的传递
- 内存管理：完成内存的分配、回收以及获取作业占用内存区大小及始址等功能。

显然，系统调用运行在系统的核心态。通过系统调用的方式来使用系统功能，可以保证系统的稳定性和安全性，防止用户随意更改或访问系统的数据或命令。系统调用命令式由操作系统提供的一个或多个子程序模块来实现的。

下图详细阐述了，Linux系统中系统调用的过程：（int 0x80中断向量是dos系统返回，int 3中断向量是断点指令——可以查中断向量表）

![点击查看源网页](https://images2015.cnblogs.com/blog/822287/201511/822287-20151126210248843-125792590.png)

 

库是可重用的模块，处于用户态。
系统调用是操作系统提供的服务，处于内核态，不能直接调用，而要使用类似int 0x80的软中断陷入内核，所以库函数中有很大部分是对系统调用的封装。

既然如此，如何调用系统调用？

用户是处于用户态，具有的权限是非常有限，肯定是不能直接使用内核态的服务，只能**间接通过有访问权限的API函数内嵌的系统调用函数来调用。**

介绍下系统调用的过程：
首先将API函数参数压到**栈**上，然后将函数内调用**系统调用的代码放入寄存器**，**通过陷入中断**，**进入内核将控制权交给操作系统**，**操作系统获得控制后**，**将系统调用代码拿出来**，**跟操作系统一直维护的一张系统调用表做比较**，**已找到该系统调用程序体的内存地址**，**接着访问该地址**，**执行系统调用**。执行完毕后，返回用户程序

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815224941484-1752275859.png)

例子：

```
int main()
{
    int fd = create("filename",0666);
    exit(0);
}
```

在执行main函数时，是在user mode下执行，当遇到create函数时，继续在user mode下执行，然后将filename和0666两个参数压入栈中寄存器，接着调用库函数create，系统仍然处于user mode。这里的库函数create实际上**调用了内核的系统调用create**，执行到这里后，系统将create系统调用的unique number压入寄存器，然后**执行指令trap使系统进入kernel mode(执行int $0x80产生中断)**。这时系统意识到要进行系统调用的invoke，于是从刚才的寄存器中取出create系统调用的unique number，从系统调用表中得知要invoke的系统调用是create，然后执行。执行完毕返回库函数create的调用，库函数负责检查系统调用的执行情况(检查某些寄存器的值)，然后库函数create根据检查的结果返回响应的值。

 

这里**trap指令类似于一个系统中断并且是软中断**，而系统调用create类似于一个中断处理函数所有的系统调用都与上边的情况类似，靠**中断机制切换到内核模式实现**。

系统调用通常比库函数要慢，因为要把上下文环境切换到内核模式。

 补充一下系统调用和库函数的区别：

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
系统调用：是操作系统为用户态运行的进程和硬件设备(如CPU、磁盘、打印机等)进行交互提供的一组接口，即就是设置在应用程序和硬件设备之间的一个接口层。可以说是操作系统留给用户程序的一个接口。再来说一下，linux内核是单内核，结构紧凑，执行速度快，各个模块之间是直接调用的关系。放眼望整个linux系统，从上到下依次是用户进程->linux内核->硬件。其中系统调用接口是位于Linux内核中的，如果再稍微细分一下的话，整个linux系统从上到下可以是：用户进程->系统调用接口->linux内核子系统->硬件，也就是说Linux内核包括了系统调用接口和内核子系统两部分；或者从下到上可以是：物理硬件->OS内核->OS服务->应用程序，其中操作系统起到“承上启下”的关键作用，向下管理物理硬件，向上为操作系服务和应用程序提供接口，这里的接口就是系统调用了。
       一般地，操作系统为了考虑实现的难度和管理的方便，它只提供一少部分的系统调用，这些系统调用一般都是由C和汇编混合编写实现的，其接口用C来定义，而具体的实现则是汇编，这样的好处就是执行效率高，而且，极大的方便了上层调用。

库函数：顾名思义是把函数放到库里。是把一些常用到的函数编完放到一个文件里，供别人用。别人用的时候把它所在的文件名用#include<>加到里面就可以了。一般是放到lib文件里的。一般是指编译器提供的可在c源程序中调用的函数。可分为两类，一类是c语言标准规定的库函数，一类是编译器特定的库函数。(由于版权原因，库函数的源代码一般是不可见的，但在头文件中你可以看到它对外的接口)
      libc中就是一个C标准库，里面存放一些基本函数，这些基本函数都是被标准化了的，而且这些函数通常都是用汇编直接实现的。
       库函数一般可以概括的分为两类，一类是随着操作系统提供的，另一类是由第三方提供的。随着系统提供的这些库函数把系统调用进行封装或者组合，可以实现更多的功能，这样的库函数能够实现一些对内核来说比较复杂的操作。比如，read()函数根据参数，直接就能读文件，而背后隐藏的比如文件在硬盘的哪个磁道，哪个扇区，加载到内存的哪个位置等等这些操作，程序员是不必关心的，这些操作里面自然也包含了系统调用。而对于第三方的库，它其实和系统库一样，只是它直接利用系统调用的可能性要小一些，而是利用系统提供的API接口来实现功能(API的接口是开放的)。部分Libc库中的函数的功能的实现还是借助了系统掉调用，比如printf的实现最终还是调用了write这样的系统调用；而另一些则不会使用系统调用，比如strlen, strcat, memcpy等。

实时上，系统调用所提供给用户的是直接而纯粹的高级服务，如果想要更人性化，具有更符合特定情况的功能，那么就要我们用户自己来定义，因此就衍生了库函数，它把部分系统调用包装起来，一方面把系统调用抽象了，一方面方便了用户级的调用。系统调用和库函数在执行的效果上很相似（当然库函数会更符合需求），但是系统调用是运行于内核状态；而库函数由用户调用，运行于用户态。

系统调用是为了方便使用操作系统的接口，而库函数则是为了人们编程的方便。
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

 

------

##  **10.线程和进程，线程可以共享进程里的哪些东西。 知道协程是什么吗**

**进程**，是并发执行的程序在执行过程中分配和管理资源的基本单位，每一个进程都有一个自己的地址空间，即进程空间或（虚空间）。进程空间的大小 只与处理机的位数有关，一个 16 位长处理机的进程空间大小为 216 ，而 32 位处理机的进程空间大小为 232 。进程至少有 5 种基本状态，它们是：初始态，执行态，等待状态，就绪状态，终止状态。

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815231836515-1655235486.png)

**线程**，在网络或多用户环境下，一个服务器通常需要接收大量且不确定数量用户的并发请求，为每一个请求都创建一个进程显然是行不通的，——无论是从系统资源开销方面或是响应用户请求的效率方面来看。因此，操作系统中线程的概念便被引进了。线程，是进程的一部分，一个没有线程的进程可以被看作是单线程的。线程有时又被称为轻权进程或轻量级进程，也是 CPU 调度的一个基本单位。

共享进程的地址空间，全局变量（数据和堆）。在一个进程中，各个线程共享堆区，而进程中的线程各自维持自己的栈。

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815231938828-201985819.png)

Each thread has its own:

- 栈区和栈指针（Stack area and stack pointer）
- 寄存器（Registers）
- 调度优先级Scheduling properties (such as policy or priority)
- 信号（阻塞和悬挂）Signals (pending and blocked signals)
- 普通变量Thread specific data ( automatic variables )

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
线程是指进程内的一个执行单元,也是进程内的可调度实体.
与进程的区别:
(1)地址空间:进程内的一个执行单元;进程至少有一个线程;它们共享进程的地址空间;而进程有自己独立的地址空间;
(2)资源拥有:进程是资源分配和拥有的单位,同一个进程内的线程共享进程的资源
(3)线程是处理器调度的基本单位,但进程不是.
4)二者均可并发执行.

进程和线程都是由操作系统所体会的程序运行的基本单元，系统利用该基本单元实现系统对应用的并发性。进程和线程的区别在于：

简而言之,一个程序至少有一个进程,一个进程至少有一个线程. 
线程的划分尺度小于进程，使得多线程程序的并发性高。 
另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。 
线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 
从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别。

进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动,进程是系统进行资源分配和调度的一个独立单位. 
线程是进程的一个实体,是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源. 
一个线程可以创建和撤销另一个线程;同一个进程中的多个线程之间可以并发执行.
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

协程：

定义：协程其实可以认为是比线程更小的执行单元。为啥说他是一个执行单元，因为他自带CPU上下文。

协程切换：协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。

　　　　　（我们在自己在进程里面完成逻辑流调度，碰着i\o我就用非阻塞式的。那么我们即可以利用到异步优势，又可以避免反复系统调用，还有进程切换造成的开销，分分钟给你上几千个　　　　逻辑流不费力。这就是协程。）　

```
协程的调度完全由用户控制，一个线程可以有多个协程，用户创建了几个线程，然后每个线程都是循环按照指定的任务清单顺序完成不同的任务，当任务被堵塞的时候执行下一个任务，当恢复的时候再回来执行这个任务，任务之间的切换只需要保存每个任务的上下文内容，就像直接操作栈一样的，这样就完全没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快；另外协程还需要保证是非堵塞的且没有相互依赖，协程基本上不能同步通讯，多采用一步的消息通讯，效率比较高。
```

多线程和多进程的优劣：

 

```
多线程还是多进程的争执由来已久，这种争执最常见到在B/S通讯中服务端并发技术的选型上，比如WEB服务器技术中，Apache是采用多进程的（perfork模式，每客户连接对应一个进程，每进程中只存在唯一一个执行线程），Java的Web容器Tomcat、Websphere等都是多线程的（每客户连接对应一个线程，所有线程都在一个进程中）。
```

 

 

 

多进程：fork

多线程：pthread_create

![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160815233254859-787365988.png)

 

------

##  **11.mysql的数据库引擎有哪些，他们的区别**

ISAM

　　ISAM是一个定义明确且历经时间考验的数据表格管理方法，它在设计之时就考虑到数据库被查询的次数要远大于更新的次数。因此，ISAM执行读取操作的速度很快，而且不占用大量的内存和存储资源。ISAM的两个主要不足之处在于，它不支持事务处理，也不能够容错：如果你的硬盘崩溃了，那么数据文件就无法恢复了。如果你正在把ISAM用在关键任务应用程序里，那就必须经常备份你所有的实时数据，通过其复制特性，MYSQL能够支持这样的备份应用程序。

MYISAM

　　MYISAM是MYSQL的ISAM扩展格式和缺省的数据库引擎。除了提供ISAM里所没有的索引和字段管理的大量功能，MYISAM还使用一种表格锁定的机制，来优化多个并发的读写操作。其代价是你需要经常运行OPTIMIZE TABLE命令，来恢复被更新机制所浪费的空间。MYISAM还有一些有用的扩展，例如用来修复数据库文件的MYISAMCHK工具和用来恢复浪费空间的MYISAMPACK工具。

  MYISAM强调了快速读取操作，这可能就是为什么MYSQL受到了WEB开发如此青睐的主要原因：在WEB开发中你所进行的大量数据操作都是读取操作。所以，大多数虚拟主机提供商和INTERNET平台提供商只允许使用MYISAM格式。

   HEAP

　　HEAP允许只驻留在内存里的临时表格。驻留在内存使得HEAP比ISAM和MYISAM的速度都快，但是它所管理的数据是不稳定的，而且如果在关机之前没有进行保存，那么所有的数据都会丢失。在数据行被删除的时候，HEAP也不会浪费大量的空间，HEAP表格在你需要使用SELECT表达式来选择和操控数据的时候非常有用。要记住，用完表格后要删除表格。 

  INNODB和BERKLEYDB

　　INNODB和BERKLEYDB（BDB）数据库引擎都是造就MYSQL灵活性的技术的直接产品，这项技术就是MySql++ API。在使用MySql的时候，你所面对的每一个挑战几乎都源于ISAM和MYIASM数据库引擎不支持事务处理也不支持外来键。尽管要比ISAM和MYISAM引擎慢很多，但是INNODB和BDB包括了对事务处理和外来键的支持，这两点都是前两个引擎所没有的。如前所述，如果你的设计需要这些特性中的一者或者两者，那你就要被迫使用后两个引擎中的一个了。

------

 

## **12.makefile吗，一个文件依赖库a，库a依赖库b，写makefile的时候，a要放在b的前面还是后面**

- Makefile概述：

什么是makefile？或许很多Winodws的程序员都不知道这个东西，因为那些Windows的IDE都为你做了这个工作，但我觉得要作一个好的和professional的程序员，makefile还是要懂。这就好像现在有这么多的HTML的编辑器，但如果你想成为一个专业人士，你还是要了解HTML的标识的含义。特别在Unix下的软件编译，你就不能不自己写makefile了，会不会写makefile，从一个侧面说明了一个人是否具备完成大型工程的能力。

因为，makefile关系到了整个工程的**编译规则**。一个工程中的源文件不计数，其按类型、功能、模块分别放在若干个目录中，**makefile定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，**因为makefile就像一个Shell脚本一样，其中也可以执行操作系统的命令。

makefile带来的好处就是——**“自动化编译”**，一旦写好，只需要一个make命令，整个工程完全自动编译，极大的提高了软件开发的效率。**make是一个命令工具，是一个解释makefile中指令的命令工具，**一般来说，大多数的IDE都有这个命令，比如：Delphi的make，Visual C++的nmake，Linux下GNU的make。可见，makefile都成为了一种在工程方面的编译方法。

现在讲述如何写makefile的文章比较少，这是我想写这篇文章的原因。当然，不同产商的make各不相同，也有不同的语法，但其本质都是在“文件依赖性”上做文章，这里，我仅对GNU的make进行讲述，我的环境是RedHat Linux 8.0，make的版本是3.80。必竟，这个make是应用最为广泛的，也是用得最多的。而且其还是最遵循于IEEE 1003.2-1992 标准的（POSIX.2）。

在这篇文档中，将以C/C++的源码作为我们基础，所以必然涉及一些关于C/C++的编译的知识，相关于这方面的内容，还请各位查看相关的编译器的文档。这里所默认的编译器是UNIX下的GCC和CC。

- 编译和连接：

编译：

定义：一般来说，无论是C、C++、还是pas，首先要把源文件编译成中间代码文件，在Windows下也就是 .obj 文件，UNIX下是 .o 文件，即 Object File，这个动作叫做编译（compile）。

描述：编译时，编译器需要的是语法的正确，函数与变量的声明的正确。只要所有的语法正确，编译器就可以编译出中间目标文件。一般来说，每个源文件都应该对应于一个中间目标文件（O文件或是OBJ文件）。

连接：

定义：然后再把大量的Object File合成执行文件，这个动作叫作链接（link）。

描述：通常是你需要告诉编译器头文件的所在位置（头文件中应该只是声明，而定义应该放在C/C++文件中），链接时，主要是链接函数和全局变量，所以，我们可以使用这些中间目标文件（O文件或是OBJ文件）来链接我们的应用程序。链接器并不管函数所在的源文件，只管函数的中间目标文件（Object File），在大多数时候，由于源文件太多，编译生成的中间目标文件太多，而在链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，我们要给中间目标文件打个包，在Windows下这种包叫“库文件”（Library File)，也就是 .lib 文件，在UNIX下，是Archive File，也就是 .a 文件。

总结一下，源文件首先会生成中间目标文件，再由中间目标文件生成执行文件。在编译时，编译器只检测程序语法，和函数、变量是否被声明。如果函数未被声明，编译器会给出一个警告，但可以生成Object File。而在链接程序时，链接器会在所有的Object File中找寻函数的实现，如果找不到，那到就会报链接错误码（Linker Error），在VC下，这种错误一般是：Link 2001错误，意思说是说，链接器未能找到函数的实现。你需要指定函数的Object File.

-  Makefile

make命令执行时，需要一个 Makefile 文件，以告诉make命令需要怎么样的去编译和链接程序。

首先，我们用一个示例来说明Makefile的书写规则。我们的规则是：
1）如果这个工程没有编译过，那么我们的所有C文件都要编译并被链接。
2）如果这个工程的某几个C文件被修改，那么我们只编译被修改的C文件，并链接目标程序。
3）如果这个工程的头文件被改变了，那么我们需要编译引用了这几个头文件的C文件，并链接目标程序。

只要我们的Makefile写得够好，所有的这一切，我们只用一个make命令就可以完成，make命令会自动智能地根据当前的文件修改的情况来确定哪些文件需要重编译，从而自己编译所需要的文件和链接目标程序。

 

```
　　Makefile的规则：

　　target…：dependecies…

　　　　command
```

target也就是一个目标文件，可以是Object File，也可以是执行文件。还可以是一个标签（Label），对于标签这种特性，在后续的“伪目标”章节中会有叙述。
dependicies就是，要生成那个target所需要的文件或是目标。
command也就是make需要执行的命令。（任意的Shell命令）
这是一个文件的依赖关系，也就是说，target这一个或多个的目标文件依赖于dependicies中的文件，其生成规则定义在command中。说白一点就是说，dependicies中如果有一个以上的文件比target文件要新的话，command所定义的命令就会被执行。这就是Makefile的规则。也就是Makefile中最核心的内容。（[深入探讨makefile](http://blog.csdn.net/liang13664759/article/details/1771246)）

　　注意事项：

　　1.命令要以[Tab]为开始

　　2.有clean

　　![img](https://images2015.cnblogs.com/blog/364303/201608/364303-20160824100605995-1299551285.png)

原文作者：[0giant](https://home.cnblogs.com/u/LUO77/)

原文链接：https://www.cnblogs.com/LUO77/p/5771237.html

# 【NO.142】百度面试题（C++方向）

# 1.在函数内定义一个字符数组，用gets函数输入字符串的时候，如果输入越界，为什么程序会崩溃？

答：因为gets无法截断数组越界部分，会将所有输入都写入内存，这样越界部分就可能覆盖其他内容，造成程序崩溃。

 

# 2.C++中引用与指针的区别

答：联系：引用是变量的别名，可以将引用看做操作受限的指针；

区别：

1） 指针是一个实体，而引用仅是个别名；

2）引用只能在定义时必须初始化，指针可以不初始化为空；

3）引用初始化之后其地址就不可改变（即始终作该变量的别名直至销毁，即从一而终。注意：并不表示引用的值不可变，因为只要所指向的变量值改变。引用的值也就改变了），但指针所指地址是不可变的；如下：

int m=23,n=13;

int& a=m;

a=12; //合法，相当于修改m=12

a=n;//合法，相当于修改m=13

# 3.C/C++程序的内存分区

答：其实C和C++的内存分区还是有一定区别的，但此处不作区分：

1）、栈区（stack）— 由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其
操作方式类似于数据结构中的栈。
2）、堆区（heap） — 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回
收 。注意它与数据结构中的堆是两回事，分配方式倒是类似于链表。
3）、全局区（静态区）（static）—，全局变量和静态变量的存储是放在一块的，初始化的
全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另
一块区域。 - 程序结束后由系统释放。
4）、文字常量区 —常量字符串就是放在这里的。 程序结束后由系统释放
5）、程序代码区—存放函数体的二进制代码。

栈区与堆区的区别：

1）堆和栈中的存储内容：栈存局部变量、函数参数等。堆存储使用new、malloc申请的变量等；

2）申请方式：栈内存由系统分配，堆内存由自己申请；

3）申请后系统的响应：栈——只要栈的剩余空间大于所申请空间，系统将为程序提供内存，否则将报异常提示栈溢出。
堆——首先应该知道操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，会遍历该链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表 中删除，并将该结点的空间分配给程序；

4）申请大小的限制：Windows下栈的大小一般是2M，堆的容量较大；

5）申请效率的比较：栈由系统自动分配，速度较快。堆使用new、malloc等分配，较慢；

总结：栈区优势在处理效率，堆区优势在于灵活；

内存模型：自由区、静态区、动态区；

根据c/c++对象生命周期不同，c/c++的内存模型有三种不同的内存区域，即：自由存储区，动态区、静态区。

自由存储区：局部非静态变量的存储区域，即平常所说的栈；

动态区： 用new ，malloc分配的内存，即平常所说的堆；

静态区：全局变量，静态变量，字符串常量存在的位置；

注：代码虽然占内存，但不属于c/c++内存模型的一部分；

 

# 4.快速排序的思想、时间复杂度、实现以及优化方法

答：快速排序的三个步骤：
(1)选择基准：在待排序列中，按照某种方式挑出一个元素，作为 "基准"（pivot）；
(2)分割操作：以该基准在序列中的实际位置，把序列分成两个子序列。此时，在基准左边的元素都比该基准小，在基准右边的元素都比基准大；
(3)递归地对两个序列进行快速排序，直到序列为空或者只有一个元素。

基准的选择：

对于分治算法，当每次划分时，算法若都能分成两个等长的子序列时，那么分治算法效率会达到最大。

即：同一数组，时间复杂度最小的是每次选取的基准都可以将序列分为两个等长的；时间复杂度最大的是每次选择的基准都是当前序列的最大或最小元素；

快排代码实现：

我们一般选择序列的第一个作为基数，那么快排代码如下：

void quicksort(vector<int> &v,int left, int right)
{  
	if(left < right)//false则递归结束
	{    
		int key=v[left];//基数赋值
		int low = left;                
		int high = right;   
		while(low < high)	//当low=high时，表示一轮分割结束
		{                        
			while(low < high && v[high] >= key)//v[low]为基数，从后向前与基数比较
			{                                
				high--;                        
			}
			swap(v[low],v[high]);

			while(low < high && v[low] <= key)//v[high]为基数，从前向后与基数比较
			{                                
				low++;                        
			}      
			swap(v[low],v[high]);
		}                 
		//分割后，对每一分段重复上述操作
		quicksort(v,left,low-1);               
		quicksort(v,low+1,right);
	}

}


注：上述数组或序列v必须是引用类型的形参，因为后续快排结果需要直接反映在原序列中；

优化：

上述快排的基数是序列的第一个元素，这样的对于有序序列，快排时间复杂度会达到最差的o(n^2)。所以，优化方向就是合理的选择基数。

常见的做法“三数取中”法（序列太短还要结合其他排序法，如插入排序、选择排序等），如下：

①当序列区间长度小于 7 时，采用插入排序；
②当序列区间长度小于 40 时，将区间分成2段，得到左端点、右端点和中点，我们对这三个点取中数作为基数；
③当序列区间大于等于 40 时，将区间分成 8 段，得到左三点、中三点和右三点，分别再得到左三点中的中数、中三点中的中数和右三点中的中数，再将得到的三个中数取中数，然后将该值作为基数。
具体代码只是在上一份的代码中将“基数赋值”改为①②③对应的代码即可：

		int key=v[left];//基数赋值
		if(right-left+1<=7){
			insertion_sort(v,left,right);//插入排序
			return;
		}else if(right-left+1<=8){
			key=SelectPivotOfThree(v,left,right);//三个取中
		}else{
			//三组三个取中，再三个取中（使用4次SelectPivotOfThree，此处不具体展示）
		}

需要调用的函数：

void insertion_sort(vector<int> &unsorted,int left, int right)  //插入排序算法      
{        
	for (int i = left+1; i <= right; i++)        
	{        
		if (unsorted[i - 1] > unsorted[i])        
		{       
			int temp = unsorted[i];       
			int j = i;       
			while (j > left && unsorted[j - 1] > temp)
         

			{         
				unsorted[j] = unsorted[j - 1];        
				j--;      	
			}   
			unsorted[j] = temp;    
		}    
	}   

}

int SelectPivotOfThree(vector<int> &arr,int low,int high)  //三数取中，同时将中值移到序列第一位
{  
    int mid = low + (high - low)/2;//计算数组中间的元素的下标  

    //使用三数取中法选择枢轴
    if (arr[mid] > arr[high])//目标: arr[mid] <= arr[high]  
    {  
        swap(arr[mid],arr[high]);
    }  
    if (arr[low] > arr[high])//目标: arr[low] <= arr[high]  
    {  
        swap(arr[low],arr[high]);
    }  
    if (arr[mid] > arr[low]) //目标: arr[low] >= arr[mid]  
    {  
        swap(arr[mid],arr[low]);
    }  
    //此时，arr[mid] <= arr[low] <= arr[high]  
    return arr[low];  
    //low的位置上保存这三个位置中间的值  
    //分割时可以直接使用low位置的元素作为枢轴，而不用改变分割函数了  

}  


这里需要注意的有两点：

 

①插入排序算法实现代码；

②三数取中函数不仅仅要实现取中，还要将中值移到最低位，从而保证原分割函数依然可用。

 

# 5.IO模型——IO多路复用机制

答：预备知识介绍：

IO模型有4中：同步阻塞IO、同步非阻塞IO、异步阻塞IO、异步非阻塞IO；IO多路复用属于IO模型中的异步阻塞IO模型，在服务器高性能IO构建中常常用到。

上述几个模型原理如下图：

同步阻塞IO：                                                           同步非阻塞IO：                                            IO多路复用（异步阻塞IO）：



如上：同步异步是表示服务端的，阻塞非阻塞是表示用户端，所以可解释为什么IO多路复用（异步阻塞）常用于服务器端的原因；

文件描述符（FD，又叫文件句柄）：描述符就是一个数字，它指向内核中的一个结构体(文件路径，数据区等属性)。具体来源：Linux内核将所有外部设备都看作一个文件来操作，对文件的操作都会调用内核提供的系统命令，返回一个fd(文件描述符)。

 

下面开始介绍IO多路复用：

（1）I/O多路复用技术通过把多个I/O的阻塞复用到同一个select、poll或epoll的阻塞上，从而使得系统在单线程的情况下可以同时处理多个客户端请求。与传统的多线程/多进程模型比，I/O多路复用的最大优势是系统开销小，系统不需要创建新的额外进程或者线程。

（2）select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。

（3）I/O多路复用的主要应用场景如下：

服务器需要同时处理多个处于监听状态或者多个连接状态的套接字；
服务器需要同时处理多种网络协议的套接字；

（4）目前支持I/O多路复用的系统调用有 select，poll，epoll，epoll与select的原理比较类似，但epoll作了很多重大改进，现总结如下：

①支持一个进程打开的文件句柄FD个数不受限制（为什么select的句柄数量受限制：select使用位域的方式来传递关心的文件描述符，因为位域就有最大长度，在Linux下是1024，所以有数量限制）；

②I/O效率不会随着FD数目的增加而线性下降；

③epoll的API更加简单；

（5）三种接口调用介绍：

①select函数调用格式：

#include <sys/select.h>
#include <sys/time.h>
int select(int maxfdp1,fd_set *readset,fd_set *writeset,fd_set *exceptset,const struct timeval *timeout)
//返回值：就绪描述符的数目，超时返回0，出错返回-1
②poll函数调用格式：

include <poll.h>

int poll ( struct pollfd * fds, unsigned int nfds, int timeout);
③epoll函数格式（操作过程包括三个函数）：

#include <sys/epoll.h>
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
（6）作用：一定程度上替代多线程/多进程，减少资源占用，保证系统运行的高效率；

 

更多细节待续……

 

# 6.常用的Linux命令

答：（1）查看CPU利用率：top

（2）查看当前目录：pwd和ls（ls -a可以查看隐藏目录）

（3）切换目录：cd

（4）查看文件占用磁盘大小：du和df

（5）创建文件夹：mkdir

（6）新建文件：touch

（7）查看文件：cat

（8）拷贝：cp  移动：mv  删除：rm

（9）查看进程：ps，如ps aux

（10）删除进程：kill -9 PID，注-9是参数

（11）程序运行时间：time，使用时在命令前添加time即可，如：time ./test，可得到三个时间：real 0m0.020s，user 0m0.000s，sys 0m0.018s

grep命令（重要的常用命令之一）：常用于打开文本修改保存，类似打windows开开TXT文本并修改；

sed命令（常用重要命令之一）：主要用于对文件的增删改查；

awk命令（重要常用命令之一）：取列是其擅长的；

find 命令（常与xargs命令配合）：查找 -type 文件类型-name 按名称查找-exec执行命令；

xargs命令：配合find/ls查找，将查找结果一条条的交给后续命令处理；

gdb调试工具：

要调试C/C++的程序，一般有如下几个步骤：

①首先在编译时，我们必须要把调试信息加到可执行文件中，编译生成可执行文件-------> g++  -g hello.cpp -o hello；

②启动GDB编译hello程序----------> gdb hello;

③显示源码------------> l;

④开始调试：break 16——设置断点在16行，break func——设置断点在函数func()入口处，info break——查看断点信息，n——单步运行，c——继续运行程序，r——运行程序；p i——打印i的值，finish——退出程序，q——退出gdb。

 

# 7.C中变量的存储类型有哪些？


答：c语言中的存储类型有auto, extern, register, static 四种；

 

# 8.动态规划的本质

答：动归，本质上是一种划分子问题的算法，站在任何一个子问题的处理上看，当前子问题的提出都要依据现有的类似结论，而当前问题的结论是后面问题求解的铺垫。任何DP都是基于存储的算法，核心是状态转移方程。 

 

# 9.实践中如何优化MySQL

答：四条从效果上第一条影响最大，后面越来越小。 

① SQL语句及索引的优化
② 数据库表结构的优化
③ 系统配置的优化
④ 硬件的优化 

 

# 10.什么情况下设置了索引但无法使用

答：① LIKE语句，模糊匹配
② OR语句
③ 数据类型出现隐式转化（如varchar不加单引号的话可能会自动转换为int型） 

 

# 11.SQL语句的优化

答：alter尽量将多次合并为一次；

insert和delete也需要合并；

尽量使用union而不是or；

 

# 12..数据库索引的底层实现原理和优化

答：B树，经过优化的B+树。主要是在所有的叶子结点中增加了指向下一个叶子节点的指针，因此InnoDB建议为大部分表使用默认自增的主键作为主索引。

 

# 13.HTTP和HTTPS的主要区别

答：见另一文章解析：http://blog.csdn.net/xiongchao99/article/details/73381280#t8

 

# 14.如何设计一个高并发的系统

答：① 数据库的优化，包括合理的事务隔离级别、SQL语句优化、索引的优化；

② 使用缓存，尽量减少数据库 IO；

③ 分布式数据库、分布式缓存；

④ 服务器的负载均衡；

 

# 15.两条相交的单向链表，如何求他们的第一个公共节点

答：思想：

①如果两个链表相交，则从相交点开始，后面的节点都相同，即最后一个节点肯定相同；
②从头到尾遍历两个链表，并记录链表长度，当二者的尾节点不同，则二者肯定不相交；
③尾节点相同，如果A长为LA，B为LB，如果LA>LB,则A前LA-LB个先跳过；

——更多如链表相关经典问题：求单向局部循环链表的入、将两个有序链表合并合成一个有序链表、链表逆序、求倒数第K个节点，判断是否有环等。

 

# 16.求单向局部循环链表的环入口

答：思路：

假如有快慢指针判断一个链表有局部环，链表起点是A，环的入口是B，快慢指针在环中的相遇点是C。那么按照原来的运动方向，有AB=CB，这是可以证明的结论。具体如下图说明：



 

# 17.IP地址如何在数据库中存储

答：常有以下几种存储方式：



说明一下：int类型的num存储在解码时是这样做的：

65=num%256；num=num/256;

120=num%256；num=num/256;

……

 

# 18.new/delete和malloc/free的底层实现

答：malloc和new的区别：

1）malloc与free是C++/C语言的标准库函数，new/delete是C++的运算符。它们都可用于申请动态内存和释放内存；

2）new 返回指定类型的指针，并且可以自动计算所需要大小。而 malloc 则必须要由程序员计算字节数，并且在返回后强行转换为实际类型的指针；

3）new/delete在对象创建的同时可以自动执行构造函数初始化，在对象在消亡之前会自动执行析构函数。而malloc 只管分配内存，并不能对所得的内存进行初始化，所以得到的一片新内存中，其值将是随机的；

既然new/delete的功能覆盖了malloc/free，为什么C++还要保留malloc/free？因为C++程序经常要调用C函数，而C程序只能用malloc/free管理动态内存。

new/delete、malloc/free底层实现原理：

概述：new/delete的底层实现是调用malloc/free函数实现的，而malloc/free的底层实现也不是直接操作内存而是调用系统API实现的。

new/delete的两种分配方式原理图如下：



注意，针对上图最末尾所述的“new[]/delete[]时会多开辟4字节用于存储对象个数”，作如下说明：
①对于内置类型：
new []不会在首地址前4个字节定义数组长度。
delete 和 delete[]是一样的执行效果，都会删除整个数组，要删除的长度从new时即可知道。
②对于自定义类型：
new []会在首地址前4个字节定义数组长度。
当delete[]时，会根据前4个字节所定义的长度来执行析构函数删除整个数组。
如果只是delete数组首地址，只会删除第一个对象的值。 

 

# 19.overload、override、overwrite的介绍

答：（1）overload（重载），即函数重载：
①在同一个类中；
②函数名字相同；
③函数参数不同（类型不同、数量不同，两者满足其一即可）；
④不以返回值类型不同作为函数重载的条件。
（2）override（覆盖，子类改写父类的虚函数），用于实现C++中多态：
①分别位于父类和子类中；
②子类改写父类中的virtual方法；
③与父类中的函数原型相同。
（3）overwrite（重写或叫隐藏，子类改写父类的非虚函数，从而屏蔽父类函数）：
①与overload类似，但是范围不同，是子类改写父类；
②与override类似，但是父类中的方法不是虚函数。

 

# 20.小端/大端机器

答：小端/大端的区别是指低位数据存储在内存低位还是高位的区别。其中小端机器指：数据低位存储在内存地址低位，高位数据则在内存地址高位；大端机器正好相反。

当前绝大部分机器都是小端机器，就是比较符合人们逻辑思维的数据存储方式，比如intel的机器基本就都是小端机器。

 

# 21.守护进程

答：（1）什么是守护进程？
守护进程（Daemon Process），也就是通常说的 Daemon 进程（精灵进程），是 Linux 中的后台服务进程。它是一个生存期较长的进程，通常独立于

控制终端并且周期性地执行某种任务或等待处理某些发生的事件。

守护进程是个特殊的孤儿进程，这种进程脱离终端，为什么要脱离终端呢？之所以脱离于终端是为了避免进程被任何终端所产生的信息所打断，其在执

行过程中的信息也不在任何终端上显示。

（2）如何查看守护进程？

在终端敲：ps axj



从上图可以看出守护进行的一些特点：
守护进程基本上都是以超级用户启动（ UID 为 0 ）
没有控制终端（ TTY 为 ？）
终端进程组 ID 为 -1 （ TPGID 表示终端进程组 ID）

更多守护进程相关参考：http://blog.csdn.net/lianghe_work/article/details/47659889

 

# 22.多线程

答：Java提供了3中多线程实现：Thread类、runable接口、使用ExecutorService、Callable、Future实现有返回结果的多线程。

而C++本身并没有提高多线程编程功能的库或接口，但Windows系统下的C++多线程编程还是可以通过<windows.h>库中的相关多线程接口实现，具体见：http://blog.csdn.net/xiongchao99/article/details/64441017#t107；

Linux写的C++多线程可以用头文件pthread.h，常用到其中两个函数pthread_create和pthread_join。下面是一个Linux下的简单C++多线程程序：

`//Threads.cpp`
`#include <iostream>`
`#include <unistd.h>`
`#include <pthread.h>`
`using namespace std;`

`void *thread(void *ptr)`
`{`
    `for(int i = 0;i < 3;i++) {`
        `sleep(1);`
        `cout << "This is a pthread." << endl;`
    `}`
    `return 0;`
`}`

`int main() {`
    `pthread_t id;`
    `int ret = pthread_create(&id, NULL, thread, NULL);//创建线程`
    `if(ret) {`
        `cout << "Create pthread error!" << endl;`
        `return 1;`
    `}`
    `for(int i = 0;i < 3;i++) {`
        `cout <<  "This is the main process." << endl;`
        `sleep(1);`
    `}`
    `pthread_join(id, NULL);//等待线程结束`
    `return 0;`
`}`

`` 

 

 

多线程相关的同步的知识不再累述，见http://blog.csdn.net/xiongchao99/article/details/74858900，此处来说说多线程优缺点：

多线程的主要优点包括: 

(1)多线程技术使程序的响应速度更快 ,因为用户界面可以在进行其它工作的同时一直处于活动状态；

(2)占用大量处理时间的任务使用多线程可以提高CPU利用率，即占用大量处理时间的任务可以定期将处理器时间让给其它任务；

(3)多线程可以分别设置优先级以优化性能。

以下是最适合采用多线程处理：

(1)耗时或大量占用处理器的任务阻塞用户界面操作;

(2)各个任务必须等待外部资源 (如远程文件或 Internet连接)。

多线程的主要缺点包括：

(1)等候使用共享资源时造成程序的运行速度变慢。这些共享资源主要是独占性的资源 ,如打印机等。

(2)对线程进行管理要求额外的 CPU开销，线程的使用会给系统带来上下文切换的额外负担。

(3)线程的死锁。即对共享资源加锁实现同步的过程中可能会死锁。

(4)对公有变量的同时读或写，可能对造成脏读等；

 

# 23.长连接与短连接

答：（1）就是TCP长连接和TCP短连接：

①TCP长连接：TCP长连接指建立连接后保持连接而不断开。若一段时间内没有数据传输，服务器会发送心跳包给客户端，判断客户端是否还在线，叫做TCP长连接中的keep alive。一般步骤：连接→数据传输→保持连接(心跳)→数据传输→保持连接(心跳)→……→关闭连接；

②TCP短连接：指连接建立并传输数据完成后，就断开连接。一般步骤：连接→数据传输→关闭连接；

③使用场景：长连接适合单对单通信且连接数不太多的情况；短连接适合连接数多且经常更换连接对象的；

（2）HTTP是什么连接：

①在HTTP/1.0中，默认使用的是短连接。但从 HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头有加入这行代码：

Connection:keep-alive
注意：此处的keep-alive和上述TCP长连接原理介绍中的keep alive不是一个意思：此处表示告知服务器本http请求是长连接模式，而TCP长连接中的keep alive表示对客户端的保活检测。

 

②http长连接并不是一直保持连接

http的长连接也不会是永久保持连接，它有一个保持时间如20s（从上一次数据传输完成开始计时），可以在不同的服务器软件（如Apache）中设定这个时间，若超过该时间限制仍然无数据通信传输，服务器就主动关闭该连接。注：实现长连接要客户端和服务端都支持长连接。

③http连接实质：http的长连接/短连接实质上就是TCP的长/短连接。

 

24、二分图应用于最佳匹配问题（游客对房间的满意度之和最大问题）
答：题目：有n个游客和n个客房，每个游客对每间房有一个满意度，现要求做出一个入住安排，使得所有游客的满意度最大。

思路：用二分图解决，游客作为一边的顶点，客房作为另一边的顶点，取出所有最大匹配中满意度之和最大的方案。

实现：涉及匈牙利算法；
————————————————
版权声明：本文为CSDN博主「空山明月_Blog」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/Xiongchao99/article/details/74524807

# 【NO.143】C++面试集锦( 面试被问到的问题）

# **1. C 和 C++ 区别**

  **
**

# **2. const 有什么用途**

  主要有三点：

   1：定义只读变量，即常量 

   2：修饰函数的参数和函数的返回值 

   3： 修饰函数的定义体，这里的函数为类的成员函数，被const修饰的成员函数代表不修改成员变量的值**
**

 

# **3. 指针和引用的区别**

  1：引用是变量的一个别名，内部实现是只读指针

  2：引用只能在初始化时被赋值，其他时候值不能被改变，指针的值可以在任何时候被改变

  3：引用不能为NULL，指针可以为NULL

  4：引用变量内存单元保存的是被引用变量的地址

  5：“sizeof 引用" = 指向变量的大小 ， "sizeof 指针"= 指针本身的大小

  6：引用可以取地址操作，返回的是被引用变量本身所在的内存单元地址

  7：引用使用在源代码级相当于普通的变量一样使用，做函数参数时，内部传递的实际是变量地址

 

# **4. C++中有了malloc / free , 为什么还需要 new / delete**   

```
  1,malloc与free是C++/C语言的标准库函数，new/delete是C++的运算符。它们都可用于申请动态内存和释放内存。
  2,对于非内部数据类型的对象而言，光用maloc/free无法满足动态对象的要求。
     对象在创建的同时要自动执行构造函数，对象在消亡之前要自动执行析构函数。
     由于malloc/free是库函数而不是运算符，不在编译器控制权限之内，不能够把执行构造函数和析构函数的任务强加于malloc/free。
  3,因此C++语言需要一个能完成动态内存分配和初始化工作的运算符new，以一个能完成清理与释放内存工作的运算符delete。注意new/delete不是库函数。
```

 

 

# **5. 编写类String 的构造函数，析构函数，拷贝构造函数和赋值函数**

 

# **6. 多态的实现**

# **7. 单链表的逆置**

 

# **8. 堆和栈的区别**  

```
  一个由c/C++编译的程序占用的内存分为以下几个部分 
  1、栈区（stack）―   由编译器自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。 
  2、堆区（heap） ―   一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收 。
     注意它与数据结构中的堆是两回事，分配方式倒是类似于链表，呵呵。 
  3、全局区（静态区）（static）―，全局变量和静态变量的存储是放在一块的，
     初始化的全局变量和静态变量在一块区域， 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。 - 程序结束后有系统释放 
  4、文字常量区  ―常量字符串就是放在这里的。 程序结束后由系统释放 
  5、程序代码区―存放函数体的二进制代码。
```

 

 

# 10. 不调用C/C++ 的字符串库函数，编写strcpy

**

```
   char * strcpy(char * strDest,const char * strSrc)
        {
                if ((strDest==NULL)||strSrc==NULL))                     
                   return NULL;    
                char * strDestCopy=strDest; 
                while ((*strDest++=*strSrc++)!='\0'); 
                *strDest = '\0';
                return strDestCopy;
        }
```

 

 

# 11. 关键字static的作用

**

  \1. 函数体内 static 变量的作用范围为该函数体，不同于 auto 变量， 该变量的内存只被分配一次，因此其值在下次调用时仍维持上次的值

  \2. 在模块内的 static 全局变量可以被模块内所有函数访问，但不能被模块外其他函数访问

  \3. 在模块内的static 函数只可被这一模块内的其他函数调用，这个函数的使用范围被限制在声明它的模块内

  \4. 在类的static 成员变量属于整个类所拥有，对类的所以对象只有一份拷贝

  \5. 在类中的 static 成员函数属于整个类所拥有，这个函数不接收 this 指针，因而只能访问类的 static 成员变量

  

   介绍它最重要的一条：隐藏。（static函数，static变量均可） --> 对应上面的2、3项
    当同时编译多个文件时，所有未加static前缀的全局变量和函数都具有全局可见性。
    举例来说明。同时编译两个源文件，一个是a.c，另一个是main.c。

```
   //a.c
    char a = 'A';               // global variable
    void msg()
    {
      printf("Hello\n");
    }
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

```
  //main.c
   int main()
   {
     extern char a;       // extern variable must be declared before use
     printf("%c ", a);
     (void)msg();
     return 0;
   }
```

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)

​    程序的运行结果是：

   A Hello

 

 

   为什么在a.c中定义的全局变量a和函数msg能在main.c中使用？

   前面说过，所有未加static前缀的全局变量和函数都具有全局可见性，其它的源文件也能访问。此例中，a是全局变量，msg是函数，并且都没有加static前缀，

​    因此对于另外的源文件main.c是可见的。

   如果加了static，就会对其它源文件隐藏。例如在a和msg的定义前加上static，main.c就看不到它们了。

   利用这一特性可以在不同的文件中定义同名函数和同名变量，而不必担心命名冲突。static可以用作函数和变量的前缀，对于函数来讲，static的作用仅限于隐藏

 

# **12. 在c++程序中调用被C编译器编译后的函数，为什么要加extern“C”**

   C++语言支持函数重载，[C语言](http://lib.csdn.net/base/c)不支持函数重载，函数被C++编译器编译后在库中的名字与C语言的不同，

   假设某个函数原型为：

1. ​     void foo(int x, inty);

  该函数被C编译器编译后在库中的名字为: _foo

  而C++编译器则会产生像: _foo_int_int  之类的名字。
  为了解决此类名字匹配的问题，C++提供了C链接交换指定符号 extern "C"。

 

 

# **13. 头文件种的ifndef/define/endif 是干什么用的**

   防止头文件被重复包含

 

# **14. 线程和进程的联系和区别**

   **http://blog.csdn[.NET](http://lib.csdn.net/base/dotnet)/wolenski/article/details/7969908**

 

# **15. 线程有哪几种状态**

   **http://blog.csdn[.Net](http://lib.csdn.net/base/dotnet)/wolenski/article/details/7969908**

 

# **16. 进程间的通信方式**

   管道、有名管道、信号、共享内存、消息队列、信号量、套接字、文件.

 

# **17. 线程同步和线程互斥的区别**

  **http://blog.csdn.net/wolenski/article/details/7969908**

 

# **18. 线程同步的方式**

   **[Linux](http://lib.csdn.net/base/linux):**  互斥锁、条件变量和信号量

   **http://blog.csdn.net/zsf8701/article/details/7844316

**

 

# **19. 网络七层**

  **
**

# **20. TCP和UDP有什么区别**

   TCP---传输控制协议,提供的是面向连接、可靠的字节流服务。

​         当客户和服务器彼此交换数据前，必须先在双方之间建立一个TCP连接，之后才能传输数据。

​         TCP提供超时重发，丢弃重复数据，检验数据，流量控制等功能，保证数据能从一端传到另一端。

   UDP---用户数据报协议，是一个简单的面向数据报的运输层协议。

​         UDP不提供可靠性，它只是把应用程序传给IP层的数据报发送出去，但是并不能保证它们能到达目的地。

​         由于UDP在传输数据报前不用在客户和服务器之间建立一个连接，且没有超时重发等机制，故而传输速度很快**
**

 

# **21. 编写socket套接字的步骤**

 

# **22. TCP三次握手和四次挥手, 以及各个状态的作用**

   **http://hi.baidu.com/suxinpingtao51/item/be5f71b3a907dbef4ec7fd0e?qq-pf-to=pcqq.c2c**

 

# **23. HTTP协议**

​      http（超文本传输协议）是一个基于请求与响应模式的、无状态的、应用层的协议，常基于TCP的连接方式，

   HTTP1.1版本中给出一种持续连接的机制，绝大多数的Web开发，都是构建在HTTP协议之上的Web应用。

  TCP 和 HTTP区别： http://blog.csdn.net/lemonxuexue/article/details/4485877

 

# **24. 使用过的 shell 命令**

​    **cp , mv , rm , mkdir , touch , pwd , cd , ls , top , cat , tail , less , df , du , man , find , kill , sudo , cat 

**

 

# **25. 使用过的 vim 命令**

​    wq!, dd , dw , yy , p , i , %s/old/new/g , /abc 向后搜索字符串abc ， ？abc向前搜索字符串abc**
**

 

# **26. 使用过的 gdb 命令**

   **http://blog.csdn.net/dadalan/article/details/3758025**

 

# **27. 常见[算法](http://lib.csdn.net/base/datastructure)**

​    快速排序、堆排序和归并排序

​    **堆排序 ： http://blog.csdn.net/xiaoxiaoxuewen/article/details/7570621**

​    **快速排序、归并排序： http://blog.csdn.net/morewindows/article/details/6684558

**

​    稳定性分析 **http://baike.baidu.com/link?url=ueoZ3sNIOvMNPrdCKbd8mhfebC85B4nRc-7hPEJWi-hFo5ROyWH2Pxs9RtvLFRJL

**

 

# **28. C库函数实现**

 

# **29. 静态链表和动态链表的区别**

   http://blog.csdn.net/toonny1985/article/details/4868786

 

# **31. 大并发( epoll )**

​    **优点:**

​       **http://blog.csdn.net/sunyurun/article/details/8194979

**

​    **实例：**

​       **http://www.cnblogs.com/ggjucheng/archive/2012/01/17/2324974.html**

​    **
**

# **32. 海量数据处理的知识点，（hash表， hash统计）**

  hash表： http://hi.baidu.com/05104106/item/62736054402852c09e26679b

  海量数据处理方法： http://blog.csdn.net/v_july_v/article/details/7382693

 

 

# **33. 什么时候要用虚析构函数**

​    通过基类的指针来删除派生类的对象时，基类的析构函数应该是虚的。否则其删除效果将无法实现。

​    一般情况下，这样的删除只能够删除基类对象，而不能删除子类对象，形成了删除一半形象，从而千万内存泄漏。

   原因：

​       在公有继承中，基类对派生类及其对象的操作，只能影响到那些从基类继承下来的成员。

​       如果想要用基类对非继承成员进行操作，则要把基类的这个操作（函数）定义为虚函数。
​       那么，析构函数自然也应该如此：如果它想析构子类中的重新定义或新的成员及对象，当然也应该声明为虚的。

   注意：

   如果不需要基类对派生类及对象进行操作，则不能定义虚函数（包括虚析构函数），因为这样会增加内存开销。

 

# **34. c++怎样让返回对象的函数不调用拷贝构造函数**

  拷贝构造函数前加 “explicit” 关键字

# **35. 孤儿进程和僵尸进程**

  http://www.cnblogs.com/Anker/p/3271773.html

原文作者：Y1
原文链接：https://www.cnblogs.com/Y1Focus/p/6707121.html

# 【NO.144】c/c++常见面试题精选

面试题有难有易，不能因为容易，我们就轻视，更不能因为难，我们就放弃。我们面对高薪就业的态度永远不变，那就是坚持、坚持、再坚持。出现问题，找原因；遇到困难，想办法。我们一直坚信只有在坚持中才能看到希望，而不是看到希望才去坚持。

人生没有如果，只有结果和后果。既然选择了，就不后悔。年轻就是资本，年轻就要吃苦，就要历练。就要学会在坚持中成长。如此感慨，至深的心得体会，绝对的经验之谈。

OK，进入正题，下面就是《必须掌握的20道技术面试题》。

# **1.请用简单的语言告诉我C++ 是什么？**

答：C++是在C语言的基础上开发的一种面向对象编程语言，应用广泛。C++支持多种编程范式 －－面向对象编程、泛型编程和过程化编程。 其编程领域众广，常用于系统开发，引擎开发等应用领域，是最受广大程序员受用的最强大编程语言之一,支持类：类、封装、重载等特性!

# **2.C和C++的区别？**

答：c++在c的基础上增添类，C是一个结构化语言，它的重点在于算法和数据结构。C程序的设计首要考虑的是如何通过一个过程，对输入（或环境条件）进行运算处理得到输出（或实现过程（事务）控制），而对于C++，首要考虑的是如何构造一个对象模型，让这个模型能够契合与之对应的问题域，这样就可以通过获取对象的状态信息得到输出或实现过程（事务）控制。

# **3.什么是面向对象（OOP）？**

答：面向对象是一种对现实世界理解和抽象的方法、思想，通过将需求要素转化为对象进行问题处理的一种思想。

# **4.什么是多态？**

答：多态是指相同的操作或函数、过程可作用于多种类型的对象上并获得不同的结果。不同的对象，收到同一消息可以产生不同的结果，这种现象称为多态。

# **5.设计模式懂嘛，简单举个例子？**

答：设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。

比如单例模式，保证一个类仅有一个实例，并提供一个访问它的全局访问点。

适用于：当类只能有一个实例而且客户可以从一个众所周知的访问点访问它时；当这个唯一实例应该是通过子类化可扩展的，并且客户应该无需更改代码就能使用一个扩展的实例时。

比如工厂模式，定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method 使一个类的实例化延迟到其子类。

适用于：当一个类不知道它所必须创建的对象的类的时候；当一个类希望由它的子类来指定它所创建的对象的时候；当类将创建对象的职责委托给多个帮助子类中的某一个，并且你希望将哪一个帮助子类是代理者这一信息局部化的时候。

# **6.STL库用过吗？常见的STL容器有哪些？算法用过哪几个？**

答：STL包括两部分内容：容器和算法。（重要的还有融合这二者的迭代器）

容器，即存放数据的地方。比如array等。

在STL中，容器分为两类：序列式容器和关联式容器。

序列式容器，其中的元素不一定有序，但都可以被排序。如：vector、list、deque、stack、queue、heap、priority_queue、slist；

关联式容器，内部结构基本上是一颗平衡二叉树。所谓关联，指每个元素都有一个键值和一个实值，元素按照一定的规则存放。如：RB-tree、set、map、multiset、multimap、hashtable、hash_set、hash_map、hash_multiset、hash_multimap。

下面各选取一个作为说明。

vector：它是一个动态分配存储空间的容器。区别于c++中的array，array分配的空间是静态的，分配之后不能被改变，而vector会自动重分配（扩展）空间。

set：其内部元素会根据元素的键值自动被排序。区别于map，它的键值就是实值，而map可以同时拥有不同的键值和实值。

算法，如排序，复制……以及个容器特定的算法。这点不用过多介绍，主要看下面迭代器的内容。

迭代器是STL的精髓，我们这样描述它：迭代器提供了一种方法，使它能够按照顺序访问某个容器所含的各个元素，但无需暴露该容器的内部结构。它将容器和算法分开，好让这二者独立设计。

# **7.数据结构会吗？项目开发过程中主要用到那些？**

答：数据结构中主要会用到数组，链表，树（较少），也会用到栈和队列的思想。

# **8.const知道吗？解释其作用。**

答：

1.const 修饰类的成员变量，表示成员常量，不能被修改。

2.const修饰函数承诺在本函数内部不会修改类内的数据成员，不会调用其它非 const 成员函数。

3.如果 const 构成函数重载，const 对象只能调用 const 函数，非 const 对象优先调用非 const 函数。

4.const 函数只能调用 const 函数。非 const 函数可以调用 const 函数。

5.类体外定义的 const 成员函数，在定义和声明处都需要 const 修饰符。。

# **9.类的static变量在什么时候初始化？函数的static变量在什么时候初始化？**

答：类的静态成员变量在类实例化之前就已经存在了，并且分配了内存。函数的static变量在执行此函数时进行初始化。

# **10.堆和栈的区别？堆和栈的生命周期？**

答：

**一、堆栈空间分配区别：**

1、栈（操作系统）：由操作系统自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈；

2、堆（操作系统）： 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收，分配方式倒是类似于链表。

**二、堆栈缓存方式区别：**

1、栈使用的是一级缓存， 他们通常都是被调用时处于存储空间中，调用完毕立即释放；

2、堆是存放在二级缓存中，生命周期由虚拟机的垃圾回收算法来决定（并不是一旦成为孤儿对象就能被回收）。所以调用这些对象的速度要相对来得低一些。

**三、堆栈数据结构区别：**

堆（数据结构）：堆可以被看成是一棵树，如：堆排序；

栈（数据结构）：一种先进后出的数据结构。

# **11.C和C++的区别？**

答：C++在C的基础上增添类

C是一个结构化语言，它的重点在于算法和数据结构。

C程序的设计首要考虑的是如何通过一个过程，对输入（或环境条件）进行运算处理得到输出（或实现过程（事务）控制），而对于C++，首要考虑的是如何构造一个对象模型，让这个模型能够契合与之对应的问题域，这样就可以通过获取对象的状态信息得到输出或实现过程（事务）控制。

 

# **12.解释下封装、继承和多态？**

答：

**一、封装：**

封装是实现面向对象程序设计的第一步，封装就是将数据或函数等集合在一个个的单元中（我们称之为类）。

封装的意义在于保护或者防止代码（数据）被我们无意中破坏。

**二、继承：**

继承主要实现重用代码，节省开发时间。

子类可以继承父类的一些东西。

**三、多态**

多态：同一操作作用于不同的对象，可以有不同的解释，产生不同的执行结果。在运行时，可以通过指向基类的指针，来调用实现派生类中的方法。

# **13.指针和引用的区别？**

答：

\1. 指针是一个变量，只不过这个变量存储的是一个地址，指向内存的一个存储单元；而引用仅是个别名；

\2. 引用使用时无需解引用(*)，指针需要解引用；

\3. 引用只能在定义时被初始化一次，之后不可变；指针可变；

\4. 引用没有 const，指针有 const；

\5. 引用不能为空，指针可以为空；

\6. “sizeof 引用”得到的是所指向的变量(对象)的大小，而“sizeof 指针”得到的是指针本身的大小；

\7. 指针和引用的自增(++)运算意义不一样；

\8. 指针可以有多级，但是引用只能是一级（int **p；合法 而 int &&a是不合法的）

9.从内存分配上看：程序为指针变量分配内存区域，而引用不需要分配内存区域。

# **14.什么是内存泄漏？面对内存泄漏和指针越界，你有哪些方法？你通常采用哪些方法来避免和减少这类错误？**

答：用动态存储分配函数动态开辟的空间，在使用完毕后未释放，结果导致一直占据该内存单元即为内存泄露。

使用的时候要记得指针的长度。

malloc的时候得确定在那里free.

对指针赋值的时候应该注意被赋值指针需要不需要释放.

动态分配内存的指针最好不要再次赋值.

# **15.常用的排序算法有哪些？简单描述几个排序算法的优缺点？**

答：选择、冒泡、快速、**、希尔、归并、堆排等。

1.快排：是冒泡排序的一种改进。

优点：快，数据移动少

缺点：稳定性不足

2.归并：分治法排序，稳定的排序算法，一般用于对总体无序，但局部有序的数列。

优点：效率高O(n)，稳定

缺点：比较占用内存

# **16.new和malloc的区别？**

答：

1、malloc与free是C++/C语言的标准库函数，new/delete是C++的运算符。它们都可用于申请动态内存和释放内存。

2、对于非内部数据类型的对象而言，光用maloc/free无法满足动态对象的要求。对象在创建的同时要自动执行构造函数，对象在消亡之前要自动执行析构函数。

3、由于malloc/free是库函数而不是运算符，不在编译器控制权限之内，不能够把执行构造函数和析构函数的任务强加于malloc/free。因此C++语言需要一个能完成动态内存分配和初始化工作的运算符new，以一个能完成清理与释放内存工作的运算符delete。注意new/delete不是库函数。

4、C++程序经常要调用C函数，而C程序只能用malloc/free管理动态内存。

5、new可以认为是malloc加构造函数的执行。new出来的指针是直接带类型信息的。而malloc返回的都是void指针。

# **17.TCP和UDP通信的差别？什么是IOCP？**

答：

1.TCP面向连接， UDP面向无连接的

2.TCP有保障的，UDP传输无保障的

3.TCP是效率低的，UDP效率高的

4.TCP是基于流的，UDP基于数据报文

5.TCP传输重要数据，UDP传输不重要的数据

IOCP全称I/O Completion Port，中文译为I/O完成端口。

IOCP是一个异步I/O的API，它可以高效地将I/O事件通知给应用程序。

与使用select()或是其它异步方法不同的是，一个套接字[socket]与一个完成端口关联了起来，然后就可继续进行正常的Winsock操作了。然而，当一个事件发生的时候，此完成端口就将被操作系统加入一个队列中。然后应用程序可以对核心层进行查询以得到此完成端口。

# **18.同步IO和异步IO的区别？**

答：

**A. 同步**

所谓同步，就是在发出一个功能调用时，在没有得到结果之前，该调用就不返回。

按照这个定义，其实绝大多数函数都是同步调用（例如sin isdigit等）。

但是一般而言，我们在说同步、异步的时候，特指那些需要其他部件协作或者需要一定时间完成的任务。

最常见的例子就是 SendMessage。

该函数发送一个消息给某个窗口，在对方处理完消息之前，这个函数不返回。

当对方处理完毕以后，该函数才把消息处理函数所返回的值返回给调用者。

**B. 异步**

异步的概念和同步相对。

当一个异步过程调用发出后，调用者不会立刻得到结果。

实际处理这个调用的部件是在调用发出后，通过状态、通知来通知调用者，或通过回调函数处理这个调用。

# **19.解释C++中静态函数和静态变量？**

答：

(1)类静态数据成员在编译时创建并初始化：在该类的任何对象建立之前就存在，不属于任何对象，而非静态类成员变量则是属于对象所有的。类静态数据成员只有一个拷贝，为所有此类的对象所共享。

(2)类静态成员函数属于整个类，不属于某个对象，由该类所有对象共享。

1、static 成员变量实现了同类对象间信息共享。

2、static 成员类外存储，求类大小，并不包含在内。

3、static 成员是命名空间属于类的全局变量，存储在 data 区的rw段。

4、static 成员只能类外初始化。

5、可以通过类名访问（无对象生成时亦可），也可以通过对象访问。

1、静态成员函数的意义，不在于信息共享，数据沟通，而在于管理静态数据成员，完成对静态数据成员的封装。

2、静态成员函数只能访问静态数据成员。原因：非静态成员函数，在调用时 this指针时被当作参数传进。而静态成员函数属于类，而不属于对象，没有 this 指针。

# **20.说下你对内存的了解？**

答：

1.栈 - 由编译器自动分配释放

2.堆 - 一般由程序员分配释放，若程序员不释放，程序结束时可能由OS回收

3.全局区(静态区)，全局变量和静态变量的存储是放在一块的，初始化的全局变量和静态变量在一块区域，未初始化的全局变量和未初始化的静态变量在相邻的另一块区域。- 程序结束释放

4.另外还有一个专门放常量的地方。- 程序结束释放

5 程序代码区，存放2进制代码。

在函数体中定义的变量通常是在栈上，用malloc， calloc， realloc等分配内存的函数分配得到的就是在堆上。在所有函数体外定义的是全局量，加了static修饰符后不管在哪里都存放在全局区(静态区)，在所有函数体外定义的static变量表示在该文件中有效，不能extern到别的文件用，在函数体内定义的static表示只在该函数体内有效。另外，函数中的"adgfdf"这样的字符串存放在常量区。

原文作者：Y1
原文链接：https://www.cnblogs.com/Y1Focus/p/6707121.html



# 【NO.145】分布式事务解决方案

## **1.简述**

分布式事务是指事务的操作位于不同的节点上，需要保证事务的ACID特性。在分布式架构下，每个节点只知晓自身操作的成功与失败，无法知悉其他节点的操作状态。当一个事务跨多个节点时，为了保持事务的原子性与一致性，从而引入一个协调者来统一管控所有参与者的操作结果，并指引它们最终是否把操作结果进行诊治的提交(commit)和回滚(rollback)。例如在购物下单场景中，库存和订单如果不在同一个节点上，就涉及分布式事务。

## **2.解决方案**

在分布式系统中要实现分布式事务，常见的解决方案有两段提交(2PC)、三段提交(3PC)、事务补偿(TCC)、本地消息表(异步确保)、MQ事务方案(可靠消息事务)、最大努力通知和Saga事务。

### **2.1 两阶段提交（2PC）**

![img](https://img2022.cnblogs.com/blog/1580332/202202/1580332-20220213214017264-1669488684.png)

二阶段提交协议（Two-phase Commit，即 2PC）是常用的分布式事务解决方案，即将事务的提交过程分为准备阶段和提交阶段两个阶段来进行处理，通过引入协调者（Coordinator）来协调参与者的行为，并最终决定这些参与者是否要真正执行事务。

- 事务协调者（事务管理器）：事务的发起者
- 事务参与者（资源管理器）：事务的执行者

**准备阶段（投票阶段）**

协调者询问参与者事务是否执行成功，参与者发回事务执行结果，但该阶段并未提交事务。

1. 协调者向所有参与者发送事务内容，询问是否可以提交事务，并等待答复；
2. 各参与者执行事务操作，将 undo 和 redo 信息记入事务日志中（但不提交事务）；
3. 如参与者执行成功，给协调者反馈同意，否则反馈终止。

**提交阶段（执行阶段）**

如果事务在每个参与者上都执行成功，事务协调者发送通知让参与者提交事务；否则，协调者发送通知让参与者回滚事务。

在准备阶段，参与者执行了事务，但是还未提交。只有在提交阶段接收到协调者发来的通知后，才进行提交或者回滚。

1. 事务协调者节点向所有参与者节点发出正式提交(`commit`)的请求；
2. 参与者节点正式完成操作，并释放在整个事务期间内占用的资源；
3. 参与者节点向协调者节点发送ACK完成消息；
4. 事务协调者节点收到所有参与者节点反馈的ACK完成消息后，完成事务。

**2PC优缺**

**优点**

- 尽量保证了数据的强一致，适合对数据强一致要求很高的关键领域。（其实也不能100%保证强一致，如宕机）

**缺点**

- **性能问题**：执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。
- **可靠性问题**：参与者发生故障。协调者需要给每个参与者额外指定超时机制，超时后整个事务失败。协调者发生故障。参与者会一直阻塞下去。需要额外的备机进行容错。
- **数据一致性问题**：二阶段无法解决的问题如协调者在发出`commit`消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。
- **实现复杂**：牺牲了可用性，对性能影响较大，不适合高并发高性能场景。

### **2.2 三阶段提交（3PC）**

![img](https://img2022.cnblogs.com/blog/1580332/202202/1580332-20220213214104494-1390181182.png)

三阶段提交协议是二阶段提交协议的改进版本，其有两个改动点。

1. 在协调者和参与者中都引入超时机制；
2. 在第一阶段和第二阶段中插入一个准备阶段，保证了在最后提交阶段之前各参与节点的状态是一致的。

即除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有`CanCommit`、`PreCommit和``DoCommit`三个阶段。

**3PC优缺点**

**优点**

相比二阶段提交，三阶段提交降低了阻塞范围，在等待超时后协调者或参与者会中断事务。避免了协调者单点问题，阶段 3 中协调者出现问题时，参与者会继续提交事务。

**缺点**

数据不一致问题依然存在，当在参与者收到 `preCommit` 请求后等待 `doCommit` 指令时，此时如果协调者请求中断事务，而协调者无法与参与者正常通信，会导致参与者继续提交事务，造成数据不一致。

### **2.3 事务补偿（TCC）**

![img](https://img2022.cnblogs.com/blog/1580332/202202/1580332-20220213193657939-2019193487.png)

TCC（Try Confirm Cancel）方案是一种应用层面侵入业务的两阶段提交，是目前最火的一种柔性事务方案。TCC采用了补偿机制，其核心思想就是针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为Try、Confirm和Cancel三个阶段。

1. Try 阶段主要是对业务系统做检测及资源预留；
2. Confirm 阶段主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的，即：只要Try成功，Confirm一定成功；
3. Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。

转账例子：假入 Bob 要向 Smith 转账，思路大概是： 我们有一个本地方法，里面依次调用。

1. 首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来；
2. 在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。
3. 如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。

**TCC优缺点**

**优点**

跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC差。

**缺点**

在2和3步中都有可能会失败。TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理。

### **2.4 本地消息表（异步确保**）

本地消息表方案的核心思路是将分布式事务拆分成本地事务进行处理。通过在事务主动发起方额外新建事务消息表，事务发起方处理业务和记录事务消息在本地事务中完成，轮询事务消息表的数据发送事务消息，事务被动方基于消息中间件消费事务消息表中的事务。 

*![img](https://img2022.cnblogs.com/blog/1580332/202202/1580332-20220213194544060-1699106774.png)*

上图中整体的处理步骤如下：

1. 事务主动方在同一个本地事务中处理业务和写消息表操作；
2. 事务主动方通过消息中间件，通知事务被动方处理事务通知事务待消息。消息中间件可以基于 Kafka、RocketMQ 消息队列，事务主动方主动写消息到消息队列，事务消费方消费并处理消息队列中的消息；
3. 事务被动方通过消息中间件，通知事务主动方事务已处理的消息；
4. 事务主动方接收中间件的消息，更新消息表的状态为已处理。

本地消息表优缺点

**优点**

避免了分布式事务，实现了最终一致性。

**缺点**

消息表会耦合到业务系统中，如果没有封装好的解决方案，会有很多杂活需要处理。

### **2.5 MQ事务方案(可靠消息事务)** 

MQ事务方案是对本地消息表的封装，将本地消息表基于 MQ 内部，其他方面的协议基本与本地消息表一致。

![img](https://img2022.cnblogs.com/blog/1580332/202202/1580332-20220213195114938-2056723236.png)

### **2.6 最大努力通知**

最大努力通知方案是对MQ事务方案的进一步优化。它在事务主动方增加了消息校对的接口，如果事务被动方没有接收到消息，此时可以调用事务主动方提供的消息校对的接口主动获取。

其适用于业务通知类型，例如微信交易的结果，就是通过最大努力通知方式通知各个商户，既有回调通知，也有交易查询接口。

![img](https://img2022.cnblogs.com/blog/1580332/202202/1580332-20220213201201220-874864581.png)

### **2.7 Saga事务**

Saga 事务核心思想是将长事务拆分为多个本地短事务，由 Saga 事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作。

Saga 事务基本协议如下：

- 每个 Saga 事务由一系列幂等的有序子事务(sub-transaction) **Ti** 组成。
- 每个 **Ti** 都有对应的幂等补偿动作 **Ci**，补偿动作用于撤销 **Ti** 造成的结果。

Saga 的执行顺序有两种：

- T1, T2, T3, ..., Tn
- T1, T2, ..., Tj, Cj,..., C2, C1，其中0 < j < n

TCC事务补偿机制有一个预留(Try)动作，相当于先报存一个草稿，然后才提交；而Saga事务没有预留动作，直接提交。对于事务异常，Saga提供了向后恢复和向前恢复两种恢复策略。

**向后恢复（backward recovery）**

backward recovery，即上面提到的第二种执行顺序，其中j是发生错误的sub-transaction，这种做法的效果是撤销掉之前所有成功的sub-transation，使得整个Saga的执行结果撤销。

![img](https://img2022.cnblogs.com/blog/1580332/202202/1580332-20220213202619446-202874736.png)

**向前恢复（forward recovery）**

forward recovery，即适用于必须要成功的场景，执行顺序是类似于这样的：T1, T2, ..., Tj(失败), Tj(重试),..., Tn，其中j是发生错误的sub-transaction。该情况下不需要Ci。
 ![img](https://img2022.cnblogs.com/blog/1580332/202202/1580332-20220213202633035-1675437339.png)

## **3.总结**

各分布式事务方案的常见使用场景：

- **2PC/3PC**：依赖于数据库，能够很好的提供强一致性和强事务性，但相对来说延迟比较高，比较适合传统的单体应用，在同一个方法中存在跨库操作的情况，不适合高并发和高性能要求的场景。
- **TCC**：适用于执行时间确定且较短，实时性要求高，对数据一致性要求高，比如互联网金融企业最核心的三个服务：交易、支付、账务。
- **本地消息表/MQ 事务**：都适用于事务中参与方支持操作幂等，对一致性要求不高，业务上能容忍数据不一致到一个人工检查周期，事务涉及的参与方、参与环节较少，业务上有对账/校验系统兜底。
- **Saga 事务**：由于 Saga 事务不能保证隔离性，需要在业务层控制并发，适合于业务场景事务并发操作同一资源较少的情况。Saga 相比缺少预提交动作，导致补偿动作的实现比较麻烦，例如业务是发送短信，补偿动作则得再发送一次短信说明撤销，用户体验比较差。Saga 事务较适用于补偿动作容易处理的场景。

## **4.Seta**

上述几种方案都是分布式事务的理论知识，Seta是分布式解放方案的一个落地实现。

Seata 是一款开源的分布式事务解决方案，致力于提供高性能和简单易用的分布式事务服务。Seata 将为用户提供了 **AT**、**TCC**、**SAGA** 和 **XA** 事务模式，为用户打造一站式的分布式解决方案。

- 对业务无侵入：即减少技术架构上的微服务化所带来的分布式事务问题对业务的侵入；
- 高性能：减少分布式事务解决方案所带来的性能消耗。
- Seta官方文档：https://seata.io/zh-cn/index.html 

原文作者：[涛姐涛哥](https://home.cnblogs.com/u/taojietaoge/)

原文链接：https://www.cnblogs.com/taojietaoge/p/15890289.html

# 【NO.146】一个故事看懂CPU的SIMD技术

好久不见，我叫阿Q，是CPU一号车间的员工。我所在的CPU有8个车间，也就是8个核心，咱们每个核心都可以同时执行两个线程，就是8核16线程，那速度杠杠滴。

我所在的一号车间，除了负责执行指令的我，还有负责读取指令的小A，负责指令译码的小胖和负责结果回写的老K，我们几个各司其职，一起完成执行程序的工作。

**一个简单的循环**

那天，我们遇到了一段代码：

```
void array_add(int data[], int len) {
  for (int i = 0; i < len; i++) {
    data[i] += 1;
  }
}
```

循环了好几百次之后，才把这段代码执行完成，每次循环都是做简单又重复的工作，把我累得够呛。

一旁负责结果回写的老K也是累的满头大汗，吐槽道：“每次都是取出来加1又写回去，要是能一次多取几个数，批量处理就好了”

老K的话让我眼前一亮，对啊，能不能批量操作呢？

心里一边想着，一边继续干活了。

繁忙的一天很快结束了，转眼又到了晚上，计算机关机后，我把大家召集了起来。

“兄弟们，还记得咱们白天遇到的那个循环吗？”

“你说哪个循环，咱们这一天可执行了不少循环呢”，小A说到。

“就是那个把整数数组每个元素都加1的那个循环”

“我想起来了，那循环怎么了？有什么问题吗？”

我看了老K一眼，说道：“我在想今天老K的话，像这种循环，每次都是取出来加1又写回去，一次操作一个数，效率太低了，咱们要是升级改造一下，支持一次取出多个数，批量加1，这样岂不是快很多？”

![img](https://img2022.cnblogs.com/blog/659280/202203/659280-20220324095041810-1015126491.png)

老K一听来了兴趣，“这敢情好，你打算怎么做？”

“这我还没想好，大家有什么建议吗？”

一旁负责指令译码的小胖说道：“可以新增一条指令，专门用来一次取出多个数据来加1”

“不行不行，不能限的这么死，今天是加1，万一下次是加2呢？指令里面不能限制为1”

“那如果每个数据要加的是不一样的怎么办？”

“你这么一说，那万一不是加法，是减法，乘法怎么办？”

“还有啊，···”

大家开始七嘴八舌讨论了起来，没想到一个小小的加法循环，一下子引出了这么多问题来，这是我们没想到的。

**并行计算**

随着讨论的深入，我觉得已经超出了咱们一号车间能把控的范围，需要上报给领导，组织八个车间代表一起来商讨。

领导一听说有提高性能的新技术，马上来了兴趣，很快便开会组织大家一起来商讨方案。

![img](https://img2022.cnblogs.com/blog/659280/202203/659280-20220324095048605-964892437.png)

“都到齐了是吧，阿Q你给大家说一下这个会议的目的”，领导说到。

我站了起来，开始把我们遇到的问题和想法跟大家讲了一遍。

“是这样的，我们一号车间那天遇到了一段循环代码，循环体的内容很简单，就是给数组中的每一个元素加1。我们执行的时候，就是不断取出每一个元素，然后将其执行加法计算后，再写回去。这样一个一个来加1，我们感觉太慢了， 要是可以一次多取几个，并行加1，那一定比一个一个加快上不少。”

我刚说完，大家都开始小声议论起来。

“我看出来了，这其实就是并行计算！”，二号车间小虎一语道出了关键。

六号车间小六问道：”阿Q，你们已经有方案了吗？“

“还没有，这正是今天开会的目的，因为情况有点复杂，还需要大家一起来出出主意”

“好像并不复杂嘛”

“我上面举的例子只是一个简单的情况，并行计算还可能不是固定的数，可能是一个数组和另一个数组相加。还有可能不是整数相加，而是浮点数，甚至，还可能不是加法，而是减法或者乘法，再或者不是算术运算，而是逻辑运算”

我刚一说完，大家又开始窃窃私语交流起来。

“我琢磨着你说的这一系列东西，咱们是要新增一套专门用来并行计算的指令集啊”，小虎说道。

“这可是大工程啊”

“是啊···”

这时，小六又问道：“咱们的计算的时候，都是把数据读取到寄存器进行的，可这寄存器一次只能装一个数，怎么一次读取多个数据呢？”

“可能需要新增一些容量大一些的寄存器，比如128bit长度，可以同时容纳4个32位的整数”

![img](https://img2022.cnblogs.com/blog/659280/202203/659280-20220324095055595-1269104379.png)

“有这个必要吗？咱们是通用CPU，又不是专门做数学计算的芯片，搞这些东西干嘛？”，四号车间代表提出了质疑。

我也不甘示弱：“那可太有必要了，在图像、视频、音频处理等领域，有大量这样的计算需求，咱们得提升处理这些数据的能力”

见我们争执不下，领导拍了拍桌子，会场一下安静了下来。

“我觉得阿Q说的有道理，咱们确实需要提升处理这类数据运算的能力了。不过不用一下搞那么复杂，先支持整数并行运算就行了。新增寄存器这个也不用着急，可以先借用一下浮点数运算单元FPU的寄存器。这件事先这么定下来，具体的方案你们再继续讨论。”，说完便离开了会议室。

领导不愧是领导，几句话就把我们安排的明明白白。

**SIMD**

又经过一阵紧张的讨论，我们终于敲定了方案。

我们借用浮点数运算单元的寄存器，还给它们起了新的名字：MM0-MM7。因为是64位的寄存器，所以可以同时存储两个32位的整数或者4个16位整数或者8个8位的整数。

我们还新增了一套叫**MMX**的指令集，用来并行执行整数的运算。

![img](https://img2022.cnblogs.com/blog/659280/202203/659280-20220324095103206-1244477371.png)

我们把这种在一条指令中同时处理多个数据的技术叫做单指令多数据流（`Single Instruction Multiple Data`），简称**SIMD**。

有了这套指令集，咱们处理这类整数运算问题的速度快了不少。

不过渐渐地发现了两个很麻烦的问题：

第一个问题，因为是借用FPU的寄存器，所以当执行SIMD指令的时候，就不能用FPU计算单元，反过来也一样，同时使用的话就会出乱子，所以要经常在不同的模式之间切换，实在是有些麻烦。

另一个更重要的问题，咱们这套指令集只能处理整数的并行运算，可现在浮点数的并行运算越来越多，尤其是图像、视频还有深度学习的一些数据处理，浮点数情况越来越多，这时候都派不上用场。

我们把这些问题给领导做了汇报，看到我们已经做出的成绩，领导终于同意继续升级。

这一次，我们扩展了一套新的SSE指令集出来，新增了XMM0-XMM7总共8个128位的寄存器，再也不用跟FPU共享寄存器了。而且位宽加了一倍，能容纳的数据更多了，能同时处理的数据自然也变多了。

后来，我们又不断的修改升级，不仅支持了对浮点数并行处理，还推出了新一代的AVX指令集，把寄存器再一次扩大为256位，现在我们的SIMD技术更加先进，处理数据运算的能力越来越强了！

原文作者：[轩辕之风](https://www.cnblogs.com/xuanyuan/)

原文链接：https://www.cnblogs.com/xuanyuan/p/16048303.html

# 【NO.147】聊一聊数据库中的锁

## 0.背景

数据库中有一张叫`后宫佳丽`的表,每天都有几百万新的小姐姐插到表中,光阴荏苒,夜以继日,日久生情,时间长了,表中就有了几十亿的`小姐姐`数据,看到几十亿的小姐姐,每到晚上,我可愁死了,这么多小姐姐,我翻张牌呢?
办法当然是精兵简政,删除那些`age>18`的,给年轻的小姐姐们留位置...
于是我在数据库中添加了一个定时执行的小程序,每到周日,就自动运行如下的脚本

```sql
Copydelete from `后宫佳丽` where age>18
```

一开始还自我感觉良好,后面我就发现不对了,每到周日,这个脚本一执行就是一整天,运行的时间有点长是小事,重点是这大好周日,我再想读这张表的数据,怎么也读不出来了,怎是一句空虚了得,我好难啊!

![Alt text](https://raw.githubusercontent.com/liuzhenyulive/GitDisk/blogs/pic/TalkAboutLockInDatabase/%E9%9A%BE.gif)

## 1.为什么

编不下去了,真实背景是公司中遇到的一张有海量数据表,每次一旦执行历史数据的清理,我们的程序就因为读不到这张表的数据,疯狂地报错,后面一查了解到,原来是因为定时删除的语句设计不合理,导致数据库中数据由行锁(`Row lock`)升级为表锁(`Table lock`)了😂.
解决这个问题的过程中把数据库锁相关的学习了一下,这里把学习成果,分享给大家,希望对大家有所帮助.
我将讨论SQL Server锁机制以及如何使用SQL Server标准动态管理视图监视SQL Server 中的锁,相信其他数据的锁也大同小异,具有一定参考意义.

## 2.铺垫知识

在我开始解释SQL Server锁定体系结构之前，让我们花点时间来描述ACID（原子性，一致性，隔离性和持久性）是什么。ACID是指数据库管理系统（DBMS）在写入或更新资料的过程中，为保证事务（transaction）是正确可靠的，所必须具备的四个特性：原子性（atomicity，或称不可分割性）、一致性（consistency）、隔离性（isolation，又称独立性）、持久性（durability）。

### 2.1 ACID

#### 2.1.1 原子性(Atomicity)

一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。

#### 2.1.2一致性(Consistency)

在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。

#### 2.1.3 隔离性(Isolation)

数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。

#### 2.1.4 持久性(Durability)

事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

来源:维基百科 https://zh.wikipedia.org/wiki/ACID

### 2.2 事务 (Transaction:)

事务是进程中最小的堆栈，不能分成更小的部分。此外，某些事务处理组可以按顺序执行，但正如我们在原子性原则中所解释的那样，即使其中一个事务失败，所有事务块也将失败。

### 2.3 锁定 (Lock)

锁定是一种确保数据一致性的机制。SQL Server在事务启动时锁定对象。事务完成后，SQL Server将释放锁定的对象。可以根据SQL Server进程类型和隔离级别更改此锁定模式。这些锁定模式是：

#### 2.3.1 锁定层次结构

SQL Server具有锁定层次结构，用于获取此层次结构中的锁定对象。数据库位于层次结构的顶部，行位于底部。下图说明了SQL Server的锁层次结构。

![Alt text](https://raw.githubusercontent.com/liuzhenyulive/GitDisk/blogs/pic/TalkAboutLockInDatabase/hierarchy.png)

#### 2.3.2 共享（S）锁 (Shared (S) Locks)

当需要读取对象时，会发生此锁定类型。这种锁定类型不会造成太大问题。

#### 2.3.3 独占（X）锁定 (Exclusive (X) Locks)

发生此锁定类型时，会发生以防止其他事务修改或访问锁定对象。

#### 2.3.4 更新（U）锁 (Update (U) Locks)

此锁类型与独占锁类似，但它有一些差异。我们可以将更新操作划分为不同的阶段：读取阶段和写入阶段。在读取阶段，SQL Server不希望其他事务有权访问此对象以进行更改,因此，SQL Server使用更新锁。

#### 2.3.5 意图锁定 (Intent Locks)

当SQL Server想要在锁定层次结构中较低的某些资源上获取共享（S）锁定或独占（X）锁定时，会发生意图锁定。实际上，当SQL Server获取页面或行上的锁时，表中需要设置意图锁。

### 2.4 SQL Server locking

了解了这些背景知识后，我们尝试再SQL Server找到这些锁。SQL Server提供了许多动态管理视图来访问指标。要识别SQL Server锁，我们可以使用sys.dm_tran_locks视图。在此视图中，我们可以找到有关当前活动锁管理的大量信息。

在第一个示例中，我们将创建一个不包含任何索引的演示表，并尝试更新此演示表。

```sql
CopyCREATE TABLE TestBlock
(Id INT ,
Nm VARCHAR(100))

INSERT INTO TestBlock
values(1,'CodingSight')
In this step, we will create an open transaction and analyze the locked resources.
BEGIN TRAN
UPDATE TestBlock SET   Nm='NewValue_CodingSight' where Id=1
select @@SPID
```

![Alt text](https://raw.githubusercontent.com/liuzhenyulive/GitDisk/blogs/pic/TalkAboutLockInDatabase/update-demo-table.png)

再获取到了SPID后，我们来看看`sys.dm_tran_lock`视图里有什么。

```sql
Copyselect * from sys.dm_tran_locks  WHERE request_session_id=74
```

![Alt text](https://raw.githubusercontent.com/liuzhenyulive/GitDisk/blogs/pic/TalkAboutLockInDatabase/sys.dm_tran_lock-view-1.png)

此视图返回有关活动锁资源的大量信息,但是是一些我们难以理解的一些数据。因此，我们必须将`sys.dm_tran_locks` join 一些其他表。

```sql
CopySELECT dm_tran_locks.request_session_id,
       dm_tran_locks.resource_database_id,
       DB_NAME(dm_tran_locks.resource_database_id) AS dbname,
       CASE
           WHEN resource_type = 'OBJECT'
               THEN OBJECT_NAME(dm_tran_locks.resource_associated_entity_id)
           ELSE OBJECT_NAME(partitions.OBJECT_ID)
       END AS ObjectName,
       partitions.index_id,
       indexes.name AS index_name,
       dm_tran_locks.resource_type,
       dm_tran_locks.resource_description,
       dm_tran_locks.resource_associated_entity_id,
       dm_tran_locks.request_mode,
       dm_tran_locks.request_status
FROM sys.dm_tran_locks
LEFT JOIN sys.partitions ON partitions.hobt_id = dm_tran_locks.resource_associated_entity_id
LEFT JOIN sys.indexes ON indexes.OBJECT_ID = partitions.OBJECT_ID AND indexes.index_id = partitions.index_id
WHERE resource_associated_entity_id > 0
  AND resource_database_id = DB_ID()
 and request_session_id=74
ORDER BY request_session_id, resource_associated_entity_id
```

![Alt text](https://raw.githubusercontent.com/liuzhenyulive/GitDisk/blogs/pic/TalkAboutLockInDatabase/join-sys.dm_tran_locks-view-to-other-views-1.png)

在上图中，您可以看到锁定的资源。SQL Server获取该行中的独占锁。（RID：用于锁定堆中单个行的行标识符）同时，SQL Server获取页中的独占锁和TestBlock表意向锁。这意味着在SQL Server释放锁之前，任何其他进程都无法读取此资源,这是SQL Server中的基本锁定机制。

现在，我们将在测试表上填充一些合成数据。

```sql
CopyTRUNCATE TABLE 	  TestBlock
DECLARE @K AS INT=0
WHILE @K <8000
BEGIN
INSERT TestBlock VALUES(@K, CAST(@K AS varchar(10)) + ' Value' )
SET @K=@K+1
 END
--After completing this step, we will run two queries and check the sys.dm_tran_locks view.
BEGIN TRAN
 UPDATE TestBlock  set Nm ='New_Value' where Id<5000
```

![Alt text](https://raw.githubusercontent.com/liuzhenyulive/GitDisk/blogs/pic/TalkAboutLockInDatabase/check-sys.dm_tran_locks-view-2.png)

在上面的查询中，SQL Server获取每一行的独占锁。现在，我们将运行另一个查询。

```sql
CopyBEGIN TRAN
 UPDATE TestBlock  set Nm ='New_Value' where Id<7000
```

![Alt text](https://raw.githubusercontent.com/liuzhenyulive/GitDisk/blogs/pic/TalkAboutLockInDatabase/run-another-query-1.png)

在上面的查询中，SQL Server在表上创建了独占锁，因为SQL Server尝试为这些将要更新的行获取大量RID锁,这种情况会导致数据库引擎中的大量资源消耗,因此，SQL Server会自动将此独占锁定移动到锁定层次结构中的上级对象(Table)。我们将此机制定义为Lock Escalation, 这就是我开篇所说的锁升级,它由行锁升级成了表锁。

根据官方文档的描述存在以下任一条件，则会触发锁定升级：

- 单个Transact-SQL语句在单个非分区表或索引上获取至少5,000个锁。
- 单个Transact-SQL语句在分区表的单个分区上获取至少5,000个锁，并且ALTER TABLE SET LOCK_ESCALATION选项设置为AUTO。
- 数据库引擎实例中的锁数超过了内存或配置阈值。

https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008-r2/ms184286(v=sql.105)

### 2.5 如何避免锁升级

防止锁升级的最简单，最安全的方法是保持事务的简短，并减少昂贵查询的锁占用空间，以便不超过锁升级阈值,有几种方法可以实现这一目标.

#### 2.5.1 将大批量操作分解为几个较小的操作

例如，在我开篇所说的在几十亿条数据中删除小姐姐的数据：

```sql
Copydelete from `后宫佳丽` where age>18
```

我们可以不要这么心急,一次只删除500个，可以显着减少每个事务累积的锁定数量并防止锁定升级。例如：

```sql
CopySET ROWCOUNT 500
delete_more:
     delete from `后宫佳丽` where age>18
IF @@ROWCOUNT > 0 GOTO delete_more
SET ROWCOUNT 0
```

#### 2.5.2 创建索引使查询尽可能高效来减少查询的锁定占用空间

如果没有索引会造成表扫描可能会增加锁定升级的可能性, 更可怕的是，它增加了死锁的可能性，并且通常会对并发性和性能产生负面影响。
根据查询条件创建合适的索引,最大化提升索引查找的效率,此优化的一个目标是使索引查找返回尽可能少的行，以最小化查询的的成本。

#### 2.5.3 如果其他SPID当前持有不兼容的表锁，则不会发生锁升级

锁定升级始总是升级成表锁，而不会升级到页面锁定。如果另一个SPID持有与升级的表锁冲突的IX（intent exclusive）锁定，则它会获取更细粒度的级别（行，key或页面）锁定，定期进行额外的升级尝试。表级别的IX（intent exclusive）锁定不会锁定任何行或页面，但它仍然与升级的S（共享）或X（独占）TAB锁定不兼容。
如下所示,如果有个操作始终在不到一小时内完成，您可以创建包含以下代码的sql，并安排在操作的前执行

```sql
CopyBEGIN TRAN
SELECT * FROM mytable (UPDLOCK, HOLDLOCK) WHERE 1=0
WAITFOR DELAY '1:00:00'
COMMIT TRAN
```

此查询在mytable上获取并保持IX锁定一小时，这可防止在此期间对表进行锁定升级。

## 3.Happy Ending

好了,不说了,小姐姐们因为不想离我开又打起来了(死锁).

![Alt text](https://raw.githubusercontent.com/liuzhenyulive/GitDisk/blogs/pic/TalkAboutLockInDatabase/ending.png)

参考文献:
SQL Server Transaction Locking and Row Versioning Guide https://docs.microsoft.com/en-us/previous-versions/sql/sql-server-guides/jj856598(v=sql.110)
SQL Server, Locks Object https://docs.microsoft.com/en-us/sql/relational-databases/performance-monitor/sql-server-locks-object?view=sql-server-2017
How to resolve blocking problems that are caused by lock escalation in SQL Server https://support.microsoft.com/es-ve/help/323630/how-to-resolve-blocking-problems-that-are-caused-by-lock-escalation-in
Main concept of SQL Server locking https://codingsight.com/main-concept-of-sql-server-locking/

原文作者：码农阿宇

原文链接：https://www.cnblogs.com/CoderAyu/p/11375088.html

# 【NO.148】如何写代码 —— 编程内功心法

# 

写代码就是学一门语言然后开始撸代码吗？看完了我的[《GoF设计模式》](https://www.bughui.com/2017/06/10/gof-design-pattern-overview/)系列文章的同学或者本身已经就是老鸟的同学显然不会这么认为。编程是一项非常严谨的工作！虽然我们自嘲为码农，但是这工作毕竟不是真正的搬砖，我们是软件工程师。编程需要关注的问题太多，不仅仅有语言，还有算法、数据结构、编程技巧、编码风格、设计、架构、工程化、开发工具、团队协作等方方面面，涉及到很多层面的问题。本文将分享一下根据我这几年来的编程经验总结出的一些关于如何写代码的个人见解。

由于“跟我混”的一些小伙伴编程功底相对来说比较薄弱，所以在此总结一篇“编程内功心法”帮助他们渡过职业生涯的第一个瓶颈期。顺便，也造福一下路过的有缘的同学！于是有了此文。

## 0.前言

首先，思考一个问题，何谓编程？编程就是写代码吗？

> 所谓的编程，其实就是不断的对这个现实世界中的问题建立模型并将其固化为代码自动化执行的过程。
> ~ [Bug辉](https://www.bughui.com/) 《[GoF设计模式 - 解释器模式](https://www.bughui.com/2017/08/14/gof-design-pattern-interpreter/#section-7)》

在对问题建立模型的过程中，我们会遇到非常多不同层面的问题，所以我们需要很多领域的知识去解决这些问题。

- 我们需要管理被操作的数据，因为数据与数据之前是相互有关联的。将数据结构化，通常是编程的第一步。关于结构化数据的相关理论以及实践，需要有一个专门的学科分支或者说课题去研究——**数据结构**。
- 我们需要解决一个具体的问题，这个具体的问题如何一步步去解决，过程是怎么样子的——**算法**。
- 我们需要将解决方案进行自动化，并以代码的形式进行交付——**编程语言**。
- 如果将一个抽象的模型进行编码实现，如何实现“这个功能”，如何实现“那个功能”——**编程技巧**。
- 问题的规模大了，众多代码糅合在一起，连程序员自己都看不懂了！怎么来拆分、模块化这些代码——**设计**。
- 代码量已经到了一个人无法完成的地步了，需要团队分工合作才能完成了——**工程化**。
- 你写的代码我看不懂，没法调用或者很难调用，我写的代码你也看不懂，或者很难看懂。还怎么愉快的玩耍——**编码风格/编码规范**。
- 问题的规模继续扩大，到了系统工程的规模了，之前学的套路已经不管用了！怎么来构建这个系统才能实现正确、安全、高性能、高可用——**架构**。

然而这些也只是一个系统工程中的冰山一角！这是一个庞大的体系。也正是因为软件开发需要考虑到的问题太多且团队成员水品参差不齐，所以团队开发中并不是每个程序员做的事情都是一样的。每个人都有自己的角色、初级工程师、中级工程师、高级工程师、架构师、CTO。。。

所以编程不仅仅只是堆砌代码！

说到这里，我想起来了一件事情——为啥业界普遍鄙视培训出来半道出家的新人？人与人的区别是很大的！我见过培训出来也很牛的。其实，说到底，被鄙视的并不是所有人。而是那些培训了几个月之后发现随便找个工作也能拿“高薪”然后还自认为编程很简单的新人。因为这种经历给了他们一种错觉——编程如此简单，我培训几个月也会嘛！有点像刚学会开车的新司机，很嚣张的对老司机说“开车很简单嘛！你看我也会啊！”。语言和开发工具只是招式，这是外功。而编程思想、经验是内功。这些内功并不是靠短短几个月的培训能够掌握的，这一点有点像中国制造业和日本制造业的区别。动不动赶英超美可不好。。。

编程并不简单！这是一件很严肃的事情。不过今天，我没有办法介绍完所有的方面！或者说，到今天为止，我也并没能掌握所有领域的知识。所以今天我只是分享一些关于编码本身的一些经验。

另外，本文主要分享如何写代码，并不是如何用Java写代码。所以文章中各种语言都有可能出现。

# 1.编码风格

先来一个圈内的段子。
大部分程序员在工作中都很讨厌这四件事情：

1. 写注释
2. 写文档
3. 别人不写注释
4. 别人不写文档

o(∩_∩)o 哈哈。。中枪了没！

这个段子其实反映出来一个问题，即大部分代码都需要通过大量注释和文档来说明才能将意图传达给维护这些代码的程序员！然而，就像上面的段子说的那样，写大量的注释和文档其实是一件很麻烦的事情。所以很多时候，由于嫌麻烦，注释和文档就没写，导致维护代码的人相当的痛苦。这个苦同学们肯定都是体会过的！相当于给你个精密仪器要你维护还不给说明书。

其实，打破上面那个段子描述的那个怪圈的一个很有效的手段就是统一编码风格。优秀的代码可以实现代码即注释，代码本身就可以非常清晰的体现出它的意图来，让别人可以很容易读懂。这就是所谓的可读性！

### 1.1 命名

计算机科学领域中最难的两件事是命名和缓存失效！命名并不简单，很复杂。好的名字可以见名知意，非常容易理解。之所以说命名难是因为命名的过程同时也是概念提取的过程！对问题建立模型，需要提取概念并赋予其“术语”。这个过程其实是“万里长征”中最难的一步。毕竟，设计也好，架构也罢，都有成熟的套路可以参考。唯独这个过程，是需要程序设计者自己进行充分的思考的创造性工作！

以下是总结出来的一些命名经验：

- 一个类是某物、某事、某人的抽象，是数据与行为的集合体。这恰好符合名词的定义，因此 **类名** 是一个名词！
- **方法名** 或者说 **函数名** 是某操作或者某过程的抽象，是一个动作。这恰好符合动词的定义，因此函数名通常是一个动词。
- 变量名宁可长一些说明清楚用途也不要用`a`、`b`、`c`之类的无意义的名称，除非是循环计数器中用`i`、`j`、`k`等约定俗成的一些变量名。比如`pageIndex`和`pageSize`就要比取名成`i`和`s`好！取成这种和用混淆器混淆过后的代码一样的名称没有什么好处，如果算法比较复杂的话，过一段时间恐怕自己都会看不懂。
- 变量名最好包含变量本身的业务含义。比如`List<Student> studentList = new ArrayList<>();`就比`List<Student> list = new ArrayList<>();`好很多。如果同一段代码里再出现一个`List<Teacher>`的话，这样就可以很方便的取名为`teacherList`或者`teachers`而不是`list1`和`list2`这样的毫无意义的名称！

#### 1.1.1 英文不好怎么办

这个问题怎么说呢。。
作为一名程序员吧，基础的英文还是要懂的。要不然发展也容易遇到天花板，学不好编程的。毕竟，最新的技术、解决方案、工具都是从国外传过来的。如果是解决一些基础性的问题，每天只做做CRUD，好像英文确实不怎么用得上。但是一旦遇到一些实质性问题，恐怕只能到英文网站上找喽！ㄟ(▔ ,▔)ㄏ 不要跟我说你编程可以不需要[Stack Overflow](https://stackoverflow.com/)。**copying and pasting from stackoverflow** 可是终极编程大法！o(∩_∩)o 这句话可是编程的真谛啊！(如果你看不懂这个梗那你有可能是伪程序员)

其实，话说回来，实在不方便用英文的时候，我认为也可以用拼音命名。这个问题上可以务实一点，量力而行。但是，拼音和英语混用的做法就不太好了。最好别这样！逼格不高。

### 1.2 注释

#### 1.2.1 怎么添加代码注释

关于注释，我们需要解决的第一个问题是如何添加代码注释。

对于Java、C#之类的语言，有专用的文档注释语法，很好处理。对于C/C++，则按约定的格式说明一下类和函数、代码片段的作用和意图即可，至少编译器会进行静态检查。在[**Python**](https://www.bughui.com/2017/04/17/python-grammar-notes-1/)中，有更牛逼的文档字符串这样的语言级特性支持，看注释用`help()`很方便。不过对于[**Lua**](https://www.bughui.com/2017/04/01/lua-grammar-points/)这样的弱类型解释型语言，注释就比较难处理了。这里以Lua为例给出一种注释的解决方案。

借用Java语言文档注释的风格。

文件注释，或者说类/模块注释。

```lua
--[[
    Object-oriented helper functions for Lua
    @author: Elvin Zeng
    @date: 2017-8-21
--]]
```

函数注释

```lua
--[[
    create a class with specified super class.
    if number of parameters is zero, derived class will extends from {}.
    @param superClass super class of target class
    @return derived class
--]]
local function createClass(superClass)
    local derivedClass = {}

    --  省略一堆代码

    return derivedClass
end
--[[
    register a new account
    @param user
      {
        username = "username",
        password = "password"
      }
    @return registered user
--]]
local function register(user)

end
```

tips: [**Lua**](https://www.bughui.com/2017/04/01/lua-grammar-points/)中可以通过**metatable**机制实现类和继承，这一点与Javascript通过原型机制来实现类和继承有点类似。

#### 1.2.2 注释里该写些什么

我们首先来看个反例。

```java
/**
 * 查询
 */
public List<Article> queryPage(int pageSize, int pageIndex) throws PageIndexOutOfBoundsException {
    //  定义一个整型变量
    int offset;

    //  省略一堆代码
}
```

首先这个方法名本身就取得不好，这个暂且不说，先说注释问题。这里的注释犯了几个错：

1. 方法注释为“查询”，这简直就是废话！方法名已经告诉别人这是查询方法了，还在这个注释里写这两个字有什么意义呢？而且到底查询些什么这里也没说！
2. 参数没有注释。没有描述每一个参数的意义以及取值范围等！
3. 什么情况下会抛出`PageIndexOutOfBoundsException`没有描述清楚。
4. “定义一个整型变量”这种垃圾注释就不要写了，这么简单的语句谁看不懂啊！如果要注释，也是写上这个变量的含义。

这里我们先不考虑设计问题（分页索引号最好做成可以自己调整成合理值），下面再来看改善注释之后的代码。

```java
/**
 * 列出指定分页的文章
 * @param pageSize 分页大小。如果等于0则表示查询出所有文章。
 * @param pageIndex 分页索引号。必须为一个大于0的整数，第一页索引为1。
 * @return 指定分页的文章列表
 * @throws PageIndexOutOfBoundsException 当分页索引号超出正常范围时抛出，即pageIndex小于0或大于最大页索引时。
 */
public List<Article> listArticle(int pageSize, int pageIndex) throws PageIndexOutOfBoundsException{
    //  第一条文章记录在MySql数据库中的偏移量
    int offset;

    //  省略一堆代码
}
```

改完之后的注释有没有感觉信息更全很多！虽然说代码本身就是最好的注释，但是必要的注释还是得写上去，毕竟调用的时候别人没法猜测你的索引号到底从0还是从1开始。另外，如果函数内算法比较复杂，可以在代码块内注释，也可以在函数注释上直接写清楚这个函数内部的大概算法/逻辑。代码写出来就是给别人调用的，如果没有基本的注释信息，那么每次调用你的代码的时候，都得去看一下你的函数内具体逻辑才能知道怎么调用。这显然是非常低效的！

命名与注释这两个基本方面没做好的话，会影响到整个团队的运作。也就是说，你封装的东西并没有给队友节省什么时间，别人用到你的代码的时候，又需要花上一些时间去读你的代码。如果团队里每个人都这样，那整个团队都会极其低效。我个人是非常不愿意与这种代码风格恶劣的人合作的。

### 1.3 参考规范

关于编码风格的问题，本文只说命名和注释这两个方面。关于缩进、空格、断行、空行等其他方面的问题，可以参考本节给出的参考规范。

不同的企业会有不同的编码规范，所以这里没有办法给出一个符合所有公司的规范。不过制定自己团队的规范的时候，可以参考一些大企业的做法。以下是世界上最大的互联网公司谷歌的编码规范，同学们可以参考这个。

- [Google Java Style Guide](https://google.github.io/styleguide/javaguide.html)
- [Google C++ Style Guide](https://google.github.io/styleguide/cppguide.html)
- [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html)
- [Google HTML/CSS Style Guide](https://google.github.io/styleguide/htmlcssguide.html)
- [Google JavaScript Style Guide](https://google.github.io/styleguide/jsguide.html)

## 2.异常处理

### 2.1 异常与返回值有什么不同

在C语言中，我们的函数通常会返回一个整型值作为状态码用于通知客户端调用的结果。比如0表示成功，非0表示失败。并且可以通过不同的数值来表示不同原因导致的失败。然而在Java、C#、C++一类面向对象语言中，一般不会用返回值来表示状态。返回值一般用于表示返回的业务值，而异常用于通知客户端程序运行状态改变了。

### 2.2 什么时候需要抛出异常

关于这个问题，我想到了一句极其精炼的话：当函数**无法完成**其**宣称的任务**的时候抛出异常！
比如上面的那个日子，当`listArticle`方法由于种种原因无法查询出文章列表的时候，则抛出异常。

抛出异常在这种场景下是非常有必要的，因为这样其他人调用你的代码时可以非常放心的去调用，只要调用了你的方法，就会返回文章列表。如果无法返回文章列表，则会抛出异常。完全不用在调用这个函数的时候去怀疑是否执行成功了。

再来一句至理名言：

> 宁愿终止程序也不要带错运行下去。

也就是说，遇到错误的时候，宁愿抛出异常终止程序，也不要带着错运行下去。这是在掩耳盗铃！

### 2.3 异常需要携带什么信息

首先，异常的类型本身会带有异常种类信息。其次，异常的`message`属性可以带上更详细一些的信息。这里需要注意，千万不要像下面这么做。

```java
throw new PageIndexOutOfBoundsException("失败！");
```

抛出异常了肯定是执行失败了呀！带上这种信息有什么用，不是带了一句废话嘛！
应该是下面这样

```java
throw new PageIndexOutOfBoundsException("参数分页索引号pageIndex不能大于分页总数");
```

此外，异常堆栈也会携带很多信息。

## 3.日志

谈到日志，首先要搞清楚一个问题，日志是干嘛用的？
用来记录运行时的错误信息啊！
是啊。好像大家都知道日志是干什么用的，但是为什么写起代码来就会忘记初衷呢！
来看看代码：

```java
/**
 * 异步发送通知邮件。
 * @param templateFile 邮件模板文件路径，相对于classpath。
 * @param modelMap 模型对象
 */
public void sendEmail(String templateFile, Map<String, String> modelMap){
    //  这里省略一些代码
    System.out.println("1");
    //  这里省略一些代码
    System.out.println("2");
    //  这里省略一些代码
}
```

这里的代码是什么意思呢？程序员们应该都能明白的！很显然，这位程序员是想借助这些标记来调试，想知道代码到底执行到哪一行了。但是，这里很明显地犯了两个错。

1. 为什么是`System.out.println("");`而不是`logger.debug("");`?
2. 为什么是`1`、`2`而不是一些更明确的文字信息呢？

在这里，合理的方式是下面这样。

```java
/**
 * 异步发送通知邮件。
 * @param templateFile 邮件模板文件路径，相对于classpath。
 * @param modelMap 模型对象
 * @throws ServiceException 当邮件模板文件不存在或者modelMap中缺少必须的字段。
 */
 public void sendEmail(String templateFile, Map<String, String> modelMap) throws ServiceException{
     //  这里省略一些代码
     if (isTemplateExists){
         logger.debug("模板文件存在");
         //  这里省略一些代码
         logger.debug("邮件发送任务成功入队。任务id：" + taskId);
         //  这里省略一些代码
     }else{
         logger.error("指定的模板文件[" + templateFile + "]不存在，邮件发送失败。");
         //  抛出异常
     }
 }
```

我想给正在犯上面的错的同学提个醒：

1. 使用日志框架，并用合适的级别输出日志非常重要。
   好多程序员从来不负责也不参与运维相关的工作，甚至是做了好几年的Web都从来没有自己发布过网站。所以压根没有后期维护的意识！
   如果没有这些日志，当项目上线之后，运维的背锅侠兄弟发现网站挂了之后只能直接重启，然后当作什么也没看到。因为没有排错的线索。
2. 输出有效信息。
   不要去输出一些像`1`、`2`、`3`、`成功`、`失败`、`hello`这样的毫无意义的日志，要输出`logger.debug("邮件发送任务成功入队。任务id：" + taskId);`这样的有效信息。
   也许当时你调试的时候，在你看来这些奇怪的字符串是有意义的，但是在其他人看来，这些就是天书。运维的背锅侠会提刀过来砍你的！另外像`"-------开始执行--------"`这种对运行期间定位问题没有半点好处的日志就不要输出了！自己用可以，提交代码前一定要删掉。
3. 日志中带上上下文信息。
   孤立的一句错误日志通常没有什么实际作用。比如上面的例子中，如果在找不到指定的模板文件的时候未将发送邮件时指定的模板文件名输出，那么排错的时候无法知道到底是少了哪个模板文件。
4. 不要在日志中输出用户的敏感信息。
   千万不要在日志中输出像用户密码、邮件内容之类的涉及用户隐私的敏感信息，也不要去输出像验证码的值之类的敏感信息。

### 3.1 参数校验

在你对外公开的方法前先插入一些检查参数的代码，以确保方法被“正确的姿势”调用。比如：

```java
/**
 * 列出指定分页的文章
 * @param pageSize 分页大小。如果等于0则表示查询出所有文章。
 * @param pageIndex 分页索引号。必须为一个大于0的整数，第一页索引为1。
 * @return 指定分页的文章列表
 * @throws PageIndexOutOfBoundsException 当分页索引号超出正常范围时抛出，即pageIndex小于0或大于最大页索引时。
 */
public List<Article> listArticle(int pageSize, int pageIndex) throws PageIndexOutOfBoundsException{
    if (pageSize < 0){
        throw new IllegalArgumentException("pageSize不能小于0");
    }
    if (pageIndex < 1){
        throw new IllegalArgumentException("pageIndex不能小于1");
    }

    //  第一条文章记录在MySql数据库中的偏移量
    int offset;

    throw new PageIndexOutOfBoundsException("");

    //  省略一堆代码
}
```

#### 3.1.1 参数校验的作用

如果在对外公开的重要方法开始的位置不插入校验参数的代码，有时恐怕方法需要运行到方法内部比较深的位置才会抛出一个异常来。而且那种情况下，抛出的异常可能就会有各种各样的了。比如空指针、除零异常等。
这种情况下，很难一眼看出引发这个异常的根源是参数传错了。需要对你的代码进行一番调试才行！如果一开始就在代码的入口插入了校验参数的代码，那么调用的时候，一眼就能看出来是参数传错了导致了一个异常。这样其他程序员看到这个异常之后就会去看一下你的方法注释。他一看，哦！原来分页索引号是从1开始计数的，那么这个问题就会就此打住，给团队节省了时间。

参数校验问题是会影响团队运行效率的一个很关键的因素。所以，请同学们重视起这个问题来。我们都是工程师，团队作战的，自己写代码快不叫快，整个团队快起来才叫真的快！用好断言，可以让你的代码更健壮。

tips: Java中默认断言是不开启的，所以建议无视Java语言的断言，自己处理。

#### 3.1.2 什么时候需要进行参数校验

我认为一个方法或者函数在满足以下条件时有必要进行参数校验：

1. 方法或者函数是对外公开的，不是私有的。
2. 参数有可能为空指针的时候。
3. 参数的合理值无法通过方法名、参数名、参数类型一眼看出来的时候！比如上面那个pageIndex是从1开始计数的，但别人并不知道你是从1开始计数的。

如果对每一方法都进行校验的话，其实挺麻烦的。程序员的时间是很宝贵的，没这么多闲工夫。不过在满足上面条件的情况下，最好还是校验一下。因为做了这个校验，你自己是会稍微浪费几分钟的时间，不过从团队整体来看，总的调试损耗的时间却降下来了。要记住方法/函数写出来就是给别人调用的！

#### 3.1.3 参数校验需要做到什么程度

我有一个标准，就是把自己当成调用这些代码的那个人，把自己想象成有可能以任何“姿势”调用的菜鸟（实际上也有可能是不了解你的代码的大牛）。如果这个时候自己也有可能会犯某些错（比如没注意边界值，没注意是否可空），那么这个时候是必须要做校验的。对于一些已经在其他层做过处理不太可能有错误的值的情况，可以不做校验。比如你的`UserService`中有一个签名为`public void register(User user)`的方法，用于注册一个用户。这种情况下，可以只校验一下`user`参数是否为空，而不用对`user`的`username`、`password`属性进行校验(用户名密码长度是否合法等)。因为你在上一层控制器层模型绑定的时候已经做过非常严谨的校验了。当然，这里如果你有充足的时间，也可以校验一下。具体做到什么程度，还需要你根据情况去自己把握。

## 4.后记

编码规范就是用来约束别人的！
o(∩_∩)o 哈哈！开玩笑的啦！
其实很多时候，出于各种原因，如“项目周期紧”、“项目还在探索阶段可行性未知，先实现了再说”、“项目中其他代码已经这样了，破罐子破摔”等，最终导致的结果可能就是我们这些自称“有经验”的程序员自己也不一定能写出完全符合这些理念的代码来。或许是吧！
ㄟ(▔ ,▔)ㄏ
我承认，我也写过奇葩代码。
但是，这好像并不是你这个作为未来优秀程序员的人不思进取的理由。

小时候，老师教我们要诚实，但是老师自己也不见得能完全做到。我们可以因为这个鄙视他。
长大后，体验过了生活中会有很多的无奈，不再鄙视“不诚实”的老师。甚至低下了高贵的头，自己也变得那般模样。
未来，你还会教育你的后代“要诚实”吗？
恐怕会！
因为，优秀的理念，不管结局如何，都应该去提倡！

**本文的观点仅代表现在的我，人是会成长的，明天的我或许又会有新的见解！** 如果你不认同部分观点或者还有其他的优秀理念，可以给我留言。我们一起成长！

作者：Bug辉
本文是我的原创文章。独立博客链接：https://www.bughui.com/2017/08/21/how-to-write-code/
版权声明：《如何写代码 —— 编程内功心法》 由 Bug辉 采用 知识共享 署名-非商业性使用-禁止演绎 4.0 国际 许可协议 进行许可。转载或引用文章时请注明原作者并带上原文链接。

# 【NO.149】性能优化知多少

## 1. 引言

最近一段时间，系统新版本要发布，在beta客户测试期间，暴露了很多问题，除了一些业务和异常问题外，其他都集中在性能上。有幸接触到这些性能调优的机会，当然要学习总结了。

性能优化是一个老生常谈的问题了，典型的性能问题如页面响应慢、接口超时，服务器负载高、并发数低，数据库频繁死锁等。而造成性能问题又有很多种，比如磁盘I/O、内存、网络、算法、大数据量等等。我们可以大致把性能问题分为四个层次：代码层次、数据库层次、算法层次、架构层次。
所以下面我会结合实际性能优化案例，和大家分享下性能调优的工具、方法和技巧。

## 2. 先说心态

说到性能问题，你可能首先就想到的是麻烦或者头大，因为一般性能问题都比较紧急，轻则影响客户体验，重则宕机导致财务损失，而且性能问题比较隐蔽，不易发现。因此一时间无从下手，而这时我们就很容易从心底开始去排斥它，不愿接这烫手的山芋。

而恰巧，性能调优是体现程序员水平的一个重要指标。

> 因为处理bug、崩溃、调优、入侵等突发事件比编程本身更能体现平庸程序员与理想程序员的差距。当面对一个未知的问题时，如何定位复杂条件下的核心问题、如何抽丝剥茧地分析问题的潜在原因、如何排除干扰还原一个最小的可验证场景、如何抓住关键数据验证自己的猜测与实验，都是体现程序员思考力的最好场景。是的，在衡量理想程序员的标准上，思考力比经验更加重要。

所以，若你不甘平庸，请拥抱性能调优的每一个机会。当你拥有一个正确的心态，你所面对的性能问题就已经解决了一半。

## 3. 再说技巧

拿到一个性能问题，不要忙着先上工具，先了解问题出现的背景，问题的严重程度。然后大致根据自己的经验积累作出预估。比如客户来了个性能问题说系统宕机了，已经造成资金损失了。这种涉及到钱的问题，大家都比较敏感，根据自己的level，决定是否要接这个锅。这不是逃避，而是自知之明。

了解问题背景之后，下一步就来尝试问题重现。如果在测试环境能够重现，那这种问题就很好跟踪分析。如果问题不能稳定重现或仅能在生产环境重现，那问题就相对比较棘手，这时要立刻收集现场证据，包括但不限于抓dump、收集应用程序以及系统日志、关注CPU内存情况、数据库备份等等，之后不妨再尝试重现，比如恢复客户数据库到测试环境重现。

不管问题能否重现，下一步，我们就要大致对问题进行分类，是代码层次的业务逻辑问题还是数据库层次的操作耗时问题，又或是系统架构的吞吐量问题。那如何确定呢？而我倾向于先从数据库动手。我的习惯做法是，使用数据库监控工具，先跟踪下Sql耗时情况。如果监控到耗时较长的SQL语句，那基本上就是数据库层次的问题，否则就是代码层次。若为代码层次，再研究完代码后，再细化为算法或架构层次问题。

确定问题种类后，是时候上工具来精准定位问题点了：

- Sql耗时问题，推荐使用免费的[Plan Explorer ](https://www.sentryone.com/plan-explorer)分析执行计划。
- 代码问题定位，优先推荐使用VS自带的Performance Analysis，其次是RedGate的性能分析套件[.NET Developer Bundle](http://www.red-gate.com/products/dotnet-development/dotnet-developer-bundle/)；然后还有Jet Brains的[dotTrace -- .NET performance profiler](http://www.jetbrains.com/profiler/?fromMenu)，[dotMemory-- .NET memory profiler](http://www.jetbrains.com/dotmemory/?fromMenu)；再然后就是反人类的Windbg；等等。

精准定位问题点后，就是着手优化了。相信到这一步，就是优化策略的选择了，这里就不展开了。

优化后，最后当然要进行测试了，毕竟优化了多少，我们也要做到心里有谱才行。

以上啰啰嗦嗦有点多，下面我们直接上案例。

## 4. 案例分享

下面就分享下我针对代码层面、数据库层面和算法层面的优化案例。

### 4.1. SQL优化案例

> 案例1：客户反馈某结算报表统计十天内的数据耗时10mins左右。

由于前几天刚学会用RedGate的分析工具，拿到这个问题，本地尝试重现后，就直接想使用工具分析。然而，这工具在使用webdev模式起站点时，总是报错，而当时时一根筋，老是想解决这个工具的报错问题。结果，白白搞了半天也没搞定。最后不得已放弃工具，转而选择使用sql server profiler去监控sql语句耗时。一跟踪不要紧，问题就直接暴露了，整个全屏的重复sql语句，如下图。

![Sql Profiler监控结果](https://upload-images.jianshu.io/upload_images/2799767-f2779b1cef7ffb3f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

这下问题就很明显了，八成是代码在循环拼接sql执行语句。根据抓取到sql关键字往代码中去搜索，果然如此。

```csharp
#region更新三张表数据结合的中间临时表数据，有上游单据的直接调拨单分多次下推时，只计算一次的调拨数量和价税合计
string sSql = string.Format(@
"SELECT FENTRYID FROM {0} GROUP BY FENTRYID HAVING COUNT(FENTRYID) > 1", sJoinDataTempTable);
using(IDataReader reader = DBUtils.ExecuteReader(this.Context, sSql)) {
    while (reader.Read()) {
        sbSql.AppendFormat(@"
UPDATE {0} SET FDIRECTQTY = 0,FALLAMOUNT = 0 
WHERE FSEQ NOT IN (
SELECT TOP 1 FSEQ FROM {0} WHERE FENTRYID = {1}) AND FENTRYID = ({1});"
, sJoinDataTempTable, Convert.ToInt32(reader["FENTRYID"]));
        listSqlObj.Add(new SqlObject(sbSql.ToString(), new List < SqlParam > ()));
        sbSql.Clear();
    }
}
#endregion
```

看到这段代码，咱先不评判这段代码的优劣，因为毕竟代码注释清晰，省了我们理清业务的功夫。这段sql主要是想做去重处理，很显然选用了错误的方案。改后代码如下：

```perl
string sqlMerge = string.Format(@"
merge into {0} t1
using(
select min(Fseq) fseq,Fentryid from {0} t2 group by fentryid
) t3 on (t1.fentryid = t3.fentryid and t1.fseq <> t3.fseq)
when matched then
update set t1.FDIRECTQTY = 0, t1.FALLAMOUNT = 0
", sJoinDataTempTable);

listSqlObj.Add(new SqlObject(sqlMerge, new List < SqlParam > ()));
sbSql.Clear();
```

改后测试相同数据量，耗时由10mins降到10s左右。

### 4.2. 代码优化案例

> 案例2：客户反馈销售订单100条分录行，保存进行可发量校验时，耗时7mins左右。

拿到这个问题后，本地重现后，监控sql耗时没有异常，那就着重分析代码了。因为可发量校验的业务逻辑极其复杂，又加上又直接再一个类文件实现该功能，3500+行的代码，加上零星注释，真是让人避之不及。逃避不是办法，还是上工具分析一把。
这次我选用的时VS自带的**[Performance Profiler](https://msdn.microsoft.com/en-us/library/ms182372.aspx)**，开发环境下极其强大的性能调优工具。针对我们当前案例，我们仅需要跟踪指定服务对应的dll即可，使用步骤如下：

1. Analyze-->Profiler-->New Performance Session
2. 打开Performance Explorer
3. 找到新添加的Performance Session，右键Targets，然后选择Add Target Binary，添加要跟踪的dll文件即可
4. 将应用跑起来
5. 选中Performance Session，右键Attach对应进程即可跟踪分析性能了
6. 在跟踪过程中，可随时暂停跟踪和停止跟踪

![图示步骤](https://upload-images.jianshu.io/upload_images/2799767-581bab6651172c90.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

跟踪结束后本案例跟踪到的采样结果如下图：

![VS Performance Profiler分析报告](https://upload-images.jianshu.io/upload_images/2799767-9bd074f58fbe605e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

同时Performance Profiler也给出了问题的建议，如下图：
![VS Performance Profiler分析提示](https://upload-images.jianshu.io/upload_images/2799767-5e091191b829b7da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中第1、4条大致说明程序I/O消耗大，第一代的GC上存在未及时释放的垃圾占比过高。而根据上图的采样结果，我们可以直接看出是由于再代码中频繁操作DataTable引起的性能瓶颈。走读代码发现的确如此，所有的数量统计都是在代码中循环遍历DataTable进行处理的。而最终的优化策略，就相当于一次大的重构，将所有代码中通过遍历DataTable的计算逻辑全部挪到SQL中去做。由于代码过多，就不再放出。

> 案例3：客户反馈批量引入1000张订单，耗时40mins左右，且容易中断。

同样，我们还是先尝试本地重写。经测试批量引入101张单据，就耗时5mins左右。下一步打开Sql监控工具也未发现耗时语句。但考虑到是批量导入操作，虽然单个耗时不多，但乘以100这个基数，就明显了。下面我们就使用RedGate的[Ants Performance Profiler](http://www.red-gate.com/products/dotnet-development/ants-performance-profiler/)跟踪一下。

该工具比较直观，可以同时监控代码和SQL执行情况。第一步，New Profiler Session，第二步进行设置，如下图。根据自己的应用程序类别，选择相应的跟踪方式。

![跟踪设置](https://upload-images.jianshu.io/upload_images/2799767-6401487a7995b6dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

针对这个问题，我们跟踪到的调用堆栈和SQL耗时结果如下图：

![调用堆栈监控结果](https://upload-images.jianshu.io/upload_images/2799767-aa1da6373d8fdfc3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![SQL监控结果](https://upload-images.jianshu.io/upload_images/2799767-93679e720a28eeb3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

首先从调用堆栈中的Hit Count，我们可以首先看出它是一个批量过程，因为入口函数仅调用一次；第二个我们可以代码中是循环处理每一个单据，因为Hit Count与我们批量引入的单据数量相符；第三个，突然来了个10201，如果有一定的数字敏感性的话，这次性能问题的原因就被你找到了。这里就不卖关子了，101 x 101 = 10201。
是不是明白了什么，存在循环嵌套循环的情况。我们走读代码确定一下：

```csharp
//Save.cs
public override void EndOperationTransaction(EndOperationTransactionArgs e) {
    //省略其他代码
    foreach(DynamicObject dyItem in e.DataEntitys) {
        //反写收款单
        WriteBackReceiveBill wb = new WriteBackReceiveBill();
        wb.WriteBackForSave(e, this.Context);
    }
}

//WriteBackReceiveBill .cs
public void WriteBackForSave(EndOperationTransactionArgs e, Context contx) {
    //省略其他代码：
    foreach(DynamicObject item in e.DataEntitys) {
        //do something 
    }
}
```

好嘛，外层套了一个空循环却什么也没做。修改就很简单了，删除无效外层循环即可。

### 4.3. 算法优化案例

> 案例4：某全流程跟踪报表超时。

这个报表是用来跟踪所有单据从下单到出库的业务流程数据流转情况。而所有的流程数据都是按照树形结果存储在数据库表中的，类似这样：

![流程树表](https://upload-images.jianshu.io/upload_images/2799767-b0c94713af838e01.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

图中的流程为：
销售合同-->销售订单-->发货通知单-->销售出库单

为了构造流程图，之前的处理方法是把流程数据取回来，通过代码构造流程图。这也就是性能差的原因。

而针对这种情况，就是考验我们平时经验积累了。对于树形结构的表，我们也是可以通过SQL来进行直接查询的，这就要用到了SQL Server的CTE语法来进行递归查询。关于递归查询，可参考我这篇文章：[SQL递归查询知多少](http://www.jianshu.com/p/ab9d268aa54c)。这里就不展开了。

## 5.总结

性能调优是一个循序渐进的过程，不可能一蹴而就，重在平时的点滴积累。关于工具的选择和使用，本文并未展开，也希望读者也不要纠结与此。当你真正想解决一个问题的时候，相信工具的使用是难不住你的。

最后就大致总结下我的调优思路：

1. 调整心态，积极应对
2. 了解性能背景， 收集证据， 尝试重现
3. 问题分类，先监控SQL耗时，大致确定是SQL或是代码层次原因
4. 使用性能分析工具，确定问题点
5. 调优测试

原文作者：[『___知多少』](https://www.cnblogs.com/sheng-jie/)

原文链接：https://www.cnblogs.com/sheng-jie/p/7109385.html

# 【NO.150】Linux 江湖系列阶段性总结

## 1.引言

我使用 Linux 已经有很多年了，最开始接触 Linux 的时候是从 RedHat 9（没有 Enterprise），中途换过 N 个不同的发行版。多年前，我在 BlogJava 上面分享 Java 经验的时候，也偶尔提一提 Linux，如怎么在 Linux 系统上面安装 JDK、Tomcat、Bugzilla 等内容，也写了一些自己选择 Linux 发行版的体会。但是后来我发现，我的体会有一些是错误的，比如我因为字体的原因喜欢中科红旗 Linux，而对 RedHat Enterprise Linux 有一些不公正的评价。于是，我整理 BlogJava 上的博客时，将一些和 Linux 有关的我认为内容有错误的随笔删除了。这次删除直接导致我在 BlogJava 上的排名从 30 几名下降到 50 几名，又经过一年多的经营，现在也只是勉强进入 50 名之内。

完全使用 Linux 作为自己的工作环境有很多困难，被 Linux 折磨久了我就有了一些自己的方法。使用这些方法来解决 Linux 中碰到的问题可以说是屡试不爽。因此，从去年 4 月底我完成硕士论文答辩后，就开始将自己折腾 Linux 的心得整理成文，到目前，正好历时一年。所以，我在这里对这一系列随笔做一下整理。



## 2.使用 Linux 的一些困难和解决方法

完全使用 Linux 进行工作和娱乐是非常困难的。所以有很多 Linux 反对阵营的人就直接说，你们那些所谓的 Linux 用户，也不过是把它当成玩具而已，玩一玩再删掉，有什么意义呢？这真的是一个问题，对于普通用户来说，使用 Linux 的难度还是太大了，即使有很先进的图形界面（如Gnome、KDE）也不行。就算是程序猿们，使用 Linux 也要经过很多波折，而且需要很大的耐心，才能最终把 Linux 从玩具变成神器。大家没有把 Linux 当成日常使用的工具，并不是 Linux 系统或者 Linux 系统下的软件不能够胜任我们的工作，而是被使用 Linux 的困难吓倒了。

举例说明，由于 Linux 是一个很“优秀”的多任务多用户操作系统，“优秀”就意味着严格，在 Linux 系统中由于用户权限的限制，有些事情干起来就不是那么方便。所以，使用 Linux 系统的第一步就是要了解 Linux 系统的用户管理方面的知识。第二步需要了解的是 Linux 的文件管理，虽然没有像 Windows 中的 C 盘 D 盘这样的盘符概念，但是文件和目录以树形组织起来还是比较容易理解的，难就难在 Linux 系统中，每一个文件或目录都有其所属的用户、用户组，并且具有不同的权限位，所以要学习 Linux 系统还要学习它的文件管理。第三步要了解的就是 Linux 系统的任务管理，怎么运行一个程序，怎么让程序在后台执行，怎么向一个程序发送信号等等。可以这么说，Linux 绝对是一个面向专家的系统，对普通用户，它有着天然的壁垒，图形界面做得再好，也无法跨越普通用户和专家之间的鸿沟。

再来说说图形界面。Linux 有非常多的桌面环境可以使用，如 Gnome、KDE、Xface 等等，但是这些桌面环境也不是完全没有问题。什么界面不好看、字体不美观、没有相应的显卡驱动等等问题不一而足。而且这些桌面系统也并不是永远那么稳定，极有可能哪天你安装一个软件包或者卸载一个软件包后，图形界面就进不去了，如果你不会在字符界面下配置网络，无法重新修复相应的软件包，就只剩下重装系统这一条路了。

但是话说回来，博客园中的用户都是程序员，是不会被我上面说的那一点点困难吓倒的。即使有问题，我们也要征服它。我在刚使用 Linux 的时候，碰到问题总是不知道到哪里找答案，有些东西书上没有，网络上也难以查到，查到的信息有时候也很过时。还有时候，我按照书上或网络上的方法下载安装一个软件包，却不知道这个软件包的文件都放到文件系统的哪个地方了，程序在哪里、文档在哪里、配置文件又在哪里呢？经过多年的使用，我总结了[《玩转Linux系统的方法论》](http://www.cnblogs.com/youxia/p/linux001.html)，按照这些方法论，Linux 中的问题基本上都可以迎刃而解。

下一个问题，就是 Linux 下的默认图形界面不够美观。如果一个系统看起来不顺眼，估计大家是没有心情继续用下去的。Fedora 默认主题就很丑，Ubuntu 倒是做得不错。我拿到 Fedora 系统后，第一件事就是换主题、换背景。Linux 系统下的字体配置也很成问题，新安装的系统上网、办公都很不舒服。因此，我总结了[《Linux桌面系统字体配置要略》](http://www.cnblogs.com/youxia/p/linux004.html)、[《在Ubuntu 14.10中借用Windows的字体》](http://www.cnblogs.com/youxia/p/linux005.html)、[《桌面美化那点事儿》](http://www.cnblogs.com/youxia/p/linux012.html)，一步一步将 Linux 打造成自己顺手顺眼的工作环境。有时候，我们还要切换到字符界面进行工作，所以我总结了[《全网络最正确的让Linux开机进入字符界面的方法及设置FrameBuffer分辨率的方法》](http://www.cnblogs.com/youxia/p/linux025.html)，这也是属于让 Linux 看起来更顺眼更舒服的范畴。

Linux 系统的图形界面是基于 X Window 的，所以对 X Window 也要有一定的了解。我又花时间探讨了[《X Window的奥秘》](http://www.cnblogs.com/youxia/p/linux003.html)。为了防止图形系统被玩坏了无法恢复，我研究了[《我该如何备份系统》](http://www.cnblogs.com/youxia/p/linux013.html)，不过在实际应用中，因为我总是不断更新系统，所以我只备份了我的一些配置文件和工作文件。然后，对于安装显卡驱动，我也有自己的总结[《Fedora 21安装Nvidia驱动以及失败后的补救方法》](http://www.cnblogs.com/youxia/p/linux024.html)。

在 Linux 系统下工作，需要一个好的输入法和编辑器吧。我在 Fedora 中使用 Intelligent PinYin 输入法，这个输入法已经很不错了，在 Ubuntu 中有更好用的搜狗输入法，当然不能错过，请看这里[《在Ubuntu 14中使用搜狗拼音输入法》](http://www.cnblogs.com/youxia/p/linux009.html)。然后，对于文本编辑器，我选择 Vim。Vim 要配到让自己顺手才行，所以有了这篇[《打造属于自己的Vim》](http://www.cnblogs.com/youxia/p/linux002.html)。

然后就是编程了，C 语言是 Linux 系统的母语，所以完善的 C 语言编程环境自然是必不可少。由于 C 语言相当简单，很多时候使用 Vim 就足够了，于是就有了[《感悟GNU C以及将Vim打造成C/C++的半自动化IDE》](http://www.cnblogs.com/youxia/p/linux006.html)。有一段时间，我非常热衷于编写自己的操作系统内核，自制操作系统需要从 16 位的实模式写起，然后再让 CPU 进入保护模式。几乎所有的书和网络教程一提到 16 位实模式代码，都使用 NASM 编译器。我偏偏喜欢 GCC，为了不使用 NASM，我有了这一篇[《使用GCC和GNU Binutils编写能在x86实模式运行的16位代码》](http://www.cnblogs.com/youxia/p/linux008.html)。这也进一步说明，使用 C 语言编程，除了 GCC，Binutils 也是一个很重要的工具包。除此之外，Gnu autotools 也是很重要的工具包，我在这一篇[《使用Eclipse和Gnu Autotools管理C/C++项目》](http://www.cnblogs.com/youxia/p/linux023.html)中进行了展示。

除了 C 语言，脚本语言在 Linux 中的地位也很重要。这一篇[《Bash脚本编程语言中的美学与哲学》](http://www.cnblogs.com/youxia/p/linux010.html)总结了我对 Bash 语言的领悟。另外，从我的随笔分类中可以看出，我有时是需要写一些和数学、数值计算有关的东西的，要写这些东西，必须要有适当的工具，关于工具的内容我后面再总结，但是这一篇[《适合数值计算的语言需要具备什么样的特色》](http://www.cnblogs.com/youxia/p/linux017.html)，要归纳到我对编程语言的理解这个类别中。

然后就是和写文章、发博客、绘图、数学公式有关的一些工具了。它们是[《优秀的文本化编辑思想大碰撞（Markdown、LaTeX、MathJax）》](http://www.cnblogs.com/youxia/p/linux014.html)、[《再来说说LaTeX》](http://www.cnblogs.com/youxia/p/linux015.html)、[《数值计算和符号计算》](http://www.cnblogs.com/youxia/p/linux016.html)，以及前面提到的[《适合数值计算的语言需要具备什么样的特色》](http://www.cnblogs.com/youxia/p/linux017.html)。当然，还有画图工具[《发博客必备的五大图片处理神器》](http://www.cnblogs.com/youxia/p/linux011.html)。

最后，目前的虚拟化技术如火如荼，再加上我折腾 Linux 系统有时也需要用到虚拟机，所以对 Linux 系统下的几个主流虚拟机进行了探讨。它们是[《虚拟机体验之QEMU篇》](http://www.cnblogs.com/youxia/p/linux019.html)、[《虚拟机体验之KVM篇》](http://www.cnblogs.com/youxia/p/linux020.html)、[《虚拟机体验之VirtualBox篇——性能强大的经典架构》](http://www.cnblogs.com/youxia/p/linux021.html)、[《虚拟机体验之Xen篇——令人脑洞大开的奇异架构》](http://www.cnblogs.com/youxia/p/linux022.html)。有时不想用虚拟机，就想办法把不同的 Linux 发行版安装到一台电脑上，像这样[《在同一个硬盘上安装多个Linux发行版及Fedora 21初体验》](http://www.cnblogs.com/youxia/p/linux018.html)。



## 3.我眼中的 Linux 哲学总纲

《Unix 编程艺术》这本书中，对 Unix 编程哲学的总纲总结为“K.I.S.S”，即Keep it simple, stupid。对这个说法，我深深地不以为然。当然，你可以认为这是 Unix 环境编程的哲学，而不是类 Unix 系统本身的哲学。事实上，一个系统中的程序如何设计，往往也影响到这个系统如何使用。例如，Linux 系统中的程序设计时往往只让自己做到最少的功能，并通过管道和其它程序配合使用，从编程的哲学来讲，这就叫只做一件事并做好，从用户的角度来讲，他就需要学习很多小工具，并将这些工具配合起来使用。正是由于将各种工具搭配使用可以有各种各样的组合方式，充满了创造性，所以，也就充满了乐趣。另外，每一个小工具都是做得那么精湛，因此它们的生命期会很长，从用户的角度来讲，就是学会一项技能以后，很长一段时间不会过时。另外一个例子就是 Linux 程序设计时的最小立异原则，它们几乎遵循同样的命令行参数、配置文件语法、正则表达式语法等等，从用户的角度来看，这个系统就会显得很整齐划一，有时候不用看文档，猜都能猜到某些工具怎么使用。

既然我不赞同 Linux 的哲学是“K.I.S.S”，那么我的答案是什么呢？我认为，Linux 哲学的总体纲领是“交流和共享”。这要从 Unix 和 Linux 以及和 Gnu 的历史说起。Unix 刚诞生的时候，是不允许当作商业产品出售的，为了交流，Unix 系统的源码是完全公开的，很多人都向 AT&T 实验室索要 Unix 的源码。那时候，Ken 伯伯是一包一包的磁盘往外寄，但是他毫无怨言，还在每一个包裹里附上一张纸条，上写“爱你的Ken”。交流和共享的回报就是全世界那么多的聪明人为 Unix 系统添砖加瓦，然后 Unix 系统飞速发展。至于 Unix 版本的分裂和最后标准的形成这样的过程我就不写了。再来说 Linux，其内核诞生之初便放到网络上和大家交流，然后借助网络协作飞速发展。Gnu 就更加不用说了，那是开源界的鼻祖，交流和共享是它的立身之本。我觉得“K.I.S.S”这个原则应该是有点过时了，如果按人年计算，Linux 的工作量已超过几千万人年，它一点也不 Simple，一点也不 Stupid。相反，它还有那么一点 Complex，而且还很 Brilliant，它的复杂会把某些人拒之门外，它闪光的思想又会把另外一些人紧紧吸引。（这里不仅是指 Linux 内核，而是指整个 Linux 系统，包括其中的各种软件。）

其它的哲学应该都是从“交流和共享”这个总纲延伸而来的。其中一个结果就是在 Linux 系统下有着海量的软件、编程工具可以使用。因为每个人都可以向 Linux 贡献自己的东西嘛。而且 Linux 也是非常鼓励大家开发自己行业领域的专业软件，鼓励大家设计适用于各种场景的专用编程语言。所以使用 Linux，你一定会有一种感觉，那就是一座大大的宝藏就放在自己眼前，入宝山怎么可能空手而归呢？另外一个结果就是在 Linux 系统中展现了很多非常闪光的思想，因为交流，那些不好的东西慢慢就被淘汰了，留下的，自然就是精粹。



## 4.我这一系列随笔中展现出的 Linux 哲学

在我的这一个系列中，首先揭示的 Linux 哲学就是它的透明性和可用性。《Linux 就是这个范儿》这本书里面说 Linux 有四大笨，分别是：万般皆文本，随处用脚本，规律无处寻，配置乱生根。我认为，这“四大笨”正是 Linux 透明性的体现，但是要改一个字，不是“规律无处寻”，而是“规律有处寻”。下面我来一一论证。

首先来说万般皆文本。在 Linux 系统中，不管是保存数据还是程序之间通讯，首选纯文本格式。一提到纯文本格式，大家首先想到的困难会有两点，其一是占用的存储空间比较大，其二是文本的解析比较困难。但是和二进制的存储格式相比，纯文本最大的优点就是适合人类阅读。在 Linux 系统中，很多程序都是基于行来解析文本文件的，甚至包括那些版本控制工具，它们也是基于行的比较来体现代码的变化。富文本格式往往也倾向于使用纯文本的方式来进行编辑和保存，只是需要在进行显示或打印的时候，再通过特定的工具转化成多媒体格式。在我的这一系列随笔中，这两篇[《优秀的文本化编辑思想大碰撞（Markdown、LaTeX、MathJax）》](http://www.cnblogs.com/youxia/p/linux014.html)、[《再来说说LaTeX》](http://www.cnblogs.com/youxia/p/linux015.html)可以说是对这一个哲学进行了阐释。另外，进程间通讯也是纯文本占优势，因为那些专用的基于二进制的协议实在是太不透明了，开发和调试都很困难，也不是很健壮。以前在 Windows 下编程时，COM 和 DCOM 都研究过，写 Java 的时候，也在 RMI 方面下过功夫，现在，仍然有很多编程语言在搞对象的序列化和反序列化，这些都是基于二进制的，不透明。后来逐渐接触到 JSON、XML，以及面向服务编程是用到的 SOAP 协议，才深深体会到纯文本格式的优美。其实目前网络上的用户层通讯协议，比如 HTTP 协议之类的，都是基于文本行的。

其次来说说随处用脚本。关于脚本编程我这里不多讲，这里只说脚本带来的好处。脚本的坏处大家也可以马上想到，那就是运行速度确实不怎么样。但是脚本的好处是显而易见的，那就是逻辑清晰、便于阅读。脚本就是一个纯文本，随便找个编辑器打开就可以阅读，如果有超级用户权限也可以随意修改。在我的这个系列中，[《Bash脚本编程语言中的美学与哲学》](http://www.cnblogs.com/youxia/p/linux010.html)总结了我对 Bash 语言的领悟。而且有很多时候，我是通过阅读脚本来理解问题和解决问题的，如这一篇[《全网络最正确的让Linux开机进入字符界面的方法及设置FrameBuffer分辨率的方法》](http://www.cnblogs.com/youxia/p/linux025.html)，通过阅读 lightdm 的启动脚本来找出让 Ubuntu 进入字符界面的方法，还有这一篇[《探索Linux系统的启动过程》](http://www.cnblogs.com/youxia/p/linux026.html)，通过阅读 lsinitrd 脚本来找到解包 initramfs 文件的方法。

然后来看规律有处寻。对于 Linux 新手来说，有时确实有点摸不着头脑，感觉似乎找不到任何规律。事实上，Linux 下的软件的文档是非常完善的。几乎每一个程序都有相应的手册页，使用`man`命令就可以阅读，如果是 Gnu 出品的，可能还有`info`文档。另外，几乎每一个程序都可以带`--help`选项。例如在[《Fedora 21安装Nvidia驱动以及失败后的补救方法》](http://www.cnblogs.com/youxia/p/linux024.html)这一篇中，我通过带`--help`选项运行从 NVIDIA 官网下载的安装文件，就得到了怎么把该安装文件解包的方法。除了手册页，还有很多软件提供的文档是 PDF 格式的，也有办法把它们找出来，如[《再来说说LaTeX》](http://www.cnblogs.com/youxia/p/linux015.html)这一篇和[《虚拟机体验之VirtualBox篇——性能强大的经典架构》](http://www.cnblogs.com/youxia/p/linux021.html)这一篇，找出文档后，软件的使用就不再是无规律可循了。

最后来分析配置乱生根的问题。配置文件、环境变量、命令行参数是我们定制软件行为的三驾马车。Linux 中有一个很讨厌的哲学——提供机制而不是策略，为什么说它讨厌呢，我后面再讲。在 Linux 中，一个软件可以有很多选项，可以表现出各种各样的行为，这些都要靠配置文件、环境变量、命令行参数进行定制。配置文件本身是纯文本格式的，阅读和修改都不是问题，其透明性是很好的，主要问题是“乱生根”，有时候根本不知道配置文件放在什么地方、文件名叫什么。这个问题也不是没办法解决，使用[《玩转Linux系统的方法论》](http://www.cnblogs.com/youxia/p/linux001.html)中介绍的方法，很容易找到某个软件包的配置文件都放在什么地方。

其实和透明性相关的还有一个硬货，那就是源代码。Linux 可以说是开源软件的代表，内核及其系统下的软件都是开源的，源代码可以随意获得和阅读。但是能随意阅读源代码就说明透明性一定好吗？那不见得，因为源代码太复杂了，就算让你随便读也不一定读得懂。所以我碰到问题后，首先考虑的是读文档，而不是读代码。Linux 内核的源码目录我也是经常逛，但是主要还是以读文档居多，基本没读代码，最多也就是看了那基本经典的专著之后进代码瞅一眼，验证验证而已。有时候源代码也可以提供一些参考，比如 OpenGL 编程，不知道它和 X Window 如何交互的话，看一看 freeglut 的代码还是很有收获的。在[《玩转Linux系统的方法论》](http://www.cnblogs.com/youxia/p/linux001.html)这一篇中，我有介绍获取源码包的方法。

我这一个系列的随笔可以说是用最好的方式展现了 Linux 的透明性。掌握了我这一系列博客中提到的方法，在 Linux 系统中碰到任何困难，都可以找到解决方法。换一种说法，其实 Linux 系统本来对用户就是非常透明的，一点都没有遮遮掩掩，如果你用不好 Linux，那是因为你水平还不够，而我的这一系列随笔就是教大家怎样去看穿它。

说完透明性再来说说可用性。通过前面的阐释可以看出 Linux 系统是非常透明的，所有困难都可以克服。另外一个问题就是 Linux 真的可以满足我们日常工作、娱乐的所有要求吗？也就是说，Linux 真的具有可用性吗？我觉得我的这一系列随笔就是对这个问题最好的回答。在这一系列随笔中，我展示了在 Linux 桌面环境中，我的日常工作是没有问题的，不管是编程、画图还是做数学工作，很多软件都非常优秀，功能强大、运行稳定且界面美观。在我没有展示的领域，如写论文、做幻灯也是没有问题的，LibreOffice 很强大，和 MS Office 一样好用，上淘宝、登网银也没有问题，因为它们的插件都支持 Linux 系统下的 Firefox 浏览器。如果不要图形界面，其在服务器领域占有的市场比例更是遥遥领先，这个可用性就不需要我来废话了，大家心知肚明。

在我的这一系列随笔中，有几篇还展示了 Linux 中某些软件前后端分离的策略以及其带来的优势。使用 Linux 系统有两种方式，那就是 CLI 和 GUI。你即可以选择字符界面，也可以选择图形界面，它们各有优势。字符界面通过输入命令来执行程序，通过脚本和管道让许多工具配合工作，如果用得好，可以获得非常高的效率。而且字符界面的程序非常适合进行脚本化和自动化，如开机启动某任务、定时启动某任务等等。图形界面的优势呢？就是使用起来非常方便，有丰富的菜单提示，用鼠标点点画画就可以完成工作，但是要完成批量任务或定时任务就不是那么方便了。也不是没有解决办法，MS Office 提供的宏就是一种解决办法，Photoshop 也可以录制动作生成 Action 然后回放。这说明不管是在 Linux 环境还是在非 Linux 环境，用户对自动化的需求都是一样的。

有些工作非使用图形界面不可，比如画图和在虚拟机中运行具有图形界面的客户机；有另外一些工作则把图形界面当成累赘，比如在没有图形界面的服务器中使用虚拟机，并且要有开机自动启动、定时启动、远程管理、批量化管理这些功能。要解决这个冲突，Linux 中广泛采用的是前后端分离的策略。在[《数值计算和符号计算》](http://www.cnblogs.com/youxia/p/linux016.html)中，我介绍的 Octave、Maxima 等软件，就都有在其命令行的版本上，加上了一个 GUI 封装的图形化版本。还有[《虚拟机体验之VirtualBox篇——性能强大的经典架构》](http://www.cnblogs.com/youxia/p/linux021.html)中介绍的 VirtualBox 虚拟机，就是一个经典的前后端分离架构，VirtualBox 的图形界面就是对其命令行工具的 GUI 封装。



## 5.Linux 之得和 Linux 之失

Linux 还有一个很讨厌的哲学，那就是提供机制而不提供策略，而且这个策略还被很多人追捧，对于这一点，我是持反对意见的。提供机制而不提供策略的几个典型例子，一是系统启动时的初始化，Linux 只决定内核初始化完成后将控制权交给`/init`程序，至于`/init`程序怎么启动其它的服务、怎么进入系统，就看各个发行版各显神通了。二是 X Window 图形界面的实现，只提供机制，不提供策略，在不同的发行版中，可以使用不同的窗口管理器，编程也可以使用不同的图形工具包。

那么这是优点还是缺点呢？不好说。支持者认为这是优点，因为可以随时更换策略，所以这些机制的生命期特别的长。就拿 X Window 来说，已经有 30 多年的历史了，但是依然没有被淘汰，每次只要换个窗口管理器，又可以生龙活虎好多年。支持者的另外一个观点就是只提供机制而不提供策略，可以给用户提供更加丰富的定制空间，正如 X Window，可供用户选择的桌面环境就有 Gnome、KDE、XFace、Enlightment 等等一大堆，还有 Ubuntu，硬是在 Gnome 的基础上再次开发，写出了一个 Unity 桌面。

我认为，它的缺点也很明显。其中一个缺点就是这个策略明显地造成了 Linux 发行版的分化。现在的 Linux 发行版太多了，每一个都搞一套自己的启动机制，每一个都搞一个自己的软件包管理机制，每一个都说自己的策略怎么好怎么好，最终，想选择 Linux 的用户被逼成了选择障碍综合征，不知道怎么选，只好抛弃 Linux。即使是同一个厂家或同一个社区的发行版，也分成不同的桌面定制版，而且有时候，有些软件专为某种桌面定制，在其它的桌面环境中运行效果就很差或者很不稳定，这造成了 Linux 用户的分裂，同时也造成了 Linux 新手学习上的困难，因为技能树的分支太多了，不知道怎么攀。

版本分化还不是 Linux 占领市场最大的问题。最大的问题还是出在提供机制而不是提供策略上，因为当 Linux 允许用户自己指定策略的时候，往往也意味着用户必须得自己指定策略。Linux 系统的发行者没有为大家提供一个可以让大部分人都满意的预先定制好的策略，相反，而是只提供基本可用的系统。几个比较明显的例子，我装完系统后，首先要做的事就是重新配置字体、选择系统主题，对于常用的工具如 Vim 之类的，还要自己修改它的配置文件。最终的结果，就是刚接触 Linux 的用户，如果不是专家或者特别有耐心，很快就知难而退，放弃了。

曾经有一段时间，字体的配置方法也是很分化的，使用不同的图形界面包（如QT、GTK）编写的软件，使用的字体配置方法都不一样，所以要改系统的字体，得改好几个地方的配置文件。这个问题也导致如果一个为 KDE 编写的程序如果运行在 Gnome 桌面上，界面往往会比较丑陋，反之亦然。幸好，正如我在某一篇随笔里说的，字体配置方法被 FontConfig 统一了，这为我们最终用户省了很多事。也证明，Linux 中的提供机制而不是策略的哲学带来的 Linux 的分化还是很有弊端的。还是合而为一比较方便，而且最终也是分久必合，如 init 系统即将被 systemd 统一。



## 6.总结

这一篇随笔只是对我这一年中所写的 Linux 应用环境实战系列随笔的一个总结，顺便阐释了一下从我的随笔中能够看出来的 Linux 的透明性、可用性。这一系列随笔没有多少编程方面的东西，主要展示的是方法论和工具论。最后，如果你早就被 Linux 深深吸引住了，但是又无法说出吸引你的究竟是什么，那么我推荐你读一下《Unix 编程艺术》，这本书虽然很老了，但是读完之后确实是会有巨大的收获。

（京山游侠于2015-04-13发布于博客园，转载不用注明出处，因为这种总结类的文章，是没有人会转载的。）

原文作者：京山游侠

原文链接：https://www.cnblogs.com/youxia/p/discard001.html



# 【NO.151】p2p之网络穿透NAT，NAT、穿透的原理

## 1.p2p是什么？

p2p是对等网络（peer-to-peer networking）其可以定义为：端对端的资源共享，每一端即可是服务端，也可以是客户端。既可以是资源的提供者，也可以是资源的共享者。

传统C/S模型需要实现端和端的资源共享， 需要将资源上传到中转服务器。另外一端再去中转服务器下载，如下图：

![img](https://pic3.zhimg.com/80/v2-b02447c4cd4ead9cb6169e86dfff507e_720w.webp)

传统CS架构，客户端1和客户端2之间是无直接交互.png

而P2P则不需要将资源上传到服务器，它是端对端传输，每一个端既可以是服务器，也可以是客户端

![img](https://pic4.zhimg.com/80/v2-8b4235fc0df3291ebf4bc91396a594fb_720w.webp)

p2p架构，无需中转服务器.png

优势：实时性最高，流量少，更加安全。在视频直播，在线教育，视频安防行业用的比较多
劣势：一旦进行p2p传输之后，用户之间的内容将无法监管，浪费用户带宽，频繁进行读写磁盘

客户端1和客户端2这样交互是p2p最理想的情况
图中客户端1和客户端2直接连接， 假如他们处于两个不同的内网呢?

## 2.NAT是什么？

NAT俗称网络地址转换，它是一种把内部私有网络地址（IP地址）转换成公网网络IP地址的技术。比如我们电脑里面网卡地址是192.168.1.100，但是我们再百度搜索“IP”却显示220.112.224.53，这就是NAT的功能。
**NAT主要是部署在路由器或者交换机上。**

## 3.为什么需要NAT？

主要还是IP地址的不足，使用少量的公有IP 地址代表较多的私有IP 地址的方式，将有助于减缓可用的IP地址空间的枯竭。用大白话：比如你有一个路由器（家用的那种就可以）这个路由器本身连接了公网（被分配到了一个公网的IP地址）。路由器后面有接了N多个设备，每个设备都分配到了一个私有的地址（内网地址），这些地址可以通过这个路由器和外网交互。

其次能够有效地避免来自网络外部的攻击，隐藏并保护网络内部的计算机。
RFC3489 中将 NAT 的实现分为四大类：

- Full Cone NAT(完全圆锥型)
- Address Restricted Cone NAT(地址限制圆锥型 )
- Port Restricted Cone NAT(端口限制圆锥型)
- Symmetric NAT(对称型)

## 4.完全圆锥型NAT

在完全圆锥型NAT（Full Cone NAT）中，NAT会将客户机地址{X:y}转换成公网地址{A:b}并绑定。任何包都可以通过地址{A:b}送到客户主机的{X:y}地址上。如图所示：

![img](https://pic3.zhimg.com/80/v2-b84f4a39e311e786608cf01f7d62cf62_720w.webp)

RFC3581——完全锥型NAT

## 5. 地址限制圆锥型NAT

地址限制圆锥型NAT（Address Restricted Cone NAT）会将客户机地址{X:y}转换成公网地址{A:b}并绑定，只有来自主机{P}的包才能和主机{X:y}通信。如下图所示：

![img](https://pic4.zhimg.com/80/v2-a329e90e007c0a1ce406349b47819c93_720w.webp)

RFC3581——地址限制型NAT

## 6.端口限制圆锥型NAT

端口限制圆锥型NAT(Port Restricted Cone NAT)会将客户机地址{X:y}转换成公网地址{A:b}并绑定，只有来自主机{P,q}的包才能和主机{X:y}通信。如下图所示：

![img](https://pic1.zhimg.com/80/v2-baaea3df8eccf4422cd8c688889e4d04_720w.webp)

RFC3581——端口限制型NAT

## 7.对称型NAT

对称型NAT（Symmetric NAT）会将客户机地址{X:y}转换成公网地址{A:b}并绑定为{X:y}|{A:b}<->{P:q}。对称型NAT只接受来自{P:q}的连接，将它转给{X:y} ，每次客户机请求一个不同的公网地址和端口，NAT会新分配一个端口号{C,d} 。如下图所示：

![img](https://pic4.zhimg.com/80/v2-beb4537f62e9b1897b1ccee19e945747_720w.webp)

RFC3581——对称型NAT

**其中完全最上层的完全圆锥形NAT的穿透性最好，而最下层的对称形NAT的安全性最高。**

## 8.如何穿透NAT？

事实上两个客户端相互通信还需要一个辅助服务器(p2pserver) 来保存两个用户的外网地址端口。
当用户A连接B时、或者B连接A时， 会向辅助服务器询问对方的外网地址和端口

![img](https://pic3.zhimg.com/80/v2-00c8c5a560148ae21565b5f8ee1dc5aa_720w.webp)

NAT穿透组合情况.png

从上面的NAT类型中可以看出，有4种NAT，一共10种组合

**1. 完全圆锥型NAT和完全圆锥型NAT**
这种最简单， 只需要B从辅助服务器拿到A的内外网信息， 就可以和A进行连接
**2. 完全圆锥型NAT和地址限制型NAT**
同上
**3. 完全圆锥型NAT和端口限制性NAT**
同上
**4. 完全圆锥型NAT和对称型NAT**
同上
**5.地址限制型NAT和地址限制型NAT**

- 当B从辅助服务器拿到A的内外网信息， B向A发送连接， 这个时候NAT A设备会丢弃掉B发送过来的连接。
- 这个时候B就向辅助服务器发送请求，让A连接B一次， 连完后B就可以连接到A了，NAT A不再拦截B过来的连接。

**6.地址限制型NAT和端口限制型NAT**
同上
**7.地址限制性NAT和对称型NAT**
同上
**8.端口限制型NAT和端口限制型NAT**
同上
**9.端口限制型NAT和对称型NAT**
这种无法穿透， 因为A需要连过B，B才能连到A，但是A无法连接到B，因为B的是对称型NAT，端口一直在变
**10.对称型NAT和对称型NAT**
这种也无法穿透，因为客户机每次请求一个不同的公网地址和端口， NAT会新分配一个端口号，所以从辅助服务器拿到的端口号是无效的（只是针对和服务器相连的端口号）。
eg:A和辅助服务器相连，NAT A会分配一个端口 8081。
A和B相连， NAT A会分配一个端口号10020，所以B连A并不知道A需要从10020进，所以无法穿透过NAT A。不过也有人通过端口预测算法成功连接， 但是这种并不可靠。

![img](https://pic1.zhimg.com/80/v2-80b87d01775cd205828d9a651ab6b48c_720w.webp)

## 9.为什么需要保活链路？

因为一个连接经过NAT设备之后，在NAT设备上面绑定的端口是有时效性的，一般是30分钟，但是最少的三五分钟就失效了，所以要不停的发送心跳包来保活NAT上的这个“洞”。

## 10.移动、联通网络为什么没有电信快？

原因是电信拨号之后分配的是公网IP。而联通、移动拨号之后还是内网IP，也就是NAT设备上面还有多层NAT， 多次转发并且最终的出口只有一个，所以总体来说比较慢

原文链接：https://zhuanlan.zhihu.com/p/299524798

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.152】同步与异步，回调与协程

**目录**

- 概念上下文：
- 同步的方式：
- 异步加回调的方式：
- 异步协程方式：
- 总结：

**正文**

　　本文主要介绍在网络请求中的同步与异步，以及异步的表现形式： 回调与协程，并通过python代码展示各自的优缺点。

## **1.概念上下文：**

回到顶部

　　当提到同步与异步，大家不免会想到另一组词语：阻塞与非阻塞。通常，同时提到这个这几个词语一般实在讨论network io的时候，在《unix network programming》中有详尽的解释，网络中也有许多讲解生动的文章。

　　本文所讨论的同步与异步，是指对于请求的发起者，是否需要等到请求的结果（同步），还是说请求完毕的时候以某种方式通知请求发起者（异步）。在这个语义环境下，阻塞与非阻塞，是指请求的受理者在处理某个请求的状态，如果在处理这个请求的时候不能做其它事情（请求处理时间不确定），那么称之为阻塞，否则为非阻塞。

　　举个例子，我去柜台办理业务，那我是请求者，柜员时受理者。如果我在柜台一直等着柜员办理，直到办理完毕，那么对于我来说，就是同步的；如果我只是在柜员那里登记，然后到一边歇着，等柜员办理完毕之后告诉我结果，那么就是异步的。对于柜员，办理业务的时候可能需要等待打印机打印，如果在这个时候柜员去处理其他人的业务，那么就是非阻塞的，如果一定得到把我的业务办完再接待下一位顾客，那么就是阻塞的。

　　本文站在请求发起者的角度来思考同步与异步，在实际开发中，一个最简单的例子就是http请求。假设这么一个场景，程序需要访问两个网址（通过url），如果只有一个线程。那么同步与异步分别怎么处理呢

## **2.同步的方式：**

回到顶部

　　python的urllib2提供了http请求的功能，我们来看看代码：

```
1 def http_blockway(url1, url2):
 2     import urllib2 as urllib
 3     import time
 4     begin = time.time()
 5     data1 = urllib.urlopen(url1).read()
 6     data2 = urllib.urlopen(url2).read()
 7     print len(data1), len(data2)
 8     print 'http_blockway cost', time.time() - begin
 9 
10 url_list = [ 'http://xlambda.com/gevent-tutorial/','https://www.bing.com']
11 if __name__ == '__main__':
12     http_blockway(*url_list)
```

运行结果：

　　45706 121483
　　http_blockway cost 2.22000002861

注意：对代码运行的时间应该多次执行求平均值，这里只是简单作为参考。下同。

　　python的urllib是同步的，即当一个请求结束之后才能发起下一个请求，我们知道http请求基于tcp，tcp又需要三次握手建立连接（https的握手会更加复杂），在这个过程中，程序很多时候都在等待IO，CPU空闲，但是又不能做其他事情。但同步模式的优点是比较直观，符合人类的思维习惯: 那就是一件一件的来，干完一件事再开始下一件事。在同步模式下，要想发挥duo多喝CPU的威力，可以使用多进程或者多线程。

## 3.**异步加回调的方式：**

回到顶部

如果发出了请求就立即返回，这个时候程序可以做其他事情，等请求完成了时候通过某种方式告知结果，然后请求者再继续再来处理请求结果，那么我们称之为异步，最常见的就是回调（callback），在python中，tornado提供了异步的http请求。对于上面的场景，我们来看看代码：

```
1 import tornado
 2 from tornado.httpclient import AsyncHTTPClient
 3 import time, sys
 4 
 5 def http_callback_way(url1, url2):
 6     http_client = AsyncHTTPClient()
 7     begin = time.time()
 8     count = [0]
 9     def handle_result(response, url):
10         print('%s : handle_result with url %s' % (time.time(), url))
11         count[0] += 1
12         if count[0] == 2:
13             print 'http_callback_way cost', time.time() - begin
14             sys.exit(0)
15 
16     http_client.fetch(url1,lambda res, u = url1:handle_result(res, u))
17     print('%s here between to request' % time.time())
18     http_client.fetch(url2,lambda res, u = url2:handle_result(res, u))
19 
20 url_list = [ 'http://xlambda.com/gevent-tutorial/','https://www.bing.com']
21 if __name__ == '__main__':
22     http_callback_way(*url_list)
23     tornado.ioloop.IOLoop.instance().start()
```

运行结果：

　　1487292402.45 **here between to reques**t
　　1487292403.09 : handle_result with url [http://xlambda.com/gevent-tutorial/](https://link.zhihu.com/?target=http%3A//xlambda.com/gevent-tutorial/)
　　1487292403.21 : handle_result with url [https://www.bing.com](https://link.zhihu.com/?target=https%3A//www.bing.com)
　　http_callback_way cost 0.759999990463

从代码可以看到，对请求的结果是放在一个额外的函数（handle_result）中进行的，这个处理函数在发出请求的时候注册的，这就是回调。从运行结果可以看到，在发出第一个请求之后就立即返回了（先于请求结果），而且运行时间大为缩短，这就是异步的优势所在：不用在IO上等待，在单核CPU上就有更好的性能。但是callback这种形式，导致代码逻辑因为异步请求分离，不符合人类的思维习惯，因为不直观。再来看一个很常见的例子：请求一个网址A，根据网页的内容来确定是接着访问B还是C，如果使用异步，那代码是这样的： 　　

```
1 import tornado
 2 from tornado.httpclient import AsyncHTTPClient
 3 http_client = AsyncHTTPClient()
 4 def handle_request_final(response):
 5     print('finally we got the result, do sth here')
 6 
 7 def handle_request_first(response, another1, another2):
 8     if response.error or 'some word' in response.body:
 9         target = another1
10     else:
11         target = another2
12     http_client.fetch(target, handle_request_final)
13 
14 def http_callback_way(url, another1, another2):
15     http_client.fetch(url, lambda res, u1 = another1, u2 = another2:handle_request_first(res, u1, u2))
16 
17 url_list = [ 'https://www.baidu.com', 'https://www.google.com','https://www.bing.com']
18 if __name__ == '__main__':
19     http_callback_way(*url_list)
20     tornado.ioloop.IOLoop.instance().start()
```

**代码表达的逻辑是连贯的，但代码的变现形式却是割裂的**，在这里分散到了三个函数里面。对于程序员来说，这样的代码难以阅读，不容易看一眼就大概明白其意图，而且这样的代码是难以维护的，更糟糕的是，编程实现中还得保存或者传递上下文（比如函数中的参数 another1, another2）。

　　在python中，由于lambda的功能还是比较弱，所以回调一般都是另外命令一个函数。而在javascript，特别是以异步IO为基石的NodeJS中，由于匿名函数的强大，很轻易嵌套实现callback，这也就出现了让编码者沉默、维护者流泪的callback hell。当然promise对callback hell有一定的改善，但还有没有更好的办法呢？

## 4.**异步协程方式：**

回到顶部

　　这个时候协程就出马了，本来是异步调用，但是程序上看上去变成了“同步”。关于协程，python中可以用原声的generator，也可以使用更严格的greenlet。首先来看看tornado封装的协程：

```
1 import tornado, sys, time
 2 from tornado.httpclient import AsyncHTTPClient
 3 from tornado import gen
 4 
 5 def http_generator_way(url1, url2):
 6     begin = time.time()
 7     count = [0]
 8     @gen.coroutine
 9     def do_fetch(url):
10         http_client = AsyncHTTPClient()
11         response = yield http_client.fetch(url, raise_error = False)
12         print url, response.error
13         count[0] += 1
14         if count[0] == 2:
15             print 'http_generator_way cost', time.time() - begin
16             sys.exit(0)
17 
18     do_fetch(url1)
19     do_fetch(url2)
20 
21 url_list = [ 'http://xlambda.com/gevent-tutorial/','https://www.bing.com']
22 if __name__ == '__main__':
23     http_generator_way(*url_list)
24     tornado.ioloop.IOLoop.instance().start()
```

运行结果：

　　[http://xlambda.com/gevent-tutorial/](https://link.zhihu.com/?target=http%3A//xlambda.com/gevent-tutorial/) None
　　[https://www.bing.com](https://link.zhihu.com/?target=https%3A//www.bing.com) None
　　http_generator_way cost 1.05999994278

　　从运行结果可以看到，效率还是优于同步的，而代码看起来是顺序执行的，但事实上有某种意义上的并行。代码中使用了decorator 和 yield关键字，在看看使用gevent的代码：

```
1 def http_coroutine_way(url1, url2):
 2     import gevent, time
 3     from gevent import monkey
 4     monkey.patch_all()
 5     begin = time.time()
 6 
 7     def looks_like_block(url):
 8         import urllib2 as urllib
 9         data = urllib.urlopen(url).read()
10         print url, len(data)
11 
12     gevent.joinall([gevent.spawn(looks_like_block, url1), gevent.spawn(looks_like_block, url2)])
13     print('http_coroutine_way cost', time.time() - begin)
14 
15 url_list = [ 'http://xlambda.com/gevent-tutorial/','https://www.bing.com']
16 if __name__ == '__main__':
17     http_coroutine_way(*url_list)
```

　　代码中没有特殊的关键字，使用的API也是跟同步方式一样的，这就是gevent的牛逼之处，通过monkey_patch就是原来的同步API变成异步，gevent的介绍可以参见《gevent调度流程解析》。

　　协程与回调对比，优势一目了然：代码更清晰直观，更加复合思维习惯。但协程也不是银弹，首先，协程是个新概念，需要花时间去理解；其次，程序员心里也得牢牢记住，在看似在一起的两句代码中间可能插入了很大其它逻辑（即使是在单线程）。但总体来说，协程给出了人们解决问题的新思路，利大于弊，在其他语言（C# Lua golang）中也有支持，还是值得程序员去了解和学习

## 5.**总结：**

回到顶部

　　最后，简单总结一下，同步比较直观，编码和维护都比较容易，但是效率低，绝大多数时间都是不能忍的。异步效率高，对于callback这种形式逻辑容易被代码割裂，代码不直观，而异步协程的方式既看起来直观，又在效率上有保证。

看完的都是真爱，点个赞再走呗？您的「三连」就是我的最大动力！

原文链接：https://zhuanlan.zhihu.com/p/330579450

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.153】Linux专家谈如何学习Linux，以及Linux的职业发展

记得最早接触linux是在2002年，那个时候，还在上大学，曾经为安装一个系统让我们忘记疲劳，挑灯夜战，不亦乐乎。那时如果会安装一个Linux系统也是可以走进不少女生宿舍，哈哈。那时Linux的学习资料还很少，能够学习的书籍也不多，网上Linux技术社区也很少，就凭着Redhat6.2自带的几页使用说明开始了学习linux的生涯。

转眼间，10几年过去了，我也与Linux相伴了10多年，10年间，随着虚拟化、云计算时代的来临，Linux迅猛发展，在服务器领域已经占据半壁江山，而基于Linux的运维也面临新的挑战：面对越来越复杂的业务，面对越来越多样化的用户需求，不断扩展的应用需要越来越合理的模式来保障Linux灵活便捷、安全稳定地持续提供服务，这种模式中的保障因素就是Linux运维。从初期的几台服务器发展到庞大的云计算数据中心，单靠人工已经无法满足在技术、业务、管理等方面的要求，那么标准化、自动化、稳定性、可靠性等业务需求越来越被人们所重视。因此，对Linux的高性能、可靠性提出了更高的要求。

2009年我完成了基于Linux操作系统的作品《循序渐进Linux——基础知识、服务器搭建、系统管理、性能调优、集群应用》，此书出版后，得到了很多同行的认可，但是，此书出版已近6年，书中介绍的部分知识点和操作系统版本已经变得陈旧，已不能满足Linux以及开源技术迅速发展的需求，抱着对读者和本书负责的态度，我决定修订此书，因此也就有了这本《循序渐进Linux（第2版）——基础知识、服务器搭建、系统管理、性能调优、虚拟化与集群应用》这本新书。

今天不讲深入的东西，只想将自己多年来学习Linux的心得与感受与大家一起分享下，希望能给爱好Linux的朋友们或者Linux的同行们一点意见和建议，一点经验心得与大家共勉！

## 1.Linux在各领域发展的现状与趋势

很多新手都有一个很疑惑的问题：“Linux我听过，但是学习linux系统，能在上面干什么呢，或者说linux系统具体能做什么”，带着这个疑问，在本书的开篇，我们先来了解下Linux与开源软件的关系以及Linux的应用领域和未来的发展趋势。

### **1.1.Linux与开源软件**

Linux是一种自由和开放源代码的类UNIX操作系统，该操作系统的内核由林纳斯托瓦兹在1991年首次发布，之后，在加上用户空间的应用程序之后，就成为了Linux操作系统。严格来讲，Linux只是操作系统内核本身，但通常采用“Linux内核”来表达该意思。而Linux则常用来指基于Linux内核的完整操作系统，它包括GUI组件和许多其他实用工具。

GNU通用公共许可协议（GNU General Public License，简称GNU GPL或GPL），是一个广泛被使用的自由软件许可协议条款，最初由理查德斯托曼为GNU计划而撰写，GPL给予了计算机程序自由软件的定义， 任何基于GPL软件开发衍生的产品在发布时必须采用GPL许可证方式，且必须公开源代码，

Linux是自由软件和开放源代码软件发展中最著名的例子。只要遵循GNU通用公共许可证，任何个人和机构都可以自由地使用Linux的所有底层源代码，也可以自由地修改和再发布。随着Linux操作系统飞速发展，各种集成在Linux上的开源软件和实用工具也得到了应用和普及，因此，Linux也成为了开源软件的代名词。

### **1.2. Linux在服务器领域的发展**

随着开源软件在世界范围内影响力日益增强，Linux服务器操作系统在整个服务器操作系统市场格局中占据了越来越多的市场份额，已经形成了大规模市场应用的局面。并且保持着快速的增长率。尤其在政府、金融、农业、交通、电信等国家关键领域。此外，考虑到Linux的快速成长性以及国家相关政策的扶持力度，Linux服务器产品一定能够冲击更大的服务器市场。

据权威部门统计，目前Linux在服务器领域已经占据75%的市场份额，同时，Linux在服务器市场的迅速崛起，已经引起全球IT产业的高度关注，并以强劲的势头成为服务器操作系统领域中的中坚力量。

### **1.3. Linux在桌面领域的发展**

近年来，特别在国内市场，Linux桌面操作系统的发展趋势非常迅猛。国内如中标麒麟Linux、红旗Linux、深度Linux等系统软件厂商都推出的Linux桌面操作系统，目前已经在政府、企业、OEM等领域得到了广泛应用。另外SUSE、Ubuntu也相继推出了基于Linux的桌面系统，特别是Ubuntu Linux，已经积累了大量社区用户。但是，从系统的整体功能、性能来看，Linux桌面系统与Windows系列相比还有一定的差距，主要表现在系统易用性、系统管理、软硬件兼容性、软件的丰富程度等方面。

### **1.4. Linux在移动嵌入式领域的发展**

Linux的低成本、强大的定制功能以及良好的移植性能，使得Linux在嵌入式系统方面也得到广泛应用，目前Linux以广泛应用于手机、平板电脑、路由器、电视和电子游戏机等领域。在移动设备上广泛使用的Android操作系统就是创建在Linux内核之上的。目前，Android已经成为全球最流行的智能手机操作系统，据2015年权威部门最新统计，Android操作系统的全球市场份额已达84.6%。

此外，思科在网络防火墙和路由器也使用了定制的Linux，阿里云也开发了一套基于Linux的操作系统“YunOS”，可用于智能手机、平板电脑和网络电视；常见的数字视频录像机、舞台灯光控制系统等都在逐渐采用定制版本的Linux来实现，而这一切均归功与Linux与开源的力量。

### **1.5. Linux在云计算/大数据领域的发展**

互联网产业的迅猛发展，促使云计算、大数据产业的形成并快速发展，云计算、大数据作为一个基于开源软件的平台，Linux占据了核心优势；据Linux基金会的研究，86%的企业已经使用Linux操作系统进行云计算、大数据平台的构建，目前，Linux已开始取代Unix成为最受青睐的云计算、大数据平台操作系统。

## 2.选择适合自己的Linux发行版

谈到Linux的发行版本，太多了，可能谁也不能给出一个准确的数字，但是有一点是可以肯定的，Linux正在变得越来越流行。面对这么多的Linux发行版，打算从其他系统转到Linux系统来的初学者可能会感到困惑，即便是忠实的Linux用户也没有时间和精力去挨个尝试，因此初学者在学习Linux之前，需要有一个明确的方向，选择一个适合自己的Linux系统至关重要。下面我们就分类介绍。

### **2.1. 常见的Linux发行版**

#### **2.1.1.Red Hat Linux**

Red Hat Linux是Red Hat最早发行的个人版本的Linux，其1.0版本于1994年11月3日发行。虽然其历史不及其他Linux发行版本悠久，但比起很多的Linux发行套件，Red Hat的历史要悠久得多。自从Red Hat 9.0版本发布后，Red Hat公司就不再开发桌面版的Linux发行套件，Red Hat Linux停止了开发，而将全部力量集中在服务器版的开发上，也就是Red Hat Enterprise Linux版。2004年4月30日，Red Hat公司正式停止对Red Hat 9.0版本的支援，标志着Red Hat Linux的正式完结。原本的桌面版Red Hat Linux发行套件则与来自开源社区的Fedora进行合并，成为Fedora Core发行版本。

目前Red Hat分为两个系列：由Red Hat公司提供收费技术支持和更新的Red Hat Enterprise Linux，以及由社区开发的免费的Fedora Core。

#### 2.1.2. Fedora Core

Fedora Core（缩写为FC）被Red Hat公司定位为新技术的测试平台，许多新的技术都会在FC中检验。如果稳定的话Red Hat公司则会考虑加入到Red Hat Enterprise Linux中。

Fedora Core 1发布于2003年年末，而FC的定位便是桌面用户。FC提供了最新的软件包，同时它的版本更新周期也非常短，仅有6个月。由于版本更新频繁，性能和稳定性得不到保证，因此一般在服务器上不推荐采用Fedora Core。

Fedora对于用户而言，是一套功能完备、更新迅速的免费操作系统，因此，个人领域的应用，例如开发、体验新功能等可选择此发行版本。

#### **2.1.3．Red Hat Enterprise Linux**

Red Hat Enterprise Linux（缩写为RHEL，Red Hat的企业版Linux）。Red Hat现在主要做服务器版的Linux开发，在版本上注重了性能和稳定性，以及对硬件的支持。由于企业版操作系统的开发周期较长，注重性能、稳定性和服务端软件支持，因此版本更新相对较缓慢。

RHEL的版本都是基于Fedora。大约每六个版本的Fedora会有一个新版本的RHEL发布，因此，RHEL大约3年发布一个新版本。其最新版本是2015年3月6日发布的RHEL 7.1。本书就是以此版本展开讲述的。

#### **2.1.4．Centos**

CentOS全名为“社区企业操作系统”（Community Enterprise Operating System）。它是来自于RHEL依照开放源代码规定发布的源代码所编译而成，由于RHEL是商业产品，因此必须将所有Red Hat的Logo标识改成自己的CentOS标识，这就产生了CentOS操作系统，两者的不同在于，CentOS并不包含封闭源代码软件。因此，CentOS不但可以自由使用，而且还能享受CentOS提供的免费长期升级和更新服务。这是CentOS的一个很大优势。

CentOS采取从RHEL的源代码包来构建，它的版本号有两个部分：一个主要版本和一个次要版本，主要和次要版本号分别对应于RHEL的主要版本与更新包，例如CentOS6.5构建在RHEL6.0更新的第5版。但是从RHEL7.0版本以后，CentOS版本命名格式又稍有变化：主要版本仍然对应于RHEL的主要版本，次要版本以RHEL更新包发布日期为准，例如RHEL7.1对应的CentOS版本为CentOS 7.1.1503，这表示RHEL7.1版本是2015年3月份发布的。

在2014年CentOS宣布与Red Hat合作，但CentOS将会在新的委员会下继续运作，并不受RHEL的影响。这个策略表明CentOS后续发展将有Red Hat作为强有力的支持。

#### **2.1.5. SuSE Linux**

SUSE是德国最著名的Linux发行版，也享有很高的声誉，不过命运相当坎坷；2003年11月4日，Novell公司对外宣布将收购SUSE。2004年1月该收购顺利完成，Novell同时将SUSE正式命名为SUSE Linux。Novell公司收购SUSE，加速了SUSE Linux的发展，将免费SUSE Linux改为openSUSE社区项目，但在2010年，Attachmate公司收购了Novell，在被收购后，SUSE Linux发展受阻，而就在3年后，SUSE再次易主，2014年9月Attachmate公司被上市公司Micro Focus收购，但幸运的是：SUSE官方宣布开源作为SUSE的发展之本，仍将持续贡献开源，SUSE仍将全力投入对openSUSE的支持。

虽然SUSE多次易主，但并不影响它的专业性，据不完全统计，SUSE Linux现在欧洲Linux市场占有将近80%的份额，大部分关键性应用都是建立在SUSE Linux下的。而由于SUSE多次易主，再加上SUSE在中国的营销模式问题，现在SUSE在中国的Linux市场份额并不大，但是这些并不影响SUSE Linux高可靠性与稳定性的事实。随着SUSE的发展，相信SUSE Linux在中国的应用会越来越多。

#### **2.1.6. Ubuntu Linux**

Ubuntu（中文谐音为友帮拓、优般图、乌班图）是一个以桌面应用为主的Linux操作系统，基于Debian GNU/Linux，Ubuntu的目标在于为一般用户提供一个最新的、同时又相当稳定的主要由自由软件构建而成的操作系统。Ubuntu具有庞大的社区力量，用户可以方便地从社区获得帮助。

#### **2.1.7. 发行版总结**

上面主要介绍了几种最常见的Linux发行版本，其实Linux的发行版本还有很多，比较常见的还有Debian GNU/Linux、Mandriva、Gentoo、Slackware、Knoppix、MEPIS和Xandros，以及国产的红旗Redflag、深度deepin Linux和中标麒麟Linux等，这里不再一一介绍。其实纵观Linux的各个发行版，Linux发行版本无非是朝着这两个方面而来，一是服务器市场，二是桌面市场。

以Ubuntu Linux为代表的Linux发行版走的是桌面市场路线，虽然它们给用户带来很多惊喜，更新也很快，但是由于桌面市场有着Windows这样强劲的对手，因此Linux桌面发展不容乐观，目前Ubuntu Linux也开始向企业级服务器市场发力。

以Red Hat系列版本为代表的Linux发行版现在主要面向企业级Linux的服务器市场，重点开发Linux的企业版本，其他的（例如国产Redflag、中标麒麟Linux等）都重点投入在了Linux服务器市场。Linux两大发布厂商现在都走了Linux服务器市场的路线，可见Linux作为企业级服务器有着巨大的发展前途。据权威部门统计，Linux在服务器市场的占有率每年都在持续上升。

其实很多Linux的应用都是针对Linux服务器的，本书的讲述也是主要针对Linux在服务器下的各种应用展开的。

### **2.2.初学者入门首选——Centos系列**

在了解了Linux几个主要发行版本后，我们就找到了为何选择CentOS作为初学者入门学习的理由了。

CentOS现在拥有庞大的网络用户群体，网络Linux资源基本80%都是基于CentOS发行版的，如果在学习过程中遇到任何问题，在网络中可以较容易地搜索到解决方案。

CentOS系列版本可以轻松获得。可以从CentOS官网或者163开源、sohu开源、阿里云开源站下载CentOS各个版本的安装介质，如果是第一次接触Linux，那么建议先安装Fedora Core。Fedora Core的安装简单，对硬件支持很好，界面也很华丽，同时也可以体验Linux的最新功能。如果对Linux有一定的了解，需要深入学习，建议使用CentOS发行版系统。

CentOS应用范围广，具有典型性和代表性，现在基本所有的互联网公司后台服务器都采用CentOS作为操作系统，可以说学会了CentOS，不但能迅速融入企业的工作环境，还能触类旁通，其它类似的Linux发行版也能很快掌握。同时，现在周围学习Linux的用户一般也都是以CentOS为主的，这样交流方便，学习中出现问题，更容易得到解决。最主要的是CentOS的安装和使用上也是最简单的，因此基本上不会在“装系统”上浪费过多时间。

### **2.3. 桌面平台首选——Ubuntu Linux**

说到Linux桌面市场，Ubuntu Linux几乎占据了桌面Linux的半壁江山，Ubuntu Linux主打Linux桌面之最，界面美观，简洁而不失华丽，如果想在Linux下进行娱乐休闲，Ubuntu Linux绝对是首选。

Ubuntu的安装非常人性化，只需按照提示一步一步进行。Ubuntu被誉为对硬件支持最好最全面的Linux发行版之一，许多在其他发行版上无法使用的，或者在默认配置时无法使用的硬件，在Ubuntu上都能轻松安装使用。因此用户可以像安装Windows一样轻松地安装Ubuntu，尽情体验Ubuntu Linux带来的乐趣。

### **2.4. 企业级应用首选——RHEL/Centos系列**

企业级的应用追求的是可靠性和稳定性，这就要求构建企业级应用的系统平台具有高可靠性和高稳定性。企业级Linux的发行版本就是解决的这个问题。

RHEL与Centos两个Linux发现版本，并没有太大差别，所不同的是RHEL属于商业Linux发行版本，如果要使用RHEL版本，则需要购买商业授权和咨询服务，Red Hat提供系统的技术支持并提供系统的免费升级。目前Red Hat官网已经不再提供可免费下载的光盘介质，如果需要试用，可通过官网下载有试用时间的评估版Linux。而CentOS属于非商业发行版，可以从网上免费下载CentOS各个版本的安装介质，但CentOS并不提供商业支持，当然使用者也不用负上任何商业责任。

那么，到底是选择CentOS还是RHEL呢，这取决于你所在公司是否拥有相应的技术力量，如果是单纯的业务型企业，那么建议选购RHEL发行版并购买相应服务，这样可以节省企业的IT管理费用，并可得到专业的技术支持服务。相反，如果企业技术力量比较强大，并且有多年Linux使用经验的话，那么CentOS发行版将是最好的选择。

## 3.养成良好的Linux操作习惯

开始Linux的学习之后，请不要用Windows的工作方式来思考问题，因为它们之间确实有很大的不同，比如它们之间的内存管理机制、进程运行机制等都有很大不同，因此抛开Windows的那种思维，用全新的理念尝试去挖掘Linux身上特有的潜质，对初学者是至关重要的。

### **3.1. 一定要习惯命令行方式**

Linux是由命令行组成的操作系统，精髓在命令行，无论图形界面发展到什么水平，命令行方式的操作永远是不会变的。Linux命令有许多强大的功能：从简单的磁盘操作、文件存取，到进行复杂的多媒体图像和流媒体文件的制作，都离不开命令行。虽然Linux也有桌面系统，但是X-window也只是运行在命令行模式下的一个应用程序。

因此，可以说命令是学习Linux系统的基础，在很大程度上学习Linux就是学习命令，很多Linux高手其实都是玩儿命令很熟练的人。

也许对于刚刚从Windows系统进入Linux学习的初学者来说，立刻进入枯燥的命令学习实在太难，但是一旦学会就爱不释手。因为它的功能实在太强大了。

### **3.2. 理论结合实践**

有很多初学者都会遇到这么一个问题，自己对系统的每个命令都很熟悉，但是在系统出现故障的时候，就无从下手了，甚至不知道在什么时候用什么命令去检查系统，这是很多Linux新手最无奈的事情了。说到底，就是学习的理论知识没有很好地与系统实际操作相结合。

很多Linux知识，例如每个命令的参数含义，在书本上说得很清楚，看起来也很容易理解，但是一旦组合起来使用，却并不那么容易，没有多次的动手练习，其中的技巧是无法完全掌握的。

人类大脑不像计算机的硬盘，除非硬盘坏掉或者硬盘被格式化，否则储存的资料将永远记忆在硬盘中，而且时刻可以调用。而在人类记忆的曲线中，必须要不断地重复练习才会将一件事情记得比较牢。学习Linux也一样，如果无法坚持学习的话，就会学了后面的，忘记了前面的。还有些Linux初学者也学了很多Linux知识，但是由于长期不用，导致学过的东西在很短的时间内又忘记了，久而久之，失去了学习的信心。

可见，要培养自己的实战技能，只有勤于动手，肯于实践，这也是学好Linux的根本。

### **3.3. 学会使用Linux联机帮助**

各个Linux的发行版本的技术支持时间都较短，这对于Linux初学者来说往往是不够的，其实当安装了完整的Linux系统后其中已经包含了一个强大的帮助，只是可能你还没有发现它，或者还没有掌握使用它的技巧。例如，对于tar命令的使用不是很熟悉，那么只要在命令行输入“man tar”，就会得到tar的详细说明和用法。

主流的Linux发行版都自带了非常详细的帮助文档，包括使用说明和FAQ，从系统的安装到系统的维护，再到系统安全，针对不同层次用户的详尽文档。仔细阅读文档后，60%的问题都可在这里得到解决。

### **3.4. 学会独立思考问题，独立解决问题**

遇到问题，首先想到的应该是如何自己去解决这个问题，解决方式有很多，比如看书查资料、网络搜索引擎搜索和浏览技术论坛等，通过这几种方式，90%的问题都能得到解决

独立思考并解决问题，不但锻炼了自己独立解决问题的能力，在技术上也能得到快速提高。如果通过以上方式实在解决不了的话，可以向人询问，得到答案后要思考为何这么做，然后做笔记记录解决过程。最忌讳的方式是只要遇到问题，就去问人，虽然这样可能会很快解决问题，但是长久下去遇到问题就会依赖别人，技术上也不会进步。

### **3.5. 学习专业英语**

如果想深入学习Linux，一定要尝试去看英文文档。因为，技术性的东西写的最好的，最全面的文档都是英语写的，最先发布的高新技术也都是用英语写的。即便是非英语国家的人发布技术文档，也都首先翻译成英语在国际学术杂志和网络上发表。安装一个新的软件时先看Readme文档，再看Install文档，然后看FAQ文档，最后才动手安装，这样遇到问题就知道原因了。因此，学习一点专业的英语是很有必要的。

## 4.Linux学习路线图

Linux开发或管理人才是企业目前急需的技术人才之一，笔者根据10多年Linux相关工作经验，总结出了一套学习Linux的线路图，大家可以此线路图为依据，抓住重点，分清主次，相信一定能达到事半功倍的效果。如果读者能认真学习完并掌握本线路图所涉及的技术要点，那么也就基本掌握了企业对Linux开发或管理人才的基本应用需求。

Linux学习线路图如下图所示，初级阶段主要是对Linux基础知识以及系统基本应用的介绍，要掌握的内容较多，如果初次接触Linux，那么入门还是有一定难度的，Linux注重的是命令操作，因此初级阶段以学习基础命令为主，多看书、多实践是学好命令的根本。

![img](https://pic3.zhimg.com/80/v2-2d0c0f2b9162d97eede19aa0b6dc52ca_720w.webp)

![img](https://pic4.zhimg.com/80/v2-04a5cf9f8e18bb3cbb107e2e30d23b6f_720w.webp)

![img](https://pic3.zhimg.com/80/v2-1c853b97fd831bdbe90cbac08f2eca26_720w.webp)

![img](https://pic2.zhimg.com/80/v2-325ca96c0ae4145eaaebc239a07651b1_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/304842470

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.154】理解MySQL——索引与优化篇

写在前面：索引对查询的速度有着至关重要的影响，理解索引也是进行数据库性能调优的起点。考虑如下情况，假设数据库中一个表有10^6条记录，DBMS的页面大小为4K，并存储100条记录。如果没有索引，查询将对整个表进行扫描，最坏的情况下，如果所有数据页都不在内存，需要读取10^4个页面，如果这10^4个页面在磁盘上随机分布，需要进行10^4次I/O，假设磁盘每次I/O时间为10ms(忽略数据传输时间)，则总共需要100s(但实际上要好很多很多)。如果对之建立B-Tree索引，则只需要进行log100(10^6)=3次页面读取，最坏情况下耗时30ms。这就是索引带来的效果，很多时候，当你的应用程序进行SQL查询速度很慢时，应该想想是否可以建索引。进入正题：

## 1.第一章、索引与优化

### **1.1.选择索引的数据类型**

MySQL支持很多数据类型，选择合适的数据类型存储数据对性能有很大的影响。通常来说，可以遵循以下一些指导原则：

(1)越小的数据类型通常更好：越小的数据类型通常在磁盘、内存和CPU缓存中都需要更少的空间，处理起来更快。
(2)简单的数据类型更好：整型数据比起字符，处理开销更小，因为字符串的比较更复杂。在MySQL中，应该用内置的日期和时间数据类型，而不是用字符串来存储时间；以及用整型数据类型存储IP地址。
(3)尽量避免NULL：应该指定列为NOT NULL，除非你想存储NULL。在MySQL中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。你应该用0、一个特殊的值或者一个空串代替空值。

### 1.2.选择标识符

选择合适的标识符是非常重要的。选择时不仅应该考虑存储类型，而且应该考虑MySQL是怎样进行运算和比较的。一旦选定数据类型，应该保证所有相关的表都使用相同的数据类型。
(1) 整型：通常是作为标识符的最好选择，因为可以更快地处理，而且可以设置为AUTO_INCREMENT。

(2) 字符串：尽量避免使用字符串作为标识符，它们消耗更好的空间，处理起来也较慢。而且，通常来说，字符串都是随机的，所以它们在索引中的位置也是随机的，这会导致页面分裂、随机访问磁盘，聚簇索引分裂（对于使用聚簇索引的存储引擎）。

## **2.索引入门**

对于任何DBMS，索引都是进行优化的最主要的因素。对于少量的数据，没有合适的索引影响不是很大，但是，当随着数据量的增加，性能会急剧下降。
如果对多列进行索引(组合索引)，列的顺序非常重要，MySQL仅能对索引最左边的前缀进行有效的查找。例如：
假设存在组合索引it1c1c2(c1,c2)，查询语句select * from t1 where c1=1 and c2=2能够使用该索引。查询语句select * from t1 where c1=1也能够使用该索引。但是，查询语句select * from t1 where c2=2不能够使用该索引，因为没有组合索引的引导列，即，要想使用c2列进行查找，必须出现c1等于某值。

### 2.1.索引的类型

索引是在存储引擎中实现的，而不是在服务器层中实现的。所以，每种存储引擎的索引都不一定完全相同，并不是所有的存储引擎都支持所有的索引类型。

#### 2.1.1.B-Tree索引

假设有如下一个表：

![img](https://pic3.zhimg.com/80/v2-1e955649a9bf6acf2eb4ae90851a82fa_720w.webp)

其索引包含表中每一行的last_name、first_name和dob列。其结构大致如下：

![img](https://pic2.zhimg.com/80/v2-a2ed65c88de52551df648b23c53cd3e9_720w.webp)

**索引存储的值按索引列中的顺序排列。可以利用B-Tree索引进行全关键字、关键字范围和关键字前缀查询，当然，如果想使用索引，你必须保证按索引的最左边前缀(leftmost prefix of the index)来进行查询。**
(1)匹配全值(Match the full value)：对索引中的所有列都指定具体的值。例如，上图中索引可以帮助你查找出生于1960-01-01的Cuba Allen。
(2)匹配最左前缀(Match a leftmost prefix)：你可以利用索引查找last name为Allen的人，仅仅使用索引中的第1列。
(3)匹配列前缀(Match a column prefix)：例如，你可以利用索引查找last name以J开始的人，这仅仅使用索引中的第1列。
(4)匹配值的范围查询(Match a range of values)：可以利用索引查找last name在Allen和Barrymore之间的人，仅仅使用索引中第1列。
(5)匹配部分精确而其它部分进行范围匹配(Match one part exactly and match a range on another part)：可以利用索引查找last name为Allen，而first name以字母K开始的人。
(6)仅对索引进行查询(Index-only queries)：如果查询的列都位于索引中，则不需要读取元组的值。
**由于B-树中的节点都是顺序存储的，所以可以利用索引进行查找(找某些值)，也可以对查询结果进行ORDER BY。当然，使用B-tree索引有以下一些限制：**
(1) 查询必须从索引的最左边的列开始。关于这点已经提了很多遍了。例如你不能利用索引查找在某一天出生的人。
(2) 不能跳过某一索引列。例如，你不能利用索引查找last name为Smith且出生于某一天的人。
(3) 存储引擎不能使用索引中范围条件右边的列。例如，如果你的查询语句为WHERE last_name=”Smith” AND first_name LIKE ‘J%’ AND dob=’1976-12-23’，则该查询只会使用索引中的前两列，因为LIKE是范围查询。

#### 2.1.2.Hash索引

MySQL中，只有Memory存储引擎显示支持hash索引，是Memory表的默认索引类型，尽管Memory表也可以使用B-Tree索引。Memory存储引擎支持非唯一hash索引，这在数据库领域是罕见的，如果多个值有相同的hash code，索引把它们的行指针用链表保存到同一个hash表项中。
假设创建如下一个表：

![img](https://pic2.zhimg.com/80/v2-5a0663bdac842c0eae3eebf776bdf0b1_720w.webp)

包含的数据如下：

![img](https://pic3.zhimg.com/80/v2-fd7efba21ab080ced3d821b1309590ce_720w.webp)

假设索引使用hash函数f( )，如下：

![img](https://pic4.zhimg.com/80/v2-1bd67054a18c773bed89497555b0b55b_720w.webp)

此时，索引的结构大概如下：

![img](https://pic2.zhimg.com/80/v2-886e62813a086edb769812affdc40525_720w.webp)

**Slots是有序的，但是记录不是有序的。当你执行**
mysql> SELECT lname FROM testhash WHERE fname=’Peter’;
MySQL会计算’Peter’的hash值，然后通过它来查询索引的行指针。因为f(‘Peter’) = 8784，MySQL会在索引中查找8784，得到指向记录3的指针。
因为索引自己仅仅存储很短的值，所以，索引非常紧凑。Hash值不取决于列的数据类型，一个TINYINT列的索引与一个长字符串列的索引一样大。

**Hash索引有以下一些限制：**
(1)由于索引仅包含hash code和记录指针，所以，MySQL不能通过使用索引避免读取记录。但是访问内存中的记录是非常迅速的，不会对性造成太大的影响。
(2)不能使用hash索引排序。
(3)Hash索引不支持键的部分匹配，因为是通过整个索引值来计算hash值的。
(4)Hash索引只支持等值比较，例如使用=，IN( )和<=>。对于WHERE price>100并不能加速查询。

#### 2.1.3.空间(R-Tree)索引

MyISAM支持空间索引，主要用于地理空间数据类型，例如GEOMETRY。

#### 2.1.4.全文(Full-text)索引

全文索引是MyISAM的一个特殊索引类型，主要用于全文检索。

## **3.高性能的索引策略**

### 3.1.聚簇索引(Clustered Indexes)

聚簇索引保证关键字的值相近的元组存储的物理位置也相同（所以字符串类型不宜建立聚簇索引，特别是随机字符串，会使得系统进行大量的移动操作），且一个表只能有一个聚簇索引。因为由存储引擎实现索引，所以，并不是所有的引擎都支持聚簇索引。目前，只有solidDB和InnoDB支持。
聚簇索引的结构大致如下：

![img](https://pic4.zhimg.com/80/v2-cbfd93998c547e68ff121d037aff0483_720w.webp)

注：叶子页面包含完整的元组，而内节点页面仅包含索引的列(索引的列为整型)。一些DBMS允许用户指定聚簇索引，但是MySQL的存储引擎到目前为止都不支持。InnoDB对主键建立聚簇索引。如果你不指定主键，InnoDB会用一个具有唯一且非空值的索引来代替。如果不存在这样的索引，InnoDB会定义一个隐藏的主键，然后对其建立聚簇索引。一般来说，DBMS都会以聚簇索引的形式来存储实际的数据，它是其它二级索引的基础。

#### 3.1.1.InnoDB和MyISAM的数据布局的比较

为了更加理解聚簇索引和非聚簇索引，或者primary索引和second索引(MyISAM不支持聚簇索引)，来比较一下InnoDB和MyISAM的数据布局，对于如下表：

![img](https://pic2.zhimg.com/80/v2-2cff1f187a42073aaf3a4a2f64da9209_720w.webp)

假设主键的值位于1—10,000之间，且按随机顺序插入，然后用OPTIMIZE TABLE进行优化。col2随机赋予1—100之间的值，所以会存在许多重复的值。
(1) MyISAM的数据布局
其布局十分简单，MyISAM按照插入的顺序在磁盘上存储数据，如下：

![img](https://pic3.zhimg.com/80/v2-f8861a6cd328f7e04ca1eada39fecc76_720w.webp)

注：左边为行号(row number)，从0开始。因为元组的大小固定，所以MyISAM可以很容易地从表的开始位置找到某一字节的位置。
据所建立的primary key的索引结构大致如下：

![img](https://pic1.zhimg.com/80/v2-004905ee1c6d24fe299f50052b922000_720w.webp)

注：MyISAM不支持聚簇索引，索引中每一个叶子节点仅仅包含行号(row number)，且叶子节点按照col1的顺序存储。
来看看col2的索引结构：

![img](https://pic1.zhimg.com/80/v2-b65c86238b46be656100e17f5d8c4860_720w.webp)

实际上，在MyISAM中，primary key和其它索引没有什么区别。Primary key仅仅只是一个叫做PRIMARY的唯一，非空的索引而已。

(2) InnoDB的数据布局
InnoDB按聚簇索引的形式存储数据，所以它的数据布局有着很大的不同。它存储表的结构大致如下：

![img](https://pic2.zhimg.com/80/v2-f480dc04123287def57824ee43e90c65_720w.webp)

注：聚簇索引中的每个叶子节点包含primary key的值，事务ID和回滚指针(rollback pointer)——用于事务和MVCC，和余下的列(如col2)。

相对于MyISAM，二级索引与聚簇索引有很大的不同。InnoDB的二级索引的叶子包含primary key的值，而不是行指针(row pointers)，这减小了移动数据或者数据页面分裂时维护二级索引的开销，因为InnoDB不需要更新索引的行指针。其结构大致如下：

![img](https://pic4.zhimg.com/80/v2-bbf7196ac2e92cefd79bf8210e803ee3_720w.webp)

聚簇索引和非聚簇索引表的对比：

![img](https://pic3.zhimg.com/80/v2-c02ef3fd0822c56ff766b5bd7c3057d6_720w.webp)

#### 3.1.2.按primary key的顺序插入行(InnoDB)

如果你用InnoDB，而且不需要特殊的聚簇索引，一个好的做法就是使用代理主键(surrogate key)——独立于你的应用中的数据。最简单的做法就是使用一个AUTO_INCREMENT的列，这会保证记录按照顺序插入，而且能提高使用primary key进行连接的查询的性能。应该尽量避免随机的聚簇主键，例如，字符串主键就是一个不好的选择，它使得插入操作变得随机。

### 3.2.覆盖索引(Covering Indexes)

如果索引包含满足查询的所有数据，就称为覆盖索引。覆盖索引是一种非常强大的工具，能大大提高查询性能。只需要读取索引而不用读取数据有以下一些优点：
(1)索引项通常比记录要小，所以MySQL访问更少的数据；
(2)索引都按值的大小顺序存储，相对于随机访问记录，需要更少的I/O；
(3)大多数据引擎能更好的缓存索引。比如MyISAM只缓存索引。
(4)覆盖索引对于InnoDB表尤其有用，因为InnoDB使用聚集索引组织数据，如果二级索引中包含查询所需的数据，就不再需要在聚集索引中查找了。
覆盖索引不能是任何索引，只有B-TREE索引存储相应的值。而且不同的存储引擎实现覆盖索引的方式都不同，并不是所有存储引擎都支持覆盖索引(Memory和Falcon就不支持)。
对于索引覆盖查询(index-covered query)，使用EXPLAIN时，可以在Extra一列中看到“Using index”。例如，在sakila的inventory表中，有一个组合索引(store_id,film_id)，对于只需要访问这两列的查询，MySQL就可以使用索引，如下：

![img](https://pic3.zhimg.com/80/v2-0b79c050794a35fbf7cc60f88db09b86_720w.webp)

在大多数引擎中，只有当查询语句所访问的列是索引的一部分时，索引才会覆盖。但是，InnoDB不限于此，InnoDB的二级索引在叶子节点中存储了primary key的值。因此，sakila.actor表使用InnoDB，而且对于是last_name上有索引，所以，索引能覆盖那些访问actor_id的查询，如：

![img](https://pic2.zhimg.com/80/v2-317461e15d1ab444ef5ff66e57f6a3b1_720w.webp)

### 3.3.利用索引进行排序

MySQL中，有两种方式生成有序结果集：一是使用filesort，二是按索引顺序扫描。利用索引进行排序操作是非常快的，而且可以利用同一索引同时进行查找和排序操作。当索引的顺序与ORDER BY中的列顺序相同且所有的列是同一方向(全部升序或者全部降序)时，可以使用索引来排序。如果查询是连接多个表，仅当ORDER BY中的所有列都是第一个表的列时才会使用索引。其它情况都会使用filesort。

![img](https://pic3.zhimg.com/80/v2-f6408fd540bafa78967e86179854ecd6_720w.webp)

![img](https://pic3.zhimg.com/80/v2-4c4d840f7e1ee707ce8d7954da0a525e_720w.webp)

![img](https://pic3.zhimg.com/80/v2-a7b8f22683ed8776af72d2cd952af306_720w.webp)

当MySQL不能使用索引进行排序时，就会利用自己的排序算法(快速排序算法)在内存(sort buffer)中对数据进行排序，如果内存装载不下，它会将磁盘上的数据进行分块，再对各个数据块进行排序，然后将各个块合并成有序的结果集（实际上就是外排序）。对于filesort，MySQL有两种排序算法。
(1)两边扫描算法(Two passes)
实现方式是先将需要排序的字段和可以直接定位到相关行数据的指针信息取出，然后在设定的内存（通过参数sort_buffer_size设定）中进行排序，完成排序之后再次通过行指针信息取出所需的Columns。
注：该算法是4.1之前采用的算法，它需要两次访问数据，尤其是第二次读取操作会导致大量的随机I/O操作。另一方面，内存开销较小。
(3) 一次扫描算法(single pass)
该算法一次性将所需的Columns全部取出，在内存中排序后直接将结果输出。
注：从 MySQL 4.1 版本开始使用该算法。它减少了I/O的次数，效率较高，但是内存开销也较大。如果我们将并不需要的Columns也取出来，就会极大地浪费排序过程所需要的内存。在 MySQL 4.1 之后的版本中，可以通过设置 max_length_for_sort_data 参数来控制 MySQL 选择第一种排序算法还是第二种。当取出的所有大字段总大小大于 max_length_for_sort_data 的设置时，MySQL 就会选择使用第一种排序算法，反之，则会选择第二种。为了尽可能地提高排序性能，我们自然更希望使用第二种排序算法，所以在 Query 中仅仅取出需要的 Columns 是非常有必要的。

当对连接操作进行排序时，如果ORDER BY仅仅引用第一个表的列，MySQL对该表进行filesort操作，然后进行连接处理，此时，EXPLAIN输出“Using filesort”；否则，MySQL必须将查询的结果集生成一个临时表，在连接完成之后进行filesort操作，此时，EXPLAIN输出“Using temporary;Using filesort”。

### 3.4.索引与加锁

索引对于InnoDB非常重要，因为它可以让查询锁更少的元组。这点十分重要，因为MySQL 5.0中，InnoDB直到事务提交时才会解锁。有两个方面的原因：首先，即使InnoDB行级锁的开销非常高效，内存开销也较小，但不管怎么样，还是存在开销。其次，对不需要的元组的加锁，会增加锁的开销，降低并发性。
InnoDB仅对需要访问的元组加锁，而索引能够减少InnoDB访问的元组数。但是，只有在存储引擎层过滤掉那些不需要的数据才能达到这种目的。一旦索引不允许InnoDB那样做（即达不到过滤的目的），MySQL服务器只能对InnoDB返回的数据进行WHERE操作，此时，已经无法避免对那些元组加锁了：InnoDB已经锁住那些元组，服务器无法解锁了。
来看个例子：

![img](https://pic2.zhimg.com/80/v2-bec04e9d2b496cb62945d7a0d1d6cefd_720w.webp)

该查询仅仅返回2—3的数据，实际已经对1—3的数据加上排它锁了。InnoDB锁住元组1是因为MySQL的查询计划仅使用索引进行范围查询（而没有进行过滤操作，WHERE中第二个条件已经无法使用索引了）：

![img](https://pic3.zhimg.com/80/v2-0814ab7feccf607f7f84eaad00cd71a6_720w.webp)

表明存储引擎从索引的起始处开始，获取所有的行，直到actor_id<4为假，服务器无法告诉InnoDB去掉元组1。
为了证明row 1已经被锁住，我们另外建一个连接，执行如下操作：

![img](https://pic2.zhimg.com/80/v2-9df1e2e18a6bf85b4ad9b2b72bfd46fd_720w.webp)

该查询会被挂起，直到第一个连接的事务提交释放锁时，才会执行（这种行为对于基于语句的复制(statement-based replication)是必要的）。
如上所示，当使用索引时，InnoDB会锁住它不需要的元组。更糟糕的是，如果查询不能使用索引，MySQL会进行全表扫描，并锁住每一个元组，不管是否真正需要。

原文链接：https://zhuanlan.zhihu.com/p/343047128

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.155】skynet源码分析：服务，Actor模型，lua接口编程，demo演示Actor编程思维

skynet刚开始是单进程多线程的，它是由一个一个的服务组成的。在skynet上做开发，实际上就是在写服务。服务与服务之间通过消息队列进行通信。

## **1.做为核心功能，Skynet 仅解决一个问题：**

把一个符合规范的 C 模块，从动态库（so 文件）中启动起来，绑定一个永不重复（即使模块退出）的数字 id 做为其 handle 。模块被称为服务（Service），服务间可以自由发送消息。每个模块可以向 Skynet 框架注册一个 callback 函数，用来接收发给它的消息。每个服务都是被一个个消息包驱动，当没有包到来的时候，它们就会处于挂起状态，对 CPU 资源零消耗。如果需要自主逻辑，则可以利用 Skynet 系统提供的 timeout 消息，定期触发。

一个服务，默认不会执行任何逻辑，需要别人向它发出请求时，才会执行对应的逻辑（定时器也是通过消息队列，告诉指定服务，要执行定时事件），并在需要时返回结果给请求者。请求者往往也是其他服务。服务间的请求、响应和推送，并不是直接调用对方的api来执行，而是通过一个消息队列，也就是说，不论是请求、回应还是推送，都需要通过这个消息队列转发到另一个服务中。skynet的消息队列，分为两级，一个全局消息队列，他包含一个头尾指针，分别指向两个隶属于指定服务的次级消息队列。skynet中的每一个服务，都有一个唯一的、专属的次级消息队列。

## 2.**skynet服务的本质**

每个skynet服务都是一个lua state，也就是一个lua虚拟机实例。而且，每个服务都是隔离的，各自使用自己独立的内存空间，服务之间通过发消息来完成数据交换。

lua state本身没有多线程支持的，为了实现cpu的摊分，skynet实现上在一个线程运行多个lua state实例。而同一时间下，调度线程只运行一个服务实例。为了提高系统的并发性，skynet会启动一定数量的调度线程。同时，为了提高服务的并发性，就利用lua协程并发处理。

### **2.1.所以，skynet的并发性有3点：**

1、多个调度线程并发
2、lua协程并发处理
3、服务调度的切换

### 2.2.**skynet服务的设计基于Actor模型。有两个特点：**

\1. 每个Actor依次处理收到的消息
\2. 不同的Actor可同时处理各自的消息

实现上，cpu会按照一定规则分摊给每个Actor，每个Actor不会独占cpu，在处理一定数量消息后主动让出cpu，给其他进程处理消息。

**对于从事游戏客户端开发的朋友，界面开发很繁琐，客户端技术更新很快，很少能接触底层开发，想转型做游戏服务端开发的朋友，学习skynet可以了解服务器的整体架构流程，基础组件以及服务端的编程思维。**

**对于从事小游戏开发，skynet是一个轻量级游戏服务器框架，同时学习服务端，能进行全栈开发**

## 3.skynet的例子是怎么调用的

服务器：
simpledb.lua： skynet.register “SIMPLEDB” 向skynet里注册一个服务
agent.lua： skynet.call(“SIMPLEDB”, “text”, text) 调用相应的服务
main.lua： skynet.newservice(“simpledb”) 启动一个服务
以上函数都在\lualib\skynet.lua 文件内

### **3.1.以下是几个写服务时经常要用到的函数。**

newservice(name, …) 启动一个名为 name 的新服务。
uniqueservice(name, …) 启动一个唯一服务，如果服务该服务已经启动，则返回已启动的服务地址。
queryservice(name) 查询一个由 uniqueservice 启动的唯一服务的地址，若该服务尚未启动则等待。
localname(name) 返回同一进程内，用 register 注册的具名服务的地址。

newservice可以在一个进程里启动多个服务，这适用于无状态的服务。
uniqueservice则是类似于设计模式中的单件(singleton)，这适用于需要唯一性的服务。举个例子，比如写日志，只想写一份。或者是全局共享的数据。

### 3.2.**消息机制**

SKYNET设计综述讲到模块被称为服务。“服务间可以自由发送消息。每个模块可以向 Skynet 框架注册一个 callback 函数，用来接收发给它的消息。”还提到“把一个符合规范的 C 模块，从动态库（so 文件）中启动起来，绑定一个永不重复（即使模块退出）的数字 id 做为其 handle 。Skynet 提供了名字服务，还可以给特定的服务起一个易读的名字，而不是用 id 来指代它。id 和运行时态相关，无法保证每次启动服务，都有一致的 id ，但名字可以。”今天要分析的两个文件skynet_handle.c和skynet_handle.h就是实现名字服务的。

skynet_handle.c实际上就做了两个核心的事情，一是给服务分配一个handle，二是把handle和name关联起来。

把handle和name关联起来比较容易懂，实际上使用一个数组，关联的时候使用二分查找到数组里查名字，如果名字不存在，就插入一个元素，然后把名字和handle关联起来。插入元素的时候，如果数组空间不足了，就扩容为原来的2倍。

而给服务分配handle稍复杂一些，实际上也是使用一个slot数组，数组下标使用的是一个hash，数组元素指向服务的上下文。这个hash的算法是比较简单粗暴的，就是看从handle_indx开始累计到slot_size，看中间有没有空闲的下标（也就是下标指向为null的），如果遍历完了还是没有，就把slot扩大一倍，还是没有就再扩大一倍，直到找到空位为止，或者是slot长度超出限制为止。

取到了handle以后呢，还要将harbor id附到handle的高8位。

### **3.3.每个服务分三个运行阶段：**

首先是服务加载阶段，当服务的源文件被加载时，就会按 lua 的运行规则被执行到。这个阶段不可以调用任何有可能阻塞住该服务的 skynet api 。因为，在这个阶段中，和服务配套的 skynet 设置并没有初始化完毕。

然后是服务初始化阶段，由 skynet.start 这个 api 注册的初始化函数执行。这个初始化函数理论上可以调用任何 skynet api 了，但启动该服务的 skynet.newservice 这个 api 会一直等待到初始化函数结束才会返回。

最后是服务工作阶段，当你在初始化阶段注册了消息处理函数的话，只要有消息输入，就会触发注册的消息处理函数。这些消息都是 skynet 内部消息，外部的网络数据，定时器也会通过内部消息的形式表达出来。

从 skynet 底层框架来看，每个服务就是一个消息处理器。但在应用层看来并非如此。它是利用 lua 的 coroutine 工作的。当你的服务向另一个服务发送一个请求（即一个带 session 的消息）后，可以认为当前的消息已经处理完毕，服务会被 skynet 挂起。待对应服务收到请求并做出回应（发送一个回应类型的消息）后，服务会找到挂起的 coroutine ，把回应信息传入，延续之前未完的业务流程。从使用者角度看，更像是一个独立线程在处理这个业务流程，每个业务流程有自己独立的上下文，而不像 nodejs 等其它框架中使用的 callback 模式。

但框架已经提供了一个叫做 snlua 的用 C 开发的服务模块，它可以用来解析一段 Lua 脚本来实现业务逻辑。也就是说，你可以在 skynet 启动任意份 snlua 服务，只是它们承载的 Lua 脚本不同。这样，我们只使用 Lua 来进行开发就足够了。

## 4.ShareData

当你把业务拆分到多个服务中去后，数据如何共享，可能是最易面临的问题。

最简单粗暴的方法是通过消息传递数据。如果 A 服务需要 B 服务中的数据，可以由 B 服务发送一个消息，将数据打包携带过去。如果是一份数据，很多地方都需要获得它，那么用一个服务装下这组数据，提供一组查询接口即可。DataCenter 模块对此做了简单的封装。

datacenter 可用来在整个 skynet 网络做跨节点的数据共享。

如果你仅仅需要一组只读的结构信息分享给很多服务（比如一些配置数据），你可以把数据写到一个 lua 文件中，让不同的服务加载它。Cluster 的配置文件就是这样做的。注意：默认 skynet 使用自带的修改版 lua ，会缓存 lua 源文件。当一个 lua 文件通过 loadfile 加载后，磁盘上的修改不会影响下一次加载。所以你需要直接用 io.open 打开文件，再用 load 加载内存中的 string 。

另一个更好的方法是使用 sharedata 模块。

当大量的服务可能需要共享一大块并不太需要更新的结构化数据，每个服务却只使用其中一小部分。你可以设想成，这些数据在开发时就放在一个数据仓库中，各个服务按需要检索出需要的部分。

整个工程需要的数据仓库可能规模庞大，每个服务却只需要使用其中一小部分数据，如果每个服务都把所有数据加载进内存，服务数量很多时，就因为重复加载了大量不会触碰的数据而浪费了大量内存。在开发期，却很难把数据切分成更小的粒度，因为很难时刻根据需求的变化重新切分。

如果使用 DataCenter 这种中心式管理方案，却无法避免每次在检索数据时都要进行一次 RPC 调用，性能或许无法承受。

sharedata 模块正是为了解决这种需求而设计出来的。sharedata 只支持在同一节点内（同一进程下）共享数据，如果需要跨节点，需要自行同步处理。

如果一个服务生产了大量数据，想传给您一个服务消费，在同一进程下，是不必经过序列化过程，而只需要通过消息传递内存地址指针即可。这个优化存在 O(1) 和 O(n) 的性能差别，不可以无视。

**架构图**

![img](https://pic1.zhimg.com/80/v2-23fbfd8ebeb0dcb42a070e130e3b7480_720w.webp)

原文链接：https://zhuanlan.zhihu.com/p/342329953

作者：Hu先生的Linux

#  【NO.156】 设计模式之工厂设计模式

# 1.开发环境

- IDEA版本： 2022.1.4
- JDK版本：17.0.3

 

# 2.模式由来

## 2.1 自定义MailSender类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102526533-1287688327.png)

## 2.2 自定义Computer类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102544456-933689781.png)

## 2.3 分析图

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213103216889-954434698.png)

## 2.4 案例分析

- 由于Computer类和MailSender类之间的耦合度比较高，而且创建对象的代码太固定了，若希望使用短信发送的功能需要修改Computer类，这种修改违背了开闭原则，为了解决该问题，就可以将创建对象的工作移交出去，而工厂设计模式就是一种创建对象的设计模式，而且可以在创建对象时不对外暴露具体的创建逻辑。

 

# 3 普通工厂方法模式

## 3.1 基本概念

- 普通工厂方法模式就是建立一个工厂类，通过生产方法的参数来进行具体实例的创建。

## 3.2 自定义Sender接口

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102635389-1815819930.png)

## 3.3 修改MailSender类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102649414-57587530.png)

## 3.4 自定义SmsSender类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102719414-1997499446.png)

## 3.5 自定义SendFactory类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102732353-849206262.png)

## 3.6 修改Computer类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102746458-1732378018.png)

## 3.7 分析图

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213103217100-2038713447.png)

## 3.8 案例分析

- 在普通工厂方法模式中，若传递的字符串出错，则不能正确创建对象，而且可能出现空指针异常，所以容错率不高。

 

# 4.多个工厂方法模式

## 4.1 基本概念

- 多个工厂方法模式就是通过多个不同的生产方法对实现同一接口的不同实现类分别进行对象的创建。

## 4.2 修改SendFactory类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102823586-931852715.png)

## 4.3 修改Computer类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102836547-837935901.png)

## 4.4 分析图

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213103217099-1967070422.png)

## 4.5 案例分析

- 在多个工厂方法模式中，为了能够正确创建对象，需要先创建工厂类的对象才能调用工厂类中的生产方法。

 

# 5.静态工厂方法模式

## 5.1 基本概念

- 静态工厂方法模式就是将多个不同的生产方法加上static关键字提升为类层级，此时只需要通过类名.的方式就可以进行方法的调用，从而进行不同实例的创建。

## 5.2 修改SendFactory类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102912371-2099806645.png)

## 5.3 修改Computer类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102931067-656536113.png)

## 5.4 分析图

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213103217104-1842082073.png)

## 5.5 案例分析

- 静态工厂方法模式中类的创建依赖工厂类，如果想要扩展程序来生产新的产品，就必须对工厂类的代码进行修改，这就违背了开闭原则。

 

# 6.抽象工厂模式

## 6.1 基本概念

- 抽象工厂模式就是将多个不同的生产方法放在不同的工厂类中，让多个工厂类实现同一个接口，此时只需要通过不同的工厂类就可以进行不同实例的创建。

## 6.2 自定义SendFactory接口

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213102956517-1621069227.png)

## 6.3 自定义MailSendFactory类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213103008325-189179913.png)

## 6.4 自定义SmsSendFactory类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213103019013-41445781.png)

## 6.5 修改Computer类

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213103030995-591961697.png)

## 6.6 分析图

![img](https://img2023.cnblogs.com/blog/3038036/202212/3038036-20221213103217101-1976135503.png)

## 6.7 案例分析

- 现在想要拓展程序生产新的产品，就只需要增加新的工厂类即可，不用违背开闭原则，同时每个工厂类对应一个产品，符合单一职责的原则。

原文作者：[格子衫007](https://home.cnblogs.com/u/gezishan007/)

原文链接：https://www.cnblogs.com/gezishan007/p/16977899.html

# 【NO.157】 Nginx 防盗链

> 本篇主要介绍一下 nginx 中 防盗链的具体配置 , 以及http 的 referer 头

![image-20221205135337893](https://img2023.cnblogs.com/other/1898722/202212/1898722-20221213091423499-169252935.png)

## 1.概述

```
防盗链其实就是 防止别的站点来引用你的 资源, 占用你的流量
```

在了解nginx 防盗链之前 我们先了解一下 什么是 HTTP 的头信息 Referer,当浏览器访问网站的时候,一般会带上Referer,`告诉后端该是从哪个页面过来的`

nginx的 防盗链'功能基于 HTTP协议的Referer机制,通过判断Referer对来源进行 识别和判断 做出一定的处理

nginx会通就过`查看referer自动和valid_referers后面的内容进行匹配`，如果匹配到了就将`invalid_referer 变量置位1` ， 如果没有匹配到 ， 则 将 invalid_referer变量置0 , 匹配的过程中不区分大小写.

| 语法   | valid_referers none blocked server_names string… |
| ------ | ------------------------------------------------ |
| 默认值 | —                                                |
| 位置   | server、location                                 |

## 2.Nginx 防盗链演示

这里我拿图片等资源 作为案例演示

### 2.1 未配置 防盗链的情况

> 如果你的图片没有做防盗链的控制 , 像如下配置一样, 那么其他人就可以直接使用你的文件图片等等

```properties
http {
    include       mime.types;

    default_type  application/octet-stream;

    sendfile        on;

    keepalive_timeout  65;


       server {
        listen       80;
        server_name  www.testfront.com;
				
        location ~* .(gif|jpg|png) {
           root /www/static;
        }

        location / {
            proxy_pass http://www.testbackend.com;
        }
    }

}
```

可以看到在其他机器上如 www.testbackend.com 直接引入 www.testfront.com的资源文件 , 也是可以展示的

![image-20221205132512265](https://img2023.cnblogs.com/other/1898722/202212/1898722-20221213091423917-1213037371.png)

### 2.2 配置nginx防盗链

如果配置了valid_referers

nginx会通就过`查看referer自动和valid_referers后面的内容进行匹配`，如果匹配到了就将`invalid_referer 变量置位1` ， 如果没有匹配到 ， 则 将 invalid_referer变量置0 , 匹配的过程中不区分大小写.

```properties
    location ~* .(gif|jpg|png) {
       # 配置校验 referer , 意思就是如果referer 是172.16.225.111 或者 www.testfront.com 都通过
       valid_referers 172.16.225.111 www.testfront.com;
       if ($invalid_referer) {
          return 403;
       }
       root /www/static;
    }
```

此时再访问 www.testbackend.com 去引用 www.testfront.com 的资源 就不能访问了

![image-20221205133441066](https://img2023.cnblogs.com/other/1898722/202212/1898722-20221213091424461-1384825207.png)

## 3. 防盗链的 具体配置

从上面可以看出, 通过配置 valid_referers 后面添加校验的域名和ip , nginx 会自动进行 http的 referer 的匹配

```
防盗链除了可以配置 ip 域名外, 还能配置 如 none 和 blocked
```

- **none**: 如果Header中的Referer为空，允许访问

- blocked:在Header中的Referer不为空，但是该值被防火墙或代理进行伪装过，如不带"http://" 、"https://"等协议头的资源允许访问。

- server_names:指定具体的域名或者IP

  可以支持正则表达式和*的字符串。如果是正则表达式，需要以~开头表示

## 4.扩展Curl 访问

可以通过curl 直接进行访问 并且指定 referer 来快速测试

```
curl -i 只看返回头信息
```

![image-20221205134817848](https://img2023.cnblogs.com/other/1898722/202212/1898722-20221213091424753-2031673148.png)

```
curl -e "" 指定 referer
```

![image-20221205134908232](https://img2023.cnblogs.com/other/1898722/202212/1898722-20221213091425637-297318403.png)

## 5.总结

本篇主要介绍了 nginx中如何配置 防盗链, 来限制别人随意的引用你的资源 造成占用你的网络资源情况, 配置还是毕竟简单的.

原文作者：[Johnny小屋](https://www.askajohnny.com/)

原文链接：https://www.cnblogs.com/askajohnny/p/16977685.html

# 【NO.158】一文读懂数据库优化之分库分表

> 本文从 5W1H 角度介绍了分库分表手段，其在解决如 IO 瓶颈、读写性能、物理存储瓶颈、内存瓶颈、单机故障影响面等问题的同时也带来如事务性、主键冲突、跨库 join、跨库聚合查询等问题。anyway，在综合业务场景考虑，正如缓存的使用一样，本着非必须勿使用原则。如数据库确实成为性能瓶颈时，在设计分库分表方案时也应充分考虑方案的扩展性，或者考虑采用成熟热门的分布式数据库解决方案，如 TiDB。

阅读此文你将了解：

- 什么是分库分表以及为什么分库分表
- 如何分库分表
- 分库分表常见几种方式以及优缺点
- 如何选择分库分表的方式



## 1.数据库常见优化方案

对于后端程序员来说，绕不开数据库的使用与方案选型，那么随着业务规模的逐渐扩大，其对于存储的使用上也需要随之进行升级和优化。

随着规模的扩大，数据库面临如下问题：

- 读压力：并发 QPS、索引不合理、SQL 语句不合理、锁粒度
- 写压力：并发 QPS、事务、锁粒度
- 物理性能：磁盘瓶颈、CPU 瓶颈、内存瓶颈、IO 瓶颈
- 其他：宕机、网络异常

面对上述问题，常见的优化手段有：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4kAtbLbmBK0zHX53rZ5lBFQ8yrf8XfRHHrS66fL3VNBA3ycWYYyGy8AQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

索引优化、主从同步、缓存、分库分表每个技术手段都可以作为一个专题进行讲解，本文主要介绍分库分表的技术方案实现。



## 2.什么是分库分表？

对于阅读本文的读者来说，分库分表概念应该并不会陌生，其拆开来讲是分库和分表两个手段：

- 分表：将一个表中的数据按照某种规则分拆到多张表中，**降低锁粒度以及索引树**，提升数据查询效率。
- 分库：将一个数据库中的数据按照某种规则分拆到多个数据库中，以**缓解单服务器的压力**（CPU、内存、磁盘、IO）。



## 3.为什么分库分表？

- **性能角度：CPU、内存、磁盘、IO 瓶颈**

- - 随着业务体量扩大，数据规模达到百万行，数据库索引树庞大，查询性能出现瓶颈。
  - 用户并发流量规模扩大，由于单库(单服务器)物理性能限制也无法承载大流量。

- **可用性角度：单机故障率影响面**

- - 如果是单库，数据库宕机会导致 100%服务不可用，N 库则可以将影响面降低 N 倍。

- 

## 4.分库分表带来的问题？

- **事务性问题**

- - 方案一：在进行分库分表方案设计过程中，从业务角度出发，尽可能保证一个事务所操作的表分布在一个库中，从而实现数据库层面的事务保证。

  - 方案二：方式一无法实现的情况下，业务层引入分布式事务组件保证事务性，如事务性消息、TCC、Seata 等分布式事务方式实现数据最终一致性。

  - 分库**可能**导致执行一次事务所需的数据分布在不同服务器上，数据库层面无法实现事务性操作，需要更上层业务引入分布式事务操作，难免会给业务带来一定复杂性，那么要想解决事务性问题一般有两种手段：

    

- **主键(自增 ID)唯一性问题**

- - 在数据库表设计时，经常会使用自增 ID 作为数据主键，这就导致后续在迁库迁表、或者分库分表操作时，会因为主键的变化或者主键不唯一产生冲突，要解决主键不唯一问题，有如下方案：
  - 方案一：自增 ID 做主键时，设置自增步长，采用等差数列递增，避免各个库表的主键冲突。但是这个方案仍然无法解决迁库迁表、以及分库分表扩容导致主键 ID 变化问题
  - 方案二：主键采用全局统一 ID 生成机制：如 UUID、雪花算法、数据库号段等方式。

- **跨库多表 join 问题**

- - 首先来自大厂 DBA 的建议是，线上服务尽可能不要有表的 join 操作，join 操作往往会给后续的分库分表操作带来各种问题，可能导致数据的死锁。可以采用多次查询业务层进行数据组装(需要考虑业务上多次查询的事务性的容忍度)

- **跨库聚合查询问题**

分库分表会导致常规聚合查询操作，如 group by，order by 等变的异常复杂。需要复杂的业务代码才能实现上述业务逻辑，其常见操作方式有：

§ 方案一：赛道赛马机制，每次从 N 个库表中查询出 TOP N 数据，然后在业务层代码中进行聚合合并操作。

```
§  假设： 以2库1表为例，每次分页查询N条数据。
§
§  第一次查询：
§  ① 每个表中分别查询出N条数据：
§  SELECT * FROM db1_table1 where $col > 0 order by $col   LIMITT  0,N
§  SELECT * FROM db2_table1 where $col > 0 order by $col   LIMITT  0,N
§  ② 业务层代码对上述两者做归并排序，假设最终取db1数据K1条，取db2数据K2条，则K1+K2 = N
§  此时的DB1 可以计算出OffSet为K1 ，DB2计算出Offset为K2
§  将获取的N条数据以及相应的Offset  K1/K2返回给 端上。
§
§  第二次查询：
§  ① 端上将上一次查询对应的数据库的Offset  K1/K2 传到后端
§  ② 后端根据Offset构造查询语句查询分别查询出N条语句
§  SELECT * FROM db1_table1 where $col > 0 order by $col   LIMITT  $K1,N
§  SELECT * FROM db2_table1 where $col > 0 order by $col   LIMITT  $K2,N
§  ③ 再次使用归并排序，获取TOP N数据，将获取的N条数据以及相应的Offset  K1/K2返回给 端上。
§
§  第三次查询:
依次类推.......
```

§ 方案二：可以将经常使用到 groupby,orderby 字段存储到一个单一库表(可以是 REDIS、ES、MYSQL)中，业务代码中先到单一表中根据查询条件查询出相应数据，然后根据查询到的主键 ID，到分库分表中查询详情进行返回。2 次查询操作难点会带来接口耗时的增加，以及极端情况下的数据不一致问题。



## 5.什么是好的分库分表方案？

- **满足业务场景需要**：根据业务场景的不同选择不同分库分表方案：比如按照时间划分、按照用户 ID 划分、按照业务能力划分等

- **方案可持续性**：

- - 何为可持续性？其实就是：业务数据量级和流量量级未来进一步达到新的量级的时候，我们的分库分表方案可以持续灵活扩容处理。

- **最小化数据迁移**：扩容时一般涉及到历史数据迁移，其扩容后需要迁移的数据量越小其可持续性越强，理想的迁移前后的状态是（同库同表>同表不同库>同库不同表>不同库不同表）

- **数据偏斜**：数据在库表中分配的均衡性，尽可能保证数据流量在各个库表中保持等量分配，避免热点数据对于单库造成压力。

- - 最大数据偏斜率：（数据量最大样本 - 数据量最小样本）/ 数据量最小样本。一般来说，如果我们的最大数据偏斜率在 5%以内是可以接受的。



## 6.如何分库分表

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4kb0OqpTPHib4yazrLDll9KIBTtDib1Y2F3m54hjuSSGBXNZBTGqNevy2Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 6.1 垂直拆分

- 垂直拆表

- - 即大表拆小表，将一张表中数据不同”字段“分拆到多张表中，比如商品库将商品基本信息、商品库存、卖家信息等分拆到不同库表中。
  - 考虑因素有将**不常用**的，**数据较大**，**长度较长**（比如 text 类型字段）的拆分到“扩展表“，表和表之间通过”主键外键“进行关联。
  - 好处：降低表数据规模，提升查询效率，也避免查询时数据量太大造成的“跨页”问题。

- 垂直拆库

- - 垂直拆库则在垂直拆表的基础上，将一个系统中的不同业务场景进行拆分，比如订单表、用户表、商品表。
  - 好处：降低单数据库服务的压力（物理存储、内存、IO 等）、降低单机故障的影响面

### 6.2 水平拆分

- 操作：将总体数据按照某种维度(时间、用户)等分拆到多个库中或者表中，典型特征不同的库和表结构完全一下，如订单按照(日期、用户 ID、区域)分库分表。

- 水平拆表

- - 将数据按照某种维度拆分为多张表，但是由于多张表还是从属于**一个库**，其降低**锁粒度**，一定程度提升查询性能，但是仍然会有 IO 性能瓶颈。

- 水平拆库

- - 将数据按照某种维度分拆到多个库中，降低单机单库的压力，提升读写性能。

### 6.3 **常见水平拆分手段**

#### **6.3.1 range 分库分表**

顾名思义，该方案根据数据范围划分数据的存放位置。

##### 6.3.1.1 思路一：时间范围分库分表

举个最简单例子，我们可以把订单表按照年份为单位，每年的数据存放在单独的库（或者表）中。

时下非常流行的分布式数据库：TiDB 数据库，针对 TiKV 中数据的打散，也是基于 Range 的方式进行，将不同范围内的[StartKey,EndKey)分配到不同的 Region 上。

缺点：

- 需要提前建库或表。
- 数据热点问题：当前时间的数据会集中落在某个库表。
- 分页查询问题：涉及到库表中间分界线查询较复杂。

例子：交易系统流水表则是按照天级别分表。

#### **6.3.2 hash 分库分表**

hash 分表是使用最普遍的使用方式，其根据“主键”进行 hash 计算数据存储的库表索引。原理可能大家都懂，但有时拍脑袋决定的分库分表方案可能会导致严重问题。

##### **6.3.2.1 思路一：独立 hash**

对于分库分表，最常规的一种思路是通过主键计算 hash 值，然后 hash 值分别对库数和表数进行取余操作获取到库索引和表索引。比如：电商订单表，按照用户 ID 分配到 10 库 100 表中。

```
const (
        // DbCnt 库数量
        DbCnt = 10
        // TableCnt 表数量
        TableCnt = 100
)

// GetTableIdx 根据用户 ID 获取分库分表索引
func GetTableIdx(userID int64) (int64, int64) {
    hash := hashCode(userID)
        return hash % DbCnt, hash % TableCnt
}
```

上述是伪代码实现，大家可以先思考一下上述代码可能会产生什么问题？

比如 1000? 1010?，1020 库表索引是多少？

思考一下........

思考一下........

思考一下........

思考一下........

思考一下........

思考一下........

答：数据偏斜问题。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4kRpn6Tia94ialUTYjr0scxhmiaUQtvoev5sofeA0vE8dv8TazNhVZHTibTg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

非互质关系导致的数据偏斜问题证明：

```
假设分库数分表数最大公约数为a，则分库数表示为 m*a , 分表数为 n*a （m,n为正整数）

某条数据的hash规则计算的值为H，

若某条数据在库D中，则H mod (m*a) == D 等价与  H=M*m*a+D （M为整数）

则表序号为 T = H % (n*a) = (M*m*a+D)%(n*a)

如果D==0 则T= [(M*m)%n]*a
```

##### **6.3.2.2 思路二：统一 hash**

思路一中，由于库和表的 hash 计算中存在公共因子，导致数据偏斜问题，那么换种思考方式：10 个库 100 张表，一共 1000 张表，那么从 0 到 999 排序，根据 hash 值对 1000 取余，得到[0,999]的索引，似乎就可以解决数据偏斜问题：

```
// GetTableIdx 根据用户 ID 获取分库分表索引
// 例子：1123011 -> 1,1
func GetTableIdx(userID int64) (int64, int64) {
    hash := hashCode(userID)
    slot := DbCnt * TableCnt
        return hash % slot % DbCnt, hash % slot / DbCnt
}
```

上面会带来的问题？

比如 1123011 号用户，扩容前是 1 库 1 表，扩容后是 0 库 11 表

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4kBvEPafQaEgGezjNLicmVM5icSOTuJaA51zhI6Gic2LxU03gdSERgLmGrg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

扩展性问题证明。

```
某条数据的hash规则计算的值为H，分库数为D，分表数为T

扩容前：
分片序号K1 = H % (D*T),则H = M*DT + K1 ，且K1 一定是小于（D*T）
D1 = K1 % D
T1 = K1 / D

扩容后：
如果M为偶数，即M= 2*N
K2 = H% (2DT) = (2NDT+K1)%(2DT) = K1%(2DT) ,K1 一定小于（2DT）,所以K2=K1
D2 = K2%（2D） = K1 %（2D）
T2 = K2/(2D) = K1 / （2D）

如果M为奇数，即M = 2*N+1
K2 = H%（2DT） = (2NDT +DT +K1)%(2DT) = (DT+K1)%(2DT) = DT + K1
D2 = K2 %(2D) = (DT+K1) % (2D)
T2 = K2 /(2D) = (DT+K1) / (2D)

结论：扩容后库序号和表序号都变化
```

##### **6.3.2.3 思路三：二次分片法**

思路二中整体思路正确，只是最后计算库序号和表序号的时候，使用了库数量作为影响表序号的因子，导致扩容时表序号偏移而无法进行。事实上，我们只需要换种写法，就能得出一个比较大众化的分库分表方案。

```
func GetTableIdx(userId int64){
        //①算Hash
        hash:=hashCode(userId)
        //②分片序号
        slot:=hash%(DbCnt*TableCnt)
        //③重新修改二次求值方案
        dbIdx:=slot/TableCnt
        tblIdx:=slot%TableCnt
        return dbIdx,tblIdx
}
```

从上述代码中可以看出，其唯一不同是在计算库索引和表索引时，采用 TableCnt 作为基数（注：扩容操作时，一般采用库个数 2 倍扩容），这样在扩容时，表个数不变，则表索引不会变。

可以做简要的证明：

```
某条数据的hash规则计算的值为H，分库数为D，分表数为T

扩容前：
分片序号K1 = H % (D*T),则H =  M*DT + K1 ，且K1 一定是小于（D*T）
D1 = K1 / T
T1 = K1 % T

扩容后：
如果M为偶数，即M= 2*N
K2 =  H% (2DT) = (2NDT+K1)%(2DT) = K1%(2DT) ,K1 一定小于（2DT）,所以K2=K1
D2 = K2/T  = K1 /T = D1
T2 = K2%T = K1 % T = T1

如果M为奇数，即M = 2*N+1
K2 = H%（2DT） = (2NDT +DT +K1)%(2DT) = (DT+K1)%(2DT) = DT + K1
D2 = K2 /T = (DT+K1) / T = D + K1/T = D + D1
T2 = K2 %T = (DT+K1) % T = K1 %T = T1

结论：
M为偶数时，扩容前后库序号和表序号都不变
M为奇数时，扩容前后表序号不变，库序号会变化。
```

##### **6.3.2.4 思路四：基因法**

由思路二启发，我们发现案例一不合理的主要原因，就是因为库序号和表序号的计算逻辑中，有公约数这个因子在影响库表的独立性。那么我们是否可以换一种思路呢？我们使用相对独立的 Hash 值来计算库序号和表序号呢？

```
func GetTableIdx(userID int64)(int64,int64){
        hash := hashCode(userID)
        return atoi(hash[0:4]) % DbCnt,atoi(hash[4:])%TableCnt
}
```

这也是一种常用的方案，我们称为基因法，即使用原分片键中的某些基因（例如前四位）作为库的计算因子，而使用另外一些基因作为表的计算因子。

在使用基因法时，要主要计算 hash 值的片段保持充分的随机性，避免造成严重数据偏斜问题。

##### **6.3.2.5 思路五：关系表冗余**

按照索引的思想，可以通过分片的键和库表索引建立一张索引表，我们把这张索引表叫做“路由关系表”。每次查询操作，先去路由表中查询到数据所在的库表索引，然后再到库表中查询详细数据。同时，对于写入操作可以采用随机选择或者顺序选择一个库表进入写入。

那么由于路由关系表的存在，我们在数据扩容时，无需迁移历史数据。同时，我们可以为每个库表指定一个权限，通过权重的比例调整来调整每个库表的写入数据量。从而实现库表数据偏斜率调整。

此种方案的缺点是每次查询操作，需要先读取一次路由关系表，所以请求耗时可能会有一定增加。本身由于写索引表和写库表操作是不同库表写操作，需要引入分布式事务保证数据一致性，极端情况可能带来数据的不一致。

且索引表本身没有分库分表，自身可能会存在性能瓶颈，可以通过存储在 redis 进行优化处理。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4kBYhPNdLO8Omxnk08Om1iag8KQvwJ9HBwRxqhcyW4uXEiarrXd01xCRgg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

##### **6.3.2.6 思路六：分段索引关系表**

思路五中，需要将全量数据存在到路由关系表中建立索引，再结合 range 分库分表方案思想，其实有些场景下完全没有必要全部数据建立索引，可以按照号段式建立区间索引，我们可以将分片键的区间对应库的关系通过关系表记录下来，每次查询操作，先去路由表中查询到数据所在的库表索引，然后再到库表中查询详细数据。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4k8oEVZ8Uia1atVca7YoKAg8PcEdW6QgaVH3ibkWCM8IdQj7MicklZ4vcrg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

##### **6.3.2.7 思路七：一致性 Hash 法**

一致性 Hash 算法也是一种比较流行的集群数据分区算法，比如 RedisCluster 即是通过一致性 Hash 算法，使用 16384 个虚拟槽节点进行每个分片数据的管理。关于一致性 Hash 的具体原理这边不再重复描述，读者可以自行翻阅资料。

其思想和思路五有异曲同工之妙。

## **7.总结**

本文从 5W1H 角度介绍了分库分表手段，其在**解决**如 IO 瓶颈、读写性能、物理存储瓶颈、内存瓶颈、单机故障影响面等**问题的同时**，**也带来**如事务性、主键冲突、跨库 join、跨库聚合查询**等问题**。anyway，在综合业务场景考虑，正如缓存的使用一样，非必须使用分库分表，则不应过度设计采用分库分表方案。如数据库确实成为性能瓶颈时，在设计分库分表方案时也应充分考虑方案的扩展性。或者说可以考虑采用成熟热门的分布式数据库解决方案，如 TiDB。

原文作者：tayroctang，腾讯 PCG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/lSxdx2QT3F9lZPZMxroQFA

# 【NO.159】深入剖析虚拟内存工作原理

## 0.**导言**

虚拟内存是当今计算机系统中最重要的抽象概念之一，它的提出是为了更加有效地管理内存并且降低内存出错的概率。虚拟内存影响着计算机的方方面面，包括硬件设计、文件系统、共享对象和进程/线程调度等等，每一个致力于编写高效且出错概率低的程序的程序员都应该深入学习虚拟内存。

本文全面而深入地剖析了虚拟内存的工作原理，帮助读者快速而深刻地理解这个重要的概念。

## 1.**计算机存储器**

存储器是计算机的核心部件之一，在完全理想的状态下，存储器应该要同时具备以下三种特性：

1. 速度足够快：存储器的存取速度应当快于 CPU 执行一条指令，这样 CPU 的效率才不会受限于存储器
2. 容量足够大：容量能够存储计算机所需的全部数据
3. 价格足够便宜：价格低廉，所有类型的计算机都能配备

但是现实往往是残酷的，我们目前的计算机技术无法同时满足上述的三个条件，于是现代计算机的存储器设计采用了一种分层次的结构：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM17ibwkuQSIPI5S5wlm7wmhfib75kn7kG7amB0XDs0icPPVn6kjficgjib6cw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

从顶至底，现代计算机里的存储器类型分别有：寄存器、高速缓存、主存和磁盘，这些存储器的速度逐级递减而容量逐级递增。存取速度最快的是寄存器，因为寄存器的制作材料和 CPU 是相同的，所以速度和 CPU 一样快，CPU 访问寄存器是没有时延的，然而因为价格昂贵，因此容量也极小，一般 32 位的 CPU 配备的寄存器容量是 32✖️32 Bit，64 位的 CPU 则是 64✖️64 Bit，不管是 32 位还是 64 位，寄存器容量都小于 1 KB，且寄存器也必须通过软件自行管理。

第二层是高速缓存，也即我们平时了解的 CPU 高速缓存 L1、L2、L3，一般 L1 是每个 CPU 独享，L3 是全部 CPU 共享，而 L2 则根据不同的架构设计会被设计成独享或者共享两种模式之一，比如 Intel 的多核芯片采用的是共享 L2 模式而 AMD 的多核芯片则采用的是独享 L2 模式。

第三层则是主存，也即主内存，通常称作随机访问存储器（Random Access Memory, RAM）。是与 CPU 直接交换数据的内部存储器。它可以随时读写（刷新时除外），而且速度很快，通常作为操作系统或其他正在运行中的程序的临时资料存储介质。

最后则是磁盘，磁盘和主存相比，每个二进制位的成本低了两个数量级，因此容量比之会大得多，动辄上 GB、TB，而缺点则是访问速度则比主存慢了大概三个数量级。机械硬盘速度慢主要是因为机械臂需要不断在金属盘片之间移动，等待磁盘扇区旋转至磁头之下，然后才能进行读写操作，因此效率很低。

## 2.**主存**

### 2.1 物理内存

我们平时一直提及的物理内存就是上文中对应的第三种计算机存储器，RAM 主存，它在计算机中以内存条的形式存在，嵌在主板的内存槽上，用来加载各式各样的程序与数据以供 CPU 直接运行和使用。

### 2.2 虚拟内存

在计算机领域有一句如同摩西十诫般神圣的哲言："**计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决**"，从内存管理、网络模型、并发调度甚至是硬件架构，都能看到这句哲言在闪烁着光芒，而虚拟内存则是这一哲言的完美实践之一。

虚拟内存是现代计算机中的一个非常重要的存储器抽象，主要是用来解决应用程序日益增长的内存使用需求：现代物理内存的容量增长已经非常快速了，然而还是跟不上应用程序对主存需求的增长速度，对于应用程序来说内存还是可能会不够用，因此便需要一种方法来解决这两者之间的容量差矛盾。为了更高效地管理内存并尽可能消除程序错误，现代计算机系统对物理主存 RAM 进行抽象，实现了***虚拟内存 (Virtual Memory, VM)\***技术。

## 3.**虚拟内存**

虚拟内存的核心原理是：为每个程序设置一段"连续"的虚拟地址空间，把这个地址空间分割成多个具有连续地址范围的页 (Page)，并把这些页和物理内存做映射，在程序运行期间动态映射到物理内存。当程序引用到一段在物理内存的地址空间时，由硬件立刻执行必要的映射；而当程序引用到一段不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令。

其实虚拟内存技术从某种角度来看的话，很像是糅合了基址寄存器和界限寄存器之后的新技术。它使得整个进程的地址空间可以通过较小的虚拟单元映射到物理内存，而不需要为程序的代码和数据地址进行重定位。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM1dJO32Z9NEDPa7UYLOF49ial5ezZjueUdLEZoibrFKFWzkoP4KiaBY8IJQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

虚拟地址空间按照固定大小划分成被称为页（Page）的若干单元，物理内存中对应的则是页框（Page Frame）。这两者一般来说是一样的大小，如上图中的是 4KB，不过实际上计算机系统中一般是 512 字节到 1 GB，这就是虚拟内存的分页技术。因为是虚拟内存空间，每个进程分配的大小是 4GB (32 位架构)，而实际上当然不可能给所有在运行中的进程都分配 4GB 的物理内存，所以虚拟内存技术还需要利用到一种 `交换（swapping）`技术，也就是通常所说的页面置换算法，在进程运行期间只分配映射当前使用到的内存，暂时不使用的数据则写回磁盘作为副本保存，需要用的时候再读入内存，动态地在磁盘和内存之间交换数据。

### 3.1 页表

页表（Page Table），每次进行虚拟地址到物理地址的映射之时，都需要读取页表，从数学角度来说页表就是一个函数，入参是虚拟页号（Virtual Page Number，简称 VPN），输出是物理页框号（Physical Page Number，简称 PPN，也就是物理地址的基址）。

页表由多个页表项（Page Table Entry, 简称 PTE）组成，页表项的结构取决于机器架构，不过基本上都大同小异。一般来说页表项中都会存储物理页框号、修改位、访问位、保护位和 "在/不在" 位（有效位）等信息。

- 物理页框号：这是 PTE 中最重要的域值，毕竟页表存在的意义就是提供 VPN 到 PPN 的映射。
- 有效位：表示该页面当前是否存在于主存中，1 表示存在，0 表示缺失，当进程尝试访问一个有效位为 0 的页面时，就会引起一个缺页中断。
- 保护位：指示该页面所允许的访问类型，比如 0 表示可读写，1 表示只读。
- 修改位和访问位：为了记录页面使用情况而引入的，一般是页面置换算法会使用到。比如当一个内存页面被程序修改过之后，硬件会自动设置修改位，如果下次程序发生缺页中断需要运行页面置换算法把该页面调出以便为即将调入的页面腾出空间之时，就会先去访问修改位，从而得知该页面被修改过，也就是脏页 (Dirty Page)，则需要把最新的页面内容写回到磁盘保存，否则就表示内存和磁盘上的副本内容是同步的，无需写回磁盘；而访问位同样也是系统在程序访问页面时自动设置的，它也是页面置换算法会使用到的一个值，系统会根据页面是否正在被访问来觉得是否要淘汰掉这个页面，一般来说不再使用的页面更适合被淘汰掉。
- 高速缓存禁止位：用于禁止页面被放入 CPU 高速缓存，这个值主要适用于那些映射到寄存器等实时 I/O 设备而非普通主存的内存页面，这一类实时 I/O 设备需要拿到最新的数据，而 CPU 高速缓存中的数据可能是旧的拷贝。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM13fT8bDfxH4Y6g0QaGuGwiaVibys4ZlFG98llT3m52sK2xwdx6w9MrddA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 3.2 地址翻译

进程在运行期间产生的内存地址都是虚拟地址，如果计算机没有引入虚拟内存这种存储器抽象技术的话，则 CPU 会把这些地址直接发送到内存地址总线上，然后访问和虚拟地址相同值的物理地址；如果使用虚拟内存技术的话，CPU 则是把这些虚拟地址通过地址总线送到内存管理单元（Memory Management Unit，简称 MMU），MMU 将虚拟地址翻译成物理地址之后再通过内存总线去访问物理内存：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM1wOG9akEibhHiaN5JvLicOxPYlMcIN2xuVja4GOjnibJnhNaiazgbh5ogwRg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

虚拟地址（比如 16 位地址 8196=0010 000000000100）分为两部分：虚拟页号（Virtual Page Number，简称 VPN，这里是高 4 位部分）和偏移量（Virtual Page Offset，简称 VPO，这里是低 12 位部分），虚拟地址转换成物理地址是通过页表（page table）来实现的。

这里我们基于一个例子来分析当页面命中时，计算机各个硬件是如何交互的：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM1PHxicmich1NiccVx4ZXn1qmPwDE5vDfs9X53oEu70vnnAp3gknZ8YzibyQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

- **第 1 步**：处理器生成一个虚拟地址 VA，通过总线发送到 MMU；
- **第 2 步**：MMU 通过虚拟页号得到页表项的地址 PTEA，通过内存总线从 CPU 高速缓存/主存读取这个页表项 PTE；
- **第 3 步**：CPU 高速缓存或者主存通过内存总线向 MMU 返回页表项 PTE；
- **第 4 步**：MMU 先把页表项中的物理页框号 PPN 复制到寄存器的高三位中，接着把 12 位的偏移量 VPO 复制到寄存器的末 12 位构成 15 位的物理地址，即可以把该寄存器存储的物理内存地址 PA 发送到内存总线，访问高速缓存/主存；
- **第 5 步**：CPU 高速缓存/主存返回该物理地址对应的数据给处理器。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM1iciam3ptaJUmGlciaKyGLzdVlwJlnBiaeUic4hYry4unRLAsKmoicSJoQtdA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

在 MMU 进行地址转换时，如果页表项的有效位是 0，则表示该页面并没有映射到真实的物理页框号 PPN，则会引发一个**缺页中断**，CPU 陷入操作系统内核，接着操作系统就会通过页面置换算法选择一个页面将其换出 (swap)，以便为即将调入的新页面腾出位置，如果要换出的页面的页表项里的修改位已经被设置过，也就是被更新过，则这是一个脏页 (Dirty Page)，需要写回磁盘更新该页面在磁盘上的副本，如果该页面是"干净"的，也就是没有被修改过，则直接用调入的新页面覆盖掉被换出的旧页面即可。

缺页中断的具体流程如下：

- **第 1 步到第 3 步**：和前面的页面命中的前 3 步是一致的；
- **第 4 步**：检查返回的页表项 PTE 发现其有效位是 0，则 MMU 触发一次缺页中断异常，然后 CPU 转入到操作系统内核中的缺页中断处理器；
- **第 5 步**：缺页中断处理程序检查所需的虚拟地址是否合法，确认合法后系统则检查是否有空闲物理页框号 PPN 可以映射给该缺失的虚拟页面，如果没有空闲页框，则执行页面置换算法寻找一个现有的虚拟页面淘汰，如果该页面已经被修改过，则写回磁盘，更新该页面在磁盘上的副本；
- **第 6 步**：缺页中断处理程序从磁盘调入新的页面到内存，更新页表项 PTE；
- **第 7 步**：缺页中断程序返回到原先的进程，重新执行引起缺页中断的指令，CPU 将引起缺页中断的虚拟地址重新发送给 MMU，此时该虚拟地址已经有了映射的物理页框号 PPN，因此会按照前面『Page Hit』的流程走一遍，最后主存把请求的数据返回给处理器。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM1e0YEykjuYMA3CzJJfbfSb3h54rI21wf8rnPRiaia6nic7CB44wyNh2eibw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 3.3 虚拟内存和高速缓存

前面在分析虚拟内存的工作原理之时，谈到页表的存储位置，为了简化处理，都是默认把主存和高速缓存放在一起，而实际上更详细的流程应该是如下的原理图：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM18geiaK1MPibMUR0laew8vzEu9nKhTy3RAqVB7bzc1dvx4eEXYp9T4kRw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

如果一台计算机同时配备了虚拟内存技术和 CPU 高速缓存，那么 MMU 每次都会优先尝试到高速缓存中进行寻址，如果缓存命中则会直接返回，只有当缓存不命中之后才去主存寻址。

通常来说，大多数系统都会选择利用物理内存地址去访问高速缓存，因为高速缓存相比于主存要小得多，所以使用物理寻址也不会太复杂；另外也因为高速缓存容量很小，所以系统需要尽量在多个进程之间共享数据块，而使用物理地址能够使得多进程同时在高速缓存中存储数据块以及共享来自相同虚拟内存页的数据块变得更加直观。

### 3.4 加速翻译&优化页表

经过前面的剖析，相信读者们已经了解了虚拟内存及其分页&地址翻译的基础和原理。现在我们可以引入虚拟内存中两个核心的需求，或者说瓶颈：

- 虚拟地址到物理地址的映射过程必须要非常快，地址翻译如何加速。
- 虚拟地址范围的增大必然会导致页表的膨胀，形成大页表。

这两个因素决定了虚拟内存这项技术能不能真正地广泛应用到计算机中，如何解决这两个问题呢？

正如文章开头所说："**计算机科学领域的任何问题都可以通过增加一个间接的中间层来解决**"。因此，虽然虚拟内存本身就已经是一个中间层了，但是中间层里的问题同样可以通过再引入一个中间层来解决。

加速地址翻译过程的方案目前是通过引入页表缓存模块 -- TLB，而大页表则是通过实现多级页表或倒排页表来解决。

#### 3.4.1 **TLB 加速**

**翻译后备缓冲器**（Translation Lookaside Buffer，TLB），也叫快表，是用来加速虚拟地址翻译的，因为虚拟内存的分页机制，页表一般是保存在内存中的一块固定的存储区，而 MMU 每次翻译虚拟地址的时候都需要从页表中匹配一个对应的 PTE，导致进程通过 MMU 访问指定内存数据的时候比没有分页机制的系统多了一次内存访问，一般会多耗费几十到几百个 CPU 时钟周期，性能至少下降一半，如果 PTE 碰巧缓存在 CPU L1 高速缓存中，则开销可以降低到一两个周期，但是我们不能寄希望于每次要匹配的 PTE 都刚好在 L1 中，因此需要引入加速机制，即 TLB 快表。

TLB 可以简单地理解成页表的高速缓存，保存了最高频被访问的页表项 PTE。由于 TLB 一般是硬件实现的，因此速度极快，MMU 收到虚拟地址时一般会先通过硬件 TLB 并行地在页表中匹配对应的 PTE，若命中且该 PTE 的访问操作不违反保护位（比如尝试写一个只读的内存地址），则直接从 TLB 取出对应的物理页框号 PPN 返回，若不命中则会穿透到主存页表里查询，并且会在查询到最新页表项之后存入 TLB，以备下次缓存命中，如果 TLB 当前的存储空间不足则会替换掉现有的其中一个 PTE。

下面来具体分析一下 TLB 命中和不命中。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM1VHjRjticzTOken0dJRaf6l0lpC8BZnOnmZObSWUtc7MhLFrQY4iaz7Iw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

**TLB 命中**：

- **第 1 步**：CPU 产生一个虚拟地址 VA；
- **第 2 步和第 3 步**：MMU 从 TLB 中取出对应的 PTE；
- **第 4 步**：MMU 将这个虚拟地址 VA 翻译成一个真实的物理地址 PA，通过地址总线发送到高速缓存/主存中去；
- **第 5 步**：高速缓存/主存将物理地址 PA 上的数据返回给 CPU。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM1KN6Rk1KVV2EGiauibT27jS4A1p7zsnMa4vyDrnI4GAyVCahSGiciax1iaPw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

**TLB 不命中**：

- **第 1 步**：CPU 产生一个虚拟地址 VA；
- **第 2 步至第 4 步**：查询 TLB 失败，走正常的主存页表查询流程拿到 PTE，然后把它放入 TLB 缓存，以备下次查询，如果 TLB 此时的存储空间不足，则这个操作会汰换掉 TLB 中另一个已存在的 PTE；
- **第 5 步**：MMU 将这个虚拟地址 VA 翻译成一个真实的物理地址 PA，通过地址总线发送到高速缓存/主存中去；
- **第 6 步**：高速缓存/主存将物理地址 PA 上的数据返回给 CPU。

#### 3.4.2 **多级页表**

TLB 的引入可以一定程度上解决虚拟地址到物理地址翻译的开销问题，接下来还需要解决另一个问题：大页表。

理论上一台 32 位的计算机的寻址空间是 4GB，也就是说每一个运行在该计算机上的进程理论上的虚拟寻址范围是 4GB。到目前为止，我们一直在讨论的都是单页表的情形，如果每一个进程都把理论上可用的内存页都装载进一个页表里，但是实际上进程会真正使用到的内存其实可能只有很小的一部分，而我们也知道页表也是保存在计算机主存中的，那么势必会造成大量的内存浪费，甚至有可能导致计算机物理内存不足从而无法并行地运行更多进程。

这个问题一般通过**多级页表**（Multi-Level Page Tables）来解决，通过把一个大页表进行拆分，形成多级的页表，我们具体来看一个二级页表应该如何设计：假定一个虚拟地址是 32 位，由 10 位的一级页表索引、10 位的二级页表索引以及 12 位的地址偏移量，则 PTE 是 4 字节，页面 page 大小是 2^12 = 4KB，总共需要 2^20 个 PTE，一级页表中的每个 PTE 负责映射虚拟地址空间中的一个 4MB 的 chunk，每一个 chunk 都由 1024 个连续的页面 Page 组成，如果寻址空间是 4GB，那么一共只需要 1024 个 PTE 就足够覆盖整个进程地址空间。二级页表中的每一个 PTE 都负责映射到一个 4KB 的虚拟内存页面，和单页表的原理是一样的。

多级页表的关键在于，我们并不需要为一级页表中的每一个 PTE 都分配一个二级页表，而只需要为进程当前使用到的地址做相应的分配和映射。因此，对于大部分进程来说，它们的一级页表中有大量空置的 PTE，那么这部分 PTE 对应的二级页表也将无需存在，这是一个相当可观的内存节约，事实上对于一个典型的程序来说，理论上的 4GB 可用虚拟内存地址空间绝大部分都会处于这样一种未分配的状态；更进一步，在程序运行过程中，只需要把一级页表放在主存中，虚拟内存系统可以在实际需要的时候才去创建、调入和调出二级页表，这样就可以确保只有那些最频繁被使用的二级页表才会常驻在主存中，此举亦极大地缓解了主存的压力。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM1oTSydTxbqwWs9IangfZaic9NIjByEHu0y7aRfGAuYy5PfOC0IUDj4lA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

多级页表的层级深度可以按照需求不断扩充，一般来说，级数越多，灵活性越高。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM1XeLrG2JvvicCO0MKGjQyLZ8ZsIcarTMCLxBQHbyHWtjOrMF6MR0Y1oA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

比如有个一个 k 级页表，虚拟地址由 k 个 VPN 和 1 个 VPO 组成，每一个 VPN i 都是一个到第 i 级页表的索引，其中 1 <= i <= k。第 j 级页表中的每一个 PTE（1 <= j <= k-1）都指向第 j+1 级页表的基址。第 k 级页表中的每一个 PTE 都包含一个物理地址的页框号 PPN，或者一个磁盘块的地址（该内存页已经被页面置换算法换出到磁盘中）。MMU 每次都需要访问 k 个 PTE 才能找到物理页框号 PPN 然后加上虚拟地址中的偏移量 VPO 从而生成一个物理地址。这里读者可能会对 MMU 每次都访问 k 个 PTE 表示性能上的担忧，此时就是 TLB 出场的时候了，计算机正是通过把每一级页表中的 PTE 缓存在 TLB 中从而让多级页表的性能不至于落后单页表太多。

#### 3.4.3 **倒排页表**

另一种针对页式虚拟内存管理大页表问题的解决方案是**倒排页表**（Inverted Page Table，简称 IPT）。倒排页表的原理和搜索引擎的倒排索引相似，都是通过反转映射过程来实现。

在搜索引擎中，有两个概念：文档 doc 和 关键词 keyword，我们的需求是通过 keyword 快速找到对应的 doc 列表，如果搜索引擎的存储结构是正向索引，也即是通过 doc 映射到其中包含的所有 keyword 列表，那么我们要找到某一个指定的 keyword 所对应的 doc 列表，那么便需要扫描索引库中的所有 doc，找到包含该 keyword 的 doc，再根据打分模型进行打分，排序后返回，这种设计无疑是低效的；所以我们需要反转一下正向索引从而得到倒排索引，也即通过 keyword 映射到所有包含它的 doc 列表，这样当我们查询包含某个指定 keyword 的 doc 列表时，只需要利用倒排索引就可以快速定位到对应的结果，然后还是根据打分模型进行排序返回。

上面的描述只是搜索引擎倒排索引的简化原理，实际的倒排索引设计是要复杂很多的，有兴趣的读者可以自行查找资料学习，这里就不再展开。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM1F6rQvMopOH583eEuItzGrsiahB4LMYEalBFtjh4kmibA6cibhKf1Z0ZdQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

回到虚拟内存的倒排页表，它正是采用了和倒排索引类似的思想，反转了映射过程：前面我们学习到的页表设计都是以虚拟地址页号 VPN 作为页表项 PTE 索引，映射到物理页框号 PPN，而在倒排页表中则是以 PPN 作为 PTE 索引，映射到 (进程号，虚拟页号 VPN)。

倒排页表在寻址空间更大的 CPU 架构下尤其高效，或者应该说更适合那些『虚拟内存空间/物理内存空间』比例非常大的场景，因为这种设计是以实际物理内存页框作为 PTE 索引，而不是以远超物理内存的虚拟内存作为索引。例如，以 64 位架构为例，如果是单页表结构，还是用 12 位作为页面地址偏移量，也就是 4KB 的内存页大小，那么以最理论化的方式来计算，则需要 2^52 个 PTE，每个 PTE 占 8 个字节，那么整个页表需要 32PB 的内存空间，这完全是不可接受的，而如果采用倒排页表，假定使用 4GB 的 RAM，则只需要 2^20 个 PTE，极大减少内存使用量。

倒排页表虽然在节省内存空间方面效果显著，但同时却引入了另一个重大的缺陷：地址翻译过程变得更加低效。我们都清楚 MMU 的工作就是要把虚拟内存地址翻译成物理内存地址，现在索引结构变了，物理页框号 PPN 作为索引，从原来的 VPN --> PPN 变成了 PPN --> VPN，那么当进程尝试访问一个虚拟内存地址之时，CPU 在通过地址总线把 VPN 发送到 MMU 之后，基于倒排页表的设计，MMU 并不知道这个 VPN 对应的是不是一个缺页，所以不得不扫描整个倒排页表来找到该 VPN，而最要命的是就算是一个非缺页的 VPN，每次内存访问还是需要执行这个全表扫描操作，假设是前面提到的 4GB RAM 的例子，那么相当于每次都要扫描 2^20 个 PTE，相当低效。

这时候又是我们的老朋友 -- TLB 出场的时候了，我们只需要把高频使用的页面缓存在 TLB 中，借助于硬件，在 TLB 缓存命中的情况下虚拟内存地址的翻译过程就可以像普通页表那样快速，然而当 TLB 失效的时候，则还是需要通过软件的方式去扫描整个倒排页表，线性扫描的方式非常低效，因此一般倒排页表会基于哈希表来实现，假设有 1G 的物理内存，那么这里就一共有 2^18 个 4KB 大小的页框，建立一张以 PPN 作为 key 的哈希表，则可以划分成 2^18 个 4KB 大小的页框，假设 0 作为 PPN 的起点，则 [0, 2^18 - 1] 就是 PPN 的取值范围，以此作为 Hash map 的 key，然后实现一个哈希函数，使用 VPN 作为入参，使得哈希函数最后输出的哈希值落在 [0, 2^18 - 1] 区间内，每一个 key 值对应的 value 中存储的是 (VPN, PNN)，那么所有具有相同哈希值的 VPN 会被链接在一起形成一个冲突链，如果我们把哈希表的槽数设置成跟物理页框数量一致的话，那么这个倒排哈希表中的冲突链的平均长度将会是 1 个 PTE，可以大大提高查询速度。当 VPN 通过倒排页表匹配到 PPN 之后，这个 (VPN, PPN) 映射关系就会马上被缓存进 TLB，以加速下次虚拟地址翻译。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauNO6QadFJvbG1OjFtc6yM11K7mrbaE9oGKyD5SDJwiaqkp6p4K6RsMTrK1TI8s8lsvyDPqPMUM88A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

倒排页表在 64 位架构的计算机中很常见，因为在 64 位架构下，基于分页的虚拟内存中即便把页面 Page 的大小从一般的 4KB 提升至 4MB，依然需要一个拥有 2^42 个 PTE 的巨型页表放在主存中（理论上，实际上不会这么实现），极大地消耗内存。

## 4.**总结**

现在让我们来回顾一下本文的核心内容：虚拟内存是存在于计算机 CPU 和物理内存之间一个中间层，主要作用是高效管理内存并减少内存出错。虚拟内存的几个核心概念有：

1. **页表**：从数学角度来说页表就是一个函数，入参是虚拟页号 VPN，输出是物理页框号 PPN，也就是物理地址的基址。页表由页表项组成，页表项中保存了所有用来进行地址翻译所需的信息，页表是虚拟内存得以正常运作的基础，每一个虚拟地址要翻译成物理地址都需要借助它来完成。
2. **TLB**：计算机硬件，主要用来解决引入虚拟内存之后寻址的性能问题，加速地址翻译。如果没有 TLB 来解决虚拟内存的性能问题，那么虚拟内存将只可能是一个学术上的理论而无法真正广泛地应用在计算机中。
3. **多级页表和倒排页表**：用来解决虚拟地址空间爆炸性膨胀而导致的大页表问题，多级页表通过将单页表进行分拆并按需分配虚拟内存页而倒排页表则是通过反转映射关系来实现节省内存的效果。

最后，虚拟内存技术中还需要涉及到操作系统的页面置换机制，由于页面置换机制也是一个较为庞杂和复杂的概念，本文便不再继续剖析这一部分的原理，我们在以后的文章中再单独拿来讲解。

## **5.参考&延伸阅读**

本文的主要参考资料是《现代操作系统》和《深入理解计算机系统》这两本书的英文原版，如果读者还想更加深入地学习虚拟内存，可以深入阅读这两本书并且搜寻其他的论文资料进行学习。

原文作者：allanpan，腾讯 IEG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/c81Fvws0J2tHjcdTgxvv6g

# 【NO.160】Redis vs Tendis：冷热混合存储版架构揭秘

> Redis 作为高性能缓存被广泛应用到各个业务, 比如游戏的排行榜, 分布式锁等场景。经过在 IEG 的长期运营, 我们也遇到 Redis 一些痛点问题, 比如内存占用高, 数据可靠性差, 业务维护缓存和存储的一致性繁琐。由 腾讯互娱 CROS DBA 团队 & 腾讯云数据库团队联合研发的 Tendis 推出了: 缓存版 、 混合存储版 和 存储版 三种不同产品形态, 针对不同的业务需求, 本文主要介绍 混合存储版 的整体架构, 并且详细揭秘内部的原理。

## 0.**导语**

本文首先介绍腾讯 IEG 运营 Redis 遇到的一些痛点问题, 然后介绍由 腾讯互娱 CROS DBA 团队 & 腾讯云数据库团队联合研发的 Tendis 的三种不同的产品形态。最后重点介绍冷热混合存储版的架构, 并且重点介绍各个组件的功能特性。

## 1.**背景介绍**

### 1.1 Redis 有哪些痛点 ?

在使用的过程中, 主要遇到以下一些痛点问题:

- 内存成本高

- 1. 业务不同阶段对 QPS 要求不同 比如游戏业务, 刚上线的新游戏特别火爆, 为了支持上千万同时在线, 需要不断的进行扩容增加机器。运营一段时间后, 游戏玩家可能变少, 访问频率(QPS)没那么高, 依然占用大量机器, 维护成本很高。
  2. 需要为 Fork 预留内存 Redis 保存全量数据时, 需要 Fork 一个进程。Linux 的 fork 系统调用基于 Copy On Write 机制, 如果在此期间 Redis 有大量的写操作, 父子进程就需要各自维护一份内存。因此部署 Redis 的机器往往需要预留一半的内存。

- 缓存一致性的问题 对于 Redis + MySQL 的架构需要业务方花费大量的精力来维护缓存和数据库的一致性。

- 数据可靠性 Redis 本质上是一个内存数据库, 用户虽然可以使用 AOF 的 Always 来落盘保证数据可靠性, 但是会带来性能的大幅下降, 因此生产环境很少有使用。另外 不支持 回档, Master 故障后, 异步复制会造成数据的丢失。

- 异步复制 Redis 主备使用异步复制, 这个是异步复制固有的问题。主备使用异步复制, 响应延迟低, 性能高, 但是 Master 故障后, 会造成数据丢失。

### 1.2 Tendis 是什么 ?

Tendis 是集腾讯众多海量 KV 存储优势于一身的 Redis 存储解决方案, 并 100% 兼容 Redis 协议和 Redis4.0 所有数据模型。作为一个高可用、高性能的分布式 KV 存储数据库, 从访问时延、持久化需求、整体成本等不同维度的考量, Tendis 推出了 **缓存版** 、 **混合存储版** 和 **存储版** 三种不同产品形态，并将存储版开源。**感兴趣的小伙伴 可以去 Github 关注我们的项目: [Tencent/Tendis](https://github.com/Tencent/Tendis)**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgON0XiaI4JibmLpz61rIh465lpzcFic1kO2f0hS1tG9I30lNV4SDNMxE9LA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

**Tendis 缓存版**适用于对延迟要求特别敏感, 并且对 QPS 要求很高的业务。基于社区 Redis 4.0 版本进行定制开发。

**Tendis 存储版**适用于大容量, 延迟不敏感型业务, 数据全部存储在 磁盘, 适合温冷数据的存储。Tendis 存储版是腾讯互娱 CROS DBA 团队 & 腾讯云数据库团队 自主设计和研发的开源分布式高性能 KV 存储系统。另外在 可靠性、复制机制、并发控制、gossip 实现以及数据搬迁等做了大量的优化, 并且解决了一些 Redis cluster 比较棘手的问题。完全兼容 Redis 协议, 并使用 RocksDB 作为底层存储引擎。

**Tendis 冷热混合存储版**冷热混合存储 综合了缓存版和存储版的优点, 缓存层存放热数据, 全量数据存放在存储层。这既保证了热数据的访问性能，同时保证了全量数据的可靠性，同时热数据支持自动降冷。

## 2.**Tendis 冷热混合存储版 整体架构**

Tendis 冷热混合存储版主要由 **Proxy** 、**缓存层 Redis**、 **存储层 Tendis 存储版** 和 **同步层 Redis-sync** 组成, 其中每个组件的功能如下:

**Proxy 组件**: 负责对客户端请求进行路由分发，将不同的 Key 的命令分发到正确的分片，同时 Proxy 还负责了部分监控数据的采集，以及高危命令在线禁用等功能。

**缓存层 Redis Cluster**: 缓存层 Redis 基于 社区 Redis 4.0 进行开发。Redis 具有以下功能: 1) 版本控制 2) 自动将 冷数据从缓存层中淘汰, 将热数据从存储层加载到缓存层; 3) 使用 Cuckoo Filter 表示全量 Keys, 防止缓存穿透; 4) 基于 RDB+AOF 扩缩容方式, 扩缩容更加高效便捷。

**存储层 Tendis Cluster**: Tendis 存储版 是腾讯基于 RocksDB 自研的 兼容 Redis 协议的 KV 存储引擎, 该引擎已经在腾讯集团内部运营多年, 性能和稳定性得到了充分的验证。在混合存储系统中主要负责全量数据的存储和读取, 以及数据备份, 增量日志备份等功能。

**同步层 Redis-sync**: 1) 并行数据导入 `存储层 Tendis`; 2) 服务无状态, 故障重新拉起; 3) 数据自动路由。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOSpJv2ZoDzvSibqTNiaFiaCMbhsrcRPqBwiaWqgZBHRvaLkAwWMRiabyUg1g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

Tendis 冷热混合存储的一些重要特性介绍:

- `缓存层 Redis Cluster` 和 `存储层 Tendis Cluster` 分别进行扩缩容, 集群自治管理等。
- 冷数据自动降冷, 降低内存成本; 热数据自动缓存, 降低访问延迟

## **3.缓存层 Redis Cluster**

冷热混合存储缓存层 Redis 在社区版的基础上增加了以下功能:

- 版本控制
- 冷热数据交互
- Cuckoo Filter 避免缓存穿透
- 智能淘汰算法
- 基于 RDB+AOF 扩缩容

下面分别对这几个特性进行详细的讲解。

### 3.1 版本控制

首先基于社区版 Redis 改动是版本控制。我们为每个 Key 和 每条 Aof 增加一个 Version , 并且 Version 是单调递增的。在每次更新/新增一个 Key 后, 将当前节点的 Version 赋值给 Key 和 Value, 然后对全局的 Version++; 如下所示, 在 redisObject 中添加 64bits, 其中 48bits 用于版本控制。

```
typedef struct redisObject {
    unsigned type:4;
    unsigned encoding:4;
    unsigned lru:LRU_BITS; /* LRU time (relative to global lru_clock) or
                            * LFU data (least significant 8 bits frequency
                            * and most significant 16 bits access time). */
    int refcount;

    /* for hybrid storage */
    unsigned flag:4;                           /* OBJ_FLAG_... */
    unsigned reserved:4;
    unsigned counter:8;                        /* for cold-data-cache-policy */
    unsigned long long revision:REVISION_BITS; /* for value version */

    void *ptr;
} robj;
```

引入版本控制主要带来以下优势:

1. **增量 RDB**

> 社区版 Redis 主备在断线重连后, 如果 slave 发送的 psync_offset 对应的数据不在当前的 Master 的 **repl_backlog** 中, 则主备需要重新进行全量同步。再引入 Version 之后, slave 断线重连, 给 Master 发送 带 Version 的 `PSYNC replid psync_offset version`命令。如果出现上述情况, Master 将大于等于 Version 的数据生成增量 RDB, 发给 Slave, 进而解决需要增量, 同步比较慢的问题。

1. **Aof 的幂等**

> 如果同步层 Redis-sync 出现网络瞬断(短暂的和缓存层或者存储层断开), 作为一个无状态的同步组件, Redis-sync 会重新拉取未同步到 Tendis 的增量数据, 重新发送给 Tendis。每条 Aof 都具有一个 Version, Tendis 在执行的时候仅会执行比当前 Version 大的 Aof, 避免 aof 执行多次导致的数据不一致。

### 3.2 冷热数据交互

冷数据的恢复指当用户访问的 Key 不在缓存层, 需要将数据从存储层重新加载到缓存层。数据恢复这里是缓存层直接和存储层直接交互, 当冷 Keys 访问的请求比较大, 数据恢复很容易成为瓶颈, 因此为每个 Tendis 节点建立一个连接池, 专门负责与这个 Tendis 节点进行冷热数据恢复。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOwO1RJrODFlKHGcVOVLeLX6yvbsyLCO6qRHsBjgXZiaZhMYmD1RGKjOA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

用户访问一个 Key 的具体流程如下:

1. 首先判断 Key 是否在缓存层, 如果缓存层存在, 则执行命令; 如果缓存层不存在, 查询 Cuckoo Filter, 判断 Key 是否有可能在存储层;
2. 如果 Key 可能在存储层, 则向存储层发送 `dumpx dbid key withttl` 命令尝试从存储层获取数据, 并且阻塞当前请求的客户端;
3. 存储层收到 dumpx , 如果 Key 在存储层, 则向缓存层返回 `RESTOREEX dbid key ttl value`; 如果 Key 不在存储层(Cuckoo Filter 的误判), 则向缓存层返回 `DUMPXERROR key`;
4. 存储层收到 RESTOREEX 或者 DUMPXERROR 后, 将冷数据恢复。然后就可以唤醒阻塞的客户端, 执行客户端的请求。

### 3.3 Key 降冷 与 Cuckoo Filter

这里主要讲解混合存储从 1:1 版的缓存层缓存全量 Keys, 到 N:M 版的缓存层将 Key 和 Value 同时驱逐的演进, 以及我们引入 Cuckoo Filter 避免缓存穿透, 同时节省大量内存。

1. **Key 降冷的背景介绍**2020 年 6 月份上线的 1:1 版的冷热混合存储, 缓存层 Redis 存储全量的 Keys 和热 Values(**All Keys + Hot values**), 存储层 Tendis 存储全量的 Keys 和 Values(**All Keys + All values**)。在上线运行了一段时间后, 发现全量 Keys 的内存开销特别大, 冷热混合的收益并不明显。为了进一步释放内存空间, 提高缓存的效率, 我们放弃了 Redis 缓存全量 Keys 的方案, 驱逐的时候将 key 和 Value 都从缓存层淘汰。
2. **Cuckoo Filter 解决缓存击穿和缓存穿透**如果缓存层不存储全量的 Keys, 就会出现缓存击穿和缓存穿透的问题。为了解决这一问题, 缓存层引入 Cuckoo Filter 表示全量的 keys 。我们需要一个支持删除、可动态伸缩并且空间利用率高的 Membership Query 结构, 经过我们的调研和对比分析, 最终选择 `Dynamic Cuckoo Filter`。
3. **Dynamic Cuckoo Filter 实现**项目初期参考了 RedisBloom 中 Cuckoo Filter 的实现, 在开发的过程中也遇到了一些坑, RedisBloom 实现的 Cuckoo Filter 在删除的时候会出现误删, 最终给 RedisBloom 提 PR([Fix Cuckoo filter compact cause deleted by mistake #260](https://github.com/RedisBloom/RedisBloom/pull/260)) 修复了问题。
4. **Key 降冷的收益**最终采用将 Key 和 Value 同时从缓存层淘汰, 降低内存的收益很大。比如现网的一个业务, 总共有 6620 W 个 Keys , 在缓存全量 Keys 的时候 占用 18408 MB 的内存, 在 Key 降冷后 仅仅占用 593MB 。

#### **3.3.1 智能淘汰/加载策略**

作为冷热混合存储系统, 热数据在缓存层, 全量数据在存储层。关键的问题是淘汰和加载策略, 这里直接影响缓存的效率, 细分主要有两点: 1) **当缓存层内存满时, 选择哪些数据淘汰**; 2) **当用户访问存储层的数据时, 是否需要将其放入缓存层**。

1. 首先介绍混合存储的淘汰策略, 主要有以下两个淘汰策略:

- maxmemory-policy 当缓存层 Redis 内存使用到达 maxmemory, 系统将按照 maxmemory-policy 的内存策略将 Key/Value 从缓存层驱逐, 释放内存空间。(驱逐是指将 Key/Value 从缓存层中淘汰掉, 存储层 和 缓存层的 Cuckoo Filter 依然存在该 Key; )
- value-eviction-policy 如果配置 value-eviction-policy, 后台会定期将用户 N 天未访问的 Key/Value 被驱逐出内存;

1. 缓存加载策略 为了避免缓存污染的问题(比如类似 Scan 的访问, 遍历存储层的数据, 将缓存层真正的热数据淘汰, 从而造成了缓存效率低下) 。我们实现缓存加载策略: 仅仅将规定时间内访问频率超过某个阈值的数据加载到缓存中, 这里的时间和阈值都是可配置的。

#### **3.3.2 基于 RDB+AOF 扩缩容**

社区版 Redis 的扩容流程:

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgONYf9z4NRjLI1ouo8We0Uca40Cl4rRrlkh4KfuZbtd5ljU9jqibzmkmQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

社区版 Redis 扩容存在的一些问题:

1. importing 和 migrating 的设置不是原子的

> 先设置目标节点 slot 为 importing 状态, 再设置源节点的 slot 为 migrating 状态。如果反过来, 由于两次操作非原子: 源节点设置为 migrating , 目标节点还未设置 migrating 状态, 请求在这两个节点间反复 Move 。

1. 搬迁以 Key 为粒度, 效率较低

> Migrate 命令每次搬迁一个或者多个 Keys, 将整个 Slot 搬迁到目标节点需要多次网络交互。

1. 大 Key 问题

> 由于 Migrate 命令是同步命令, 在搬迁过程中是不能处理其他用户请求的, 因此可能会影响业务。(延迟时间波动较大)

由于社区版 Redis 存在的上述问题, 我们实现了基于 RDB+Aof 的扩缩容方式, 大致流程如下:

1. 管控添加新节点, 规划待搬迁 slots;
2. 管控端向目标节点下发 slot 同步命令: `cluster slotsync beginSlot endSlot [[beginSlot endSlot]...]`
3. 目标节点向源节点发送 `sync [slot ...]`, 命令请求同步 slot 数据
4. 源节点生成指定 slot 数据的一致性快照全量数据(RDB), 并将其发送给目标节点
5. 源节点开始持续发送增量数据(Aof)
6. 管控端定位获取源节点和目标节点的落后值 (diff_bytes), 如果落后值在指定的阈值内, 管控端向目标节点发送 `cluster slotfailover` (流程类似 Redis 的 cluster failover, 首先阻塞源节点写入, 然后等待目标节点和源节点的落后值为 0, 最后将 搬迁的 slots 归属目标节点)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgO57AusuE9uAJ7wG8VwlJBGVusGolKxtPVLibXCRrAAJtuOLtLSH6ntMQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## 4.**同步层 Redis-sync**

同步层 Redis-sync 模拟 Redis Slave 的行为, 接收 RDB 和 Aof, 然后并行地导入到存储层 Tendis。同步层主要需要解决以下问题:

- 并发地导入到存储层 Tendis, 如何保证时序正确 ?
- 特殊命令的处理, 比如 FLUSHALL/FLUSHDB/SWAPDB/SELECT/MULTI 等 ?
- 作为一个无状态的同步组件, 如何保证故障后, 数据断点续传 ?
- 缓存层和存储层 分别进行扩缩容, 如何将请求路由到正确的 Tendis 节点 ?

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOPDJgboyvMKoIFd6ObrpQ6LibaDsPrA4GuALWanJSfD8RsibrgwzkoRFA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

为了解决上述的三个问题, 我们实现了下面的功能:

- Slot 内串行, Slot 间并行 针对问题 1, Redis-sync 中采用与 Redis 相同的计算 Slot 的算法, 解析到具体的命令后, 根据 Key 所属的 slot, 将其放到对应的 队列中( slot%QueueSize )。因此同一个 Slot 的数据是串行写入, 不同 slot 的数据可以并行写入, 不会出现时序错乱的行为。
- 串并转换 针对问题 2, Redis-sync 会在并行和串行模式之间进行转换。比如收到 FLUSHDB 命令, 这是需要将 FLUSHDB 命令 前的命令都执行完, 再执行 FLUSHDB 命令。
- 定期上报 针对问题 3, Redis-sync 会定期将已发送给存储层的 aof 的 Version 持久化到 存储层。如何 Redis-sync 故障, 首先从 存储层获取上次已发送的位置, 然后向对应的 Redis 节点发送 psync, 请求同步。
- 数据自动路由 针对问题 4, Redis-sync 会定期从存储层获取 `Slot` 到 `Tendis 节点`的映射关系, 并且维护这些 Tendis 节点的连接池。请求从 缓存层到达, 然后计算请求所属的 slot, 然后发送到正确的 Tendis 节点。

## 5.**存储层 Tendis Cluster**

Tendis 是兼容 Redis 核心数据结构与协议的分布式高性能 KV 数据库, 主要具有以下特性:

- 兼容 Redis 协议 完全兼容 redis 协议，支持 redis 主要数据结构和接口，兼容大部分原生 Redis 命令。
- 持久化存储 使用 rocksdb 作为存储引擎，所有数据以特定格式存储在 rocksdb 中，最大支持 PB 级存储。
- 去中心化架构 类似于 redis cluster 的分布式实现，所有节点通过 gossip 协议通讯，可指定 hashtag 来控制数据分布和访问，使用和运维成本极低。
- 水平扩展 集群支持增删节点，并且数据可以按照 slot 在任意两节点之间迁移，扩容和缩容过程中对应用运维人员透明，支持扩展至 1000 个节点。
- 故障自动切换 自动检测故障节点，当故障发生后，slave 会自动提升为 master 继续对外提供服务。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOWEWwoVYcIECqTvpeplTIp8EXpmsCObmBcJ8e8G7ibcR4acyr4H0Lhuw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

**想要更深入了解 Tendis 的特性和使用可以去 Github 关注我们的项目：**

[**https://github.com/Tencent/Tendis**](https://github.com/Tencent/Tendis)

**我们的官方文档: [http://tendis.cn/#/ ](http://tendis.cn/#/)**



![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOSBHibvBuNvQbqEvGxY5cdcMsEw0lB9QxJ2iaoxonOS5ib1O9KVK41jKtA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

原文作者：jingjunli，腾讯 IEG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/MeYkfOIdnU6LYlsGb24KjQ



# 【NO.161】深入理解TCP/IP 协议栈

TCP/IP 协议栈是一系列网络协议的总和，是构成网络通信的核心骨架，它定义了电子设备如何连入因特网，以及数据如何在它们之间进行传输。TCP/IP 协议采用4层结构，分别是**应用层、传输层、网络层和链路层**，每一层都呼叫它的下一层所提供的协议来完成自己的需求。由于我们大部分时间都工作在应用层，下层的事情不用我们操心；其次网络协议体系本身就很复杂庞大，入门门槛高，因此很难搞清楚TCP/IP的工作原理，通俗一点讲就是，**一个主机的数据要经过哪些过程才能发送到对方的主机上**。 接下来，我们就来探索一下这个过程。

## **1.物理介质**

物理介质就是把电脑连接起来的物理手段，常见的有光纤、双绞线，以及无线电波，它决定了电信号(0和1)的传输方式，物理介质的不同决定了电信号的传输带宽、速率、传输距离以及抗干扰性等等。

TCP/IP协议栈分为四层，每一层都由特定的协议与对方进行通信，而**协议之间的通信最终都要转化为 0 和 1 的电信号，通过物理介质进行传输才能到达对方的电脑**，因此物理介质是网络通信的基石。

下面我们通过一张图先来大概了解一下TCP/IP协议的基本框架：

![img](https://pic4.zhimg.com/80/v2-d61906b4feb0889e3732dd29570008ab_720w.webp)

当通过http发起一个请求时，应用层、传输层、网络层和链路层的相关协议依次对该请求进行包装并携带对应的**首部**，最终在链路层生成**以太网数据包**，以太网数据包通过物理介质传输给对方主机，对方接收到数据包以后，然后再一层一层采用对应的协议进行拆包，最后把应用层数据交给应用程序处理。

网络通信就好比送快递，商品外面的一层层包裹就是各种协议，协议包含了商品信息、收货地址、收件人、联系方式等，然后还需要配送车、配送站、快递员，商品才能最终到达用户手中。

一般情况下，快递是不能直达的，需要先转发到对应的配送站，然后由配送站再进行派件。

配送车就是物理介质，配送站就是网关， 快递员就是路由器，收货地址就是IP地址，联系方式就是MAC地址。

快递员负责把包裹转发到各个配送站，配送站根据收获地址里的省市区，确认是否需要继续转发到其他配送站，当包裹到达了目标配送站以后，配送站再根据联系方式找到收件人进行派件。

有了整体概念以后，下面我们详细了解一下各层的分工。

1、链路层

网络通信就是把有特定意义的数据通过物理介质传送给对方，单纯的发送 0 和 1 是没有意义的，要传输有意义的数据，就需要以字节为单位对 0 和 1 进行分组，并且要标识好每一组电信号的信息特征，然后按照分组的顺序依次发送。以太网规定一组电信号就是一个数据包，一个数据包被称为**一帧，** 制定这个规则的协议就是**以太网协议**。一个完整的以太网数据包如下图所示：

![img](https://pic3.zhimg.com/80/v2-18f378bb56a46f06da5c5eeafb5fac76_720w.webp)

整个数据帧由**首部**、**数据**和**尾部**三部分组成，首部固定为14个字节，包含了目标MAC地址、源MAC地址和类型；数据最短为46个字节，最长为1500个字节，如果需要传输的数据很长，就必须分割成多个帧进行发送；尾部固定为4个字节，表示数据帧校验序列，用于确定数据包在传输过程中是否损坏。因此，以太网协议通过对电信号进行分组并形成数据帧，然后通过物理介质把数据帧发送给接收方。那么以太网如何来识接收方的身份呢？

以太网规协议定，接入网络的设备都必须安装网络适配器，即**网卡，** 数据包必须是从一块网卡传送到另一块网卡。而**网卡地址**就是数据包的发送地址和接收地址，也就是帧首部所包含的**MAC地址，**MAC地址是每块网卡的身份标识，就如同我们身份证上的身份证号码，具有全球唯一性。MAC地址采用十六进制标识，共6个字节， 前三个字节是厂商编号，后三个字节是网卡流水号，例如**4C-0F-6E-12-D2-19**

有了MAC地址以后，以太网采用**广播**形式，把数据包发给该**子网内**所有主机，子网内每台主机在接收到这个包以后，都会读取首部里的**目标MAC地址**，然后和自己的MAC地址进行对比，如果相同就做下一步处理，如果不同，就丢弃这个包。

所以链路层的主要工作就是**对电信号进行分组并形成具有特定意义的数据帧，然后以广播的形式通过物理介质发送给接收方。**

## **2.网络层**

对于上面的过程，有几个细节问题值得我们思考：

发送者如何知道接收者的MAC地址？

发送者如何知道接收者和自己同属一个子网？

如果接收者和自己不在同一个子网，数据包如何发给对方？

为了解决这些问题，网络层引入了三个协议，分别是**IP协议**、**ARP协议**、**路由协议。**

【1】IP协议

通过前面的介绍我们知道，MAC地址只与厂商有关，与所处的网络无关，所以无法通过MAC地址来判断两台主机是否属于同一个子网。

因此，网络层引入了IP协议，制定了一套新地址，使得我们能够区分两台主机是否同属一个网络，这套地址就是网络地址，也就是所谓的**IP地址。**

IP地址目前有两个版本，分别是**IPv4**和**IPv6**，IPv4是一个32位的地址，常采用4个十进制数字表示。IP协议将这个32位的地址分为两部分，前面部分代表网络地址，后面部分表示该主机在局域网中的地址。由于各类地址的分法不尽相同，以C类地址**192.168.24.1**为例**，**其中前24位就是网络地址，后8位就是主机地址。因此，**如果两个IP地址在同一个子网内，则网络地址一定相同。**为了判断IP地址中的网络地址，IP协议还引入了**子网掩码，**IP地址和子网掩码通过**按位与**运算后就可以得到网络地址。

由于发送者和接收者的IP地址是已知的(应用层的协议会传入)， 因此我们只要通过子网掩码对两个IP地址进行AND运算后就能够判断双方是否在同一个子网了。

【2】ARP协议

即地址解析协议，是根据**IP地址**获取**MAC地址**的一个网络层协议。其工作原理如下：

ARP首先会发起一个请求数据包，数据包的首部包含了目标主机的IP地址，然后这个数据包会在链路层进行再次包装，生成**以太网数据包，**最终由以太网广播给子网内的所有主机，每一台主机都会接收到这个数据包，并取出标头里的IP地址，然后和自己的IP地址进行比较，如果相同就返回自己的MAC地址，如果不同就丢弃该数据包。ARP接收返回消息，以此确定目标机的MAC地址；与此同时，ARP还会将返回的MAC地址与对应的IP地址存入本机ARP缓存中并保留一定时间，下次请求时直接查询ARP缓存以节约资源。cmd输入 arp -a 就可以查询本机缓存的ARP数据。

【3】路由协议

通过ARP协议的工作原理可以发现，**ARP的MAC寻址还是局限在同一个子网中**，因此网络层引入了路由协议，首先通过IP协议来判断两台主机是否在同一个子网中，如果在同一个子网，就通过ARP协议查询对应的MAC地址，然后以广播的形式向该子网内的主机发送数据包；如果不在同一个子网，以太网会将该数据包转发给本子网的**网关**进行路由。网关是互联网上子网与子网之间的桥梁，所以网关会进行多次转发，最终将该数据包转发到目标IP所在的子网中，然后再通过ARP获取目标机MAC，最终也是通过广播形式将数据包发送给接收方。

而完成这个路由协议的物理设备就是**路由器，**在错综复杂的网络世界里，路由器扮演者**交通枢纽**的角色，它会根据信道情况，选择并设定路由，以最佳路径来转发数据包。

【4】IP数据包

在网络层被包装的数据包就叫**IP数据包，**IPv4数据包的结构如下图所示：

![img](https://pic3.zhimg.com/80/v2-31fd5c1561cc7370a075d05390aa3812_720w.webp)

IP数据包由首部和数据两部分组成，首部长度为20个字节，主要包含了目标IP地址和源IP地址，目标IP地址是网关路由的线索和依据；数据部分的最大长度为65515字节，理论上一个IP数据包的总长度可以达到65535个字节，而以太网数据包的最大长度是1500个字符，如果超过这个大小，就需要对IP数据包进行分割，分成多帧发送。

所以，网络层的主要工作是**定义网络地址，区分网段，子网内MAC寻址，对于不同子网的数据包进行路由。**

## 3.传输层

链路层定义了主机的身份，即MAC地址， 而网络层定义了IP地址，明确了主机所在的网段，有了这两个地址，数据包就从可以从一个主机发送到另一台主机。但实际上数据包是从一个主机的某个应用程序发出，然后由对方主机的应用程序接收。而每台电脑都有可能同时运行着很多个应用程序，所以当数据包被发送到主机上以后，是无法确定哪个应用程序要接收这个包。

因此传输层引入了**UDP协议**来解决这个问题，为了给每个应用程序标识身份，UDP协议定义了**端口**，同一个主机上的每个应用程序都需要指定唯一的端口号，并且规定网络中传输的数据包必须加上端口信息。 这样，当数据包到达主机以后，就可以根据端口号找到对应的应用程序了。UDP定义的数据包就叫做UDP数据包，结构如下所示：

![img](https://pic1.zhimg.com/80/v2-35c9a3a99fbc8409861d99a64c2d1964_720w.webp)

UDP数据包由首部和数据两部分组成，首部长度为8个字节，主要包括源端口和目标端口；数据最大为65527个字节，整个数据包的长度最大可达到65535个字节。

UDP协议比较简单，实现容易，但它没有确认机制， 数据包一旦发出，无法知道对方是否收到，因此可靠性较差，为了解决这个问题，提高网络可靠性，**TCP协议**就诞生了，TCP即传输控制协议，是一种面向连接的、可靠的、基于字节流的通信协议。简单来说**TCP就是有确认机制的UDP协议，**每发出一个数据包都要求确认，如果有一个数据包丢失，就收不到确认，发送方就必须重发这个数据包。

为了保证传输的可靠性，TCP 协议在 UDP 基础之上建立了**三次对话**的确认机制，也就是说，在正式收发数据前，必须和对方建立可靠的连接。由于建立过程较为复杂，我们在这里做一个形象的描述：

主机A：我想发数据给你，可以么？

主机B：可以，你什么时候发？

主机A：我马上发，你接着！

经过三次对话之后，主机A才会向主机B发送正式数据，而UDP是面向非连接的协议，它不与对方建立连接，而是直接就把数据包发过去了。所以 TCP 能够保证数据包在传输过程中不被丢失，但美好的事物必然是要付出代价的，相比 UDP，TCP 实现过程复杂，消耗连接资源多，传输速度慢。

TCP 数据包和 UDP 一样，都是由首部和数据两部分组成，唯一不同的是，TCP 数据包没有长度限制，理论上可以无限长，但是为了保证网络的效率，通常 TCP 数据包的长度不会超过IP数据包的长度，以确保单个 TCP 数据包不必再分割。

总结一下，传输层的主要工作是**定义端口，标识应用程序身份，实现端口到端口的通信，TCP协议可以保证数据传输的可靠性**。

## 4.应用层

理论上讲，有了以上三层协议的支持，数据已经可以从一个主机上的应用程序传输到另一台主机的应用程序了，但此时传过来的数据是字节流，不能很好的被程序识别，操作性差。因此，应用层定义了各种各样的协议来规范数据格式，常见的有 HTTP、FTP、SMTP 等，HTTP 是一种比较常用的应用层协议，主要用于B/S架构之间的数据通信，其报文格式如下：

![img](https://pic2.zhimg.com/80/v2-31ec165ae9364b9e95b899b2d4df08a1_720w.webp)

在 Resquest Headers 中，Accept 表示客户端期望接收的数据格式，而 ContentType 则表示客户端发送的数据格式；在 Response Headers 中，ContentType 表示服务端响应的数据格式，这里定义的格式，一般是和 Resquest Headers 中 Accept 定义的格式是一致的。

有了这个规范以后，服务端收到请求以后，就能正确的解析客户端发来的数据，当请求处理完以后，再按照客户端要求的格式返回，客户端收到结果后，按照服务端返回的格式进行解析。

所以应用层的主要工作就是**定义数据格式并按照对应的格式解读数据。**

## 5.全流程

首先我们梳理一下每层模型的职责：

**链路层**：对0和1进行分组，定义数据帧，确认主机的物理地址，传输数据；

**网络层**：定义IP地址，确认主机所在的网络位置，并通过IP进行MAC寻址，对外网数据包进行路由转发；

**传输层**：定义端口，确认主机上应用程序的身份，并将数据包交给对应的应用程序；

**应用层**：定义数据格式，并按照对应的格式解读数据。

然后再把每层模型的职责串联起来，用一句通俗易懂的话讲就是：

当你输入一个网址并按下回车键的时候，首先，应用层协议对该请求包做了格式定义；紧接着传输层协议加上了双方的端口号，确认了双方通信的应用程序；然后网络协议加上了双方的IP地址，确认了双方的网络位置；最后链路层协议加上了双方的MAC地址，确认了双方的物理位置，同时将数据进行分组，形成数据帧，采用广播方式，通过传输介质发送给对方主机。而对于不同网段，该数据包首先会转发给网关路由器，经过多次转发后，最终被发送到目标主机。目标机接收到数据包后，采用对应的协议，对帧数据进行组装，然后再通过一层一层的协议进行解析，最终被应用层的协议解析并交给服务器处理。

## 6.总结

以上内容是对TCP/IP四层模型做了简单的介绍，而实际上每一层模型都有很多协议，每个协议要做的事情也很多，但我们首先得有一个清晰的脉络结构，掌握每一层模型最基本的作用，然后再去丰富细枝末节的东西，也许会更容易理解。

原文链接：https://zhuanlan.zhihu.com/p/273434776

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.162】作为C++后端开发程序员，应该彻底理解Epoll实现原理

这篇文章读不懂的没关系，可以先收藏一下。

当然，这些核心思想，也会在之后的文章中慢慢做详细讲解，欢迎关注。

epoll 是Linux平台下的一种特有的多路复用IO实现方式，与传统的 select 相比，epoll 在性能上有很大的提升。本文主要讲解 epoll 的实现原理，而对于 epoll 的使用可以参考相关的书籍或文章。

## 1.epoll 的创建

要使用 epoll 首先需要调用 epoll_create() 函数创建一个 epoll 的句柄，epoll_create() 函数定义如下：

```
int epoll_create(int size);
```

参数 size 是由于历史原因遗留下来的，现在不起作用。当用户调用 epoll_create() 函数时，会进入到内核空间，并且调用 sys_epoll_create() 内核函数来创建 epoll 句柄，sys_epoll_create() 函数代码如下：

```
asmlinkage long sys_epoll_create(int size)
{
    int error, fd = -1;
    struct eventpoll *ep;
    error = -EINVAL;
    if (size <= 0 || (error = ep_alloc(&ep)) < 0) {
        fd = error;
        goto error_return;
    }
    fd = anon_inode_getfd("[eventpoll]", &eventpoll_fops, ep);
    if (fd < 0)
        ep_free(ep);
error_return:
    return fd;
}
```

sys_epoll_create() 主要做两件事情：

1. 调用 ep_alloc() 函数创建并初始化一个 eventpoll 对象。
2. 调用 anon_inode_getfd() 函数把 eventpoll 对象映射到一个文件句柄，并返回这个文件句柄。

我们先来看看 eventpoll 这个对象，eventpoll 对象用于管理 epoll 监听的文件列表，其定义如下：

```
struct eventpoll {
    ...
    wait_queue_head_t wq;
    ...
    struct list_head rdllist;
    struct rb_root rbr;
    ...
};
```

先来说明一下 eventpoll 对象各个成员的作用：

1. wq: 等待队列，当调用 epoll_wait(fd) 时会把进程添加到 eventpoll 对象的 wq 等待队列中。
2. rdllist: 保存已经就绪的文件列表。
3. rbr: 使用红黑树来管理所有被监听的文件。

下图展示了 eventpoll 对象与被监听的文件关系：

![img](https://pic2.zhimg.com/80/v2-24a3c622ba516a87009334fad2059331_720w.webp)

由于被监听的文件是通过 epitem 对象来管理的，所以上图中的节点都是以 epitem 对象的形式存在的。为什么要使用红黑树来管理被监听的文件呢？这是为了能够通过文件句柄快速查找到其对应的 epitem 对象。红黑树是一种平衡二叉树，如果对其不了解可以查阅相关的文档。

## 2.向 epoll 添加文件句柄

前面介绍了怎么创建 epoll，接下来介绍一下怎么向 epoll 添加要监听的文件。

通过调用 epoll_ctl() 函数可以向 epoll 添加要监听的文件，其原型如下：

```
long epoll_ctl(int epfd, int op, int fd,struct epoll_event *event);
```

下面说明一下各个参数的作用：

1. epfd: 通过调用 epoll_create() 函数返回的文件句柄。
2. op: 要进行的操作，有3个选项：
3. EPOLL_CTL_ADD：表示要进行添加操作。
4. EPOLL_CTL_DEL：表示要进行删除操作。
5. EPOLL_CTL_MOD：表示要进行修改操作。
6. fd: 要监听的文件句柄。
7. event: 告诉内核需要监听什么事。其定义如下：

```
struct epoll_event {
    __uint32_t events;  /* Epoll events */
    epoll_data_t data;  /* User data variable */
};
```

events 可以是以下几个宏的集合：

- EPOLLIN ：表示对应的文件句柄可以读（包括对端SOCKET正常关闭）；
- EPOLLOUT：表示对应的文件句柄可以写；
- EPOLLPRI：表示对应的文件句柄有紧急的数据可读；
- EPOLLERR：表示对应的文件句柄发生错误；
- EPOLLHUP：表示对应的文件句柄被挂断；
- EPOLLET：将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。
- EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里。

data 用来保存用户自定义数据。

epoll_ctl() 函数会调用 sys_epoll_ctl() 内核函数，sys_epoll_ctl() 内核函数的实现如下：

```
asmlinkage long sys_epoll_ctl(int epfd, int op,
    int fd, struct epoll_event __user *event)
{
    ...
    file = fget(epfd);
    tfile = fget(fd);
    ...
    ep = file->private_data;
    mutex_lock(&ep->mtx);
    epi = ep_find(ep, tfile, fd);
    error = -EINVAL;
    switch (op) {
    case EPOLL_CTL_ADD:
        if (!epi) {
            epds.events |= POLLERR | POLLHUP;
            error = ep_insert(ep, &epds, tfile, fd);
        } else
            error = -EEXIST;
        break;
    ...
    }
    mutex_unlock(&ep->mtx);
    ...
    return error;
}
```

sys_epoll_ctl() 函数会根据传入不同 op 的值来进行不同操作，比如传入 EPOLL_CTL_ADD 表示要进行添加操作，那么就调用 ep_insert() 函数来进行添加操作。

我们继续来分析添加操作 ep_insert() 函数的实现：

```
static int ep_insert(struct eventpoll *ep, struct epoll_event *event,
             struct file *tfile, int fd)
{
    ...
    error = -ENOMEM;
    // 申请一个 epitem 对象
    if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL)))
        goto error_return;
    // 初始化 epitem 对象
    INIT_LIST_HEAD(&epi->rdllink);
    INIT_LIST_HEAD(&epi->fllink);
    INIT_LIST_HEAD(&epi->pwqlist);
    epi->ep = ep;
    ep_set_ffd(&epi->ffd, tfile, fd);
    epi->event = *event;
    epi->nwait = 0;
    epi->next = EP_UNACTIVE_PTR;
    epq.epi = epi;
    // 等价于: epq.pt->qproc = ep_ptable_queue_proc
    init_poll_funcptr(&epq.pt, ep_ptable_queue_proc);
    // 调用被监听文件的 poll 接口.
    // 这个接口又各自文件系统实现, 如socket的话, 那么这个接口就是 tcp_poll().
    revents = tfile->f_op->poll(tfile, &epq.pt);
    ...
    ep_rbtree_insert(ep, epi); // 把 epitem 对象添加到epoll的红黑树中进行管理
    spin_lock_irqsave(&ep->lock, flags);
    // 如果被监听的文件已经可以进行对应的读写操作
    // 那么就把文件添加到epoll的就绪队列 rdllink 中, 并且唤醒调用 epoll_wait() 的进程.
    if ((revents & event->events) && !ep_is_linked(&epi->rdllink)) {
        list_add_tail(&epi->rdllink, &ep->rdllist);
        if (waitqueue_active(&ep->wq))
            wake_up_locked(&ep->wq);
        if (waitqueue_active(&ep->poll_wait))
            pwake++;
    }
    spin_unlock_irqrestore(&ep->lock, flags);
    ...
    return 0;
    ...
}
```

被监听的文件是通过 epitem 对象进行管理的，也就是说被监听的文件会被封装成 epitem 对象，然后会被添加到 eventpoll 对象的红黑树中进行管理（如上述代码中的 ep_rbtree_insert(ep, epi)）。

tfile->f_op->poll(tfile, &epq.pt) 这行代码的作用是调用被监听文件的 poll() 接口，如果被监听的文件是一个socket句柄，那么就会调用 tcp_poll()，我们来看看 tcp_poll() 做了什么操作：

```
unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)
{
    struct sock *sk = sock->sk;
    ...
    poll_wait(file, sk->sk_sleep, wait);
    ...
    return mask;
}
```

每个 socket 对象都有个等待队列（waitqueue, 关于等待队列可以参考文章: 等待队列原理与实现），用于存放等待 socket 状态更改的进程。

从上述代码可以知道，tcp_poll() 调用了 poll_wait() 函数，而 poll_wait() 最终会调用 ep_ptable_queue_proc() 函数，ep_ptable_queue_proc() 函数实现如下：

```
static void ep_ptable_queue_proc(struct file *file,
    wait_queue_head_t *whead, poll_table *pt)
{
    struct epitem *epi = ep_item_from_epqueue(pt);
    struct eppoll_entry *pwq;
    if (epi->nwait >= 0 && (pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL))) {
        init_waitqueue_func_entry(&pwq->wait, ep_poll_callback);
        pwq->whead = whead;
        pwq->base = epi;
        add_wait_queue(whead, &pwq->wait);
        list_add_tail(&pwq->llink, &epi->pwqlist);
        epi->nwait++;
    } else {
        epi->nwait = -1;
    }
}
```

ep_ptable_queue_proc() 函数主要工作是把当前 epitem 对象添加到 socket 对象的等待队列中，并且设置唤醒函数为 ep_poll_callback()，也就是说，当socket状态发生变化时，会触发调用 ep_poll_callback() 函数。ep_poll_callback() 函数实现如下：

```
static int ep_poll_callback(wait_queue_t *wait, unsigned mode, int sync, void *key)
{
    ...
    // 把就绪的文件添加到就绪队列中
    list_add_tail(&epi->rdllink, &ep->rdllist);
is_linked:
    // 唤醒调用 epoll_wait() 而被阻塞的进程
    if (waitqueue_active(&ep->wq))
        wake_up_locked(&ep->wq);
    ...
    return 1;
}
```

ep_poll_callback() 函数的主要工作是把就绪的文件添加到 eventepoll 对象的就绪队列中，然后唤醒调用 epoll_wait() 被阻塞的进程。

## 3.等待被监听的文件状态发生改变

把被监听的文件句柄添加到epoll后，就可以通过调用 epoll_wait() 等待被监听的文件状态发生改变。epoll_wait() 调用会阻塞当前进程，当被监听的文件状态发生改变时，epoll_wait() 调用便会返回。

epoll_wait() 系统调用的原型如下：

```
long epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

各个参数的意义：

1. epfd: 调用 epoll_create() 函数创建的epoll句柄。
2. events: 用来存放就绪文件列表。
3. maxevents: events 数组的大小。
4. timeout: 设置等待的超时时间。

epoll_wait() 函数会调用 sys_epoll_wait() 内核函数，而 sys_epoll_wait() 函数最终会调用 ep_poll() 函数，我们来看看 ep_poll() 函数的实现：

```
static int ep_poll(struct eventpoll *ep,
    struct epoll_event __user *events, int maxevents, long timeout)
{
    ...
    // 如果就绪文件列表为空
    if (list_empty(&ep->rdllist)) {
        // 把当前进程添加到epoll的等待队列中
        init_waitqueue_entry(&wait, current);
        wait.flags |= WQ_FLAG_EXCLUSIVE;
        __add_wait_queue(&ep->wq, &wait);
        for (;;) {
            set_current_state(TASK_INTERRUPTIBLE); // 把当前进程设置为睡眠状态
            if (!list_empty(&ep->rdllist) || !jtimeout) // 如果有就绪文件或者超时, 退出循环
                break;
            if (signal_pending(current)) { // 接收到信号也要退出
                res = -EINTR;
                break;
            }
            spin_unlock_irqrestore(&ep->lock, flags);
            jtimeout = schedule_timeout(jtimeout); // 让出CPU, 切换到其他进程进行执行
            spin_lock_irqsave(&ep->lock, flags);
        }
        // 有3种情况会执行到这里:
        // 1. 被监听的文件集合中有就绪的文件
        // 2. 设置了超时时间并且超时了
        // 3. 接收到信号
        __remove_wait_queue(&ep->wq, &wait);
        set_current_state(TASK_RUNNING);
    }
    /* 是否有就绪的文件? */
    eavail = !list_empty(&ep->rdllist);
    spin_unlock_irqrestore(&ep->lock, flags);
    if (!res && eavail
        && !(res = ep_send_events(ep, events, maxevents)) && jtimeout)
        goto retry;
    return res;
}
```

## 4.ep_poll() 函数主要做以下几件事：

1. 判断被监听的文件集合中是否有就绪的文件，如果有就返回。
2. 如果没有就把当前进程添加到epoll的等待队列中，并且进入睡眠。
3. 进程会一直睡眠直到有以下几种情况发生：
4. 1. 被监听的文件集合中有就绪的文件
   2. 设置了超时时间并且超时了
   3. 接收到信号
5. 如果有就绪的文件，那么就调用 ep_send_events() 函数把就绪文件复制到 events 参数中。
6. 返回就绪文件的个数。

## 5.最后，我们通过一张图来总结epoll的原理：

![img](https://pic3.zhimg.com/80/v2-ebddc2c59d9488e76e4d983252a853ee_720w.webp)

下面通过文字来描述一下这个过程：

1. 通过调用 epoll_create() 函数创建并初始化一个 eventpoll 对象。
2. 通过调用 epoll_ctl() 函数把被监听的文件句柄 (如socket句柄) 封装成 epitem 对象并且添加到 eventpoll 对象的红黑树中进行管理。
3. 通过调用 epoll_wait() 函数等待被监听的文件状态发生改变。
4. 当被监听的文件状态发生改变时（如socket接收到数据），会把文件句柄对应 epitem 对象添加到 eventpoll 对象的就绪队列 rdllist 中。并且把就绪队列的文件列表复制到 epoll_wait() 函数的 events 参数中。
5. 唤醒调用 epoll_wait() 函数被阻塞（睡眠）的进程。

原文链接：https://zhuanlan.zhihu.com/p/353142302

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.163】协程的原理和应用，C++现实协程

## 1.协程的原理

协程（coroutine）跟具有操作系统概念的线程不一样，实际上协程就是类函数一样的程序组件，你可以在一个线程里面轻松创建数十万个协程，就像数十万次函数调用一样。只不过函数只有一个调用入口起始点，返回之后就结束了，而协程入口既可以是起始点，又可以从上一个返回点继续执行，也就是说协程之间可以通过 yield 方式转移执行权，对称（symmetric）、平级地调用对方，而不是像函数那样上下级调用关系。当然 协程也可以模拟函数那样实现上下级调用关系，这就叫非对称协程（asymmetric coroutines）。

我们举一个例子来看看一种**对称协程**调用场景，大家最熟悉的“生产者-消费者”事件驱动模型，一个协程负责生产产品并将它们加入队列，另一个负责从队列中取出产品并使用它。为了提高效率，你想一次增加或删除多个产品。伪代码可以是这样的：

```
# producer coroutine
loop
while queue is not full
  create some new items
  add the items to queue
yield to consumer
# consumer coroutine
loop
while queue is not empty
  remove some items from queue
  use the items
yield to producer
```

如果用多线程实现生产者-消费者模式，线程之间需要使用同步机制来避免产生全局资源的竟态，这就不可避免产生了休眠、调度、切换上下文一类的系统开销，而且线程调度还会产生时序上的不确定性。

而对于协程来说，“挂起”的概念只不过是转让代码执行权并调用另外的协程，待到转让的协程告一段落后重新得到调用并从挂起点“唤醒”，这种协程间的调用是逻辑上可控的，时序上确定的，可谓一切尽在掌握中。

当今一些具备协程语义的语言，比较重量级的如C#、erlang、golang，以及轻量级的python、lua、javascript、ruby，还有函数式的scala、scheme等。相比之下，作为原生态语言的 C 反而处于尴尬的地位，原因在于 C 依赖于一种叫做**栈帧**的例程调用，例程内部的状态量和返回值都保留在堆栈上，这意味着生产者和消费者相互之间无法实现平级调用，当然你可以改写成把生产者作为主例程然后将产品作为传递参数调用消费者例程，这样的代码写起来费力不讨好而且看起来会很难受，特别当协程数目达到十万数量级，这种写法就过于僵化了。

**如果将每个协程的上下文（比如程序计数器）保存在其它地方而不是堆栈上，协程之间相互调用时，被调用的协程只要从堆栈以外的地方恢复上次出让点之前的上下文即可，这有点类似于 CPU 的上下文切换，**C 标准库给我们提供了两种协程调度原语：一种是 setjmp/longjmp，另一种是 ucontext 组件，它们内部（当然是用汇编语言）实现了协程的上下文切换，相较之下前者在应用上会产生相当的不确定性（比如不好封装，具体说明参考联机文档），所以后者应用更广泛一些，网上绝大多数 C 协程库也是基于 ucontext 组件实现的。

我们知道 python 的 yield 语义功能类似于一种迭代生成器，函数会保留上次的调用状态，并在下次调用时会从上个返回点继续执行，例如：

```
def cols():
    for i in range(10):
        yield i
g=cols()
for k in g:
    print(k) 
```

下面看看C语言的yiled语义是如何实现的：

```
int function(void) {
  static int i, state = 0;
  switch (state) {
    case 0: goto LABEL0;
    case 1: goto LABEL1;
  }
  LABEL0: /* start of function */
  for (i = 0; i < 10; i++) {
    state = 1; /* so we will come back to LABEL1 */
    return i;
    LABEL1:; /* resume control straight after the return */
  }
}
```

这是利用了static变量和goto跳转来实现的，如果不用goto，而是直接利用switch的跳转功能：

```
int function(void) {
  static int i, state = 0;
  switch (state) {
    case 0: /* start of function */
    for (i = 0; i < 10; i++) {
      state = 1; /* so we will come back to "case 1" */
      return i;
      case 1:; /* resume control straight after the return */
    }
  }
} 
```

我们还可以用 **LINE** 宏使其更加一般化：

```
int function(void) {
  static int i, state = 0;
  switch (state) {
    case 0: /* start of function */
    for (i = 0; i < 10; i++) {
      state = __LINE__ + 2; /* so we will come back to "case __LINE__" */
      return i;
      case __LINE__:; /* resume control straight after the return */
    }
  }
}
```

这样一来我们可以用宏提炼出一种范式，封装成组件：

```
#define Begin() static int state=0; switch(state) { case 0:
#define Yield(x) do { state=__LINE__; return x; case __LINE__:; } while (0)
#define End() }
int function(void) {
  static int i;
  Begin();
  for (i = 0; i < 10; i++)
    Yield(i);
  End();
}
```

这种协程实现方法有个使用上的局限，就是**协程调度状态的保存依赖于 static 变量，而不是堆栈上的局部变量**，实际上也无法用局部变量（堆栈）来保存状态，这就使得代码不具备可重入性和多线程应用。如果将局部变量包装成函数参数传入的一个虚构的上下文结构体指针，然后用动态分配的堆来“模拟”堆栈，解决了线程可重入问题。但这样一来反而有损代码清晰，比如所有局部变量都要写成对象成员的引用方式，特别是局部变量很多的时候很麻烦，再比如宏定义 malloc/free 的玩法过于托大，不易控制。

既然协程本身是一种单线程的方案，那么我们应该假定应用环境是单线程的，不存在代码重入问题，所以我们可以大胆地使用 static 变量，维持代码的简洁和可读性。事实上**我们也不应该在多线程环境下考虑使用这么简陋的协程**，非要用的话，前面提到 glibc 的 ucontext 组件也是一种可行的替代方案，它提供了一种协程私有堆栈的上下文，当然这种用法在跨线程上也并非没有限制，请仔细阅读其文档。

## 2.协程的并发应用

协程就是在单线程中使用同步编程思想来实现异步的处理流程，从而实现单线程能并发处理成百上千个请求，而且每个请求的处理过程是线性的，没有使用晦涩难懂的callback机制来衔接处理流程。

## 3.基于事件驱动状态机

传统的网络服务器（如nginx、squid等）都采用了 EDSM (event-driven state machine，事件驱动状态机) 机制并发处理请求，这是一种异步处理的方式，通过使用callback 方法避免阻塞线程。

EDSM最常见的方式就是I/O事件的异步回调。基本上都会有一个叫做dispatcher的单线程主循环（又叫event loop），用户通过向dispatcher注册回调函数（又叫event handler）来实现异步通知，从而不必在原地空耗资源干等。在dispatcher主循环中通过select()/epoll()等系统调用来等待各种I/O事件的发生，当内核检测到事件触发并且数据可达或可用时，select()/epoll()会返回从而使dispatcher调用相应的回调函数来对处理用户的请求。

整个过程都是单线程的。这种处理本质上就是将一堆相互独立（disjoint）的回调实现同步控制，就像串联在一个顺序链表上。如下图，黑色的双箭头表示I/O事件复用，回调是个筐，里面装着对各种请求的处理（当然不是每个请求都有回调，一个请求也可以对应不同的回调），每个回调被串联起来由dispatcher激活。这里请求等价于thread的概念（不是操作系统的线程），只不过“上下文切换”（context switch）发生在每个回调结束之时（假设不同请求对应不同回调），注册下一个回调以待事件触发时恢复其它请求的处理。至于dispatcher的执行状态（execute state）可作为回调函数的参数保存和传递

![动图封面](https://pic2.zhimg.com/v2-bba58100c28d4a7750121423658749a5_b.jpg)

异步回调的缺陷在于难以实现和扩展，虽然已经有libevent这样的通用库，以及其它actor/reacotor的设计模式及其框架，但正如Dean Gaudet（Apache开发者）所说：“其内在的复杂性——将线性思维分解成一堆回调的负担（breaking up linear thought into a bucketload of callbacks）——仍然存在”。从上图可见，回调之间请求例程不是连续的，比如回调之间的切换会打断部分请求，又比如有新的请求需要重新注册。

协程本质上仍然是基于EDSM模型，但旨在取代传统的异步回调方式。协程将请求抽象为thread概念以更接近自然编程模式（所谓的linear thought吧，就像操作系统的线程之间切换那样自然）。

![img](https://pic3.zhimg.com/80/v2-969155a5546f0d867adfa0a4c2a4d66a_720w.webp)

下面介绍一种协程的实现方案：State Threads库。

## 4.ST库

ST (State Threads) 库提供了一种高性能、可扩展服务器（比如web server、proxy server、mail agent等）的实现方案。

ST 库简化了multi-threading编程范式，每个请求对应一个线程，注意这里的线程其实是一种coroutine（协程），跟pthread那种内核线程不是一回事。

这里稍微解释一下ST调度工作原理，ST运行环境维护了四种队列，分别是IOQ（等待队列）、RUNQ（运行队列）、SLEEPQ（超时队列）以及ZOMBIEQ。当每个thread处于不同队列中对应不同的状态（ST顾名思义所谓thread状态机）。比如polling请求的时候，当前thread就加入IOQ表示等待事件（如果有timeout同时会被放到SLEEPQ中），当事件触发时，thread就从IOQ（如果有timeout同时会从SLEEPQ）移除并转移到RUNQ等待被调度，成为当前的running thread，相当于操作系统的就绪队列，跟传统EDSM对应起来就是注册回调以及激活回调。再比如模拟同步控制wait/sleep/lock的时候，当前thread会被放入SLEEPQ，直到被唤醒或者超时再次进入RUNQ以待调度。

ST的调度具备性能与内存双重优点：在性能上，ST实现自己的setjmp/longjmp来模拟调度，无任何系统开销，并且context（就是jmp_buf）针对不同平台和架构用底层语言实现的，可移植性媲美libc。下面放一段代码解释一下调度实现：

```
/*
 * Switch away from the current thread context by saving its state 
 * and calling the thread scheduler
 */
#define _ST_SWITCH_CONTEXT(_thread)       \
    ST_BEGIN_MACRO                        \
    if (!MD_SETJMP((_thread)->context)) { \
      _st_vp_schedule();                  \
    }                                     \
    ST_END_MACRO
/*
 * Restore a thread context that was saved by _ST_SWITCH_CONTEXT 
 * or initialized by _ST_INIT_CONTEXT
 */
#define _ST_RESTORE_CONTEXT(_thread)   \
    ST_BEGIN_MACRO                     \
    _ST_SET_CURRENT_THREAD(_thread);   \
    MD_LONGJMP((_thread)->context, 1); \
    ST_END_MACRO
void _st_vp_schedule(void)
{
    _st_thread_t *thread;
    if (_ST_RUNQ.next != &_ST_RUNQ) {
        /* Pull thread off of the run queue */
        thread = _ST_THREAD_PTR(_ST_RUNQ.next);
        _ST_DEL_RUNQ(thread);
    } else {
        /* If there are no threads to run, switch to the idle thread */
        thread = _st_this_vp.idle_thread;
    }
    ST_ASSERT(thread->state == _ST_ST_RUNNABLE);
    /* Resume the thread */
    thread->state = _ST_ST_RUNNING;
    _ST_RESTORE_CONTEXT(thread);
}
```

如果你熟悉setjmp/longjmp的用法，你就知道当前thread在调用MD_SETJMP将现场上下文保存在jmp_buf中并返回返回0，然后自己调用*st*vp_schedule()将自己调度出去。调度器先从RUNQ上找，如果队列为空就找idle thread，这是在整个ST初始化时创建的一个特殊thread，然后将当前线程设为自己，再调用MD_LONGJMP切换到其上次调用MD_SETJMP的地方，从thread->context恢复现场并返回1，该thread就接着往下执行了。整个过程就同EDSM一样发生在操作系统单线程下，所以没有任何系统开销与阻塞。

其实真正的阻塞是发生在等待I/O事件复用上，也就是select()/epoll()，这是整个ST唯一的系统调用。ST当前的状态是，整个环境处于空闲状态，所有threads的请求处理都已经完成，也就是RUNQ为空。这时在*st*idle_thread_start维护了一个主循环（类似于event loop），主要负责三种任务：1.对IOQ所有thread进行I/O复用检测；2.对SLEEPQ进行超时检查；3.将idle thread调度出去，代码如下：

```
void *_st_idle_thread_start(void *arg)
{
    _st_thread_t *me = _ST_CURRENT_THREAD();
    while (_st_active_count > 0) {
        /* Idle vp till I/O is ready or the smallest timeout expired */
        _ST_VP_IDLE();
        /* Check sleep queue for expired threads */
        _st_vp_check_clock();
        me->state = _ST_ST_RUNNABLE;
        _ST_SWITCH_CONTEXT(me);
    }
    /* No more threads */
    exit(0);
    /* NOTREACHED */
    return NULL;
}
```

这里的me就是idle thread，因为*st*idle_thread_start就是创建idle thread的启动点，每从上次*ST*SWITCH_CONTEXT()切换回来的时候，接着在*ST*VP_IDLE()里轮询I/O事件的发生，一旦检测到发生了别的thread事件或者SLEEPQ里面发生超时，再用*ST*SWITCH_CONTEXT()把自己切换出去，如果此时RUNQ中非空的话就切换到队列第一个thread。这里主循环是不会退出的。

在内存方面，ST的执行状态作为局部变量保存在栈上，而不是像回调需要动态分配，用户可能分别这样使用thread模式和callback模式：

```
/* thread land */
int foo()
{
    int local1;
    int local2;
    do_some_io();
}
/* callback land */
struct foo_data {
    int local1;
    int local2;
};
void foo_cb(void *arg)
{
    struct foo_data *locals = arg;
    ...
}
void foo()
{
    struct foo_data *locals = malloc(sizeof(struct foo_data));
    register(foo_cb, locals);
} 
```

另外有两点要注意，一是ST的thread是无优先级的非抢占式调度，也就是说ST基于EDSM的，每个thread都是事件或数据驱动，迟早会把自己调度出去，而且调度点是明确的，并非按时间片来的，从而简化了thread管理；二是ST会忽略所有信号处理，在*st*io_init中会把sigact.sa_handler设为SIG_IGN，这样做是因为将thread资源最小化，避免了signal mask及其系统调用（在ucontext上是避免不了的）。但这并不意味着ST就不能处理信号，实际上ST建议将信号写入pipe的方式转化为普通I/O事件处理，示例详见这里。

## 5.multi-threading编程范式

Posix Thread（以下简称PThread）是个通用的线程库，它是将用户级线程（thread）同内核执行对象（kernel execution entity，有些书又叫lightweight processes）做了1:1或m:n映射，从而实现multi-threading模式。例如，Apache服务器就是使用了PThread来实现并发请求的处理，每个线程处理一个请求，线程是以同步、阻塞的方式处理请求的，在线程的当前请求处理完成之前不会接受其它请求。

而ST是单线程（n:1映射），它的thread实际上就是协程（coroutine）。通常的网络应用上，多线程范式绕不开操作系统，但在某些特定的服务器领域，线程间的共享资源会带来额外复杂度，锁、竞态、并发、文件句柄、全局变量、管道、信号等，面对这些Pthread的灵活性会大打折扣。而ST的调度是精确的，它只会在明确的I/O和同步函数调用点上发生上下文切换，这正是协程的特性，如此一来ST就不需要互斥保护了，进而也可以放心使用任何静态变量和不可重入库函数了（这在同样作为协程的Protothreads里是不允许的，因为那是stack-less的，无法保存上下文），极大的简化了编程和调试同时增加了性能。

这里顺便说一句，C语言实现的协程据我所知只有三种方式：

1、Protothread为代表利用switch-case语义跳转；

2、以ST为代表不依赖libc的setjmp/longjmp上下文切换；

3、依赖glibc的ucontext接口（云风的coroutine）；

其中，Protothread最轻，但受限最大，ucontext耗资源性能慢，目前看来ST是最好使的。

## 6.总结

ST的核心思想就是利用multi-threading的简单优雅范式胜过传统异步回调的复杂晦涩实现，又利用EDSM的性能和解耦架构避免了multi-threading在系统上的开销和暗礁。

ST的主要限制在于，应用程序所有I/O操作必须使用ST提供的API，因为只有这样thread才能被调度器管理，并且避免阻塞。

其实最后在罗嗦一句，ngx_lua模块也是利用coroutine简化了Nginx流程的处理流程，每个请求对应一个lua coroutine，从而在coroutine内部完全使用线性的方式处理请求，避免了使用回调的异步写法

原文链接：https://zhuanlan.zhihu.com/p/352888811

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.164】redis源码分析——内存布局

## 1. 介绍

众所周知，redis是一个开源、短小、高效的key-value存储系统，相对于memcached，redis能够支持更加丰富的数据结构，包括：

1. 字符串（string）
2. 哈希表（map）
3. 列表（list）
4. 集合（set）
5. 有序集（zset）

主流的key-value存储系统，都是在系统内部维护一个hash表，因为对hash表的操作时间复杂度为O(1)。如果数据增加以后，导致冲突严重，时间复杂度增加，则可以对hash表进行rehash，以此来保证操作的常量时间复杂度。

那么，对于这样一个基于hash表的key-value存储系统，是如何提供这么丰富的数据结构的呢？这些数据结构在内存中如何存储呢？这篇文章将用大量的图片演示redis的内存布局和数据存储。

## 2. redisServer

在redis系统内部，有一个redisServer结构体的全局变量server，server保存了redis服务端所有的信息，包括当前进程的PID、服务器的端口号、数据库个数、统计信息等等。当然，它也包含了数据库信息，包括数据库的个数、以及一个redisDb数组。

```
struct redisServer {
    ……
    redisDb *db;
    int dbnum;                      /* Total number of configured DBs */
    ……
}
```

显然，dbnum就是redisDb数组的长度，每一个数据库，都对应于一个redisDb，在redis的客户端中，可以通过select N来选择使用哪一个数据库，各个数据库之间互相独立。例如：可以在不同的数据库中同时存在名为”redis”的key。

![img](https://pic2.zhimg.com/80/v2-72c3ee3cbc59f0afb6dc2671bcdc82ed_720w.webp)

从上面的分析中可以看到，server是一个全局变量，它包含了若干个redisDb，每一个redisDb是一个keyspace，各个keyspace互相独立，互不干扰。

下面来看一下redisDb的定义：

```
/* Redis database representation. There are multiple databases identified
 * by integers from 0 (the default database) up to the max configured
 * database. The database number is the 'id' field in the structure. */
typedef struct redisDb {
    dict *dict;                 /* The keyspace for this DB */
    dict *expires;              /* Timeout of keys with a timeout set */
    dict *blocking_keys;        /* Keys with clients waiting for data (BLPOP) */
    dict *ready_keys;           /* Blocked keys that received a PUSH */
    dict *watched_keys;         /* WATCHED keys for MULTI/EXEC CAS */
    struct evictionPoolEntry *eviction_pool;    /* Eviction pool of keys */
    int id;                     /* Database ID */
    long long avg_ttl;          /* Average TTL, just for stats */
} redisDb;
```

redis的每一个数据库是一个独立的keyspace，因此，我们理所当然的认为，redis的数据库是一个hash表。但是，从redisDb的定义来看，它并不是一个hash表，而是一个包含了很多hash表的结构。之所以这样做，是因为redis还需要提供除了set、get以外更加丰富的功能(例如：键的超时机制)。我们今天只关注最重要的数据结构：

```
typedef struct redisDb {
    dict *dict;                 /* The keyspace for this DB */
    ……
} redisDb;
```

redisDb与redisServer的关系如下所示：

![img](https://pic2.zhimg.com/80/v2-df5c11fa7afd75c8f84b9165c6dd6d0d_720w.webp)

下面再看dict的定义：

```
typedef struct dict {
    ……
    dictht ht[2];
    long rehashidx; /* rehashing not in progress if rehashidx == -1 */
    ……
} dict;
```

dict包含了两个hash表，这样做的目的是为了支持渐进式的rehash，即：在大多数情况下，只使用第一个hash表，如果第一个hash表的数据太多，则需要执行rehash。

dict与redisDb、redisServer的关系如下：

![img](https://pic4.zhimg.com/80/v2-e70b6b45721917b956649ef45ae134f3_720w.webp)

下面看一下dictht的定义，至此，我们总算见到了redis的hash表，与绝大多数的hash表没有什么两样：

```
/* This is our hash table structure. Every dictionary has two of this as we
* implement incremental rehashing, for the old to the new table. */
typedef struct dictht {
    dictEntry **table;
    unsigned long size;
    unsigned long sizemask;
    unsigned long used;
} dictht;
```

dictht与dict、redisDb、redisServer之间的关系如下：

![img](https://pic3.zhimg.com/80/v2-6be09d9ba1b71e7cc0145741378ccda2_720w.webp)

redis对hash表的节点也进行了简单的封装，hash表的每一个节点都是一个dictEntry，redis的hash表看起来是这样：

![img](https://pic4.zhimg.com/80/v2-0ebd4b50776cac81e3af6f161b262b7f_720w.webp)

**总结：** redis内存有一个全局变量redisServer server，该变量包含若干个数据库，每个数据库都用一个redisDb表示，redisDb包含若干个字典，其中，存储数据的是dict* dict，dict内部包含两个hash表，一般情况下面，我们只会使用ht[0]，在rehash时，我们会同时使用两个hash表，hash表的每一项，都是一个dictEntry结构体的变量。

从宏观角度来看，redis的数据存储应该是这样的：

![img](https://pic3.zhimg.com/80/v2-e4865350a4e562adc5bd2ef587f5d8ee_720w.webp)

## 3. 存储不同的数据类型

在上一节中，详细介绍了redis的hash表以及核心数据结构之间的关系，至此，以及对redis存储数据有了一个初步的印象，但是，到目前为止还没有回答文章最开始的问题：**redis如何存储不同的数据结构？**

要理解redis如何存储不同的数据结构，首先来看一下redisObject的定义：

```
typedef struct redisObject {
    unsigned type:4;
    unsigned encoding:4;
    unsigned lru:REDIS_LRU_BITS; /* lru time (relative to server.lruclock) */
    int refcount;
    void *ptr;
} robj;
```

其中，type是逻辑数据类型，即redis提供给用户的字符串、列表、hash表等。type的取值如下：

```
/* Object types */
#define REDIS_STRING 0
#define REDIS_LIST 1
#define REDIS_SET 2
#define REDIS_ZSET 3
#define REDIS_HASH 4
```

type虽然很关键，但是，在我们这篇文章中，更多的需要关注encoding字段，该字段的含义是逻辑数据类型的具体实现。encoding的取值如下：

```
#define REDIS_ENCODING_RAW 0     /* Raw representation */
#define REDIS_ENCODING_INT 1     /* Encoded as integer */
#define REDIS_ENCODING_HT 2      /* Encoded as hash table */
#define REDIS_ENCODING_ZIPMAP 3  /* Encoded as zipmap */
#define REDIS_ENCODING_LINKEDLIST 4 /* Encoded as regular linked list */
#define REDIS_ENCODING_ZIPLIST 5 /* Encoded as ziplist */
#define REDIS_ENCODING_INTSET 6  /* Encoded as intset */
#define REDIS_ENCODING_SKIPLIST 7  /* Encoded as skiplist */
#define REDIS_ENCODING_EMBSTR 8  /* Embedded sds string encoding */
#define REDIS_ENCODING_QUICKLIST 9 /* Encoded as linked list of ziplists */
```

例如，对于list这种数据类型，在redis内部，可以使用ziplist实现（更加省内存），也可以使用linkedlist实现。

在满足以下两个条件时，使用ziplist实现，否则，使用linkedlist实现。

1. 列表对象保存的所有字符串元素的长度都小于64字节
2. 列表对象保存的元素数量小于512

再次强调：对于同一种数据类型，redis内部提供了多种实现，不同的实现适用于不同的场景，且用户只能通过redis.conf文件进行有限的控制，具体使用哪一种实现，完全是redis内部决定。可以通过object encoding key查看当前key的内部编码，即内部实现。

这篇文章介绍redis的内存布局，自然更应该关系的是内部的具体实现，而不是逻辑数据类型。不管是逻辑类型(type)还是具体实现(encoding)，都保存在redisObject中，**redisObject相当于是所有数据结构的父类**，redis的hash表的每一个项都是dictEntry，而每一个dictEntry，都指向一个redisObject。

redis在数据的存取时，首先通过key找到对应的dictEntry，接着通过dictEntry获取redisObject对象，然后通过redisObject的encoding的取值，对redisObject的ptr指针进行强制类型转换。

**例如：** 对于一个简短的list，redis很有可能使用的是quicklist存储，因此，在读取list的数据时，redis首先通过key找到dictEntry，然后通过dictEntry找到redisObject， 通过redisObject的encoding对ptr指针进行强制类型转换，在本例中，将ptr强制转换为quicklist，转换为quicklist以后，就能够获取head和tail指针，可以使用head和tail访问数据。

![img](https://pic4.zhimg.com/80/v2-2d41782de2b84026c6ca33c40a4b1033_720w.webp)

原文链接：https://zhuanlan.zhihu.com/p/351294802

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.165】服务器开发必备-【数据库】Redis集群篇

## 1.哨兵模式

### 1.1.背景

当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑哨兵模式。

### 1.2.定义

Sentinel（哨兵）是Redis 的高可用性解决方案：由一个或多个Sentinel 实例 组成的Sentinel 系统可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器，并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器。

![img](https://pic1.zhimg.com/80/v2-6df0a25c7f7487480a8020b087fbbd84_720w.webp)

![img](https://pic1.zhimg.com/80/v2-27be2d4a34fdf8a8ce2154978548ccd4_720w.webp)

![img](https://pic4.zhimg.com/80/v2-10db3634673ff4f727dbfcefca7e8b3f_720w.webp)

## 2.实战配置

1.首先配置Redis的主从服务器，修改redis.conf文件如下

```
# 使得Redis服务器可以跨网络访问
bind 0.0.0.0
# 设置密码
requirepass "123456"
# 指定主服务器，注意：有关slaveof的配置只是配置从服务器，主服务器不需要配置
slaveof 192.168.11.128 6379
# 主服务器密码，注意：有关slaveof的配置只是配置从服务器，主服务器不需要配置
masterauth 
```

上述内容主要是配置Redis服务器，从服务器比主服务器多一个slaveof的配置和密码。

1. 配置3个哨兵，每个哨兵的配置都是一样的。在Redis安装目录下有一个sentinel.conf文件，copy一份进行修改

```
# 禁止保护模式
protected-mode no
# 配置监听的主服务器，这里sentinel monitor代表监控，mymaster代表服务器的名称，可以自定义，192.168.11.128代表监控的主服务器，6379代表端口，2代表只有两个或两个以上的哨兵认为主服务器不可用的时候，才会进行failover操作。
sentinel monitor mymaster 192.168.11.128 6379 2
# sentinel author-pass定义服务的密码，mymaster是服务名称，123456是Redis服务器密码
# sentinel auth-pass <master-name> <password>
sentinel auth-pass mymaster 123456
```

1. 有了上述的修改，我们可以进入Redis的安装目录的src目录，通过下面的命令启动服务器和哨兵

```
# 启动Redis服务器进程
./redis-server ../redis.conf
# 启动哨兵进程
./redis-sentinel ../sentinel.conf
```

注意启动的顺序。首先是主机（192.168.11.128）的Redis服务进程，然后启动从机的服务进程，最后启动3个哨兵的服务进程。

## 3.集群

搭建集群工作需要以下三个步骤：

### 3.1.准备节点

Redis集群一般由多个节点组成，节点数量至少为6个才能保证组成完整高可用的集群。每个节点需要开启配置cluster-enabled yes，让Redis运行在集群模式下。建议为集群内所有节点统一目录，一般划分三个目录：conf、data、log，分别存放配置、数据和日志相关文件。把6个节点配置统一放在conf目录下

```
#节点端口
port 6379
# 开启集群模式
cluster-enabled yes
# 节点超时时间，单位毫秒
cluster-node-timeout 15000
# 集群内部配置文件
cluster-config-file "nodes-6379.conf"
```

其他配置和单机模式一致即可，配置文件命名规则redis-{port}.conf，准备好配置后启动所有节点，命令如下

```
redis-server conf/redis-6379.conf
redis-server conf/redis-6380.conf
redis-server conf/redis-6381.conf
redis-server conf/redis-6382.conf
redis-server conf/redis-6383.conf
redis-server conf/redis-6384.conf
```

### 3.2.节点握手

节点握手是指一批运行在集群模式下的节点通过Gossip协议彼此通信，达到感知对方的过程。节点握手是集群彼此通信的第一步，由客户端发起命令：cluster meet{ip}{port}

![img](https://pic4.zhimg.com/80/v2-35ee27ff163b54d27a2009f1d5a17da3_720w.webp)

图中执行的命令是：cluster meet127.0.0.16380让节点6379和6380节点进 行握手通信。cluster meet命令是一个异步命令，执行之后立刻返回。内部发起与目标节点进行握手通信。

```
127.0.0.1:6379>cluster meet 127.0.0.1 6381
127.0.0.1:6379>cluster meet 127.0.0.1 6382
127.0.0.1:6379>cluster meet 127.0.0.1 6383
127.0.0.1:6379>cluster meet 127.0.0.1 6384
```

最后执行cluster nodes命令确认6个节点都彼此感知并组成集群

```
127.0.0.1:6379> cluster nodes
4fa7eac4080f0b667ffeab9b87841da49b84a6e4 127.0.0.1:6384 master - 0 1468073975551
5 connected
cfb28ef1deee4e0fa78da86abe5d24566744411e 127.0.0.1:6379 myself,master - 0 0 0 connected
be9485a6a729fc98c5151374bc30277e89a461d8 127.0.0.1:6383 master - 0 1468073978579
4 connected
40622f9e7adc8ebd77fca0de9edfe691cb8a74fb 127.0.0.1:6382 master - 0 1468073980598
3 connected
8e41673d59c9568aa9d29fb174ce733345b3e8f1 127.0.0.1:6380 master - 0 1468073974541
1 connected
40b8d09d44294d2e23c7c768efc8fcd153446746 127.0.0.1:6381 master - 0 1468073979589
2 connected
```

节点建立握手之后集群还不能正常工作，这时集群处于下线状态，所有的数据读写都被禁止。

### 3.3.分配槽

Redis集群把所有的数据映射到16384个槽中。每个key会映射为一个固定的槽，只有当节点分配了槽，才能响应和这些槽关联的键命令。通过cluster addslots命令为节点分配槽。这里利用bash特性批量设置槽（slots）

```
redis-cli -h 127.0.0.1 -p 6379 cluster addslots {0...5461}
redis-cli -h 127.0.0.1 -p 6380 cluster addslots {5462...10922}
redis-cli -h 127.0.0.1 -p 6381 cluster addslots {10923...16383}
```

关于集群伸缩、故障转移、节点通信等知识。 可参考《Redis开发与运维》

## 4.缓存设计

### 4.1.穿透优化

缓存穿透是指查询一个根本不存在的数据，缓存层和存储层都不会命中，通常出于容错的考虑，如果从存储层查不到数据则不写入缓存层。 整个过程分为如下3步

> 1.缓存层不命中。 2.存储层不命中，不将空结果写回缓存。 3.返回空结果

### 4.2.解决办法

#### **4.2.1.缓存空对象** 

存储层不命中后，仍然将空对象保留到缓存层中，之后再访问这个数据将会从缓存中获取，这样就保护了后端数据源。

![img](https://pic4.zhimg.com/80/v2-78c9759861a668cad7a502f16b296a63_720w.webp)

缓存空对象会有两个问题： 第一，空值做了缓存，意味着缓存层中存了更多的键，需要更多的内存空间（如果是攻击，问题更严重），比较有效的方法是针对这类数据设置一个较短的过期时间，让其自动剔除。

第二，缓存层和存储层的数据会有一段时间窗口的不一致，可能会对业务有一定影响。例如过期时间设置为5分钟，如果此时存储层添加了这个数据，那此段时间就会出现缓存层和存储层数据的不一致，此时可以利用消息系统或者其他方式清除掉缓存层中的空对象。

类似代码实现如下：

```
String get(String key) {
// 从缓存中获取数据
String cacheValue = cache.get(key);
// 缓存为空
if (StringUtils.isBlank(cacheValue)) {
    // 从存储中获取
    String storageValue = storage.get(key);
    cache.set(key, storageValue);
    // 如果存储数据为空，需要设置一个过期时间(300秒)
    if (storageValue == null) {
      cache.expire(key, 60 * 5);
    }
    return storageValue;
} else {
    // 缓存非空
    return cacheValue;
  }
}
```

#### **4.2.2.布隆过滤器拦截**

bloomfilter就类似于一个hash set，用于快速判某个元素是否存在于集合中，其典型的应用场景就是快速判断一个key是否存在于某容器，不存在就直接返回。布隆过滤器的关键就在于hash算法和容器大小，下面先来简单的实现下看看效果，我这里用guava实现的布隆过滤器：

```
<dependencies>  
     <dependency>  
         <groupId>com.google.guava</groupId>  
         <artifactId>guava</artifactId>  
         <version>23.0</version>  
     </dependency>  
</dependencies>  
复制代码
public class BloomFilterTest {
    private static final int capacity = 1000000;
    private static final int key = 999998;
    private static BloomFilter<Integer> bloomFilter = BloomFilter.create(Funnels.integerFunnel(), capacity);
    static {
        for (int i = 0; i < capacity; i++) {
            bloomFilter.put(i);
        }
    }
    public static void main(String[] args) {
        /*返回计算机最精确的时间，单位微妙*/
        long start = System.nanoTime();
        if (bloomFilter.mightContain(key)) {
            System.out.println("成功过滤到" + key);
        }
        long end = System.nanoTime();
        System.out.println("布隆过滤器消耗时间:" + (end - start));
        int sum = 0;
        for (int i = capacity + 20000; i < capacity + 30000; i++) {
            if (bloomFilter.mightContain(i)) {
                sum = sum + 1;
            }
        }
        System.out.println("错判率为:" + sum);
    }
}
```

可以看到，100w个数据中只消耗了约0.2毫秒就匹配到了key，速度足够快。然后模拟了1w个不存在于布隆过滤器中的key，匹配错误率为318/10000，也就是说，出错率大概为3%，跟踪下BloomFilter的源码发现默认的容错率就是0.03：

```
public String getByKey(String key) {
    // 通过key获取value
    String value = redisService.get(key);
    if (StringUtil.isEmpty(value)) {
        if (bloomFilter.mightContain(key)) {
            value = userService.getById(key);
            redisService.set(key, value);
            return value;
        } else {
            return null;
        }
    }
    return value;
}
```

## 5.雪崩优化

由于缓存层承载着大量请求，有效地保护了存储层，但是如果缓存层由于某些原因不能提供服务，于是所有的请求都会达到存储层，存储层的调用量会暴增，造成存储层也会级联宕机的情况。缓存雪崩的英文原意是stampeding herd（奔逃的野牛），指的是缓存层宕掉后，流量会像奔逃的野牛一样，打向后端存储。

![img](https://pic2.zhimg.com/80/v2-ebd7de7714e9545973edc2d34b00a54d_720w.webp)

预防和解决缓存雪崩问题，可以从以下三个方面进行着手

**1.保证缓存层服务高可用性。** 和飞机都有多个引擎一样，如果缓存层设计成高可用的，即使个别节点、个别机器、甚至是机房宕掉，依然可以提供服务，例如前面介绍过的Redis Sentinel和Redis Cluster都实现了高可用

**2.依赖隔离组件为后端限流并降级。**

无论是缓存层还是存储层都会有出错的概率，可以将它们视同为资源。作为并发量较大的系统，假如有一个资源不可用，可能会造成线程全部阻塞（hang）在这个资源上，造成整个系统不可用。降级机制在高并发系统中是非常普遍的：比如推荐服务中，如果个性化推荐服务不可用，可以降级补充热点数据，不至于造成前端页面是开天窗。 推荐一个Java依赖隔离工具Hystrix

**3. 提前演练。** 在项目上线前，演练缓存层宕掉后，应用以及后端的负载情况以及可能出现的问题，在此基础上做一些预案设定。

原文链接：https://zhuanlan.zhihu.com/p/353345711

作者：Hu先生的Linux

# 【NO.166】深入解读无服务器架构下的数据库

## 1.**Serverless 数据库**

随着业务的专注度越来越高，抽象的程度也越来越高，李志阳以汽车作为 Serverless 的类比，我们以前去购买一辆汽车，是为了开车去买车，现在可以租车、打车了，我们只需要知道目的地就行了，不需要关注过程，而是关注核心诉求。

在计算服务上面，演进也是类似的，我们从前是自建机房、维护整个机房；到后来在云上购买虚拟机部署业务，去负责里面的扩缩容；再到后来的函数计算，我们只需要关注业务带，整个 CICD 到部署扩容这些东西完全不用关注，整个业界的抽象程度会越来越高。

狭义的 Serverless 分为 FAAS 和 BAAS 两个方面，其基本特点是无需运维、主要以 API 的方式提供服务、按实际使用计费或无使用无费用等。假如用户去浏览网页的时候可能会涉及 CDN 资源，CDN 资源里面如果是静态内容，Serverless 就会通过对象存储里面把照片和视频拉取出来，如果是动态的内容就会触发一个函数计算，函数计算里面再去相应的云数据库里面拉取相应的资源，生成用户所要的动态内容。

如果要将数据库 Serverless 化，传统数据库是怎么样的呢？内存 CPU 是一个固定规格，用户会选择规格去购买，磁盘相对灵活，支持一定步长设置上限，以月预付的方式付费。Serverless 的特点，第一，自动扩缩容，用户不需要关注它的规格，当访问量上来的时候能够自动扩，当访问量下来的时候自动缩，不需要关注规格。第二，按照实际使用去付费。第三，不使用则不计费，存储方面，如果我计数据的存储只需要按实际的存储去计费，如果不使用，这些计算的资源其实不应该去收费。

## 2.**Serverless 数据库选型**

在讲述 Serverless 数据库选型之前，李志阳先介绍了云数据库架构的演进。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOqOpPCxLoMCGzH261uBczibS1KZdFMYFAYjd9CUQncSQYYG10EWQVprg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

左边是现在主流的架构——单体冗余架构，俗称一主多从，是现在绝大部分用户会使用的一种架构。这种架构的问题是什么呢？就是它的扩展性，不管是做实际的升降级，还是做扩展，都是需要数据搬迁去实现，随着用户量越来越大，搬迁的时间会越来越长。

为了解决这个问题，业界整体趋势是存算分离，计算和存储分离开独自扩展。延伸出来有两类，一个是 ShareNothing 架构，支持水平扩展，它的扩展能力非常强，这是它的最大优势；也存在部分缺点，其中最重要的是它是自研产品，存在 SQL 兼容性的问题，需要构建自己的生态，让用户进到相应生态里面使用，这它一直在努力的方向。另外一种是 SharedStorage 共享存储的架构，共享存储的架构里并没有改变查询引擎和 ACI 这些基础特性，整个兼容性可以做到 100%，完全兼容 MySQL。但它也有个缺点，就是只做了存储的池化，所以它的计算节点目前来说写还是没有办法扩展的，这个也是未来演进的方向。

随后，李志阳又关注到了 Serverless 数据库的用户群，主要面向中长尾用户，他们对于扩展性的诉求并不强，更多的关注使用的便利。兼容性是最重要的一个点，所以我们决定优先去做 Serverless 化，会选择 SharedStorage 的方式去做。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOE1fjA1E3aoHuiblGMTUq8dkw2YQdSQ9wUHAb6ShENFiabadc8tN0rSyA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

李志阳对 TDSQL-C 的总体架构进行了介绍，TDSQL-C 是腾讯云共享存储数据库，于 2017 年开始研发，在一开始就定下了一个基本原则，即复用云上的成熟组件。在计算层使用腾讯维护的 TXSQL，复用它的 bugfix 和新的特性；存储侧选择在腾讯内部有十几年历史的云硬盘 CBS，把 CBS 的存储部分和硬盘部分进行剖离，打造了 HiSTOR 存储平台，支持云硬盘、云间系统和数据库，数据安全完全由 HiSTOR 去保证，它的副本同步、故障自动迁移、数据校验平台都有一个完整的团队去支撑，这是产品能够完整对外售卖的重要基础。另外，它有很强的特性，比如它的备份/回档速度非常快，快照以 MB 粒度并发，可以达到 GB/s 级的速度。另外，提供 SSD 的场景之外，还有混存和 EC 版本，可以应对归档类的业务，提供更低成本的存储。

基于上述两个存储组件，在计算侧实现物质复制，使用 dbstore 做数据同步，实时生成并实时同步到备机，延时非常低，小于 1 毫秒。同时做日志下沉，传统的数据库先写日志异步，TDSQL-C 对存储只会写日志，通过后端 dbstore 的模块去将日志转化数据，日志下沉有非常多的优点这里不做赘述。

腾讯云是国内首家提供 Serverless 数据库的厂家，当时参考了国外 AWS 的 Aurora Serverless，它的三大特性是怎么实现的？

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOKQiarSg5A8icFibbgt7KwtTQlELpJ8TX4ia7AZQlYiaBskXcq3stGD5jszg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

最右边是有一个共享的虚拟池，规格不尽相同，当它扩容的时候是从 1 核 2G 到 4 核 8G 递增式的扩容，比如从 1 核 2G 扩大到 2 核 4G 里面就去一个池子里面找到 2 核 4G 的虚拟机将它挂载在虚拟机里面自动服务就可以提供自动扩容了。这里面有一个问题，假设用户访问过来本身需要 4 核 8G，他仍然需要 1 核 2G 一直递增到 4 核 8G，这个扩容的过程会相对慢一点。另外一个点，他每次去扩容的时候会选择一个新的虚拟机，所以说它的 BP 会失效，每次扩容的时候用户这边会有一次冷启动的过程。

按使用量计费做法比较简单，使用哪一个规格就按照那个规格计费就可以了。不使用不计费，最短是 5 分钟，上面有一个代理节点，知道用户有访问之后会按照刚才的方法共享池子里面找虚拟机拉起来业务的访问，对业务来说就是一个卡顿，但是他的链接是不会有影响的。优点说清楚了，但是它的缺点是什么呢？因为有代理节点，用户需要为这个代理节点去付费，整个恢复时长可能 30 秒，耗时相对比较长。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOM9E6LkhUMyz5h6o1sgRViaaBzgOvUEZ2Gy0I7xlUvqectJO7TweeVow/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)扩容时BP失效导致的问题

## 3.**TDSQL-C Serverless**

了解完业界情况之后，李志阳介绍了 TDSQL-C Serverless。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOs7tr9wG7P3vh6QwhTPV4JJADjRjuFP7ZbpJ26qMCWwqvoVeAbSNjUQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

整体架构方面，核心的模块就是 svls scheduler。中控节点做决策，要不要去扩缩容，按照计费的规则上传到云控制台那边去进行计费。这里相对于 Aurora Serverless 的区别在于暂停的应对，TDSQL-C Serverless 有 faker 模块，当用户上这个计算节点的时候会把四层的 vip pod 绑定到 faker 端口，用户过来可以识别出来是协议把它拉起，其优点在于用户不需要为代理节点付费。

整体架构介绍完以后，李志阳介绍了 TDSQL-C Serverless 在实现三大特性方面的能力。

从自动扩缩容来看，我们希望做到秒级的扩缩容，这个期间用户是无感知的，很平滑的。用户购买时会选择最小和最大规格，从 0.25 核开始到 4 核 8G，用户可以选择最小最大规格。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOnExv68sRJJPLyCMkmHSCrZnKkwr8zMHpjXuLQgme0O8XjibeQa8gorg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

右边的例子可以看到，如果用户选择最小是 1 核，最大是 2 核的情况下，在这边 Amazon Aurzora 是怎么做的呢？当业务访问过来的时候，纵坐标是 CPU，已经把 CPU1 核占满了，持续一段时间会扩大到 2 核 4G。TDSQL-C Serverless 是一上来就会给用户最大的规格，它的 CPU 资源是不会受限的，内存里面是从最小规格开始，假设用户的 CPU 超过了 1 核，一段时间之后就会把他的内存从 2G 扩到 4G，但是他的 CPU 资源不会受限，可以在设置的最大规格上任意使用他的 CPU 资源。

TDSQL-C Serverless 的优点是性能不受限，但是缺点是整机给他最大的资源规格，整机容易出现满负载的情况，因为我们 TDSQL-C 是做计算存储分离的，一旦监控整机的资源超过一定的比例之后，就会去做快速的迁移，迁移的概念就是在另外一个机组拉起这个实例就 OK 了，这个速度可以做到秒级，在资源整体的负载上面可以控制的比较精准。现在云数据库里面普遍的情况是 CPU 整机使用率都是相对偏低的，基于这两个 TDSQL-C Serverless 去做这个应对。

按使用量计费上面我们希望是秒级粒度，我们定义了一个算力单元，CPU 和内存指定的最小规格，规格都是 CPU 和内存比都是 1：2，内存除以 2 可以把它换算成 CPU，整体还是以 CPU 决定整个算力的。我们就通过每个小时 CCU 的值平均给用户进行计费。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavBiahJRsRkCic4cPL7t5DcgOx1VpvSbiar72micLZQPgGhT4kicDhP4iceOic4GicibnBgzK3ygkZVqIrVgnA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

李志阳举例说，用户假设选择 0.25 核到 4 核之间，可以看到整个表格 CCU 的计算。右边这边可以看到，如果业务的峰值过来，一开始会用到 3 核的时候，右边图里面可以直接上到 3 核的 CPU，那么就按照 3 核 CPU 的 CCU 去计费，很好的应对整个业务的负载。

最后一部分就是说不使用无计费，里面很核心的点是怎么做到快速恢复，自动启停的逻辑也是比较简单，只要 10 分钟内监测到没有用户访问就回收掉，业务访问回来的时候就把节点拉起。这里面核心的点是怎么快速的拉起，之前提过做日志下沉很大的好处，后端接收到日志之后会源源不断的回放，整个数据库在计算节点启动的过程不需要像传统数据库一样加载到日志然后回放，没有这个过程，所以启动相对比较简单。VDL 是日志已经持久化的日志点，小于 VDL 的话所有的日志多已经持久化了，在运行阶段把日志下放推行 VDL，同时把 VDL 具体值存储到后端。Recovery 阶段，第一个从后端获取 last-vdl，广播所有相关的小表获取，会找到最后的一个连续的 vdl 点作为日志恢复的点，就可以把这个实例拉起来，整个过程都是并行化的，也没有数据存放的过程，时间可以小于 100 毫秒。另外，我们也做了对整个 MySQL 的启动过程做了濒行数据化，现在能做到 2 秒内就能恢复这个实例。

## 4.**总结与展望**

李志阳表示，后续 TDSQL-C Serverless 会将冷启动从 2 秒缩到 200 毫秒，贴近云函数的时间做冷启动优化，整体思路跟 Aurora 相似，以共享池子在线挂载存储，减少进程启动时间。

另外，在进一步降低用户的存储成本方面正在考虑的优化方案，如果很长时间没有访问之后，将用户的数据转存到对象存储里面，用户只需要付对象存储的费用就可以了。

原文作者：李向阳

原文链接：https://mp.weixin.qq.com/s/_MfuhpSAtZFnB8p-xp5HUw

# 【NO.167】Facebook、谷歌、微软和亚马逊的网络架构揭秘

## **0. 前言**

本文主要讲一下国外的互联网巨头的骨干网，每家公司的网络都有独特设计，其中 Facebook 和 Google 的网络主要是服务自身的产品和广大互联网用户，Amazon 和 Microsoft 在云服务的业务相对多些。

Facebook、Google、Microsoft 的网络在公开的论文都有比较详细的描述，而 Amazon 的底层网络相对的公开资料不多，在即刻构建 AWS 技术峰会 2019，AWS 的架构师分享了 AWS 的底层网络（第 3 章节或者参考文献），还是很值得学习。

## **1. Facebook Network**

目前 Facebook 公司系列产品有月活 30+亿用户，他们需要一个服务不间断、随时能访问的网站。为了实现这个目标， Facebook 在后端部署了很多先进的子系统和基础设施 ，可扩展、高性能网络就是其中之 一。

### 1.1 Facebook 全球网络概述

#### **1.1.1 概述**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2kMWaibw2qms6BCYz1HQohdRsqOjUVo1iaTTYEibQEqnvkrjM8bjDamKag/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

Facebook 的网络本身就是一个大型分布式系统，针对不同任务划分成不同层次并采用不同的技术：

- 边缘（edge）
- 骨干（backbone）
- 数据中心（data centers）

遍布在用户密集区的数量庞大的**PoP/LB/cache**通过**骨干网**作为偏远地区、低成本、数量可控的超大型**数据中心**的延伸。

#### **1.1.2 Facebooke 网络流量模型**

从流量模型看，Facebook 分为两种类型。

1、外部流量：到互联网的流量（Machine-to-User）。

2、内部流量：数据中心内部的流量（Machine-to-Machine）。

其中，Facebook 数据中心内部的流量要比到互联网的流量大几个数量级，如下图所示。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2zvic8wvxvMEfETFd16Xdp2fY0NOsrAjEhH55eBzTx1GdWoX5rxjvPHw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **1.1.3 解决方案**

机器对机器的流量通常会大量爆发，这可能会干扰并影响正常的用户流量，从而影网络可靠性目标。Facebook 将跨数据中心与面向 Internet 的流量分离到不同的网络中，并分别进行优化。

Facebook 设计了连接数据中心的网络**Express Backbone** (EBB)。在边缘互联网出口则推出**Edge Fabric** 架构。

### 1.2 Facebook 骨干网 EBB（Express Backbone）

#### **1.2.1 设计理念**

- 快速演进、模块化、便于部署
- 避免分布式流量工程（基于 RSVP-TE 带宽控制）的问题，例如带宽利用率低，路由收敛速度慢。
- 在网络的边缘利用 MPLS Segment Routing 保证网络的精确性。

#### **1.2.2 EBB 架构**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2aSyIwhvP7GxibEuOs6Q8oK54tcCjkHfJwtAvz1PAPickLTbUgCjUU9Ag/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

- BGP 注入器：集中式的 BGP 注入路由的控制方式。
- sFlow 收集器：采集设备的状态传递给流量工程控制器。
- Open / R：运行在网络设备上，提供 IGP 和消息传递功能。
- LSP 代理（agent）：运行在网络设备上，代表中央控制器与设备转发表对接。

#### **1.2.3 流量工程控制平面（Traffic Engineering Controller）**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2xoeT6eRPr4taNdYdTwP34ePeGJdaX1Q2oe12cmj6aGZvu9eCz1pialA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

**NetworkStateSnapshot module**：网络状态快照模块，负责构建活动的网络状态和流量矩阵**PathAllocation module**：路径分配模块，负责基于活动流量矩阵并满足某些最优性标准来计算抽象源路由。**Drivermodule**：驱动程序模块，负责将路径分配模块计算出的源路由以 MPLS 段路由的形式推送到网络设备。

### 1.3 Edge Fabric

2017 年 Sigcomm 大会，Facebook 推出了面向互联网出口的边缘网络架构**Edge Fabric**。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2uMGVR0y4AAbtwodibPQcy7bHkklgRv1NdlQxgqgFyFALccxKLZryfNQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

A PoP has Peering Routers, Aggregation SWitches, and servers. A private WAN connects to datacenters and other PoPs。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2ovvAficqPH3HNswLWKxvcF6Jyibc2O1FOic3fs0NYCsvw6Uiau1LEQwnlQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)Edge Fabric组件

- Edge Fabric 有一个 SDN/BGP 控制器。
- SDN 控制器采用多重方式搜集网络信息，如 BMP 采集器、流量采集器等。
- 控制器基于实时流量等相关信息，来产生针对某个 Prefix 的最优下一跳，指导不同 Prefix 在多个设备之间负载均衡。
- 控制器和每个 Peering Router 建立另一个 BGP 控制 session，每隔特定时间用来改写 Prefix，通过调整 Local Preference 来改变下一跳地址。
- BGP 不能感知网络质量，Edge Server 对特定流量做 eBPF 标记 DSCP，并动态随机选一小部分流量来测量主用和备用 BGP 路径的端到端性能。调度发生在 PR 上，出向拥塞的 PR 上做 SR Tunnel 重定向到非拥塞的 PR 上，如下图所示.

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2Z4jBbTQCian52CJ1vJNxeTXv15CnlwVc7uV6k8AhCPsmJ7IHGOAibMQQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)img

由此看见 Edge Fabric 有一些限制：

- SDN 控制器单点控制 PR 设备。如果 PR 设备已经 Overload，需要通过 PBR 和 ISIS SR Tunnel 转移到另一个没有拥塞的 PR，流量路径不够全局优化。
- 控制器只能通过 Prefix 来控制流量，但是同一个 prefix，可能承载视频和 Voice 流量，带宽和时延要求不同，Edge Fabric 没有 Espresso 那么灵活。

### 1.4 总结

Facebook 在骨干网、边缘网络都是使用 BGP 路由协议进行分布式控制，控制通道简单，避免多协议导致的复杂性，而对于流量工程采用集中的处理。

## **2. Google Network**

### 2.1 Google 全球网络概述

Google 全球有 30+个数据中心，100+多个 POP 站点，同时在不同运营商网络中有很多 Cache 站点。信息从 Google 数据中心信息大致经过 Data Center-POP-Cache 三级网络发送到最终用户。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2t5sM22eFDpu8BL21p7R2q9NtZLib2TIdiaWpFxOsx0rPRwGtibyW6E9xQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)Google Cloud Netwok

Google 的广域网实际上分为 B2 全球骨干网和 B4 数据中心互联网，边缘网络是 Espresso，数据中心内部则是 Jupiter，如下图所示。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2AiaxSv4dcljN9VDDYtk1cMo8bg6JibhPtpYSH2qCxkKrw6Yr6fb86tUA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)Google网络构成

**B2**：面向用户的骨干网，用于连接 DC、CDN、POP、ISPs 等。B2 主要承载了面向用户的流量，和少部分内部流量（10%），带宽昂贵，整体可用性要求很高，利用率在 30%~40%之间。B2 采用商用路由器设备，并且运行 MPLS RSVP-TE 进行流量工程调节。

**B4**：数据中心内部数据交换的网络，网络节点数量可控，带宽庞大，承载的 Google 数据中心间的大部分流量。B4 承载的业务容错能力强，带宽廉价，整体利用率超过 90%。使用自研交换机设备，为 Google SDN 等新技术的试验田。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2bDIZoPicOZNTUb3Q4mgGUcd4TlX6z6XLf2z2MGb7E2EjSpwtM4LUZQw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)B4和B2架构

**Espresso**：边缘网络或者互联网出口网络，将 SDN 扩展到 Google 网络的对等边缘，连接到全球其他网络，使得 Google 根据网络连接实时性的测量动态智能化地为个人用户提供服务。

### 2.2 B2

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2sYkicbEibboCqrwqjX38DGaCm99lK55QaRC7UO4Z2Z0A56bebsTTaZfw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)B2物理架构

- PR：Peering Router，对等路由器，类似 PE 设备，主要是其他运营商网络进行对接。
- BR：Backbone Router，骨干网路由器，类似 P 设备。
- LSR：Label Switch Router，标签交互路由器。
- DR：Datacenter Route：数据中心路由器。

### 2.3 B4

B4 是业界第一个成功商用的数据中心互联的 SDN 网络。

- 交换机硬件是 Google 定制的，负责转发流量，不运行复杂的控制软件。
- OpenFlow Controller (OFC) 根据网络控制应用的指令和交换机事件，维护网络状态。
- Central TE Server 是整个网络逻辑上的中心控制器。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2CAqE9lQGXkvWqh3EwyTH1G96mCr8dfib83pDVez2XaytYzuvibEp4YBg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)B4的架构

Central TE (Traffic Engineering) Server：进行流量工程。

Network Control Server (NCS)：数据中心（Site）的控制器，其上运行着 OpenFlow Controller (OFC) 集群，使用 Paxos 协议选出一个 master，其他都是热备。

交换机（switch）：运行着 OpenFlow Agent (OFA)，接受 OFC 的指令并将 TE 规则写到硬件 flow-table 里。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2iaP59Qibz7wCMicqEHEVVfsiboyEPfR3DoMoYm8dmiayQHh0I7Uiaag44lJQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)流量工程

注：网上有很多介绍 B4 的文章，本文从略。

### 2.4 Espresso

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2W2Bq3uPUcOzyKbTyX2olpLps4hBVnjDdsPWtlq4jFnvGNesOvlMj3A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

为了更好进行流量调度，Espresso 引入了全球 TE 控制器和本地控制器（Location Controller）来指导主机（host）发出流量选择更好的外部 Peering 路由器/链路，进行 per flow Host 到 Peer 的控制，并且解耦了传统 Peering 路由器，演进为 Peering Fabric 和服务器集群（提供反向 Web 代理）。

控制和转发流程：

- 外部系统请求进入 Espresso Metro，在 Peering Fabric 上被封装成 GRE，送到负载均衡和反向 web 代理主机处理。如果可以返回高速缓存上的内容以供用户访问，则该数据包将直接从此处发回。如果 CDN 没有缓存，就发送报文通过 B2 去访问远端数据中心。
- 主机把实时带宽需求发送给全局控制器（Global Controller）。
- 全局控制器根据搜集到的全球 Internet Prefix 情况，Service 类型和带宽需求来计算调整不同应用采用不同的 Peering 路由器和端口进行转发，实现全局出向负载均衡。
- 全局控制器通知本地控制器来对 host 进行转发表更改。同时在 Peering Fabric 交换机上也配置相应的 MPLSoGRE 解封装。
- 数据报文从主机出发，根据全局控制器指定的策略，首先找到 GRE 的目的地址，Peering Fabric 收到报文之后，解除 GRE 报文头。根据预先分配给不同外部 Peering 的 MPLS 标签进行转发。

## **3. Amazon Global Network**

注:本章节主要是参考**即刻构建 AWS 技术峰会 2019**的介绍，详细文档和视频见参考文献。

### 3.1 AWS 全球网络概述

#### **3.1.1 概述**

AWS 是一家全球公有云提供商，它拥有一个全球基础设施网络，以运行和管理其支持全球客户的众多不断增长的云服务。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2pZCId0OWgibWKfFOQqJBKGI7fYmibicImPYlzZvnicnn34PJ8HUf81jRNQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

构成 AWS Global Infrastructure 的组件有：

1. Availability Zones (AZs)
2. Regions
3. Edge PoPs

- Regional Edge Caches

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2IcfQHKEdmBvYWQn8LmuRNEjl5eQ7fCwB9e2dekmavh8lGUhHhwJA5w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **3.1.2 设计理念**

- 安全性：安全是云网络的生命线。
- 可用性：需要保证当某条线路出现故障的时候，不会影响整个网络的可用性。
- 故障强隔离：当网络发生故障的时候，尽量把故障限制在某个区域内。
- 蜂窝架构：一个个网络模块构成的蜂窝式网络架构。
- 规模：支撑上百万客户的应用网络需求。
- 性能：对网络的吞吐量、延迟要求较高。

#### **3.1.3 网络通信案例解析**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw29XOZkehKs6Mhs4f7FxctvuiaO8GbRyxYh3peFXRobB5mjW9PIZSIcpw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

如上图所示，一个猫咪要去 AWS 的服务器中获取一张图片，流量首先通过 Internet 进入到 AWS Region，Region 包括 AZ，AZ 中有 VPC，在 VPC 中有 Server，Server 上面有图片，这是一种比较简单的抽象流程，但是如果把网络剥开一层再去看，其实会变得更加复杂，如下图：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw28dT6gCwxqEX7p3m5iavfmn4U1p90QZUPOcd03JCKHPuicVv0S7ZDSrkw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

可以看到包含了更多的 AWS 网络基础设施，流量首先进入 Edge PoP，这个也是 CDN 的站点，流量进来之后到骨干网络 Backbone，然后再进入 AWS Region，经过 Transit Center 进入 AZ。

### 3.2 AWS 全球网络的 Region 架构

#### **3.2.1 Availability Zone**

AWS Region 是由 Availability Zone 组成。可用区(Availability Zones)实质上是 AWS 的物理数据中心。在 VPC 中创建的计算资源、存储资源、网络资源和数据库资源都是托管在 AWS 的物理数据中心。

每个 AZ 至少会有一个位于同一区域内的其他 AZ，通常是一个城市，他们之间由高弹性和极低延迟的专用光纤连接相连。但是，每个 AZ 将使用单独的电源和网络连接，这使得 AZ 之间相互隔离，以便在单个 AZ 发生故障时最大限度地减少对其他 AZ 的影响。

每个 Region 有两个 Transit Center，每个 Transit Center 和下面的每个 Datacenter 都有网络互联，同样 Datacenter 之间也有网络互联，这样可以确保 AWS 网络的可用性，部分网络基础设施故障也不会影响整个 Region 的可用性。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2GsGq48ZP8EXOwx3PNCnldwmcyic6gAamqUzdz4smKgX0BVmjVLD93vw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **3.2.2 AWS Regon 数据中心构造**

AWS 采用蜂窝式的网络架构。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2svg49lYOoMqstY6dGCvu2KJerlibdREQEUmKFglmOFGOYwueFSQ3icBg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

在图中间都是一个个小的模块，每个模块都有不同的一些功能，如 Access Cell 主要做主机的网络接入，Core Edge Cell 联通着 Transit Centers，进而把网络流量送进 AWS backbone。

每个 Cell 都肩负着不同的功能，Cell 和 Cell 之间都进行互联，在每一层，都可以通过平行扩展 Cell 来扩展整个网络的承载量，达到一个可伸展的网络。

每个 Cell 是一个单核路由器，端口比较少，可以控制故障域，转发架构更简单。

### 3.3 AWS 全球骨干网（Global Backbone）

AWS Direct Connect、互联网连接、区域到区域传播和 Amazon CloudFront 到 AWS 服务的连接都是需要 AWS 骨干网。

和 Region 相似，全球骨干网也是采用了蜂窝式的一个网络架构，中间是大量的光纤连接，外层是负责一些网络功能的 Cell。

Transit Center Cell 用来连接 Region 内部的数据中心，Edge Pop Cell 用来连接 PoP 节点，Backbone Cell 用来连接远端的 PoP 节点进而连接到远端 Region 的数据中心。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw290vfu6ZdywLgGr5ibTdfic0nD6OsWibNDmoz5V0v3TrakPWOjZ6EqtspQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 3.4 AWS Edge PoP

AWS Edge PoP 是部署在全球主要城市和人口稠密地区的 AWS 站点。它们远远超过可用区域的数量。

AWS Edge PoP 对外就是连接的一张张 ISP 的网络。运营商接入 AWS 的骨干网络两个地方，一个是 Edge PoPs，另外一个 AWS 区域的网络中转中心（Transit Centers）。

Edge PoP 很大的一个作用就是对外扩充 AWS 的网络，同一个 Edge PoP 可以和运营商进行多次互联，获得至外网网络最优的互联。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2nQmOpI3fsbLsJyRc195Y2mnG8eVexY9HHHibKqfvQbQAvkBvibBN3F0Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

边缘站点也是采用了蜂窝式的架构，Backbone Cell 连接 AWS 骨干网络，External Internet Cell 连接外部的 Internet 网络，同时还包括一些 AWS Edge 服务网的一些 Cell，如连接 CloudFront、Route 53、Direct Connect 和 AWS Shield，这些服务都存在于 AWS Edge PoPs 中。

### 3.5 总结

可以看到 AWS 在网络的各部分都采用了蜂窝式的架构，让这个网络的扩展性大大提升。并且通过采用主动式数据信道监控，从 AWS 服务日志采集互联网性能数据，以及互联网流量工程管理来达到互联网边缘的监控与自我修复。

## **4. Microsoft Network**

### 4.1 Microsoft Network 概述

Microsoft 在布局云计算取得很大的成功，网络的布局功不可没、其中 Azure 拥有超过 165,000 英里的私有光纤，跨越全球 60 多个区域和 170 多个网络 PoP。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2WeE0CorbHKJKay6PpqC27HPFf8KibK1vicft0aRjKbOpeCvMpVtfUpUA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)Azure网络

### 4.2 Microsoft SWAN 网络

微软 SWAN 广域网 DCI 控制器也是一个典型的 SDN 网络，从最早的静态单层 MPLS Label 构造的端到端隧道，到最新的基于 BGP-TE SR 的全球 DCI 互联解决方案，可以实现 95%的跨数据中心链路利用率。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw2tNq9FI3tRicwkUZK3UhCSWd5ZRaqQobsBbmdiaTCWM55EwwIHr1KBLHg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)Microsoft SWAN架构

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauHZ22Jq0USicIzHuX3bflw27ZZnaIplQpkFx1R4hlpyjC8LAddLzZOdboyIcAx9Fsg26FSQmOpRVg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)Microsoft SWAN控制面

- RSVP-TE/SR-TE。
- 集中式 TE 资源分配算法。
- 服务间通过资源分配模块协作。
- 每个 Host 上都有代理，负责带宽请求和限流。

**注：具体细节可以见参考文献的论文**

## **5. 数据中心网络**

再补充一下 Google 和 Facebook 的数据中心网络设计。

### 5.1 Facebook 的 f4 和 f6 中文翻译

http://arthurchiao.art/blog/facebook-f4-data-center-fabric-zh/

http://arthurchiao.art/blog/facebook-f16-minipack-zh/

### 5.2 Google Jupiter 中文解读

http://zeepen.com/2015/12/31/20151231-dive-into-google-data-center-networks/ 原文 http://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p183.pdf

## 6. 总结

从**技术**的角度看，互联网公司的网络演进是一个 SDN 的过程。SDN 是一个网络标准化的过程，是一个通信系统互联网化的状态，贯穿着网络的控制面、转发面、管理面。

从**运营**的角度看，互联网公司的网络演进是从“所有流量 all in one” 到互联网思路构建网络，网络具有分布式、模块化、低耦合等特点。

从**布局**的角度看，互联网公司的网络布局也是技术实力全球化扩张的缩影。也希望中国的互联网公司也能不断的扩张边界，进入全球化的食物链的顶端。

**参考文献**

**Facebook 网络**

https://engineering.fb.com/2017/05/01/data-center-engineering/building-express-backbone-facebook-s-new-long-haul-network/

**Google 网络**

https://cseweb.ucsd.edu/~vahdat/papers/b4-sigcomm13.pdf

**AWS 网络**

揭秘 AWS 底层网络是如何构成的

https://blog.51cto.com/14929722/2533206

AWS 底层网络揭秘

https://www.bilibili.com/video/av99088073/

**Microsoft 网络**

https://www2.cs.duke.edu/courses/cps296.4/compsci590.4/fall14/Papers/SWAN.pdf

**Facebook 的 F4，F16 网络架构**

http://arthurchiao.art/blog/facebook-f4-data-center-fabric-zh/

http://arthurchiao.art/blog/facebook-f16-minipack-zh/

**Google Jupiter 实现**

中文解读

http://zeepen.com/2015/12/31/20151231-dive-into-google-data-center-networks/

原文

http://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p183.pdf

原文作者：云网漫步

原文链接：https://mp.weixin.qq.com/s/MPBk9wdYsE48H7OXWAd5bA

# 【NO.168】Nginx 架构浅析

## **1.Nginx 基础架构**

nginx 启动后以 daemon 形式在后台运行，后台进程包含一个 master 进程和多个 worker 进程。如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavkzcOqibK2l3xbf5wHIhibA05XyKrmjqibXn76CzlYj1wA1GUmjCCw0tuWAMNI2oT6kfMfGOkTDFxbQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)master与worker

nginx 是由一个 master 管理进程，多个 worker 进程处理工作的多进程模型。基础架构设计，如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavkzcOqibK2l3xbf5wHIhibA0rJpg7py6IQko9XQuAKEnib9xHlkSzSElcKBvQ2265cOyvABicfuk9cPg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)基础架构设计

master 负责管理 worker 进程，worker 进程负责处理网络事件。整个框架被设计为一种依赖事件驱动、异步、非阻塞的模式。

如此设计的优点：

- 1.可以充分利用多核机器，增强并发处理能力。
- 2.多 worker 间可以实现负载均衡。
- 3.Master 监控并统一管理 worker 行为。在 worker 异常后，可以主动拉起 worker 进程，从而提升了系统的可靠性。并且由 Master 进程控制服务运行中的程序升级、配置项修改等操作，从而增强了整体的动态可扩展与热更的能力。

## **2.Master 进程**

### 2.1 核心逻辑

master 进程的主逻辑在`ngx_master_process_cycle`，核心关注源码：

```
ngx_master_process_cycle(ngx_cycle_t *cycle)
{
    ...
    ngx_start_worker_processes(cycle, ccf->worker_processes,
                                        NGX_PROCESS_RESPAWN);
    ...


    for ( ;; ) {
        if (delay) {...}

        ngx_log_debug0(NGX_LOG_DEBUG_EVENT, cycle->log, 0, "sigsuspend");

        sigsuspend(&set);

        ngx_time_update();

        ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle->log, 0,
                             "wake up, sigio %i", sigio);

        if (ngx_reap) {
            ngx_reap = 0;
            ngx_log_debug0(NGX_LOG_DEBUG_EVENT, cycle->log, 0, "reap children");
            live = ngx_reap_children(cycle);
        }

        if (!live && (ngx_terminate || ngx_quit)) {...}

        if (ngx_terminate) {...}

        if (ngx_quit) {...}

        if (ngx_reconfigure) {...}

        if (ngx_restart) {...}

        if (ngx_reopen) {...}

        if (ngx_change_binary) {...}

        if (ngx_noaccept) {
            ngx_noaccept = 0;
            ngx_noaccepting = 1;
            ngx_signal_worker_processes(cycle,
                                                  ngx_signal_value(NGX_SHUTDOWN_SIGNAL));
        }
    }
 }
```

由上述代码，可以理解，master 进程主要用来管理 worker 进程，具体包括如下 4 个主要功能：

- 1.接受来自外界的信号。其中 master 循环中的各项标志位就对应着各种信号，如：`ngx_quit`代表`QUIT`信号，表示优雅的关闭整个服务。
- 2.向各个 worker 进程发送信。比如`ngx_noaccept`代表`WINCH`信号，表示所有子进程不再接受处理新的连接，由 master 向所有的子进程发送 QUIT 信号量。
- 3.监控 worker 进程的运行状态。比如`ngx_reap`代表`CHILD`信号，表示有子进程意外结束，这时需要监控所有子进程的运行状态，主要由`ngx_reap_children`完成。
- 4.当 woker 进程退出后（异常情况下），会自动重新启动新的 woker 进程。主要也是在`ngx_reap_children`

### 2.2 热更

#### **2.2.1 热重载-配置热更**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavkzcOqibK2l3xbf5wHIhibA0XPu1mQ1Ev3VQ2g3dlXVSXy6Ws6hzepxaToYibSBtZ5Z4dF5uEoIas7Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)热重载

nginx 热更配置时，可以保持运行中平滑更新配置，具体流程如下：

- 1.更新 nginx.conf 配置文件，向 master 发送 SIGHUP 信号或执行 nginx -s reload
- 2.master 进程使用新配置，启动新的 worker 进程
- 3.使用旧配置的 worker 进程，不再接受新的连接请求，并在完成已存在的连接后退出

#### **2.2.2 热升级-程序热更**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavkzcOqibK2l3xbf5wHIhibA0bicSzcPO3JTr5aib6NkRibicuesib0JAlqt70dgvlsvYW8ubgdibB8L76tHQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)热升级

nginx 热升级过程如下：

- 1.将旧 Nginx 文件换成新 Nginx 文件（注意备份）
- 2.向 master 进程发送 USR2 信号（平滑升级到新版本的 Nginx 程序）
- 3.master 进程修改 pid 文件号，加后缀.oldbin
- 4.master 进程用新 Nginx 文件启动新 master 进程，此时新老 master/worker 同时存在。
- 5.向老 master 发送 WINCH 信号，关闭旧 worker 进程，观察新 worker 进程工作情况。若升级成功，则向老 master 进程发送 QUIT 信号，关闭老 master 进程；若升级失败，则需要回滚，向老 master 发送 HUP 信号（重读配置文件），向新 master 发送 QUIT 信号，关闭新 master 及 worker。

## **3.Worker 进程**

### 3.1 核心逻辑

worker 进程的主逻辑在`ngx_worker_process_cycle`，核心关注源码：

```
ngx_worker_process_cycle(ngx_cycle_t *cycle, void *data)
{
    ngx_int_t worker = (intptr_t) data;

    ngx_process = NGX_PROCESS_WORKER;
    ngx_worker = worker;

    ngx_worker_process_init(cycle, worker);

    ngx_setproctitle("worker process");

    for ( ;; ) {

        if (ngx_exiting) {...}

        ngx_log_debug0(NGX_LOG_DEBUG_EVENT, cycle->log, 0, "worker cycle");

        ngx_process_events_and_timers(cycle);

        if (ngx_terminate) {...}

        if (ngx_quit) {...}

        if (ngx_reopen) {...}
    }
}
```

由上述代码，可以理解，worker 进程主要在处理网络事件，通过`ngx_process_events_and_timers`方法实现，其中事件主要包括：网络事件、定时器事件。

### 3.2 事件驱动-epoll

worker 进程在处理网络事件时，依靠 epoll 模型，来管理并发连接，实现了事件驱动、异步、非阻塞等特性。如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavkzcOqibK2l3xbf5wHIhibA08AoXwjpIHibw3gKsqUwx5OFSe0gyp3TJibdK2W9VeGm7iaEhTN4Cjx2YA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)infographic-Inside-NGINX_nonblocking

通常海量并发连接过程中，每一时刻（相对较短的一段时间），往往只需要处理一小部分有事件的连接即`活跃连接`。基于以上现象，epoll 通过将`连接管理`与`活跃连接管理`进行分离，实现了高效、稳定的网络 IO 处理能力。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavkzcOqibK2l3xbf5wHIhibA0Te31anxlCPmr6BfiatKnFvK2yELrhnd91jPInoTmibicpq7zMnOwiaPlQA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)网络模型对比

其中，epoll 利用红黑树高效的增删查效率来管理`连接`，利用一个双向链表来维护`活跃连接`。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavkzcOqibK2l3xbf5wHIhibA0wafvhzDibWWdTmAMgq9EfP0X3bREwPsE57pucUg919pcZIWEXKpOWsQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)epoll数据结构

### 3.3 惊群

由于 worker 都是由 master 进程 fork 产生，所以 worker 都会监听相同端口。这样多个子进程在 accept 建立连接时会发生争抢，带来著名的“惊群”问题。worker 核心处理逻辑`ngx_process_events_and_timers`核心代码如下：

```
void ngx_process_events_and_timers(ngx_cycle_t *cycle){
    //这里面会对监听socket处理
    ...

    if (ngx_accept_disabled > 0) {
            ngx_accept_disabled--;
    } else {
        //获得锁则加入wait集合,
        if (ngx_trylock_accept_mutex(cycle) == NGX_ERROR) {
            return;
        }
        ...
        //设置网络读写事件延迟处理标志，即在释放锁后处理
        if (ngx_accept_mutex_held) {
            flags |= NGX_POST_EVENTS;
        }
    }
    ...
    //这里面epollwait等待网络事件
    //网络连接事件，放入ngx_posted_accept_events队列
    //网络读写事件，放入ngx_posted_events队列
    (void) ngx_process_events(cycle, timer, flags);
    ...
    //先处理网络连接事件，只有获取到锁，这里才会有连接事件
    ngx_event_process_posted(cycle, &ngx_posted_accept_events);
    //释放锁，让其他进程也能够拿到
    if (ngx_accept_mutex_held) {
        ngx_shmtx_unlock(&ngx_accept_mutex);
    }
    //处理网络读写事件
    ngx_event_process_posted(cycle, &ngx_posted_events);
}
```

由上述代码可知，Nginx 解决惊群的方法：

- 1.将连接事件与读写事件进行分离。连接事件存放为`ngx_posted_accept_events`，读写事件存放为`ngx_posted_events`。
- 2.设置`ngx_accept_mutex`锁，只有获得锁的进程，才可以处理连接事件。

### 3.4 负载均衡

worker 间的负载关键在于各自接入了多少连接，其中接入连接抢锁的前置条件是`ngx_accept_disabled > 0`，所以`ngx_accept_disabled`就是负载均衡机制实现的关键阈值。

```
ngx_int_t             ngx_accept_disabled;
ngx_accept_disabled = ngx_cycle->connection_n / 8 - ngx_cycle->free_connection_n;
```

因此，在 nginx 启动时，`ngx_accept_disabled`的值就是一个负数，其值为连接总数的 7/8。当该进程的连接数达到总连接数的 7/8 时，该进程就不会再处理新的连接了，同时每次调用'ngx_process_events_and_timers'时，将`ngx_accept_disabled`减 1，直到其值低于阈值时，才试图重新处理新的连接。因此，nginx 各 worker 子进程间的负载均衡仅在某个 worker 进程处理的连接数达到它最大处理总数的 7/8 时才会触发，其负载均衡并不是在任意条件都满足。如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavkzcOqibK2l3xbf5wHIhibA0ib3xvf51icrkAibq9skzqfCCtXFjTaibUqKeicoVP88dfltKmW2nxSZYKeA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)实际工作情况

其中'pid'为 1211 的进程为 master 进程，其余为 worker 进程

## **4.思考**

### 4.1 为什么不采用多线程模型管理连接？

- 1.无状态服务，无需共享进程内存
- 2.采用独立的进程，可以让互相之间不会影响。一个进程异常崩溃，其他进程的服务不会中断，提升了架构的可靠性。
- 3.进程之间不共享资源，不需要加锁，所以省掉了锁带来的开销。

### 4.2 为什么不采用多线程处理逻辑业务？

- 1.进程数已经等于核心数，再新建线程处理任务，只会抢占现有进程，增加切换代价。
- 2.作为接入层，基本上都是数据转发业务，网络 IO 任务的等待耗时部分，已经被处理为非阻塞/全异步/事件驱动模式，在没有更多 CPU 的情况下，再利用多线程处理，意义不大。并且如果进程中有阻塞的处理逻辑，应该由各个业务进行解决，比如 openResty 中利用了 Lua 协程，对阻塞业务进行了优化。

原文作者：handsomeli，腾讯 IEG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/gd59U50tlJPa4B4avXRG1Q

# 【NO.169】内核调试技巧--systemtap定位丢包原因



内核收发包，可能会由于backlog队列满、内存不足、包校验失败、特性开关如rpf、路由不可达、端口未监听等等因素将包丢弃。

在内核里面，数据包对应一个叫做skb(sk_buff结构)。当发生如上等原因丢包时，内核会调用***kfree_skb***把这个包释放（丢掉）。kfree_skb函数中已经埋下了trace点，并且通过__builtin_return_address(0)记录下了调用kfree_skb的函数地址并传给location参数，因此可以利用systemtap kernel.trace来跟踪kfree_skb获取丢包函数。考虑到该丢包函数可能调用了子函数，子函数继续调用子子函数，如此递归。为了揪出最深层的函数，本文通过举例几个丢包场景，来概述一种通用方法，来定位丢包原因及精确行号。

## 1.**用例1：ospf hello组播报文被drop，导致ospf 邻居不能建立**

### 1.1 方法：saddr是10.10.2.4的skb是我们关心的数据。

1、 drop_watch跟踪下kfree_skb，定位函数位置：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWt7b2VIqjUyrLCdbr1nkx6AuUKz5gmic3CHAZ0CaYibMAveS4jxD4kaBA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

2、 查看ip_rcv_finish内核源码，编写stap脚本，通过pp()行号来跟踪执行流：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWR5z053Sj8uztIBiaMFg8vfEovM8xs6w887g60X3uicR7q2xGA573V4sQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWEB7BUQKcgL2JlUDaKjic2ZHlWeiamgQvdkjMw4QRuINY6E0lslsNWOJg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3、 从行号可知ip_route_input_noref返回错误，编写stap脚本，查看ip_route_input_noref的返回值：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWPBkwQCH6d9xSsiasicPE7SpCIiag7c0CNqk0VK4AVy4jeQ7mKlg95wBBg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

返回-22，即-EINVAL。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWb3q5pYd2AFkWR161x4K0lAB5hWhZDDicMialCibb7P4aUopE9iaFLtdkHQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

4、 即ip_route_input_rcu返回错误，同样方法，通过pp()行号来跟踪执行流：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWR6FNIjH5vrgiaQDicYic34lHz0V9cbwl4B6PZ3cc4yWdjtbvicuf3ibnkrg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWjC3gz5mRkA6iavia70BTicpAvibMKsLia99LgB4umDicQic95ToRWWUYkVK6w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWOmXD64Z5PGSOvYXf8CxyskI61qbM6SrfJpYmXsQzG7mIlRovng3iaVQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

此路不通，看下原因：原来有些行号$saddr不能访问。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLW5xdhWjAGWJDqYicxjPVZScgP6v6ZmVBNMxDGO9Yyuhpq3uMwzIDpCpw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

5、 此时需要码一码代码了，由于ospf的hello报文是组播,所以下图中红色方框的ipv4_is_multicast为真：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWaJgHns98IDDjiadl7qS2w5K0y9vH2iatodJK0yKrhOnSg5kias2NJW2Qw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

先看下__in_dev_get_rcu(dev)的返回值是否为null，同时也把dev->priv_flags的值打印出来：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWtAxE3pKOEqwUp4xzZBs3rxZnxW1ibfrBLLL7emSXGUOvhtcQuHa41hA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

__in_dev_get_rcu(dev)返回不为null，再看ip_check_mc_rcu函数的返回值：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLW7E4M1VDicmVnWSzeHRzRgbp6IWQFRoggHoCuHOEu1je7y0SfArqzsiaw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

返回值为0，同时通过打印的dev->priv_flags 为0x00029820，可判断netif_is_l3_slave为假：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWXoicDF05Dp8ribicfr6ZnzPWBMpt4eXL68NfRLOdpSwhz2icwh67b6catA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

另外ospf使用的组播ip为224.0.0.5或224.0.0.6，本例中使用的是224.0.0.5。因此ipv4_is_local_multicast为真：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLW4KE16ia9ufic6rfiaazjjCBGug4t8NxQCS3xKyHLYOeSxk8rYW2gLvDDA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

最终精确定位到了ip_check_mc_rcu：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWUt6OVlrFwjIK0XRVZic4jtFUYvLRsQnafTrOhAtpr0CeqjfrfsftcDg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

先看下in_dev->mc_hash是否为null，通过cast强转来访问结构体成员mc_hash：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWEdHnuBF7ITfSXZjdQvIRdUhsZeRdeTcPqyUTESSiaibCG1yJsRt6UVRA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

则需要走如下分支：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWc5jC2NYRJsux16U9q98Zf0iclkibo6BSvH2s3OKqicF0BMCqFwnrTDrPg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

需要遍历in_dev->mc_list，有两种方法，一种是通过crash工具查看，遍历in_dev->mc_list，输出im->multiaddr,如下：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWleEWV3o2rcJ5JnicnlFBTDeeSMAkicVZMjSX2YwNmoLz63SIqo2YGLEA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

既然我们的主题是stap，那么继续stap，这里通过嵌入c代码来遍历mc_list,输出multiaddr到dmesg：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWbotsGluNnsicibh1HnnwMHcSVRFn6eB0WibKIe8Wia7EsXZkcLaBS14VXA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWB9hofbnvb8AkHHzbBegwjmgfWboNKPPPSDBClHVlysQDNnKbBPYOVg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

存在接口加入了224.0.0.5组播组，革命尚未成功，继续跟踪下面的代码：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWJ0kq9byAA67I1CDLXibfs8E7GibJgE5D9iad3diaicCfbyf4WTVmgaNAiaYw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

同样的方式，编写stap脚本或者crash通过struct展开im->sources来查看。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLW8CyKyhbIVTH9LrOJiaHbibGd8HZNfpp6nIKRkFgZib4zZJmqwBiath6fQA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWGrzQRcLoGkj2hJzXKpGVPKAjwCvQibJyB2ozGCXUg9ezvHJbVrTglsw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

至此，一目了然，原来报文接收的接口没有加入组播组224.0.0.5。google一下：

https://www.cnblogs.com/my_life/articles/6077569.html

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWVYwtP5kxugJ7OserXwABmydL3LgbBbvwogAvtmIeuKTvUs78cYpCjg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

综述：那为什么ens5没有加入组播组呢，这要从ospf的原理来说起，ospf建立邻居的时候，是不需要指定接口的，那用于建立邻居的接口是如何选择的呢：实际上是根据指定的area network配置来选择的。当配置area network的时候，会查看系统当前路由，选择合适的接口加入组播组，进而创建邻居。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWV7oYrKUdg1LBnJFx1DLxdyEmm5oLcgdqzyZxd4Ra9Xf6iadpPtGt7mQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLW4x9aSucxBWmrdyaUpETGicYeBB1RdRoHYOPb8DgQIibqCzV92Zkt7MrQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWI20bvgLVRZBI9aZBQWcvJwwzpY8oykHz51LfDu9YgMwzmbZhiapHX9Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWfjSCjraL9mk9lVs3LTPhUQ0ibzFXIDviaMwXlS1mD9wF0OHz6eBhy1icA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLW4fbgWnQdSXuAALslrPmc056icsdKkD9ib9C5vSIBGzUN8RuoIkOibYUsg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## 2.**用例2：gre报文的version字段被置位，导致skb被drop。**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWjnFZYbH3bH9jKFztx0QGJWkJoxoI9DKaqr0fFWL4pNK8KlW496JSWw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWib1hf37UtgK0qM8ZNZyaiatErEH1btk5XfFx76icV67ldpILqyS5A0w2A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 2.1 方法：saddr是10.10.2.2的skb是我们关心的数据。

1、 依然是drop_watch跟踪下kfree_skb，定位函数位置：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLW9qa0b5jeh5E1icFuvDhEcf9b1SMuJ6dD8mrB2cmiclNRIDcKg0LruoGA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

2、 查看gre_rcv的源码，有两个内核模块都存在gre_rcv,由于上面的drop_watch已经定位为ffffffffc099411a，通过crash查看模块的加载地址，来确定调用的是哪一个模块的gre_rcv:

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWm16uF1Yarz7nhwRmmzlEWhwQhB10LzP1T8s8LicEgymcUOzZn2QELew/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

3、 依然是pp()行号来跟踪执行流,和上述不同的是，gre是模块的形式，使用stap的probe module的方式：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWCmjwNRfFRnAC1MUgS78ojSUC558qEnW9sawYyCd2Tah1ib1PD86VRTQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWowBJWBYKJ2ziakFwWM45wibbOZu8ibrztFOc13p8LicxBt5StSmo70KIMg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

综述：这里大家要小心，这个存在”写”skb相关成员，不建议嵌入c代码的方式，去修改skb，如果真的要怎么做，记得恢复回来。

## 3.**用例3: overlay报文根据路由找到出的vxlan隧道接口后，没有从underlay接口出,导致网络不通：**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWCesCQLu7uicDHUicssUdEyya3aW6zqNQNxrs0VWrwuY7TLOCwW3oXOkg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 3.1 方法：saddr是172.16.14.2的skb是我们关心的数据。

1、 依然是drop_watch跟踪下kfree_skb，定位函数位置：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWXP6khzFBdMp48sgDCWyRmeZiaOM0W16poF65gpopDV9LfWErhiaLpezg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

2、 查看vxlan_xmit_one，依然是pp()行号来跟踪执行流：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWWXGpfbge7tibRKW6g0r0YNPm9p4dPSvjuWcevSRmqcia5Jom02TqE2Tw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLWqmP9WxMWNyEEepRLe0vO3WibJZJXKFIZJoVshOz5VfNTxpRSxjGmvNw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat460JtIJeNfiam0HJOcUOLW3vydBdLGDoujNG9MD0f9UjNEAcGuZZmibp596aZic2L1uNCUEKQJbhSQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

综述：从接口中取到underlay信息后，再去查找路由，由于underlay路由不存在，导致skb被drop。这个问题较简单，也可直接通过查看ip route定位。但是细心的你一定会发现一个有趣的问题，关键字overlay arp，欢迎读者来撩。

## 4.总结，丢包精确定位行的方法：

1、 drop_watch先定位函数。

2、 使用pp()定位行。必要的时候，编写一些脚本，直接抄写内核代码或者调用stap库就可以了。

3、 递归重复步骤1和2。

是不是跃跃欲试的感觉。

**最后：**

这里”rpf检查”，”accept_local检查”留给读者来尝试了。实际上systemtap可以做的更多，如内存泄露，系统调用失败，统计流量等等，github上也有很多实用的脚本。

参考链接：

https://cloud.tencent.com/developer/article/1631874

https://www.cnblogs.com/wanpengcoder/p/11768483.html

原文作者：wqiangwang，腾讯 TEG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/xO2dkyZqKnUcaEXn_B39rw

# 【NO.170】五分钟了解互联网Web技术发展史

> 1991年8月，第一个静态页面诞生了，这是由Tim Berners-Lee发布的，想要告诉人们什么是万维网。从静态页面到Ajax技术，从Server Side Render到React Server Components，历史的车轮滚滚向前，一个又一个技术诞生和沉寂。

## 0.**前言**

1994年，万维网联盟（W3C，World Wide Web Consortium）成立，超文本标记语言（HTML，Hyper Text Markup Language）正式确立为网页标准语言，我们的旅途从此开始。

本文将沿着时间线，从“**发现问题-解决问题**”的角度，带领大家了解 Web 技术发展的关键历程，了解典型技术的诞生以及技术更迭的缘由，思考技术发展的原因。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6I7ZdRa6qwIPGoaZAfaIboDjnIMFWbxEKsoK5gYtibmy6pjaA3jpyDOVg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## 1.**Tim Berners-Lee**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IJ0SRczTJFYFE4LR0hJXPmXkmpiamsVGr0Mdk95kick7ugIibPiaxtOYo6Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

Tim Berners-Lee（蒂姆·伯纳斯·李），英国科学家，万维网之父，于1989年在欧洲核子研究组织（CERN）正式提出万维网的设想。该网络最初是**为了满足世界各地大学和研究所的科学家之间对自动信息共享**的需求而设计和开发的，这也是为什么HTML的顶层声明是 `document`，标签名、文档对象模型的名称也是由此而来。

1990年12月，他开发出了世界上第一个网页浏览器。1993年4月30日，欧洲核子研究组织将万维网软件置于公共领域，把万维网推广到全世界，让万维网科技获得迅速的发展，深深改变了人类的生活面貌。

他创造了超文本标记语言（HTML），并创建了历史上第一个网站。当然，现在只剩下了由 CERN 恢复的网站副本：[info.cern.ch](http://info.cern.ch/).

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IAo9QmsQYMLMI0hib00yRQ92gbOCsMA2ojbhkRsNHJq7o8lyzp1P2PGg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## 2.**静态网页时代**

早期的静态网页，只有最基本的单栏布局，HTML所支持的标签也仅有`<h1>`、`<p>`、`<a>`。后来为了丰富网页的内容，`<img>`、`<table>`标签诞生了。

这一阶段，Web服务器基本上只是一个静态资源服务器，每当客户端浏览器发来访问请求，它都来者不拒的建立连接，查找URL指向的静态页面，再返回给客户端。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IespBSpLf6JkjnibkzicM3o2ibqxN7qHDxlFukicrOYXQsiaXaT6G9WSTaUQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

随着网页的飞速发展，人们发现要**人工实现所有信息的编写是非常困难**的，而且非常耗时。

设想一下，假如一个页面有两块区域展示的内容是互相独立的，那么你需要涵盖所有的可能，需要编写的页面数量是两块区域的内容数量的乘积！

此外，静态网站只能够根据用户的请求返回指向的网页，除了进行超链接跳转，没办法实现任何交互。

此时，人们想要

- **网页能够动态显示**
- **直接使用数据库里的数据**
- **网页实现一些用户交互**
- **让页面更美观**

## 3.**JavaScript的诞生**

1994年，网景公司发布了 Navigator 浏览器，但他们急需一种网页脚本语言，以使浏览器可以与网页互动。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6I94iclLsWWgn71g9Az0Hv9R71GqQZO7BjNULcLDPedRDSV9lS8yW116Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

1995年，网景公司的 Brendan Eich 迫于公司的压力，只花了十天就设计了 JS 的最初版本，并命名为 Mocha。后来网景公司为了蹭 Java 的热度，把 JS 最终改名为 JavaScript。但实际情况是，网景公司和Sun公司结成联盟，才更名为 JavaScript。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IDz8bq0bofr6pXAoTiaKQMTEJianql6UNicrjlZbBFnPrn5ZLibMuQqc4Sg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

**从此网页有了一些简单的用户交互**，比如表单验证；也**有了一些JS为基础的动效**，如走马灯。

但是让网页真正开始进入动态网页时代的却是以 PHP 为代表的后端网站技术。

### 3.1 扩展资料：第一次浏览器大战

在网景公司推出JavaScript的时候，微软以JS为基础，编写了JScript和VBScript作为浏览器语言，并在 1995 年的 8 月推出了 IE 1.0。

由于微软在系统里捆绑浏览器，而 90% 的人都在使用 Windows 操作系统，大量用户被动地选择了IE。面对微软快速抢占浏览器份额，网景公司无奈之下只能快速将 JavaScript 向 ECMA 提交标准，制定了 ECMAScript 标准。

> 在这段时间，还发生过一件趣事，IE 4.0 发布当天 Netscape 的员工们发现公司的草坪上出现了一个大大的 IE 图标，这明显是一个挑衅的举动。作为回应，Netscape 把自己的吉祥物 “Mozilla” 放在 IE 的图标上，并挂上胸牌，写着 “Netscape 72，Microsoft 18”——在当时， IE 的市场份额确实不如 Netscape Navigator。
>
> ![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IacHxZcqianZ8g3SOmLk3HGb9T0X1uic1ug3077q6XnWnENpM0dAOaIGg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

但这无法解决份额的问题，网景公司最终在第一次浏览器大战中落败，于1998年，被美国在线（AOL）以42亿美元收购。

在1998年网景公司被收购前，网景公司公开了 Navigator 源代码，想通过广大程序员的参与重新获得市场份额。Navigator 改名为 Mozilla。这也是火狐浏览器的由来，也是第二次浏览器大战的伏笔。

## 4.**CSS**

1994年，Hkon Wium Lie 最初提出了 CSS 的想法。1996年12月，W3C 推出了 CSS 规范的第一版本。

美观是所有人的追求。HTML诞生以来，网页基本上就是一个简陋的富文本容器。由于缺少布局和美化手段，早期网页流行用table标签进行布局。为了解决网页“丑”的问题，Hkon Wium Lie 和 Bert Bos 共同起草了 CSS 提案，同期的 W3C 也对这个很感兴趣。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IWtwEeiaMbReicogrTTYIPyqe8m6Gy4ENjQQ5OJo8WicE6syNWLsmOiaTvw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)早期网页外观

早期的 CSS 存在多种版本，在PSL96版本你甚至可以在里面使用逻辑表达式。但因为它太容易扩展，浏览器厂商那么多，会变得很难统一，最终被放弃。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6I6bCH6s9PIehNmtGcVq1emrdYAvChDjdGEUg5fY9A9KMfOsKxPrM6tg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

在众多提案中，Håkon W Lie 的 CHSS（Cascading HTML Style Sheets）最早提出了**样式表可叠加**的概念。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IVKBqa80XNnjPYcCyXxs2OptjElpQd6oDNU0k173fFwqicicwM26pwL9g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

行尾的百分比表示这条样式的权重，最终将根据权重计算最终值。图中将会计算 `30pt * 40% + 20pt * 60%` 作为h2字体大小的最终值。

为了解决 CSS 兼容性的问题，网景公司甚至还将 CSS 用 JS 来编写。

CSS 从诞生开始就伴随着大量的bug，不同浏览器表现不同坑害了无数的程序员。今天我们能用上相对靠谱的 css，不得不说这是一个奇迹。

## 5.**动态网页技术**

1995年，Rasmus Lerdof 创造的 PHP 开始活跃在各大网站，它让 Web 可以访问数据库了，PHP 实现了人们渴望的动态网页。

这里的动态网页不是指网页动效，而是指内容的动态展示、丰富的用户交互。PHP 就像给网络世界打开了一扇窗，各种动态网页技术（如ASP、JSP）雨后春笋般的冒了出来，万维网也因此开始高速发展，MVC模式也开始出现在后端网站技术中。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IOFdia6jMafxNRrviagOHlO4wbq94LGmibuXiaO9cic3Pc1VYMBvKbfQwlXg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

动态网页技术解决了以前各种令人无法呼吸的痛，生活总会越来越好的：

- **可以用数据库作为基础来展示网页内容**
- **可以实现表单和一些简单交互**
- **再也不用编写一大堆静态页面了**

PHP等动态网页技术的原理，大体上都是根据客户端的请求，从数据库里获取相对应的数据，然后塞到网页里去，返回给客户端一个填充好内容的网页。**这个阶段也是前后端耦合的**。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6Inia4DBsDlKOE4iciaaoYbe4bQ2cwyrbXj7vOrh8PfxJrXYDDICpSvbibdw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)网页开发流程

而当一些基础的需求被满足之后，**动态网页技术带来的不足也渐渐暴露出来**：

- **网页总是刷新**。用户名密码校验需要刷新以展示错误提示；因下拉选择器选择不同而展示的内容需要刷新才能展示；每次数据交互必然会刷新一次页面。
- **网页和后端逻辑混合**。相信老前端们都有过这样的经历：开发完HTML后，会把页面发给后端修改，加上数据注入逻辑；联调或者debug的时候两个人坐在一块看，查问题的效率很低。
- **有大量重复代码无法复用**。举一个典型的例子，论坛。很多时候只有内容有变化，菜单、侧边栏等几乎不会有改变，但每次请求的时候还是得再将整个网页传输一遍。不仅页面会刷新，速度慢，还挺耗流量（这个年代上网也是一种奢侈）。

然后AJAX站了出来。

## 6.**AJAX**

AJAX，Async JavaScript And XML，于1998年开始初步应用，2005年开始普及。AJAX的广泛使用，标志着Web2.0时代的开启。这同时也是各大浏览器争锋的时代。

现在，我们可以通过AJAX来动态获取数据，利用DOM操作动态更新网页内容了。来看看加入了AJAX的网页是怎么工作的：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IzrdN2b8M42ibrhUWuFT99pDmXp2IGUYZYL2OH1BaYbzu12Wics0q9CibQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

这个时候前端路由还没有兴起，大多数情况下还是后端返回一整个页面，部分内容通过AJAX进行获取。

随着智能手机的出现，APP开始萌芽。相比起网页，APP编写好之后只需要数据接口就能工作；而网页不仅需要后端写业务逻辑，控制跳转，还要写一部分接口用于AJAX请求。

这个阶段前端能做的事情还是很少，还背负着“切图仔”的绰号。随着HTML5草案的提出，前端能做的交互越来越多，程序员们急需解决以下问题：

- **后端业务代码和数据接口混合**，还得兼容APP的接口（很多企业既有APP又有网站）
- **前端的代码复杂度急剧增加**

能不能让前端也像APP一样，只需要请求数据接口即可展现内容呢？

### 6.1 扩展资料：第二次浏览器大战

2004年 Firefox 发布，拉开了第二次浏览器大战的序幕。同期市面上诞生的各种新兴浏览器，如Safari、Chrome等，也加入了战争。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IpuVgictTujkGNxzkLPKWGZoU9tcMic3PDubQ9JEzXZPmMhpC3icYoQwTA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

此前由于 XP 系统实在过于火爆，导致 IE 6 无任何竞争对手，微软甚至解散了浏览器的大部分员工，只留下几个人象征性地维护顺便修补一下 bug。这让开发人员非常痛苦。

此时 Firefox 以优越于IE的性能和非常友好的编程工具，迅速将那些被 IE6 搞得焦头烂额的网页开发人员们，从水火之中救出，导致先让前端工程师成为忠实的第一批用户，然后，经由这些有经验的开发人员们推广到了普通的用户群体。

基于webkit内核的Safari，借助自家产品（iOS、MacOS）的垄断快速收割移动端和mac端市场份额；同样基于webkit内核的Chrome，趁着微软放松警惕，凭借优越于市场上所有浏览器的性能，如同中国历史上的成吉思汗一样大杀四方，快速扩展市场份额。

微软知道，自己已经失去了最初能称霸的机会，这次它不想失去，IE再次开始迭代，各大浏览器厂商又开始不顾标准，迭代再次开始，为了统一化标准，W3C开发了HTML5，但是迟迟得不到微软的认可。在其他浏览器纷纷支持HTML5后，微软发现，自己又成了孤家寡人，份额不断缩水。

2016年，Chrome浏览器份额超越IE，第二次浏览器大战结束。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6I80nrFpnjwQaQEsszLP6ASkQicpU6xgHuice950cYRgbksm360ODsovmg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

浏览器大战极大的推动了技术进步，正是 Google 研发出的 V8 引擎极大的提升了 JS 的运行效率，NodeJS 才有机会诞生，前端才能走向全栈。**JS其实没有你想象的那么慢**。

## 7.**SPA**

2008年HTML5草案提出，各大浏览器开启良性竞争，争先实现HTML5功能。由于HTML5带来前端**代码复杂度的增加**，前端为了寻求良好的可维护性和可复用性，也不得不参考后端MVC进行了设计和拆分，后来出现了三大前端框架：Vue（2014）、React（2010）、AngularJS（2009）。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6I1NrNum63v0YGSFhnLMibl9zzxCicIHCw9UTL1via9HQQGpibic4t1nhEpXw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

单页应用返回一个空白的HTML，并通过JS脚本进行动态生成内容，从此和页面刷新说拜拜。

后端不再负责模板渲染，前端和APP开始对等，后端的API也可以通用化了。**前后端终于得以分离**。（PS: 最终目标是成为后端）

但SPA因为返回的是空HTML，所有JS也被打包为一个文件，需要在一开始就加载完所有的资源，

- 请求网页后**白屏时间比传统网页要长**
- 爬虫爬到的是空白页面，**没办法做SEO**
- 在业务复杂的情况下，**请求文件很大，渲染非常慢**。

这使得前端不得不拆分过于庞大的单页应用，出现了框架的多页面概念，也出现了多种解决方案。

很多网页首次加载的时候其实并不需要太多的东西，比如论坛首页与贴子详情页，完全可以将其拆开，用户在新打开的页面阅读反而体验更好（**多页应用**）。

又比如管理后台，可以在页面框架内，将每个菜单对应的管理页拆出来**动态加载**（import）。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6I5S6gMLqDmjenKhnVTMsiarGquCY1PIVNibAtQJxo5X4nfvncC9jWd6cw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## 8.**Server Side Render**

Server Side Render，服务端渲染，简称SSR，又称服务端同构、直出，一般使用NodeJS实现。

这里的服务端渲染和以前的不一样，SSR会利用已经“脱水”的首屏数据来渲染首屏页面返回给客户端，到了浏览器再注入浏览器事件，并且保留单页应用的能力，对SEO非常友好。但学习成本高，限制较多。

让我们看看传统SPA和加入了SSR的SPA在请求上的区别：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6I6ricTtAsfD7LeGBcaaJ3m0gsT61TRvZP8P9bl0MX11UC131ZMneQ19g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)客户端渲染示意

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IRvlUJTlL3WicQzSd3WMRBTIogLic35dvkECRG0H3AX0SdiccxwEHibibficQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)服务端渲染示意

传统SPA可以更快的返回页面，请求响应时间更短；加载JS后才开始渲染，白屏时间更长，loading结束后用户感知到的相对可交互时间更早。

而SSR在接到浏览器请求时，先从后端拉取首屏数据渲染在页面内才返回，请求响应时间更长；因为节约了一段浏览器请求首屏数据的时间，白屏时间更短。由于JS异步加载，用户感知的相对可交互时间变晚。但体验上SSR一般更好。

在极端情况下，用户眼中传统SPA会一直显示loading，使用了SSR的页面则会出现“点不动”的情况。

大多数时候SSR体验会更佳，因为服务端承担了大部分渲染工作，这也导致服务端负载变高。但在业务复杂的情况下，SSR首屏请求的接口数很多，导致返回HTML变慢。

归根结底，SSR不能很好的应付业务复杂的情况，首屏**要加载的东西还是太多了**。所以我们要怎样让用户感知到的白屏时间变短呢？

- **减小加载体积**
- **减少接口请求数**
- **PWA缓存**
- **分块渲染**
- …

IMWEB的企鹅辅导落地了 SSR + PWA 之后，达到了几乎秒开的程度。

## 9.**NodeJS**

说完了 SSR，必须说一下 NodeJS。2010年 NodeJS 正式立项到现在已经11个年头了，NodeJS 的诞生来自于 Ryan Dahl（下图） 的灵感。他想**以非阻塞的方式做所有事情**，用完全异步方式可以处理非常多的请求（高并发）。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IIetS62OMxc3kJjTbMCRViafdHZowibUAdTs9YV1SRPeJzhEzXDhjxNrg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

NodeJS 的出现让前端向全栈的发展迈出了重大的一步。很多公司开始用 NodeJS 搞 BFF（backend for frontend），我们也开始把 Controller 层放到 NodeJS 来处理，后端只负责基础业务数据。也就是现在的三层架构：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IzJRG41uVTawGzjZiaibZKqmN5QYtOeYy2vs5NfMXxqCYYy2h7YYHyfjQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

这种架构在跨端的时候具有良好的适配性，我们可以根据业务需求，为不同端设计不同的 Controller 和 View，而后台可以不做变更。这种架构省去了很多沟通成本，前端专注页面的展示，后端专注业务逻辑。

当然，NodeJS 还可以对后端数据进行预处理，前端根据自己的需要自己设计数据结构，**页面开发与接口调试形成闭环**，还为后端分担了压力。

### 9.1 扩展资料：第三次浏览器大战

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IT1DgEJp5bJicqVQk3mbLRZiaqCH5SicGfMFbU7hppkBXZkKbQ7ntsO1Ww/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)智能手机的飞速发展，这张图表现的淋漓尽致。第三次浏览器大战是争夺移动端市场份额的一战，也是当下正在进行的一战。

> Benedict Evans: “Mobile is eating the world.”（移动设备正在蚕食世界） “Mobile remakes the Internet.”（移动设备正在重构Internet）

而未来，浏览器真正的对手不再是浏览器，而是小程序这样结合了APP和网页优点的新兴技术。

## 10.**未来**

早在2009年，Facebook的工程师就开发了[bigPipe](https://zhuanlan.zhihu.com/p/82398532)，让Facebook页面打开速度提高了两倍。bigPipe使用 **分块渲染** 的思想，将网页的渲染变成了一小块一小块的，服务端渲染好一块页面就发送给客户端。他们直接把木桶拆了，打破了**短板效应**。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/j3gficicyOvavO9wC2Qp77snsNmCibPmX6I7yyvguYXVdN0JY4MpO51QMTRT9WQV2MvqCPI3iaxqSzjjgrKjtJFpvg/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)服务端渲染 VS 流式分块渲染

时隔11年，也就是2020年12月，React 团队提出了 React Server Components，算是一个可扩展的前后端融合方案。其理念和 bigPipe 类似，把组件放在服务端渲染，节省了从浏览器进行数据请求的开支，一些运行时也可以不用放到浏览器，减小了包大小（如 markdown 在服务端渲染好了，也就不再需要把工具库发送给浏览器了）。React Server Components 的引入，也同步做到了自动的 Code Split。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IyZnszQEcgtMLJpF5391vP1ZolXYw7qqQ3Vo3Y1ZtOXKWXVFcSbicn8w/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)

React Server Components 原理

不同的是 React Server Components 返回的不是 HTML，而是带有结构和数据的自定义类JSON数据。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IhmnQUvhnt4pUxN6SAPz5aamtPIfVmHDuOTaZibcs3JGojNVktj5NXpA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

这种结构，是对服务端渲染的核心（结构+数据）进行抽象，结合 React 的工作方式（如Suspense），平缓的从服务端过渡到了客户端，维持了组件状态，并且可以更自由的拼装服务器组件和客户端组件。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavO9wC2Qp77snsNmCibPmX6IDSqaBBVmjF5xQpdIAXO48hg8dxLEMZ7pastLL1ChTXP7YWZHp8MWAw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)客户端组件和服务端组件混用

关于拆分这条思路，让我想到微前端，虽然现在微前端还有很多问题，但微应用即服务也不乏为一条解决之道。未来前端或许会往“小而美”的方向发展，甚至形成一个以服务端组件为单位的包管理器，网页打包大小会越来越小，更多的组件是从网络上直接获取。

此外，我也很期待 Web Components 的发展，有了原生的支持，0kb runtime也不是不可能了。合久必分分久必合，现存很多前端框架也可以得到统一了。当然现在 Web Components 想要投入使用，首先离不开浏览器的支持，而且必须有一个平缓的过渡，此外兼容性也是一个大问题（最后还是苦了程序员们）。

## 11.**结语**

从 JavaScript 的诞生一路走来，从“发现问题-解决问题”的角度，我们看到了技术发展的原因和必然性。2021年的今天，Web APP 仍然距离原生 APP 体验有一定的差距。或许以后会出现一个小程序桌面APP，小程序能够得到统一；或许会在 PWA 的道路上越走越远；又或者浏览器开放更多原生系统 API，利用各种加载方式，再模拟 APP 的各种体验，达到近似 APP 的效果。

每个时代都诞生了许多的技术，大浪淘沙，留下的却也只是只存在于这个时代的王者。技术总是不断的更迭，重要的不是慌慌张张的追赶技术的脚步，而是去思考技术为什么这么如此演变，思考这样的演变方式的利与弊。如果是你，又会怎么解决当代技术的问题呢？

欢迎在评论区各抒己见。

原文作者：charryhuang，腾讯 CSIG 前端开发工程师

原文链接：https://mp.weixin.qq.com/s/HUknNfaxNULc4Yvf5ajRBA

# 【NO.171】快速实现一个分布式定时器

> 定时器（Timer）是一种在业务开发中常用的组件，主要用在执行延时通知任务上。本文以笔者在工作中的实践作为基础，介绍如何使用平时部门最常用的组件快速实现一个业务常用的分布式定时器服务。同时介绍了过程中遇到问题的一些解决方案，希望能够给类似场景提供一些解决思路。

## **1.什么是定时器**

定时器（Timer）是一种在指定时间开始执行某一任务的工具（也有周期性反复执行某一任务的Timer，我们这里暂不讨论）。它常常与延迟队列这一概念关联。那么在什么场景下我才需要使用定时器呢？

我们先看看以下业务场景：

- 当订单一直处于未支付状态时，如何及时的关闭订单，并退还库存？
- 如何定期检查处于退款状态的订单是否已经退款成功？
- 新创建店铺，N天内没有上传商品，系统如何知道该信息，并发送激活短信？

为了解决以上问题，最简单直接的办法就是定时去扫表。每个业务都要维护一个自己的扫表逻辑。当业务越来越多时，我们会发现扫表部分的逻辑会非常类似。我们可以考虑将这部分逻辑从具体的业务逻辑里面抽出来，变成一个公共的部分。这个时候定时器就出场了。

## **2.定时器的本质**

一个定时器本质上是这样的一个数据结构：deadline越近的任务拥有越高优先级，提供以下几种基本操作：

1. Add 新增任务；
2. Delete 删除任务；
3. Run 执行到期的任务/到期通知对应业务处理；
4. Update 更新到期时间 (可选)。

Run通常有两种工作方式：1.轮询 每隔一个时间片就去查找哪些任务已经到期；2.睡眠/唤醒 不停地查找deadline最近的任务，如到期则执行；否则sleep直到其到期。在sleep期间，如果有任务被Add或Delete，则deadline最近的任务有可能改变，线程会被唤醒并重新进行1的逻辑。

它的设计目标通常包含以下几点要求：

1. 支持任务提交（消息发布）、任务删除、任务通知（消息订阅）等基本功能。
2. 消息传输可靠性：消息进入延迟队列以后，保证至少被消费一次（到期通知保证At-least-once ，追求Exactly-once）。
3. 数据可靠性：数据需要持久化，防止丢失。
4. 高可用性：至少得支持多实例部署。挂掉一个实例后，还有后备实例继续提供服务，可横向扩展。
5. 实时性：尽最大努力准时交付信息，允许存在一定的时间误差，误差范围可控。

## **3.数据结构**

下面我们谈谈定时器的数据结构。定时器通常与延迟队列密不可分，延时队列是什么？顾名思义它是一种带有延迟功能的消息队列。而延迟队列底层通常可以采用以下几种数据结构之一来实现：

1. 有序链表，这个最直观，最好理解。
2. 堆，应用实例如Java JDK中的DelayQueue、Go内置的定时器等。
3. 时间轮/多级时间轮，应用实例如Linux内核定时器、Netty工具类HashedWheelTimer、Kafka内部定时器等。

这里重点介绍一下时间轮（TimeWheel）。一个时间轮是一个环形结构，可以想象成时钟，分为很多格子，一个格子代表一段时间（越短Timer精度越高），并用一个List保存在该格子上到期的所有任务，同时一个指针随着时间流逝一格一格转动，并执行对应List中所有到期的任务。任务通过取模决定应该放入哪个格子。示意图如下所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavmiblg50ElKlS9pw0ps49BdApF2iaVrvrRic7iaDcDScqPoMQT3dUVfU3Y6w6AkSaD4vWtC8If9FEGug/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)时间轮

如果任务的时间跨度很大，数量也多，传统的单轮时间轮会造成任务的round很大，单个格子的任务List很长，并会维持很长一段时间。这时可将Wheel按时间粒度分级（与水表的思想很像），示意图如下所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavmiblg50ElKlS9pw0ps49BdDPcr0QaNsyiaWKjO04rlEfqnlLHs7Caf16TFWRNxP0xLGqswb5qBXlQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)多级时间轮.png

时间轮是一种比较优雅的实现方式，且如果采用多级时间轮时其效率也是比较高的。

## **4.业界实现方案**

业界对于定时器/延时队列的工程实践，则通常基于以下几种方案来实现:

1. 基于Redis ZSet实现。
2. 采用某些自带延时选项的队列实现，如RabbitMQ、Beanstalkd、腾讯TDMQ等。
3. 基于Timing-Wheel时间轮算法实现。

## **5.方案详述**

介绍完定时器的背景知识，接下来看下我们系统的实现。我们先看一下需求背景。在我们组的实际业务中，有延迟任务的需求。一种典型的应用场景是：商户发起扣费请求后，立刻为用户下发扣费前通知，24小时后完成扣费；或者发券给用户，3天后通知用户券过期。基于这种需求背景，我们引出了定时器的开发需求。

我们首先调研了公司内外的定时器实现，避免重复造轮子。调研了诸如例如公司外部的Quartz、有赞的延时队列等，以及公司内部的PCG tikker、TDMQ等，以及微信支付内部包括营销、代扣、支付分等团队的一些实现方案。最后从可用性、可靠性、易用性、时效性以及代码风格、运维代价等角度考虑，我们决定参考前人的一些优秀的技术方案，并根据我们团队的技术积累和组件情况，设计和实现一套定时器方案。

首先要确定定时器的存储数据结构。这里借鉴了时间轮的思想，基于微信团队最常用的存储组件tablekv进行任务的持久化存储。使用到tablekv的原因是它天然支持按uin分表，分表数可以做到千万级别以上；其次其单表支持的记录数非常高，读写效率也很高，还可以如mysql一样按指定的条件筛选任务。

我们的目标是实现秒级时间戳精度，任务到期只需要单次通知业务方。故我们方案主要的思路是基于tablekv**按任务执行时间分表**，也就是使用使用方指定的start_time（时间戳）作为分表的uin，也即是时间轮bucket。为什么不使用多轮时间轮？主要是因为首先kv支持单表上亿数据， 其二kv分表数可以非常多，例如我们使用1000万个分表需要约115天的间隔才会被哈希分配到同一分表内。故暂时不需要使用到多轮时间轮。

最终我们采用的分表数为1000w，uin=时间戳mod分表数。这里有一个注意点，通过mod分表数进行Key收敛, 是为了避免时间戳递增导致的key无限扩张的问题。示例图如下所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavmiblg50ElKlS9pw0ps49BdR2xBVG7wRQhkLSKicXxywwTEPUarHKabLQGtOUl9erYx8nkyEv7ZiahQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)kv时间轮.png

任务持久化存储之后，我们采用一个Daemon程序执行定期扫表任务，将到期的任务取出，最后将请求中带的业务信息（biz_data添加任务时带来，定时器透传，不关注其具体内容）回调通知业务方。这么一看流程还是很简单的。

这里扫描的流程类似上面讲的时间轮算法，会有一个指针（我们在这里不妨称之为time_pointer）不断向后移动，保证不会漏掉任何一个bucket的任务。这里我们采用的是commkv（可以简单理解为可以按照key-value形式读写的kv，其底层仍是基于tablekv实现）存储CurrentTime，也就是当前处理到的时间戳。每次轮询时Daemon都会通过GetByKey接口获取到CurrentTime，若大于当前机器时间，则sleep一段时间。若小于等于当前机器时间，则取出tablekv中以CurrentTime为uin的分表的TaskList进行处理。本次轮询结束，则CurrentTime加一，再通过SetByKey设置回commkv。这个部分的工作模式我们可以简称为Scheduler。

Scheduler拿到任务后只需要回调通知业务方即可。如果采用同步通知业务方的方式，由于业务方的超时情况是不可控的，则一个任务的投递时间可能会较长，导致拖慢这个时间点的任务整体通知进度。故而这里自然而然想到采用**异步解耦**的方式。即将任务发布至事件中心（微信内部的高可用、高可靠的消息平台，支持事务和非事务消息。

由于一个任务的投递到事件中心的时间仅为几十ms，理论上任务量级不大时1s内都可以处理完。此时time_pointer会紧跟当前时间戳。当大量任务需要处理时，需要采用多线程/多协程的方式并发处理，保证任务的准时交付。broker订阅事件中心的消息，接受到消息后由broker回调通知业务方，故broker也充当了Notifier的角色。整体架构图如下所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavmiblg50ElKlS9pw0ps49BdFic2J4dj2T4ibFxicId4w2loGYm5PLchLkR9A66KsvCQZquP9Hdsyey0w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)架构图小.png

主要模块包括：

**任务扫描Daemon**：充当Scheduler的角色。扫描所有到期任务，投递到事件中心，让它通知broker，由broker的Notifier通知业务方。

**定时器broker**：集业务接入、Notifier两者功能于一身。

任务状态机图如下所示，只有两种状态。当任务插入kv成功时即为pending状态，当任务成功被取出并通知业务方成功时即为finish状态。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvavmiblg50ElKlS9pw0ps49BdicEamObByTSic5U9aXQickwJ4nZQSicEUMUM8JL8Lf6VUyTV0oNedUyibNA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)状态图.png

## **6.实现细节与难点思考**

下面就上面的方案涉及的几个技术细节进行进一步的解释。

### 6.1 业务隔离

通过biz_type定义不同的业务类型，不同的biz_type可以定义不同的优先级（目前暂未支持），任务中保存biz_type信息。业务信息（主键为biz_type）采用境外配置中心进行配置管理。方便新业务的接入和配置变更。业务接入时，需要在配置中添加诸如回调通知信息、回调重试次数限制、回调限频等参数。业务隔离的目的在于使各个接入业务不受其他业务的影响，这一点由于目前我们的定时器用于支持本团队内部业务的特点，仅采取对不同的业务执行不同业务限频规则的策略，并未做太多优化工作，就不详述了。

### 6.2 时间轮空转问题

由于1000w分表，肯定是大部分Bucket为空，时间轮的指针推进存在低效问题。联想到在饭店排号时，常有店员来登记现场尚存的号码，就是因为可以跳过一些号码，加快叫号进度。同理，为了减少这种“空推进”，Kafka引入了DelayQueue，以bucket为单位入队，每当有bucket到期，即queue.poll能拿到结果时，才进行时间的“推进”，减少了线程空转的开销。在这里类似的，我们也可以做一个优化，维护一个有序队列，保存表不为空的时间戳。大家可以思考一下如何实现，具体方案不再详述。

### 6.3 限频

由于定时器需要写kv，还需要回调通知业务方。因此需要考虑对调用下游服务做限频，保证下游服务不会雪崩。这是一个**分布式限频**的问题。这里使用到的是微信支付的限频组件。保证1.任务插入时不超过定时器管理员配置的频率。2.Notifier回调通知业务方时不超过业务方申请接入时配置的频率。这里保证了1.kv和事件中心不会压力太大。2.下游业务方不会受到超过其处理能力的请求量的冲击。

### 6.4 分布式单实例容灾

出于容灾的目的，我们希望Daemon具有容灾能力。换言之若有Daemon实例异常挂起或退出，其他机器的实例进程可以继续执行任务。但同时我们又希望同一时刻只需要一个实例运行，即“分布式单实例”。所以我们完整的需求可以归纳为**“分布式单实例容灾部署”**。

实现这一目标，方式有很多种，例如：

- 接入“调度中心”，由调度中心来负责调度各个机器
- 各节点在执行任务前先分布式抢锁，只有成功占用锁资源的节点才能执行任务
- 各节点通过通信选出“master"来执行逻辑，并通过心跳包持续通信，若“master”掉线，则备机取代成为master继续执行

主要从开发成本，运维支撑两方面来考虑，选取了基于chubby分布式锁的方案来实现单实例容灾部署。这也使得我们真正执行业务逻辑的机器具有随机性。

### 6.5  可靠交付

这是一个核心问题，如何保证任务的通知满足At-least-once的要求？

我们系统主要通过以下两种方式来保证。

1.任务达到时即存入tablekv持久化存储，任务成功通知业务方才设置过期（保留一段时间后删除），故而所有任务都是落地数据，保证事后可以对账。

2.引入可靠事件中心。在这里使用的是事件中心的普通消息，而非事务消息。实质是当做一个高可用性的消息队列。

这里引入消息队列的意义在于：

- 将任务调度和任务执行解耦（调度服务并不需要关心任务执行结果）。
- 异步化，保证调度服务的高效执行，调度服务的执行是以ms为单位。
- 借助消息队列实现任务的可靠消费。

事件中心相比普通的消息队列还具有哪些优点呢？

- 某些消息队列可能丢消息（由其实现机制决定），而事件中心本身底层的分布式架构，使得事件中心保证极高的可用性和可靠性，基本可以忽略丢消息的情况。
- 事件中心支持按照配置的不同事件梯度进行多次重试（回调时间可以配置）。
- 事件中心可以根据自定义业务ID进行消息去重。

事件中心的引入，基本保证了任务从Scheduler到Notifier的可靠性。

当然，最为完备的方式，是增加另一个异步Daemon作为兜底策略，扫出所有超时还未交付的任务进行投递。这里思路较为简单，不再详述。

### 6.6 及时交付

若同一时间点有大量任务需要处理，如果采用串行发布至事件中心，则仍可能导致任务的回调通知不及时。这里自然而然想到采用多线程/多协程的方式并发处理。在本系统中，我们使用到了微信的BatchTask库，BatchTask是这样一个库，它把每一个需要并发执行的RPC任务封装成一个函数闭包（返回值+执行函数+参数），然后调度协程（BatchTask的底层协程为libco）去执行这些任务。对于已有的同步函数，可以很方便的通过BatchTask的Api去实现任务的批量执行。Daemon将发布事件的任务提交到BatchTask创建的线程池+协程池（线程和协程数可以根据参数调整）中，充分利用流水线和并发，可以将任务List处理的整体时延大大缩短，尽最大努力及时通知业务方。

### 6.7 任务过期删除

从节省存储资源考虑，任务通知业务成功后应当删除。但删除应该是一个异步的过程，因为还需要保留一段时间方便查询日志等。这种情况，通常的实现方式是启动一个Daemon异步删除已完成的任务。我们系统中，是利用了tablekv的自动删除机制，回调通知业务完成后，除了设置任务状态为完成外，同时通过tablekv的update接口设置kv的过期时间为1个月，避免了异步Daemon扫表删除任务，简化了实现。

### 6.8 其他风险项

1.由于time_pointer的CurrentTime初始值置为首次运行的Daemon实例的机器时间，而每次轮询时都会对比当前Daemon实例的机器时间与CurrentTime的差别，故机器时间出错可能会影响任务的正常调度。这里考虑到现网机器均有时间校正脚本在跑，这个问题基本可以忽略。

2.本系统的架构对事件中心构成了强依赖。定时器的可用性和可靠性依赖于事件中心的可用性和可靠性。虽然目前事件中心的可用性和可靠性都非常高，但如果要考虑所有异常情况，则事件中心的短暂不可用、或者对于订阅者消息出队的延迟和堆积，都是需要正视的问题。一个解决方案是使用MQ做双链路的消息投递，解决对于事件中心单点依赖的问题。

## 7.**结语**

这里的定时器服务目前仅用于支持境外的定时器需求，调用量级尚不大，已可满足业务基本要求。如果要支撑更高的任务量级，还需要做更多的思考和优化。随时欢迎大家和和我交流探讨。

## 8.**加入我们**

境外支付团队在不断追求卓越的路上寻找同路人，欢迎加入我们的团队。

原文作者：刘若愚

原文链接：https://mp.weixin.qq.com/s/ggPftQm2ewGOJwlRDQGgDQ

# 【NO.172】MySQL 深入学习总结

## **1.数据库基础**

### 1.1 MySQL 架构

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNj8xvPB51Q6fuLFiatUpBPyMMAccahJwLgia74sxRXvCtQ8icm3pXyPOS2w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

和其它数据库相比，MySQL 有点与众不同，它的架构可以在多种不同场景中应用并发挥良好作用。主要体现在存储引擎的架构上，插件式的存储引擎架构将查询处理和其它的系统任务以及数据的存储提取相分离。这种架构可以根据业务的需求和实际需要选择合适的存储引擎，各层介绍：

#### **1.1.1 连接层**

最上层是一些客户端和连接服务，包含本地 sock 通信和大多数基于客户端/服务端工具实现的类似于 tcp/ip 的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于 SSL 的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。

#### **1.1.2 服务层**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjRnaus2X6PTY5qjzfoSnbOuiadJGpzbp8VBf1ic8xasqy0FgAx6NibDtRg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **1.1.3 引擎层**

存储引擎层，存储引擎真正的负责了 MySQL 中数据的存储和提取，服务器通过 API 与存储引擎进行通信。不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取。

#### **1.1.4 存储层**

数据存储层，主要是将数据存储在运行于裸设备的文件系统之上，并完成与存储引擎的交互。

### 1.2 数据引擎

不同的存储引擎都有各自的特点，以适应不同的需求，如表所示。为了做出选择，首先要考虑每一个存储引擎提供了哪些不同的功能。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjqZpsuouicEx4PVBcwWvWUicNDbFjncRMryh9S4IUJ3zn4klqoiaoc56sQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **1.2.1 MyISAM**

使用这个存储引擎，每个 MyISAM 在磁盘上存储成三个文件。

1. frm 文件：存储表的定义数据
2. MYD 文件：存放表具体记录的数据
3. MYI 文件：存储索引

#### **1.2.2 InnoDB**

InnoDB 是默认的数据库存储引擎，他的主要特点有：

1. 可以通过自动增长列，方法是 auto_increment；
2. 支持事务。默认的事务隔离级别为可重复度，通过 MVCC（并发版本控制）来实现的；
3. 使用的锁粒度为行级锁，可以支持更高的并发；
4. 支持外键约束；外键约束其实降低了表的查询速度，但是增加了表之间的耦合度；
5. 配合一些热备工具可以支持在线热备份；
6. 在 InnoDB 中存在着缓冲管理，通过缓冲池，将索引和数据全部缓存起来，加快查询的速度；
7. 对于 InnoDB 类型的表，其数据的物理组织形式是聚簇表。所有的数据按照主键来组织。数据和索引放在一块，都位于 B+数的叶子节点上。

#### **1.2.3 Memory**

将数据存在内存，为了提高数据的访问速度，每一个表实际上和一个磁盘文件关联。文件是 frm。

1. 支持的数据类型有限制，比如：不支持 TEXT 和 BLOB 类型，对于字符串类型的数据，只支持固定长度的行；VARCHAR 会被自动存储为 CHAR 类型；
2. 支持的锁粒度为表级锁。所以，在访问量比较大时，表级锁会成为 MEMORY 存储引擎的瓶颈；
3. 由于数据是存放在内存中，一旦服务器出现故障，数据都会丢失；
4. 查询的时候，如果有用到临时表，而且临时表中有 BLOB，TEXT 类型的字段，那么这个临时表就会转化为 MyISAM 类型的表，性能会急剧降低；
5. 默认使用 hash 索引；
6. 如果一个内部表很大，会转化为磁盘表。

### 1.3 表与字段设计

#### **1.3.1 数据库基本设计规范**

1. 尽量控制单表数据量的大小，建议控制在 500 万以。500 万并不是 MySQL 数据库的限制，过大会造成修改表结构、备份、恢复都会有很大的问题，可以用历史数据归档（应用于日志数据），分库分表（应用于业务数据）等手段来控制数据量大小；
2. 谨慎使用 MySQL 分区表。分区表在物理上表现为多个文件，在逻辑上表现为一个表 谨慎选择分区键，跨分区查询效率可能更低 建议采用物理分表的方式管理大数据；
3. 禁止在数据库中存储图片，文件等大的二进制数据。通常文件很大，会短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机 IO 操作，文件很大时，IO 操作很耗时 通常存储于文件服务器，数据库只存储文件地址信息；
4. 禁止在线上做数据库压力测试。

#### **1.3.2 数据库字段设计规范**

1. 优先选择符合存储需要的最小的数据类型。列的字段越大，建立索引时所需要的空间也就越大，这样一页中所能存储的索引节点的数量也就越少也越少，在遍历时所需要的 IO 次数也就越多， 索引的性能也就越差；
2. 避免使用 TEXT、BLOB 数据类型，最常见的 TEXT 类型可以存储 64k 的数据；
3. 尽可能把所有列定义为 NOT NULL。

#### **1.3.3 索引设计规范**

1. 限制每张表上的索引数量，建议单张表索引不超过 5 个；
2. 禁止给表中的每一列都建立单独的索引；
3. 每个 InnoDB 表必须有个主键；
4. 建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。区分度最高的放在联合索引的最左侧（区分度 = 列中不同值的数量 / 列的总行数）。尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好）。使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引）。

#### **1.3.4 数据库 SQL 开发规范**

1. 充分利用表上已经存在的索引,避免使用双 % 号的查询条件。如 a like '%123%'，（如果无前置 %，只有后置 %，是可以用到列上的索引的）一个 SQL 只能利用到复合索引中的一列进行范围查询，如：有 a,b,c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b,c 列上的索引将不会被用到，在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧；使用 left join 或 not exists 来优化 not in 操作， 因为 not in 也通常会使用索引失效；
2. 禁止使用 SELECT * 必须使用 SELECT <字段列表> 查询；
3. 避免使用子查询，可以把子查询优化为 JOIN 操作；
4. 避免使用 JOIN 关联太多的表。

#### 

### 1.4 范式与反范式

#### **1.4.1 第一范式**

该范式是为了排除 重复组 的出现，因此要求数据库的每个列的值域都由原子值组成；每个字段的值都只能是单一值。1971 年埃德加·科德提出了第一范式。即表中所有字段都是不可再分的。解决方案：想要消除重复组的话，只要把每笔记录都转化为单一记录即可。

#### **1.4.2 第二范式**

表中必须存在业务主键，并且非主键依赖于全部业务主键。解决方案：拆分将依赖的字段单独成表。

#### **1.4.3 第三范式**

表中的非主键列之间不能相互依赖，将不与 PK 形成依赖关系的字段直接提出单独成表即可。

### 1.5 sql 索引

1. B 树只适合随机检索，适合文件操作，B+树同时支持随机检索和顺序检索；
2. B+树的磁盘读写代价更低， B+树的内部结点并没有指向关键字具体信息的指针；
3. B+树的查询效率更加稳定。B 树搜索有可能会在非叶子结点结束；
4. 只要遍历叶子节点就可以实现整棵树的遍历，数据库中基于范围的查询是非常频繁，B 树这样的操作效率非常低。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjfne8jicVmEJiaMMuS1j9FNQ1eib60O9UUubcwupIibRIs5EQwic3ykerNkg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 1.6 join 连表

#### **1.6.1 JOIN 按照功能大致分为如下三类：**

1. INNER JOIN（内连接,或等值连接）：获取两个表中字段匹配关系的记录。![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvaufKISYMVA6xk1p9XpneMNjHzm5WKGPnqvnyBuUKqh4HyabWMSHHn9DouFK6ia84jr7mpyT2WdpU3g/640?wx_fmt=gif&wxfrom=5&wx_lazy=1&wx_co=1)
2. LEFT JOIN（左连接）：获取左表所有记录，即使右表没有对应匹配的记录。![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvaufKISYMVA6xk1p9XpneMNjFA9iaDKpvdyuOS4W1NNNL0Z4PticHdB4bJiaFicH8RAAPibXmvVVk35E0icQ/640?wx_fmt=gif&wxfrom=5&wx_lazy=1&wx_co=1)
3. RIGHT JOIN（右连接）：与 LEFT JOIN 相反，用于获取右表所有记录，即使左表没有对应匹配的记录。![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvaufKISYMVA6xk1p9XpneMNjLG9BszRWO07ZaCysDfUsosia2MPTYYAGp8Kiah4GplRerPMIYzeicRfWw/640?wx_fmt=gif&wxfrom=5&wx_lazy=1&wx_co=1)

#### **1.6.2 join 的原理**

MySQL 使用了嵌套循环（Nested-Loop Join）的实现方式。Nested-Loop Join 需要区分驱动表和被驱动表，先访问驱动表，筛选出结果集，然后将这个结果集作为循环的基础，访问被驱动表过滤出需要的数据。Nested-Loop Join 分下面几种类型：

1. SNLJ，简单嵌套循环。这是最简单的方案，性能也一般。实际上就是通过驱动表的结果集作为循环基础数据，然后一条一条的通过该结果集中的数据作为过滤条件到下一个表中查询数据，然后合并结果。如果还有第三个参与 Join，则再通过前两个表的 Join 结果集作为循环基础数据，再一次通过循环查询条件到第三个表中查询数据，如此往复。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjQfBpDaMUiadkYynibYCats5l27FC4nn8PQBGM52td3MCqb1xnokvJsoQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



相关图片来源于网络

这个算法相对来说就是很简单了，从驱动表中取出 R1 匹配 S 表所有列，然后 R2，R3,直到将 R 表中的所有数据匹配完，然后合并数据，可以看到这种算法要对 S 表进行 RN 次访问，虽然简单，但是相对来说开销还是太大了

1. INLJ，索引嵌套循环。索引嵌套联系由于非驱动表上有索引，所以比较的时候不再需要一条条记录进行比较，而可以通过索引来减少比较，从而加速查询。这也就是平时我们在做关联查询的时候必须要求关联字段有索引的一个主要原因。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjIoXSJee6o9WBibHQrkvsyHtOkSM9VXf9xZmjxiawJgEnPEr3aicFRHWTQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



相关图片来源于网络

1. BNLJ，块嵌套循环。如果 join 字段没索引，被驱动表需要进行扫描。这里 MySQL 并不会简单粗暴的应用前面算法，而是加入了 buffer 缓冲区，降低了内循环的个数，也就是被驱动表的扫描次数。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjutXPdYZukU6K4R14OHCLU5cTPp15DcaxJ86DINdddhvIBceAzgn6Vw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

这个 buffer 被称为 join buffer，顾名思义，就是用来缓存 join 需要的字段。MySQL 默认 buffer 大小 256K，如果有 n 个 join 操作，会生成 n-1 个 join buffer。

#### **1.6.3 join 的优化**

1. 小结果集驱动大结果集。用数据量小的表去驱动数据量大的表，这样可以减少内循环个数，也就是被驱动表的扫描次数。
2. 用来进行 join 的字段要加索引，会触发 INLJ 算法，如果是主键的聚簇索引，性能最优。例子：第一个子查询是 72075 条数据，join 的第二条子查询是 50w 数据，主要的优化还是驱动表是小表，后面的是大表，on 的条件加上了唯一索引。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjGzCfc2mhkhPwCxY5kDOZbHbCFpDM8ETTCicIRjlwpWVNIEsZSUn5ia6A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

1. 如果无法使用索引，那么注意调整 join buffer 大小，适当调大些
2. 减少不必要的字段查询（字段越少，join buffer 所缓存的数据就越多）

## **2.数据进阶**

### 2.1 sql 执行过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjZrBQdiadbZNDWdc0mibfGJbK2Gard1VtHmEJ65U4hPCcdugeL3FCleZw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

如上图所示，当向 MySQL 发送一个请求的时候，MySQL 到底做了什么：

1. 客户端发送一条查询给服务器。服务器先检查查询缓存，如果命中了缓存，则立刻返回存储在缓存中的结果。否则进入下一阶段；
2. 在解析一个查询语句之前，如果查询缓存是打开的，那么 MYSQL 会优先检查这个查询是否命中查询缓存中的数据；
3. 这个检查是通过一个对大小写敏感的哈希查找的。查询和缓存中的查询即使只有一个不同，也不会匹配缓存结果；
4. 如果命中缓存，那么在但会结果前 MySQL 会检查一次用户权限，有权限则跳过其他步骤直接返回数据；
5. 服务器端进行 SQL 解析、预处理，再由优化器生成对应的执行计划。MySQL 解析器将使用 MySQL 语法规则验证和解析查询。例如验证是否使用错误的关键字、关键字顺序、引号前后是否匹配等；预处理器则根据一些 MySQL 规则进一步解析树是否合法，例如检查数据表和数据列是否存在，解析名字和别名是否有歧义等；
6. MySQL 根据优化器生成的执行计划，再调用存储引擎的 API 来执行查询。

MySQL 的查询优化器使用很多策略来生成一个最优的执行计划。优化策略可以简单的分为两种：

1. 静态优化：静态优化可以直接对解析树进行分析，并完成优化。例如优化器可以通过简单的代数变化将 WHERE 条件转换成另外一种等价形式，静态优化在第一次完成后就一直有效，即使使用不同的参数重复执行查询也不会变化。可以认为是一种”编译时优化“。

1. 动态优化：和查询的上下文有关，也可能和其他因素有关，例如 WHERE 中取值、索引中条目对应的数据行数等。这需要在每次查询的时候重新评估，可以让那位 u 是"运行时优化"。

使用 show status like ‘Last_query_cost’ 可以查询上次执行的语句的成本，单位为数据页。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjeNuApvD7wlLiafM66u5kibxvnwZhW4H9GjkGBhcX95CbajvdClY7mVJw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjRYrfOr34WQuVQEVufdusrffU2YPrqXXTm9l1oCibJpMH85R5sEXkP0A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 2.2 sql 查询计划

使用 explain 进行执行计划分析：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjGMsDq0rNFqdJLW8icBLhUsBib4IbibDTfVkOmh5DRL0Guo87SB15PCvOA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjzahuIvAwpafVlhwr2ytAdR7bm1ibRw5VuOcwZEwdUAxeic2uKLbSElvQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNj5S7iayr7UySToEibxPwDticX2y1OGmY9j3fiajkd2VibicwPTKJaahBNX1Ww/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 2.3 sql 索引优化

遵循索引原则适合大部分的常规数据库查询场景，但不是所有的索引都能符合预期，从索引原理本身来分析对索引的创建会更有帮助。

1. 小表的全表扫描往往会比索引更快 ；
2. 中大型表使用索引会有很大的查询效率提升；超大型表，索引也无法解决慢查询，过多和过大的索引会带来更多的磁盘占用和降低 INSERT 效率。

#### **2.3.1 前缀索引**

当要索引的列字符很多时 索引则会很大且变慢( 可以只索引列开始的部分字符串 节约索引空间 从而提高索引效率 )

例如：一个数据表的 x_name 值都是类似 23213223.434323.4543.4543.34324 这种值，如果以整个字段值做索引，会使索引文件过大，但是如果设置前 7 位来做索引则不会出现重复索引值的情况了。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjdlz0kUvNGK6d1VhWE6d2FqweqGibH0aGFYb6HXX52vdqicHDosEcQF5g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

查询效率会大大提升：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjlsEOxicwvJJic5swOk1MlE0g3BY6eYKlurw50ibiay1VLyaUMOAT2icITVg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **2.3.2 联合索引顺序**

```
alter table table1 add key （distribute_type，plat_id）
```

使用选择基数更高（不重复的数据）的字段作为最左索引：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjDbXCtiamka1bwSgzKuzpiaiaODh5Bd6tKcbfoV8LrGoGGMVqqLGPe5ozQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **2.3.3 联合索引左前缀匹配**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjPkAlD5sg1tqibj4jSmzTO6D62sAj0o7MjVT7gVAnFXRicKdzAuGqHW2g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

1. a=? and b=? and c=?；查询效率最高，索引全覆盖
2. a=? and b=?；索引覆盖 a 和 b
3. a=? or b=?；索引覆盖 a 和 b
4. b=? or a=?；无法覆盖索引(>、<、between、like)
5. b=? and a=?；经过 mysql 的查询分析器的优化，索引覆盖 a 和 b
6. a=?；索引覆盖 a
7. b=? and c=?；没有 a 列，不走索引，索引失效
8. c=?；没有 a 列，不走索引，索引失效

### 2.4 慢查询分析

#### **2.4.1 先对 sql 语句进行 explain，查看语句存在的问题**

#### **2.4.2 使用 show profile 查看执行耗时，分析具体耗时原因**

show profile 的使用指引：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjusUmiblTqicyibxHPiaia7XHosUoyV3QwYgficeWbibVhjLfGGISuDErmVialg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 2.5 改表与 sql 日志

#### **2.5.1 改表**

改表会直接触发表锁，改表过程非常耗时，对于大表修改，无论是字段类型调整还是字段增删，都需要谨慎操作，防止业务表操作被阻塞，大表修改往往有以下几种方式。

1. 主备改表切换，先改冷库表，再执行冷热切换；
2. 直接操作表数据文件，拷贝文件替换；
3. 使用类似 percona-toolkit 工具操作表。

常用方法：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjDGeFQibKHXlOo3vJz5O2gSMLG1BzLp1KxRiad0IB3LU82fVicFXsu6icGA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **2.5.2 sql 日志**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjbrD7UomRQd6b4ZsNjtgR4RLsfcGH7Uc4pLGQV1Jt0QKTwOCkWsbmuw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjgKS26768iaRZ86tyZdI5dypric0vITkj1bLJAqiaX1Okt68tPFMQSvxnA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 2.6 分库与分表

#### **2.6.1 数据库瓶颈**

不管是 IO 瓶颈，还是 CPU 瓶颈，最终都会导致数据库的活跃连接数增加，进而逼近甚至达到数据库可承载活跃连接数的阈值。在业务 Service 来看就是，可用数据库连接少甚至无连接可用。接下来就可以想象了吧（并发量、吞吐量、崩溃）。

1. IO 瓶颈 第一种：磁盘读 IO 瓶颈，热点数据太多，数据库缓存放不下，每次查询时会产生大量的 IO，降低查询速度 -> 分库和垂直分表。

第二种：网络 IO 瓶颈，请求的数据太多，网络带宽不够 -> 分库。

1. CPU 瓶颈 第一种：SQL 问题，如 SQL 中包含 join，group by，order by，非索引字段条件查询等，增加 CPU 运算的操作 -> SQL 优化，建立合适的索引，在业务 Service 层进行业务计算。

第二种：单表数据量太大，查询时扫描的行太多，SQL 效率低，CPU 率先出现瓶颈 -> 水平分表。

#### **2.6.2 分库分表**

1. 水平分库![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNj8Ia5K9ibTicpLibzzT6Nyco07Tgyr5V4O1qc875OXbnbS0vFjgcurMj3Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

相关图片来源于网络

概念：以字段为依据，按照一定策略（hash、range 等），将一个库中的数据拆分到多个库中。结果：每个库的结构都一样；每个库的数据都不一样，没有交集；所有库的并集是全量数据；场景：系统绝对并发量上来了，分表难以根本上解决问题，并且还没有明显的业务归属来垂直分库。分析：库多了，io 和 cpu 的压力自然可以成倍缓解。

1. 水平分表![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjTNs8uqR60d9OGjYB4WKlIiaIQ0diaskXNoRx56sRR8fhIicqLUkRyTYiaw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)相关图片来源于网络

概念：以字段为依据，按照一定策略（hash、range 等），将一个表中的数据拆分到多个表中。结果：每个表的结构都一样；每个表的数据都不一样，没有交集；所有表的并集是全量数据。

场景：系统绝对并发量并没有上来，只是单表的数据量太多，影响了 SQL 效率，加重了 CPU 负担，以至于成为瓶颈。

分析：表的数据量少了，单次 SQL 执行效率高，自然减轻了 CPU 的负担。

1. 垂直分库![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjicT4ZWCEXMnv6yNA1NTqibAHVsG10k1DiaquQrO5KDVYJ21ibtfjfibSSCA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

   相关图片来源于网络

概念：以表为依据，按照业务归属不同，将不同的表拆分到不同的库中。

结果：每个库的结构都不一样；每个库的数据也不一样，没有交集；所有库的并集是全量数据。

场景：系统绝对并发量上来了，并且可以抽象出单独的业务模块。

分析：到这一步，基本上就可以服务化了。例如，随着业务的发展一些公用的配置表、字典表等越来越多，这时可以将这些表拆到单独的库中，甚至可以服务化。再有，随着业务的发展孵化出了一套业务模式，这时可以将相关的表拆到单独的库中，甚至可以服务化。

1. 垂直分表![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjV1lXZJWVANqECRdbPFtV6HqVYx4lJDgY9dtWhiaLBpFEs1OhEQ0Z7RQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

相关图片来源于网络

概念：以字段为依据，按照字段的活跃性，将表中字段拆到不同的表（主表和扩展表）中。

结果：每个表的结构都不一样；每个表的数据也不一样，一般来说，每个表的字段至少有一列交集，一般是主键，用于关联数据；所有表的并集是全量数据。

#### **2.6.3 分库分表工具**

目前市面上的分库分表中间件相对较多，其中基于代理方式的有 MySQL Proxy 和 Amoeba， 基于 Hibernate 框架的是 Hibernate Shards，基于 jdbc 的有当当 sharding-jdbc， 基于 mybatis 的类似 maven 插件式的有蘑菇街的蘑菇街 TSharding， 通过重写 spring 的 ibatis template 类的 Cobar Client。

还有一些大公司的开源产品：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjOdRiaA53bchSbSLvEraC0M1Bu3uoOdc0BAiaBTuXibSsgvrj4IKQsv2IA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## **3.分布式数据库**

### 3.1 什么是分布式数据库

分布式系统数据库系统原理(第三版)中的描述：“我们把分布式数据库定义为一群分布在计算机网络上、逻辑上相互关联的数据库。分布式数据库管理系统(分布式 DBMS)则是支持管理分布式数据库的软件系统，它使得分布对于用户变得透明。有时，分布式数据库系统(Distributed Database System,DDBS)用于表示分布式数据库和分布式 DBMS 这两者。

在以上表述中，“一群分布在网络上、逻辑上相互关联”是其要义。在物理上一群逻辑上相互关联的数据库可以分布式在一个或多个物理节点上。当然，主要还是应用在多个物理节点。这一方面是 X86 服务器性价比的提升有关，另一方面是因为互联网的发展带来了高并发和海量数据处理的需求，原来的单物理服务器节点不足以满足这个需求。

### 3.2 分布式数据库的理论基础

**1. CAP 理论**首先，分布式数据库的技术理论是基于单节点关系数据库的基本特性的继承，主要涉及事务的 ACID 特性、事务日志的容灾恢复性、数据冗余的高可用性几个要点。

其次，分布式数据的设计要遵循 CAP 定理，即：一个分布式系统不可能同时满足 一致性( Consistency ) 、可用性 ( Availability ) 、分区容 忍 性 ( Partition tolerance ) 这三个基本需求，最 多只能同时满足其中的两项， 分区容错性 是不能放弃的，因此架构师通常是在可用性和一致性之间权衡。这里的权衡不是简单的完全抛弃，而是考虑业务情况作出的牺牲，或者用互联网的一个术语“降级”来描述。

CAP 三个特性描述如下 ：一致性：确保分布式群集中的每个节点都返回相同的 、 最近 更新的数据 。一致性是指每个客户端具有相同的数据视图。有多种类型的一致性模型 ， CAP 中的一致性是指线性化或顺序一致性，是强一致性。

可用性：每个非失败节点在合理的时间内返回所有读取和写入请求的响应。为了可用，网络分区两侧的每个节点必须能够在合理的时间内做出响应。

分区容忍性：尽管存在网络分区，系统仍可继续运行并 保证 一致性。网络分区已成事实。保证分区容忍度的分布式系统可以在分区修复后从分区进行适当的恢复。

**2. BASE 理论**

基于 CAP 定理的权衡，演进出了 BASE 理论 ，BASE 是 Basically Available(基本可用)、Soft state(软状态)和 Eventually consistent(最终一致性)三个短语的缩写。BASE 理论的核心思想是：即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。

BA：Basically Available 基本可用，分布式系统在出现故障的时候，允许损失部分可用性，即保证核心可用；S：Soft state 软状态，允许系统存在中间状态，而该中间状态不会影响系统整体可用性；E：Consistency 最终一致性，系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。

BASE 理论本质上是对 CAP 理论的延伸，是对 CAP 中 AP 方案的一个补充。

### 3.3 分布式数据库的架构演变

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaufKISYMVA6xk1p9XpneMNjCHJ9wcmptqYxsYnaMXaXcoevzUOWCzZ8koib3sEuL20ToDH0BFE3xJw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)三类数据库架构特点：

1. Shard-everting：共享数据库引擎和数据库存储，无数据存储问题。一般是针对单个主机，完全透明共享 CPU/MEMORY/IO，并行处理能力是最差的，典型的代表 SQLServer；
2. Shared-storage：引擎集群部署，分摊接入压力，无数据存储问题；
3. Shard-noting：引擎集群部署，分摊接入压力，存储分布式部署，存在数据存储问题。各个处理单元都有自己私有的 CPU/内存/硬盘等，不存在共享资源，类似于 MPP（大规模并行处理）模式，各处理单元之间通过协议通信，并行处理和扩展能力更好。典型代表 DB2 DPF 和 hadoop ，各节点相互独立，各自处理自己的数据，处理后的结果可能向上层汇总或在节点间流转。

原文作者：yandeng，腾讯 PCG 应用开发工程师

原文链接：https://mp.weixin.qq.com/s/sRFmW57KUY3yyyRkyw0L4A

# 【NO.173】QUIC 协议原理浅解

## **1.QUIC 是啥？**

### 1.1 什么是 QUIC？

QUIC(Quick UDP Internet Connection)是谷歌推出的一套基于 UDP 的传输协议，它实现了 TCP + HTTPS + HTTP/2 的功能，目的是保证可靠性的同时降低网络延迟。因为 UDP 是一个简单传输协议，基于 UDP 可以摆脱 TCP 传输确认、重传慢启动等因素，建立安全连接只需要一的个往返时间，它还实现了 HTTP/2 多路复用、头部压缩等功能。

众所周知 UDP 比 TCP 传输速度快，TCP 是可靠协议，但是代价是双方确认数据而衍生的一系列消耗。其次 TCP 是系统内核实现的，如果升级 TCP 协议，就得让用户升级系统，这个的门槛比较高，而 QUIC 在 UDP 基础上由客户端自由发挥，只要有服务器能对接就可以。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaucFnwJjyX7XslGKqicQsId1kLiaWKAcuzRGnHRn0rxOU8U089iatVGaqalm1qQSAPSok2Y2ghPib7SOQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)图1-1 HTTP与QUIC

> (图引自《浅谈 QUIC 协议原理与性能分析及部署方案》-by 周陆军）

### 1.2 HTTP 协议发展

#### **1.2.1 HTTP 历史进程**

1. HTTP 0.9（1991 年）只支持 get 方法不支持请求头；
2. HTTP 1.0（1996 年）基本成型，支持请求头、富文本、状态码、缓存、连接无法复用；
3. HTTP 1.1（1999 年）支持连接复用、分块发送、断点续传；
4. HTTP 2.0（2015 年）二进制分帧传输、多路复用、头部压缩、服务器推送等；
5. HTTP 3.0（2018 年）QUIC 于 2013 年实现；2018 年 10 月，IETF 的 HTTP 工作组和 QUIC 工作组共同决定将 QUIC 上的 HTTP 映射称为 "HTTP/3"，以提前使其成为全球标准。

#### **1.2.2 HTTP1.0 和 HTTP1.1**

1. **队头阻塞：** 下个请求必须在前一个请求返回后才能发出，导致带宽无法被充分利用，后续请求被阻塞（HTTP 1.1 尝试使用流水线（Pipelining）技术，但先天 FIFO（先进先出）机制导致当前请求的执行依赖于上一个请求执行的完成，容易引起队头阻塞，并没有从根本上解决问题）；
2. **协议开销大：** header 里携带的内容过大，且不能压缩，增加了传输的成本；
3. **单向请求：** 只能单向请求，客户端请求什么，服务器返回什么；
4. **HTTP 1.0 和 HTTP 1.1 的区别：** 5. **HTTP 1.0**：仅支持保持短暂的 TCP 连接（连接无法复用）；不支持断点续传；前一个请求响应到达之后下一个请求才能发送，存在队头阻塞 6. **HTTP 1.1**：默认支持长连接（请求可复用 TCP 连接）；支持断点续传（通过在 Header 设置参数）；优化了缓存控制策略；管道化，可以一次发送多个请求，但是响应仍是顺序返回，仍然无法解决队头阻塞的问题；新增错误状态码通知；请求消息和响应消息都支持 Host 头域

#### **1.2.3 HTTP2**

解决 HTTP1 的一些问题，但是解决不了底层 TCP 协议层面上的队头阻塞问题。2015 年

1. **二进制传输：** 二进制格式传输数据解析起来比文本更高效；
2. **多路复用：** 重新定义底层 http 语义映射，允许同一个连接上使用请求和响应双向数据流。同一域名只需占用一个 TCP 连接，通过数据流（Stream）以帧为基本协议单位，避免了因频繁创建连接产生的延迟，减少了内存消耗，提升了使用性能，并行请求，且慢的请求或先发送的请求不会阻塞其他请求的返回；
3. **Header 压缩：** 减少请求中的冗余数据，降低开销；
4. **服务端可以主动推送：** 提前给客户端推送必要的资源，这样就可以相对减少一点延迟时间；
5. **流优先级：** 数据传输优先级可控，使网站可以实现更灵活和强大的页面控制；
6. **可重置：** 能在不中断 TCP 连接的情况下停止数据的发送。

**缺点**：`HTTP 2`中，多个请求在一个 TCP 管道中的，出现了丢包时，`HTTP 2`的表现反倒不如`HTTP 1.1`了。因为 TCP 为了保证可靠传输，有个特别的“丢包重传”机制，丢失的包必须要等待重新传输确认，`HTTP 2`出现丢包时，整个 TCP 都要开始等待重传，那么就会阻塞该 TCP 连接中的所有请求。而对于 `HTTP 1.1` 来说，可以开启多个 TCP 连接，出现这种情况反到只会影响其中一个连接，剩余的 TCP 连接还可以正常传输数据

#### **1.2.4 HTTP3 —— HTTP Over QUIC**

HTTP 是建立在 TCP 协议之上，所有 HTTP 协议的瓶颈及其优化技巧都是基于 TCP 协议本身的特性，HTTP2 虽然实现了多路复用，底层 TCP 协议层面上的问题并没有解决（HTTP 2.0 同一域名下只需要使用一个 TCP 连接。但是如果这个连接出现了丢包，会导致整个 TCP 都要开始等待重传，后面的所有数据都被阻塞了），而 HTTP3 的 QUIC 就是为解决 HTTP2 的 TCP 问题而生。

## **2.QUIC 的关键特性**

关于 QUIC 的原理，相关介绍的文章很多，这里再列举一下 QUIC 的重要特性。这些特性是 QUIC 得以被广泛应用的关键。不同业务也可以根据业务特点利用 QUIC 的特性去做一些优化。同时，这些特性也是我们去提供 QUIC 服务的切入点。

### **2.1 连接迁移**

#### 2.1.1 tcp 的连接重连之痛

一条 TCP 连接是由**四元组**标识的（源 IP，源端口，目的 IP，目的端口）。什么叫连接迁移呢？就是当其中任何一个元素发生变化时，这条连接依然维持着，能够保持业务逻辑不中断。当然这里面主要关注的是客户端的变化，因为客户端不可控并且网络环境经常发生变化，而服务端的 IP 和端口一般都是固定的。

比如大家使用手机在 WIFI 和 4G 移动网络切换时，客户端的 IP 肯定会发生变化，需要重新建立和服务端的 TCP 连接。

又比如大家使用公共 NAT 出口时，有些连接竞争时需要重新绑定端口，导致客户端的端口发生变化，同样需要重新建立 TCP 连接。

所以从 TCP 连接的角度来讲，这个问题是无解的。

#### **2.1.2 基于 UDP 的 QUIC 的连接迁移实现**

当用户的地址发生变化时，如 WIFI 切换到 4G 场景，基于 TCP 的 HTTP 协议无法保持连接的存活。QUIC 基于连接 ID 唯一识别连接。当源地址发生改变时，QUIC 仍然可以保证连接存活和数据正常收发。

那 QUIC 是如何做到连接迁移呢？很简单，QUIC 是基于 UDP 协议的，任何一条 QUIC 连接不再以 IP 及端口四元组标识，而是以一个 64 位的随机数作为 ID 来标识，这样就算 IP 或者端口发生变化时，只要 ID 不变，这条连接依然维持着，上层业务逻辑感知不到变化，不会中断，也就不需要重连。

由于这个 ID 是客户端随机产生的，并且长度有 64 位，所以冲突概率非常低。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaucFnwJjyX7XslGKqicQsId1dD6M352qiciazXfZK1ibwNMG5OHcdOU8tdWwHEiaiaq7zumvGJTFovjq5Ag/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)图2-1 TCP 和 QUIC 在 Wi-Fi 和 cellular 切换时，唯一标识的不同情况

> （图引自《跟坚哥学 QUIC 系列：连接迁移（Connection Migration)》- by Xiaojian Hong)

### **2.2 低连接延时**

#### **2.2.1 TLS 的连接时延问题**

以一次简单的浏览器访问为例，在地址栏中输入https://www.abc.com，实际会产生以下动作：

1. DNS 递归查询 www.abc.com，获取地址解析的对应 IP；
2. TCP 握手，我们熟悉的 TCP 三次握手需要需要 1 个 RTT；
3. TLS 握手，以目前应用最广泛的 TLS 1.2 而言，需要 2 个 RTT。对于非首次建连，可以选择启用会话重用，则可缩小握手时间到 1 个 RTT；
4. HTTP 业务数据交互，假设 abc.com 的数据在一次交互就能取回来。那么业务数据的交互需要 1 个 RTT；经过上面的过程分析可知，要完成一次简短的 HTTPS 业务数据交互，需要经历：新连接 **4RTT + DNS**；会话重用 **3RTT + DNS**。

所以，对于数据量小的请求而言，单一次的请求握手就占用了大量的时间，对于用户体验的影响非常大。同时，在用户网络不佳的情况下，RTT 延时会变得较高，极其影响用户体验。

下图对比了 TLS 各版本与场景下的延时对比：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvaucFnwJjyX7XslGKqicQsId1UXIDxCdCjIlxCrN90c4B7ibeaAd1ribA8tq6ntjhdIPcziaHyfSlqOYLw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图2-2 tls各个版本握手时延

> (图引自《QUIC 0-RTT 实现简析及一种分布式的 0-RTT 实现方案》）

从对比我们可以看到，即使用上了 TLS 1.3，精简了握手过程，最快能做到 0-RTT 握手(首次是 1-RTT)；但是对用户感知而言, 还要加上 1RTT 的 TCP 握手开销。Google 有提出 Fastopen 的方案来使得 TCP 非首次握手就能附带用户数据，但是由于 TCP 实现僵化，无法升级应用，相关 RFC 到现今都是 experimental 状态。这种分层设计带来的延时,有没有办法进一步降低呢? QUIC 通过合并加密与连接管理解决了这个问题，我们来看看其是如何实现真正意义上的 0-RTT 的握手, 让与 server 进行第一个数据包的交互就能带上用户数据。

#### **2.2.2 真·0-RTT 的 QUIC 握手**

QUIC 由于基于 UDP，无需 TCP 连接，在最好情况下，短连接下 QUIC 可以做到 0RTT 开启数据传输。而基于 TCP 的 HTTPS，即使在最好的 TLS1.3 的 early data 下仍然需要 1RTT 开启数据传输。而对于目前线上常见的 TLS1.2 完全握手的情况，则需要 3RTT 开启数据传输。对于 RTT 敏感的业务，QUIC 可以有效的降低连接建立延迟。

究其原因一方面是 TCP 和 TLS 分层设计导致的：分层的设计需要每个逻辑层次分别建立自己的连接状态。另一方面是 TLS 的握手阶段复杂的密钥协商机制导致的。要降低建连耗时，需要从这两方面着手。

QUIC 具体握手过程如下：

1. 客户端判断本地是否已有服务器的全部配置参数（证书配置信息），如果有则直接跳转到(5)，否则继续；
2. 客户端向服务器发送 inchoate client hello(CHLO)消息，请求服务器传输配置参数；
3. 服务器收到 CHLO，回复 rejection(REJ)消息，其中包含服务器的部分配置参数；
4. 客户端收到 REJ，提取并存储服务器配置参数，跳回到(1) ；
5. 客户端向服务器发送 full client hello 消息，开始正式握手，消息中包括客户端选择的公开数。此时客户端根据获取的服务器配置参数和自己选择的公开数，可以计算出初始密钥 K1；
6. 服务器收到 full client hello，如果不同意连接就回复 REJ，同(3)；如果同意连接，根据客户端的公开数计算出初始密钥 K1，回复 server hello(SHLO)消息，SHLO 用初始密钥 K1 加密，并且其中包含服务器选择的一个临时公开数；
7. 客户端收到服务器的回复，如果是 REJ 则情况同(4)；如果是 SHLO，则尝试用初始密钥 K1 解密，提取出临时公开数；
8. 客户端和服务器根据临时公开数和初始密钥 K1，各自基于 SHA-256 算法推导出会话密钥 K2；
9. 双方更换为使用会话密钥 K2 通信，初始密钥 K1 此时已无用，QUIC 握手过程完毕。之后会话密钥 K2 更新的流程与以上过程类似，只是数据包中的某些字段略有不同。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvaucFnwJjyX7XslGKqicQsId1uEibyZ4hsaML3LKpw1sCiaSPM9h2glfydgtGddXTvmnibyuez9pZiaicGSQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图2-3 quic 0-rtt 握手

> (图引自《QUIC 0-RTT 实现简析及一种分布式的 0-RTT 实现方案》）

#### **2.3 可自定义的拥塞控制**

Quic 使用可插拔的拥塞控制，相较于 TCP，它能提供更丰富的拥塞控制信息。比如对于每一个包，不管是原始包还是重传包，都带有一个新的序列号(seq)，这使得 Quic 能够区分 ACK 是重传包还是原始包，从而避免了 TCP 重传模糊的问题。Quic 同时还带有收到数据包与发出 ACK 之间的时延信息。这些信息能够帮助更精确的计算 rtt。此外，Quic 的 ACK Frame 支持 256 个 NACK 区间，相比于 TCP 的 SACK(Selective Acknowledgment)更弹性化，更丰富的信息会让 client 和 server 哪些包已经被对方收到。

QUIC 的传输控制不再依赖内核的拥塞控制算法，而是实现在应用层上，这意味着我们根据不同的业务场景，实现和配置不同的拥塞控制算法以及参数。GOOGLE 提出的 BBR 拥塞控制算法与 CUBIC 是思路完全不一样的算法，在弱网和一定丢包场景，BBR 比 CUBIC 更不敏感，性能也更好。在 QUIC 下我们可以根据业务随意指定拥塞控制算法和参数，甚至同一个业务的不同连接也可以使用不同的拥塞控制算法。

![图片](https://mmbiz.qpic.cn/mmbiz_gif/j3gficicyOvaucFnwJjyX7XslGKqicQsId1qUVa6riamiamaYC5kAwEGBQe8KZY8D6BRQTToqEKXudB2iaqicFd1FQQgw/640?wx_fmt=gif&wxfrom=5&wx_lazy=1)图2-4 BBR拥塞弱网下算法效果对比

> （图引自《TCP BBR - Exploring TCP congestion control》-by Andree Toonk)

### **2.4 无队头阻塞**

#### **2.4.1 TCP 的队头阻塞问题**

虽然 HTTP2 实现了多路复用，但是因为其基于面向字节流的 TCP，因此一旦丢包，将会影响多路复用下的所有请求流。QUIC 基于 UDP，在设计上就解决了队头阻塞问题。

TCP 队头阻塞的主要原因是数据包超时确认或丢失阻塞了当前窗口向右滑动，我们最容易想到的解决队头阻塞的方案是不让超时确认或丢失的数据包将当前窗口阻塞在原地。QUIC 也正是采用上述方案来解决 TCP 队头阻塞问题的。

TCP 为了保证可靠性，使用了基于字节序号的 Sequence Number 及 Ack 来确认消息的有序到达。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaucFnwJjyX7XslGKqicQsId1p6CwhPC4vAicsiaibOCkknhYbHG8msSStqcC7dLDBVwg7c98hlhdYuVOg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)图2-5 HTTP2队头阻塞

> （图引自《科普：QUIC 协议原理分析》）

如上图，应用层可以顺利读取 stream1 中的内容，但由于 stream2 中的第三个 segment 发生了丢包，TCP 为了保证数据的可靠性，需要发送端重传第 3 个 segment 才能通知应用层读取接下去的数据。所以即使 stream3 stream4 的内容已顺利抵达，应用层仍然无法读取，只能等待 stream2 中丢失的包进行重传。

在弱网环境下，HTTP2 的队头阻塞问题在用户体验上极为糟糕。

#### **2.4.2 QUIC 的无队头阻塞解决方案**

QUIC 同样是一个可靠的协议，它使用 Packet Number 代替了 TCP 的 Sequence Number，并且每个 Packet Number 都严格递增，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值，比如 Packet N+M。

QUIC 使用的 Packet Number 单调递增的设计，可以让数据包不再像 TCP 那样必须有序确认，QUIC 支持乱序确认，当数据包 Packet N 丢失后，只要有新的已接收数据包确认，当前窗口就会继续向右滑动。待发送端获知数据包 Packet N 丢失后，会将需要重传的数据包放到待发送队列，重新编号比如数据包 Packet N+M 后重新发送给接收端，对重传数据包的处理跟发送新的数据包类似，这样就不会因为丢包重传将当前窗口阻塞在原地，从而解决了队头阻塞问题。那么，既然重传数据包的 Packet N+M 与丢失数据包的 Packet N 编号并不一致，我们怎么确定这两个数据包的内容一样呢？

QUIC 使用 Stream ID 来标识当前数据流属于哪个资源请求，这同时也是数据包多路复用传输到接收端后能正常组装的依据。重传的数据包 Packet N+M 和丢失的数据包 Packet N 单靠 Stream ID 的比对一致仍然不能判断两个数据包内容一致，还需要再新增一个字段 Stream Offset，标识当前数据包在当前 Stream ID 中的字节偏移量。

有了 Stream Offset 字段信息，属于同一个 Stream ID 的数据包也可以乱序传输了（HTTP/2 中仅靠 Stream ID 标识，要求同属于一个 Stream ID 的数据帧必须有序传输），通过两个数据包的 Stream ID 与 Stream Offset 都一致，就说明这两个数据包的内容一致。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaucFnwJjyX7XslGKqicQsId1vGyYmNretcBH72paD7KTJxJheAogQQ9qs9SPLtH2AQZQdXbCPpFYzg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)图2-6 QUIC无队头阻塞

> （图引自《科普：QUIC 协议原理分析》）

## **3.QUIC 协议组成**

QUIC 的 packet 除了个别报文比如 PUBLIC_RESET 和 CHLO，所有报文头部都是经过认证的，报文 Body 都是经过加密的。这样只要对 QUIC 报文任何修改，接收端都能够及时发现，有效地降低了安全风险。

如图 3-1 所示，红色部分是 Stream Frame 的报文头部，有认证。绿色部分是报文内容，全部经过加密。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvaucFnwJjyX7XslGKqicQsId1nZhdMHAD33EQBHviaAPwL2d2HVKUbiatCfQegVa2v4pDPhg7YVZgMicuA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图3-1 QUIC的协议组成

> （图引自《科普：QUIC 协议原理分析》）

- **Flags**: 用于表示 Connection ID 长度、Packet Number 长度等信息；
- **Connection ID**：客户端随机选择的最大长度为 64 位的无符号整数。但是，长度可以协商；
- **QUIC Version**：QUIC 协议的版本号，32 位的可选字段。如果 Public Flag & FLAG_VERSION != 0，这个字段必填。客户端设置 Public Flag 中的 Bit0 为 1，并且填写期望的版本号。如果客户端期望的版本号服务端不支持，服务端设置 Public Flag 中的 Bit0 为 1，并且在该字段中列出服务端支持的协议版本（0 或者多个），并且该字段后不能有任何报文；
- **Packet Number**：长度取决于 Public Flag 中 Bit4 及 Bit5 两位的值，最大长度 6 字节。发送端在每个普通报文中设置 Packet Number。发送端发送的第一个包的序列号是 1，随后的数据包中的序列号的都大于前一个包中的序列号；
- **Stream ID**：用于标识当前数据流属于哪个资源请求；
- **Offset**：标识当前数据包在当前 Stream ID 中的字节偏移量。

QUIC 报文的大小需要满足路径 MTU 的大小以避免被分片。当前 QUIC 在 IPV6 下的最大报文长度为 1350，IPV4 下的最大报文长度为 1370。

## **4.结语**

QUIC 具有众多优点，它融合了 UDP 协议的速度、性能与 TCP 的安全与可靠，同时也解决了 HTTP1、HTTP1.1、HTTP2 中引入的一些缺点，大大优化了互联网传输体验。

腾讯云 CDN 也紧跟技术浪潮，于今年年初迭代中加入了 QUIC 的功能支持，目前正在内测当中。相关介绍以及内测申请可以戳这个链接：

https://cloud.tencent.com/document/product/228/51800

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvaucFnwJjyX7XslGKqicQsId1tAvNdvjkTnz0QjvUHiaWgE43YcycXxrUvgwkdFR4DcGHG13iaZnXbFibw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

## 5.参考

- [QUIC 在 Facebook 是如何部署的？](https://mp.weixin.qq.com/s?__biz=MzIzNjUxMzk2NQ==&mid=2247501925&idx=1&sn=5bec81739040f0c6bf844e2ab877048c&chksm=e8d437a7dfa3beb19cccca6dddaa7a9ed6d2395576702b52c21cdc3100f29617c215d99b4fca&mpshare=1&scene=21&srcid=0303jyzNPWUpyKn4s8fZj6eW&sharer_sharetime=1614737763782&sharer_shareid=0cd65f7f398401a991092e2cd18a8b64&version=3.1.0.2353&platform=mac#wechat_redirect)
- [STGW 下一代互联网标准传输协议 QUIC 大规模运营之路](https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==&mid=2649756393&idx=1&sn=1202bd71cfa837395b48f43d0943c484&chksm=becc819289bb088441f2f65c6e98ebc6816e10318ae1ec13d43cc999656989efe919cc47753e&mpshare=1&scene=21&srcid=0302NNKNLSd3dP1KkjHlO2jl&sharer_sharetime=1614737748553&sharer_shareid=0cd65f7f398401a991092e2cd18a8b64&version=3.1.0.2353&platform=mac#wechat_redirect)
- [QUIC 0-RTT 实现简析及一种分布式的 0-RTT 实现方案](https://cloud.tencent.com/developer/article/1594468)
- [科普：QUIC 协议原理分析](https://zhuanlan.zhihu.com/p/32553477)
- [TCP BBR - Exploring TCP congestion control](https://atoonk.medium.com/tcp-bbr-exploring-tcp-congestion-control-84c9c11dc3a9)
- [浅谈 QUIC 协议原理与性能分析及部署方案](https://zhuanlan.zhihu.com/p/146473513)
- [QUIC 是如何解决 TCP 性能瓶颈的？](https://blog.csdn.net/m0_37621078/article/details/106506532)
- [QUIC 协议 和 TCP/UDP 协议](https://nolaaaaa.github.io/2019/04/11/QUIC协议-和-TCP-UDP-协议/)
- [QUIC 的那些事 | 包类型及格式](https://blog.csdn.net/u014023993/article/details/86432341)
- [跟坚哥学 QUIC 系列：连接迁移（Connection Migration)](https://zhuanlan.zhihu.com/p/311221111)

原文作者：wellsjiang，腾讯 CSIG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/Bm_4M-QCcWYRqv1V8a-J-A

# 【NO.174】微信直播聊天室架构演进

## 1.**聊天室概述**

------

​    

随着直播和类直播场景在微信内的增长，业务对临时消息通道的需求日益增长，聊天室组件应运而生。聊天室组件是一个基于房间的临时消息信道，主要提供消息收发、在线状态统计等功能。





## 2.**1500w在线的挑战**

------

​    

视频号直播上线后，在产品上提出了直播后台需要有单房间支撑1500w在线的技术能力。接到这个项目的时候，自然而然就让人联想到了一个非常有趣的命题：能不能做到把13亿人拉个群？



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtppY71TPqsXjWgu5kpgFdOPBP7olYMg9VIHTvdA3I8XdHMFls0tIjFQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

本文将深入浅出地介绍聊天室组件在演进过程的思考，对这个命题做进一步对探索，尝试提出更接近命题答案的方案。





## 3.**聊天室1.0架构**

------





![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtx2UdGfZ8VjsysBxfSNz9oUYTuGnOUgLRWoEEHY8LgqUBOGibPWVFzQA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



聊天室1.0诞生于2017年，主要服务于微信电竞直播间，核心是实现高性能、高实时、高可扩展的消息收发架构。



### 3.1 **消息框架选型：读扩散**

------



|          | 微信群 | 聊天室 |
| -------- | ------ | ------ |
| 参与人数 | <=500  | 数万   |
| 关系链   | 有     | 无     |
| 成员流动 | 低     | 高     |
| 离线消息 | 关注   | 不关注 |



微信群消息使用写扩散的机制，而聊天室跟微信群有着巨大的差异。且同一时间只能关注一个聊天室，决定了聊天室应该使用读扩散的机制。



### **3.2 longpolling机制**

------



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowt178Gy3xXvWrG0C9n6I9Lh2UstOEhXcOhichN8Hia3CicPMOjmds9RAzBQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

为了让用户需要实时同步到新消息，我们采用的是longpolling模式。很多人会疑惑为什么不用websocket，原因有3个：

\1. websocket主要考虑推模式，而推模式则有可能丢，做到不丢还是有需要拉模式来兜底；

\2. 推模式下，需要精准维护每个时刻的在线列表，难度很大。

\3. longpolling本质是一个短连，客户端实现更简单。





### 3.3 **无状态cache的设计**



很明显，单纯的读扩散，会造成巨大读盘的压力。按照国际惯例，这里理所应当地增加了一个cache，也就是上面架构图中的recvsvr。

​            

![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtD3mZT4z1K11fsCvjx1EwYpUze22ibMxhAeYGDYdVzbzqAtksSWpkhwg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

普通的cache都是有状态的、可穿透的，对经常会出现突发流量的聊天室不是特别友好。而通过异步线程任务，恰好可以解决这两个点。



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtuHOOGLvrmdod8prytZep3ib2pC3ks68al10wQEGXIjv5fFBfd8oyWCw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



① **实时通知**：发送消息时，在写入列表后，向recvsvr集群发送通知；

② **异步拉取**：recvsvr机器收到通知后，触发异步线程拉取；

③ **兜底轮询**：当recvsvr机器上接收到某个聊天室的请求时，触发该聊天室的轮询，保证1s内至少访问一次消息列表，避免通知失效导致无法更cache，同时做到机器启动时数据的自动恢复。

​            

![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAlMCOGC0iaQAp3Gk80b7goic4iaEbhyIQddicfEIibZaDlht2MSwF1ZicELUNASMmGUcxXcOL0TjbHQPmFw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



④ **无锁读取**：通过读写表分离和原子切换，做到消息的无锁读取



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtZJTzsKqjg7jWcWSicDb4GsOmbNUtK75tAufkSDhmnwvbbV4wjkFJ5Wg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

⑤ **sect化部署**：群数量增多时，扩sect可以把群分摊到新的sect上。



无状态消息cache的设计，不仅极大地提高了系统的性能，而且帮助聊天室建立了一个高扩展性消息收发架构。

​     

### 3.4 **痛点**

------



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtIroL0OVEH8mDt5HRDubu1Ld3cicFiazWLXYVW8aOaL2I0fx97iapzcluw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

尽管做到了高性能的消息收发，1.0版本却并不能实现单房间1500w同时在线的目标。通过对整个架构和逻辑进一步的分析，我们发现4个阻碍我们前进的痛点：

（1）大直播间里，消息信道不保证所有消息都下发，连麦成功信令丢失会使得连麦功能不可用，大礼物打赏动画信令丢失会带来客诉；

（2）一个房间的在线列表，是由recvsvr把最近有收取该房间的消息的user聚合到同一台statsvr得到的，有单点瓶颈，单机失败会导致部分房间在线数跳变、在线列表和打赏排行榜不可用等；

（3）没有提供历史在线人数统计功能；

（4）裸的longpolling机制在消息一直有更新的情况下，无法控制请求量。



## 4.**聊天室2.0架构**

------





![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtgvnS2k4aupOpk2p2xekhzs0MeyS7Fic8Tt2wp3ZO9Qg2ENyCMLUsdew/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



从上面分析的痛点，我们得出了聊天室2.0需要解决的问题：

（1）**解决丢重要信令问题**，保证热点访问下功能的**可靠性**。

（2）**解决在线统计的单点瓶颈**，保证热点访问下在线统计模块的**可扩展性**。

（3）**实现一个高效准确的历史在线统计**，保证大数据量下统计的**高性能**和**准确性**。

（4）**灵活把控流量**，进一步提升**隔离**和**容灾**能力，保证热点访问下系统的**可用性**。



### 4.1 **优先级消息列表**

![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtnibk4ormZebSf8BduWnaMyI7WFMPT9ZMMzcPBrURNVqBibicWTvmQVzcA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

丢信令的本质原因：recvsvr只保留最近2000条消息，大直播间里，有些消息客户端还没来的及收就被cache淘汰了。



在聊天室1.0版本，我们已经证实了写扩散不可行，因此这里也不可能通过写扩散解决。



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtIlI2c84r5u88bReGvERlatDbB5eicyEeI3QQrxejVQiaVZdTNcUhebHA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



另外一个比较直观的方案：是将重要的系统信令写到另外一个列表里面，recvsvr同时读取两个消息表。带来的消耗是recvsvr对kv层增加将近一倍的访问量。于是，我们思考有没有更优的方案。

![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtxTnIFT5DWGkC90mQ9korrE5QSrd73Ept161LbGa6ZntCUMrsJ6JskQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



回到1.0版本的一个方案细节，我们可以看到大部分情况下，当新消息到来的时候，recvsvr它都是能及时感知到的，因此recvsvr一次拉取到的消息条数并不会很多，因此这一步骤上不会丢消息。所以我们是可以把消息表这个操作收归到recvsvr里面的：

  

![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtrNWwh2zcjQRIgSI4fcuWM5IoW5naq2TH7kyibvMSZGVA2O2SE6I0Fhg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

① **打优先级标记** ：依然只写一张消息表，给重要的信令打上优先级标记；（**节省RPC消耗**）

② **cache****内分表**：recvsvr拉到消息后分开普通消息列表和重要消息列表；（**最小化改动**）

③ **优先收取**：收取时分normal seq和important seq，先收重要消息表，再收取普通消息表。（**优先下发**）

通过一个简单的优化，我们以最小的改造代价，提供到了一条可靠的重要消息信道，做到了连麦和大礼物动画的零丢失。



### 4.2 **分布式在线统计**

（1）写共享内存，主从互备

![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtQL3FVblKO6gPQVtcibvTeJRMiaia8jibwg9URguNVMiaLsalryx3brxhx5Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

参考微信设备在线模块，我们可以有这个一个方案：

① 分sect，一个直播间选一个sect；

② 按roomid选一台机作为master， 读写该机器的共享内存；

③ master把这个roomid的数据同步到sect内其它机器，master挂了的情况可以选其它机器进行读写。

​      

优点：解决了换机跳变问题。

缺点：主备同步方案复杂；读写master，大直播间下依然有单机热点问题。

​    

结论：**用分布式存储作为数据的中心节点**。



（2）写tablekv



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowt4weVmkjfonhrAN56oEdI7cJzM8ibkIZNKG9hmacFget0HVNib25m5XHA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

① 用tablekv的一个表来存在线列表，每行记录用户id和活跃时间；

② 定期更新用户的心跳时间，维护在线。



优点：解决了换机跳变问题，数据做到了分布式。

缺点：1500w在线10s心跳一次 => 9000w/min，穿透写单表有并发和性能问题；离线不会实时从磁盘删数据，历史活跃人数远大于当前在线，造成数据冗余。



逐点击破，单key是可以通过拆key来解决的，数据冗余可以通过key-val存储做全量替换解决，而穿透问题其实可以参考recvsvr的实现方式。因此，我们得到一个比较好的方案：**拆key + 读写分离 + 异步聚合落盘**。





![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtJvib6SAszMhl3FUSmzmAPDsHFyvENozJnkORFcyeXkIOp90U550vJMQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

① **分布统计** :

(1) 每台机负责部分在线统计；

(2) 每台机内按uin哈希再分多shard打散数据；

(3) 每个shard对应kv的一个key；

​        

![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtGTO3Vu9jsLuxCodluEj2gN3GuKwF2HbqHoCHSkK7dJdYC18B5396eA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

② **组合数据**：让每台机都拉取所有key的数据，组合出一个完整的在线列表；



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowttpYsZqEoZ1RB7Qp8wseF9IVjfodEa2jbpDU9LwOIHx0WHTJCjI3fsw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



③ **异步聚合更新**：心跳只更新内存，异步任务清理离线用户，并把列表序列化到一个key的val；

④ **异步拉取**：由异步任务来执行②的拉取和组合数据；

⑤ **原子切换**：完整的在线列表做双指针，利用原子操作无锁切换，做到无锁查询。



由此，我们提高了心跳更新和在线查询的性能，做到了在线统计模块的分布式部署和可平行扩展。



### 4.3 **基于hyperloglog的历史在线统计**

------



历史在线统计，是要曾经看过该直播的用户数uv，在产品上的体验就是视频号直播的“xxx人看过”。

在分布式在线统计的章节，我们已经谈到了，用tablekv来记录成员列表是不太可行的。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavb69qtKBmXXyziah1WCZhyflwRb4QDHA4bkSMJyGdwme72GgEE3pRI50wHITnAE9wCjB9tzQGyTGw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



另外一个想法，是利用bloomfilter做数据压缩和去重统计，额外去维护一个count做累加。那么这里有两点，一是bloomfilter和count之间要考虑一致性问题，二是bloomfilter准确率跟压缩率相关，较好的准确率还是需要比较大的数据量。

 

于是我们调研了业界的一些uv统计方案，最终找到了redis的hyperloglog，它以极小的空间复杂度就能做到64位整形级别的基数估算。

（1）hyperloglog是什么？

hyperLogLog 是一种概率数据结构，它使用概率算法来统计集合的近似基数，算法的最本源则是伯努利过程。



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtib2zKFaItwl0RO3vzx2GOVxEdeaaa0nzExHvic60Ju2kZkFz1VxO6KSw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



伯努利过程：设一个硬币反面为0，正面为1，抛一枚硬币直到结果为1为止。

如果做n次伯努利实验，记录每次伯努利过程需要抛硬币的次数为Ki，则可以估算n=2^Kmax。

![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtdfdGvqoE3Nc4xCKBdcq25d2kOiaJhMqjdgLtIPjgZ2oPY9uU7BaEBXA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

hyperloglog对Kmax的计算进行了分桶和调和平均值优化，使得在准确率比裸的伯努利估算要高：

① 将要统计的数据hash成一个64位整形；

② 用低14位来寻找桶的位置；

③ 剩下的高位里寻找第一个1出现的位置，作为上述伯努利过程的Ki；

④ 对桶的值进行更新 Rj = max(Rj, Ki);

⑤ 估算时，对各个桶的值算调和平均值DV来替代上述的Kmax。



从上述算法的描述来看，hyperloglog无非就是存了m个桶的数值(m=10000+)，本来空间复杂度也不高了。再通过一些位压缩，hyperloglog把整个数据结构优化到了最大空间复杂度为12K。



（2）tablekv+hyperloglog双管齐下

由于hyperloglog产生的毕竟是近似值，基数较少的时候误差会更明显，所以我们可以用tablekv来补全历史在线数较小时的体验。



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtPXPWKsBJvRulTqZObURCjeQXuNWLq0dgpL5Nem8BWpqVicA5e00rMAw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

① 历史在线数较小时，**双写tablekv + hyperloglog**，以**tablekv selectcount为准**；

② 历史在线数较大时，**只写hyperloglog**，以**hyperloglog估算值为准**；

③ 在线统计模块**定期**把在线列表**merge到hyperloglog**避免丢数据。



最终我们达到的效果是，历史在线不超过1w时完全准确，超过1w时准确率大于95%。



### 4.4 **流量隔离vipsect**

------



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtJ73a6bo4LEPS7xiawicjPDEib4tlX5TyV6ucf1UFWw2Ovplx3Tunz2LWg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



大家都知道，大直播间会带来爆发式的请求量，我们不能让大直播间引起的失败影响占大多数的小直播间。另外大直播间影响力大，也要去保证它的良好体验，那需要用比小直播间更多的机器去支撑。而聊天室对kv层的请求数，跟机器数成正比，小直播间在多机器下会造成大量不必要的消耗。

​    

对于这种情况，我们参考了微信支付应对大商户和小商户的方法，流量隔离，在聊天室的里设立vip sect。



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtc4RythXA1XulowA32BNEVglYCMaYyaFbTrlbdkzYuibYN4qUHO0nssw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



① 对可预测的大直播提前加白，直接走vip sect；

② 其它直播直走普通sect；

③ 大小直播策略分级，大直播在线列表才拆key。



虽然还有些依赖运营，但是通过这种方式，我们切走大部分的大直播流量，也降低了整个系统对kv层的压力。

​    

**Q：为什么不做自动切vip sect ?

**

**A：是一个future work，目前有了一些初步方案，还需要去验证切换过程带来影响，进一步细化策略，也欢迎大家提出宝贵建议。**



### 4.5 **自动柔性下的流量把控**



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtHuP883HS6wmgCy4vSJl6FCHysXcB9HZKgnanOA2iad175ibv2yOxYuUQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

在longpolling的机制下，直播间一直有消息的话，100w的在线每分钟至少会产生6kw/min的请求，而1500w更是高达9亿/min。logicsvr是cpu密集型的服务，按30w/min的性能来算，至少需要3000台。所以这个地方必须要有一些柔性措施把控请求量，寻找一个体验和成本的平衡点。

​    

而这个措施一定不能通过logicsvr拒绝请求来实现，原因是longpolling机制下，客户端接收到回包以后是会马上发起一次新请求的。logicsvr拒绝越快，请求量就会越大，越容易造成滚雪球。



![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowt178Gy3xXvWrG0C9n6I9Lh2UstOEhXcOhichN8Hia3CicPMOjmds9RAzBQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)回到longpolling机制，我们可以发现，正常运行下，recvsvr没有新消息时，是可以让请求挂在proxy层hold住，等待连接超时或者longpolling notify的。

​    

所以，我们可以利用这个特性，柔性让请求或者回包在proxy hold一段时间，来降低请求频率。



![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavb69qtKBmXXyziah1WCZhyfpoRLYXvAW7hZnqEX0Snet3PRfGK6wSY3qm6iacz3wyb8cGghuOardFQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

① 根据不同的在线数设定收取间隔；

② 客户端上下文里增加字段，记录上一次成功收取的时间；

③ 成功收取后的一个时间间隔内，请求hold在proxy层；

④ 根据不同的在线数丢弃longpolling notify。

根据400w在线的压测效果，开启自适应大招时触发8～10s档位，请求量比没有大招的预期值降低58%，有效地控制了大直播间对logicsvr的压力。



### 4.6 **成果**

------



（1）支撑多个业务稳定运行

![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAlMCOGC0iaQAp3Gk80b7goic4ngK2AO9hUJCawctWZLiaDEXmguWibgqR7QVsM4ib1UvUES75UEyMzXmxg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



（2） 压测1500w同时在线

![图片](https://mmbiz.qpic.cn/mmbiz_png/UqFrHRLeCAmuJia1uqgAxbYlVAfPRHowtXO8hhZ4pCc8tQFyJe7AJWCyYFibmuaRvnlEK7WbfYvibMaX41WEfPRFQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



## 5.**参考文献**

https://zhuanlan.zhihu.com/p/77289303

https://www.jianshu.com/p/4748af30d194



## **6.总结与展望**

------



我们通过抽象问题、精准分析、合理设计完成了liveroom2.0的迭代，从性能、可靠性、可扩展性、容灾等方面达到支撑单房间1500w同时在线甚至更高在线的标准。

   

在未来我们将继续优化，比如实现大房间自动从普通sect切换到vip sect，比如针对房间内个人的重要消息通道，使聊天室的功能和架构更加强大。

原文作者：kellyliang

原文链接：https://mp.weixin.qq.com/s/JQaXbt9loJ3k7YFDgyLnog

# 【NO.175】2021 有哪些不容错过的后端技术趋势

## 0.**前言**

2020 年注定是不平凡的一年，虽疫情肆虐，但我国互联网产业展现出巨大韧性，不仅为精准有效防控疫情发挥了关键作用，还在数字基建、数字经济等方面取得了显著进展，成为我国应对新挑战、建设新经济的重要力量。

腾讯在线教育部后台中心团队，作为在线教育行业的从业者，我们尝试整理一下 2020 年后端技术要点，以此窥探后台未来技术的发展趋势：

1. 云计算进程提速，一切皆服务。
2. 云上安全越来越受到企业的重视。
3. 从资源云向业务云化转变，最终全面云原生化。
4. 微服务、DDD、中台技术并非企业技术架构设计的银弹。
5. Python、Go、Rust 成为后端未来最先考虑学习编程语言。
6. Go 语言生态发展稳健，越来越多企业在生产中使用 Go 语言落地业务。
7. 疫情催化在线教育行业产品升级转型，音视频技术不断迭代升级。

## 1.**云原生**

### 1.1 业内趋势

#### 1.1.1 **云原生技术生态日趋完善，细分项目不断涌现**

云原生关键技术正在被广泛采纳，如 43.9%的用户已在生产环境中采纳容器技术，超过七成的用户已经或计划使用微服务架构进行业务开发部署。

#### 1.1.2 **容器云平台将传统云计算的 IaaS 层和 PaaS 层融合**

从技术角度看，容器云平台采用容器、容器编排、服务网格，无服务等技术构建的一种轻量化 PaaS 平台，为应用提供了开发、编排、发布、治理和运维等全生命周期管理。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXricVRsFIOn8gVF1SicZ8HUBIht5UicjRaymUxKSWU6qGsPnJ1iaT0qZvlQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

容器云平台的整体架构，自下而上包括交互(UI)层、接口(API)层、PaaS 服务层、基础层。运维和安全则涵盖了从应用层到容器云引擎层的部分：

- 交互层：提供界面供用户使用
- 接口层：提供 OpenAPI 能力供第三方调用
- PaaS 服务层：提供数据服务、应用服务（微服务、中间件）、DevOps、平台管理、平台运营、应用管理能力，为实现业务应用其自身的生命周期管理
- 基础层：以 Kubernetes 为核心，包括服务网格（ServiceMesh）、无服务计算（Serverless）、容器引擎（Docker）、容器镜像管理等，主要实现对计算、网络和存储资源的池化管理，以及以 Pod 为核心的调度和管理。

#### 1.1.3 **服务网格为微服务带来新的变革**

Mesh 化加速业务逻辑与非业务逻辑的解耦，将非业务功能从客户端 SDK 中分离出来放入独立进程，利用 Pod 中容器共享资源的特性，实现用户无感知的治理接管。

#### 1.1.4 **从资源云向业务云化转变，最终全面云原生化**

云原生技术通过标准化资源，轻量化弹性调度等特征，应用场景较为广泛，随着技术和生态不断成熟和完善，有效缓解企业上云顾虑，拉动全行业的上云程度。

#### 1.1.5 **云原生技术栈的标准统一化**

架构标准统一（微服务之间标准 API 接口通信）、交付标准统一（标准容器化的打包方式实现真正的应用可移植）、研运过程标准统一（DevOps 工具链标准统一），通过标准化后提整体研发运维效能。

### 1.2 在线教育实践

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXcFuUtC3gtENroQLFfiaCticdoC1t3zzJoZPI325oLkibs0hZjBZ92w6gw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)enter image description here

#### 1.2.1 **各类 PaaS 服务上云**

2019 年我们完成了 IaaS、存储层、直播、回放以及各类 PaaS 服务的上云。

#### 1.2.2 **服务全面容器化升级**

2020 年我们重点进行服务全面的容器化升级，目前已经完成企鹅辅导和开心鼠英语两个产品的全面改造，到年底会完成腾讯课堂剩余部分的升级，实现全面完成改造。

#### **1.2.3 完善 DevOps 流程**

完善 CI/CD/CO、蓝盾流水线、容器化、STKE、全链路监控等，提高研发效率，降低现网运营难度

#### 1.2.4 **业务中台架构演进**

在整体架构上，我们依托腾讯云，确定了教育业务中台的架构演进方向，不断的进行重复模块的抽象和整合。我们在腾讯云上实现部署了接入中台、Push 中台、支付中台、音视频中台、运营中台等服务，让各个业务之间的相似能力得以复用。

#### 1.2.5 **存储层上云**

存储层上云后，一方面稳定性提高。

- 异地容灾。通过挂载异地的灾备机器，可以实现 master 主机异地灾备。
- 负载均衡。服务连接 RO 组，RO 组的多个实例会对请求进行负载均衡。
- 数据备份。RO 发生异常，将会被剔除 RO 组，恢复后自动加入 RO 组，保证了 RO 组的可用性。
- 数据加密。提供透明数据加密（Transparent Data Encryption，TDE）功能，透明加密指数据的加解密操作对用户透明，支持对数据文件进行实时 I/O 加密和解密，在数据写入磁盘前进行加密，从磁盘读入内存时进行解密，可满足静态数据加密的合规性要求。

另一方面运营能力也有所提升。

- 可以实时看到数据库连接情况，慢查询、全表扫描、查询、更新、删除、插入情况
- 实时 CPU、内存、磁盘使用情况，并根据设置阈值进行告警优化微服务架构

下面是在线教育上云前后架构对比

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXSibYZcOXHGic8trd84LZCK4rKOZnf0DSPuuibgRNxwGQ9FL7A5GcQUE6g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## 2.**微服务**

微服务,Service Mesh 在过去的一年依旧保持着热度。在已经过去的 2020，微服务可以说有坚守也有破局，有对服务微化共识的形成也有对特殊场景的理性思考。我们可以看到服务框架依然在持续演进，奔向云原生，拥抱云化。越来越多的企业开始跟上服务化云化步伐。

微服务框架：加速奔向云原生

### 2.1 **SpringCloud**

2018 年开始 Hystrix、Ribbon 等核心组件相继进入维护状态，开发者们一度变得忧心忡忡，时至今日我们回过头来再看一下，Spring Cloud 已经针对这些担忧给出了解决方案，Zuul 由 Spring Cloud GateWay 子项目替代，Hystrix 由 Spring Cloud Circuit Breaker 替代，同时也给出了长期的演进方案。在经历了这段小小的波折后，Spring Cloud 也改变了策略，将这些企业贡献的 OSS 库独立出来成为其子项目。目前我们可以看到有 Azure，Alibaba, Amazon 等 3 个带有企业名字的子项目，这种策略在某种程度上可以说解绑了企业开源策略对开源核心组件的影响。截至目前 Spring Cloud 下面的子项目已经新增至 34 个，越来越庞大。供开发者选择的组件越来越多。

### 2.2 **Dubbo**

2019 年 05 月 20 日 Dubbo 毕业，成为 Apache 的顶级项目，在过去的一年社区还是非常努力的，一年 release 5 个版本，加速奔向云原生。在 2.7.5 版本中，其服务模型调整以及协议支持调整带来的新旧版本兼容问题，稳定性等问题值得我们持续关注。

### 2.3 **Istio**

记得 2019 年我们一直在谈 istio 版本难产问题，在 2020 年却出乎意外的因为商标问题上了头条，让我们吃了个大瓜。Google 与 IBM 在商标问题上发生分歧，Istio 商标被 Google 捐献给 Open Usage Commons 组织，而非 CNCF。而这在加速了 Service Mesh 阵营依旧的分化，各大软件厂商纷纷发布了自己的 Service Mesh 产品，如微软发布了 Open Service Mesh，Kong 推出了 Kuma，Nginx 也推出了 NGINX Service Mesh（NSM）。

### 2.4 **企业微服务建设：长期修行，苦练内功**

在微服务框架的演进过程中我们看到都在朝着趋同的方向发展，主要聚焦于微服务治理形态上组件的差异化以及应对场景的方案细化，可能你们家服务中心用的 ZK，我们家就自研。恰逢内源的兴起，似乎在企业内部再造一次轮子，研发一套特定的框架来适配企业业务以及标准化企业内部 IT 治理也是一件很容易的事情，基于这种写实的场景部分企业开始涌现出了内源的服务框架如，腾讯的 tRPC 框架。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXMibsibicg9KtvUo69xbSyEU3bmoE6SYQTt3wicrWKX6LhsvymVicb7bY53w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)enter image description here

**腾讯 tRPC 建设情况** 目前 tRPC 在腾讯内部已经大面积推广使用，覆盖 5 个 BG，40+部门，2700+服务，10000+容器，支持 c++,go,java,js,rust,python 6 种编程语言。其可插拔的插件化架构，高性能，友好的架构兼容特性正在吸引内部越来越多的开发者以及业务用户。

在线教育业务也在积极的拥抱这套框架，逐步将各业务牵引到 tRPC 框架。在解决历史技术架构痛点的过程中，通过微服务构筑，形成微服务群，构筑稳定的支付,音视频等小中台以及面向 C、B 端用户的互联网业务系统。

## 3.**中台**

中台，是最近几年最火热的技术名词之一，关于中台的讨论，甚至是争论，一直都没有停止过。我们尝试结合腾讯在线教育部在中台方向的实践经验，谈谈中台对我们的意义和建设情况。

腾讯在线教育部从 2018 年开始规划部门内的中台建设，2019 年基本完成组织架构和技术架构的中台转型。我们和大多数公司一样，并不是从 0 开始构建中台，而是在保证现有业务快速迭代的前提下，同时完成架构的转型，大家形象的称这个过程是“开着飞机换引擎”。对于一个风险看起来这么高的事情，在决定做之前，我们要回答好几个问题：

### 3.1 我们为什么要做中台，部门需要什么样的中台？

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXEz65VsUwlIx8XqMHHcr5Cj3QcehUFTrNOPiccibG5Aic44icvZg3cIYzvg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

我们部门主要有三款产品：腾讯课堂——职业在线教育学习平台，具有 2B 和 2C 的双重属性。支持教育机构的入驻、直播上课、售卖、结算 等功能。腾讯企鹅辅导——腾讯自营的，主打 K12 名师教学的学习应用，为老师和学生提供了丰富的在线教学的功能。腾讯开心鼠英语——主打 3-8 岁的少儿英语学习，通过生动有趣的交互式学习设计，实现了边玩边学的有趣的学习体验。可以看到，这三个产品有不少类似的功能，例如直播、回放、支付、退款 等，因为这些共性，决定了他们会有很多相同的产品和技术需求。这些形成我们做中台的业务基础。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXKJAQq1hyT7FXsvoMf9rtBNfEwVGXTGV9Kribiaz4zNg9Jp7FuAsGKe9g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

在产品发展的初期，由于时间窗口非常紧，需求变化也很频繁。为了快速并行迭代，我们拉起了三个独立的团队进行研发，除了基础设施外，业务逻辑部分完全是独立的。这种组织架构，在当时确实为我们达成了上线时间的目标，帮助产品实现了从 0 到 1 的突破。但是，随着产品形态的成熟，3 个问题越来越突出：第一个问题，功能无法在不同的产品间快速复用。因为独立的代码和架构，复用变得非常的困难，很多开发同学反馈复用代码还不如重写一遍更快。第二个问题，同类的 Bug 的解决和技术优化在不同的产品之间重复进行，非常的浪费人力。第三个问题，不同的时期，我们需要发力的产品方向不一样。当一个产品面临发展窗口期的时候，对研发人力的需求就会成倍的增长。而独立的研发模式，让人力调配非常的困难。基于业务的模型，和团队碰到的痛点，我们提出了中台化的解决方案。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBX2E2mFRO2vicFUbcGXroCwYwJ1ibgrQIkaCA7Ic9pex6sGDSat7DoAa4g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

我们需要的中台应该长什么样子？经过前期的思考，我们总结出在线教育的中台应该包含 4 个部分。首先 是业务支持部分，这个很好理解，包含了各类共性的产品功能，例如前面提到的 直播、回放、支付 等等。其次 是技术支持部分，服务开发过程中 必然会涉及到例如 技术栈怎么选择，高可用怎么做 的共性技术问题，我们希望这部分有一个统一的技术实现。接下来 是数据支持部分，数据上报、计算、汇总、分析 已经是现在互联网产品必不可少的能力，也具备很强的通用性。因此这部分也应该有一个中台来承载。最后，是研发效能部分，我们需要有一整套好用的工具来提高研发效率和保障研发质量。到这里，我们对于中台的模样，应该是比较清晰了。

根据这个规划，我们画出了我们中台的组成图。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBX4Ez7wicyB4xvvKTBupemIQtDj2H3BiaPBzQnvvYttZ7nlK0moFVT1wag/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 3.2 怎么保证中台能做成功，最大的风险是什么？

很多人说，中台是“一把手工程”，意思是一定是自上而下推动的，需要老板的鼎力支持，是因为做中台确实是非常的需要人力，并且很大程度上改变了团队的资源投入模式，在多个业务需求压力都很大的情况下，做这么大的转变，团队短期内一定会碰到各种不适应的问题。虽然我们的中台方案也得到了老板的支持，但是绝不是中台优先，毕竟保障业务的高速发展才是团队追求的结果。一开始我们就意识到了困难的存在，为了不让中台死在半路，我们定了几个原则：

1. 控制人力占比：在人力有限的情况下，为了保障业务需求不受太大的进度影响，中台的人力投入原则上不超过总人力的 30%。
2. 不做过度设计：中台的设计目标只控制在部门内的需求（腾讯课堂、企鹅辅导、开心鼠英语），不面向行业做完全通用化的设计，根据实际需求做决策。
3. 完整规划，逐步实施：在做好完整的技术方案设计之后，我们不追求一次性完成中台的建设，而是结合业务产品需求的情况逐步实施，每半年也会 review 一次方案是否需要调整。

以上的规则，可以说很好的帮助我们保障了中台平滑转型的过程。另外，我们也在一个合适的时间点进行了团队人员组织结构的中台化匹配升级，保证了技术架构和组织架构的一致。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXasT1LlVr2zaSlibUia1qwFib25dJmv9sl7oNaSLOS5YGcicYSAFd4wIia1g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 3.3 中台建设的指导原则是什么，目标是什么？

我们中台服务设计的原则是什么，应该做哪些，什么时候做，做到什么程度。这个在业务部门里面其实是一个非常棘手的问题。很多时候我们需要在“时效性” 和 “扩展性”方面做选择。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXowQPlN53WBoeeBeegkxg4PeKqpW41lywsTLOGKd5OMI4c2ia5bHOSsg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

最后我们总结出来的一套方法是这样：对于一个新的需求，如果看到了可复用性，优先让业务团队自己做，一般这种需求时间紧、不明确，如果这时候讨论宏大的中台设计，往往效率低，耽误上线时间，但是，我会在过方案的时时候问大家“通用性是怎么考虑的”，最终在方案设计上做好通用型的前期 准备即可。后面如果再次收到另外一个产品的类似需求，这时候我们就认真考虑要把这个服务交接到中台组来维护了，由中台组安排人力来进行中台化。最后再有新的业务接入，就直接由中台组来承接了，就会非常的简单，我们很多中台服务都是这么跑出来的。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBX3HibeKf74HXvF3RFr2H0yslZNSbklA6amUFibOpGzFe6qtoL6hO4e3VA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

还有一个需要思考的问题是，中台服务的建设目标是什么。提出这个问题的背景是，我们希望给中台团队一个统一的、清晰的阶段性目标，当然也可以用来做团队考核。我们总结出来三个点：

1. 功能的复用：这个是最基本的，也是提出中台的初衷。具体到落地上，必须是一套代码。
2. 统一运营：要求中台服务能分产品输出标准化的实时监控看板和报表邮件，让业务一目了然。
3. 容灾调度的能力：不同业务的多套部署之间，在紧急情况下可以互备。需要进行实际的线上演习。

我们认为，达成这三个目标之后，才真正的发挥了中台的威力，可以实现 1+1 大于 2 的效果。

## 4.**DevOps**

2020 年，在云原生的浪潮下，devops 相关的技术栈也在稳步地向前演进，下面将从以下几个方面分别进行阐述：

- 敏捷的应用交付流程
- 监控告警系统
- tracing 系统
- 云原生对提升 devops 的展望

### 4.1 敏捷的应用交付流程

#### 4.1.1 **业内趋势**

当前出现了一系列的基于 Kubernetes 的 CI/CD 工具，如 Jenkins-x、Gitkube，它提供了从代码提交、自动编译、打包镜像、配置注入、发布部署到 Kubernetes 平台的一系列自动化流程。甚至出现了像 ballerina 这样的云原生编程语言，它的出现就是为了解决应用开发到服务集成之间的鸿沟。然后结合监控告警系统实时掌握服务运行情况，结合调用链系统进行服务故障定位。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXB5viaQLaialh7f5CHH42znHV013ASs5vPbfJLqILYNyOs18HHibFIh9RQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 4.1.2 **在线教育的实践**

蓝盾是腾讯从业务安全出发，贯穿产品研发、测试和运营的全生命周期；助力业务平滑过渡到敏捷研发模式，打造的一站式研发运维体系。它助力业务持续快速交付高质量的产品。蓝盾提供了丰富的特性：

- 可视化
- 一键式部署
- 和持续集成无缝集成
- 支持并行部署
- 架构水平扩展，相同逻辑的节点无主备关系
- 数据安全
- 小核心，大扩展！可插件化扩展，优先添加所需要的流程控制部件，同时可方便的扩展其他部件
- 可监控，监控一切异常的构建并告警
- 灰度切换，达到切换时不影响正在构建的流水线

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXGeWelspV6pMtevueROn33g0SUMMMQjK6MY5uQgUjOHjfOn4icGiagWcg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 4.2 监控告警系统

#### 4.2.1 **监控系统事实上的标准 prometheus**

Prometheus 在度量领域的统治力虽然还暂时不如日志领域中 Elastic Stack 的统治地位那么稳固，但在云原生时代里，基本也已经能算是事实标准了。2020 年，对 prometheus 来说，是忙碌的一年：

grafana cloud agent 发布：为 grafana cloud 优化的的一个轻量级 prometheus 分支，它使用 prometheus 的代码，保证 prometheus 生态依赖的一些属性：数据完整性、数据过期处理等等。允许使用一种更灵活的方式定义从何处以什么方法拉取和传输数据，它已经成为一种更受欢迎的方式将数据写入后端存储。它的 remote_write 方式相比于传统 prometheus agent 的 remote_write，内存使用降低了 40%；

v2.19.0 的发布：最核心的功能是，对于完整的 chunk，在内存中使用 head 结构映射磁盘中的 block，单就这一个点，内存使用就降低了 20%-40%，并且使得 prometheus 的重启速度更快；

v2.20.0 的发布：它为 Docker Swarm 和 DigitalOcean 支持了本地的服务发现；

提升批量插入一大批老数据到 prometheus 的效率；

Grafana Metrics Enterprise 的启动：提供针对大企业的 prometheus-as-a-service 的解决方案

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXvNEicnE71PVs5Bkh2bA5BibrlmqKz3k1pGqhm5pZCgDdQVGCsCp7R1lw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 4.2.2 **在线教育的实践**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXbO09MNmgeEiatbkHj24R0YoXrTkxtPbVxPadqnFX1JmlLuZuBibhF7PQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

nginx 访问日志和服务间模块调用相关信息都上报到全链路 es 集群

通过一个 exporter 对 es 中的数据按照项目和接口维度进行汇聚，将相关的聚合数据存入 prometheus

全链路 es 和 prometheus 都可以作为 grafana 的数据源进行数据展示和告警判断

alert 模块将告警信息发送到相应的渠道

收到告警之后，去 grafana 查看对应时间段的趋势图，去全链路 es 集群查看对应时间段的原始日志

### 4.3 tracing 系统

比起日志与度量， tracing 这个领域的产品竞争要相对激烈得多。一方面，目前还没有像日志、度量那样出现具有明显统治力的产品。另一方面，几乎市面上所有的追踪系统都是以 Dapper 的论文为原型发展出来的，功能上并没有太本质的差距，却又受制于实现细节，彼此互斥，很难搭配工作。

#### 4.3.1 **OpenTracing 规范**

为了推进追踪领域的产品的标准化，2016 年 11 月，CNCF 技术委员会接受了 OpenTracing 作为基金会第三个项目。OpenTracing 是一套与平台无关、与厂商无关、与语言无关的追踪协议规范。

#### 4.3.2 **OpenCensus 规范**

OpenTracing 规范公布后，几乎所有业界有名的追踪系统，譬如 Zipkin、Jaeger、SkyWalking 等都很快宣布支持 OpenTracing，但是 Google 自己却在此时提出了与 OpenTracing 目标类似的 OpenCensus 规范。OpenCensus 不仅涉及到追踪，还把指标度量也纳入进来。

#### 4.3.3 **OpenTelemetry 规范**

2019 年，OpenTracing 和 OpenCensus 又共同发布了可观测性的终极解决方案 OpenTelemetry，并宣布会各自冻结 OpenTracing 和 OpenCensus 的发展。OpenTelemetry 野心颇大，不仅包括追踪规范，还包括日志和度量方面的规范、各种语言的 SDK、以及采集系统的参考实现。

#### 4.3.4 **在线教育的实践**

腾讯基于 OpenTelemetry 规范，自研了天机阁系统。天机阁主要分为三大部分：

- 分布式追踪（distributed tracing）
- 监控（monitoring，metrics）
- 日志（logging）

与之对应提供七大能力：

- 故障定位：天机阁中能够提供整个请求全链路上下文信息，具体哪个环节出错一目了然
- 耗时分析：天机阁的耗时分布图，可以快速了解全链路耗时情况
- 多维染色：在天机阁基于通用的设计理念下，天机阁提供的染色能力，不会局限在某个业务的具体字段，同样也不会局限单个维度
- 架构治理：天机阁架构治理的核心功能是微服务架构拓扑，基于微服务架构拓扑可以构建更加丰富更加具体的上层分析能力
- 全链路日志：天机阁核心建立在分布式追踪方法论下，提供将分散在各个微服务的日志根据因果和时间有机进行组合，达到提供全链路上下文日志的效果
- 服务监控：天机阁在监控领域将会重点推出如下几大领域监控（机器基础指标监控、数据库监控、进程监控、模调监控）
- 业务看板：主要用于业务定制化的数据指标配置和展示

系统架构图

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXl7L12RLDwwG00w4Ywp6GYiblnJx54bHo5ppaqRsoWyOGx03jsykYPWg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

功能模块图

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXVibz1nKNyR1Nh4x98ia2yWictrWf2uffgNWoPNdhntfGKpMr1nNorc2Aw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 4.4 云原生对提升 devops 的展望

#### 4.4.1 **serverless 对提升 devops 的展望**

- 降低运维需求
- 缩短迭代周期、上线时间
- 快速试错
- 极致弹性
- 降低运营成本

#### 4.4.2 **service mesh 对提升 devops 的展望**

- 更好发挥掌握不同编程语言的人才优势
- 框架的部分能力平台化，加速应用迭代速度
- 微服务软件架构从解决"拆"到解决"连"的加速落地
- 灵活的流量治理能力改善软件的发布与回滚效率

## 5.**音视频**

### 5.1 音视频技术回顾

随着 AI 技术的兴起、5G 时代的到来，音视频技术不断加速应用发展，像直播、短视频这样的产品遍地开花，火热程度相信大家或多或少都接触过。

音视频技术的加速应用依赖底层编解码标准的发展，当前主流的 H.264 编解码技术已经不能满足未来 4K、8K 的需求，今年年中刚发布的 H266/VCC，与 H.265 相比进一步提高了压缩效率，这项耗时 3 年的标准，主要面向未来的 4K 和 8K，后续的落地应用非常值得期待。

在实时音视频技术领域，不得不提及谷歌的开源项目 WebRTC，可以在浏览器上快速开发出各种音视频应用，目前主流的浏览器包括 Chrome、Firefox、Safari 等都将 WebRTC 作为首选的实时音视频方案。同时也催生了像声网、即构科技这样的专门音视频服务商。从 StackOverflow Trends 和 GoogleTrends 来看，未来关注度仍会持续上升，腾讯也是 WebRTC 应用的主力军。

目前直播后台开发主要分为三大块：

1. 标准直播：我们日常生活中使用频率最高的直播，例如电视节目直播、游戏直播、直播带货。
2. 快直播：标准直播在超低延迟直播场景基础上的延伸，毫秒级超低延迟播放的同时，也兼顾了秒开、卡顿等核心问题，为用户带来超低延迟流畅度的直播体验。
3. 慢直播：能够提供更稳定清晰的直播画面，基本的使用场景都用于监控领域，国内疫情早期，4000 万人同时在线，通过一个固定机位观看雷神山医院建筑工地的现场直播，让云监工迅速火爆。2020 行至年终，各大机构评选的网络热词相继出炉，其中，“云监工”频繁出没于「十大网络热词」榜单中，与之并列的多是“后浪”“网抑云”“打工人”等。

随着网络技术的不断发展，持续提供高质量的视频信号传播已经算不上浪费网络资源，即使一个直播无人观看，未来慢直播具有极大的延伸价值以及发展前景，让我们期待慢直播行业的蓬勃发展。

借助 5G 技术低时延、高速率、大容量等显著优势，音视频的大赛道，从目前的短视频慢慢走向中长视频发展，这是未来的大风口。

### 5.2 平台的新技术点

目前腾讯在线教育音视频直播已完成整体上云，腾讯云的互动直播也从早期的 opensdk 全面升级到 TRTC，TRTC 是腾讯实时音视频[Tencent Real-Time Communication]，源自 QQ 音视频团队，是基于 QQ 十几年来的音视频技术积累。

腾讯云提供 TRTC(全球延时<300ms)+WebRTC 快直播(上行走 RTMP 推流或 FLV、HLS、RTMP 回源，下行支持标准 WebRTC 协议输出，延时 500ms 左右）+标准 LVB 直播(FLV/HLS/DASH，平均延时 3-5 秒)融合解决方案，如下图中用户可以针对自己的业务场景组合不同的直播解决方案。承载大规模带宽、支撑高并发，保证客户业务正常运作，达到 99.9%以上的可用性，整体资源储备及业务突发承接能力行业领先。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXIPibIkPv1weU8ompic9ib1LpibfTme7ZSU6W3BjPo6n3aNEE7YADrYrwMA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

随着全民抗疫，“停课不停学”的号召，在线教育也成为直播的主力军，直播的进房成功率/首帧延迟/卡顿率/音画同步时延/分辨率等指标直接影响用户核心体验。站在云的肩膀上，在线教育直播业务通过组合云上多种直播模式，结合业务流控系统，对各端直播接入进行多级流控及直播模式切换，在保证直播质量的前提下支撑远超互动直播极限的房间容量，下图是具体的直播架构。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXWrQ4hWM9sEWzqX81ymIQVTZXtywVkctW1l9I2snuLBZwE6LVu3m4Bw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 5.3 业务应用新技术的能力扩展

目前直播课普遍采用大班授课方式，老师在上课的时候，跟学生的互动有限，学生的注意力和参与感有限。大班教室人数太多，老师无法提供足量的 presentation 机会，学生与学生之间缺少有效的学习互动。

腾讯在线教育部推出如下图的六人小班课，基于 TRTC 在互动课堂场景下，为学员提供了稳定优质的服务，延迟低至原来的 1/10，互动效果得到很大提升。六人小班课给用户带来更多“被关注”的感觉，相比于大班课，家长的价值感知更高。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXVGTCoEVtRtHMPuKZqzCRKyPdyn5YyfzQglQh2zm0N5nIaKjWCkic6ibg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## 6.**接入网关**

### 6.1 网关发展历程

接入网关有四大职能

- API 入口：作为所有 API 接口服务请求的接入点，负责请求的转发。
- 业务聚合：网关封装了系统内部架构，为每个客户端提供一个定制的 API。作为所有后端业务服务的聚合点，所有的业务服务都可以在这里被调用。
- 中介策略：身份验证、路由、过滤、流控、缓存、监控、负载均衡、请求分片与管理、静态响应处理等策略，进行一些必要的中介处理。
- 统一管理：提供统一的管理界面，提供配置管理工具，对所有 API 服务的调用生命周期和相应的中介策略进行统一管理

开源网关发展迅速。从 nginx 横空出世，到 openresty 解放程序员，更加专注解决业务需求，再到 kong 成为 api 网关的独角兽，以及最近出现不久的 apisix，当然也不能少了大名鼎鼎的 envoy。下面介绍主要的几个网关。

#### 6.1.1 **nginx**

2004 年 10 月 4 日发布的第一个公开版本以来，nginx 已成为高性能 web 服务器、反向代理服务器的代名词。相比 Apache，Nginx 使用更少的资源，支持更多的并发连接，能够支持高达 50,000 个并发连接数的响应。模块化和将一个请求分为多个阶段的设计，方便开发人员扩展。

#### 6.1.2 **openresty**

OpenResty® 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。python、js、lua 三种语言中，lua 是解析器最小、性能最高的语言，而 LuaJIT 比 lua 又快数 10 倍，开发人员和系统工程师可以使用 Lua 脚本语言调动 Nginx 支持的各种 C 以及 Lua 模块，快速构造出足以胜任 10K 乃至 1000K 以上单机并发连接的高性能 Web 应用系统。

#### **6.1.3 kong**

kong 是 API 管理的强大效率工具，主要有 4 个特点

- 扩展性：通过增添更多的服务器实例达到横向扩展
- 灵活性：可以部署在单个或多个数据中心环境的私有云或公有云上。支持大多数流行的操作系统，比如 Linux、Mac 和 Windows。包括许多实用技巧，以便针对大多数现代平台完成安装和配置工作
- 模块性：可以与新的插件协同运行，扩展基本功能。可将 API 与许多不同的插件整合起来，以增强安全、分析、验证、日志及/或监测机制
- 生态：开源免费使用，同时也能获得企业版，此外还提供初始安装、从第三方 API 管理工具来迁移、紧急补丁、热修复程序及更多特性。

### 6.2 在线教育网关实践

#### 6.2.1 **在线教育网关发展过程中的包袱**

- 通道 proxy 多语言, 多框架, 多协议功能无法服用，维护成本高

- 配置动态加载能力和插件能力不统一

- 一个接口上线配置多次，验证多次。

- 缺乏完善的监控

- 频控，容灾、熔断、下载等能力缺失

- 非云原生应用，不支持自动扩缩容

  #### 6.2.2 **tiny 网关**

tiny 网关主要的能力如下：

- 提供 app 端, web, pc 端快速接入，统一 sdk 和协议
- 支持智能路由，支持按照 cmd, uid, roomid, cid 字段的路由
- 全房间和多维度组合推送策略
- 可靠 push 保障
- 业务级别的监控告警
- 命令字配置集中管理，支持热加载
- 支持插件化的能力，方便添加业务特性的插件
- 全面落地容器化，支持自动扩缩容

完整的架构如下图所示

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXH63yr5QxgMKGic33cVwxISM2U9GKqzSa2p4fsJpKIul73BNap8FuDvA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

## 7.**go 语言**

随着云原生在互联网行业的普及,golang 从众多语言中,脱颖而出,成为了云原生时代的新秀。越来越多的开源项目采用 golang 语言来实现。因此学习和掌握 golang 语言,越来越成为一种趋势。本篇文章,主要围绕 golang 语言的主要特性来展开讲解,希望对大家有帮助。

### 7.1 **语法简单**

golang 素以简单著称,总共 25 个保留字,相比 c++的 82 个,java 语言的 50 个,少的不能再少了。golang 官方也比较吝于新增命令字。常见的结构和判断,对于 golang 来说,就只有 if,for,switch 等非常简单的命令字。自带的 array,slice,map 等数据结构,基本可以 cover 大多数的使用场景。

### 7.2 **部署方便**

编译好的 golang 程序,是一个独立的二进制程序,只依赖操作系统的一些基础库,除此之外,没有任何其他外部依赖。有使用过 golang 语言开发开源软件的同学,应该感触。

### 7.3 **静态编译**

golang 是一门静态强类型的编译型语言,与 c++类似,golang 也是也有一个完整的编译链接过程,并且有严格的编译期的语法检查过程。配合 golang 强大的工具链,在编译期可以提前解决脚本语言运行时才能发现的诸多问题。

### 7.4 **垃圾回收**

一直以来,c++程序员饱受内存问题的困扰,常见的比如内存泄漏,溢出,double free 等问题层出不穷,并且定位起来费时费力。c++官方为了降低使用成本,也在 c++0x 之后,引入了智能指针来解决内存使用的问题。但是内存问题依然存在。golang 跟 java 语言一样,从语言层面提供了 GC 能力,自带的垃圾回收机制,有效解决了内存使用的诸多问题。但是垃圾回收并非完美无缺, 不合理的内存使用方式,依然会导致程序出现严重的 gc 问题，从而导致程序出现性能问题,因此也有一定的 trick 需要遵循。

### 7.5 **工具链支持**

除了 golang 语言自带的编译,安装,单元测试等工具之外,c++的调试神器 gdb 也能够使用。同时第三方提供的 delve 调试工具, 兼容性和易用性更好,同时还提供了远程调试的能力。原生自带的 perf 工具,配合第三方的 go-torch 工具,生成的火焰图,调试的时候非常方便, 让性能瓶颈能够一目了然。

### 7.6 **泛型**

golang 被人诟病的特性之一,就是不支持泛型。官方认为虽然泛型很赞，但会使语言设计复杂度提升，所以并没有把这个泛型支持作为紧急需要增加的特性，也许在不久的将来, 会引入这个特性。现阶段可以通过使用 Interface 作为中间层,起到抽象和适配的作用。一些第三方工具,比如 genny,通过模板化生成代码,也可以作为泛型的一种解决方案。

### 7.7 **错误处理**

golang 语言, 错误处理从语言层面得到了支持, 基于 Error 接口来实现,配合函数的多返回值机制,, ，一般情况下, 错误码也会作为函数的最后一个返回值。在 golang 中, 错误处理非常重要, 语言的设计和规范,也鼓励开发人员显示的检查错误。也正因为如此,golang 的错误处理，也被很多人所诟病，觉得不像其他高级语言,比如 java 的错误处理那么简洁。不过整体来说,golang 作者将错误码显性化,目的是为了让大家能够重视错误处理，所以应该说是各有特色。

### 7.8 **包管理**

golang 语言刚诞生的时候,并不支持版本管理。GOPATH 方式,也只能算是包管理基本的雏形。后来经过一系列的演变，社区先后支持了 dodep,glide 的工具。直到 2016 年,官方才提出采用外部依赖包的方式,引入了 vendor 机制。2017 的时候推出的 dep 工具,基本可以作为准官方的解决方案了。然而,直到 2019,go modules 的推出, golang 的包管理争论才最终尘埃落定。基于 go mod 的版本管理机制,基本上可以说是一统江湖。具体的 golang 包管理演进过程,如下图所示:

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXBibicCoVng3ccSAUlpDzZq629gHyElRET6Rqia9OjN8zPR8Leu9640GQw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 7.9 **并发性能**

golang 从语言层面就支持了并发,大大简化了并发程序的编写。这也是 golang 广受大家欢迎的原因之一。goroutine 是 golang 并发设计的核心, 本质上讲就是协程,也叫做用户态线程,它比线程更易用,更高效,更轻便,占用内存更小,并对开发者屏蔽了底层的协程调度细节。提供的 go,select,channel 等关键字,易用性非常好。配合 golang 中提供的 sync 包,可以非常高效的实现并发控制能力。

### 7.10 **常见网站**

- golang 百科全书: https://awesome-go.com/
- golang developer roadmap: https://github.com/Alikhll/golang-developer-roadmap
- sql2go 工具: http://stming.cn/tool/sql2go.html
- toml2go 工具: https://xuri.me/toml-to-go/
- curl2go 工具: https://mholt.github.io/curl-to-go/
- json2go 工具: https://mholt.github.io/json-to-go/
- 泛型工具: https://github.com/cheekybits/genny

### 7.11 **生态**

Go 在未来企业会有更多布道：Go Conference 一直都是 Gopher 关注的技术大会，20 年 11 月国内的 Go Conferernce(https://github.com/gopherchina/conference)分享主题主要包括 Go 语言基础（Go 编程模式、Go Module、Go 编译器、组件包）和架构落地实践（微服务实践、微服务框架、云原生实践、大数据和高并发挑战），侧面也印证了 Go 语言在后端领域具备较强的业务实战落地能力，对打算采用 Go 的互联网企业，具有较强的指导和借鉴意义。

### **7.12 业界认可度**

golang 作为云原生的首选语言,在业界获得广泛的认可。基于 golang 的很多明星项目,包括 docker,k8s,etcd,influxdb,tidb,prometheus,kibana,nsq 等覆盖了容器化,容器编排,存储,监控,消息队列等各个场景, 在各大公司都获得了大量的应用。同时从 github 拉取数据查看语言流行程度, 我们对比了 java,c++,c,go 等语言发现,golang 在 github 开源库的使用上越来越流行。如下图所示的 golang 占比情况:

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXayq6QuCFr42cUx5uBBPBm4IeDavOCGOC0FnpXR22xL97tM1u1iaNfYA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat3NmhGhH1ia9XZaVsLmOpBXo2M4ISUBRaHBg7ZNZO5lkrdbTicTrsFHRAiblfoWicmLEl05FkElcqEhA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 7.13 **趋势**

全面转到 Go Module：官方会终止对 GOPATH 的开发支持，全面转到 Go Module。

2021 年,golang 中的泛型还要持续打磨。

随着云原生浪潮，越来越多的企业将会考虑将 Go 作为其主要后端开发语言。

## 8.**总结**

2020 年已经过去，可以确定技术的发展是一分钟也不会停滞。可以预见云原生、微服务等新技术依旧是后台技术发展趋势，2021 年也会有更多创新出现。

原文作者：[腾讯技术工程](javascript:void(0);)

原文链接：https://mp.weixin.qq.com/s/EuqJ2Q9CAWSJ0rTc_p_gjg

