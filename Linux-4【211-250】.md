

# 【NO.211】【网络】【操作系统】详解select、poll、epoll



![img](https://pic4.zhimg.com/80/v2-164fd025311cab5f387a438474e77c3f_720w.webp)

看到这张图你能够完全理解吗。

## **1.多路复用的意思：**

多路复用的意思，就是在任何一路 I/O 有“事件”发生的情况下，通知应用程序去处理相应的 I/O 事件，这样我们的程序就变成了“多面手”，在同一时刻仿佛可以处理多个 I/O 事件。

## **2.应用条件：**

- 标准输入文件描述符准备好可以读。
- 监听套接字准备好，新的连接已经建立成功。
- 已连接套接字准备好可以写。
- 如果一个 I/O 事件等待超过了 10 秒，发生了超时事件。

## **3.中断的概念：**

在cpu运行着进程A时，进程B发起一个中断请求IRQ，中断处理程序进行处理请求，将进程A挂起，然后运行进程B。

**中断的过程：**将当前运行的进程的运行信息保存到该进程的描述符。根据进程描述符的内核态堆栈指针切换到内核态。根据来到的IRQ在中断表里面查找所属的中断处理程序。运行该中断处理程序。（看似是进程执行的中断，但和进程没什么关系）

## **4.硬中断的概念：**

对于计算机硬件的，一般来说是与当时CPU请求是异步的（没关系的），比如网关来了一个报文，

## **5.软中断的理解：**

CPU在执行一段代码段是 遇到问题，进行中断，由用户态切换到内核态。

## **6.每次中断都有其对应的中断处理程序。**

## **7.每个进程都在用户态和内核态拥有一个堆栈。**

## **8.为什么要有两种状态（内核态、用户态）**

内核态和用户态的”权限不同“。**防止每个程序都分配过多资源。**用户态的进程能够访问的资源受到了极大的控制，而运行在内核态的进程可以“为所欲为”。

一个进程可以运行在用户态也可以运行在内核态，那它们之间肯定存在用户态和内核态切换的过程。

打一个比方：C库接口malloc申请动态内存，malloc的实现内部最终还是会调用brk（）或者mmap（）系统调用来分配内存。

## **9.从用户态到内核态切换可以通过三种方式：**

1. 系统调用，这个上面已经讲解过了，在我公众号之前的文章也有讲解过。其实系统调用本身就是中断，但是**软件中断**，跟硬中断不同。
2. 异常：如果当前进程运行在用户态，如果这个时候发生了异常事件，就会触发切换。例如：缺页异常。
3. 外设中断：当外设完成用户的请求时，会向CPU发送中断信号。

## **10.socket的小demo**

![img](https://pic1.zhimg.com/80/v2-1908a6bb8b382e4cfd6779565addc598_720w.webp)

![img](https://pic4.zhimg.com/80/v2-db0bfbf01fa386987908941c54abb4cb_720w.webp)

## **11.socket底层逻辑图**

![img](https://pic2.zhimg.com/80/v2-5e46360d0e1a8d2095ef77cd2bc1c689_720w.webp)

## **12.阻塞模式下的情况**

![img](https://pic4.zhimg.com/80/v2-9a94669ac2e140bc22be8444fb3674eb_720w.webp)

写

（1）要传输的数据大于 输出缓冲区的大小，需要分开传输，还没传输的挂起。

（2）输出缓冲区TCP正在输出别的数据，需要TCP释放输出缓冲区才能写。

读

（1）如果输入缓存区没有数据，则系统调用的方法挂起。

（2）缓冲区数据太多，每次read只能读一部分，需要一直慢慢读直到读完

## **13.非阻塞模式下**

写

（1）如果向输出缓冲区写的数据太多了 ，分批发送，但是会马上告诉你发送了多少。

（2）如果空间为0，会马上告诉你缓冲区满了，由上次程序决定怎么做。（而阻塞则会一直挂起，等待缓冲区释放）

## **14.进程进行用户态到内核态的切换过程：**

　　1.从当前进程的描述符中提取其内核栈的ss0及esp0信息。

　　2.使用ss0和esp0指向的内核栈将当前进程的cs,eip,eflags,ss,esp信息保存起来，这个过程也完成了由用户栈到内核栈的切换过程，同时保存了z暂停执行的程序的下一条指令。

　　3.将先前由中断向量检索得到的中断处理程序的cs,eip信息装入相应的寄存器，开始执行中断处理程序，这时就转到了内核态的程序执行了。

## **15.Linux的一切都是文件fd。**

## 16.select函数

```
#include <sys/select.h>   
    int select(int maxfdp1, fd_set *readset, fd_set *writeset, fd_set *exceptset,struct timeval *timeout);
```

1）maxfdpl 　　最大有效位。（因为fd_set是以bitmap来保存数据的 linux默认1024位bitmap 最大有效位之后不需要检查）

2）fd_set *readset 　　可读文件描述符集

3)fd_set *writeset　　 可写文件描述符集

4)fd_set *exceptset　　 异常文件描述符集

5)timeout　　超时时间

Linux select函数的宏

```
#include <sys/select.h>   
int FD_ZERO(int fd, fd_set *fdset);  //将描述符集全部置0
int FD_CLR(int fd, fd_set *fdset);   //
int FD_SET(int fd, fd_set *fd_set);   //将某个位置位
int FD_ISSET(int fd, fd_set *fdset); //检查某个位是否被置位
```

![img](https://pic1.zhimg.com/80/v2-53cc1a75c89db88c54379aa8f3bc3ae4_720w.webp)

select函数缺点：

1）固定文件描述符1024位的bitmap，请求过大则无法描述，有大小限制

2）fd_set不可重用，每次都有重新置位

3）有用户态到内核态切换的开销

4）每次函数返回都需要O（n）的时间进行遍历

## 17.poll函数

我们将select函数的bitmap结构的fd_set变为 自己实现的pollfd

![img](https://pic4.zhimg.com/80/v2-f9364ebfef17a8664d0b5bcd76a63b1b_720w.webp)

使用链表来进行文件描述，不存在大小限制问题。

![img](https://pic3.zhimg.com/80/v2-b0e20d3e303ea72cf1b0295e420bf3ca_720w.webp)

## 18.epoll函数

![img](https://pic2.zhimg.com/80/v2-70b8611c49d96f83969b614fb11785d5_720w.webp)

## **19.同步调用、异步调用：**

同步：B来向A拿取数据，B一直等到A正确交付数据。

异步：B来向A拿取数据，B见A还在准备则返回 进行自己的事情。

## **20.阻塞、非阻塞**

A向B拿数据，C也向A要数据，C要等待 则是阻塞。

A向B拿数据，C也向A要数据，A同时服务B C 则是非阻塞。

同步与异步

- 同步： 同步就是发起一个调用后，被调用者未处理完请求之前，调用不返回。
- 异步： 异步就是发起一个调用后，立刻得到被调用者的回应表示已接收到请求，但是被调用者并没有返回结果，此时我们可以处理其他的请求，被调用者通常依靠事件，回调等机制来通知调用者其返回结果。

同步和异步的区别最大在于异步的话调用者不需要等待处理结果，被调用者会通过回调等机制来通知调用者其返回结果。

**阻塞和非阻塞**

- 阻塞： 阻塞就是发起一个请求，调用者一直等待请求结果返回，也就是当前线程会被挂起，无法从事其他任务，只有当条件就绪才能继续。
- 非阻塞： 非阻塞就是发起一个请求，调用者不用一直等着结果返回，可以先去干其他事情。

## **21.网络通信例子**

服务端

![img](https://pic1.zhimg.com/80/v2-7fd58166430da4022b1205bd65b04df0_720w.webp)

**客户端**

![img](https://pic2.zhimg.com/80/v2-f185a2f5a4567b52bbaf2e3d7bfdc12d_720w.webp)

![img](https://pic2.zhimg.com/80/v2-9f5bd298c21e0957e8ba6b71f9cba311_720w.webp)

我们可以看到服务端有2个方法 accept和read都在阻塞，这就是我们的BIO（同步阻塞I/O模式）

优化：我们可以使用多线程、线程池等来优化，关键是有很多不活跃的线程时，占用资源过多，上下文切换过多。

## **22.使用NIO优化通信例子（服务器端 客户端无所谓）**

![img](https://pic1.zhimg.com/80/v2-5403f6300311199babbc468b5a6755a4_720w.webp)

![img](https://pic2.zhimg.com/80/v2-cf6fb4e93394f16b2178f9d2a2e87a71_720w.webp)

![img](https://pic4.zhimg.com/80/v2-540655fd43e1f44b1e77c6e253da83ab_720w.webp)

## **23.windows的NIO是select linux的NIO底层是eqpoll。**

Redis的NIO是epoll，只能在Linux环境运行，但是有win版本，是大神修改了redis代码。

原文链接：https://zhuanlan.zhihu.com/p/367468443

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.212】亿级流量架构之网关设计思路、常见网关对比

本文准备围绕七个点来讲网关,分别是网关的基本概念、网关设计思路、网关设计重点、流量网关、业务网关、常见网关对比,对基础概念熟悉的朋友可以根据目录查看自己感兴趣的部分。

## 1.什么是网关

网关,很多地方将网关比如成门, 没什么问题, 但是需要区分网关与网桥的区别,

**网桥** 工作在数据链路层，在不同或相同类型的LAN之间存储并转发数据帧，必要时进行链路层上的协议转换。可连接两个或多个网络，在其中传送信息包。

**网关** 是一个大概念，不具体特指一类产品，只要连接两个不同的网络都可以叫网关,网桥一般只转发信息,而网关可能进行包装。

## 2.网关通俗理解

根据网关的特性,举个例子:

假如你要去找集团老板(这儿只是举个例子), 大家都知道老板肯定不是谁想见就能见的, 也怕坏人嘛, 那么你去老板所在的办公楼,假如是集团总部, 大楼这个门就充当了网关的角色, 大门一般都有看门员 ,看门员会做哪些事情呢?

首先所有想见老板的人肯定都得从这个门进(**统一入口**), 这个门相当于将办公室和外界隔离了,主要为了保护里面的安全以及正常工作, 来到这个门之后, 门卫肯定会让你出示相关证件(**鉴权检验**),

意思就是判断你要见老板这个请求是否合理,

如果不合理直接就拒绝了, 让你回家等消息 ,

如果鉴权之后, 发现你找老板其实只是为了和他谈谈两元店的生意, 门卫会跟你说这个用不着找老板, 你去集团投资部就行了(**动态路由**, 将请求路由到不同的后端集群中), 此时会对你进行一些**包装**,例如给你出具一个访问证类似的,然后告诉你路该怎么走,等等。

你看看,网关的作用是不是就是这三个, 最终目的就是减少你与集团的耦合,

具体到计算机上就是减少客户端与服务端的耦合,

如果没有网关意味着所有请求都会直接调用服务器上的资源,这样耦合太强了,服务器出了问题,客户端会直接报错,

例如老板换工作的地方了,如果没有网关你直接去原来的地方找, 肯定会被告知老板不在这儿。

## 3.为什么需要网关

当使用单体应用程序架构时，客户端（Web 或移动端）通过向后端应用程序发起一次 REST 调用来获取数据。

负载均衡器将请求路由给 N 个相同的应用程序实例中的一个。然后应用程序会查询各种数据库表，并将响应返回给客户端。

微服务架构下，单体应用被切割成多个微服务，如果将所有的微服务直接对外暴露，势必会出现安全方面的各种问题,另外内外耦合严重。

客户端可以直接向每个微服务发送请求，其问题主要如下：

- 客户端需求和每个微服务暴露的细粒度 API 不匹配。
- 部分服务使用的协议不是Web友好协议。可能使用 Thrift 二进制 RPC，也可能使用 AMQP 消息传递协议。
- 微服务难以重构。如果合并两个服务，或者将一个服务拆分成两个或更多服务，这类重构就非常困难了。

服务端的各个服务直接暴露给客户端调用势必会引起各种问题。同时，服务端的各个服务可扩展和伸缩性很差。API 网关是微服务架构中的基础组件，位于接入层之下和业务服务层之上，如前所述的这些功能适合在 API 网关实现。

## 4.网关与服务器集群

回到我们服务器上,下面图介绍了网关(Gateway)作用,可知 Gateway 方式下的架构，

可以细到为每一个服务的实例配置一个自己的 Gateway，

也可以粗到为一组服务配置一个，

甚至可以粗到为整个架构配置一个接入的 Gateway。于

是，整个系统架构的复杂度就会变得简单可控起来。

![img](https://pic3.zhimg.com/80/v2-301a788c3586b85e81bcd1a81820ce72_720w.webp)

这张图展示了一个多层 Gateway 架构，

其中有一个总的 Gateway 接入所有的流量(**流量网关**)，并分发给不同的子系统，

还有第二级 Gateway 用于做各个子系统的接入 Gateway(**业务网关**)。

可以看到，网关所管理的服务粒度可粗可细。

通过网关，我们可以把分布式架构组织成一个星型架构，由网络对服务的请求进行路由和分发。

下面来聊聊好的网关应该具备哪些功能,也就是网关设计模式。

## 5.网关设计思路

一个网关需要有以下的功能:

## 6.请求路由

网关一定要有请求路由的功能。这样一来，对于调用端来说，也是一件非常方便的事情。因为调用端不需要知道自己需要用到的其它服务的地址，全部统一地交给 Gateway 来处理。

## 7.服务注册

为了能够代理后面的服务，并把请求路由到正确的位置上，网关应该有服务注册功能，也就是后端的服务实例可以把其提供服务的地址注册、取消注册。一般来说，注册也就是注册一些 API 接口。

比如，HTTP 的 Restful 请求，

可以注册相应 API 的 URI、方法、HTTP 头。

这样，Gateway 就可以根据接收到的请求中的信息来决定路由到哪一个后端的服务上。

## 8.负载均衡

因为一个网关可以接收多个服务实例，所以网关还需要在各个对等的服务实例上做负载均衡策略。

简单点就是直接 Round-Robin 轮询，

复杂点的可以设置上权重进行分发，

再复杂一点还可以做到 session 粘连。

## 9.弹力设计

网关还可以把弹力设计中的那些异步、重试、幂等、流控、熔断、监视等都可以实现进去。

这样，同样可以像 Service Mesh 那样，让应用服务只关心自己的业务逻辑（或是说数据面上的事）而不是控制逻辑（控制面）。

## 10.安全方面

SSL 加密及证书管理、Session 验证、授权、数据校验，以及对请求源进行恶意攻击的防范。

错误处理越靠前的位置就是越好，

所以，网关可以做到一个全站的接入组件来对后端的服务进行保护。

当然，网关还可以做更多更有趣的事情，比如：灰度发布、API聚合、API编排。

**灰度发布**

网关完全可以做到对相同服务不同版本的实例进行导流，还可以收集相关的数据。这样对于软件质量的提升，甚至产品试错都有非常积极的意义。

**API 聚合**

使用网关可以将多个单独请求聚合成一个请求。在微服务体系的架构中，因为服务变小了，所以一个明显的问题是，客户端可能需要多次请求才能得到所有的数据。

这样一来，客户端与后端之间的频繁通信会对应用程序的性能和规模产生非常不利的影响。

于是，我们可以让网关来帮客户端请求多个后端的服务（有些场景下完全可以并发请求），然后把后端服务的响应结果拼装起来，回传给客户端（当然，这个过程也可以做成异步的，但这需要客户端的配合）。

**API 编排**

同样在微服务的架构下，要走完一个完整的业务流程，我们需要调用一系列 API，就像一种工作流一样，这个事完全可以通过网页来编排这个业务流程。

我们可能通过一个 DSL 来定义和编排不同的 API，也可以通过像 AWS Lambda 服务那样的方式来串联不同的 API。

## 11.网关设计重点

网关设计重点主要是三个, 高性能、高可用、高扩展:

## 12.高性能

在技术设计上，网关不应该也不能成为性能的瓶颈。对于高性能，最好使用高性能的编程语言来实现，如 C、C++、Go 和 Java。

网关对后端的请求，以及对前端的请求的服务一定要使用异步非阻塞的 I/O 来确保后端延迟不会导致应用程序中出现性能问题。

C 和 C++ 可以参看 Linux 下的 epoll 和 Windows 的 I/O Completion Port 的异步 IO 模型，Java 下如 Netty、Spring Reactor 的 NIO 框架。

## 13.高可用

因为所有的流量或调用经过网关，所以网关必须成为一个高可用的技术组件，它的稳定直接关系到了所有服务的稳定。

网关如果没有设计，就会成变一个单点故障。因此，一个好的网关至少要做到以下几点。

- **集群化**。网关要成为一个集群，其最好可以自己组成一个集群，并可以自己同步集群数据，而不需要依赖于一个第三方系统来同步数据。
- **服务化**。网关还需要做到在不间断的情况下修改配置，一种是像 Nginx reload 配置那样，可以做到不停服务，另一种是最好做到服务化。也就是说，得要有自己的 Admin API 来在运行时修改自己的配置。
- **持续化**。比如重启，就是像 Nginx 那样优雅地重启。有一个主管请求分发的主进程。当我们需要重启时，新的请求被分配到新的进程中，而老的进程处理完正在处理的请求后就退出。

## 14.高扩展

因为网关需要承接所有的业务流量和请求，所以一定会有或多或少的业务逻辑。

而我们都知道，业务逻辑是多变和不确定的。

比如，需要在网关上加入一些和业务相关的东西。因此，一个好的 Gateway 还需要是可以扩展的，并能进行二次开发的。当然，像 Nginx 那样通过 Module 进行二次开发的固然可以。

另外，在**运维方面**，网关应该有以下几个设计原则。

- **业务松耦合，协议紧耦合**。在业务设计上，网关不应与后面的服务之间形成服务耦合，也不应该有业务逻辑。网关应该是在网络应用层上的组件，不应该处理通讯协议体，只应该解析和处理通讯协议头。另外，除了服务发现外，网关不应该有第三方服务的依赖。
- **应用监视，提供分析数据**。网关上需要考虑应用性能的监控，除了有相应后端服务的高可用的统计之外，还需要使用 Tracing ID 实施分布式链路跟踪，并统计好一定时间内每个 API 的吞吐量、响应时间和返回码，以便启动弹力设计中的相应策略。
- **用弹力设计保护后端服务**。网关上一定要实现熔断、限流、重试和超时等弹力设计。如果一个或多个服务调用花费的时间过长，那么可接受超时并返回一部分数据，或是返回一个网关里的缓存的上一次成功请求的数据。你可以考虑一下这样的设计。
- **DevOps**。因为网关这个组件太关键了，所以需要 DevOps 这样的东西，将其发生故障的概率降到最低。这个软件需要经过精良的测试，包括功能和性能的测试，还有浸泡测试。还需要有一系列自动化运维的管控工具。

## 15.网关设计注意事项

1. 不要在网关中的代码里内置聚合后端服务的功能，而应考虑将聚合服务放在网关核心代码之外。可以使用 Plugin 的方式，也可以放在网关后面形成一个 Serverless 服务。
2. 网关应该靠近后端服务，并和后端服务使用同一个内网，这样可以保证网关和后端服务调用的低延迟，并可以减少很多网络上的问题。这里多说一句，网关处理的静态内容应该靠近用户（应该放到 CDN 上），而网关和此时的动态服务应该靠近后端服务。
3. 网关也需要做容量扩展，所以需要成为一个集群来分担前端带来的流量。这一点，要么通过 DNS 轮询的方式实现，要么通过 CDN 来做流量调度，或者通过更为底层的性能更高的负载均衡设备。
4. 对于服务发现，可以做一个时间不长的缓存，这样不需要每次请求都去查一下相关的服务所在的地方。当然，如果你的系统不复杂，可以考虑把服务发现的功能直接集成进网关中。
5. 为网关考虑 bulkhead 设计方式。用不同的网关服务不同的后端服务，或是用不同的网关服务前端不同的客户。

另外，因为网关是为用户请求和后端服务的桥接装置，所以需要考虑一些安全方面的事宜。具体如下：

1. **加密数据**。可以把 SSL 相关的证书放到网关上，由网关做统一的 SSL 传输管理。
2. **校验用户的请求**。一些基本的用户验证可以放在网关上来做，比如用户是否已登录，用户请求中的 token 是否合法等。但是，我们需要权衡一下，网关是否需要校验用户的输入。因为这样一来，网关就需要从只关心协议头，到需要关心协议体。而协议体中的东西一方面不像协议头是标准的，另一方面解析协议体还要耗费大量的运行时间，从而降低网关的性能。对此，我想说的是，看具体需求，一方面如果协议体是标准的，那么可以干；另一方面，对于解析协议所带来的性能问题，需要做相应的隔离。
3. **检测异常访问**。网关需要检测一些异常访问，比如，在一段比较短的时间内请求次数超过一定数值；还比如，同一客户端的 4xx 请求出错率太高……对于这样的一些请求访问，网关一方面要把这样的请求屏蔽掉，另一方面需要发出警告，有可能会是一些比较重大的安全问题，如被黑客攻击。

## 16.流量网关

流量网关,顾名思义就是控制流量进入集群的网关,有很多工作需要在这一步做,对于一个服务集群,势必有很多非法的请求或者无效的请求,这时候要将请求拒之门外,降低集群的流量压力。

![img](https://pic1.zhimg.com/80/v2-7de5c7ebecd2c8ec565a1f66d02feb54_720w.webp)

定义全局性的、跟具体的后端业务应用和服务完全无关的策略网关就是上图所示的架构模型——流量网关。流量网关通常只专注于全局的Api管理策略，比如全局流量监控、日志记录、全局限流、黑白名单控制、接入请求到业务系统的负载均衡等，有点类似防火墙。**Kong 就是典型的流量网关。**

下面是kong的架构图,来自官网:

![img](https://pic1.zhimg.com/80/v2-9cc39bdbbef22ea6f77ae2b9d0058e80_720w.webp)

这里需要补充一点的是，业务网关一般部署在流量网关之后、业务系统之前，比流量网关更靠近业务系统。通常API网指的是业务网关。 有时候我们也会模糊流量网关和业务网关，让一个网关承担所有的工作,所以这两者之间并没有严格的界线。

## 17.业务网关

当一个单体应用被拆分成许许多多的微服务应用后，也带来了一些问题。一些与业务非强相关的功能，比如权限控制、日志输出、数据加密、熔断限流等，每个微服务应用都需要，因此存在着大量重复的代码实现。而且由于系统的迭代、人员的更替，各个微服务中这些功能的实现细节出现了较大的差异，导致维护成本变高。另一方面，原先单体应用下非常容易做的接口管理，在服务拆分后没有了一个集中管理的地方，无法统计已存在哪些接口、接口定义是什么、运行状态如何。

网关就是为了解决上述问题。作为微服务体系中的核心基础设施，一般需要具备接口管理、协议适配、熔断限流、安全防护等功能，各种开源的网关产品（比如 zuul）都提供了优秀高可扩展性的架构、可以很方便的实现我们需要的一些功能、比如鉴权、日志监控、熔断限流等。

与流量网关相对应的就是业务网关,业务网关更靠近我们的业务,也就是与服务器应用层打交道,那么有很多应用层需要考虑的事情就可以依托业务网关,例如在线程模型、协议适配、熔断限流，服务编排等。下面看看业务网关体系结构:

![img](https://pic2.zhimg.com/80/v2-42d78918168eb1258fb82101d6da33f1_720w.webp)

从这个途中可以看出业务网关主要职责以及所做的事情, 目前业务网关比较成熟的 API 网关框架产品有三个 分别是:Zuul1、Zuul2 和 SpringCloud Gateway, 后面再进行对比。

## 18.常见网关对比

既然对比,就先宏观上对各种网关有一个了解,后面再挑一些常用的或者说应用广泛的详细了解。

目前常见的开源网关大致上按照语言分类有如下几类：

- Nginx+lua：OpenResty、Kong、Orange、Abtesting gateway 等
- Java：Zuul/Zuul2、Spring Cloud Gateway、Kaazing KWG、gravitee、Dromara soul 等
- Go：Janus、fagongzi、Grpc-gateway
- Dotnet：Ocelot
- NodeJS：Express Gateway、Micro Gateway

按照使用数量、成熟度等来划分，主流的有 4 个：

- OpenResty
- Kong
- Zuul/Zuul2
- Spring Cloud Gateway

## 19.OpenResty

OpenResty是一个流量网关,根据前面对流量网关的介绍就可以知道流量网关的指责。

OpenResty基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。

通过揉和众多设计良好的 Nginx 模块，OpenResty 有效地把 Nginx 服务器转变为一个强大的 Web 应用服务器，基于它开发人员可以使用 Lua 编程语言对 Nginx 核心以及现有的各种 Nginx C 模块进行脚本编程，构建出可以处理一万以上并发请求的极端高性能的 Web 应用

OpenResty 最早是顺应 OpenAPI 的潮流做的，所以 Open 取自“开放”之意，而Resty便是 REST 风格的意思。虽然后来也可以基于 ngx_openresty 实现任何形式的 web service 或者传统的 web 应用。

也就是说 Nginx 不再是一个简单的静态网页服务器，也不再是一个简单的反向代理了。第二代的 openresty 致力于通过一系列 nginx 模块，把nginx扩展为全功能的 web 应用服务器。

ngx_openresty 是用户驱动的项目，后来也有不少国内用户的参与，从 [http://openresty.org](https://link.zhihu.com/?target=http%3A//openresty.org) 的点击量分布上看，国内和国外的点击量基本持平。

ngx_openresty 目前有两大应用目标：

1. 通用目的的 web 应用服务器。在这个目标下，现有的 web 应用技术都可以算是和 OpenResty 或多或少有些类似，比如 Nodejs, PHP 等等。ngx_openresty 的性能（包括内存使用和 CPU 效率）算是最大的卖点之一。
2. Nginx 的脚本扩展编程，用于构建灵活的 Web 应用网关和 Web 应用防火墙。有些类似的是 NetScaler。其优势在于 Lua 编程带来的巨大灵活性。

## 20.Kong

Kong基于OpenResty开发，也是流量层网关, 是一个云原生、快速、可扩展、分布式的Api 网关。继承了OpenResty的高性能、易扩展性等特点。

Kong通过简单的增加机器节点，可以很容易的水平扩展。

同时功能插件化，可通过插件来扩展其能力。而且在任何基础架构上都可以运行。具有以下特性：

- 提供了多样化的认证层来保护Api。
- 可对出入流量进行管制。
- 提供了可视化的流量检查、监视分析Api。
- 能够及时的转换请求和相应。
- 提供log解决方案
- 可通过api调用Serverless 函数。

**Kong解决了什么问题**

当我们决定对应用进行微服务改造时，应用客户端如何与微服务交互的问题也随之而来，毕竟服务数量的增加会直接导致部署授权、负载均衡、通信管理、分析和改变的难度增加。

面对以上问题，API GATEWAY是一个不错的解决方案，其所提供的访问限制、安全、流量控制、分析监控、日志、请求转发、合成和协议转换功能，

可以解放开发者去把精力集中在具体逻辑的代码，而不是把时间花费在考虑如何解决应用和其他微服务链接的问题上。

![img](https://pic2.zhimg.com/80/v2-35f40f81587aed9a0a9fa8be29bd4039_720w.webp)

可以看到Kong解决的问题。专注于全局的Api管理策略，全局流量监控、日志记录、全局限流、黑白名单控制、接入请求到业务系统的负载均衡等。

**Kong的优点以及性能**

在众多 API GATEWAY 框架中，Mashape 开源的高性能高可用API网关和API服务管理层——KONG（基于 NGINX+Lua）特点尤为突出，它可以通过插件扩展已有功能，这些插件（使用 lua 编写）在API请求响应循环的生命周期中被执行。于此同时，KONG本身提供包括 HTTP 基本认证、密钥认证、CORS、TCP、UDP、文件日志、API请求限流、请求转发及 NGINX 监控等基本功能。目前，Kong 在 Mashape 管理了超过 15,000 个 API，为 200,000 开发者提供了每月数十亿的请求支持。

**Kong架构**

Kong提供一些列的服务,这就不得不谈谈内部的架构:

![img](https://pic3.zhimg.com/80/v2-cce151e53dd8351914321301a9d92ac6_720w.webp)

首先最底层是基于Nginx, Nginx是高性能的基础层, 一个良好的负载均衡、反向代理器,然后在此基础上增加Lua脚本库,形成了OpenResty,拦截请求, 响应生命周期,可以通过Lua编写脚本,所以插件比较丰富。

## 21.Zuul1.0

Zuul是所有从设备和web站点到Netflix流媒体应用程序后端请求的前门。作为一个边缘服务应用程序，Zuul被构建来支持动态路由、监视、弹性和安全性。它还可以根据需要将请求路由到多个Amazon自动伸缩组。

Zuul使用了一系列不同类型的过滤器，使我们能够快速灵活地将功能应用到服务中。

**过滤器**

过滤器是Zuul的核心功能。它们负责应用程序的业务逻辑，可以执行各种任务。

- **Type** ： 通常定义过滤器应用在哪个阶段
- **Async** ： 定义过滤器是同步还是异步
- **Execution Order** ： 执行顺序
- **Criteria** ： 过滤器执行的条件
- **Action** ： 如果条件满足，过滤器执行的动作

Zuul提供了一个动态读取、编译和运行这些过滤器的框架。过滤器之间不直接通信，而是通过每个请求特有的RequestContext共享状态。

下面是Zuul的一些过滤器:

**Incoming**

Incoming过滤器在请求被代理到Origin之前执行。这通常是执行大部分业务逻辑的地方。例如:认证、动态路由、速率限制、DDoS保护、指标。

**Endpoint**

Endpoint过滤器负责基于incoming过滤器的执行来处理请求。Zuul有一个内置的过滤器（ProxyEndpoint），用于将请求代理到后端服务器，因此这些过滤器的典型用途是用于静态端点。例如:健康检查响应，静态错误响应，404响应。

**Outgoing**

Outgoing过滤器在从后端接收到响应以后执行处理操作。通常情况下，它们更多地用于形成响应和添加指标，而不是用于任何繁重的工作。例如:存储统计信息、添加/剥离标准标题、向实时流发送事件、gziping响应。

**过滤器类型**

下面是与一个请求典型的生命周期对应的标准的过滤器类型：

- **PRE** ： 路由到Origin之前执行
- **ROUTING** ： 路由到Origin期间执行
- **POST** ： 请求被路由到Origin之后执行
- **ERROR** ： 发生错误的时候执行

这些过滤器帮助我们执行以下功能：

- **身份验证和安全性** ： 识别每个资源的身份验证需求，并拒绝不满足它们的请求
- **监控** ： 在边缘跟踪有意义的数据和统计数据，以便给我们一个准确的生产视图
- **动态路由** ： 动态路由请求到不同的后端集群
- **压力测试** ： 逐渐增加集群的流量，以评估性能
- **限流** ： 为每种请求类型分配容量，并丢弃超过限制的请求
- **静态响应处理** ： 直接在边缘构建一些响应，而不是将它们转发到内部集群

**Zuul 1.0 请求生命周期**

![img](https://pic3.zhimg.com/80/v2-aefc96cf001481ad52556bee69f346fe_720w.webp)

Netflix宣布了通用API网关Zuul的架构转型。Zuul原本采用同步阻塞架构，转型后叫作Zuul2，采用异步非阻塞架构。Zuul2和Zuul1在架构方面的主要区别在于，Zuul2运行在异步非阻塞的框架上，比如Netty。Zuul1依赖多线程来支持吞吐量的增长，而Zuul 2使用的Netty框架依赖事件循环和回调函数。

## 22.Zuul2.0

Zuul 2.0 架构图

![img](https://pic1.zhimg.com/80/v2-8e6bdfe75fba354d8cf33b44e0930488_720w.webp)

上图是Zuul2的架构，和Zuul1没有本质区别，两点变化：

1. 前端用Netty Server代替Servlet，目的是支持前端异步。后端用Netty Client代替Http Client，目的是支持后端异步。
2. 过滤器换了一下名字，用Inbound Filters代替Pre-routing Filters，用Endpoint Filter代替Routing Filter，用Outbound Filters代替Post-routing Filters。

**Inbound Filters** ： 路由到 Origin 之前执行，可以用于身份验证、路由和装饰请求

**Endpoint Filters** ： 可用于返回静态响应，否则内置的ProxyEndpoint过滤器将请求路由到Origin

**Outbound Filters** ： 从Origin那里获取响应后执行，可以用于度量、装饰用户的响应或添加自定义header

有两种类型的过滤器：sync 和 async。因为Zuul是运行在一个事件循环之上的，因此从来不要在过滤中阻塞。如果你非要阻塞，可以在一个异步过滤器中这样做，并且在一个单独的线程池上运行，否则可以使用同步过滤器。

上文提到过**Zuul2开始采用了异步模型**

**优势**是异步非阻塞模式启动的线程很少，基本上一个CPU core上只需启一个事件环处理线程，它使用的线程资源就很少，上下文切换(Context Switch)开销也少。非阻塞模式可以接受的连接数大大增加，可以简单理解为请求来了只需要进队列，这个队列的容量可以设得很大，只要不超时，队列中的请求都会被依次处理。

**不足**,异步模式让编程模型变得复杂。一方面Zuul2本身的代码要比Zuul1复杂很多，Zuul1的代码比较容易看懂，Zuul2的代码看起来就比较费劲。另一方面异步模型没有一个明确清晰的请求->处理->响应执行流程(call flow)，它的流程是通过事件触发的，请求处理的流程随时可能被切换断开，内部实现要通过一些关联id机制才能把整个执行流再串联起来，这就给开发调试运维引入了很多复杂性，比如你在IDE里头调试异步请求流就非常困难。另外ThreadLocal机制在这种异步模式下就不能简单工作，因为只有一个事件环线程，不是每个请求一个线程，也就没有线程局部的概念，所以对于CAT这种依赖于ThreadLocal才能工作的监控工具，调用链埋点就不好搞(实际可以工作但需要进行特殊处理)。

总体上，异步非阻塞模式比较适用于IO密集型(IO bound)场景，这种场景下系统大部分时间在处理IO，CPU计算比较轻，少量事件环线程就能处理。

**Zuul 与 Zuul 2 性能对比**

图片来源:Zuul’s Journey to Non-Blocking

![img](https://pic1.zhimg.com/80/v2-d3d3feead501f6e261348e76bca13968_720w.webp)

Netflix给出了一个比较模糊的数据，**大致Zuul2的性能比Zuul1好20%左右**，这里的性能主要指每节点每秒处理的请求数。为什么说模糊呢？因为这个数据受实际测试环境，流量场景模式等众多因素影响，你很难复现这个测试数据。即便这个20%的性能提升是确实的，其实这个性能提升也并不大，和异步引入的复杂性相比，这20%的提升是否值得是个问题。Netflix本身在其博文22和ppt11中也是有点含糊其词，甚至自身都有一些疑问的。

## 23.Spring Cloud Gateway

相关链接:官网、中文官方文档

SpringCloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。

SpringCloud Gateway 作为 Spring Cloud 生态系统中的网关，目标是替代 Zuul，在Spring Cloud 2.0以上版本中，没有对新版本的Zuul 2.0以上最新高性能版本进行集成，仍然还是使用的Zuul 2.0之前的非Reactor模式的老版本。而为了提升网关的性能，SpringCloud Gateway是基于WebFlux框架实现的，而WebFlux框架底层则使用了高性能的Reactor模式通信框架Netty。

Spring Cloud Gateway 的目标，不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。

**Spring Cloud Gateway 底层使用了高性能的通信框架Netty**。

**SpringCloud Gateway 特征**

SpringCloud官方，对SpringCloud Gateway 特征介绍如下：

（1）基于 Spring Framework 5，Project Reactor 和 Spring Boot 2.0

（2）集成 Hystrix 断路器

（3）集成 Spring Cloud DiscoveryClient

（4）Predicates 和 Filters 作用于特定路由，易于编写的 Predicates 和 Filters

（5）具备一些网关的高级功能：动态路由、限流、路径重写

从以上的特征来说，和Zuul的特征差别不大。SpringCloud Gateway和Zuul主要的区别，还是在底层的通信框架上。

简单说明一下上文中的三个术语：

**Filter**（过滤器）

和Zuul的过滤器在概念上类似，可以使用它拦截和修改请求，并且对上游的响应，进行二次处理。过滤器为org.springframework.cloud.gateway.filter.GatewayFilter类的实例。

**Route**（路由）

网关配置的基本组成模块，和Zuul的路由配置模块类似。一个**Route模块**由一个 ID，一个目标 URI，一组断言和一组过滤器定义。如果断言为真，则路由匹配，目标URI会被访问。

**Predicate**（断言）：

这是一个 Java 8 的 Predicate，可以使用它来匹配来自 HTTP 请求的任何内容，例如 headers 或参数。**断言的**输入类型是一个 ServerWebExchange。

## 24.几种网关的对比

|         网关         |                             限流                             |                 鉴权                  |                             监控                             |                        易用性                        |               可维护性                |                 成熟度                 |
| :------------------: | :----------------------------------------------------------: | :-----------------------------------: | :----------------------------------------------------------: | :--------------------------------------------------: | :-----------------------------------: | :------------------------------------: |
| Spring Cloud Gateway |     可以通过IP，用户，集群限流，提供了相应的接口进行扩展     |           普通鉴权、auth2.0           |                    Gateway Metrics Filter                    |                       简单易用                       | spring系列可扩展强，易配置 可维护性好 |   spring社区成熟，但gateway资源较少    |
|        Zuul2         | 可以通过配置文件配置集群限流和单服务器限流亦可通过filter实现限流扩展 |             filter中实现              |                         filter中实现                         |                     参考资料较少                     |             可维护性较差              |            开源不久，资料少            |
|      OpenResty       |                         需要lua开发                          |              需要lua开发              |                           需要开发                           |         简单易用，但是需要进行的lua开发很多          | 可维护性较差，将来需要维护大量lua脚本 |             很成熟资料很多             |
|         Kong         | 根据秒，分，时，天，月，年，根据用户进行限流。可在原码的基础上进行开发 | 普通鉴权，Key Auth鉴权，HMAC，auth2.0 | 可上报datadog，记录请求数量，请求数据量，应答数据量，接收于发送的时间间隔，状态码数量，kong内运行时间 | 简单易用，api转发通过管理员接口配置，开发需要lua脚本 | “可维护性较差，将来需要维护大量lua库  | 相对成熟，用户问题汇总，社区，插件开源 |

原文链接：https://zhuanlan.zhihu.com/p/369091358

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.213】【Redis】利用 Redis 实现分布式锁

## 1.技术背景

首先我们需要先来了解下什么是分布式锁，以及为什么需要分布式锁。

对于这个问题，我们可以简单将锁分为两种——内存级锁以及分布式锁，内存级锁即我们在 Java 中的 synchronized 关键字（或许加上进程级锁修饰更恰当些），而分布式锁则是应用在分布式系统中的一种锁机制。分布式锁的应用场景举例以下几种：

- 互联网秒杀
- 抢优惠卷
- 接口幂等校验

我们接下来以一段简单的秒杀系统中的判断库存及减库存来描述下为什么需要到分布式锁：

```
Copypublic String deductStock() throws InterruptedException {
    // 1.从 Redis 中获取库存值
    int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
    // 2.判断库存
    if (stock > 0) {
        int readStock = stock - 1;
        // 3.从新设置库存
        stringRedisTemplate.opsForValue().set("stock", realStock + ""); 
        System.out.println("扣减成功，剩余库存：" + readStock + "");
    } else {
        System.out.println("扣减失败，库存不足");
    }
    return "end";
}
```

上面这段代码中，实现了电商系统中的一个简单小需求，即判断商品的剩余数量是否充足，充足则可以成功卖出商品，并将库存减去 1。我们很容易了解这段代码的目的。接下来我们就来一步一步地分析这段代码的缺陷。

## 2.基本实现

## 3.原子性问题

上面代码中的注释1~3部分，并没有实现原子性的逻辑。所以假设现在如果只剩下一件商品，那么可能会出现以下情况：

- 线程 A 运行到代码2，判断库存大于0，进入条件体中将 stock - 1 赋值给 readStock，在执行代码 3 前停止了下来；
- 线程 B 同样运行到代码2，判断出库存大于0（线程A并没有写回Redis），之后并没有停止，而是继续执行到方法结束；
- 线程 A 此时恢复执行，执行完代码 3，将库存写回 Redis。

现在我们就发现了问题，明明只有一件商品，却被两个线程卖出去了两次，这就是没有保证这部分代码的原子性所带来的安全问题。

那对于这个问题如何解决呢？

常规的方式自然就是加锁以保证并发安全。那么以我们 Java 自带的锁去保证并发安全，如下：

```
Copypublic Synchronized String deductStock() throws InterruptedException {    
    // 业务逻辑...
}
```

我们知道 synchronized 和 Lock 支持 JVM 内同一进程内的线程互斥，所以如果我们的项目是单机部署的话，到这里也就能保证这段代码的原子性了。不过以互联网项目来说，为了避免单点故障以及并发量的问题，一般都是以分布式的形式部署的，很少会以单机部署，这种情况就会带来新的问题。

## 4.分布式问题

刚刚我们将到了如果项目分布式部署的话，那么就会产生新的并发问题。接下来我们以 Nginx 配置负载均衡为例来演示并发问题，同样的请求可能会被分发到多台服务器上，那么我们刚刚所讲的 synchronized 或者 Lock 在此时就失效了。同样的代码，在 A 服务器上确实可以避免其他线程去竞争资源，但是此时 A 服务器上的那段 synchronized 修饰的方法并不能限制 B 服务器上的程序去访问那段代码，所以依旧会产生我们一开始所讲到的线程并发问题。

![img](https://pic4.zhimg.com/80/v2-f24ccd6b6bd815c03a85bfb17034d9e7_720w.webp)

那么如何解决掉这个问题呢？这个是否就需要 Redis 上场了，Redis 中有一个命令SETNX key value，SETNX 是 “SET if not exists” （如果不存在，则 SET）的缩写。那么这条指令只在 key 不存在的情况下，将键 key 的值设置为 value。若键 key 已经存在，则 SETNX 命令不做任何动作。

有了上面命令做支撑，同时我们了解到 Redis 是单线程模型（不要去计较它的网络读写和备份状态下的多线程）。那么我们就可以这么实现，当一个服务器成功的向 Redis 中设置了该命令，那么就认定为该服务器获得了当前的分布式锁，而其他服务器此时就只能一直等待该服务器释放了锁为止。我们来看下代码实现：

```
Copy// 为了演示方便，这里简单定义了一个常量作为商品的id
public static final String PRODUCT_ID = "100001";
public String deductStock() throws InterruptedException {
    // 通过 stringRedisTemplate 来调用 Redis 的 SETNX 命令，key 为商品的id，value的值在这不重要
    Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(RODUCT_ID, "jojo");
    if (!result) {
        return "error";
    }
    int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
    if (stock > 0) {
        int readStock = stock - 1;
        stringRedisTemplate.opsForValue().set("stock", realStock + ""); 
        System.out.println("扣减成功，剩余库存：" + readStock + "");
    } else {
        System.out.println("扣减失败，库存不足");
    }
    // 业务执行完成，删除PRODUCT_ID key
    stringRedisTemplate.delete(PRODUCT_ID);
    return "end";
}
```

到这里我们就成功地利用 Redis 实现了一把简单的分布式锁，那么这样实现是否就没有问题了呢？

## 5.锁释放问题

生产环境比我们想象中要复杂得多，上面代码并不能正真地运用在我们的生产环境中，我们可以试想一下，如果服务器 A 中的程序成功地给线程加锁，并且执行完了减库存的逻辑，但是最终却没有安全地运行stringRedisTemplate.delete(PRODUCT_ID)这行代码，也就是没有成功释放锁，那其他服务器就永远无法拿到 Redis 中的分布式锁了，也就会陷入死锁的状态。

解决这个方法，可能许多人都会想到想到——try-finally语句块，像下面代码这样：

```
Copypublic String deductStock() throws InterruptedException {
    Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(RODUCT_ID, "jojo");
    if (!result) {
        return "error";
    }
    try {
        int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get("stock"));
        if (stock > 0) {
            int readStock = stock - 1;
            stringRedisTemplate.opsForValue().set("stock", realStock + ""); 
            System.out.println("扣减成功，剩余库存：" + readStock + "");
        } else {
            System.out.println("扣减失败，库存不足");
        }
    } finally {
        //业务执行完成，删除PRODUCT_ID key
        stringRedisTemplate.delete(PRODUCT_ID);
    }
    return "end";
}
```

但是上面代码是否正真解决问题了呢？单看代码本身是没什么问题的，但是前面提到，生产环境是非常复杂的。我们假设这种情况：当线程在成功加锁之后，执行业务代码时，还没来得及删除 Redis 中的锁标志，此时，这台服务器宕机了，程序并没有想我们想象中地去执行 finally 块中的代码。这种情况也会使得其他服务器或者进程在后续过程中无法去获取到锁，从而导致死锁，最终导致业务崩溃的情况。所以说，对于锁释放问题来说，try-finally 语句块在这里还不够，那么我们就需要新的方法来解决这个问题了。

## 6.Redis 超时机制

Redis 中允许我们设置缓存的自动过期时间，我们可以将其引入我们上面的锁机制中，这样就算 finally 语句块中的释放语句没有被正确执行，Redis 中的缓存也能在设定时间内自动过期，不会形成死锁：

```
Copy// 设置过期时间
stringRedisTemplate.expire(lockKey, 10, TimeUnit.SECONDS);
```

当然如果只是简单的在代码中加入上述语句的话，还是有可能产生死锁的，因为加锁以及设置过期时间是分开来执行的，并不能保证原子性。所以为了解决这个问题，Redis 中也提供了将设置值与设置过期时间合一的操作，对于 Java 代码如下：

```
Copy// 将设置值与设置过期时间合一
stringRedisTemplate.opsForValue().opsForValue().setIfAbsent(lockKey, "jojo", 10, TimeUnit.SECONDS);
```

到这一步，我们可以确保 Redis 中我们上的锁，最终无论如何都能成功地被释放掉，避免了造成死锁的情况。但是以当前的代码实现来看，在一些高并发场景下还是可能产生锁失效的情况。我们可以试想一下，上面代码我们设置的过期时间为 10s，那么如果这个进程在 10s 内并没有完成这段业务逻辑，会产生什么样的情况？不过在此之前我们先将代码的公共部分抽出作一个组件类，这样有助于我们关注锁的逻辑。

## 7.代码集成

## 8.公共方法的提取

我们这里先定义一个 RedisLock 接口，代码如下所示：

```
Copypublic interface RedisLock {
    /**
     * 尝试加锁
     */
    boolean tryLock(String key, long timeout, TimeUnit unit);
    /**
     * 解锁操作
     */
    void releaseLock(String key);
}
```

接下来，我们基于上面已经实现的分布式锁的思路，来实现这个接口，代码如果所示：

```
Copypublic class RedisLockImpl implements RedisLock {
    @Autowired
    private StringRedisTemplate stringRedisTemplate;
    @Override
    public boolean tryLock(String key, long timeout, TimeUnit unit) {
        return stringRedisTemplate.opsForValue().setIfAbsent(key, "jojo", timeout, unit);
    }
    @Override
    public void releaseLock(String key) {
        stringRedisTemplate.delete(key);
    }
}

```

## 9.加锁&解锁的归一化

我们先来继续分析上面代码。从开发的角度来说，当一个线程从上到下执行一个需要加分布式锁的业务时，它首先需要进行加锁操作，当业务执行完毕后，再进行释放锁的操作。也就是先调用 tryLock() 函数再调用 releaseLock() 函数。

但是真正可靠代码并不依靠人性，其他开发人员有可能在编写代码的时候并没有调用 tryLock() 方法，而是直接调用了 releaseLock() 方法，并且可能在调用 releaseLock() 时传入的 Key 值与你调用 tryLock() 时传入的 Key 值是相同的，那么此时就可能出现问题：另一段代码在运行时，硬生生将你代码中加的锁给释放掉了，那么此时的锁就失效了。所以上述代码依旧是有不可靠的地方，锁的可能误删操作会使得程序存在很严重的问题。

那么针对这一问题，我们就需要实现加锁&解锁的归一化。

首先我们解释一下什么叫做加锁和解锁的归一化，简单来说，就是一个线程执行了加锁操作后，后续的解锁操作只能由该线程来执行，即加锁操作和解锁只能由同一线程来进行。

这里我们使用 ThreadLocal 和 UUID 来实现，代码如下：

```
Copypublic class RedisLockImpl implements RedisLock {
    @Autowired
    private StringRedisTemplate stringRedisTemplate;
    private ThreadLock<string> threadLock = new ThreadLock<>();
    @Override
    public boolean tryLock(String key, long timeout, TimeUnit unit) {
        String uuid = UUID.randomUUID().toString();
        threadlocal.set(uuid);
        return stringRedisTemplate.opsForValue().setIfAbsent(key, uuid, timeout, unit);
    }
    @Override
    public void releaseLock(String key) {
        if (threadLocal.get().equals(stringRedisTemplate.opsForValue().get(key))) {
            stringRedisTemplate.delete(key);
        }
    }
}
```

## 10.可重入发布式锁实现

上面的代码实现，可以保证当一个线程成功在 Redis 中设置了锁标志位后，其他线程再设置锁标志位时，返回 false。但是在一些场景下我们需要实现线程的重入，即相同的线程能够多次获取同一把锁，不需要等待锁释放后再去加锁。所以我们需要利用一些方式来实现分布式锁的可重入型，在 JDK 1.6 之后提供的内存级锁很多都支持可重入型，比如 synchronized 和 J.U.C 下的 Lock，其本质都是一样的，比对已经获得锁的线程是否与当前线程相同，是则重入，当释放锁时则需要根据重入的次数，来判断此时锁是否真正释放掉了。那么我们就按照这个思路来实现一个可重入的分布式锁：

```
Copypublic class RedisLockImpl implements RedisLock {
    @Autowired
    private StringRedisTemplate stringRedisTemplate;
    private ThreadLocal<String> threadLocal = new ThreadLocal<String>();
    private ThreadLocal<Integer> threadLocalInteger = new ThreadLocal<Integer>();
    @Override
    public boolean tryLock(String key, long timeout, TimeUnit unit) {
        Boolean isLocked = false;
        if (threadLocal.get() == null) {
            String uuid = UUID.randomUUID().toString();
            threadLocal.set(uuid);
            isLocked = stringRedisTemplate.opsForValue().setIfAbsent(key, uuid, timeout, unit);
        } else {
            isLocked = true;
        }
        // 重入次数加1
        if (isLocked) {
            Integer count = threadLocalInteger.get() == null ? 0 : threadLocalInteger.get();
            threadLocalInteger.set(count++);
        }
        return isLocked;
    }
    @Override
    public void releaseLock(String key) {
        // 判断当前线程所对应的uuid是否与Redis对应的uuid相同，再执行删除锁操作
        if (threadLocal.get().equals(stringRedisTemplate.opsForValue().get(key))) {
            Integer count = threadLocalInteger.get();
            // 计数器减为0时才能释放锁
            if (count == null || --count <= 0) {
                stringRedisTemplate.delete(key);
            }
        }
    }
}
```

## 11.分布式自旋锁实现

上面代码实现中，加入我们不能一次性获取到锁，那么就会直接返回失败，这对业务来说是十分不友好的，假设用户此时下单，刚好有另外一个用户也在下单，而且获取到了锁资源，那么该用户尝试获取锁之后失败，就只能直接返回“下单失败”的提示信息的。所以我们需要实现以自旋的形式来获取到锁，即不停的重试，基于这个想法，实现代码如下：

```
Copypublic class RedisLockImpl implements RedisLock {
    @Autowired
    private StringRedisTemplate stringRedisTemplate;
    private ThreadLocal<String> threadLocal = new ThreadLocal<>();
    private ThreadLocal<Integer> threadLocalInteger = new ThreadLocal<>();
    @Override
    public boolean tryLock(String key, long timeout, TimeUnit unit) {
        Boolean isLocked = false;
        if (threadLocal.get() == null) {
            String uuid = UUID.randomUUID().toString();
            threadLocal.set(uuid);
            isLocked = stringRedisTemplate.opsForValue().setIfAbsent(key, uuid, timeout, unit);
            // 尝试获取锁失败，则自旋获取锁直至成功
            if (!isLocked) {
                for (;;) {
                    isLocked = stringRedisTemplate.opsForValue().setIfAbsent(key, uuid, timeout, unit);
                    if (isLocked) {
                        break;
                    }
                }
            }
        } else {
            isLocked = true;
        }
        // 重入次数加1
        if (isLocked) {
            Integer count = threadLocalInteger.get() == null ? 0 : threadLocalIntger.get();
            threadLocalInteger.set(count++);
        }
        return isLocked;
    }
    @Override
    public void releaseLock(String key) {
        // 判断当前线程所对应的uuid是否与Redis对应的uuid相同，再执行删除锁操作
        if (threadLocal.get().equals(stringRedisTemplate.opsForValue().get(key))) {
            Integer count = threadLocalInteger.get();
            // 计数器减为0时才能释放锁
            if (count == null || --count <= 0) {
                stringRedisTemplate.delete(key);
            }
        }
    }
}
```

## 12.基础优化

## 13.超时问题

在高并发场景下，一把锁可能会被 N 多的进程竞争，获取锁后的业务代码也可能十分复杂，其运行时间可能偶尔会超过我们设置的过期时间，那么这个时候锁就会自动释放，而其他的进程就有可能来争抢这把锁，而此时原来获得锁的进程也在同时运行，这就有可能导致超卖现象或者其他并发安全问题。

那么如何解决这个问题呢？思路很简单，就是每隔一段时间去检查当前线程是否还在运行，如果还在运行，那么就继续更新锁的占有时长，而在释放锁的时候。具体的实现稍微复杂些，这里给出简易的代码实现：

```
Copypublic class RedisLockImpl implements RedisLock {
    @Autowired
    private StringRedisTemplate stringRedisTemplate;
    private ThreadLocal<String> threadLocal = new ThreadLocal<>();
    private ThreadLocal<Integer> threadLocalInteger = new ThreadLocal<>();
    @Override
    public boolean tryLock(String key, long timeout, TimeUnit unit) {
        Boolean isLocked = false;
        if (threadLocal.get() == null) {
            String uuid = UUID.randomUUID().toString();
            threadLocal.set(uuid);
            isLocked = stringRedisTemplate.opsForValue().setIfAbsent(key, uuid, timeout, unit);
            // 尝试获取锁失败，则自旋获取锁直至成功
            if (!isLocked) {
                for (;;) {
                    isLocked = stringRedisTemplate.opsForValue().setIfAbsent(key, uuid, timeout, unit);
                    if (isLocked) {
                        break;
                    }
                }
            }
            // 启动新的线程来定期检查当前线程是否执行完成，并更新过期时间
            new Thread(new UpdateLockTimeoutTask(uuid, stringRedisTemplate, key)).start();
        } else {
            isLocked = true;
        }
        // 重入次数加1
        if (isLocked) {
            Integer count = threadLocalInteger.get() == null ? 0 :threadLocalInteger.get();
            threadLocalInteger.set(count++);
        }
        return isLocked;
    }
    @Override
    public void releaseLock(String key) {
        // 判断当前线程所对应的uuid是否与Redis对应的uuid相同，再执行删除锁操作
        if (threadLocal.get().equals(stringRedisTemplate.opsForValue().get(key))) {
            Integer count = threadLocalInteger.get();
            // 计数器减为0时才能释放锁
            if (count == null || --count <= 0) {
                stringRedisTemplate.delete(key);
                // 获取更新锁超时时间的线程并中断
                long threadId = stringRedisTemplate.opsForValue().get(uuid);
                Thread updateLockTimeoutThread = ThreadUtils.getThreadByThreadId(threadId);
                if (updateLockTimeoutThread != null) {
                    // 中断更新锁超时时间的线程
                    updateLockTimeoutThread.interrupt();
                    stringRedisTemplate.delete(uuid);
                }
            }
        }
    }
}
```

接下来我们就创建 UpdateLockTimeoutTask 类来执行更新锁超时的时间。

```
Copypublic class UpdateLockTimeoutTask implements Runnable {
    private long uuid;
    private String key;
    private StringRedisTemplate stringRedisTemplate;
    public UpdateLockTimeoutTask(long uuid, StringRedisTemplate stringRedisTemplate, String key) {
        this.uuid = uuid;
        this.key = key;
        this.stringRedisTemplate = stringRedisTemplate;
    }
    @Override
    public void run() {
        // 将以uuid为Key，当前线程Id为Value的键值对保存到Redis中
        stringRedisTemplate.opsForValue().set(uuid, Thread.currentThread().getId());
        // 定期更新锁的过期时间
        while (true) {
            stringRedisTemplate.expire(key, 10, TimeUnit.SECONDS);
            try{
                // 每隔3秒执行一次
                Thread.sleep(10000);
            }catch (InterruptedException e){
                e.printStackTrace();
            }
        }
    }
}
```

最后，我们定义一个 ThreadUtils 工具类，这个工具类中我们定义一个根据线程 id 获取线程的方法 getThreadByThreadId(long threadId)，代码如下所示：

```
Copypublic class ThreadUtils {
    // 根据线程 id 获取线程句柄
    public static Thread getThreadByThreadId(long threadId) {
        ThreadGroup group = Thread.currentThread().getThreadGroup();
        while(group != null){
            Thread[] threads = new Thread[(int)(group.activeCount() * 1.2)];
            int count = group.enumerate(threads, true);
            for (int i = 0; i < count; i++){
                if (threadId == threads[i].getId()) {
                    return threads[i];
                }
            }
        }
    }
}
```

上述解决分布式锁失效的方案在分布式锁领域有一个专业的术语叫做 **“异步续命”** 。需要注意的是：当业务代码执行完毕后，我们需要停止更新锁超时时间的线程。所以，这里，我对程序的改动是比较大的，首先，将更新锁超时的时间任务重新定义为一个 UpdateLockTimeoutTask 类，并将 uuid 和StringRedisTemplate 注入到任务类中，在执行定时更新锁超时时间时，首先将当前线程保存到Redis中，其中Key为传递进来的 uuid。

## 14.高并发

如果我们系统中利用 Redis 来实现分布式锁，而 Redis 的读写并发量约合 5 万左右。假设现在一个秒杀业务需要支持的并发量超过百万级别，那么如果这 100万的并发全部打入 Redis 中去请求锁资源，Redis 将会直接挂掉。所以我们现在应该来考虑如何解决这个问题，即如何在高并发的环境下保证 Redis 实现的分布式锁的可用性，接下来我们就来考虑一下这个问题。

> 在高并发的商城系统中，如果采用 Redis 缓存数据，则 Redis 缓存的并发能力是关键，因为很多的前缀操作都需要访问 Redis。而异步削峰只是基本操作，关键还是要保证 Redis 的并发处理能力。

解决这个问题的关键思想就是：分而治之，将商品库存分开放。

我们在 Redis 中存储商品的库存数量时，可以将商品的库存进行“分割”存储来提升 Redis 的读写并发量。

例如，原来的商品的 id 为 10001，库存为1000件，在Redis中的存储为(10001, 1000)，我们将原有的库存分割为5份，则每份的库存为200件，此时，我们在Redis 中存储的信息为(10001_0, 200)，(10001_1, 200)，(10001_2, 200)，(10001_3, 200)，(10001_4, 200)。

![img](https://pic1.zhimg.com/80/v2-850e6e2e3f618a411504afc939925790_720w.webp)

此时，我们将库存进行分割后，每个分割的库存使用商品 id 加上一个数字标识来存储，这样，在对存储商品库存的每个 key 进行 Hash 运算时，得出的 Hash 结果是不同的，这就说明，存储商品库存的 Key 有很大概率不在 Redis 的同一个槽位中，这就能够提升 Redis 处理请求的性能和并发量。

分割库存后，我们还需要在 Redis 中存储一份商品 ID 和 分割库存后的 Key 的映射关系，此时映射关系的 Key 为商品的 ID，也就是 10001，Value 为分割库存后存储库信息的 Key，也就是 10001_0，10001_1，10001_2，10001_3，10001_4。在 Redis 中我们可以使用 List 来存储这些值。

在真正处理库存信息时，我们可以先从 Redis 中查询出商品对应的分割库存后的所有 Key，同时使用 AtomicLong 来记录当前的请求数量，使用请求数量对从Redis 中查询出的商品对应的分割库存后的所有Key的长度进行求模运算，得出的结果为0，1，2，3，4。再在前面拼接上商品id就可以得出真正的库存缓存的Key。此时，就可以根据这个Key直接到Redis中获取相应的库存信息。

同时，我们可以将分隔的不同的库存数据分别存储到不同的 Redis 服务器中，进一步提升 Redis 的并发量。

## 15.基础升级

## 16.移花接木

> 在高并发业务场景中，我们可以直接使用 Lua 脚本库（OpenResty）从负载均衡层直接访问缓存。

这里，我们思考一个场景：如果在高并发业务场景中，商品被瞬间抢购一空。此时，用户再发起请求时，如果系统由负载均衡层请求应用层的各个服务，再由应用层的各个服务访问缓存和数据库，其实，本质上已经没有任何意义了，因为商品已经卖完了，再通过系统的应用层进行层层校验已经没有太多意义了！而应用层的并发访问量是以百为单位的，这又在一定程度上会降低系统的并发度。

为了解决这个问题，此时，我们可以在系统的负载均衡层取出用户发送请求时携带的用户Id，商品id和活动Id等信息，直接通过 Lua 脚本等技术来访问缓存中的库存信息。如果商品的库存小于或者等于 0，则直接返回商品已售完的提示信息，而不用再经过应用层的层层校验了。

## 17.数据同步

![img](https://pic3.zhimg.com/80/v2-80d1bdd6a9f01e798e41c5dd84422ee2_720w.webp)

假设我们使用 Redis 来实现分布式锁，我们知道 Redis 是基于 CAP 中 AP 来实现的，那么就可能存在数据未同步的问题。具体的场景就是，我在 Redis 的 Master 上设置了锁标志，然而在 Redis 的主从节点上还未完全同步之时，Redis 主节点宕机了，那么此时从节点上就没有锁标志，从而导致并发安全问题。对于这个问题，常见的解法有两种，基于 Zookeeper 来实现分布式锁（废话），而另外一种就是 RedLock 了。

Redlock 同很多的分布式算法一样，也使用“大多数机制”。加锁时，它会向过半节点发送 set(key, value, nx=True, ex=xxx) 指令，只要过半节点 set 成功，就认为加锁成功。释放锁时，需要向所有节点发送 del 指令。不过 Redlock 算法还需要考虑出错重试、时钟漂移等很多细节，同时因为 RedLock 需要向多个节点进行读写，意味着其相比单实例 Redis 的性能会下降一些。

如果你很在乎高可用性，希望即使挂了一台 Redis 也完全不受影响，就应该考虑 Redlock。不过代价也是有的，需要更多的 Redis 实例，性能也下降了，代码上还需要引入额外的 library，运维上也需要特殊对待，这些都是需要考虑的成本。

原文链接：https://zhuanlan.zhihu.com/p/368633034

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.214】Linux 的 IO 通信 以及 Reactor 线程模型详解

## 1.目录

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/11/202211291749277712803.png)
随着计算机硬件性能不断提高，服务器 CPU 的核数越来越越多，为了充分利用多核 CPU 的处理能力，提升系统的处理效率和并发性能，多线程并发编程越来越显得重要。无论是 C++ 还是 Java 编写的网络框架，大多数都是基于 Reactor 模式进行设计和开发，Reactor 模式基于事件驱动，特别适合处理海量的 I/O 事件，今天我们就简单聊聊 Reactor 线程模型，主要内容分为以下几个部分：

- 经典的 I/O 通信模型；
- Reactor 线程模型详述；
- Reactor 线程模型几种模式；
- Netty Reactor 线程模型的实践；

## 2.IO 通信模型

我们先要来谈谈 I/O 通信。说到 I/O 通信，往往会提到同步（synchronous）I/O 、异步（asynchronous）I/O、阻塞（blocking）I/O 和非阻塞（non-blocking）I/O 四种。有关同步、异步、阻塞和非阻塞的区别很多时候解释不清楚，不同的人知识背景不同，对概念很难达成共识。本文讨论的背景是 Linux 环境下的 Network I/O。

## 3.一次 I/O 过程分析

对于一次 Network I/O (以 read 举例)，它会涉及到两个系统对象，一个是调用这个 I/O 的进程或线程，另一个就是系统内核 (kernel)。当一个 read 操作发生时，会经历两个阶段（记住这两个阶段很重要，因为不同 I/O 模型的区别就是在两个阶段上各有不同的处理）:

- 第一个阶段：等待数据准备 (Waiting for the data to be ready)；
- 第二个阶段：将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)；

## 4.五种 I/O 模型

Richard Stevens 的《UNIX® Network Programming Volume》提到了 5 种 I/O 模型:

1. Blocking I/O (同步阻塞 I/O)
2. Nonblocking I/O（同步非阻塞 I/O）
3. I/O multiplexing（多路复用 I/O）
4. Signal driven I/O（信号驱动 I/O，实际很少用，Java 不支持）
5. Asynchronous I/O (异步 I/O)

接下来我们对这 5 种 I/O 模型进行说明和对比。

## 5.Blocking I/O

在 Linux 中，默认情况下所有的 Socket 都是 blocking 的，也就是阻塞的。一个典型的读操作时，流程如图：

![img](https://pic4.zhimg.com/80/v2-b17c6bb778a6dab5d77449ac84f247fb_720w.webp)

当用户进程调用了 recvfrom 这个系统调用, 这次 I/O 调用经历如下 2 个阶段:

1. 准备数据： 对于网络请求来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的 UDP 包），这个时候 kernel 就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞。
2. 数据返回：kernel 一但等到数据准备好了，它就会将数据从 kernel 中拷贝到用户内存，然后 kernel 返回结果，用户进程才解除 block 的状态，重新运行起来。

## 6.Nonblocking IO

Linux 下，可以通过设置 socket 使其变为 non-blocking，也就是非阻塞。当对一个 non-blocking socket 执行读操作时，流程如图：

![img](https://pic3.zhimg.com/80/v2-00c89f3e6088320a264a5b3e9c4b16ce_720w.webp)

当用户进程发出 read 操作具体过程分为如下 3 个过程：

1. 开始准备数据：如果 Kernel 中的数据还没有准备好，那么它并不会 block 用户进程，而是立刻返回一个 error。
2. 数据准备中： 从用户进程角度讲，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个 error 时，它就知道数据还没有准备好，于是它可以再次发送 read 操作（重复轮训）。
3. 一旦 kernel 中的数据准备好了，并且又再次收到了用户进程的 system call，那么它马上就将数据拷贝到了用户内存，然后返回。

## 7.I/O multiplexing

这种 I/O 方式也可称为 event driven I/O。Linux select/epoll 的好处就在于单个 process 就可以同时处理多个网络连接的 I/O。它的基本原理就是 select/epoll 会不断的轮询所负责的所有 socket，当某个 socket 有数据到达了，就通知用户进程。流程如图：

![img](https://pic4.zhimg.com/80/v2-30de18a6a658a61d2c1ff23410074e57_720w.webp)

当用户进程调用了 select:

1. 整个进程会被 block，与此同时kernel 会 “监视” 所有 select 负责的 socket，当任何一个 socket 中的数据准备好了，select 就会返回。
2. 户进程再调用 read 操作，将数据从 kernel 拷贝到用户进程。这时和 blocking I/O 的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个 system call (select 和 recvfrom)，而 blocking I/O 只调用了一个 system call (recvfrom)。
3. 在 I/O multiplexing Model 中，实际中，对于每一个 socket，一般都设置成为 non-blocking，但是，如上图所示，整个用户的 process 其实是一直被 block 的。只不过 process 是被 select 这个函数 block，而不是被 socket I/O 给 block。

## 8.Asynchronous IO

Linux 下的 asynchronous I/O，即异步 I/O，其实用得很少（需要高版本系统支持）。它的流程如图：

![img](https://pic3.zhimg.com/80/v2-52a73feffc9b135fd3c8fe758943749e_720w.webp)

当用户进程发出 read 操作具体过程：

1. 用户进程发起 read 操作之后，并不需要等待，而是马上就得到了一个结果，立刻就可以开始去做其它的事。
2. 从 kernel 的角度，当它受到一个 asynchronous read 之后，首先它会立刻返回，所以不会对用户进程产生任何 block。然后，kernel 会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个 signal，告诉它 read 操作完成了。

通过以上 4 种 I/O 通信模型的说明，总结一下它们各自的特点：

- Blocking I/O 的特点就是在 I/O 执行的两个阶段都被 block 了。
- Non-blocking I/O 特点是如果 kernel 数据没准备好不需要阻塞。
- I/O multiplexing 的优势在于它用 select 可以同时处理多个 connection。（如果处理的连接数不是很高的话，使用 select/epoll 的 web server 不一定比使用 multi-threading + blocking I/O 的 web server 性能更好，可能延迟还更大。select/epoll 的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）
- Asynchronous IO 的特点在于整个调用过程客户端没有任何 block 状态，但是需要高版本的系统支持。

## 9.生活中通信模型

以上五种 I/0 模型的介绍，比如枯燥，其实在生活中也存在类似的 “通信模型”，为了帮助理解，我们用生活中约妹纸吃饭这个不是很恰当的例子来说明这几个 I/O Model（假设我现在要用微信叫几个妹纸吃饭）:

- 发个微信问第一个妹纸好了没，妹子没回复就一直等，直到回复在发第二个 （blocking I/O）。
- 发个微信问第一个妹纸好了没，妹子没回复先不管，发给第二个，但是过会要继续问之前 没有回复的妹纸有没有好（nonblocking I/O）。
- 将所有妹纸拉一个微信群，过会在群里问一次，谁好了回复一下（I/O multiplexing）。
- 直接告诉妹纸吃饭的时间地址，好了自己去就行（Asynchronous I/O）。

## 10.Reactor 线程模型

## 11.Reactor 是什么？

**Reactor 是一种处理模式。** Reactor 模式是处理并发 I/O 比较常见的一种模式，用于同步 I/O，中心思想是将所有要处理的IO事件注册到一个中心 I/O 多路复用器上，同时主线程/进程阻塞在多路复用器上；一旦有 I/O 事件到来或是准备就绪(文件描述符或 socket 可读、写)，多路复用器返回并将事先注册的相应 I/O 事件分发到对应的处理器中。

**Reactor 也是一种实现机制。** Reactor 利用事件驱动机制实现，和普通函数调用的不同之处在于：应用程序不是主动的调用某个 API 完成处理，而是恰恰相反，Reactor 逆置了事件处理流程，应用程序需要提供相应的接口并注册到 Reactor 上，如果相应的事件发生，Reactor 将主动调用应用程序注册的接口，这些接口又称为 “回调函数”。用 “好莱坞原则” 来形容 Reactor 再合适不过了：不要打电话给我们，我们会打电话通知你。

## 12.为什么要使用 Reactor？

一般来说通过 I/O 复用，epoll 模式已经可以使服务器并发几十万连接的同时，维持极高 TPS，为什么还需要 Reactor 模式？原因是原生的 I/O 复用编程复杂性比较高。

一个个网络请求可能涉及到多个 I/O 请求，相比传统的单线程完整处理请求生命期的方法，I/O 复用在人的大脑思维中并不自然，因为，程序员编程中，处理请求 A 的时候，假定 A 请求必须经过多个 I/O 操作 A1-An（两次 IO 间可能间隔很长时间），每经过一次 I/O 操作，再调用 I/O 复用时，I/O 复用的调用返回里，非常可能不再有 A，而是返回了请求 B。即请求 A 会经常被请求 B 打断，处理请求 B 时，又被 C 打断。这种思维下，编程容易出错。

## 13.Reactor 线程模型

Reactor 有三种线程模型，用户能够更加自己的环境选择适当的模型。

1. 单线程模型
2. 多线程模型（单 Reactor）
3. 多线程模型（多 Reactor)

## 14.单线程模式

单线程模式是最简单的 Reactor 模型。Reactor 线程是个多面手，负责多路分离套接字，Accept 新连接，并分派请求到处理器链中。该模型适用于处理器链中业务处理组件能快速完成的场景。不过这种单线程模型不能充分利用多核资源，所以实际使用的不多。

![img](https://pic2.zhimg.com/80/v2-90a2d245c6a1f0b8c7f6d17b58133d49_720w.webp)

## 15.多线程模式(单 Reactor)

该模型在事件处理器（Handler）链部分采用了多线程（线程池），也是后端程序常用的模型。

![img](https://pic2.zhimg.com/80/v2-0b4c80242e24b735efaf2b62ee32401d_720w.webp)

## 16.多线程模式(多 Reactor)

比起多线程单 Rector 模型，它是将 Reactor 分成两部分，mainReactor 负责监听并 Accept新连接，然后将建立的 socket 通过多路复用器（Acceptor）分派给subReactor。subReactor 负责多路分离已连接的 socket，读写网络数据；业务处理功能，其交给 worker 线程池完成。通常，subReactor 个数上可与 CPU 个数等同。

![img](https://pic4.zhimg.com/80/v2-79ae041c0b16afdeda6407ddab671343_720w.webp)

## 17.Reactor 使用

软件领域很多开源的产品使用了 Ractor 模型，比如 Netty。

## 18.Netty Reactor 实践

## 19.服务端线程模型

服务端监听线程和 I/O 线程分离，类似于 Reactor 的多线程模型，它的工作原理图如下：

![img](https://pic4.zhimg.com/80/v2-35f737e207f05d07451a5f750663a827_720w.webp)

## 20.服务端用户线程创建

- 创建服务端的时候实例化了 2 个 EventLoopGroup。bossGroup 线程组实际就是 Acceptor 线程池，负责处理客户端的 TCP 连接请求。workerGroup 是真正负责 I/O 读写操作的线程组。通过这里能够知道 Netty 是多 Reactor 模型。
- ServerBootstrap 类是 Netty 用于启动 NIO 的辅助类，能够方便开发。通过 group 方法将线程组传递到 ServerBootstrap 中，设置 Channel 为 NioServerSocketChannel，接着设置 NioServerSocketChannel 的 TCP 参数，最后绑定 I/O 事件处理类 ChildChannelHandler。
- 辅助类完成配置之后调用 bind 方法绑定监听端口，Netty 返回 ChannelFuture，f.channel().closeFuture().sync() 对同步阻塞的获取结果。
- 调用线程组 shutdownGracefully 优雅推出，释放资源。

```
public class TimeServer {
    public void bind(int port) {
        // 配置服务端的NIO线程组
        EventLoopGroup bossGroup = new NioEventLoopGroup();
        EventLoopGroup workGroup = new NioEventLoopGroup();
        try {
            ServerBootstrap b = new ServerBootstrap();
            b.group(bossGroup, workGroup).channel(NioServerSocketChannel.class)
                    .option(ChannelOption.SO_BACKLOG, 1024)
                    .childHandler(new ChildChannelHandler());
            // 绑定端口，同步等待成功
            ChannelFuture f = b.bind(port).sync();
            // 等待服务端监听端口关闭
            f.channel().closeFuture().sync();
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            // 释放线程池资源
            bossGroup.shutdownGracefully();
            workGroup.shutdownGracefully();
        }
    }
    private class ChildChannelHandler extends ChannelInitializer<SocketChannel> {
        @Override
        protected void initChannel(SocketChannel ch) throws Exception {
            ch.pipeline().addLast(new TimeServerHandler());
        }
    }}
```

## 21.服务端 I/O 线程处理（TimeServerHandler）

- exceptionCaught 方法: 当 I/O 处理发生异常时被调用，关闭 ChannelHandlerContext，释放资源。
- channelRead 方法: 是真正处理读写数据的方法，通过 buf.readBytes 读取请求数据。通过 ctx.write(resp) 将相应报文发送给客户端。
- channelReadComplete 方法: 为了提高性能，Netty write 是将数据先写到缓冲数组，通过 flush 方法可以将缓冲数组的所有消息发送到 SocketChannel 中。

```
public class TimeServerHandler extends ChannelHandlerAdapter {
    @Override
    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause)
            throws Exception {
        ctx.close();
    }
    @Override
    public void channelRead(ChannelHandlerContext ctx, Object msg)
            throws Exception {
        // msg转Buf
        ByteBuf buf = (ByteBuf) msg;
        // 创建缓冲中字节数的字节数组
        byte[] req = new byte[buf.readableBytes()];
        // 写入数组
        buf.readBytes(req);
        String body = new String(req, "UTF-8");
        String currenTime = "QUERY TIME ORDER".equalsIgnoreCase(body) ? new Date(
                System.currentTimeMillis()).toString() : "BAD ORDER";
        // 将要返回的信息写入Buffer
        ByteBuf resp = Unpooled.copiedBuffer(currenTime.getBytes());
        // buffer写入通道
        ctx.write(resp);
    }
    @Override
    public void channelReadComplete(ChannelHandlerContext ctx) throws Exception {
        // write读入缓冲数组后通过invoke flush写入通道
        ctx.flush();
    }
}
```

## 22.总结

通过以上大概了解 Reactor 相关知识。最后做个总结一下使用 Reactor 模型的优缺点。

- 优点
- - 响应快，虽然 Reactor 本身依然是同步的，不必为单个同步时间所阻塞。
  - 编程相对简单，可以最大程度的避免复杂的多线程及同步问题，并且避免了多线程/进程的切换开销。
  - 可扩展性，通过并发编程的方式增加 Reactor 个数来充分利用 CPU 资源。
  - 可复用性，Reactor 框架本身与具体事件处理逻辑无关，具有很高的复用性。
- 缺点
- - 相比传统的简单模型，Reactor增加了一定的复杂性，因而有一定的门槛，调试相对复杂。
  - Reactor 模式需要底层的 Synchronous Event Demultiplexer 支持，例如 Java 中的 Selector，操作系统的 select 系统调用支持。
  - 单线程 Reactor 模式在 I/O 读写数据时还是在同一个线程中实现的，即使使用多 Reactor 机制的情况下，共享一个 Reactor 的 Channel 如果出现一个长时间的数据读写，会影响这个 Reactor 中其他 Channel 的相应时间，比如在大文件传输时，I/O 操作就会影响其他 Client 的相应时间，因而对这种操作，使用传统的 Thread-Per-Connection 或许是一个更好的选择，或则此时使用 Proactor 模式。

原文链接：https://zhuanlan.zhihu.com/p/370451460

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.215】MySQL数据库的性能的影响分析及优化

**MySQL数据库的性能的影响**

**一. 服务器的硬件的限制**

**二. 服务器所使用的操作系统**

**三. 服务器的所配置的参数设置不同**

四. 数据库存储引擎的选择

**五. 数据库的参数配置的不同**

六. (重点)数据库的结构的设计和SQL语句

## 1. 服务器的配置和设置(cpu和可用的内存的大小)

```
1.网络和I/O资源 
 2.cpu的主频和核心的数量的选择
 (对于密集型的应用应该优先考虑主频高的cpu)
 (对于并发量大的应用优先考虑的多核的cpu)
 3.磁盘的配置和选择
 (使用传统的机械硬盘:
    特点：读写较慢、存储空间大、最常见、使用最多、价格低;
    工作过程：移动磁头到磁盘表面上的正确位置;
             等待磁盘的旋转，使得所得所需的数据在磁头之下;
             等待磁盘旋转过去，所有所需的数据都被磁头读出
    选择因素：存储容量、传输速度、访问时间、主轴转速、物理尺寸)
 (使用RAID增强传统的机器硬盘的性能：
    特点：利用小的磁盘组成大的磁盘并提供数据的冗余保证数据的完整性的技术
    数据库中所使用的RAID的级别：
        RAID0级别、RAID1级别、RAID5级别[分布式奇偶校验磁盘阵列]、RAID10[分片的镜像(数据库最好的方式)]
    RAID级别选择：如下图)
 (使用固态存储的SSD和PCI-E卡:
    特点：相对于机械盘固态磁盘有更好的随机读写性能;
         相对于机械固态磁盘能更好的支持并发;
         相对于机械固态磁盘更容易损坏
    SSD:使用SATA接口可以替换传统的磁盘而不需要任何的改变[受到接口的速度的限制];
        SATA接口的SSD同样支持RAID技术
    PCI-E卡(Fusion-IO卡)：无法使用在SATA接口[需要使用独特的驱动和配置];
                         价格贵,使用了cpu的资源和内存
    使用的场景：适用于存在大量的随机I/O的场景;
              适用于解决单线程负载的I/O瓶颈)
 (使用网络存储NAS和SAN：
    SAN[光纤接入服务器]:大量顺序读写操作、读写I/O、缓存、I/O合并、随机读写慢（不如本地的RAID）
    NAS设备使用网络连接，基于文件的协议如NFS或者SMB来访问
    适合场景：数据库的备份、)
```

![img](https://pic1.zhimg.com/80/v2-f47b12fb0984a8948a5dce8b331f00fc_720w.webp)

![img](https://pic4.zhimg.com/80/v2-3e8d2315b134c440ea629b55ba10430b_720w.webp)

![img](https://pic2.zhimg.com/80/v2-37201284feb41d5712aa3af90920afb1_720w.webp)

![img](https://pic2.zhimg.com/80/v2-d3a0e6fbb6ae23bb4cdb088bb8943631_720w.webp)

## 2.不同REAID级别的对比：

![img](https://pic4.zhimg.com/80/v2-98cbee262c189a88ad72cb0c58b299cf_720w.webp)

注意事项：

```
1.64位数据库的版本使用32位的服务器的版本
 2.内存的主频的选择主板所能支持的最大内存的频率
```

## 3.总结：

```
对于cpu：
        1.64位的cpu一定能够要工作在64位的系统下
        2.对于并发比较高的场景cpu的数量比频率重要
        3.对于cpu密集型的场景和复杂SQL则频率越高越好
    对于内存：
        1.选择主板所能使用的最高频率的内存
        2.内存的大小对性能很重要，所以尽可能的大
    I/O子系统：
        1.PCIe -> SSD -> RAID10 -> 磁盘 -> SAN
```

## 4. 操作系统对性能的影响

```
Windows、FreeBSD、Solaris、Linux
centos的参数优化的设置：
    (1)内核相关的参数（/etc/sysctl.conf）
        net.core.somaxconn = 65535
        net.core.netdev_max_backlog = 65535
        net.ipv4.tcp_max_syn_backlog = 65535
        net.ipv4.tcp_fin_timeout = 10
        net.ipv4.tcp_tw_reuse = 1
        net.ipv4.tcp_tw_recycle = 1
        net.core.wmem_defaullt = 87380
        net.core.wmem_max = 16777216
        net.core.rmem_defaullt = 87380
        net.core.rmem_max = 16777216
        net.ipv4.tcp_keepalive_time = 120
        net.ipv4.tcp_keepalive_intvl = 30
        net.ipv4.tcp_keepalive_probes = 3
        kernel.shmmax = 4294967295
        vm.swappiness = 0
    (2)增加资源限制（/etc/security/limit.conf）
        * soft nofile 65535
        * hard nofile 65535
            * 表示对所有的用户有效
            soft 指的是当前系统的生效的设置
            hard 表明系统中所能设定的最大值
            nofile 表示所限制的资源是打开文件的最大数目
            65535 就是限制的数量
    (3).磁盘调度策略(/sys/block/devname/queue/scheduler)
        noop(电梯式调度策略）、deadline（截止时间调度策略）、anticipatory(预料I/O调度策略)
        cat /sys/block/sda/queue/scheduler
        noop anticipatory deadline [cfq]
            echo deadline > /sys/block/sda/queue/scheduler
```

## 5.MySQl的数据库的体系

![img](https://pic2.zhimg.com/80/v2-c8de4414bb8e3f61f9611dcf90dab3a9_720w.webp)

## 6.MySQl的数据库的存储引擎

```
(1).Mysql之存储引擎MyISAM
    组成的结构：表为MYD和MYI、frm的文件组成
    特性：并发性和锁级别
         MyISAM表支持索引类型
         MyISAM表支持数据的压缩(命令行：myisampack)
             myisampack -b -f myIsam.MYI;
            压缩后的表不能进行写操作，只能进行读操作
    修复:对数据库中的表进行检查并修复:
        check table mytable;
        repair table mytable;
        myisamchk工具,修复时数据库服务必须停止
    限制：使用MySQL5.0之前时默认表的大小4G(存储大表修改MAX_Rows和AVG_ROW_LENGTH)
         使用MySQL5.0之后的版本默认支持256TB
    适用的场景:非事务型的应用
              只读类的应用
              空间类的应用(GPS的数据)
(2).Mysql之存储引擎InnoDB
    mysql5.5.8之后版本默认使用的存储引擎
    组成结构：通过设置innodb_file_per_table参数存储的位置不同
                ON:独立表空间：tablename.ibd
                OFF:系统表空间:ibdataX
    建议：对于mysql中建议使用InnoDB的独立表空间
    特性：事务性存储引擎
         完全支持事务的存储引擎
         Redo log(存储已经提交的事务)和Undo log(存储未提交的事务)
         InnoDB支持行级别锁
         最大程序的支持并发
         行级别的锁是由存储引擎层实现的
    锁：共享锁(读锁)、独占锁(写锁)
         表级锁、行级锁
        阻塞：确保事务并发的正常的执行
        死锁：两个或者两个以上的事务执行过程中相互等待对方的资源而产生的一种异常
    InnoDB状态检查：
        show engine innodb status;    
    适用场景：InooDB适用于大多数OLTP应用
(3).Mysql之存储引擎CSV
    特点：数据以文本的方式存储在文件中
        .CSV文件存储表的内容
        .CSM文件存储表的元数据如表的状态和数据量
        .frm文件存储表的结构的信息
        以CSV格式进行数据的存储
        所有的列必须不能为NULL的
        不支持索引(不适合大表，不适合在线处理)
        可以对数据文件直接进行编辑
    适用的场景：适合作为数据交换的中间表
              mysql数据目录->csv文件->其他web程序
              excel电子表格 -> csv文件 -> mysql数据目录
(4).Mysql之存储引擎Archive
    特点：以zlib对表数据进行压缩，磁盘I/O更少
         数据存储在ARZ为后缀的文件中
         只支持insert和select操作
         只支持在自增的ID列上加索引
    适用场景：
         日志和数据采集类的应用
(4).Mysql之存储引擎Memory
    特点：数据只保存在内存中
         Memory存储引擎的I/O效率特别高
         支持HASH索引和BTree索引
         所有的字段为固定长度
         不支持BLOG和TEXT等大字段
         Memory存储引擎使用表级锁
         表中存储数据的最大值由max_heap_table_size参数决定
    适用场景：用于查找或者映射表，例如邮编和地区
             用于保存数据分析产生的中间表
             用于缓存周期性聚合数据的结果表
```

## 7.MySQl的数据库的服务器参数

```
(1).Mysql配置参数作用域
    全局参数 
        set global 参数名=参数值；
        set @@global.参数名：=参数值；
    会话参数
        set[session] 参数名=参数值；
        set @@session.参数名：=参数值；
(2).内存配置相关的参数
        确定可以使用的内存的上限
        确定MySQL的每个连接使用的内存
            sort_buffer_size join_buffer_size
            read_buffer_size read_rnd_buffer_size
        确定需要为操作系统保留多少内存
        如何为缓存池分配内存
            Innodb_buffer_pool_size
            总内存-（每个线程锁需要的内存*连接数）- 系统的保留内存
            key_buffer_size
(3).I/O相关配置参数
        InnoDb存储引擎的I/O参数设置：
        Innodb_log_file_size
        Innodb_log_file_in_group
        Innodb_log_buffer_size
        Innodb_flush_log_at_trx_commit
        Innodb_flush_method = O_DIRECT
        Innodb_file_per_table = 1
        Innodb_doublewrite = 1
      MySIAM存储引擎的I/O参数设置：
        delay_key_write
            OFF:每次操作后刷新键缓冲中的脏块到磁盘
            ON:只对在键表时指定了delay_key_write选项的表使用延迟刷新
            ALL:对所有MYSIAM表都使用延迟键写入
(4).安全相关配置参数
        expire_logs_days 指定自动清理binlog的天数
        max_allowed_packet 控制MySQL可以接受的包的大小（32M）
        skip_name_resolve 禁用DNS查找
        sysdate_is_now 确保sysdate()返回确定性的日期
        read_only 禁止非super权限的用户写权限
        skip_slave_start 禁止Slave自动恢复
        sql_mode 设置MySQL所使用的SQL模式
            strict_trans_tables
            no_engine_subtitutoion
            no_zero_date
            no_zero_in_date
            only_full_group_by
(5).其他相关配置参数
        sync_binlog = 1控制MySQL如何向磁盘刷新binlog
        tmp_table_size和max_heap_table_size 控制内存临时表的大小(设置一致)
        max_connections = 2000 控制允许的最大连接数
```

## 8.MySQl的数据库的结构设计和SQL的优化

```
(1).过分的反范式化为表的建立太多的列
(2).过分的范式化造成太多的表关联
(3).在OLTP环境中使用不恰当的分区表
(4).使用外键保证数据的完整性
```

## 9.性能优化的顺序

- 数据库结构设计和SQL语句优化
- 数据库的存储引擎的选择和参数的配置
- 系统的选择及其优化
- 硬件升级

原文链接：https://zhuanlan.zhihu.com/p/371524818

作者：Hu先生的Linux



# 【NO.216】Nginx-接入层Nginx架构及模块介绍

文章简介：

**1）帮助大家对Nginx有一定的认识**
**2）熟悉Nginx有哪些应用场景**
**3）熟悉Nginx特点和架构模型以及相关流程**
**4）熟悉Nginx定制化开发的几种模块分类**

## 1. Nginx简介以及特点

**Nginx简介:**

Nginx (engine x) 是一个高性能的web服务器和反向代理服务器，也是一个IMAP/POP3/SMTP服务器

- 俄罗斯程序员Igor Sysoev于2002年开始
- Nginx是增长最快的Web服务器，市场份额已达33.3％
- 全球使用量排名第二2011年成立商业公司

**Nginx社区分支：**

- Openresty作者 [@agentzh](https://github.com/agentzh)(章宜春)开发的，最大特点是引入了ngx_lua模块，支持使用lua开发插件，并且集合了很多丰富的模块，以及lua库。
- Tengine主要是淘宝团队开发。特点是融入了因淘宝自身的一些业务带来的新功能。
- Nginx官方版本，更新迭代比较快，并且提供免费版本和商业版本。

**Nginx源码结构：**

- 代码量大约11万行C代码
- 源代码目录结构
- - core （主干和基础设置）
  - event （事件驱动模型和不同的IO复用模块）
  - http （HTTP服务器和模块）
  - mail （邮件代理服务器和模块）
  - os （操作系统相关的实现）
  - misc （杂项）

**Nginx特点：**

- 反向代理，负载均衡器
- 高可靠性、单master多worker模式
- 高可扩展性、高度模块化
- 非阻塞
- 事件驱动
- 低内存消耗
- 热部署

## 2. Nginx应用场景

**场景如下：**

- 静态文件服务器
- 反向代理，负载均衡
- 安全防御
- 智能路由(企业级灰度测试、地图POI一键切流)
- 灰度发布
- 静态化
- 消息推送
- 图片实时压缩
- 防盗链

## 3. Nginx框架模型介绍

**进程组件角色：**

- master进程
- - 监视工作进程的状态
  - 当工作进程死掉后重启一个新的
  - 处理信号和通知工作进程
- worker进程
- - 处理客户端请求
  - 从主进程处获得信号做相应的事情
- cache loader进程
- - 加载缓存索引文件信息，然后退出
- cache manager进程
- - 管理磁盘的缓存大小，超过预定值大小后最少使用数据将被删除

**框架模型：**

![img](https://pic1.zhimg.com/80/v2-6cc21dc22cb69f589b2331c5a708a338_720w.webp)

**框架模型流程：**

![img](https://pic1.zhimg.com/80/v2-f635e3ab2d216e1d52fcad678aeb1634_720w.webp)

## 4. Nginx内部流程介绍

### 4.1. 框架模型流程

![img](https://pic4.zhimg.com/80/v2-cfb33c24986d001c289d94232c76bcef_720w.webp)

![img](https://pic2.zhimg.com/80/v2-378f87b6eb04da3cbe84cf417e6980e1_720w.webp)

### 4.2. master初始化流程

![img](https://pic2.zhimg.com/80/v2-f9ca34cf514787041ab71c592b79f2c5_720w.webp)

### 4.3. worker初始化

![img](https://pic3.zhimg.com/80/v2-ae7b6abff4be9b31f39e5699d510f8c2_720w.webp)

### 4.4. worker初始化流程

![img](https://pic1.zhimg.com/80/v2-1a1be68c874107712f98573318ed69fc_720w.webp)

### 4.5. 静态文件请求IO流程

![img](https://pic2.zhimg.com/80/v2-5d685acdb3bec4588db1c3efcefe4601_720w.webp)

### 4.6. http请求流程

![img](https://pic1.zhimg.com/80/v2-2f20e35650f447c7b68bbf461e1225d4_720w.webp)

### 4.7. http请求11个阶段

![img](https://pic4.zhimg.com/80/v2-2a1db7e6ed3b2f1fb48a7382a6966ac7_720w.webp)

### 4.8. upstream模块

- 访问第三方Server服务器
- 底层HTTP通信非常完善
- 异步非阻塞
- 上下游内存零拷贝，节省内存
- 支持自定义模块开发

#### 4.8.1 upstream框架流程

![img](https://pic1.zhimg.com/80/v2-16827e0fbdc6f621925c08f7a0f6e210_720w.webp)

#### 4.8.2 upstream内部流程

![img](https://pic2.zhimg.com/80/v2-735b470e9498c78f2b893f057c2127dd_720w.webp)

### 4.9 反向代理流程

![img](https://pic2.zhimg.com/80/v2-4512a75c9866c2fca26c2ca215a9d039_720w.webp)

## 5. Nginx定制化模块开发

### 5.1. Nginx的模块化设计特点

- 高度抽象的模块接口
- 模块接口非常简单，具有很高的灵活性
- 配置模块的设计
- 核心模块接口的简单化
- 多层次、多类别的模块设计

### 5.2. 内部核心模块

![img](https://pic4.zhimg.com/80/v2-0a2f149846bb6d463e70fb0c74ea3e97_720w.webp)

![img](https://pic1.zhimg.com/80/v2-9fe9c202c6c489c6a742176fa666d144_720w.webp)

### 5.3. handler模块

- 接受来自客户端的请求并构建响应头和响应体。

![img](https://pic4.zhimg.com/80/v2-d9e02f5fde12fedd98dde9d8a4d6d17b_720w.webp)

### 5.4. filter模块

- 过滤（filter）模块是过滤响应头和内容的模块，可以对回复的头和内容进行处理。它的处理时间在获取回复内容之后，向用户发送响应之前。

![img](https://pic3.zhimg.com/80/v2-4973e0693ac310c6feac07640d6fc746_720w.webp)

### 5.5. upstream模块

- 使nginx跨越单机的限制，完成网络数据的接收、处理和转发，纯异步的访问后端服务。

![img](https://pic4.zhimg.com/80/v2-72fbc635533e1a898b568cf95687e923_720w.webp)

**load_balance：**

- 负载均衡模块，实现特定的算法，在众多的后端服务器中，选择一个服务器出来作为某个请求的转发服务器。

![img](https://pic4.zhimg.com/80/v2-36296ebc2f69ba725d531ecca66776c7_720w.webp)

### 5.6. ngx_lua模块

- 脚本语言
- 内存开销小
- 运行速度快
- 强大的 Lua 协程
- 非阻塞
- 业务逻辑以自然逻辑书写

![img](https://pic2.zhimg.com/80/v2-7e33a6c32db06083ed320598c4b2887d_720w.webp)

### 5.7. 定制化开发Demo

**Handler模块：**

- 编写config文件
- 编写模块产生内容响应信息

1. \#配置文件：
2. server {
3. …
4. location test {
5. test_counter on;
6. }
7. }
8. \#config
9. ngx_addon_name=ngx_http_test_module
10. HTTP_MODULES=”$HTTP_MODULES ngx_http_test_module”
11. NGX_ADDON_SRCS=”$NGX_ADDON_SRCS $ngx_addon_dir/ngx_http_test_module.c”
12. \#ngx_http_test_module.c
13. staticngx_int_t
14. ngx_http_test_handler(ngx_http_request_t*r)
15. {
16. ngx_int_t rc;
17. ngx_buf_t*b;
18. ngx_chain_tout;
19. ngx_http_test_conf_t*lrcf;
20. ngx_str_t ngx_test_string = ngx_string(“hello test”);
21. lrcf = ngx_http_get_module_loc_conf(r, ngx_http_test_module);
22. if( lrcf->test_counter ==0){
23. return NGX_DECLINED;
24. }
25. /* we response to ‘GET’ and ‘HEAD’ requests only */
26. if(!(r->method &(NGX_HTTP_GET|NGX_HTTP_HEAD))){
27. return NGX_HTTP_NOT_ALLOWED;
28. }
29. /* discard request body, since we don’t need it here */
30. rc = ngx_http_discard_request_body(r);
31. if( rc != NGX_OK ){
32. return rc;
33. }
34. /* set the ‘Content-type’ header */
35. /*
36. *r->headers_out.content_type.len = sizeof(“text/html”) - 1;
37. *r->headers_out.content_type.data = (u_char *)”text/html”;
38. */
39. ngx_str_set(&r->headers_out.content_type,”text/html”);
40. /* send the header only, if the request type is http ‘HEAD’ */
41. if( r->method == NGX_HTTP_HEAD ){
42. r->headers_out.status = NGX_HTTP_OK;
43. r->headers_out.content_length_n = ngx_test_string.len;
44. return ngx_http_send_header(r);
45. }
46. /* set the status line */
47. r->headers_out.status = NGX_HTTP_OK;
48. r->headers_out.content_length_n = ngx_test_string.len;
49. /* send the headers of your response */
50. rc = ngx_http_send_header(r);
51. if( rc == NGX_ERROR || rc > NGX_OK || r->header_only ){
52. return rc;
53. }
54. /* allocate a buffer for your response body */
55. b = ngx_pcalloc(r->pool,sizeof(ngx_buf_t));
56. if( b == NULL ){
57. return NGX_HTTP_INTERNAL_SERVER_ERROR;
58. }
59. /* attach this buffer to the buffer chain */
60. out.buf = b;
61. out.next= NULL;
62. /* adjust the pointers of the buffer */
63. b->pos = ngx_test_string.data;
64. b->last= ngx_test_string.data + ngx_test_string.len;
65. b->memory =1;/* this buffer is in memory */
66. b->last_buf =1;/* this is the last buffer in the buffer chain */
67. /* send the buffer chain of your response */
68. return ngx_http_output_filter(r,&out);
69. }

## 6. Nginx核心时间点模块介绍

解决接入层故障定位慢的问题，帮助OP快速判定问题根因，优先自证清白，提高接入层高效的生产力。

![img](https://pic2.zhimg.com/80/v2-891074a145f92b5d0c367823ca619681_720w.webp)

## 7. Nginx分流模块介绍

**特点：**
实现非常灵活的动态的修改策略从而进行切流量。
实现平滑无损的方式进行流量的切换。
通过秒级切换流量可以缩小影响范围，从而减少损失。
按照某一城市或者某个特征，秒级进行切换流量或者禁用流量。
容忍单机房级别容量故障，缩短了单机房故障的止损时间。
快速的将流量隔离或者流量抽样。
高效的灰度测试，提高生产力。

![img](https://pic2.zhimg.com/80/v2-1049b286fa13f53b4bbf814dfe406419_720w.webp)

## 8. Nginx动态upstream模块介绍

让接入层可以适配动态调度的云环境，实现服务的平滑上下线、弹性扩/缩容。
从而提高接入层高效的生产力以及稳定性，保证业务流量的平滑无损。

![img](https://pic2.zhimg.com/80/v2-dddc164ce367d6cf0733e98db42c5249_720w.webp)

## 9. Nginx query_upstream模块介绍

链路追踪，梳理接口到后端链路的情况。查询location接口对应upstream server信息。

![img](https://pic2.zhimg.com/80/v2-121ef7f1c9c7a8fc8d87d4c5a43547d1_720w.webp)

## 10. Nginx query_conf模块介绍

获取nginx配置文件格式化为json格式信息。

![img](https://pic2.zhimg.com/80/v2-87dbc0cf71ae43ec8656726cb0e715a1_720w.webp)

## 11.Nginx 共享内存支持redis协议模块介绍

根据配置文件来动态的添加共享内存。
[http://github.com/lidaohang/n](https://link.zhihu.com/?target=http%3A//github.com/lidaohang/n)…

- ngx_shm_dict
  共享内存核心模块(红黑树，队列)
- ngx_shm_dict_manager
  添加定时器事件，定时的清除共享内存中过期的key
  添加读事件，支持redis协议，通过redis-cli get,set,del,ttl
- ngx_shm_dict_view
  共享内存查看

![img](https://pic1.zhimg.com/80/v2-95e9acb1c6d279e3ad2c97a354558d4c_720w.webp)

## 12. Nginx日志回放压测工具

- 解析日志进行回放压测，模拟后端服务器慢等各种异常情况
  [http://github.com/lidaohang/p](https://link.zhihu.com/?target=http%3A//github.com/lidaohang/p)…

## 13.方案说明

- 客户端解析access.log构建请求的host,port,url,body
- 把后端响应时间，后端响应状态码，后端响应大小放入header头中
- 后端服务器获取相应的header，进行模拟响应body大小，响应状态码，响应时间

## 14.使用方式

- 拷贝需要测试的access.log的日志到logs文件夹里面
- 搭建需要测试的nginx服务器，并且配置upstream 指向后端服务器断端口
- 启动后端服务器实例 server/backserver/main.go
- 进行压测 bin/wrk -c30 -t1 -s conf/nginx_log.lua [http://localhost:8095](http://localhost:8095/)

原文链接：https://zhuanlan.zhihu.com/p/372403172

作者：Hu先生的Linux

# 【NO.217】深入Linux C/C++ Timer定时器的实现核心原理

我曾以为像定时器这样基础的功能，操作系统会有一个完备的实现。当需要开启一个定时任务的时候，会有一个优雅的、如下形式的接口：

```
typedef void (*callback)(void*);
void setTimeout(unsigned int second,callback cb，void* arg);
```

可是事与愿违，Linux下不存在这样的接口。

## 1.定时器的实现原理

定时器的实现依赖的是CPU时钟中断，时钟中断的精度就决定定时器精度的极限。一个时钟中断源如何实现多个定时器呢？对于内核，简单来说就是用特定的数据结构管理众多的定时器，在时钟中断处理中判断哪些定时器超时，然后执行超时处理动作。而用户空间程序不直接感知CPU时钟中断，通过感知内核的信号、IO事件、调度，间接依赖时钟中断。用软件来实现动态定时器常用数据结构有：时间轮、最小堆和红黑树。下面就是一些知名的实现：

> Hierarchy 时间轮算法：Linux内核
> 红黑树最小堆算法：Asio C++ Library或nginx

## 2.Linux上的定时函数

要想使用上面那样的定时器功能，我们必须利用Linux上现有的定时通知函数，封装一个定时器。Linux上的定时通知函数五花八门，要封装我们自己的定时器，首先需要选用一个定时通知的函数。查阅资料整理出了Linux上所有的定时函数，如下表：

|      Function      |       Type        |  Precision  |                            Remark                            |
| :----------------: | :---------------: | :---------: | :----------------------------------------------------------: |
|      sleep(3)      |   unsigned int    |   second    |                                                              |
|     usleep(3)      |    useconds_t     | microsecond |                                                              |
|    nanosleep(2)    |  struct timespec  | nanosecond  |                                                              |
| clock_nanosleep(2) |  struct timespec  | nanosecond  | It differs in allowing the caller to select the clock against which the sleep interval is to be measured, and in allowingthe sleep interval to be specified as either an absolute or a relative value. |
|      alarm(2)      |   unsigned int    |   second    |                           SIGALRM                            |
|    setitimer(2)    | struct itimerval  | microsecond |                           SIGALRM                            |
|  timer_settime(2)  | struct itimerspec | nanosecond  |               notify method : struct sigevent                |
|    Timerfd API     |  File descriptor  | nanosecond  |                   From linux kernel 2.6.25                   |

前四个函数比较鸡肋，会让调用线程挂起，原地等待定时器超时，否定。

alarm()和setitimer()，它们的通知机制采用了信号SIGALRM，由于SIGALRM信号不可靠，会造成超时通知不可靠，而且多线程中处理信号也是一个麻烦事，也不考虑。

timer_create()/timer_settime()系列函数是POSIX规定，精度达到纳秒级，提供了一个数据结构struct sigevent可以指定一个实时信号作为通知信号，同时也可以设置线程ID，将信号传递到指定的线程。相比前两个函数，有了不小的改进，可以作为一个备选的实现，但是可以预见到封装起来不会很轻松。此外使用此系列的函数，需要链接librt库。

事实上，我们遗漏掉了几个同样具有定时的功能的API——多路复用。在Linux上的多路复用机制有select/poll/epoll几种，它们轮询时都允许指定一个超时时间，如果在指定时间内，监控的事件没有到达，轮询函数会超时返回。精度也足够用，poll/epoll是毫秒级的(millisecond),select超时参数是struct timeval，是微秒级的(microsecond)。

选择epoll的优势很明显，能将定时功能完美的融入已有的event loop里，同时epoll有着天然的高并发的能力，millisecond级的精度也足够用。

## 3.获取当前时间

要实现一个定时器，有了定时函数，我们还需要选用一个获取时间的函数。同样地，这些函数我也整理了一下：

|      Function      |      Type       |  Precision  |                    Remark                    |
| :----------------: | :-------------: | :---------: | :------------------------------------------: |
|      time(2)       |     time_t      |   second    |                                              |
|      ftime(3)      |  struct timeb   | millisecond |                   obsolete                   |
|  gettimeofday(2)   | struct timeval  | microsecond |                                              |
|  clock_gettime(2)  | struct timespec | nanosecond  |                                              |
| Time Stamp Counter | 64-bit register | CPU related | on all x86 processors since the Pentium(TSC) |

time()精度太低，不合适。

ftime() 毫秒级精度，但是被废弃了，也不合适。

gettimeofday() 精度达到微秒级，并且在x86-64平台上该函数的调用不是系统调用(vdso)，似乎很合适，不幸的是POSIX.1-2008中也将这个函数废弃了。

Time Stamp Counter 使用汇编指定获取时间戳的计数器，精度应该是最高的，效率*可能*也应该是最高的，一条汇编指令rdtscp(相比rdtsc，rdtscp可以避免，因为cpu乱序执行带来的误差问题)即可。是可以作为一个选择的，腾讯的libco就是优先使用这个方法获取时间的。

clock_gettime() 。默认是nanosecond 级精度，是系统调用(*sys*clock_gettime())，会有开销。调用频繁的话，可能造成损失性能。但是Linux 2.6.32后可以指定参数CLOCK_REALTIME_COARSE和CLOCK_MONOTONIC_COARSE，粗粒度地获取时间，而不需要发生上下文切换(和gettimeofday()一样也是vdso技术，[https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/reference_guide/sect-posix_clocks#CLOCK_MONOTONIC_COARSE_and_CLOCK_REALTIME_COARSE](https://link.zhihu.com/?target=https%3A//access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/reference_guide/sect-posix_clocks%23CLOCK_MONOTONIC_COARSE_and_CLOCK_REALTIME_COARSE))。使用_COARSE后缀获取的时间，精度是millisecond级。对我们来说也够用了。

要对clock_gettime系统调用的开销有一个直观的感受的话可以，借助于strace工具，利用-T参数可以追踪每个系统调用的时间开销，比我我的环境上CLOCK_MONOTONIC获取时间的开销大概是50微秒

![img](https://pic2.zhimg.com/80/v2-6f4ae6c2ec731d3ed3b3788247331361_720w.webp)

换成CLOCK_MONOTONIC_COARSE方式再去获取时间，用strace就追踪不到了。

## 4.定时器的设计

有了获取时间函数clock_gettime和定时函数epoll之后，我们就可以开始设计定时器了。首先明确一点，epoll和其他的定时通知函数一样，一次也只能设置一个超时时间，依然不能满足我开篇提出的需求。

主流的做法是利用一个容器保存所有设置的超时时间，将容器里最快的超时的时间设置为epoll_wait的超时时间。比如，我先后设置了1400ms,800ms,300ms,2900ms，那么下一次事件循环就将epoll_wait的第四个参数设置为300ms。

如果用链表保存的话，每次设置定时器都要遍历一遍链表才能选到最快超时的那个时间，复杂度太高，如果设置了定时器特别多的话，这样的开销不能接受。

要像O(1)的时间获取到最小的哪个值，用最小堆保存超时时间正合适，效率大大提高。事实上libevent就是这么实现的(C语言实现的min_heap_t)。

## 5.最小堆实现

先实现一个类Timer表示每一个被添加的定时，构造时需要一个millisecond为单位的超时时间，一个回调函数，一个回调函数的参数。为了简化实现，我测试用的超时的回调函数，并未使用回调函数的参数，但也没有去掉，仅仅是占个坑的作用。本来是想打算把args抽象，将Timer写成模板类，防止本末倒置，本文仅为演示定时器的实现，越简单越好。

expire时间用的是相对系统启动的时间,是一个不可以设置的恒定的时间(nonsettable monotonic clock)，而不是用的真实的时间(Wall time ，墙上时间)，因为这个时间可能会随着设置系统的日期时间而发生跳跃性的改变。

```
class Timer
{
public:
    Timer(unsigned long long expire, std::function<void(void)> fun, void *args)
        : expire_(expire), fun(fun){    }
    inline void active() { fun(); }
    inline unsigned long long getExpire() const{ return expire_; }
private:
    std::function<void(void)> fun;
    void *args;
    unsigned long long expire_;
};
```

TimerManager是用户操作的接口，提供增加，删除定时器的功能。STL中提供能优先队列，直接可以拿来用。

```
class TimerManager
{
public:
    TimerManager() {}
    Timer *addTimer(int timeout, std::function<void(void)> fun, void *args = NULL);
    void delTimer(Timer* timer);
    unsigned long long getRecentTimeout();
    void takeAllTimeout();
    unsigned long long getCurrentMillisecs();
private:
    struct cmp
    {
        bool operator()(Timer*& lhs, Timer*& rhs) const { return lhs->getExpire() > rhs->getExpire(); }
    };
    std::priority_queue<Timer*,std::vector<Timer*>,cmp> queue_;
};
```

add
Timer()参数和Timer构造函数一直，实现就是构造一个Timer然后加入到std::priority_queue后，返回Timer指针。

delTimer() 删除一个指定的Timer，由于priority_queue没有提供erease()接口，因此删除Timer的操作，我这里采用了新建一个priority_queue的做法，复杂度O(n)。

getRecentTimeout()获取一个最近的超时时间(超时时间 = 优先队列里的时间 - 当前获取的系统启动时间)。如果这个值小于0，那么说明这个定时器已经超时了，将其置为0，稍后的epoll_wait将会立马返回。

takeAllTimeout() 函数，处理超时的定时，并回调其绑定的回调函数。由于超时的可能不止一个定时，需要用一个循环遍历所有超时的Timer，一一处理。

getCurrentMillisecs()对clock_gettime()的封装，获取到的struct timespec转换为millisecond。

这两个类的完整实现，我放到了Github上了:[https://gist.github.com/baixiangcpp/b2199f1f1c7108f22f47d2ca617f6960](https://link.zhihu.com/?target=https%3A//gist.github.com/baixiangcpp/b2199f1f1c7108f22f47d2ca617f6960)。使用的时候，只需要在你的主循环里，把epoll_wait的超时参数设置为TimerManager::getRecentTimeout()，每次epoll_wait()返回后，处理一下超时事件TimerManager::takeAllTimeout()。

使用示例：

```
int dispatch()
{
    ...
    TimerManager tm;
    tm.addTimer(1000, []() { std::cout << "hello world" << std::endl; }, NULL);
    tm.addTimer(5000, []() { std::cout << "hello baixiancpp" << std::endl; }, NULL);
    for(;;)
    {
        int ret = epoll_wait(epollfd,events,events_num,tm.getRecentTimeout());
        tm.takeAllTimeout();
    }
    ...
}
```

## 6.时间轮实现

另外一种常见的定时器设计使用的存放超时时间的容器叫做”时间轮”。微信的开源项目libco中使用的就是这种数据结构。

Hierarchy 时间轮的原理大致如下，下面是一个时分秒的Hierarchy时间轮，不同于Linux内核的实现，但原理类似。对于时分秒三级时间轮，每个时间轮都维护一个cursor，新建一个timer时，要挂在合适的格子，剩余轮数以及时间都要记录，到期判断超时并调整位置。原理图大致如下:

![img](https://pic2.zhimg.com/80/v2-aaf44a8470a08ca07c7a3faffa3235e9_720w.webp)

![img](https://pic3.zhimg.com/80/v2-98f18c350c20b4b37f4520cf6461dc5a_720w.webp)

对于时间轮的实现，Timer依然是存放在链表上，但是借助了hash的思想，将相同间隔(或者相同周期的整数倍)的超时Timer放在同一个时间轮子上的槽(slot)上。时间轮上有一个指针，按照一个基准的频率(比如1ms,5ms,10ms等，libco中设置的是1ms)向前移动。这个基准的频率就是传递给epoll_wait()超时的值，也是定时器精度的基本单位。

```
class Timer
{
public:
    Timer(int rotations,int slot,std::function<void(void)> fun,void* args)
        : rotations_(rotations),slot_(slot),fun(fun) { }
    inline int getRotations() { return rotations_; }
    inline void decreaseRotations() { --rotations_; }
    inline void active() { fun(); }
    inline int getSlot() { return slot_; }
private:
    int rotations_;
    int slot_;
    std::function<void(void)> fun;
    void* args;
};
```

时间轮中的Timer类和最小堆中的实现，多了两个参数，rotations表示时间轮转多少圈后当前的Timer会触发，slot表示当前的Timer应该挂在对应的槽指向的链表上。成员函数比较简单，不多赘述。

```
class TimeWheel
{
  public:
    TimeWheel(int nslots)
        : nslosts_(nslots),curslot_(0),
            slots_(nslosts_,std::vector<Timer*>()),starttime_(getCurrentMillisecs()) { }
    ~TimeWheel();
    unsigned long long getCurrentMillisecs();
    Timer *addTimer(int timeout,std::function<void(void)> fun,void* args);
    void delTimer(Timer *timer);
    void tick();
    void takeAllTimeout();
private:
    int nslosts_;
    int curslot_;
    unsigned long long starttime_;
    std::vector<std::vector<Timer*>> slots_;
};
```

curslot_表示时间轮当前指向的那个slot。nslosts_表示这个时间轮拥有多少个slot，不要上图迷惑了，实际上slot会远远超过这8个。要想效率足够高，slot就要越大，当然占用的内存也会越大(现代计算机，这点内存可以忽略不计)，libco默认使用了 60 * 1000 个slot 。

addTimer()是添加一个Timer到TimeWheel上，需要根据传递的timeout参数，计算出该Timer所对应的slot (slot = (curslot_ + (timeout % nslosts_)) % nslosts_;) ,还有到这个Timer超时时间轮的指针需要转过的圈数(timeout / nslosts_)。

delTimer() 根据Timer*参数，删除时间轮上对应的Timer。

tick() 时间轮的指针走动一下。同时遍历当前slot上链表里的每一个Timer，如果Timer的圈数大于0，将Timer里的圈数-1，否则激活这个Timer。

takeAllTimeout() 是必要的，由于误差的存在，每次epoll_wait超时后(一个基准频率)，时间轮可能需要走动好几步。如果每次epoll_wait后直接tick()而不是takeAllTimeout()，会导致误差一直被累积，时间轮上剩余的定时器被滞后触发。

时间轮的实现我也放到了Github上:[https://gist.github.com/baixiangcpp/63278c0087201a655f940ab8de543abd](https://link.zhihu.com/?target=https%3A//gist.github.com/baixiangcpp/63278c0087201a655f940ab8de543abd)。使用方式和之前的最小堆实现的基本是一样的。

使用示例：

```
int dispatch()
{
    ...
    TimeWheel tw(60 * 1000);
    tw.addTimer(1000, []() { std::cout << "hello world" << std::endl; }, NULL);
    tw.addTimer(5000, []() { std::cout << "hello baixiancpp" << std::endl; }, NULL);
    for(;;)
    {
        int ret = epoll_wait(epollfd,events,events_num,1); // 基准频率 1ms
        // tw.tick(); 不要这么做，会导致误差累积
        tw.takeAllTimeout();
    }
    ...
}
```

最小堆和时间轮的时间复杂度，如下：

|    Type    |  add   | exec |
| :--------: | :----: | :--: |
|    list    |  O(1)  | O(n) |
|  min-heap  | O(lgn) | O(1) |
| time wheel |  O(1)  | O(n) |

这里我只列出添加定时器和触发定时器的时间复杂度，因为这几种实现中，删除一个定时器可以优化到O(1)的时间复杂度————把其对应的回调函数置空。前面的例子我没有这么做，仅为展示，有兴趣的话可以自行修改。

乍看下来，时间轮的复杂度和链表是一样的。其实不然，时间轮上触发一个定时器，仅仅是理论上的O(n),只要slot的数量设置合理，时间复杂度会下降至接近O(1)。

可以根据实际需要，选择合适的定时器容器。

## 7.要不要用Timerfd？

开篇的表格里有提到，从Linux2.6.25开始，timerfd系列API，带来了一种全新的定时机制。把超时事件转换为了文件描述符，当超时发生后该文件描述符会变成可读。于是超时事件就变成了普通的IO事件。如果未对timerfd设置阻塞，对其read操作会一直阻塞到超时发生。此外timerfd的精度达到了纳秒级。不考虑跨平台等因素，这是一个非常不错的选择。

libevent2.1的源码里也支持timerfd了，在版本说明里也很明确了说明了使用多路复用的超时参数和使用timerfd之间的差异 ，它使用了两个词”efficient”和”precise”，分别表示这种实现之间的差异，我想着这还是非常有说服力的。

## 8.每个超时事件独享一个timerfd

如果对于每一个超时事件都用timerfd_create()创建一个对应的fd，放到epoll中统一管理。这样的做法是不合适的。每增加一个定时事件，都需要额外的3个系统调用:

![img](https://pic4.zhimg.com/80/v2-b7a7250d3a6ac4f4fd25687a23cf422b_720w.webp)

此外，文件描述符还是稀缺的资源，每个进程能够使用的文件描述符是受系统限制的，如果定时器过多，会造成严重的浪费。

这种方式的定时器，比较容易实现，这里我就不再浪费篇幅了。

## 9.所有超时事件共享一个timerfd

libevent就是使用的这种方式。定时时间仍然使用最小堆来保存，每个event loop共享同一个timerfd。每次事件循环之前，取出最近的一个超时的时间，将这个timerfd设置为这个超时时间。

```
int epoll_dispatch( ...)
{
    ...
    if (epollop->timerfd >= 0)
    {
        struct itimerspec is;
        is.it_value.tv_sec = tv->tv_sec;
        is.it_value.tv_nsec = tv->tv_usec * 1000;
        timerfd_settime(epollop->timerfd, 0, &is, NULL);
    }
    res = epoll_wait(epollop->epfd, events, epollop->nevents, -1);
    for (i = 0; i < res; i++) 
    {
        if (events[i].data.fd == epollop->timerfd)
            ;//
    }
}
```

这样的改进规避了前一种方式提到的造成文件描述符资源浪费的问题，仅仅需要1个额外的文件描述符。

额外的系统调用从额外的3个，降到了1个。而且还有改进的空间，只有当栈顶的timeout变化时，才调用timerfd_settime()改变。

这种方式实现的定时器，精度提高了但是多了1个额外的系统调用。libevent把选择权给了用户，用户可以根据实际情况在创建event base的时候是否配置EVENT_BASE_FLAG_PRECISE_TIMER宏而选择使用哪个定时器实现。

## 10.总结

std::priority_queue是一个容器适配器，底层的容器默认使用的std::vector(make_heap())。但是这不意味着往std::priority_queue插入一个元素的开销是O(n)，C++标准对此实现有要求，可以放心大胆的去用。但是std::priority_queue没有提供高效删除元素的接口，我们可以通过将回调函数置空的方式，以O(1)的时间复杂度实现删除。

以C++实现的muduo网络库使用的是std::set集合存放Timer：

```
typedef std::pair<Timestamp, Timer*> Entry;
typedef std::set<Entry> TimerList;
TimerList timers_;
```

实际上std::set实现应该是二叉搜索树，因此效率可能会比用std::priority_queue略差一点（《linux多线程网络编程》 8.2 ）。

此外，libev 允许使用一个宏EV_USE_4HEAP指定以一个4-heap的数据结构保存定时器，*据说*效率更高，我也没有测试。

以上就是目前一些c/c++语言实现的网络库里边定时器常用的设计手法。

原文链接：https://zhuanlan.zhihu.com/p/372551679

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.218】深入理解MySQL索引和优化丨MySQL的核心原理

## 1.索引是什么

- 官方介绍索引是帮助MySQL高效获取数据的数据结构。更通俗的说，数据库索引好比是一本书前面的目录，能加快数据库的查询速度。
- 一般来说索引本身也很大，不可能全部存储在内存中，因此索引往往是存储在磁盘上的文件中的（可能存储在单独的索引文件中，也可能和数据一起存储在数据文件中）。
- 我们通常所说的索引，包括聚集索引、覆盖索引、组合索引、前缀索引、唯一索引等，没有特别说明，默认都是使用B+树结构组织（多路搜索树，并不一定是二叉的）的索引。

## 2.索引的优势和劣势

优势：

- 可以提高数据检索的效率，降低数据库的IO成本，类似于书的目录。
- 通过索引列对数据进行排序，降低数据排序的成本，降低了CPU的消耗。
- - 被索引的列会自动进行排序，包括【单列索引】和【组合索引】，只是组合索引的排序要复杂一些。
  - 如果按照索引列的顺序进行排序，对应order by语句来说，效率就会提高很多。

劣势：

- 索引会占据磁盘空间
- 索引虽然会提高查询效率，但是会降低更新表的效率。比如每次对表进行增删改操作，MySQL不仅要保存数据，还有保存或者更新对应的索引文件。

## 3.主键索引

索引列中的值必须是唯一的，不允许有空值。

### 3.1.普通索引

MySQL中基本索引类型，没有什么限制，允许在定义索引的列中插入重复值和空值。

### 3.2.唯一索引

索引列中的值必须是唯一的，但是允许为空值。

### 3.3.全文索引

只能在文本类型CHAR,VARCHAR,TEXT类型字段上创建全文索引。字段长度比较大时，如果创建普通索引，在进行like模糊查询时效率比较低，这时可以创建全文索引。MyISAM和InnoDB中都可以使用全文索引。

### 3.4.空间索引

MySQL在5.7之后的版本支持了空间索引，而且支持OpenGIS几何数据模型。MySQL在空间索引这方面遵循OpenGIS几何数据模型规则。

### 3.5.前缀索引

在文本类型如CHAR,VARCHAR,TEXT类列上创建索引时，可以指定索引列的长度，但是数值类型不能指定。

### 3.6.其他（按照索引列数量分类）

1. 单列索引
2. 组合索引组合索引的使用，需要遵循最左前缀匹配原则（最左匹配原则）。一般情况下在条件允许的情况下使用组合索引替代多个单列索引使用。

## 4.索引的数据结构

### 4.1.Hash表

Hash表，在Java中的HashMap，TreeMap就是Hash表结构，以键值对的方式存储数据。我们使用Hash表存储表数据Key可以存储索引列，Value可以存储行记录或者行磁盘地址。Hash表在等值查询时效率很高，时间复杂度为O(1)；但是不支持范围快速查找，范围查找时还是只能通过扫描全表方式。

显然这种并不适合作为经常需要查找和范围查找的数据库索引使用。

### 4.2.二叉查找树

二叉树，我想大家都会在心里有个图。

![img](https://pic3.zhimg.com/80/v2-7cb56e5fff5df1a1fa8c819ce0fa279a_720w.webp)

二叉树特点：每个节点最多有2个分叉，左子树和右子树数据顺序左小右大。

这个特点就是为了保证每次查找都可以这折半而减少IO次数，但是二叉树就很考验第一个根节点的取值，因为很容易在这个特点下出现我们并发想发生的情况“树不分叉了”，这就很难受很不稳定。

![img](https://pic2.zhimg.com/80/v2-7b223419fb21988290a8a096a9b55505_720w.webp)

显然这种情况不稳定的我们再选择设计上必然会避免这种情况的

### 4.3.平衡二叉树

平衡二叉树是采用二分法思维，平衡二叉查找树除了具备二叉树的特点，最主要的特征是树的左右两个子树的层级最多相差1。在插入删除数据时通过左旋/右旋操作保持二叉树的平衡，不会出现左子树很高、右子树很矮的情况。

使用平衡二叉查找树查询的性能接近于二分查找法，时间复杂度是 O(log2n)。查询id=6，只需要两次IO。

![img](https://pic3.zhimg.com/80/v2-7cb56e5fff5df1a1fa8c819ce0fa279a_720w.webp)

就这个特点来看，可能各位会觉得这就很好，可以达到二叉树的理想的情况了。然而依然存在一些问题：

1. 时间复杂度和树高相关。树有多高就需要检索多少次，每个节点的读取，都对应一次磁盘 IO 操作。树的高度就等于每次查询数据时磁盘 IO 操作的次数。磁盘每次寻道时间为10ms，在表数据量大时，查询性能就会很差。（1百万的数据量，log2n约等于20次磁盘IO，时间20*10=0.2s）
2. 平衡二叉树不支持范围查询快速查找，范围查询时需要从根节点多次遍历，查询效率不高。

### 4.4.B树：改造二叉树

MySQL的数据是存储在磁盘文件中的，查询处理数据时，需要先把磁盘中的数据加载到内存中，磁盘IO 操作非常耗时，所以我们优化的重点就是尽量减少磁盘 IO 操作。访问二叉树的每个节点就会发生一次IO，如果想要减少磁盘IO操作，就需要尽量降低树的高度。那如何降低树的高度呢？

假如key为bigint=8字节，每个节点有两个指针，每个指针为4个字节，一个节点占用的空间16个字节（8+4*2=16）。

因为在MySQL的InnoDB存储引擎一次IO会读取的一页（默认一页16K）的数据量，而二叉树一次IO有效数据量只有16字节，空间利用率极低。为了最大化利用一次IO空间，一个简单的想法是在每个节点存储多个元素，在每个节点尽可能多的存储数据。每个节点可以存储1000个索引（16k/16=1000），这样就将二叉树改造成了多叉树，通过增加树的叉树，将树从高瘦变为矮胖。构建1百万条数据，树的高度只需要2层就可以（1000*1000=1百万），也就是说只需要2次磁盘IO就可以查询到数据。磁盘IO次数变少了，查询数据的效率也就提高了。

这种数据结构我们称为B树，B树是一种多叉平衡查找树，如下图主要特点：

1. B树的节点中存储着多个元素，每个内节点有多个分叉。
2. 节点中的元素包含键值和数据，节点中的键值从大到小排列。也就是说，在所有的节点都储存数据。
3. 父节点当中的元素不会出现在子节点中。
4. 所有的叶子结点都位于同一层，叶节点具有相同的深度，叶节点之间没有指针连接。

![img](https://pic4.zhimg.com/80/v2-a93007e43eeb1acd9efd7eed3d795927_720w.webp)

举个例子，在b树中查询数据的情况：

假如我们查询值等于10的数据。查询路径磁盘块1->磁盘块2->磁盘块5。

第一次磁盘IO：将磁盘块1加载到内存中，在内存中从头遍历比较，10<15，走左路，到磁盘寻址磁盘块2。

第二次磁盘IO：将磁盘块2加载到内存中，在内存中从头遍历比较，7<10，到磁盘中寻址定位到磁盘块5。

第三次磁盘IO：将磁盘块5加载到内存中，在内存中从头遍历比较，10=10，找到10，取出data，如果data存储的行记录，取出data，查询结束。如果存储的是磁盘地址，还需要根据磁盘地址到磁盘中取出数据，查询终止。

相比二叉平衡查找树，在整个查找过程中，虽然数据的比较次数并没有明显减少，但是磁盘IO次数会大大减少。同时，由于我们的比较是在内存中进行的，比较的耗时可以忽略不计。B树的高度一般2至3层就能满足大部分的应用场景，所以使用B树构建索引可以很好的提升查询的效率。

过程如图：

![img](https://pic4.zhimg.com/80/v2-dca84a7e6681f27bdb65b64f0a59df63_720w.webp)

## 5.B树索引查询过程

看到这里一定觉得B树就很理想了，但是前辈们会告诉你依然存在可以优化的地方：

> B树不支持范围查询的快速查找，你想想这么一个情况如果我们想要查找10和35之间的数据，查找到15之后，需要回到根节点重新遍历查找，需要从根节点进行多次遍历，查询效率有待提高。
> 如果data存储的是行记录，行的大小随着列数的增多，所占空间会变大。这时，一个页中可存储的数据量就会变少，树相应就会变高，磁盘IO次数就会变大。

### 5.1.B+树：改造B树

B+树，作为B树的升级版，在B树基础上，MySQL在B树的基础上继续改造，使用B+树构建索引。B+树和B树最主要的区别在于非叶子节点是否存储数据的问题

> B树：非叶子节点和叶子节点都会存储数据。B+树：只有叶子节点才会存储数据，非叶子节点至存储键值。叶子节点之间使用双向指针连接，最底层的叶子节点形成了一个双向有序链表。

![img](https://pic4.zhimg.com/80/v2-207efbc004c1cbca8d4a9d5ee629f49f_720w.webp)

### 5.2.B+树数据结构

> B+树的最底层叶子节点包含了所有的索引项。从图上可以看到，B+树在查找数据的时候，由于数据都存放在最底层的叶子节点上，所以每次查找都需要检索到叶子节点才能查询到数据。
> 所以在需要查询数据的情况下每次的磁盘的IO跟树高有直接的关系，但是从另一方面来说，由于数据都被放到了叶子节点，放索引的磁盘块锁存放的索引数量是会跟这增加的，相对于B树来说，B+树的树高理论上情况下是比B树要矮的。
> 也存在索引覆盖查询的情况，在索引中数据满足了当前查询语句所需要的全部数据，此时只需要找到索引即可立刻返回，不需要检索到最底层的叶子节点。

举个例子：等值查询

假如我们查询值等于9的数据。查询路径磁盘块1->磁盘块2->磁盘块6。

第一次磁盘IO：将磁盘块1加载到内存中，在内存中从头遍历比较，9<15，走左路，到磁盘寻址磁盘块2。

第二次磁盘IO：将磁盘块2加载到内存中，在内存中从头遍历比较，7<9<12，到磁盘中寻址定位到磁盘块6。

第三次磁盘IO：将磁盘块6加载到内存中，在内存中从头遍历比较，在第三个索引中找到9，取出data，如果data存储的行记录，取出data，查询结束。如果存储的是磁盘地址，还需要根据磁盘地址到磁盘中取出数据，查询终止。（这里需要区分的是在InnoDB中Data存储的为行数据，而MyIsam中存储的是磁盘地址。）

过程如图：

![img](https://pic1.zhimg.com/80/v2-f5a8af15d5a3c0171add1dda49ec60d4_720w.webp)

### 5.3.B+树根据索引等值查询过程

范围查询：

假如我们想要查找9和26之间的数据。查找路径是磁盘块1->磁盘块2->磁盘块6->磁盘块7。

首先查找值等于9的数据，将值等于9的数据缓存到结果集。这一步和前面等值查询流程一样，发生了三次磁盘IO。

查找到15之后，底层的叶子节点是一个有序列表，我们从磁盘块6，键值9开始向后遍历筛选所有符合筛选条件的数据。

第四次磁盘IO：根据磁盘6后继指针到磁盘中寻址定位到磁盘块7，将磁盘7加载到内存中，在内存中从头遍历比较，9<25<26，9<26<=26，将data缓存到结果集。

主键具备唯一性（后面不会有<=26的数据），不需再向后查找，查询终止。将结果集返回给用户。

![img](https://pic2.zhimg.com/80/v2-3e01daf1ec16efff50accf268e63bac1_720w.webp)

可以看到B+树可以保证等值和范围查询的快速查找，MySQL的索引就采用了B+树的数据结构。

### 5.4.Mysql的索引实现

介绍完了索引数据结构，那肯定是要带入到Mysql里面看看真实的使用场景的，所以这里分析Mysql的两种存储引擎的索引实现：MyISAM索引和InnoDB索引

### 5.5.MyIsam索引

以一个简单的user表为例。user表存在两个索引，id列为主键索引，age列为普通索引

```
CREATE TABLE `user`(  `id`       int(11) NOT NULL AUTO_INCREMENT,  `username` varchar(20) DEFAULT NULL,  `age`      int(11)     DEFAULT NULL,  PRIMARY KEY (`id`) USING BTREE,  KEY `idx_age` (`age`) USING BTREE) ENGINE = MyISAM  AUTO_INCREMENT = 1  DEFAULT CHARSET = utf8;
```

![img](https://pic4.zhimg.com/80/v2-ba02e0eb12194112697fe7021830a6e7_720w.webp)

### 5.6.MyIsam_user查询数据

MyISAM的数据文件和索引文件是分开存储的。MyISAM使用B+树构建索引树时，叶子节点中存储的键值为索引列的值，数据为索引所在行的磁盘地址。

## 6.主键索引

![img](https://pic3.zhimg.com/80/v2-cdceccb2cce61955a458dcb3b6d5b7e6_720w.webp)

### 6.1.MyIsam主键索引

表user的索引存储在索引文件user.MYI中，数据文件存储在数据文件 user.MYD中。

简单分析下查询时的磁盘IO情况：

根据主键等值查询数据：

```
select * fromuserwhereid = 28;
```

1. 先在主键树中从根节点开始检索，将根节点加载到内存，比较28<75，走左路。（1次磁盘IO）
2. 将左子树节点加载到内存中，比较16<28<47，向下检索。（1次磁盘IO）
3. 检索到叶节点，将节点加载到内存中遍历，比较16<28，18<28，28=28。查找到值等于30的索引项。（1次磁盘IO）
4. 从索引项中获取磁盘地址，然后到数据文件user.MYD中获取对应整行记录。（1次磁盘IO）
5. 将记录返给客户端。

磁盘IO次数：3次索引检索+记录数据检索。

![img](https://pic1.zhimg.com/80/v2-18211b5eee1faafb7b3e3a8299950b64_720w.webp)

根据主键范围查询数据：

```
select * from user where id between 28 and 47;
```

1. 先在主键树中从根节点开始检索，将根节点加载到内存，比较28<75，走左路。（1次磁盘IO）
2. 将左子树节点加载到内存中，比较16<28<47，向下检索。（1次磁盘IO）
3. 检索到叶节点，将节点加载到内存中遍历比较16<28，18<28，28=28<47。查找到值等于28的索引项。根据磁盘地址从数据文件中获取行记录缓存到结果集中。（1次磁盘IO）我们的查询语句时范围查找，需要向后遍历底层叶子链表，直至到达最后一个不满足筛选条件。
4. 向后遍历底层叶子链表，将下一个节点加载到内存中，遍历比较，28<47=47，根据磁盘地址从数据文件中获取行记录缓存到结果集中。（1次磁盘IO）
5. 最后得到两条符合筛选条件，将查询结果集返给客户端。

磁盘IO次数：4次索引检索+记录数据检索。

![img](https://pic2.zhimg.com/80/v2-0f052612a720e581f4e785e91af80839_720w.webp)

### 6.2.MyIsam索引范围查询过程

备注：以上分析仅供参考，MyISAM在查询时，会将索引节点缓存在MySQL缓存中，而数据缓存依赖于操作系统自身的缓存，所以并不是每次都是走的磁盘，这里只是为了分析索引的使用过程。

### 6.3.辅助索引

在 MyISAM 中,辅助索引和主键索引的结构是一样的，没有任何区别，叶子节点的数据存储的都是行记录的磁盘地址。只是主键索引的键值是唯一的，而辅助索引的键值可以重复。

查询数据时，由于辅助索引的键值不唯一，可能存在多个拥有相同的记录，所以即使是等值查询，也需要按照范围查询的方式在辅助索引树中检索数据。

## 7.InnoDB索引

### 7.1.主键索引（聚簇索引）

每个InnoDB表都有一个聚簇索引 ，聚簇索引使用B+树构建，叶子节点存储的数据是整行记录。一般情况下，聚簇索引等同于主键索引，当一个表没有创建主键索引时，InnoDB会自动创建一个ROWID字段来构建聚簇索引。InnoDB创建索引的具体规则如下：

> 在表上定义主键PRIMARY KEY，InnoDB将主键索引用作聚簇索引。如果表没有定义主键，InnoDB会选择第一个不为NULL的唯一索引列用作聚簇索引。如果以上两个都没有，InnoDB 会使用一个6 字节长整型的隐式字段 ROWID字段构建聚簇索引。该ROWID字段会在插入新行时自动递增。

除聚簇索引之外的所有索引都称为辅助索引。在中InnoDB，辅助索引中的叶子节点存储的数据是该行的主键值都。在检索时，InnoDB使用此主键值在聚簇索引中搜索行记录。

这里以user_innodb为例，user_innodb的id列为主键，age列为普通索引。

```
CREATE TABLE `user_innodb`(  `id`       int(11) NOT NULL AUTO_INCREMENT,  `username` varchar(20) DEFAULT NULL,  `age`      int(11)     DEFAULT NULL,  PRIMARY KEY (`id`) USING BTREE,  KEY `idx_age` (`age`) USING BTREE) ENGINE = InnoDB;
```

![img](https://pic2.zhimg.com/80/v2-0f052612a720e581f4e785e91af80839_720w.webp)

### 7.2.user数据

InnoDB的数据和索引存储在一个文件t_user_innodb.ibd中。InnoDB的数据组织方式，是聚簇索引。

主键索引的叶子节点会存储数据行，辅助索引只会存储主键值。

![img](https://pic1.zhimg.com/80/v2-4c8f4b45f0e15f77192a432cf56272e4_720w.webp)

### 7.3.InnoDB主键索引

等值查询数据：

```
select * from user_innodb where id = 28;
```

1. 先在主键树中从根节点开始检索，将根节点加载到内存，比较28<75，走左路。（1次磁盘IO）将左子树节点加载到内存中，比较16<28<47，向下检索。（1次磁盘IO）检索到叶节点，将节点加载到内存中遍历，比较16<28，18<28，28=28。查找到值等于28的索引项，直接可以获取整行数据。将改记录返回给客户端。（1次磁盘IO）磁盘IO数量：3次。

![img](https://pic3.zhimg.com/80/v2-093c0a7b7dd58125041c62c2e5c52236_720w.webp)

### 7.4.辅助索引

除聚簇索引之外的所有索引都称为辅助索引，InnoDB的辅助索引只会存储主键值而非磁盘地址。

以表user_innodb的age列为例，age索引的索引结果如下图。

![img](https://pic4.zhimg.com/80/v2-93b6e79380bda5852e52a27f0689eb4b_720w.webp)

### 7.5.InnoDB辅助索引

底层叶子节点的按照（age，id）的顺序排序，先按照age列从小到大排序，age列相同时按照id列从小到大排序。

使用辅助索引需要检索两遍索引：首先检索辅助索引获得主键，然后使用主键到主索引中检索获得记录。

画图分析等值查询的情况：

```
select * from t_user_innodb where age=19;
```

![img](https://pic4.zhimg.com/80/v2-4a430f2bd54ced2fab51768b73246b63_720w.webp)

### 7.6.InnoDB辅助索引查询

根据在辅助索引树中获取的主键id，到主键索引树检索数据的过程称为回表查询。

磁盘IO数：辅助索引3次+获取记录回表3次

### 7.7.组合索引

还是以自己创建的一个表为例：表 abc_innodb，id为主键索引，创建了一个联合索引idx_abc(a,b,c)。

```
CREATE TABLE `abc_innodb`(  `id` int(11) NOT NULL AUTO_INCREMENT,  `a`  int(11)     DEFAULT NULL,  `b`  int(11)     DEFAULT NULL,  `c`  varchar(10) DEFAULT NULL,  `d`  varchar(10) DEFAULT NULL,  PRIMARY KEY (`id`) USING BTREE,  KEY `idx_abc` (`a`, `b`, `c`)) ENGINE = InnoDB;
```

select * from abc_innodb order by a, b, c, id;

![img](https://pic1.zhimg.com/80/v2-fc7bd7e5a1874ce655151c896df3cbbc_720w.webp)

组合索引的数据结构：

![img](https://pic3.zhimg.com/80/v2-c3b476f8dc6028da07818ed480d6ff4e_720w.webp)

### 7.8.组合索引结构1

组合索引的查询过程：

```
select * from abc_innodb where a = 13 and b = 16 and c = 4;
```

![img](https://pic2.zhimg.com/80/v2-e157f8c8a7008d47ed14a2c5f548de5d_720w.webp)

## 8.组合索引的查询过程

最左匹配原则：

最左前缀匹配原则和联合索引的索引存储结构和检索方式是有关系的。

在组合索引树中，最底层的叶子节点按照第一列a列从左到右递增排列，但是b列和c列是无序的，b列只有在a列值相等的情况下小范围内递增有序，而c列只能在a，b两列相等的情况下小范围内递增有序。

就像上面的查询，B+树会先比较a列来确定下一步应该搜索的方向，往左还是往右。如果a列相同再比较b列。但是如果查询条件没有a列，B+树就不知道第一步应该从哪个节点查起。

可以说创建的idx_abc(a,b,c)索引，相当于创建了(a)、（a,b）（a,b,c）三个索引。、

组合索引的最左前缀匹配原则：使用组合索引查询时，mysql会一直向右匹配直至遇到范围查询(>、<、between、like)就停止匹配。

## 9.覆盖索引

覆盖索引并不是说是索引结构，覆盖索引是一种很常用的优化手段。因为在使用辅助索引的时候，我们只可以拿到主键值，相当于获取数据还需要再根据主键查询主键索引再获取到数据。但是试想下这么一种情况，在上面abc_innodb表中的组合索引查询时，如果我只需要abc字段的，那是不是意味着我们查询到组合索引的叶子节点就可以直接返回了，而不需要回表。这种情况就是覆盖索引。

可以看一下执行计划：

覆盖索引的情况：

![img](https://pic2.zhimg.com/80/v2-6d5100c03041cd465d79608a0cfdaa81_720w.webp)

## 10.使用到覆盖索引

未使用到覆盖索引：

![img](https://pic3.zhimg.com/80/v2-d0798efea3ac8cdb612e9410ec0a783a_720w.webp)

## 11.总结

看到这里，你是不是对于自己的sql语句里面的索引的有了更多优化想法呢。

比如：

## 12.避免回表

在InnoDB的存储引擎中，使用辅助索引查询的时候，因为辅助索引叶子节点保存的数据不是当前记录的数据而是当前记录的主键索引，索引如果需要获取当前记录完整数据就必然需要根据主键值从主键索引继续查询。这个过程我们成位回表。想想回表必然是会消耗性能影响性能。那如何避免呢？

使用索引覆盖，举个例子：现有User表（id(PK),name(key),sex,address,hobby…）

如果在一个场景下，select id,name,sex from user where name =’zhangsan’;这个语句在业务上频繁使用到，而user表的其他字段使用频率远低于它，在这种情况下，如果我们在建立 name 字段的索引的时候，不是使用单一索引，而是使用联合索引（name，sex）这样的话再执行这个查询语句是不是根据辅助索引查询到的结果就可以获取当前语句的完整数据。

这样就可以有效地避免了回表再获取sex的数据。

这里就是一个典型的使用覆盖索引的优化策略减少回表的情况。

## 13.联合索引的使用

联合索引，在建立索引的时候，尽量在多个单列索引上判断下是否可以使用联合索引。联合索引的使用不仅可以节省空间，还可以更容易的使用到索引覆盖。

试想一下，索引的字段越多，是不是更容易满足查询需要返回的数据呢。比如联合索引（a_b_c），是不是等于有了索引：a，a_b，a_b_c三个索引，这样是不是节省了空间，当然节省的空间并不是三倍于（a，a_b，a_b_c）三个索引，因为索引树的数据没变，但是索引data字段的数据确实真实的节省了。

联合索引的创建原则，在创建联合索引的时候因该把频繁使用的列、区分度高的列放在前面，频繁使用代表索引利用率高，区分度高代表筛选粒度大，这些都是在索引创建的需要考虑到的优化场景，也可以在常需要作为查询返回的字段上增加到联合索引中，如果在联合索引上增加一个字段而使用到了覆盖索引，那我建议这种情况下使用联合索引。

联合索引的使用

1. 考虑当前是否已经存在多个可以合并的单列索引，如果有，那么将当前多个单列索引创建为一个联合索引。
2. 当前索引存在频繁使用作为返回字段的列，这个时候就可以考虑当前列是否可以加入到当前已经存在索引上，使其查询语句可以使用到覆盖索引。

原文链接：https://zhuanlan.zhihu.com/p/373665690

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.219】perf-网络协议栈性能分析

分析 Linux 网络协议栈性能有多种方式和工具。本文主要通过 Perf 生成 On-CPU 火焰图的方式，分析 Linux 内核网络协议栈在特定场景下的性能瓶颈，从而知晓当前协议栈的网络状况。

## 1.关于 On/Off-CPU

### 1.1.概念定义

![img](https://pic4.zhimg.com/80/v2-140bbf8313007f59c92008d0cf6d87c3_720w.webp)

### 1.2.On/Off-CPU 选择

在工程实践中，如果是 CPU 消耗型使用 On-CPU 火焰图，如果是 IO 消耗型则使用 Off-CPU 火焰图。如果无法确定, 可以通过压测工具来确认: 通过压测工具看能否让 CPU 使用率趋于饱和，从而判断是否为 CPU 消耗型。

### 1.3.分析方法

Perf 火焰图整个图形看起来就像一团跳动的火焰，这也正是其名字的由来。燃烧在火苗尖部的就是 CPU 正在执行的操作，不过需要说明的是颜色是随机的，本身并没有特殊的含义，纵向表示调用栈的深度，横向表示消耗的时间。因为调用栈在横向会按照字母排序，并且同样的调用栈会做合并，所以一个格子的宽度越大越说明其可能是瓶颈。综上所述，主要就是看那些比较宽大的火苗，特别留意那些类似平顶山的火苗。

## 2.火焰图原理

火焰图是基于 stack 信息生成图片, 用来展示 CPU 调用栈。

- y 轴表示调用栈, 每一层都是一个函数。调用栈越深, 火焰就越高, 顶部就是正在执行的函数, 下方都是它的父函数。
- x 轴表示抽样数, 如果一个函数在x轴占据越宽, 则表示它被抽到的次数多, 即执行的时间长。

火焰图就是看顶层的哪个函数占据的宽度最大。只要有“平顶”(plateaus)，就表示该函数可能存在性能问题。

## 3.On-CPU 采集原理

![img](https://pic1.zhimg.com/80/v2-a682f2717063b9dc0ca22c4da9989d74_720w.webp)

![img](https://pic1.zhimg.com/80/v2-081e26d5bef572c4e585d05ca522990c_720w.webp)

Linux 内核网络协议栈分析

server

```
yum install iperfyum install perfgit clone https://github.com/brendangregg/FlameGraph.git
```

运行 iperf server

```
[root@bogon ~]# iperf -s -u -DRunning Iperf Server as a daemon[root@bogon ~]#
```

查看 CPU 以及 softirqd 进程号

```
[root@bogon ~]# lscpuArchitecture:          aarch64Byte Order:            Little EndianCPU(s):                64On-line CPU(s) list:   0-63Thread(s) per core:    1Core(s) per socket:    4Socket(s):             16NUMA node(s):          4Model:                 2BogoMIPS:              100.00NUMA node0 CPU(s):     0-15NUMA node1 CPU(s):     16-31NUMA node2 CPU(s):     32-47NUMA node3 CPU(s):     48-63Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid
```

![img](https://pic4.zhimg.com/80/v2-a4150c37c195fd209c70f582b0959d9b_720w.webp)

查看网卡中断亲和性

![img](https://pic2.zhimg.com/80/v2-12bd0c929fad16b85392da3063e4f001_720w.webp)

![img](https://pic2.zhimg.com/80/v2-a2d31aad24d547a15829d926cb6bbdcd_720w.webp)

```
VM001: 通过如下方式采集信息(采集原理见上)perf record -F 1000 -a -g -p 162 -- sleep 60
```

发送 UDP 数据报文

```
iperf  -c 10.254.2.161 -i 1 -P 10 -t 10 -u -b 1000M
```

## 4.生成 On-CPU 火焰图

![img](https://pic1.zhimg.com/80/v2-b088f3056d212b56812a3b60285b3a04_720w.webp)

![img](https://pic1.zhimg.com/80/v2-bddd0d6ad7f632560c3f4ae6dfc8ff48_720w.webp)

![img](https://pic1.zhimg.com/80/v2-503a3f2918d0b2e0474fc90de58b6558_720w.webp)

原文链接：https://linuxcpp.0voice.com/?id=442

作者：[HG ](https://linuxcpp.0voice.com/?auth=10)

# 【NO.220】【epoll】epoll多路复用和Reactor设计思想

## 1、Reactor设计思想

### 1.1.小前言：

reactor是对epoll的一层封装 ，epoll是对io进行管理，reactor将对io的管理转化为对事件的管理

### 1.2.Reactor必要

#### 1.2.1.传统OIO模式

如图2.1所示为传统IO模式处理示意图：

![img](https://pic3.zhimg.com/80/v2-e642fdd136b6ab64c7bb3a387db57ac6_720w.webp)

图中所示一般是一个请求一个单独的处理线程。

缺点：server的accpet操作是阻塞的，业务处理中的handler中的读写请求也是阻塞的。那么这样的一种IO模式将会导致一个线程的请求没有处理完成无法处理下一个请求，这样就大大降低了吞吐量，这将是一个严重的问题。

为了解决这种问题就出现了一个经典的模式——Connection Per Thread即一个线程处理一个请求。

![img](https://pic1.zhimg.com/80/v2-2b449cc8768b2530cc6d1984d3bd1990_720w.webp)

对于每一个新的请求都会分配一个新的线程来处理，这样的好处就是每个socket的请求相互之间不受影响，每个请求的业务逻辑相互之间也不影响。任何socket的读写操作都不会影响到后面的请求。

缺点：不是每个链接都有请求发生，这样就浪费了很多的线程资源。

这个时候可以采用多路复用IO模型的方式来处理IO事件，使用Reactor将响应IO事件和业务处理分开，一个或多个线程来处理IO事件，然后将就绪得到事件分发到业务处理handlers线程去异步非阻塞处理。

## 2. Reactor模式

### 2.1.单线程Reactor模式

什么是单线程Reactor模式，单线程模式采用一个Reactor线程来【处理套接字、新连接的创建】，并且【将接收到的请求分发到处理器Handler中】。

如图2.2为简单的单线程Reactor模式示意图，Reactor和数据处理(handler)都在一个线程里，图2.2参考doug lea论文《Scalable IO in Java》论文。

![img](https://pic2.zhimg.com/80/v2-0a5e783f2636f511ec715e1e6a289041_720w.webp)

图2.2单线程Reactor模式示意图

### 2.2.单Reactor多线程模式：

![img](https://pic4.zhimg.com/80/v2-45b9e4adbd4aeb8205027d15d1ef445b_720w.webp)

### 2.3.多线程Reactor模式

多线程reactor模式的设计思想就是将handler线程放入到线程次中，在多核的情况下也可以考虑多个Selector选择器来处理事件，如图2.3为简单的多线程Reactor示意图;

![img](https://pic4.zhimg.com/80/v2-66f6e3046461028a473d5eeaae7dc05b_720w.webp)

图2.3多线程Reactor模式示意图

## 3.封装Epoll实现并发

第一次学epoll时，容易错误的认为epoll可以实现并发，其实正确的说法是借助epoll可以实现高性能并发服务器，epoll只是提供了IO复用，在IO复用，真正的并发只能通过线程进程实现。

## 4.Reactor模式：

Reactor模式实现非常简单，使用同步IO模型，即业务线程处理数据需要**主动等待或询问**，主要特点是利用epoll监听listen描述符是否有响应，及时将**客户连接**信息**放于一个队列**，epoll和队列都是在主进程/线程中，由子进程/线程来接管各个描述符，对描述符进行下一步操作，包括connect和数据读写。主程读写就绪事件。

大致流程图如下：

![img](https://pic2.zhimg.com/80/v2-ff471b48d8e48554f93eae27e08f9e49_720w.webp)

Preactor模式：

Preactor模式完全将IO处理和业务分离，使用异步IO模型，即**内核完成数据处理后主动通知给应用处理**，主进程/线程不仅要完成listen任务，还需要完成内核数据缓冲区的映射，直接将数据buff传递给业务线程，业务线程只需要处理业务逻辑即可。

大致流程如下：

![img](https://pic4.zhimg.com/80/v2-22447cac5c8bb8e44c801cc38335140b_720w.webp)

## 5.封装Epoll实现reactor模式的高性能并发服务器

### 5.1.epoll的api

首先介绍epoll的api

```
int epoll_create(int size);  // 创建epfdint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); //向epfd注册(fd,event)// epoll_event结构体定义struct epoll_event { __uint32_t events; /* epoll 事件 */ epoll_data_t data; /* 传递的数据,用于处理ready的fd,获得上下文关系 */}// data联合体,一般用其指针域ptr,因为要从这个data读取到上下文信息typedef union epoll_data {    void *ptr;    int fd;    uint32_t u32;    uint64_t u64;} epoll_data_t;// op宏 /* EPOLL_CTL_ADD(注册新的fd到epfd) * EPOLL_CTL_MOD(修改已经注册的fd的监听事件) * EPOLL_CTL_DEL(epfd删除一个fd) */* * events : {EPOLLIN, EPOLLOUT, EPOLLPRI,             EPOLLHUP, EPOLLET, EPOLLONESHOT} */int epoll_wait(int epfd, struct epoll_event *event,             int maxevents, int timeout);  //用于轮询注册的fd,若满足相应的注册事件,                                           //  则结束epoll_wait阻塞/* @param timeout 超时时间 *     -1: 永久阻塞 *     0: 立即返回，非阻塞 *     >0: 指定微秒*/
```

### 5.2.Reactor模式:

io模式的历程:

单线程,一般阻塞->多线程,一般阻塞（一条连接一线程）->线程池(减少线程创建销毁开销)->reactor(更小粒度的线程)

所谓更小的粒度的线程是指,传统的多线程是一个连接一个线程,粒度太大,比如可以把一个连接继续细分成三个步骤:read,process,send三个步骤,每个步骤占一个线程,处理完后交给主线程调度,进入下一个处理模块

### 5.3.EPOLL实现的要点

\0. 创建epoll_fd = epoll_create(MAX_EVENT+1)

\1. 维护event数组

1.1 一个交给epoll_wait维护(空的放进去,ready的出来)

1.1.1 特定的结构体 struct epoll_event

1.2 一个自己维护(维持对所有注册事件的监控)(全局)

1.2.1 由于自己维护,想怎么写怎么写,一般的会让epoll_event.data.ptr = myevent,以作为回调函数的参数,保持一个上下文关系

\2. 创建并初始化事件(epoll_event)

2.1 结构体有两个字段 event和data , event是EPOLLIN这样的宏,data的作用是记录一些消息,这样ready的时候可以访问这个消息,比如fd, 回调函数指针,status等等

\3. 向epfd注册事件

3.1 epoll_ctl(epfd , op_macro , target_fd , &epoll_event )

3.2 op_macro是代表epoll_ctl的类型,诸如EPOLL_CTL_ADD

3.3 target_fd是要监听的fd,比如tcp监听套接字ls_socket,epoll_event就是当事件发生时,epoll_wait()里面 event[]将会出现的结构体

\4. 写回调函数

4.1 这是reactor模式的重点.

4.2 典型的,检测到ls_socket可读后(epoll_wait不再阻塞),进入回调函数(通过event[i].data.ptr->callback),假如命名为accept_fn(),在这个函数简单来看要做的事就是

1.conn_socket = accept(ls_socket)

2.创建,初始化epoll_event并注册到epfd(当然还要加进自己维护的fd列表),**由于epoll是轮询的模式,需要将conn_socket用fcntl设为O_NONBLOCK,非阻塞.**

4.3 这个不像select,需要每次重新加入fd到列表里面,注册一次即可

4.4 conn_socket被通过事件EPOLLIN注册到epfd,下一步马上的,epoll_wait检测出conn_socket可读(假设确实可读),然后回调进入recvdata函数(需要在创建并初始化epoll_*event指定, 实际上是自定义一个结构体,让data.ptr指向这个结构体就行),一般的,就以这个结构体组成自己维护的fd list,*

*4.5 recvdata函数首先*调用recv(fd,my_event->buf,…),把待发送的数据存在my_event->buf , 然后修改(fd,my*event)到epfd为发送事件EPOLLOUT,以及改变回调函数为callback_send*

4.6 来到epoll_wait,检测到该fd可写,则回调进入senddata函数,该函数调用send(fd,my*event->buf)发送数据,然后修改fd的注册事件为EPOLLIN(即EPOLL_CTLMOD),清空my_*event->buf….如此反复

注意点:

\1. callback函数如何获得my_event? 我们在epoll_event中能获得event和data,在data.ptr中找到my_event,典型的my_event可能包括回调函数指针,event_type,fd,buf[BUFLEN],last_active_time,…

```
nfd = epoll_wait(epfd , epoll_events , MAX_EVENT+1 , 1000)assert(nfd>0)for i in range(nfd):  my_event* ev = (my_event*) epoll_events[i].data.ptr  // if (epoll_events[i].events& EPOLLIN){}  用按位与的方式检测相等,因为这种flag宏一般都是位不相同的                                    // 用回调函数的方法时,不需要比较event_type,直接调用函数即可  // ev->callback_fn(ev)  不可以    callback_fn的原型应该是void(*callvback_fn)(void*),                                  // 因为定义callback原型的时候,ev还是不完整的类型  ev->callback_fn((void*)ev)
```

原文链接：https://zhuanlan.zhihu.com/p/373963070

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.221】实例分析Linux内存泄漏检测方法

## **1.mtrace分析内存泄露**

mtrace（memory trace），是 GNU Glibc 自带的内存问题检测工具，它可以用来协助定位内存泄露问题。它的实现源码在glibc源码的malloc目录下，其基本设计原理为设计一个函数 void mtrace ()，函数对 libc 库中的 malloc/free 等函数的调用进行追踪，由此来检测内存是否存在泄漏的情况。mtrace是一个C函數，在<mcheck.h>里声明及定义，函数原型为：

```text
void mtrace(void);
```

### **1.1 mtrace原理**

`mtrace() `函数会为那些和动态内存分配有关的函数（譬如 malloc()、realloc()、memalign() 以及 free()）安装 “钩子（hook）” 函数，这些 hook 函数会为我们记录所有有关内存分配和释放的跟踪信息，而 muntrace() 则会卸载相应的 hook 函数。基于这些 hook 函数生成的调试跟踪信息，我们就可以分析是否存在 “内存泄露” 这类问题了。

### **1.2 设置日志生成路径**

mtrace 机制需要我们实际运行一下程序，然后才能生成跟踪的日志，但在实际运行程序之前还有一件要做的事情是需要告诉 mtrace （即前文提到的 hook 函数）生成日志文件的路径。设置日志生成路径有两种，一种是设置环境变量：`export MALLOC_TRACE=./test.log // 当前目录下` 另一种是在代码层面设置：`setenv("MALLOC_TRACE", "output_file_name", 1);``output_file_name`就是储存检测结果的文件的名称。

### **1.3 测试实例**

```text
#include <mcheck.h>
#include <stdlib.h>
#include <stdio.h>

int main(int argc, char **argv)
{
    mtrace();  // 开始跟踪

    char *p = (char *)malloc(100);
    free(p);
    p = NULL;
    p = (char *)malloc(100);

    muntrace();   // 结束跟踪，并生成日志信息
    return 0;
}
```

从上述代码中，我们希望能够在程序开始到结束检查内存是否泄漏的问题，例子简单，一眼就能看出存在内存泄漏的问题，所以我们需要验证 mtrace 是否能够检查出来内存泄漏问题，且检查的结果如何分析定位。 `gcc -g test.c -o test`生成可执行文件。

### **1.4 日志**

程序运行结束，会在当前目录生成 test.log 文件，打开可以看到一下内容：

```text
= Start
@ ./test:[0x400624] + 0x21ed450 0x64
@ ./test:[0x400634] - 0x21ed450
@ ./test:[0x400646] + 0x21ed450 0x64
= End
```

从这个文件中可以看出中间三行分别对应源码中的 malloc -> free -> malloc 操作；解读：./test 指的是我们执行的程序名字，[0x400624] 是第一次调用 malloc 函数机器码中的地址信息，+ 表示申请内存（ - 表示释放），0x21ed450 是 malloc 函数申请到的地址信息，0x64 表示的是申请的内存大小。由此分析第一次申请已经释放，第二次申请没有释放，存在内存泄漏的问题。

### **1.5 泄露分析**

**使用addr2line工具定位源码位置**

通过使用 "addr2line" 命令工具，得到源文件的行数（通过这个可以根据机器码地址定位到具体源码位置）

```text
# addr2line -e test 0x400624
/home/test.c:9
```

**使用mtrace工具分析日志信息**

mtrace + 可执行文件路径 + 日志文件路径 `mtrace test ./test.log`执行，输出如下信息：

```text
Memory not freed:
-----------------
           Address     Size     Caller
0x00000000021ed450     0x64  at /home/test.c:14
```



## **2.Valgrind分析内存泄露**

### 2.1 **Valgrind工具介绍**

Valgrind是一套Linux下，开放源代码（GPL V2）的仿真调试工具的集合。Valgrind由内核（core）以及基于内核的其他调试工具组成。内核类似于一个框架（framework），它模拟了一个CPU环境，并提供服务给其他工具；而其他工具则类似于插件 (plug-in)，利用内核提供的服务完成各种特定的内存调试任务。Valgrind的体系结构如下图所示

![img](https://pic3.zhimg.com/80/v2-b9cfeec617d871c4623ffdb3c52a3352_720w.webp)

**1、Memcheck**

最常用的工具，用来检测程序中出现的内存问题，所有对内存的读写都会被检测到，一切对malloc() / free() / new / delete 的调用都会被捕获。所以，它能检测以下问题：对未初始化内存的使用；读/写释放后的内存块；读/写超出malloc分配的内存块；读/写不适当的栈中内存块；内存泄漏，指向一块内存的指针永远丢失；不正确的malloc/free或new/delete匹配；memcpy()相关函数中的dst和src指针重叠。

**2、Callgrind**

和 gprof 类似的分析工具，但它对程序的运行观察更是入微，能给我们提供更多的信息。和 gprof 不同，它不需要在编译源代码时附加特殊选项，但加上调试选项是推荐的。Callgrind 收集程序运行时的一些数据，建立函数调用关系图，还可以有选择地进行 cache 模拟。在运行结束时，它会把分析数据写入一个文件。callgrind_annotate 可以把这个文件的内容转化成可读的形式。

**3、Cachegrind**

Cache 分析器，它模拟 CPU 中的一级缓存 I1，Dl 和二级缓存，能够精确地指出程序中 cache 的丢失和命中。如果需要，它还能够为我们提供 cache 丢失次数，内存引用次数，以及每行代码，每个函数，每个模块，整个程序产生的指令数。这对优化程序有很大的帮助。

**4、Helgrind**

它主要用来检查多线程程序中出现的竞争问题。Helgrind 寻找内存中被多个线程访问，而又没有一贯加锁的区域，这些区域往往是线程之间失去同步的地方，而且会导致难以发掘的错误。Helgrind 实现了名为“Eraser”的竞争检测算法，并做了进一步改进，减少了报告错误的次数。不过，Helgrind 仍然处于实验阶段。

**5、Massif**

堆栈分析器，它能测量程序在堆栈中使用了多少内存，告诉我们堆块，堆管理块和栈的大小。Massif 能帮助我们减少内存的使用，在带有虚拟内存的现代系统中，它还能够加速我们程序的运行，减少程序停留在交换区中的几率。此外，lackey 和 nulgrind 也会提供。Lackey 是小型工具，很少用到；Nulgrind 只是为开发者展示如何创建一个工具。

### **2.1Memcheck原理**

本文的重点是在检测内存泄露，所以对于valgrind的其他工具不做过多的说明，主要说明下Memcheck的工作。Memcheck检测内存问题的原理如下图所示：

![img](https://pic3.zhimg.com/80/v2-21fb6d668ed503d540c206ab3fdf79f2_720w.webp)

Memcheck 能够检测出内存问题，关键在于其建立了两个全局表。

- Valid-Value 表 对于进程整个地址空间中的每一个字节(byte)，都有与之对应的 8个bits；对于 CPU 的每个寄存器，也有一个与之对应的 bit 向量。这些 bits 负责记录该字节或者寄存器值是否具有有效的、已初始化的值。
- Valid-Address 表 对于进程整个地址空间中的每一个字节(byte)，还有与之对应的1个 bit，负责记录该地址是否能够被读写。
- 检测原理：当要读写内存中某个字节时，首先检查这个字节对应的Valid-Address 表中的 A bit。如果该 A bit显示该位置是无效位置，memcheck 则报告读写错误。内核（core）类似于一个虚拟的 CPU 环境，这样当内存中的某个字节被加载到真实的 CPU 中时，该字节对应的Valid-Value 表中的 V bit 也被加载到虚拟的 CPU 环境中。一旦寄存器中的值，被用来产生内存地址，或者该值能够影响程序输出，则 memcheck 会检查对应的V bits，如果该值尚未初始化，则会报告使用未初始化内存错误。

### **2.2 内存泄露类型**

valgrind 将内存泄漏分成 4 类：

- 确立泄露（definitely lost）：运行内存还没有释放出来，但早已沒有表针偏向运行内存，运行内存早已不能浏览。确立泄露的运行内存是强烈要求修补的。
- 间接性泄露（indirectly lost）：泄露的运行内存表针储存在确立泄露的运行内存中，伴随着确立泄露的运行内存不能浏览，造成间接性泄露的运行内存也不能浏览。例如：

```text
struct list {
 struct list *next;
};

int main(int argc, char **argv)
{
 struct list *root;
 root = (struct list *)malloc(sizeof(struct list));
 root->next = (struct list *)malloc(sizeof(struct list));
 printf("root %p roop->next %p\n", root, root->next);
 root = NULL;
 return 0;
}
```

这里遗失的是 root 表针（是确立泄露类型），造成 root 储存的 next 表针变成了间接性泄露。间接性泄露的运行内存毫无疑问也要修补的，但是一般会伴随着 确立泄露 的修补而修补。

- 很有可能泄露（possibly lost）：表针并不偏向运行内存头详细地址，只是偏向运行内存內部的部位。valgrind 往往会猜疑很有可能泄露，是由于表针早已偏位，并沒有偏向运行内存头，只是有运行内存偏位，偏向运行内存內部的部位。有一些情况下，这并并不是泄露，由于这种程序流程便是那么设计方案的，比如为了更好地完成内存对齐，附加申请办理运行内存，回到两端对齐后的内存地址。
- 仍可访达（still reachable）：表针一直存有且偏向运行内存头顶部，直到程序流程撤出时运行内存还没有释放出来。

### **2.3 Valgrind参数设置**

- --leak-check=<no|summary|yes|full> 如果设为 yes 或 full，在被调程序结束后，valgrind 会详细叙述每一个内存泄露情况 默认是summary，只报道发生了几次内存泄露
- --log-file=
- --log-fd= [default: 2, stderr] valgrind 打印日志转存到指定文件或者文件描述符。如果没有这个参数，valgrind 的日志会连同用户程序的日志一起输出，会显得非常乱。
- --trace-children=<yes | no> [default: no] 是否跟踪子进程，若是多进程的程序，则建议使用这个功能。不过单进程使能了也不会有多大影响。
- --keep-debuginfo=<yes | no> [default: no] 如果程序有使用 动态加载库（dlopen），在动态库卸载时（dlclose），debug信息都会被清除。使能这个选项后，即使动态库被卸载，也会保留调用栈信息。
- --keep-stacktraces=<alloc | free | alloc-and-free | alloc-then-free | none> [default: alloc-and-free] 内存泄漏不外乎申请和释放不配对，函数调用栈是只在申请时记录，还是在申请释放时都记录 如果我们只关注内存泄漏，其实完全没必要申请释放都记录，因为这会占用非常多的额外内存和更多的 CPU 损耗，让本来就执行慢的程序雪上加霜。
- --freelist-vol= 当客户程序用 free 或 delete 释放一个内存块时，这个内存块不会立即可用于再分配，它只会被放在一个freed blocks的队列中（freelist）并被标记为不可访问，这样有利于探测到在一段很重要的时间后，客户程序又对被释放的块进行访问的错误。这个选项规定了队列所占的字节块大小，默认是20MB。增大这个选项的会增大memcheck的内存开销，但查这类错的能力也会提升。
- --freelist-big-blocks= 当从 freelist 队列中取可用内存块用于再分配时，memcheck 将会从那些比 number 大的内存块中按优先级取出一个块出来用。这个选项就防止了 freelist 中那些小的内存块的频繁调用，这个选项提高了 查到针对小内存块的野指针错误的几率。若这个选项设为0，则所有的块将按先进先出的原则用于再分配。默认是1M。参考：valgrind 简介(内存检查工具)

### **2.4 编译参数推荐**

为了更好地在出难题时要详尽打印出出去栈信息内容，实际上大家最好是在编译程序时加上 -g 选择项。如果有动态性载入的库，必须再加上 `--keep-debuginfo=yes `，不然假如发觉是动态性载入的库发生泄露，因为动态库被卸载掉了，造成找不到符号表。编码编译程序提升，不建议应用 -O2既之上。-O0很有可能会造成运作变慢，建议使用-O1。

### **2.5 检测实例说明**

**申请不释放内存**

```text
#include <stdlib.h>
#include <stdio.h>
void func()
{
  //只申请内存而不释放
    void *p=malloc(sizeof(int));
}
int main()
{
    func();
    return 0;
}
```

使用valgrind命令来执行程序同时输出日志到文件

```text
valgrind --log-file=valReport --leak-check=full --show-reachable=yes --leak-resolution=low ./a.out
```

参数说明：

- –log-file=valReport 是指定生成分析日志文件到当前执行目录中，文件名为valReport
- –leak-check=full 显示每个泄露的详细信息
- –show-reachable=yes 是否检测控制范围之外的泄漏，比如全局指针、static指针等，显示所有的内存泄露类型
- –leak-resolution=low 内存泄漏报告合并等级
- –track-origins=yes表示开启“使用未初始化的内存”的检测功能，并打开详细结果。如果没有这句话，默认也会做这方面的检测，但不会打印详细结果。执行输出后，报告解读，其中54017是指进程号，如果程序使用了多进程的方式来执行，那么就会显示多个进程的内容。

```text
==54017== Memcheck, a memory error detector
==54017== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==54017== Using Valgrind-3.15.0 and LibVEX; rerun with -h for copyright info
==54017== Command: ./a.out
==54017== Parent PID: 52130
```

第二段是对堆内存分配的总结信息，其中提到程序一共申请了1次内存，其中0次释放了，4 bytes被分配(`1 allocs, 0 frees, 4 bytes allocated`)。在head summary中，有该程序使用的总heap内存量，分配内存次数和释放内存次数，如果分配内存次数和释放内存次数不一致则说明有内存泄漏。

```text
==54017== HEAP SUMMARY:
==54017==   in use at exit: 4 bytes in 1 blocks
==54017==   total heap usage: 1 allocs, 0 frees, 4 bytes allocated
```

第三段的内容描述了内存泄露的具体信息，其中有一块内存占用4字节（`4 bytes in 1 blocks`），在调用malloc分配，调用栈中可以看到是func函数最后调用了malloc，所以这一个信息是比较准确的定位了我们泄露的内存是在哪里申请的。

```text
==54017== 4 bytes in 1 blocks are definitely lost in loss record 1 of 1
==54017==    at 0x4C29F73: malloc (vg_replace_malloc.c:309)
==54017==    by 0x40057E: func() (in /home/oceanstar/CLionProjects/Share/src/a.out)
==54017==    by 0x40058D: main (in /home/oceanstar/CLionProjects/Share/src/a.out)
```

最后这一段是总结，4字节为一块的内存泄露

```text
==54017== LEAK SUMMARY:
==54017==    definitely lost: 4 bytes in 1 blocks  // 确立泄露
==54017==    indirectly lost: 0 bytes in 0 blocks  // 间接性泄露
==54017==    possibly lost: 0 bytes in 0 blocks   // 很有可能泄露
==54017==    still reachable: 0 bytes in 0 blocks // 仍可访达
==54017==    suppressed: 0 bytes in 0 blocks
```

**读写越界**

```text
#include <stdio.h>
#include <iostream>
int main()
{
    int len = 5;
    int *pt = (int*)malloc(len*sizeof(int)); //problem1: not freed
    int *p = pt;
    for (int i = 0; i < len; i++){
        p++;
    }
    *p = 5; //problem2: heap block overrun
    printf("%d\n", *p); //problem3: heap block overrun
    // free(pt);
    return 0;
}
```

problem1: 指针pt申请了空间，但是没有释放; problem2: pt申请了5个int的空间，p经过5次循环已达到p[5]的位置, `*p = 5`时，访问越界（写越界）。(下面valgrind报告中 Invalid write of size 4)

```text
==58261== Invalid write of size 4
==58261==    at 0x400707: main (main.cpp:12)
==58261==  Address 0x5a23054 is 0 bytes after a block of size 20 alloc'd
==58261==    at 0x4C29F73: malloc (vg_replace_malloc.c:309)
==58261==    by 0x4006DC: main (main.cpp:7)
```

problem1: 读越界 (下面valgrind报告中 Invalid read of size 4 )

```text
==58261== Invalid read of size 4
==58261==    at 0x400711: main (main.cpp:13)
==58261==  Address 0x5a23054 is 0 bytes after a block of size 20 alloc'd
==58261==    at 0x4C29F73: malloc (vg_replace_malloc.c:309)
==58261==    by 0x4006DC: main (main.cpp:7)
```

**重复释放**

```text
#include <stdio.h>
#include <iostream>
int main()
{
    int *x;
    x = static_cast<int *>(malloc(8 * sizeof(int)));
    x = static_cast<int *>(malloc(8 * sizeof(int)));
    free(x);
    free(x);
    return 0;
}
```

报告如下，`Invalid free() / delete / delete[] / realloc()`

```text
==59602== Invalid free() / delete / delete[] / realloc()
==59602==    at 0x4C2B06D: free (vg_replace_malloc.c:540)
==59602==    by 0x4006FE: main (main.cpp:10)
==59602==  Address 0x5a230a0 is 0 bytes inside a block of size 32 free'd
==59602==    at 0x4C2B06D: free (vg_replace_malloc.c:540)
==59602==    by 0x4006F2: main (main.cpp:9)
==59602==  Block was alloc'd at
==59602==    at 0x4C29F73: malloc (vg_replace_malloc.c:309)
==59602==    by 0x4006E2: main (main.cpp:8)
```

**申请释放接口不匹配**

申请释放接口不匹配的报告如下，用malloc申请空间的指针用free释放；用new申请的空间用delete释放(`Mismatched free() / delete / delete []`)：

```text
==61950== Mismatched free() / delete / delete []
==61950==    at 0x4C2BB8F: operator delete[](void*) (vg_replace_malloc.c:651)
==61950==    by 0x4006E8: main (main.cpp:8)
==61950==  Address 0x5a23040 is 0 bytes inside a block of size 5 alloc'd
==61950==    at 0x4C29F73: malloc (vg_replace_malloc.c:309)
==61950==    by 0x4006D1: main (main.cpp:7)
```

**内存覆盖**

```text
int main()
{
    char str[11];
    for (int i = 0; i < 11; i++){
        str[i] = i;
    }
    memcpy(str + 1, str, 5);
    char x[5] = "abcd";
    strncpy(x + 2, x, 3);
}
```

问题出在memcpy上， 将str指针位置开始copy 5个char到str+1所指空间，会造成内存覆盖。strncpy也是同理。报告如下，`Source and destination overlap`：

```text
==61609== Source and destination overlap in memcpy(0x1ffefffe31, 0x1ffefffe30, 5)
==61609==    at 0x4C2E81D: memcpy@@GLIBC_2.14 (vg_replace_strmem.c:1035)
==61609==    by 0x400721: main (main.cpp:11)
==61609== 
==61609== Source and destination overlap in strncpy(0x1ffefffe25, 0x1ffefffe23, 3)
==61609==    at 0x4C2D453: strncpy (vg_replace_strmem.c:552)
==61609==    by 0x400748: main (main.cpp:14)
```

## **3.总结**

内存检测方式无非分为两种：

1、维护一个内存操作链表，当有内存申请操作时，将其加入此链表中，当有释放操作时，从申请操作从链表中移除。如果到程序结束后此链表中还有内容，说明有内存泄露了；如果要释放的内存操作没有在链表中找到对应操作，则说明是释放了多次。使用此方法的有内置的调试工具，Visual Leak Detecter，mtrace, memwatch, debug_new。

2、模拟进程的地址空间。仿照操作系统对进程内存操作的处理，在用户态下维护一个地址空间映射，此方法要求对进程地址空间的处理有较深的理解。因为Windows的进程地址空间分布不是开源的，所以模拟起来很困难，因此只支持Linux。采用此方法的是valgrind。

原文地址：https://zhuanlan.zhihu.com/p/597168559

作者：linux

# 【NO.222】Linux基础组件之无锁消息队列ypipe/yqueue详解

## 1.CAS定义

比较并交换（compare and swap,CAS），是原子操作的一种，可用于在多线程编程中实现不被打断的数据交换操作，从而避免多线程同时改写某一数据时由于执行顺序不确定性以及中断的不可预知性产生的数据不一致问题。该操作通过将内存中的值与指定数据进行比较，当数值一样时将内存中的数据替换为新值。

```text
bool CAS( int * pAddr, int nExpected, int nNew ) 
atomically 
{ 
	if ( *pAddr == nExpected ) 
	{ 
		*pAddr = nNew ; 
		return true ; 
	}
	else
		return false ; 
// 返回bool告知原子性交换是否成功
}
```

## 2.为什么需要无锁队列

锁引起的问题：

（1）cache损坏 / 失效
（2）在同步机制上的争抢队列
（3）动态内存分配

![img](https://pic4.zhimg.com/80/v2-d25627fddb63327da1711e0ea8fc880b_720w.webp)

## 3.有锁导致线程切换引发cache损坏

在保存和恢复上下午的过程中还隐藏了额外的开销：Cache中的数据会失效，因为它缓存的是将被换出的任务数据，这些数据对于新换进的任务是没有用的。

CPU的运行速度比主存快很多，所以大量的处理器时间被浪费在处理器与主存的数据传输上，因此，在处理器和主存之间引入Cache。Cache是一种速度更快但容量更小的内存(也更加昂贵),当处理器要访问主存中的数据时，这些数据首先被拷贝到Cache

中，因为这些数据在不久的将来可能又会被处理器访问。Cache misses对性能有非常大的影响，因为处理器访问Cache中的数据将比直接访问主存快得多。线程被频繁抢占产生的Cache损坏将导致应用程序性能下降。

![img](https://pic4.zhimg.com/80/v2-54716fc65cb07e0a55c37cb617511c3b_720w.webp)

## 4.在同步机制上的争抢队列

阻塞导致系统暂停当前的任务或使其进入睡眠状态（等待，不占用CPU资源），直到资源（例如锁机制）可用，被阻塞的任务才能解除阻塞状态（唤醒）。在一个负载较重的应用程序中使用这样的阻塞队列来在线程之间传递消息会导致严重的争用问题。也就是说，任务将大量的时间(睡眠，等待，唤醒)浪费在获得保护队列数据的互斥锁，而不是处理队列中的数据上。

非阻塞机制大展伸手的机会到了。任务之间不争抢任何资源，在队列中预定一个位置，然后在这个位置上插入或提取数据。这中机制使用了一种被称之为CAS(比较和交换)的特殊操作，这个特殊操作是一种特殊的指令，它可以原子的完成以下操作:它需要3个操作数m，A，B，其中m是一个内存地址，操作将m指向的内存中的内容与A比较，如果相等则将B写入到m指向的内存中并返回true，如果不相等则直接返回false。

## 5.动态内存分配

在多线程中，需要仔细考虑动态内存分配。当一个任务从堆中分配内存时，标准的内存分配机制会阻塞所有与这个任务共享地址空间的其他任务（进程中的其他线程）。这样做的原因是让处理更简单，且其工作很好。两个线程不会被分配到一块相同的地址的内存，因为它们没有办法同时执行分配请求。显然线程频繁分配内存会导致应用程序性能下降(必须注意,向标准队列或map插入数据的时候都会导致堆上的动态内存分配)。

## 6.无锁队列的实现

无锁队列由两个类构成：ypipe_t和yqueue_t。zeromq，最快的消息队列。

（1）适用于一读一写的应用场景，比如一个epool+线程池中每个线程绑定一个唯一的队列。

![img](https://pic1.zhimg.com/80/v2-47d6a9f4bd43b90bf62ac68f3a49bcd4_720w.webp)

（2）通过chunk模式批量分配结点，减少因为动态内存分配线程之间的互斥。写线程申请内存、读线程释放内存也会导致动态内存的互斥。

批量分配结点数量没有固定的，需要根据业务场景进行调节；一般设置比较大没有什么问题，设置小了相对容易会产生问题而已。

（3）通过spare_chunk的作用（消息队列水位局部性原理，一般消息数量在一个位置上下波动）来降低chunk的频繁分配和释放。

消息数量在一个位置上下波动时，已经读取元素的chunk不立即释放，而是放在spare_chunk存储，当下一次需要分配chunk时，检查spare_chunk（如果有保存chunk就复用，没有再执行分配）。

![img](https://pic1.zhimg.com/80/v2-3f3e5e9f5f53a766536eec12739ef3a0_720w.webp)

（4）通过预写机制，批量更新写入位置，减少CAS的调用（同时读写消息队列对于CAS是有竞争的）。

![img](https://pic1.zhimg.com/80/v2-90225c4681559fe3788ac2e6874e8cec_720w.webp)

（5）巧妙的唤醒机制。读端没有数据可读时可以进行wait状态；写端在写入数据时可以根据返回值获知写入数据前消息队列是否为空，如果写入之前为空则可以唤醒读端。注意wait是业务层的，无锁消息队列本身没有wait / notify机制。

## 7.ypipe_t无锁队列的使用

![img](https://pic4.zhimg.com/80/v2-0f0677c73bfdd0245fa0cbabbbac3cdb_720w.webp)

yqueue.write(count,false)，写入元素为count，false代表这次已经写完数据，true表示还没写完数据。

yquue.flush()使读端能看到更新后的数据；返回false表示刷新之前队列为空，可notify唤醒读端；返回true说明队列本身有数据。flush才真正调用CAS。

[yqueue.read](https://link.zhihu.com/?target=http%3A//yqueue.read)(&value)读取元素，返回true表示读到元素；返回false表示消息队列为空，可以让出CPU或者进入wait状态等待写端唤醒。

示例1：

```text
// ...

static int s_queue_item_num = 2000000; // 每个线程插入的元素个数
ypipe_t<int, 100> yqueue;
void *yqueue_producer_thread(void *argv)
{
	int count=0;
	for(int i=0;i<s_queue_item_num;)
	{
		yqueue.write(count,false);// write
		count=lxx_atomic_add(&s_count_push,1);//线程安全，原子操作
		i++;
		yqueue.flush();// 刷新
	}
	return NULL;
}

// ...
```

示例2：

```text
void *yqueue_producer_thread_batch(void *argv)
{
  int count = 0;
  int item_num = s_queue_item_num / 10;
  for (int i = 0; i < item_num;)
  {
    yqueue.write(count, true);  // 写true
    count = lxx_atomic_add(&s_count_push, 1);
    yqueue.write(count, true);
    count = lxx_atomic_add(&s_count_push, 1);
    yqueue.write(count, true);
    count = lxx_atomic_add(&s_count_push, 1);
    yqueue.write(count, true);
    count = lxx_atomic_add(&s_count_push, 1);
    yqueue.write(count, true);
    count = lxx_atomic_add(&s_count_push, 1);
    yqueue.write(count, true);
    count = lxx_atomic_add(&s_count_push, 1);
    yqueue.write(count, true);
    count = lxx_atomic_add(&s_count_push, 1);
    yqueue.write(count, true);
    count = lxx_atomic_add(&s_count_push, 1);
    yqueue.write(count, true);
    count = lxx_atomic_add(&s_count_push, 1);
    yqueue.write(count, false);   //最后一个元素 写false
    count = lxx_atomic_add(&s_count_push, 1);
    i++;
    yqueue.flush();//刷新
  }
  return NULL;
}
```

示例3：

```text
// ...

static int s_queue_item_num = 2000000; // 每个线程插入的元素个数
std::mutex ypipe_mutex_;
std::condition_variable ypipe_cond_;
ypipe_t<int, 100> yqueue;
void *yqueue_producer_thread(void *argv)
{
	int count=0;
	for(int i=0;i<s_queue_item_num;)
	{
		yqueue.write(count,false);// write
		count=lxx_atomic_add(&s_count_push,1);//线程安全，原子操作
		i++;
		if(!yqueue.flush())// 返回false，说明读端没有读到数据
		{
			std::unique_lock<std::mutex> lock(ypipe_mutex);
			// 注意，业务层自己实现notify，yqueue本身没有notyfy机制的
			ypipe_cond_.notify_one();
		}
	}
	std::unique_lock<std::mutex> lock(ypipe_mutex);
	ypipe_cond_.notify_one();
	return NULL;
}

// ...
```

## 8.源码分析原子操作函数

![img](https://pic2.zhimg.com/80/v2-14319e6a20998276d132beaabd18b449_720w.webp)

set函数，把私有成员ptr指针设置成参数ptr_的值，不是一个原子操作，需要使用者确保执行set过程没有其他线程使用ptr的值。

```text
// This class encapsulates several atomic operations on pointers. 
template <typename T> class atomic_ptr_t 
{
public: 
	inline void set (T *ptr_); //非原子操作 
	inline T *xchg (T *val_); //原子操作，设置一个新的值，然后返回旧的值 
	inline T *cas (T *cmp_, T *val_);//原子操作 
private: 
	volatile T *ptr; 
}
```

## 9.源码分析yqueue_t

yqueue_t是比ypipe_t更底层的类。用于消息队列结点元素存储；不涉及CAS。

### 9.1类接口和变量

```text
#ifndef __ZMQ_YQUEUE_HPP_INCLUDED__
#define __ZMQ_YQUEUE_HPP_INCLUDED__

#include <stdlib.h>
#include <stddef.h>

// #include "err.hpp"
#include "atomic_ptr.hpp"

//  即是yqueue_t一个结点可以装载N个T类型的元素， yqueue_t的一个结点是一个数组
template <typename T, int N>
class yqueue_t
{
public:
    //  创建队列.
    inline yqueue_t();

    //  销毁队列.
    inline ~yqueue_t();

    // 返回队列头部元素的引用，调用者可以通过该引用更新元素，结合pop实现出队列操作。
    inline T &front(); // 返回的是引用，是个左值，调用者可以通过其修改容器的值
    
    // 返回队列尾部元素的引用，调用者可以通过该引用更新元素，结合push实现插入操作。
    // 如果队列为空，该函数是不允许被调用。
    inline T &back(); // 返回的是引用，是个左值，调用者可以通过其修改容器的值
    
    //  Adds an element to the back end of the queue.
    inline void push();

    // 必须要保证队列不为空，参考ypipe_t的uwrite
    inline void unpush();

    //  Removes an element from the front end of the queue.
    inline void pop();

private:
    //  Individual memory chunk to hold N elements.
    // 链表结点称之为chunk_t
    struct chunk_t
    {
        T values[N]; //每个chunk_t可以容纳N个T类型的元素，以后就以一个chunk_t为单位申请内存
        chunk_t *prev;
        chunk_t *next;
    };

    //  Back position may point to invalid memory if the queue is empty,
    //  while begin & end positions are always valid. Begin position is
    //  accessed exclusively be queue reader (front/pop), while back and
    //  end positions are accessed exclusively by queue writer (back/push).
    chunk_t *begin_chunk; // 链表头结点
    int begin_pos;        // 起始点
    chunk_t *back_chunk;  // 队列中最后一个元素所在的链表结点
    int back_pos;         // 尾部
    chunk_t *end_chunk;   // 拿来扩容的，总是指向链表的最后一个结点
    int end_pos;

    //  People are likely to produce and consume at similar rates.  In
    //  this scenario holding onto the most recently freed chunk saves
    //  us from having to call malloc/free.
    atomic_ptr_t<chunk_t> spare_chunk; //空闲块（把所有元素都已经出队的块称为空闲块），读写线程的共享变量

    //  Disable copying of yqueue.
    yqueue_t(const yqueue_t &);
    const yqueue_t &operator=(const yqueue_t &);
};

#endif
```

### 9.2 数据结构和逻辑

```text
// 链表结点称之为chunk_t
struct chunk_t
{
    T values[N]; //每个chunk_t可以容纳N个T类型的元素，以后就以一个chunk_t为单位申请内存
    chunk_t *prev;
    chunk_t *next;
};
```

yqueue_t的实现，每次批量分配一批元素，减少内存的分配和释放，解决不断动态内存分配的问题。yqueue_t内部由一个个chunk构成，每个chunk保存N个元素；chunk_t是一个双向链表。

![img](https://pic4.zhimg.com/80/v2-205a7b33e506e6cca5511a9b813f18cb_720w.webp)

当队列空间不足时每次分配一个chunk_t，每个chunk_t能存储N个元素。

在数据出队列后，队列有多余空间的时候，回收的chunk也不是马上释放，而是根据局部性原理先回收到spare_chunk里面，当再次需要分配chunk_t的时候从spare_chunk中获取。

![img](https://pic2.zhimg.com/80/v2-466847b514e239c06376bb249b099981_720w.webp)

yqueue_t内部有三个chunk_t类型指针以及对应的索引位置：

begin_chunk/begin_pos：begin_chunk用于指向队列头的chunk，begin_pos用于指向队列第一个元素在当前chunk中的位置。

back_chunk/back_pos：back_chunk用于指向队列尾的chunk，back_pos用于指向队列最后一个元素在当前chunk的位置。

end_chunk/end_pos：由于chunk是批量分配的，所以end_chunk用于指向分配的最后一个chunk位置。



这里特别需要注意区分back_chunk/back_pos和end_chunk/end_pos的作用：

back_chunk/back_pos：对应的是元素存储位置。

end_chunk/end_pos：决定是否要分配chunk或者回收chunk。



示例：

![img](https://pic1.zhimg.com/80/v2-de2a91a1a10c4b9bba8191b2646814f4_720w.webp)

另外还有一个spare_chunk指针，用于保存释放的chunk指针，当需要再次分配chunk的时候，会首先查看这里，从这里分配chunk。这里使用了原子的cas操作来完成，利用了操作系统的局部性原理。

### 9.3 yqueue_t构造函数

```text
//  创建队列.
inline yqueue_t()
{
    begin_chunk = (chunk_t *)malloc(sizeof(chunk_t));
    alloc_assert(begin_chunk);
    begin_pos = 0;
    back_chunk = NULL; //back_chunk总是指向队列中最后一个元素所在的chunk，现在还没有元素，所以初始为空
    back_pos = 0;
    end_chunk = begin_chunk; //end_chunk总是指向链表的最后一个chunk
    end_pos = 0;
}
```

![img](https://pic4.zhimg.com/80/v2-8589d70f626be7770c88f77850c77e8b_720w.webp)

end_chunk总是指向最后分配的chunk，刚分配出来的chunk，end_pos也总是为0。

back_chunk需要chunk有元素插入的时候才指向对应的chunk。

### 9.4 front()和back()函数

这两个函数的作用与C++ STL queue的front和back函数相同的效果。

```text
//  Returns reference to the front element of the queue.
//  If the queue is empty, behaviour is undefined.
// 返回队列头部元素的引用，调用者可以通过该引用更新元素，结合pop实现出队列操作。
inline T &front() // 返回的是引用，是个左值，调用者可以通过其修改容器的值
{
    return begin_chunk->values[begin_pos];
}

//  Returns reference to the back element of the queue.
//  If the queue is empty, behaviour is undefined.
// 返回队列尾部元素的引用，调用者可以通过该引用更新元素，结合push实现插入操作。
// 如果队列为空，该函数是不允许被调用。
inline T &back() // 返回的是引用，是个左值，调用者可以通过其修改容器的值
{
    return back_chunk->values[back_pos];
}
```

这里的front()或者back()函数，需要注意的返回的是左值引用，我们可以修改其值。

对于先进后出队列而言：

begin_chunk->values[begin_pos]代表队列头可读元素， 读取队列头元素即是读取begin_pos位置的元素；

back_chunk->values[back_pos]代表队列尾可写元素，写入元素时则是更新back_pos位置的元素，要确保元素真正生效，还需要调用push函数更新back_pos的位置，避免下次更新的时候又是更新当前back_pos位置对应的元素。

### 9.5 push()函数

更新下一个元素写入位置，如果end_pos超过chunk的索引位置(==N)则申请一个chunk（先尝试从spare_chunk获取，如果为空再申请分配全新的chunk）。

**最终都是要更新end_chunk和end_pos。**

```text
//  Adds an element to the back end of the queue.
inline void push()
{
    back_chunk = end_chunk;
    back_pos = end_pos; //

    if (++end_pos != N) //end_pos!=N表明这个chunk节点还没有满
        return;

    chunk_t *sc = spare_chunk.xchg(NULL); // 为什么设置为NULL？ 因为如果把之前值取出来了则没有spare chunk了，所以设置为NULL
    if (sc)                               // 如果有spare chunk则继续复用它
    {
        end_chunk->next = sc;
        sc->prev = end_chunk;
    }
    else // 没有则重新分配
    {
        // static int s_cout = 0;
        // printf("s_cout:%d\n", ++s_cout);
        end_chunk->next = (chunk_t *)malloc(sizeof(chunk_t)); // 分配一个chunk
        alloc_assert(end_chunk->next);
        end_chunk->next->prev = end_chunk;  
    }
    end_chunk = end_chunk->next;
    end_pos = 0;
}
```

push()函数的使用：

（1）通过back()获取可写入位置，写入数据；

（2）**通过push()更新下一个可写位置**。

```text
// 写入数据，incomplete参数表示写入是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。
inline void write(const T &value_, bool incomplete_)
{
    //  Place the value to the queue, add new terminator element.
    queue.back() = value_;
    queue.push();

    //  Move the "flush up to here" poiter.
    if (!incomplete_)
    {
        f = &queue.back(); // 记录要刷新的位置
    }
    else
    {
        //  ...
    }
}
```

### 9.6 pop()函数

这里主要更新下一次读取的位置，并检测是否需要释放chunk（先保存到spare_chunk，然后检测spare_chunk返回值是否为空，如果返回值不为空说明之前有保存chunk，但我们只能保存一个chunk，所以把之前的chunk释放掉）

```text
//  Removes an element from the front end of the queue.
inline void pop()
{
    if (++begin_pos == N) // 删除满一个chunk才回收chunk
    {
        chunk_t *o = begin_chunk;
        begin_chunk = begin_chunk->next;
        begin_chunk->prev = NULL;
        begin_pos = 0;

        //  'o' has been more recently used than spare_chunk,
        //  so for cache reasons we'll get rid of the spare and
        //  use 'o' as the spare.
        chunk_t *cs = spare_chunk.xchg(o); //由于局部性原理，总是保存最新的空闲块而释放先前的空闲快
        free(cs);
    }
}
```

整个chunk的元素都被取出队列才去回收chunk，而且是把最后回收的chunk保存到spare_chunk，然后释放之前保存的chunk。

需要注意：

（1）pop掉的元素，其销毁工作交给调用者完成，即是pop前调用者需要通过front()接口读取并进行销毁（比如动态分配的对象）。

（2）空闲块的保存，要求是原子操作。因为闲块是读写线程的共享变量，因为在push中也使用了spare_chunk。

push()函数的使用：

（1）通过front()读取数据；

（2）读完数据后通过pop()更新下一个可读位置。

### 9.7 源码

```text
/*
    Copyright (c) 2007-2013 Contributors as noted in the AUTHORS file

    This file is part of 0MQ.

    0MQ is free software; you can redistribute it and/or modify it under
    the terms of the GNU Lesser General Public License as published by
    the Free Software Foundation; either version 3 of the License, or
    (at your option) any later version.

    0MQ is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Lesser General Public License for more details.

    You should have received a copy of the GNU Lesser General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.
*/

#ifndef __ZMQ_YQUEUE_HPP_INCLUDED__
#define __ZMQ_YQUEUE_HPP_INCLUDED__

#include <stdlib.h>
#include <stddef.h>

// #include "err.hpp"
#include "atomic_ptr.hpp"

//  yqueue is an efficient queue implementation. The main goal is
//  to minimise number of allocations/deallocations needed. Thus yqueue
//  allocates/deallocates elements in batches of N.
//
//  yqueue allows one thread to use push/back function and another one
//  to use pop/front functions. However, user must ensure that there's no
//  pop on the empty queue and that both threads don't access the same
//  element in unsynchronised manner.
//
//  T is the type of the object in the queue. 队列中元素的类型
//  N is granularity(粒度) of the queue (how many pushes have to be done till  actual memory allocation is required).
//  即是yqueue_t一个结点可以装载N个T类型的元素， yqueue_t的一个结点是一个数组
template <typename T, int N>
class yqueue_t
{
public:
    //  创建队列.
    inline yqueue_t()
    {
        begin_chunk = (chunk_t *)malloc(sizeof(chunk_t));
        alloc_assert(begin_chunk);
        begin_pos = 0;
        back_chunk = NULL; //back_chunk总是指向队列中最后一个元素所在的chunk，现在还没有元素，所以初始为空
        back_pos = 0;
        end_chunk = begin_chunk; //end_chunk总是指向链表的最后一个chunk
        end_pos = 0;
    }

    //  销毁队列.
    inline ~yqueue_t()
    {
        while (true)
        {
            if (begin_chunk == end_chunk)
            {
                free(begin_chunk);
                break;
            }
            chunk_t *o = begin_chunk;
            begin_chunk = begin_chunk->next;
            free(o);
        }

        chunk_t *sc = spare_chunk.xchg(NULL);
        free(sc);
    }

    //  Returns reference to the front element of the queue.
    //  If the queue is empty, behaviour is undefined.
    // 返回队列头部元素的引用，调用者可以通过该引用更新元素，结合pop实现出队列操作。
    inline T &front() // 返回的是引用，是个左值，调用者可以通过其修改容器的值
    {
        return begin_chunk->values[begin_pos];
    }

    //  Returns reference to the back element of the queue.
    //  If the queue is empty, behaviour is undefined.
    // 返回队列尾部元素的引用，调用者可以通过该引用更新元素，结合push实现插入操作。
    // 如果队列为空，该函数是不允许被调用。
    inline T &back() // 返回的是引用，是个左值，调用者可以通过其修改容器的值
    {
        return back_chunk->values[back_pos];
    }

    //  Adds an element to the back end of the queue.
    inline void push()
    {
        back_chunk = end_chunk;
        back_pos = end_pos; //

        if (++end_pos != N) //end_pos!=N表明这个chunk节点还没有满
            return;

        chunk_t *sc = spare_chunk.xchg(NULL); // 为什么设置为NULL？ 因为如果把之前值取出来了则没有spare chunk了，所以设置为NULL
        if (sc)                               // 如果有spare chunk则继续复用它
        {
            end_chunk->next = sc;
            sc->prev = end_chunk;
        }
        else // 没有则重新分配
        {
            // static int s_cout = 0;
            // printf("s_cout:%d\n", ++s_cout);
            end_chunk->next = (chunk_t *)malloc(sizeof(chunk_t)); // 分配一个chunk
            alloc_assert(end_chunk->next);
            end_chunk->next->prev = end_chunk;  
        }
        end_chunk = end_chunk->next;
        end_pos = 0;
    }

    //  Removes element from the back end of the queue. In other words
    //  it rollbacks last push to the queue. Take care: Caller is
    //  responsible for destroying the object being unpushed.
    //  The caller must also guarantee that the queue isn't empty when
    //  unpush is called. It cannot be done automatically as the read
    //  side of the queue can be managed by different, completely
    //  unsynchronised thread.
    // 必须要保证队列不为空，参考ypipe_t的uwrite
    inline void unpush()
    {
        //  First, move 'back' one position backwards.
        if (back_pos) // 从尾部删除元素
            --back_pos;
        else
        {
            back_pos = N - 1; // 回退到前一个chunk
            back_chunk = back_chunk->prev;
        }

        //  Now, move 'end' position backwards. Note that obsolete end chunk
        //  is not used as a spare chunk. The analysis shows that doing so
        //  would require free and atomic operation per chunk deallocated
        //  instead of a simple free.
        if (end_pos) // 意味着当前的chunk还有其他元素占有
            --end_pos;
        else
        {
            end_pos = N - 1; // 当前chunk没有元素占用，则需要将整个chunk释放
            end_chunk = end_chunk->prev;
            free(end_chunk->next);
            end_chunk->next = NULL;
        }
    }

    //  Removes an element from the front end of the queue.
    inline void pop()
    {
        if (++begin_pos == N) // 删除满一个chunk才回收chunk
        {
            chunk_t *o = begin_chunk;
            begin_chunk = begin_chunk->next;
            begin_chunk->prev = NULL;
            begin_pos = 0;

            //  'o' has been more recently used than spare_chunk,
            //  so for cache reasons we'll get rid of the spare and
            //  use 'o' as the spare.
            chunk_t *cs = spare_chunk.xchg(o); //由于局部性原理，总是保存最新的空闲块而释放先前的空闲快
            free(cs);
        }
    }

private:
    //  Individual memory chunk to hold N elements.
    // 链表结点称之为chunk_t
    struct chunk_t
    {
        T values[N]; //每个chunk_t可以容纳N个T类型的元素，以后就以一个chunk_t为单位申请内存
        chunk_t *prev;
        chunk_t *next;
    };

    //  Back position may point to invalid memory if the queue is empty,
    //  while begin & end positions are always valid. Begin position is
    //  accessed exclusively be queue reader (front/pop), while back and
    //  end positions are accessed exclusively by queue writer (back/push).
    chunk_t *begin_chunk; // 链表头结点
    int begin_pos;        // 起始点
    chunk_t *back_chunk;  // 队列中最后一个元素所在的链表结点
    int back_pos;         // 尾部
    chunk_t *end_chunk;   // 拿来扩容的，总是指向链表的最后一个结点
    int end_pos;

    //  People are likely to produce and consume at similar rates.  In
    //  this scenario holding onto the most recently freed chunk saves
    //  us from having to call malloc/free.
    atomic_ptr_t<chunk_t> spare_chunk; //空闲块（把所有元素都已经出队的块称为空闲块），读写线程的共享变量

    //  Disable copying of yqueue.
    yqueue_t(const yqueue_t &);
    const yqueue_t &operator=(const yqueue_t &);
};

#endif
```

## 10.源码分析ypipe_t

ypipe_t相对yqueue难理解。ypipe_t用于控制读写位置；这涉及到CAS的问题；读写存在临界点。ypipe_t在yqueue_t的基础上构建一个单写单读的无锁队列。

### 10.1类接口和变量

```text
#ifndef __ZMQ_YPIPE_HPP_INCLUDED__
#define __ZMQ_YPIPE_HPP_INCLUDED__

#include "atomic_ptr.hpp"
#include "yqueue.hpp"

//  Lock-free queue implementation.
//  Only a single thread can read from the pipe at any specific moment.
//  Only a single thread can write to the pipe at any specific moment.
//  T is the type of the object in the queue.
//  N is granularity of the pipe, i.e. how many items are needed to
//  perform next memory allocation.

template <typename T, int N>
class ypipe_t
{
public:
    //  Initialises the pipe.
    inline ypipe_t();

    //  The destructor doesn't have to be virtual. It is mad virtual
    //  just to keep ICC and code checking tools from complaining.
    inline virtual ~ypipe_t()
    {
    }

    //  Following function (write) deliberately copies uninitialised data
    //  when used with zmq_msg. Initialising the VSM body for
    //  non-VSM messages won't be good for performance.

#ifdef ZMQ_HAVE_OPENVMS
#pragma message save
#pragma message disable(UNINIT)
#endif

    //  Write an item to the pipe.  Don't flush it yet. If incomplete is
    //  set to true the item is assumed to be continued by items
    //  subsequently written to the pipe. Incomplete items are neverflushed down the stream.
    // 写入数据，incomplete参数表示写入是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。
    inline void write(const T &value_, bool incomplete_);

#ifdef ZMQ_HAVE_OPENVMS
#pragma message restore
#endif

    //  Pop an incomplete item from the pipe. Returns true is such
    //  item exists, false otherwise.
    inline bool unwrite(T *value_);

    //  Flush all the completed items into the pipe. Returns false if
    //  the reader thread is sleeping. In that case, caller is obliged to
    //  wake the reader up before using the pipe again.
    // 刷新所有已经完成的数据到管道，返回false意味着读线程在休眠，在这种情况下调用者需要唤醒读线程。
    // 批量刷新的机制， 写入批量后唤醒读线程；
    // 反悔机制 unwrite
    inline bool flush();

    //  Check whether item is available for reading.
    // 这里面有两个点，一个是检查是否有数据可读，一个是预取
    inline bool check_read();
    //  Reads an item from the pipe. Returns false if there is no value.
    //  available.
    inline bool read(T *value_);

    //  Applies the function fn to the first elemenent in the pipe
    //  and returns the value returned by the fn.
    //  The pipe mustn't be empty or the function crashes.
    inline bool probe(bool (*fn)(T &));

protected:
    //  Allocation-efficient queue to store pipe items.
    //  Front of the queue points to the first prefetched item, back of
    //  the pipe points to last un-flushed item. Front is used only by
    //  reader thread, while back is used only by writer thread.
    yqueue_t<T, N> queue;

    //  Points to the first un-flushed item. This variable is used
    //  exclusively by writer thread.
    T *w; //指向第一个未刷新的元素,只被写线程使用

    //  Points to the first un-prefetched item. This variable is used
    //  exclusively by reader thread.
    T *r; //指向第一个还没预提取的元素，只被读线程使用

    //  Points to the first item to be flushed in the future.
    T *f; //指向下一轮要被刷新的一批元素中的第一个

    //  The single point of contention between writer and reader thread.
    //  Points past the last flushed item. If it is NULL,
    //  reader is asleep. This pointer should be always accessed using
    //  atomic operations.
    atomic_ptr_t<T> c; //读写线程共享的指针，指向每一轮刷新的起点（看代码的时候会详细说）。当c为空时，表示读线程睡眠（只会在读线程中被设置为空）

    //  Disable copying of ypipe object.
    ypipe_t(const ypipe_t &);
    const ypipe_t &operator=(const ypipe_t &);
};

#endif
```

**核心要点：**

（1）T *w：指向第一个未刷新的元素，只被写线程使用；用来控制是否需要唤醒读端，当读端没有数据可以读取的时候，将c变量设为NULL。

（2）T *r：指向第一个还没有预取的元素，只被读线程使用；用来控制可读位置，（注意）这个r不是读位置的索引（读位置索引是begin_pos，可写位置索引是back_pos），而是读索引的位置等于r的时候说明队列已经为空。

（3）T *f：指向下一轮要被刷新的一批元素中的第一个；用来控制写入位置，当f被更新到c的时候读端才能看到写入的数据。

（4）atomic_ptr_t c：读写线程共享的指针，指向每一轮刷新的起点；当c为空时，表示读线程睡眠（只会在读线程中被设置为空）。

**主要接口：**

（1）void write (const T &value, bool incomplete)：写入数据，incomplete参数表示写入是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。

（2）bool flush ()：刷新所有已经完成的数据到管道，返回false意味着读线程在休眠，在这种情况下调用者需要唤醒读线程。

（3）bool read (T *value_):读数据，将读出的数据写入value指针中，返回false意味着没有数据可读。

### 10.2 ypipe_t()初始化

```text
//  Initialises the pipe.
inline ypipe_t()
{
    //  Insert terminator element into the queue.
    queue.push(); //yqueue_t的尾指针加1，开始back_chunk为空，现在back_chunk指向第一个chunk_t块的第一个位置

    //  Let all the pointers to point to the terminator.
    //  (unless pipe is dead, in which case c is set to NULL).
    r = w = f = &queue.back(); //就是让r、w、f、c四个指针都指向这个end迭代器
    c.set(&queue.back());
}
```

![img](https://pic1.zhimg.com/80/v2-73d4bae28965c1a4e16d77575173ec90_720w.webp)

### 10.3 write()函数

```text
// 写入数据，incomplete参数表示写入是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。
inline void write(const T &value_, bool incomplete_)
{
    //  Place the value to the queue, add new terminator element.
    queue.back() = value_;
    queue.push();//更新下一次写的位置

    //  Move the "flush up to here" poiter.
    if (!incomplete_)// 如果f不更新，flush的时候 read也是没有数据
    {
        f = &queue.back(); // 记录要刷新的位置
        // printf("1 f:%p, w:%p\n", f, w);
    }
    else
    {
        //  printf("0 f:%p, w:%p\n", f, w);
    }
}
```

![img](https://pic2.zhimg.com/80/v2-48c8725e3a9789ff9d840cdc8f4eb811_720w.webp)

write(val, false); 触发更新f的位置。f实际是back_pos的位置，即是下一次可以写入的位置。

### 10.4 cas()函数

cas函数，原子操作，线程安全，把私有成员ptr指针与参数cmp_指针比较：

（1）如果相等，就把ptr设置为参数val_的值，返回ptr设置之前的值；

（2）如果不相等直接返回ptr值。

```text
        //  Perform atomic 'compare and swap' operation on the pointer.
        //  The pointer is compared to 'cmp' argument and if they are
        //  equal, its value is set to 'val'. Old value of the pointer
        //  is returned.
        // 原来的值(ptr指向)如果和 comp_的值相同则更新为val_,并返回原来的ptr
        //   ○ 如果相等返回ptr设置之前的值，并把ptr更新为参数val_的值，；
        //   ○ 如果不相等直接返回ptr值。
        inline T *cas (T *cmp_, T *val_)//原子操作
        {
#if defined ZMQ_ATOMIC_PTR_ATOMIC_H
            return (T*) atomic_cas_ptr (&ptr, cmp_, val_);
#elif defined ZMQ_ATOMIC_PTR_TILE
            return (T*) arch_atomic_val_compare_and_exchange (&ptr, cmp_, val_);
#elif defined ZMQ_ATOMIC_PTR_X86
            T *old;
            __asm__ volatile (
                "lock; cmpxchg %2, %3"
                : "=a" (old), "=m" (ptr)
                : "r" (val_), "m" (ptr), "0" (cmp_)
                : "cc");
            return old;
#else
#error atomic_ptr is not implemented for this platform
#endif
        }
```

### 10.5 flush()函数

主要是将w更新到f位置（伴随着是否更新c），说明已经写到的位置。

```text
// 刷新所有已经完成的数据到管道，返回false意味着读线程在休眠，在这种情况下调用者需要唤醒读线程。
// 批量刷新的机制， 写入批量后唤醒读线程；
// 反悔机制 unwrite
inline bool flush()
{
    //  If there are no un-flushed items, do nothing.
    if (w == f) // 不需要刷新，即是还没有新元素加入
        return true;

    //  Try to set 'c' to 'f'.
    // read时如果没有数据可以读取则c的值会被置为NULL
    if (c.cas(w, f) != w) // 尝试将c设置为f，即是准备更新w的位置
    {
		// - 如果c==w，则更新f->c，并返回原来的c； 
		// - 如果c!=w， 则返回原来的c；
        //  Compare-and-swap was unseccessful because 'c' is NULL.
        //  This means that the reader is asleep. Therefore we don't
        //  care about thread-safeness and update c in non-atomic
        //  manner. We'll return false to let the caller know
        //  that reader is sleeping.
        c.set(f); // 更新w的位置
        w = f;
        return false; //线程看到flush返回false之后会发送一个消息给读线程，这需要写业务去做处理
    }
    else  // 读端还有数据可读取
    {
        //  Reader is alive. Nothing special to do now. Just move
        //  the 'first un-flushed item' pointer to 'f'.
        w = f;             // 只需要更新w的位置
        return true;
    }
}
```

flush()后w一定为f，w的作用主要是用来控制return false/true。

刷新之后，w、f、c、r的关系：

![img](https://pic4.zhimg.com/80/v2-1ce2f1922721c872f53c3178e1db48ff_720w.webp)

### 10.6 read()函数

r实际上是用来控制可以读取到的位置（**注意不是读到r，而是r的前一位置可以读取，r位置是不可以读取的**），当front和r重叠的时候说明没有数据可以读取。以此来检测是否有数据可以读取。

```text
//  Check whether item is available for reading.
// 这里面有两个点，一个是检查是否有数据可读，一个是预取
inline bool check_read()
{
    //  Was the value prefetched already? If so, return.
    if (&queue.front() != r && r) //判断是否在前几次调用read函数时已经预取数据了return true;
        return true;

    //  There's no prefetched value, so let us prefetch more values.
    //  Prefetching is to simply retrieve the
    //  pointer from c in atomic fashion. If there are no
    //  items to prefetch, set c to NULL (using compare-and-swap).
    // 两种情况
    // 1. 如果c值和queue.front()， 返回c值并将c值置为NULL，此时没有数据可读
    // 2. 如果c值和queue.front()， 返回c值，此时可能有数据度的去
    r = c.cas(&queue.front(), NULL); //尝试预取数据

    //  If there are no elements prefetched, exit.
    //  During pipe's lifetime r should never be NULL, however,
    //  it can happen during pipe shutdown when items are being deallocated.
    if (&queue.front() == r || !r) //判断是否成功预取数据
        return false;

    //  There was at least one value prefetched.
    return true;
}

//  Reads an item from the pipe. Returns false if there is no value.
//  available.
inline bool read(T *value_)
{
    //  Try to prefetch a value.
    if (!check_read())
        return false;

    //  There was at least one value prefetched.
    //  Return it to the caller.
    *value_ = queue.front();
    queue.pop();
    return true;
}
```

指针r指向队头元素【r==&queue.front()】或者r不指向任何数据（即NULL），说明队列中没有可读的数据；这个时候check_read()会尝试去预取数据（就是令 r=c）。而c在write中被指向f（见上图），这时从queue.front()到f这个位置的数据都被预取出来了，然后每次调用read都能取出一段。值得注意的是，当c==&queue.front()时，代表数据被取完了，这时把c指向NULL，接着读线程会睡眠，这也是给写线程检查读线程是否睡眠的标志。

继续上图的场景：

![img](https://pic1.zhimg.com/80/v2-bf75d1a04f107f8f9f6840a8a30d5308_720w.webp)

在 【[7.read](https://link.zhihu.com/?target=http%3A//7.read)(&ret),函数返回false,ret没有获取到值】的时候，front()和r相等。

（1）如果此时在r = c.cas(&queue.front(), NULL); 执行时没有flush的操作。则说明没有数据可以读取，最终返回false；

（2）如果在r = c.cas(&queue.front(), NULL); 之前写入方write新数据后并调用了flush，则r被更新，最终返回true。

### 10.7 源码

```text
/*
    Copyright (c) 2007-2013 Contributors as noted in the AUTHORS file

    This file is part of 0MQ.

    0MQ is free software; you can redistribute it and/or modify it under
    the terms of the GNU Lesser General Public License as published by
    the Free Software Foundation; either version 3 of the License, or
    (at your option) any later version.

    0MQ is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Lesser General Public License for more details.

    You should have received a copy of the GNU Lesser General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.
*/

#ifndef __ZMQ_YPIPE_HPP_INCLUDED__
#define __ZMQ_YPIPE_HPP_INCLUDED__

#include "atomic_ptr.hpp"
#include "yqueue.hpp"

//  Lock-free queue implementation.
//  Only a single thread can read from the pipe at any specific moment.
//  Only a single thread can write to the pipe at any specific moment.
//  T is the type of the object in the queue.
//  N is granularity of the pipe, i.e. how many items are needed to
//  perform next memory allocation.

template <typename T, int N>
class ypipe_t
{
public:
    //  Initialises the pipe.
    inline ypipe_t()
    {
        //  Insert terminator element into the queue.
        queue.push(); //yqueue_t的尾指针加1，开始back_chunk为空，现在back_chunk指向第一个chunk_t块的第一个位置

        //  Let all the pointers to point to the terminator.
        //  (unless pipe is dead, in which case c is set to NULL).
        r = w = f = &queue.back(); //就是让r、w、f、c四个指针都指向这个end迭代器
        c.set(&queue.back());
    }

    //  The destructor doesn't have to be virtual. It is mad virtual
    //  just to keep ICC and code checking tools from complaining.
    inline virtual ~ypipe_t()
    {
    }

    //  Following function (write) deliberately copies uninitialised data
    //  when used with zmq_msg. Initialising the VSM body for
    //  non-VSM messages won't be good for performance.

#ifdef ZMQ_HAVE_OPENVMS
#pragma message save
#pragma message disable(UNINIT)
#endif

    //  Write an item to the pipe.  Don't flush it yet. If incomplete is
    //  set to true the item is assumed to be continued by items
    //  subsequently written to the pipe. Incomplete items are neverflushed down the stream.
    // 写入数据，incomplete参数表示写入是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。
    inline void write(const T &value_, bool incomplete_)
    {
        //  Place the value to the queue, add new terminator element.
        queue.back() = value_;
        queue.push();

        //  Move the "flush up to here" poiter.
        if (!incomplete_)
        {
            f = &queue.back(); // 记录要刷新的位置
            // printf("1 f:%p, w:%p\n", f, w);
        }
        else
        {
            //  printf("0 f:%p, w:%p\n", f, w);
        }
    }

#ifdef ZMQ_HAVE_OPENVMS
#pragma message restore
#endif

    //  Pop an incomplete item from the pipe. Returns true is such
    //  item exists, false otherwise.
    inline bool unwrite(T *value_)
    {
        if (f == &queue.back())
            return false;
        queue.unpush();
        *value_ = queue.back();
        return true;
    }

    //  Flush all the completed items into the pipe. Returns false if
    //  the reader thread is sleeping. In that case, caller is obliged to
    //  wake the reader up before using the pipe again.
    // 刷新所有已经完成的数据到管道，返回false意味着读线程在休眠，在这种情况下调用者需要唤醒读线程。
    // 批量刷新的机制， 写入批量后唤醒读线程；
    // 反悔机制 unwrite
    inline bool flush()
    {
        //  If there are no un-flushed items, do nothing.
        if (w == f) // 不需要刷新，即是还没有新元素加入
            return true;

        //  Try to set 'c' to 'f'.
        // read时如果没有数据可以读取则c的值会被置为NULL
        if (c.cas(w, f) != w) // 尝试将c设置为f，即是准备更新w的位置
        {

            //  Compare-and-swap was unseccessful because 'c' is NULL.
            //  This means that the reader is asleep. Therefore we don't
            //  care about thread-safeness and update c in non-atomic
            //  manner. We'll return false to let the caller know
            //  that reader is sleeping.
            c.set(f); // 更新w的位置
            w = f;
            return false; //线程看到flush返回false之后会发送一个消息给读线程，这需要写业务去做处理
        }
        else  // 读端还有数据可读取
        {
            //  Reader is alive. Nothing special to do now. Just move
            //  the 'first un-flushed item' pointer to 'f'.
            w = f;             // 只需要更新w的位置
            return true;
        }
    }

    //  Check whether item is available for reading.
    // 这里面有两个点，一个是检查是否有数据可读，一个是预取
    inline bool check_read()
    {
        //  Was the value prefetched already? If so, return.
        if (&queue.front() != r && r) //判断是否在前几次调用read函数时已经预取数据了return true;
            return true;

        //  There's no prefetched value, so let us prefetch more values.
        //  Prefetching is to simply retrieve the
        //  pointer from c in atomic fashion. If there are no
        //  items to prefetch, set c to NULL (using compare-and-swap).
        // 两种情况
        // 1. 如果c值和queue.front()， 返回c值并将c值置为NULL，此时没有数据可读
        // 2. 如果c值和queue.front()， 返回c值，此时可能有数据度的去
        r = c.cas(&queue.front(), NULL); //尝试预取数据

        //  If there are no elements prefetched, exit.
        //  During pipe's lifetime r should never be NULL, however,
        //  it can happen during pipe shutdown when items are being deallocated.
        if (&queue.front() == r || !r) //判断是否成功预取数据
            return false;

        //  There was at least one value prefetched.
        return true;
    }

    //  Reads an item from the pipe. Returns false if there is no value.
    //  available.
    inline bool read(T *value_)
    {
        //  Try to prefetch a value.
        if (!check_read())
            return false;

        //  There was at least one value prefetched.
        //  Return it to the caller.
        *value_ = queue.front();
        queue.pop();
        return true;
    }

    //  Applies the function fn to the first elemenent in the pipe
    //  and returns the value returned by the fn.
    //  The pipe mustn't be empty or the function crashes.
    inline bool probe(bool (*fn)(T &))
    {
        bool rc = check_read();
        // zmq_assert(rc);

        return (*fn)(queue.front());
    }

protected:
    //  Allocation-efficient queue to store pipe items.
    //  Front of the queue points to the first prefetched item, back of
    //  the pipe points to last un-flushed item. Front is used only by
    //  reader thread, while back is used only by writer thread.
    yqueue_t<T, N> queue;

    //  Points to the first un-flushed item. This variable is used
    //  exclusively by writer thread.
    T *w; //指向第一个未刷新的元素,只被写线程使用

    //  Points to the first un-prefetched item. This variable is used
    //  exclusively by reader thread.
    T *r; //指向第一个还没预提取的元素，只被读线程使用

    //  Points to the first item to be flushed in the future.
    T *f; //指向下一轮要被刷新的一批元素中的第一个

    //  The single point of contention between writer and reader thread.
    //  Points past the last flushed item. If it is NULL,
    //  reader is asleep. This pointer should be always accessed using
    //  atomic operations.
    atomic_ptr_t<T> c; //读写线程共享的指针，指向每一轮刷新的起点（看代码的时候会详细说）。当c为空时，表示读线程睡眠（只会在读线程中被设置为空）

    //  Disable copying of ypipe object.
    ypipe_t(const ypipe_t &);
    const ypipe_t &operator=(const ypipe_t &);
};

#endif
```

## 11.总结

ypipe_t / yqueue_t无锁队列是单写单读，通过chunk机制避免频繁内存动态分配（内存分配或释放时，多个线程之间存在锁的竞争）。

ypipe_t / yqueue_t局部性原理，消息队列（概率性）在某一段时间（时间极短）可能存在波动，复用最近回收的chunk，提升效率。

flush()函数可以检测队列之前是否为空（目的是通知对端唤醒），flush()后就有数据可读了。

原文地址：https://zhuanlan.zhihu.com/p/596885618

作者：linux

# 【NO.233】Nginx 的异步非阻塞体现在哪里？从理论分析到源码验证

## 1.**理论分析**

1、首先要明确一点，这里讲的 “异步” 是业务层面上的。

2、那业务层面的异步是怎么个异步法？同步异步的概念我就不说了，前面文章有。异步最重要的标志就是通知，通知，通知！！！

这两天很累，不想多说话，长话短说吧： 以epoll为例，(nginx有提供select和poll的代码)，你可以同时监控很多个文件描述符，调用epoll是阻塞的，但是真实场景下不会让你有那个机会阻塞的。当有事件可读，就处理它。它准备了多少，就处理多少，当读写返回EAGAIN时，我们将它再次加入到epoll里面。等下次再可读了再出来被处理。只有当所有事件都没准备好时，才在epoll里面等着。

切换也是因为异步事件未准备好，而主动让出的。这里的切换是没有任何代价，你可以理解为循环处理多个准备好的事件，事实上就是这样的。

就这么个异步法，很高效。

定时器：nginx 借助 epoll_wait 的 timewait 设置超时时间，nginx 里面的定时器事件放在一颗维护定时器的红黑树里面，每次在进入epoll_wait 前，先从该红黑树里面拿到所有定时器事件的最小时间，在计算出 epoll_wait 的超时时间后进入 epoll_wait。所以，当没有事件产生，也没有中断信号时，epoll_wait会超时，也就是说，定时器事件到了。这时，nginx会检查所有的超时事件，将他们的状态设置为超时，然后再去处理网络事件。

## **2.源码体现**

（看了半天，感觉是 reactor 模型，下一个项目我要用这个！）

### 2.1 **worker进程对事件模块的初始化：**

事件模块的初始化就发生在ngx_worker_process_init函数中。

其调用关系：

```text
main()->
ngx_master_process_cycle()->
ngx_start_worker_processes()->
ngx_spawn_process()->
ngx_worker_process_cycle()->
ngx_worker_process_init()
```

在这个函数里面，调用了各个模块的启动方法：

```text
for (i = 0; ngx_modules[i]; i++) {
    if (ngx_modules[i]->init_process) {
        if (ngx_modules[i]->init_process(cycle) == NGX_ERROR) {
            /* fatal */
            exit(2);
        }
    }
}
```

在此处，会调用ngx_event_core_module的ngx_event_process_init函数。

这个函数（很长，很长）主要做了这么几件事情：

1、处理惊群问题（回头会专门来一篇讲惊群）

2、初始化两个队列，一个用于存放不能及时处理的建立连接事件，一个用于存储不能及时处理的读写事件。

```text
ngx_queue_init(&ngx_posted_accept_events);
ngx_queue_init(&ngx_posted_events);
```

3、初始化定时器。

4、调用 ngx_epoll_init 。

5、分配连接池空间、读事件结构体数组、写事件结构体数组。

6、为每个监听端口分配连接。

7、为每个监听端口的连接的读事件设置handler，并将每个监听端口的连接的读事件添加到epoll中。

### 2.2 **worker 开始循环干活了**

ngx_worker_process_cycle 函数（也不是很长，但是我不想放）。

不过这个我可以压缩一下，因为看了好几遍了，没上面那个那么恐怖：

```text
for ( ;; ) {
    ......
 
    /* 处理IO事件和时间事件 */
    ngx_process_events_and_timers(cycle);
    ......
}
```

短哈。

在worker的主循环中，所有的事件都是通过函数ngx_process_events_and_timers处理的，那我们自然就要再往下走了嘛，今天我还非要看看它到底是怎么吃一半了再塞回去的！

### **2.3 ngx_process_events_and_timers**

这个函数又干了些什么好事儿呢？

1、配置更新时间方式。是吧，原先我是不把定时器当回事儿的，但是这代码到处都是定时器，所以我就把定时器当回事儿了。

2、处理了一下惊群锁的事情。主要思想就是：负载过高咱就不抢，不然赶紧的冲上去。后面讲惊群的时候放这个代码。

3、调用事件处理函数ngx_process_events，epoll使用的是ngx_epoll_process_events函数。这个咱一会儿还得进去。

4、计算ngx_process_events函数的调用时间。

5、处理ngx_posted_accept_events队列的连接事件。这里accept事件的handler为ngx_event_accept。

6、处理定时器事件，具体操作是在定时器红黑树中查找过期的事件，调用其handler方法。

7、处理ngx_posted_events队列的读写事件，即遍历ngx_posted_events队列，调用事件的handler方法。

### **2.4 ngx_epoll_process_events**

```text
static ngx_int_t ngx_epoll_process_events(ngx_cycle_t *cycle, ngx_msec_t timer, ngx_uint_t flags)
{
······

	ngx_event_t *rev, *wev;

    /* 调用epoll_wait，从epoll中获取发生的事件 */
    events = epoll_wait(ep, event_list, (int) nevents, timer);
 
······
    /* 处理epoll_wait返回为-1的情况 */
    if (err) {
······
    }
 
    /* 若events返回为0，判断是因为epoll_wait超时还是其他原因 */
    if (events == 0) {
        if (timer != NGX_TIMER_INFINITE) {
            return NGX_OK;
        }
 
        ngx_log_error(NGX_LOG_ALERT, cycle->log, 0,
                      "epoll_wait() returned no events without timeout");
        return NGX_ERROR;
    }
 
    /* 对epoll_wait返回的链表进行遍历 */
    for (i = 0; i < events; i++) {
        c = event_list[i].data.ptr;
 
        /* 从data中获取connection & instance的值，并解析出instance和connection */
        instance = (uintptr_t) c & 1;
        c = (ngx_connection_t *) ((uintptr_t) c & (uintptr_t) ~1);
 
        /* 取出connection的read事件 */
        rev = c->read;
 
        /* 判断读事件是否过期 */
        if (c->fd == -1 || rev->instance != instance) {
            ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle->log, 0,
                           "epoll: stale event %p", c);
            continue;
        }
 
        /* 取出事件的类型 */
        revents = event_list[i].events;
 
        ngx_log_debug3(NGX_LOG_DEBUG_EVENT, cycle->log, 0,
                       "epoll: fd:%d ev:%04XD d:%p",
                       c->fd, revents, event_list[i].data.ptr);
 
        /* 若连接发生错误，则将EPOLLIN、EPOLLOUT添加到revents中，在调用读写事件时能够处理连接的错误 */
        if (revents & (EPOLLERR|EPOLLHUP)) {
            ngx_log_debug2(NGX_LOG_DEBUG_EVENT, cycle->log, 0,
                           "epoll_wait() error on fd:%d ev:%04XD",
                           c->fd, revents);
 
            revents |= EPOLLIN|EPOLLOUT;
        }
 
        /* 事件为读事件且读事件在epoll中 */
        if ((revents & EPOLLIN) && rev->active) {
 ······
 
            rev->ready = 1;
 
            /* 事件是否需要延迟处理？对于抢到锁监听端口的worker，会将事件延迟处理 */
            if (flags & NGX_POST_EVENTS) {
                /* 根据事件的是否是accept事件，加到不同的队列中 */
                queue = rev->accept ? &ngx_posted_accept_events
                                    : &ngx_posted_events;
 
                ngx_post_event(rev, queue);
 
            } else {
                /* 若不需要延迟处理，直接调用read事件的handler */
                rev->handler(rev);
            }
        }
 
        /* 取出connection的write事件 */
        wev = c->write;
 
        /* 事件为写事件且写事件在epoll中 */
        if ((revents & EPOLLOUT) && wev->active) {
 
            /* 判断写事件是否过期 */
            if (c->fd == -1 || wev->instance != instance) {
                ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle->log, 0,
                               "epoll: stale event %p", c);
                continue;
            }
 
            wev->ready = 1;
······
 
            /* 事件是否需要延迟处理？对于抢到锁监听端口的worker，会将事件延迟处理 */
            if (flags & NGX_POST_EVENTS) {
                ngx_post_event(wev, &ngx_posted_events);
 
            } else {
                /* 若不需要延迟处理，直接调用write事件的handler */
                wev->handler(wev);
            }
        }
    }
 
    return NGX_OK;
}
```

代码中可以看出，其中accept事件（即监听端口上的可读事件）会被缓存到队列ngx_posted_accept_events，普通事件会被缓存到队列ngx_posted_events。

缓存完事件，接下来就是处理新建连接事件（accept事件），因为当前进程已经监听了某个客户端的端口，该端口的请求中的可读事件先要处理下，该读的数据读完，即处理队列ngx_posted_accept_events中的新建连接事件，如果在处理新建连接期间还有新的请求连接事件，会阻塞，等待下次进程获取锁后读取。读完可读事件后就执行解锁操作ngx_shmtx_unlock。

锁释放完之后就处理连接套接口之后的连接事件了，即保存在队列ngx_posted_events中的事件。

### **2.5 ngx_event_process_posted**

```text
void ngx_event_process_posted(ngx_cycle_t *cycle, ngx_queue_t *posted)
{
    ngx_queue_t  *q;
    ngx_event_t  *ev;

    while (!ngx_queue_empty(posted)) {

        q = ngx_queue_head(posted);
        ev = ngx_queue_data(q, ngx_event_t, queue);

        ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle->log, 0,
                      "posted event %p", ev);

        ngx_delete_posted_event(ev);

        ev->handler(ev);
    }
}
```

可以看出，就是不断遍历队列，调用对应的handler处理事件。

原文地址：https://zhuanlan.zhihu.com/p/596770134

作者：linux

# 【NO.224】Redis：6.0多线程无锁(lock-free)设计和多线程Reactor模式助力Redis QPS更上一层楼

![img](https://pic2.zhimg.com/80/v2-60bcc53ba55dfb01e77c721699f6bd71_720w.webp)

**干货:**

1. 单线程模式-----并非CPU瓶颈
2. 多线程网络模型-----多线程Reactor模式
3. 多线程I/O-----lock-free无锁模式

因为我们的主题是多线程，所以不会过多涉及单线程。

## **1. 单线程模式-并非CPU瓶颈**

咱们都知道单线程的程序是没法利用服务器的多核CPU的，那么早期的Redis为何还要使用单线程呢？咱们不妨先看一下Redis官方给出的回答：

![img](https://pic3.zhimg.com/80/v2-3a4e16bd84875870d892fef144e9670e_720w.webp)

核心意思是：CPU并非制约Redis性能表现的瓶颈所在，更多状况下是受到内存大小和网络I/O的限制，因此Redis核心网络模型使用单线程并无什么问题，若是你想要使用服务的多核CPU，能够在一台服务器上启动多个实例或者采用分片集群的方式。

咱们知道Redis的I/O线程除了在等待事件，其它的事件都是非阻塞的，没有浪费任何的CPU时间，这也是Redis可以提供高性能服务的缘由。

## **2. 多线程网络模型-多线程Reactor模式**

Redis在 6.0 版本以后正式在核心网络模型中引入了多线程，它的工做模式是这样的：

![img](https://pic2.zhimg.com/80/v2-5441df4c0e45a9f411e76faf89a58a35_720w.webp)

区别于单 Reactor 模式，这种模式再也不是单线程的事件循环，而是有多个线程（Sub Reactors）各自维护一个独立的事件循环，由 Main Reactor 负责接收新链接并分发给 Sub Reactors 去独立处理，最后 Sub Reactors 回写响应给客户端。

`Multiple Reactors` 模式一般也能够等同于 `Master-Workers` 模式，好比`Nginx(前期文章有分享哈,可以回头去看)`等就是采用这种多线程模型，虽然不一样的项目实现细节略有区别，但整体来讲模式是一致的。

### **2.1 多线程工作流程**

![img](https://pic4.zhimg.com/80/v2-8b808b124769154d14e7a8ecd240063f_720w.webp)

1. Redis 服务器启动，开启主线程事件循环（Event Loop），注册 acceptTcpHandler 链接应答处理器到用户配置的监听端口对应的文件描述符，等待新链接到来；
2. 客户端和服务端创建网络链接；
3. acceptTcpHandler 被调用，主线程使用 AE 的 API 将 readQueryFromClient 命令读取处理器绑定到新链接对应的文件描述符上，并初始化一个 client 绑定这个客户端链接；
4. 客户端发送请求命令，触发读就绪事件，服务端主线程不会经过 socket 去读取客户端的请求命令，而是先将 client 放入一个 LIFO 队列 clients_pending_read；
5. 在事件循环（Event Loop）中，主线程执行 beforeSleep -->handleClientsWithPendingReadsUsingThreads，利用 Round-Robin 轮询负载均衡策略，把 clients_pending_read队列中的链接均匀地分配给 I/O 线程各自的本地 FIFO 任务队列 io_threads_list[id] 和主线程本身，并且用io_threads_pending[id]来记录每个线程的分配任务数量，因为线程需要读取这个io_threads_pending[id]这个数量来消费任务，消费完成会初始化为0。I/O 线程经过 socket 读取客户端的请求命令(是通过io_threads_op这个变量来判断是读(IO_THREADS_OP_READ)还是写(IO_THREADS_OP_WRITE)， 这里是io_threads_op == IO_THREADS_OP_READ)，存入 client->querybuf 并解析第一个命令，但不执行命令，主线程忙轮询，等待全部 I/O 线程完成读取任务；
6. 主线程和全部 I/O 线程都完成了读取任务(通过遍历io_threads_pending[id]，把每个线程的分配任务数量累加起来如果和等于0代表多线程已经消费完了任务)，主线程结束忙轮询，遍历 clients_pending_read 队列，执行全部客户端链接的请求命令，先调用 processCommandAndResetClient 执行第一条已经解析好的命令，而后调用 processInputBuffer 解析并执行客户端链接的全部命令，在其中使用 processInlineBuffer 或者 processMultibulkBuffer 根据 Redis 协议解析命令，最后调用 processCommand 执行命令；
7. 根据请求命令的类型（SET, GET, DEL, EXEC 等），分配相应的命令执行器去执行，最后调用 addReply 函数族的一系列函数将响应数据写入到对应 client 的写出缓冲区：client->buf 或者 client->reply ，client->buf 是首选的写出缓冲区，固定大小 16KB，通常来讲能够缓冲足够多的响应数据，可是若是客户端在时间窗口内须要响应的数据很是大，那么则会自动切换到 client->reply 链表上去，使用链表理论上可以保存无限大的数据（受限于机器的物理内存），最后把 client 添加进一个 LIFO 队列 clients_pending_write；
8. 在事件循环（Event Loop）中，主线程执行 beforeSleep --> handleClientsWithPendingWritesUsingThreads，利用 Round-Robin 轮询负载均衡策略，把 clients_pending_write 队列中的链接均匀地分配给 I/O 线程各自的本地 FIFO 任务队列 io_threads_list[id] 和主线程本身，并且用io_threads_pending[id]来记录每个线程的分配任务数量，因为线程需要读取这个io_threads_pending[id]这个数量来消费任务，消费完成会置为0。I/O 线程经过调用 writeToClient(io_threads_op == IO_THREADS_OP_WRITE)把 client 的写出缓冲区里的数据回写到客户端，主线程忙轮询，等待全部 I/O 线程完成写出任务；
9. 主线程和全部 I/O 线程都完成了写出任务(通过遍历io_threads_pending[id]，把每个线程的分配任务数量累加起来如果和等于0代表多线程已经消费完了任务)， 主线程结束忙轮询，遍历 clients_pending_write 队列，若是 client 的写出缓冲区还有数据遗留，则注册 sendReplyToClient 到该链接的写就绪事件，等待客户端可写时在事件循环中再继续回写残余的响应数据。

这里大部分逻辑和以前的单线程模型是一致的，变更的地方仅仅是把读取客户端请求命令和回写响应数据的逻辑异步化了，交给 I/O 线程去完成，这里须要特别注意的一点是：I/O 线程仅仅是读取和解析客户端命令而不会真正去执行命令，客户端命令的执行最终仍是要回到主线程上完成。

### **2.2 图解详细流程**

![img](https://pic3.zhimg.com/80/v2-397545336d30362438357f4871745b92_720w.webp)

## **3. I/O多线程lock-free（无锁设计）**

主线程和 I/O 线程之间共享的变量有三个：io_threads_pending 计数器、io_threads_op I/O 标识符和 io_threads_list 线程本地任务队列。

- io_threads_pending

- - 统计每个线程分配任务的数量，主线程在分配的时候，子线程是被换新的，它一直在执行100w次的CPU空转即自旋操作；当主线程把任务分配好之后，子线程会判断io_threads_pending[id]不为0就去消费任务，消费完成会置为0。

- io_threads_op

- - IO_THREADS_OP_WRITE socket可写
  - IO_THREADS_OP_READ socket可读

- io_threads_list

- - 线程本地任务队列(链表)

lock-free无锁设计核心：

\1. 原子变量，不需要加锁保护

- io_threads_pending变量在声明的时候加上了_Atomic限定符：_Atomic unsigned long; _Atomic是C11标准中引入的原子操作：被_Atomic修饰的变量被认为是原子变量，对原子变量的操作是不可分割的(Atomicity)，且操作结果对其他线程可见，执行的顺序也不能被重排。所以io_threads_pending是属于线程安全的变量。

\2. 交错访问来规避共享数据竞争

- io_threads_op 和 io_threads_list 这两个变量则是经过控制主线程和 I/O 线程交错访问来规避共享数据竞争问题。
- I/O 线程启动以后会经过忙轮询和锁休眠等待主线程的信号，在这以前它不会去访问本身的本地任务队列 io_threads_list[id]，而主线程会在分配完全部任务到各个 I/O 线程的本地队列以后才去唤醒 I/O 线程开始工做，而且主线程以后在 I/O 线程运行期间只会访问本身的本地任务队列 io_threads_list[0] 而不会再去访问 I/O 线程的本地队列，这也就保证了主线程永远会在 I/O 线程以前访问 io_threads_list 而且以后再也不访问，保证了交错访问。
- io_threads_op 同理，主线程会在唤醒 I/O 线程以前先设置好 io_threads_op 的值，而且在 I/O 线程运行期间不会再去访问这个变量，这也就变相保证了原子性。在源码src/server.h中 : extern int io_threads_op;

## **4. 源码**

源码真的太多了，拷贝到这里实在影响阅读，因此为了大家能迅速定位，我这里贴出地址哈。

- 子线程入口Main函数 处理read，write和解析操作

- - IOThreadMain：源码文件3665行
  - [https://github.com/redis/redis/blob/64f6159646337b4a3b56a400522ad4d028d55dac/src/networking.c](https://link.zhihu.com/?target=https%3A//github.com/redis/redis/blob/64f6159646337b4a3b56a400522ad4d028d55dac/src/networking.c)

- 主线程执行回复入口函数

- - handleClientsWithPendingWritesUsingThreads：3810行
  - 地址同上

- 主线程执行读取数据入口函数

- - handleClientsWithPendingReadsUsingThreads：3935行
  - 地址同上

- 主线程初始化多线程入口函数

- - initThreadedIO: 3712行
  - 地址同上

- 其他源码

- - readQueryFromClient：2266行
  - postponeClientRead：3908行
  - 地址同上

原文地址：https://zhuanlan.zhihu.com/p/595289451

作者：linux

# 【NO.225】从Reactor模式俯瞰Nginx，你会发现你与高手的差距就在设计模式上

![img](https://pic3.zhimg.com/80/v2-e8c90469b2de8240e12470a8d4d76932_720w.webp)

我们知道了Nginx是做什么的以及它为何如此高效，以至于全宇宙拿它来做负载均衡或者说web server。

但是如果你只是了解了使用和知道了原理就认为已经掌握了它，那只能说你肤浅了，原理和使用技能看看大家都知道了，没必要拿出去和别人拽，但凡你和别人说Nginx的epoll我清楚，Master-Worker是如何工作的，初级选手可能觉得你真牛，你真厉害，可是碰到高手了，你那最多只是熟悉了这个组件而已，你并没有多大的成长，而高手通过对Nginx的深入了解，他能发现其中的秘籍，这个秘籍可以帮助他触类旁通其他组件，从而用最短的时间掌握更多的技术，在互联网领域立足不败之地。

那么高手是如何通过对Nginx的学习使自己达到更高的境界呢？

设计模式

别笑，学会了设计模式，你就可以自豪的说出金庸先生在《倚天屠龙记》里九阳真经的口诀：`他强由他强，清风拂山岗；他横由他横，明月照大江`。他自狠来他自恶，我自一口真气足。懂得人都懂，不懂的人自悟。

原谅我说这么多，请开始正题`Reactor模式`。

## **1. Reactor模式介绍**

前面有写Reactor模式的文章，那篇文章主要是普及下概念，这次是重点介绍。

`Reactor 模式`，`是指通过一个或多个输入同时传递给服务处理器的服务请求的事件驱动处理模式`。`服务端程序处理传入多路请求，并将它们同步分派给请求对应的处理线程`，`Reactor 模式也叫 Dispatcher 模式`。

即I/O多路复用统一监听事件，收到事件后分发(Dispatch 给某进程)，是编写`高性能网络服务器`的必备技术之一。

Reactor 模式中有 2 个关键组成：

1. Reactor：Reactor在一个单独的线程中运行，负责监听和分发事件，分发给适当的处理程序来对 IO 事件做出反应。它就像公司的电话接线员，它接听来自客户的电话并将线路转移到适当的联系人；
2. Handlers：处理程序执行 I/O 事件要完成的实际事件，类似于客户想要与之交谈的公司中的实际官员。Reactor 通过调度适当的处理程序来响应 I/O 事件，处理程序执行非阻塞操作。

根据 Reactor 的数量和处理资源池线程的数量不同，有 3 种典型的实现：

1. 单 Reactor 单线程
2. 单 Reactor 多线程
3. 主从 Reactor 多线程

下面详细介绍这 3 种实现方式。

## **2. 单 Reactor 单线程**

![img](https://pic3.zhimg.com/80/v2-820e46b78809e89487f0b40cb97ac01a_720w.webp)

其中，Select是前面 I/O 复用模型介绍的标准网络编程API，可以实现应用程序通过一个阻塞对象监听多路连接请求，其他方案示意图类似。

方案说明：

1. Reactor 对象通过 Select 监控客户端请求事件，收到事件后通过 Dispatch 进行分发；
2. 如果是建立连接请求事件，则由 Acceptor 通过 Accept 处理连接请求，然后创建一个 Handler 对象处理连接完成后的后续业务处理；
3. 如果不是建立连接事件，则 Reactor 会分发调用连接对应的 Handler 来响应；
4. Handler 会完成 Read→业务处理→Send 的完整业务流程。

- 优点：模型简单，没有多线程、进程通信、竞争的问题，全部都在一个线程中完成（Reactor单线程）。
- 缺点：性能问题，只有一个线程，无法完全发挥多核 CPU 的性能。Handler 在处理某个连接上的业务时，整个进程无法处理其他连接事件，很容易导致性能瓶颈。

## **3. 单 Reactor 多线程**

![img](https://pic2.zhimg.com/80/v2-33d3739d51bcde43169f8b19b05edfa9_720w.webp)

方案说明：

1. Reactor 对象通过 Select 监控客户端请求事件，收到事件后通过 Dispatch 进行分发；
2. 如果是建立连接请求事件，则由 Acceptor 通过 Accept 处理连接请求，然后创建一个 Handler 对象处理连接完成后续的各种事件；
3. 如果不是建立连接事件，则 Reactor 会分发调用连接对应的 Handler 来响应；
4. Handler 只负责响应事件，不做具体业务处理，通过 Read 读取数据后，会分发给后面的 Worker 线程池进行业务处理；
5. Worker 线程池会分配独立的线程完成真正的业务处理，最后将响应结果发给 Handler 进行处理；
6. Handler 收到响应结果后通过 Send 将响应结果返回给 Client。

- 优点：可以充分利用多核 CPU 的处理能力。
- 缺点：多线程数据共享和访问比较复杂；Reactor 承担所有事件的监听和响应，在单线程中运行，高并发场景下容易成为性能瓶颈。

## **4. 主从 Reactor 多线程**

![img](https://pic2.zhimg.com/80/v2-8e13a79a1f313f0dd2fdabd2fa1b30f1_720w.webp)

针对单 Reactor 多线程模型中，Reactor 在单线程中运行，高并发场景下容易成为性能瓶颈，可以让 Reactor 在多线程中运行。

方案说明：

1. Reactor 主线程 MainReactor 对象通过 Select 监控建立连接事件，收到事件后通过 Acceptor 接收，处理建立连接事件；
2. Acceptor 处理建立连接事件后，MainReactor 将连接分配 Reactor 子线程给 SubReactor 进行处理；
3. SubReactor 将连接加入连接队列进行监听，并创建一个 Handler 用于处理各种连接事件；
4. 当有新的事件发生时，SubReactor 会调用连接对应的 Handler 进行响应；
5. Handler 通过 Read 读取数据后，会分发给后面的 Worker 线程池进行业务处理；
6. Worker 线程池会分配独立的线程完成真正的业务处理，最后将响应结果发给 Handler 进行处理；
7. Handler 收到响应结果后通过 Send 将响应结果返回给 Client。

- 优点：

1. 父线程与子线程的数据交互简单职责明确，父线程只需要接收新连接，子线程完成后续的业务处理。
2. 父线程与子线程的数据交互简单，Reactor 主线程只需要把新连接传给子线程，子线程无需返回数据。

缺点：代码写起来太复杂了：即编程复杂度较高。

## **5. Reactor小结**

3种模式可以用个比喻来理解：（餐厅常常雇佣接待员负责迎接顾客，当顾客入坐后，侍应生专门为这张桌子服务）

1. 单 Reactor 单线程，接待员和侍应生是同一个人，全程为顾客服务；
2. 单 Reactor 多线程，1 个接待员，多个侍应生，接待员只负责接待；
3. 主从 Reactor 多线程，多个接待员，多个侍应生。

Reactor 模式具有如下的优点：

- 响应快，不必为单个同步时间所阻塞，虽然 Reactor 本身依然是同步的；
- 编程相对简单，可以最大程度的避免复杂的多线程及同步问题，并且避免了多线程/进程的切换开销；
- 可扩展性，可以方便的通过增加 Reactor 实例个数来充分利用 CPU 资源；
- 可复用性，Reactor 模型本身与具体事件处理逻辑无关，具有很高的复用性。

## **6. Nginx利用的就是主从Reactor模式**

但它和主从reactor模式又有一定的区别，区别主要就是这个master进程，这个master进程不同于一般的主从式reactor(一般的主从式reactor设计会是主reactor负责将连接accept下来，然后再将连接fd挂载到子reactor中)，这个master进程的主要任务就是监听信号的，也就是对nginx的一些命令做处理，然后再将这些处理通过sockerpair()或者信号等方式通知给worker进程，master进程同时监控worker进程的运行状态，当worker进程退出后(异常情况下)，会自动重新启动新的worker进程。同时，这个master进程负责listen这个整个服务器的监听fd，然后worker进程通过竞争accept_mutex互斥锁来将连接从全连接队列里取出来，然后进行后续的事件循环处理。

也就是说`除了Master与主从Reactor中的主线程Reactor不同以外，Worker的处理流程和子线程Reactor的处理流程几乎一摸一样`，用陈硕老师的话来说就是`reactors in process`。

## **7. 小结**

知道了Reactor模式之后再回头想想看看哪些你了解的服务或者中间件使用了此模式，要学会触类旁通才能更胜一筹，进而成为高手。`这个模式还有一个最精妙的地方在于把复杂的问题通过"中间层"的方式简单化`。计算机界不是有句老话：“凡是服务不能通过现有常规技术手段解决的，就加一个中间层来解决”。此话真的一语点醒梦中人，Reactor主从模式把频繁的accept过程，其他fd的并发IO处理过程以及业务处理逻辑部分分层，通过加层的方式让整个模式快速而高效的运行起来，这就是智慧，人类的智慧。

原文地址：https://zhuanlan.zhihu.com/p/595184782

作者：linux

# 【NO.226】Redis 多线程网络模型全面揭秘

## 0.**导言**

在目前的技术选型中，Redis 俨然已经成为了系统高性能缓存方案的事实标准，因此现在 Redis 也成为了后端开发的基本技能树之一，Redis 的底层原理也顺理成章地成为了必须学习的知识。

Redis 从本质上来讲是一个网络服务器，而对于一个网络服务器来说，网络模型是它的精华，搞懂了一个网络服务器的网络模型，你也就搞懂了它的本质。

本文通过层层递进的方式，介绍了 Redis 网络模型的版本变更历程，剖析了其从单线程进化到多线程的工作原理，此外，还一并分析并解答了 Redis 的网络模型的很多抉择背后的思考，帮助读者能更深刻地理解 Redis 网络模型的设计。

## 1.**Redis 有多快？**

根据官方的 benchmark，通常来说，在一台普通硬件配置的 Linux 机器上跑单个 Redis 实例，处理简单命令（时间复杂度 O(N) 或者 O(log(N))），QPS 可以达到 8w+，而如果使用 pipeline 批处理功能，则 QPS 至高能达到 100w。

仅从性能层面进行评判，Redis 完全可以被称之为高性能缓存方案。

## 2.**Redis 为什么快？**

Redis 的高性能得益于以下几个基础：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRv4vcI9ozcWceomicZtwbhElPzboBJvwicvBh6GwHQ8phib2PkeLjQhBUQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **C 语言实现**，虽然 C 对 Redis 的性能有助力，但语言并不是最核心因素。
- **纯内存 I/O**，相较于其他基于磁盘的 DB，Redis 的纯内存操作有着天然的性能优势。
- **I/O 多路复用**，基于 epoll/select/kqueue 等 I/O 多路复用技术，实现高吞吐的网络 I/O。
- **单线程模型**，单线程无法利用多核，但是从另一个层面来说则避免了多线程频繁上下文切换，以及同步机制如锁带来的开销。

## 3.**Redis 为何选择单线程？**

Redis 的核心网络模型选择用单线程来实现，这在一开始就引起了很多人的不解，Redis 官方的对于此的回答是：

> It's not very frequent that CPU becomes your bottleneck with Redis, as usually Redis is either memory or network bound. For instance, using pipelining Redis running on an average Linux system can deliver even 1 million requests per second, so if your application mainly uses O(N) or O(log(N)) commands, it is hardly going to use too much CPU.

核心意思就是，对于一个 DB 来说，CPU 通常不会是瓶颈，因为大多数请求不会是 CPU 密集型的，而是 I/O 密集型。具体到 Redis 的话，如果不考虑 RDB/AOF 等持久化方案，Redis 是完全的纯内存操作，执行速度是非常快的，因此这部分操作通常不会是性能瓶颈，Redis 真正的性能瓶颈在于网络 I/O，也就是客户端和服务端之间的网络传输延迟，因此 Redis 选择了单线程的 I/O 多路复用来实现它的核心网络模型。

上面是比较笼统的官方答案，实际上更加具体的选择单线程的原因可以归纳如下：

### 3.1 避免过多的上下文切换开销

多线程调度过程中必然需要在 CPU 之间切换线程上下文 context，而上下文的切换又涉及程序计数器、堆栈指针和程序状态字等一系列的寄存器置换、程序堆栈重置甚至是 CPU 高速缓存、TLB 快表的汰换，如果是进程内的多线程切换还好一些，因为单一进程内多线程共享进程地址空间，因此线程上下文比之进程上下文要小得多，如果是跨进程调度，则需要切换掉整个进程地址空间。

如果是单线程则可以规避进程内频繁的线程切换开销，因为程序始终运行在进程中单个线程内，没有多线程切换的场景。

### 3.2 避免同步机制的开销

如果 Redis 选择多线程模型，又因为 Redis 是一个数据库，那么势必涉及到底层数据同步的问题，则必然会引入某些同步机制，比如锁，而我们知道 Redis 不仅仅提供了简单的 key-value 数据结构，还有 list、set 和 hash 等等其他丰富的数据结构，而不同的数据结构对同步访问的加锁粒度又不尽相同，可能会导致在操作数据过程中带来很多加锁解锁的开销，增加程序复杂度的同时还会降低性能。

### 3.3 简单可维护

Redis 的作者 Salvatore Sanfilippo (别称 antirez) 对 Redis 的设计和代码有着近乎偏执的简洁性理念，你可以在阅读 Redis 的源码或者给 Redis 提交 PR 的之时感受到这份偏执。因此代码的简单可维护性必然是 Redis 早期的核心准则之一，而引入多线程必然会导致代码的复杂度上升和可维护性下降。

事实上，多线程编程也不是那么尽善尽美，首先多线程的引入会使得程序不再保持代码逻辑上的串行性，代码执行的顺序将变成不可预测的，稍不注意就会导致程序出现各种并发编程的问题；其次，多线程模式也使得程序调试更加复杂和麻烦。网络上有一幅很有意思的图片，生动形象地描述了并发编程面临的窘境。

你期望的多线程编程 **VS** 实际上的多线程编程：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat1HcdK6LgF6qcXcENK8AcR4VVq0ITnfMdVzgm2mSYNFSzicDxz0bDBEB8Wzkwnu1hKRTYXK06SuLQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)你期望的多线程VS实际上的多线程

前面我们提到引入多线程必须的同步机制，如果 Redis 使用多线程模式，那么所有的底层数据结构都必须实现成线程安全的，这无疑又使得 Redis 的实现变得更加复杂。

总而言之，Redis 选择单线程可以说是多方博弈之后的一种权衡：在保证足够的性能表现之下，使用单线程保持代码的简单和可维护性。

## 4.**Redis 真的是单线程？**

在讨论这个问题之前，我们要先明确『单线程』这个概念的边界：它的覆盖范围是核心网络模型，抑或是整个 Redis？如果是前者，那么答案是肯定的，在 Redis 的 v6.0 版本正式引入多线程之前，其网络模型一直是单线程模式的；如果是后者，那么答案则是否定的，Redis 早在 v4.0 就已经引入了多线程。

因此，当我们讨论 Redis 的多线程之时，有必要对 Redis 的版本划出两个重要的节点：

1. Redis v4.0（引入多线程处理异步任务）
2. Redis v6.0（正式在网络模型中实现 I/O 多线程）

### 4.1 单线程事件循环

我们首先来剖析一下 Redis 的核心网络模型，从 Redis 的 v1.0 到 v6.0 版本之前，Redis 的核心网络模型一直是一个典型的单 Reactor 模型：利用 epoll/select/kqueue 等多路复用技术，在单线程的事件循环中不断去处理事件（客户端请求），最后回写响应数据到客户端：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRQjOP938GeNx5AUv8ibY0Yvpzn9g3g5AfImJqLqv5EQ5bAI27hZia4gug/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

这里有几个核心的概念需要学习：

- **client**：客户端对象，Redis 是典型的 CS 架构（Client <---> Server），客户端通过 **socket** 与服务端建立网络通道然后发送请求命令，服务端执行请求的命令并回复。**Redis** 使用结构体 **client** 存储客户端的所有相关信息，包括但不限于`封装的套接字连接 -- *conn`，`当前选择的数据库指针 -- *db`，`读入缓冲区 -- querybuf`，`写出缓冲区 -- buf`，`写出数据链表 -- reply`等。
- **aeApiPoll**：I/O 多路复用 API，是基于 epoll_wait/select/kevent 等系统调用的封装，监听等待读写事件触发，然后处理，它是事件循环（Event Loop）中的核心函数，是事件驱动得以运行的基础。
- **acceptTcpHandler**：连接应答处理器，底层使用系统调用 `accept` 接受来自客户端的新连接，并为新连接注册绑定命令读取处理器，以备后续处理新的客户端 TCP 连接；除了这个处理器，还有对应的 `acceptUnixHandler` 负责处理 Unix Domain Socket 以及 `acceptTLSHandler` 负责处理 TLS 加密连接。
- **readQueryFromClient**：命令读取处理器，解析并执行客户端的请求命令。
- **beforeSleep**：事件循环中进入 aeApiPoll 等待事件到来之前会执行的函数，其中包含一些日常的任务，比如把 `client->buf` 或者 `client->reply` （后面会解释为什么这里需要两个缓冲区）中的响应写回到客户端，持久化 AOF 缓冲区的数据到磁盘等，相对应的还有一个 afterSleep 函数，在 aeApiPoll 之后执行。
- **sendReplyToClient**：命令回复处理器，当一次事件循环之后写出缓冲区中还有数据残留，则这个处理器会被注册绑定到相应的连接上，等连接触发写就绪事件时，它会将写出缓冲区剩余的数据回写到客户端。

Redis 内部实现了一个高性能的事件库 --- AE，基于 epoll/select/kqueue/evport 四种事件驱动技术，实现 Linux/MacOS/FreeBSD/Solaris 多平台的高性能事件循环模型。Redis 的核心网络模型正式构筑在 AE 之上，包括 I/O 多路复用、各类处理器的注册绑定，都是基于此才得以运行。

至此，我们可以描绘出客户端向 Redis 发起请求命令的工作原理：

1. Redis 服务器启动，开启主线程事件循环（Event Loop），注册 `acceptTcpHandler` 连接应答处理器到用户配置的监听端口对应的文件描述符，等待新连接到来；
2. 客户端和服务端建立网络连接；
3. `acceptTcpHandler` 被调用，主线程使用 AE 的 API 将 `readQueryFromClient` 命令读取处理器绑定到新连接对应的文件描述符上，并初始化一个 `client` 绑定这个客户端连接；
4. 客户端发送请求命令，触发读就绪事件，主线程调用 `readQueryFromClient` 通过 socket 读取客户端发送过来的命令存入 `client->querybuf` 读入缓冲区；
5. 接着调用 `processInputBuffer`，在其中使用 `processInlineBuffer` 或者 `processMultibulkBuffer` 根据 Redis 协议解析命令，最后调用 `processCommand` 执行命令；
6. 根据请求命令的类型（SET, GET, DEL, EXEC 等），分配相应的命令执行器去执行，最后调用 `addReply` 函数族的一系列函数将响应数据写入到对应 `client` 的写出缓冲区：`client->buf` 或者 `client->reply` ，`client->buf` 是首选的写出缓冲区，固定大小 16KB，一般来说可以缓冲足够多的响应数据，但是如果客户端在时间窗口内需要响应的数据非常大，那么则会自动切换到 `client->reply` 链表上去，使用链表理论上能够保存无限大的数据（受限于机器的物理内存），最后把 `client` 添加进一个 LIFO 队列 `clients_pending_write`；
7. 在事件循环（Event Loop）中，主线程执行 `beforeSleep` --> `handleClientsWithPendingWrites`，遍历 `clients_pending_write` 队列，调用 `writeToClient` 把 `client` 的写出缓冲区里的数据回写到客户端，如果写出缓冲区还有数据遗留，则注册 `sendReplyToClient` 命令回复处理器到该连接的写就绪事件，等待客户端可写时在事件循环中再继续回写残余的响应数据。

对于那些想利用多核优势提升性能的用户来说，Redis 官方给出的解决方案也非常简单粗暴：在同一个机器上多跑几个 Redis 实例。事实上，为了保证高可用，线上业务一般不太可能会是单机模式，更加常见的是利用 Redis 分布式集群多节点和数据分片负载均衡来提升性能和保证高可用。

### 4.2 多线程异步任务

以上便是 Redis 的核心网络模型，这个单线程网络模型一直到 Redis v6.0 才改造成多线程模式，但这并不意味着整个 Redis 一直都只是单线程。

Redis 在 v4.0 版本的时候就已经引入了的多线程来做一些异步操作，此举主要针对的是那些非常耗时的命令，通过将这些命令的执行进行异步化，避免阻塞单线程的事件循环。

我们知道 Redis 的 `DEL` 命令是用来删除掉一个或多个 key 储存的值，它是一个阻塞的命令，大多数情况下你要删除的 key 里存的值不会特别多，最多也就几十上百个对象，所以可以很快执行完，但是如果你要删的是一个超大的键值对，里面有几百万个对象，那么这条命令可能会阻塞至少好几秒，又因为事件循环是单线程的，所以会阻塞后面的其他事件，导致吞吐量下降。

Redis 的作者 antirez 为了解决这个问题进行了很多思考，一开始他想的办法是一种渐进式的方案：利用定时器和数据游标，每次只删除一小部分的数据，比如 1000 个对象，最终清除掉所有的数据，但是这种方案有个致命的缺陷，如果同时还有其他客户端往某个正在被渐进式删除的 key 里继续写入数据，而且删除的速度跟不上写入的数据，那么将会无止境地消耗内存，虽然后来通过一个巧妙的办法解决了，但是这种实现使 Redis 变得更加复杂，而多线程看起来似乎是一个水到渠成的解决方案：简单、易理解。于是，最终 antirez 选择引入多线程来实现这一类非阻塞的命令。更多 antirez 在这方面的思考可以阅读一下他发表的博客：[Lazy Redis is better Redis](http://antirez.com/news/93)。

于是，在 Redis v4.0 之后增加了一些的非阻塞命令如 `UNLINK`、`FLUSHALL ASYNC`、`FLUSHDB ASYNC`。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRplfowbOAzxBrakDhbibteUuBibSRRfF3oiau3LamJnnsMCLCcyEzDU5ag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

`UNLINK` 命令其实就是 `DEL` 的异步版本，它不会同步删除数据，而只是把 key 从 keyspace 中暂时移除掉，然后将任务添加到一个异步队列，最后由后台线程去删除，不过这里需要考虑一种情况是如果用 `UNLINK` 去删除一个很小的 key，用异步的方式去做反而开销更大，所以它会先计算一个开销的阀值，只有当这个值大于 64 才会使用异步的方式去删除 key，对于基本的数据类型如 List、Set、Hash 这些，阀值就是其中存储的对象数量。

## 5.**Redis 多线程网络模型**

前面提到 Redis 最初选择单线程网络模型的理由是：CPU 通常不会成为性能瓶颈，瓶颈往往是**内存**和**网络**，因此单线程足够了。那么为什么现在 Redis 又要引入多线程呢？很简单，就是 Redis 的网络 I/O 瓶颈已经越来越明显了。

随着互联网的飞速发展，互联网业务系统所要处理的线上流量越来越大，Redis 的单线程模式会导致系统消耗很多 CPU 时间在网络 I/O 上从而降低吞吐量，要提升 Redis 的性能有两个方向：

- 优化网络 I/O 模块
- 提高机器内存读写的速度

后者依赖于硬件的发展，暂时无解。所以只能从前者下手，网络 I/O 的优化又可以分为两个方向：

- 零拷贝技术或者 DPDK 技术
- 利用多核优势

零拷贝技术有其局限性，无法完全适配 Redis 这一类复杂的网络 I/O 场景，更多网络 I/O 对 CPU 时间的消耗和 Linux 零拷贝技术，可以阅读我的另一篇文章：[Linux I/O 原理和 Zero-copy 技术全面揭秘](https://strikefreedom.top/linux-io-and-zero-copy)。而 DPDK 技术通过旁路网卡 I/O 绕过内核协议栈的方式又太过于复杂以及需要内核甚至是硬件的支持。

因此，利用多核优势成为了优化网络 I/O 性价比最高的方案。

6.0 版本之后，Redis 正式在核心网络模型中引入了多线程，也就是所谓的 *I/O threading*，至此 Redis 真正拥有了多线程模型。前一小节，我们了解了 Redis 在 6.0 版本之前的单线程事件循环模型，实际上就是一个非常经典的 Reactor 模型：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcR5VNqhl8TpY2iaCiaED7ic8ZkgMaNfUAfYa6onXs6cRsoREiblYL3icf6pog/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

目前 Linux 平台上主流的高性能网络库/框架中，大都采用 Reactor 模式，比如 netty、libevent、libuv、POE(Perl)、Twisted(Python)等。

Reactor 模式本质上指的是使用 `I/O 多路复用(I/O multiplexing) + 非阻塞 I/O(non-blocking I/O)` 的模式。

更多关于 Reactor 模式的细节可以参考我之前的文章：[Go netpoller 原生网络模型之源码全面揭秘](https://strikefreedom.top/go-netpoll-io-multiplexing-reactor)，Reactor 网络模型那一小节，这里不再赘述。

Redis 的核心网络模型在 6.0 版本之前，一直是单 Reactor 模式：所有事件的处理都在单个线程内完成，虽然在 4.0 版本中引入了多线程，但是那个更像是针对特定场景（删除超大 key 值等）而打的补丁，并不能被视作核心网络模型的多线程。

通常来说，单 Reactor 模式，引入多线程之后会进化为 Multi-Reactors 模式，基本工作模式如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRJLN1JHqxe4n2jQvvLYDSAeH1iax9Bsb5VqC0ZAATlE6y97xoOe4ibtiaw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

区别于单 Reactor 模式，这种模式不再是单线程的事件循环，而是有多个线程（Sub Reactors）各自维护一个独立的事件循环，由 Main Reactor 负责接收新连接并分发给 Sub Reactors 去独立处理，最后 Sub Reactors 回写响应给客户端。

Multiple Reactors 模式通常也可以等同于 Master-Workers 模式，比如 Nginx 和 Memcached 等就是采用这种多线程模型，虽然不同的项目实现细节略有区别，但总体来说模式是一致的。

### 5.1 设计思路

Redis 虽然也实现了多线程，但是却不是标准的 Multi-Reactors/Master-Workers 模式，这其中的缘由我们后面会分析，现在我们先看一下 Redis 多线程网络模型的总体设计：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRGtQ2jB57bdFbiawdd4krQVfNibfYlicxyYkLjkMPdTgH8ep6Av8jniaSsA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

1. Redis 服务器启动，开启主线程事件循环（Event Loop），注册 `acceptTcpHandler` 连接应答处理器到用户配置的监听端口对应的文件描述符，等待新连接到来；
2. 客户端和服务端建立网络连接；
3. `acceptTcpHandler` 被调用，主线程使用 AE 的 API 将 `readQueryFromClient` 命令读取处理器绑定到新连接对应的文件描述符上，并初始化一个 `client` 绑定这个客户端连接；
4. 客户端发送请求命令，触发读就绪事件，服务端主线程不会通过 socket 去读取客户端的请求命令，而是先将 `client` 放入一个 LIFO 队列 `clients_pending_read`；
5. 在事件循环（Event Loop）中，主线程执行 `beforeSleep` -->`handleClientsWithPendingReadsUsingThreads`，利用 Round-Robin 轮询负载均衡策略，把 `clients_pending_read`队列中的连接均匀地分配给 I/O 线程各自的本地 FIFO 任务队列 `io_threads_list[id]` 和主线程自己，I/O 线程通过 socket 读取客户端的请求命令，存入 `client->querybuf` 并解析第一个命令，**但不执行命令**，主线程忙轮询，等待所有 I/O 线程完成读取任务；
6. 主线程和所有 I/O 线程都完成了读取任务，主线程结束忙轮询，遍历 `clients_pending_read` 队列，**执行所有客户端连接的请求命令**，先调用 `processCommandAndResetClient` 执行第一条已经解析好的命令，然后调用 `processInputBuffer` 解析并执行客户端连接的所有命令，在其中使用 `processInlineBuffer` 或者 `processMultibulkBuffer` 根据 Redis 协议解析命令，最后调用 `processCommand` 执行命令；
7. 根据请求命令的类型（SET, GET, DEL, EXEC 等），分配相应的命令执行器去执行，最后调用 `addReply` 函数族的一系列函数将响应数据写入到对应 `client` 的写出缓冲区：`client->buf` 或者 `client->reply` ，`client->buf` 是首选的写出缓冲区，固定大小 16KB，一般来说可以缓冲足够多的响应数据，但是如果客户端在时间窗口内需要响应的数据非常大，那么则会自动切换到 `client->reply` 链表上去，使用链表理论上能够保存无限大的数据（受限于机器的物理内存），最后把 `client` 添加进一个 LIFO 队列 `clients_pending_write`；
8. 在事件循环（Event Loop）中，主线程执行 `beforeSleep` --> `handleClientsWithPendingWritesUsingThreads`，利用 Round-Robin 轮询负载均衡策略，把 `clients_pending_write` 队列中的连接均匀地分配给 I/O 线程各自的本地 FIFO 任务队列 `io_threads_list[id]` 和主线程自己，I/O 线程通过调用 `writeToClient` 把 `client` 的写出缓冲区里的数据回写到客户端，主线程忙轮询，等待所有 I/O 线程完成写出任务；
9. 主线程和所有 I/O 线程都完成了写出任务， 主线程结束忙轮询，遍历 `clients_pending_write` 队列，如果 `client` 的写出缓冲区还有数据遗留，则注册 `sendReplyToClient` 到该连接的写就绪事件，等待客户端可写时在事件循环中再继续回写残余的响应数据。

这里大部分逻辑和之前的单线程模型是一致的，变动的地方仅仅是把读取客户端请求命令和回写响应数据的逻辑异步化了，交给 I/O 线程去完成，这里需要特别注意的一点是：**I/O 线程仅仅是读取和解析客户端命令而不会真正去执行命令，客户端命令的执行最终还是要在主线程上完成**。

### 5.2 源码剖析

> 以下所有代码基于目前最新的 [Redis v6.0.10](https://github.com/redis/redis/tree/6.0.10) 版本。

#### 5.2.1 **多线程初始化**

```
void initThreadedIO(void) {
    server.io_threads_active = 0; /* We start with threads not active. */

    // 如果用户只配置了一个 I/O 线程，则不会创建新线程（效率低），直接在主线程里处理 I/O。
    if (server.io_threads_num == 1) return;

    if (server.io_threads_num > IO_THREADS_MAX_NUM) {
        serverLog(LL_WARNING,"Fatal: too many I/O threads configured. "
                             "The maximum number is %d.", IO_THREADS_MAX_NUM);
        exit(1);
    }

    // 根据用户配置的 I/O 线程数，启动线程。
    for (int i = 0; i < server.io_threads_num; i++) {
        // 初始化 I/O 线程的本地任务队列。
        io_threads_list[i] = listCreate();
        if (i == 0) continue; // 线程 0 是主线程。

        // 初始化 I/O 线程并启动。
        pthread_t tid;
        // 每个 I/O 线程会分配一个本地锁，用来休眠和唤醒线程。
        pthread_mutex_init(&io_threads_mutex[i],NULL);
        // 每个 I/O 线程分配一个原子计数器，用来记录当前遗留的任务数量。
        io_threads_pending[i] = 0;
        // 主线程在启动 I/O 线程的时候会默认先锁住它，直到有 I/O 任务才唤醒它。
        pthread_mutex_lock(&io_threads_mutex[i]);
        // 启动线程，进入 I/O 线程的主逻辑函数 IOThreadMain。
        if (pthread_create(&tid,NULL,IOThreadMain,(void*)(long)i) != 0) {
            serverLog(LL_WARNING,"Fatal: Can't initialize IO thread.");
            exit(1);
        }
        io_threads[i] = tid;
    }
}
```

`initThreadedIO` 会在 Redis 服务器启动时的初始化工作的末尾被调用，初始化 I/O 多线程并启动。

Redis 的多线程模式默认是关闭的，需要用户在 `redis.conf` 配置文件中开启：

```
io-threads 4
io-threads-do-reads yes
```

#### 5.2.2 **读取请求**

当客户端发送请求命令之后，会触发 Redis 主线程的事件循环，命令处理器 `readQueryFromClient` 被回调，在以前的单线程模型下，这个方法会直接读取解析客户端命令并执行，但是多线程模式下，则会把 `client` 加入到 `clients_pending_read` 任务队列中去，后面主线程再分配到 I/O 线程去读取客户端请求命令：

```
void readQueryFromClient(connection *conn) {
    client *c = connGetPrivateData(conn);
    int nread, readlen;
    size_t qblen;

    // 检查是否开启了多线程，如果是则把 client 加入异步队列之后返回。
    if (postponeClientRead(c)) return;
    
    // 省略代码，下面的代码逻辑和单线程版本几乎是一样的。
    ... 
}

int postponeClientRead(client *c) {
    // 当多线程 I/O 模式开启、主线程没有在处理阻塞任务时，将 client 加入异步队列。
    if (server.io_threads_active &&
        server.io_threads_do_reads &&
        !ProcessingEventsWhileBlocked &&
        !(c->flags & (CLIENT_MASTER|CLIENT_SLAVE|CLIENT_PENDING_READ)))
    {
        // 给 client 打上 CLIENT_PENDING_READ 标识，表示该 client 需要被多线程处理，
        // 后续在 I/O 线程中会在读取和解析完客户端命令之后判断该标识并放弃执行命令，让主线程去执行。
        c->flags |= CLIENT_PENDING_READ;
        listAddNodeHead(server.clients_pending_read,c);
        return 1;
    } else {
        return 0;
    }
}
```

接着主线程会在事件循环的 `beforeSleep()` 方法中，调用 `handleClientsWithPendingReadsUsingThreads`：

```
int handleClientsWithPendingReadsUsingThreads(void) {
    if (!server.io_threads_active || !server.io_threads_do_reads) return 0;
    int processed = listLength(server.clients_pending_read);
    if (processed == 0) return 0;

    if (tio_debug) printf("%d TOTAL READ pending clients\n", processed);

    // 遍历待读取的 client 队列 clients_pending_read，
    // 通过 RR 轮询均匀地分配给 I/O 线程和主线程自己（编号 0）。
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_read,&li);
    int item_id = 0;
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;
    }

    // 设置当前 I/O 操作为读取操作，给每个 I/O 线程的计数器设置分配的任务数量，
    // 让 I/O 线程可以开始工作：只读取和解析命令，不执行。
    io_threads_op = IO_THREADS_OP_READ;
    for (int j = 1; j < server.io_threads_num; j++) {
        int count = listLength(io_threads_list[j]);
        io_threads_pending[j] = count;
    }

    // 主线程自己也会去执行读取客户端请求命令的任务，以达到最大限度利用 CPU。
    listRewind(io_threads_list[0],&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        readQueryFromClient(c->conn);
    }
    listEmpty(io_threads_list[0]);

    // 忙轮询，累加所有 I/O 线程的原子任务计数器，直到所有计数器的遗留任务数量都是 0，
    // 表示所有任务都已经执行完成，结束轮询。
    while(1) {
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += io_threads_pending[j];
        if (pending == 0) break;
    }
    if (tio_debug) printf("I/O READ All threads finshed\n");

    // 遍历待读取的 client 队列，清除 CLIENT_PENDING_READ 和 CLIENT_PENDING_COMMAND 标记，
    // 然后解析并执行所有 client 的命令。
    while(listLength(server.clients_pending_read)) {
        ln = listFirst(server.clients_pending_read);
        client *c = listNodeValue(ln);
        c->flags &= ~CLIENT_PENDING_READ;
        listDelNode(server.clients_pending_read,ln);

        if (c->flags & CLIENT_PENDING_COMMAND) {
            c->flags &= ~CLIENT_PENDING_COMMAND;
            // client 的第一条命令已经被解析好了，直接尝试执行。
            if (processCommandAndResetClient(c) == C_ERR) {
                /* If the client is no longer valid, we avoid
                 * processing the client later. So we just go
                 * to the next. */
                continue;
            }
        }
        processInputBuffer(c); // 继续解析并执行 client 命令。

        // 命令执行完成之后，如果 client 中有响应数据需要回写到客户端，则将 client 加入到待写出队列 clients_pending_write
        if (!(c->flags & CLIENT_PENDING_WRITE) && clientHasPendingReplies(c))
            clientInstallWriteHandler(c);
    }

    /* Update processed count on server */
    server.stat_io_reads_processed += processed;

    return processed;
}
```

这里的核心工作是：

- 遍历待读取的 `client` 队列 `clients_pending_read`，通过 RR 策略把所有任务分配给 I/O 线程和主线程去读取和解析客户端命令。
- 忙轮询等待所有 I/O 线程完成任务。
- 最后再遍历 `clients_pending_read`，执行所有 `client` 的命令。

#### 5.2.3 **写回响应**

完成命令的读取、解析以及执行之后，客户端命令的响应数据已经存入 `client->buf` 或者 `client->reply` 中了，接下来就需要把响应数据回写到客户端了，还是在 `beforeSleep` 中， 主线程调用 `handleClientsWithPendingWritesUsingThreads`：

```
int handleClientsWithPendingWritesUsingThreads(void) {
    int processed = listLength(server.clients_pending_write);
    if (processed == 0) return 0; /* Return ASAP if there are no clients. */

    // 如果用户设置的 I/O 线程数等于 1 或者当前 clients_pending_write 队列中待写出的 client
    // 数量不足 I/O 线程数的两倍，则不用多线程的逻辑，让所有 I/O 线程进入休眠，
    // 直接在主线程把所有 client 的相应数据回写到客户端。
    if (server.io_threads_num == 1 || stopThreadedIOIfNeeded()) {
        return handleClientsWithPendingWrites();
    }

    // 唤醒正在休眠的 I/O 线程（如果有的话）。
    if (!server.io_threads_active) startThreadedIO();

    if (tio_debug) printf("%d TOTAL WRITE pending clients\n", processed);

    // 遍历待写出的 client 队列 clients_pending_write，
    // 通过 RR 轮询均匀地分配给 I/O 线程和主线程自己（编号 0）。
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_write,&li);
    int item_id = 0;
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        c->flags &= ~CLIENT_PENDING_WRITE;

        /* Remove clients from the list of pending writes since
         * they are going to be closed ASAP. */
        if (c->flags & CLIENT_CLOSE_ASAP) {
            listDelNode(server.clients_pending_write, ln);
            continue;
        }

        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;
    }

    // 设置当前 I/O 操作为写出操作，给每个 I/O 线程的计数器设置分配的任务数量，
    // 让 I/O 线程可以开始工作，把写出缓冲区（client->buf 或 c->reply）中的响应数据回写到客户端。
    io_threads_op = IO_THREADS_OP_WRITE;
    for (int j = 1; j < server.io_threads_num; j++) {
        int count = listLength(io_threads_list[j]);
        io_threads_pending[j] = count;
    }

    // 主线程自己也会去执行读取客户端请求命令的任务，以达到最大限度利用 CPU。
    listRewind(io_threads_list[0],&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        writeToClient(c,0);
    }
    listEmpty(io_threads_list[0]);

    // 忙轮询，累加所有 I/O 线程的原子任务计数器，直到所有计数器的遗留任务数量都是 0。
    // 表示所有任务都已经执行完成，结束轮询。
    while(1) {
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += io_threads_pending[j];
        if (pending == 0) break;
    }
    if (tio_debug) printf("I/O WRITE All threads finshed\n");

    // 最后再遍历一次 clients_pending_write 队列，检查是否还有 client 的写出缓冲区中有残留数据，
    // 如果有，那就为 client 注册一个命令回复器 sendReplyToClient，等待客户端写就绪再继续把数据回写。
    listRewind(server.clients_pending_write,&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);

        // 检查 client 的写出缓冲区是否还有遗留数据。
        if (clientHasPendingReplies(c) &&
                connSetWriteHandler(c->conn, sendReplyToClient) == AE_ERR)
        {
            freeClientAsync(c);
        }
    }
    listEmpty(server.clients_pending_write);

    /* Update processed count on server */
    server.stat_io_writes_processed += processed;

    return processed;
}
```

这里的核心工作是：

- 检查当前任务负载，如果当前的任务数量不足以用多线程模式处理的话，则休眠 I/O 线程并且直接同步将响应数据回写到客户端。
- 唤醒正在休眠的 I/O 线程（如果有的话）。
- 遍历待写出的 `client` 队列 `clients_pending_write`，通过 RR 策略把所有任务分配给 I/O 线程和主线程去将响应数据写回到客户端。
- 忙轮询等待所有 I/O 线程完成任务。
- 最后再遍历 `clients_pending_write`，为那些还残留有响应数据的 `client` 注册命令回复处理器 `sendReplyToClient`，等待客户端可写之后在事件循环中继续回写残余的响应数据。

#### 5.2.4 **I/O 线程主逻辑**

```
void *IOThreadMain(void *myid) {
    /* The ID is the thread number (from 0 to server.iothreads_num-1), and is
     * used by the thread to just manipulate a single sub-array of clients. */
    long id = (unsigned long)myid;
    char thdname[16];

    snprintf(thdname, sizeof(thdname), "io_thd_%ld", id);
    redis_set_thread_title(thdname);
    // 设置 I/O 线程的 CPU 亲和性，尽可能将 I/O 线程（以及主线程，不在这里设置）绑定到用户配置的
    // CPU 列表上。
    redisSetCpuAffinity(server.server_cpulist);
    makeThreadKillable();

    while(1) {
        // 忙轮询，100w 次循环，等待主线程分配 I/O 任务。
        for (int j = 0; j < 1000000; j++) {
            if (io_threads_pending[id] != 0) break;
        }

        // 如果 100w 次忙轮询之后如果还是没有任务分配给它，则通过尝试加锁进入休眠，
        // 等待主线程分配任务之后调用 startThreadedIO 解锁，唤醒 I/O 线程去执行。
        if (io_threads_pending[id] == 0) {
            pthread_mutex_lock(&io_threads_mutex[id]);
            pthread_mutex_unlock(&io_threads_mutex[id]);
            continue;
        }

        serverAssert(io_threads_pending[id] != 0);

        if (tio_debug) printf("[%ld] %d to handle\n", id, (int)listLength(io_threads_list[id]));


        // 注意：主线程分配任务给 I/O 线程之时，
        // 会把任务加入每个线程的本地任务队列 io_threads_list[id]，
        // 但是当 I/O 线程开始执行任务之后，主线程就不会再去访问这些任务队列，避免数据竞争。
        listIter li;
        listNode *ln;
        listRewind(io_threads_list[id],&li);
        while((ln = listNext(&li))) {
            client *c = listNodeValue(ln);
            // 如果当前是写出操作，则把 client 的写出缓冲区中的数据回写到客户端。
            if (io_threads_op == IO_THREADS_OP_WRITE) {
                writeToClient(c,0);
              // 如果当前是读取操作，则socket 读取客户端的请求命令并解析第一条命令。
            } else if (io_threads_op == IO_THREADS_OP_READ) {
                readQueryFromClient(c->conn);
            } else {
                serverPanic("io_threads_op value is unknown");
            }
        }
        listEmpty(io_threads_list[id]);
        // 所有任务执行完之后把自己的计数器置 0，主线程通过累加所有 I/O 线程的计数器
        // 判断是否所有 I/O 线程都已经完成工作。
        io_threads_pending[id] = 0;

        if (tio_debug) printf("[%ld] Done\n", id);
    }
}
```

I/O 线程启动之后，会先进入忙轮询，判断原子计数器中的任务数量，如果是非 0 则表示主线程已经给它分配了任务，开始执行任务，否则就一直忙轮询一百万次等待，忙轮询结束之后再查看计数器，如果还是 0，则尝试加本地锁，因为主线程在启动 I/O 线程之时就已经提前锁住了所有 I/O 线程的本地锁，因此 I/O 线程会进行休眠，等待主线程唤醒。

主线程会在每次事件循环中尝试调用 `startThreadedIO` 唤醒 I/O 线程去执行任务，如果接收到客户端请求命令，则 I/O 线程会被唤醒开始工作，根据主线程设置的 `io_threads_op` 标识去执行命令读取和解析或者回写响应数据的任务，I/O 线程在收到主线程通知之后，会遍历自己的本地任务队列 `io_threads_list[id]`，取出一个个 `client` 执行任务：

- 如果当前是写出操作，则调用 `writeToClient`，通过 socket 把 `client->buf` 或者 `client->reply` 里的响应数据回写到客户端。
- 如果当前是读取操作，则调用 `readQueryFromClient`，通过 socket 读取客户端命令，存入 `client->querybuf`，然后调用 `processInputBuffer` 去解析命令，这里最终只会解析到第一条命令，然后就结束，不会去执行命令。
- 在全部任务执行完之后把自己的原子计数器置 0，以告知主线程自己已经完成了工作。

```
void processInputBuffer(client *c) {
// 省略代码
...

    while(c->qb_pos < sdslen(c->querybuf)) {
        /* Return if clients are paused. */
        if (!(c->flags & CLIENT_SLAVE) && clientsArePaused()) break;

        /* Immediately abort if the client is in the middle of something. */
        if (c->flags & CLIENT_BLOCKED) break;

        /* Don't process more buffers from clients that have already pending
         * commands to execute in c->argv. */
        if (c->flags & CLIENT_PENDING_COMMAND) break;
        /* Multibulk processing could see a <= 0 length. */
        if (c->argc == 0) {
            resetClient(c);
        } else {
            // 判断 client 是否具有 CLIENT_PENDING_READ 标识，如果是处于多线程 I/O 的模式下，
            // 那么此前已经在 readQueryFromClient -> postponeClientRead 中为 client 打上该标识，
            // 则立刻跳出循环结束，此时第一条命令已经解析完成，但是不执行命令。
            if (c->flags & CLIENT_PENDING_READ) {
                c->flags |= CLIENT_PENDING_COMMAND;
                break;
            }

            // 执行客户端命令
            if (processCommandAndResetClient(c) == C_ERR) {
                /* If the client is no longer valid, we avoid exiting this
                 * loop and trimming the client buffer later. So we return
                 * ASAP in that case. */
                return;
            }
        }
    }

...
}
```

这里需要额外关注 I/O 线程初次启动时会设置当前线程的 CPU 亲和性，也就是绑定当前线程到用户配置的 CPU 上，在启动 Redis 服务器主线程的时候同样会设置 CPU 亲和性，Redis 的核心网络模型引入多线程之后，加上之前的多线程异步任务、多进程（BGSAVE、AOF、BIO、Sentinel 脚本任务等），Redis 现如今的系统并发度已经很大了，而 Redis 本身又是一个对吞吐量和延迟极度敏感的系统，所以用户需要 Redis 对 CPU 资源有更细粒度的控制，这里主要考虑的是两方面：CPU 高速缓存和 NUMA 架构。

首先是 CPU 高速缓存（这里讨论的是 L1 Cache 和 L2 Cache 都集成在 CPU 中的硬件架构），这里想象一种场景：Redis 主进程正在 CPU-1 上运行，给客户端提供数据服务，此时 Redis 启动了子进程进行数据持久化（BGSAVE 或者 AOF），系统调度之后子进程抢占了主进程的 CPU-1，主进程被调度到 CPU-2 上去运行，导致之前 CPU-1 的高速缓存里的相关指令和数据被汰换掉，CPU-2 需要重新加载指令和数据到自己的本地高速缓存里，浪费 CPU 资源，降低性能。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRIXxGMfcYjfnic2crvbVO6SZiaGCibLNIiaX6Cld5QNZQibCgw4mlQFNT0eA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

因此，Redis 通过设置 CPU 亲和性，可以将主进程/线程和子进程/线程绑定到不同的核隔离开来，使之互不干扰，能有效地提升系统性能。

其次是基于 NUMA 架构的考虑，在 NUMA 体系下，内存控制器芯片被集成到处理器内部，形成 CPU 本地内存，访问本地内存只需通过内存通道而无需经过系统总线，访问时延大大降低，而多个处理器之间通过 QPI 数据链路互联，跨 NUMA 节点的内存访问开销远大于本地内存的访问：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRoERwicIEI7HcLazMZnWibb1PEichu4ddOfospnUjrO0QkTV8egOzVt0MQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

因此，Redis 通过设置 CPU 亲和性，让主进程/线程尽可能在固定的 NUMA 节点上的 CPU 上运行，更多地使用本地内存而不需要跨节点访问数据，同样也能大大地提升性能。

关于 NUMA 相关知识请读者自行查阅，篇幅所限这里就不再展开，以后有时间我再单独写一篇文章介绍。

最后还有一点，阅读过源码的读者可能会有疑问，Redis 的多线程模式下，似乎并没有对数据进行锁保护，事实上 Redis 的多线程模型是全程无锁（Lock-free）的，这是通过原子操作+交错访问来实现的，主线程和 I/O 线程之间共享的变量有三个：`io_threads_pending` 计数器、`io_threads_op` I/O 标识符和 `io_threads_list` 线程本地任务队列。

`io_threads_pending` 是原子变量，不需要加锁保护，`io_threads_op` 和 `io_threads_list` 这两个变量则是通过控制主线程和 I/O 线程交错访问来规避共享数据竞争问题：I/O 线程启动之后会通过忙轮询和锁休眠等待主线程的信号，在这之前它不会去访问自己的本地任务队列 `io_threads_list[id]`，而主线程会在分配完所有任务到各个 I/O 线程的本地队列之后才去唤醒 I/O 线程开始工作，并且主线程之后在 I/O 线程运行期间只会访问自己的本地任务队列 `io_threads_list[0]` 而不会再去访问 I/O 线程的本地队列，这也就保证了主线程永远会在 I/O 线程之前访问 `io_threads_list` 并且之后不再访问，保证了交错访问。`io_threads_op` 同理，主线程会在唤醒 I/O 线程之前先设置好 `io_threads_op` 的值，并且在 I/O 线程运行期间不会再去访问这个变量。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRJBRrpB3m53MVSxWicicdVRVrmKbTbqiaj2UlnLyxBgdGct0qjDAUlGMvg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 5.3 性能提升

Redis 将核心网络模型改造成多线程模式追求的当然是最终性能上的提升，所以最终还是要以 benchmark 数据见真章：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRiaQSN3kHrmK0gzlT6Mt2AJAibfRVTy1wo7WUVYVWZYfMn9VzFkM8VxJA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

测试数据表明，Redis 在使用多线程模式之后性能大幅提升，达到了一倍。更详细的性能压测数据可以参阅这篇文章：[Benchmarking the experimental Redis Multi-Threaded I/O](https://itnext.io/benchmarking-the-experimental-redis-multi-threaded-i-o-1bb28b69a314)。

以下是美图技术团队实测的新旧 Redis 版本性能对比图，仅供参考：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRmiaMAFBWiaEJTp8LzYyPNKibsIbictEh6icgIVgODElCW0TkX9PgLDLZWAw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRoyRxiaUZEYmMNicQCEXtAicEC3Kgu0eLTA9NS4keQna51zrmmxicxJ1rKA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 5.4 模型缺陷

首先第一个就是我前面提到过的，Redis 的多线程网络模型实际上并不是一个标准的 Multi-Reactors/Master-Workers 模型，和其他主流的开源网络服务器的模式有所区别，最大的不同就是在标准的 Multi-Reactors/Master-Workers 模式下，Sub Reactors/Workers 会完成 `网络读 -> 数据解析 -> 命令执行 -> 网络写` 整套流程，Main Reactor/Master 只负责分派任务，而在 Redis 的多线程方案中，I/O 线程任务仅仅是通过 socket 读取客户端请求命令并解析，却没有真正去执行命令，所有客户端命令最后还需要回到主线程去执行，因此对多核的利用率并不算高，而且每次主线程都必须在分配完任务之后忙轮询等待所有 I/O 线程完成任务之后才能继续执行其他逻辑。

Redis 之所以如此设计它的多线程网络模型，我认为主要的原因是为了保持兼容性，因为以前 Redis 是单线程的，所有的客户端命令都是在单线程的事件循环里执行的，也因此 Redis 里所有的数据结构都是非线程安全的，现在引入多线程，如果按照标准的 Multi-Reactors/Master-Workers 模式来实现，则所有内置的数据结构都必须重构成线程安全的，这个工作量无疑是巨大且麻烦的。

所以，在我看来，Redis 目前的多线程方案更像是一个折中的选择：既保持了原系统的兼容性，又能利用多核提升 I/O 性能。

其次，目前 Redis 的多线程模型中，主线程和 I/O 线程的通信过于简单粗暴：忙轮询和锁，因为通过自旋忙轮询进行等待，导致 Redis 在启动的时候以及运行期间偶尔会有短暂的 CPU 空转引起的高占用率，而且这个通信机制的最终实现看起来非常不直观和不简洁，希望后面 Redis 能对目前的方案加以改进。

## 6.**总结**

Redis 作为缓存系统的事实标准，它的底层原理值得开发者去深入学习，Redis 自 2009 年发布第一版之后，其单线程网络模型的选择在社区中从未停止过讨论，多年来一直有呼声希望 Redis 能引入多线程从而利用多核优势，但是作者 antirez 是一个追求大道至简的开发者，对 Redis 加入任何新功能都异常谨慎，所以在 Redis 初版发布的十年后才最终将 Redis 的核心网络模型改造成多线程模式，这期间甚至诞生了一些 Redis 多线程的替代项目。虽然 antirez 一直在推迟多线程的方案，但却从未停止思考多线程的可行性，Redis 多线程网络模型的改造不是一朝一夕的事情，这其中牵扯到项目的方方面面，所以我们可以看到 Redis 的最终方案也并不完美，没有采用主流的多线程模式设计。

让我们来回顾一下 Redis 多线程网络模型的设计方案：

- 使用 I/O 线程实现网络 I/O 多线程化，I/O 线程只负责网络 I/O 和命令解析，不执行客户端命令。
- 利用原子操作+交错访问实现无锁的多线程模型。
- 通过设置 CPU 亲和性，隔离主进程和其他子进程，让多线程网络模型能发挥最大的性能。

通读本文之后，相信读者们应该能够了解到一个优秀的网络系统的实现所涉及到的计算机领域的各种技术：设计模式、网络 I/O、并发编程、操作系统底层，甚至是计算机硬件。另外还需要对项目迭代和重构的谨慎，对技术方案的深入思考，绝不仅仅是写好代码这一个难点。

## 7.参考&延伸阅读

- [Redis v5.0.10](https://github.com/redis/redis/tree/5.0.10)
- [Redis v6.0.10](https://github.com/redis/redis/tree/6.0.10)
- [Lazy Redis is better Redis](http://antirez.com/news/93)
- [An update about Redis developments in 2019](http://antirez.com/news/126)
- [How fast is Redis?](https://redis.io/topics/benchmarks)
- [Go netpoller 原生网络模型之源码全面揭秘](https://strikefreedom.top/go-netpoll-io-multiplexing-reactor)
- [Linux I/O 原理和 Zero-copy 技术全面揭秘](https://strikefreedom.top/linux-io-and-zero-copy)
- [Benchmarking the experimental Redis Multi-Threaded I/O](https://itnext.io/benchmarking-the-experimental-redis-multi-threaded-i-o-1bb28b69a314)
- [NUMA DEEP DIVE PART 1: FROM UMA TO NUMA](https://frankdenneman.nl/2016/07/07/numa-deep-dive-part-1-uma-numa/)

原文作者：allanpan，腾讯 IEG 后台开发工程师，公众号：远赴星辰。

原文链接：https://mp.weixin.qq.com/s/-op5WR1wSkgAuP7JYZWP8g

# 【NO.227】网络 IO 演变发展过程和模型介绍

在互联网中提起网络，我们都会避免不了讨论高并发、百万连接。而此处的百万连接的实现，脱离不了网络 IO 的选择，因此本文作为一篇个人学习的笔记，特此进行记录一下整个网络 IO 的发展演变过程。以及目前广泛使用的网络模型。

## **1.网络 IO 的发展**

在本节内容中，我们将一步一步介绍网络 IO 的演变发展过程。介绍完发展过程后，再对网络 IO 中几组容易混淆的概念进行对比、分析。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HETu42nzJ6nvmrltaxMIlZJdXr2TaY9pPbZoSMASuG5NhCh4sQzIXPDA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 1.1 网络 IO 的各个发展阶段

通常，我们在此讨论的网络 IO 一般都是针对 linux 操作系统而言。网络 IO 的发展过程是随着 linux 的内核演变而变化，因此网络 IO 大致可以分为如下几个阶段：

**1. 阻塞 IO(BIO)**
**2. 非阻塞 IO(NIO)**
**3. IO 多路复用第一版(select/poll)**
**4. IO 多路复用第二版(epoll)**
**5. 异步 IO(AIO)**

而每一个阶段，都是因为当前的网络有一些缺陷，因此又在不断改进该缺陷。这是**网络 IO 一直演变过程中的本质**。下面将对上述几个阶段进行介绍，并对每个阶段的网络 IO 解决了哪些问题、优点、缺点进行剖析。

### **1.2 网络的两个阶段**

在网络中，我们通常可以将其广义上划分为以下两个阶段：

**第一阶段：硬件接口到内核态**
**第二阶段：内核态到用户态**

本人理解：我们通常上网，大部分数据都是通过网线传递的。因此对于两台计算机而言，要进行网络通信，其数据都是先从应用程序传递到传输层(TCP/UDP)到达内核态，然后再到网络层、数据链路层、物理层，接着数据传递到硬件网卡，最后通过网络传输介质传递到对端机器的网卡，然后再一步一步数据从网卡传递到内核态，最后再拷贝到用户态。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEZDwwibVibDL8RCj0Vhia4xMMbO5FqvRcdIEicjYyjQSpBe4kanbfFbRrRg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 1.3 阻塞 IO 和非阻塞 IO 的区别

根据 1.2 节的内容，我们可以知道，网络中的数据传输从网络传输介质到达目的机器，需要如上两个阶段。此处我们把从**硬件到内核态**这一阶段，是否发生阻塞等待，可以将网络分为**阻塞 IO**和**非阻塞 IO**。如果用户发起了读写请求，但内核态数据还未准备就绪，该阶段不会阻塞用户操作，内核立马返回，则称为非阻塞 IO。如果该阶段一直阻塞用户操作。直到内核态数据准备就绪，才返回。这种方式称为阻塞 IO。

因此，区分阻塞 IO 和非阻塞 IO 主要看第一阶段是否阻塞用户操作。

### 1.4 同步 IO 和异步 IO 的区别

从前面我们知道了，数据的传递需要两个阶段，在此处只要任何一个阶段会阻塞用户请求，都将其称为同步 IO，两个阶段都不阻塞，则称为异步 IO。

在目前所有的操作系统中，linux 中的 epoll、mac 的 kqueue 都属于同步 IO，因为其在第二阶段(数据从内核态到用户态)都会发生拷贝阻塞。而只有 windows 中的 IOCP 才真正属于异步 IO，即 AIO。

## **2.阻塞 IO**

在本节，我们将介绍最初的阻塞 IO，阻塞 IO 英文为 blocking IO，又称为 BIO。根据前面的介绍，阻塞 IO 主要指的是第一阶段(硬件网卡到内核态)。

### 2.1 阻塞 IO 的概念

阻塞 IO，顾名思义当用户发生了系统调用后，如果数据未从网卡到达内核态，内核态数据未准备好，此时会一直阻塞。直到数据就绪，然后从内核态拷贝到用户态再返回。具体过程可以参考 2.2 的图示。

### 2.2 阻塞 IO 的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEGibuyiacHXVXK7gdQ95jEUAwSuy3s6G5v61Tms5R368FXesWOhiajiaq4A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 2.3 阻塞 IO 的缺点

在一般使用阻塞 IO 时，都需要配置多线程来使用，最常见的模型是**阻塞 IO+多线程**，每个连接一个单独的线程进行处理。

**我们知道，一般一个程序可以开辟的线程是有限的，而且开辟线程的开销也是比较大的。也正是这种方式，会导致一个应用程序可以处理的客户端请求受限。面对百万连接的情况，是无法处理。**

既然发现了问题，分析了问题，那就得解决问题。既然阻塞 IO 有问题，本质是由于其阻塞导致的，因此自然而然引出了下面即将介绍的主角：**非阻塞 IO**

## **3.非阻塞 IO**

非阻塞 IO 是为了解决前面提到的阻塞 IO 的缺陷而引出的，下面我们将介绍非阻塞 IO 的过程。

### 3.1 非阻塞 IO 的概念

非阻塞 IO：见名知意，就是在第一阶段(网卡-内核态)数据未到达时不等待，然后直接返回。因此非阻塞 IO 需要不断的用户发起请求，询问内核数据好了没，好了没。

### 3.2 非阻塞 IO 的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HESdhj8nSKbRFuZ6U6GuoT5gnyttj1uic3YcC0LQy2kHC45s3jSC82aXA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

非阻塞 IO 是需要系统内核支持的，在创建了连接后，可以调用 setsockop 设置 noblocking

### 3.3 非阻塞 IO 的优点

正如前面提到的，非阻塞 IO 解决了阻塞 IO**每个连接一个线程处理的问题**，所以其最大的优点就是 **一个线程可以处理多个连接**，这也是其非阻塞决定的。

### 3.4 非阻塞 IO 的缺点

但这种模式，也有一个问题，就是需要用户多次发起系统调用。**频繁的系统调用**是比较消耗系统资源的。

因此，既然存在这样的问题，那么自然而然我们就需要解决该问题：**保留非阻塞 IO 的优点的前提下，减少系统调用**

## **4.IO 多路复用第一版**

为了解决非阻塞 IO 存在的频繁的系统调用这个问题，随着内核的发展，出现了 IO 多路复用模型。那么我们就需要搞懂几个问题：

1. IO 多路复用到底复用什么？
2. IO 多路复用如何复用？

**IO 多路复用：** 很多人都说，IO 多路复用是用一个线程来管理多个网络连接，但本人不太认可，因为在非阻塞 IO 时，就已经可以实现一个线程处理多个网络连接了，这个是由于其非阻塞而决定的。

**在此处，个人观点，多路复用主要复用的是通过有限次的系统调用来实现管理多个网络连接。最简单来说，我目前有 10 个连接，我可以通过一次系统调用将这 10 个连接都丢给内核，让内核告诉我，哪些连接上面数据准备好了，然后我再去读取每个就绪的连接上的数据。因此，IO 多路复用，复用的是系统调用。通过有限次系统调用判断海量连接是否数据准备好了**

**无论下面的 select、poll、epoll，其都是这种思想实现的，不过在实现上，select/poll 可以看做是第一版，而 epoll 是第二版**

### 4.1IO 多路复用第一版的概念

**IO 多路复用第一版，这个概念是本人想出来的，主要是方便将 select/poll 和 epoll 进行区分**

所以此处 IO 多路复用第一版，主要特指 select 和 poll 这两个。

select 的 api

```
// readfds:关心读的fd集合；writefds：关心写的fd集合；excepttfds：异常的fd集合
int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

> select 函数监视的文件描述符分 3 类，分别是 writefds、readfds、和 exceptfds。调用后 select 函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有 except），或者超时（timeout 指定等待时间，如果立即返回设为 null 即可），函数返回。当 select 函数返回后，可以 通过遍历 fdset，来找到就绪的描述符。

> select 目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select 的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在 Linux 上一般为 1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。

poll 的 api

```
int poll (struct pollfd *fds, unsigned int nfds, int timeout);

struct pollfd {
    int fd; /* file descriptor */
    short events; /* requested events to watch */
    short revents; /* returned events witnessed */
};
```

> pollfd 结构包含了要监视的 event 和发生的 event，不再使用 select“参数-值”传递的方式。同时，pollfd 并没有最大数量限制（但是数量过大后性能也是会下降）。和 select 函数一样，poll 返回后，需要轮询 pollfd 来获取就绪的描述符。

> 从上面看，select 和 poll 都需要在返回后，通过遍历文件描述符来获取已经就绪的 socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。

**从本质来说：IO 多路复用中，select()/poll()/epoll_wait()这几个函数对应第一阶段；read()/recvfrom()对应第二阶段**

### 4.2IO 多路复用第一版的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEbPaUbbrbibQsCw9xPib0FfZXfOKQ6OdekEicBSyr5micL6iaUAIQqRrDqAA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 4.3IO 多路复用第一版的优点

**IO 多路复用，主要在于复用，通过 select()或者 poll()将多个 socket fds 批量通过系统调用传递给内核，由内核进行循环遍历判断哪些 fd 上数据就绪了，然后将就绪的 readyfds 返回给用户。再由用户进行挨个遍历就绪好的 fd，读取或者写入数据。**

**所以通过 IO 多路复用+非阻塞 IO，一方面降低了系统调用次数，另一方面可以用极少的线程来处理多个网络连接。**

### 4.4IO 多路复用第一版的缺点

虽然第一版 IO 多路复用解决了之前提到的频繁的系统调用次数，但同时引入了新的问题：**用户需要每次将海量的 socket fds 集合从用户态传递到内核态，让内核态去检测哪些网络连接数据就绪了**

**但这个地方会出现频繁的将海量 fd 集合从用户态传递到内核态，再从内核态拷贝到用户态。所以，这个地方开销也挺大。**

既然还有这个问题，那我们继续开始解决这个问题，因此就引出了第二版的 IO 多路复用。

**其实思路也挺简单，既然需要拷贝，那就想办法，不拷贝。既然不拷贝，那就在内核开辟一段区域咯**

### 4.5IO 多路复用第一版的区别

**select 和 poll 的区别**

1. select 能处理的最大连接，默认是 1024 个，可以通过修改配置来改变，但终究是有限个；而 poll 理论上可以支持无限个
2. select 和 poll 在管理海量的连接时，会频繁的从用户态拷贝到内核态，比较消耗资源。

## **5.IO 多路复用第二版**

IO 多路复用第二版主要指 epoll，epoll 的出现也是随着内核版本迭代才诞生的，在网上到处看到，epoll 是内核 2.6 以后开始支持的

**epoll 的出现是为了解决前面提到的 IO 多路复用第一版的问题**

### 5.1IO 多路复用第二版的概念

epoll 提供的 api

```
//创建epollFd，底层是在内核态分配一段区域，底层数据结构红黑树+双向链表
int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大

//往红黑树中增加、删除、更新管理的socket fd
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；

//这个api是用来在第一阶段阻塞，等待就绪的fd。
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
1. int epoll_create(int size);
创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。
当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。

2. int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；
函数是对指定描述符fd执行op操作。
- epfd：是epoll_create()的返回值。
- op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。
- fd：是需要监听的fd（文件描述符）
- epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下：

struct epoll_event {
  __uint32_t events;  /* Epoll events */
  epoll_data_t data;  /* User data variable */
};

//events可以是以下几个宏的集合：
EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
EPOLLOUT：表示对应的文件描述符可以写；
EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
EPOLLERR：表示对应的文件描述符发生错误；
EPOLLHUP：表示对应的文件描述符被挂断；
EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。
EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里

3. int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
等待epfd上的io事件，最多返回maxevents个事件。
参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。
```

二 工作模式

epoll 对文件描述符的操作有两种模式：LT（level trigger）和 ET（edge trigger）。LT 模式是默认模式，LT 模式与 ET 模式的区别如下：　　 LT 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用 epoll_wait 时，会再次响应应用程序并通知此事件。　　 ET 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用 epoll_wait 时，不会再次响应应用程序并通知此事件。

1. LT 模式

LT(level triggered)是缺省的工作方式，并且同时支持 block 和 no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的 fd 进行 IO 操作。如果你不作任何操作，内核还是会继续通知你的。

1. ET 模式

ET(edge-triggered)是高速工作方式，只支持 no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过 epoll 告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个 EWOULDBLOCK 错误）。但是请注意，如果一直不对这个 fd 作 IO 操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once)

ET 模式在很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。epoll 工作在 ET 模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

### 5.2IO 多路复用第二版的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEbPaUbbrbibQsCw9xPib0FfZXfOKQ6OdekEicBSyr5micL6iaUAIQqRrDqAA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

当 epoll_wait()调用后会阻塞，然后完了当返回时，会返回了哪些 fd 的数据就绪了，用户只需要遍历就绪的 fd 进行读写即可。

### 5.3IO 多路复用第二版的优点

**IO 多路复用第二版 epoll 的优点在于：**

一开始就在内核态分配了一段空间，来存放管理的 fd,所以在每次连接建立后，交给 epoll 管理时，需要将其添加到原先分配的空间中，后面再管理时就不需要频繁的从用户态拷贝管理的 fd 集合。通通过这种方式大大的提升了性能。

所以现在的 IO 多路复用主要指 epoll

### 5.4IO 多路复用第二版的缺点

**个人猜想：** 如何降低占用的空间

## **6.异步 IO**

### 6.1 异步 IO 的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEXGYke8ibPkbXEkaQ861CSg8YvQ3qFdvGcXicAicyrD9Vaicx77liakCLBnA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

前面介绍的所有网络 IO 都是同步 IO，因为当数据在内核态就绪时，在内核态拷贝用用户态的过程中，仍然会有短暂时间的阻塞等待。而异步 IO 指：**内核态拷贝数据到用户态这种方式也是交给系统线程来实现，不由用户线程完成**，目前只有 windows 系统的 IOCP 是属于异步 IO。

## **7.网络 IO 各种模型**

### 7.1 reactor 模型

目前 reactor 模型有以下几种实现方案：

**1. 单 reactor 单线程模型**
**2. 单 reactor 多线程模型**
**3. multi-reactor 多线程模型**
**4. multi-reactor 多进程模型**

> 下文网络模型的图，均摘自[这篇文章](https://zhuanlan.zhihu.com/p/95662364)

#### **7.1.1 单 reactor 单线程模型**

此种模型，通常是只有一个 epoll 对象，所有的**接收客户端连接**、**客户端读取**、**客户端写入**操作都包含在一个线程内。该种模型也有一些中间件在用，比如 redis

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEibnlbqiclVmzSkNpIym8ibYaXOAAnn2cZSibDr7fiaHXGjypWk0iacO7kkdA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> 但在目前的单线程 Reactor 模式中，不仅 I/O 操作在该 Reactor 线程上，连非 I/O 的业务操作也在该线程上进行处理了，这可能会大大延迟 I/O 请求的响应。所以我们应该将非 I/O 的业务逻辑操作从 Reactor 线程上卸载，以此来加速 Reactor 线程对 I/O 请求的响应。

#### **7.1.2 单 reactor 多线程模型**

该模型主要是通过将，前面的模型进行改造，将读写的业务逻辑交给具体的线程池来实现，这样可以显示 reactor 线程对 IO 的响应，以此提升系统性能![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEvzbIZnWXH0HUWLgUZTRzHGzNsRK0F2JC0bxHmia6XG02wocUUvianoIQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> 在工作者线程池模式中，虽然非 I/O 操作交给了线程池来处理，但是所有的 I/O 操作依然由 Reactor 单线程执行，在高负载、高并发或大数据量的应用场景，依然较容易成为瓶颈。所以，对于 Reactor 的优化，又产生出下面的多线程模式。

#### **7.1.3 multi-reactor 多线程模型**

在这种模型中，主要分为两个部分：mainReactor、subReactors。mainReactor 主要负责接收客户端的连接，然后将建立的客户端连接通过负载均衡的方式分发给 subReactors，

subReactors 来负责具体的每个连接的读写

对于非 IO 的操作，依然交给工作线程池去做，对逻辑进行解耦

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEJ1BVQvZoIJB0xqBLxNvCOcOWkTknoMP4t1XkUgHfsoN7J2jzClJuaw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> mainReactor 对应 Netty 中配置的 BossGroup 线程组，主要负责接受客户端连接的建立。一般只暴露一个服务端口，BossGroup 线程组一般一个线程工作即可 subReactor 对应 Netty 中配置的 WorkerGroup 线程组，BossGroup 线程组接受并建立完客户端的连接后，将网络 socket 转交给 WorkerGroup 线程组，然后在 WorkerGroup 线程组内选择一个线程，进行 I/O 的处理。WorkerGroup 线程组主要处理 I/O，一般设置 2*CPU 核数个线程

### 7.2 proactor 模型

proactor 主要是通过对异步 IO 的封装的一种模型，它需要底层操作系统的支持，目前只有 windows 的 IOCP 支持的比较好。详细的介绍可以参考[这篇文章](https://zhuanlan.zhihu.com/p/95662364)

### 7.3 主流的中间件所采用的网络模型

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEiaTnSvliaMcYCEDmUiaDlbRQTlURhaKHY8N0qwsgLuXZpkOgeJ3UtXMlQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 7.4 主流网络框架

- netty
- gnet
- libevent
- evio(golang)
- ACE(c++)
- boost::asio(c++)
- muduo （linux only)

关于c++和c的上述几个库对比，感兴趣的话大家可以自行搜索资料或者阅读[这篇文章](https://www.cnblogs.com/leijiangtao/p/5197566.html)。



## **8.参考资料**

1. [IO 模式和 IO 多路复用](https://juejin.im/post/5bf7b89e518825369c564059)
2. [Linux IO 模式及 select、poll、epoll 详解](https://segmentfault.com/a/1190000003063859)
3. [Chapter 6. I/O Multiplexing: The select and poll Functions](http://www.masterraghu.com/subjects/np/introduction/unix_network_programming_v1.3/ch06.html)
4. [高性能 IO 模型分析-Reactor 模式和 Proactor 模式（二）](https://zhuanlan.zhihu.com/p/95662364)

原文作者：jaydenwen，腾讯 pcg 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/EDzFOo3gcivOe_RgipkTkQ

# 【NO.228】操作系统与存储：解析Linux内核全新异步IO引擎io_uring设计与实现

## **0.引言**

存储场景中，我们对性能的要求非常高。在存储引擎底层的IO技术选型时，可能会有如下讨论关于IO的讨论。

> http://davmac.org/davpage/linux/async-io.html
> So from the above documentation, it seems that Linux doesn't have a true async file I/O that is not blocking (AIO, Epoll or POSIX AIO are all broken in some ways). I wonder if tlinux has any remedy. We should reach out to tlinux experts to get their opinions.

看完这段话，读者可能会有如下的问题。

1. 这是在讨论什么，为何会有此番讨论？
2. 有没有更好的解决方案？
3. 更好的解决方案是通过怎样的设计和实现解决问题？
4. ...

2019年，Linux Kernel正式进入5.x时代，众多新特性中，与存储领域相关度最高的便是最新的IO引擎——io_uring。从一些性能测试的结论来看，io_uring性能远高于native AIO方式，带来了巨大的性能提升，这对当前异步IO领域也是一个big news。

1. 对于问题1，本文简述了Linux过往的的IO发展历程，同步IO接口、原生异步IO接口AIO的缺陷，为何原有方式存在缺陷。
2. 对于问题2，本文从设计的角度出发，介绍了最新的IO引擎io_uring的相关内容。
3. 对于问题3，本文深入最新版内核linux-5.10中解析了io_uring的大体实现（关键数据结构、流程、特性实现等）。
4. ...

## 1.**一切过往，皆为序章**

以史为镜，可以知兴替。我们先看看现存过往IO接口的缺陷。

### 1.1 过往同步IO接口

当今Linux对文件的操作有很多种方式，过往同步IO接口从功能上划分，大体分为如下几种。

- 原始版本
- offset版本
- 向量版本
- offset+向量版本

#### 1.1.1 **read，write**

最原始的文件IO系统调用就是read，write

read系统调用从文件描述符所指代的打开文件中读取数据。

read简单介绍：

```
NAME
    read - read from a file descriptor
SYNOPSIS
    #include <unistd.h>

    ssize_t read(int fd, void *buf, size_t count);
DESCRIPTION
    read() attempts to read up to count bytes from file descriptor fd
    into the buffer starting at buf.
    
    On files that support seeking, the read operation commences at the
    file offset, and the file offset is incremented by the number of
    bytes read.  If the file offset is at or past the end of file, no
    bytes are read, and read() returns zero.
    
    If count is zero, read() may detect the errors described below.  In
    the absence of any errors, or if read() does not check for errors, a
    read() with a count of 0 returns zero and has no other effects.
    
    According to POSIX.1, if count is greater than SSIZE_MAX, the result
    is implementation-defined; see NOTES for the upper limit on Linux.
```

write系统调用将数据写入一个已打开的文件中。

write简单介绍：

```
NAME
    write - write to a file descriptor
SYNOPSIS
    #include <unistd.h>
    
    ssize_t write(int fd, const void *buf, size_t count);
DESCRIPTION
    write() writes up to count bytes from the buffer starting at buf to
    the file referred to by the file descriptor fd.
    
    The number of bytes written may be less than count if, for example,
    there is insufficient space on the underlying physical medium, or the
    RLIMIT_FSIZE resource limit is encountered (see setrlimit(2)), or the
    call was interrupted by a signal handler after having written less
    than count bytes.  (See also pipe(7).)
    
    For a seekable file (i.e., one to which lseek(2) may be applied, for
    example, a regular file) writing takes place at the file offset, and
    the file offset is incremented by the number of bytes actually
    written.  If the file was open(2)ed with O_APPEND, the file offset is
    first set to the end of the file before writing.  The adjustment of
    the file offset and the write operation are performed as an atomic
    step.
    
    POSIX requires that a read(2) that can be proved to occur after a
    write() has returned will return the new data.  Note that not all
    filesystems are POSIX conforming.
    
    According to POSIX.1, if count is greater than SSIZE_MAX, the result
    is implementation-defined; see NOTES for the upper limit on Linux.
```

#### 1.1.2 **在文件特定偏移处的IO：pread，pwrite**

在多线程环境下，为了保证线程安全，需要保证下列操作的原子性。

```
    off_t orig;
    orig = lseek(fd, 0, SEEK_CUR); // Save current offset
    lseek(fd, offset, SEEK_SET);
    s = read(fd, buf, len);
    lseek(fd, orig, SEEK_SET); // Restore original file offset
```

让使用者来保证原子性较繁，从接口上就有保证是一个好的选择，后来出现的pread便实现了这一点。

与read, write类似，pread, pwrite调用时可以指定位置进行文件IO操作，而非始于文件的当前偏移处，且他们不会改变文件的当前偏移量。这种方式，减少了编码，并提高了代码的健壮性。

pread、pwrite简单介绍：

```
NAME
       pread,  pwrite  -  read from or write to a file descriptor at a given
       offset
SYNOPSIS
       #include <unistd.h>

       ssize_t pread(int fd, void *buf, size_t count, off_t offset);

       ssize_t pwrite(int fd, const void *buf, size_t count, off_t offset);
       
DESCRIPTION
       pread() reads up to count bytes from file descriptor fd at offset
       offset (from the start of the file) into the buffer starting at buf.
       The file offset is not changed.

       pwrite() writes up to count bytes from the buffer starting at buf to
       the file descriptor fd at offset offset.  The file offset is not
       changed.

       The file referenced by fd must be capable of seeking.
```

当然，往read，write接口参数的标志位集合中加入新标志，用以表征新逻辑，可能达到相同的效果，但是这可能不够优雅——如果某个参数有多种可能的值，而函数内又以条件表达式检查这些参数值，并根据不同参数值做出不同的行为，那么以明确函数取代参数（Replace Parameter with Explicit Methods）也是一种合适的重构手法。

如果需要反复执行lseek，并伴之以文件IO，那么pread和pwrite系统调用在某些情况下是具有性能优势的。这是因为执行单个pread或pwrite系统调用的成本要低于执行lseek和read/write两个系统调用（当然，相对地，执行实际IO的开销通常要远大于执行系统调用，系统调用的性能优势作用有限）。历史上，一些数据库，通过使用kernel的这一新接口，获得了不菲的收益。如PostgreSQL：[*[PATCH\] Using pread instead of lseek (with analysis)*](https://www.postgresql-archive.org/PATCH-Using-pread-instead-of-lseek-with-analysis-td2215257.html)

#### 1.1.3 **分散输入和集中输出（Scatter-Gather IO）：readv, writev**

“物质的组成与结构决定物质的性质，性质决定用途，用途体现性质。”是自然科学的重要思想，在计算机科学中也是如此。现有计算机体系结构下，数据存储由一个或多个基本单元组成，物理、逻辑上的结构，决定了数据存储的性质——可能是连续的，也可能是不连续的。

对于不连续的数据的处理相对较繁，例如，使用read将数据读到不连续的内存，使用write将不连续的内存发送出去。更具体地看，如果要从文件中读一片连续的数据至进程的不同区域，有两种方案：

1. 使用read一次将它们读至一个较大的缓冲区中，然后将它们分成若干部分复制到不同的区域。
2. 调用read若干次分批将它们读至不同区域。

同样地，如果想将程序中不同区域的数据块连续地写至文件，也必须进行类似的处理。而且这种方案需要多次调用read、write系统调用，有损性能。

那么如何简化编程，如何解决这种开销呢？一种有效的解法就是使用特定的数据结构对非连续的数据进行管理，批量传输数据。从接口上就有此保证是一个好的选择，后来出现的readv，writev便实现了这一点。

这种基于向量的，分散输入和集中输出的系统调用并非只对单个缓冲区进行读写操作，而是一次即可传输多个缓冲区的数据，免除了多次系统调用的开销。该机制使用一个数组iov定义了一组用来传输数据的缓冲区，一个整形数iovcnt指定iov的成员个数，其中，iov中的每个成员都是如下形式的数据结构。

```
struct iovec {
   void  *iov_base;    /* Starting address */
   size_t iov_len;     /* Number of bytes to transfer */
};
```

**功能交集：preadv，pwritev**

上述两种功能都是一种进步，不过似乎格格不入，那么是否能合二为一，进两步呢？

数学上，集合是指具有某种特定性质的具体的或抽象的对象汇总而成的集体。其中，构成集合的这些对象则称为该集合的元素。我这里将接口定义成一种集合，一种特定功能就是其中的一个元素。根据已知有限集构造一个子集，该子集对于每一个元素要么包含要么不包含，那么根据乘法原理，这个子集共有2^N 种构造方式，即有2^N个子集。这么多可能的集合，显然较繁。基于场景对于功能子集的需求、元素之间的容斥、集合中元素是否需要有序（接口层面对功能的表现）、简约性等因素，我们会确立一些优雅的接口，这也是函数接口设计的一个哲学话题。

后来出现的preadv，pwritev，便是偏移和向量的交集，也是一种在排列组合的巨大可能性下确立的少部分简约的接口。

**带标志位集合的IO：preadv2，pwritev2**

再后来，还出现了变种函数preadv2和pwritev2，相比较preadv，pwritev，v2版本还能设置本次IO的标志，比如RWF_DSYNC、RWF_HIPRI、RWF_SYNC、RWF_NOWAIT、RWF_APPEND。

readv、preadv、preadv2系列简单介绍：

```
NAME
    readv,  writev,  preadv,  pwritev,  preadv2, pwritev2 - read or write
       data into multiple buffers

SYNOPSIS
    #include <sys/uio.h>

   ssize_t readv(int fd, const struct iovec *iov, int iovcnt);

   ssize_t writev(int fd, const struct iovec *iov, int iovcnt);

   ssize_t preadv(int fd, const struct iovec *iov, int iovcnt,
                  off_t offset);

   ssize_t pwritev(int fd, const struct iovec *iov, int iovcnt,
                   off_t offset);

   ssize_t preadv2(int fd, const struct iovec *iov, int iovcnt,
                   off_t offset, int flags);

   ssize_t pwritev2(int fd, const struct iovec *iov, int iovcnt,
                    off_t offset, int flags);

DESCRIPTION
   The readv() system call reads iovcnt buffers from the file associated
       with the file descriptor fd into the buffers described by iov
       ("scatter input").

       The writev() system call writes iovcnt buffers of data described by
       iov to the file associated with the file descriptor fd ("gather
       output").

       The pointer iov points to an array of iovec structures, defined in
       <sys/uio.h> as:

           struct iovec {
               void  *iov_base;    /* Starting address */
               size_t iov_len;     /* Number of bytes to transfer */
           };

       The readv() system call works just like read(2) except that multiple
       buffers are filled.

       The writev() system call works just like write(2) except that multi‐
       ple buffers are written out.

       Buffers are processed in array order.  This means that readv() com‐
       pletely fills iov[0] before proceeding to iov[1], and so on.  (If
       there is insufficient data, then not all buffers pointed to by iov
       may be filled.)  Similarly, writev() writes out the entire contents
       of iov[0] before proceeding to iov[1], and so on.

       The data transfers performed by readv() and writev() are atomic: the
       data written by writev() is written as a single block that is not in‐
       termingled with output from writes in other processes (but see
       pipe(7) for an exception); analogously, readv() is guaranteed to read
       a contiguous block of data from the file, regardless of read opera‐
       tions performed in other threads or processes that have file descrip‐
       tors referring to the same open file description (see open(2)).

   preadv() and pwritev()
       The preadv() system call combines the functionality of readv() and
       pread(2).  It performs the same task as readv(), but adds a fourth
       argument, offset, which specifies the file offset at which the input
       operation is to be performed.

       The pwritev() system call combines the functionality of writev() and
       pwrite(2).  It performs the same task as writev(), but adds a fourth
       argument, offset, which specifies the file offset at which the output
       operation is to be performed.

       The file offset is not changed by these system calls.  The file re‐
       ferred to by fd must be capable of seeking.

   preadv2() and pwritev2()
       These system calls are similar to preadv() and pwritev() calls, but
       add a fifth argument, flags, which modifies the behavior on a per-
       call basis.

       Unlike preadv() and pwritev(), if the offset argument is -1, then the
       current file offset is used and updated.

       The flags argument contains a bitwise OR of zero or more of the fol‐
       lowing flags:

       RWF_DSYNC (since Linux 4.7)
              Provide a per-write equivalent of the O_DSYNC open(2) flag.
              This flag is meaningful only for pwritev2(), and its effect
              applies only to the data range written by the system call.

       RWF_HIPRI (since Linux 4.6)
              High priority read/write.  Allows block-based filesystems to
              use polling of the device, which provides lower latency, but
              may use additional resources.  (Currently, this feature is us‐
              able only on a file descriptor opened using the O_DIRECT
              flag.)

       RWF_SYNC (since Linux 4.7)
              Provide a per-write equivalent of the O_SYNC open(2) flag.
              This flag is meaningful only for pwritev2(), and its effect
              applies only to the data range written by the system call.

       RWF_NOWAIT (since Linux 4.14)
              Do not wait for data which is not immediately available.  If
              this flag is specified, the preadv2() system call will return
              instantly if it would have to read data from the backing stor‐
              age or wait for a lock.  If some data was successfully read,
              it will return the number of bytes read.  If no bytes were
              read, it will return -1 and set errno to EAGAIN.  Currently,
              this flag is meaningful only for preadv2().

       RWF_APPEND (since Linux 4.16)
              Provide a per-write equivalent of the O_APPEND open(2) flag.
              This flag is meaningful only for pwritev2(), and its effect
              applies only to the data range written by the system call.
              The offset argument does not affect the write operation; the
              data is always appended to the end of the file.  However, if
              the offset argument is -1, the current file offset is updated.
```

### 1.2 同步IO接口的缺陷

上述接口，尽管形式多样，但它们都有一个共同的特征，就是同步，即在读写IO时，系统调用会阻塞住等待，在数据读取或写入后才返回结果。

对于传统、普通的编程模型，这类同步接口编程简单，结果可预测，倒也无妨，但是在要求高效的场景下，同步导致的后果就是caller在阻塞的同时无法继续执行其他的操作，只能等待IO结果返回，其实caller本可以利用这段时间继续往后执行。例如，一个FTP server，接收到客户机上传的文件，然后将文件写入到本机的过程中，若FTP服务程序忙于等待文件读写结果的返回，则会拒绝其他此刻正需要连接的客户机请求。在这种场景下，更好的方式是采用异步编程模型，就上述例子而言，当服务器接收到某个客户机上传文件后，直接、无阻塞地将写入IO的buffer提交给内核，然后caller继续接受下一个客户请求，内核处理完IO之后，主动调用某种通知机制，告诉caller该IO已完成，完成状态保存在某位置。

存储场景中，我们对性能的要求非常高，所以我们需要异步IO。

### 1.3 AIO

后来，应这类诉求，产生了异步IO接口，即Linux Native异步IO——AIO。

AIO接口简单介绍（表格引用自*Understanding Nginx Modules Development and Architecture Resolving(Second Edition)*）：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16GKicX3CuSeI3AKVeqH4VnILf1PJWuHIQMhsovNDRib0bbED8cBDQwFYrg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

类似地，如同前文所提PostgreSQL——历史上，也有一些项目通过使用kernel的新接口，获得了不菲的收益。

例如，高性能服务器nginx就使用了这样的机制，nginx把读取文件的操作异步地提交给内核后，内核会通知IO设备独立地执行操作，这样，nginx进程可以继续充分地占用CPU，而且，当大量读事件堆积到IO设备的队列中时，将会发挥出内核中“电梯算法”的优势，从而降低随机读取磁盘扇区的成本。

### 1.4 AIO的缺陷

但是，AIO仍然不够完美，同样存在很多缺陷，同样以nginx为例，目前，nginx仅支持在读取文件时使用AIO，因为正常写入文件往往是写入内存就立刻返回，效率很高，而如果替换成AIO写入速度会明显下降。

这是因为AIO不支持缓存操作，即使需要操作的文件块在linux文件缓存中存在，也不会通过操作缓存中的文件块来代替实际对磁盘的操作，这可能降低实际处理的性能。需要看具体的使用场景，如果大部分用户请求对文件操作都会落到文件缓存中，那么使用AIO可能不是一个好的选择。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16GkJPRD77tIhdrIRp6DNEqU3uyCv1Dbw9V8dVMBlCfBBJJYoOYA2fcxw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

以上是AIO的不足之一，分析AIO缘何不足，需要较大的篇幅，这里按下不表直接总结一下其他不足之处。

- **仅支持direct IO**。在采用AIO的时候，只能使用O_DIRECT，不能借助文件系统缓存来缓存当前的IO请求，还存在size对齐（直接操作磁盘，所有写入内存块数量必须是文件系统块大小的倍数，而且要与内存页大小对齐。）等限制，这直接影响了aio在很多场景的使用。
- **仍然可能被阻塞。语义不完备**。即使应用层主观上，希望系统层采用异步IO，但是客观上，有时候还是可能会被阻塞。io_getevents(2)调用read_events读取AIO的完成events，read_events中的wait_event_interruptible_hrtimeout等待aio_read_events，如果条件不成立（events未完成）则调用__wait_event_hrtimeout进入睡眠（当然，支持用户态设置最大等待时间）。
- **拷贝开销大**。每个IO提交需要拷贝64+8字节，每个IO完成需要拷贝32字节，总共104字节的拷贝。这个拷贝开销是否可以承受，和单次IO大小有关：如果需要发送的IO本身就很大，相较之下，这点消耗可以忽略，而在大量小IO的场景下，这样的拷贝影响比较大。
- **API不友好**。每一个IO至少需要两次系统调用才能完成（submit和wait-for-completion)，需要非常小心地使用完成事件以避免丢事件。
- **系统调用开销大**。也正是因为上一条，io_submit/io_getevents造成了较大的系统调用开销，在存在spectre/meltdown（CPU熔断幽灵漏洞，CVE-2017-5754）的机器上，若要避免漏洞问题，则系统调用性能会大幅下降。所以在存储场景下，高频系统调用的性能影响较大。

在过去的数年间，针对上述限制的很多改进努力都未尽如人意。

终于，全新的异步IO引擎io_uring就在这样的环境下诞生了。

## 2.**设计——应该是什么样子**

既然是全新实现，我们是否可以不囿于现状，思考它应该是什么样子？

关于“应该是什么样子”，我曾听智超兄说过这样的一句话：“Linux应该是什么样子，它现在就是什么样子。”，这并不是类似于“存在即合理”这样的谬传，而是对Linux系统优雅哲学的高度概括，同时也是对开源自由软件精神的肯定——因为自始至终都是自由的，所以大家觉得应该是什么样子（哪里有缺陷，哪里不够优雅），大家就会自由地去修改它，所以，经过时代的发展，它的面貌与大家所期望的最相符，即众人拾柴，众望所归。

以后世上可能会有无数文章讲述io_uring是什么样子，我们先看看它应该是什么样子。

### 2.1 设计原则

如上所述，历史实现在一定场景下，会有一定问题，新实现理应反思问题、解决问题。与此同时，需要遵循一定设计原则，如下是若干原则。

- 简单：接口需要足够简单，这一点不言自明。
- 易用：同时需要足够克制，保持易于理解，就不容易误用，对于使用者来说，这是一种有效的助推（之所以如上没有采用“简单易用”这样的惯用语，是因为简单并不一定意味着易用。我们尽量避免这种不合逻辑的隐喻）。
- 可扩展：接口要有足够的扩展性，尽管某个接口是为了某种场景（如存储）而建立，但是我们需要面向未来，若有朝一日需要支持非阻塞设备（非块存储）以及网络I/O时，这里不应是桎梏。
- 特性丰富：当然，接口需要支持足够丰富的功能。
- 高效：在存储场景下，高效率始终是关键目标。
- 可伸缩性：满足峰值场景的性能需要（高效和低延迟很重要，但是峰值速率对于存储设备来讲也很重要）底层软件是基于硬件建构的，为了适应新硬件的要求，接口还需要考虑到伸缩性。

另外，因为我们的部分目标之间，本质上往往是存在一定互斥性的（如可伸缩与足够简单互斥、特性丰富与高效互斥）很难同时满足，所以，我们设计时也需要权衡。其中，io_uring始终需要围绕高效进行设计。

### 2.2 实现思路

#### 2.2.1 **解决“系统调用开销大”的问题**

针对这个问题，考虑是否每次都需要系统调用。如果能将多次系统调用中的逻辑放到有限次数中来，就能将消耗降为常数时间复杂度。

#### 2.2.2 **解决“拷贝开销大”的问题**

之所以在提交和完成事件中存在大量的内存拷贝，是因为应用程序和内核之间的通信需要拷贝数据，所以为了避免这个问题，需要重新考量应用与内核间的通信方式。我们发现，两者通信，不是必须要拷贝，通过现有技术，可以让应用与内核共享内存，用于彼此通信，需要生产者-消费者模型。

要实现核外与内核的零拷贝，最佳方式就是实现一块内存映射区域，两者共享一段内存，核外往这段内存写数据，然后通知内核使用这段内存数据，或者内核填写这段数据，核外使用这部分数据。因此，我们需要一对共享的ring buffer用于应用程序和内核之间的通信。

共享ring buffer的设计主要带来以下几个好处：

- 提交、完成请求时节省应用和内核之间的内存拷贝
- 使用SQPOLL高级特性时，应用程序无需调用系统调用
- 无锁操作，用memory ordering实现同步，通过几个简单的头尾指针的移动就可以实现快速交互。

一块用于核外传递数据给内核，一块是内核传递数据给核外，一方只读，一方只写。

- 提交队列SQ(submission queue)中，应用是IO提交的生产者，内核是消费者。
- 完成队列CQ(completion queue)中，内核是IO完成的生产者，应用是消费者。

内核控制SQ ring的head和CQ ring的tail，应用程序控制SQ ring的tail和CQ ring的head

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16GAAbF0n5q3joFXXS6dUonrrXyZVqqEeVfcHwYIZdCia80NLzbMeUia2RQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

那么他们分别需要保存的是什么数据呢？

假设A缓存区为核外写，内核读，就是将IO数据写到这个缓存区，然后通知内核来读；再假设B缓存区为内核写，核外读，他所承担的责任就是返回完成状态，标记A缓存区的其中一个entry的完成状态为成功或者失败等信息。

#### 2.2.3 **解决“API不友好”的问题**

问题在于需要多个系统调用才能完成，考虑是否可以把多个系统调用合而为一。

你可能会想到，这与上文所说的重构手法相悖，即以明确函数取代参数（Replace Parameter with Explicit Methods）——如果某个参数有多种可能的值，而函数内又以条件表达式检查这些参数值，并根据不同参数值做出不同的行为。

然而，手法只是手法，选择具体的重构手法需要遵循重构原则。在不同场景下，可能事实恰恰相反——令函数携带参数（Parameterize Method）可能是一个好的选择。

话说天下大势，分久必合，合久必分。你可能会发现这样的两个函数，它们做着类似的工作，但因少数几个值致使行为略有不同。在这种情况下，你可以将这些各自分离的函数统一起来，并通过参数来处理那些变化情况，用以简化问题。这样的修改可以去除重复代码，并提高灵活性，因为你可以用这个参数处理更多的变化情况。

也许你会发现，你无法用这种办法处理整个函数，但可以处理函数中的一部分代码。这种情况下，你应该首先将这部分代码提炼到一个独立函数中，然后再对那个提炼所得的函数使用令函数携带参数（Parameterize Method）。

2.3 实现——现在是什么样子

推导完了应该是什么样子，解析一下现在是什么样子。

#### 2.2.4 **关键数据结构**

程序等于数据结构加算法，这里先解析io_uring有哪些关键数据结构。

**io_uring、io_rings结构**

结构前面是一些标志位集合和掩码，尾部是一个柔性数组。这两个数据在前面使用mmap分配内存的时候，对应到了不同的offset，即前面IORING_OFF_SQ_RING、IORING_OFF_CQ_RING和IORING_OFF_SQES的预定于的值。

其中io_rings结构中sq, cq成员，分别代表了提交的请求的ring和已经完成的请求返回结构的ring。io_uring结构中是head和tail，用于控制队列中的头尾索引。即前文提到的，内核控制SQ ring的head和CQ ring的tail，应用程序控制SQ ring的tail和CQ ring的head。

```
struct io_uring {
 u32 head ____cacheline_aligned_in_smp;
 u32 tail ____cacheline_aligned_in_smp;
};

/*
 * This data is shared with the application through the mmap at offsets
 * IORING_OFF_SQ_RING and IORING_OFF_CQ_RING.
 *
 * The offsets to the member fields are published through struct
 * io_sqring_offsets when calling io_uring_setup.
 */
struct io_rings {
 /*
  * Head and tail offsets into the ring; the offsets need to be
  * masked to get valid indices.
  *
  * The kernel controls head of the sq ring and the tail of the cq ring,
  * and the application controls tail of the sq ring and the head of the
  * cq ring.
  */
 struct io_uring  sq, cq;
 /*
  * Bitmasks to apply to head and tail offsets (constant, equals
  * ring_entries - 1)
  */
 u32   sq_ring_mask, cq_ring_mask;
 /* Ring sizes (constant, power of 2) */
 u32   sq_ring_entries, cq_ring_entries;
 /*
  * Number of invalid entries dropped by the kernel due to
  * invalid index stored in array
  *
  * Written by the kernel, shouldn't be modified by the
  * application (i.e. get number of "new events" by comparing to
  * cached value).
  *
  * After a new SQ head value was read by the application this
  * counter includes all submissions that were dropped reaching
  * the new SQ head (and possibly more).
  */
 u32   sq_dropped;
 /*
  * Runtime SQ flags
  *
  * Written by the kernel, shouldn't be modified by the
  * application.
  *
  * The application needs a full memory barrier before checking
  * for IORING_SQ_NEED_WAKEUP after updating the sq tail.
  */
 u32   sq_flags;
 /*
  * Runtime CQ flags
  *
  * Written by the application, shouldn't be modified by the
  * kernel.
  */
 u32                     cq_flags;
 /*
  * Number of completion events lost because the queue was full;
  * this should be avoided by the application by making sure
  * there are not more requests pending than there is space in
  * the completion queue.
  *
  * Written by the kernel, shouldn't be modified by the
  * application (i.e. get number of "new events" by comparing to
  * cached value).
  *
  * As completion events come in out of order this counter is not
  * ordered with any other data.
  */
 u32   cq_overflow;
 /*
  * Ring buffer of completion events.
  *
  * The kernel writes completion events fresh every time they are
  * produced, so the application is allowed to modify pending
  * entries.
  */
 struct io_uring_cqe cqes[] ____cacheline_aligned_in_smp;
};
```

**Submission Queue Entry单元数据结构**

Submission Queue（下称SQ）是提交队列，核外写内核读的地方。Submission Queue Entry（下称SQE），即提交队列中的条目，队列由一个个条目组成。

描述一个SQE会复杂很多，不仅是因为要描述更多的信息，也是因为可扩展性这一设计原则。

我们需要操作码、标志集合、关联文件描述符、地址、偏移量，另外地，可能还需要表示优先级。

```
/*
 * IO submission data structure (Submission Queue Entry)
 */
struct io_uring_sqe {
 __u8 opcode;  /* type of operation for this sqe */
 __u8 flags;  /* IOSQE_ flags */
 __u16 ioprio;  /* ioprio for the request */
 __s32 fd;  /* file descriptor to do IO on */
 union {
  __u64 off; /* offset into file */
  __u64 addr2;
 };
 union {
  __u64 addr; /* pointer to buffer or iovecs */
  __u64 splice_off_in;
 };
 __u32 len;  /* buffer size or number of iovecs */
 union {
  __kernel_rwf_t rw_flags;
  __u32  fsync_flags;
  __u16  poll_events; /* compatibility */
  __u32  poll32_events; /* word-reversed for BE */
  __u32  sync_range_flags;
  __u32  msg_flags;
  __u32  timeout_flags;
  __u32  accept_flags;
  __u32  cancel_flags;
  __u32  open_flags;
  __u32  statx_flags;
  __u32  fadvise_advice;
  __u32  splice_flags;
 };
 __u64 user_data; /* data to be passed back at completion time */
 union {
  struct {
   /* pack this to avoid bogus arm OABI complaints */
   union {
    /* index into fixed buffers, if used */
    __u16 buf_index;
    /* for grouped buffer selection */
    __u16 buf_group;
   } __attribute__((packed));
   /* personality to use, if used */
   __u16 personality;
   __s32 splice_fd_in;
  };
  __u64 __pad2[3];
 };
};
```

- opcode是操作码，例如IORING_OP_READV，代表向量读。
- flags是标志位集合。
- ioprio是请求的优先级，对于普通的读写，具体定义可以参照ioprio_set(2)，
- fd是这个请求相关的文件描述符
- off是操作的偏移量
- addr表示这次IO操作执行的地址，如果操作码opcode描述了一个传输数据的操作，这个操作是基于向量的，addr就指向struct iovec的数组首地址，这和前文所说的preadv系统调用是一样的用法；如果不是基于向量的，那么addr必须直接包含一个地址，len这里（非向量场景）就表示这段buffer的长度，而向量场景就表示iovec的数量。
- 接下来的是一个union，表示一系列针对特定操作码opcode的一些flag。例如，对于上文所提的IORING_OP_READV，随后的flags就遵循preadv2系统调用。
- user_data是各操作码opcode通用的，内核并未染指，仅仅只是拷贝给完成事件completion event
- 结构的最后用于内存对齐，对齐到64字节，为了更丰富的特性，未来这个请求结构应该会包含更多的内容。

这就是核外往内核填写的Submission Queue Entry的数据结构，准备好这样的一个数据结构，将它写到对应的sqes所在的内存位置，然后再通知内核去对应的位置取数据，这样就完成了一次数据交接。

**Completion Queue Entry单元数据结构**

Completion Queue（下称CQ）是完成队列，内核写核外读的地方。Completion Queue Entry（下称CQE），即完成队列中的条目，队列由一个个条目组成。

描述一个CQE就简单得多。

```
/*
 * IO completion data structure (Completion Queue Entry)
 */
struct io_uring_cqe {
 __u64 user_data; /* sqe->data submission passed back */
 __s32 res;  /* result code for this event */
 __u32 flags;
};
```

- user_data就是sqe发送时核外填写的，只不过在完成时回传而已，一个常见的用例就是作为一个指针，指向原始请求。从submission queue到completion queue，内核不会动这里面的数据。
- res用来保存最终的这个sqe的执行结果，就是这个event的返回码，可以认为是系统调用的返回值，表示成功或失败等。如果接口成功的话返回传输的字节数，如果失败的话，就是错误码。如果错误发生，res就等于-EIO。
- flags是标志位集合。如果flags设置为IORING_CQE_F_BUFFER，则前16位是buffer ID（调用链：io_uring_enter -> io_iopoll_check -> io_iopoll_getevents -> io_do_iopoll -> io_iopoll_complete -> io_put_rw_kbuf -> io_put_kbuf，最终会调用io_put_kbuf，如代码所示）。

```
/*
 * cqe->flags
 *
 * IORING_CQE_F_BUFFER If set, the upper 16 bits are the buffer ID
 */
#define IORING_CQE_F_BUFFER  (1U << 0)

enum {
 IORING_CQE_BUFFER_SHIFT  = 16,
};
static unsigned int io_put_kbuf(struct io_kiocb *req, struct io_buffer *kbuf)
{
 unsigned int cflags;

 cflags = kbuf->bid << IORING_CQE_BUFFER_SHIFT;
 cflags |= IORING_CQE_F_BUFFER;
 req->flags &= ~REQ_F_BUFFER_SELECTED;
 kfree(kbuf);
 return cflags;
}
```

**上下文结构io_ring_ctx**

前面介绍了SQE/CQE等关键的数据结构，他们是用来承载数据流的关键部分，有了数据流的关键数据结构我们还需要一个上下文数据结构，用于整个io_uring控制流。这就是io_ring_ctx，贯穿整个io_uring所有过程的数据结构，基本上在任何位置只需要你能持有该结构就可以找到任何数据所在的位置，例如，sq_sqes就是指向io_uring_sqe结构的指针，指向SQEs的首地址。

```
struct io_ring_ctx {
 struct {
  struct percpu_ref refs;
 } ____cacheline_aligned_in_smp;

 struct {
  unsigned int  flags;
  unsigned int  compat: 1;
  unsigned int  limit_mem: 1;
  unsigned int  cq_overflow_flushed: 1;
  unsigned int  drain_next: 1;
  unsigned int  eventfd_async: 1;
  unsigned int  restricted: 1;

  /*
   * Ring buffer of indices into array of io_uring_sqe, which is
   * mmapped by the application using the IORING_OFF_SQES offset.
   *
   * This indirection could e.g. be used to assign fixed
   * io_uring_sqe entries to operations and only submit them to
   * the queue when needed.
   *
   * The kernel modifies neither the indices array nor the entries
   * array.
   */
  u32   *sq_array;
  unsigned  cached_sq_head;
  unsigned  sq_entries;
  unsigned  sq_mask;
  unsigned  sq_thread_idle;
  unsigned  cached_sq_dropped;
  unsigned  cached_cq_overflow;
  unsigned long  sq_check_overflow;

  struct list_head defer_list;
  struct list_head timeout_list;
  struct list_head cq_overflow_list;

  wait_queue_head_t inflight_wait;
  struct io_uring_sqe *sq_sqes;
 } ____cacheline_aligned_in_smp;

 struct io_rings *rings;

 /* IO offload */
 struct io_wq  *io_wq;

 /*
  * For SQPOLL usage - we hold a reference to the parent task, so we
  * have access to the ->files
  */
 struct task_struct *sqo_task;

 /* Only used for accounting purposes */
 struct mm_struct *mm_account;

#ifdef CONFIG_BLK_CGROUP
 struct cgroup_subsys_state *sqo_blkcg_css;
#endif

 struct io_sq_data *sq_data; /* if using sq thread polling */

 struct wait_queue_head sqo_sq_wait;
 struct wait_queue_entry sqo_wait_entry;
 struct list_head sqd_list;

 /*
  * If used, fixed file set. Writers must ensure that ->refs is dead,
  * readers must ensure that ->refs is alive as long as the file* is
  * used. Only updated through io_uring_register(2).
  */
 struct fixed_file_data *file_data;
 unsigned  nr_user_files;

 /* if used, fixed mapped user buffers */
 unsigned  nr_user_bufs;
 struct io_mapped_ubuf *user_bufs;

 struct user_struct *user;

 const struct cred *creds;

#ifdef CONFIG_AUDIT
 kuid_t   loginuid;
 unsigned int  sessionid;
#endif

 struct completion ref_comp;
 struct completion sq_thread_comp;

 /* if all else fails... */
 struct io_kiocb  *fallback_req;

#if defined(CONFIG_UNIX)
 struct socket  *ring_sock;
#endif

 struct idr  io_buffer_idr;

 struct idr  personality_idr;

 struct {
  unsigned  cached_cq_tail;
  unsigned  cq_entries;
  unsigned  cq_mask;
  atomic_t  cq_timeouts;
  unsigned long  cq_check_overflow;
  struct wait_queue_head cq_wait;
  struct fasync_struct *cq_fasync;
  struct eventfd_ctx *cq_ev_fd;
 } ____cacheline_aligned_in_smp;

 struct {
  struct mutex  uring_lock;
  wait_queue_head_t wait;
 } ____cacheline_aligned_in_smp;

 struct {
  spinlock_t  completion_lock;

  /*
   * ->iopoll_list is protected by the ctx->uring_lock for
   * io_uring instances that don't use IORING_SETUP_SQPOLL.
   * For SQPOLL, only the single threaded io_sq_thread() will
   * manipulate the list, hence no extra locking is needed there.
   */
  struct list_head iopoll_list;
  struct hlist_head *cancel_hash;
  unsigned  cancel_hash_bits;
  bool   poll_multi_file;

  spinlock_t  inflight_lock;
  struct list_head inflight_list;
 } ____cacheline_aligned_in_smp;

 struct delayed_work  file_put_work;
 struct llist_head  file_put_llist;

 struct work_struct  exit_work;
 struct io_restriction  restrictions;
};
```

#### **2.2.5  关键流程**

数据结构定义好了，逻辑实现具体是如何驱动这些数据结构的呢？使用上，大体分为准备、提交、收割过程。

有几个io_uring相关的系统调用：

```
#include <linux/io_uring.h>

int io_uring_setup(u32 entries, struct io_uring_params *p);

int io_uring_enter(unsigned int fd, unsigned int to_submit,
                   unsigned int min_complete, unsigned int flags,
                   sigset_t *sig);
                   
int io_uring_register(unsigned int fd, unsigned int opcode,
                      void *arg, unsigned int nr_args);
```

下面分析关键流程。

**io_uring准备阶段**

io_uring通过io_uring_setup完成准备阶段。

```
int io_uring_setup(u32 entries, struct io_uring_params *p);
```

io_uring_setup系统调用的过程就是初始化相关数据结构，建立好对应的缓存区，然后通过系统调用的参数io_uring_params结构传递回去，告诉核外环内存地址在哪，起始指针的地址在哪等关键的信息。

需要初始化内存的内存分为三个区域，分别是SQ，CQ，SQEs。内核初始化SQ和CQ，此外，提交请求在SQ，CQ之间有一个间接数组，即内核提供了一个Submission Queue Entries（SQEs）数组。之所以额外采用了一个数组保存SQEs，是为了方便通过环形缓冲区提交内存上不连续的请求。SQ和CQ中每个节点保存的都是SQEs数组的索引，而不是实际的请求，实际的请求只保存在SQEs数组中。这样在提交请求时，就可以批量提交一组SQEs上不连续的请求。

通常，SQE被独立地使用，意味着它的执行不影响在ring中的连续SQE条目。它允许全面、灵活的操作，并且使它们最高性能地并行执行完成。一个顺序的使用案例就是数据的整体写入。它的一个通常的例子就是一系列写，随之的是fsync/fdatasync，应用通常转变成程序同步-等待操作。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16GCgEc4Twct3f5E0OaWI2gCwjc00chtKHu8JrsW8YYfkIvclSNUKvQeA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

先从参数上来解析

- 核外需要告诉io_uring_setup提交的整个缓存区数组的大小。（代表 queue depth？），这里就是entries参数。

- params这个参数从IO的角度看有两种，一种是输入参数，一种是输出参数。

- - sq_entries是输出参数，由内核填充，让应用程序知道这个ring支持多少SQE。
  - 类似地，cq_entries告诉应用程序，CQ ring有多大。
  - sq_off和cq_off分别是io_sqring_offsets和io_cqring_offsets结构，是内核与核外的约定，分别描述了SQ和CQ的指针在mmap中的offset
  - 其他的结构成员涉及到高级用法，暂时按下不表。
  - 比如params->flags，这个成员变量是用来设置当前整个io_uring 的标志的，它将决定是否启动sq_thread，是否采用iopoll模式等等
  - sq_thread_cpu、sq_thread_idle也由用户设置，用来指定io_sq_thread内核线程CPU、idle时间。
  - 一部分属于输入参数，是用户设置、核外传递给核外的，用于定义io_uring在内核中的行为，这些都是在创建阶段就决定了的。
  - 还有一部分属于输出参数，由内核设置（io_uring_create）、传递数据到核外的，核外根据这些数据来使用mmap分配内存，初始化一些数据结构。

```
/*
 * Passed in for io_uring_setup(2). Copied back with updated info on success
 */
struct io_uring_params {
 __u32 sq_entries;
 __u32 cq_entries;
 __u32 flags;
 __u32 sq_thread_cpu;
 __u32 sq_thread_idle;
 __u32 features;
 __u32 wq_fd;
 __u32 resv[3];
 struct io_sqring_offsets sq_off;
 struct io_cqring_offsets cq_off;
};

/*
 * io_uring_params->features flags
 */
#define IORING_FEAT_SINGLE_MMAP  (1U << 0)
#define IORING_FEAT_NODROP  (1U << 1)
#define IORING_FEAT_SUBMIT_STABLE (1U << 2)
#define IORING_FEAT_RW_CUR_POS  (1U << 3)
#define IORING_FEAT_CUR_PERSONALITY (1U << 4)
#define IORING_FEAT_FAST_POLL  (1U << 5)
#define IORING_FEAT_POLL_32BITS  (1U << 6)
```

再从实现上来解析，如下为io_uring_setup代码。

```
/*
 * Sets up an aio uring context, and returns the fd. Applications asks for a
 * ring size, we return the actual sq/cq ring sizes (among other things) in the
 * params structure passed in.
 */
static long io_uring_setup(u32 entries, struct io_uring_params __user *params)
{
 struct io_uring_params p;
 int i;

 if (copy_from_user(&p, params, sizeof(p)))
  return -EFAULT;
 for (i = 0; i < ARRAY_SIZE(p.resv); i++) {
  if (p.resv[i])
   return -EINVAL;
 }

 if (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |
   IORING_SETUP_SQ_AFF | IORING_SETUP_CQSIZE |
   IORING_SETUP_CLAMP | IORING_SETUP_ATTACH_WQ |
   IORING_SETUP_R_DISABLED))
  return -EINVAL;

 return  io_uring_create(entries, &p, params);
}
```

经过标志位非法检查之后，关键是调用内部函数io_uring_create实现实例创建过程。

- 首先需要创建一个上下文结构io_ring_ctx用来管理整个会话。
- 随后实现SQ和CQ内存区的映射，使用IORING_OFF_CQ_RING偏移量，使用io_cqring_offsets结构的实例，即io_uring_params中cq_off这个成员，SQ使用IORING_OFF_SQES这个偏移量。
- 其余的是一些错误检查、权限检查、资源配额检查等检查逻辑。

```
static int io_uring_create(unsigned entriesstatic int io_uring_create(unsigned entries, struct io_uring_params *p,
      struct io_uring_params __user *params)
{
 struct user_struct *user = NULL;
 struct io_ring_ctx *ctx;
 bool limit_mem;
 int ret;

 if (!entries)
  return -EINVAL;
 if (entries > IORING_MAX_ENTRIES) {
  if (!(p->flags & IORING_SETUP_CLAMP))
   return -EINVAL;
  entries = IORING_MAX_ENTRIES;
 }

 /*
  * Use twice as many entries for the CQ ring. It's possible for the
  * application to drive a higher depth than the size of the SQ ring,
  * since the sqes are only used at submission time. This allows for
  * some flexibility in overcommitting a bit. If the application has
  * set IORING_SETUP_CQSIZE, it will have passed in the desired number
  * of CQ ring entries manually.
  */
 p->sq_entries = roundup_pow_of_two(entries);
 if (p->flags & IORING_SETUP_CQSIZE) {
  /*
   * If IORING_SETUP_CQSIZE is set, we do the same roundup
   * to a power-of-two, if it isn't already. We do NOT impose
   * any cq vs sq ring sizing.
   */
  if (!p->cq_entries)
   return -EINVAL;
  if (p->cq_entries > IORING_MAX_CQ_ENTRIES) {
   if (!(p->flags & IORING_SETUP_CLAMP))
    return -EINVAL;
   p->cq_entries = IORING_MAX_CQ_ENTRIES;
  }
  p->cq_entries = roundup_pow_of_two(p->cq_entries);
  if (p->cq_entries < p->sq_entries)
   return -EINVAL;
 } else {
  p->cq_entries = 2 * p->sq_entries;
 }

 user = get_uid(current_user());
 limit_mem = !capable(CAP_IPC_LOCK);

 if (limit_mem) {
  ret = __io_account_mem(user,
    ring_pages(p->sq_entries, p->cq_entries));
  if (ret) {
   free_uid(user);
   return ret;
  }
 }

 ctx = io_ring_ctx_alloc(p);
 if (!ctx) {
  if (limit_mem)
   __io_unaccount_mem(user, ring_pages(p->sq_entries,
        p->cq_entries));
  free_uid(user);
  return -ENOMEM;
 }
 ctx->compat = in_compat_syscall();
 ctx->user = user;
 ctx->creds = get_current_cred();
#ifdef CONFIG_AUDIT
 ctx->loginuid = current->loginuid;
 ctx->sessionid = current->sessionid;
#endif
 ctx->sqo_task = get_task_struct(current);

 /*
  * This is just grabbed for accounting purposes. When a process exits,
  * the mm is exited and dropped before the files, hence we need to hang
  * on to this mm purely for the purposes of being able to unaccount
  * memory (locked/pinned vm). It's not used for anything else.
  */
 mmgrab(current->mm);
 ctx->mm_account = current->mm;

#ifdef CONFIG_BLK_CGROUP
 /*
  * The sq thread will belong to the original cgroup it was inited in.
  * If the cgroup goes offline (e.g. disabling the io controller), then
  * issued bios will be associated with the closest cgroup later in the
  * block layer.
  */
 rcu_read_lock();
 ctx->sqo_blkcg_css = blkcg_css();
 ret = css_tryget_online(ctx->sqo_blkcg_css);
 rcu_read_unlock();
 if (!ret) {
  /* don't init against a dying cgroup, have the user try again */
  ctx->sqo_blkcg_css = NULL;
  ret = -ENODEV;
  goto err;
 }
#endif

 /*
  * Account memory _before_ installing the file descriptor. Once
  * the descriptor is installed, it can get closed at any time. Also
  * do this before hitting the general error path, as ring freeing
  * will un-account as well.
  */
 io_account_mem(ctx, ring_pages(p->sq_entries, p->cq_entries),
         ACCT_LOCKED);
 ctx->limit_mem = limit_mem;

 ret = io_allocate_scq_urings(ctx, p);
 if (ret)
  goto err;

 ret = io_sq_offload_create(ctx, p);
 if (ret)
  goto err;

 if (!(p->flags & IORING_SETUP_R_DISABLED))
  io_sq_offload_start(ctx);

 memset(&p->sq_off, 0, sizeof(p->sq_off));
 p->sq_off.head = offsetof(struct io_rings, sq.head);
 p->sq_off.tail = offsetof(struct io_rings, sq.tail);
 p->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);
 p->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);
 p->sq_off.flags = offsetof(struct io_rings, sq_flags);
 p->sq_off.dropped = offsetof(struct io_rings, sq_dropped);
 p->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;

 memset(&p->cq_off, 0, sizeof(p->cq_off));
 p->cq_off.head = offsetof(struct io_rings, cq.head);
 p->cq_off.tail = offsetof(struct io_rings, cq.tail);
 p->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);
 p->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);
 p->cq_off.overflow = offsetof(struct io_rings, cq_overflow);
 p->cq_off.cqes = offsetof(struct io_rings, cqes);
 p->cq_off.flags = offsetof(struct io_rings, cq_flags);

 p->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |
   IORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |
   IORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |
   IORING_FEAT_POLL_32BITS;

 if (copy_to_user(params, p, sizeof(*p))) {
  ret = -EFAULT;
  goto err;
 }

 /*
  * Install ring fd as the very last thing, so we don't risk someone
  * having closed it before we finish setup
  */
 ret = io_uring_get_fd(ctx);
 if (ret < 0)
  goto err;

 trace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);
 return ret;
err:
 io_ring_ctx_wait_and_kill(ctx);
 return ret;
}
```

io_sqring_offsets、io_cqring_offsets等相关结构、标志位集合。

预定义offset
如果要表征分配的是io uring相关的一些内存，就需要预定义一些offset，如IORING_OFF_SQ_RING、IORING_OFF_SQES和IORING_OFF_CQ_RING，这些offset值定义了保存到这个三个结构保存到位置。这里mmap的时候，就使用了这些offset。

```
/*
 * Magic offsets for the application to mmap the data it needs
 */
#define IORING_OFF_SQ_RING  0ULL
#define IORING_OFF_CQ_RING  0x8000000ULL
#define IORING_OFF_SQES   0x10000000ULL

/*
 * Filled with the offset for mmap(2)
 */
struct io_sqring_offsets {
 __u32 head;
 __u32 tail;
 __u32 ring_mask;
 __u32 ring_entries;
 __u32 flags;
 __u32 dropped;
 __u32 array;
 __u32 resv1;
 __u64 resv2;
};

/*
 * sq_ring->flags
 */
#define IORING_SQ_NEED_WAKEUP (1U << 0) /* needs io_uring_enter wakeup */
#define IORING_SQ_CQ_OVERFLOW (1U << 1) /* CQ ring is overflown */

struct io_cqring_offsets {
 __u32 head;
 __u32 tail;
 __u32 ring_mask;
 __u32 ring_entries;
 __u32 overflow;
 __u32 cqes;
 __u32 flags;
 __u32 resv1;
 __u64 resv2;
};

/*
 * cq_ring->flags
 */

/* disable eventfd notifications */
#define IORING_CQ_EVENTFD_DISABLED (1U << 0)

/*
 * io_uring_enter(2) flags
 */
#define IORING_ENTER_GETEVENTS (1U << 0)
#define IORING_ENTER_SQ_WAKEUP (1U << 1)
#define IORING_ENTER_SQ_WAIT (1U << 2)

/*
 * io_uring_register(2) opcodes and arguments
 */
enum {
 IORING_REGISTER_BUFFERS   = 0,
 IORING_UNREGISTER_BUFFERS  = 1,
 IORING_REGISTER_FILES   = 2,
 IORING_UNREGISTER_FILES   = 3,
 IORING_REGISTER_EVENTFD   = 4,
 IORING_UNREGISTER_EVENTFD  = 5,
 IORING_REGISTER_FILES_UPDATE  = 6,
 IORING_REGISTER_EVENTFD_ASYNC  = 7,
 IORING_REGISTER_PROBE   = 8,
 IORING_REGISTER_PERSONALITY  = 9,
 IORING_UNREGISTER_PERSONALITY  = 10,
 IORING_REGISTER_RESTRICTIONS  = 11,
 IORING_REGISTER_ENABLE_RINGS  = 12,

 /* this goes last */
 IORING_REGISTER_LAST
};
```

具体的实践，可以参考如下liburing中的初始化函数io_uring_queue_init中对io_uring_setup的使用（http://git.kernel.dk/cgit/liburing/tree/src/setup.c）。

liburing中使用io_uring_setup的部分代码

```
/*
 * Returns -1 on error, or zero on success. On success, 'ring'
 * contains the necessary information to read/write to the rings.
 */
int io_uring_queue_init(unsigned entries, struct io_uring *ring, unsigned flags)
{
 struct io_uring_params p;
 int fd, ret;

 memset(&p, 0, sizeof(p));
 p.flags = flags;

 fd = io_uring_setup(entries, &p);
 if (fd < 0)
  return fd;

 ret = io_uring_queue_mmap(fd, &p, ring);
 if (ret)
  close(fd);

 return ret;
}
```

注意mmap的时候需要传入MAP_POPULATE参数，为文件映射通过预读的方式准备好页表，随后对映射区的访问不会被page fault。

**IO提交**

在初始化完成之后，应用程序就可以使用这些队列来添加IO请求，即填充SQE。当请求都加入SQ后，应用程序还需要某种方式告诉内核，生产的请求待消费，这就是提交IO请求，可以通过io_uring_enter系统调用。

```
int io_uring_enter(unsigned int fd, unsigned int to_submit,
                   unsigned int min_complete, unsigned int flags,
                   sigset_t *sig);
```

内核将SQ中的请求提交给Block层。这个系统调用既能提交，也能等待。

具体的实现是找到一个空闲的SQE，根据请求设置SQE，并将这个SQE的索引放到SQ中。SQ是一个典型的ring buffer，有head，tail两个成员，如果head == tail，意味着队列为空。SQE设置完成后，需要修改SQ的tail，以表示向ring buffer中插入了一个请求。

先从参数上来解析

- fd即由io_uring_setup(2)返回的文件描述符，

- to_submit告诉内核待消费和提交的SQE的数量，表示一次提交多少个 IO，

- min_complete请求完成请求的个数。

- flags是修饰接口行为的标志集合，这里主要例举两个flags

- - 如果在io_uring_setup的时候flag设置了IORING_SETUP_SQPOLL，内核会额外启动一个特定的内核线程来执行轮询的操作，称作SQ线程，这里使用的轮询结构会最终对应到struct file_operations中的iopoll操作，这个操作作为一个新的接口在最近才添加到这里，Linux native aio的新功能也使用了这个iopoll。这里io _uring实际上只有vfs层的改动，其它的都是使用已经存在的东西，而且几个核心的东西和aio使用的相同/类似。直接通过访问相关的队列就可以获取到执行完的任务，不需要经过系统调用。关于这个线程，通过io_uring_params结构中的sq_thread_cpu配置，这个内核线程可以运行在某个指定的 CPU核心 上。这个内核线程会不停的 Poll SQ，直到在通过sq_thread_idle配置的时间内没有Poll到任何请求为止。
  - 如果flags中设置了IORING_ENTER_GETEVENTS，并且min_complete > 0，这个系统调用会一直 block，直到 min_complete 个 IO 已经完成才返回。这个系统调用会同时处理 IO 收割。
  - 另外的，IORING_SQ_NEED_WAKEUP可以表示在一些时候唤醒休眠中的轮询线程。

static int io_sq_thread(void *data)即内核轮询线程。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16GgXCtyc2gibXmpBJAjCiaRfDI0XHanoP7RK9yGsk72TF2kWlzfm3Dr3lg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

同样地，可以用这个系统调用等待完成。除非应用程序，内核会直接修改CQ，因此调用io_uring_enter系统调用时不必使用IORING_ENTER_GETEVENTS，完成就可以被应用程序消费。

io_uring提供了submission offload模式，使得提交过程完全不需要进行系统调用。当程序在用户态设置完SQE，并通过修改SQ的tail完成一次插入时，如果此时SQ线程处于唤醒状态，那么可以立刻捕获到这次提交，这样就避免了用户程序调用io_uring_enter。如上所说，如果SQ线程处于休眠状态，则需要通过使用IORING_SQ_NEED_WAKEUP标志位调用io_uring_enter来唤醒SQ线程。

以io_iopoll_check为例，正常情况下执行路线是io_iopoll_check -> io_iopoll_getevents -> io_do_iopoll -> (kiocb->ki_filp->f_op->iopoll). 在完成请求的操作之后，会调用下面这个函数提交结果到cqe数组中，这样应用就能看到结果了。这里的io_cqring_fill_event就是获取一个目前可以写入到cqe，写入数据。这里最终调用的会是io_get_cqring，可以见就是返回目前tail的后面的一个。

更详细的内容可以直接参考io_uring_enter(2)的man page。

内核中io_uring_enter的相关代码如下。

```
SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
  u32, min_complete, u32, flags, const sigset_t __user *, sig,
  size_t, sigsz)
{
 struct io_ring_ctx *ctx;
 long ret = -EBADF;
 int submitted = 0;
 struct fd f;

 io_run_task_work();

 if (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |
   IORING_ENTER_SQ_WAIT))
  return -EINVAL;

 f = fdget(fd);
 if (!f.file)
  return -EBADF;

 ret = -EOPNOTSUPP;
 if (f.file->f_op != &io_uring_fops)
  goto out_fput;

 ret = -ENXIO;
 ctx = f.file->private_data;
 if (!percpu_ref_tryget(&ctx->refs))
  goto out_fput;

 ret = -EBADFD;
 if (ctx->flags & IORING_SETUP_R_DISABLED)
  goto out;

 /*
  * For SQ polling, the thread will do all submissions and completions.
  * Just return the requested submit count, and wake the thread if
  * we were asked to.
  */
 ret = 0;
 if (ctx->flags & IORING_SETUP_SQPOLL) {
  if (!list_empty_careful(&ctx->cq_overflow_list))
   io_cqring_overflow_flush(ctx, false, NULL, NULL);
  if (flags & IORING_ENTER_SQ_WAKEUP)
   wake_up(&ctx->sq_data->wait);
  if (flags & IORING_ENTER_SQ_WAIT)
   io_sqpoll_wait_sq(ctx);
  submitted = to_submit;
 } else if (to_submit) {
  ret = io_uring_add_task_file(ctx, f.file);
  if (unlikely(ret))
   goto out;
  mutex_lock(&ctx->uring_lock);
  submitted = io_submit_sqes(ctx, to_submit);
  mutex_unlock(&ctx->uring_lock);

  if (submitted != to_submit)
   goto out;
 }
 if (flags & IORING_ENTER_GETEVENTS) {
  min_complete = min(min_complete, ctx->cq_entries);

  /*
   * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user
   * space applications don't need to do io completion events
   * polling again, they can rely on io_sq_thread to do polling
   * work, which can reduce cpu usage and uring_lock contention.
   */
  if (ctx->flags & IORING_SETUP_IOPOLL &&
      !(ctx->flags & IORING_SETUP_SQPOLL)) {
   ret = io_iopoll_check(ctx, min_complete);
  } else {
   ret = io_cqring_wait(ctx, min_complete, sig, sigsz);
  }
 }

out:
 percpu_ref_put(&ctx->refs);
out_fput:
 fdput(f);
 return submitted ? submitted : ret;
}
```

io_iopoll_complete实现

```
/*
 * Find and free completed poll iocbs
 */
static void io_iopoll_complete(struct io_ring_ctx *ctx, unsigned int *nr_events,
          struct list_head *done)
{
 struct req_batch rb;
 struct io_kiocb *req;
 LIST_HEAD(again);

 /* order with ->result store in io_complete_rw_iopoll() */
 smp_rmb();

 io_init_req_batch(&rb);
 while (!list_empty(done)) {
  int cflags = 0;

  req = list_first_entry(done, struct io_kiocb, inflight_entry);
  if (READ_ONCE(req->result) == -EAGAIN) {
   req->result = 0;
   req->iopoll_completed = 0;
   list_move_tail(&req->inflight_entry, &again);
   continue;
  }
  list_del(&req->inflight_entry);

  if (req->flags & REQ_F_BUFFER_SELECTED)
   cflags = io_put_rw_kbuf(req);

  __io_cqring_fill_event(req, req->result, cflags);
  (*nr_events)++;

  if (refcount_dec_and_test(&req->refs))
   io_req_free_batch(&rb, req);
 }

 io_commit_cqring(ctx);
 if (ctx->flags & IORING_SETUP_SQPOLL)
  io_cqring_ev_posted(ctx);
 io_req_free_batch_finish(ctx, &rb);

 if (!list_empty(&again))
  io_iopoll_queue(&again);
}
```

io_get_cqring实现

```
static struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)
{
 struct io_rings *rings = ctx->rings;
 unsigned tail;

 tail = ctx->cached_cq_tail;
 /*
  * writes to the cq entry need to come after reading head; the
  * control dependency is enough as we're using WRITE_ONCE to
  * fill the cq entry
  */
 if (tail - READ_ONCE(rings->cq.head) == rings->cq_ring_entries)
  return NULL;

 ctx->cached_cq_tail++;
 return &rings->cqes[tail & ctx->cq_mask];
}
```

**IO收割**

来都来了，搞点事情吧，在我们提交IO的同时，使用同一个io_uring_enter系统调用就可以回收完成状态，这样的好处就是一次系统调用接口就完成了原本需要两次系统调用的工作，大大的减少了系统调用的次数，也就是减少了内核核外的切换，这是一个很明显的优化，内核与核外的切换极其耗时。

当IO完成时，内核负责将完成IO在SQEs中的index放到CQ中。由于IO在提交的时候可以顺便返回完成的IO，所以收割IO不需要额外系统调用。

如果使用了IORING_SETUP_SQPOLL参数，IO收割也不需要系统调用的参与。由于内核和用户态共享内存，所以收割的时候，用户态遍历[cring->head, cring->tail)区间，即已经完成的IO队列，然后找到相应的CQE并进行处理，最后移动head指针到tail，IO收割至此而终。

所以，在最理想的情况下，IO提交和收割都不需要使用系统调用。

### 2.3 高级特性

此外，我们可以使用一些优化思想，进行更进一步的优化，这些优化，以一种可选的方式成为io_uring的其它一些高级特性。

## 3.**Fixed Files模式**

### 3.1 优化思想

非关键逻辑上提至循环外，简化关键路径。

### 3.2 优化实现

可以调用io_uring_register系统调用，使用IORING_REGISTER_FILES操作码，将一组file注册到内核，最终调用io_sqe_files_register，这样内核在注册阶段就批量完成文件的一些基本操作（对于这组文件填充相应的数据结构fixed_file_data，其中fixed_file_table是维护的file表。内核态下，如何获得文件描述符获取相关的信息呢，就需要通过fget，根据fd号获得指向文件的struct file），之后的再次批量IO时就不需要重复地进行此类基本信息设置（更具体地，例如对文件进行fget/fput操作）。如果需要进行IO操作的文件相对固定（比如数据库日志），这会节省一定量的IO时间。

### 3.3 fixed_file_data结构

```
struct fixed_file_data {
 struct fixed_file_table  *table;
 struct io_ring_ctx  *ctx;

 struct fixed_file_ref_node *node;
 struct percpu_ref  refs;
 struct completion  done;
 struct list_head  ref_list;
 spinlock_t   lock;
};
```

### 3.4 io_sqe_files_register实现Fixed Files操作

```
static int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,
     unsigned nr_args)
{
 __s32 __user *fds = (__s32 __user *) arg;
 unsigned nr_tables, i;
 struct file *file;
 int fd, ret = -ENOMEM;
 struct fixed_file_ref_node *ref_node;
 struct fixed_file_data *file_data;

 if (ctx->file_data)
  return -EBUSY;
 if (!nr_args)
  return -EINVAL;
 if (nr_args > IORING_MAX_FIXED_FILES)
  return -EMFILE;

 file_data = kzalloc(sizeof(*ctx->file_data), GFP_KERNEL);
 if (!file_data)
  return -ENOMEM;
 file_data->ctx = ctx;
 init_completion(&file_data->done);
 INIT_LIST_HEAD(&file_data->ref_list);
 spin_lock_init(&file_data->lock);

 nr_tables = DIV_ROUND_UP(nr_args, IORING_MAX_FILES_TABLE);
 file_data->table = kcalloc(nr_tables, sizeof(*file_data->table),
       GFP_KERNEL);
 if (!file_data->table)
  goto out_free;

 if (percpu_ref_init(&file_data->refs, io_file_ref_kill,
    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))
  goto out_free;

 if (io_sqe_alloc_file_tables(file_data, nr_tables, nr_args))
  goto out_ref;
 ctx->file_data = file_data;

 for (i = 0; i < nr_args; i++, ctx->nr_user_files++) {
  struct fixed_file_table *table;
  unsigned index;

  if (copy_from_user(&fd, &fds[i], sizeof(fd))) {
   ret = -EFAULT;
   goto out_fput;
  }
  /* allow sparse sets */
  if (fd == -1)
   continue;

  file = fget(fd);
  ret = -EBADF;
  if (!file)
   goto out_fput;

  /*
   * Don't allow io_uring instances to be registered. If UNIX
   * isn't enabled, then this causes a reference cycle and this
   * instance can never get freed. If UNIX is enabled we'll
   * handle it just fine, but there's still no point in allowing
   * a ring fd as it doesn't support regular read/write anyway.
   */
  if (file->f_op == &io_uring_fops) {
   fput(file);
   goto out_fput;
  }
  table = &file_data->table[i >> IORING_FILE_TABLE_SHIFT];
  index = i & IORING_FILE_TABLE_MASK;
  table->files[index] = file;
 }

 ret = io_sqe_files_scm(ctx);
 if (ret) {
  io_sqe_files_unregister(ctx);
  return ret;
 }

 ref_node = alloc_fixed_file_ref_node(ctx);
 if (IS_ERR(ref_node)) {
  io_sqe_files_unregister(ctx);
  return PTR_ERR(ref_node);
 }

 file_data->node = ref_node;
 spin_lock(&file_data->lock);
 list_add_tail(&ref_node->node, &file_data->ref_list);
 spin_unlock(&file_data->lock);
 percpu_ref_get(&file_data->refs);
 return ret;
out_fput:
 for (i = 0; i < ctx->nr_user_files; i++) {
  file = io_file_from_index(ctx, i);
  if (file)
   fput(file);
 }
 for (i = 0; i < nr_tables; i++)
  kfree(file_data->table[i].files);
 ctx->nr_user_files = 0;
out_ref:
 percpu_ref_exit(&file_data->refs);
out_free:
 kfree(file_data->table);
 kfree(file_data);
 ctx->file_data = NULL;
 return ret;
}
```

## **4.Fixed Buffers模式**

### 4.1 优化思想

优化思想也是将非关键逻辑上提至循环外，简化关键路径。

### 4.2 优化实现

如果应用提交到内核的虚拟内存地址是固定的，那么可以提前完成虚拟地址到物理pages的映射，将这个并不是每次都要做的非关键路径从关键的IO 路径中剥离，避免每次I/O都进行转换，从而优化性能。可以在io_uring_setup之后，调用io_uring_register，使用IORING_REGISTER_BUFFERS 操作码，将一组buffer注册到内核（参数是一个指向iovec的数组，表示这些地址需要map到内核），最终调用io_sqe_buffer_register，这样内核在注册阶段就批量完成buffer的一些基本操作（减小get_user_pages、put_page开销，提前使用get_user_pages来获得userspace虚拟地址对应的物理pages，初始化在io_ring_ctx上下文中用于管理用户态buffer的io_mapped_ubuf数据结构，map/unmap，传递IOV的地址和长度等），之后的再次批量IO时就不需要重复地进行此类内存拷贝和基础信息检测。

在操作IO的时，如果需要进行IO操作的buffer相对固定，提交的虚拟地址曾经被注册过，那么可以使用带FIXED系列的opcode（IORING_OP_READ_FIXED/IORING_OP_WRITE_FIXED）IO，可以看到底层调用链：io_issue_sqe->io_read->io_import_iovec->__io_import_iovec->io_import_fixed，会直接使用已经完成的“成果”，如此就免去了虚拟地址到pages的转换，这会节省一定量的IO时间。

#### 4.2.1 **io_mapped_ubuf结构**

```
struct io_mapped_ubuf {
 u64  ubuf;
 size_t  len;
 struct  bio_vec *bvec;
 unsigned int nr_bvecs;
 unsigned long acct_pages;
};
```

#### **4.2.2 io_sqe_buffer_register实现Fixed Buffers操作**

```
static int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,
      unsigned nr_args)
{
 struct vm_area_struct **vmas = NULL;
 struct page **pages = NULL;
 struct page *last_hpage = NULL;
 int i, j, got_pages = 0;
 int ret = -EINVAL;

 if (ctx->user_bufs)
  return -EBUSY;
 if (!nr_args || nr_args > UIO_MAXIOV)
  return -EINVAL;

 ctx->user_bufs = kcalloc(nr_args, sizeof(struct io_mapped_ubuf),
     GFP_KERNEL);
 if (!ctx->user_bufs)
  return -ENOMEM;

 for (i = 0; i < nr_args; i++) {
  struct io_mapped_ubuf *imu = &ctx->user_bufs[i];
  unsigned long off, start, end, ubuf;
  int pret, nr_pages;
  struct iovec iov;
  size_t size;

  ret = io_copy_iov(ctx, &iov, arg, i);
  if (ret)
   goto err;

  /*
   * Don't impose further limits on the size and buffer
   * constraints here, we'll -EINVAL later when IO is
   * submitted if they are wrong.
   */
  ret = -EFAULT;
  if (!iov.iov_base || !iov.iov_len)
   goto err;

  /* arbitrary limit, but we need something */
  if (iov.iov_len > SZ_1G)
   goto err;

  ubuf = (unsigned long) iov.iov_base;
  end = (ubuf + iov.iov_len + PAGE_SIZE - 1) >> PAGE_SHIFT;
  start = ubuf >> PAGE_SHIFT;
  nr_pages = end - start;

  ret = 0;
  if (!pages || nr_pages > got_pages) {
   kvfree(vmas);
   kvfree(pages);
   pages = kvmalloc_array(nr_pages, sizeof(struct page *),
      GFP_KERNEL);
   vmas = kvmalloc_array(nr_pages,
     sizeof(struct vm_area_struct *),
     GFP_KERNEL);
   if (!pages || !vmas) {
    ret = -ENOMEM;
    goto err;
   }
   got_pages = nr_pages;
  }

  imu->bvec = kvmalloc_array(nr_pages, sizeof(struct bio_vec),
      GFP_KERNEL);
  ret = -ENOMEM;
  if (!imu->bvec)
   goto err;

  ret = 0;
  mmap_read_lock(current->mm);
  pret = pin_user_pages(ubuf, nr_pages,
          FOLL_WRITE | FOLL_LONGTERM,
          pages, vmas);
  if (pret == nr_pages) {
   /* don't support file backed memory */
   for (j = 0; j < nr_pages; j++) {
    struct vm_area_struct *vma = vmas[j];

    if (vma->vm_file &&
        !is_file_hugepages(vma->vm_file)) {
     ret = -EOPNOTSUPP;
     break;
    }
   }
  } else {
   ret = pret < 0 ? pret : -EFAULT;
  }
  mmap_read_unlock(current->mm);
  if (ret) {
   /*
    * if we did partial map, or found file backed vmas,
    * release any pages we did get
    */
   if (pret > 0)
    unpin_user_pages(pages, pret);
   kvfree(imu->bvec);
   goto err;
  }

  ret = io_buffer_account_pin(ctx, pages, pret, imu, &last_hpage);
  if (ret) {
   unpin_user_pages(pages, pret);
   kvfree(imu->bvec);
   goto err;
  }

  off = ubuf & ~PAGE_MASK;
  size = iov.iov_len;
  for (j = 0; j < nr_pages; j++) {
   size_t vec_len;

   vec_len = min_t(size_t, size, PAGE_SIZE - off);
   imu->bvec[j].bv_page = pages[j];
   imu->bvec[j].bv_len = vec_len;
   imu->bvec[j].bv_offset = off;
   off = 0;
   size -= vec_len;
  }
  /* store original address for later verification */
  imu->ubuf = ubuf;
  imu->len = iov.iov_len;
  imu->nr_bvecs = nr_pages;

  ctx->nr_user_bufs++;
 }
 kvfree(pages);
 kvfree(vmas);
 return 0;
err:
 kvfree(pages);
 kvfree(vmas);
 io_sqe_buffer_unregister(ctx);
 return ret;
}
```

## 5.**Polled IO模式**

### 5.1 优化思想

将较多的CPU时间放到重要的事情上，全速完成关键路径。

状态从未完成变成已完成，就需要对完成状态进行探测，很多时候，可以使用中断模型，也就是等待后端数据处理完毕之后，内核会发起一个SIGIO或eventfd的EPOLLIN状态提醒核外有数据已经完成了，可以开始处理。但是，中断其实是比较耗时的，如果是高IOPS的场景，就会不停地中断，中断开销就得不偿失。

我们可以更激进一些，让内核采用Polled IO模式收割块设备层请求。这在一定的程度上加速了IO，这在追求低延时和高IOPS的应用场景非常有用。

### 5.2 优化实现

io_uring_enter通过正确设置IORING_ENTER_GETEVENTS，IORING_SETUP_IOPOLL等flag（如下代码设置IORING_SETUP_IOPOLL并且不设置IORING_SETUP_SQPOLL，即没有使用SQ线程）调用io_iopoll_check。

```
SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
  u32, min_complete, u32, flags, const sigset_t __user *, sig,
  size_t, sigsz)
{
 struct io_ring_ctx *ctx;
 long ret = -EBADF;
 int submitted = 0;
 struct fd f;

 io_run_task_work();

 if (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |
   IORING_ENTER_SQ_WAIT))
  return -EINVAL;

 f = fdget(fd);
 if (!f.file)
  return -EBADF;

 ret = -EOPNOTSUPP;
 if (f.file->f_op != &io_uring_fops)
  goto out_fput;

 ret = -ENXIO;
 ctx = f.file->private_data;
 if (!percpu_ref_tryget(&ctx->refs))
  goto out_fput;

 ret = -EBADFD;
 if (ctx->flags & IORING_SETUP_R_DISABLED)
  goto out;

 /*
  * For SQ polling, the thread will do all submissions and completions.
  * Just return the requested submit count, and wake the thread if
  * we were asked to.
  */
 ret = 0;
 if (ctx->flags & IORING_SETUP_SQPOLL) {
  if (!list_empty_careful(&ctx->cq_overflow_list))
   io_cqring_overflow_flush(ctx, false, NULL, NULL);
  if (flags & IORING_ENTER_SQ_WAKEUP)
   wake_up(&ctx->sq_data->wait);
  if (flags & IORING_ENTER_SQ_WAIT)
   io_sqpoll_wait_sq(ctx);
  submitted = to_submit;
 } else if (to_submit) {
  ret = io_uring_add_task_file(ctx, f.file);
  if (unlikely(ret))
   goto out;
  mutex_lock(&ctx->uring_lock);
  submitted = io_submit_sqes(ctx, to_submit);
  mutex_unlock(&ctx->uring_lock);

  if (submitted != to_submit)
   goto out;
 }
 if (flags & IORING_ENTER_GETEVENTS) {
  min_complete = min(min_complete, ctx->cq_entries);

  /*
   * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user
   * space applications don't need to do io completion events
   * polling again, they can rely on io_sq_thread to do polling
   * work, which can reduce cpu usage and uring_lock contention.
   */
  if (ctx->flags & IORING_SETUP_IOPOLL &&
      !(ctx->flags & IORING_SETUP_SQPOLL)) {
   ret = io_iopoll_check(ctx, min_complete);
  } else {
   ret = io_cqring_wait(ctx, min_complete, sig, sigsz);
  }
 }

out:
 percpu_ref_put(&ctx->refs);
out_fput:
 fdput(f);
 return submitted ? submitted : ret;
}
```

io_iopoll_check开始poll核外程序可以不停的轮询需要的完成事件数量min_complete，循环内主要调用io_iopoll_getevents。

```
static int io_iopoll_check(struct io_ring_ctx *ctx, long min)
{
 unsigned int nr_events = 0;
 int iters = 0, ret = 0;

 /*
  * We disallow the app entering submit/complete with polling, but we
  * still need to lock the ring to prevent racing with polled issue
  * that got punted to a workqueue.
  */
 mutex_lock(&ctx->uring_lock);
 do {
  /*
   * Don't enter poll loop if we already have events pending.
   * If we do, we can potentially be spinning for commands that
   * already triggered a CQE (eg in error).
   */
  if (io_cqring_events(ctx, false))
   break;

  /*
   * If a submit got punted to a workqueue, we can have the
   * application entering polling for a command before it gets
   * issued. That app will hold the uring_lock for the duration
   * of the poll right here, so we need to take a breather every
   * now and then to ensure that the issue has a chance to add
   * the poll to the issued list. Otherwise we can spin here
   * forever, while the workqueue is stuck trying to acquire the
   * very same mutex.
   */
  if (!(++iters & 7)) {
   mutex_unlock(&ctx->uring_lock);
   io_run_task_work();
   mutex_lock(&ctx->uring_lock);
  }

  ret = io_iopoll_getevents(ctx, &nr_events, min);
  if (ret <= 0)
   break;
  ret = 0;
 } while (min && !nr_events && !need_resched());

 mutex_unlock(&ctx->uring_lock);
 return ret;
}
```

io_iopoll_getevents调用io_do_iopoll。

```
/*
 * Poll for a minimum of 'min' events. Note that if min == 0 we consider that a
 * non-spinning poll check - we'll still enter the driver poll loop, but only
 * as a non-spinning completion check.
 */
static int io_iopoll_getevents(struct io_ring_ctx *ctx, unsigned int *nr_events,
    long min)
{
 while (!list_empty(&ctx->iopoll_list) && !need_resched()) {
  int ret;

  ret = io_do_iopoll(ctx, nr_events, min);
  if (ret < 0)
   return ret;
  if (*nr_events >= min)
   return 0;
 }

 return 1;
}
```

io_do_iopoll中的kiocb->ki_filp->f_op->iopoll，即blkdev_iopoll，不断地轮询探测确认提交给Block层的请求的完成状态，直到足够数量的IO完成。

```
static int io_do_iopoll(struct io_ring_ctx *ctx, unsigned int *nr_events,
   long min)
{
 struct io_kiocb *req, *tmp;
 LIST_HEAD(done);
 bool spin;
 int ret;

 /*
  * Only spin for completions if we don't have multiple devices hanging
  * off our complete list, and we're under the requested amount.
  */
 spin = !ctx->poll_multi_file && *nr_events < min;

 ret = 0;
 list_for_each_entry_safe(req, tmp, &ctx->iopoll_list, inflight_entry) {
  struct kiocb *kiocb = &req->rw.kiocb;

  /*
   * Move completed and retryable entries to our local lists.
   * If we find a request that requires polling, break out
   * and complete those lists first, if we have entries there.
   */
  if (READ_ONCE(req->iopoll_completed)) {
   list_move_tail(&req->inflight_entry, &done);
   continue;
  }
  if (!list_empty(&done))
   break;

  ret = kiocb->ki_filp->f_op->iopoll(kiocb, spin);
  if (ret < 0)
   break;

  /* iopoll may have completed current req */
  if (READ_ONCE(req->iopoll_completed))
   list_move_tail(&req->inflight_entry, &done);

  if (ret && spin)
   spin = false;
  ret = 0;
 }

 if (!list_empty(&done))
  io_iopoll_complete(ctx, nr_events, &done);

 return ret;
}
```

块设备层相关file_operations。

```
const struct file_operations def_blk_fops = {
 .open  = blkdev_open,
 .release = blkdev_close,
 .llseek  = block_llseek,
 .read_iter = blkdev_read_iter,
 .write_iter = blkdev_write_iter,
 .iopoll  = blkdev_iopoll,
 .mmap  = generic_file_mmap,
 .fsync  = blkdev_fsync,
 .unlocked_ioctl = block_ioctl,
#ifdef CONFIG_COMPAT
 .compat_ioctl = compat_blkdev_ioctl,
#endif
 .splice_read = generic_file_splice_read,
 .splice_write = iter_file_splice_write,
 .fallocate = blkdev_fallocate,
};
```

当使用POLL IO时，大多数CPU时间花费在blkdev_iopoll上。即全速完成关键路径。

```
static int blkdev_iopoll(struct kiocb *kiocb, bool wait)
{
 struct block_device *bdev = I_BDEV(kiocb->ki_filp->f_mapping->host);
 struct request_queue *q = bdev_get_queue(bdev);

 return blk_poll(q, READ_ONCE(kiocb->ki_cookie), wait);
}
```

### 5.3 Kernel Side Polling

IORING_SETUP_SQPOLL，当前应用更新SQ并填充一个新的SQE，内核线程sq_thread会自动完成提交，这样应用无需每次调用io_uring_enter系统调用来提交IO。应用可通过IORING_SETUP_SQ_AFF和sq_thread_cpu绑定特定的CPU。

实际机器上，不仅有高IOPS场景，还有些场景的IOPS有些时间段会非常低。为了节省无IO场景的CPU开销，一段时间空闲，该内核线程可以自动睡眠。核外在下发新的IO时，通过IORING_ENTER_SQ_WAKEUP唤醒该内核线程。

### 5.4 小结

如上可见，内核提供了足够多的选择，不同的方案有着不同角度的优化方向，这些优化方案可以自行组合。通过合理地使用，可以使io_uring 全速运转。

## 6.**io_uring用户态库liburing**

正如前文所说，简单并不一定意味着易用——io_uring的接口足够简单，但是相对于这种简单，操作上需要手动mmap来映射内存，稍显复杂。为了更方便地使用io_uring，原作者Jens Axboe还开发了一套liburing库。liburing库提供了一组辅助函数实现设置和内存映射，应用不必了解诸多io_uring的细节就可以简单地使用起来。例如，无需担心memory barrier，或者是ring buffer管理之类等。上文所提的一些高级特性，在liburing中也有封装。

### 6.1 核心数据结构

liburing中，核心的结构有io_uring、io_uring_sq、io_uring_cq

```
/*
 * Library interface to io_uring
 */
struct io_uring_sq {
 unsigned *khead;
 unsigned *ktail;
 unsigned *kring_mask;
 unsigned *kring_entries;
 unsigned *kflags;
 unsigned *kdropped;
 unsigned *array;
 struct io_uring_sqe *sqes;

 unsigned sqe_head;
 unsigned sqe_tail;

 size_t ring_sz;
};

struct io_uring_cq {
 unsigned *khead;
 unsigned *ktail;
 unsigned *kring_mask;
 unsigned *kring_entries;
 unsigned *koverflow;
 struct io_uring_cqe *cqes;

 size_t ring_sz;
};

struct io_uring {
 struct io_uring_sq sq;
 struct io_uring_cq cq;
 int ring_fd;
};
```

### 6.2 核心接口

相关接口在头文件linux/tools/io_uring/liburing.h，如果是通过标准方式安装的liburing，则在/usr/include/下。

```
/*
 * System calls
 */
extern int io_uring_setup(unsigned entries, struct io_uring_params *p);
extern int io_uring_enter(int fd, unsigned to_submit,
 unsigned min_complete, unsigned flags, sigset_t *sig);
extern int io_uring_register(int fd, unsigned int opcode, void *arg,
 unsigned int nr_args);

/*
 * Library interface
 */
extern int io_uring_queue_init(unsigned entries, struct io_uring *ring,
 unsigned flags);
extern int io_uring_queue_mmap(int fd, struct io_uring_params *p,
 struct io_uring *ring);
extern void io_uring_queue_exit(struct io_uring *ring);
extern int io_uring_peek_cqe(struct io_uring *ring,
 struct io_uring_cqe **cqe_ptr);
extern int io_uring_wait_cqe(struct io_uring *ring,
 struct io_uring_cqe **cqe_ptr);
extern int io_uring_submit(struct io_uring *ring);
extern struct io_uring_sqe *io_uring_get_sqe(struct io_uring *ring);
```

### 6.3 主要流程

- 使用io_uring_queue_init，完成io_uring相关结构的初始化。在这个函数的实现中，会调用多个mmap来初始化一些内存。

- 初始化完成之后，为了提交IO请求，需要获取里面queue的一个项，使用io_uring_get_sqe。

- - 获取到了空闲项之后，使用io_uring_prep_readv、io_uring_prep_writev初始化读、写请求。和前文所提preadv、pwritev的思想差不多，这里直接以不同的操作码委托io_uring_prep_rw，io_uring_prep_rw只是简单地初始化io_uring_sqe。

- 准备完成之后，使用io_uring_submit提交请求。

- 提交了IO请求时，可以通过非阻塞式函数io_uring_peek_cqe、阻塞式函数io_uring_wait_cqe获取请求完成的情况。默认情况下，完成的IO请求还会存在内部的队列中，需要通过io_uring_cqe_seen表标记完成操作。

- 使用完成之后要通过io_uring_queue_exit来完成资源清理的工作。

### 6.4 核心实现

io_uring_queue_init的实现，前文已略有提及。其中的操作主要就是io_uring_setup和io_uring_queue_mmap，io_uring_setup前文已解析过，这里主要看io_uring_queue_mmap。

```
/*
 * Returns -1 on error, or zero on success. On success, 'ring'
 * contains the necessary information to read/write to the rings.
 */
int io_uring_queue_init(unsigned entries, struct io_uring *ring, unsigned flags)
{
 struct io_uring_params p;
 int fd, ret;

 memset(&p, 0, sizeof(p));
 p.flags = flags;

 fd = io_uring_setup(entries, &p);
 if (fd < 0)
  return fd;

 ret = io_uring_queue_mmap(fd, &p, ring);
 if (ret)
  close(fd);

 return ret;
}
```

io_uring_queue_mmap初始化io_uring结构，然后主要调用io_uring_mmap。

```
/*
 * For users that want to specify sq_thread_cpu or sq_thread_idle, this
 * interface is a convenient helper for mmap()ing the rings.
 * Returns -1 on error, or zero on success.  On success, 'ring'
 * contains the necessary information to read/write to the rings.
 */
int io_uring_queue_mmap(int fd, struct io_uring_params *p, struct io_uring *ring)
{
 int ret;

 memset(ring, 0, sizeof(*ring));
 ret = io_uring_mmap(fd, p, &ring->sq, &ring->cq);
 if (!ret)
  ring->ring_fd = fd;
 return ret;
}
```

io_uring_mmap初始化io_uring_sq结构和io_uring_cq结构的内存，另外还会分配一个io_uring_sqe结构的数组。

```
static int io_uring_mmap(int fd, struct io_uring_params *p,
    struct io_uring_sq *sq, struct io_uring_cq *cq)
{
 size_t size;
 void *ptr;
 int ret;

 sq->ring_sz = p->sq_off.array + p->sq_entries * sizeof(unsigned);
 ptr = mmap(0, sq->ring_sz, PROT_READ | PROT_WRITE,
   MAP_SHARED | MAP_POPULATE, fd, IORING_OFF_SQ_RING);
 if (ptr == MAP_FAILED)
  return -errno;
 sq->khead = ptr + p->sq_off.head;
 sq->ktail = ptr + p->sq_off.tail;
 sq->kring_mask = ptr + p->sq_off.ring_mask;
 sq->kring_entries = ptr + p->sq_off.ring_entries;
 sq->kflags = ptr + p->sq_off.flags;
 sq->kdropped = ptr + p->sq_off.dropped;
 sq->array = ptr + p->sq_off.array;

 size = p->sq_entries * sizeof(struct io_uring_sqe);
 sq->sqes = mmap(0, size, PROT_READ | PROT_WRITE,
    MAP_SHARED | MAP_POPULATE, fd,
    IORING_OFF_SQES);
 if (sq->sqes == MAP_FAILED) {
  ret = -errno;
err:
  munmap(sq->khead, sq->ring_sz);
  return ret;
 }

 cq->ring_sz = p->cq_off.cqes + p->cq_entries * sizeof(struct io_uring_cqe);
 ptr = mmap(0, cq->ring_sz, PROT_READ | PROT_WRITE,
   MAP_SHARED | MAP_POPULATE, fd, IORING_OFF_CQ_RING);
 if (ptr == MAP_FAILED) {
  ret = -errno;
  munmap(sq->sqes, p->sq_entries * sizeof(struct io_uring_sqe));
  goto err;
 }
 cq->khead = ptr + p->cq_off.head;
 cq->ktail = ptr + p->cq_off.tail;
 cq->kring_mask = ptr + p->cq_off.ring_mask;
 cq->kring_entries = ptr + p->cq_off.ring_entries;
 cq->koverflow = ptr + p->cq_off.overflow;
 cq->cqes = ptr + p->cq_off.cqes;
 return 0;
}
```

### 6.5 具体例程

如下是一个基于liburing的helloworld示例。

```
#include <unistd.h>
#include <fcntl.h>
#include <string.h>
#include <stdio.h>
#include <liburing.h>

#define ENTRIES 4

int main(int argc, char *argv[])
{
    struct io_uring ring;
    struct io_uring_sqe *sqe;
    struct io_uring_cqe *cqe;
    struct iovec iov = {
        .iov_base = "Hello World",
        .iov_len = strlen("Hello World"),
    };
    int fd, ret;
    if (argc != 2) {
        printf("%s: <testfile>\n", argv[0]);
        return 1;
    }
    /* setup io_uring and do mmap */
    ret = io_uring_queue_init(ENTRIES, &ring, 0);
    if (ret < 0) {
        printf("io_uring_queue_init: %s\n", strerror(-ret));
        return 1;
    }
    fd = open("testfile", O_WRONLY | O_CREAT);
    if (fd < 0) {
        printf("open failed\n");
        ret = 1;
        goto exit;
    }
    /* get an sqe and fill in a WRITEV operation */
    sqe = io_uring_get_sqe(&ring);
    if (!sqe) {
        printf("io_uring_get_sqe failed\n");
        ret = 1;
        goto out;
    }
    io_uring_prep_writev(sqe, fd, &iov, 1, 0);
    /* tell the kernel we have an sqe ready for consumption */
    ret = io_uring_submit(&ring);
    if (ret < 0) {
        printf("io_uring_submit: %s\n", strerror(-ret));
        goto out;
    }
    /* wait for the sqe to complete */
    ret = io_uring_wait_cqe(&ring, &cqe);
    if (ret < 0) {
        printf("io_uring_wait_cqe: %s\n", strerror(-ret));
        goto out;
    }
    /* read and process cqe event */
    io_uring_cqe_seen(&ring, cqe);
out:
    close(fd);
exit:
    /* tear down */
    io_uring_queue_exit(&ring);
    return ret;
}
```

更多的示例可参考：
http://git.kernel.dk/cgit/liburing/tree/examples
https://git.kernel.dk/cgit/liburing/tree/test

## 7.**性能**

如上，推演过了设计与实现，回归到存储的需求上来，io_uring子系统是否能满足我们对高性能的极致需求呢？这一切还是需要profile。

### 7.1 测试方法

io_uring原作者Jens Axboe在fio中提供了ioengine=io_uring的支持，可以使用fio进行测试，使用ioengine选项指定异步IO引擎。

可以基于不同的IO栈：

- libaio
- kernel+io_uring
- kernel+io_uring polling mode

可以基于一些硬件之上：

- NVMe SSD
- ...

测试过程中主要4k数据的顺序读、顺序写、随机读、随机写，对比几种IO引擎的性能及QoS等指标

io_uring polling mode测试实例：

```
fio -name=testname -filename=/mnt/vdd/testfilename -iodepth=64 -thread -rw=randread -ioengine=io_uring -sqthread_poll=1 -direct=1 -bs=4k -size=10G -numjobs=1 -runtime=600 -group_reporting
```

### 7.2 测试结果

网上可以找到一些关于io uring的性能测试，这里列出部分供参考：

- [*Im**proved Flash Performance Using the New Linux Kernel I/O Interface*](https://www.flashmemorysummit.com/Proceedings2019/08-07-Wednesday/20190807_SOFT-202-1_Verma.pdf)
- [*io_uring echo server benchs*](https://github.com/frevib/io_uring-echo-server/blob/io-uring-feat-fast-poll/benchs/benchs.md)
- [*[PATCHSET v5\] io_uring IO interface*](https://lore.kernel.org/linux-block/20190116175003.17880-1-axboe@kernel.dk/)
- ...

主要有以下几个测试结果

- io_uring在非polling模式下，相比libaio，性能提升不是非常显著。
- io_uring在polling模式下，性能提升显著，与spdk接近，在队列深度较高时性能更好。
- 在meltdown和spectre漏洞没有修复的场景下，io_uring的提升并不太高。虽然减少了大量的用户态到内核态的上下文切换，在meldown和spectre漏洞没有修复的场景下，用户态到内核态的切换开销本比较小，所以提升不太高。
- 在某些场景下使用io_uring + Kernel NVMe的驱动，效果甚至要比使用SPDK 用户态NVMe 驱动更好

从测试中，我们可以得出结论，在存储中使用io_uring，相比使用libaio，应用的性能会有显著的提升。

在同样的硬件平台上，仅仅更换IO引擎，就可以带来较大的提升，是很难得的，对于存储这种延时敏感的应用而言十分宝贵。

### 7.3 io_uring的优势

综合前文和测试，io_uring有如此出众的性能，主要来源于以下几个方面：

- 用户态和内核态共享提交队列SQ和完成队列CQ实现零拷贝。
- IO提交和收割可以offload给Kernel，不需要经过系统调用。
- 支持块设备层的Polling模式。
- 可以提前注册用户态内存地址，从而减少地址映射的开销。
- 相比libaio，支持buffered IO

## 8.**发展方向**

事物的发展是一个哲学话题。前文阐述了io_uring作为一个新事物，发展的根本动力、内因和外因，谨此简述一些可预见的未来的发展方向。

### 8.1 普及

应用层多使用。目前主要应用在存储的场景中，这是一个不仅需要高性能，也需要稳定的场景，而一般来说，新事物并不具备“稳定”的属性。但是io_uring同样也是稳定的，因为虽然io_uring使用到了若干新概念，但是这些新的东西已经有了实践的检验，如eventfd通知机制，SIGIO信号机制，与AIO基本相似。它是一个质变的新事物。

就我们腾讯而言，内核使用tlinux，tlinux3基于4.14.99主线；tlinux4基于5.4.23主线。

所以，tlinux3可以用native aio，tlinux4之后已经可以用native io_uring。

相信通过大家的努力，正如前文所说的PostgreSQL使用彼时新接口pread，Nginx使用彼时的新接口AIO一样，通过使用新街口，我们的工程也能获得巨大收益。

### 8.2 优化方向

#### 8.2.1 **降低本身的工作负载**

持续降低系统调用开销、拷贝开销、框架本身的负载。

#### 8.2.2 **重构**

> "Politics are for the moment. An equation is for eternity.
> 　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　——Albert Einstein

追求真理的人不可避免地追求永恒。“政治只是一时，方程却是永恒。”——爱因斯坦如是说，时值以色列的第一任总统魏兹曼于1952年逝世，继任首相古理安建议邀请爱因斯坦担任第二任总统。

我们说折衷权衡、精益求精，字里行间都是永恒，然而软件应该持续重构，这实际上并不只是io_uring需要做的，有机会我会写一篇关于重构的文章。

## 9.**总结**

首先，本文简述了Linux过往的的IO发展历程，同步IO接口、原生异步IO接口AIO的缺陷，为何原有方式存在缺陷。其次，再从设计的角度出发，介绍了最新的IO引擎io_uring的相关内容。最后，深入最新版内核linux-5.10中解析了io_uring的大体实现（关键数据结构、流程、特性实现等）。

## 10.**关于**

难免纰漏，欢迎交流，可以通过以下网址找到本文。

- 知乎：https://www.zhihu.com/people/linkerist-61
- Github: https://github.com/Linkerist/blog/issues

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16Gl18nIYG3o715BRWUw0uqkv90xxBM3F7xYrBjjvmXGFa4AnsV0EHIMg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

内容会更新，可以关注我的公众号，欢迎交流。

## 11.**参考**

[*PATCH 12/19\]io_uring: add support for pre-mapped user IO buffers*](https://lore.kernel.org/linux-block/20190211190049.7888-14-axboe@kernel.dk/)

[*Add pread/pwrite support bits to match the lseek bit*](https://lwn.net/Articles/97178/)

[*Toward non-blocking asynchronous I/O*](https://lwn.net/Articles/724198/)

[*A new kernel polling interface*](https://lwn.net/Articles/743714/)

[*The rapid growth of io_uring*](https://lwn.net/Articles/810414/)

[*Ringing in a new asynchronous I/O API*](https://lwn.net/Articles/776703/)

[*Efficient IO with io_uring*](http://kernel.dk/io_uring.pdf)

[*The current state of kernel page-table isolation*](https://lwn.net/Articles/741878)

[The Linux man-pages project](https://www.kernel.org/doc/man-pages/)

https://zhuanlan.zhihu.com/p/62682475

[*why we need io_uring? by byteisland*](https://www.byteisland.com/io_uring（1）-我们为什么会需要-io_uring/)

*Computer Systems: A Programmer's Perspective, Third Edition*

*Advanced Programming in the UNIX Environment, Third Edition*

*The Linux Programming Interface: A Linux and UNIX System Programming Handbook*

[*Introduction to io_uring*](http://kernel.taobao.org/2020/08/Introduction_to_IO_uring/)

*Understanding Nginx Modules Development and Architecture Resolving(Second Edition)*

原文作者：子晨

原文链接：https://mp.weixin.qq.com/s/QshDG-nbmBcF1OBZbBFwjg

# 【NO.229】云时代，我们需要怎样的数据库？

本文作者腾讯云TDSQL负责人潘安群。潘安群主要负责腾讯云分布式数据库研发，拥有超过13年分布式数据库研发经验，研发成果多次入选国际顶会VLDB、SIGMOD等。他带领团队打造的安全可控分布式数据库TDSQL，是业内首个应用于互联网银行核心交易系统、首个进入银行传统核心系统、首个助力传统大型银行实现银行业首例“大型机”下移分布式平台的国产企业级分布式数据库。

## **0.引言**

数据库技术发展已达半个世纪之久，数据库图灵奖得主Michael Stonebraker曾在Readings in Database Systems中将数据库模型技术分为9个不同的时代与类型，而云时代开始以后，我们可以从全新的视角审视数据库等基础技术的过去和未来。

基于云计算，包括数据库在内的IT基础技术发生从技术形态到线上线下整个市场结合的大幅变化，数据库技术呈现从传统集中式到云时代分布式迁移替换的趋势，这也给国产数据库赋予机遇与挑战。

在2020年11月，Gartner发布了2020年度的数据库厂商评估报告，国数据库厂商占据三席，标志着国内数据库进入全新发展阶段。

同时，Gartner预测，到2022年，世界上3/4的数据库都会跑在云上，而我们认为，云数据库的发展，目前正在经历从第一阶段“数据库上云，即从数据库到云数据库”，到第二阶段“从云数据库到云原生数据库”的变革。

归根结底，云数据库做了什么得到了业内的认可？未来数据库发展趋势是什么？我们可以如何在新机遇下的云融合时代把握技术创新的脉搏？在当前国产数据库也成为一个热门话题之际，我们谈一谈我们的理解和思考，与大家共勉。

## **1.云时代的IT基础技术形态演变**

随着云计算的发展，整个IT基础技术翻天覆地的变化体现在几个方面：

IT设施部署，从过去的零散化走向今天的集中化、规模化。过去，每一个企业自建各自的数据中心等IT基础设施，包括服务器、网络到操作系统、数据库等，形成企业市场上零散化的IT设施模式。而今天基于云计算服务，企业IT设施呈现集中化、规模化效应，对效率、性能、成本的要求提升。

IT服务交付，从过去的软件交付模式走向服务交付模式。过去购买商业化软件，或者是使用开源软件产品，基本是通过商业化或开源的方式进行分发，而现在完全变成一个个服务的形式进行交付。这带来的变化是，用户不需要再盘算该购买几台服务器，而是在具有数据库使用需求时，直接云上使用即可。

开发方式，将呈现从过去业务进行非常底层的开发以及调用底层API等操作的模式，转向SaaS化、Severless模式的服务。在云上，开发者可以使用各种各样的SaaS服务。无论从效率、基础技术能力等方面来说，这都是一个巨大的变化。

而数据形式及应用场景领域而言，事实上过去的数据形式或应用场景相对单一，以传统数据库为例，场景主要集中在了比如金融、运营商、政务等传统行业领域。随着互联网、移动互联网、产业互联网的发展，各个行业也正逐步加速其电子化、信息化发展趋势，应用服务形式呈多样化发展，使得当前行业的数据形式及应用场景也越来越多样化，并对底层数据库能力提出更多的要求和挑战。过去，行业场景中更多以结构化的数据为主，关系型数据库可以支撑极大部分场景需求，现在我们可以看到涌现出了许多如NoSQL、Graph图数据库等各种类型的数据库，NoSQL下属同时还可以细分KV型、文档型等多种类别，而且整体数据库类型还有持续增加的趋势。这是非常合理的现象。也就是说，对于未来数据库来说，其自身发展也会呈现多样化的、而且是融合、创新的趋势。我们知道，按照传统经验来说，如果一个技术产品是单一的形态，那么追求的是尽量做到通用化，然而，在当前多样化需求的趋势下，技术应用层面需要进行各种权衡和取舍。

因此可以说，这是云时代的发展变化，对数据库带来的新的挑战和要求。在当前云数据库成为大势所趋的同时，我们认为，国产云数据库要发展好，需要持续在基础能力、成本效率、产品化、未来技术融合等各个层面进行探索突破。

## **2.云数据库技术演进的挑战**

结合云计算的特点，国产云数据库发展面临着需要持续探索可用性与一致性、高并发性能、弹性可扩展等基础能力突破，同时面向云时代的多样化趋势打造新一代分布式数据库产品的挑战和要求。

### 2.1 可用性与一致性。

作为数据库，高可用性、数据一致性是最基础的挑战。高可用性，要求达到99.999%以上；数据强一致性，意味着数据不出错，数据库高度可靠。云计算时代，技术设施的升级换代对技术实现方式带来变革。过去，比如金融行业，系统基于稳定性较高的传统集中式大型机或小型机来保障系统的可用性与一致性。然而，传统集中式结构存在明显的技术边界，包括性能和吞吐量的边界，今天它们已然面临较大的吞吐和性能瓶颈，无法满足云时代的产业需求。自然地，当前产业趋势是向分布式架构转型升级，转向基于x86等的分布式、开放式平台。传统架构系统依赖于大型机或小型机在硬件层面进行的大量的冗余设计，在硬件层面实现可用性与一致性保障。而相对来说，基于x86机器部署的新一代分布式架构系统，则在如何实现性能、无限水平扩展的基础上保证数据一致以及系统高可用提出新的挑战要求。

### 2.2 性能成本。

云计算时代，如果实现了规模化以后，还不能实现成本降低的话，是不可接受的。云计算要帮助提升整个社会资源利用率，性能成本需要控制到最低。

对于腾讯云的服务来说，我们需要考虑的是如何能够保证客户以最便宜的价格买到最高级的服务——比如花最少的钱买到最大的磁盘空间、以及最好的TPS等产品表现。而在这个过程中，最核心的就是资源利用率。举个例子，云计算服务商如果把资源利用率提升20%，对客户、服务商本身而言将能极大地降低一部分成本。

### 2.3 云原生意味着一定是弹性伸缩的。

弹性伸缩，也就是可以根据用户的实际需求进行资源分配与使用，而不再是过去通过预采购或预分配的方式。过去，客户大部分都是先预估，然后采购，所以资源利用率一直被诟病；现在则不需要用户再预估自己未来可能会用到多少资源，而是可以根据实时的使用需求实现弹性伸缩。也因为这样，通过提高资源利用率，云数据库才可以实现成本上的优势。但是，极致弹性伸缩对数据库在更高程度的SQL支持、分布式事务能力方面，提出了更高的要求。

### 2.4 云数据库产品化服务化程度。

国内数据库发展历程也经历多个阶段，但正是云计算、互联网的时代兴起，国内诸多腾讯这类云厂商得以抓住机会，基于自身业务场景特点和需求，发展新一代数据库等基础软件技术。在过去很多年的时间当中，腾讯非常重看重的一点就是，如何打磨提升整个数据库的产品化程度，提升用户体验，包括技术产品化、服务完善等方面。互联网厂商基于内部业务场景发展自己的技术体系，这是优势的一面，而在to B开放的过程中，同时也面临产品标准化、通用性、使用体验等挑战。面向行业客户提供技术产品，其要求比支撑内部使用高得多。对于传统企业客户而言，腾讯云希望提供给到客户的是一个完整的产品，而不是一个半成品。因此，产品化程度，是腾讯一直持续强调的能力。

### 2.5 海量场景验证。

最后关键的一点是，对于云数据库而言，包括稳定性、特性需求等基础能力的发展，核心条件是需要有足够的应用场景进行打磨。数据库系统的研发、完善是一个非常复杂的过程，如何让数据库得到实践、得到应用？走到今天，我们认为，持续的、海量的场景打磨，是产品发展的关键条件。得益于腾讯自身以及云上各行各业的应用，超过百万开发者的使用，腾讯云数据库能够有足够的空间打磨产品。这是我们的挑战，也是推动我们发展的土壤。

这些挑战是云数据库发展过程中的必经之路，也是我们在云计算时代创造出新一代分布式数据库产品的机遇。

## **3.云数据库未来关键趋势**

基于这些挑战以及云计算时代赋予的机会，我们认为未来云数据库发展将包括几大趋势要求：

**弹性伸缩：解决成本核心问题——资源利用率**

前面提到，成本与性能是核心的要素。这里引申出来一个云计算时代的差异，那就是我们需要实现对CPU、内存和磁盘等基础设施资源的灵活调度。

云数据库时代我们将通过对极致的弹性伸缩架构探索，来综合解决性能、效率和成本问题。针对不同的场景侧重，云原生分布式数据库可分为两种架构：一种是Shared Nothing，一种是Shared Storage，两者都可以通过实现计算与存储分离架构来整体获得更优秀的弹性伸缩能力，克服传统架构下的存储量受限、扩展难、主从延迟高等缺点，同时也能够帮助我们将成本控制得更低，充分释放领先技术的成本效益。

而计算与存储全Serverless架构的数据库服务也是未来可重点关注的方向，它在可自动无感扩缩容的基础上，同时实现可按实际使用计费，不用不付费，提升云数据库效用。

**多模多引擎趋势下的数据库底层与服务超融合**

新基建、产业互联网快速发展，各行各业数字化进程加速，数据形式越来越多样化，越来越海量，如何能最高效地解决数据库在性能、成本、服务等方面问题，超融合是必然趋势。

当下我们处于各行各业都在推进电子化、信息化建设和数字化转型的趋势浪潮之下，行业不断涌现出大量的新兴场景。数据库作为支撑各类IT系统架构的基础软件技术，其整个技术形态也随之出现各类新的应用实现，包括大量的NoSQL实践，以及存储领域有传统的B+ Tree、现在的LSM Tree，和行存、列存等架构形态产品；而根据workload类型区分的话，则涌现出包括OLTP、OLAP，或者两者混合形成的HTAP型数据库等。

而多种多样的引擎产品，在大多数情况下不会独立存在来服务于一个企业或系统。One size fits none。从技术角度看，极致的性能成本与通用性有着天然的矛盾，因此，在多样化场景下，一定会是多引擎共存，充分发挥各种引擎的特点与优势，才能实现极致与通用的兼得。

但是不是作为云数据库服务厂商，我们把这些各类引擎产品都暴露给客户、开发者自行选择呢？从产品服务体验的角度看，必然不是。多模态技术引擎的现状必然对开发者选型带来选型、开发应用上的困难——即如何能够在保证适应不同的场景下，同时获得足够高的性能表现，这也是当前数据库发展面临的一个困境。为了解决这个问题，未来我们希望是不需要用户来进行这些复杂的选择，而是系统基于AI智能调度、serverless等解决方案，来彻底实现多引擎的统一标准化服务。从底层的角度看，未来开发者无需感知具体的产品选型，而比如在做数据分析的时候，系统能够自动帮助调度性能最好、事务交易一致性得到保障的方案。

在此基础上，未来云数据库服务的趋势还是交付方式的融合，包括软硬件一体化、私有云与公有云平台融合等多种产品和服务交付方案，能够让客户在敏感业务和运营成本之间实现更加精细化管理。

### 3.1 **智能化：AI+DB**

智能化技术等底层技术生态融合变革，实现数据库自治与智能管理也是未来数据库趋势之一。过去，对于一个企业，也许几个DBA来管理几十套实例就足够了，但比如对腾讯来说，数十万的数据库实例，难以通过配置人力来维持运营，因此倒逼我们必须要通过工具或平台来解决运营效率的问题。此外，当前分布式微服务改造的趋势下，未来企业IT运营也将具有越来越强烈的自治需求。智能化技术与数据库底层的融合，能实现对数据库进行全生命周期智能管理。

### **3.2 加速释放新硬件红利**

过去一个新硬件的推广周期很长，很多传统企业在采购新硬件方面相对非常保守。而对于云厂商来说，相对有条件逐步率先探索新硬件的应用，比如先在非关键性应用，同时也具备海量的场景验证，来实现稳步规模化的推广。这个角度来说，基于云计算服务，云原生数据库相对更加容易探索、释放到新硬件带来的红利。

当前我们也处于新硬件创新层出不穷的时代，包括SSD、NVM、RDMA+SPDK、千核服务器、异构处理器等，基于云数据库服务，广大的客户、普通开发者也能够更快速地享受到新硬件带来的加持。

因此，融合、自治、效用是未来企业级分布式数据库基本特点。腾讯云数据库将从实践层面对以上趋势进行落地推进，来满足各行各业客户未来对数据库的多样性需求。

原文作者：潘安群

原文链接：https://mp.weixin.qq.com/s/2uG1WjCjqOJWJceCTj5zSA

# 【NO.230】STGW 下一代互联网标准传输协议QUIC大规模运营之路

## **0.前言**

QUIC 作为互联网下一代标准传输协议，能够明显提升业务访问速度，提升弱网请求成功率以及改善网络变化场景下的平滑体验。

STGW 作为公司级的 7 层接入网关以及腾讯云 CLB（负载均衡器）的底层支撑框架，每天都为公司内部业务和腾讯云外部客户提供数万亿次的请求服务，对请求处理的性能、传输效率、运营的可靠性都有非常严苛的要求。

本文主要介绍 STGW 大规模运营 QUIC 过程中的一些经验和开发工作。

## 1.**QUIC 简介**

### 1.1 QUIC 的诞生和发展

> 在 QUIC 诞生之前，HTTP 协议经历了几次重要的升级：
>
> HTTP1.0 -> HTTP1.1：增加了长连接支持，大大提升了长连接场景下的性能。
>
> HTTP -> HTTPS：增加安全特性，对请求的性能综合来看会有一定的影响。
>
> HTTP1.1 -> HTTP2：主要特性是多路复用与头部压缩，提高了单连接的并发能力。

这些重要变化都是围绕安全与性能展开，对 HTTP 协议的应用和发展起到了很重要的作用。但是，它没有绕开内核 TCP 的限制，导致其协议的发展终究存在瓶颈。

GOOGLE 在引领业界从 HTTP1.1 迈向 HTTP2（GOOGLE SPDY 协议的标准版）后，再一次走在了前头，在 2012 年提出了实验性的 QUIC 协议，首次使用 UDP 重构了 TLS 和 TCP 协议。QUIC 协议不仅仅只应用于 HTTP，QUIC 在设计时除了考虑 HTTP 外，更是设计作为一个通用的传输层协议。在安全性上，GOOGLE 设计了 QUIC 加密协议作为握手协议以解决 QUIC 协议上的安全问题。一般来说，QUIC 握手协议+QUIC 传输层+HTTP2 就是我们常说的 GQUIC（这里指 web 部分）。GQUIC 协议的版本不断演化，从 Q46 开始，GQUIC 协议也不断向 IETF QUIC 和 HTTP3 靠拢。

2015 年，QUIC 的网络草案被正式提交至互联网工程任务组，这意味着新的 QUIC 协议标准将要诞生。在标准 QUIC 协议起草过程中，QUIC 协议上的标准 HTTP 协议作为 HTTP3 也同时被起草。而作为 QUIC 的标准握手协议，IETF 将 TLS1.3 应用其中。TLS1.3+QUIC+HTTP3，这就是我们常说的 IETF QUIC（这里指 web 部分）。截止目前，QUIC 标准的草案已经更新到 34 版，仍没形成正式的 RFC。但是，QUIC 已进入 IETF 最后征求意见，预计标准 QUIC/HTTP3 协议会很快问世。

### 1.2 QUIC 的关键特性

关于 QUIC 的原理，相关介绍的文章很多，这里再列举一下 QUIC 的重要特性。这些特性是 QUIC 得以被广泛应用的关键。不同业务也可以根据业务特点利用 QUIC 的特性去做一些优化。同时，这些特性也是我们去提供 QUIC 服务的切入点。

1. 低连接延时：QUIC 由于基于 UDP，无需 TCP 连接，在最好情况下，短连接下 QUIC 可以做到 0RTT 开启数据传输。而基于 TCP 的 HTTPS，即使在最好的 TLS1.3 的 early data 下仍然需要 1RTT 开启数据传输。而对于目前线上常见的 TLS1.2 完全握手的情况，则需要 3RTT 开启数据传输。对于 RTT 敏感的业务，QUIC 可以有效的降低连接建立延迟。
2. 可自定义的拥塞控制：QUIC 的传输控制不再依赖内核的拥塞控制算法，而是实现在应用层上，这意味着我们根据不同的业务场景，实现和配置不同的拥塞控制算法以及参数。GOOGLE 提出的 BBR 拥塞控制算法与 CUBIC 是思路完全不一样的算法，在弱网和一定丢包场景，BBR 比 CUBIC 更不敏感，性能也更好。在 QUIC 下我们可以根据业务随意指定拥塞控制算法和参数，甚至同一个业务的不同连接也可以使用不同的拥塞控制算法。
3. 无队头阻塞：虽然 HTTP2 实现了多路复用，但是因为其基于面向字节流的 TCP，因此一旦丢包，将会影响多路复用下的所有请求流。QUIC 基于 UDP，在设计上就解决了队头阻塞问题。同时，IETF 设计了 QPACK 编码替换 HPACK 编码，在一定程度上也减轻了 HTTP 请求头的队头阻塞问题。无队头阻塞使得 QUIC 相比 TCP 在弱网和一定丢包环境上有更强大的性能。
4. 连接迁移：当用户的地址发生变化时，如 WIFI 切换到 4G 场景，基于 TCP 的 HTTP 协议无法保持连接的存活。QUIC 基于连接 ID 唯一识别连接。当源地址发生改变时，QUIC 仍然可以保证连接存活和数据正常收发。

## 2.**QUIC 协议栈的选择**

对于协议的实现，STGW 与 CDN 业务团队在 LEGO（STGW 与 CDN 自研的高性能转发框架）上实现过完整的 HTTP2 协议。同时，STGW 也在业界最早实现了 TLS 异步代理计算的方案。对于 HTTP1.1/2 和 TLS 协议有不少工程和优化经验。QUIC 协议栈的自研目前也在按计划展开，但尚不成熟。

本文就基于开源方案，给大家简单介绍一下 QUIC 协议栈的深度定制和优化工作。

关于 QUIC 协议栈的实现，当前功能广泛，协议支持齐全的实现并不多。NGINX 官方目前实现了一个实验版本，但是该实现很多问题没解决，同时，其仅支持 IETF 最新的 DRAFT，甚至连一个完整的拥塞控制算法也没有实现。CLOUDFLARE 的 QUIC 基于 RUST 实现，性能从公开数据来看并不强。

其它的很多实现诸如 MSQUIC, NGHTTP3 等都只支持 IETF QUIC，并不支持 GQUIC。

GOOGLE 是 QUIC 协议的开创者，其基于 CHROME 的 QUIC 协议栈实现最早，功能最齐，实现上也最为标准。

不论是哪种 QUIC 协议栈，其接入都需要我们对 QUIC 的基本特性和概念有较深的理解。比如常见的连接，流，QUIC 连接 ID，QUIC 定时器，统一的调度器等等。这些概念与 QUIC 协议的内容息息相关。

下面以 CHROMIUM QUIC 为例的将 QUIC 协议栈与高性能转发框架 NGINX 与 LEGO 融合的架构图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxGiaMqAupt74SpwpWHicrD3ZkGhtvX7gGn0TTsRkNykYBAzfibJBC4elmg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)CHROMIUM QUIC接入高性能框架NGINX/LEGO

## **3.STGW 的工作**

STGW 作为公司级的 7 层接入网关以及腾讯云 CLB（负载均衡器）的底层支撑框架，为公司内部业务和腾讯云外部客户提供数万亿次请求的服务，对请求处理的性能、传输效率、运营质量的高可靠性都有非常严苟的要求。

为此，我们对 QUIC 协议栈做了大量优化和深度定制，以满足大规模运营和业务需求。主要的工作有：

1. 单机与传输性能优化

2. 1. QUIC 协议单机性能/成本优化：QUIC 将协议栈搬到应用层来，从目前一些公开的实现看，性能比 TCP 差不少，优化 QUIC 协议的性能是大规模推广 QUIC 协议很重要的一环。
   2. 与高性能转发框架融合：当前开源的 QUIC 协议栈的实现仅仅提供单核支持，并仅提供简单的 demo。要想大规模应用，需要将 QUIC 协议栈接入到我们使用的高性能网络转发框架中来，如 NGINX,LEGO 等。
   3. 传输性能，拥塞控制定制化：可以允许不同业务根据业务特性选择不同的拥塞控制算法。
   4. 做到安全高比例的 0RTT，以降低业务的连接延迟。

3. 功能特性的定制和增强

4. 1. 如何让 QUIC 连接迁移从理论走向应用：QUIC 的连接 ID 是 QUIC 协议的特性，但是实际应用中，要做到连接迁移并不容易，需要充分考虑 QUIC 包的各个路径。即使在同一台机器上，也需要正确转发到对应的核上去。
   2. QUIC 私有协议的支持：QUIC 不仅仅用于 HTTP，作为通用的传输层协议，除了支持 GQUIC,IETF HTTP3 外，QUIC 的私有协议也需要我们提供给用户。
   3. QUIC 定制化 SDK：除了高性能 QUIC 服务端外，要想使用 QUIC 需要客户端 SDK 的支持。对此我们也开发了 QUIC 的 SDK，并针对不同的场景做了定制化。
   4. 满足业务各种定制化需求：如有些业务需要 QUIC 明文传输，一些业务需要 QUIC 回源等。

5. 高可用运营

6. 1. 日常变更与平滑升级：在配置频繁变更和模块升级时，我们需要做到对 QUIC 连接无损。
   2. 抓包分析工具：分析定位为更方便。
   3. 统计监控：QUIC 的关键统计指标，需做到可视化运营。

我们围绕着这些问题展开了 QUIC 的相关工作，力求将 QUIC 特性，QUIC 运营，QUIC 性能，QUIC 定制化需求等做到最好。

## 4.**QUIC 处理性能优化**

QUIC 协议基于 UDP 将 TCP 的特性从内核移到了应用层，从当前各种 QUIC 实现来看，性能相比 TCP 差不少。TCP 长期以来使用非常广泛，这也使得其从协议栈到网卡已经经过了非常多的优化，与之相比，UDP 的优化则少了很多。除了内核和硬件外，QUIC 协议的性能也与实现有关，不同的实现版本可能也会有很大的差别。

我们对 QUIC 的性能利用火焰图等各种工具进行了详细分析，找出了一些影响 QUIC 性能的关键点：

1. 密码相关算法的开销：对于小包来说，RSA 的计算占比很高，对于大包来说，对称加解密也会占到 15%左右的比例。

2. UDP 收发包的开销：特别是对于大文件下载来说，sendmsg 占比很高，可以达到 35%-40%以上。

3. 协议栈开销：主要受协议栈实现，如 ACK 的处理，MTU 探测和发包大小，内存管理和拷贝等。

   我们基于影响 QUIC 性能的关键点进行了优化。

### 4.1 QUIC 的 RSA 硬件 OFFLOAD

在小文件请求场景中，RSA 的计算在 QUIC 请求同 HTTPS 一样，仍然是最消耗 CPU 的开销。RSA 在 HTTPS 请求可以利用硬件 offload，在 QUIC 握手过程中，RSA 同样可以利用硬件进行 offload。

使用硬件做 RSA 卸载一个很重要原因是，CPU 计算 RSA 性能较差，而专门做加解密的加速卡性能则很强。一般来说，单块 RSA 加解密卡的成本差不多是一台服务器的 5%-7%，但是其对 RSA 签名的操作性能是服务器的 2-3 倍左右，一台机器插入 2 块卡就可以带来 5 倍的 RSA 性能提升。

将 QUIC 的 RSA 计算进行硬件卸载在不同的 QUIC 协议栈上方法并不相同，下面介绍一种 RSA HOOK + ASYNC JOB 通用的 RSA 卸载方案。其特点是代码侵入性小，不需要额外修改太多 quic 协议栈或者 openssl 的代码。

Openssl1.1.0 之后，开始支持 Async Job。Async job 机制利用类协程方式切换上下文方式实现异步任务，并且提供了一个通知机制通知异步任务的返回结果。

Async Job 里的 2 个重要函数是：

> async_fibre_makecontext
>
> async_fibre_swapcontext

它们利用 setjmp，longjmp，setcontext，makecontext 这些调用，保存和切换当前上下文，从而达到状态保留和代码跳转的能力。

使用 RSA callback 将握手过程中的 RSA 进行拦截，并在 RSA 的 HOOK 函数中本地或者远程向加速卡请求 RSA 操作。同时使用 Async job 方式将同步方式异步化，从而实现 RSA 操作的异步卸载。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwx5OJYmTec2NQibibjU6tl3MuHRPch4ibVibMylWanO38Uh0IAexFvoqRjcA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

在 QUIC TLS1.3 上 RSA HOOK + Async Job 进行 RSA offload。

QUIC 在进行了 RSA 的硬件 OFFLOAD 后，对于小包短连接，性能得到了很大的提升。

以 CHROMIUM QUIC 为例，在 1RTT 场景，QUIC 在使用了 RSA OFFLOAD 后，性能为原来的 256%；0RTT 场景，QUIC 在使用了 RSA OFFLOAD 后，性能为原来的 205%。在 QUIC 协议栈开销更小的实现上，这个性能提升会更加明显。

### 4.2 QUIC 发包的 GSO 优化

在大文件下载中，QUIC 发包的逻辑占比很大，通常在 35%-40%以上。因此优化发包逻辑可以提升大文件传输的性能。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxKdF7e0EY2DLr1hYKFCbW3JVCFTtPANkJtMjvt50CW8sqcbPHdiczDuA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)QUIC大文件请求火焰图

GSO(Generic Segmentation Offload)在内核 4.8 之后开始支持 UDP，其原理是让数据跨过 IP 层，链路层，在数据离开协议栈，进入网卡驱动前进行分段，不论是 TCP 还是 UDP，都是分段(每个包都附加 TCP/UDP 头部)，这样，当一个段丢失，不需要发送整个 TCP/UDP 报文。同时，路径上的 CPU 消耗也会减少。

若网卡硬件本身支持 UDP 分段，则称为 GSO OFFLOAD，其将分段工作放在网卡硬件上做，可以进一步节省 CPU。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwxz3pue6TaVib8YPeQpfStexliaTsfERae7mpNCUlhOKCW721UR5dhxVtw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)GSO原理示意图

QUIC 协议在实现上，一般为了不进行 MTU 分片，通常会在发包前就将发送数据进行分段，从而无需再进行 MTU 分片。对于大包来说，QUIC 会将每个包控制在 1400 字节左右，然后通过 sendmsg 发送出去。大文件发送场景，这种性能是很低的。如果在 sendmsg 时发送大包不做分段，然后利用内核 GSO 延迟分段，会减少路径的 CPU 消耗。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwx5d0kRjv9nTHaXDkKxWtqUciaFictdCskdz16TNIfZicYQQuXOQkF5uEUg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)使用GSO发不同大小包时的吞吐

从表中可以看出，使用 GSO 连续 20 个包进行 sendmsg 发送，相比于 1400 个包单独发送，性能提升了 2-3 倍。

在实际 QUIC 场景中，发包并不是 QUIC 的全部逻辑。同时，并不是每次发包正好都可以凑齐 20 个连续包。我们对大文件下使用 GSO 进行 QUIC 压力测试，相同 CPU 使用情况下，吞吐提高了大约在 15%-20%。

### 4.3 **QUIC 协议栈的优化**

QUIC 协议栈的性能与 QUIC 协议栈实现有关。对于一些常见的协议栈实现，其优化空间主要有：

1. 一些实现如 CHROMIUM 在 0RTT 和 1RTT 请求中分别多了一次 RSA 计算，这个多余的 RSA 计算是可以去掉的。优化后，0RTT 和 1RTT 的 RSA 计算分别为 0 次和 1 次。
2. 大文件下载中服务端会收到并处理大量的 ACK。在 ACK 处理上，并不需要接收一个处理一个。可以将一轮中所有的 ACK 解析后再同时进行处理。
3. 一次发包大小尽可能接近 MTU，QUIC 协议本身也提供了 MTU 探测的特性。
4. 尽可能减少协议栈的内存拷贝。

下图是小文件 0RTT 请求场景中，协议栈优化前和优化后的性能对比：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxQ64bRvaMOhYqTUrSlXSNcLryvJnEbtsIib6hsSjtia9ZQVQqKunbbiaMQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)优化前

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxSFicTVxv7TfQIXYE10VWSibgOVsBEEibEDicfcmY0soDWd3wL9icJ4eWAaQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

优化后

**4.4 QUIC 性能优化小结**

目前，我们将 QUIC 协议栈无缝接入到了高性能转发模块 NGINX 与 LEGO。在小包请求上，QUIC 的性能开销基本可以达到 HTTPS 的 90%以上。如果 QUIC 使用加速卡做 RSA OFFLOAD，性能甚至比原生的 HTTPS 强。在大包请求上，优化过后的 QUIC CPU 性能可以达到 HTTPS 70%，但在大部分机型中，大文件请求通常都是网卡先到达瓶颈。总的来说，QUIC 目前的性能问题做大规模部署已经不在是大问题。当然，这里面仍然存在优化空间，我们也为此做继续的优化之中。

## 5.**QUIC 的 0RTT 优化**

下图展示了一个 HTTPS 请求与一个 QUIC 请求的对比。可以看出，一个完全握手的 HTTPS 请求，在 HTTP 请求正式发出时经历了 3 个 RTT。而 QUIC 请求可以做到发 HTTP 请求之前的 0RTT 消耗。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwx91RaNRWQic9mCxO4PGDQ4MCwq2icwbDDXCKTX90xa9GyOuhwfjnGmB8g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

为什么 QUIC 可以做到 0RTT 呢？这里分为 QUIC 握手协议和 IETF QUIC 的 TLS1.3 协议。我们以 GQUIC 握手为例，如下图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwx6QsGdNF2Ortdw0wicNVvskEWzwJuhQQL0CJ5G5plMobPcJLDibXcCylQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

用户第一个 QUIC 包时发送一个没有带 server config 的 client hello，这个时候 server 回一个 REJ 的包，包含 server config 等信息。用户后续带上 server config 继续发 client hello，服务端会回 server hello。此时握手建立成功。

QUIC 加密握手基于 DH 的非对称秘钥交换算法进行秘钥交换，Server config 包含服务端的算法与公钥等信息，客户端利用服务端的公钥和自己的公私钥计算协商出连接的对称秘钥。

因此，第一次请求，客户端在没有保存服务端 server config 信息时，需要 1RTT 请求来完成第一次 QUIC 请求。而在后续请求中，客户端可以直接带上之前的 server config 来完成 0RTT 请求。

所以，这里的关键是：如何提升 0RTT 的比例。一种典型的场景就是，同一个用户在第一次 1RTT 请求获取到的 server config 信息，在后续多次请求中，不论路由到哪台 7 层 STGW 服务器，都能够尽可能的处理对应的 server config 信息。对此，我们尝试过很多方案，主要有：

1）4 层通过会话保持将同一个 IP 尽可能转发到同一个 7 层 STGW 服务器。这样的缺点是：1 用户的 ip 可能发生变化，2 四层基于 IP 的会话保持和基于连接 ID 的会话保持冲突，这可能导致 0RTT 提升的同时，连接迁移特性可能无法使用。

2）类似于 HTTPS 的分布式 session cache，同一个集群通过远端模块共享 server config 信息。这需要额外引进新的模块，并且会带来一定的延时。

3）类似于 session ticket，支持分布式无状态的 server config 生成。实际过程中可以根据日期和参数生成多组 SCFG，进一步提高可用性和安全性。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxNDfFofsXqvgdaYsbRVAW93lbxmgCPUwicS7KliaMrRbnx5Nk96wiaFLrg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

关于 0RTT 的优化目前我们做了不少工作，对于一些不敏感的数据传输，我们可以做到 100% 0RTT。

## 6.**QUIC 连接迁移实现**

QUIC 连接迁移是 QUIC 协议一个很重要的特点。QUIC 使用连接 ID 唯一区分连接，以应对用户的网络突然发生变化。一种典型的场景是 4G 与 wifi 之间的切换，之后用户的地址发生变化，原始的客户端 fd 已无法使用。这时只需要在客户端使用 QUIC SDK 重新创建新的 fd，并继续之前连接的发包，即可发出相同连接 ID 的包出去。

用户的 QUIC 包可能经过中间很多路径最后到达实际的业务服务器。我们以典型的腾讯业务走网关的场景分析：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxJqXSKh6Xiap0LK787MSC3tavDvYQC1ba42CZib3qhqaAUt4ypD4aZyqQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

一次 QUIC 请求经过外网后会先到四层 TGW 集群，然后转发给七层 STGW 集群的一台服务器。到达 STGW 服务器后，包会到达一个特定的 worker 进程处理。Worker 进行 QUIC 协议卸载后使用 TCP 或 UDP 转发给具体业务的一个 RS。如果用户的 QUIC 连接在处理过程中突然源地址发生了变化，我们如何继续正确的响应和维持这个 QUIC 连接？另一个场景是：用户的源地址没有发生变化，但是 7 层 STGW 服务器需要做配置变更和升级，这时 QUIC 连接是否可以维持？

### 6.1 四层基于 QUIC 连接 ID 的会话保持

当用户网络地址发生变化时，虽然源地址变化，但是 QUIC 连接 ID 仍可保持一致。包经过中间网络后首先会到 TGW 集群。为了保证用户的地址发生变化时，QUIC 连接得以维持，TGW 集群需要做正确的转发。

TGW 集群对 QUIC 的会话保持需要考虑 GQUIC 和 IETF QUIC 不同的情况。对于 GQUIC（Q043 以下）实现起来较为简单，因为 GQUIC 协议里的连接 ID 由客户产生，并在整个连接保持不变。对于 IETF QUIC，连接 ID 由客户端和服务端协商产生，需要考虑 long header 包和 short header 包等不同的场景。下图为 IETF 连接 ID 的协商过程以及 GQUIC 和 IETF QUIC 不同类型包的抓包分析。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwxc84LTFDJJ5kEZGIr99wkuiat6y2W1fLSvpdoPPytm1DeUgIrriar6RyQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)IETF 下连接 ID 的协商过程

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxYEwrSdHqfLJnQ0Ne7s5MWEfDf5uznkYO7PfF6NcFfHyrjpNG4Fyy8g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)GQUIC包

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxnZq9OdCpLau3elV1J6MHicAu6LzKqE4LOLKwX0MGZFbNmKSdiaKG0bMQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxtN5SOzicKicwMy3n2P83CnfLWibPObZByKEHW1mbIiccTD07EPqGyZicr7Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)IETF不同类型包

目前 TGW 集群已经支持 QUIC 的会话保持，基本原理是在同集群不同的 TGW 服务器之间同步 QUIC 连接信息，同时能够区分不同 QUIC 协议，将相同 QUIC 连接 ID 的包转发到相同的 7 层 STGW 服务器去。

### 6.2 七层单机多核的连接迁移

当包到达 STGW 服务器时，由于 STGW 服务器多核转发，此时还需要将 QUIC 包转发到同一个进程(或线程)去处理。当前，7 层网络框架一般使用多核+REUSEPORT 模型来提供高性能转发能力。对于 QUIC 服务，上层不同的进程在同一个 UDP 端口使用 REUSEPORT 监听。LINUX 内核默认基于 4 元组 hash，因此原生情况下，不同源地址的包是无法保证到达同一个进程的。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwx9855WHzJnibCTcUeBqjevl0p1tYq6GMpVoIKlfSUFEZVIcqJhVlQj2w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

EBPF 在内核 4.19 引入 BPF_PROG_TYPE_SK_REUSEPORT 这个 HOOK，可以策略性的控制一个包到达后发往到哪一个 accept 队列。这使得使用 EBPF 可以实现因为用户源地址变化引起的 QUIC 连接迁移。具体方法是在 EBPF 的 REUSEPORT 钩子处，解析 QUIC 包以及连接 ID，根据 QUIC 连接 ID 将包转发到对应的 worker 去。

### 6.3 配置加载和热升级连接保持

QUIC 连接迁移还有一种典型的场景是配置加载和热升级：当 STGW 服务器进程配置变更或者进行模块升级时，原生 NGINX 对 TCP 是可以保持连接不中断的。但是对于基于 UDP 的 QUIC，在未经过优化的情况下，我们无法在配置变更和模块升级过程中保持包的正常转发。

以 NGINX 的配置和热升级变更为例，NGINX 在配置变更和热升级时，会产生新的一组 worker，同时老的 worker 进入 shutting down 状态，而老的连接状态都在老的 worker 中。此时新老 worker 共用一组 fd。若老的 worker 关闭 fd 监听，则对于老的请求的连接都会超时。若老的 worker 继续监听 fd，则存在新老 worker 惊群读同一个 fd 的问题，这使得任意新老连接的包可能会到任意一个 worker，对新老连接都存在影响。

STGW 作为一个平台，每天的配置变更需求非常多，某些集群甚至达到了几秒一次配置变更。虽然我们实现了动态配置加载可以做到绝大部分场景不需要 reload 程序，但是仍然有少部分程序要 reload。同时，热升级这种场景也是比较常见的。如果配置 reload 或者模块升级就导致存量 QUIC 连接超时或中断，必然会对业务产生很大影响。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxuCdLzQkOhhoAyoiayViaOvujZU0J8L1LZLfuaLkMPXKuzhKScJXCf6oQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)NGINX配置变更和热升级时worker的收包

那么，如何解决这个问题呢？

基于内核的 EBPF 方案可以较好的处理 4G 与 WIFI 切换的场景，但是对于 STGW 服务器配置变更和模块升级的场景，却很难实现。

为此，STGW 使用了基于共享内存的 QUIC 连接迁移方案，使用共享内存管理不同进程的所有连接信息。同时为每个 worker 设定了一个 packet queue 用于接收来自别的进程连接迁移的包的转发。

可以说，目前 STGW 完全支持 4G 与 WIFI 互切的 QUIC 连接迁移场景。同时对于线上大规模运营来说，持续的配置变更和模块升级也不会影响 QUIC 连接的保持。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxdNoAVhtdb0EbIKcCWUZ2OXKNDKEjvDCdYXIibGQN3hZhicNolGT9AQfw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)STGW基于共享内存的连接迁移和连接保持方案

### 6.4 连接迁移的应用场景

一切重连开销很大的场景可以说都是 QUIC 连接迁移的使用场景。

例如游戏，视频以及业务信道传输就是比较典型的场景。当用户在 WIFI 网络切换到 4G 时，使用原始的 TCP 方案，在网络切换后会有一个连接重建过程。一般重连后，业务会有一些初始化操作，这会消耗几个甚至十几个 RTT，现象就是应用的卡顿或者菊花旋转。在使用 QUIC 连接迁移功能后，可以保证在 WIFI 与 4G 网络切换过程中，连接的正确迁移和存活，无需建立新的连接，从而使得业务的流畅度在网络切换时会得到很大提升。。

## 7.**灵活的拥塞算法与 TCP 重定义**

TCP 拥塞控制算法的目的可以简单概括为：充分利用网络带宽、降低网络延时、优化用户体验。然而就目前而言要实现这些目标就难免有权衡和取舍。LINUX 的拥塞控制算法经过很多次迭代，主流都是使用的 CUBIC 算法。在 Linux4.19 内核后，拥塞控制算法从 CUBIC 改为了 BBR。

BBR 算法相比之前拥塞控制算法，进行了非常重大的改变。BBR 通过实时计算带宽和最小 RTT 来决定发送速率 pacing rate 和窗口大小 cwnd。BBR 主要致力于：

1）在有一定丢包率的网络链路上充分利用带宽。

2）降低网络链路上的 buffer 占用率，从而降低延迟。

BBR 完全摒弃丢包作为拥塞控制的直接反馈因素，这也导致其对丢包并不是非常敏感。通过测试我们得出，在模拟一定概率丢包的网络情况下，对 QUIC 大文件的请求，BBR 的下载性能会比 CUBIC 更好。

QUIC 将拥塞控制做在了应用层，这也使得我们能够灵活的选择不同的拥塞控制算法。目前我们在 QUIC 上支持常见的 CUBIC,BBR 等算法，并实现了业务的自主配置，根据不同业务，针对请求的不同 VIP 使用不同的拥塞控制算法。同时，我们也支持针对同一个业务不同的用户的 RTT 动态的选择拥塞控制算法。另外，我们也同 CDN 的拥塞控制算法团队密切合作，以优化拥塞控制算法在不同场景下的业务体验。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxAibXTooLzSyeqslED3SiaPvb9icXgU5MEFrxiatnKxGH4YIo4IzNSCEutw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)业务可配的拥塞控制算法

除了拥塞控制，基于 QUIC 也可以在传输层针对特定的应用场景去不同的定制化。QUIC 将 TCP 的特性带到了应用层，这也使得在传输层上，我们有更多的操作可能性。例如，参照音视频领域一些常见的用法，发送数据时发送冗余数据，在一定丢包情况下，QUIC 传输层可以自动恢复数据，而不需要等待数据包的重传，以降低音视频的卡顿率。这些基于 TCP 是很难做到的。业务如果需要重定义 TCP 的一些功能或特性，来提升业务体验，QUIC 将会有很大的发挥空间。

## 8.**支持 QUIC 私有协议**

STGW 作为 7 层网关，提供通用 WEB 协议卸载和转发。因此，支持 QUIC 的 WEB 协议如 GQUIC,HTTP/3 是我们的基本能力。

但是，如前面所说，QUIC 作为通用传输层协议，不仅仅应用于 WEB，任何私有协议都可以改造到 QUIC 下。使用 QUIC 握手协议之后，客户端就可以根据自己的业务需求，发送 GQUIC,HTTP/3 等 WEB 请求，或者可以发送任意自己的私有协议。

STGW 基于 NGINX 的 STREAM 模块，对其进行深度改造，使得任意私有协议都可以跑在 QUIC 协议之下。这也大大增加了 QUIC 的应用场景。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwxp0ibxxI1J8hnw7Goz8685YlKn0micnaQGicswKIwo30ehXdyib6CwvEcPg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

所以，不要觉得不是 HTTP 协议就用不了 QUIC。只要你理解了 QUIC 的特性，并且觉得 QUIC 的特性能够优化业务体验，那么，来试试 QUIC 吧。

## 9.**QUIC 定制化 SDK**

由于 QUIC 尚未标准化，当前使用 QUIC 相对来说门槛较高。

客户端方面，由于 Google 将其浏览器以 Chromium 项目开源出来，其网络协议栈 Cronet 成为业界 QUIC 客户端的主要参考对象。但 Cronet 因为 API 支持有限，代码复杂，难以满足个性化需求等，不适合直接用在我们的移动客户端上。同时，QUIC 作为一个还在高速发展的协议，服务端和客户端都在快速迭代，需要保持紧密的跟进。

基于上述痛点以及 QUIC 渐渐流行的趋势，我们提供比 Google QUIC 更定制化的 TQUIC SDK。TQUIC SDK 相比 Cronet，有体积更加轻量，简单易用，支持私有协议，连接迁移等诸多优点。目前，TQUIC SDK 已应用于公司内部多个业务之中。

## 10.**总结**

本文综合介绍了 STGW 在大规模应用 QUIC 协议过程中做的一些优化和成果。当前：

1. 我们将 QUIC 协议栈与高性能网络框架做了深度融合，并支持 QUIC WEB 协议，QUIC 私有协议，带外拥塞控制配置等大部分 QUIC 功能和特性。满足 QUIC 大规模部署与运营。
2. 我们对 QUIC 协议栈 0RTT，1RTT，小包，高带宽等多场景做了大量的性能优化，解决了 QUIC 严重消耗 CPU 资源的几个瓶颈。在小包请求上性能基本可以达到 HTTPS 的 90%。
3. 针对 RTT 敏感的短连接业务，我们大大提升了 0RTT 的比例，某些场景可以做到 100% 0RTT。
4. 更全面的连接迁移，解决了 4 层，7 层，多集群、多机器、多进程以及进程重启、重加载，模块升级等各种场景下的连接迁移问题。
5. 我们提供了定制化的 QUIC SDK，以用于客户端满足定制化 QUIC 的各种特性。

QUIC 仍然有很多特性需要充分挖掘，如 QUIC 本身基于 UDP 没有队头阻塞特性以及 QPACK 编码在 HTTP/3 的 HTTP 头部上对队头阻塞的优化等。这些特性在弱网环境下对业务都会有较好的性能提升和卡顿率降低，特别是多路复用场景。目前我们也在结合业务积累更多的实际数据，并期望在这块能够有更多的优化。

QUIC 以及相关的 HTTP/3 等协议即将形成最终的标准，我们也在不断跟进 QUIC 协议的演进。

STGW 将持续为自研业务和腾讯云 CLB 客户提供 QUIC 的统一接入和优化，帮助业务更好的提升用户体验。

原文作者：wentaomao，腾讯 TEG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/ciR-1N4z0zvGOJSoyrvMUA

# 【NO.231】Redis 多线程网络模型全面揭秘

## 0.**导言**

在目前的技术选型中，Redis 俨然已经成为了系统高性能缓存方案的事实标准，因此现在 Redis 也成为了后端开发的基本技能树之一，Redis 的底层原理也顺理成章地成为了必须学习的知识。

Redis 从本质上来讲是一个网络服务器，而对于一个网络服务器来说，网络模型是它的精华，搞懂了一个网络服务器的网络模型，你也就搞懂了它的本质。

本文通过层层递进的方式，介绍了 Redis 网络模型的版本变更历程，剖析了其从单线程进化到多线程的工作原理，此外，还一并分析并解答了 Redis 的网络模型的很多抉择背后的思考，帮助读者能更深刻地理解 Redis 网络模型的设计。

## 1.**Redis 有多快？**

根据官方的 benchmark，通常来说，在一台普通硬件配置的 Linux 机器上跑单个 Redis 实例，处理简单命令（时间复杂度 O(N) 或者 O(log(N))），QPS 可以达到 8w+，而如果使用 pipeline 批处理功能，则 QPS 至高能达到 100w。

仅从性能层面进行评判，Redis 完全可以被称之为高性能缓存方案。

## 2.**Redis 为什么快？**

Redis 的高性能得益于以下几个基础：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRv4vcI9ozcWceomicZtwbhElPzboBJvwicvBh6GwHQ8phib2PkeLjQhBUQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **C 语言实现**，虽然 C 对 Redis 的性能有助力，但语言并不是最核心因素。
- **纯内存 I/O**，相较于其他基于磁盘的 DB，Redis 的纯内存操作有着天然的性能优势。
- **I/O 多路复用**，基于 epoll/select/kqueue 等 I/O 多路复用技术，实现高吞吐的网络 I/O。
- **单线程模型**，单线程无法利用多核，但是从另一个层面来说则避免了多线程频繁上下文切换，以及同步机制如锁带来的开销。

## 3.**Redis 为何选择单线程？**

Redis 的核心网络模型选择用单线程来实现，这在一开始就引起了很多人的不解，Redis 官方的对于此的回答是：

> It's not very frequent that CPU becomes your bottleneck with Redis, as usually Redis is either memory or network bound. For instance, using pipelining Redis running on an average Linux system can deliver even 1 million requests per second, so if your application mainly uses O(N) or O(log(N)) commands, it is hardly going to use too much CPU.

核心意思就是，对于一个 DB 来说，CPU 通常不会是瓶颈，因为大多数请求不会是 CPU 密集型的，而是 I/O 密集型。具体到 Redis 的话，如果不考虑 RDB/AOF 等持久化方案，Redis 是完全的纯内存操作，执行速度是非常快的，因此这部分操作通常不会是性能瓶颈，Redis 真正的性能瓶颈在于网络 I/O，也就是客户端和服务端之间的网络传输延迟，因此 Redis 选择了单线程的 I/O 多路复用来实现它的核心网络模型。

上面是比较笼统的官方答案，实际上更加具体的选择单线程的原因可以归纳如下：

### 3.1 避免过多的上下文切换开销

多线程调度过程中必然需要在 CPU 之间切换线程上下文 context，而上下文的切换又涉及程序计数器、堆栈指针和程序状态字等一系列的寄存器置换、程序堆栈重置甚至是 CPU 高速缓存、TLB 快表的汰换，如果是进程内的多线程切换还好一些，因为单一进程内多线程共享进程地址空间，因此线程上下文比之进程上下文要小得多，如果是跨进程调度，则需要切换掉整个进程地址空间。

如果是单线程则可以规避进程内频繁的线程切换开销，因为程序始终运行在进程中单个线程内，没有多线程切换的场景。

### 3.2 避免同步机制的开销

如果 Redis 选择多线程模型，又因为 Redis 是一个数据库，那么势必涉及到底层数据同步的问题，则必然会引入某些同步机制，比如锁，而我们知道 Redis 不仅仅提供了简单的 key-value 数据结构，还有 list、set 和 hash 等等其他丰富的数据结构，而不同的数据结构对同步访问的加锁粒度又不尽相同，可能会导致在操作数据过程中带来很多加锁解锁的开销，增加程序复杂度的同时还会降低性能。

### 3.3 简单可维护

Redis 的作者 Salvatore Sanfilippo (别称 antirez) 对 Redis 的设计和代码有着近乎偏执的简洁性理念，你可以在阅读 Redis 的源码或者给 Redis 提交 PR 的之时感受到这份偏执。因此代码的简单可维护性必然是 Redis 早期的核心准则之一，而引入多线程必然会导致代码的复杂度上升和可维护性下降。

事实上，多线程编程也不是那么尽善尽美，首先多线程的引入会使得程序不再保持代码逻辑上的串行性，代码执行的顺序将变成不可预测的，稍不注意就会导致程序出现各种并发编程的问题；其次，多线程模式也使得程序调试更加复杂和麻烦。网络上有一幅很有意思的图片，生动形象地描述了并发编程面临的窘境。

你期望的多线程编程 **VS** 实际上的多线程编程：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat1HcdK6LgF6qcXcENK8AcR4VVq0ITnfMdVzgm2mSYNFSzicDxz0bDBEB8Wzkwnu1hKRTYXK06SuLQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)你期望的多线程VS实际上的多线程

前面我们提到引入多线程必须的同步机制，如果 Redis 使用多线程模式，那么所有的底层数据结构都必须实现成线程安全的，这无疑又使得 Redis 的实现变得更加复杂。

总而言之，Redis 选择单线程可以说是多方博弈之后的一种权衡：在保证足够的性能表现之下，使用单线程保持代码的简单和可维护性。

## 4.**Redis 真的是单线程？**

在讨论这个问题之前，我们要先明确『单线程』这个概念的边界：它的覆盖范围是核心网络模型，抑或是整个 Redis？如果是前者，那么答案是肯定的，在 Redis 的 v6.0 版本正式引入多线程之前，其网络模型一直是单线程模式的；如果是后者，那么答案则是否定的，Redis 早在 v4.0 就已经引入了多线程。

因此，当我们讨论 Redis 的多线程之时，有必要对 Redis 的版本划出两个重要的节点：

1. Redis v4.0（引入多线程处理异步任务）
2. Redis v6.0（正式在网络模型中实现 I/O 多线程）

### 4.1 单线程事件循环

我们首先来剖析一下 Redis 的核心网络模型，从 Redis 的 v1.0 到 v6.0 版本之前，Redis 的核心网络模型一直是一个典型的单 Reactor 模型：利用 epoll/select/kqueue 等多路复用技术，在单线程的事件循环中不断去处理事件（客户端请求），最后回写响应数据到客户端：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRQjOP938GeNx5AUv8ibY0Yvpzn9g3g5AfImJqLqv5EQ5bAI27hZia4gug/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

这里有几个核心的概念需要学习：

- **client**：客户端对象，Redis 是典型的 CS 架构（Client <---> Server），客户端通过 **socket** 与服务端建立网络通道然后发送请求命令，服务端执行请求的命令并回复。**Redis** 使用结构体 **client** 存储客户端的所有相关信息，包括但不限于`封装的套接字连接 -- *conn`，`当前选择的数据库指针 -- *db`，`读入缓冲区 -- querybuf`，`写出缓冲区 -- buf`，`写出数据链表 -- reply`等。
- **aeApiPoll**：I/O 多路复用 API，是基于 epoll_wait/select/kevent 等系统调用的封装，监听等待读写事件触发，然后处理，它是事件循环（Event Loop）中的核心函数，是事件驱动得以运行的基础。
- **acceptTcpHandler**：连接应答处理器，底层使用系统调用 `accept` 接受来自客户端的新连接，并为新连接注册绑定命令读取处理器，以备后续处理新的客户端 TCP 连接；除了这个处理器，还有对应的 `acceptUnixHandler` 负责处理 Unix Domain Socket 以及 `acceptTLSHandler` 负责处理 TLS 加密连接。
- **readQueryFromClient**：命令读取处理器，解析并执行客户端的请求命令。
- **beforeSleep**：事件循环中进入 aeApiPoll 等待事件到来之前会执行的函数，其中包含一些日常的任务，比如把 `client->buf` 或者 `client->reply` （后面会解释为什么这里需要两个缓冲区）中的响应写回到客户端，持久化 AOF 缓冲区的数据到磁盘等，相对应的还有一个 afterSleep 函数，在 aeApiPoll 之后执行。
- **sendReplyToClient**：命令回复处理器，当一次事件循环之后写出缓冲区中还有数据残留，则这个处理器会被注册绑定到相应的连接上，等连接触发写就绪事件时，它会将写出缓冲区剩余的数据回写到客户端。

Redis 内部实现了一个高性能的事件库 --- AE，基于 epoll/select/kqueue/evport 四种事件驱动技术，实现 Linux/MacOS/FreeBSD/Solaris 多平台的高性能事件循环模型。Redis 的核心网络模型正式构筑在 AE 之上，包括 I/O 多路复用、各类处理器的注册绑定，都是基于此才得以运行。

至此，我们可以描绘出客户端向 Redis 发起请求命令的工作原理：

1. Redis 服务器启动，开启主线程事件循环（Event Loop），注册 `acceptTcpHandler` 连接应答处理器到用户配置的监听端口对应的文件描述符，等待新连接到来；
2. 客户端和服务端建立网络连接；
3. `acceptTcpHandler` 被调用，主线程使用 AE 的 API 将 `readQueryFromClient` 命令读取处理器绑定到新连接对应的文件描述符上，并初始化一个 `client` 绑定这个客户端连接；
4. 客户端发送请求命令，触发读就绪事件，主线程调用 `readQueryFromClient` 通过 socket 读取客户端发送过来的命令存入 `client->querybuf` 读入缓冲区；
5. 接着调用 `processInputBuffer`，在其中使用 `processInlineBuffer` 或者 `processMultibulkBuffer` 根据 Redis 协议解析命令，最后调用 `processCommand` 执行命令；
6. 根据请求命令的类型（SET, GET, DEL, EXEC 等），分配相应的命令执行器去执行，最后调用 `addReply` 函数族的一系列函数将响应数据写入到对应 `client` 的写出缓冲区：`client->buf` 或者 `client->reply` ，`client->buf` 是首选的写出缓冲区，固定大小 16KB，一般来说可以缓冲足够多的响应数据，但是如果客户端在时间窗口内需要响应的数据非常大，那么则会自动切换到 `client->reply` 链表上去，使用链表理论上能够保存无限大的数据（受限于机器的物理内存），最后把 `client` 添加进一个 LIFO 队列 `clients_pending_write`；
7. 在事件循环（Event Loop）中，主线程执行 `beforeSleep` --> `handleClientsWithPendingWrites`，遍历 `clients_pending_write` 队列，调用 `writeToClient` 把 `client` 的写出缓冲区里的数据回写到客户端，如果写出缓冲区还有数据遗留，则注册 `sendReplyToClient` 命令回复处理器到该连接的写就绪事件，等待客户端可写时在事件循环中再继续回写残余的响应数据。

对于那些想利用多核优势提升性能的用户来说，Redis 官方给出的解决方案也非常简单粗暴：在同一个机器上多跑几个 Redis 实例。事实上，为了保证高可用，线上业务一般不太可能会是单机模式，更加常见的是利用 Redis 分布式集群多节点和数据分片负载均衡来提升性能和保证高可用。

### 4.2 多线程异步任务

以上便是 Redis 的核心网络模型，这个单线程网络模型一直到 Redis v6.0 才改造成多线程模式，但这并不意味着整个 Redis 一直都只是单线程。

Redis 在 v4.0 版本的时候就已经引入了的多线程来做一些异步操作，此举主要针对的是那些非常耗时的命令，通过将这些命令的执行进行异步化，避免阻塞单线程的事件循环。

我们知道 Redis 的 `DEL` 命令是用来删除掉一个或多个 key 储存的值，它是一个阻塞的命令，大多数情况下你要删除的 key 里存的值不会特别多，最多也就几十上百个对象，所以可以很快执行完，但是如果你要删的是一个超大的键值对，里面有几百万个对象，那么这条命令可能会阻塞至少好几秒，又因为事件循环是单线程的，所以会阻塞后面的其他事件，导致吞吐量下降。

Redis 的作者 antirez 为了解决这个问题进行了很多思考，一开始他想的办法是一种渐进式的方案：利用定时器和数据游标，每次只删除一小部分的数据，比如 1000 个对象，最终清除掉所有的数据，但是这种方案有个致命的缺陷，如果同时还有其他客户端往某个正在被渐进式删除的 key 里继续写入数据，而且删除的速度跟不上写入的数据，那么将会无止境地消耗内存，虽然后来通过一个巧妙的办法解决了，但是这种实现使 Redis 变得更加复杂，而多线程看起来似乎是一个水到渠成的解决方案：简单、易理解。于是，最终 antirez 选择引入多线程来实现这一类非阻塞的命令。更多 antirez 在这方面的思考可以阅读一下他发表的博客：[Lazy Redis is better Redis](http://antirez.com/news/93)。

于是，在 Redis v4.0 之后增加了一些的非阻塞命令如 `UNLINK`、`FLUSHALL ASYNC`、`FLUSHDB ASYNC`。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRplfowbOAzxBrakDhbibteUuBibSRRfF3oiau3LamJnnsMCLCcyEzDU5ag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

`UNLINK` 命令其实就是 `DEL` 的异步版本，它不会同步删除数据，而只是把 key 从 keyspace 中暂时移除掉，然后将任务添加到一个异步队列，最后由后台线程去删除，不过这里需要考虑一种情况是如果用 `UNLINK` 去删除一个很小的 key，用异步的方式去做反而开销更大，所以它会先计算一个开销的阀值，只有当这个值大于 64 才会使用异步的方式去删除 key，对于基本的数据类型如 List、Set、Hash 这些，阀值就是其中存储的对象数量。

## **5.Redis 多线程网络模型**

前面提到 Redis 最初选择单线程网络模型的理由是：CPU 通常不会成为性能瓶颈，瓶颈往往是**内存**和**网络**，因此单线程足够了。那么为什么现在 Redis 又要引入多线程呢？很简单，就是 Redis 的网络 I/O 瓶颈已经越来越明显了。

随着互联网的飞速发展，互联网业务系统所要处理的线上流量越来越大，Redis 的单线程模式会导致系统消耗很多 CPU 时间在网络 I/O 上从而降低吞吐量，要提升 Redis 的性能有两个方向：

- 优化网络 I/O 模块
- 提高机器内存读写的速度

后者依赖于硬件的发展，暂时无解。所以只能从前者下手，网络 I/O 的优化又可以分为两个方向：

- 零拷贝技术或者 DPDK 技术
- 利用多核优势

零拷贝技术有其局限性，无法完全适配 Redis 这一类复杂的网络 I/O 场景，更多网络 I/O 对 CPU 时间的消耗和 Linux 零拷贝技术，可以阅读我的另一篇文章：[Linux I/O 原理和 Zero-copy 技术全面揭秘](https://strikefreedom.top/linux-io-and-zero-copy)。而 DPDK 技术通过旁路网卡 I/O 绕过内核协议栈的方式又太过于复杂以及需要内核甚至是硬件的支持。

因此，利用多核优势成为了优化网络 I/O 性价比最高的方案。

6.0 版本之后，Redis 正式在核心网络模型中引入了多线程，也就是所谓的 *I/O threading*，至此 Redis 真正拥有了多线程模型。前一小节，我们了解了 Redis 在 6.0 版本之前的单线程事件循环模型，实际上就是一个非常经典的 Reactor 模型：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcR5VNqhl8TpY2iaCiaED7ic8ZkgMaNfUAfYa6onXs6cRsoREiblYL3icf6pog/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

目前 Linux 平台上主流的高性能网络库/框架中，大都采用 Reactor 模式，比如 netty、libevent、libuv、POE(Perl)、Twisted(Python)等。

Reactor 模式本质上指的是使用 `I/O 多路复用(I/O multiplexing) + 非阻塞 I/O(non-blocking I/O)` 的模式。

更多关于 Reactor 模式的细节可以参考我之前的文章：[Go netpoller 原生网络模型之源码全面揭秘](https://strikefreedom.top/go-netpoll-io-multiplexing-reactor)，Reactor 网络模型那一小节，这里不再赘述。

Redis 的核心网络模型在 6.0 版本之前，一直是单 Reactor 模式：所有事件的处理都在单个线程内完成，虽然在 4.0 版本中引入了多线程，但是那个更像是针对特定场景（删除超大 key 值等）而打的补丁，并不能被视作核心网络模型的多线程。

通常来说，单 Reactor 模式，引入多线程之后会进化为 Multi-Reactors 模式，基本工作模式如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRJLN1JHqxe4n2jQvvLYDSAeH1iax9Bsb5VqC0ZAATlE6y97xoOe4ibtiaw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

区别于单 Reactor 模式，这种模式不再是单线程的事件循环，而是有多个线程（Sub Reactors）各自维护一个独立的事件循环，由 Main Reactor 负责接收新连接并分发给 Sub Reactors 去独立处理，最后 Sub Reactors 回写响应给客户端。

Multiple Reactors 模式通常也可以等同于 Master-Workers 模式，比如 Nginx 和 Memcached 等就是采用这种多线程模型，虽然不同的项目实现细节略有区别，但总体来说模式是一致的。

### 5.1 设计思路

Redis 虽然也实现了多线程，但是却不是标准的 Multi-Reactors/Master-Workers 模式，这其中的缘由我们后面会分析，现在我们先看一下 Redis 多线程网络模型的总体设计：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRGtQ2jB57bdFbiawdd4krQVfNibfYlicxyYkLjkMPdTgH8ep6Av8jniaSsA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

1. Redis 服务器启动，开启主线程事件循环（Event Loop），注册 `acceptTcpHandler` 连接应答处理器到用户配置的监听端口对应的文件描述符，等待新连接到来；
2. 客户端和服务端建立网络连接；
3. `acceptTcpHandler` 被调用，主线程使用 AE 的 API 将 `readQueryFromClient` 命令读取处理器绑定到新连接对应的文件描述符上，并初始化一个 `client` 绑定这个客户端连接；
4. 客户端发送请求命令，触发读就绪事件，服务端主线程不会通过 socket 去读取客户端的请求命令，而是先将 `client` 放入一个 LIFO 队列 `clients_pending_read`；
5. 在事件循环（Event Loop）中，主线程执行 `beforeSleep` -->`handleClientsWithPendingReadsUsingThreads`，利用 Round-Robin 轮询负载均衡策略，把 `clients_pending_read`队列中的连接均匀地分配给 I/O 线程各自的本地 FIFO 任务队列 `io_threads_list[id]` 和主线程自己，I/O 线程通过 socket 读取客户端的请求命令，存入 `client->querybuf` 并解析第一个命令，**但不执行命令**，主线程忙轮询，等待所有 I/O 线程完成读取任务；
6. 主线程和所有 I/O 线程都完成了读取任务，主线程结束忙轮询，遍历 `clients_pending_read` 队列，**执行所有客户端连接的请求命令**，先调用 `processCommandAndResetClient` 执行第一条已经解析好的命令，然后调用 `processInputBuffer` 解析并执行客户端连接的所有命令，在其中使用 `processInlineBuffer` 或者 `processMultibulkBuffer` 根据 Redis 协议解析命令，最后调用 `processCommand` 执行命令；
7. 根据请求命令的类型（SET, GET, DEL, EXEC 等），分配相应的命令执行器去执行，最后调用 `addReply` 函数族的一系列函数将响应数据写入到对应 `client` 的写出缓冲区：`client->buf` 或者 `client->reply` ，`client->buf` 是首选的写出缓冲区，固定大小 16KB，一般来说可以缓冲足够多的响应数据，但是如果客户端在时间窗口内需要响应的数据非常大，那么则会自动切换到 `client->reply` 链表上去，使用链表理论上能够保存无限大的数据（受限于机器的物理内存），最后把 `client` 添加进一个 LIFO 队列 `clients_pending_write`；
8. 在事件循环（Event Loop）中，主线程执行 `beforeSleep` --> `handleClientsWithPendingWritesUsingThreads`，利用 Round-Robin 轮询负载均衡策略，把 `clients_pending_write` 队列中的连接均匀地分配给 I/O 线程各自的本地 FIFO 任务队列 `io_threads_list[id]` 和主线程自己，I/O 线程通过调用 `writeToClient` 把 `client` 的写出缓冲区里的数据回写到客户端，主线程忙轮询，等待所有 I/O 线程完成写出任务；
9. 主线程和所有 I/O 线程都完成了写出任务， 主线程结束忙轮询，遍历 `clients_pending_write` 队列，如果 `client` 的写出缓冲区还有数据遗留，则注册 `sendReplyToClient` 到该连接的写就绪事件，等待客户端可写时在事件循环中再继续回写残余的响应数据。

这里大部分逻辑和之前的单线程模型是一致的，变动的地方仅仅是把读取客户端请求命令和回写响应数据的逻辑异步化了，交给 I/O 线程去完成，这里需要特别注意的一点是：**I/O 线程仅仅是读取和解析客户端命令而不会真正去执行命令，客户端命令的执行最终还是要在主线程上完成**。

### 5.2 源码剖析

> 以下所有代码基于目前最新的 [Redis v6.0.10](https://github.com/redis/redis/tree/6.0.10) 版本。

#### 5.2.1 **多线程初始化**

```
void initThreadedIO(void) {
    server.io_threads_active = 0; /* We start with threads not active. */

    // 如果用户只配置了一个 I/O 线程，则不会创建新线程（效率低），直接在主线程里处理 I/O。
    if (server.io_threads_num == 1) return;

    if (server.io_threads_num > IO_THREADS_MAX_NUM) {
        serverLog(LL_WARNING,"Fatal: too many I/O threads configured. "
                             "The maximum number is %d.", IO_THREADS_MAX_NUM);
        exit(1);
    }

    // 根据用户配置的 I/O 线程数，启动线程。
    for (int i = 0; i < server.io_threads_num; i++) {
        // 初始化 I/O 线程的本地任务队列。
        io_threads_list[i] = listCreate();
        if (i == 0) continue; // 线程 0 是主线程。

        // 初始化 I/O 线程并启动。
        pthread_t tid;
        // 每个 I/O 线程会分配一个本地锁，用来休眠和唤醒线程。
        pthread_mutex_init(&io_threads_mutex[i],NULL);
        // 每个 I/O 线程分配一个原子计数器，用来记录当前遗留的任务数量。
        io_threads_pending[i] = 0;
        // 主线程在启动 I/O 线程的时候会默认先锁住它，直到有 I/O 任务才唤醒它。
        pthread_mutex_lock(&io_threads_mutex[i]);
        // 启动线程，进入 I/O 线程的主逻辑函数 IOThreadMain。
        if (pthread_create(&tid,NULL,IOThreadMain,(void*)(long)i) != 0) {
            serverLog(LL_WARNING,"Fatal: Can't initialize IO thread.");
            exit(1);
        }
        io_threads[i] = tid;
    }
}
```

`initThreadedIO` 会在 Redis 服务器启动时的初始化工作的末尾被调用，初始化 I/O 多线程并启动。

Redis 的多线程模式默认是关闭的，需要用户在 `redis.conf` 配置文件中开启：

```
io-threads 4
io-threads-do-reads yes
```

#### 5.2.2 **读取请求**

当客户端发送请求命令之后，会触发 Redis 主线程的事件循环，命令处理器 `readQueryFromClient` 被回调，在以前的单线程模型下，这个方法会直接读取解析客户端命令并执行，但是多线程模式下，则会把 `client` 加入到 `clients_pending_read` 任务队列中去，后面主线程再分配到 I/O 线程去读取客户端请求命令：

```
void readQueryFromClient(connection *conn) {
    client *c = connGetPrivateData(conn);
    int nread, readlen;
    size_t qblen;

    // 检查是否开启了多线程，如果是则把 client 加入异步队列之后返回。
    if (postponeClientRead(c)) return;
    
    // 省略代码，下面的代码逻辑和单线程版本几乎是一样的。
    ... 
}

int postponeClientRead(client *c) {
    // 当多线程 I/O 模式开启、主线程没有在处理阻塞任务时，将 client 加入异步队列。
    if (server.io_threads_active &&
        server.io_threads_do_reads &&
        !ProcessingEventsWhileBlocked &&
        !(c->flags & (CLIENT_MASTER|CLIENT_SLAVE|CLIENT_PENDING_READ)))
    {
        // 给 client 打上 CLIENT_PENDING_READ 标识，表示该 client 需要被多线程处理，
        // 后续在 I/O 线程中会在读取和解析完客户端命令之后判断该标识并放弃执行命令，让主线程去执行。
        c->flags |= CLIENT_PENDING_READ;
        listAddNodeHead(server.clients_pending_read,c);
        return 1;
    } else {
        return 0;
    }
}
```

接着主线程会在事件循环的 `beforeSleep()` 方法中，调用 `handleClientsWithPendingReadsUsingThreads`：

```
int handleClientsWithPendingReadsUsingThreads(void) {
    if (!server.io_threads_active || !server.io_threads_do_reads) return 0;
    int processed = listLength(server.clients_pending_read);
    if (processed == 0) return 0;

    if (tio_debug) printf("%d TOTAL READ pending clients\n", processed);

    // 遍历待读取的 client 队列 clients_pending_read，
    // 通过 RR 轮询均匀地分配给 I/O 线程和主线程自己（编号 0）。
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_read,&li);
    int item_id = 0;
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;
    }

    // 设置当前 I/O 操作为读取操作，给每个 I/O 线程的计数器设置分配的任务数量，
    // 让 I/O 线程可以开始工作：只读取和解析命令，不执行。
    io_threads_op = IO_THREADS_OP_READ;
    for (int j = 1; j < server.io_threads_num; j++) {
        int count = listLength(io_threads_list[j]);
        io_threads_pending[j] = count;
    }

    // 主线程自己也会去执行读取客户端请求命令的任务，以达到最大限度利用 CPU。
    listRewind(io_threads_list[0],&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        readQueryFromClient(c->conn);
    }
    listEmpty(io_threads_list[0]);

    // 忙轮询，累加所有 I/O 线程的原子任务计数器，直到所有计数器的遗留任务数量都是 0，
    // 表示所有任务都已经执行完成，结束轮询。
    while(1) {
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += io_threads_pending[j];
        if (pending == 0) break;
    }
    if (tio_debug) printf("I/O READ All threads finshed\n");

    // 遍历待读取的 client 队列，清除 CLIENT_PENDING_READ 和 CLIENT_PENDING_COMMAND 标记，
    // 然后解析并执行所有 client 的命令。
    while(listLength(server.clients_pending_read)) {
        ln = listFirst(server.clients_pending_read);
        client *c = listNodeValue(ln);
        c->flags &= ~CLIENT_PENDING_READ;
        listDelNode(server.clients_pending_read,ln);

        if (c->flags & CLIENT_PENDING_COMMAND) {
            c->flags &= ~CLIENT_PENDING_COMMAND;
            // client 的第一条命令已经被解析好了，直接尝试执行。
            if (processCommandAndResetClient(c) == C_ERR) {
                /* If the client is no longer valid, we avoid
                 * processing the client later. So we just go
                 * to the next. */
                continue;
            }
        }
        processInputBuffer(c); // 继续解析并执行 client 命令。

        // 命令执行完成之后，如果 client 中有响应数据需要回写到客户端，则将 client 加入到待写出队列 clients_pending_write
        if (!(c->flags & CLIENT_PENDING_WRITE) && clientHasPendingReplies(c))
            clientInstallWriteHandler(c);
    }

    /* Update processed count on server */
    server.stat_io_reads_processed += processed;

    return processed;
}
```

这里的核心工作是：

- 遍历待读取的 `client` 队列 `clients_pending_read`，通过 RR 策略把所有任务分配给 I/O 线程和主线程去读取和解析客户端命令。
- 忙轮询等待所有 I/O 线程完成任务。
- 最后再遍历 `clients_pending_read`，执行所有 `client` 的命令。

#### 5.2.3 **写回响应**

完成命令的读取、解析以及执行之后，客户端命令的响应数据已经存入 `client->buf` 或者 `client->reply` 中了，接下来就需要把响应数据回写到客户端了，还是在 `beforeSleep` 中， 主线程调用 `handleClientsWithPendingWritesUsingThreads`：

```
int handleClientsWithPendingWritesUsingThreads(void) {
    int processed = listLength(server.clients_pending_write);
    if (processed == 0) return 0; /* Return ASAP if there are no clients. */

    // 如果用户设置的 I/O 线程数等于 1 或者当前 clients_pending_write 队列中待写出的 client
    // 数量不足 I/O 线程数的两倍，则不用多线程的逻辑，让所有 I/O 线程进入休眠，
    // 直接在主线程把所有 client 的相应数据回写到客户端。
    if (server.io_threads_num == 1 || stopThreadedIOIfNeeded()) {
        return handleClientsWithPendingWrites();
    }

    // 唤醒正在休眠的 I/O 线程（如果有的话）。
    if (!server.io_threads_active) startThreadedIO();

    if (tio_debug) printf("%d TOTAL WRITE pending clients\n", processed);

    // 遍历待写出的 client 队列 clients_pending_write，
    // 通过 RR 轮询均匀地分配给 I/O 线程和主线程自己（编号 0）。
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_write,&li);
    int item_id = 0;
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        c->flags &= ~CLIENT_PENDING_WRITE;

        /* Remove clients from the list of pending writes since
         * they are going to be closed ASAP. */
        if (c->flags & CLIENT_CLOSE_ASAP) {
            listDelNode(server.clients_pending_write, ln);
            continue;
        }

        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;
    }

    // 设置当前 I/O 操作为写出操作，给每个 I/O 线程的计数器设置分配的任务数量，
    // 让 I/O 线程可以开始工作，把写出缓冲区（client->buf 或 c->reply）中的响应数据回写到客户端。
    io_threads_op = IO_THREADS_OP_WRITE;
    for (int j = 1; j < server.io_threads_num; j++) {
        int count = listLength(io_threads_list[j]);
        io_threads_pending[j] = count;
    }

    // 主线程自己也会去执行读取客户端请求命令的任务，以达到最大限度利用 CPU。
    listRewind(io_threads_list[0],&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        writeToClient(c,0);
    }
    listEmpty(io_threads_list[0]);

    // 忙轮询，累加所有 I/O 线程的原子任务计数器，直到所有计数器的遗留任务数量都是 0。
    // 表示所有任务都已经执行完成，结束轮询。
    while(1) {
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += io_threads_pending[j];
        if (pending == 0) break;
    }
    if (tio_debug) printf("I/O WRITE All threads finshed\n");

    // 最后再遍历一次 clients_pending_write 队列，检查是否还有 client 的写出缓冲区中有残留数据，
    // 如果有，那就为 client 注册一个命令回复器 sendReplyToClient，等待客户端写就绪再继续把数据回写。
    listRewind(server.clients_pending_write,&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);

        // 检查 client 的写出缓冲区是否还有遗留数据。
        if (clientHasPendingReplies(c) &&
                connSetWriteHandler(c->conn, sendReplyToClient) == AE_ERR)
        {
            freeClientAsync(c);
        }
    }
    listEmpty(server.clients_pending_write);

    /* Update processed count on server */
    server.stat_io_writes_processed += processed;

    return processed;
}
```

这里的核心工作是：

- 检查当前任务负载，如果当前的任务数量不足以用多线程模式处理的话，则休眠 I/O 线程并且直接同步将响应数据回写到客户端。
- 唤醒正在休眠的 I/O 线程（如果有的话）。
- 遍历待写出的 `client` 队列 `clients_pending_write`，通过 RR 策略把所有任务分配给 I/O 线程和主线程去将响应数据写回到客户端。
- 忙轮询等待所有 I/O 线程完成任务。
- 最后再遍历 `clients_pending_write`，为那些还残留有响应数据的 `client` 注册命令回复处理器 `sendReplyToClient`，等待客户端可写之后在事件循环中继续回写残余的响应数据。

#### 5.2.4 **I/O 线程主逻辑**

```
void *IOThreadMain(void *myid) {
    /* The ID is the thread number (from 0 to server.iothreads_num-1), and is
     * used by the thread to just manipulate a single sub-array of clients. */
    long id = (unsigned long)myid;
    char thdname[16];

    snprintf(thdname, sizeof(thdname), "io_thd_%ld", id);
    redis_set_thread_title(thdname);
    // 设置 I/O 线程的 CPU 亲和性，尽可能将 I/O 线程（以及主线程，不在这里设置）绑定到用户配置的
    // CPU 列表上。
    redisSetCpuAffinity(server.server_cpulist);
    makeThreadKillable();

    while(1) {
        // 忙轮询，100w 次循环，等待主线程分配 I/O 任务。
        for (int j = 0; j < 1000000; j++) {
            if (io_threads_pending[id] != 0) break;
        }

        // 如果 100w 次忙轮询之后如果还是没有任务分配给它，则通过尝试加锁进入休眠，
        // 等待主线程分配任务之后调用 startThreadedIO 解锁，唤醒 I/O 线程去执行。
        if (io_threads_pending[id] == 0) {
            pthread_mutex_lock(&io_threads_mutex[id]);
            pthread_mutex_unlock(&io_threads_mutex[id]);
            continue;
        }

        serverAssert(io_threads_pending[id] != 0);

        if (tio_debug) printf("[%ld] %d to handle\n", id, (int)listLength(io_threads_list[id]));


        // 注意：主线程分配任务给 I/O 线程之时，
        // 会把任务加入每个线程的本地任务队列 io_threads_list[id]，
        // 但是当 I/O 线程开始执行任务之后，主线程就不会再去访问这些任务队列，避免数据竞争。
        listIter li;
        listNode *ln;
        listRewind(io_threads_list[id],&li);
        while((ln = listNext(&li))) {
            client *c = listNodeValue(ln);
            // 如果当前是写出操作，则把 client 的写出缓冲区中的数据回写到客户端。
            if (io_threads_op == IO_THREADS_OP_WRITE) {
                writeToClient(c,0);
              // 如果当前是读取操作，则socket 读取客户端的请求命令并解析第一条命令。
            } else if (io_threads_op == IO_THREADS_OP_READ) {
                readQueryFromClient(c->conn);
            } else {
                serverPanic("io_threads_op value is unknown");
            }
        }
        listEmpty(io_threads_list[id]);
        // 所有任务执行完之后把自己的计数器置 0，主线程通过累加所有 I/O 线程的计数器
        // 判断是否所有 I/O 线程都已经完成工作。
        io_threads_pending[id] = 0;

        if (tio_debug) printf("[%ld] Done\n", id);
    }
}
```

I/O 线程启动之后，会先进入忙轮询，判断原子计数器中的任务数量，如果是非 0 则表示主线程已经给它分配了任务，开始执行任务，否则就一直忙轮询一百万次等待，忙轮询结束之后再查看计数器，如果还是 0，则尝试加本地锁，因为主线程在启动 I/O 线程之时就已经提前锁住了所有 I/O 线程的本地锁，因此 I/O 线程会进行休眠，等待主线程唤醒。

主线程会在每次事件循环中尝试调用 `startThreadedIO` 唤醒 I/O 线程去执行任务，如果接收到客户端请求命令，则 I/O 线程会被唤醒开始工作，根据主线程设置的 `io_threads_op` 标识去执行命令读取和解析或者回写响应数据的任务，I/O 线程在收到主线程通知之后，会遍历自己的本地任务队列 `io_threads_list[id]`，取出一个个 `client` 执行任务：

- 如果当前是写出操作，则调用 `writeToClient`，通过 socket 把 `client->buf` 或者 `client->reply` 里的响应数据回写到客户端。
- 如果当前是读取操作，则调用 `readQueryFromClient`，通过 socket 读取客户端命令，存入 `client->querybuf`，然后调用 `processInputBuffer` 去解析命令，这里最终只会解析到第一条命令，然后就结束，不会去执行命令。
- 在全部任务执行完之后把自己的原子计数器置 0，以告知主线程自己已经完成了工作。

```
void processInputBuffer(client *c) {
// 省略代码
...

    while(c->qb_pos < sdslen(c->querybuf)) {
        /* Return if clients are paused. */
        if (!(c->flags & CLIENT_SLAVE) && clientsArePaused()) break;

        /* Immediately abort if the client is in the middle of something. */
        if (c->flags & CLIENT_BLOCKED) break;

        /* Don't process more buffers from clients that have already pending
         * commands to execute in c->argv. */
        if (c->flags & CLIENT_PENDING_COMMAND) break;
        /* Multibulk processing could see a <= 0 length. */
        if (c->argc == 0) {
            resetClient(c);
        } else {
            // 判断 client 是否具有 CLIENT_PENDING_READ 标识，如果是处于多线程 I/O 的模式下，
            // 那么此前已经在 readQueryFromClient -> postponeClientRead 中为 client 打上该标识，
            // 则立刻跳出循环结束，此时第一条命令已经解析完成，但是不执行命令。
            if (c->flags & CLIENT_PENDING_READ) {
                c->flags |= CLIENT_PENDING_COMMAND;
                break;
            }

            // 执行客户端命令
            if (processCommandAndResetClient(c) == C_ERR) {
                /* If the client is no longer valid, we avoid exiting this
                 * loop and trimming the client buffer later. So we return
                 * ASAP in that case. */
                return;
            }
        }
    }

...
}
```

这里需要额外关注 I/O 线程初次启动时会设置当前线程的 CPU 亲和性，也就是绑定当前线程到用户配置的 CPU 上，在启动 Redis 服务器主线程的时候同样会设置 CPU 亲和性，Redis 的核心网络模型引入多线程之后，加上之前的多线程异步任务、多进程（BGSAVE、AOF、BIO、Sentinel 脚本任务等），Redis 现如今的系统并发度已经很大了，而 Redis 本身又是一个对吞吐量和延迟极度敏感的系统，所以用户需要 Redis 对 CPU 资源有更细粒度的控制，这里主要考虑的是两方面：CPU 高速缓存和 NUMA 架构。

首先是 CPU 高速缓存（这里讨论的是 L1 Cache 和 L2 Cache 都集成在 CPU 中的硬件架构），这里想象一种场景：Redis 主进程正在 CPU-1 上运行，给客户端提供数据服务，此时 Redis 启动了子进程进行数据持久化（BGSAVE 或者 AOF），系统调度之后子进程抢占了主进程的 CPU-1，主进程被调度到 CPU-2 上去运行，导致之前 CPU-1 的高速缓存里的相关指令和数据被汰换掉，CPU-2 需要重新加载指令和数据到自己的本地高速缓存里，浪费 CPU 资源，降低性能。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRIXxGMfcYjfnic2crvbVO6SZiaGCibLNIiaX6Cld5QNZQibCgw4mlQFNT0eA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

因此，Redis 通过设置 CPU 亲和性，可以将主进程/线程和子进程/线程绑定到不同的核隔离开来，使之互不干扰，能有效地提升系统性能。

其次是基于 NUMA 架构的考虑，在 NUMA 体系下，内存控制器芯片被集成到处理器内部，形成 CPU 本地内存，访问本地内存只需通过内存通道而无需经过系统总线，访问时延大大降低，而多个处理器之间通过 QPI 数据链路互联，跨 NUMA 节点的内存访问开销远大于本地内存的访问：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRoERwicIEI7HcLazMZnWibb1PEichu4ddOfospnUjrO0QkTV8egOzVt0MQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

因此，Redis 通过设置 CPU 亲和性，让主进程/线程尽可能在固定的 NUMA 节点上的 CPU 上运行，更多地使用本地内存而不需要跨节点访问数据，同样也能大大地提升性能。

关于 NUMA 相关知识请读者自行查阅，篇幅所限这里就不再展开，以后有时间我再单独写一篇文章介绍。

最后还有一点，阅读过源码的读者可能会有疑问，Redis 的多线程模式下，似乎并没有对数据进行锁保护，事实上 Redis 的多线程模型是全程无锁（Lock-free）的，这是通过原子操作+交错访问来实现的，主线程和 I/O 线程之间共享的变量有三个：`io_threads_pending` 计数器、`io_threads_op` I/O 标识符和 `io_threads_list` 线程本地任务队列。

`io_threads_pending` 是原子变量，不需要加锁保护，`io_threads_op` 和 `io_threads_list` 这两个变量则是通过控制主线程和 I/O 线程交错访问来规避共享数据竞争问题：I/O 线程启动之后会通过忙轮询和锁休眠等待主线程的信号，在这之前它不会去访问自己的本地任务队列 `io_threads_list[id]`，而主线程会在分配完所有任务到各个 I/O 线程的本地队列之后才去唤醒 I/O 线程开始工作，并且主线程之后在 I/O 线程运行期间只会访问自己的本地任务队列 `io_threads_list[0]` 而不会再去访问 I/O 线程的本地队列，这也就保证了主线程永远会在 I/O 线程之前访问 `io_threads_list` 并且之后不再访问，保证了交错访问。`io_threads_op` 同理，主线程会在唤醒 I/O 线程之前先设置好 `io_threads_op` 的值，并且在 I/O 线程运行期间不会再去访问这个变量。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRJBRrpB3m53MVSxWicicdVRVrmKbTbqiaj2UlnLyxBgdGct0qjDAUlGMvg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 5.3 性能提升

Redis 将核心网络模型改造成多线程模式追求的当然是最终性能上的提升，所以最终还是要以 benchmark 数据见真章：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRiaQSN3kHrmK0gzlT6Mt2AJAibfRVTy1wo7WUVYVWZYfMn9VzFkM8VxJA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

测试数据表明，Redis 在使用多线程模式之后性能大幅提升，达到了一倍。更详细的性能压测数据可以参阅这篇文章：[Benchmarking the experimental Redis Multi-Threaded I/O](https://itnext.io/benchmarking-the-experimental-redis-multi-threaded-i-o-1bb28b69a314)。

以下是美图技术团队实测的新旧 Redis 版本性能对比图，仅供参考：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRmiaMAFBWiaEJTp8LzYyPNKibsIbictEh6icgIVgODElCW0TkX9PgLDLZWAw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRoyRxiaUZEYmMNicQCEXtAicEC3Kgu0eLTA9NS4keQna51zrmmxicxJ1rKA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 5.4 模型缺陷

首先第一个就是我前面提到过的，Redis 的多线程网络模型实际上并不是一个标准的 Multi-Reactors/Master-Workers 模型，和其他主流的开源网络服务器的模式有所区别，最大的不同就是在标准的 Multi-Reactors/Master-Workers 模式下，Sub Reactors/Workers 会完成 `网络读 -> 数据解析 -> 命令执行 -> 网络写` 整套流程，Main Reactor/Master 只负责分派任务，而在 Redis 的多线程方案中，I/O 线程任务仅仅是通过 socket 读取客户端请求命令并解析，却没有真正去执行命令，所有客户端命令最后还需要回到主线程去执行，因此对多核的利用率并不算高，而且每次主线程都必须在分配完任务之后忙轮询等待所有 I/O 线程完成任务之后才能继续执行其他逻辑。

Redis 之所以如此设计它的多线程网络模型，我认为主要的原因是为了保持兼容性，因为以前 Redis 是单线程的，所有的客户端命令都是在单线程的事件循环里执行的，也因此 Redis 里所有的数据结构都是非线程安全的，现在引入多线程，如果按照标准的 Multi-Reactors/Master-Workers 模式来实现，则所有内置的数据结构都必须重构成线程安全的，这个工作量无疑是巨大且麻烦的。

所以，在我看来，Redis 目前的多线程方案更像是一个折中的选择：既保持了原系统的兼容性，又能利用多核提升 I/O 性能。

其次，目前 Redis 的多线程模型中，主线程和 I/O 线程的通信过于简单粗暴：忙轮询和锁，因为通过自旋忙轮询进行等待，导致 Redis 在启动的时候以及运行期间偶尔会有短暂的 CPU 空转引起的高占用率，而且这个通信机制的最终实现看起来非常不直观和不简洁，希望后面 Redis 能对目前的方案加以改进。

## 6.**总结**

Redis 作为缓存系统的事实标准，它的底层原理值得开发者去深入学习，Redis 自 2009 年发布第一版之后，其单线程网络模型的选择在社区中从未停止过讨论，多年来一直有呼声希望 Redis 能引入多线程从而利用多核优势，但是作者 antirez 是一个追求大道至简的开发者，对 Redis 加入任何新功能都异常谨慎，所以在 Redis 初版发布的十年后才最终将 Redis 的核心网络模型改造成多线程模式，这期间甚至诞生了一些 Redis 多线程的替代项目。虽然 antirez 一直在推迟多线程的方案，但却从未停止思考多线程的可行性，Redis 多线程网络模型的改造不是一朝一夕的事情，这其中牵扯到项目的方方面面，所以我们可以看到 Redis 的最终方案也并不完美，没有采用主流的多线程模式设计。

让我们来回顾一下 Redis 多线程网络模型的设计方案：

- 使用 I/O 线程实现网络 I/O 多线程化，I/O 线程只负责网络 I/O 和命令解析，不执行客户端命令。
- 利用原子操作+交错访问实现无锁的多线程模型。
- 通过设置 CPU 亲和性，隔离主进程和其他子进程，让多线程网络模型能发挥最大的性能。

通读本文之后，相信读者们应该能够了解到一个优秀的网络系统的实现所涉及到的计算机领域的各种技术：设计模式、网络 I/O、并发编程、操作系统底层，甚至是计算机硬件。另外还需要对项目迭代和重构的谨慎，对技术方案的深入思考，绝不仅仅是写好代码这一个难点。

## 7.参考&延伸阅读

- [Redis v5.0.10](https://github.com/redis/redis/tree/5.0.10)
- [Redis v6.0.10](https://github.com/redis/redis/tree/6.0.10)
- [Lazy Redis is better Redis](http://antirez.com/news/93)
- [An update about Redis developments in 2019](http://antirez.com/news/126)
- [How fast is Redis?](https://redis.io/topics/benchmarks)
- [Go netpoller 原生网络模型之源码全面揭秘](https://strikefreedom.top/go-netpoll-io-multiplexing-reactor)
- [Linux I/O 原理和 Zero-copy 技术全面揭秘](https://strikefreedom.top/linux-io-and-zero-copy)
- [Benchmarking the experimental Redis Multi-Threaded I/O](https://itnext.io/benchmarking-the-experimental-redis-multi-threaded-i-o-1bb28b69a314)
- [NUMA DEEP DIVE PART 1: FROM UMA TO NUMA](https://frankdenneman.nl/2016/07/07/numa-deep-dive-part-1-uma-numa/)

原文作者：作者：allanpan，腾讯 IEG 后台开发工程师，公众号：远赴星辰。

原文链接：https://mp.weixin.qq.com/s/-op5WR1wSkgAuP7JYZWP8g

# 【NO.232】网络 IO 演变发展过程和模型介绍

在互联网中提起网络，我们都会避免不了讨论高并发、百万连接。而此处的百万连接的实现，脱离不了网络 IO 的选择，因此本文作为一篇个人学习的笔记，特此进行记录一下整个网络 IO 的发展演变过程。以及目前广泛使用的网络模型。

### **1.网络 IO 的发展**

在本节内容中，我们将一步一步介绍网络 IO 的演变发展过程。介绍完发展过程后，再对网络 IO 中几组容易混淆的概念进行对比、分析。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HETu42nzJ6nvmrltaxMIlZJdXr2TaY9pPbZoSMASuG5NhCh4sQzIXPDA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 1.1 网络 IO 的各个发展阶段

通常，我们在此讨论的网络 IO 一般都是针对 linux 操作系统而言。网络 IO 的发展过程是随着 linux 的内核演变而变化，因此网络 IO 大致可以分为如下几个阶段：

**1. 阻塞 IO(BIO)**
**2. 非阻塞 IO(NIO)**
**3. IO 多路复用第一版(select/poll)**
**4. IO 多路复用第二版(epoll)**
**5. 异步 IO(AIO)**

而每一个阶段，都是因为当前的网络有一些缺陷，因此又在不断改进该缺陷。这是**网络 IO 一直演变过程中的本质**。下面将对上述几个阶段进行介绍，并对每个阶段的网络 IO 解决了哪些问题、优点、缺点进行剖析。

**1.2 网络的两个阶段**

在网络中，我们通常可以将其广义上划分为以下两个阶段：

**第一阶段：硬件接口到内核态**
**第二阶段：内核态到用户态**

本人理解：我们通常上网，大部分数据都是通过网线传递的。因此对于两台计算机而言，要进行网络通信，其数据都是先从应用程序传递到传输层(TCP/UDP)到达内核态，然后再到网络层、数据链路层、物理层，接着数据传递到硬件网卡，最后通过网络传输介质传递到对端机器的网卡，然后再一步一步数据从网卡传递到内核态，最后再拷贝到用户态。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEZDwwibVibDL8RCj0Vhia4xMMbO5FqvRcdIEicjYyjQSpBe4kanbfFbRrRg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 1.3 阻塞 IO 和非阻塞 IO 的区别

根据 1.2 节的内容，我们可以知道，网络中的数据传输从网络传输介质到达目的机器，需要如上两个阶段。此处我们把从**硬件到内核态**这一阶段，是否发生阻塞等待，可以将网络分为**阻塞 IO**和**非阻塞 IO**。如果用户发起了读写请求，但内核态数据还未准备就绪，该阶段不会阻塞用户操作，内核立马返回，则称为非阻塞 IO。如果该阶段一直阻塞用户操作。直到内核态数据准备就绪，才返回。这种方式称为阻塞 IO。

因此，区分阻塞 IO 和非阻塞 IO 主要看第一阶段是否阻塞用户操作。

#### 1.4 同步 IO 和异步 IO 的区别

从前面我们知道了，数据的传递需要两个阶段，在此处只要任何一个阶段会阻塞用户请求，都将其称为同步 IO，两个阶段都不阻塞，则称为异步 IO。

在目前所有的操作系统中，linux 中的 epoll、mac 的 kqueue 都属于同步 IO，因为其在第二阶段(数据从内核态到用户态)都会发生拷贝阻塞。而只有 windows 中的 IOCP 才真正属于异步 IO，即 AIO。

### **2.阻塞 IO**

在本节，我们将介绍最初的阻塞 IO，阻塞 IO 英文为 blocking IO，又称为 BIO。根据前面的介绍，阻塞 IO 主要指的是第一阶段(硬件网卡到内核态)。

#### 2.1 阻塞 IO 的概念

阻塞 IO，顾名思义当用户发生了系统调用后，如果数据未从网卡到达内核态，内核态数据未准备好，此时会一直阻塞。直到数据就绪，然后从内核态拷贝到用户态再返回。具体过程可以参考 2.2 的图示。

#### 2.2 阻塞 IO 的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEGibuyiacHXVXK7gdQ95jEUAwSuy3s6G5v61Tms5R368FXesWOhiajiaq4A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 2.3 阻塞 IO 的缺点

在一般使用阻塞 IO 时，都需要配置多线程来使用，最常见的模型是**阻塞 IO+多线程**，每个连接一个单独的线程进行处理。

**我们知道，一般一个程序可以开辟的线程是有限的，而且开辟线程的开销也是比较大的。也正是这种方式，会导致一个应用程序可以处理的客户端请求受限。面对百万连接的情况，是无法处理。**

既然发现了问题，分析了问题，那就得解决问题。既然阻塞 IO 有问题，本质是由于其阻塞导致的，因此自然而然引出了下面即将介绍的主角：**非阻塞 IO**

### **3.非阻塞 IO**

非阻塞 IO 是为了解决前面提到的阻塞 IO 的缺陷而引出的，下面我们将介绍非阻塞 IO 的过程。

#### 3.1 非阻塞 IO 的概念

非阻塞 IO：见名知意，就是在第一阶段(网卡-内核态)数据未到达时不等待，然后直接返回。因此非阻塞 IO 需要不断的用户发起请求，询问内核数据好了没，好了没。

#### 3.2 非阻塞 IO 的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HESdhj8nSKbRFuZ6U6GuoT5gnyttj1uic3YcC0LQy2kHC45s3jSC82aXA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

非阻塞 IO 是需要系统内核支持的，在创建了连接后，可以调用 setsockop 设置 noblocking

#### 3.3 非阻塞 IO 的优点

正如前面提到的，非阻塞 IO 解决了阻塞 IO**每个连接一个线程处理的问题**，所以其最大的优点就是 **一个线程可以处理多个连接**，这也是其非阻塞决定的。

#### 3.4 非阻塞 IO 的缺点

但这种模式，也有一个问题，就是需要用户多次发起系统调用。**频繁的系统调用**是比较消耗系统资源的。

因此，既然存在这样的问题，那么自然而然我们就需要解决该问题：**保留非阻塞 IO 的优点的前提下，减少系统调用**

### **4.IO 多路复用第一版**

为了解决非阻塞 IO 存在的频繁的系统调用这个问题，随着内核的发展，出现了 IO 多路复用模型。那么我们就需要搞懂几个问题：

1. IO 多路复用到底复用什么？
2. IO 多路复用如何复用？

**IO 多路复用：** 很多人都说，IO 多路复用是用一个线程来管理多个网络连接，但本人不太认可，因为在非阻塞 IO 时，就已经可以实现一个线程处理多个网络连接了，这个是由于其非阻塞而决定的。

**在此处，个人观点，多路复用主要复用的是通过有限次的系统调用来实现管理多个网络连接。最简单来说，我目前有 10 个连接，我可以通过一次系统调用将这 10 个连接都丢给内核，让内核告诉我，哪些连接上面数据准备好了，然后我再去读取每个就绪的连接上的数据。因此，IO 多路复用，复用的是系统调用。通过有限次系统调用判断海量连接是否数据准备好了**

**无论下面的 select、poll、epoll，其都是这种思想实现的，不过在实现上，select/poll 可以看做是第一版，而 epoll 是第二版**

#### 4.1IO 多路复用第一版的概念

**IO 多路复用第一版，这个概念是本人想出来的，主要是方便将 select/poll 和 epoll 进行区分**

所以此处 IO 多路复用第一版，主要特指 select 和 poll 这两个。

select 的 api

```
// readfds:关心读的fd集合；writefds：关心写的fd集合；excepttfds：异常的fd集合
int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

> select 函数监视的文件描述符分 3 类，分别是 writefds、readfds、和 exceptfds。调用后 select 函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有 except），或者超时（timeout 指定等待时间，如果立即返回设为 null 即可），函数返回。当 select 函数返回后，可以 通过遍历 fdset，来找到就绪的描述符。

> select 目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select 的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在 Linux 上一般为 1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。

poll 的 api

```
int poll (struct pollfd *fds, unsigned int nfds, int timeout);

struct pollfd {
    int fd; /* file descriptor */
    short events; /* requested events to watch */
    short revents; /* returned events witnessed */
};
```

> pollfd 结构包含了要监视的 event 和发生的 event，不再使用 select“参数-值”传递的方式。同时，pollfd 并没有最大数量限制（但是数量过大后性能也是会下降）。和 select 函数一样，poll 返回后，需要轮询 pollfd 来获取就绪的描述符。

> 从上面看，select 和 poll 都需要在返回后，通过遍历文件描述符来获取已经就绪的 socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。

**从本质来说：IO 多路复用中，select()/poll()/epoll_wait()这几个函数对应第一阶段；read()/recvfrom()对应第二阶段**

#### 4.2IO 多路复用第一版的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEbPaUbbrbibQsCw9xPib0FfZXfOKQ6OdekEicBSyr5micL6iaUAIQqRrDqAA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 4.3IO 多路复用第一版的优点

**IO 多路复用，主要在于复用，通过 select()或者 poll()将多个 socket fds 批量通过系统调用传递给内核，由内核进行循环遍历判断哪些 fd 上数据就绪了，然后将就绪的 readyfds 返回给用户。再由用户进行挨个遍历就绪好的 fd，读取或者写入数据。**

**所以通过 IO 多路复用+非阻塞 IO，一方面降低了系统调用次数，另一方面可以用极少的线程来处理多个网络连接。**

#### 4.4IO 多路复用第一版的缺点

虽然第一版 IO 多路复用解决了之前提到的频繁的系统调用次数，但同时引入了新的问题：**用户需要每次将海量的 socket fds 集合从用户态传递到内核态，让内核态去检测哪些网络连接数据就绪了**

**但这个地方会出现频繁的将海量 fd 集合从用户态传递到内核态，再从内核态拷贝到用户态。所以，这个地方开销也挺大。**

既然还有这个问题，那我们继续开始解决这个问题，因此就引出了第二版的 IO 多路复用。

**其实思路也挺简单，既然需要拷贝，那就想办法，不拷贝。既然不拷贝，那就在内核开辟一段区域咯**

#### 4.5IO 多路复用第一版的区别

**select 和 poll 的区别**

1. select 能处理的最大连接，默认是 1024 个，可以通过修改配置来改变，但终究是有限个；而 poll 理论上可以支持无限个
2. select 和 poll 在管理海量的连接时，会频繁的从用户态拷贝到内核态，比较消耗资源。

### **5.IO 多路复用第二版**

IO 多路复用第二版主要指 epoll，epoll 的出现也是随着内核版本迭代才诞生的，在网上到处看到，epoll 是内核 2.6 以后开始支持的

**epoll 的出现是为了解决前面提到的 IO 多路复用第一版的问题**

#### 5.1IO 多路复用第二版的概念

epoll 提供的 api

```
//创建epollFd，底层是在内核态分配一段区域，底层数据结构红黑树+双向链表
int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大

//往红黑树中增加、删除、更新管理的socket fd
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；

//这个api是用来在第一阶段阻塞，等待就绪的fd。
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
1. int epoll_create(int size);
创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。
当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。

2. int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；
函数是对指定描述符fd执行op操作。
- epfd：是epoll_create()的返回值。
- op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。
- fd：是需要监听的fd（文件描述符）
- epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下：

struct epoll_event {
  __uint32_t events;  /* Epoll events */
  epoll_data_t data;  /* User data variable */
};

//events可以是以下几个宏的集合：
EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
EPOLLOUT：表示对应的文件描述符可以写；
EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
EPOLLERR：表示对应的文件描述符发生错误；
EPOLLHUP：表示对应的文件描述符被挂断；
EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。
EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里

3. int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
等待epfd上的io事件，最多返回maxevents个事件。
参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。
```

二 工作模式

epoll 对文件描述符的操作有两种模式：LT（level trigger）和 ET（edge trigger）。LT 模式是默认模式，LT 模式与 ET 模式的区别如下：　　 LT 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用 epoll_wait 时，会再次响应应用程序并通知此事件。　　 ET 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用 epoll_wait 时，不会再次响应应用程序并通知此事件。

1. LT 模式

LT(level triggered)是缺省的工作方式，并且同时支持 block 和 no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的 fd 进行 IO 操作。如果你不作任何操作，内核还是会继续通知你的。

1. ET 模式

ET(edge-triggered)是高速工作方式，只支持 no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过 epoll 告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个 EWOULDBLOCK 错误）。但是请注意，如果一直不对这个 fd 作 IO 操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once)

ET 模式在很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。epoll 工作在 ET 模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

#### 5.2IO 多路复用第二版的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEbPaUbbrbibQsCw9xPib0FfZXfOKQ6OdekEicBSyr5micL6iaUAIQqRrDqAA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

当 epoll_wait()调用后会阻塞，然后完了当返回时，会返回了哪些 fd 的数据就绪了，用户只需要遍历就绪的 fd 进行读写即可。

#### 5.3IO 多路复用第二版的优点

**IO 多路复用第二版 epoll 的优点在于：**

一开始就在内核态分配了一段空间，来存放管理的 fd,所以在每次连接建立后，交给 epoll 管理时，需要将其添加到原先分配的空间中，后面再管理时就不需要频繁的从用户态拷贝管理的 fd 集合。通通过这种方式大大的提升了性能。

所以现在的 IO 多路复用主要指 epoll

#### 5.4IO 多路复用第二版的缺点

**个人猜想：** 如何降低占用的空间

### **6.异步 IO**

#### 6.1 异步 IO 的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEXGYke8ibPkbXEkaQ861CSg8YvQ3qFdvGcXicAicyrD9Vaicx77liakCLBnA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

前面介绍的所有网络 IO 都是同步 IO，因为当数据在内核态就绪时，在内核态拷贝用用户态的过程中，仍然会有短暂时间的阻塞等待。而异步 IO 指：**内核态拷贝数据到用户态这种方式也是交给系统线程来实现，不由用户线程完成**，目前只有 windows 系统的 IOCP 是属于异步 IO。

### **7.网络 IO 各种模型**

#### 7.1 reactor 模型

目前 reactor 模型有以下几种实现方案：

**1. 单 reactor 单线程模型**
**2. 单 reactor 多线程模型**
**3. multi-reactor 多线程模型**
**4. multi-reactor 多进程模型**

> 下文网络模型的图，均摘自[这篇文章](https://zhuanlan.zhihu.com/p/95662364)

##### **7.1.1 单 reactor 单线程模型**

此种模型，通常是只有一个 epoll 对象，所有的**接收客户端连接**、**客户端读取**、**客户端写入**操作都包含在一个线程内。该种模型也有一些中间件在用，比如 redis

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEibnlbqiclVmzSkNpIym8ibYaXOAAnn2cZSibDr7fiaHXGjypWk0iacO7kkdA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> 但在目前的单线程 Reactor 模式中，不仅 I/O 操作在该 Reactor 线程上，连非 I/O 的业务操作也在该线程上进行处理了，这可能会大大延迟 I/O 请求的响应。所以我们应该将非 I/O 的业务逻辑操作从 Reactor 线程上卸载，以此来加速 Reactor 线程对 I/O 请求的响应。

##### **7.1.2 单 reactor 多线程模型**

该模型主要是通过将，前面的模型进行改造，将读写的业务逻辑交给具体的线程池来实现，这样可以显示 reactor 线程对 IO 的响应，以此提升系统性能![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEvzbIZnWXH0HUWLgUZTRzHGzNsRK0F2JC0bxHmia6XG02wocUUvianoIQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> 在工作者线程池模式中，虽然非 I/O 操作交给了线程池来处理，但是所有的 I/O 操作依然由 Reactor 单线程执行，在高负载、高并发或大数据量的应用场景，依然较容易成为瓶颈。所以，对于 Reactor 的优化，又产生出下面的多线程模式。

##### **7.1.3 multi-reactor 多线程模型**

在这种模型中，主要分为两个部分：mainReactor、subReactors。mainReactor 主要负责接收客户端的连接，然后将建立的客户端连接通过负载均衡的方式分发给 subReactors，

subReactors 来负责具体的每个连接的读写

对于非 IO 的操作，依然交给工作线程池去做，对逻辑进行解耦

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEJ1BVQvZoIJB0xqBLxNvCOcOWkTknoMP4t1XkUgHfsoN7J2jzClJuaw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> mainReactor 对应 Netty 中配置的 BossGroup 线程组，主要负责接受客户端连接的建立。一般只暴露一个服务端口，BossGroup 线程组一般一个线程工作即可 subReactor 对应 Netty 中配置的 WorkerGroup 线程组，BossGroup 线程组接受并建立完客户端的连接后，将网络 socket 转交给 WorkerGroup 线程组，然后在 WorkerGroup 线程组内选择一个线程，进行 I/O 的处理。WorkerGroup 线程组主要处理 I/O，一般设置 2*CPU 核数个线程

#### 7.2 proactor 模型

proactor 主要是通过对异步 IO 的封装的一种模型，它需要底层操作系统的支持，目前只有 windows 的 IOCP 支持的比较好。详细的介绍可以参考[这篇文章](https://zhuanlan.zhihu.com/p/95662364)

#### 7.3 主流的中间件所采用的网络模型

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEiaTnSvliaMcYCEDmUiaDlbRQTlURhaKHY8N0qwsgLuXZpkOgeJ3UtXMlQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 7.4 主流网络框架

- netty
- gnet
- libevent
- evio(golang)
- ACE(c++)
- boost::asio(c++)
- muduo （linux only)

关于c++和c的上述几个库对比，感兴趣的话大家可以自行搜索资料或者阅读[这篇文章](https://www.cnblogs.com/leijiangtao/p/5197566.html)。



### **8.参考资料**

1. [IO 模式和 IO 多路复用](https://juejin.im/post/5bf7b89e518825369c564059)
2. [Linux IO 模式及 select、poll、epoll 详解](https://segmentfault.com/a/1190000003063859)
3. [Chapter 6. I/O Multiplexing: The select and poll Functions](http://www.masterraghu.com/subjects/np/introduction/unix_network_programming_v1.3/ch06.html)
4. [高性能 IO 模型分析-Reactor 模式和 Proactor 模式（二）](https://zhuanlan.zhihu.com/p/95662364)

原文作者：jaydenwen，腾讯 pcg 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/EDzFOo3gcivOe_RgipkTkQ

# 【NO.233】编译原理初学者入门指南

### **1.引子**

最近的工作需要用表达式做一些参数的配置，然后发现大脑一片空白，在 Google 里试了几个关键词（起初搜了下“符号引擎”，发现根本不是我想要的）之后，明白过来自己应该是需要补一些编译原理的知识了。在掉了两晚上头发之后，决定整理一下自己的知识网络。

要解析的表达式大概长这个样子：

```
avg(teams[*].players.attributes[skill])*rules[latency].maxLatency
```

正则表达式是个办法，但不是最优解，除了很难通过一句正则解析整条语句外，以后扩展更多语法时，正则表达式的修改也分外麻烦。

作为非计算机科班出身的工程师，还是知道一点，任何领域发展出的课题都需要产、学、研三者的配合才能完成，“产” 指企业，“学” 指高等院校，“研” 指科研机构。

其实自打 C 语言从贝尔实验室走出来之后，“研” 究这一步就已经完成大半了，于是被下放到 “学” 校，有了《编译原理》这门课程，最后流入 “产” 业中大规模应用。

所以这篇文章主要从两方面给初学者（尤其是跟我一样非科班出身的 coder）一个指南：

- 在科学原理上，通俗的解释一些专有名词，厘清基本概念——编译原理这块的术语简直太多了，多到糊脸的那种；
- 在工程实践上，给到一个可行的实现方法，主要是面向 golang 的 `goyacc`，如果本文有幸被你搜索到，你肯定最想看这一部分（现网关于 goyacc 的中文资料太少了）。

### **2.理论原理**

以下内容均为个人理解，欢迎探讨，如有不精确之处，以教科书为准～

#### 2.1 计算机语言是怎么回事儿

编译器由词法分析、语法分析、语义检查再到中间表示输出和最后二进制生成的流程，这些已经可以作为前置知识，就不提了。

随手打开一个工程，我们就能发现形形色色的语言文件，比如 yaml 格式的服务配置文件、json 格式的工程配置文件、js 和 go 等源代码文件等。忽略掉他们繁杂的用途，按其表达能力，可以分为两种：

- DSL（Domain Specific Language）：特定领域语言，比如用来描述数据的 json、用来查询数据的 sql、标记型的 xml 和 html，都属于面向特定领域的专用语言，用在正确的领域上就是利器，用错地方就是自找麻烦（比如用 sql 来一段冒泡排序）；
- GPL（General Purpose Language）：通用用途语言，比如 C、JavaScript、Golang，这类语言是 **图灵完备** 的，你可以用一门 GPL 语言去设计和实现一种 DSL 语言。

歪个楼，这里有一个吊诡的事实，yaml 竟然是图灵完备的！甚至很多语言都需要特别使用 safe_load 来加载 yaml 文件，比如用 java 直接 load 这段 yaml，会执行一次 HTTP 请求。

```
!!javax.script.ScriptEngineManager [!!java.net.URLClassLoader [[!!java.net.URL [\"http://localhost\"]]]]
```

不管是为特定领域而发明的各类 DSL，还是图灵完备的 GPL 语言，他们基本都符合 BNF（**巴科斯范式**）。

BNF 是一种 **上下文无关文法**，举个例子就是，人类的语言就是一种 上下文**有关**文法，我随时都可以讲一句 “以上说的都是废话”，戏弄一下读者阅读本文所花的时间（每当回忆起来，我都会坐在轮椅上大呼过瘾）。

关于 BNF 具体定义，这里摘抄一下维基百科，后面做详细解说：

> **BNF** 规定是[推导规则](https://zh.wikipedia.org/w/index.php?title=推导规则&action=edit&redlink=1)(产生式)的集合，写为：
>
> <符号> ::= <使用符号的表达式>
>
> 这里的 <符号> 是[非终结符](https://zh.wikipedia.org/wiki/非终结符)，而[表达式](https://zh.wikipedia.org/wiki/表达式)由一个符号序列，或用指示[选择](https://zh.wikipedia.org/wiki/选择)的[竖杠](https://zh.wikipedia.org/w/index.php?title=竖杠&action=edit&redlink=1) '|' 分隔的多个符号序列构成，每个符号序列整体都是左端的符号的一种可能的[替代](https://zh.wikipedia.org/w/index.php?title=替代&action=edit&redlink=1)。从未在左端出现的符号叫做[终结符](https://zh.wikipedia.org/wiki/终结符)。

暂且不用理解里面提到的 “终结符” 和 “非终结符”，在明白来龙去脉之前去查这些，说不定大脑会 stackoverflow。但是也别慌，所有术语和英文缩写都是纸老虎，其实他们都是很简单的概念，但是你需要一个合适的场景来理解它们起到的作用。

#### 2.2 学科交叉：自然语言理解

上节我们说到，计算机语言多数是符合 BNF 的上下文无关语言，从表达能力上分为 DSL 和 GPL 两类；而人类语言属于上下文有关语言，其实正是由于这一点，才给在 NLP（自然语言理解）领域带来了不少困难。

好，知道了这些英文缩写，再去读那些专业文章会简单得多。

这些其实都是在 **静态层面** 上对语言的描述，为了实际执行这些语言，就需要对其进行解析，还原出语言本身所描述的信息结构。这件事，在计算机领域的课程叫《编译原理》，在智能科学与技术的课程叫《自然语言理解》。

- 编译原理（一张图）：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat76PRwqySU55JKQXm8SCibEMlFTeuJ3QnaYteoVBdsknaZHriaCffRPuic96G82z9It9pzLZRfIFZ9A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)编译原理

- 自然语言理解（两张图）：

  ![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat76PRwqySU55JKQXm8SCibEEAnMCOFDeiceOKhib7CI7LZ19CZMAl6gvm2WOScgFsusb6thOhKibqZicw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat76PRwqySU55JKQXm8SCibELQhLnkJaocT1mHh2ic9523I7ziaHOzOzH52Fpuuk8uJvOu6VzUU6mnBQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)NLP

不难看出，两者的流程惊人的相似：

- 都需要先进行 tokenize 处理，编译器做的是**词法分析**（常用工具搜 lexer），NLP 做的是 分词（最常见的是 jieba 分词）
- 词法分析的产物是有含义的 token，下面都需要进行**语法分析**（即 parser），NLP 里通常会做 向量化（最常见的是 word2vec 方法）
- 这两步完成后，编译器前端得到的产物是 AST（Abstract Syntax Tree，抽象语法树），NLP 得到的产物是一段话的向量化表示

两者的共同点止步于此，鉴于 NLP 技术仍在高速发展（而编译原理早就是老生常谈了），向量化得到的产物难以处理同义词，所以后面的步骤也局限于分析一句话的意图、和提取有效信息（利用这些可以做一个简陋版的 Siri）。最新（其实是两年前了）的进展是 BERT 模型和衍生出来的许多研究上下文关系的方法，现在的 NLP 技术已经可以做阅读理解问题了。

此外，DSL 和 GPL 的共同点也止步于此。要记得，DSL 是面向特定用途的语言，以 JSON 为例，得到 AST 就已经有完整的信息结构了，在面向对象语言里无非再多一步：利用反射将其映射到一个 class 的所有字段里；以 HTML 为例，得到 AST 就已经有完整的 DOM 树了，浏览器已经具备渲染出整个网页所需的大部分信息。

最后，对 GPL 语言来说，编译型语言目的是生成机器可执行的代码，解释型语言的目的是生成虚拟机认识的中间代码。这部分职责由编译器后端承担，现代编译器领域的最佳拍档就是 Clang + LLVM。

#### 2.3 别慌：英文缩写都是纸老虎

现在我们知道了事情的来龙去脉，也就明白了开头的需求属于哪种问题。对工程师来说，解决问题的第一步就是先知道你面对的是什么问题：使用编译原理的知识来解析开头的表达式，相当于定义一个简陋的 DSL 语言，并编写词法解析器和语法解析器（lexer & parser）来将其转换成 AST（抽象语法树），进而对其进行处理。

在进行工程实践之前，还有些术语不得不先行了解。

首先是前面提到的终结符和非终结符，重复一下上面解释 BNF 时举的抽象表达式：`<符号> ::= <使用符号的表达式>`。可以这样来理解：

- 由词法解析器生成的符号，也叫 token，是终结符。终结符是最小表义单位，无法继续进行拆解和解析
- 规则左侧定义的符号，是非终结符。非终结符需要进行语法解析，最终由终结符构成其表示形式

其次是 NFA 和 DFA，FA 表示 Finite Automata（有穷状态机），即根据不同的输入来转换内部状态，其内部状态是有限个数的。而 NFA 和 DFA 分别代表 有穷不确定状态机 和 有穷确定状态机。运用子集构造法可以将 NFA 转换为 DFA，让构造得到的 DFA 的每个状态对应于 NFA 的一个状态的集合。

词法分析器（lexer）生成终结符，而语法分析器（parser）则利用自顶向下或自底向上的方法，利用文法中定义的终结符和非终结符，将输入信息转换为 AST（抽象语法树）。也就是我们在此次需求中需要获得的东西。

### **3.工程实践**

我们的案例是使用 golang 来编写 lexer 和 parser。

在工程上，不同语言的实践方式是不一样的。你可以选择自己编写 lexer 和 parser，也可以选择通过定义 yacc 文件的方式让工具自动生成。在参考文献中会给出自己编写它们的相关文章，在 golang 的案例里，lexer 需要自己编写，而 parser 则由工具生成。

如果使用 Antlr 的话，可以将 lexer 和 parser 一同搞定，用得好的话，可以实现诸如像 JS 和 Swift 语言互相转换的特技。不在本文实践范围内。

#### 3.1 goyacc 的安装

Golang 1.8 之前自带 goyacc 工具，由于使用量太少，之后版本就需要手动安装了。

```
go get -u github.com/golang/tools/tree/master/cmd/goyacc
```

使用起来参数如下：![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat76PRwqySU55JKQXm8SCibEYXWTuhPsIS4rKDYnwa5eG5bkS9vrKcMOrhjUtOAOThulPTuDJkuCpw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

然后我们需要搞定词法分析器和语法分析器。

#### 3.2 使用 goyacc 的思路

yacc 类工具的共同特点就是，通过编写 .y 格式的说明文件定义语法，然后使用 yacc 命令行工具生成对应语言的源代码。

所以尝起来就比较像 protobuf，proto 文件就像 .y 文件一样本身不可执行，需要用一些 protogen 工具来生成对应每种语言的源代码文件。

在 goyacc 中，lexer 本身相对简单，自己编写 go 代码实现就够了，parser 部分所需的文法约定，需要我们编写 .y 文件，也就需要了解 yacc 的文法约定。goyacc 会在生成好的 go 源代码中提供 `yyParse` 、`yyText` 、`yyLex` 等接口，最后我们自己编写调用 parser 的文件，就能把流程跑起来了。

我们的目的，就是给定如下示例输入，然后输出能代表 AST 的数据结构：

```
# 示例输入
avg(teams[*].maxPlayers) *flatten(rules[red].players.playerAttributes[exp])

# 示例输出
parsed obj: [map[avg:[map[teams:*] map[maxPlayers:]]] map[flatten:[map[rules:red] map[players:] map[playerAttributes:exp]] last_operator:*]]

[
    {
        "avg": [
            {
                "teams": "*"
            },
            {
                "maxPlayers": ""
            }
        ]
    },
    {
        "flatten": [
            {
                "rules": "red"
            },
            {
                "players": ""
            },
            {
                "playerAttributes": "exp"
            }
        ],
        "last_operator": "*"
    }
]
```

#### 3.3 词法分析器

lexer 我们选择自己用 golang 编写。lexer 的基本功能是通过一个 buffer reader 不断读取文本，然后告诉 goyacc 遇到的是什么符号。

Lex 函数的返回值类型（即词法分析器的实际产物）需要在后面的 yacc 文件的 token 部分定义。

为了与 goyacc 结合，我们需要定义和实现以下接口：

```
type Scanner struct {
 buf   *bufio.Reader
 data  interface{}
 err   error
 debug bool
}

func NewScanner(r io.Reader, debug bool) *Scanner {
 return &Scanner{
  buf:   bufio.NewReader(r),
  debug: debug,
 }
}

func (sc *Scanner) Error(s string) {
 fmt.Printf("syntax error: %s\n", s)
}

func (sc *Scanner) Reduced(rule, state int, lval *yySymType) bool {
 if sc.debug {
  fmt.Printf("rule: %v; state %v; lval: %v\n", rule, state, lval)
 }
 return false
}

func (s *Scanner) Lex(lval *yySymType) int {
 return s.lex(lval)
}
```

我们可以定义私有函数完成 lex 的实际工作。

#### 3.4 语法分析器

上节我们有说，yacc 文件最终会生成 go 源代码文件，里面包含了 `yyParse` 、`yyText` 、`yyLex` 等接口的具体实现。

而 yacc 只包含定义文法的语法，不含各类编程语言的语法，所以聪明的你肯定能猜到，yacc 文件中免不了会出现类似宏定义的东西，会直接嵌入各类编程语言的代码片段。

有了这个心理预期，我们看一下 yacc 文件的结构：

```
{%
嵌入代码
%}
文法定义
%%
文法规则
%%
嵌入代码 （golang代码，通常忽略此部分直接在写在代码头中）
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat76PRwqySU55JKQXm8SCibEOicwmjnLfX5Qf9c8T1xZW3RJ5Jtogbez1qV5ZF1OicezSrGufEZHmicqg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

其文法定义如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat76PRwqySU55JKQXm8SCibEUdgAia8TTWjzFCA7s1HBfwL2aCKdodVDZWpJGcRaFbKw5ls5TwbkScQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

我们自己编写 yacc 实现 parser，最少需要知道的就是前面四种描述符。一开始我们只实现最简单的语法规则，后面自己就会逐渐了解更高级的文法规则了。

#### 3.5 参考工程

goyacc 的示例工程不多，不推荐用 yacc 实现计算器的例子，参考性比较差。

如下工程实现了用 golang 解析 JSON 数据，只需要补一个 go.mod 和 main 函数就能拿来边调试边参考着实现自己的需求了，十分推荐：https://github.com/sjjian/yacc-examples

```
module example.com/m

go 1.14

require github.com/pkg/errors v0.9.1
package main

import (
 "encoding/json"
 "fmt"
 "io/ioutil"

 "example.com/m/yacc_parseJson"
)

func check(e error) {
 if e != nil {
  panic(e)
 }
}

func main() {
 dat, err := ioutil.ReadFile("json.txt")
 check(err)
 fmt.Printf("raw str: %s\n", string(dat))
 jsonobj, err := yacc_parseJson.ParseJson(string(dat), true)
 fmt.Printf("parsed obj: %+v\n", jsonobj)
 jsonStr, _ := json.Marshal(jsonobj)
 fmt.Printf(string(jsonStr))
}
```

### **4.参考文献**

- [编译原理（基础篇）](https://www.cnblogs.com/antispam/p/4015116.html)
- [golang 实现自定义语言的基础](https://www.1thx.com/golang/189.html)
- [什么是 NFA 和 DFA](https://www.cnblogs.com/AndyEvans/p/10240790.html)
- [从 antlr 扯淡到一点点编译原理](https://awhisper.github.io/2016/11/18/从antlr到语法解析/)
- [How to Write a Parser in Go](https://about.sourcegraph.com/go/gophercon-2018-how-to-write-a-parser-in-go/)

原文作者：pixelcao，腾讯 IEG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/ZTxVG6KG-4vzbvclC_Q1LQ

# 【NO.234】分布式之系统底层原理

## 0.**导言**

分布式事务是分布式系统必不可少的组成部分，基本上只要实现一个分布式系统就逃不开对分布式事务的支持。本文从分布式事务这个概念切入，尝试对分布式事务以及分布式系统最核心的底层原理逐一进行剖析，内容包括但不限于 **BASE 原则**、**两阶段原子提交协议**、**三阶段原子提交协议**、**Paxos/Multi-Paxos 分布式共识算法的原理与证明**、**Raft 分布式共识算法**和**分布式事务的并发控制**等内容。

## 1.**事务**

*事务***是访问并可能更新各种数据项的一个程序执行**单元**(unit)。事务由一个或多个步骤组成，一般使用形如 `begin transaction` 和 `end transaction` 语句或者函数调用作为事务界限，事务内的所有步骤必须作为一个单一的、不可分割的单元去执行，因此事务的结果只有两种：1. 全部步骤都执行完成，2. 任一步骤执行失败则整个事务回滚。

事务最早由数据库管理系统(**database management system**，**DBMS**)引入并实现，**数据库事务**是数据库管理系统执行过程中的一个逻辑单位，由一个有限的数据库操作序列构成。数据库事务严格遵循 `ACID` 原则，属于刚性事务，一开始数据库事务仅限于对单一数据库资源对象的访问控制，这一类事务称之为**本地事务** (Local Transaction)，后来随着分布式系统的出现，数据的存储也不可避免地走向了分布式，**分布式事务**（Distributed Transaction）便应运而生。

### 1.1 刚性事务

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHrc8fz9RIZ4wzkT2ficdvrc6UZnnRo9JbaLAdyibHy23rxhCarVap9w3A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

刚性事务（如单一数据库事务）完全遵循 `ACID` 规范，即数据库事务的四大基本特性：

- Atomicity（原子性）：一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。
- Consistency（一致性）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。
- Isolation（隔离性）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。
- Durability（持久性）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

**刚性事务也能够以分布式 CAP 理论中的 CP 事务来作为定义**。

### 1.2 柔性事务

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHOKj6cPboPgbBzMiaT858taVoDroSboCFx8iclwStbiaB83Ezb6cfYFwXA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

在电商领域等互联网场景下，传统的事务在数据库性能和处理能力上都遇到了瓶颈。因此，柔性事务被提了出来，柔性事务基于分布式 `CAP` 理论以及延伸出来的 `BASE` 理论，相较于数据库事务这一类完全遵循 `ACID` 的刚性事务来说，柔性事务保证的是 “基本可用，最终一致”，`CAP` 原理相信大家都很熟悉了，这里我们讲一下 `BASE` 原则：

- 基本可用（**B**asically **A**vailable）：系统能够基本运行、一直提供服务。
- 软状态（**S**oft-state）：系统不要求一直保持强一致状态。
- 最终一致性（**E**ventual consistency）：系统需要在某一时刻后达到一致性要求。

柔性事务（如分布式事务）为了满足可用性、性能与降级服务的需要，降低一致性（Consistency）与隔离性（Isolation）的要求，遵循 `BASE` 理论，传统的 `ACID` 事务对隔离性的要求非常高，在事务执行过程中，必须将所有的资源对象锁定，因此对并发事务的执行极度不友好，柔性事务（比如分布式事务）的理念则是将锁资源对象操作从本地资源对象层面上移至业务逻辑层面，再通过放宽对强一致性要求，以换取系统吞吐量的提升。

此外，虽然柔性事务遵循的是 `BASE` 理论，但是还需要遵循部分 `ACID` 规范：

- 原子性：严格遵循。
- 一致性：事务完成后的一致性严格遵循；事务中的一致性可适当放宽。
- 隔离性：并行事务间不可影响；事务中间结果可见性允许安全放宽。
- 持久性：严格遵循。

## 2.**本地事务**

**本地事务**（Local Transaction）指的是仅仅对单一节点/数据库资源对象进行访问/更新的事务，在这种事务模式下，`BASE` 理论派不上用场，事务完全遵循 `ACID` 规范，确保事务为刚性事务。

## 3.**分布式事务**

在分布式架构成为主流的当下，系统对资源对象的访问不能还局限于单节点，多服务器、多节点的资源对象访问成为刚需，因此，本地事务无法满足分布式架构的系统的要求，分布式事务应运而生。

访问/更新由多个服务器管理的资源对象的**平面事务**或者**嵌套事务**称之为**分布式事务**（Distributed Transaction），分布式事务是相对于本地事务来说的。

平面事务：单一事务，访问多个服务器节点的资源对象，一个平面事务完成一次请求之后才能发起下一个请求。

嵌套事务：多事务组成，顶层事务可以不断创建子事务，子事务又可以进一步地以任意深度嵌套子事务。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHAGOj7Tk0la8E5cpOt8OiarZMqUKCVDfKF1Ic6XcxibrR77Z1sDEw0UFw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

对于分布式事务来说，有两个最核心的问题：

1. 如何管理分布式事务的提交/放弃决定？如果事务中的一个节点在执行自己的本地事务过程中遇到错误，希望放弃整个分布式事务，与此同时其他节点则在事务执行过程中一切顺利，希望提交这个分布式事务，此时我们应该如何做决策？
2. 如何保证并发事务在涉及多个节点上资源对象访问的可串行性（规避分布式死锁）？如果事务 T 对某一个服务器节点上的资源对象 S 的并发访问在事务 U 之前，那么我们需要保证在所有服务器节点上对 S 和其他资源对象的冲突访问，T 始终在 U 之前。

问题 1 的解决需要引入一类分布式原子提交协议的算法如两阶段提交协议等，来对分布式事务过程中的提交或放弃决策进行管理，并确保分布式提交的原子性。而问题 2 则由分布式事务的并发控制机制来处理。

## 4.**原子提交协议**

> 原子性是分布式事务的前置性约束，没有原子性则分布式事务毫无意义。

原子性约束要求在分布式事务结束之时，它的所有操作要么全部执行，要么全部不执行。以分布式事务的原子性来分析，客户端请求访问/更新多个服务器节点上的资源对象，在客户端提交或放弃该事务从而结束事务之后，多个服务器节点的最终状态要么是该事务里的所有步骤都执行成功之后的状态，要么恢复到事务开始前的状态，不存在中间状态。满足这种约束的分布式事务协议则称之为原子提交协议。

当一个分布式事务结束时，事务的原子特性要求所有参与该事务的服务器节点必须全部提交或者全部放弃该事务，为了实现这一点，必须引入一个协调者（Coordinator）的角色，从参与事务的所有服务器节点中挑选一个作为协调者，由它来保证在所有服务器节点上最终获得同样的结果。协调者的工作原理取决于分布式事务选用的协议。

一般来说，分布式事务中包含的两个最基础的角色就是：

- Coordinator -- 协调者
- Participants -- 参与者

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHCF2nPzswAHPZk7dvsQAqOffw4KhaoxuRFjib20oaFcO6UhABQ5zibL7g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 4.1 单阶段原子提交协议

**单阶段原子提交协议**（one-phase atomic commit protocol, 1APC）是最简单的一种原子提交协议，它通过设置一个协调者并让它不断地向所有参与者发送提交（commit）或放弃（abort）事务的请求，直到所有参与者确认已执行完相应的操作。

1APC 协议的优点是简单易用，对一些事务不复杂的场景比较合适，但在复杂事务场景则显得捉襟见肘，因为该协议不允许任何服务器节点单方面放弃事务，事务的放弃必须由协调者来发起，这个设计会导致很多问题：首先因为只有一次通信，协调者并不会收集所有参与者的本地事务执行的情况，所以协调者决定提交还是放弃事务只基于自己的判断，在参与者执行事务期间可能会遇到错误从而导致最终事务未能真正提交，错误一般与事务的并发控制有关，比如事务执行期间对资源对象加锁，遇到死锁，需要放弃事务从而解开死锁，而协调者并不知道，因此在发起下一个请求之前，客户端完全不知道事务已被放弃。另一种情况就是利用乐观并发控制机制访问资源对象，某一个服务器节点的验证失败将导致事务被放弃，而协调者完全不知情。

### 4.2 两阶段提交协议

#### 4.2.1 **定义**

**两阶段提交协议**（two-phase commit protocol, 2PC）的设计初衷是为了解决 1APC 不允许任意一个服务器节点自行放弃它自己的那部分本地事务的痛点，2PC 允许任何一个参与者自行决定要不要放弃它的本地事务，而由于原子提交协议的约束，任意一个本地事务被放弃将导致整个分布式事务也必须放弃掉。

两阶段提交协议基于以下几个假设：

- 存在一个节点作为协调者（Coordinator），分布式事务通常由协调者发起（当然也可以由参与者发起），其余节点作为参与者（Participants），且节点之间可以自由地进行网络通信，协调者负责启动两阶段提交流程以及决定事务最终是被提交还是放弃。
- 每个节点会记录该节点上的本地操作日志（op logs），日志必须持久化在可靠的存储设备上（比如磁盘），以便在节点重启之后需要恢复操作日志。另外，不记录全局操作日志。
- 所有节点不能发生永久性损坏，也就是说节点就算是损坏了也必须能通过可靠性存储恢复如初，不允许出现数据永久丢失的情况。
- 参与者对协调者的回复必须要去除掉那些受损和重复的消息。
- 整个集群不会出现拜占庭故障（Byzantine Fault）-- 服务器要么崩溃，要么服从其发送的消息。

#### 4.2.2 **原理**

两阶段提交协议，顾名思义整个过程需要分为两个阶段：

1. 准备阶段（Prepare Phase）
2. 提交阶段（Commit Phase）

在进行两阶段提交的过程中，协调者会在以下四种状态间流转：

1. `init`
2. `preparing`
3. `committed`
4. `aborted`

而参与者则会在以下三种状态间流转：

1. `working`
2. `prepared`
3. `committed`

**阶段 I**（投票表决阶段）

1. 任意一个参与者发起分布式事务 T 并执行本地事务成功，接着将一条 `<ready T>` 记录追加到本地日志 buffer 中并 flush 到可靠性存储设备如磁盘上，从 `working` 状态进入 `prepared` 状态，然后向协调者发送 `prepare T` 消息；
2. 收到参与者发来的 `prepare T` 消息后，协调者将一条 `<prepare T>` 记录追加到日志中，然后从 `init` 状态进入 `preparing` 状态，紧接着向分布式事务的其他参与者发出 `canCommit?` 消息，发起事务表决过程；
3. 当参与者收到 `canCommit?` 请求后，除了发起事务的那一个之外，其他还在 `working` 状态的参与者会先尝试执行本地事务，如果本地事务执行成功，则会往本地日志 buffer 写入一条 `<ready T>` 记录并 flush 到可靠性存储中，但不提交事务，进入 `prepared` 状态，然后回复一条 `ready T` 消息对此事务投 YES 票；如果本地事务执行失败，则参与者会往本地日志 buffer 写入一条 `<don't commit T>` 记录并 flush 到可靠性存储中，然后回复一条 `don't commit T` 消息投 NO 票。

**阶段 II**（收集投票结果完成事务）

1. 协调者收集所有的投票（包括它自己的投票）；

   (a) 如果所有的投票都是 `ready T`，则表示没有故障发生，那么协调者决定提交该事务，首先它会在其本地日志中追加一条 `<commit T>` 记录，从 `preparing` 状态进入 `committed` 状态，然后向所有的参与者发送 `doCommit` 请求消息，要求参与者提交它们的本地事务；

   (b) 如果有任一个投票是 No，则协调者决定放弃掉该事务，首先它会往本地日志中追加一条记录，从 `preparing` 状态进入 `aborted` 状态，然后发送 `doAbort` 请求消息给所有的参与者，通知它们回滚各自的本地事务。

2. 投了 YES 票的参与者阻塞等待协调者给它发来 `doCommit` 或 `doAbort` 消息，如果接收到的是 `doCommit` 消息则提交本地事务并在此过程中记录日志 `<commit T>`，然后进入 `committed` 状态，最后回复一个 `haveCommitted` 的消息通知协调者本地事务已经成功提交；反之，如果收到的是 `doAbort` 消息则回滚本地事务并写入日志 `<abort T>`，然后进入 `aborted`状态。

上面的过程是一种更通用的流程，即由任意的参与者发起一个分布式事务，而在实践中一般把分布式事务的发起交给协调者来做，减少事务发起者确认该事务已被提交所需等待的网络消息延迟：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHTJvfggFJykUI8V8z4Xywib3a0Sh12pufe0enQ1ydn1VEuiaYkHSLZW2g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### 4.2.3 **性能**

**网络 I/O 开销**

假设两阶段提交过程一切运行正常，即协调者和参与者都不出现崩溃和重启，网络通信也都正常。那么假设有一个协调者和 N 个参与者，两阶段提交过程中将会发送如下的消息：

- 任意一个参与者从 `working` 状态进入 `prepared` 状态并发送 `Prepared` 消息给协调者，1 条消息。
- 协调者收到消息后，向其他参与者发送 `canCommit?` 请求消息，N - 1 条消息。
- 收到 `canCommit?` 消息的参与者各自回复协调者投票消息，N - 1 条消息。
- 协调者统计投票情况之后，发送 `doCommit` 消息给其他参与者，N 条消息。

所以，事务发起者在经过 4 条网络消息延迟之后确认该分布式事务已被提交，而整个过程共计发送 3N - 1 条网络消息（因为 `haveCommitted` 在 2PC 仅仅是用于最后通知协调者而已，属于可有可无的一次网络消息，2PC 在该消息缺省的情况下也能正常运行，因此 `haveCommitted` 一般不计入网络延迟成本中）。

前面我们提到，在实践中一般是由协调者来发起事务，如果考虑这种情况的话，事务发起者 -- 协调者在经过 3 条网络消息延迟之后确认该分布式事务已经被提交，而整个过程实际发送的网络消息则变成 3N 条。

总而言之，两阶段提交协议的网络通信开销和集群节点的数量成 3 倍正比。

**本地存储设备 I/O 开销**

基于前文中叙述的两阶段提交协议的基本假设之一：每个节点会通过日志来记录在本地执行的操作，以便在节点发生故障并重启节点之后能利用日志恢复到故障前的状态，因此两阶段提交过程中除了网络 I/O 的开销之外，还有本地存储设备 I/O 的开销：

- 发起事务的参与者执行本地事务，1 次写操作。
- 其余参与者执行各自的本地事务，N - 1 次写操作。
- 协调者统计投票结果并决定提交事务，1 次写操作。

所以事务发起者在经过 3 次本地存储设备 I/O 延迟之后确认该事务已被提交，整个过程总计有 N + 1 次本地存储设备 I/O，而如果由协调者来发起事务的话，则还是需要 N + 1 次本地存储设备 I/O，但是只需要经过 2 次本地存储设备 I/O 延迟即可确认事务已被提交。

#### 4.2.4 **恢复**

在分布式事务中，所有的参与者节点都可能发生故障，所以我们需要保证在该故障节点恢复时发生的一切都和分布式事务 T 的全局决策保持一致。节点在恢复的时候会读取 T 的最后一个本地日志记录并作出相应的操作：

1. 如果 T 的最后一条日志记录是 `<commit T>`，那么说明协调者在节点发生故障时的全局决策是提交 T，根据本地事务所使用的日志方式，在该节点上可能需要执行 `redo T`。
2. 如果 T 的最后一条日志记录是 `<abort T>`，那么说明协调者在节点发生故障时的全局决策是中止 T，根据本地事务所使用的日志方式，在该节点上可能需要执行 `undo T`。
3. 如果 T 的最后一条日志记录是 `<don't commit T>`，则和第 2 中情况类似，执行 `undo T`。
4. 如果 T 的最后一条日志记录是 `<ready T>`，这种情况比较麻烦，因为恢复节点无法确认在它故障之后协调者发出的最终全局决策是什么，因此它必须要和集群中其余至少一个节点取得联系，询问 T 的最终结果是什么：恢复节点先尝试询问协调者，如果此时协调者正在工作，则告知恢复节点 T 的最终结果，如果是提交就执行 `redo T`，中止就执行 `undo T`；如果协调者因故不在工作，则恢复节点可以要求其他某一个参与者节点去查看本地日志以找出 T 的最终结果并告知恢复节点。在最坏的情况下，恢复节点无法和集群中其他所有节点取得联系，这时恢复节点只能阻塞等待，直至得知 T 的最终结果是提交还是中止。
5. 如果本地日志中没有记录任何关于 T 在两阶段提交过程中的操作，那么根据前面的两阶段提交流程可知恢复节点还没来得及回复协调者的 `canCommit?` 请求消息就发生了故障，因此根据两阶段算法，恢复节点只能执行 `undo T`。

#### 4.2.5 **缺陷**

1. **同步阻塞**：两阶段提交协议是一个阻塞的协议，在第二阶段期间，参与者在事务未提交之前会一直锁定其占有的本地资源对象，直到接收到来自协调者的 `doCommit` 或 `doAbort` 消息。
2. **单点故障**：两阶段提交协议中只有一个协调者，而由于在第二阶段中参与者在收到协调者的进一步指示之前会一直锁住本地资源对象，如果唯一的协调者此时出现故障而崩溃掉之后，那么所有参与者都将无限期地阻塞下去，也就是一直锁住本地资源对象而导致其他进程无法使用。
3. **数据不一致**：如果在两阶段提交协议的第二阶段中，协调者向所有参与者发送 `doCommit` 消息之后，发生了局部网络抖动或者异常，抑或是协调者在只发送了部分消息之后就崩溃了，那么就只会有部分参与者接收到了 `doCommit` 消息并提交了本地事务；其他未收到 `doCommit` 消息的参与者则不会提交本地事务，因而导致了数据不一致问题。

#### 4.2.6 **XA 标准接口**

2PC 两阶段提交协议本身只是一个通用协议，不提供具体的工程实现的规范和标准，在工程实践中为了统一标准，减少行业内不必要的对接成本，需要制定标准化的处理模型及接口标准，国际开放标准组织 Open Group 定义了分布式事务处理模型 **DTP**（Distributed Transaction Processing）Model，现在 XA 已经成为 2PC 分布式事务提交的事实标准，很多主流数据库如 Oracle、MySQL 等都已经实现 XA。

两阶段事务提交采用的是 X/OPEN 组织所定义的 [DTP Model](http://pubs.opengroup.org/onlinepubs/009680699/toc.pdf) 所抽象的 AP（应用程序）, TM（事务管理器）和 RM（资源管理器） 概念来保证分布式事务的强一致性。 其中 TM 与 RM 间采用 XA 的协议进行双向通信。 与传统的本地事务相比，XA 事务增加了准备阶段，数据库除了被动接受提交指令外，还可以反向通知调用方事务是否可以被提交。 `TM` 可以收集所有分支事务的准备结果，并于最后进行原子提交，以保证事务的强一致性。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHIGh83HCH5gBYCq1k31tWIcXch3iaYR0UAn45qwZ4ib3s0IJMb65hge5g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

Java 通过定义 JTA 接口实现了 XA 模型，JTA 接口中的 `ResourceManager` 需要数据库厂商提供 XA 驱动实现， `TransactionManager` 则需要事务管理器的厂商实现，传统的事务管理器需要同应用服务器绑定，因此使用的成本很高。 而嵌入式的事务管器可以以 jar 包的形式提供服务，同 Apache ShardingSphere 集成后，可保证分片后跨库事务强一致性。

通常，只有使用了事务管理器厂商所提供的 XA 事务连接池，才能支持 XA 的事务。Apache ShardingSphere 在整合 XA 事务时，采用分离 XA 事务管理和连接池管理的方式，做到对应用程序的零侵入。

#### 4.2.7 **三阶段提交协议**

由于前文提到的两阶段提交协议的种种弊端，研究者们后来又提出了一种新的分布式原子提交协议：三阶段提交协议（three-phase commit protocol, 3PC）。

三阶段提交协议是对两阶段提交协议的扩展，它在特定假设下避免了同步阻塞的问题。该协议基于以下两个假设：

1. 集群不发生网络分区；
2. 故障节点数不超过 K 个（K 是预先设定的一个数值）。

基于这两个假设，三阶段提交协议通过引入超时机制和一个额外的阶段来解决阻塞问题，三阶段提交协议把两阶段提交协议的第一个阶段拆分成了两步：1) 评估，2) 资源对象加锁，最后才真正提交：

1. **CanCommit 阶段**：协调者发送 `CanCommit` 请求消息，询问各个参与者节点，参与者节点各自评估本地事务是否可以执行并回复消息（可以执行则回复 YES，否则回复 NO），此阶段不执行事务，只做判断；
2. **PreCommit 阶段**：协调者根据上一阶段收集的反馈决定通知各个参与者节点执行（但不提交）或中止本地事务；有两种可能：1) 所有回复都是 YES，则发送 `PreCommit` 请求消息，要求所有参与者执行事务并追加记录到 undo 和 redo 日志，如果事务执行成功则参与者回复 ACK 响应消息，并等待下一阶段的指令；2) 反馈消息中只要有一个 NO，或者等待超时之后协调者都没有收到参与者的回复，那么协调者会中止事务，发送 `Abort` 请求消息给所有参与者，参与者收到该请求后中止本地事务，或者参与者超时等待仍未收到协调者的消息，同样也中止当前本地事务。
3. **DoCommit 阶段**：协调者根据上一阶段收集到的反馈决定通知各个参与者节点提交或回滚本地事务，分三种情况：1) 协调者收到全部参与者回复的 ACK，则向所有参与者节点广播 `DoCommit` 请求消息，各个参与者节点收到协调者的消息之后决定提交事务，然后释放资源对象上的锁，成功之后向协调者回复 ACK，协调者接收到所有参与者的 ACK 之后，将该分布式事务标记为 `committed`；2) 协调者没有收到全部参与者回复的 ACK（可能参与者回复的不是 ACK，也可能是消息丢失导致超时），那么协调者就会中止事务，首先向所有参与者节点广播 `Abort` 请求消息，各个参与者收到该消息后利用上一阶段的 undo 日志进行事务的回滚，释放占用的资源对象，然后回复协调者 ACK 消息，协调者收到参与者的 ACK 消息后将该分布式事务标记为 `aborted`；3) 参与者一直没有收到协调者的消息，等待超时之后会直接提交事务。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkH7QIbPf3DR6G2F3kkXp6rI1oT57jEiaF0AzqhicZKClWe1IZR9uPVy04g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

事实上，在最后阶段，协调者不是通过追加本地日志的方式记录提交决定的，而是首先保证让至少 K 个参与者节点知道它决定提交该分布式事务。如果协调者发生故障了，那么剩下的参与者节点会重新选举一个新的协调者，这个新的协调者就可以在集群中不超过 K 个参与者节点故障的情况下学习到旧协调者之前是否已经决定要提交分布式事务，若是，则重新开始协议的第三阶段，否则就中止该事务，重新发起分布式事务。

**在最后的 DoCommit 阶段，如果参与者一直没有收到协调者的 `DoCommit` 或者 `Abort` 请求消息时，会在等待超时之后，直接提交事务。这个决策机制是基于概率学的：当已经进入第三阶段之后，说明参与者在第二阶段已经收到了 `PreCommit` 请求消息，而协调者发出 `PreCommit` 请求的前提条件是它在第二阶段开头收集到的第一阶段向所有参与者发出的 `CanCommit` 请求消息的反馈消息都是 YES。所以参与者可以根据自己收到了 `PreCommit` 请求消息这一既定事实得出这样的一个结论：其他所有参与者都同意了进行这次的事务执行，因此当前的参与者节点有理由相信，进入第三阶段后，其他参与者节点的本地事务最后成功提交的概率很大，而自己迟迟没有收到 `DoCommit` 或 `Abort` 消息可能仅仅是因为网络抖动或异常，因此直接提交自己的本地事务是一个比较合理的选择**。

三阶段提交协议主要着重于解决两阶段提交协议中因为协调者单点故障而引发的同步阻塞问题，虽然相较于两阶段提交协议有所优化，但还是没解决可能发生的数据不一致问题，比如由于网络异常导致部分参与者节点没有收到协调者的 `Abort` 请求消息，超时之后这部分参与者会直接提交事务，从而导致集群中的数据不一致，另外三阶段提交协议也无法解决脑裂问题，同时也因为这个协议的网络开销问题，导致它并没有被广泛地使用，有关该协议的具体细节可以参阅本文最后的延伸阅读一节中的文献进一步了解，这里不再深入。

## 5.**共识算法**

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHBtFFANgJcZbBLZtSuQ3e0pLvE7ewMB5fnFHmqnRWhj6xkJ6wa9qGJA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

共识（Consensus），很多时候会见到与一致性（Consistency）术语放在一起讨论。严谨地讲，两者的含义并不完全相同。

一致性的含义比共识宽泛，在不同场景（基于事务的数据库、分布式系统等）下意义不同。具体到分布式系统场景下，一致性指的是多个副本对外呈现的状态。如前面提到的顺序一致性、线性一致性，描述了多节点对数据状态的共同维护能力。而共识，则特指在分布式系统中多个节点之间对某个事情（例如多个事务请求，先执行谁？）达成一致意见的过程。因此，达成某种共识并不意味着就保障了一致性。

实践中，要保证系统满足不同程度的一致性，往往需要通过共识算法来达成。

共识算法解决的是分布式系统对某个提案（Proposal），大部分节点达成一致意见的过程。提案的含义在分布式系统中十分宽泛，如多个事件发生的顺序、某个键对应的值、谁是主节点……等等。可以认为任何可以达成一致的信息都是一个提案。

对于分布式系统来讲，各个节点通常都是相同的确定性状态机模型（又称为状态机复制问题，State-Machine Replication），从相同初始状态开始接收相同顺序的指令，则可以保证相同的结果状态。因此，系统中多个节点最关键的是对多个事件的顺序进行共识，即排序。

算法共识/一致性算法有两个最核心的约束：1) 安全性（Safety），2) 存活性（Liveness）：

- Safety：保证决议（Value）结果是对的，无歧义的，不会出现错误情况。

- - 只有是被提案者提出的提案才可能被最终批准；
  - 在一次执行中，只批准（chosen）一个最终决议。被多数接受（accept）的结果成为决议；

- Liveness：保证决议过程能在有限时间内完成。

- - 决议总会产生，并且学习者最终能获得被批准的决议。

### 5.1 Paxos

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHDIfDeQWLMH3wmWU6hhjRPicPzuaGtr9WRNXEYicbUy7tLVRDsZXiastOg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

Google Chubby 的作者 Mike Burrows 说过， `there is only one consensus protocol, and that’s Paxos” – all other approaches are just broken versions of Paxos.`

意即**世上只有一种共识算法，那就是 Paxos，其他所有的共识算法都只是 Paxos 算法的残缺版本**。虽然有点武断，但是自从 Paxos 问世以来，它便几乎成为了分布式共识算法的代名词，后来的许多应用广泛的分布式共识算法如 Raft、Zab 等的原理和思想都可以溯源至 Paxos 算法。

Paxos 是由 Leslie Lamport (LaTeX 发明者，图灵奖得主，分布式领域的世界级大师) 在 1990 年的论文[《The PartTime Parliament》](https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf)里提出的，Lamport 在论文中以一个古希腊的 Paxos 小岛上的议会制订法律的故事切入，引出了 Paxos 分布式共识算法。

#### 5.1.1 **Basic Paxos**

业界一般将 Lamport 论文里最初提出分布式算法称之为 Basic Paxos，这是 Paxos 最基础的算法思想。

**Basic Paxos 算法的最终目标是通过严谨和可靠的流程来使得集群基于某个提案（Proposal）达到最终的共识**。

#### 5.1.2 **基础概念**

- **Value**：提案值，是一个抽象的概念，在工程实践中可以是任何操作，如『更新数据库某一行的某一列』、『选择 xxx 服务器节点作为集群中的主节点』。
- **Number**：提案编号，全局唯一，单调递增。
- **Proposal**：集群需要达成共识的提案，提案 = 编号 + 值。

Proposal 中的 Value 就是在 Paxos 算法完成之后需要达成共识的值。

Paxos 算法中有三个核心角色：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHD3ib2XL4sm2zFzNhulUrnicmdyyia6edibBjGFA9AoQZYHVcmNzRJmeduQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **Proposer**：生成提案编号 `n` 和值 `v`，然后向 Acceptors 广播该提案，接收 Acceptors 的回复，如果有超过半数的 Acceptors 同意该提案，则选定该提案，否则放弃此次提案并生成更新的提案重新发起流程，提案被选定之后则通知所有 Learners 学习该最终选定的提案值（也可以由 Acceptor 来通知，看具体实现）。Basic Paxos 中允许有多个 Proposers。
- **Acceptor**：接收 Proposer 的提案并参与提案决策过程，把各自的决定回复给 Proposer 进行统计。Acceptor 可以接受来自多个 proposers 的多个提案。
- **Learner**：不参与决策过程，只学习最终选定的提案值。

**在具体的工程实践中，一个节点往往会充当多种角色，比如一个节点可以既是 Proposer 又是 Acceptor，甚至还是 Learner。**

#### 5.1.3 **算法流程**

相较于直接给出 Paxos 算法的流程，我想沿袭 Lamport 大师的经典 Paxos 论文[《Paxos Made Simple》](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)中的思路：通过循序渐进的方式推导出 Paxos 算法。

首先需要了解 Paxos 算法中的两个重要的约束：

> C1. 一个 Acceptor 必须接受它收到的第一个提案。

> C2. 只有当**超过半数**的 Acceptors 接受某一个提案，才能最终选定该提案。

C2 其实有一个隐含的推论：一个 Acceptor 可以接受多个提案，这也是为什么我们需要给每一个提案生成一个编号的原因，用来给提案排序。

我们前面提到过 Paxos 的最终目标是通过严谨和可靠的流程来使得集群基于某个提案（Proposal）达到最终的共识，也就是说基于某一个提案发起的一次 Paxos 流程，最终目的是希望集群对该提案达成一致的意见，而为了实现并维持集群中的这种一致性，前提是 Paxos 算法必须具有幂等性：一旦提案（Proposal）中的值（Value）被选定（Chosen），那么只要还在此次 Paxos 流程中，就算不断按照 Paxos 的规则重复步骤，未来被 Chosen 的 Value 都会是同一个。如果不满足这种幂等性，将可能导致不一致的问题。

因此，我们可以把 Paxos 的基本命题提炼出来：

> P1. 在一次 Paxos 流程中，如果一个值（Value）为 `v` 的提案（Proposal）被选定（Chosen）了，那么后续任何被最终选定的带有更大编号（Number）的提案中的 Value 也必须是 `v`。

提案在被最终选定之前必须先被 Acceptor 接受，于是我们可以再进一步总结一个具有更强约束的命题：

> P2. 在一次 Paxos 流程中，如果一个值（Value）为 `v` 的提案（Proposal）被选定（Chosen）了，那么后续任何被 Acceptor 接受的带有更大编号（Number）的提案中的 Value 也必须是 `v`。

这还不是具备最强约束的命题，因为提案在被 Acceptor 接受之前必须先由 Proposer 提出，因此还可以继续强化命题：

> P3. 在一次 Paxos 流程中，如果一个值（Value）为 `v` 的提案（Proposal）被选定（Chosen）了，那么后续任何 Proposer 提议的带有更大编号（Number）的提案中的 Value 也必须是 `v`。

从上述的三个命题，我们可以很容易地看出来，P3 可以推导出 P2，进而推导出 P1，也就是说这是一个[归约](https://zh.wikipedia.org/wiki/歸約)的过程，因此只要 P3 成立则 P1 成立，也就是 Paxos 算法的正确性得到保证。

那么要如何实现呢 P3 呢？只需满足如下约束：

> C3. 对于一个被 Proposer 提议的提案中任意的 `v` 和 `n`，存在一个数量超过半数 Acceptors 的集合 S，满足以下两个条件中的任意一个：
>
> - S 中的任何一个 Acceptor 都没有接受过编号小于 `n` 的提案。
> - S 中所有的 Acceptors 接受过的最大编号的提案的 Value 为 `v`。

为了满足 C3 从而实现 P3，需要引入一条约束：Proposer 每次生成自己的 `n` 之后，发起提案之前，必须要先去『学习』那个已经被选定或者将要被选定的小于 `n` 的提案，如果有这个提案的话则把那个提案的 `v` 作为自己的此次提案的 Value，没有的话才可以自己指定一个 Value，这样的话 Proposer 侧就可以保证更高编号的提案的值只会是已选定的 `v` 了，但是 Acceptor 侧还无法保证，因为 Acceptor 有可能还会接受其他的 Proposers 的提案值，于是我们需要对 Acceptor 也加一条约束，让它承诺在收到编号为 `n` 的 `v` 之后，不会再接受新的编号小于 `n` 的提案值。

所以我们可以得到一个 Paxos 在 Proposer 侧的算法流程：

1. Proposer 生成一个新的提案编号 `n` 然后发送一个 ***prepare*** 请求给**超过半数**的 Acceptors 集合，要求集合中的每一个 Acceptor 做出如下响应：

   (a) 向 Proposer 承诺在收到该消息之后就不再接受编号小于 `n` 的提案。

   (b) 如果 Acceptor 在收到该消息之前已经接受过其他提案，则把当前接受的编号最大的提案回复给 Proposer。

2. 如果 Proposer 收到了**超过半数**的 Acceptors 的回复，那么就可以生成 `(n, v)` 的提案，这里 `v` 是所有 Acceptors 回复中编号最大的那个提案里的值，如果所有 Acceptors 回复中都没有附带上提案的话，则可以由 Proposer 自己选择一个 `v`。

3. Proposer 将上面生成的提案通过一个 ***accept*** 请求发送给一个**超过半数**的 Acceptors 集合。（需要注意的是这个集合不一定和第二步中的那个集合是同一个。）

Paxos 在 Proposer 侧的算法流程已经确定了，接下来我们需要从 Acceptor 的视角来完成剩下的算法推导。前面我们提到过，Acceptor 是可以接受多个 Proposers 的多个提案的，但是在收到一个 Proposer 的 ***prepare*** 消息后会承诺不再接受编号小于 `n` 的新提案，也就是说 Acceptor 也是可以忽略掉其他 Proposers 消息（包括 ***prepare*** 和 ***accept***）而不会破坏算法的**安全性**，当然了，在工程实践中也可以直接回复一个错误，让 Proposer 更早知道提案被拒绝然后生成提案重新开始流程。这里我们应该重点思考的场景是一个 Acceptor 接受一个提案请求的时候，根据前面 Proposer 要求 Acceptor 的承诺，我们可以给 Acceptor 设置一个这样的约束：

> C4. 如果一个 Proposer 发出了带 `n` 的 ***prepare*** 请求，只要 Acceptor 还没有回复过任何其他编号大于 `n` 的prepare 请求，则该 Acceptor 可以接受这个提案。

因为 Acceptor 需要对 Proposer 做出不接受编号小于 `n` 的提案的承诺，因此它需要做持久化记录，那么它就必须是有状态的，也因此每个 Acceptor 都需要利用可靠性存储（日志）来保存两个对象：

1. Acceptor 接受过的编号最大的提案；
2. Acceptor 回复过的最大的 ***prepare*** 请求提案编号。

以上这就是 Acceptor 侧的约束。接下来我们就可以得到 Paxos 的整个算法流程了。

Paxos 算法可以归纳为两大基本过程：

1. 选择过程；
2. 学习过程。

#### 5.1.4 **选择过程**

选择过程分为两个阶段：

- **阶段一（Phase 1）：**

  (a) Proposer 生成一个全局唯一且单调递增的提案编号 `n`，然后发送编号为 `n` 的 ***prepare*** 请求（P1a msg）给**超过半数**的 Acceptors 集合。

  (b) 当一个 Acceptor 收到一个编号为 `n` 的 ***prepare*** 请求，如果 `n` 比它此前接受过其他的提案编号（如果有）都要大的话，那么将这个提案编号 `n` 写入本地日志，这里记为 `max_n`，然后作出『两个承诺，一个回复』:

  否则就忽略该 ***prepare*** 消息或者回复一个错误。

- - 在不违背以前作出的承诺下，回复消息（P1b msg），附带上自己已经接受过的提案中编号最大的那个提案的 `v` 和 `n`，没有则返回空值。
  - 不再接受编号小于等于 `n` 的 ***prepare*** 请求
  - 不再接受编号小于等于 `n` 的 ***accept*** 请求
  - 两个承诺：
  - 一个回复：

- **阶段二（Phase 2）：**

  (a) 当 Proposer 收到**超过半数**的 Acceptors 回复它的编号为 `n` 的 ***prepare*** 请求的响应，此时有两种可能：

  (b) 当 Acceptor 收到一个编号为 `n` 的提案的 ***accept*** 请求消息，需要分两种情况处理：

- - 如果 `n` >= `max_n`（通常情况下这两个值是相等的），则接受该提案并回复消息（P2b msg）。
  - 如果 `n` < `max_n`，则忽略该 ***accept*** 消息或者回复一个错误（P2b error）。
  - **Free**：没有任何一个 Acceptor 的回复消息中附带已被接受的提案，意味着当前流程中还没有提案值被最终接受，此时 Proposer 可以自由地选择提案值 Value，最后发送一个包含 `(n, v)` 提案的 ***accept*** 请求消息（P2a msg）给 Acceptors 集合。
  - **Forced**：某些 Acceptors 的回复消息中附带已被接受的提案，那么 Proposer 必须强制使用这些回复消息中编号最大的提案 Value 作为自己的提案值，最后发送一个包含 `(n, v)` 提案的 ***accept*** 请求消息（P2a msg）给 Acceptors 集合。

#### 5.1.5 **学习过程**

选择过程结束之后，我们得到了一个提案值，接下来就是要让集群中的所有 Learner 『学习』到这个值了，以求达到集群的共识。

Learner 学习提案值的方式可以分成三种：

1. 任意一个 Acceptor 接受了一个提案后就立刻将该提案发送给**所有 Learner**。优点：Learner 能实时学习到被 Paxos 流程选定的 Value；缺点：网络通信次数太多，如果有 N 个 Acceptors 和 M 个 Learner，则需要的网络通信是 N*M 次。
2. 设置一个主 Learner，Acceptor 接受了一个提案后只将该提案发送给主 Learner，主 Learner 再转发给剩下的 Learners。优点：网络通信次数只需 N+M-1 次；缺点：主 Learner 有单点故障的风险。
3. Acceptor 接受了一个提案后将该提案发送给一个 Learner 集合，由这个集合去通知剩下的 Learners。优点：用集合替代单点，可靠性更高；缺点：增加系统复杂度，需要维护一个 Learner 小集群。

至此，我们就推导出了整个 Paxos 算法的流程：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHaZ5eE9x0hE3ed6augyiapOa5icXgD54kR4l4fuqFtGvRFFLpRPyczsvQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 5.2 算法证明

这一节我们来证明 Paxos 算法的正确性。

上一节我们已经提炼出来了 Paxos 的基本命题 P1，并通过归约 P1 得到了约束性更强的另外两个命题 P2 和 P3，根据归约的原理，我们知道 P3 可以最终推导出 P1，也就是说如果要证明 Paxos 的基本命题 P1，只需要证明 P3 即可。为什么之前我们要不断强化 Paxos 的命题呢？因为从数学的层面来讲，一个具有更强约束（更多假设）的命题一般会更容易证明。

现在我们把 P1, P2 和 P3 用更严格的数学语言来描述：

> P1. 在一次 Paxos 流程中，如果一个包含 (n, v) 的提案被选定（Chosen），那么存在未来被选定的提案 (k, v1)，必然满足 k > n，v1 = v。
>
> P2. 在一次 Paxos 流程中，如果一个包含 (n, v) 的提案被选定（Chosen），那么存在未来被超过半数的 Acceptors 接受的提案 (k, v1)，必然满足 k > n，v1 = v。
>
> P3. 在一次 Paxos 流程中，如果一个包含 (n, v) 的提案被选定（Chosen），那么存在未来由 Proposer 提议的提案 (k, v1)，必然满足 k > n，v1 = v。

现在我们利用数学归纳法来证明 P3：

**假设 k = m 时 P3 成立，由于 (n, v) 已经是被选定的提案，因此 Proposer 发起的从 n 到 k 的提案中的 Value 都会是 v，其中 m >= n，那么根据归约的原理可证 k = m 时 P1 也成立**。

现在令 k = m+1，Proposer 发送带编号 k 的 ***prepare*** 请求消息到 Acceptors 集合。

由于此前已经有了选定的提案，那么根据 Paxos 的约束 C2 可知参与这一个提案投票的 Acceptors 集合必定和上一个集合有重合。

根据 Acceptors 集合重叠和 Paxos 的 P1b 阶段可知，回复的消息中必定附带有已被大多数 Acceptors 接受的提案 (i, v0)。

然后根据 P2a 阶段，Proposer 提案 (k, v1)，其中 v1 = v0。

还是根据 P1b，可知 i 是所有回复消息里编号最大的，可得 i >= m，又根据 P1a 可知 i < k，因此可以得出提案 (i, v0) 中有 v0 = v。

可知当 k = m+1 时，提案 (k, v1) 中的 v1 = v。

根据数学归纳法的原理，我们还需要找到一个特例来使得命题成立，然后由特例推广到普遍，我们这里选择 k = 1 作为特例，证明 k = 1 时 P3 成立：根据 Paxos 的约束 C1 易知在 n = 0，k = 1 的场景下，P3 成立。

因此可根据数学归纳法基于 k = 1 进行推广至 k = m（m 代表任意自然数），最后 P3 命题得证。

再由归约的原理可知，P3 可推导出 P2，最后 P2 推导出 P1。至此， Paxos 算法原理正确性的证明完成。

**上述的证明只是一种比较简单且粗浅的证明方法，但是对于工程师理解 Paxos 原理来说已经足够了，如果希望进一步学习 Paxos 原理的严格数学证明，可以参阅 Leslie Lamport 的原始论文[《The PartTime Parliament》](https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf)，里面给出了 Paxos 算法的严格数学证明。**

## 6.**Multi-Paxos**

自 Lamport 于 1990 年在论文[《The PartTime Parliament》](https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf)中提出 Paxos 算法之后，这个算法一直被评价为难以理解和实现，这篇论文中运用了大量的数学对 Paxos 的原理进行证明，而又由于 Lamport 在论文里用讲故事的形式解释 Paxos，进一步增大了人们彻底理解 Paxos 的难度，事实上 Lamport 的这篇论文也因此在发表过程中一波三折，这里不展开，有兴趣的读者可以自行去了解这段这段背景故事。

因为业界在理解 Paxos 算法上持续的怨声载道，Lamport 在 2001 年发表了论文[《Paxos Made Simple》](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)，对原论文进行精简，以更通俗易懂的语言和形式阐述 Paxos 算法，并在其中提出了更加具备工程实践性的 Multi-Paxos 的思想。

关于 Paxos 难以理解的问题上，我个人的一点愚见是：Paxos 算法的思想其实并不难理解，真正难的地方是：

1. Paxos 背后那一套完整的数学原理和证明
2. 在复杂分布式环境将 Paxos 进行工程落地

我个人建议的 Paxos 学习资料是：[《Paxos Made Simple》](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)，[《Paxos Made Live - An Engineering Perspective》](https://read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf)以及 [Paxos lecture (Raft user study)](https://www.youtube.com/watch?v=JEpsBg0AO6o)。第一篇论文可以说是 Lamport  1990 年那篇最初的论文的精简版，可读性提高了很多，论文里也没有使用任何数学公式，只需一点英文基础就可以通读，第二篇论文讲的则是 Google 内部基于 Multi-Paxos 实现的分布式锁机制和小文件存储系统，这是业界较早的实现了 Multi-Paxos 的大规模线上系统，十分具有参考性，最后的 Youtube 视频则是 Raft 的作者 Diego Ongaro 为了对比 Raft 和 Multi-Paxos 的学习的难易程度而做的，非常适合作为学习 Paxos 和 Raft 的入门资料。

从上一节可知 Basic Paxos 算法有几个天然缺陷：

- 只能就单个值（Value）达成共识，不支持多值共识。在实际的工程实践中往往是需要对一系列的操作达成共识，比如分布式事务，由很多执行命令组成。
- 至少需要 2 轮往返 4 次 ***prepare*** 和 ***accept*** 网络通信才能基于一项提案达成共识。对于一个分布式系统来说，网络通信是最影响性能的因素之一，过多的网络通信往往会导致系统的性能瓶颈。
- 不限制 Proposer 数量导致非常容易发生提案冲突。极端情况下，多 Proposer 会导致系统出现『活锁』，破坏分布式共识算法的两大约束之一的活性（liveness）。

关于第三点，前文提到分布式共识算法必须满足两个最核心的约束：安全性（safety）和活性（liveness），从上一节我们可以看出 Basic Paxos 主要着重于 safety，而对 liveness 并没有进行强约束，让我们设想一种场景：两个 Proposers (记为 P1 和 P2) 轮替着发起提案，导致两个 Paxos 流程重叠了：

1. 首先，P1 发送编号 N1 的 ***prepare*** 请求到 Acceptors 集合，收到了过半的回复，完成阶段一。
2. 紧接着 P2 也进入阶段一，发送编号 N2 的 ***prepare*** 请求到过半的 Acceptors 集合，也收到了过半的回复，Acceptors 集合承诺不再接受编号小于 N2 的提案。
3. 然后 P1 进入阶段二，发送编号 N1 的 ***accept*** 请求被 Acceptors 忽略，于是 P1 重新进入阶段一发送编号 N3 的 ***prepare*** 请求到 Acceptors 集合，Acceptors 又承诺不再接受编号小于 N3 的提案。
4. 紧接着 P2 进入阶段二，发送编号 N2 的 ***accept*** 请求，又被 Acceptors 忽略。
5. 不断重复上面的过程......

在极端情况下，这个过程会永远持续，导致所谓的『活锁』，永远无法选定一个提案，也就是 liveness 约束无法满足。

为了解决这些问题，Lamport 在[《Paxos Made Simple》](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)论文中提出了一种基于 Basic Paxos 的 Multi-Paxos 算法思想，并基于该算法引出了一个分布式银行系统状态机的实现方案，感兴趣的读者不妨看一下。

Multi-Paxos 算法在 Basic Paxos 的基础上做了两点改进：

1. **多 Paxos 实例**：针对每一个需要达成共识的单值都运行一次 Basic Paxos 算法的实例，并使用 Instance ID 做标识，最后汇总完成多值共识。
2. **选举单一的 Leader Proposer**：选举出一个 Leader Proposer，所有提案只能由 Leader Proposer 来发起并决策，Leader Proposer 作为 Paxos 算法流程中唯一的提案发起者，『活锁』将不复存在。此外，由于单一 Proposer 不存在提案竞争的问题，Paxos 算法流程中的阶段一中的 ***prepare*** 步骤也可以省略掉，从而将两阶段流程变成一阶段，大大减少网络通信次数。

关于多值共识的优化，如果每一个 Basic Paxos 算法实例都设置一个 Leader Proposer 来工作，还是会产生大量的网络通信开销，因此，多个 Paxos 实例可以共享同一个 Leader Proposer，这要求该 Leader Proposer 必须是稳定的，也即 Leader 不应该在 Paxos 流程中崩溃或改变。

由于 Lamport 在论文中提出的 Multi-Paxos 只是一种思想而非一个具体算法，因此关于 Multi-Paxos 的很多细节他并没有给出具体的实现方案，有些即便给出了方案也描述得不是很清楚，比如他在论文中最后一节提出的基于银行系统的状态机中的多 Paxos 实例处理，虽然给了具体的论述，但是在很多关键地方还是没有指明，这也导致了后续业界里的 Multi-Paxos 实现各不相同。kd

我们这里用 Google Chubby 的 Multi-Paxos 实现来分析这个算法。

首先，Chubby 通过引入 Master 节点，实现了 Lamport 在论文中提到的 single distinguished proposer，也就是 Leader Proposer，Leader Proposer 作为 Paxos 算法流程中唯一的提案发起者，规避了多 Proposers 同时发起提案的场景，也就不存在提案冲突的情况了，从而解决了『活锁』的问题，保证了算法的活性（liveness）。

Lamport 在论文中指出，选择 Leader Proposer 的过程必须是可靠的，那么具体如何选择一个 Leader Proposer 呢？在 Chubby 中，集群利用 Basic Paxos 算法的共识功能来完成对 Leader Proposer 的选举，这个实现是具有天然合理性的，因为 Basic Paxos 本身就是一个非常可靠而且经过严格数学证明的共识算法，用来作为选举算法再合适不过了，在 Multi-Paxos 流程期间，Master 会通过不断续租的方式来延长租期（Lease）。比如在实际场景中，一般在长达几天的时期内都是同一个服务器节点作为 Master。万一 Master 故障了，那么剩下的 Slaves 节点会重新发起 Paxos 流程票选出新的 Master，也就是说主节点是一直存在的，而且是唯一的。

此外，Lamport 在论文中提到的过一种优化网络通信的方法：“当 Leader Proposer 处于稳定状态时，可以跳过阶段一，直接进入阶段二”，在 Chubby 中也实现了这个优化机制，Leader  Proposer 在为多个 Paxos 算法实例服务的时候直接跳过阶段一进入阶段二，只发送 ***accept*** 请求消息给 Acceptors 集合，将算法从两阶段优化成了一阶段，大大节省网络带宽和提升系统性能。

最后，Multi-Paxos 是一个"脑裂"容错的算法思想，就是说当 Multi-Paxos 流程中因为网络问题而出现多 Leaders 的情况下，该算法的安全性（safety ）约束依然能得到保证，因为在这种情况下，Multi-Paxos 实际上是退化成了 Basic Paxos，而 Basic Paxos 天然就支持多 Proposers。

**在分布式事务中，Paxos 算法能够提供比两阶段提交协议更加可靠的一致性提交：通过将提交/放弃事务的决定从原来两阶段协议中单一的协调者转移到一个由 Proposer + Acceptors 组成的集群中。Lamport 曾经发表过一篇[《Consensus on Transaction Commit》](https://lamport.azurewebsites.net/video/consensus-on-transaction-commit.pdf)的论文，通过将两阶段提交协议和基于 Paxos 实现的分布式提交协议做对比，对基于 Paxos 实现的提交协议有非常精彩的论述，感兴趣的读者不妨一读**。

## 7.**Raft**

Raft 算法实际上是 Multi-Paxos 的一个变种，通过新增两个约束：

1. **追加日志约束**：Raft 中追加节点的日志必须是串行连续的，而 Multi-Paxos 中则可以并发追加日志（实际上 Multi-Paxos 的并发也只是针对日志追加，最后应用到内部 State Machine 的时候还是必须保证顺序）。
2. **选主限制**：Raft 中只有那些拥有最新、最全日志的节点才能当选 Leader 节点，而 Multi-Paxos 由于允许并发写日志，因此无法确定一个拥有最新、最全日志的节点，因此可以选择任意一个节点作为 Leader，但是选主之后必须要把 Leader 节点的日志补全。

基于这两个限制，Raft 算法的实现比 Multi-Paxos 更加简单易懂，不过由于 Multi-Paxos 的并发度更高，因此从理论上来说 Multi-Paxos 的性能会更好一些，但是到现在为止业界也没有一份权威的测试报告来支撑这一观点。

对比一下 Multi-Paxos 和 Raft 下集群中可能存在的日志顺序：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHSqrk7GWECzbF16ZtLgnhRdL5LEAdictQHafexooWrbNaT9kJTFcnpMQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

可以看出，Raft 中永远满足这样一个约束：follower log 一定会是 leader log 的子集并且顺序一定是连续的，而 Multi-Paxos 则不一定满足这个约束，日志记录通常是乱序的。

由于 Raft 的核心思想源自 Multi-Paxos，在实现过程中做了很多改进优化，然而万变不离其宗，我相信理解了 Multi-Paxos 之后再去学习 Raft 会事半功倍（Raft 在诞生之初也是打着"容易理解"的旗号来对标 Paxos 的），由于前面已经深度剖析过 Paxos 算法的流程和原理了，碍于本文的篇幅所限，这里就不再对 Raft 算法的细节进行深入探讨了，如果有意深入学习 Raft，可以从 [The Raft Consensus Algorithm](https://raft.github.io/) 处找到相关的论文、源码等资料进行全面的学习。

最后有一些概念要澄清一下，Basic Paxos 是一个经过了严格数学证明的分布式共识算法，但是由于前文提到的 Basic Paxos 算法应用在实际工程落地中的种种问题，现实中几乎没有直接基于 Basic Paxos 算法实现的分布式系统，绝大多数都是基于 Multi-Paxos，然而 Multi-Basic 仅仅是一种对 Basic Paxos 的延伸思想而非一个具体算法，问题在于目前业界并没有一个统一的 Multi-Paxos 实现标准，因此 Multi-Paxos 的工程实现是建立在一个未经严格证明的前提之上的，工程实现最终的正确性只能靠实现方自己去验证，而 Raft 则是一个具有统一标准实现的、正确性已经过严格证明的**具体算法**，因此在分布式系统的工程实践中大多数人往往还是会选择 Raft 作为底层的共识算法。

## 8.**算法类型**

需要特别指出的一点是，根据解决的场景是否允许拜占庭（Byzantine）错误，共识算法可以分为 Crash Fault Tolerance (CFT) 和 Byzantine Fault Tolerance（BFT）两类。

对于非拜占庭错误的情况，已经存在不少经典的算法，包括 Paxos（1990 年）、Raft（2014 年）及其变种等。这类容错算法往往性能比较好，处理较快，容忍不超过一半的故障节点。

对于要能容忍拜占庭错误的情况，包括 PBFT（Practical Byzantine Fault Tolerance，1999 年）为代表的确定性系列算法、PoW（1997 年）为代表的概率算法等。确定性算法一旦达成共识就不可逆转，即共识是最终结果；而概率类算法的共识结果则是临时的，随着时间推移或某种强化，共识结果被推翻的概率越来越小，最终成为事实上结果。拜占庭类容错算法往往性能较差，容忍不超过 1/3 的故障节点。

本文主要讨论的分布式共识算法是 CFT 类算法，毕竟对于大多数分布式系统来说，集群节点和网络消息一般都是可控的，系统只会出现节点故障而不会出现像拜占庭错误那样伪造的、欺骗性的网络消息，在这种场景下，CFT 类算法更具有现实意义；BFT/PBFT 类算法更多是用在系统被恶意入侵，故意伪造网络消息的场景里。

## 9.**并发控制**

在分布式事务中，集群中的每个服务器节点要管理很多资源对象，每个节点必须保证在并发事务访问这些资源对象时，它们能够始终保持一致性。因此，每个服务器节点需要对自己的管理的资源对象应用一定的并发控制机制。分布式事务中需要所有服务器节点共同保证事务以串行等价的的方式执行。

也就是说，如果事务 T 对某一个服务器节点上的资源对象 S 的并发访问在事务 U 之前，那么我们需要保证在所有服务器节点上对 S 和其他资源对象的冲突访问，T 始终在 U 之前。

### 9.1 锁并发控制

在分布式事务中，某个对象的锁总是本地持有的（在同一个服务器节点上）。是否加锁是由本地锁管理器（Local Lock Manager，LLM）决定的。LLM 决定是满足客户端持锁的请求，还是阻塞客户端发起的分布式事务。但是，事务在所有服务器节点上被提交或者放弃之前，LLM 不能释放任何锁。在使用加锁机制的并发控制中，原子提交协议在进行的过程中资源对象始终被锁住，并且是排他锁，其他事务无法染指这些资源对象。但如果事务在两阶段提交协议的阶段一就被放弃，则互斥锁可以提前释放。

由于不同服务器节点上的 LLM 独立设置资源对象锁，因此，对于不同的事务，它们加锁的顺序也可能出现不一致。考虑一个场景：事务 T 和 U在服务器 X 和 Y 之间的交错执行：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatg8EFMVQNuaWhrPpPv4VkHMxicRN6iczksOnJzQ3fCfRv2duSN7iaeGWP9HicWIalQIShYFBdUtXsegA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

1. 事务 T 锁住了服务器节点 X 上的资源对象 A，做写入操作；
2. 事务 U 锁住了服务器节点 Y 上的资源对象 B，做写入操作；
3. 事务 T 试图读取服务器节点 Y 上的资源对象 B，此时 B 被事务 U 锁住，因此 T 等待锁释放；
4. 事务 U 试图读取服务器节点 X 上的资源对象 A，此时 A 被事务 T 锁住，因此 U 等待锁释放。

在服务器节点 X 上，事务 T 在事务 U 之前；而在服务器节点 Y 上，事务 U 在事务 T 之前。这种不一致的事务次序导致了事务之间的循环依赖，从而引起分布式死锁。分布式死锁需要通过特定的方法/算法来检测并解除，一旦检测到死锁，则必须放弃其中的某个事务来解除死锁，然后通知事务协调者，它将会放弃该事务所涉及的所有参与者上的事务。

### 9.2 时间戳并发控制

对于单一服务器节点的事务来说，协调者在每个事务启动时会为其分配一个全局唯一的时间戳。通过按照访问资源对象的事务时间戳顺序提交资源对象的版本来强制保证以事务执行的串行等价性。在分布式事务中，协调者必须保证每个事务都会附带全局唯一的时间戳。全局唯一的时间戳由事务访问的第一个协调者发给客户端。如果任意一个服务器节点上的资源对象执行了事务中的一个操作，那么事务时间戳会被发送给该服务器节点上的协调者。

分布式事务中的所有服务器节点共同保证事务以串行等价的方式执行。例如，如果在某服务器节点上，由事务 U 访问的资源对象版本在事务 T 访问之后提交；而在另一个服务器节点上，事务 T 和事务 U 又访问了同一个资源对象，那么它们也必须按照相同的次序提交资源对象。为了保证所有服务器节点上的事务执行的相同顺序，协调者必须就时间戳排序达成一致。时间戳是一个二元组 < 本地时间戳，服务器 ID > 对。在时间戳的比较排序过程中，首先比较本地时间戳，然后再比较服务器 ID。

一个可靠的时间戳并发控制应该保证即使各个服务器节点之间的本地时间不同步，也能保证事务之间的相同顺序。但是考虑到效率，各个协调者之间的时间戳还是最好还是要求大致同步。这样的话，事务之间的顺序通常与它们实际开始的时间顺序相一致。可以利用一些本地物理时钟同步方法来保证时间戳的大致同步。

如果决定利用时间戳机制进行分布式事务的并发控制，那么还需要通过某些方法来解决事务冲突问题。如果为了解决冲突需要放弃某个事务时，相应的协调者会收到通知，并且它将在所有的参与者上放弃该事务。这样，如果事务能够坚持到客户端发起提交请求命令的那个时候，那么这个事务就总能被提交。因此在两阶段提交协议中，正常情况下参与者都会同意提交，唯一一种不同意提交的情况是参与者在事务执行过程中曾经崩溃过。

### 9.3 乐观并发控制

加锁机制这一类悲观并发控制有许多明显的缺陷：

- **锁的维护带来了很多新的开销**。这些开销在不支持对共享数据并发访问的系统中是不存在的。即使是只读事务（如查询），就算这一类事务不会改变数据的完整性，却仍然需要利用锁来保证数据在读取过程中不会被其他事务修改，然而锁却只在最极端的情况下才会发挥作用。
- **锁机制非常容易引发死锁**。预防死锁会严重降低并发度，因此必须利用超时或者死锁检测来解除死锁，但这些死锁解除方案对于交互式的程序来说并不是很理想。
- **锁周期过长**。为了避免事务的连锁（雪崩）放弃，锁必须保留到事务结束之时才能释放，这再一次严重降低了系统的并发度。

由于锁这一类的悲观并发控制有上述的种种弊端，因此研究者们提出了另一种乐观并发控制的机制，以求规避锁机制的天然缺陷，研究者们发现这样的一个现象：在大多数应用中两个客户端事务访问同一个资源对象的可能性其实很低，事务总是能够成功执行，就好像事务之间不存在冲突一样。

所以事务的乐观并发控制的基本思路就是：各个并发事务只有在执行完成之后并且发出 `closeTransaction` 请求时，再去检测是否有冲突，如果确实存在冲突，那么就放弃一些事务，然后让客户端重新启动这些事务进行重试。

在乐观并发控制中，每个事务在提交之前都必须进行验证。事务在验证开始时首先要附加一个事务号，事务的串行化就是根据这些事务号的顺序实现的。分布式事务的验证由一组独立的服务器节点共同完成，每个服务器节点验证访问自己资源对象的事务。这些验证在两阶段提交协议的第一个阶段进行。

**关于分布式事务的并发控制就暂时介绍到这里，如果想要继续深入学习更多并发控制的细节，可以深入阅读《分布式系统：概念与设计》、《数据库系统实现》和《数据库系统概念》等书籍或者其他资料。**

## 10.**总结**

本文通过讲解 **BASE 原则**、**两阶段原子提交协议**、**三阶段原子提交协议**、**Paxos/Multi-Paxos 分布式共识算法的原理与证明**、**Raft 分布式共识算法**和**分布式事务的并发控制**等内容，为读者全面而又深入地讲解分析了分布式事务以及分布式系统的底层核心原理，特别是通过对原子提交协议中的 2PC/3PC 的阐述和分析，以及对分布式共识算法 Paxos 的原理剖析和正确性的证明，最后还有对分布式事务中几种并发控制的介绍，相信能够让读者对分布式事务和分布式系统底层的一致性和并发控制原理有一个深刻的认知，对以后学习和理解分布式系统大有裨益。

本文不仅仅是简单地介绍分布式事务和分布式系统的底层原理，更是在介绍原理的同时，通过层层递进的方式引导读者去真正地理解分布式系统的底层原理和设计思路，而非让读者死记硬背一些概念，所以希望通过这篇抛砖引玉的文章，能够对本文读者在以后学习、操作甚至是设计分布式系统以及分布式事务时的思路有所开拓。

**参考&延伸**

- [ACID](https://en.wikipedia.org/wiki/ACID)
- [Eventual consistency](https://en.wikipedia.org/wiki/Eventual_consistency)
- [Atomic commit](https://en.wikipedia.org/wiki/Atomic_commit)
- [A Two-Phase Commit Protocol and its Performance](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=558282)
- [The PartTime Parliament](https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf)
- [Paxos Made Simple](https://lamport.azurewebsites.net/pubs/paxos-simple.pdf)
- [Fast Paxos](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2005-112.pdf)
- [The Performance of Paxos and Fast Paxos](https://arxiv.org/pdf/1308.1358.pdf)
- [Paxos Made Live - An Engineering Perspective](https://read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf)
- [Paxos (computer science)](https://en.wikipedia.org/wiki/Paxos_(computer_science))
- [The Chubby lock service for loosely-coupled distributed systems](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/chubby-osdi06.pdf)
- [Consensus on Transaction Commit](https://lamport.azurewebsites.net/video/consensus-on-transaction-commit.pdf)
- [Life beyond Distributed Transactions: an Apostate’s Opinion](https://www.ics.uci.edu/~cs223/papers/cidr07p15.pdf)
- [In Search of an Understandable Consensus Algorithm](https://raft.github.io/raft.pdf)
- [Paxos lecture (Raft user study)](https://www.youtube.com/watch?v=JEpsBg0AO6o)
- [Distributed Systems: Concepts and Design](https://ce.guilan.ac.ir/images/other/soft/distribdystems.pdf)
- [How to Build a Highly Available System Using Consensus](https://www.microsoft.com/en-us/research/uploads/prod/1996/10/Acrobat-58-Copy.pdf)
- [数学归纳法](https://zh.wikipedia.org/wiki/数学归纳法)
- [共识算法](https://yeasy.gitbook.io/blockchain_guide/04_distributed_system/algorithms)
- [Distributed Transaction Processing: The XA Specification](https://pubs.opengroup.org/onlinepubs/009680699/toc.pdf)

原文作者：allanpan，腾讯 IEG 高级后台工程师

原文链接：https://mp.weixin.qq.com/s/KKrxuVCrjlXXWMPTXQ-fvA

# 【NO.235】Kubernetes 入门&进阶实战

### **0.写在前面**

笔者今年 9 月从端侧开发转到后台开发，第一个系统开发任务就强依赖了 K8S，加之项目任务重、排期紧，必须马上对 K8S 有概念上的了解。然而，很多所谓“K8S 入门\概念”的文章看的一头雾水，对于大部分新手来说并不友好。经历了几天痛苦地学习之后，回顾来看，K8S 根本不复杂。于是，决心有了这一系列的文章：一方面希望对新手同学有帮助；另一方面，以文会友，希望能够有机会交流讨论技术。

本文组织方式：

\1. K8S 是什么，即作用和目的。涉及 K8S 架构的整理，Master 和 Node 之间的关系，以及 K8S 几个重要的组件：API Server、Scheduler、Controller、etcd 等。
\2. K8S 的重要概念，即 K8S 的 API 对象，也就是常常听到的 Pod、Deployment、Service 等。
\3. 如何配置 kubectl，介绍kubectl工具和配置办法。
\4. 如何用kubectl 部署服务。
\5. 如何用kubectl 查看、更新/编辑、删除服务。
\6. 如何用kubectl 排查部署在K8S集群上的服务出现的问题

### **1.K8S 概览**

##### **1.1 K8S 是什么？**

K8S 是[Kubernetes](https://kubernetes.io/zh/docs/concepts/overview/what-is-kubernetes/)的全称，官方称其是：

> Kubernetes is an open source system for managing [containerized applications](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/) across multiple hosts. It provides basic mechanisms for deployment, maintenance, and scaling of applications.
>
> 用于自动部署、扩展和管理“容器化（containerized）应用程序”的开源系统。

翻译成大白话就是：“**K8S 是 负责自动化运维管理多个 Docker 程序的集群**”。那么问题来了：Docker 运行可方便了，为什么要用 K8S，它有什么优势？

插一句题外话：

- 为什么 Kubernetes 要叫 Kubernetes 呢？[维基百科](https://zh.wikipedia.org/wiki/Kubernetes#cite_note-3)已经交代了（老美对星际是真的痴迷）：

  > Kubernetes（在希腊语意为“舵手”或“驾驶员”）由 Joe Beda、Brendan Burns 和 Craig McLuckie 创立，并由其他谷歌工程师，包括 Brian Grant 和 Tim Hockin 等进行加盟创作，并由谷歌在 2014 年首次对外宣布 。该系统的开发和设计都深受谷歌的 Borg 系统的影响，其许多顶级贡献者之前也是 Borg 系统的开发者。在谷歌内部，Kubernetes 的原始代号曾经是[Seven](https://zh.wikipedia.org/wiki/九之七)，即[星际迷航](https://zh.wikipedia.org/wiki/星际迷航)中的 Borg（[博格人](https://zh.wikipedia.org/wiki/博格_(星际旅行))）。Kubernetes 标识中舵轮有七个轮辐就是对该项目代号的致意。

- 为什么 Kubernetes 的缩写是 K8S 呢？我个人赞同[Why Kubernetes is Abbreviated k8s](https://medium.com/@rothgar/why-kubernetes-is-abbreviated-k8s-905289405a3c#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImQ5NDZiMTM3NzM3Yjk3MzczOGU1Mjg2YzIwOGI2NmU3YTM5ZWU3YzEiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2MDUxNjk3MjAsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjExMDc5MTA1ODc0OTMzMDE5NDUwOCIsImVtYWlsIjoibWFvamlhbmd5dW45OTk5QGdtYWlsLmNvbSIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJuYW1lIjoiSmlhbmd5dW4gTWFvIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hLS9BT2gxNEdpclJJYnVrcEltb0dHemt4Q01xbzJsMWFtUjdlX1pSSTdEZVZjWD1zOTYtYyIsImdpdmVuX25hbWUiOiJKaWFuZ3l1biIsImZhbWlseV9uYW1lIjoiTWFvIiwiaWF0IjoxNjA1MTcwMDIwLCJleHAiOjE2MDUxNzM2MjAsImp0aSI6IjYwYjc1ZjczYjkwNzBlZDYwODY2MzFiN2RmZjY2ZGQ1YjE0YzNlZGYifQ.Z2jxeJpyVs_hKdXirBUaM1B_llDVFmWX3M4Yb--VM2wpd0WwTXQ_48g88ShWsAqGuoVP0nOlTXFktg2DZKn5wj7H7W_URgE5nxxiXOBZAqAxpoiPN-_Uup73PaATVvDHg-dKuWWRIZQ21E8nyhSnFAQA2tHQenTIh6UpQBMPUpcI7v6M-c_b1X8n4_EB0KEPOeFeJb3Yz8xFpm9hqb0D6B6L8ovZBFa6d576S56D6f_9RdJS67vDDf4wOjqr1aIxSEgOTV_m-nJJgdCZEr3OgGLuTXm86mh9jg1d8PdMbcxoRjG9LVeQz68-lUxnxN798zYavZjnLsmtV9QYeM0Nfw)中说的观点“嘛，写全称也太累了吧，不如整个缩写”。其实只保留首位字符，用具体数字来替代省略的字符个数的做法，还是比较常见的。

##### **1.2 为什么是 K8S?**

试想下传统的后端部署办法：把程序包（包括可执行二进制文件、配置文件等）放到服务器上，接着运行启动脚本把程序跑起来，同时启动守护脚本定期检查程序运行状态、必要的话重新拉起程序。

有问题吗？显然有！最大的一个问题在于：**如果服务的请求量上来，已部署的服务响应不过来怎么办？**传统的做法往往是，如果请求量、内存、CPU 超过阈值做了告警，运维马上再加几台服务器，部署好服务之后，接入负载均衡来分担已有服务的压力。

问题出现了：从监控告警到部署服务，中间需要人力介入！那么，**有没有办法自动完成服务的部署、更新、卸载和扩容、缩容呢？**

**这，就是 K8S 要做的事情：自动化运维管理 Docker（容器化）程序**。

##### **1.3 K8S 怎么做？**

我们已经知道了 K8S 的核心功能：自动化运维管理多个容器化程序。那么 K8S 怎么做到的呢？这里，我们从宏观架构上来学习 K8S 的设计思想。首先看下图，图片来自文章[Components of Kubernetes Architecture](https://medium.com/@kumargaurav1247/components-of-kubernetes-architecture-6feea4d5c712)：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwwXk0rxy9CzHyow9mHPQiaNm0VB6R8R1Amz9q18cQt9KcDicVNQyBjiaVA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

K8S 是属于**主从设备模型（Master-Slave 架构）**，即有 Master 节点负责核心的调度、管理和运维，Slave 节点则在执行用户的程序。但是在 K8S 中，主节点一般被称为**Master Node 或者 Head Node**（本文采用 Master Node 称呼方式），而从节点则被称为**Worker Node 或者 Node**（本文采用 Worker Node 称呼方式）。

要注意一点：Master Node 和 Worker Node 是分别安装了 K8S 的 Master 和 Woker 组件的实体服务器，每个 Node 都对应了一台实体服务器（虽然 Master Node 可以和其中一个 Worker Node 安装在同一台服务器，但是建议 Master Node 单独部署），**所有 Master Node 和 Worker Node 组成了 K8S 集群**，同一个集群可能存在多个 Master Node 和 Worker Node。

首先来看**Master Node**都有哪些组件：

- **API Server**。**K8S 的请求入口服务**。API Server 负责接收 K8S 所有请求（来自 UI 界面或者 CLI 命令行工具），然后，API Server 根据用户的具体请求，去通知其他组件干活。
- **Scheduler**。**K8S 所有 Worker Node 的调度器**。当用户要部署服务时，Scheduler 会选择最合适的 Worker Node（服务器）来部署。
- **Controller Manager**。**K8S 所有 Worker Node 的监控器**。Controller Manager 有很多具体的 Controller，在文章[Components of Kubernetes Architecture](https://medium.com/@kumargaurav1247/components-of-kubernetes-architecture-6feea4d5c712)中提到的有 Node Controller、Service Controller、Volume Controller 等。Controller 负责监控和调整在 Worker Node 上部署的服务的状态，比如用户要求 A 服务部署 2 个副本，那么当其中一个服务挂了的时候，Controller 会马上调整，让 Scheduler 再选择一个 Worker Node 重新部署服务。
- **etcd**。**K8S 的存储服务**。etcd 存储了 K8S 的关键配置和用户配置，K8S 中仅 API Server 才具备读写权限，其他组件必须通过 API Server 的接口才能读写数据（见[Kubernetes Works Like an Operating System](https://thenewstack.io/how-does-kubernetes-work/)）。

接着来看**Worker Node**的组件，笔者更赞同[HOW DO APPLICATIONS RUN ON KUBERNETES](https://thenewstack.io/how-do-applications-run-on-kubernetes/)文章中提到的组件介绍：

- **Kubelet**。**Worker Node 的监视器，以及与 Master Node 的通讯器**。Kubelet 是 Master Node 安插在 Worker Node 上的“眼线”，它会定期向 Worker Node 汇报自己 Node 上运行的服务的状态，并接受来自 Master Node 的指示采取调整措施。
- **Kube-Proxy**。**K8S 的网络代理**。私以为称呼为 Network-Proxy 可能更适合？Kube-Proxy 负责 Node 在 K8S 的网络通讯、以及对外部网络流量的负载均衡。
- **Container Runtime**。**Worker Node 的运行环境**。即安装了容器化所需的软件环境确保容器化程序能够跑起来，比如 Docker Engine。大白话就是帮忙装好了 Docker 运行环境。
- **Logging Layer**。**K8S 的监控状态收集器**。私以为称呼为 Monitor 可能更合适？Logging Layer 负责采集 Node 上所有服务的 CPU、内存、磁盘、网络等监控项信息。
- **Add-Ons**。**K8S 管理运维 Worker Node 的插件组件**。有些文章认为 Worker Node 只有三大组件，不包含 Add-On，但笔者认为 K8S 系统提供了 Add-On 机制，让用户可以扩展更多定制化功能，是很不错的亮点。

总结来看，**K8S 的 Master Node 具备：请求入口管理（API Server），Worker Node 调度（Scheduler），监控和自动调节（Controller Manager），以及存储功能（etcd）；而 K8S 的 Worker Node 具备：状态和监控收集（Kubelet），网络和负载均衡（Kube-Proxy）、保障容器化运行环境（Container Runtime）、以及定制化功能（Add-Ons）。**

到这里，相信你已经对 K8S 究竟是做什么的，有了大概认识。接下来，再来认识下 K8S 的 Deployment、Pod、Replica Set、Service 等，但凡谈到 K8S，就绕不开这些名词，而这些名词也是最让 K8S 新手们感到头疼、困惑的。

### **2.K8S 重要概念**

##### **2.1 Pod 实例**

官方对于**Pod**的解释是：

> **Pod**是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。

这样的解释还是很难让人明白究竟 Pod 是什么，但是对于 K8S 而言，Pod 可以说是所有对象中最重要的概念了！因此，我们**必须首先清楚地知道“Pod 是什么”**，再去了解其他的对象。

从官方给出的定义，联想下“最小的 xxx 单元”，是不是可以想到本科在学校里学习“进程”的时候，教科书上有一段类似的描述：资源分配的最小单位；还有”线程“的描述是：CPU 调度的最小单位。什么意思呢？**”最小 xx 单位“要么就是事物的衡量标准单位，要么就是资源的闭包、集合**。前者比如长度米、时间秒；后者比如一个”进程“是存储和计算的闭包，一个”线程“是 CPU 资源（包括寄存器、ALU 等）的闭包。

同样的，**Pod 就是 K8S 中一个服务的闭包**。这么说的好像还是有点玄乎，更加云里雾里了。简单来说，**Pod 可以被理解成一群可以共享网络、存储和计算资源的容器化服务的集合**。再打个形象的比喻，在同一个 Pod 里的几个 Docker 服务/程序，好像被部署在同一台机器上，可以通过 localhost 互相访问，并且可以共用 Pod 里的存储资源（这里是指 Docker 可以挂载 Pod 内的数据卷，数据卷的概念，后文会详细讲述，暂时理解为“需要手动 mount 的磁盘”）。笔者总结 Pod 如下图，可以看到：**同一个 Pod 之间的 Container 可以通过 localhost 互相访问，并且可以挂载 Pod 内所有的数据卷；但是不同的 Pod 之间的 Container 不能用 localhost 访问，也不能挂载其他 Pod 的数据卷**。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0Xw8tMnpYpyNAQn3o2FicHmuayH2do04NuzfQlvsrP66EyjE8O8omthHBA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

对 Pod 有直观的认识之后，接着来看 K8S 中 Pod 究竟长什么样子，具体包括哪些资源？

K8S 中所有的对象都通过 yaml 来表示，笔者从[官方网站](https://kubernetes.io/zh/docs/tasks/configure-pod-container/assign-memory-resource/)摘录了一个最简单的 Pod 的 yaml：

```
apiVersion: v1
kind: Pod
metadata:
  name: memory-demo
  namespace: mem-example
spec:
  containers:
  - name: memory-demo-ctr
    image: polinux/stress
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
    volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}
```

看不懂不必慌张，且耐心听下面的解释：

- `apiVersion`记录 K8S 的 API Server 版本，现在看到的都是`v1`，用户不用管。

- `kind`记录该 yaml 的对象，比如这是一份 Pod 的 yaml 配置文件，那么值内容就是`Pod`。

- `metadata`记录了 Pod 自身的元数据，比如这个 Pod 的名字、这个 Pod 属于哪个 namespace（命名空间的概念，后文会详述，暂时理解为“同一个命名空间内的对象互相可见”）。

- `spec`记录了 Pod 内部所有的资源的详细信息，看懂这个很重要：

- - `containers`记录了 Pod 内的容器信息，`containers`包括了：`name`容器名，`image`容器的镜像地址，`resources`容器需要的 CPU、内存、GPU 等资源，`command`容器的入口命令，`args`容器的入口参数，`volumeMounts`容器要挂载的 Pod 数据卷等。可以看到，**上述这些信息都是启动容器的必要和必需的信息**。
  - `volumes`记录了 Pod 内的数据卷信息，后文会详细介绍 Pod 的数据卷。

**2.2 Volume 数据卷**

K8S 支持很多类型的 volume 数据卷挂载，具体请参见[K8S 卷](https://kubernetes.io/zh/docs/concepts/storage/volumes/)。前文就“如何理解 volume”提到：“**需要手动 mount 的磁盘**”，此外，有一点可以帮助理解：**数据卷 volume 是 Pod 内部的磁盘资源**。

其实，单单就 Volume 来说，不难理解。但是上面还看到了`volumeMounts`，这俩是什么关系呢？

**volume 是 K8S 的对象，对应一个实体的数据卷；而 volumeMounts 只是 container 的挂载点，对应 container 的其中一个参数**。但是，**volumeMounts 依赖于 volume**，只有当 Pod 内有 volume 资源的时候，该 Pod 内部的 container 才可能有 volumeMounts。

##### **2.3 Container 容器**

本文中提到的镜像 Image、容器 Container，都指代了 Pod 下的一个`container`。关于 K8S 中的容器，在 2.1Pod 章节都已经交代了，这里无非再啰嗦一句：**一个 Pod 内可以有多个容器 container**。

在 Pod 中，容器也有分类，对这个感兴趣的同学欢迎自行阅读更多资料：

- **标准容器 Application Container**。
- **初始化容器 Init Container**。
- **边车容器 Sidecar Container**。
- **临时容器 Ephemeral Container**。

一般来说，我们部署的大多是**标准容器（ Application Container）**。

##### **2.4 Deployment 和 ReplicaSet（简称 RS）**

除了 Pod 之外，K8S 中最常听到的另一个对象就是 Deployment 了。那么，什么是 Deployment 呢？官方给出了一个要命的解释：

> 一个 *Deployment* 控制器为 [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 和 [ReplicaSets](https://kubernetes.io/zh/docs/concepts/workloads/controllers/replicaset/) 提供声明式的更新能力。
>
> 你负责描述 Deployment 中的 *目标状态*，而 Deployment 控制器以受控速率更改实际状态， 使其变为期望状态。你可以定义 Deployment 以创建新的 ReplicaSet，或删除现有 Deployment，并通过新的 Deployment 收养其资源。

翻译一下：**Deployment 的作用是管理和控制 Pod 和 ReplicaSet，管控它们运行在用户期望的状态中**。哎，打个形象的比喻，**Deployment 就是包工头**，主要负责监督底下的工人 Pod 干活，确保每时每刻有用户要求数量的 Pod 在工作。如果一旦发现某个工人 Pod 不行了，就赶紧新拉一个 Pod 过来替换它。

新的问题又来了：那什么是 ReplicaSets 呢？

> ReplicaSet 的目的是维护一组在任何时候都处于运行状态的 Pod 副本的稳定集合。因此，它通常用来保证给定数量的、完全相同的 Pod 的可用性。

再来翻译下：ReplicaSet 的作用就是管理和控制 Pod，管控他们好好干活。但是，ReplicaSet 受控于 Deployment。形象来说，**ReplicaSet 就是总包工头手下的小包工头**。

笔者总结得到下面这幅图，希望能帮助理解：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0Xw4ickITAbMdSSn2JYyhlT8n7rf3TOMZ3Th1vdniab4p4nwTGJogVytPbA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

新的问题又来了：**如果都是为了管控 Pod 好好干活，为什么要设置 Deployment 和 ReplicaSet 两个层级呢，直接让 Deployment 来管理不可以吗？**

回答：不清楚，但是私以为是因为先有 ReplicaSet，但是使用中发现 ReplicaSet 不够满足要求，于是又整了一个 Deployment（**有清楚 Deployment 和 ReplicaSet 联系和区别的小伙伴欢迎留言啊**）。

但是，从 K8S 使用者角度来看，用户会直接操作 Deployment 部署服务，而当 Deployment 被部署的时候，K8S 会自动生成要求的 ReplicaSet 和 Pod。在[K8S 官方文档](https://www.kubernetes.org.cn/replicasets)中也指出用户只需要关心 Deployment 而不操心 ReplicaSet：

> This actually means that you may never need to manipulate ReplicaSet objects: use a Deployment instead, and define your application in the spec section.
>
> 这实际上意味着您可能永远不需要操作 ReplicaSet 对象：直接使用 Deployments 并在规范部分定义应用程序。

补充说明：在 K8S 中还有一个对象 --- **ReplicationController（简称 RC）**，[官方文档](https://kubernetes.io/zh/docs/concepts/workloads/controllers/replicationcontroller/)对它的定义是：

> *ReplicationController* 确保在任何时候都有特定数量的 Pod 副本处于运行状态。换句话说，ReplicationController 确保一个 Pod 或一组同类的 Pod 总是可用的。

怎么样，和 ReplicaSet 是不是很相近？在[Deployments, ReplicaSets, and pods](https://www.ibm.com/cloud/architecture/content/course/kubernetes-101/deployments-replica-sets-and-pods/)教程中说“ReplicationController 是 ReplicaSet 的前身”，官方也推荐用 Deployment 取代 ReplicationController 来部署服务。

##### **2.5 Service 和 Ingress**

吐槽下 K8S 的概念/对象/资源是真的多啊！**前文介绍的 Deployment、ReplicationController 和 ReplicaSet 主要管控 Pod 程序服务；那么，Service 和 Ingress 则负责管控 Pod 网络服务**。

我们先来看看[官方文档](https://kubernetes.io/zh/docs/concepts/services-networking/service/)中 Service 的定义：

> 将运行在一组 [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 上的应用程序公开为网络服务的抽象方法。
>
> 使用 Kubernetes，您无需修改应用程序即可使用不熟悉的服务发现机制。Kubernetes 为 Pods 提供自己的 IP 地址，并为一组 Pod 提供相同的 DNS 名， 并且可以在它们之间进行负载均衡。

翻译下：K8S 中的服务（Service）并不是我们常说的“服务”的含义，而更像是网关层，是若干个 Pod 的流量入口、流量均衡器。

那么，**为什么要 Service 呢**？

私以为在这一点上，[官方文档](https://kubernetes.io/zh/docs/concepts/services-networking/service/)讲解地非常清楚：

> Kubernetes [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 是有生命周期的。它们可以被创建，而且销毁之后不会再启动。如果您使用 [Deployment](https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/) 来运行您的应用程序，则它可以动态创建和销毁 Pod。
>
> 每个 Pod 都有自己的 IP 地址，但是在 Deployment 中，在同一时刻运行的 Pod 集合可能与稍后运行该应用程序的 Pod 集合不同。
>
> 这导致了一个问题：如果一组 Pod（称为“后端”）为群集内的其他 Pod（称为“前端”）提供功能， 那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用工作量的后端部分？

补充说明：K8S 集群的网络管理和拓扑也有特别的设计，以后会专门出一章节来详细介绍 K8S 中的网络。这里需要清楚一点：K8S 集群内的每一个 Pod 都有自己的 IP（是不是很类似一个 Pod 就是一台服务器，然而事实上是多个 Pod 存在于一台服务器上，只不过是 K8S 做了网络隔离），在 K8S 集群内部还有 DNS 等网络服务（一个 K8S 集群就如同管理了多区域的服务器，可以做复杂的网络拓扑）。

此外，笔者推荐[k8s 外网如何访问业务应用](https://www.jianshu.com/p/50b930fa7ca3)对于 Service 的介绍，不过对于新手而言，推荐阅读前半部分对于 service 的介绍即可，后半部分就太复杂了。我这里做了简单的总结：

**Service 是 K8S 服务的核心，屏蔽了服务细节，统一对外暴露服务接口，真正做到了“微服务”**。举个例子，我们的一个服务 A，部署了 3 个备份，也就是 3 个 Pod；对于用户来说，只需要关注一个 Service 的入口就可以，而不需要操心究竟应该请求哪一个 Pod。优势非常明显：**一方面外部用户不需要感知因为 Pod 上服务的意外崩溃、K8S 重新拉起 Pod 而造成的 IP 变更，外部用户也不需要感知因升级、变更服务带来的 Pod 替换而造成的 IP 变化，另一方面，Service 还可以做流量负载均衡**。

但是，Service 主要负责 K8S 集群内部的网络拓扑。那么集群外部怎么访问集群内部呢？这个时候就需要 Ingress 了，[官方文档](https://kubernetes.io/zh/docs/concepts/services-networking/ingress/)中的解释是：

> Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。
>
> Ingress 可以提供负载均衡、SSL 终结和基于名称的虚拟托管。

翻译一下：Ingress 是整个 K8S 集群的接入层，复杂集群内外通讯。

最后，笔者把 Ingress 和 Service 的关系绘制网络拓扑关系图如下，希望对理解这两个概念有所帮助：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwaaRkshAhKGmeJuRwZNbz7aWQh5f9ftA33iaBarA62rfKQ34VKNpuxzg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### **2.6 namespace 命名空间**

和前文介绍的所有的概念都不一样，namespace 跟 Pod 没有直接关系，而是 K8S 另一个维度的对象。或者说，前文提到的概念都是为了服务 Pod 的，而 namespace 则是为了服务整个 K8S 集群的。

那么，namespace 是什么呢？

上[官方文档](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/namespaces/)定义：

> Kubernetes 支持多个虚拟集群，它们底层依赖于同一个物理集群。这些虚拟集群被称为名字空间。

翻译一下：**namespace 是为了把一个 K8S 集群划分为若干个资源不可共享的虚拟集群而诞生的**。

也就是说，**可以通过在 K8S 集群内创建 namespace 来分隔资源和对象**。比如我有 2 个业务 A 和 B，那么我可以创建 ns-a 和 ns-b 分别部署业务 A 和 B 的服务，如在 ns-a 中部署了一个 deployment，名字是 hello，返回用户的是“hello a”；在 ns-b 中也部署了一个 deployment，名字恰巧也是 hello，返回用户的是“hello b”（要知道，在同一个 namespace 下 deployment 不能同名；但是不同 namespace 之间没有影响）。前文提到的所有对象，都是在 namespace 下的；当然，也有一些对象是不隶属于 namespace 的，而是在 K8S 集群内全局可见的，官方文档提到的可以通过命令来查看，具体命令的使用办法，笔者会出后续的实战文章来介绍，先贴下命令：

```
# 位于名字空间中的资源
kubectl api-resources --namespaced=true

# 不在名字空间中的资源
kubectl api-resources --namespaced=false
```

不在 namespace 下的对象有：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0Xw40s4vIe4cxeFjx9J7WVT92MCuo48ImLUic2HW3SsgbOmDyH2EcFvibrQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

在 namespace 下的对象有（部分）：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0Xw7WujZuJjhdNqYK2DCCmC9Y2vutHqyYibzKT6j7530hKn1fcvZhTEoGQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### **2.7 其他**

K8S 的对象实在太多了，2.1-2.6 介绍的是在实际使用 K8S 部署服务最常见的。其他的还有 Job、CronJob 等等，在对 K8S 有了比较清楚的认知之后，再去学习更多的 K8S 对象，不是难事。

### **3. 配置 kubectl**

##### **3.1 什么是 kubectl？**

[官方文档](https://kubernetes.io/zh/docs/reference/kubectl/overview/)中介绍 kubectl 是：

> Kubectl 是一个命令行接口，用于对 Kubernetes 集群运行命令。Kubectl 的配置文件在$HOME/.kube 目录。我们可以通过设置 KUBECONFIG 环境变量或设置命令参数--kubeconfig 来指定其他位置的 kubeconfig 文件。

也就是说，可以通过 kubectl 来操作 K8S 集群，基本语法：

> 使用以下语法 `kubectl` 从终端窗口运行命令：
>
> ```
> kubectl [command] [TYPE] [NAME] [flags]
> ```
>
> 其中 `command`、`TYPE`、`NAME` 和 `flags` 分别是：
>
> - `command`：指定要对一个或多个资源执行的操作，例如 `create`、`get`、`describe`、`delete`。
> - `TYPE`：指定[资源类型](https://kubernetes.io/zh/docs/reference/kubectl/overview/#resource-types)。资源类型不区分大小写，可以指定单数、复数或缩写形式。例如，以下命令输出相同的结果:
>
> ```
> ```shell
> kubectl get pod pod1
> kubectl get pods pod1
> kubectl get po pod1
> - `NAME`：指定资源的名称。名称区分大小写。如果省略名称，则显示所有资源的详细信息 `kubectl get pods`。
> 
> 在对多个资源执行操作时，您可以按类型和名称指定每个资源，或指定一个或多个文件：
> 
> - 要按类型和名称指定资源：
>   - 要对所有类型相同的资源进行分组，请执行以下操作：`TYPE1 name1 name2 name<#>`。
>  例子：`kubectl get pod example-pod1 example-pod2`
>   - 分别指定多个资源类型：`TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE<#>/name<#>`。
>  例子：`kubectl get pod/example-pod1 replicationcontroller/example-rc1`
> - 用一个或多个文件指定资源：`-f file1 -f file2 -f file<#>`
>   - [使用 YAML 而不是 JSON](https://kubernetes.io/zh/docs/concepts/configuration/overview/#general-config-tips) 因为 YAML 更容易使用，特别是用于配置文件时。
>  例子：`kubectl get -f ./pod.yaml`
> - `flags`: 指定可选的参数。例如，可以使用 `-s` 或 `-server` 参数指定 Kubernetes API 服务器的地址和端口。
> ```

就如何使用 kubectl 而言，官方文档已经说得非常清楚。不过对于新手而言，还是需要解释几句：

1. kubectl 是 K8S 的命令行工具，并不需要 kubectl 安装在 K8S 集群的任何 Node 上，但是，需要确保安装 kubectl 的机器和 K8S 的集群能够进行网络互通。
2. kubectl 是通过本地的配置文件来连接到 K8S 集群的，默认保存在$HOME/.kube 目录下；也可以通过 KUBECONFIG 环境变量或设置命令参数--kubeconfig 来指定其他位置的 kubeconfig 文件【官方文档】。

接下来，一起看看怎么使用 kubectl 吧，切身感受下 kubectl 的使用。

请注意，如何安装 kubectl 的办法有许多非常明确的教程，比如《[安装并配置 kubectl](https://kubernetes.io/zh/docs/tasks/tools/install-kubectl/)》，本文不再赘述。

##### **3.2 怎么配置 kubectl？**

**第一步，必须准备好要连接/使用的 K8S 的配置文件**，笔者给出一份杜撰的配置：

```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: thisisfakecertifcateauthoritydata00000000000
    server: https://1.2.3.4:1234
  name: cls-dev
contexts:
- context:
    cluster: cls-dev
    user: kubernetes-admin
  name: kubernetes-admin@test
current-context: kubernetes-admin@test
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    token: thisisfaketoken00000
```

解读如下：

- `clusters`记录了 clusters（一个或多个 K8S 集群）信息：

- - `name`是这个 cluster（K8S 集群）的名称代号
  - `server`是这个 cluster（K8S 集群）的访问方式，一般为 IP+PORT
  - `certificate-authority-data`是证书数据，只有当 cluster（K8S 集群）的连接方式是 https 时，为了安全起见需要证书数据

- `users`记录了访问 cluster（K8S 集群）的账号信息：

- - `name`是用户账号的名称代号
  - `user/token`是用户的 token 认证方式，token 不是用户认证的唯一方式，其他还有账号+密码等。

- `contexts`是上下文信息，包括了 cluster（K8S 集群）和访问 cluster（K8S 集群）的用户账号等信息：

- - `name`是这个上下文的名称代号
  - `cluster`是 cluster（K8S 集群）的名称代号
  - `user`是访问 cluster（K8S 集群）的用户账号代号

- `current-context`记录当前 kubectl 默认使用的上下文信息

- `kind`和`apiVersion`都是固定值，用户不需要关心

- `preferences`则是配置文件的其他设置信息，笔者没有使用过，暂时不提。

**第二步，给 kubectl 配置上配置文件**。

1. **`--kubeconfig`参数**。第一种办法是每次执行 kubectl 的时候，都带上`--kubeconfig=${CONFIG_PATH}`。给一点温馨小提示：每次都带这么一长串的字符非常麻烦，可以用 alias 别名来简化码字量，比如`alias k=kubectl --kubeconfig=${CONFIG_PATH}`。

2. **`KUBECONFIG`环境变量**。第二种做法是使用环境变量`KUBECONFIG`把所有配置文件都记录下来，即`export KUBECONFIG=$KUBECONFIG:${CONFIG_PATH}`。接下来就可以放心执行 kubectl 命令了。

3. **$HOME/.kube/config 配置文件**。第三种做法是把配置文件的内容放到$HOME/.kube/config 内。具体做法为：

4. 1. 如果$HOME/.kube/config 不存在，那么`cp ${CONFIG_PATH} $HOME/.kube/config`即可；
   2. 如果如果 $HOME/.kube/config已经存在，那么需要把新的配置内容加到 $HOME/.kube/config 下。单单只是`cat ${CONFIG_PATH} >> $HOME/.kube/config`是不行的，正确的做法是：`KUBECONFIG=$HOME/.kube/config:${CONFIG_PATH} kubectl config view --flatten > $HOME/.kube/config` 。解释下这个命令的意思：先把所有的配置文件添加到环境变量`KUBECONFIG`中，然后执行`kubectl config view --flatten`打印出有效的配置文件内容，最后覆盖$HOME/.kube/config 即可。

请注意，上述操作的优先级分别是 1>2>3，也就是说，kubectl 会优先检查`--kubeconfig`，若无则检查`KUBECONFIG`，若无则最后检查$HOME/.kube/config，如果还是没有，报错。但凡某一步找到了有效的 cluster，就中断检查，去连接 K8S 集群了。

**第三步：配置正确的上下文**

按照第二步的做法，如果配置文件只有一个 cluster 是没有任何问题的，但是对于有多个 cluster 怎么办呢？到这里，有几个关于配置的必须掌握的命令：

- `kubectl config get-contexts`。列出所有上下文信息。

  ![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwBG2ZBE0ONEhcltc7tbhAPUu3nxcyEyezEuvbsO5FeR1tOODgibmoRibg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- `kubectl config current-context`。查看当前的上下文信息。其实，命令 1 线束出来的*所指示的就是当前的上下文信息。

  ![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0Xw2YVf2MObzvFCibfxJmgVTibY7SiaZ0vtSE4mDhUYM01vVCTLJ36gytckQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- `kubectl config use-context ${CONTEXT_NAME}`。更改上下文信息。

  ![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0Xwpn7yHyKIX9Onh2hBc3HEID0iaBTOTmYZrwv3dEERiaXUR8ibgPCMiabCPw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- `kubectl config set-context ${CONTEXT_NAME}|--current --${KEY}=${VALUE}`。修改上下文的元素。比如可以修改用户账号、集群信息、连接到 K8S 后所在的 namespace。

  ![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0Xwaiayk7TTibIRccxs8jMXib1UrCCOthzibkiaZGGWy1h1lm3n2WgL3NExl6g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

  关于该命令，还有几点要啰嗦的：

- - `config set-context`可以修改任何在配置文件中的上下文信息，只需要在命令中指定上下文名称就可以。而--current 则指代当前上下文。

  - 上下文信息所包括的内容有：cluster 集群（名称）、用户账号（名称）、连接到 K8S 后所在的 namespace，因此有`config set-context`严格意义上的用法：

    `kubectl config set-context [NAME|--current] [--cluster=cluster_nickname] [--user=user_nickname] [--namespace=namespace] [options]`

    （备注：[options]可以通过 kubectl options 查看）

综上，如何操作 kubectl 配置都已交代。

### **4.kubectl 部署服务**

K8S 核心功能就是部署运维容器化服务，因此最重要的就是如何又快又好地部署自己的服务了。本章会介绍如何部署 Pod 和 Deployment。

##### **4.1 如何部署 Pod？**

通过 kubectl 部署 Pod 的办法分为两步：1). 准备 Pod 的 yaml 文件；2). 执行 kubectl 命令部署

**第一步：准备 Pod 的 yaml 文件**。关于 Pod 的 yaml 文件初步解释，本系列上一篇文章《K8S 系列一：概念入门》已经有了初步介绍，这里再复习下：

```
apiVersion: v1
kind: Pod
metadata:
  name: memory-demo
  namespace: mem-example
spec:
  containers:
  - name: memory-demo-ctr
    image: polinux/stress
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
    volumeMounts:
    - name: redis-storage
      mountPath: /data/redis
  volumes:
  - name: redis-storage
    emptyDir: {}
```

继续解读：

- `metadata`，对于新入门的同学来说，需要重点掌握的两个字段：

- - `name`。这个 Pod 的名称，后面到 K8S 集群中查找 Pod 的关键字段。
  - `namespace`。命名空间，即该 Pod 隶属于哪个 namespace 下，关于 Pod 和 namespace 的关系，上一篇文章已经交代了。

- `spec`记录了 Pod 内部所有的资源的详细信息，这里我们重点查看`containers`下的几个重要字段：

- - `name`。Pod 下该容器名称，后面查找 Pod 下的容器的关键字段。

  - `image`。容器的镜像地址，K8S 会根据这个字段去拉取镜像。

  - `resources`。容器化服务涉及到的 CPU、内存、GPU 等资源要求。可以看到有`limits`和`requests`两个子项，那么这两者有什么区别吗，该怎么使用？在[What's the difference between Pod resources.limits and resources.requests in Kubernetes?](https://stackoverflow.com/questions/55047093/whats-the-difference-between-pod-resources-limits-and-resources-requests-in-kub)回答了：

    **`limits`是 K8S 为该容器至多分配的资源配额；而`requests`则是 K8S 为该容器至少分配的资源配额**。打个比方，配置中要求了 memory 的`requests`为 100M，而此时如果 K8S 集群中所有的 Node 的可用内存都不足 100M，那么部署服务会失败；又如果有一个 Node 的内存有 16G 充裕，可以部署该 Pod，而在运行中，该容器服务发生了内存泄露，那么一旦超过 200M 就会因为 OOM 被 kill，尽管此时该机器上还有 15G+的内存。

  - `command`。容器的入口命令。对于这个笔者还存在很多困惑不解的地方，暂时挖个坑，有清楚的同学欢迎留言。

  - `args`。容器的入口参数。同上，有清楚的同学欢迎留言。

  - `volumeMounts`。容器要挂载的 Pod 数据卷等。请务必记住：**Pod 的数据卷只有被容器挂载后才能使用**！

**第二步：执行 kubectl 命令部署**。有了 Pod 的 yaml 文件之后，就可以用 kubectl 部署了，命令非常简单：`kubectl create -f ${POD_YAML}`。

随后，会提示该命令是否执行成功，比如 yaml 内容不符合要求，则会提示哪一行有问题：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwPhD4JBdG4RScd3epywzJMFneKGRL29WkofibcWfFZ73cmXKGQU7k7gA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

修正后，再次部署：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwNEVqlrV83dAbc37tZX7z11XJhy3tXGW0BVA54d25X0pchfEEQHdKLg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### **4.2 如何部署 Deployment？**

**第一步：准备 Deployment 的 yaml 文件**。首先来看 Deployment 的 yaml 文件内容：

```
 apiVersion: extensions/v1beta1
 kind: Deployment
 metadata:
   name: rss-site
   namespace: mem-example
 spec:
   replicas: 2
   template:
     metadata:
       labels:
         app: web
     spec:
      containers:
       - name: memory-demo-ctr
         image: polinux/stress
         resources:
         limits:
           emory: "200Mi"
         requests:
           memory: "100Mi"
         command: ["stress"]
         args: ["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"]
         volumeMounts:
         - name: redis-storage
           mountPath: /data/redis
     volumes:
     - name: redis-storage
       emptyDir: {}
```

继续来看几个重要的字段：

- `metadata`同 Pod 的 yaml，这里提一点：如果没有指明 namespace，那么就是用 kubectl 默认的 namespace（如果 kubectl 配置文件中没有指明 namespace，那么就是 default 空间）。

- `spec`，可以看到 Deployment 的`spec`字段是在 Pod 的`spec`内容外“包了一层”，那就来看 Deployment 有哪些需要注意的：

- - `metadata`，新手同学先不管这边的信息。
  - `spec`，会发现这完完全全是上文提到的 Pod 的`spec`内容，在这里写明了 Deployment 下属管理的每个 Pod 的具体内容。
  - `replicas`。副本个数。也就是该 Deployment 需要起多少个相同的 Pod，**如果用户成功在 K8S 中配置了 n（n>1）个，那么 Deployment 会确保在集群中始终有 n 个服务在运行**。
  - `template`。

**第二步：执行 kubectl 命令部署**。Deployment 的部署办法同 Pod：`kubectl create -f ${DEPLOYMENT_YAML}`。由此可见，**K8S 会根据配置文件中的`kind`字段来判断具体要创建的是什么资源**。

这里插一句题外话：**部署完 deployment 之后，可以查看到自动创建了 ReplicaSet 和 Pod**，如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwlVqHX3aqo6xm1ibqNaibZ3rKOicn5noaAEnDfaSQXTj43lQ8NjkZEBzqA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

还有一个有趣的事情：**通过 Deployment 部署的服务，其下属的 RS 和 Pod 命名是有规则的**。读者朋友们自己总结发现哦。

综上，如何部署一个 Pod 或者 Deployment 就结束了。

### **5. kubectl 查看、更新/编辑、删除服务**

作为 K8S 使用者而言，更关心的问题应该是本章所要讨论的话题：如何通过 kubectl 查看、更新/编辑、删除在 K8S 上部署着的服务。

##### **5.1 如何查看服务？**

请务必记得一个事情：**在 K8S 中，一个独立的服务即对应一个 Pod。即，当我们说要 xxx 一个服务的就是，也就是操作一个 Pod**。而与 Pod 服务相关的且需要用户关心的，有 Deployment。

通过 kubectl 查看服务的基本命令是：

```
$ kubectl get|describe ${RESOURCE} [-o ${FORMAT}] -n=${NAMESPACE}
# ${RESOURCE}有: pod、deployment、replicaset(rs)
```

在此之前，还有一个需要回忆的事情是：Deployment、ReplicaSet 和 Pod 之间的关系 - 层层隶属；以及这些资源和 namespace 的关系是 - 隶属。如下图所示。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwRFoqfcv3mvhDbrtDianUF0CGZ69iaZTia7J4fvRyw3JvcNO8Xwe0stAmg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

因此，**要查看一个服务，也就是一个 Pod，必须首先指定 namespace**！那么，如何查看集群中所有的 namespace 呢？`kubectl get ns`：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0Xwkg4iaQuibpBI9Q02nI6plCqHy0vwt1LsNvq5NmLT8ibBgnZToyjgOEicNA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

于是，只需要通过`-n=${NAMESPACE}`就可以指定自己要操作的资源所在的 namespace。比如查看 Pod：`kubectl get pod -n=oona-test`，同理，查看 Deployment：`kubectl get deployment -n=oona-test`。

问题又来了：**如果已经忘记自己所部属的服务所在的 namespace 怎么办？这么多 namespace，一个一个查看过来吗？**

```
kubectl get pod --all-namespaces
```

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwvuLlao7Mmx4Q2wx8RiaUnpRK6NgaiaicsxWCjyTQTb0JOrqMgJj6wjx3g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

这样子就可以看到所有 namespace 下面部署的 Pod 了！同理，要查找所有的命名空间下的 Deployment 的命令是：`kubectl get deployment --all-namespaces`。

于是，就可以开心地查看 Pod：`kubectl get pod [-o wide] -n=oona-test`，或者查看 Deployment：`kubectl get deployment [-o wide] -n=oona-test`。

哎，这里是否加`-o wide`有什么区别吗？实际操作下就明白了，其他资源亦然：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwDsTrWRffgGkGSicEwTZZQD1zaDFLJAJ0u2Q44U2wticNWOe3LywN07NA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

哎，我们看到之前部署的 Pod 服务 memory-demo 显示的“ImagePullBackOff”是怎么回事呢？先不着急，我们慢慢看下去。

##### **5.2 如何更新/编辑服务？**

两种办法：1). 修改 yaml 文件后通过 kubectl 更新；2). 通过 kubectl 直接编辑 K8S 上的服务。

**方法一：修改 yaml 文件后通过 kubectl 更新**。我们看到，创建一个 Pod 或者 Deployment 的命令是`kubectl create -f ${YAML}`。但是，如果 K8S 集群当前的 namespace 下已经有该服务的话，会提示资源已经存在：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwXrpqsyug4JkXJIibdeAgGj6v5I4eibINwK808avEFCrLFicGBtL2ciajnA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

通过 kubectl 更新的命令是`kubectl apply -f ${YAML}`，我们再来试一试：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwclTRDSZUVbosAKJIjMCHlAIbSIMEV4EPqGXOTicibmD9ONWvEPVvB6gw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

（备注：命令`kubectl apply -f ${YAML}`也可以用于首次创建一个服务哦）

**方法二：通过 kubectl 直接编辑 K8S 上的服务**。命令为`kubectl edit ${RESOURCE} ${NAME}`，比如修改刚刚的 Pod 的命令为`kubectl edit pod memory-demo`，然后直接编辑自己要修改的内容即可。

但是请注意，无论方法一还是方法二，能修改的内容还是有限的，从笔者实战下来的结论是：**只能修改/更新镜像的地址和个别几个字段**。如果修改其他字段，会报错：

> The Pod "memory-demo" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`, `spec.initContainers[*].image`, `spec.activeDeadlineSeconds` or `spec.tolerations` (only additions to existing tolerations)

如果真的要修改其他字段怎么办呢？恐怕只能删除服务后重新部署了。

##### **5.3 如何删除服务？**

在 K8S 上删除服务的操作非常简单，命令为`kubectl delete ${RESOURCE} ${NAME}`。比如删除一个 Pod 是：`kubectl delete pod memory-demo`，再比如删除一个 Deployment 的命令是：`kubectl delete deployment ${DEPLOYMENT_NAME}`。但是，请注意：

- **如果只部署了一个 Pod，那么直接删除该 Pod 即可**；

  ![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwbHHLGOltdOASRbQ9Bt5hrZmhmkg0v9yc76mgRC1WUoOhicdTUnQ38vw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **如果是通过 Deployment 部署的服务，那么仅仅删除 Pod 是不行的，正确的删除方式应该是：先删除 Deployment，再删除 Pod**。

  ![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwcTxc1QtDRZNeESiazh6TsESZx9AibeRj8dIHeVfay75nGwp2HwJP3N3A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

关于第二点应该不难想象：仅仅删除了 Pod 但是 Deployment 还在的话，Deployment 定时会检查其下属的所有 Pod，如果发现失败了则会再拉起。因此，会发现过一会儿，新的 Pod 又被拉起来了。

另外，还有一个事情：有时候会发现一个 Pod 总也删除不了，这个时候很有可能要实施强制删除措施，命令为`kubectl delete pod --force --grace-period=0 ${POD_NAME}`。

### **6.kubectl 排查服务问题**

上文说道：部署的服务 memory-demo 失败了，是怎么回事呢？本章就会带大家一起来看看常见的 K8S 中服务部署失败、服务起来了但是不正常运行都怎么排查呢？

首先，祭出笔者最爱的一张 K8S 排查手册，来自博客《[Kubernetes Deployment 故障排除图解指南](https://tonybai.com/2019/12/08/k8s-deployment-troubleshooting/)》：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwAsWj5hj9A6xfMmUWWRJLznObQZicxf2JjGwb8ibZEqRlGIFeydOfibKCg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

哈哈哈，对于新手同学来说，上图还是不够友好，下面我们简单来看两个例子：

##### **6.1 K8S 上部署服务失败了怎么排查？**

请一定记住这个命令：`kubectl describe ${RESOURCE} ${NAME}`。比如刚刚的 Pod 服务 memory-demo，我们来看：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauLok8egCcGJbUzUQjEd0XwYRwE63AA98ibln2shEyWaU0wUKlmlHD5S2tge0AXTPiaogC0BhNdIdFg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

拉到最后看到`Events`部分，会显示出 K8S 在部署这个服务过程的关键日志。这里我们可以看到是拉取镜像失败了，好吧，大家可以换一个可用的镜像再试试。

一般来说，通过`kubectl describe pod ${POD_NAME}`已经能定位绝大部分部署失败的问题了，当然，具体问题还是得具体分析。大家如果遇到具体的报错，欢迎分享交流。

##### **6.2 K8S 上部署的服务不正常怎么排查？**

如果服务部署成功了，且状态为`running`，那么就需要进入 Pod 内部的容器去查看自己的服务日志了：

- 查看 Pod 内部某个 container 打印的日志：`kubectl log ${POD_NAME} -c ${CONTAINER_NAME}`。
- 进入 Pod 内部某个 container：`kubectl exec -it [options] ${POD_NAME} -c ${CONTAINER_NAME} [args]`，嗯，这个命令的作用是通过 kubectl 执行了`docker exec xxx`进入到容器实例内部。之后，就是用户检查自己服务的日志来定位问题。

显然，线上可能会遇到更复杂的问题，需要借助更多更强大的命令和工具。

### 7.**写在后面**

本文希望能够帮助对 K8S 不了解的新手快速了解 K8S。笔者一边写文章，一边查阅和整理 K8S 资料，过程中越发感觉 K8S 架构的完备、设计的精妙，是值得深入研究的，K8S 大受欢迎是有道理的。

原文作者：oonamao毛江云，腾讯 CSIG 应用开发工程师

原文链接：https://mp.weixin.qq.com/s/mUF0AEncu3T2yDqKyt-0Ow

# 【NO.236】万字详解：腾讯如何自研大规模知识图谱 Topbase

> Topbase 是由 TEG-AI 平台部构建并维护的一个专注于通用领域知识图谱，其涉及 226 种概念类型，共计 1 亿多实体，三元组数量达 22 亿。在技术上，Topbase 支持图谱的自动构建和数据的及时更新入库。此外，Topbase 还连续两次获得过知识图谱领域顶级赛事 KBP 的大奖。目前，Topbase 主要应用在微信搜一搜，信息流推荐以及智能问答产品。本文主要梳理 Topbase 构建过程中的技术经验，从 0 到 1 的介绍了构建过程中的重难点问题以及相应的解决方案，希望对图谱建设者有一定的借鉴意义。

### **1.简介**

知识图谱（ Knowledge Graph）以结构化的形式描述客观世界中概念、实体及其关系，便于计算机更好的管理、计算和理解互联网海量信息。通常结构化的知识是以图形式进行表示，图的节点表示语义符号（实体，概念），图的边表示符号之间的语义关系（如图 1 所示），此外每个实体还有一些非实体级别的边（通常称之为属性），如：人物的出生日期，主要成就等。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIQr7Kx5z63es9PsyQVaCohBNFFCfUPE81g37ePOdInHmTUmSHMCA68g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图1 知识图谱的示列

TEG-AI 平台部的 Topbase 是专注于通用领域知识。数据层面，TopBase 覆盖 51 个领域的知识，涉及 226 种概念类型，共计 1 亿多个实体，三元组数量达 22 亿多。技术层面，Topbase 已完成图谱自动构建和更新的整套流程，支持重点网站的监控，数据的及时更新入库，同时具备非结构化数据的抽取能力。此外，Topbase 还连续两次获得过知识图谱领域顶级赛事 KBP 的大奖，分别是 2017 年 KBP 实体链接的双项冠军，以及 2019 年 KBP 大赛第二名。在应用层面，Topbase 主要服务于微信搜一搜，信息流推荐以及智能问答产品。本文主要梳理 Topbase 构建过程中的重要技术点，介绍如何从 0 到 1 构建一个知识图谱，内容较长，建议先收藏。

### **2.知识图谱技术架构**

TopBase 的技术框架如图 2 所示，主要包括知识图谱体系构建，数据生产流程，运维监控系统以及存储查询系统。其中知识图谱体系是知识图谱的骨架，决定了我们采用什么样的方式来组织和表达知识，数据生产流程是知识图谱构建的核心内容，主要包括下载平台，抽取平台，知识规整模块，知识融合模块，知识推理模块，实体重要度计算模块等。Topbase 应用层涉及知识问答（基于 topbase 的 KB-QA 准确率超 90%），实体链接（2017 图谱顶级赛事 KBP 双料冠军），相关实体推荐等。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIeria6AMjSUL9vTKyobt9Q84tutPnMhbu7USvmLTUQUnvHwib18o4wrUw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图2 知识图谱Topbase的技术框架

1. **下载平台-知识更新**：下载平台是知识图谱获取源数据平台，其主要任务包括新实体的发现和新实体信息的下载。
2. **抽取平台-知识抽取**：下载平台只负责爬取到网页的源代码内容，抽取平台需要从这些源码内容中生成结构化的知识，供后续流程进一步处理。
3. **知识规整**：通过抽取平台以及合作伙伴提供的数据我们可以得到大量的多源异构数据。为了方便对多源数据进行融合，知识规整环节需要对数据进行规整处理，将各路数据映射到我们的知识体系中。
4. **知识融合**：知识融合是对不同来源，不同结构的数据进行融合，其主要包括实体对齐和属性融合。
5. **知识推理**：由于处理数据的不完备性，上述流程构建的知识图谱会存在知识缺失现象（实体缺失，属性缺失）。知识推理目的是利用已有的知识图谱数据去推理缺失的知识，从而将这些知识补全。此外，由于已获取的数据中可能存在噪声，所以知识推理还可以用于已有知识的噪声检测，净化图谱数据。
6. **实体知名度计算**：最后，我们需要对每一个实体计算一个重要性分数，这样有助于更好的使用图谱数据。比如：名字叫李娜的人物有网球运动员，歌手，作家等，如果用户想通过图谱查询“李娜是谁”那么图谱应该返回最知名的李娜（网球运动员）。

### **3.知识体系构建**

知识体系的构建是指采用什么样的方式来组织和表达知识，核心是构建一个本体（或 schema）对目标知识进行描述。在这个本体中需要定义：1）知识的类别体系（如：图 1 中的人物类，娱乐人物，歌手等）；2）各类别体系下实体间所具有的关系和实体自身所具有的属性；3）不同关系或者属性的定义域，值域等约束信息（如：出生日期的属性值是 Date 类型，身高属性值应该是 Float 类型，简介应该是 String 类型等）。我们构建 Topbase 知识体系主要是以人工构建和自动挖掘的方式相结合，同时我们还大量借鉴现有的第三方知识体系或与之相关的资源，如：Schema.org、Dbpedia、大词林、百科（搜狗）等。知识体系构建的具体做法：

1. 首先是定义概念类别体系：概念类别体系如图 1 的概念层所示，我们将知识图谱要表达的知识按照层级结构的概念进行组织。在构建概念类别体系时，必须保证上层类别所表示的概念完全包含下层类别表示的概念，如娱乐人物是人物类的下层类别，那么所有的娱乐人物都是人物。在设计概念类别体系时，我们主要是参考 schema.org、DBpedia 等已有知识资源人工确定顶层的概念体系。同时，我们要保证概念类别体系的鲁棒性，便于维护和扩展，适应新的需求。除了人工精心维护设计的顶层概念类别体系，我们还设计了一套上下位关系挖掘系统，用于自动化构建大量的细粒度概念（或称之为上位词），如：《不能说的秘密》还具有细粒度的概念：“青春校园爱情电影”，“穿越电影”。
2. 其次是定义关系和属性：定义了概念类别体系之后我们还需要为每一个类别定义关系和属性。关系用于描述不同实体间的联系，如：夫妻关系（连接两个人物实体），作品关系（连接人物和作品实体）等；属性用于描述实体的内在特征，如人物类实体的出生日期，职业等。关系和属性的定义需要受概念类别体系的约束，下层需要继承上层的关系属性，例如所有歌手类实体应该都具有人物类的关系和属性。我们采用半自动的方式生成每个概念类别体系下的关系属性。我们通过获取百科 Infobox 信息，然后将实体分类到概念类别体系下，再针对各类别下的实体关系属性进行统计分析并人工审核之后确定该概念类别的关系属性。关系属性的定义也是一个不断完善积累的过程。
3. 定义约束：定义关系属性的约束信息可以保证数据的一致性，避免出现异常值，比如：年龄必须是 Int 类型且唯一（单值），演员作品的值是 String 类型且是多值。

### **4.下载平台-知识更新**

知识更新主要包括两方面内容，一个是新出现的热门实体，需要被及时发现和下载其信息，另一个是关系属性变化的情况需要对其值进行替换或者补充，如明星的婚姻恋爱关系等。知识更新的具体流程如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIVYJia9S9YV3mpKM41vc59PjZXRJx7QCeAvUz6HpIKt2NesWvnPd7xow/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图3 Topbase知识更新流程图

1. 针对热门实体信息的更新策略主要有：

- 从各大站点主页更新，定时遍历重点网站种子页，采用广搜的方式层层下载实体页面信息；
- 从新闻语料中更新，基于新闻正文文本中挖掘新实体，然后拼接实体名称生成百科 URL 下载；
- 从搜索 query log 中更新，通过挖掘 querylog 中的实体，然后拼接实体生成百科 URL 下载。基于 querylog 的实体挖掘算法主要是基于实体模板库和我们的 QQSEG-NER 工具；
- 从知识图谱已有数据中更新，知识图谱已有的重要度高的实体定期重新下载；
- 从人工运营中更新，将人工（业务）获得的 URL 送入下载平台获取实体信息；
- 从相关实体中更新，如果某个热门实体信息变更，则其相关实体信息也有可能变更，所以需要获得热门实体的相关实体，进行相应更新。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIic2ov3jLQHIvaOKibpa9QMEez6Mwt8ibfGNiaSpsoEv25mwJzEsfOvdx6g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

表 1 最近 7 日下载数据统计情况

2.针对其他关系属性易变的情况，我们针对某些重要关系属性进行专项更新。如明星等知名人物的婚姻感情关系我们主要通过事件挖掘的方式及时更新，如：离婚事件会触发已有关系“妻子”“丈夫”变化为“前妻”“前夫”，恋爱事件会触发“男友”“女友”关系等。此外，基于非结构化抽取平台获得的三元组信息也有助于更新实体的关系属性。

### **5.抽取平台 - 知识抽取**

Topbase 的抽取平台主要包括结构化抽取，非结构化抽取和专项抽取。其中结构化抽取主要负责抽取网页编辑者整理好的规则化知识，其准确率高，可以直接入库。由于结构化知识的局限性，大量的知识信息蕴含在纯文本内容中，因此非结构化抽取主要是从纯文本数据中挖掘知识弥补结构化抽取信息的不足。此外，某些重要的知识信息需要额外的设计专项策略进行抽取，比如：事件信息，上位词信息（概念），描述信息，别名信息等。这些重要的知识抽取我们统称专项抽取，针对不同专项的特点设计不同的抽取模块。

**1. 结构化抽取平台**

许多网站提供了大量的结构化数据，如（图 4 左）所示的百科 Infobox 信息。这种结构化知识很容易转化为三元组，如：“<姚明，妻子，叶莉>”。针对结构化数据的抽取，我们设计了基于 Xpath 解析的抽取平台，如（图 4 右）所示，我们只需要定义好抽取网页的种子页面如：baike.com,然后从网页源码中拷贝 Infobox 中属性的 xpath 路径即可实现结构化知识的自动抽取，入库。通过结构化抽取平台生成的数据准确率高，因此无需人工参与审核即可直接入库，它是知识图谱的重要数据来源。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFI08IvI3JpGg5FlnBcZyksfNyTXDMuAhm1oiag5O0HD7KicuIu2jszzPMQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图4 Topbase结构化抽取平台的xpath配置界面

1. **非结构化抽取平台**

由于大量的知识是蕴含在纯文本中，为了弥补结构化抽取信息的不足，我们设计了非结构化抽取平台。非结构化抽取流程如图 5 所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIbC6I8YhOnh6pOD8BIXAmtPqXfyqDwHFX7yTBoQibdw4ZBatsOicoiaNNw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图5 Topbase非结构化抽取平台的技术框架

首先我们获取知识图谱中重要度高的实体名构建 Tri 树，然后回标新闻数据和百科正文数据，并将包含实体的句子作为候选抽取语料（新闻和百科数据需要区别对待，新闻数据往往包含最及时和最丰富的三元组信息，百科数据质量高，包含准确的知识，且百科摘要或正文描述相对简单，抽取结果的准确率高）。

然后，我们利用 Topbase 的实体链接服务，将匹配上的实体链接到知识库的已有实体中，避免了后期的数据融合。比如：实体“李娜”匹配到一句话是“歌手李娜最终归一了佛门”，那么这句话中的李娜会对应到知识库中的歌手李娜，而不是网球李娜，从这句话中抽取的结果只会影响歌手李娜的。实体链接之后，我们将候选语料送入我们的抽取服务，得到实体的三元组信息。

最后，三元组结果会和知识库中已有的三元组数据进行匹配并给每一个抽取得到的三元组结果进行置信度打分，如果知识库已经存在该三元组信息则过滤，如果知识库中三元组和抽取得到的三元组发生冲突则进入众包标注平台，如果三元组是新增的知识则根据他们的分值决定是否可以直接入库或者送入标注平台。此外，标注平台的结果数据会加入到抽取服务中 Fine-tune 模型，不断提升抽取模型的能力。

上述流程中的核心是抽取服务模块，它是非结构化抽取策略的集合。抽取服务构建流程如图 6 所示，其主要包括离线模型构建部分以及在线服务部分。离线模型构建的重点主要在于如何利用远监督的方式构建抽取模型的训练数据以及训练抽取模型。在线流程重点是如何针对输入的文本进行预处理，走不同的抽取策略，以及抽取结果的后处理。针对不同属性信息的特点，抽取策略主要可以简单归纳为三大类方法：

- 基于规则的抽取模块：有些属性具有很强的模板（规则）性质，所以可以通过人工简单的配置一些模板规则就可以获得高准确率的三元组结果。一般百科摘要文本内容描述规范，适合于规则抽取的输入数据源。此外，适用于规则抽取的属性主要有上位词，别名，地理位置，人物描述 tag 等。当然，规则模块召回有限往往还得搭配模型抽取模块，但是规则模块结果适合直接入库，无需标注人员审核。
- 基于 mention 识别+关系分类模块：基本思想是先用 NER 或者词典匹配等方式识别出句子中的 mention，然后利用已有的实体信息以及识别出来的 mention 进行属性分类。举例：给定识别出 mention 的句子“<org>腾讯</org>公司是由<per>马化腾</per>创立的。”,用 schema 对输入进行调整，一种情况是 org 作为头实体，per 作为尾实体，那么该样本的分类结果是关系“创始人”，另一种情况是 per 作为头实体，org 作为尾实体，那么该样本的分类结果是“所属公司”，所以最终可以得到三元组<腾讯，创始人，马化腾>和<马化腾，所属公司，腾讯>。一般人物，地点，机构，影视剧，时间等实体可以利用 qqseg-ner 识别。词典性质的实体如：职业，名族，国籍，性别等适合于词典匹配的方式识别。
- 基于序列标注模块：此外，还有许多属性值是无法进行 mention 识别，因此针对这类属性，我们采用一种序列标注的联合抽取方式来同时识别实体的属性值以及属性。这类属性主要有人物的“主要成就”信息，人物的描述 tag 信息，以及一些数值型属性信息。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIWibKjzickykjliajCUj26S09eAqDZsH1UFklwE6sNmB32XickSia09a2BJQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图6 Topbase的非结构化抽取服务

**3. 专项抽取**

专项抽取模块主要是针对一些重要知识的抽取。目前知识图谱设计的专项抽取内容主要有：上位词抽取（概念），实体描述抽取，事件抽取，别名抽取等。

**1 ) 上位词抽取**: 上位词可以理解为实体细粒度的概念，有助于更好的理解实体含义。图 7 是构建上位词图谱的一个简要流程图，其中主要从三路数据源中抽取上位词数据，主要包括：知识图谱的属性数据，百科人工标注 Tag，纯文本语料。由于抽取得到的上位词表述多样性问题，所以需要在抽取后进行同义上位词合并。此外，抽取生成的上位词图谱也会存在着知识补全的问题，所以需要进一步的进行图谱的连接预测，进行上位词图谱的补全。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFI2ObqZbOcaUJEmuw2dGIOM9CwW0GHgw8DMuXRg8VdwXeasLM6SKiatEQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图7 上位词抽取流程

**2) 实体描述 tag 抽取**: 实体描述 tag 是指能够描述实体某个标签的短句，图 7 是从新闻文本数据中挖掘到的实体“李子柒”的部分描述 tag。描述 tag 目前主要用于相关实体推荐理由生成，以及搜索场景中实体信息展示。描述 tag 抽取的核心模块以 QA-bert 为主的序列标注模型，query 是给定的实体信息，答案是句子中的描述片段。此外，还包括一系列的预处理过滤模块和后处理规整过滤模块。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFI2PqSPH8Y06UblmLt3t2uKMFWZVicqXLhJIE6NlJzs5GxMsfibrUmrVhQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图8  描述tag的示列说明

**3)事件抽取:** 事件抽取的目的是合并同一事件的新闻数据并从中识别出事件的关键信息生成事件的描述。事件抽取的基本流程如图 8 所示。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIUwt9m0STIpL8BEPoVTHpK4qttTozJibL4gicbh31cGsyUu7vnsUDFWqA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图9  Topbase的事件抽取流程框图

- 预处理阶段主要是对新闻流数据按照实体进行分堆处理。
- 事件聚类阶段主要是对每一堆的新闻数据进行关键词的提取等操作，将堆内的新闻进一步的聚类。
- 事件融合主要包括同批次事件融合和增量事件融合。事件抽取流程是分批次对输入数据进行处理。同批次事件融合主要解决不同实体属于同一事件的情况，将前一步得到的类簇进行合并处理。增量事件融合是将新增的新闻数据和历史 Base 的事件库进行增量融合。
- 最后，我们需要识别每一个事件类簇中的事件元素，过滤无效事件，生成事件的描述。

### **6.知识规整 - 实体分类**

知识规整目的是将实体数据映射到知识体系，并对其关系属性等信息进行去噪，归一化等预处理。如图 9 所示，左侧是从百科页面获取的武则天人物信息，右侧是从电影相关网站中获得的武则天信息，那么左侧的“武则天”应该被视为“人物类--历史人物--帝王”，右侧“武则天”应该被视为“作品--影视作品--电影”。左侧人物的“民族”属性的原始名称为“民族族群”，所以需要将其规整为 schema 定义的“民族”，这称之为属性归一。此外，由于不同来源的数据对实体名称会有不同的注释，如豆瓣的“武则天”这部电影后面加了一个年份备注，所以我们还需要对实体名进行还原处理等各种清洗处理。知识规整的核心模块是如何将实体映射到知识体系，即实体分类。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIKPTROibiazMfNJtIYYZ5D50AibdiaH2SKxajoC7PCwgeBp9z87BiabVYDvA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图10 数据规整的示列说明

**1. 实体分类的挑战**：

- 概念类别多（200+类），具有层次性，细分类别差异小（电影，电视剧）；
- 实体属性存在歧义：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIYVqmbP6c5nxN9uqVfTmAywlvq8rEmUuiccqaQ9Vbib3UiapoZ9Gt534jw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图11 实体分类中属性歧义问题

- 实体名称或者实体简介信息具有迷惑性：例如实体"菅直人"是一个政治家，其名称容易和民族类别混淆，电影“寄生虫”简介如下图所示，其内容和人物概念极其相似。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFI84cDMZCGtDLPPEtdWY2KKRSgumLm8NGBq3C0lpAC3Sc5t4hlibkKQ8g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图12 实体分类中简介迷惑性问题

**2.实体分类方法**：实体分类本质是一个多分类问题。针对知识库的特点以及上述挑战，我们分别从训练样本构建，特征选择以及模型设计三方面实现实体分类模块。

**1 ）实体分类的训练样本构建**：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFICqy6JYUbfXgWBTicSRSOTh7ib5bvob77WjjXIsYogBnEUEj60ictFvfwA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图13 实体分类训练数据构建流程

- **属性规则模块**：每个实体页面包含了实体结构化属性信息，利用这些属性字段可以对实体进行一个规则的分类。如：人物类别的实体大多包含民族，出生日期，职业等字段，歌手类实体的职业字段中可能有“歌手”的属性值。通过构建正则式规则，可以批量对实体页面进行分类。基于规则模块得到的类别信息准确率高，但是泛化能力弱，它的结果既可以作为后续分类模型的训练数据 1 也可以作为实体分类的一路重要分类结果。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIGQ59RIWVnvKEpC18VnTJ07qDiaQulAu6AlBBU91eicVoXhb2SIJwWib8A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图14 Topbase中用于实体分类的属性规则配置页面

- **简介分类模块**：简介分类模块以规则模块的数据作为训练数据，可以得到一个以简介为实体分类依据的分类模型，然后基于该模型预测属性规则模块无法识别的实体，选择高置信度的结果作为训练数据 2。
- **自动构建的训练数据去噪模块**：基于规则和简介分类模块可以得到部分分类样本，但是这些训练样本不可避免的会引入噪声，所以我们引入 N-折交叉训练预测自清洗数据，进一步保留高置信的训练样本，清洗思路如下图所示。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIsvyYtasJUMRibLU4RLwicFPfctEyjib8OREc9whkKfkTEU94hkNB9o3Xg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图15 实体分类训练数据自清洗流程

- **运营模块**：运营模块主要包括日常 badcase 收集以及标注人员审核的预测置信度不高的样本。运营数据会结合自动构建数据，联合训练最终的实体分类模型。

**2） 实体分类的特征选择**：

- 属性名称：除了通用类的属性名称，如：中文名，别名，正文，简介等，其他属性名称都作为特征；
- 属性值：不是所有的属性值都是有助于实体分类，如性别的属性值“男”或者“女”对区分该实体是“商业人物”和“娱乐人物”没有帮助，但是职业的属性值如“歌手”“CEO”等对于实体的细类别则有很强的指示作用，这些属性值可以作为实体细分类的重要特征。一个属性值是否需要加入他的属性值信息，我们基于第一部分得到的训练数据，利用特征选择指标如卡方检验值，信息增益等进行筛选。
- 简介：由于简介内容相对较长且信息冗余，并非用得越多越好。针对简介的利用我们主要采用百科简介中头部几句话中的主语是该实体的句子。

**3） 实体分类模型**

- 模型架构：基于 bert 预训练语言模型的多 Label 分类模型

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIo31UXXCm3QhCO3Bet7dJsItWYEnLDVdITib6OuCQ5VIccMdjCddmbsg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图16 实体分类基础模型

- 模型输入：我们针对上述特征进行拼接作为 bert 的输入，利用[sep]隔开实体的两类信息，每一类信息用逗号隔开不同部分。第一类信息是实体名称和实体简介，刻画了实体的一个基本描述内容，第二类信息是实体的各种属性，刻画了实体的属性信息。例如，刘德华的输入形式如下：

  ![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIicGj0Qu7AibDEDSEzDLicjpCWYZmA90JfN130rqcib7hx3nm9u1Pr7zr2w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

  图17  实体分类模型的输入形式

- 模型 loss：基于层次 loss 方式，实体 Label 是子类：父类 Label 要转换为正例计算 loss；实体 Label 是父类：所有子类 label 以一定概率 mask 不产生负例 loss，避免训练数据存在的细类别漏召回问题。

### 7.知识融合 - 实体对齐

知识融合的目的是将不同来源的数据进行合并处理。如从搜狗百科，体育页面以及 QQ 音乐都获取到了"姚明"信息，首先需要判断这些来源的"姚明"是否指同一实体，如果是同一个实体（图 18 中的搜狗和虎扑的姚明页面）则可以将他们的信息进行融合，如果不是（QQ 音乐的姚明页面）则不应该将其融合。知识融合的核心是实体对齐，即如何将不同来源的同一个实体进行合并。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIehnVRyExeb3oacaxkCMFBzkgrbVB99wlr96A42afHSw38xrdqJqbcg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIgFCPw7QHEiaOAL4q5rUbL0Fef1J6lv1icvAXkCluWTMumIicpficgFPkTw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



图18  知识融合示列说明

\1. **实体对齐挑战**

- 不同来源实体的属性信息重叠少，导致相似度特征稀疏，容易欠融合；

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIf8XpTWHQM5F0MGrCv97BUWElzSeE8EyN90VK4DgoElmmicAqxxRaAMg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFISHPUJZThOPVdlf0ZDwFwabx5h7jUcnTA12LV6QAuPueWoygd94naSg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图19  来自于百科和旅游网站的武夷山页面信息

- 同系列作品（电影，电视剧）相似度高，容易过融合，如两部还珠格格电视剧

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIciauWHbgjghNPvR5RlhBsng1YJAbJZjIcIBSTAcFp19yQFsKriaJTCMA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIV2XLAGgYKNn4drFEl1SAFBa0pn5puJrw8Kwtywicj1hiaEM2OLCOvzlA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图20  两部还珠格格的信息内容

- 多路来源的实体信息量很大（亿级别页面），如果每次进行全局融合计算复杂度高，而且会产生融合实体的 ID 漂移问题。

\2. **实体对齐的解决思路**

实体对齐的整体流程如图所示，其主要环节包括数据分桶，桶内实体相似度计算，桶内实体的聚类融合。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIDoLoU9zWiaj0xIuLy0lA8e16O4d1PTHicsSfNGficqB631ZlOqyJGcpUA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图21  Topbase实体对齐流程图

**1)数据分桶**：数据分桶的目的是对所有的多源实体数据进行一个粗聚类，粗聚类的方法基于简单的规则对数据进行分桶，具体规则主要是同名（原名或者别名相同）实体分在一个桶内，除了基于名称匹配，我们还采用一些专有的属性值进行分桶，如出生年月和出生地一致的人物分在一个桶。

**2)实体相似度计算**：实体相似度直接决定了两个实体是否可以合并，它是实体对齐任务中的核心。为了解决相似属性稀疏导致的欠融合问题，我们引入异构网络向量化表示的特征，为了解决同系列作品极其相似的过融合问题，我们引入了互斥特征。

- 异构网络向量化表示特征：每个来源的数据可以构建一个同源实体关联网络，边是两个实体页面之间的超链接，如下图所示，百科空间可以构建一个百科实体关联网络，影视剧网站可以构建一个影视剧网站的实体关联网络。不同空间的两个实体，如果存在高重合度信息，容易判别二者相似度的两个实体，可以建立映射关系（如影视剧网站的梁朝伟页面和百科的梁朝伟页面信息基本一致，则可以认为二者是同一个实体，建立链接关系），这样可以将多源异构网络进行合并，梁朝伟和刘德华属于连接节点，两个无间道重合信息少，则作为两个独立的节点。然后基于 deepwalk 方式得到多源异构网络的节点向量化表示特征。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIDmmB6VLue7Q2EwMd5DMc85Gw0gJxLFIzkUSBBARra49cbJY75LZicJQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图22 多源异构网络关联图

- 文本相似特征：主要是针对存在简介信息的实体，利用 bert 编码得到向量，如果两个实体都存在简介信息，则将两个简介向量进行点乘得到他们的文本相似度特征；
- 基本特征：其他属性的相似度特征，每一维表示属性，每一维的值表示该属性值的一个 Jaccard 相似度；
- 互斥特征：主要解决同系列作品及其相似的问题，人工设定的重要区分度特征，如电视剧的集数，系列名，上映时间。
- 最后，按照下图结构将上述相似度特征进行融合预测两两实体是否是同一实体；

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIU1fo3uIxeeeNOAeLcg9zBJ2y1MsxYgkqicbXA30YEEJPUKEoZpic5xug/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图23 实体对相似度打分模块

**3) 相似实体的聚类合并：**

- **Base 融合**：在上述步骤的基础上，我们采用层次聚类算法，对每一个桶的实体进行对齐合并，得到 base 版的融合数据，然后赋予每一个融合后的实体一个固定的 ID 值，这就得到了一个 Base 的融合库；
- **增量融合**：对于每日新增的实体页面信息，我们不再重新进行聚类处理，而是采用“贴”的模式，将每一个新增实体页面和已有的融合实体进行相似度计算，判断该实体页面应该归到哪一个融合实体中，如果相似度都低于设置的阈值，则该新增实体独立成一堆，并设置一个新的融合实体 ID。增量融合的策略可以避免每次重复计算全量实体页面的融合过程，方便数据及时更新，同时保证各个融合实体的稳定性，不会轻易发生融合实体 ID 的漂移问题；
- **融合拆解**：由于 Base 融合可能存在噪声，所以我们增加了一个融合的修复模块，针对发现的 badcase，对以融合成堆的实体进行拆解重新融合，这样可以局部修复融合错误，方便运营以及批量处理 badcase。

### 8.知识关联和推理

**知识关联（链接预测）**是将实体的属性值链接到知识库的实体中，构建一条关系边，如图 24 所示“三国演义”的作者属性值是“罗贯中”字符串，知识关联需要将该属性值链接到知识库中的实体“罗贯中”，这样实体“三国演义”和“罗贯中”之间存在一条“作者”的关系边。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIfjQ6BsVT9GlAW3BO52tnicgRJtRqrr9tke4tALqWttdYiaQ9Xic1wTib1w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图24  基于超链接关联的示列说明

Topbase 的知识关联方案分为基于超链接的关联和基于 embedding 的文本关联两种方式。超链接关联是 Topbase 进行关联和推理的第一步，它是利用网页中存在的超链接对知识图谱中的实体进行关联，如百科“三国演义”页面中，其“作者”属性链接到“罗贯中”的百科页面（如图 24 所示），基于这种超链接的跳转关系，可以在 Topbase 的实体之间建立起一条边关系，如该示列会在实体“三国演义”与“罗贯中”之间生成一条“作者”关系，而“曹操”并没有该超链接，所以三国演义的主要人物属性中的字符串“曹操”不会关联到具体的实体页面中。在进行超链接关联之前，Topbase 中的实体是一个个孤立的个体，超链接关联为知识图谱补充了第一批边关系，但是超链接关联无法保证链接的覆盖率。

基于此，Topbase 提出基于 embedding 的文本关联。基于 embedding 的文本关联是在已知头实体、关系的基础上，在候选集中对尾实体进行筛选，尾实体的候选集是通过别名匹配召回。如上述百科示列中的“主要人物”属性，我们利用其属性值字符串”曹操“去 Topbase 库里匹配，召回所有和”曹操”同名称的实体作为建立链接关系的候选。然后利用知识库 embedding 的方法从候选实体中选择最相似的实体作为他的链接实体。基于文本名称的匹配召回候选可以大大提高知识库 embeding 方法的链接预测效果。基于 embedding 的链接关系预测是通过模型将实体和关系的属性信息、结构信息嵌入到一个低维向量中去，利用低维向量去对缺失的尾实体进行预测。

当前采用的嵌入模型是 TextEnhanced+TransE，模型结构如图 25 所示。TransE 是将实体与关系映射到同一向量空间下，它是依据已有的边关系结构对实体之间的边关系进行预测，对孤立实体或链接边较少的实体预测效果较差。为了引入文本信息，解决模型对孤立实体预测的难题，模型使用 TextEnhanced 对文本信息进行嵌入。TextEnhanced 通过 NN 模型对文本信息嵌入后，利用 Attention 机制将文本信息嵌入到 Trans 系列的实体向量中，进而对尾实体进行预测。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIznCR1ZCPgpvjLLHj1siaA28lPYUlXJpUq6EOloXibBBvVwf3N1VrDgQA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图25  TextEnhanced+TransE结构图

由于知识关联是在已知属性值的前提下，通过名称匹配的方式得到关联实体的候选集，所以知识关联无法补充缺失属性值的链接关系。如上图中“三国演义”的信息中并没有“关羽”，知识推理目的是希望能够挖掘“三国演义”和“关羽”的潜在关系。为了保证图谱数据的准确率，Topbase 的知识推理主要以规则推理为主，具体的规则方法可以归纳为以下几类：

- **伴随推理**是在已经被链接的两个实体之间，根据两个实体的属性信息，发现两者间蕴含的其它关系。例如实体 A 已经通过“配偶”关系与实体 B 相连，实体 A 的性别为“男”，实体 B 的性别为“女”，则伴随推理会生成一条“妻子”关系边，将实体 A 与实体 B 链接在一起，代表 B 为 A 的妻子。伴随推理的规则可以通过统计同时关联起两个实体的属性共现比例得到。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFI0qgK8samnuiclxr6tqXhsnuKNibE4fqK94BT5wlIALkVDc3tnjKyIwKA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图26  伴随推理的示列说明

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIPibuzCrQFcN5ePpeCJDzYkDxp3CAvDKXSJpfSHgcHY9l9icG5Ap50FhA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

表2 Topbase的伴随推理规则库示列

- **反向推理**是依据边之间的互反关系，为已经链接的两个实体再添加一条边。比如实体 A 通过“作者”边与实体 B 相连，代表实体 B 是实体 A 的作者，则可以直接生成一条从实体 B 指向实体 A 的“作品”边，代表实体 A 是实体 B 的作品，因为“作品”与“作者”是一条互反关系。反向推理与伴随推理类似，都是在已经存在边关系的实体之间，挖掘新的边关系，不同的是，伴随推理在生成边关系时需要满足一定的属性条件，如上例中的“性别”限制，而反向推理直接通过已有的边关系，无需参考其它属性值，直接生成一条互反边关系。反向推理规则可以通过统计 A-B，B-A 的属性共现数量筛选。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIugcoRt2CxvtgAVCvOHGibiaWzBUibUWSv27z798naer2JYibNERY6GetLw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图27  反向推理的示列说明

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIbiabsZ771BdSLyODJuLMuFvRUtVvXkHhV4UjW5mm4cxkJpukyx7ZyDQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

表3 Topbase的反向关联规则库示列

- **多实体推理**是在多个实体之间挖掘蕴含的边关系，是一种更复杂的关联规则，如第一种形式：A 的父亲是 B，B 的母亲是 C，则 A 的奶奶是 C，该形式通过统计 A+PATH = C，A+R0=C，情况得到规则  [PATH(R1R2)=R0]；第二种形式是 A 的母亲是 B，A 的儿子 C，则 B 的孙子是 C，该形式通过统计：A+R1 = B，A+R2=C，B+R0=C 的情况，得到规则[R1 &R2 = R0]。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFISFvibM0NBywwzbEI1Qus3aSE1ib7O1a0kFCEYwmg372lpQ4CsCuwia0UA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIiaYpOialwkX0prU4bibiag0ZpdGyvS7ojx038nQWkVkElXAfKfP7pCFdZg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图28 多实体推理的两种形式示列说明

### 9.实体知名度计算

实体的知名度（Popularity）指标可以用于量化不同实体的重要性程度，方便我们更好的使用图谱数据。Topbase 知识库的 popularity 计算以基于实体链接关系的 pagerank 算法为核心，以对新热实体的 popularity 调整为辅，并配以直接的人工干预来快速解决 badcase。具体地，首先抽取实体页面之间的超链接关系，以此为基础通过修改后的 pagerank 算法来计算所有实体的 popularity；对于难以通过 pagerank 算法计算的新热实体的 popularity，再进行规则干预。最后对于仍然难以解决的 case，则直接对其 popularity 值进行人工赋值。Popularity 计算模块的整体流程如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIupGBMAZsuchrLiaOM7Lv5dLP9tUpJCMyJxnKF2SlPEcBnyssr3cMk9A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图29  Topbase实体知名度计算流程

- **多类型边关系的 pagerank 算法：** 基于链接关系的 popularity 计算方法的出发点在于：一个实体 A 对另一个实体 B 的引用（链接），表示实体 A 对于实体 B 的认可，链接到 B 的实体越多，表示 B 受到的认可越多，由此推断它的知名度也就越高。但实际上有很多的链接关系并不是出于“认可”而产生的，只是简单的表示它们之间有某种关系。比如歌手与专辑、音乐之间的各种关系。一个专业的音乐网站会收录歌手、专辑、音乐之间的完整从属关系，这会导致同一个歌手或同一张专辑之内的热门歌曲与其它歌曲之间没有任何区分性。并且由于这几类实体之间高密度的链接关系，会导致它们的计算结果比其它类别的实体的都高出很多。

  因此有必要对实体之间不同的链接关系进行区别对待。与最基础的 pagerank 算法的不同在于：实体之间可以有多条边，且有多种类型的边。在进行迭代计算的过程中，不同类型的边对流经它的概率分布会有不同程度的拟制作用。之所以进行这样的修改，是因为知识库中实体的信息有多种不同的来源。有的实体来源于通用领域百科，有的实体来源于垂类领域网站等。甚至同一个实体内部，不同的属性信息也会有不同的来源。由此，实体之间的链接关系也会属于不同的来源。比如“刘德华”与“朱丽倩”之间的“夫妻”关系可能抽取自百科，而与“无间道”之间的“参演”关系可能来自于电影网站。不同来源的信息有着不同的可信度，有的经过人工的审核编辑，可信度很高；而有的则属于算法自动生成，会有不同程度的错误。

  因此链接关系之间也有可信度的差别，无法做到将它们一视同仁地看待。其次，有的链接关系即使在可靠性方面完全正确，但它们对于 popularity 的正确计算不仅没有太大帮助，反而会导致 popularity 的计算结果与预期不符。修改后的 pagerank 算法的计算过程与基础 pagerank 算法基本一致，只是在进行分布概率的流转时有所区别。下面进行举例说明：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIYozR3iaczaodQic8JCuvYsbEKudBIY9COTg5wialfobSSDDvDmJiakfVgQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图30  多类型边的PageRank算法说明

实体 A 指向实体 B、C、D。其与 B 之间的链接类型为 X，与 C 之间的链接类型为 Y，与 D 之间的为 Z。通过先验知识或实验总结，我们认为链接类型 Y 可信性不高，相比于 X，对 rank 值的流转有拟制作用，因此对其赋予一个系数 0.8，Z 的可信度很准确，但其性质与上述的音乐网站的关系类似，因此对于其赋予一个系数 0.2，而 X 类型的完全可行，其系数则为 1.0。在某一迭代阶段，实体 A 的 rank 值为 3，B、C、D 的 rank 值分别为 4、2、3。由于 A 有 3 条出边，因此到 B、C、D 的初始流出值均为 3/ 3 = 1。加上系数的影响，实际到 C、D 的流出值分别为 0.8 和 0.2，未流出的剩余值为(1 -0.8) + (1 - 0.2) = 1.0。

因此迭代过后，B、C、D 的 rank 值分别为 4 + 1.0 = 5，2 + 0.8= 2.8，3 + 0.2 =3.2，而 A 的 rank 值需要在所有指向它的实体流入到它的值之和的基础上，再加上未流出的 1.0。

- **新热实体的 Popularity 调整：**新热实体的含义为最新出现的热门实体。这类实体需要较高的 popularity 值。但由于是新近出现的实体，其与其它实体的链接关系非常匮乏，因此无法通过基于实体链接关系的这类方法来计算。对此我们采取的方案侧重于对新热实体的发现，然后对发现的新热实体的 popularity 进行调整，使其 popularity 值在同名实体中处于最高的位置。新热实体的发现目前基于两类方法：一类方法发现的热门实体可以直接对应到知识库中的某个实体，另一个方法只能发现热门的实体名，需要通过一些对齐方法与知识库中的某个实体关联起来。

  第一种方法从 Topbase 监控的重点网站页面中直接获取最近热门的实体。这种方法获取的实体可以直接通过 url 与知识库中的某个实体准确无误地关联起来。第二类方法首先发现一些热门的实体名，包括：一、从微博热搜榜中爬取热门话题，通过命名实体识别方法识别其中的人名和机构名，将其作为热门实体名；二、将新闻中每天曝光的高频次标签作为实体名。以上两种方法发现的实体名带有一定的附加信息，通过实体链接可以将其对齐到知识库中的某个实体。

### 10.知识库的存储和查询

知识图谱是一种典型的图结构数据集合，实体是图中的节点，关系（属性）是带有标签的边。因此，基于图结构的存储方式能够直接正确地反映知识图谱的内部结构，有利于知识的查询。如下图所示，红色圈代表实体，实线是边（妻子），表示实体间的关系，如“刘德华的妻子是朱丽倩”，*虚线*是属性（出生日期），表示实体具有的属性，如“刘德华的出生日期是 1961 年 9 月 27 日”。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIEmOjlh67LKWLo7828VZIJgkSEXaH3FhxMqP81DJnNRwU6UGK3M6vBA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图31 图数据说明

Topbase 知识图谱的存储是基于分布式图数据库 JanusGraph，选择 JanusGraph 的主要理由有：1）JanusGraph 完全开源，像 Neo4j 并非完全开源；2）JanusGraph 支持超大图，图规模可以根据集群大小调整；3）JanusGraph 支持超大规模并发事务和可操作图运算，能够毫秒级的响应在海量图数据上的复杂的遍历查询操作等。

**Topbase**基于**JanusGraph**存储查询架构如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav5tj5KtuTS1I7BMtgFSSFIEyxYWjwdtknPqw1xdXfLib6RNoZIE9NBtcicB19BxxH1uENjaIicv4Qvg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图32  基于JanusGraph的存储查询系统

- Graph_Loader 模块主要是将上述数据生产流程得到的图谱数据转换为 JanusGraph 存储要求的格式，批量的将图谱数据写入图数据库存储服务中，以及相关索引建立。
- 图数据库存储服务：**JanusGraph**数据存储服务可以选用 ScyllaDb、HBase 等作为底层存储，topbase 选用的是 ScyllaDb。Graph_loader 会每天定时的将数据更新到图数据库存储服务。
- 图数据库索引：由于 JanusGraph 图数据库存储服务只支持一些简单查询，如：“刘德华的歌曲”，但是无法支持复杂查询，如多条件查询：“刘德华的 1999 年发表的粤语歌曲”。所以我们利用 Es 构建复杂查询的数据索引，graph_loader 除了批量写入数据到底层存储之外，还会建立基于复杂查询的索引。
- 图数据库主服务：主服务通过 Gremlin 语句对图数据库的相关内容进行查询或者改写等操作。

### **11.总结**

由于知识图谱的构建是一项庞大的数据工程，其中各环节涉及的技术细节无法在一篇文档中面面俱到。本文主要梳理 Topbase 构建过程中的技术经验，从 0 到 1 的介绍了图谱构建流程，希望对图谱建设者有一定的借鉴意义。

原文作者：郑孙聪，腾讯 TEG 应用研究员

原文链接：https://mp.weixin.qq.com/s/Qp6w7uFcgqKXzM7dWhYwFg

# 【NO.237】浅谈如何搭建知识体系

> 在电影《教父》中，有一句台词：“在一秒钟内看到本质的人和花半辈子也看不清一件事本质的人，自然是不一样的命运。”

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav1C1dASM293M0xYPjgzlJTy5ThgcxoZCTEKlxJR60zk4m79uibQMaiapNnzUBJROWw0sRDRibpDWFzg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

为什么你只能看见的是豹子身上的花斑？为什么看到本质的人和他人的命运会不同呢？

这是因为他人的知识水平比你更加全面，更加深刻。知识的全面性能让你的选择面更广，每多掌握一门知识，就多一种未来。其次它能够让你的主动权更强，本领域的专精能让你更早接触新的知识领域，获得先发优势。以一道有趣的题目为例：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav1C1dASM293M0xYPjgzlJTEwknUgTr2ibiccH7wxCOFdO7Mib5JE65FRiamhNexfFcY7DEuq3mHorgYg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

有一只熊掉到一个陷阱里，陷阱深 19.617 米，下落时间正好 2 秒，求熊是什么颜色的?  

看到这道题的陷阱深度以及下落时间，相信朋友们会很容易联想到重力加速度，可是题目的却让我们选择熊的颜色，这样的题目应该怎么解析呢？ 

其答案如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav1C1dASM293M0xYPjgzlJTzdMK68SLicicbChXxpsTLsGBWLoiam9eF32nqfmlFoWsr0hwhT3yiay0jQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

在这里我们无须过于细究其解题实际步骤，但它解题的方式却使用了物理、地理、生物以及数学等多个学科的知识。 

如果我们仅仅具有某一学科的知识，则无法选出正确的答案。这也是知识体系的魅力之一，它让我们在解决问题时拥有“十八般武艺”。

### 1.**知识 VS 知识体系**

#### 1.1 什么是知识

根据柏拉图的定义，一条陈述能称得上是知识必须满足三个条件，它一定是被验证过的，正确的，而且是被人们相信的。

在这里又会有一个常见的疑问，信息是不是知识？个人认为知识一定是信息，而信息则需要加以过滤、加工方可成为知识。知识是客观并可重复的，它能够指导我们决策或行动。

#### 1.2 什么是知识体系

知识体系有三个特性：目标性、体系性以及抽象性，多使用逻辑树进行构建。 

目标是指方向聚焦，体系有重点。体系是指结构完整、层次分明，前 2 层分支的平衡性越好，归纳程度也越强。而抽象则是知识体系是知识从具体到抽象的表现，呈现了知识的特征或本质。再进一层，什么是好的知识体系呢？除了上述所描述的有目标、够全面、抽象程度高，还有一点则是知识体系应该是**相互独立但非无限穷尽**的。 

有的知识仅须停留在知道，有的则需要运用并且创新。

#### 1.3 知识体系的作用

16 年美国生物学科学家估算大脑的存储容量约为 1000 万亿字节，相当于 1000TB。但同样容量的数据为什么在检索、使用的时候，人脑却远比电脑低效呢？检索从数据表的角度理解是索引和表结构，使用从代码的角度理解是类的封装和继承。建立知识体系，目的是建立类似电脑的数据存储和应用结构。从而认知知识全貌及迅速找到知识的关联。 前者用于查漏补缺，后者则用于知识的高效检索及组合使用。

### 2. **构建知识体系的步骤**

整体分为 4 步：找方法、建框架、收集知识、学习方式。

#### 2.1 找到自己的普适性方法

查理芒格曾经说过：“在手拿铁锤的人看来，世界就像一颗钉子。”因为你的手上只有铁锤，所以你只能用处理钉子的方式去处理一切，用铁锤去旋转螺丝钉，用铁锤去炒菜。 很多时候我们并不是不会解决问题，而是缺少解决问题的方法，构建知识体系也是如此。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav1C1dASM293M0xYPjgzlJTV1jpiaY6I1qOVFGW5FlkaGLia6Y24Vgn6m6dHcKwBydmOicLvMOQTGSVA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

在构建之前，我们应找到自己的普适性方法。对于笔者而言它是逻辑思维，但对于你而言可能是经济学的定律、数学的定理亦或者生物学中的生物群落。 上图仅仅只是一个示例，解决问题更多不应给自己设定框架，我们应使用多种方法的组合。

#### 2.2 明确体系的方向

我们总说这是一个信息爆炸的时代，每时每刻我们接收着大量的信息和噪声，如果不予以治理、过滤，我们将无法听到真正的声音。早在 2300 多年前，庄子曾经说过以有限的生命追逐无限的知识，那么一定会失败。搭建知识体系的目的在于**圈定范围、有序学习。**

**1）明确体系目标**

目标决定了我们要建立什么样的知识体系。使用逆向思维以目标作为终点，反向度量与目标的距离。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav1C1dASM293M0xYPjgzlJTbTEHgcYVuUAMNWHygkm41OEksBcIiaYpJVzgJnaUPygsmMia1Yn7NKng/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

思考产品经理需要学习什么知识，思路会无序延伸，容易遗漏并导致知识框架不平衡。 这种情况不妨反过来想，要应聘的岗位要求产品经理拥有什么样的知识？从岗位的 JD 入手反推我们的知识体系。

**2）明确体系的广度以及深度**

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav1C1dASM293M0xYPjgzlJTtDACKibwybrrXQD65N71ywSOV3CIch74LcWibFoGh7XH3sRhF5ghMZPQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

广度，体现在知识树的纵向分类，解决的是要不要的问题。深度，则体现在知识树的横向延伸，面向的是学习程度。 归纳程度越高，知识检索的效率越高。共性越强，后续在学习同类知识时，复用可能性越大。

通过树枝的纵向广度和横向深度，我们可以了解哪部分知识是目前缺失的，结合主干的优先级确认学习路径。

#### 2.3 收集知识

**1）评估优先级**

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav1C1dASM293M0xYPjgzlJTicdu3wdNR4g0gUzGs3QVhLwVNyVLPeVEKTClOTc1iacaVQtjqvVkgtrA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

知识优先级的评估方式可以使用耗时和价值四象限图进行评估。 一般来说大部分耗时和价值呈正比例关系。耗时越短，说明被过滤的知识越多，知识也越片面。被过滤的知识只能作为学习的起点，每个人知识的侧重都是不同的，学习之前我们应尽可能的了解全貌。

**2）主动降噪**

主动降噪指知识卸载和去重。时间是我们最宝贵的资源。当占用你时间的信息源越多，耗费在学习的时间越少。 去重则是，减少同类信息源。同样是新闻，同类领域保留 2-3 个就够了。

**3）找到专业领域的大佬**

要想变成专业领域的牛人，一定踩过不少的坑。对比度娘，他们是更好的搜索引擎和导航地图，让他们帮助你提高获取知识的质量和效率。 选择的标准只需要比当前的你专业就足够了。

**4）付费知识＞免费知识**

免费知识的特征是：干扰信息多（广告）、获取路径长（关注、下载）、知识不完整（试听体验）。 如果获取知识耗费的时间成本已经大于金钱成本，不妨尝试一下付费，花钱是为了更好的挣钱。

#### 2.4 学习方式

由于社畜大部分的时间都在于工作，所以本节想分享工作中的学习方式。

**1）在翻译中学习**

在翻译中学习，可理解为遇见问题时**先用对方的解法**。对话的前提，是拥有对等的角度和相近的高度。只有足够全面的了解，解决需求的方法才能不拘泥于产品设计。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav1C1dASM293M0xYPjgzlJTcx7ZHWga9qdopLcvg4tx6Ij6a8ib4FFpkrMswBr8p4M6UmBcbWoRYUA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

如：在撰写 PRD 时，为了减少沟通成本。我会将可能将需求能翻译为研发的语言，减少理解的时间。其次也随着“翻译”能力的加深，系统的实现方式及约束也了解愈深。 不仅加深了对技术的了解，也能更好从系统实现方式的角度评估需求的可行性、实现成本以及周期。

**2）在使用中学习**

学习新的知识，应以使用为导向。当某个节点使用了常规的手段，关键指标仍无法提升。不妨尝试新的领域，现学现用并以使用作为终点，反推需要学习的知识。 以使用作为目的的好处是避免**踌躇不前**和**半途而废**，实践出新知。

**3）在复盘中学习**

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvav1C1dASM293M0xYPjgzlJTO60UxbSP5ILQJtc2DG5egIhIaawNHf2Rjs1AuVHbFL41QYLz428wlw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

复盘的目的为知识提炼，让其形成体系。在现实生活中没有太多的灵光一现，更多的是旧知识新组合。 

带着原有的基础再去学习新知识，能够让我们迅速找到知识的共性，从而提高学习的效率或同类问题的解决之中。而体系的扩张，也让知识不再是孤岛，每一个知识都能找到和其他知识联结的枢纽，让我们在输出解决方案时拥有更多的排列组合。

单次知识的提炼很容易，但是坚持才能带来质变。1900 年到 1999 年，道琼斯指数增长了 176 倍，但它的年复合增长不过 5.3%而已。

**写在最后**

一篇纯理论的通识文章写的非常吃力，5 个工作日的凌晨完成了这篇文章，质量上的欠缺也请多多包涵。 感谢你看到这里，谢谢。

原文作者：wisehuang

原文链接：https://mp.weixin.qq.com/s/E0u7LmT__x4R9mWNxfDDjA

# 【NO.238】为什么微信推荐这么快？

# **1. 背景** 

在一些推荐系统、图片检索、文章去重等场景中，对基于特征数据进行 k 近邻检索有着广泛的需求：

- 支持亿级索引的检索，同时要求非常高的检索性能；
- 支持索引的批量实时更新；
- 支持多模型、多版本以灵活开展 ABTest 实验；
- 支持过滤器、过期删除以排除不符合特定条件的数据。

在经过调研后，发现已有的解决方案存在以下问题：

- 在学术界中，已经存在有成熟并开源的 ANN 搜索库，然而这些搜索库仅仅是作为单机引擎存在，而不能作为**高性能、可依赖、可拓展的分布式组件为推荐系统提供服务**；
- 在业界中，大多数的组件都是基于 ANN 搜索库做一层简单的封装，在可拓展、高可用上的表现达不到在线系统的要求；而对于少数在实现上已经较为成熟的分布式检索系统，在**功能上却难以做到紧跟业务发展**；
- 而在更新机制上，很多组件都是要么只支持离线更新、要么只支持在线接口更新，无法满足在微信侧**小至秒级千数量、大至小时级亿数量的索引更新需求**，因此需要可以兼顾近实时更新及离线大批量更新的分布式系统。

基于上述的这些要求以及业内组件的限制，我们借助 WFS 和 Chubby 设计并实现了 SimSvr，它是一个高性能、功能丰富的特征检索组件，具有以下特点：

- 分布式可伸缩的架构，支持亿级以上的索引量，以及索引的并发加速查询，实现了 **10ms 以内检索数亿**的索引；
- 高性能召回引擎，使用了召回性能极佳的 hnswlib 作为首选召回引擎，大部分请求可在 **2ms 内**完成检索；
- 集群化管理，集成了完善的数据调度及动态路由功能；
- 多样的更新机制，支持任务式更新及自动更新，同时也支持全量更新与增量更新，跨越**秒级千数量**到**小时级亿数量**的索引更新；
- 读写分离的机制，在离线利用庞大的计算资源加速构建索引的同时，不影响在线服务的高性能读；
- 丰富的功能特性，支持轻量 embedding kv 库、单表多索引、多版本索引、过滤器、过期删除等特性。

SimSvr 目前已广泛应用于微信视频号、看一看、搜一搜、微信安全、表情搜索等业务，接下来会阐述 SimSvr 的设计以及如何解决来自于业务的难题。

# **2. 检索引擎**

## **2.1 引擎的选择** 

ANN 问题在学术界已被长期研究，并且已有成熟的开源 ANN 搜索库存在，如 nmslib、hnswlib、faiss 等。在 SimSvr 中，**性能及集群的存储容量**是最主要考量的两个指标，因此选择了以下两个检索引擎：

- 在 ann-benchmarks 中检索性能最好的 hnswlib，能够满足在线服务对召回率及检索耗时的高要求（**大于 90% 召回率的情况下，能在 1ms 内完成召回**）；
- faiss 的 IVFx_HNSWy + PQz 算法，支持将向量压缩 10 ~ 30 倍，能够满足资源有限情况下的高维大数据量的索引要求（亿级索引数据，容纳在内存 64G 的机器上）。

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPfIMXIWjiaHADSibibe5WQh6OnUhACial2GTm19tl7d5picdDNWicAly3KBjw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**ANN检索引擎效果对比**

## **2.2 巧妙利用资源，提升 50% 的数据容纳量**

- hnswlib 是单机检索引擎，在资源使用方面仅考虑了单模型的情况；而 SimSvr 是提供在线服务的组件，一般容纳了多个模型；
- SimSvr 在大部分场景下，拥有读写分离的特点；
  基于以上特点，我们在引入 hnswlib 之后，进行了资源整合，使得 SimSvr 单机情况下可以容纳更多的模型索引：

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPEq9OlibAtIAgtLJGUXCUAW5SjVIe3ib720dnopkAAfbw2tTuJ8UewZuQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- 极限情况下（以 worker 线程数 80、部署 10 张 2kw 索引量的表为例）：

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPU8JbgtExpKanSok79kCaHycxjU0DGicqgCSCQof1tAGIchibH47HN9sQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- 现网运营中（以某现网模块(11台实例机器，worker 线程 240）为例）：

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPLiaDcfTLTJ8Cu5BIwSOxJiaxKERq6hmCz45OlmNwhicvnVbORjRDbYUJQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

## **2.3 点积距离召回率从 62.6% 到 97.8% 的蜕变心路历程**

- HNSW 算法在**余弦距离**表现优秀，但在**点乘距离**的数据集上存在效果差的情况；

- 点乘距离非度量空间（metric space)，**不满足三角不等式**，距离比较没有传递性；

- - 维基百科中关于度量空间的定义:

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPibLSU08eUZWqxTWF87p4cW92bJb2eRjr5ib8iaibI6C0z4Rl4gIE1pL7nQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- - hnswlib 中说明点积属于非度量空间：

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXP3lFdss02FsuCuVOh9hsia9hiazHctIpZ4WheJEQzWqShwY2xbCEF6egQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- 而在论文 Non-metric Similarity Graphs for
  Maximum Inner Product Search 中，提到了将**点乘距离转换为余弦距离**计算的方法，我们将这种方法简称为 ip2cos；

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPnSkpuUKHAjv8rBNSI67TK02EDUqRKHickER52tjSlp3LCY7dlUVB83g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



在 ip2cos 距离转换的理论基础上，我们使用看一看视频实时 DSSM 模型进行了实际召回情况的效果对比（64 维、ip 距离、100 万索引数据量，进行 1 万次查询取平均耗时），并见证了 ip2cos 的神奇效果：

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPf8pCbhiaqghjMiapvzrrXEmfNo4u6CiaEWIV1LwzJOpu9q4z3cdzrm5wQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

## **2.4 如何使用 faiss 省下 2h 的训练时间并提升 30% 的召回率**

- 在 faiss 中增加了 batch kmeans 聚类方法，在保证较好聚类效果的同时大幅加快训练速度。IVF 系类方法训练耗时主要体现在需要从数据中学习 nlist 个聚类中心，对于千万级数据 nlist 的大小在 20 万以上，在 cpu 上使用传统 kmeans 方法训练会非常耗时，下面展示在 128 维、IP 距离、1000 万条数据的情况下 batch kmeans 对训练速度的加速效果：

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPVRbRjywVzZFNzLKSHqUZIX4RBmicES12aqw9JwFmX9MlCENYiaXywhVA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

从结果中可以看到，在相同迭代轮次下，不使用 batch kmeans 的方法训练耗时更长，且没有很好收敛，导致召回率不高。

# **3. 总体设计**

## **3.1 数据结构 - 为达成一个小目标，需要做出怎样的改变**

为了满足单模块多模型的需求，SimSvr 使用了表的概念进行**多模型的管理**；另外，为**支持亿级以上 HNSW 索引的表**，并且希望能够并发加速构建索引，我们根据单表的数据情况，将一张表分成了多个 sharding，使得每个 sharding 承担表数据的其中一部分：
tablei 的索引，由 shard0、shard1、…、shardn 构成一份完整的索引数据；而 sect 的数量则决定了表的副本数（可用于伸缩读能力、提供容灾等）。
在 SimSvr 中，我们将一个 shardi_sectj 称之为一个 container，这是 SimSvr 中最小的数据调度和加载单位。

## **3.2 系统架构 - 如何支撑亿级索引、5毫秒级的检索**

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPxJwfp1YcgukFUa2iapNJ8ria4LrdmXJZQMwSvxJksOtNlmWy0D9I1LaA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**SimSvr 架构**

- **SimSvr 与 FeatureKV 一样，涉及的外部依赖也是三个：**

- - Chubby：用来保存元数据、路由信息、worker 资源信息等；SimSvr 中的数据协同、分布式任务执行均是依赖于 Chubby；
  - USER_FS：业务侧存放原始数据的分布式文件系统，可以是 WFS/HDFS，该文件系统的路径及信息保存在表/任务的元信息中；
  - SimSvr_FS：Simsvr 使用的分布式文件系统，用于存放生成的索引文件或者原始的增量数据文件。

- **worker**

- - 负责对外提供检索服务，通过对 Chubby 的轮询检查索引的更新，进而将索引加载至本机以提供服务；
  - 每台 worker 负责的数据，由 master 进行调度，worker 根据 master 保存在 Chubby 上的分配信息进行数据的加载/卸载；
  - worker 的数据是根据 master 分配得来的，除此之外没有其他状态的差别，因此 worker 是易于扩缩容的。

- **master**

- - 数据调度：通过表的元信息及 worker 状态，将未分配的数据或者失效 worker 上的数据调度给其他有效的 worker；
  - 生成路由表：根据 worker 的数据加载情况及状态，生成集群的路由表；
  - 感知数据更新：检查表的自动更新目录，若最大数字目录发生了增长，则建一个任务以供 trainer 进行索引的构建；
  - master 是一个无状态的服务，通过 Chubby 提供的分布式锁保证数据调度以及路由生成的唯一执行。

- **trainer**

- - 负责构建表的索引及资源回收；
  - trainer 单次可构建一张表中一个 sharding 的索引，因此如果表有多个 sharding 时，可通过增加 trainer 的个数实现构建索引的并发加速；
  - trainer 是无状态的服务，通常部署在微信 Yard 系统上，充分了利用微信闲置机器上的资源。

- **数据自动更新**

- - 在建表时，对其指定了一个 fs 的目录，该目录下，是一系列数字递增的目录；
  - 当业务侧需要更新索引时，将最新的数据 dump 到更大的数字目录中；
  - master 感知最大数字目录的更新，从而更新了元信息；
  - trainer 感知元信息的更新并触发建索引；
  - worker 加载索引完成索引的更新。

- **数据任务式更新**

- - 由业务侧主动通过接口的调用，创建一个索引任务；
  - 在索引任务中，指定了数据的配置信息（如 fs 信息及路径等）；
  - trainer 按照表的任务序列，执行任务并构建索引；
  - worker 加载索引完成索引的更新。

## **3.3 数据调度 - 鸡蛋怎么放在多个篮子中**

- SimSvr 在每张表创建时就指定了 sharding 数 n 及 sect 数 m，因此这张表拥有了 n * m 个 Conatiner 以供 master 调度；
- master 会根据 worker 的健康情况及资源使用情况进行数据的调度及路由表的生成；
- 路由表带有递增的版本号，可根据版本号感知路由的变化。

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPKZenkTQO3kSicgMZMKeClGIiaOuFvRHibFpGC0ER2YB2aY84lWFWmL9TA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



- worker 定期轮询 Chubby 获取数据的调度情况及最新的路由表信息；
- client 首次请求时，将随机请求一台 worker 获取最新的路由表信息并将其缓存在本地；
- client 在本地有路由表的情况下，将根据表的数据分布情况，带上版本号并发地向目标 worker 发起请求，最终合并所有 sharding 的结果，将其返回给业务端。

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXP99eKIOibMNLnfrkrmAz9CVa331xCSzumOtAxzlXic8nK5X5QU8uibLh6w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



## **3.4 系统拓展 - 篮子装满了该怎么办**

- SimSvr 将表拆分成了更小粒度的数据调度单位，且不要求每台机器上的数据一样，因此可以用拓展机器的方式，将集群的存储容量扩大；
- 对于单表而言，当读能力达到瓶颈时，可以单独扩展此表的读副本数；

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXP8qMZ0g2zAqIJZhRFqfRt8maJIjibmk9eElVHgiaGu47N5BI3su4OAHibw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

# **4. 近实时增量更新的挑战 - 十秒内完成索引的更新**

- 数据一致性与持久化

- - 对于大多数的分布式存储组件来说，都是使用 raft 或者 paxos 等一致性协议保证数据一致性并持久化至本机上；
  - 对于 SimSvr 来说，每张表会被分为多个 sharding，且 sharding 数不保证为奇数；
  - 在 worker 中加入一致性组件及额外的存储引擎，会使得整体的结构变得复杂；
  - 最终在考量后，结合业务的批量增量更新的特点，选择了先将数据落地 fs，再由 worker 拉取数据加载的方案；在这种方案下，1000 以内数量的 key 插入，能够在 10s 内完成，达到了业务的要求。

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPKReraaeenzKkowEe8ukjGsRxSpmw3lUGaOFvcmic5XgFqokjicQ5Q0uA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**增量持久化**

- 增量更新的性能保障

- - 由于在线建索引是非常消耗 cpu 资源的过程，因此为了不影响现网的读服务，worker 仅提供少量的 cpu 资源用于增量数据的更新；
  - 对于小批量的增量数据，worker 可以直接加载存放在 fs 上的数据并直接进行索引的在线插入；
  - 对于大批量的增量数据，为了避免影响读服务及大增量更新慢的问题，SimSvr 将大批量数据在 trainer 进行合并且并发重建索引，最后再由 worker 直接加载建好的索引。

![图片](https://mmbiz.qpic.cn/mmbiz_png/0Az9wtrYXDVvJshxncI3RWvQiaMbtdUXPA6ADk29yN4lAf9Lh2SKOGYr3un98vNbbrqU0g8nibZW51iaI5NsKpn2A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**增量更新**

# **5. 丰富的功能特性**

## **5.1 支持额外的特征存储库**

- 在推荐系统中，同一个模型，产生的数据除了用于检索的索引库，常常还有视频特征/用户画像的特征数据；
- 这类数据，仅仅只需要查询的功能，并且与同个模型同个版本产出的索引库相互作用，产生正确的召回效果；
- 基于这种原子性更新的特性，SimSvr 支持了额外的特征存储库，用于存储与模型一同更新且仅用于查询的特征数据，帮助业务省去了数据同步与对齐的烦恼。

## **5.2 支持原子性更新的单表多索引**

- 在推荐系统中，ABTest 是非常常见的，多个模型的实验往往也是需要同时进行的；
- 另外，在某些场景下，同一个模型会产生不同的索引数据，在线上使用时要求同模型的索引要同时生效；
- 对于以上两种情况，如果使用多表支持多模型，在索引更新上存在生效时间的差异从而无法支持；
- SimSvr 对于这种情况，支持了同一张表多份索引的原子性更新，保证了索引能够同时生效。

## **5.3 多版本索引**

- 在 ABTest 场景下，除了有多模型间的实验，还有相同模型不同版本数据的实验；
- 在相同模型中，版本迭代/不同版本进行实验的场景是广泛存在的；
- 如果使用多表支持这样的多版本索引，不管在业务方的使用上，还是在 SimSvr 的管理上，都显得不是那么地优雅；
- 对此，SimSvr 支持了同一张表的多版本管理，并且多版本支持在现网下同时进行服务，业务可以按需请求目标版本，进行灵活的实验。

## **5.4 支持布隆过滤器、阈值过滤器等**

- 在视频号场景中，业务使用 SimSvr 对视频进行索引；
- 在使用某个用户的特征进行召回时，常常召回了许多用户已看过的视频，影响用户体验；
- 通过增加召回结果并在结果中进行过滤，对于重度用户，一样存在上述问题，并且还会导致不必要的性能开销；
- SimSvr 改造 hnswlib，嵌入了过滤器的逻辑，使得其支持在检索过程中实时对符合特定条件的 key 进行过滤，保证了召回结果的有效性。

## **5.5 支持过期删除**

- 对于一些推荐系统来说，对于数据的时效性要求是非常高的，在数据过了其最佳召回时间段之后，就不应该出现在召回结果中，以免出现不合时宜的尴尬；
- SimSvr 支持导入带过期时间的数据，在现网召回过程中，实时淘汰过期的 key 以达到准确的召回要求。

# **6. 现网运营情况**

- SimSvr 目前已部署 160+ 个模型索引，使用逻辑核 8000+，总索引量超过 20 亿特征向量，广泛应用于视频号、看一看、搜一搜等推荐业务中。
- 搜一搜基于 SimSvr 建立小程序优质文章的向量索引，提升小程序文章搜索的优质结果召回率。新方案相比旧方案，优质结果召回率提升 7%；
- 搜一搜使用 SimSvr 检索视频指纹，进行相似视频去重；单表索引量高达 1.7 亿 * 128 维，检索平均耗时小于 8ms，日检索量 12.5 亿。

# **7. 总结**

随着推荐系统的强势发展，特征检索的使用场景越来越广泛。而作为基础组件，除了要拥有支持亿级索引的基本素养外，在功能特性上也需要不断迎合业务的发展。因此我们开发了 SimSvr，搭配特征存储 FeatureKV，在视频号、看一看、搜一搜等推荐系统中发挥了重要的作用。

原文作者：sauronzhang、flashlin、fengshanliu，微信后台开发工程师

原文链接：https://mp.weixin.qq.com/s/rXXm6c8LrTqqP4iWf9mtxA

# 【NO.239】万字详文告诉你如何做 Code Review

### 0.**前言**

作为公司代码委员会 golang 分会的理事，我 review 了很多代码，看了很多别人的 review 评论。发现不少同学 code review 与写出好代码的水平有待提高。在这里，想分享一下我的一些理念和思路。

### 1.**为什么技术人员包括 leader 都要做 code review**

谚语曰: 'Talk Is Cheap, Show Me The Code'。知易行难，知行合一难。嘴里要讲出来总是轻松，把别人讲过的话记住，组织一下语言，再讲出来，很容易。绝知此事要躬行。设计理念你可能道听途说了一些，以为自己掌握了，但是你会做么？有能力去思考、改进自己当前的实践方式和实践中的代码细节么？不客气地说，很多人仅仅是知道并且认同了某个设计理念，进而产生了一种虚假的安心感---自己的技术并不差。但是，他根本没有去实践这些设计理念，甚至根本实践不了这些设计理念，从结果来说，他懂不懂这些道理/理念，有什么差别？变成了自欺欺人。

代码，是设计理念落地的地方，是技术的呈现和根本。同学们可以在 review 过程中做到落地沟通，不再是空对空的讨论，可以在实际问题中产生思考的碰撞，互相学习，大家都掌握团队里积累出来最好的实践方式！当然，如果 leader 没时间写代码，仅仅是 review 代码，指出其他同学某些实践方式不好，要给出好的实践的意见，即使没亲手写代码，也是对最佳实践要有很多思考。

### 2.**为什么同学们要在 review 中思考和总结最佳实践**

我这里先给一个我自己的总结：所谓架构师，就是掌握大量设计理念和原则、落地到各种语言及附带工具链（生态）下的实践方法、垂直行业模型理解，定制系统模型设计和工程实践规范细则。进而控制 30+万行代码项目的开发便利性、可维护性、可测试性、运营质量。

厉害的技术人，主要可以分为下面几个方向：

- 奇技淫巧

掌握很多技巧，以及发现技巧一系列思路，比如很多编程大赛，比的就是这个。但是，这个对工程，用处好像并不是很大。

- 领域奠基

比如约翰*卡马克，他创造出了现代计算机图形高效渲染的方法论。不论如果没有他，后面会不会有人发明，他就是第一个发明了。1999 年，卡马克登上了美国时代杂志评选出来的科技领域 50 大影响力人物榜单，并且名列第 10 位。但是，类似的殿堂级位置，没有几个，不够大家分，没我们的事儿。

- 理论研究

八十年代李开复博士坚持采用隐含马尔可夫模型的框架，成功地开发了世界上第一个大词汇量连续语音识别系统 Sphinx。我辈工程师，好像擅长这个的很少。

- 产品成功

小龙哥是标杆。

- 最佳实践

这个是大家都可以做到，按照上面架构师的定义。在这条路上走得好，就能为任何公司组建技术团队，组织建设高质量的系统。

从上面的讨论中，可以看出，我们普通工程师的进化之路，就是不断打磨最佳实践方法论、落地细节。

### 3.**代码变坏的根源**

在讨论什么代码是好代码之前，我们先讨论什么是不好的。计算机是人造的学科，我们自己制造了很多问题，进而去思考解法。

#### 3.1 重复的代码

```
// BatchGetQQTinyWithAdmin 获取QQ uin的tinyID, 需要主uin的tiny和登录态
// friendUins 可以是空列表, 只要admin uin的tiny
func BatchGetQQTinyWithAdmin(ctx context.Context, adminUin uint64, friendUin []uint64) (
 adminTiny uint64, sig []byte, frdTiny map[uint64]uint64, err error) {
 var friendAccountList []*basedef.AccountInfo
 for _, v := range friendUin {
  friendAccountList = append(friendAccountList, &basedef.AccountInfo{
   AccountType: proto.String(def.StrQQU),
   Userid:      proto.String(fmt.Sprint(v)),
  })
 }

 req := &cmd0xb91.ReqBody{
  Appid:       proto.Uint32(model.DocAppID),
  CheckMethod: proto.String(CheckQQ),
  AdminAccount: &basedef.AccountInfo{
   AccountType: proto.String(def.StrQQU),
   Userid:      proto.String(fmt.Sprint(adminUin)),
  },
  FriendAccountList: friendAccountList,
 }
```

因为最开始协议设计得不好，第一个使用接口的人，没有类似上面这个函数的代码，自己实现了一个嵌入逻辑代码的填写请求结构结构体的代码，一开始，挺好的。但当有第二个人，第三个人干了类似的事情，我们将无法再重构这个协议，必须做到麻烦的向前兼容。而且每个同学，都要理解一遍上面这个协议怎么填，理解有问题，就触发 bug。或者，如果某个错误的理解，普遍存在，我们就得找到所有这些重复的片段，都修改一遍。

当你要读一个数据，发现两个地方有，不知道该选择哪个。当你要实现一个功能，发现两个 rpc 接口、两个函数能做到，你不知道选哪一个。你有面临过这样的'人生难题'么？其实怎么选并不重要了，你写的这个代码已经在走向 shit 的道路上迈出了坚实的一步。

但是，A little copying is better than a little dependency。这里提一嘴，不展开。

这里，我必须额外说一句。大家使用 trpc。感觉自己被鼓励'每个服务搞一个 git'。那，你这个服务里访问 db 的代码，rpc 的代码，各种可以复用的代码，是用的大家都复用的 git 下的代码么？每次都重复写一遍，db 字段细节改了，每个使用过 db 的 server 对应的 git 都改一遍？这个通用 git 已经写好的接口应该不知道哪些 git 下的代码因为自己不向前兼容的修改而永远放弃了向前不兼容的修改？

#### 3.2 早期有效的决策不再有效

很多时候，我们第一版代码写出来，是没有太大的问题的。比如，下面这个代码

```
// Update 增量更新
func (s *FilePrivilegeStore) Update(key def.PrivilegeKey,
 clear, isMerge bool, subtract []*access.AccessInfo, increment []*access.AccessInfo,
 policy *uint32, adv *access.AdvPolicy, shareKey string, importQQGroupID uint64) error {
 // 获取之前的数据
 info, err := s.Get(key)
 if err != nil {
  return err
 }

 incOnlyModify := update(info, &key, clear, subtract,
  increment, policy, adv, shareKey, importQQGroupID)
 stat := statAndUpdateAccessInfo(info)
 if !incOnlyModify {
  if stat.groupNumber > model.FilePrivilegeGroupMax {
   return errors.Errorf(errors.PrivilegeGroupLimit,
    "group num %d larger than limit %d",
    stat.groupNumber, model.FilePrivilegeGroupMax)
  }
 }

 if !isMerge {
  if key.DomainID == uint64(access.SPECIAL_FOLDER_DOMAIN_ID) &&
   len(info.AccessInfos) > model.FilePrivilegeMaxFolderNum {
   return errors.Errorf(errors.PrivilegeFolderLimit,
    "folder owner num %d larger than limit %d",
    len(info.AccessInfos), model.FilePrivilegeMaxFolderNum)
  }
  if len(info.AccessInfos) > model.FilePrivilegeMaxNum {
   return errors.Errorf(errors.PrivilegeUserLimit,
    "file owner num %d larger than limit %d",
    len(info.AccessInfos), model.FilePrivilegeMaxNum)
  }
 }

 pbDataSt := infoToData(info, &key)
 var updateBuf []byte
 if updateBuf, err = proto.Marshal(pbDataSt); err != nil {
  return errors.Wrapf(err, errors.MarshalPBError,
   "FilePrivilegeStore.Update Marshal data error, key[%v]", key)
 }
 if err = s.setCKV(generateKey(&key), updateBuf); err != nil {
  return errors.Wrapf(err, errors.Code(err),
   "FilePrivilegeStore.Update setCKV error, key[%v]", key)
 }
 return nil
}
```

现在看，这个代码挺好的，长度没超过 80 行，逻辑比价清晰。但是当 isMerge 这里判断逻辑，如果加入更多的逻辑，把局部行数撑到 50 行以上，这个函数，味道就坏了。出现两个问题：

1）函数内代码不在一个逻辑层次上，阅读代码，本来在阅读着顶层逻辑，突然就掉入了长达 50 行的 isMerge 的逻辑处理细节，还没看完，读者已经忘了前面的代码讲了什么，需要来回看，挑战自己大脑的 cache 尺寸。

2）代码有问题后，再新加代码的同学，是改还是不改前人写好的代码呢？出 bug 谁来背？这是一个灵魂拷问。

#### 3.3 过早的优化

这个大家听了很多了，这里不赘述。

#### 3.4 对合理性没有苛求

'两种写法都 ok，你随便挑一种吧'，'我这样也没什么吧'，这是我经常听到的话。

```
// Get 获取IP
func (i *IPGetter) Get(cardName string) string {
 i.l.RLock()
 ip, found := i.m[cardName]
 i.l.RUnlock()

 if found {
  return ip
 }

 i.l.Lock()
 var err error
 ip, err = getNetIP(cardName)
 if err == nil {
  i.m[cardName] = ip
 }

  i.l.Unlock()
 return ip
}
```

i.l.Unlock()可以放在当前的位置，也可以放在 i.l.Lock()下面，做成 defer。两种在最初构造的时候，好像都行。这个时候，很多同学态度就变得不坚决。实际上，这里必须是 defer 的。

```
  i.l.Lock()
 defer i.l.Unlock()

 var err error
 ip, err = getNetIP(cardName)
 if err != nil {
  return "127.0.0.1"
 }

 i.m[cardName] = ip
 return ip
```

这样的修改，是极有可能发生的，它还是要变成 defer，那，为什么不一开始就是 defer，进入最合理的状态？不一开始就进入最合理的状态，在后续协作中，其他同学很可能犯错！

#### 3.5 总是面向对象/总喜欢封装

我是软件工程科班出身。学的第一门编程语言是 c++。教材是[这本](https://book.douban.com/subject/1112255/) 。当时自己读完教材，初入程序设计之门，对于里面讲的'封装'，惊为天人，多么美妙的设计啊，面向对象，多么智慧的设计啊。但是，这些年来，我看到了大牛'云风'对于'毕业生使用 mysql api 就喜欢搞个 class 封装再用'的嘲讽；看到了各种莫名其妙的 class 定义；体会到了经常要去看一个莫名其妙的继承树，必须要把整个继承树整体读明白才能确认一个细小的逻辑分支；多次体会到了我需要辛苦地压抑住自己的抵触情绪，去细度一个自作聪明的被封装的代码，确认我的 bug。除了 UI 类场景，我认为少用继承、多用组合。

```
template<class _PKG_TYPE>
class CSuperAction : public CSuperActionBase {
  public:
    typedef _PKG_TYPE pkg_type;
    typedef CSuperAction<pkg_type> this_type;
    ...
}
```

这是 sspp 的代码。CSuperAction 和 CSuperActionBase，一会儿 super，一会儿 base，Super 和 SuperBase 是在怎样的两个抽象层次上，不通读代码，没人能读明白。我想确认任何细节，都要把多个层次的代码都通读了，有什么封装性可言？

好，你说是作者没有把 class name 取得好。那，问题是，你能取得好么？一个刚入职的 T1.2 的同学能把 class name、class 树设计得好么？即使是对简单的业务模型，也需要无数次'坏'的对象抽象实践，才能培养出一个具有合格的 class 抽象能力的同学，这对于大型却松散的团队协作，不是破坏性的？已经有了一套继承树，想要添加功能就只能在这个继承树里添加，以前的继承树不再适合新的需求，这个继承树上所有的 class，以及使用它们的地方，你都去改？不，是个正常人都会放弃，开始堆屎山。

封装，就是我可以不关心实现。但是，做一个稳定的系统，每一层设计都可能出问题。abi，总有合适的用法和不合适的用法，真的存在我们能完全不关心封装的部分是怎么实现的？不，你不能。bug 和性能问题，常常就出现在，你用了错误的用法去使用一个封装好的函数。即使是 android、ios 的 api，golang、java 现成的 api，我们常常都要去探究实现，才能把 api 用好。那，我们是不是该一上来，就做一个透明性很强的函数，才更为合理？使用者想知道细节，进来吧，我的实现很易读，你看看就明白，使用时不会迷路！对于逻辑复杂的函数，我们还要强调函数内部工作方式'可以让读者在大脑里想象呈现完整过程'的可现性，让使用者轻松读懂，有把握，使用时，不迷路！

#### 3.6 根本没有设计

这个最可怕，所有需求，上手就是一顿撸，'设计是什么东西？我一个文件 5w 行，一个函数 5k 行，干不完需求？'从第一行代码开始，就是无设计的，随意地踩着满地的泥坑，对于旁人的眼光没有感觉，一个人独舞，产出的代码，完成了需求，毁灭了接手自己代码的人。这个就不举例了，每个同学应该都能在自己的项目类发现这种代码。

### 4.**必须形而上的思考**

常常，同学们听演讲，公开课，就喜欢听一些细枝末节的'干活'。这没有问题。但是，你干了几年活，学习了多少干货知识点？构建起自己的技术思考'面'，进入立体的'工程思维'，把技术细节和系统要满足的需求在思考上连接起来了么？当听一个需求的时候，你能思考到自己的 code package 该怎么组织，函数该怎么组织了么？

那，技术点要怎么和需求连接起来呢？答案很简单，你需要在时间里总结，总结出一些明确的原则、思维过程。思考怎么去总结，特别像是在思考哲学问题。从一些琐碎的细节中，由具体情况上升到一些原则、公理。同时，大家在接受原则时，不应该是接受和记住原则本身，而应该是结构原则，让这个原则在自己这里重新推理一遍，自己完全掌握这个原则的适用范围。

再进一步具体地说，对于工程最佳实践的形而上的思考过程，就是：

把工程实践中遇到的问题，从问题类型和解法类型，两个角度去归类，总结出一些有限适用的原则，就从点到了面。把诸多总结出的原则，组合应用到自己的项目代码中，就是把多个面结合起来构建了一套立体的最佳实践的方案。当你这套方案能适应 30w+行代码的项目，超过 30 人的项目，你就架构师入门了！当你这个项目，是多端，多语言，代码量超过 300w 行，参与人数超过 300 人，代码质量依然很高，代码依然在高效地自我迭代，每天消除掉过时的代码，填充高质量的替换旧代码和新生的代码。恭喜你，你已经是一个很高级的架构师了！再进一步，你对某个业务模型有独到或者全面的理解，构建了一套行业第一的解决方案，结合刚才高质量实现的能力，实现了这么一个项目。没啥好说的，你已经是专家工程师了。级别再高，我就不了解了，不在这里讨论。

那么，我们要重头开始积累思考和总结？不，有一本书叫做《unix 编程艺术》，我在不同的时期分别读了 3 遍，等一会，我讲一些里面提到的，我觉得在腾讯尤其值得拿出来说的原则。这些原则，正好就能作为 code review 时大家判定代码质量的准绳。但，在那之前，我得讲一下另外一个很重要的话题，模型设计。

### 5.**model 设计**

没读过 oauth2.0 RFC，就去设计第三方授权登陆的人，终归还要再发明一个撇脚的 oauth。

2012 年我刚毕业，我和一个去了广州联通公司的华南理工毕业生聊天。当时他说他工作很不开心，因为工作里不经常写代码，而且认为自己有 ACM 竞赛金牌级的算法熟练度+对 CPP 代码的熟悉，写下一个个指针操作内存，什么程序写不出来，什么事情做不好。当时我觉得，挺有道理，编程工具在手，我什么事情做不了？

现在，我会告诉他，复杂如 linux 操作系统、Chromium 引擎、windows office，你做不了。原因是，他根本没进入软件工程的工程世界。不是会搬砖就能修出港珠澳大桥。但是，这么回答并不好，举证用的论据离我们太遥远了。见微知著。我现在会回答，你做不了，简单如一个权限系统，你知道怎么做么？堆积一堆逻辑层次一维展开的 if else？简单如一个共享文件管理，你知道怎么做么？堆积一堆逻辑层次一维展开的 ife lse？你联通有上万台服务器，你要怎么写一个管理平台？堆积一堆逻辑层次一维展开的 ife lse？

上来就是干，能实现上面提到的三个看似简单的需求？想一想，亚马逊、阿里云折腾了多少年，最后才找到了容器+Kubernetes 的大杀器。这里，需要谷歌多少年在 BORG 系统上的实践，提出了优秀的服务编排领域模型。权限领域，有 RBAC、DAC、MAC 等等模型，到了业务，又会有细节的不同。如 Domain Driven Design 说的，没有良好的领域思考和模型抽象，逻辑复杂度就是 n^2 指数级的，你得写多少 ifelse，得思考多少可能的 if 路径，来 cover 所有的不合符预期的情况。你必须要有 Domain 思考探索、model 拆解/抽象/构建的能力。有人问过我，要怎么有效地获得这个能力？这个问题我没能回答，就像是在问我，怎么才能获得 MIT 博士的学术能力？我无法回答。唯一回答就是，进入某个领域，就是首先去看前人的思考，站在前人的肩膀上，再用上自己的通识能力，去进一步思考。至于怎么建立好的通识思考能力，可能得去常青藤读个书吧：）或者，就在工程实践中思考和锻炼自己的这个能力！

同时，基于 model 设计的代码，能更好地适应产品经理不断变更的需求。比如说，一个 calendar(日历)应用，简单来想，不要太简单！以'userid_date'为 key 记录一个用户的每日安排不就完成了么？只往前走一步，设计了一个任务，上限分发给 100w 个人，创建这么一个任务，是往 100w 个人下面添加一条记录？你得改掉之前的设计，换 db。再往前走一步，要拉出某个用户和某个人一起要参与的所有事务，是把两个人的所有任务来做 join？好像还行。如果是和 100 个人一起参与的所有任务呢？100 个人的任务来 join？不现实了吧。好，你引入一个群组 id，那么，你最开始的'userid_date'为 key 的设计，是不是又要修改和做数据迁移了？经常来一个需求，你就得把系统推翻重来，或者根本就只能拒绝用户的需求，这样的战斗力，还好意思叫自己工程师？你一开始就应该思考自己面对的业务领域，思考自己的日历应用可能的模型边界，把可能要做的能力都拿进来思考，构建一个 model，设计一套通用的 store 层接口，基于通用接口的逻辑代码。当产品不断发展，就是不停往模型里填内容，而不是推翻重来。这，思考模型边界，构建模型细节，就是两个很重要的能力，也是绝大多数腾讯产品经理不具备的能力，你得具备，对整个团队都是极其有益的。你面对产品经理时，就听取他们出于对用户体验负责思考出的需求点，到你自己这里，用一个完整的模型去涵盖这些零碎的点。

model 设计，是形而上思考中的一个方面，一个特别重要的方面。接下来，我们来抄袭抄袭 unix 操作系统构建的实践为我们提出的前人实践经验和'公理'总结。在自己的 coding/code review 中，站在巨人的肩膀上去思考。不重复地发现经典力学，而是往相对论挺进。

### 6.**UNIX 设计哲学**

不懂 Unix 的人注定最终还要重复发明一个撇脚的 Unix。--Henry Spenncer, 1987.11

下面这一段话太经典，我必须要摘抄一遍(自《UNIX 编程艺术》)：“工程和设计的每个分支都有自己的技术文化。在大多数工程领域中，就一个专业人员的素养组成来说，有些不成文的行业素养具有与标准手册及教科书同等重要的地位(并且随着专业人员经验的日积月累，这些经验常常会比书本更重要)。资深工程师们在工作中会积累大量的隐性知识，他们用类似禅宗'教外别传'的方式，通过言传身教传授给后辈。软件工程算是此规则的一个例外：技术变革如此之快，软件环境日新月异，软件技术文化暂如朝露。然而，例外之中也有例外。确有极少数软件技术被证明经久耐用，足以演进为强势的技术文化、有鲜明特色的艺术和世代相传的设计哲学。“

接下来，我用我的理解，讲解一下几个我们常常做不到的原则。

#### 6.1 Keep It Simple Stuped!

KISS 原则，大家应该是如雷贯耳了。但是，你真的在遵守？什么是 Simple？简单？golang 语言主要设计者之一的 Rob Pike 说'大道至简'，这个'简'和简单是一个意思么？

首先，简单不是面对一个问题，我们印入眼帘第一映像的解法为简单。我说一句，感受一下。"把一个事情做出来容易，把事情用最简单有效的方法做出来，是一个很难的事情。"比如，做一个三方授权，oauth2.0 很简单，所有概念和细节都是紧凑、完备、易用的。你觉得要设计到 oauth2.0 这个效果很容易么？要做到简单，就要对自己处理的问题有全面的了解，然后需要不断积累思考，才能做到从各个角度和层级去认识这个问题，打磨出一个通俗、紧凑、完备的设计，就像 ios 的交互设计。简单不是容易做到的，需要大家在不断的时间和 code review 过程中去积累思考，pk 中触发思考，交流中总结思考，才能做得愈发地好，接近'大道至简'。

两张经典的模型图，简单又全面，感受一下，没看懂，可以立即自行 google 学习一下：RBAC:![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvateFibZunwicamXNtmtrXYQ6TJME4nHnTKJ2BAnU4eDRjsjIYzMMSUeIFUPoQLUq9sGUmoO0WCPfdvA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

logging:

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvateFibZunwicamXNtmtrXYQ6TeE8VCDD3htjWxgBIPrSibzOXqXY0n7gb82LuMAfDBTOzeFXGiaVzactA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### 6.2 原则 3 组合原则: 设计时考虑拼接组合

关于 OOP，关于继承，我前面已经说过了。那我们怎么组织自己的模块？对，用组合的方式来达到。linux 操作系统离我们这么近，它是怎么架构起来的？往小里说，我们一个串联一个业务请求的数据集合，如果使用 BaseSession，XXXSession inherit BaseSession 的设计，其实，这个继承树，很难适应层出不穷的变化。但是如果使用组合，就可以拆解出 UserSignature 等等各种可能需要的部件，在需要的时候组合使用，不断添加新的部件而没有对老的继承树的记忆这个心智负担。

使用组合，其实就是要让你明确清楚自己现在所拥有的是哪个部件。如果部件过于多，其实完成组合最终成品这个步骤，就会有较高的心智负担，每个部件展开来，琳琅满目，眼花缭乱。比如 QT 这个通用 UI 框架，看它的[Class 列表](https://doc.qt.io/qt-5/classes.html)，有 1000 多个。如果不用继承树把它组织起来，平铺展开，组合出一个页面，将会变得心智负担高到无法承受。OOP 在'需要无数元素同时展现出来'这种复杂度极高的场景，有效的控制了复杂度 。'那么，古尔丹，代价是什么呢？'代价就是，一开始做出这个自上而下的设计，牵一发而动全身，每次调整都变得异常困难。

实际项目中，各种职业级别不同的同学一起协作修改一个 server 的代码，就会出现，职级低的同学改哪里都改不对，根本没能力进行修改，高级别的同学能修改对，也不愿意大规模修改，整个项目变得愈发不合理。对整个继承树没有完全认识的同学都没有资格进行任何一个对继承树有调整的修改，协作变得寸步难行。代码的修改，都变成了依赖一个高级架构师高强度监控继承体系的变化，低级别同学们束手束脚的结果。组合，就很好的解决了这个问题，把问题不断细分，每个同学都可以很好地攻克自己需要攻克的点，实现一个 package。产品逻辑代码，只需要去组合各个 package，就能达到效果。

这是 golang 标准库里 http request 的定义，它就是 Http 请求所有特性集合出来的结果。其中通用/异变/多种实现的部分，通过 duck interface 抽象，比如 Body io.ReadCloser。你想知道哪些细节，就从组合成 request 的部件入手，要修改，只需要修改对应部件。[这段代码后，对比.NET 的 HTTP 基于 OOP 的抽象]

```
// A Request represents an HTTP request received by a server
// or to be sent by a client.
//
// The field semantics differ slightly between client and server
// usage. In addition to the notes on the fields below, see the
// documentation for Request.Write and RoundTripper.
type Request struct {
	// Method specifies the HTTP method (GET, POST, PUT, etc.).
	// For client requests, an empty string means GET.
	//
	// Go's HTTP client does not support sending a request with
	// the CONNECT method. See the documentation on Transport for
	// details.
	Method string

	// URL specifies either the URI being requested (for server
	// requests) or the URL to access (for client requests).
	//
	// For server requests, the URL is parsed from the URI
	// supplied on the Request-Line as stored in RequestURI.  For
	// most requests, fields other than Path and RawQuery will be
	// empty. (See RFC 7230, Section 5.3)
	//
	// For client requests, the URL's Host specifies the server to
	// connect to, while the Request's Host field optionally
	// specifies the Host header value to send in the HTTP
	// request.
	URL *url.URL

	// The protocol version for incoming server requests.
	//
	// For client requests, these fields are ignored. The HTTP
	// client code always uses either HTTP/1.1 or HTTP/2.
	// See the docs on Transport for details.
	Proto      string // "HTTP/1.0"
	ProtoMajor int    // 1
	ProtoMinor int    // 0

	// Header contains the request header fields either received
	// by the server or to be sent by the client.
	//
	// If a server received a request with header lines,
	//
	//	Host: example.com
	//	accept-encoding: gzip, deflate
	//	Accept-Language: en-us
	//	fOO: Bar
	//	foo: two
	//
	// then
	//
	//	Header = map[string][]string{
	//		"Accept-Encoding": {"gzip, deflate"},
	//		"Accept-Language": {"en-us"},
	//		"Foo": {"Bar", "two"},
	//	}
	//
	// For incoming requests, the Host header is promoted to the
	// Request.Host field and removed from the Header map.
	//
	// HTTP defines that header names are case-insensitive. The
	// request parser implements this by using CanonicalHeaderKey,
	// making the first character and any characters following a
	// hyphen uppercase and the rest lowercase.
	//
	// For client requests, certain headers such as Content-Length
	// and Connection are automatically written when needed and
	// values in Header may be ignored. See the documentation
	// for the Request.Write method.
	Header Header

	// Body is the request's body.
	//
	// For client requests, a nil body means the request has no
	// body, such as a GET request. The HTTP Client's Transport
	// is responsible for calling the Close method.
	//
	// For server requests, the Request Body is always non-nil
	// but will return EOF immediately when no body is present.
	// The Server will close the request body. The ServeHTTP
	// Handler does not need to.
	Body io.ReadCloser

	// GetBody defines an optional func to return a new copy of
	// Body. It is used for client requests when a redirect requires
	// reading the body more than once. Use of GetBody still
	// requires setting Body.
	//
	// For server requests, it is unused.
	GetBody func() (io.ReadCloser, error)

	// ContentLength records the length of the associated content.
	// The value -1 indicates that the length is unknown.
	// Values >= 0 indicate that the given number of bytes may
	// be read from Body.
	//
	// For client requests, a value of 0 with a non-nil Body is
	// also treated as unknown.
	ContentLength int64

	// TransferEncoding lists the transfer encodings from outermost to
	// innermost. An empty list denotes the "identity" encoding.
	// TransferEncoding can usually be ignored; chunked encoding is
	// automatically added and removed as necessary when sending and
	// receiving requests.
	TransferEncoding []string

	// Close indicates whether to close the connection after
	// replying to this request (for servers) or after sending this
	// request and reading its response (for clients).
	//
	// For server requests, the HTTP server handles this automatically
	// and this field is not needed by Handlers.
	//
	// For client requests, setting this field prevents re-use of
	// TCP connections between requests to the same hosts, as if
	// Transport.DisableKeepAlives were set.
	Close bool

	// For server requests, Host specifies the host on which the
	// URL is sought. For HTTP/1 (per RFC 7230, section 5.4), this
	// is either the value of the "Host" header or the host name
	// given in the URL itself. For HTTP/2, it is the value of the
	// ":authority" pseudo-header field.
	// It may be of the form "host:port". For international domain
	// names, Host may be in Punycode or Unicode form. Use
	// golang.org/x/net/idna to convert it to either format if
	// needed.
	// To prevent DNS rebinding attacks, server Handlers should
	// validate that the Host header has a value for which the
	// Handler considers itself authoritative. The included
	// ServeMux supports patterns registered to particular host
	// names and thus protects its registered Handlers.
	//
	// For client requests, Host optionally overrides the Host
	// header to send. If empty, the Request.Write method uses
	// the value of URL.Host. Host may contain an international
	// domain name.
	Host string

	// Form contains the parsed form data, including both the URL
	// field's query parameters and the PATCH, POST, or PUT form data.
	// This field is only available after ParseForm is called.
	// The HTTP client ignores Form and uses Body instead.
	Form url.Values

	// PostForm contains the parsed form data from PATCH, POST
	// or PUT body parameters.
	//
	// This field is only available after ParseForm is called.
	// The HTTP client ignores PostForm and uses Body instead.
	PostForm url.Values

	// MultipartForm is the parsed multipart form, including file uploads.
	// This field is only available after ParseMultipartForm is called.
	// The HTTP client ignores MultipartForm and uses Body instead.
	MultipartForm *multipart.Form

	// Trailer specifies additional headers that are sent after the request
	// body.
	//
	// For server requests, the Trailer map initially contains only the
	// trailer keys, with nil values. (The client declares which trailers it
	// will later send.)  While the handler is reading from Body, it must
	// not reference Trailer. After reading from Body returns EOF, Trailer
	// can be read again and will contain non-nil values, if they were sent
	// by the client.
	//
	// For client requests, Trailer must be initialized to a map containing
	// the trailer keys to later send. The values may be nil or their final
	// values. The ContentLength must be 0 or -1, to send a chunked request.
	// After the HTTP request is sent the map values can be updated while
	// the request body is read. Once the body returns EOF, the caller must
	// not mutate Trailer.
	//
	// Few HTTP clients, servers, or proxies support HTTP trailers.
	Trailer Header

	// RemoteAddr allows HTTP servers and other software to record
	// the network address that sent the request, usually for
	// logging. This field is not filled in by ReadRequest and
	// has no defined format. The HTTP server in this package
	// sets RemoteAddr to an "IP:port" address before invoking a
	// handler.
	// This field is ignored by the HTTP client.
	RemoteAddr string

	// RequestURI is the unmodified request-target of the
	// Request-Line (RFC 7230, Section 3.1.1) as sent by the client
	// to a server. Usually the URL field should be used instead.
	// It is an error to set this field in an HTTP client request.
	RequestURI string

	// TLS allows HTTP servers and other software to record
	// information about the TLS connection on which the request
	// was received. This field is not filled in by ReadRequest.
	// The HTTP server in this package sets the field for
	// TLS-enabled connections before invoking a handler;
	// otherwise it leaves the field nil.
	// This field is ignored by the HTTP client.
	TLS *tls.ConnectionState

	// Cancel is an optional channel whose closure indicates that the client
	// request should be regarded as canceled. Not all implementations of
	// RoundTripper may support Cancel.
	//
	// For server requests, this field is not applicable.
	//
	// Deprecated: Set the Request's context with NewRequestWithContext
	// instead. If a Request's Cancel field and context are both
	// set, it is undefined whether Cancel is respected.
	Cancel <-chan struct{}

	// Response is the redirect response which caused this request
	// to be created. This field is only populated during client
	// redirects.
	Response *Response

	// ctx is either the client or server context. It should only
	// be modified via copying the whole Request using WithContext.
	// It is unexported to prevent people from using Context wrong
	// and mutating the contexts held by callers of the same request.
	ctx context.Context
}
```

看看[.NET 里对于 web 服务的抽象](https://docs.microsoft.com/en-us/dotnet/api/system.servicemodel.description.webhttpendpoint?view=netframework-4.8#definition)，仅仅看到末端，不去看完整个继承树的完整图景，我根本无法知道我关心的某个细节在什么位置。进而，我要往整个 http 服务体系里修改任何功能，都无法抛开对整体完整设计的理解和熟悉，还极容易没有知觉地破坏者整体的设计。

说到组合，还有一个关系很紧密的词，叫插件化。大家都用 vscode 用得很开心，它比 visual studio 成功在哪里？如果 vscode 通过添加一堆插件达到 visual studio 具备的能力，那么它将变成另一个和 visual studio 差不多的东西，叫做 vs studio 吧。大家应该发现问题了，我们很多时候其实并不需要 visual studio 的大多数功能，而且希望灵活定制化一些比较小众的能力，用一些小众的插件。甚至，我们希望选择不同实现的同类型插件。这就是组合的力量，各种不同的组合，它简单，却又满足了各种需求，灵活多变，要实现一个插件，不需要事先掌握一个庞大的体系。体现在代码上，也是一样的道理。至少后端开发领域，组合，比 OOP，'香'很多。

#### 6.3 原则 6 吝啬原则: 除非确无它法, 不要编写庞大的程序

可能有些同学会觉得，把程序写得庞大一些才好拿得出手去评 T11、T12。leader 们一看评审方案就容易觉得：很大，很好，很全面。但是，我们真的需要写这么大的程序么？

我又要说了"那么，古尔丹，代价是什么呢？"。代价是代码越多，越难维护，难调整。C 语言之父 Ken Thompson 说"删除一行代码，给我带来的成就感要比添加一行要大"。我们对于代码，要吝啬。能把系统做小，就不要做大。腾讯不乏 200w+行的客户端，很大，很牛。但是，同学们自问，现在还调整得动架构么。手 Q 的同学们，看看自己代码，曾经叹息过么。能小做的事情就小做，寻求通用化，通过 duck interface(甚至多进程，用于隔离能力的多线程)把模块、能力隔离开，时刻想着删减代码量，才能保持代码的可维护性和面对未来的需求、架构，调整自身的活力。客户端代码，UI 渲染模块可以复杂吊炸天，非 UI 部分应该追求最简单，能力接口化，可替换、重组合能力强。

落地到大家的代码，review 时，就应该最关注核心 struct 定义，构建起一个完备的模型，核心 interface，明确抽象 model 对外部的依赖，明确抽象 model 对外提供的能力。其他代码，就是要用最简单、平平无奇的代码实现模型内部细节。

#### 6.4 原则 7 透明性原则: 设计要可见，以便审查和调试

首先，定义一下，什么是透明性和可显性。

"如果没有阴暗的角落和隐藏的深度，软件系统就是透明的。透明性是一种被动的品质。如果实际上能预测到程序行为的全部或大部分情况，并能建立简单的心理模型，这个程序就是透明的，因为可以看透机器究竟在干什么。

如果软件系统所包含的功能是为了帮助人们对软件建立正确的'做什么、怎么做'的心理模型而设计，这个软件系统就是可显的。因此，举例来说，对用户而言，良好的文档有助于提高可显性；对程序员而言，良好的变量和函数名有助于提高可显性。可显性是一种主动品质。在软件中要达到这一点，仅仅做到不晦涩是不够的，还必须要尽力做到有帮助。"

我们要写好程序，减少 bug，就要增强自己对代码的控制力。你始终做到，理解自己调用的函数/复用的代码大概是怎么实现的。不然，你可能就会在单线程状态机的 server 里调用有 IO 阻塞的函数，让自己的 server 吞吐量直接掉到底。进而，为了保证大家能对自己代码能做到有控制力，所有人写的函数，就必须具备很高的透明性。而不是写一些看了一阵看不明白的函数/代码，结果被迫使用你代码的人，直接放弃了对掌控力的追取，甚至放弃复用你的代码，另起炉灶，走向了'制造重复代码'的深渊。

透明性其实相对容易做到的，大家有意识地锻炼一两个月，就能做得很好。可显性就不容易了。有一个现象是，你写的每一个函数都不超过 80 行，每一行我都能看懂，但是你层层调用，很多函数调用，组合起来怎么就实现了某个功能，看两遍，还是看不懂。第三遍可能才能大概看懂。大概看懂了，但太复杂，很难在大脑里构建起你实现这个功能的整体流程。结果就是，阅读者根本做不到对你的代码有好的掌控力。

可显性的标准很简单，大家看一段代码，懂不懂，一下就明白了。但是，如何做好可显性？那就是要追求合理的函数分组，合理的函数上下级层次，同一层次的代码才会出现在同一个函数里，追求通俗易懂的函数分组分层方式，是通往可显性的道路。

当然，复杂如 linux 操作系统，office 文档，问题本身就很复杂，拆解、分层、组合得再合理，都难建立心理模型。这个时候，就需要完备的文档了。完备的文档还需要出现在离代码最近的地方，让人'知道这里复杂的逻辑有文档'，而不是其实文档，但是阅读者不知道。再看看上面 golang 标准库里的 http.Request，感受到它在可显性上的努力了么？对，就去学它。

#### 6.5 原则 10 通俗原则: 接口设计避免标新立异

设计程序过于标新立异的话，可能会提升别人理解的难度。

一般，我们这么定义一个'点'，使用 x 表示横坐标，用 y 表示纵坐标：

```
type Point struct {
 X float64
 Y float64
}
```

你就是要不同、精准：

```
type Point struct {
 VerticalOrdinate   float64
 HorizontalOrdinate float64
}
```

很好，你用词很精准，一般人还驳斥不了你。但是，多数人读你的 VerticalOrdinate 就是没有读 X 理解来得快，来得容易懂、方便。你是在刻意制造协作成本。

上面的例子常见，但还不是最小立异原则最想说明的问题。想想一下，一个程序里，你把用'+'这个符号表示数组添加元素，而不是数学'加'，'result := 1+2' --> 'result = []int{1, 2}'而不是'result=3'，那么，你这个标新立异，对程序的破坏性，简直无法想象。"最小立异原则的另一面是避免表象想死而实际却略有不同。这会极端危险，因为表象相似往往导致人们产生错误的假定。所以最好让不同事物有明显区别，而不要看起来几乎一模一样。" -- Henry Spencer。

你实现一个 db.Add()函数却做着 db.AddOrUpdate()的操作，有人使用了你的接口，错误地把数据覆盖了。

**原则 11 缄默原则: 如果一个程序没什么好说的，就沉默**

这个原则，应该是大家最经常破坏的原则之一。一段简短的代码里插入了各种'log("cmd xxx enter")', 'log("req data " + req.String())'，非常害怕自己信息打印得不够。害怕自己不知道程序执行成功了，总要最后'log("success")'。但是，我问一下大家，你们真的耐心看过别人写的代码打的一堆日志么？不是自己需要哪个，就在一堆日志里，再打印一个日志出来一个带有特殊标记的日志'log("this_is_my_log_" + xxxxx)'？结果，第一个作者打印的日志，在代码交接给其他人或者在跟别人协作的时候，这个日志根本没有价值，反而提升了大家看日志的难度。

一个服务一跑起来，就疯狂打日志，请求处理正常也打一堆日志。滚滚而来的日志，把错误日志淹没在里面。错误日志失去了效果，简单地 tail 查看日志，眼花缭乱，看不出任何问题，这不就成了'为了捕获问题'而让自己'根本无法捕获问题'了么？

沉默是金。除了简单的 stat log，如果你的程序'发声'了，那么它抛出的信息就一定要有效！打印一个 log('process fail')也是毫无价值，到底什么 fail 了？是哪个用户带着什么参数在哪个环节怎么 fail 了？如果发声，就要把必要信息给全。不然就是不发声，表示自己好好地 work 着呢。不发声就是最好的消息，现在我的 work 一切正常！

"设计良好的程序将用户的注意力视为有限的宝贵资源，只有在必要时才要求使用。"程序员自己的主力，也是宝贵的资源！只有有必要的时候，日志才跑来提醒程序员'我有问题，来看看'，而且，必须要给到足够的信息，让一把讲明白现在发生了什么。而不是程序员还需要很多辅助手段来搞明白到底发生了什么。

每当我发布程序 ，我抽查一个机器，看它的日志。发现只有每分钟外部接入、内部 rpc 的个数/延时分布日志的时候，我就心情很愉悦。我知道，这一分钟，它的成功率又是 100%，没任何问题！

#### 6.6 原则 12 补救原则: 出现异常时，马上退出并给出足够错误信息

其实这个问题很简单，如果出现异常，异常并不会因为我们尝试掩盖它，它就不存在了。所以，程序错误和逻辑错误要严格区分对待。这是一个态度问题。

'异常是互联网服务器的常态'。逻辑错误通过 metrics 统计，我们做好告警分析。对于程序错误 ，我们就必须要严格做到在问题最早出现的位置就把必要的信息搜集起来，高调地告知开发和维护者'我出现异常了，请立即修复我!'。可以是直接就没有被捕获的 panic 了。也可以在一个最上层的位置统一做好 recover 机制，但是在 recover 的时候一定要能获得准确异常位置的准确异常信息。不能有中间 catch 机制，catch 之后丢失很多信息再往上传递。

很多 Java 开发的同学，不区分程序错误和逻辑错误，要么都很宽容，要么都很严格，对代码的可维护性是毁灭性的破坏。"我的程序没有程序错误，如果有，我当时就解决了。"只有这样，才能保持程序代码质量的相对稳定，在火苗出现时扑灭火灾是最好的扑灭火灾的方式。当然，更有效的方式是全面自动化测试的预防：）

### 7.**具体实践点**

前面提了好多思考方向的问题。大的原则问题和方向。我这里，再来给大家简单列举几个细节执行点吧。毕竟，大家要上手，是从执行开始，然后才是总结思考，能把我的思考方式抄过去。下面是针对 golang 语言的，其他语言略有不同。以及，我一时也想不全我所执行的 所有细则，这就是我强调'原则'的重要性，原则是可枚举的。

- 对于代码格式规范，100%严格执行，严重容不得一点沙。
- 文件绝不能超过 800 行，超过，一定要思考怎么拆文件。工程思维，就在于拆文件的时候积累。
- 函数对决不能超过 80 行，超过，一定要思考怎么拆函数，思考函数分组，层次。工程思维，就在于拆文件的时候积累。
- 代码嵌套层次不能超过 4 层，超过了就得改。多想想能不能 early return。工程思维，就在于拆文件的时候积累。

```
if !needContinue {
 doA()
 return
} else {
 doB()
 return
}
if !needContinue {
 doA()
 return
}

doB()
return
```

下面这个就是 early return，把两端代码从逻辑上解耦了。

- 从目录、package、文件、struct、function 一层层下来 ，信息一定不能出现冗余。比如 file.FileProperty 这种定义。只有每个'定语'只出现在一个位置，才为'做好逻辑、定义分组/分层'提供了可能性。
- 多用多级目录来组织代码所承载的信息，即使某一些中间目录只有一个子目录。
- 随着代码的扩展，老的代码违反了一些设计原则，应该立即原地局部重构，维持住代码质量不滑坡。比如:拆文件；拆函数；用 Session 来保存一个复杂的流程型函数的所有信息；重新调整目录结构。
- 基于上一点考虑，我们应该尽量让项目的代码有一定的组织、层次关系。我个人的当前实践是除了特别通用的代码，都放在一个 git 里。特别通用、修改少的代码，逐渐独立出 git，作为子 git 连接到当前项目 git，让 goland 的 Refactor 特性、各种 Refactor 工具能帮助我们快速、安全局部重构。
- 自己的项目代码，应该有一个内生的层级和逻辑关系。flat 平铺展开是非常不利于代码复用的。怎么复用、怎么组织复用，肯定会变成'人生难题'。T4-T7 的同学根本无力解决这种难题。
- 如果被 review 的代码虽然简短，但是你看了一眼却发现不咋懂，那就一定有问题。自己看不出来，就找高级别的同学交流。这是你和别 review 代码的同学成长的时刻。
- 日志要少打。要打日志就要把关键索引信息带上。必要的日志必须打。
- 有疑问就立即问，不要怕问错。让代码作者给出解释。不要怕问出极低问题。
- 不要说'建议'，提问题，就是刚，你 pk 不过我，就得改！
- 请积极使用 trpc。总是要和老板站在一起！只有和老板达成的对于代码质量建设的共识，才能在团队里更好地做好代码质量建设。
- 消灭重复！消灭重复！消灭重复！

### 8.**主干开发**

最后，我来为'主干开发'多说一句话。道理很简单，只有每次被 review 代码不到 500 行，reviewer 才能快速地看完，而且几乎不会看漏。超过 500 行，reviewer 就不能仔细看，只能大概浏览了。而且，让你调整 500 行代码内的逻辑比调整 3000 行甚至更多的代码，容易很多，降低不仅仅是 6 倍，而是一到两个数量级。有问题，在刚出现的时候就调整了，不会给被 revew 的人带来大的修改负担。

关于 CI(continuous integration)，还有很多好的资料和书籍，大家应该及时去学习学习。

### 9.**《unix 编程艺术》**

建议大家把这本书找出来读一读。特别是，T7 及更高级别的同学。你们已经积累了大量的代码实践，亟需对'工程性'做思考总结。很多工程方法论都过时了，这本书的内容，是例外中的例外。它所表达出的内容没有因为软件技术的不断更替而过时。

佛教禅宗讲'不立文字'(不立文字，教外别传，直指人心，见性成佛)，很多道理和感悟是不能用文字传达的，文字的表达能力，不能表达。大家常常因为"自己听说过、知道某个道理"而产生一种安心感，认为"我懂了这个道理"，但是自己却不能在实践中做到。知易行难，知道却做不到，在工程实践里，就和'不懂这个道理'没有任何区别了。

曾经，我面试过一个别的公司的总监，讲得好像一套一套，代码拉出来遛一遛，根本就没做到，仅仅会道听途说。他在工程实践上的探索前路可以说已经基本断绝了。我只能祝君能做好向上管理，走自己的纯管理道路吧。请不要再说自己对技术有追求，是个技术人了！

所以，大家不仅仅是看看我这篇文章，而是在实践中去不断践行和积累自己的'教外别传'吧。

[Software Engineering at Google](https://www.oreilly.com/library/view/software-engineering-at/9781492082781/)也是一本必读好书，可惜没找到中文翻译。

原文作者：cheaterlin，腾讯 PCG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/c3RApB8a98tWahgC9mahJg

# 【NO.240】HTTP/3 原理实战

015 年 HTTP/2 标准发表后，大多数主流浏览器也于当年年底支持该标准。此后，凭借着多路复用、头部压缩、服务器推送等优势，HTTP/2 得到了越来越多开发者的青睐。不知不觉的 HTTP 已经发展到了第三代，鹅厂也紧跟技术潮流，很多项目也在逐渐使用 HTTP/3。本文基于兴趣部落接入 HTTP/3 的实践，聊一聊 HTTP/3 的原理以及业务接入的方式。

## **1. HTTP/3 原理**

### 1.1 HTTP 历史

在介绍 HTTP/3 之前，我们先简单看下 HTTP 的历史，了解下 HTTP/3 出现的背景。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWnvOermFicEiaDXia4lnGho0CVXCVAqjOakxAluLcrPVdYM44kqUQeKX2w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

随着网络技术的发展，1999 年设计的 HTTP/1.1 已经不能满足需求，所以 Google 在 2009 年设计了基于 TCP 的 SPDY，后来 SPDY 的开发组推动 SPDY 成为正式标准，不过最终没能通过。不过 SPDY 的开发组全程参与了 HTTP/2 的制定过程，参考了 SPDY 的很多设计，所以我们一般认为 SPDY 就是 HTTP/2 的前身。无论 SPDY 还是 HTTP/2，都是基于 TCP 的，TCP 与 UDP 相比效率上存在天然的劣势，所以 2013 年 Google 开发了基于 UDP 的名为 QUIC 的传输层协议，QUIC 全称 Quick UDP Internet Connections，希望它能替代 TCP，使得网页传输更加高效。后经[提议](https://mailarchive.ietf.org/arch/msg/quic/RLRs4nB1lwFCZ_7k0iuz0ZBa35s)，互联网工程任务组正式将基于 QUIC 协议的 HTTP （HTTP over QUIC）重命名为 HTTP/3。

### 1.2 QUIC 协议概览

TCP 一直是传输层中举足轻重的协议，而 UDP 则默默无闻，在面试中问到 TCP 和 UDP 的区别时，有关 UDP 的回答常常寥寥几语，长期以来 UDP 给人的印象就是一个很快但不可靠的传输层协议。但有时候从另一个角度看，缺点可能也是优点。QUIC（Quick UDP Internet Connections，快速 UDP 网络连接） 基于 UDP，正是看中了 UDP 的速度与效率。同时 QUIC 也整合了 TCP、TLS 和 HTTP/2 的优点，并加以优化。用一张图可以清晰地表示他们之间的关系。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWYBhWzvwiaL1e7urxqw61NLjQjtIY3RSBRG79iaG7AKLguu0LxCdWZD5g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

那 QUIC 和 HTTP/3 什么关系呢？QUIC 是用来替代 TCP、SSL/TLS 的传输层协议，在传输层之上还有应用层，我们熟知的应用层协议有 HTTP、FTP、IMAP 等，这些协议理论上都可以运行在 QUIC 之上，其中运行在 QUIC 之上的 HTTP 协议被称为 HTTP/3，这就是”HTTP over QUIC 即 HTTP/3“的含义。

因此想要了解 HTTP/3，QUIC 是绕不过去的，下面主要通过几个重要的特性让大家对 QUIC 有更深的理解。

### 1.3 零 RTT 建立连接

用一张图可以形象地看出 HTTP/2 和 HTTP/3 建立连接的差别。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWmEEdP9aadwVFaSdqpMZyMfqAo1gMibB9UIjDxEiatGc6HJ70GG7OdYyQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

HTTP/2 的连接需要 3 RTT，如果考虑会话复用，即把第一次握手算出来的对称密钥缓存起来，那么也需要 2 RTT，更进一步的，如果 TLS 升级到 1.3，那么 HTTP/2 连接需要 2 RTT，考虑会话复用则需要 1 RTT。有人会说 HTTP/2 不一定需要 HTTPS，握手过程还可以简化。这没毛病，HTTP/2 的标准的确不需要基于 HTTPS，但实际上所有浏览器的实现都要求 HTTP/2 必须基于 HTTPS，所以 HTTP/2 的加密连接必不可少。而 HTTP/3 首次连接只需要 1 RTT，后面的连接更是只需 0 RTT，意味着客户端发给服务端的第一个包就带有请求数据，这一点 HTTP/2 难以望其项背。那这背后是什么原理呢？我们具体看下 QUIC 的连接过程。

**Step1**：首次连接时，客户端发送 Inchoate Client Hello 给服务端，用于请求连接；

**Step2**：服务端生成 g、p、a，根据 g、p 和 a 算出 A，然后将 g、p、A 放到 Server Config 中再发送 Rejection 消息给客户端；

**Step3**：客户端接收到 g、p、A 后，自己再生成 b，根据 g、p、b 算出 B，根据 A、p、b 算出初始密钥 K。B 和 K 算好后，客户端会用 K 加密 HTTP 数据，连同 B 一起发送给服务端；

**Step4**：服务端接收到 B 后，根据 a、p、B 生成与客户端同样的密钥，再用这密钥解密收到的 HTTP 数据。为了进一步的安全（前向安全性），服务端会更新自己的随机数 a 和公钥，再生成新的密钥 S，然后把公钥通过 Server Hello 发送给客户端。连同 Server Hello 消息，还有 HTTP 返回数据；

**Step5**：客户端收到 Server Hello 后，生成与服务端一致的新密钥 S，后面的传输都使用 S 加密。

这样，QUIC 从请求连接到正式接发 HTTP 数据一共花了 1 RTT，这 1 个 RTT 主要是为了获取 Server Config，后面的连接如果客户端缓存了 Server Config，那么就可以直接发送 HTTP 数据，实现 0 RTT 建立连接。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWECanehia5OEtUVNOehe2Y8T2HOfqHjhzDETiaTwSeVZqNuFIFuPjC1sg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

这里使用的是 DH 密钥交换算法，DH 算法的核心就是服务端生成 a、g、p 3 个随机数，a 自己持有，g 和 p 要传输给客户端，而客户端会生成 b 这 1 个随机数，通过 DH 算法客户端和服务端可以算出同样的密钥。在这过程中 a 和 b 并不参与网络传输，安全性大大提高。因为 p 和 g 是大数，所以即使在网络中传输的 p、g、A、B 都被劫持，那么靠现在的计算机算力也没法破解密钥。

### 1.4 连接迁移

TCP 连接基于四元组（源 IP、源端口、目的 IP、目的端口），切换网络时至少会有一个因素发生变化，导致连接发生变化。当连接发生变化时，如果还使用原来的 TCP 连接，则会导致连接失败，就得等原来的连接超时后重新建立连接，所以我们有时候发现切换到一个新网络时，即使新网络状况良好，但内容还是需要加载很久。如果实现得好，当检测到网络变化时立刻建立新的 TCP 连接，即使这样，建立新的连接还是需要几百毫秒的时间。

QUIC 的连接不受四元组的影响，当这四个元素发生变化时，原连接依然维持。那这是怎么做到的呢？道理很简单，QUIC 连接不以四元组作为标识，而是使用一个 64 位的随机数，这个随机数被称为 Connection ID，即使 IP 或者端口发生变化，只要 Connection ID 没有变化，那么连接依然可以维持。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWHQtiapr3Ou7KEXpN73yBzibfnTY7H3qNNSOyJKYHXzZ85OaRPoGAxYKA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 1.5 队头阻塞/多路复用

HTTP/1.1 和 HTTP/2 都存在队头阻塞问题（Head of line blocking），那什么是队头阻塞呢？

TCP 是个面向连接的协议，即发送请求后需要收到 ACK 消息，以确认对方已接收到数据。如果每次请求都要在收到上次请求的 ACK 消息后再请求，那么效率无疑很低。后来 HTTP/1.1 提出了 Pipelining 技术，允许一个 TCP 连接同时发送多个请求，这样就大大提升了传输效率。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWM59JxDoQiaBBpyw9R3IfhwiahY1Pn7NZMwo6Wq84F7I3pvb1hopSQwkg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

在这个背景下，下面就来谈 HTTP/1.1 的队头阻塞。下图中，一个 TCP 连接同时传输 10 个请求，其中第 1、2、3 个请求已被客户端接收，但第 4 个请求丢失，那么后面第 5 - 10 个请求都被阻塞，需要等第 4 个请求处理完毕才能被处理，这样就浪费了带宽资源。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWTrHQW4h5T1cgt4rBMPuvJ5pj4CtfcgQR4Tl0sgWlrmYzFXQCdJDRSQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

因此，HTTP 一般又允许每个主机建立 6 个 TCP 连接，这样可以更加充分地利用带宽资源，但每个连接中队头阻塞的问题还是存在。

HTTP/2 的多路复用解决了上述的队头阻塞问题。不像 HTTP/1.1 中只有上一个请求的所有数据包被传输完毕下一个请求的数据包才可以被传输，HTTP/2 中每个请求都被拆分成多个 Frame 通过一条 TCP 连接同时被传输，这样即使一个请求被阻塞，也不会影响其他的请求。如下图所示，不同颜色代表不同的请求，相同颜色的色块代表请求被切分的 Frame。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWpYxO466rVmFzmic38XsyZ9Ydd0Im4L671icWnZmmhuicQu58EibwkGNkEA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

事情还没完，HTTP/2 虽然可以解决“请求”这个粒度的阻塞，但 HTTP/2 的基础 TCP 协议本身却也存在着队头阻塞的问题。HTTP/2 的每个请求都会被拆分成多个 Frame，不同请求的 Frame 组合成 Stream，Stream 是 TCP 上的逻辑传输单元，这样 HTTP/2 就达到了一条连接同时发送多条请求的目标，这就是多路复用的原理。我们看一个例子，在一条 TCP 连接上同时发送 4 个 Stream，其中 Stream1 已正确送达，Stream2 中的第 3 个 Frame 丢失，TCP 处理数据时有严格的前后顺序，先发送的 Frame 要先被处理，这样就会要求发送方重新发送第 3 个 Frame，Stream3 和 Stream4 虽然已到达但却不能被处理，那么这时整条连接都被阻塞。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWmSLxgFU3bT0Ccic1wdXDuzJab4BJzYYMD4wArCKMczwjj4WbIysjqQQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

不仅如此，由于 HTTP/2 必须使用 HTTPS，而 HTTPS 使用的 TLS 协议也存在队头阻塞问题。TLS 基于 Record 组织数据，将一堆数据放在一起（即一个 Record）加密，加密完后又拆分成多个 TCP 包传输。一般每个 Record 16K，包含 12 个 TCP 包，这样如果 12 个 TCP 包中有任何一个包丢失，那么整个 Record 都无法解密。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWdoAdNLia6SY2PnmfFqHqPwtPJzfYqJkrGLba8j7s7udVUqlm9z260FQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

队头阻塞会导致 HTTP/2 在更容易丢包的弱网络环境下比 HTTP/1.1 更慢！

那 QUIC 是如何解决队头阻塞问题的呢？主要有两点。

- QUIC 的传输单元是 Packet，加密单元也是 Packet，整个加密、传输、解密都基于 Packet，这样就能避免 TLS 的队头阻塞问题；
- QUIC 基于 UDP，UDP 的数据包在接收端没有处理顺序，即使中间丢失一个包，也不会阻塞整条连接，其他的资源会被正常处理。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWuO3G08LUiaSGKNwyWeUlicOcV6Vm4jKZbBPfGcmqn5uX93KibLLDn70pw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 1.6 拥塞控制

拥塞控制的目的是避免过多的数据一下子涌入网络，导致网络超出最大负荷。QUIC 的拥塞控制与 TCP 类似，并在此基础上做了改进。所以我们先简单介绍下 TCP 的拥塞控制。

TCP 拥塞控制由 4 个核心算法组成：慢启动、拥塞避免、快速重传和快速恢复，理解了这 4 个算法，对 TCP 的拥塞控制也就有了大概了解。

- 慢启动：发送方向接收方发送 1 个单位的数据，收到对方确认后会发送 2 个单位的数据，然后依次是 4 个、8 个……呈指数级增长，这个过程就是在不断试探网络的拥塞程度，超出阈值则会导致网络拥塞；
- 拥塞避免：指数增长不可能是无限的，到达某个限制（慢启动阈值）之后，指数增长变为线性增长；
- 快速重传：发送方每一次发送时都会设置一个超时计时器，超时后即认为丢失，需要重发；
- 快速恢复：在上面快速重传的基础上，发送方重新发送数据时，也会启动一个超时定时器，如果收到确认消息则进入拥塞避免阶段，如果仍然超时，则回到慢启动阶段。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWjGibTw5K3FJaticxkIuTv0KTL4UfZswO9743OOibCebvaeDF5UxKIvpdg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

QUIC 重新实现了 TCP 协议的 Cubic 算法进行拥塞控制，并在此基础上做了不少改进。下面介绍一些 QUIC 改进的拥塞控制的特性。

#### **1.6.1 热插拔**

TCP 中如果要修改拥塞控制策略，需要在系统层面进行操作。QUIC 修改拥塞控制策略只需要在应用层操作，并且 QUIC 会根据不同的网络环境、用户来动态选择拥塞控制算法。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWaoEc2pM2rXsgLqZbtUZgxWppaLlFmibuYEUdNSOYNnzCL0SIiajk93qA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **1.6.2 前向纠错 FEC**

QUIC 使用前向纠错(FEC，Forward Error Correction)技术增加协议的容错性。一段数据被切分为 10 个包后，依次对每个包进行异或运算，运算结果会作为 FEC 包与数据包一起被传输，如果不幸在传输过程中有一个数据包丢失，那么就可以根据剩余 9 个包以及 FEC 包推算出丢失的那个包的数据，这样就大大增加了协议的容错性。

这是符合现阶段网络技术的一种方案，现阶段带宽已经不是网络传输的瓶颈，往返时间才是，所以新的网络传输协议可以适当增加数据冗余，减少重传操作。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWtFEqffsZuZb21l36HrmeQcae4QgjaLMYc1epb1xQpNuWA9Peqqxt8g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **1.6.3 单调递增的 Packet Number**

TCP 为了保证可靠性，使用 Sequence Number 和 ACK 来确认消息是否有序到达，但这样的设计存在缺陷。

超时发生后客户端发起重传，后来接收到了 ACK 确认消息，但因为原始请求和重传请求接收到的 ACK 消息一样，所以客户端就郁闷了，不知道这个 ACK 对应的是原始请求还是重传请求。如果客户端认为是原始请求的 ACK，但实际上是左图的情形，则计算的采样 RTT 偏大；如果客户端认为是重传请求的 ACK，但实际上是右图的情形，又会导致采样 RTT 偏小。图中有几个术语，RTO 是指超时重传时间（Retransmission TimeOut），跟我们熟悉的 RTT（Round Trip Time，往返时间）很长得很像。采样 RTT 会影响 RTO 计算，超时时间的准确把握很重要，长了短了都不合适。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWoC6QKq0BIKbpKlXYGH85EickJ63iaH2CrhOFTRd5AbzSVllXrd3ib71lw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

QUIC 解决了上面的歧义问题。与 Sequence Number 不同的是，Packet Number 严格单调递增，如果 Packet N 丢失了，那么重传时 Packet 的标识不会是 N，而是比 N 大的数字，比如 N + M，这样发送方接收到确认消息时就能方便地知道 ACK 对应的是原始请求还是重传请求。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWuQ8pcjaOB2nqsnjkVTxx5yXGSE10NrX9iaN54vFU1WSCQWmpBxVLOTg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **1.6.4 ACK Delay**

TCP 计算 RTT 时没有考虑接收方接收到数据到发送确认消息之间的延迟，如下图所示，这段延迟即 ACK Delay。QUIC 考虑了这段延迟，使得 RTT 的计算更加准确。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWdaia1QzrXBL2CkUMW2oSGzibiceEgBM9XF4w1d5uzjRKMnUicpiayFFmPYQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### **1.6.5 更多的 ACK 块**

一般来说，接收方收到发送方的消息后都应该发送一个 ACK 回复，表示收到了数据。但每收到一个数据就返回一个 ACK 回复太麻烦，所以一般不会立即回复，而是接收到多个数据后再回复，TCP SACK 最多提供 3 个 ACK block。但有些场景下，比如下载，只需要服务器返回数据就好，但按照 TCP 的设计，每收到 3 个数据包就要“礼貌性”地返回一个 ACK。而 QUIC 最多可以捎带 256 个 ACK block。在丢包率比较严重的网络下，更多的 ACK block 可以减少重传量，提升网络效率。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWehdleicj8WJiahBoOn06vYqPICDdMWSaaKHMvHGSDQy6pWVhLAWYYekQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 1.7 流量控制

TCP 会对每个 TCP 连接进行流量控制，流量控制的意思是让发送方不要发送太快，要让接收方来得及接收，不然会导致数据溢出而丢失，TCP 的流量控制主要通过滑动窗口来实现的。可以看出，拥塞控制主要是控制发送方的发送策略，但没有考虑到接收方的接收能力，流量控制是对这部分能力的补齐。

QUIC 只需要建立一条连接，在这条连接上同时传输多条 Stream，好比有一条道路，两头分别有一个仓库，道路中有很多车辆运送物资。QUIC 的流量控制有两个级别：连接级别（Connection Level）和 Stream 级别（Stream Level），好比既要控制这条路的总流量，不要一下子很多车辆涌进来，货物来不及处理，也不能一个车辆一下子运送很多货物，这样货物也来不及处理。

那 QUIC 是怎么实现流量控制的呢？我们先看单条 Stream 的流量控制。Stream 还没传输数据时，接收窗口（flow control receive window）就是最大接收窗口（flow control receive window），随着接收方接收到数据后，接收窗口不断缩小。在接收到的数据中，有的数据已被处理，而有的数据还没来得及被处理。如下图所示，蓝色块表示已处理数据，黄色块表示未处理数据，这部分数据的到来，使得 Stream 的接收窗口缩小。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWLwmYjffpz4mUJmCqCjp05e945NcZnkHKvH8GIk2m0y6NGnIUgAC1xQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

随着数据不断被处理，接收方就有能力处理更多数据。当满足 (flow control receive offset - consumed bytes) < (max receive window / 2) 时，接收方会发送 WINDOW_UPDATE frame 告诉发送方你可以再多发送些数据过来。这时 flow control receive offset 就会偏移，接收窗口增大，发送方可以发送更多数据到接收方。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWfIwUqJOaoPCzh4ic2mOeI6bFicO65fAHVH3wWTLSFNblFd6OPqO5mFoQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

Stream 级别对防止接收端接收过多数据作用有限，更需要借助 Connection 级别的流量控制。理解了 Stream 流量那么也很好理解 Connection 流控。Stream 中，接收窗口(flow control receive window) = 最大接收窗口(max receive window) - 已接收数据(highest received byte offset) ，而对 Connection 来说：接收窗口 = Stream1 接收窗口 + Stream2 接收窗口 + ... + StreamN 接收窗口 。

## **2. HTTP/3 实践**

### 2.1 X5 内核与 STGW

X5 内核是腾讯开发的适用于安卓系统的浏览器内核，为了解决传统安卓系统浏览器内核适配成本高、不安全、不稳定等问题而开发的统一的浏览器内核。STGW 是 Secure Tencent Gateway 的缩写，意思是腾讯安全云网关。两者早在前两年便支持了 QUIC 协议。

那作为运行在 X5 上的业务，我们该如何接入 QUIC 呢？得益于 X5 和 STGW，业务在接入 QUIC 时所需要做的改动非常小，只需要两步。

**Step 1.** 在 STGW 上开启白名单，允许业务域名接入 QUIC 协议；

**Step 2.** 业务资源的 Response Header 添加 alt-svc 属性，示例：alt-svc: quic=":443"; ma=2592000; v="44,43,39"。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWoiafEXezdpgh5rA3YNfLcibgxKjRiata4CfdbT0OJnMPgrOz8jibWGMl6g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

接入 QUIC 时，STGW 的优势非常明显，由 STGW 与支持 QUIC 的客户端(这里是 X5)进行通信，而业务后台与 STGW 仍然使用 HTTP/1.1 通信，QUIC 所需要的 Server Config 等缓存信息也都是由 STGW 维护。

### 2.2 协商升级与竞速

业务域名加入了 STGW 的白名单，业务资源的 Response Header 也添加了 alt-svc 属性，那 QUIC 是如何建立连接的呢？这里有个关键的步骤：协商升级。客户端不确定服务器是否支持 QUIC，如果贸然地请求建立 QUIC 连接可能会失败，所以需要经历协商升级过程才能决定是否使用 QUIC。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWVZjCk0mIhIfnbQhbdcBDkKgN1KJdrABMsMiaibD1EFJ8VF2s72ibsLbFw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

首次请求时，客户端会使用 HTTP/1.1 或者 HTTP/2，如果服务器支持 QUIC，则在响应的数据中返回 alt-svc 头部，告诉客户端下次请求可以走 QUIC。alt-svc 主要包含以下信息：

- quic：监听的端口；
- ma：有效时间，单位是秒，承诺在这段时间内都支持 QUIC；
- 版本号：QUIC 的迭代很快，这里列出所有支持的版本号。

确认服务器支持 QUIC 之后，客户端向服务端同时发起 QUIC 连接和 TCP 连接，比较两个连接的速度，然后选择较快的协议，这个过程叫“竞速”，一般都是 QUIC 获胜。

### 2.3 QUIC 性能表现

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWkOia4ngYsTQXtJ552XqgchZMrUuO4WqUREY2dOR7AD3LVD8t4DwFyqA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

QUIC 建立连接的成功率在 90% 以上，竞速成功率也接近 90%，0 RTT 率在 55% 左右。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWZkJs7ufuV8MJ2LQw8sKQiaWPXQEYdL2ibMUtEj4K4MaczBhEOdN8m2vg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

使用 QUIC 协议时页面首屏耗时要比非 QUIC 协议减少 10%。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvWyxXibIPKnH4lowsamFU16SFtuRXc3DCwBbj8JI3sqsibvrJiaMTgZ3qYQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

从资源获取的不同阶段看，QUIC 协议在连接阶段节省的时间比较明显。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatOuue8A7K8MQAlFoCOtDvW95OV7N3o5uV4qFNQ7DkWKx5bLgzQUupakFt5qy140gRNgAia9MR56Rw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

从页面首屏区间占比图中可以看出，使用了 QUIC 协议后，首屏耗时在 1 秒内的占比提升明显，大约在 12% 左右。

## **3. 总结**

QUIC 丢掉了 TCP、TLS 的包袱，基于 UDP，并对 TCP、TLS、HTTP/2 的经验加以借鉴、改进，实现了一个安全高效可靠的 HTTP 通信协议。凭借着 0 RTT 建立连接、平滑的连接迁移、基本消除了队头阻塞、改进的拥塞控制和流量控制等优秀的特性，QUIC 在绝大多数场景下获得了比 HTTP/2 更好的效果。

一周前，微软宣布开源自己的内部 QUIC 库 -- MsQuic，将全面推荐 QUIC 协议替换 TCP/IP 协议。

原文作者：billpchen，腾讯看点前端开发工程师

原文链接：https://mp.weixin.qq.com/s/MHYMOYHqhrAbQ0xtTkV2ig

# 【NO.241】协程及c++ 20原生协程研究报告

> 以下内容来自于腾讯后台研发工程师johnyao

## 0.**引言**

最近对C++20协程的进行了预研, 作为对比，同时研究了下市面上已经存在的其他协程实现方案。

虽然工作重点是C++20协程的预研，但作为一篇完整的文章, 不可避免的要从协程的基础开始讲起。

本文第一部分介绍现有一些协程的实现方案。如果你已经对协程非常熟悉，尤其是知道栈(stack)，帧(frame)在协程知识体系中意义，可以跳过第一部分。

本文第二部分介绍C++20协程的实现。如果你已经对C++20协程有所了解，知道C++20协程的基础实现，可以跳过第二部分。

本书第三部分是实验数据，以及C++20协程在项目内落地的设想，欢迎各位同学参与讨论。

## **1. 协程现状**

### **1.1 协程概述**

关于协程的定义和实现，并没有像进程和线程那样有统一的标准。而且由于都有一个“程”字，初学者很容易去和线程和进程做类比，往往走进理解的误区。

所以我们一开始只是引用维基的定义，简单说明下协程是什么：“协程是允许执行被挂起与被恢复子例程（也称为子程序 subroutine）”。

这一章节，我从函数切换的寄存器操作入手，继而通过协程的实现，和不同协程分类标准的介绍，帮助读者理解协程的本质。

### **1.2 函数切换 & 栈 & 帧**

**背景知识**

我们都知道进程内存空间被划分为下面的各内存段（引自维基百科Code segment）：

1. text 代码段
2. data 已经初始化的全局数据
3. bss 未初始化的全局数据
4. heap 堆区
5. stack 栈区

对于函数切换以及后面介绍的协程切换，我们最关心的是栈区的内存的管理。

当函数调用发生后，会扩展栈区内存，用于存放函数体内声明的局部变量。当被调函数返回时，则释放这部分内存。每次函数调用扩展的这部分内存我们称之为栈帧。**函数调用栈(call stack)就是由栈帧组成的，函数调用和返回就是栈帧（stack frame）入栈出栈的过程。**

我们以如下代码为例，看下这个过程中详细步骤。

```
#include <cstdint>

int64_t f(int p1, int p2, int p3, int p4, int p5, int p6, int p7, int p8)

{

int x = 0;

int y = 0;

int64_t z = 0;

int64_t l = 0;

x++;

p1++;

p7++;

p8++;

return 0;

}



int main()

{

int64_t i = 0;

i++;

i = f(1, 2, 3, 4, 5, 6, 7, 8);

return 0;

}
```

这里需要一些汇编的基础知识，但由于这部分不是此篇文章的重点，感兴趣的同学可以自行学习。 对于本文要解释的函数切换过程，我们只需要了解如下内容：维持栈帧需要两个寄存器的支持，对于x86_64架构，即%rbp，%rsp。在本文中, 我们把他们称为

1. rbp **栈帧基地址寄存**： 当前栈帧的起始内存地址。
2. rsp **栈顶地址寄存器**：整个调用栈的栈顶地址。

此外还需要了解：函数需要执行的下一条指令存放在%rip中，我们称之为指令地址寄存器。

有了如上的背景知识，我们看下在main函数中调用函数f，具体发生了什么。以此来加深栈帧的概念。

### 1.3 **参数传递**

根据上一节的内容，在函数f调用发生之前，%rbp指向main函数对应栈帧的基地址。%rsp指向整个栈区的顶地址。

函数调用的第一步是参数传递。上面例子参数传递部分，在gcc编译器下，产生的汇编如下：

pushq $8

pushq $7

movl $6, %r9d

movl $5, %r8d

movl $4, %ecx

movl $3, %edx

movl $2, %esi

movl $1, %edi

参数个数小于等于6时，采用寄存器传递，对于gcc使用的是%rdi, %rsi, %rdx, %rcx, %r8, %r9这6个寄存器。当大于6个参数时，需要使用栈区内存进行传递。本例中的参数7和8，都被执行了压栈操作pushq。 pushq，可以认为是两步操作

1. 扩展栈区地址空间，也就是将%rsp寄存器内容增加8。
2. 将目的操作数，拷贝到这个新扩展的内存空间。

可以看到参数传递是在调用方完成的。

### 1.4 **函数调用**

函数调用产生的汇编代码如下:

call f(int, int, int, int, int, int, int, int)

call指令也相当于三步操作

1. 扩展栈区地址空间，也就是将%rsp寄存器内容增加8（x86_64架构）。
2. 将%rip的内容（一下条指令，函数返回后需要执行的指令），拷贝到这个地址空间。
3. jmp到f函数标号。

### 1.5 **被调函数的处理**

pushq %rbp

movq %rsp, %rbp

subq $56, %rsp

被调用函数生成的汇编，需要执行三步初始化操作：

1. 保存调用者（main函数）的栈帧基地址寄存器的地址。
2. 将当前的栈顶指针，作为新的栈帧的基地址。
3. 根据当前栈帧（f函数）内申请的局部变量的总大小，直接扩展栈的大小。这里之所以用sub, 是因为栈的地址空间是从高地址向低地址部分扩展的。

### 1.6 **切换总结**

以上部分，可以总结为下图。

![img](https://pic3.zhimg.com/80/v2-5c8f22be47bcf67f27e9440c6e761fa2_720w.webp)

需要说明下，虽然为了画图方便，%rsp指向了一个未使用的地址，**但实际上栈顶指针指向实际使用的地址: 比如参数传递部分，指向的是参数7的地址; 函数调用部分指向的是返回地址对应的地址。**

在被调函数完成相关初始化处理后, **我们称绿色部分为函数f对应的栈帧。也可以把 (%rbp %rip] 部分当做函数f的栈帧。**对栈帧的理解，可能会有些许差别，但在此处并不关键。

理解了上述函数调用的过程后,函数返回的过程可以简单的理解为以上操作的反向操作。

movl $0, %eax

leave

ret

也是三步:

1. 函数返回值保存在%rax中。
2. leave指令会把当前%rbp的值恢复到%rsp（恢复栈顶）；会将保存在f函数栈帧中的main函数的栈帧基地址弹出, 恢复到%rbp寄存器中
3. ret指令会将保存在f函数栈帧中的返回地址(main函数函数调用后的下一条指令地址)弹出, 恢复到%rip寄存器中

## **2.有栈协程的实现**

### 2.1**基于栈帧切换的协程**

如果我们理解了上述函数调用的实现细节, 如果我们允许函数f 在执行某些等待异步操作的时机, 将它的执行上下文, 主要是各寄存器(通用寄存器, %rbp, %rsp, %rip等)的值, 保存在某个协程上下文对象(内存)中。

在异步操作完成，需要切换回来时, 我们再将这些保存的值恢复到各寄存器, 那么函数f 就成为了一个协程。当然这个过程还涉及到栈帧这块内存本身的处理, 这点在后面的小节马上介绍。

```
int64_t f(int p1, int p2, int p3, int p4, int p5, int p6, int p7, int p8)

{

int x = 0;

int y = 0;

int64_t z = 0;

int64_t l = 0;

<等待异步操作-yield>

x++;

p1++;

p7++;

p8++;

return 0;

}
```

posix ucontex, boost fcontext和tencent libco都是这种类型的协程。 **对于有栈协程, 时刻要记住一点: 栈帧中使用的指针型变量, 如果不是指向该栈帧中的局部变量, 在协程恢复后其意义可能已经发生改变。**

### 2.2 **有栈协程定义**

有栈协程是指协程本身有自己独立的调用栈。基于栈帧切换的协程, 除了寄存器上下文,一般需要我们给协程指定其使用的栈空间。在协程切换后, 你会发现调用方的函数调用栈替换为了, 被调方协程的调用栈。 以libco为例, 协程的寄存器上下文保存在regs[ 14 ], 而协程使用的栈由ss_sp指定。

```
struct coctx_t

{

\#if defined(__i386__)

void *regs[ 8 ];

\#else

void *regs[ 14 ];

\#endif

size_t ss_size;

char *ss_sp;

};
```

### 2.3 **有栈协程切换**

我们以libco为例看下有栈协程的切换。 libco的协程入口函数遵循如下原型：

typedef void* (*coctx_pfn_t)( void* s, void* s2 );

在函数切换前，我们要完成对上下文的初始化，主要是完成如下三点：

1. 将RSP（此时还不是寄存器，而是保存该寄存器的内存）设置为之前指定的ss_sp对应的地址空间的最大值-8（可以想下为什么设置为栈空间的最大值，前面已经提过）。
2. 将返回地址设置为协程函数pfn的起始地址，这样协程上下文切换后，就可以从指定的函数执行。
3. 将函数的参数保存在RDI， RSI（此时还不是寄存器，而是保存该寄存器的内存）

```
int coctx_make(coctx_t* ctx, coctx_pfn_t pfn, const void* s, const void* s1) {

char* sp = ctx->ss_sp + ctx->ss_size - sizeof(void*);

sp = (char*)((unsigned long)sp &amp; -16LL);



memset(ctx->regs, 0, sizeof(ctx->regs));

void** ret_addr = (void**)(sp);

*ret_addr = (void*)pfn;



ctx->regs[kRSP] = sp;



ctx->regs[kRETAddr] = (char*)pfn;



ctx->regs[kRDI] = (char*)s;

ctx->regs[kRSI] = (char*)s1;

return 0;

}
```

协程的切换过程相较于函数切换的call指令，使用的是coctx_swap函数。这是汇编实现的一个函数，函数原型如下：

extern void coctx_swap(coctx_t*, coctx_t*) asm("coctx_swap");

想下上面的“参数传递”小节，coctx_swap作为一个函数， 在进入该函数前。函数第一个参数coctx_t*会设置到%rdi，这个结构体用于保存切换前的协程上下文。第二个参数会设置到%rsi，这个结构体就是切换后的协程上下文。如果第二个参数对应的上下文结构刚通过上面coctx_make初始化完成。那么通过下述恢复操作，

movq 48(%rsi), %rbp

movq 104(%rsi), %rsp

<other-code>

movq 56(%rsi), %rdi

<other-code>

leaq 8(%rsp), %rsp

pushq 72(%rsi)

movq 64(%rsi), %rsi

ret

关键的寄存器会被设置：

1. 设置新协程的栈帧基地址寄存器，此处是0。
2. 设置新协程的栈顶地址寄存器（前面已经介绍过，coctx_t结构中指定的ss_sp对应的地址空间的最大值-8）。
3. 通过"movq 56(%rsi), %rdi" 把coctx_make的第三个参数 void *s放置在参数传递寄存器%rdi。*
4. *通过"leaq 8(%rsp), %rsp" 将栈顶降低8（用于保存下一步的返回地址）*
5. *通过"pushq 72(%rsi)" 把coctx_swap返回后执行的指令设置为协程如果为函数pfn（coctx_make的第二个参数） 进行压栈。*
6. *通过"movq 64(%rsi), %rsi" 把coctx_make的第四个参数 void* s1放置在参数传递寄存器%rsi。
7. 通过ret指令将第5步的压栈的地址弹出到%rip，开始了新协程函数的执行。

### 2.4 **切换总结**

在执行完被调函数初始化后，会开始新的栈的执行，后续该协程栈上的函数调用和普通函数调用没有区别。 完整的流程如下图：

![img](https://pic3.zhimg.com/80/v2-313cd5a6e396b2162d0f6ec00c231cee_720w.webp)

再次说明下，虽然为了画图方便，%rsp指向了一个未使用的地址，**但实际上栈顶指针指向实际使用的地址: 比如第二步中，指向返回地址对应的栈空间。**

其他的有栈协程切换方式和libco类似，不一一赘述。

### 2.5 **私有栈 vs 共享栈**

libco在协程切换之上，还提供了私有栈和共享栈的封装。 私有栈是针对每个新开的协程都指定独立的栈空间，栈空间不能太大，有越界风险。 共享栈则是定义一个默认线程栈空间大小的栈，多个协程共享同一空间，使用者不用担心越界风险。但在协程切换时，涉及到栈空间的保存。

## **3.无栈协程**

### 3.1 **基于状态机的协程**

这种方法, 本人还没有仔细研究。简单说下和其他同事讨论的相关结论: 这种方式并不会执行寄存器级的上下文保存和恢复, 只是将函数执行到的行号记录在协程对象的成员变量中, 协程函数通过switch case和跳转宏, 在恢复执行时跳转到指定的行号继续执行。 这种实现方式就可以认为是一种无栈协程。无栈并不是没有stack。而是在现有的stack上创建协程栈帧。不会为协程指定独立的stack空间。 C++20的原生协程就是此种实现。这里可以提前透露下，相较于其他无栈协程，**C++20的原生协程创建的栈帧存在于堆上，我们可称之为堆帧，并不会随函数的挂起而销毁。**

## **4.对称协程 vs 非对称协程**

关于协程还有一种分类方法，对称，非对称。对称协程只提供了一种协程间的控制转移的语义即pass control, 而非对称提供了两种, invoke和suspend。 利用libco可以实现对称协程，也可以实现非对称协程。但我们一般倾向于实现非对称协程，实现如下程序架构。

![img](https://pic2.zhimg.com/80/v2-cc9695e939eee3d2ade8c8b6b2b127dd_720w.webp)

c++20的原生协程也是非对称式的。在协程挂起时会返回到它的调用方。但我们还是可以实现它的对称转移，其中原因下篇文章会讲到。 对称协程的控制转移示意图如下：

![img](https://pic4.zhimg.com/80/v2-7d3d29745247921928e5ac474e21f88f_720w.webp)

## **5.C++ 20协程**

这一章节我们会给出，C++20协程的定义，并列举协程需要的所有接口。这一章节会一下涌现很多术语和概念，可能你会感到有些困扰，但不用担心，后续章节会逐一解释各个接口的具体使用。

### **5.1 C++20协程总览**

我们先看下C++20协程的定义。C++20协程标准引入了3个新的关键字, co_await, co_yield, co_return。如果一个函数包含了如上3个关键字之一，则该函数就是一个协程。

除了这3个关键字，实现一个C++20协程还需要实现两个鸭子类型，分别是promise type和awaiter type。

举个例子：对于如下函数some_coroutine，由于在函数体内使用了co_await, 所以在C++20标准下，它就成为一个协程。

```
T some_coroutine(P param)

{

<declare x>

co_await x;

}
```

按照编译器的约定，该函数的返回值类型T，必须包含名为promise_type的子类型，且该子类型必须拥有约定的接口。

```
class T

{

public:

class promise_type

{

public:

<method of convention>

};



};
```

对于co_await操作数x，可能是如下类型：

1. 鸭子类型awaiter type。
2. 可以通过 T::promise_type::await_transform 接口转换为awaiter type的类型。
3. 第三种鸭子类型，awaitable type（和awaiter关系紧密，但有所区别）。

### 5.2 **接口清单**

关于上文中提到的三种鸭子类型，我们将相关接口约定列举如下，后续章节会介绍基础接口的使用。

- awaiter type需要实现如下名字的函数:

1. await_ready
2. await_suspend
3. await_resume



- awaitable type需要实现如下的操作符重载:

1. operator co_await()



- promise type需要实现如下名字的函数：

1. get_return_object
2. initial_suspend
3. final_suspend
4. unhandled_exception
5. return_void



- promise type可选实现如下名字的函数：

1. return_value
2. operater new
3. operater delete
4. get_return_object_on_allocation_failure
5. yield_value
6. await_transform



## **6.C++20协程实现原理**

### 6.1 **awaiter type**

我们先从co_await的语义实现说起。

co_await x;

假设x是我们之前说的awaiter type的变量。我们知道awaiter type有三个必须实现的接口，await_ready, await_suspend, await_resume。

那么co_await的执行过程相当于如下伪代码：(引自参考文献1)

```
{

if (!awaiter.await_ready())

{

using handle_t = std::experimental::coroutine_handle<P>;



using await_suspend_result_t =

decltype(awaiter.await_suspend(handle_t::from_promise(p)));



<suspend-coroutine>



if constexpr (std::is_void_v<await_suspend_result_t>)

{

awaiter.await_suspend(handle_t::from_promise(p));

<return-to-caller-or-resumer>

}

else

{

static_assert(

std::is_same_v<await_suspend_result_t, bool>,

"await_suspend() must return 'void' or 'bool'.");



if (awaiter.await_suspend(handle_t::from_promise(p)))

{

<return-to-caller-or-resumer>

}

}



<resume-point>

}



return awaiter.await_resume();

}
```

简单的讲，就是如下步骤

1. 首先调用await_ready判断是否需要执行挂起（异步操作是否已经完成）
2. 然后调用await_suspend

- 如果返回值是void版本的实现，则直接挂起。
- 如果返回值是bool版本的实现，则根据返回值决定是否挂起。
- 如果返回值是coroutine_handle<>版本的实现，挂起并返回到该返回值对应的协程。



1. 当协程唤醒后，会执行await_resume()。其返回值作为(co_await x)表达式的值。

coroutine_handle<>是新出现的一个类型。从名字我们就可以知道，它是协程的句柄。后续在介绍promise type类型时会继续介绍它。

总览部分也提到了co_await操作数x，除了awaiter type，还可能是如下其他类型：

所以对于非awaiter type的x变量，可能经历如下转换步骤(引自参考文献1)。

```
template<typename P, typename T>

decltype(auto) get_awaitable(P&amp; promise, T&amp;&amp; expr)

{

if constexpr (has_any_await_transform_member_v<P>)

return promise.await_transform(static_cast<T&amp;&amp;>(expr));

else

return static_cast<T&amp;&amp;>(expr);

}



template<typename Awaitable>

decltype(auto) get_awaiter(Awaitable&amp;&amp; awaitable)

{

if constexpr (has_member_operator_co_await_v<Awaitable>)

return static_cast<Awaitable&amp;&amp;>(awaitable).operator co_await();

else if constexpr (has_non_member_operator_co_await_v<Awaitable&amp;&amp;>)

return operator co_await(static_cast<Awaitable&amp;&amp;>(awaitable));

else

return static_cast<Awaitable&amp;&amp;>(awaitable);

}
```

简单的讲就是：

1. 调用T::promise_type::await_transform，将x作为参数传入，返回新的对象y（如果没有定义该函数，y就是x本身）。
2. 如果上一步得到的y对象的类重载了co_await()运算符，或者有全局的co_await()运算符，则调用该运算符，返回一个awaiter。

### 6.2 **promise type**

上一小节，我们已经介绍了promise type的其中一个接口await_transform。下面我们继续看下promise type其他接口，借此了解协程函数本身的实现细节。

针对上面的协程some_coroutine，以及它的返回值类型T，调用协程的语句可以理解为如下过程 (引自参考文献1)

```
// Pretend there's a compiler-generated structure called 'coroutine_frame'

// that holds all of the state needed for the coroutine. It's constructor

// takes a copy of parameters and default-constructs a promise object.

struct coroutine_frame { ... };



T some_coroutine(P param)

{

auto* f = new coroutine_frame(std::forward<P>(param));



auto returnObject = f->promise.get_return_object();



// Start execution of the coroutine body by resuming it.

// This call will return when the coroutine gets to the first

// suspend-point or when the coroutine runs to completion.

coroutine_handle<decltype(f->promise)>::from_promise(f->promise).resume();



// Then the return object is returned to the caller.

return returnObject;

}
```

如果你对之前文章中提到的函数切换，协程切换还有印象的话，作为一个被调用的函数，他需要保存其局部变量的栈帧空间。 对于C++20的原生协程，可以看到，编译器首先会为协程在堆上分配这块空间，我称之为**堆帧**。堆栈的大小可以认为是，T::promise_type的大小，协程局部变量以及参数的大小累计相加得到的。

在协程的**堆帧**上，会同时创建协程对应的T::promise_type的变量。 然后调用其get_return_object()接口。这个接口负责返回一个T类型的变量。 这里有一点我个人的理解：这里的伪代码只是演示方便，**执行过程并不会封装为一个函数，并不会启动新的栈帧，而是在原有栈帧上执行此逻辑**。所以协程函数返回的T类型的变量，只是一个临时变量。

这里我们再次看到coroutine_handle。在介绍了堆帧后，我们现在可以说，这个句柄维持了指向协程堆帧的指针。我们可以调用该句柄的resume函数恢复挂起状态协程的执行。

协程本身的执行遵循如下伪代码的流程(引自参考文献1)

```
{

co_await promise.initial_suspend();

try

{

<body-statements>

}

catch (...)

{

promise.unhandled_exception();

}

FinalSuspend:

co_await promise.final_suspend();

}
```



1. 首先会调用协程对应的promise变量的initial_suspend函数，该函数返回值应可以作为co_await的操作数（参见上一小节的内容）。这里主要是允许C++20协程的使用者，可以在协程执行前就挂起。
2. 然后开始执行我们编写的协程代码。 执行代码过程中，如果遇到了挂起，则会返回到调用者。
3. 最后，无论是否中间经历了挂起，在协程完全结束后，还会调用协程对应的promise变量的final_suspend函数，该函数返回值应可以作为co_await的操作数。这里主要是允许C++20协程的使用者，可以在退出前做适当的处理。
4. 这里还需要实现unhandled_exception()，用于处理协程本身未处理的异常。

除此外，promise type还有一个必须实现的接口，return_void() 或者 return value() 二选一。 在使用co_return时， 会调用你实现的函数，并跳转到FinalSuspend。

### 6.3 **co_yield**

至此，我们还剩一个关键字没有解释。在协程内调用co_yield

co_yield <expr>



相当于调用



co_await promise.yield_value(<expr>).

也就是说，对于要支持co_yield的协程，promise_type需要实现yield_value函数，同样的，该函数返回值应可以作为co_await的操作数。

## **7.一个简单的实现**

有了以上的理解，那么我们及可以实现一个简单的demo了。

```
std::coroutine_handle<> g_handle;

struct BaseSwapTestCoro

{

struct awaiter

{

bool await_ready() { return false; }

void await_suspend(std::coroutine_handle<> h) { g_handle = h; }

void await_resume() {}

};



struct promise_type

{

BaseSwapTestCoro get_return_object() { return {}; }



std::suspend_never initial_suspend() { return {}; }

std::suspend_never final_suspend() noexcept { return {}; }

void unhandled_exception() {}

void return_void() {}

};

};
```

需要说明的是std::suspend_never是预定义的变量，表明是nerver suspend的awaiter。

测试代码如下

```
BaseSwapTestCoro SomeFunc()

{

LOG(0, "in coroutine: before await");

co_await BaseSwapTestCoro::awaiter();

LOG(0, "in coroutine: after await");

}



TEST(base, swap)

{

SomeFunc();

LOG(0, "in main: before resume");

g_handle.resume();

LOG(0, "in main: after resume");

}
```

测试输出如下

```
[ RUN ] base.swap

base_test.cpp:29|in coroutine: before await

base_test.cpp:37|in main: before resume

base_test.cpp:31|in coroutine: after await

base_test.cpp:39|in main: after resume

[ OK ] base.swap (0 ms)
```

关于C++20协程实现的基本原理，先介绍到这么多。如果想进一步了解其他可选接口的使用，可以阅读参考资料1。这里需要说明一点，协程的语义并没有改变C++的基本语法规则，比如：

1. co_await BaseSwapTestCoro::awaiter(); 这里会创建awaiter的一个临时变量，那么这个临时变量在该语句执行完成后就会释放。
2. 协程退出后，栈帧就会销毁。g_handle就会指向一块已经释放的内存，再次resume就会的导致crash。 所以对于上面的例子，可以在await_resume, 将g_handle置空，以防野指针问题。

## **8.实验及落地设想**

### **8.1 基础性能测试**

在了解了C++20的实现原理后，我做了协程的基础创建和切换的试验

```
std::coroutine_handle<> g_handle;

struct BaseSwapTestCoro

{

struct awaiter

{

bool await_ready() { return false; }

void await_suspend(std::coroutine_handle<> h) { g_handle = h; }

int await_resume() { return 1; }

};



struct promise_type

{

BaseSwapTestCoro get_return_object() { return {}; }



awaiter initial_suspend() { return awaiter{}; }

std::suspend_never final_suspend() noexcept { return {}; }

void unhandled_exception() {}



void return_void() {}



auto await_transform() = delete; // no use co_wait

auto yield_value(int) { return awaiter{}; } // how to use void

};

};



TEST(base, swap)

{

base_swap::SomeFunc();

// test

int MAX_LOOP_COUNT = 1000000;

auto begin = CALC_CLOCK_NOW();

for (int i = 0; i < MAX_LOOP_COUNT; i++)

{

base_swap::g_handle.resume();

}

auto end = CALC_CLOCK_NOW(); // 340ns

LOG(0, "cost %lld ps", CALC_PS_AVG_CLOCK(end - begin, MAX_LOOP_COUNT) / 2);

// EXPECT_EQ(g_counter, MAX_LOOP_COUNT);

}
```

对比libco的方案，有如下数据

| 方案                   | 耗时（单位：皮秒=0.001纳秒） |
| ---------------------- | ---------------------------- |
| libco原生实现          | 17,000 ps                    |
| libco opt by walkerdu2 | 4,243 ps                     |
| c20上下文切换          | 1,660 ps                     |

此外， 还得到了c20协程创建开销 1,400 ps。

看到这个数据还是很令人振奋的。

### **8.2 落地设想**

考虑项目内的使用情况，我们往往会将某些协程函数进行封装，这样就会出现某个协程函数等待另一个协程函数的请求。

举个例子，某个RPC请求的响应函数，由于需要请求其他的服务，所以被实现为一个协程A。某些常用的其他服务请求被封装为协程B。A使用B完成部分功能。

假设协程调用过程如下

```
T B()

{

<co_await service b>

}



T A()

{

B();

<其他同步操作>

<co_await service c>

};
```

之前有提过，C++20协程是非对称的。如果这样实现的话， 在B函数挂起时， 会返回到A协程的下一条语句继续执行。 且B协程后续唤醒后，执行完成相关逻辑，并不会回到A。而是回到他的唤醒者。如下图所示

![img](https://pic4.zhimg.com/80/v2-4719610b739680af51e4b6b795c75107_720w.webp)

而我们想得到的效果是某种对称转移的语义（如果对协程的对称性不了解，可以参见前面的章节）。

![img](https://pic1.zhimg.com/80/v2-17714de9fedd0afa6c490eaf11a92900_720w.webp)

上面对称转移到语义就要求我们在协程A中可以 co_await B协程, 等待其执行完成。

T A()

{

co_await B();

<其他同步操作>

<co_await service c>

};

### 8.3 **实现对称转移语义**

参考Lewiss Baker的第四篇文章3，我试着实现了这种对称转移的语义。思路如下 ，针对 co_await B(); 这个语句执行如下步骤：

1. B协程启动后通过initial_suspend立即挂起，并返回对应的T类型对象，此T类型对象保存了B协程句柄。
2. 通过await_transform将T类型对象转换为一个awaiter type，并在其await_suspend函数，通过保存的B协程句柄，在其对应的promise对象中记录他的调用者A。
3. 在B协程被唤醒，执行完后，利用final_suspend，恢复A的执行。

代码地址 [https://git.woa.com/johnyao/c20_coroutine/blob/master/include/coro.h](https://link.zhihu.com/?target=https%3A//git.woa.com/johnyao/c20_coroutine/blob/master/include/coro.h)

## **9.实验对比**

在完成初版的封装后，我们对比了三个方案：项目内已经落地的基于libco的协程池方案，Asio Library内的协程方案，以及本人实验封装的版本。

创建协程测试用例（其中项目内的测试数据由组内的seanxchen提供）：

| 方案                       | 耗时（单位：纳秒） |
| -------------------------- | ------------------ |
| 项目内libco协程池 首次创建 | 6436 ns            |
| 项目内libco协程池 预创建   | 140 ns             |
| C++20实验封装              | 42~368 ns          |
| C++20 asio co_spawn        | 450 ns             |

可以看到，有栈协程由于初始化的时候需要额外申请栈区空间等操作，首次使用的时候效率是最低的。达到了微秒级，6.5微秒左右。但由于协程池的使用，在初始化后再次使用，效率是最高的，140ns。Asio Library中使用的co_spawn，则要450 ns。 这里需要说明的是，本人实验封装的版本不包含协程管理的开销，由于没有提供统一的spawn接口，所以协程运行时间和协程的嵌套层级有关系。

C++20的无栈协程，会随着协程调用层级的增加，增加执行耗时。而有栈协程不会有这部分的开销，一旦协程创建成功，则协程内的函数执行就是普通函数调用。所以我只对比了自己的实验封装和Asio Library针对不同嵌套层级的开销。

| 嵌套层级  | C++20实验封装 | C++20 asio封装 |
| --------- | ------------- | -------------- |
| 1层(同步) | 42 ns         | 34 ns          |
| 3层       | 221 ns        | 124 ns         |
| 5层       | 368 ns        | 241 ns         |

测试用例git地址： [https://git.woa.com/johnyao/c20_coroutine/tree/master/tests](https://link.zhihu.com/?target=https%3A//git.woa.com/johnyao/c20_coroutine/tree/master/tests)

以上结果只是初步结论，后续还会持续校验和跟进。

## 10.**结语**

虽然对比项目内现有协程实现，性能有所降低，但个人认为相较于有栈协程，C++20的协程还是有他的优势。

1. 基础性能确实优越。
2. 语言原生支持， 后续可能有高效的对称转移语义的标准库。
3. 堆帧空间可认为不受限制，不用担心爆栈。

作为初步的预研，C++20协程可以总结为，在语言层面实现了一种非对称的无栈协程。作为语言原生支持的协程，基础的效率表现很亮眼。在项目中实际落地，还需要进一步的探索。后续有空闲时间，会继续关注如下三点

1. 如何提高协程的对称转移的效率。
2. 如何提高协程管理的效率。
3. 针对特定框架定制更高效的协程封装。

最后也欢迎感兴趣的同学，一起讨论C++20协程实际落地过程中的最佳实践。

## 11.**参考**

1. [https://lewissbaker.github.io/2018/09/05/understanding-the-promise-type](https://link.zhihu.com/?target=https%3A//lewissbaker.github.io/2018/09/05/understanding-the-promise-type)
2. [https://github.com/walkerdu/libco](https://link.zhihu.com/?target=https%3A//github.com/walkerdu/libco)
3. [https://lewissbaker.github.io/2020/05/11/understanding_symmetric_transfer](https://link.zhihu.com/?target=https%3A//lewissbaker.github.io/2020/05/11/understanding_symmetric_transfer)
4. [https://km.woa.com/articles/show/355152?kmref=search&from_page=1&no=1](https://link.zhihu.com/?target=https%3A//km.woa.com/articles/show/355152%3Fkmref%3Dsearch%26from_page%3D1%26no%3D1)

**欢迎点赞分享，关注[@鹅厂架构师](https://www.zhihu.com/org/e-han-jia-gou-shi)，一起探索更多业界领先产品技术。**

原文作者：鹅厂架构师

原文链接：https://zhuanlan.zhihu.com/p/509698893

# 【NO.242】从零开始学架构（上篇）

以下内容来自于腾讯PCG工程师Chaoweili

# **1. 概念和基础**

架构设计：高性能、高可用、高扩展

# 1.架构基础

## 1.1 “架构”到底指什么

### 1.1.1 系统与子系统

**系统：**

![img](https://pic3.zhimg.com/80/v2-46e884fedc5a8907bbe9a9e36cd134b6_720w.webp)

**子系统：**

![img](https://pic4.zhimg.com/80/v2-832906590f5fc63001841a47c1cb654f_720w.webp)

### 1.1.2 模块与组件

**模块：**

![img](https://pic1.zhimg.com/80/v2-ec7a6b82d3db39cd92d29119c0bfa298_720w.webp)

**组件：**

![img](https://pic4.zhimg.com/80/v2-4e8b5a39999d2e04269839be48a2db93_720w.webp)

从逻 辑的角度来拆分后得到的单元就是“模块”，从物理的角度来拆分系统得到的单元就是“组件”; 划分模块的主要目的是职责分 离，划分组件 的主要目的是单元复用

### 1.1.3 框架与架构

**框架：**

![img](https://pic4.zhimg.com/80/v2-5a5e515e07982dbc48564a40e50a20a7_720w.webp)

**架构：**

![img](https://pic4.zhimg.com/80/v2-b43bb9c7d7f07792024a71676f3f955f_720w.webp)

框架关注的是“规范”，架构关注的是“结构”。框架： Framework，架构：Architecture

### 1.1.4 重新定义架构

软件架构：指软件系统的顶层结构!

## 1.2 架构设计的目的

架构设计的主要目的是：为了解决**复杂度带来的问题。**

也为了高性能、高可用、可扩展

## 1.3 复杂度来源

### 1.3.1 高性能

为了高性能，单台性能，扩张多多台集群性能，将业务分配功能模块，分解到各个子系统。带来了系统的机器复杂

单机复杂度

集群复杂度

任务分配

任务分解

### 1.3.2 高可用

高可用指“系统**无中断**地执行其功能”的能力，代表系统的可用性程度，是进行系统设计

时的准则之 一。一般都是通过“冗余”增加可用性，带来了复杂性。

1）计算高可用：双机/多机

2）存储高可用：备份数据，减少或规避数据不一致对业务造成的影 晌 。

高可用状态决策：无论计算高可用，还是存储高可用，其基础都是“状态决策”

几种 常见的决策方式：

1） 独裁式

只有一个决策者，如果决策者异常，整个系统就异常。

2） 协商世

两个独立的个体交流信息，然后根据规则进行决策，最常用的协商式决策就是主备决策

3） 民主式

民主式决策指的是多个独立的个体通过投票的方式来进行状态决策。例如， ZooKeeper集 群在选举 leader 时就是采用这种方式，

多个个体会出现“脑裂”，解决办法：投票节点数必须超过系统总节点数一半（过半原则）

### 1.3.3 可扩展性

**可扩展性：**无需或少量更改满足未来需求，不必整个重写或者重构

预测变化：不能全无扩展，也无需全都扩展

应对变化：抽象稳定层和变化层

### 1.3.4 低成本

大公司创造新的技术；小公司引入新技术。解决某个关键问题

### 1.3.5 安全

**功能安全：**

xss 攻击、 cs盯攻击、 SQL 注入、 Windows 漏洞、密码破解

代码实现上的漏洞、开源框架的漏洞

**架构安全：**

防火墙，依靠运营商或者云服务商强大的带宽和流量清洗能力

### 1.3.6 规模

功能越来越多、数据越来越多，引起系统复杂度上升

# 2.架构设计原则

## 2.1 合适原则

“合适”优于“业界领先”

系统架构设计要考虑人力物力选择合适的，不激进的造轮子，一步步的发展和优化

## 2.2 简单原则

“简单”优于“复杂”

结构复杂性，逻辑复杂性，都存在问题，如简单的和复杂的方案都满足需求，选择简单的

## 2.3 演化原则

“演化”优于“一步到位”

不贪大贪全，不全量照搬，分析主要问题，选择合理架构快速落地，然后在运行中不断优化完善演化架构。

## 2.4 本章小结

架构设计原则：合适、简单、演化

# 3.架构设计流程

## 3.1 有的放矢—识别复杂度

分析系统的复杂性，明确目标。架构复杂性主要源于：高性能、高可用、可扩展。一个系统往往只涉及一方面。从多个方案中选性价比最高的方案

## 3.2 按图索骥—设计备选方案

分析复杂度，有了目标后，开始方案设计。

高可用的主备方案、集群方案，高性能的负载均衡、多路复 用，可扩展的分层、插件化等技术，绝大部分时候我 们 有了明确的目标后，按图索骥就能够找 到可选的解决方案。在现有成熟技术上选择组合来满足需求，实在没有时才考虑创新。

方案设计：一个主要方案，多个备选方案。各个方案要差异明显，其中备选方案不一定基于现有技术，不一定要很详细。

## 3.3 深思熟虑—评估和选择备选方案

方案确认挑战：

![img](https://pic1.zhimg.com/80/v2-a55a2986218a5e8cbf608973695dc210_720w.webp)

方案质 量属性点有:：性能、可用性、硬件成本、项目投入、复杂度、安全性、可扩展性等。

遵循架构设计原则： “合适原则”和“简单原则”。 避免贪大求全，基本上某个质量属性能够满足一定时期内业务发展就可以了

例子：

1）业务背景

![img](https://pic4.zhimg.com/80/v2-ec7305e08f06191cf7d00c283de871eb_720w.webp)

2）备选方案设计

![img](https://pic4.zhimg.com/80/v2-17354f0a6a0c185edfb19ef1499866bf_720w.webp)

3）备选方案360度环评

![img](https://pic2.zhimg.com/80/v2-234e8eb9be4103112de9458f8aba4639_720w.webp)

## 3.4 精雕细琢—详细方案设计

方案评审确认后，进一步细化，例如ES索引划分、数据库分表、Ngnix负载均衡（轮询、加权轮询、ipHash、fair响应时间、urlHash）等

# 2.高性能架构模式

# 4.存储高性能

## 4.1 关系数据库

### 4.1.1 读写分离

读写分散到不同节点上

![img](https://pic2.zhimg.com/80/v2-8dd42beb40a5ee16de4ce3a4e0cea5bd_720w.webp)

读写分离的基本实现如下:

![img](https://pic3.zhimg.com/80/v2-cfee496816f4015aa69496d072f6c81a_720w.webp)

![img](https://pic3.zhimg.com/80/v2-a18fea5e7380e8820897b1aa2c59f392_720w.webp)

解决主从复制延迟有几种常见的方法：

![img](https://pic3.zhimg.com/80/v2-172b645d726a2ef9db5a68a551902916_720w.webp)

4.1.2 分库分表

常见的分散存储 的方法有“分库”和“分表”两大类

1） 分库

按“业务模块”将分库

问题：

![img](https://pic2.zhimg.com/80/v2-bb2799e3f8b792a1676ed6b9d5d5c2e1_720w.webp)

2） 分表

a. 垂直分表：

按字段进行分表，一些字段在A表，一些字段在B表

b. 水平分表：

表行特别大，按一定算法分表比如用户ID分段，订单IDhash等分表。

分表方法：

![img](https://pic3.zhimg.com/80/v2-ed8bf551f0a5279962307f17123c283a_720w.webp)

一些问题处理：

![img](https://pic3.zhimg.com/80/v2-0e5b88c5aecfdfc2c9ac1baf4b5dc4a2_720w.webp)

### 4.1.3 实现方法

可以实现一些通用中间件或者使用开源中间件，来实现分裤分表的各种操作。

![img](https://pic1.zhimg.com/80/v2-a7dc919641da9be10542150454a8af04_720w.webp)

## 4.2 NoSQL

常见的 NoSQL 方案有如下 4 类。

![img](https://pic2.zhimg.com/80/v2-a38961f4da5c236bfd9d831796ebca89_720w.webp)

### 4.2.1 K-V存储

Key-Value存储，代表：redis。

redis的事物只保证：隔离性（I）和一致性（C），无法保证原子性（A）和持久性（D）

![img](https://pic2.zhimg.com/80/v2-33279be7f83fb04140395848e0548815_720w.webp)

### 4.2.2 文档数据库

文档数据：no-schema，可以存储和读取任意的数据，大部分文档数据库存储的数据格式是JSON（或者BSON）。

优点：自描述，无需事先定义，新增字段无需改表结构，可以很容易存储复杂数据。特别适合电商和游戏这类的业务场景。以电商为例 ， 属性差异很大。例如，冰箱的属性和笔记本电脑的属性差异非常大。

缺点：不支持事务；无法join。例如MongoDB存储商品，可能库存被扣了，订单没有生成。

文档数据库，一般作为关系数据库的一种补充，比较适合电商和游戏场景，例如，冰箱的属性和笔记本电脑的属性差异非常大

例如：库存和订单用关系**型数据库；**商品详情用**文档数据库**存储。

![img](https://pic2.zhimg.com/80/v2-041d8d0d54dee35f30279a67844f2ab1_720w.webp)

![img](https://pic1.zhimg.com/80/v2-90e377d3a20779af8ec7a3d94472b7f0_720w.webp)

### 4.2.3 列式数据库

关系型数据库：按行进行存储的；

列式数据库：按列进行存储。

优点：减少IO（只读需要字段）、高压缩比（列上有更多重复内容，8:1 ~ 30:1）

缺点：由于高压缩比，更新会先解压，修改，再压缩写入

应用：应用在离线的大数据分析和统计场景中，因为这种场景主要是针对部分列进行操作，且数据写入后就无须再更新删除

### 4.2.4 全文搜索引擎

**正排索引**：基本原理是建立文档到单词的索引

**倒排索引（**Inverted index**反向索引）：**是一种索引方法，其基本原理是建立单词到文档的索引。

![img](https://pic2.zhimg.com/80/v2-508ccf3057619e58a19474d0c1408879_720w.webp)

**ElasticSearch：** 是分布式的文挡存储方式。 每个字段的所有数据都是默认被索引的 ，即每个字段都高为了快速检索设置的专用倒排索引。

## 4.3 缓存

为了提升存储系统性能，使用缓存，一次生成，多次读取，避免每次都直接访问存储

1） 需要复杂计算才能得出的数据，存储无能为力。

2） 读多写少：绝大部分都是读多写少，减少存储读压力

### 4.3.1 缓存穿透

1） **缓存穿透：**缓存没有数据，直接去查询存储

2） 穿透原因：zookeeper

a. 首次读取：读取后下一次就可以直接读缓存

b. 数据在存储中也不存在：穿透一次数据库，在缓存中记录是空Key

c. 生成缓存耗时大：读取时刚好还计算缓存内容。安全监控非法查询拦截，加到缓存时间

### 4.3.2 缓存雪崩

**缓存雪崩**：缓存失效，大量请求来，每个请求都触发一个数据读取和缓存数据计算过程，造成系统存储系统性能急剧下降，甚至宕机，造成整个系统瘫痪。

缓存雪崩有两种处理方法：

1） 更新锁机制

a. 同一时刻只有一个线程建立缓存

b. 对于分布式系统，每个节点都同时建立缓存，也会冲击缓存，可以用zookeeper建立分布式锁来实现缓存读数据库更新逻辑

2） 后台更新机制：缓存本身永久有效，后台定时更新或者消息通知触发更新

### 4.3.3 缓存热点

**缓存热点**：对于特别热点的数据，所有服务都请求同一份缓存数据，导致缓存也有压力。

解决办法：缓存多份，hash选取一个节点去读取。

# 5.计算高性能

磁盘、操作系统、 CPU、内存、缓存、网络、编 程语 言 、架构等都有可能影响系统性能。

一 行不恰当的 debug 日志，就可能将服 务器的性能从3万TPS降低到8。一个tcp_nodelay参数， 就可能将响应时间从2ms延 长到 40ms

高性能方法：1）单机性能； 2）集群

## 5.1 单服务器高性能

单机性能关键就是网络编程模型：1）处理链接管理；2）处理请求

两个设计点最终都和操作系统的 I/O 模型及进程模型 ：(1) IO模型:阻塞、非阻塞、同步、异步；(2)进程模型:单进程、 多进程、多线程。

### 5.1.1 PPC

**PPC(Process per Connection)**：有连接来fork子进程处理 。

![img](https://pic2.zhimg.com/80/v2-6d1807de4fc1026649ed901772537011_720w.webp)

![img](https://pic1.zhimg.com/80/v2-13abddbf95e0bbce1481a6d47d7ad8d4_720w.webp)

简单，适用量不大的情况：1）fork成本大 2）父子进程通信管理成本大 3）进程多了对系统也有压力

### 5.1.2 prefork

**prefork:** 预先fork子进程池子，来处理随时回来的请求，减少fork代价。

**惊群现象**：一个请求来了，只有一个子进程能 accept成功 ， 但所有阻塞在 accept上的子进程都会被唤醒，这样就导致 了不必要的进程调度和上下文切换。 （linux2.6已经解决惊群现象）

### 5.1.3 TPC

**TPC** **（Thread per Connection )**：每次有新的连接就新建一个线程去专门处理请求。 比进程轻量

![img](https://pic4.zhimg.com/80/v2-6445a91202aed4b23db6e82b53fc6527_720w.webp)

和 PPC 相比，主进程不用“ close”连接了。原因是在于子线程 是共享主进程的进程空间的，连接的文件描述符并没有被复制，因此只需要一次 close 即可 。

### 5.1.4 prethread

**prethread：**预先创建线程

### 5.1.5 Reactor

**IO多路复用：**一条连接上传 输多种数据。

![img](https://pic1.zhimg.com/80/v2-ec7c1054fb7dcf92bf7c5d412be74394_720w.webp)

**Reactor（反应堆）**：IO多路复用结合线程池。I/O 多路复用 统一监昕事件，收到事件后分配( Dispatch)给某个进程。包括 Reactor和处理资源池 (进程池或线程池)，其中 Reactor 负责监听和分配事件，处理资源池负责处理事件

(1)单 Reactor 单进程/单线程

![img](https://pic4.zhimg.com/80/v2-eddfd4b2e2ca0ec8172e7bde1c4cd21f_720w.webp)

流程：

![img](https://pic4.zhimg.com/80/v2-8bf3269b7857b3f1df540aab0341c257_720w.webp)

优点：

![img](https://pic3.zhimg.com/80/v2-b303cedd97eb588bc16c56843d117526_720w.webp)

缺点：

![img](https://pic1.zhimg.com/80/v2-56da44b3977ecee6c3b49c6b46e7aec4_720w.webp)

(2)单 Reactor 多线程

![img](https://pic3.zhimg.com/80/v2-fd24c405e81c28f7d1efb5c3de8acc16_720w.webp)

**流程：**

![img](https://pic3.zhimg.com/80/v2-728350ffac79e0875f4786f74c0ffe3e_720w.webp)

优点：充分利用多核CPU

缺点：

![img](https://pic2.zhimg.com/80/v2-9db953924b6620776f6839bda41f1639_720w.webp)

(3)多 Reactor多进程/线程

![img](https://pic3.zhimg.com/80/v2-136de7afd223db47d42c54a7bc810a0a_720w.webp)

流程：

![img](https://pic2.zhimg.com/80/v2-9d041b9e2e2609981ce1cadfd0c09b0d_720w.webp)

优点：

![img](https://pic1.zhimg.com/80/v2-e9f592faca55b63ce462caa1946d5eac_720w.webp)

案例：

![img](https://pic1.zhimg.com/80/v2-b910ff6863959a788b01387dab50cce0_720w.webp)

### 5.1.6 Proactor

Reactor 是非阻塞同步网络模型（ read 和 send 操作都需要用户进程同步操作）

异步网络模型 Proactor：把 I/O 操作 改为异步。

![img](https://pic4.zhimg.com/80/v2-3687b4769f383587a7e7f625231772eb_720w.webp)

![img](https://pic1.zhimg.com/80/v2-25923e3bba18a033b751b202346b1004_720w.webp)

![img](https://pic2.zhimg.com/80/v2-623beace875ae16d04300795d2555c65_720w.webp)

异步IO实现：

![img](https://pic4.zhimg.com/80/v2-02a57645eac1a8028e3327b9b3a0ba4f_720w.webp)

## 5.2 集群高性能

### 5.2.1 负载均衡分类

常见的负载均衡系统： 1）DNS 负载均衡、2）硬件负载均衡、3）软件负载均衡。

1） DNS 负载均衡

域名解析按地区给不同的ip

![img](https://pic1.zhimg.com/80/v2-bd323065d4ae2bbcb9a9dc6dbdbf4888_720w.webp)

**优点：**1）简单，低成本；2）就近访问，可以提升访问速度

**缺点：**

1） 更新不及时，由于缓存更新；

2） 扩展差；

3） 分配策略简单：只能简单按地区分配ip，不能判断服务状态负载情况来分配

**HTTP-DNS**：用 HTTP 协议实现一个私有的 DNS 系统

2） 硬件负载均衡

**硬件负载均衡：**通过单独的硬件设备来实现负载均衡功能，这类设备和路由器交换机类似， 可以理解为一个用于负载均衡的基础网络设备。

两款：F5和A10

功能、性能、稳定性都很好，还有安全防护，就是很贵

3） 软件负载均衡

Nginx 和 LVS等利用软件实现负载均衡。

![img](https://pic3.zhimg.com/80/v2-fca072a2870850894881e231276800a6_720w.webp)

### 5.2.2 负载均衡架构

可以三种结合：域名解析，然后地区内，再到机房内可以不同层级的负载均衡

### 5.2.3 负载均衡的算法

1） 轮询：只简单轮询

2） 加权轮询：根据性能按比例分配

3） 负载最低优先：按qps，cpu等负载进行分配

4） 性能最优优先：性能强劲的机器优先分配

5） Hash：按ip，用户ID等hash分配

# 3.高可用架构模式

# 6. CAP

## 6.1 CAP理论

第一版：

![img](https://pic4.zhimg.com/80/v2-71748f66c5f5621eb8b594c28beb1ea3_720w.webp)

第二版：

![img](https://pic1.zhimg.com/80/v2-53cd5fca039ea56e85dcc5719adaf6ac_720w.webp)

### 6.1.1 一致性（Consistency）

第一版：

![img](https://pic2.zhimg.com/80/v2-20e544f1d9e51a3acb6f0d3a38a19829_720w.webp)

第二版：

![img](https://pic3.zhimg.com/80/v2-78e242afab63356b1408339f07fd7842_720w.webp)

第一版：从节点看数据一致；第二版：从客户端读的角度看数据一致。第二版更佳符合时间应用

### 6.1.2 可用性

第一版：

![img](https://pic4.zhimg.com/80/v2-8f029872b28fc14dae9b63c78842461b_720w.webp)

第二版：

![img](https://pic4.zhimg.com/80/v2-ced183711549ba38c270e29afcf7b60f_720w.webp)

第二版更加合理，强调非故障节点，没有错误和超时情况下返回合理的响应

### 6.1.3 分区容忍性（Partition Tolerance）

第一版：

![img](https://pic4.zhimg.com/80/v2-3c34cdd36fb1f5fef78c58c0135e419b_720w.webp)

第二版：

![img](https://pic1.zhimg.com/80/v2-ca870cff1ad37f6c38abf7048aaf3a3c_720w.webp)

## 6.2 CAP应用

在分布式系统中，P(分区容忍)必须选择，网络本身无法做到100%可靠。

### 6.2.1 CP—Consistency/Partition Tolerance

CP：一致性+分区容忍性

如果要求一致性，节点之间相互访问失败，则返回错误，就不能保证可用性

### 6.2.2 AP—Availability/Partition Tolerance

AP：可用性+分区容忍性

为了保证可用性，某节点读数据失败，则返回自己缓存的数据，这样就不能保证一致性。

## 6.3 CAP细节

\1) CAP关注的粒度是数据， 而不是整个系统

\2) CAP 是忽略网络延迟的

\3) 正常运行情况下，不存在CP和AP的选择， 可以同时满足CA

\4) 放弃并不等于什么都不做，需要为分区恢复后做准备

### 6.4 ACID、BASE

ACID： 是数据库事务完整性的理论

CAP： 是分布式系统设计理论，

BASE：是 CAP 理论中 AP 方案的延伸

### 6.4.1 ACID

ACID：数据库保证事务

**1） Atomicity (原子性)**

一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节 。事

务在执行过程中发生错误，会被回滚到事务开始前 的状态， 就像这个事务从来没有 执行过 一样。

**2） Consistency (一致性)**

在事务开始之前和事务结束以后，数据库的完整性没有被破坏 。

**3） Isolation （隔离性)**

数据库允许多个并发事务同时对数据进行读写和修改的能力。隔离性可以防止多个事务并

发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级 别，包括读未提 交( Read uncommitted)、读提交( read committed)、可重复读( repeatable read)和串行化(Serializable)。

**4） Durability (持久性)**

事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

### 6.4.2 BASE

**BASE** 是 Basically Available (基本可用)、 Soft State (软状态)和 Eventually Consistency (最终一致性〉三个短语的缩写 ， 其核心思想是即使无法做到强一致性( CAP的一致性就是强一致性)，但可以采用适合的方式达到最终一致性 (Eventual Consistency)。

**1） 基本可用(Basically Available)**

分布式系统在出现故障时，允许损失部分可用性，即保证核心可用 。

**2） 软状态(Soft State)**

允许系统存在中间状态，而该中间状态不会影响系统整体可用性 。这里的中间状态就是 CAP 理论中的数据不 一致

**3） 最终一致性(Eventual Consistency)**

系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。

# 7.FMEA

**FMEA (Failure mode and effects analysis，故障模式与影响分析)**又称为失效模式与后果分 析、失效模式与效应分析、故障模式与后果分析等。FMEA 通过对系统范围内潜在的故障模式加以分析，并按照严重程度进行分类，以确定失效对于系统的最终影响。

## 7.1 FMEA介绍

FMEA分析方法：

![img](https://pic3.zhimg.com/80/v2-842c47a30b1a30c59d964f27dbd285c6_720w.webp)

## 7.2 FMEA方法

**1） 功能点**

登录/注册等

**2） 故障模式**

故障点和故障形式：比如mysql读取耗时特别大，具体是磁盘、内存等

**3） 故障影响**

比如：注册登录变慢2s等

**4） 严重程度**

业务角度影响程度，例如：

![img](https://pic2.zhimg.com/80/v2-077a9127bae0f88a7922be96dc7af449_720w.webp)

**5） 故障原因**

某个子模块出问题的具体原因：比如低版本bug等

**6） 故障概率**

硬件：超过N年后故障率升高；开源软件：维护力度不同bug率不同；自研系统：新老系统故障率不同

**7） 风险程度**

严重程度*故障率

**8） 已有措施**

检查告警：自动告警

容错：出问题切换副本等

自恢复：系统自动恢复

**9） 规避措施**

数据备份

网络双入口；双供电

为了减少异常，合适的时间自动重启

**10） 解决措施**

防止无线重试：限制重试次数

防止拖库：密码加密存储

访问权限：白名单控制敏感权限

**11） 后续规划**

对于不足的处理：备份，双机房，供电保证防止掉电数据丢失等

## 7.3 FMEA实战

![img](https://pic3.zhimg.com/80/v2-401e677d0db8089e77034ef824d28386_720w.webp)

# 8.存储高可用

高可用存储架构：1）主备、2）主从、3）主主、4）集群、5）分区

## 8.1 主备复制

![img](https://pic3.zhimg.com/80/v2-4970b4f91429a4585c635fe4bd9a904a_720w.webp)

主备：读写都从主节点，故障后人工切换到备用节点。简单，但是需要人工切换。对于频率不高的系统比较合适（学生管理/图书管理，等并发小的系统）

## 8.2 主从复制

![img](https://pic2.zhimg.com/80/v2-c91a78920a041d74b24801ed63381551_720w.webp)

主从复制：

主节点：可写可读

从节点：只读。

主节点故障时，从节点继续可读，如果主节点不可恢复，人工进行主从切换。

适合读多写少的系统，比如：新闻，BBS博客类等

## 8.3 主备倒换与主从倒换

检查主节点、切换策略（断开多久？），主从切换（自动切换，主从倒换）

互连：主备节点两者相互通信

中介：主备节点都上报状态到中介服务，检查状态，判断主故障

模拟：从节点将写操作透传给主节点

## 8.4 主主复制

主主复制：两个节点都可读写，相互复制数据。

## 8.5 数据集群

集群：一批机器处理。分为：数据集中集群、数据分散集群

### 8.5.1 数据集中集群

所有数据都在一个集群，由主节点进行写入，复制给从节点；主节点失效后重新选举新的主节点

### 8.5.2 数据分散集群

数据按分类分布到不同机器，比如账号，联系人等

### 8.5.3 分布式事务算法

**1） 2PC（Two-phase commit protocol）**

2阶段提交：2PC：1）commit 请求阶段；2）commit 执行阶段。

2PC基于假设：

![img](https://pic4.zhimg.com/80/v2-cb00ad629b7438c78c9236c77a1190c3_720w.webp)

**第一阶段：提交请求阶段（投票阶段）**

![img](https://pic1.zhimg.com/80/v2-a37dd8d1c4cb088edbf2f0b9a146b010_720w.webp)

![img](https://pic4.zhimg.com/80/v2-27c099b043dcbeba8517b3786ba41aef_720w.webp)

简单来说：协调者发起请求；参与者执行，执行成功返回Yes，执行失败返回No

**第二阶段：提交执行阶段（完成阶段）**

![img](https://pic2.zhimg.com/80/v2-85a79fb39695100a4ae4b41e9a686d71_720w.webp)

【成功】

当协调者从所有参与者获得的相应消息都为“ Yes”时 :

![img](https://pic1.zhimg.com/80/v2-c8440e1b844b3a089c7ddc79eef22fe8_720w.webp)

投票成功，开始提交执行，执行完成ACK确认，最后完成事务

【失败】

第一阶段参与者在第一阶段返回的响应消息为“No气或者协调者在第一阶段的询问超时之

前无法获取所有参与者的响应消息时 :

![img](https://pic4.zhimg.com/80/v2-f21b04eaeef3e20ebd125deb0ce58e87_720w.webp)

投票有一个失败，开始提交回滚，回滚完成ACK确认，取消事务

如果参与者没有收到执行请求，超时后也会自动回滚

2PC实现简单，缺点：

![img](https://pic2.zhimg.com/80/v2-630dbc3cad454f9185850bc9275c0085_720w.webp)

**2） 3PC（Three-phase commit protocol）**

3PC主要解决2PC单点问题：第一和第二阶段之间插入一个”准备阶段“

![img](https://pic2.zhimg.com/80/v2-ab90381b322c4c4ca8ec3e5770dca629_720w.webp)

**第一阶段(提交判断阶段):**

![img](https://pic4.zhimg.com/80/v2-961876c0a3bb4473707a15dfe43d517b_720w.webp)

**第二阶段(准备提交阶段):**

![img](https://pic1.zhimg.com/80/v2-6181489a807370307762a5e449675cc0_720w.webp)

**第三阶段(提交执行阶段):**

![img](https://pic1.zhimg.com/80/v2-67a7f3a3c874be8fb6d2ed6a51cc0494_720w.webp)

### 8.5.4 分布式一致性算法

**分布式事务算法：**为了保证分散在多个节点上的数据统一提交或回滚，以满足ACID的要求;

**分布式一致性算法：**为了保证同一份数据在多个节点上的一致性；

**1） Paxos**

![img](https://pic2.zhimg.com/80/v2-1fc76182d9e35e555ff1c1130c4f5c01_720w.webp)

**2） Raft**

![img](https://pic2.zhimg.com/80/v2-33338f098423e85ead54275e1679e50d_720w.webp)

**3） ZAB**

![img](https://pic1.zhimg.com/80/v2-3d5ff7111681bb1618814d0fd1fa7900_720w.webp)

## 8.6 数据分区

将数据按一定规则分区，存储在不同地区，一部分故障，不影响其他地区。

### 8.6.1 数据量

数据量的大小直接决定了分区的规则复杂度。例如，使用 MySQL 来存储数据，假设一台 MySQL 存储能力是500GB，那么 2TB 的数据就至少需要 4 台 MySQL 服务器;而如果数据是 200TB，并不是增加到 800 台的 MySQL 服务器那么简单。

### 8.6.2 分区规则

洲际分区、国家分区，城市分区

### 8.6.3 复制规则

集中式：所有节点从一个节点集中复制

![img](https://pic3.zhimg.com/80/v2-eea46494a62ff12a2e12ea307f4e924e_720w.webp)

互备式：两两互相备份

![img](https://pic1.zhimg.com/80/v2-0582e3e9346b8ebc8d50a84c16ce7350_720w.webp)

独立式：就近独立备份

![img](https://pic3.zhimg.com/80/v2-7ea6270ee527d960fa6632bdd6d2e2a6_720w.webp)

# 9.计算高可用

计算高可用：部分硬件损坏，计算任务依旧能执行

## 9.1 主备

主机：读写

从机：当主机故障时，将任务分配给从机

冷备：备机上程序配置都在，只是不启动服务，切换时需要先启动再切换流量过来

温备份：程序服务就绪状态，只是空跑，切换备份时直接切流量来即可。

## 9.2 主从

将任务按特征分配给主从机器，比如：主可读写，从只读。

## 9.3 对称集群

每个节点都对等可以执行每个任务，利用负载均衡来分配任务。

## 9.4 非对称集群

不同节点执行不同任务：比如登陆服务、下单服务，够买VIP服务。

原文作者：鹅厂架构师

原文链接：https://zhuanlan.zhihu.com/p/540651638

# 【NO.243】c++异步从理论到实践 -总览篇

**以下内容来自于腾讯研发工程师fangshen**

## 1. 纠结的开篇

之前设计我们游戏用的c++框架的时候, 刚好c++20的coroutine已经发布, 又因为是专门 给game server用的c++ framework, 对多线程的诉求相对有限, 或者本着少并发少奇怪的错误的原则, 除网络和IO和日志等少量模块外, 大部分模块主要还是工作在主线程上的, 所以当时设计的重点也就放在了c++20 coroutine的包装和使用上, 更多的使用coroutine来完善异步的支持. 但如果考虑到framework作为前后端公用框架的话, 原来主要针对主线程使用的包装的coroutine调度器就显得有些不够用, 以此作为基础, 我们开始了尝试结合比较新的c++异步思路, 来重新思考应该如何实现一个尽量利用c++新特性, 业务层简单易用的异步框架了. 本系列的主要内容也是围绕这条主线来铺开, 过程中我们 主要以: 1. **自有的framework异步实现** - 主要落地尝试利用c++20的coroutine实现一个业务级的调度器. 2. **asio** - 这个应该不用多说了, 近年来一直高频迭代, 业界广泛使用的开源第三方库, 中间的异步任务调度, 网络部分的代码实现都非常优质. 3. **libunifex** - 最接近当前sender/receiver版 execution提案的可实操版本, c++17/20兼容, 但不推荐使用c++17的版本进行任何尝试, 原因后续文件会展开.
这几个库作为基础, 逐步展开我们对c++异步的探索, 然后再回到落地实践这条主线上, 探讨一个业务侧使用简单, 内部高效的异步库应该如何来实现并落地. 当然, 我们的侧重点主要还是c++异步的调度和处理上, 网络相关的有部分内容可能会简单提到, 但不会进行深入的展开. 其实整个尝试的过程只能说非常不顺利了, 当然, 随着对相关实现的深入理解和细节的深挖, 收益也是颇多的. 闲话不多说了, 我们直接切入主题, 以对异步的思考来展开这篇总览的内容.

## 2. 前尘往事 - rstudio framework实现

rstudio framework的异步框架由两块比较独立的部分组成: 1. 一部分是源自asio几年前版本的post和strand部分实现, 另外附加了一些业务侧较常用的像Fence等对象; 2. 另外一部分是主线程的协程调度器实现, 这部分最早是基于c++17实现的一版stackless 协程; 另外一版则是gcc11.1正式发布后, 直接用c++20重构了整个实现, 直接使用c++20的coroutine的一个版本.

### 2.1 asio 部分

这一部分的内容因为后续有asio scheduler实现具体的分析篇章, 这个地方主要以业务侧使用进行展开了.

#### 2.1.1 executor概述

- 来源于1.6X boost同期的asio standalone版本
- 去除了各平台网络处理相关的代码
- 仅保留了post和相关的功能(新版本有executor实现)
- 早期c++11兼容, 无coroutine支持
- 除网络库外, asio非常有使用价值的一部分代码

#### 2.1.2 一个简单的使用示例

```text
  GJobSystem->Post([]() {
        //some calculate task here
        //...
        GJobSystem->Post(
            []() {
                //task notify code here
                //...
            },
            rstudio::JobSystemType::kLogicJob);
      }, rstudio::JobSystemType::kWorkJob);
```

**相关的时序图:**

```text
sequenceDiagram
    Logic Job ->>+Work Job: calculate task
    Work Job ->>-Logic Job: calculate result
```

#### 2.1.3 当前框架使用的线程结构



![img](https://pic2.zhimg.com/80/v2-d7fd1433d68f436b7a2dab777226aad5_720w.webp)



**预定义的枚举值:**

```text
enum class JobSystemType : int {
  kLogicJob = 0,       // logic thread(main thread)
  kWorkJob,            // work thread
  kSlowJob,            // slow work thread(run io or other slow job)
  kNetworkJob,         // add a separate thread for network
  kNetworkConnectJob,  // extra connect thread for network
  kLogJob,             // log thread
  kNotifyExternalJob,  // use external process to report something, 1 thread only~~
  kTotalJobTypes,
};
```

**不同Job说明:** - **kLogicJob** - 主线程(逻辑线程)执行任务 - **kWorkJob** - Work Thread线程池执行任务(多个), 一般是计算量可控的小任务 - **kSlowJob** - IO专用线程池, IO相关的任务投递到本线程池

------

- **kNetworkJob**

- - 目前tbuspp专用的处理线程



- **kNetworkConnectJob**

- - 专用的网络连接线程, tbuspp模式下不需要

- **kLogJob**

- - 日志专用线程, 目前日志模块是自己起的线程, 可以归并到此处管理



- **kNotifyExternalJob**

- - 专用的通知线程, 如lua error的上报, 使用该类型

------

#### 2.1.4 Timer任务相关

**相关接口:**

```text
//NoIgnore version
uint64_t JobSystemModule::AddAlwaysRunJob(JobSystemType jobType,
                        threads::ThreadJobFunction&& periodJob, 
                        unsigned long periodTimeMs);

uint64_t JobSystemModule::AddTimesRunJob(JobSystemType jobType, 
                        threads::ThreadJobFunction&& periodJob, 
                        unsigned long periodTimeMs, 
                        unsigned int runCount);

uint64_t JobSystemModule::AddDelayRunJob(JobSystemType jobType,     
                        threads::ThreadJobFunction&& periodJob,
                        unsigned long delayTimeMs);

void JobSystemModule::KillTimerJob(uint64_t tid);
```

> 本部分并未直接使用asio原始的basic_waitable_timer实现, 而是自己实现的定时任务.

#### 2.1.5 在线程池上关联执行任务 - Strand

- 特定的情况下, 被派发到Work线程池的任务存在依赖关系
- 需要串联执行的时候, 这个时候我们需要额外的设施 JobStrand
- 来保证任务是按先后依赖关系来串行执行的
- 如下图中part1, part2, part3, part4串行执行的情况所示

------

```text
sequenceDiagram
    participant L as Logic Job
    participant W1 as Work Job1
    participant W2 as Work Job2
    participant W3 as Work Job3
    L ->>W1: part 1
    activate W1
    W1 ->>W2: part 2
    deactivate W1
    activate W2
    W2 ->>W3: part 3
    deactivate W2
    activate W3
    W3 ->>W2: part 4
    deactivate W3
    activate W2
    W2 ->>L: return
    deactivate W2
```

**示例代码:**

```text
auto strand = GJobSystem->RequestStrand(rstudio::JobSystemType::kWorkJob);
starnd.Post([](){ 
    //part1~
    // ...
});
starnd.Post([](){ 
    //part2~
    // ...
});
starnd.Post([](){ 
    //part3~ 
    // ...
});
starnd.Post([](){ 
    //part4~ 
    // ...
});
starnd.Post([](){ 
    GJobSystem->Post([](){
        //return code here
        // ...
    }, rstudio::JobSystemType::kLogicJob); 
});
```

#### 2.1.6 其他辅助设施

**JobFence**

```text
jobs::JobFencePtr JobSystemModule::RequestFence();
```

- 字面义, 栅栏, 起到拦截执行的作用.
- 一般多用于模块的初始化和结束
- 如tbuspp在kNetworkJob上的初始化和结束.

```text
sequenceDiagram
    participant L as Logic Job
    participant N as Network Job
    N ->> N: some jobs run here
    L ->> N: fence notify
    activate N
    N ->> L: fence finish
    deactivate N
```

**示例代码(TcpService的初始化)**:

```text
job_system_module_->Post(
    [this, workTicket]() {
        if (!workTicket || workTicket->IsExpired()) return;

        InitInNetworkThread();
    },
    JobSystemType::kNetworkJob);

period_task_ptr = job_system_module_->AddAlwaysRunJob(
    JobSystemType::kNetworkJob,
    [this, workTicket]() {
        if (!workTicket || workTicket->IsExpired()) return;

        LoopInNetworkThread();
    },
    10);

fence_->FenceTo((int)JobSystemType::kNetworkJob);
fence_->Wait();
```

**JobNotify && JobWaiter**

```text
jobs::JobWaiterPtr JobSystemModule::RequestWaiter();
jobs::JobNotifyPtr JobSystemModule::RequestNotify();
```

- 批量任务管理使用

- 等待的方式的区别

- - **JobNotify**: 执行完成调用额外指定的回调.
  - **JobWaiter**: 以Wait的方式在特定线程等待所有Job执行完成.

**JobTicket**

```text
jobs::JobTicketPtr JobSystemModule::RequestTicket();
```

- 令牌对象
- 一般用来处理跨线程的生命周期控制
- 回调之前先通过IsExpired()来判断对应对象是否已经释放

示例代码:

```text
GJobSystem->Post(
  [this, workTicket]() {
    if (!workTicket || workTicket->IsExpired()) return;

    InitInNetworkThread();
  },
  JobSystemType::kNetworkJob);
```

### 2.2 asio 与其他实现的对比

正好今年的GDC上有一个\<\<One Frame In Halo Infinite>>的分享, 里面主要讲述的是对Halo Infinite的引擎升级, 提供新的JobSystem和新的动态帧的机制来支撑项目的, 我们直接以它为例子来对比一下framework和Halo的实现, 并且也借用Halo Infinite的例子, 来更好的了解这种lambda post模式的缺陷, 以及可以改进的点. Halo引入新的JobSystem主要是为了将老的Tetris结构的并发模式:

![img](https://pic4.zhimg.com/80/v2-eed32a602c5db8f7ba3084e40ed3030b_720w.webp)

向新的基于Dependency的图状结构迁移:

![img](https://pic3.zhimg.com/80/v2-6fe599f2c77ecd105fa4e87208f5b9aa_720w.webp)

他使用的JobSystem的业务Api其实很简单, 我们直接来看一下相关的代码:

```text
JobSystem& jobSsytem = JobSystem::Get();
JobGraphHandle graphHandle = jobSystem.CreateJobGraph();

JobHandle jobA = jobSystem.AddJob( 
    graphHandle, 
    "JobA",
    [](){...} );

JobHandle jobB = jobSystem.AddJob(
    graphHandle,
    "JobB",
    [](){...} );

jobSystem.AddJobToJobDependency(jobA, jobB);

jobSystem.SubmitJobGraph(graphHandle);
```

通过这样的机制, 就很容易形成如:

![img](https://pic4.zhimg.com/80/v2-650e3b68551520bab3188baf0072fe77_720w.webp)



另外还有一个用于同步的SyncPoint:

```text
JobSystem& jobSystem = JobSystem::Get();
JobGraphHandle graphHandle = jobSystem.CreateJobGraph();

SyncPointHandle syncPointX = jobSystem.CreateSyncPoint(graphHandle, "SyncPointX");

JobHandle jobA = jobSystem.AddJob(graphHandle, "JobA", [](){...});
JobHandle jobB = jobSystem.AddJob(graphHandle, "JobB", [](){...});

jobSystem.AddJobToSyncPointDependency(jobA, syncPointX);
jobSystem.AddSyncPointToJobDependency(syncPointX, jobB);

jobSystem.SubmitJobGraph(graphHandle);
```

大致的作用如下:

![img](https://pic2.zhimg.com/80/v2-5da70698108df4a3420b32c3a917d479_720w.webp)

这样在workload主动触发SyncPoint后, 整体执行才会继续往下推进, 这样就能方便的加入一些主动的同步点对整个Graph的执行做相关的控制了.

回到asio, 我们前面也介绍了, 使用strand和post(), 我们也能很方便的构造出Graph形的执行情况 , 而SyncPoint其实类型framework中提供的Event, 表达上会略有差异, 但很容易看出两套实现其实是相当类同的. 这样的话, Halo 的JobSystem有的所有优缺点, framework基本也同样存在了, 这里简单搬运一下:

![img](https://pic2.zhimg.com/80/v2-b7a4963433dfda4751e3495c28755a11_720w.webp)

对于复杂并发业务的表达以lambda内嵌为主, 虽然这种方式尽可能保证所有代码上下文是比较集中的, 对比纯粹使用callback的模式有所进步, 但这种自由度过高的方式本身也会存在一些问题, 纯粹靠编码者来维系并发上下文的正确性, 这种情况下状态值在lambda之间的传递也需要特别的小心, 容易出错, 并且难以调试.

### 2.3 coroutine实现部分

coroutine部分之前的帖子里已经写得比较详细了, 这里仅给出链接以及简单的代码示例: 1. [如何在C++17中实现stackless coroutine以及相关的任务调度器](https://zhuanlan.zhihu.com/p/411834453) 2. [C++20 Coroutine实例教学](https://zhuanlan.zhihu.com/p/414506528) 2. 另外还有一个purecpp大会的演讲视频, 主要内容与上述的两篇文章相关度比较高, 这里也给出相关的链接, 感兴趣的同学可以自行观看: [C++20 coroutine原理与应用](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1fZ4y197AL%3Fspm_id_from%3D333.999.0.0)

**代码示例:**

```text
//C++ 20 coroutine
auto clientProxy = mRpcClient->CreateServiceProxy("mmo.HeartBeat");
mScheduler.CreateTask20([clientProxy]() 
                        -> rstudio::logic::CoResumingTaskCpp20 {
    auto* task = rco_self_task();

    printf("step1: task is %llu\n", task->GetId());
    co_await rstudio::logic::cotasks::NextFrame{};

    printf("step2 after yield!\n");
    int c = 0;
    while (c < 5) {
        printf("in while loop c=%d\n", c);
        co_await rstudio::logic::cotasks::Sleep(1000);
        c++;
    }
    for (c = 0; c < 5; c++) {
        printf("in for loop c=%d\n", c);
        co_await rstudio::logic::cotasks::NextFrame{};
    }

    printf("step3 %d\n", c);
    auto newTaskId = co_await rstudio::logic::cotasks::CreateTask(false, 
                                        []()-> logic::CoResumingTaskCpp20 {
        printf("from child coroutine!\n");
        co_await rstudio::logic::cotasks::Sleep(2000);
        printf("after child coroutine sleep\n");
    });
    printf("new task create in coroutine: %llu\n", newTaskId);
    printf("Begin wait for task!\n");
    co_await rstudio::logic::cotasks::WaitTaskFinish{ newTaskId, 10000 };
    printf("After wait for task!\n");

    rstudio::logic::cotasks::RpcRequest 
        rpcReq{clientProxy, "DoHeartBeat", rstudio::reflection::Args{ 3 }, 5000};
    auto* rpcret = co_await rpcReq;
    if (rpcret->rpcResultType == rstudio::network::RpcResponseResultType::RequestSuc) {
        assert(rpcret->totalRet == 1);
        auto retval = rpcret->retValue.to<int>();
        assert(retval == 4);
        printf("rpc coroutine run suc, val = %d!\n", retval);
    }
    else {
        printf("rpc coroutine run failed! result = %d \n", (int)rpcret->rpcResultType);
    }
    co_await rstudio::logic::cotasks::Sleep(5000);
    printf("step4, after 5s sleep\n");
    co_return rstudio::logic::CoNil;
} );
```

**执行结果:**

```text
step1: task is 1
step2 after yield!
in while loop c=0
in while loop c=1
in while loop c=2
in while loop c=3
in while loop c=4
in for loop c=0
in for loop c=1
in for loop c=2
in for loop c=3
in for loop c=4
step3 5
new task create in coroutine: 2
Begin wait for task!
from child coroutine!
after child coroutine sleep
After wait for task!
service yield call finish!
rpc coroutine run suc, val = 4!
step4, after 5s sleep
```

整体来看, 协程的使用还是给异步编程带来了很多便利, 但框架本身的实现其实还是有比较多迭代优化的空间的: 1. asio的调度部分与coroutine部分的实现是分离的 2. coroutine暂时只支持主线程

### 2.4 小结

上面也结合halo的实例说到了一些限制, 那么这些问题有没有好的解决办法了, 答案是肯定的, 虽然execution并未完全通过提案, 但整体而言, execution新的sender/reciever模型, 对于解决上面提到的一些缺陷, 应该是提供了非常好的思路, 我们下一章节中继续展开.

## 3. so easy - execution就是解?

最开始的想法其实比较简单, 结合原来的framework, 适当引入提案中的execution一些比较可取的思路, 让framework的异步编程能更多的吸取c++新特性和execution比较高级的框架抽象能力, 提升整个异步库的实现质量. 所以最开始定的主线思路其实是更多的向execution倾斜, 怎么了解掌握execution, 怎么与现在的framework结合成了主线思路. 我们选择的基础参考库是来自冲元宇宙这波改名的Meta公司的libunifex, 客观来说, Meta公司的folly库, 以及libunifex库的实现质量, 肯定都是业界前沿的, 对c++新特性的使用和探索, 也是相当给力的. 这些我们后续在分析libunifex具体实现的篇章中也能实际感受到. 但深入了解libunifex后, 我们会发现, 它的优点有不少: 1. 尝试为c++提供表达异步的框架性结构. 2. 泛型用得出神入化, ponder在它前面基本是小弟级别的, 一系列泛用性特别强的template 编程示例, 比如隐含在sender/receiver思路内的lazy evaluate表达, 如何在大量使用泛型的情况下提供业务定制点等等. 3. 结构化的表达并发和异步, 相关代码的编写从自由发挥自主把控走向框架化, 约束化, 能够更有序更可靠的表达复杂异步逻辑 4. 整个执行pipeline的组织, 所有信息是compile time和runtime完备的, dependencies不会丢失. 5. 节点之间的值类型是强制检查的, 有问题的情况 , 大多时候compiler time就会报错. 有不少优点的同时, 也有很多缺点: 1. 整个库的实现严重依赖了c++20 ranges采用的一种定制手段 cpo, 并且也使用了类似ranges的pipe表达方法, 理解相关代码存在一定的门坎.(后续会有具体的篇章展开相关的内容) 2. 库同时向下兼容了c++17, 但由于c++17本身特性的限制, 引入了大量的宏, 以及X Macros展开的方式, 导致相关的代码阅读难度进一步提升. 但实际上c++17版本并不具备可维护的价值, 依赖SIFINAE的实现, 如果中间任何一环报错, 必然需要在N屏的报错中寻找有效信息. 3. libunifex对coroutine的支持存疑, 虽然让coroutine可以作为一种reciever存在, 但本质上来说, coroutine其实更适合拿来做流程控制的胶水, 而不是作为异步中的某个节点存在. 4. 默认的scheduler实现质量离工业级还存在一定的距离, 这一点后续的代码分析中也会具体提到. 诸多问题的存在, 可能也是execution提案没有短时间内获得通过的原因吧, 但整体来说, execution本身的理念还是很有参考价值的, 但以它的现状来说, 离最终的解肯定还是有比较大的距离的.

## 4. 尝试重新思考 - 要什么, 用什么

事情到这个点就有点尴尬了, 原有的asio, 架构层面来说, 跟新的execution是存在落差的. 而项目实践上来说, asio相当稳扎稳打, 而以libunifex当前的状态来说, 离工业化使用其实是有一定距离的. 但asio作者在21年时候的两篇演讲(更像coding show): 1. [Talking Async Ep1: Why C++20 is the Awesomest Language for Network Programming](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DicgnqFM-aY4%26t%3D868s) 2. [Talking Async Ep2: Cancellation in depth](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DhHk5OXlKVFg) 第一篇基本整个演示了asio从最开始的callback, 到融入c++20 coroutine后的优雅异步表达, 我们可以通过下面的代码片断感受一下:

**asio相关示例代码1**

```text
awaitable<void> listen(tcp::acceptor& acceptor, tcp::endpoint target)
{
  for (;;)
  {
    auto [e, client] = co_await acceptor.async_accept(use_nothrow_awaitable);
    if (e)
      break;

    auto ex = client.get_executor();
    co_spawn(ex, proxy(std::move(client), target), detached);
  }
}
```

**asio相关示例代码2**

```text
  auto [e] = co_await server.async_connect(target, use_nothrow_awaitable);
  if (!e)
  {
    co_await (
        (
          transfer(client, server, client_to_server_deadline) ||
          watchdog(client_to_server_deadline)
        )
        &&
        (
          transfer(server, client, server_to_client_deadline) ||
          watchdog(server_to_client_deadline)
        )
      );
  }
```

对比原来每个async_xxx()函数后接callback的模式, 整个实现可以说是相当的优雅了, 代码的可读性也得到了极大的提高, 这两段代码都来自于上面的演讲中, 想深入了解的可以直接打开相关的链接观看视频, 很推荐大家去看一下. 能够把复杂的事情用更简洁易懂的方法表达, 这肯定是让人振奋的, 当然, 深入了解相关实现后, 也会发现存在一些问题, 但我们的本意是参考学习, 得出最终想要的可以比较好的支撑并发和异步业务的基础框架, 有这些, 其实已经可以理出一条比较清晰的思路了: 1. execution部分主要使用它的sender/receiver概念, 和它提供的一些通用的算法. 移除掉所有因为fallback c++17引入的大量代码噪声. 抛弃它并不完备的各种scheduler实现 2. 协程借鉴部分asio的思路, 首先让协程可以基于context上下文, 在跨线程的情况下使用, 另外更多还是使用原有框架有明确的scheduler的方式对所有协程进行管理和定制的模式. 3. 使用asio的scheduler部分作为execution的底层scheduler实现, 同时也使用asio的timer表达, 去除原始libunifex依赖不同scheduler提供schedule_at()方法来执行定时器相关逻辑的实现. 4. 根据业务需要, 定制一些必要的sender adapter等简化业务的使用. 5. 尝试用execution框架对接ISPC等特殊的并发库, 能够以一个清晰的方式来表达这种混合环境上执行的逻辑.

本系列涉及的基础知识和相关内容比较多, 先给出一个临时的大纲, 后续可能会有调整. 目前的思路是先介绍大家相对熟悉度不那么高的execution基础知识和libunifex, 后面再介绍asio相关的scheduler以及coroutine实现, 最后再回归笔者正在迭代的framework, 这样一个顺序来展开.

## 5. 系列的大纲

~~<< c++异步从理论到实践 - 1. 总览篇 >>~~ << c++异步从理论到实践 - 2. range的pipeline机制简介(execution基础)>> << c++异步从理论到实践 - 3. CPO介绍与实践(execution基础)>> << c++异步从理论到实践 - 4. libunifex的实现概述>> << c++异步从理论到实践 - 5. libunifex的scheduler实现>> << c++异步从理论到实践 - 6. asio的scheduler实现 >> << c++异步从理论到实践 - 7. asio的coroutine实现>> << c++异步从理论到实践 - 8. framework的异步库改造>>

## 6. 参考

1. [One Frame in Halo Infinite](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DIUiNUky-ibM)
2. [asio官网](https://link.zhihu.com/?target=https%3A//think-async.com/Asio/)
3. [libunifex源码库](https://link.zhihu.com/?target=https%3A//github.com/facebookexperimental/libunifex)

原文作者：鹅厂架构师

原文链接：https://zhuanlan.zhihu.com/p/515309214

# 【NO.244】大数据组件选型对比及架构

## 1.MQ架构设计及选型对比

### 1.1 **RocketMQ vs Kafka vs Pulsar**

RocketMQ: **后台业务开发、高性能及高可靠场景**，如阿里双十一电商业务，阿里开源,现在为apache rocketmq；queue模式，支持dead letter queue可延迟投递,pull +push皆可支持。可靠同步+可靠异步传输

Kafka: 分布式日志流传输系统，更多用于大数据领域，顺序磁盘写入、zero-copy等特大幅提升kafka性能，特别适合**海量数据传输**。计算和存储耦合；streaming模式。仅支持pull模式。

**Pulsar**： **Kafka+RocketMQ**，支持streaming+queue模式，既可以用于后台业务开发，如腾讯计费系统-米大师，又可以使用到大数据领域用于数据传递; 支持pull+push模式;支持**多租户管理、跨地域复制；存算分离;支持dead-letter-queue，实现延迟发送**



### 1.2 **Queue vs Topic**

![img](https://pic3.zhimg.com/80/v2-2dd2ecb8eb5ebf19032b633d641a01a2_720w.webp)

![img](https://pic1.zhimg.com/80/v2-0692c8eb2f9013965528b0ba0a3c6aa8_720w.webp)

### 1.3 **RocketMQ架构图**

![img](https://pic3.zhimg.com/80/v2-d57e6b0ca968f8e94733845df69323ba_720w.webp)

发送方式及可靠性：

![img](https://pic1.zhimg.com/80/v2-ae6fe49a95516520d103226feeb8914c_720w.webp)



### 1.4 **Kafka架构图**

![img](https://pic4.zhimg.com/80/v2-7117f7aaaf1e9df4b92c59d8c013685b_720w.webp)

![img](https://pic3.zhimg.com/80/v2-61c735fb5dd2e08d30c7fcbe8f71fa8e_720w.webp)



### 1.5 **Pulsar架构**

![img](https://pic1.zhimg.com/80/v2-3cfc7145f08d3336e96b5d1bf162d228_720w.webp)

**Pulsar消费方式:Streaming + Queuing**

![img](https://pic2.zhimg.com/80/v2-6b9310cddc8983e6eaac9d88fdb5a235_720w.webp)

## 2.**KV架构设计及选型对比**

KV存储使用场景高性能缓存，配合数据库完成海量服务的后台设计。具体的KV有多种，具体有如下几种：

**mongodb**: 可以存储文档doc的kv数据库，在博客系统使用很广泛

**redis**：延迟极低（5ms），经常用于后台开发的缓存系统，基于内存数据存储，通过持久化化策略AOF、RDB 将数据固化到磁盘；断电有丢失数据风险。常用的结构有hash、map、list、sortedset

**hbase**: 延迟还可以（500ms），适合海量数据的存储，如日增量200G数据的场景;一般来说hbase运维比较复杂。

**rocksdb**：高性能KV存储，有广泛的用途，一般以基础建设方式存在，如tidb底层基于rocksdb存储、flink的状态存储也采用rocksdb、阿里高性能的TairDB、美团的Cellar的KV存储等都是基于rocksdb修改和完善



### 2.1 **mongodb集群**

![img](https://pic3.zhimg.com/80/v2-509e446a63e75d69c141297062d070de_720w.webp)

### 2.2 **redis集群**

**redis sentinel 哨兵模式**

![img](https://pic3.zhimg.com/80/v2-51f52f749927dc21b60f084d2e68d922_720w.webp)



**redis codis集群模式（豌豆荚开放，刘奇负责开发，后续创建了pingcap公司）**

![img](https://pic2.zhimg.com/80/v2-fabd4d53145203e741457cfeb05ba1e9_720w.webp)



**redis 3.0 官方支持，最多不能超过1000个节点**

![img](https://pic4.zhimg.com/80/v2-cea754ba02ebbd48047fdd2a08f6cafb_720w.webp)



### 2.3 **Hbase集群**

![img](https://pic1.zhimg.com/80/v2-0b7946e0e49b7b87c8cd27d98fbac710_720w.webp)

## 3.**OLAP架构设计及选型对比**

海量数据自由聚合和分析，目前分为两个派系，一类是预聚合好，提供服务的；另外一类是MPP数据库，按需执行计算和聚合。

**预聚合类：**

kylin: 离线T+1的报表分析，将数仓的数据导出到kylin，做分析和使用

**apahe druid**: 偏向于实时的olap多维分析，做实时报表和风控；一般可做实时+离线报表；而且维度支持上万列

**MPP数据库：**

**apache doris**：MPP数据库，集存储和计算于一体，多表join性能不错。可以做实时+离线的MPP数据库，支持高并发的写入和查询；可以做线性扩容。最初百度开源，商业化公司starrocks

clickhouse：MPP数据库，百亿数据做group by统计秒级返回。俄罗斯公司yandex开放使用。

**MPP计算引擎：**

**presto**/impala: 不存储数据，直接以MPP的方式查询存储到Hadoop里面的数据，presto 使用java编写，impala使用c++编写。美团、哈啰等使用presto查询HDFS数据，要比hive自带的mr更快速。



**Apache druid架构：**

按节点角色分类架构图：

![img](https://pic4.zhimg.com/80/v2-f2a81ab8f7d55a87ebc05007273a724f_720w.webp)

数据摄入及服务提供：

![img](https://pic2.zhimg.com/80/v2-9e07441032d223165994eddf371a2589_720w.webp)

**MPP架构：**

大规模并行处理

![img](https://pic4.zhimg.com/80/v2-707343adf98e2cffbd88749c73af969b_720w.webp)









**Doris架构：**

整体架构：

![img](https://pic4.zhimg.com/80/v2-424a6c0abf7e37d3eb18806a4783ea23_720w.webp)

doris上下游生态

![img](https://pic2.zhimg.com/80/v2-7be7c372b2d09c2490f46b3fe3da03a1_720w.webp)

**presto架构：**

![img](https://pic3.zhimg.com/80/v2-6f156ae3db3a8f015a064807605f1f4e_720w.webp)



1. **流式计算框架（flink、kafka streaming、apache nifi、apache beam）对比**

**flink**：分布式低延时、海量数据实时处理框架，生态十分完善，如cep、图计算、flink ml等，是实时计算的标杆

kafka streaming：是一个lib,可以嵌入到程序里面进行实时数据流的处理，不适合处理亿级甚至千万级的数据

apache nifi：很好支持了**物联网MQTT协议**，可以做物联网边缘端数据的清洗和处理，有图像化操作界面，方便易用，可将数据处理完成后传递到大数据中心如flink集群进行最终处理

apache beam:是一个不强绑定的大数据实时计算、离线计算的SDK，它的代码可以打包部署到flink集群、spark集群、google data cloud platform集群等等，它指定的大数据计算的api，底层运行可以兼容多种框架。



### 3.1 **flink架构：**

![img](https://pic1.zhimg.com/80/v2-4e395a64889c14d720417f36c4421bd4_720w.webp)

### 3.2 **Kafka Stream**

后台程序配合kafka stream lib的程序应用架构

![img](https://pic2.zhimg.com/80/v2-52ef65b936af3bec3c1ebce2528a20a9_720w.webp)

后台程序分布式处理：

![img](https://pic3.zhimg.com/80/v2-061c024754276cadbf00c371287ca292_720w.webp)



### 3.3 **Apache NIFI**

架构图：

![img](https://pic4.zhimg.com/80/v2-3fa8a3ba474ec06de28d4b96c80ef20b_720w.webp)

界面化pipeline操作

![img](https://pic3.zhimg.com/80/v2-f5902016cfa04d5ce9abde4230b37b7e_720w.webp)



### 3.4 **Apache Beam:**

![img](https://pic1.zhimg.com/80/v2-e86d06ab87adb0e3dab2dbd42ec75c90_720w.webp)



编程语言、批流模式、编程模型、运行Runner

![img](https://pic3.zhimg.com/80/v2-e486cb5a6129b7704fb7718df55e2ffe_720w.webp)



## 4.**数据湖选型对比（deltalake、hudi、iceberg）**

**deltalake**：spark后面商业公司databricks开源，跟spark强耦合，是比较早提出数据湖概念，目前市场热度不高，很大一部分公司实时计算使用flink，所以集成度不高。

**iceberg**: 较为标准的数据湖公司，由uber公司开源，目前有腾讯、阿里、网易、去哪儿网内部使用iceberg构建数据湖；

**hudi**; uber公司开源的数据湖解决方案，目前使用比较广泛；在googgle云、微软云、IBM云阿里云、腾讯云EMR、亚马逊云EMR等大数据套件提供了对hudi的集成支持，目前像T3出行；hudi背后成立商业公司OneHouse，整体来说，hudi有商业公司加持，发展更好



### 4.1 **hudi 上下游生态：**

![img](https://pic3.zhimg.com/80/v2-cd4be6e970f96604e5841e2dca5c1836_720w.webp)



### 4.2 **Flink+iceberg构建实时数据湖：**

![img](https://pic2.zhimg.com/80/v2-0fcd9926d40cc8df6758104a54256449_720w.webp)

### 4.3 **HTAP选型及对比（Google Spanner+F1、TIDB、CocksRoachDB）**

TP和AP数据库很难兼容统一完成，只能不断组合提供更完备的混合型TP+AP的数据库；TiDB和CocksRoachDB都是模仿google的论文Spanner、F1论文完成的。目前TIDB在国内发展很快，tidb很好打造成功了金融级解决方案。cocksroachdb在国外发展不错，主打跨洲际数据复制。

无论tidb还是cockroachdb，从开发之处就支持k8s云原生部署。

Spanner 是Google的全球级的分布式数据库 (Globally-Distributed Database) 。Spanner的扩展性达到了令人咋舌的全球级，可以扩展到数百万的机器，数已百计的数据中心，上万亿的行。

### 4.4 **Google Spanner +F1架构**



![img](https://pic4.zhimg.com/80/v2-eecf91d4257198e137470ab3b1ef16b3_720w.webp)

F1+Spanner分布式架构全景图：

![img](https://pic3.zhimg.com/80/v2-b2f68deaa31a64bbec5e9cf4149c5f96_720w.webp)



### 4.5 **TiDB架构**

tidb整体架构

![img](https://pic4.zhimg.com/80/v2-be496de2529401ee1fb9f81cf9ca174f_720w.webp)

tidb结合后台程序的整体架构

![img](https://pic2.zhimg.com/80/v2-bad58abc1df848bf23ce3d819d7969d5_720w.webp)

### 4.6 **CockRoachdb 架构：**

整体架构：

![img](https://pic2.zhimg.com/80/v2-4a4c6ce0e333a65e26130d5bd6aea579_720w.webp)



cockroachdb跨洲际数据同步复制：

![img](https://pic4.zhimg.com/80/v2-b66d6db41304a783046b4560f004caf7_720w.webp)

原文作者：鹅厂架构师

原文链接：https://zhuanlan.zhihu.com/p/510249281

# 【NO.245】由CPU高负载引发内核探索之旅

> 以下内容来自腾讯应用框架团队 dalek

导语：**STGW（腾讯云CLB）**在腾讯云和自研业务中承担多种网络协议接入、请求加速、流量转发等功能，有着业务数量庞大、接入形式多样、流量规模巨大的特点，给产研团队带来了各种挑战，经常要深入剖析各种疑难杂症。本文介绍了STGW在实际运营过程中，一次没有造成业务影响的CPU高负载被发现后，团队进行深入分析从内核端口发现问题根源，在经过与内外部linux内核专家们共同协作，输出解决方案并最终修复问题。

## 1.问题起源

值班期间，运维同学偶然发现一台机器CPU消耗异常，从监控视图上看出现较多毛刺。而属于同一集群的其他机器在同一时间段CPU消耗相对稳定。

![img](https://pic4.zhimg.com/80/v2-3778fa83820c3d25c3b4da99588d4fb7_720w.webp)

从机器维度的监控无法掌握更多的信息，通过自建的秒级监控系统，我们拿到了更多的性能数据。实际的消耗情况比机器监控上看到的更加严重，高负载来自于sys消耗，全核cpu都被内核彻底消耗掉了。

![img](https://pic1.zhimg.com/80/v2-531469ef0a41c2cd9d115ff5915c5574_720w.webp)

秒级监控除了会捕获细粒度的系统负载外，针对发生高负载场景，会触发分析工具进行分析。虽然高负载发生的时间很短，依靠这套系统我们先拿到了导致高负载的直接原因，发生在inet_hash_connect函数中。

![img](https://pic1.zhimg.com/80/v2-4dc29a95ec5c401be0b4b311c66637cc_720w.webp)

## 2.关于inet_hash_connect

inet_hash_connect这个函数是内核处理tcp连接的必经之路。我之前一篇关于高负载的文章也分析了tcp连接引发的该函数单核高负载的场景，详见：[从STGW流量下降探秘内核收包机制](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/70ARIx-JzXwTDe0a4vg_ig)，当时引发问题的函数是inet_lookup_listener。
不同点在于，inet_lookup_listener是服务端收到新连接时寻找监听端口，而inet_hash_connect函数是主动建立tcp连接，对应到我们的场景，就是STGW服务器与后端RealServer（后面简称RS）建立连接。
通过内核代码分析，该函数的简化流程如下，其作用是tcp连接主动选取一个端口，检查可用后，进行bind绑定操作，该端口即发起方用于收发连接数据的端口。

```text
// 主动发起tcp连接
connect(fd, servaddr, addrlen);
-> sock->ops->connect() == inet_stream_connect
-> tcp_v4_connect()
    -> inet_hash_connect()
        -> __inet_hash_connect()
           /*
           如果指定了port，则使用指定的port作为客户端端口
           否则，随机选取一个port
           */
           // 端口可用性检查
             -> check_established()
           // bind端口
             -> inet_bind_bucket_create
             -> inet_bind_hash       
```

为什么inet_hash_connect会出现高负载？
从perf看，直接原因是raw_spin_lock锁带来的剧烈消耗，我们先找到这个锁所在位置，根据对应内核源码找inet_hash_connect实现及内部调用中，发现只有inet_check_establish里会进行spin_lock（其他几处为spin_lock_bh，如果是其他地方，应该为raw_spin_lock_bh）。

![img](https://pic3.zhimg.com/80/v2-290718f1af81bff777334b87b9663eba_720w.webp)

## 3.前期排查

通常锁造成高负载我们会怀疑是否有死锁产生，从cpu现象来看只是短时间突增并非死锁。那么我们有另外两种猜想：
\1. 锁覆盖的范围执行极慢，导致锁了很长时间。
\2. 频繁执行该函数执行加锁导致高负载。
先看第一种情况，我们假设inet_check_establisted函数中加锁区域代码执行效率慢，导致高负载。
分析代码容易看出，加锁部分是一个遍历哈希链表的操作，通过传入的参数计算一个哈希值，拿到哈希桶后遍历其中可用的节点，这种遍历操作确实值得怀疑，历史case告诉我们，哈希桶挂载的节点非常多导致遍历复杂度急剧上升，拖累整个cpu。例如[从STGW流量下降探秘内核收包机制](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/70ARIx-JzXwTDe0a4vg_ig)中分析到，由于哈希桶数量太少，当监听端口足够多时，遍历效率太低导致高负载的案例。
因此，这里合理怀疑是否是ehash（TCP Established hash table）桶遍历太久，导致加锁时间过长？
根据之前的经验，我习惯性先找哈希桶初始化地方，尝试看这个问题是否如之前一般由哈希桶数量太少导致，在tcp.c文件的tcp_init函数中找到其初始化函数。

![img](https://pic2.zhimg.com/80/v2-f34ee3b6564564871467fbba718140c9_720w.webp)

发现ehash的桶大小是由alloc_large_system_hash为其申请的空间，而该函数实现较为晦涩，引入了根据机器物理内存大小动态调整申请空间，很难通过代码直接看出到底分配了多大数量的哈希桶。好在函数最后会向系统日志打印出此次申请的哈希桶的大小。

```text
pr_info("%s hash table entries: %ld (order: %d, %lu bytes)\n",
		tablename, 1UL << log2qty, ilog2(size) - PAGE_SHIFT, size);
```

根据代码中的打印方式在机器上果然找到对应日志，从而得知ehash table的桶超过了100w个

![img](https://pic4.zhimg.com/80/v2-456e7709a698fd63d1485b68bc1819ab_720w.webp)

这意味着在哈希均匀的情况下，ehash table可以容纳百万条establish状态的连接，不会出现遍历热点。
而问题机器的连接数峰值大概50w左右，远没有达到阈值。

![img](https://pic3.zhimg.com/80/v2-2667180ff73c3593894dd702c2d8f652_720w.webp)

根据经验，另外一个猜想是：存在不均匀的流量，ehash计算的哈希值可能都是同一个或几个，导致大量连接都落到了少数几个桶里，这种也有可能导致高负载。
通过监控和日志分析，确实发现STGW服务器与某个RS（RealServer）连接数较大，超过了2w个连接。（客户转发规则仅配置了一个RS，因此STGW收到的请求，都只能向这个RS发起连接并转发数据）。
那么往单个RS建立连接数过多会造成哈希桶使用不均匀吗？

```text
static inline unsigned int __inet_ehashfn(const __be32 laddr,
					  const __u16 lport,
					  const __be32 faddr,
					  const __be16 fport,
					  u32 initval)
{
	return jhash_3words((__force __u32) laddr,
			    (__force __u32) faddr,
			    ((__u32) lport) << 16 | (__force __u32)fport,
			    initval);
}
```

分析看，ehash table在查找和插入过程计算哈希值都是通过TCP四元组进行哈希，而至少源端口这里是足够散列的，因此理论上不存在往同一个桶插入过多节点的情况。

## 4.峰回路转

此时回过头看，我们一直在问题机器上折腾，但还有个关键信息一直没有被深入挖掘。

![img](https://pic1.zhimg.com/80/v2-f8cd85e103572737328cdd7346fc990c_720w.webp)

根据运维同学提醒，同集群有另外一种机型可以作为参照，高负载仅出现在其中一种代号为25G的机型上，也就是说，承担了同样的转发流量，另外一种10G机型却并没有高负载出现。
根据这个线索，我们排查对比了集群内两种机型的异同点，首先流量大小、请求成分这些都是一致的，而网卡型号、CPU型号、内核版本这些都是不一样的，根据上面的排查，CPU高负载的热点主要在内核协议栈函数中，于是我们主要对比不同内核版本的实现差异。
发现不同内核版本在inet_hash_connect函数实现上，确实有明显区别。

- 先来看不出问题的机型，其内核版本为基于linux 3.10.107

```text
// Linux 3.10.107 x86_64 GNU/Linux

int __inet_hash_connect(struct inet_timewait_death_row *death_row,
		struct sock *sk, u32 port_offset,
		int (*check_established)(struct inet_timewait_death_row *,
			struct sock *, __u16, struct inet_timewait_sock **),
		int (*hash)(struct sock *sk, struct inet_timewait_sock *twp))
{
	struct inet_hashinfo *hinfo = death_row->hashinfo;
	const unsigned short snum = inet_sk(sk)->inet_num;
	struct inet_bind_hashbucket *head;
	struct inet_bind_bucket *tb;
	int ret;
	struct net *net = sock_net(sk);
	int twrefcnt = 1;

	if (!snum) { // 未指定端口
		int i, remaining, low, high, port;
		static u32 hint;
		u32 offset = hint + port_offset;
		struct inet_timewait_sock *tw = NULL;
        // 获取本地可用端口范围
        // 统一配置的10241 ~ 59999
		inet_get_local_port_range(net, &low, &high);
		remaining = (high - low) + 1;

		local_bh_disable();
        // remaining 即为 local_port_range规定的端口数量
		for (i = 1; i <= remaining; i++) {
			int ret;
            // 注意：选取的port是逐次递增1，最多执行remaining次
			port = low + (i + offset) % remaining;
			if (inet_is_reserved_local_port(port))
				continue;
			head = &hinfo->bhash[inet_bhashfn(net, port,
					hinfo->bhash_size)];
			ret = spin_trylock(&head->lock);
			if (!ret)
				continue;

			/* Does not bother with rcv_saddr checks,
			 * because the established check is already
			 * unique enough.
			 */
			inet_bind_bucket_for_each(tb, &head->chain) {
				if (net_eq(ib_net(tb), net) &&
				    tb->port == port) {
					if (tb->fastreuse >= 0 ||
					    tb->fastreuseport >= 0)
						goto next_port;
					WARN_ON(hlist_empty(&tb->owners));
                    // 检查端口是否可用
					if (!check_established(death_row, sk,
								port, &tw))
						goto ok;
					goto next_port;
				}
			}

			tb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,
					net, head, port);
			if (!tb) {
				spin_unlock(&head->lock);
				break;
			}
			tb->fastreuse = -1;
			tb->fastreuseport = -1;
			goto ok;

		next_port:
			spin_unlock(&head->lock);
		}
		local_bh_enable();

		return -EADDRNOTAVAIL;

/* 省略部分问题无关代码 */
}
```

- 问题机型的内核版本基于4.14.105版本，该版本相较于上述更先进，其中函数实现如下

```text
// Linux 4.14.105-1 x86_64 GNU/Linux

int __inet_hash_connect(struct inet_timewait_death_row *death_row,
		struct sock *sk, u32 port_offset,
		int (*check_established)(struct inet_timewait_death_row *,
			struct sock *, __u16, struct inet_timewait_sock **))
{
	struct inet_hashinfo *hinfo = death_row->hashinfo;
	struct inet_timewait_sock *tw = NULL;
	struct inet_bind_hashbucket *head;
	int port = inet_sk(sk)->inet_num;
	struct net *net = sock_net(sk);
	struct inet_bind_bucket *tb;
	u32 remaining, offset;
	int ret, i, low, high;
	static u32 hint;

	/*省略部分无关代码*/

	inet_get_local_port_range(net, &low, &high);
	high++; /* [32768, 60999] -> [32768, 61000[ */
    // remaining为local_port_range规定的端口数量
	remaining = high - low;
	if (likely(remaining > 1))
		remaining &= ~1U;

	offset = (hint + port_offset) % remaining;
	/* In first pass we try ports of @low parity.
	 * inet_csk_get_port() does the opposite choice.
	 */
    // 注意：这里意味着，如果offset为奇数，强制变成偶数
	offset &= ~1U;
other_parity_scan:
    // 注意：由于offset第一次必为偶数，port的奇偶性完全取决于low
    // (注：下面会goto other_parity_scan，再次回到这里，奇偶性反转)
	port = low + offset;
    // 注意：该循环每次递增2，意味着只会查询remaining/2个端口，并且这些端口奇偶性与port初始值一致
	for (i = 0; i < remaining; i += 2, port += 2) {
		int ret;
		if (unlikely(port >= high))
			port -= remaining;
		if (inet_is_local_reserved_port(net, port))
			continue;
		head = &hinfo->bhash[inet_bhashfn(net, port,
						  hinfo->bhash_size)];
		ret = spin_trylock_bh(&head->lock);
		if (!ret)
			continue;

		/* Does not bother with rcv_saddr checks, because
		 * the established check is already unique enough.
		 */
		inet_bind_bucket_for_each(tb, &head->chain) {
			if (net_eq(ib_net(tb), net) && tb->port == port) {
				if (tb->fastreuse >= 0 ||
				    tb->fastreuseport >= 0)
					goto next_port;
				WARN_ON(hlist_empty(&tb->owners));
				if (!check_established(death_row, sk,
						       port, &tw))
					goto ok;
				goto next_port;
			}
		}

		tb = inet_bind_bucket_create(hinfo->bind_bucket_cachep,
					     net, head, port);
		if (!tb) {
			spin_unlock_bh(&head->lock);
			return -ENOMEM;
		}
		tb->fastreuse = -1;
		tb->fastreuseport = -1;
		goto ok;
next_port:
		spin_unlock_bh(&head->lock);
		cond_resched();
	}
    // 注意：循环结束后，走到这里意味着依然没有找到合适的端口
    // 此时将offset的奇偶性改变，通过goto返回上面循环，重新查找可用端口
	offset++;
	if ((offset & 1) && remaining > 1)
		goto other_parity_scan;

	return -EADDRNOTAVAIL;

/*省略部分问题不相关代码*/
}
```

从前面的内容交代过，inet_hash_connect的一个最重要的功能就是选取本地socket的端口并且检查其是否可用。从上述两段不同内核代码分析，很明显在端口选取实现两个内核差异很大。

- 3.10内核不出问题：其端口选取过程为for循环在port_offset基础上逐次递增1，直到找到可用端口为止，或者超过了local_port_range个数，则返回EADDRNOTAVAIL
- 4.14内核出现高负载问题：其端口选取过程为for循环在port_offset基础上逐次递增2，**只把奇数范围（取决于local_port_range左边界）端口进行遍历，如果整个奇数范围都找不到可用端口，再遍历所有偶数端口，**直到找到可用端口为止，或者超过了local_port_range个数，则返回EADDRNOTAVAIL。


4.14内核做了奇偶区分，每次递增2就是为了只找奇数或偶数的端口，这种遍历方式乍一看似乎没毛病，因为可用端口总数限定了（local_port_range），在port_offset初始值Z足够离散的情况下，遍历过程不管递增1还是递增2，应该是差不多的，都有概率在递增后碰到可用端口。

上面提到，我们的问题服务器，有一个业务只绑定了一个RS，在高负载的时候，服务器与这个RS建立的连接超过了2w条。结合这个现象，很快发现了问题所在，由于我们的local_port_range为10241~59999，总的可用端口数为49758个，其中奇数、偶数分别占2.4万多个。
所以，高负载期间，问题服务器与该RS建立了2w多条连接，**实际上将inet_hash_connect中的奇数端口几乎耗尽，然而每次与该RS建立新连接，内核都要首先遍历奇数端口，进行2w多次无效的端口查找与检查（inet_check_establish进行spin_lock），才有可能开始遍历偶数端口，从而找到可用端口。**
相比之下，3.10内核的实现，在49758个端口内查找，即便是已经有2w多个端口不可用了，借助于port_offset足够分散的前提（port_offset基于源+目的地址进行散列），平均下来每次都可以较快找到可用的端口。

## 5.复现验证

为了最终确认这个问题，我们将现象及分析同步给了系统测试同学及tlinux研发，tlinux同学帮忙修改了4.14内核中的inet_hash_connect函数，使其不再按奇偶性选择端口，帮忙打包了一个4.14.105内核修复版。
系统测试同学对比测试了 4.14.105原版本 vs 4.14.105修复版，结果如下：

- 原版本向RS 172.16.0.1:20000建立2.8w条连接后，连接数很难继续上升，同时CPU全核出现sys高负载，perf发现就是inet_hash_connect导致

![img](https://pic2.zhimg.com/80/v2-3e38af0f533d52c9455748caa7e16c29_720w.webp)

![img](https://pic3.zhimg.com/80/v2-40a611eaacdb5728b507bb53f7fe8796_720w.webp)

- 新内核版本，对于单个RS，可以打到4.3w条连接以上，CPU不出现高负载，消耗最大的是usr中SSL握手消耗

![img](https://pic3.zhimg.com/80/v2-4bf229f53c25e2d45f9ff4aded58fbca_720w.webp)

![img](https://pic1.zhimg.com/80/v2-4cedbeaefb535566946501ee19d3dda0_720w.webp)

## 6.溯源

到这里，我们在理论上与实践上都证实了，问题是由于更先进的内核（4.14版本）修改了inet_hash_connect中选取端口的遍历逻辑，优先遍历奇数或者偶数端口，当服务器与单个RS建连数量超过local_port_range/2后就会导致无效遍历过多从而cpu高负载。
所以，为什么linux官方要将选端口逻辑改成奇偶遍历呢？通过git修改历史，很快定位到了提交点。

![img](https://pic2.zhimg.com/80/v2-50817883e00976dcf89e69f192b6b189_720w.webp)

![img](https://pic3.zhimg.com/80/v2-8a67d11e6a30c8d32dbe1b3d609ccfb2_720w.webp)

如图，在4.6版本开始，Google内核专家提交了两个commit，将可用端口按奇偶划分两部分，一部分给connect使用，另一部分给bind使用。我们追溯了一下bind的实现（细节可参见inet_csk_get_port）发现确实与inet_hash_connect做了类似的操作，只不过一个是取奇数另一个取偶数。
结合自身情况来看，我们并没有随机bind端口的场景（例如listen端口，一般都是指定端口进行监听，不存在随机的情况）。因此接下来进一步找了作者确认。
作者给我的回信明确了对于不需要bind()随机端口的用户，这个方式是有害的。同时只提到是google需要大量随机bind()，为了避免频繁发生事故加了这个功能，但google到底什么场景需要如此频繁的随机bind，作者没有给明确回应。

## 7.解决问题

即便找到了原作者反馈问题，他并不乐意进一步做方案优化，而是让我们自己按需解决。对于这个问题，我们从短期到长期给出的解决方案如下：

###  7.1 短期方案

通过增加四元组的数量缓解问题，该问题实际上是源地址+目的地址+目的端口都确定的情况下，新连接只能从本地端口这一个维度进行扩展。即便没有高负载问题（老版本内核）连接数也无法超过5w（受限于local_port_range）。因此，可以通过拓宽四元组的另外维度，来大幅度提升可建立的连接数，具体有三个办法：

- 增加后端RS数量：扩展目的地址数量
- 增加后端RS端口数：扩展目的端口数量
- 增加客户端机器本地地址数量：扩展源地址数量

### 7.2 最终方案

在腾讯的tlinux系统中，已进行优化内核奇偶遍历逻辑，给用户更自由的端口分配权限，彻底解决该问题。

## 8.总结

最后，总结整个问题和定位过程，我们从一次CPU高负载问题出发，从最开始在问题机器上猜想和分析，找不到确切答案。通过比较不同机型函数实现差异找到关键点。再通过代码分析、复现验证、社区求证这三个方面将问题的前因后果分析清楚，最终给出了解决方案。

原文作者：鹅厂架构师

原文链接：https://zhuanlan.zhihu.com/p/581496129

# 【NO.246】（建议收藏）万字长文总结分布式事务，总有一款适合你

> 以下内容来自腾讯后台研发工程师yanqing



导语：本文参考网络相关文章，主要总结了XA, 2PC, 3PC, 本地事务状态表, 可靠消息队列, 最大努力通知, TCC, SAGA等分布式事务的特点和适用场景，为大家选择分布式事务提供一些参考。

## 0.**概述**

分布式事务是指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点之上，相对的，传统事务也称之为单机事务。在单机事务时代，我们通常可以使用数据库的事务操作来解决数据的一致性问题；那么在微服务越来越流行的当下，我们应该如何保证不同服务器上数据的一致性？本文先从CAP理论和BASE理论说起，之后从一致性强弱的角度梳理当前主流的强一致性方案、最终一致性方案和弱一致性方案，最后总结一下各个方案的特点和适用场景，希望对你有所帮助。

## 1.**预备知识**

### **1.1 CAP理论**

CAP理论可以说是分布式系统的基石，它说的是**一个分布式系统最多只能同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance）这三项中的两项**，而不能同时满足。

> 2000年7月，加州大学伯克利分校的Eric Brewer教授在98年提出CAP猜想，99年发表(Harvest, Yield and Scalable Tolerant Systems)，2000年在ACM PODC主题演讲(CAP keynote)。2年后，麻省理工学院的Seth Gilbert和Nancy Lynch从理论上证明了CAP。之后，CAP理论正式成为分布式计算领域的公认定理。

CAP是Consistency、Availability、Partition tolerance三个词的缩写:

![img](https://pic1.zhimg.com/80/v2-342eb04cec544fd6dd0c9f35607cf030_720w.webp)

- **C：一致性**，所有客户端看到都是同一份数据，即使在数据更新和删除之后
- **A：可用性**，即使部分节点发生故障，所有客户端也能找到可用的数据备份
- **P：分区容忍性**，即使发生网络分区故障，系统仍然能够按照预期正常工作

**CAP定理在分布式领域至关重要，在构建大型分布式系统的时候我们必须根据自己业务的独特性在三者之间进行权衡**。

由于网络的各种不确定因素，在构建分布式应用的时候我们往往不得不考虑分区容忍性，这个时候我们通常只能在一致性和可用性之间进行选择。

### 1.2 **BASE理论**

根据CAP定理，如果要完整的实现事务的ACID特性，只能放弃可用性选择一致性，即CP模型。然而如今大多数的互联网应用中，可用性也同样至关重要。于是eBay架构师根据CAP定理进行妥协提出一种ACID替代性方案，即BASE，从而来达到可用性和一致性之间的某种微妙的平衡，选择AP模型的同时最大限度的满足一致性。

> 由 eBay 架构师 Dan Pritchett 于 2008 年在《BASE: An Acid Alternative》论文中首次提出

BASE是下面三部分的英文缩写简称：

- **BA**：**Basically Available** ，基本可用性
- **S：Soft State**，软状态
- **E：Eventually Consistency**，最终一致性

“**基本可用**”是相对CAP的“完全可用”而言的，即在部分节点出现故障的时候不要求整个系统完全可用，允许系统出现部分功能和性能上的损失：比如增加响应时间，引导用户到一个降级提示页面等等。

“**软状态**”则是相对CAP定理强一致性的“硬状态”而言，CAP定理的一致性要求数据变化要立即反映到所有的节点副本上去，是一种强一致性。“软状态”不要求数据变化立即反映到所有的服务器节点上，允许存在一个中间状态进行过渡，比如允许放大延时等。

“**最终一致性**”则是相对强一致性而言，它不要求系统数据始终保持一致的状态，只要求系统经过一段时间后最终会达到一致状态即可。

Base 理论是对 CAP 中一致性和可用性权衡的结果，其来源于对大型互联网分布式实践的总结，是基于 CAP 定理逐步演化而来的。其核心思想是：**强一致性（Strong consistency）无法得到保障时，我们可以根据业务自身的特点，采用适当的方式来达到最终一致性（Eventual consistency）**

## 2.**强一致性方案**

强一致性的方案便是前面提到的舍A保C的CP模型，即通过牺牲可用性来保证一致性，这种方案适用于对一致性要求很高的场景，比如金融交易等。

### 2.1 **2PC-二阶段提交**

二阶段提交(Two-phaseCommit)是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。通常，二阶段提交也被称为是一种协议(Protocol))。

> 第一个一致性问题实例应该是Lamport的“Time, Clocks and the Ordering of Events in a Distributed System” (1978)，大概在这篇论文发表的同一时间，JimGray在“Notes on Database Operating Systems” (1979)中描述了两阶段提交(2PC)。

当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。

因此，二阶段提交的算法思路可以概括为： 参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。

所谓的两个阶段是指：

***第一阶段：voting phase 投票阶段\***

事务协调者给每个参与者发送Prepare消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的redo和undo日志，但不提交，到达一种“万事俱备，只欠东风”的状态。

***第二阶段：commit phase 提交阶段\***

如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)

二阶段提交的操作时序图如下：

![img](https://pic4.zhimg.com/80/v2-5e2f6a17fc96203bb80843ddadf68bd7_720w.webp)

以上这两个过程被称为“两段式提交”（2 Phase Commit，2PC）协议，而它能够成功保证一致性还需要一些其他前提条件。

- 必须假设**网络在提交阶段的短时间内是可靠的**，即提交阶段不会丢失消息。两段式提交中投票阶段失败了可以补救（回滚），而提交阶段失败了无法补救（不再改变提交或回滚的结果，只能等崩溃的节点重新恢复），因而此阶段耗时应尽可能短，这也是为了尽量控制网络风险的考虑。
- 必须假设因为网络分区、机器崩溃或者其他原因而导致失联的**节点最终能够恢复**，不会永久性地处于失联状态。由于在准备阶段已经写入了完整的重做日志，所以当失联机器一旦恢复，就能够从日志中找出已准备妥当但并未提交的事务数据，并向协调者查询该事务的状态，确定下一步应该进行提交还是回滚操作。

两段式提交的原理很简单，也不难实现，但有几个非常明显的缺点：

**1. 单点故障问题**

协调者在两段提交中具有举足轻重的作用，协调者等待参与者回复时可以有超时机制，允许参与者宕机，但参与者等待协调者指令时无法做超时处理。一旦协调者宕机，所有参与者都会受到影响。如果协调者一直没有恢复，没有正常发送 Commit 或者 Rollback 的指令，那所有参与者都必须一直等待。

**2. 同步阻塞问题**

执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。也就是说从投票阶段到提交阶段完成这段时间，资源是被锁住的。

**3. 数据一致性问题**

前面已经提到，两段式提交的成立是有前提条件的，当**网络稳定性**和**宕机恢复能力**的假设不成立时，两阶段提交可能会出现一致性问题。

对于**宕机恢复能力**这一点无需多说。1985 年 Fischer、Lynch、Paterson 用定理（被称为FLP 不可能原理，在分布式中与 CAP 定理齐名）证明了如果宕机最后不能恢复，那就不存在任何一种分布式协议可以正确地达成一致性结果。

对于**网络稳定性**来说，尽管提交阶段时间很短，但仍是明确存在的危险期。如果协调者在发出准备指令后，根据各个参与者发回的信息确定事务状态是可以提交的，协调者就会先持久化事务状态，并提交自己的事务。如果这时候网络忽然断开了，无法再通过网络向所有参与者发出 Commit 指令的话，就会导致部分数据（协调者的）已提交，但部分数据（参与者的）既未提交也没办法回滚，导致数据不一致。

### 2.2 **3PC-三阶段提交**

为了解决两段式提交的单点故障问题、同步阻塞问题和数据一致性问题，**“三段式提交”（3 Phase Commit，3PC）**协议出现了。

> Dale Skeen在“NonBlocking Commit Protocols” (1981)中指出，对于一个分布式系统，需要3阶段的提交算法来避免2PC中的阻塞问题

与两阶段提交不同的是，三阶段提交有两个改动点。

- **引入超时机制**: 同时在协调者和参与者中都引入超时机制。
- **把原本的2PC的准备阶段再细分为两个阶段**: 将准备阶段一分为二的理由是，这个阶段是重负载的操作，一旦协调者发出开始准备的消息，每个参与者都将马上开始写重做日志，这时候涉及的数据资源都会被锁住。如果此时某一个参与者无法完成提交，相当于所有的参与者都做了一轮无用功。所以，增加一轮询问阶段，如果都得到了正面的响应，那事务能够成功提交的把握就比较大了，也意味着因某个参与者提交时发生崩溃而导致全部回滚的风险相对变小了。

三个阶段分别为：

***第一阶段：CanCommit阶段\***

3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。

***第二阶段：PreCommit阶段\***

本阶段协调者会根据第一阶段的询盘结果采取相应操作，询盘结果主要有两种：

- 情况1-假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行：
- 情况2-假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。

***第三阶段：doCommit阶段\***

该阶段进行真正的事务提交，也可以分为以下两种情况。

- 情况1-执行提交:针对第一种情况，协调者向各个参与者发起事务提交请求
- 情况2-中断事务:协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。

三段式提交的操作时序如下图所示:

![img](https://pic4.zhimg.com/80/v2-45bbd31f3baff755444e8e7c252f31fb_720w.webp)

可以看出，**3PC可以解决单点故障问题，并减少阻塞**，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit，而不会一直持有事务资源并处于阻塞状态。

但是**3PC对于数据一致性问题并未有任何改进**，比如在进入PreCommit阶段后，如果协调者发送的是abort指令，而此时由于网络问题，有部分参与者在等待超时后仍未收到Abort指令的话，那这些参与者就会执行commit，这样就产生了不同参与者之间数据不一致的问题。

**由于3PC非常难实现，目前市面上主流的分布式事务解决方案都是2PC协议**。

### 2.3 **XA协议**

2PC 的传统方案是在数据库层面实现的，如 Oracle、MySQL 都支持 2PC 协议，为了统一标准减少行业内不必要的对接成本，需要制定标准化的处理模型及接口标准，国际开放标准组织Open Group 在1994年定义了**分布式事务处理模型 DTP（Distributed Transaction Processing Reference Model）** 。

DTP 规范中主要包含了 AP、RM、TM 三个部分，如下图所示：

![img](https://pic2.zhimg.com/80/v2-12101f2d59370c49664b1c0c2dab8889_720w.webp)

它定义了三大组件：

- **AP(\*Application Program\*)**：应用程序，一般指事务的**发起者**（比如数据库客户端或者访问数据库的程序），定义事务对应的操作（比如更新操作 UPDATE table SET xxx WHERE xxx）
- **RMs(\*Resource Managers\*)**：资源管理器，是分布式事务的**参与者**，管理共享资源，并提供访问接口，供外部程序来访问共享资源，比如数据库、打印服务等，另外 RM 还应该具有事务提交或回滚的能力。
- **TM(\*Transaction Manager\*)**：事务管理器，是分布式事务的**协调者**，管理全局事务，与每个RM进行通信，协调事务的提交和回滚，并协助进行故障恢复。

其中XA约定了TM和RM之间双向通讯的接口规范，并实现了二阶段提交协议，从而在多个数据库资源下保证 ACID 四个特性。所以，DTP模型可以理解为：应用程序访问、使用RM的资源，并通过TM的事务接口（TX interface）定义需要执行的事务操作，然后TM和RM会基于 XA 规范，执行二阶段提交协议进行事务的提交/回滚：

- **准备阶段**：TM 向所有与当前事务相关的 RM 发起准备请求，每个 RM 评估自身是否能正常进行提交，并在响应中告知 TM。这类似于一个 TM 发起投票，各个 RM 进行投票的过程。
- **提交阶段**：在第一阶段中，如果所有 RM 能在一定时间内表示 OK，则第二阶段由 TM 向所有 RM 发起提交请求。有任何一个 RM 没有准备好，TM 则向所有 RM 发起回滚请求。

目前大多数实现XA的都是一些关系型数据库（包括PostgreSQL，MySQL，DB2，SQL Server和Oracle）和消息中间件（包括ActiveMQ，HornetQ，MSMQ和IBM MQ），所以**提起XA往往指基于资源层的底层分布式事务解决方案**。

> MySQL 从5.0.3开始支持XA分布式事务(只有InnoDB引擎才支持XA事务)，业务开发人员在编写代码时，不应该直接操作这些XA事务操作的接口。因为在DTP模型中，RM上的事务分支的开启、结束、准备、提交、回滚等操作，都应该是由事务管理器TM来统一管理。

XA是资源层面的分布式事务，强一致性，在两阶段提交的整个过程中，一直会持有资源的锁。基于两阶段提交的分布式事务在提交事务时需要在多个节点之间进行协调,最大限度延后了提交事务的时间点，客观上延长了事务的执行时间，这会导致事务在访问共享资源时发生冲突和死锁的概率增高。因此，**XA并发性能不理想**（使用 XA 协议的 MySQL 集群，操作延时是单机的 10 倍），**无法满足高并发场景，在互联网中使用较少**。

## 3.**最终一致性方案**

上面所述的强一致性方案在性能上都不理想，在CAP定理中属于CP范畴；在互联网应用中，为了提升性能和可用性，基于BASE理论使用最终一致性来替代强一致性，也就是通过牺牲部分一致性来换取性能和可用性的提升。

### 3.1 **本地事务状态表**

本地事务状态表方案的大概处理流程是：

1. 在调用方请求外部系统前将待执行的事务流程及其状态信息存储到数据库中，依赖数据库本地事务的原子特性保证本地事务和调用外部系统事务的一致性，这个存储事务执行状态信息的表称为本地事务状态表。
2. 在将事务状态信息存储到DB后，调用方才会开始继续后面流程，同步调用外部系统，并且每次调用成功后会更新相应的子事务状态，某一步失败时则中止执行。
3. 同时在后台运行一个定时任务，定期扫描事务状态表中未完成的子事务，并重新发起调用，或者执行回滚，或者在失败重试指定次数后触发告警让人工介入进行修复。

如下图所示：

![img](https://pic2.zhimg.com/80/v2-f25ae78ab3882feedaeea1d0b86e14d1_720w.webp)

其中本地事务表的设计由业务方自己来定，可以是如上图中所示拆分为多个子事务来管理，简单点也可以只有一条记录，然后通过状态的流转来控制程序调用不同的外部系统。

### 3.2 **可靠消息队列**

可靠消息队列方案是指当事务发起方执行完成本地事务后并发出一条消息，事务参与方（消息消费者）一定能够接收到消息并处理事务成功，此方案强调的是只要消息发给事务参与方，则最终事务要达到一致。

此方案是利用消息中间件完成，如下图：

![img](https://pic3.zhimg.com/80/v2-e2cf0d91d6015779c44290b513447146_720w.webp)

事务发起方（消息生产方）将消息发给消息中间件，事务参与方从消息中间件接收消息，事务发起方和消息中间件之间，事务参与方（消息消费方）和消息中间件之间都是通过网络通信，由于网络通信的不确定性会导致分布式事务问题。

因此可靠消息最终一致性方案要解决以下几个问题：

1. **本地事务与消息发送的原子性问题**：要求事务发起方在本地事务执行成功后消息必须发出去，否则就丢弃消息
2. **事务参与方接收消息的可靠性**：要求事务参与方必须能够从消息队列接收到消息，如果接收消息失败可以重复接收消息
3. **消息重复消费的问题**：要解决消息重复消费的问题就要实现事务参与方的方法幂等性。

目前主要的解决方案有2种，一种是本地消息表方案，一种是事务消息方案。

***本地消息表***

如果是使用 Kafka(< 0.11.0) 这类不支持事务消息的消息中间件，参与事务的系统需要在给消息中间件发送消息之前，把消息的信息和状态存储到本地的消息表中，如下图所示：

![img](https://pic4.zhimg.com/80/v2-68216b206589b140f56e479e334f68af_720w.webp)

主要流程如下：

1. 参与分布式事务的系统A接收到请求后，在执行本地事务的同时把将待发送的消息记录到事务消息表中，将业务表和消息表放在一个数据库事务里，保证两者的原子性；
2. 执行完后系统A不直接给消息中间件发消息，而是通过后台定时任务扫描消息表将消息push到消息中间件，对于push失败的消息则会不断重试，直到消息中间件成功返回 ack 消息，并更细消息表中投递状态，从而保证消息的不丢失。
3. 消息中间件收到消息后，会将消息投递给订阅消息的外部系统1/2/3，外部系统1/2/3收到消息后执行本地事务，只有成功才应答 Ack 消息，消息中间件只有在收到Ack消息后才将该条消息丢弃，否则会不断的重复发送直到成功，所以事务的所有参与者需要自行保证事务执行的幂等性。

***事务消息方案\***

如果是基于RocketMQ或Kafka（>=0.11.0）这类的支持事务操作的消息中间件，上述的方案则可以简化，此时上面的的定时任务的工作将交给消息中间件来提供，如下图所示：

![img](https://pic4.zhimg.com/80/v2-5c0b0cec130478842c0c05cce24924f3_720w.webp)

流程比较简单不再赘述，需要说明的是，消息中间件如果收到Comfirm消息，则会将消息转为对消费者可见，并开始投递；如果收到Rollback消息，则会删除之前的事务消息；如果未收到确认消息，则会通过事务回查机制定时检查本地事务的状态，决定是否可以提交投递。

### 3.3 **最大努力通知**

最大努力通知方案( Best-effort delivery)是最简单的一种柔性事务，**适用于一些最终一致性时间敏感度低的业务，且被动方处理结果不影响主动方的处理结果**。典型的使用场景：如银行通知、商户通知等。最大努力通知型的实现方案，一般符合以下特点：

- **不可靠消息**：业务活动主动方，在完成业务处理之后，向业务活动的被动方发送消息，直到通知N次后不再通知，允许消息丢失(不可靠消息)。
- **定期校对**：业务活动的被动方，根据定时策略，向业务活动主动方查询(主动方提供查询接口)，恢复丢失的业务消息。

比如充值的一个例子：

![img](https://pic1.zhimg.com/80/v2-587b72117d8811ea2de7f820ee5f11d0_720w.webp)

**与可靠消息队列方案区别：**

- **解决方案思想不同**

可靠消息一致性，发起通知方需要保证将消息发出去，并且将消息发到接收通知方，消息的可靠性关键由发起通知方来保证。最大努力通知，发起通知方尽最大的努力将业务处理结果通知为接收通知方，但是可能消息接收不到，此时需要接 收通知方主动调用发起通知方的接口查询业务处理结果，通知的可靠性关键在接收通知方。

- **两者的业务应用场景不同**

可靠消息一致性关注的是交易过程的事务一致，以异步的方式完成交易。最大努力通知关注的是交易后的通知事务，即将交易结果可靠的通知出去。

- **技术解决方向不同**

可靠消息一致性要解决消息从发出到接收的一致性，即消息发出并且被接收到。最大努力通知无法保证消息从发出到接收的一致性，只提供消息接收的可靠性机制。可靠机制是，最大努力的将消息通知给接收方，当消息无法被接收方接收时，由接收方主动查询消息（业务处理结果）。

### 3.4 **TCC**

前面介绍的可靠消息队列方案能保证最终的结果是相对可靠的，过程也足够简单，但是可靠消息队列的整个实现过程完全没有任何隔离性可言。虽然在有些业务中，有没有隔离性不是很重要，比如说搜索系统。

但在有些业务中，一旦缺乏了隔离性，就会带来许多麻烦，比如下面一个简化版的订销存交易流程：

![img](https://pic1.zhimg.com/80/v2-d478741c39410376ebae8703e6583784_720w.webp)

用户在电商网站下订单后通知库存服务扣减粗存，最后通过积分服务给用户增加积分。整个交易操作应该具有原子性，这些交易步骤要么一起成功，要么一起失败，必须是一个整体性的事务。

假设用户下完订单通知库存服务扣减库存失败时，比如原本是10件商品卖了1件剩余9件，但由于库存DB操作失败，导致库存还是10件，这时就出现了数据不一致的情况，此时如果有其它用户也进行了购买操作，则可能**出现超卖的问题**。

如果采用2PC的解决方案，在整个交易成功完成或者失败回滚之前，其它用户的操作将会处于阻塞等待的状态，这会大大的降低系统的性能和用户体验。

如果业务需要隔离，通常就应该重点考虑 TCC（Try-Confirm-Cancel）方案，**TCC天生适用于需要强隔离性的分布式事务**中，它是由数据库专家帕特 · 赫兰德（Pat Helland）在 2007 年撰写的论文《Life beyond Distributed Transactions: An Apostate’s Opinion》中提出的。

在具体实现上，TCC 的操作其实有点儿麻烦和复杂，它是一种业务侵入性较强的事务方案，要求业务处理过程必须拆分为“预留业务资源”和“确认 / 释放消费资源”两个子过程。另外，你看名字也能看出来，TCC 的实现过程分为了三个阶段：

- **Try：尝试执行阶段，**完成所有业务可执行性的检查（保障一致性），并且预留好事务需要用到的所有业务资源（保障隔离性）。
- **Confirm：确认执行阶段**，不进行任何业务检查，直接使用 Try 阶段准备的资源来完成业务处理。注意，Confirm 阶段可能会重复执行，因此需要满足幂等性。
- **Cancel：取消执行阶段**，释放 Try 阶段预留的业务资源。注意，Cancel 阶段也可能会重复执行，因此也需要满足幂等性。

TCC是基于BASE理论的类2PC方案，根据业务的特性对2PC的流程进行了优化，与2PC的区别在一些步骤的细节上，如下图：

![img](https://pic3.zhimg.com/80/v2-7cd7a57082093328b3df2f84f52bdafe_720w.webp)

可以看出，不同于2PC第一阶段的Prepare，**TCC在Try阶段主要是对资源的预留操作这类的轻量级操作**，比如冻结部分库存数量，它不需要像2PC在第二阶段完成之后才释放整个资源，也就是它不需要等待整个事务完成后才进行提交，这时其它用户的购买操作可以继续正常进行，因此它的**阻塞范围小时间短暂，性能上比2PC方案要有很大的提升**。

另外，**TCC是位于用户代码层面**，而不是在基础设施层面，这为它的实现带来了较高的灵活性，**可以根据需要设计资源锁定的粒度**。TCC 在业务执行时只操作预留资源，**几乎不会涉及锁和资源的争用，具有很高的性能潜力。**

但是 TCC要求所有的事务参与方都必须要提供三个操作接口：Try/Confirm/Cancel，**带来了更高的开发成本和业务侵入性，意味着有更高的开发成本和更换事务实现方案的替换成本**，特别是对一些难以改动的老旧系统来说甚至是不可行的。

### 3.5 **SAGA事务**

SAGA 事务模式的历史十分悠久，比分布式事务的概念提出还要更早。SAGA 的意思是“长篇故事、长篇记叙、一长串事件”，它起源于 1987 年普林斯顿大学的赫克托 · 加西亚 · 莫利纳（Hector Garcia Molina）和肯尼斯 · 麦克米伦（Kenneth Salem）在 ACM 发表的一篇论文《SAGAS》。

文中提出了一种如何提升“长时间事务”（Long Lived Transaction）运作效率的方法，大致思路是把一个大事务分解为可以交错运行的一系列子事务的集合。**原本提出 SAGA 的目的，是为了避免大事务长时间锁定数据库的资源，后来才逐渐发展成将一个分布式环境中的大事务，分解为一系列本地事务的设计模式**。

Saga 事务基本协议如下：

- 每个 Saga 事务由一系列幂等的有序子事务(sub-transaction) T1，T2，…，Ti，…，Tn组成。
- 每个 Ti 都有对应的幂等补偿动作C1，C2，…，Ci，…，Cn，补偿动作用于撤销 T1，T2，…，Ti，…，Tn造成的结果。

如果 T1 到 Tn 均成功提交，那么事务就可以顺利完成。否则，就要采取恢复策略，恢复策略分为向前恢复和向后恢复两种。

***向前恢复（Forward Recovery）\***

如果 Ti 事务提交失败，则一直对 Ti 进行重试，直至成功为止（最大努力交付）。这种恢复方式不需要补偿，适用于事务最终都要成功的场景，比如在别人的银行账号中扣了款，就一定要给别人发货。正向恢复的执行模式为：T1，T2，…，Ti（失败），Ti（重试）…，Ti+1，…，Tn，**该情况下不需要Ci**。

![img](https://pic1.zhimg.com/80/v2-6216a8bc00292b7a4f235e2fb0f22ac4_720w.webp)

***向后恢复（Backward Recovery）***

如果 Ti 事务提交失败，则一直执行 Ci 对 Ti 进行补偿，直至成功为止（最大努力交付）。这里要求 Ci 必须（在持续重试后）执行成功。**向后恢复的执行模式为：T1，T2，…，Ti（失败），Ci（补偿），…，C2，C1**。

![img](https://pic2.zhimg.com/80/v2-7516365644f71e79e0de19f436bf28b1_720w.webp)

Saga 事务常见的有两种不同的实现方式。

***命令协调模式\***

这种模式由中央协调器（Orchestrator，简称 OSO）集中处理事件的决策和业务逻辑排序，以命令/回复的方式与每项服务进行通信，全权负责告诉每个参与者该做什么以及什么时候该做什么。

![img](https://pic3.zhimg.com/80/v2-5c5d3165d73837fb4d3b67d3c620494e_720w.webp)

以电商订单的例子为例：

1. 事务发起方的主业务逻辑请求 OSO 服务开启订单事务。
2. OSO 向库存服务请求扣减库存，库存服务回复处理结果。
3. OSO 向订单服务请求创建订单，订单服务回复创建结果。
4. OSO 向支付服务请求支付，支付服务回复处理结果。
5. 主业务逻辑接收并处理 OSO 事务处理结果回复。

中央协调器必须事先知道执行整个订单事务所需的流程(例如通过读取配置)。如果有任何失败，它还负责通过向每个参与者发送命令来撤销之前的操作来协调分布式的回滚。

基于中央协调器协调一切时，回滚要容易得多，因为协调器默认是执行正向流程，回滚时只要执行反向流程即可。

***事件编排模式\***

这种模式没有中央协调器（没有单点风险），由每个服务产生并观察其他服务的事件，并决定是否应采取行动。

在事件编排方法中，第一个服务执行一个事务，然后发布一个事件。该事件被一个或多个服务进行监听，这些服务再执行本地事务并发布（或不发布）新的事件。

当最后一个服务执行本地事务并且不发布任何事件时，意味着分布式事务结束，或者它发布的事件没有被任何 Saga 参与者听到都意味着事务结束。

![img](https://pic2.zhimg.com/80/v2-fbc95ec2c29c4fcd01062ebe44a0c11d_720w.webp)

电商订单的例子为例：

1. 事务发起方的主业务逻辑发布开始订单事件。
2. 库存服务监听开始订单事件，扣减库存，并发布库存已扣减事件。
3. 订单服务监听库存已扣减事件，创建订单，并发布订单已创建事件。
4. 支付服务监听订单已创建事件，进行支付，并发布订单已支付事件。
5. 主业务逻辑监听订单已支付事件并处理。

事件/编排是实现 Saga 模式的自然方式，它很简单，容易理解，不需要太多的代码来构建。如果事务涉及 2 至 4 个步骤，则可能是非常合适的。

**SAGA的适用场景主要是以下几种：**

- 业务流程长、业务流程多
- 参与者包含第三方或遗留系统服务，无法提供TCC模式要求的三个接口
- 典型业务系统：如金融网络（与外部金融机构对接）、互联网微贷、渠道整合、分布式架构服务集成等业务系统
- 银行业金融机构使用广泛

**SAGA优势主要体现在：**

- 一阶段提交本地数据库事务，无锁，高性能；
- 参与者可以采用事务驱动异步执行，高吞吐；
- 补偿服务即正向服务的“反向”，易于理解，易于实现；

**但是**Saga 模式由于一阶段已经提交本地数据库事务，且没有进行“预留”动作，所以**不能保证隔离性**。

## 4.**弱一致性方案**

前面最终一致性方案基本能满足大多数的场景，但在一些场景下，我们对系统的性能和可用性有更高的要求。比如海量请求的高并发秒杀场景中，如何保证服务的高可用是个很大的挑战，除了要对秒杀的非核心功能进行降级、增加响应时间外，根据CAP定理，还需要对对一致性的再进行妥协，从最终一致性弱化到弱一致性。

**弱一致性是指数据更新后，容忍后续只能访问到部分或者全部访问不到(也不承诺多久可以访问到)，并且不会对业务产生重大影响**。下面介绍的几个方案都是根据自身业务特点做的妥协，不是严格意义上完备的技术方案，而是一种解决思路，是适合业务自身特点、满足性能要求、满足成本要求或技术架构要求下的一种解决思路，仅供参考。

### 4.1 基于状态的补偿

这是一个根据业务特性进行妥协的一种方案，根据实际的业务场景对立面的数据重要性进行划分，放弃传统的全局数据一致，允许部分不重要的数据出现不一致，但不会对业务产生重大影响。

比如在电商网站购物场景中，其中两个主要的步骤是创建订单和扣库存，这分别由两个服务进行处理：订单服务和库存服务。

- 如果采用前面可靠消息队列方案，创建订单的消息通知库存服务扣除库存，由于异步消息的延迟则会导致超卖；
- 如果采用TCC的方案，每次请求操作都需要Try、Confirm两次请求调用，性能又不能达标；
- 如果采用本地事务状态表，则需要对海量的事务进行状态更新操作，性能和延迟也同样会是个问题。

但是我们可以依据实际的电商购物场景进行取舍：**允许少卖，但不能超卖**。于是我们可以先扣库存，库存扣减成功后才创建订单并关联库存，若扣库存失败则不创建订单。有以下几种情况：

|      | 扣库存 | 创建订单 | 返回结果 | 可能结果       |
| ---- | ------ | -------- | -------- | -------------- |
| 1    | √      | √        | √        | 正常           |
| 2    | √      | ×        | ×        | 多扣库存，少卖 |
| 3    | ×      | ×        | ×        | 下单失败       |

对于第2种情况，会出现多扣库存的情况，这时可以**基于状态进行补偿，就不会出现超卖的问题了：**根据库存流水记录查找那些一段时间内未关联订单的库存记录进行撤销操作。这个和我们在12306上的买车票，如果30分钟内未支付的话车票会被释放，是一个道理。

这是一种事后处理机制，即使补偿失败，也不会有严重后果，对业务来说也是可接受的，大不了手工重新上架。

### 4.2 **事后对账**

**对于那些业务流程复杂，涉及外部服务比较多，并且需要维护的状态也很复杂的场景**，就很难根据状态进行自动补偿，这时可以进一步简化操作：**不做自动的状态补偿**。

还是拿上面那个订单和库存的例子进行说明，比如先扣库存，然后创建订单，如果订单创建失败则重试，重试还是失败则回滚，回滚失败则触发告警，然后由脚本对业务数据自动进行对账，并对异常数据进行修复。

对账的关键是找出数据的特征，有些好找，有些难，但是它的基本要求是数据记录是“完备”的，然后研发人员根据经验，对不同特征的数据执行不同的修复，对于常见的问题可能会有一些修复脚本来辅助处理。

事后对账一般会根据业务特点设计自动对账脚本，实现对业务数据的自动检查，发现业务中可能存在的问题（比如异常、假账）等，然后触发执行对应的动作，至少是要有告警，通知研发同学介入，如果做的更好一点的话，可以对特定类型的异常数据自动进行修复，减少人工成本。

## 5.**总结**

**强一致性方案主要用于对数据一致性要求比较高的场景**，比如金融银行等，且大多是在数据库层面实现，然后业务方直接使用；

**在常规的互联网应用中，对性能和可用性要求更高，可以采用基于BASE理论的最终一致性方案**；

**弱一致性方案需要容忍数据的部分不一致，主要用于一些极端的场景**中，比如高并发秒杀场景。

各个方案的特点总结如下：

- **2PC/3PC**：依赖于数据库，能够很好的提供强一致性和强事务性，但相对来说延迟比较高，比较适合传统的单体应用，不适合高并发和高性能要求的场景。
- **XA协议**：基于XA协议的强一致事务使用起来相对简单，但是无法很好地应对互联网的短事务和高并发场景。
- **本地事务状态表**：方案轻量，容易实现，但与具体的业务场景耦合较高，不可公用。
- **可靠消息队列**：适合执行周期长且实时性要求不高的场景。引入消息机制后，同步的事务操作变为基于消息执行的异步操作, 避免了分布式事务中的同步阻塞操作的影响，并实现了两个服务的解耦。典型的使用场景：注册送积分，登录送优惠券等。
- **最大努力通知**：是分布式事务中要求最低的一种,适用于一些最终一致性时间敏感度低的业务；允许发起通知方处理业务失败，在接收通知方收到通知后积极进行失败处理，无论发起通知方如何处理结果都会不影响到接收通知方的后续处理；发起通知方需提供查询执行情况接口，用于接收通知方校对结果。典型的使用场景：银行通知、支付结果通知等。
- **TCC**：适用于执行时间确定且较短，实时性要求高，对数据一致性要求高，比如互联网金融企业最核心的三个服务：交易、支付、账务。但是对于业务的侵入性非常强，业务逻辑的每个分支都需要实现try、confirm、cancel三个操作。此外，其实现难度也比较大，需要按照网络状态、系统故障等不同的失败原因实现不同的回滚策略。
- **SAGA**：适合于“业务流程长、业务流程多”的场景。特别是针对参与事务的服务是遗留系统服务。但由于 Saga 事务不能保证隔离性，需要在业务层控制并发，适合于业务场景事务并发操作同一资源较少的情况。另外， Saga 相比缺少预提交动作，导致补偿动作的实现比较麻烦，例如业务是发送短信，补偿动作则得再发送一次短信说明撤销，用户体验比较差。Saga 事务较适用于补偿动作容易处理的场景。
- **弱一致性方案**：上面给出的几种弱一致性方案是在高并发等场景下，为了提高系统的性能和可用性而在一致性方面做的妥协，一般需要结合具体的业务特点、实现成本等各方因素对一些最终一致性方案做改造。

**总之，分布式事务没有能一揽子包治百病的解决方案，只有因地制宜地选用适合自己的，才是唯一有效的做法。**

**以下是在网上看到的一些技术大佬的经验之谈，共勉之：**

- 实际运用理论时进行架构设计时，许多人容易犯“手里有了锤子，看什么都觉得像钉子”的错误，设计方案时考虑的问题场景过多，各种重试，各种补偿机制引入系统，导致系统过于复杂，落地遥遥无期。世界上解决一个计算机问题最简单的方法：“恰好”不需要解决它！
- 有些问题，看起来很重要，但实际上我们可以通过合理的设计或者将问题分解来规避。
- 设计分布式事务系统也不是需要考虑所有异常情况，不必过度设计各种回滚，补偿机制。
- 如果硬要把时间花在解决问题本身，实际上不仅效率低下，而且也是一种浪费。
- 如果系统要实现回滚流程的话，有可能系统复杂度将大大提升，且很容易出现 Bug，估计出现 Bug 的概率会比需要事务回滚的概率大很多。
- 在设计系统时，我们需要衡量是否值得花这么大的代价来解决这样一个出现概率非常小的问题，可以考虑当出现这个概率很小的问题，能否采用人工解决的方式，这也是大家在解决疑难问题时需要多多思考的地方。

## 6.开源框架

目前我所了解的分布式事务解决方案的开源框架主要是Seata和Dtm，但都未实际使用过，所以没有发言权，在此只是列出来各自框架的主要特点，仅供参考。

### 6.1 **Seata**

Seata（Simple Extensible Autonomous Transaction Architecture，一站式分布式事务解决方案）是 2019 年 1 月份蚂蚁金服和阿里巴巴共同开源的分布式事务解决方案([https://github.com/seata/seata](https://link.zhihu.com/?target=https%3A//github.com/seata/seata))。目前start数有21k。

如下图所示，Seata 中有三大模块，分别是 TM、RM 和 TC。 其中 TM 和 RM 是作为 Seata 的客户端与业务系统集成在一起，TC 作为 Seata 的服务端独立部署。

![img](https://pic1.zhimg.com/80/v2-590e500ac060c6a07c84af74a7266448_720w.webp)

在 Seata 中，分布式事务的执行流程：

- TM 开启分布式事务（TM 向 TC 注册全局事务记录）；
- 按业务场景，编排数据库、服务等事务内资源（RM 向 TC 汇报资源准备状态 ）；
- TM 结束分布式事务，事务一阶段结束（TM 通知 TC 提交/回滚分布式事务）；
- TC 汇总事务信息，决定分布式事务是提交还是回滚；
- TC 通知所有 RM 提交/回滚 资源，事务二阶段结束；

Seata 会有 4 种分布式事务解决方案，分别是 **AT 模式、TCC 模式、Saga 模式和 XA 模式**。

### 6.2 DTM

DTM是一款golang开发的分布式事务管理器([https://github.com/dtm-labs/dtm](https://link.zhihu.com/?target=https%3A//github.com/dtm-labs/dtm))，目前star数4.6k，它解决了跨数据库、跨服务、跨语言栈更新数据的一致性问题。他优雅的解决了幂等、空补偿、悬挂等分布式事务难题，提供了简单易用、高性能、易水平扩展的解决方案。

**亮点如下：**

- **极易接入**：零配置启动服务，提供非常简单的HTTP接口，极大降低上手分布式事务的难度，新手也能快速接入
- **跨语言**：可适合多语言栈的公司使用。方便go、python、php、nodejs、ruby、c# 各类语言使用。
- **使用简单**：开发者不再担心悬挂、空补偿、幂等各类问题，首创子事务屏障技术代为处理
- **易部署、易扩展**：依赖mysql|redis，部署简单，易集群化，易水平扩展
- **多种分布式事务协议支持**：TCC、SAGA、XA、二阶段消息，一站式解决所有分布式事务问题

**与其他框架对比**(非Java语言类的，暂未看到除dtm之外的成熟框架，因此这里将DTM和Java中最成熟的Seata对比)：

![img](https://pic2.zhimg.com/80/v2-a1ffa3650d043e6737747e5f11af8ab9_720w.webp)

## 7.**参考文档**

[分布式事务 Seata Saga 模式首秀以及三种模式详解 | Meetup#3 回顾 · SOFAStack](https://link.zhihu.com/?target=https%3A//www.sofastack.tech/blog/sofa-meetup-3-seata-retrospect/)

[10分钟说透Saga分布式事务 - 云+社区 - 腾讯云 (tencent.com)](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/developer/article/1839642)

[“分布式事务各大方案详细解读2PC、3PC、TCC、本地消息表、MQ事务、Saga”丶Java教程网-IT开发者们的技术天堂 (liangzl.com)](https://link.zhihu.com/?target=https%3A//www.liangzl.com/get-article-detail-199811.html)

[A brief history of Consensus, 2PC and Transaction – duanple](https://link.zhihu.com/?target=http%3A//duanple.com/%3Fp%3D176)

[分布式系统一致性的发展历史 (二) (danielw.cn)](https://link.zhihu.com/?target=https%3A//danielw.cn/history-of-distributed-systems-2)

[全局事务 | 凤凰架构 (icyfenix.cn)](https://link.zhihu.com/?target=http%3A//icyfenix.cn/architect-perspective/general-architecture/transaction/global.html)

[分布式事物之 xa协议两阶段提交 - 云+社区 - 腾讯云 (tencent.com)](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/developer/news/284586)

[聊聊事务与分布式系统-从零讲到通透 - InfoQ 写作平台](https://link.zhihu.com/?target=https%3A//xie.infoq.cn/article/1d7b352523b5bcd2ce2248a11)



欢迎点赞分享，关注[@鹅厂架构师](https://www.zhihu.com/org/e-han-jia-gou-shi)，一起探索更多业界领先产品技术。

原文作者：鹅厂架构师

原文链接：https://zhuanlan.zhihu.com/p/461294722

# 【NO.247】MySQL WriteSet并行复制分析

> 以下内容首发于腾讯数据库技术（公众号）

## 0.背景

在mysql支持基于LOGICAL CLOCK的复制后，主从延迟得到了很大的改善，但是LOGICAL CLOCK一定程度上会受到master的并发度的影响。当master的并发度较低，每次组提交的事务数较少的时候，binlog在slave上的回放的并发度也会因此而降低，即使这些事务之间并没有任何冲突。示例：

```text
Trx1 -----L----C---------------------------------->
Trx2 ---------------L----C------------------------>
Trx3 ------------------------L----C--------------->
Trx4 --------------------------------L----C------->
```

假定这些事务之间并没有任何冲突，由于它们的LOGICAL CLOCK周期没有重叠，在slave上也只能串行回放。为了解决这个问题，mysql8.0.1引入了基于WriteSet的复制。

## 1. 原理

LOGICAL CLOCK由两部分组成，分别是commit parent和sequence number。commit parent表示当前事务所依赖的事务的序号，只有依赖的事务完成后当前事务才能进行。WriteSet是一种更细粒度的事务冲突检测手段，它是在LOGICAL CLOCK的基础上，对事务的commit parent进行处理。例如，有如下两个事务：

```text
Trx1 -----L----C------------->
Trx2 ---------------L----C--->
```

假定这两个事务没有任何冲突，但是他们的LOGICAL CLOCK区间没有重叠，原本不能进行并行回放，经过WriteSet处理后，这两个事务的commit parent可能会被修改，让LOGICAL CLOCK区间有可能重叠，使并行回放成为可能。

WriteSet冲突检测的原理：

- 全局有一个数据结构(实际上使用std::map)维护了一定数量的行的hash值与修改行的事务的sequence number之间的映射。
- 一个事务会记录所修改行的hash值，在事务提交写入binlog的时候，遍历该事务修改的行的hash值，在全局的map中进行查找，如果有相同的hash值表明有两个事务修改了同一行，记录有冲突的sequence number，取最大的sequence number作为该事务的commit parent（如果没有任何冲突，则commit parent设置为map中最小的sequence number）。

更详细的示例：

![img](https://pic4.zhimg.com/80/v2-a5c0725a008ddc3d4aaeaa11de6fff63_720w.webp)

图中每一个方块代表一个事务，方块对应的区域代表事务影响的范围，如果有重叠则表示事务有冲突，每一个step代表一次组提交，T1-T8代表事务的执行顺序。如果不使用WriteSet，在slave上回放的时候的顺序：

```text
<T1,T2>, <T3>, <T4,T5>, <T6>, <T7,T8>
```

使用WriteSet后，在slave上的回放效果：

![img](https://pic4.zhimg.com/80/v2-1a70899f13293cef694e3755f859e543_720w.webp)

回放顺序：

```text
<T1,T2,T3>, <T4,T5,T6,T7>, <T8>
```

这里有一个地方需要注意，就是对于T2和T3，他们对应于同一个连接，然而在回放的时候却是并行的，可能导致事务的提交顺序不一致。解决这个问题有两种方法：

- slave_preserve_commit_order设置为on
- 使用writeset_session模式

writeset_session模式下同一个session的事务不能并发执行。它的原理很简单，在writeset的基础上，将事务的commit parent与当前session的last sequence number进行比较，取较大值作为新的commit parent。

## 2.源码分析

### **2.1 事务writeset更新**

MySQL 5.7.6引入了事务的写集合，在计算事务依赖的时候可以直接使用。在开启了gtid，且binlog_format为row格式，且transaction_write_set_extraction不为OFF的实例上，写binlog的时候会将事务所修改的行的hash值添加到事务的写集合中，对应的函数是binlog_log_row和add_pke(pke是primary key equivalent的缩写)。函数add_pke所做的事情：

- 对所修改的表的主键和唯一键的key各计算一个hash值（hash字符串由更新行的index，db，table，value按照一定规则拼接，这里不深究），放入到事务的写集合中（利用主键和unique key的唯一性来检测冲突和依赖）。
- 记录表的外键信息，如果表是某一些表的父表，调用Rpl_transaction_write_set_ctx::set_has_related_foreign_keys进行标记；如果表是某一些表的子表，外键列不为NULL，且foreign_key_checks不为0，也会对这样的外键列计算一个hash值。
- 如果没有添加任何hash值到写集合中，调用Rpl_transaction_write_set_ctx::set_has_missing_keys进行标记，说明记录因为某些原因没有计算hash值（比如有的表没有主键）。

需要注意的是，如果表没有人为定义主键（不包括innodb内部自动生成的主键）也没有定义非空唯一键，则不会计算任何hash值，即使表有其它索引。

### **2.2 事务依赖计算**

在函数MYSQL_BIN_LOG::write_transaction入口处会调用Transaction_dependency_tracker::get_dependency来获取事务的依赖（获取sequence number和commit parent）。函数Transaction_dependency_tracker::get_dependency会根据变量binlog_transaction_dependency_tracking来决定使用哪种方式计算事务的依赖：

```text
void Transaction_dependency_tracker::get_dependency(THD *thd,
                                                    int64 &sequence_number,
                                                    int64 &commit_parent) {
  sequence_number = commit_parent = 0;
  switch (m_opt_tracking_mode) {
    // 根据提交时的timestamp来决定依赖，COMMIT_ORDER是5.7引入的，这里不再深究
    case DEPENDENCY_TRACKING_COMMIT_ORDER:
      m_commit_order.get_dependency(thd, sequence_number, commit_parent);
      break;
    // 根据事务的写集合来决定依赖
    case DEPENDENCY_TRACKING_WRITESET:
      m_commit_order.get_dependency(thd, sequence_number, commit_parent);
      // writeset是在COMMIT_ORDER的基础上进行优化
      m_writeset.get_dependency(thd, sequence_number, commit_parent);
      break;
    // writeset_session，同一个session的事务不能并发执行
    case DEPENDENCY_TRACKING_WRITESET_SESSION:
      m_commit_order.get_dependency(thd, sequence_number, commit_parent);
      m_writeset.get_dependency(thd, sequence_number, commit_parent);
      // 在writeset的基础上对同一个session的事务做限制
      m_writeset_session.get_dependency(thd, sequence_number, commit_parent);
      break;
    default:
      assert(0);  // blow up on debug
      /*
        Fallback to commit order on production builds.
       */
      m_commit_order.get_dependency(thd, sequence_number, commit_parent);
  }
}
```

函数Writeset_trx_dependency_tracker::get_dependency根据写集合对事务的commit parent进行优化，关键代码：

```text
void Writeset_trx_dependency_tracker::get_dependency(THD *thd,
                                                     int64 &sequence_number,
                                                     int64 &commit_parent) {
  Rpl_transaction_write_set_ctx *write_set_ctx =
      thd->get_transaction()->get_transaction_write_set_ctx();
  std::vector<uint64> *writeset = write_set_ctx->get_write_set();
  // 检查是否能使用writeset
  bool can_use_writesets =
      // empty writeset implies DDL or similar, except if there are missing keys
      (writeset->size() != 0 || write_set_ctx->get_has_missing_keys() ||
       /*
         The empty transactions do not need to clear the writeset history, since
         they can be executed in parallel.
       */
       is_empty_transaction_in_binlog_cache(thd)) &&
      // hashing algorithm for the session must be the same as used by other
      // rows in history
      (global_system_variables.transaction_write_set_extraction ==
       thd->variables.transaction_write_set_extraction) &&
      // must not use foreign keys
      !write_set_ctx->get_has_related_foreign_keys() &&
      // it did not broke past the capacity already
      !write_set_ctx->was_write_set_limit_reached();
  bool exceeds_capacity = false;
  if (can_use_writesets) {
    /*
     Check if adding this transaction exceeds the capacity of the writeset
     history. If that happens, m_writeset_history will be cleared only after
     using its information for current transaction.
    */
    exceeds_capacity =
        m_writeset_history.size() + writeset->size() > m_opt_max_history_size;
    // 起始的parent值，history为空时为0，不为空时为当前history中最小的sequence
    // number
    int64 last_parent = m_writeset_history_start;
    // 遍历一个事务所有修改的行的hash值
    for (std::vector<uint64>::iterator it = writeset->begin();
         it != writeset->end(); ++it) {
      // 对每一个hash值都去history中寻找是否有对应的hash
      Writeset_history::iterator hst = m_writeset_history.find(*it);
      if (hst != m_writeset_history.end()) {
        // 如果一个行的hash存在于history中且对应的事务先于当前事务
        if (hst->second > last_parent && hst->second < sequence_number)
          // 修改当前事务所依赖的事务的sequence number
          last_parent = hst->second;
        // 标记该行由当前事务修改
        hst->second = sequence_number;
      } else {
        // 将hash值和事务的sequence number插入到history中
        if (!exceeds_capacity)
          m_writeset_history.insert(
              std::pair<uint64, int64>(*it, sequence_number));
      }
    }
    // 同时没有主键和非空唯一键的表不能使用writeset
    if (!write_set_ctx->get_has_missing_keys()) {
      // 当前事务所操作的table都有主键的前提下
      // 取last parent和commit parent中的较小值
      // 作为当前事务的commit parent (last_committed)
      commit_parent = std::min(last_parent, commit_parent);
    }
    if (exceeds_capacity || !can_use_writesets) {
      // 超过history最大值或者当前事务不能使用writeset则清空当前history
      m_writeset_history_start = sequence_number;
      m_writeset_history.clear();
    }
  }
}
```

使用writeset有一定的限制：

- DDL不可以使用writeset
- 当前session 的 hash 算法和 writeset history 必须相同
- 事务所更新的列不能被其它表引用
- 事务的写集合存放的hash值数量不能超过binlog_transaction_dependency_history_size设定的最大值
- 没有主键或者非空唯一键的表，事务的依赖获取会退化到COMMIT_ORDER的方式

函数Writeset_session_trx_dependency_tracker::get_dependency对同一个session的事务的commit parent做出限制：

```text
void Writeset_session_trx_dependency_tracker::get_dependency(
    THD *thd, int64 &sequence_number, int64 &commit_parent) {
  // 获取当前session提交的上一个事务的sequence number
  int64 session_parent = thd->rpl_thd_ctx.dependency_tracker_ctx()
                             .get_last_session_sequence_number();
  // 在commit parent和session parent中取较大值作为事务的commit parent
  if (session_parent != 0 && session_parent < sequence_number)
    commit_parent = std::max(commit_parent, session_parent);
  // 更新当前session的last sequence number
  thd->rpl_thd_ctx.dependency_tracker_ctx().set_last_session_sequence_number(
      sequence_number);
}
```

## 3.简易测试

测试数据由sysbench生成：10张表，每张表10万行。使用sysbench进行300s的read-write测试，然后让slave回放压测产生的binlog，slave回放并行度为8，记录回放时间。

从机每秒回放事务数对比图：

![img](https://pic4.zhimg.com/80/v2-6ce8174375a4a6221e0825c6ee7566c3_720w.webp)

可以看到，基于writeset的复制比基于commit_order的复制会快很多；当master并发度较低的时候，基于writeset_session的复制也会较慢，但比commit_order快；当并发度较高的时候writeset_session和writeset的复制速度比较接近，总体上writeset_session的速度要低于writeset。

## 4.总结

在LOGICAL CLOCK的基础上，根据事务的写集合将事务的依赖进一步细化，让事务在从机上的回放的并发度进一步提高。WriteSet主要适用于master上并发度不高的情况，如果主并发度较高或者主从没有延迟则不需要使用，因为WriteSet会带来额外的内存与CPU的消耗，在一些小的实例上可能会造成资源紧张。

## 5.参考

**MySQL :: WL#9556: Writeset-based MTS dependency tracking on master：***[https://dev.mysql.com/worklog/task/?id=9556](https://link.zhihu.com/?target=https%3A//dev.mysql.com/worklog/task/%3Fid%3D9556)*

**Improving the Parallel Applier with Writeset-based Dependency Tracking | MySQL High Availability：***[https://mysqlhighavailability.com/improving-the-parallel-applier-with-writeset-based-dependency-tracking](https://link.zhihu.com/?target=https%3A//mysqlhighavailability.com/improving-the-parallel-applier-with-writeset-based-dependency-tracking)*



**欢迎点赞分享，关注**[@鹅厂架构师](https://www.zhihu.com/org/e-han-jia-gou-shi)**，一起探索更多业界领先产品技术。**

原文作者：鹅厂架构师

原文链接：https://zhuanlan.zhihu.com/p/410720981

# 【NO.248】从一道数据库面试题彻谈MySQL加锁机制

以下内容来自于腾讯工程师joeycheng。

| 导语有一道关于数据库锁的面试题，发现其实很多DBA包括工作好几年的DBA都答的不太好，说明MySQL锁的机制其实还是比较复杂，值得深入研究。本文对3条简单的查询语句加锁情况进行分析，彻底搞清楚加锁细节。

首先来看这个面试题：
已知表t是innodb引擘，有主键：id（int类型) ，下面3条语句是否加锁？加锁的话，是什么锁？
\1. select * from t where id=X;
\2. begin;select * from t where id=X;
\3. begin;select * from t where id=X for update;

这里其实有很多依赖条件，并不能一开始就给出一个很确定的答复。我们一层层展开来分析。

## 1.MySQL有哪些锁？

![img](https://pic4.zhimg.com/80/v2-1c319cbb084c605470802c180b3dfdaf_720w.webp)

首先要知道MySQL有哪些锁，如上图所示，至少有12类锁（其中**自增锁**是事务向包含了AUTO_INCREMENT列的表中新增数据时会持有，**predicate locks for spatial index** 为空间索引专用，本文不讨论这2类锁）。

锁按**粒度**可分为分为**全局，表级，行级**3类。

### **1.1 全局锁**

对整个数据库实例加锁。
加锁表现：数据库处于只读状态，阻塞对数据的所有DML/DDL
加锁方式：**Flush tables with read lock** 释放锁：**unlock tables**(发生异常时会自动释放)
作用场景：全局锁主要用于做数据库实例的逻辑备份，与设置数据库只读命令**set global readonly=true**相比，全局锁在发生异常时会自动释放

### 1.2 表锁

对操作的整张表加锁， 锁定颗粒度大，资源消耗少，不会出现死锁，但会导致写入并发度低。具体又分为3类：
**1）显式表锁：**分为共享锁（S）和排他锁（X）
显示加锁方式：**lock tables ... read/write**
释放锁：**unlock tables**(连接中断也会自动释放)
**2）Metadata-Lock（元数据锁）**：MySQL5.5版本开始引入，主要功能是并发条件下，防止session1的查询事务未结束的情况下，session2对表结构进行修改，保护元数据的一致性。在session1持有 metadata-lock的情况下，session2处于等待状态：show processlist可见**Waiting for table metadata lock**。其具体加锁机制如下：

- DML->先加MDL 读锁（SHARED_READ，SHARED_WRITE）
- DDL->先加MDL 写锁（EXCLUSIVE）
- 读锁之间兼容
- 读写锁之间、写锁之间互斥

**3）Intention Locks（意向锁）**：**意向锁**为表锁（表示为IS或者IX），由存储引擎自己维护，用户无法干预。下面举一个例子说明其功能：
假设有2个事务：T1和T2
T1: 锁住表中的一行，只能读不能写（行级读锁）。
T2: 申请整个表的写锁（表级写锁）。
如T2申请成功，则能任意修改表中的一行，但这与T1持有的行锁是冲突的。故数据库应识别这种冲突，让T2的锁申请被阻塞，直到T1释放行锁。
有2种方法可以实现冲突检测：
\1. 判断表是否已被其它事务用表锁锁住。
\2. 判断表中的每一行是否已被行锁锁住。
其中2需要遍历整个表，**效率太低**。因此innodb使用意向锁来解决这个问题：
T1需要先申请表的**意向共享锁（IS）**，成功后再申请某一行的**记录锁S**。
在意向锁存在的情况下，上面的判断可以改为：
T2发现表上有意向共享锁IS，因此申请表的写锁被阻塞。


**1.3 行锁**

InnoDB引擘支持行级别锁，行锁粒度小，并发度高，但加锁开销大，也可能会出现死锁。
加锁机制：innodb行锁锁住的是索引页，回表时，主键的聚簇索引也会加上锁。

![img](https://pic2.zhimg.com/80/v2-d736695de3e65e17a52737669895e56d_720w.webp)

行锁具体类别如上图所示，包括：**Record lock/Gap Locks/Next-Key Locks**，每类又可分为**共享锁（S）**或者**排它锁（X）**，一共2*3=6类，最后还有1类插入意向锁：
**Record lock（记录锁）：**最简单的行锁，仅仅锁住一行。记录锁永远都是加在索引上的，即使一个表没有索引，InnoDB也会隐式的创建一个索引，并使用这个索引实施记录锁。
**Gap Locks（间隙锁）：**加在两个索引值之间的锁，或者加在第一个索引值之前，或最后一个索引值之后的间隙。使用间隙锁锁住的是一个区间，而不仅仅是这个区间中的每一条数据。间隙锁只阻止其他事务插入到间隙中，不阻止其它事务在同一个间隙上获得间隙锁，所以 gap x lock 和 gap s lock 有相同的作用。它是一个**左开右开**区间：如（1，3）
**Next-Key Locks：记录锁和间隙锁的组合，它指的是加在某条记录以及这条记录前面间隙上的锁。它是一个左开右闭**区间：如（1，3】
**Insert Intention（插入意向锁）**：该锁只会出现在insert操作执行前（并不是所有insert操作都会出现），目的是为了提高并发插入能力。它在插入一行记录操作之前设置一种特殊的间隙锁，多个事务在相同的索引间隙插入时，如果不是插入间隙中相同的位置就不需要互相等待。

**TIPS:**

1.不存在unlock tables … read/write，只有unlock tables
2.If a session begins a transaction, an implicit UNLOCK TABLES is performed

## 2.锁的兼容情况

引入意向锁后，表锁之间的兼容性情况如下表：

![img](https://pic2.zhimg.com/80/v2-eb3fda6550b257f3f9f101ed349de9c9_720w.webp)

总结：

1. 意向锁之间都兼容
2. X,IX和其它都不兼容（除了1）
3. S,IS和其它都兼容（除了1,2）

## 3.锁信息查看方式

- MySQL 5.6.16版本之前，需要建立一张特殊表innodb_lock_monitor，然后使用**show engine innodb status**查看

CREATE TABLE innodb_lock_monitor (a INT) ENGINE=INNODB;

DROP TABLE innodb_lock_monitor;

- MySQL 5.6.16版本之后，修改系统参数innodb_status_output后，使用**show engine innodb status**查看

set GLOBAL innodb_status_output=ON;

set GLOBAL innodb_status_output_locks=ON;

每15秒输出一次INNODB运行状态信息到错误日志

![img](https://pic4.zhimg.com/80/v2-64281e84309ecc4678faf76a9b6acc0b_720w.webp)

- MySQL5.7版本之后

可以通过information_schema.innodb_locks查看事务的锁情况，但只能看到阻塞事务的锁；如果事务并未被阻塞，则在该表中看不到该事务的锁情况

- MySQL8.0

删除information_schema.innodb_locks，添加performance_schema.data_locks，即使事务并未被阻塞，依然可以看到事务所持有的锁，同时通过performance_schema.table_handles、performance_schema.metadata_locks可以非常方便的看到元数据锁等表锁。

## 4.测试环境搭建

### 4.1 建立测试表

该表包含一个主键，一个唯一键和一个非唯一键：

CREATE TABLE `t` (

`id` int(11) NOT NULL,

`a` int(11) DEFAULT NULL,

`b` int(11) DEFAULT NULL,

`c` varchar(10),

PRIMARY KEY (`id`),

unique KEY `a` (`a`),

key b(b))

ENGINE=InnoDB;

### 4.2 写入测试数据

insert into t values(1,10,100,'a'),(3,30,300,'c'),(5,50,500,'e');

## 5.记录存在时的加锁

对于innodb引擘来说，加锁的2个决定因素：

1）当前的**事务隔离级别**
2）当前**记录是否存在**

假设id为3的记录存在，则在不同的4个隔离级别下3个语句的加锁情况汇总如下表(select 3表示 select * from t where id=3)：

| 隔离级别 | select 3 | begin;select 3                | begin;select 3 for update      |
| -------- | -------- | ----------------------------- | ------------------------------ |
| RU       | 无       | SHARED_READ                   | SHARED_WRITEIXX,REC_NOT_GAP：3 |
| RC       | 无       | SHARED_READ                   | SHARED_WRITEIXX,REC_NOT_GAP：3 |
| RR       | 无       | SHARED_READ                   | SHARED_WRITEIXX,REC_NOT_GAP：3 |
| Serial   | 无       | SHARED_READISS,REC_NOT_GAP：1 | SHARED_WRITEIXX,REC_NOT_GAP：3 |

分析：

1. 使用以下语句在4种隔离级别之间切换：
   set global transaction_isolation='READ-UNCOMMITTED';
   set global transaction_isolation='READ-COMMITTED';
   set global transaction_isolation='REPEATABLE-READ';
   set global transaction_isolation='Serializable';
2. 对于auto commit=true，select 没有显式开启事务（begin）的语句，元数据锁和行锁都不加，是真的“**读不加锁**”
3. 对于begin; select ... where id=3这种只读事务，会加**元数据锁SHARED_READ**，防止事务执行期间表结构变化，查询**performance_schema.metadata_locks**表可见此锁：

![img](https://pic4.zhimg.com/80/v2-09617f06b0066268a365d3d2b528e6b3_720w.webp)

4.对于begin; select ... where id=3这种只读事务，MySQL在RC和RR隔离级别下，使用MVCC快照读，不加行锁，但在Serial隔离级别下，读写互斥，会加**意向共享锁（表锁）**和**共享记录锁（行锁）**

5.对于begin; select ... where id=3 for update，会加**元数据锁SHARED_WRITE**

6.对于begin; select ... where id=3 or update，4种隔离级别都会加**意向排它锁（表锁）**和**排它记录锁（行锁）,**查询**performance_schema.data_locks**可见此2类锁

![img](https://pic4.zhimg.com/80/v2-f05caee858dd450aed01c78de8385f43_720w.webp)

6 记录不存在时的加锁

| 隔离级别 | select 2 | begin;select 2        | begin;select 2 for update |
| -------- | -------- | --------------------- | ------------------------- |
| RU       | 无       | SHARED_READ           | SHARED_WRITEIX            |
| RC       | 无       | SHARED_READ           | SHARED_WRITEIX            |
| RR       | 无       | SHARED_READ           | SHARED_WRITEIXX,GAP：3    |
| Serial   | 无       | SHARED_READISS,GAP：3 | SHARED_WRITEIXX,GAP：3    |

分析：

1. 当记录不存在的时候，RU和RC隔离级别只有意向锁，没有行锁了
2. RR，Serial隔离级别下，记录锁变成了**Gap Locks（间隙锁），**可以防止**幻读，**lock_data为3的GAP lock锁住区间（1，3），此时ID=2的记录插入会被阻塞。

![img](https://pic2.zhimg.com/80/v2-6bb1c557a252a61d90f2cb52722d8259_720w.webp)

![img](https://pic2.zhimg.com/80/v2-b35275724285a207814701d26d886f3d_720w.webp)

那么对于主键范围查询，唯一键查询，非唯一键查询，在不同隔离级别下又是如何加锁的呢？
《从一道数据库面试题彻谈MySQL加锁机制（下）》再做分享。

原文作者：鹅厂架构师

原文链接：https://zhuanlan.zhihu.com/p/566194828

# 【NO.249】从零开始学架构（下篇）

以下内容来自于腾讯PCG工程师Chaoweili

## 10.业务高可用

### 10.1 异地多活

不同地区都提供业务服务，一个地区故障（机器宕机、机房故障、网络故障），其他地区可用，故障地区用户可请求到其他地区进行服务。异地多活实现复杂成本高，并不是所有系统都无脑的实现异地多活。

#### 10.1.1 异地多活架构

同城异区：同一城市双机房部署，例如北京双机房

跨地异区：不同城市多机房，例如：北京和广州两个机房

跨国异区：不同国家多个机房，例如：中国亚马逊，美国亚马逊

#### 10.1.2 异地多活设计技巧

1） 核心业务异地多活

2） 核心数据最终一致性

3） 多种手段同步数据：1）消息队列表； 2）二次读取；3）存储同步；4）回源同步；5）重新生成数据

4） 只保证大部分用户异地多活

#### 10.1.3 异地多活设计步骤

1） 业务分级：访问量大；核心业务；收入高的业务

2） 数据分类：数据量；唯一性；实时性；可丢失性；可恢复性

3） 数据同步：存储系统同步；消息队列同步；重复生成

4） 异常处理：多通道同步；同步和访问结合（数据分区同步）；日志记录；用户补偿（点券会员资格等安抚用户）

### 10.2 接口级的故障应对方案

内部：程序bug；外部：黑客攻击

#### 10.2.1 降级

业务接口功能降低，只提供部分功能。比如：只读不写；部分功能停止；尽量保证核心功能。

降级操作：1）单个服务提供接口打开降级开关；2）降级系统批量操作

#### 10.2.2 熔断

某个接口响应很慢会拖慢整个系统，熔断就是不在访问这个接口直接报错。

降级：接口本身功能故障处理；熔断：外部被调接口故障

#### 10.2.3 限流

降级：故障时按优先级处理

限流：故障时减少用户请求压力

**限流方式：**

1） 基于请求限流：

a. 总量限流：一个直播不能超过100万人

b. 时间段限流：一分钟最多接收N个人请求

2） 基于资源限流

例如：cpu、内存、句柄数等按可支持的qps负载限流

#### 10.2.4 排队

排队是限流的一个形式，限流是拒绝，排队是等待，比如用kafka队列缓存用户请求

排队模块将超量请求放入消息队列，调度模块读取队列，调用服务模块提供服务处理

**【第4部分】 可扩展架构模式**

## 11.可扩展模式

### 11.1 可扩展概述

不断迭代，产生新的功能

### 11.2 可扩展的基本思想

拆成不同子模块，拆分方式：按流程；按服务；按功能

### 11.3 可扩展方式

1） 面向流程拆分: 分层架构

2） 面向服务拆分: SOA、 微服务

3） 面向功能拆分 : 微内核架构。

## 12.分层架构

1） C/S、B/S架构

2） MVC结构、MVP架构

3） 逻辑分层架构：接入层、应用层、逻辑层、存储层

## 13.SOA架构

SOA（ Service Oriented Architecture）：面向服务的架构，解决传统企业IT系统重复建设和扩展效率低的问题。

1） 服务：所有业务功能都是一个服务

2） ESB（Eneterprise Service Bus）企业服务总线

3） 松耦合：各个服务之间减少依赖和相互影响

## 14.微服务

### 14.1 微服务的问题

1） 划分过细，服务之间的关系复杂

2） 服务数量过多，团队效率下降：功能改动需要N个服务同时配合改动

3） 调用链太长，性能下降，也增加了定位问题的成本：rpc链路长，增加了网络耗时

4） 如果没有自动化支撑，无法快速交付

5） 如果没有服务治理，微服务数量多了后管理混乱

### 14.2 微服务最佳实践

1） 合理拆分，粒度不易过细

2） 基础设施：自动发布、服务治理、服务发现、负载均衡、监控上报

### 14.3 拆分方法

1） 基于业务逻辑拆分：按功能拆分服务模块

2） 基于可扩展拆分：稳定服务粒度粗一些，不稳定服务拆分细一些

3） 基于可靠性拆分：核心服务保证高可高，和非核心服务拆分开

4） 基于性能拆分：对于要求不同的模块拆分开，同一个模块的读写性能按需也可以拆分（比如，写少读量非常巨大）

### 14.4 基础设施

![img](https://pic3.zhimg.com/80/v2-119f9921620ff8ce6579e5d281b0f552_720w.webp)

1） 自动化测试：单元测试，接口测试，集成测试

2） 自动化部署：自动编译发布部署，版本管理，资源管理，灰度和回滚

3） 配置中心

4） RPC框架和协议：HTTP/REST，json/pb

5） API网关：微服务内部互联互通，对外部使用API网关：接入鉴权、权限控制、加密、路由、流量控制、协议转换

6） 服务发现：各个服务注册、状态维护、路由调度等。服务发现两种实现方式：自理式和代理式

7） 服务路由：服务路由调度算法：随机、轮询、最小负载（cpu内存等）、最小连接数等

8） 服务容错：容错方式：重试、流控（频控）、服务隔离（故障节点剔除）

9） 服务监控：上报监控系统

10） 服务跟踪：跟踪一个请求的整个链路。跟踪方式：采样跟踪、染色跟踪

11） 服务安全：接入安全、数据安全、传输安全

## 15.微内核架构

### 15.1 基本概念

**微内核架构（Microkemel Architecture）**：也被称为插件化架构( Plug-in Architecture)，是一种面向功能进行拆分的可扩展性架构。包含：核心系统和插件模块

### 15.2 设计关键点

1） 插件管理

2） 插件连接

3） 插件通信

### 15.3 OSGi架构简析

OSGi 的全称是 Open Services Gateway initiative，本身其实是指 OSGi Alliance。

![img](https://pic3.zhimg.com/80/v2-dfc1c8ccf6eb955a78091d8e2aead96e_720w.webp)

(1) 模块层( Module 层〉：插件管理，插件配置文件等

(2) 生命周期层( Lifrcycle层）：插件连接，提供执行时模块管理，对层OSGi框架的访问

(3) 服务层( Service层）：插件通信

**【第5部分】 架构实战**

## 16.消息队列设计实战

消息队列为例

列出三种方案：kafka、MySQL、自研存储

![img](https://pic4.zhimg.com/80/v2-6163e6c2c33745d133f363a3d350d64b_720w.webp)

![img](https://pic1.zhimg.com/80/v2-86d482db16a301a2deba1f214f265bc4_720w.webp)

评审对比后选择：2，MySQL

然后继续优化：表设计、主从部署、主备切换、读写服务

## 17.互联网架构演进

1） 潮流派

2） 保守派

3） 跟风派

除非是开创新的技术能够创造一种新业务，其他情况下都是业务发展推动技术的发展。

### 17.3 互联网业务发展

#### 17.3.1 业务复杂性

1） 初创期：创新

2） 发展期：快。功能迭代和优化

3） 架构期：量级越来越大，海量服务之道

4） 竞争期：平台化/服务化

5） 成熟期：对于弱势不断优化

#### 17.3.2 用户规模

1） 性能

2） 可用性

## 18.互联网架构模板

### 18.1 总体结构

![img](https://pic3.zhimg.com/80/v2-45da97d7d81599aa9c29e884534d31fa_720w.webp)

### 18.2 存储层技术

1）SQL

2）NoSQL

3）小文件存储

4）大文件存储

### 18.3 开发层技术

1） 开发框架

2） Web服务器

3） 容器

### 18.4 服务层技术

1） 配置中心

2） 服务中心

3） 消息队列

### 18.5 网络层技术

1） 负载均衡：CDN、Ngnix 、LVS、F5

2） CDN

3） 多机房：同城多机房、跨城、跨国

4） 多中心

### 18.6 用户层技术

1） 用户管理

2） 消息推送

3） 存储云与图片云

### 18.7 业务层技术

复杂性的一个主要原因就是系统越来越庞大，业务越来越多 ，降低复杂性最好的方式就是 “拆”，化整为零、分而治之，将整体复杂性分散到多个子业务或子系统里面去 。

### 18.8 平台技术

#### 18.8.1 运维平台

1） 配置

2） 部署

3） 监控

4） 应急

运维平台核心要素：标准化、平台化、可视化

#### 18.8.2 测试平台

1） 用例管理

2） 资源管理

3） 任务管理

4） 数据管理

#### 18.8.3 数据平台

1） 数据管理

2） 数据分析

3） 数据应用

#### 18.8.4 管理平台

1） 身份认证

2） 权限控制

## 19.架构重构

### 19.1 有的放矢

1） 后台系统重构：解决不合理的耦合

2） 游戏后台重构：解决全局单点的可用性问题

3） X系统：解决大系统带来的开发效率问题

### 19.2 合纵连横

#### 19.2.1 合纵

团队内部达成一致

#### 19.2.2 连横

外部团队协调合作

### 19.3 运筹帷幄

重构实施：

1） 划分优先级

2） 问题分类

3） 先易后难

### 19.4 文武双全—项目管理+技术能力

项目管理

技术能力

## 20.开源系统

1） 开源：解决痛点

2） 二次修改开源项目：优化轮子

原文作者：鹅厂架构师

原文链接：https://zhuanlan.zhihu.com/p/540745359

# 【NO.250】C++一行代码实现任意系统函数 Hook

作者：joliphzhu，腾讯 IEG 客户端开发工程师

> 一句话实现系统 API 的 Hook，参数记录以及数据过滤与修改，关注敏感数据本身而不是哪个 API 的哪个参数可能有敏感的需要处理的信息，写工具的时候想到上述能力可以借助模板实现，赶紧尝试了下，顺便做个笔记分享。

## 0.**AnyCall**

## 1.**背景**

一般来说所有 ApiHook 库都会需要提供一个与被 HookApi 相似/相同的 Myxxx 函数以实现参数访问，这里以 BlackBone 的 LocalHook 举例，其需要的是被 HookApi 的引用参数形式，如下所示

```text
bool TestFunc1(char a, int b)
{
    return a + b;
}

bool MyTestFunc1(char& a, int& b)
{
    return a + b;
}

blackbone::Detour<decltype(&TestFunc1)> hook;
hook.Hook(&TestFunc1, &MyTestFunc1, blackbone::HookType::Inline);
```

上述使用方式需要为每个被挂钩的函数都写一个符合参数要求的 Myxxx 函数并将其所有的参数加上引用符号，多写些 API 就产生了大量重复性的代码

## 2.**通用化处理逻辑的优势**

既然在这里已经知道被钩挂的函数类型，那么是否可以利用 C++模板为我们自动生成一个通用函数，以实现一行代码完成任意 API 的 Hook 呢？进一步来说，这样的处理方式是否可以分离 API 和参数的对应关系，使我们不再关注需要修改哪个 API 的哪个参数的内容，而是只关注什么数据是敏感数据，对所有参数只要出现敏感数据的参数就进行修改呢，下面是尝试实现上述逻辑的代码笔记

## 3.**类型萃取生成函数**

函数的参数类型萃取需要借助 struct 辅助实现，先看下如果不使用 struct 辅助直接定义模板函数的困难在哪，代码如下

```text
template<typename RET, typename... ARGS>
RET FunctionCreater(ARGS&... args)
{
 //do something...
}

blackbone::Detour<decltype(&TestFunc1)> hook;
hook.Hook(&TestFunc1, &FunctionCreater<bool,char,int>, blackbone::HookType::Inline);
```

这里的模板参数需要用 testFuncHooker<bool,char,int>形式传递，即先是返回值类型再是各个参数类型，如果需要进一步自动化处理的话则需要实现自动提取参数类型并将其逐个依次在此展开的能力，使用 struct 可以避免实现上述复杂的逻辑，代码如下

```text
template<typename RET, typename... ARGS>
struct AnyCall;

template<typename RET, typename... ARGS>
struct AnyCall<RET(ARGS...)>
{
    static RET FunctionCreater(ARGS&... args)
    {
  //do something...
    }
};
```

这里利用变参模板 + 类型萃取，struct 先申明返回值和可变参数包类型的名称，并在特化匹配阶段将 decltype(&TestFunc1) 整体拆分出其中的返回值类型和各个参数类型,再通过叠加使用宏定义即可在代码层面实现一行钩挂指定 API 的能力，如下

```text
template<typename RET, typename... ARGS>
struct AnyCall;

template<typename RET, typename... ARGS>
struct AnyCall<RET(ARGS...)>
{
    static RET FunctionCreater(ARGS&... args)
    {
  //do something...
    }
};

#define HookApi(FuncName)   static blackbone::Detour<decltype(&FuncName)> hook##FuncName; \
                            hook##FuncName.Hook(&FuncName, &AnyCall<decltype(FuncName)>::FunctionCreater, blackbone::HookType::Inline);

HookApi(ReadFile);
HookApi(WriteFile);
HookApi(CreateFile);
......
```

## 4.**任意函数调用参数监控**

### 4.1 **函数名称获取**

Hook 的一大目标就是需要辅助分析关键 API 调用信息，用上述 AnyCall 可以很好地解决参数打印需求，但首先需要解决的就是函数名获取的问题，不然日志会很难读，Anycall 的模板参数中只传递了函数的类型，是感知不到函数名的，因此函数名的信息只有在宏定义的阶段才能访问到，好在从 c++ 17 起静态局部字符串变量可以作为模板参数传递，这使得我们可以较为轻松的把他纳入我们的宏定义中去实现，如下

```text
#define HookApi(FuncName) static const wchar_t name##FuncName[] = L#FuncName; \
       static blackbone::Detour<decltype(&FuncName)> hook##FuncName; \
       hook##FuncName.Hook(&FuncName, &AnyCall<name##FuncName,decltype(FuncName)>::FunctionCreater, blackbone::HookType::Inline);

HookApi(ReadFile);
//宏展开后代码如下
static const wchar_t nameReadFile[] = L"ReadFile";
static blackbone::Detour<decltype(&ReadFile)> hookReadFile;
hookReadFile.Hook(&ReadFile, &AnyCall<nameReadFile, decltype(ReadFile)>::FunctionCreater, blackbone::HookType::Inline);
```

### 4.2 **展开可变参数包打印**

对变参模板使用递归的方式进行展开 + 任意日志库即可实现参数信息的打印，这里以打印到控制台为例

```text
template<typename RET, typename... ARGS>
struct AnyCall;

template<typename RET, typename... ARGS>
struct AnyCall<RET(ARGS...)>
{
    static RET FunctionCreater(ARGS&... args)
    {
  std::initializer_list<int> expandLog{ (std::wcout << args << "|", 0)... };
    }
};
```

LogArgs 使用初始化列表 + 逗号表达式的方式逐个展开可变参数包，并在每个参数间添加"|"符号分割，但这么写会有些问题，比如遇到为空的字符串指针会崩溃以及遇到特殊的不能被 wstringstream 处理的类型就会报错，前者为运行时的问题可以通过运行时判断处理，后者作为类型问题可以通过模板参数匹配解决

### 4.3 **适配特殊参数的处理逻辑**

由于需要额外的判断逻辑，因此在初始化列表内不能再使用 operator<<默认处理了，需要调用自定义的包装函数，修改如下

```text
template<typename ArgType>
void LogArgs(std::wstringstream& logInfo, ArgType&& arg)
{
    logInfo << typeid(ArgType).name() << "|";
}

template<typename RET, typename... ARGS>
struct AnyCall;

template<typename RET, typename... ARGS>
struct AnyCall<RET(ARGS...)>
{
    static RET FunctionCreater(ARGS&... args)
    {
  std::wstringstream logInfo;
  std::initializer_list<int> expandLog{ (LogArgs(logInfo,args),0)... };
  std::wcout << logInfo.str() << std::endl;
    }
};
```

后续解决上面提到的两个问题：

1. 首先是空字符串指针的崩溃问题：对指针类型且值为 nullptr 的情况特殊处理
2. 其次是没被 wstringstream 的 operator<<重载的参数类型的打印问题：使用 requires 定义一个 concept 让编译器帮助判断参数是否可被打印，然后特化处理可以被打印的部分逻辑，在不能处理的类型部分将其类型名称打印出来

```text
template<typename T>    concept CANLOG_TYPE = requires(std::wstringstream & logInfo, T x) { logInfo << x; };

template<CANLOG_TYPE ArgType>
void LogArgs(std::wstringstream& logInfo, ArgType&& arg)
{
 if (std::is_pointer_v<std::decay_t<ArgType>> && !arg)
 {
  logInfo << "nullptr|";
  return;
 }

 logInfo << arg << "|";
}

template<typename ArgType>
void LogArgs(std::wstringstream& logInfo, ArgType&& arg)
{
    logInfo << typeid(ArgType).name() << "|";
}
```

### 4.4 **任意函数调用参数过滤**

Hook 的第二大目的一般是需要对指定数据进行过滤/欺骗，数据获取可以用上述方案通用化解决，但是参数的过滤方面用 AnyCall 会有一些挑战，尤其是如果希望做到完全通用化的敏感数据过滤的目标的话，后面会提，先看下如何进行相关逻辑处理，类似参数日志打印的处理方式，将参数逐个展开传递给 ArgHandler，在 ArgHandler 内即可实现基于参数类型的数据过滤策略，AnyCall 实现如下

```text
template<const wchar_t* funcName, typename RET, typename... ARGS>
struct AnyCall;

template<const wchar_t* funcName, typename RET, typename... ARGS>
struct AnyCall<funcName, RET(ARGS...)>
{
    static RET FunctionCreater(ARGS&... args)
    {
  std::wstringstream logInfo;
  std::initializer_list<int> expandLog{ (LogArgs(logInfo,args),0)... };
  LOG() << funcName << L": " << logInfo.str();

  std::initializer_list<int> expandScan{ (ArgHandler(args),0)... };

  if constexpr (!std::is_same_v<RET, void>)
  {
   return RET{};
  }
 }
```

ArgHandle 需要对不同类型的参数做处理

### 4.5 **特殊修饰符的参数类型**

首先是对具有 const 修饰符的参数不做处理(系统函数具有 const 修饰符的参数一般也不会有需要被修改的内容)

```text
template<typename ARG>
void ArgHandler(const ARG& x) {}
```

### 4.6 **指针参数类型**

然后即是对指针类型的参数的处理，结构体指针类型的参数需要解引用才能获取到其结构体的大小

```text
template<typename T>    concept POINTER_TYPE = std::is_pointer_v<T> && !std::is_same_v<T, void*>;

template<POINTER_TYPE ARG>
void ArgHandler(ARG x)
{
 if (!x) return;
 //...
 if constexpr (!std::is_pointer_v<std::remove_pointer_t<ARG>>)
 {
  ArgHandler((byte*)x, sizeof(std::remove_pointer_t<ARG>));
 }
 else
 {
  ArgHandler(*x);
 }
}
```

这里的处理方式是将指针移除拿到其结构体的大小，拥有地址和大小后对其数据进行处理(两个参数的 ArgHandler 函数在此省略实现)，多级指针使用递归的方式解决，此处递归过程在编译后可以全部优化掉，另外在//...处省略了对字符串指针类型的处理过程

### 4.7 **链表形结构体的处理**

上述的参数通用处理逻辑在处理非内存连续性结构体时会出现遗漏，比如链表形结构体这样内部有类似 next 指针变量就会导致只能扫描到头结点，这种结构体内部的特殊字段导致结构体的实际范围扩展的情况，由于很难有更通用的处理方式，只能使用特化解决，但可以使用 if constexpr 替代特化简化相关代码，让用不到此逻辑的函数优化掉该分支

```text
template<typename T>    concept POINTER_TYPE = std::is_pointer_v<T> && !std::is_same_v<T, void*>;

template<POINTER_TYPE ARG>
void ArgHandler(ARG x)
{
 if (!x) return;

 if constexpr (std::is_same_v<ARG, PNodeList>)
 {
        while (x)
        {
   //dosometing
            x = x->Next;
        }
        return;
 }

 if constexpr (!std::is_pointer_v<std::remove_pointer_t<ARG>>)
 {
  ArgHandler((BYTE*)x, sizeof(std::remove_pointer_t<ARG>));
 }
 else
 {
  ArgHandler(*x);
 }
}
```

### 4.8 **最难以处理的 void\*类型**

上述指针逻辑在概念阶段就排除掉了 void*类型的指针，因为此类指针通常的使用方式是强制类型转换成其他的结构体指针类型，这里的转换逻辑 API 文档定义的，编译期不可能推导出来，以一个驱动通信函数 DeviceIoControl 为例，其实现形式如下：

```text
BOOL DeviceIoControl(
  [in]                HANDLE       hDevice,
  [in]                DWORD        dwIoControlCode,
  [in, optional]      LPVOID       lpInBuffer,
  [in]                DWORD        nInBufferSize,
  [out, optional]     LPVOID       lpOutBuffer,
  [in]                DWORD        nOutBufferSize,
  [out, optional]     LPDWORD      lpBytesReturned,
  [in, out, optional] LPOVERLAPPED lpOverlapped
);
```

其中的 lpOutBuffer 是我们关注的内容，但他是 LPVOID 类型，实际在使用的时候，外部会强制转换成当前需要的结构体指针进行访问，这里外部对 lpOutBuffer 的大小的感知是通过随 lpOutBuffer 一同返回的 nOutBufferSize 确定的。但问题就在这里，一是 ArgHandler 参数扫描每次只能接受一个参数，二是对于编译器来说 AnyCall 的内部是无法感知这里参数间人为定义的关系，所以这种问题也只能通过特化去解决，那么可以使用字符串编译期比较解决特化问题吗？

```text
template<const wchar_t* funcName, typename RET, typename... ARGS>
struct AnyCall;

template<const wchar_t* funcName, typename RET, typename... ARGS>
struct AnyCall<funcName, RET(ARGS...)>
{
    static RET FunctionCreater(ARGS&... args)
    {
  if constexpr(wcscmp_compiletime(funName,L"DeviceIoControl"))
  {
   //指定第4与第5号参数的关联性
   //......
  }

  std::initializer_list<int> expandScan{ (ArgHandler(args),0)... };

  if constexpr (!std::is_same_v<RET, void>)
  {
   return RET{};
  }
    }
};
```

这里即使 wcscmp_compiletime 函数可以实现编译期的字符串比较也不能实现编译期的结果计算，测试是这样原因，应该是编译器还是将 funcName 当做一个外部符号有关？这里暂不清楚原因，引入 std::integer_sequence 拆分字符串的话可能能解决这里的问题，当然还有次一级的解决方案，即类型比较的特化，代码如下

```text
template<const wchar_t* funcName, typename RET, typename... ARGS>
struct AnyCall;

template<const wchar_t* funcName, typename RET, typename... ARGS>
struct AnyCall<funcName, RET(ARGS...)>
{
    static RET FunctionCreater(ARGS&... args)
    {
        if constexpr (std::is_same_v<RET(ARGS...), decltype(DeviceIoControl)>)
        {
            //指定第4与第5号参数的关联性
   //......
        }

        std::initializer_list<int> expandScan{ (ArgHandler(args),0)... };

        if constexpr (!std::is_same_v<RET, void>)
        {
            return RET{};
        }
    }
};
```

相对字符比较来说可能会存在不同的函数同一个类型的问题，但是同类型大概率关联性指定也是一样的，这里参数关联使用 tuple 去访问即可

```text
template<unsigned INDEX1, unsigned INDEX2, typename... ARGS>
void RelationArgHandler(ARGS&... args)
{
    std::tuple<ARGS...> argsTuple(args...);
    auto buffer = std::get<INDEX1>(argsTuple);
    auto size = std::get<INDEX2>(argsTuple);
    //do someting...
}

template<const wchar_t* funcName, typename RET, typename... ARGS>
struct AnyCall;

template<const wchar_t* funcName, typename RET, typename... ARGS>
struct AnyCall<funcName, RET(ARGS...)>
{
    static RET FunctionCreater(ARGS&... args)
    {
        if constexpr (std::is_same_v<RET(ARGS...), decltype(DeviceIoControl)>)
        {
            //指定第4与第5号参数的关联性
   RelationArgHandler<4, 5, ARGS...>(args...);
        }

        std::initializer_list<int> expandScan{ (ArgHandler(args),0)... };

        if constexpr (!std::is_same_v<RET, void>)
        {
            return RET{};
        }
    }
};
```

### 4.9 **从汇编角度看生成的一个 API 案例**

简化后的测试代码如下：

```text
bool TestFunc1(int a, int* b,int** c)
{
    return a + b;
}

template<typename T>    concept POINTER_TYPE = std::is_pointer_v<T>;

template<typename ARG>
__forceinline void ArgHandler(ARG x)
{
    printf("%s|", typeid(ARG).name());
}

template<POINTER_TYPE ARG>
__forceinline void ArgHandler(ARG x)
{
    ArgHandler(*x);
}

template<typename RET, typename... ARGS>
struct AnyCall;

template<typename RET, typename... ARGS>
struct AnyCall<RET(ARGS...)>
{
    static RET FunctionCreater(ARGS&... args)
    {
        std::initializer_list<int> expandLog{ (std::wcout << args << "|", 0)... };
        std::initializer_list<int> expandScan{ (ArgHandler(args),0)... };
        if constexpr (!std::is_void_v<RET>) return RET{};
    }
};

auto func = AnyCall<decltype(TestFunc1)>::FunctionCreater;

int a = 1;
int* b = nullptr;
int** c = nullptr;
func(a, b, c);
```

逻辑为先打印参数值再打印参数类型，测试输出为: 1|0000000000000000|0000000000000000|int|int|int|，符合预期(记得开启优化) 在 Compiler Explorer ([http://godbolt.org](https://link.zhihu.com/?target=http%3A//godbolt.org))上查看对应的汇编代码，可以看到生成的逻辑很简单就是依次将参数输出以及依次将参数调用 ArgHandler 函数。

![img](https://pic1.zhimg.com/80/v2-4b7ae682bd650dc963813c9c2773613c_720w.webp)

## 5.**总结**

1. 一句话实现系统 API 的 HOOK
2. 参数记录以及数据过滤与修改
3. 关注敏感数据本身而不是哪个 API 的哪个参数可能有敏感的需要处理的信息
4. 完全通用化的参数处理逻辑

起初的 3 点设想能够比较好的实现，但是在第 4 点完全通用化的处理逻辑层面尚且存在些难题，尤其是特殊参数的处理上还是得少量的使用特化逻辑去解决。

原文作者：[腾讯技术工程](https://www.zhihu.com/org/teng-xun-ji-zhu-gong-cheng)

原文链接：https://zhuanlan.zhihu.com/p/545872317