# 【NO.526】Linux服务器开发,fastdfs架构分析和配置

## 1.前言

了解fastdfs存储，对了解ceph、hdfs都有很大的帮助。对于数据存储，我们最关心的问题是：

- 文件丢失，比如某一个盘崩了会不会导致我们所有的盘丢失。
- 上传速度
- 下载速度
- 水平扩展-group多个分组

fastdfs可以应对单点故障，是弱一致性的存储方案，一个storage是一个服务器，3个storage就够了否则会影响同步的效率。

## 2.框架



![在这里插入图片描述](https://img-blog.csdnimg.cn/49c0a312577f4e71941d438646f08276.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



Tracker Server：只是作为代理，并非实际存储文件的。
Storage Server:是文件存储服务，同一个group可以有多喝storage，每个同group的storage文件是一样的。一个storage可以挂载多个磁盘。

- /组名/磁盘/目录/文件名

比如当我们上传一个文件到group1时，存储到了storage1，那就要询问storage2有没有被同步，否则不能到storage2中去下载，后面会继续深入探讨。

![在这里插入图片描述](https://img-blog.csdnimg.cn/997e685c93ab4c91a7c3407083800bfd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



在少写多读的场景，可以一个group多个storage。小视频播放，1000人观看，3个和10个storage对比，肯定10个storage更加滋润。

一般的，Tracker Server和Storage Server不会部署在同一台服务器上，要分开部署。

```bash
lsof -i:23200
```

## 3.总结

Darren老师建议备份一下nginx服务，因为担心不熟悉。我觉得从这件事可以明白一个道理，就是对于不熟悉的东西，应该留雨余地谨慎操作。通过今天老师的讲解，对fastfds项目有了一个大致的了解，对于一个新手来说，配置文件和通讯流程还是比较复杂的，相信下一节课我会涨更多的见识。

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/605063191

# 【NO.527】用户态协议栈



那我们先呢跟大家解释这个协议栈这个东西啊协议栈这个东西呢或多或少啊各个朋友应该都听过，我们站在一个设计者的角度，站在一个设计者的角度，站在tcpip的个人的角度，我们怎么去设计这个协议的？
设计队的角度来设计这个网络协议战。

![在这里插入图片描述](https://img-blog.csdnimg.cn/cd52da5ab57b494aaf3e39522d5aa4db.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



就是各位朋友们你想一下这个网络协议战，
有很多朋友就会想到一个点，那为什么我们还需要去设计一个网络协议栈，我们不是去学这个东西就可以吗？其实我也跟大家解释一下，
其实你在那把网络协议上理解的很透彻的话，你一定要站在一个设计者的角度，
就是你抛开所有的东西，抛开所有的那些框架性的东西，就是你自己去想这个两台PC机之间他们如何通信的，他们通信是a发一段数据b如何收到？
收到完了之后如何去想这个过程是怎么样的，以及发送的数据格式是怎么样的，啊那我们今天就站在这么一个角度来跟大家讲。
当然在讲这个的时候，
我们还会引入一个概念，站在一个设计者的角度去设计一个协议栈的话那怎么去设计？呢那当然我们就跟大家讲到这个用户态协议栈，
因为用户态的协议栈呢他是把协议账当做一个应用程序
来运行，就好比我们很多时候我们写了一个服务器，我们写了一个代码，好，我们跑的时候我们调用网络的接口是调用send,recive，
这个我们在之前讲网络编程的时候给大家讲过，就是我们调用的调用的connect Listen accept
啊这些接口我们是调用的这些接口，这些接口呢是我们系统早就已经帮忙完成的这些接口。
那如果做一个用户态的协议栈呢请大家注意，就是把网络的这一层把我们网络协商对于网络数据解析的这一层，
把它重新拎出来，跟我们的应用程序坐在一起，就把这个网络协议的解析放到进程里面的一部分。

就这么个意思好吧？
就是

好，把协议栈这是协议栈这是我们的应用程序，如果不是这么做，呢
本来的做法呢是这样的，是把网络协议，但它是在操作系统的，把这一部分跟我们的应用程序分开，
放到操作系统里面就这么一种情况，现在用户态协议栈就是把这个协议栈放到应用程序跟应用程序放在一起，这么明显的这个能理解吧。
好，那我们现在再来分析一下，
为什么会要有这个用户态协议栈，呢啊为什么会要有这个用户态协议栈？呢
好，我在这里问一下大家，好减少拷贝，我在这里问一下大家大家有没有接触过或者有用过用户态协议栈的

好，这里有朋友说应该是很多朋友是没有用的，没有用过是很正常的，啊串口通信串口通信不是走的网络吧，应该绝大多数朋友是没有用过的。
那我们接着来跟大家讲一讲，如果没有在这种场景下面，但是你是很难用得上的，就是跟大家解释一下，为什么会有用户态协议栈。

就是

啊各公司私有协议算了，
呃私有协议这是应该是属于用户的协议，就是类似于在TCP的上面去定义的协议，好吧？
好减少CPU上下文切换，那好，

我们来给大家解解释一下为什么是这么个说法，我们这里要跟大家解释一下，首先第一个这里是一个网卡，这里是一个网卡，然后在对应上
中间这一层是我们的协议栈。

![在这里插入图片描述](https://img-blog.csdnimg.cn/cbd3595bb7ad4c53a8ad734f9f9ad931.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



好，还有这边是我们的应用程序，应用程序就是我们自己编译出来的进程这么一个概念。
这三个点
第一个数据是从哪里来的？这三个这三个我们用一个框把它框加入，就是我们
在服务端也好或者在客户端也好，就是我们能够进行网络通信的也好，就这样一个一个应用程序就是把它放到了一起，就这一段。
好，现在比如说我们应用程序通过一个客户端一个应用程序就是一个进程，没错，应用程序就是一个进程进程是运行阶段，
现在一个客户端也好，然后我们通过来通信就是一个客户端，就是我pc机现在给百度访问百度这个过程也好，或者去访问淘宝这个过程也好，就这样的。
访问先数据是先到达网卡，先到达网卡，这边是客户端也好，就是对端的一台对端的机器，然后发送数据先经过网卡网卡先接收到这个数据，
然后网卡把它处理完了之后，再把这个数据然后copy到协议栈里面，然后协议栈再把这个数据
我们通过系统调用都从协议栈里面copy到我们应用程序上面来。
好，这个过程能不能理解？这个过程应该能理解，就是

我们先把数据先到了网卡那这里，**我们问一下网卡的作用用来做什么**？
好，
网卡资源大家打开这个7层模型啊打开这个7层模型，

![在这里插入图片描述](https://img-blog.csdnimg.cn/28a39e39fedc4b45945f38186ab10ee3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



**网卡是属于哪一层的**，大家看吧打开这7层模型，大家可能很多朋友说的网卡属于哪一层，其实我跟大家讲一下
就是在光纤里面传的时候，在光纤里面传的时候传的是
光信号在双绞线里面传的是电信号，这个物理层的什么物理层就是所说的我们光信号或者电信号
网卡的作用呢就是把这个光电信号转换为数字信号，
转化为数字信号，也就是说一个AD在三个的过程中呢就是把数字信号转化为
我们的模拟信号转化为光电信号，光电就是模拟信号，那也就是说一个AD转化和da转换作用。
好，那地方它所以网卡它不是在任何一层，
它既不是在物理层，也不是在数据链路层，它是在物理层数据链数据链路层之间做这个物理层转化为数据链路层这么一个概念，等于说就是把这个模拟信号转化为数字信号数字信号转化为模拟信号这么一个概念
来理解，如果大家能理解这个网卡的作用

那我们网卡对象在这一版对端机器接收完之后，我们不管是网卡，不管是光纤还是双绞线
网卡接收完这个数据之后，通过AD转化
把模拟信号转为数字信号，然后把这个数据放到协议上，怎么把这个数据从网卡迁移到协议上，请大家注意这里有个东西也要跟大家解释一下，这有个工具
不是叫工具，有一个东西叫做 SK Buffer叫做SK buffer。
好sk_buff这个东西就是就是用来从网卡里面
数据运到协议栈里面，协议栈里面主要对网卡数据进行解析的。

这就是网卡的数据，这就是协议栈
协议栈把网卡数据解析完之后，把SK包的数据解析完之后，对应的这一帧一帧的数据，
然后放到这个recv缓冲区里面，然后我们通过系统调用调用receive这个函数
要用receive，好，然后从协议栈里面把数据从协议栈上发到我们的应用程序，所以我们就能够读到这个数据来理解，这就是中间经过这么两个方面，
就是从网卡copy到协议栈，再从协议站copy到我们的应用程序，这是我们现在操作系统，他工作方式就是这样的，

来理解一个网卡对应一个Mac地址没错，一个网卡对应一个Mac。
好，就是以我们现在的linux我们为例，他就是这么工作的，
就是我们现在每接收一堆数据，就是你现在写的服务器也好，你写的客户端也好，你把数据发送出去接收数据也好，
它都是这么一个过程，每一帧从网卡里面需要copy到协议栈上，再从协议栈上copy再用程序每一次都用，所以很多朋友就在想一个方法，想一个方法，
就是这里面从网卡里面copy到协议栈再从协议栈copy到应用程序，这个过程它有两次拷贝，两次copy，
两次拷贝来理解，这两次拷贝分别是从网卡copy到协议栈，再从协议栈copy到应用程序，其他的我们还没算，就是大家你是因为里面我们copy多，
消息队里面他的消消息对列把copy数据库它的数据库里面拿出来，那这个东西没算，就是每一次系统调用都需要经过这么两次，所以很多问题就在考虑一个问题，
就是我们能不能简化一下，好，后面就出现了一个新的方法，就是这样的。
好，这里是一块内存，这是一块内存，然后通过这么一个方法，
就是通过网卡的这个 dma的方式，就是将网卡映射到内存中间。

好，将网卡映射到内存中间，就是网卡里面解析完的数据这里有块存储，把这块存储的空间映射到内存中间，跟内存的空间是一一对应的，

也就是说接收数据网卡解析完之后，数据就直接映射到内存中间这一个方法来理解，这个方法呢就是跟大家讲到的一个叫做内存映射叫m map的方式，
它底层是走的一种叫做dma的方式，叫做内存直接从直接通道这么一个方法。

这里跟大家讲的大家可以看到从网卡里面的数据到达内存中间，然后应用程序是直接在内存中间可以直接读取这个数据的直接读取
映射过来的这一块数据了，所以在这个过程中间就减少了这么一次拷贝，但是过程中间有可能会，说这不是减少一次吗？有这么斤斤计较吗？请大家注意。
这一次dma的方式，他从严格意义上来说，它不叫copy，什么叫做拷贝
好，首先我们把这数据
数据的通道大家能够理解，就是现在通过网卡映射需要到内存中间，如果大家能理解这个方式，那我们接着再跟大家讲一讲。
就是第一个上面这一条路是有两次拷贝，
下面这一次是采用一种DMA的方式，也就是说这个 DNA的方式什么意思？可以跟大家讲它是没有拷贝的。
没有copy，那很多朋友说那这个网格数据怎么到内存里难道没有拷贝吗？请**大家注意拷贝是什么？就是复制什么意思？**

或者是通过我们CPU执行的这，要通过 CPU指令的啊是通过CPU指令才能够做得到的，
其他注意那这个 dma的方式它是自己操作的，是CPU是不需要去干预的，
CPU不需要干预的，啊就是说网卡的数据直接到达内存中间，
能理解这一个请大家注意，所以这里两次拷贝，但是这个网卡的数据到达内存中间它是没有拷贝的，我们应用程序是可以直接去取这个数据的，所以在这个过程它是没有拷贝的，也叫做零拷贝，

这个地方不是有一次拷贝的，这个零拷贝是怎么理解？就是我刚才讲零拷贝就是利用dma CPU是没有干预的，CPU是没有操作的，所以它叫做零拷贝。
好，如果大家能理解这个零拷贝这个词没有，

那我们在这里跟大家延伸一下延伸一下，有很多朋友问到这个 mmap的原理好，m map的原理
没错，有那么问到mmap里面的那 mmap，不对应，啊我可以跟大家解释一下，mmap我们可以这么做，我们可以从磁盘中间对
对磁盘中的一个文件我们可以映射，我们同样也可以对网卡进行映射，
也可以同样大家可以对一个USB或者对一个U盘，你也可以叫做dma叫做mmap你也可以把它映射出来，包括有一些蓝牙的设备，包括WiFi的设备，
都是可以直接通过m map映射到内存中间来进行操作的，能理解**那 m map它的原理是怎么样？**
好这一步也跟大家解释一下，
也跟他解释讲要依赖dma对应来说它是一个总线，它是一个总线，请大家注意啊这一步我们就没有深入进去跟大家讲计算机系统体系结构了，因为这东西就比较多了，请大家注意。
像这种dma的方式，mmap包括对于磁盘操作，它也是一个dma的方式，也是绕开了这个 CPU去复制，
包括包括我们对于蓝牙操作或者外设或者USB操作它都是这样的，通过一个DNA的方式，然后直接把对应的存储映射到内存中间，
也就是数据是直接过来的，请大家注意这里有一个前提是需要有一条总线的，
好，请大家注意这一点就可以了，也就是mmap它底层实现的东西，它是由于底层是有DMA的这种方式的支持才可以做到的，好吧？

好CPU如何知道就这样吧这里有个情况就是当做数据，
映射完了之后，这时候会给CPU触发一个中断，就是数据已经就绪了，这一步这是体系结构里面已经讲过，这个这是DMA的方式，数据已经运行，转过来之后，dma
这边传输完了之后会给CPU引发一个中断，好吧？接着跟大家讲网卡上面网卡上面是直接芯片的，好，这个我给大家解释一下。

好，讲到这里，我相信大家可能还有一些概念上的原理，概念上的原理在这里我也问大家一下，就是既然大家有问到这个问题，啊我问一下大家就是关于这个网卡，
网卡的驱动它是运行在哪里好？
答案一是运行在网卡上面的，
第二运行在CPU上面的，运行到我们的操作系统里面的，运行到CPU，就跟我们操作系统运行在一起的，内核里面的啊是选择一还是选择二？
没错，请大家注意，请大家注意。
这里有一个概念都有吧，请大家注意这里就有一个概念的问题，请大家注意，这里有一步就以CPU

![在这里插入图片描述](https://img-blog.csdnimg.cn/4b8352946d5d4f33b87c01eb09f4552b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)



好，这个我们把它简化一下我们这两部分，一部分这两个框这一部分是操作系统，也就是我们内核，好，这是网卡。
好，这是网卡。
好，请大家注意啊这个网卡中间它也有芯片，请大家注意这个芯片上面运行的东西**叫做物检**？，它是本来在芯片出厂的时候，在网卡出厂的时候早就已经做好了，

那这往返驱动请大家注意它是内核里面的一部分是去驱动
使得这个网卡进行正常工作的，所以很多时候跟网卡的驱动，包括我们后面跟大家讲的这个 Nic系统的这种
它是运行在内核里面的，它是去兼容使网卡能够正常工作的人，那就它是取使网卡正常工作的，
这么一个东西叫做驱动叫做网卡驱动，能理解这个概念，一定跟大家讲的网卡驱动是运行在内核里面的，它是使得网卡能够正常工作的，以及能够去接受网卡的数据，使得网卡能够发送数据这样一个正常工作，这是网卡系统

好，网卡上面是，芯片芯片上面它也是需要有程序的，以及包括这个网卡上面跟大家解释一下，比如这个网卡上面它接收数据的处理，它的怎么一种处理方法，也就是说对于这个
模模数转换这高低电瓶怎么去以多少作为一个参数，它也是需要有代码的，也是需要有程序收入进去的。
好，
这是关于底层的原理这一部分，我们就关于网络底层这个东西给大家讲到这里。

因为这里面还有一部分就是关于这个网卡它的作用还有很大一部分的作用，包括我们对于这个网卡可以做虚拟化，对于它的一些功能我们还可以做，对于有用户态协议栈这个东西之后，
我们的对于网卡的想象空间就会更大，有了用户态协议栈，我们对网卡的想象空间会更大。
举个例子跟大家解释一下举个例子跟大家解释一下，第一个比如说比如说沟通，比如说我们现在一台PC机，
一台pc机，如果我们能够把网卡自己能够通过代码去控制的话，那我们是不是可以把我们的做成一个交换机也可以，或者说我们做了一个路由器也是可以，
或者说我们包括像一些数据的过滤我们也是ok的，就是一些数据我们不去处理也是ok的，所以对于这个网卡我们的可操作性它就会变得更强。
好，

讲这个讲这里那我们核心的原理还是跟大家来讲，这个协议上这底层原理是依赖这样一个动作，
那对于网卡我们接收完一帧完整的数据，我们怎么处理好吧？
那我们接着来跟大家讲，假设现在假设有一个前提，
假设我们能够取到一帧完整的网络数据，一个完整的数据包。
取得一个完整的数据包，这个完整的数据包呢就是包括网卡里面接收什么数据，我们就能够用什么数据。
好那在这个基础上面我们就有的去实现这个协议栈的这个前提，有了实现就有了实现这个协议上的最基础的东西。
好，
在这里我也跟大家分享，我们怎么去取到一张网络的数据，啊得到一个完整的网络数据包的数据，这里有这么几个方法，
第一个方法，
第一个方法我们可以利用原生的sock的好如何取到一定完整数据，这是第一个方法。
第二个方法，
我们可以利用一些开源的框架比如netmap。第三个我们也可以用一些成熟的，啊我们可以用比如说一些商业的框架比如dpdk，

![在这里插入图片描述](https://img-blog.csdnimg.cn/f32ced4031ed4bef9989f1da509abb3e.png)



大概是这么三种方法，当然还有一些其他的，比如像这种pfl这种方向都是可以的，大概能够在网卡中间取一点完整的数据的，有这么三种方法。
好吧，那我们今天在这个基础上面，
我们来跟大家先来封装一下，今天的代码量会比较多，代码量会比较多，我们今天要跟大家封装几个协议，为了我们后面就跟大家去实现TCP有关系。
现在我们在我们实现的这个前提下面，在这个底层的框架的基础上面我们实现一个协议栈，那也就是说我们应用的这个框架利用netmap，因为netmap开源的

dbdk我们后面会有专门的主题，专门的内容来给大家讲这一部分。
好，如果各部门现在没有，那地方可以现在在github里面搜一下这个框架，然后把它放下来，然后编译一下然后就可以了，我们在这就跟大家实现这个协议栈

![在这里插入图片描述](https://img-blog.csdnimg.cn/405c76b32f1a4f5ebdf6acf77d421e01.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



这里是个服务器，这里是个客户端，现在客户端给服务器发送一份数据好。
还是，我们还是采用用一个udp的系统，先从udp开始封装起，比如客户端现在发一些数据到服务器，服务器接收这个数据接上一个完整的udp的数据包，然后怎么处理呢？
我们对一个udp的数据包啊这里要从头开始跟大家讲，就是以一个udp的数据包为例，啊 udp的数据包大概分为这几个方面
第一个
以太网头，第二个IP头，第三个udp头，第四部分才是我们的用户数据，**才是我们接收到，我们调用recvfrom接收到的那一段的数据。**
也就是说一针udp的数据包分为这么几个层次，几个包，第一个以太网的头，第二个IP头，第三个udp头，第四个是我们的用户数据。
也就是说以太网络头是对应的在数据链路层的，然后IP头是在网络层的传输层，udp是在传输层的，然后以及用户层的数据，
每一个我们对应的来跟大家来封装一下，大家可以大家就可以对应的代码，并且我们要把它跑起来。

这里我有必要再打开这个情节给大家解释一下，就是本身这个题目的话，今天我们是讲这个
滑动窗口，我认为在讲滑动窗口之前，啊
然后包括像TCP协议上的实现之前，我们先把这个环境先把它跑起来，先在后面的时候我们再去跟大家去讲到TCP协议具体实现的时候，再去跟大家讲这个关于滑动窗口具体是怎么做的。

第一个就是以太网头怎么封装，呢
以太网的头包括14个字节的以太网的头，
前面6个字节是目的地址，后面6个字节是原地址以及2个字节的类型，请大家注意这个目的地址六个字节什么意思？
就是所谓的那个
MAC地址
请他认为这个MAC地址这个东西
每个网卡都有一个MAC地址，网卡出场时的那个MAC地址那你可以改的，因为我们在每发送一帧数据包的时候，在软件上面我们是可以对这个数据进行修改的，那就是关于这个 MAC地址，
原厂出的mac地址是可以改的，也就是说大家你所接到的MAC地址也好，IP地址也好，端口也好，请大家注意，
我们没有在计算机没有哪个物件，它叫做MAC地址，没有哪个物件叫IP地址，也没有哪个固体的东西叫做端口都没有，请大家注意。

![在这里插入图片描述](https://img-blog.csdnimg.cn/00d71d3bcf2443b09bdbc4af3d5553e9.png)


所谓的MAC地址也好，ID地址要端口也好，全是协议栈里面一个字段名，那地方全是这样一个字段名而已，它并不是一个固体的部件。

![在这里插入图片描述](https://img-blog.csdnimg.cn/77ef50ea10e84685937c805aac0f5ed8.png)



好6个字节目的地址，再加上6个字体的原地址，再加上1个协议，
这就是关于以太网的头
6个字节的目的地址，6个字节原地址，请大家注意这1个数组包
大家等一下我们去取的时候
这个我们取出来这么一个一帧数据包，这个数据包按照我们这里应该用数组吧 ；注意这个内存啊这个内存在排布的时候，
结构体的使用它跟数组是一样的，是一样，就是对于一块内存的使用，比如同样是8个字节，
同样是14个字节，那我们是用数组去存也是可以的，我们用结构体去存它也是ok的，好会不会有对齐的问题肯定有。
好，这是关于以太网头，



![在这里插入图片描述](https://img-blog.csdnimg.cn/ccae32bcb2a74354a3a4f87cc122ef49.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_16,color_FFFFFF,t_70,g_se,x_16)



然后第二个就是IP地址

好在这呢关于这个长度我有必要再跟大家聊一聊，就是关于这个长度，啊大家可以看到这16位的这个长度它有多少？
16位，一个udp的是4，节数，16位总长度它有多少？65535，好。
65535，也就是从理论上面来讲，一个IP包它有多长，一个IP包最长它能够有多长？各位最长它能有多长？

也就是说最大的传输单元是1500，最大的传输单元是1500，那一IP包它最大可以传65535，
但是很多人就不太理解，那653这里面不是已经规定了最大只能传1500，嘛为什么这个IP波还能做64k还能做64k?

这 Mtu那个东西它是以太网的限制，你比如说我们一个64k的数据，
一个IP的64k的包，就一个流媒体的数据包，我们发过去了，一个大64k满包发过去，发过去之后请他注意在以太网这层在网卡传出去的时候，它会分片
分成。

这么大一个1500一个一个包把它发出去，连续发多个把它发出去

我们都会有问过这个 mtu和这个最大传输是不是会有些冲突？
比如说如果避免分包分包的过程是避免不了的比如说你去访问百度的过程中间或者访问一个公共IP，你中间要访问的时候，你要经过那些路由器那些路由器或者网关，它也是一个网卡的设备，它也是要协商进行解析，对数据进行分析，请大家注意，
他也会去分包，它在关于分包的这个避免的情况它是避免不了，除非你传输的包特别小，然后每一个包中间的时间间隔足够长，

![在这里插入图片描述](https://img-blog.csdnimg.cn/3af5981b67dc4f03b977b77fd487b221.png)



然后就是协议。
可以看到以太网这里有一个proto，这呢关于IP里面呢也有一个协议，
为什么每一层都会有一个协议，
这是标志着让数据链来传输的时候，从这头里面能够解析出来网络层是用的什么协议，通过网络层我们能够解析出来传输层用的什么协议对不对？IP头里面的这一个proto是用来去形容传输层我们用什么协议

ID包里面哪有端口，IP包没有端口

![在这里插入图片描述](https://img-blog.csdnimg.cn/0b00b03fc341451e8e2ab5eed4c0aa52.png)



![在这里插入图片描述](https://img-blog.csdnimg.cn/9a4521990bf9467583a3e69f01cb2b93.png)



UDP包里面对于每一个包它有没有一个ID？
IP包里面没有，
它没有这个每一个包它是哪一个包发的它是没有的，那这就造成了一个现象，请大家注意就是这个 ud p在发送的时候，
他的协议上面是不可能去实现，从协议本身它不可能去实现。
对于包的定义的就是一个udp，发数据它是没有边界的，你是很难去给udp这个数据包定义一个包的，就是udp协议本身它就没有这个包的概念能理解。
所以各位在这里讲的是udp研发店所有的udp的包，你发现它的包的头
唯一的就是通过这个 check能够效去检验这个包对不对，有没有丢失，但是我们很难去把它去看出来，这里总管一起我们发了多少个包，从它的定义的格式上面，从这个逻辑上面它就应该体会出来，
udp它是没有数据包的概念，所以说啊所以说我们后面会去以一个ud的包给他发送出去，但是请到你udp的头在定义的时候，他压根就没有这个包的概念。
那有一个包肯你知道这个包括有个ID吗？它至少会有一个分割会知道的，包括是有个ID，我知道这个包收到了，UDP是没有

udp只有8个字节协议头。

得出这样一个结论，就是关于IP层为什么没有为什么没有这个端口，

MAC地址它是以太网是数据电路层的产物，
IP地址它是网络层的产物，端口号它是传输层的产物，所以各方面在IP层在网络上它是没有端口这个概念的，所以在另外一个层次我们也可以帮助大家更好去理解就是
如果没有MAC没有IP地址的话，换一句话说换句话说没有IP地址，也就是说哪些如果路由器路由器它是工作在网络层的，
如果没有路由器没有交换机的话，从一定程度上交换机本身就是二层的交换机，二层交机它只适合在局域网内工作，那就它只适合在局域网内工作，
如果要跨网络的话，那就需要借助路由器来调节，或者三层调换器三层调节三层调换器就是它能够工作在网络上来理解，
就是交换机他只能工作在局域网内，但是如果要跨网络，从a网的话请不引这里一定要引入一个路由器或者三层交换机在内。
网好。对于这个端口后呢

很多时候再来问大家一个问题，再问大家一个问题，大家有没有听过NAT的东西？网络地址映射网络地址映射它是什么？将端口
和IP地址做映射的，它是需要工作的传输层，它是要对传输层进行解析的工作，
他需要对传输层的协议进行解析，所以很多时候我们听到一些东西，工作上2层3层4层5层，
工作在哪一层，请大家注意，你就可以看到它是对哪一层产物进行解析的，你就能够判断出来它能够在哪一层，比如说交换机它只对MAC地址进行处理，
所以交换机是二层的产物



![在这里插入图片描述](https://img-blog.csdnimg.cn/8e172f4cd25346ad8d2f7aaf36b2419c.png)



nginx工作到应用层，他是对应用层协议进行解析的东西。
haproxy它是对TCP端口他是传输层的，
lvs它是对IP地址也是网络层的，
f5数据链路层

关于负载均衡每一层工作的概念，能理解他工作在哪一层，

这是跟大家讲那个分层的时候，他每一层工作在哪个产品上面，大家能够理解上他，你也能够从他这个工作在哪个层面，你自己应该也能想明白他原理。

第一部分它是需要有一个以太网的，第二个就是IP，
第三个就是udp的，还有一段用户数据就是data，那这个这个用户数据我们怎么定义？
那个用户数据我们怎么地，因为这里面有几个概念，刚才说我想到用数据，
好，那用数组可以，那是我的长度怎么定，第二个有什么数组好像这个长度不那么低，就想到我们用指针指针可不可以用？指针好像不太对，那我来给大家解释一下。

大家看到这里面我们定一下这个长度我们怎么定？好这个 data，

大家呢有两个情况，我们第一个用数组，数组的长度不太好定义，因为我们不知道用户数据有多少，第二个用指针的话就出现一个现象，我比如用指针去定义，你会发现这4个字节的指针会在哪，，
也就是说这个数据包是在这里截止的，后面这个是没有也是这4个字节，它是1个指针指向另外一块内存的，指针肯定是不合适的，

这里给大家介绍叫做一个柔性数组，也叫做零长数组。

柔性速度用在两个方面，有两种前提它是可以用的，
第一个内存是已经分配好的，
第二个跟这个柔性数组它的长度我们是可以通过其他方法来计算出来的，这两种条件下面我们是可以用柔性数组

我们怎么去抓到udp数据包？

我们就用netmap方案来跟大家讲。

**柔性数组仅此它是有一个标签而已，**
大家看到就是对于这样一个数据包，
前面这4个字节是以太网头，20个字节是IP头，8个字节是udp的头那这个payload就是1个标签，指向的
是这个udp8个字节头后面的这个位置，就是指向这个位置，至于后面多长，拿着payload这个标签加1，
这里有两个前提就在手里头，第一个内存是先分配好的，
第二个就是我们可以通过一某种计算能够得出这个数组的长度我们才可以用，不然的话很容易会造成这个类型越界。

------



![在这里插入图片描述](https://img-blog.csdnimg.cn/3bbc01cef67643488571bbe7e457b635.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_16,color_FFFFFF,t_70,g_se,x_16)



第一个这里画三个东西，第一个这里是一个网卡，这里是个CPU，CPU上面在这里有一个不等同的概念，就是可以在这里理解为那个内核
被理解为我们所有在CPU上面执行的东西需要通过CPU，这里面，包括内部包括我们的应用程序，包括我们自己写的代码应用程序就在CPU上执行的。
好还有一个板块就是内存将网卡映射到内存中间，内存将网卡映射到内存中间，
现在我们的应用程序是直接在网卡里面取数据的，直接在网卡里面取数据，所以在这里面我们写的第一个，
首先第一个先让neymap先工作起来，那就是直接发出去没错，直接接收数据和发送数据，
有的网卡安装出来之后叫eth0也有的叫ens33，

ens33是虚拟机的网卡，然后eth0是物理网卡



![在这里插入图片描述](https://img-blog.csdnimg.cn/b279d066f1fd41a9b6453a014b644ee9.png)



这个网卡接下来它的数据就被接管了，**请大家注意这一行之后，这个网卡的所有数据就被映射到内存中间，**

用你的虚拟机开始启动两个网卡来做，那例如也就是说我们用SSH连接的是eth0，
但是我们接管的是eth1来理解这个做法就不会去影响，有时候你发现一开始工作的时候，如果你只有一个网卡，工作之后你会发现断网了，发现SSH连不上了，为什么？
就是因为你这个所有的数据都被让他们去接管了

好，现在跟大家再解决一个问题，**就是我们怎么知道这个网卡来数据，怎么知道这个网卡里面有数据来了？**
好，今天表这里也有一个方法，就这个网卡数据来，它会有个中断，这里有一个东西这是那个 map实现的东西，就是他把这个数据接收完的这个方法，他把它跟我们的io多路复用，也是我们open出来之后。
我们借助一个poll能够去知道，有一个fd能够去知道这里面有数据来了，如果映射完之后tcpdump是抓不到数据。

![在这里插入图片描述](https://img-blog.csdnimg.cn/c6f875427fc44a88aa6d816b58a1fddb.png)



接下来我们就开始去处理它对应的数据。
这里有一个东西要给大家解释一下，大家看到这个摆完之后写完之后，这个poll就判断它对应有没有数据了，就是对应它就是这里有个fd，
natmap在工作的时候，他把这个网卡比较做到一好的一点，他就直接把这个网卡做成了/dev下的一个文件，就做成一个设备文件，
比如说这个设备文件里面，网卡里面所有的数据，所有数据都会到这设备文件里面，也就是natmap里面，这个fd是监测这个设备文件，
我们判断它里面有数据就有数据，这个fd里面有数据来了，我们就可以去读取它。

这里也有一个情况，
读这个动作大家有没有去想过这个关于read或者write什么叫读？
读这个动作，那一定是从外存读到内存，这从包括我们读文件，
读文件或者说我们去读一个设备，包括我们读数据库这个也是ok的，他从外面读到内存中间，从外层读到内存中间叫读，
内存的操作，我们不叫内存叫操作内存，请大家注意这个读这个动作，它叫做这里面一旦我们检测有数据中，这里面我们去操作，那时候我们不叫做读，叫做nm_nextpkt()

叫做去操作获取下一包，这个怎么理解呢？
这个给大家讲这也是netmap实现的，就是网卡里面过来的数据的时候，网卡里面处理完数据之后，把数据映射到内存中间，这个映射的过程中间，
一个包一个包的映射，映射的这个包叫package，如果你再来一个包再来一包， n个客户端连接的网络，n个客户端给这个网卡发送数据的话，
那就有n个包，**那内存中间这是n个包是如何组织起来了？**

其他的东西这里用的一个东西叫做循环队列，叫做ring_buffer。
就是来一个包映射过来的时候，把这包加入这个循环队列里面，所以我们在取的时候，**只要记住它头在哪个地方**，就是取下一包，拿出下一包我们再使用，
就是过来的包这里有一个循环的队列，
好，映射一个包到我们在这里，读的时候这里叫做nm_nextpkt，读出下一个包出来。
这个地方为什么叫做nm_nextpkt,为什么这个地方不叫read。

**就是零拷贝应用在哪些场景？**其实大家可以看到，包括大家你能够想到去做持久化的部分，做日志操作，我们调用的都是像fwrite或者调用write这两个函数，我们在操作日志的时候能够操作文件，那我们如果用mmap这个方法零拷贝就可以使用，我们可以去open一个文件，把它映射到内存中间，然后我们的日志在落盘的时候，我们直接写到这个内存区域中间，然后由他同步过去，不用经过文件，映射到内存就同步过去

知道长度不用担心“/0”

这里还有一个前提也跟大家讲一下就是关于这个
这一步强转可能很多朋友也不太理解，就是我们针对这个数组针对于这一块，stream，你不要简单的把它理解为字符数组，
请你把它理解为单一的一块内存，一块内存的一个开口位置，我们需要对它单独的每一个字节进行操作，所以我们利用的是这个无符号的数组，请注意理解它是指向一块内存，而这个内存多少我们是可以自己去计算的。

**有人说这个方法是不是阻塞的，这个nm_nextpkt是不是阻塞的？**
这个nm_nextpkt是在内存中间，它是内存中间一个循环的
一个环形队列，每一个快给每个包是在内存中间的，但内存操作的时候，我们去操作内存的时候，就在这个点上面，我们没有所谓的阻塞和非阻塞，就是因为这个数据包早就已经准备就绪了，已经有这个数据了。

现在我们调用的是一个poll，他通知到我们应用程序，现在这里已经有包了，我们现在再来取，再来把那个nm_nextpkt包取出来，请注意它是取，但是我们没有做copy。

这个方法,它不能够阻塞和非阻塞来讲，它是存内存操作，

接着我们看一下定义几个东西，这些东西呢在各方面这不是我要定义的，而是这个文档里面有

![在这里插入图片描述](https://img-blog.csdnimg.cn/b1baa43251494a339bb1fc81c3b8a096.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_13,color_FFFFFF,t_70,g_se,x_16)



标准的定义

![在这里插入图片描述](https://img-blog.csdnimg.cn/7857fe9e1ec241f1937b871c84cf744a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_12,color_FFFFFF,t_70,g_se,x_16)



我们先把这些数据包给抓起来，
它能够执行而已，我们也可以采用循环啊我们可以采用循环，

![在这里插入图片描述](https://img-blog.csdnimg.cn/f371543702fd4f59b53f61bfa13913c2.png)



对于这个 Udp，这里我们取出的是一个以太网的帧，以太网的帧，这里当然会有一些特殊情况的存在，就比如说你如果发这个包很大的话，
他不一定那个就是有一些数据是在后面的，就是你如果挖这个数据包很大，那这个nm_nextpkt取得这种包，它可能还有一部分在后面，
这个以太网的帧接受这个数据包，也就是说在我们小于1500的数据的时候，你不用担心，
就是我们如果发送数据小于这个1460的时候，我们根本就不用去担心啊它的数据，比如一半到前面，一半到后面，你的担心是多余的，它都不会存在的好吧？
如果超过1640，那就可能它就会有一个分包现象，就有可能我们一个包在下一个包后面。

接着我们跟大家讲这里面还有一个字节对齐问题

这里这个以太网的头是多少？14个字节，然后 IP头现在是多少？20个字节，然后udp的头是多少？8个字节。
**但是现在这个 sizeof(udppkt)有多少？**
在我们现在这个情况下面等于多少？
等于44号，为什么？
这里会有自己对齐的问题，这前面是4字节,14字节我们一块内存以4个字节
对齐为例，前面4个字节再4个字节，再4个字节，再加上前面2个字节，这里面合到一起14个字节，也就是这一块。

![在这里插入图片描述](https://img-blog.csdnimg.cn/f28f34fb62744cae9b509a3a4fbc5127.png)



这里面在紧接IP包的时候它是顶齐，从这里开始再分配，这里就留下了两个字节的一个空窗期，这中间会有小窗口，这两个字节里面是没有数据的，所以在 sizeof(udppkt)后
里面是等于44的，不是42，
因为这地方有一个小窗口，为了保证就是我们接收的是一个完整的数据包，，一个packet它中间是没有这两个之间的空空格期的，所以在这里我们要对它加上一个对齐，以一个字节的方式对齐，netmap编译的时候，请代表要加上这个东西，就是

![在这里插入图片描述](https://img-blog.csdnimg.cn/141427a0764e44ca89f357e0e1b91805.png)



刚开始是可以运行的，是可以接收数据的，
但是你要过一段时间，这不行了，好这是第一个问题。
第二个问题刚刚我们是可以拼这个 IP地址，我们现在拼一下，这个我把它看到了我们再跑一下，刚开始我们跑起来时候我们再比较有没有发现这个点？
就是我们刚刚我们在没启动之前它是可以拼的，但你发现我们现在启动了之后现在是不可以拼的，这是第二个问题。

![在这里插入图片描述](https://img-blog.csdnimg.cn/06d09ac5db774c74a9b5a956578e4766.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_11,color_FFFFFF,t_70,g_se,x_16)



这里请大家注意第一个问题为什么它不行？
这里要给大家引入一个概念叫做arp，
因为我们现在做的这个协议栈跑的这个东西压根就没有去实现arp的协议，
只是简单的把这个 udp的数据包能够接受而已，**那为什么最开始它又可以？**

arp的工作是这样的，arp的工作是在局域网内全部进行广播，
比如每一台机器都会对局域网内从
一段一开始1~255中间每一个区某每一台机器都会去广播，
就是我是192.168点多少，你的MAC地址是多少？

![在这里插入图片描述](https://img-blog.csdnimg.cn/49249d422a764e32a8d692c6842c7c8c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_9,color_FFFFFF,t_70,g_se,x_16)



紧接着这台机器接收到这个数据包之后接收到这个arp请求之后，
这台机器就会返回我是某某某我的MAC地址是多少，然后收到响应会在本地建立一个叫做**ARP的表，这里面包含一下IP地址是多少，MAC地址是多少，**每一台都有，我们可以看一下，

![在这里插入图片描述](https://img-blog.csdnimg.cn/f8d01007eadd4c5cbfce75e836c1fb15.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



我们可以看一下在这里就没有192.168.2.217的地址

![在这里插入图片描述](https://img-blog.csdnimg.cn/53a1a728b5ff4ce098981327c123743f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_17,color_FFFFFF,t_70,g_se,x_16)



过一段时间是没了，是因为我们现在这台物理机的这个 arp表里面已经丧失了已经没有了，它已经超时了，这个 IP地址和MAC地址它的这个 arp表的这条信息已经超时了，

好，还有几个问题就是为什么不能这里面也有一个协议，叫做acmp协议。

这里面除了arp和acmp，还有一些其他的协议，比如一些广播上其他的给他发过来，我们这里识别不了你，可是乱码请注意这里面一个数据包都是有自己协议的，
它的表实效是因为现在windows电脑的arp表失效，
他发出的请求还在给他发，但是我现在时间接管网卡的这个应用程序接收完之后没有给他回响应，来理解这个打印

![在这里插入图片描述](https://img-blog.csdnimg.cn/6c7bbc52a62c4b86ab5598cdd228bab7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_12,color_FFFFFF,t_70,g_se,x_16)



就在这地方挺重要，这一层是说的网络层的协议，IP层如果我们在这个 Ip的这个协议地方，我们引入一个else if这个包等于arp的协议，好我们可以对它进行arp的处理，这是这第一个情况。
第二个情况，如果在这里面判断它是不是acmp协议

**arp是跟IP是一层的，icmp是跟udp一层的，它是传输层的，**也就是大家可以看到这里面所谓的网络分层，其实对于我们来说是先发的那一层协议，是一个谁先谁后的问题。

![在这里插入图片描述](https://img-blog.csdnimg.cn/048b8e510f5f4045b9bef9c83bae67f7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_13,color_FFFFFF,t_70,g_se,x_16)



**ICMP它是传输层的协议**，就是在这个定义的时候是在IP头里面定义的，是在IP头里面。

udp有这么几个特点，
第一个它的实时性比较强，
第二个就是他不带拥塞控制。
传输速度要比TCP快。

是在udp的基础上面，我们是需要封装一层应用协议的，如果不封装应用协议的话，那 udp它是没办法用的对吧？那封装的观点它也是个一对一的传承也是一个一对一的传输好吧？
udp协议它是用于哪一些场景？
nginx

原文作者：[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605019321

# 【NO.528】Linux服务器开发,手写死锁检测组件

前言：
当项目收尾阶段，毫无疑问都要进行死锁的检测。如果线程数量少，可以利用查看日志、GDB调试或者干脆看cpu占用率检测出死锁问题。现在问题来了，如果线程数量多，我们该如何操作呢？今天我们就来研究学习一下！





![在这里插入图片描述](https://img-blog.csdnimg.cn/9fb1fa360f324018b1fac17073ab30aa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_17,color_FFFFFF,t_70,g_se,x_16#pic_center)



# 1.死锁的构建

## 1.1 死锁cpu展示



![看不出来，换成spin_lock就清晰了！](https://img-blog.csdnimg.cn/43e8446442de45e48036d85339626ab8.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



## 1.2. 基础代码实现

```c
#include<stdio.h>



#include<stdlib.h>



#include<stdint.h>



#include<pthread.h>



#include<unistd.h>



#include<iostream>



pthread_mutex_t mtx1 = PTHREAD_MUTEX_INITIALIZER;



pthread_mutex_t mtx2 = PTHREAD_MUTEX_INITIALIZER;



pthread_mutex_t mtx3 = PTHREAD_MUTEX_INITIALIZER;



pthread_mutex_t mtx4 = PTHREAD_MUTEX_INITIALIZER;



void* thread_routine_1(void *)



{



    std::cout << __FUNCTION__ <<" Start..." << std::endl;



    pthread_mutex_lock(&mtx1);



    sleep(1);



    pthread_mutex_lock(&mtx2);



    pthread_mutex_unlock(&mtx2);



    pthread_mutex_unlock(&mtx1);



    std::cout << __FUNCTION__ << " End..." << std::endl;



}



void* thread_routine_2(void*)



{



    std::cout << __FUNCTION__ << " Start..." << std::endl;;



    pthread_mutex_lock(&mtx2);



    sleep(1);



    pthread_mutex_lock(&mtx3);



    pthread_mutex_unlock(&mtx3);



    pthread_mutex_unlock(&mtx2);



    std::cout << __FUNCTION__ << " End..." << std::endl;



}



void* thread_routine_3(void*)



{



    std::cout << __FUNCTION__ << " Start..." << std::endl;



    pthread_mutex_lock(&mtx3);



    sleep(1);



    pthread_mutex_lock(&mtx4);



    pthread_mutex_unlock(&mtx4);



    pthread_mutex_unlock(&mtx3);



    std::cout << __FUNCTION__ << " End..." << std::endl;



}



void* thread_routine_4(void*)



{



    std::cout << __FUNCTION__ << " Start..." << std::endl;



    pthread_mutex_lock(&mtx4);



    sleep(1);



    pthread_mutex_lock(&mtx1);



    pthread_mutex_unlock(&mtx1);



    pthread_mutex_unlock(&mtx4);



    std::cout << __FUNCTION__ << " End..." << std::endl;



}



int main()



{



    pthread_t th1, th2,th3,th4;



    pthread_create(&th1,NULL,thread_routine_1,NULL);



    pthread_create(&th2,NULL,thread_routine_2,NULL);



    pthread_create(&th2, NULL, thread_routine_3, NULL);



    pthread_create(&th2, NULL, thread_routine_4, NULL);



 



    pthread_join(th1,NULL);



    pthread_join(th2,NULL);



    pthread_join(th3,NULL);



    pthread_join(th4,NULL);



    return 0;



}
```

## 1.3 简单理解记录

刚开始King老师演示了两个线程两个锁的情况，不过死锁的情况只是偶现，偶现的问题往往最难排查，后来在线程函数中加上sleep(1)，踏踏实实说一秒，线程抢占之间存在稳态就变为了必现，可以看出King老师对死锁这件事有一个超乎常人的看法。
代码为四个线程抢占四个锁的额资源，线程虽然只是增加了两个，但是代码的复杂程度直线飙升，大量重复冗余的代码也随之出现。在这里如果能运用函数指针进行赋值，感觉也许会更好些。
如果屏蔽掉线程四的内容，锁之间没有闭环，死锁瞬间解决，这会不会也是一个新的思路呢？当遇到复杂棘手的困难问题，也许先尽可能的简化，这才是更快的解决问题的方法吧？

## 1.4 死锁终端显示



![在这里插入图片描述](https://img-blog.csdnimg.cn/0ef5bde3578240169eb5d76e44692e76.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



# 2.pthread的hook

如何知道线程和锁的对应关系，哪把线程占用哪把锁？hook就是解决这个问题。

## 2.1 代码增改

### 2.1.1 pthread_mutex_lock_f、

```c
typedef int (*pthread_mutex_lock_t)(pthread_mutex_t * mutex);



pthread_mutex_lock_t pthread_mutex_lock_f;



typedef int (*pthread_mutex_unlock_t)(pthread_mutex_t * mutex);



pthread_mutex_unlock_t pthread_mutex_unlock_f;
```

- 这里笔者觉得完全可以定义成一个函数指针，因为笔者是个懒汉。

### 2.1.2 pthread_mutex_lock()

```cpp
int pthread_mutex_lock(pthread_mutex_t* mutex)



{



    std::cout << __FUNCTION__ << " Start..." << std::endl;



    std::cout <<"pthread="<<pthread_self()<< " mutex=" <<mutex<< std::endl;



    pthread_mutex_lock_f(mutex);



    std::cout << __FUNCTION__ << " End..." << std::endl;



}
```

### 2.1.3 pthread_mutex_unlock()

```cpp
int pthread_mutex_unlock(pthread_mutex_t* mutex)



{



    std::cout << __FUNCTION__ << " Start..." << std::endl;



    pthread_mutex_unlock_f(mutex);



    std::cout << __FUNCTION__ << " End..." << std::endl;



}
```

### 2.1.4 init_hook()

```c
static int init_hook()



{



    pthread_mutex_lock_f =dlsym(RTLD_NEXT,"pthread_mutex_lock");



    pthread_mutex_lock_f =dlsym(RTLD_NEXT,"pthread_unmutex_lock");



}
```

- 参数1：RTLD_NEXT 表示fd，从这个系统库中获取
- 参数2：pthread_mutex_lock: 第二个参数表示标签，是函数的位置
- 返回值：phtread_mutex_lock_f 表示函数指针，记录一个和目标函数同类型的地址

### 2.1.5 终端输入man dlsym，查看外挂专用函数dlsym()



![在这里插入图片描述](https://img-blog.csdnimg.cn/c085411425cd483b98b0aed715cb9aff.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



### 2.1.6 宏和头文件

- _GNU_SOUCE宏会在预编译阶段打开<dlfc.h>中的部分功能参与编译

```c
#define _GNU_SOUCE



#include <dlfc.h>



//int main()函数中记得执行初始化函数！



init_hook（）;
```

## 2.2 编译命令

- gcc -o deadlock deadlock.c -lpthread -ldl
- 记得加上库一起编译

## 2.3 死锁状态



![在这里插入图片描述](https://img-blog.csdnimg.cn/7fcf89b650254f6397b038fee1de5fb2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



# 3.图的构建

dfs深度优先算法，依次访问每一个节点，标记置1，如果下一个节点已经访问了那说明有环产生。

## 3.1 原理

线程占用的锁存入locklist当中，其他线程再想占用时先前去申请。

## 3.2 结果



![在这里插入图片描述](https://img-blog.csdnimg.cn/ad77320173f241cd9f8f609bbc24fb03.png)



# 4.三个源语的构建

## 4.1 示意图



![在这里插入图片描述](https://img-blog.csdnimg.cn/9507a5a8b7b14ae7aa6b193df7f211df.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_18,color_FFFFFF,t_70,g_se,x_16)



## 4.2 分析图



![在这里插入图片描述](https://img-blog.csdnimg.cn/ce5545d40e404f6c96b37059cf7f3119.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



## 4.3 逻辑思考

追求女孩之前，要先去获得目标的情报，对方有没有男朋友？有没有谈恋爱的意愿，这样才能事半功倍。
当心仪对象发出自己的意愿，都应该有对应的逻辑考虑。当她愿意跟咱搞，咱应该先进行朋友圈官宣，然后让其抹去前任的记忆，再去拉手等更多肢体上的接触，要有条不紊的进行。
分手的时候，应该互相抹去共同的记忆，释放各自的资源，把她的昵称改为“渣女”，也许有一种爱真的叫放手吧！

## 4.4 代码增改

```c
#define MAX 100



enum Type{PROCESS,RESOURCE};



struct source_type{



        unint64 id;



        enum Type type;



        unint64 lock_id;



        int degrssl



}



struct vertex{



        struct source_typ s;



        struct vertex *next;



};



struct task_graph{



        struct vertex list[MAX];



        int num;



        struct source_type locklist[MAX];



        int lockidx;



    



        pthread_mutex_t mutex;



};



struct task_graph *tg=NULL;



int path[MAX+1];



int visiited[MAX];



int k=0;



int deadlock = 0;



struct vertex *create_vertex(){



    struct vertex *tex=(struct vertex *)malloc(sizeof(struct vertex));



    tx->s =type;



    tex->next=NULL;



    return tex;



};



int serach_vertex(){



    for(int i{};i<tg->num;i++){



            if(tg->list[i].s.type==type.type && tg->list[i].s.id == typpe.id){



                            return i;



            }



    }



    return -1;



};



void add_vertex(){



        if(serch_vertex(type)==-1){



        tg->list[tg->num].s=type;



        tg->list[tf->num].next=NULL;



        tg->num++;



    }



};



int add_edg(struct sourec_type from,struct source_type to){



        add_vrtex(from);



        add_vertex(to);



        struct vertex *v = &(tg->list[idx]);



        while(v !=NULL){



        if(v->s.id==j.id) return 1;



        v=v->nex;



        }



};



    return 0;



int remove_edge(struct sourece_type from,struct source_type to){



        int idxi=search_vertex(from);



        int idxj=search_vertex(to);



        if(idxi !=-1 && idxj !=-1){



            struct vertex *v=&tg->list[idx];



            struct vertex 



        }



    }  



int DFS(int idx) {



    struct vertex *ver = &tg->list[idx];



    if (visited[idx] == 1) {



 



        path[k++] = idx;



        print_deadlock();



        deadlock = 1;



        return 0;



    }



    visited[idx] = 1;



    path[k++] = idx;



    while (ver->next != NULL)



     {



        DFS(search_vertex(ver->next->s));



        k --;



        ver = ver->next;



    }



 



    return 1;



 



}    
```

# 5.启动线程检测

命令：gcc -o deadlock_succeess deadlock_success.c -lpthread -ldl

# 6.调试运行



![在这里插入图片描述](https://img-blog.csdnimg.cn/c14ba707f2d6427895786eb8250061c3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



# 7.总结

通过本节King老师的讲解，小生对检测死锁组件这部分知识有了初步的认识。老师不仅传授了知识，在对人生伴侣方面也给了我一定得启发。尽管代码自己感觉还是没有吃透，图的构件这一部分知识并没有很好的掌握，但是我会继续努力的研究出个所以然，自己的心态过于急于求成，应该戒骄戒躁，踏踏实实把这块知识重新掌握。如果不能解决，就不要前进。
接下来还会利用C++11中的std::thread线程新特性写出一个不一样风格的检测死锁模块，各位敬请期待吧！

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/604966093

# 【NO.529】海量数据去重hash与布隆过滤器

## 1.背景

- 在使⽤word⽂档时，word如何判断某个单词是

- 否拼写正确？

- ⽹络爬⾍程序，怎么让它不去爬相同的url⻚⾯？允许有误差

- 垃圾邮件（短信）过滤算法如何设计？允许有误差

- 公安办案时，如何判断某嫌疑⼈是否在⽹逃名单中？控制误差 假阳率

- 缓存穿透问题如何解决？允许有误差

- **缓存穿透**

  

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/8f1a03c8b17249e7b4afc4b1715b484a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)

- 描述缓存场景，为了减轻落盘数据库（mysql）的访问压⼒，在server端与mysql之间加⼊⼀层缓冲数据层（⽤来存放热点数据）；

- 缓存穿透发⽣的场景是server端向数据库请求数据时，缓存数据库（redis）和落盘数据库（mysql）都不包含该数据，数据请求压⼒全部涌向落盘数据库（mysql）。

- 数据请求步骤：如上图 2 的描述；

- 发⽣原因：⿊客利⽤漏洞伪造数据攻击或者内部业务bug重复⼤量请求不存在的数据；

- 解决⽅法：如上图 3 的描述；

## 2.需求

- 从海量数据中查询某字符串是否存在。

## 3.set和map

- c++标准库（STL）中的set和map结构都是采⽤红⿊树实现的

- 图结构示例

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/67042995564c4fbca23ccf901dff40c7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)

- 对于严格平衡⼆叉搜索树(AVL)，100w条数据组成的红⿊树，只需要⽐较20次就能找到该值；对于10亿条数据只需要⽐较30次就能找到该数据；也就是查找次数跟树的⾼度是⼀致的；

- 对于红⿊树来说平衡的是⿊节点⾼度，所以研究⽐较次数需要考虑树的⾼度差，最好情况某条树链路全是⿊节点，假设此时⾼度为h1，最差情况某条树链路全是⿊红节点间隔，那么此时树⾼度为2*h1;

- 在红⿊树中每⼀个节点都存储key和val字段，key是⽤来做⽐较的字段；红⿊树并没有要求key字段唯⼀，在set和map实现过程中限制了key字段唯⼀。我们来看nginx的红⿊树实现：

```cpp
// 这个是截取 nginx 的红⿊树的实现，这段代码是 insert 操作中的⼀部分，执⾏完这个函数还需要检测插⼊节点后是否平衡（主要是看他的⽗节点是否也是红⾊节点）



// 调⽤ ngx_rbtree_insert_value 时，temp传的参数为 红⿊树的根节点，node传的参数为待插⼊的节点



void ngx_rbtree_insert_value(ngx_rbtree_node_t *temp, ngx_rbtree_node_t



                             *node,



                             ngx_rbtree_node_t *sentinel)



{



    ngx_rbtree_node_t **p;



    for ( ;; )



    {



        p = (node->key < temp->key) ? &temp->left : &temp->right;// 这⾏很重要



        if (*p == sentinel)



        {



            break;



        }



        temp = *p;



    }



    *p = node;



    node->parent = temp;



    node->left = sentinel;



    node->right = sentinel;



    ngx_rbt_red(node);



}



// 不插⼊相同节点 如果插⼊相同 让它变成修改操作 此时 红⿊树当中就不会有相同的key了定时器 key 时间戳



// 如果我们插⼊key = 12，如上图红⿊树，12号节点应该在哪个位置？ 如果我们要实现插⼊存在的节点变成修改操作，该怎么改上⾯的函数



void ngx_rbtree_insert_value_ex(ngx_rbtree_node_t *temp, ngx_rbtree_node_t



                                *node,



                                ngx_rbtree_node_t *sentinel)



{



    ngx_rbtree_node_t **p;



    for ( ;; )



    {



// {-------------add-------------



        if (node->key == temp->key)



        {



            temp->value = node->value;



            return;



        }



// }-------------add-------------



        p = (node->key < temp->key) ? &temp->left : &temp->right;// 这⾏很重要



        if (*p == sentinel)



        {



            break;



        }



        temp = *p;



    }



    *p = node;



    node->parent = temp;



    node->left = sentinel;



    node->right = sentinel;



    ngx_rbt_red(node);



}
```

- 另外set和map的关键区别是set不存储val字段；
- 优点：存储效率⾼，访问速度⾼效；
- 缺点：对于数据量⼤且查询字符串⽐较⻓且查询字符串相似时将会是噩梦；

我们知道了第一个流程呢我们是要找到这个位置，然后呢把它设置为红色节点，找到红色节点之后呢我们就需要重新着色，如果我们的节点是一样的，颜色是一样的话都是红色的话，如果我们插入的节点跟它的父节点都是红色的话，那么我们就不需要重新着色，可能如果这个树黑节点的高度不一致，呢我们还需要进行左旋转右旋转对不对？
好，那么在这里呢是插入找到代理你们这一段代码，它主要是找到我们带插入的节点的位置，找到这个位置并着色。大家看到了最后一行呢就是给我们这个节点进行着色。
**这个函数的作用？**我们主要是做了两个工作，第一，找到待插入节点的位置，然后呢我们就将这个节点设置为红色，
好，当然后面当然不是我们这个函数，啊接着还需要就是预父节点判断，对不对？颜色比较如果都是红色，那么我们就需要重新着色，对不对？好，重新着色之后，如果引起数的不平衡，重新着色，如果引起了不平衡，就是黑色节点不平衡，那么还需要通过旋转来达到平衡的作用。
现在我们这个函数是这两点，就是1和2，我们这个功能的作用呢就是1和2。
我们这个函数的作用它就是我们的1和2这个操作。
那么我们来看一下，那么我们是怎样查找这个节点呢我们来看一下这个判断。

大家看一下这个判断这个判断呢就是告诉我们，首先跟我们的这个节点比较，如果
大于它，大家看一下，如果这个诺的呢我们来跟大家说一下我们这个 temp，
这个 temp他传的参数呢是我们红色的根节点，这是红黑树的根节点，是我们的根节点，
而我们的node呢是我们待插入的节点，待插入的节点。
好，大家记住一下大家记住一下对双目运算这里，

![在这里插入图片描述](https://img-blog.csdnimg.cn/afe99f76f7aa46aea97a683860660271.png)



也就是说我们待插入的节点，比如说我们来插入一个12，大家来思考一下，我们插入12，怎么来找这个节点？
首先8我们跟8节点进行比较，它大于我们8，所以呢它往右边查找，往右边边查找。
大家看一下这个是小于temp是我们的根节点对不对？根节点。
如果我们小于它的话，现在我们是大于它这个点大于8这个节点，
大于8这个节点大于8这个节点是不是往右边走？那么就是15对不对？15，好，然后呢这个 temp会变成15这个节点，**这个 temp会变成15号节点，**
这个时候呢我们的12会跟15比较，它是不是小于我们的15, 12小于15，它往左边走，
是不是到12了？
大家注意这个时候我们待插入的节点是12，这个 temp也是指向12，我们现在待插入的节点是12，
而且跟我们比较的节点呢也是12，那么我们现在应该往怎么应该怎么走应该怎么走？
大家来看一下我们应该怎么走？
我们是往左边走还是往右边走？大家看一下这这张代码我们是往左边走还是往右边走？我们这个node等于temp key，这个是等于的，那么我们应该往哪边走？
往左边走还是往右边走？我们这个是等于它的是不是往右边走对不对？对，是往右边走。
我们的node,node就是我们代插入的节点是12，这个12小于12 12会小于12吗？显然是不成立的对不对？成立才往左边走，不成立往右边走，所以呢就是往14这边走对不对？好，那么现在12跟14比较了，那么我们往左边走还是往右边走？现在我们跟14号节点进行比较好，对我们往左边走，好我们到达13这个节点是往左边走还是往右边走？
是不是小于它是不是往左边走？是不是接着往左边走？那么我们现在插入的如果我们要插插入node等于12号节点的话，那么我们是不是插入这个位置？

我们12应该要插入在这里，所以这个告诉我们什么呢这段代码告诉我们什么呢?
**我们可以插入相同的key**，
那这个的结论呢就是我们可以插入相同的node，就是红黑树当中我们可以插入相同的节点，
我们可以插入相同的节点，那么插入相同的节点的
第一句是什么？就是我们这个运算是怎么做的，好，那么我们现在想要实现，
现在我们想要实现不能插入相同的节点，我们要实现一个不能插入相同的节点应该怎么做？
我们不插入相同的节点应该怎么做？我们是不是可以约定一下，如果插入相同的节点，让它变为赋值操作，这是修改操作。
对不对？那么如果相同的话，那么我们就变成修改的操作，那么我们应该怎么做？
大概改了一个版本，那么我们是不是可以在这个二目运算之前做一个判断？
如果相等的话，如果我们当前的t跟我们现在比较的k相等的话，那么我们是不是直接进行赋值就行了？
我们把这个值修改就行了。

![在这里插入图片描述](https://img-blog.csdnimg.cn/321f480c94814869ad9a6a6df651cd39.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_11,color_FFFFFF,t_70,g_se,x_16)



我们加入了这一个代码之后，我们就会只是变成修改操作，那么我们此时
此时大家注意了，此时我们这个红黑树当中就不会有相同的key了。
好，在这里呢也跟大家讲解一下就是我们的红黑树呢它可以用来实现我们的定时器，
定时器大家都知道我们如果用t做时间戳的话，
k存储我们的时间戳的话，我们是不是不能保证这个时间戳是否相同相同，我们可能会插入一个相同的时间戳进来，
也就是说我们的红黑树呢它可以存储相同的key，好在这里呢就跟啊这个颜色是需要改变的，我们刚刚在这里12需要重新着色的，
我们这个10号插入的时候它是一个红色节点，需要重新着色

那么呢我们来看一下我们的set跟map它的关键区别，什么叫关键区别？就是最重要的区别。
刚刚跟大家写了，我们红黑树的节点呢,它既存储我们的key也存储我们的value，是不是？我们的map就不存储这个 Value字段就行了。
就是这个呢就是在我们平常开发过程中呢,大家如果对红黑树很了解的话，我们就会
知道怎么去分析它我们的map，第二个字段的value值是不是不需要存储，因为我们
是跟map的key是我们比较的关键元素，这个 Key value这个key呢是我们用来比较的用来排序的对不对？用来比较的。
**好，我们来看一下他们的优点和缺点，**
如果呢大家思考一下我们的红黑树它是不是没有浪费空间，没有浪费空间，我们的key和value都是存储在我们的这个节点当中，它并没有额外的浪费空间去存储我们的元素，
这个应该能够理解吧？我们的红黑树呢它并没有浪费我们的格外的空间来存储我们的节点**，而且呢访问速度高效，当然这个访问速度高效呢**
是相对的对不对？如果我们存储一个整数那当然会很快对不对？
那么如果100万条数据比较20次，**如果我们是整数作为key的话，那么20次是不是很很快啊几乎可以忽略不计**对不对？那么1亿条数据是不是也可以忽略不计，就是我们去查询的时候，
那么它的缺点呢大家想想，如果我们的数据量，因为我们这个呢它会存储的key和value都需要存储，
**如果数据量大且我们查询字符串比较长的时候，我们比较的它是比较麻烦的。**
**如果我们这个字符串像我们的邮件跟短信作为key的话，那么我们比较起来呢会非常耗时，它的效率呢是非常低的**，如果我们的查询字符串相似的时候，什么叫相似？
**字符串相似的时候，它的比较次数,比较呢就会越耗时**，这个大家应该能理解吗？就是字符串相似，那么我们比较起来呢也会耗时间，那么如果字符串比较长，它也会耗时间，数据量大，我们存储的也比较多。
那么我们来思考一下另外一种数据结构，也就是我们的STL当中那个unordered_map

## 4.unordered_map

- c++标准库（STL）中的unordered_map<string, bool>是采⽤hashtable实现的；

- 构成：数组+hash函数；

- 它是将字符串通过hash函数⽣成⼀个整数再映射到数组当中；它增删改查的时间复杂度是o(1);

- 图结构示例：

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/998161296326490486bce207857d9dca.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)

  

- hash函数的作⽤：避免插⼊的时候字符串的⽐较；hash函数计算出来的值通过对数组⻓度的取模能随机分布在数组当中；

- hash函数⼀般返回的是64位整数，将多个⼤数映射到⼀个⼩数组中，必然会产⽣冲突；

- 如何选取hash函数？

  1.选取计算速度快；
  2.哈希相似字符串能保持强随机分布性（防碰撞）；

- murmurhash1，murmurhash2，murmurhash3，siphash（redis6.0当中使⽤，rust等⼤多数语⾔选⽤的hash算法来实现hashmap），cityhash都具备强随机分布性；测试地址如下：
  https://github.com/aappleby/smhasher

- 负载因⼦：数组存储元素的个数/数组⻓度；负载因⼦越⼩，冲突越⼩；负载因⼦越⼤，冲突越⼤；

- hash冲突解决⽅案：

  - 链表法
    引⼊链表来处理哈希冲突；也就是将冲突元素⽤链表链接起来；这也是常⽤的处理冲突的⽅式；但是可能出现⼀种极端情况，冲突元素⽐较多，该冲突链表过⻓，这个时候可以将这个链表转换为红⿊树；由原来链表时间复杂度 o(n) 转换为红⿊树时间复杂度 ；那么判断该链表过⻓的依据是多少？可以采⽤超过256（经验值）个节点的时候将链表结构转换为红⿊树结构；
  - 开放寻址法
    将所有的元素都存放在哈希表的数组中，不使⽤额外的数据结构；⼀般使⽤线性探查的思路解决；

  1. 当插⼊新元素的时，使⽤哈希函数在哈希表中定位元素位置；

  2. 检查数组中该槽位索引是否存在元素。如果该槽位为空，则插⼊，否则3；

  3. 在 2 检测的槽位索引上加⼀定步⻓接着检查2；

     加⼀定步⻓分为以下⼏种：

     1. i+1,i+2,i+3,i+4 ... i+n
     2. i- ,i+ ,i- ,1+ ...

这两种都会导致**同类hash聚集**；也就是近似值它的hash值也近似，那么它的数组槽位也靠近，形成hash聚集；第⼀种同类聚集冲突在前，第⼆种只是将聚集冲突延后；
另外还可以使⽤双重哈希来解决上⾯出现hash聚集现象：

> 在.net HashTable类的hash函数Hk定义如下：
> Hk(key) = [GetHash(key) + k * (1 + (((GetHash(key) >> 5) + 1) %
> (hashsize – 1)))] % hashsize
> 在此 (1 + (((GetHash(key) >> 5) + 1) % (hashsize – 1))) 与 hashsize
> 互为素数（两数互为素数表示两者没有共同的质因⼦）；
> 执⾏了 hashsize 次探查后，哈希表中的每⼀个位置都有且只有⼀次被访问到，也就是说，对于给定的 key，对哈希表中的同⼀位置不会同时使⽤ Hi 和 Hj；
> 具体原理：https://www.cnblogs.com/organic/p/6283476.html

- 同样的hashtable中节点存储了key和val，hashtable并没有要求key的⼤⼩顺序，我们同样可以修改代码让插⼊存在的数据变成修改操作；
- 优点：访问速度更快；不需要进⾏字符串⽐较；
- 缺点：需要引⼊策略避免冲突，存储效率不⾼；空间换时间；

也就是我们的c++标准库（STL）中的unordered_map<string, bool>。
好，大家应该使用过对不对？它是不是使用起来，使用起来跟我们的map没什么多大差异，对不对？跟我们的map使用起来呢没有多大的差异对不对？
那么他呢是如何来实现，啊有的朋友没有使用过他们呢 unordered_map跟我们的map使用起来没什么差异，它也是一个key和value。

它使用起来没什么差异，那么我们来看一下他们到底有什么不同，大家平常在工作的过程当中如何来选择map或者来选择unordered_map，我们应该如何来选择呢？
好，在这里呢首先我我们需要明确的是unordered_map呢它是由哈希table来实现的,
unordered_map它的构成呢就是数组加上我们的哈希函数，数组加上哈希函数，也就是跟我们这个图的结构呢很类似，

![在这里插入图片描述](https://img-blog.csdnimg.cn/5b268bafe4aa434abbee766e2a2454d3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



跟我们这个结构很类似，就是我们准备一个数组 ，然后呢我们每一个数组里面呢它会用链表把同时哈希到相同槽位的把它链接起来，这样的一个结构就是哈希函数，哈希函数是将我们的字符串，避免插⼊的时候字符串的⽐较；hash函数计算出来的值通过对数组⻓度的取模能随机分布在数组当中;
比如说我们今天的任务就是将字符串去一个海量字符串库当中去查询是否存在，那么
我们这个哈希函数呢就是对我们的这个 key这个 key进行哈希，
然后生成一个整数，生成整数之后，这个一般是我们64位的整数，64位的整数呢然后再映射对我们的这个数组长度进行取余。
比如说我们这个长度是12，那么生成的整数呢对我们的12进行取余，取余之后呢我们就会落到我们这个数组当中的某一个位置当中，就是将我们这个数
在位置当中，我们的这个哈希的数据结构，它就是将我们的横串通过哈希函数哈希成一个整数，然后映射到我们这个数组当中，然后我们就将这个 key和value存储在我们的这个数组当中，好，就跟我们当前的这个结构呢比较类似。
好，那么我们就需要明确一个概念了，
**我们为什么要使用哈希函数？为什么要使用哈希函数？**

当然大家是否还记得我们前面使用map结构的时候，如果字符串很长的话，我们的说查询效率高，好这效率高是其次，那么我们前面map大家还记得吧我们的红黑树它进行节点比较的时候，它需要比较key
对不对？我们需要比较key那么我们按unordered_map呢它就解决一个我们避免了字符串的比较，
大家明白吗？
我们去插入节点的时候，呢我们不需要跟他们进行比较，我们直接通过一个哈希函数映射到某一个位置，
那么我们去查找它的时候，呢同样的是通过这个哈希函数映射到相同的位置，对不对？它会哈希到相同的位置，
那么我们是不是不需要比较，我们哈奇的函数是不是解决了一个比较的问题？
**我们的map虽然它的查找效率高，但是比较字符串是非常的慢**。

当然如果是有冲突的时候呢是需要比较的，我们落到这个曹位就认为他
就是如果这里有冲突的时候，呢我们会要跟key依次的比较。
那么如果他们都映射到这个位置，映射到这个位置他们的哈希值，
然后再跟我们的数组取余以后呢,它们落在相同的位置，那么呢我们还需要比较这两个字符串的，
我们如果有冲突的情况下，依然要比较我们的字符串，依然要进行字符串的比较，
我们只是插入操作的时候不需要比较.

取模之后呢它能够随机分布到数组当中，
随机的分布在数组当中，也就是说我们哈希函数的目标呢就是想让我们插入节点的时候，呢能够随机分布到我们这个数组当中，这样我们就避免了比如说这里产生冲突了，产生冲突了，我们是不是还需要进行字符串的比较，如果查找的时候还是需要进行字符串的比较，那么我们来看一下它的哈希函数呢它的目标呢是想让我们能够随机的分布在我们的数组当中，
我们的哈希函数呢一般返回的是64位整数，我们将一个64位的整数插入到一个很小的数组当中，
那么很有可能会产生冲突，必然会产生冲突，为什么会产生冲突？
呢在这里呢对优化了比较次数，那么我们来看一下为什么
大家来看一下为什么我们会必然产生冲突？
假设我们通过两次哈希函数，第一个哈希函数呢哈希出来的值是一，
第二个哈希函数出来的值呢是5，那么如果我们把它插入到4当中，
如果我们把它映射到4这个再字等于4的数字当中，是不是我们的一进行举？
一对我们的次序模，然后等于1，对不对？那么5呢
对我们的4取模是不是等于1？也是等于1，那么他们是不是都落在我们的一号槽位当中，这个数组的一号槽位当中？
对，
我们的磁均类型的红黑树呢是比较慢的，那么我们来看一下，

好，那么我们来看一下size等于8的时候，size等于8的时候是不是一个映射到1，一个映射到5，那么这里是不是就避免了一个
就没有冲突了，这个能看明白吗？
就是同样的是1和5，我们的数组的长度呢不同，呢它们可能就一个产生冲突，另一个不产生冲突，这个应该能看明白吧？
好，那么我们应该如何来选择哈希函数呢？
我们根据哈希函数的作用，我们就知道如何来选择了，我们首先是不是要选择计算速度快的？
我们通过一个长的字符串，经过我们的哈希函数能够快速的算出我们64位整数，对不对？
那么第二个呢就是我们字符串这个操作能够保持一个强随机分布性，如果什么叫强随机分布性？就是尽量的减少冲突次数，我们的朋友呢全部落在我们的数字当中，减少避免的避免我们的冲突，对不对？

那么我们一般选取怎样的哈希函数呢像murmurhash1，murmurhash2，murmurhash3，siphash（redis6.0当中使⽤，rust等⼤多数语⾔选⽤的hash算法来实现hashmap），随机分布性就是比如说我们插入一个MAC一跟MAC二，他们哈希出来的值呢会间隔比较大，它不会像我们的一个有聚集作用，比如说我们的MAC一哈希到三这里，那么Mark二呢
他知道四这里这种呢就是一种聚集的一种聚集的，就是一个不具备强随机分布性的，
那么如果我们的一个在2,一个在11，它们之间间隔距离很远，那么呢它这种呢就具备强随机分布性。
好那么一般呢我们就使用了这些
murmurhash哈希系列跟我们的siphash跟我们的c里哈希呢他们都具备一个强随机分布性。

我们再来聊聊一下负载因子，什么叫负载因子？就是我们这个数组
我们在这里是不是我们的哈希table它是由数组跟哈希函数呢来构成的，那么我们这里插入的元素跟我们这个数组的长度的关系的比值呢就是我们的这个负载因子，也就是说我们数组存储元素的个数跟我们这个数组的长度的比值呢就是负载因子。

就是我们这个数组，占用率的问题，我们存储的元素占我们这个长度总共占了几个位置，对不对？当然这里可能会产生冲突，对不对？
那么负载因子越小的话，冲突就越小，什么叫负载因子越小？
呢就是我们有一个大数组，
对数组越长冲突越小，对我们是不是就冲突就越小，负载因子越大，那么呢我们的冲突呢就会越大。
比如说大于一的情况下，那必然会产生冲突对不对？什么叫大于一？就是我们存储的元素都大于我们这个数组的长度了，那么必然会产生冲突，
对不对？这个就必然大于一的时候必然冲突。
就是冲击原理，比如说7个苹果放到6个箱子里面，是不是至少有1个箱子里面有2个苹果，这个应该很容易理解，
大于一的时候必然会产生冲突，对不对？
好，那么我们再来看一下，大家一定要认真听，啊现在非常关键，因为我们的哈希冲突的解决方案呢就直接引出了我们今天的
因今天的主角，也就是我们的速溶过滤器，我们来看一下我们的哈希冲突它有哪些解决方案。
在这里呢 演示了两种哈希冲突的解决方案，为什么只列出这两种，呢因为这两种呢是使用的最多的两种，其他的都是不那么好用的，而且我见到过的当中呢是没有使用的，
只是在文章中搜到了有那种，但是呢我在其他的解决冲突的方案方案当中呢没有看到他们使用过，
我只看到过这两种的使用方法，就是解决冲突的两种方法，第一种呢就是我们的链条法，啊我们的临时这个结构就是1013号
对这个情况要说的很重要那么，这个呢就是为了避免内存的大量的内存消耗大量的内存，因为
这里会有一个负载因子，负载因子呢就说明我们这个数据呢有很多像我们这张图像这张图里面
是不是有的数组的曹魏它是没有存储东西的，像69 10 11它并没有存储东西，并没有存储内容。
好，那么我们接着来讲练表法，列表法呢是大多数使用
大多数的解决方案，像我们的radius当中，它就是使用的链表法，链表法呢就是将冲突的元素呢
用链表的方式呢把它链接起来，将这个节点把它链接起来，这个呢就是链表法常用的这个是比较常用的处理冲突的方式，
但是呢它可能会出现一种极端的情况，大家可思考一下，比如说在这里呢它这里有三个节点，呢
有三个节点发生冲突了，那么呢我们在这里会链接很长，大家是否还记得我们的map，
他的最差的情况是不是要比较20次，比如说我们有100万的数据要比较20次，那么如果我们的哈希冲突这里有20个的话，那么是不是效率也会很低，
对不对？
那么如果像这种哈希冲突很严重的朋友呢呃很严重的情况下，呢我们就可以
将这个链表转化成红黑树转化成红黑树，这样呢我们又可以
减少比较的次数，又可以减少比较的次数，在我们的Java Java八大中呢就是这么处理的，就是我们的Java语言当中呢他是这么处理的，
就是我们可以呢将这个链表如果这个链表过长，我们就把它转化成红黑树。
那么
因为我们的电表它的时间复杂度是on也就是我们最差的情况下，要将所有的电表都需要比较，
那么如果我们把它转成红黑树，是不是又转化成o log了，那么我们的比较次数呢能够大大的减小，对不对？
那么我们再来看一下，那么在实际应用过程当中，我们这个列表过长判断这个列表过长的
据是什么，我们可以采用超过256或者是512，这个都有看到过就是其他的开源项目当中有使用512的，也有使用256的，那么呢这个呢就是一个经验值。
就是超过这个节点的时候，我们就将这个链表结构转化为红黑树的结构，这样呢我们就减少这个链表的比较的次数，这个是我们的链表法。
好，那么第二种方面表法呢它并没它引入了一个新的结构，也就是说它引入了一个链表的结构呢来存储我们的数据对不对？
引入了链表的方式，那么另外一种呢**开放寻址法**，它就是直接将数据存储在我们的数据当中，不引入任何其他的数据结构，
不引入任何其他的数据结构，这个就是我们的开放寻执法，一般呢是使用我们的线性探查的这样的一个解决思路，
对超过这个数字呢就无法忍受，

## 5.总结

**红⿊树和hashtable都不能解决海量数据问题，它们都需要存储具体字符串，如果数据量⼤，提供不了⼏百G的内存；所以需要尝试探寻不存储key的⽅案，并且拥有hashtable的优点（不需要⽐较字符串）**

我们首先呢是从这个背景引入的，背景引入
得出一个结论**就是我们是用一个字符串去海量一个数据数据串是字符串的一个库当中呢查询这个字符串是否存在**，
这是我们的一个总体需求，然后呢还跟大家总结了一下就是我们的**缓存穿透**的问题，他是怎么我们在面试的过程中如何来回答这个问题，以及
我们这个场景是怎样子的，场景是怎样子的，是在什么样的情况下，他不是在写入数据，是在我们请求数据的时候
出现的问题，那么出现的问题是什么？

就是我们的redis跟mysql呢这个数据都不存在那么呢我们可能黑客就可能利用此漏洞，就不妨的往我们的mysql灌数据对不对？往我的mysql数据，这样呢可能会让我们的mysql压力过大，可能会引起我们整个系统就陷入瘫痪，就不能对外提供服务，这样的一个情况呢就叫做**缓存穿透问题**。

那么我们的解决方案呢也有两种，第一种呢就不跟大家解释了，第二种就是我们需要在serve端这里一个数据结构能够快速的判断这个字符串是否在mysql当中，我们就用一个数据结构呢能够快速的判断这个数据是否在我们的mysql当中，好，这个呢就是我们的缓存穿透问题的解决，然后呢我们就引入了我们的set跟map。

为什么在这里需要跟大家介绍一下这个 set跟map呢？
为什么用set的map来引入这个问题，因为对这种处理方案不是很熟悉的朋友呢很容易联想到的一种解决方案
大家还记得这个还记得这个负载因子，我们的负载因子是不是要
小于一最好啊小于一，也就是我们的数组的长度要大于我们数组存储元素的个数，我们这个越大是不是我们的冲突就越小？

冲突太多，我们的效率就会变低了。
好，我们接着来回顾啊我们为什么要使用这个红黑树，就是我们的set跟map啊跟大家讲的，第一个是帮大家回顾一下我们前面文章当中红黑树红黑树它的
具体的实现以及我们插入的时候应该如何来插入，然后呢在这里也跟大家介绍了跟大家介绍了我们这个红黑树的节点，它可以存储相同keys的值，可以存储相同key的值，只是我们的map跟设置呢它做了运算对不对？
它做了这种判断，
也就是我们通过这种判断呢等于它我们就把它改成修改操作，我们插入的时候呢把它如果插入相同的元素，呢我们就把它改成修改操作，
这个是我们的色这个 map在我们的红黑树上面呢做了修改，我们的红黑树并没有限定我们，并没有限定大家一定要记住它，并没有限定我们是否可以存储相同，

我们这个有什么问题呢 set跟map来实现，我们是不是要将这个 key全部要存储，这是第一个，**我们需要全部存储。**
第二个呢**就是如果字符串比较长的时候**，大家
一起来回顾一下，就是如果字母算比较长的时候，我们就算值比较20次30次，那么呢也是一个非常大的操作，那么这个效率呢
最主要的是不是我们这个字符串长的时候我们不好解决。
还有一个就是我们的存这个数据量大，我们需要全部存储啊，是不是这两个问题？

那么我们先来解决这个字符串比较长的问题，
**字符串比较长怎么办？**因为我们的用红黑树来实现的话，我们就必然要进行字符串的比较，那么呢我们能不能解决一个不需要进行字符串比较，呢那么我们就引入了我们的unordered_map的办法，
它就不需要进行字符串的比较，因为我们是通过哈希函数将这个字符串将这个字符串呢把它
哈希到哈希成一个整数，然后呢再映射到我们的数组当中，这样呢我们就可以
可以不需要进行比较，我们下次来查找，这个比如说我们这个查找这个单位，我们查找单位这个节点，那么我们下一次
通过哈希函数是不是还是映射到这三这个位置，我们没有跟任何的字符串这里面的任何字符串进行比较

比如说我们查找daring这个节点，我们是不是还是将daring这个字符串通过哈希函数哈希成整数，然后又重新映射到三这个位置，
那么是不是我们没有跟任何字符串地址比较？
那么呢也这里就引出了我们哈希函数的作用，它的作用呢就是可以避免我们插入的时候呢一个字符串的比较。
同时我们哈希函数呢它还必须有必须要具备一个功能，就是它必须要具备一个强随机分布性。
大家思考一下，如果我们刚刚跟大家举个例子了，就是 Mark一跟Mark二，如果他们不具备强随机分布分布性的话，就是我们这个哈希函数，那么它们可能会相邻，如果相邻的话，我们下次插入的时候它
它的冲突的几率呢就会大大的增加，如果我们它的冲突的几率呢会大大的增加，
在这里呢也跟大家举了几个强随机分布性的几个行几个哈希函数，使用的最多的就是我们这两个，
第一个是murmurhash2，
我们的rads6.0之前，radies6.0之前呢就是使用的我们的murmurhash2，二大家也可以看一下源码，在这里呢也会有注释，我们的radish当中呢它有注释，
我们可以搜一下大家可以看一下我们的murmurhash2，呢在我们之前呢它是使用的我们的murmurhash2，
就是我们之前是采用的murmurhash2，，现在呢我们使用的是siphash，啊在这呢是注释当中呢
给我们的提示，也就是我们在6.0之前，它是采用的我们的murmurhash2，
我的等会的测试代码，也就是我们从线上代码当中抠出来的代码，呢它也是采用的我们的murmurhash2，
我们的siphash呢是我们的redis6.0当中目前使用到的，

想一下哈希冲突的解决方案哈希冲突呢它是必然产生的，为什么呢有两种情况必然产生哈希冲突？第一个第一种原有两种原因，第一种原因呢就是我们这个
**将一个很大的数映射到一个小数组当中，它很有可能就是必然会产生冲突。第二种呢就是我们可能是负载因子的原因，就是我们负载因子越大，**
比如说我们这个负载因子大于2>3，那么呢它必然会产生冲突的，因为我们的数组的长度都小于了都小于这个数组存储元素的个数了，那么呢它也会必然产生冲突。
那么我们能产生冲已经产生冲突了，我们就要考虑去怎么解决哈希冲突的问题。第一种呢就是我们的列表法也是常用的一种
像我们的redis6.0，它就是使用的我们的链表法，那么redis为什么使用我们的链表法？呢它主要原因是我们的radies6.0，**它对内存是有严格的要求的**，
**也就是说我们不能够浪费空间来来保证你的查找效率，所以呢我们就必须使用链表法，**并且呢还采用的是一个头叉法。

那么我们来讲一下今天最主要的一个方法就是开放寻址法，我们的列表法呢它是引入了一个数据结构来
来链接这个冲突的节点，那么我们的开放寻址法呢它是将所有的元素呢都放在这个数组当中，不使用额外的数据结构，
**它不使用任何额外的数据结构。**
好，那么我们的开放形式法呢它又有三种方法又有三种方法，第一种呢
好，我们先来讲解一下开放选址的它的一个步骤。
首先呢我们插入新元素的时候，使用哈希函数的去定位元素的位置，这个大家应该很很容易理解。
第二种呢就是我们索引判断这个索引是否存在元素，如果该槽位为空则插入，如果不为空那就是产生冲突了，产生冲突呢我们就有解决方案，因为我们这个
数组不为空，这个数组的曹魏不为空，那么其他的曹魏可能会为空对不对？那么我们就去尝试，
我们就去尝试其他的曹魏是不是没有元素对不对？那么我们的最开放巡视法最主要的核心思想就是
去其他曹魏没有元素的地方去探查什么叫探查，探查其他数组其他的曹魏没有元素的那些曹魏，
这个是我们的核心思想，那么有哪几种方式来探查？
呢第一种就是我们很容易想到的，
比如说我们当前插入的元素的索引，这种数组的索引是I那么我们可以I加一尝试一下，
就是我们往往右边移动一项，就是我们这个数组，比如说我们1号位置这里有有数据，噢我们5号这里数据有数据，那么我们加1我们在6号位置看一下有没有数据，如果没有数据，那么我们就插入到这个位置这是，这是第一种。

那么第二个问题
这个有什么问题呢是不是跟我们前面讲的聚集哈希是一样的，就是
我们把它放在一起了，那么如果
那么是不是很容易引起很容易引起下一次冲突？
是不是很容易引起下一次冲突，那么就引入了第二种方法，我们把这个步长加大一点我们把这个步长加大一点，
用平方，比如说我们先往左边探查，然后再往右边探查，就是减号就是往左边探查，
加号呢就是往右边探查，这是我们左右互相的探查，我们每一次探查的步伐呢把它扩大，就是给它一个平方，比如说3的平方9，
通过这种方式呢往左右两边探查，这样呢我们也可以解决前面这种聚集哈希的问题。
好，我们来解释一下什么叫同类哈希聚集。就是虽然我们对插入后那映射规则就不好锁定，
啊不是，我们当然我们下次查找还是要通过这种，我们是采用这种规则去探查的，那么我们下一次去查找这个位置的时候，呢也需要通过这种方法依次去探查，一直找到这个 T为止，
所以呢这个是比较麻烦的对不对？
我们是采采用这种策略去探查的，那么我们下次来查找这个元素的时候，依然要按照这个规则去查找那个元素，
这个应该能明白了，这个呢就是我们
为了避免同类哈希聚集，那么我们就把这个步长加大一点，加大一点，往左右两边的探查，左右两边依次去尝试探查。
当然如果我们插入节点之后，下一次同样的也要按照这种规则去探查，
打倒那个元素明白了吗？我们依然要按照这个规则去探查那个元素。好，
这个呢就是解释同类哈希聚集，也就是它的近似值，呢比如说我们的哈希它的就是我们这个字符串它的值相似的时候，它的哈希值呢理论上它也会比较近似，当然我们如果采用前面的这种墙随机
像随机分布式的哈奇函数呢这种情况比较少，但是不能说它不存在明白了吗？
就是它这种情况会比较少，它不是百%的，
这个墙是相对性的，也会比较靠近，也会比较靠近这个墙随机分布性，呢它是相对而言的，
那么这种他的哈希值也近似，所以呢他的数组的槽位呢也会比较靠近，
也会比较靠近，那么呢就会形成一个哈希聚集，也就是说我们通过这种
步长很短的会一直会发生碰撞，会一直发生碰撞，那么在这里可能会延伸到a加n
都没有找到曹魏，明白了吗？

那么我们来看一下，
我们第一种呢哈希冲突呢会马上出现，我们第二种大家思考一下，虽然是这种往左右两边进行扩张，他只是将我们的聚集冲突之后
两种方法停止，呃这个只要你规则上只要你规则定了，那么你下次查找再按照你这种规则依次去探查，那么也是可以的。
**我们第二种只是将聚集冲突延后了**，
我们虽然把步长增大了，但是呢它还会产生聚集冲突，**它只是把这个聚集冲突对没有根治，那么怎么根治呢？**

那么怎么根治这类问题？那么就出现了双重哈希，引入布隆过滤器的最重要的一个依据，最重要的一个依据就是我们的双重哈希解决上面的哈希聚集问题。
好，在这里呢是找到了一个对迟早会碰撞，那么我们怎么用双重哈希来解决上面的哈希聚集问题？呢在我们点那时当中，
也就是我们的c #语言当中，哈希table类呢它的哈哈希函数呢是这样定义的，大家看一下，这是它的哈希函数的定义。

> 在.net HashTable类的hash函数Hk定义如下：
> Hk(key) = [GetHash(key) + k * (1 + (((GetHash(key) >> 5) + 1) %
> (hashsize – 1)))] % hashsize
> 在此 (1 + (((GetHash(key) >> 5) + 1) % (hashsize – 1))) 与 hashsize
> 互为素数（两数互为素数表示两者没有共同的质因⼦）；
> 执⾏了 hashsize 次探查后，哈希表中的每⼀个位置都有且只有⼀次被访问到，也就是
> 说，对于给定的 key，对哈希表中的同⼀位置不会同时使⽤ Hi 和 Hj；

好，我们把它稍微移到这边来，
这种定义呢然后在这里呢做了一个运算，当然我们使用的是一个哈希函数，大家一定要明确一点，我们使用的是同样的哈希函数，
同样的哈奇函数通过这种运算，呢通过这种运算避免通过这种运算之后，大家看一下，这里面呢它会有一个
我们这一块跟我们的哈希赛子减一呢它互为素数，什么叫互为素数？也就是他们之间没有公费就是没有公约数，

那个呢比如说我们相邻的这个元素，比如说我们这个相邻的元素，相邻的元素跟它互值的话，那么同时也跟这个值进行互值。
他的户籍有传递性对不对？户籍具有传递性，那么是不是
通过如果他相邻的，他是不是也会找一个速速去探查，也会找一个速速的位置跟他相隔速速的位置呢进行探查。
好，这个有点抽象，大家一定要理解一下，就是我们素数呢它具有传递性，啊就是质数，互为质数，嘛它们具有传递性，
就是如果我们这个相邻的元素他们估值的话，
就是我们第一个元素他会找一个复制的地方，然后第二个元素呢也会找一个复制的地方，那么他们之间不会进行碰撞。
好，放在左边，好，
那么我们把这个移移移过来一点点，好，这个能听明白吧？
举个例子，
这个例子不好举，因为我们这个很难取个值出来，这个只能理解，就是我们通过这种质数粒子直观的多是吧？
那么我们应该怎么举例，呢这个因为是数学我也不好举例，
因为这个是数学我不好举例，这个只是
一种共识吧之一种共识，大家记住的前提就是我们可以能够给他移动次数的位置，这个是不是移动相当于给他移动一个位置，啊
就是我们相当于给他移动一个速速的位置，让他们之间减少减少碰撞，他是这么实现的。
通过这种方式呢我们能够减少碰撞，就是我就是有这个原理里面他看到了这里有原理，

就是最多的情况下会出现三次碰撞。
就是三就是我们这里会有三次就像我们刚刚画的这个图一样，
明白了吗？双重体现在哪里？双重主要体现在我们
我们这个哈希函数哈希函数这里是不是有一个补偿这个补偿的哈希函数呢又跟前面的哈希值呢？
这里就相当于两个哈希值，明白吗？这里就相当于一个哈希一，好，我们把这个东西把它简化一下，就相当于我们的哈希一加上我们的哈希二配备的
哈希二，明白吧？我们是通过不懂是吧？因为我们的后面呢对哈希的值，因为我们的哈希二呢是根据哈希一算出来的，
我们的哈希二呢是根据哈希一算出来的，我们实现不溶过滤器的时候呢也是采用这种思想，也是采用这种思想来实现的，
那么我们就相当于一个哈希一加上一个哈希二，采用这样的补偿呢进行
进行试探，就是我们的线性试探我们的试探方法呢就是这样的试探方法。
好，大家先记住这个，可以吧这个大家可以理解吧？
这个是哈希哈希一函数，我们这里哈希一函数这一波运算，呢我们把它看成看成哈希二，明白了吗？
好，这里呢不跟大家详细的解释解释了，这个这个是一个数学推论过来的。

我们主要的目的，是来看来看一下我们这个技术
他是如何来实现的，我们应该怎么来使用它，我们知道它的原理就明白了。
好对使用当然你也能够说出它的原理，
这个原因这个数学推论的原理就不用不用大家去学习了，这个有必要吗？我觉得没有必要。
我们不需要去创作这些哈希函数，有数学家帮我们去实现这些哈希函数，以及有数学家帮我们去研究这个理论，为什么这样能够减少碰撞？
明白了吧这个明白了，对，那得开个数学班了。好，那么我们在这里就过了具体原理，呢大家可以看一下。

好，同样的我们的哈希paper像我们的这个哈希冲突的解决方案，就是我们这个方案，呢就是我们这个哈希 table呢它的节点呢也同时存储了我们的key和value，
就是两个相似的字符上可能产生相似的哈希值，双重是不是再将两个相似的哈希值再做一次二？
次对可以这么理解可以这么理解。
就是我们当然呢这两次花期呢我们不是两个字符串之间，呢我们不考虑，我们每次考虑只考虑一个字符串，
但是呢我们并没有要求key的大小顺序，跟我们前面红黑树一样的，跟我们的前面红黑树一样的，我们同样的可以修改代码，
让插入存在的数据呢进行修改操作，这个大家应该容易理解我们在这里呢并没有要求我们t的大小顺序，
如果我们要修改那个字段的话，我们是不是也要
找到那个 Key，然后修改值，而不是变成插入操作，我们的hash table跟我们的红黑树是一模一样的，就是它
没有要求我们的是否一样，明白了吧？

好，那么我们接着来讲它的优点和缺点，它的优点呢
不需要进行字符串的比较，这个大家可以理解。
第二个呢就是访问速度更快，它的时间复杂度呢是o一，对不对？它的时间复杂度是o一，
o一就是一次就能够找到对不对？好，**那么它的缺点呢它需要引入策略来避免冲突**，因为冲突了的话，
像这里冲突了三次，那么如果这里是长字符串的话，我们依然要比较三次字符串，字符串的比较依然有三次对不对？
所以呢我们需要引入一个策略来避免冲突，减少冲突，对不对？它整体来说呢这是一个空间换时间的操作。
好，那么我们来看一下，
我们刚刚说了，我们需要大量的内存来存储这个 t和管理，像不管我们使用的是色子或者是map，还是使用的我们的unordered_map，
是不是都需要存储这个值？
就是我们前面讲的三个结构都需要存储这个值，大家思考一下，如果我们
海量垃圾邮件垃圾邮件，
大家思考一下，如果有11条垃圾邮件，11条垃圾邮件，这个邮件要内容又比较长的话，那么是不是要可能要占用GT的
内存，或者是几十g几百g的内存来存储来存储我们的t和value的TT值，这两个结构是不是都要存储我们的这个 string就是t值？
那么这样是不是并没有我们这几个结构是都不能解决我们前面的问题，我们的map或者以及我们unordered_map
都不能解决上面的问题，为什么不能解决？因为我们需要存储这个string，如果海量数据的情况下，我们没有这么大的内存，
那么如果我们不可能引入一个超级计算机吧对不对？
我们不可能用一个超级计算机来来解决一个这样的问题，嘛对不对？这个听明白的朋友慢慢说说完毕好吗？

## 6.布隆过滤器

我们这三个点，那么我们现在
不能过滤进来，也是在面试的过程当中经常会问到的，就是
面试官可能会问我们的过滤器是怎样的一个实现方案，他为什么可以解决这个问题？
那么今天呢我们来一步一步来分析，我们布隆过滤器它是怎样的一个结构？

布隆过滤器呢它既然我们在这里需要存储string，那么显然我们的布隆过滤器呢它是不需要存储的这个string的，那么我们看一下我们的布隆过滤器
解决不需要存储我们这个string也能够确定它存在还是不存在这个问题的。

好，我们来看一下布隆过滤器的定义，布隆过滤器的定义呢它是**一种概率型的**，大家一定要清楚。
在这里面呢有一个**概率性的数据结构**，在我们以还会跟大家讲解跳表，
跳表呢它也是一个**概率性的数据结构**，
大家已经看到了概率性呢它的这种数据结构呢它的威力非常强大，那么我们后面讲跳表的时候呢也会跟大家来讲解另外一种概率性的数据结构，
它的特点呢就是高效的插入和查询，**能够明确的告知某个字符串一定不存在，**
大家看到一定要注意，**一定不存在，可能存在，一定要注意这两个词一定跟可能也就是说我们不能够确定它**
**一定存在，我们不能确定它一定能存在，只能说它可能存在，但是我们能明确的知道它一定不存在，
这样是不是可以解决我们上面的问题。**

大家看一下我们上面的几个背景程序，除了这个公安办案除了这个公安办案以及我们的这个 Word查询，
这里面这三个是不是允许有误差？
这三种都是允许有误差的，
对不对？我们是允许有误差的，如果我们的第三第二种第一种跟第四种能够控制，**如果能够控制假阳率的话，也能够满足需求的话，那么我们不能过滤器呢解决这5个问题都是可以的。**好，那么等一下呢

**误差不是零容忍**，我们来看一下布隆过滤器，
布隆过滤器呢它相对于传统的查询结构，像我们刚刚说的这些结构呢它更加高效，不理解的好，更加高效，占用空间呢更小，
我们刚刚是不是上面的这里面第一个，
他们的问题就是要存储，就是我们占用空间大，然后呢高效呢谈不上对不对？
我们的map跟设置它需要对我们的字符串进行比较，
maps需要跟上进行比较，这个高校谈不上，那么我们的unordered_map呢它的高校就跟我们的冲突率相关了，unordered_map高校跟我们的冲突率相关，
如果冲突率比较大的话，也就是说我们这个链表很长的话，我们也会要进行很多次的比较，也会需要进行很多次的比较，才能够找到我们那个数据，所以呢这两个都是不能使用的。
好，这个我相信大家应该能够理解他们都谈不上高效，因为他们都有各自的缺陷，有一个共同的缺陷，就是他们要占用大量的内存，
那么我们的unordered_map呢比他们两个要稍微好一点，不需要进行字符串比较，如果冲突率很高的情况下，
那么也是不行的，那么如果我们引入双重滑行呢能够缓解这种问题，但是不能不能
解决这个问题。好，那么我们来布隆过滤器，呢它就是在我们的这三个基础上的吸收他们的优点，吸收我们的优点来进行
实现的。

好，在这里呢必须要跟大家明确一下他的缺点，这个缺点呢也是面试的过程中经常会问到的，他的缺点就是
**这个误差它是存在误差的，就是可能存在不能确定它一定存在。**
好，我们来看一下它是如何来组成的，我们刚刚的我们刚刚的哈希table呢它是数组加哈希函数，

而我们的布隆过滤器呢**它是位图加上n个哈希函数构成的**，那么我们来看一下位图，**什么叫位图？呢位图它的单位是我们的**
对，这是疑问，
呃一个比特位一个比特位。
对可以解决存储，我我们的不能过滤器呢主要**对不支持删除操作。**
对，他不支支持删除操作是他的最大的缺点，面试的过程中经常会问到就是他不支持删除操作，那么我们能不能
给他以就是改善他，让他支持删除操作的？等一下我们在后面的后面的源码讲析的过程中呢会跟大家讲解。
好，那么我们来看一下，
**它是位图加上n个哈希函数**，n个哈希函数，那么我们来首先来看一下位图是怎样构成的，我们如何来做位图来实现位图？
在c加加当中呢我们可以使用这种bite。
比如说我们在这里面跟大家举个例子，我们是不是可以使用on sine
差来来代替我们的这个盖子，对不对？这个盖子里面是不是有8个是不是有8个比特？
有8个比特对不对？有8个比特，那么我们用袋子8分8是不是能够构建一个64位64比特的位图？

就是我们如何实现位置这个位置？
好，我相信呢这个应该很容易理解很容易理解吧？好，那么我们要插入具体的位置，呢假设我们现在的哈希函数
这里有一个string通过哈希函数生成了173，那么我们怎么将173哈希到我们这个位图当中呢?我们怎么将173哈希到我们这个位图当中？呢
我们是不是首先要对我们整个长度整个位图的长度88 64对不对？那么我们需要对64取余，取余之后我们得出来的结果。
好，**我们对64取余就相当于对63进行取&操作，取&操作这就变成一个位运算的对不对**？位运算显然要快一些对不对？
那么位运算快一些，那么我们呢在这里呢是45，
那么45我们怎么样找到这个数组的位置，呢找到数组的位置，呢这个 I和j是在哪里？呢我们的横坐标跟竖坐标，
我们 I是不是我们的竖坐标？我们的j是不是我们的横坐标？那么我们的竖坐标和横坐标怎么来查找这个点？呢我们是不是可以对八取余呢就找到了我们的横坐标横坐标，它的位置是5，那么它的纵坐标
I是也是5，那么我们就找到55，那么我们将这个位置呢置为一，这个呢就是我们的一个哈希函数加上位图是如何来哈希的？

![在这里插入图片描述](https://img-blog.csdnimg.cn/d5c787fb56cd4a96ae8d10694ac12ff4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



就是第这个图告诉我们位图是如何来做。第二个呢就是我们怎么去
去置位，将我们的位图这个值置位，比如说我们将这个 Value映射到我们这个位图当中，那么我们就可以在5和5这个位置呢把它置为一
就可以了。
好，我们来看一下它的原理，**就是当一个元素加入位图时，k个哈希函数呢会将这个元素映射到我们位图中的k个点，**
位位于我们位图当中的k个点，并把它们制为一，并把它们制为一，也就是当我们检索的时候，呢再通过k个哈希函数检索的时候，就是我们查询的时候，
这个是我们插入的时候这个是插入的时候，这个是我们查询的时候，我们查询的时候呢也是通过这k个函数检测，这k个点是某个纬度，
如果有不唯一的点那么一定不存在，
比如说我们的是正一，它在这个位置这个位置跟这个位置是一，假设我们插入一个试卷，二它第一个位置在我们的
啊我们已经存在的这个位点里面，那么我们再看第二个哈希函数，第二个哈希函数它不在我们原来的这三个点当中，那么我们认为sting二肯定不等于string1
就是我们用QQ函数呢都通过三个哈希函数来进行到我们这个位置当中的不同的点，不同的点，那么我们去查询的时候呢我们就去
一也是通过这三个哈希函数哈希出三个点，看这三个位图当中的位置是不是都唯一，如果都唯一，那么说明它可能存在，如果有一个不唯一有一个不唯一，那么就说明它
肯定不存在，一定不存在。
这个呢就是我们的原理图，就是它的原理图，那么
我们位图中每个槽位它只有两种状态，不是0就是1，
对不对？我们像上图一样，我们的string一跟string二在这个位置，在这个位置它有被两个
映射到，它被两个映射到对不对？这里被两个地方两个呃4G1跟4G2都映射到这个位置了，
**那么我们就不知道他被映射了几次，不知道这个一被设置了几次对不对？也不知道被多少个string给哈希映射了，以及是被哪个哈希函数映射过来的，我们不确定是哈希一映射过来的，还是哈希二映射过来的，对不对？**
**所以呢，我们不能够支持删除操作。**
为什么我们不能过滤器，他不支持删除操作，因为
**我们这里每个槽位只有两个状态，我们不知道他被设置了几次，同时我们也不知道被哪个
哈希函数映射过来的，所以呢我们删除的时候可能会引起很多不同，对不对？**
好，我们知道这个不支持删除操作之后，我们来看一下我们在实际应用过程当中，呢我们的布隆过滤器应该如何来使用？
我们在这里是三个哈希函数，那么我们到底要选择多少个哈希函数？对，不确定性比较大，
那么我们应该选择多少个哈希函数？呢我们到底刚刚我们前面的有朋友已经问了，我们到底分配多少空间的位图呢？
我们能够这个位置当中能够存储多少个元素，呢因为大家都知道我们这里有一个
有一个我们有一个负载因子，所以呢我们尽量的为了减少冲突，呢应该是我们的数组长度要大于我们的存储元素的个数，那么在这里也是一样，**我们是不是这个位图的大小要大于我们
要带插入的字符串的个数要多一些，**我们这个数组要大于我们这个是这里是正一跟是正二是两个，我们的数组长度很大，对不对？
我们这个数组长度呢要大于两个字，
**那么我们如何我们能不能够控制假阳率呢？**
我们能不能控制假阳率？什么叫假阳率？就是不能过滤器能够明确一定一定不存在，不能够明确一定存在，那么存在判断是有误差的。
**也就是说我们错误判断存在的概率就叫假阳率，**我们
错误的判断这个东西存在的概率叫做假的率，就是比如说我们的是正一跟是正二，
好，在这里举个例子，为什么是选三字哈行？对误判率。对我是这里
来推导告诉我们要选举要选取多少个元素，
然后我们要选取多少个位图，就是这个空间位图手段空间以及我们这个函数，那么呢我们是有工序
公式，它会告诉我们应该选择这个这几个数值是多少。
好，这个假良心率能够听明白了不怕，这个如果不明白的话，那么好，那么我们在这里呢跟大家解释一下，
在我们的实际应用过程当中，呢我们一般是明确这两个，
**就是我们要往布能过滤器中存多少元素，这个是我们提前可以写好的，我们自己填的，我们人为填的。
那么第二个要填的是我们能够接受的假阳率，**
就是误判率就是我们能够接受的，比如说0.000，像我们这0.1有几个04个01个一对不对？是不是一夜
1/1亿对不对？那么就是1亿个里面只有一个出错，这个明白了吗？我们1亿次判断当中只有一个判断出错，
就是我们这个 P的数值填什么值，我们就是填这个值。
好，那么我们一般呢是根据这两个值去算m和km呢就是根据我们当前
不能问这种存在的元素跟我们的假药率，**我们来算出我们的位数应该要要有多少，我们的哈希函数的个数要有多少？**
好，那么呢这个是我们的实际应用场景，我们只会列出一个公式，大家呢直接带出来就行了，这个推导的过程我们就交给数学家他们来推导，我们直接用结论就行了，
我们直接用结论就行了，我们知道怎么用就行了，怎么用我们就知我们要填两个值，第一个是我们不能过这些元素的个数，以及我们能够接受的假阳率就是误判率。
好，那么我们就来选定这4个值，然后来看一下这个函，我们的这里呢也会有1个
网有一个网站，他呢把我们的噢在这里已经写好了，我们放在这边，这里有个网站大家看到n大家还记得它的概念，吧就是我们
提前要填的，我们往不能过滤器中需要插入多少条数据，假设我们插入4000条数据，那么呢我们在这里呢
是千千万/1，那么我们的假阳率呢能接受的是千万/1，也就是我们1,000万次判断的当中有一次出错，那么我们填这个值。
那么第三个参数跟第四个参数呢就由他们来计算，由他们来计算，我们提交一下让他们来计算。
那么它算出来的值呢就是这个是我们要用的空间的数量，以及算出来我们的k的值是多少。
我们可以根据这两个的计算的算出来我们这个 m和k的值，
就是我们到底要使用多少，这个位置的倒角是多少，以及我们这个哈希函数的个数是多少，那么呢我们就可以通过这个网站可以提前的预演。
同时呢我们也可以看出他们的规律，就是我们
让两个值不不变，比如说我们要看p和 n的关系p和n的关系，我们把mk
的值不变，也就是我们的存储空间位图的大小不变，以及我们party函数的不变

好，我们来看一下他们的关系，这个是他们的关系，我们这个的关系呢
我们让这两个值不变，这两个值不变。我们来看一下n和p的关系，p就是我们的假阳率的关系，那么n呢就是我们的
布隆过滤器当中的个数的关系。我们来看一下，随着我们数量的增加，也就是我们往不溶过滤器增加的数量增加，
那个权益一定要增加，它的假人率越来越大。我们n和b的关系呢就是随意的按计算账，我们的p呢它越来越大。
那么同样的我们来看一下第二个，我们固定住n和k我们固定住n和k我们固定住n和k
我们来看一下p跟m的关系，随着我们的内层的震荡，随着我们位图内层的震荡，我们的假阳率越来越低，
明白了吗？负相关对的，这个也很容易就是我们的这个数位图的大小很大的话，那么假阳率肯定会小，对不对？假阳率肯定会小。
那么我们接着来看第三个的关系，那么我们第三个p
p和k那么呢我们就是将我们的m跟n把它
固定住，我们先把它固定这两个鞍钢，我们的
m就是我们的存储空间跟我们的数量，不变，我们来看一下p和k的关系，那么这个关系呢
大家可以看到了，我们这个23呢它是一个最低点，它是这样一种好，先
先往下然后再往上的这样的一种趋势，也就是说到达23的时候呢它是一个最佳位置点，
在这个23的时候呢是一个最佳的位置点，就是这个位置。
啊这个麦克老师这个方式图没画好，
那么呢是在这个位置呢它是最最最优点，在这里呢它会有一个最优点。
选择对
就是23以上它又会增加了，我们的假压率又会增加了，就是我们的函数个数增大了，大家也可以想象得到我们的k
哈希函数哈希函数越多的话，哈希函数越多的话，那么我们要在位图当中设置的点也会越多，大家明白吗？就是我们的哈希函数越多，那么我们在这个绘图当中的点也会点的越多，
点的越多是不是？内存占用越多啊，课程占用越多，就是那个设置为一的点越多，不是内存占用内存越多，是设置
唯一的也非得多，那么相应的它的假阳率也会增加，这个都可以理解对不对？
好，这个讲完了之后呢我们已经理解了这4个概念之后，我们再来
跟大家再来强调一下，我们实际工作过程当中，我们使用不能过滤器的时候，我们是先填这两个参数，
n和p指定n和p去算出来m和k好，那么我们来看一下，我们的我们再来讲一个，我们再来看源码，
就是我们如何来选择k个哈希函数？
呢这k个哈希函数就跟我们的开放选址当中的双重，大家双重哈希是一样的，
双重哈希是一样的，双重哈希大家看一下，这个呢就是我们选取k个函数怎么来选取，我们采用的是mama哈 c二me m哈希二，我们将
这个 p存入到我们的曼曼曼哈希二，然后呢指定一个种子偏移量种子偏移量，然后呢我们将这个哈希一的值呢带入到我们的这个
哈希这是，我们同样的是采用的哈希函数，然后呢把它放到这个二里面，
这样呢我们通过哈希一跟哈希二模拟出这里k个k是我们哈希函数的个数，k是哈希函数的个数，
而我们的m呢就是我们的位图的大小，对不对？
M就是位图的大小。
好，
这个呢我们通过哈希一乘以I乘以我们的哈希二，再对我们的绘图进行取余，呢
就算出我们要在哪些位图哪些点进行搞死呃进行设置为一，设置一，
通过这种K歌哈希函数呢这样就是模拟k个哈希函数是不是跟我们的双重
双重哈希是一模一样的，我们的双重哈希是不是刚刚马克老师跟大家操作了，他是不是其实就是
我们前面的可以把它看作哈希一，然后加上一个k贝的k贝的后面的我们不管了，后面的我们不管了，把它
把这里一坨看成我们的哈希二函数就行了，我们通过这种双重哈希去探测没有着色的点，那么我们同样的
使用双重哈希呢能够解决一个，就是比如说我们这里三个函数，我们不可能让这三个函数都都着在一个点，吧
如果我们这三个函数找到一个点，那么就没有意义了。
比如说我们这三个哈希函数都找到这个点行不行？那显然不行，对不对？我们是不是
利用前面的发生函数的那个探测是双头发起探测，让他至少不跟自己
跟钱跟之前的冲突，明白了吗？好，这个听明白了没有？发Mark刷个鲜花好吗？就是为什么我们在这里采用双重双重花心？就是我们使用
双种花饰的这三个全部落在这一个点上，因为他们是复制的，双种花是一样的往这边看一处看长
就是这是，我们在第二道我们要去探讨一下，我们再去探索另外一个，
让他们减少冲突，至少让我们k个函数之间减少冲击，对不对？
如果我们k个函数之间都有冲突了，那我们这个不能过滤器肯定效率很低了，对不对？
避免重叠，我们的嘎斯蒂的朋友呢说的很对，这样呢我们就可以避免这几个函数重叠的，我们这里呢就是采用的双头发型，
所以呢我们的双重哈希呢也能够解决，我们那个聚集聚集哈希就是同类聚集哈希也是一样的道理。
那么这里怎么推论呢大家就不用考虑了，怎么推你只知道我们的双重哈希能够解决这种冲突就行了。
能够避免冲突就行了
好，那么呢我们来看一下这个源码，k等于2k1不是等于2，k一是我们
算出来的值，比如说我们这里要插入4000个数据，要插入4000个数数据，而且是1/千万的假阳率，
那么他会算出来我们要用23个函数，我们的杨波朋友听明白了，这个是我们填的，这两个是通过这个公式算出来的。
对，我们的k是根据n我们的n和p算出来了，根据前面一步一步算出来的，那么23
其实我们只用了一个哈希函数，明白了吗？我们的其实我们只用了一个函数，看到我们都是用的mama哈奇二，看到没有？这23个函数是通过我们这种
双重发息，就我们给他加一个步长I然后我们巴西二跟哈希跟我之间的关系，
谢谢
这个运算呢是不是跟我们前面的运算很像，左移32位跟我们的点赞这里面它是差不多的。对，明白了吧？
从这个呢通过这两个函数呢去算出来的，这32个k呃23个。
对明白了，对对对我们的衣服就是你我要听明白了，
并不是真的，我们这里并不是有23个哈希函数，我们只是用一个哈希函数呢通过这种偏移的方式呢让它伪造出来
32个哈奇函数，明白了吗？好，这个呢这是我们最核心的思想，就是我们的布隆过滤最核心的思想，那么我们来真正的来使用了
我们真正的来使用这个太好以后在工作中要使用布隆过滤器的朋友的，可以直接拿过来使用就行了。
好，我们来看一下，我们是首先我们来看一下是如何来使用的。
首先我们要初始化我们的不能过滤器，大家看一下我们初始布隆过滤器填的哪些参数，第一个呢当然是我们这个结构体，第二个呢我们来看一下，
第二个是我们的这个 C的，就是我们的k一量开始是排g的跟那个 k种子k一亮，那么我们在这里设为里，
这个就是我们要带插入的4000个元素，明白了吗？
我们要插入4000个元素，跟我们现现在这里举的例子是一样的，我们要插入4000个元素，
然后我们的假阳率是多少？这里呢是
也是千万/1，对不对？也是千万/1，我们把它设置假阳率呢是1/千万，那么我们根据我们来看一下它是怎么算的，我们通过这个
假阳率跟要插入的元素去算出我们位图要占多少空间，以及我们要用多少个哈希函数，那么我们这个
塌砌函数啊我们的位图的空间的大小是多少，呢通过这个东西我们就可以算出来算出来就是这个塞子塞子，呢我们再把它分配这个这么多的内存就行了。
这个就是分配出我们的位生成的我们的位图，这个就是我们的位图绘图的大小，绘图的那个内存块对不对？这个就是我们的位图，这个呢就是我们的
空间的，这个是我们k的大小k的个数，我们的k的个数来看一下就是这个哈希方式。
好，大家看一下我们的哈希方式，
在这里大家看一下，我们是不是首先它吸然后把它制为一，
相当于这个操作。这个代码在这里就相当于这样的。对，我们都是喜欢写c风格的代码，好，我们看一下二的，好，我们来看一下，

然后呢我们再把它计算出来就行了，然后再把它加到这个就是加入元素的时候，这个是我们往里面添加元素，把那个 key key呢
知道没有？
好，这个呢我们将t把它放进去，然后呢把它设置地址，就是把那个设置为一设置为一以后，我们就把它
每天出去，如果我们超过我们的最大数量，这里还会有1个最大数量，这个最大数量是不是我们算出来的？啊不是，是我们选定的就是那个4个。
如果呃4000，个如果我们想出来4000个，那就等于如果大于4000个，我就把它称之为一，
啊没什么区别没什么区别，当然不为零更好，但是你一定要就是实际项目中这里是填一个时间种子，填一个时间种子，在这里呢随便填一个值。
好，我们来运行一下这个代码我来运行一下这个代码。首先编译一下，
我们要指定头文件就是我们的main CPD然后上来，然后输入到我们的test里面，把我们的再运行一下。
好，大家看一下我们的功能我们的功能就是往里面插入了art item，这里1000个插入了1000个字符串，大家看到了我们插入了1000个字符串，
我们插入了1000个字不算，那么它的开头都是0words，这是个我们的URL对不对？然后我们来检测一下这个0和10,001是否在里面？
我们的0和10,001是否在我们的这个库里面，在我们的不能过滤器当中，那么呢我们通过运算大家可以看到了，我们的0呢它是存在的，是在我们的字符串当中，
我们的10,001呢它是不存在的，因为我们这里插入的是1000个数据1000个数据，那么我们这里最大是不是1000点html，这里大家应该能够清楚吧？

对应该写可能存在的，因为我们的误差率呢是千万/1，大家看到我们写的误差是1/千万。
好，这个布隆过滤器呢其实是比较简单的，关键是呢我们要将我们所学的知识能够串接起来，能够将我们所学的知识串接起来，我们的set map以及我们按unordered_map为什么他们不行，以及他们的细节有什么问题，
为什么我们的布隆过滤器可以，呢我们的布隆过滤器它没有存储这个 String，没有存储这个 key，
我们是通过这种概率型的来通过概率型的然后来映射这个点，通过这种设置假阳率的方式，就是我们能够接受多大的误差率。

## 7.代码

```cpp
#ifndef __MICRO_BLOOMFILTER_H__



#define __MICRO_BLOOMFILTER_H__



 



/**



 *



 *  仿照Cassandra中的BloomFilter实现，Hash选用MurmurHash2，通过双重散列公式生成散列函数，参考：http://hur.st/bloomfilter



 *    Hash(key, i) = (H1(key) + i * H2(key)) % m



 *



**/



 



#include <stdio.h>



#include <stdlib.h>



#include <stdint.h>



#include <string.h>



#include <math.h>



 



#define __BLOOMFILTER_VERSION__ "1.1"



#define __MGAIC_CODE__          (0x01464C42)



 



/**



 *  BloomFilter使用例子：



 *  static BaseBloomFilter stBloomFilter = {0};



 *



 *  初始化BloomFilter(最大100000元素，不超过0.00001的错误率)：



 *      InitBloomFilter(&stBloomFilter, 0, 100000, 0.00001);



 *  重置BloomFilter：



 *      ResetBloomFilter(&stBloomFilter);



 *  释放BloomFilter:



 *      FreeBloomFilter(&stBloomFilter);



 *



 *  向BloomFilter中新增一个数值（0-正常，1-加入数值过多）：



 *      uint32_t dwValue;



 *      iRet = BloomFilter_Add(&stBloomFilter, &dwValue, sizeof(uint32_t));



 *  检查数值是否在BloomFilter内（0-存在，1-不存在）：



 *      iRet = BloomFilter_Check(&stBloomFilter, &dwValue, sizeof(uint32_t));



 *



 *  (1.1新增) 将生成好的BloomFilter写入文件:



 *      iRet = SaveBloomFilterToFile(&stBloomFilter, "dump.bin")



 *  (1.1新增) 从文件读取生成好的BloomFilter:



 *      iRet = LoadBloomFilterFromFile(&stBloomFilter, "dump.bin")



**/



 



// 注意，要让Add/Check函数内联，必须使用 -O2 或以上的优化等级



#define FORCE_INLINE __attribute__((always_inline))



 



#define BYTE_BITS           (8)



#define MIX_UINT64(v)       ((uint32_t)((v>>32)^(v)))



 



#define SETBIT(filter, n)   (filter->pstFilter[n/BYTE_BITS] |= (1 << (n%BYTE_BITS)))



#define GETBIT(filter, n)   (filter->pstFilter[n/BYTE_BITS] & (1 << (n%BYTE_BITS)))



 



#pragma pack(1)



 



// BloomFilter结构定义



typedef struct



{



    uint8_t cInitFlag;                              // 初始化标志，为0时的第一次Add()会对stFilter[]做初始化



    uint8_t cResv[3];



 



    uint32_t dwMaxItems;                            // n - BloomFilter中最大元素个数 (输入量)



    double dProbFalse;                              // p - 假阳概率 (输入量，比如万分之一：0.00001)



    uint32_t dwFilterBits;                          // m = ceil((n * log(p)) / log(1.0 / (pow(2.0, log(2.0))))); - BloomFilter的比特数



    uint32_t dwHashFuncs;                           // k = round(log(2.0) * m / n); - 哈希函数个数



 



    uint32_t dwSeed;                                // MurmurHash的种子偏移量



    uint32_t dwCount;                               // Add()的计数，超过MAX_BLOOMFILTER_N则返回失败



 



    uint32_t dwFilterSize;                          // dwFilterBits / BYTE_BITS



    unsigned char *pstFilter;                       // BloomFilter存储指针，使用malloc分配



    uint32_t *pdwHashPos;                           // 存储上次hash得到的K个bit位置数组(由bloom_hash填充)



} BaseBloomFilter;



 



// BloomFilter文件头部定义



typedef struct



{



    uint32_t dwMagicCode;                           // 文件头部标识，填充 __MGAIC_CODE__



    uint32_t dwSeed;



    uint32_t dwCount;



 



    uint32_t dwMaxItems;                            // n - BloomFilter中最大元素个数 (输入量)



    double dProbFalse;                              // p - 假阳概率 (输入量，比如万分之一：0.00001)



    uint32_t dwFilterBits;                          // m = ceil((n * log(p)) / log(1.0 / (pow(2.0, log(2.0))))); - BloomFilter的比特数



    uint32_t dwHashFuncs;                           // k = round(log(2.0) * m / n); - 哈希函数个数



 



    uint32_t dwResv[6];



    uint32_t dwFileCrc;                             // (未使用)整个文件的校验和



    uint32_t dwFilterSize;                          // 后面Filter的Buffer长度



} BloomFileHead;



 



#pragma pack()



 



 



// 计算BloomFilter的参数m,k



static inline void _CalcBloomFilterParam(uint32_t n, double p, uint32_t *pm, uint32_t *pk)



{



    /**



     *  n - Number of items in the filter



     *  p - Probability of false positives, float between 0 and 1 or a number indicating 1-in-p



     *  m - Number of bits in the filter



     *  k - Number of hash functions



     *



     *  f = ln(2) × ln(1/2) × m / n = (0.6185) ^ (m/n)



     *  m = -1 * ln(p) × n / 0.6185 , 这里有错误



     *  k = ln(2) × m / n = 0.6931 * m / n



     * darren修正：



     * m = -1*n*ln(p)/((ln(2))^2) = -1*n*ln(p)/(ln(2)*ln(2)) = -1*n*ln(p)/(0.69314718055995*0.69314718055995))



     *   = -1*n*ln(p)/0.4804530139182079271955440025



     * k = ln(2)*m/n



    **/



 



    uint32_t m, k, m2;



 



    //    printf("ln(2):%lf, ln(p):%lf\n", log(2), log(p)); // 用来验证函数正确性



 



    // 计算指定假阳(误差)概率下需要的比特数



    m =(uint32_t) ceil(-1.0 * n * log(p) / 0.480453); //darren 修正



    m = (m - m % 64) + 64;                              // 8字节对齐



 



    // 计算哈希函数个数



    double double_k = (0.69314 * m / n); // ln(2)*m/n // 这里只是为了debug出来看看具体的浮点数值



    k = round(double_k);    // 返回x的四舍五入整数值。



    printf("orig_k:%lf, k:%u\n", double_k, k);



 



    *pm = m;



    *pk = k;



    return;



}



 



 



// 根据目标精度和数据个数，初始化BloomFilter结构



/**



 * @brief 初始化布隆过滤器



 * @param pstBloomfilter 布隆过滤器实例



 * @param dwSeed    hash种子



 * @param dwMaxItems 存储容量



 * @param dProbFalse 允许的误判率



 * @return 返回值



 *      -1 传入的布隆过滤器为空



 *      -2 hash种子错误或误差>=1



 */



inline int InitBloomFilter(BaseBloomFilter *pstBloomfilter, uint32_t dwSeed, uint32_t dwMaxItems,



                           double dProbFalse)



{



    if (pstBloomfilter == NULL)



        return -1;



    if ((dProbFalse <= 0) || (dProbFalse >= 1))



        return -2;



 



    // 先检查是否重复Init，释放内存



    if (pstBloomfilter->pstFilter != NULL)



        free(pstBloomfilter->pstFilter);



    if (pstBloomfilter->pdwHashPos != NULL)



        free(pstBloomfilter->pdwHashPos);



 



    memset(pstBloomfilter, 0, sizeof(BaseBloomFilter));



 



    // 初始化内存结构，并计算BloomFilter需要的空间



    pstBloomfilter->dwMaxItems = dwMaxItems;    // 最大存储



    pstBloomfilter->dProbFalse = dProbFalse;    // 误差



    pstBloomfilter->dwSeed = dwSeed;            // hash种子



 



    // 计算 m, k



    _CalcBloomFilterParam(pstBloomfilter->dwMaxItems, pstBloomfilter->dProbFalse,



                          &pstBloomfilter->dwFilterBits, &pstBloomfilter->dwHashFuncs);



 



    // 分配BloomFilter的存储空间



    pstBloomfilter->dwFilterSize = pstBloomfilter->dwFilterBits / BYTE_BITS;



    pstBloomfilter->pstFilter = (unsigned char *) malloc(pstBloomfilter->dwFilterSize);



    if (NULL == pstBloomfilter->pstFilter)



        return -100;



 



    // 哈希结果数组，每个哈希函数一个



    pstBloomfilter->pdwHashPos = (uint32_t*) malloc(pstBloomfilter->dwHashFuncs * sizeof(uint32_t));



    if (NULL == pstBloomfilter->pdwHashPos)



        return -200;



 



    printf(">>> Init BloomFilter(n=%u, p=%e, m=%u, k=%d), malloc() size=%.2fMB, items:bits=1:%0.1lf\n",



           pstBloomfilter->dwMaxItems, pstBloomfilter->dProbFalse, pstBloomfilter->dwFilterBits,



           pstBloomfilter->dwHashFuncs, (double)pstBloomfilter->dwFilterSize/1024/1024,



           pstBloomfilter->dwFilterBits*1.0/pstBloomfilter->dwMaxItems);



 



    // 初始化BloomFilter的内存



    memset(pstBloomfilter->pstFilter, 0, pstBloomfilter->dwFilterSize);



    pstBloomfilter->cInitFlag = 1;



    return 0;



}



 



// 释放BloomFilter



inline int FreeBloomFilter(BaseBloomFilter *pstBloomfilter)



{



    if (pstBloomfilter == NULL)



        return -1;



 



    pstBloomfilter->cInitFlag = 0;



    pstBloomfilter->dwCount = 0;



 



    free(pstBloomfilter->pstFilter);



    pstBloomfilter->pstFilter = NULL;



    free(pstBloomfilter->pdwHashPos);



    pstBloomfilter->pdwHashPos = NULL;



    return 0;



}



 



// 重置BloomFilter



// 注意: Reset()函数不会立即初始化stFilter，而是当一次Add()时去memset



inline int ResetBloomFilter(BaseBloomFilter *pstBloomfilter)



{



    if (pstBloomfilter == NULL)



        return -1;



 



    pstBloomfilter->cInitFlag = 0;



    pstBloomfilter->dwCount = 0;



    return 0;



}



 



// 和ResetBloomFilter不同，调用后立即memset内存



inline int RealResetBloomFilter(BaseBloomFilter *pstBloomfilter)



{



    if (pstBloomfilter == NULL)



        return -1;



 



    memset(pstBloomfilter->pstFilter, 0, pstBloomfilter->dwFilterSize);



    pstBloomfilter->cInitFlag = 1;



    pstBloomfilter->dwCount = 0;



    return 0;



}



 



///



///  函数FORCE_INLINE，加速执行



///



// MurmurHash2, 64-bit versions, by Austin Appleby



// https://sites.google.com/site/murmurhash/



FORCE_INLINE uint64_t MurmurHash2_x64 ( const void * key, int len, uint32_t seed )



{



    const uint64_t m = 0xc6a4a7935bd1e995;



    const int r = 47;



 



    uint64_t h = seed ^ (len * m);



 



    const uint64_t * data = (const uint64_t *)key;



    const uint64_t * end = data + (len/8);



 



    while(data != end)



    {



        uint64_t k = *data++;



 



        k *= m;



        k ^= k >> r;



        k *= m;



 



        h ^= k;



        h *= m;



    }



 



    const uint8_t * data2 = (const uint8_t*)data;



 



    switch(len & 7)



    {



    case 7: h ^= ((uint64_t)data2[6]) << 48;



    case 6: h ^= ((uint64_t)data2[5]) << 40;



    case 5: h ^= ((uint64_t)data2[4]) << 32;



    case 4: h ^= ((uint64_t)data2[3]) << 24;



    case 3: h ^= ((uint64_t)data2[2]) << 16;



    case 2: h ^= ((uint64_t)data2[1]) << 8;



    case 1: h ^= ((uint64_t)data2[0]);



        h *= m;



    };



 



    h ^= h >> r;



    h *= m;



    h ^= h >> r;



 



    return h;



}



 



// 双重散列封装



FORCE_INLINE void bloom_hash(BaseBloomFilter *pstBloomfilter, const void * key, int len)



{



    //if (pstBloomfilter == NULL) return;



    int i;



    uint32_t dwFilterBits = pstBloomfilter->dwFilterBits;



    uint64_t hash1 = MurmurHash2_x64(key, len, pstBloomfilter->dwSeed);



    uint64_t hash2 = MurmurHash2_x64(key, len, MIX_UINT64(hash1));



 



    for (i = 0; i < (int)pstBloomfilter->dwHashFuncs; i++)



    {



        pstBloomfilter->pdwHashPos[i] = (hash1 + i*hash2) % dwFilterBits;



    }



 



    return;



}



 



// 向BloomFilter中新增一个元素



// 成功返回0，当添加数据超过限制值时返回1提示用户



FORCE_INLINE int BloomFilter_Add(BaseBloomFilter *pstBloomfilter, const void * key, int len)



{



    if ((pstBloomfilter == NULL) || (key == NULL) || (len <= 0))



        return -1;



 



    int i;



 



    if (pstBloomfilter->cInitFlag != 1)



    {



        // Reset后没有初始化，使用前需要memset



        memset(pstBloomfilter->pstFilter, 0, pstBloomfilter->dwFilterSize);



        pstBloomfilter->cInitFlag = 1;



    }



 



    // hash key到bloomfilter中



    bloom_hash(pstBloomfilter, key, len);



    for (i = 0; i < (int)pstBloomfilter->dwHashFuncs; i++)



    {



        SETBIT(pstBloomfilter, pstBloomfilter->pdwHashPos[i]);



    }



 



    // 增加count数



    pstBloomfilter->dwCount++;



    if (pstBloomfilter->dwCount <= pstBloomfilter->dwMaxItems)



        return 0;



    else



        return 1;       // 超过N最大值，可能出现准确率下降等情况



}



 



// 检查一个元素是否在bloomfilter中



// 返回：0-存在，1-不存在，负数表示失败



FORCE_INLINE int BloomFilter_Check(BaseBloomFilter *pstBloomfilter, const void * key, int len)



{



    if ((pstBloomfilter == NULL) || (key == NULL) || (len <= 0))



        return -1;



 



    int i;



 



    bloom_hash(pstBloomfilter, key, len);



    for (i = 0; i < (int)pstBloomfilter->dwHashFuncs; i++)



    {



        // 如果有任意bit不为1，说明key不在bloomfilter中



        // 注意: GETBIT()返回不是0|1，高位可能出现128之类的情况



        if (GETBIT(pstBloomfilter, pstBloomfilter->pdwHashPos[i]) == 0)



            return 1;



    }



 



    return 0;



}



 



 



/* 文件相关封装 */



// 将生成好的BloomFilter写入文件



inline int SaveBloomFilterToFile(BaseBloomFilter *pstBloomfilter, char *szFileName)



{



    if ((pstBloomfilter == NULL) || (szFileName == NULL))



        return -1;



 



    int iRet;



    FILE *pFile;



    static BloomFileHead stFileHeader = {0};



 



    pFile = fopen(szFileName, "wb");



    if (pFile == NULL)



    {



        perror("fopen");



        return -11;



    }



 



    // 先写入文件头



    stFileHeader.dwMagicCode = __MGAIC_CODE__;



    stFileHeader.dwSeed = pstBloomfilter->dwSeed;



    stFileHeader.dwCount = pstBloomfilter->dwCount;



    stFileHeader.dwMaxItems = pstBloomfilter->dwMaxItems;



    stFileHeader.dProbFalse = pstBloomfilter->dProbFalse;



    stFileHeader.dwFilterBits = pstBloomfilter->dwFilterBits;



    stFileHeader.dwHashFuncs = pstBloomfilter->dwHashFuncs;



    stFileHeader.dwFilterSize = pstBloomfilter->dwFilterSize;



 



    iRet = fwrite((const void*)&stFileHeader, sizeof(stFileHeader), 1, pFile);



    if (iRet != 1)



    {



        perror("fwrite(head)");



        return -21;



    }



 



    // 接着写入BloomFilter的内容



    iRet = fwrite(pstBloomfilter->pstFilter, 1, pstBloomfilter->dwFilterSize, pFile);



    if ((uint32_t)iRet != pstBloomfilter->dwFilterSize)



    {



        perror("fwrite(data)");



        return -31;



    }



 



    fclose(pFile);



    return 0;



}



 



// 从文件读取生成好的BloomFilter



inline int LoadBloomFilterFromFile(BaseBloomFilter *pstBloomfilter, char *szFileName)



{



    if ((pstBloomfilter == NULL) || (szFileName == NULL))



        return -1;



 



    int iRet;



    FILE *pFile;



    static BloomFileHead stFileHeader = {0};



 



    if (pstBloomfilter->pstFilter != NULL)



        free(pstBloomfilter->pstFilter);



    if (pstBloomfilter->pdwHashPos != NULL)



        free(pstBloomfilter->pdwHashPos);



 



    //



    pFile = fopen(szFileName, "rb");



    if (pFile == NULL)



    {



        perror("fopen");



        return -11;



    }



 



    // 读取并检查文件头



    iRet = fread((void*)&stFileHeader, sizeof(stFileHeader), 1, pFile);



    if (iRet != 1)



    {



        perror("fread(head)");



        return -21;



    }



 



    if ((stFileHeader.dwMagicCode != __MGAIC_CODE__)



            || (stFileHeader.dwFilterBits != stFileHeader.dwFilterSize*BYTE_BITS))



        return -50;



 



    // 初始化传入的 BaseBloomFilter 结构



    pstBloomfilter->dwMaxItems = stFileHeader.dwMaxItems;



    pstBloomfilter->dProbFalse = stFileHeader.dProbFalse;



    pstBloomfilter->dwFilterBits = stFileHeader.dwFilterBits;



    pstBloomfilter->dwHashFuncs = stFileHeader.dwHashFuncs;



    pstBloomfilter->dwSeed = stFileHeader.dwSeed;



    pstBloomfilter->dwCount = stFileHeader.dwCount;



    pstBloomfilter->dwFilterSize = stFileHeader.dwFilterSize;



 



    pstBloomfilter->pstFilter = (unsigned char *) malloc(pstBloomfilter->dwFilterSize);



    if (NULL == pstBloomfilter->pstFilter)



        return -100;



    pstBloomfilter->pdwHashPos = (uint32_t*) malloc(pstBloomfilter->dwHashFuncs * sizeof(uint32_t));



    if (NULL == pstBloomfilter->pdwHashPos)



        return -200;



 



 



    // 将后面的Data部分读入 pstFilter



    iRet = fread((void*)(pstBloomfilter->pstFilter), 1, pstBloomfilter->dwFilterSize, pFile);



    if ((uint32_t)iRet != pstBloomfilter->dwFilterSize)



    {



        perror("fread(data)");



        return -31;



    }



    pstBloomfilter->cInitFlag = 1;



 



    printf(">>> Load BloomFilter(n=%u, p=%f, m=%u, k=%d), malloc() size=%.2fMB\n",



           pstBloomfilter->dwMaxItems, pstBloomfilter->dProbFalse, pstBloomfilter->dwFilterBits,



           pstBloomfilter->dwHashFuncs, (double)pstBloomfilter->dwFilterSize/1024/1024);



 



    fclose(pFile);



    return 0;



}



 



#endif
#include "bloomfilter.h"



#include <stdio.h>



 



#define MAX_ITEMS 4000      // 设置最大元素



#define ADD_ITEMS 1000      // 添加测试元素



#define P_ERROR 0.0000001   // 设置误差



 



 



int main(int argc, char** argv)



{



 



    printf(" test bloomfilter\n");



 



    // 1. 定义BaseBloomFilter



    static BaseBloomFilter stBloomFilter = {0};



 



    // 2. 初始化stBloomFilter，调用时传入hash种子，存储容量，以及允许的误判率



    InitBloomFilter(&stBloomFilter, 0, MAX_ITEMS, P_ERROR);



 



    // 3. 向BloomFilter中新增数值



    char url[128] = {0};



    for(int i = 0; i < ADD_ITEMS; i++){



        sprintf(url, "https://0voice.com/%d.html", i);



        if(0 == BloomFilter_Add(&stBloomFilter, (const void*)url, strlen(url))){



            // printf("add %s success", url);



        }else{



            printf("add %s failed", url);



        }



        memset(url, 0, sizeof(url));



    }



 



    // 4. check url exist or not



    const char* str = "https://0voice.com/0.html";



    if (0 == BloomFilter_Check(&stBloomFilter, str, strlen(str)) ){



        printf("https://0voice.com/0.html exist\n");



    }



 



    const char* str2 = "https://0voice.com/10001.html";



    if (0 != BloomFilter_Check(&stBloomFilter, str2, strlen(str2)) ){



          printf("https://0voice.com/10001.html not exist\n");



    }



 



    // 5. free bloomfilter



    FreeBloomFilter(&stBloomFilter);



    getchar();



    return 0;



}



 
```

原文作者：[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/604966502

# 【NO.530】Linux服务器开发,内存池原理与实现

## 0.前言

内存池顾名思义，是管理栈空间中堆的组件。有了内存池，可以避免频繁的分配和释放堆空间。从另一个角度说，内存池是为了提升变量的生存周期。

## 1.作用

频繁分配导致内存分配碎片化。
内存管理不是几分钟几天的内存问题，而是解决当程序长时间运行时，改变资源不进行管理造成被系统杀死的命运。如果运行几个月后偶现的问题较难排查，对于这种问题排查也是束手无策，所以使用内存池是一件非常有必要的事。

## 2.注意点

- 一定要用内存池。

- 不要自己实现内存池！听到这一点我就开心了，毕竟减少了时间劳动的成本。因为这种组件很大部分都工程师都没有这个能力。

- 内存池的原理一定要懂。

  jemallocl是c实现的，tcmaloc是google用c++实现的，更推荐这个，内存回收机制有些小问题，绝大部分使用没问题。用起来简单，加个宏定义，会hook住malloc。

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/fcc2ca6330174b66bad1d8ad78236dd4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

  小块回收是一个极其麻烦的事情，随着时间的推移小字节内存块越来越多浪费也越来越多。解决方案是固定大小的内存。但是固定大小的也存在着不灵活的问题，很多内存用不了的情况。

  改进的方法使用梯度的固定大小，大块的内存要多少给多少，小块内存分档次进行处理。

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/39d66345099548859d6016f40ab530ac.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

  但是仍存在的问题时：

- 链表查找速度慢。——>1、区分使用过否。2、红黑树，哈希。

- 出现了间隙，不利于回收，极其麻烦。——>影响小块的回收。

几种使用场景
1、全局内存池 jemmaloc/tcmalloc。
2、一个连接做一个内存池，一个连接释放一个内存池被销毁。生命周期短所以小块不用回收。
3、消息x，浪费。

## 3.实现内存池

使用大块和小块来实现。

```c
void *mp_memalign(struct mp_pool_s *pool, size_t size, size_t alignment)



 {



    void *p;



    //posix_memalign()这个函数可以分配出大空间的内存



    int ret = posix_memalign(&p, alignment, size);



    if (ret) {



        return NULL;



    }



    struct mp_large_s *large = mp_alloc(pool, sizeof(struct mp_large_s));



    if (large == NULL) 



    {



        free(p);



        return NULL;



    }



    large->alloc = p;



    large->next = pool->large;



    pool->large = large;



 



    return p;



}
struct mp_pool_s *mp_create_pool(size_t size)



 {



    struct mp_pool_s *p;



    int ret = posix_memalign((void **)&p, MP_ALIGNMENT, size + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s));



    if (ret)



     {



        return NULL;



    }



    p->max = (size < MP_MAX_ALLOC_FROM_POOL) ? size : MP_MAX_ALLOC_FROM_POOL;



    p->current = p->head;



    p->large = NULL;



    p->head->last = (unsigned char *)p + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s);



    p->head->end = p->head->last + size;



 



    p->head->failed = 0;



 



    return p;



}
```

posix_memallign((void **)&pool,ALIGNMENT,size+sizeof(struct mp_pool));
//分配超过4K就分配不出来，所有调用此api

注意释放顺序，必须大块先释放，再释放小块。将指向大块节点的那块内存放在小块中，可以减少释放内存时的难度系数。

## 4.总结

今天通过零声学院King老师绘声绘色的讲述内存池，让我感触颇深。
从技术知识层面，我又从新的一个角度认识了内存池。以前一直感觉内存池是一个臃肿的组件，从来没有感受到内存池存在着重要的意义。有了内存池就可以轻松避免内存管理出现的问题，有的程序工作几个月以上出现崩溃的情况，偶现的问题往往是极难排查的，用上内存池可以更加保险。尽管不需要自己手写内存池，但是我已经足够深入的了解了内存池的历史演变和工作原理，可以说这在工作中完全足够在同事面前炫耀一番了。
感觉这句话是我印象最深的：线程池、内存池其实没有标准，要根据实际情况去确定。个人理解没有标准往往比有标准更加痛苦，我只能通过自己的努力不断的探索标准，才能更快的进步。
最后，还是非常感谢零声学院的King老师，啊，有King老师的陪伴学习的路上变得异常平坦开阔。

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/604988800

# 【NO.531】基础的网络服务器开发

## 1.需求分析

实现回声服务器的客户端/服务器程序，客户端通过网络连接到服务器，并发送任意一串英文信息，服务器端接收信息后，将每个字符转换为大写并回送给客户端显示。

## 2.项目实现

echo_server.c

```cpp
#include <stdio.h>



#include <unistd.h>



#include <sys/types.h>



#include <sys/socket.h>



#include <string.h>



#include <ctype.h>



#include <arpa/inet.h>



 



#define SERVER_PORT 666



 



int main(void)



{



 



    int sock; //代表信箱



    struct sockaddr_in server_addr;



 



    // 1.美女创建信箱



    sock = socket(AF_INET, SOCK_STREAM, 0);



 



    // 2.清空标签，写上地址和端口号



    bzero(&server_addr, sizeof(server_addr));



 



    server_addr.sin_family = AF_INET;                //选择协议族IPV4



    server_addr.sin_addr.s_addr = htonl(INADDR_ANY); //监听本地所有IP地址



    server_addr.sin_port = htons(SERVER_PORT);       //绑定端口号



 



    //实现标签贴到收信得信箱上



    bind(sock, (struct sockaddr *)&server_addr, sizeof(server_addr));



 



    //把信箱挂置到传达室，这样，就可以接收信件了



    listen(sock, 128);



 



    //万事俱备，只等来信



    printf("等待客户端的连接\n");



 



    int done = 1;



 



    while (done)



    {



        struct sockaddr_in client;



        int client_sock, len, i;



        char client_ip[64];



        char buf[256];



 



        socklen_t client_addr_len;



        client_addr_len = sizeof(client);



        client_sock = accept(sock, (struct sockaddr *)&client, &client_addr_len);



 



        //打印客服端IP地址和端口号



        printf("client ip: %s\t port : %d\n",



               inet_ntop(AF_INET, &client.sin_addr.s_addr, client_ip, sizeof(client_ip)),



               ntohs(client.sin_port));



        /*读取客户端发送的数据*/



        len = read(client_sock, buf, sizeof(buf) - 1);



        buf[len] = '\0';



        printf("receive[%d]: %s\n", len, buf);



 



        //转换成大写



        for (i = 0; i < len; i++)



        {



            /*if(buf[i]>='a' && buf[i]<='z'){



                buf[i] = buf[i] - 32;



            }*/



            buf[i] = toupper(buf[i]);



        }



 



        len = write(client_sock, buf, len);



 



        printf("finished. len: %d\n", len);



        close(client_sock);



    }



    close(sock);



    return 0;



}
```

echo_client.c

```cpp
#include <stdio.h>



#include <stdlib.h>



#include <string.h>



#include <unistd.h>



#include <sys/socket.h>



#include <netinet/in.h>



 



#define SERVER_PORT 666



#define SERVER_IP "127.0.0.1"



 



int main(int argc, char *argv[])



{



 



    int sockfd;



    char *message;



    struct sockaddr_in servaddr;



    int n;



    char buf[64];



 



    if (argc != 2)



    {



        fputs("Usage: ./echo_client message \n", stderr);



        exit(1);



    }



 



    message = argv[1];



 



    printf("message: %s\n", message);



 



    sockfd = socket(AF_INET, SOCK_STREAM, 0);



 



    memset(&servaddr, '\0', sizeof(struct sockaddr_in));



 



    servaddr.sin_family = AF_INET;



    inet_pton(AF_INET, SERVER_IP, &servaddr.sin_addr);



    servaddr.sin_port = htons(SERVER_PORT);



 



    connect(sockfd, (struct sockaddr *)&servaddr, sizeof(servaddr));



 



    write(sockfd, message, strlen(message));



 



    n = read(sockfd, buf, sizeof(buf) - 1);



 



    if (n > 0)



    {



        buf[n] = '\0';



        printf("receive: %s\n", buf);



    }



    else



    {



        perror("error!!!");



    }



 



    printf("finished.\n");



    close(sockfd);



 



    return 0;



}
```

## 3.网络通信与Socket



![在这里插入图片描述](https://img-blog.csdnimg.cn/ad031e9fab8e42e4984e5b1a6b915bb1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_19,color_FFFFFF,t_70,g_se,x_16)


Socket通信3要素:



1. 通信的目的地址
2. 使用的端口号 http 80 smtp 25
3. 使用的传输层协议 (如TCP,UDP)

Socket 通信模型



![在这里插入图片描述](https://img-blog.csdnimg.cn/e462013f3aa34e2b9001e870d23e8205.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_19,color_FFFFFF,t_70,g_se,x_16)



## 4.Socket 编程详解

### 4.1 套接字概念

**套接字概念**
Socket中文意思是“插座”，在Linux环境下，用于表示进程x间网络通信的特殊文件类型。本质为内核借助缓冲区形成的伪文件。

既然是文件，那么理所当然的，我们可以使用文件描述符引用套接字。Linux系统将其封装成文件的目的是为了统一接口，使得读写套接字和读写文件的操作一致。区别是文件主要应用于本地持久化数据的读写，而套接字多应用于网络进程间数据的传递。

在TCP/IP协议中，“IP地址+TCP或UDP端口号”唯一标识网络通讯中的一个进程。“IP地址+端口号”就对应一个socket。欲建立连接的两个进程各自有一个socket来标识，那么这两个socket组成的socket pair就唯一标识一个连接。因此可以用Socket来描述网络连接的一对一关系。

套接字通信原理如下图所示：



![在这里插入图片描述](https://img-blog.csdnimg.cn/70044d4b21164af3bce89a4d97d6c171.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


在网络通信中，套接字一定是成对出现的。一端的发送缓冲区对应对端的接收缓冲区。我们使用同一个文件描述符发送缓冲区和接收缓冲区。



**Socket 通信创建流程图**



![在这里插入图片描述](https://img-blog.csdnimg.cn/9723d84d1c5d48358f345e71bd04b158.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



### 5.Socket编程基础

#### 5.1 网络字节序

在计算机世界里，有两种字节序：

```javascript
大端字节序 - 低地址高字节,高地址低字节



小端字节序 - 低地址低字节,高地址高字节
```



![在这里插入图片描述](https://img-blog.csdnimg.cn/04cf2cf518314aba99c4941b66426d56.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)


内存中的多字节数据相对于内存地址有大端和小端之分，磁盘文件中的多字节数据相对于文件中的偏移地址也有大端小端之分。网络数据流同样有大端小端之分，那么如何定义网络数据流的地址呢？发送主机通常将发送缓冲区中的数据按内存地址从低到高的顺序发出，接收主机把从网络上接到的字节依次保存在接收缓冲区中，也是按内存地址从低到高的顺序保存，因此，网络数据流的地址应这样规定：先发出的数据是低地址，后发出的数据是高地址。
TCP/IP协议规定，网络数据流应采用大端字节序，即低地址高字节。
例如端口号是1001（0x3e9），由两个字节保存，采用大端字节序，则低地址是0x03，高地址是0xe9，也就是先发0x03，再发0xe9，这16位在发送主机的缓冲区中也应该是低地址存0x03，高地址存0xe9。但是，如果发送主机是小端字节序的，这16位被解释成0xe903，而不是1001。因此，发送主机把1001填到发送缓冲区之前需要做字节序的转换。同样地，接收主机如果是小端字节序的，接到16位的源端口号也要做字节序的转换。如果主机是大端字节序的，发送和接收都不需要做转换。同理，32位的IP地址也要考虑网络字节序和主机字节序的问题。
为使网络程序具有可移植性，使同样的C代码在大端和小端计算机上编译后都能正常运行，可以调用以下库函数做网络字节序和主机字节序的转换。



\#include <arpa/inet.h>

uint32_t htonl(uint32_t hostlong);
uint16_t htons(uint16_t hostshort);
uint32_t ntohl(uint32_t netlong);
uint16_t ntohs(uint16_t netshort);
h表示host，n表示network，l表示32位长整数，s表示16位短整数。
如果主机是小端字节序，这些函数将参数做相应的大小端转换然后返回，如果主机是大端字节序，这些函数不做转换，将参数原封不动地返回。

#### 5.2 sockaddr数据结构

很多网络编程函数诞生早于IPv4协议，那时候都使用的是sockaddr结构体,为了向前兼容，现在sockaddr退化成了（void *）的作用，传递一个地址给函数，至于这个函数是sockaddr_in还是其他的，由地址族确定，然后函数内部再强制类型转化为所需的地址类型。
*

*![在这里插入图片描述](https://img-blog.csdnimg.cn/371cfc24e246428faabb4ecfcfed2f48.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_10,color_FFFFFF,t_70,g_se,x_16)*

*
struct sockaddr {
sa_family_t sa_family; /* address family, AF_xxx */
char sa_data[14]; /* 14 bytes of protocol address */
};



struct sockaddr_in {
sa_family_t sin_family; /* address family: AF_INET */
in_port_t sin_port; /* port in network byte order */
struct in_addr sin_addr; /* internet address */
};

/* Internet address. */
struct in_addr {
uint32_t s_addr; /* address in network byte order */
};

IPv4的地址格式定义在netinet/in.h中，IPv4地址用sockaddr_in结构体表示，包括16位端口号和32位IP地址，但是sock API的实现早于ANSI C标准化，那时还没有void *类型，因此这些像bind 、accept函数的参数都用struct sockaddr *类型表示，在传递参数之前要强制类型转换一下，例如：
struct sockaddr_in servaddr;
bind(listen_fd, (struct sockaddr *)&servaddr, sizeof(servaddr)); /* initialize servaddr */

#### 5.3 IP地址转换函数



![在这里插入图片描述](https://img-blog.csdnimg.cn/8f8e0ea022db46f88a83be7215b3457c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)


\#include <arpa/inet.h>
int inet_pton(int af, const char *src, void *dst);
const char *inet_ntop(int af, const void *src, char *dst, socklen_t size);
af 取值可选为 AF_INET 和 AF_INET6 ，即和 ipv4 和ipv6对应
支持IPv4和IPv6



其中inet_pton和inet_ntop不仅可以转换IPv4的in_addr，还可以转换IPv6的in6_addr。
因此函数接口是void *addrptr。

```cpp
#include <stdio.h>



#include <string.h>



#include <arpa/inet.h>



 



 



 



int main(void){



 



    char ip[]="2.3.4.5";



    char server_ip[64];



 



    struct sockaddr_in server_addr;



 



 



    inet_pton(AF_INET, ip, &server_addr.sin_addr.s_addr);



 



    printf("s_addr : %x\n", server_addr.sin_addr.s_addr);



 



    printf("s_addr from net to host: %x\n", ntohl(server_addr.sin_addr.s_addr));



 



    inet_ntop(AF_INET, &server_addr.sin_addr.s_addr, server_ip, 64);



 



    printf("server ip : %s\n", server_ip);



 



    printf("INADDR_ANY: %d\n", INADDR_ANY);



    server_addr.sin_addr.s_addr = INADDR_ANY;



    inet_ntop(AF_INET, &server_addr.sin_addr.s_addr, server_ip, 64);



    printf("INADDR_ANY ip : %s\n", server_ip);



    return 0;



}
```

### 6.Socket编程函数

#### 6.1 socket 函数

> \#include <sys/types.h> /* See NOTES */
> \#include <sys/socket.h>
> int socket(int domain, int type, int protocol);
> domain:
> AF_INET 这是大多数用来产生socket的协议，使用TCP或UDP来传输，用IPv4的地址
> AF_INET6 与上面类似，不过是来用IPv6的地址
> AF_UNIX 本地协议，使用在Unix和Linux系统上，一般都是当客户端和服务器在同一台及其上的时候使用
> type:
> SOCK_STREAM 这个协议是按照顺序的、可靠的、数据完整的基于字节流的连接。这是一个使用最多的socket类型，这个socket是使用TCP来进行传输。
> SOCK_DGRAM 这个协议是无连接的、固定长度的传输调用。该协议是不可靠的，使用UDP来进行它的连接。
> SOCK_SEQPACKET该协议是双线路的、可靠的连接，发送固定长度的数据包进行传输。必须把这个包完整的接受才能进行读取。
> SOCK_RAW socket类型提供单一的网络访问，这个socket类型使用ICMP公共协议。（ping、traceroute使用该协议）
> SOCK_RDM 这个类型是很少使用的，在大部分的操作系统上没有实现，它是提供给数据链路层使用，不保证数据包的顺序
> protocol:
> 传0 表示使用默认协议。
> 返回值：
> 成功：返回指向新创建的socket的文件描述符，失败：返回-1，设置errno

```javascript
    socket()打开一个网络通讯端口，如果成功的话，就像open()一样返回一个文件描述符，应用程序可以像读写文件一样用read/write在网络上收发数据，如果socket()调用出错则返回-1。对于IPv4，domain参数指定为AF_INET。对于TCP协议，type参数指定为SOCK_STREAM，表示面向流的传输协议。如果是UDP协议，则type参数指定为SOCK_DGRAM，表示面向数据报的传输协议。protocol参数的介绍从略，指定为0即可。
```

#### 6.2 bind 函数

> \#include <sys/types.h> /* See NOTES */
> \#include <sys/socket.h>
> int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
> sockfd：
> socket文件描述符
> addr:
> 构造出IP地址加端口号
> addrlen:
> sizeof(addr)长度
> 返回值：
> 成功返回0，失败返回-1, 设置errno

服务器程序所监听的网络地址和端口号通常是固定不变的，客户端程序得知服务器程序的地址和端口号后就可以向服务器发起连接，因此服务器需要调用bind绑定一个固定的网络地址和端口号。

bind()的作用是将参数sockfd和addr绑定在一起，使sockfd这个用于网络通讯的文件描述符监听addr所描述的地址和端口号。前面讲过，struct sockaddr *是一个通用指针类型，addr参数实际上可以接受多种协议的sockaddr结构体，而它们的长度各不相同，所以需要第三个参数addrlen指定结构体的长度。如：

> struct sockaddr_in servaddr;
> bzero(&servaddr, sizeof(servaddr));
> servaddr.sin_family = AF_INET;
> servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
> servaddr.sin_port = htons(6666);
> 首先将整个结构体清零，然后设置地址类型为AF_INET，网络地址为INADDR_ANY，这个宏表示本地的任意IP地址，因为服务器可能有多个网卡，每个网卡也可能绑定多个IP地址，这样设置可以在所有的IP地址上监听，直到与某个客户端建立了连接时才确定下来到底用哪个IP地址，端口号为6666。

#### 6.3 listen 函数

> \#include <sys/types.h> /* See NOTES */
> \#include <sys/socket.h>
> int listen(int sockfd, int backlog);
> sockfd:
> socket文件描述符
> backlog:
> 在Linux 系统中，它是指排队等待建立3次握手队列长度

查看系统默认backlog

> cat /proc/sys/net/ipv4/tcp_max_syn_backlog

改变 系统限制的backlog 大小

> vim /etc/sysctl.conf

最后添加

> net.core.somaxconn = 1024
> net.ipv4.tcp_max_syn_backlog = 1024

保存，然后执行

> sysctl -p

典型的服务器程序可以同时服务于多个客户端，当有客户端发起连接时，服务器调用的accept()返回并接受这个连接，如果有大量的客户端发起连接而服务器来不及处理，尚未accept的客户端就处于连接等待状态，listen()声明sockfd处于监听状态，并且最多允许有backlog个客户端处于连接待状态，如果接收到更多的连接请求就忽略。listen()成功返回0，失败返回-1。

#### 6.4 accept 函数

> \#include <sys/types.h> /* See NOTES */
> \#include <sys/socket.h>
> int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
> sockdf:
> socket文件描述符
> addr:
> 传出参数，返回链接客户端地址信息，含IP地址和端口号
> addrlen:
> 传入传出参数（值-结果）,传入sizeof(addr)大小，函数返回时返回真正接收到地址结构体的大小
> 返回值：
> 成功返回一个新的socket文件描述符，用于和客户端通信，失败返回-1，设置errno



![在这里插入图片描述](https://img-blog.csdnimg.cn/6256480c77584d2c89112ae653f080df.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



三次握手完成后，服务器调用accept()接受连接，如果服务器调用accept()时还没有客户端的连接请求，就阻塞等待直到有客户端连接上来。addr是一个传出参数，accept()返回时传出客户端的地址和端口号。addrlen参数是一个传入传出参数（value-result argument），传入的是调用者提供的缓冲区addr的长度以避免缓冲区溢出问题，传出的是客户端地址结构体的实际长度（有可能没有占满调用者提供的缓冲区）。如果给addr参数传NULL，表示不关心客户端的地址。
我们的服务器程序结构是这样的：
while (1) {
cliaddr_len = sizeof(cliaddr);
connfd = accept(listenfd, (struct sockaddr *)&cliaddr, &cliaddr_len);
n = read(connfd, buf, MAXLINE);
......
close(connfd);
}
整个是一个while死循环，每次循环处理一个客户端连接。由于cliaddr_len是传入传出参数，每次调用accept()之前应该重新赋初值。accept()的参数listenfd是先前的监听文件描述符，而accept()的返回值是另外一个文件描述符connfd，之后与客户端之间就通过这个connfd通讯，最后关闭connfd断开连接，而不关闭listenfd，再次回到循环开头listenfd仍然用作accept的参数。accept()成功返回一个文件描述符，出错返回-1。

#### 6.5 connect函数

> \#include <sys/types.h> /* See NOTES */
> \#include <sys/socket.h>
> int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
> sockdf:
> socket文件描述符
> addr:
> 传入参数，指定服务器端地址信息，含IP地址和端口号
> addrlen:
> 传入参数,传入sizeof(addr)大小
> 返回值：
> 成功返回0，失败返回-1，设置errno

客户端需要调用connect()连接服务器，connect和bind的参数形式一致，区别在于bind的参数是自己的地址，而connect的参数是对方的地址。connect()成功返回0，出错返回-1。

#### 6.6 出错处理函数

我们知道，系统函数调用不能保证每次都成功，必须进行出错处理，这样一方面可以保证程序逻辑正常，另一方面可以迅速得到故障信息。

出错处理函数

> \#include <errno.h>
> \#include <string.h>
> char *strerror(int errnum); /* See NOTES */
> errnum:
> 传入参数,错误编号的值，一般取 errno 的值
> 返回值：
> 错误原因
> \#include <stdio.h>
> \#include <errno.h>
> void perror(const char *s); /* See NOTES */
> s:
> 传入参数,自定义的描述
> 返回值：
> 无
> 向标准出错stderr 输出出错原因

```c
#include <stdio.h>



#include <stdlib.h>



#include <unistd.h>



#include <sys/types.h>



#include <sys/socket.h>



#include <string.h>



#include <ctype.h>



#include <arpa/inet.h>



#include <errno.h>



 



#define IP "1.1.1.1"



 



#define SERVER_PORT 666



 



perror_exit(const char *des)



{



    // fprintf(stderr, "%s error, reason: %s\n", des, strerror(errno));



    perror(des);



    exit(1);



}



 



int main(void)



{



 



    int sock; //代表信箱



    int i, ret;



    struct sockaddr_in server_addr;



 



    // 1.美女创建信箱



    sock = socket(AF_INET, SOCK_STREAM, 0);



 



    if (sock == -1)



    {



        perror_exit("create socket");



    }



 



    // 2.清空标签，写上地址和端口号



    bzero(&server_addr, sizeof(server_addr));



 



    server_addr.sin_family = AF_INET; //选择协议族IPV4



    inet_pton(AF_INET, IP, &server_addr.sin_addr.s_addr);



    // server_addr.sin_addr.s_addr = htonl(INADDR_ANY);//监听本地所有IP地址



    server_addr.sin_port = htons(SERVER_PORT); //绑定端口号



 



    //实现标签贴到收信得信箱上



    ret = bind(sock, (struct sockaddr *)&server_addr, sizeof(server_addr));



    if (ret == -1)



    {



        perror_exit("bind");



    }



 



    //把信箱挂置到传达室，这样，就可以接收信件了



    ret = listen(sock, 128);



    if (ret == -1)



    {



        perror_exit("listen");



    }



 



    //万事俱备，只等来信



    printf("等待客户端的连接\n");



 



    int done = 1;



 



    while (done)



    {



        struct sockaddr_in client;



        int client_sock, len;



        char client_ip[64];



        char buf[256];



 



        socklen_t client_addr_len;



        client_addr_len = sizeof(client);



        client_sock = accept(sock, (struct sockaddr *)&client, &client_addr_len);



 



        //打印客服端IP地址和端口号



        printf("client ip: %s\t port : %d\n",



               inet_ntop(AF_INET, &client.sin_addr.s_addr, client_ip, sizeof(client_ip)),



               ntohs(client.sin_port));



        /*读取客户端发送的数据*/



        len = read(client_sock, buf, sizeof(buf) - 1);



        buf[len] = '\0';



        printf("recive[%d]: %s\n", len, buf);



 



        //转换成大写



        for (i = 0; i < len; i++)



        {



            /*if(buf[i]>='a' && buf[i]<='z'){



                buf[i] = buf[i] - 32;



            }*/



            buf[i] = toupper(buf[i]);



        }



 



        len = write(client_sock, buf, len);



        printf("write finished. len: %d\n", len);



        close(client_sock);



    }



    return 0;



}
```

原文作者：[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605724645

# 【NO.532】实现高并发http 服务器

(实现高并发http 服务器)



# 1.需求分析

实现一个http 服务器项目，服务器启动后监听80端口的tcp 连接，当用户通过任意一款浏览器（IE、火狐和腾讯浏览器等）访问我们的http服务器，http服务器会查找用户访问的html页面是否存在，如果存在则通过http 协议响应客户端的请求，把页面返回给浏览器，浏览器显示html页面；如果页面不存在，则按照http 协议的规定，通知浏览器此页面不存在（404 NOT FOUND）

# 2.何为Html 页面

html，全称Hypertext Markup Language，也就是“超文本链接标示语言”。HTML文本是由 HTML命令组成的描述性文本，HTML 命令可以说明文字、 图形、动画、声音、表格、链接等。 即平常上网所看到的的网页。

```html
<html lang=\"zh-CN\">



<head>



<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\">



<title>This is a test</title>



</head>



<body>



<div align=center height=\"500px\" >



<br/><br/><br/>



<h2>大家好</h2><br/><br/>



<form action="commit" method="post">



尊姓大名: <input type="text" name="name" />



<br/>芳龄几何: <input type="password" name="age" />



<br/><br/><br/><input type="submit" value="提交" />



<input type="reset" value="重置" />



</form>



</div>



</body>



</html>
```

# 3.何为http 协议

HTTP协议是Hyper Text Transfer Protocol(超文本传输协议)的缩写,是用于从万维网(WWW:World Wide Web )服务器传输超文本到本地浏览器的传送协议。
请求格式：
**客户端请求**
客户端发送一个HTTP请求到服务器的请求消息包括以下格式：请求行（request line）、请求头部（header）、空行和请求数据四个部分组成，下图给出了请求报文的一般格式。

![在这里插入图片描述](https://img-blog.csdnimg.cn/43ca25e5af6a4ee292e4d09f54b8088b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



**服务端响应**
服务器响应客户端的HTTP响应也由四个部分组成，分别是：状态行、消息报头、空行和响应正文。

![在这里插入图片描述](https://img-blog.csdnimg.cn/3692a9e4bbe145649c5c5e51a7022574.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



# 4.实现Mini型http 服务器



![在这里插入图片描述](https://img-blog.csdnimg.cn/cee19b52ccc2450ca95b37dfbd420e5f.png)



## 4.1 接收http请求

实现按行读取请求头部

```cpp
//返回值： -1 表示读取出错， 等于0表示读到一个空行， 大于0 表示成功读取一行



int get_line(int sock, char *buf, int size){



    int count = 0;



    char ch = '\0';



    int len = 0;



    



    



    while( (count<size - 1) && ch!='\n'){



        len = read(sock, &ch, 1);



        



        if(len == 1){



            if(ch == '\r'){



                continue;



            }else if(ch == '\n'){



                //buf[count] = '\0';



                break;



            }



            



            //这里处理一般的字符



            buf[count] = ch;



            count++;



            



        }else if( len == -1 ){//读取出错



            perror("read failed");



            count = -1;



            break;



        }else {// read 返回0,客户端关闭sock 连接.



            fprintf(stderr, "client close.\n");



            count = -1;



            break;



        }



    }



    



    if(count >= 0) buf[count] = '\0';



    



    return count;



}
```

如果碰到两个连续的回车换行，即，意味着请求头部结束

```cpp
// 1.读取请求行



void do_http_request(int client_sock)



{



    int len = 0;



    char buf[256];



    char method[64];



    char url[256];



    char path[256];



 



    /*读取客户端发送的http 请求*/



 



    // 1.读取请求行



    len = get_line(client_sock, buf, sizeof(buf));



 



    if (len > 0)



    { //读到了请求行



        int i = 0, j = 0;



        while (!isspace(buf[j]) && (i < sizeof(method) - 1))



        {



            method[i] = buf[j];



            i++;



            j++;



        }



 



        method[i] = '\0';



        if (debug)



            printf("request method: %s\n", method);



 



        if (strncasecmp(method, "GET", i) == 0)



        { //只处理get请求



            if (debug)



                printf("method = GET\n");



 



            //获取url



            while (isspace(buf[j++]))



                ; //跳过白空格



            i = 0;



 



            while (!isspace(buf[j]) && (i < sizeof(url) - 1))



            {



                url[i] = buf[j];



                i++;



                j++;



            }



 



            url[i] = '\0';



 



            if (debug)



                printf("url: %s\n", url);



 



            //继续读取http 头部



            do



            {



                len = get_line(client_sock, buf, sizeof(buf));



                if (debug)



                    printf("read: %s\n", buf);



 



            } while (len > 0);



 



            //***定位服务器本地的html文件***



 



            //处理url 中的?



            {



                char *pos = strchr(url, '?');



                if (pos)



                {



                    *pos = '\0';



                    printf("real url: %s\n", url);



                }



            }



 



            sprintf(path, "./html_docs/%s", url);



            if (debug)



                printf("path: %s\n", path);



 



            //执行http 响应



        }



        else



        {



            //非get请求, 读取http 头部，并响应客户端 501     Method Not Implemented



            fprintf(stderr, "warning! other request [%s]\n", method);



            do



            {



                len = get_line(client_sock, buf, sizeof(buf));



                if (debug)



                    printf("read: %s\n", buf);



 



            } while (len > 0);



            // unimplemented(client_sock);   //在响应时再实现



        }



    }



    else



    {   //请求格式有问题，出错处理



        // bad_request(client_sock);   //在响应时再实现



    }



}
```

## 4.2 解析请求

### 4.2.1 响应http 请求

```cpp
void do_http_request(int client_sock)



{



    int len = 0;



    char buf[256];



    char method[16];



    char url[256];



 



    /*读取客户端发送的http请求*/



 



    // 1.读取请求行



    len = get_line(client_sock, buf, sizeof(buf));



 



    if (len > 0)



    {



        int i, j;



        while (!isspace(buf[j]) && (i < sizeof(method) - 1))



        {



            method[i] = buf[j];



            i++;



            j++;



        }



        method[i] = '\0';



 



        //判断方法是否合法



 



        if (strncasecmp(method, "GET", i) == 0)



        { // GET 方法



            printf("requst = %s\n", method);



 



            //获取url



            while (isspace(buf[++j]))



                ;



            i = -1;



            while (!isspace(buf[j]) && (i < sizeof(url) - 1))



            {



                url[i] = buf[j];



                i++;



                j++;



            }



            url[i] = '\0';



 



            printf("url: %s\n", url);



 



            //读取http 头部，不做任何处理



            do



            {



                len = get_line(client_sock, buf, sizeof(buf));



                printf("read line: %s\n", buf);



            } while (len > 0);



            do_http_response(client_sock);



        }



        else



        {



            printf("other requst = %s\n", method);



            //读取http 头部，不做任何处理



            do



            {



                len = get_line(client_sock, buf, sizeof(buf));



                printf("read line: %s\n", buf);



            } while (len > 0);



        }



    }



    else



    { //出错的处理



    }



}



void do_http_response(int client_sock)



{



    const char *main_header = "HTTP/1.0 200 OK\r\nServer: Martin Server\r\nContent-Type: text/html\r\nConnection: Close\r\n";



 



    const char *welcome_content = "\



<html lang=\"zh-CN\">\n\



<head>\n\



<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\">\n\



<title>This is a test</title>\n\



</head>\n\



<body>\n\



<div align=center height=\"500px\" >\n\



<br/><br/><br/>\n\



<h2>大家好</h2><br/><br/>\n\



<form action=\"commit\" method=\"post\">\n\



尊姓大名: <input type=\"text\" name=\"name\" />\n\



<br/>芳龄几何: <input type=\"password\" name=\"age\" />\n\



<br/><br/><br/><input type=\"submit\" value=\"提交\" />\n\



<input type=\"reset\" value=\"重置\" />\n\



</form>\n\



</div>\n\



</body>\n\



</html>";



 



    char send_buf[64];



    int wc_len = strlen(welcome_content);



    int len = write(client_sock, main_header, strlen(main_header));



 



    if (debug)



        fprintf(stdout, "... do_http_response...\n");



    if (debug)



        fprintf(stdout, "write[%d]: %s", len, main_header);



 



    len = snprintf(send_buf, 64, "Content-Length: %d\r\n\r\n", wc_len);



    len = write(client_sock, send_buf, len);



    if (debug)



        fprintf(stdout, "write[%d]: %s", len, send_buf);



 



    len = write(client_sock, welcome_content, wc_len);



    if (debug)



        fprintf(stdout, "write[%d]: %s", len, welcome_content);



}
```

## 4.3 读取文件

### 4.3.1 **文件概念简介**



![在这里插入图片描述](https://img-blog.csdnimg.cn/333a29aa6b824c079aafe3d702137a5c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


inode - "索引节点",储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。每个inode都有一个号码，操作系统用inode号码来识别不同的文件。ls -i 查看inode 号



## 4.4 stat函数

作用：返回文件的状态信息
\#include <sys/types.h>
\#include <sys/stat.h>
\#include <unistd.h>

```javascript
   int stat(const char *path, struct stat *buf);



   int fstat(int fd, struct stat *buf);



   int lstat(const char *path, struct stat *buf);
```

path:
文件的路径
buf:
传入的保存文件状态的指针，用于保存文件的状态
返回值：
成功返回0，失败返回-1，设置errno

struct stat {
dev_t st_dev; /* ID of device containing file */
ino_t st_ino; /* inode number */
mode_t st_mode; /* S_ISREG(st_mode) 是一个普通文件 S_ISDIR(st_mode) 是一个目录*/

```javascript
           nlink_t   st_nlink;   /* number of hard links */



           uid_t     st_uid;     /* user ID of owner */



           gid_t     st_gid;     /* group ID of owner */



           dev_t     st_rdev;    /* device ID (if special file) */



           off_t     st_size;    /* total size, in bytes */



           blksize_t st_blksize; /* blocksize for filesystem I/O */



           blkcnt_t  st_blocks;  /* number of 512B blocks allocated */



           time_t    st_atime;   /* time of last access */



           time_t    st_mtime;   /* time of last modification */



           time_t    st_ctime;   /* time of last status change */



       };
#include <stdio.h>



#include <unistd.h>



#include <sys/types.h>



#include <sys/socket.h>



#include <string.h>



#include <ctype.h>



#include <arpa/inet.h>



#include <errno.h>



#include <sys/stat.h>



#include <unistd.h>



 



#define SERVER_PORT 80



 



static int debug = 1;



 



int get_line(int sock, char *buf, int size);



void do_http_request(int client_sock);



void do_http_response(int client_sock, const char *path);



int headers(int client_sock, FILE *resource);



void cat(int client_sock, FILE *resource);



 



void not_found(int client_sock);



void inner_error(int client_sock);



 



int main(void)



{



 



    int sock; //代表信箱



    struct sockaddr_in server_addr;



 



    // 1.美女创建信箱



    sock = socket(AF_INET, SOCK_STREAM, 0);



 



    // 2.清空标签，写上地址和端口号



    bzero(&server_addr, sizeof(server_addr));



 



    server_addr.sin_family = AF_INET;                //选择协议族IPV4



    server_addr.sin_addr.s_addr = htonl(INADDR_ANY); //监听本地所有IP地址



    server_addr.sin_port = htons(SERVER_PORT);       //绑定端口号



 



    //实现标签贴到收信得信箱上



    bind(sock, (struct sockaddr *)&server_addr, sizeof(server_addr));



 



    //把信箱挂置到传达室，这样，就可以接收信件了



    listen(sock, 128);



 



    //万事俱备，只等来信



    printf("等待客户端的连接\n");



 



    int done = 1;



 



    while (done)



    {



        struct sockaddr_in client;



        int client_sock, len, i;



        char client_ip[64];



        char buf[256];



 



        socklen_t client_addr_len;



        client_addr_len = sizeof(client);



        client_sock = accept(sock, (struct sockaddr *)&client, &client_addr_len);



 



        //打印客服端IP地址和端口号



        printf("client ip: %s\t port : %d\n",



               inet_ntop(AF_INET, &client.sin_addr.s_addr, client_ip, sizeof(client_ip)),



               ntohs(client.sin_port));



        /*处理http 请求,读取客户端发送的数据*/



        do_http_request(client_sock);



        close(client_sock);



    }



    close(sock);



    return 0;



}



 



void do_http_request(int client_sock)



{



    int len = 0;



    char buf[256];



    char method[64];



    char url[256];



    char path[256];



 



    struct stat st;



 



    /*读取客户端发送的http 请求*/



 



    // 1.读取请求行



    len = get_line(client_sock, buf, sizeof(buf));



 



    if (len > 0)



    { //读到了请求行



        int i = 0, j = 0;



        while (!isspace(buf[j]) && (i < sizeof(method) - 1))



        {



            method[i] = buf[j];



            i++;



            j++;



        }



 



        method[i] = '\0';



        if (debug)



            printf("request method: %s\n", method);



 



        if (strncasecmp(method, "GET", i) == 0)



        { //只处理get请求



            if (debug)



                printf("method = GET\n");



 



            //获取url



            while (isspace(buf[j++]))



                ; //跳过白空格



            i = 0;



 



            while (!isspace(buf[j]) && (i < sizeof(url) - 1))



            {



                url[i] = buf[j];



                i++;



                j++;



            }



 



            url[i] = '\0';



 



            if (debug)



                printf("url: %s\n", url);



 



            //继续读取http 头部



            do



            {



                len = get_line(client_sock, buf, sizeof(buf));



                if (debug)



                    printf("read: %s\n", buf);



 



            } while (len > 0);



 



            //***定位服务器本地的html文件***



 



            //处理url 中的?



            {



                char *pos = strchr(url, '?');



                if (pos)



                {



                    *pos = '\0';



                    printf("real url: %s\n", url);



                }



            }



 



            sprintf(path, "./html_docs/%s", url);



            if (debug)



                printf("path: %s\n", path);



 



            //执行http 响应



            //判断文件是否存在，如果存在就响应200 OK，同时发送相应的html 文件,如果不存在，就响应 404 NOT FOUND.



            if (stat(path, &st) == -1)



            { //文件不存在或是出错



                fprintf(stderr, "stat %s failed. reason: %s\n", path, strerror(errno));



                not_found(client_sock);



            }



            else



            { //文件存在



 



                if (S_ISDIR(st.st_mode))



                {



                    strcat(path, "/index.html");



                }



 



                do_http_response(client_sock, path);



            }



        }



        else



        { //非get请求, 读取http 头部，并响应客户端 501     Method Not Implemented



            fprintf(stderr, "warning! other request [%s]\n", method);



            do



            {



                len = get_line(client_sock, buf, sizeof(buf));



                if (debug)



                    printf("read: %s\n", buf);



 



            } while (len > 0);



 



            // unimplemented(client_sock);   //在响应时再实现



        }



    }



    else



    {   //请求格式有问题，出错处理



        // bad_request(client_sock);   //在响应时再实现



    }



}



 



void do_http_response(int client_sock, const char *path)



{



    int ret = 0;



    FILE *resource = NULL;



 



    resource = fopen(path, "r");



 



    if (resource == NULL)



    {



        not_found(client_sock);



        return;



    }



 



    // 1.发送http 头部



    ret = headers(client_sock, resource);



 



    // 2.发送http body .



    if (!ret)



    {



        cat(client_sock, resource);



    }



 



    fclose(resource);



}



 



/****************************



 *返回关于响应文件信息的http 头部



 *输入：



 *     client_sock - 客服端socket 句柄



 *     resource    - 文件的句柄



 *返回值： 成功返回0 ，失败返回-1



 ******************************/



int headers(int client_sock, FILE *resource)



{



    struct stat st;



    int fileid = 0;



    char tmp[64];



    char buf[1024] = {0};



    strcpy(buf, "HTTP/1.0 200 OK\r\n");



    strcat(buf, "Server: Martin Server\r\n");



    strcat(buf, "Content-Type: text/html\r\n");



    strcat(buf, "Connection: Close\r\n");



 



    fileid = fileno(resource);



 



    if (fstat(fileid, &st) == -1)



    {



        inner_error(client_sock);



        return -1;



    }



 



    snprintf(tmp, 64, "Content-Length: %ld\r\n\r\n", st.st_size);



    strcat(buf, tmp);



 



    if (debug)



        fprintf(stdout, "header: %s\n", buf);



 



    if (send(client_sock, buf, strlen(buf), 0) < 0)



    {



        fprintf(stderr, "send failed. data: %s, reason: %s\n", buf, strerror(errno));



        return -1;



    }



 



    return 0;



}



 



/****************************



 *说明：实现将html文件的内容按行



        读取并送给客户端



 ****************************/



void cat(int client_sock, FILE *resource)



{



    char buf[1024];



 



    fgets(buf, sizeof(buf), resource);



 



    while (!feof(resource))



    {



        int len = write(client_sock, buf, strlen(buf));



 



        if (len < 0)



        { //发送body 的过程中出现问题,怎么办？1.重试？ 2.



            fprintf(stderr, "send body error. reason: %s\n", strerror(errno));



            break;



        }



 



        if (debug)



            fprintf(stdout, "%s", buf);



        fgets(buf, sizeof(buf), resource);



    }



}



 



void do_http_response1(int client_sock)



{



    const char *main_header = "HTTP/1.0 200 OK\r\nServer: Martin Server\r\nContent-Type: text/html\r\nConnection: Close\r\n";



 



    const char *welcome_content = "\



<html lang=\"zh-CN\">\n\



<head>\n\



<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\">\n\



<title>This is a test</title>\n\



</head>\n\



<body>\n\



<div align=center height=\"500px\" >\n\



<br/><br/><br/>\n\



<h2>大家好</h2><br/><br/>\n\



<form action=\"commit\" method=\"post\">\n\



尊姓大名: <input type=\"text\" name=\"name\" />\n\



<br/>芳龄几何: <input type=\"password\" name=\"age\" />\n\



<br/><br/><br/><input type=\"submit\" value=\"提交\" />\n\



<input type=\"reset\" value=\"重置\" />\n\



</form>\n\



</div>\n\



</body>\n\



</html>";



 



    // 1. 发送main_header



    int len = write(client_sock, main_header, strlen(main_header));



 



    if (debug)



        fprintf(stdout, "... do_http_response...\n");



    if (debug)



        fprintf(stdout, "write[%d]: %s", len, main_header);



 



    // 2. 生成Content-Length



    char send_buf[64];



    int wc_len = strlen(welcome_content);



    len = snprintf(send_buf, 64, "Content-Length: %d\r\n\r\n", wc_len);



    len = write(client_sock, send_buf, len);



 



    if (debug)



        fprintf(stdout, "write[%d]: %s", len, send_buf);



 



    len = write(client_sock, welcome_content, wc_len);



    if (debug)



        fprintf(stdout, "write[%d]: %s", len, welcome_content);



}



 



//返回值： -1 表示读取出错， 等于0表示读到一个空行， 大于0 表示成功读取一行



int get_line(int sock, char *buf, int size)



{



    int count = 0;



    char ch = '\0';



    int len = 0;



 



    while ((count < size - 1) && ch != '\n')



    {



        len = read(sock, &ch, 1);



 



        if (len == 1)



        {



            if (ch == '\r')



            {



                continue;



            }



            else if (ch == '\n')



            {



                // buf[count] = '\0';



                break;



            }



 



            //这里处理一般的字符



            buf[count] = ch;



            count++;



        }



        else if (len == -1)



        { //读取出错



            perror("read failed");



            count = -1;



            break;



        }



        else



        { // read 返回0,客户端关闭sock 连接.



            fprintf(stderr, "client close.\n");



            count = -1;



            break;



        }



    }



 



    if (count >= 0)



        buf[count] = '\0';



 



    return count;



}



 



void not_found(int client_sock)



{



    const char *reply = "HTTP/1.0 404 NOT FOUND\r\n\



Content-Type: text/html\r\n\



\r\n\



<HTML lang=\"zh-CN\">\r\n\



<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\">\r\n\



<HEAD>\r\n\



<TITLE>NOT FOUND</TITLE>\r\n\



</HEAD>\r\n\



<BODY>\r\n\



    <P>文件不存在！\r\n\



    <P>The server could not fulfill your request because the resource specified is unavailable or nonexistent.\r\n\



</BODY>\r\n\



</HTML>";



 



    int len = write(client_sock, reply, strlen(reply));



    if (debug)



        fprintf(stdout, reply);



 



    if (len <= 0)



    {



        fprintf(stderr, "send reply failed. reason: %s\n", strerror(errno));



    }



}



 



void inner_error(int client_sock)



{



    const char *reply = "HTTP/1.0 500 Internal Sever Error\r\n\



Content-Type: text/html\r\n\



\r\n\



<HTML lang=\"zh-CN\">\r\n\



<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\">\r\n\



<HEAD>\r\n\



<TITLE>Inner Error</TITLE>\r\n\



</HEAD>\r\n\



<BODY>\r\n\



    <P>服务器内部出错.\r\n\



</BODY>\r\n\



</HTML>";



 



    int len = write(client_sock, reply, strlen(reply));



    if (debug)



        fprintf(stdout, reply);



 



    if (len <= 0)



    {



        fprintf(stderr, "send reply failed. reason: %s\n", strerror(errno));



    }



}
```

## 4.5 并发处理

### 4.5.1 并发概述

通俗的并发通常是指同时能并行的处理多个任务。

![在这里插入图片描述](https://img-blog.csdnimg.cn/8de35c3069ad4f1cb429dbce1900c0c1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)



### 4.5.2 并发

同时拥有两个或者多个线程，如果程序在单核处理器上运行，多个线程将交替的换入或者换出内存，这些线程是同时“存在”的。
每个线程都处于执行过程中的某个状态，如果运行在多核处理器上，此时，程序中的每个线程都将分配到一个处理器核上，因此可以同时运行。

### 4.5.3 高并发

高并发是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够 同时并行处理 很多请求

### 4.5.4 pthread_create函数

创建一个新线程，并行的执行任务。
\#include <pthread.h>
int pthread_create(pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine) (void *), void *arg);
返回值：成功：0； 失败：错误号。
参数：
pthread_t：当前Linux中可理解为：typedef unsigned long int pthread_t;
参数1：传出参数，保存系统为我们分配好的线程ID
参数2：通常传NULL，表示使用线程默认属性。若想使用具体属性也可以修改该参数。
参数3：函数指针，指向线程主函数(线程体)，该函数运行结束，则线程结束。
参数4：线程主函数执行期间所使用的参数。
在一个线程中调用pthread_create()创建新的线程后，当前线程从pthread_create()返回继续往下执行，而新的线程所执行的代码由我们传给pthread_create的函数指针start_routine决定。start_routine函数接收一个参数，是通过pthread_create的arg参数传递给它的，该参数的类型为void *，这个指针按什么类型解释由调用者自己定义。start_routine的返回值类型也是void *，这个指针的含义同样由调用者自己定义。start_routine返回时，这个线程就退出了，其它线程可以调用pthread_join得到start_routine的返回值，以后再详细介绍pthread_join。
pthread_create成功返回后，新创建的线程的id被填写到thread参数所指向的内存单元。
attr参数表示线程属性，本节不深入讨论线程属性，所有代码例子都传NULL给attr参数，表示线程属性取缺省值。

### 4.5.5 并发回声服务器改造

```cpp
#include <stdio.h>



#include <unistd.h>



#include <sys/types.h>



#include <sys/socket.h>



#include <string.h>



#include <ctype.h>



#include <arpa/inet.h>



#include <pthread.h>



#include <stdlib.h>



#include <errno.h>



 



#define SERVER_PORT 666



 



void *thread_handleRequest(void *arg);



 



int main(void)



{



 



    int sock; //代表信箱



    struct sockaddr_in server_addr;



 



    // 1.美女创建信箱



    sock = socket(AF_INET, SOCK_STREAM, 0);



 



    // 2.清空标签，写上地址和端口号



    bzero(&server_addr, sizeof(server_addr));



 



    server_addr.sin_family = AF_INET;                //选择协议族IPV4



    server_addr.sin_addr.s_addr = htonl(INADDR_ANY); //监听本地所有IP地址



    server_addr.sin_port = htons(SERVER_PORT);       //绑定端口号



 



    //实现标签贴到收信得信箱上



    bind(sock, (struct sockaddr *)&server_addr, sizeof(server_addr));



 



    //把信箱挂置到传达室，这样，就可以接收信件了



    listen(sock, 128);



 



    //万事俱备，只等来信



    printf("等待客户端的连接\n");



 



    int done = 1;



 



    while (done)



    {



        struct sockaddr_in client;



        int client_sock, len;



        char client_ip[64];



        // char buf[256];



        // int  i=0;



 



        socklen_t client_addr_len;



        client_addr_len = sizeof(client);



        client_sock = accept(sock, (struct sockaddr *)&client, &client_addr_len);



 



        //打印客服端IP地址和端口号



        printf("client ip: %s\t port : %d\n",



               inet_ntop(AF_INET, &client.sin_addr.s_addr, client_ip, sizeof(client_ip)),



               ntohs(client.sin_port));



 



        //启动线程，完成和和客户端的交互



        {



            pthread_t tid;



            int *ptr_int = NULL;



            int err = 0;



 



            ptr_int = (int *)malloc(sizeof(int));



            *ptr_int = client_sock;



 



            if (err = pthread_create(&tid, NULL, thread_handleRequest, (void *)ptr_int))



            {



                fprintf(stderr, "Can't create thread, reason: %s\n", strerror(errno));



                if (ptr_int)



                    free(ptr_int);



            }



        }



 



        /*读取客户端发送的数据*/



        /*len = read(client_sock, buf, sizeof(buf)-1);



        buf[len] = '\0';



        printf("receive[%d]: %s\n", len, buf);







        //转换成大写



        for(i=0; i<len; i++){



            //if(buf[i]>='a' && buf[i]<='z'){



            //    buf[i] = buf[i] - 32;



            //}



            buf[i] = toupper(buf[i]);



        }











        len = write(client_sock, buf, len);







        printf("finished. len: %d\n", len);



        close(client_sock);



        */



    }



    close(sock);



    return 0;



}



 



void *thread_handleRequest(void *arg)



{



    int client_sock = *(int *)arg;



    char buf[256];



    int i = 0;



 



    int len = read(client_sock, buf, sizeof(buf) - 1);



    buf[len] = '\0';



    printf("receive[%d]: %s\n", len, buf);



 



    //转换成大写



    for (i = 0; i < len; i++)



    {



        // if(buf[i]>='a' && buf[i]<='z'){



        //     buf[i] = buf[i] - 32;



        // }



        buf[i] = toupper(buf[i]);



    }



 



    len = write(client_sock, buf, len);



 



    printf("finished. len: %d\n", len);



    close(client_sock);



 



    if (arg)



        free(arg);



}
```

### 4.5.6 并发Mini http 服务器改造

```cpp
#include <stdio.h>



#include <unistd.h>



#include <sys/types.h>



#include <sys/socket.h>



#include <string.h>



#include <ctype.h>



#include <arpa/inet.h>



#include <errno.h>



#include <sys/stat.h>



#include <unistd.h>



#include <pthread.h>



 



#define SERVER_PORT 80



 



static int debug = 1;



 



int get_line(int sock, char *buf, int size);



void *do_http_request(void *client_sock);



void do_http_response(int client_sock, const char *path);



int headers(int client_sock, FILE *resource);



void cat(int client_sock, FILE *resource);



 



void not_found(int client_sock);     // 404



void unimplemented(int client_sock); // 500



void bad_request(int client_sock);   // 400



void inner_error(int client_sock);



 



int main(void)



{



 



    int sock; //代表信箱



    struct sockaddr_in server_addr;



 



    // 1.美女创建信箱



    sock = socket(AF_INET, SOCK_STREAM, 0);



 



    // 2.清空标签，写上地址和端口号



    bzero(&server_addr, sizeof(server_addr));



 



    server_addr.sin_family = AF_INET;                //选择协议族IPV4



    server_addr.sin_addr.s_addr = htonl(INADDR_ANY); //监听本地所有IP地址



    server_addr.sin_port = htons(SERVER_PORT);       //绑定端口号



 



    //实现标签贴到收信得信箱上



    bind(sock, (struct sockaddr *)&server_addr, sizeof(server_addr));



 



    //把信箱挂置到传达室，这样，就可以接收信件了



    listen(sock, 128);



 



    //万事俱备，只等来信



    printf("等待客户端的连接\n");



 



    int done = 1;



 



    while (done)



    {



        struct sockaddr_in client;



        int client_sock, len, i;



        char client_ip[64];



        char buf[256];



        pthread_t id;



        int *pclient_sock = NULL;



 



        socklen_t client_addr_len;



        client_addr_len = sizeof(client);



        client_sock = accept(sock, (struct sockaddr *)&client, &client_addr_len);



 



        //打印客服端IP地址和端口号



        printf("client ip: %s\t port : %d\n",



               inet_ntop(AF_INET, &client.sin_addr.s_addr, client_ip, sizeof(client_ip)),



               ntohs(client.sin_port));



 



        /*处理http 请求,读取客户端发送的数据*/



        // do_http_request(client_sock);



 



        //启动线程处理http 请求



        pclient_sock = (int *)malloc(sizeof(int));



        *pclient_sock = client_sock;



 



        pthread_create(&id, NULL, do_http_request, (void *)pclient_sock);



 



        // close(client_sock);



    }



    close(sock);



    return 0;



}



 



void *do_http_request(void *pclient_sock)



{



    int len = 0;



    char buf[256];



    char method[64];



    char url[256];



    char path[256];



    int client_sock = *(int *)pclient_sock;



 



    struct stat st;



 



    /*读取客户端发送的http 请求*/



 



    // 1.读取请求行



    len = get_line(client_sock, buf, sizeof(buf));



 



    if (len > 0)



    { //读到了请求行



        int i = 0, j = 0;



        while (!isspace(buf[j]) && (i < sizeof(method) - 1))



        {



            method[i] = buf[j];



            i++;



            j++;



        }



 



        method[i] = '\0';



        if (debug)



            printf("request method: %s\n", method);



 



        if (strncasecmp(method, "GET", i) == 0)



        { //只处理get请求



            if (debug)



                printf("method = GET\n");



 



            //获取url



            while (isspace(buf[j++]))



                ; //跳过白空格



            i = 0;



 



            while (!isspace(buf[j]) && (i < sizeof(url) - 1))



            {



                url[i] = buf[j];



                i++;



                j++;



            }



 



            url[i] = '\0';



 



            if (debug)



                printf("url: %s\n", url);



 



            //继续读取http 头部



            do



            {



                len = get_line(client_sock, buf, sizeof(buf));



                if (debug)



                    printf("read: %s\n", buf);



 



            } while (len > 0);



 



            //***定位服务器本地的html文件***



 



            //处理url 中的?



            {



                char *pos = strchr(url, '?');



                if (pos)



                {



                    *pos = '\0';



                    printf("real url: %s\n", url);



                }



            }



 



            sprintf(path, "./html_docs/%s", url);



            if (debug)



                printf("path: %s\n", path);



 



            //执行http 响应



            //判断文件是否存在，如果存在就响应200 OK，同时发送相应的html 文件,如果不存在，就响应 404 NOT FOUND.



            if (stat(path, &st) == -1)



            { //文件不存在或是出错



                fprintf(stderr, "stat %s failed. reason: %s\n", path, strerror(errno));



                not_found(client_sock);



            }



            else



            { //文件存在



 



                if (S_ISDIR(st.st_mode))



                {



                    strcat(path, "/index.html");



                }



 



                do_http_response(client_sock, path);



            }



        }



        else



        { //非get请求, 读取http 头部，并响应客户端 501     Method Not Implemented



            fprintf(stderr, "warning! other request [%s]\n", method);



            do



            {



                len = get_line(client_sock, buf, sizeof(buf));



                if (debug)



                    printf("read: %s\n", buf);



 



            } while (len > 0);



 



            unimplemented(client_sock); //请求未实现



        }



    }



    else



    {                             //请求格式有问题，出错处理



        bad_request(client_sock); //在响应时再实现



    }



 



    close(client_sock);



    if (pclient_sock)



        free(pclient_sock); //释放动态分配的内存



 



    return NULL;



}



 



void do_http_response(int client_sock, const char *path)



{



    int ret = 0;



    FILE *resource = NULL;



 



    resource = fopen(path, "r");



 



    if (resource == NULL)



    {



        not_found(client_sock);



        return;



    }



 



    // 1.发送http 头部



    ret = headers(client_sock, resource);



 



    // 2.发送http body .



    if (!ret)



    {



        cat(client_sock, resource);



    }



 



    fclose(resource);



}



 



/****************************



 *返回关于响应文件信息的http 头部



 *输入：



 *     client_sock - 客服端socket 句柄



 *     resource    - 文件的句柄



 *返回值： 成功返回0 ，失败返回-1



 ******************************/



int headers(int client_sock, FILE *resource)



{



    struct stat st;



    int fileid = 0;



    char tmp[64];



    char buf[1024] = {0};



    strcpy(buf, "HTTP/1.0 200 OK\r\n");



    strcat(buf, "Server: Martin Server\r\n");



    strcat(buf, "Content-Type: text/html\r\n");



    strcat(buf, "Connection: Close\r\n");



 



    fileid = fileno(resource);



 



    if (fstat(fileid, &st) == -1)



    {



        inner_error(client_sock);



        return -1;



    }



 



    snprintf(tmp, 64, "Content-Length: %ld\r\n\r\n", st.st_size);



    strcat(buf, tmp);



 



    if (debug)



        fprintf(stdout, "header: %s\n", buf);



 



    if (send(client_sock, buf, strlen(buf), 0) < 0)



    {



        fprintf(stderr, "send failed. data: %s, reason: %s\n", buf, strerror(errno));



        return -1;



    }



 



    return 0;



}



 



/****************************



 *说明：实现将html文件的内容按行



        读取并送给客户端



 ****************************/



void cat(int client_sock, FILE *resource)



{



    char buf[1024];



 



    fgets(buf, sizeof(buf), resource);



 



    while (!feof(resource))



    {



        int len = write(client_sock, buf, strlen(buf));



 



        if (len < 0)



        { //发送body 的过程中出现问题,怎么办？1.重试？ 2.



            fprintf(stderr, "send body error. reason: %s\n", strerror(errno));



            break;



        }



 



        if (debug)



            fprintf(stdout, "%s", buf);



        fgets(buf, sizeof(buf), resource);



    }



}



 



void do_http_response1(int client_sock)



{



    const char *main_header = "HTTP/1.0 200 OK\r\nServer: Martin Server\r\nContent-Type: text/html\r\nConnection: Close\r\n";



 



    const char *welcome_content = "\



<html lang=\"zh-CN\">\n\



<head>\n\



<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\">\n\



<title>This is a test</title>\n\



</head>\n\



<body>\n\



<div align=center height=\"500px\" >\n\



<br/><br/><br/>\n\



<h2>大家好</h2><br/><br/>\n\



<form action=\"commit\" method=\"post\">\n\



尊姓大名: <input type=\"text\" name=\"name\" />\n\



<br/>芳龄几何: <input type=\"password\" name=\"age\" />\n\



<br/><br/><br/><input type=\"submit\" value=\"提交\" />\n\



<input type=\"reset\" value=\"重置\" />\n\



</form>\n\



</div>\n\



</body>\n\



</html>";



 



    // 1. 发送main_header



    int len = write(client_sock, main_header, strlen(main_header));



 



    if (debug)



        fprintf(stdout, "... do_http_response...\n");



    if (debug)



        fprintf(stdout, "write[%d]: %s", len, main_header);



 



    // 2. 生成Content-Length



    char send_buf[64];



    int wc_len = strlen(welcome_content);



    len = snprintf(send_buf, 64, "Content-Length: %d\r\n\r\n", wc_len);



    len = write(client_sock, send_buf, len);



 



    if (debug)



        fprintf(stdout, "write[%d]: %s", len, send_buf);



 



    len = write(client_sock, welcome_content, wc_len);



    if (debug)



        fprintf(stdout, "write[%d]: %s", len, welcome_content);



}



 



//返回值： -1 表示读取出错， 等于0表示读到一个空行， 大于0 表示成功读取一行



int get_line(int sock, char *buf, int size)



{



    int count = 0;



    char ch = '\0';



    int len = 0;



 



    while ((count < size - 1) && ch != '\n')



    {



        len = read(sock, &ch, 1);



 



        if (len == 1)



        {



            if (ch == '\r')



            {



                continue;



            }



            else if (ch == '\n')



            {



                // buf[count] = '\0';



                break;



            }



 



            //这里处理一般的字符



            buf[count] = ch;



            count++;



        }



        else if (len == -1)



        { //读取出错



            perror("read failed");



            count = -1;



            break;



        }



        else



        { // read 返回0,客户端关闭sock 连接.



            fprintf(stderr, "client close.\n");



            count = -1;



            break;



        }



    }



 



    if (count >= 0)



        buf[count] = '\0';



 



    return count;



}



 



void not_found(int client_sock)



{



    const char *reply = "HTTP/1.0 404 NOT FOUND\r\n\



Content-Type: text/html\r\n\



\r\n\



<HTML lang=\"zh-CN\">\r\n\



<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\">\r\n\



<HEAD>\r\n\



<TITLE>NOT FOUND</TITLE>\r\n\



</HEAD>\r\n\



<BODY>\r\n\



    <P>文件不存在！\r\n\



    <P>The server could not fulfill your request because the resource specified is unavailable or nonexistent.\r\n\



</BODY>\r\n\



</HTML>";



 



    int len = write(client_sock, reply, strlen(reply));



    if (debug)



        fprintf(stdout, reply);



 



    if (len <= 0)



    {



        fprintf(stderr, "send reply failed. reason: %s\n", strerror(errno));



    }



}



 



void unimplemented(int client_sock)



{



    const char *reply = "HTTP/1.0 501 Method Not Implemented\r\n\



Content-Type: text/html\r\n\



\r\n\



<HTML>\r\n\



<HEAD>\r\n\



<TITLE>Method Not Implemented</TITLE>\r\n\



</HEAD>\r\n\



<BODY>\r\n\



    <P>HTTP request method not supported.\r\n\



</BODY>\r\n\



</HTML>";



 



    int len = write(client_sock, reply, strlen(reply));



    if (debug)



        fprintf(stdout, reply);



 



    if (len <= 0)



    {



        fprintf(stderr, "send reply failed. reason: %s\n", strerror(errno));



    }



}



 



void bad_request(client_sock)



{



    const char *reply = "HTTP/1.0 400 BAD REQUEST\r\n\



Content-Type: text/html\r\n\



\r\n\



<HTML>\r\n\



<HEAD>\r\n\



<TITLE>BAD REQUEST</TITLE>\r\n\



</HEAD>\r\n\



<BODY>\r\n\



    <P>Your browser sent a bad request！\r\n\



</BODY>\r\n\



</HTML>";



 



    int len = write(client_sock, reply, strlen(reply));



    if (len <= 0)



    {



        fprintf(stderr, "send reply failed. reason: %s\n", strerror(errno));



    }



}



 



void inner_error(int client_sock)



{



    const char *reply = "HTTP/1.0 500 Internal Sever Error\r\n\



Content-Type: text/html\r\n\



\r\n\



<HTML lang=\"zh-CN\">\r\n\



<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\">\r\n\



<HEAD>\r\n\



<TITLE>Inner Error</TITLE>\r\n\



</HEAD>\r\n\



<BODY>\r\n\



    <P>服务器内部出错.\r\n\



</BODY>\r\n\



</HTML>";



 


    int len = write(client_sock, reply, strlen(reply));



    if (debug)



        fprintf(stdout, reply);



 



    if (len <= 0)



    {



        fprintf(stderr, "send reply failed. reason: %s\n", strerror(errno));



    }



}
```

原文作者：[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605742285

# 【NO.533】nginx过滤器模块

**什么是过滤器呢？**

你在申请网页的时候，打开浏览器
可以看到这里有两个字叫做广告，

![在这里插入图片描述](https://img-blog.csdnimg.cn/edd89fff2f81433bb34af658486439f3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



在我们申请网页的过程，这个广告是怎么做的？
我们去申请这个网页的时候，请注意广告的显示，它不是在我们请求的时候，在我们请求的过程中间，在http十一个阶段的时候，它是没有插入广告地方，在response里面这个过程中间它在哪个地方去插入广告？
就是在过滤器的时候

![在这里插入图片描述](https://img-blog.csdnimg.cn/00399bf7557947479d95a1e6e9acb4c8.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)



在这里讲另外一个新的方法，就是我们在过滤器中间同样也可以做，

![在这里插入图片描述](https://img-blog.csdnimg.cn/c598d979bfb74d3bab511e08d24536db.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



Handler模块：接收到请求并返回结果，

![在这里插入图片描述](https://img-blog.csdnimg.cn/f0ac3500615141dab20a40933242787f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_13,color_FFFFFF,t_70,g_se,x_16)


Upstream:

![在这里插入图片描述](https://img-blog.csdnimg.cn/653cd9e13c5b4ceebaa93bb827a77ef1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


Filter:

![在这里插入图片描述](https://img-blog.csdnimg.cn/512c329d147a40afb62f4f49b63215b0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



**过滤器模块它主要是用来做什么？**
过滤器模块主要起到的作用就是在response的时候返回结果的时候，我们加上一些想要的,加上一些特别的信息往里面

也就是我们从后端返回结果，然后再返回前端的时候，这时候我们可以加东西，比如说哪些功能，我们对一个结果，我们请求结果返回参数，我们对它作MD5的校验，验证对还是不对？

就是俄罗斯人的这个英文，我们不能用正常人的思维来理解，它过滤器其实是在这个过程中要去添加一些东西，把这个 filter也就在这个过程数据就像漏斗一样，然后往外流的时候我们可以这个进行删减添加
比如说我们想屏蔽一些一些参数，我们也可以通过过滤器去实现。
过滤器主要是作用在response返回的这个流程中间，

```cpp
void  *ngx_http_ads_filter_create_loc_conf(ngx_conf_t *cf);



char  *ngx_http_ads_filter_merge_loc_conf(ngx_conf_t *cf, void *prev, void *conf);



ngx_int_t ngx_http_ads_filter_init(ngx_conf_t *cf);



 



ngx_int_t ngx_http_ads_filter_header_filter(ngx_http_request_t *r);



ngx_int_t ngx_http_ads_filter_body_filter(ngx_http_request_t *r, ngx_chain_t *chain);



 



ngx_str_t ads_content = ngx_string("<h2>Author : King</h2><p><a href=\"http://www.0voice.com\">0voice</a></p>");



 



 



typedef struct {



    ngx_flag_t enable;



} ngx_http_ads_filter_conf_t;



 



 



// nginx.conf --> add_ads on/off



static ngx_command_t ngx_http_ads_filter_cmd[] = {



    // 从两个维度来描述command,



    //第一个就是形容command在conf文件的那个位置？ 那个位置怎么理解，



    //你是放location里面还是server里面



    //第二个维度就是他带的参数，可以带一个参数两个参数三个参数或者on,off参数



    {



        ngx_string("add_ads"), // on / off



        NGX_HTTP_LOC_CONF | NGX_CONF_FLAG,



        ngx_conf_set_flag_slot,



        NGX_HTTP_LOC_CONF_OFFSET,



        offsetof(ngx_http_ads_filter_conf_t, enable),



        NULL



    },



    ngx_null_command



}



 



// nginx -c nginx.conf



static ngx_http_module_t ngx_http_ads_filter_ctx = {



 



    NULL, //pre



    ngx_http_ads_filter_init, //post



 



    NULL, 



    NULL,



 



    NULL,



    NULL,



 



    ngx_http_ads_filter_create_loc_conf, //nginx.conf 



    ngx_http_ads_filter_merge_loc_conf,  // 



    



 



}



 



// config --> 



// ./configure --> objs/ngx_modules.c 



ngx_module_t ngx_http_ads_filter_module = {



    NGX_MODULE_V1,



    &ngx_http_ads_filter_ctx,



    ngx_http_ads_filter_cmd,



    NGX_HTTP_MODULE,



 



    NULL,



    NULL,



    NULL,



    NULL,



 



    NULL,



    NULL,



    NULL,



 



    NGX_MODULE_V1_PADDING



 



};



 



static ngx_http_output_header_filter_pt ngx_http_next_header_filter;



static ngx_http_output_body_filter_pt ngx_http_next_body_filter;



 



// nginx -c nginx.conf



ngx_int_t ngx_http_ads_filter_init(ngx_conf_t *cf) {



 



    // head



    ngx_http_next_header_filter = ngx_http_top_header_filter;



    ngx_http_top_header_filter = ngx_http_ads_filter_header_filter;



 



    // body



    ngx_http_next_body_filter = ngx_http_top_body_filter;



    ngx_http_top_body_filter = ngx_http_ads_filter_body_filter;



}



 



// content_length + strlen(add_ads)



ngx_int_t ngx_http_ads_filter_header_filter(ngx_http_request_t *r) {



 



    //ngx_http_ads_filter_conf_t *conf = ngx_http_get_module_loc_conf(r, ngx_http_ads_filter_module)



    



    if (r->headers_out.content_length_n > 0) {



        r->headers_out.content_length_n += ads_content.len;



    }



    return ngx_http_next_header_filter(r);



}



 



// add content 



// content.str



 



// html = h1 + h2 + h3 + h4



 



// send(fd, html, length, 0);



 



 



// send(fd, h1);



// send(h2);



// send(h3);



// send(h4);



 



 



 



ngx_int_t ngx_http_ads_filter_body_filter(ngx_http_request_t *r, ngx_chain_t *chain) {



 



    // ads_content --> body



    //if () {



    //    return ngx_http_next_body_filter(r, chain);



    //}



 



    ngx_buf_t *b = ngx_create_temp_buf(r->pool, ads_content->len);



    b->start = b->pos = ads_content.data;



    b->last = b->pos + ads_content.len;



    



    // html = h1 + h2 + h3 + h4类似



 



    ngx_chain_t *cl = ngx_alloc_chain_link(r->pool);



    cl->buf = b;



    cl->next = chain;



    



    return ngx_http_next_body_filter(r, cl);



    //return ngx_http_ads_filter_body_filter(r, chain);



 



}



 



 
```

从两个维度来描述command,第一个就是形容command在conf文件的那个位置？ 那个位置怎么理解，你是放location里面还是server里面
第二个维度就是他带的参数，可以带一个参数两个参数三个参数或者on,off参数

container_of这个宏就是通过成员找结构体，通过这个成员变量的地址去找到结构体的指针



![在这里插入图片描述](https://img-blog.csdnimg.cn/b72ba9e51c49437ba53e0274f2781dda.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



现在只要清楚这个执行完了之后，我们这个enable里面就有值就可以了。



![在这里插入图片描述](https://img-blog.csdnimg.cn/a4b4eeddb35a4438bbea84dd30afec3d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



这个过滤器它的入口函数在哪里？这个请求我们怎么就使得他能够请求到我们对应的这个模块里面，我们在整个配置文件
这个结束完了之后，我们还要实现一个东西，我们去初始化这个过滤器模块的入口，也就是说这个入口是在这个配置环节解析完以后，nginx -c nginx.conf执行完后，再去执行ngx_http_ads_filter_init

接下来给大家写的就是函数的编写。



![在这里插入图片描述](https://img-blog.csdnimg.cn/95e56ec5904f4094ae0b3259ee3a55db.png)



三个过滤性模板怎么组织起来的，我在加一个模块怎么加入呢？
首先这些模块已经是一个链了，过滤器的模块是通过头插法这个方式来做的
nginx提供了两个过滤器模块

![在这里插入图片描述](https://img-blog.csdnimg.cn/11dea5da5d3646c4bdce115bfcdc5554.png)



![在这里插入图片描述](https://img-blog.csdnimg.cn/54660a941b7f4218b36422cf389fe55e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_19,color_FFFFFF,t_70,g_se,x_16)





![在这里插入图片描述](https://img-blog.csdnimg.cn/0b2409bc17344f1489516d40d1bed2a0.png)



过滤器模块请注意是一个结构体里面，它指向一个next，那这个next里面有一个指针，有一个函数指针，它是一个函数调用，每一个结构体里面都有一个函数指针，我们通过for循环去遍历这里头每一个节点，遍历到节点一的时候调用它的回调函数，遍历节点二的时候调用它的回调函数，这个过程中间就形成了一种责任链的方式来理解

调用完ngx_http_ads_filter_init以后会去分配一个节点，在这个节点里面它有个指指向next，然后再把当前的Top的这个指针赋值给这个结构体里面这个函数指针，从而在我们循环的时候，它就能够走到这个函数

我们任何一个模块在跑起来的时候，他有两个地方会跑到模块里面跑到，第一个就是在nginx初始化的时候，就在nginx启动的时候，这时候主要是去解析这个配置文件，那配置文件的信息拿到对应的模块里面，
这是第一个地方

这个模块在这个配置文件解析完了以后，会去设置对应的每一次请求的入口函数，也就是说这两个地方，
一个就是解析conf的数据放到模块内，并且设置每一次请求的入口函数。
第二个就是在我们每一次请求的时候，它能够走到对应这个函数。
每一个模块都在执行的时候，都是这两个地方

如果去捋这个流程的时候，就看哪一些是这个解析配置文件的时候走的，哪一些它是在我们每一次请求的时候走的



![在这里插入图片描述](https://img-blog.csdnimg.cn/3bbf931949f5445cad353555013edc37.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



Handler他是拿到了请求的信息去修改，
后端返回过来的信息，我们对后端的信息修改用过滤器

抓住两个点，一个初始化的地方，另外一个是每次请求以及它的具体在运行时走的地方，

http入口在哪个地方
找到这个关键字，

第一个哪些是在初始化的时候，
第二个就是我们现在的入口函数在哪个地方。
第三个就是每次请求



![在这里插入图片描述](https://img-blog.csdnimg.cn/3e348229da82478287e1977b1d6b632b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



这个ngx_http_block这个函数它是在什么时候执行的？
是在配置文件的时候执行还是在我们每一次请求是执行？

原文作者：[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605225430

# 【NO.534】随处可见的红黑树

## 1.红黑树为什么常用

1.当做查找以key-value
通过key去查找value,查找性能比较快
比如通过socket去查找客户端id,还有内核内存怎么使用?每用malloc分配一个内存就加入一颗红黑树,free(ptr)释放的时候key-value去查找对应的块
2.通过中序遍历是顺序的
进程的调度与红黑树什么关系呢?有些进程由于需要满足某些条件而处于等待状态等待某一个时间在未来某个时刻会再次运行,就把这些等待的再未来某个时刻会再次运行的进程加入红黑树去管理,

![在这里插入图片描述](https://img-blog.csdnimg.cn/1acc0ebae6ca48a9959e66b304d79470.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)



## 2.那么红黑树怎么实现?

那就不得不先聊二叉排序树了,名义上是二叉树,但是二叉树有一种最坏的情况是链表,违背了快速查找,为了更好的解决这个问题就引入了红黑树



![在这里插入图片描述](https://img-blog.csdnimg.cn/d1d3387a17b0418fba0f0d2d5c35312d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



**二叉树的定义与实现**

```cpp
#include <stdio.h>



#include <stdlib.h>



#include <string.h>



 



#include <assert.h>



 



#if 0



 



typedef int KEY_VALUE;



 



struct bstree_node {



    KEY_VALUE data;



    struct bstree_node *left;



    struct bstree_node *right;



};



 



struct bstree {



    struct bstree_node *root;



};



 



struct bstree_node *bstree_create_node(KEY_VALUE key) {



    struct bstree_node *node = (struct bstree_node*)malloc(sizeof(struct bstree_node));



    if (node == NULL) {



        assert(0);



    }



    node->data = key;



    node->left = node->right = NULL;



 



    return node;



}



 



int bstree_insert(struct bstree *T, int key) {



 



    assert(T != NULL);



 



    if (T->root == NULL) {



        T->root = bstree_create_node(key);



        return 0;



    }



 



    struct bstree_node *node = T->root;



    struct bstree_node *tmp = T->root;



 



    while (node != NULL) {



        tmp = node;



        if (key < node->data) {



            node = node->left;



        } else {



            node = node->right;



        }



    }



 



    if (key < tmp->data) {



        tmp->left = bstree_create_node(key);



    } else {



        tmp->right = bstree_create_node(key);



    }



    



    return 0;



}



 



int bstree_traversal(struct bstree_node *node) {



    if (node == NULL) return 0;



    



    bstree_traversal(node->left);



    printf("%4d ", node->data);



    bstree_traversal(node->right);



}



 



 



#define ARRAY_LENGTH        20



int main() {



 



    int keyArray[ARRAY_LENGTH] = {24,25,13,35,23, 26,67,47,38,98, 20,13,17,49,12, 21,9,18,14,15};



 



    struct bstree T = {0};



    int i = 0;



    for (i = 0;i < ARRAY_LENGTH;i ++) {



        bstree_insert(&T, keyArray[i]);



    }



 



    bstree_traversal(T.root);



 



    printf("\n");



}



 



#else



 



typedef int KEY_VALUE;



 



 



#define BSTREE_ENTRY(name, type)     \



    struct name {                    \



        struct type *left;            \



        struct type *right;            \



    }



 



struct bstree_node {



    KEY_VALUE data;



    BSTREE_ENTRY(, bstree_node) bst;



};



 



struct bstree {



    struct bstree_node *root;



};



 



struct bstree_node *bstree_create_node(KEY_VALUE key) {



    struct bstree_node *node = (struct bstree_node*)malloc(sizeof(struct bstree_node));



    if (node == NULL) {



        assert(0);



    }



    node->data = key;



    node->bst.left = node->bst.right = NULL;



 



    return node;



}



 



int bstree_insert(struct bstree *T, int key) {



 



    assert(T != NULL);



 



    if (T->root == NULL) {



        T->root = bstree_create_node(key);



        return 0;



    }



 



    struct bstree_node *node = T->root;



    struct bstree_node *tmp = T->root;



 



    while (node != NULL) {



        tmp = node;



        if (key < node->data) {



            node = node->bst.left;



        } else {



            node = node->bst.right;



        }



    }



 



    if (key < tmp->data) {



        tmp->bst.left = bstree_create_node(key);



    } else {



        tmp->bst.right = bstree_create_node(key);



    }



    



    return 0;



}



 



int bstree_traversal(struct bstree_node *node) {



    if (node == NULL) return 0;



    



    bstree_traversal(node->bst.left);



    printf("%4d ", node->data);



    bstree_traversal(node->bst.right);



}



 



#define ARRAY_LENGTH        20



int main() {



 



    int keyArray[ARRAY_LENGTH] = {24,25,13,35,23, 26,67,47,38,98, 20,13,17,49,12, 21,9,18,14,15};



 



    struct bstree T = {0};



    int i = 0;



    for (i = 0;i < ARRAY_LENGTH;i ++) {



        bstree_insert(&T, keyArray[i]);



    }



 



    bstree_traversal(T.root);



 



    printf("\n");



}



 



 



#endif



 
```

业务与数据结构混合在一起,同样的这套树能不能在不同的业务,比如进程的各种状态,就是这棵树的左右子节点不一样
1.如何创建
void* value ps:没有问题,还是需要再次寻址找内存,往往是这个value不知道是什么的时候写它,不管你定义什么把值直接往里面放,考虑到进程可有不同状态树就按这个技巧可以落在不同的树上结果,宏有美观作用;
malloc 红黑树内存分配如何组织起来,看到这个malloc应该就能想到在虚拟内存堆上面分配一块内存把它加入操作系统的内核,红黑树就是在这个基础上的一个变种;

## 3.红黑树的定义



![在这里插入图片描述](https://img-blog.csdnimg.cn/df25b6b2795f4fee884a71e8df24e10c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



定义怎么来的?可以有数学归纳法去证明
与二叉树这里还有不同之处是定义多一个nil,所有的叶子节点都指向这个nil



![在这里插入图片描述](https://img-blog.csdnimg.cn/7757e58053bc4a189f535de7316ed07c.png)



```cpp
#include <stdio.h>



#include <stdlib.h>



#include <string.h>



 



#define RED                1



#define BLACK             2



 



typedef int KEY_TYPE;



 



typedef struct _rbtree_node {



    unsigned char color;



    struct _rbtree_node *right;



    struct _rbtree_node *left;



    struct _rbtree_node *parent;



    KEY_TYPE key;



    void *value;



} rbtree_node;



 



typedef struct _rbtree {



    rbtree_node *root;



    rbtree_node *nil;



} rbtree;



 



rbtree_node *rbtree_mini(rbtree *T, rbtree_node *x) {



    while (x->left != T->nil) {



        x = x->left;



    }



    return x;



}



 



rbtree_node *rbtree_maxi(rbtree *T, rbtree_node *x) {



    while (x->right != T->nil) {



        x = x->right;



    }



    return x;



}



 



rbtree_node *rbtree_successor(rbtree *T, rbtree_node *x) {



    rbtree_node *y = x->parent;



 



    if (x->right != T->nil) {



        return rbtree_mini(T, x->right);



    }



 



    while ((y != T->nil) && (x == y->right)) {



        x = y;



        y = y->parent;



    }



    return y;



}



 



 



void rbtree_left_rotate(rbtree *T, rbtree_node *x) {



 



    rbtree_node *y = x->right;  // x  --> y  ,  y --> x,   right --> left,  left --> right



 



    x->right = y->left; //1 1



    if (y->left != T->nil) { //1 2



        y->left->parent = x;



    }



 



    y->parent = x->parent; //1 3



    if (x->parent == T->nil) { //1 4



        T->root = y;



    } else if (x == x->parent->left) {



        x->parent->left = y;



    } else {



        x->parent->right = y;



    }



 



    y->left = x; //1 5



    x->parent = y; //1 6



}



 



 



void rbtree_right_rotate(rbtree *T, rbtree_node *y) {



 



    rbtree_node *x = y->left;



 



    y->left = x->right;



    if (x->right != T->nil) {



        x->right->parent = y;



    }



 



    x->parent = y->parent;



    if (y->parent == T->nil) {



        T->root = x;



    } else if (y == y->parent->right) {



        y->parent->right = x;



    } else {



        y->parent->left = x;



    }



 



    x->right = y;



    y->parent = x;



}



 



void rbtree_insert_fixup(rbtree *T, rbtree_node *z) {



 



    while (z->parent->color == RED) { //z ---> RED



        if (z->parent == z->parent->parent->left) {



            rbtree_node *y = z->parent->parent->right;



            if (y->color == RED) {



                z->parent->color = BLACK;



                y->color = BLACK;



                z->parent->parent->color = RED;



 



                z = z->parent->parent; //z --> RED



            } else {



 



                if (z == z->parent->right) {



                    z = z->parent;



                    rbtree_left_rotate(T, z);



                }



 



                z->parent->color = BLACK;



                z->parent->parent->color = RED;



                rbtree_right_rotate(T, z->parent->parent);



            }



        }else {



            rbtree_node *y = z->parent->parent->left;



            if (y->color == RED) {



                z->parent->color = BLACK;



                y->color = BLACK;



                z->parent->parent->color = RED;



 



                z = z->parent->parent; //z --> RED



            } else {



                if (z == z->parent->left) {



                    z = z->parent;



                    rbtree_right_rotate(T, z);



                }



 



                z->parent->color = BLACK;



                z->parent->parent->color = RED;



                rbtree_left_rotate(T, z->parent->parent);



            }



        }



        



    }



 



    T->root->color = BLACK;



}



 



 



void rbtree_insert(rbtree *T, rbtree_node *z) {



 



    rbtree_node *y = T->nil;



    rbtree_node *x = T->root;



 



    while (x != T->nil) {



        y = x;



        if (z->key < x->key) {



            x = x->left;



        } else if (z->key > x->key) {



            x = x->right;



        } else { //Exist



            return ;



        }



    }



 



    z->parent = y;



    if (y == T->nil) {



        T->root = z;



    } else if (z->key < y->key) {



        y->left = z;



    } else {



        y->right = z;



    }



 



    z->left = T->nil;



    z->right = T->nil;



    z->color = RED;



 



    rbtree_insert_fixup(T, z);



}



 



void rbtree_delete_fixup(rbtree *T, rbtree_node *x) {



 



    while ((x != T->root) && (x->color == BLACK)) {



        if (x == x->parent->left) {



 



            rbtree_node *w= x->parent->right;



            if (w->color == RED) {



                w->color = BLACK;



                x->parent->color = RED;



 



                rbtree_left_rotate(T, x->parent);



                w = x->parent->right;



            }



 



            if ((w->left->color == BLACK) && (w->right->color == BLACK)) {



                w->color = RED;



                x = x->parent;



            } else {



 



                if (w->right->color == BLACK) {



                    w->left->color = BLACK;



                    w->color = RED;



                    rbtree_right_rotate(T, w);



                    w = x->parent->right;



                }



 



                w->color = x->parent->color;



                x->parent->color = BLACK;



                w->right->color = BLACK;



                rbtree_left_rotate(T, x->parent);



 



                x = T->root;



            }



 



        } else {



 



            rbtree_node *w = x->parent->left;



            if (w->color == RED) {



                w->color = BLACK;



                x->parent->color = RED;



                rbtree_right_rotate(T, x->parent);



                w = x->parent->left;



            }



 



            if ((w->left->color == BLACK) && (w->right->color == BLACK)) {



                w->color = RED;



                x = x->parent;



            } else {



 



                if (w->left->color == BLACK) {



                    w->right->color = BLACK;



                    w->color = RED;



                    rbtree_left_rotate(T, w);



                    w = x->parent->left;



                }



 



                w->color = x->parent->color;



                x->parent->color = BLACK;



                w->left->color = BLACK;



                rbtree_right_rotate(T, x->parent);



 



                x = T->root;



            }



 



        }



    }



 



    x->color = BLACK;



}



 



rbtree_node *rbtree_delete(rbtree *T, rbtree_node *z) {



 



    rbtree_node *y = T->nil;



    rbtree_node *x = T->nil;



 



    if ((z->left == T->nil) || (z->right == T->nil)) {



        y = z;



    } else {



        y = rbtree_successor(T, z);



    }



 



    if (y->left != T->nil) {



        x = y->left;



    } else if (y->right != T->nil) {



        x = y->right;



    }



 



    x->parent = y->parent;



    if (y->parent == T->nil) {



        T->root = x;



    } else if (y == y->parent->left) {



        y->parent->left = x;



    } else {



        y->parent->right = x;



    }



 



    if (y != z) {



        z->key = y->key;



        z->value = y->value;



    }



 



    if (y->color == BLACK) {



        rbtree_delete_fixup(T, x);



    }



 



    return y;



}



 



rbtree_node *rbtree_search(rbtree *T, KEY_TYPE key) {



 



    rbtree_node *node = T->root;



    while (node != T->nil) {



        if (key < node->key) {



            node = node->left;



        } else if (key > node->key) {



            node = node->right;



        } else {



            return node;



        }    



    }



    return T->nil;



}



 



 



void rbtree_traversal(rbtree *T, rbtree_node *node) {



    if (node != T->nil) {



        rbtree_traversal(T, node->left);



        printf("key:%d, color:%d\n", node->key, node->color);



        rbtree_traversal(T, node->right);



    }



}



 



int main() {



 



    int keyArray[20] = {24,25,13,35,23, 26,67,47,38,98, 20,19,17,49,12, 21,9,18,14,15};



 



    rbtree *T = (rbtree *)malloc(sizeof(rbtree));



    if (T == NULL) {



        printf("malloc failed\n");



        return -1;



    }



    



    T->nil = (rbtree_node*)malloc(sizeof(rbtree_node));



    T->nil->color = BLACK;



    T->root = T->nil;



 



    rbtree_node *node = T->nil;



    int i = 0;



    for (i = 0;i < 20;i ++) {



        node = (rbtree_node*)malloc(sizeof(rbtree_node));



        node->key = keyArray[i];



        node->value = NULL;



 



        rbtree_insert(T, node);



        



    }



 



    rbtree_traversal(T, T->root);



    printf("----------------------------------------\n");



 



    for (i = 0;i < 20;i ++) {



 



        rbtree_node *node = rbtree_search(T, keyArray[i]);



        rbtree_node *cur = rbtree_delete(T, node);



        free(cur);



 



        rbtree_traversal(T, T->root);



        printf("----------------------------------------\n");



    }



    



 



    



}



 



 



 



 



 
```

## 4.红黑树节点旋转



![在这里插入图片描述](https://img-blog.csdnimg.cn/38d57bddc0954b7997f885de278ba3b3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


左旋与右旋,一个操作6根指针,成双成对的先x后y,从右下开始直接下层X



## 5.红黑树的添加问题

1.添加之前满足红黑树的性质
它就是一个合法的树,
**插入的这个节点是红色的好还是黑色的好?**
红色好,是因为红色更容易满足性质,
**什么时候需要调整?**
只有性质四**如果一个结点是红的，则它的两个儿子都是黑的**违背
违背当前节点是红色的,同时父节点是红色的,从而出现多种情况?

![在这里插入图片描述](https://img-blog.csdnimg.cn/b8757bce93cb4edfb9131144735273a9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


父结点是祖父结点的左子树的情况



1. 叔结点是红色的

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/da5771d41fd84080b8e4d41be9b67362.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)

2. 叔结点是黑色的，而且当前结点是右孩子

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/ede2f74bf1954ca780f6d6d3d487b51a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)

3. 叔结点是黑色的，而且当前结点是左孩子

   ![在这里插入图片描述](https://img-blog.csdnimg.cn/fdaa6963d4ea4e54b7db0b1b98512b31.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)

```cpp
void rbtree_insert(rbtree *T, rbtree_node *z) {



 



    rbtree_node *y = T->nil;



    rbtree_node *x = T->root;



 



    while (x != T->nil) {



        y = x;



        if (z->key < x->key) {



            x = x->left;



        } else if (z->key > x->key) {



            x = x->right;



        } else { //Exist



            return ;



        }



    }



 



    z->parent = y;



    if (y == T->nil) {



        T->root = z;



    } else if (z->key < y->key) {



        y->left = z;



    } else {



        y->right = z;



    }



 



    z->left = T->nil;



    z->right = T->nil;



    z->color = RED;



 



    rbtree_insert_fixup(T, z);



}



 



void rbtree_delete_fixup(rbtree *T, rbtree_node *x) {



 



    while ((x != T->root) && (x->color == BLACK)) {



        if (x == x->parent->left) {



 



            rbtree_node *w= x->parent->right;



            if (w->color == RED) {



                w->color = BLACK;



                x->parent->color = RED;



 



                rbtree_left_rotate(T, x->parent);



                w = x->parent->right;



            }



 



            if ((w->left->color == BLACK) && (w->right->color == BLACK)) {



                w->color = RED;



                x = x->parent;



            } else {



 



                if (w->right->color == BLACK) {



                    w->left->color = BLACK;



                    w->color = RED;



                    rbtree_right_rotate(T, w);



                    w = x->parent->right;



                }



 



                w->color = x->parent->color;



                x->parent->color = BLACK;



                w->right->color = BLACK;



                rbtree_left_rotate(T, x->parent);



 



                x = T->root;



            }



 



        } else {



 



            rbtree_node *w = x->parent->left;



            if (w->color == RED) {



                w->color = BLACK;



                x->parent->color = RED;



                rbtree_right_rotate(T, x->parent);



                w = x->parent->left;



            }



 



            if ((w->left->color == BLACK) && (w->right->color == BLACK)) {



                w->color = RED;



                x = x->parent;



            } else {



 



                if (w->left->color == BLACK) {



                    w->right->color = BLACK;



                    w->color = RED;



                    rbtree_left_rotate(T, w);



                    w = x->parent->left;



                }



 



                w->color = x->parent->color;



                x->parent->color = BLACK;



                w->left->color = BLACK;



                rbtree_right_rotate(T, x->parent);



 



                x = T->root;



            }



 



        }



    }



 



    x->color = BLACK;



}
```

if 可以?还是while 肯定得是while直到root的根节点始终扣红色,层层迭代
z=z->parent->parent是关键,z一直是红色

原文作者：[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/604954753Linux

# 【NO.535】服务器开发,无锁消息队列实现(初步认识)

## 1.多种锁效率对比



![在这里插入图片描述](https://img-blog.csdnimg.cn/76463554390143c4958019abe5b00fb1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)


mutex
blockqueue:condition+mutex
lockfreequeue:指针方式的无锁队列
arraylockfreeque:数组方式实现无锁队列
ypipe:是接下来的额重点
无锁队列应用在元素非常多，每秒处理几十万的数据，几百个数据的话就是杀鸡用牛刀，没有任何帮助。为什么几百个数据不合适？因为几百个数据经常会造成condition_wait处于阻塞的状态，所以建议数据量要足够的大。



## 2.ypipe的特性：

- 一写一读，不支持多读多写，多读多写会消耗一定的内容。

- 链表方式去分配节点，采用chunk机制，减少节点的分配时间。chunk就是一次分配多个节点。

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/f44336eb2e294b9883d671723e48c671.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)

  个人理解Darren老师讲解的意思是使用chunk一次分配多个节点，可以节省分配资源的耗时。从20-30-20数据量有这个变动的过程中，chunk元素也会有所波动。为了提上性能，中间30会回到20个元素的状态，不过多出来的10给元素位置并不会立刻释放，而是保留最新的数据，等到时机成熟便可以轻松使用。

  ## 批量写入

  批量写入在kafka，tcp都有所应用，通过多次写入统一调用flush函数读端才能显示，目的是提高吞吐量。

## 3.无锁队列原理

如果使用condition+mutex这种方式，如何知道读端是在休眠我正好去唤醒？以前也从来没有考虑过这个问题，不可能发一个消息就去notify，效率会大打折扣。notify这个东西导致用户态和内核态相互切换，肯定是会影响效率的。

### 3.1 当读端没有数据，这时候应该怎么办？

- sleep睡一两毫秒算是一个办法，不过这样会导致吞吐量上不去。只能用condition_wait+mutx 这钟方式。

  #### 写端如何去唤醒数据呢？

  condition_wait+mutex这种方式了。



![在这里插入图片描述](https://img-blog.csdnimg.cn/5b16795d2e104438a0fd214c6bf5c71f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



- 大部分情况下，end_chunk和back_chunk指向的都是相同的一个chunk。

- back_pos表示当前chunk要插入的位置，而end_pos表示结束的位置。

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/2fcdf8ce0cf54fdc9662763b8414005c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/bfbef2f5b3164deb994b1438475c20a1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)

  需要注意的是，使用无锁队列也要解决阻塞和唤醒的问题，如果性能差别不大那使用无锁队列就值得商榷了，性能提示10%都没必要考虑使用无锁队列。

  ## 4.总结

  源码剖析这部分笔者的理解确实还不到位，所以不想妄加评论，留在下一期继续更新。通过Darren老师本节课程的讲解，我只是初步认识到了无锁队列的概念，大概明白了原理。更重要的是，认识到了无锁队列的重要性，如果性能提升不大就可以不优先考虑无锁队列。如果项目上有需要，最优的策略也是先完成工作，应付好任务，到项目性能需要提升时，再仔细的研究无锁队列，毕竟无锁队列想要商业上应用，还是一件比较困难的事。确实是有必要学习上一期的无锁队列，一天能把无锁队列研究清楚就已经是一件令人开心的事了！

  原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

  原文链接：https://bbs.csdn.net/topics/604954011

# 【NO.536】Linux系统中的文件操作

## 1.文件的作用

```javascript
 linux中，一切皆文件（网络设备除外）



硬件设备也“是”文件，通过文件来使用设备



目录（文件夹）也是一种文件
```

## 2.Linux的文件结构



![在这里插入图片描述](https://img-blog.csdnimg.cn/58bb261d0ad14997a8c19264dd03a5d0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_17,color_FFFFFF,t_70,g_se,x_16)


root：该目录为系统管理员(也称作超级管理员)的用户主目录。



bin：bin是Binary的缩写, 这个目录存放着最经常使用的命令。

boot：这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件。

dev：dev是Device(设备)的缩写, 该目录下存放的是Linux的外部设备，在Linux中访问设备的方式和访问文件的方式是相同的。

etc：所有的配置文件, 所有的系统管理所需要的配置文件和子目录都存放在这里。

home：用户的主目录，在Linux中，每个用户都有一个自己的目录，一般该目录名是以用 户的账号命名的。

var：存放着在不断变化的文件数据，我们习惯将那些经常被修改的目录放在这个目录下。 包括各种日志文件。

lib：这个目录里存放着系统最基本的动态连接共享库，其作用类似于Windows里的DLL文 件。几乎所有的应用程序都需要用到这些共享库。

usr：系统用户工具和程序
-- bin：用户命令
-- sbin：超级用户使用的比较高级的管理程序和系统守护程序。
-- include：标准头文件
-- lib：库文件
-- src：内核源代码

tmp：用来存放一些临时文件。

media：linux 系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别 的设备挂载到这个目录下。

mnt：临时挂载其他文件。
proc: 包含了进程的相关信息。

## 3.文件操作方式

1. 文件描述符 fd

   ```javascript
     是一个>=0的整数
   
   
   
     每打开一个文件，就创建一个文件描述符，通过文件描述符来操作文件
   
   
   
     
   
   
   
     预定义的文件描述符：
   
   
   
     0：标准输入，对应于已打开的标准输入设备（键盘）
   
   
   
     1：标准输出，对应于已打开的标准输出设备（控制台）
   
   
   
     2 : 标准错误， 对应于已打开的标准错误输出设备（控制台）
   
   
   
     
   
   
   
     多次打开同一个文件，可得到多个不同的文件描述符。
   ```

1） 使用底层文件操作（系统调用）
比如：read
可使用man 2 查看
2） 使用I/O库函数
比如：fread
可使用man 3 查看

## 4.底层文件操作(关于文件的系统调用）

> 1. write
>
>    ```javascript
>      (1) 用法
>    ```
>
> 
>
>            man 2 write
>
> 
>
>      (2) 返回值
>
> 
>
>            成功：返回实际写入的字节数
>
> 
>
>            失败：返回 -1， 错误编号设置 errno  可用( strerror(errno) ) 查看
>
> 
>
>       注意：是从文件的当前指针位置写入！
>
> 
>
>                文件刚打开时，文件的位置指针指向文件头
>
>    ```
> 
>    ```

```cpp
// main1.c



#include <errno.h>



#include <string.h>



 



int main(void)



{



    int len = 0;



 



    char buff[] = "hello world\n";



 



    len = write(1, buff, sizeof(buff));



    if (len < 0)



    {



        printf("write to stdout failed. reason: %s\n", strerror(errno));



    }



    else



    {



        printf("write %d bytes.\n", len);



    }



 



    len = write(2, buff, sizeof(buff));



 



    if (len < 0)



    {



        printf("write to stderr failed. reason: %s\n", strerror(errno));



    }



 



    return 0;



}
```

> 1. read
>
>    ```javascript
>        (1)用法
>    ```
>
> 
>
>             man 2 read
>
> 
>
>        (2)返回值
>
> 
>
>              大于0 : 实际读取的字节数
>
> 
>
>              0  : 已读到文件尾
>
> 
>
>              -1 ：出错
>
> 
>
>        注意：参数3表示最多能接受的字节数，而不是指一定要输入的字节数
>
> 
>
>        实例：main2.c
>
> 
>
>        运行：# ./a.out          /* 用户输入回车符时结束输入 */
>
> 
>
>              # ./a.out < main.c      /* 利用重定向， 使用文件main.c作为输入 */
>
>    ```
> 
>    ```

```cpp
// main2.c



#include <stdio.h>



#include <stdlib.h>



 



int main(void)



{



 



    char buffer[1024];



    int cnt = 0;



 



    cnt = read(0, buffer, sizeof(buffer));



 



    write(1, buffer, cnt);



 



    return 0;



}
```

> 1. open
>
>    ```javascript
>        (1) 用法
>    ```
>
> 
>
>             main 2 open
>
> 
>
>        (2) 返回值
>
> 
>
>              成功：文件描述符
>
> 
>
>              失败：-1
>
> 
>
>        (3) 打开方式
>
> 
>
>              O_RDONLY        只读
>
> 
>
>              O_WRONLY        只写
>
> 
>
>              O_RDWR           读写
>
> 
>
>                O_CREAT           如果文件不存在，则创建该文件，并使用第3个 
>
>    ```
>    参数设置权限,如果文件存在 ，则只打开文件
> 
>    ```javascript
>              O_EXCL            如果同时使用O_CREAT而且该文件又已经存在
>    ```
>
>    时，则返回错误, 用途：以防止多个进程同时创建
>
>    同一个文件
>
>    ```javascript
>              O_APPEND         尾部追加方式（打开后，文件指针指向文件的末尾）
> 
> 
> 
>              O_TRUNC          若文件存在，则长度被截为0，属性不变
> 
> 
> 
>     example:  open("/dev/hello", O_RDONLY|O_CREAT|O_EXCL, 0777)              
> 
> 
> 
>        (4) 参数3 (设置权限）
> 
> 
> 
>              当使用O_CREAT时，使用参数3             
> 
> 
> 
>              S_I(R/W/X)(USR/GRP/OTH)
> 
> 
> 
>              例：
> 
> 
> 
>                S_IRUSR | S_IWUSR    文件的所有者对该文件可读可写
> 
> 
> 
>                (八进制表示法)0600     文件的所有者对该文件可读可写    
> 
> 
> 
>        注意：
> 
> 
> 
>              返回的文件描述符是该进程未打开的最小的文件描述符
>    ```

```cpp
// open_demo.c



#include <stdlib.h>



#include <sys/types.h>



#include <sys/stat.h>



#include <fcntl.h>



#include <errno.h>



#include <string.h>



 



#define FILE_RW_LEN 1024



 



int main(void)



{



    int fd = 0;



    int count = 0;



    char buffer[FILE_RW_LEN] = "I 'm Martin.";



 



    fd = open("./martin.txt", O_CREAT | O_RDWR | O_APPEND | O_TRUNC, S_IRWXU | S_IRGRP | S _IXGRP | S_IROTH);



    // fd = open("./martin.txt", O_CREAT|O_EXCL|O_RDWR, S_IRWXU|S_IRGRP|S_IXGRP|S                                 _IROTH);



 



    if (fd < 0)



    {



        printf("open file martin.txt failed. reason: %s\n", strerror(errno));



        exit(-1);



    }



 



    count = write(fd, buffer, strlen(buffer));



 



    printf("written: %d bytes.\n", count);



 



    close(fd);



}
```

> 1. close
>
>    ```javascript
>       (1) 用法
>    ```
>
> 
>
>             man 2 close                
>
> 
>
>             终止指定文件描述符与对应文件之间的关联，
>
> 
>
>             并释放该文件描述符，即该文件描述符可被重新使用              
>
> 
>
>       (2）返回值
>
> 
>
>            成功： 0
>
> 
>
>            失败： -1              
>
> 
>
>       实例：
>
> 
>
>          使用read/write实现文件复制 
>
> 
>
>          close_demo.c 
>
>    ```
> 
>    ```

```cpp
// close_demo.c



#include <stdlib.h>



#include <stdio.h>



 



#define FILE1_NAME "file1.txt"



#define FILE2_NAME "file2.txt"



 



int main(void)



{



 



    int file1, file2;



    char buffer[4096];



    int len = 0;



 



    file1 = open(FILE1_NAME, O_RDONLY);



    if (file1 < 0)



    {



        printf("open file %s failed\n, reason: %s\n", FILE1_NAME, strerror(errno));



        exit(-1);



    }



 



    file2 = open(FILE2_NAME, O_CREAT | O_WRONLY, S_IRUSR | S_IWUSR);



 



    if (file2 < 0)



    {



        printf("open file %s failed\n, reason: %s\n", FILE2_NAME, strerror(errno));



        exit(-1);



    }



 



    while ((len = read(file1, buffer, sizeof(buffer))) > 0)



    {



        write(file2, buffer, len);



    }



 



    close(file2);



    close(file1);



    return 0;



}
```

观察耗时
./a.out
time ./a.out

```javascript
         补充：time命令



          time命令分别输出:



              real - 程序总的执行时间、



              usr - 该程序本身所消耗的时间、



              sys - 系统调用所消耗的时间
```

> 1. lseek
>
>    ```javascript
>      (1) 用法
>    ```
>
> 
>
>           man 2 lseek
>
> 
>
>      (2) 返回值
>
> 
>
>           成功：返回新的文件位置与文件头之间偏移
>
> 
>
>           失败： -1
>
> 
>
>      实例：从文件偏移量100的位置拷贝100个字节到另一个文件
>
> 
>
>      lseek_demo.c
>
>    ```
> 
>    ```

```cpp
// lseek_demo.c



#include <sys/types.h>



#include <sys/stat.h>



#include <fcntl.h>



#include <errno.h>



#include <stdlib.h>



#include <stdio.h>



#include <string.h>



 



#define FILE1_NAME "lseek_demo.c"



#define FILE2_NAME "lseek_demo_2.c"



 



#define SIZE 100



 



int main(void)



{



    int file1, file2;



    char buffer[1024];



    int ret;



 



    file1 = open(FILE1_NAME, O_RDONLY);



    if (file1 < 0)



    {



        printf("open file %s failed\n", FILE1_NAME);



        exit(-1);



    }



 



    file2 = open(FILE2_NAME, O_WRONLY | O_CREAT, S_IRUSR | S_IWUSR);



    if (file2 < 0)



    {



        printf("open file %s failed\n", FILE2_NAME);



        exit(-1);



    }



 



    // file size



    ret = lseek(file1, 0, SEEK_END);



    printf("file size: %d\n", ret);



 



    ret = lseek(file1, 100, SEEK_SET);



    printf("lseek ret: %d\n", ret);



 



    ret = read(file1, buffer, SIZE);



    if (ret > 0)



    {



        buffer[ret] = '\0';



        printf("read[%d]: %s\n", ret, buffer);



        write(file2, buffer, SIZE);



    }



 



    close(file1);



    close(file2);



    return 0;



}
```

> 1. ioctl
>
>    ```javascript
>      ioctl是设备驱动程序中对设备的I/O通道进行管理的函数。所谓对I/O通道进行管理，就是对设备的一些特性进行控制，例如串口的传输波特率、马达的转速等等。是设备驱动程序中设备控制接口函数,用来控制设备.
>    ```
>
>    函数名: ioctl
>
>    功 能: 控制I/O设备
>
>    用 法: int ioctl(int fd, int cmd,[int *argdx, int argcx]);
>
>    参数：fd是用户程序打开设备时使用open函数返回的文件标示符，cmd是用户程序对设备的控制命令，后面是一些补充参数，一般最多一个，这个参数的有无和cmd的意义相关。

## 5.系统调用

### 5.1 标准I/O库

直接使用系统调用的缺点
(1) 影响系统性能
系统调用比普通函数调用开销大
因为，频繁的系统调用要进行用户空间和内核空间的切换
(2) 系统调用一次所能读写的数据量大小，受硬件的限制

```javascript
       解决方案: 使用带缓冲功能的标准I/O库（以减少系统调用次数）



       



/* C语言中的文件操作中已描述 */



1) fwrite



2) fread



3) fopen



4) fclose



5) fseek    



6) fflush
```

## 6.proc文件系统

/proc是一个特殊的文件系统，
该目录下文件用来表示与启动、内核相关的特殊信息

```javascript
1) /proc/cpuinfo



   CPU详细信息



   



2) /proc/meminfo



   内存相关信息



 



3) /proc/version



   版本信息



 



4) /proc/sys/fs/file-max



   系统中能同时打开的文件总数



   可修改该文件



 



5) 进程的相关信息



   /proc/32689/ 表示指定进程（进程号为32689)的相关信息



   



6) /proc/devices



    已分配的字符设备、块设备的设备号
```

## 7.文件锁

1. 并发对文件I/O操作的影响

   ```javascript
    解决办法？
   ```

   2)文件锁

   ```javascript
    用法：man 2 fcntl
   
   
   
    
   
   
   
    头文件：#include <unistd.h>
   
   
   
            #include <fcntl.h>
   
   
   
    
   
   
   
   函数定义：int fcntl(int fd, int cmd, ... /* arg */ );
   
   
   
       参数： cmd 取值  F_GETLK,   F_SETLK 和 F_SETLKW ,分别表示获取锁、设置锁和同步设置锁.
   
   
   
    
   
   
   
    文件锁的表示
   
   
   
          struct flock
   ```

> // struct flock 结构体说明
> struct flock {
> short l_type; /*F_RDLCK, F_WRLCK, or F_UNLCK */
> off_t l_start; /*offset in bytes, relative to l_whence */
> short l_whence; /*SEEK_SET, SEEK_CUR, or SEEK_END */
> off_t l_len; /*length, in bytes; 0 means lock to EOF */
> pid_t l_pid; /*returned with F_GETLK */
> };
> l_type: 第一个成员是加锁的类型：只读锁，读写锁，或是解锁。
> l_start和l_whence: 用来指明加锁部分的开始位置。
> l_len: 是加锁的长度。
> l_pid: 是加锁进程的进程id。
> 举例：
> 我们现在需要把一个文件的前三个字节加读锁，则该结构体的l_type=F_RDLCK, l_start=0,
> l_whence=SEEK_SET, l_len=3, l_pid不需要指定，然后调用fcntl函数时，
> cmd参数使F_SETLK.

```cpp
//



#include <unistd.h>



#include <fcntl.h>



#include <stdio.h>



#include <stdlib.h>



 



#define FILE_NAME "test.txt"



 



int flock_set(int fd, int type)



{



    printf("pid=%d into...\n", getpid());



 



    struct flock flock;



    flock.l_type = type;



    flock.l_whence = SEEK_SET;



    flock.l_start = 0;



    flock.l_len = 0;



    flock.l_pid = -1;



 



    fcntl(fd, F_GETLK, &flock);



 



    if (flock.l_type != F_UNLCK)



    {



        if (flock.l_type == F_RDLCK)



        {



            printf("flock has been set to read lock by %d\n", flock.l_pid);



        }



        else if (flock.l_type == F_WRLCK)



        {



            printf("flock has been set to write lock by %d\n", flock.l_pid);



        }



    }



 



    flock.l_type = type;



    if (fcntl(fd, F_SETLKW, &flock) < 0)



    {



        printf("set lock failed!\n");



        return -1;



    }



 



    switch (flock.l_type)



    {



    case F_RDLCK:



        printf("read lock is set by %d\n", getpid());



        break;



    case F_WRLCK:



        printf("write lock is set by %d\n", getpid());



        break;



    case F_UNLCK:



        printf("lock is released by %d\n", getpid());



        break;



    default:



        break;



    }



 



    printf("pid=%d out.\n", getpid());



    return 0;



}



 



int main(void)



{



    int fd;



 



    fd = open(FILE_NAME, O_RDWR | O_CREAT, 0666);



    if (fd < 0)



    {



        printf("open file %s failed!\n", FILE_NAME);



    }



 



    // flock_set(fd, F_WRLCK);



    flock_set(fd, F_RDLCK);



    getchar();



    flock_set(fd, F_UNLCK);



    getchar();



 



    close(fd);



    return 0;



}
```

原文作者：[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605725262

# 【NO.537】Linux服务器开发,异步请求池框架实现，协程前传

## 1.前言

服务器发送完请求不用等待对端回数据，而是用另一个线程进行等待接收收据。
\#
1、发送请教，1个io还是多个io 是多个io
2、接收结果的线程，如何拿到fd
解决方法是每次发送出的fd，放到epoll中，接收线程有io可读就读。

## 2.King式四元组

1、init

- epoll_create
- pthread_create
  2、commit
- 创建socket
- 连接server
- 准备协议encode
- send
- 加入epoll
  3、thread_callback
- while(){
  recv();
  parser();
  fd->epoll_delete
  }
  4、destory
- close(fd)
- phthred_cancel

服务器向第三方服务提供请求，拿redis举例：set，get等命令，每一个命令都有一个回调函数处理请教，通过epoll_ctl()增加节点，epoll_wait()函数去等待检测的数据。只是传入一个指针，不会影响红黑树的大小。

c和c++最大的特点就是内存泄漏。
发送信号提交commit后，让出协程call_back，检测是否有数据到达。

线程创建确实会失败，程序运行时间较短确实不容易捕捉这一现象，但是长时间就不一定了，所以要判断返回值。

## 3.DNS 工作原理举例



![在这里插入图片描述](https://img-blog.csdnimg.cn/6c28b303ff584d9aa5dadd8758cadc91.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



## 4.补充

内存池最不好理解，需要重点去关注的方面：

- 内存块的组织
- 内存分配
- 内存回收

## 5.总结

通过今天的异步请求池框架的实现学习，我对异步请求池的实现思路已经是非常清晰了。异步请求池被King老师成为协程的前传，和后面要学的skynet以及协程的调度有着紧密的关系，后续的课程始终在一种懵懂的状态，看完这节课有一种如梦方醒的感觉，这个感觉美妙极了。
最后，感谢一路上帮助小生的朋友们，感谢零声学院的King老师~

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/604990131

# 【NO.538】Linux服务器开发,原子操作CAS与锁实现

## 1.互斥锁 mutex

锁这个东西之所以会存在是因为资源不能被两个以上的线程同时使用，锁是对临界资源的一种保护。大家知道，”idx++”其中的“++”这个操作，转成汇编为三个指令：

- Mov[idx],%eax(将idx从内存中转到寄存器);
- Inc %eax(通过寄存器进行自增);
- Mov %eax,[idx](从寄存器中读出)。
  在不同线程间切换，先前线程会将idx的值保留至本线程内的寄存器，线程切换后即便值已经发生改变却无法做到更新同步，最终导致竞争资源得到的结果小于理想值。
  \```c
  \#include<stdio.h>
  \#include<unistd.h>
  \#include<pthread.h>

pthread_mutex_t mutex;
//加锁
pthread_mutex_lock(&mutex);
(*pcount) ++;
pthread_mutex_unlock(&mutex);

//锁的初始化
pthread_mutex_init(&mutex,NULL);

~~~javascript
##  二、自旋锁 spinlock



自旋锁的用法和互斥锁是十分相似的。



 



```c



#include<stdio.h>



#include<unistd.h>



#include<pthread.h>



 



pthread_spin_t spinlock;



//加锁



pthread_spin_lock(&spinlock);



    (*pcount) ++;



pthread_spin_unlock(&spinlock);



 



//锁的初始化，共享锁



pthread_spinlock_init(&spinlock,PTHREAD_PROCESS_SHARED);
~~~

虽然使用起来看样子非常相似，但是还是有着细微的差别：

- spinlock在等待时，不会有线程和cpu的切换，mutex会出现线程的切换。

- 临界资源操作简单的，没有发生系统调用，没有耗时操作可以优先选择spinlock。

- 原子<spinlock<mutex。

  ## 三、原子操作

  //xaddl 的意思是将value=value+add；锁住内存总线

```c
int inc(int *value,int add)



{



    int old;



    __asm__ volatile (



        : "lock; xaddl %2 ,%1;“



        :"=a" (old)



        :"m" (*value),"a" (add)



        :"cc"."memory"



    );



    return old;



}



//调用



inc(pcont,1);
```



![在这里插入图片描述](https://img-blog.csdnimg.cn/63783b8335ef44e1ab0b020cb72adb45.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_11,color_FFFFFF,t_70,g_se,x_16)


哈哈，最后同样要是一百万的结果！
原子操作的本质需要cpu指令集的操作支持，如果想把链表用原子锁锁住那是不行的，因为这个动作要需要四个动作。
原子操作知识cpu支持的指令集。当联想到CAS（compare and swap）时，要注意的时，cas只是原子操作的一种。



## 2.线程私有空间 pthread_key

```c
/***************************************************************************************/



#define THREAD_COUNT 4



pthread_key_t key;



pthread_key_create(&key,NULL);



typedef void *（thread_cb)*(void *);



/***************************************************************************************/



void print_thread1_key(void)



{



    int *p=(int *)pthread_getspecific(key);



    printf("thread 1:%d\n",*p);



}



void *thread1_proc(void * arg){



        int i=5;



        pthread_setspecific(key,&i);



        print_thread1_key();



}



/***************************************************************************************/



void print_thread2_key(void)



{



    (char *)pthread_getspecific(key);



    printf("thread 2:%d\n",*p);



}



void *thread2_proc(void * arg){



        cha *p=”thread2_proc“;



        pthread_setspecific(key,p);



        print_thread2_key();



}



/***************************************************************************************/



struct pair



{



    int x;



    int y;



}



void print_thread3_key(void)



{



    istruct pair *p=(int *)pthread_getspecific(key);



    printf("thread 3:x=%d y=%d\n",*p);



}



void *thread3_proc(void * arg){



        struct pair p={1,2};



        pthread_setspecific(key,&p);



        print_thread3_key();



}



/***************************************************************************************/



int main(){



thread_cb callback[THREAD_COUNT ]={



                        thread1_proc,



                        thread2_proc,



                        thread3_proc        



                                  };



for(int i{};i<THREAD_COUNT ;i++)



{



        pthread_create(&thid[i],NULL,callback[i],&count);



}



for(int i{};i<THREAD_COUNT ;i++)



{



        pthread_join(thid[i],NULL);



}



    return 0;



}
```

## 3.共享内存

## 4.cpu亲缘性



![在这里插入图片描述](https://img-blog.csdnimg.cn/827091a423d846f2be6ac237c1851a28.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



```c
//让cpu进行粘合的作用。



#include<stdio.h>



#include<pthread.h>



#iclude<setjmp.h>



#include<sched.h>



#include<sys/syscall.h>



#include<stdlib.h>



#include<string.h>



 



#define __USE_GNU



#include<unistd.h>



void process_affinity(int num)



{



    //gttid();和下一行代码同意



    pid_t selfid=syscall(_NR_gettid);



    cpu_set_t mask;



    CPU_ZERO(&MASK);



    CPU_SET(selfid %num,&mask);



    sched_setaffiniity(selfid,sizeof(mask),&mask);



    while(1)



    {}



}



int main()



{



    int num=sysconf(_SC_NPROCESSORS_CONF);



    int i=0;



    pid_t pid=0;



    for(i=0;i<num;i++)



    {



        pid=fork();



            if(pid<=(pid_t)0)



            {



                    break;



            }



    }



    if(pid==0)



    {



        process_affinity(num);



    }



    while(1)



        usleep(1);



}
```

## 5.setjmp/longjmp

```c
#include<stdio.h>



#include<unistd.h>



#include<pthread.h>



#iclude<setjmp.h>



//跨函数的跳跃



#define Try count=setjmp(env);if(count)==0



#define Catch(type) else if(count==type)



#define Throw(type) longjmp(env,type)



#define Finally



 



struct ExceptionFrame{



    jmp_buf env;



    int count=0;



    struct ExceptionFrame *next;



};



void sub_func(int idx)



{



    //longjmp(env,idx);



    Throw(idx);



}



 



int main()



{



#if 0



    count=setjmp(env);



    if(0==count)



    {



        sub_func(++count);



    }else if(1==count)



    {



        sub_func(++count);



    }else if(2==count)



    {



        sub_func(++count);



    }else



    {



        printf("other item");



    }



    



#else



    Try{



    sub_func(++count);



    }catch(1){



    sub_func(++count);



    }catch(2){



    sub_func(++count);



    }catch(3){



    sub_func(++count);



    }Finally{



        printf("other item\n");



    }    



}
```

## 6.try/catch 嵌套怎么解决？

用链表将ExceptionFrame串起来。

## 7.线程安全怎么解决？

线程的私有空间解决这个问题。

## 8.线上参考版本待完成

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/604939496

# 【NO.539】Linux服务器开发,线程池原理与实现

## 0.前言

## 1.线程池究竟解决了什么问题？

- 可能第一感觉是线程池减少线程创建销毁，但是这是站在线程的角度去思考的。
- 异步解耦的作用 主循环只做一个抛任务，写日志交给线程池，提高了程序运行效率。

**注重cpu的处理能力的时候才会去粘合，注重的任务的话还是不要粘合为好。**

## 2.API

- create、init
- push_task 返回的结果对于业务来说作用不大
- destroy、deinit
  锦上添花的api：task_count,free_thread。

## 3.线程池的作用

- 线程池是管理任务队列和工作队列的管理组件。

## 4.工作原理

- 线程池并非内存池那般，从池子里面取用完归还，而是线程之间争夺资源。

- 通过线程池的入口函数去获取任务，循环往复，再取任务再循环。

- 生产者消费者模型：任务队列是生产者，工作队列是消费者，加锁获取资源。

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/720ae11ba22c4e8182e488396a6d1f32.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

## 5.手写线程池

- 可以对任务进行区分，加优先级，更优秀的做法可以创建多个线程池对象。
- 条件等待，进来时候解锁，出来加锁。
- 当往任务队列中添加任务，应该先加锁锁住资源，中间发送条件变量的信号。

```c
#define LL_ADD(item, list) do {     \



    item->prev = NULL;                \



    item->next = list;                \



    if(list !< NULL) list *prev =item  \



    list = item;                    \



} while(0)
```

- 利用宏向链表中添加数据,这个感觉奇妙啊~

  ```c
  while (1) {
  
  
  
        pthread_mutex_lock(&worker->workqueue->jobs_mtx);
  
  
  
        while (worker->workqueue->waiting_jobs == NULL) {
  
  
  
            if (worker->terminate) break;
  
  
  
            pthread_cond_wait(&worker->workqueue->jobs_cond, &worker->workqueue->jobs_mtx);
  
  
  
        }
  
  
  
        if (worker->terminate) {
  
  
  
            pthread_mutex_unlock(&worker->workqueue->jobs_mtx);
  
  
  
            break;}
  
  
  
        nJob *job = worker->workqueue->waiting_jobs;
  
  
  
        if (job != NULL) {
  
  
  
            LL_REMOVE(job, worker->workqueue->waiting_jobs);}
  
  
  
        pthread_mutex_unlock(&worker->workqueue->jobs_mtx);
  
  
  
        if (job == NULL) continue;
  
  
  
        job->job_function(job);
  
  
  
    }
  ```

- 条件变量这块，是先解锁再加锁，King老师反复强调。不停的判断job为空，需要注意一下，担心万一被别的线程占用了怎么办。
  pthread_mutex_lock(&workqueue->jobs_mtx);

```c
workqueue->workers = NULL;



workqueue->waiting_jobs = NULL;



pthread_cond_broadcast(&workqueue->jobs_cond);



pthread_mutex_unlock(&workqueue->jobs_mtx);
```

- 通知等待着的线程，不折不扣的生产者，不过发送信号也需要加锁。防止竞争。

## 6.对比nginx线程池

我们的线程池为双链表，nginx的队列为什么使用二级指针。因为一级指针能指向struct ngx_thread_task_s的第一项ngx_thread_task_t *next的位置，这也正是ngx的巧妙之处。

## 7.总结

通过今天的学习，不仅学习到了有关线程池的知识，更重要的King老师讲述了学习的三个过程：1、逻辑想通；2、代码调通；3、对照学习。目测小生只是刚刚能做好第一步，接下来还是要付出更多的努力才能将这些知识驾驭娴熟。
这节课确实是比较轻松的，不过需要注意的是King老师在添加删除链表时使用的宏方法，这个也不是第一次见到了，希望能掌握好成为自己的东西。线程之间的条件等待也绝不是第一次听说，wait使得所有线程都爬在此处，等待任务队列中有数据，随时触发线程之前抢占资源。最后就是不停的判断job是否为空，因为担心被其他线程抢走。
在未来的时间里，我会利用C++11的std::thread函数从新改造这个线程池，希望达到异曲同工之妙。

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：hhttps://bbs.csdn.net/topics/604976777

# 【NO.540】Linux服务器开发,应用层协议设计ProtoBuf/Thrift

## 0.前言

协议对于前后端通讯是非常重要的，作为开发新手往往不能理解和掌握协议的设计，导致增加了很多不必要的工作。

## 1.为什么需要协议设计

让发送端和接收端的通道能理解之间发送的数据。

## 2.消息帧的完整性判断

四种判断消息的完整性

- 固定长度
- 特别标记/r/n
- 固定消息头+消息体结构
- 序列化后的buffer前面增加一个字符流的头部，其中有一个字段村村消息总长度。

## 3.协议设计范例



![在这里插入图片描述](https://img-blog.csdnimg.cn/f85bc2f79e8a4c59b1ec54b2e33bcc15.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



## 4.jason/xml/protobuf不同序列化对比

- xm本地配置，ui配置qt，Androidl
- json，websocket http协议，注册账号，web里面登录
- protobuf，业务内部使用，各个服务之间rpc调用，即时通讯项目，游戏项目，带宽占用少的。

IDL(Interface description language)接口描述语言：将文件通过工具生成.c文件。
完整的protobuf库支持C++反射。

项目.模块.proto



![在这里插入图片描述](https://img-blog.csdnimg.cn/08a22ca87cdf492082acf5c88a78f4c7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



## 5.protobuf工程实践和原理

base128 Bariants 表示值 ： 每个字节的最高位不能直接用来表示我们的value，它是用来判断自己是否结束。0代表解释，1表示没有结束。小端形式base128。
Zigzag：针对负数进行优化，内部将int32类型负数转化为uint64来处理。都是整数用int32，有负数用sint32。

## 6.总结

Darren老师建议把person对象进行手写一下，最起码要把protobuf文件进行修改，我觉得也是非常有必要的。从目前自己对知识掌握的程度上来看，二次学习是必须的，既然已经花了大力气就应该把知识完全掌握，那就既来之则安之吧！

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/605060882

# 【NO.541】Linux服务器开发,stl容器，智能指针，正则表达式(C++STL中的智能指针)

## 0.前言

C++标准库中有四种只能指针，即std::auto_ptr、std::unique_ptr、td::shared_ptr、std::weak_ptr。每一个都有适用的场合。智能指针的作用事帮助程序员管理动态分配的对象的生命周期，更有效的防止内存的泄露。
这四种智能指针中，std::auto_ptr是98年遗留的产物，其他三种是C++11标准推出的。std::auto_ptr已经完全被std::unique_ptr取代，所以这里也不在讨论。
有了智能指针，程序员的妈妈再也不必担心内存释放影响程序员的心情了，即便忘记了delete，系统也能够帮助程序员delete，交给智能指针大家都放心了。

## 1.独占式指针 std::unique_ptr

同一个时间内只有一个指针能够指向该对象，当然，该对象的拥有权也能移交出去。独占式智能指针应该是最优先考虑的，可以理解为专属所有权的概念。换句话说，同一时刻只有一个指针指向这个对象，**指针在对象在，指针亡对象亡。**

![在这里插入图片描述](https://img-blog.csdnimg.cn/fa7914b475f9497dad146f14cf0e676f.png)



```cpp
#include<iostream>



#include<string>



#include<memory>



class Test



{



public:



    Test() {}



    ~Test() {}//自己有析构函数



};



auto Function_10()



{



    return std::unique_ptr<std::string>(new std::string("I love Youzi"));



}



std::unique_ptr<std::string> Function_11()



{



    return std::unique_ptr< std::string>(new std::string("I love Youzi"));



}



void Function_12(std::string * arg)



{



    delete arg;



    arg = nullptr;



}



using fp = void(*)(std::string *);



typedef decltype(Function_12)* fp1;



int main()



{



    //初步认识



    std::unique_ptr<int> p1;



    if (nullptr == p1)



        std::cout << "p_test为空指针" << std::endl;



 



    std::unique_ptr<int> p2(new int(10));



 



    std::unique_ptr<int> p3 = std::make_unique<int>(10);



    auto p4 = std::make_unique<int>(10);



 



    //常用操作



    //1、不允许赋值、赋值等动作，只能移动不能复制的类型。



    std::unique_ptr<std::string> pa1(new std::string("I Love Ovoice"));



    //std::unique_ptr<std::string> pa2(pa1); error



    //std::unique_ptr<std::string> pa3=pa1;  error



    std::unique_ptr<std::string> pa4; 



    //pa4 = pa1; error



 



    //2、移动语义写法



    std::unique_ptr<std::string> pb1(new std::string("I Love Ovoice"));



    std::unique_ptr<std::string> pb2 = std::move(pb1);



 



    //3、放弃控制权 release()



    //放弃控制权后，返回裸指针，智能指针置空。返回的裸指针可以手工delete，也可以初始化新的，也可以去赋值。



    std::unique_ptr<std::string> pc1(new std::string("I Love Ovoice"));



    std::unique_ptr<std::string> pc2(pc1.release());



 



    //4、reset()成员函数



    //reset()不带参数时，释放对象，指针置空；有参数时，释放对象，指针指向新内存。



    std::unique_ptr<std::string> pd1(new std::string("I Love Ovoice"));



    pd1.reset();



    if (nullptr == pd1)



        std::cout << "pd1 == nullptr" << std::endl;



 



    std::unique_ptr<std::string> pd2(new std::string("I Love Ovoice"));



    std::unique_ptr<std::string> pd3(new std::string("I Love Ovoice"));



    pd3.reset(pd2.release());



    pd3.reset(new std::string("I Love Ovoice"));



 



    //5、释放智能指针 =nullptr



    std::unique_ptr<std::string> pe1(new std::string("I Love Ovoice"));



    pe1 = nullptr;                //高级啊,和指针一样的。



 



    //6、关于数组



    std::unique_ptr<int[]> pf1(new int[10]);



    pf1[0] = 11;



    pf1[1] = 22;



    pf1[2] = 33;



    



    //有析构函数的类类型，需要自己写删除器进行释放



    auto mydel = [](Test *p) {



        delete []p;



    };



    std::unique_ptr<Test[],decltype(mydel)> pf2(new Test[10], mydel);



    



    //7、获取裸指针 get()



    std::unique_ptr<std::string> pg1(new std::string("I Love Ovoice"));



    std::string * pg2 = pg1.get();



    const char* pg3 = pg2->c_str();



    *pg2 = "Ovoice is a good school!";



    const char* pg4 = pg2->c_str();//调试发现pg3和pg4内存地址不同，由string内部决定



 



    //8、*解引用



    std::unique_ptr<std::string> ph1(new std::string("I Love Ovoice"));



    const char* ph2 = ph1->c_str();



    *ph1 = "Ovoice is a good school!";



    const char* ph3 = ph1->c_str();



    //数组并没有解引用，注意别丢人啦



    std::unique_ptr<int[]> ph4(new int[10]);



    //*ph4; error



 



    //9、交换指向对象 swap()



    std::unique_ptr<std::string> pi1(new std::string("I Love Ovoice"));



    std::unique_ptr<std::string> pi2(new std::string("I Love Youzi"));



    std::swap(pi1,pi2); //个人推荐这种~



    pi1.swap(pi2);



 



    //10、转成std::share_ptr类型



    //share_ptr有一个右值构造函数可以直接将unique_ptr放入



    std::shared_ptr < std::string> pj1 = Function_10();



 



    std::unique_ptr<std::string> pj2(new std::string("I Love Youzi"));



    std::shared_ptr<std::string> pj3 = std::move(pj2);//pj2为空，pj3时share_ptr引用计数为1



 



    //11、被复制的例外



    std::unique_ptr < std::string> pk1;



    pk1 = Function_11();



    



    //12、删除器



    std::unique_ptr<std::string,fp> pl1(new std::string("I Love Youzi"),Function_12);



    std::unique_ptr<std::string, fp1> pl2(new std::string("I Love Youzi"), Function_12);



    std::unique_ptr<std::string, decltype(Function_12)*> pl3(new std::string("I Love Youzi"), Function_12);



    



    auto mydel = [](std::string* arg)



    {



        delete arg;



        arg = nullptr;



    };



    std::unique_ptr<std::string, decltype(mydel)> pl4(new std::string("I Love Youzi"), mydel);



    



    return 0;



}
```

## 2.强引用共享式指针 std::share_ptr

多个指针指向同一个对象，最后一个指针被销毁，引用计数为0，这个对象就会被销毁。



![img](https://img-community.csdnimg.cn/images/053432fa294542e59b524c72656368ab.png)



```cpp
#include<iostream>



#include<string>



#include<memory>



#include "CShare_ptr.h"



void Function_9(int *p)



{



    std::cout << __FUNCTION__ << "现在删除器被调用" << std::endl;



    delete p;



}



template<typename T>



void Function_10(T* p)



{



    std::cout << __FUNCTION__ << "现在删除器被调用" << std::endl;



    delete []p;



}



template<typename T>



std::shared_ptr<T> make_shared_array(size_t size)



{



    return std::shared_ptr<T>(new T[size],Function_10<T[]>);



}



 



auto lambda1 = [](int* p)



{



    std::cout << __FUNCTION__ << "现在删除器被调用" << std::endl;



    delete p;



};



auto lambda2 = [](int* p)



{



    std::cout << __FUNCTION__ << "现在删除器被调用" << std::endl;



    delete p;



};



 



int main()



{



    int i{};



    std::shared_ptr<int> p=std::make_shared<int>(10);



    //1、引用计数 use_count()



    std::cout << "****** 1、引用计数 use_count()" << std::endl;



    std::shared_ptr<int> pa1(new int (100));



    std::cout << ++i<<" pa1.use_count()=" << pa1.use_count() << std::endl;



    std::shared_ptr<int> pa2(pa1);



    std::cout << ++i<<" pa1.use_count()=" << pa1.use_count() << std::endl;



    std::shared_ptr<int> pa3=pa1;



    std::cout << ++i << " pa1.use_count()=" << pa1.use_count() << std::endl;



    std::cout << ++i << " pa3.use_count()=" << pa3.use_count() << std::endl;



 



    //2、是否独占 unique()



    std::cout << "****** 2、是否独占 unique()" << std::endl;



    std::shared_ptr<int> pb1(new int(100));



    if (pb1.unique())



    {



        std::cout << ++i<< " pb1 unique 返回真" << std::endl;



    }



    else



    {



        std::cout << ++i << "pb1 unique 返回假" << std::endl;



    }



 



    //3、指针重置 reset()



    std::cout << "****** 3、指针重置 reset()" << std::endl;



    //引用计数先减1，如果为0指向的对象就要清空。



    std::shared_ptr<int> pc1(new int(100));



    pc1.reset();



    if (nullptr == pc1)



    {



        std::cout <<++i<< " pc1被置空" << std::endl;



    }



    std::shared_ptr<int> pc2(new int(100));



    //释放原内存，指向新内存



    pc2.reset(new int(100));



    



    std::shared_ptr<int> pc3;



    //利用reset初始化空指针



    pc3.reset(new int(10));



    //4、*解引用



    std::cout << "******4、*解引用" << std::endl;



    std::shared_ptr<int> pd1(new int(100));



    char buf[1024];



    sprintf_s(buf,sizeof(buf),"%d",*pd1);



    //5、获取裸指针 get()



    std::cout << "******5、获取裸指针 get()" << std::endl;



    //如果智能指针释放了对象，裸指针也将失效！这给函数的目的是为没有用智能指针的代码提供可能



    std::shared_ptr<int> pe1(new int(100));



    int* pe2 = pe1.get();



    *pe2 = 200;



    //6、交换指针 swap()



    std::cout << "******6、交换指针 swap()" << std::endl;



    std::shared_ptr<int> pf1(new int(100));



    std::shared_ptr<int> pf2(new int(100));



    std::swap(pf1,pf2);



    pf1.swap(pf2);



    //7、赋空 =nullptr



    std::cout << "7、******赋空 =nullptr" << std::endl;



    std::shared_ptr<std::string> pg1(new std::string("I love Mark!"));



    pg1 = nullptr;



 



    //8、删除器和数组



    std::cout << "8、******8、删除器和数组" << std::endl;



    std::shared_ptr<int> ph1(new int(100),Function_9);



    std::shared_ptr<int> ph2(ph1);



    ph1.reset();



    ph2.reset();//现在删除器会被调用



 



    //std::shared_ptr<int> ph3 = make_shared_array<int>(10); 也可以选用std::default_delete<T[]>()



    



    std::shared_ptr<int> ph4(new int(20),lambda1);



    std::shared_ptr<int> ph5(new int(20),lambda2);



    ph5 = ph4;



}
```

## 3.弱引用共享式指针std::weak_ptr

是std::share_ptr 做辅助工作的。

![在这里插入图片描述](https://img-blog.csdnimg.cn/9a0bd558223d4d1ab0d01be734e51538.png)



```cpp
#include "CWeak_ptr.h"



#include<iostream>



#include<string>



#include<memory>



int main()



{



    int i{};



    //1、引用计数 use_count()



    std::cout << "****** 1、引用计数 use_count()" << std::endl;



    auto pa1 = std::make_shared<int>(10);



    auto pa2(pa1);



    std::weak_ptr<int> pa3(pa1);



    std::cout << ++i << " pa3.use_count()=" << pa3.use_count() << std::endl;



 



    //2.过期 xpired()



    pa1.reset();



    pa2.reset();



    if (pa3.expired())



    {



        std::cout << ++i << " pa3.expired()为真" << std::endl;



    }



 



    //3、重置 reset()



    auto pb1 = std::make_shared<int>(10);



    std::weak_ptr<int> pb2(pb1);



    pb2.reset();//pb1为强引用，弱引用已经被重置



 



 



    //4、是否锁住 lock()



    std::cout << "****** 4、是否锁住 lock()" << std::endl;



    auto pc1 = std::make_shared<int>(20);



    std::weak_ptr<int> pc2;



    pc2 = pc1;



    if (!pc2.expired())



    {



        auto pc3 = pc2.lock();



        if (pc3 != nullptr)



        {



            std::cout << "pc3 != nullptr" << "所指对象不存在" << std::endl;



        }



    }



    else



    {



        std::cout << "pc2已经过期" << std::endl;



    }



    return 0;



}
```

## 4.总结

智能指针主要目的是不光是帮助程序员用来吹牛，主要是帮助释放内存，防止内存泄露。对于严谨的程序员内存泄露是低级错误，对于新手却是家常便饭。这里只是简单的浅谈了三种智能指针日常使用的方法，日常生活中还要注意慎用裸指针、enabl_shared_from_this、循环引用等问题，避免踩坑。笔者作为一个C++初学者，还是希望在项目中多多使用智能指针，年轻人毕竟上手能力比较强嘛！
最后，还是要感谢零声学院的Darren老师，在《Linux服务器开发：stl容器，智能指针，正则表达式(C++STL中的智能指针)》这门课中着重的介绍了智能指针，让我对C++11有了新的认识！正所谓“前人播种，后人乘凉。”希望老师们继续努力，砥砺前行。

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/604955515

# 【NO.542】协程的设计原理与汇编实现

**第一个问题，为什么会协程？以及协程到底解决了什么问题？**

第一个同步的方式实现异步的效率
**同步为什么效率低，而异步的为什么效率高？**
等待没错，他需要等待，那这个等待是什么意思？
第一个案例之前跟大家讲的100万并发的案例
第二个案例异步请求的案例

![在这里插入图片描述](https://img-blog.csdnimg.cn/c7510c55a62244e9903d72b54be90333.png)



现象区别：
if 1 用了 workqueue，每1000个耗时 1400
对fd直接进行写，每秒接入量5600

**那现在这个工作队列他为什么在这个过程引入工作队列，他就能够提升性能呢？**，

他是从哪些方面原因那这个要跟他分析这个要跟大家分析，
直接进行读写

![在这里插入图片描述](https://img-blog.csdnimg.cn/e8109b97b4614da18a5bb6fd1695479f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_8,color_FFFFFF,t_70,g_se,x_16)


工作队列

![在这里插入图片描述](https://img-blog.csdnimg.cn/76dd3e5e62cc44b2b6e198d4f30e9c8d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_8,color_FFFFFF,t_70,g_se,x_16)


原理类似

![在这里插入图片描述](https://img-blog.csdnimg.cn/597eef35327143c1be6bfdabbd376efa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_17,color_FFFFFF,t_70,g_se,x_16)


同步：检测io与读写io在同一个流程
异步：检测io与读写io不在一个流程



同一个流程没有切换的概念

**异步性能高，同步有什么好处？**
逻辑清晰，效率不高

**那有没有一种方式采用同步的编程方式还同时拥有异步的性能？**

那我们如何把它改进成一个同步的编程方式
**那实现的原理是什么呢？**

![在这里插入图片描述](https://img-blog.csdnimg.cn/c8a502c265d0400781abdce81ed0c025.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



king式4元组异步连接池。
有没有一种方式只管发sql过去，不去等待他的结果，这样就造成异步做法

能不能把MySQL请求改成异步的，包括mongodb,redis,rpc都是可以改的。
这里是DNS请求
异步的：

![在这里插入图片描述](https://img-blog.csdnimg.cn/af2fa6a7f994421093b99cfca6b9ec6a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_13,color_FFFFFF,t_70,g_se,x_16)


同步的

![在这里插入图片描述](https://img-blog.csdnimg.cn/273114928dbc4f028449f2ceb2cb75f5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_11,color_FFFFFF,t_70,g_se,x_16)


异步结果是一起方法的，是有方法保证顺序的。



**核心：同步的编程方式，要有异步的性能
那么是怎么做的呢？**
两个关键的地方：
1.commit
2.callback

![在这里插入图片描述](https://img-blog.csdnimg.cn/a6082a8f0e984e78a2210fa8a09848e0.png)





![在这里插入图片描述](https://img-blog.csdnimg.cn/5c6124c607ea4567be8fe55094b0a11d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)



**异步提交完了之后我们怎么改成同步的方式？**
**这一点就是协程是如何实现的？**



![在这里插入图片描述](https://img-blog.csdnimg.cn/22caf11f269d4f55aa0dab4dfb7cf9f5.png)



对应的callback地方就是检测结果的返回有没有数据。

![在这里插入图片描述](https://img-blog.csdnimg.cn/b274494fbfcb4df8b4101531629bea82.png)



确确实实不在一个流程里面，callback是申请一个子线程做的

![在这里插入图片描述](https://img-blog.csdnimg.cn/df15ed9045f543b8ab203d173aa402b1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)



![在这里插入图片描述](https://img-blog.csdnimg.cn/0495ea4bff7246c7aebac74458132a0a.png)


我们怎么改成一个流程里面：我们提交完就可以接受数据

![在这里插入图片描述](https://img-blog.csdnimg.cn/27875b8c8de74f86bbcc36395c0d0689.png)



看上去他们不在一个流程里面但是我们要改起来让他们在一个流程里面并且还没有副作用。
**那怎么改？**

在fd加入到epoll里面我们在应用层引入一个yield();
**注意这个让出，让出完了之后到达哪呢？**

![在这里插入图片描述](https://img-blog.csdnimg.cn/93bc12491c4f4d48a403e0b4b02d46a4.png)



**让出完了之后到达read这个地方，解析完了之后会有一个resume恢复;**

![在这里插入图片描述](https://img-blog.csdnimg.cn/cc14e8cdcd924aacb230a13e955119c7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_9,color_FFFFFF,t_70,g_se,x_16)





![在这里插入图片描述](https://img-blog.csdnimg.cn/2662938d5ebb4a3c863a51f9078ffbcf.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_9,color_FFFFFF,t_70,g_se,x_16)



给我们感觉它没有切换的，对应的引入了两个原语操作
一个是跟大家讲的yield()，另外一个就是resume()。
**让出是让出对应的流程在resume返回对应的流程
这神来之笔就是协程的实现**

**那么我们该如何实现yield()以及resume()？**

![在这里插入图片描述](https://img-blog.csdnimg.cn/da34d059d37c403a9bc4755034a00d47.png)



1.yield()让出时使用longjmp跳转到read fd；resume()恢复时在使用setjmp跳回来
2.linux下面的ucontext
3.或者汇编自己跳转

yeild() 让出这里有俩个步骤，一个是longjmp() 标签一; setjmp() 标签二回来
让出到哪呢？

![在这里插入图片描述](https://img-blog.csdnimg.cn/cdc4653ec8c342d6beed48d2285f9698.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)


在讲yield/resume()之前有没有发现他们有共同的动作：是俩个上下文环境的切换，从a往b跳，从b往a跳；

![在这里插入图片描述](https://img-blog.csdnimg.cn/2798f3c83bbe40028c1e15893f52d62e.png)


看似是汇编其实代码可读性比longjmp/setjmp还有ucontext都要强很多



把读io，io没有准备就绪的这个等待取消掉，性能会无线的接近异步方式



![在这里插入图片描述](https://img-blog.csdnimg.cn/a52de782b2414859b0e6ee0ed5b6d48b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_9,color_FFFFFF,t_70,g_se,x_16)



协程究竟解决什么问题？
注意同步与异步的效率？

reactor和协程之间区别？

![在这里插入图片描述](https://img-blog.csdnimg.cn/92685f900ae14333986eea01639a0887.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_14,color_FFFFFF,t_70,g_se,x_16)


接收完数据，解释完数据，把数据放到sendbuff里面，再把对应的fd设置epollout
recv和send是在俩个函数本质上它还是同步；
coroutin协程是在一个函数，他们性能会差不多，只是协程会跟符合人思考方式

![在这里插入图片描述](https://img-blog.csdnimg.cn/0608ff2452cf4298828ae8a8a4ea225e.png)


栈是函数调用必须要用到的东西，没有栈的话里面就根本没办法做函数调用，不推荐共享栈协程，往往共享栈协程的管理会更加复杂，
怎么确定栈大小？是自己设定的，如果你要做文件传输的话可以设置成1M或者10M，如果你只是单纯接收一些字符数据你可以开到1K或者4K；



**协程的切换到底怎么实现？**



![在这里插入图片描述](https://img-blog.csdnimg.cn/390309ee5bc4421b8b32c7e34648a82e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


**这个让出动作怎么做呢？**就是把CPU里面的寄存器组的值保存到协程A里面，**这个A怎么去保存CPU寄存器的值呢？**



## 1.协程切换

以X86为例，32位的，那这个寄存器的值也是32位的，它里面是有具体的值的，所以我们可以定义一个结构体去对应的保存值

![在这里插入图片描述](https://img-blog.csdnimg.cn/9269c751467e403b9fa0a995d2f86507.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_11,color_FFFFFF,t_70,g_se,x_16)


保存完了之后在把B加载到寄存器这样就实现了A和B的切换；

![在这里插入图片描述](https://img-blog.csdnimg.cn/83c52fba5c494af68bbd46e041e55947.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)


为什么后面是定义r1,r2,r3,r4,r5呢？为了32位与64位兼容





![在这里插入图片描述](https://img-blog.csdnimg.cn/2aac8210824943cc8fa035b560ec0124.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_19,color_FFFFFF,t_70,g_se,x_16)


前面这一段是保存的意思

![在这里插入图片描述](https://img-blog.csdnimg.cn/9a6013616331431db2dcdd753adb9e9b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_16,color_FFFFFF,t_70,g_se,x_16)



将rsi保存到rdi里面

![在这里插入图片描述](https://img-blog.csdnimg.cn/dbd142563e87423189b0eef985278a68.png)


这里偏移0就是结构体第一个值，就是说的每一个大小是8个字节，64位里面一个指针对应的是8个字节；

![在这里插入图片描述](https://img-blog.csdnimg.cn/d459ddbf6ea54906ae2b0938a4f1a578.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


movq的过程

![在这里插入图片描述](https://img-blog.csdnimg.cn/9da071522eae41e0acb4ba9d910c53f1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



下面这一段就是加载的过程，最后ret返回

![在这里插入图片描述](https://img-blog.csdnimg.cn/2990cb9075cf4db6a5b9137ce6b98b36.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)


PS：

![在这里插入图片描述](https://img-blog.csdnimg.cn/fff06a0ea4a4416dbc1a12b76be92fde.png)



![在这里插入图片描述](https://img-blog.csdnimg.cn/85e538d51de241b1ad0717dc74f5d2da.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


有一些值存的是flag还有一些存的是临时变量的值我们可以不用保存；



## 2.yield 和resume的过程



![在这里插入图片描述](https://img-blog.csdnimg.cn/8b0baed72ddb44acb693f10fc0105efd.png)



![在这里插入图片描述](https://img-blog.csdnimg.cn/28274b73660a47cc8370a110258a1a25.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_16,color_FFFFFF,t_70,g_se,x_16)


可以注意到都依赖于_switch这个函数



## 3.协程的启动

协程的启动有如下三个问题：
思考一下线程是怎么启动的，启动是什么意思？



![在这里插入图片描述](https://img-blog.csdnimg.cn/652d6b9f08994e789a86951495e4fcd9.png)



**现在有一个问题入口函数的这个函数把它加到哪里？**
刚刚比较好理解就是A和B是已经运行的比较好切换，如果说现在B是一个NEW的状态，**那么此时他的寄存器里面保存什么值的，一加入进来怎么保持运行呢？**
**就是第三点这个协程运行把它推到CPU上面这个指令怎么做？**

![在这里插入图片描述](https://img-blog.csdnimg.cn/93dfe58f39bf4923bf70d60f9f05ec22.png)



第一个把eip指针指向入口函数fun，也就是把fun的地址指向eip，然后再对应的把它的参数保存下来
还有一个东西就是关于这个栈指针这个栈esp，等下会有函数调用这个栈指针的首地址也要保存起来，

![在这里插入图片描述](https://img-blog.csdnimg.cn/27635eadd6884223b27d0e2c89d65252.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


是为_exec压栈准备好的，所以要esp栈顶指针减去那个4，就是参数



把入口函数的地址换了

![在这里插入图片描述](https://img-blog.csdnimg.cn/68c0641117c341799db3e3d644d9a0e1.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_16,color_FFFFFF,t_70,g_se,x_16)


栈是从最底部开始指的：

![在这里插入图片描述](https://img-blog.csdnimg.cn/d92a127d464747318a0458469fe76224.png)


核心的就是俩个指针：esp还有eip



## 4.协程到底是怎么调度的

在说明调度之前先理清一下协程是怎么定义的？

![在这里插入图片描述](https://img-blog.csdnimg.cn/4982b73ebf344ccaa10f046a30a18828.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_8,color_FFFFFF,t_70,g_se,x_16)



假如有十个协程我们使用什么数据结构来存储？

看一看协程源码：有红黑树还有就绪队列去存储集合

![在这里插入图片描述](https://img-blog.csdnimg.cn/4c2af5ec63f340f084f8b50225037401.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_11,color_FFFFFF,t_70,g_se,x_16)



![在这里插入图片描述](https://img-blog.csdnimg.cn/4b1352e722404d9fbf5b6c36a213e280.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_14,color_FFFFFF,t_70,g_se,x_16)


这些集合之间的关系是怎样的？





![在这里插入图片描述](https://img-blog.csdnimg.cn/789bc5f2e13d40a3971852fcfef9af19.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)


这些数据结构组织在一起，它是由多种数据结构共用的这些节点，6，7，8这几个是同时拥有的的至少在其中一个；



## 5.调度器又如何实现的

假如这里有50个协程，那么这50个coroutine我们如何组织起来，并且使用何种顺序去调度呢？
1curr_coroutine当前栈是哪个？
2.//如果是共享栈就可以放调度器里面不放协程定义2里面，
3.调度器的调度方法的具体实现可以拆出来，内核里面有cfs还有rt



![在这里插入图片描述](https://img-blog.csdnimg.cn/7fdb0123644b4e5eb2f5a6361cec1c28.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_19,color_FFFFFF,t_70,g_se,x_16)


协程调度里这三个集合是怎么组织起来的呢？



**fd如何知道是否就绪？**
在调用send还有read之前，我们将fd加入epoll管理，在引起切换yeild()，在epoll_wait()

![在这里插入图片描述](https://img-blog.csdnimg.cn/c42329b4ec9547aaaf97406de8504093.png)

原文作者：[[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605053587

# 【NO.543】redis计数，布隆过滤器，hyperloglog

# 1.redis计数



![img](https://img-community.csdnimg.cn/images/92ed170fa1b045df9424d0f3b041be73.png)



# 2.布隆过滤器

## 2.1 redis扩展

redis通过对外提供一套API和一些数据结构，可以供开发者开发自己的模块并加载到redis中。

### 2.1.1 本质

在不侵入redis源码的基础上，提供一种高效的扩展数据结构的方式。

### 2.1.2 API及数据结构

参考redismodule.h

## 2.2 RedisBloom

RedisBloom是redis的一个扩展，我们主要使用了它的布隆过滤器。

关于布隆过滤器的原理，参考[《hash，bloomfilter，分布式一致性hash》](https://blog.csdn.net/congchp/article/details/122882760)

### 2.2.1 加载到redis中的方法

```shell
git clone https://github.com/RedisBloom/RedisBloom.git



cd RedisBloom



git submodule update --init --recursive



make



cp redisbloom.so /path/to



 



# 命令方式加载



redis-server --loadmodule /path/to/redisbloom.so



 



#配置文件加载



vi redis.conf



# loadmodules /path/to/redisbloom.so
```

### 2.2.2 命令

redis计数中的使用方法如下：

```shell
# 为 bfkey 所对应的布隆过滤器分配内存，参数是误差率以及预期元素数量，根据运算得出需要多少hash函数



以及所需位图大小



bf.reserve bfkey 0.0000001 10000



# 检测 bfkey 所对应的布隆过滤器是否包含 userid



bf.exists bfkey userid



# 往 key 所对应的布隆过滤器中添加 userid 字段



bf.add bfkey userid
```

# 3.hyperloglog

布隆过滤器提供了一种节省内存的概率型数据结构，它不保存具体数据，存在误差。

hyperloglog也是一种概率型数据结构，相对于布隆过滤器，使用的内存更少。

为什么叫hyperloglog？

是因为空间复杂度非常低，是$O(log_2log_2n)$。

redis提供了hyperloglog。在redis实现中，每个hyperloglog只是用**固定的12kb**进行计数，使用16384($2^{14}$)个桶子，标准误差为`0.8125%`，可以统计$2^{64}$个元素。



![img](https://img-community.csdnimg.cn/images/013f90d6c04f46058abd4b1e29cce768.png)



## 3.1 本质

使用少量内存统计集合的基数的近似值。存在一定的误差。

HLL的误差率：$\frac{1.04}{\sqrt{m}}$, m是桶子的个数；对于redis，误差率就是`0.8125%`。

## 3.2 原理

HyperLogLog 原理是通过给定 n 个的元素集合，记录集合中数字的比特串第一个1出现位置的最大值k，也可以理解为统计二进制低位连续为零（前导零）的最大个数。通过k值可以估算集合中不重复元素的数量m，m近似等于$2^k$。

但是这种预估方法存在较大误差，为了改善误差情况，HyperLogLog中引入分桶平均的概念，计算 m 个桶的调和平均值。下面公式中的const是一个修正常量。

$DV_HLL = const*m*\frac{m}{\sum_{j=1}^m\frac{1}{2^{R_j}}}$

$\frac{1}{2^{R_j}}$: 每个桶的估算值

$\frac{m}{\sum_{j=1}^m\frac{1}{2^{R_j}}}$：所有桶估值计值的调和平均值。

redis的hyperloglog中有16384($2^{14}$)个桶，每一个桶占6bit；

对于一个要放入集合中的字符串，hash生成64位整数，其中后14位用来索引桶子；前面50位用来统计低位连续为0的最大个数, 最大个数49。6bit对应的是$2^6$对应的整数值64可以存储49。在设置前，要设置进桶的值是否大于桶中的旧值，如果大于才进行设置，否则不进行设置。



![img](https://img-community.csdnimg.cn/images/a3ffcbeccef64e329af797c9b68bb51b.png)



## 3.3 去重

相同元素通过hash函数会生成相同的 64 位整数，它会索引到相同的桶子中，累计0的个数也会相同，按照上面计算最长累计0的规则，它不会改变该桶子的最长累计0；

## 3.4 存储

redis 中的 hyperloglog 存储分为稀疏存储和紧凑存储；

当元素很少的时候，redis采用节约内存的策略，hyperloglog采用稀疏存储方案；当存储大小超过3000 的时候，hyperloglog 会从稀疏存储方案转换为紧凑存储方案；紧凑存储不会转换为稀疏存储，因为hyperloglog数据只会增加不会减少（不存储具体元素，所以没法删除）；

## 3.5 命令

```shell
# 往key对应的hyperloglog对象添加元素



pfadd key userid_1 userid_2 userid_3



# 获取key对应hyperloglog中集合的基数



pfcount key



# 往key对应过的hyperloglog中添加重复元素。已存在，并不会添加



pfadd key userid_1



pfadd key1 userid_2 userid_3 userid_4



# 合并key，key1到key2，会去重



pfmerge key2 key key1
```

原文作者：[congchp](https://blog.csdn.net/congchp)

原文链接：https://bbs.csdn.net/topics/604985776

# 【NO.544】Linux服务器开发,Makefile/cmake/configure

## 1.Makefile语法

目前，互联网主要应用cmake，但是cmake又依赖于Makefile，所以，学些Makefile是有必要的。
Makefile的三要素：

- 目标：是执行的结果的感觉
- 依赖：是执行的条件的感觉
- 命令：是具体执行操作的感觉

### 1.1 工作原理



![在这里插入图片描述](https://img-blog.csdnimg.cn/f0e440fe0ddc43c3aea94872a0c8579b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_16,color_FFFFFF,t_70,g_se,x_16)



#### 1.1.1 范例1：规则展示

```c
all:test



        @echo "hello all"



test:



        @echo "hello test"
```

**值得注意的是：在@echo前，要用tab，千万不能使用空格，否则报错后果自负。**

#### 1.1.2 范例2：伪对象

```c
$(EXE)：$(OBJS)



        $(CC) -o $(EXE) $(OBJS)
.PHONY:main clean



CC=gcc



RM=rm



EXE=simple



OBJS=main.o foo.o



$(EXE)=$(OBJS)



    $(CC) -o $(EXE) $(OBJS)



simple:main foo.o    



    @echo "gcc -o simple main.o foo.o"



    gcc -o simple main.o foo.o



main:main.c



    @echo "gcc -o main.o -c main.c"



    gcc -o main.o -c main.c



foo.o:foo.c



    @echo "gcc -o foo.o -c foo.c"



    fcc -o foo.o -c foo.c



clean:



    rm simple main.o foo.o
```

- 伪对象: .PHONY 后面加上原本有的文件名字，这样就会防止执行make时重名报错。解决提示”make 'clean' is up to date“的问题。
- =左面得符号名字用等号右面进行替换，gcc -o A B要遵循从左至右得原则。

#### 1.1.3 范例3：自动变量

```c
.PHONY:all



all:first second third



        @echo "\$$@ = $@"



        @echo "$$^ =$^"



        @echo "$$< =$<"



first:



        @echo "1 first"



second: 



        @echo "2 second"



third:



        @echo "3 third"
```

- $@ 表示一个规则中的目标。当我们的一个规则中有多个目标时，$@所指的是其中**任何造成命令被运行的目标**。
- $^ 表示规则中的所有先决条件。
- $< 表示规则中的第一个先决条件。
  个人理解$@指的是例子中$(EXE) $(OBJS),$<表示的main.o foo.o，$<表示的是main.c或是foo.c。

#### 1.1.4 范例4：相关函数

```c
。PHONY:clean



CC=gcc



RM=rm



EXE=simple



SRCS=$(wildcard *.c)



OBJS=$(patsubst %.c,%.o,$(SRCS))



$(EXE):$(OBJS)



        $(CC) -o $@ $^



%.o:%.c



        $(CC) -o $@ -c $^



clean:



$(RM) $(EXE) $(OBJS)



src:#调试make src 显示相应的xx.c



        @echo $(SRCS)



objs:#测试make objs显示相应的xx.o



        @echo $(OBJS)        
```

- $(wildcard *.c)表示通配出所有 .c 的文件。
- $(patsubst pattern,replacement,text)表示模式字符串替换函数。

## 2.CMake语法

### 2.1 范例一：规则展示



![在这里插入图片描述](https://img-blog.csdnimg.cn/abd5ddc45842418dab3af212a98c25fe.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



```c
#CMakeLists.txt文件



#单个目录实现



#CMake 最低版本号要求



cmake_minimum_required(version 2.8)



#工程，它不是执行文件名



PROJECT(0VOICE)



#手动加入文件



SET(SRC_LIST main.c)



SET(SRC_LIST2 main2.c)



MESSAGE(STATUS "THIS IS BINARY DIR " ${PROJECT_BINARY_DIR})



MESSAGE(STATUS "THIS IS SOURCE DIR" ${PROJECT_SOURCE_DIR})



 



#生成执行文件名0voice 0voice2



ADD_EXECUTABLE(0voice $(SRC_LIST))



ADD_EXECUTABLE(0voice2 ${SRC_LIST2})
```

### 2.2 范例二：层次结构



![在这里插入图片描述](https://img-blog.csdnimg.cn/0758cac7fe8c4908b51c15651f7785a7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_9,color_FFFFFF,t_70,g_se,x_16)



```c
#CMake 最低版本号要求



cmake_minium_required (VERSION 2.8)



PROJECT(0VOICE)



#添加子目录



ADD_SUBDIRECTORY(src)



#INSTALL(FILES COPYRIGHT README DESTINATION share/doc/cmake/ovoice)



#安装doc到 share/doc/cmake/ovoice 目录



#默认/usr/local/



#指定自定义目录，比如cmake -DCMAKE_INSTALL_PREFIX=/tm[/usr



INSTALL(DIRECTORY doc/ DESTINATION share/doc/cmake/ovoice)
```

- 安装目录到某个路径，终端输入

  cmake -DCMAKE_INSTALL_PREFIX=/tmp.usr ..

  #### 范例三：编译库

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/b24f7e4e90994c83807e674351829e73.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_9,color_FFFFFF,t_70,g_se,x_16)

```c
#单个目录实现



#CMake 最低版本号要求



cmake_mininun_required (VERSION 2.8)



#工程



PROJECT(0VOICE)



#手动加入文件



SET(SRC_LIST main.c)



MESSAGE(STATUS "THIS IS BINARY DIR " ${PROJECT_BINARY_DIR})



MESSAGE(STATUS "THIS IS SOURCE DIR" ${PROJECT_SOURCE_DIR})



 



#添加头文件路径



INCLUDE_DIRECTORIES("${CMAKE_CURRENT_SOURCE_DIR}/dir1")



INCLUDE_DIRECTORIES(dir1)



MESSAGE(STATUS "CMAKE_CURRENT_SOURCE_DIR ->" ${CMAKE_CURRENT_SOURCE_DIR})



#添加dur1子目录



ADD_SUBDIRETORY("${CMAKE_CURRENT_SOURCE_DIR}/dir1")



#添加头文件路径



INCLUDE_DIRECTORIES(${CMAKE_CURRENT_SOURCE_DIR}/dir1)



#添加dir2子目录



ADD_SUBDIRECTORY("${CMAKE_CURRENT_SOURCE_DIR}"/dir2)



ADD_EXECUTABLE(darren ${SRC_LIST})



TARGET_LINK_LIBRARIES(darren dir1 dir2)
#dir1 的Makelist



#加载所有的源码，和makefile wildcard类似AUX_SOURCE_DIRECTORY(. DIR_SRCS)



# SET(DIR_SRCS dir1.c dir12.c)



#默认是静态库



ADD_LIBRARY (dir1 SHARED ${DIR_SRCS})



+ 如果静态库和动态库同时存在，会优先连接动态库。
#引用动态库



ADD_EXECUTABLE(darren ${SRC_LIST})



#TARGET_LINK_LIBRARIES(darren Dir1)



#引用静态库



#TARGET_LINK_LIBRARIES(darren libDira1,1)



#TARGET_LINK_LIBRARIES(darren lDir1)
```

### 2.3 范例四：省去子目录MakeList文件

```c
#单个目录实现



#CMake 最低版本号要求



cmake_mininun_required (VERSION 2.8)



#工程



PROJECT(0VOICE)



#手动加入文件



SET(SRC_LIST main.c)



MESSAGE(STATUS "THIS IS BINARY DIR " ${PROJECT_BINARY_DIR})



MESSAGE(STATUS "THIS IS SOURCE DIR" ${PROJECT_SOURCE_DIR})



#设置子目录



set(SUB_DIR_LIST "${CMAKE_CURRENT_SOURCE_DIR}/dir1" "${CMAKE_CURRENT_SOURCE_DIR}/dir2")



foreach(SUB_DIR ${sSUB_DIR_LIST})



#遍历源文件



aux_source_direatory(${SUB_DIR} SRC_LIST)



MESSAGE(STATUS "SUB_DIR->" ${SUB_DIR})



MESSAGE(STATUS "SUB_DIR->" ${SUB_DIR})



ebdfireach()



#添加头文件路径



INCLUDE_DIRECTORIES("dir1")



INCLUDE_DIRECTORIES("dir2")



ADD_EXECUTABLE(darren ${SRC_LIST})
```

### 2.4 范例五：设置版本

```c
cmake_minimum_requires(VERSION 2.8s)



#设置release版本还是debug版本



if(${CMAKE_BUILD_TYPE} MATCHES "Release")



        MESSAGE(STATUS "Release版本")



        SET(BuildType，”Release“)



else()



        SET(BuildType "Debug")



        MESSAGE(STATUS "Debug版本")



endif()



#设置lib库目录



SET(RELEASE_DIR ${PROJECT_SOURCE_DIR}/release)



#debug和release版本目录不一样



#设置生成的so动态库组后输出的路径



SET(LIBRARY_OUTPUT_PATH ${RELEASE_DIR}/linux/${BuildType})



ADD_COMPILE_OPTIONS(-fPIC)



 



#查找当前目录下所有与源文件并保存到DIR_LIB_SRCS变量



AUX_SOURCE_DIRECTORY(. DIR_LIB_SRCS)        
```

- CMakeList 是可以传递的。

原文作者：[[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/604939855

# 【NO.545】磁盘存储链式的 B 树与 B+树

## 1.B树的介绍

**在讲B树之前我们先讨论一下内存与磁盘有什么区别?**
对于这个问题很多朋友或多或少可以说点出来
可能很多朋友答的第一点就是
1.内存快磁盘慢
2.断电以后数据消失,磁盘持久存储

![在这里插入图片描述](https://img-blog.csdnimg.cn/7dbb7a85fc114a48951ce80de5fd0311.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


在这个简单的过程当中就构建了三级存储,寄存器,内存,磁盘
寄存器是少量的,速度很快,然后就是内存再是磁盘;



这个概念捋清楚的话我们再聊下:我们在访问内存的时候接下来有几条指令要解释一下为什么讲B树需要从这个地方开始讲,不然肯定不清楚为什么B树与B+树只适合使用在磁盘上面,不时候用在内存里面.
mov eax [0008h] 这条指令就是指内存中的一个值放到寄存器里面去,cpu可以通过指令访问内存中的所有值,我们可以通过cpu指令访问内存中的任意位置;如果当内存中间没有数据的时候,没有命中它就会产生一个缺页异常,比如我们访问磁盘,访问一个文件这个文件很大,这时候访问数据,它首先会把文件加入到内存里面,如果在访问的过程中间发现内存里头这块没有,这时候就会引起一个缺页的动作,这个缺页的动作会发送什么呢?会去磁盘 当中进行寻址,寻址的过程当中请注意,我们在这里面看到的大家可能就是3个框,给大家的直观感受就是三个框,很难感受到磁盘比内存为什么会慢?慢在哪里?没有一个直观的感受,内存是我们可以通过cpu的指令汇编代码是可以随意访问的,访问中间任何一个快,任何一个地址都是可以的,都是磁盘是访问不了的.
**磁盘是通过什么访问的?**

每一次访问磁盘的时候,磁盘就相当于那个风扇,它不断的再旋转,转到固定的位置,磁盘是有很多个面组成然后这个过程当中我们怎么从这个面上面拿到数据呢?
它就通过这个磁头顶住这个磁盘,然后寻址找到这个固定的磁道再拿到对应的数据,可能一看有点绕口,可以这样理解,这里就相当于是一个碗橱柜,你家里那个放碗筷的橱柜,一大堆的盘子,在这个盘子中间存储了一些固定数据,磁盘它慢,慢在哪呢?就慢在这个寻址的过程中间,寻址的时候很慢,它转到那个固定的位置,拿到确定的数据它很慢,顶住磁盘磁头,找到磁盘对应的扇区拿到数据.
做操作系统的时候,你会指定扇区,这个地方就是存储了那个boot,操作系统启动的时候也是从这个扇区里面刚刚开始启动拿到数据,然后在开始加载内存中间执行指令,这不是今天这篇文章重点,重点是磁盘慢,慢在寻址.
**这个慢和这个内存访问有什么区别呢?**
接着就可以参考我上篇红黑树的文章,红黑树是怎样的呢?可以理解为二叉树,对二叉树举一个例子,1024个节点大家构想一下1024个节点什么意思,层高有10层,10层是通过什么方式找到的?
先拿到根节点,通过一次寻址?寻址什么意思?**就是这个指针指向下一个位置,就是一次寻址**;1024节点最差的情况需要寻址10次,才能拿到对应我们想要的数据,请注意这种方式是没有问题的.同样的红黑树我们把它拿到磁盘中去运行的时候就会发现出现一个问题,它不是说做不了,大家就结合这个情况,红黑树来做我们把它放到磁盘里面,同样也是需要寻址,同样是1024个节点我们需要进行10次磁盘的寻址,拿这个10次的磁盘寻址什么意思呢?就相当于这个磁盘在磁道上面转十次,就是我们需要找十次,请注意这十次还是在查找的过程中,读一个数据我们需要寻址10次写一个数据也要寻址10次,你会发现访问磁盘的速度会大大的降低
**有没有一种方法减少寻址次数**,**也就是说降低树层高,寻址次数越少越好,就一俩次越快越好**
在这个前景下面就有了我们的多叉树,多叉树是为了在同样的节点下面,比如6叉8叉可能两次寻址就可以搞定,这个多叉树就是降低层高减少寻址速度,那么在多叉树中间就有了一种树叫B树,那么在多叉树中间也有很多种的树,
什么3叉树4 叉树,5,btree也是多叉树,在定义的时候呢没有定义叉树?
叉是根据你有多少个子节点多少分叉是由应用程序,是由我们自己写代码自己去定义的,它就比什么三叉树,4叉树,乃至N叉树多了更多的变动性,**我们在讲这个多叉树的时候很多时候就等同与在讲这个B树**,B树的定义上面就没有定义多少分叉,
**首先解释B树为什么常用解释完B树后再来解释B+树,**
按照这样一个情况比如说每个节点4K一个页为单位寻址操作内存的时候最小单位是4k,
4Gd的存储我们需要多少次寻址
B树开1024叉,只要寻址两次,
多叉树B树是怎么定义的?
**所有的叶子节点都在同一层决定了平衡**

我相信很多朋友都在解释这样一个问题?这个定义怎么来的
**我想问一下是先有了B树还是先有了定义?**
先有了树的定义慢慢去版本迭代约束我们所做的东西

![在这里插入图片描述](https://img-blog.csdnimg.cn/ca9295cce3934ddc9a4a2756f74129d5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



是先有了结果?
**逻辑矛盾点:思考这个定义再去思考这个树,**
**先把树理解了后再去思考定义,一个约定俗成是OK的**
26个字母B树

![在这里插入图片描述](https://img-blog.csdnimg.cn/723b56d4f538402fa877871216fbac7d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)



根节点最少有两个子树这个是成立的,第二个所有叶子节点同一层,
还有这个概念里面跟大家解释一下
**这个阶最大的一个树,也就是这个节点里面最大能存储多少个字母**呢?
5个,最大的这个5,跟我们定义的这个M阶5+1等于M阶,**这个5形容的是有多少key**

**key最多是5,那么它的子树就有6个,M阶代表的就是这个子树里面最大有多少个节点,5个人排队中间有6个空隙**
能够结合代码看出B树;

同样的字母B树的组织方式是N多样的

B树用代码怎么定义?
先定义一个节点出来,这个节点里面包含多少K多少子树然后再包含其它信息比如M阶怎么定义?有多少K值?是否是叶子节点?



![在这里插入图片描述](https://img-blog.csdnimg.cn/aaa463a21e8e46af8c99fdbfee5cb25a.png)



![在这里插入图片描述](https://img-blog.csdnimg.cn/61d7be27c76b4316a22b5207f6f6938c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



一个扇区就是节点,几次寻址找到扇区;
**柔性数组是用在内存里面的,内存已经分配好了我们不需要知道它的大小的时候,就可以直接通过一个标签来访问,**
那么关于B树我们怎么从一个节点变成有n多个节点呢?

总共有多少个单元?
一百万快怎么组织

26个字母是不是只有确定的方式呢?

先分裂再添加

![在这里插入图片描述](https://img-blog.csdnimg.cn/f99852eb6c3840899b4ea1366b24c573.png)


M选偶数k奇数
先讲分裂再创建

![在这里插入图片描述](https://img-blog.csdnimg.cn/0dc99611cdab4d349ff3082c2262cf26.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_17,color_FFFFFF,t_70,g_se,x_16)



如果只有一个节点怎么分裂第i个子节点分裂不出来,它有父节点第几个孩子,

分裂完怎么插入对应位置插入节点分俩步骤:
找到对应位置,把对应节点放进去,
二叉排序树一样,对比节点k找到合适的位置

怎么实现?插的这个节点
随便看一颗B树,这个节点新插的k值最终插入叶子节点上面,不可能插入内节点,
所有节点插都是插到叶子节点上面,那么我们代码怎么实现呢?
比如插入aa比u大,然后分裂
还要引入函数

这个x就是我们要插的未满的节点,不是一个意思,
不是我们我们插的这个函数他就是插的一个为满的节点
x就是相当于这个过程是递归的方法就是a不是叶子节点找到一个然后找插进去,
if(是叶子节点插,不是叶子节点就往下递归
对比的是k值其实找到是它子树位置找到第一个比他小的
它的子树是他最贴近的位置

如果发现子树是满的先进行分裂
找到合适位置然后判断子树是不是满的,如果是满的就先进行分裂
是满的然后进行分裂

和到一起三步骤
分裂完再继续找对应子树是不是满,分裂完再插入进去,分裂完就不要判断了不是满的了

插入一个自己好吧好吧满,会满是再下一次插入节点时再分裂,刚满不分裂,

删除节点,比较复杂
找规律

删除先找的时候正好等于最小的值,遍历到正好等于下限,这时候出现借位动作,

发生了几个步骤:
1.什么时候需要借位,删除的时候
这么几种情况4
1.对比俩颗子树,idx-1,inx+1,如果idx等于最小数量a.借位借位又分两种情况,什么时候向前借位什么时候向后借位,下判断前面前面没有再判断后面
B.合并如果idx
2.如果子树大于,就顺其自然,借不来就合并,
为什么合并要把C合并,

删除的时候同样第一步递归找到对应的子树
迭部进行刚刚的动作找子树的过程就是递归的过程,

删除的这个节点删除的这个值也会是在叶子节点上面

## 2.B树的组成



![在这里插入图片描述](https://img-blog.csdnimg.cn/de1ed1f7e45a482fb545917469690904.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



## 3.B树的实现

```cpp
#include <stdio.h>



#include <stdlib.h>



#include <string.h>



 



#include <assert.h>



//M/2的值方便后面直接判断合并分裂



#define DEGREE        3



typedef int KEY_VALUE;



 



typedef struct _btree_node {



    KEY_VALUE *keys;//数组M-1



    struct _btree_node **childrens;



    int num;//没有占满



    int leaf;//隐其实它是个状态为什么不是bool,情况一样的



} btree_node;



 



typedef struct _btree {



    btree_node *root;



    int t;



} btree;



 



btree_node *btree_create_node(int t, int leaf) {



 



    btree_node *node = (btree_node*)calloc(1, sizeof(btree_node));



    if (node == NULL) assert(0);



 



    node->leaf = leaf;



    node->keys = (KEY_VALUE*)calloc(1, (2*t-1)*sizeof(KEY_VALUE));



    node->childrens = (btree_node**)calloc(1, (2*t) * sizeof(btree_node));



    node->num = 0;



 



    return node;



}



 



void btree_destroy_node(btree_node *node) {



 



    assert(node);



 



    free(node->childrens);



    free(node->keys);



    free(node);



    



}



 



 



void btree_create(btree *T, int t) {



    T->t = t;



    



    btree_node *x = btree_create_node(t, 1);



    T->root = x;



    



}



 



//传那个节点好?1还是2没有父节点要上去,第i个分裂



void btree_split_child(btree *T, btree_node *x, int i) {



    int t = T->t;



 



    btree_node *y = x->childrens[i];



    btree_node *z = btree_create_node(t, y->leaf);



 



    z->num = t - 1;



 



    int j = 0;



    for (j = 0;j < t-1;j ++) {



        z->keys[j] = y->keys[j+t];



    }



    if (y->leaf == 0) {



        for (j = 0;j < t;j ++) {



            z->childrens[j] = y->childrens[j+t];



        }



    }



 



    y->num = t - 1;



    for (j = x->num;j >= i+1;j --) {



        x->childrens[j+1] = x->childrens[j];



    }



 



    x->childrens[i+1] = z;



 



    for (j = x->num-1;j >= i;j --) {



        x->keys[j+1] = x->keys[j];



    }



    x->keys[i] = y->keys[t-1];



    x->num += 1;



    



}



 



void btree_insert_nonfull(btree *T, btree_node *x, KEY_VALUE k) {



 



    int i = x->num - 1;



 



    if (x->leaf == 1) {



        



        while (i >= 0 && x->keys[i] > k) {



            x->keys[i+1] = x->keys[i];



            i --;



        }



        x->keys[i+1] = k;



        x->num += 1;



        



    } else {



        while (i >= 0 && x->keys[i] > k) i --;



 



        if (x->childrens[i+1]->num == (2*(T->t))-1) {



            btree_split_child(T, x, i+1);



            if (k > x->keys[i+1]) i++;



        }



 



        btree_insert_nonfull(T, x->childrens[i+1], k);



    }



}



 



void btree_insert(btree *T, KEY_VALUE key) {



    //int t = T->t;



 



    btree_node *r = T->root;



    if (r->num == 2 * T->t - 1) {



        



        btree_node *node = btree_create_node(T->t, 0);



        T->root = node;



 



        node->childrens[0] = r;



 



        btree_split_child(T, node, 0);



 



        int i = 0;



        if (node->keys[0] < key) i++;



        btree_insert_nonfull(T, node->childrens[i], key);



        



    } else {



        btree_insert_nonfull(T, r, key);



    }



}



 



void btree_traverse(btree_node *x) {



    int i = 0;



 



    for (i = 0;i < x->num;i ++) {



        if (x->leaf == 0) 



            btree_traverse(x->childrens[i]);



        printf("%C ", x->keys[i]);



    }



 



    if (x->leaf == 0) btree_traverse(x->childrens[i]);



}



 



void btree_print(btree *T, btree_node *node, int layer)



{



    btree_node* p = node;



    int i;



    if(p){



        printf("\nlayer = %d keynum = %d is_leaf = %d\n", layer, p->num, p->leaf);



        for(i = 0; i < node->num; i++)



            printf("%c ", p->keys[i]);



        printf("\n");



#if 0



        printf("%p\n", p);



        for(i = 0; i <= 2 * T->t; i++)



            printf("%p ", p->childrens[i]);



        printf("\n");



#endif



        layer++;



        for(i = 0; i <= p->num; i++)



            if(p->childrens[i])



                btree_print(T, p->childrens[i], layer);



    }



    else printf("the tree is empty\n");



}



 



 



int btree_bin_search(btree_node *node, int low, int high, KEY_VALUE key) {



    int mid;



    if (low > high || low < 0 || high < 0) {



        return -1;



    }



 



    while (low <= high) {



        mid = (low + high) / 2;



        if (key > node->keys[mid]) {



            low = mid + 1;



        } else {



            high = mid - 1;



        }



    }



 



    return low;



}



 



 



//{child[idx], key[idx], child[idx+1]} 



void btree_merge(btree *T, btree_node *node, int idx) {



 



    btree_node *left = node->childrens[idx];



    btree_node *right = node->childrens[idx+1];



 



    int i = 0;



 



    /////data merge



    left->keys[T->t-1] = node->keys[idx];



    for (i = 0;i < T->t-1;i ++) {



        left->keys[T->t+i] = right->keys[i];



    }



    if (!left->leaf) {



        for (i = 0;i < T->t;i ++) {



            left->childrens[T->t+i] = right->childrens[i];



        }



    }



    left->num += T->t;



 



    //destroy right



    btree_destroy_node(right);



 



    //node 



    for (i = idx+1;i < node->num;i ++) {



        node->keys[i-1] = node->keys[i];



        node->childrens[i] = node->childrens[i+1];



    }



    node->childrens[i+1] = NULL;



    node->num -= 1;



 



    if (node->num == 0) {



        T->root = left;



        btree_destroy_node(node);



    }



}



 



void btree_delete_key(btree *T, btree_node *node, KEY_VALUE key) {



 



    if (node == NULL) return ;



 



    int idx = 0, i;



 



    while (idx < node->num && key > node->keys[idx]) {



        idx ++;



    }



 



    if (idx < node->num && key == node->keys[idx]) {



 



        if (node->leaf) {



            



            for (i = idx;i < node->num-1;i ++) {



                node->keys[i] = node->keys[i+1];



            }



 



            node->keys[node->num - 1] = 0;



            node->num--;



            



            if (node->num == 0) { //root



                free(node);



                T->root = NULL;



            }



 



            return ;



        } else if (node->childrens[idx]->num >= T->t) {



 



            btree_node *left = node->childrens[idx];



            node->keys[idx] = left->keys[left->num - 1];



 



            btree_delete_key(T, left, left->keys[left->num - 1]);



            



        } else if (node->childrens[idx+1]->num >= T->t) {



 



            btree_node *right = node->childrens[idx+1];



            node->keys[idx] = right->keys[0];



 



            btree_delete_key(T, right, right->keys[0]);



            



        } else {



 



            btree_merge(T, node, idx);



            btree_delete_key(T, node->childrens[idx], key);



            



        }



        



    } else {



 



        btree_node *child = node->childrens[idx];



        if (child == NULL) {



            printf("Cannot del key = %d\n", key);



            return ;



        }



 



        if (child->num == T->t - 1) {



 



            btree_node *left = NULL;



            btree_node *right = NULL;



            if (idx - 1 >= 0)



                left = node->childrens[idx-1];



            if (idx + 1 <= node->num) 



                right = node->childrens[idx+1];



 



            if ((left && left->num >= T->t) ||



                (right && right->num >= T->t)) {



 



                int richR = 0;



                if (right) richR = 1;



                if (left && right) richR = (right->num > left->num) ? 1 : 0;



 



                if (right && right->num >= T->t && richR) { //borrow from next



                    child->keys[child->num] = node->keys[idx];



                    child->childrens[child->num+1] = right->childrens[0];



                    child->num ++;



 



                    node->keys[idx] = right->keys[0];



                    for (i = 0;i < right->num - 1;i ++) {



                        right->keys[i] = right->keys[i+1];



                        right->childrens[i] = right->childrens[i+1];



                    }



 



                    right->keys[right->num-1] = 0;



                    right->childrens[right->num-1] = right->childrens[right->num];



                    right->childrens[right->num] = NULL;



                    right->num --;



                    



                } else { //borrow from prev



 



                    for (i = child->num;i > 0;i --) {



                        child->keys[i] = child->keys[i-1];



                        child->childrens[i+1] = child->childrens[i];



                    }



 



                    child->childrens[1] = child->childrens[0];



                    child->childrens[0] = left->childrens[left->num];



                    child->keys[0] = node->keys[idx-1];



                    



                    child->num ++;



 



                    node->keys[idx-1] = left->keys[left->num-1];



                    left->keys[left->num-1] = 0;



                    left->childrens[left->num] = NULL;



                    left->num --;



                }



 



            } else if ((!left || (left->num == T->t - 1))



                && (!right || (right->num == T->t - 1))) {



 



                if (left && left->num == T->t - 1) {



                    btree_merge(T, node, idx-1);                    



                    child = left;



                } else if (right && right->num == T->t - 1) {



                    btree_merge(T, node, idx);



                }



            }



        }



 



        btree_delete_key(T, child, key);



    }



    



}



 



 



int btree_delete(btree *T, KEY_VALUE key) {



    if (!T->root) return -1;



 



    btree_delete_key(T, T->root, key);



    return 0;



}



 



 



int main() {



    btree T = {0};



 



    btree_create(&T, 3);



    srand(48);



 



    int i = 0;



    char key[26] = "ABCDEFGHIJKLMNOPQRSTUVWXYZ";



    for (i = 0;i < 26;i ++) {



        //key[i] = rand() % 1000;



        printf("%c ", key[i]);



        btree_insert(&T, key[i]);



    }



 



    btree_print(&T, T.root, 0);



 



    for (i = 0;i < 26;i ++) {



        printf("\n---------------------------------\n");



        btree_delete(&T, key[25-i]);



        //btree_traverse(T.root);



        btree_print(&T, T.root, 0);



    }



    



}



 



 
```



![在这里插入图片描述](https://img-blog.csdnimg.cn/59f9926c949f4768ad2df850430d869c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)

原文作者：[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/604954819

# 【NO.546】互斥锁、读写锁、自旋锁，以及原子操作指令xaddl、cmpxchg的使用场景剖析

## 1.前言

  本文介绍锁与临界资源与原子操作的的使用场景。

  本专栏知识点是通过零声教育的线上课学习，进行梳理总结写下文章，对c/c++linux课程感兴趣的读者，可以点击链接 C/C++后台高级服务器课程介绍 详细查看课程的服务。

## 2.临界资源

### 2.1 什么是临界资源

  临界资源就是被多个线程/进程共享，但在某一时刻只能被一个线程/进程所使用的资源。
  下文以一个经典案例(多线程同时进行i++)介绍三种锁，以及cpu指令集支持的原子操作和CAS。

  主线程启动后创建十个线程，并将主线程中的count变量当作参数传入子线程中，也就是说十个线程同时操作一个共享资源count，子线程执行10w次count++ 操作，主线程每隔两秒打印一次count的值。下面来看看加锁与不加锁的区别。

### 2.2 多线程操作临界资源且不加锁

```
//
// Created by 68725 on 2022/8/3.
//

#include <stdio.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/mman.h>

#define THREAD_SIZE     10


//callback
void *func(void *arg) {
    int *pcount = (int *) arg;
    int i = 0;
    while (i++ < 100000) {
        (*pcount)++;
        usleep(1);
    }
}


int main() {
    pthread_t th_id[THREAD_SIZE] = {0};

    int i = 0;
    int count = 0;
    
    for (i = 0; i < THREAD_SIZE; i++) {
        int ret = pthread_create(&th_id[i], NULL, func, &count);
        if (ret) {
            break;
        }
    }
    for (i = 0; i < 100; i++) {
        printf("count --> %d\n", count);
        sleep(2);
    }

}
```


  我们预期count最终是达到100w，为什么在不加锁的时候没有达到预期效果？很明显，count++不是原子操作

![在这里插入图片描述](https://img-blog.csdnimg.cn/89e8bc5c2c284e6e9c4c28a79dcee5b3.png)

### 2.3 i++不是原子操作,i++对应三条汇编指令

  如果i++是原子操作，那么必然会累加到100w，那么i++到底对应着那几步呢？

  下面以idx++举例，idx的值是存储在内存里面，首先从内存MOV到eax寄存器里面，然后通过寄存器进行自增，最后再从eax写回到内存中。在编译器不做任何优化的情况下，idx++就对应这三个步骤。

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/4661d51cd58b49b2be658a9da18ded83.png)

在大多数情况下，程序是这样执行的

![在这里插入图片描述](https://img-blog.csdnimg.cn/7b1600f44c3440888d0e08dcc37ba3ac.png)

  但是也会存在下面这两种情况。线程1首先将idx的值读到了寄存器，然后cpu执行线程2，线程2执行完三步骤后，又回到线程1，线程1接着执行剩下的两步。有人可能会想，两个线程不都执行完了吗？有什么不同？

![在这里插入图片描述](https://img-blog.csdnimg.cn/28c4e3f1b92147979a6db1d74b9c4d76.png) 

 首先，在线程1让出后，线程1的上下文(比如这里的eax)，是存储到线程1里面的，线程1恢复后，又将上下文load回去。这里就涉及到yield和resume了，详细介绍看纯c协程框架NtyCo实现与原理的第二节与第三节。 理解了上下文的切换后，就容易理解了，有没有发现，两次++操作，最终会漏加。

![在这里插入图片描述](https://img-blog.csdnimg.cn/cedefbea49504537962be1ead0dd4b7c.png)  

所以在多线程中，操作临界资源时，那么这个临界资源是原子的，那么就不用加锁，要么就必须加锁，否在就会出现上述问题！

  那么所谓加锁是什么意思？就是将这三条汇编指令变成一个原子操作，只要有一个线程lock加锁了，别的线程就不能执行进来，直到加锁的线程解锁，别的线程才能加锁。那么这三条汇编指令就是原子的了。下面将介绍3中锁以及2个原子操作。

![在这里插入图片描述](https://img-blog.csdnimg.cn/6ef7a59ea5d64c45b90edac6b5b000c9.png)

### 2.4 多线程操作临界资源且加互斥锁

  下面来看看两种加锁方式，这两种都可以跑了100w，但是这两种加锁的粒度是不一样的，在这个程序中，谁是临界资源？是pcount，而不是while，所以第二种加锁虽然可以跑通，但是它加锁的粒度太大了，就本程序而言，第二种加锁方式这和单线程跑有什么区别？所以我们要对临界资源加锁,不是临界资源的不加锁，掌控好锁的粒度

```
//正确加锁
while (i++ < 100000) {
    pthread_mutex_lock(&mutex);
    (*pcount)++;
    pthread_mutex_unlock(&mutex);
}
//错误加锁
pthread_mutex_lock(&mutex);
while (i++ < 100000) {
    (*pcount)++;
}
pthread_mutex_unlock(&mutex);

```



```
//
// Created by 68725 on 2022/8/3.
//

#include <stdio.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/mman.h>

#define THREAD_SIZE     10
pthread_mutex_t mutex;


//callback
void *func(void *arg) {
    int *pcount = (int *) arg;
    int i = 0;
    while (i++ < 100000) {
        pthread_mutex_lock(&mutex);
        (*pcount)++;
        pthread_mutex_unlock(&mutex);


        usleep(1);
    }

}


int main() {
    pthread_t th_id[THREAD_SIZE] = {0};
    pthread_mutex_init(&mutex, NULL);

    int i = 0;
    int count = 0;
    
    for (i = 0; i < THREAD_SIZE; i++) {
        int ret = pthread_create(&th_id[i], NULL, func, &count);
        if (ret) {
            break;
        }
    }
    for (i = 0; i < 100; i++) {
        printf("count --> %d\n", count);
        sleep(2);
    }

}
```


  可以看到加锁之后，成功达到我们的预期

![在这里插入图片描述](https://img-blog.csdnimg.cn/452374f996034d739e4bef0d41da1cab.png)

### 2.5 多线程操作临界资源且加读写锁

  读写锁，顾名思义，读临界资源的时候加读锁，写临界资源的时候加写锁。适用于读多写少的场景。

A线程加了读锁，B线程可以继续加读锁，但是不能加写锁。
A线程加了写锁，B线程不能加读锁，也不能加写锁。

```
//
// Created by 68725 on 2022/8/3.
//

#include <stdio.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/mman.h>

#define THREAD_SIZE     10
pthread_rwlock_t rwlock;


//callback
void *func(void *arg) {
    int *pcount = (int *) arg;
    int i = 0;
    while (i++ < 100000) {
        pthread_rwlock_wrlock(&rwlock);
        (*pcount)++;
        pthread_rwlock_unlock(&rwlock);


        usleep(1);
    }

}


int main() {
    pthread_t th_id[THREAD_SIZE] = {0};
    pthread_rwlock_init(&rwlock, NULL);


    int i = 0;
    int count = 0;
    
    for (i = 0; i < THREAD_SIZE; i++) {
        int ret = pthread_create(&th_id[i], NULL, func, &count);
        if (ret) {
            break;
        }
    }
    for (i = 0; i < 100; i++) {
        pthread_rwlock_rdlock(&rwlock);
        printf("count --> %d\n", count);
        pthread_rwlock_unlock(&rwlock);
    
        sleep(2);
    }

}



```

![在这里插入图片描述](https://img-blog.csdnimg.cn/89a73d861af543ce8462f5bbea3b7f1b.png)

### 2.6 多线程操作临界资源且加自旋锁

  spinlock与mutex一样，mutex在哪里加锁，spinlock就在哪加锁，使用方法是一样的，但是其内部行为不一样。那么mutex和spinlock的区别在哪呢？

互斥锁在获取不到锁时，会进入休眠，等待释放时被唤醒。会让出CPU。
自旋锁在获取不到锁时，一直等待，在等待过程种不会有进程，线程切换。只会一直等，死等。
  互斥锁与自旋锁的使用场景下文介绍。

```
//
// Created by 68725 on 2022/8/3.
//

#include <stdio.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/mman.h>

#define THREAD_SIZE     10
pthread_spinlock_t spinlock;


//callback
void *func(void *arg) {
    int *pcount = (int *) arg;
    int i = 0;
    while (i++ < 100000) {
        pthread_spin_lock(&spinlock);
        (*pcount)++;
        pthread_spin_unlock(&spinlock);

        usleep(1);
    }

}


int main() {
    pthread_t th_id[THREAD_SIZE] = {0};
    pthread_spin_init(&spinlock, PTHREAD_PROCESS_SHARED);


    int i = 0;
    int count = 0;
    
    for (i = 0; i < THREAD_SIZE; i++) {
        int ret = pthread_create(&th_id[i], NULL, func, &count);
        if (ret) {
            break;
        }
    }
    for (i = 0; i < 100; i++) {
        printf("count --> %d\n", count);
    
        sleep(2);
    }

}
```


![在这里插入图片描述](https://img-blog.csdnimg.cn/fa940a2de3df421badc9e2b8fd362763.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/36e70b5ff42f46a380333c76b23351fb.png)

### 2.7 原子操作

  我们发现加锁，都是将i++对应的汇编的三个步骤，变成原子性。那么我们有没有办法直接将i++对应的汇编指令，变成一条指令？可以，我们使用xaddl这条指令。

  Intel X86指令集提供了指令前缀lock⽤于锁定前端串⾏总线FSB，保证了指令执⾏时不会收到其他处理器的⼲扰。


  所谓原子操作，它不是某条具体的指令，它是CPU支持的指令集，都是原子操作。比如说CAS，CAS是原子操作的一种，而不能说原子操作就是CAS。

```
xaddl -----> Inc
xaddl：第二个参数加第一个参数，并把值存储到第一个参数里
//
// Created by 68725 on 2022/8/3.
//

#include <stdio.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/mman.h>

#define THREAD_SIZE     10

int inc(int *value, int add) {
    int old;
    __asm__ volatile (
    "lock; xaddl %2, %1;" 
    : "=a" (old)
    : "m" (*value), "a" (add)
    : "cc", "memory"
    );
    return old;
}

//callback
void *func(void *arg) {
    int *pcount = (int *) arg;
    int i = 0;
    while (i++ < 100000) {
        inc(pcount, 1);

        usleep(1);
    }

}


int main() {
    pthread_t th_id[THREAD_SIZE] = {0};

    int i = 0;
    int count = 0;
    
    for (i = 0; i < THREAD_SIZE; i++) {
        int ret = pthread_create(&th_id[i], NULL, func, &count);
        if (ret) {
            break;
        }
    }
    for (i = 0; i < 100; i++) {
        printf("count --> %d\n", count);
    
        sleep(2);
    }

}
```




![在这里插入图片描述](https://img-blog.csdnimg.cn/41965f2cf3424a12b58c9e381365b26f.png)

cmpxchg-----> CAS
CAS：Compare And Swap，先比较，再赋值，翻译成代码就是下面

```
if(a==b){//Compare
	a=c;//Swap
}
```



  CPU的指令集支持了先比较后赋值的指令，叫cmpxchg。正因为CPU执行了这个指令，它才是原子操作。

```
//  Perform atomic 'compare and swap' operation on the pointer.
//  The pointer is compared to 'cmp' argument and if they are
//  equal, its value is set to 'val'. Old value of the pointer is returned.
inline T *cas (T *cmp_, T *val_)
{
	T *old;
	__asm__ volatile (
		"lock; cmpxchg %2, %3"
		: "=a" (old), "=m" (ptr)
		: "r" (val_), "m" (ptr), "0" (cmp_)
		: "cc");
	return old;
}
```



## 3.三种锁的api介绍

### 3.1 互斥锁 mutex

  有两个特殊的api，pthread_mutex_trylock 尝试加锁，如果没有获取到锁则返回，而不是休眠。pthread_mutex_timedlock 等待一段时间，超时了还没获取倒锁则返回。

```
/* Mutex handling.  */

/* Initialize a mutex.  */
extern int pthread_mutex_init (pthread_mutex_t *__mutex,
			       const pthread_mutexattr_t *__mutexattr)
     __THROW __nonnull ((1));

/* Destroy a mutex.  */
extern int pthread_mutex_destroy (pthread_mutex_t *__mutex)
     __THROW __nonnull ((1));

/* Try locking a mutex.  */
extern int pthread_mutex_trylock (pthread_mutex_t *__mutex)
     __THROWNL __nonnull ((1));

/* Lock a mutex.  */
extern int pthread_mutex_lock (pthread_mutex_t *__mutex)
     __THROWNL __nonnull ((1));

#ifdef __USE_XOPEN2K
/* Wait until lock becomes available, or specified time passes. */
extern int pthread_mutex_timedlock (pthread_mutex_t *__restrict __mutex,
				    const struct timespec *__restrict
				    __abstime) __THROWNL __nonnull ((1, 2));
#endif

/* Unlock a mutex.  */
extern int pthread_mutex_unlock (pthread_mutex_t *__mutex)
     __THROWNL __nonnull ((1));
```



### 3.2 读写锁 rdlock

  读写锁适用于多读少写的情况，否则还是用互斥锁。

A线程加了读锁，B线程可以继续加读锁，但是不能加写锁。
A线程加了写锁，B线程不能加读锁，也不能加写锁。

```
/* Functions for handling read-write locks.  */

/* Initialize read-write lock RWLOCK using attributes ATTR, or use
   the default values if later is NULL.  */
extern int pthread_rwlock_init (pthread_rwlock_t *__restrict __rwlock,
				const pthread_rwlockattr_t *__restrict
				__attr) __THROW __nonnull ((1));

/* Destroy read-write lock RWLOCK.  */
extern int pthread_rwlock_destroy (pthread_rwlock_t *__rwlock)
     __THROW __nonnull ((1));

/* Acquire read lock for RWLOCK.  */
extern int pthread_rwlock_rdlock (pthread_rwlock_t *__rwlock)
     __THROWNL __nonnull ((1));

/* Try to acquire read lock for RWLOCK.  */
extern int pthread_rwlock_tryrdlock (pthread_rwlock_t *__rwlock)
  __THROWNL __nonnull ((1));

# ifdef __USE_XOPEN2K

/* Try to acquire read lock for RWLOCK or return after specfied time.  */
extern int pthread_rwlock_timedrdlock (pthread_rwlock_t *__restrict __rwlock,
				       const struct timespec *__restrict
				       __abstime) __THROWNL __nonnull ((1, 2));

# endif

/* Acquire write lock for RWLOCK.  */
extern int pthread_rwlock_wrlock (pthread_rwlock_t *__rwlock)
     __THROWNL __nonnull ((1));

/* Try to acquire write lock for RWLOCK.  */
extern int pthread_rwlock_trywrlock (pthread_rwlock_t *__rwlock)
     __THROWNL __nonnull ((1));

# ifdef __USE_XOPEN2K

/* Try to acquire write lock for RWLOCK or return after specfied time.  */
extern int pthread_rwlock_timedwrlock (pthread_rwlock_t *__restrict __rwlock,
				       const struct timespec *__restrict
				       __abstime) __THROWNL __nonnull ((1, 2));

# endif

/* Unlock RWLOCK.  */
extern int pthread_rwlock_unlock (pthread_rwlock_t *__rwlock)
     __THROWNL __nonnull ((1));
```





### 3.3 自旋锁 spinlock

  自旋锁最大的特点是，获取不到锁就一直等待，即使CPU时间片用完了也不会发生切换，死等。而上面两种锁不一样，获取不到就会休眠，让出CPU时间片，切换到其他线程或进程执行。

```
/* Functions to handle spinlocks.  */

/* Initialize the spinlock LOCK.  If PSHARED is nonzero the spinlock can
   be shared between different processes.  */
extern int pthread_spin_init (pthread_spinlock_t *__lock, int __pshared)
     __THROW __nonnull ((1));

/* Destroy the spinlock LOCK.  */
extern int pthread_spin_destroy (pthread_spinlock_t *__lock)
     __THROW __nonnull ((1));

/* Wait until spinlock LOCK is retrieved.  */
extern int pthread_spin_lock (pthread_spinlock_t *__lock)
     __THROWNL __nonnull ((1));

/* Try to lock spinlock LOCK.  */
extern int pthread_spin_trylock (pthread_spinlock_t *__lock)
     __THROWNL __nonnull ((1));

/* Release spinlock LOCK.  */
extern int pthread_spin_unlock (pthread_spinlock_t *__lock)
     __THROWNL __nonnull ((1));
```



## 4.三种锁的使用场景

  比如说读一个文件，就使用mutex。而如果是简单的加加减减操作，就是用spinlock。如果系统提供了原子操作的接口，对于i++这种操作来说，用原子操作更合适。

spinlock：临界资源操作简单/没有发生系统调用/持续时间较短（自旋锁就主要用在临界区持锁时间非常短且CPU资源不紧张的情况下，等待时消耗cpu资源较多，自旋锁一般用于多核的服务器。）

mutex：临界资源操作复杂/发生系统调用/持续时间比较长

- 临界区有IO操作
- 临界区代码复杂或者循环量大
- 临界区竞争非常激烈
- 单核处理器

原子操作：使用场景很小，必须需要CPU的指令集支持才行。（原子操作适用于简单的加减等数学运算，属于粒度最小的操作。比如往链表里增加一个结点，可以做出原子操作吗？不行，因为CPU指令集没有同时多个赋值的指令。cas 多线程同时竞争的时候效率并不会特别高，如果互斥锁和自旋锁能满足要求了尽量不要用cas）

## 5.原子操作的接口

## c

  对于gcc、g++编译器来讲，它们提供了⼀组API来做原⼦操作：

```
type __sync_fetch_and_add (type *ptr, type value, ...)
type __sync_fetch_and_sub (type *ptr, type value, ...)
type __sync_fetch_and_or (type *ptr, type value, ...)
type __sync_fetch_and_and (type *ptr, type value, ...)
type __sync_fetch_and_xor (type *ptr, type value, ...)
type __sync_fetch_and_nand (type *ptr, type value, ...)

bool __sync_bool_compare_and_swap (type *ptr, type oldval type newval, ...)
type __sync_val_compare_and_swap (type *ptr, type oldval type newval, ...)
type __sync_lock_test_and_set (type *ptr, type value, ...)
void __sync_lock_release (type *ptr, ...)
```


  详细⽂档⻅：https://gcc.gnu.org/onlinedocs/gcc-4.1.1/gcc/Atomic-Builtins.html#AtomicBuiltins

  详细⽂档⻅：https://gcc.gnu.org/onlinedocs/gcc-4.1.1/gcc/Atomic-Builtins.html#AtomicBuiltins

c++
  对于c++11来讲，也有⼀组atomic的接⼝：https://en.cppreference.com/w/cpp/atomic
————————————————
版权声明：本文为CSDN博主「cheems~」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_42956653/article/details/126141284

# 【NO.547】网络通信模型和网络IO管理



> 简单的C/S通信模型（accept阻塞的话，就只能一个客户端接进来）
>
> ![img](https://img-community.csdnimg.cn/images/83055856fe174eecaef0f6b12f4abc5e.png)

socket()函数

```c++
//函数原型。返回：若是成功则为非负数，如出错则为 -1



int socket(int domin, int type, int protocol);



 



//调用 参数1：使用多少位地址AF_INET 为32，参数2：数据报的类型。 参数3：TCP/UDP 协议



int clientfd = socket(AF_INET, SOCK_STREAM, 0);
```

connect() 函数,用来向服务器建立连接

```c++
//函数原型。 返回：成功为 0， 出错为 -1



int connect(int clientfd, const struct sockaddr *addr, socklen_t addrlen );
```

bind()函数，绑定地址

```c++
//函数原型 返回：成功为 0，出错为 -1



int     bind(int, const struct sockaddr *, socklen_t) __DARWIN_ALIAS(bind);



//使用：



    struct sockaddr_in _addr;



    _addr.sin_family = AF_INET;



    _addr.sin_port = htons(8888);



    _addr.sin_addr.s_addr = INADDR_ANY;



 



    bind(sockfd, (sockaddr *)&_addr, sizeof(_addr) );
```

listen() 函数，将sock从一个主动套接字转为一个监听套接字

```c++
//函数原型,返回一个非负数的连接描述符



int  listen(int sockfd, int backlog);
```

accept()函数，等待来自客户端的连接请求，到达监听描述符，然后在adr中填写客户端的套接字地址，并返回一个已连接描述符

```c++
int accept(int listenfd, struct sockaddr *addr, int *addrlen);
```

一个类似以上模型的小故事：



![img](https://img-community.csdnimg.cn/images/b9e10b6ba3ff41009b3265c9ef319e21.png)



```c++
#include <netinet/tcp.h>



#include <arpa/inet.h>



#include <errno.h>



#include <fcntl.h>



#include <unistd.h>



#include <stdio.h>



#include <stdlib.h>



using namespace std;



 



int main()



{



    int sockfd = socket(AF_INET, SOCK_STREAM, 0);//创建socket



 



    struct sockaddr_in _addr;



    _addr.sin_family = AF_INET;



    _addr.sin_port = htons(8888);



    _addr.sin_addr.s_addr = INADDR_ANY;



    socklen_t socklen = sizeof(_addr);



    bind(sockfd, (sockaddr *)&_addr, socklen );



 



    int listenfd = listen(sockfd, 0);



    struct sockaddr_in clientAddr;



    socklen_t clientlen = sizeof(clientAddr); 



 



    for (;;){



        int client_sockfd = accept(sockfd,(sockaddr *)&clientAddr, &clientlen);



        char buffer[1024] = {0};



        //等待用户发信息



        ssize_t len_recv = recv(client_sockfd,buffer,1024,0);



        //不做错误处理



        printf("Recv:%s, %d Bytes\n", buffer, len_recv);



        //给用户发信息



        send(client_sockfd,buffer,len_recv,0);



        //关闭这个连接



        close(client_sockfd);



    }



    close(sockfd);



    



 



    return 0;



}
```

socket 五元组标识 ：<客户端地址，客户端端口，服务器地址，服务器端口，使用的协议tcp/udp>，五元组中的任何一个元素发生变化都表示一个新的客户端socket连接。

## 1.网络IO

网络IO会涉及到两个系统对象，一个是用户空间调用IO的进程或线程，另一个是内核空间的内核系统，比如发生IO操作read时，它会经历两个阶段：

> 1. 等待数据准备就绪(数据准备)
> 2. 将数据从内核拷贝到进程或者线程中。(数据读写)

因为在以上两个阶段上各有不同的情况，所以出现了多种网络IO模型

> 阻塞，非阻塞，同步，异步
>
> ![img](https://img-community.csdnimg.cn/images/46746503154646d18719c9f186e84516.png)

### 1.1 五种IO网络模型

#### 1.1.1 阻塞IO（blocking IO）linux默认情况下都是阻塞IO



![img](https://img-community.csdnimg.cn/images/8972d7c087654721a917d7b50e52f3ae.png)



> 当用户进程调用了read系统函数，内核就开始第一阶段数据的准备。对于网路IO来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的数据包），这个时候 内核 就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞（数据一直没准备好，就会一直空占CPU）。当 内核一直等到数据准备好了，它就会将数据从 内核 中拷贝到用户内存，然后 内核 返回结果，用户进程才解除 阻塞 的状态，重新运行起来。

#### 1.1.2 非阻塞 (non-blocking IO)



![img](https://img-community.csdnimg.cn/images/4db23e98077045c58fa63659d92b890d.png)



> 从上图可以看出，应用进程调用了read函数之后，立刻就有结果返回，这个结果给用户判断结果是个error时，它就知道数据没有准备好，于是它再次发送read函数。直到内核中的数据准备好，并且又再次收到用户进程的system call，那么它马上就将数据拷贝到用户内存，然后返回。所以非阻塞模式IO中，用户进程其实就是不断的主动询问内核数据准备好了没有。
> recv() 返回值大于0时，表示接受数据完毕，返回值既是接收到的字节数
> recv() 返回0，表示连接已正常断开；
> recv() 返回-1，且errno等于EAGAIN，表示recv操作还没执行完成；如果errno不等于EAGAIN，表示recv操作遇到系统错误errno
> 设置非阻塞： fcntl( fd, F_SETFL, O_NONBLOCK ); 不推荐使用，因为循环调用read(),会占用大量资源

##### 1.1.2.1 IO多路复用

> 当用户进程调用了 select，那么整个进程会被 block，而同时，kernel 会“监视”所 有 select 负责的 socket，当任何一个 socket 中的数据准备好了，select 就会返回。这个时候用户进程再调用 read 操作，将数据从 kernel 拷贝到用户进程。这个图和 blocking IO 的图其实并没有太大的不同，事实上还更差一些。因为这里需要使用两个系统调用(select 和 read)，而 blocking IO 只调用了一个系统调用(read)。但是使用 select 以后最大的优势是用户可以在一个线程内同时处理多个 socket 的 IO 请求。用户可以注册多个 socket，然后不断地调用 select 读取被激活的 socket，即可达到在同一个线程内同时处理多个 IO 请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。**（所以，如果处理的连接数不是很高的话，使用select/epoll 的 web server 不一定比使用 multi-threading + blocking IO 的 web server 性能更好，可能延迟还更大。select/epoll 的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）**



![img](https://img-community.csdnimg.cn/images/a900640f8cf14ef9b957980a9df904e0.png)



##### 1.1.2.2 信号驱动 （signal-driven）

> 内核在第一阶段是异步，在第二个阶段是同步；于非阻塞IO的区别在于它提供了消息通知机制，不需要用户进程不断的轮询检查，减少了系统API的调用次数，提高效率



![img](https://img-community.csdnimg.cn/images/b36a5ef329a0421f8f882a045a86acb1.png)



#### 1.1.3 异步（asynchronous）



![img](https://img-community.csdnimg.cn/images/32d5d91451dd4ab4b5cf53ee22e18fcf.png)



```c++
struct aiocb { int aio_fildes off_t aio_offset volatile void *aio_buf size_t aio_nbytes int aio_reqprio struct sigevent aio_sigevent int aio_lio_opcode }
```

> 用户进程发起 read 操作之后，立刻就可以开始去做其它的事。而另一方面，从 kernel的角度，当它受到一个 asynchronous read 之后，首先它会立刻返回，所以不会对用户进程产生任何 block。然后，kernel 会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel 会给用户进程发送一个 signal，告诉它 read 操作完成了。

**总结：经上面的介绍，会发现non-blocking IO 和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。
而asynchronous IO则完全不同。它就像是用户进程将整个IO操作都交给了内核去完成，然后内核做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。**

原文作者：[CID( ͡ _ ͡°)](https://blog.csdn.net/weixin_40533189)

原文链接：https://bbs.csdn.net/topics/604975082

# 【NO.548】MYSQL---服务器配置相关问题

## 1.相关面试问题

1. 请分析一个Group By 语句的异常原因
2. 如何比较系统运行配置和配置文件中配置是否一致？
3. 举几个mysql 中的关键性能参数

## 2. 请分析一个Group By 语句的异常原因


可能这个问题会被认为是sql语句的原因， 刚开始我也是这么认为的。不过不急，可以思考思考在看看下面解释

假设有一个这样的表

![在这里插入图片描述](https://img-blog.csdnimg.cn/9acbac0309bc4cce8c4710cd7290165e.png)

sql 语句 ：
select prodcut_id, warehouse_id, sum(count) as cnt from stock group by product_id;

结果：

![在这里插入图片描述](https://img-blog.csdnimg.cn/f558e972ef4145b48f8a3aeb56287ce1.png)

在mysql 中的能执行成功，在其他的数据库可能出现语法错误

为什么能在mysql 中执行成功，是因为在mysql中的一个配置起了作用, 它就是 SQL MODE。

### 2.1 SQL MODE 的作用及设置

SQL_MODE值 ： 会影响mysql 执行sql语句的结果。

配置mysql 处理SQL 的方式
set [session/global/persist] sql_mode = ‘xxxxxx’ (persist是在mysql 8.0 中的)
[mysqld] sql_mode = xxxxxxx

### 2.2 常用的SQL Mode

SQL_MODE= ‘ANSI’ 

![在这里插入图片描述](https://img-blog.csdnimg.cn/8c609b6b2345484e9219ca9465ea7f16.png)SQL_MODE = ‘TRADITIONAL’

![在这里插入图片描述](https://img-blog.csdnimg.cn/05d87f00bb0d48fdbabf3f605283acee.png)

### 2.3 演示 only_full_group_by

1.刚开始查看sql_mode

![在这里插入图片描述](https://img-blog.csdnimg.cn/d4f1db7f25f941a5b2afa145c810561d.png)

2.执行sql语句， 可以的看到是执行成功的

![在这里插入图片描述](https://img-blog.csdnimg.cn/3c777b3e4ff24d09a887cbb3144d8951.png)

3.修改sql_mode

![在这里插入图片描述](https://img-blog.csdnimg.cn/0136ee82d3b24bccac1728aa8d4211b0.png)

4.再次执行上面的sql语句，会报错

![在这里插入图片描述](https://img-blog.csdnimg.cn/2c7abcb14df54fa5a8146616945c822d.png)

5.此时必须在group by 后面写完整

![在这里插入图片描述](https://img-blog.csdnimg.cn/95675139bc554415871e59d68a44ac33.png)

### 2.4 演示 ansi_quotes

使用之后只能用单引号引字符串

![在这里插入图片描述](https://img-blog.csdnimg.cn/6ed2b6d18446476081b8010f2199399a.png)

### 2.5 演示 strict_trans_table

用普通模式，字符串插入int 类型 会成功
用严格模式，则会进行检查，字符串不能插入int 类型成功



## 3.比较系统运行配置和配置文件中配置

### 3.1 知识点

### 3.2 使用set 命令 配置动态参数

使用pt-config-diff 工具比较配置文件 （检查在运行中的配置和系统配置）
使用set 命令 配置动态参数
set [session | @@session.] system_var_name = expr
set [global | @@global .] system_var_name = expr
set [persist | @@persist .] system_var_name = expr (mysql 8.0 中增加)

### 3.3 检查在运行中的配置和系统配置（mysql 5.x）

pt-config-diff u=root, p=, h = localhost /etc/my.cnf

![在这里插入图片描述](https://img-blog.csdnimg.cn/24e7155427734dc5a0cc8d1636b0d341.png)

## 4.举几个mysql 中的关键性能参数

### 4.1 常用的性能参数

![在这里插入图片描述](https://img-blog.csdnimg.cn/6b2bb2968aea45ce8edb6a9638db3a89.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/0178572f3afb49ce8fdd1ab55b51f1f2.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/214d16326f104c66b000a92c69a25ab9.png)版权声明：本文为CSDN博主「_刘小雨」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_39486027/article/details/125210884

# 【NO.549】Linux服务器开发,定时器方案红黑树，时间轮，最小堆

## 1.如何组织定时任务？

首先，定时器组件通常和网络组件一起工作。举个最简单的例子来说：

```c
int event=epoll_wait(epfd,ev,nev,timeout);
```

timeout作为参数值，<0为一直阻塞，=0相当于非阻塞检测双端队列就绪情况0代表立刻返回,>0数值表示最长阻塞的时间。在时间上需要应对的问题填入epoll_wait函数，就可以兼顾网络事件的处理和定时任务的处理。

### 1.1 定时器收网络IO处理造成误差特别大，该怎么处理？

- nginx,redis中通过定时信号去处理+epoll_wait解决

  ，nginx_timer_revolusion()函数定时发送时间信号会打断epoll_wait()的处理，让它尽快的处理定时事件，从而解决了误差较大的问题。很明显，如果网络IO处理事件时，会造成一定的误差。定时任务事件处理=epoll_wait处理时间+网络事件处理时间。

  ```c
  //第一种:通过发送定时信号去打断epoll_wait函数
  
  
  
  while(!quit）
  
  
  
  {
  
  
  
    int now=get_now_time();//单位:ms
  
  
  
    int timeout=get_nearest_time()-now;
  
  
  
    if(timeout<0)
  
  
  
        timeout=0;
  
  
  
    int nevent=epoll_wait(epfd,ev,nev,timeout);
  
  
  
    for(int i=0;i<nevent;i++)
  
  
  
    {
  
  
  
        //网络事件处理
  
  
  
    }
  
  
  
    update_timer();、//时间事件处理
  
  
  
  }
  ```

- 单独开启一个线程，去处理定时任务。

  ```c
  //第二种：再其他线程添加定时任务
  
  
  
  void* thread_timer(void * thread_param)
  
  
  
  {
  
  
  
    init_timer();
  
  
  
    while(!quit)
  
  
  
    {
  
  
  
        update_timer();
  
  
  
        sleep(t);
  
  
  
    }
  
  
  
    clear_timer();
  
  
  
    return nullptr;
  
  
  
  }
  ```

  

  ### 1.2 用何种数据机构存储定时器？

  考虑这个问题要清楚，

  越近要触发的事件优先级越高

  。

  定时器存在的意义是处理延时任务，具体来说比如：

  定期检测客户连接状态，心跳检测，技能冷却CD，倒计时，定时广播，界面实时刷新

  等等。

| 数据结构     | 应用                       | 备注                                                         |
| :----------- | :------------------------- | :----------------------------------------------------------- |
| 红黑树       | nginx                      | 平衡二叉搜索树保证有序，利用时间作为键值，根据触发的时间来排序。 |
| 最小堆       | libevent,go,libev          | 工作中大部分是最小堆。go和libev是最小四叉堆，一般是最小二叉堆。当有大量定时任务时，最小四叉堆比最小二叉堆有5%的性能提升。 |
| 多层级时间轮 | netty,kafka,skynet,crontab | netty是由JAVA开发。                                          |
| 跳表         |                            |                                                              |

#### 1.1.1 红黑树如何解决相同时间的key值的？

大家知道，stl::map和stl::set是内部都是采用红黑树实现的，并没有要求key值一定要不同、

```c
int find_nares_expire_timer(){



        ngx_rbtree_node_t *node;



        if(timer.root==&sentinel){



            return -1;



    }



    node=ngx_rbtree_min(timer.root,timer.sentinel);



    int diff=(int)node->key-(int)current_time();



    return diff>0 ? diff :0;



}
```

#### 1.1.2 最小堆

最小堆是堆排序中一个子的流程，最小堆是一个完全二叉树（其他叶子节点都是满的，而最深的节点叶子都是靠在最左侧），用数组组织其中的元素。与用链表表示堆相比，数组表示堆不仅节省空间，而且更容易实现堆的插入、删除等操作。

![在这里插入图片描述](https://img-blog.csdnimg.cn/a6d8ee615e084648a5937020af05ea3a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



- 某个节点:x
- 左子树索引:2x+1
- 右子树索引:2x+2
- 父节点索引:floor((x-1)/2)

**需要注意的是：最小堆只关注父子的大小，不关注兄弟的大小。**
增加节点只有上升操作，删除节点有上升和下降，个人理解不管是删除还是增加，都是要先对树进行操作，而后维护树的正常秩序。
红黑树和最小堆的增删都是O(logn),在查找最小的就节点方面红黑树是O(logh)，最小堆是O(log1)，很明显最小堆比红黑树更加我稳定一些。

#### 1.1.3 时间轮

crontab是linux的定时服务，它就是用时间轮实现的。与红黑树和最小堆不同的是，时间轮是多线程环境下使用的。
tcp的滑动窗口就是时间窗口的概念，这就是是单层级时间轮的实现。

![限流](https://img-blog.csdnimg.cn/bbe3984697ae42dca0d82c65d280b5f1.png)



![在这里插入图片描述](https://img-blog.csdnimg.cn/978798d73b194b26b85fa4d9cce2047e.png)



![在这里插入图片描述](https://img-blog.csdnimg.cn/e47015398edc48429e8dca27b53ee65f.png)



- 限流:动态的，5秒内有100次操作，换句话说一秒一秒的移动，限制在100次的操作。单层级时间轮就是实现的限流这个操作。更精确，但是
- 熔断:静态的，5秒测一次，5秒测一次的感觉。

##### 1.1.3.1 一个帮助理解单层级时间轮的例子

在客户端给服务器发送心跳的这个过程中，老师举例客户端可能五秒发一次服务器十秒检测一次，如果没有收到心跳就会踢除连接。检测是否有过期连接的方法有两种：一种是每一个连接启动一个定时器，另一种是将所有的连接存放在map<int,connect *>中，用一个定时器去检测上万个连接，因为有很多是新上来的连接，每秒去检测肯定是有很多次浪费的检测。
要设计单层级时间轮要从两个方面考虑:一个是检测间隔时间的大小，另一个是时间轮的精度。

![在这里插入图片描述](https://img-blog.csdnimg.cn/0d0498f4592a4812a683f138017f65ad.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)



公式:(5+10)/8=7

- 5表示当前时间。
- 10表示检测周期。
- 8表示连接数。

计算机内部取余操作m%n=m-n*floor(m/n)，x%16=x &(16-1)。
时间轮根本就不怕任务多，任务越多越好。

##### 1.1.3.2 如何解决空推进的问题？

- kafka利用的是最小堆+单层级时间轮
- 多层级时间轮

##### 1.1.3.3 为什么多线程使用时间轮

多线程要加锁，条件反射第一反应是锁的粒度。增加删除节点的时间复杂度都是O(log1)，所以锁的粒度是最小的。上面也说到，红黑树和最小堆的时间复杂度都是O(logn),所以都使用时间轮。

### 1.2 设计哪些接口，如何设计？

笔者认为，分析定时器该如何设计，当有新设计需求时完全可以借用此模式，所以是有必要用心学的。

```c
//初始化定时器



void init)timer();



//添加定时器



Node * add_timer(int expire,callback cb);



//删除定时器



bool del_timer(Node* node);



//找到最近发生的定时任务



Node* find_nearest_timer();



//清除定时器



void clear_timer();
```

### 1.3 满足哪些条件才能作为定时器的数据结构？

- 能够快速找到这个节点，增加和删除。
- 能够快速找到最小节点。

## 2.定时的方法有哪些？

### 2.1 究竟什么是定时？

定时是指在一段时间后欻某段代码的机制，我们可以利用这段代码有条不紊的处理所有到期的定时器。定时机制是定时器得以比处理的原动力。Linux有三种定时方法:

- socket选项SO_RCVTIMEO和SO_SNDTIMEO。
- SIGALARM信号。
- I/O复用系统调用的超时参数。（上文提到的epoll_wait()）。

socket选项的SO_RCVTIMEO和SO_SNDTIMEO是用来设置socket几首数据和发送数据超时时间的。send,sendmsg,rcv,recvmsg,accept和connect等系统调用都可以设置这个选项，根据这些系统调用的返回值以及errno来判断超时时间是否一道，然后去处理超时任务。
SIGALARM信号是当alarm()和settimer()函数设置实时闹钟时被触发，然后我们利用这个信号的信号处理函数来处理定时任务。定时周期T反应了定时的精度，如果某个定时任务的超时时间不是T的整数倍，那么它实际被执行的时间和恶预期的时间将略有偏差。
Linux下的三组I/O复用系统调用都带有超时参数，因此不仅能统一处理信号和I/事件，也能同意处理定时事件。值得注意的是：I/O复用系统调用可能在超时时间到期内就返回(I/O事件发生)。如果我们要利用它们来定时，就需要不断的更新定时参数以反映剩余的时间。
更多的细节内容还要细致的研读游双先生的《Linux高性能服务器编程》。

## 3.总结

通过零声学院Mark老师精彩的讲述定时器，让小生在定时器这个方面有了新的认识和突破，给鄙人领入了新的世界，开启了新的大门。
想想曾经，小可只沉溺于Qt中的QTimer类，就像一个烂醉如泥的懒汉，终日沉迷于花天酒地，只知开(颠)开(鸾)心(倒)心(凤)的酣畅，却不知阳春白雪正在向自己告别。虽然能够完成简单的任务，但是却从来没有发现其中藏着如此多的秘密。
现在的我，更像是一个满身泥泞，刚刚从大海中登陆抢滩的战士。
在Mark老师吹响嘹亮的冲锋号声中，尽管前进的道路上充满荆棘坎坷，但是为了胜利和尊严，不惜一切代价，奋勇争先。
**生命不息，战斗不止**

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/604944089

# 【NO.550】Posix API 与 网络协议栈 详细介绍

## 0.前言

  本文详细介绍 Posix API 与 网络协议栈 之间的关系；三次握手、数据传输、四次挥手的过程。上下文耦合性较高，不建议跳跃阅读。

  本专栏知识点是通过零声教育的线上课学习，进行梳理总结写下文章，对c/c++linux课程感兴趣的读者，可以点击链接 C/C++后台高级服务器课程介绍 详细查看课程的服务。

## 1.Posix API 有哪些

  哪些是Posix API呢，就是Linux网络编程的这些API，本文介绍下列8种。

Tcp Server

1.socket 2.bind 3.listen 4.accept 5.recv 6.send 7.close

Tcp Client
1.socket 2.bind(option可选) 3.connect 4.send 5.recv 6.close

设置socket参数
1.setsockopt 2.getsockopt 3.shudown 4.fcntl

![image-20230224205136249](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205136249.png)

### 1.1 socket

  socket 是什么东西？中文翻译过来是插座的意思，那插座有两个部分，一个插一个座。

  socket也是由两部分组成，一个是fd（文件系统的文件描述符，是插头），任何我们能对 socket 进行操作的地方都是对这个 fd 进行操作。

  那插到什么地方呢，插头插到插座上，对于TCP而言，每个连接背后都有一个TCB（tcp控制块，tcp control block）。什么是TCB呢，服务器和客户端建立TCP连接，先建立了socket，那么socket底层看不到的东西，就是tcp控制块TCB，而能够看到的，就是文件描述符fd。

  我们操作fd，调用send，其实就是将数据放到TCB里面。调用recv，就是从TCB里拷贝出来。这个fd是我们用户操作的，而TCB是tcp协议栈里面的。一个fd对应一个TCB，这就是socket。

![image-20230224205151107](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205151107.png)

### 1.2 bind

  刚开始创建socket的时候，其底层的TCB是没有被初始化的，没有任何数据，TCB里面的状态机的状态也是close的，发送不了数据，也接收不了数据。

  下面介绍一个新的概念：五元组，当连接有很多很多的时候，哪个包到底是哪个连接的呢，这个时候就需要五元组。也就是说，通过五元组来确定一个TCB。五元组 < 源IP地址 , 源端口 , 目的IP地址 , 目的端口 , 协议 >

  bind的作用就是绑定本地的ip和端口，还有协议。也就是将TCB的五元组填充 <目的IP地址，目的端口，协议> ，注意客户端可以不使用bind函数，但其会默认分配。

## 2.三次握手 建立连接的过程

### 2.1 connect

  三次握手发生在协议栈和协议栈之间，而posix api connect 只是一个导火索，我们写的代码里面是没有写三次握手的。

  首先客户端先发三次握手的第一次数据包，这时候里面带有一个同步头syn，seq=x，这是由客户端内核协议栈发送的数据包。

  服务端接收到之后，返回三次握手的第二个数据包，syn=1,ack=1,seq=y,ack=x+1 。其中ack=x+1代表确认了x+1以前的都收到了，也就是说告诉对端，你发送的数据包，序号在x+1之前的我都收到了。同样也携带自己的一个同步头给对端。

  再往下面走，就是三次握手的第三次，返回一个ack确认包给服务器。

  这就反应了一个现象，tcp是双向的。双向怎么体现的呢，客户端发送一个数据包告诉服务器我现在发送的数据包序号是多少，服务端返回的时候也告诉客户端我这个数据包的序号是多少。这就是双向。

  有人会问三次握手为什么是三次？因为tcp是双向的，A给B发syn，B回一个ack，这里确定了B是存在的，这里两次。B给A发syn，A回一个ack，这里确定了A是存在的，这里两次，而中间的两次可以合到一起，那就是三次了。客户端发送一次syc，服务器返回一个ack并且携带自己的syn，这时候能确定服务器存在，客户端再返回一个ack，这时候能确定客户端存在，这时候就确定了这个双向通道是ok了。

![image-20230224205209641](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205209641.png)

  connect调用connect之后，协议栈开始三次握手。那么connect函数到底组不阻塞呢，取决于传进去的fd，如果fd是阻塞，那么直到第三次握手包发送之后就会返回。如果fd是非阻塞，那么返回-1的时候说明连接建立中，返回0代表连接建立成功。

```
n=-1  , err:Operation now in progress
n=0 
```



### 2.2 listen

  服务器内核协议栈在接收到三次握手的第一次syn包的时候，从这个sync包里面可以解析出来源IP地址 , 源端口 , 目的IP地址 , 目的端口 , 协议 ，那么五元组五元组 < 源IP地址 , 源端口 , 目的IP地址 , 目的端口 , 协议 > 就可以确定下来了，从而构建出来一个TCB,只不过目前这个TCB还不能用，因为还没有分配socket，还没有分配fd。这时候会将TCB加入到半连接队列（sync队列）里面。

  同样当第三次握手收到ACK之后，也有一个队列，叫做全连接队列。所以在第一次收到syn包的时候，服务端做两件事，1返回ACK，2创建一个TCB结点，加入半连接队列里面。在第三次握手的时候，先查找半连接队列。那么怎么查找呢，通过五元组查找。

![image-20230224205231716](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205231716.png)

  那这个半连接队列和全连接队列的建立有没有前提？难道是想建立就建立吗，建立的前提就是必须先进入listen状态。我们来看一下TCP状态转移过程。

  服务器首先进入Listen状态，收SYN，发SYN和，ACK，然后就进入了SYN_RCVD的状态。注意，这里进入SYN_RCVD的状态是刚才新创建的那个TCB，改变的是新创建的TCB的状态，而不是被动监听fd的状态。SYN_RCVD这个状态就暗示这TCB已经进入了半连接队列里面，也就是说半连接队列里面所有的TCB的状态都是SYN_RCVD。

  客户端首先发收SYN包，则客户端进入SYN_SENT状态

![image-20230224205244819](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205244819.png)

![image-20230224205258105](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205258105.png)  

服务器当收到第三次握手的ACK的时候，会将对应的在半连接队列里面的TCB移到全连接队列里面，这个时候TCB的状态由SYN_RCVD变成ESTABLISHED状态。

  客户端发送第三次握手的ACK的时候，也会进入ESTABLISHED状态。

![image-20230224205315759](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205315759.png)


![image-20230224205338370](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205338370.png)现在有一个问题，listen这个函数有两个参数,这个backlog是什么意思？

```
listen(fd,backlog);
```


backlog有两种理解情况

在unix，mac系统里面，半连接队列与全连接队列的总和 <= backlog
在Linux系统里面, 全连接队列<=backlog
ddos攻击：客户端不断的模拟三次握手的第一次，发syn包

如果在Linux系统中，backlog无论设置多少都是没用的。如果是在unix，mac系统中，设置backlog还是有一定作用的。

### 2.3 accept

  这个时候连接已经建立完了，双方都知道对方的存在了，现在就可以调用accept了，accept函数只做两件事情

```
int clientfd=accept()
```


从全连接队列里面取出一个TCB结点
为这个TCB结点分配一个fd，把fd和TCB做一个一对一对应的关系。
直到现在，应用程序才可以操作这个tcp的会话。

![image-20230224205437450](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205437450.png)

## 3.数据传输 发送与接收

### 3.1 send & recv

  我们知道发送用send，接收用recv。

  send只是了将用户态的数据拷贝到内核协议栈对应的TCB里面。至于真正数据发送的时机，什么时候发送的，发送的数据有没有与之前的数据粘在一起，都不是由应用程序决定的，应用程序只能将数据拷贝到内核buffer缓冲区里面。 然后协议栈将sendbuffer的数据，加上TCP的头，加上IP的头，加上以太网的头，一起打包发出去。所以调用send将buffer拷贝到内核态缓冲区，与tcp发送数据到对端，是异步的过程。

  对端网络协议栈接收到数据，同样开始解析，以太网的头mac地址是谁，ip地址从哪里来的，源端口是多少，目的端口是发到哪个进程里面，然后将数据写进对应的TCB里。recv只是将内核态的缓冲区数据拷贝到用户态里面，所以tcp数据到达TCP的recv buffer缓冲区里，与调用recv将缓冲区buffer拷贝到用户态，这两个过程也是异步的。

![image-20230224205627650](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205627650.png)

![image-20230224205638673](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205638673.png)

### 3.2 滑动窗口

  如果客户端不断的send，服务器对应的tcb的recvbuffer缓冲区满了怎么办？

  首先，如果不停的send，直到sendbuffer缓冲区满了，这个时候send会返回-1，代表内核缓冲区满了，send的copy失败。而如果recvbuffer缓冲区满了而应用程序没有去接收，这时候TCP协议栈会告诉对端，我的缓冲区空间还有多大，超过这个大小就不要发（滑动窗口），也就是说recvbuffer缓冲区会通知对端，我能接收多少数据，而对端发送的数据量一定要在这个范围内才行。

![image-20230224205648596](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205648596.png)

  一般send的时候，在TCP协议头里面有一个push的标志位，置1代表立即通知应用程序来读取。

![image-20230224205656952](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205656952.png)

### 3.3 粘包 & 分包

  假设我们连续send 1k的数据三次，那么在内核tcb的缓冲区里面，就有3k的数据，这3k的数据是1次tcp发走，还是分2次，分3次，都不是由应用程序控制的。这就出现了 粘包 和 分包 的问题。

  假设send 了3次，而协议栈只发送了2次，那么在recv的时候读两次，就避免不了数据包合在一起的现象。

解决的方法有两种：

第一种：在数据包前面加上这个包有多长
第二种：为每一个包加一个特定的分隔符

![image-20230224205710246](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205710246.png)

### 3.4 延迟确认（延迟ACK）

  上面两种解决方法有一个很大的前提，就是这个数据包是顺序的。先发的先到，后发的后到，这就是顺序。那TCP是怎么保证顺序的？

  在TCP发送的时候，数据包都是确定的，第一个包发完之后，对端等待一会，再确认这个包。假设现在发5个包，A到了，B到了，每收到一个包，对端都会重置延迟定时器200ms。

  现在假设B包第一个到，现在启动定时器200ms，然后C包到了，重置定时器，在200ms以内A包也到了，再重置，E包到了，重置，最后200ms超时了，D包没到。这时候就会ACK=D，代表D之前的都收到了。接下来D以及D后面的数据包都会重发。

  这样就解决了包的无序的问题，这里的操作都是TCP协议栈来做的。

![在这里插入图片描述](https://img-blog.csdnimg.cn/2b15432170cb41c0b6ba23b07d263b6c.png)



![在这里插入图片描述](https://img-blog.csdnimg.cn/2c9697faf13044cbb6a5c9ecb74e860a.png)

### 3.5 udp场景

  延迟ACK确认时间长，超时重传的时候，重传的包较多，很费带宽。这就有了udp的使用空间。

  随着带宽越来越高，udp的使用场景在不断弱化，但是在弱网的环境下，做大量数据传输的时候，TCP就不合适了，因为一旦出现丢包的情况，后面的包都要重传了。

  并且TCP也没有办法保证实时性，虽然可以关闭延迟ACK来解决这个问题。但是实时性也会用到udp。

  udp场景：1弱网的环境下（电梯里网就很烂） 2实时性要求高的环境下（游戏打团，秒人只在一瞬间）。

## 4.四次挥手 断开连接的过程

  这6个状态就是四次挥手的过程，对于TCP而言建立连接时间很短，断开连接时间很短，中间传输数据是绝大部分时间，但是中间传输数据的状态只有一个，就是ESTABLISHED。断开连接的过程只有一个函数，就是close。

![image-20230224205751721](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205751721.png)

### 4.1 正常情况

  在四次挥手的过程中，没有客户端和服务器之分，只有主动方和被动方之分。主动方首先发送一个fin，被动方返回一个ack。被动方再发送一个fin，主动方返回一个ack。这就是4次挥手

  第一次的fin是哪来的呢，调用close这个函数，协议栈会将最后一个包fin位，置1，被动方接收之后，会触发一个可读事件，recv=0。被动方会做两件事情，第一件事情推给应用程序一个空包，第二件事情直接返回一个ack的包返回给对端。然后被动方recv=0读到之后，应用程序会调用close，这时候被动方也会发送一个fin，对端收到fin会回复一个ack，至此四次挥手完毕。

![image-20230224205805923](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205805923.png)

  四次挥手为什么要有四次？因为tcp是双向的，就跟两个人分手一样，女的跟男的分手，男的说我知道了，那我也跟你分手吧。第一对fin和ack就是女方终止与男方交往，第二对fin和ack就是男方终止与女方交往。这个过程就形成断开了。四次是因为女方跟男方分手之后，男方需要缓和一段时间，中间间隔一段时间（因为这个时候被动方的数据可能还没有发完，在缓和期要把数据都发过去）。

  主动方在调用close之前它的状态是确定的ESTABLISHED状态，发送fin后，进入FIN_WAIT_1状态，收到ack以后进入FIN_WAIT_2状态。如果中间两次ack和fin一起的话，那就没有这个状态，直接进入TIME_WAIT状态。也就是说收到fin后进入TIME_WAIT状态。在等待2MSL后，进入CLOSED状态。TIME_WAIT存在的原因避免最后一个ack丢失,而对端一直超时重发fin，导致连接得不到释放。

  被动方在接收到fin后，进入CLOSE_WAIT状态，之后调用close发送fin后，进入LAST_ACK状态，收到ack之后，进入CLOSED状态。

![image-20230224205826568](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205826568.png)



![image-20230224205838425](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205838425.png)

![image-20230224205847590](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205847590.png)

### 4.2 特殊情况

  有没有一种可能，主动方调用close，被动方也调用close。也就是说一对情侣同时提出分手。在FIN_WAIT1状态期间接收到fin，这时候就进入CLOSING状态，之后收到ack就进入TIME_WAIT状态。

![image-20230224205900107](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205900107.png)

![image-20230224205915052](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230224205915052.png)

### 4.3 一些面试问题

  现在假设客户端连接进入FIN_WAIT_1状态，会在这个状态很久吗？不会，因为即使没有收到ack，也会超时重传fin，进入FIN_WAIT_2状态。

  如果连接停在FIN_WAIT_2状态的时间很久怎么办呢？既然客户端停在FIN_WAIT_2状态，那也就是说服务器停在CLOSE_WAIT状态。服务器出现大量CLOSE_WAIT状态，造成这一现象是因为对方调用关闭，而服务器没有调用close，再去分析，其实就是业务逻辑的问题。

  recv=0，但是没有调用close，原因在哪呢。也就是说从recv到调用close这个过程中间，时间太长，为什么时间太长呢，可能在调用close之前，有去关闭一些fd相关联的业务信息，造成比较耗时的情况。假设现在是一款即时通讯的程序，现在客户端掉线调用了close，服务器接收到recv=0后，服务器需要把这个客户端对应的临时数据同步到数据库里面，会出现一个很耗时的操作，那么这个时间内就是出现CLOSE_WAIT的状态。

  那么如何解决呢，1. 要么先调用close 2. 要么把业务信息抛到消息队列里面交给线程池进行处理。把业务的清理当成一个任务交给另一个线程处理。 原来的线程把网络这一层处理好。

  作为客户端去连第三方服务，长时间卡在FIN_WAIT_2状态，有没有办法去终止它？从FIN_WAIT_2是不能直接到CLOSED状态的，所以这个问题要么再起一个连接，要么就杀死进程，要么就等待FIN_WAIT_2定时器超时。

  如果服务器在调用close之前宕机了，fin是肯定发不到客户端的，那么客户端一直在FIN_WAIT_2状态，这个时候怎么办呢,如果开启了keepalive，检测到是死链接后会被终止掉。那没有开启keepalive呢？

  FIN_WAIT_2 状态的一端一直等不到对端的FIN。如果没有外力的作用，连接两端会一直分别处于 FIN_WAIT_2 和 CLOSE_WAIT 状态。这会造成系统资源的浪费，需要对其进行处理。（内核协议栈就有参数提供了对这种异常情况的处理，无需应用程序操作），也就是说，等着就行。

  如果应用程序调用的是完全关闭（而不是半关闭），那么内核将会起一个定时器，设置最晚收到对端FIN报文的时间。如果定时器超时后仍未收到FIN，且此时TCP连接处于空闲状态，则TCP连接就会从 FIN_WAIT_2 状态直接转入 CLOSED 状态，关闭连接。在Linux系统中可以通过参数 net.ipv4.tcp_fin_timeout 设置定时器的超时时间，默认为60s。

## 5.回收fd和tcb

  被动方调用close之后，fd被回收。在接收到ack以后进入CLOSED后，TCB被回收

  主动方调用close之后，fd被回收，在time_wait时间到了进入CLOSED后，TCB被回收

![在这里插入图片描述](https://img-blog.csdnimg.cn/fbc3c205bc764e0a97a57c75b663fbee.png)

## 6.TCP状态迁移图

![在这里插入图片描述](https://img-blog.csdnimg.cn/b982966e09bc4a15accf484e86a2324a.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/f09987d83e22424d909b1e91faeca7d1.png)————————————————
版权声明：本文为CSDN博主「cheems~」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_42956653/article/details/125727563

# 【NO.551】Linux服务器百万并发实现与问题排查

## 0.前言

  实现一台服务器的百万并发，服务器支撑百万连接会出现哪些问题，如何排查与解决这些问题 是本文的重点

- 服务器能够同时建立连接的数量 不是 并发量，它只是并发量一个基础。

- 服务器的并发量：一个服务器能够同时承载客户端的数量；
- 承载：服务器能够稳定的维持这些连接，能够响应请求，在200ms内返回响应就认为是ok的，其中这200ms包括数据库的操作，网络带宽，内存操作，日志等时间。



## 1.测试介绍

  服务器 采用 1台 centos7 12G 1核虚拟机

  客户端 采用 2台 centos7 3G 1核虚拟机

  服务器代码：单reactor单线程，IO多路复用使用epoll

  客户端代码：IO多路复用使用epoll，每个客户端发51w个连接，每个连接发送一次数据，读取一次数据之后不再发送数据

## 2.服务器代码

  由于fd的数量未知，这里设计ntyreactor 里面包含 eventblock ，eventblock 包含1024个fd。每个fd通过 fd/1024定位到在第几个eventblock，通过fd%1024定位到在eventblock第几个位置。

![在这里插入图片描述](https://img-blog.csdnimg.cn/cd1bc4dc86e84de8ae69958fd5676d49.png)

```
struct ntyevent {
    int fd;
    int events;
    void *arg;

    NCALLBACK callback;
    
    int status;
    char buffer[BUFFER_LENGTH];
    int length;

};
struct eventblock {
    struct eventblock *next;
    struct ntyevent *events;
};

struct ntyreactor {
    int epfd;
    int blkcnt;
    struct eventblock *evblk;
};



```

```
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <arpa/inet.h>
#include <fcntl.h>
#include <unistd.h>
#include <errno.h>

#define BUFFER_LENGTH           4096
#define MAX_EPOLL_EVENTS        1024
#define SERVER_PORT             8081
#define PORT_COUNT              100

typedef int (*NCALLBACK)(int, int, void *);

struct ntyevent {
    int fd;
    int events;
    void *arg;

    NCALLBACK callback;
    
    int status;
    char buffer[BUFFER_LENGTH];
    int length;

};
struct eventblock {
    struct eventblock *next;
    struct ntyevent *events;
};

struct ntyreactor {
    int epfd;
    int blkcnt;
    struct eventblock *evblk;
};


int recv_cb(int fd, int events, void *arg);

int send_cb(int fd, int events, void *arg);

struct ntyevent *ntyreactor_find_event_idx(struct ntyreactor *reactor, int sockfd);

void nty_event_set(struct ntyevent *ev, int fd, NCALLBACK *callback, void *arg) {
    ev->fd = fd;
    ev->callback = callback;
    ev->events = 0;
    ev->arg = arg;
}

int nty_event_add(int epfd, int events, struct ntyevent *ev) {
    struct epoll_event ep_ev = {0, {0}};
    ep_ev.data.ptr = ev;
    ep_ev.events = ev->events = events;
    int op;
    if (ev->status == 1) {
        op = EPOLL_CTL_MOD;
    }
    else {
        op = EPOLL_CTL_ADD;
        ev->status = 1;
    }
    if (epoll_ctl(epfd, op, ev->fd, &ep_ev) < 0) {
        printf("event add failed [fd=%d], events[%d]\n", ev->fd, events);
        return -1;
    }
    return 0;
}

int nty_event_del(int epfd, struct ntyevent *ev) {
    struct epoll_event ep_ev = {0, {0}};
    if (ev->status != 1) {
        return -1;
    }
    ep_ev.data.ptr = ev;
    ev->status = 0;
    epoll_ctl(epfd, EPOLL_CTL_DEL, ev->fd, &ep_ev);
    return 0;
}

int recv_cb(int fd, int events, void *arg) {
    struct ntyreactor *reactor = (struct ntyreactor *) arg;
    struct ntyevent *ev = ntyreactor_find_event_idx(reactor, fd);
    int len = recv(fd, ev->buffer, BUFFER_LENGTH, 0); //
    nty_event_del(reactor->epfd, ev);

    if (len > 0) {
        ev->length = len;
        ev->buffer[len] = '\0';

//        printf("recv[%d]:%s\n", fd, ev->buffer);
        printf("recv fd=[%d\n", fd);

        nty_event_set(ev, fd, send_cb, reactor);
        nty_event_add(reactor->epfd, EPOLLOUT, ev);
    }
    else if (len == 0) {
        close(ev->fd);
        //printf("[fd=%d] pos[%ld], closed\n", fd, ev-reactor->events);
    }
    else {
        close(ev->fd);

//        printf("recv[fd=%d] error[%d]:%s\n", fd, errno, strerror(errno));
    }
    return len;
}


int send_cb(int fd, int events, void *arg) {
    struct ntyreactor *reactor = (struct ntyreactor *) arg;
    struct ntyevent *ev = ntyreactor_find_event_idx(reactor, fd);

    int len = send(fd, ev->buffer, ev->length, 0);
    if (len > 0) {

//        printf("send[fd=%d], [%d]%s\n", fd, len, ev->buffer);
        printf("send fd=[%d\n]", fd);

        nty_event_del(reactor->epfd, ev);
        nty_event_set(ev, fd, recv_cb, reactor);
        nty_event_add(reactor->epfd, EPOLLIN, ev);
    }
    else {
        nty_event_del(reactor->epfd, ev);
        close(ev->fd);
        printf("send[fd=%d] error %s\n", fd, strerror(errno));
    }
    return len;

}

int accept_cb(int fd, int events, void *arg) {//非阻塞
    struct ntyreactor *reactor = (struct ntyreactor *) arg;
    if (reactor == NULL) return -1;

    struct sockaddr_in client_addr;
    socklen_t len = sizeof(client_addr);
    
    int clientfd;
    if ((clientfd = accept(fd, (struct sockaddr *) &client_addr, &len)) == -1) {
        printf("accept: %s\n", strerror(errno));
        return -1;
    }
    if ((fcntl(clientfd, F_SETFL, O_NONBLOCK)) < 0) {
        printf("%s: fcntl nonblocking failed, %d\n", __func__, MAX_EPOLL_EVENTS);
        return -1;
    }
    struct ntyevent *event = ntyreactor_find_event_idx(reactor, clientfd);
    
    nty_event_set(event, clientfd, recv_cb, reactor);
    nty_event_add(reactor->epfd, EPOLLIN, event);
    
    printf("new connect [%s:%d], pos[%d]\n",
           inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), clientfd);
    return 0;

}

int init_sock(short port) {
    int fd = socket(AF_INET, SOCK_STREAM, 0);
    fcntl(fd, F_SETFL, O_NONBLOCK);
    struct sockaddr_in server_addr;
    memset(&server_addr, 0, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(port);

    bind(fd, (struct sockaddr *) &server_addr, sizeof(server_addr));
    
    if (listen(fd, 20) < 0) {
        printf("listen failed : %s\n", strerror(errno));
    }
    return fd;

}


int ntyreactor_alloc(struct ntyreactor *reactor) {
    if (reactor == NULL) return -1;
    if (reactor->evblk == NULL) return -1;

    struct eventblock *blk = reactor->evblk;
    while (blk->next != NULL) {
        blk = blk->next;
    }
    
    struct ntyevent *evs = (struct ntyevent *) malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    if (evs == NULL) {
        printf("ntyreactor_alloc ntyevents failed\n");
        return -2;
    }
    memset(evs, 0, (MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    
    struct eventblock *block = (struct eventblock *) malloc(sizeof(struct eventblock));
    if (block == NULL) {
        printf("ntyreactor_alloc eventblock failed\n");
        return -2;
    }
    memset(block, 0, sizeof(struct eventblock));
    
    block->events = evs;
    block->next = NULL;
    
    blk->next = block;
    reactor->blkcnt++; //
    return 0;

}

struct ntyevent *ntyreactor_find_event_idx(struct ntyreactor *reactor, int sockfd) {
    int blkidx = sockfd / MAX_EPOLL_EVENTS;

    while (blkidx >= reactor->blkcnt) {
        ntyreactor_alloc(reactor);
    }
    int i = 0;
    struct eventblock *blk = reactor->evblk;
    while (i++ < blkidx && blk != NULL) {
        blk = blk->next;
    }
    return &blk->events[sockfd % MAX_EPOLL_EVENTS];

}


int ntyreactor_init(struct ntyreactor *reactor) {
    if (reactor == NULL) return -1;
    memset(reactor, 0, sizeof(struct ntyreactor));

    reactor->epfd = epoll_create(1);
    if (reactor->epfd <= 0) {
        printf("create epfd in %s err %s\n", __func__, strerror(errno));
        return -2;
    }
    
    struct ntyevent *evs = (struct ntyevent *) malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    if (evs == NULL) {
        printf("ntyreactor_alloc ntyevents failed\n");
        return -2;
    }
    memset(evs, 0, (MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    
    struct eventblock *block = (struct eventblock *) malloc(sizeof(struct eventblock));
    if (block == NULL) {
        printf("ntyreactor_alloc eventblock failed\n");
        return -2;
    }
    memset(block, 0, sizeof(struct eventblock));
    
    block->events = evs;
    block->next = NULL;
    
    reactor->evblk = block;
    reactor->blkcnt = 1;
    return 0;

}

int ntyreactor_destory(struct ntyreactor *reactor) {
    close(reactor->epfd);
    //free(reactor->events);

    struct eventblock *blk = reactor->evblk;
    struct eventblock *blk_next = NULL;
    
    while (blk != NULL) {
        blk_next = blk->next;
        free(blk->events);
        free(blk);
        blk = blk_next;
    }
    return 0;

}


int ntyreactor_addlistener(struct ntyreactor *reactor, int sockfd, NCALLBACK *acceptor) {
    if (reactor == NULL) return -1;
    if (reactor->evblk == NULL) return -1;

    struct ntyevent *event = ntyreactor_find_event_idx(reactor, sockfd);
    
    nty_event_set(event, sockfd, acceptor, reactor);
    nty_event_add(reactor->epfd, EPOLLIN, event);
    return 0;

}


_Noreturn int ntyreactor_run(struct ntyreactor *reactor) {
    if (reactor == NULL) return -1;
    if (reactor->epfd < 0) return -1;
    if (reactor->evblk == NULL) return -1;

    struct epoll_event events[MAX_EPOLL_EVENTS + 1];
    
    int i;
    
    while (1) {
        int nready = epoll_wait(reactor->epfd, events, MAX_EPOLL_EVENTS, 1000);
        if (nready < 0) {
            printf("epoll_wait error, exit\n");
            continue;
        }
        for (i = 0; i < nready; i++) {
            struct ntyevent *ev = (struct ntyevent *) events[i].data.ptr;
            if ((events[i].events & EPOLLIN) && (ev->events & EPOLLIN)) {
                ev->callback(ev->fd, events[i].events, ev->arg);
            }
            if ((events[i].events & EPOLLOUT) && (ev->events & EPOLLOUT)) {
                ev->callback(ev->fd, events[i].events, ev->arg);
            }
        }
    }

}


// <remoteip, remoteport, localip, localport,protocol>
int main(int argc, char *argv[]) {
    unsigned short port = SERVER_PORT; // listen 8081
    if (argc == 2) {
        port = atoi(argv[1]);
    }
    struct ntyreactor *reactor = (struct ntyreactor *) malloc(sizeof(struct ntyreactor));
    ntyreactor_init(reactor);
    int i = 0;
    int sockfds[PORT_COUNT] = {0};
    for (i = 0; i < PORT_COUNT; i++) {
        sockfds[i] = init_sock(port + i);
        ntyreactor_addlistener(reactor, sockfds[i], accept_cb);
    }
    ntyreactor_run(reactor);
    ntyreactor_destory(reactor);
    for (i = 0; i < PORT_COUNT; i++) {
        close(sockfds[i]);
    }
    free(reactor);
    return 0;
}
```

## 3.客户端代码

```
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <errno.h>
#include <netinet/tcp.h>
#include <arpa/inet.h>
#include <netdb.h>
#include <fcntl.h>
#include <sys/time.h>
#include <unistd.h>

#define MAX_BUFFER		128
#define MAX_EPOLLSIZE	(384*1024)
#define MAX_PORT		100
#define TIME_SUB_MS(tv1, tv2)  ((tv1.tv_sec - tv2.tv_sec) * 1000 + (tv1.tv_usec - tv2.tv_usec) / 1000)

int isContinue = 0;

static int ntySetNonblock(int fd) {
	int flags;

	flags = fcntl(fd, F_GETFL, 0);
	if (flags < 0) return flags;
	flags |= O_NONBLOCK;
	if (fcntl(fd, F_SETFL, flags) < 0) return -1;
	return 0;

}

static int ntySetReUseAddr(int fd) {
	int reuse = 1;
	return setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, (char *)&reuse, sizeof(reuse));
}



int main(int argc, char **argv) {
	if (argc <= 2) {
		printf("Usage: %s ip port\n", argv[0]);
		exit(0);
	}
	const char *ip = argv[1];
	int port = atoi(argv[2]);
	int connections = 0;
	char buffer[128] = {0};
	int i = 0, index = 0;
	struct epoll_event events[MAX_EPOLLSIZE];
	int epoll_fd = epoll_create(MAX_EPOLLSIZE);
	strcpy(buffer, " Data From MulClient\n");
	struct sockaddr_in addr;
	memset(&addr, 0, sizeof(struct sockaddr_in));
	addr.sin_family = AF_INET;
	addr.sin_addr.s_addr = inet_addr(ip);
	struct timeval tv_begin;
	gettimeofday(&tv_begin, NULL);

	while (1) {
		if (++index >= MAX_PORT) index = 0;
		struct epoll_event ev;
		int sockfd = 0;
		if (connections < 340000 && !isContinue) {
			sockfd = socket(AF_INET, SOCK_STREAM, 0);
			if (sockfd == -1) {
				perror("socket");
				goto err;
			}
			//ntySetReUseAddr(sockfd);
			addr.sin_port = htons(port+index);
			if (connect(sockfd, (struct sockaddr*)&addr, sizeof(struct sockaddr_in)) < 0) {
				perror("connect");
				goto err;
			}
			ntySetNonblock(sockfd);
			ntySetReUseAddr(sockfd);
			sprintf(buffer, "Hello Server: client --> %d\n", connections);
			send(sockfd, buffer, strlen(buffer), 0);
			ev.data.fd = sockfd;
			ev.events = EPOLLIN | EPOLLOUT;
			epoll_ctl(epoll_fd, EPOLL_CTL_ADD, sockfd, &ev);
			connections ++;
		}
		//connections ++;
		if (connections % 1000 == 999 || connections >= 340000) {
			struct timeval tv_cur;
			memcpy(&tv_cur, &tv_begin, sizeof(struct timeval));
			gettimeofday(&tv_begin, NULL);
			int time_used = TIME_SUB_MS(tv_begin, tv_cur);
			printf("connections: %d, sockfd:%d, time_used:%d\n", connections, sockfd, time_used);
			int nfds = epoll_wait(epoll_fd, events, connections, 100);
			for (i = 0;i < nfds;i ++) {
				int clientfd = events[i].data.fd;
				if (events[i].events & EPOLLOUT) {
					sprintf(buffer, "data from %d\n", clientfd);
					send(sockfd, buffer, strlen(buffer), 0);
				} else if (events[i].events & EPOLLIN) {
					char rBuffer[MAX_BUFFER] = {0};				
					ssize_t length = recv(sockfd, rBuffer, MAX_BUFFER, 0);
					if (length > 0) {
						printf(" RecvBuffer:%s\n", rBuffer);
						if (!strcmp(rBuffer, "quit")) {
							isContinue = 0;
						}
					} else if (length == 0) {
						printf(" Disconnect clientfd:%d\n", clientfd);
						connections --;
						close(clientfd);
					} else {
						if (errno == EINTR) continue;
						printf(" Error clientfd:%d, errno:%d\n", clientfd, errno);
						close(clientfd);
					}
				} else {
					printf(" clientfd:%d, errno:%d\n", clientfd, errno);
					close(clientfd);
				}
			}
		}
		usleep(1 * 1000);
	}
	return 0;

err:
	printf("error : %s\n", strerror(errno));
	return 0;
}
```



## 4.**error : Too many open files**

### 4.1 确定问题

  程序执行到一半，创建了1023个连接后，报错Too many open files

```
//服务端
new connect [192.168.109.101:36994], pos[1019]
new connect [192.168.109.101:55832], pos[1020]
new connect [192.168.109.101:43460], pos[1021]
new connect [192.168.109.101:59938], pos[1022]
new connect [192.168.109.101:46098], pos[1023]
accept: Too many open files
accept: Too many open files

//客户端
connect: Connection refused
error : Connection refused
```

  怀疑是文件系统默认允许打开文件描述符数量个数（默认1024）的限制，使用ulimit -a查看open files的数量

open files：一个进程能够打开文件描述符的数量

```
[root@master temp]# ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 47748
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 1024
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 47748
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
```


  那么我们把open files调大一点点，看是否会停在2047，如果是，则说明问题就是open files太小的问题，实验发现就是这个原因。

```
[root@master temp]# ulimit -n 2048
[root@master temp]# ulimit -a
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 47748
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 2048
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 47748
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited


new connect [192.168.109.101:53996], pos[2046]
new connect [192.168.109.101:60742], pos[2047]
accept: Too many open files
```

### 4.2 解决问题

1. 临时修改，只在当前这个会话有效：ulimit -n 1048576
2. 永久修改，对所有会话有效：添加下面两行代码


注意这里修改的是：一个进程能够打开文件描述符的数量

```
[root@master temp]# vim /etc/security/limits.conf

# 修改

[root@master temp]# reboot

# 重启生效
```


* ```
  *               soft    nofile          1048576
  *               hard    nofile          1048576
  ```

  软限制：超出软限制会发出警告
  硬限制：绝对限制,在任何情况下都不允许用户超过这个限制

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/f7b87a1dd9914d28a8cbc785f987cb01.png)

  这里还需要注意一点：file-max : 系统一共可以打开的最大文件数（所有进程加起来）

```
[root@master temp]# cat /proc/sys/fs/file-max
1202172
```



```
# 编辑内核参数配置文件

vim /etc/sysctl.conf

# 修改fs.file-max参数

fs.file-max = 1048576

# 重新加载配置文件

sysctl -p
```


  另外这里建议ulimit -n 和limits.conf里nofile 设定最好不要超过/proc/sys/fs/file-max的值（虽然我测试了超过也没关系），这个小问题仁者见仁智者见智了，网上找到比较好的文章是这篇linux最大文件句柄数量之（file-max ulimit -n limit.conf）

## 5.error : Cannot assign requested address

### 5.1 确定问题

  现在的环境背景：服务器只开放一个端口，客户端不断的去请求去连接。然后客户端error : Cannot assign requested address

  Cannot assign requested address这代表着客户端端口耗尽，我们先来看看如何确定一个fd，反过来说一个fd代表着什么

  socket fd --- < 源IP地址 , 源端口 , 目的IP地址 , 目的端口 , 协议 > 一个fd就是一个五元组，在现在的环境中，五元组里面确定了四个，所以最多创建 1 * 源端口 * 1 * 1 * 1个fd

```
# 服务端

new connect [192.168.109.101:57921], pos[28234]
new connect [192.168.109.101:57923], pos[28235]
send[fd=21003] error Connection reset by peer
send[fd=22003] error Connection reset by peer

# 客户端

connections: 26999, sockfd:27002, time_used:2399
connections: 27999, sockfd:28002, time_used:2404
connect: Cannot assign requested address
error : Cannot assign requested address
```


  我们看到大概创建了2.8w的fd ， 可是我们知道端口一个有6w多个，也就是说有6w个端口，为什么我们只使用了2.8w个？

  我们看到大概创建了2.8w的fd ， 可是我们知道端口一个有6w多个，也就是说有6w个端口，为什么我们只使用了2.8w个？

  Linux中有限定端口的使用范围:60999 - 32768 = 2.8w ,与我们上面实验结果相符。

```
The /proc/sys/net/ipv4/ip_local_port_range defines the local port range that is used by TCP and UDP traffic to choose the local port. You will see in the parameters of this file two numbers: The first number is the first local port allowed for TCP and UDP traffic on the server, the second is the last local port number. For high-usage systems you may change its default parameters to 32768-61000 -first-last.
```

```
proc/sys/net/ipv4/ip_local_port_range范围定义TCP和UDP通信用于选择本地端口的本地端口范围。您将在该文件的参数中看到两个数字：第一个数字是服务器上允许TCP和UDP通信的第一个本地端口，第二个是最后一个本地端口号。对于高使用率的系统，您可以将其默认参数更改为32768-61000(first-last)。
```



```
[root@master temp]# sysctl net.ipv4.ip_local_port_range
net.ipv4.ip_local_port_range = 32768	60999
```



### 5.2 解决问题

  

修改net.ipv4.ip_local_port_range的范围，一般不这样做，我们这里研究的是服务器，怎么会去对客户端进行修改呢
之前已经说了这个问题的背景，就是只开放了一个端口，并且socket fd --- < 源IP地址 , 源端口, 目的IP地址 , 目的端口 , 运输层协议 >，在这个背景下才产生的这个问题，所以我们可以开放更多的端口，比如说100个，那么一个客户端就能连到280w了
error : Connection timed out
确定问题
  我们将服务器端口开100个，按理说客户端可以连280w，但是现在只连接到13w就error : Connection timed out，与我们的预期不符

```
//服务端
new connect [192.168.109.101:54585], pos[131165]
new connect [192.168.109.101:48265], pos[131166]
new connect [192.168.109.101:51997], pos[131167]
new connect [192.168.109.101:43239], pos[131168]
send[fd=20102] error Connection reset by peer
send[fd=21102] error Connection reset by peer
send[fd=22102] error Connection reset by peer

//客户端
connections: 127999, sockfd:128002, time_used:7576
connections: 128999, sockfd:129002, time_used:2683
connections: 129999, sockfd:130002, time_used:2669
connections: 130999, sockfd:131002, time_used:4610

connect: Connection timed out
error : Connection timed out
```


  网卡接收的数据，会发送到协议栈里面，通过sk_buff将数据传到协议栈，协议栈处理完再交给应用程序。由于操作系统在使用的时候，为防止被攻击，在数据发送给协议栈之前进行一个过滤，在协议栈前面加了一个小组件：过滤器，叫做netfilter。
  netfilter主要是对网络数据包进行一个过滤，在netfilter的基础上我们就可以实现防火墙，在linux里面有一个就叫做iptables，iptables是基于netfilter做的，iptables分为两部分，一部分是内核实现的netfilter接口，一部分是应用程序提供给用户使用的。iptables真正实现的是netfilter提供的接口。

![在这里插入图片描述](https://img-blog.csdnimg.cn/d61f3f57bcb6484a83437fc952e68a6a.png)

  Connection timed out译为连接超时，也就是说，client发送的请求超时了，那么这个超时有两种情况，第一种：三次握手第一次的SYN没发出去，第二种：三次握手第二次ACK没收到。

![在这里插入图片描述](https://img-blog.csdnimg.cn/ff4ef956709d408b810464af46e3380e.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/b388247e018b45a5ba21ff565fe7987b.png)  

netfilter不管对发送的数据，还是对接收的数据，都是可以过滤的。当连接数量达到一定数量的时候，netfilter就会不允许再对外发连接了。所以现在推测是情况1造成的，发送的SYN被netfilter拦截了。

  事实是这样吗，我们来查看一下netfilter允许对外最大连接数量是多少。13w，与我们上面建立成功的数量一致，所以现在就可以确定是netfilter允许对外开放的最大连接数造成的了

```
[root@node1 temp]# cat /proc/sys/net/netfilter/nf_conntrack_max
131072
```


解决问题
  我们可以通过设置netfilter允许对外最大连接数量，来解决这个问题

```
# 查看允许对外最大连接数量

[root@node1 temp]# cat /proc/sys/net/netfilter/nf_conntrack_max
131072

# 进行配置

vim /etc/sysctl.conf

# 在配置文件中把net.nf_conntrack_max参数修改为1048576（如果配置就自己添加一行）

net.nf_conntrack_max = 1048576

# 重新加载配置文件

sysctl -p

# 再次查看，发现生效了

[root@node1 temp]# cat /proc/sys/net/netfilter/nf_conntrack_max
1048576
```



## 6.killed（已杀死）

### 6.1 确定问题

  这里我们先给客户端虚拟机2G的内存，然后发现到24w的时候，客户端进程被杀死了

```
connections: 239999, sockfd:240002, time_used:9837
connections: 240999, sockfd:241002, time_used:10608
connections: 241999, sockfd:242002, time_used:13109
connections: 242999, sockfd:243002, time_used:15112
connections: 243999, sockfd:244002, time_used:12606
已杀死
```


  我们来看一下kill记录，发现是内存不足。

```
[root@node1 ~]# dmesg | egrep -i -B100 'killed process'
[ 2310.265218] Out of memory: Kill process 7266 (C1000Kclient) score 1 or sacrifice child
[ 2310.265962] Killed process 7266 (C1000Kclient) total-vm:8708kB, anon-rss:2960kB, file-rss:0kB, shmem-rss:0kB
```


  这里直接说原因吧，是因为程序每个fd都有一个tcp接收缓冲区和tcp发送缓冲区。而默认的太大了，导致Linux内存不足，进程被杀死，所有我们需要适当的缩小。进程空间，代码段，堆栈都是要占用内存的。

### 6.2 解决问题

  我们只需要对net.ipv4.tcp_mem，net.ipv4.tcp_wmem，net.ipv4.tcp_rmem进行适合的修改即可

```
# 编辑内核参数配置文件

vim /etc/sysctl.conf

# 添加以下内容

# 					最小值   默认值   最大值

net.ipv4.tcp_mem = 252144 524288 786432	# tcp协议栈的大小，单位为内存页（4K），分别是 1G 2G 3G，如果大于2G，tcp协议栈会进行一定的优化
net.ipv4.tcp_wmem = 1024 1024 2048 # tcp接收缓存区(用于tcp接受滑动窗口)的最小值，默认值和最大值（单位byte）1k 1k 2k,每一个连接fd都有一个接收缓存区
net.ipv4.tcp_rmem = 1024 1024 2048 # tcp发送缓存区(用于tcp发送滑动窗口)的最小值，默认值和最大值（单位byte）1k 1k 2k,每一个连接fd都有一个发送缓存区

# 总缓存 = （每个fd发送缓存区 + 每个fd接收缓存区） * fd数量

# （1024byte + 1024byte ） * 100w 约等于 2G
```

  如果服务器是用来接收大文件，传输量很大的时候，就要把send buffer和read buffer调大。
  如果服务器只是接收小数据字符的时候。把buffer调小是为了把fd的数量做到更多，并发数量能做到更大。如果buffer调大的话，内存会不够。

![在这里插入图片描述](https://img-blog.csdnimg.cn/a1489051ed9147a396c1be49dc2253b5.png)

## 7.百万并发测试结果

## 8.出现的问题总结

  想要实现服务器百万并发：

一个进程能够打开文件描述符的数量open files 和 file-max 改成100w以上
在不同的环境下要看开放的端口够不够socket fd --- < 源IP地址 , 源端口 , 目的IP地址 , 目的端口 , 协议 >
设置netfilter允许对外最大连接数量100w以上
根据内存和场景，适当调整net.ipv4.tcp_mem，net.ipv4.tcp_wmem，net.ipv4.tcp_rmem
————————————————
版权声明：本文为CSDN博主「cheems~」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_42956653/article/details/125653754

# 【NO.552】ZMQ无锁队列的原理与实现

## 0.前言

  本文介绍ZMQ无锁队列的原理与实现

  无锁队列用在什么地方？每秒几十万的元素时再考虑使用无锁队列，比如股票行情这种。如果队列里一秒就几千几万的元素，那就不需要使用无锁队列，性能没有太大的提高。

  源码：ypipe.hpp

  本专栏知识点是通过零声教育的线上课学习，进行梳理总结写下文章，对c/c++linux课程感兴趣的读者，可以点击链接 C/C++后台高级服务器课程介绍 详细查看课程的服务。

## 1.为什么需要⽆锁队列

锁是解决并发问题的万能钥匙，可是并发问题只有锁能解决吗？锁引起的问题：

Cache损坏(Cache trashing)
  线程间频繁切换的时候会导致 Cache 中数据的丢失，Cache中的数据会失效,因为它缓存的是将被换出任务的数据,这些数据对于新换进的任务是没⽤的。处理器的运⾏速度⽐主存快N倍,所以⼤量的处理器时间被浪费在处理器与主存的数据传输上。这就是在处理器和主存之间引⼊Cache的原因。Cache是⼀种速度更快但容量更⼩的内存(也更加昂贵),当处理器要访问主存中的数据时,这些数据⾸先被拷⻉到Cache中，因为这些数据在不久的将来可能⼜会被处理器访问。Cache misses对性能有⾮常⼤的影响,因为处理器访问Cache中的数据将⽐直接访问主存快得多。在保存和恢复上下⽂的过程中还隐藏了额外的开销。

在同步机制上争抢队列
  阻塞不是微不⾜道的操作。它导致操作系统暂停当前的任务或使其进⼊睡眠状态(等待，不占⽤任何的处理器)。直到资源(例如互斥锁)可⽤，被阻塞的任务才可以解除阻塞状态(唤醒)。在⼀个负载较重的应⽤程序中使⽤这样的阻塞队列来在线程之间传递消息会导致严重的争⽤问题。也就是说，任务将⼤量的时间(睡眠，等待，唤醒)浪费在获得保护队列数据的互斥锁，⽽不是处理队列中的数据上。

  ⾮阻塞机制⼤展伸⼿的机会到了。任务之间不争抢任何资源，在队列中预定⼀个位置，然后在这个位置上插⼊或提取数据。这中机制使⽤了⼀种被称之为CAS(⽐较和交换)的特殊操作，这个特殊操作是⼀种特殊的指令，它可以原⼦的完成以下操作:它需要3个操作数m，A，B，其中m是⼀个内存地址，操作将m指向的内存中的内容与A⽐较，如果相等则将B写⼊到m指向的内存中并返回true，如果不相等则什么也不做返回false。简而言之非阻塞的机制使用了 CAS 的特殊操作，使得任务之间可以不争抢任何资源，然后在队列中预定的位置上，插入或者提取数据。 CAS底层实现

多线程动态内存分配malloc性能下降
  在多线程系统中,需要仔细的考虑动态内存分配。当⼀个任务从堆中分配内存时，标准的内存分配机制会阻塞所有与这个任务共享地址空间的其它任务(进程中的所有线程)。这样做的原因是让处理更简单，且它⼯作得很好。两个线程不会被分配到⼀块相同的地址的内存，因为它们没办法同时执⾏分配请求。显然线程频繁分配内存会导致应⽤程序性能下降(必须注意,向标准队列或map插⼊数据的时候都会导致堆上的动态内存分配)

## 2.无锁队列的实现(参考zmq，只支持一写一读的场景)

### 2.1 无锁队列前言

  //TODO git地址补充 源码的ypipe.hpp、yqueue.hpp，这些源码可以在⼯程项⽬使⽤，但要注意，这⾥只⽀持单写单读的场景。 其中yqueue 是用来设计队列，ypipe 用来设计队列的写入/读取时机、回滚以及 flush，首先我们来看 yqueue 的设计。

### 2.2 原⼦操作函数介绍

```
template<typename T>
class atomic_ptr_t {
public:
    void set(T *ptr_); //⾮原⼦操作
    T *xchg(T *val_); //原⼦操作，设置⼀个新的值，然后返回旧的值
    T *cas(T *cmp_, T *val_);//原⼦操作
private:
    volatile T *ptr;
};
```


set函数，把私有成员ptr指针设置成参数ptr_的值，不是⼀个原⼦操作，需要使⽤者确保执⾏set过程没有其他线程使⽤ptr的值。
xchg函数，把私有成员ptr指针设置成参数val_的值，并返回ptr设置之前的值。原⼦操作，线程安全。
cas函数，原⼦操作，线程安全，把私有成员ptr指针与参数cmp_指针⽐较：如果相等返回ptr设置之前的值，并把ptr更新为参数val_的值，如果不相等直接返回ptr值。

### 2.3 yqueue_t的chunk块机制

#### 2.3.1 chunk块机制 一次分配多个元素

  首先我们需要考虑元素的分配，元素存在哪里？yqueue 中的数据结构使用的 chunk 块机制，每次批量分配一批元素，这样可以减少内存的分配和释放yqueue_t内部由⼀个⼀个chunk组成，每个chunk保存N个元素：spare_chunk⾥⾯，当再次需要分配chunk_t的时候从spare_chunk中获取。

  当队列空间不⾜时每次分配⼀个chunk_t，每个chunk_t能存储N个元素。在数据出队列后，队列有多余空间的时候，回收的chunk也不是⻢上释放，⽽是根据局部性原理先回收到

```
struct chunk_t {
   T values[N]; //每个chunk_t可以容纳N个T类型的元素，以后就以一个chunk_t为单位申请内存
   chunk_t *prev;
   chunk_t *next;
};
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/d0638af66f4442db94addeab8ce4b9fa.png)

#### 2.3.2 chunk块机制 局部性原理

程序局部性原理：是指程序在执行时呈现出局部性规律，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域，具体来说，局部性通常有两种形式：时间局部性和空间局部性。

时间局部性：被引用过一次的存储器位置在未来会被多次引用（通常在循环中）。

空间局部性：如果一个存储器的位置被引用，那么将来他附近的位置也会被引用。

  在yqueue_t类中有一个spare_chunk用于保存最近的空闲块 。也就是说，在将一个chunk中的所有元素都pop掉了，那么我们可以free这个chunk。但是我们可以保存一块最近的空闲块，以后如果chunk不够用时，扩容chunk就不用malloc，直接复用该spare_chunk即可。根据局部性原理，这个spare_chunk的地址或者内存页很有可能还在cache里，那么这样的机制就可以减少一次malloc以及存入cache的操作。

```
//  class yqueue_t
//  People are likely to produce and consume at similar rates.  In
//  this scenario holding onto the most recently freed chunk saves
//  us from having to call malloc/free.
atomic_ptr_t<chunk_t> spare_chunk; //空闲块（把所有元素都已经出队的块称为空闲块），读写线程的共享变量
```


  可以看到在pop的时候，如果删除满一格chunk，就把这个chunk放到spare_chunk里。

```
//  Removes an element from the front end of the queue.
inline void pop() {
    if (++begin_pos == N) // 删除满一个chunk才回收chunk
    {
        chunk_t *o = begin_chunk;
        begin_chunk = begin_chunk->next;
        begin_chunk->prev = NULL;
        begin_pos = 0;

        //  'o' has been more recently used than spare_chunk,
        //  so for cache reasons we'll get rid of the spare and
        //  use 'o' as the spare.
        chunk_t *cs = spare_chunk.xchg(o); //由于局部性原理，总是保存最新的空闲块而释放先前的空闲快
        free(cs);
    }

}
```


  在push的时候，如果chunk满了，说明要发生扩容，那么我们优先从spare_chunk取出最近的空闲块当新的chunk来使用

  在push的时候，如果chunk满了，说明要发生扩容，那么我们优先从spare_chunk取出最近的空闲块当新的chunk来使用

```
//  Adds an element to the back end of the queue.
inline void push() {
    back_chunk = end_chunk;
    back_pos = end_pos; //

    if (++end_pos != N) //end_pos!=N表明这个chunk节点还没有满
        return;
    
    chunk_t *sc = spare_chunk.xchg(NULL); // 为什么设置为NULL？ 因为如果把之前值取出来了则没有spare chunk了，所以设置为NULL
    if (sc)                               // 如果有spare chunk则继续复用它
    {
        end_chunk->next = sc;
        sc->prev = end_chunk;
    }
    else // 没有则重新分配
    {
        // static int s_cout = 0;
        // printf("s_cout:%d\n", ++s_cout);
        end_chunk->next = (chunk_t *) malloc(sizeof(chunk_t)); // 分配一个chunk
        alloc_assert(end_chunk->next);
        end_chunk->next->prev = end_chunk;
    }
    end_chunk = end_chunk->next;
    end_pos = 0;

}
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/229615b5cbe9409a9de7e4363d1a3f38.png)

### 2.4 yqueue_t成员和接口介绍

  yqueue_t的作用就是管理元素、管理chunk。chunk和spare_chunk上文已经说过了，这里不再赘述。

```
// T is the type of the object in the queue.队列中元素的类型
// N is granularity(粒度) of the queue，简单来说就是chunk_t ⼀个结点可以装载N个T类型的元素
template<typename T, int N>
class yqueue_t {
public:
    inline yqueue_t();// 创建队列.
    inline ~yqueue_t();// 销毁队列.
    inline T &front();// Returns reference to the front element of the queue. If the queue is empty, behaviour is undefined.
    inline T &back();// Returns reference to the back element of the queue.If the queue is empty, behaviour is undefined.
    inline void push();// Adds an element to the back end of the queue.
    inline void pop();// Removes an element from the front of the queue.
    inline void unpush()// Removes element from the back end of the queue。 回滚时使⽤
private:
// Individual memory chunk to hold N elements.
    struct chunk_t {
        T values[N];
        chunk_t *prev;
        chunk_t *next;
    };
    chunk_t *begin_chunk;
    int begin_pos;
    chunk_t *back_chunk;
    int back_pos;
    chunk_t *end_chunk;
    int end_pos;
    atomic_ptr_t<chunk_t> spare_chunk; //空闲块（我把所有元素都已经出队的块称为空闲块），读写线程的共享变量
};
```

#### 2.4.1 begin/back/end_chunk 与 begin/back/end_pos 成员介绍

```
chunk_t *begin_chunk;
int begin_pos;
chunk_t *back_chunk;
int back_pos;
chunk_t *end_chunk;
int end_pos;
```


  yqueue_t内部有三个chunk_t类型指针以及对应的索引位置：

begin_chunk/begin_pos：begin_chunk用于指向队列的第一个chunk，begin_pos用于指向第一个chunk的第一个元素的索引位置，因为pop()，所以第一个元素不可能永远是0，会随着pop而改变。同理第一个chunk也会被回收，也需要记录第一个chunk的位置。

back_chunk/back_pos：begin_chunk用于指向队列的最后一个chunk，back_pos用于指向最后一个chunk的最后一个元素的索引位置。

end_chunk/end_pos：在最后一个chunk未满的情况下，end_chunk和back_chunk是相同的，back_pos的下一个就是end_pos。在最后一个chunk满的情况下，end_chunk指向新分配的chunk，end_pos=0。也就是说end_chunk和end_pos是辅助back_chunk/back_pos的，可以理解为探测。

![在这里插入图片描述](https://img-blog.csdnimg.cn/c2904d3617f0470bb53209856d0f014d.png)

#### 2.4.2 函数介绍

frount和pop连用。back和push连用。

**构造函数yqueue_t**
  预先分配⼀个chunk。

```
//  创建队列.
inline yqueue_t() {
    begin_chunk = (chunk_t *) malloc(sizeof(chunk_t));
    alloc_assert(begin_chunk);
    begin_pos = 0;
    back_chunk = NULL; //back_chunk总是指向队列中最后一个元素所在的chunk，现在还没有元素，所以初始为空
    back_pos = 0;
    end_chunk = begin_chunk; //end_chunk总是指向链表的最后一个chunk
    end_pos = 0;
}
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/6b0c8f0f9b9e40e7a4fb1e24edaca650.png)

**稀构函数~yqueue_t**
  销毁所有的chunk

```
//  销毁队列.
inline ~yqueue_t() {
    while (true) {
        if (begin_chunk == end_chunk) {
            free(begin_chunk);
            break;
        }
        chunk_t *o = begin_chunk;
        begin_chunk = begin_chunk->next;
        free(o);
    }

    chunk_t *sc = spare_chunk.xchg(NULL);
    free(sc);

}
```


**front、back函数**
这⾥的front()或者back()函数，需要注意的返回的是左值引⽤，我们可以修改其值。
对于先进后出队列⽽⾔：

begin_chunk->values[begin_pos]代表队列头可读元素， 读取队列头元素即是读取begin_pos位置的元素；
back_chunk->values[back_pos]代表队列尾可写元素，写⼊元素时则是更新back_pos位置的元素，要确保元素真正⽣效，还需要调⽤push函数更新back_pos的位置，避免下次更新的时候⼜是更新当前back_pos位置对应的元素。

```
//  Returns reference to the front element of the queue.
//  If the queue is empty, behaviour is undefined.
// 返回队列头部元素的引用，调用者可以通过该引用更新元素，结合pop实现出队列操作。
inline T &front() // 返回的是引用，是个左值，调用者可以通过其修改容器的值
{
    return begin_chunk->values[begin_pos];
}

//  Returns reference to the back element of the queue.
//  If the queue is empty, behaviour is undefined.
// 返回队列尾部元素的引用，调用者可以通过该引用更新元素，结合push实现插入操作。
// 如果队列为空，该函数是不允许被调用。
inline T &back() // 返回的是引用，是个左值，调用者可以通过其修改容器的值
{
    return back_chunk->values[back_pos];
}
```


**push函数**
当++end_pos != N 时，说明当前的chunk还有空余位置可以插入，则不需要扩容
当++end_pos == N时，说明当前的chunk已经插入满了，下一次插入就要插入到新的chunk了，所以需要发生扩容
  需要新分配chunk时，先尝试从spare_chunk获取，如果获取到则直接使⽤，如果spare_chunk为NULL则需要重新分配chunk。最终都是要更新end_chunk和end_pos。

```
//  Adds an element to the back end of the queue.
    inline void push() {
        back_chunk = end_chunk;
        back_pos = end_pos; //

​    if (++end_pos != N) //end_pos!=N表明这个chunk节点还没有满
​        return;

​    chunk_t *sc = spare_chunk.xchg(NULL); // 为什么设置为NULL？ 因为如果把之前值取出来了则没有spare chunk了，所以设置为NULL
​    if (sc)                               // 如果有spare chunk则继续复用它
​    {
​        end_chunk->next = sc;
​        sc->prev = end_chunk;
​    }
​    else // 没有则重新分配
​    {
​        // static int s_cout = 0;
​        // printf("s_cout:%d\n", ++s_cout);
​        end_chunk->next = (chunk_t *) malloc(sizeof(chunk_t)); // 分配一个chunk
​        alloc_assert(end_chunk->next);
​        end_chunk->next->prev = end_chunk;
​    }
​    end_chunk = end_chunk->next;
​    end_pos = 0;
}
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/c2904d3617f0470bb53209856d0f014d.png)

**unpush函数**
  unpush函数没什么好说的，也是考虑有没有发生扩容的情况，然后分两种情况回退即可。

```
//  Removes element from the back end of the queue. In other words
 //  it rollbacks last push to the queue. Take care: Caller is
 //  responsible for destroying the object being unpushed.
 //  The caller must also guarantee that the queue isn't empty when
 //  unpush is called. It cannot be done automatically as the read
 //  side of the queue can be managed by different, completely
 //  unsynchronised thread.
 // 必须要保证队列不为空，参考ypipe_t的uwrite
 inline void unpush() {
     //  First, move 'back' one position backwards.
     if (back_pos) // 从尾部删除元素
         --back_pos;
     else {
         back_pos = N - 1; // 回退到前一个chunk
         back_chunk = back_chunk->prev;
     }

     //  Now, move 'end' position backwards. Note that obsolete end chunk
     //  is not used as a spare chunk. The analysis shows that doing so
     //  would require free and atomic operation per chunk deallocated
     //  instead of a simple free.
     if (end_pos) // 意味着当前的chunk还有其他元素占有
         --end_pos;
     else {
         end_pos = N - 1; // 当前chunk没有元素占用，则需要将整个chunk释放
         end_chunk = end_chunk->prev;
         free(end_chunk->next);
         end_chunk->next = NULL;
     }

 }
```


**pop函数**
++begin_pos != N，说明当前chunk还有元素没被取出，该chunk还要继续被使⽤；
++end_pos == N，说明该chunk的所有元素已经被取出，所以该chunk要被回收。把最后回收的chunk保存到spare_chunk，然后释放之前spare_chunk保存的chunk。
这⾥有两个点需要注意：

pop掉的元素，其销毁⼯作交给调⽤者完成，即是pop前调⽤者需要通过front()接⼝读取并进⾏销毁
空闲块的保存，要求是原⼦操作。因为闲块是读写线程的共享变量，因为在push中也使⽤了spare_chunk。

```
//  Removes an element from the front end of the queue.
inline void pop() {
    if (++begin_pos == N) // 删除满一个chunk才回收chunk
    {
        chunk_t *o = begin_chunk;
        begin_chunk = begin_chunk->next;
        begin_chunk->prev = NULL;
        begin_pos = 0;

        //  'o' has been more recently used than spare_chunk,
        //  so for cache reasons we'll get rid of the spare and
        //  use 'o' as the spare.
        chunk_t *cs = spare_chunk.xchg(o); //由于局部性原理，总是保存最新的空闲块而释放先前的空闲快
        free(cs);
    }

}
```

### 2.5 ypipe—> yqueue的封装

  yqueue 负责元素内存的分配与释放，入队以及出队列；ypipe 负责 yqueue 读写指针的变化。ypipe_t在yqueue_t的基础上构建⼀个单写单读的⽆锁队列

```
template<typename T, int N>
class ypipe_t {
public:
    // Initialises the pipe.
    inline ypipe_t();

    // The destructor doesn't have to be virtual. It is mad virtual
    // just to keep ICC and code checking tools from complaining.
    inline virtual ~ypipe_t();
    
    // Write an item to the pipe. Don't flush it yet. If incomplete is
    // set to true the item is assumed to be continued by items
    // subsequently written to the pipe. Incomplete items are never flushed down the stream.
    // 写⼊数据，incomplete参数表示写⼊是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。
    inline void write(const T &value_, bool incomplete_);
    
    // Pop an incomplete item from the pipe. Returns true is such
    // item exists, false otherwise.
    inline bool unwrite(T *value_);
    
    // Flush all the completed items into the pipe. Returns false if
    // the reader thread is sleeping. In that case, caller is obliged to
    // wake the reader up before using the pipe again.
    // 刷新所有已经完成的数据到管道，返回false意味着读线程在休眠，在这种情况下调⽤者需要唤醒读线程。
    inline bool flush();
    
    // Check whether item is available for reading.
    // 这⾥⾯有两个点，⼀个是检查是否有数据可读，⼀个是预取
    inline bool check_read();
    
    // Reads an item from the pipe. Returns false if there is no value.
    // available.
    inline bool read(T *value_);
    
    // Applies the function fn to the first elemenent in the pipe
    // and returns the value returned by the fn.
    // The pipe mustn't be empty or the function crashes.
    inline bool probe(bool (*fn)(T &));

protected:
    // Allocation-efficient queue to store pipe items.
    // Front of the queue points to the first prefetched item, back of
    // the pipe points to last un-flushed item. Front is used only by
    // reader thread, while back is used only by writer thread.
    yqueue_t<T, N> queue;
    // Points to the first un-flushed item. This variable is used
    // exclusively by writer thread.
    T *w;//指向第⼀个未刷新的元素,只被写线程使⽤
    // Points to the first un-prefetched item. This variable is used
    // exclusively by reader thread.
    T *r;//指向第⼀个还没预提取的元素，只被读线程使⽤
    // Points to the first item to be flushed in the future.
    T *f;//指向下⼀轮要被刷新的⼀批元素中的第⼀个
    // The single point of contention between writer and reader thread.
    // Points past the last flushed item. If it is NULL,
    // reader is asleep. This pointer should be always accessed using
    // atomic operations.
    atomic_ptr_t<T> c;//读写线程共享的指针，指向每⼀轮刷新的起点（看代码的时候会详细说）。当c为空时，表示读线程睡眠（只会在读线程中被设置为空）
    // Disable copying of ypipe object.
    ypipe_t(const ypipe_t &);
    const ypipe_t &operator=(const ypipe_t &);
};
```

#### 2.5.1 如何写入和读取

  这一节的目的是找出改变wrfc这四个指针的的函数，至于函数的具体作用会放下下面写。

  写入可以单独写，也可以批量写，先来看看write函数。可以看到如果incomplete_=true，则说明在批量写，直到incomplete_=false时，进行写提交。

```
//  Write an item to the pipe.  Don't flush it yet. If incomplete is
//  set to true the item is assumed to be continued by items
//  subsequently written to the pipe. Incomplete items are never flushed down the stream.
// 写入数据，incomplete参数表示写入是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。
inline void write(const T &value_, bool incomplete_) {
    //  Place the value to the queue, add new terminator element.
    queue.back() = value_;
    queue.push();

    //  Move the "flush up to here" poiter.
    if (!incomplete_) {
        f = &queue.back(); // 记录要刷新的位置
    }

}
```

```
//1. 单次写
yquque.write(count,false);
yquque.flush();
//2. 批量写
yquque.write(count,true);
yquque.write(count,true);
yquque.write(count,false);
yquque.flush();
```


  上面两种方式最后都用到了flush，下面来看看flush。

```
//  Flush all the completed items into the pipe. Returns false if
//  the reader thread is sleeping. In that case, caller is obliged to
//  wake the reader up before using the pipe again.
// 刷新所有已经完成的数据到管道，返回false意味着读线程在休眠，在这种情况下调用者需要唤醒读线程。
// 批量刷新的机制， 写入批量后唤醒读线程；
// 反悔机制 unwrite
inline bool flush() {
    //  If there are no un-flushed items, do nothing.
    if (w == f) // 不需要刷新，即是还没有新元素加入
        return true;

    //  Try to set 'c' to 'f'.
    // read时如果没有数据可以读取则c的值会被置为NULL
    if (c.cas(w, f) != w) // 尝试将c设置为f，即是准备更新w的位置
    {
    
        //  Compare-and-swap was unseccessful because 'c' is NULL.
        //  This means that the reader is asleep. Therefore we don't
        //  care about thread-safeness and update c in non-atomic
        //  manner. We'll return false to let the caller know
        //  that reader is sleeping.
        c.set(f); // 更新为新的f位置
        w = f;
        return false; //线程看到flush返回false之后会发送一个消息给读线程，这需要写业务去做处理
    }
    else  // 读端还有数据可读取
    {
        //  Reader is alive. Nothing special to do now. Just move
        //  the 'first un-flushed item' pointer to 'f'.
        w = f;             // 更新f的位置
        return true;
    }

}
```


  写入分析完了，来看看如何读。

```
//  Check whether item is available for reading.
// 这里面有两个点，一个是检查是否有数据可读，一个是预取
inline bool check_read() {
    //  Was the value prefetched already? If so, return.
    if (&queue.front() != r && r) //判断是否在前几次调用read函数时已经预取数据了return true;
        return true;

    //  There's no prefetched value, so let us prefetch more values.
    //  Prefetching is to simply retrieve the
    //  pointer from c in atomic fashion. If there are no
    //  items to prefetch, set c to NULL (using compare-and-swap).
    // 两种情况
    // 1. 如果c值和queue.front()， 返回c值并将c值置为NULL，此时没有数据可读
    // 2. 如果c值和queue.front()， 返回c值，此时可能有数据度的去
    r = c.cas(&queue.front(), NULL); //尝试预取数据
    
    //  If there are no elements prefetched, exit.
    //  During pipe's lifetime r should never be NULL, however,
    //  it can happen during pipe shutdown when items are being deallocated.
    if (&queue.front() == r || !r) //判断是否成功预取数据
        return false;
    
    //  There was at least one value prefetched.
    return true;

}

//  Reads an item from the pipe. Returns false if there is no value.
//  available.
inline bool read(T *value_) {
    //  Try to prefetch a value.
    if (!check_read())
        return false;

    //  There was at least one value prefetched.
    //  Return it to the caller.
    *value_ = queue.front();
    queue.pop();
    return true;

}
```


  下面来多分析一下，如果read返回false，那么我们应该怎么做？读失败意味着管道内没有可读的数据，所以我们可以休眠，可以让出cpu，也可以条件等待。

  这里最正确的做法是用条件等待。上面的flush返回false代表着读端在等待，那么flush返回false后我们应该通知读端。

```
//读端
if (yqueue.read(&value)) {
    //数据处理
}
else {
    // usleep(100);
    std::unique_lock<std::mutex> lock(ypipe_mutex_);
    ypipe_cond_.wait(lock);
    // sched_yield();
}

//写端
yqueue.write(count, false);
if (!yqueue.flush()) {
    // printf("notify_one\n");
    std::unique_lock<std::mutex> lock(ypipe_mutex_);
    ypipe_cond_.notify_one();
}
```


  其实我们初略的观察这些函数，就能发现，这几个函数改变的是w,r,f,c这四个指针，下面来看看这四个指针的具体作用吧。

#### 2.5.2 w,r,f,c图文结合详解（重点理解）

  这里这几个变量非常抽象，要结合着函数来讲

T *f：指向下一轮要被刷新的一批元素的第一个。

T *w：指向第一个未刷新的元素，只被写线程使用；

T *r：指向第一个没有被预提取的元素，只被读线程使用；

atomic_ptr_t c：读写线程共享的指针，指向每⼀轮刷新的起点。当c为空时，表示读线程睡眠（只会在读线程中被设置为空）

  

  

write()：写⼊数据，incomplete参数表示写⼊是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。完成后会将f = &queue.back();

unwrite()：在数据没有flush之前可以运⾏反悔 Pop an incomplete item from the pipe. Returns true is such item exists, false otherwise.

bool flush()：将write的元素真正刷新到队列，使读端可以访问对应的数据。返回false意味着读线程在休眠，在这种情况下调⽤者需要唤醒读线程。如果读端阻塞，则c=f;w=f;否则w=f;

bool check_read()：检测是否有数据可读，如果c==queue.front则c=NULL,否则r=c

bool read (T *value_)：读数据，将读出的数据写⼊value指针中，返回false意味着没有数据可读

  这样写感觉还是非常抽象，下面结合着函数和图来讲这些函数与四个变量的关系吧。

**构造函数ypipe_t()**
  在构造函数里面，下一轮要被刷新的元素的第一个(f)，必然是第一个位置；第一个未刷新的元素(w)，也是第一个位置；第一个没有被预读取的元素( r )，也是第一个位置；每一轮刷新的起点，也是第一个位置( c );

![在这里插入图片描述](https://img-blog.csdnimg.cn/42c1568a93414e338b5df01f5f1058ef.png)

```
inline ypipe_t() {
	//  Insert terminator element into the queue.
	queue.push(); //yqueue_t的尾指针加1，开始back_chunk为空，现在back_chunk指向第一个chunk_t块的第一个位置
	

	//  Let all the pointers to point to the terminator.
	//  (unless pipe is dead, in which case c is set to NULL).
	r = w = f = &queue.back(); //就是让r、w、f、c四个指针都指向这个end迭代器
	c.set(&queue.back());

}
```

**写入函数write(const T &value_, bool incomplete_)**
  第二个参数决定是否要刷新一批元素，false时，刷新一批元素，那么下一轮要被刷新的元素的第一个( f ) 就要改变了。

![在这里插入图片描述](https://img-blog.csdnimg.cn/fef85d08827b41f38eeb3646468f3835.png)

```
//  Write an item to the pipe.  Don't flush it yet. If incomplete is
//  set to true the item is assumed to be continued by items
//  subsequently written to the pipe. Incomplete items are never flushed down the stream.
// 写入数据，incomplete参数表示写入是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。
inline void write(const T &value_, bool incomplete_) {
    //  Place the value to the queue, add new terminator element.
    queue.back() = value_;
    queue.push();

​    //  Move the "flush up to here" poiter.
​    if (!incomplete_) {
​        f = &queue.back(); // 记录要刷新的位置
​    }
}
```


**刷新元素使元素对读线程可见 bool flush()**
  还记得c吗？指向每一轮刷新的起点。如果c和w一样，则尝试将c置为f。刷新元素，指向第一个未刷新的元素( w )，那么必然w=f了。此时前面的元素都可以被读线程可见。

  我们来看看什么情况下c != w。

在未更新前队列没有数据可读，没有数据可读的时候，check_read将c⾥⾯的ptr置为NULL。所以会走下面的流程。返回false的⽬的是告诉调⽤者数据读端(接收端)没有数据可读，可能处于休眠的状态，可以结合condition机制，发送⼀个notify唤醒读端继续读取数据。

```
//  Try to set 'c' to 'f'.
// read时如果没有数据可以读取则c的值会被置为NULL
if (c.cas(w, f) != w) // 尝试将c设置为f，即是准备更新w的位置
{

​    //  Compare-and-swap was unseccessful because 'c' is NULL.
​    //  This means that the reader is asleep. Therefore we don't
​    //  care about thread-safeness and update c in non-atomic
​    //  manner. We'll return false to let the caller know
​    //  that reader is sleeping.
​    c.set(f); // 更新为新的f位置
​    w = f;
​    return false; //线程看到flush返回false之后会发送一个消息给读线程，这需要写业务去做处理
}
```


未更新前队列有数据可读，此时只需要更新w即可，但此时c值不去更新。

```
else  // 读端还有数据可读取
{
    //  Reader is alive. Nothing special to do now. Just move
    //  the 'first un-flushed item' pointer to 'f'.
    w = f;             // 更新f的位置
    return true;
}
```

 ![在这里插入图片描述](https://img-blog.csdnimg.cn/1ea57df9ce504077a67e08db39632476.png)

 从write和flush我们也可以看出来，在更新w和f的时候并没有互斥的保护，所以此程序插⼊数据的时候不适合⽤于多线程场景。

  flush函数主要是将w更新到f的位置，说明已经写到的位置。

```
//  Flush all the completed items into the pipe. Returns false if
//  the reader thread is sleeping. In that case, caller is obliged to
//  wake the reader up before using the pipe again.
// 刷新所有已经完成的数据到管道，返回false意味着读线程在休眠，在这种情况下调用者需要唤醒读线程。
// 批量刷新的机制， 写入批量后唤醒读线程；
// 反悔机制 unwrite
inline bool flush() {
    //  If there are no un-flushed items, do nothing.
    if (w == f) // 不需要刷新，即是还没有新元素加入
        return true;

​    //  Try to set 'c' to 'f'.
​    // read时如果没有数据可以读取则c的值会被置为NULL
​    if (c.cas(w, f) != w) // 尝试将c设置为f，即是准备更新w的位置
​    {

​        //  Compare-and-swap was unseccessful because 'c' is NULL.
​        //  This means that the reader is asleep. Therefore we don't
​        //  care about thread-safeness and update c in non-atomic
​        //  manner. We'll return false to let the caller know
​        //  that reader is sleeping.
​        c.set(f); // 更新为新的f位置
​        w = f;
​        return false; //线程看到flush返回false之后会发送一个消息给读线程，这需要写业务去做处理
​    }
​    else  // 读端还有数据可读取
​    {
​        //  Reader is alive. Nothing special to do now. Just move
​        //  the 'first un-flushed item' pointer to 'f'.
​        w = f;             // 更新f的位置
​        return true;
​    }
}
```


**预取读取函数ckeck_read()**
  如果指针r指向的是队头元素（r==&queue.front()）或者r没有指向任何元素（NULL）则说明队列中并没有可读的数据，这个时候check_read尝试去预取数据。所谓的预取就是令 r=c (cas函数就是返回c本身的值，看上⾯关于cas的实现)， ⽽c在write中被指向f（⻅上图），这时从queue.front()到f这个位置的数据都被预取出来了，然后每次调⽤read都能取出⼀段。

![在这里插入图片描述](https://img-blog.csdnimg.cn/7def7e4a2047482f98a5ca741e0a3d43.png)

  值得注意的是，当c==&queue.front()时，代表数据被取完了，这时把c指向NULL，接着读线程会睡眠，这也是给写线程 检查 读线程是否睡眠的标志（c指向NULL）。

  继续上⾯写⼊AB数据的场景，第⼀次调⽤read时，会先check_read，把指针r指向指针c的位置（所谓的预取），这时r,c,w,f的关系如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/6afcecfc52bb4598bd2ab91c2ef13893.png)

  为什么要预读取？当front()和r相等时：

r = c.cas(&queue.front(), NULL);执行之前，如果写端没有flush，那么c置为NULL，说明没有数据可读，返回false。
r = c.cas(&queue.front(), NULL);执行之前，如果写端调用flush，那么c就不等于front()，则r返回了新的f值，最终返回true。

```
//  Check whether item is available for reading.
// 这里面有两个点，一个是检查是否有数据可读，一个是预取
inline bool check_read() {
    //  Was the value prefetched already? If so, return.
    if (&queue.front() != r && r) //判断是否在前几次调用read函数时已经预取数据了return true;
        return true;

​    //  There's no prefetched value, so let us prefetch more values.
​    //  Prefetching is to simply retrieve the
​    //  pointer from c in atomic fashion. If there are no
​    //  items to prefetch, set c to NULL (using compare-and-swap).
​    // 两种情况
​    // 1. 如果c值和queue.front()， 返回c值并将c值置为NULL，此时没有数据可读
​    // 2. 如果c值和queue.front()， 返回c值，此时可能有数据度的去
​    r = c.cas(&queue.front(), NULL); //尝试预取数据

​    //  If there are no elements prefetched, exit.
​    //  During pipe's lifetime r should never be NULL, however,
​    //  it can happen during pipe shutdown when items are being deallocated.
​    if (&queue.front() == r || !r) //判断是否成功预取数据
​        return false;

​    //  There was at least one value prefetched.
​    return true;
}

//  Reads an item from the pipe. Returns false if there is no value.
//  available.
inline bool read(T *value_) {
    //  Try to prefetch a value.
    if (!check_read())
        return false;

​    //  There was at least one value prefetched.
​    //  Return it to the caller.
​    *value_ = queue.front();
​    queue.pop();
​    return true;
}
```


**总结**
_c指针，则是读写线程都可以操作，因此需要使⽤原⼦的CAS操作来修改，它的可能值有以下⼏种：

NULL：读线程设置，此时意味着已经没有数据可读，读线程在休眠。
⾮零：写线程设置，这⾥⼜区分两种情况：

- 旧值为_w的情况下，cas(_w,_f)操作修改为_f，意味着如果原先的值为_w，则原⼦性的修改为_f，表示有更多已被刷新的数据可读。
- 在旧值为NULL的情况下，此时读线程休眠，因此可以安全的设置为当前_f指针的位置。

- 写端yquque.write(count,false)；将f = &queue.back();
- 写端yquque.flush();如果c==w，则c=f;w=f;否则w=f;
- 读端check_read();如果c==queue.front则c=NULL否则r更新为f。


## 3.ZMQ无锁队列1写1读性能测试

这里分三种测试情况：

一次写就提交，read失败就usleep
10次写才提交，read失败就yield
flush失败就notify，read失败就wait
  可以看到用cond是效率是最高的，usleep的情况和yield的情况类似，实时性没有cond高。并且按照道理来说，正确的使用方法也是用cond

![在这里插入图片描述](https://img-blog.csdnimg.cn/b60915b2e493488c9450d0ddafe66b27.png)

  下面来看一看互斥锁队列 vs 互斥锁+条件变量队列 vs 内存屏障链表 vs RingBuffer CAS 实现。可以看到在一个写线程一个读线程的情况下，我们的ZMQ无锁队列是最快的。

![在这里插入图片描述](https://img-blog.csdnimg.cn/187d4391c2fa455e86929d3cfe9aeac5.png)

  那么在一写一读的场景下，我们就优先选用ZMQ无锁队列即可

![在这里插入图片描述](https://img-blog.csdnimg.cn/ad8bb3e0e99e4720b589be7b0efa4673.png)

## 4.如何实现多写多读的无锁队列？

后续的多写多读的无锁队列由下一篇文章循环数组无锁队列的原理与实现再来介绍。
————————————————
版权声明：本文为CSDN博主「cheems~」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_42956653/article/details/126284313

# 【NO.553】redis7.0源码阅读（四）：Redis中的IO多线程（线程池）

## 1.Redis中的IO多线程原理

![在这里插入图片描述](https://img-blog.csdnimg.cn/9d7df24f9cfe4cbc82f2637e470ba537.png)

服务端收到一条信息，给它deconde成一条命令
然后根据命令获得一个结果(reply)
然后将结果encode后，发送回去

![在这里插入图片描述](https://img-blog.csdnimg.cn/de3334ff73344778bbcae78f2817002c.png)

redis的单线程是指，命令执行(logic)都是在单线程中运行的
接受数据read和发送数据write都是可以在io多线程（线程池）中去运行

在Redis中，生产者也可以作为消费者，反之亦然，没有明确界限。

![在这里插入图片描述](https://img-blog.csdnimg.cn/4a24b096a61b464e80f0d25161c17e9b.png)

## 2.设置io多线程（调试设置）

在redis.conf中
设置io-threads-do-reads yes就可以开启io多线程
设置io-threads 2,设置为2（为了方便调试,真正使用的时候，可以根据需要设置），其中一个为主线程，另外一个是io线程

![在这里插入图片描述](https://img-blog.csdnimg.cn/196a1e5a5d3c4a4ab1bd5c03b4654ab2.png)

在networking.c中找到stopThreadedIOIfNeeded，如果在redis-cli中输入一条命令，是不会执行多线程的，因为它会判断，如果pending（需要做的命令）个数比io线程数少，就不会执行多线程
因此提前return 0，确保执行多线程,便于调试

```
int stopThreadedIOIfNeeded(void) {
    int pending = listLength(server.clients_pending_write);

    /* Return ASAP if IO threads are disabled (single threaded mode). */
    if (server.io_threads_num == 1) return 1;
    return 0;//为了调试，提前退出（自己添加的一行）
    if (pending < (server.io_threads_num*2)) {
        if (server.io_threads_active) stopThreadedIO();
        return 1;
    } else {
        return 0;
    }

}
```

到此为止，只需要，运行redis-server,在networking.c的 readQueryFromClient中打个断点，然后在redis-cli中输入任意set key value就可以进入io多线程，进行调试

下图可以看到箭头指向的两个线程，一个是主线程，另一个是io线程

![在这里插入图片描述](https://img-blog.csdnimg.cn/e5a5870258d048c1ad6920ba6b30d85f.png)

## 3.Redis中的IO线程池

### 3.1 读取任务readQueryFromClient

postponeClientRead(c)就是判断io多线程模式，并将任务添加到 任务队列中

```
void readQueryFromClient(connection *conn) { 
    client *c = connGetPrivateData(conn);
    int nread, big_arg = 0;
    size_t qblen, readlen;

    /* Check if we want to read from the client later when exiting from
     * the event loop. This is the case if threaded I/O is enabled. */
    if (postponeClientRead(c)) return; 
    //后面省略......

}
```

### 3.2 主线程将 待读客户端 添加到Read任务队列（生产者）postponeClientRead

如果是io多线程模式，那么将任务添加到任务队列。
（这个函数名的意思，延迟读，就是将任务加入到任务队列，后续去执行）

```
int postponeClientRead(client *c) {
    if (server.io_threads_active &&
        server.io_threads_do_reads &&
        !ProcessingEventsWhileBlocked &&
        !(c->flags & (CLIENT_MASTER|CLIENT_SLAVE|CLIENT_BLOCKED)) &&
        io_threads_op == IO_THREADS_OP_IDLE)
    {
        listAddNodeHead(server.clients_pending_read,c);//往任务队列中插入任务
        c->pending_read_list_node = listFirst(server.clients_pending_read);
        return 1;
    } else {
        return 0;
    }
}
```



### 3.3 多线程Read IO任务 handleClientsWithPendingReadsUsingThreads

基本原理和多线程Write IO是一样的，直接看多线程Write IO就行了。

```
其中processInputBuffer是解析协议

int handleClientsWithPendingReadsUsingThreads(void) {
    if (!server.io_threads_active || !server.io_threads_do_reads) return 0;
    int processed = listLength(server.clients_pending_read);
    if (processed == 0) return 0;

    /* Distribute the clients across N different lists. */
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_read,&li);
    int item_id = 0;
    
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;
    }
    
    /* Give the start condition to the waiting threads, by setting the
     * start condition atomic var. */
    io_threads_op = IO_THREADS_OP_READ;
    for (int j = 1; j < server.io_threads_num; j++) {
        int count = listLength(io_threads_list[j]);
        setIOPendingCount(j, count);
    }
    
    /* Also use the main thread to process a slice of clients. */
    listRewind(io_threads_list[0],&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        readQueryFromClient(c->conn);
    }
    listEmpty(io_threads_list[0]);
    
    /* Wait for all the other threads to end their work. */
    while(1) {
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += getIOPendingCount(j);
        if (pending == 0) break;
    }
    
    io_threads_op = IO_THREADS_OP_IDLE;
    
    /* Run the list of clients again to process the new buffers. */
    while(listLength(server.clients_pending_read)) {
        ln = listFirst(server.clients_pending_read);
        client *c = listNodeValue(ln);
        listDelNode(server.clients_pending_read,ln);
        c->pending_read_list_node = NULL;
    
        serverAssert(!(c->flags & CLIENT_BLOCKED));
    
        if (beforeNextClient(c) == C_ERR) {
            /* If the client is no longer valid, we avoid
             * processing the client later. So we just go
             * to the next. */
            continue;
        }
    
        /* Once io-threads are idle we can update the client in the mem usage buckets */
        updateClientMemUsageBucket(c);
    
        if (processPendingCommandsAndResetClient(c) == C_ERR) {
            /* If the client is no longer valid, we avoid
             * processing the client later. So we just go
             * to the next. */
            continue;
        }
    
        if (processInputBuffer(c) == C_ERR) {
            /* If the client is no longer valid, we avoid
             * processing the client later. So we just go
             * to the next. */
            continue;
        }
    
        /* We may have pending replies if a thread readQueryFromClient() produced
         * replies and did not install a write handler (it can't).
         */
        if (!(c->flags & CLIENT_PENDING_WRITE) && clientHasPendingReplies(c))
            clientInstallWriteHandler(c);
    }
    
    /* Update processed count on server */
    server.stat_io_reads_processed += processed;
    
    return processed;

}
```



### 3.4 多线程write IO任务（消费者）handleClientsWithPendingWritesUsingThreads

1.判断是否有必要开启IO多线程
2.如果没启动IO多线程，就启动IO多线程
3.负载均衡：write任务队列，均匀分给不同io线程
4.启动io子线程
5.主线程执行io任务
6.主线程等待io线程写结束

```
/* This function achieves thread safety using a fan-out -> fan-in paradigm:

 * Fan out: The main thread fans out work to the io-threads which block until

 * setIOPendingCount() is called with a value larger than 0 by the main thread.

 * Fan in: The main thread waits until getIOPendingCount() returns 0. Then

 * it can safely perform post-processing and return to normal synchronous

 * work. */
   int handleClientsWithPendingWritesUsingThreads(void) {
    int processed = listLength(server.clients_pending_write);
    if (processed == 0) return 0; /* Return ASAP if there are no clients. */

    /* If I/O threads are disabled or we have few clients to serve, don't

     * use I/O threads, but the boring synchronous code. */
       if (server.io_threads_num == 1 || stopThreadedIOIfNeeded()) {//判断是否有必要开启IO多线程
       return handleClientsWithPendingWrites();
        }

    /* Start threads if needed. */
    if (!server.io_threads_active) startThreadedIO();//开启io多线程

    /* Distribute the clients across N different lists. */
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_write,&li);//创建一个迭代器li，用于遍历任务队列clients_pending_write
    int item_id = 0;//默认是0，先分配给主线程去做（生产者也可能是消费者），如果设置成1，则先让io线程1去做
    //io_threads_list[0] 主线程
    //io_threads_list[1] io线程
    //io_threads_list[2] io线程   
    //io_threads_list[3] io线程   
    //io_threads_list[4] io线程
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);//取出一个任务
        c->flags &= ~CLIENT_PENDING_WRITE;

        /* Remove clients from the list of pending writes since
         * they are going to be closed ASAP. */
        if (c->flags & CLIENT_CLOSE_ASAP) {//表示该客户端的输出缓冲区超过了服务器允许范围,将在下一次循环进行一个关闭,也不返回任何信息给客户端，删除待读客户端
            listDelNode(server.clients_pending_write, ln);
            continue;
        }
       
        /* Since all replicas and replication backlog use global replication
         * buffer, to guarantee data accessing thread safe, we must put all
         * replicas client into io_threads_list[0] i.e. main thread handles
         * sending the output buffer of all replicas. */
        if (getClientType(c) == CLIENT_TYPE_SLAVE) {
            listAddNodeTail(io_threads_list[0],c);
            continue;
        }
        //负载均衡：将任务队列中的任务 添加 到不同的线程消费队列中去，每个线程就可以从当前线程的消费队列中取任务就行了
        //这样做的好处是，避免加锁。当前是在主线程中，进行分配任务
        //通过取余操作，将任务均分给不同io线程
        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;

    }

    /* Give the start condition to the waiting threads, by setting the

     * start condition atomic var. */
       io_threads_op = IO_THREADS_OP_WRITE;
        for (int j = 1; j < server.io_threads_num; j++) {
       int count = listLength(io_threads_list[j]);
       setIOPendingCount(j, count);//设置io线程启动条件，启动io线程
        }

    /* Also use the main thread to process a slice of clients. */
    listRewind(io_threads_list[0],&li);//让主线程去处理一部分任务（io_threads_list[0]）
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        writeToClient(c,0);
    }
    listEmpty(io_threads_list[0]);

    /* Wait for all the other threads to end their work. */
    while(1) {//剩下的任务io_threads_list[1]，io_threads_list[2].....给io线程去做，等待io线程完成任务
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += getIOPendingCount(j);//等待io线程结束，并返回处理的数量
        if (pending == 0) break;
    }

    io_threads_op = IO_THREADS_OP_IDLE;

    /* Run the list of clients again to install the write handler where

     * needed. */
       listRewind(server.clients_pending_write,&li);
        while((ln = listNext(&li))) {
       client *c = listNodeValue(ln);

       /* Update the client in the mem usage buckets after we're done processing it in the io-threads */
       updateClientMemUsageBucket(c);

       /* Install the write handler if there are pending writes in some

        * of the clients. */
          if (clientHasPendingReplies(c) &&
               connSetWriteHandler(c->conn, sendReplyToClient) == AE_ERR)
          {
           freeClientAsync(c);
          }
           }
           listEmpty(server.clients_pending_write);

    /* Update processed count on server */
    server.stat_io_writes_processed += processed;

    return processed;
   }



```


负载均衡：将任务队列中的任务 添加 到不同的线程消费队列中去，每个线程就可以从当前线程的消费队列中取任务就行了。这样做的好处是，避免加锁。当前是在主线程中，进行分配任务通过取余操作，将任务均分给不同的io线程。

## 4.线程调度

### 4.1 开启io线程startThreadedIO

每个io线程都有一把锁，如果主线程把锁还回去了，那么io线程就会启动，不再阻塞
并设置io线程标识为活跃状态io_threads_active=1

```
void startThreadedIO(void) {
    serverAssert(server.io_threads_active == 0);
    for (int j = 1; j < server.io_threads_num; j++)
        pthread_mutex_unlock(&io_threads_mutex[j]);
    server.io_threads_active = 1;
}
```



### 4.2 关闭io线程stopThreadedIO

每个io线程都有一把锁，如果主线程拿了，那么io线程就会阻塞等待，也就是停止了IO线程
并设置io线程标识为非活跃状态io_threads_active=0

   * ```
     void stopThreadedIO(void) {
         /* We may have still clients with pending reads when this function
     
        * is called: handle them before stopping the threads. */
          andleClientsWithPendingReadsUsingThreads();
              serverAssert(server.io_threads_active == 1);
              for (int j = 1; j < server.io_threads_num; j++)
          pthread_mutex_lock(&io_threads_mutex[j]);//
              server.io_threads_active = 0;
          }
     ```

     ————————————————
     版权声明：本文为CSDN博主「菊头蝙蝠」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
     原文链接：https://blog.csdn.net/qq_21539375/article/details/124670895

# 【NO.554】SQL之增删改查命令操作详解

## 1.CRUD

注意，操作数据库的时候为避免名称可能存在关键字的影响，最好使用反引号包含起来；这样MySQL在做词法语法分析的时候，就不会把其作为关键字进行分析。

### 1.1 创建数据库

语法：

```
CREATE DATABASE `数据库名` DEFAULT CHARACTER SET utf8;
```


示例：

```
create DATABASE `fly_test` DEFAULT CHARACTER set utf8;
```


执行信息：

```
create DATABASE `fly_test` DEFAULT CHARACTER set utf8

OK
时间: 0 秒
```



### 1.2 选择数据库

语法：

```
USE `数据库名`;
```


示例：

```
use fly_test;
```


执行信息：

```
use fly_test

OK
时间: 0 秒
```



### 1.3 删除数据库

语法：

```
DROP DATABASE `数据库名`;
```


示例：

```
DROP DATABASE `fly_test2`;
```


执行信息：

```
DROP DATABASE `fly_test2`

OK
时间: 0 秒
```



### 1.4 CRUD的五大约束

1.not null，即非空约束。
2.Autoincrement，即自增约束。可以指定初始值，没有指定默认是0；插入节点的时候会自增。这对事务的一致性非常重要。
3.unique，即唯一约束。
4.primary，即主键约束。设计表的时候，每张表都需要一个主键约束；就算没有设置，MySQL也会自动帮生成一个主键。
5.foreign，即外键约束。用于表与表之间的联动关系。

### 1.5 创建表

语法：

```
CREATE TABLE `table_name` (column_name column_type);
```


创建表的时候，要有列名称、列类型、约束。

示例：

```
CREATE TABLE IF NOT EXISTS `fly_table1` (
`id` INT UNSIGNED AUTO_INCREMENT COMMENT '编号',
`course` VARCHAR(100) NOT NULL COMMENT '课程',
`teacher` VARCHAR(40) NOT NULL COMMENT '讲师',
`price` DECIMAL(8,2) NOT NULL COMMENT '价格',
PRIMARY KEY ( `id` )
)ENGINE=innoDB DEFAULT CHARSET=utf8 COMMENT = '课程表';
```


ENGINE指定引擎，charset指定编码方式，COMMENT是注释。

执行信息：

```
CREATE TABLE IF NOT EXISTS `fly_table1` (
`id` INT UNSIGNED AUTO_INCREMENT COMMENT '编号',
`course` VARCHAR(100) NOT NULL COMMENT '课程',
`teacher` VARCHAR(40) NOT NULL COMMENT '讲师',
`price` DECIMAL(8,2) NOT NULL COMMENT '价格',
PRIMARY KEY ( `id` )
)ENGINE=innoDB DEFAULT CHARSET=utf8 COMMENT = '课程表'

OK
时间: 0.006 秒
```



### 1.6 删除数据表

#### 1.6.1 删除表

语法：

```
DROP TABLE `table_name`;
```


删除表结构及其表数据。

#### 1.6.2 清空数据表

语法：

```
TRUNCATE TABLE `table_name`; -- 截断表 以页为单位（至少有两行数据），有自增索引的话，从初始值开始累加
DELETE TABLE `table_name`; -- 逐行删除，有自增索引的话，从之前值继续累加
```


TRUNCATE TABLE：截断表 以页为单位（至少有两行数据），有自增索引的话，从初始值开始累加。
DELETE TABLE ： 逐行删除，有自增索引的话，从之前值继续累加

#### 1.6.3 区别

![image-20230225152833323](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230225152833323.png)
drop和truncate会阻塞其他操作。只有停机维护状态才使用这两个命令。

![image-20230225152850505](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230225152850505.png)
因为delete是以行为单位，逐行删除的；而drop是以页（8K)为单位进行删除的；所以delete比drop慢。

### 1.7 增

语法：

```
INSERT INTO `table_name`(`field1`, `field2`, ...,`fieldn`) VALUES (value1, value2, ..., valuen);
```


如果列设置了自增组件，那么操作时可以不列出来，系统会将其自增的。

示例：

```
INSERT INTO fly_table1 (`course`,`teacher`,`price`) VALUES ('linux C/C++','fly',0.0);
```


执行信息：

```
INSERT INTO fly_table1 (`course`,`teacher`,`price`) VALUES ('linux C/C++','fly',0.0)

Affected rows: 1
时间: 0.046 秒
```

### 1.8 删

语法：

```
DELETE FROM `table_name` [WHERE Clause];
```


示例：

```
DELETE FROM `fly_table1` where id = 2;
```


执行信息：

```
DELETE FROM `fly_table1` where id = 2

Affected rows: 1
时间: 0.001 秒
```

### 1.9 改

语法：

```
UPDATE table_name SET field1=new_value1,field2=new_value2 [, fieldn=new_valuen]
```


示例：

```
UPDATE `fly_table1` SET price=price+100,course='linux MySQL' WHERE id =3;
```


执行信息：

```
UPDATE `fly_table1` SET price=price+100,course='linux MySQL' WHERE id =3

Affected rows: 1
时间: 0.003 秒
```



### 1.10 查

语法：

```
SELECT field1, field2,...fieldN FROM table_name [WHERE Clause]
```


实际使用中最好不要使用select *的方式。

示例：

```
SELECT price FROM fly_table1 WHERE course='linux C/C++';
```


执行信息：

```
SELECT price FROM fly_table1 WHERE course='linux C/C++'

OK
时间: 0.001 秒
```

### 1.11 去重的方式

（1）group by column。
（2）select distinct column。

### 1.12 条件判断类型

（1）… where condition。
（2）group by column having condition。
（3）… join … on condition。

其中，condition是条件，column是列名。

## 2.高级查询

做一下准备，为下面的查询操作建立数据库。

```
DROP TABLE IF EXISTS `class`;
CREATE TABLE `class` (
`cid` int(11) NOT NULL AUTO_INCREMENT,
`caption` varchar(32) NOT NULL,
PRIMARY KEY (`cid`)
) ENGINE=innoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;



DROP TABLE IF EXISTS `student`;
CREATE TABLE `student` (
`sid` int(11) NOT NULL AUTO_INCREMENT,
`gender` char(1) NOT NULL,
`class_id` int(11) NOT NULL,
`sname` varchar(32) NOT NULL,
PRIMARY KEY (`sid`),
KEY `fk_class` (`class_id`),
CONSTRAINT `fk_class` FOREIGN KEY (`class_id`) REFERENCES `class` (`cid`)
) ENGINE=innoDB AUTO_INCREMENT=17 DEFAULT CHARSET=utf8;


DROP TABLE IF EXISTS `teacher`;
CREATE TABLE `teacher` (
`tid` int(11) NOT NULL AUTO_INCREMENT,
`tname` varchar(32) NOT NULL,
PRIMARY KEY (`tid`)
) ENGINE=innoDB AUTO_INCREMENT=6 DEFAULT CHARSET=utf8;


DROP TABLE IF EXISTS `course`;
CREATE TABLE `course` (
`cid` int(11) NOT NULL AUTO_INCREMENT,
`cname` varchar(32) NOT NULL,
`teacher_id` int(11) NOT NULL,
PRIMARY KEY (`cid`),
KEY `fk_course_teacher` (`teacher_id`),
CONSTRAINT `fk_course_teacher` FOREIGN KEY (`teacher_id`) REFERENCES `teacher` (`tid`)
) ENGINE=innoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;

DROP TABLE IF EXISTS `score`;
CREATE TABLE `score` (
`sid` int(11) NOT NULL AUTO_INCREMENT,
`student_id` int(11) NOT NULL,
`course_id` int(11) NOT NULL,
`num` int(11) NOT NULL,
PRIMARY KEY (`sid`),
KEY `fk_score_student` (`student_id`),
KEY `fk_score_course` (`course_id`),
CONSTRAINT `fk_score_course` FOREIGN KEY (`course_id`) REFERENCES `course` (`cid`),
CONSTRAINT `fk_score_student` FOREIGN KEY (`student_id`) REFERENCES `student` (`sid`)
) ENGINE=innoDB AUTO_INCREMENT=53 DEFAULT CHARSET=utf8;
```


innoDB 有外键约束，myisam 注释的作用。

### 2.1 基础查询

```
-- 全部查询
SELECT * FROM student;
-- 只查询部分字段
SELECT `sname`, `class_id` FROM student;
-- 别名 列明 不要用关键字
SELECT `sname` AS '姓名' , `class_id` AS '班级ID'
FROM student;
-- 把查询出来的结果的重复记录去掉
SELECT distinct `class_id` FROM student;
```

### 2.2 条件查询

```
-- 查询姓名为 fly1 的学生信息
SELECT * FROM `student` WHERE `sname` = 'fly1';
-- 查询性别为 男，并且班级为 2 的学生信息
SELECT * FROM `student` WHERE `gender`="男" AND `class_id`=5;
```

### 2.3 范围查询

```
-- 查询班级id 1 到 3 的学生的信息
SELECT * FROM `student` WHERE `class_id` BETWEEN 1 AND 3;
```



### 2.4 判空查询

is null 判断造成索引失效。

```
# 索引 B+ 树

SELECT * FROM `student` WHERE `class_id` IS NOT NULL; #判断不为空
SELECT * FROM `student` WHERE `class_id` IS NULL;#判断为空


SELECT * FROM `student` WHERE `gender` <> '';
#判断不为空字符串
SELECT * FROM `student` WHERE `gender` = '';
#判断为空字符串
```



### 2.5 模糊查询

使用 like关键字，"%"代表任意数量的字符，”_”代表占位符。

```
-- 查询名字为 m 开头的学生的信息
SELECT * FROM `teacher` WHERE `tname` LIKE '谢%';
-- 查询姓名里第二个字为 小 的学生的信息
SELECT * FROM `teacher` WHERE `tname` LIKE '_小%';
```



### 2.6 分页查询

分页查询主要用于查看第N条 到 第M条的信息，通常和排序查询一起使用。

使用limit关键字，第一个参数表示从条记录开始显示，第二个参数表示要显示的数目。表中默认第一条记录的参数为0。

```
-- 查询第二条到第三条内容
SELECT * FROM `student` LIMIT 1,2;
```



### 2.7 查询后排序

关键字：order by field。
asc:升序。
desc:降序

```
SELECT * FROM `score` ORDER BY `num` ASC;
-- 按照多个字段排序
SELECT * FROM `score` ORDER BY `course_id` DESC,`num` DESC;
```

### 2.8 聚合查询

![image-20230225153905600](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230225153905600.png)

```
SELECT sum(`num`) FROM `score`;
SELECT avg(`num`) FROM `score`;
SELECT max(`num`) FROM `score`;
SELECT min(`num`) FROM `score`;
SELECT count(`num`) FROM `score`;
```



### 2.9 分组查询

分组加group_concat。

```
-- 分组加group_concat
SELECT `gender`, group_concat(`age`) as ages FROM `student` GROUP BY `gender`;
-- 可以把查询出来的结果根据某个条件来分组显示
SELECT `gender` FROM `student` GROUP BY `gender`;
-- 分组加聚合
SELECT `gender`, count(*) as num FROM `student` GROUP BY `gender`;
-- 分组加条件
SELECT `gender`, count(*) as num FROM `student` GROUP BY `gender` HAVING num > 6;
```

## 3.联表查询

可以把表想象成集合。
联表查询分为内联（inner join）和外联（left join，right join，full join）。内联类似交集，full join类似并集。
如果只写了join，那么默认是内联。

![sql_joins](https://img-blog.csdnimg.cn/10576e8519514a7c84c4d1c23ece2cd7.png)

### 3.1 INNER JOIN

只取两张表有对应关系的记录。

```
SELECT cid FROM `course` INNER JOIN `teacher` ON course.teacher_id = teacher.tid;
```



### 3.2 LEFT JOIN

在内连接的基础上保留左表没有对应关系的记录。

```
select course.cid from `course` left join `teacher` on course.teacher_id = teacher.tid;
```



### 3.3 RIGHT JOIN

在内连接的基础上保留右表没有对应关系的记录。

```
select course.cid from `course` right join `teacher` on course.teacher_id = teacher.tid;
```

## 4.子查询/合并查询

### 4.1 单行子查询

```
select * from course where teacher_id = (select tid from teacher where tname = 'lucien')
```



### 4.2 多行子查询

多行子查询即返回多行记录的子查询。

IN 关键字：运算符可以检测结果集中是否存在某个特定的值，如果检测成功就执行外部的查询。
EXISTS 关键字：内层查询语句不返回查询的记录。而是返回一个真假值。如果内层查询语句查询到满足条件的记录，就返回一个真值（ true ），否则，将返回一个假值（ false ）。当返回的值为 true 时，外层查询语句将进行查询；当返回的为false 时，外层查询语句不进行查询或者查询不出任何记录。
ALL 关键字：表示满足所有条件。使用 ALL 关键字时，只有满足内层查询语句返回的所有结果，才可以执行外层查询语句。
ANY 关键字：允许创建一个表达式，对子查询的返回值列表，进行比较，只要满足内层子查询中的，任意一个比较条件，就返回一个结果作为外层查询条件。

在 FROM 子句中使用子查询：子查询出现在 from 子句中，这种情况下将子查询当做一个临时表使用。

示例：

```
select * from student where class_id in (select cid from course where teacher_id = 6);

select * from student where exists(select cid from course where cid = 5);


select student_id,sname FROM (SELECT * FROM score WHERE course_id = 5 OR course_id = 2) AS A LEFT JOIN student ON A.student_id = student.sid;
```

### 4.3 正则表达式

使用关键字：REGEXP。

![image-20230225154145269](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230225154145269.png)

```
SELECT * FROM `teacher` WHERE `tname` REGEXP '^long';
```



## 5.总结

1.在实际使用中最好不要使用select *的方式查询数据，这种查询方式既不好分析数据，也会使查询效率降低。
2.SQL查询中，如果不清楚名称是不是关键字，最好使用反引号括起来，避免在词法语法分析时被当成关键字处理。
3.group by会去重，group_concat会分组。
————————————————
版权声明：本文为CSDN博主「Lion Long」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/Long_xu/article/details/127545941

# 【NO.555】数据库设计的三范式和反范式

## 1.范式的概念

为了建立冗余较小、结构合理的数据库，设计数据库时必须遵循一定的规则。在关系型数据库中这种规则就称为范式。范式是符合某一种设计要求的总结。要想设计一个结构合理的关系型数据库，必须满足一定的范式。

三范式和反范式是空间和时间的关系。三范式是为了降低空间；反范式是通过增加空间来提升运行效率。

## 2.三范式

（1）目的：减少空间占用。
（2）内容：列不可分、依赖主键（联合索引）、在依赖主键（联合索引）的基础上直接依赖。

### 2.1 范式一

确保每列保持原子性；数据库表中的所有字段都是不可分解的原子值。
例如：某表中有一个地址字段，如果经常需要访问地址字段中的城市属性，则需要将该字段拆分为多个字段，省份、城市、详细地址等。

![image-20230225155314101](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230225155314101.png)
可以才分为：

![image-20230225155327216](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230225155327216.png)

### 2.2 范式二

确保表中的每列都和主键相关，而不能只与主键的某一部分相关（组合索引)。

![image-20230225155346455](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230225155346455.png)
因为订单编号与客户的信息相关，订单编号和商品编号一起唯一确定数量，商品编号和商品信息相关；所以可以拆分成三个表：

![image-20230225155359192](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230225155359192.png)

### 2.3 范式三

确保每列都和主键直接相关，而不是间接相关；减少数据冗余。

例如

![image-20230225155412047](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230225155412047.png)
可以拆分为：

![image-20230225155432650](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230225155432650.png)

## 3.反范式

反范式是经常使用的设计。比如用户表采用的就是反范式，因为如果用户表不采用反范式设计，将会产生很多的关联关系表，这就会涉及到联表查询，非常影响效率，特别对登录来说，是不可容忍的。

因此，反范式允许冗余存储，为了提升查询效率。

## 4.总结

范式二中，对于联合索引，主键不能依赖一部分，而要依赖整体；出现重复的要拆分。
反范式是经常使用的设计。三范式可以避免数据冗余，减少数据库的空间，减小维护数据完整性的麻烦。但是采用数据库范式化设计，可能导致数据库业务涉及的表变多，并且造成更多的联表查询，将导致整个系统的性能降低。因此处于性能考虑，可能需要进行反范式设计。
————————————————
版权声明：本文为CSDN博主「Lion Long」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/Long_xu/article/details/127532598

# 【NO.556】基于C++11实现的高效线程池及工作原理

## 1.简介

线程池（thread pool）：一种线程的使用模式，线程过多会带来调度开销，进而影响缓存局部性和整体性能。而线程池维护着多个线程，等待着监督管理者分配可并发执行的任务。这避免了在处理短时间任务时创建与销毁线程的代价。线程池不仅能够保证内核的充分利用，还能防止过分调度。可用线程数量应该取决于可用的并发处理器、处理器内核、内存、网络sockets等的数量。

## 2.线程池的组成

### 2.1 线程池管理器

创建一定数量的线程，启动线程，调配任务，管理着线程池。
本篇线程池目前只需要启动(start())，停止方法(stop())，及任务添加方法(addTask).
start()创建一定数量的线程池,进行线程循环.
stop()停止所有线程循环，回收所有资源.
addTask()添加任务.

### 2.2 工作线程

线程池中线程，在线程池中等待并执行分配的任务.
本篇选用条件变量实现等待与通知机制.

### 2.3 任务接口，

添加任务的接口,以供工作线程调度任务的执行。

### 2.4 任务队列

用于存放没有处理的任务。提供一种缓冲机制
同时任务队列具有调度功能，高优先级的任务放在任务队列前面;本篇选用priority_queue 与pair的结合用作任务优先队列的结构.

## 3.线程池工作的四种情况.

假设我们的线程池大小为3，任务队列目前不做大小限制.

### 3.1 主程序当前没有任务要执行，线程池中的任务队列为空闲状态.

此情况下所有工作线程处于空闲的等待状态，任务缓冲队列为空.

![img](https://pic2.zhimg.com/80/v2-8bcc65c1c14b27769b3375b43fb381d9_720w.webp)

### 3.2 主程序添加小于等于线程池中线程数量的任务.

此情况基于情形1，所有工作线程已处在等待状态，主线程开始添加三个任务，添加后通知(notif())唤醒线程池中的线程开始取(take())任务执行. 此时的任务缓冲队列还是空。

![img](https://pic1.zhimg.com/80/v2-cb23d5bbfa2c11fc439ba1214abecf24_720w.webp)

### 3.3 主程序添加任务数量大于当前线程池中线程数量的任务.

此情况发生情形2后面，所有工作线程都在工作中，主线程开始添加第四个任务，添加后发现现在线程池中的线程用完了,于是存入任务缓冲队列。工作线程空闲后主动从任务队列取任务执行.

![img](https://pic4.zhimg.com/80/v2-57e567f4a0af1aada913dbfa1f19f8b3_720w.webp)

### 3.4 主程序添加任务数量大于当前线程池中线程数量的任务,且任务缓冲队列已满.

此情况发生情形3且设置了任务缓冲队列大小后面，主程序添加第N个任务，添加后发现池子中的线程用完了，任务缓冲队列也满了，于是进入等待状态、等待任务缓冲队列中的任务腾空通知。
但是要注意这种情形会阻塞主线程,本篇暂不限制任务队列大小,必要时再来优化.

![img](https://pic3.zhimg.com/80/v2-e96db4030a3766c27a66093c9934fc0e_720w.webp)

## 4.实现

等待通知机制通过条件变量实现，Logger和CurrentThread,用于调试，可以无视.

```text
#ifndef _THREADPOOL_HH
#define _THREADPOOL_HH

#include <vector>
#include <utility>
#include <queue>
#include <thread>
#include <functional>
#include <mutex>

#include "Condition.hh"

class ThreadPool{
public:
  static const int kInitThreadsSize = 3;
  enum taskPriorityE { level0, level1, level2, };
  typedef std::function<void()> Task;
  typedef std::pair<taskPriorityE, Task> TaskPair;

  ThreadPool();
  ~ThreadPool();

  void start();
  void stop();
  void addTask(const Task&);
  void addTask(const TaskPair&);

private:
  ThreadPool(const ThreadPool&);//禁止复制拷贝.
  const ThreadPool& operator=(const ThreadPool&);

  struct TaskPriorityCmp
  {
    bool operator()(const ThreadPool::TaskPair p1, const ThreadPool::TaskPair p2)
    {
        return p1.first > p2.first; //first的小值优先
    }
  };

  void threadLoop();
  Task take();

  typedef std::vector<std::thread*> Threads;
  typedef std::priority_queue<TaskPair, std::vector<TaskPair>, TaskPriorityCmp> Tasks;

  Threads m_threads;
  Tasks m_tasks;

  std::mutex m_mutex;
  Condition m_cond;
  bool m_isStarted;
};

#endif

//Cpp

#include <assert.h>

#include "Logger.hh" // debug
#include "CurrentThread.hh" // debug
#include "ThreadPool.hh"

ThreadPool::ThreadPool()
  :m_mutex(),
  m_cond(m_mutex),
  m_isStarted(false)
{

}

ThreadPool::~ThreadPool()
{
  if(m_isStarted)
  {
    stop();
  }
}

void ThreadPool::start()
{
  assert(m_threads.empty());
  m_isStarted = true;
  m_threads.reserve(kInitThreadsSize);
  for (int i = 0; i < kInitThreadsSize; ++i)
  {
    m_threads.push_back(new std::thread(std::bind(&ThreadPool::threadLoop, this)));
  }

}

void ThreadPool::stop()
{
  LOG_TRACE << "ThreadPool::stop() stop.";
  {
    std::unique_lock<std::mutex> lock(m_mutex);
    m_isStarted = false;
    m_cond.notifyAll();
    LOG_TRACE << "ThreadPool::stop() notifyAll().";
  }

  for (Threads::iterator it = m_threads.begin(); it != m_threads.end() ; ++it)
  {
    (*it)->join();
    delete *it;
  }
  m_threads.clear();
}


void ThreadPool::threadLoop()
{
  LOG_TRACE << "ThreadPool::threadLoop() tid : " << CurrentThread::tid() << " start.";
  while(m_isStarted)
  {
    Task task = take();
    if(task)
    {
      task();
    }
  }
  LOG_TRACE << "ThreadPool::threadLoop() tid : " << CurrentThread::tid() << " exit.";
}

void ThreadPool::addTask(const Task& task)
{
  std::unique_lock<std::mutex> lock(m_mutex);
  /*while(m_tasks.isFull())
    {//when m_tasks have maxsize
      cond2.wait();
    }
  */
  TaskPair taskPair(level2, task);
  m_tasks.push(taskPair);
  m_cond.notify();
}

void ThreadPool::addTask(const TaskPair& taskPair)
{
  std::unique_lock<std::mutex> lock(m_mutex);
  /*while(m_tasks.isFull())
    {//when m_tasks have maxsize
      cond2.wait();
    }
  */
  m_tasks.push(taskPair);
  m_cond.notify();
}

ThreadPool::Task ThreadPool::take()
{
  std::unique_lock<std::mutex> lock(m_mutex);
  //always use a while-loop, due to spurious wakeup
  while(m_tasks.empty() && m_isStarted)
  {
    LOG_TRACE << "ThreadPool::take() tid : " << CurrentThread::tid() << " wait.";
    m_cond.wait(lock);
  }

  LOG_TRACE << "ThreadPool::take() tid : " << CurrentThread::tid() << " wakeup.";

  Task task;
  Tasks::size_type size = m_tasks.size();
  if(!m_tasks.empty() && m_isStarted)
  {
    task = m_tasks.top().second;
    m_tasks.pop();
    assert(size - 1 == m_tasks.size());
    /*if (TaskQueueSize_ > 0)
    {
      cond2.notify();
    }*/
  }

  return task;

}
```

## 5.测试程序

### 5.1 start() 、stop()

测试线程池基本的创建退出工作，及检测资源是否正常回收.

```text
int main()
{
  {
  ThreadPool threadPool;
  threadPool.start();

  getchar();
  }

  getchar();

  return 0;
}
./test.out 
2021-10-11 16:50:36.054805 [TRACE] [ThreadPool.cpp:53] [threadLoop] ThreadPool::threadLoop() tid : 3680 start.
2021-10-11 16:50:36.054855 [TRACE] [ThreadPool.cpp:72] [take] ThreadPool::take() tid : 3680 wait.
2021-10-11 16:50:36.055633 [TRACE] [ThreadPool.cpp:53] [threadLoop] ThreadPool::threadLoop() tid : 3679 start.
2021-10-11 16:50:36.055676 [TRACE] [ThreadPool.cpp:72] [take] ThreadPool::take() tid : 3679 wait.
2021-10-11 16:50:36.055641 [TRACE] [ThreadPool.cpp:53] [threadLoop] ThreadPool::threadLoop() tid : 3681 start.
2021-10-11 16:50:36.055701 [TRACE] [ThreadPool.cpp:72] [take] ThreadPool::take() tid : 3681 wait.
2021-10-11 16:50:36.055736 [TRACE] [ThreadPool.cpp:53] [threadLoop] ThreadPool::threadLoop() tid : 3682 start.
2021-10-11 16:50:36.055746 [TRACE] [ThreadPool.cpp:72] [take] ThreadPool::take() tid : 3682 wait.

2021-10-11 16:51:01.411792 [TRACE] [ThreadPool.cpp:36] [stop] ThreadPool::stop() stop.
2021-10-11 16:51:01.411863 [TRACE] [ThreadPool.cpp:39] [stop] ThreadPool::stop() notifyAll().
2021-10-11 16:51:01.411877 [TRACE] [ThreadPool.cpp:76] [take] ThreadPool::take() tid : 3680 wakeup.
2021-10-11 16:51:01.411883 [TRACE] [ThreadPool.cpp:62] [threadLoop] ThreadPool::threadLoop() tid : 3680 exit.
2021-10-11 16:51:01.412062 [TRACE] [ThreadPool.cpp:76] [take] ThreadPool::take() tid : 3682 wakeup.
2021-10-11 16:51:01.412110 [TRACE] [ThreadPool.cpp:62] [threadLoop] ThreadPool::threadLoop() tid : 3682 exit.
2021-10-11 16:51:01.413052 [TRACE] [ThreadPool.cpp:76] [take] ThreadPool::take() tid : 3679 wakeup.
2021-10-11 16:51:01.4130982021-10-11 [TRACE] [ThreadPool.cpp:62] [threadLoop] ThreadPool::threadLoop() tid : 3679 exit.
2021-10-11 16:51:01.413112 [TRACE] [ThreadPool.cpp:76] [take] ThreadPool::take() tid : 3681 wakeup.
2021-10-11 16:51:01.413141 [TRACE] [ThreadPool.cpp:62] [threadLoop] ThreadPool::threadLoop() tid : 3681 exit.
```

### 5.2 addTask()、PriorityTaskQueue

测试添加任务接口，及优先任务队列.

主线程首先添加了5个普通任务、 1s后添加一个高优先级任务，当前3个线程中的最先一个空闲后，会最先执行后面添加的priorityFunc().

```text
std::mutex g_mutex;

void priorityFunc()
{
  for (int i = 1; i < 4; ++i)
  {
      std::this_thread::sleep_for(std::chrono::seconds(1));
      std::lock_guard<std::mutex> lock(g_mutex);
      LOG_DEBUG << "priorityFunc() [" << i << "at thread [ " << CurrentThread::tid() << "] output";// << std::endl;
  }

}

void testFunc()
{
  // loop to print character after a random period of time
  for (int i = 1; i < 4; ++i)
  {
      std::this_thread::sleep_for(std::chrono::seconds(1));
      std::lock_guard<std::mutex> lock(g_mutex);
      LOG_DEBUG << "testFunc() [" << i << "] at thread [ " << CurrentThread::tid() << "] output";// << std::endl;
  }

}


int main()
{
  ThreadPool threadPool;
  threadPool.start();

  for(int i = 0; i < 5 ; i++)
    threadPool.addTask(testFunc);

  std::this_thread::sleep_for(std::chrono::seconds(1));

  threadPool.addTask(ThreadPool::TaskPair(ThreadPool::level0, priorityFunc));

  getchar();
  return 0;
}
./test.out 
2021-10-11 18:24:20.886837 [TRACE] [ThreadPool.cpp:56] [threadLoop] ThreadPool::threadLoop() tid : 4121 start.
2021-10-11 18:24:20.886893 [TRACE] [ThreadPool.cpp:103] [take] ThreadPool::take() tid : 4121 wakeup.
2021-10-11 18:24:20.887580 [TRACE] [ThreadPool.cpp:56] [threadLoop] ThreadPool::threadLoop() tid : 4120 start.
2021-10-11 18:24:20.887606 [TRACE] [ThreadPool.cpp:103] [take] ThreadPool::take() tid : 4120 wakeup.
2021-10-11 18:24:20.887610 [TRACE] [ThreadPool.cpp:56] [threadLoop] ThreadPool::threadLoop() tid : 4122 start.
2021-10-11 18:24:20.887620 [TRACE] [ThreadPool.cpp:103] [take] ThreadPool::take() tid : 4122 wakeup.
2021-10-11 18:24:21.887779 [DEBUG] [main.cpp:104] [testFunc] testFunc() [1] at thread [ 4120] output
2021-10-11 18:24:21.887813 [DEBUG] [main.cpp:104] [testFunc] testFunc() [1] at thread [ 4122] output
2021-10-11 18:24:21.888909 [DEBUG] [main.cpp:104] [testFunc] testFunc() [1] at thread [ 4121] output
2021-10-11 18:24:22.888049 [DEBUG] [main.cpp:104] [testFunc] testFunc() [2] at thread [ 4120] output
2021-10-11 18:24:22.888288 [DEBUG] [main.cpp:104] [testFunc] testFunc() [2] at thread [ 4122] output
2021-10-11 18:24:22.889978 [DEBUG] [main.cpp:104] [testFunc] testFunc() [2] at thread [ 4121] output
2021-10-11 18:24:23.888467 [DEBUG] [main.cpp:104] [testFunc] testFunc() [3] at thread [ 4120] output
2021-10-11 18:24:23.888724 [TRACE] [ThreadPool.cpp:103] [take] ThreadPool::take() tid : 4120 wakeup.
2021-10-11 18:24:23.888778 [DEBUG] [main.cpp:104] [testFunc] testFunc() [3] at thread [ 4122] output
2021-10-11 18:24:23.888806 [TRACE] [ThreadPool.cpp:103] [take] ThreadPool::take() tid : 4122 wakeup.
2021-10-11 18:24:23.890413 [DEBUG] [main.cpp:104] [testFunc] testFunc() [3] at thread [ 4121] output
2021-10-11 18:24:23.890437 [TRACE] [ThreadPool.cpp:103] [take] ThreadPool::take() tid : 4121 wakeup.
2021-10-11 18:24:24.889247 [DEBUG] [main.cpp:92] [priorityFunc] priorityFunc() [1at thread [ 4120] output
2021-10-11 18:24:24.891187 [DEBUG] [main.cpp:104] [testFunc] testFunc() [1] at thread [ 4121] output
2021-10-11 18:24:24.893163 [DEBUG] [main.cpp:104] [testFunc] testFunc() [1] at thread [ 4122] output
2021-10-11 18:24:25.889567 [DEBUG] [main.cpp:92] [priorityFunc] priorityFunc() [2at thread [ 4120] output
2021-10-11 18:24:25.891477 [DEBUG] [main.cpp:104] [testFunc] testFunc() [2] at thread [ 4121] output
2021-10-11 18:24:25.893450 [DEBUG] [main.cpp:104] [testFunc] testFunc() [2] at thread [ 4122] output
2021-10-11 18:24:26.890295 [DEBUG] [main.cpp:92] [priorityFunc] priorityFunc() [3at thread [ 4120] output
2021-10-11 18:24:26.890335 [TRACE] [ThreadPool.cpp:99] [take] ThreadPool::take() tid : 4120 wait.
2021-10-11 18:24:26.892265 [DEBUG] [main.cpp:104] [testFunc] testFunc() [3] at thread [ 4121] output
2021-10-11 18:24:26.892294 [TRACE] [ThreadPool.cpp:99] [take] ThreadPool::take() tid : 4121 wait.
2021-10-11 18:24:26.894274 [DEBUG] [main.cpp:104] [testFunc] testFunc() [3] at thread [ 4122] output
2021-10-11 18:24:26.894299 [TRACE] [ThreadPool.cpp:99] [take] ThreadPool::take() tid : 4122 wait.

2021-10-11 18:24:35.359003 [TRACE] [ThreadPool.cpp:37] [stop] ThreadPool::stop() stop.
2021-10-11 18:24:35.359043 [TRACE] [ThreadPool.cpp:42] [stop] ThreadPool::stop() notifyAll().
2021-10-11 18:24:35.359061 [TRACE] [ThreadPool.cpp:103] [take] ThreadPool::take() tid : 4120 wakeup.
2021-10-11 18:24:35.359067 [TRACE] [ThreadPool.cpp:65] [threadLoop] ThreadPool::threadLoop() tid : 4120 exit.
2021-10-11 18:24:35.359080 [TRACE] [ThreadPool.cpp:103] [take] ThreadPool::take() tid : 4122 wakeup.
2021-10-11 18:24:35.359090 [TRACE] [ThreadPool.cpp:65] [threadLoop] ThreadPool::threadLoop() tid : 4122 exit.
2021-10-11 18:24:35.359123 [TRACE] [ThreadPool.cpp:103] [take] ThreadPool::take() tid : 4121 wakeup.
2021-10-11 18:24:35.359130 [TRACE] [ThreadPool.cpp:65] [threadLoop] ThreadPool::threadLoop() tid : 4121 exit.
```

原文地址：https://zhuanlan.zhihu.com/p/421440660

作者：linux

# 【NO.557】Linux内存管理-详解mmap原理

## **1. 一句话概括mmap**

mmap的作用，在应用这一层，是让你把文件的某一段，当作内存一样来访问。将文件映射到物理内存，将进程虚拟空间映射到那块内存。

这样，进程不仅能像访问内存一样读写文件，多个进程映射同一文件，还能保证虚拟空间映射到同一块物理内存，达到内存共享的作用。

## **2.** 虚拟内存**？虚拟空间？**

其实是一个概念，前一篇对于这个词没有确切的定义，现在定义一下：

虚拟空间就是进程看到的所有地址组成的空间，虚拟空间是某个进程对分配给它的所有物理地址（已经分配的和将会分配的）的重新映射。

而虚拟内存，为啥叫虚拟内存，是因为它就不是真正的内存，是假的，因为它是由地址组成的空间，所以在这里，使用虚拟空间这个词更加确切和易懂。（不过虚拟内存这个词也不算错）

### **2.1 虚拟空间原理**

#### **2.1.1物理内存**

首先，物理地址实际上也不是连续的，通常是包含作为主存的DRAM和IO寄存器

![img](https://pic4.zhimg.com/80/v2-acf9edf723cc9fec950f48a4bf72158f_720w.webp)

以前的CPU（如X86）是为IO划分单独的地址空间，所以不能用直接访问内存的方式（如指针）IO，只能用专门的方法（in/read/out/write）诸如此类。

现在的CPU利用PCI总线将IO寄存器映射到物理内存，所以出现了基于内存访问的IO。

还有一点补充的，就如同进程空间有一块内核空间一样，物理内存也会有极小一部分是不能访问的，为内核所用。

#### **2.1.2三个总线**

这里再补充下三个总线的知识，即：地址总线、数据总线、控制总线

- 地址总线，用来传输地址
- 数据总线，用来传输数据
- 控制总线，用来传输命令

比如CPU通过控制总线发送读取命令，同时用地址总线发送要读取的数据虚地址，经过MMU后到内存

内存通过数据总线将数据传输给CPU。

虚拟地址的空间和指令集的地址长度有关，不一定和物理地址长度一致，比如现在的64位处理器，从VA角度看来，可以访问64位的地址，但地址总线长度只有48位，所以你可以访问一个位于2^52这个位置的地址。

#### **2.1.3虚拟内存地址转换（虚地址转实地址）**

上面已经明确了虚拟内存是虚拟空间，即地址的集合这一概念。基于此，来说说原理。

如果还记得操作系统课程里面提到的虚地址，那么这个虚地址就是虚拟空间的地址了，虚地址通过转换得到实地址，转换方式课程内也讲得很清楚，虚地址头部包含了页号（段地址和段大小，看存储模式： 页存储、段存储，段页式），剩下部分是偏移量，经过MMU转换成实地址。

![img](https://pic3.zhimg.com/80/v2-77aad2f066c524ae9e3e12123f54ea42_720w.webp)

存储方式

![img](https://pic3.zhimg.com/80/v2-210e67b7c2b59e0c583e3c9f0c3221de_720w.webp)

如图则是页式存储动态地址变换的方式

虚拟地址头部为页号通过查询页表得到物理页号，假设一页时1K，那么页号*偏移量就得到物理地址

![img](https://pic4.zhimg.com/80/v2-135c6be0231c3323b0a3aa5a315d4447_720w.webp)

如图所示，段式存储

虚拟地址头部为段号，段表中找到段基地址加上偏移量得到实地址

![img](https://pic2.zhimg.com/80/v2-940e5377430e3cb2d64f3afea2a20489_720w.webp)

段页式结合两者，如图所示。

## **3. mmap映射**

至此，如果对虚拟空间已经了解了，那么接下来，作为coder，应该自动把虚拟空间无视掉，因为Linux的目的也是要让更多额进程能享用内存，又不让进程做麻烦的事情，是将虚拟空间和MMU都透明化，让进程（和coder）只需要管对内存怎样使用。

所以现在开始不再强调虚拟空间了。

mmap就是将文件映射到内存上，进程直接对内存进行读写，然后就会反映到磁盘上。

![img](https://pic2.zhimg.com/80/v2-888968f6d713ccdcb5837e2ec85d5565_720w.webp)

- 虚拟空间获取到一段连续的地址
- 在没有读写的时候，这个地址指向不存在的地方（所以，上图中起始地址和终止地址是还没分配给 进程的）
- 好了，根据偏移量，进程要读文件数据了，数据占在两个页当中（物理内存着色部分）
- 这时，进程开始使用内存了，所以OS给这两个页分配了内存（即缺页异常）（其余部分还是没有分配）
- 然后刚分配的页内是空的，所以再将相同偏移量的文件数据拷贝到物理内存对应页上。

原文地址：https://zhuanlan.zhihu.com/p/465336136

作者：linux

# 【NO.558】通过实战理解CPU上下文切换

Linux是一个多任务的操作系统，可以支持远大于CPU数量的任务同时运行，但是我们都知道这其实是一个错觉，真正是系统在很短的时间内将CPU轮流分配给各个进程，给用户造成多任务同时运行的错觉。所以这就是有一个问题，在每次运行进程之前CPU都需要知道进程从哪里加载、从哪里运行，也就是说需要系统提前帮它设置好CPU寄存器和程序计数器

## 1.CPU上下文

CPU上下文其实是一些环境正是有这些环境的支撑，任务得以运行，而这些环境的硬件条件便是CPU寄存器和程序计数器。CPU寄存器是CPU内置的容量非常小但是速度极快的存储设备，程序计数器则是CPU在运行任何任务时必要的，里面记录了当前运行任务的行数等信息，这就是CPU上下文

![img](https://pic1.zhimg.com/80/v2-be79c25ed9aae9f8981179cda6836870_720w.webp)

## 2.CPU上下文切换

根据任务的不同，CPU的上下文切换就可以分为进程上下文切换、线程上下文切换、中断上下文切换

### 2.1 进程上下文切换

在Linux中，Linux按照特权等级，将进程的运行空间分为内核空间和用户空间：

- 内核空间具有最高权限，可以直接访问所有资源
- 用户空间只能访问受限资源，不能直接访问内存等硬件设备，要想访问这些特权资源，必须通过系统调用

对于一个进程来说，一般是运行在用户态的，但是当需要访问内存、磁盘等硬件设备的时候需要陷入到内核态中，也就是要从用户态到内核态的转变，而这种转变需要通过系统调用来实现，例如一个打开文件的操作，需要调用open()打开文件，read()读取文件内容，write()将文件内容输出到控制台，最后close()关闭文件，这就是系统调用

在系统调用的过程中同样发发生了CPU上下文切换：

- CPU寄存器里面原来用户态的指令位置，需要先保存起来，接着运行内核态代码
- CPU寄存器需要更新为内核态指令的位置，执行内核态代码

系统调用结束后，CPU寄存器需要恢复原来保存的用户态，然后切换为用户空间，所以一次系统调用的过程，会发生两次的CPU上下文切换

但是我们一般说系统调用是特权模式切换而不是上下文切换，因为这里没有涉及到虚拟内存等这些进程用户态的资源，也不会切换进程是属于进程之内的上下文切换

进程是由内核来管理和调度的，进程的切换只能发生在内核态，所以进程的上下文包含了虚拟内存、栈、全局变量等用户空间的资源，还包含了内核堆栈、寄存器等内核空间的状态，所以进程的上下文切换要比系统调用更多一步，保存该进程的虚拟内存、栈等用户空间的资源

进程上下文切换一般需要几十纳秒到数微秒的CPU时间，当进程上下文切换次数比较多的情况下爱，将导致CPU将大量的时间耗费在寄存器、内核栈即虚拟内存等资源的保存和恢复上，另外，Linux通过TLB快表来管理虚拟内存到物理内存的映射关系，当虚拟内存更新之后，需要刷新缓存，在这多处理系统上是很复杂的，因为多个处理器共享一个缓存

下面再来说说什么时候会进行进程的上下文切换，其实就是进程在被调度的时候需要切换上下文，可能是主动地，也有可能是被动的

- 系统进程正常调度算法导致进程上下文切换，例如目前使用的时间片轮转算法，当一个进程的时间片耗尽之后，CPU会进项进程的调度切换到其他进程
- 进程在资源不足的时候，会被挂起例如在等待IO或者内存不足的时候，会主动挂起，并且等待系统调度其他进程
- 当进程通过一些睡眠函数sleep()主动挂起的时候，也会重新调度
- 当有高优先级的进程运行时，当前进程也会被挂起
- 当发生硬件中断时，CPU上的进程会被中断挂起

### 2.2 线程上下文切换

线程是调度的基本单位，而进程则是资源拥有的基本单位，也就是说对于内核中的任务调度是以线程为单位，但是进程只是给线程提供了虚拟内存、全局变量等资源，进程与线程之间的区别这里不再介绍

那么线程上下文的切换，其实分为两种情况：

- 前后两个线程属于不同进程，因为资源不共享，所以这时候的线程上下文切换和进程上下文切换是一致的
- 前后两个线程属于同一个进程，因为虚拟内存是共享的，所以在切换的时候，虚拟内存这些资源保持不动，只有切换线程的私有数据、寄存器等不共享的资源

所以同进程内的线程切换要比多进程内的线程切换消耗更少的资源

### 2.3 中断上下文切换

中断是为了快速响应硬件的事件，简单来shu就是计算机停下当前的事情，去处理其他的事情，然后在回来继续执行之前的任务，例如我们在调用print函数的时候，其实汇编的底层会帮我们调用一条 int 0x80的指令，便是调用0x80号中断

当然，中断要先将当前进程的状态保存下来，这样中断结束后进程仍然可以从原来的状态恢复运行，中断上下文的切换并不涉及进程的用户态，所以当中断程序打断了正在处于用户态的进程，不需要保存和恢复这个进程的虚拟内存、全局变量等用户态资源，只需要保存和恢复这个进程的内核态中的资源包括CPU寄存器、内核堆栈等

对于同一个CPU来说，中断处理比进程拥有更高的优先级，所以中断上下文切换并不会与进程上下文切换同时发生，一般来说中断程序都执行比较快短小精悍，以便快速结束执行之前的任务。当中断上下文切换次数比较多的时候，会耗费大量的CPU

### 2.4 怎么查看系统上下文

上面已经介绍到CPU上下文切换分为进程上下文切换、线程上下文切换、中断上下文切换，那么过多的上下文切换会把CPU的时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，缩短进程真正运行的时间，成为系统性能大幅下降的一个因素

所以我们可以使用vmstat这个工具来查询系统的上下文切换情况，vmstat是一个常用的系统性能分析工具，可以用来分析CPU上下文切换和中断的次数

![img](https://pic4.zhimg.com/80/v2-97b24494c5c70f691fef0d4e6de62d3f_720w.webp)

需要特别关注的是：

- cs(context switch)：每秒上下文切换的次数
- in(interrupt)：每秒中断的次数
- r(Running or Runnable)：就绪队列的长度，也就是正在运行和等待CPU的进程
- b(Blocked)：处于不可中断睡眠状态的进程数

vmstat是给出整个系统总体的上下文切换情况，要想查看每个进程的详细情况就需要使用pidstat，加上-w选项就可以查看进程上下文切换的情况

![img](https://pic1.zhimg.com/80/v2-33ff027184b6b9e1cf4fffeaaccf1e14_720w.webp)

需要特别关注的是：

- cswch(voluntary context switches)：表示每秒自愿上下文切换的次数
- nvcswch(non voluntary context switches)：表示每秒非自愿上下文切换的次数

这两个概念的分别含义：

- 自愿上下文切换：进程无法获取所需的资源，导致的上下文切换，例如IO、内存等资源不足时，就会发生自愿上下文切换
- 非自愿上下文切换：进程由于时间片已到等时间，被系统强制调度，进而发生的上下文切换，例如大量的进程都在争抢CPU时，就容易发生非自愿上下文切换

## 3.实战分析

通过上面的工具已经可以初步查看到系统上下文切换的次数，但是当系统上下文切换的次数为多少时是不正常的呢？

案例使用sysbench工具来模拟多线程调度切换的情况，sysbench是一个多线程的基准测试工具，可以模拟上下文切换过多的问题

首先在第一个终端运行stsbench，模拟多线程切换问题

```text
# 以 10 个线程运行 5 分钟的基准测试，模拟多线程切换的问题
sysbench --threads=10 --max-time=300 threads run
```

然后在第二个终端运行vmstat，每1秒查看上下文切换的情况

![img](https://pic1.zhimg.com/80/v2-cea4f145d8738f60ce2aabe41fefa574_720w.webp)

可以观察到如下指标：

- r列：就绪队列的长度已经到了8左右，已经超过了2个cpu，所以会有大量的CPU竞争
- us(user)列和sy(system)列，这两列的CPU使用率已经到达100%，并且大量是由sy造成的，说明CPU主要是被内核占用了
- in(interrupt)：in列的数值也到了解决1万，所以中断处理也是一个问题

那我们接着使用pidstat来查看是那一个进程出现了问题，由于pidstat默认是显示进程的指标数据，但是我们使用sysbench模拟的线程的数据，所以需要加上-t选项

```text
gpw@gopuwe:~$ pidstat -wt
```

![img](https://pic4.zhimg.com/80/v2-5597fd751b16a8bdafe379615547e403_720w.webp)

所以到这里可以分析出是sysbench的子线程的上下文切换次数有很多

还有一个问题，在使用vmstat的时候，发现in指标的数据也比较多，那么我们需要找出是什么类型的中断导致了中断上升，中断肯定是发生在内核态，但是pidstat只是一个进程的性能分析工具，并不提供任何关于中断的详细信息

我们可以从/proc/interrupts这个只读文件中读取，/proc是一个虚拟文件系统，用于内核空间和用户空间之间的通信，/proc/interrupts则提供了一个只读的中断使用情况，可以使用cat命令查看/proc/interrupts可以发现变化速度最快的是重调度中断RES，这个中断类型表示唤醒空闲状态的CPU来调度新的任务运行，也被成为处理器中断

那么到底上下文切换的次数为多少合适呢？

这个数值其实取决于系统本身的 CPU 性能，在我看来，如果系统的上下文切换次数比较稳

定，那么从数百到一万以内，都应该算是正常的。但当上下文切换次数超过一万次，或者切

换次数出现数量级的增长时，就很可能已经出现了性能问题，这个时候还要根据上下文切换的类型，做具体的分析，例如：

- 自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 I/O 等其他问题；
- 非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢 CPU，说明 CPU的确成了瓶颈；
- 中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看 /proc/interrupts 文件

原文地址：https://zhuanlan.zhihu.com/p/444801883

作者：linux

# 【NO.559】Linux I/O复用中select poll epoll模型的介绍及其优缺点的比较

关于I/O多路复用：

I/O多路复用(又被称为“事件驱动”)，首先要理解的是，操作系统为你提供了一个功能，当你的某个socket可读或者可写的时候，它可以给你一个通知。这样当配合非阻塞的socket使用时，只有当系统通知我哪个描述符可读了，我才去执行read操作，可以保证每次read都能读到有效数据而不做纯返回-1和EAGAIN的无用功。写操作类似。操作系统的这个功能通过select/poll/epoll之类的系统调用来实现，这些函数都可以同时监视多个描述符的读写就绪状况，这样，**多个描述符的I/O操作都能在一个线程内并发交替地顺序完成，这就叫I/O多路复用，这里的“复用”指的是复用同一个线程。

## 1.I/O复用之select

1.1 介绍：

select系统调用的目的是：在一段指定时间内，监听用户感兴趣的文件描述符上的可读、可写和异常事件。poll和select应该被归类为这样的系统调用，它们可以阻塞地同时探测一组支持非阻塞的IO设备，直至某一个设备触发了事件或者超过了指定的等待时间——也就是说它们的职责不是做IO，而是帮助调用者寻找当前就绪的设备。

下面是select的原理图：



![img](https://pic2.zhimg.com/80/v2-58978583c025aa938901fc953b34601d_720w.webp)



1.2 select系统调用API如下：

```text
#include <sys/time.h>
#include <sys/types.h>
#include <unistd.h>
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

fd_set结构体是文件描述符集，该结构体实际上是一个整型数组，数组中的每个元素的每一位标记一个文件描述符。fd_set能容纳的文件描述符数量由FD_SETSIZE指定，一般情况下，FD_SETSIZE等于1024，这就限制了select能同时处理的文件描述符的总量。

1.3 下面介绍一下各个参数的含义：

1）nfds参数指定被监听的文件描述符的总数。通常被设置为select监听的所有文件描述符中最大值加1；

2）readfds、writefds、exceptfds分别指向可读、可写和异常等事件对应的文件描述符集合。这三个参数都是传入传出型参数，指的是在调用select之前，用户把关心的可读、可写、或异常的文件描述符通过FD_SET（下面介绍）函数分别添加进readfds、writefds、exceptfds文件描述符集，select将对这些文件描述符集中的文件描述符进行监听，如果有就绪文件描述符，select会重置readfds、writefds、exceptfds文件描述符集来通知应用程序哪些文件描述符就绪。这个特性将导致select函数返回后，再次调用select之前，必须重置我们关心的文件描述符，也就是三个文件描述符集已经不是我们之前传入 的了。

3）timeout参数用来指定select函数的超时时间（下面讲select返回值时还会谈及）。

```text
struct timeval
{
    long tv_sec;        //秒数
    long tv_usec;       //微秒数
};
```

1.4 下面几个函数（宏实现）用来操纵文件描述符集：

```text
void FD_SET(int fd, fd_set *set);   //在set中设置文件描述符fd
void FD_CLR(int fd, fd_set *set);   //清除set中的fd位
int  FD_ISSET(int fd, fd_set *set); //判断set中是否设置了文件描述符fd
void FD_ZERO(fd_set *set);          //清空set中的所有位（在使用文件描述符集前，应该先清空一下）
    //（注意FD_CLR和FD_ZERO的区别，一个是清除某一位，一个是清除所有位）
```

1.5 select的返回情况：

1）如果指定timeout为NULL，select会永远等待下去，直到有一个文件描述符就绪，select返回；

2）如果timeout的指定时间为0，select根本不等待，立即返回；

3）如果指定一段固定时间，则在这一段时间内，如果有指定的文件描述符就绪，select函数返回，如果超过指定时间，select同样返回。

4）返回值情况：

a)超时时间内，如果文件描述符就绪，select返回就绪的文件描述符总数（包括可读、可写和异常），如果没有文件描述符就绪，select返回0；

b)select调用失败时，返回 -1并设置errno，如果收到信号，select返回 -1并设置errno为EINTR。

1.6 文件描述符的就绪条件：

在网络编程中，

1）下列情况下socket可读：

a) socket内核接收缓冲区的字节数大于或等于其低水位标记SO_RCVLOWAT；

b) socket通信的对方关闭连接，此时该socket可读，但是一旦读该socket，会立即返回0（可以用这个方法判断client端是否断开连接）；

c) 监听socket上有新的连接请求；

d) socket上有未处理的错误。

2）下列情况下socket可写：

a) socket内核发送缓冲区的可用字节数大于或等于其低水位标记SO_SNDLOWAT；

b) socket的读端关闭，此时该socket可写，一旦对该socket进行操作，该进程会收到SIGPIPE信号；

c) socket使用connect连接成功之后；

d) socket上有未处理的错误。

## 2.I/O复用之poll

1、poll系统调用的原理与原型和select基本类似，也是在指定时间内轮询一定数量的文件描述符，以测试其中是否有就绪者。

2、poll系统调用API如下：

```text
#include <poll.h>
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```

3、下面介绍一下各个参数的含义：

1）第一个参数是指向一个结构数组的第一个元素的指针，每个元素都是一个pollfd结构，用于指定测试某个给定描述符的条件。

```text
struct pollfd
{
    int fd;             //指定要监听的文件描述符
    short events;       //指定监听fd上的什么事件
    short revents;      //fd上事件就绪后，用于保存实际发生的时间
}；
```

待监听的事件由events成员指定，函数在相应的revents成员中返回该描述符的状态（每个文件描述符都有两个事件，一个是传入型的events，一个是传出型的revents，从而避免使用传入传出型参数，注意与select的区别），从而告知应用程序fd上实际发生了哪些事件。events和revents都可以是多个事件的按位或。

2）第二个参数是要监听的文件描述符的个数，也就是数组fds的元素个数；

3）第三个参数意义与select相同。

4、poll的事件类型：



![img](https://pic3.zhimg.com/80/v2-22d23cb65fbcb15d2b3e572186159c0a_720w.webp)



在使用POLLRDHUP时，要在代码开始处定义_GNU_SOURCE

5、poll的返回情况：

与select相同。

## 3.I/O复用之epoll

首先给大家介绍一个[epoll实战视频](https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/BV1iJ411S7mv/)，点击即可观看

1、介绍：

epoll 与select和poll在使用和实现上有很大区别。首先，epoll使用一组函数来完成，而不是单独的一个函数；其次，epoll把用户关心的文件描述符上的事件放在内核里的一个事件表中，无须向select和poll那样每次调用都要重复传入文件描述符集合事件集。

2、创建一个文件描述符，指定内核中的事件表：

```text
#include<sys/epoll.h>
int epoll_create(int size);
    //调用成功返回一个文件描述符，失败返回-1并设置errno。
```

size参数并不起作用，只是给内核一个提示，告诉它事件表需要多大。该函数返回的文件描述符指定要访问的内核事件表，是其他所有epoll系统调用的句柄。

3、操作内核事件表：

```text
#include<sys/epoll.h>
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
    //调用成功返回0，调用失败返回-1并设置errno。
```

epfd是epoll_create返回的文件句柄，标识事件表，op指定操作类型。操作类型有以下3种：

- a）EPOLL_CTL_ADD， 往事件表中注册fd上的事件；
- b）EPOLL_CTL_MOD， 修改fd上注册的事件；
- c）EPOLL_CTL_DEL， 删除fd上注册的事件。

event参数指定事件，epoll_event的定义如下：

```text
struct epoll_event
{
    __int32_t events;       //epoll事件
    epoll_data_t data;      //用户数据
};
typedef union epoll_data
{
    void *ptr;
    int  fd;
    uint32_t u32;
    uint64_t u64;
}epoll_data;
```

在使用epoll_ctl时，是把fd添加、修改到内核事件表中，或从内核事件表中删除fd的事件。如果是添加事件到事件表中，可以往data中的fd上添加事件events，或者不用data中的fd，而把fd放到用户数据ptr所指的内存中（因为epoll_data是一个联合体，只能使用其中一个数据）,再设置events。

3、epoll_wait函数

epoll系统调用的最关键的一个函数epoll_wait，它在一段时间内等待一个组文件描述符上的事件。

```text
#include<sys/epoll.h>
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
    //函数调用成功返回就绪文件描述符个数，失败返回-1并设置errno。
```

timeout参数和select与poll相同，指定一个超时时间；maxevents指定最多监听多少个事件；events是一个传出型参数，epoll_wait函数如果检测到事件就绪，就将所有就绪的事件从内核事件表（epfd所指的文件）中复制到events指定的数组中。这个数组用来输出epoll_wait检测到的就绪事件，而不像select与poll那样，这也是epoll与前者最大的区别，下文在比较三者之间的区别时还会说到。

## 4.三组I/O复用函数的比较

相同点：

1）三者都需要在fd上注册用户关心的事件；

2）三者都要一个timeout参数指定超时时间；

不同点：

1）select：

a）select指定三个文件描述符集，分别是可读、可写和异常事件，所以不能更加细致地区分所有可能发生的事件；

b）select如果检测到就绪事件，会在原来的文件描述符上改动，以告知应用程序，文件描述符上发生了什么时间，所以再次调用select时，必须先重置文件描述符；

c）select采用对所有注册的文件描述符集轮询的方式，会返回整个用户注册的事件集合，所以应用程序索引就绪文件的时间复杂度为O(n)；

d）select允许监听的最大文件描述符个数通常有限制，一般是1024，如果大于1024，select的性能会急剧下降；

e）只能工作在LT模式。

2）poll：

a）poll把文件描述符和事件绑定，事件不但可以单独指定，而且可以是多个事件的按位或，这样更加细化了事件的注册，而且poll单独采用一个元素用来保存就绪返回时的结果，这样在下次调用poll时，就不用重置之前注册的事件；

b）poll采用对所有注册的文件描述符集轮询的方式，会返回整个用户注册的事件集合，所以应用程序索引就绪文件的时间复杂度为O(n)。

c）poll用nfds参数指定最多监听多少个文件描述符和事件，这个数能达到系统允许打开的最大文件描述符数目，即65535。

d）只能工作在LT模式。

3）epoll：

a）epoll把用户注册的文件描述符和事件放到内核当中的事件表中，提供了一个独立的系统调用epoll_ctl来管理用户的事件，而且epoll采用回调的方式，一旦有注册的文件描述符就绪，讲触发回调函数，该回调函数将就绪的文件描述符和事件拷贝到用户空间events所管理的内存，这样应用程序索引就绪文件的时间复杂度达到O(1)。

b）epoll_wait使用maxevents来制定最多监听多少个文件描述符和事件，这个数能达到系统允许打开的最大文件描述符数目，即65535；

c）不仅能工作在LT模式，而且还支持ET高效模式（即EPOLLONESHOT事件，读者可以自己查一下这个事件类型，对于epoll的线程安全有很好的帮助）。

select/poll/epoll总结：



![img](https://pic2.zhimg.com/80/v2-e177f887784cbb59f1bd1ce701f2d0d1_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/141447239

作者：linux

# 【NO.560】Linux内核时钟系统和定时器实现

## 1. Linux内核时钟系统和定时器实现

Linux 2.6.16之前，内核只支持低精度时钟，内核定时器的工作方式：

- 系统启动后，会读取时钟源设备(RTC, HPET，PIT…),初始化当前系统时间；
- 内核会根据HZ(系统定时器频率，节拍率)参数值，设置时钟事件设备，启动tick(节拍)中断。HZ表示1秒种产生多少个时钟硬件中断，tick就表示连续两个中断的间隔时间。在我电脑上，HZ=250， 一个tick = 1/HZ, 所以默认一个tick为4ms。

```text
cat /boot/config-`uname -r` | grep 'CONFIG_HZ='
CONFIG_HZ=250
```

- 设置时钟事件设备后，时钟事件设备会定时产生一个tick中断，触发时钟中断处理函数，更新系统时钟,并检测timer wheel，进行超时事件的处理。

在上面工作方式下，Linux 2.6.16 之前，内核软件定时器采用timer wheel多级时间轮的实现机制，维护操作系统的所有定时事件。timer wheel的触发是基于系统tick周期性中断。

所以说这之前，linux只能支持ms级别的时钟，随着时钟源硬件设备的精度提高和软件高精度计时的需求，有了高精度时钟的内核设计。

Linux 2.6.16 ，内核支持了高精度的时钟，内核采用新的定时器hrtimer，其实现逻辑和Linux 2.6.16 之前定时器逻辑区别：

- hrtimer采用红黑树进行高精度定时器的管理，而不是时间轮；
- 高精度时钟定时器不在依赖系统的tick中断，而是基于事件触发。

旧内核的定时器实现依赖于系统定时器硬件定期的tick，基于该tick，内核会扫描timer wheel处理超时事件，会更新jiffies，wall time(墙上时间，现实时间)，process的使用时间等等工作。

新的内核不再会直接支持周期性的tick，新内核定时器框架采用了基于事件触发，而不是以前的周期性触发。新内核实现了hrtimer(high resolution timer)，hrtimer的设计目的，就是为了解决time wheel的缺点：

- 低精度；timer wheel只能支持ms级别的精度，hrtimer可以支持ns级别；
- Timer wheel与内核其他模块的高耦合性；

新内核的hrtimer的触发和设置不像之前在定期的tick中断中进行，而是动态调整的，即基于事件触发，hrtimer的工作原理：通过将高精度时钟硬件的下次中断触发时间设置为红黑树中最早到期的 Timer 的时间，时钟到期后从红黑树中得到下一个 Timer 的到期时间，并设置硬件，如此循环反复。

在高精度时钟模式下，操作系统内核仍然需要周期性的tick中断，以便刷新内核的一些任务。前面可以知道，hrtimer是基于事件的，不会周期性出发tick中断，所以为了实现周期性的tick中断(dynamic tick)：系统创建了一个模拟 tick 时钟的特殊 hrtimer，将其超时时间设置为一个tick时长，在超时回来后，完成对应的工作，然后再次设置下一个tick的超时时间，以此达到周期性tick中断的需求。

引入了dynamic tick,是为了能够在使用高精度时钟的同时节约能源,，这样会产生tickless 情况下，会跳过一些 tick。这里只是简单介绍，有兴趣可以读kernel源码。

![img](https://pic1.zhimg.com/80/v2-1fa3869a41c1d711486376abde889da0_720w.webp)

上图1是Linux 2.6.16以来内核定时器实现的结构，

新内核对相关的时间硬件设备进行了统一的封装，定义了主要有下面两个结构：

- 时钟源设备(closk source device)：抽象那些能够提供计时功能的系统硬件，比如 RTC(Real Time Clock)、TSC(Time Stamp Counter)，HPET，ACPI PM-Timer，PIT等。不同时钟源提供的精度不一样，现在pc大都是支持高精度模式(high-resolution mode)也支持低精度模式(low-resolution mode)。
- 时钟事件设备(clock event device)：系统中可以触发 one-shot（单次）或者周期性中断的设备都可以作为时钟事件设备。

当前内核同时存在新旧timer wheel 和 hrtimer两套timer的实现，内核启动后会进行从低精度模式到高精度时钟模式的切换，hrtimer模拟的tick中断将驱动传统的低精度定时器系统（基于时间轮）和内核进程调度。

内核定时器系统增加了hrtimer之后，对于用户层开放的定时器相关接口基本都是通过hrtimer进行实现的，从内核源码可以看到:

```text
    *  These timers are currently used for:
    *   - itimers
    *   - POSIX timers
    *   - nanosleep
    *   - precise in-kernel timing
    *
```

## 2. 用户层定时器API接口

上面介绍完linux内核定时器的实现后，下面简单说一下，基于内核定时器实现的，对用户层开放的定时器API：间隔定时器itimer和POSIX定时器。

### **2.1 常见定时功能的API：sleep系列**

在介绍itimer和POSIX定时器之前，我们先看看我们经常遇到过具有定时功能的库函数API接口：

```text
alarm()
sleep()
usleep()
nanosleep()
```

alarm:

alarm()函数可以设置一个定时器，在特定时间超时，并产生SIGALRM信号，如果不忽略或不捕捉该信号，该进程会被终止。

```text
#include <unistd.h>
unsigned int alarm(unsigned int seconds);
                                              return：0或到期剩余秒数
```

那么alarm在是**如何实现**的？Glibc中alarm是基于间隔定时器itimer来实现的(文章后面会说到itimer是基于hrtimer实现的)。如下alarm在库函数下的实现,alarm调用了setitimer系统调用：

```text
unsigned int 
alarm (seconds)
     unsigned int seconds;
{
  ...
  if (__setitimer (ITIMER_REAL, &new, &old) < 0)
    return 0;
  ...
}
libc_hidden_def (alarm)
```

sleep:

sleep和alarm的功能类似，不过sleep会让进程挂起， 在定时器超时内核会产生SIGALRM信号，如果不忽略或不捕捉该信号，该进程会被终止。

那么sleep是如何实现的？Glibc的sleep实现如下：可见其实调用alarm实现的，在alarm的基础上注册了SIGALRM信号处理函数，用于在定时器到期时，捕获到信号，回到睡眠的地方。所以其实可以看出sleep就是对alarm的特化。

```text
unsigned int
__sleep (unsigned int seconds)
{
    ...
    struct sigaction act, oact;
    ...
    //注册信号回调函数
    act.sa_handler = sleep_handler;
    act.sa_flags = 0;
    act.sa_mask = oset;
    if (sigaction (SIGALRM, &act, &oact) < 0)
        return seconds;
    ...
    //调用alarm API进行操作
    remaining = alarm (seconds);

}
weak_alias (__sleep, sleep)
```

usleep:

usleep支持精度更高的微妙级别的定时操作，

```text
int usleep (useconds_t useconds)
{
  struct timespec ts = { .tv_sec = (long int) (useconds / 1000000),
             .tv_nsec = (long int) (useconds % 1000000) * 1000ul };

  /* Note the usleep() is a cancellation point.  But since we call
     nanosleep() which itself is a cancellation point we do not have
     to do anything here.  */
  return __nanosleep (&ts, NULL);
}
```

Bsd的usleep实现如下：

```text
int usleep (useconds)
     useconds_t useconds;
{
  struct timeval delay;

  delay.tv_sec = 0;
  delay.tv_usec = useconds;

  return __select (0, (fd_set *) NULL, (fd_set *) NULL, (fd_set *) NULL,
           &delay);
}
```

nanosleep:

nanosleep()glibc的API是直接调用linux内核的nanosleep，内核的nanosleep采用了hrtimer进行实现。

alarm(), sleep()系列，以及后面的间隔定时器itimer都是基于SIGALRM信号进行触发的。所以它们是不能同时使用的。

### **2.2 间隔定时器itimer**

间隔定时器的接口如下：

```text
#include <sys/time.h>

int getitimer(int which, struct itimerval *curr_value);
int setitimer(int which, const struct itimerval *new_value,
        struct itimerval *old_value);
结构定义：
struct itimerval {
    struct timeval it_interval; /* next value ：间隔时间*/
    struct timeval it_value;    /* current value：到期时间*/
};
struct timeval {
    time_t      tv_sec;         /* seconds */
    s
```

可以通过调用上面两个API接口来设置和获取间隔定时器信息。系统为每个进程提供了3种itimer，每种定时器在不同时间域递减，当定时器超时，就会向进程发送一个信号，并且重置定时器。3种定时器的类型，如下表所示：

![img](https://pic2.zhimg.com/80/v2-5a41167745ba9df61cc2fc4c50b21859_720w.webp)

表1 参数which与定时器类型

在Linux 2.6.16 之前，itimer的实现是基于内核定时器timer wheel封装成的定时器接口。内核封装的定时器接口是提供给其他内核模块使用，也是其他定时器的基础。itimer通过内核定时器的封装，生成提供给用户层使用的接口setitimer和getitimer。内核定时器timer wheel提供的内核态调用接口为:可参考

```text
add_timer() 
del_timer() 
init_timer()
```

在Linux 2.6.16 以来，itimer不再采用基于timer wheel的内核定时器进行实现。而是换成了高精度的内核定时器hrtimer进行实现。

如果定时器超时时，进程是处于运行状态，那么超时的信号会被立刻传送给该进程，否则信号会被延迟传送，延迟时间要根据系统的负载情况。所以这里有一个BUG：当系统负载很重的情况下，对于ITIMER_REAL定时器有可能在上一次超时信号传递完成前再次超时，这样就会导致第二次超时的信号丢失。

每个进程中同一种定时器只能使用一次。

该系统调用在POSIX.1-2001中定义了，但在POSIX.1-2008中已被废弃。所以建议使用POSIX定时器API（timer_gettime, timer_settime）代替。

函数alarm本质上设置的是低精确、非重载的ITIMER_REAL类定时器，它只能精确到秒，并且每次设置只能产生一次定时。函数setitimer 设置的定时器则不同，它们不但可以计时到微妙（理论上），还能自动循环定时。在一个Unix进程中，不能同时使用alarm和ITIMER_REAL类定时器。

下面是测试代码：

```text
#include <unistd.h>
#include <signal.h>
#include <sys/time.h>

#include <iostream>

void sig_handler(int signo)
{
    std::cout<<"recieve sigal: "<<signo<<std::endl;
}

int main()
{
    signal(SIGALRM, sig_handler);

    struct itimerval timer_set;

    //启动时间（5s后启动）
    timer_set.it_value.tv_sec = 5;
    timer_set.it_value.tv_usec = 0;

    //间隔定时器间隔：2s
    timer_set.it_interval.tv_sec = 2;
    timer_set.it_interval.tv_usec = 0;

    if(setitimer(ITIMER_REAL, &timer_set, NULL) < 0)
    {
        std::cout<<"start timer failed..."<<std::endl;
        return 0;
    }

    int temp;
    std::cin>>temp;

    return 0;
}
```

### **2.3 POSIX定时器**

POSIX定时器的是为了解决间隔定时器itimer的以下问题：

- 一个进程同一时刻只能有一个同一种类型(ITIMER_REAL, ITIMER_PROF, ITIMER_VIRT)的itimer。POSIX定时器在一个进程中可以创建任意多个timer。
- itimer定时器到期后，只能通过信号(SIGALRM,SIGVTALRM,SIGPROF)的方式通知进程，POSIX定时器到期后不仅可以通过信号进行通知，还可以使用自定义信号，还可以通过启动一个线程来进行通知。
- itimer支持us级别，POSIX定时器支持ns级别。

POSIX定时器提供的定时器API如下：

```text
int timer_create(clockid_t clock_id, struct sigevent *evp, timer_t *timerid)；
int timer_settime(timer_t timerid, int flags, const struct itimerspec *value, struct itimerspect *ovalue);
int timer_gettime(timer_t timerid,struct itimerspec *value);
int timer_getoverrun(timer_t timerid);
int timer_delete (timer_t timerid);
```

其中时间结构itimerspec定义如下：该结构和itimer的itimerval结构用处和含义类似，只是提供了ns级别的精度

```text
struct itimerspec
{
    struct timespec it_interval;    // 时间间隔
    struct timespec it_value;       // 首次到期时间
};

struct timespec
{
    time_t  tv_sec    //Seconds.
    long    tv_nsec   //Nanoseconds.
};
```

it_value表示定时间经过这么长时间到时，当定时器到时候，就会将it_interval的值赋给it_value。如果it_interval等于0，那么表示该定时器不是一个时间间隔定时器，一旦it_value到期后定时器就回到未启动状态。

timer_create(clockid_t clock_id, struct sigevent *evp, timer_t *timerid)

创建一个POSIX timer,在创建的时候，需要指出定时器的类型，定时器超时通知机制。创建成功后通过参数返回创建的定时器的ID。

参数clock_id用来指定定时器时钟的类型，时钟类型有以下6种：

CLOCK_REALTIME：系统实时时间，即日历时间；

- CLOCK_MONOTONIC：从系统启动开始到现在为止的时间；
- CLOCK_PROCESS_CPUTIME_ID：本进程启动到执行到当前代码，系统CPU花费的时间；
- CLOCK_THREAD_CPUTIME_ID：本线程启动到执行到当前代码，系统CPU花费的时间；
- CLOCK_REALTIME_HR：CLOCK_REALTIME的细粒度（高精度）版本；
- CLOCK_MONOTONIC_HR：CLOCK_MONOTONIC的细粒度版本；

struct sigevent设置了定时器到期时的通知方式和处理方式等，结构的定义如下：

```text
struct sigevent
{
    int sigev_notify;   //设置定时器到期后的行为
    int sigev_signo;    //设置产生信号的信号码
    union sigval   sigev_value; //设置产生信号的值
    void (*sigev_notify_function)(union sigval);//定时器到期，从该地址启动一个线程
    pthread_attr_t *sigev_notify_attributes;    //创建线程的属性
}

union sigval
{
    int sival_int;  //integer value
    void *sival_ptr; //pointer value
}
```

如果sigevent传入NULL，那么定时器到期会产生默认的信号，对CLOCK_REALTIMER来说，默认信号就是SIGALRM，如果要产生除默认信号之外的其他信号，程序必须将evp->sigev_signo设置为期望的信号码。

如果几个定时器产生了同一个信号，处理程序可以用 sigev_value来区分是哪个定时器产生了信号。要实现这种功能，程序必须在为信号安装处理程序时，使用struct sigaction的成员sa_flags中的标志符SA_SIGINFO。

sigev_notify的值可取以下几种：

- SIGEV_NONE：定时器到期后什么都不做，只提供通过timer_gettime和timer_getoverrun查询超时信息。
- SIGEV_SIGNAL：定时器到期后，内核会将sigev_signo所指定的信号，传送给进程，在信号处理程序中，si_value会被设定为sigev_value的值。
- SIGEV_THREAD：定时器到期后，内核会以sigev_notification_attributes为线程属性创建一个线程，线程的入口地址为sigev_notify_function，传入sigev_value作为一个参数。

timer_settime(timer_t timerid, int flags, const struct itimerspec *value, struct itimerspect *ovalue)

创建POSIX定时器后，该定时器并没有启动，需要通过timer_settime()接口设置定时器的到期时间和周期触发时间。

flags字段标识到期时间是一个绝对时间还是一个相对时间。

```text
/* Flag to indicate time is absolute.  */
#   define TIMER_ABSTIME        1
```

如果flags的值为TIMER_ABSTIME，则value的值为一个绝对时间。否则，value为一个相对时间。

timer_getoverrun(timer_t timerid)

取得一个定时器的超限运行次数：有可能一个定时器到期了，而同一定时器上一次到期时产生的信号还处于挂起状态。在这种情况下，其中的一个信号可能会丢失。这就是定时器超限。程序可以通过调 用timer_getoverrun来确定一个特定的定时器出现这种超限的次数。定时器超限只能发生在同一个定时器产生的信号上。由多个定时器，甚至是那 些使用相同的时钟和信号的定时器，所产生的信号都会排队而不会丢失。

执行成功时，timer_getoverrun()会返回定时器初次到期与通知进程(例如通过信号)定时器已到期之间额外发生的定时器到期次数。举例来说，在我们之前的例子中，一个1ms的定时器运行了10ms，则此调用会返回9。如果超限运行的次数等于或大于DELAYTIMER_MAX，则此调用会返回DELAYTIMER_MAX。

执行失败时，此函数会返回-1并将errno设定会EINVAL，这个唯一的错误情况代表timerid指定了无效的定时器。

timer_delete (timer_t timerid)

删除一个定时器：一次成功的timer_delete()调用会销毁关联到timerid的定时器并且返回0。执行失败时，此调用会返回-1并将errno设定会 EINVAL，这个唯一的错误情况代表timerid不是一个有效的定时器。

POSIX定时器通过调用内核的posix_timer进行实现，但glibc对POSIX timer进行了一定的封装，例如如果POSIX timer到期通知方式被设置为 SIGEV_THREAD 时，glibc 需要自己完成一些辅助工作，因为内核无法在 Timer 到期时启动一个新的线程。

```text
int
timer_create (clock_id, evp, timerid)
     clockid_t clock_id;
     struct sigevent *evp;
     timer_t *timerid;
{
    if (evp == NULL || __builtin_expect (evp->sigev_notify != SIGEV_THREAD, 1))
    {
        ...
    }
    else
    {
          ...
          /* Create the helper thread.  */
          pthread_once (&__helper_once, __start_helper_thread);
          ...
    }
    ...
}
```

可以看到 GLibc 发现用户需要启动新线程通知时，会自动调用 pthread_once 启动一个辅助线程（__start_helper_thread），用 sigev_notify_attributes 中指定的属性设置该辅助线程。

然后 glibc 启动一个普通的 POSIX Timer，将其通知方式设置为：SIGEV_SIGNAL | SIGEV_THREAD_ID。这样就可以保证内核在 timer 到期时通知辅助线程。通知的 Signal 号为 SIGTIMER，并且携带一个包含了到期函数指针的数据。这样，当该辅助 Timer 到期时，内核会通过 SIGTIMER 通知辅助线程，辅助线程可以在信号携带的数据中得到用户设定的到期处理函数指针，利用该指针，辅助线程调用 pthread_create() 创建一个新的线程来调用该处理函数。这样就实现了 POSIX 的定义。

## 3. 自定义定时器实现方案

在业务项目中，对于系统提供的定时器API往往很难满足我们的需求：

itimer在进程中每种timer类型(ITIMER_REAL, ITIMER_PROF, ITIMER_VIRT)只能使用一个，另外一点就是他是基于signal进行超时提醒，不仅和alarm，sleep这些api冲突，而且在业务代码中signal是个很不可控的机制，尽量都会减少使用。

POSIX定时器在itimer基础上进行了很大的改进，解决了itimer的不足，可以说POSIX定时器可以满足了业务很多的需求。

### **3.1 基于小根堆的定时器实现**

小根堆定时器的实现方式，是最常见的一种实现定时器的方式。堆顶时钟保存最先到期的定时器，基于事件触发的定时器系统可以根据堆顶定时器到期时间，进行睡眠。基于周期性睡眠的定时器系统，每次只需遍历堆顶的定时器是否到期，即可。堆顶定时器超时后，继续调整堆，使其保持为小根堆并同时对堆顶定时器进行超时判断。

小根堆定时器在插入时的时间复杂度在O(lgn)(n为插入时定时器堆的定时器数量)，定时器超时处理时调整堆的复杂度在所有定时器都超时情况下为：O(nlgn)。在一般情况下，小根堆的实现方式可满足业务的基本需求。

### **3.2 基于时间轮的定时器实现**

定时器的实现方式有两种：一级时间轮和多级时间轮。

一级时间轮

一级时间轮如下图所示：一个轮子(数组实现)，轮子的每个槽(spoke)后面会挂一个timer列表，表示当命中该spoke时，timer列表的所有定时器全部超时。

如果定时器轮的精度是1ms，那么spoke个数为2^10时，仅仅能够表示1s，2^20表示17.476min.

如果精度为1s，那么spoke个数为2^10时，能够表示17min，2^20表示12day.

所有这种一级时间轮的实现方式所带来的空间复杂度还是不小的。特别是在需要跨度比较长的定时器时。基于此，就出现了多级时间轮，也就是linux2.6.16之前内核所采用的定时器的实现方式。

![img](https://pic2.zhimg.com/80/v2-063fb227adafdb630407c57e252d1419_720w.webp)

简单时间轮还有很多实现方式，此时时间轮的每个spoke的含义不再是时间精度，而是某个hashkey， 例如ACE当中采用的简单时间轮，轮子的含义：

( 触发时间 >> 分辨率的位数)&(spoke大小-1)

每个spoke所链接的timer列表可以采用很高效的multimap来实现，让timer的插入时间可以降到O(lgn)，到期处理时间最坏为O(nlgn)，n为每个spoke中的timer个数。

![img](https://pic4.zhimg.com/80/v2-1867b7b063c38e2d532db2abf7df4173_720w.webp)

多级时间轮

多级时间轮的实现方式被比作经典的”水表实现方式”，一级时间轮只有一个进制，多级时间轮采用了不同的进制，最低级的时间轮每个spoke表示基本的时间精度，次级时间轮每个spoke表示的时间精度为最低级时间轮所能表示时间长度，依次类推。例如内核的时间轮采用5级时间轮，每一级时间轮spoke个数从低级到高级分别为：8,6,6,6,6，所能表达的时间长度为：2^6 * 2^6 * 2^6 * 2^6 * 2^8 = 2^32 ticks，在系统tick精度为10ms时，内核时间轮所能表示的时间跨度为42949672.96s，约为497天。

那么多级时间轮如何工作的呢：

![img](https://pic1.zhimg.com/80/v2-df207fbc714f38e1ffa9c294c21f8944_720w.webp)

- Insert：

定时器的插入，首先都要根据定时器的超时时间与每级时间轮所能表示的时长进行比较，来觉得插入到那个轮子中，再根据当前轮子已走的索引，计算出待插入定时器在该轮子中应插入的spoke。

```text
#define WHEEL_THRESHOLD_LEVEL_1 (0x01 << 8) 
#define WHEEL_THRESHOLD_LEVEL_2 (0x01 << (8 + 6))
#define WHEEL_THRESHOLD_LEVEL_3 (0x01 << (8 + 2 * 6))
#define WHEEL_THRESHOLD_LEVEL_4 (0x01 << (8 + 3 * 6))
#define WHEEL_THRESHOLD_LEVEL_5 (0x01 << (8 + 4 * 6))
```

- Schedule：

多级时间轮定时器触发机制为周期性tick出发，每个tick到来，最低级的tv1的spoke index都会+1，如果该spoke中有timer，那么就处理该timer list中的所有超时timer。

- Cascade：

Cascade可以翻译成降级处理。每个tick到来，都只会去检测最低级的tv1的时间轮，因为多级时间轮的设计决定了最低级的时间轮永远保存这最近要超时的定时器。

多级时间轮最重要的一个处理流程就是cascade，当每一级(除了最高级)时间轮走到超出该级时间轮的范围时，就会触发上一级时间轮所在spoke+1的cascade过程，如果上一级时间轮也走出来时间轮的范围，也同样会触发cascade过程，这是一个递归过程。

在cascade过程中存在一种极限情况，所有的时间轮都会进行cascade处理，这个时候tv2, tv3, tv4, tv5的hlsit_head[0]都会发送变动，这个过程在定时数量比较大的情况下，会是一个比较耗时的处理流程。

原文地址：https://zhuanlan.zhihu.com/p/399441638

作者：linux

# 【NO.561】linux下C++多线程并发之原子操作与无锁编程

## 1.何为原子操作

原子操作：顾名思义就是不可分割的操作，该操作只存在未开始和已完成两种状态，不存在中间状态；

原子类型：原子库中定义的数据类型，对这些类型的所有操作都是原子的，包括通过原子类模板std::atomic< T >实例化的数据类型，也都是支持原子操作的。

## 2.如何使用原子类型

### 2.1 原子库atomic支持的原子操作

原子库< atomic >中提供了一些基本原子类型，也可以通过原子类模板实例化一个原子对象，下面列出一些基本原子类型及相应的特化模板如下：



![img](https://pic1.zhimg.com/80/v2-60840a12d389dcf8adb1cf93c5ba3398_720w.webp)



对原子类型的访问，最主要的就是读和写，但原子库提供的对应原子操作是load()与store(val)。原子类型支持的原子操作如下：



![img](https://pic2.zhimg.com/80/v2-40d19dda930d3803e9b556d808a67709_720w.webp)



### 2.2 原子操作中的内存访问模型

原子操作保证了对数据的访问只有未开始和已完成两种状态，不会访问到中间状态，但我们访问数据一般是需要特定顺序的，比如想读取写入后的最新数据，原子操作函数是支持控制读写顺序的，即带有一个数据同步内存模型参数std::memory_order，用于对同一时间的读写操作进行排序。C++11定义的6种类型如下：

- memory_order_relaxed: 宽松操作，没有同步或顺序制约，仅对此操作要求原子性；
- memory_order_release & memory_order_acquire: 两个线程A&B，A线程Release后，B线程Acquire能保证一定读到的是最新被修改过的值；这种模型更强大的地方在于它能保证发生在A-Release前的所有写操作，在B-Acquire后都能读到最新值；
- memory_order_release & memory_order_consume: 上一个模型的同步是针对所有对象的，这种模型只针对依赖于该操作涉及的对象：比如这个操作发生在变量a上，而s = a + b; 那s依赖于a，但b不依赖于a; 当然这里也有循环依赖的问题，例如：t = s + 1，因为s依赖于a，那t其实也是依赖于a的；
- memory_order_seq_cst: 顺序一致性模型，这是C++11原子操作的默认模型；大概行为为对每一个变量都进行Release-Acquire操作，当然这也是一个最慢的同步模型；

内存访问模型属于比较底层的控制接口，如果对编译原理和CPU指令执行过程不了解的话，容易引入bug。内存模型不是本章重点，这里不再展开介绍，后续的代码都使用默认的顺序一致性模型或比较稳妥的Release-Acquire模型。

### 2.3 使用原子类型替代互斥锁编程

为便于比较，直接基于前篇文章：线程同步之互斥锁中的示例程序进行修改，用原子库取代互斥库的代码如下：

```text
//atomic1.cpp 使用原子库取代互斥库实现线程同步
#include <chrono>
#include <atomic>
#include <thread>
#include <iostream> 
std::chrono::milliseconds interval(100);
std::atomic<bool> readyFlag(false);     //原子布尔类型，取代互斥量
std::atomic<int> job_shared(0); //两个线程都能修改'job_shared',将该变量特化为原子类型
int job_exclusive = 0; //只有一个线程能修改'job_exclusive',不需要保护
//此线程只能修改 'job_shared'
void job_1()
{   
    std::this_thread::sleep_for(5 * interval);
    job_shared.fetch_add(1);
    std::cout << "job_1 shared (" << job_shared.load() << ")\n";
    readyFlag.store(true);      //改变布尔标记状态为真
}
// 此线程能修改'job_shared'和'job_exclusive'
void job_2()
{
    while (true) {    //无限循环，直到可访问并修改'job_shared'
        if (readyFlag.load()) {     //判断布尔标记状态是否为真，为真则修改‘job_shared’
            job_shared.fetch_add(1);
            std::cout << "job_2 shared (" << job_shared.load() << ")\n";
            return;
        } else {      //布尔标记为假,则修改'job_exclusive'
            ++job_exclusive;
            std::cout << "job_2 exclusive (" << job_exclusive << ")\n";
            std::this_thread::sleep_for(interval);
        }
    }
}
int main() 
{
    std::thread thread_1(job_1);
    std::thread thread_2(job_2);
    thread_1.join();
    thread_2.join();
    getchar();
    return 0;
}
```

由示例程序可以看出，原子布尔类型可以实现互斥锁的部分功能，但在使用条件变量condition variable时，仍然需要mutex保护对condition variable的消费，即使condition variable是一个atomic object。

### 2.4 使用原子类型实现自旋锁

自旋锁（spinlock）与互斥锁(mutex)类似，在任一时刻最多只能有一个持有者，但如果资源已被占用，互斥锁会让资源申请者进入睡眠状态，而自旋锁不会引起调用者睡眠，会一直循环判断该锁是否成功获取。自旋锁是专为防止多处理器并发而引入的一种锁，它在内核中大量应用于中断处理等部分（对于单处理器来说，防止中断处理中的并发可简单采用关闭中断的方式，即在标志寄存器中关闭/打开中断标志位，不需要自旋锁）。

对于多核处理器来说，检测到锁可用与设置锁状态两个动作需要实现为一个原子操作，如果分为两个原子操作，则可能一个线程在获得锁后设置锁前被其余线程抢到该锁，导致执行错误。这就需要原子库提供对原子变量“读-修改-写(Read-Modify-Write)”的原子操作，上文原子类型支持的操作中就提供了RMW(Read-Modify-Write)原子操作，比如a.exchange(val)与a.compare_exchange(expected,desired)。

标准库还专门提供了一个原子布尔类型std::atomic_flag，不同于所有 std::atomic 的特化，它保证是免锁的，不提供load()与store(val)操作，但提供了test_and_set()与clear()操作，其中test_and_set()就是支持RMW的原子操作，可用std::atomic_flag实现自旋锁的功能，代码如下：

```text
//atomic2.cpp 使用原子布尔类型实现自旋锁的功能
#include <thread>
#include <vector>
#include <iostream>
#include <atomic>
 
std::atomic_flag lock = ATOMIC_FLAG_INIT;       //初始化原子布尔类型
 
void f(int n)
{
    for (int cnt = 0; cnt < 100; ++cnt) {
        while (lock.test_and_set(std::memory_order_acquire))  // 获得锁
             ; // 自旋
        std::cout << n << " thread Output: " << cnt << '\n';
        lock.clear(std::memory_order_release);               // 释放锁
    }
}
 
int main()
{
    std::vector<std::thread> v;     //实例化一个元素类型为std::thread的向量
    for (int n = 0; n < 10; ++n) {
        v.emplace_back(f, n);       //以参数(f,n)为初值的元素放到向量末尾，相当于启动新线程f(n)
    }
    for (auto& t : v) {     //遍历向量v中的元素，基于范围的for循环，auto&自动推导变量类型并引用指针指向的内容
        t.join();           //阻塞主线程直至子线程执行完毕
    }
    getchar();
    return 0;
}
```

自旋锁除了使用atomic_flag的TAS(Test And Set)原子操作实现外，还可以使用普通的原子类型std::atomic实现：其中a.exchange(val)是支持TAS原子操作的，a.compare_exchange(expected,desired)是支持CAS(Compare And Swap)原子操作的，感兴趣可以自己实现出来。其中CAS原子操作是无锁编程的主要实现手段，我们接着往下介绍无锁编程。

## 3.如何进行无锁编程

### 3.1 什么是无锁编程

在原子操作出现之前，对共享数据的读写可能得到不确定的结果，所以多线程并发编程时要对使用锁机制对共享数据的访问过程进行保护。但锁的申请释放增加了访问共享资源的消耗，且可能引起线程阻塞、锁竞争、死锁、优先级反转、难以调试等问题。

现在有了原子操作的支持，对单个基础数据类型的读、写访问可以不用锁保护了，但对于复杂数据类型比如链表，有可能出现多个核心在链表同一位置同时增删节点的情况，这将会导致操作失败或错序。所以我们在对某节点操作前，需要先判断该节点的值是否跟预期的一致，如果一致则进行操作，不一致则更新期望值，这几步操作依然需要实现为一个RMW(Read-Modify-Write)原子操作，这就是前面提到的CAS(Compare And Swap)原子操作，它是无锁编程中最常用的操作。

既然无锁编程是为了解决锁机制带来的一些问题而出现的，那么无锁编程就可以理解为不使用锁机制就可保证多线程间原子变量同步的编程。无锁(lock-free)的实现只是将多条指令合并成了一条指令形成一个逻辑完备的最小单元，通过兼容CPU指令执行逻辑形成的一种多线程编程模型。

无锁编程是基于原子操作的，对基本原子类型的共享访问由load()与store(val)即可保证其并发同步，对抽象复杂类型的共享访问则需要更复杂的CAS来保证其并发同步，并发访问过程只是不使用锁机制了，但还是可以理解为有锁止行为的，其粒度很小，性能更高。对于某个无法实现为一个原子操作的并发访问过程还是需要借助锁机制来实现。

### 3.2 CAS原子操作实现无锁编程

CAS原子操作主要是通过函数a.compare_exchange(expected,desired)实现的，其语义为“我认为V的值应该为A，如果是，那么将V的值更新为B，否则不修改并告诉V的值实际为多少”，CAS算法的实现伪码如下：

```text
bool compare_exchange_strong(T& expected, T desired) 
{ 
    if( this->load() == expected ) { 
        this->store(desired); 
        return true; 
    } else {
        expected = this->load();
    	return false; 
    } 
}
```

下面尝试实现一个无锁栈，代码如下：

```text
//atomic3.cpp 使用CAS操作实现一个无锁栈
#include <atomic>
#include <iostream>
template<typename T>
class lock_free_stack
{
private:
    struct node
    {
        T data;
        node* next;
        node(const T& data) : data(data), next(nullptr) {}
    };
    std::atomic<node*> head;
 public:
    lock_free_stack(): head(nullptr) {}
    void push(const T& data)
    {
        node* new_node = new node(data);
        do{
            new_node->next = head.load();   //将 head 的当前值放入new_node->next
        }while(!head.compare_exchange_strong(new_node->next, new_node));
        // 如果新元素new_node的next和栈顶head一样，证明在你之前没人操作它，使用新元素替换栈顶退出即可；
        // 如果不一样，证明在你之前已经有人操作它，栈顶已发生改变，该函数会自动更新新元素的next值为改变后的栈顶；
        // 然后继续循环检测直到状态1成立退出；
    }
    T pop()
    {
        node* node;
        do{
            node = head.load();
        }while (node && !head.compare_exchange_strong(node, node->next));
        if(node) 
            return node->data;
    }
};
 
int main()
{
    lock_free_stack<int> s;
    s.push(1);
    s.push(2);
    s.push(3);
    std::cout << s.pop() << std::endl;
    std::cout << s.pop() << std::endl;
    
    getchar();
    return 0;
}
```

程序注释中已经解释的很清楚了，在将数据压栈前，先通过比较原子类型head与新元素的next指向对象是否相等来判断head是否已被其他线程修改，根据判断结果选择是继续操作还是更新期望，而这一切都是在一个原子操作中完成的，保证了在不使用锁的情况下实现共享数据的并发同步。

CAS 看起来很厉害，但也有缺点，最著名的就是 ABA 问题，假设一个变量 A ，修改为 B之后又修改为 A，CAS 的机制是无法察觉的，但实际上已经被修改过了。如果在基本类型上是没有问题的，但是如果是引用类型呢？这个对象中有多个变量，我怎么知道有没有被改过？聪明的你一定想到了，加个版本号啊。每次修改就检查版本号，如果版本号变了，说明改过，就算你还是 A，也不行。

上面的例子节点指针也属于引用类型，自然也存在ABA问题，比如在线程2执行pop操作，将A,B都删掉，然后创建一个新元素push进去，因为操作系统的内存分配机制会重复使用之前释放的内存，恰好push进去的内存地址和A一样，我们记为A’，这时候切换到线程1，CAS操作检查到A没变化成功将B设为栈顶，但B是一个已经被释放的内存块。该问题的解决方案就是上面说的通过打标签标识A和A’为不同的指针，具体实现代码读者可以尝试实现。

原文地址：https://zhuanlan.zhihu.com/p/149464798

作者：linux

# 【NO.562】Linux网络编程——tcp并发服务器（多线程）实例分享

tcp多线程并发服务器

多线程服务器是对多进程服务器的改进，由于多进程服务器在创建进程时要消耗较大的系统资源，所以用线程来取代进程，这样服务处理程序可以较快的创建。据统计，创建线程与创建进程要快 10100 倍，所以又把线程称为“轻量级”进程。线程与进程不同的是：一个进程内的所有线程共享相同的全局内存、全局变量等信息，这种机制又带来了同步问题。

tcp多线程并发服务器框架:



![img](https://pic2.zhimg.com/80/v2-d429c123c5a669463e65af2789909b65_720w.webp)



我们在使用多线程并发服务器时，直接使用以上框架，我们仅仅修改client_fun()里面的内容。

代码示例：

> \#include <stdio.h>
> \#include <stdlib.h>
> \#include <string.h>
> \#include <unistd.h>
> \#include <sys/socket.h>
> \#include <netinet/in.h>
> \#include <arpa/inet.h>
> \#include <pthread.h>
> /************************************************************************
> 函数名称： void *client_fun(void *arg)
> 函数功能： 线程函数,处理客户信息
> 函数参数： 已连接套接字
> 函数返回： 无
> ************************************************************************/
> **void** *client_fun(**void** *arg)
> {
> **int** recv_len = 0;
> **char** recv_buf[1024] = ""; // 接收缓冲区
> **int** connfd = (**int**)arg; // 传过来的已连接套接字
> // 接收数据
> **while**((recv_len = recv(connfd, recv_buf, **sizeof**(recv_buf), 0)) > 0)
> {
> printf("recv_buf: %s\n", recv_buf); // 打印数据
> send(connfd, recv_buf, recv_len, 0); // 给客户端回数据
> }
>
> printf("client closed!\n");
> close(connfd); //关闭已连接套接字
> **return** NULL;
> }
> //===============================================================
> // 语法格式： void main(void)
> // 实现功能： 主函数，建立一个TCP并发服务器
> // 入口参数： 无
> // 出口参数： 无
> //===============================================================
> **int** main(**int** argc, **char** *argv[])
> {
> **int** sockfd = 0; // 套接字
> **int** connfd = 0;
> **int** err_log = 0;
> **struct** sockaddr_in my_addr; // 服务器地址结构体
> unsigned **short** port = 8080; // 监听端口
> pthread_t thread_id;
>
> printf("TCP Server Started at port %d!\n", port);
>
> sockfd = socket(AF_INET, SOCK_STREAM, 0); // 创建TCP套接字
> **if**(sockfd < 0)
> {
> perror("socket error");
> exit(-1);
> }
>
> bzero(&my_addr, **sizeof**(my_addr)); // 初始化服务器地址
> my_addr.sin_family = AF_INET;
> my_addr.sin_port = htons(port);
> my_addr.sin_addr.s_addr = htonl(INADDR_ANY);
>
> printf("Binding server to port %d\n", port);
>
> // 绑定
> err_log = bind(sockfd, (**struct** sockaddr*)&my_addr, **sizeof**(my_addr));
> **if**(err_log != 0)
> {
> perror("bind");
> close(sockfd);
> exit(-1);
> }
>
> // 监听，套接字变被动
> err_log = listen(sockfd, 10);
> **if**( err_log != 0)
> {
> perror("listen");
> close(sockfd);
> exit(-1);
> }
>
> printf("Waiting client...\n");
>
> **while**(1)
> {
> **char** cli_ip[INET_ADDRSTRLEN] = ""; // 用于保存客户端IP地址
> **struct** sockaddr_in client_addr; // 用于保存客户端地址
> socklen_t cliaddr_len = **sizeof**(client_addr); // 必须初始化!!!
>
> //获得一个已经建立的连接 connfd = accept(sockfd, (**struct** sockaddr*)&client_addr, &cliaddr_len);
> **if**(connfd < 0)
> {
> perror("accept this time");
> **continue**;
> }
> // 打印客户端的 ip 和端口
> inet_ntop(AF_INET, &client_addr.sin_addr, cli_ip, INET_ADDRSTRLEN);
> printf("----------------------------------------------\n");
> printf("client ip=%s,port=%d\n", cli_ip,ntohs(client_addr.sin_port));
>
> **if**(connfd > 0)
> {
> //由于同一个进程内的所有线程共享内存和变量，因此在传递参数时需作特殊处理，值传递。
> pthread_create(&thread_id, NULL, (**void** *)client_fun, (**void** *)connfd); //创建线程
> pthread_detach(thread_id); // 线程分离，结束时自动回收资源
> }
> }
>
> close(sockfd);
>
> **return** 0;
> }

运行结果：



![img](https://pic2.zhimg.com/80/v2-b77b2f9c3fe0bf19619980f2c4d48101_720w.webp)



注意：

1.上面pthread_create()函数的最后一个参数是void *类型，为啥可以传值connfd？

> **while**(1)
> {
> **int** connfd = accept(sockfd, (**struct** sockaddr*)&client_addr, &cliaddr_len);
> pthread_create(&thread_id, NULL, (**void** *)client_fun, (**void** *)connfd);
> pthread_detach(thread_id);
> }

因为void *是4个字节，而connfd为int类型也是4个字节，故可以传值。如果connfd为char、short,上面传值就会出错

2.上面pthread_create()函数的最后一个参数是可以传地址吗？可以，但会对服务器造成不可预知的问题

**while**(1)

{

**int** connfd = accept(sockfd, (**struct** sockaddr*)&client_addr, &cliaddr_len);

pthread_create(&thread_id, NULL, (**void** *)client_fun, (**void** *)&connfd);

pthread_detach(thread_id);

}

原因：假如有多个客户端要连接这个服务器，正常的情况下，一个客户端连接对应一个 connfd，相互之间独立不受影响，但是，假如多个客户端同时连接这个服务器，A 客户端的连接套接字为 connfd，服务器正在用这个 connfd 处理数据，还没有处理完，突然来了一个 B 客户端，accept()之后又生成一个 connfd, 因为是地址传递， A 客户端的连接套接字也变成 B 这个了，这样的话，服务器肯定不能再为 A 客户端服务器了

2.如果我们想将多个参数传给线程函数，我们首先考虑到就是结构体参数，而这时传值是行不通的，只能传递地址。

这时候，我们就需要考虑多任务的互斥或同步问题了，这里通过互斥锁来解决这个问题，确保这个结构体参数值被一个临时变量保存过后，才允许修改。

> \#include <pthread.h>
>
> pthread_mutex_t mutex; // 定义互斥锁，全局变量
>
> pthread_mutex_init(&mutex, NULL); // 初始化互斥锁，互斥锁默认是打开的
>
> // 上锁，在没有解锁之前，pthread_mutex_lock()会阻塞
> pthread_mutex_lock(&mutex);
> **int** connfd = accept(sockfd, (**struct** sockaddr*)&client_addr, &cliaddr_len);
>
> //给回调函数传的参数，&connfd，地址传递
> pthread_create(&thread_id, NULL, (**void** *)client_process, (**void** *)&connfd); //创建线程
>
> // 线程回调函数
> **void** *client_process(**void** *arg)
> {
> **int** connfd = *(**int** *)arg; // 传过来的已连接套接字
>
> // 解锁，pthread_mutex_lock()唤醒，不阻塞 pthread_mutex_unlock(&mutex);
>
> **return** NULL;
> }

示例代码：

> \#include <stdio.h>
> \#include <stdlib.h>
> \#include <string.h>
> \#include <unistd.h>
> \#include <sys/socket.h>
> \#include <netinet/in.h>
> \#include <arpa/inet.h>
> \#include <pthread.h>
>
> pthread_mutex_t mutex; // 定义互斥锁，全局变量
>
> /************************************************************************
> 函数名称： void *client_process(void *arg)
> 函数功能： 线程函数,处理客户信息
> 函数参数： 已连接套接字
> 函数返回： 无
> ************************************************************************/
> **void** *client_process(**void** *arg)
> {
> **int** recv_len = 0;
> **char** recv_buf[1024] = ""; // 接收缓冲区
> **int** connfd = *(**int** *)arg; // 传过来的已连接套接字
>
> // 解锁，pthread_mutex_lock()唤醒，不阻塞
> pthread_mutex_unlock(&mutex);
>
> // 接收数据
> **while**((recv_len = recv(connfd, recv_buf, **sizeof**(recv_buf), 0)) > 0)
> {
> printf("recv_buf: %s\n", recv_buf); // 打印数据
> send(connfd, recv_buf, recv_len, 0); // 给客户端回数据
> }
>
> printf("client closed!\n");
> close(connfd); //关闭已连接套接字
>
> **return** NULL;
> }
>
> //===============================================================
> // 语法格式： void main(void)
> // 实现功能： 主函数，建立一个TCP并发服务器
> // 入口参数： 无
> // 出口参数： 无
> //===============================================================
> **int** main(**int** argc, **char** *argv[])
> {
> **int** sockfd = 0; // 套接字
> **int** connfd = 0;
> **int** err_log = 0;
> **struct** sockaddr_in my_addr; // 服务器地址结构体
> unsigned **short** port = 8080; // 监听端口
> pthread_t thread_id;
>
> pthread_mutex_init(&mutex, NULL); // 初始化互斥锁，互斥锁默认是打开的
>
> printf("TCP Server Started at port %d!\n", port);
>
> sockfd = socket(AF_INET, SOCK_STREAM, 0); // 创建TCP套接字
> **if**(sockfd < 0)
> {
> perror("socket error");
> exit(-1);
> }
>
> bzero(&my_addr, **sizeof**(my_addr)); // 初始化服务器地址
> my_addr.sin_family = AF_INET;
> my_addr.sin_port = htons(port);
> my_addr.sin_addr.s_addr = htonl(INADDR_ANY);
>
>
> printf("Binding server to port %d\n", port);
>
> // 绑定
> err_log = bind(sockfd, (**struct** sockaddr*)&my_addr, **sizeof**(my_addr));
> **if**(err_log != 0)
> {
> perror("bind");
> close(sockfd);
> exit(-1);
> }
>
> // 监听，套接字变被动
> err_log = listen(sockfd, 10);
> **if**( err_log != 0)
> {
> perror("listen");
> close(sockfd);
> exit(-1);
> }
>
> printf("Waiting client...\n");
>
> **while**(1)
> {
> **char** cli_ip[INET_ADDRSTRLEN] = ""; // 用于保存客户端IP地址
> **struct** sockaddr_in client_addr; // 用于保存客户端地址
> socklen_t cliaddr_len = **sizeof**(client_addr); // 必须初始化!!!
>
> // 上锁，在没有解锁之前，pthread_mutex_lock()会阻塞
> pthread_mutex_lock(&mutex);
>
> //获得一个已经建立的连接
> connfd = accept(sockfd, (**struct** sockaddr*)&client_addr, &cliaddr_len);
> **if**(connfd < 0)
> {
> perror("accept this time");
> **continue**;
> }
>
> // 打印客户端的 ip 和端口
> inet_ntop(AF_INET, &client_addr.sin_addr, cli_ip, INET_ADDRSTRLEN);
> printf("----------------------------------------------\n");
> printf("client ip=%s,port=%d\n", cli_ip,ntohs(client_addr.sin_port));
>
> **if**(connfd > 0)
> {
> //给回调函数传的参数，&connfd，地址传递
> pthread_create(&thread_id, NULL, (**void** *)client_process, (**void** *)&connfd); //创建线程
> pthread_detach(thread_id); // 线程分离，结束时自动回收资源
> }
> }
>
> close(sockfd);
>
> **return** 0;
> }

运行结果：



![img](https://pic2.zhimg.com/80/v2-944dd7b5f73b0d01f40ccced2cda4919_720w.webp)



注意：这种用互斥锁对服务器的运行效率有致命的影响

原文地址：https://zhuanlan.zhihu.com/p/96531501

作者：linux

# 【NO.563】linux下wait/waitpid处理僵死进程详解(SIGCHLD信号)

## 1.僵尸进程处理

客户正常断开但服务器未处理SIGCHLD信号，会使得服务器子进程僵死。

**设置僵尸进程的目的：**

维护子进程信息，以便父进程在以后某时候获取。信息包括子进程ID，终止状态，资源利用信息（CPU时间、内存使用量等）。如果某进程结束，而该进程有子进程处于僵死状态，那么它的所有僵死子进程的父进程ID会变为1（init进程），init进程会清理它们，即init进程会wait它们，去除其僵死状态。

**处理僵死进程方法：**

（1）忽略SIGCHLD信号可以防止僵尸进程，在server端main函数listen之后添加： signal(SIGCHLD, SIG_IGN);

如下是代码：

```text
//使用sigaction函数取代unix早期版本的signal函数的实现
typedef void Sigfunc(int);
Sigfunc *
Signal(int signo, Sigfunc *func)
{
   struct sigaction act, oact;
   act.sa_handler = func;
   sigemptyset(&act.sa_mask);
   act.sa_flags = 0;
   if (signo == SIGALRM) {
#ifdef SA_INTERRUPT
      act.sa_flags |= SA_INTERRUPT;
#endif
   }
   else {
#ifdef SA_RESTART  //如果系统可以重启被中断的系统调用的话，被中断的系统调用将由内核重启
      act.sa_flags |= SA_RESTART;
#endif
   }
   if (sigaction(signo, &act, &oact) < 0)
      return(SIG_ERR);
   return(oact.sa_handler);
}
```

（2）通过wait/waitpid方法。

```text
//使用waitpid的信号处理函数
void onSignalCatch(int signalNumber)
{
    pid_t pid;
    pid = wait(NULL);
    printf("child %d terminated.\n", pid);
    return;
 }
```

在服务器端listen调用后加入：

```text
Signal(SIGCHLD, onSignalCatch); //这个处理信号的函数必须在fork第一个子进程之前完成，且仅做一次
```

再次执行程序，三个客户端正常终止后，服务器端结果如下：

![img](https://pic1.zhimg.com/80/v2-de1c996f30f6d0a093a4b0cef2099314_720w.webp)

上述的方法调用wait的函数onSignalCatch作为信号处理函数，其正常结束的过程如下：

（1）键入EOF终止客户。客户TCP发送FIN给服务器，服务器响应ACK。

（2）收到客户FIN的服务器TCP发送EOF给子进程阻塞的read，子进程结束。

（3）当SIGCHLD信号递交时，父进程阻塞于accept调用。信号处理函数onSignalCatch执行，其wait调用子进程pid并打印、返回。

（4）信号是父进程在**阻塞于慢系统调用accept时捕获**的，所以，内核就会使accept返回EINTR错误（被中断的系统调用）。而**父进程不处理该错误，被中断的系统调用重启（上述自定义的Signal函数中：设置了SA_RESTART标志），所以accept没返回错误**。

## 2.处理被中断的系统调用

accept被称为**慢系统调用**，多数网络支持函数都属于这一类。**适用于慢系统调用的基本规则是**：当阻塞于某个慢系统调用的进程捕获某个信号且相应信号处理函数返回后，该系统调用可能返回一个EINTR错误。

从上图可以看出，三次客户的终止都没有使得阻塞于accept的服务器终止；因为内核会自动重启被中断的系统调用。而有些系统的标准C函数库中的signal函数不会使得内核自动重启被中断的系统调用，也就是SA_RESTART标志在系统函数的signal函数中没有被设置，在这些系统中服务器程序将终止。
那么并非所有的系统都会将被中断的系统调用重启，所以要处理被中断的系统调用，将accept的调用改为：

```text
if ((connfd = accept(listenfd, (struct sockaddr*) &cliaddr, &clilen)) < 0) {
          if (errno == EINTR)
             continue;  //有些系统不会重启被这中断的系统调用，所以要处理
          else
             err_exit("accept error, server.\n");
      }
```

## 3.wait函数和waitpid函数

```text
#include <sys/wait.h>
pid_t wait(int *statloc);
pid_t waitpid(pid_t pid, int *statloc, int options);  //参数pid允许指定想等待的进程ID，为-1表示等待第一个终止的子进程
//均返回：成功返回进程ID，出错返回0或-1
//statloc指针返回子进程终止状态（一个整数）
//options是附加选项，最常用的是WNOHANG，告知内核在有尚未终止的子进程在运行时不阻塞
```



![img](https://pic3.zhimg.com/80/v2-95785cf20abd74c14b8f278e1f24cbb2_720w.webp)



![img](https://pic4.zhimg.com/80/v2-dba55148256ccc6c8e9777b16a838de3_720w.webp)

**问题：**

在上述情况下，建立信号处理函数在其中调用wait仍然会有僵死进程。问题在于：所有5个信号都在信号处理函数执行前产生；而在一台机器上运行客户和服务器端，信号处理函数只执行一次，因为unix信号一般是不排队的。如果在不同机器上运行客户端服务器端，信号处理函数执行次数不确定，对于留下几个僵死进程以及哪几个会僵死都是不确定的。

如下图分别为创建的5个连接、客户端exit后服务端的四个僵死子进程：

![img](https://pic3.zhimg.com/80/v2-8fe7edbd31b9c3812bdb7f588cc564a2_720w.webp)

![img](https://pic4.zhimg.com/80/v2-544b8793eb8828bcafa853aab5a27b67_720w.webp)

**解决方法：**

使用waitpid而不是wait。如下的处理函数管用，因为在一个循环内调用waitpid，以获取进程终止状态。**waitpid函数的参数options指定为WNOHANG**，告知waitpid在尚有未终止的子进程时，不阻塞。而使用wait无法阻止其在还有子进程未结束时阻塞。

```text
//使用waitpid的信号处理函数
void onSignalCatch(int signalNumber)
{
    pid_t pid;
    int stat;
    //pid = wait(&stat);
    //下列函数的第一个参数为－1表示等待第一个终止的子进程
    while ((pid = waitpid(-1, &stat, WNOHANG)) > 0) 
        printf("child %d terminated.\n", pid);
    return;
 }
```

结果如下：

![img](https://pic3.zhimg.com/80/v2-31fed25821890af77f75cc25349374b6_720w.webp)

![img](https://pic3.zhimg.com/80/v2-28ab44b7b1e169a8c2d02047b48314d6_720w.webp)

至此，客户服务器代码就完成了，加入了对僵死进程的处理。

原文地址：https://zhuanlan.zhihu.com/p/272700092

作者：linux

# 【NO.564】从TCP协议到TCP通信的各种异常现象和分析

很多人总觉得学习TCP/IP协议没什么用，**觉得**日常编程开发只需要知道socket接口怎么用就可以了。**如果大家定位过线上问题就会知道，实际上并非如此**。如果应用在局域网内，且设备一切正常的情况下可能确实如此，但如果一旦出现诸如中间交换机不稳定、物理服务器宕机或者其它异常情况时，此时引起的问题如果只停留在套接字接口的理解层面将无法解决。因此，深入理解TCP/IP协议**，对我们分析异常问题有很大的帮助**。

下图是网络通信中常见的架构，也就是CS架构。其中程序包括两部分，分别为客户端（Client）和服务端（Server）。当然，实际的环境还要复杂的多，在客户端和服务端之间可能有多种不同种类和数量的设备，这些设备都会增加网络通信的复杂性。自然，也会增加程序开发容错的复杂性。

![img](https://pic2.zhimg.com/80/v2-1d9959a443cc821615f8ca8069a0d891_720w.webp)

图1 基本架构

## 1.TCP的基本流程

在分析异常情况之前，我们先回忆一下TCP协议的基本逻辑。在客户端和服务端能够收发数据之前首先必需建立连接。连接的建立在协议层面也是通过收发数据包完成，只不过在用户层面就是客户端调用了一个connect函数。连接的过程俗称“三次握手”，具体流程如图2所示。

![img](https://pic4.zhimg.com/80/v2-95019dcc6544bb9630821e925fbb0dcf_720w.webp)

图2 TCP的三次握手流程

TCP连接的断开也是比较复杂的，需要经过所谓的“四次挥手”的流程。其原因是因为TCP是双工通信，分别需要从客户端和服务端2侧断开连接。

![img](https://pic4.zhimg.com/80/v2-6f546dc14bbec738eb5e94332a3823f7_720w.webp)

图3 TCP的四次挥手

另外一个比较重要的内容是TCP协议的状态转换，理解了这个内容，我们才能清楚出现各种异常情况下数据包的内容。

![img](https://pic4.zhimg.com/80/v2-0639084c5305508f1c0276f4791d3b27_720w.webp)

图4 TCP状态转换图

本文只是简单回忆一下TCP的基本流程

## 2.异常情况分析

了解了TCP的基本流程之后，我们再看一下各种异常情况。这些异常情况才是我们在后续解决问题的时候的关键。了解了这些异常情况及原理，后面解决问题才能游刃有余。

**1. 试图与一个不存在的端口建立连接（主机正常）**

这里的不存在的端口是指在**服务器端没有程序监听在该端口**。我们的客户端就调用connect，试图与其建立连接。这时会发生什么呢？

这种情况下我们在客户端通常会收到如下异常内容：

[Errno 111] Connection refused（连接拒绝）

具体含义可以查一下Linux的相关手册，或者用搜索引擎搜索一下。试想一下，服务端本来就没有程序监听在这个接口，因此在服务端是无法完成连接的建立过程的。我们参考‘三次握手’的流程可以知道当客户端的SYNC包到达服务端时，TCP协议没有找到监听的套接字，就会向客户端发送一个错误的报文，告诉客户端产生了错误。而该错误报文就是一个包含RST的报文。这种异常情况也很容易模拟，我们只需要写一个小程序，连接服务器上没有监听的端口即可。如下是通过wireshark捕获的数据包，可以看到红色部分的RST报文。

![img](https://pic4.zhimg.com/80/v2-9dcaa89b6e3e9839211e3e079d643893_720w.webp)

图5 数据包截图

**继续深入理解一下**，在操作系统层面，**TCP的服务端实际上就是从网卡的寄存器中读取数据，然后进行解析**。对于TCP自然会解析出目的端口这个关键信息，然后根据这个信息查看有没有这样的套接字。这个套接字是什么呢？在用户层面是一个文件句柄，但在内核中实际是一个数据结构，里面记录了很多信息。这个数据结构存储在一个哈希表中，通过函数**__inet_lookup_skb**（net/inet_hashtables.h）可以实现对该数据结构的查找。对于上述情况，自然无法找到该套接字，因此TCP服务端会进行错误处理，处理的方式就是给客户端发送一个RST（通过函数tcp_v4_send_reset进行发送）。

**2. 试图与一个某端口建立连接但该主机已经宕机（主机宕机）**

这也是一种比较常见的情况，当某台服务器主机宕机了，而客户端并不知道，仍然尝试去与其建立连接。这种场景也是分为2种情况的，一种是刚刚宕机，另外一种是宕机了很长时间。**为什么要分这2种情况？**

这主要根ARP协议有关系，ARP会在本地缓存失效，TCP客户端就无法想目的服务端发送数据包了。

(192.168.1.100) 位于 08:00:27:1a:7a:0a [ether] 在 eth0

了解了上述情况，我们分析一下刚刚宕机的情况，此时客户端是可以向服务端发送数据包的。但是由于服务器宕机，因此不会给客户端发送任何回复。

![img](https://pic4.zhimg.com/80/v2-dee14f1219dfaec8f7595e052ae36c53_720w.webp)

图6 数据包截图

由于客户端并不知道服务端宕机，因此会重复发送SYNC数据包，如图6所示，可以看到客户端每隔几秒会向服务端发送一个SYNC数据包。这里面具体的时间是跟TCP协议相关的，具体时间不同的操作系统实现可能稍有不同。

**3. 建立连接时，服务器应用被阻塞（或者僵死）**

还有一种情况是在客户端建立连接的过程中服务端应用处于僵死状态，这种情况在实际中也会经常出现（我们假设仅仅应用程序僵死，而内核没有僵死）。此时会出现什么状态？TCP的三次是否可以完成？客户端是否可以收发数据？

在用户层面我们知道，**服务端通过accept接口返回一个新的套接字**，这时就可以和客户端进行数据往来了。也就是在用户层面来说，accept返回结果说明3次握手完成了，否则accept会被阻塞。在我们假设的情况下，其实就相当于应用程序无法进行accept操作了。

如果想彻底理解上面我们假设的问题，需要理解两点，一点是accept函数具体做了什么，另外一点是TCP三次握手的本质。

我们先试着理解第一点，accept会通过软中断陷入内核中，最终会调用tcp协议的inet_csk_accept函数，该函数会从队列中查找是否有处于ESTABLISHED状态的套接字。如果有则返回该套接字，否则阻塞当前进程。也就是说这里只是一个查询的过程，并不参与三次握手的任何逻辑。

三次握手的本质是什么呢？实际上就是客户端与服务端一个不断交流的过程，而这个交流过程就是通过3个数据包完成的。而这个数据包的发送和处理实际上都是在内核中完成的。对于TCP的服务端来说，当它收到SYNC数据包时，就会**创建一个套接字的数据结构**并给客户端回复ACK，再次收到客户端的ACK时会将套接字数据结构的状态转换为ESTABLISHED，并将其发送就绪队列中。而这整个过程跟应用程序没有半毛钱的关系。

当上面套接字加入就绪队列时，accept函数就被唤醒了，然后就可以获得新的套接字并返回。但我们回过头来看一下，在accept返回之前，其实三次握手已经完成，也就是连接已经建立了。

![img](https://pic1.zhimg.com/80/v2-d409bebab10c4b6a375d6708c8c68174_720w.webp)

图7 TCP缓存与应用进程

另外一个是如果accept没有返回，客户端是否可以发送数据？答案是可以的。因为数据的发送和接受都是在内核态进行的。客户端发送数据后，服务端的网卡会先接收，然后通过中断通知IP层，再上传到TCP层。TCP层根据目的端口和地址将数据存入关联的缓冲区。如果此时应用程序有读操作（例如read或recv），那么数据会从内核态的缓冲区拷贝到用户态的缓存。否则，数据会一直在内核态的缓冲区中。**总的来说，TCP的客户端是否可以发送数据与服务端程序是否工作没有任何关系。**

当然，如果是整个机器都卡死了，那就是另外一种情况了。这种情况就我们之前分析的第2种情况一直了。因为，由于机器完全卡死，TCP服务端无法接受任何消息，自然也无法给客户端发送任何应答报文。

上面分析了在连接过程中的各种异常，下面**重点介绍的是在数据传输过程中的各种异常**，以及出现异常后的TCP连接的情况。

## 3.异常情况分析

这里的分析假设连接已经建立，目前正在数据收发过程。这种情况下会出现各种异常，比如服务器宕机、进程crash或者进程被kill等等。下面我们分别介绍上述集中情况在TCP通信中的表现。

**服务进程crash**

服务进程crash恐怕是我们日常生成环境最长遇到的情况，没有之一吧。**那么在这种情况下客户端软件是什么反应？客户端是否可以感知？**

我们分别写客户端和服务端的程序，客户端不断的发送数据，服务端接收数据。异常的模拟很简单，我们可以在服务端制造一个指针访问异常。此时服务端的程序就会crash掉。然后我们观察客户端的表现。先上结果，客户端的表现如下图所示。

![img](https://pic4.zhimg.com/80/v2-be0eb7201279dcc345518401ea7a8323_720w.webp)

可以看到客户端被reset掉了。我们在结合通过wireshark抓获的此时的数据报文内容，可以看到是一个RST报文。

![img](https://pic1.zhimg.com/80/v2-4eff5b81fc5e6eea4e435267c62af47c_720w.webp)

回忆一下什么情况下服务端会发送RST报文。这种场景跟我们前文介绍的服务端没有监听的情况是类似的。由于服务端程序crash了，此时在操作系统中的套接字数据结构已经被释放，因此在协议层收到数据包的时候无法找到对应的套接字进行处理，于是发送了一个RST报文。

**手动杀死服务端应用**

这也是线上比较常见的操作，当一个模块上线时，ops同学总是会先把旧的进程杀死，然后再启动新的进程。**那么在这个过程中TCP连接又会发生了什么呢？是否会像上一种情况一样被RST呢？**同样，我们先看一下结果，如下是客户端的情况。

![img](https://pic2.zhimg.com/80/v2-85eececc3725a9885f5cf8e6e403ef81_720w.webp)

从上面错误码来看是管道破裂，其实也就是连接被中断了。我们再看一下通过wireshark的抓包结果可以看出服务端发送了一个FIN报文，这个报文表示服务端发起了关闭的请求。而接下来的一个报文是客户端对该请求的确认。

![img](https://pic4.zhimg.com/80/v2-ecd2976e18d4e601fcc7d1ab125a98fb_720w.webp)

所以，从上面客户端的错误码和报文情况我们可以知道，在kill进程时TCP协议是能够感知到的，并且发送的FIN报文。

我们再进一步的思考一下，**为什么kill进程会有FIN呢？这个与前面crash的差异在哪？**其实kill进程是通过shell想内核发送了SIGKILL或者SIGTERM，内核接收到该信号之后会进行相应的扫尾工作，因此可以看到服务端发送了FIN报文。

**Server进程所在的主机关机**

主机关机（这里指手动关机）的情况与进程被kill是类似的。这时因为在系统关闭时，init进程会给所有进程发送SIGTERM信号，等待一段时间（5~20秒），然后再给所有仍在运行的进程发送SIGKILL信号。当服务器进程死掉时，会关闭所有文件描述符。带来的影响和上面杀死server相同。

**Server进程所在的主机宕机**

这是我们线上另一种比较常见的状况。即使宕机是一个小概率事件，线上几千台服务器动不动一两台挂掉也是常有的事。这里挂掉其实包括2种情况，**一种是内核panic，另外一种情况是出现了掉电**。对于内核panic的情况不会像关机那样会预先杀死上面的进程，而是突然性的。那么此时我们的客户端准备给服务器端发送一个请求，它由write写入内核，由TCP作为一个报文发出，但因为主机已经挂掉，因此客户端无法收到ACK。于是客户端TCP持续重传分节，试图从服务器上接收一个ACK，然而服务器始终不能应答，重传数次之后，**大约几分钟才停止**，之后返回一个ETIMEDOUT错误。在这种情况下，如果我们调用的是同步发送接口，则在发送缓冲区慢的情况下会阻塞在这里，导致程序阻塞。

这个时间真的很长，**对于某些应用这种长时间的卡顿是不能接受的**。因此，需要一种手段处理这种情况，在套接字接口中可以通过SO_SNDTIMEO标记进行设置。但是有利也有弊，如果设置了该参数，可能会出现这的数据发送超时的情况，进而出现向服务端发送重复数据的情况，此时需要服务端做去重处理。

**服务器进程所在的主机宕机后重启**

在客户端发出请求前，服务器端主机经历了**宕机—重启**的过程。当客户端TCP把分节发送到服务器端所在的主机，服务器端所在主机的TCP丢失了崩溃前所有连接信息，即TCP收到了一个根本不存在连接上（也就是我们前文介绍的查找不到socket数据结构）的报文，所以会响应一个RST分节。

至此，关于TCP协议中各种异常情况介绍完了，详细了解这些内容后对后续线上问题的分析和解决会有很大的帮助。

原文地址：https://zhuanlan.zhihu.com/p/454474331

作者：linux

# 【NO.565】低延迟场景下的性能优化实践

Scott Meyers 曾说到过，如果你不在乎性能，为什么要在C++这里，而不去隔壁的 Python room 呢？

今天我们就从“低延迟的概述”、“低延迟系统调整”、“低延迟系统编译选项”、“低延迟软件设计与编码”四个部分来聊聊低延迟场景下的性能优化实践。

## 1.**低延迟概述**

**低延迟场景**

很多系统都会关注延迟，比如：电信系统、游戏行业、音视频解码，或者一些金融系统。这里我们就以金融场景为例。

在程序化交易系统下，为什么需要关注低延迟？

![img](https://pic2.zhimg.com/80/v2-5ded1e625959bdf44f1892a87de408f9_720w.webp)

程序化交易系统是接收市场的行情再去进行运算，然后发出交易信号。发出交易信号越早，就越可能挣到钱，如果晚了，钱都被别人挣了，自己可能就会亏钱。所以在这种场景下，**低延迟是第一需求，不会追求吞吐量**。交易所都有流速权，即每秒的报单速度是有限的，不允许做很大的吞吐，所以金融对低延迟的要求是比较高的，也**不在意资源利用率**。因为我们的 CPU 会进行绑核，绑核会让 CPU 处于 busy looping，让它的占有率达到100%，那么资源利用率就没有任何参考价值。

当然，程序化交易系统资源都是超配的，比如内存、硬盘，虽然 CPU 没有超配这一说，但尽可能配最好的。

**低延迟优化特点**

**![img](https://pic3.zhimg.com/80/v2-e1939fd2746bbb05c66eb6208cab405e_720w.webp)**

常用的性能优化就是做一些压力测试、关注一下QPS、看看系统负载需不需要内存、使用率怎么样，用 perf 工具去找出程序的热点。“不成熟的优化是万恶之源。”Profile 就是一个非常好的优化工具。

但对低延迟性能优化来说，Profile 可能就不是特别关键了。低延迟系统有追求延迟的线程，也有不追求延迟的、没那么 critical 的线程。critical 线程在我们系统整个代码量中并不是特别大，这种情况下用 Profile 的数据是不准的，Profile 工具是采样的，延迟很低就更难采到。所以在系统、设计、编码的层次上需要提前考虑低延迟，也会提前规划好哪些代码要走 critical path 并对它进行优化。还要测试各单元的延迟，这个延迟可能是一个 tick-to-trade，即从行情开始到最后交易完成的整个系统的延迟，也可能是各个模块、各个 function、各个语句块、甚至各条语句的延迟，最后再去优化 critical path。

**常见操作时延**

我们来看一组以前的操作数据。

![img](https://pic3.zhimg.com/80/v2-2aef9a474acf143e830aa1b2d4160c66_720w.webp)

从最底下开始看，Disk read 一旦涉及到磁盘就和延迟无关了，这个结果显然是不允许的。Context switch 是系统调用，在内核中会做很多操作，线程被调度出去再被调度回来，本身切换过程的耗时就非常大，再加上运行其他的线程，cache 可能都已经冷了，这里的其他开销可能就更难衡量。假如是 10K 的 CPU cycle，即便是10GHz 的超频服务器耗时也需要一个微秒，这在低延迟系统里已经是非常大的开销。这里的异常抛出和 cache 处理占的时间也比较长，如果代码进了内核态再切换回来，这个延迟也是非常可观的。

Allocation deallocation pair，这个延迟是指用 malloc/free 或 delete，申请内存的过程中会有内存管理器这一层，比如 Glibc ptmalloc，大多数情况下是不会系统调用，但它本身开销也很可观。如果你申请的内存本身core比较大，直接调用 mindmap，或者 Glibc 的缓存里没有 free 内存去分配，就会走到 kernel 再回来，这个时间开销就更大了。

内存读取包括主内存读取、NUMA 去读取另外一个节点的数据，性能开销都是很大的。Mutex 在低延迟代码里也基本不会用。至于函数调用，不可能一个都不用，但可以用 inline 来减少函数调用。除了普通的函数调用，还有多态调用，即vptr、vtable。Div操作是 CPU 不喜欢的。

CPU都是流水线执行的，"wrong" branch of "if"和“right" branch of "if"，就是 CPU 执行到一个 if-else 时会自己去猜，如果猜对了，就几乎可以忽略，如果猜错了，代价就比较惨。

## 2.**低延迟系统调整**

**硬件&系统**

首先既对处理器的核数有要求，同时也对单核的频率有要求，但这两点是矛盾的。想要一个核数又多、频率又高的，就要用到超频服务器，执行效率越高越好，不需要虚拟化功能。内存也要充足

![img](https://pic2.zhimg.com/80/v2-239de9037def0b4e80fd34d0744c1cb9_720w.webp)

超线程一般是关闭的，同一个程序在开超线程和不开超线程的机器跑的话，肯定是不开超线程的更快。另外，如果进行内核绑定，Critical thread 会独占一个核，如果绑定一个开了超线程的核就相当于绑定了同一个核，或者是一个核不用扔在那儿，这是没有意义的。

操作系统是64位的 Linux，一般是 CentOS 或是 RHEL，最小化安装，toolchain 升级，因为默认自带的可能是比较老的 GCC，我一般都习惯升级到9或10。

最小化安装还有一个比较有意思的点，因为我个人是坚定的 Emacs党，不喜欢 vim，但 Emacs 会默认安装一些图形化插件，所以要在你的生产机器上装 Emacs 的话就要装 Emacs-nw 版本。Rtkernel 看起来好像和低延迟实施有关系，但实际上它是保证一种硬抢占的内核 patch，这个对我们来说是完全不需要的。RHEL 一般都可以照着 Tuning guide 去对系统进行调整，如果是买服务器或超频服务器，vendor 也会有 guide，可以斟酌一下要不要打开。

**CPU相关优化**

![img](https://pic1.zhimg.com/80/v2-3d710c69a9a04d2a0f30520a1ad27120_720w.webp)

CPU 优化最核心的就是要让 Critical 线程独占 CPU，不能被打断，要求极致的低延迟，而普通线程就无所谓。我们要做的就是先把一定数量的核 isolate，这样操作系统就不会把任何的用户态线程再调度到这个核上，然后再做 thread bonding，把 Critical thread绑定到这些 isolate 的核里去，这样就保证了 thread 可以独占这个核。也可以设置 scheduler，对于 Critical thread 我们一般都是设置 FIFO 这种实时的优先级调度策略，对于普通的线程用 default(CFS) 就可以。

**中断**

![img](https://pic4.zhimg.com/80/v2-286538e598d5768470ef4b9cca45aa1b_720w.webp)

当遇到 kernel 中断、时钟中断或 workqueue 等情况时还是可能会侵占 CPU 时间，可以把中断的 balance 关掉，设置中断 affinity 到非 isolate 核心，这样可以让中断对你的影响尽可能地小。这里要提一下，时钟中断是不可能完全关闭的，除非改内核。

![img](https://pic3.zhimg.com/80/v2-4ee70c78d71175fa28c076578f7bda42_720w.webp)

网卡要绑定到相应的 slot，一般和 Critical 线程绑定的 slot 是同一个。

![img](https://pic3.zhimg.com/80/v2-3abc8caa6093d23f9aeb9139b2aed3ea_720w.webp)

内存优化也要避免进入内核态，一方面是分配的时候可能进入， 另一方面是触发 fault 的时候。

fault，对于Linux操作系统来说，在内核层面上是不区分线程和进程的，都是用 task_struct 来表示线程。进程和线程唯一的区别就是进程的 tid 和 pid 是相等的，因为一个进程的内存是共享的，所以每一个 task_struct 里其实都有一个 mm_struct 指向同一个内存 object，这个内存的 object 分各个 area，每个 area 都标识了这块内存的虚拟地址是否合法。

我们平时写代码的时候，不考虑 Glibc 有缓存内存，假如malloc 或 new 一块内存的请求到了操作系统，那么操作系统做的一件事就是在刚才所说的 mm_struct 里的 vm_area 里划分并标识一块合法的区域，这些操作都是在虚拟地址层面上，并没有真实的物理地址层面，然后做完这个操作以后它就返回了。但实际上虚拟地址和物理地址之间需要有一个映射，即虚拟页面。假如说是一个4K页，和一个物理4K页之间的映射关系没有建立，那什么时候建立呢？当CPU访问这块内存的时候就会触发一个 fault，因为 CPU 在 MMU 单元通过虚拟地址去找这块物理地址找不到，这个 fault 交到操作系统，操作系统再进行处理，这相当于是一种操作系统 lazy 处理的模式。但这些过程都是需要内核深度参与的，一旦出现要在内核态做这么多事情的情况，和低延迟就差得很远了。

major fault 是指当内存不够时，内存可能被交换到磁盘，再用到这块内存时再从磁盘交换回去 。major fault 比较好解决，一种是禁用 swap 分区 ，而且内存比较充足的话一般也不会触发，我们在系统里还有一个 mlock 调用，mlock 调用以后就可以阻止你这个进程的内存被 swap 到硬盘上。

minor fault 就比较难搞了，这个过程中可能有多个手段，但也不保证能百分百把它消除掉。一种是用 huge page。因为 fault 是以页面为单位的，huge page 可以把一个页从4K变成2M，这样的好处是页面 fault 的几率就明显会小很多。另外，虚拟地址页面去找物理地址页面需要 CPU 的 MMU 单元去找，它会优先去找TLB。TLB相当于映射的缓存，你可以认为它是一个哈希，如果找不到就会到页表里面一级一级去找，可能是两级，可能是三级，TLB 可以大大的提升这个这个寻找的时间。用了 huge page 以后，页表总体更少了，TLB miss 几率也就更低了。

![img](https://pic4.zhimg.com/80/v2-aa5c66e05a5eed8b7489c17e42a198d3_720w.webp)

对于 NUMA 来说，尽可能要它访问自己的线程，不要跨 slot 访问。NUMA 有多个内存的分配策略，一般默认的就是 localalloc，让这个槽的线程分配的内存在 local 分配，不要到 remote 分配。还有一种是 Interleave，即平均在几个 slot 里面分配，这种是我们不想要的。

prefault 是很大的一个话题，就是可以分配内存，但是分配了之后要想办法在真正使用之前先触发它的 minor fault。这有多个层面去解决，一是可以 hack 内存管理器，可以自己写，也可以优化 ptmalloc，当然如果有第三方的内存管理器可能会更好地解决这个问题。

![img](https://pic2.zhimg.com/80/v2-49803836579388cce9e5db9859726769_720w.webp)

**网络**

现在 TCP 延迟较高基本是业界共识，大家都在想怎么去解决这个问题，现在有趋势就是交易这方面也往 UDP 转，尤其是行情部分会越来越多地转到 UDP。无论怎么优化，你的缓冲区也好，中断也好，还是会有硬件的中断触发，陷入内核态，只要你的协议栈在内核态，性能就不会很好，所以这种情况下就要用用户态的协议栈。还有一些 FPGA 解决方案的，一般是券商或期货公司在用。

## 3.**低延迟系统编译选项**

![img](https://pic4.zhimg.com/80/v2-99a7610c554774fe9c2acf55046bd7e7_720w.webp)

先说一下编译器选择。Linux 主流的编译器无非就是 GCC、Clang、ICC。ICC 一直作为性能标杆的存在，但 ICC 在 C++ 的标准支持是比较落后的。ICC 做的比较好的是 CPU patch，它会针对不同的 CPU 的指令集生成很多份代码，运行的时候会根据具体的指令去选择最优的 function。

我们用的更多的还是 GCC，GCC 现在讨论最多的就是-O2 和-O3。这个在选择上没有标准答案，我们就来看看 -O3 比 -O2多了什么吧。

首先，-finline-functions 除了代码里写了 inline，或者用 GCC 的扩展 always_inline，GCC-O2还会默认开一个 inline call_once function，还有一个 inline_function 我个人觉得是很有用的。GCC 10 开始就已经 include -O2了，也会针对它不同的优化，不断地把 -O2 move到 -O3。但-O3 不一定整个项目都能用，可以只针对某个 function 或某个 file 来打开。

-floop-unroll-and-jam 是指如果有多层循环的话会把外层循环展开。

-fipa-cp-clone 是指如果有多个参数，其中一个传了常数的话，它有可能把这个 function clone两份，其中一份会去做一些常量展开、常量传播，这个有时能用得上。也许你会说“我代码写得比较好，我用 (const expression) 之类也能达到相似效果”，但是你不能保证所有人写代码或第三方库都能做到这一点。

![img](https://pic4.zhimg.com/80/v2-41a4df77549659a812579933bc0a2983_720w.webp)

这张图中上面两段代码都不是 cache friendly 的代码，都是比较低效的内存访问模式，但如果开了 -floop-interchange ，编译器就帮我们优化到我们想要的样子，cache friendly就没有问题了。

![img](https://pic1.zhimg.com/80/v2-132385291516e3eed611831341851d60_720w.webp)

可能有的编码规范上说不要在 foo 里面加 branch，但这段代码中看起来每个 foo 里都加了 branch，其实如果开了编译选项以后，GCC 会自动把 if 放到 foo 外面，如果这个 foo 里面有一条赋值语句且和 foo 无关的话，也会被移到外面。

![img](https://pic3.zhimg.com/80/v2-5259362091389224e8e4506786405956_720w.webp)

loop distribution 是目前的热点话题，distribution 是把一个循环展开成两个，但在这个例子中，展开成两个看起来是反优化的：a[i] = b[i]， b[i] = 0，在 cache 里肯定是最快的，那为什么要拆开呢？loop distribution pattern 能发现这个代码有一定的 pattern，上面用 memcpy() 搞定，下面用 memset() 搞定。

其他情况比如 a[i] = b[i]，但对 b[i] + 1 有一些依赖，那对流水线是不友好的，这种情况也有可能拆开。

当然还有 loop fusion 这种相反的情况，本来写的两个循环，它发现合并了以后更有利于 cache friendly，可能就会做合并，但在 GCC 里没有做合并这个选项，我们自己写代码的时候需要注意一下。

![img](https://pic3.zhimg.com/80/v2-c55c05815b2d64f39b75c35fbd007fe2_720w.webp)

loop-vectorize 是我认为最关键的一个，这里源自 Stack Overflow 的一个问题：在执行过程中有没有 sort，性能差异是巨大的，为什么？

有了 sort 以后，CPU Branch Prediction 更好了，成功率很高，性能就很高。GCC-O3 比 -O2 更慢，核心原因是最初 cmov 指令在老的处理器架构上比较慢，而现代新的编译器都用 cmov 做优化，不再用条件 jump 语句了，执行效率非常高。有了cmov 以后就没有分支了，也就不存在 sort 和 unsort 的区别了，也不存在 -O3 比 -O2 更慢的情况。

如果我们把 sum += data[c] 改成 sum += data[c] + data[c]，那么无论 GCC 还是 Clang 都不会再用 cmov，而是用传统条件跳转的方式，这种情况下性能就又有差别了，loop-vectorize 就可以起作用了。

如果启用了 loop vectorize，它就会用 SSE指令集 去优化这个循环的过程，也就是说，这个性能和 cmov 版本相比不会差，甚至是更好，所以说 loop vectorize 很多时候是非常有用的。如果是针对 -march=native，让编译器针对当前的处理器架构做一些优化的话，如果你的处理器支持 AVX 2 或是更高的 AVX-512 指令集，那它可以给你做更进一步的优化，性能提升得更大。

![img](https://pic3.zhimg.com/80/v2-4bfb4411a1ba526f591912d41b2beac6_720w.webp)

O3 与 Ofast 的主要区别在于 -ffast-math 是针对浮点数进行运算的。

Profile-Guided Optimisations 和 Profile 有点像，对于金融来说是测不准的。

-funroll-loops 在 Clang-O2 就有这个优化，但基于 GCC 只有开了 Profile Guided 优化才会把循环展开，这种情况下如果希望强制展开，可以用 #pragma。

-march 要么=native，要么等于目标架构。

-flto 一般也是打开的，可以减少 binary size，跨文件单元进行优化。

irace 是一个开源工具，是把各个编译选项排列组合，你提供一个测试程序，看一看哪个性能最高。

![img](https://pic2.zhimg.com/80/v2-e322e318ef15dd2bd0dbe97326b4c46d_720w.webp)

loop-vectorize 对于 int 是能提升性的，但如果把 type 改成 double，由于 floating 运算不支持结合律，loop vectorize 就做不了。那要如何进行优化呢？

如果你对浮点数的要求的精度运算没有那么高，可以打开 -ffast-math 下 -funsafe-math-optimizations 三个选项：

- -fassociative-math
- -fno-signed-zeros
- -fno-trapping-math



但要注意打开 -ffast-math 有可能产生一些精度问题，一定要对你的程序进行一些精确的测量，否则会出现一些莫名其妙的运算错误，对交易来说，这个运算错误肯定是非常致命的。

总结来说，开哪个编译并没有标准答案，我个人会开 Ofast 也会开 -march=native，需要结合你的具体项目需要。

## 4.**低延迟软件设计与编码**

![img](https://pic4.zhimg.com/80/v2-928e27c320465c202efce8cbe2fb63af_720w.webp)

由于我们相当于是一个client，不会用多进程，线程也都是提前创建的，因为创建线程肯定是要进内核态，而且内核态开销比较大，线程池也不太用，静态链接会比动态链接有百分之几的性能提升，建议用静态链接。数据拷贝和数据共享也都尽可能避免。

![img](https://pic1.zhimg.com/80/v2-66c8579090716a6162638f1974d6e1bc_720w.webp)

因为我们有数很多数据，即使是 critical 线程，critical path，系统刚启动时你总有一定的时间可以进行一些比较耗时的操作，你可以把这些计算量比较大的东西先算好，然后每来一些新的数据就可以用一些增量的方法来更新。最简单的，比如算一些均线、布林线、MACD指标等都可以考虑怎样增量计算，毕竟预算量越小、指令越少，性能就越高，让代码尽可能减少间接层，慎用第三方库。

![img](https://pic1.zhimg.com/80/v2-955870370ac13c098b17e1fe9f446f70_720w.webp)

运行时多态，通过 vptr 去找 vtable 会有一定的性能开销，另外如果是虚函数调用就没法 inline，这个带来的性能损失可能更大。这里有一些模板去解决这个问题，比如 CRTP、Policy based class design 之类。如果集成 path 上虚函数和具体函数只有一个实现的话，这个编译选项有可能会把虚函数间接的调用优化掉，还会有一个 vtable 的比较，我觉得这个可有可无，性能开销也不会很大，仅有一个实现的话还是可以考虑的。

![img](https://pic4.zhimg.com/80/v2-f39e602a79bb4c7cdcfebcc079651743_720w.webp)

这个案例中上面是基类，下面是派生类。基类里有一些 virtual function，这里用了一个 Strategy 设计模式，可能也是一个抽象类，运行时指向几个具体的类，通常写代码时这样写肯定是没有问题的，但如果这里面的触发至少有两次的虚函数都用开销，并且也不能 inline ，我们一般都会用 CRTP 这种模板的方式去解决。首先，基类 OnTick 会调用派生类的 OnTickImpl，这个 OMType* _om 也不再作为 Strategy 设计模式，直接继承下来并作为一个模板参数，在编译时就把它决议掉，然后在具体的类再去实现 OnTickImpl，这样就没有虚函数的开销，也可以 inline。

![img](https://pic1.zhimg.com/80/v2-664a1cdbebf36f556be8e31b5558e108_720w.webp)

我们做这个模板的时候有很多类型信息可能拿不到，所以就用一些 traits 的方式，比如各个接口之间用这个 concept 定一个traits，每个接口都把这个 traits 实现好，基类就可以去根据这个 traits 去取派生类的一些信息。这里 if constexpr 也相当于一个编译态的 if，和 enable_if 很像，通过这个也可以针对类型做一些分支处理，这些在运行时都是没有任何开销的。enable_if 这有一个参数，来判断它是不是 bool Critical 线程，如果是，就直接用 write(msg) ， 因为有另外一个线程会 busy loop 去做这件事情，可能就需要 _cv.notify。

![img](https://pic4.zhimg.com/80/v2-b825fe039c94f5eabadff0272bc8455b_720w.webp)

系统调用要尽可能避免，除非就是 vdso 支持的可以考虑派出。我们知道 vdso 就相当于是内核开辟了一块空间，方便系统调用拿数据。vdso 只支持几个时间相关的以及 getcpu 有限的几个。

rdtsc 是最方便的取时间的方式，而 clock 是真正的系统调用，它就不在 vdso 里，性能开销非常大。

打日志也是要非常谨慎，我们很多地方注意了低延迟，但其实打日志的延迟是非常大的，它开销主要是两部分：一部分是 format 开销，一部分是获取时间的开销 。format 开销有一种方式是编译的时候生成一些信息，运行时把这些 format 延迟推后，然后通过一些离线的工具，在生成 log 的时候再去做这个 format，比如说一些低开销的日志库就能做这些事情。

![img](https://pic4.zhimg.com/80/v2-f96dac15421c6a221e6182842a27f723_720w.webp)

动态内存分配有可能触发系统调用，并且会引发配置 fault，所以要尽量避免。避免的方式有很多，你可以用 placement new、memory pool。但 STL 及第三方库带来的内存分配很难避免，要避免的话，一是要提前分配内存，用 ring buffer 之类，map/set 数量少的话可以用 sorted array 替代。pool allocator 也可以实现内存池。

![img](https://pic2.zhimg.com/80/v2-5dd703b0f2b1a55df8552031fd156c85_720w.webp)

减少内存分配就是让 Glibc 的内存分配了以后尽可能不回收，一旦回收给操作系统，下次再申请就比较麻烦。

这里有一个例子。

![img](https://pic1.zhimg.com/80/v2-6b6fad7b4f9131ba9dab8152187f3c54_720w.webp)

vector 和 string 一样是可以提前分配内存的，你可以先 alloc 一块memory，目的就是 presort，每隔4096都写一下，sort 完之后 clear，如果系统调整好了，即使是默认的，Glibc 也不会回收这块内存。然后下次你再一个个 push_back，也不会触发 page fault。你可以通过工具，包括这个 perf 来看 minor-faults 有多少来验证。这仅限于 Glibc，对于谷歌 tcmalloc 和 Facebook jemalloc，因为是互联网环境，它们对内存回收都比较积极，所以这个方法对于它们都不适用。

![img](https://pic4.zhimg.com/80/v2-0b8ef4616639b51f64d89c9e6b0fc997_720w.webp)

string 的开销其实也是比较大的，但好处是它引起的开销就是堆上开销。它在堆上分配内存，在堆上分配一个差的数组，如果 std::string 比较小，就会在栈上分配内存，那速度就是比较快的。

![img](https://pic2.zhimg.com/80/v2-2269365da92b780e3cecd1ba87b6c97d_720w.webp)

hashmap 也很关键，因为里面的数据也会触发堆内存分配，这个也是不可以接受的。因此不能用那种链式的，都是用线性搜索。如果一块连续的地址填满了一个 hashmap，就会到紧邻的位置去找。

![img](https://pic1.zhimg.com/80/v2-8e0b284b1f30dbef07d5a2e137c708c4_720w.webp)

消息传递会用一些 lockfree queue 无锁编程，其实它内部也是有 lock 指令的，也是有开销的，所以如非必要也不建议用。如果极端情况下确实需要lock的话，用spinlock，不要用mutex/semaphore，因为spinlock是一种懵懂的状态，它不会进内核态等你的内核唤醒，所以它的开销还是比较小的。对于 mutex，现在是用 futex 优化，如果没有发生锁冲突，它也不会进内核态，但即使这样，mutex 的实现也比较复杂，开销也是比较大的。

![img](https://pic2.zhimg.com/80/v2-e2bb3bc55b3424e9425ef551936604d1_720w.webp)

这张图中我们可以看到 spinlock 开销还是比较低的，atomic 的操作也不高，当然原生的肯定是最高的。对于 mutax，我这里测的都是在没有资源竞争的情况下，这个数据已经非常不好了。

![img](https://pic2.zhimg.com/80/v2-bd4061e99ac1077f9f002af6e7d4bf01_720w.webp)

代码尽可能少用branch，比如这里是一个lookup table，我们在 table 里写的这些 function 就可以减少一些 switch case 或者 else 的分支判断。

![img](https://pic3.zhimg.com/80/v2-fe9cb23a55b31d680d6c96a204210926_720w.webp)

写代码的时候，为了节省时间资源，往往都倾向于鼓励提前退出，都会把比较高的 if 放到前面。但对于我们的情况可能正好相反。

![img](https://pic1.zhimg.com/80/v2-9cd6cf650b0e5092dcaa26d6d147e49c_720w.webp)

在低延迟场景下，大多数时候我们最终的信号是不会发出执行的，这种情况下如果让它过早退出，那这段很大的代码，包括数据，中间的 cache 都是冷的，那下次真正执行的效率就会比较低，这种是我们不想要的。

我们会在不crash的情况下，尽可能多执行代码，让它把一个完整的流程走完，不在意 CPU 时间浪费。在这里我们不是用或操作符，而是用按位操作符。用这种操作来换取 branch 只会有一个分支，就不会有三个分支了。

![img](https://pic4.zhimg.com/80/v2-75e430b37ab3e6c799514ddbebfa8ac3_720w.webp)

分支预测基本上是 [[likely]], [[unlikely]]，相当于是给编译器的一个参考。但实际上它只是决定静态的分支预测，到实际 CPU 运行的时候会按照实际的 branch 是否 take 来决定下一次怎么预测。

这和刚才那个例子一样，可能99.9%的情况下我们这个交易信号是触发不了的，那我永远走的是不触发的那个 branch，这样CPU 也记住了，每次都会走到不触发的 branch。当我真正想要触发时是最需要低延迟的时候，这个 branch 就给我预测错，并且所有 cache 都是冷的。这种情况有一些技巧，比如，可以用一些假的程序尽可能地往下执行，到最后一步停。还有最简单的办法就是我这个订单真的发到柜台，只不过我把精度扩大一位，比如说有效价格是两位，我给它设三位，那这个单子发出去了也会被柜台拒绝，但我所有的 branch 都走到了。但这样做可能券商或期货公司不喜欢，因为会有大量的废单进来。

![img](https://pic3.zhimg.com/80/v2-6e6fab16609fff346860185325bd860e_720w.webp)

异常如果不触发，对性能基本上没有什么影响，大家就没有什么心理负担。写代码的时候，作用域尽可能小，尽可能用 const ，连接性也尽可能低。这样目的就是让编译器知道更多，编译器知道更多的信息，就可能帮你做更多的优化。

![img](https://pic4.zhimg.com/80/v2-c47a31ec422df60c6cf1e8186ca83c4b_720w.webp)

智能指针 unique_ptr 的开销基本可以忽略，开销本身是动态分配内存的开销，shared_ptr 里面有两个 atomic 变量，当然它底层不是用 atomic，而是用的更原始的操作去做的，但这个也会有性能开销，传参的时候也不要觉得它是智能指针可以自动加一减一就直接传了，还是按照引用的方式传比较好。

最后，C++ 20的一些新特性对低延迟有一些帮助。atomic shared_ptr目前内部还是用锁实现的，也是暂时不能用，希望以后可能有更优化的实现。

原文地址：https://zhuanlan.zhihu.com/p/591314858

作者：linux

# 【NO.566】万字长文漫谈高可用高并发技术

互联网应用通常面向海量用户，其后台系统必须支撑高并发请求。在后端开发面试中，高并发技术也是一个常见的考察点。

那么，高并发系统通常是怎么设计的呢？需要采用哪些技术呢？本文就简单聊一聊高并发背后的各种技术栈。

必须明确高并发本身是目的，而不是某一项技术；只要能够提高连接数或系统处理吞吐的技术都算。因此，这是一个涉及面非常广的话题，本文无法事无巨细地展开。

我会重点罗列实现高并发的常见技术栈，并简单介绍每种技术的工作原理和应用场景，帮大家快速建立整体的知识脉络。

虽然每种技术的讲解篇幅有限，但我会列出更详细的学习资料，方便查阅学习。

大家可以以此知识地图为指引，制定学习计划，查漏补缺。

- 并发与并行

- 应用程序

- - 多进程
  - 多线程
  - 非阻塞轮询
  - IO多路复用
  - 内核参数调优

- 负载均衡

- - Nginx
  - HAProxy
  - LVS
  - F5
  - DNS

- 数据库

- - 数据库调优
  - 主从同步
  - 读写分离
  - 数据分片
  - MongoDB分布式架构

- 队列异步化

## 1.**并发与并行**

通常大家对并发的理解就是同时处理多个任务，虽说没有错，但不够具体。因此，开始之前，我们有必要更准确地理解：什么是 **并发** （ *concurrentcy* ）？并发跟 **并行**（ *parallel* ）有何不同？

以文件下载场景为例，假设你需要下载 *10* 个文件，服务器响应时间是 *1* 秒，那么串行下载需要耗时 *10* 秒。文件下载这种 **IO密集型应用** ，绝大部分时间都消耗在等待上。如果我们同时建立 *10*个连接发出请求，再逐个等待服务器响应，那么最终可能只需消耗 *1* 秒左右。

这就是一个典型的并发场景，最显著的特征是 **同时发起** 多个任务。

再来看另一个场景，假设你有一台单核的电脑，正在压缩一个很大的文件。压缩程序需要消耗很长时间，期间你要处理邮件，编辑文档。操作系统提供分时机制，压缩程序时间片用完就调度其他程序到 *CPU* 上执行。

这也是一个并发的场景，用户同时发起多个任务进程，它们轮流使用 *CPU* 。这样用户不用等待压缩程序执行完毕就能同时收发邮件，操作体验好很多。

如果你同时压缩两个大文件，假设每个都要消耗 *2* 分钟，最终还是要至少消耗 *4* 分钟。因为 *CPU*是单核的，就算两个压缩程序都在运行，但同一时间只有一个能在 *CPU* 上执行。因此，像这种 **计算密集型应用** ，并发并不能缩短执行时间。

这时您可能会想到多核 *CPU* ——没错！多核 *CPU* 每个核心可以近似看成一个完整的 *CPU* ，能够独立执行程序。这样一来，两个压缩程序可以分布在不同核心上，同时执行，整体耗时将减半！

这个场景就是所谓的 **并行**（ *parallel* ），一种比并发更进一步的形态，最显著的特征是 **同时执行**。

- **并发**（ *concurrency* ），同时发起，但不一定同时执行；
- **并行**（ *parallel* ），同时执行；

## 2.**应用程序**

互联网应用系统想要支持高并发，势必要具备同时处理大量 *TCP* 连接的能力。应用进程通过操作系统提供的套接字进行 *TCP* 通信，如果采用经典同步阻塞模式，一个程序同一时间只能处理一个套接字连接，何谈高并发？

> 在同步阻塞 *IO* 编程模型中，套接字默认是阻塞的。程序读写套接字时，*recv* 和 *send* 等系统调用会一直等待直到对方数据到达，或者本方数据发出。在阻塞等待期间，程序无法执行其他任务。

那么，如何实现在同一个程序中同时处理多个 *TCP* 连接呢？

### **2.1 多进程**

在远古时代，我们通常利用多进程技术来处理并发 *TCP* 连接：

- 主进程负责监听端口，收到新连接后 *fork* 子进程进行处理；
- 新连接套接字被子进程继承，子进程为新连接提供服务，处理完毕后便退出；

这个方案在同步阻塞 *IO* 模式下采用多进程实现并发处理能力，胜在开发简单，但缺点也很明显：

- 每个连接占用一个进程，资源开销将严重制约程序的最大并发数；
- 多个进程轮流竞争 *CPU* 执行权，上下文切换开销大；

### 2.2 **多线程**

由于进程的资源开销相对较大，操作系统提供了一种更轻量的进程——线程。一个进程可以创建多个执行线程，它们共享进程内存等资源，因此开销相对较小。编程模式上跟前面提到的经典多进程方案类似：

- 主线程负责监听端口，收到新连接后创建一个子线程进行处理；
- 子线程启动后为新连接提供服务，服务完毕后则退出；

相比多进程，多线程方案有不少优点：

- 所有线程共用进程中的内存资源，内存开销相对较少；
- 线程切换成本比进程小，因为内存是共享的，页表不用切换，*TLB* 缓存也不用刷；

> 现代操作系统进程采用虚拟地址来访问内存，由 *CPU* 负责将虚拟地址转换成物理地址。虚拟地址和物理地址的映射关系由 **页表**（ *page table* ）维护。*CPU* 根据页表做地址转换，并采用 *TLB* 缓存提升转换速度。
> 当 *CPU* 发生进程切换，需要先加载新进程的页表，并刷新 *TLB* 缓存。这是一个开销相对较大的操作，会制约系统的最终性能。因此，应用工作进程通常不能超过 *CPU* 核数，有些场景甚至会将主要进程跟 CPU 核心一一绑定，以避免进程切换开销。

那么，线程是不是就没有开销了呢？当然不是啦！

- 线程有独立的执行栈，线程数一多也要消耗很多内存；
- 线程切换要保存旧进程 *CPU* 寄存器（执行状态），然后加载新线程的寄存器，由此导致的 *CPU*缓存和指令流水线失效，也会带来不少损耗；

关于多进程和多线程编程技术，本文就简单介绍这些。想要进一步理解进程和线程的工作原理、区别和联系，可以复习一下操作系统：

推荐书籍：《现代操作系统 原书第4版 操作系统教材 从入门到精通》

想要学习 *CPU* 的运行原理，更深入理解页表和 *TLB* 缓存的作用，可以复习计算机组成原理：

推荐书籍：《计算机组成与设计：硬件/软件接口（原书第5版 RISC-V版）》

想要学习多进程和多线程编程方法，掌握相关系统调用和代码开发技巧，可以看看 *APUE* ：

推荐书箱：《UNIX环境高级编程 第3版》

### 2.3 **非阻塞轮询**

既然一个连接开一个进程或线程来处理开销太大，那有办法在一个执行流中同时处理多个连接吗？

同步阻塞模式下最棘手的问题是，套接字读写操作是阻塞的，会卡住进程或线程执行流。为此，操作系统支持将套接字设置成非阻塞模式：就算数据仍未就绪，读写操作也不会一直等待；而是直接返回错误，程序可以接着执行。

- 阻塞模式：读写操作会等待，直到数据就绪（函数调用迟迟不返回）；
- 非阻塞模式：读写操作不会等待（函数调用立马返回）；

由于应用程序不知道每个连接的数据何时就绪，因此只能进行轮询。举个例子，假设服务器进程通过 *accept* 系统调用接收到 *3* 个新连接，它并不知道这三个连接何时发请求上来。它只能采用轮询策略，定期尝试读取每个新套接字，看哪个有发数据过来。

非阻塞轮询方案也不完美，主要硬伤还是性能问题：

- 轮询时无差别读取，空闲连接也要读一遍，开销较大；
- 连接数据发上来后，要下一次轮询才能读到，及时性差；
- 为提升及时性，势必要提高轮询频率，但这样开销更大；

### 2.4 **IO多路复用**

为克服非阻塞轮询方案的种种缺点，操作系统又提供了 **IO多路复用** （ *IO multiplexing* ）技术。

*IO* 多路复用顾名思义，就是通过一次系统调用同时监视多个 *IO* 对象（套接字）：应用进程执行系统调用告诉操作系统监视哪些文件描述符，当它们有可读、可写或错误事件时，系统调用返回告知应用程序。

*IO* 多路复用技术发展至今，已历经若干代，以 *Linux* 为例：

- 第一代：*select* ，解决有无问题；
- 第二代：*poll* ，优化文件描述符传递，突破数量限制；
- 第三代：*epoll* ，全面解决性能问题；

#### **2.4.1 select**

应用进程执行 *select* 系统调用，将要监视的文件描述符（套接字）列表传给内核：

```text
int select(int nfds, fd_set *readfds, fd_set *writefds,
                  fd_set *exceptfds, struct timeval *timeout);
```

文件描述符列表分为 *3* 组，分别是：

- *readfds* ，监视可读事件，套接字上有数据到底时触发；
- *writefds* ，监视可写事件，数据送达对端套接字发送缓冲区释放时触发；
- *exceptfds* ，监视异常事件，内核网络协议栈检测到异常时触发；

内核接到系统调用后，逐一遍历每个文件描述符；当有文件描述符就绪时，*select* 系统调用便返回。就绪的套接字同样通过这几个参数，传回应用程序（通过指针修改数据）。

如果当前所有文件描述符均为就绪，*select* 系统调用会阻塞等待，直到：

1. 有文件描述符就绪；
2. 进程通过 *timeout* 指定的超时时间到达；

应用进程只需执行一次 *select* 系统调用，即可监视多个套接字，效率比应用进程自己轮询显著提高。不过，您可能也猜到了，*select* 也有它的局限性：

1. 文件描述符列表 *fd_set* 大小有限制，通常只支持 *1024* 个文件描述符；

- 该限制由一个宏定义指定，虽然可以调节，但需要重新编译内核

每次调用需要将关注的全部文件描述符拷贝到内核，开销很大；

内核需要遍历每个文件描述符，本质上还是轮询；

> *select* 在内核中轮询比进程在用户空间自己轮询高效，因为系统调用的开销降低了：本来每检查一个套接字就需要执行一次系统调用，现在总共只需要执行一次。
> 进程执行系统调用是有开销的，因为需要进行用户态和内核态切换（ *CPU* 执行栈和寄存器）。只不过跟进、线程切换相比，系统调用上下文切换开销要小很多。
> 进程切换 > 线程切换 > 用户态内核态切换

更多关于 *select* IO多路复用的编程技巧，可以参考 *APUE* 或 *Unix*网络编程 ：

推荐书箱：《UNIX网络编程 卷1 套接字联网API 第3版》

更多关于内核态、用户态、系统调用、上下文切换的细节，请查阅操作系统和 *Linux* 内核相关资料：

推荐书箱：《Linux内核设计与实现 原书第3版》

#### 2.4.2 **poll**

*poll* 主要解决 *select* 文件描述符列表大小限制问题，它采用 *pollfd* 结构，列表大小可灵活调节：

```text
int poll(struct pollfd *fds, nfds_t nfds, int timeout)
```

- *fds* ，即待监控文件描述符列表，它是一个 *pollfd* 结构体数组，数组大小是动态的；
- *nfds* ，*fds* 数组长度；

虽然 *poll* 解决了文件描述符大小限制问题，但其他实现跟 *select* 差不多：

- 文件描述符集合仍需在用户态和内核态间复制；
- 检测仍需要轮询遍历每个文件描述符；

总体而言，随着监控套接字集合的增加，*poll* 性能会线性下降，因此也不适用于大并发场景。

#### 2.4.3 **epoll**

为解决 *select* 和 *poll* 的性能问题，*Linux* 内核后来实现了 *epoll* 机制，有针对性地进行了优化：

- 文件描述符只需注册一次，不用每次都传进内核；

- - 执行 *epoll_ctl* 系统调用，注册/修改/取消要监视的文件描述符；
  - 内核用红黑树维护注册的文件描述符，提高查找效率（避免遍历）；

- 采用回调函数机制订阅 *IO* 事件，避免轮询；

- - *epoll_ctl* 添加新文件描述符的同时，内核会注册 *IO* 事件回调函数；
  - 当相关 *IO* 事件发生，回调函数就会执行，内核就知道哪些文件描述符就绪了；

- 只返回就绪文件描述符到用户空间；

- - 回调函数执行时，内核会将对应的活跃描述符加入就绪链表；
  - *epoll_wait* 等待文件描述符就绪，然后只需取出就绪链表并返回，不用遍历；
  - 应用进程只处理就绪文件描述符，无需遍历每个文件描述符进行判断；

采用 *epoll* 机制，应用进程通常这样操作：

1. 执行 *epoll_create* 系统调用，创建 *epoll* 专用文件描述符（保存相关上下文）；

- 参数 *size* 告诉内核监听规模，这不是一个限制，只是一个建议值，内核据此分配资源初始化相关数据结构；
- *epoll* 创建红黑树用于保存待监听文件描述符；
- *epoll* 创建就绪链表用于保存就绪的文件描述符；

执行 *epoll_ctl* 系统调用，注册、修改、移除待监听文件描述符；

- *EPOLL_CTL_ADD* 注册新的文件描述符，内核将该文件描述符保存到红黑树，然后注册 *IO* 事件回调函数；
- 当 *IO* 事件发生，回调函数被执行，对应文件描述符将被放入就绪链表；
- *EPOLL_CTL_MOD* 修改已注册的文件描述符，根据要订阅的事件，调整回调函数注册；
- *EPOLL_CTL_DEL* 移除文件描述符注册，删除回调函数注册；

执行 *epoll_wait* 系统调用，等待文件描述符就绪；

- 内核直接从链表中取出就绪文件描述符并返回；
- 应用程序直接处理就绪文件描述符，无需逐个遍历；
- 如果就行链表为空，*epoll_wait* 可以阻塞直到其中有一个就绪；

另外，*epoll* 还支持不同的触发模式：

- 水平触发，以读事件为例，只要文件描述符可读，就会一直返回；
- 边缘触发，以读事件为例，仅当文件描述符从不可读变成可读时，才会返回；

边缘触发模式通常效率更高——因为 *epoll* 只在文件描述符变为可读时通知一次，不会重复通知。但边缘触发模式代码编写起来要复杂一些，应用进程必须自己持续读取，以免遗漏。

更多关于 *epoll* 的工作原理和编程技巧，可以参考《Unix网络编程》：

推荐书箱：《UNIX网络编程 卷1 套接字联网API 第3版》

#### **2.4.4 操作系统调优**

采用 *IO* 多路复用之后，进程连接数可能还是上不去，这时需要检查 *Linux* 系统参数。有两个非常重要的参数制约一个进程能够打开的文件数上限，包括套接字。第一个是内核参数 `/proc/sys/fs/file-max` ：

```text
$ cat /proc/sys/fs/file-max
100000
```

这个参数控制 *Linux* 系统最多允许同时打开多少个文件，是系统级别的硬限制。如果这个参数设置过小，进程连接数肯定上不去。通常 *Linux* 内核会根据系统硬件资源（内存）状况自动计算该限制，如无特殊需要不必调整。

如果该参数当前设置太小，可以手工调大：

```text
$ echo 1000000 > /proc/sys/fs/file-max
```

还有另一个限制是 *ulimit* ，更具体讲是其中的 *nofile* ，它控制一个进程能够打开的最大文件数。*nofile* 参数分两个值来设置，其中：

- *hard* ，设置硬限制，进程能自己调节，但只能调小；
- *soft* ，设置软限制，进程能自己调节，既能调大也能调小，但不能超过 *hard* ；

最终进程能打开的最大文件数受 *soft* 值直接控制，受 *hard* 值间接控制。之所以将参数分成两个值，我猜是内核想想让进程自主控制最大文件数，又不想其超过既定范围，分成两个值更灵活一些。

很多 *Linux* 系统 *ulimit.nofile* 默认设置为 *1024* ，这意味着进程最多能同时处理的连接数只有一千出头！如果需要调大，可以执行 *ulimit* 命令或编辑 */etc/security/limits.conf* 文件，本文不再赘述。

关于 *Linux* **资源限制**（ *resource limit* 或 *rlimit* ）更多细节，可以查阅 *Linux* 经典书籍：

推荐书箱：《鸟哥的Linux私房菜 基础学习篇 第3版》

采用 *IO* 多路复用技术，并调好关键内核参数，单个应用进程能够实现相当可观的并发。由于现代 *CPU* 通常有很多核心，因此我们还可以采用多进程技术，进一步提高并发能力。

这里的多进程跟之前的已有本质区别，每个进程都能同时支持很多并发连接，而不仅仅是一个。如果还想继续优化，将单机性能发挥到极致，就只能深挖处理器架构和 *Unix* 内核相关知识了。

处理器方面要了解 *SMP* 和 *MUMA* 架构、流水线以及缓存原理，才能有针对性地进行优化。举个例子，处理同样的大矩阵，按行还是按列遍历性能可能会相差一个数量级，因为不好的访问方式会让 *CPU* 缓存失效。

这方面我推荐看《深入理解计算机系统》，里面有很多例子：

推荐书箱：《深入理解计算机系统 原书第3版》

*Linux* 内核方面则可以看看《Linux内核设计与实现》，补全关于进程调度、内存管理等知识。

## 3. **负载均衡**

采用 *IO* 多路复用技术配合多进程，我们可以将多核机器的性能发挥到极致。然而性能再好的机器，它能够支撑的并发负载也是有限的，毕竟 *CPU* 核心就那么多个。这时又该如何优化呢？

我们可以将应用从 **单机架构** 升级到 **分布式架构** ，以实现 **水平扩展** 。水平扩展的思路简单粗暴，一台机器不够，那就两台；两台不够就三台，还不够还可以继续加！

那么，如何实现能够水平扩展的分布式架构呢？

我们可以将应用部署到若干台机器上，请求可以送到任意一台机器上处理。这样一来，一个 *N* 节点组成的应用，其处理能力理论上可以达到单机模式的 *N* 倍。

换句话讲，系统的处理能力，跟系统的节点数成线性关系。这是一种非常有弹性的系统架构，特别适用于应用负载会随时间变化的场景。当系统负载上升时，只需为系统扩容新机器；当负载下降时，则反过来缩容一些。

还有一个问题，怎么将请求分发到不同的机器上执行，并且尽量均匀呢？总不能让客户端自己来决定请求哪个节点吧？这就需要引入 **负载均衡** （ *load balancing* ）技术。

负载均衡顾名思义就是负责将请求负载，均匀地分发到背后的工作节点。按负载均衡工作的网络层次，可以进一步分为两种：

- 四层负载均衡，工作在传输层；
- 七层负载均衡，工作在应用层；

业界的负载均衡技术很多，常用的有

- *Nginx*
- *HAProxy*
- *LVS*
- *DNS*
- *etc*

### **3.1 Nginx**

*Nginx* 是一款非常强大的 *Web* 服务器，常用于 **路由转发** 和 **负载均衡** 场景。

假设有一个电商站点，后台应用原来部署在一台机器上。现在由于用户量快速上涨，急需扩容。我们可以再找一台机器，部署上后台应用，然后前面再部署一个 *Nginx* 来分发流量：

![img](https://pic2.zhimg.com/80/v2-527f682bddc1f1143f692f3ad1b7121d_720w.webp)

如上图，*Nginx* 充当 **反向代理**（ *reversed proxy* ）的角色，客户端直接将请求发给 *Nginx* ，由 *Nginx* 负责转发到其后面的两台 *Web* 应用服务器。这一切对客户端来说是完全透明的，它们只跟 *Nginx* 打交道，因此反向代理常被称为透明代理。

*Nginx* 反向代理配置起来也很简单，我们来看一个例子：

```text
server {
    # 监听端口
    listen 80;
    # 代理对外域名
    server_name proxy-site.com;

    location / {
        # 转向服务器
        proxy_pass http://dest-site.com;
        proxy_redirect default;
    }
}

# 服务器集群及权重(可选)
upstream dest-site.com {
    server 10.0.0.1:80 weight=1;
    server 10.0.0.2:80 weight=2;
}
```

这个例子定义了一个 *upstream* ，包含两台应用服务器，假设 *IP* 分别是 *10.0.0.1* 和 *10.0.0.2* ；*server* 节则定义 *proxy_pass* 将所有流量转到这个 *upstream* 。注意到，*upstream server* 还能设置权重，给性能好点的服务器分发更多流量。

简单业务场景，请求通常可以落在任一台服务器处理，因此 *Nginx* 只需随机转发。但在某些复杂的业务场景可能就不行，比如有的需要实现会话保持。

举个例子：如果应用服务器将登陆会话保存在本地，那么 *Nginx* 必须保证登录后的请求都落在原来那台。否则若登录时落在 *A* 节点上处理，之后又落在 *B* 节点就会有问题，因为 *B* 并没有保存该请求的会话信息。

那么，*Nginx* 如何配置会话保持呢？常用的配置手段有两种：

- *ip hash* ，根据客户端 *IP* 哈希决定转发到哪个节点；

- - 客户端 *IP* 如果发生变化，转发节点很可能也会变化，进而导致会话失效；
  - 根据 *IP* 哈希，理论上可以将请求均匀地分布在不同节点；
  - 但来自同一局域网的客户端，由于出口 *IP* 相同，会被转发到同一节点，可能导致负载失衡；

- *sticky_cookie_insert* ，启用会话亲缘关系，*Nginx* 设置 *cookie* 并据此决定转发；

- - 基于 *cookie* 而非客户端 *IP* 来判断，可以避免同一局域网或前端代理导致的负载均衡；

> **哈希映射** 是一种简单高效的数据映射方法，做法很简单：

1. 对映射节点进行编号，*N* 个节点可以编号成 *0* ，*1* ，…… ， *N-1* ；
2. 对数据（如 *IP* 地址）求哈希值，并对 *N* 取模（ *hash % N* ），得到一个 *0* 到 *N-1* 的数值，就是映射到的节点编号；

*Nginx* 还支持配置健康检查，对 *upstream* 后端服务进行监控，发现故障就将其剔除：

![img](https://pic4.zhimg.com/80/v2-0caf927608c4b53846b231b3d575ce63_720w.webp)

如上图，*Nginx* 背后配了 *3* 台 *Web* 应用服务器，其中第一台服务器发生故障；*Nginx* 通过健康检查发现了故障，并将其从 *upstream* 中摘除；新进来的请求将分发到正常的服务上，因此客户端对此完全无感。这种集群架构允许集群部分节点故障，系统功能完全不受影响，因此同时实现了 **高可用**（ *high availability* ）。

因此，利用负载均衡技术，我们可以实现高并发，也可以实现高可用，或者兼而有之。

### 3.2 **HAProxy**

*Nginx* 通常应用于七层（应用层），它可以配置复杂策略，根据 *HTTP* 请求内容（如 *cookie* ）来转发请求。如果采用其他应用层协议，或者不关心应用层协议，只需按端口转发，则可以采用 *HAProxy* 。

*HAProxy* 本质上是一个端口转发器：它监听自己的服务端口，然后把客户端的连接转发到背后的服务器上：

![img](https://pic2.zhimg.com/80/v2-fa42d06c117f2ed30de86b98277241b9_720w.webp)

1. 一个客户端连上了后，*HAProxy* 会拿到一个连接套接字；
2. *HAProxy* 根据转发策略，代表该客户端与背后的服务器建立连接，得到另一个套接字；
3. 此后 *HAProxy* 负责在这两个套接字间来回拷贝数据；

由于 *HAProxy* 只是一个四层转发转发器，它配置起来比 *Nginx* 要简单一些。当然了，功能也更为单一，但在简单的业务场景下，也是够用的。

### 3.3 **LVS**

根据套接字编程接口，*HAProxy* 转发数据时需要先将数据从一个套接字读到（ *recv* ）用户空间，再通过对应的套接字发送（ *send* ）出去。很显然，这会导致了大量的数据拷贝：读取时从内核空间拷贝到用户空间，发送时从用户空间又重新拷贝回内核空间。

为避免不必要的数据拷贝，内核提供了 *sendfile* 系统调用进行优化。*sendfile* 系统调用将数据从一个文件描述符读取出来，并写到另一个，期间无需在用户态和内核态间拷贝数据。

虽然 *sendfile* 避免了数据拷贝，但系统调用还是避免不了。如果端口转发可以绕过进程，直接在内核中做，那性能肯定还会大幅提升。为此，章文嵩博士在内核中实现了 *LVS*（ *Linux Virtual Server* ）。

*LVS* 是一个基于四层，工作于内核且性能十分强悍的反向代理服务器，支持很多模式：

- *DR* 模式；
- *NAT* 模式；
- *FULLNAT* 模式；

对比 *HAProxy* ，*LVS* 的工作原理要复杂很多。因篇幅关系，本文就不再深入介绍了。后续有机会，我再写篇文章详细讲解一下。现在，我们只这样理解 *LVS* ：

- 工作于内核；
- 实现四层转发；
- 无数据拷贝，性能强悍；
- 支持回程流量不经过代理接入节点直接回给客户端，能有效缓解接入节点的带宽瓶颈；

### 3.4 **F5**

*LVS* 性能比 *HAProxy* 要好很多，但毕竟只是软件负载均衡技术，性能仍达不到极致。如果想要得到更好的性能，可以采用一些硬件解决方案，比如 *F5* 。

硬件负载均衡技术，本质上跟前面介绍的软件解决方案是一样的。只不过硬件不存在操作系统中断、上下文切换等软件处理开销，因此性能会好很多。当然了，硬件解决方案通常也比较贵，因为需要买专用设备。

### 3.5 **DNS**

根据 *DNS* 协议，一个域名可以解析到多个不同的 *IP* 上。因此，*DNS* 也可以用来做负载均衡，水平扩展系统的处理能力。

![img](https://pic1.zhimg.com/80/v2-415ffffdce5b6d4aba761398ae2e6840_720w.webp)

举个例子，假设我的站点 *[http://fasionchan.com](https://link.zhihu.com/?target=http%3A//fasionchan.com)* 的访问量很大，我可以多部署多几台服务器，然后把 *IP* 都配置到 *DNS* 。用户访问站点需要先解析域名，由于得到的 *IP* 有多个，它会随机挑一个来访问。正因 *IP* 选择是随机的，每台服务器的负载大致是均匀的。

## 4.**数据库**

互联网后台通常是由应用程序和数据库组成的大型系统，如果数据库性能跟不上，就算应用程序优化得再好也是白搭。那这部分我们就一起来研究一下，如何对数据库进行优化。

### 4.1 **数据库调优**

数据库调优这个话题还是很大，涉及面非常广。小到字段类型，大到索引设计，*SQL* 语句写法都会影响查询性能。

举个例子，有一个长度很大的字段需要建索引，最好另存一个 *MD5* 字段，把索引建在 *MD5* 字段上。因为 *MD5* 值的长度是固定的，而且长度相对较短，不管是索引空间还是查询效率都会显著提高。

由于本文只是指引性介绍，姑且以这个例子抛砖引玉，有兴趣的读者可以深入学习《高性能MySQL》之类的权威著作：



### 4.2 **主从同步高可用**

前面提到，应用程序可以部署多个节点，配合负载均衡技术实现高可用。那么，数据库也可以这样实现高可用吗？

我们来考察一个简单的场景，假设我们有一个类似微信朋友圈的应用：

![img](https://pic2.zhimg.com/80/v2-a8969a68ba98aa3730f7f4026f54b2dd_720w.webp)

我们部署了多台应用服务器，提供后端 *API* 接口，它们都连同一个数据库。客户端的请求上来后，先经过一个负载均衡器，由它转发给应用服务器。应用服务器则根据业务逻辑，对数据库进行读写。

这个架构应用服务器是高可用的，因为不管哪个节点挂了，负载均衡器都可以将流量送到健康节点。想要容忍更多服务器故障，我们只需部署更多节点。只要不是所有服务器都挂点，我们的服务就不受影响。

但是，我们的数据库只有一台！但由于数据库是典型的有状态服务，不能无差别地部署多实例。试想用户发朋友圈写 *A* 库，读朋友圈读 *B* 库不就发现自己的数据不见了吗？

![img](https://pic4.zhimg.com/80/v2-991d2c1712f6a5418cf7461abd21a62f_720w.webp)

因此，数据库部署多实例后还需要同步数据，确保每个实例数据是一致的。由于多点写入会给数据同步带来挑战，数据冲突可能难以解决，因此通常采用主从同步模式：

![img](https://pic4.zhimg.com/80/v2-4c54c9cca2bc38735010564bfaf2c2ab_720w.webp)

如上图，应用连接主库读写数据，从库连接主库同步数据。平时应用不会连接从库，因此从库只起到备份数据的作用。当主库故障无法恢复时，管理员可以将应用切到从库，从库转为主库，继续提供服务：

![img](https://pic4.zhimg.com/80/v2-e645dcedfa4d23b5af115c34aa03419f_720w.webp)

- 由于数据同步存在时延，因此从库跟主库可能不一致；
- 主从切换需要人工判断是否安全，以及切换后是否需要进行数据维护，以保证数据一致性；
- 当主库恢复后，可以转为从库，连接新主库同步数据；

采用主从同步架构后，就算主数据库故障短时间无法恢复，管理员也可以将应用切到从库从而恢复服务，可用性在一定程度上得到有效保障。

### 4.3 **读写分离**

假设我们要实现一个类型朋友圈的功能，后端提供一个 *API* 模块。用户发朋友圈时，前端调用 *API*写数据库；用户刷朋友圈时则调 *API* 读数据库。这是一个应用的最小化模型，很简单对吧？

假设现在应用的用户量很大， 很多人都在刷朋友圈，数据库查询压力很大，怎么办呢？

最简单的做法是部署主从同步架构来做读写分离：

![img](https://pic4.zhimg.com/80/v2-1dbd64a6f1cacf2bc292edfbe3e0314b_720w.webp)

如上图，数据库部署主从同步结构，应用写操作连到主库上，读操作连接到从库上。这样读压力就从主库分散到从库上，从而获得更大的吞吐量。如果读压力很大，我们还可以多部署几个从库，进一步分散读压力。

![img](https://pic4.zhimg.com/80/v2-9553e2e22cc9a36e8b9e7fc05b58ff67_720w.webp)

因此，主从同步结构特别适用于读压力很大的业务场景。

### 4.4 **数据同步延迟**

用户发朋友圈，数据写入主库后，还要经过一小段时间之后才能同步到从库。假设你发了朋友圈，数据还没同步到从库，这时你刷新自己的首页，岂不是发现自己发的朋友圈丢了！

因此，采用读写分离架构的应用，必须关注数据同步延迟，并加以处理。拿这个例子来说，应用服务只要保证当前用户的朋友圈总是到主库读，就不会有问题。

### 4.5 **单调读**

假设好友发了一条朋友圈，写入主库，但只有从库①已经同步，从库②和③尚未完成同步，这时你刷朋友圈会发生什么现象？

由于刷朋友圈是一个读操作，应用服务器会连接从库读取数据。如果连的是从库①，这时可以读到好友的朋友圈。假设这时你刷新了页面，应用服务器连接从库②读取数据， 这时你会发现好友发的朋友圈消失了！你再刷新，如果又连回从库①，这时朋友圈又出现了，再刷一下可能又消失了……

由于从库的同步速度不是完全一致的，而且步调也难以控制，因此应用肯定会读到不一致的数据。那么，我们应该怎么解决这个问题呢？

其实很简单，应用服务器可以根据用户算哈希值，来决定连哪个从库，以此保证同个用户会固定读一个从库。这样一来，只要朋友圈记录从主库同步到当前从库，被用户读到之后就不会再消失。

从库数据的同步进度，决定了其数据快照的时间点。一旦读取到新的数据快照，就不会重新读到旧快照，这就是所谓的 **单调读** 。

关于读写分离架构就先简单介绍这些，更多细节大家可以阅读《数据密集型应用系统设计》深入学习：



### 4.6 **数据缓存**

数据库读压力太大，还可以通过引入缓存来解决。举个例子，网络论坛应用首页展示热门帖子，这需要查询数据库。而且这不是一个简单查询，开销可能较大。如果每个用户刷新首页都要查一遍数据库，压力可想而知。

热门帖子通常在短时间内不会变化，因此我们可以将查询结构缓存起来：

![img](https://pic1.zhimg.com/80/v2-932552b2683f05497cb2b1d8d781fa50_720w.webp)

- 应用服务器先查缓存——①；
- 如果缓存数据不存在或者已经过期，查询数据库——②；
- 将数据库查询结构写到缓存，同时可以设置有效期——③；
- 只要缓存数据尚未失效，应用程序可以直接从缓存中读取——④；

有了缓存挡在前面，应用服务器仅当缓存失效时才会查询数据库，因此数据库压力大幅降低。如果应用服务器采用多节点部署，查库写缓存操作（如图②和③）可能需要用分布式锁加以保护。

至于缓存模块的实现，可以采用目前很流行的中间件，比如：

- *Redis*
- *Memcached*

### 4.7 **数据分片**

在海量业务面前，以上这些套路还是远远不够的。假设你有一个国民级应用，你光用户表就得有十几亿条数据！那问题来了，什么样的数据库才能支持单表十几亿规模的在线查询呢？

常用数据库对此可能都无能为力，以 *MySQL* 为例，单表数据规模达到千万级以上后，性能就会有明显下降。那问题是否就完全无解了呢？

计算机科学中有一个非常有名的分治思想，几乎放之四海而皆准。我们可以对大表进行划分，分成若干小表，然后存放到不同的数据库上。

![img](https://pic1.zhimg.com/80/v2-f54969e95722de4c03288c180ef672b0_720w.webp)

这样一来，每个数据库中的数据量下降了，压力也就下降了。不过我们还有一个问题尚未解决，数据应该如何划分呢？如果是随机划分，查询时如何确定查哪个库呢？难不成每个库都查一遍？

### 4.8 **数据划分**

一般而言，可以采用以下两种策略来划分数据：

- 哈希划分，对数据键（唯一用户名）求哈希值，决定保存在哪个库；
- 范围划分，对数据区间进行分段后保存，比如身份证号为 *44* 开头的保存在某个库；

### 4.9 **哈希划分**

回到前面的例子，我们可以先对用户名求哈希值，再根据哈希值决定数据保存到哪个子库。这种划分方式通常可以保证数据基本上是均匀的，因为哈希映射是随机的。

查询时，应用需要根据查询条件决定查询哪个数据库。举个例子，如果查询 *tom* 的用户信息，计算 *tom* 的哈希值并进行映射即可知道应该查询数据库②。

不过有些查询无法确定子数据库，只能每一个都查一遍，再合并结果。例如查询 *tom* 的好友，查询条件没有包含哈希用的字段，也就无法做哈希映射；而 *tom* 的好友可能保存在任一个子库中，因此只能全部查一遍。

### 4.10 **一致性哈希**

顺便提一下，如果只是采用普通哈希方法，增减子库时会导致大量的数据迁移。因为 *N* 一变，哈希值取模出来的序号也跟着变，那映射关系也就全变了。映射关系一变意味着我们需要将数据从旧库，迁移到新映射的库上去。

这种问题可以采用 **一致性哈希** （ *consistent hashing* ）算法加以解决，它通过引入一层虚拟节点，来减少映射关系的变动。

### 4.11 **范围划分**

范围划分，顾名思义就是将字段值的范围划分为若干区间，再分别保存到不同的子库。举个例子，有个应用用户 *ID* 的范围在 *1~1000000* 之间，用户表需要分 *5* 个子库存储，可以这样分：

- *1~200000* ，存在子库①；
- *200001~400000* ，存在子库②；
- *400001~600000* ，存在子库③；
- *600001~800000* ，存在子库④；
- *800001~1000000* ，存在子库⑤；

分段太大通常不太灵活，例如数据分别可能不均匀。因此，我们可能需要缩小分段范围，比如以 *100* 为一个段，*1* 百万的 *ID* 范围可以划分为 *1* 万个区间，再映射到子库上去，而映射关系作为元数据保存。

如果发现某个子库负载较高，可以将上面的某些区间数据迁移到其他节点上，数据库维护起来就灵活多了。

### 4.12 **MongoDB案例**

*MongoDB* 是一个典型的分布式数据库，我们就以它为学习案例，深入考察 **数据复制** 和 **数据分片**相关架构设计。

- **数据复制**（ *replication* ），通常为了实现高可用，同时也实现压力分散；
- **数据分片**（ *partitioning* ），通常为了实现水平扩展；

*MongoDB* 支持主从同步，通常是一个主节点加两个从节点组成一个 **复制集**（ *replica set* ），检查 *RS* ：

![img](https://pic4.zhimg.com/80/v2-0b5d559fb85a170031f592b3025e231b_720w.webp)

如上图是一个三节点组成的复制集，三个节点保持通信，并通过分布式共识算法选举主节点。主节点接收写请求，从节点从主节点同步复制数据。读请求可以视业务场景配置读取策略：

- 只读 *primary* ；
- 只读 *secondary*（读写分离）；
- 两者都可读；

应用程序通常将 *RS* 每个节点的 *IP* 都配上，它可以连接任一节点，读取 *RS* 集群信息，然后再连接主节点。选举时要满足过半数原则，因此节点数通常为奇数，即 ， 则为过半数。

由此一来，一个 *RS* 至少需要有 *3* 个节点：一主两从，数据为三副本。如果只要一主一从两副本模式，可以加一个仲裁节点。仲裁节点只参与选举，不存储数据。

*RS* 实现数据多副本，达到高可用和分散压力的目的。一方面，*RS* 允许有节点挂掉，只要故障节点不超过半数，数据库服务就不受影响；另一方面，通过读写分离，可以将读压力分散到 *secondary* 节点，从而降低 *primary* 的压力。

除了数据副本复制功能，*MongoDB* 还提供了数据分片功能：

![img](https://pic3.zhimg.com/80/v2-abea70261eb68b3c541053cf84821fa6_720w.webp)

如上图，这是一个 *MongoDB* 分片集群：集群中有 *5* 个分片，每个分片的数据由一个 *RS* 负责存储，*RS* 做数据副本复制以实现高可用；*MongoDB* 支持通过哈希或者范围划分，将数据分到某一个 *RS* 上保存；数据划分策略、分片映射关系则由一个独立的 *RS* 保存，称为 *config* ；由于这类元数据不大，因此 *config* 通常都很小；*mongos* 则负责查询路由，它解析客户端请求，根据 *config* 中的元数据判断数据位于哪个 *RS* ；如果查询条件无法确定数据位置，它会向每个 *RS* 都发起查询，再合并结果；*mongos* 是无状态的，通常可以部署多个实例，来实现高可用；此外，*MongoDB* 还可以自动平衡数据，将数据从负载高的 *RS* 迁移到负载低的 *RS* （迁移好后更新 *config* 元数据）。

## 5.**队列异步化**

应用负载通常不是均匀的，比如电商大促时请求量可能是平时的好几倍，甚至几十倍。面对突发而来的极高并发，系统通常难以应对。何况按瞬时峰值负载来规划系统容量，放在平时会造成极大的浪费。

笔者曾负责一个主机监控系统的研发，最开始的架构大致如下：

![img](https://pic2.zhimg.com/80/v2-00bac57b3b317bdd28faeff517442fe5_720w.webp)

每台服务器主机上都部署 *agent* ，定期采集性能指标，并提交给后端处理。由于定时任务总是在整点执行，因此会出现主机总在采集时点同时请求 *api* 提交数据的情况。

所有主机几乎在同一时间请求 *api* 提交数据，但由于数据库写入吞吐是有限的，经常会有数据写入失败。如果 *agent* 没有失败重试机制，则意味着数据丢失。

然而瞬间请求过后，系统就又空闲下来了。如果可以将数据写入分摊到整个提交周期，系统的表现应该要好很多。就像拦河大坝，发生洪灾时先把水拦住，再慢慢放水，就不会造成严重损失。

因此，我们可以引入消息队列来做水库，对 *api* 进行异步化改造：

![img](https://pic1.zhimg.com/80/v2-e329f75f7d19d750e235b98203969980_720w.webp)

1. *api* 收到数据提交后，先把数据写入 **消息队列**（ *message queue* 简称 *MQ* ）；

- 消息队列通常是顺序写入磁盘，因此写性能要比数据库高很多；

后台服务 *writer* 负责从 *MQ* 消费数据，并写入数据库；

这样一来，当采集时刻瞬时数据提交并发上来后，*api* 可以从容地将全部数据写到 *MQ* 。这时数据可能仍堆积在 *MQ* ，尚未写入数据库。*writer* 模块会慢慢消费数据，完成写库任务。

只要在下次提交时间到达前可以顺利写完，系统就可平稳运行。由此可见，*MQ* 就像一个水库，对数据流起到 **削峰平谷** 的作用。

此外，*MQ* 的引入还有效降低了系统模块间的耦合。回到这个例子，如果 *writer* 或数据库发生故障或正在维护，对 *api* 完全没有影响。*agent* 还能继续提交数据，只是数据会堆积在 *MQ* 中。*writer* 和数据库恢复后，再慢慢追数据即可。

总结一下，引入消息队列可以发挥以下效果：

- 流量削峰平谷；
- 处理异步化；
- 模块解耦；

目前，比较流行的消息队列系统有 *Kafka* ，*RabbitMQ* 等等，有兴趣的同学可以自行了解一下。

原文地址：https://zhuanlan.zhihu.com/p/607413349

作者：linux

# 【NO.567】万字长文讲解 | linux内核性能调优

## 1.系统性能定义

让我们先来说说如何什么是系统性能。这个定义非常关键，如果我们不清楚什么是系统性能，那么我们将无法定位之。我见过很多朋友会觉得这很容易，但是仔细一问，其实他们并没有一个比较系统的方法，所以，在这里我想告诉大家如何系统地来定位性能。 总体来说，系统性能就是两个事：

1. **Throughput** ，吞吐量。也就是每秒钟可以处理的请求数，任务数。
2. **Latency**， 系统延迟。也就是系统在处理一个请求或一个任务时的延迟。

一般来说，一个系统的性能受到这两个条件的约束，缺一不可。比如，我的系统可以顶得住一百万的并发，但是系统的延迟是2分钟以上，那么，这个一百万的负载毫无意义。系统延迟很短，但是吞吐量很低，同样没有意义。所以，一个好的系统的性能测试必然受到这两个条件的同时作用。 有经验的朋友一定知道，这两个东西的一些关系：

- **Throughput越大，Latency会越差。**因为请求量过大，系统太繁忙，所以响应速度自然会低。
- **Latency越好，能支持的Throughput就会越高。**因为Latency短说明处理速度快，于是就可以处理更多的请求。

## 2.系统性能测试

经过上述的说明，我们知道要测试系统的性能，需要我们收集系统的Throughput和Latency这两个值。

- 首先，**需要定义Latency这个值**，比如说，对于网站系统响应时间必需是5秒以内（对于某些实时系统可能需要定义的更短，比如5ms以内，这个更根据不同的业务来定义）
- 其次，**开发性能测试工具**，一个工具用来制造高强度的Throughput，另一个工具用来测量Latency。对于第一个工具，你可以参考一下“十个免费的Web压力测试工具”，关于如何测量Latency，你可以在代码中测量，但是这样会影响程序的执行，而且只能测试到程序内部的Latency，真正的Latency是整个系统都算上，包括操作系统和网络的延时，你可以使用Wireshark来抓网络包来测量。这两个工具具体怎么做，这个还请大家自己思考去了。
- 最后，**开始性能测试**。你需要不断地提升测试的Throughput，然后观察系统的负载情况，如果系统顶得住，那就观察Latency的值。这样，你就可以找到系统的最大负载，并且你可以知道系统的响应延时是多少。

再多说一些，

- 关于Latency，如果吞吐量很少，这个值估计会非常稳定，当吞吐量越来越大时，系统的Latency会出现非常剧烈的抖动，所以，我们在测量Latency的时候，我们需要注意到Latency的分布，也就是说，有百分之几的在我们允许的范围，有百分之几的超出了，有百分之几的完全不可接受。也许，平均下来的Latency达标了，但是其中仅有50%的达到了我们可接受的范围。那也没有意义。
- 关于性能测试，我们还需要定义一个时间段。比如：在某个吞吐量上持续15分钟。因为当负载到达的时候，系统会变得不稳定，当过了一两分钟后，系统才会稳定。另外，也有可能是，你的系统在这个负载下前几分钟还表现正常，然后就不稳定了，甚至垮了。所以，需要这么一段时间。这个值，我们叫做峰值极限。
- 性能测试还需要做Soak Test，也就是在某个吞吐量下，系统可以持续跑一周甚至更长。这个值，我们叫做系统的正常运行的负载极限。

性能测试有很多很复要的东西，比如：burst test等。 这里不能一一详述，这里只说了一些和性能调优相关的东西。总之，性能测试是一细活和累活。

## 3.定位性能瓶颈

有了上面的铺垫，我们就可以测试到到系统的性能了，再调优之前，我们先来说说如何找到性能的瓶颈。我见过很多朋友会觉得这很容易，但是仔细一问，其实他们并没有一个比较系统的方法。

### 3.1 查看操作系统负载

首先，当我们系统有问题的时候，我们不要急于去调查我们代码，这个毫无意义。我们首要需要看的是操作系统的报告。看看操作系统的CPU利用率，看看内存使用率，看看操作系统的IO，还有网络的IO，网络链接数，等等。Windows下的perfmon是一个很不错的工具，Linux下也有很多相关的命令和工具，比如：SystemTap，LatencyTOP，vmstat, sar, iostat, top, tcpdump等等 。通过观察这些数据，我们就可以知道我们的软件的性能基本上出在哪里。比如：

1）先看CPU利用率，如果CPU利用率不高，但是系统的Throughput和Latency上不去了，这说明我们的程序并没有忙于计算，而是忙于别的一些事，比如IO。（另外，CPU的利用率还要看内核态的和用户态的，内核态的一上去了，整个系统的性能就下来了。而对于多核CPU来说，CPU 0 是相当关键的，如果CPU 0的负载高，那么会影响其它核的性能，因为CPU各核间是需要有调度的，这靠CPU0完成）

2）然后，我们可以看一下IO大不大，IO和CPU一般是反着来的，CPU利用率高则IO不大，IO大则CPU就小。关于IO，我们要看三个事，一个是磁盘文件IO，一个是驱动程序的IO（如：网卡），一个是内存换页率。这三个事都会影响系统性能。

3）然后，查看一下网络带宽使用情况，在Linux下，你可以使用iftop, iptraf, ntop, tcpdump这些命令来查看。或是用Wireshark来查看。

4）如果CPU不高，IO不高，内存使用不高，网络带宽使用不高。但是系统的性能上不去。这说明你的程序有问题，比如，你的程序被阻塞了。可能是因为等那个锁，可能是因为等某个资源，或者是在切换上下文。

**通过了解操作系统的性能，我们才知道性能的问题，比如：带宽不够，内存不够，TCP缓冲区不够，等等，很多时候，不需要调整程序的，只需要调整一下硬件或操作系统的配置就可以了**。

### 3.2 使用Profiler测试

接下来，我们需要使用性能检测工具，也就是使用某个Profiler来差看一下我们程序的运行性能。如：Java的JProfiler/TPTP/CodePro Profiler，GNU的gprof，IBM的PurifyPlus，Intel的VTune，AMD的CodeAnalyst，还有Linux下的OProfile/perf，后面两个可以让你对你的代码优化到CPU的微指令级别，如果你关心CPU的L1/L2的缓存调优，那么你需要考虑一下使用VTune。 使用这些Profiler工具，可以让你程序中各个模块函数甚至指令的很多东西，如：**运行的时间** ，**调用的次数**，**CPU的利用率**，等等。这些东西对我们来说非常有用。

我们重点观察运行时间最多，调用次数最多的那些函数和指令。这里注意一下，对于调用次数多但是时间很短的函数，你可能只需要轻微优化一下，你的性能就上去了（比如：某函数一秒种被调用100万次，你想想如果你让这个函数提高0.01毫秒的时间 ，这会给你带来多大的性能）

使用Profiler有个问题我们需要注意一下，因为Profiler会让你的程序运行的性能变低，像PurifyPlus这样的工具会在你的代码中插入很多代码，会导致你的程序运行效率变低，从而没发测试出在高吞吐量下的系统的性能，对此，一般有两个方法来定位系统瓶颈：

1）在你的代码中自己做统计，使用微秒级的计时器和函数调用计算器，每隔10秒把统计log到文件中。

2）分段注释你的代码块，让一些函数空转，做Hard Code的Mock，然后再测试一下系统的Throughput和Latency是否有质的变化，如果有，那么被注释的函数就是性能瓶颈，再在这个函数体内注释代码，直到找到最耗性能的语句。

最后再说一点，**对于性能测试，不同的Throughput会出现不同的测试结果，不同的测试数据也会有不同的测试结果。所以，用于性能测试的数据非常重要，性能测试中，我们需要观测试不同Throughput的结果**。

## 4.常见的系统瓶颈

下面这些东西是我所经历过的一些问题，也许并不全，也许并不对，大家可以补充指正，我**纯属抛砖引玉**。关于系统架构方面的性能调优，大家可移步看一下《由[http://12306.cn](https://link.zhihu.com/?target=http%3A//12306.cn)谈谈网站性能技术》，关于Web方面的一些性能调优的东西，大家可以看看《Web开发中需要了解的东西》一文中的性能一章。我在这里就不再说设计和架构上的东西了。

一般来说，性能优化也就是下面的几个策略：

- **用空间换时间**。各种cache如CPU L1/L2/RAM到硬盘，都是用空间来换时间的策略。这样策略基本上是把计算的过程一步一步的保存或缓存下来，这样就不用每次用的时候都要再计算一遍，比如数据缓冲，CDN，等。这样的策略还表现为冗余数据，比如数据镜象，负载均衡什么的。
- **用时间换空间**。有时候，少量的空间可能性能会更好，比如网络传输，如果有一些压缩数据的算法（如前些天说的“Huffman 编码压缩算法” 和 “rsync 的核心算法”），这样的算法其实很耗时，但是因为瓶颈在网络传输，所以用时间来换空间反而能省时间。
- **简化代码**。最高效的程序就是不执行任何代码的程序，所以，代码越少性能就越高。关于代码级优化的技术大学里的教科书有很多示例了。如：减少循环的层数，减少递归，在循环中少声明变量，少做分配和释放内存的操作，尽量把循环体内的表达式抽到循环外，条件表达的中的多个条件判断的次序，尽量在程序启动时把一些东西准备好，注意函数调用的开销（栈上开销），注意面向对象语言中临时对象的开销，小心使用异常（不要用异常来检查一些可接受可忽略并经常发生的错误），…… 等等，等等，这连东西需要我们非常了解编程语言和常用的库。
- **并行处理**。如果CPU只有一个核，你要玩多进程，多线程，对于计算密集型的软件会反而更慢（因为操作系统调度和切换开销很大），CPU的核多了才能真正体现出多进程多线程的优势。并行处理需要我们的程序有Scalability，不能水平或垂直扩展的程序无法进行并行处理。从架构上来说，这表再为——是否可以做到不改代码只是加加机器就可以完成性能提升？

总之，**根据2：8原则来说，20%的代码耗了你80%的性能，找到那20%的代码，你就可以优化那80%的性能**。 下面的一些东西都是我的一些经验，我只例举了一些最有价值的性能调优的的方法，供你参考，也欢迎补充。

**4.1）算法调优**。算法非常重要，好的算法会有更好的性能。举几个我经历过的项目的例子，大家可以感觉一下。

- 一个是**过滤算法**，系统需要对收到的请求做过滤，我们把可以被filter in/out的东西配置在了一个文件中，原有的过滤算法是遍历过滤配置，后来，我们找到了一种方法可以对这个过滤配置进行排序，这样就可以用二分折半的方法来过滤，系统性能增加了50%。
- 一个是**哈希算法**。计算哈希算法的函数并不高效，一方面是计算太费时，另一方面是碰撞太高，碰撞高了就跟单向链表一个性能（可参看Hash Collision DoS 问题）。我们知道，算法都是和需要处理的数据很有关系的，就算是被大家所嘲笑的“冒泡排序”在某些情况下（大多数数据是排好序的）其效率会高于所有的排序算法。哈希算法也一样，广为人知的哈希算法都是用英文字典做测试，但是我们的业务在数据有其特殊性，所以，对于还需要根据自己的数据来挑选适合的哈希算法。对于我以前的一个项目，公司内某牛人给我发来了一个哈希算法，结果让我们的系统性能上升了150%。（关于各种哈希算法，你一定要看看StackExchange上的这篇关于各种hash算法的文章 ）
- **分而治之和预处理**。以前有一个程序为了生成月报表，每次都需要计算很长的时间，有时候需要花将近一整天的时间。于是我们把我们找到了一种方法可以把这个算法发成增量式的，也就是说我每天都把当天的数据计算好了后和前一天的报表合并，这样可以大大的节省计算时间，每天的数据计算量只需要20分钟，但是如果我要算整个月的，系统则需要10个小时以上（SQL语句在大数据量面前性能成级数性下降）。这种分而治之的思路在大数据面前对性能有很帮助，就像merge排序一样。SQL语句和数据库的性能优化也是这一策略，如：使用嵌套式的Select而不是笛卡尔积的Select，使用视图，等等。

**4.2）代码调优**。从我的经验上来说，代码上的调优有下面这几点：

- **字符串操作**。这是最费系统性能的事了，无论是strcpy, strcat还是strlen，最需要注意的是字符串子串匹配。所以，能用整型最好用整型。举几个例子，第一个例子是N年前做银行的时候，我的同事喜欢把日期存成字符串（如：2012-05-29 08:30:02），我勒个去，一个select where between语句相当耗时。另一个例子是，我以前有个同事把一些状态码用字符串来处理，他的理由是，这样可以在界面上直接显示，后来性能调优的时候，我把这些状态码全改成整型，然后用位操作查状态，因为有一个每秒钟被调用了150K次的函数里面有三处需要检查状态，经过改善以后，整个系统的性能上升了30%左右。还有一个例子是，我以前从事的某个产品编程规范中有一条是要在每个函数中把函数名定义出来，如：const char fname[]=”functionName()”, 这是为了好打日志，但是为什么不声明成 static类型的呢？
- **多线程调优**。有人说，thread is evil，这个对于系统性能在某些时候是个问题。因为多线程瓶颈就在于互斥和同步的锁上，以及线程上下文切换的成本，怎么样的少用锁或不用锁是根本（比如：多版本并发控制(MVCC)在分布式系统中的应用 中说的乐观锁可以解决性能问题），此外，还有读写锁也可以解决大多数是读操作的并发的性能问题。这里多说一点在C++中，我们可能会使用线程安全的智能指针AutoPtr或是别的一些容器，只要是线程安全的，其不管三七二十一都要上锁，上锁是个成本很高的操作，使用AutoPtr会让我们的系统性能下降得很快，如果你可以保证不会有线程并发问题，那么你应该不要用AutoPtr。我记得我上次我们同事去掉智能指针的引用计数，让系统性能提升了50%以上。对于Java对象的引用计数，如果我猜的没错的话，到处都是锁，所以，Java的性能问题一直是个问题。另外，线程不是越多越好，线程间的调度和上下文切换也是很夸张的事，尽可能的在一个线程里干，尽可能的不要同步线程。这会让你有很多的性能。
- **内存分配**。不要小看程序的内存分配。malloc/realloc/calloc这样的系统调非常耗时，尤其是当内存出现碎片的时候。我以前的公司出过这样一个问题——在用户的站点上，我们的程序有一天不响应了，用GDB跟进去一看，系统hang在了malloc操作上，20秒都没有返回，重启一些系统就好了。这就是内存碎片的问题。这就是为什么很多人抱怨STL有严重的内存碎片的问题，因为太多的小内存的分配释放了。有很多人会以为用内存池可以解决这个问题，但是实际上他们只是重新发明了Runtime-C或操作系统的内存管理机制，完全于事无补。当然解决内存碎片的问题还是通过内存池，具体来说是一系列不同尺寸的内存池（这个留给大家自己去思考）。当然，少进行动态内存分配是最好的。说到内存池就需要说一下池化技术。比如线程池，连接池等。池化技术对于一些短作业来说（如http服务） 相当相当的有效。这项技术可以减少链接建立，线程创建的开销，从而提高性能。
- **异步操作**。我们知道Unix下的文件操作是有block和non-block的方式的，像有些系统调用也是block式的，如：Socket下的select，Windows下的WaitforObject之类的，如果我们的程序是同步操作，那么会非常影响性能，我们可以改成异步的，但是改成异步的方式会让你的程序变复杂。异步方式一般要通过队列，要注间队列的性能问题，另外，异步下的状态通知通常是个问题，比如消息事件通知方式，有callback方式，等，这些方式同样可能会影响你的性能。但是通常来说，异步操作会让性能的吞吐率有很大提升（Throughput），但是会牺牲系统的响应时间（latency）。这需要业务上支持。
- **语言和代码库**。我们要熟悉语言以及所使用的函数库或类库的性能。比如：STL中的很多容器分配了内存后，那怕你删除元素，内存也不会回收，其会造成内存泄露的假像，并可能造成内存碎片问题。再如，STL某些容器的size()==0 和 empty()是不一样的，因为，size()是O(n)复杂度，empty()是O(1)的复杂度，这个要小心。Java中的JVM调优需要使用的这些参数：-Xms -Xmx -Xmn -XX:SurvivorRatio -XX:MaxTenuringThreshold，还需要注意JVM的GC，GC的霸气大家都知道，尤其是full GC（还整理内存碎片），他就像“恐龙特级克赛号”一样，他运行的时候，整个世界的时间都停止了。

**4.3）网络调优**

关于网络调优，尤其是TCP Tuning（你可以以这两个关键词在网上找到很多文章），这里面有很多很多东西可以说。看看Linux下TCP/IP的那么多参数就知道了（顺便说一下，你也许不喜欢Linux，但是你不能否认Linux给我们了很多可以进行内核调优的权力）。强烈建议大家看看《TCP/IP 详解 卷1:协议》这本书。我在这里只讲一些概念上的东西。

**A） TCP调优**

我们知道TCP链接是有很多开销的，一个是会占用文件描述符，另一个是会开缓存，一般来说一个系统可以支持的TCP链接数是有限的，我们需要清楚地认识到TCP链接对系统的开销是很大的。正是因为TCP是耗资源的，所以，很多攻击都是让你系统上出现大量的TCP链接，把你的系统资源耗尽。比如著名的SYNC Flood攻击。

所以，我们要注意配置KeepAlive参数，这个参数的意思是定义一个时间，如果链接上没有数据传输，系统会在这个时间发一个包，如果没有收到回应，那么TCP就认为链接断了，然后就会把链接关闭，这样可以回收系统资源开销。（注：HTTP层上也有KeepAlive参数）对于像HTTP这样的短链接，设置一个1-2分钟的keepalive非常重要。这可以在一定程度上防止DoS攻击。有下面几个参数（下面这些参数的值仅供参考）：

![img](https://pic3.zhimg.com/80/v2-e46362582d48de263c55cd867c1dbf7e_720w.webp)

对于TCP的TIME_WAIT这个状态，主动关闭的一方进入TIME_WAIT状态，TIME_WAIT状态将持续2个MSL(Max Segment Lifetime)，默认为4分钟，TIME_WAIT状态下的资源不能回收。有大量的TIME_WAIT链接的情况一般是在HTTP服务器上。对此，有两个参数需要注意，

![img](https://pic1.zhimg.com/80/v2-6434029bda5e5b519774a2343561c020_720w.webp)

前者表示重用TIME_WAIT，后者表示回收TIME_WAIT的资源。

TCP还有一个重要的概念叫RWIN（TCP Receive Window Size），这个东西的意思是，我一个TCP链接在没有向Sender发出ack时可以接收到的最大的数据包。为什么这个很重要？因为如果Sender没有收到Receiver发过来ack，Sender就会停止发送数据并会等一段时间，如果超时，那么就会重传。这就是为什么TCP链接是可靠链接的原因。重传还不是最严重的，如果有丢包发生的话，TCP的带宽使用率会马上受到影响（会盲目减半），再丢包，再减半，然后如果不丢包了，就逐步恢复。相关参数如下：

![img](https://pic3.zhimg.com/80/v2-e1fa7786e080f37dc9f2d86776fbd386_720w.webp)

一般来说，理论上的RWIN应该设置成：吞吐量 * 回路时间。Sender端的buffer应该和RWIN有一样的大小，因为Sender端发送完数据后要等Receiver端确认，如果网络延时很大，buffer过小了，确认的次数就会多，于是性能就不高，对网络的利用率也就不高了。也就是说，对于延迟大的网络，我们需要大的buffer，这样可以少一点ack，多一些数据，对于响应快一点的网络，可以少一些buffer。因为，如果有丢包（没有收到ack），buffer过大可能会有问题，因为这会让TCP重传所有的数据，反而影响网络性能。（当然，网络差的情况下，就别玩什么高性能了） 所以，高性能的网络重要的是要让网络丢包率非常非常地小（基本上是用在LAN里），如果网络基本是可信的，这样用大一点的buffer会有更好的网络传输性能（来来回回太多太影响性能了）。

另外，我们想一想，如果网络质量非常好，基本不丢包，而业务上我们不怕偶尔丢几个包，如果是这样的话，那么，我们为什么不用速度更快的UDP呢？你想过这个问题了吗？

**B）UDP调优**

说到UDP的调优，有一些事我想重点说一样，那就是MTU——最大传输单元（其实这对TCP也一样，因为这是链路层上的东西）。所谓最大传输单元，你可以想像成是公路上的公交车，假设一个公交车可以最多坐70人，带宽就像是公路的车道数一样，如果一条路上最多可以容下100辆公交车，那意味着我最多可以运送7000人，但是如果公交车坐不满，比如平均每辆车只有20人，那么我只运送了2000人，于是我公路资源（带宽资源）就被浪费了。 所以，我们对于一个UDP的包，我们要尽量地让他大到MTU的最大尺寸再往网络上传，这样可以最大化带宽利用率。对于这个MTU，以太网是1500字节，光纤是4352字节，802.11无线网是7981。但是，当我们用TCP/UDP发包的时候，我们的有效负载Payload要低于这个值，因为IP协议会加上20个字节，UDP会加上8个字节（TCP加的更多），所以，一般来说，你的一个UDP包的最大应该是1500-8-20=1472，这是你的数据的大小。当然，如果你用光纤的话， 这个值就可以更大一些。（顺便说一下，对于某些NB的千光以态网网卡来说，在网卡上，网卡硬件如果发现你的包的大小超过了MTU，其会帮你做fragment，到了目标端又会帮你做重组，这就不需要你在程序中处理了）

再多说一下，使用Socket编程的时候，你可以使用setsockopt() 设置 SO_SNDBUF/SO_RCVBUF 的大小，TTL和KeepAlive这些关键的设置，当然，还有很多，具体你可以查看一下Socket的手册。

最后说一点，UDP还有一个最大的好处是multi-cast多播，这个技术对于你需要在内网里通知多台结点时非常方便和高效。而且，多播这种技术对于机会的水平扩展（需要增加机器来侦听多播信息）也很有利。

**C）网卡调优**

对于网卡，我们也是可以调优的，这对于千兆以及网网卡非常必要，在Linux下，我们可以用ifconfig查看网上的统计信息，如果我们看到overrun上有数据，我们就可能需要调整一下txqueuelen的尺寸（一般默认为1000），我们可以调大一些，如：ifconfig eth0 txqueuelen 5000。Linux下还有一个命令叫：ethtool可以用于设置网卡的缓冲区大小。在Windows下，我们可以在网卡适配器中的高级选项卡中调整相关的参数（如：Receive Buffers, Transmit Buffer等，不同的网卡有不同的参数）。把Buffer调大对于需要大数据量的网络传输非常有效。

**D）其它网络性能**

关于多路复用技术，也就是用一个线程来管理所有的TCP链接，有三个系统调用要重点注意：一个是select，这个系统调用只支持上限1024个链接，第二个是poll，其可以突破1024的限制，但是select和poll本质上是使用的轮询机制，轮询机制在链接多的时候性能很差，因主是O(n)的算法，所以，epoll出现了，epoll是操作系统内核支持的，仅当在链接活跃时，操作系统才会callback，这是由操作系统通知触发的，但其只有Linux Kernel 2.6以后才支持（准确说是2.5.44中引入的），当然，如果所有的链接都是活跃的，过多的使用epoll_ctl可能会比轮询的方式还影响性能，不过影响的不大。

另外，关于一些和DNS Lookup的系统调用要小心，比如：gethostbyaddr/gethostbyname，这个函数可能会相当的费时，因为其要到网络上去找域名，因为DNS的递归查询，会导致严重超时，而又不能通过设置什么参数来设置time out，对此你可以通过配置hosts文件来加快速度，或是自己在内存中管理对应表，在程序启动时查好，而不要在运行时每次都查。另外，在多线程下面，gethostbyname会一个更严重的问题，就是如果有一个线程的gethostbyname发生阻塞，其它线程都会在gethostbyname处发生阻塞，这个比较变态，要小心。（你可以试试GNU的gethostbyname_r()，这个的性能要好一些） 这种到网上找信息的东西很多，比如，如果你的Linux使用了NIS，或是NFS，某些用户或文件相关的系统调用就很慢，所以要小心。

**4.4）系统调优**

**A）I/O模型**

前面说到过select/poll/epoll这三个系统调用，我们都知道，Unix/Linux下把所有的设备都当成文件来进行I/O，所以，那三个操作更应该算是I/O相关的系统调用。说到 I/O模型，这对于我们的I/O性能相当重要，我们知道，Unix/Linux经典的I/O方式是（关于Linux下的I/O模型，大家可以读一下这篇文章《使用异步I/O大大提高性能》）：

第一种，同步阻塞式I/O，这个不说了。

第二种，同步无阻塞方式。其通过fctnl设置 O_NONBLOCK 来完成。

第三种，对于select/poll/epoll这三个是I/O不阻塞，但是在事件上阻塞，算是：I/O异步，事件同步的调用。

第四种，AIO方式。这种I/O 模型是一种处理与 I/O 并行的模型。I/O请求会立即返回，说明请求已经成功发起了。在后台完成I/O操作时，向应用程序发起通知，通知有两种方式：一种是产生一个信号，另一种是执行一个基于线程的回调函数来完成这次 I/O 处理过程。

第四种因为没有任何的阻塞，无论是I/O上，还是事件通知上，所以，其可以让你充分地利用CPU，比起第二种同步无阻塞好处就是，第二种要你一遍一遍地去轮询。Nginx之所所以高效，是其使用了epoll和AIO的方式来进行I/O的。

再说一下Windows下的I/O模型，

a）一个是WriteFile系统调用，这个系统调用可以是同步阻塞的，也可以是同步无阻塞的，关于看文件是不是以Overlapped打开的。关于同步无阻塞，需要设置其最后一个参数Overlapped，微软叫Overlapped I/O，你需要WaitForSingleObject才能知道有没有写完成。这个系统调用的性能可想而知。

b）另一个叫WriteFileEx的系统调用，其可以实现异步I/O，并可以让你传入一个callback函数，等I/O结束后回调之， 但是这个回调的过程Windows是把callback函数放到了APC（Asynchronous Procedure Calls）的队列中，然后，只用当应用程序当前线程成为可被通知状态（Alterable）时，才会被回调。只有当你的线程使用了这几个函数时WaitForSingleObjectEx, WaitForMultipleObjectsEx, MsgWaitForMultipleObjectsEx, SignalObjectAndWait 和 SleepEx，线程才会成为Alterable状态。可见，这个模型，还是有wait，所以性能也不高。

c）然后是IOCP – IO Completion Port，IOCP会把I/O的结果放在一个队列中，但是，侦听这个队列的不是主线程，而是专门来干这个事的一个或多个线程去干（老的平台要你自己创建线程，新的平台是你可以创建一个线程池）。IOCP是一个线程池模型。这个和Linux下的AIO模型比较相似，但是实现方式和使用方式完全不一样。

当然，真正提高I/O性能方式是把和外设的I/O的次数降到最低，最好没有，所以，对于读来说，内存cache通常可以从质上提升性能，因为内存比外设快太多了。对于写来说，cache住要写的数据，少写几次，但是cache带来的问题就是实时性的问题，也就是latency会变大，我们需要在写的次数上和相应上做权衡。

**B）多核CPU调优**

关于CPU的多核技术，我们知道，CPU0是很关键的，如果0号CPU被用得过狠的话，别的CPU性能也会下降，因为CPU0是有调整功能的，所以，我们不能任由操作系统负载均衡，因为我们自己更了解自己的程序，所以，我们可以手动地为其分配CPU核，而不会过多地占用CPU0，或是让我们关键进程和一堆别的进程挤在一起。

- 对于Windows来说，我们可以通过“任务管理器”中的“进程”而中右键菜单中的“设置相关性……”（Set Affinity…）来设置并限制这个进程能被运行在哪些核上。
- 对于Linux来说，可以使用taskset命令来设置（你可以通过安装schedutils来安装这个命令：apt-get install schedutils）

多核CPU还有一个技术叫NUMA技术（Non-Uniform Memory Access）。传统的多核运算是使用SMP(Symmetric Multi-Processor )模式，多个处理器共享一个集中的存储器和I/O总线。于是就会出现一致存储器访问的问题，一致性通常意味着性能问题。NUMA模式下，处理器被划分成多个node， 每个node有自己的本地存储器空间。关于NUMA的一些技术细节，你可以查看一下这篇文章《Linux 的 NUMA 技术》，在Linux下，对NUMA调优的命令是：**numactl** 。如下面的命令：（指定命令“myprogram arg1 arg2”运行在node 0 上，其内存分配在node 0 和 1上）

| 1    | numactl --cpubind=0 --membind=0,1 myprogram arg1 arg2 |
| ---- | ----------------------------------------------------- |
|      |                                                       |

当然，上面这个命令并不好，因为内存跨越了两个node，这非常不好。最好的方式是只让程序访问和自己运行一样的node，如：

| 1    | $ numactl --membind 1 --cpunodebind 1 --localalloc myapplication |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

**C）文件系统调优**

关于文件系统，因为文件系统也是有cache的，所以，为了让文件系统有最大的性能。首要的事情就是分配足够大的内存，这个非常关键，在Linux下可以使用free命令来查看 free/used/buffers/cached，理想来说，buffers和cached应该有40%左右。然后是一个快速的硬盘控制器，SCSI会好很多。最快的是Intel SSD 固态硬盘，速度超快，但是写次数有限。

接下来，我们就可以调优文件系统配置了，对于Linux的Ext3/4来说，几乎在所有情况下都有所帮助的一个参数是关闭文件系统访问时间，在/etc/fstab下看看你的文件系统 有没有noatime参数（一般来说应该有），还有一个是dealloc，它可以让系统在最后时刻决定写入文件发生时使用哪个块，可优化这个写入程序。还要注间一下三种日志模式：data=journal、data=ordered和data=writeback。默认设置data=ordered提供性能和防护之间的最佳平衡。

当然，对于这些来说，ext4的默认设置基本上是最佳优化了。

这里介绍一个Linux下的查看I/O的命令—— iotop，可以让你看到各进程的磁盘读写的负载情况。

其它还有一些关于NFS、XFS的调优，大家可以上google搜索一些相关优化的文章看看。关于各文件系统，大家可以看一下这篇文章——《Linux日志文件系统及性能分析》

**4.5）数据库调优**

数据库调优并不是我的强项，我就仅用我非常有限的知识说上一些吧。注意，下面的这些东西并不一定正确，因为在不同的业务场景，不同的数据库设计下可能会得到完全相反的结论，所以，我仅在这里做一些一般性的说明，具体问题还要具体分析。

**A）数据库引擎调优**

我对数据库引擎不是熟，但是有几个事情我觉得是一定要去了解的。

- **数据库的锁的方式**。这个非常非常地重要。并发情况下，锁是非常非常影响性能的。各种隔离级别，行锁，表锁，页锁，读写锁，事务锁，以及各种写优先还是读优先机制。性能最高的是不要锁，所以，分库分表，冗余数据，减少一致性事务处理，可以有效地提高性能。NoSQL就是牺牲了一致性和事务处理，并冗余数据，从而达到了分布式和高性能。
- **数据库的存储机制**。不但要搞清楚各种类型字段是怎么存储的，更重要的是数据库的数据存储方式，是怎么分区的，是怎么管理的，比如Oracle的数据文件，表空间，段，等等。了解清楚这个机制可以减轻很多的I/O负载。比如：MySQL下使用show engines;可以看到各种存储引擎的支持。不同的存储引擎有不同的侧重点，针对不同的业务或数据库设计会让你有不同的性能。
- **数据库的分布式策略**。最简单的就是复制或镜像，需要了解分布式的一致性算法，或是主主同步，主从同步。通过了解这种技术的机理可以做到数据库级别的水平扩展。

**B）SQL语句优化**

关于SQL语句的优化，首先也是要使用工具，比如：MySQL SQL Query Analyzer，Oracle SQL Performance Analyzer，或是微软SQL Query Analyzer，基本上来说，所有的RMDB都会有这样的工具，来让你查看你的应用中的SQL的性能问题。 还可以使用explain来看看SQL语句最终Execution Plan会是什么样的。

还有一点很重要，数据库的各种操作需要大量的内存，所以服务器的内存要够，优其应对那些多表查询的SQL语句，那是相当的耗内存。

下面我根据我有限的数据库SQL的知识说几个会有性能问题的SQL：

- **全表检索**。比如：select * from user where lastname = “xxxx”，这样的SQL语句基本上是全表查找，线性复杂度O(n)，记录数越多，性能也越差（如：100条记录的查找要50ms，一百万条记录需要5分钟）。对于这种情况，我们可以有两种方法提高性能：一种方法是分表，把记录数降下来，另一种方法是建索引（为lastname建索引）。索引就像是key-value的数据结构一样，key就是where后面的字段，value就是物理行号，对索引的搜索复杂度是基本上是O(log(n)) ——用B-Tree实现索引（如：100条记录的查找要50ms，一百万条记录需要100ms）。

- **索引**。对于索引字段，最好不要在字段上做计算、类型转换、函数、空值判断、字段连接操作，这些操作都会破坏索引原本的性能。当然，索引一般都出现在Where或是Order by字句中，所以对Where和Order by子句中的子段最好不要进行计算操作，或是加上什么NOT之类的，或是使用什么函数。

- **多表查询**。关系型数据库最多的操作就是多表查询，多表查询主要有三个关键字，EXISTS，IN和JOIN（关于各种join，可以参看图解SQL的Join一文）。基本来说，现代的数据引擎对SQL语句优化得都挺好的，JOIN和IN/EXISTS在结果上有些不同，但性能基本上都差不多。有人说，EXISTS的性能要好于IN，IN的性能要好于JOIN，我各人觉得，这个还要看你的数据、schema和SQL语句的复杂度，对于一般的简单的情况来说，都差不多，所以千万不要使用过多的嵌套，千万不要让你的SQL太复杂，宁可使用几个简单的SQL也不要使用一个巨大无比的嵌套N级的SQL。还有人说，如果两个表的数据量差不多，Exists的性能可能会高于In，In可能会高于Join，如果这两个表一大一小，那么子查询中，Exists用大表，In则用小表。这个，我没有验证过，放在这里让大家讨论吧。另，有一篇关于SQL Server的文章大家可以看看《IN vs JOIN vs EXISTS》

- **JOIN操作**。有人说，Join表的顺序会影响性能，只要Join的结果集是一样，性能和join的次序无关。因为后台的数据库引擎会帮我们优化的。Join有三种实现算法，嵌套循环，排序归并，和Hash式的Join。（MySQL只支持第一种）

- - 嵌套循环，就好像是我们常见的多重嵌套循环。注意，前面的索引说过，数据库的索引查找算法用的是B-Tree，这是O(log(n))的算法，所以，整个算法复法度应该是O(log(n)) * O(log(m)) 这样的。
  - Hash式的Join，主要解决嵌套循环的O(log(n))的复杂，使用一个临时的hash表来标记。
  - 排序归并，意思是两个表按照查询字段排好序，然后再合并。当然，索引字段一般是排好序的。

还是那句话，具体要看什么样的数据，什么样的SQL语句，你才知道用哪种方法是最好的。

- **部分结果集。**我们知道MySQL里的Limit关键字，Oracle里的rownum，SQL Server里的Top都是在限制前几条的返回结果。这给了我们数据库引擎很多可以调优的空间。一般来说，返回top n的记录数据需要我们使用order by，注意在这里我们需要为order by的字段建立索引。有了被建索引的order by后，会让我们的select语句的性能不会被记录数的所影响。使用这个技术，一般来说我们前台会以分页方式来显现数据，Mysql用的是OFFSET，SQL Server用的是FETCH NEXT，这种Fetch的方式其实并不好是线性复杂度，所以，如果我们能够知道order by字段的第二页的起始值，我们就可以在where语句里直接使用>=的表达式来select，这种技术叫seek，而不是fetch，seek的性能比fetch要高很多。

- **字符串**。正如我前面所说的，字符串操作对性能上有非常大的恶梦，所以，能用数据的情况就用数字，比如：时间，工号，等。

- **全文检索**。千万不要用Like之类的东西来做全文检索，如果要玩全文检索，可以尝试使用Sphinx。

- **其它**。

- - 不要select *，而是明确指出各个字段，如果有多个表，一定要在字段名前加上表名，不要让引擎去算。
  - 不要用Having，因为其要遍历所有的记录。性能差得不能再差。
  - 尽可能地使用UNION ALL 取代 UNION。
  - 索引过多，insert和delete就会越慢。而update如果update多数索引，也会慢，但是如果只update一个，则只会影响一个索引表。
  - 等等。

原文地址：https://zhuanlan.zhihu.com/p/606956932

作者：Linux

# 【NO.568】详解进程的虚拟内存，物理内存，共享内存

## 1.写在前面：

想必在Linux上写过程序的同学都有分析进程占用多少内存的经历，或者被问到这样的问题，你的程序在运行时占用了多少内存（物理内存）？

通常我们可以通过top命令查看进程占用了多少内存。这里我们可以看到VIRT、RES和SHR三个重要的指标，他们分别代表什么意思呢？这是本文需要跟大家一起探讨的问题。

当然如果更加深入一点，你可能会问进程所占用的那些物理内存都用在了哪些地方？这时候top命令可能不能给到你想要的答案了，不过我们可以分析proc文件系统提供的 smaps文件，这个文件详尽地列出了当前进程所占用物理内的使用情况。

这篇文章总共分为三个部分。

第一部分简要阐述虚拟内存和驻留内存这两个重要的概念；第二部分解释top命令中VIRT、RES以及SHR三个参数的实际参考意义；最后一部分向大家介绍下 smaps文件的格式，通过分析 smaps文件我们可以详细了解进程物理内存的使用情况，比如mmap文件占用了多少空间、动态内存开辟消耗了多少空间、函数调用消耗了多少空间等等。

## 2.关于内存的两个概念

要理解top命令关于内存使用情况的输出，我们必须首先搞清楚虚拟内存（ Virtual Memory）和驻留内存（ Resident Memory）两个概念。

### 2.1 虚拟内存

首先需要强调的是虚拟内存不同于物理内存，虽然两者都包含内存字眼但是它们属于两个不同层面的概念。

进程占用虚拟内存空间大并非意味着程序的物理内存也一定占用很大。

虚拟内存是操作系统内核为了对进程地址空间进行管理（ process address space management）而精心设计的一个逻辑意义上的内存空间概念。我们程序中的指针其实都是这个虚拟内存空间中的地址。比如我们在写完一段C++程序之后都需要采用g++进行编译，这时候编译器采用的地址其实就是虚拟内存空间的地址。因为这时候程序还没有运行，何谈物理内存空间地址？凡是程序运行过程中可能需要用到的指令或者数据都必须在虚拟内存空间中。

既然说虚拟内存是个逻辑意义上（假象的）的内存空间，为了能够让程序在物理机器上运行，那么必须有一套机制可以让这些假象的虚拟内存空间映射到物理内存空间（实实在在的RAM内存条上的空间）。这其实就是操作系统中页映射表（ page table）所做的事情了。内核会为系统中每个进程维护一份相互独立的页映射表。页映射表的基本原理是将程序运行过程中需要访问的段虚拟内存空间通过页映射表映射到一段物理内存空间上，这样CPU访问对应虚拟内存地址的时候就可以通过这种查找页映射表的机制访问物理内存上的某个对应的地址。“页（page）"是虚拟内存空间向物理内存空间映射的基本单元.

下图1演示了虚拟内存空间和物理内存空间的映射关系，它们通过 Page Table关联起来，其中虚拟内存空间中着色的部分分别被映射到物理内存空间对应相同着色的部分。而虚拟内存空间中灰色的部分表示在物理内存空间中没有与之对应的部分，也就是说灰色部分没有被映射到物理内存空间中。这么做也是本着“按需映射”的指导思想，因为虚拟内存空间很大，可能其中很多部分在一次程序运行过程中根本不需要访问，所以也就没有必要将虚拟内存空间中的这些部分映射到物理内存空间上。

到这里为止已经基本阐述了什么是虚拟内存了。

**总结一下就是，虚拟内存是一个假象的内存空间，在程序运行过程中虚拟内存空间中需要被访问的部分会被映射到物理内存空间中。虚拟内空间大只能表示程序运行过程中可访问的空间比较大，不代表物理内存空间占用也大。**

![img](https://pic3.zhimg.com/80/v2-129f3316721f119d45d36bca3a66bd1e_720w.webp)

图1 虚拟内存空间和物理内存空间的映射关系

![img](https://pic2.zhimg.com/80/v2-a389303a2343637529f53e327753b715_720w.webp)

图1-2

### 2.2 驻留内存

驻留内存，顾名思义是指那些被映射到进程虛拟内存空间的物理内存。上图1中，在系统物理内存空间中被着色的部分都是驻留内存。比如，A1、A2、A3和A4是进程A的驻留内存B1、B2和B3是进程B的驻留内存。

进程的驻留内存就是进程实实在在占用的物理内存。一般我们所讲的进程占用了多少内存，其实就是说的占用了多少驻留内存而不是多少虚拟内存。因为虛拟内存大并不意味着占用的物理内存大。

关于虚拟内存和驻留内存这两个概念我们说到这里。

## 3.详解top命令中VIRT、RES和SHR

下面一部分我们来看看top命令中ⅥRT RES和SHR分别代表什么意思。

top命令作为 Linux下最常用的性能分析工具之一，可以监控、收集进程的CPU、IO存使用情况。比如我们可以通过top命令获得个进程使用了多少虚拟内存（VIRT）、物理内存（RES）、共享内存（SHR）

**top命令中ⅥRT、RES和SHR的含义**

VIRT的含义。搞清楚了虚拟内存的概念之后解释ⅥRT的含义就很简单了。ⅥRT表示的是进程虚拟内存空间大小。对应到图1中的进程A来说就是A1、A2、A3、A4以及灰色部分所有空间的总和.也就是说VIRT包含了在已经映射到物理内存空间的部分和尚未映射到物理内存空间的部分和。

RES的含义。指进程虚拟內存空间中已经映射到物理內存空间的那部分的大小。对应到图1中的进程A来说就是A1、A2、A3以及A4几个部分空间的总和。所以说，看进程在运行过程中占了多少内存应该看RES的值而不是VIRT的值。

SHR的含义。SHR是 share（共享）的缩写，它表示的是进程占用的共享内存大小。在上图1中我们看到进程A虚拟内存空间中的A4和进程B虚拟内存空间中的B3都映射到了物理内存空间的A4/B3。为什么会出现这样的情况呢？其实我们写的程序会依赖于很多外部的动态库（.so），比如libc.so、libd.so等等。这些动态库在内存中仅仅会保存/映射一份，如果某个进程运行时需要这个动态库，那么动态加载器会将这块内存映到对应进程的虚拟内存空间中。多个进程之间通过共享内存的方式相互通信也会出现这样的凊况。这么一来，就会出现不同进程的虚拟内存空间会映射到相同的物理内存空间。这部分物理内存空间其实是被多个进程所共享的，所以我们将他们称为共享内存，用SHR来表示。某个进程占用的内存除了和别的进程共享的内存之外就是自己的独占内存了。所以要计算进程独占内存的大小只要用RES的值减去SHR值即可 。

## 4.进程的smaps文件

查看命令是：cat/proc/进程的pid/ smaps

通过top命令我们已经能看出进程的虚拟空间大小（VIRT）、占用的物理内存（RES）以及和其他进程共享的内存（SHR）。但是仅此而已，如果我想知道如下问题:

进程的虚拟内存空间的分布情况，比如heap占用了多少空间、文件映射（mmap）占用了多少空间、 stack占用了多少空间？

进程是否有被交换到swap空间的内存，如果有，被交换出去的大小？

mmap方式打开的数据文件有多少页在内存中是脏页（ dirty page）没有被写回到磁盘的？

mmap方式打开的数据文件当前有多少页面已经在内存中，有多少页面还在磁盘中没有加载到 page caret中？

等等

以上这些问题都无法通过top命令给出答案，但是有时候这些问题正是我们在对程序进行性能瓶颈分析和优化时所需要回答的问题。所幸的是，世界上解决问题的方法总比问题本身要多得多。 linux通过proc文件系统为每个进程都提供了一个 smaps文件，通过分析该文件我们就可以一一回答以上提出的问题。

在 smaps文件中，每一条记录（如下图2所示）表示进程虚拟内存空间中一块连续的区域。其中第一行从左到右依次表示地址范围、权限标识、映射文件偏移、设备号、 inode、文件路径。

接下来8个字段的含义分别如下

Size：表示该映射区域在虚拟内存空间中的大小

Rss：表示该映射区域当前在物理内存中占用了多少空间。

Shared_dean：和其他进程共享的未被改写的page的大小

Shared_Dirty：和其他进程共享的被改写的page的大小

Private_clean：未被改写的私有页面的大小。

swap：表示非mmap内存（也叫 anonymous memory，比如malloc动态分配出来的内存）

由于物理内存不足被swap到交换空间的大小。

Ps：该虚拟內存区域平摊计算后使用的物理內存大小（有些内存会和其他进程共享，例如mmap进来的）。比如该区域所映射的物理内存部分同时也被另一个进程映射了，且该部分物理内存的大小为1000KB，那么该进程分摊其中一半的内存，即PSs=500KB

![img](https://pic1.zhimg.com/80/v2-a5a09f10b6e3eb68e10a305094f526d8_720w.webp)

图2. smaps文件示例

最后希望所有读者能够通过阅读本文对进程的虚拟内存和物理内存有一个更加清晰认识，并能更加准确理解top命令关于内存的输出，最后可以通过 smaps文件更进一步分析进程使用内存的情况。

原文地址：https://zhuanlan.zhihu.com/p/606881804

作者：Linux

# 【NO.569】浅谈TCP/IP网络编程中socket的行为

想要熟练掌握Linux下的TCP/IP网络编程，至少有三个层面的知识需要熟悉：

\1. TCP/IP协议（如连接的建立和终止、重传和确认、滑动窗口和拥塞控制等等）

\2. Socket I/O系统调用（重点如read/write），这是TCP/IP协议在应用层表现出来的行为。

\3. 编写Performant, Scalable的服务器程序。包括多线程、IO Multiplexing、非阻塞、异步等各种技术。

关于TCP/IP协议，建议参考Richard Stevens的《TCP/IP Illustrated，vol1》（TCP/IP详解卷1）。

关于第二层面，依然建议Richard Stevens的《Unix network proggramming，vol1》（Unix网络编程卷1），这两本书公认是Unix网络编程的圣经。

至于第三个层面，UNP的书中有所提及，也有著名的[C10K问题](https://link.zhihu.com/?target=http%3A//www.kegel.com/c10k.html)，业界也有各种各样的框架和解决方案，本人才疏学浅，在这里就不一一敷述。

本文的重点在于第二个层面，主要总结一下Linux下TCP/IP网络编程中的read/write系统调用的行为，知识来源于自己网络编程的粗浅经验和对《Unix网络编程卷1》相关章节的总结。

## 1. read/write的语义：为什么会阻塞？

先从write说起：

```text
#include <unistd.h>
ssize_t write(int fd, const void *buf, size_t count);
```

首先，write成功返回，只是buf中的数据被复制到了kernel中的TCP发送缓冲区。至于数据什么时候被发往网络，什么时候被对方主机接收，什么时候被对方进程读取，系统调用层面不会给予任何保证和通知。

write在什么情况下会阻塞？当kernel的该socket的发送缓冲区已满时。对于每个socket，拥有自己的send buffer和receive buffer。从Linux 2.6开始，两个缓冲区大小都由系统来自动调节（autotuning），但一般在default和max之间浮动。

```text
# 获取socket的发送/接受缓冲区的大小：（后面的值是在我在Linux 2.6.38 x86_64上测试的结果）
sysctl net.core.wmem_default       #126976
sysctl net.core.wmem_max　　　　    #131071
sysctl net.core.wmem_default       #126976
sysctl net.core.wmem_max           #131071
```

已经发送到网络的数据依然需要暂存在send buffer中，只有收到对方的ack后，kernel才从buffer中清除这一部分数据，为后续发送数据腾出空间。接收端将收到的数据暂存在receive buffer中，自动进行确认。但如果socket所在的进程不及时将数据从receive buffer中取出，最终导致receive buffer填满，由于TCP的滑动窗口和拥塞控制，接收端会阻止发送端向其发送数据。这些控制皆发生在TCP/IP栈中，对应用程序是透明的，应用程序继续发送数据，最终导致send buffer填满，write调用阻塞。

一般来说，由于接收端进程从socket读数据的速度跟不上发送端进程向socket写数据的速度，最终导致发送端write调用阻塞。

而read调用的行为相对容易理解，从socket的receive buffer中拷贝数据到应用程序的buffer中。read调用阻塞，通常是发送端的数据没有到达。

## 2.blocking（默认）和nonblock模式下read/write行为的区别：

将socket fd设置为nonblock（非阻塞）是在服务器编程中常见的做法，采用blocking IO并为每一个client创建一个线程的模式开销巨大且可扩展性不佳（带来大量的切换开销），更为通用的做法是采用线程池+Nonblock I/O+Multiplexing（select/poll，以及Linux上特有的epoll）。

```text
// 设置一个文件描述符为nonblock
int  set_nonblocking( int  fd)
{
     int  flags;
     if  ((flags = fcntl(fd, F_GETFL, 0)) == -1)
         flags = 0;
     return  fcntl(fd, F_SETFL, flags | O_NONBLOCK);
}
```

**几个重要的结论：**

\1. read总是在接收缓冲区有数据时立即返回，而不是等到给定的read buffer填满时返回。

只有当receive buffer为空时，blocking模式才会等待，而nonblock模式下会立即返回-1（errno = EAGAIN或EWOULDBLOCK）

\2. blocking的write只有在缓冲区足以放下整个buffer时才返回（与blocking read并不相同）

nonblock write则是返回能够放下的字节数，之后调用则返回-1（errno = EAGAIN或EWOULDBLOCK）

对于blocking的write有个特例：当write正阻塞等待时对面关闭了socket，则write则会立即将剩余缓冲区填满并返回所写的字节数，再次调用则write失败（connection reset by peer），这正是下个小节要提到的：

## 3. read/write对连接异常的反馈行为：

对应用程序来说，与另一进程的TCP通信其实是完全异步的过程：

\1. 我并不知道对面什么时候、能否收到我的数据

\2. 我不知道什么时候能够收到对面的数据

\3. 我不知道什么时候通信结束（主动退出或是异常退出、机器故障、网络故障等等）

对于1和2，采用write() -> read() -> write() -> read() ->...的序列，通过blocking read或者nonblock read+轮询的方式，应用程序基于可以保证正确的处理流程。

对于3，kernel将这些事件的“通知”通过read/write的结果返回给应用层。

假设A机器上的一个进程a正在和B机器上的进程b通信：某一时刻a正阻塞在socket的read调用上（或者在nonblock下轮询socket）

当b进程终止时，无论应用程序是否显式关闭了socket（OS会负责在进程结束时关闭所有的文件描述符，对于socket，则会发送一个FIN包到对面）。

”同步通知“：进程a对已经收到FIN的socket调用read，如果已经读完了receive buffer的剩余字节，则会返回EOF:0

”异步通知“：如果进程a正阻塞在read调用上（前面已经提到，此时receive buffer一定为空，因为read在receive buffer有内容时就会返回），则read调用立即返回EOF，进程a被唤醒。

socket在收到FIN后，虽然调用read会返回EOF，但进程a依然可以其调用write，因为根据TCP协议，收到对方的FIN包只意味着对方不会再发送任何消息。 在一个双方正常关闭的流程中，收到FIN包的一端将剩余数据发送给对面（通过一次或多次write），然后关闭socket。

但是事情远远没有想象中简单。优雅地（gracefully)关闭一个TCP连接，不仅仅需要双方的应用程序遵守约定，中间还不能出任何差错。

假如b进程是异常终止的，发送FIN包是OS代劳的，b进程已经不复存在，当机器再次收到该socket的消息时，会回应RST（因为拥有该socket的进程已经终止）。a进程对收到RST的socket调用write时，操作系统会给a进程发送SIGPIPE，默认处理动作是终止进程，知道你的进程为什么毫无征兆地死亡了吧：）

from 《Unix Network programming, vol1》 3rd Edition：

> "It is okay to write to a socket that has received a FIN, but it is an error to write to a socket that has received an RST."

通过以上的叙述，内核通过socket的read/write将双方的连接异常通知到应用层，虽然很不直观，似乎也够用。

**这里说一句题外话：**

不知道有没有同学会和我有一样的感慨：在写TCP/IP通信时，似乎没怎么考虑连接的终止或错误，只是在read/write错误返回时关闭socket，程序似乎也能正常运行，但某些情况下总是会出奇怪的问题。想完美处理各种错误，却发现怎么也做不对。

原因之一是：socket（或者说TCP/IP栈本身）对错误的反馈能力是有限的。

考虑这样的错误情况：

不同于b进程退出（此时OS会负责为所有打开的socket发送FIN包），当B机器的OS崩溃（注意不同于人为关机，因为关机时所有进程的退出动作依然能够得到保证）/主机断电/网络不可达时，a进程根本不会收到FIN包作为连接终止的提示。

如果a进程阻塞在read上，那么结果只能是永远的等待。

如果a进程先write然后阻塞在read，由于收不到B机器TCP/IP栈的ack，TCP会持续重传12次（时间跨度大约为9分钟），然后在阻塞的read调用上返回错误：ETIMEDOUT/EHOSTUNREACH/ENETUNREACH

假如B机器恰好在某个时候恢复和A机器的通路，并收到a某个重传的pack，因为不能识别所以会返回一个RST，此时a进程上阻塞的read调用会返回错误ECONNREST

恩，socket对这些错误还是有一定的反馈能力的，前提是在对面不可达时你依然做了一次write调用，而不是轮询或是阻塞在read上，那么总是会在重传的周期内检测出错误。如果没有那次write调用，应用层永远不会收到连接错误的通知。

write的错误最终通过read来通知应用层，有点阴差阳错？

## 4. 还需要做什么?

至此，我们知道了仅仅通过read/write来检测异常情况是不靠谱的，还需要一些额外的工作：

**1. 使用TCP的KEEPALIVE功能？**

```text
cat /proc/sys/net/ipv4/tcp_keepalive_time
7200

cat /proc/sys/net/ipv4/tcp_keepalive_intvl
75

cat /proc/sys/net/ipv4/tcp_keepalive_probes
9
```

以上参数的大致意思是：keepalive routine每2小时（7200秒）启动一次，发送第一个probe（探测包），如果在75秒内没有收到对方应答则重发probe，当连续9个probe没有被应答时，认为连接已断。（此时read调用应该能够返回错误，待测试）

但在我印象中keepalive不太好用，默认的时间间隔太长，又是整个TCP/IP栈的全局参数：修改会影响其他进程，Linux的下似乎可以修改per socket的keepalive参数？（希望有使用经验的人能够指点一下），但是这些方法不是portable的。

**2. 进行应用层的心跳**

严格的网络程序中，应用层的心跳协议是必不可少的。虽然比TCP自带的keep alive要麻烦不少（怎样正确地实现应用层的心跳，我或许会用一篇专门的文章来谈一谈），但有其最大的优点：可控。

当然，也可以简单一点，针对连接做timeout，关闭一段时间没有通信的”空闲“连接。

原文地址：https://zhuanlan.zhihu.com/p/606584696

作者：linux

# 【NO.570】内存碎片优化（内存池）

学过内存池，跟着视频实现过c的内存池代码，也看过nginx的内存池，在自己的角度对内存池的概念，实现有一定的认知。

但有一次面试，面试官问到内存池的实现原理，如果基于内存池用new对对象进行内存申请如何实现？

第一次思考这个问题，当时却懵了，感谢这次经历，让我发现又一个知识点。

## 1.引入思考（操作系统管理内存==>内存池）

### 1.1 如何实现内存池

操作系统管理内存的方式是 提供malloc/calloc/realloc 及free接口。

参考操作系统管理内存的方式，我们要实现内存池，即控制一块内存并进行管理，需要实现可供对外使用的类似的接口（这里可以采用hook/重载的方式）。

==》hook:接管系统库函数，调用自己函数的技术。

### 1.2 实现内存池的意义

**1.2.1：进程空间，堆、栈内存（没有十分搞懂）**

linux虚拟地址空间范围为0~4G(32位操作系统)。

每个进程运行在属于自己的内存沙盒中（虚拟地址空间），在内存中，由页表（操作系统控制）映射到物理内存。

linux内核将4G空间分为两部分，

===》（从虚拟地址0xC0000000到0xFFFFFFFF）供内核使用，称为“内核空间”。

===》而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF）供各个进程使用，称为“用户空间。

===》可以通过系统调用进入内核，内核由所有进程共享。

===》内核空间持续存在，所有进程中映射到同样的物理内存，公用内核地址，内核的代码，数据是一样的

===》进程用的虚拟内存，由操作系统页表映射到对应的物理内存。

===》虚拟地址保护内核空间不被用户破坏。

===》内核为每个进程维护一张页表？描述每页在虚拟内存的地址以及对应的物理地址。

![img](https://pic1.zhimg.com/80/v2-6fb22a5c42e79725733e5bab230cafa4_720w.webp)

简单分析：

===》进程启动时会分配进程空间。

===》运行时，对堆和栈内存区域进行控制。

===》内存映射段在进程申请大块内存（malloc）时会使用。

===》栈内存：遵循FIFO的顺序，由进程控制。

===》堆内存，提供对应的api(malloc/calloc/realloc/free)，由用户控制。

===》栈内存操作系统提供了大小限制（看有的文档说是8M,我没验证），那么申请过多，会有栈溢出的报错。

===》堆内存：需要程序员自己释放，或者程序结束时，由os回收，由链表进行控制，查找空闲链表进行分配内存。（会有内存碎片）

遗留：如果不考虑物理内存，这里的虚拟内存逻辑大约这些，如果考虑物理内存呢？ 以及linux环境下进程的个数最多有多少个？ 有参数限制，与内存有什么关系？等内存相关一系列问题。

**1.2.2：内存碎片**

1：操作系统提供api(malloc/calloc/realloc/free)由程序员控制内存的申请与释放，底层是通过链表进行管理内存（已经使用的内存/已经释放的可重用内存）的。

2：链表管理堆内存，频繁的申请和释放（例如，频繁申请和释放小内存），

===》小内存释放后，链表管理中，无法有效重用，内存利用率不高，内存碎片过多，甚至可能用完内存，导致程序崩溃。

![img](https://pic3.zhimg.com/80/v2-6c133a38b9819bdc4c16c1eb79b4fd06_720w.webp)

注意：项目中大多数的异常崩溃，长时间偶现问题，都是与内存相关的问题。

**1.2.3：内存碎片解决方案：内存池**

解决问题：依赖操作系统api,频繁的申请和释放内存，可能使内存使用率过低，内存碎片过多，对于长时间运行的程序，可能导致内存不够，程序崩溃。

原理：参考操作系统管理（申请/释放）内存的方案，把内存的管理上升到用户层，由我们自己管理。

## 2.实现内存池

### 2.1 引申实现方案

目的： 还是实现类似malloc/free（申请/释放内存）相关api，把由操作系统控制的这类库函数，用自己的方案实现。

实现方案：基本思路，用链表管理每一个申请的内存，设计内存可重用。（最简单方案）

![img](https://pic1.zhimg.com/80/v2-2e330ef35eb8aa6446929f7f7d313644_720w.webp)

===》1：定义节点，保存内存地址，申请大小。

===》2：用链表对当前申请内存进行管理。

===》3：malloc申请内存策略,首先遍历链表，查找可用，可用则用，不可用则申请。

===》4：free内存释放，先查找内存，找到则修改对应节点标志，下次malloc可重用。

===》5：伪代码：

```text
struct memnode{
	void* alloc; //指针
	struct memnode * next; //控制链表
	int flag;   //标识是否释放
	int size;   //申请内存的大小
}; //每一次申请内存的节点，管理内存

// malloc时：
{
	if(memnode_t = Traverse(node_list, size))
	{
		return memnode_t->alloc;
	}
	void *ptr = malloc(size);	//申请节点
	memnode_t->alloc = ptr;		//管理每个节点
	memnode_t->size = size;
	add_node(node_list, memnode_t);
}

//free时：
{
	memnode_t = search(node_list, ptr); //查找要释放的节点
	node->flag = 0;	//bu
}
```

思考： （貌似没有多大意义，和操作系统底层差距不大，引入思考。。。）

缺点：内存的利用率不高，内存只申请没有释放。

 链表遍历查找效率低，free时查找O(n)

### 2.2 nginx的内存池（简单描述）

思考：在内存申请上优化，申请大一点的固定内存，对内部节点的使用进行管理，以一个大块内存为节点进行重用。（貌似nginx就是这样，进程有生命周期，进程的内存会随着进程的释放而销毁，如果特定的业务场景，如socket链接的生命周期使用，可以不关注内存释放，对内存块进行管理即可）

![img](https://pic4.zhimg.com/80/v2-0c7a4240d8459e77da85866aa3a6c9df_720w.webp)

实现方案：申请一块大一点的内存，重载malloc/free函数，对内存块进行管理。

 ===》1：申请一个固定大小块内存（有内存节点对齐问题），以固定块内存为节点组织数据结构进行管理，如链表，用于申请及管理小内存。

 ===》2：每个块内存中，只标识了可申请内存的尾节点，小块内存在根据尾节点及剩余大小判断进行申请还是重新申请块内存。

 ===》3：大块内存使用另外的逻辑，链表进行管理，申请和释放。

 =》4：释放：大块内存直接释放，小块内存不释放。=》（这里只是符合nginx业务的一种处理）

```text
//存储大块内存的指针
typedef struct mp_large_t{
	struct mp_large_t *next;
	void * alloc;
}MP_LARGE_T;

//存储小节点的结构
typedef struct mp_node_t
{
	unsigned char * last;
	unsigned char * end;
	struct mp_node_t *next;
	size_t failed;
}MP_NODE_T;

//定义管理节点的结构 柔性数组，实际的内存是head[0]
typedef struct mp_pool_t
{
	size_t max;		//标识块的大小

	struct mp_node_t * current;	//小块内存申请时使用时用
	struct mp_large_t * large;  //大块内存的节点，起始节点，头插法

	struct mp_node_t head[0]; //小块内存的起始节点,销毁用
}MP_POOL_T;
```

===》扩展：其实可以对小块内存内部的内存进行链表管理，进行重用/释放处理，更可靠。

===》思考及遗留：内存池的设计有时候考虑具体的业务场景，这里未对小块内存进行释放在某些持续运行场景下是不合理的。

### 2.3 优化内存碎片

场景一中，对申请的每块内存进行释放/重用管理减少碎片，和场景中对申请的一块大块固定内存的小块内存进行释放/重用管理，其实是同一种方案喽，

内存池的方案思考： 假设在堆上申请一大块内存（内存池子），专供我们小块内存的申请释放使用，如何设计策略呢？

有两个问题：

===》1：内存的高效使用，可重用，碎片少。 （malloc申请内存策略）

===》2：查找内存的速度，使用数据结构/算法进行优化。（链表，红黑树，hash,b-tree，b+tree，skip-list）（free时，需要找到节点进行释放删除）

查找速度，可以考虑红黑树，依次从内存大小和内存地址作为key值进行考虑。

可行性方案：

站在malloc速度，可用性进行分析：（以申请内存的大小size进行管理）

===》加一些限制，申请小块内存时，不是随机指定，而是以2^n进行申请。

===》根据申请内存的大小，使用数据结构进行管理。

![img](https://pic2.zhimg.com/80/v2-2721044ca94678ecb645c8a216e20eb9_720w.webp)

==》如图，我们可以限定内存申请的大小（始终以2^n进行限定申请内存）。

==》可以使用数组对限定的内存大小进行标识。

==》数组中的每个元素对应一个list，管理的是对应内存大小，申请的链表指向（如array[0]标识内存大小为16的地址申请list节点，下次malloc申请会查找这个list，有则使用，无则申请）

伪代码：

```text
struct memnode{
    void *ptr;
    int flag;		//标识是否释放，如果释放，下次可重用
    struct memnode *next;
};

struct memlist_hdr{
    struct memnode * list;
    int size;
}

struct memtable{
	   struct memlist_hdr array_node[9];//假设了9个
};

//特大大块内存可以重新考虑

void * malloc(int size) //对size进行校验，取离2^n最近的一个值
{
	//用偏移位对size进行处理
	void *ptr = search_table(size); //==在size对应的数组节点链表中进行查找，找到有释放的空闲节点则使用，没找到则申请并插入
	if(!ptr)
	{
		ptr = sys_malloc(size); //这里的size经过偏移处理2^n
		add_node_list(size, ptr);//加入对应的size对应的数组管理的list中
	}
	return ptr;
}

void free()
{
	//如果要对某个节点进行释放，只知道节点的地址，需要遍历所有的节点进行查找，效率太低
}
```

**思考**：这种方案一定程度上可以降低内存碎片的产生，同时，需要考虑极限情况，如果内存不够用如何扩展。（不同的数组之间，如何进行内存的调整扩展）

===》1：大块拆小块，（拆分原则：128=64+32+16+16）

===》2：小块合并大块（依次分析节点中的list，根据相邻节点大小偏移，排序可以合并）

### 2.4 优化释放时查找速度（红黑树）

释放的时候只知道内存的地址，如何有效在1.3.3的基础上对其释放性能进行提升？

===》在上面的基础上，使用红黑树，以地址作为key值，对所有的内存节点进行管理。

伪代码如下：

```text
struct memnode{
    void *ptr;
    int flag;		//标识是否释放，如果释放，下次可重用
    struct memnode *next;
    rbtree_node(memnode) rbtree;
};

struct memlist_hdr{
    struct memnode * list;
    int size;
}

struct memtable{
	struct memlist_hdr array_node[9];//假设了9个
    rbtree_node(memnode) root;
};//管理节点

//释放时，通过这里的管理节点，我们可以很快找到要释放的节点memnode
//通过红黑树中节点的指向，可以对list链表节点进行控制
//同时，在malloc时，往list中插入节点时，通过节点中红黑树的根节点指向插入红黑树。
```

### 2.5 遗留

如果把该实现作为一个框架组件：

 1：线程安全的保证，需要加锁/其他方案。

 2：在整个流程中，没有释放内存也是一个问题，如何回收？（大块拆小块，小块合并大块）

## 3.总结

1：频繁的申请和释放小块内存的业务场景，会导致系统内存碎片过多，甚至可能内存不够用。

2：可以用线程池解决该问题，本质还是对申请的内存指针进行管理，有关内存管理，已经有一些算法（伙伴算法，buddy,slab等）

3：内存池的设计，可以从申请内存的大小和内存的地址两个角度作为key值进行考虑。

4：内存释放问题需要考虑，如果没有把内存回归系统，需要考虑如何回收内存（大块拆小块，小块合并大块）

5：内存池的使用是建立在具体的业务场景下的，可以具体情况具体分析，同时，内存/内存池的生命周期，与使用它的进程/线程生命周期相关。

6：线程安全问题。

说明：

 只是在理论的基础上，对内存池的实现方案进行学习和梳理。

遗留：

 1：内存池中重载new 对象的实现代码。

 2：进程的内存空间，虚拟内存，操作系统进程的内存管理

 3：相关的一些内存控制算法（伙伴算法，buddy,slab等）

 4：已有的实际的一些内存池应用源码库具体了解一下，验证理论。

原文地址：https://zhuanlan.zhihu.com/p/606253119

作者：Linux