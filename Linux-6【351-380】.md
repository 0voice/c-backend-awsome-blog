# 【NO.351】TCP通信过程详解以及tcp长连接和短连接

## **1. TCP连接**

当网络通信时采用TCP协议时，在真正的读写操作之前，server与client之间必须建立一个连接，当读写操作完成后，双方不再需要这个连接 时它们可以释放这个连接，连接的建立是需要三次握手的，而释放则需要4次挥手，所以说每个连接的建立都是需要资源消耗和时间消耗的

经典的三次握手示意图： 

![img](https://pic1.zhimg.com/80/v2-962ac7137ec361006e7712449ede2a00_720w.webp)



经典的四次挥手图： 

![img](https://pic4.zhimg.com/80/v2-9a7535774ab3f2cd3388c67d341f6fcb_720w.webp)



tcp的十种状态图： 

![img](https://pic1.zhimg.com/80/v2-548450728f841a0d593ab683d7065b58_720w.webp)



注意:

当一端收到一个FIN，内核让read返回0来通知应用层另一端已经终止了向本端的数据传送； 
发送FIN通常是应用层对socket进行关闭的结果。

tcp的2MSL问题 

![img](https://pic1.zhimg.com/80/v2-97667acaa84a37f46e7ebdab75648ac4_720w.webp)



## **2. TCP短连接**

我们模拟一下TCP短连接的情况，client向server发起连接请求，server接到请求，然后双方建立连接。client向server 发送消息，server回应client，然后一次读写就完成了，这时候双方任何一个都可以发起close操作，不过一般都是client先发起 close操作。为什么呢，一般的server不会回复完client后立即关闭连接的，当然不排除有特殊的情况。从上面的描述看，短连接一般只会在 client/server间传递一次读写操作

短连接的优点是：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段

## **3.TCP长连接**

接下来我们再模拟一下长连接的情况，client向server发起连接，server接受client连接，双方建立连接。Client与server完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。

首先说一下TCP/IP详解上讲到的TCP保活功能，保活功能主要为服务器应用提供，服务器应用希望知道客户主机是否崩溃，从而可以代表客户使用资 源。如果客户已经消失，使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，则服务器将应远等待客户端的数据，保活功能就是试图在服务 器端检测到这种半开放的连接。

如果一个给定的连接在两小时内没有任何的动作，则服务器就向客户发一个探测报文段，客户主机必须处于以下4个状态之一：

客户主机依然正常运行，并从服务器可达。客户的TCP响应正常，而服务器也知道对方是正常的，服务器在两小时后将保活定时器复位。 
客户主机已经崩溃，并且关闭或者正在重新启动。在任何一种情况下，客户的TCP都没有响应。服务端将不能收到对探测的响应，并在75秒后超时。服务器总共发送10个这样的探测 ，每个间隔75秒。如果服务器没有收到一个响应，它就认为客户主机已经关闭并终止连接。 
客户主机崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 
客户机正常运行，但是服务器不可达，这种情况与2类似，TCP能发现的就是没有收到探查的响应。 
从上面可以看出，TCP保活功能主要为探测长连接的存活状况，不过这里存在一个问题，存活功能的探测周期太长，还有就是它只是探测TCP连接的存活，属于比较斯文的做法，遇到恶意的连接时，保活功能就不够使了。

在长连接的应用场景下，client端一般不会主动关闭它们之间的连接，Client与server之间的连接如果一直不关闭的话，会存在一个问 题，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可 以避免一些恶意连接导致server端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的 客户端连累后端服务。

长连接和短连接的产生在于client和server采取的关闭策略，具体的应用场景采用具体的策略，没有十全十美的选择，只有合适的选择。

**什么是“长连接”和“短连接”？**

解释1

所谓长连接指建立SOCKET连接后不管是否使用都保持连接，但安全性较差；

所谓短连接指建立SOCKET连接后发送后接收完数据后马上断开连接，一般银行都使用短连接

解释2

长连接就是指在基于tcp的通讯中，一直保持连接，不管当前是否发送或者接收数据。

而短连接就是只有在有数据传输的时候才进行连接，客户－服务器通信/传输数据完毕就关闭连接。

解释3

长连接和短连接这个概念好像只有移动的CMPP协议中提到了，其他的地方没有看到过。 
通信方式 
各网元之间共有两种连接方式：长连接和短连接。所谓长连接，指在一个TCP连接上可以连续发送多个数据包，在TCP连接保持期间，如果没有数据包发送，需要双方发检测包以维持此连接。短连接是指通信双方有数据交互时，就建立一个TCP连接，数据发送完成后，则断开此TCP连接，即每次TCP连接只完成一对 CMPP消息的发送。 
现阶段，要求ISMG之间必须采用长连接的通信方式，建议SP与ISMG之间采用长连接的通信方式。

解释4

短连接：比如http的，只是连接、请求、关闭，过程时间较短,服务器若是一段时间内没有收到请求即可关闭连接。 
长连接：有些服务需要长时间连接到服务器，比如CMPP，一般需要自己做在线维持。

**HTTP协议之长、短连接**

一、长连接与短连接：

长连接：client方与server方先建立连接，连接建立后不断开，然后再进行报文发送和接收。 
这种方式下由于通讯连接一直存在。此种方式常用于P2P通信。 
短连接：Client方与server每进行一次报文收发交易时才进行通讯连接，交易完毕后立即断开连接。 
此方式常用于一点对多点通讯。C/S通信。 
二、长连接与短连接的操作过程：

短连接的操作步骤是： 
建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接 
长连接的操作步骤是： 
建立连接——数据传输…（保持连接）…数据传输——关闭连接 
三、长连接与短连接的使用时机：

长连接：长连接多用于操作频繁，点对点的通讯，而且连接数不能太多的情况。 
每个TCP连接的建立都需要三次握手，每个TCP连接的断开要四次握手。 
如果每次操作都要建立连接然后再操作的话处理速度会降低，所以每次操作后，下次操作时直接发送数据就可以了，不用再建立TCP连接。例如：数据库的连接用长连接，如果用短连接频繁的通信会造成socket错误，频繁的socket创建也是对资源的浪费。 
短连接：web网站的http服务一般都用短连接。因为长连接对于服务器来说要耗费一定的资源。像web网站这么频繁的成千上万甚至上亿客户端的连接用短连接更省一些资源。试想如果都用长连接，而且同时用成千上万的用户，每个用户都占有一个连接的话，可想而知服务器的压力有多大。所以并发量大，但是每个用户又不需频繁操作的情况下需要短连接。总之：长连接和短连接的选择要根据需求而定。 
四、发送接收方式：

1、异步：报文发送和接收是分开的，相互独立，互不影响的。这种方式又分两种情况： 
异步双工：接收和发送在同一个程序中，有两个不同的子进程分别负责发送和接送。 
异步单工：接送和发送使用两个不同的程序来完成。 
2、同步：报文发送和接收是同步进行，即报文发送后等待接送返回报文。同步方式一般需要考虑超时问题，试想我们发送报文以后也不能无限等待啊，所以我们要设定一个等待 
时候。超过等待时间发送方不再等待读返回报文。直接通知超时返回。 
五、报文格式：

通信报文格式多样性更多，相应地就必须设计对应的读写报文的接收和发送报文函数。 
阻塞与非阻塞方式

1、非阻塞方式：读函数不停的进行读动作，如果没有报文接收到，等待一段时间后超时返回，这种情况一般需要指定超时时间。 
2、阻塞方式：如果没有接收到报文，则读函数一直处于等待状态，知道报文到达。 
循环读写方式

1、一次直接读写报文：在一次接收或发送报文动作中一次性不加分别地全部读取或全部发送报文字节。 
2、不指定长度循环读写：这一版发生在短连接进程中，受网络路由等限制，一次较长的报文可能在网络传输过程中被分解成很多个包，一次读取可能不能全部读完一次报文，这就需要循环读取报文，直到读完为止。 
3、带长度报文头循环读写：这种情况一般在长连接中，由于在长连接中没有条件能够判断循环读写什么时候结束。必须要加长度报文头。读函数先是读取报文头的长度，再根据这个长度去读报文，实际情况中，报头码制格式还经常不一样，如果是非ASCII的报文头，还必须转换成ASCII常见的报文头编制有： 
1、n个字节的ASCII码。 
2、n个字节的BCD码。 
3、n个字节的网络整型码。 
以上是几种比较典型的读写报文方式，可以与通信方式模板一起预先提供一些典型的API读写函数。当然在实际问题中，可能还必须编写与对方报文格式配套的读写API. 在实际情况中，往往需要把我们自己的系统与别人的系统进行连接， 有了以上模板与API,可以说连接任何方式的通信程序都不存在问题。

**什么时候用长连接，短连接？**

长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。

而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的 连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频 繁操作情况下需用短连好。 
总之，长连接和短连接的选择要视情况而定。

原文地址：https://zhuanlan.zhihu.com/p/165977927

作者：linux

# 【NO.352】Linux系统编程之进程间通信：共享内存

共享内存是进程间通信中最简单的方式之一。共享内存允许两个或更多进程访问同一块内存，就如同 malloc() 函数向不同进程返回了指向同一个物理内存区域的指针。当一个进程改变了这块地址中的内容的时候，其它进程都会察觉到这个更改。



![img](https://pic3.zhimg.com/80/v2-a280de08b184b5fd2a5c35c3330deee2_720w.webp)

共享内存的特点：

1）共享内存是进程间共享数据的一种最快的方法。

一个进程向共享的内存区域写入了数据，共享这个内存区域的所有进程就可以立刻看到其中的内容。

2）使用共享内存要注意的是多个进程之间对一个给定存储区访问的互斥。

若一个进程正在向共享内存区写数据，则在它做完这一步操作前，别的进程不应当去读、写这些数据。

常用函数

1）创建共享内存

所需头文件：

```text
#include <sys/ipc.h>
#include <sys/shm.h>
```

int shmget(key_t key, size_t size,int shmflg);

功能：

创建或打开一块共享内存区。

参数：

key：进程间通信键值，ftok() 的返回值。

size：该共享存储段的长度(字节)。

shmflg：标识函数的行为及共享内存的权限，其取值如下：

IPC_CREAT：如果不存在就创建

IPC_EXCL： 如果已经存在则返回失败

位或权限位：共享内存位或权限位后可以设置共享内存的访问权限，格式和 open() 函数的 mode_t 一样（open() 的使用请点此链接），但可执行权限未使用。

返回值：

成功：共享内存标识符。

失败：-1。

示例代码如下：

```text
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/ipc.h>
#include <sys/shm.h>
 
#define BUFSZ 1024
 
int main(int argc, char *argv[])
{
	int shmid;
	key_t key;
	
	key = ftok("./", 2015); 
	if(key == -1)
	{
		perror("ftok");
	}
	
	//创建共享内存
	shmid = shmget(key, BUFSZ, IPC_CREAT|0666);	
	if(shmid < 0) 
	{ 
		perror("shmget"); 
		exit(-1); 
	} 
 
	return 0;
}
```

运行结果如下：

![img](https://pic2.zhimg.com/80/v2-8108c55cd015c8282af81fc37f467249_720w.webp)



2）共享内存映射

所需头文件：

```text
#include <sys/types.h>
#include <sys/shm.h>
```

void *shmat(int shmid, const void *shmaddr, int shmflg);

功能：

将一个共享内存段映射到调用进程的数据段中。简单来理解，让进程和共享内存建立一种联系，让进程某个指针指向此共享内存。

参数：

shmid：共享内存标识符，shmget() 的返回值。

shmaddr：共享内存映射地址(若为 NULL 则由系统自动指定)，推荐使用 NULL。

shmflg：共享内存段的访问权限和映射条件( 通常为 0 )，具体取值如下：

0：共享内存具有可读可写权限。

SHM_RDONLY：只读。

SHM_RND：（shmaddr 非空时才有效）

返回值：

成功：共享内存段映射地址( 相当于这个指针就指向此共享内存 )

失败：-1

3）解除共享内存映射

所需头文件：

```text
#include <sys/types.h>
#include <sys/shm.h>
```

int shmdt(const void *shmaddr);

功能：

将共享内存和当前进程分离( 仅仅是断开联系并不删除共享内存，相当于让之前的指向此共享内存的指针，不再指向)。

参数：

shmaddr：共享内存映射地址。

返回值：

成功：0

失败：-1

4）共享内存控制

所需的头文件：

\#include <sys/ipc.h>

\#include <sys/shm.h>

int shmctl(int shmid, int cmd, struct shmid_ds *buf);

功能：

共享内存属性的控制。

参数：

shmid：共享内存标识符。

cmd：函数功能的控制，其取值如下：

IPC_RMID：删除。(常用 )

IPC_SET：设置 shmid_ds 参数，相当于把共享内存原来的属性值替换为 buf 里的属性值。

IPC_STAT：保存 shmid_ds 参数，把共享内存原来的属性值备份到 buf 里。

SHM_LOCK：锁定共享内存段( 超级用户 )。

SHM_UNLOCK：解锁共享内存段。

SHM_LOCK 用于锁定内存，禁止内存交换。并不代表共享内存被锁定后禁止其它进程访问。其真正的意义是：被锁定的内存不允许被交换到虚拟内存中。这样做的优势在于让共享内存一直处于内存中，从而提高程序性能。

buf：shmid_ds 数据类型的地址(具体类型请点此链接 )，用来存放或修改共享内存的属性。

返回值：

成功：0

失败：-1

实战示例

接下来我们做这么一个例子：创建两个进程，在 A 进程中创建一个共享内存，并向其写入数据，通过 B 进程从共享内存中读取数据。

写端代码如下：

```text
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/ipc.h>
#include <sys/shm.h>
 
#define BUFSZ 512
 
int main(int argc, char *argv[])
{
	int shmid;
	int ret;
	key_t key;
	char *shmadd;
	
	//创建key值
	key = ftok("../", 2015); 
	if(key == -1)
	{
		perror("ftok");
	}
	
	//创建共享内存
	shmid = shmget(key, BUFSZ, IPC_CREAT|0666);	
	if(shmid < 0) 
	{ 
		perror("shmget"); 
		exit(-1); 
	}
	
	//映射
	shmadd = shmat(shmid, NULL, 0);
	if(shmadd < 0)
	{
		perror("shmat");
		_exit(-1);
	}
	
	//拷贝数据至共享内存区
	printf("copy data to shared-memory\n");
	bzero(shmadd, BUFSZ); // 共享内存清空
	strcpy(shmadd, "how are you, mike\n");
	
	return 0;
}
```

读端代码如下：

```text
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/ipc.h>
#include <sys/shm.h>
 
#define BUFSZ 512
 
int main(int argc, char *argv[])
{
	int shmid;
	int ret;
	key_t key;
	char *shmadd;
	
	//创建key值
	key = ftok("../", 2015); 
	if(key == -1)
	{
		perror("ftok");
	}
	
	system("ipcs -m"); //查看共享内存
	
	//打开共享内存
	shmid = shmget(key, BUFSZ, IPC_CREAT|0666);
	if(shmid < 0) 
	{ 
		perror("shmget"); 
		exit(-1); 
	} 
	
	//映射
	shmadd = shmat(shmid, NULL, 0);
	if(shmadd < 0)
	{
		perror("shmat");
		exit(-1);
	}
	
	//读共享内存区数据
	printf("data = [%s]\n", shmadd);
	
	//分离共享内存和当前进程
	ret = shmdt(shmadd);
	if(ret < 0)
	{
		perror("shmdt");
		exit(1);
	}
	else
	{
		printf("deleted shared-memory\n");
	}
	
	//删除共享内存
	shmctl(shmid, IPC_RMID, NULL);
	
	system("ipcs -m"); //查看共享内存
	
	return 0;
}
```

运行结果如下：

![img](https://pic3.zhimg.com/80/v2-ea961b44c86d0b293bee95b83d730d06_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/147826545

作者：linux

# 【NO.353】内存泄漏的原因，内存泄漏如何避免？内存泄漏如何定位？

## 1. 内存溢出

内存溢出 OOM （out of memory），是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个int,但给它存了long才能存下的数，那就是内存溢出。

## 2. 内存泄漏

内存泄露 memory leak，是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存,迟早会被占光。最终的结果就是导致OOM。

内存泄漏是指你向系统申请分配内存进行使用(new)，可是使用完了以后却不归还(delete)，结果你申请到的那块内存你自己也不能再访问（也许你把它的地址给弄丢了），而系统也不能再次将它分配给需要的程序。

## 3. 造成内存泄露常见的三种情况

1，指针重新赋值

2，错误的内存释放

3，返回值的不正确处理

**3.1 指针重新赋值**

如下代码：

```text
char * p = (char *)malloc(10);
char * np = (char *)malloc(10);
```

其中，指针变量 p 和 np 分别被分配了 10 个字节的内存。

如果程序需要执行如下赋值语句：

```text
p=np;
```

这时候，指针变量 p 被 np 指针重新赋值，其结果是 p 以前所指向的内存位置变成了孤立的内存。它无法释放，因为没有指向该位置的引用，从而导致 10 字节的内存泄漏。

因此，在对指针赋值前，一定确保内存位置不会变为孤立的。

类似的情况，连续重复new的情况也是类似：

```text
 int *p = new int; 
 p = new int...;//错误
```

**3.2 错误的内存释放**

假设有一个指针变量 p，它指向一个 10 字节的内存位置。该内存位置的第三个字节又指向某个动态分配的 10 字节的内存位置。

如果程序需要执行如下赋值语句时：

```text
free(p);
```

很显然，如果通过调用 free 来释放指针 p，则 np 指针也会因此而变得无效。np 以前所指向的内存位置也无法释放，因为已经没有指向该位置的指针。换句话说，np 所指向的内存位置变为孤立的，从而导致内存泄漏。

因此，每当释放结构化的元素，而该元素又包含指向动态分配的内存位置的指针时，应首先遍历子内存位置（如本示例中的 np），并从那里开始释放，然后再遍历回父节点，如下面的代码所示：

```text
free(p->np);
free(p);
```

**3.3 返回值的不正确处理**

有时候，某些函数会返回对动态分配的内存的引用，如下面的示例代码所示：

```text
char *f(){
	return (char *)malloc(10);
}
void f1(){
	f();
}
```

函数 f1 中对 f 函数的调用并未处理该内存位置的返回地址，其结果将导致 f 函数所分配的 10 个字节的块丢失，并导致内存泄漏。

**4 在内存分配后忘记使用 free 进行释放**

## 4. 如何避免内存泄露？

- 确保没有在访问空指针。
- 每个内存分配函数都应该有一个 free 函数与之对应，alloca 函数除外。
- 每次分配内存之后都应该及时进行初始化，可以结合 memset 函数进行初始化，calloc 函数除外。
- 每当向指针写入值时，都要确保对可用字节数和所写入的字节数进行交叉核对。
- 在对指针赋值前，一定要确保没有内存位置会变为孤立的。
- 每当释放结构化的元素（而该元素又包含指向动态分配的内存位置的指针）时，都应先遍历子内存位置并从那里开始释放，然后再遍历回父节点。
- 始终正确处理返回动态分配的内存引用的函数返回值。

## 5.定位内存泄漏（valgrind）（重点）

**5.1、基本概念**

Valgrind是一个GPL的软件，用于Linux（For x86, amd64 and ppc32）程序的内存调试和代码剖析。你可以在它的环境中运行你的程序来监视内存的使用情况，比如C 语言中的malloc和free或者 C++中的new和 delete。使用Valgrind的工具包，你可以自动的检测许多内存管理和线程的bug，避免花费太多的时间在bug寻找上，使得你的程序更加稳固。

安装Valgrind

```text
//valgrind下载：
http://valgrind.org/downloads/valgrind-3.12.0.tar.bz2

valgrind安装：
1. tar -jxvf valgrind-3.12.0.tar.bz2
2. cd valgrind-3.12.0
3. ./configure
4. make
5. sudo make install
```

应用环境：Linux

编程语言：C/C++

使用方法： 编译时加上-g选项，如 gcc -g filename.c -o filename,使用如下命令检测内存使用情况：

```text
最常用的命令格式：
valgrind --tool=memcheck --leak-check=full ./test

valgrind --tool=memcheck --leak-check=full --show-reachable=yes --trace-children=yes  ./filename

其中--leak-check=full指的是完全检查内存泄漏，--show-reachable=yes是显示内存泄漏的地点，--trace-children=yes是跟入子进程。
```

如果您的程序是会正常退出的程序，那么当程序退出的时候valgrind自然会输出内存泄漏的信息。如果您的程序是个守护进程，那么也不要紧，我们 只要在别的终端下杀死memcheck进程（因为valgrind默认使用memcheck工具，就是默认参数--tools=memcheck）

参数选择

```text
 -tool=<name> 最常用的选项。运行 valgrind中名为toolname的工具。默认memcheck。
        memcheck ------> 这是valgrind应用最广泛的工具，一个重量级的内存检查器，能够发现开发中绝大多数内存错误使用情况，比如：使用未初始化的内存，使用已经释放了的内存，内存访问越界等。
        callgrind ------> 它主要用来检查程序中函数调用过程中出现的问题。
        cachegrind ------> 它主要用来检查程序中缓存使用出现的问题。
        helgrind ------> 它主要用来检查多线程程序中出现的竞争问题。
        massif ------> 它主要用来检查程序中堆栈使用中出现的问题。
        extension ------> 可以利用core提供的功能，自己编写特定的内存调试工具
    -h –help 显示帮助信息。
    -version 显示valgrind内核的版本，每个工具都有各自的版本。
    -q –quiet 安静地运行，只打印错误信息。
    -v –verbose 更详细的信息, 增加错误数统计。
    -trace-children=no|yes 跟踪子线程? [default: no]
    -track-fds=no|yes 跟踪打开的文件描述？[default: no]
    -time-stamp=no|yes 增加时间戳到LOG信息? [default: no]
    -log-fd=<number> 输出LOG到描述符文件 [2=stderr]
    -log-file=<file> 将输出的信息写入到filename.PID的文件里，PID是运行程序的进行ID
    -log-file-exactly=<file> 输出LOG信息到 file
    -log-file-qualifier=<VAR> 取得环境变量的值来做为输出信息的文件名。 [none]
    -log-socket=ipaddr:port 输出LOG到socket ，ipaddr:port

LOG信息输出

    -xml=yes 将信息以xml格式输出，只有memcheck可用
    -num-callers=<number> show <number> callers in stack traces [12]
    -error-limit=no|yes 如果太多错误，则停止显示新错误? [yes]
    -error-exitcode=<number> 如果发现错误则返回错误代码 [0=disable]
    -db-attach=no|yes 当出现错误，valgrind会自动启动调试器gdb。[no]
    -db-command=<command> 启动调试器的命令行选项[gdb -nw %f %p]
```

设计思路：根据软件的内存操作维护一个有效地址空间表和无效地址空间表（进程的地址空间）

**5.2、多个工具**

1、Memcheck

最常用的工具，用来检测程序中出现的内存问题，所有对内存的读写都会被检测到，一切对malloc()/free()/new/delete的调用都会被捕获。所以，Memcheck 工具主要检查下面的程序错误

能够检测：

- 使用未初始化的内存 (Use of uninitialised memory)
- 使用已经释放了的内存 (Reading/writing memory after it has been free’d)
- 使用超过 malloc分配的内存空间(Reading/writing off the end of malloc’d blocks)
- 对堆栈的非法访问 (Reading/writing inappropriate areas on the stack)
- 申请的空间是否有释放 (Memory leaks – where pointers to malloc’d blocks are lost forever)
- malloc/free/new/delete申请和释放内存的匹配(Mismatched use of malloc/new/new [] vs free/delete/delete [])
- src和dst的重叠(Overlapping src and dst pointers in memcpy() and related functions)
- 重复free

![img](https://pic3.zhimg.com/80/v2-b698870a15e09c53a8a023c668e8d38a_720w.webp)

Callgrind

和gprof类似的分析工具，但它对程序的运行观察更是入微，能给我们提供更多的信息。和gprof不同，它不需要在编译源代码时附加特殊选项，但加上调试选项是推荐的。Callgrind收集程序运行时的一些数据，建立函数调用关系图，还可以有选择地进行cache模拟。在运行结束时，它会把分析数据写入一个文件。callgrind_annotate可以把这个文件的内容转化成可读的形式。

Cachegrind

Cache分析器，它模拟CPU中的一级缓存I1，Dl和二级缓存，能够精确地指出程序中cache的丢失和命中。如果需要，它还能够为我们提供cache丢失次数，内存引用次数，以及每行代码，每个函数，每个模块，整个程序产生的指令数。这对优化程序有很大的帮助。

Helgrind

它主要用来检查多线程程序中出现的竞争问题。Helgrind寻找内存中被多个线程访问，而又没有一贯加锁的区域，这些区域往往是线程之间失去同步的地方，而且会导致难以发掘的错误。Helgrind实现了名为“Eraser”的竞争检测算法，并做了进一步改进，减少了报告错误的次数。不过，Helgrind仍然处于实验阶段。

Massif

堆栈分析器，它能测量程序在堆栈中使用了多少内存，告诉我们堆块，堆管理块和栈的大小。Massif能帮助我们减少内存的使用，在带有虚拟内存的现代系统中，它还能够加速我们程序的运行，减少程序停留在交换区中的几率。

**5.3、使用原理**

![img](https://pic3.zhimg.com/80/v2-7d0f81e4c1a96f95d1b9a6a69763dd7a_720w.webp)

![img](https://pic2.zhimg.com/80/v2-abb0316ea56fd7260878196540f5e559_720w.webp)

![img](https://pic1.zhimg.com/80/v2-210cf3c1a5ad0f62f74bc3a95217f8a8_720w.webp)

Memcheck 能够检测出内存问题，关键在于其建立了两个全局表。

1、Valid-Value 表：

对于进程的整个地址空间中的每一个字节(byte)，都有与之对应的 8 个 bits；对于 CPU 的每个寄存器，也有一个与之对应的 bit 向量。这些 bits 负责记录该字节或者寄存器值是否具有有效的、已初始化的值。

2、Valid-Address 表

对于进程整个地址空间中的每一个字节(byte)，还有与之对应的 1 个 bit，负责记录该地址是否能够被读写。

检测原理：

- 当要读写内存中某个字节时，首先检查这个字节对应的 A bit。如果该A bit显示该位置是无效位置，memcheck 则报告读写错误。
- 内核（core）类似于一个虚拟的 CPU 环境，这样当内存中的某个字节被加载到真实的 CPU 中时，该字节对应的 V bit也被加载到虚拟的 CPU 环境中。一旦寄存器中的值，被用来产生内存地址，或者该值能够影响程序输出，则 memcheck 会检查对应的V bits，如果该值尚未初始化，则会报告使用未初始化内存错误。

**5.4、具体使用**

\1. 使用未初始化的内存（使用野指针）

这里我们定义了一个指针p，但并未给他开辟空间，即他是一个野指针，但我们却使用它了

![img](https://pic1.zhimg.com/80/v2-b042352183808f63bc5e94f347dc1078_720w.webp)

Valgrind检测出我们程序使用了未初始化的变量，但并未检测出内存泄漏。

![img](https://pic3.zhimg.com/80/v2-06dc0e4201d006c17f789e41eac15452_720w.webp)

2.在内存被释放后进行读/写（使用野指针）

p所指向的内存被释放了，p变成了野指针，但是我们却继续使用这片内存。

![img](https://pic3.zhimg.com/80/v2-acd367b68bf765932b3c20589b4e2932_720w.webp)

Valgrind检测出我们使用了已经free掉的内存，并给出这片内存是哪里分配哪里释放的。

![img](https://pic2.zhimg.com/80/v2-5e889c52245362966222a1456586821d_720w.webp)

3.从已分配内存块的尾部进行读/写（动态内存越界）

我们动态地分配了一段数组，但我们在访问个数组时发生了越界读写，程序crash掉。

![img](https://pic2.zhimg.com/80/v2-03928d457a18abb5bfe973db54d85121_720w.webp)

Valgrind检测出越界的位置。

![img](https://pic3.zhimg.com/80/v2-8ac0a30fd7438532de6a2afc1212cef6_720w.webp)

注意：Valgrind不检查静态分配数组的使用情况！所以对静态分配的数组，Valgrind表示无能为力！比如下面的例子，程序crash掉，我们却不知道为什么。

![img](https://pic1.zhimg.com/80/v2-b26b13508ce182b2e59d471b2c0b4b58_720w.webp)

![img](https://pic2.zhimg.com/80/v2-2d3c16429c39cc063777e58db9b4fc35_720w.webp)

4.内存泄漏

内存泄漏的原因在于没有成对地使用malloc/free和new/delete，比如下面的例子。

![img](https://pic2.zhimg.com/80/v2-de361db64325f9c93d61c2bbd5450741_720w.webp)

Valgrind会给出程序中malloc和free的出现次数以判断是否发生内存泄漏，比如对上面的程序运行memcheck，Valgrind的记录显示上面的程序用了1次malloc，却调用了0次free，明显发生了内存泄漏！

![img](https://pic4.zhimg.com/80/v2-9d32d6c4faf6ebb0943e11d33b3baed3_720w.webp)

上面提示了我们可以使用–leak-check=full进一步获取内存泄漏的信息，比如malloc和free的具体行号。

![img](https://pic3.zhimg.com/80/v2-01d09eaf22538c4af7b091c6646f9d0a_720w.webp)

\5. 不匹配地使用malloc/new/new[] 和 free/delete/delete[]

正常使用new/delete和malloc/free是这样子的：

![img](https://pic4.zhimg.com/80/v2-b411a6ffbd053018a2ec34015ce9bb23_720w.webp)

![img](https://pic3.zhimg.com/80/v2-d35526f6668e7b0498d89c756403ecb6_720w.webp)

而不匹配地使用malloc/new/new[] 和 free/delete/delete[]则会被提示mismacth：

![img](https://pic2.zhimg.com/80/v2-0aedd0d926b7c7a35de1493975c38601_720w.webp)

![img](https://pic2.zhimg.com/80/v2-bf9c45b31ba38a218916c43cbc73aeb9_720w.webp)

6.两次释放内存

double free的情况同样是根据malloc/free的匹配对数来体现的，比如free多了一次，Valgrind也会提示。

![img](https://pic4.zhimg.com/80/v2-0baa71eabaa30d0552b4730a0d1d0e6f_720w.webp)

![img](https://pic1.zhimg.com/80/v2-f2cf40cc91eb282f972c2bce6e9777a4_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/458541056

作者：linux

# 【NO.354】线上大量CLOSE_WAIT的原因深入分析

这一次重启真的无法解决问题了：一次 **MySQL** 主动关闭，导致服务出现大量 **CLOSE_WAIT** 的全流程排查过程。

近日遇到一个线上服务 **socket** 资源被不断打满的情况。通过各种工具分析线上问题,定位到问题代码。这里对该问题发现、修复过程进行一下复盘总结。

先看两张图。一张图是服务正常时监控到的 **socket** 状态，另一张当然就是异常啦！

![img](https://pic2.zhimg.com/80/v2-8f3018ad4fe06866dca6ba4e0a4d75cd_720w.webp)

**图一：正常时监控**

![img](https://pic2.zhimg.com/80/v2-8fb3e0f1f315cf82c45a8c4d68166c75_720w.webp)

**图二：异常时监控**

从图中的表现情况来看，就是从 **04:00** 开始，socket 资源不断上涨，每个谷底时重启后恢复到正常值，然后继续不断上涨不释放，而且每次达到峰值的间隔时间越来越短。

重启后，排查了日志，没有看到 **panic** ，此时也就没有进一步检查，真的以为重启大法好。

## 1.情况说明

该服务已经上线正常运行将近一年，提供给其它服务调用，主要底层资源有DB/Redis/MQ。

为了后续说明的方便，将服务的架构图进行一下说明。

![img](https://pic2.zhimg.com/80/v2-682793da0ae7bbd23d9b5e0c565e4a55_720w.webp)

**图三：服务架构**

架构是非常简单。
问题出现在早上 **08:20** 左右开始的，报警收到该服务出现 **504**，此时第一反应是该服务长时间没有重启（快两个月了），可能存在一些内存泄漏，没有多想直接进行了重启。也就是在图二第一个谷底的时候，经过重启服务恢复到正常水平（重启真好用，开心）。

将近 **14:00** 的时候，再次被告警出现了 **504** ，当时心中略感不对劲，但由于当天恰好有一场大型促销活动，因此先立马再次重启服务。直到后续大概过了1小时后又开始告警，连续几次重启后，发现需要重启的时间间隔越来越短。此时发现问题绝不简单。**这一次重启真的解决不了问题老**，因此立马申请机器权限、开始排查问题。下面的截图全部来源我的重现demo，与线上无关。

## 2.发现问题

出现问题后，首先要进行分析推断、然后验证、最后定位修改。根据当时的表现是分别进行了以下猜想。

**推断一**

> socket 资源被不断打满，并且之前从未出现过，今日突然出现，**怀疑是不是请求量太大压垮服务**

经过查看实时 **qps** 后，放弃该想法，虽然量有增加，但依然在服务器承受范围（远远未达到压测的基准值）。

**推断二**

> 两台机器故障是同时发生，重启一台，另外一台也会得到缓解，作为独立部署在两个集群的服务非常诡异

有了上面的的依据，推出的结果是肯定是该服务依赖的底层资源除了问题，要不然不可能独立集群的服务同时出问题。

由于监控显示是 **socket** 问题，因此通过 **netstat** 命令查看了当前tcp链接的情况（本地测试，线上实际值大的多）

```text
/go/src/hello # netstat -na | awk '/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}'
LISTEN 2 
CLOSE_WAIT 23 # 非常异常 
TIME_WAIT 1
```

发现绝大部份的链接处于**CLOSE_WAIT**状态，这是非常不可思议情况。然后用`netstat -an`命令进行了检查。

![img](https://pic4.zhimg.com/80/v2-1f548421104f9df1fc5e378b11501973_720w.webp)

**图四：大量的CLOSE_WAIT**

> CLOSED 表示socket连接没被使用。 LISTENING 表示正在监听进入的连接。 SYN_SENT 表示正在试着建立连接。 SYN_RECEIVED 进行连接初始同步。 ESTABLISHED 表示连接已被建立。 CLOSE_WAIT 表示远程计算器关闭连接，正在等待socket连接的关闭。 
> FIN_WAIT_1 表示socket连接关闭，正在关闭连接。 CLOSING 先关闭本地socket连接，然后关闭远程socket连接，最后等待确认信息。 LAST_ACK 远程计算器关闭后，等待确认信号。 FIN_WAIT_2 socket连接关闭后，等待来自远程计算器的关闭信号。 TIME_WAIT 连接关闭后，等待远程计算器关闭重发。

然后开始重点思考为什么会出现大量的mysql连接是**CLOSE_WAIT**呢？为了说清楚，我们来插播一点TCP的四次挥手知识。

**TCP四次挥手**

我们来看看**TCP**的四次挥手是怎么样的流程：

![img](https://pic2.zhimg.com/80/v2-a793ed2e4cb23af8c34d4394732c995d_720w.webp)

用中文来描述下这个过程：

Client: `服务端大哥，我事情都干完了，准备撤了`，这里对应的就是客户端发了一个**FIN**

Server：`知道了，但是你等等我，我还要收收尾`，这里对应的就是服务端收到 **FIN** 后回应的 **ACK**

经过上面两步之后，服务端就会处于 **CLOSE_WAIT** 状态。过了一段时间 **Server** 收尾完了

Server：`小弟，哥哥我做完了，撤吧`，服务端发送了**FIN**

Client：`大哥，再见啊`，这里是客户端对服务端的一个 **ACK**

到此服务端就可以跑路了，但是客户端还不行。为什么呢？客户端还必须等待 **2MSL** 个时间，这里为什么客户端还不能直接跑路呢？主要是为了防止发送出去的 **ACK** 服务端没有收到，服务端重发 **FIN** 再次来询问，如果客户端发完就跑路了，那么服务端重发的时候就没人理他了。这个等待的时间长度也很讲究。

> **Maximum Segment Lifetime**报文最大生存时间，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃

这里一定不要被图里的 **client／server** 和项目里的客户端服务器端混淆，你只要记住：主动关闭的一方发出 **FIN** 包（Client），被动关闭（Server）的一方响应 **ACK** 包，此时，被动关闭的一方就进入了 **CLOSE_WAIT** 状态。如果一切正常，稍后被动关闭的一方也会发出 **FIN** 包，然后迁移到 **LAST_ACK** 状态。

既然是这样，**TCP**抓包分析下：

```text
/go # tcpdump -n port 3306
# 发生了 3次握手
11:38:15.679863 IP 172.18.0.5.38822 > 172.18.0.3.3306: Flags [S], seq 4065722321, win 29200, options [mss 1460,sackOK,TS val 2997352 ecr 0,nop,wscale 7], length 0

11:38:15.679923 IP 172.18.0.3.3306 > 172.18.0.5.38822: Flags [S.], seq 780487619, ack 4065722322, win 28960, options [mss 1460,sackOK,TS val 2997352 ecr 2997352,nop,wscale 7], length 0

11:38:15.679936 IP 172.18.0.5.38822 > 172.18.0.3.3306: Flags [.], ack 1, win 229, options [nop,nop,TS val 2997352 ecr 2997352], length 0

# mysql 主动断开链接
11:38:45.693382 IP 172.18.0.3.3306 > 172.18.0.5.38822: Flags [F.], seq 123, ack 144, win 227, options [nop,nop,TS val 3000355 ecr 2997359], length 0 # MySQL负载均衡器发送fin包给我

11:38:45.740958 IP 172.18.0.5.38822 > 172.18.0.3.3306: Flags [.], ack 124, win 229, options [nop,nop,TS val 3000360 ecr 3000355], length 0 # 我回复ack给它

... ... # 本来还需要我发送fin给他，但是我没有发，所以出现了close_wait。那这是什么缘故呢？
```

> **src > dst: flags data-seqno ack window urgent options**
> src > dst 表明从源地址到目的地址 flags 是TCP包中的标志信息,S 是SYN标志, F(FIN), P(PUSH) , R(RST) "."(没有标记) data-seqno 是数据包中的数据的顺序号 ack 是下次期望的顺序号 window 是接收缓存的窗口大小 urgent 表明数据包中是否有紧急指针 options 是选项

结合上面的信息，我用文字说明下：**MySQL负载均衡器** 给我的服务发送 **FIN** 包，我进行了响应，此时我进入了 **CLOSE_WAIT** 状态，但是后续作为被动关闭方的我，并没有发送 **FIN**，导致我服务端一直处于 **CLOSE_WAIT** 状态，无法最终进入 **CLOSED** 状态。

那么我推断出现这种情况可能的原因有以下几种：

**1、负载均衡器**异常退出了，

> 这基本是不可能的，他出现问题绝对是大面积的服务报警，而不仅仅是我一个服务

2、**MySQL负载均衡器**的超时设置的太短了，导致业务代码还没有处理完，**MySQL负载均衡器**就关闭tcp连接了

> 这也不太可能，因为这个服务并没有什么耗时操作，当然还是去检查了负载均衡器的配置，设置的是60s。

3、代码问题，**MySQL**连接无法释放

> 目前看起来应该是代码质量问题，加之本次数据有异常，触发到了以前某个没有测试到的点，目前看起来很有可能是这个原因

## 3.查找错误原因

由于代码的业务逻辑并不是我写的，我担心一时半会看不出来问题，所以直接使用`perf`把所有的调用关系使用火焰图给绘制出来。既然上面我们推断代码中没有释放mysql连接。无非就是：

1. 确实没有调用close
2. 有耗时操作（火焰图可以非常明显看到），导致超时了
3. mysql的事务没有正确处理，例如：rollback 或者 commit

由于火焰图包含的内容太多，为了让大家看清楚，我把一些不必要的信息进行了折叠。

![img](https://pic4.zhimg.com/80/v2-d174693ee7adbcfb27d19aa1a092b053_720w.webp)

**图五：有问题的火焰图**

火焰图很明显看到了开启了事务，但是在余下的部分，并没有看到**Commit**或者是**Rollback**操作。这肯定会操作问题。然后也清楚看到出现问题的是：

**MainController.update**方法内部，话不多说，直接到 update 方法中去检查。发现了如下代码：

```text
func (c *MainController) update() (flag bool) {
	o := orm.NewOrm()
	o.Using("default")
	
	o.Begin()
	nilMap := getMapNil()
	if nilMap == nil {// 这里只检查了是否为nil，并没有进行rollback或者commit
		return false
	}

	nilMap[10] = 1
	nilMap[20] = 2
	if nilMap == nil && len(nilMap) == 0 {
		o.Rollback()
		return false
	}

	sql := "update tb_user set name=%s where id=%d"
	res, err := o.Raw(sql, "Bug", 2).Exec()
	if err == nil {
		num, _ := res.RowsAffected()
		fmt.Println("mysql row affected nums: ", num)
		o.Commit()
		return true
	}

	o.Rollback()
	return false
}
```

至此，全部分析结束。经过查看**getMapNil**返回了nil，但是下面的判断条件没有进行回滚。

```text
if nilMap == nil {
    o.Rollback()// 这里进行回滚
    return false
}
```

## 4.总结

整个分析过程还是废了不少时间。最主要的是主观意识太强，觉得运行了一年没有出问题的为什么会突然出问题？因此一开始是质疑 SRE、DBA、各种基础设施出了问题（人总是先怀疑别人）。导致在这上面费了不少时间。

理一下正确的分析思路：

1、出现问题后，立马应该检查日志，确实日志没有发现问题；

2、监控明确显示了socket不断增长，很明确立马应该使用 `netstat` 检查情况看看是哪个进程的锅；

3、根据 `netstat` 的检查，使用 `tcpdump` 抓包分析一下为什么连接会**被动断开**（TCP知识非常重要）；

4、如果熟悉代码应该直接去检查业务代码，如果不熟悉则可以使用 `perf` 把代码的调用链路打印出来；

5、不论是分析代码还是火焰图，到此应该能够很快定位到问题。

那么本次到底是为什么会出现**CLOSE_WAIT**呢？大部分同学应该已经明白了，我这里再简单说明一下：

由于那一行代码没有对事务进行回滚，导致服务端没有主动发起close。因此 **MySQL负载均衡器**在达到 60s 的时候主动触发了close操作，但是通过tcp抓包发现，服务端并没有进行回应，这是因为代码中的事务没有处理，因此从而导致大量的端口、连接资源被占用。在贴一下挥手时的抓包数据：

```text
# mysql 主动断开链接
11:38:45.693382 IP 172.18.0.3.3306 > 172.18.0.5.38822: Flags [F.], seq 123, ack 144, win 227, options [nop,nop,TS val 3000355 ecr 2997359], length 0 # MySQL负载均衡器发送fin包给我
11:38:45.740958 IP 172.18.0.5.38822 > 172.18.0.3.3306: Flags [.], ack 124, win 229, options [nop,nop,TS val 3000360 ecr 3000355], length 0 # 我回复ack给它
```

希望此文对大家排查线上问题有所帮助。为了便于帮助大家理解，下面附上正确情况下的火焰图与错误情况下的火焰图。大家可以自行对比。

![img](https://pic4.zhimg.com/80/v2-1b24d965d1ed017dc0b167d920ae7267_720w.webp)

正确的火焰图

![img](https://pic1.zhimg.com/80/v2-d9584323cde779a43079c884260f13bc_720w.webp)

错误的火焰图

这里提出两个思考题，我觉得非常有意义，大家自己思考下：

1. 为什么一台机器几百个 **CLOSE_WAIT** 就导致不可继续访问？我们不是经常说一台机器有 **65535** 个文件描述符可用吗？
2. 为什么我有负载均衡，而两台部署服务的机器确几乎同时出了 **CLOSE_WAIT** ?

原文地址：https://zhuanlan.zhihu.com/p/377044076

作者：linux

# 【NO.355】一文弄懂tcp连接中各种状态及故障排查

## **1.TCP网络常用命令**

了解TCP之前，先了解几个命令：

linux查看tcp的状态命令：
1）、netstat -nat 查看TCP各个状态的数量
2）、lsof -i:port 可以检测到打开套接字的状况
3)、 sar -n SOCK 查看tcp创建的连接数
4)、tcpdump -iany tcp port 9000 对tcp端口为9000的进行抓包
5)、tcpdump dst port 9000 -w dump9000.pcap 对tcp目标端口为9000的进行抓包保存pcap文件wireshark分析。
6)、tcpdump tcp port 9000 -n -X -s 0 -w tcp.cap 对tcp/http目标端口为9000的进行抓包保存pcap文件wireshark分析。

网络测试常用命令;

1）ping:检测网络连接的正常与否,主要是测试延时、抖动、丢包率。

但是很多服务器为了防止攻击，一般会关闭对ping的响应。所以ping一般作为测试连通性使用。ping命令后，会接收到对方发送的回馈信息，其中记录着对方的IP地址和TTL。TTL是该字段指定IP包被路由器丢弃之前允许通过的最大网段数量。TTL是IPv4包头的一个8 bit字段。例如IP包在服务器中发送前设置的TTL是64，你使用ping命令后，得到服务器反馈的信息，其中的TTL为56，说明途中一共经过了8道路由器的转发，每经过一个路由，TTL减1。

2）traceroute：raceroute 跟踪数据包到达网络主机所经过的路由工具

traceroute hostname

3）pathping：是一个路由跟踪工具，它将 ping 和 tracert 命令的功能与这两个工具所不提供的其他信息结合起来，综合了二者的功能

pathping [http://www.baidu.com](https://link.zhihu.com/?target=http%3A//www.baidu.com)

4）mtr：以结合ping nslookup tracert 来判断网络的相关特性

\5) nslookup:用于解析域名，一般用来检测本机的DNS设置是否配置正确。

## **2.TCP建立连接三次握手相关状态**

TCP是一个面向连接的协议，所以在连接双方发送数据之前，都需要首先建立一条连接。

**Client连接Server三次握手过程：**

当Client端调用socket函数调用时，相当于Client端产生了一个处于Closed状态的套接字。
( 1) 第一次握手SYN：Client端又调用connect函数调用，系统为Client随机分配一个端口，连同传入connect中的参数(Server的IP和端口)，这就形成了一个连接四元组，客户端发送一个带SYN标志的TCP报文到服务器。这是三次握手过程中的报文1。connect调用让Client端的socket处于SYN_SENT状态，等待服务器确认；SYN：同步序列编号(Synchronize Sequence Numbers)。

( 2)第二次握手SYN+ACK： 服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器进入SYN_RECV状态；

( 3) 第三次握手ACK：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户器和客务器进入ESTABLISHED状态，完成三次握手。连接已经可以进行读写操作。

一个完整的三次握手也就是： 请求---应答---再次确认。

TCP协议通过三个报文段完成连接的建立，这个过程称为三次握手(three-way handshake)，过程如下图所示。

![img](https://pic4.zhimg.com/80/v2-5bd65e5e102cd2041a424d20f5040c57_720w.webp)

Server对应的函数接口：

当Server端调用socket函数调用时，相当于Server端产生了一个处于Closed状态的监听套接字
Server端调用bind操作，将监听套接字与指定的地址和端口关联，然后又调用listen函数，系统会为其分配未完成队列和完成队列，此时的监听套接字可以接受Client的连接，监听套接字状态处于LISTEN状态。
当Server端调用accept操作时，会从完成队列中取出一个已经完成的client连接，同时在server这段会产生一个会话套接字，用于和client端套接字的通信，这个会话套接字的状态是ESTABLISH。

从图中可以看出，当客户端调用connect时，触发了连接请求，向服务器发送了SYN J包，这时connect进入阻塞状态；服务器监听到连接请求，即收到SYN J包，调用accept函数接收请求向客户端发送SYN K ，ACK J+1，这时accept进入阻塞状态；客户端收到服务器的SYN K ，ACK J+1之后，这时connect返回，并对SYN K进行确认；服务器收到ACK K+1时，accept返回，至此三次握手完毕，连接建立。

我们可以通过wireshark抓包，可以清晰看到查看具体的流程：

第一次握手：syn的seq= 0
第二次握手：服务器收到syn包，必须确认客户的SYN（ACK=j+1=1）=，同时自己也发送一个SYN包（syn=0）

第三次握手：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1)

![img](https://pic2.zhimg.com/80/v2-e5e5bb47ad0a3f2db7ef4ddf690d4509_720w.webp)

2、建立连接的具体状态：

1)、LISTENING：侦听来自远方的TCP端口的连接请求.

首先服务端需要打开一个socket进行监听，状态为LISTEN。

有提供某种服务才会处于LISTENING状态，TCP状态变化就是某个端口的状态变化，提供一个服务就打开一个端口，例如：提供www服务默认开的是80端口，提供ftp服务默认的端口为21，当提供的服务没有被连接时就处于LISTENING状态。FTP服务启动后首先处于侦听（LISTENING）状态。处于侦听LISTENING状态时，该端口是开放的，等待连接，但还没有被连接。就像你房子的门已经敞开的，但还没有人进来。
看LISTENING状态最主要的是看本机开了哪些端口，这些端口都是哪个程序开的，关闭不必要的端口是保证安全的一个非常重要的方面，服务端口都对应一个服务（应用程序），停止该服务就关闭了该端口，例如要关闭21端口只要停止IIS服务中的FTP服务即可。关于这方面的知识请参阅其它文章。
如果你不幸中了服务端口的木马，木马也开个端口处于LISTENING状态。

2)、SYN-SENT：客户端SYN_SENT状态

再发送连接请求后等待匹配的连接请求:客户端通过应用程序调用connect进行active open.于是客户端tcp发送一个SYN以请求建立一个连接.之后状态置为SYN_SENT. /*The socket is actively attempting to establish a connection. 在发送连接请求后等待匹配的连接请求 */
当请求连接时客户端首先要发送同步信号给要访问的机器，此时状态为SYN_SENT，如果连接成功了就变为ESTABLISHED，正常情况下SYN_SENT状态非常短暂。例如要访问网站[http://www.baidu.com](https://link.zhihu.com/?target=http%3A//www.baidu.com),如果是正常连接的话，用TCPView观察IEXPLORE.EXE（IE）建立的连接会发现很快从SYN_SENT变为ESTABLISHED，表示连接成功。SYN_SENT状态快的也许看不到。
如果发现有很多SYN_SENT出现，那一般有这么几种情况，一是你要访问的网站不存在或线路不好，二是用扫描软件扫描一个网段的机器，也会出出现很多SYN_SENT，另外就是可能中了病毒了，例如中了"冲击波"，病毒发作时会扫描其它机器，这样会有很多SYN_SENT出现。


3)、SYN-RECEIVED：服务器端握手状态SYN_RCVD

再收到和发送一个连接请求后等待对方对连接请求的确认

当服务器收到客户端发送的同步信号时，将标志位ACK和SYN置1发送给客户端，此时服务器端处于SYN_RCVD状态，如果连接成功了就变为ESTABLISHED，正常情况下SYN_RCVD状态非常短暂。
如果发现有很多SYN_RCVD状态，那你的机器有可能被SYN Flood的DoS(拒绝服务攻击)攻击了。

SYN Flood的攻击原理是：

在进行三次握手时，攻击软件向被攻击的服务器发送SYN连接请求（握手的第一步），但是这个地址是伪造的，如攻击软件随机伪造了51.133.163.104、65.158.99.152等等地址。服务器在收到连接请求时将标志位ACK和SYN置1发送给客户端（握手的第二步），但是这些客户端的IP地址都是伪造的，服务器根本找不到客户机，也就是说握手的第三步不可能完成。

这种情况下服务器端一般会重试（再次发送SYN+ACK给客户端）并等待一段时间后丢弃这个未完成的连接，这段时间的长度我们称为SYN Timeout，一般来说这个时间是分钟的数量级（大约为30秒-2分钟）；一个用户出现异常导致服务器的一个线程等待1分钟并不是什么很大的问题，但如果有一个恶意的攻击者大量模拟这种情况，服务器端将为了维护一个非常大的半连接列表而消耗非常多的资源----数以万计的半连接，即使是简单的保存并遍历也会消耗非常多的CPU时间和内存，何况还要不断对这个列表中的IP进行SYN+ACK的重试。此时从正常客户的角度看来，服务器失去响应，这种情况我们称做：服务器端受到了SYN Flood攻击（SYN洪水攻击）


4)、ESTABLISHED：代表一个打开的连接。

ESTABLISHED状态是表示两台机器正在传输数据，观察这个状态最主要的就是看哪个程序正在处于ESTABLISHED状态。

服务器出现很多ESTABLISHED状态： netstat -nat |grep 9502或者使用lsof -i:9502可以检测到。

当客户端未主动close的时候就断开连接：即客户端发送的FIN丢失或未发送:
这时候若客户端断开的时候发送了FIN包，则服务端将会处于CLOSE_WAIT状态；
这时候若客户端断开的时候未发送FIN包，则服务端处还是显示ESTABLISHED状态；
结果客户端重新连接服务器。
而新连接上来的客户端（也就是刚才断掉的重新连上来了）在服务端肯定是ESTABLISHED; 如果客户端重复的上演这种情况，那么服务端将会出现大量的假的ESTABLISHED连接和CLOSE_WAIT连接。
最终结果就是新的其他客户端无法连接上来，但是利用netstat还是能看到一条连接已经建立，并显示ESTABLISHED，但始终无法进入程序代码。

## **3. TCP关闭四次握手的相关状态**

　 由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个 FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。

### **3.1 四次握手关闭连接的过程**

建立一个连接需要三次握手，而终止一个连接要经过四次握手，这是由TCP的半关闭(half-close)造成的，如图：

![img](https://pic1.zhimg.com/80/v2-fef04d9d627497cb0f072f5600c063cc_720w.webp)

调用过程和对应函数接口：

- \1) 客户端发送一个FIN: 用来关闭客户A到服务器B的数据传送,当client想要关闭它与server之间的连接。client（某个应用进程）首先调用close主动关闭连接，这时TCP发送一个FIN M；client端处于FIN_WAIT1状态。
- \2) 服务器确认客户端FIN: 当server端接收到FIN M之后，执行被动关闭。对这个FIN进行确认，返回给client ACK。确认序号为收到的序号加1（报文段5）。和SYN一样，一个FIN将占用一个序号。当server端返回给client ACK后，client处于FIN_WAIT2状态，server处于CLOSE_WAIT状态。它的接收也作为文件结束符传递给应用进程，因为FIN的接收 意味着应用进程在相应的连接上再也接收不到额外数据；
- \3) 服务器B发送一个FIN关闭与客户端A的连接: 一段时间之后，当server端检测到client端的关闭操作(read返回为0)。接收到文件结束符的server端调用close关闭它的socket。这导致server端的TCP也发送一个FIN N；此时server的状态为LAST_ACK。
- \4) 客户端A发回ACK报文确认: 当client收到来自server的FIN后 。 client端的套接字处于TIME_WAIT状态，它会向server端再发送一个ack确认，此时server端收到ack确认后，此套接字处于CLOSED状态。

这样每个方向上都有一个FIN和ACK。

### **3.2 四次握手关闭连接的具体状态**

1）FIN-WAIT-1：

等待远程TCP连接中断请求，或先前的连接中断请求的确认

主动关闭(active close)端应用程序调用close，于是其TCP发出FIN请求主动关闭连接，之后进入FIN_WAIT1状态./* The socket is closed, and the connection is shutting down. 等待远程TCP的连接中断请求，或先前的连接中断请求的确认 */

如果服务器出现shutdown再重启，使用netstat -nat查看，就会看到很多FIN-WAIT-1的状态。就是因为服务器当前有很多客户端连接，直接关闭服务器后，无法接收到客户端的ACK。


2）FIN-WAIT-2：从远程TCP等待连接中断请求

主动关闭端接到ACK后，就进入了FIN-WAIT-2 ./* Connection is closed, and the socket is waiting for a shutdown from the remote end. 从远程TCP等待连接中断请求 */

这就是著名的半关闭的状态了，这是在关闭连接时，客户端和服务器两次握手之后的状态。在这个状态下，应用程序还有接受数据的能力，但是已经无法发送数据，但是也有一种可能是，客户端一直处于FIN_WAIT_2状态，而服务器则一直处于WAIT_CLOSE状态，而直到应用层来决定关闭这个状态。


3）CLOSE-WAIT：等待从本地用户发来的连接中断请求

被动关闭(passive close)端TCP接到FIN后，就发出ACK以回应FIN请求(它的接收也作为文件结束符传递给上层应用程序),并进入CLOSE_WAIT. /* The remote end has shut down, waiting for the socket to close. 等待从本地用户发来的连接中断请求 */


4）CLOSING：等待远程TCP对连接中断的确认

比较少见./* Both sockets are shut down but we still don't have all our data sent. 等待远程TCP对连接中断的确认 */

实际情况中应该是很少见，属于一种比较罕见的例外状态。正常情况下，当一方发送FIN报文后，按理来说是应该先收到（或同时收到）对方的ACK报文，再收到对方的FIN报文。但是CLOSING状态表示一方发送FIN报文后，并没有收到对方的ACK报文，反而却也收到了对方的FIN报文。什么情况下会出现此种情况呢？
有两种情况可能导致这种状态：
其一，如果双方几乎在同时关闭连接，那么就可能出现双方同时发送FIN包的情况；
其二，如果ACK包丢失而对方的FIN包很快发出，也会出现FIN先于ACK到达。

4）LAST-ACK：等待原来的发向远程TCP的连接中断请求的确认

被动关闭端一段时间后，接收到文件结束符的应用程序将调用CLOSE关闭连接。这导致它的TCP也发送一个 FIN,等待对方的ACK.就进入了LAST-ACK . /* The remote end has shut down, and the socket is closed. Waiting for acknowledgement. 等待原来发向远程TCP的连接中断请求的确认 */

使用并发压力测试的时候，突然断开压力测试客户端，服务器会看到很多LAST-ACK。


5）TIME-WAIT：等待足够的时间以确保远程TCP接收到连接中断请求的确认

在主动关闭端接收到FIN后，TCP就发送ACK包，并进入TIME-WAIT状态。/* The socket is waiting after close to handle packets still in the network.等待足够的时间以确保远程TCP接收到连接中断请求的确认 */

TIME_WAIT等待状态，这个状态又叫做2MSL状态，说的是在TIME_WAIT2发送了最后一个ACK数据报以后，要进入TIME_WAIT状态，这个状态是防止最后一次握手的数据报没有传送到对方那里而准备的（注意这不是四次握手，这是第四次握手的保险状态）。这个状态在很大程度上保证了双方都可以正常结束。

由于插口的2MSL状态（插口是IP和端口对的意思，socket），使得应用程序在2MSL时间内是无法再次使用同一个插口的，对于客户程序还好一些，但是对于服务程序，例如httpd，它总是要使用同一个端口来进行服务，而在2MSL时间内，启动httpd就会出现错误（插口被使用）。为了避免这个错误，服务器给出了一个平静时间的概念，这是说在2MSL时间内，虽然可以重新启动服务器，但是这个服务器还是要平静的等待2MSL时间的过去才能进行下一次连接。

6）CLOSED：没有任何连接状态

被动关闭端在接受到ACK包后，就进入了closed的状态。连接结束./* The socket is not being used. 没有任何连接状态 */

## **4.TCP状态迁移路线图**

client/server两条路线讲述TCP状态迁移路线图：

![img](https://pic4.zhimg.com/80/v2-6d56c6b3173a4c20f645190f15b6b0cf_720w.webp)

这是一个看起来比较复杂的状态迁移图，因为它包含了两个部分---服务器的状态迁移和客户端的状态迁移，如果从某一个角度出发来看这个图，就会清晰许多，这里面的服务器和客户端都不是绝对的，发送数据的就是客户端，接受数据的就是服务器。

客户端应用程序的状态迁移图

客户端的状态可以用如下的流程来表示：

CLOSED->SYN_SENT->ESTABLISHED->FIN_WAIT_1->FIN_WAIT_2->TIME_WAIT->CLOSED

以上流程是在程序正常的情况下应该有的流程，从书中的图中可以看到，在建立连接时，当客户端收到SYN报文的ACK以后，客户端就打开了数据交互地连接。而结束连接则通常是客户端主动结束的，客户端结束应用程序以后，需要经历FIN_WAIT_1，FIN_WAIT_2等状态，这些状态的迁移就是前面提到的结束连接的四次握手。

服务器的状态迁移图

服务器的状态可以用如下的流程来表示：

CLOSED->LISTEN->SYN收到->ESTABLISHED->CLOSE_WAIT->LAST_ACK->CLOSED

在建立连接的时候，服务器端是在第三次握手之后才进入数据交互状态，而关闭连接则是在关闭连接的第二次握手以后（注意不是第四次）。而关闭以后还要等待客户端给出最后的ACK包才能进入初始的状态。

其他状态迁移

还有一些其他的状态迁移，这些状态迁移针对服务器和客户端两方面的总结如下

LISTEN->SYN_SENT，对于这个解释就很简单了，服务器有时候也要打开连接的嘛。

SYN_SENT->SYN收到，服务器和客户端在SYN_SENT状态下如果收到SYN数据报，则都需要发送SYN的ACK数据报并把自己的状态调整到SYN收到状态，准备进入ESTABLISHED

SYN_SENT->CLOSED，在发送超时的情况下，会返回到CLOSED状态。

SYN_收到->LISTEN，如果受到RST包，会返回到LISTEN状态。

SYN_收到->FIN_WAIT_1，这个迁移是说，可以不用到ESTABLISHED状态，而可以直接跳转到FIN_WAIT_1状态并等待关闭。

![img](https://pic2.zhimg.com/80/v2-c3f60b5fa9c26d76c5e38a379186f9d5_720w.webp)

怎样牢牢地将这张图刻在脑中呢？那么你就一定要对这张图的每一个状态，及转换的过程有深刻的认识，不能只停留在一知半解之中。下面对这张图的11种状态详细解析一下，以便加强记忆！不过在这之前，先回顾一下TCP建立连接的三次握手过程，以及关闭连接的四次握手过程。

## **5.具体问题**

1．为什么建立连接协议是三次握手，而关闭连接却是四次握手呢？

> 这是因为服务端的LISTEN状态下的SOCKET当收到SYN报文的建连请求后，它可以把ACK和SYN（ACK起应答作用，而SYN起同步作用）放在一个报文里来发送。但关闭连接时，当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；但未必你所有的数据都全部发送给对方了，所以你可以未必会马上会关闭SOCKET,也即你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你同意现在可以关闭连接了，所以它这里的ACK报文和FIN报文多数情况下都是分开发送的。

![img](https://pic4.zhimg.com/80/v2-78914591d205d84098862bcefdfeb2ff_720w.webp)

2、什么是2MSL

> MSL是Maximum Segment Lifetime英文的缩写，中文可以译为“报文最大生存时间”，他是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为tcp报文（segment）是ip数据报（datagram）的数据部分，具体称谓请参见《数据在网络各层中的称呼》一文，而ip头中有一个TTL域，TTL是time to live的缩写，中文可以译为“生存时间”，这个生存时间是由源主机设置初始值但不是存的具体时间，而是存储了一个ip数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减1，当此值为0则数据报将被丢弃，同时发送ICMP报文通知源主机。RFC 793中规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等。
> 2MSL即两倍的MSL，TCP的TIME_WAIT状态也称为2MSL等待状态，当TCP的一端发起主动关闭，在发出最后一个ACK包后，即第3次握手完成后发送了第四次握手的ACK包后就进入了TIME_WAIT状态，必须在此状态上停留两倍的MSL时间，等待2MSL时间主要目的是怕最后一个ACK包对方没收到，那么对方在超时后将重发第三次握手的FIN包，主动关闭端接到重发的FIN包后可以再发一个ACK应答包。在TIME_WAIT状态时两端的端口不能使用，要等到2MSL时间结束才可继续使用。当连接处于2MSL等待阶段时任何迟到的报文段都将被丢弃。不过在实际应用中可以通过设置SO_REUSEADDR选项达到不必等待2MSL时间结束再使用此端口。
> 这是因为虽然双方都同意关闭连接了，而且握手的4个报文也都协调和发送完毕，按理可以直接回到CLOSED状态（就好比从SYN_SEND状态到ESTABLISH状态那样）：
> 一方面是可靠的实现TCP全双工连接的终止，也就是当最后的ACK丢失后，被动关闭端会重发FIN，因此主动关闭端需要维持状态信息，以允许它重新发送最终的ACK。
> 另一方面，但是因为我们必须要假想网络是不可靠的，你无法保证你最后发送的ACK报文会一定被对方收到，因此对方处于LAST_ACK状态下的SOCKET可能会因为超时未收到ACK报文，而重发FIN报文，所以这个TIME_WAIT状态的作用就是用来重发可能丢失的ACK报文。
> TCP在2MSL等待期间，定义这个连接(4元组)不能再使用，任何迟到的报文都会丢弃。设想如果没有2MSL的限制，恰好新到的连接正好满足原先的4元组，这时候连接就可能接收到网络上的延迟报文就可能干扰最新建立的连接。

3．为什么TIME_WAIT状态还需要等2MSL后才能返回到CLOSED状态？

> 第一，保证可靠的实现TCP全双工链接的终止：为了保证主动端A发送的最后一个ACK报文能够到达被动段B，确保被动端能进入CLOSED状态。
> 对照上图，在四次挥手协议中，当B向A发送Fin+Ack后，A就需要向B发送ACK+Seq报文，A这时候就处于TIME_WAIT 状态，但是这个报文ACK有可能会发送失败而丢失，因而使处在LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认。B会超时重传这个FIN+ACK报文段，而A就能在2MSL时间内收到这个重传的FIN+ACK报文段。如果A在TIME-WAIT状态不等待一段时间，而是在发送完ACK报文段后就立即释放连接，就无法收到B重传的FIN+ACK报文段，因而也不会再发送一次确认报文段。这样B就无法按照正常的步骤进入CLOSED状态。
>
> 第二，防止已失效的连接请求报文段出现在本连接中。A在发送完ACK报文段后，再经过2MSL时间，就可以使本连接持续的时间所产生的所有报文段都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求的报文段。
> 假设在A的XXX1端口和B的80端口之间有一个TCP连接。我们关闭这个连接，过一段时间后在相同的IP地址和端口建立另一个连接。后一个链接成为前一个的化身。因为它们的IP地址和端口号都相同。TCP必须防止来自某一个连接的老的重复分组在连 接已经终止后再现，从而被误解成属于同一链接的某一个某一个新的化身。为做到这一点，TCP将不给处于TIME_WAIT状态的链接发起新的化身。既然 TIME_WAIT状态的持续时间是MSL的2倍，这就足以让某个方向上的分组最多存活msl秒即被丢弃，另一个方向上的应答最多存活msl秒也被丢弃。 通过实施这个规则，我们就能保证每成功建立一个TCP连接时。来自该链接先前化身的重复分组都已经在网络中消逝了。

4、解决linux发现系统存在大量TIME_WAIT状态

> 如果linux发现系统存在大量TIME_WAIT状态的连接，可以通过调整内核参数解决：vi /etc/sysctl.conf 加入以下内容：
> net.ipv4.tcp_max_tw_buckets=5000 #TIME-WAIT Socket 最大数量
> \#注意：不建议开启該设置，NAT 模式下可能引起连接 RST
> net.ipv4.tcp_tw_reuse = 1 #表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；
> net.ipv4.tcp_tw_recycle = 1 #表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。
> 然后执行 /sbin/sysctl -p 让参数生效。

## **6.TCP的FLAGS说明**

在TCP层，有个FLAGS字段，这个字段有以下几个标识：SYN, FIN, ACK, PSH, RST, URG.
其中，对于我们日常的分析有用的就是前面的五个字段。
一、字段含义：
1、SYN表示建立连接：

步序列编号(Synchronize Sequence Numbers)栏有效。该标志仅在三次握手建立TCP连接时有效。它提示TCP连接的服务端检查序列编号，该序列编号为TCP连接初始端(一般是客户端)的初始序列编号。在这里，可以把TCP序列编号看作是一个范围从0到4，294，967，295的32位计数器。通过TCP连接交换的数据中每一个字节都经过序列编号。在TCP报头中的序列编号栏包括了TCP分段中第一个字节的序列编号。

2、FIN表示关闭连接：
3、ACK表示响应：

确认编号(Acknowledgement Number)栏有效。大多数情况下该标志位是置位的。TCP报头内的确认编号栏内包含的确认编号(w+1，Figure-1)为下一个预期的序列编号，同时提示远端系统已经成功接收所有数据。

4、PSH表示有DATA数据传输：


5、RST表示连接重置： 复位标志有效。用于复位相应的TCP连接。

一、字段组合含义：

![img](https://pic2.zhimg.com/80/v2-c9ed1c439eec32428d230c952987ff49_720w.webp)

其中，ACK是可能与SYN，FIN等同时使用的，比如SYN和ACK可能同时为1，它表示的就是建立连接之后的响应，
如果只是单个的一个SYN，它表示的只是建立连接。
TCP的几次握手就是通过这样的ACK表现出来的。
但SYN与FIN是不会同时为1的，因为前者表示的是建立连接，而后者表示的是断开连接。
RST一般是在FIN之后才会出现为1的情况，表示的是连接重置。
一般地，当出现FIN包或RST包时，我们便认为客户端与服务器端断开了连接；

![img](https://pic1.zhimg.com/80/v2-27c560ccbe6b9dc0596438aa97fc21b4_720w.webp)

RST与ACK标志位都置一了，并且具有ACK number，非常明显，这个报文在释放TCP连接的同时，完成了对前面已接收报文的确认。

而当出现SYN和SYN＋ACK包时，我们认为客户端与服务器建立了一个连接。
PSH为1的情况，一般只出现在 DATA内容不为0的包中，也就是说PSH为1表示的是有真正的TCP数据包内容被传递。
TCP的连接建立和连接关闭，都是通过请求－响应的模式完成的。

## **7.TCP通信中服务器处理客户端意外断开**

如果TCP连接被对方正常关闭，也就是说，对方是正确地调用了closesocket(s)或者shutdown(s)的话，那么上面的Recv或Send调用就能马上返回，并且报错。这是由于close socket(s)或者shutdown(s)有个正常的关闭过程，会告诉对方“TCP连接已经关闭，你不需要再发送或者接受消息了”。

但是，如果意外断开，客户端（3g的移动设备）并没有正常关闭socket。双方并未按照协议上的四次挥手去断开连接。

那么这时候正在执行Recv或Send操作的一方就会因为没有任何连接中断的通知而一直等待下去，也就是会被长时间卡住。

像这种如果一方已经关闭或异常终止连接，而另一方却不知道，我们将这样的TCP连接称为半打开的。

解决意外中断办法都是利用保活机制。而保活机制分又可以让底层实现也可自己实现。

1、自己编写心跳包程序

简单的说也就是在自己的程序中加入一条线程，定时向对端发送数据包，查看是否有ACK，如果有则连接正常，没有的话则连接断开

2、启动TCP编程里的keepAlive机制

一、双方拟定心跳（自实现）

一般由客户端发送心跳包，服务端并不回应心跳，只是定时轮询判断一下与上次的时间间隔是否超时（超时时间自己设定）。服务器并不主动发送是不想增添服务器的通信量，减少压力。

但这会出现三种情况：

情况1.

客户端由于某种网络延迟等原因很久后才发送心跳（它并没有断），这时服务器若利用自身设定的超时判断其已经断开，而后去关闭socket。若客户端有重连机制，则客户端会重新连接。若不确定这种方式是否关闭了原本正常的客户端，则在ShutDown的时候一定要选择send,表示关闭发送通道，服务器还可以接收一下，万一客户端正在发送比较重要的数据呢，是不？

情况2.

客户端很久没传心跳，确实是自身断掉了。在其重启之前，服务端已经判断出其超时，并主动close，则四次挥手成功交互。

情况3.

客户端很久没传心跳，确实是自身断掉了。在其重启之前，服务端的轮询还未判断出其超时，在未主动close的时候该客户端已经重新连接。

这时候若客户端断开的时候发送了FIN包，则服务端将会处于CLOSE_WAIT状态；

这时候若客户端断开的时候未发送FIN包，则服务端处还是显示ESTABLISHED状态；

而新连接上来的客户端（也就是刚才断掉的重新连上来了）在服务端肯定是ESTABLISHED;这时候就有个问题，若利用轮询还未检测出上条旧连接已经超时（这很正常，timer总有个间隔吧），而在这时，客户端又重复的上演情况3，那么服务端将会出现大量的假的ESTABLISHED连接和CLOSE_WAIT连接。

最终结果就是新的其他客户端无法连接上来，但是利用netstat还是能看到一条连接已经建立，并显示ESTABLISHED，但始终无法进入程序代码。个人最初感觉导致这种情况是因为假的ESTABLISHED连接和CLOSE_WAIT连接会占用较大的系统资源，程序无法再次创建连接（因为每次我发现这个问题的时候我只连了10个左右客户端却已经有40多条无效连接）。而最近几天测试却发现有一次程序内只连接了2，3个设备，但是有8条左右的虚连接，此时已经连接不了新客户端了。这时候我就觉得我想错了，不可能这几条连接就占用了大量连接把，如果说几十条还有可能。但是能肯定的是，这个问题的产生绝对是设备在不停的重启，而服务器这边又是简单的轮询，并不能及时处理，暂时还未能解决。

二、利用KeepAlive

其实keepalive的原理就是TCP内嵌的一个心跳包,

以服务器端为例，如果当前server端检测到超过一定时间（默认是 7,200,000 milliseconds，也就是2个小时）没有数据传输，那么会向client端发送一个keep-alive packet（该keep-alive packet就是ACK和当前TCP序列号减一的组合），此时client端应该为以下三种情况之一：

\1. client端仍然存在，网络连接状况良好。此时client端会返回一个ACK。server端接收到ACK后重置计时器（复位存活定时器），在2小时后再发送探测。如果2小时内连接上有数据传输，那么在该时间基础上向后推延2个小时。

\2. 客户端异常关闭，或是网络断开。在这两种情况下，client端都不会响应。服务器没有收到对其发出探测的响应，并且在一定时间（系统默认为1000 ms）后重复发送keep-alive packet，并且重复发送一定次数（2000 XP 2003 系统默认为5次, Vista后的系统默认为10次）。

\3. 客户端曾经崩溃，但已经重启。这种情况下，服务器将会收到对其存活探测的响应，但该响应是一个复位，从而引起服务器对连接的终止。

对于应用程序来说，2小时的空闲时间太长。因此，我们需要手工开启Keepalive功能并设置合理的Keepalive参数。

全局设置可更改/etc/sysctl.conf,加上:
net.ipv4.tcp_keepalive_intvl = 20
net.ipv4.tcp_keepalive_probes = 3
net.ipv4.tcp_keepalive_time = 60

在程序中设置如下:

```text
#include <sys/socket.h>
 #include <netinet/in.h>
 #include <arpa/inet.h>
 #include <sys/types.h>
 #include <netinet/tcp.h>
 
 int keepAlive = 1; // 开启keepalive属性
 int keepIdle = 60; // 如该连接在60秒内没有任何数据往来,则进行探测 
 int keepInterval = 5; // 探测时发包的时间间隔为5 秒
 int keepCount = 3; // 探测尝试的次数.如果第1次探测包就收到响应了,则后2次的不再发.
 
  setsockopt(rs, SOL_SOCKET, SO_KEEPALIVE, (void *)&keepAlive, sizeof(keepAlive));
  setsockopt(rs, SOL_TCP, TCP_KEEPIDLE, (void*)&keepIdle, sizeof(keepIdle));
  setsockopt(rs, SOL_TCP, TCP_KEEPINTVL, (void *)&keepInterval, sizeof(keepInterval));
  setsockopt(rs, SOL_TCP, TCP_KEEPCNT, (void *)&keepCount, sizeof(keepCount));
```

在程序中表现为,当tcp检测到对端socket不再可用时(不能发出探测包,或探测包没有收到ACK的响应包),select会返回socket可读,并且在recv时返回-1,同时置上errno为ETIMEDOUT.

## **8.Linux错误信息(errno)列表**

经常出现的错误：

22：参数错误，比如ip地址不合法，没有目标端口等

101：网络不可达，比如不能ping通

111：链接被拒绝，比如目标关闭链接等

115：当链接设置为非阻塞时，目标没有及时应答，返回此错误，socket可以继续使用。比如socket连接

附录：Linux的错误码表（errno table)

_ 124 EMEDIUMTYPE_ Wrong medium type
_ 123 ENOMEDIUM__ No medium found
_ 122 EDQUOT___ Disk quota exceeded
_ 121 EREMOTEIO__ Remote I/O error
_ 120 EISNAM___ Is a named type file
_ 119 ENAVAIL___ No XENIX semaphores available
_ 118 ENOTNAM___ Not a XENIX named type file
_ 117 EUCLEAN___ Structure needs cleaning
_ 116 ESTALE___ Stale NFS file handle
_ 115 EINPROGRESS +Operation now in progress

操作正在进行中。一个阻塞的操作正在执行。


_ 114 EALREADY__ Operation already in progress
_ 113 EHOSTUNREACH No route to host
_ 112 EHOSTDOWN__ Host is down
_ 111 ECONNREFUSED Connection refused

1、拒绝连接。一般发生在连接建立时。

拔服务器端网线测试，客户端设置keep alive时，recv较快返回0， 先收到ECONNREFUSED (Connection refused)错误码，其后都是ETIMEOUT。

2、an error returned from connect(), so it can only occur in a client (if a client is defined as the party that initiates the connection


_ 110 ETIMEDOUT_ +Connection timed out
_ 109 ETOOMANYREFS Too many references: cannot splice
_ 108 ESHUTDOWN__ Cannot send after transport endpoint shutdown
_ 107 ENOTCONN__ Transport endpoint is not connected

在一个没有建立连接的socket上，进行read，write操作会返回这个错误。出错的原因是socket没有标识地址。Setsoc也可能会出错。

还有一种情况就是收到对方发送过来的RST包,系统已经确认连接被断开了。


_ 106 EISCONN___ Transport endpoint is already connected

一般是socket客户端已经连接了，但是调用connect，会引起这个错误。


_ 105 ENOBUFS___ No buffer space available
_ 104 ECONNRESET_ Connection reset by peer

连接被远程主机关闭。有以下几种原因：远程主机停止服务，重新启动;当在执行某些操作时遇到失败，因为设置了“keep alive”选项，连接被关闭，一般与ENETRESET一起出现。

1、在客户端服务器程序中，客户端异常退出，并没有回收关闭相关的资源，服务器端会先收到ECONNRESET错误，然后收到EPIPE错误。

2、连接被远程主机关闭。有以下几种原因：远程主机停止服务，重新启动;当在执行某些操作时遇到失败，因为设置了“keep alive”选项，连接被关闭，一般与ENETRESET一起出现。

3、远程端执行了一个“hard”或者“abortive”的关闭。应用程序应该关闭socket，因为它不再可用。当执行在一个UDP socket上时，这个错误表明前一个send操作返回一个ICMP“port unreachable”信息。

4、如果client关闭连接,server端的select并不出错(不返回-1,使用select对唯一一个socket进行non- blocking检测),但是写该socket就会出错,用的是send.错误号:ECONNRESET.读(recv)socket并没有返回错误。

5、该错误被描述为“connection reset by peer”，即“对方复位连接”，这种情况一般发生在服务进程较客户进程提前终止。

主动关闭调用过程如下：

![img](https://pic2.zhimg.com/80/v2-1cabceadd31e988518e6c41a0deb2619_720w.webp)

服务器端主动关闭：

1）当服务器的服务因为某种原因，进程提前终止时会向客户 TCP 发送 FIN 分节，服务器端处于FIN_WAIT1状态。

2）客户TCP回应ACK后，服务TCP将转入FIN_WAIT2状态。

3）此时如果客户进程没有处理该 FIN （如阻塞在其它调用上而没有关闭 Socket 时），则客户TCP将处于CLOSE_WAIT状态。

4）当客户进程再次向 FIN_WAIT2 状态的服务 TCP 发送数据时，则服务 TCP 将立刻响应 RST。

一般来说，这种情况还可以会引发另外的应用程序异常，客户进程在发送完数据后，往往会等待从网络IO接收数据，很典型的如 read 或 readline 调用，此时由于执行时序的原因，如果该调用发生在RST分节收到前执行的话，那么结果是客户进程会得到一个非预期的 EOF 错误。此时一般会输出“server terminated prematurely”－“服务器过早终止”错误。


_ 103 ECONNABORTED Software caused connection abort

1、软件导致的连接取消。一个已经建立的连接被host方的软件取消，原因可能是数据传输超时或者是协议错误。

2、该错误被描述为“software caused connection abort”，即“软件引起的连接中止”。原因在于当服务和客户进程在完成用于 TCP 连接的“三次握手”后，客户 TCP 却发送了一个 RST （复位）分节，在服务进程看来，就在该连接已由 TCP 排队，等着服务进程调用 accept 的时候 RST 却到达了。POSIX 规定此时的 errno 值必须 ECONNABORTED。源自 Berkeley 的实现完全在内核中处理中止的连接，服务进程将永远不知道该中止的发生。服务器进程一般可以忽略该错误，直接再次调用accept。

当TCP协议接收到RST数据段，表示连接出现了某种错误，函数read将以错误返回，错误类型为ECONNERESET。并且以后所有在这个套接字上的读操作均返回错误。错误返回时返回值小于0。
_ 102 ENETRESET__ Network dropped connection on reset

网络重置时丢失连接。

由于设置了"keep-alive"选项，探测到一个错误，连接被中断。在一个已经失败的连接上试图使用setsockopt操作，也会返回这个错误。
_ 101 ENETUNREACH_ Network is unreachable

网络不可达。Socket试图操作一个不可达的网络。这意味着local的软件知道没有路由到达远程的host。
_ 100 ENETDOWN__ Network is down
_ 99 EADDRNOTAVAIL Cannot assign requested address
_ 98 EADDRINUSE_ Address already in use
_ 97 EAFNOSUPPORT Address family not supported by protocol
_ 96 EPFNOSUPPORT Protocol family not supported
_ 95 EOPNOTSUPP_ Operation not supported
_ 94 ESOCKTNOSUPPORT Socket type not supported

Socket类型不支持。指定的socket类型在其address family中不支持。如可选选中选项SOCK_RAW，但实现并不支持SOCK_RAW sockets。


_ 93 EPROTONOSUPPORT Protocol not supported

不支持的协议。系统中没有安装标识的协议，或者是没有实现。如函数需要SOCK_DGRAM socket，但是标识了stream protocol.。


_ 92 ENOPROTOOPT_ Protocol not available

该错误不是一个 Socket 连接相关的错误。errno 给出该值可能由于，通过 getsockopt 系统调用来获得一个套接字的当前选项状态时，如果发现了系统不支持的选项参数就会引发该错误。
_ 91 EPROTOTYPE_ Protocol wrong type for socket

协议类型错误。标识了协议的Socket函数在不支持的socket上进行操作。如ARPA Internet

UDP协议不能被标识为SOCK_STREAM socket类型。
_ 90 EMSGSIZE__ +Message too long

消息体太长。

发送到socket上的一个数据包大小比内部的消息缓冲区大，或者超过别的网络限制，或是用来接收数据包的缓冲区比数据包本身小。
_ 89 EDESTADDRREQ Destination address required

需要提供目的地址。

在一个socket上的操作需要提供地址。如往一个ADDR_ANY 地址上进行sendto操作会返回这个错误。
_ 88 ENOTSOCK__ Socket operation on non-socket

在非socket上执行socket操作。
_ 87 EUSERS___ Too many users
_ 86 ESTRPIPE__ Streams pipe error
_ 85 ERESTART__ Interrupted system call should be restarted
_ 84 EILSEQ___ Invalid or incomplete multibyte or wide character
_ 83 ELIBEXEC__ Cannot exec a shared library directly
_ 82 ELIBMAX___ Attempting to link in too many shared libraries
_ 81 ELIBSCN___ .lib section in a.out corrupted
_ 80 ELIBBAD___ Accessing a corrupted shared library
_ 79 ELIBACC___ Can not access a needed shared library
_ 78 EREMCHG___ Remote address changed
_ 77 EBADFD___ File descriptor in bad state
_ 76 ENOTUNIQ__ Name not unique on network
_ 75 EOVERFLOW__ Value too large for defined data type
_ 74 EBADMSG__ +Bad message
_ 73 EDOTDOT___ RFS specific error
_ 72 EMULTIHOP__ Multihop attempted
_ 71 EPROTO___ Protocol error
_ 70 ECOMM____ Communication error on send
_ 69 ESRMNT___ Srmount error
_ 68 EADV____ Advertise error
_ 67 ENOLINK___ Link has been severed
_ 66 EREMOTE___ Object is remote
_ 65 ENOPKG___ Package not installed
_ 64 ENONET___ Machine is not on the network
_ 63 ENOSR____ Out of streams resources
_ 62 ETIME____ Timer expired
_ 61 ENODATA___ No data available
_ 60 ENOSTR___ Device not a stream
_ 59 EBFONT___ Bad font file format
_ 57 EBADSLT___ Invalid slot
_ 56 EBADRQC___ Invalid request code
_ 55 ENOANO___ No anode
_ 54 EXFULL___ Exchange full
_ 53 EBADR____ Invalid request descriptor
_ 52 EBADE____ Invalid exchange
_ 51 EL2HLT___ Level 2 halted
_ 50 ENOCSI___ No CSI structure available
_ 49 EUNATCH___ Protocol driver not attached
_ 48 ELNRNG___ Link number out of range
_ 47 EL3RST___ Level 3 reset
_ 46 EL3HLT___ Level 3 halted
_ 45 EL2NSYNC__ Level 2 not synchronized
_ 44 ECHRNG___ Channel number out of range
_ 43 EIDRM____ Identifier removed
_ 42 ENOMSG___ No message of desired type
_ 40 ELOOP____ Too many levels of symbolic links
_ 39 ENOTEMPTY_ +Directory not empty
_ 38 ENOSYS___ +Function not implemented
_ 37 ENOLCK___ +No locks available
_ 36 ENAMETOOLONG +File name too long
_ 35 EDEADLK__ +Resource deadlock avoided
_ 34 ERANGE___ +Numerical result out of range
_ 33 EDOM____ +Numerical argument out of domain
_ 32 EPIPE___ +Broken pipe

接收端关闭(缓冲中没有多余的数据),但是发送端还在write:

1、Socket 关闭，但是socket号并没有置-1。继续在此socket上进行send和recv，就会返回这种错误。这个错误会引发SIGPIPE信号，系统会将产生此EPIPE错误的进程杀死。所以，一般在网络程序中，首先屏蔽此消息，以免发生不及时设置socket进程被杀死的情况。

2、write(..) on a socket that has been closed at the other end will cause a SIGPIPE.

3、错误被描述为“broken pipe”，即“管道破裂”，这种情况一般发生在客户进程不理会（或未及时处理）Socket 错误，继续向服务 TCP 写入更多数据时，内核将向客户进程发送 SIGPIPE 信号，该信号默认会使进程终止（此时该前台进程未进行 core dump）。结合上边的 ECONNRESET 错误可知，向一个 FIN_WAIT2 状态的服务 TCP（已 ACK 响应 FIN 分节）写入数据不成问题，但是写一个已接收了 RST 的 Socket 则是一个错误。


_ 31 EMLINK___ +Too many links
_ 30 EROFS___ +Read-only file system
_ 29 ESPIPE___ +Illegal seek
_ 28 ENOSPC___ +No space left on device
_ 27 EFBIG___ +File too large
_ 26 ETXTBSY___ Text file busy
_ 25 ENOTTY___ +Inappropriate ioctl for device
_ 24 EMFILE___ +Too many open files

打开了太多的socket。对进程或者线程而言，每种实现方法都有一个最大的可用socket数目处理，或者是全局的，或者是局部的。

_ 23 ENFILE___ +Too many open files in system
_ 22 EINVAL___ +Invalid argument

无效参数。提供的参数非法。有时也会与socket的当前状态相关，如一个socket并没有进入listening状态，此时调用accept，就会产生EINVAL错误。


_ 21 EISDIR___ +Is a directory
_ 20 ENOTDIR__ +Not a directory
_ 19 ENODEV___ +No such device
_ 18 EXDEV___ +Invalid cross-device link
_ 17 EEXIST___ +File exists
_ 16 EBUSY___ +Device or resource busy
_ 15 ENOTBLK___ Block device required
_ 14 EFAULT___ +Bad address地址错误
_ 13 EACCES___ +Permission denied
_ 12 ENOMEM___ +Cannot allocate memory
_ 11 EAGAIN___ +Resource temporarily unavailable

在读数据的时候,没有数据在底层缓冲的时候会遇到,一般的处理是循环进行读操作,异步模式还会等待读事件的发生再读

1、Send返回值小于要发送的数据数目，会返回EAGAIN和EINTR。

2、recv 返回值小于请求的长度时说明缓冲区已经没有可读数据，但再读不一定会触发EAGAIN，有可能返回0表示TCP连接已被关闭。

3、当socket是非阻塞时,如返回此错误,表示写缓冲队列已满,可以做延时后再重试.

4、在Linux进行非阻塞的socket接收数据时经常出现Resource temporarily unavailable，errno代码为11(EAGAIN)，表明在非阻塞模式下调用了阻塞操作，在该操作没有完成就返回这个错误，这个错误不会破坏socket的同步，不用管它，下次循环接着recv就可以。对非阻塞socket而言，EAGAIN不是一种错误。


_ 10 ECHILD___ +No child processes

__ 9 EBADF___ +Bad file descriptor

__ 8 ENOEXEC__ +Exec format error
__ 7 E2BIG___ +Argument list too long

__ 6 ENXIO___ +No such device or address

__ 5 EIO____ +Input/output error
__ 4 EINTR___ +Interrupted system call

阻塞的操作被取消阻塞的调用打断。如设置了发送接收超时，就会遇到这种错误。

只能针对阻塞模式的socket。读，写阻塞的socket时，-1返回，错误号为INTR。另外，如果出现EINTR即errno为4，错误描述Interrupted system call，操作也应该继续。如果recv的返回值为0，那表明连接已经断开，接收操作也应该结束。
__ 3 ESRCH___ +No such process

__ 2 ENOENT___ +No such file or directory

__ 1 EPERM___ +Operation not permitted

原文地址：https://zhuanlan.zhihu.com/p/451702640

作者：linux

# 【NO.356】QQ音乐高可用架构体系

### **1. QQ音乐高可用架构体系全景**

故障无处不在，而且无法避免。（[分布式计算谬误](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing)）

在分布式系统建设的过程中，我们思考的重点不是避免故障，而是拥抱故障，通过构建高可用架构体系来获得优雅应对故障的能力。QQ音乐高可用架构体系包含三个子系统：架构、工具链和可观测性。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZe6zbQANoB1icltLwzqsS8GTps2KXYjPXdu9OehD4JYUqibrjkpDmMngA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



- **架构**：架构包括冗余架构、自动故障转移和稳定性策略。高可用架构的基础是通过冗余架构来避免单点故障。其中，基础设施的冗余包括集群部署、多机房部署，多中心部署等；软件架构上要支持横向扩展、负载均衡和自动故障转移。这样系统就具备了基础的容灾容错能力。在冗余架构的基础之上，可以通过一系列稳定性策略来进一步提升架构的可用性。稳定性策略包括分布式限流，熔断，动态超时等。
- **工具链**：工具链指一套可互相集成和协作的工具，从外围对架构进行实验和测试以达到提升架构可用性的目的，包括混沌工程和全链路压测等。混沌工程通过注入故障的方式，来发现系统的脆弱环节并针对性地加固，帮助我们提升系统的可用性。全链路压测通过真实、高效的压测，来帮助业务完成性能测试任务，进行容量评估和瓶颈定位，保障系统稳定。
- **可观测性**：通过观测系统的详细运行状态来提升系统的可用性，包括日志、指标、全链路跟踪、性能分析和panic分析。可观测性可以用于服务生命周期的所有阶段，包括服务开发，测试，发布，线上运行，混沌工程，全链路压测等各种场景。



### **2. 容灾架构：**

业内主流的容灾方案，包括异地冷备，同城双活，两地三中心，异地双活/多活等。

- **异地冷备**：冷备中心不工作，成本浪费，关键时刻不敢切换。
- **同城双活**：同城仍然存在很多故障因素(如自然灾害)导致整体不可用。
- **异地双活/多活**：双写/多写对数据一致性是个极大挑战。有些做法是按用户ID哈希，使用户跟数据中心绑定，避免写冲突问题，但这样一来舍弃了就近接入的原则，而且灾难发生时要手动调度流量，也无法做到API粒度的容灾。

容灾架构的选型我们需要衡量投入产出比，不要为了预防哪些极低概率的风险事件而去投入过大的成本，毕竟业务的规模和收入才是重中之重。QQ音乐的核心体验是听歌以及围绕听歌的一系列行为，这部分业务以只读服务为主。而写容灾需要支持双写，带来的数据一致性风险较大，且有一定的实施成本。综合权衡，我们舍弃写容灾，采用**一写双读的异地双活**模式。

#### 2.1. 异地双中心

在原有深圳中心的基础上，建设上海中心，覆盖接入层、逻辑层和存储层，形成异地双中心。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZQ1SBfRSEHxmeL67PXWnKjwgicunOToPcMbtXhjy3MiaBSOhTR18sdtDQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



- **接入层**：深圳中心和上海中心各部署一套STGW和API网关。通过GSLB把流量按就近原则划分为两份，分配给深圳中心和上海中心，深圳中心和上海中心做流量隔离。
- **逻辑层**：服务做读写分离。深圳部署读/写服务，上海部署只读服务，上海的写请求由API网关路由到深圳中心处理。
- **存储层**：深圳中心和上海中心各有一套存储。写服务从深圳写入存储，通过同步中心/存储组件同步到上海，同步中心/存储组件确保两地数据的一致性。同步方式有两种，对于有建设异地同步能力的组件Cmongo和CKV+，依赖存储组件的异地同步能力，其他不具备异地同步能力的，如ckv，tssd等老存储，使用同步中心进行同步。

#### 2.2. 自动故障转移

异地容灾支持自动故障转移才有意义。如果灾难发生后，我们在灾难发现、灾难响应以及评估迁移风险上面浪费大量时间，灾难可能已经恢复。

我们最初的方案是，客户端对两地接入IP进行动态评分（请求成功加分，请求失败减分），取最优的IP接入来达到容灾的效果。经过近两年多的外网实践，遇到一些问题，动态评分算法敏感度高会导致流量在两地频繁漂移，算法敏感度低起不了容灾效果。而算法调优依赖客户端版本更新也导致成本很高。

后来我们基于异地自适应重试对容灾方案做了优化，核心思想是在API网关上做故障转移，降低客户端参与度。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZZHuHDDDZA5gtVa4oef2NXLOs7Fa0sqehTIbhLIJ5zjIrUK3ibGTG2bw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



方案主要有两点：

- **API网关故障转移**：当本地中心API返回失败时（包括触发熔断和限流），API网关把请求路由到异地处理。以此解决API故障的场景。
- **客户端故障转移**：当API网关发生超时的时候，客户单进行异地重试。如果网关有回包，即使API返回失败，客户端也不重试。解决API网关故障的场景。

使用最新方案后，API网关重试比客户端调度更可控，双中心流量相对稳定，一系列自适应限流和熔断策略也抵消重试带来的请求量放大问题。接下来介绍方案细节。

#### 2.3. API网关故障转移

API网关故障转移需要考虑重试流量不能压垮异地，否则会造成双中心同时雪崩。这里我们做了一个自适应重试的方案，在异地成功率下降的时候，取消重试。

**自适应重试方案**：

- 引入重试窗口：如果当前周期窗口为10，则最多只能重试10次，超过的部分丢弃。
- 网关请求服务失败，判断重试窗口是否耗光。如果耗光则不重试，如果还有余额，重试异地。

上述方案中的重试窗口，由**探测及退避策略**决定：

- **探测策略**：当探测成功率正常时，增大下一次窗口并继续探测。通过控制窗口大小，避免重试流量瞬间把异地打垮。
- **退避策略**：在探测成功率出现异常时，重试窗口快速退避。
- **增加重试开关**，控制整体及服务两个维度的重试。

**探测策略及退避策略图示：**

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZnrPjHA4KECPw28RXF1Nuza94JgicRDRsCJicksf2zyYkaSB1O1datXTg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)





**探测策略及退避策略的算法描述：**

```
// 设第 i 次探测的窗口为 f(i)，实际探测量为 g(i)，第 i 次探测的成功率为 s(i)，第 i 次本地总请求数为 t。
// 那么第 i+1 次探测的窗口为 f(i+1)，取值如下：
if  s(i) = [98%, 100%]       // 第 i 次探测成功率 >= 98%，探测正常
    if  g(i) >= f(i)      // 如果第 i 次实际探测量等于当前窗口，增大第 i+1 次窗口大小
        f(i+1) = f(i) + max (min ( 1% * t, f(i) ) , 1)    
    else
        f(i+1) = f(i)        // 如果第 i 次实际探测量小于当前窗口，第 i+1 次探测窗口维持不变
else   
    f(i+1) = max(1,f(i)/2)        // 如果第 i 次探测异常，第 i+1 次窗口退避置 1
// 其中，重试窗口即 f(i) 初始大小为 1。算法中参数及细节，根据实际测试和线上效果进行调整。
```

**自适应重试效果：**

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZYngHib3AAm32O7mU2hmenricEZrX7PGQibqyyiaYprUOrnJVIxmibFo2wHQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)





#### 2.4. 客户端故障转移

- 当客户端未收到响应时，说明API网关异常或者网络不通，客户端重试异地。
- 当客户端收到响应，而http状态码为5xx，说明API网关异常，客户端重试异地。当http状态码正常，说明API网关正常，此时即使API失败也不重试。
- 当双中心均超时，探测网络是否正常，如果网络正常，说明两地API网关均异常，所有客户端请求冻结一段时间。



### **3. 稳定性策略：**

#### 3.1. 分布式限流

即使我们在容量规划上投入大量精力，根据经验充分考虑各种请求来源，评估出合理的请求峰值，并在生产环境中准备好足够的设备。但预期外的突发流量总会出现，对我们规划的容量造成巨大冲击，极端情况下甚至会导致雪崩。我们对容量规划的结果需要坚守不信任原则，做好防御式架构。

限流可以帮助我们应对突发流量，我们有几个选择：

- **固定窗口计算器**优点是简单，但存在临界场景无法限流的情况。
- **漏桶**是通过排队来控制消费者的速率，适合瞬时突发流量的场景，面对恒定流量上涨的场景，排队中的请求容易超时饿死。
- **令牌桶**允许一定的突发流量通过，如果下游（callee）处理不了会有风险。
- **滑动窗口计数器**可以相对准确地完成限流。

我们采用的是滑动窗口计数器，主要考虑以下几点：

- 超过限制后微服务框架直接丢弃请求。
- 对原有架构不引入关键依赖，用分布式限流的方式代替全局限流。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZpqRmeluEhvEVyMUsUL1rhdZtlEfxHUhfZGdNemjUdnh4cibUibk7DyWg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



上图描述ServceA到ServiceB之间的RPC调用过程中的限流：

- Sidecar用于接管微服务的信令控制，限流器则以插件的方式集成到Sidecar。限流器通过限流中心Agent，从限流中心获取限流结果数据。
- ServiceA发起请求到ServiceB前，通过Sidecar的限流器判断是否超出限制，如果超出，则进入降级逻辑。
- 限流中心采用滑动窗口的方式，计算每个Service的限流数据。

限流算法的选择，还有一种可行的方案是，框架提供不同的限流组件，业务方根据业务场景来选择，但也要考虑成本。社区也有Sentinel等成熟解决方案，新业务可以考虑集成现成的方案。

#### 3.2. 自适应限流

上一节的分布式限流是在Client-side限制流量，即请求量超出阈值后在主调直接丢弃请求，被调不需要承担拒绝请求的资源开销，可最大程度保护被调。然而，Client-side限制流量强依赖主调接入分布式限流，这一点比较难完全受控。同时，分布式限流在集群扩缩容后需要及时更新限流阈值，而全量微服务接入有一定的维护成本。而且分布式限流直接丢弃请求更偏刚性。作为分布式限流的互补能力，自适应限流是在Server-side对入口流量进行控制，自动嗅探负载、入口QPS、请求耗时等指标，让系统的入口QPS和负载达到一个平衡，确保系统以高水位QPS正常运行，而且不需要人工维护限流阈值。相比分布式限流，自适应限流更偏柔性。

**指标说明：**

| 指标名称  |               指标含义                |
| :-------: | :-----------------------------------: |
| CPU usage | 当系统CPU使用率超过阈值启动自适应限流 |
| inflight  |       系统中正在处理的请求数量        |
|    qps    |    窗口内每个桶的请求处理成功的量     |
|  MaxQPS   |         滑动窗口中QPS的最大值         |
|    rt     |   窗口内每个桶的请求成功的响应耗时    |
|   MinRt   |      滑动窗口中响应延时的最小值       |

**算法原理：**

根据Little's Law，inflight = 延时 *QPS。则最优inflight为MaxPass* MinRt，当系统当前inflight超过最优inflight，执行限流动作。

用公式表示为：cpu > 800 AND InFlight > (MaxQPS * MinRt)

其中MaxQPS和MinRt的估算需要增加平滑策略，避免秒杀场景下最优inflight的估算失真。

**限流效果：**

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZo6yzGW8nDB0jJjianvKn4sLdeia1aniaLKBTWjJRibZsKl3tV7aA847P2g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)





#### 3.3. 熔断

在微服务系统中，服务会依赖于多个服务，并且有一些服务也依赖于它。如下图，“统一权限”服务，依赖歌曲权限配置、购买信息、会员身份等服务，综合判断用户是否拥有对某首歌曲进行播放/下载等操作的权限，而这些权限信息，又会被歌单、专辑等歌曲列表服务依赖。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZo8DXa3Y0AvoqWLc9Llj8DibmvYNxRvt1XsAwPz8OKriaiaoYXudiajg9CA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



当“统一权限”服务的其中一个依赖服务（比如歌曲权限配置服务）出现故障，“统一权限”服务只能被动的等待依赖服务报错或者请求超时，下游连接池会逐渐被耗光，入口请求大量堆积，CPU、内存等资源逐渐耗尽，导致服务宕掉。而依赖“统一权限”服务的上游服务，也会因为相同的原因出现故障，一系列的级联故障最终会导致整个系统宕掉。

合理的解决方案是断路器和优雅降级，通过尽早失败来避免局部不稳定而导致的整体雪崩。

传统熔断器实现Closed、Half Open、Open三个状态，当进入Open状态时会拒绝所有请求，而进入Closed状态时瞬间会有大量请求，服务端可能还没有完全恢复，会导致熔断器又切换到Open状态，一种比较刚性的熔断策略。SRE熔断只有打Closed和Half-Open两种状态，根据请求成功率自适应地丢弃请求，尽可能多地让请求成功请求到服务端，是一种更弹性的熔断策略。QQ音乐采用更弹性的SRE熔断器：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZ47danuia5ANN1oiamLlDEViccAw3CDk1JE013LlCJIQg4ktPmribIDC6qQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



- **requests**：窗口时间内的请求总数。
- **accepts**：正常处理的请求数量。
- **K**：敏感度，K越小丢弃概率越大，一般在1.5~2之间。

正常情况下，requests 等于 accepts，所以丢弃概率为0。随着正常处理的请求减少，直到 requests 等于 K * accepts ，一旦超过这个限制，熔断器就会打开，并按照概率丢弃请求。

#### 3.4. 动态超时

超时是一件很容易被忽视的事情。在早期架构发展阶段，相信大家都有因为遗漏设置超时或者超时设置太长导致系统被拖慢甚至挂起的经历。随着微服务架构的演进，超时逐渐被标准化到RPC中，并可通过微服务治理平台快捷调整超时参数。但仍有不足，传统超时会设定一个固定的阈值，响应时间超过阈值就返回失败。在网络短暂抖动的情况下，响应时间增加很容易产生大规模的成功率波动。另一方面，服务的响应时间并不是恒定的，在某些长尾条件下可能需要更多的计算时间，为了有足够的时间等待这种长尾请求响应，我们需要把超时设置足够长，但超时设置太长又会增加风险，超时的准确设置经常困扰我们。

其实我们的微服务系统对这种短暂的延时上涨具备足够的容忍能力，可以考虑基于EMA算法动态调整超时时长。EMA算法引入“平均超时”的概念，用平均响应时间代替固定超时时间，只要平均响应时间没有超时即可，而不是要求每次都不能超时。主要算法：总体情况不能超标；平均情况表现越好，弹性越大；平均情况表现越差，弹性越小。

如下图，当平均响应时间(EMA)大于超时时间限制(Thwm)，说明平均情况表现很差，动态超时时长(Tdto)就会趋近至超时时间限制(Thwm)，降低弹性。当平均响应时间(EMA)小于超时时间限制(Thwm)，说明平均情况表现很好，动态超时时长(Tdto)就可以超出超时时间限制(Thwm)，但会低于最大弹性时间(Tmax)，具备一定的弹性。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZIY1ibuskViccd3NU5vUm4NAbTBg3U0Ax1YnOus9qMib5Duq1gwOESldoQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



为降低使用门槛，QQ音乐微服务只提供超时时间限制(Thwm)和最大弹性时间(Tmax)两个参数的设置，并可在微服务治理平台调整参数。算法实现参考：https://github.com/jiamao/ema-timeout

#### 3.5. 服务分级

我们做了很多弹性能力，比如限流，我们是否可以根据服务的重要程度来决策丢弃请求。此外，在架构迭代的过程中，有许多涉及整体系统的大工程，如微服务改造，容器化，限流熔断能力落地等项目，我们需要根据服务的重要程度来决策哪些服务先行。

如何为服务确定级别：

- **1级**：系统中最关键的服务，如果出现故障会导致用户或业务产生重大损失，比如登录服务、流媒体服务、权限服务、数专服务等。
- **2级**：对于业务非常重要，如果出现故障会导致用户体验受到影响，但是不会完全无法使用我们的系统，比如排行榜服务、评论服务等。
- **3级**：会对用户造成较小的影响，不容易注意或很难发现，比如用户头像服务，弹窗服务等。
- **4级**：即使失败，也不会对用户体验造成影响，比如红点服务等。

服务分级的应用场景：

- 核心接口运营质量日报：每日邮件推送1级服务和2级服务的观测数据。
- SLA：针对1级服务和2级服务，制定SLO。
- API网关根据服务分级限流，优先确保1级服务通过。
- 重大项目参考服务重要程度制定优先级计划，如容灾演练，大型活动压测等。

#### 3.6. API网关分级限流

API网关既是用户访问的流量入口，也是后台业务响应的最终出口，其可用性是QQ音乐架构体系的重中之重。除了支持自适应限流能力，针对服务重要程度，当触发限流时优先丢弃不重要的服务。

效果如下图，网关高负载时，2级、3级、4级服务丢弃，只有1级服务通过。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZ0nZkdkTJsn6ySGB6pqo0t2VjG6k3f9YQILnNqIicfepb1yYhmkA6hLg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)





### **4. 工具链：**

随着产品的迭代，系统不断在变更，性能、延时、业务逻辑、用户量等因素的变化都有可能引入新的风险，而现有的架构弹性能力可能不足以优雅地应对新的风险。事实上，即使架构考虑的场景再多，外网仍然存在很多未知的风险，在某些特殊条件满足后就会引发故障。一般情况下，我们只能等着告警出现。当告警出现后，复盘总结，讨论规避方案，进行下一轮的架构优化。应对故障的方式比较被动。

那么，我们有没有办法变被动为主动？在故障触发之前，尽可能多地识别风险，针对性地加固和防范，而不是等着故障发生。业界有比较成熟的理论和工具，混沌工程和全链路压测。

#### 4.1. 混沌工程

混沌工程通过在生产环境上进行实验，注入网络超时等故障，主动找出系统中的脆弱环节，不断提升系统的弹性。

TMEChaos 以ChaosMesh为底层故障注入引擎，结合TME微服务架构、mTKE容器平台打造成云原生混沌工程实验平台。支持丰富的故障模拟类型，具有强大的故障场景编排能力，方便研发同学在开发测试中以及生产环境中模拟现实世界中可能出现的各类异常，帮助验证架构设计是否合理，系统容错能力是否符合预期，为组织提供常态化应急响应演练，帮助业务推进高可用建设。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZbOW74FFNNoseJOV90XlJ0xtibhc7343ax5dOLacuH0C8CZwsdYLsibxQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



- **TMEChaos Dashboard Web**：TMEChaos的可视化组件，提供了一套用户友好的 Web 界面，用户可通过该界面对混沌实验进行操作和观测。
- **TMEChaos Dashboard Backend**：NodeJS实现的Dashboard中间层，为Web提供Rest API接口，并进行TMEOA权限/微服务权限验证。
- **TMEChaos APIServer**：TMEChaos的逻辑组件，主要负责服务维度的爆炸半径设置，ChaosMesh多集群管理、实验状态管理、Mock行为注入。
- **Status Manager**：负责查询各ChaosMesh集群中的实验状态同步到TMEChaos的存储组件。
- **SteadyState Monitor**：稳态指标组件，负责监控混沌实验运行过程中IAAS层/服务相关指标，如果超出阈值自动终止相关实验。
- **ChaosMesh Dashboard API**：ChaosMesh 对外暴露的Rest API接口层，用于实验的增删改查，直接跟K8S APIServer交互。
- **Chaos Controller Manager**：ChaosMesh 的核心逻辑组件，主要负责混沌实验的调度与管理。该组件包含多个 CRD Controller，例如 Workflow Controller、Scheduler Controller 以及各类故障类型的 Controller。
- **Chaos Daemon**：ChaosMesh 的主要执行组件，以 DaemonSet 的方式运行。该组件主要通过侵入目标 Pod Namespace 的方式干扰具体的网络设备、文件系统、内核等。

#### 4.2. 全链路压测

上一节的混沌工程是通过注入故障的方式，来发现系统的脆弱环节。而全链路压测，则是通过注入流量给系统施加压力的方式，来发现系统的性能瓶颈，并帮助我们进行容量规划，以应对生产环境各种流量冲击。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZ5AV1Cd2JkxMtTT2J7DMsgzXdXvYYUmib3myTibxia99qUm4exXYdibRXXg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



全链路压测的核心模块有4个：流量构造、流量染色、压测引擎和智能监控。

- **流量构造**：为了保证真实性，从正式环境API网关抽样采集真实流水，同时也提供自定义流量构造。
- **流量染色**：全链路压测在生产环境进行。生产环境需要具备数据与流量隔离能力，不能影响到原有的用户数据、BI报表以及推荐算法等等。要实现这个目标，首先要求压测流量能被识别，我们采用的做法是在RPC的context里面增加染色标记，所有的压测流量都带有这个染色标记，并且这些标记能够随RPC调用关系进行传递。然后，业务系统根据标记识别压测流量，在存储时，将压测数据存储到影子存储而不是覆盖原有数据。
- **压测引擎**：对接各类协议，管理发压资源，以可调节的速率发送请求。
- **智能监控**：依靠可观测能力，收集链路数据并展示，根据熔断规则智能熔断。



### **5. 可观测性：**

随着微服务架构和基础设施变得越来越复杂，传统监控已经无法完整掌握系统的健康状况。此外，服务等级目标要求较小的故障恢复时间，所以我们需要具备快速发现和定位故障的能力。可观测性可以帮助我们解决这些问题。

指标、日志和链路追踪构成可观测的三大基石，为我们提供架构感知、瓶颈定位、故障溯源等能力。借助可观测性，我们可以对系统有更全面和精细的洞察，发现更深层次的系统问题，从而提升可用性。

在实践方面，目前业界已经有很多成熟的技术栈，包括Prometheus，Grafana，ELK，Jaeger等。基于这些技术栈，我们可以快速搭建起可观测系统。

#### 5.1. Metrics

指标监控能够从宏观上对系统的状态进行度量，借助QPS、成功率、延时、系统资源、业务指标等多维度监控来反映系统整体的健康状况和性能。

我们基于Prometheus构建联邦集群，实现千万指标的采集和存储，提供秒级监控，搭配Grafana做可视化展示。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZUYfk95heBkAvHTV1mTcoOkGicicU6S01lHfaLR8VicWcsZGMHaNVO61iag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



我们在微服务框架中重点提供四个黄金指标的观测：

- **Traffic（QPS）**
- **Latency（延时）**
- **Error（成功率）**
- **Staturation（资源利用率）**

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZNOMCe9cQbBsZiaibWPKWQqyqrCgLfI2cb1bribG3mPNucgwRzeIb64ib5g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZ2zTLX5TVbHSQ2FwqmaPvouFlGNWoF2Hd3mhxJ6RTTNNebTd67WQd9g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)





![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZIaInAKDVETLFgWMMiaXJ7oaJ9lwVSbE82cnYwIR1yxCl1KhrkibDB9IQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



QQ音乐Metrics解决方案优势：

- **秒级监控**：QQ音乐存在多个高并发场景，数专、演唱会直播、明星空降评论/社区等，会带来比日常高出几倍甚至数十倍的流量，而且流量集中在活动开始头几秒，容量评估难度极大。我们通过搭建Prometheus联邦，每3秒抓取一次数据，实现了准实时监控。并对活动进行快照采集，记录活动发生时所有微服务的请求峰值，用于下次同级别艺人的峰值评估。
- **历史数据回溯**：QQ音乐海量用户及上万微服务，每天产生的数据量级很大。当我们需要回溯近一个月甚至一年前的指标趋势时，性能是个极大挑战。由于历史数据的精度要求不高，我们通过Prometheus联邦进行阶梯降采样，可以永久存放历史数据，同时也极大降低存储成本。

#### 5.2. Logging

随着业务体量壮大，机器数量庞大，使用SSH检索日志的方式效率低下。我们需要有专门的日志处理平台，从庞大的集群中收集日志并提供集中式的日志检索。同时我们希望业务接入日志处理平台的方式是无侵入的，不需要使用特定的日志打印组件。

我们使用ELK（ElasticSearch、Logstash、Kibana）构建日志处理平台，提供无侵入、集中式的远程日志采集和检索系统。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZdSgXhyibBpBe98KPFSRpVK6Jx0jvxlialZJQnDSdFwQyIcoU2bQ3QlPA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



- **Filebeat** 作为日志采集和传送器。Filebeat监视服务日志文件并将日志数据发送到Kafka。
- **Kafka** 在Filebeat和Logstash之间做解耦。
- **Logstash** 解析多种日志格式并发送给下游。
- **ElasticSearch** 存储Logstash处理后的数据，并建立索引以便快速检索。
- **Kibana** 是一个基于ElasticSearch查看日志的系统，可以使用查询语法来搜索日志，在查询时制定时间和日期范围或使用正则表达式来查找匹配的字符串。

下图为音乐馆首页服务的远程日志：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZXK6czF8Vpy1IhQQke4pxIvjunOV3j8WV4X1z8bmpv5lnda79llNrBg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



#### 5.3. Tracing

在微服务架构的复杂分布式系统中，一个客户端请求由系统中大量微服务配合完成处理，这增加了定位问题的难度。如果一个下游服务返回错误，我们希望找到整个上游的调用链来帮助我们复现和解决问题，类似gdb的backtrace查看函数的调用栈帧和层级关系。

Tracing在触发第一个调用时生成关联标识Trace ID，我们可以通过RPC把它传递给所有的后续调用，就能关联整条调用链。Tracing还通过Span来表示调用链中的各个调用之间的关系。

我们基于jaeger构建分布式链路追踪系统，可以实现分布式架构下的事务追踪、性能分析、故障溯源、服务依赖拓扑。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZRP3z1QWPUTE6x7MQTf3sS5GbWDjQ0V9ic8Cvm5j23FWshoBjCP1MSKQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



- **jaeger-agent** 作为代理，把jaeger client发送的spans转发到jaeger-collector。
- **jaeger-collector** 接收来自jaeger-agent上报的数据，验证和清洗数据后转发至kafka。
- **jaeger-ingester** 从kafka消费数据，并存储到ElasticSearch。
- **jaeger-query** 封装用于从ElasticSearch中检索traces的APIs。

下图为音乐馆首页服务的链路图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZPCRtFt1LrTZue01viaMEn8vjFYSria7icE8Dpyt1hZtojibMjwNs98icXbg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



#### 5.4. Profiles

想必大家都遇到过服务在凌晨三点出现CPU毛刺。一般情况下，我们需要增加pprof分析代码，然后等待问题复现，问题处理完后删掉pprof相关代码，效率底下。如果这是个偶现的问题，定位难度就更大。我们需要建设一个可在生产环境使用分析器的系统。

建设这个系统需要解决三个问题：

- 性能数据采集后需要持久化，方便回溯分析。
- 可视化检索和分析性能数据。
- 分析器在生产环境采集数据会有额外开销，需要合理采样。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZaPA88iafQMG4IKBRGLafPzZsbnEpxl3teaibqPicZWtNChGBpib7Dibicianw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



我们基于conprof搭建持续性能分析系统：

- 线上服务根据负载以及采样决定采集时机，并暴露profile接口。
- conprof定时将profile信息采集并存储。
- conprof提供统一的可视化平台检索和分析。

如下图，我们可以通过服务名、实例、时间区间来检索profile信息，每一个点对应一个记录。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZNS0iaibDDpktrwILpEPjHwIxk8OuoWv46EZFNP0uxnOnia7ibhRvFN1Elg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZiaTibPzTlFIxgRCkAJOYA9AAP5M9fGZjAmJv1v5tdpgWloia74XHhfDJw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



#### 5.5. Dumps

传统的方式是在进程崩溃时把进程内存写入一个镜像中以供分析，或者把panic信息写到日志中。core dumps的方式在容器环境中实施困难，panic信息写入日志则容易被其他日志冲掉且感知太弱。QQ音乐使用的方式是在RPC框架中以拦截器的方式注入，发生panic后上报到sentry平台。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatT4zrm9QGfBt43Pd4uicIVZtic76Mfm2RTvv6qTwqEbMb0RNTTApAHrUmJaSNVt5EONmZ1SkWEfIRw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



### **6. 总结**

本文从架构、工具链、可观测三个维度，介绍了QQ音乐多年来积累的高可用架构实践。先从架构出发，介绍了双中心容灾方案以及一系列稳定性策略。再从工具链维度，介绍如何通过工具平台对架构进行测试和风险管理。最后介绍如何通过可观测来提升架构可用性。这三个维度的子系统紧密联系，相互协同。架构的脆弱性是工具链和可观测性的建设动力，工具链和可观测性的不断完善又会反哺架构的可用性提升。

此外，QQ音乐微服务建设、Devops建设、容器化建设也是提升可用性的重要因素。单体应用不可用会导致所有的功能不可用，而微服务化按单一职责拆分服务，可以很好地处理服务不可用和功能降级问题。Devops把服务生命周期的管理自动化，通过持续集成、持续测试、持续发布等来降低人工失误的风险。容器化最大程度降低基础设施的影响，让我们能够将更多精力放在服务的可用性上，此外，资源隔离，HPA，健康检查等，也在一定程度上提升可用性。

至此，基础架构提供了各种高可用的能力，但可用性最终还是要回归业务架构本身。业务系统需要根据业务特性选择最优的可用性方案，并在系统架构中遵循一些原则，如最大限度减少关键依赖；幂等性等可重试设计；消除扩容瓶颈；预防和缓解流量峰值；过载时做好优雅降级等等。而更重要的一点是，我们需要时刻思考架构如何支撑业务的长期增长。



原文作者：brightnfeng，腾讯 QQ 音乐后台开发工程师

原文链接：https://mp.weixin.qq.com/s/G00cwGYAr6l2Px6-DiwXLA



# 【NO.357】QQ 浏览器搜索相关性实践



### 1.前言

搜索相关性主要指衡量Query和Doc的匹配程度，是信息检索的核心基础任务之一，也是商业搜索引擎的体验优劣最朴素的评价维度之一。本文主要介绍QQ浏览器搜索相关性团队，在相关性系统、算法方面的实践经历，特别是在QQ浏览器·搜索、搜狗搜索两个大型系统融合过程中，在系统融合、算法融合、算法突破方面的一些实践经验，希望对搜索算法、以及相关领域内的同学有所帮助及启发。

### 2.业务介绍

搜索业务是QQ浏览器的核心功能之一，每天服务于亿万网民的查询检索，为用户提供信息查询服务，区别于一些垂直领域的站内搜索，从索引规模、索引丰富度来看，QQ浏览器的搜索业务可以定位成综合型的全网搜索引擎。具体来说，检索结果的类型，不仅包含传统Web网页、Web图片，也包含新型富媒体形态，例如小程序、微信公众号文章、视频四宫格卡片、智能问答等移动互联网生态下的新型富媒体资源。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revgdZwMCBslNDuWDN8E9KGM757yu0Mep0CtBH3PjD9Aic297wuayaicuibVQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

从相关性的视角看，QQ浏览器的业务场景，既包含传统综合搜索引擎的基本特点，即承接不同群体、不同兴趣、不同地域的海量用户的查询Query。从需求角度来看，QQ浏览器的搜索业务有着大量的用户主动查询，其需求种类、表达形式、结果偏好，存在非常大的差异性，对系统的检索、Query理解、相关性判别有着巨大的挑战；同时，从资源类型角度看，依托集团自有的生态优势，QQ浏览器的搜索场景包含海量的新形态的内容搜索，例如微信公众号文章、企鹅号图文、企鹅号视频，这些资源与传统网页在内容表述、内容形式上与传统网页有着较大的区别，也对相关性算法提出了新的要求。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revgI2fWxDtXSEvbdmhpVF5Er7c8GUMR0vfwjFmQ0nB9TI5Jb9FhsUAPxw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 3.搜索相关性介绍

#### **3.1 搜索主体框架**

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revg2tfXVweHhArOYX9BAlficQzl1Hxv2mKsEXIuRTxIJjxI0rSfjTmwGiaA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

在介绍相关性实践前，首先介绍下系统当前的现状，我们于2021年完成了看点、搜狗两套系统的系统级融合，经过不断地思考、讨论、推演、演化后，整体系统的整体最终演化为如图所示的样子（示意图）。在整个系统融合的过程中，整个团队进行了充分的人员、技术融合，同时也进行了相当长时间的系统改造。系统从逻辑上分为了两大搜索子系统，即主搜子系统和通用垂搜子系统，分别由搜狗系统、看点系统演化而来，同时在系统顶层将两个子系统结果进行进一步融合排序，最终输出检索结果。具体来说分位，分位三个逻辑层次：

● **融合系统**：对自然结果、垂搜特型结果（卡片）进行整页异构排序，包含点击预估、异构多目标排序等阶段，同时也会进行一些业务顶层的轻量重排序或微调。

● **通用垂搜子系统**：垂搜检索系统由看点搜索系统演化而来，主要用于对接入对高速迭代、快速部署有很高要求，与通用检索逻辑有较大差别的业务。整体系统的特点是部署便捷、快速，这套系统从设计之初就充分考虑了多业务快速接入的场景，目前承接的主要是特型形态的结果。

● **主搜子系统**：对十亿级规模的索引库中，对用户的Query进行检索，一般会经历召回、精排两个重要阶段。主要的Doc形态是传统Web网页、Web图片、H5形态网页等，这套系统的特点为，业务形态、效果相对稳定、持续，问题类型有相对的共性，适合算法处于稳定器的业务，主要的难点在于满足用户的中长尾需求。

#### **3.2 算法架构**

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revgCj3DYXl2H6oraMVjKHU1ib5fQyeQOVXLRLWXPW697b1mjoniaBM4oXBw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

搜索算法的计算流程，大致可以分为召回和排序两大逻辑部分。从算法处理的Doc规模来看，在工业界的一般算法架构，都是类似金字塔型的漏斗结构（QQ浏览器目前的主搜子系统、垂搜子系统，虽然定位不同，但都遵照了上述模式）：单个Query会从海量的索引中，检索出一个初始Doc集合，然后经过系统的几个重要的Ranking阶段，逐步对上一个阶段的Doc集合进行筛选，最终筛序出系统认为最好的N条结果。具体来说，如图所以可以分为：

● **召回层**：包含文本检索和向量检索两部分，文本检索会按照Query的核心词进行语法树构建，由倒排系统进行Doc归并、截断产出文本召回集合。向量检索部分利用深度模型将Query、Doc映射到隐空间，在线利用向量检索引擎召回与Query相似的N条结果，相比倒排检索能够充分利用PLM对Query和Doc的表示进行学习，实现近似一段式检索，相比传统的召回+粗排的二段式检索有更好的效果。

● **粗排层**：粗排层使用计算复杂度相对低的方式进行特征捕捉，基本上分位三类：第一类为相关性类特征，文本相关性、语义相关性，其中语义相关性受限于这个位置的算力，主要采用双塔结构，将Query、Doc表示为向量，用点积或者半交互得到。第二类为Query、Doc的静态特征，例如Query的一些长度、频次、Doc质量、Doc发布时间等。第三类特征为统计类特征，例如历史窗口下的用户行为数据。

● **精排层**：对粗排层输入的Doc集合进行更精细化的区分，按照搜索多目标来，精排层要对Doc以下几个维度进行综合判断，例如相关性、时效性、质量权威性、点击预估等几个维度进行综合考量。

**相关性计算的位置**：按照上述介绍的算法架构，QQ浏览器的搜索相关性计算主要分为粗排相关性、精排相关性两部分，其中粗排相关性用于在万级别->百级别这个筛选阶段，算法大部分使用基于倒排的文本匹配特征，同时加上双塔结构的语义特征，在计算复杂度相比精排更轻量；精排相关性，主要用于百级别->个级别的筛选，算法相比粗排，利用了Doc的正排数据，建模方式更精细和计算复杂度也相对更高，本文在算法实践方面，会偏向于介绍团队在精算阶段的经验。

#### **3.3 评估体系**

搜索相关性的评估，主要分为离线和在线评估。离线评估主要看重PNR以及DCG的指标变化，在线评估上主要看重interleaving实验以及人工的GSB评估。下面将详介绍几种评估指标的计算方式：

● PNR：Positive-Negative Ratio是一种pairwise的评估手段，用来评估搜索相关性效果。它的物理含义是在一个排序列表中的结果按照query划分，对每个query下的结果进行两两组pair，计算正序pair的数量/逆序pair的数量。值越大说明整个排序列表中正序的比例越多

● DCG：Discounted Cumulative Gain是一种listwise的评估手段。它的物理含义是整个排序相关性，并且越靠前的item收益越高。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revgOzB7z8Q7hEc8hficCNNibNBBias4RoUJlZDnhpVfUOVpr8EZso6cVLQaQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

其中r(i)代表相关性label。一般而言K选择1或者3.

● interleaving：Interleaving是一种在线评估用户点击偏好的实验。它是将两个排序列表的结果交织在一起曝光给用户，并记录用户最总的点击偏好。整体的感知增益计算逻辑:

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revgGNDsAmZChRSCfOic7VxwNEhdDBxfdZjOEwWQ6wyOlHicFmdNiaVnR3PPQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

其中 wins代表用户最总点击了A列表结果，ties代表持平，loss则代表落败。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revgH5K8S3kOtIqQMHIIwBYVs6Z4vTBrGfv0H51rbMtJLhvlEQwROViaLuA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

则代表感知增益胜出，反之则代表落败。

● GSB：Good vs Same vs Bad 是一种采用专家评估的手段。标注专家会对左右两边的排序列表进行评估，一边是来自基线线上，一边是来自试验组线上。对于标注专家而言，他不清楚那边的结果是试验组产生的，然后对这两个排序列表进行打分，Good or Same or Bad。最后统计统计整体的GSB指标：(Good-Bad)/(Good + Same +Bad)



### 4.相关性精算的系统演进

搜狗搜索作为一款历经迭代18年的搜索产品，在数据积累、技术打磨、系统成熟度方面有很强的先天优势；QQ浏览器·搜索是搜索行业较为年轻的新人，在架构选型、技术代际、历史债务方面有很强的后发优势。为了兼顾两家之长，在系统融合的过程中，团队的首要目标就是充分融合两套系统的特有优势。以相关性视角来看，我们大致经历了以下几个改造时期

#### 4.1 1.0时代，群雄割据 -> 三国争霸

从相关性的视角看，面临最大的难题是两套系统相关性得分不可比的问题。具体来说：

● **标准差异**：两套系统的相关性判定标准、标注方法不同，从根本上不可比。

● **建模差异**：两个系统对于多目标（相关性、时效性、点击、权威性）的建模方式存在较大差异：主搜系统以End-To-End思路解决搜索多目标的问题，具体来说使用GBDT作为融合模型，所有子特征一并送入融合模型，我们后继称之为「大一统」模型。垂搜系统对多目标进行了进一步的拆解，尽量将同一个维度的特征系列汇聚形成高级特征，以相关性为例，垂搜的会存在一个单独的基础相关性精算阶段，输出相关性高级特征，再将高级特征替换所有的子特征的方式进入融合排序，我们后继称之为「抽象高级特征」。

**对比思考**：从系统设计上看，「大一统」VS「抽象高级特征」，是两种完全不同的思路，前者更符合机器学习的理念，暴露更多的子特征细节能够提供更多的信息；后者的思路，对目标进行了高度抽象，具有更好的可解释性。从表面看似乎没有明显的优劣可言，但从工业实践经验看，这里还是有较强的实践结论的。

下面揭晓一下结论，从工业系统设计的角度看，更倾向于「抽象高级特征」这种方案，而非「大一统」的方式。理由有以下几点：

● 可解释性：工业算法系统的首要考虑就是如何支撑算法持续、高效迭代。在多目标导向下，「大一统」方式下子特征规模已经达到了100维以上，逆序的问题归因相比「高级特征」来讲，归因难度大、问题会更分散，这个模式也间接鼓励算法同学去新增能够带来指标提升的新特征，而不是去迭代已有的特征。

● 业务需求：「大一统」方式下，一旦脱离该阶段的多目标排序后，后继的更High-Level的融合场景即失去判断相关性的载体，无法对相关性维度进行比较。更High-Level的融合不得不将必要的子特征继续向上传递，往往看到某些子特征从最底层一路透传到最顶层，对子特征的可比性、覆盖率、迭代维护成本都要很大的要求

● 特征管理：High-Level的业务同学大量使用子特征也会造成管理混乱，一旦某些子特征在后继的业务中使用，该特征迭代就与其在后继业务中的形成了耦合，例如比较常见的通过某个特征MagicNumber进行过滤，很有可能的情况是，特征迭代时也要去调整该MagicNumber。所以，以相关性为例，使用具有物理含义的统一「高级特征」会大大减少子特征的管理问题。

**改进方式：**简单来说，我们在垂搜子系统、主搜系统按照同样的设计思路，抽象了一个基础相关性计算阶段，这个阶段的目标是单目标的相关性，即不考察Doc的质量、时效性等。这一阶段会接管所有刻化相关性目标的特征，通过相关性模型，输出相关性高级特征。同时，相关性高级特征，会经过Probility Calibration算法将score转化为是否相关的概率（对齐标准、档位，跨系统可比），同时具有较好的分布稳定性、跨Query可比性，即具有物理含义的相关性得分。应用视角上看，分位两部分，即交给融合排序模型，替换一批刻化相关性的子特征，另外一部分是直接用于High-Level的场景，例如某些业务会将相关性大于某个阈值的Doc进行过滤或者提权。

**演进总结：**

● 标准对齐：主要的业务场景，对齐了相关性标准，特别是每个档位物理含义。

● 具有物理含义的相关性得分：对相关性特征进行归纳和融合，通过Probility Calibration算法对得分进行相关概率校准，在ranking任务能力尚可的情况下，能够保证跨Query、跨业务可比，同时从特征管理的角度看，也从特征割据的时代进入了三足鼎立的时代。

#### 4.2 2.0时代，统一复用

1.0阶段我们通过校准算法、相关性标准统一，输出了具有一定的物理含义相关性得分，可以基本做到子特征保持差异的情况下，基本实现跨业务可比的问题。此时，虽然校准可以解决系统内部的实现上的差异问题，但团队面临更核心问题是系统的近一步融合问题，具体来说：

**算法融合：**如果说「大一统」「高级特征」两种模式的统一是系统级方法论的对齐，那么「**相关性算法融合**」角度，则需要近一步将执行细节对齐。如何最大化算法能力，兼两家之长，是最基本的融合初衷。

**人效问题：**系统细节的差异，算法角度看，在内部的模型、特征体系、数据结构、代码库，全部是完全不同的。维护两套大型复杂系统，分别投入则必须要面对人力折半的问题，背后的压力是可想而知的。

在上述背景下，22年重新对两套系统进行了整合，力图用统一的一套相关性服务，服务于主搜索系统和垂搜系统。这里介绍下我们其中一项重要的重构，重新设计构建了相关性精算服务，统一了主搜系统和垂搜系统的相关性能力，做到90%代码级别的复用。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revgVsSibicId9eFwf3RXIQPkicbXEkjvKWn07lfDOEe2XPMibUDd80oLawr7Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**相关性精算服务：**新的相关性精算服务，定位于精算旁路系统，为搜索精排阶段提供高级相关性得分，服务内部可以高速并行获取Doc正排，进行精细化的相关性特征计算、GPU计算、模型预测等。算法统一，一套代码，90%的特征属于通用基础匹配，10%特征根据场景差异，对该业务的独有问题进行独立刻化。具体来看，新的服务相比之前提供的能力包括：

**调研实验效率：**新的相关性精算服务，调研实验周期由周级下降为天级，背后的效率提升，主要是由于模块位置带来的调研环境搭建成本上的区别。在以前的系统，相关性大部分非GPU类的特征，均在召回层实现，这样带来的问题是，由于召回层的架构大部分都是分布式系统，调研成本相比精算模块需要更多的机器成本，这也造成了该阶段的调研需要团队共用1-2套调研环境，调研&实验成本将会大大增加。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revgT3fV1aqeaMCgZTStwlsXC4wwAdPhH66BiaoSdojdibzsLwvoOI6ZXwOg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**算力能力：**相关性分布式计算，最重要的贡献是能够让系统的计算条数变的更多，这种思路在GPU并行技术出现以前是非常有效的设计，将相关性计算放到召回层不仅能够最大限度的利用分布式架构，同时也节省了Doc正排在HighLevel获取的存储和带宽，这部分正排数据往往是召回层必须的可以兼顾复用。但最近几年随着深度学习、GPU并行加速技术在搜索系统重越来越多的应用，业务越来越需要重型计算，这样的重型计算是召回层的算力远远无法满足的，召回层的相关性计算只有基于倒排的特征，更关心是否命中、命中距离，缺少对未命中词与query的关系刻化。

**算法独立性：**相比之前最大的区别是，新的相关性精算服务，与召回层解耦。从基础数据结构、特包括Query信息、Doc正排，进行重构对齐，传导至特征设计、实现，也能够相应的进行统一。最终做到算法统一，一套代码，90%的特征属于通用基础匹配，10%特征根据场景差异，对该业务的独有问题进行独立刻化。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revgKu5Gl9x26iaZsNsiaNs7YAIgwgflUiciaZYxgzwia9l0zgXJV0ibXD1iaSic3A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



### 5.搜索相关性技术实践

#### **5.1 相关性标准**

QQ浏览器搜索下的相关性标准，主要用于基础相关性样本的标注，为了能精细化的表达是否相关这一概率，我们将相关、不相关这个二分类任务，拓展到了五档分类，能够提供更多的监督信息。同时，每一档的物理含义，在不同的业务下，尽量保持对等。例如，搜用搜索场景、视频搜索场景下，同一档位的Doc需要具有对等的相关程度，即应具备同一等级的相关性。这样做的好处是，在High-Level场景下，当分类能力尚可的情况下，通过Probility Calibration可以对不同的业务下的doc进行得分的比较，但仍可以对相关性内部特征的实现保留一定的差异性，对系统非常友好。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revg3mEKubuliaBev3p7BcEAOwpZqRPGg3Cyx1gucIr9EPK9s6MGRIic6ibiaQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### **5.2 相关性的技术架构**

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revg6v6z0dXxDTJCoBGibsL8qbwRUZpDBx2zzudWocderYGXRAWj918jdLw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### **5.3 深度语义匹配实践**

##### 5.3.1 QQ浏览器搜索相关性的困难与挑战

QQ浏览器的搜索业务每天服务于亿万网民的查询检索，因为业务场景偏向于综合搜索业务，每天的用户的查询表达都呈现海量量级，在这个场景下的用户Query天然的具备很强的长尾效应，对搜索相关性的匹配能力提出了巨大的挑战。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revghKAqqWmqsic7m7fghuHD68S6eXTInMBGSUgFXPg0vvua2sY4BWU1wmA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### 5.3.2 深度语义的现状

为了解决一词多义等模糊表达的问题，QQ浏览器的搜索相关性场景，进行了大量的语义匹配工作实践。随着深度学习技术的兴起，基于预训练语言模型的方法，特别是基于BERT模型的语义匹配，目前是我们工作的主要研究方向。当前系统按照表达方式来看，主要包括基于表示的匹配方法（Representation-based）和基于交互的匹配方法（Interaction-based）。

**基于表示的匹配方法**：使用深度模型分别学习Query和Doc的Embbeding，在线通过cosine计算Query和Doc相似度来作为语义匹配分数。计算框架上，借鉴百度的SimNet双塔结构，由于在线计算相对交互式模型更友好，目前普遍应用于粗排语义相关性的计算。

**基于交互的匹配方法**：将Query和Doc（Title）拼接后输入给BERT模型，经过N层Transformer Block后，将CLS Token的Embbeding接入下游相关性任务，由于交互式普遍需要比较高的计算复杂度，一般用于QQ浏览器的精排阶段。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatfjoObmZQiajFO0urX4revgIj99I770GsNGU42a8vNIzy5RGwQlyJ2iagibXWSPsAHThkX3qs4b1abg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

##### 5.3.3 QQ浏览器搜索相关性深度语义实践

###### 5.3.3.1 相关性Ranking Loss

目前我们的相关性标注标准共分为五个档位，最直接的建模方式，其实是进行N=5的N分类任务，即使用Pointwise的方式建模。搜索场景下，我们其实并不关心分类能力的好坏，而更关心不同样本之前的偏序关系，例如对于同一个Query的两个相关结果DocA和DocB，Pointwise模型只能判断出两者都与Query相关，无法区分DocA和DocB相关性程度。因此搜索领域的任务，更多更广泛的建模思路是将其视为一个文档排序场景，广泛使用Leaning To Rank思想进行业务场景建模。

Pairwise 方法通过考虑两两文档之间的相关对顺序来进行排序，相比 Pointwise 方法有明显改善，因此我们对BERT模型的Fine-tuning任务，也进行了RankingLoss的针对性改进。Pairwise Loss下的训练框架，任务输入的单条样本为三元组的形式，在多档标注下，我们对于同一Query的多个候选Doc，选择任意一个高档位Doc和一个低档位Doc组合成三元组作为输入样本。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvatfjoObmZQiajFO0urX4revghFBVDrPErqbynqDvuq6KicHgo1icgAphBH0EOUwF8n9evbpJ0XfxVB8g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

###### **5.3.3.2 深度语义特征的校准问题**

**Ranking Loss的问题：**相关性是搜索排序的基础能力，在整个计算流程的视角看，相关性计算不是最后一个阶段，所以当相关性内部子特征的目标如果直接使用RankingLoss，要特别注意与上下游的配合应用，特别要关注单特征的RankingLoss持续减少，是否与整体任务的提升一致。同时，RankLoss由于不具有全局的物理含义，即不同Query下的DocA和DocB的得分是不具有可比性，这直接导致了其作为特征值应用到下游模型时，如果我们使用例如决策树这种基于全局分裂增益来划分阈值的模型，会有一定的损失。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatfjoObmZQiajFO0urX4revgg8Hb8IEvxUUlksA8zZSyhSkZuN4YCdEpfXCGctXMUqicmANRTbENqrg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

搜索系统，一般为了追求可解释性，往往会将高级特征通过一些解释性较强的模型进行融合。以相关性高级特征的产出过程为例，我们在产出整体的相关性得分时，会使用例如XGB模型对相关性N维子特征进行最终的打分预测，如果此时放大这个打分过程，即当训练好的决策树进行最终模型预测时，当执行到某一个决策树时，会按照特征分裂值判断走左子树还是右子树，这个分裂值就要求该特征在全部Query下都按照此分裂点判断，这里如果当前的特征值域在不同Query下差异很大，在个别Query下的打分准确率一定会大打折扣。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatfjoObmZQiajFO0urX4revgHmFygGrdKWKwfmWyTIhMDIkwrhEzSMSWH0qXOOtfOdRyhDIfgY4SyA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

实践中我们对语义特征的ranking loss，也同时进行了一部分pointwise loss结合，目的是希望单特征得分的分布尽量在全局有一定的可比性，即对其进行一定Calibration能够帮助相关性模型整体的PNR提升。由图所示，当单特征持续以PairwiseLoss训练，随着训练步数的增加，单特征PNR是持续上升的，但其放入相关性模型后，整体的PNR并不是线性上升的，此时观察单特征ECE（Expected Calibration Error 期望标定误差）有较大波动；如果将单特征变成Pairwise+PointwiseLoss，发现随着训练过程的进行，模型ECE持续下降，单特征PNR微弱上升，且相关性整体的PNR能够上升，且最终高于单纯使用Pairwise的方式。

###### 5.3.3.3 领域自适应

最近几年的NLP领域，预训练方向可以称得上AI方向的掌上明珠，从模型的参数规模、预训练的方法、多语言多模态等几个方向持续发展，不断地刷新着领域Benchmark。预训练通过自监督学习，从大规模数据中获得与具体任务无关的预训练模型。那么，在搜索领域下，如何将预训练语言模型，与搜索语料更好的结合，是我们团队一直在探索的方向。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatfjoObmZQiajFO0urX4revg9xQ8GQHk899LiblzTic39fPaFFziakjsGZDTZChxQh8C30LQ8I6PgmgsA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatfjoObmZQiajFO0urX4revgf6qKcibtM2qC8dEbIDA0NiaZXs4X6W8Msahxb9YfsaU4pLiapzfCCLMpg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

在实践过程中，我们发现通用预训练的语料，与搜索场景的任务，依然存在不小的gap，所以一个比较朴素的思想是，是否可以将搜索领域的自有数据进行预训练任务。在实际的实验中，我们发现将搜索领域的语料，在基础预训练模型后，继续进行post-pretrain，能够有效的提升业务效果，对下游任务的提升，最大可以大致9%。

#### **5.4 相关性语义匹配增强实践**

##### 5.4.1 深度语义匹配的鲁棒性问题

在NLP领域，预训练语言模型（Pretrained Language Model）已经在很多任务上取得了显著的成绩，PLM搭配领域Finetune也同时在工业界成为解决搜索、推荐等领域的标准范式。在搜索相关性业务中，行业内在2019年开始，就已将神经网络模型全面转为基于Transformer结构的模型结构上来。区别于传统的字面匹配，语言模型能够有效解决Term模糊匹配的问题，但大力出奇迹的同时，也引入了很多核心词缺失等问题。例如，基于预训练语言模型，“二手车”和“二手摩托车”会判定为比较匹配，但实际上二者明显不同。如何解决此类鲁棒性问题，是预训练语言模型下的语义匹配要解决的核心问题。

##### 5.4.2 什么是相关性匹配（RelevanceMatching）

搜索业务下的核心词缺失问题，我们认为传统的预训练方向并不能提供一个统一的解决方案，因为该问题属于搜索领域的特型问题，我们在实际工作中发现，搜索场景下很多形态的问题，与NLP的SemanticMatching任务的差异还是比较明显的，例如短Query和长Title的匹配。对此，我们更倾向于通过对特型问题独立建模和处理，为了强化搜索相关性的鲁棒性，提出了Relevance Matching的概念和对应的建模方式，二者的区别，具体来说：

**Relevance Matching**：注重关键词的精确匹配，相应的需要考虑核心词的识别、多种维度的要求。(一般需要关注query的重要性以及提取匹配信号，同时形态上Q比较短)

**Semantic Matching**：注重Term间的相似关系，建模Term、Phrase、Sentence间的相似关系。(偏向query，title表达是不是相似，同时认为query和title的重要性一样)

```
● 相似度匹配信号  Similarity matching signals：和准确的单词匹配相比，捕获单词、短语和句子的语义相关性/相似性更重要。

● 语义结构  Compositional meanings：语义匹配的文本通过是具有一定语法结构的，使用语义结构的含义会非常有效。

● 全局匹配  Global matching requirement：语义匹配通常将文本的两个片段作为一个整体来推理它们之间的语义关系。
```

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatfjoObmZQiajFO0urX4revgL7XM7SjFUzShCDdS3kGa9kWA3cTB7u8ibnTtEuw3xV43c8sFXY92wQA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

##### 5.4.3 相关性匹配的相关工作

**早期的做法**：行业内其实很早就有提出Relevance Matching的概念，在Transformer结构以前的主要工作，大多通过对Query和Doc的文本建立匹配矩阵，矩阵中的每一个元素是对应位置的Term相似度，然后再通过对匹配矩阵的命中Pattern进行提取，具体来说：

```
● MatchPyramid（中科院 2016 AAAI），构建了基于字面匹配或Embedding匹配，构建query-document匹配矩阵，命中提取使用CNN + Dynamic Pooling + MLP完成。

● DRMM (2016 中科院 CIKM)，提出了一个交互得模型结构，Query中的每一个Term分别与Doc中的所有的Term交互，将相似度离散到直方图上，通过MLP，以及Q中的Term Gating Network产出得分；其中Term Gating尝试了两种方式，分别是单层FeedForward+softmax和无监督的IDF，实验效果是后者更好。由于Embedding是直接使用的300d word2vec，因此参数量非常小 —— Matching部分有155个参数，Term Gating部分有300个参数。

● K-NRM (2017 SIGIR) ，主要贡献在于提出了RBF Kernel的Pooling方式，与前作最大的不同是，Embedding使用随机初始化并端到端训练的方式，总参数量达到了约5000w（绝大部分来自Embedding层）实验效果显著优于DRMM，其中端到端训练Embedding带来了最大幅度的提升，Kernel Pooling相比基线的pooling方式能带来小幅提升。

● PACRR (2017 EMNLP)，主要创新点：在对每一个query term完成pooling后，使用LSTM建模整体的query coverage。LSTM每个timestep的输入是concat（pooling后的query term representation，normalized_IDF）。LSTM的输出维度是1，LSTM的输出直接作为最终的score。
```

**Bert以后的做法：**大部分从预训练语言模型的角度，在MASK机制、外部知识引入、参数规模等角度进行研究，也取得了显著的效果提升。但在搜索相关性业务上，大部分交互式的应用方式，是将Query和Title完全拼接后输入Bert，最后在输出层基于CLS这个特殊Token的Embbeding做领域任务。目前我们了解到的是，除了CEDR这个工作外，很少有直接使用非CLS以外的Token的模型架构。这里可能对Transformer比较熟悉的同学会觉得，每一个Transformer Block内部架构其实会天然的对两两Term进行Attention计算，形成多头AttentionMap，与Relevance Matching中的Matrix的设计思路几乎一致，是否还有必要继续再手动进行一次MatrixMatching的计算。对此我们在22年通过一系列实践，证明Relevance Matching的重要意义。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatfjoObmZQiajFO0urX4revgDhyvKEVk82hOaV4E7KjefNlcAkt7hJSAK8LPJGlRNnMXicxmxu7xb3g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

##### 5.4.4 相关性匹配增强

为了兼顾SemanticMatching和RelevanceMatching两者的能力，我们提出了HybridMratrixMatching（HMM）模型，提升模型在核心成分精确匹配和语义泛化匹配两方面的综合能力。具体优化点为：

```
● Query-Title匹配矩阵建模：
```

**隐式匹配矩阵构造**：基于BERT产出的最后一层的token embedding，通过dense + cosine similarity的方式构造Q-T语义匹配矩阵。

**显式文本匹配矩阵构造**：基于query与title分词后的词粒度命中信息，构造Q-T精确匹配矩阵，并进一步打平映射到与BERT输入信息相同的token粒度。

```
● 语义匹配与文本匹配信息融合：
```

**CNN汇聚两种匹配矩阵信息**：在模型输出层，对隐式和显式匹配矩阵拼接产出N个|Q|x|T|匹配矩阵 ，通过3D-CNN + Weighted Sum Pooling的方式来捕捉语义匹配和Term显式匹配相结合的命中pattern，产出匹配矩阵特征向量。

**最终得分融合**：将匹配矩阵侧产出的特征向量与BERT CLS特征向量拼接，融合产出最终的模型得分。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatfjoObmZQiajFO0urX4revgCSg9GQQF4AOQU8KJtia7pDpSftV6V9OtZA5Jia0GrmJUDjcJpeAO7Luw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

##### 5.4.5 实验&效果

为了能够验证Hybrid MratrixMatching（HMM）模型在搜索场景下的匹配能力，我们对模型进行了离线和在线两方面的效果验证。

A、离线实验：

我们对新模型进行了消融实验分析，其中几个比较重要的实验结论为：（1）隐式MatchingMatrix结构，单独进行下游任务预测时，测试集的PNR、NDCG等指标几乎与只用CLS进行下游任务相同 (2)隐式Matrix+CNN后与CLS拼接融合后，整体去做相关性任务，在PNR、NDCG指标上看，相对只用CLS进行下游任务，相对提升大约1.8%  (3)外部Matrix的引入，包括多层显示匹配矩阵，能够继续为HMM模型整体的提升带来2.3%的提升。外部匹配Matrix带来的额外信息能够带来效果提升，也证明了精确匹配能力在搜索这个任务中的考核占比是比较高的，将外部精确匹配信号的引入，能够帮助模型强化这部分能力。

B、在线实验：

HMM模型目前已在搜索相关性场景下全量部署，实验期间我们通过ABTest系统和Interleaving系统对实验组效果进行观察，其中Interleaving感知相关性指标在实验期间显著正向，这也与模型升级对精确匹配、核心词命中能力提升等预期比较吻合。同时，我们每次项目实验评估，需要将实验效果送第三方评估团队进行SideBySide评估，由专家标注员对实验组和对照组进行Good、Same、Bad打分，最终随机Query下的送评结果显示，有比较显著的变好趋势。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvatfjoObmZQiajFO0urX4revgdSttjHRgYxhwyXaIknuVf4f6Z838F5POoP7JahXYzDgZ4XCgcNarZQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 6.小结与思考

搜索相关性方向，是一个充满了技术挑战的硬核方向，无数网民的检索需求、五花八门的查询表达、越来越新颖的内容模态，全部对系统的效果提出了极其艰巨的挑战。目前QQ浏览器搜索相关性团队的小伙伴们，在搜狗并入腾讯的大背景下，逐步将两套系统的优势合并，完成大量的技术重构、技术债务清理，逐步形成了一个高可用、高性能的业界头部大型搜索系统。接下来，我们将继续在搜索相关性领域持续投入，结合工业界、学术界在NLP领域、AI领域等最前沿的技术突破，为提升业务效果不断努力。

原文作者：jesangliu，腾讯 PCG 应用研究员

原文链接：https://mp.weixin.qq.com/s/-OS4QwIqwThAk6HopH_YQg

# 【NO.358】ClickHouse 查询优化详细介绍

> 你想要的 ClickHouse 优化，都在这里。

ClickHouse 是 OLAP（Online analytical processing）数据库，以速度见长[[1\]](https://km.woa.com/group/571/articles/show/527756#fn:1)。ClickHouse 为什么能这么快？有两点原因[[2\]](https://km.woa.com/group/571/articles/show/527756#fn:2)：

- 架构优越

- - 列式存储
  - 索引
  - 数据压缩
  - 向量化执行
  - 资源利用

- 关注底层细节

但是，数据库设计再优越也拯救不了错误的使用方式，本文以 MergeTree 引擎家族为例讲解如何对查询优化。

## 1.ClickHouse 查询执行过程

> ⚠️ 本节基于 ClickHouse 22.3 版本分析

`clickhouser-server`启动后会在 while 循环中等待请求，接收到查询后会调用`executeQueryImpl()`行数构建 AST、优化并生成执行计划 pipeline，最后在`executeImpl()`中多线程执行 DAG 获取结果，这篇文章只关心 SQL 执行，省略掉网络交互部分，查询执行流程如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvattWZDn1dwv0vjYR7hBqEQuzrIiaVYBFRFH3XBGYwEG0KWc8HWOST0OVd7dCHQmPBiasgzbRxE19iaEw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

SQL 的解析优化和编译原理息息相关，本节将包含大量编译原理和代码细节，属扩展知识。

### 1.1 **词法解析和语法解析**

ClickHouse 拿到需要执行的 SQL，首先需要将 String 格式的字符串解析为它能理解的数据结构，也就是 AST 和执行计划。构造 AST 部分代码如下所示：

```
// src/Interpreters/executeQuery.cpp

static std::tuple<ASTPtr, BlockIO> executeQueryImpl()
{
    // 构造Parser
    ParserQuery parser(end, settings.allow_settings_after_format_in_insert);

    // 将SQL转为抽象语法树
    ast = parseQuery(parser, begin, end, "", max_query_size, settings.max_parser_depth);

    // 设置query的上下文，比如SETTINGS
    ...

    if (async_insert)
    {
        ...
    } else {
        // 生成interpreter实例
        interpreter = InterpreterFactory::get(ast, context, SelectQueryOptions(stage).setInternal(internal));

        // interpreter优化AST并返回执行计划
        res = interpreter->execute();
    }

    // 返回抽象语法树和执行计划
    return std::make_tuple(ast, std::move(res));
}
```

值得一提的是，解析 SQL 生成语法树这是编译原理中词法分析和语法分析部分覆盖的事情。词法分析只是简单拆解数据流为一个个 token，而语法分析分为自顶向下和自底向上两种方式，常见的语法分析方式也分为手写语法分析（往往是自顶向下的有限状态机，递归下降分析）和语法分析工具（往往是自底向上，如 Flex、Yacc/Bison 等）。

- 曾经 GCC 使用 yacc/bison 作为语法解析器，在 3.x 某个版本之后改为手写递归下降语法分析[[3\]](https://km.woa.com/group/571/articles/show/527756#fn:3)
- clang 一直是手写递归下降语法分析[[4\]](https://km.woa.com/group/571/articles/show/527756#fn:4)

手写语法分析比起语法分析工具有几个优势（当然要写得好的情况）：

- 性能更好。可以优化热点路径等
- 诊断和错误恢复更清晰明了。手写状态机可以完全掌控系统状态，错误处理更容易
- 简单。不需要掌握新语法

ClickHouse 解析 SQL 的函数如下所示：

```
// src/Parsers/parseQuery.cpp

ASTPtr tryParseQuery()
{
    // 将SQL拆分为token流
    Tokens tokens(query_begin, all_queries_end, max_query_size);
    IParser::Pos token_iterator(tokens, max_parser_depth);

    // 将token流解析为语法树
    ASTPtr res;
    const bool parse_res = parser.parse(token_iterator, res, expected);

    return res;
}
```

可以看到先将 SQL 字符串拆解为 token 流（词法分析），再调用`perser.parse()`函数进行语法分析，它的实现如下：

```
// src/Parsers/ParserQuery.cpp

bool ParserQuery::parseImpl(Pos & pos, ASTPtr & node, Expected & expected)
{
    ParserQueryWithOutput query_with_output_p(end, allow_settings_after_format_in_insert);
    ParserInsertQuery insert_p(end, allow_settings_after_format_in_insert);
    ParserUseQuery use_p;
    ParserSetQuery set_p;
    ParserSystemQuery system_p;
    ParserCreateUserQuery create_user_p;
    ParserCreateRoleQuery create_role_p;
    ParserCreateQuotaQuery create_quota_p;
    ParserCreateRowPolicyQuery create_row_policy_p;
    ParserCreateSettingsProfileQuery create_settings_profile_p;
    ParserCreateFunctionQuery create_function_p;
    ParserDropFunctionQuery drop_function_p;
    ParserDropAccessEntityQuery drop_access_entity_p;
    ParserGrantQuery grant_p;
    ParserSetRoleQuery set_role_p;
    ParserExternalDDLQuery external_ddl_p;
    ParserTransactionControl transaction_control_p;
    ParserBackupQuery backup_p;

    bool res = query_with_output_p.parse(pos, node, expected)
        || insert_p.parse(pos, node, expected)
        || use_p.parse(pos, node, expected)
        || set_role_p.parse(pos, node, expected)
        || set_p.parse(pos, node, expected)
        || system_p.parse(pos, node, expected)
        || create_user_p.parse(pos, node, expected)
        || create_role_p.parse(pos, node, expected)
        || create_quota_p.parse(pos, node, expected)
        || create_row_policy_p.parse(pos, node, expected)
        || create_settings_profile_p.parse(pos, node, expected)
        || create_function_p.parse(pos, node, expected)
        || drop_function_p.parse(pos, node, expected)
        || drop_access_entity_p.parse(pos, node, expected)
        || grant_p.parse(pos, node, expected)
        || external_ddl_p.parse(pos, node, expected)
        || transaction_control_p.parse(pos, node, expected)
        || backup_p.parse(pos, node, expected);

    return res;
}
```

可以发现 ClickHouse 将 Query 分为了 18 种类型（截止 2022-11-12 日），每种 Query 都有自己的 Parser，通过关键词匹配构造 AST 上的节点，最终生成语法树。递归下降部分超纲了，这里就不铺开讲。

### 1.2 **优化器**

经过语法分析后生成的 AST 并不是执行最优解，ClickHouse 包含大量基于规则的优化（rule based optimization），每个 Query 会遍历一遍优化规则，将满足的情况进行**不改变查询语义地重写**。

每一种 Query 类型都有对应的 Interpreter，后文都以 Select 查询举例，代码如下：

```
// src/Interpreters/InterpreterFactory.cpp

std::unique_ptr<IInterpreter> InterpreterFactory::get()
{
    ...
    if (query->as<ASTSelectQuery>())
    {
        return std::make_unique<InterpreterSelectQuery>(query, context, options);
    }
    ...
}
```

在`InterpreterSelectQuery`类的构造函数中将 AST 优化、重写，代码详见`src/Interpreters/InterpreterSelectQuery.cpp`，这里只画流程图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvattWZDn1dwv0vjYR7hBqEQuLLbiaE7MOZibbmUWVFriaBZxe3se8gxsWWWyT3d3a9NZbEZOahH2BPJ3g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

是否初始化 settings 优化 with 优化 joins 谓词下推将 where 下推到 prewhere 是否要再次优化检查 storage 权限生成 analysis_result 和 result_header

### 1.3 **构造执行计划**

`src/Interpreters/InterpreterSelectQuery.cpp`文件`InterpreterSelectQuery::executeImpl()`方法将优化分析得到的中间数据辅助生成最终的执行计划，代码如下：

```
// src/Interpreters/InterpreterSelectQuery.cpp

void InterpreterSelectQuery::executeImpl()
{
    ...
    // 个人理解针对EXPLAIN PLAN，只构建执行计划不执行
    if (options.only_analyze)
    {
        ...
    }
    else
    {
        // 从磁盘读取所需列，注意这一行，后文跳转进去分析
        executeFetchColumns(from_stage, query_plan);
    }
    if (options.to_stage > QueryProcessingStage::FetchColumns)
    {
        // 在分布式执行Query时只在远程节点执行
        if (expressions.first_stage)
        {
            // 当storage不支持prewhere时添加FilterStep
            if (!query_info.projection && expressions.filter_info)
            {
                ...
            }
            if (expressions.before_array_join)
            {
                ...
            }
            if (expressions.array_join)
            {
                ...
            }
            if (expressions.before_join)
            {
                ...
            }
            // 可选步骤：将join key转为一致的supertype
            if (expressions.converting_join_columns)
            {
                ...
            }
            // 添加Join
            if (expressions.hasJoin())
            {
                ...
            }
            // 添加where
            if (!query_info.projection && expressions.hasWhere())
                executeWhere(query_plan, expressions.before_where, expressions.remove_where_filter);
            // 添加aggregation
            if (expressions.need_aggregate)
            {
                executeAggregation(
                    query_plan, expressions.before_aggregation, aggregate_overflow_row, aggregate_final, query_info.input_order_info);
                /// We need to reset input order info, so that executeOrder can't use it
                query_info.input_order_info.reset();
                if (query_info.projection)
                    query_info.projection->input_order_info.reset();
            }
            // 准备执行：
            // 1. before windows函数
            // 2. windows函数
            // 3. after windows函数
            // 4. 准备DISTINCT
            if (expressions.need_aggregate)
            {
                // 存在聚合函数，在windows函数/ORDER BY之前不执行
            }
            else
            {
                // 不存在聚合函数
                // 存在windows函数，应该在初始节点运行
                // 并且，ORDER BY和DISTINCT依赖于windows函数，这里也不能运行
                if (query_analyzer->hasWindow())
                {
                    executeExpression(query_plan, expressions.before_window, "Before window functions");
                }
                else
                {
                    // 没有windows函数，执行before ORDER BY、准备DISTINCT
                    assert(!expressions.before_window);
                    executeExpression(query_plan, expressions.before_order_by, "Before ORDER BY");
                    executeDistinct(query_plan, true, expressions.selected_columns, true);
                }
            }
            // 如果查询没有GROUP、HAVING，有ORDER或LIMIT，会在远程排序、LIMIT
            preliminary_sort();
        }
        // 在分布式执行Query时只在初始节点执行或optimize_distributed_group_by_sharding_key开启时
        if (expressions.second_stage || from_aggregation_stage)
        {
             if (from_aggregation_stage)
            {
                // 远程节点聚合过，这里啥也不干
            }
            else if (expressions.need_aggregate)
            {
                // 从不同节点拉取数据合并
                if (!expressions.first_stage)
                    executeMergeAggregated(query_plan, aggregate_overflow_row, aggregate_final);

                if (!aggregate_final)
                {
                    // 执行group by with totals/rollup/cube
                    ...
                }
                // 添加Having
                else if (expressions.hasHaving())
                    executeHaving(query_plan, expressions.before_having, expressions.remove_having_filter);
            }
            // 报个错
            else if (query.group_by_with_totals || query.group_by_with_rollup || query.group_by_with_cube)
                throw Exception("WITH TOTALS, ROLLUP or CUBE are not supported without aggregation", ErrorCodes::NOT_IMPLEMENTED);
            // 准备执行：
            // 1. before windows函数
            // 2. windows函数
            // 3. after windows函数
            // 4. 准备DISTINCT
            if (from_aggregation_stage)
            {
                if (query_analyzer->hasWindow())
                    throw Exception(
                        "Window functions does not support processing from WithMergeableStateAfterAggregation",
                        ErrorCodes::NOT_IMPLEMENTED);
            }
            else if (expressions.need_aggregate)
            {
                executeExpression(query_plan, expressions.before_window,
                    "Before window functions");
                executeWindow(query_plan);
                executeExpression(query_plan, expressions.before_order_by, "Before ORDER BY");
                executeDistinct(query_plan, true, expressions.selected_columns, true);
            }
            else
            {
                if (query_analyzer->hasWindow())
                {
                    executeWindow(query_plan);
                    executeExpression(query_plan, expressions.before_order_by, "Before ORDER BY");
                    executeDistinct(query_plan, true, expressions.selected_columns, true);
                }
                else
                {
                    // Neither aggregation nor windows, all expressions before
                    // ORDER BY executed on shards.
                }
            }
            // 添加order by
            if (expressions.has_order_by)
            {
                // 在分布式查询中，没有聚合函数却有order by，将会在远端节点order by
                ...
            }
            // 多source order by优化
            ...

            // 多条流时再次执行distinct
            if (!from_aggregation_stage && query.distinct)
                executeDistinct(query_plan, false, expressions.selected_columns, false);

            // 处理limit
            ...
            // 处理projection
            ...
            // 处理offset
            ...
        }

        // 需要子查询结果构建set
        if (!subqueries_for_sets.empty())
            executeSubqueriesInSetsAndJoins(query_plan, subqueries_for_sets);
    }
}
```

其中`InterpreterSelectQuery::executeFetchColumns()`函数是读取所需列的阶段。从代码中可以看到它也做了很多的优化：

- `count()`优化
- 只有 LIMIT 情况的优化
- `quota`限制

可以看到：

1. limit 大部分情况下是计算完成后再执行，而 quota 是在读取数据时执行的
2. 加速的关键是减少读入的数据量，也就是说善用索引
3. 用`count()`、`count(1)`和`count(*)`，ClickHouse 都有优化，但不要`count(any_field)`

## 2.索引设计

索引是 ClickHouse 快速查询最重要的一环，分为主键索引（sparse indexes）和跳表索引（data skipping indexes）。在执行查询时，索引命中顺序如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvattWZDn1dwv0vjYR7hBqEQudEOiaIKzbF5BOgX83SfChLY7MnNfXQZs4INKBWues9VUjZSvVMmEZGg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

Partition Key MinMax IndexPartitionPrimary Key Sparse IndexData Skipping Indexes

详见代码：

```
// src/Processors/QueryPlan/ReadFromMergeTree.cpp

MergeTreeDataSelectAnalysisResultPtr ReadFromMergeTree::selectRangesToRead()
{
    ...
    try
    {
        // 使用partition by选取需要parts
        MergeTreeDataSelectExecutor::filterPartsByPartition(...);
        // 处理抽样
        ...
        // 使用主键索引和跳表索引
        result.parts_with_ranges = MergeTreeDataSelectExecutor::filterPartsByPrimaryKeyAndSkipIndexes(...);
    }
    catch(...)
    {
        ...
    }
    ...
}
```

值得注意的是，主键的 sparse index 使用二分查找直接缩小范围到所需要的 parts，而跳表索引就需要在选出来的 parts 里，每 n 个（用户自定义）granules 就需要比较 n 次。

**最佳实践**：

partition by 需要一个可以转为时间的列，比如 Datatime、Date 或者时间戳，而如果 primary key 中也有时间字段，可以使用同一个字段避免查询时需要同时指定两个时间字段。比如：指定为数据处理时间。

### 2.1 **Partition**

首先要辨析 part 和 partition 的区别，ClickHouse 应用层面定义了 partition，用户指定 partition by 关键词设置不同的 partition，但是 partition 只是逻辑分区。真正存储到磁盘时按 part 来存储，每一个 part 一个文件夹，里面存储不同字段的`.mrk`和`.bin`文件，以及一个`minmax_{PARTITION_KEY_COLUMN}.idx`文件，不同 part 的 minmax 作为一个索引存储于内存。

当查询的 WHERE 带有 partition key 时，首先会比较每一个 part 的 minmax 索引过滤不相关 parts。之后再根据 PARTITION BY 定义的规则过滤不相关 partition。

**可是 partition 不是越小越好**。

partitioning 并不会加速查询（有主键存在），过小的 partition 反而会导致大量的 parts 无法合并（MergeTree 引擎家族会在后台不断合并 parts），因为属于不同 partition 的 parts 无法合并。[[5\]](https://km.woa.com/group/571/articles/show/527756#fn:5)

**最佳实践**[[6\]](https://km.woa.com/group/571/articles/show/527756#fn:6)：

- 一个(Replicated)MergeTree 的 partition 大概 1 ～ 300GB

- - Summing/ReplacingMergeTree 的 partition 大概 400MB ～ 40GB

- 查询时涉及尽量少 partition

- 插入时最好只有 1 ～ 2 个分区

- 一张表维持 100 个分区以内

### **2.2 Primary key index**

主键是 ClickHouse 最重要的索引，没有之一。好的主键应该能有效排除大量无关的数据 granules，减少磁盘读取的字节数。

先讲几个主键的背景知识：

- 主键用于数据排序

- - ClickHouse 讲数据按主键排序，再按`index_granularity`设置的大小（默认 8192）将数据分为一个个 granules[[7\]](https://km.woa.com/group/571/articles/show/527756#fn:7)
  - 每个 granules 的第一行作为主键索引中的一个元素[[8\]](https://km.woa.com/group/571/articles/show/527756#fn:8)

- 查询时在主键上使用二分查找跳过无关 granules[[9\]](https://km.woa.com/group/571/articles/show/527756#fn:9)

- 主键只能通过前缀命中索引[[10\]](https://km.woa.com/group/571/articles/show/527756#fn:10)

- 每一个 part 内的`.bin`文件存储了 n 个 granules，用`.mrk`文件记录每一个 granules 在`.bin`文件的地址偏移[[11\]](https://km.woa.com/group/571/articles/show/527756#fn:11)

- ClickHouse 会在后台不断合并同一个 partition 的不同 parts，直到大小/分布达到“预期”

主键的选择应该尽可能考虑周全，因为主键是无法修改的，只能建新表后数据迁移。

**最佳实践**[[12\]](https://km.woa.com/group/571/articles/show/527756#fn:12)（针对(Replicated)MergeTree 引擎）：

1. 选择永远会用于过滤条件的列
2. 越重要的、基数越低的放左边
3. 主键中不要出现两个高基数字段，一般最后一列可以为总体增长的时间字段
4. 将行的特征字段加入，将相似的行放一起，提高压缩率
5. 若主键包含主从关系，主放左边，从放右边

### 2.3 **Data skipping indexes**

最后一步是跳表索引，这个没有太多可以讲的地方，和其他数据库相同，跳表索引用于尽量减少读取的行数。具体参看[官方文档](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#table_engine-mergetree-data_skipping-indexes)。

## 3.配置优化

配置优化分为两部分，全局配置优化和 MergeTree 表配置优化。

### 3.1 **全局配置优化**

参看[Altinity](https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-settings-to-adjust/)选择性配置优化项。

这里写三个推荐的配置：

1. 添加`force_index_by_date`和`force_primary_key`避免全盘读取
2. 调整内存配置，参考[Altinity](https://kb.altinity.com/altinity-kb-setup-and-maintenance/altinity-kb-memory-configuration-settings/)
3. 系统表添加 TTL 和`ttl_only_drop_parts`表配置

### 3.2 **表配置优化**

除了全局配置，MergeTree 引擎家族每张表也有自己的配置项。[[13\]](https://km.woa.com/group/571/articles/show/527756#fn:13)

推荐设置如下配置：

1. `ttl_only_drop_parts=1`。只有 parts 中所有数据都过期了才会 DROP，可以有效减少`TTL_MERGE`发生的频率，降低磁盘负载。
2. `merge_with_ttl_timeout=86400`。配合上一项配置，将 TTL 检查调整为 1 天一次（默认 4 小时一次）。
3. `use_minimalistic_part_header_in_zookeeper=1`。可以有效降低 Zookeeper 负载，避免 Zookeeeper 成为性能瓶颈（插入）。

## 4.字段优化

除了索引、分区和配置外，还有表字段可以优化。接下来将讲述 Schema 类型、CODEC 和缓存三个方面。

注意，尽量避免使用 Null，在 ClickHouse 中 Null 会用一个单独 Null masks 文件存储哪些行为 Null[[14\]](https://km.woa.com/group/571/articles/show/527756#fn:14)，因此读取某个普通字段只需要`.bin`和`.mrk`两个文件，而读取 Nullable 字段时需要`.bin`、`.mrk`和 masks 文件。社区查询验证，最高会有 2 倍性能损失。[[15\]](https://km.woa.com/group/571/articles/show/527756#fn:15)

### 4.1 **Schema 类型**

使用 ClickHouse 存储时，一般用户都会创建大宽表，包含大量数值、字符串类型的字段。这里提及两种 Schema 类型[[16\]](https://km.woa.com/group/571/articles/show/527756#fn:16)，没有哪个更优越，由读者执行评估业务适合哪一种。

#### 4.1.1 平铺字段

这是我们主表正在使用的类型，将可能用到的字段预留平铺，除了一系列基础字段外，增加大量`metric1`, `metric2`...`metricN`和`tag1`, `tag2`...`tagN`等等字段。

优点：

- 简单
- 只读取所需要的列，非常高效
- 每个指标、标记都可以有特殊类型
- 适合密集记录（所有预留字段几乎全用上）

缺点：

- 添加字段需要改变 schema
- 预留字段不能过多，最多 100 ～ 200 个
- 如果使用很稀疏，会创建大量 sparse file 字段
- 需要标识“数据缺失”的情况（Null 或者默认值）
- 读取的列越多，需要读取文件越多，IO 次数越多

#### 4.1.2 arrays/nested/map 字段

这是我们 ctree 功能正在使用的类型。将业务字段塞入嵌套数据类型中，比如 array、nested struct 和 map。后文以 array 举例：`metric_array`、`tag_array`。

优点：

- 动态扩展
- ClickHouse 有大量高效的相关处理函数，甚至可以针对 Array、Map 设置索引
- 适合稀疏记录（每行存储少量值，尽管总基数很高）

缺点：

- 只需要其中一个 metric/tag 时，需要将整个 array 全部读入内存
- 不通用，与其他系统交互时比较麻烦。比如 spark 使用 jdbc 时，嵌套类型无法支持比如 array(array(string))
- 不通意义的值存储在相同字段，压缩率变低
- 需要不同类型的预留字段时需要创建不同类型

#### 4.1.3 总结

关于 Schema 设计这里，读者可以考虑 28 原则，理论上 80%查询只会用到 20%的业务字段，因此可以将使用频率高的业务字段平铺，将使用频率低的字段放入嵌套结构中。

### 4.2 **CODEC**

CODEC 分为压缩算法 CODEC、存储格式 CODEC 和加密 CODEC，一般可以组合一起使用。在 ClickHouse 中，未显示指定 CODEC 的字段都会被分配一个 DEFAULT 默认 CODEC LZ4（除非用户修改 clickhouse 配置 compression 部分[[17\]](https://km.woa.com/group/571/articles/show/527756#fn:17)）。

压缩算法 CODEC 的选择是一个平衡板问题，更高的压缩度可以有更少的 IO 但是更高的 CPU，更低的压缩度有更多的 IO 但是更少的 CPU。这需要读者根据部署机器配置自行选择合适的压缩算法和压缩等级。

这里提供两个判断策略：

- 存在索引的字段可以设置更高的压缩等级
- 用于 where 条件的字段应该设置更低压缩等级

存储格式 CODEC 主要是`Delta`、`DoubleDelta`、`Gorilla`、`FPC`和`T64`几种。

- `Delta`存储行之间的变化值，适合变化较小且比较固定的列，比如时间戳。需要配合 ZSTD 使用
- `DoubleDelta`存储`Delta`的`Delta`。适合变化很慢的序列
- `Gorilla`适合不怎么变动的 integer、float 类型[[18\]](https://km.woa.com/group/571/articles/show/527756#fn:18)
- `FPC`适合于 float 类型，由于我们未使用 float 字段这里略过
- `T64`存储编码范围内最大、最小值，以转为 64bit 存储，适合较小的 integer 类型

扩展阅读：

- [Altinity Blog: New Encodings to Improve ClickHouse Efficiency](https://altinity.com/blog/2019/7/new-encodings-to-improve-clickhouse)
- [Altinity Wiki: Codecs sped](https://kb.altinity.com/altinity-kb-schema-design/codecs/codecs-speed/)

### 4.3 **缓存**

`mark_cache_size`可以调整`.mrk`文件的缓存大小，默认为 5GB。适当调大可以减少查询时 IO 次数，有效降低磁盘压力。[[19\]](https://km.woa.com/group/571/articles/show/527756#fn:19)

- 字段越多，`.mrk`文件越大
- `index_granularity`与`.mrk`文件大小成负相关

可以通过如下 SQL 查询当前所有表的 parts 信息：

```
SELECT
    database,
    table,
    count() AS parts,
    uniqExact(partition_id) AS partition_cnt,
    sum(rows),
    formatReadableSize(sum(data_compressed_bytes) AS comp_bytes) AS comp,
    formatReadableSize(sum(data_uncompressed_bytes) AS uncomp_bytes) AS uncomp,
    uncomp_bytes / comp_bytes AS ratio,
    formatReadableSize(sum(marks_bytes) AS mark_sum) AS marks,
    mark_sum / uncomp_bytes AS mark_ratio
FROM cluster(default_cluster, system.parts)
WHERE active
GROUP BY
    database,
    table
ORDER BY comp_bytes DESC
```

可以通过如下查询获取当天 mrk 缓存命中情况：

```
WITH (ProfileEvents.Values[indexOf(ProfileEvents.Names, 'MarkCacheHits')]) AS MARK_CACHE_HITS
SELECT
    toHour(event_time) AS time,
    countIf(MARK_CACHE_HITS != 0) AS hit_query_count,
    count() AS total_query_count,
    hit_query_count / total_query_count AS hit_percent,
    avg(MARK_CACHE_HITS) AS average_hit_files,
    min(MARK_CACHE_HITS) AS minimal_hit_files,
    max(MARK_CACHE_HITS) AS maximal_hit_files,
    quantile(0.5)(MARK_CACHE_HITS) AS "50",
    quantile(0.9)(MARK_CACHE_HITS) AS "90",
    quantile(0.99)(MARK_CACHE_HITS) AS "99"
FROM clusterAllReplicas('default_cluster', system.query_log)
WHERE event_date = toDate(now())
  AND (type = 2 OR type = 4)
  AND query_kind = 'Select'
GROUP BY time
ORDER BY time ASC
```

以及如下查询获取当前 mrk 缓存内存占用情况：

```
SELECT formatReadableSize(value)
FROM asynchronous_metrics
WHERE metric = 'MarkCacheBytes'
```

以及 mrk 缓存具体缓存多少文件：

```
SELECT value
FROM asynchronous_metrics
WHERE metric = 'MarkCacheFiles'
```

除此之外，ClickHouse 还可以调整`uncompressed_cache`缓存一定量原始数据于内存中。[[20\]](https://km.woa.com/group/571/articles/show/527756#fn:20)但是这个缓存只对大量短查询有效，对于 OLAP 来说，查询千奇百怪，不太建议调整这个配置。

## 5.业务优化

到了最难的部分，由于接下来的部分和不同业务息息相关，为了讲解我们业务上的优化，我先介绍下我们业务情况：

QAPM 主打应用性能监控，主要分为指标、个例两张表。个例表包含更多基础字段，一般用户展示；指标表主要用于聚合计算。

首先确定主键，毋庸置疑的前两个一定是

- app_id。放首位，因为可能存在同一个产品不同功能联动的情况，比如会话分析
- category。放第二位，因为功能之间独立，大量查询只涉及单功能

指标没有特征键值，因此只添加处理时间作为第三个主键。

对于指标表，设置的主键为：`app_id, category, entrance_time`

个例存在特征 feature，由于：

1. 大量查询都包含 feature_md5
2. feature 是行的特征，相同的特征表明两行相似，

将特征的 md5 增加到主键中，用于加速查询、提高压缩率。但是这里有两个方向：

- 若 feature_md5 是高基数、大量长尾的字段

- - 设置的主键为：`app_id, category, intDiv(entrance_time, 3600000), feature_md5`

- 若 feature_md5 基数可以降低到千、万量级

- - 设置的主键为：`app_id, category, feature_md5, entrance_time`

```
分区键设置为`PARTITION BY intDiv(entrance_time, 2592000000)
```

鉴于`SAMPLE BY`需要将 xxHash 字段放在主键中，主键都包含高基数字段，就不设置抽样键，而是在需要的时候软抽样[[21\]](https://km.woa.com/group/571/articles/show/527756#fn:21)：

```
SELECT count() FROM table WHERE ... AND cityHash64(some_high_card_key) % 10 = 0; -- Deterministic
SELECT count() FROM table WHERE ... AND rand() % 10 = 0; -- Non-deterministic
```

### 5.1 **插入优化**

数据插入看起来和查询性能没什么联系，但是有间接影响。不合理的插入会导致更多的写盘、更多的数据 merge 甚至有可能插入失败，影响读盘性能。

#### 5.1.1 聚合写入

ClickHouse 作为 OLAP 并不适合小批量、大并发写入，相反而适合大批量、小并发写入，官方建议插入数据每批次至少 1000 行，或者每秒钟最多 1 次插入。[[22\]](https://km.woa.com/group/571/articles/show/527756#fn:22)

这一小节我想强调原子（Atomic Insert）写入的概念：一次插入创建一个数据 part。

前文提及，ClickHouse 一个 part 是一个文件夹，后台有个 merge 线程池不断 merge 不同的 part。原子插入可以减少 merge 次数，让 ClickHouse 负载更低，性能更好。

原子写入的充分条件[[23\]](https://km.woa.com/group/571/articles/show/527756#fn:23)：

- 数据直接插入`MergeTree`表（不能有 Buffer 表）

- 数据只插入一个 partition（注意前文提到的 partition 和 part 的区别）

- 对于 INSERT FORMAT

- - 插入行数少于`max_insert_block_size`（默认 1048545）
  - 关闭并行格式化`input_format_parallel_parsing=0`

- 对于 INSERT SELECT

- - 插入行数少于`max_block_size`

- 小 block 被合并到合适的 block 大小`min_insert_block_size_rows` and `min_insert_block_size_bytes`

- `MergeTree`表不包含物化视图

这里贴一下我们生产的配置（users.xml）。

经过统计，个例表每行大约 2KB，指标表每行大约 100B（未压缩）。

设置`min_insert_block_size_rows`为 10000000，指标会先满足这个条件，大概一个 block 原始大小 1GB。设置`min_insert_block_size_bytes`为 4096000000，个例会先满足这个条件，大概一个 block 原始大小 1G，约 1024000 行。

这三个配置项是**客户端配置**，需要在插入的 session 中设置，而不是在那几个`.xml`中配置。

```
max_insert_block_size: 16777216
input_format_parallel_parsing: 0
min_insert_block_size_rows: 10000000
min_insert_block_size_bytes: 1024000000
```

注意，`min_insert_block_size_rows`和`min_insert_block_size_bytes`是“或”的关系：

```
// src/Interpreters/SquashingTransform.cpp

bool SquashingTransform::isEnoughSize(size_t rows, size_t bytes) const
{
    return (!min_block_size_rows && !min_block_size_bytes)
        || (min_block_size_rows && rows >= min_block_size_rows)
        || (min_block_size_bytes && bytes >= min_block_size_bytes);
}
```

#### 5.1.2 读写分离

> ⚠️：本方案并没有经过生产验证，酌情考虑

ClickHouse 有 Shard 和 Replica 可以配置，作用如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvattWZDn1dwv0vjYR7hBqEQuYCsQib11zzZhd2hicf4ZsVI00QzFVemWuSgDA3vO0JFtKboTQ5ea1Cww/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

所谓读写分离也就是将 Shard 分为两半，一半只用于查询，只要让分布式表查询都导入到 Shard1 即可（在`users.xml`中配置`load_balancing`为`first_or_random`）；一半用于写入，插入的程序手动控制插入 Shard2 的节点，由 ClickHouse 的 ReplicatedMergeTree 不同 Shard 数据依靠 zookeeper 自动同步的策略将数据同步到 Shard1。[[24\]](https://km.woa.com/group/571/articles/show/527756#fn:24)

这种策略有天然的缺陷：

- 写的那半 Shard 持续有一定量（不会很高）的资源消耗用于写入
- 读的那半 Shard 会有资源消耗用于同步写入（由于不用处理，会比直接写入的情况资源消耗更低），但是读请求会导致资源消耗突增
- 并发增加时性能不如混合情况，因为读写分离相当于将读资源砍半

> 🤔：或许可以配置两边 Shard 资源不一致来解决问题，比如写入的 Shard 资源拉低，专用于处理数据插入；读的 Shard 资源更高，专门用于处理突增并发流量。

#### 5.1.3 BufferEngine

Buffer 并不推荐常规业务使用，只有在迫切需要查询实时性+插入无法大批量预聚合时使用：

- 无法 atomic insert
- 即使使用 BufferEngine，数据插入也至少 1000 行每次，或者每秒钟最多 1 次插入[[25\]](https://km.woa.com/group/571/articles/show/527756#fn:25)

#### 5.1.4 KafkaEngine+MV

**该部分待补充，想看的同学可以在评论区踢踢 😄**

### 5.2 **预聚合**

预聚合有三种方法，ETL、物化视图和投影，他们的区别如下[[26\]](https://km.woa.com/group/571/articles/show/527756#fn:26)：

|                                                            |                             ETL                              |                              MV                              |             Projections             |
| :--------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :---------------------------------: |
|                          Realtime                          |                              no                              |                             yes                              |                 yes                 |
| How complex queries can be used to build the preaggregaton |                             any                              |                           complex                            |             very simple             |
|                  Impacts the insert speed                  |                              no                              |                             yes                              |                 yes                 |
|                Are inconsistancies possible                |   Depends on ETL. If it process the errors properly - no.    |              yes (no transactions / atomicity)               |                 no                  |
|                  Lifetime of aggregation                   |                             any                              |                             any                              |        Same as the raw data         |
|                        Requirements                        |                need external tools/scripting                 |                 is a part of database schema                 |      is a part of table schema      |
|               How complex to use in queries                | Depends on aggregation, usually simple, quering a separate table | Depends on aggregation, sometimes quite complex, quering a separate table | Very simple, quering the main table |
|   Can work correctly with ReplacingMergeTree as a source   |                             Yes                              |                              No                              |                 No                  |
|  Can work correctly with CollapsingMergeTree as a source   |                             Yes                              |                   For simple aggregations                    |       For simple aggregations       |
|                       Can be chained                       |          Yes (Usually with DAGs / special scripts)           | Yes (but may be not straightforward, and often is a bad idea) |                 No                  |
|        Resources needed to calculate the increment         |                      May be signigicant                      |                         Usually tiny                         |            Usually tiny             |

在我们业务中，个例是不应该预聚合的，因为数据需要被拉取展示而不用计算。指标需要聚合，数据量较大，每次实时计算对 ClickHouse 负载太大。

其实还有一种聚合方式，[过期数据聚合](https://kb.altinity.com/altinity-kb-queries-and-syntax/ttl/ttl-group-by-examples/)。可以参考，同样限制要求 group by 的键值为主键前缀。

在我们业务使用时，什么时候用哪一个呢？

1. 需要针对某个功能加速时，可以考虑物化视图/投影
2. 全表预聚合加速查询，需要使用 ETL

### 5.3 **资源控制**

最后，为了避免集群被某个查询、插入弄垮，需要合理安排内存使用，需要给访问账户分权限，在我们业务分为：

- `default`：最高级账号，不使用
- `root`：数据插入，配置聚合写入部分的几个配置项
- `monitor`：内部开发使用，权限较高
- `viewer`：web 使用，添加大量限制

`viewer`账户配置如下所示：

```
<yandex>
    <profiles>
        <query>
            <max_memory_usage>10000000000</max_memory_usage>
            <max_memory_usage_for_all_queries>100000000000</max_memory_usage_for_all_queries>
            <max_rows_to_read>1000000000</max_rows_to_read>
            <max_bytes_to_read>100000000000</max_bytes_to_read>
            <max_rows_to_group_by>1000000</max_rows_to_group_by>
            <group_by_overflow_mode>any</group_by_overflow_mode>
            <max_rows_to_sort>1000000</max_rows_to_sort>
            <max_bytes_to_sort>1000000000</max_bytes_to_sort>
            <max_result_rows>100000</max_result_rows>
            <max_result_bytes>100000000</max_result_bytes>
            <result_overflow_mode>break</result_overflow_mode>
            <max_execution_time>60</max_execution_time>
            <min_execution_speed>1000000</min_execution_speed>
            <timeout_before_checking_execution_speed>15</timeout_before_checking_execution_speed>
            <max_columns_to_read>25</max_columns_to_read>
            <max_temporary_columns>100</max_temporary_columns>
            <max_temporary_non_const_columns>50</max_temporary_non_const_columns>
            <max_subquery_depth>2</max_subquery_depth>
            <max_pipeline_depth>25</max_pipeline_depth>
            <max_ast_depth>50</max_ast_depth>
            <max_ast_elements>100</max_ast_elements>
            <readonly>1</readonly>
        </query>
    </profiles>
</yandex>
```

同时建议设置 quota，减少大量读盘计算、LIMIT 少量数据返回的情况发生。



我们是 CSIG 性能工程二组 QAPM 团队，QAPM 时一款应用性能监控工具，覆盖 android、ios、小程序、mac 和 win 多端，已有腾讯会议、优衣库等大用户接入，值得信赖，欢迎同事试用我们 QAPM 产品～[跳转链接](https://cloud.tencent.com/product/qapm)

在 ClickHouse 优化过程遇到无数的问题，卡在 ClickHouse 自身监控无法覆盖的角落时，全靠性能工程三组员工的 Drop（雨滴）工具的鼎力相助，高效直观监控 CVM 各项指标，降低优化门槛，助力业务增效～[跳转链接](https://drop.qcloud.com/)

## 6.参考

脚注

[[1\]](https://km.woa.com/group/571/articles/show/527756#fnref:1)https://clickhouse.com/docs/en/faq/general/why-clickhouse-is-so-fast/

[[2\]](https://km.woa.com/group/571/articles/show/527756#fnref:2)[https://benchmark.clickhouse.com](https://benchmark.clickhouse.com/)

[[3\]](https://km.woa.com/group/571/articles/show/527756#fnref:3)https://gcc.gnu.org/wiki/New_C_Parser

[[4\]](https://km.woa.com/group/571/articles/show/527756#fnref:4)https://clang.llvm.org/features.html

[[5\]](https://km.woa.com/group/571/articles/show/527756#fnref:5)https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#mergetree-data-storage

[[6\]](https://km.woa.com/group/571/articles/show/527756#fnref:6)https://kb.altinity.com/engines/mergetree-table-engine-family/pick-keys/#partition-by

[[7\]](https://km.woa.com/group/571/articles/show/527756#fnref:7)https://clickhouse.com/docs/en/guides/improving-query-performance/sparse-primary-indexes/sparse-primary-indexes-design/#data-is-organized-into-granules-for-parallel-data-processing

[[8\]](https://km.woa.com/group/571/articles/show/527756#fnref:8)https://clickhouse.com/docs/en/guides/improving-query-performance/sparse-primary-indexes/sparse-primary-indexes-design/#the-primary-index-has-one-entry-per-granule

[[9\]](https://km.woa.com/group/571/articles/show/527756#fnref:9)https://clickhouse.com/docs/en/guides/improving-query-performance/sparse-primary-indexes/sparse-primary-indexes-design/#the-primary-index-is-used-for-selecting-granules

[[10\]](https://km.woa.com/group/571/articles/show/527756#fnref:10)https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#primary-keys-and-indexes-in-queries

[[11\]](https://km.woa.com/group/571/articles/show/527756#fnref:11)https://clickhouse.com/docs/en/guides/improving-query-performance/sparse-primary-indexes/sparse-primary-indexes-design/#mark-files-are-used-for-locating-granules

[[12\]](https://km.woa.com/group/571/articles/show/527756#fnref:12)https://kb.altinity.com/engines/mergetree-table-engine-family/pick-keys/#how-to-pick-an-order-by--primary-key

[[13\]](https://km.woa.com/group/571/articles/show/527756#fnref:13)https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#settings

[[14\]](https://km.woa.com/group/571/articles/show/527756#fnref:14)https://clickhouse.com/docs/en/sql-reference/data-types/nullable/#storage-features

[[15\]](https://km.woa.com/group/571/articles/show/527756#fnref:15)https://groups.google.com/g/clickhouse/c/AP2FbQ-uoj8

[[16\]](https://km.woa.com/group/571/articles/show/527756#fnref:16)https://kb.altinity.com/altinity-kb-schema-design/best-schema-for-storing-many-metrics-registered-from-the-single-source/

[[17\]](https://km.woa.com/group/571/articles/show/527756#fnref:17)https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/#server-settings-compression

[[18\]](https://km.woa.com/group/571/articles/show/527756#fnref:18)http://www.vldb.org/pvldb/vol8/p1816-teller.pdf

[[19\]](https://km.woa.com/group/571/articles/show/527756#fnref:19)https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/#server-mark-cache-size

[[20\]](https://km.woa.com/group/571/articles/show/527756#fnref:20)https://clickhouse.com/docs/en/operations/caches/

[[21\]](https://km.woa.com/group/571/articles/show/527756#fnref:21)https://kb.altinity.com/altinity-kb-queries-and-syntax/altinity-kb-sample-by/#sample-emulation-via-where-condition

[[22\]](https://km.woa.com/group/571/articles/show/527756#fnref:22)https://clickhouse.com/docs/en/about-us/performance/#performance-when-inserting-data

[[23\]](https://km.woa.com/group/571/articles/show/527756#fnref:23)https://github.com/ClickHouse/ClickHouse/issues/9195#issuecomment-587500824

[[24\]](https://km.woa.com/group/571/articles/show/527756#fnref:24)https://www.jianshu.com/p/c3a4cc528ce8

[[25\]](https://km.woa.com/group/571/articles/show/527756#fnref:25)https://github.com/ClickHouse/ClickHouse/issues/11783#issuecomment-647778852

[[26\]](https://km.woa.com/group/571/articles/show/527756#fnref:26)https://kb.altinity.com/altinity-kb-schema-design/preaggregations/



原文作者：oliverdding，腾讯 CSIG 测试开发工程师

原文链接：https://mp.weixin.qq.com/s/38RMVbw25P3iuE4IIuxdog

# 【NO.359】腾讯云OCR性能是如何提升2倍的

> 腾讯云 OCR 团队近期进行了耗时优化，通用 OCR 优化前平均耗时 1815ms，优化后平均耗时 824ms，提升 2.2 倍。本文旨在让大家了解 OCR 团队在耗时优化中的思路和方法(如工程优化、模型优化、TIACC 加速)，希望能给大家在工作中提供一些新的思路。

### 1.背景介绍

#### **1. 业务背景**

近期某重要客户反馈，受当前正在使用的 OCR 服务可用性(非腾讯云)的影响，业务不可用长达半个小时，而且这样的情况时有发生。为了更好的服务，客户开始调研，主要是从服务可用性，准确率和服务耗时三个方面评估。

通过详细体验和测试，发现腾讯云 OCR 在服务可用性和准确率方面都表现非常不错，但客户对腾讯云 OCR 耗时不满意，希望我们进行耗时优化，在达到要求之后再考虑接入。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1VSxIyl0tba5DOVQwfxdsVoyFebM9ibKpk6wcXVDMw9AJBS8C9vJN9ew/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

除了该客户的反馈，之前也陆陆续续收到其他客户类似的反馈。**因此，优化腾讯云 OCR 服务耗时，提供更快的文字识别服务势在必行。**

#### **2. 问题挑战**

耗时优化是一个系统性工程，需要多方的支持和协作，文字识别服务进行耗时优化，主要有以下挑战：

● 环节多：耗时优化涉及多个环节，包括模型算法、TI-ACC、工程等，多环节都需要分析各自阶段耗时，制定完整的耗时优化目标。

● 时间短：客户耗时优化诉求强烈，但是客户的耐心有限，留给我们优化的时间很短。

● 成本考量：降本增效大背景下，单纯依赖机器的情况一去不复返，需要做成本优化。

我们也成立了专项团队进行攻坚，从工程优化，模型优化，TI-ACC 优化等方面发力，逐步降低服务耗时。优化前平均耗时 1815ms，优化后平均耗时 824ms，耗时性能提升 2.2 倍，并最终得到重要客户的肯定。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf18bU2iaQjZogfiaGRyYcfCVuNJ9gnqukicv5aVkHJ9bicT2OkFkaPB1RvFg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

接下来将介绍我们在耗时优化方面的具体实践。



### 2.分析问题

#### **2.1 框架和链路分析**

OCR 服务通过云 API 接入，内部多个微服务间通过 TRPC 进行调用，基本架构图如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1kemibl3YYueq2mznJ8qJ4aNhyapNs77nhibkYicCGaskQgGv21lwlQoZw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

从客户发起请求到 OCR 服务处理完成主要链路为：

1. 云 API 层：请求首先会传输到离客户最近的云 API 接入点，云 API 接入点会进行相应的鉴权、寻址、转发等操作
2. 业务逻辑层：文字识别逻辑层服务会对数据做处理、下载、计费、上报等操作
3. 引擎层：算法引擎服务对图片进行处理，识别出文字

#### **2.2 主要阶段耗时**

主要涉及到下面几个阶段的耗时：

● 客户传输耗时：客户请求到云 API 和云 API 响应到客户链路的传输耗时在测试过程中发现波动很大。文字识别业务场景下请求传输的都是图片数据，受客户网络带宽和质量的影响大。而且客户请求的文字识别服务在广州，请求需要跨地域，进一步增加了传输耗时。

● 业务逻辑耗时：业务逻辑中有很多复杂的工程处理，主要耗时点包括数据处理、编解码、图片下载、路由、上报等。

● 算法引擎耗时：为了达到更好的识别效果和泛化性，通用 OCR 模型会比较复杂，其中检测和识别都会涉及到复杂的浮点计算，耗时长。

因此，要优化文字识别服务的整体耗时，就需要从各个阶段进行详细分析和优化。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1P2tDISYqPcFAyueicBubcbJMrOvL7acoqib9Z5J5RhtNFUXWfffPFbLw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

#### **2.3 测试方法和结论**

| 测试机器 | 测试机器地域     | 请求地域参数                          | OCR 部署地域 | 图片大小  | 图片编码 |
| :------- | :--------------- | :------------------------------------ | :----------- | :-------- | :------- |
| 云服务器 | 北京、上海、广州 | ap-beijing、ap-shanghai、ap-guangzhou | 广州         | 200KB~3MB | Base64   |

客户生产环境的机器在云服务器上，为了保持和客户测试条件一致，我们购买了和客户相同环境的云服务器(含北京、上海、广州)进行模拟测试，详细分析文字识别服务各个阶段耗时。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1MZyarNI6mH8H7m82PLX2ryMZmY3UV4ibxTjWjcIloxNVLW2LJ4jzC4g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

端到端耗时为客户请求发起直到收到响应的总耗时，客户到腾讯云 API 接入点，以及云 API 响应给客户存在着公网传输的耗时，从云 API 接入后就是腾讯云内网链路服务的耗时。端到端耗时主要由客户传输耗时、云 API 耗时、业务逻辑层耗时、引擎耗时组成，经过多次请求通用 OCR 后统计各个阶段的平均耗时，具体情况为：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1W2668I5LbAdPD6njCJrUia4tVhmicR6JianwVcICMAiaBRL1JCfXTwQ0kw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

1. 客户的传输耗时占比很大，主要是因为客户在北京发起请求，需要跨地域到广州的服务，存在很大的传输耗时，这部分需要我们通过优化部署来减低耗时。
2. 在服务链路中，业务逻辑处理的耗时也较多，这部分耗时需要我们精益求精，优化工程逻辑和架构，进而降低耗时。
3. 文字识别服务的引擎耗时占比最高，我们需要对算法引擎的耗时进行重点优化。

通过详细的各阶段耗时测试可以发现，引擎耗时占主要部分，所以会重点优化引擎耗时，主要手段是模型优化和 TI-ACC 加速。为了做到极致优化，我们还通过日志分流和编解码优化降低了业务逻辑层耗时，同时服务就近多地部署，优化了客户传输耗时。



### 3.解决问题

#### **3.1 模型优化—引入 TSA 算法减少模型耗时**

优化之前耗时长度会随着文字字数增加显著变大，引入 TSA 算法可以明显改善这种情况，减少模型耗时

##### 3.1.1 OCR 特点与算法过程分析

基于 OCR 的特点与难点，针对 OCR 问题学术界算法可以划分成两大类：

1. 基于 CTC (Connectionist Temporal Classification)的流式文本识别方案
2. 基于 Attention 的机器翻译的框架方案。

在基于深度学习的 OCR 识别算法中，整个流程可以归纳成了四个步骤如下图: ① 几何变换 ② 特征提取 ③ 序列建模 ④ 对齐与输出。CTC 方案与 Attention 方案区别主要是在步骤 ④，它作为衔接视觉特征与语义特征的关键桥梁，可以根据上下文图像特征和语义特征做精确输入、输出的对齐，是 OCR 模型关键的过程。

针对上述特点与耗时问题，我们提出了 TSA 文本识别模型。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1DficWibXUwplOZ4AILZOhbLicr0SMbJgnfjLicuEkjY3wtSUFBFicC9NG3A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

文本识别的主要流程

##### 3.1.2 TSA 研发背景

① 研究表明：图像对 CNN 的依赖是非必需的，当直接应用于图像块序列时，transformer 也能很好地执行图像分类任务 。

② 63%耗时在(BiLSTM+Global Attn，或 attention 形式)序列建模!

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1h1IvSRJwIn8zouD6nHkt4sCbWOyPlsfmalzloMqFf9AanNQDSUy5nw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

OCR 识别模型耗时统计分析

##### 3.1.3 识别模型特点

特点 1: 引入 self-attention 算法，对输入图像进行像素级别建模，并替代 RNN 完成序列建模。相比 CNN 与 RNN，self-attention 不受感受野和信息传递的限制，可以更好地处理长距离信息。

特点 2: 设计 self-attention 计算过程中的掩码 mask。由于 self-attention 天然可以“无视”距离带来的影响，因此需要对输入像素间自注意力进行约束 。其过程如下图所示。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1omLZQf4Vrib1cb8e6GhAStjcTx33ic5z3VLQM5ibY8zNJAk8lLV55AYUg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

掩码注意力计算过程示意图

总结：通过 multi-head self-attention 对序列建模，可以将两个任意时间片的建模复杂度由 lstm 的 O (N)降低到 O(1), 预测速度大幅提升；优化后的算法和文本长度无关，对大文本得到成倍的加速提升。

#### **3.2 TIACC 加速优化—继续减少模型耗时**

为了进一步降低模型的耗时，我们使用了 TI-ACC 进行加速，[TI-ACC](https://cloud.tencent.com/product/tiacc)支持多种框架和复杂场景，面向算法和业务工程师提供一键式推理加速功能。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1WvYqrR26qoC7Tk6y7glodK5rAMpgHiaX9sibISzrcILRAD155KWOo3rg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

TIACC 框架

##### 3.2.1 支持复杂结构模型

OCR 任务可以分为两大类：检测任务和识别任务，检测任务和识别任务一般会保存成两个独立的 TorchScript 模型，每个任务又分为了多个阶段。通用 OCR 检测任务包含了 backbone(RepVGG 等)、neck(FPN 等）、head（Hourglass 等）等部分。

识别任务包含特征提取部分和序列建模部分，其中特征提取部分既有传统 CNN 网络，也有最新 ResT 等基于 Transformer 的网络，序列建模部分既有传统 RNN 网络，也有基于 Attention 的网络。

对 OCR 业务模型的加速，是对推理加速引擎兼容性的考验。下表分别对比了 TIACC 和 Torch-TensorRT（22.08-py3-patch-v1）对本文模型加速能力：

|                | 通用检测 | 通用识别 | 身份证检测 | 身份证识别 | 高精度 encoder | 高精度 decoder |
| :------------- | :------- | :------- | :--------- | :--------- | :------------- | :------------- |
| Torch-TensorRT | √        | √        | ×          | ×          | ×              | ×              |
| TIACC          | √        | √        | √          | √          | √              | √              |

TIACC 和 Torch-TensorRT 兼容性对比

##### 3.2.2 降低请求延迟并提高系统吞吐

OCR 业务不仅模型结构复杂，模型输入也很有特点，下图显示了本文其中一个模型的输入

| 输入名字    | 类型  | 最小 shape | 最大 shape   |
| :---------- | :---- | :--------- | :----------- |
| input_0     | int32 | 1*10       | 1*2000       |
| input_1     | int32 | 1*10       | 1*2000       |
| input_2     | int32 | 1*10*4     | 1*2000*4     |
| input_3     | float | 1*10*10    | 1*2000*4000  |
| input_4     | float | 2*0*768    | 2*2000*768   |
| input_5     | float | 6*12*64*0  | 6*12*64*2000 |
| input_6     | float | 6*12*0*64  | 6*12*2000*64 |
| input_7     | float | 2*0*768    | 2*2000*768   |
| input_8     | float | 6*12*64*0  | 6*12*64*2000 |
| input_9     | float | 6*12*0*64  | 6*12*2000*64 |
| input_10(0) | int64 | 10         | 2000         |
| input_10(1) | int32 | 1*0        | 1*2000       |

上述输入，包括以下特点：1）输入类型多，2）输入 shape 变化范围大， 3）有些最小 shape 维度是 0，4) 输入不仅包含 Tensor, 也包含 Tuple。仅仅上述模型输入格式，就会导致一些推理引擎不可用或加速效果不明显。比如 TensorRT 不支持 int64 输入，Torch-TensorRT 不支持 Tuple 输入。输入变化范围大，会导致推理引擎显存消耗过高，导致某些推理引擎加速失败或无法单卡多路并行推理，进而导致无法有效提升 TPS。对于 OCR 这种大调用量业务，TPS 提升可以有效降低 GPU 卡规模从而带来可观成本降低。

针对 OCR 模型 shape 范围过大，显存占用量高问题，TIACC 通过显存共享机制，有效降低了显存占用。针对动态 shape 模型，推理加速引擎在部分 shape 加速明显，部分 shape 反而会性能降低的问题，TIACC 通过重写关键算子，并且根据模型结构，选择业务全流程最优的算子实现，有效解决了实际业务中部分 shape 加速明显，整体加速不理想问题。

通过 OCR 业务模型的打磨，最终不仅提升模型的推理延迟，即使多路部署也可以有效提升模型的吞吐。下表显示了 TIACC FP16 相比 libtorch FP16 的加速倍数，数字越高越好。

|          | 通用检测 | 通用识别 | 身份证检测 | 身份证识别 | 高精度 encoder | 高精度 decoder |
| :------- | :------- | :------- | :--------- | :--------- | :------------- | :------------- |
| 延迟降低 | 1.67     | 1.58     | 2.2        | 1.6        | 1.6            | 2.1            |
| 吞吐提升 | 1.73     | 1.47     | 1.43       | 1.47       | 1.71           | 1.97           |

TIACC 加速后延迟降低和吞吐提升统计

TIACC 底层使用了 TNN 作为基础框架，性能强大，其中 TNN 是优图实验室结合自身在 AI 场景多年的落地经验，向行业开源的部署框架。TIACC 基于 TNN 最新研发的子图切分功能基础上进行产品化封装，为企业提供 AI 模型训练、推理加速服务，显著提高模型训练推理效率，降低成本。

#### **3.3 工程优化——优化逻辑和传输耗时**

##### 3.3.1 编解码优化

文字识别采用微服务架构设计，整体服务链路长，其中涉及到 TRPC、HTTP、Nginx-PB 协议的转换和调用，所以请求和响应需要有很多的编解码操作。文字识别场景下，请求和响应包体都非常大，RPC 协议之间的转换和编解码增加了计算耗时，为了进一步降低服务耗时，我们对这些编解码操作进行了整体的优化，减少了协议转换和编解码次数。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1JyAECTq5GYBwKtEEj1ZAh7P3AgxsqSBkqpoibxj7XtibuUGFd0mhUWQw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### 3.3.2 日志分流处理

在业务中有很多关键节点需要记录日志，便于问题定位。在文字识别业务场景下，日志需要脱敏处理大量识别出的文本数据，写入速度慢。为了避免日志操作影响服务响应耗时，我们设计了日志分流上报服务，将日志操作全部通过异步流程上报到其他微服务完成，减少主逻辑耗时。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1VMRxASjdlkmpmSeIEJqQeBBwyYKfapom7tW8MLIpaSxXR8cdmZvFVQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### 3.3.3 就近部署—降低传输耗时

经过多次详细测试文字识别业务在北京、上海、广州区域机器发起请求的耗时差异，我们对跨地域传输耗时有了比较明确的认识。服务跨地域请求时（比如在北京发起请求，实际服务部署在广州），会存在很大的传输耗时波动，客户的使用体验会下降，因此我们针对通用 OCR 接口进行了就近多地部署，在服务部署的架构上对耗时进行了优化。

在文字识别跨地域请求时，进行多地部署后耗时有明显下降。通用 OCR 接口线上多地部署发布以后，我们对线上云 API 记录的总耗时监控进行了观察和对比，耗时有了很明显的下降，某段时间内耗时从平均 1100ms 下降到 800ms，下降了 300ms。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf11WW9TKDdwGyYibO8pq4vgIBpzsVeQJoBCwZSYcGtdNkjdib9Fgib9xYyg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

##### 3.3.4 GPU 显存优化-提高系统吞吐

随着 OCR 业务功能点越来越多，业务中使用的 AI 模型越来越多，且更复杂，对显存的要求也越来越大。这时，显卡显存大小影响服务能开启并发数，而并发数影响服务的吞吐，所以显存往往成为业务吞吐量的瓶颈。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf12JktylxyOCgkTqXAKKCDvibFrBjB5QhiaoExE5cbZoMptqEEEjYQ7hEQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

显存优化方案框架

显存管理主要就是解决上述问题，主要思想是解耦合显存占用大小跟服务并发路数之间的关系，提高并发路数不再导致显存增大，进而提升服务整体吞吐量；并且由服务路数实现并行方式转换为不同模型之间并行方式，提高了 GPU 计算的并行度，更好的充分利用 GPU 资源。以通用 OCR 为例，下图可以看使用前后 GPU 利用率变化和显存占用变化。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1Z2mFY25OibpHxcwNCcTbTGy7xKrfLWcV7MofMrcHPAbb8duQCplGZnQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

优化后平均 GPU 利用率明显提高



![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1MhX61ibEpNcoTZeuJvfPk4S4tcUC3CgxGUKywAkdbOgdot5EKLzp4MQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

优化后显存明显降低



### 4.最终效果

#### **4.1 通用 OCR 平均耗时优化 54.6%**

通用 OCR 三地的平均耗时，优化前是 1815ms，优化后 824ms，优化比例 54.6%。

| 地区     | 优化前(ms) | 优化后(ms) | 优化比例 |
| :------- | :--------- | :--------- | :------- |
| 华东     | 2110       | 747        | 64.6%    |
| 华北     | 1946       | 1129       | 42.0%    |
| 华南     | 1390       | 596        | 57.1%    |
| 三地平均 | 1815       | 824        | 54.6%    |

#### **4.2 图片文字越多，耗时优化越明显**

1. 随着图片中的字数的增多，耗时优化效果更明显，耗时优化比例从 23%到 69%。
2. 在耗时优化之前，服务耗时和图片文字正相关，相关性强；优化后，相关性降低。

| 华南-测试集(2048P) | 优化前(ms) | 优化后(ms) | 优化比例 |
| :----------------- | :--------- | :--------- | :------- |
| 字数 0-10          | 829        | 637        | 23%      |
| 字数 10-100        | 835        | 565        | 32%      |
| 字数 100-500       | 1493       | 732        | 51%      |
| 字数 500-1000      | 3338       | 820        | 75%      |
| 字数 1000 以上     | 2751       | 847        | 69%      |
| 平均               | 1849       | 720        | 50%      |

#### **4.3 召回率提升 1.1%**

效果：优化后的版本精度有所提升，召回率提升 1.1%，准确率提升 2.3%。

|              | 优化前       | 优化后       | 差值         |              |              |              |              |
| :----------- | :----------- | :----------- | :----------- | :----------- | :----------- | :----------- | :----------- |
| 测试集       | metric       | recall       | precision    | recall       | precision    | recall       | precision    |
| 测试集 1     | char         | 91.94        | 93.82        | 94.87        | 95.59        | 2.93         | 1.77         |
| 测试集 1     | word         | 81.83        | 84.51        | 82.62        | 84.13        | 0.79         | -0.38        |
| 测试集 2     | char         | 91.34        | 89.97        | 91.91        | 95.52        | 0.57         | 5.55         |
| 测试集 2     | word         | 80.03        | 76.83        | 83.64        | 86.9         | 3.61         | 10.07        |
| ............ | ............ | ............ | ............ | ............ | ............ | ............ | ............ |
| 测试集 n     | char         | 97.49        | 97.92        | 97.74        | 98.8         | 0.25         | 0.88         |
| 测试集 n     | word         | 67.25        | 74.4         | 74.67        | 81.43        | 7.42         | 7.03         |
| 平均指标     | 92.5%        | 93.0%        | 93.6%        | 95.4%        | **1.1%**     | **2.3%**     |              |

### 5.成本降低

通过 TI-ACC、工程优化等手段，不仅满足了客户的耗时要求，成功牵引重要客户落地，同时也提升了每个接口的 tps，平均 tps 提升 51.4%。

|          | 通用印刷 OCR | 高精度 OCR | 高精度文档 | 平均      |
| :------- | :----------- | :--------- | :--------- | :-------- |
| tps 提升 | 43%          | 65%        | 60%        | **51.4%** |
| 显存降低 | 降低 30%~80% |            |            |           |

由于 tps 的提升，每月可以节省等比例的机器成本。



### 6.结语

通用全链路优化，整体耗时降低 54.6%，GPU 利用率也从平均 35%优化到 85%。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavJfeqMS3YgOt122Gic9Chf1kB5rS0yoh0Nujp3yXuAEs3FTXqCmUJgAnYJDic2ib7wTykMxScRbAF6w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

本次优化取得了阶段性的成果，但耗时是一个持续不断的过程，通用 OCR pipleline 等环节可能还存在优化空间，后面将继续跟踪。

整个耗时优化涉及到的环节比较多，特别感谢优图实验室、TIACC、测试团队在耗时优化中的支持和给力配合。

注：本文所述数据均为实验测试数据。

相关链接：

1.OCR:https://cloud.tencent.com/product/generalocr

2.TNN:https://github.com/Tencent/TNN

3.TIACC:https://cloud.tencent.com/product/tiacc



原文作者：benpeng，腾讯 CSIG 应用开发工程师

原文链接：https://mp.weixin.qq.com/s/ByQl3oIbxe3sqUzhfQ9rkQ

# 【NO.360】一文读懂数据库优化之分库分表

> 本文从 5W1H 角度介绍了分库分表手段，其在解决如 IO 瓶颈、读写性能、物理存储瓶颈、内存瓶颈、单机故障影响面等问题的同时也带来如事务性、主键冲突、跨库 join、跨库聚合查询等问题。anyway，在综合业务场景考虑，正如缓存的使用一样，本着非必须勿使用原则。如数据库确实成为性能瓶颈时，在设计分库分表方案时也应充分考虑方案的扩展性，或者考虑采用成熟热门的分布式数据库解决方案，如 TiDB。

阅读此文你将了解：

- 什么是分库分表以及为什么分库分表
- 如何分库分表
- 分库分表常见几种方式以及优缺点
- 如何选择分库分表的方式



### 1.数据库常见优化方案

对于后端程序员来说，绕不开数据库的使用与方案选型，那么随着业务规模的逐渐扩大，其对于存储的使用上也需要随之进行升级和优化。

随着规模的扩大，数据库面临如下问题：

- 读压力：并发 QPS、索引不合理、SQL 语句不合理、锁粒度
- 写压力：并发 QPS、事务、锁粒度
- 物理性能：磁盘瓶颈、CPU 瓶颈、内存瓶颈、IO 瓶颈
- 其他：宕机、网络异常

面对上述问题，常见的优化手段有：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4kAtbLbmBK0zHX53rZ5lBFQ8yrf8XfRHHrS66fL3VNBA3ycWYYyGy8AQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

索引优化、主从同步、缓存、分库分表每个技术手段都可以作为一个专题进行讲解，本文主要介绍分库分表的技术方案实现。



### 2.什么是分库分表？

对于阅读本文的读者来说，分库分表概念应该并不会陌生，其拆开来讲是分库和分表两个手段：

- 分表：将一个表中的数据按照某种规则分拆到多张表中，**降低锁粒度以及索引树**，提升数据查询效率。
- 分库：将一个数据库中的数据按照某种规则分拆到多个数据库中，以**缓解单服务器的压力**（CPU、内存、磁盘、IO）。



### 3.为什么分库分表？

- **性能角度：CPU、内存、磁盘、IO 瓶颈**

- - 随着业务体量扩大，数据规模达到百万行，数据库索引树庞大，查询性能出现瓶颈。
  - 用户并发流量规模扩大，由于单库(单服务器)物理性能限制也无法承载大流量。

- **可用性角度：单机故障率影响面**

- - 如果是单库，数据库宕机会导致 100%服务不可用，N 库则可以将影响面降低 N 倍。

- 

### 4.分库分表带来的问题？

- **事务性问题**

- - 方案一：在进行分库分表方案设计过程中，从业务角度出发，尽可能保证一个事务所操作的表分布在一个库中，从而实现数据库层面的事务保证。

  - 方案二：方式一无法实现的情况下，业务层引入分布式事务组件保证事务性，如事务性消息、TCC、Seata 等分布式事务方式实现数据最终一致性。

  - 分库**可能**导致执行一次事务所需的数据分布在不同服务器上，数据库层面无法实现事务性操作，需要更上层业务引入分布式事务操作，难免会给业务带来一定复杂性，那么要想解决事务性问题一般有两种手段：

    

- **主键(自增 ID)唯一性问题**

- - 在数据库表设计时，经常会使用自增 ID 作为数据主键，这就导致后续在迁库迁表、或者分库分表操作时，会因为主键的变化或者主键不唯一产生冲突，要解决主键不唯一问题，有如下方案：
  - 方案一：自增 ID 做主键时，设置自增步长，采用等差数列递增，避免各个库表的主键冲突。但是这个方案仍然无法解决迁库迁表、以及分库分表扩容导致主键 ID 变化问题
  - 方案二：主键采用全局统一 ID 生成机制：如 UUID、雪花算法、数据库号段等方式。

- **跨库多表 join 问题**

- - 首先来自大厂 DBA 的建议是，线上服务尽可能不要有表的 join 操作，join 操作往往会给后续的分库分表操作带来各种问题，可能导致数据的死锁。可以采用多次查询业务层进行数据组装(需要考虑业务上多次查询的事务性的容忍度)

- **跨库聚合查询问题**

分库分表会导致常规聚合查询操作，如 group by，order by 等变的异常复杂。需要复杂的业务代码才能实现上述业务逻辑，其常见操作方式有：

§ 方案一：赛道赛马机制，每次从 N 个库表中查询出 TOP N 数据，然后在业务层代码中进行聚合合并操作。

```
§  假设： 以2库1表为例，每次分页查询N条数据。
§
§  第一次查询：
§  ① 每个表中分别查询出N条数据：
§  SELECT * FROM db1_table1 where $col > 0 order by $col   LIMITT  0,N
§  SELECT * FROM db2_table1 where $col > 0 order by $col   LIMITT  0,N
§  ② 业务层代码对上述两者做归并排序，假设最终取db1数据K1条，取db2数据K2条，则K1+K2 = N
§  此时的DB1 可以计算出OffSet为K1 ，DB2计算出Offset为K2
§  将获取的N条数据以及相应的Offset  K1/K2返回给 端上。
§
§  第二次查询：
§  ① 端上将上一次查询对应的数据库的Offset  K1/K2 传到后端
§  ② 后端根据Offset构造查询语句查询分别查询出N条语句
§  SELECT * FROM db1_table1 where $col > 0 order by $col   LIMITT  $K1,N
§  SELECT * FROM db2_table1 where $col > 0 order by $col   LIMITT  $K2,N
§  ③ 再次使用归并排序，获取TOP N数据，将获取的N条数据以及相应的Offset  K1/K2返回给 端上。
§
§  第三次查询:
依次类推.......
```

§ 方案二：可以将经常使用到 groupby,orderby 字段存储到一个单一库表(可以是 REDIS、ES、MYSQL)中，业务代码中先到单一表中根据查询条件查询出相应数据，然后根据查询到的主键 ID，到分库分表中查询详情进行返回。2 次查询操作难点会带来接口耗时的增加，以及极端情况下的数据不一致问题。



### 5.什么是好的分库分表方案？

- **满足业务场景需要**：根据业务场景的不同选择不同分库分表方案：比如按照时间划分、按照用户 ID 划分、按照业务能力划分等

- **方案可持续性**：

- - 何为可持续性？其实就是：业务数据量级和流量量级未来进一步达到新的量级的时候，我们的分库分表方案可以持续灵活扩容处理。

- **最小化数据迁移**：扩容时一般涉及到历史数据迁移，其扩容后需要迁移的数据量越小其可持续性越强，理想的迁移前后的状态是（同库同表>同表不同库>同库不同表>不同库不同表）

- **数据偏斜**：数据在库表中分配的均衡性，尽可能保证数据流量在各个库表中保持等量分配，避免热点数据对于单库造成压力。

- - 最大数据偏斜率：（数据量最大样本 - 数据量最小样本）/ 数据量最小样本。一般来说，如果我们的最大数据偏斜率在 5%以内是可以接受的。



### 6.如何分库分表

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4kb0OqpTPHib4yazrLDll9KIBTtDib1Y2F3m54hjuSSGBXNZBTGqNevy2Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

#### 6.1 垂直拆分：

- 垂直拆表

- - 即大表拆小表，将一张表中数据不同”字段“分拆到多张表中，比如商品库将商品基本信息、商品库存、卖家信息等分拆到不同库表中。
  - 考虑因素有将**不常用**的，**数据较大**，**长度较长**（比如 text 类型字段）的拆分到“扩展表“，表和表之间通过”主键外键“进行关联。
  - 好处：降低表数据规模，提升查询效率，也避免查询时数据量太大造成的“跨页”问题。

- 垂直拆库

- - 垂直拆库则在垂直拆表的基础上，将一个系统中的不同业务场景进行拆分，比如订单表、用户表、商品表。
  - 好处：降低单数据库服务的压力（物理存储、内存、IO 等）、降低单机故障的影响面

#### 6.2 水平拆分：

- 操作：将总体数据按照某种维度(时间、用户)等分拆到多个库中或者表中，典型特征不同的库和表结构完全一下，如订单按照(日期、用户 ID、区域)分库分表。

- 水平拆表

- - 将数据按照某种维度拆分为多张表，但是由于多张表还是从属于**一个库**，其降低**锁粒度**，一定程度提升查询性能，但是仍然会有 IO 性能瓶颈。

- 水平拆库

- - 将数据按照某种维度分拆到多个库中，降低单机单库的压力，提升读写性能。

#### 6.3 **常见水平拆分手段**

##### 6.3.1 **range 分库分表**

顾名思义，该方案根据数据范围划分数据的存放位置。

###### 6.3.1.1 思路一：时间范围分库分表

举个最简单例子，我们可以把订单表按照年份为单位，每年的数据存放在单独的库（或者表）中。

时下非常流行的分布式数据库：TiDB 数据库，针对 TiKV 中数据的打散，也是基于 Range 的方式进行，将不同范围内的[StartKey,EndKey)分配到不同的 Region 上。

缺点：

- 需要提前建库或表。
- 数据热点问题：当前时间的数据会集中落在某个库表。
- 分页查询问题：涉及到库表中间分界线查询较复杂。

例子：交易系统流水表则是按照天级别分表。

##### 6.3.2 **hash 分库分表**

hash 分表是使用最普遍的使用方式，其根据“主键”进行 hash 计算数据存储的库表索引。原理可能大家都懂，但有时拍脑袋决定的分库分表方案可能会导致严重问题。

###### 6.3.2.1 **思路一：独立 hash**

对于分库分表，最常规的一种思路是通过主键计算 hash 值，然后 hash 值分别对库数和表数进行取余操作获取到库索引和表索引。比如：电商订单表，按照用户 ID 分配到 10 库 100 表中。

```
const (
        // DbCnt 库数量
        DbCnt = 10
        // TableCnt 表数量
        TableCnt = 100
)

// GetTableIdx 根据用户 ID 获取分库分表索引
func GetTableIdx(userID int64) (int64, int64) {
    hash := hashCode(userID)
        return hash % DbCnt, hash % TableCnt
}
```

上述是伪代码实现，大家可以先思考一下上述代码可能会产生什么问题？

比如 1000? 1010?，1020 库表索引是多少？

思考一下........

思考一下........

思考一下........

思考一下........

思考一下........

思考一下........

答：数据偏斜问题。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4kRpn6Tia94ialUTYjr0scxhmiaUQtvoev5sofeA0vE8dv8TazNhVZHTibTg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

非互质关系导致的数据偏斜问题证明：

```
假设分库数分表数最大公约数为a，则分库数表示为 m*a , 分表数为 n*a （m,n为正整数）

某条数据的hash规则计算的值为H，

若某条数据在库D中，则H mod (m*a) == D 等价与  H=M*m*a+D （M为整数）

则表序号为 T = H % (n*a) = (M*m*a+D)%(n*a)

如果D==0 则T= [(M*m)%n]*a
```

###### 6.3.2.2 **思路二：统一 hash**

思路一中，由于库和表的 hash 计算中存在公共因子，导致数据偏斜问题，那么换种思考方式：10 个库 100 张表，一共 1000 张表，那么从 0 到 999 排序，根据 hash 值对 1000 取余，得到[0,999]的索引，似乎就可以解决数据偏斜问题：

```
// GetTableIdx 根据用户 ID 获取分库分表索引
// 例子：1123011 -> 1,1
func GetTableIdx(userID int64) (int64, int64) {
    hash := hashCode(userID)
    slot := DbCnt * TableCnt
        return hash % slot % DbCnt, hash % slot / DbCnt
}
```

上面会带来的问题？

比如 1123011 号用户，扩容前是 1 库 1 表，扩容后是 0 库 11 表

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4kBvEPafQaEgGezjNLicmVM5icSOTuJaA51zhI6Gic2LxU03gdSERgLmGrg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

扩展性问题证明。

```
某条数据的hash规则计算的值为H，分库数为D，分表数为T

扩容前：
分片序号K1 = H % (D*T),则H = M*DT + K1 ，且K1 一定是小于（D*T）
D1 = K1 % D
T1 = K1 / D

扩容后：
如果M为偶数，即M= 2*N
K2 = H% (2DT) = (2NDT+K1)%(2DT) = K1%(2DT) ,K1 一定小于（2DT）,所以K2=K1
D2 = K2%（2D） = K1 %（2D）
T2 = K2/(2D) = K1 / （2D）

如果M为奇数，即M = 2*N+1
K2 = H%（2DT） = (2NDT +DT +K1)%(2DT) = (DT+K1)%(2DT) = DT + K1
D2 = K2 %(2D) = (DT+K1) % (2D)
T2 = K2 /(2D) = (DT+K1) / (2D)

结论：扩容后库序号和表序号都变化
```

###### 6.3.2.3 **思路三：二次分片法**

思路二中整体思路正确，只是最后计算库序号和表序号的时候，使用了库数量作为影响表序号的因子，导致扩容时表序号偏移而无法进行。事实上，我们只需要换种写法，就能得出一个比较大众化的分库分表方案。

```
func GetTableIdx(userId int64){
        //①算Hash
        hash:=hashCode(userId)
        //②分片序号
        slot:=hash%(DbCnt*TableCnt)
        //③重新修改二次求值方案
        dbIdx:=slot/TableCnt
        tblIdx:=slot%TableCnt
        return dbIdx,tblIdx
}
```

从上述代码中可以看出，其唯一不同是在计算库索引和表索引时，采用 TableCnt 作为基数（注：扩容操作时，一般采用库个数 2 倍扩容），这样在扩容时，表个数不变，则表索引不会变。

可以做简要的证明：

```
某条数据的hash规则计算的值为H，分库数为D，分表数为T

扩容前：
分片序号K1 = H % (D*T),则H =  M*DT + K1 ，且K1 一定是小于（D*T）
D1 = K1 / T
T1 = K1 % T

扩容后：
如果M为偶数，即M= 2*N
K2 =  H% (2DT) = (2NDT+K1)%(2DT) = K1%(2DT) ,K1 一定小于（2DT）,所以K2=K1
D2 = K2/T  = K1 /T = D1
T2 = K2%T = K1 % T = T1

如果M为奇数，即M = 2*N+1
K2 = H%（2DT） = (2NDT +DT +K1)%(2DT) = (DT+K1)%(2DT) = DT + K1
D2 = K2 /T = (DT+K1) / T = D + K1/T = D + D1
T2 = K2 %T = (DT+K1) % T = K1 %T = T1

结论：
M为偶数时，扩容前后库序号和表序号都不变
M为奇数时，扩容前后表序号不变，库序号会变化。
```

###### **6.3.2.4 思路四：基因法**

由思路二启发，我们发现案例一不合理的主要原因，就是因为库序号和表序号的计算逻辑中，有公约数这个因子在影响库表的独立性。那么我们是否可以换一种思路呢？我们使用相对独立的 Hash 值来计算库序号和表序号呢？

```
func GetTableIdx(userID int64)(int64,int64){
        hash := hashCode(userID)
        return atoi(hash[0:4]) % DbCnt,atoi(hash[4:])%TableCnt
}
```

这也是一种常用的方案，我们称为基因法，即使用原分片键中的某些基因（例如前四位）作为库的计算因子，而使用另外一些基因作为表的计算因子。

在使用基因法时，要主要计算 hash 值的片段保持充分的随机性，避免造成严重数据偏斜问题。

###### 6.3.2.5 **思路五：关系表冗余**

按照索引的思想，可以通过分片的键和库表索引建立一张索引表，我们把这张索引表叫做“路由关系表”。每次查询操作，先去路由表中查询到数据所在的库表索引，然后再到库表中查询详细数据。同时，对于写入操作可以采用随机选择或者顺序选择一个库表进入写入。

那么由于路由关系表的存在，我们在数据扩容时，无需迁移历史数据。同时，我们可以为每个库表指定一个权限，通过权重的比例调整来调整每个库表的写入数据量。从而实现库表数据偏斜率调整。

此种方案的缺点是每次查询操作，需要先读取一次路由关系表，所以请求耗时可能会有一定增加。本身由于写索引表和写库表操作是不同库表写操作，需要引入分布式事务保证数据一致性，极端情况可能带来数据的不一致。

且索引表本身没有分库分表，自身可能会存在性能瓶颈，可以通过存储在 redis 进行优化处理。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4kBYhPNdLO8Omxnk08Om1iag8KQvwJ9HBwRxqhcyW4uXEiarrXd01xCRgg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

###### 6.3.2.6 **思路六：分段索引关系表**

思路五中，需要将全量数据存在到路由关系表中建立索引，再结合 range 分库分表方案思想，其实有些场景下完全没有必要全部数据建立索引，可以按照号段式建立区间索引，我们可以将分片键的区间对应库的关系通过关系表记录下来，每次查询操作，先去路由表中查询到数据所在的库表索引，然后再到库表中查询详细数据。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvaty2bicRwSMXohe1GFDfCH4k8oEVZ8Uia1atVca7YoKAg8PcEdW6QgaVH3ibkWCM8IdQj7MicklZ4vcrg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

###### 6.3.2.7 **思路七：一致性 Hash 法**

一致性 Hash 算法也是一种比较流行的集群数据分区算法，比如 RedisCluster 即是通过一致性 Hash 算法，使用 16384 个虚拟槽节点进行每个分片数据的管理。关于一致性 Hash 的具体原理这边不再重复描述，读者可以自行翻阅资料。

其思想和思路五有异曲同工之妙。

### **7.总结**

本文从 5W1H 角度介绍了分库分表手段，其在**解决**如 IO 瓶颈、读写性能、物理存储瓶颈、内存瓶颈、单机故障影响面等**问题的同时**，**也带来**如事务性、主键冲突、跨库 join、跨库聚合查询**等问题**。anyway，在综合业务场景考虑，正如缓存的使用一样，非必须使用分库分表，则不应过度设计采用分库分表方案。如数据库确实成为性能瓶颈时，在设计分库分表方案时也应充分考虑方案的扩展性。或者说可以考虑采用成熟热门的分布式数据库解决方案，如 TiDB。

原文作者：tayroctang，腾讯 PCG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/lSxdx2QT3F9lZPZMxroQFA

# 【NO.361】Linux下跨语言调用C++实践

## 1. 背景

查询理解（QU, Query Understanding）是美团搜索的核心模块，主要职责是理解用户查询，生成查询意图、成分、改写等基础信号，应用于搜索的召回、排序、展示等多个环节，对搜索基础体验至关重要。该服务的线上主体程序基于C++语言开发，服务中会加载大量的词表数据、预估模型等，这些数据与模型的离线生产过程有很多文本解析能力需要与线上服务保持一致，从而保证效果层面的一致性，如文本归一化、分词等。

而这些离线生产过程通常用Python与Java实现。如果在线、离线用不同语言各自开发一份，则很难维持策略与效果上的统一。同时这些能力会有不断的迭代，在这种动态场景下，不断维护多语言版本的效果打平，给我们的日常迭代带来了极大的成本。因此，我们尝试通过跨语言调用动态链接库的技术解决这个问题，即开发一次基于C++的so，通过不同语言的链接层封装成不同语言的组件库，并投入到对应的生产过程。这种方案的优势非常明显，主体的业务逻辑只需要开发一次，封装层只需要极少量的代码，主体业务迭代升级，其它语言几乎不需要改动，只需要包含最新的动态链接库，发布最新版本即可。同时C++作为更底层的语言，在很多场景下，它的计算效率更高，硬件资源利用率更高，也为我们带来了一些性能上的优势。

本文对我们在实际生产中尝试这一技术方案时，遇到的问题与一些实践经验做了完整的梳理，希望能为大家提供一些参考或帮助。

## 2. 方案概述

为了达到业务方开箱即用的目的，综合考虑C++、Python、Java用户的使用习惯，我们设计了如下的协作结构：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7icEdm9KuDfAp3iaVXGnaKW58I9ibQZ7LQ2C9v1gZayCOawaphgOB1LiawQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图 1

## 3. 实现详情

Python、Java支持调用C接口，但不支持调用C++接口，因此对于C++语言实现的接口，必须转换为C语言实现。为了不修改原始C++代码，在C++接口上层用C语言进行一次封装，这部分代码通常被称为“胶水代码”(Glue Code)。具体方案如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7wSxeETdIwW7tSSLMhe34rhw9iaH4D5069k4BB41XDIwuRaTcrRJibPZw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图 2

本章节各部分内容如下：

- 【功能代码】部分，通过打印字符串的例子来讲述各语言部分的编码工作。
- 【打包发布】部分，介绍如何将生成的动态库作为资源文件与Python、Java代码打包在一起发布到仓库，以降低使用方的接入成本。
- 【业务使用】部分，介绍开箱即用的使用示例。
- 【易用性优化】部分，结合实际使用中遇到的问题，讲述了对于Python版本兼容，以及动态库依赖问题的处理方式。

### 3.1 功能代码

#### 3.1.1 C++代码

作为示例，实现一个打印字符串的功能。为了模拟实际的工业场景，对以下代码进行编译，分别生成动态库 libstr_print_cpp.so、静态库libstr_print_cpp.a。

str_print.h

```
#pragma once
#include <string>
class StrPrint {
 public:
    void print(const std::string& text);
};
```

str_print.cpp

```
#include <iostream>
#include "str_print.h"
void StrPrint::print(const std::string& text) {
    std::cout << text << std::endl;
}
```

#### 3.1.2 c_wrapper代码

如上文所述，需要对C++库进行封装，改造成对外提供C语言格式的接口。

c_wrapper.cpp

```
#include "str_print.h"
extern "C" {
void str_print(const char* text) {
    StrPrint cpp_ins;
    std::string str = text;
    cpp_ins.print(str);
}
}
```

#### 3.1.3 生成动态库

为了支持Python与Java的跨语言调用，我们需要对封装好的接口生成动态库，生成动态库的方式有以下三种。

- **方式一**：源码依赖方式，将c_wrapper和C++代码一起编译生成libstr_print.so。这种方式业务方只需要依赖一个so，使用成本较小，但是需要获取到C++源码。对于一些现成的动态库，可能不适用。

```
g++ -o libstr_print.so str_print.cpp c_wrapper.cpp -fPIC -shared
```

- **方式二**：动态链接方式，这种方式生成的libstr_print.so，发布时需要携带上其依赖库libstr_print_cpp.so。业务方需要同时依赖两个so，使用的成本相对要高，但是不必提供原动态库的源码。

```
g++ -o libstr_print.so c_wrapper.cpp -fPIC -shared -L. -lstr_print_cpp
```

- **方式三**：静态链接方式，这种方式生成的libstr_print.so，发布时无需携带上libstr_print_cpp.so。业务方只需依赖一个so，不必依赖源码，但是需要提供静态库。

```
g++ c_wrapper.cpp libstr_print_cpp.a -fPIC -shared -o libstr_print.so
```

上述三种方式，各自有适用场景和优缺点。在我们本次的业务场景下，因为工具库与封装库均由我们自己开发，能够获取到源码，因此选择第一种方式，业务方依赖更加简单。

#### 3.1.4 Python接入代码

Python标准库自带的ctypes可以实现加载C的动态库的功能，使用方法如下：

str_print.py

```
# -*- coding: utf-8 -*-
import ctypes
# 加载 C lib
lib = ctypes.cdll.LoadLibrary("./libstr_print.so")
# 接口参数类型映射
lib.str_print.argtypes = [ctypes.c_char_p]
lib.str_print.restype = None
# 调用接口
lib.str_print('Hello World')
```

LoadLibrary会返回一个指向动态库的实例，通过它可以在Python里直接调用该库中的函数。argtypes与restype是动态库中函数的参数属性，前者是一个ctypes类型的列表或元组，用于指定动态库中函数接口的参数类型，后者是函数的返回类型（默认是c_int，可以不指定，对于非c_int型需要显示指定）。该部分涉及到的参数类型映射，以及如何向函数中传递struct、指针等高级类型，可以参考附录中的文档。

#### 3.1.5 Java接入代码

Java调用C lib有JNI与JNA两种方式，从使用便捷性来看，更推荐JNA方式。

##### 3.1.5.1 JNI接入

Java从1.1版本开始支持JNI接口协议，用于实现Java语言调用C/C++动态库。JNI方式下，前文提到的c_wrapper模块不再适用，JNI协议本身提供了适配层的接口定义，需要按照这个定义进行实现。JNI方式的具体接入步骤为：

Java代码里，在需要跨语言调用的方法上，增加native关键字，用以声明这是一个本地方法。

```
import java.lang.String;
public class JniDemo {
    public native void print(String text);
}
```

通过javah命令，将代码中的native方法生成对应的C语言的头文件。这个头文件类似于前文提到的c_wrapper作用。

```
javah JniDemo
```

得到的头文件如下（为节省篇幅，这里简化了一些注释和宏）：

```
#include <jni.h>
#ifdef __cplusplus
extern "C" {
#endif
JNIEXPORT void JNICALL Java_JniDemo_print
  (JNIEnv *, jobject, jstring);
#ifdef __cplusplus
}
#endif
```

jni.h在JDK中提供，其中定义了Java与C语言调用所必需的相关实现。


JNIEXPORT和JNICALL是JNI中定义的两个宏，JNIEXPORT标识了支持在外部程序代码中调用该动态库中的方法，JNICALL定义了函数调用时参数的入栈出栈约定。


Java_JniDemo_print是一个自动生成的函数名，它的格式是固定，由Java_{className}_{methodName}构成，JNI会按照这个约定去注册Java方法与C函数的映射。


三个参数里，前两个是固定的。JNIEnv中封装了jni.h里的一些工具方法，jobject指向Java中的调用类，即JniDemo，通过它可以找到Java里class中的成员变量在C的堆栈中的拷贝。jstring指向传入参数text，这是对于Java中String类型的一个映射。有关类型映射的具体内容，会在后文详细展开。


编写实现Java_JniDemo_print方法。

JniDemo.cpp

```
#include <string>
#include "JniDemo.h"
#include "str_print.h"
JNIEXPORT void JNICALL Java_JniDemo_print (JNIEnv *env, jobject obj, jstring text)
{
    char* str=(char*)env->GetStringUTFChars(text,JNI_FALSE);
    std::string tmp = str;
    StrPrint ins;
    ins.print(tmp);
}
```

编译生成动态库。

```
g++ -o libJniDemo.so JniDemo.cpp str_print.cpp -fPIC -shared -I<$JAVA_HOME>/include/ -I<$JAVA_HOME>/include/linux
```

编译运行。

```
java -Djava.library.path=<path_to_libJniDemo.so> JniDemo
```

JNI机制通过一层C/C++的桥接，实现了跨语言调用协议。这一功能在Android系统中一些图形计算相关的Java程序下有着大量应用。一方面能够通过Java调用大量操作系统底层库，极大的减少了JDK上的驱动开发的工作量，另一方面能够更充分的利用硬件性能。但是通过3.1.5.1中的描述也可以看到，JNI的实现方式本身的实现成本还是比较高的。尤其桥接层的C/C++代码的编写，在处理复杂类型的参数传递时，开发成本较大。为了优化这个过程，Sun公司主导了JNA(Java Native Access)开源工程的工作。

##### 3.1.5.2 JNA接入

JNA是在JNI基础上实现的编程框架，它提供了C语言动态转发器，实现了Java类型到C类型的自动转换。因此，Java开发人员只要在一个Java接口中描述目标native library的函数与结构，不再需要编写任何Native/JNI代码，极大的降低了Java调用本地共享库的开发难度。

JNA的使用方法如下：

在Java项目中引入JNA库。

```
<dependency>
  <groupId>com.sun.jna</groupId>
  <artifactId>jna</artifactId>
  <version>5.4.0</version>
</dependency>
```

声明与动态库对应的Java接口类。

```
public interface CLibrary extends Library {
    void str_print(String text); // 方法名和动态库接口一致，参数类型需要用Java里的类型表示，执行时会做类型映射，原理介绍章节会有详细解释
}
```

加载动态链接库，并实现接口方法。

JnaDemo.java

```
package com.jna.demo;
import com.sun.jna.Library;
import com.sun.jna.Native;
public class JnaDemo {
    private CLibrary cLibrary;
    public interface CLibrary extends Library {
        void str_print(String text);
    }

    public JnaDemo() {
        cLibrary = Native.load("str_print", CLibrary.class);
    }

    public void str_print(String text)
    {
        cLibrary.str_print(text);
    }
}
```

对比可以发现，相比于JNI，JNA不再需要指定native关键字，不再需要生成JNI部分C代码，也不再需要显示的做参数类型转化，极大地提高了调用动态库的效率。

### 3.2 打包发布

为了做到开箱即用，我们将动态库与对应语言代码打包在一起，并自动准备好对应依赖环境。这样使用方只需要安装对应的库，并引入到工程中，就可以直接开始调用。这里需要解释的是，我们没有将so发布到运行机器上，而是将其和接口代码一并发布至代码仓库，原因是我们所开发的工具代码可能被不同业务、不同背景（非C++）团队使用，不能保证各个业务方团队都使用统一的、标准化的运行环境，无法做到so的统一发布、更新。

#### 3.2.1 Python 包发布

Python可以通过setuptools将工具库打包，发布至pypi公共仓库中。具体操作方法如下：

创建目录。

```
  .
  ├── MANIFEST.in            #指定静态依赖
  ├── setup.py               # 发布配置的代码
  └── strprint               # 工具库的源码目录
      ├── __init__.py        # 工具包的入口
      └── libstr_print.so    # 依赖的c_wrapper 动态库
```

编写__init__.py， 将上文代码封装成方法。

```
  # -*- coding: utf-8 -*-
  import ctypes
  import os
  import sys
  dirname, _ = os.path.split(os.path.abspath(__file__))
  lib = ctypes.cdll.LoadLibrary(dirname + "/libstr_print.so")
  lib.str_print.argtypes = [ctypes.c_char_p]
  lib.str_print.restype = None
  def str_print(text):
      lib.str_print(text)
```

编写setup.py。

```
  from setuptools import setup, find_packages
  setup(
      name="strprint",
      version="1.0.0",
      packages=find_packages(),
      include_package_data=True,
      description='str print',
      author='xxx',
      package_data={
          'strprint': ['*.so']
      },
  )
```

编写MANIFEST.in。

```
include strprint/libstr_print.so
```

打包发布。

```
python setup.py sdist upload
```

#### 3.2.2 Java接口

对于Java接口，将其打包成JAR包，并发布至Maven仓库中。

编写封装接口代码JnaDemo.java。

```
  package com.jna.demo;
  import com.sun.jna.Library;
  import com.sun.jna.Native;
  import com.sun.jna.Pointer;
  public class JnaDemo {
      private CLibrary cLibrary;
      public interface CLibrary extends Library {
          Pointer create();
          void str_print(String text);
      }

      public static JnaDemo create() {
          JnaDemo jnademo = new JnaDemo();
          jnademo.cLibrary = Native.load("str_print", CLibrary.class);
          //System.out.println("test");
          return jnademo;
      }

      public void print(String text)
      {
          cLibrary.str_print(text);
      }
  }
```

创建resources目录，并将依赖的动态库放到该目录。

通过打包插件，将依赖的库一并打包到JAR包中。

```
  <plugin>
    <artifactId>maven-assembly-plugin</artifactId>
    <configuration>
        <appendAssemblyId>false</appendAssemblyId>
        <descriptorRefs>
            <descriptorRef>jar-with-dependencies</descriptorRef>
        </descriptorRefs>
    </configuration>
    <executions>
        <execution>
            <id>make-assembly</id>
            <phase>package</phase>
            <goals>
                <goal>assembly</goal>
            </goals>
        </execution>
    </executions>
  </plugin>
```

### 3.3 业务使用

#### 3.3.1 Python使用

安装strprint包。

```
  pip install strprint==1.0.0
```

使用示例：

```
  # -*- coding: utf-8 -*-
  import sys
  from strprint import *
  str_print('Hello py')
```

#### 3.3.2 Java使用

pom引入JAR包。

```
  <dependency>
      <groupId>com.jna.demo</groupId>
      <artifactId>jnademo</artifactId>
      <version>1.0</version>
  </dependency>
```

使用示例：

```
  JnaDemo jnademo = new JnaDemo();
  jnademo.str_print("hello jna");
```

### 3.4 易用性优化

#### 3.4.1 Python版本兼容

Python2与Python3版本的问题，是Python开发用户一直诟病的槽点。因为工具面向不同的业务团队，我们没有办法强制要求使用统一的Python版本，但是我们可以通过对工具库做一下简单处理，实现两个版本的兼容。Python版本兼容里，需要注意两方面的问题：

- 语法兼容
- 数据编码

Python代码的封装里，基本不牵扯语法兼容问题，我们的工作主要集中在数据编码问题上。由于Python 3的str类型使用的是unicode编码，而在C中，我们需要的char* 是utf8编码，因此需要对于传入的字符串做utf8编码处理，对于C语言返回的字符串，做utf8转换成unicode的解码处理。于是对于上例子，我们做了如下改造：

```
# -*- coding: utf-8 -*-
import ctypes
import os
import sys
dirname, _ = os.path.split(os.path.abspath(__file__))
lib = ctypes.cdll.LoadLibrary(dirname + "/libstr_print.so")
lib.str_print.argtypes = [ctypes.c_char_p]
lib.str_print.restype = None
def is_python3():
    return sys.version_info[0] == 3

def encode_str(input):
    if is_python3() and type(input) is str:
        return bytes(input, encoding='utf8')
    return input

def decode_str(input):
    if is_python3() and type(input) is bytes:
        return input.decode('utf8')
    return input

def str_print(text):
    lib.str_print(encode_str(text))
```

#### 3.4.2 依赖管理

在很多情况下，我们调用的动态库，会依赖其它动态库，比如当我们依赖的gcc/g++版本与运行环境上的不一致时，时常会遇到`glibc_X.XX not found`的问题，这时需要我们提供指定版本的`libstdc.so`与`libstdc++.so.6`。

为了实现开箱即用的目标，在依赖并不复杂的情况下，我们会将这些依赖也一并打包到发布包里，随工具包一起提供。对于这些间接依赖，在封装的代码里，并不需要显式的load，因为Python与Java的实现里，加载动态库，最终调用的都是系统函数dlopen。这个函数在加载目标动态库时，会自动的加载它的间接依赖。所以我们所需要做的，就只是将这些依赖放置到dlopen能够查找到路径下。

dlopen查找依赖的顺序如下：

1. 从dlopen调用方ELF(Executable and Linkable Format)的DT_RPATH所指定的目录下寻找，ELF是so的文件格式，这里的DT_RPATH是写在动态库文件的，常规手段下，我们无法修改这个部分。
2. 从环境变量LD_LIBRARY_PATH所指定的目录下寻找，这是最常用的指定动态库路径的方式。
3. 从dlopen调用方ELF的DT_RUNPATH所指定的目录下寻找，同样是在so文件中指定的路径。
4. 从/etc/ld.so.cache寻找，需要修改/etc/ld.so.conf文件构建的目标缓存，因为需要root权限，所以在实际生产中，一般很少修改。
5. 从/lib寻找， 系统目录，一般存放系统依赖的动态库。
6. 从/usr/lib寻找，通过root安装的动态库，同样因为需要root权限，生产中，很少使用。

从上述查找顺序中可以看出，对于依赖管理的最好方式，是通过指定LD_LIBRARY_PATH变量的方式，使其包含我们的工具包中的动态库资源所在的路径。另外，对于Java程序而言，我们也可以通过指定java.library.path运行参数的方式来指定动态库的位置。Java程序会将java.library.path与动态库文件名拼接到一起作为绝对路径传递给dlopen，其加载顺序排在上述顺序之前。



最后，在Java中还有一个细节需要注意，我们发布的工具包是以JAR包形式提供，JAR包本质上是一个压缩包，在Java程序中，我们能够直接通过Native.load()方法，直接加载位于项目resources目录里的so，这些资源文件打包后，会被放到JAR包中的根目录。



但是dlopen无法加载这个目录。对于这一问题，最好的方案可以参考【3.1.3生成动态库】一节中的打包方法，将依赖的动态库合成一个so，这样无须做任何环境配置，开箱即用。但是对于诸如libstdc++.so.6等无法打包在一个so的中系统库，更为通用的做法是，在服务初始化时将so文件从JAR包中拷贝至本地某个目录，并指定LD_LIBRARY_PATH包含该目录。

## 4. 原理介绍

### 4.1 为什么需要一个c_wrapper

实现方案一节中提到Python/Java不能直接调用C++接口，要先对C++中对外提供的接口用C语言的形式进行封装。这里根本原因在于使用动态库中的接口前，需要根据函数名查找接口在内存中的地址，动态库中函数的寻址通过系统函数dlsym实现，dlsym是严格按照传入的函数名寻址。

在C语言中，函数签名即为代码函数的名称，而在C++语言中，因为需要支持函数重载，可能会有多个同名函数。为了保证签名唯一，C++通过name mangling机制为相同名字不同实现的函数生成不同的签名，生成的签名会是一个像__Z4funcPN4printE这样的字符串，无法被dlsym识别（注：Linux系统下可执行程序或者动态库多是以ELF格式组织二进制数据，其中所有的非静态函数(non-static)以“符号(symbol)”作为唯一标识，用于在链接过程和执行过程中区分不同的函数，并在执行时映射到具体的指令地址，这个“符号”我们通常称之为函数签名）。

为了解决这个问题，我们需要通过extern "C" 指定函数使用C的签名方式进行编译。因此当依赖的动态库是C++库时，需要通过一个c_wrapper模块作为桥接。而对于依赖库是C语言编译的动态库时，则不需要这个模块，可以直接调用。

### 4.2 跨语言调用如何实现参数传递

C/C++函数调用的标准过程如下：

1. 在内存的栈空间中为被调函数分配一个栈帧，用来存放被调函数的形参、局部变量和返回地址。
2. 将实参的值复制给相应的形参变量（可以是指针、引用、值拷贝）。
3. 控制流转移到被调函数的起始位置，并执行。
4. 控制流返回到函数调用点，并将返回值给到调用方，同时栈帧释放。

由以上过程可知，函数调用涉及内存的申请释放、实参到形参的拷贝等，Python/Java这种基于虚拟机运行的程序，在其虚拟机内部也同样遵守上述过程，但涉及到调用非原生语言实现的动态库程序时，调用过程是怎样的呢？

由于Python/Java的调用过程基本一致，我们以Java的调用过程为例来进行解释，对于Python的调用过程不再赘述。

#### 4.2.1 内存管理

在Java的世界里，内存由JVM统一进行管理，JVM的内存由栈区、堆区、方法区构成，在较为详细的资料中，还会提到native heap与native stack，其实这个问题，我们不从JVM的角度去看，而是从操作系统层面出发来理解会更为简单直观。以Linux系统下为例，首先JVM名义上是一个虚拟机，但是其本质就是跑在操作系统上的一个进程，因此这个进程的内存会存在如下左图所示划分。而JVM的内存管理实质上是在进程的堆上进行重新划分，自己又“虚拟”出Java世界里的堆栈。如右图所示，native的栈区就是JVM进程的栈区，进程的堆区一部分用于JVM进行管理，剩余的则可以给native方法进行分配使用。



![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7wDmd3mRKyefN4mXUH08HYUjAuJtsicOHiaibFZaPiamAcXibu25XGU8zVMw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图 3

#### 4.2.2 调用过程

前文提到，native方法调用前，需要将其所在的动态库加载到内存中，这个过程是利用Linux的dlopen实现的，JVM会把动态库中的代码片段放到Native Code区域，同时会在JVM Bytecode区域保存一份native方法名与其所在Native Code里的内存地址映射。

一次native方法的调用步骤，大致分为四步：

1. 从JVM Bytecode获取native方法的地址。
2. 准备方法所需的参数。
3. 切换到native栈中，执行native方法。
4. native方法出栈后，切换回JVM方法，JVM将结果拷贝至JVM的栈或堆中。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7Efq11gxvOOibuAMvwJp9ndUQ3K11viafp4mhRS0vta6a4m5xMyGkxTEQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图 4

由上述步骤可以看出，native方法的调用同样涉及参数的拷贝，并且其拷贝是建立在JVM堆栈和原生堆栈之间。

对于原生数据类型，参数是通过值拷贝方式与native方法地址一起入栈。而对于复杂数据类型，则需要一套协议，将Java中的object映射到C/C++中能识别的数据字节。原因是JVM与C语言中的内存排布差异较大，不能直接内存拷贝，这些差异主要包括：

- 类型长度不同，比如char在Java里为16比特，在C里面却是8个比特。
- JVM与操作系统的字节顺序（Big Endian还是Little Endian）可能不一致。
- JVM的对象中，会包含一些meta信息，而C里的struct则只是基础类型的并列排布，同样Java中没有指针，也需要进行封装和映射。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7gzDQRHIJbJEWfDWReC7AwSpxq2dzvkKP5NpyqTCgib7z0QQ4MHMIicgw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图 5

上图展示了native方法调用过程中参数传递的过程，其中映射拷贝在JNI中是由C/C++链接部分的胶水代码实现，类型的映射定义在jni.h中。

Java基本类型与C基本类型的映射（通过值传递。将Java对象在JVM内存里的值拷贝至栈帧的形参位置）：

```
typedef unsigned char   jboolean;
typedef unsigned short  jchar;
typedef short           jshort;
typedef float           jfloat;
typedef double          jdouble;
typedef jint            jsize;
```

Java复杂类型与C复杂类型的映射（通过指针传递。首先根据基本类型一一映射，将组装好的新对象的地址拷贝至栈帧的形参位置）：

```
typedef _jobject *jobject;
typedef _jclass *jclass;
typedef _jthrowable *jthrowable;
typedef _jstring *jstring;
typedef _jarray *jarray;
```

**注**：在Java中，非原生类型均是Object的派生类，多个object的数组本身也是一个object，每个object的类型是一个class，同时class本身也是一个object。

```
class _jobject {};
class _jclass : public _jobject {};
class _jthrowable : public _jobject {};
class _jarray : public _jobject {};
class _jcharArray : public _jarray {};
class _jobjectArray : public _jarray {};
```

jni.h 中配套提供了内存拷贝和读取的工具类，比如前面例子中的`GetStringUTFChars`能够将JVM中的字符串中的文本内容，按照utf8编码的格式，拷贝到native heap中，并将char*指针传递给native方法使用。

整个调用过程，产生的内存拷贝，Java中的对象由JVM的GC进行清理，Native Heap中的对象如果是由 JNI框架分配生成的，如上文JNI示例中的参数，均由框架进行统一释放。而在C/C++中新分配的对象，则需要用户代码在C/C++中手动释放。简而言之，Native Heap中与普通的C/C++进程一致，没有GC机制的存在，并且遵循着谁分配谁释放的内存治理原则。

### 4.3 扩展阅读（JNA直接映射）

相比于JNI，JNA使用了其函数调用的基础框架，其中的内存映射部分，由JNA工具库中的工具类自动化的完成类型映射和内存拷贝的大部分工作，从而避免大量胶水代码的编写，使用上更为友好，但相应的这部分工作则产生了一些性能上的损耗。

JNA还额外提供了一种“直接映射”(DirectMapping)的调用方式来弥补这一不足。但是直接映射对于参数有着较为严格的限制，只能传递原生类型、对应数组以及Native引用类型，并且不支持不定参数，方法返回类型只能是原生类型。

直接映射的Java代码中需要增加native关键字，这与JNI的写法一致。

DirectMapping示例

```
import com.sun.jna.*;
public class JnaDemo {
    public static native double cos(DoubleByReference x);
    static {
        Native.register(Platform.C_LIBRARY_NAME);
    }

    public static void main(String[] args) {
        System.out.println(cos(new DoubleByReference(1.0)));
    }
}
```

DoubleByReference即是双精度浮点数的Native引用类型的实现，它的JNA源码定义如下（仅截取相关代码）：

```
//DoubleByReference
public class DoubleByReference extends ByReference {
    public DoubleByReference(double value) {
        super(8);
        setValue(value);
    }
}

// ByReference
public abstract class ByReference extends PointerType {
    protected ByReference(int dataSize) {
        setPointer(new Memory(dataSize));
    }
}
```

Memory类型是Java版的shared_ptr实现，它通过引用引数的方式，封装了内存分配、引用、释放的相关细节。这种类型的数据内存实际上是分配在native的堆中，Java代码中，只能拿到指向该内存的引用。JNA在构造Memory对象的时候通过调用malloc在堆中分配新内存，并记录指向该内存的指针。

在ByReference的对象释放时，调用free，释放该内存。JNA的源码中ByReference基类的finalize 方法会在GC时调用，此时会去释放对应申请的内存。因此在JNA的实现中，动态库中的分配的内存由动态库的代码管理，JNA框架分配的内存由JNA中的代码显示释放，但是其触发时机，则是靠JVM中的GC机制释放JNA对象时来触发运行。这与前文提到的Native Heap中不存在GC机制，遵循谁分配谁释放的原则是一致的。

```
@Override
protected void finalize() {
    dispose();
}

/** Free the native memory and set peer to zero */
protected synchronized void dispose() {
    if (peer == 0) {
        // someone called dispose before, the finalizer will call dispose again
        return;
    }

    try {
        free(peer);
    } finally {
        peer = 0;
        // no null check here, tracking is only null for SharedMemory
        // SharedMemory is overriding the dispose method
        reference.unlink();
    }
}
```

### 4.4 性能分析

提高运算效率是Native调用中的一个重要目的，但是经过上述分析也不难发现，在一次跨语言本地化的调用过程中，仍然有大量的跨语言工作需要完成，这些过程也需要支出对应的算力。因此并不是所有Native调用，都能提高运算效率。为此我们需要理解语言间的性能差异在哪儿，以及跨语言调用需要耗费多大的算力支出。

语言间的性能差异主要体现在三个方面：

- Python与Java语言都是解释执行类语言，在运行时期，需要先把脚本或字节码翻译成二进制机器指令，再交给CPU进行执行。而C/C++编译执行类语言，则是直接编译为机器指令执行。尽管有JIT等运行时优化机制，但也只能一定程度上缩小这一差距。
- 上层语言有较多操作，本身就是通过跨语言调用的方式由操作系统底层实现，这一部分显然不如直接调用的效率高。
- Python与Java语言的内存管理机制引入了垃圾回收机制，用于简化内存管理，GC工作在运行时，会占用一定的系统开销。这一部分效率差异，通常以运行时毛刺的形态出现，即对平均运行时长影响不明显，但是对个别时刻的运行效率造成较大影响。

而跨语言调用的开销，主要包括三部分：

- 对于JNA这种由动态代理实现的跨语言调用，在调用过程中存在堆栈切换、代理路由等工作。
- 寻址与构造本地方法栈，即将Java中native方法对应到动态库中的函数地址，并构造调用现场的工作。
- 内存映射，尤其存在大量数据从JVM Heap向Native Heap 进行拷贝时，这部分的开销是跨语言调用的主要耗时所在。

我们通过如下实验简单做了一下性能对比，我们分别用C语言、Java、JNI、JNA以及JNA直接映射五种方式，分别进行100万次到1000万次的余弦计算，得到耗时对比。在6核16G机器，我们得到如下结果：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz71kUMJd5AErTXkibrRS6T02ia3PZbjrBLLoTicbRDSiau0nZYF39VOpVN8A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7VMvtlhAZMM50qp48NxFZQsv0ic9ibCB26lwvq7P8SQWpsvjibBHAPYtZg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图 6

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7djHOfQysvlLz1bZ8GnwmqvWfBtGGDebmyn0BQbWH8xtZNvQ1a28aPw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图 7

由实验数据可知，运行效率依次是 `C > Java > JNI > JNA DirectMapping > JNA`。C语言高于Java的效率，但两者非常接近。JNI与JNA DirectMapping的方式性能基本一致，但是会比原生语言的实现要慢很多。普通模式下的JNA的速度最慢，会比JNI慢5到6倍。

综上所述，跨语言本地化调用，并不总是能够提升计算性能，需要综合计算任务的复杂度和跨语言调用的耗时进行综合权衡。我们目前总结到的适合跨语言调用的场景有：

- **离线数据分析**：离线任务可能会涉及到多种语言开发，且对耗时不敏感，核心点在于多语言下的效果打平，跨语言调用可以节省多语言版本的开发成本。
- **跨语言RPC调用转换为跨语言本地化调用**：对于计算耗时是微秒级以及更小的量级的计算请求，如果通过RPC调用来获得结果，用于网络传输的时间至少是毫秒级，远大于计算开销。在依赖简单的情况下，转化为本地化调用，将大幅缩减单请求的处理时间。
- 对于一些复杂的模型计算，Python/Java跨语言调用C++可以提升计算效率。

## 5. 应用案例

如上文所述，通过本地化调用的方案能够在性能和开发成本上带来一些收益。我们将这些技术在离线任务计算与实时服务调用做了一些尝试，并取得了比较理想的结果。

### 5.1 离线任务中的应用

搜索业务中会有大量的词表挖掘、数据处理、索引构建等离线计算任务。这个过程会用到较多查询理解里的文本处理和识别能力，如分词、名命体识别等。因为开发语言的差异，将这些能力在本地重新开发一遍，成本上无法接受。因此之前的任务中，在离线计算过程中会通过RPC方式调用线上服务。这个方案带来如下问题：

- 离线计算任务的量级通常较大，执行过程中请求比较密集，会占用占用线上资源，影响线上用户请求，安全性较低。
- 单次RPC的耗时至少是毫秒级，而实际的计算时间往往非常短，因此大部分时间实际上浪费在了网络通信上，严重影响任务的执行效率。
- RPC服务因为网络抖动等因为，调用成功率不能达到100%，影响任务执行效果。
- 离线任务需引入RPC调用相关代码，在Python脚本等轻量级计算任务里，这部分的代码往往因为一些基础组件的不完善，导致接入成本较高。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7ALVpOf3L7hibmwp6GFJjNcmcwf0zNticZX5k8iaBibnYLKfUyNZictBPAjg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图 8

将RPC调用改造为跨语言本地化调用后，上述问题得以解决，收益明显。

- 不再调用线上服务，流量隔离，对线上安全不产生影响。
- 对于1000万条以上的离线任务，累计节省至少10小时以上的网络开销时间。
- 消除网络抖动导致的请求失败问题。
- 通过上述章节的工作，提供了开箱即用的本地化工具，极大的简化了使用成本。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7d37IeFdY5KD6jqZujInaKzwBCyeCfz4Kq84icPFEk2F0nrKZFDibicM4A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图 9

### 5.2 在线服务中的应用

查询理解作为美团内部的基础服务平台，提供分词词性、查询纠错、查询改写、地标识别、异地识别、意图识别、实体识别、实体链接等文本分析，是一个较大的CPU密集型服务，承接了公司内非常多的本文分析业务场景，其中有部分场景只是需要个别信号，甚至只需要查询理解服务中的基础函数组件，对于大部分是通过Java开发的业务服务，无法直接引用查询理解的C++动态库，此前一般是通过RPC调用获取结果。通过上述工作，在非C++语言的调用方服务中，可以将RPC调用转化为跨语言本地化调用，能够明显的提升调用端的性能以及成功率，同时也能有效减少服务端的资源开销。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7XhbibP37oiaFkhHZdZMm77thx3DNKPsjUNPPqlfWx1xHX4aPEbKWnlIw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图 10

## 6. 总结

微服务等技术的发展使得服务创建、发布和接入变得越来越简单，但是在实际工业生产中，并非所有场景都适合通过RPC服务完成计算。尤其在计算密集型和耗时敏感型的业务场景下，当性能成为瓶颈时，远程调用带来的网络开销就成了业务不可承受之痛。本文对语言本地化调用的技术进行了总结，并给出一些实践经验，希望能为大家解决类似的问题提供一些帮助。

当然，本次工作中还有许多不足，例如因为实际生产环境的要求，我们的工作基本都集中在Linux系统下，如果是以开放库形式，让使用方可以自由使用的话，可能还需要考虑兼容Windows下的DLL，Mac OS下的dylib等等。本文可能还存在其他不足之处，欢迎大家指留言指正、探讨。

本文例子的源代码请访问：[GitHub](https://github.com/linyang02/python_java_on_cpp)。

## 7. 参考文献

- [JNI内存相关文档](https://www.zhihu.com/question/438698030/answer/1670533755)
- [JNI类型映射](https://docs.oracle.com/javase/7/docs/technotes/guides/jni/spec/types.html#wp914)
- [JNA开源地址](https://github.com/java-native-access/jna)
- [Linux dlopen](https://man7.org/linux/man-pages/man3/dlopen.3.html)
- [Linux dlclose](https://man7.org/linux/man-pages/man3/dlclose.3p.html)
- [Linux dlsym](https://man7.org/linux/man-pages/man3/dlsym.3.html)
- [CPython源码](https://github.com/python/cpython)
- [CPython中ctypes的介绍](https://docs.python.org/zh-cn/3/library/ctypes.html#)
- [CTypes Struct实现](https://blog.csdn.net/mergerly/article/details/76858589)
- [Python项目分发打包](https://zhuanlan.zhihu.com/p/276461821)
- [本文所涉及的例子源码](https://github.com/linyang02/python_java_on_cpp)
- [C与C++函数签名](https://blog.csdn.net/lwj1396/article/details/5204484)
- [JNI，JNA与JNR性能对比](https://blog.csdn.net/weixin_34288121/article/details/88722310)

## 8. 本文作者

林阳、朱超、识瀚，均来自美团平台/搜索与NLP部/搜索技术部。

原文链接：https://mp.weixin.qq.com/s/vA69QGfZeKRlxZs_v_DQkw

# 【NO.362】数据库异常智能分析与诊断

## 1. 现状与问题

### 1.1 规模增长与运维能力发展之间的不平衡问题凸显

伴随着最近几年美团业务的快速发展，数据库的规模也保持着高速增长。而作为整个业务系统的“神经末梢”，数据库一旦出现问题，对业务造成的损失就会非常大。同时，因数据库规模的快速增长，出现问题的数量也大大增加，完全依靠人力的被动分析与定位已经不堪重负。下图是当时数据库实例近年来的增长趋势：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2x1KoT62DWAib7Sw5DticNkmwAYJJd6VN48yA6VqpH6kB7LvXzPicXI4BeQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图1 数据库实例增长趋势

### 1.2 理想很丰满，现实很骨感

美团数据库团队当前面临的主要矛盾是：实例规模增长与运维能力发展之间的不平衡，而主要矛盾体现在数据库稳定性要求较高与关键数据缺失。由于产品能力不足，只能依赖专业DBA手动排查问题，异常处理时间较长。因此，我们决定补齐关键信息，提供自助或自动定位问题的能力，缩短处理时长。

我们复盘了过去一段时间内的故障和告警，深入分析了这些问题的根因，发现任何一个异常其实都可以按时间拆分为异常预防、异常处理和异常复盘三阶段。针对这三阶段，结合MTTR的定义，然后调研了美团内部及业界的解决方案，我们做了一张涵盖数据库异常处理方案的全景图。如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xYQTOGCicdWgfQQKQIxhusvpQtm390MJT8AaoamybMu3fvxb6iaTxRVtw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图2 运维能力的现状

通过对比，我们发现：

- 每个环节我们都有相关的工具支撑，但能力又不够强，相比头部云厂商大概20%～30%左右的能力，短板比较明显。
- 自助化和自动化能力也不足，工具虽多，但整个链条没有打通，未形成合力。

那如何解决这一问题呢？团队成员经过深入分析和讨论后，我们提出了一种比较符合当前发展阶段的解决思路。

## 2. 解决的思路

### 2.1 既解决短期矛盾，也立足长远发展

从对历史故障的复盘来看，80%故障中80%的时间都花在分析和定位上。解决异常分析和定位效率短期的ROI（投资回报率）最高。长期来看，只有完善能力版图，才能持续不断地提升整个数据库的稳定性及保障能力。因此，我们当时的一个想法就是既要解决短期矛盾，又要立足长远发展（Think Big Picture, Think Long Term）。新的方案要为未来留足足够的发展空间，不能只是“头痛医头、脚痛医脚”。

在宏观层面，我们希望能将更多的功能做到自动定位，基于自动定位来自助或自动地处理变更，从而提高异常恢复的效率，最终提升用户体验。将异常处理效率提高和用户体验提升后，运维人员（主要是DBA）的沟通成本将会极大被降低，这样运维人员就有更多时间进行技术投入，能将更多“人肉处理”的异常变成自助或自动处理，从而形成“飞轮效应”。最终达成高效的稳定性保障的目标。

在微观层面，我们基于已有的数据，通过结构化的信息输出，提升可观测性，补齐关键数据缺失的短板。同时，我们基于完善的信息输出，通过规则（专家经验）和AI的配合，提供自助或自动定位的能力，缩短处理时长。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xEAdA2UousZudDkm1q3TSR1j1vbQB0DXqHtd5mnqfuxcwzBNZ0xKicPQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图3 宏观和微观

### 2.2 夯实基础能力，赋能上层业务，实现数据库自治

有了明确的指导思想，我们该采取怎样的发展策略和路径呢？就当时团队的人力情况来看，没有同学有过类似异常自治的开发经验，甚至对数据库的异常分析的能力都还不具备，人才结构不能满足产品的终极目标。所谓“天下大事必作于细，天下难事必作于易”。我们思路是从小功能和容易的地方入手，先完指标监控、慢查询、活跃会话这些简单的功能，再逐步深入到全量SQL、异常根因分析和慢查询优化建议等这些复杂的功能，通过这些基础工作来“借假修真”，不断提升团队攻坚克难的能力，同时也可以为智能化打下一个良好的基础。

以下便是我们根据当时人才结构以及未来目标设定的2年路径规划（实现数据自治目标规划在2022以后的启动，下图会省略掉这部分）：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xD6NBk9euz9EaOlQZ4HQMe7PBjmx6Ob11Hw1rKp7huVzufep4e93aDg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图4 演进策略

### 2.3 建立科学的评估体系，持续的跟踪产品质量

美国著名管理学者卡普兰说过：“没有度量就没有管理”。只有建立科学的评估体系，才能推进产品不断迈向更高峰，怎样评估产品的质量并持续改善呢？之前我们也做过很多指标，但都不可控，没有办法指导我们的工作。比如，我们最开始考虑根因定位使用的是结果指标准确率和召回率，但结果指标不可控难以指导我们的日常工作。这就需要找其中的可控因素，并不断改善。

我们在学习亚马逊的时候，刚好发现他们有一个可控输入和输出指标的方法论，就很好地指导了我们的工作。只要在正确的可控输入指标上不断优化和提升，最终我们的输出指标也能够得到提升（这也印证了曾国藩曾说过的一句话：“在因上致力，但在果上随缘”）。

以下是我们关于根因定位的指标设计和技术实现思路（在模拟环境不断提升可控的部分，最终线上的实际效果也会得到提升。主要包括“根因定位可控输入和输出指标设计思路”和“根因定位可控输入指标获取的技术实现思路”）。

**根因定位可控输入和输出指标设计思路**

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xZX2PhBIJfwd45vOTdnOicoN93dbx8p9htKMVZ2pGIO7ldA1skpzvlDg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图5 可控输入与输出指标设计

**根因定位可控输入指标获取的技术实现思路**

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xxBXjuXCLsFfow6x3Yh4lKvBicmyMsHen8IdrYpXtTdPMciakzRpp9Efg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图6 可控输入与输出指标技术设计

在图5中，我们通过场景复现方式，用技术手段来模拟一些用低成本就能实现的异常（绝大部份异常）。在对于复现成本比较高的异常（极少部分），比如机器异常、硬件故障等，我们目前的思路是通过“人肉运营”的方式，发现和优化问题，等到下次线上异常重复发生后，根据优化后诊断的结果，通过和预期比较来确定验收是否通过。

未来我们会建立回溯系统，将发生问题时刻的异常指标保存，通过异常指标输入給回溯系统后的输出结果，判断系统改进的有效性，从而构建更加轻量和更广覆盖的复现方式。图6是复现系统的具体技术实现思路。

有了指导思想，符合当前发展阶段的路径规划以及科学的评估体系后，接下来聊聊技术方案的构思。

## 3. 技术方案

### 3.1 技术架构的顶层设计

在技术架构顶层设计上，我们秉承平台化、自助化、智能化和自动化四步走的演进策略。

首先，我们要完善可观测的能力，通过关键信息的展示，构建一个易用的数据库监控平台。然后我们根据这些关键信息为变更（比如数据变更和索引变更等）提供赋能，将一部分高频运维工作通过这些结构化的关键信息（比如索引变更，可以监测近期是否有访问流量，来确保变更安全性）让用户自主决策，也就是自助化。接下来，我们加入一些智能的元素（专家经验+AI），进一步覆盖自助化的场景，并逐步将部分低风险的功能自动化，最终通过系统的不断完善，走到高级或完全自动化的阶段。

为什么我们将自动化放在智能化之后？因为我们认为智能化的目标也是为了自动化，智能化是自动化的前提，自动化是智能化的结果。只有不断提升智能化，才能达到高级或者完全自动化。下图便是我们的顶层架构设计（左侧是演进策略，右侧是技术架构的顶层设计以及2021年底的现状）：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xAJM3WOMJibf0MJL7Z1dAbRqjic5TzxBH62QzYDV7LJ3Sz9gshuDjL2YA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图7 架构顶层设计

顶层设计只是“万里长征第一步”，接下来我们将自底向上逐步介绍我们基于顶层设计开展的具体工作，将从数据采集层的设计、计算存储层的设计和分析决策层的设计逐步展开。

### 3.2 数据采集层的设计

这上面的架构图里，数据采集层是所有链路的最底层和最重要的环节，采集数据的质量直接决定了整个系统的能力。同时，它和数据库实例直接打交道，任何设计上的缺陷都将可能导致大规模的故障。所以，技术方案上必须兼顾数据采集质量和实例稳定性，在二者无法平衡的情况下，宁可牺牲掉采集质量也要保证数据库的稳定性。

在数据采集上，业界都采取基于内核的方式，但美团自研内核较晚，而且部署周期长，所以我们短期的方式是采用抓包的方式做一个过渡，等基于内核的采集部署到一定规模后再逐步切换过来。以下是我们基于抓包思路的技术方案调研：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xVKwxpakIJibW6iaUT44UANocMIib1JgmL2XmY3gGxIssPmYDQhQYlia4Cw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

从调研上我们可以看到，基于pf_ring和dpdk的方案都有较大的依赖性，短期难以实现，之前也没有经验。但是，基于pcap的方式没有依赖，我们也有过一定的经验，之前美团酒旅团队基于抓包的方式做过全量SQL数据采集的工具，并经过了1年时间的验证。因此，我们最终采取了基于pcap抓包方式的技术方案。以下是采集层方案的架构图和采集质量以及对数据库性能带来的影响情况。

**Agent的技术设计**



![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xKufnNAdHb469ZdQm3XMO5aDdHJqRdb1s7QAic9l2NDVP7Uw0gGzgRog/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图8 Agent的技术设计

**对数据库的影响**

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xI8T6YJPZe8wcgzLZwQ8AnqQU4s7AoqRVoc4zT9QiczO8ek7aby9AJ7Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图9 Agent对数据库的影响测试

### 3.3 计算存储层的设计

由于美团整个数据库实例数量和流量巨大，而且随着业务的快速发展，还呈现出快速增长的态势。所以，我们的设计不仅要满足当前，还要考虑未来5年及更长的时间也能够满足要求。同时，对数据库故障分析来说，数据的实时性和完备性是快速和高效定位问题的关键，而保证数据实时性和完备性需要的容量成本也不容忽视。因此，结合上述要求和其他方面的一些考虑，我们对该部分设计提出了一些原则，主要包括：

- **全内存计算**：确保所有的计算都在单线程内或单进程内做纯内存的操作，追求性能跟吞吐量的极致。
- **上报原始数据**：MySQL实例上报的数据尽量维持原始数据状态，不做或者尽量少做数据加工。
  **数据压缩**：由于上报量巨大，需要保障上报的数据进行极致的压缩。
- **内存消耗可控**：通过理论和实际压测保障几乎不可能会发生内存溢出。
- **最小化对MySQL实例的影响**：计算尽量后置，不在Agent上做复杂计算，确保不对RDS实例生产较大影响。以下是具体的架构图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2x6N50wA2I4qw5zpuLUpvKroyIUnv35poykT6LhibX055c2NH5X3jlJdw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图10 Agent对数据库的影响测试

全量SQL（所有访问数据库的SQL）是整个系统最具挑战的功能，也是数据库异常分析最重要的信息之一，因此会就全量SQL的聚合方式、聚合和压缩的效果和补偿设计做一些重点的介绍。

#### 3.3.1 全量SQL的聚合方式

由于明细数据巨大，我们采取了聚合的方式。消费线程会对相同模板SQL的消息按分钟粒度进行聚合计算，以“RDSIP+DBName+SQL模版ID+SQL查询结束时间所在分钟数”为聚合键。聚合健的计算公式为：Aggkey=RDS_IP_DBName_SQL_Template_ID_Time_Minute （Time_Minute的值取自SQL查询结束时间所在的“年、月、日、时、分钟”）

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xyDSy2ZibsibT8sa18OXM2TDpR53ywGDk1kGqibfYrKb8BhiakUtGsF2Ftg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图11 SQL模版聚合设计

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2x268qqmjAJK3OY55PwsZbITHxzlOAbibQpica1UyibhN8ASPGoU0ibyyOOw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图12 SQL模版聚合方法

#### 3.3.2 全量SQL数据聚合和压缩的效果

在数据压缩方面，遵循层层减流原则，使用消息压缩、预聚合、字典化、分钟级聚合的手段，保证流量在经过每个组件时进行递减，最终达到节省带宽减少存储量的目的。以下是相关的压缩环节和测试数据表现情况（敏感数据做了脱敏，不代表美团实际的情况）：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xjksdAR2Jx1HEibNA3icCHl25wg5ggO2ADCfs7HMgwfAzFzrr0PvlY8Tw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图13 全量SQL压缩设计与效果

#### 3.3.3 全量SQL数据补偿机制

如上所述，在数据聚合端是按一分钟进行聚合，并允许额外一分钟的消息延迟，如果消息延迟超过1分钟会被直接丢弃掉，这样在业务高峰期延迟比较严重的场景下，会丢失比较大量的数据，从而对后续数据库异常分析的准确性造成较大的影响。

因此，我们增加了延迟消息补偿机制，对过期的数据发入补偿队列（采用的是美团消息队列服务Mafka），通过过期数据补偿的方式，保证延迟久的消息也能正常存储，通过最终一致性保证了后续的数据库异常分析的准确性。以下是数据补偿机制的设计方案：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2x275uDmh33rD0PKL9KZqU2mEzhzpK2DJ1NDEvqaribNKTnTsmswpOsgw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
图14 全量SQL补全技术设计

### 3.4 分析决策层的设计

在有了比较全的数据之后，接下来就是基于数据进行决策，推断出可能的根因。这部分我们使用了基于专家经验结合AI的方式。我们把演进路径化分为了四个阶段：

- **第一阶段**：完全以规则为主，积累领域经验，探索可行的路径。
- **第二阶段**：探索AI场景，但以专家经验为主，在少量低频场景上使用AI算法，验证AI能力。
- **第三阶段**：在专家经验和AI上齐头并进，专家经验继续在已有的场景上迭代和延伸，AI在新的场景上进行落地，通过双轨制保证原有能力不退化。
- **第四阶段**：完成AI对大部分专家经验的替换，以AI为主专家经验为辅，极致发挥AI能力。

以下是分析决策部分整体的技术设计（我们参考了华为一篇文章：[《网络云根因智荐的探索与实践》](https://mp.weixin.qq.com/s?__biz=MzAwMzcxNDM4Mg==&mid=2649079833&idx=1&sn=f131de51defe1e3a6125f1f0d0f117e7&scene=21#wechat_redirect)）：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xWHqicVzwoIA0fnhMxs9Zh0icYlibYyNcmFrXWHMXf5DIuax3jpLZ1ObIA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图15 分析决策的技术设计

在决策分析层，我们主要采取了专家经验和AI两者方式，接下来会介绍专家经验（基于规则的方式）和AI方式（基于AI算法的方式）的相关实现。

#### 3.4.1 基于规则的方式

专家经验部分，我们采取了GRAI（Goal、Result、Analysis和Insight的简称）的复盘方法论来指导工作，通过持续、大量、高频的复盘，我们提炼了一些靠谱的规则，并通过持续的迭代，不断提升准确率和召回率。下面例举的是主从延迟的规则提炼过程：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xcx9IHy4UJ5qPmcwdian1AdnrIsQzVyiaicDvfibX2qxJAqcj2HhUT13g0g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
图16 专家经验的复盘和改进

#### 3.4.2 基于AI算法的方式

**异常数据库指标检测**

数据库核心指标的异常检测依赖于对指标历史数据的合理建模，通过将离线过程的定期建模与实时过程的流检测相结合，将有助于在数据库实例存在故障或风险的情况下，有效地定位根本问题所在，从而实现及时有效地解决问题。

建模过程主要分为3个流程。首先，我们通过一些前置的模块对指标的历史数据进行预处理，包含缺失数值填充，数据的平滑与聚合等过程。随后，我们通过分类模块创建了后续的不同分支，针对不同类型的指标运用不同的手段来建模。最终，将模型进行序列化存储后提供Flink任务读取实现流检测。

**以下是检测的设计图**

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xKLtnTMaArlFlvgAQbic5unp4UdmO3zibvXic6hUq06uczicHgt9iciaRiaKTw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图17 基于AI的异常检测设计

**根因诊断（构建中）**

订阅告警消息（基于规则或者异常检测触发），触发诊断流程，采集、分析数据，推断出根因并筛选出有效信息辅助用户解决。诊断结果通过大象通知用户，并提供诊断诊断详情页面，用户可通过标注来优化诊断准确性。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xNuv3kYxHG68nlJ8ynfYFzVMrRNOwicjfdSjgPECO0MIUqNoMs17oUpA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图18 基于AI的异常检测设计

- **数据采集**：采集数据库性能指标、数据库状态抓取、系统指标、硬件问题、日志、记录等数据。
- **特征提取**：从各类数据中提取特征，包括算法提取的时序特征、文本特征以及利用数据库知识提取的领域特征等。
- **根因分类**：包括特征预处理、特征筛选、算法分类、根因排序等部分。
- **根因扩展**：基于根因类别进行相关信息的深入挖掘，提高用户处理问题的效率。具体包括SQL行为分析、专家规则、指标关联、维度下钻、日志分析等功能模块。

**4 建设成果**

**4.1 指标表现**

我们目前主要是通过“梳理触发告警场景->模拟复现场景->根因分析和诊断->改进计划->验收改进质量->梳理触发告警场景”的闭环方法（详情请参考前文**建立科学的评估体系，持续的跟踪产品质量**部分），持续不断的进行优化和迭代。通过可控输入指标的提升，来优化改善线上的输出指标，从而保证系统不断的朝着正确的方向发展。以下是近期根因召回率和准确率指标表现情况。

**用户告警根因反馈准确率**

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xlxQ559xUkeRwlgXRYLjuZUSiaCXvvXdWI5ybZWZAxwPs8zyd894FVzw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图19 用户反馈准确率

**告警诊断分析总体召回率**



![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xric7vG40rL9bmswTchjll1v8TpBOayg362M8LvjH1kVqbc998bEJcwg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图20 根因分析召回率

### 4.2 用户案例

在根因结果推送上，我们和美团内部的IM系统（大象）进行了打通，出现问题后通过告警发现异常->大象推送诊断根因->点击诊断链接详情查看详细信息->一键预案处理->跟踪反馈处理的效果->执行成功或者回滚，来完成异常的发现、定位、确认和处理的闭环。以下是活跃会话规则触发告警后根因分析的一个案例。

**自动拉群，并给出根因**

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2x2RTOU2KiaRxTGiczfMAZPaHuicib2KfbBEL3zbUFZAaQvjGIekjdGDA5Mg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

图21 锁阻塞导致活跃会话过高

**点击诊断报告，查看详情**

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xreoQdkU4uFFrSeuy27pbvbo352RCmqXXNkz3lVibEaO6Exb4gJNGmsA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
图22 锁阻塞导致活跃会话过高

以下是出现Load告警后，我们的一个慢查询优化建议推送案例（脱敏原因，采用了测试环境模拟的案例）。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVBw9zgLdYXhY0DKVsVOn2xElGiaa38jyCKFYiaWaiaUl6PgTW5o7whptxs5Liaz5rsicNZ1z4iaMZ3QByw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
图23 慢查询优化建议

## 5. 总结与未来展望

数据库自治服务经过2年左右的发展，已基本夯实了基础能力，在部分业务场景上完成了初步赋能（比如针对问题SQL，业务服务上线发布前自动识别，提示潜在风险；针对索引误变更，工单执行前自动检测索引近期访问流量，阻断误变更等）。接下来，我们的目标除了在已完成工作上继续深耕，提升能力外，重点会瞄准数据库自治能力。主要的工作规划将围绕以下3个方向：

**（1）计算存储能力增强**：随着数据库实例和业务流量的持续高速增长，以及采集的信息的不断丰富，亟需对现有数据通道能力进行增强，确保能支撑未来3-5年的处理能力。

**（2）自治能力在少部分场景上落地**：数据库自治能力上，会采取三步走的策略：

- 第一步：建立根因诊断和SOP文档的关联，将诊断和处理透明化；
- 第二步：SOP文档平台化，将诊断和处理流程化；
- 第三步：部分低风险无人干预，将诊断和处理自动化，逐步实现数据库自治。

**（3）更灵活的异常回溯系统**：某个场景根因定位算法在上线前或者改进后的验证非常关键，我们会完善验证体系，建立灵活的异常回溯系统，通过基于现场信息的回放来不断优化和提升系统定位准确率。

## 6. 本文作者

金龙，来自美团基础技术部/数据库研发中心/数据库平台研发组。

原文链接：https://mp.weixin.qq.com/s/PmMVBjAzjeJYWBJI39gf_g



# 【NO.363】标准化思想及组装式架构在后端BFF中的实践

## 1. 前言

在本地生活服务领域，面向C端的信息展示类功能存在着类生物系统的复杂性，具体体现在以下三个方面：**功能多**，为了帮助用户高效找店、找服务，信息会在尽可能多的地方展示；**差异大**，同样的信息，在不同客户端、不同页面及模块下的展示逻辑会存在一些差异；**功能易变**，产品逻辑经常调整。以上三个方面的特点给研发同学带来了很大的挑战，比如当我们面临数千个功能模块，数十个行业产品的持续需求时，如何快速响应呢？

进入互联网“下半场”，靠“堆人力”的研发方式已经不再具备竞争力了，真正可行且有效的方式是让系统能力变得可沉淀、可组合复用、可灵活应对各种变化。在多业态、大规模定制需求的背景下，本文分享了如何通过组装式开发的方法来提升业务的竞争力。

## 2. 背景与问题

### 2.1 业务背景

先来讲一下我们的业务和产品，美团到店是一个生活服务平台，通过“信息”连接消费者和商家，帮助用户降低交易成本，这是信息产品功能的业务价值。当我们打开美团/点评App，搜索“美发”，就可以看到一个搜索结果页，展示着基于关键词召回的美发商户（如下图左所示）。商户下面挂着当前门店所提供的团购、会员卡概要信息，我们选择一家门店进入商户详情页，自上而下滑动，可以看到商户的地址模块、营业信息模块等基础模块（如下图右所示）。继续往下还能看到商品货架模块、会员卡模块、发型师信息等等，以上就是信息展示产品的具体形态。

![图片](data:image/svg+xml,<%3Fxml version='1.0' encoding='UTF-8'%3F><svg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'><title></title><g stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'><g transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'><rect x='249' y='126' width='1' height='1'></rect></g></g></svg>)图1 产品功能形态

前文我们提到过本地生活服务行业信息类产品功能的核心特点是功能多、差异大、功能易变，为了帮助读者更好地了解相关的业务背景，针对这几个特点我们进一步补充：

- **功能多**：在多业态背景下，信息展示功能总体上表现为功能模块非常多。主要是因为同样的内容会在多个地方展示，比如某个行业的商品信息会在App的首页、搜索结果页、频道页、详情页、订单页、运营页等多个页面进行展示。并且当新行业新内容出现的时候，又会全面铺开，进而导致增加更多的功能。截至目前，我们已有上千个展示功能，呈规模化势态。
- **差异大**：差异化主要体现在相同的内容，在不同行业、不同客户端、不同模块、不同版本甚至是不同用户条件下，都会有不同展示逻辑。比如商户详情页货架的商品标题这个字段，有的行业展示的规则是“服务类型+商品名字”，如“[玻璃贴膜]龙膜全车车窗隔热膜套餐”。有的行业的展示规则是“服务特性+商品名字”，如“[洗吹]单人明星洗吹+造型”。再比如跳转链接这个字段，H5、小程序和App内的跳转链接的拼接规则都不一样。诸如此类的差异几乎贯穿所有的功能。
- **功能易变**：主要体现在产品逻辑会经常发生迭代。分析变化原因来自多个方面，首先是这类信息产品面向海量互联网用户，用户体验敏感度高，细微的展示规则差别都可能会导致不同的转化效果，到底是哪个展示规则效果比较好，产品只能通过不断的调整来进行验证。其次，本地生活服务标准化程度低，内容本身的结构也在不断迭代，内容变更同时也决定了展示功能要跟着变。最后一点，互联网行业中产品的职责也会经常进行调整，不同的产品对功能的理解是不一样的，这也是导致功能更迭的原因之一。

以上是生活服务行业信息产品的特点，面对大规模、差异化的信息展示类功能的挑战，产品在持续迭代，研发同学又面临怎样的问题和挑战呢？

### 2.2 研发挑战

在分享技术挑战之前，可以先看看研发同学的日常。这里有两个小场景：

- 场景一，由B端（商家/运营）直接生产出来的信息，不能直接展示给用户。B端主要关注信息能否高效录入，录入的信息不适合直接展示给用户，需要经过一些逻辑加工，同一份B端录入的信息可能会有多种加工展示规则。
- 场景二，由于B/C端业务领域问题差别较大，为降低开发难度，B/C端一般会做精细化分工，一拨人专注B端的信息录入能力建设，一拨人专注C端的信息展示。

而我们就是专注信息展示的这拨人。这类系统业界也有一些标准的术语，叫BFF（Backend For Frontend）。BFF的主要职责是组合使用底层数据，额外处理C端展示逻辑。综上所述，我们研发同学具体的工作通常是：通过外部数据源将原始数据查到，然后按照产品的要求，把查到的原始信息加工成可以展示给用户的信息，最后发送给客户端使用。如下图所示，这部分工作主要由中间的BFF API服务负责：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVo31poV3gqtBEwWAt8iav5t6tWbEliauaEXc7b1iaJH1dvol4fSUkkBK4meILmRJu1GpFAAic2FBia2Dw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图2 研发的主要工作

看到这里，我们猜你可能会这么想：就这么简单的工作，能有啥技术挑战？确实，如果是站在纯编码的角度，代码“撸上”就完事了，确实没什么挑战。但，这不是一个简单的敲代码问题，而是一个工程问题。当有上千个这样的功能，产品需求持续涌入时，如何用有限的研发资源满足无限的业务需求，同时能够控制系统的复杂性及运维成本，还要考虑人的成长问题，这才是我们面临的关键挑战。

- **问题1 - 效率**：天下武功，唯快不破。产品功能追求快速上线似乎是个永恒的命题，在互联网行业C端海量用户的背景下，这个命题尤为突出。此外，很小一个功能点的改动就会对用户体验产生非常大的影响。在人力有限的情况下，如何满足大量产品需求快速上线是研发团队面临的首要挑战。
- **问题2 - 复杂性**：稍微有点追求的工程师都会考虑代码复用的问题，不管代码写得优不优雅，绝对不能允许重复，于是代码中就充斥着各种`if…else…`，因为有的逻辑只有当某些特定情况下才会运行。但是，复用需要良好的建模，嵌入过多的逻辑，只会让系统的复杂性变得越来越高，进而让系统难以进行下一步的演进，这通常会消耗大量的隐性成本。如何控制好系统的复杂性，让系统长期保持可理解、可修改，是我们面临的又一挑战。
- **问题3 - 运维成本**：如果缺少抽象的过程式开发，还会导致系统功能变得非常繁多，接口就好几百个，平时这些接口的运维工作也要耗费大量的人力。如果功能的开发缺少统一的章法，代码逻辑复杂交错，就会给运维工作带来非常大的挑战。
- **问题4 - 成就感**：根据马斯洛需求层次的理论，顶层是“自我实现”的需求。如果落实到工程师的日常，大家也需要在工作中找到成就感。可问题在于，如果每天都“过程式”地编写数据的查询、加工和组装的业务，对大家来说，很难产生什么成就感。

导致这些问题的根本原因是什么呢？这里想借用美团联合创始人王慧文在知乎上说的一句话，**以科学和工程追求真理**。真理是什么难以定义，但我们一定要运用科学的、工程的方法。下面将会进一步介绍对这个问题的思考以及我们的解决思路。

## 3. 问题分析与解决思路

### 3.1 问题思考

常规编程的基本工作，总是基于某种编程范式展开的，比如面向对象、过程式、函数式。我们很容易就想到，如果解决问题的范式不能很好地和问题相匹配，那么就会引起矛盾。所以历史上有很多使用汇编语言难以完成大规模项目、使用结构化编程难以应对超大规模项目的故事。

那么，前文提到的问题，是不是因为开发方式和业务问题不匹配而导致的呢？举个通俗的例子，好比我们现在要做的是一道西红柿炒蛋，但是拿出的工具却是电烤箱，后果可想而知。当然，有人说可能会说“真正的高手是可以的”，但毕竟绝世高手是极少数。而如果我们拿的出是平底锅，肯定会更容易一些。所以解决问题不能一味地追求成为“绝世高手”，降低解决问题的门槛才是真正行之有效的方式。

静心反思，我们认为真正原因在于：**我们所使用的开发范式与业务问题不匹配**。换句话来讲，我们对问题缺少针对性的建模，缺少针对性的方法论。比如在业务层面，我们的诉求是能够快速交付，能够满足需求的多样性，并且能够快速响应产品功能的灵活变化。在技术层面，我们的诉求是在人力有限的背景下，让系统复杂度可控，且代码复杂度不会与业务逻辑呈现“乘积式”关系的增加。此外，还要保证运维成本可控，系统数不会和功能数呈现“线性关系”的增加。

而我们当前的开发方式却是“过程式”的，这种“过程式”体现为面向产品需求文档的直译式编程。但这种开发模式和我们的诉求并不匹配。因此，我们需要取寻找适合我们自己的开发范式。

### 3.2 标准化及组装式思想

John Ousterhout曾说过：**复杂性是由模糊性和依赖性引起的**。模糊性主要源于对事物缺少清晰的概念描述，因此复杂性通常会通过应用很多关键概念来解决，这些概念通过抽象、分解、迭代和细化这样的方法来进行表达，建立明确的概念是消除模糊性的关键，也是我们解决复杂问题的常规思维方法。

在这个过程中，分解指的是把一个较大的问题分解成较小的、可管理的单元，每个单元都可以单独处理，这些单元被称为模块、包或组件。这个思路可以追溯到哲学上的“还原论”，目前已成为软件工程的很多方法论的核心。依赖性指的是模块之间依赖关系的多少以及强弱程度，比如一个模块是否依赖另一个模块的实现，模块之间的依赖是否遵循统一的契约，这些都会影响到系统的复杂性。

组装式开发指的是将系统分解为标准组件，再由标准组件组装成系统，以此形成的架构被称为组装式架构。跟传统代码复用技术关键的不同点在于组件的含义。组件是高度标准化的单元，具备可复用性、标准化、可替换性、可包装及独立自治这些特征。组装式开发背后的核心逻辑是基于标准化思想的代码复用。

关于标准化，历史上还有这样的一个故事：18世纪末，美国刚建立不久，由于国内外战火尚未停息，政府担心会与法国作战，急需准备大量的军火。但是，当时的传统制枪方式是依靠熟练的工匠采用磨、削、锤等工序制成一个个非标准的枪机零件，然后将它们组装成枪支，这种制作方法即使全部都是“能工巧匠”，生产效率也非常低下。于是，在政府的敦促下，伊莱·惠特尼（美国发明家、机械工程师和企业家，发明了轧花机、铣床）把整个工序分成若干工序，并把一个零件都比照标准来福抢的样品纺制成通用的零件，由此在军火生产中成功地引进了零件可替换性的原理，这是工业标准化划时代的开端。此后，标准化的互换性原理促进了工业的迅速发展，惠特尼也被誉为现代工业的“标准化之父”。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVo31poV3gqtBEwWAt8iav5tSP5knMWHRS5SibI9eW1cvdVqutbjQCvdaTEicD9iaaDRBBic2YsvuO5ibFA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图3 “标准化原理”的应用

再举一个例子，乐高积木玩具我们都知道，玩具厂商制造了一批标准的积木，孩童可以基于这些积木组装成不同款式的玩具，喜欢飞机，就组装飞机，喜欢坦克，就组装坦克，这也是利用了标准化的可换性原理。再回到软件行业，基于组件实现大规模的软件复用这个概念，最早来源于Doug McIlroy在1968年的一次软件工程学会上的演讲，演讲名为“大规模生产的软件组件”，之后这被公认为软件复用的起源。基于这个思想，如果我们能够引入组装式开发的思想，将业务代码分解成标准化的组件，然后再基于这些组件组装成不同的功能，进而满足不同场景下的业务需求，这不就是符合我们需求的开发方式吗？

但知易行难，因为它忽略了很多现实的细节，我们在实际应用的时候总是要面临各种现实问题的挑战。柏拉图曾对人生的终极问题做了定义：我是谁？我来自哪里？我将要到哪里去？这些问题延续至今，一直困扰着人们人类。而软件工程也向工程师门提出了软件设计的终极问题：什么是抽象层次？什么颗粒度？以及如何应对变化？

所以，组装式开发的历史坎坷崎岖，更难的是在每个技术领域这些问题的答案还都不一样。比如在颗粒度的问题上，组件的颗粒度到底要多大，颗粒度越大，被修改的风险也就越大，而过小的颗粒度可以让组件更稳定，但是会带来组装的复杂性。再比如在应对变化这个问题上，这段逻辑到底是通用的还是个性的，非常难以辨别，但是我们的系统设计又强依赖于这个判断，如果最初的判断失误，就有可能导致系统最终的失败。我们就遇到过这种情况，有一次自信满满地封装了一个组件，感觉应该可以满足各种复杂的情况，刚好就在下一个需求来临时，发现不太匹配。

### 3.3 我们的解决思路

前文讲，组装式开发看起来正是我们所需要的开发模式，然后我们也讨论了组装式开发面临的关键挑战。那么，在我们的实际业务中，组装式开发真的可行吗？如果可行，我们又怎样应对随之而来的挑战？特别是本地生活行业细分行业多、功能多。当然在业务给技术带来了挑战的同时也蕴含着巨大的机会。因为涉及行业越多，系统功能越多，代码复用的机会也会越高（如下图左所示）。虽说不同的功能在感官上给人的感觉差别很大，但是有很多功能的底层存在着太多的共性。我们认为，解决问题的关键在于对颗粒度的把控，以及对可变性的处理。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVo31poV3gqtBEwWAt8iav5txXkfBWYeOdMbtj06KyaVIcticMWuQMcx0xQlQ9L08BVOL9bd58sEUHQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图4 组装式开发可行性以及关键思路

然后，我们主要从粒度及可变性两个方面着手。在粒度问题上，我们引入多层次多颗粒度的组件体系设计；在可变性问题上，我们通过可变性建模让整个功能具备灵活应对变化的能力，最终形成了如上右图所示的概念模型。除此之外，我们还通过可视化组装的方式来简化组装过程，总体思路如下：

- **产品功能归类之系列化**：福特T型车生产的案例告诉我们，单一的产品无法满足市场化的需求，多样性更具备市场竞争力。但是高效的多样性需要范围的约束，所以现代汽车生产商会对产品进行系列化，比如宝马轿车有7系、5系、3系，系列化是厂商在效率和多样性之间追求平衡的产物。在软件开发领域亦然，组件的复用应该是有范围的，我们首先将产品功能进行系列化归类，进一步降低了系统整体的复杂性及设计难度。
- **功能组件提取与预组装**：组件标准化是组装式开发的前提，所以我们需要先将组件提取出来。在最终组装功能的时候，为了降低组装的复杂性，我们引入了“预组装”的概念。针对每个系列的产品功能，我们将通用部分提前组装好，将定制部分提前预留出来，定制部分包括功能和特性的定制，这部分可延迟到需要的时候再组装。
- **变化应对之可变性建模**：在电动车出现以前，所有的汽车几乎都需要燃油来发动，因此燃油机属于共性功能的需求。但是我们发现，有的车主是新手，对车子非常爱惜，因此需要全景的倒车仪。有的车主是老司机，他们根本不需要全景的倒车仪，所以全景倒车仪就属于变化功能的需求。不仅如此，我们还发现对于同样有全景倒车仪诉求的车主，他们也会选择具有不同特性的设备功能，这部分属于更细粒度的定制需求。所以，变化本身是复杂的，是有层次的，变化需要被单独建模，才能够被有效管理。
- **可视化组装与配置填充**：细颗粒度的组件带来了一定的组装复杂性，为简化组装过程，我们将可用组件呈现在用户的界面上，开发同学通过点击鼠标即可完成组件的组装，而对于功能特性组件的配置填充，也是在用户界面上直接完成的。

通过以上几个策略，我们将信息展示场景的研发模式打造成一个多系列产品的生产线，每个生产线都支持组装式生产一个系列的定制功能。下一章节我们将介绍更多的技术细节。

## 4. 标准化思想及组装式架构在后端BFF中的实践

### 4.1 产品功能归类之系列化

#### 4.1.1 产品系列化

在官方语言里，系列化指的是“对同一类产品的结构形式和主要参数规格进行科学规划的一种标准化形式”。在我们这里，系列化指的是对信息类产品功能进行归类，目的包含两个方面，一方面是为了降低系统整体的复杂性，另一方面也为建设组装这些功能的“生产线”做准备，一个系列的功能由一个“生产线”来组装，组件可以在不同范围内进行复用。

信息类产品展示的内容通常来自多个领域，比如一个商品展示模块，可能要聚合门店的信息，很难直接通过领域来进行划分，那么怎么划分系列呢？显性的差异我们能直接看出来，主要包括两方面，首先是展示内容方面，我们能够比较容易地发现每个展示模块都有主要的展示内容的差别，比如主要内容分别是门店信息、评价信息、内容信息、商品信息等内容，其他信息往往附属于主要内容。其次展示样式方面，有的展示样式差别很明显，比如商品详情页和商品货架模块；有的展示样式差别没那么大，比如同样是商品货架模块，只是个别字段有差异。隐性的差别主要是内部的实现，因为这些实现直观上是看不出来的。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVo31poV3gqtBEwWAt8iav5tCMoekj838bxPxibJCzquuUGp0QgcamTcfb8tgVqVWExXD1D5NOFibcqw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图5 信息类产品功能归类思路

我们主要从展示内容、展示样式及展示逻辑实现等几个方面来对功能进行归类合并：

- **基于内容**：一个模块展示的内容通常分为主要内容和附属内容，不同展示模块最大的差别来源于展示主要内容的差别。比如以展示门店为主的模块和以展示商品为主的模块，不管是系统实现，还是功能形态，都存在比较明显的差别。因此我们首先把展示内容的主体是谁作为归类依据，将不同内容的展示功能区分开来。
- **基于样式**：功能展示的样式差别往往会决定展示数据的差别，如果展示数据差别较大，那么接口则不容易做标准化。比如商品详情页和商品货架模块，商品详情只展示一个商品，同时会展示更多的商品信息。商品货架模块会展示多个商品，但是每个商品只展示少部分的信息，商品货架模块还会展示筛选项。显然它们的接口很难统一，因此展示形式是一个重要的归类维度。
- **基于实现**：最后要从实现层面看好不好抽象，另外两个维度的抽象已经为这个维度打好了良好的基础。怎么抽象呢？比如有两个功能的步骤和依赖功能的组合能够抽象的大致相似，那么这两个功能可以归为一个系列。

以上维度并不是绝对的优先级关系，但能解决绝大多数的问题。也存在例外情况，比如有的功能也可能同时展示多种信息，但找不到主要展示的对象，那么我们可以基于实现这个因素来进行选择。经过上面一波操作，我们基本可以得到产品功能的系列化全景，上千个功能经过系列化之后，也就仅仅只有几个系列。以上只是业务层面的划分，那么系统对应有怎样的设计呢？

#### 4.1.2 接口标准化

在产品功能系列化归类之后，同一系列内的产品功能之间仍然会存在逻辑差异，这些差异主要体现在展示模型以及内部实现上。展示模型是后端吐给前端的数据结构，主要的职责是承载展示数据，不同功能存在字段上的差别，所以导致模型会有差别。比如一个简单的例子，有的功能有标签字段，有的没有，那在标签这个字段上就形成了差异。内部实现主要包括数据的查询逻辑、加工逻辑等。针对这两方面差异，我们的核心思路是接口模型标准化及统一业务身份来串联差异化逻辑。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVo31poV3gqtBEwWAt8iav5tVhOzauY1GuoyXmarnPOJkLwnNANVR1zfnzHpXjXiaoKficlR7wgaCqUQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图6 标准接口+多租户模式

系列化本身更有利于接口模型做统一抽象，因为同一系列内部的功能在结构上的差异不会太大，我们只需要稍微做一些抽象，大多数字段都可以收敛。对于极少的个别情况，比如某个字段就个别功能才有，我们就通过K-V结构来进行应对。

模型设计的具体细节在这里不过多展开，重点是接口统一化、标准化之后有明显的好处。前后端的协作效率提高了，前端和后端不再需要在每次需求变更的时候都当面沟通一次接口。不仅如此，系统层面接口的标准化也能够让前端的代码和后端接口的集成关系变得更加稳定。

在搞定接口之后，内部的差异怎么办？下文会介绍我们在这部分的组装式思路。在这个思路下，内部实现方面的差异最终会表现为一系列不同颗粒度组件的集合差异，所以这里核心要解决的问题是如何能够识别功能差异化组装组件的问题。针对这个问题，我们的思路是引入“业务身份”这个概念，这个概念目前应用得也比较广泛，我们通过业务身份来串联不同业务场景的数据组装组件，从而实现差异化逻辑的处理。

### 4.2 功能组件提取与预组装

#### 4.2.1 功能组件提取

系统划分方法在业界应用的比较多的是领域驱动方法（DDD），基于领域驱动方法，我们一般会按照实体或者聚合根来划分子系统或模块，但是对于信息展示类的系统来说，很难应用领域驱动的方法。因为我们开发的不是一个单领域的小系统，而是一类跨多个领域的、属于由几千人共同开发的复杂分布式系统之上的一个子系统。这个系统负责查询和组装由底层系统提供的数据，然后将数据加工、裁剪、组装展示模型给到前端。这类系统距离业务实体很远，因此对于这类系统的组件化分解，不能应用传统领域驱动的方法，而需要使用一种特殊的方法。我们的基本的思路是梳理现有流程步骤，将现有功能按相关性归类，同一类功能封装成一个功能组件，然后再由多个组件组合成一个系列功能。这里举个例子：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVo31poV3gqtBEwWAt8iav5t1RuYEvDSp0F0EuhUdB14ADNF5EyTkGt0jclrNOJeUL2PSrX3U2BMeQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图7 组件提取案例

左图所示的是一个商品货架的场景，右图展示的是要生产这个货架所需展示数据需要经历的流程步骤。通过上图我们可以看到，货架的展示数据的生产过程主要包括以下几个步骤：

1. 根据查询条件查询商品ID，比如查询门店下可售卖的团单ID；
2. 聚合商品维度信息，这些信息包括商品的标题、价格、服务流程、优惠促销、标签等等；
3. 查询货架的筛选数据及商品的摆放规则，比如推荐、热销、新品等标签数，以及哪些商品放在哪些标签下面的摆放规则；
4. 组装结果展示模型，涉及聚合数据的再加工，如标题、标签的拼接。

以上流程步骤相对比较清晰，并且能够适应一类场景，只是不同场景在步骤内存在部分差异而已。所以我们可以将这几个步骤抽取出来，每个步骤分别封装成单独组件负责解决一类问题，同时组件可以组合复用。值得强调的是，这些组件的封装都是基于标准的接口实现。

#### 4.2.2 功能组件预组装

传统的业务流程编排适合于流程类业务场景，比如OA办公审核系统，基于业务流程引擎的好处，一方面是容易实现能力的复用，复用现有能力编排出新的流程。另一方面是更容易应对流程的变化，因此特别适合流程类且流程易变化的场景。组件类似业务流程编排类系统中的“能力”，如果通过业务流程引擎，也可以实现类似的效果。但是实际上，信息展示场景的业务流程更像是一个图，我们姑且称之为“活动图”，而不是一条长长的管道流程，如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVo31poV3gqtBEwWAt8iav5tSvoJaROWro2tX8vXVb8ia1AEzWNxB459mRM2j7wjxPwNCLczTIg5QOg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图8 有筛选和没筛选之间的差别

流程引擎适合应对流程的变化，而我们业务场景中，变化之处不在于流程本身，而在于在这个活动图上执行的步骤集合。如上图的示例，左侧是有筛选的货架，需要查询筛选标签数据，所以执行的是整个活动图。右侧是没有筛选的情况，不需要查筛选标签数据，所以执行的是活动图的子图，实际业务场景更复杂一点的活动图也有，这里只是举个简单的例子。

我们会发现，不同情况的不同之处在于遍历这个活动图的节点的集合不同，总体类似在一个完整的图中选取一个子图。因此，我们选择以图的方式组织我们的组件，而不是传统的流程，这样更贴合我们的实际情况，也更容易理解。

另外，为了提高组件组装的效率，我们将一个系列功能使用到的所有组件提前组装好，得到一张全景图。那么在需要的时候，我们只需要对着这个完整的图选择子图即可，然后再基于子图组装定制部分的组件。预组装不仅能够提升组装的效率，同时也能够避免错误，让系统变得更稳定。传统业务流程编排中，能力的应用上下文其实是有限制的，虽然流程引擎足够灵活，但是实际上在编排能力时仍然需要人工对能力做检测，确认是不是能够满足当前的流程。而预组装可以从根本上避免这个问题，因为只要是存在的路径，都是可以执行的。

### 4.3 变化应对之可变性建模

通过将功能组件组织成一个个活动图，每个活动图负责解决一个系列的产品功能展示信息组装问题，此时的组件颗粒度还是较大的。组件颗粒度大的问题在于，容易不稳定，从软件设计的角度来看，是因为变化的因素太多。比如在组装展示模型环节，展示模型组装组件负责将数据组装成发给前端的展示模型，实际业务场景中不同情况对于同一个展示字段的组装存在不同的拼接策略，我们拿开篇的例子来讲解：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVo31poV3gqtBEwWAt8iav5thhUTCia0fAQE7icvx3ZvJMJeVrf3f8e8LiaeP7z3rkiboEm5vRicFZibOtvw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图9 不同情况组装逻辑不一样

左侧丽人行业商品标题的组装规则是“服务类型+商品名字”，右图养车/用车行业商品标题组装规则是“服务特性+商品名字”。其实这个组件的颗粒度刚刚好，因为它让我们的活动图看起来不至于太复杂，但怎么应对这种变化呢？作为有经验的程序员，可能自然会想到条件分支语句`if…else…`，而且实际中很多项目针对这种问题的处理方式都是`if…else…`。

对于简单的情况，使用这种方式无可厚非，我们这里讨论更复杂的情况，过多的使用条件分支至少存在两个方面的问题。一方面是代码将会变得非常复杂，就像密密麻麻缠绕在一起的电线，这样的代码难以理解和维护。另一方面，这种模式本身会让共享组件变得极其不稳定。如果我们的系统建立在经常有变动的根基上，那么我们很难保证系统的稳定性，每一次共享组件的变更都面临着故障风险，为了让变化可管控，我们要对变化进行建模。

#### 4.3.1 可变性建模

这些年，软件工程在如何应对“变化”这个问题上，最具革命性的创新是将共性和变化分离，分离的变化通过使用扩展点代码或配置化变量的方式实现。这些经典思想真是太棒了。对于我们也很有启发，如果将容易变化的逻辑和变化的逻辑分离开来，同时引入配置化能力，那么我们的组件将会很容易应对变化。因此，对于可变性的管理，我们通过标准功能组件引入了可变性分离这个思想来解决。如下图所示，组件本身具备变化点和配置项这两种应变能力：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVo31poV3gqtBEwWAt8iav5twrJdtAk28zp7B1coib5ibribLR6Qnz8diaiayqxmDTGgkDuScz4orhdPV8A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图10 易变功能通过变化点-可选项-配置项建模

**变化点**这个概念是我们对扩展点代码的具象化，寓意容易变化的地方；**配置项**用来承接变量的抽离。变化点和配置项都是应对变化的实现方式，那么实际应用时怎么选择？针对可枚举的变化，可以提取变量配置化。针对复杂度多变性，可以通过变化点来扩展。变化点的具体形式是个接口，将变化点的具体实现放到组件之外，组件内部公共逻辑部分只调用变化点接口。这样的话，不管变化点的地方怎么变，即使有一百种变体，都不会影响组件的公共部分。

针对具体案例，比如查询商品ID这个组件，我们实际上有多个查询索引，比如商品推荐索引、商品筛选索引，还有一些强实时索引，但总体还是相对稳定可枚举的，所以我们将不同的查询索引建模为查询渠道配置项，使用的时候只要填写查询渠道配置项即可。再比如展示模型组装这个组件，因为标题的拼接规则会有所变化，而且较为不可枚举，产品规则变化多，因此我们在标题这个地方设定一个变化点，不同标题组装的逻辑作为变化点的不同实现，这样更能够应对变化。

不同变化点的实现可以认为是组件提供的多种功能特性，因此我们又抽象出可选项这个概念来描述变化点实现。一个变化点有多个选项，选项是可复用的，贴合现实世界，更容易理解。在组装的时候只需要“选择需要的特性选项”即可满足定制需求。以上通过**变化点-可选项-配置项**这组概念解决了组件的灵活应变的问题，同时能够让组件本身变得更为稳定。

#### 4.3.2 可选项爆炸问题

我们发现如果每个选项都通过硬编码实现的话，有的变化点可能会存在非常多的可选项。比如标签这个字段，标签往往来源于扩展属性，不同商品的模型和扩展属性不一样，造成这个标签的来源差异很大。如果我们都是通过硬编码实现选项，那么由于标签来源有所差异所导致的选项扩散问题就会很明显，可能有多少种商品，就会有多少种选项。选项本质上是一种更细粒度的组件，这个组件内部本身也需要有应对变化的能力。因此，我们的解决思路是为选项这个细粒度的组件增加可配置能力，每个选项可以设计自己的配置项，将易变规则通过配置来实现，这样选项就有了一定程度应对变化的能力，从而得到了收束。

### 4.4 可视化组装与配置填充

在以前，当我们需要开发一个展示功能的时候，我们需要做以下几件事情：

1. 编写将外部数据粘合在一起的代码，包括远程RPC的访问代码和数据的组装代码；
2. 依照PRD编写一遍展示加工逻辑；
3. 将加工好的数据字段填充在和前端同学一起定义好的展示模型上；
4. 构建和集成代码。

组装式开发的基本要求是：将系统功能基于标准接口封装成不同颗粒度的组件单元，这个要求为产品功能的生产过程带来了革命性的转变。功能的组装不再需要手写“胶水代码”，而是通过系统就可以完成自动化的组装。大部分的功能是复用已有的组件，而不是重头编写，实际上经过功能特性组件的不断沉淀，我们已经实现了80%以上的产品逻辑都是复用已有组件。

此时，我们的研发过程整体上可分为**组件开发**和**组装集成**两个阶段。组件开发环节关注功能的抽象和封装，基于已有组件，组装集成环节做的事情就是选择需要的组件，填充配置，一旦组装结果被保存之后，即完成系统的集成，不再需要构件和部署。下图展示的是选择和集成组件的过程：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVo31poV3gqtBEwWAt8iav5teKtSVS8rdhnfVkj9l1sAupKvribRm6orlVCVL8MrMXd2leaRfb6Mobw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)图11 选功能-选特性-填配置

组件的选择和组装这个过程是围绕活动图展开的，总体经过选功能-选特性-填配置三个步骤，每个步骤所做的具体工作如下：

- **功能选择**：基于对产品需求功能点的捕捉，在预先组装好的组件活动图上选择当前要开发的展示功能所需要的功能组件，这一步操作决定了大体的功能点，比如这个货架需要筛选功能，那么就把筛选组件选中，不需要就不选。
- **特性选择**：功能确定之后再确定更细粒度的功能特性，比如选择展示模型组装组件之后，这个组件要组装多个字段，每个字段都有多种展示策略，再比如标题这个字段，到底是要带括号的还是不带括号的，此时需要勾选。
- **配置填充**：经过以上两个步骤，基本确定了当前要组装的展示功能所需的组件集合，但有的组件是有配置项的，比如前文举的查询商品ID这个组件，填充查询渠道配置项就是在这一步完成的。

填充好配置就可以发布了，系统运行时，多个产品功能在运行时候共享一套组件实例，差别在于执行组件的组合和配置不同。这一点是通过前文提到过的业务身份来实现的，不同业务身份关联不同的组件组装DSL及组件用户配置，最终实现差异化功能组装和执行。

## 5. 总结

组装式开发实践有没有解决最初的问题？组件分为大颗粒度的功能组件和小颗粒度的特性组件，不同颗粒度的组件都具备复用性和应变能力，因此在展示功能搭建方面的效率有了显著提升。内部数据显示，我们组的开发效率至少提升50%以上。

其次，每个组件单元都是经过良好设计的逻辑单元，单个组件的规模都有所控制，因此代码的复杂度得到明显的降低。实践结果显示，研发同学自然开发的业务组件代码圈复杂度不超过10。另外，通过信息功能的系列化编制，整个信息展示系统也有所收束，接口数由上百个减少到了个位数，大大降低了接口的维护成本。

最后，以前研发人员“过程式”地翻译业务需求，现在则需要考虑组件怎么设计。因为架构本身提供了这种条件，并且也有这种要求，研发同学在为系统“添砖加瓦”的过程中需要考虑封装和抽象问题，以集成到系统中。封装和抽象是基本的软件工程思维，这就让“体力活动”变成了“脑力活动”，现在研发同学更像是一个软件工程师，工作上也更有成就感。所以，总体上我们取得了不错的效果。

每个领域都有各自领域的复杂性，比如有的领域问题在于计算复杂，有的领域在于模型的存储和维护复杂。由于软件开发是一个工程问题，我们不能仅仅考虑技术的复杂性，同时还要考虑业务及人员的问题。科学的思维告诉我们，解决问题要讲究范式，当一个范式不满足的时候，需要有敢于突破的勇气。本文主要介绍了我们在信息展示场景下，如何通过新的开发范式来解决我们所面临的问题，希望对大家有所帮助，也欢迎大家在文末留言，跟我们交流。

## 6. 参考文献

[1] [Pattern: Backends For Frontends](https://samnewman.io/patterns/architectural/bff/)

[2] [GraphQL及元数据驱动架构在后端BFF中的实践](https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&mid=2651762036&idx=1&sn=744a07b22a3988b93403c34c4dd3ec1e&scene=21#wechat_redirect)

[3] [福特T型车丨成也标准化，败也标准化](https://zhuanlan.zhihu.com/p/29793053)

[4] [中台之上（十三）：探讨支持组装式开发的业务架构设计方法](https://www.infoq.cn/article/uk_txzgPb03mxpme4uJt)

[5] [美团内部的 Slogan「以科学和技术追求真理」是什么意思？](https://www.zhihu.com/question/309806729/answer/585745193)

[6] 叶柏林. 标准化. 中国科学技术出版, 1988.

[7] Alan W. Brown. 大规模基于构件的软件开发. 机械工业出版社, 2003.

[8] Thomas S. Kuhn. 科学的革命结构.北京大学出版社, 2012.

[9] John Ousterhout. A Philosophy Of Software Design, 2018.

[10] Tassio Vale et al. Twenty-eight Years of Component-based Software Engineering[J]. The Journal of Systems and Software, 2016.

[11] Rafael Capilla, Barbara Gallina et al. Opportunities for Software Reuse in an Uncertain World: from past to emerging trends. Software:Evolution and Process, 2019.

[12] Peter Naur, Brian Randell et al. Software Engineering[R]. NATO Science Committee, 1968.

## 7. 本文作者

陆晨、致远、陈琦等，均来自美团到店综合技术团队。

原文链接：https://mp.weixin.qq.com/s/7VlXBl9syw2ppiR3x237bA

# 【NO.364】基于代价的慢查询优化建议

## 1. 背景

慢查询是指数据库中查询时间超过指定阈值（美团设置为100ms）的SQL，它是数据库的性能杀手，也是业务优化数据库访问的重要抓手。随着美团业务的高速增长，日均慢查询量已经过亿条，此前因慢查询导致的故障约占数据库故障总数的10%以上，而且高级别的故障呈日益增长趋势。因此，对慢查询的优化已经变得刻不容缓。

那么如何优化慢查询呢？最直接有效的方法就是选用一个查询效率高的索引。关于高效率的索引推荐，主要有基于经验规则和代价的两种算法。在日常工作中，基于经验规则的推荐随处可见，对于简单的SQL，如`select * from sync_test1 where name like 'Bobby%'`，直接添加索引IX(name) 就可以取得不错的效果；但对于稍微复杂点的SQL，如`select * from sync_test1 where name like 'Bobby%' and dt > '2021-07-06'`，到底选择IX(name)、IX(dt)、IX(dt,name) 还是IX(name,dt)，该方法也无法给出准确的回答。更别说像多表Join、子查询这样复杂的场景了。所以采用基于代价的推荐来解决该问题会更加普适，因为基于代价的方法使用了和数据库优化器相同的方式，去量化评估所有的可能性，选出的是执行SQL耗费代价最小的索引。

## 2. 基于代价的优化器介绍

### 2.1 SQL执行与优化器

一条SQL在MySQL服务器中执行流程主要包含：SQL解析、基于语法树的准备工作、优化器的逻辑变化、优化器的代价准备工作、基于代价模型的优化、进行额外的优化和运行执行计划等部分。具体如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7mM6vFd0EMAYd7hrAZA4uv8MaeJDcBbHcp8jlKJWFG2uR5RZkiaVr23g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)SQL执行与优化器

### 2.2 代价模型介绍

而对于优化器来说，执行一条SQL有各种各样的方案可供选择，如表是否用索引、选择哪个索引、是否使用范围扫描、多表Join的连接顺序和子查询的执行方式等。如何从这些可选方案中选出耗时最短的方案呢？这就需要定义一个量化数值指标，这个指标就是代价(Cost)，我们分别计算出可选方案的操作耗时，从中选出最小值。

代价模型将操作分为Server层和Engine（存储引擎）层两类，Server层主要是CPU代价，Engine层主要是IO代价，比如MySQL从磁盘读取一个数据页的代价io_block_read_cost为1，计算符合条件的行代价为row_evaluate_cost为0.2。除此之外还有：

1. memory_temptable_create_cost (default 2.0) 内存临时表的创建代价。
2. memory_temptable_row_cost (default 0.2) 内存临时表的行代价。
3. key_compare_cost (default 0.1) 键比较的代价，例如排序。
4. disk_temptable_create_cost (default 40.0) 内部myisam或innodb临时表的创建代价。
5. disk_temptable_row_cost (default 1.0) 内部myisam或innodb临时表的行代价。

在MySQL 5.7中，这些操作代价的默认值都可以进行配置。为了计算出方案的总代价，还需要参考一些统计数据，如表数据量大小、元数据和索引信息等。MySQL的代价优化器模型整体如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7zHFyuOhfw449FWRvWRz3sIEJzVQpjvKKl0NaVgGXhwTRMxNIiaZxT0Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)代价模型

### 2.3 基于代价的索引选择

还是继续拿上述的`SQL select * from sync_test1 where name like 'Bobby%' and dt > '2021-07-06'`为例，我们看看MySQL优化器是如何根据代价模型选择索引的。首先，我们直接在建表时加入四个候选索引。

```
Create Table: CREATE TABLE `sync_test1` (
    `id` int(11) NOT NULL AUTO_INCREMENT,
    `cid` int(11) NOT NULL,
    `phone` int(11) NOT NULL,
    `name` varchar(10) NOT NULL,
    `address` varchar(255) DEFAULT NULL,
    `dt` datetime DEFAULT NULL,
    PRIMARY KEY (`id`),
    KEY `IX_name` (`name`),
    KEY `IX_dt` (`dt`),
    KEY `IX_dt_name` (`dt`,`name`),
    KEY `IX_name_dt` (`name`,`dt`)
    ) ENGINE=InnoDB
```

通过执行explain看出MySQL最终选择了IX_name索引。

```
mysql> explain  select * from sync_test1 where name like 'Bobby%' and dt > '2021-07-06';
+----+-------------+------------+------------+-------+-------------------------------------+---------+---------+------+------+----------+------------------------------------+
| id | select_type | table      | partitions | type  | possible_keys                       | key     | key_len | ref  | rows | filtered | Extra                              |
+----+-------------+------------+------------+-------+-------------------------------------+---------+---------+------+------+----------+------------------------------------+
|  1 | SIMPLE      | sync_test1 | NULL       | range | IX_name,IX_dt,IX_dt_name,IX_name_dt | IX_name | 12      | NULL |  572 |    36.83 | Using index condition; Using where |
+----+-------------+------------+------------+-------+-------------------------------------+---------+---------+------+------+----------+------------------------------------+
```

然后再打开MySQL追踪优化器Trace功能。可以看出，没有选择其他三个索引的原因均是因为在其他三个索引上使用range scan的代价均>= IX_name。

```
mysql> select * from INFORMATION_SCHEMA.OPTIMIZER_TRACE\G;
*************************** 1. row ***************************

TRACE: {
...
"rows_estimation": [
{
"table": "`sync_test1`",
"range_analysis": {
"table_scan": {
  "rows": 105084,
  "cost": 21628
},
...
"analyzing_range_alternatives": {
  "range_scan_alternatives": [
    {
      "index": "IX_name",
      "ranges": [
        "Bobby\u0000\u0000\u0000\u0000\u0000 <= name <= Bobbyÿÿÿÿÿ"
      ],
      "index_dives_for_eq_ranges": true,
      "rowid_ordered": false,
      "using_mrr": false,
      "index_only": false,
      "rows": 572,
      "cost": 687.41,
      "chosen": true
    },
    {
      "index": "IX_dt",
      "ranges": [
        "0x99aa0c0000 < dt"
      ],
      "index_dives_for_eq_ranges": true,
      "rowid_ordered": false,
      "using_mrr": false,
      "index_only": false,
      "rows": 38698,
      "cost": 46439,
      "chosen": false,
      "cause": "cost"
    },
    {
      "index": "IX_dt_name",
      "ranges": [
        "0x99aa0c0000 < dt"
      ],
      "index_dives_for_eq_ranges": true,
      "rowid_ordered": false,
      "using_mrr": false,
      "index_only": false,
      "rows": 38292,
      "cost": 45951,
      "chosen": false,
      "cause": "cost"
    },
    {
      "index": "IX_name_dt",
      "ranges": [
        "Bobby\u0000\u0000\u0000\u0000\u0000 <= name <= Bobbyÿÿÿÿÿ"
      ],
      "index_dives_for_eq_ranges": true,
      "rowid_ordered": false,
      "using_mrr": false,
      "index_only": false,
      "rows": 572,
      "cost": 687.41,
      "chosen": false,
      "cause": "cost"
    }
  ],
  "analyzing_roworder_intersect": {
    "usable": false,
    "cause": "too_few_roworder_scans"
  }
},
"chosen_range_access_summary": {
  "range_access_plan": {
    "type": "range_scan",
    "index": "IX_name",
    "rows": 572,
    "ranges": [
      "Bobby\u0000\u0000\u0000\u0000\u0000 <= name <= Bobbyÿÿÿÿÿ"
    ]
  },
  "rows_for_plan": 572,
  "cost_for_plan": 687.41,
  "chosen": true
}
...
}
```

下面我们根据代价模型来推演一下代价的计算过程：

1. 走全表扫描的代价：io_cost + cpu_cost = （数据页个数 * io_block_read_cost）+ (数据行数 * row_evaluate_cost + 1.1)  = （data_length / block_size + 1）+ (rows * 0.2 + 1.1) =  (9977856 / 16384 + 1) + (105084 * 0.2 + 1.1) =  21627.9。
2. 走二级索引IX_name的代价：io_cost + cpu_cost = (预估范围行数 * io_block_read_cost + 1) + (数据行数 * row_evaluate_cost + 0.01) =  (572 * 1 +  1) + (572*0.2 + 0.01) = 687.41。
3. 走二级索引IX_dt的代价：io_cost + cpu_cost = (预估范围行数 * io_block_read_cost + 1) + (数据行数 * row_evaluate_cost + 0.01)  = (38698 * 1 + 1) + (38698*0.2 + 0.01) = 46438.61。
4. 走二级索引IX_dt_name的代价: io_cost + cpu_cost = (预估范围行数 * io_block_read_cost + 1) + (数据行数 * row_evaluate_cost + 0.01)  = (38292 * 1 + 1) + (38292 * 0.2 + 0.01) = 45951.41。
5. 走二级索引IX_name_dt的代价：io_cost + cpu_cost = (预估范围行数 * io_block_read_cost + 1) + (数据行数 * row_evaluate_cost + 0.01)  = (572 * 1 +  1) + (572*0.2 + 0.01) = 687.41。

**补充说明**

1. 计算结果在小数上有偏差，因为MySQL使用%g打印浮点数，小数会以最短的方式输出。 
2. 除“+1.1 +1”这种调节值外，Cost计算还会出现+0.01, 它是为了避免index scan和range scan出现Cost的竞争。
3. Cost计算是基于MySQL的默认参数配置，如果Cost Model参数改变，optimizer_switch的选项不同，数据分布不同都会导致最终Cost的计算结果不同。 
4. data_length可查询information_schema.tables，block_size默认16K。

### 2.4 基于代价的索引推荐思路

如果想借助MySQL优化器给慢查询计算出最佳索引，那么需要真实地在业务表上添加所有候选索引。对于线上业务来说，直接添加索引的时间空间成本太高，是不可接受的。MySQL优化器选最佳索引用到的数据是索引元数据和统计数据，所以我们想是否可以通过给它提供候选索引的这些数据，而非真实添加索引的这种方式来实现。

通过深入调研MySQL的代码结构和优化器流程，我们发现是可行的：一部分存在于Server层的frm文件中，比如索引定义；另一部分存在于Engine层中，或者通过调用Engine层的接口函数来获取，比如索引中某个列的不同值个数、索引占据的页面大小等。索引相关的信息，如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz72C6lxY9mGHK0XItFGpbmLOV51yAwV8pZpEcqhuTbjfPcMmYLbotRcA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)基于代价的索引推荐思路

因为MySQL本身就支持自定义存储引擎，所以索引推荐思路是构建一个支持虚假索引的存储引擎，在它上面建立包含候选索引的空表，再采集样本数据，计算出统计数据提供给优化器，让优化器选出最优索引，整个调用关系如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7JU49Uf9QbT9Uc2SWUg8MEvMd48Dk7Vvl5PhBtGH5h2kftxHOrrC01Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)基于代价的索引推荐思路

## 3. 索引推荐实现

因为存储引擎本身并不具备对外提供服务的能力，直接在MySQL Server层修改也难以维护，所以我们将整个索引推荐系统拆分成支持虚假索引的Fakeindex存储引擎和对外提供服务的Go-Server两部分，整体架构图如下：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz76CTgUXlYHDM7JmmGuelQpI9ZF5KiaPicn3s6DRhrUkRLXQc39sxLRafw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)架构图

首先简要介绍一下Fakeindex存储引擎，这是一个轻量级的存储引擎，负责将索引的相关接口透传到Go-Server部分。因为它必须采用C++实现，与Go-Server间存在跨语言调用的问题，我们使用了Go原生的轻量级RPC技术+cgo来避免引入重量级的RPC框架，也不必引入第三方依赖包。函数调用链路如下所示，MySQL优化器调用Fakeindex的C++函数，参数转换成C语言，然后通过cgo调用到Go语言的方法，再通过Go自带的RPC客户端向服务端发起调用。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7Yd8LP3AgzBUPLjX6vOl3mV3QJM6cJLVv6R0MlMYYBgFumUsdDEtkpQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)调用链路

下面将重点阐述核心逻辑Go-Server部分，主要流程步骤如下。

### 3.1 前置校验

首先根据经验规则，排除一些不支持通过添加索引来提高查询效率的场景，如查系统库的SQL，非select、update、delete SQL等。

### 3.2 提取关键列名

这一步提取SQL可用来添加索引的候选列名，除了选择给出现在where中的列添加索引，MySQL对排序、聚合、表连接、聚合函数（如max）也支持使用索引来提高查询效率。我们对SQL进行语法树解析，在树节点的where、join、order by、group by、聚合函数中提取列名，作为索引的候选列。值得注意的是，对于某些SQL，还需结合表结构才能准确地提取，比如：

1. select * from tb1, tb2 where a = 1，列a归属tb1还是tb2取决于谁唯一包含列a。
2. select * from  tb1 natural join tb2 where tb1.a = 1，在自然连接中，tb1和tb2默认使用了相同列名进行连接，但SQL中并没有暴露出这些可用于添加索引的列。

### 3.3 生成候选索引

将提取出的关键列名进行全排列即包含所有的索引组合，如列A、B、C的所有索引组合是['A', 'B', 'C', 'AB', 'AC', 'BA', 'BC', 'CA', 'CB', 'ABC', 'ACB', 'BAC', 'BCA', 'CAB', 'CBA']，但还需排除一些索引才能得到所有的候选索引，比如：

1. 已经存在的索引，如存在AB，需排除AB、A，因为MySQL支持使用前缀索引。
2. 超过最大索引长度3072字节限制的索引。
3. 一些暂时不支持的索引，如带地理数据类型列的空间索引。

### 3.4 数据采集

直接从业务数据库采集，数据分成元数据、统计数据、样本数据三部分：

1. **元数据**：即表的定义数据，包括列定义、索引定义，可通过show create table获取。
2. **统计数据**：如表的行数、表数据大小、索引大小，可以通过查询infromation_schema.tables获取；已存在索引的cardinality（关键值：即索引列的不同值个数，值越大，索引优化效果越明显），可以通过查询mysql.innodb_index_stats表获取。
3. **样本数据**：候选索引为假索引，采集的统计数据并不包含假索引的数据，这里我们通过采集原表的样本数据来计算出假索引的统计数据。



![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7gTLBiaO14micSakg3SiblVPaG1JYMGm9OpaK80XSWbxFr92MNDp6aEia9g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)数据采集

下面介绍样本数据的采样算法，好的采样算法应该尽最大可能采集到符合原表数据分布的样本。比如基于均匀随机采样的方式`select * from table where rand() < rate`，然而它会给线上数据库造成大量I/O的问题，严重时可引发数据库故障。所以我们采用了基于块的采样方式：它参考了MySQL 8.0的直方图采样算法，如对于一张100万的表，采集10万行数，根据主键的最小值最大值将表数据均分成100个区间，每个区间取一块1000行数据，采集数据的SQL，最后将采集到的数据塞入采样表中。代码如下：

```
select A,B,C,id from table where id >= 1000 and id <= 10000 limit 1000;
select A,B,C,id from table where id >= 10000 and id <= 20000 limit 1000;
...
```

### 3.5 统计数据计算

下面举例说明两个核心统计数据的计算方式。首先是records_in_range，优化器在处理范围查询时，如果可以用索引，就会调用该函数估算走该索引可过滤出的行数，以此决定最终选用的索引。

比如，对于SQL`select * from table1 where A > 100 and B < 1000`，候选索引A、B来说，优化器会调用此函数在索引页A上估算A > 100有多少行数，在索引页B上估计B<1000的行数，例如满足条件的A有200行，B有50行，那么优化器会优先选择使用索引B。对于假索引来说，我们按照该公式：样本满足条件的范围行数 * (原表行数 / 样本表行数)，直接样本数据中查找，然后按照采样比例放大即可估算出原表中满足条件的范围行数。

其次是用于计算索引区分度的cardinality。如果直接套用上述公式：样本列上不同值个数 * (原表行数 / 样本表行数)， 如上述的候选索引A，根据样本统计出共有100个不同值，那么在原表中，该列有多少不同值？一般以为是10,000 =100 *（1,000,000/100,000）。但这样计算不适用某些场景，比如状态码字段，可能最多100个不同值。针对该问题，我们引入斜率和两趟计算来规避，流程如下：

- **第一趟计算**：取样本数据一半来统计A的不同值个数R1，区间[min_id, min_id+(max_id - min_id) / 2]。
- **第二趟计算**：取所有样本据统计A的不同值个数R2，区间[min_id, max_id] 计算斜率：R2/R1。
- **判断斜率**：如果斜率小于1.1，为固定值100，否则根据采样比例放大，为10,000。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz75HWy70B0gt1LicIFvw5MeFlar5HwNNT0UvCb13unibCnCWpoicR2iaGRjg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)统计数据计算

### 3.6 候选索引代价评估

这一步让优化器帮助我们从候选索引中选出最佳索引，主要步骤如下：

1. 建包含候选索引的表：将候选索引塞入原表定义，并把存储引擎改为Fakeindex，在推荐引擎的mysqld上创建表。
2. 通过在推荐引擎mysqld上explain format=json SQL，获取优化器选择的索引。

值得注意的是，MySQL表最多建64个索引（二级索引），计算所有候选索引的可能时，使用的是增幅比指数还恐怖的全排列算法。如下图所示，随着列数的增加，候选索引数量急剧上升，在5个候选列时的索引组合数量就超过了MySQL最大值，显然不能满足一些复杂SQL的需求。统计美团线上索引列数分布后，我们发现，95%以上的索引列数都<=3个。同时基于经验考虑，3列索引也可满足绝大部分场景，剩余场景会通过其他方式，如库表拆分来提高查询性能，而不是增加索引列个数。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7I9rZdoLibKBjYJr2lD2stsuWFicSbepBlPC4aUNamgUXfIlxTRLQ9srQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

候选索引代价评估

但即便最多推荐3列索引，在5个候选列时其排列数量85=也远超64。这里我们采用归并思路。如下图所示，将所有候选索引拆分到多个表中，采用两次计算，先让MySQL优化器选出批次一的最佳索引，可采用并行计算保证时效性，再MySQL选出批次一所有最佳索引的最佳索引，该方案可以最多支持4096个候选索引，结合最大索引3列限制，可以支持计算出17个候选列的最佳索引。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7qDLw0RSWYEKsy8NY6z6U6GVX77UbHh92LeXiaC9u4gH4y40CotgYTOg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)候选索引代价评估

## 4. 推荐质量保证

为了得到索引推荐质量大致的整体数据，我们使用美团数据库最近一周的线下慢查询数据，共246G、约3万个SQL模板用例做了一个初步测试。

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7iabJwjT4mwDFGvN6LiaIsqqRicYw7YvorTjWhwaagFfV8RibLshkX7CFDA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)建议质量保证

从结果可以看出，系统基本能覆盖到大部分的慢查询。但还是会出现无效的推荐，大致原因如下：

1. 索引推荐计算出的Cost严重依赖样本数据的质量，在当表数据分布不均或数据倾斜时会导致统计数据出现误差，导致推荐出错误索引。
2. 索引推荐系统本身存在缺陷，从而导致推荐出错误索引。
3. MySQL优化器自身存在的缺陷，导致推荐出错误索引。

因此，我们在业务添加索引前后增加了索引的有效性验证和效果追踪两个步骤，整个流程如下所示：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7VNEYEAUWGEv9bRUoH2MKzvstEPLf2OISXraw0MQTFzsrhib86QQ7uEQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)全链路

### 4.1 有效性验证

因为目前还不具备大规模数据库备份快速还原的能力，所以无法使用完整的备份数据做验证。我们近似地认为，如果推荐索引在业务库上取得较好的效果，那么在样本库也会取得不错效果。通过真正地在样本库上真实执行SQL，并添加索引来验证其有效性，验证结果展示如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7y1K27K97ajRSfCOoK7JT0O8VAAuHKYEOpBPfQia5rTia1Sl5UxPftnTg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)有效性验证

### 4.2 效果追踪

考虑到使用采样数据验证的局限性，所以当在生产环境索引添加完毕之后，会立即对添加的索引进行效果追踪。一方面通过explain验证索引是否被真正用到，以及Cost是否减小；另一方面用Flink实时跟踪该数据库的全量SQL访问数据，通过对比索引添加前后，该SQL的真实执行时间来判断索引是否有效。如果发现有性能方面的回退，则立即发出告警，周知到DBA和研发人员。生成的报告如下：



![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7te3Wl4Xv35s3keiaZBicph1ibXMs9yNSvhjfFqUXYFibIUQtwfjneiaFOIA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)效果追踪

### 4.3 仿真环境

当推荐链路出现问题时，直接在线上排查验证问题的话，很容易给业务带来安全隐患，同时也降低了系统的稳定性。对此我们搭建了离线仿真环境，利用数据库备份构建了和生产环境一样的数据源，并完整复刻了线上推荐链路的各个步骤，在仿真环境回放异常案例，复现问题、排查根因，反复验证改进方案后再上线到生产系统，进而不断优化现有系统，提升推荐质量。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz76kPmRMCL38GGgp2Dr5L7IE125LIRwSKNutNgq5iaaQYz9f8mhIVXHbg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)仿真环境

### 4.4 测试案例库

在上线过程中，往往会出现改进方案修复了一个Bug，带来了更多Bug的情况。能否做好索引推荐能力的回归测试，直接决定了推荐质量的稳定性。于是，我们参考了阿里云的技术方案，计划构建一个尽可能完备的测试案例库用于衡量索引推荐服务能力强弱。但考虑影响MySQL索引选择的因素众多，各因素间的组合，SQL的复杂性，如果人为去设计测试用例是是不切实际的，我们通过下列方法自动化收集测试用例：

1. 利用美团线上的丰富数据，以影响MySQL索引选择的因素特征为抓手，直接从全量SQL和慢SQL中抽取最真实的案例，不断更新现有测试案例库。
2. 在生产的推荐系统链路上埋点，自动收集异常案例，回流到现有的测试案例库。
3. 对于现有数据没有覆盖到的极端场景，采用人为构造的方案，补充测试用例。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7jaYA8u7hp6Dm6zjIibJtTic5e7lOiaibcWzOrupQlVoYff1Gr41ltTzF2A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)测试案例库

## 5. 慢查询治理运营

我们主要从时间维度的三个方向将慢查询接入索引推荐，推广治理：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7WBqPZ772NaoGmTdCkVZLAp3GKsoa4AWWWQe3V0Tu3zuKVGXDhDGP4A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)慢查询治理运营

### 5.1 过去-历史慢查询

这类慢查询属于过去产生的，并且一直存在，数量较多，治理推动力不足，可通过收集历史慢查询日志发现，分成两类接入：

1. **核心数据库**：该类慢查询通常会被周期性地关注，如慢查询周报、月报，可直接将优化建议提前生成出来，接入它们，一并运营治理。
2. **普通数据库**：可将优化建议直接接入数据库平台的慢查询模块，让研发自助地选择治理哪些慢查询。

### 5.2 现在-新增慢查询

这类慢查询属于当前产生的，数量较少，属于治理的重点，也可通过实时收集慢查询日志发现，分成两类接入：

1. **影响程度一般的慢查询**：可通过实时分析慢查询日志，对比历史慢查询，识别出新增慢查询，并生成优化建议，为用户创建数据库风险项，跟进治理。
2. **影响程度较大的慢查询**：该类通常会引发数据库告警，如慢查询导致数据库Load过高，可通过故障诊断根因系统，识别出具体的慢查询SQL，并生成优化建议，及时推送到故障处理群，降低故障处理时长。

### 5.3 未来-潜在慢查询

这类查询属于当前还没被定义成慢查询，随着时间推进可能变成演变成慢查询，对于一些核心业务来说，往往会引发故障，属于他们治理的重点，分成两类接入：

1. **未上线的准慢查询**：项目准备上线而引入的新的准慢查询，可接入发布前的集成测试流水线，Java项目可通过 agentmain的代理方式拦截被测试用例覆盖到的SQL，再通过经验+explain识别出慢查询，并生成优化建议，给用户在需求管理系统上创建缺陷任务，解决后才能发布上线。
2. **已上线的准慢查询**：该类属于当前执行时间较快的SQL，随着表数据量的增加，会演变成慢查询，最常见的就是全表扫描，这类可通过增加慢查询配置参数log_queries_not_using_indexes记录到慢日志，并生成优化建议，为用户创建数据库风险项，跟进治理。

## 6. 项目运行情况

当前，主要以新增慢查询为突破点，重点为全表扫描推荐优化建议。目前我们已经灰度接入了一小部分业务，共分析了六千多条慢查询，推荐了一千多条高效索引建议。另外，美团内部的研发同学也可通过数据库平台自助发起SQL优化建议工单，如下图所示：



![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz7spmwtSR1u3dbS5QGUZOZZjarZicNQEh1rtnWIQ5J0sFhAgH1MVcqu5w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)RDS平台发起

另外在美团内部，我们已经和数据库告警打通，实现了故障发现、根因分析、解决方案的自动化处理，极大地提高了故障处理效率。下面是一个展示案例，当数据库集群发生告警，我们会拉一个故障群，先通过根因定位系统，如果识别出慢查询造成的，会马上调用SQL优化建议系统，推荐出索引，整个处理流程是分钟级别，都会在群里面推送最新消息。如下图所示：



![图片](https://mmbiz.qpic.cn/mmbiz_jpg/hEx03cFgUsUZwgCRT3IibKB8g48hV0Fz70Ks7icGD2ID2erY4kfEEmXiaKVagAjLnuicXmebrUU4HQkpo0L3YtB6bg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)告警诊断

## 7. 未来规划

考虑到美团日均产生近亿级别的慢查询数据，为了实现对它们的诊断分析，我们还需要提高系统大规模的数据并发处理的能力。另外，当前该系统还是针对单SQL的优化，没有考虑维护新索引带来的代价，如占用额外的磁盘空间，使写操作变慢，也没有考虑到MySQL选错索引引发其他SQL的性能回退。对于业务或者DBA来说，我们更多关心的是整个数据库或者集群层面的优化。

业界如阿里云的DAS则是站在全局的角度考量，综合考虑各个因素，输出需要创建的新索引、需要改写的索引、需要删除的索引，实现数据库性能最大化提升，同时最大化降低磁盘空间消耗。未来我们也将不断优化和改进，实现类似基于Workload的全局优化。

## 参考资料

- [MySQL Writing a Custom Storage Engine](https://dev.mysql.com/doc/internals/en/custom-engine.html)
- [MySQL Optimizer Guide](https://www.slideshare.net/morgo/mysql-80-optimizer-guide)
- [MySQL 直方图](http://mysql.taobao.org/monthly/2021/05/03/)
- [Golang cgo](https://chai2010.cn/advanced-go-programming-book/ch2-cgo/ch2-02-basic.html)
- [阿里云-DAS之基于Workload的全局自动优化实践](https://developer.aliyun.com/article/781036?spm=a2c6h.14164896.0.0.485d58c1BBNlgQ)
- [SQL诊断优化，以后就都交给数据库自治服务DAS吧](https://developer.aliyun.com/article/754388?spm=a2c6h.14164896.0.0.522e2025ZYrP98)

**本文作者**

粟含，美团基础研发平台/基础技术部/数据库平台研发组工程师。

原文链接：https://mp.weixin.qq.com/s/MaQTI4afIh2Zehc-F-iisQ

# 【NO.365】设计模式二三事

## 1.引言

话说这是在程序员世界里一对师徒的对话：

“老师，我最近在写代码时总感觉自己的代码很不优雅，有什么办法能优化吗？”

“嗯，可以考虑通过教材系统学习，从注释、命名、方法和异常等多方面实现整洁代码。”

“然而，我想说的是，我的代码是符合各种编码规范的，但是从实现上却总是感觉不够简洁，而且总是需要反复修改！”学生小明叹气道。

老师看了看小明的代码说：“我明白了，这是系统设计上的缺陷。总结就是抽象不够、可读性低、不够健壮。”

“对对对，那怎么能迅速提高代码的可读性、健壮性、扩展性呢？”小明急不可耐地问道。

老师敲了敲小明的头：“不要太浮躁，没有什么方法能让你立刻成为系统设计专家。但是对于你的问题，我想**设计模式**可以帮到你。”

“设计模式？”小明不解。

“是的。”老师点了点头，“世上本没有路，走的人多了，便变成了路。在程序员的世界中，本没有设计模式，写代码是人多了，他们便总结出了一套能提高开发和维护效率的套路，这就是设计模式。设计模式不是什么教条或者范式，它可以说是一种在特定场景下普适且可复用的解决方案，是一种可以用于提高代码可读性、可扩展性、可维护性和可测性的最佳实践。”

“哦哦，我懂了，那我应该如何去学习呢？”

“不急，接下来我来带你慢慢了解设计模式。”

## 2.奖励的发放策略

第一天，老师问小明：“你知道活动营销吗？”

“这我知道，活动营销是指企业通过参与社会关注度高的已有活动，或整合有效的资源自主策划大型活动，从而迅速提高企业及其品牌的知名度、美誉度和影响力，常见的比如有抽奖、红包等。”

老师点点头：“是的。我们假设现在就要做一个营销，需要用户参与一个活动，然后完成一系列的任务，最后可以得到一些奖励作为回报。活动的奖励包含美团外卖、酒旅和美食等多种品类券，现在需要你帮忙设计一套奖励发放方案。”

因为之前有过类似的开发经验，拿到需求的小明二话不说开始了编写起了代码：

```
// 奖励服务
class RewardService {
    // 外部服务
    private WaimaiService waimaiService;
    private HotelService hotelService;
    private FoodService foodService;
    // 使用对入参的条件判断进行发奖
    public void issueReward(String rewardType, Object ... params) {
        if ("Waimai".equals(rewardType)) {
            WaimaiRequest request = new WaimaiRequest();
            // 构建入参
            request.setWaimaiReq(params);
            waimaiService.issueWaimai(request);
        } else if ("Hotel".equals(rewardType)) {
            HotelRequest request = new HotelRequest();
            request.addHotelReq(params);
            hotelService.sendPrize(request);
        } else if ("Food".equals(rewardType)) {
            FoodRequest request = new FoodRequest(params);
            foodService.getCoupon(request);
        } else {
           throw new IllegalArgumentException("rewardType error!");
        }
    }
}
```

小明很快写好了Demo，然后发给老师看。

“假如我们即将接入新的打车券，这是否意味着你必须要修改这部分代码？”老师问道。

小明愣了一愣，没等反应过来老师又问：”假如后面美团外卖的发券接口发生了改变或者替换，这段逻辑是否必须要同步进行修改？”

小明陷入了思考之中，一时间没法回答。

经验丰富的老师一针见血地指出了这段设计的问题：“你这段代码有两个主要问题，一是不符合**开闭原则**，可以预见，如果后续新增品类券的话，需要直接修改主干代码，而我们提倡代码应该是对修改封闭的；二是不符合**迪米特法则**，发奖逻辑和各个下游接口高度耦合，这导致接口的改变将直接影响到代码的组织，使得代码的可维护性降低。”

小明恍然大悟：“那我将各个同下游接口交互的功能抽象成单独的服务，封装其参数组装及异常处理，使得发奖主逻辑与其解耦，是否就能更具备扩展性和可维护性？”

“这是个不错的思路。之前跟你介绍过设计模式，这个案例就可以使用**策略模式**和**适配器模式**来优化。”

小明借此机会学习了这两个设计模式。首先是策略模式：

> 策略模式定义了一系列的算法，并将每一个算法封装起来，使它们可以相互替换。策略模式通常包含以下角色：
>
> - 抽象策略（Strategy）类：定义了一个公共接口，各种不同的算法以不同的方式实现这个接口，环境角色使用这个接口调用不同的算法，一般使用接口或抽象类实现。
> - 具体策略（Concrete Strategy）类：实现了抽象策略定义的接口，提供具体的算法实现。
> - 环境（Context）类：持有一个策略类的引用，最终给客户端调用。

然后是适配器模式：

> 适配器模式：将一个类的接口转换成客户希望的另外一个接口，使得原本由于接口不兼容而不能一起工作的那些类能一起工作。适配器模式包含以下主要角色：
>
> - 目标（Target）接口：当前系统业务所期待的接口，它可以是抽象类或接口。
> - 适配者（Adaptee）类：它是被访问和适配的现存组件库中的组件接口。
> - 适配器（Adapter）类：它是一个转换器，通过继承或引用适配者的对象，把适配者接口转换成目标接口，让客户按目标接口的格式访问适配者。

结合优化思路，小明首先设计出了策略接口，并通过适配器的思想将各个下游接口类适配成策略类：

```
// 策略接口
interface Strategy {
    void issue(Object ... params);
}
// 外卖策略
class Waimai implements Strategy {
   private WaimaiService waimaiService;
    @Override
    public void issue(Object... params) {
        WaimaiRequest request = new WaimaiRequest();
        // 构建入参
        request.setWaimaiReq(params);
        waimaiService.issueWaimai(request);
    }
}
// 酒旅策略
class Hotel implements Strategy {
   private HotelService hotelService;
    @Override
    public void issue(Object... params) {
        HotelRequest request = new HotelRequest();
        request.addHotelReq(params);
        hotelService.sendPrize(request);
    }
}
// 美食策略
class Food implements Strategy {
   private FoodService foodService;
    @Override
    public void issue(Object... params) {
        FoodRequest request = new FoodRequest(params);
        foodService.payCoupon(request);
    }
}
```

然后，小明创建策略模式的环境类，并供奖励服务调用：

```
// 使用分支判断获取的策略上下文
class StrategyContext {
    public static Strategy getStrategy(String rewardType) {
        switch (rewardType) {
            case "Waimai":
                return new Waimai();
            case "Hotel":
                return new Hotel();
            case "Food":
                return new Food();
            default:
                throw new IllegalArgumentException("rewardType error!");
        }
    }
}
// 优化后的策略服务
class RewardService {
    public void issueReward(String rewardType, Object ... params) {
        Strategy strategy = StrategyContext.getStrategy(rewardType);
        strategy.issue(params);
    }
}
```

小明的代码经过优化后，虽然结构和设计上比之前要复杂不少，但考虑到健壮性和拓展性，还是非常值得的。

“看，我这次优化后的版本是不是很完美？”小明洋洋得意地说。

“耦合度确实降低了，但还能做的更好。”

“怎么做？”小明有点疑惑。

“我问你，策略类是有状态的模型吗？如果不是是否可以考虑做成单例的？”

“的确如此。”小明似乎明白了。

“还有一点，环境类的获取策略方法职责很明确，但是你依然没有做到完全对修改封闭。”

经过老师的点拨，小明很快也领悟到了要点：“那我可以将策略类单例化以减少开销，并实现自注册的功能彻底解决分支判断。”

小明列出单例模式的要点：

> 单例模式设计模式属于创建型模式，它提供了一种创建对象的最佳方式。
>
> 这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。

最终，小明在策略环境类中使用一个注册表来记录各个策略类的注册信息，并提供接口供策略类调用进行注册。同时使用**饿汉式单例模式**去优化策略类的设计：

```
// 策略上下文，用于管理策略的注册和获取
class StrategyContext {
    private static final Map<String, Strategy> registerMap = new HashMap<>();
    // 注册策略
    public static void registerStrategy(String rewardType, Strategy strategy) {
        registerMap.putIfAbsent(rewardType, strategy);
    }
    // 获取策略
    public static Strategy getStrategy(String rewardType) {
        return registerMap.get(rewardType);
    }
}
// 抽象策略类
abstract class AbstractStrategy implements Strategy {
    // 类注册方法
    public void register() {
        StrategyContext.registerStrategy(getClass().getSimpleName(), this);
    }
}
// 单例外卖策略
class Waimai extends AbstractStrategy implements Strategy {
    private static final Waimai instance = new Waimai();
   private WaimaiService waimaiService;
    private Waimai() {
        register();
    }
    public static Waimai getInstance() {
        return instance;
    }
    @Override
    public void issue(Object... params) {
        WaimaiRequest request = new WaimaiRequest();
        // 构建入参
        request.setWaimaiReq(params);
        waimaiService.issueWaimai(request);
    }
}
// 单例酒旅策略
class Hotel extends AbstractStrategy implements Strategy {
   private static final Hotel instance = new Hotel();
   private HotelService hotelService;
    private Hotel() {
        register();
    }
    public static Hotel getInstance() {
        return instance;
    }
    @Override
    public void issue(Object... params) {
        HotelRequest request = new HotelRequest();
        request.addHotelReq(params);
        hotelService.sendPrize(request);
    }
}
// 单例美食策略
class Food extends AbstractStrategy implements Strategy {
   private static final Food instance = new Food();
   private FoodService foodService;
    private Food() {
        register();
    }
    public static Food getInstance() {
        return instance;
    }
    @Override
    public void issue(Object... params) {
        FoodRequest request = new FoodRequest(params);
        foodService.payCoupon(request);
    }
}
```

最终，小明设计完成的结构类图如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVm2AEU6AQbUAyCqHgKROft5pgQOhtR90ia1fng0MAd8CLhTOWZA1J7SvIAZPTp5azBtREeQJwwrfQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)奖励发放策略_类图

如果使用了Spring框架，还可以利用Spring的Bean机制来代替上述的部分设计，直接使用`@Component`和`@PostConstruct`注解即可完成单例的创建和注册，代码会更加简洁。

至此，经过了多次讨论、反思和优化，小明终于得到了一套低耦合高内聚，同时符合开闭原则的设计。

“老师，我开始学会利用设计模式去解决已发现的问题。这次我做得怎么样？”

“合格。但是，依然要戒骄戒躁。”

## 3.任务模型的设计

“之前让你设计奖励发放策略你还记得吗？”老师忽然问道。

“当然记得。一个好的设计模式，能让工作事半功倍。”小明答道。

“嗯，那会提到了活动营销的组成部分，除了奖励之外，貌似还有任务吧。”

小明点了点头，老师接着说：“现在，我想让你去完成任务模型的设计。你需要重点关注状态的流转变更，以及状态变更后的消息通知。”

小明欣然接下了老师给的难题。他首先定义了一套任务状态的枚举和行为的枚举：

```
// 任务状态枚举
@AllArgsConstructor
@Getter
enum TaskState {
    INIT("初始化"),
    ONGOING( "进行中"),
    PAUSED("暂停中"),
    FINISHED("已完成"),
    EXPIRED("已过期")
    ;
    private final String message;
}
// 行为枚举
@AllArgsConstructor
@Getter
enum ActionType {
    START(1, "开始"),
    STOP(2, "暂停"),
    ACHIEVE(3, "完成"),
    EXPIRE(4, "过期")
    ;
    private final int code;
    private final String message;
}
```

然后，小明对开始编写状态变更功能：

```
class Task {
    private Long taskId;
    // 任务的默认状态为初始化
    private TaskState state = TaskState.INIT;
    // 活动服务
    private ActivityService activityService;
    // 任务管理器
    private TaskManager taskManager;
    // 使用条件分支进行任务更新
    public void updateState(ActionType actionType) {
        if (state == TaskState.INIT) {
            if (actionType == ActionType.START) {
                state = TaskState.ONGOING;
            }
        } else if (state == TaskState.ONGOING) {
            if (actionType == ActionType.ACHIEVE) {
                state = TaskState.FINISHED;
                // 任务完成后进对外部服务进行通知
                activityService.notifyFinished(taskId);
                taskManager.release(taskId);
            } else if (actionType == ActionType.STOP) {
                state = TaskState.PAUSED;
            } else if (actionType == ActionType.EXPIRE) {
                state = TaskState.EXPIRED;
            }
        } else if (state == TaskState.PAUSED) {
            if (actionType == ActionType.START) {
                state = TaskState.ONGOING;
            } else if (actionType == ActionType.EXPIRE) {
                state = TaskState.EXPIRED;
            }
        }
    }
}
```

在上述的实现中，小明在`updateState`方法中完成了2个重要的功能：

1. 接收不同的行为，然后更新当前任务的状态；
2. 当任务过期时，通知任务所属的活动和任务管理器。

诚然，随着小明的系统开发能力和代码质量意识的提升，他能够认识到这种功能设计存在缺陷。

“老师，我的代码还是和之前说的那样，不够优雅。”

“哦，你自己说说看有什么问题？”

“第一，方法中使用条件判断来控制语句，但是当条件复杂或者状态太多时，条件判断语句会过于臃肿，可读性差，且不具备扩展性，维护难度也大。且增加新的状态时要添加新的if-else语句，这违背了开闭原则，不利于程序的扩展。”

老师表示同意，小明接着说：“第二，任务类不够高内聚，它在通知实现中感知了其他领域或模块的模型，如活动和任务管理器，这样代码的耦合度太高，不利于扩展。”

老师赞赏地说道：“很好，你有意识能够自主发现代码问题所在，已经是很大的进步了。”

“那这个问题应该怎么去解决呢？”小明继续发问。

“这个同样可以通过设计模式去优化。首先是状态流转的控制可以使用**状态模式**，其次，任务完成时的通知可以用到**观察者模式**。”

收到指示后，小明马上去学习了状态模式的结构：

> 状态模式：对有状态的对象，把复杂的“判断逻辑”提取到不同的状态对象中，允许状态对象在其内部状态发生改变时改变其行为。状态模式包含以下主要角色：
>
> - 环境类（Context）角色：也称为上下文，它定义了客户端需要的接口，内部维护一个当前状态，并负责具体状态的切换。
> - 抽象状态（State）角色：定义一个接口，用以封装环境对象中的特定状态所对应的行为，可以有一个或多个行为。
> - 具体状态（Concrete State）角色：实现抽象状态所对应的行为，并且在需要的情况下进行状态切换。

根据状态模式的定义，小明将TaskState枚举类扩展成多个状态类，并具备完成状态的流转的能力；然后优化了任务类的实现：

```
// 任务状态抽象接口
interface State {
    // 默认实现，不做任何处理
    default void update(Task task, ActionType actionType) {
        // do nothing
    }
}
// 任务初始状态
class TaskInit implements State {
    @Override
    public void update(Task task, ActionType actionType) {
        if  (actionType == ActionType.START) {
            task.setState(new TaskOngoing());
        }
    }
}
// 任务进行状态
class TaskOngoing implements State {
    private ActivityService activityService;
    private TaskManager taskManager; 
    @Override
    public void update(Task task, ActionType actionType) {
        if (actionType == ActionType.ACHIEVE) {
            task.setState(new TaskFinished());
            // 通知
            activityService.notifyFinished(taskId);
            taskManager.release(taskId);
        } else if (actionType == ActionType.STOP) {
            task.setState(new TaskPaused());
        } else if (actionType == ActionType.EXPIRE) {
            task.setState(new TaskExpired());
        }
    }
}
// 任务暂停状态
class TaskPaused implements State {
    @Override
    public void update(Task task, ActionType actionType) {
        if (actionType == ActionType.START) {
            task.setState(new TaskOngoing());
        } else if (actionType == ActionType.EXPIRE) {
            task.setState(new TaskExpired());
        }
    }
}
// 任务完成状态
class TaskFinished implements State {

}
// 任务过期状态
class TaskExpired implements State {

}
@Data
class Task {
    private Long taskId;
    // 初始化为初始态
    private State state = new TaskInit();
    // 更新状态
    public void updateState(ActionType actionType) {
        state.update(this, actionType);
    }
}
```

小明欣喜地看到，经过状态模式处理后的任务类的耦合度得到降低，符合开闭原则。状态模式的优点在于符合单一职责原则，状态类职责明确，有利于程序的扩展。但是这样设计的代价是状态类的数目增加了，因此状态流转逻辑越复杂、需要处理的动作越多，越有利于状态模式的应用。除此之外，状态类的自身对于开闭原则的支持并没有足够好，如果状态流转逻辑变化频繁，那么可能要慎重使用。

处理完状态后，小明又根据老师的指导使用**观察者模式**去优化任务完成时的通知：

> 观察者模式：指多个对象间存在一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。这种模式有时又称作发布-订阅模式、模型-视图模式，它是对象行为型模式。观察者模式的主要角色如下。
>
> - 抽象主题（Subject）角色：也叫抽象目标类，它提供了一个用于保存观察者对象的聚集类和增加、删除观察者对象的方法，以及通知所有观察者的抽象方法。
> - 具体主题（Concrete Subject）角色：也叫具体目标类，它实现抽象目标中的通知方法，当具体主题的内部状态发生改变时，通知所有注册过的观察者对象。
> - 抽象观察者（Observer）角色：它是一个抽象类或接口，它包含了一个更新自己的抽象方法，当接到具体主题的更改通知时被调用。
> - 具体观察者（Concrete Observer）角色：实现抽象观察者中定义的抽象方法，以便在得到目标的更改通知时更新自身的状态。

小明首先设计好抽象目标和抽象观察者，然后将活动和任务管理器的接收通知功能定制成具体观察者：

```
// 抽象观察者
interface Observer {
    void response(Long taskId); // 反应
}
// 抽象目标
abstract class Subject {
    protected List<Observer> observers = new ArrayList<Observer>();
    // 增加观察者方法
    public void add(Observer observer) {
        observers.add(observer);
    }
    // 删除观察者方法
    public void remove(Observer observer) {
        observers.remove(observer);
    }
    // 通知观察者方法
    public void notifyObserver(Long taskId) {
        for (Observer observer : observers) {
            observer.response(taskId);
        }
    }
}
// 活动观察者
class ActivityObserver implements Observer {
    private ActivityService activityService;
    @Override
    public void response(Long taskId) {
        activityService.notifyFinished(taskId);
    }
}
// 任务管理观察者
class TaskManageObserver implements Observer {
    private TaskManager taskManager;
    @Override
    public void response(Long taskId) {
        taskManager.release(taskId);
    }
}
```

最后，小明将任务进行状态类优化成使用通用的通知方法，并在任务初始态执行状态流转时定义任务进行态所需的观察者：

```
// 任务进行状态
class TaskOngoing extends Subject implements State {  
    @Override
    public void update(Task task, ActionType actionType) {
        if (actionType == ActionType.ACHIEVE) {
            task.setState(new TaskFinished());
            // 通知
            notifyObserver(task.getTaskId());
        } else if (actionType == ActionType.STOP) {
            task.setState(new TaskPaused());
        } else if (actionType == ActionType.EXPIRE) {
            task.setState(new TaskExpired());
        }
    }
}
// 任务初始状态
class TaskInit implements State {
    @Override
    public void update(Task task, ActionType actionType) {
        if  (actionType == ActionType.START) {
            TaskOngoing taskOngoing = new TaskOngoing();
            taskOngoing.add(new ActivityObserver());
            taskOngoing.add(new TaskManageObserver());
            task.setState(taskOngoing);
        }
    }
}
```

最终，小明设计完成的结构类图如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVm2AEU6AQbUAyCqHgKROft0Kw1MaCKWtIGT9nOia4JrfAicRdbLwXicjjnia8IA7YfY8PR2iabI1nbgAA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)任务模型设计_类图

通过观察者模式，小明让任务状态和通知方实现松耦合（实际上观察者模式还没能做到完全的解耦，如果要做进一步的解耦可以考虑学习并使用**发布-订阅模式**，这里也不再赘述）。

至此，小明成功使用状态模式设计出了高内聚、高扩展性、单一职责的任务的整个状态机实现，以及做到松耦合的、符合依赖倒置原则的任务状态变更通知方式。

“老师，我逐渐能意识到代码的设计缺陷，并学会利用较为复杂的设计模式做优化。”

“不错，再接再厉！”

## 4.活动的迭代重构

“小明，这次又有一个新的任务。”老师出现在正在认真阅读《设计模式》的小明的面前。

“好的。刚好我已经学习了设计模式的原理，终于可以派上用场了。”

“之前你设计开发了活动模型，现在我们需要在任务型活动的参与方法上增加一层风险控制。”

“OK。借此机会，我也想重构一下之前的设计。”

活动模型的特点在于其组成部分较多，小明原先的活动模型的构建方式是这样的：

```
// 抽象活动接口
interface ActivityInterface {
   void participate(Long userId);
}
// 活动类
class Activity implements ActivityInterface {
    private String type;
    private Long id;
    private String name;
    private Integer scene;
    private String material;
      
    public Activity(String type) {
        this.type = type;
        // id的构建部分依赖于活动的type
        if ("period".equals(type)) {
            id = 0L;
        }
    }
    public Activity(String type, Long id) {
        this.type = type;
        this.id = id;
    }
    public Activity(String type, Long id, Integer scene) {
        this.type = type;
        this.id = id;
        this.scene = scene;
    }
    public Activity(String type, String name, Integer scene, String material) {
        this.type = type;
        this.scene = scene;
        this.material = material;
        // name的构建完全依赖于活动的type
        if ("period".equals(type)) {
            this.id = 0L;
            this.name = "period" + name;
        } else {
            this.name = "normal" + name;
        }
    }
    // 参与活动
   @Override
    public void participate(Long userId) {
        // do nothing
    }
}
// 任务型活动
class TaskActivity extends Activity {
    private Task task;
    public TaskActivity(String type, String name, Integer scene, String material, Task task) {
        super(type, name, scene, material);
        this.task = task;
    }
    // 参与任务型活动
    @Override
    public void participate(Long userId) {
        // 更新任务状态为进行中
        task.getState().update(task, ActionType.START);
    }
}
```

经过自主分析，小明发现活动的构造不够合理，主要问题表现在：

1. 活动的构造组件较多，导致可以组合的构造函数太多，尤其是在模型增加字段时还需要去修改构造函数；
2. 部分组件的构造存在一定的顺序关系，但是当前的实现没有体现顺序，导致构造逻辑比较混乱，并且存在部分重复的代码。

发现问题后，小明回忆自己的学习成果，马上想到可以使用创建型模式中的**建造者模式**去做重构：

> 建造者模式：指将一个复杂对象的构造与它的表示分离，使同样的构建过程可以创建不同的表示。它是将一个复杂的对象分解为多个简单的对象，然后一步一步构建而成。它将变与不变相分离，即产品的组成部分是不变的，但每一部分是可以灵活选择的。建造者模式的主要角色如下:
>
> 1. 产品角色（Product）：它是包含多个组成部件的复杂对象，由具体建造者来创建其各个零部件。
> 2. 抽象建造者（Builder）：它是一个包含创建产品各个子部件的抽象方法的接口，通常还包含一个返回复杂产品的方法 getResult()。
> 3. 具体建造者(Concrete Builder）：实现 Builder 接口，完成复杂产品的各个部件的具体创建方法。
> 4. 指挥者（Director）：它调用建造者对象中的部件构造与装配方法完成复杂对象的创建，在指挥者中不涉及具体产品的信息。

根据建造者模式的定义，上述活动的每个字段都是一个产品。于是，小明可以通过在活动里面实现静态的建造者类来简易地实现：

```
// 活动类
class Activity implements ActivityInterface {
    protected String type;
    protected Long id;
    protected String name;
    protected Integer scene;
    protected String material;
    // 全参构造函数
   public Activity(String type, Long id, String name, Integer scene, String material) {
        this.type = type;
        this.id = id;
        this.name = name;
        this.scene = scene;
        this.material = material;
    }
    @Override
    public void participate(Long userId) {
        // do nothing
    }
    // 静态建造器类，使用奇异递归模板模式允许继承并返回继承建造器类
    public static class Builder<T extends Builder<T>> {
        protected String type;
        protected Long id;
        protected String name;
        protected Integer scene;
        protected String material;
        public T setType(String type) {
            this.type = type;
            return (T) this;
        }
        public T setId(Long id) {
            this.id = id;
            return (T) this;
        }
        public T setId() {
            if ("period".equals(this.type)) {
                this.id = 0L;
            }
            return (T) this;
        }
        public T setScene(Integer scene) {
            this.scene = scene;
            return (T) this;
        }
        public T setMaterial(String material) {
            this.material = material;
            return (T) this;
        }
        public T setName(String name) {
            if ("period".equals(this.type)) {
                this.name = "period" + name;
            } else {
                this.name = "normal" + name;
            }
            return (T) this;
        }
        public Activity build(){
            return new Activity(type, id, name, scene, material);
        }
    }
}
// 任务型活动
class TaskActivity extends Activity {
    protected Task task;
   // 全参构造函数
    public TaskActivity(String type, Long id, String name, Integer scene, String material, Task task) {
        super(type, id, name, scene, material);
        this.task = task;
    }
   // 参与任务型活动
    @Override
    public void participate(Long userId) {
        // 更新任务状态为进行中
        task.getState().update(task, ActionType.START);
    }
    // 继承建造器类
    public static class Builder extends Activity.Builder<Builder> {
        private Task task;
        public Builder setTask(Task task) {
            this.task = task;
            return this;
        }
        public TaskActivity build(){
            return new TaskActivity(type, id, name, scene, material, task);
        }
    }
}
```

小明发现，上面的建造器没有使用诸如抽象建造器类等完整的实现，但是基本是完成了活动各个组件的建造流程。使用建造器的模式下，可以先按顺序构建字段type，然后依次构建其他组件，最后使用build方法获取建造完成的活动。这种设计一方面封装性好，构建和表示分离；另一方面扩展性好，各个具体的建造者相互独立，有利于系统的解耦。可以说是一次比较有价值的重构。在实际的应用中，如果字段类型多，同时各个字段只需要简单的赋值，可以直接引用Lombok的@Builder注解来实现轻量的建造者。

重构完活动构建的设计后，小明开始对参加活动方法增加风控。最简单的方式肯定是直接修改目标方法：

```
public void participate(Long userId) {
    // 对目标用户做风险控制，失败则抛出异常
    Risk.doControl(userId);
    // 更新任务状态为进行中
    task.state.update(task, ActionType.START);
}
```

但是考虑到，最好能尽可能避免对旧方法的直接修改，同时为方法增加风控，也是一类比较常见的功能新增，可能会在多处使用。

“老师，风险控制会出现在多种活动的参与方法中吗？”

“有这个可能性。有的活动需要风险控制，有的不需要。风控像是在适当的时候对参与这个方法的装饰。”

“对了，装饰器模式！”

小明马上想到用**装饰器模式**来完成设计：

> 装饰器模式的定义：指在不改变现有对象结构的情况下，动态地给该对象增加一些职责（即增加其额外功能）的模式，它属于对象结构型模式。装饰器模式主要包含以下角色：
>
> 1. 抽象构件（Component）角色：定义一个抽象接口以规范准备接收附加责任的对象。
> 2. 具体构件（ConcreteComponent）角色：实现抽象构件，通过装饰角色为其添加一些职责。
> 3. 抽象装饰（Decorator）角色：继承抽象构件，并包含具体构件的实例，可以通过其子类扩展具体构件的功能。
> 4. 具体装饰（ConcreteDecorator）角色：实现抽象装饰的相关方法，并给具体构件对象添加附加的责任。

小明使用了装饰器模式后，新的代码就变成了这样：

```
// 抽象装饰角色
abstract class ActivityDecorator implements ActivityInterface {
    protected ActivityInterface activity;
    public ActivityDecorator(ActivityInterface activity) {
        this.activity = activity;
    }
    public abstract void participate(Long userId);
}
// 能够对活动做风险控制的包装类
class RiskControlDecorator extends ActivityDecorator {
    public RiskControlDecorator(ActivityInterface activity) {
        super(activity);
    }
    @Override
   public void participate(Long userId) {
        // 对目标用户做风险控制，失败则抛出异常
       Risk.doControl(userId);
        // 更新任务状态为进行中
        activity.participate(userId);
    }
}
```

最终，小明设计完成的结构类图如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/hEx03cFgUsVm2AEU6AQbUAyCqHgKROftuibJkJibU5BHr7pHeKicDI6ekm7TWN8eFrGDVpSFD8LLpzCHuibUzqn0UA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)活动迭代重构_类图

最终，小明通过自己的思考分析，结合学习的设计模式知识，完成了活动模型的重构和迭代。

“老师，我已经能做到自主分析功能特点，并合理应用设计模式去完成程序设计和代码重构了，实在太感谢您了。”

“设计模式作为一种软件设计的最佳实践，你已经很好地理解并应用于实践了，非常不错。但学海无涯，还需持续精进！”

## 5.结语

本文以三个实际场景为出发点，借助小明和老师两个虚拟的人物，试图以一种较为诙谐的“对话”方式来讲述设计模式的应用场景、优点和缺点。如果大家想要去系统性地了解设计模式，也可以通过市面上很多的教材进行学习，都介绍了经典的23种设计模式的结构和实现。不过，很多教材的内容即便配合了大量的示例，但有时也会让人感到费解，主要原因在于：一方面，很多案例比较脱离实际的应用场景；另一方面，部分设计模式显然更适用于大型复杂的结构设计，而当其应用到简单的场景时，仿佛让代码变得更加繁琐、冗余。因此，本文希望通过这种“对话+代码展示+结构类图”的方式，以一种更易懂的方式来介绍设计模式。

当然，本文只讲述了部分比较常见的设计模式，还有其他的设计模式，仍然需要同学们去研读经典著作，举一反三，学以致用。我们也希望通过学习设计模式能让更多的同学在系统设计能力上得到提升。

## 6.参考资料

- Gamma E. 设计模式: 可复用面向对象软件的基础 [M]. 机械工业出版社, 2007.
- 弗里曼. Head First 设计模式 [M]. 中国电力出版社, 2007.
- [oodesign.com](http://www.oodesign.com/)
- [java-design-patterns.com](http://java-design-patterns.com/)
- [**Java设计模式：23种设计模式全面解析**](http://c.biancheng.net/design_pattern/)

## 7.作者简介

嘉凯、杨柳，来自美团金融服务平台/联名卡研发团队。

原文链接：https://mp.weixin.qq.com/s/H2toewJKEwq1mXme_iMWkA

# 【NO.366】即时通信IM核心能力及应用场景

本次分享的内容分为三块，一是腾讯云即时通信IM的产品概述，二是IM的核心功能特点，三是IM的应用场景介绍。

## **1.即时通信IM是什么**



即时通信IM是一款PaaS产品，以提供SDK的形式，集成至用户的APP或业务系统中，帮助用户快速实现类似QQ、微信那样的聊天能力。利用IM，用户可以实现APP内的单聊、群聊等稳定的消息传输能力；实现好友与黑名单等关系链管理能力；实现群成员与群资料等群组管理能力；实现聊天会话置顶、未读计数等会话管理能力。IM还开放了丰富的DEMO源码，最快1分钟即可跑通，再结合开源UI库（TUIKit），实现 UI 功能的同时调用 IM SDK 相应接口，仅需1天即可帮助用户搭建出自己的专属IM应用。

IM还为出海客户提供新加坡、韩国、德国、印度等海外数据中心，数据存储在当地，满足客户出海合规需求；IM覆盖了全球超过2500个加速节点，自研调度路由算法，“指挥”消息更快到达；支持业内独有的QUIC双通道能力，一条消息经过2路通道进行传输，双份保障，稳定不丢失，在网络质量差的部分国家和地区，消息依旧畅通无阻。IM出海详细指引请见出海专区文档（https://cloud.tencent.com/document/product/269/81906）

IM的底层还与实时音视频、直播等进行了打通，通过集成相应SDK可以轻松应对连麦PK、直播互动、音视频通话等热门音视频场景。除此之外，IM还可适配电商带货、在线课程、互动游戏、客服咨询、社交沟通、企业办公、医疗健康等众多场景，实现泛互、教育、金融、政企、IoT等全行业覆盖。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLgFk8wpEJlYkhIVynBW2d7qqCh7wvKq6BQs1hWlF4jhSch2cYPria6YQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



## 2.**即时通信IM核心能力**



### **2.1 消息传输&会话管理**

在消息传输中，IM支持多种消息类型，包括图片、文字、语音、短视频、表情、自定义消息等等，可以实现APP内的双人聊天，支持APP管理员在后台模拟其他用户身份发送消息或是下发系统消息。IM也支持类似QQ群、微信群的聊天方式，支持云端的消息存储，用户更换终端依然可以获取其聊天记录。在APP退出后台或进程被kill的情况下，如果有新的消息提醒，IM支持离线推送能力将这条消息推送给客户。IM还具有完备的会话管理能力，用户可以拉取最近会话列表，并可以对会话列表进行置顶、删除、清空等操作。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLquxbX0cws9ic2kXhd7UicwIibswUE2MatYnqLumQhjwJia6rdvql3cic6lQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



### **2.2 群组管理**

与大家平常使用的QQ群、微信群一样，IM可以提供丰富的群组资料管理能力，比如设置群公告、修改群组名称、修改群组简介等等，还支持修改用户群内身份、加群选项、接收群消息选项等等信息。IM还支持对群组及群成员进行管理，可以实现创建、转让、解散群组，可以进行添加/删除群成员、设置管理员、修改群成员资料等操作。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLm34Us0cmTxTPM5cJsdXZtRyoFVfefcHhQju2oG8ic2zrUYYicicnHrzDQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



### **2.3 用户资料及关系链**

IM支持昵称、头像、加好友验证方式等标准资料字段，也支持用户根据自身业务属性，将一些额外的自定义资料字段附加到用户资料上，并通过现有接口进行读写操作。在关系链方面，IM支持3000个好友，支持添加/删除/校验好友，支持添加/删除/拉取/校验黑名单。在非好友情况下，IM也可以支持用户之间相互聊天。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuL6rF3Bjqooj7VNmeLvM9IGEq2WCbud9tWDiaBRVgpFH1UA9bYxv49mzg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



最后，即时通信IM最核心的能力是后台系统的稳定性和抗并发能力。每月服务用户数超过10亿，消息收发成功率、服务可靠性高于99.99%。IM还提供人数无上限的音视频直播群，非常适用于音视频场景，并且支持多级扩散、冷热分离、多地容灾等技术。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuL04SuQibNQSZ5yWicZO7q6ePnb2wKCibfRib8Iicpb7xhInMt0Y9w1Wpl7dQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



## 3.**即时通信IM核心应用场景**



第一个场景是社交沟通。如果用户想要在APP中实现社交聊天，那么IM可以支持单聊/群聊中的文字、表情、图片、短语音、短视频等多种消息类型，有效提升用户活跃度。IM也支持丰富的群组类型，例如私有群、公开群、聊天室等，满足特定群聊场景。同时，IM还支持支持群头像、群昵称、群简介，群成员头像，群成员昵称等资料管理，并且支持最多20个自定义字段，实现群组等级展示、群组个性化展示等能力，轻松满足社交沟通场景下的用户需求。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLJl04ox0lp2jJtoKmEjcxOcMIU3xVWjlKib2cC8x4P5RiaxK8sqBB3gsw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



第二个场景是直播互动。IM为音视频场景提供无人数上限的音视频聊天室，能够为百万级的直播保驾护航。日常直播中的点赞、送礼、打赏等能力，都是通过IM自定义消息实现的。以点赞为例，用户在应用上进行点赞后，点赞操作的次数将通过客户端上报至服务器，之后由服务器下发一条消息至直播群，告知点赞数量达到了多少，客户端接收后更新点赞数显示，就实现了直播点赞功能。IM还提供弹幕聊天功能，实时获取弹幕信息，通过自定义消息还能实现弹幕变色、悬停、加速以及图片弹幕等特殊效果。另外，在直播互动中，抽奖也是不可或缺的能力。IM的自定义消息可以将链接、文本、图片组合成一条消息下发，用户点击后就可以进行抽奖，轻松实现直播场景下的抽奖互动。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLLsQFN21njeUaRp6OBLiaeo84vTH4kmBqNe9wYjL88kyEgLlNgPBVdSg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



第三个场景是电商带货。除了点赞送礼、弹幕消息、抽奖互动等能力外，IM的自定义消息和第三方回调能力还能帮助客户打通业务后台，实现领取优惠券，加购商品等能力。用户点击优惠券/加购后，系统自动实现流转。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLAxcjtfaZZgTmp1Q7ZMaQmb3x9iah6vbTeA6UFMYRGSKiaCsoxx4kt9xw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



第四个场景是互动游戏。很多游戏的游戏内社交是通过IM实现的，例如游戏内常见的组队聊天、世界聊天等等。IM还能够提供创建游戏内社群的能力，支持群头像、群昵称、群简介，群成员头像、群成员昵称等资料的编辑，还可以通过自定义字段为群成员分配游戏定制版本的身份、等级、勋章……另外，对于很多出海游戏，IM支持全球消息互通，提供包括亚太、北美、欧洲、中东、非洲、拉美等覆盖全球的海外接入点与加速点，实现玩家就近接入，保障全球玩家通讯质量，并且提供了国际站的独立数据存储节点，保证客户数据安全合规。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLa09XJjpatexy7TicNZCyicw5QtXSj8qpPU9VGqPRuq1ZPpgloX2m7g0Q/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



第五个场景是在线教育。IM提供了很多与教育场景适配的能力，比如开课提醒，就可以通过调用IM服务端的API，对所有成员进行消息提醒来实现。同时IM可以为在线教育提供强大的班级管理能力，老师或助教可以邀请成员进入课堂群，针对学员禁言/解禁或踢出课堂群。教师在上课过程中，可以利用IM的消息传输能力，向学生下发课件、PPT、文件、图片、视频等内容。举手发言/随堂测试/互动问答等在线课堂中常用的功能也都可以通过IM的自定义消息能力实现。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLI21QsrkAkHb5Ye4JjlBtItnshoHwNGrFUOuDJrYCxncSKUibokvBbNg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



第六个场景是在线客服。这个场景的典型应用就是智能客服，IM通过第三方回调将消息实时抄送至业务或智能客服后台，得到答案后通过Rest API下发，就能够实现用户咨询后自动获取对应的答复，自动化完成客服工作。当然，用户对答复不满意的话也可以要求转人工，人工客服利用IM也可以和客户实现文字/语音/图片等多种形式的实时在线沟通。对于在线客服场景中存在的很多监管需求，IM支持商家或超管随时加入或离开顾客咨询群，实时监督客服服务质量，也支持消息下载与实时抄送，将客服与客户聊天记录保存本地，供监管抽查、考核。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLttdoQbibC4nEOibMcDgnwzcjIcDGJSFPnic4uL7YNK0PG5pick5UMhZkSQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



第七个场景是企业通信。IM支持用户自定义字段，可自定义企业通信录信息及用户权限，允许非好友直接通信，满足超大型企业通信需要。通过IM的自定义消息能力，还可实现远程打卡、投票、企业云盘、在线文档、页面分享等多种定制化消息类型，并可将OA、e-HR等内部系统流程，通过系统消息方式发送给指定接收者，实现快速审批，全方位提升企业线上办公效率。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLNpwCiblgR9qP7UZ2QGDDiciboCkJKX3MnkAPwMCMMvZVosVkVgPWgHFkQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



第八个场景是在线会议。IM支持10万人大群，对于超大规模的万人级企业大会也能够满足需求。并且IM为会议提供强大的成员管理能力，支持禁言、踢人、设置联席主持人、邀请入会、禁止入会等多种功能。还可在会议过程中，通过IM的自定义消息能力将图片/文档/投票等会议相关内容分享至会议群内。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLu1ulZq6BTLJURpX9jMQyP7JFKrIAnEt8RUx2j9xriaZ0iaWMaLpWcCBw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



最后一个场景是商业沟通。在日常的打车、配送等服务中，都会涉及到服务双方的简单沟通。IM能够在服务人员接单/抢单后，通过REST API快速创建服务人员与用户的单聊/群聊，实现定向分配对接，同时自动下发订单信息，为双方提供互动沟通的平台。IM还支持发送实时位置信息，通过用户自定义字段，可实时获取并发送服务人员轨迹信息。用户可在应用中实时确认服务人员的位置轨迹，了解服务进度。另外，IM有很强的安全保护能力，能够满足商业沟通场景对信息保护和用户安全的要求。IM能够选择性拉取用户资料，仅展示必要信息，避免用户信息泄露，并通过第三方回调与实名认证服务打通，提高App安全性，还可在服务人员与客户建立群聊后，同步添加安全员，对双方使用者不可见，保护使用者安全。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAGzTolhJia9mclIf2GvByzuLpdS5MiaJvObfIicZCicbSqCV7dcORqrrlAsdNicOU7YZrmsriaav1IW9jGw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



IM还支持私有化部署。对于政务/金融/医疗等数据安全更高的领域，IM 可直接部署在客户本地，数据资产也全部放置在客户本地，保证系统及数据安全。对于想要将聊天能力嵌入到自有系统中的企业，企业微信、钉钉飞书这样的标准产品无法满足他们的需要，也可以通过IM的私有化部署，将IM SDK集成至自有系统，搭建自己的专属企业通信平台。

原文作者：腾讯云音视频产品经理——郑聪兴

原文链接：https://mp.weixin.qq.com/s/fmRkCQHtl-AyBVI0D_EkRg

# 【NO.367】新知 | 腾讯云视立方播放器技术实现与应用

本次分享的主要内容分为三块，一是腾讯云视立方播放器的相关技术背景，二是业务侧经典场景应用方案，三是短视频场景应用的技术实现方案。

## 1.**腾讯云视立方播放器技术背景**



腾讯云视立方播放器基于腾讯视频同款内核打造，完美融合了腾讯视频的能力，视频兼容性、适配能力以及播放稳定性均大幅提升，解决了系统引擎各种播放异常问题。

- **功能全面：**覆盖长短视频点播、直播场景，具备业界领先的自适应技术、画质提升及版权保护等解决方案，满足各项业务诉求。
- **性能可靠：**经过亿级用户验证，性能稳定可靠。具备业界领先的启播解决方案，对启播速度做了深度优化，启播平均速度低至100毫秒。
- **兼容适配：**主流视频格式协议100%覆盖，对于大量在系统播放器中播放异常的非常规编码的视频也能可靠稳定的播放。
- **多平台支持：**支持安卓、iOS、 Web以及Flutter等多种平台。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAEMo6iaZV1XaBiaCaD2qv39JLCrYygnVqxYtCkd043NKHtrzA1ziaY3fDedUtMqvMYwXibibx2aZ0nX3gQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

在追求卓越内核的同时，腾讯云视立方播放器也非常关注业务的接受成本。为了降低业务侧的开发难度及工作量，所有主流场景均有完整组件&解决方案Demo提供。这些Demo全部开源，本身完整可直接使用且支持自定义修改。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAEMo6iaZV1XaBiaCaD2qv39JLOPmoOGSJPXL6VD63nltPdgbDGNppyOicUbZ5ewH9uWr9msy6vpTiaAaQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



## 2.**经典场景应用方案**



播放器UI组件封装了完备的基础操作功能，可以实现小窗播放、切换全屏播放、滑动控制亮度音量、滑动控制进度等业务侧常见的应用操作。同时播放器UI组件也支持弹幕、动态水印、会员试看、剧集播放等进阶功能。弹幕能够在视频播放的同时，于视频上方滑动显示其他用户的评论等信息。动态水印可以实现用户ID在视频上滚动显示，达到防盗录、盗播的效果，提升视频安全性。会员试看能力可以设定非会员可试看时长，超过时长后会弹出会员提示、购买窗等信息。各种业务侧经典的应用场景，都可以通过播放器UI组件实现。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAEMo6iaZV1XaBiaCaD2qv39JL2MSu41ALrn9j7DYp71KVniac4aBauNSdFQAETx17HHOIRr9bj2iabDoA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



播放器UI组件的接入也非常简单，仅需极少量的代码即可完成。首先集成播放器UI组件，之后在UI界面加入SuperPlayerView组件。封装SuperPlayerMdel，将播放URL、封面地址等填入。再调用SuperPlayerView的PlayWithModel就可以启动播放。对于进阶场景，比如弹幕、会员试看、动态水印等，开发者也只需在SuperPlayerModel中配置相关信息，例如可观看时长、动态水印文本、大小等等，就可以轻松地实现。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAEMo6iaZV1XaBiaCaD2qv39JLYibg2KRyPmd7mFicSKPGzhgs1F5aD5sqO1Df3muEkUCP3PY3qQfSpXSA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



## 3.**短视频场景应用优化**



主流的短视频场景主要分两种，一种是沉浸式播放场景，类似微视、抖音，一次只能看到一个视频，通过上下滑动切换其他的视频。另一种是Feed流场景，一屏页面可同时出现多个视频，第一个完整出现的视频将自动播放。短视频应用场景的界面丰富，所以内存性能是一个很重要的指标，同时为了更加顺滑的观看体验，启播速度也非常重要。

短视频是一个快速消耗内容的场景，很多视频用户可能看都不看就会直接划过。为了降低业务的流量成本消耗，短视频场景需要一些流控策略。常规的流控实现思路是利用列表组件，在播放第一个视频时，对下一个视频进行预播放，以达到滑动至下一个视频时能够马上播放。但如果一个界面可以看到很多视频，这种策略就可能会对多个播放器进行预播放，这会导致内存消耗巨大，且反复销毁创建播放器也会带来比较多的内存碎片。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAEMo6iaZV1XaBiaCaD2qv39JLGcOy0OppLnR1sQ6myLsuOibqEbOpFDCex7aZSetdjDzOg4LryEKUETg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



那应该怎样优化呢？腾讯云采用的优化思路是使用不超过两个播放器实例，并通过服务去管理播放器的复用与使用。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAEMo6iaZV1XaBiaCaD2qv39JLgS5vkUQerxKUyzdme74JYiaTNTfNWDKg3y77DQQIxD3G9R4siagMicQog/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



如上图所示，在上层的应用逻辑层，也就是UI和业务层，业务侧可以根据自己的业务特性进行设计，这里可以采用MVC或者MVVM的设计模式。那如何使用两个播放器实例进行复用呢？在应用逻辑层下创建一个服务层，并创建一个类似线程池的管理PlayerPoolManager。服务层还对播放器做了一层封装，命名为TxPlayerWrapper。这层封装，可以便于业务侧统一控制类似playerconfig以及一些业务特性的行为。PlayerPoolManager提供两个接口，其中一个接口是UpdatePoolPlayers，主要运营播放器对复用的管理。具体流程可参考右侧的表格，假设用户停留在第1个视频，之后是序列号2和3的视频。此时PlayerPoolManager可以将1和2的URL放进PoolPlayer去创建播放器，并且设置autoplay为false进行预播放。因为当前要播放的是第1个视频，所以业务侧可以通过PlayerPoolManager提供的另一个接口getPlayer，以URL作为Key去取得要播放的player，然后调用resume进行播放。当用户向下滑动来到视频2时，PoolPlayer中视频1和视频2的URL的播放器，需要更新为视频2和视频3的URL的播放器。因为视频2的URL的播放器仍在PoolPlayer中，所以仅需将原视频1的player复用给视频3去进行预播放，而视频2的player则进行resumeplay。如果用户跳过视频3，快速滑至视频4，此时PoolPlayer中视频2和视频3就需要变成视频4和视频5。这就会产生一个问题，PoolPlayer中的视频2和视频3被清空后，视频4和视频5重新加入，但它们都没有经过预播放，也就导致启播速度受到很大影响。所以针对启播速度还要进一步做优化。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAEMo6iaZV1XaBiaCaD2qv39JLHjqa4l7Bb4jicTqLv8zI0jpC4uv5zmCKL85Y5lsyXia5QlVico8ia8CY8w/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



上图是一个简化的启播流程，从获取数据开始，之后UI展示，封面展示，获取URL/fileID以及配置视频参数，传递到播放器调起播放。通过向服务器请求读取视频文件，把读下的文件进行解封装、解码，到达一定的buffer后，就会启播并回调一个首帧事件。最后业务侧收到首帧事件回调后，进行封面隐藏，整个流程结束。对启播流程的优化，重点在那些引起耗时操作的地方，而耗时操作首先要关注IO的操作。流程中第一个引起耗时操作的地方，便是获取数据。这是一个网络IO相关的操作。这里需要提前进行数据获取，或做好缓存相关管理来减少耗时。流程中第二个引起耗时操作的地方在获取视频链接。在这个步骤中如果使用fileID播放的话，由于fileID仅是一个ID样式并非URL，所以会额外引入换链的过程。换链是一个比较耗时的网络请求过程，所以需要提前换好链来减少耗时。如果配置了防盗链，那么URL还会有一个有效期，同样需要做好管理。如果是多码率视频还需要事先配置好要指定播放的码流，不要等到播放器启动时再做切流，这会引起比较大的耗时。

除了IO操作的耗时外，另外两个优化启播时间的点分别是网络和解封装、解码。对于网络，重点在于视频的CDN部署情况。如果是刚上传的视频，那新增节点是否预热，是否可以保证良好的访问情况都很重要。对端侧来说，可以通过预下载或预播放的方式，把要播放的视频提前下载一部分。对于解封装、解码的耗时，也可以通过预播放机制去解决。

预下载和预播放这两者有什么区别呢？预播放机制就是创建一个播放器实例，并且启动下载和解码环节，首帧解析出来之后再暂停播放器。使用方法就是在startPlay之前设置setAutoPlay为false，要播放的时候调用resume接口来启动。而播放器的预下载机制不需要创建播放器实例，只预先下载视频的部分内容，不启动解码和解封装环节。使用方法就是调用TxVodPreloadManager接口进行预下载，启动播放时跟正常启动播放器一致。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAEMo6iaZV1XaBiaCaD2qv39JLLgglZ6AS1WmjNoI7M0wqrjwSuibtib0iaT3wCFQoFO5ibKibBImjuw7oLpw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



两种方式都有各自的优缺点。预播放需要启动播放器实例以及解码器的相关操作，会带来一定的性能消耗，所以会占用内存和CPU消耗。但是预下载不需要启动播放器，不需要解码环节，性能消耗相比预播放会低很多。但预下载的启播速度比预播放慢，会有100毫秒左右的差距。因此在实际场景中需要根据业务特性，采用其中一种或者两种都采用的结合方式。

针对M3U8多码流视频，腾讯云也做了针对性的优化。因为在播放器未播放之前，无法知道多码率M3U8中有几个码流，所以开发者可以在启播前指定优先播放的视频分辨率。播放器会查找小于或等于该偏好分辨率的流进行启播，启播后就不必再通过set bitrate index进行切换。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAEMo6iaZV1XaBiaCaD2qv39JLUWmkysia0ap8vsOwJ9XdV0A981nu4tdD7ic3Cwq0Mm3AfN6XsP0hS2Pw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

最后，通过流控策略还可以进一步精细化降低流量成本。腾讯云视立方播放器提供了三个阶段的流控功能。第一个是预播放流控，设置启播前阶段的最大缓存大小。第二个是预下载流控，预下载阶段流控与预播放类似，也可以控制预下载的大小。这些都可以通过相关接口进行参数设置。第三个是播放阶段的流控，即最大播放缓存的大小，默认是30秒，也可以进行自定义。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAEMo6iaZV1XaBiaCaD2qv39JLdNxxSDaIKDaZBSQ55ovw4cvpA4JXprBM41ibzypJBR0HF3J23hWpMiaQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

原文作者：腾讯云音视频技术导师——李正通

原文链接：https://mp.weixin.qq.com/s/7JCpeuz6CX1hieBSSUuJ0Q

# 【NO.368】AXP-QUIC：自适应X路QUIC网络传输加速

**导语:**  腾讯云即时通信IM实现了一种网络自适应的X路QUIC传输加速技术AXP-QUIC（Adaptive X-PATH QUIC），已应用于IM SDK客户端到服务端的数据传输。该技术建立了一套客户端弱网自评估模型，根据网络链路的RTT，丢包率，吞吐量，并结合主动探测，判断终端当前是否处于弱网络环境。同时将QUIC协议和多通道传输技术相结合，根据终端所处的网络环境，实时自动决定切换网络链路或使用多链路进行传输。**通过AXP-QUIC技术，即时通信IM能够在70%丢包的弱网络环境下，保证消息100%可靠传输，且不大幅度增加消息延时。**



随着互联网技术的发展，依托互联网通信的应用对网络传输质量也提出了更高的要求。特别是在即时通信、实时音视频、实时信令交互等对实时性或可靠性有高要求的场景，网络传输质量是影响产品体验的决定性因素之一。

通常，应用服务通过广泛的节点部署或者接入加速网络，结合最优调度，使服务接入点尽可能靠近终端用户，以降低终端到接入点的网络延时（第一公里）。数据到达接入点后，通过中继转发、最短路径、加速协议、多路传输等加速技术实现骨干网的传输加速（第二公里）。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAE0mCoxavmMcib5Xat0amiaaZWQT9RJt8CRcqWFEtZ4OMHXzO6rFEwKLtRRJHeYFYrV9QEtTGDthoyQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

**移动弱网络**

在移动网络环境下，受终端物理位置变化、信号强度、AP接入等因素影响，用户网络质量具有较大的不可靠性。对于传输层协议，UDP是不可靠传输，数据的准确性需要应用层来保证。而TCP在高丢包网络环境传输效率表现差，1%的丢包情况下吞吐率降低90%（RTT 250ms）。虽然通过使用BBR等基于带宽预测的拥塞控制算法，可以提升传输性能，但在丢包率超过15%的弱网环境下，传输能力也大幅下降。

![图片](https://mmbiz.qpic.cn/mmbiz_png/APDZeM2BxAEmibsThfmfroQxGgsKbmm9fLRr2ImrIajvwOaibRS2mCIVGnmo5eJjQpso7Q0hdYss7eUFvibcCy2rw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



因此，研究如何实现在不可靠的移动弱网络环境下提供稳定可靠的数据传输，是提升移动互联网应用竞争力的关键问题。

**方案探索**

**1. 传输协议和策略**

本文探索如何实现在不可靠的终端网络环境下提供稳定可靠的数据传输。我们知道，TCP是可靠传输协议，但是存在重传歧义等问题，抗丢包能力较差。而UDP是不可靠的传输协议，但传输快，开销小。针对TCP和UDP的优缺点，为了实现快速、可靠、稳定的传输目标，一般的思路是基于UDP构建可靠的应用层传输协议，比如KCP和QUIC，再配合多发、FEC等策略，实现弱网环境下更好的传输表现。另外，随着移动操作系统的优化升级，能够支持应用同时使用cellular网络和wifi网络进行数据传输，以提升传输效率。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/APDZeM2BxAEmibsThfmfroQxGgsKbmm9f0xvVQ8jEBYdCCf2ZwbOpUZU6ib03AZiamNfiaBqHxw7EmFWF5sm343lyQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)



**2. QUIC简介**

QUIC (Quick UDP Internet Connection) 是Google在UDP上实现的一种多路复用安全传输层，提供了多路复用和流控机制，安全等效于TLS，可靠性和拥塞控制类似于TCP。QUIC完全在用户空间中运行，可以理解为利用UDP封装实现的安全传输层。所以相比对TCP/UDP这些操作系统协议栈优化，QUIC迭代起来更方便。

![图片](https://mmbiz.qpic.cn/mmbiz_png/APDZeM2BxAEmibsThfmfroQxGgsKbmm9fG3YE2NlfHu8WWGVZ4Wibnrne8RmU5VJPX9P7koJ8MQiac22EF5pgpe6A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



QUIC作为基于UDP实现的一种通用可靠的“传输层”，具有如下关键特性：

- **建联延时**。对于TCP+TLS，建联完成需要经过TCP 3次握手+TLS握手，共3个 RTT。QUIC的Client和Server建联后，QUIC Client和Server在QUIC层缓存维护和socket五元组无关的逻辑连接session。如果需要传输数据，直接使用该session即可。这个session包含加解密的上下文信息，并且在QUIC连接数据传输过程中更新。因此，后续的数据传输是0 RTT。
- **灵活的拥塞控制****。**QUIC集成了TCP这么多年来积累的优秀思想，如FACK、TLP、 F-RTO、Early Retransmit、Pacing等，提供丰富可扩展的拥塞控制策略。另外，QUIC可以提供更丰富的信息供这些策略使用，比如每一个包携带新的seq，包括原始包和重传包。这样发送者能够区分ACK是属于重传包还是属于原始包，以避免TCP重传二义性，以及丢失重传包造成的RTO问题。QUIC ACK包同时携带了从收到包到回复ACK的延时，这样结合递增的包序号，能够精确的计算RTT。
- **无队头阻塞的多路复用****。**类似于基于TCP的HTTP/2多路复用协议有队头阻塞的问题。TCP丢包可能会影响该TCP连接上的所有流。QUIC多路复用的stream流有独立的收发窗口，某个stream的丢包，不会影响其他stream的数据传输。
- **TLS传输层安全****。**保证数据传输的安全。
- **连接迁移****。**TCP是基于4元组，在链路变化时，需要重新建立连接才能传输数据。而QUIC使用64位的Connection ID来维护客户端和服务端的逻辑连接，因此即使UDP链路发生变化，QUIC层的逻辑连接维持不变，两端收到的QUIC包能够被正常解析。

**AXP - QUIC**

综合在不可靠网络环境下实现稳定可靠传输的一般策略方法，并了解到QUIC传输协议的优势，我们将使用QUIC作为客户端和服务器之间的传输层协议。

但是在实践的过程中，我们发现，有时候终端处在弱网环境，wifi信号并未彻底断开，但数据传输实际上已经有损。并且可能此时终端cellular网络是可用的，但因系统没有切换网络，导致应用层无法及时更换传输链路。为解决这种场景下的问题，我们建立了一套客户端弱网自评估模型，根据网络链路的RTT，丢包率，吞吐率，并结合主动探测，判断终端的网络链路是否处于弱网络环境，在数据通道有损的情况下，尽可能早的主动进行连接迁移，减少对上层业务的影响。

另外，结合多网卡可多路径传输的优势，综合多链路的RTT，丢包，吞吐量等指标，AXP-QUIC客户端自动决定使用单条通道还是多条通道同时进行数据传输，以达到传输质量、速率和成本的最优。

**1. AXP-QUIC架构**

![图片](https://mmbiz.qpic.cn/mmbiz_png/APDZeM2BxAEmibsThfmfroQxGgsKbmm9fu7Af4pAm5IHMK7iaRyia9EhWiaNbjQzxyvOzYLPMIuhiaCnX8yjElKQbGg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



**2. 链路质量衡量**

移动网络质量能够通过RTT，丢包率，信号强度，带宽延时乘积等指标体现。这些指标可以通过被动网络采集或主动探测的方式获取。通过将这些指标量化转换成一个衡量链路质量的数值，这样链路之间可以进行网络质量对比。

```
PathQuality = f ( RTT, Lost, Signal, Throughput... )
```



**3. 链路选择**

我们定义如下几种链路选择的类型：

```
enum XPathType {XPathTypeNone,     // 不使用多链路XPathTypeHandover,  // 只有当主链路不可用，才会使用第二条链路XPathTypeInteractive, // 当主链路不够用，比如丢包或延时高，就会启用第二条链路XPathTypeAggregate  // 为了更大的带宽，多条链路可以一起使用}
```



在多条路径链路场景下，需要考虑slow path blocking问题。因此，在选择链路时，需要考虑各链路的传输能力差距。

```
func ChoosePath (Path p1, PathQuality q1, Path p2, PathQuality q2, XPathType xtype){if (q1 - q2 > MAX_DIFF_QUALITY)return p1, xtype;if (q2 - q1 > MAX_DIFF_QUALITY)return p2, xtype;return p1, p2, xtype}
```

**AXP - QUIC**

目前AXP-QUIC已经应用于腾讯云即时通信IM服务。按照40条/秒的消息发送速率，并测试不同上行丢包率下的表现。实测表明，该方案在对抗网络丢包方面，相比于TCP有显著的效果。

![图片](https://mmbiz.qpic.cn/mmbiz_png/APDZeM2BxAEmibsThfmfroQxGgsKbmm9fDMGPv8T7wEfteYgtKEDVh176o5SCtPqnIB3ia0cRicSoyaaUyiaQn6S8w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/APDZeM2BxAEmibsThfmfroQxGgsKbmm9f60BqxiaFuVwU67NWmbVGt82eKhEe6A0rCicY4mDej2ruYf62Hqd2YqsQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



此外，对于wifi和cellular网络只有一条链路网络质量差的场景下，AXP-QUIC 能够实时感知终端各链路网络状况，并及时切换链路，获得预期最优传输效率。

![图片](https://mmbiz.qpic.cn/mmbiz_png/APDZeM2BxAEmibsThfmfroQxGgsKbmm9fQdfNudAIvsrZt2Lg29bJIQYCWrYWfUndhL5ejVZ62WVurCDxDbKibbA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



![图片](https://mmbiz.qpic.cn/mmbiz_png/APDZeM2BxAEmibsThfmfroQxGgsKbmm9feAdR1zicnB6PDiaibDSwb1vHopbAhKJZ9ZvtAsF2He0my0G0bp5rNDD5Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

原文作者： 腾讯云音视频

原文链接：https://mp.weixin.qq.com/s/DFxC-c9L2DFc0Cv-cne7fA

# 【NO.369】SRS5优化：如何将DVR性能提升一倍

## 1.Summary

SRS支持将直播录制为VoD文件，在压测时，如果流路数很多，会出现CPU消耗很多的问题。

原因是写入较小视频包时，SRS使用了write，由于没有缓冲能力，导致频繁的系统调用和磁盘繁忙。

优化方案，可以选择fwrite(v5.0.133+)，或者老版本用内存盘方案，可将DVR性能提升一倍以上。

## 2.Environments

SRS服务器配置如下：

- • CPU：INTEL Xeon 4110 双路16和32线程
- • 内存：32G
- • 网卡：10Gb
- • 磁盘：两块980G的SSD盘做成RAID0（可用空间共1.8T）
- • 操作系统：CentOS 7.6。
- • 流码率：3Mbps

这里需要说明一下，采用SSD盘主要是为了确保磁盘性能足够，以确保能够支撑大的并发压力，从而在大并发压测的情况下观察系统性能情况，如果本身磁盘I/O性能比较低下，大量的I/O等待可能导致观察不到CPU瓶颈的现象。

另外，在我的测试环境中，SRS经过了多进程改造，能够支持推流进来后自动将不同的流均衡到不同的SRS进程上面，从而能够充分利用服务器多核的能力，但是由此得出的结论同样适合于单进程SRS。

SRS开启DVR录存功能，使用如下命令启动SRS：

```
env SRS_LISTEN=1935 SRS_MAX_CONNECTIONS=3000 SRS_DAEMON=off SRS_SRS_LOG_TANK=console \
    SRS_HTTP_API_ENABLED=on SRS_VHOST_DVR_ENABLED=on ./objs/srs -e
```

压测工具，用srs_bench套件中的sb_rtmp_publish模拟推流客户端进行大并发量推流模拟，一台机器压测能力不够可以开启多台机器进行压测。

```
./objs/sb_rtmp_publish  -i doc/source.200kbps.768x320.flv \
    -c 100 -r rtmp://127.0.0.1:1935/live/livestream_{i}
```

启动srs后，用压测工具进行压测，观察测试过程中的CPU、网络IO、磁盘IO相关数据，并进行对比。

## 3.write SSD Disk

SRS优化前，默认的方式就是使用write方法，直接写入磁盘。测试能支持1000路写入，CPU跑满。

![图片](https://mmbiz.qpic.cn/mmbiz_png/GTjOhG7LXjQACA0hu4ptGJ22LLMD5Yk1GptymBBCUNg1pRibycxhw5ofRZ9ZUWHDuHt7VuDeiaT3QGC62jW4jrJA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

从上图可以看到，1000路3M的DVR录制已经将系统的CPU都跑满了，特别需要关注的是cpu的时间主要消耗在了内核空间上面，占了87.5%。

用nload查看当时的输入带宽情况，发现系统输入带宽平均只有2.17Gb，没有达到预期的3Gb的带宽，应该是CPU负载过高导致SRS来不及处理网络I/O引起的性能下降。

再用perf工具对其中一个srs 进程进行性能采样分析，得到下面的火焰图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/GTjOhG7LXjQACA0hu4ptGJ22LLMD5Yk1ZCvicUFkZXghej6icvCPX7r9a5lA0RWWkeRDGeMBGrM04Aicibia537v2vg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

可以发现，sys_write操作占用的时间消耗是最多的，对比上面用top看到的内核态消耗的时长占比可以得出的结论是一致的。

最后看磁盘I/O情况：

![图片](https://mmbiz.qpic.cn/mmbiz_png/GTjOhG7LXjQACA0hu4ptGJ22LLMD5Yk16Rn9LERKx50usxdeC97aoxwnnO9EKXicuGtbBiazKqWC6v9fy0RBpJSA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)



从上图看磁盘的利用率没有到100%，虽然有一定的波动，但是总体上还是在合理的可以接受的性能范围内。

## 4.fwrite SSD Disk

SRS优化后，使用fwrite写入磁盘。录制1000路流，占用32%的CPU，性能提升一倍以上。

![图片](https://mmbiz.qpic.cn/mmbiz_png/GTjOhG7LXjQACA0hu4ptGJ22LLMD5Yk1ibH7V1B7ficvZShZWRp1o4z2OuAtQwpPmegPERXjrx5qdZASZezc2JRw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

从上图可以看到，1000路3M的DVR录制已经将系统的CPU整体来说还有很多空闲（这里说明一下，部分进程的SRS占比高的原因是因为当时任务分配的不够均衡引起的）。特别值得注意的是本次测试内核时间占比大幅下降，只有9.1%。

再用nload看网络i/o情况，网络i/o相当平稳，和预期的3Gb完全吻合。

再看磁盘i/o的情况，磁盘的利用率没有到100%，虽然有一定的波动，但是总体上还是在合理的可以接受的性能范围内。

最后看火焰图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/GTjOhG7LXjQACA0hu4ptGJ22LLMD5Yk1qr21QCD4bibXpS7uJHL9Jv3brziauEpajJe92kYEhNKj6MluOKnwuicrw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

系统调用的时间占比大幅度缩短了，在上图几乎找不到sys_write的位置了。

## 5.write Memory Disk

SRS优化前，也可以挂载内存盘，使用write写入内存盘。

需要说明一下，由于我手上的服务器只有32G内存，只能分配16G内存给内存盘使用， 由于内存盘比较小，按照3Gb的写入速度，最多能写42s的DVR。

采用如下命令挂载内存盘：

```
mount -t tmpfs -o size=16G,mode=0755 tmpfs /data/memdisk
```

并且修改srs的配置文件将文件写入到内存盘：

```
env SRS_LISTEN=1935 SRS_MAX_CONNECTIONS=3000 SRS_DAEMON=off SRS_SRS_LOG_TANK=console \
    SRS_VHOST_DVR_DVR_PATH=/data/memdisk/[app]/[stream].[timestamp].flv \
    SRS_HTTP_API_ENABLED=on SRS_VHOST_DVR_ENABLED=on ./objs/srs -e
```



测试数据如下，占用CPU27%左右：



![图片](https://mmbiz.qpic.cn/mmbiz_png/GTjOhG7LXjQACA0hu4ptGJ22LLMD5Yk1N03C3Qkfkydq0ecYSDF4c8uFib20XpLdGprWxC5n6gHYhTQzicHic7o5g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

从CPU的情况看，采用内存盘也比较理想，load average只有 7.5，性能也不错。如果不需要录制大量的流，这种方式也是非常好的。

## 6.macOS Test Data

在macOS环境下，也做了一组数据，供参考：

1. \1. macOS: MacBook Pro, 16-inch, 2019, 12CPU(2.6 GHz 6-Core Intel Core i7), 16GB memory(16 GB 2667 MHz DDR4).
2. \2. v5.0.132优化前: RTMP to HLS, **200 streams**, SRS **CPU 87%, 740MB**
3. \3. v5.0.133优化后: RTMP to HLS, **200 streams**, SRS **CPU 56%, 618MB**
4. \4. v5.0.132优化前: DVR RTMP to FLV, **500 streams**, SRS **CPU 83%, 759MB**
5. \5. v5.0.133优化后: DVR RTMP to FLV, **500 streams**, SRS **CPU 35%, 912MB**
6. \6. v5.0.133优化后: DVR RTMP to FLV, **1200 streams**, SRS **CPU 79%, 1590MB**

## 7.Conclusion

从以上4个测试可以得出以下结论：

1. \1. 无论ssd盘还是内存盘，采用fwrite的性能比采用write的性能有明显的提升，其主要得益于fwrite内置的缓存功能减少了系统调用的数量，带来内核时间消耗的减少，从而提升了性能。
2. \2. 在ssd盘情况下，fwrite的缓冲能力可以大幅度降低对于CPU的消耗，但是在采用内存盘的情况下，CPU的消耗虽然也能够降低，但是不是那么明显。
3. \3. 录制到内存盘性能也很好，如果流路数不多也可以考虑这种方案。

> Note: 之前想当然地认为用write写内存盘，因为系统调用引起的用户态到核心态的切换还是会导致cpu大量消耗，一样会导致CPU消耗高居不下，但是事实看到是采用内存盘以后cpu消耗明显下降了，是不是可以认为系统调用引起的用户态到核心态的切换消耗实际上并没有想象的那么大，而是内核态在处理小块的文件write写入磁盘的时候还存在着其他因素引起消耗大量的cpu。譬如，因为最终写入磁盘都是按照扇区写入的，而小块写入需要操作系统将这个小块对齐并填充到一个完整的磁盘扇区，从而引起性能大幅下降，而内存盘是不是就不会存在这个问题？由于我自己没有内核方面的经验，所以只能存疑了，也请懂内核的朋友给予指点。

## 8.What's Next

在linux环境中，对于文件进行读写操作的时候，我们可以采用libc提供的fread/fwrite系列的一套函数，也可以采用操作系统提供的read/write系列的一套系统api函数。

对于libc提供的文件读写函数，首先它可移植性比较好，因为libc为我们屏蔽了操作系统的底层差异，在linux、windows等不同的操作系统环境下面都有标准的接口实现，因此不需要我们为不同的操作系统进行适配。其次，libc提供了带缓冲功能的读写能力，而操作系统底层文件读写API却不提供这种能力，缓冲能力在大多数情况下能够为我们带来文件i/o性能的提升。

当然libc的文件读写api函数也存在不足之处，缺少了writev/readv之类的函数。不过readv/writev的功能无非就是将多个缓冲区的内容合并成一次批量读写操作，而不需要进行多次API调用，从而减少实际物理I/O的次数，我想libc没有提供这类函数主要也是因为其缓冲功能已经能够将本来需要多次的小块物理I/O操作合并成了一次更大块的物理i/o操作，所以就没有必要再提供readv/writev了。

不管SRS也好，还是NGINX也好，虽然前者采用st-thread框架的协程能力来实现网络异步i/o，但是和后者一样，最终还是采用epoll事件循环来实现网络异步i/o的，但是对于文件i/o，目前存在的问题是，无论是write还是fwrite都是同步操作，在磁盘请求比较繁忙的情况下，必然会导致进程或者线程阻塞，从而引起系统并发性能的下降。

由于操作系统本身不支持epoll异步（linux下的ext4本身没有实现poll的回调），所以寄希望于epoll来实现文件i/o的异步操作是行不通的。NGINX对于文件异步i/o采用了aio+多线程的方式来实现的，个人感觉是由于和epoll模型来说是一套独立的框架，还是相对比较复杂。

不过，好在linux在5.1内核以后提供了io_uring的异步i/o框架，它可以统一网络i/o和磁盘i/o的异步模型，并支持buffer IO，值得我们去关注学习一下，也值得我们后面一起去探讨一下未来如何在srs上采用io_uring来实现带有fwrite一样的缓冲能力的磁盘i/o的操作，来彻底解决磁盘i/o引起的性能瓶颈的问题。

原文作者：Written by 王磊(bluestn).

原文链接：https://mp.weixin.qq.com/s/ALWofs-Z4NDxo16QzuBj7Q

# 【NO.370】SRS配置升级，云原生友好的配置能力

什么才是更好的配置方法？NGINX的conf，还是MySQL的ini，还是新潮的yaml，或者JS友好的json？它们都有各自的问题，最好的方式是conf+环境变量，也就是Grafana的配置方法。

## 1.Why Important?

为何配置这么重要？因为它是最基本的API，也就是程序和人的接口，也决定了使用体验是否良好。想象下xml的配置文件，想起来都觉得烦躁，这是因为xml并不是对人友好的接口。

SRS的配置一直都对人挺友好的，因为是使用的NGINX的配置方法，熟悉NGINX的同学可能会觉得很熟悉，比如：

```
listen 1935;
max_connections 1000;
vhost __defaultVhost__ {
}
```

相当直观和好理解，这些年也就这么过来了，但它并非没有问题，比如：

- • 程序读写不友好，要用代码修改配置，就需要自己实现这个解析。NGINX是有生态支撑，有工具支持读写，但它只支持NGINX的配置，并不是所有conf格式都能支持。
- • 在文档或Wiki中，或者在给出例子时，总是要给出一个配置文件，而一般还需要修改现有的配置文件，很不方便，也有可能会出错。
- • 在K8s部署时，或者Docker启动时，需要创建文件，并映射到Docker中，哪怕只需要修改某个配置项，也需要这么做，这套机制很麻烦。

也正是因为有这些问题，陆陆续续的有同学提出来支持其他的配置方式，详细可以参考#2277[1]中的详细讨论，大概有几种解决方案：

- • 支持JSON或YAML，这是为了解决程序读写的问题，JSON对JS程序员友好，YAML对DevOps友好。无论哪种方式，都不能对所有人友好，而且还解决不了后面两个问题，还是依赖文件。
- • 支持Redis那种命令行参数的方式，这解决了配置文件的问题，对人也相对比较友好，但若有参数变更，则还是需要依赖文件。
- • 环境变量，方便设置和命令行启动，是基本的传递配置的办法，但多了后不太友好，另外和目前的文件配置方法有差异，导致Reload等机制需要修改。

> Note: 如果直接换成新的配置方式，都会对目前支持的NGINX的conf文件的方式造成不兼容，影响使用习惯。因此最好的办法不是替代，而是结合现有配置方法，实现配置能力的增强。

直到看到了Grafana的配置方式，发现了在目前基础上，可以更好的配置方法。

## 2.Solution

目前NGINX的conf的配置方法，是对人比较友好的，需要支持的是对程序接口友好的方式，而且是和这种方式是可以配合而不是替代的方案。这就是环境变量的方式，先看Grafana的启动方式：

```
docker run --rm -it --name grafana \
    --env GF_SECURITY_ADMIN_USER=admin \
    --env GF_SECURITY_ADMIN_PASSWORD=admin \
    --env GF_DATABASE_TYPE=mysql \
    --env GF_DATABASE_HOST=host.docker.internal:3306 \
    --env GF_DATABASE_NAME=grafana \
    --env GF_DATABASE_USER=root \
    --env GF_DATABASE_PASSWORD=****** \
    --env GF_INSTALL_PLUGINS=tencentcloud-monitor-app \
    -p 3000:3000 \
    grafana/grafana
```

这种方式是最方便用Docker启动的方式，在K8s中也是一样的。Grafana所有的配置既可以通过配置文件配置，也可以通过环境变量配置。

借鉴这种办法，SRS的配置也可以支持环境变量，比如WebRTC over TCP[2]：

```
docker run --rm -it -p 8080:8080 -p 1985:1985 -p 8000:8000 \
  -e CANDIDATE="192.168.3.82" \
  -e SRS_RTC_SERVER_TCP_ENABLED=on \
  -e SRS_RTC_SERVER_PROTOCOL=tcp \
  registry.cn-hangzhou.aliyuncs.com/ossrs/srs:v5.0.60
```

无论是命令行启动，还是Docker启动，还是分享命令给其他人，还是K8s启动，这都是最简单直接的办法。

> Note: 一般我们并不会需要修改特别多的配置，往往只需要修改几个配置，因此这种方式是最便捷的。

当然也不是所有SRS的配置都支持环境变量，因为有些配置比如Transcode是数组，就很难支持，具体以full.conf[3]中标记为`Overwrite by env SRS_XXX`为准，比如：

```
# Overwrite by env SRS_LISTEN
listen 1935;
# Overwrite by env SRS_MAX_CONNECTIONS
max_connections 1000;

rtc_server {
    # Overwrite by env SRS_RTC_SERVER_ENABLED
    enabled on;
}

vhost __defaultVhost__ {
    rtc {
        # Overwrite by env SRS_VHOST_RTC_ENABLED for all vhosts.
        enabled on;
    }
}
```

基本上SRS的配置都支持了环境变量的覆盖，非常感谢mapengfei53[4]的贡献。

## 3.Next

目前SRS支持了环境变量的配置，还需要改进支持Reload。

由于Reload依赖配置文件，在收到Reload信号后，重新加载配置文件，对比发现变更后，实现定向的快速Reload。而环境变量的配置，则需要实现对应的变更检测机制，我们会在后续改进和完善。

此外，之前Reload的机制过度设计，有些其实没有必要支持Reload，比如侦听的端口，是不会在运行中变化，而且变化会导致很多异常问题。因此，会在未来把很多不常用的Reload功能去掉，只支持必要的Reload。

还有，K8s中的配置是通过ConfigMap加载到容器，而通过inotify机制，可以在文件内容修改后，主动加载配置Reload，而不需要发送Reload信号，这样在K8s集群中只需要修改ConfigMap就会自动生效到容器。这个机制同样也需要支持环境变量，如何在环境变量变更后，在K8s集群中生效。

最后补充一句：SRS会永远支持配置文件，以及对应的配置变更Reload机制，环境变量的配置方法，是对目前配置能力的完善，并不是替代。

**引用链接**

`[1]` #2277: *https://github.com/ossrs/srs/issues/2277*
`[2]` WebRTC over TCP: *https://ossrs.net/lts/zh-cn/docs/v5/doc/webrtc#webrtc-over-tcp*
`[3]` full.conf: *https://github.com/ossrs/srs/blob/develop/trunk/conf/full.conf*
`[4]` mapengfei53: *https://github.com/mapengfei53*

原文作者：Written by 马鹏飞, Winlin

原文链接：https://mp.weixin.qq.com/s/ABMrvGEer1FYrdR9PrearA

# 【NO.371】linux服务器网络编程之线程模型

## 1.前言

本文将主要介绍传统的和目前流行的进程/线程模型，在讲进程/线程模型之前需要先介绍一种设计模式： Reactor 模式。Reactor 模式首先是事件驱动的，有一个或多个并发输入源，有一个Service Handler，有多个Request Handlers；这个Service Handler会同步的将输入的请求（Event）多路复用的分发给相应的Request Handler。如果用图表示的如下：

![img](https://pic1.zhimg.com/80/v2-5d48f0a255c09ee7977fb4baf7796e8c_720w.webp)

不知道读者有没有发现 Reactor 模式跟 IO 模型中的 IO 多路复用模型非常相似 ，在学习网络编程过程中也被这两个概念迷惑了很久。其实在设计模式层面 IO 多路复用也是采用 Reactor 模式的。IO 多路复用模型可以看成是 Reactor 模式在 IO 模型上的应用，而今天我们要讲的是 Reactor 模式在进程/线程模型上的应用。

在我的看来，进程/线程模型可以分为非 Reactor 模式和 Reactor 模式两种（当然还有 Proactor 模式，这种本文先不讲，因为使用的比较少，而且我也还没搞懂这种模式）。非 Reactor 模式和 Reactor 模式的两种进程/线程模型下具体又分很多种，后面会一一列举。非 Reactor 模式的进程/线程模型是传统的模型，现在已经很少见，放在这里主要是让读者做个了解同时与 Reactor 模式的进程/线程模式做个对比。

## 2.非 Reactor 模式的进程/线程模型

传统模型不使用 IO 多路使用，所以问题比较多。初学者建议看下这部分，如果你觉得被迷糊了或者不感兴趣可以跳过该部分，直接看 Reactor 模式的部分即可。但该部分的第 1、2 点需要了解下。

### 2.1.单进程单线程

![img](https://pic1.zhimg.com/80/v2-31c3ac7928691f977743f78ec6a8c12c_720w.webp)

描述：这种模型所有的逻辑都在在一个进程中，包括建立连接->Read 连接上的数据->业务处理->Write 回一些数据然后一直循环下去。该模型一次只能处理一个连接，这个在真正的应用中是没有的。初学者在模仿《Unix网络编程卷 I》例子编写网络程序时应该会使用这种模型，当然例子中一般会在 Write 后面多个 Close 连接的动作，以免在处理下一个连接的时候造成前一个句柄泄露。

优点：代码简单，无需去了解进程、线程的概念，适合学习网络编程的初学者。在不了解进程/线程模型情况下的默认模型。

缺点：没有任何实用价值。

### 2.2.单进程多线程

![img](https://pic2.zhimg.com/80/v2-1db2022195cc7fe25ce1fbac2cdd82f9_720w.webp)

描述：进程只做建立连接的动作，每接收一个连接就创建一个线程，在此连接上的读->业务处理->写->关闭连接都在线程中去做，可以采用线程池的方式减少线程的创建和销毁。这种线程模型有一定的应用场景，Tomcat 三种线程模型之 BIO 用的就是这种进程/线程模型。初学者在学习完单进程单线程模型后对线程有所了解即可开始学习该种模型，可以实现一个简单的聊天室程序。

优点：可以同时与多个 Client 建立连接，接收连接和处理连接业务分开。

缺点：每个连接占用一个线程，当连接上没有数据的时候造成线程资源浪费，可以建立的连接数比较有限。

### 2.3.多进程单线程

![img](https://pic3.zhimg.com/80/v2-d944ae6ebdd5a0e09733a0a6e69a4e3e_720w.webp)

描述：

(1) 主进程启动时创建监听套接字并监听，然后 fork 出 N 个子进程。

(2) 由于父子进程的继承性，子进程同时也在端口监听，然后在父进程中关闭监听。

(3) 父进程负责子进程的创建、销毁、资源回收等，子进程负责连接的建立->Read->业务处理->Write 等。

由于所有进程都在同一个端口监听，该模型会出现一个比较知名的现象—惊群现象：当有一个连接来临时，所有子进程都会被唤醒，但是最后能与 Client 建立连接的只有一个，造成资源浪费（系通调度也是消耗 CPU 的）。不过 linux 2.6 版本以后已经在内核消除了惊群，当有连接来临只会唤醒一个等待在 accept() 上的进程。即使内核没修复，在应用层也可以用锁的方式防止惊群。

缺点：这种模型是单进程单线程的进化版本，然而并没有什么卵用。且增加了开发的难度。所以不列出它的优点，介绍这种模型主要是引出惊群的概念，在后面的 Reactor 模型中的多进程情况下也会出现类似的情况。

## 3.Reactor 模式的进程/线程模型

该模式一般是 Reactor 模型 + IO 多路复用，下面的任何一种模型都具有一定的实用场景。

### 3.1.单进程单线程

![img](https://pic4.zhimg.com/80/v2-acbbe0a0e34d3a46ab769c32a99a4d73_720w.webp)

描述：只有一个进程，监听套接字和连接套接字上的事件都由 Select 来处理，

(1) 如果有建立连接的请求过来，Acceptor 负责接受并与之建立连接，同时将连接套接字加入 Select 进行监听；

(2) 如果某个连接上有读事件则进行 Read->业务处理->Write 等操作；

(3) 如此循环反复。

优点：编程简单，对于业务处理不复杂的后台，基本能满足服务器端网络编程。老东家的服务器端程序全是这种模式，主要原因有如下

(1) 如果一台机器性能不行，那就向集群中新增一台。

(2) 业务处理并不复杂。

(3) 扩展成多进程的话，如果不是多核意义不大。

(4) 如果采用单进程多线程，C++ 处理线程不像 Java 简单，还要考虑并发的问题，收益比不大。

缺点：会有阻塞，在进行业务处理的时候不能进行其他操作：如建立连接，读取其他套接字上的数据等。

### 3.2.单进程多线程

![img](https://pic1.zhimg.com/80/v2-396df963e256537ffc9bf0ce879c131c_720w.webp)

描述：与单进程单线程类似，不同的是该模型将业务处理放在线程中，进程就不会阻塞在业务处理上。

优点：比较完美的进程/线程模型，在 Java 实现中复杂度也不高。很多网络库都是基于此，比如 Netty 。

缺点：待补充。

### 3.3.多进程单线程：

![img](https://pic1.zhimg.com/80/v2-3fb9ca9439a9470de24caefc400484b0_720w.webp)

描述：与非 Reactor 模式中的多进程单线程相似，只是本模式在子进程中使用了 IO 多路复用，实用性以下就上来了。大名鼎鼎的 nginx 就采用这种进程/线程模型

优点：编程相对简单，充分利用多核。能满足高并发，不然 nginx 也不可能采用这种模式。

缺点：子进程还是会阻塞在业务处理上。

### 3.4.多进程多线程

描述：这里不再画出图形，就是在在子进程上将业务处理交给多线程处理，参考单进程多线程里的线程池那里。

优点：充分利用多核同时子进程不会阻塞在业务处理上

缺点：编程复杂。

### 3.5.主从进程 +多线程：

![img](https://pic1.zhimg.com/80/v2-f1e0589284aa5392930b326503d4a0d0_720w.webp)

描述：前面几种 Reactor 模式的进程/线程模型中，连接的建立和连接的读写都是在同一进程中。本模型中将连接的建立和连接读写放在不同的进程中。

(1) 主进程在监听套接字上 Select 阻塞，一旦有请求过来则与之建立连接，并将连接套接字传递给从进程。

(2) 从进程在连接套接字上 Select 阻塞，一旦连接上有数据过来则进行 Read，并将业务处理通过线程来处理。如果有必要还会向连接 Write 数据。

优点：连接的建立和连接的读写分开在不同进程中，处理效率会更高。该模型比单进程多线程模式还更优一点，且也可以利用多核。

缺点：编程复杂。

## 4.总结

以上就是常见的进程/线程模型，使用了 IO 多路复用的线程模型一般都可以称为 Reactor 模型，所以不用纠结 IO 多路复用与 Reactor 模式之间的关系。

原文链接：https://zhuanlan.zhihu.com/p/399449053

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.372】腾讯跟阿里两位王者之间的对比

## 1.前言

程序员朋友圈有一篇比较火的文章《当下（2018 年）腾讯的技术建设是否处于落后同体量公司的状态？》，虽然网上不乏介绍腾讯与阿里不同之处（包括文化、薪资待遇、公司氛围和技术建设等方面）的文章，今天就来跟大家分析一下，不免具有一定的片面性，不喜勿喷。

## 2.关于腾讯

腾讯整体上是一家公司，但很多时候各个部门之间各自为政。部门之间的合作更像一个公司与另一个公司之间的合作，在沟通的时候如果不是负责人不能向对方透露任何跟工作有关的细节、进度等。在腾讯某部门的时候由于此还吃过几次亏，以至于来到阿里之后跨部门间合作总是战战兢兢，生怕说错、做错了什么。后来才发现，在阿里大部分情况下都不需要考虑这些。部门之间的合作就是公司内部的合作，就这样我才慢慢晃过神来。

微信想必大家都知道，其实当初腾讯做微信这个产品的时候并不是只有广州微信团队在做。成都也有团队在做，当时是谁先做好、谁先推广就能活下去。最后当然是广州的微信做成了，另一个类似“微信”的产品就只能死掉了。

还有之前吃鸡游戏比较火的时候。腾讯一下推出了两款吃鸡手游：光子工作室的刺激战场和天美工作室的全军出击来进行瓜分市场。这样的例子还有很多，就我所在的腾讯某部门的一个核心安全产品在公司内都存在竞争对手。可能这就是腾讯的文化，一个个好的产品就这样被竞争、打磨出来了。都说百度的技术、阿里的运营和腾讯的产品，也不是没有道理的。

## 3.关于阿里

而阿里就完全不一样了，来阿里某 BU 的第一个月。师兄带着我做一款新的产品，产品的功能需要自己去摸索。主管只指定了大的方向，对我们的唯一要求就是不要做别人/别组/别BU已经做过的东西、一定要善于使用公司已有的平台给自己的产品赋能。可见阿里内部更注重合作和创新，同样的事绝不做两遍。

在腾讯，正是因为隔离和竞争，所以各个部门之间的交流、合作和共享就很少，特别是在技术方面，每个部门闭门造车，没有或很少有积累，这也是导致腾讯在技术建设方面落后的主因。就我这几个月的亲身体会而言，阿里的技术建设要比腾讯好很多。所以来到阿里后，我常常对那些喊苦的同事说，在阿里上班够幸福了，只需要专心的写业务代码，不用关心一些杂七杂八的破事。

## 4.两者对比

腾讯更像是在一家创业公司，一个应用从开始到结束所涉及到的所有流程都需要开发参与，包括申请服务器、搭建环境、安装 DB、部署和运维等。也就是说开发要做的事往往包括开发、测试和运维。常常让人叫苦不迭，给我最大的感受就是很忙，但是不知道在忙什么。 当然在阿里也是有这些流程的，但很多时候只需要在页面上点一点就了。这极大的节省了开发的时间、运维的成本和出错的概率。这主要得益于阿里的技术建设的完善，我举两个简单的例子来说明下腾讯和阿里在技术建设方面的差异就更能直观感受两者之间的差异了：

### 4.1.统一 DB 管理平台

在腾讯某部门的时候，应用要使用 DB 大部分情况下只能在 linux 服务器上自己搭建 mysql 环境，使用本地 DB。然后很多操作都是在 linux shell中进行 CRUD 操作，非常的原始。最痛苦的是对 DB 的运维，出了问题还得自己背锅。在阿里某 BU 的时候，新建应用直接上公司内部平台申请下就好了，而且会自动分配给你两套（测试和正式）。所有的操作都是在页面上，DB 也不需要自己去运维了。读写都做了很好的权限控制，根本不怕误操作。

### 4.2.统一配置中心

很多应用中的变量都需要在配置文件中进行配置，在腾讯某部门的时候都是以文件的形式和应用一起部署。配置文件的读写都是应用自己实现的，当然这个已经有封装好的库了。但是如果要修改配置文件呢？得手动修改后重启应用，其运维成本可想而知。而在阿里有整个公司公用的统一配置中心中间件：Diamond，其简单易用。使用的时候在页面上修改下，点发布就会自动热更新到应用下，不用重启、不容易出错。

可以说阿里在开发、测试、运维和部署等流程都已经有着很成熟的方案和平台了，而腾讯还在使用比较原始的方式。以上两例只是腾讯与阿里在技术建设差距上的一瞥，其他很多方面不想赘述。至于具体原因，知乎上的回答已经有了很好的阐述。所以说平台化、流程化和标准化是多么重要。腾讯每个部门都有自己的一套，各个部门之间很少共享，所以导致很多东西仅限于有而不精。阿里整个公司共用一套，所以慢慢的沉淀出公司级的产品。腾讯可能也已经认识到自己在这方面的不足，不久前刚做了一次组织架构的调整。

## 5.总结

技术建设完善最大的收益者当然是程序员了，在腾讯和阿里每天上班工作时间差不多。但是在阿里明显轻松多了，只需要一心专注业务代码就好；在腾讯很多时候都在要为环境、运维和部署发愁。写代码的时间很少，还经常被压榨。

不过凡事有利有弊，在腾讯由于什么都需要自己亲力亲为，所以员工很多东西都懂一些。特别是底层的一些基础知识、算法和优化方面，但精不精就不知道了。由于历史原因腾讯的主要编程语言是 C++，而阿里则是 Java。所以注定了腾讯偏底层应用开发，而阿里偏上层业务开发。个人觉得腾讯员工在底层技术方面可能要强于阿里的，而阿里员工在面向对象、设计模式和业务沟通方面是要强于腾讯员工的。

**今天我给大家整理了一下腾讯T1~T9的后端工程师分别需要具备哪些能力以及对应的腾讯核心技术点学习路线总结图。希望可以给各位广大学后端的朋友一面镜子映照自身所学，早日拿到心仪的offer，进入大厂。**

![img](https://pic4.zhimg.com/80/v2-3cf60d6bc744aa90b5c61671595ba173_720w.webp)

原文链接：[https://zhuanlan.zhihu.com/p/399751135](https://www.zhihu.com/people/huhu520-10)

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.373】IM（即时通讯）服务端

## 1.前言

首先讲讲IM（即时通讯）技术可以用来做什么：

- 聊天：qq、微信
- 直播：斗鱼直播、抖音
- 实时位置共享、游戏多人互动等等

可以说几乎所有高实时性的应用场景都需要用到IM技术。

本篇将带大家从零开始搭建一个轻量级的IM服务端，麻雀虽小，五脏俱全，我们搭建的IM服务端实现以下功能：

- 一对一的文本消息、文件消息通信
- 每个消息有“已发送”/“已送达”/“已读”回执
- 存储离线消息
- 支持用户登录，好友关系等基本功能。
- 能够方便地水平扩展

**这个项目涵盖了很多后端必备知识：**

- rpc通信
- 数据库
- 缓存
- 消息队列
- 分布式、高并发的架构设计
- docker部署

## 2.消息通信

### 2.1.文本消息

我们先从最简单的特性开始实现：一个普通消息的发送

消息格式如下：

```
message ChatMsg{
    id = 1;
    //消息id
    fromId = Alice
    //发送者userId
    destId = Bob
    //接收者userId
    msgBody = hello
    //消息体
}
```

![img](https://pic3.zhimg.com/80/v2-ed5a6448fa6a63dd0cb7a51e87ded53e_720w.webp)

如上图，我们现在有两个用户：Alice和Bob连接到了服务器，当Alice发送消息message(hello)给Bob，服务端接收到消息，根据消息的destId进行转发，转发给Bob。

### 2.2.发送回执

**那我们要怎么来实现回执的发送呢？**

我们定义一种回执数据格式ACK，MsgType有三种，分别是sent（已发送）,delivered（已送达）, read（已读）：

```
message AckMsg {
    id;
    //消息id
    fromId;
    //发送者id
    destId;
    //接收者id
    msgType;
    //消息类型
    ackMsgId;
    //确认的消息id
}
enum MsgType {
    DELIVERED;
    READ;
}
```

当服务端接受到Alice发来的消息时：

**1.向Alice发送一个sent(hello)表示消息已经被发送到服务器。**

```
message AckMsg {
    id = 2;
    fromId = Alice;
    destId = Bob;
    msgType = SENT;
    ackMsgId = 1;
}
```

![img](https://pic4.zhimg.com/80/v2-6803c7cf665cab48b17907dae9133a5f_720w.webp)

**2.服务器把hello转发给Bob后，立刻向Alice发送delivered(hello)表示消息已经发送给Bob。**

```
message AckMsg {
    id = 3;
    fromId = Bob;
    destId = Alice;
    msgType = DELIVERED;
    ackMsgId = 1;
}
```

![img](https://pic4.zhimg.com/80/v2-52e095ddf63863dfbcdb2ee05a17467b_720w.webp)

**3.Bob阅读消息后，客户端向服务器发送read(hello)表示消息已读**

```
message AckMsg {
    id = 4;
    fromId = Bob;
    destId = Alice;
    msgType = READ;
    ackMsgId = 1;
}
```

这个消息会像一个普通聊天消息一样被服务器处理，最终发送给Alice。

![img](https://pic1.zhimg.com/80/v2-aeb5b18074e37791520dfc72354cffc4_720w.webp)

**在服务器这里不区分ChatMsg和AckMsg，处理过程都是一样的：解析消息的destId并进行转发。**

## 3.水平扩展

当用户量越来越大，必然需要增加服务器的数量，用户的连接被分散在不同的机器上。此时，就需要存储用户连接在哪台机器上。

我们引入一个新的模块来管理用户的连接信息。

## 4.管理用户状态

![img](https://pic2.zhimg.com/80/v2-464d2e2722f7c282157e4aa3b3187a59_720w.webp)

模块叫做user status，共有三个接口：

```
public interface UserStatusService {
    /**
     * 用户上线，存储userId与机器id的关系
     *
     * @param userId
     * @param connectorId
     * @return 如果当前用户在线，则返回他连接的机器id，否则返回null
     */
    String online(String userId, String connectorId);
    /**
     * 用户下线
     *
     * @param userId
     */
    void offline(String userId);
    /**
     * 通过用户id查找他当前连接的机器id
     *
     * @param userId
     * @return
     */
    String getConnectorId(String userId);
}
```

这样我们就能够对用户连接状态进行管理了，具体的实现应考虑服务的用户量、期望性能等进行实现。

此处我们使用redis来实现，将userId和connectorId的关系以key-value的形式存储。

## 5.消息转发

除此之外，还需要一个模块在不同的机器上转发消息，如下结构：

![img](https://pic2.zhimg.com/80/v2-acac7d7fb6cc6740b0131192d139f279_720w.webp)

此时我们的服务被拆分成了connector和transfer两个模块，connector模块用于维持用户的长链接，而transfer的作用是将消息在多个connector之间转发。

现在Alice和Bob连接到了两台connector上，那么消息要如何传递呢？

**1.Alice上线，连接到机器[1]上时**

- 将Alice和它的连接存入内存中。
- 调用user status的online方法记录Alice上线。

**2.Alice发送了一条消息给Bob**

- 机器[1]收到消息后，解析destId，在内存中查找是否有Bob。
- 如果没有，代表Bob未连接到这台机器，则转发给transfer。

**3.transfer调用user status的getConnectorId(Bob)方法找到Bob所连接的connector，返回机器[2]，则转发给机器[2]。**

## 6.流程图：

![img](https://pic1.zhimg.com/80/v2-8f65c8d5fa01c633aa0e6d2475a48134_720w.webp)

## 7.总结：

- 引入user status模块管理用户连接，transfer模块在不同的机器之间转发，使服务可以水平扩展。
- 为了满足实时转发，transfer需要和每台connector机器都保持长链接。

## 8.离线消息

如果用户当前不在线，就必须把消息持久化下来，等待用户下次上线再推送，这里使用mysql存储离线消息。

为了方便地水平扩展，我们使用消息队列进行解耦。

- transfer接收到消息后如果发现用户不在线，就发送给消息队列入库。
- 用户登录时，服务器从库里拉取离线消息进行推送。

## 9.用户登录、好友关系

用户的注册登录、账户管理、好友关系链等功能更适合使用http协议，因此我们将这个模块做成一个restful服务，对外暴露http接口供客户端调用。

至此服务端的基本架构就完成了：

![img](https://pic2.zhimg.com/80/v2-baf1c6930b38586df4ad6a3279231061_720w.webp)

## 10.总结

以上就是这篇博客的所有内容，本篇帮大家构建了IM服务端的架构，但还有很多细节需要我们去思考，例如：

- 如何保证消息的顺序和唯一
- 多个设备在线如何保证消息一致性
- 如何处理消息发送失败
- 消息的安全性
- 如果要存储聊天记录要怎么做
- 数据库分表分库
- 服务高可用

原文链接：https://zhuanlan.zhihu.com/p/401109228

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.374】一文带你了解大厂亿级并发下高性能服务器是如何实现的！

## 1.多进程

历史上最早出现也是最简单的一种并行处理多个请求的方法就是利用**多进程**。

比如在Linux世界中，我们可以使用fork、exec等**系统调用**创建多个进程，我们可以在父进程中接收用户的连接请求，然后创建子进程去处理用户请求，就像这样：

![img](https://pic3.zhimg.com/80/v2-8943df0d0fc077832f017c8a38ebb8d2_720w.webp)

这种方法的优点就在于：

1. 编程简单，非常容易理解
2. 由于各个进程的地址空间是相互隔离的，因此一个进程崩溃后并不会影响其它进程
3. 充分利用多核资源

多进程并行处理的优点很明显，但是缺点同样明显：

1. 各个进程地址空间相互隔离，这一优点也会变成缺点，那就是进程间要想通信就会变得比较困难，你需要借助进程间通信(IPC，interprocess communications)机制，想一想你现在知道哪些进程间通信机制，然后让你用代码实现呢？显然，进程间通信编程相对复杂，而且性能也是一大问题
2. 我们知道创建进程开销是比线程要大的，频繁的创建销毁进程无疑会加重系统负担。

幸好，除了进程，我们还有线程。

## 2.多线程

不是创建进程开销大吗？不是进程间通信困难吗？这些对于线程来说统统不是问题。

由于线程共享进程地址空间，因此线程间通信天然不需要借助任何通信机制，直接读取内存就好了。线程创

建销毁的开销也变小了，要知道线程就像寄居蟹一样，房子(地址空间)都是进程的，自己只是一个租客，因此非常的**轻量级**，创建销毁的开销也非常小。

我们可以为每个请求创建一个线程，即使一个线程因执行I/O操作——比如读取数据库等——被阻塞暂停运行也不会影响到其它线程，就像这样：

![img](https://pic3.zhimg.com/80/v2-201fefe461b9c10d75c34343a74cf2a2_720w.webp)

但线程就是完美的、包治百病的吗，显然，计算机世界从来没有那么简单。

由于线程共享进程地址空间，这在为线程间通信带来便利的同时也带来了无尽的麻烦。

正是由于线程间共享地址空间，因此一个线程崩溃会导致整个进程崩溃退出，同时线程间通信简直太简单了，简单到线程间通信只需要直接读取内存就可以了，也简单到出现问题也极其容易，死锁、线程间的同步互斥、等等，这些极容易产生bug，**无数程序员宝贵的时间就有相当一部分用来解决多线程带来的无尽问题**。

虽然线程也有缺点，但是相比多进程来说，线程更有优势，**但想单纯的利用多线程就能解决高并发问题也是不切实际的**。

因为虽然线程创建开销相比进程小，但依然也是有开销的，对于动辄数万数十万的链接的高并发服务器来说，创建数万个线程会有性能问题，这包括内存占用、线程间切换，也就是调度的开销。

因此，我们需要进一步思考。

## 3.Event Loop：事件驱动

到目前为止，我们提到“并行”二字就会想到进程、线程。但是，并行编程只能依赖这两项技术吗，并不是这样的。

还有另一项并行技术广泛应用在GUI编程以及服务器编程中，这就是近几年非常流行的事件驱动编程，event-based concurrency。

大家不要觉得这是一项很难懂的技术，实际上事件驱动编程原理上非常简单。这一技术需要两种原料：

1. event
2. 处理event的函数，这一函数通常被称为event handler

剩下的就简单了：

你只需要安静的等待event到来就好，当event到来之后，检查一下event的类型，并根据该类型找到对应的event处理函数，也就是event handler，然后直接调用该event handler就好了。

![img](https://pic2.zhimg.com/80/v2-5fc49b3d520c2b2506651642dcefb725_720w.webp)

**That’s it !**

以上就是事件驱动编程的全部内容，是不是很简单！从上面的讨论可以看到，我们需要不断的接收event然后处理event，因此我们需要一个循环(用while或者for循环都可以)，这个循环被称为Event loop。

使用伪代码表示就是这样：

```
while(true) {
    event = getEvent();
    handler(event);
}
```

Event loop中要做的事情其实是非常简单的，只需要等待event的带来，然后调用相应的event处理函数即可。

注意，这段代码只需要运行在一个线程或者进程中，只需要这一个event loop就可以同时处理多个用户请求。

有的同学可以依然不明白为什么这样一个event loop可以同时处理多个请求呢？

原因很简单，对于web服务器来说，处理一个用户请求时大部分时间其实都用在了I/O操作上，像数据库读写、文件读写、网络读写等。当一个请求到来，简单处理之后可能就需要查询数据库等I/O操作，我们知道I/O是非常慢的，当发起I/O后**我们大可以不用等待该I/O操作完成就可以继续处理接下来的用户请求**。

![img](https://pic3.zhimg.com/80/v2-aa4b29f3965e2d035b120035d9817b72_720w.webp)

现在你应该明白了吧，虽然上一个用户请求还没有处理完我们其实就可以处理下一个用户请求了，这也是并行，这种并行就可以用事件驱动编程来处理。

这就好比餐厅服务员一样，一个服务员不可能一直等上一个顾客下单、上菜、吃饭、买单之后才接待下一个顾客，服务员是怎么做的呢？当一个顾客下完单后直接处理下一个顾客，当顾客吃完饭后会自己回来买单结账的。

看到了吧，同样是一个服务员也可以同时处理多个顾客，这个服务员就相当于这里的Event loop，即使这个event loop只运行在一个线程(进程)中也可以同时处理多个用户请求。

相信你已经对事件驱动编程有一个清晰的认知了，那么接下来的问题就是事件驱动、事件驱动，那么这个事件也就是event该怎么获取呢？

## 4.事件来源：IO多路复用

在Linux/Unix世界中一切皆文件，而我们的程序都是通过文件描述符来进行I/O操作的，当然对于socket也不例外，那我们该如何同时处理多个文件描述符呢？

IO多路复用技术正是用来解决这一问题的，通过IO多路复用技术，我们一次可以监控多个文件描述，当某个文件(socket)可读或者可写的时候我们就能得到通知啦。

这样IO多路复用技术就成了event loop的原材料供应商，源源不断的给我们提供各种event，这样关于event来源的问题就解决了。

![img](https://pic4.zhimg.com/80/v2-ee98b1280d796711d00ef3972fff8eff_720w.webp)

至此，关于利用事件驱动来实现并发编程的所有问题都解决了吗？event的来源问题解决了，当得到event后调用相应的handler，看上去大功告成了。

想一想还有没有其它问题？

## 5.问题：阻塞式IO

现在，我们可以使用一个线程(进程)就能基于事件驱动进行并行编程，再也没有了多线程中让人恼火的各种锁、同步互斥、死锁等问题了。

但是，计算机科学中从来没有出现过一种能解决所有问题的技术，现在没有，在可预期的将来也不会有。

那上述方法有什么问题吗？

不要忘了，我们event loop是运行在一个线程(进程)，这虽然解决了多线程问题，但是如果在处理某个event时需要进行IO操作会怎么样呢？

程序员最常用的这种IO方式被称为阻塞式IO，也就是说，当我们进行IO操作，比如读取文件时，如果文件没有读取完成，那么我们的程序(线程)会被阻塞而暂停执行，这在多线程中不是问题，因为操作系统还可以调度其它线程。

但是在单线程的event loop中是有问题的，原因就在于当我们在event loop中执行阻塞式IO操作时整个线程(event loop)会被暂停运行，这时操作系统将没有其它线程可以调度，因为系统中只有一个event loop在处理用户请求，这样当event loop线程被阻塞暂停运行时所有用户请求都没有办法被处理，你能想象当服务器在处理其它用户请求读取数据库导致你的请求被暂停吗？

![img](https://pic4.zhimg.com/80/v2-acd3c9bea7d909a144b4936b69c46b17_720w.webp)

因此，在基于事件驱动编程时有一条注意事项，**那就是不允许发起阻塞式IO**。

有的同学可能会问，如果不能发起阻塞式IO的话，那么该怎样进行IO操作呢？有阻塞式IO，就有非阻塞式IO。

## 6.非阻塞IO

为克服阻塞式IO所带来的问题，现代操作系统开始提供一种新的发起IO请求的方法，这种方法就是异步IO，对应的，阻塞式IO就是同步IO。

异步IO时，假设调用aio_read函数(具体的异步IO API请参考具体的操作系统平台)，也就是异步读取，当我们调用该函数后可以**立即返回**，并继续其它事情，虽然此时该文件可能还没有被读取，这样就不会阻塞调用线程了。此外，操作系统还会提供其它方法供调用线程来检测IO操作是否完成。

就这样，在操作系统的帮助下IO的阻塞调用问题也解决了。

## 7.基于事件编程的难点

虽然有异步IO来解决event loop可能被阻塞的问题，但是基于事件编程依然是困难的。

首先，我们提到，event loop是运行在一个线程中的，显然一个线程是没有办法充分利用多核资源的，有的同学可能会说那就创建多个event loop实例不就可以了，这样就有多个event loop线程了，但是这样一来多线程问题又会出现。

另一点在于编程方面，编程方式需要把处理逻辑分为两部分，一部分调用方自己处理，另一部分在回调函数中处理，这一编程方式的改变加重了程序员在理解上的负担，基于事件编程的项目后期会很难扩展以及维护。

那么有没有更好的方法呢？

要找到更好的方法，我们需要解决问题的本质，那么这个本质问题是什么呢？

## 8.更好的方法

为什么我们要使用异步这种难以理解的方式编程呢？

是因为阻塞式编程虽然容易理解但会导致线程被阻塞而暂停运行。

那么聪明的你一定会问了，有没有一种方法既能结合同步IO的简单理解又不会因同步调用导致线程被阻塞呢？

答案是肯定的，这就是用户态线程，user level thread，也就是大名鼎鼎的协程，关于协程值得单独拿出一篇文章来讲解，就在下一篇。

虽然基于事件编程有这样那样的缺点，但是在当今的高性能高并发服务器上基于事件编程方式依然非常流行，但已经不是纯粹的基于单一线程的事件驱动了，而是event loop + multi thread + user level thread。

## 9.总结

高并发技术从最开始的多进程一路演进到当前的事件驱动，计算机技术就像生物一样也在不断演变进化，但不管怎样，了解历史才能更深刻的理解当下。希望这篇文章能对大家理解高并发服务器有所帮助。

原文链接：https://zhuanlan.zhihu.com/p/398701843

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.375】你真的懂Redis与MySQL双写一致性如何保证吗？

![img](https://pic2.zhimg.com/80/v2-bf659604e4c91ca143c6d076860d76c1_720w.webp)

一致性就是数据保持一致，在分布式系统中，可以理解为多个节点中数据的值是一致的。

- **强一致性**：这种一致性级别是最符合用户直觉的，它要求系统写入什么，读出来的也会是什么，用户体验好，但实现起来往往对系统的性能影响大
- **弱一致性**：这种一致性级别约束了系统在写入成功后，不承诺立即可以读到写入的值，也不承诺多久之后数据能够达到一致，但会尽可能地保证到某个时间级别（比如秒级别）后，数据能够达到一致状态
- **最终一致性**：最终一致性是弱一致性的一个特例，系统会保证在一定时间内，能够达到一个数据一致的状态。这里之所以将最终一致性单独提出来，是因为它是弱一致性中非常推崇的一种一致性模型，也是业界在大型分布式系统的数据一致性上比较推崇的模型。

![img](https://pic2.zhimg.com/80/v2-b6e219313e5a7bb717ebe5c7e4986145_720w.webp)

## 1.三个经典的缓存模式

缓存可以提升性能、缓解数据库压力，但是使用缓存也会导致数据**不一致性**的问题。一般我们是如何使用缓存呢？有三种经典的缓存使用模式：

- Cache-Aside Pattern
- Read-Through/Write-through
- Write-behind

## 2.Cache-Aside Pattern

Cache-Aside Pattern，即**旁路缓存模式**，它的提出是为了尽可能地解决缓存与数据库的数据不一致问题。

## 3.Cache-Aside读流程

**Cache-Aside Pattern**的读请求流程如下：

![img](https://pic4.zhimg.com/80/v2-07928a879bba5403e8c612686d827f4b_720w.webp)

1. 读的时候，先读缓存，缓存命中的话，直接返回数据
2. 缓存没有命中的话，就去读数据库，从数据库取出数据，放入缓存后，同时返回响应。

## 4.Cache-Aside 写流程

**Cache-Aside Pattern**的写请求流程如下：

![img](https://pic1.zhimg.com/80/v2-b13d98e2bdd1e3dadfb94cafadc2353c_720w.webp)

更新的时候，先**更新数据库，然后再删除缓存**。

## 5.Read-Through/Write-Through（读写穿透）

**Read/Write-Through**模式中，服务端把缓存作为主要数据存储。应用程序跟数据库缓存交互，都是通过**抽象缓存层**完成的。

## 6.Read-Through

**Read-Through**的简要流程如下

![img](https://pic1.zhimg.com/80/v2-e6b5141a91306068a7d50d8f525ccd58_720w.webp)

1. 从缓存读取数据，读到直接返回
2. 如果读取不到的话，从数据库加载，写入缓存后，再返回响应。

这个简要流程是不是跟**Cache-Aside**很像呢？其实**Read-Through**就是多了一层**Cache-Provider**而已，流程如下：

![img](https://pic2.zhimg.com/80/v2-8b32355be4dd53489a5c3d7244431fcd_720w.webp)

## 7.Write-behind （异步缓存写入）

**Write-behind** 跟Read-Through/Write-Through有相似的地方，都是由**Cache Provider**来负责缓存和数据库的读写。它们又有个很大的不同：**Read/Write-Through**是同步更新缓存和数据的，**Write-Behind**则是只更新缓存，不直接更新数据库，通过**批量异步**的方式来更新数据库。

![img](https://pic1.zhimg.com/80/v2-3dd2fa657d87e1077be59747a6bd09b8_720w.webp)

这种方式下，缓存和数据库的一致性不强，**对一致性要求高的系统要谨慎使用**。但是它适合频繁写的场景，MySQL的**InnoDB Buffer Pool机制**就使用到这种模式。

## 8.操作缓存的时候，到底是删除缓存呢，还是更新缓存？

日常开发中，我们一般使用的就是**Cache-Aside**模式。有些小伙伴可能会问， **Cache-Aside**在写入请求的时候，为什么是**删除缓存而不是更新缓存**呢？

![img](https://pic4.zhimg.com/80/v2-f4ec7a0a0eff2dc7d1b99b48b0d422af_720w.webp)

我们在操作缓存的时候，到底应该删除缓存还是更新缓存呢？我们先来看个例子：

![img](https://pic1.zhimg.com/80/v2-47f6dbdd8a3fd153edb68880608c2c8c_720w.webp)

1. 线程A先发起一个写操作，第一步先更新数据库
2. 线程B再发起一个写操作，第二步更新了数据库
3. 由于网络等原因，线程B先更新了缓存
4. 线程A更新缓存。

这时候，缓存保存的是A的数据（老数据），数据库保存的是B的数据（新数据），数据**不一致**了，脏数据出现啦。如果是**删除缓存取代更新缓存**则不会出现这个脏数据问题。

**更新缓存相对于删除缓存**，还有两点劣势：

- 如果你写入的缓存值，是经过复杂计算才得到的话。更新缓存频率高的话，就浪费性能啦。
- 在写数据库场景多，读数据场景少的情况下，数据很多时候还没被读取到，又被更新了，这也浪费了性能呢(实际上，写多的场景，用缓存也不是很划算的,哈哈)

## 9.双写的情况下，先操作数据库还是先操作缓存？

Cache-Aside缓存模式中，有些小伙伴还是会有疑问，在写请求过来的时候，为什么是**先操作数据库呢**？为什么**不先操作缓存**呢？

假设有A、B两个请求，请求A做更新操作，请求B做查询读取操作。

![img](https://pic2.zhimg.com/80/v2-db77fe1eaf1ded90fce5d8f57ae43bd1_720w.webp)

1. 线程A发起一个写操作，第一步del cache
2. 此时线程B发起一个读操作，cache miss
3. 线程B继续读DB，读出来一个老数据
4. 然后线程B把老数据设置入cache
5. 线程A写入DB最新的数据

酱紫就有问题啦，**缓存和数据库的数据不一致了。缓存保存的是老数据，数据库保存的是新数据**。因此，Cache-Aside缓存模式，选择了先操作数据库而不是先操作缓存。

- 个别小伙伴可能会问，先操作数据库再操作缓存，不一样也会导致数据不一致嘛？它俩又不是原子性操作的。这个是**会的**，但是这种方式，一般因为删除缓存失败等原因，才会导致脏数据，这个概率就很低。小伙伴们可以画下操作流程图，自己先分析下哈。接下来我们再来分析这种**删除缓存失败**的情况，**如何保证一致性**。

## 10.数据库和缓存数据保持强一致，可以嘛？

实际上，没办法做到数据库与缓存**绝对的一致性**。

- 加锁可以嘛？并发写期间加锁，任何读操作不写入缓存？
- 缓存及数据库封装CAS乐观锁，更新缓存时通过lua脚本？
- 分布式事务，3PC？TCC？

其实，这是由**CAP理论**决定的。缓存系统适用的场景就是非强一致性的场景，它属于CAP中的AP。**个人觉得，追求绝对一致性的业务场景，不适合引入缓存**。

![img](https://pic1.zhimg.com/80/v2-c27e30e3e562611267e217a293e14e44_720w.webp)

但是，通过一些方案优化处理，是可以**保证弱一致性，最终一致性**的。

## 11.3种方案保证数据库与缓存的一致性

### 11.1.缓存延时双删

有些小伙伴可能会说，并不一定要先操作数据库呀，采用**缓存延时双删**策略，就可以保证数据的一致性啦。什么是延时双删呢？

![img](https://pic2.zhimg.com/80/v2-0ce4b2d379ea35b61ce7a23e7c43ec2d_720w.webp)

1. 先删除缓存
2. 再更新数据库
3. 休眠一会（比如1秒），再次删除缓存。

这个休眠一会，一般多久呢？都是1秒

![img](https://pic1.zhimg.com/80/v2-e24ffa86eafb57aed70e214684917d08_720w.webp)

这种方案还算可以，只有休眠那一会（比如就那1秒），可能有脏数据，一般业务也会接受的。但是如果**第二次删除缓存失败**呢？缓存和数据库的数据还是可能不一致，对吧？给Key设置一个自然的expire过期时间，让它自动过期怎样？那业务要接受**过期时间**内，数据的不一致咯？还是有其他更佳方案呢？

### 11.2.删除缓存重试机制

不管是**延时双删**还是**Cache-Aside的先操作数据库再删除缓存**，都可能会存在第二步的删除缓存失败，导致的数据不一致问题。可以使用这个方案优化：删除失败就多删除几次呀,保证删除缓存成功就可以了呀~ 所以可以引入**删除缓存重试机制**

![img](https://pic1.zhimg.com/80/v2-0fadcdbd48c120488995ef21589b4ae4_720w.webp)

1. 写请求更新数据库
2. 缓存因为某些原因，删除失败
3. 把删除失败的key放到消息队列
4. 消费消息队列的消息，获取要删除的key
5. 重试删除缓存操作

### 11.3.读取biglog异步删除缓存

重试删除缓存机制还可以吧，就是会造成好多**业务代码入侵**。其实，还可以这样优化：通过数据库的**binlog来异步淘汰key**。

![img](https://pic1.zhimg.com/80/v2-4aaccecf677e2f0a0e532090ffcf3d38_720w.webp)

以mysql为例吧

- 可以使用阿里的canal将binlog日志采集发送到MQ队列里面
- 然后通过ACK机制确认处理这条更新消息，删除缓存，保证数据缓存一致性

觉得对你有帮助的话就点个赞吧~~

原文链接：https://zhuanlan.zhihu.com/p/401477769

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.376】基于后端开发Redisson实现分布式锁源码分析解读

## 1.分布式锁的概念和使用场景

分布式锁是控制分布式系统之间同步访问共享资源的一种方式。

　　在分布式系统中，常常需要协调他们的动作。如果不同的系统或是同一个系统的不同主机之间共享了一个或一组资源，那么访问这些资源的时候，往往需要互斥来防止彼此干扰来保证一致性，这个时候，便需要使用到分布式锁。

## 2.将redis官网对于分布式锁（红锁）的定义和Redisson实现做概括性总结

　　该部分可以先粗略的浏览一下，领略其官方的理论定义，读完后续内容会对该环节有更清晰的理解。

　　对于（红锁）官网定义：

![img](https://pic1.zhimg.com/80/v2-c74366a82c4ff8a656cc48aabdc0efdc_720w.webp)

　　　　　　　中文对如上5点做出解释：

redis红锁算法：

　　在Redis的分布式环境中，我们假设有N个Redis master。这些节点完全互相独立，不存在主从复制或者其他集群协调机制。我们确保将在N个实例上使用与在Redis单实例下相同方法获取和释放锁。现在我们假设有5个Redis master节点，同时我们需要在5台服务器上面运行这些Redis实例，这样保证他们不会同时都宕掉。

　　为了取到锁，客户端应该执行以下操作:

- 1、获取当前时间，以毫秒为单位。
- 2、依次尝试从5个实例，使用相同的key和随机值（Redisson中给出的是UUID + ThreadId）获取锁。当向Redis请求获取锁时，客户端应该设置一个网络连接和响应超时时间（我们接下来会在加锁的环节多次提到这个时间），这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端Redis已经挂掉的情况下，客户端还在一直等待响应结果。如果服务器端没有在规定时间内响应，客户端应该尽快尝试去另外一个Redis实例请求获取锁。
- 3、客户端使用当前时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。当且仅当从大多数（N/2+1，这里是3个节点）的Redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功。
- 4、如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间（步骤3计算的结果）。
- 5、如果因为某些原因，获取锁失败（没有在至少N/2+1个Redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁（即便某些Redis实例根本就没有加锁成功，防止某些节点获取到锁但是客户端没有得到响应而导致接下来的一段时间不能被重新获取锁）。

针对如上几点，redisson的实现：

![img](https://pic3.zhimg.com/80/v2-965d7bbd67287531bf3527b21967916e_720w.webp)

![img](https://pic2.zhimg.com/80/v2-ee9c885c59fed33e714fd1f507426091_720w.webp)

## 3.基于Redisson的分布式实现方案

　　在分析Redisson的源码前，先重申一下我们本文的重点放在分布式锁的加锁、锁重入、未获取到锁的线程继续获取锁、释放锁四个过程！希望可以对大家有所帮助。

　　锁重入：我们假设，一次加锁时间为30秒，当然Redisson默认的也是30秒，但是业务执行时间大于30秒，如果没有锁重入的实现，那么30秒后锁失效，业务逻辑就会陷入无法保证正确性的严重后果中。

### 3.1.第一步：添加依赖

```
<dependency>
     <groupId>org.redisson</groupId>
     <artifactId>redisson</artifactId>
     <version>3.12.5</version>
</dependency>
```

在正式编码前，我们先看下有关Redisson实现分布式锁的核心类之间的关系，如下图：

![img](https://pic2.zhimg.com/80/v2-a9ba6b1d66b107f43880696461c69a31_720w.webp)

### 3.2.第二步：正式编码测试代码

```
@Slf4j
@RunWith(SpringRunner.class)
@SpringBootTest(classes = BootIntegrationComponentApplication.class)
public class ReidsRedLockTest {
    private ExecutorService executorService = Executors.newCachedThreadPool();
    public RedissonRedLock getRedLock(){
        Config config1 = new Config();
        config1.useClusterServers()
                .addNodeAddress("redis://127.0.0.1:9001","redis://127.0.0.1:9002","redis://127.0.0.1:9003"
                        ,"redis://127.0.0.1:9004","redis://127.0.0.1:9005","redis://127.0.0.1:9006")
                .setPassword("123");
        RedissonClient redissonClient1 = Redisson.create(config1);//创建redissonClient对象，设置一系列的redis参数
        RLock rLock1 = redissonClient1.getLock("red_lock");
        //如果有多个redis cluster集群，则参考如上的写法创建对应的RLock对象，并传入下面的RedissonRedLock构造方法中。
        return new RedissonRedLock(rLock1);//获取redisson红锁
    }
    @Test
    public void redisRedLock() throws Exception {
        RedissonRedLock redLock = getRedLock();
        int[] count = {0};
        for (int i = 0; i < 1000; i++) {
            executorService.submit(() -> {
                try {
                    redLock.tryLock(10, TimeUnit.SECONDS);//加锁
                    count[0]++;
                    Thread.sleep(50000L);
                } catch (Exception e) {
                    log.error("添加分布式锁异常：",e);
                } finally {
                    try {
                        redLock.unlock();//释放锁
                    } catch (Exception e) {
                        log.error("解除分布式锁异常：",e);
                    }
                }
            });
        }
        executorService.shutdown();
        executorService.awaitTermination(1, TimeUnit.HOURS);
        log.info("计算后的结果：{}",count[0]);
    }
}
```

## 4.加锁过程分析

首先我们将加锁过程的方法调用栈列出，按照调用步骤分析加锁的源码实现：

![img](https://pic4.zhimg.com/80/v2-c7665d6ab7331f3db5aaca354fd85b53_720w.webp)

由上述调用栈可以看到，实现加锁的核心方法是：

![img](https://pic3.zhimg.com/80/v2-0e96dbe4979eefa29be91074cc4cdfd6_720w.webp)

这是一个调用lua脚本的执行过程，接下来对该方法做详细解释：

![img](https://pic3.zhimg.com/80/v2-500c9d173bb20c4b72130ab03c768d66_720w.webp)

针对lua脚本中参数占位符的问题：

- KEYS[1] = getName()，
- ARGV[1] = internalLockLeaseTime
- ARGV[2] = getLockName(threadId)

针对getLockName(threadId)方法，在创建redis连接管理器时，设置了id = UUID，具体如下

![img](https://pic2.zhimg.com/80/v2-8d3517ef126244883028510ddd33c7e1_720w.webp)

　　我们假设线程A，执行完上面的lua脚本，并且持有了该分布式锁，接下来针对线程A来说，直到业务逻辑结束，释放锁之前，该线程A，都将进入锁重入的环节，一直持续到业务逻辑执行完成，线程主动释放锁。而没有持有锁的线程，则进入争抢锁的过程，一直到持有锁（至于是公平竞争还是非公平竞争，我们先留一个悬念，欢迎各位看官老爷在评论区留言讨论）。

## 5.锁重入过程分析

再让我们回到加锁过程中方法调用栈的图片上，我们可以看到方法：

![img](https://pic3.zhimg.com/80/v2-463a66464ee2552dfac5af7e8224573e_720w.webp)

上图中的红框即是锁重入的实现方法，详细解释如下：

![img](https://pic1.zhimg.com/80/v2-0b8ee65443f8be6cd266a763c97efab8_720w.webp)

同样是利用lua脚本实现，

![img](https://pic3.zhimg.com/80/v2-696aabe7c32bddcbae74fc041808099a_720w.webp)

具体逻辑为：

- 0、我们假设线程A持有了该锁，则后台线程会在该锁持续了初始失效时间除3取整数的时间节点，做锁重入的操作。
- 1、if判断指定的key是否存在，且是否为当前线程所持有
- 2、如果被当前线程持有，则将失效时间重置为初始失效时间，redisson默认为30秒。
- 3、如果上面两步操作成功，则返回1，也即是true；否则返回false。

## 6.未获取到锁的线程继续获取锁

让我们将思路继续回到线程A获取锁的逻辑中，我们通过加锁方法调用栈可以看到方法：

```
public boolean tryLock(long waitTime, long leaseTime, TimeUnit unit) throws InterruptedException
```

该方法实属有些长，我们就分段截取分析。

![img](https://pic2.zhimg.com/80/v2-5bedabdcbdb7afe01a7ffb27306bbf45_720w.webp)

![img](https://pic4.zhimg.com/80/v2-2d66f0cf2ecacfc369003a86b1be83f3_720w.webp)

　　通过上图的分析，我们知道，如果一个线程初次没有获取到锁，则会一直尝试获取锁，直到我们设置的针对获取该redis实例锁的超时时间耗尽才罢休，在这个过程中没有获取到锁，则认为在该redis实例获取锁失败。

## 7.锁释放过程分析

我们还是先将锁释放过程方法调用栈列出：

![img](https://pic2.zhimg.com/80/v2-39662eb399ee7e6985b5d72079e64879_720w.webp)

![img](https://pic3.zhimg.com/80/v2-05209456dc3ba4146c931059366bcfbe_720w.webp)

通过上图可以看到，在锁释放的过程中，最核心的方法就是：

![img](https://pic3.zhimg.com/80/v2-ace0eb2174d7c0121c64d50f5fc244ea_720w.webp)

分析其lua脚本实现逻辑：

![img](https://pic2.zhimg.com/80/v2-ba5d908efff2a6113c3423acc13c5f15_720w.webp)

　　*分析可知，在删除对应的key之后，会发布一条消息以供其他未获取到锁的线程订阅，此逻辑和加锁过程遥相呼应，并且在删除key之后做了移除锁重入资格的操作，以保证当前线程彻底释放锁。*

## 8.易混淆概念

我们所说的一个redis实例，并不是一个Redis集群中的某一个master节点或者Slave节点，针对redis集群，一个集群在redLock算法中只是一个实例节点，至于我们的key值放在了哪个slot，是由Redis集群的一致性算法决定的。同样对于哨兵模式也是这样。所以针对RedLock算法来说，如果有N个实例，则是指N个cluster集群、N个sentinel集群、N个redis单实例节点。而不是一个集群中的N个实例。

原文链接：https://zhuanlan.zhihu.com/p/398304736

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.377】聊聊对不同I/O模型的理解 (阻塞/非阻塞IO，同步/异步IO)

## 1.关于I/O模型的问题

　　最近通过对ucore操作系统的学习，让我打开了操作系统内核这一黑盒子，与之前所学知识结合起来，解答了长久以来困扰我的关于I/O的一些问题。

**1. 为什么redis能以单工作线程处理高达几万的并发请求？**

　　**2. 什么是I/O多路复用？为什么redis、nginx、nodeJS以及netty等以高性能著称的服务器其底层都利用了I/O多路复用技术？**

　　**3. 非阻塞I/O为什么会流行起来，在许多场景下取代了传统的阻塞I/O？**

　　**4. 非阻塞I/O真的是银弹吗？为什么即使在为海量用户提供服务的，追求高性能的互联网公司中依然有那么多的服务器在传统的阻塞IO模型下工作？**

　　**5. 什么是协程？为什么Go语言这么受欢迎？**

　　在这篇博客中，将介绍不同层面、不同I/O模型的原理，并尝试着给出我对上述问题的回答。如果你也或多或少的对上述问题感到疑惑，希望这篇博客能为你提供帮助。

　　I/O模型和硬件、操作系统内核息息相关，博客中会涉及到诸如**保护模式、中断、特权级、进程/线程、上下文切换、系统调用**等关于操作系统、硬件相关的概念。由于计算机中的知识是按照层次组织起来的，如果对这些相对底层的概念不是很了解的话可能会影响对整体内容的理解。

## 2.硬件I/O模型

　　**软件的功能总是构建在硬件上的，计算机中的I/O本质上是CPU/内存与外设(网卡、磁盘等)进行数据的单向或双向传输。**

　　**从外设读入数据到CPU/内存称作Input输入，从CPU/内存中写出数据到外设称作Output输出。**

　　要想理解软件层次上的不同I/O模型，必须先对其基于的硬件I/O模型有一个基本的认识。硬件I/O模型大致可以分为三种：**程序控制I/O、中断驱动I/O、使用DMA的I/O**。

### 2.1.程序控制I/O：

　　程序控制I/O模型中，通过指令控制CPU不断的轮询外设是否就绪，当硬件就绪时一点一点的反复读/写数据。

　　**从CPU的角度来说，程序控制I/O模型是同步、阻塞的(同步指的是I/O操作依然是处于程序指令控制，由CPU主导的；阻塞指的是在发起I/O后CPU必须持续轮询完成状态，无法执行别的指令)。**

**程序控制I/O的优点：**

　　**硬件结构简单，编写对应程序也简单。**

**程序控制I/O的缺点：**

　　**十分消耗CPU，持续的轮训令宝贵的CPU资源无谓的浪费在了等待I/O完成的过程中，导致CPU利用率不高。**

### 2.2.中断驱动I/O：

为了解决上述程序控制I/O模型对CPU资源利用率不高的问题，计算机硬件的设计者令CPU拥有了处理中断的功能。

　　在中断驱动I/O模型中，CPU发起对外设的I/O请求后，就直接去执行别的指令了。当硬件处理完I/O请求后，通过中断异步的通知CPU。接到读取完成中断通知后，CPU负责将数据从外设缓冲区中写入内存；接到写出完成中断通知后，CPU需要将内存中后续的数据接着写出交给外设处理。

　　**从CPU的角度来说，中断驱动I/O模型是同步、非阻塞的(同步指的是I/O操作依然是处于程序指令控制，由CPU主导的；非阻塞指的是在发起I/O后CPU不会停下等待，而是可以执行别的指令)。**

**中断驱动I/O的优点：**

　　**由于I/O总是相对耗时的，比起通过程序控制I/O模型下CPU不停的轮训。在等待硬件I/O完成的过程中CPU可以解放出来执行另外的命令，大大提高了I/O密集程序的CPU利用率。**

**中断驱动I/O的缺点：**

　　**受制于硬件缓冲区的大小，一次硬件I/O可以处理的数据是相对有限的。在处理一次大数据的I/O请求中，CPU需要被反复的中断，而处理读写中断事件本身也是有一定开销的。**

### 2.3.使用DMA的I/O：

为了解决中断驱动I/O模型中，大数据量的I/O传输使得CPU需要反复处理中断的缺陷，计算机硬件的设计者提出了基于DMA模式的I/O(**DMA Direct Memory Access 直接存储器访问**)。DMA也是一种处理器芯片，和CPU一样也可以访问内存和外设，但DMA芯片是被设计来专门处理I/O数据传输的，因此其成本相对CPU较低。

　　在使用DMA的I/O模型中，CPU与DMA芯片交互，指定需要读/写的数据块大小和需要进行I/O数据的目的内存地址后，便异步的处理别的指令了。由DMA与外设硬件进行交互，一次大数据量的I/O需要DMA反复的与外设进行交互，当DMA完成了整体数据块的I/O后(**完整的将数据读入到内存或是完整的将某一内存块的数据写出到外设**)，再发起DMA中断通知CPU。

　　**从CPU的角度来说，使用DMA的I/O模型是异步、非阻塞的(异步指的是整个I/O操作并不是由CPU主导，而是由DMA芯片与外设交互完成的；非阻塞指的是在发起I/O后CPU不会停下等待，而是可以执行别的指令)。**

**使用DMA的I/O优点：**

　　**比起外设硬件中断通知，对于一次完整的大数据内存与外设间的I/O，CPU只需要处理一次中断。CPU的利用效率相对来说是最高的。**

**使用DMA的I/O缺点：**

　　**1. 引入DMA芯片令硬件结构变复杂，成本较高。**

　　**2. 由于DMA芯片的引入，使得DMA和CPU并发的对内存进行操作，在拥有高速缓存的CPU中，引入了高速缓存与内存不一致的问题。**

　　总的来说，自DMA技术被发明以来，由于其极大减少了CPU在I/O时的性能损耗，已经成为了绝大多数通用计算机的硬件标配。随着技术的发展又出现了更先进的**通道I/O方式**，相当于并发的DMA，允许并发的处理涉及多个不同内存区域、外设硬件的I/O操作。

## 3.操作系统I/O模型

　　介绍完硬件的I/O模型后，下面介绍这篇博客的重点：操作系统I/O模型。

　　操作系统帮我们屏蔽了诸多硬件外设的差异，为应用程序的开发者提供了友好、统一的服务。为了避免应用程序破坏操作系统内核，CPU提供了保护模式机制，使得应用程序无法直接访问被操作系统管理起来的外设，而必须通过内核提供的**系统调用**间接的访问外设。关于操作系统I/O模型的讨论针对的就是应用程序与内核之间进行I/O交互的系统调用模型。

‘　　**操作系统内核提供的I/O模型大致可以分为几种：同步阻塞I/O、同步非阻塞I/O、同步I/O多路复用、异步非阻塞I/O（信号驱动I/O用的比较少，就不在这里展开了）。**

### 3.1.同步阻塞I/O(Blocking I/O BIO)

　　我们已经知道，高效的硬件层面I/O模型对于CPU来说是异步的，但应用程序开发者总是希望在执行完I/O系统调用后能同步的返回，线性的执行后续逻辑(例如当磁盘读取的系统调用返回后，下一行代码中就能直接访问到所读出的数据)。但这与硬件层面耗时、异步的I/O模型相违背(程序控制I/O过于浪费CPU)，因此操作系统内核提供了基于同步、阻塞I/O的系统调用(BIO)来解决这一问题。

　　举个例子：当线程通过基于BIO的系统调用进行磁盘读取时，内核会令当前线程进入阻塞态，让出CPU资源给其它并发的就绪态线程，以便更有效率的利用CPU。当DMA完成读取，异步的I/O中断到来时，内核会找到先前被阻塞的对应线程，将其唤醒进入就绪态。当这个就绪态的线程被内核CPU调度器选中再度获得CPU时，便能从对应的缓冲区结构中得到读取到的磁盘数据，程序同步的执行流便能顺利的向下执行了。(感觉好像线程卡在了那里不动，过了一会才执行下一行，且指定的缓冲区中已经有了所需的数据)

**下面的伪代码示例中参考linux的设计，将不同的外设统一抽象为文件，通过文件描述符(file descriptor)来统一的访问。**

**BIO伪代码实例 :**

```
// 创建TCP套接字并绑定端口8888，进行服务监听
listenfd = serverSocket(8888,"tcp");
while(true){
    // accept同步阻塞调用
    newfd = accept(listenfd);
    // read会阻塞，因此使用线程异步处理，避免阻塞accpet(一般使用线程池)
    new thread(()->{
        // 同步阻塞读取数据
        xxx = read(newfd);
        ... dosomething
        // 关闭连接
        close(newfd);
    });
}
```

**BIO模型的优点：**

　　**BIO的I/O模型由于同步、阻塞的特性，屏蔽了底层实质上异步的硬件交互方式，令程序员可以编写出简单易懂的线性程序逻辑。**

**BIO模型的缺点：**

　　**1. BIO的同步、阻塞特性在简单易用的同时，也存在一些性能上的缺陷。由于BIO在等待I/O完成的时间中，线程虽然被阻塞不消耗CPU，但内核维护一个系统级线程本身也有一定的开销(维护线程控制块、内核线程栈空间等等)。**

　　**2. 不同线程在调度时的上下文切换CPU开销较大，在如今大量用户、高并发的互联网时代越来越成为web服务器性能的瓶颈。线程上下文切换本身需要需要保存、恢复现场，同时还会清空CPU指令流水线，以及令高速缓存大量失效。对于一个web服务器，如果使用BIO模型，服务器将至少需要1:1的维护同等数量的系统级线程(内核线程)，由于持续并发的网络数据交互，导致不同线程由于网络I/O的完成事件被内核反复的调度。**

　　**在著名的C10K问题的语境下，一台服务器需要同时维护1W个并发的tcp连接和对等的1W个系统级线程。量变引起质变，1W个系统级线程调度引起的上下文切换和100个系统级线程的调度开销完全不同，其将耗尽CPU资源，令整个系统卡死，崩溃。**

**BIO交互流程示意图：**

![img](https://pic4.zhimg.com/80/v2-2a02edff3c01c72425faed9e35524d73_720w.webp)

### 3.2.同步非阻塞I/O(NonBlocking I/O NIO)

　　BIO模型简单易用，但其阻塞内核线程的特性使得其已经不适用于需要处理大量(1K以上)并发网络连接场景的web服务器了。为此，操作系统内核提供了非阻塞特性的I/O系统调用，即**NIO(NonBlocking-IO)**。

　　针对BIO模型的缺陷，NIO模型的系统调用不会阻塞当前调用线程。但由于I/O本质上的耗时特性，无法立即得到I/O处理的结果，NIO的系统调用在I/O未完成时会返回特定标识，代表对应的I/O事件还未完成。因此需要应用程序按照一定的频率反复调用，以获取最新的IO状态。

**NIO伪代码实例 :**

```
// 创建TCP套接字并绑定端口8888，进行服务监听
listenfd = serverSocket(8888,"tcp");
clientFdSet = empty_set();
while(true){ // 开启事件监听循环
    // accept同步非阻塞调用,判断是否接收了新的连接
    newfd = acceptNonBlock(listenfd);
    if(newfd != EMPTY){
        // 如果存在新连接将其加入监听连接集合
        clientFdSet.add(newfd);
    }
    // 申请一个1024字节的缓冲区
    buffer = new buffer(1024);
    for(clientfd in clientFdSet){
        // 非阻塞read读
        num = readNonBlock(clientfd,buffer);
        if(num > 0){
            // 读缓冲区存在数据
            data = buffer;
            ... dosomething
            if(needClose(data)){
                // 关闭连接时，移除当前监听的连接
                clientFdSet.remove(clientfd)；
            }
        }
        ... dosomething
        // 清空buffer
        buffer.clear();
    }
}
```

**NIO模型的优点：**

　　**NIO因为其非阻塞的特性，使得一个线程可以处理多个并发的网络I/O连接。在C10K问题的语境下，理论上可以通过一个线程处理这1W个并发连接(对于多核CPU，可以创建多个线程在每个CPU核心中分摊负载，提高性能)。**

**NIO模型的缺点：**

　　**NIO克服了BIO在高并发条件下的缺陷，但原始的NIO系统调用依然有着一定的性能问题。在上述伪代码示例中，每个文件描述符对应的I/O状态查询，都必须通过一次NIO系统调用才能完成。**

　　**由于操作系统内核利用CPU提供的保护模式机制，使内核运行在高特权级，而令用户程序运行在执行、访问受限的低特权级。这样设计的一个好处就是使得应用程序无法直接的访问硬件，而必须由操作系统提供的系统调用间接的访问硬件(网卡、磁盘甚至电源等)。执行系统调用时，需要令应用线程通过系统调用陷入内核(即提高应用程序的当前特权级CPL，使其能够访问受保护的硬件)，并在系统调用返回时恢复为低特权级，这样一个过程在硬件上是通过中断实现的。**

　　**通过中断实现系统调用的效率远低于应用程序本地的函数调用，因此原始的NIO模式下通过系统调用循环访问每个文件描述符I/O就绪状态的方式是低效的。**

**NIO交互流程示意图：**

![img](https://pic4.zhimg.com/80/v2-d7f7d3ec7878df7d53fab464d2a737d3_720w.webp)

### 3.3.同步I/O多路复用(I/O Multiplexing)

　　为了解决上述NIO模型的系统调用中，一次事件循环遍历进行N次系统调用的缺陷。操作系统内核在NIO系统调用的基础上提供了I/O多路复用模型的系统调用。

　　**I/O多路复用相对于NIO模型的一个优化便是允许在一次I/O状态查询的系统调用中，一次传递复数个文件描述符进行批量的I/O状态查询。在一次事件循环中只需要进行一次I/O多路复用的系统调用就能得到所传递文件描述符集合的I/O状态，减少了原始NIO模型中不必要的系统调用开销。**

　　**多路复用I/O模型大致可以分为三种实现(虽然不同操作系统在最终实现上略有不同，但原理是类似的，示例代码以linux内核举例)：select、poll、epoll。**

### 3.4.select多路复用器介绍

　　select I/O多路复用器允许应用程序传递需要监听事件变化的文件描述符集合，监听其读/写，接受连接等I/O事件的状态。

　　select系统调用本身是**同步、阻塞**的，当所传递的文件描述符集合中都没有就绪的I/O事件时，执行select系统调用的线程将会进入阻塞态，直到至少一个文件描述符对应的I/O事件就绪，则唤醒被select阻塞的线程(**可以指定超时时间来强制唤醒并返回**)。唤醒后获得CPU的线程在select系统调用返回后可以遍历所传入的文件描述符集合，处理完成了I/O事件的文件描述符。

**select伪代码示例：**

```
// 创建TCP套接字并绑定端口8888，进行服务监听
listenfd = serverSocket(8888,"tcp");
fdNum = 1;
clientFdSet = empty_set();
clientFdSet.add(listenfd);
while(true){ // 开启事件监听循环
    // man 2 select(查看linux系统文档)
    // int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
    // 参数nfds：一共需要监听的readfds、writefds、exceptfds中文件描述符个数+1
    // 参数readfds/writefds/exceptfds： 需要监听读、写、异常事件的文件描述符集合
    // 参数timeout：select是同步阻塞的，当timeout时间内都没有任何I/O事件就绪，则调用线程被唤醒并返回(ret=0)
    //         timeout为null代表永久阻塞，直到有I/O事件完成
    // 返回值ret：
    //  1.返回大于0的整数，代表传入的readfds/writefds/exceptfds中共有ret个被激活(需要应用程序自己遍历)，
    //    2.返回0，在阻塞超时前没有任何I/O事件就绪
    //    3.返回-1，出现错误
    listenReadFd = clientFdSet;
    // select多路复用，一次传入需要监听事件的全量连接集合（超时时间1s）
    result = select(fdNum+1,listenReadFd,null,null,timeval("1s"));
    if(result > 0){
        // 如果服务器监听连接存在读事件
        if(IN_SET(listenfd,listenReadFd)){
            // 接收并建立连接
            newClientFd = accept(listenfd);
            // 加入客户端连接集合
            clientFdSet.add(newClientFd);
　　　　　　　fdNum++;
        }
        // 遍历整个需要监听的客户端连接集合
        for(clientFd : clientFdSet){
            // 如果当前客户端连接存在读事件
            if(IN_SET(clientFd,listenReadFd)){
                // 阻塞读取数据
                data = read(clientfd);
                ... dosomething
                if(needClose(data)){
                    // 关闭连接时，移除当前监听的连接
                    clientFdSet.remove(clientfd)；
　　　　　　　　　　　　fdNum--;
                }
            }
        }
    }
}
```

**select的优点：**

　　**1. select多路复用避免了上述原始NIO模型中无谓的多次查询I/O状态的系统调用，将其聚合成集合，批量的进行监听并返回结果集。**

　　**2. select实现相对简单，windows、linux等主流的操作系统都实现了select系统调用，跨平台的兼容性好。**

**select的缺点：**

　　**1. 在事件循环中，每次select系统调用都需要从用户态全量的传递所需要监听的文件描述符集合，并且select返回后还需要全量遍历之前传入的文件描述符集合的状态。**

　　**2. 出于性能的考量，内核设置了select所监听文件描述符集合元素的最大数量(一般为1024，可在内核启动时指定)，使得单次select所能监听的连接数受到了限制。**

　　**3. 抛开性能的考虑，从接口设计的角度来看，select将系统调用的参数与返回值混合到了一起(返回值覆盖了参数)，增加了使用者理解的困难度。**

**I/O多路复用交互示意图：**

![img](https://pic4.zhimg.com/80/v2-3885324b25c86b1aed94ab6c44d7f4a7_720w.webp)

### 3.5.poll多路复用器介绍

　　poll I/O多路复用器在使用上和select大同小异，也是通过传入指定的文件描述符集合以及指定内核监听对应文件描述符上的I/O事件集合，但在实现的细节上基于select做了一定的优化。

　　和select一样，poll系统调用在没有任何就绪事件发生时也是**同步、阻塞**的(**可以指定超时时间强制唤醒并返回**)，当返回后要判断是否有就绪事件时，也一样需要全量的遍历整个返回的文件描述符集合。

**poll伪代码示例：**

```
/*
// man 2 poll(查看linux系统文档)
// 和select不同将参数events和返回值revents分开了
struct pollfd {
               int   fd;         // file descriptor 对应的文件描述符 
               short events;     // requested events 需要监听的事件
               short revents;    // returned events 返回时，就绪的事件
           };
// 参数fds,要监听的poolfd数组集合
// 参数nfds，传入fds数组中需要监听的元素个数
// 参数timeout，阻塞的超时时间(传入-1代表永久阻塞，直到I/O事件完成)
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
//events/revents是位图表示的
//revents & POLLIN == 1 存在就绪的读事件
//revents & POLLOUT == 1 存在就绪的写事件
//revents & POLLHUP == 1 存在对端断开连接或是通信完成事件
*/
// 创建TCP套接字并绑定端口8888，进行服务监听
listenfd = serverSocket(8888,"tcp");
MAX_LISTEN_SIZE = 100;
struct pollfd fds[MAX_LISTEN_SIZE];
// 设置服务器监听套接字(监听读事件)
fds[0].fd = listenfd;
fds[0].events = POLLIN;
fds[0].revents = 0;
// 客户端连接数一开始为0
int clientCount = 0;
while(true){
    // poll同步阻塞调用(超时时间-1表示永久阻塞直到存在监听的就绪事件)
    int ret = poll(fds, clientCount + 1, -1);
    for (int i = 0; i < clientCount + 1; i++){
        if(fds[i].fd == listenfd && fds[i].revents & POLLIN){
            // 服务器监听套接字读事件就绪，建立新连接
            clientCount++；
            fds[clientCount].fd = conn;
            fds[clientCount].events = POLLIN | POLLRDHUP ;
            fds[clientCount].revents = 0;
        }else if(fds[i].revents & POLLIN){
            // 其他链接可读,进行读取
            read(fds[i].fd);
            ... doSomething
        }else if(fds[i].revents & POLLRDHUP){
            // 监听到客户端连接断开，移除该连接
            fds[i] = fds[clientCount];
            i--;
            clientCount--;
            // 关闭该连接
            close(fd);
        }
    }
}
```

**poll的优点：**

　　**1. poll解决了select系统调用受限于内核配置参数的限制问题，可以同时监听更多文件描述符的I/O状态(但不能超过内核限制当前进程所能拥有的最大文件描述符数目限制)。**

　　**2. 优化了接口设计，将参数与返回值进行了分离。**

**poll的缺点：**

　　**1. poll优化了select，但在处理大量闲置连接时，即使真正产生I/O就绪事件的活跃文件描述符数量很少，依然免不了线性的遍历整个监听的文件描述符集合。每次调用时，需要全量的将整个感兴趣的文件描述符集合从用户态复制到内核态。**

　　**2. 由于select/poll都需要全量的传递参数以及遍历返回值，因此其时间复杂度为O(n)，即处理的开销随着并发连接数n的增加而增加，而无论并发连接本身活跃与否。但一般情况下即使并发连接数很多，大量连接都产生I/O就绪事件的情况并不多，更多的情况是1W的并发连接，可能只有几百个是处于活跃状态的，这种情况下select/poll的性能并不理想，还存在优化的空间。**

### 3.6.epoll多路复用器：

　　epoll是linux系统中独有的，针对select/poll上述缺点进行改进的高性能I/O多路复用器。

**针对select/poll的第一个缺点：在每次事件循环时都需要从用户态全量传递整个需要监听的文件描述符集合**。

　　epoll在内核中分配内存空间用于缓存被监听的文件描述符集合。通过创建epoll的系统调用（epoll_create），在内核中维护了一个epoll结构，而在应用程序中只需要保留epoll结构的句柄就可对其进行访问(也是一个文件描述符)。可以动态的在epoll结构的内核空间中增加/删除/更新所要监听的文件描述符以及不同的监听事件（epoll_ctl），而不必每次都全量的传递需要监听的文件描述符集合。

　　**针对select/poll的第二个缺点：在系统调用返回后通过修改所监听文件描述符结构的状态，来标识文件描述符对应的I/O事件是否就绪。每次系统调用返回时，都需要全量的遍历整个监听文件描述符集合，而无论是否真的完成了I/O。**

epoll监听事件的系统调用完成后，只会将真正活跃的、完成了I/O事件的文件描述符返回，避免了全量的遍历。在并发的连接数很大，但闲置连接占比很高时，epoll的性能大大优于select/poll这两种I/O多路复用器。**epoll的时间复杂度为O(m)，即处理的开销不随着并发连接n的增加而增加，而是仅仅和监控的活跃连接m相关；在某些情况下n远大于m，epoll的时间复杂度甚至可以认为近似的达到了O(1)。**

**通过epoll_wait系统调用，监听参数中传入对应epoll结构中关联的所有文件描述符的对应I/O状态。epoll_wait本身是同步、阻塞的(可以指定超时时间强制唤醒并返回)，当epoll_wait同步返回时，会返回处于活跃状态的完成I/O事件的文件描述符集合，避免了select/poll中的无效遍历。同时epoll使用了mmap机制，将内核中的维护的就绪文件描述符集合所在空间映射到了用户态，令应用程序与内核共享epoll结构对应区域的内存，避免了epoll返回就绪文件描述符集合时的一次内存复制。**

**epoll伪代码示例：**

```
/**
    epoll比较复杂，使用时大致依赖三个系统调用 (man 7 epoll)
    1. epoll_create 创建一个epoll结构,返回对应epoll的文件描述符 (man 2 epoll_create)
        int epoll_create();
    2. epoll_ctl 控制某一epoll结构(epfd)，向其增加/删除/更新(op)某一其它连接(fd)，监控其I/O事件(event) (man 2 epoll_ctl)
        op有三种合法值：EPOLL_CTL_ADD代表新增、EPOLL_CTL_MOD代表更新、EPOLL_CTL_DEL代表删除
        int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
    3. epoll_wait 令某一epoll同步阻塞的开始监听(epfd)，感兴趣的I/O事件(events),所监听fd的最大个数(maxevents)，指定阻塞超时时间(timeout) (man 2 epoll_wait)
        int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);
*/
// 创建TCP套接字并绑定端口8888，进行服务监听
listenfd = serverSocket(8888,"tcp");
// 创建一个epoll结构
epollfd = epoll_create();
ev = new epoll_event();
ev.events = EPOLLIN; // 读事件
ev.data.fd = listenfd;
// 通过epoll监听服务器端口读事件(新连接建立请求)
epoll_ctl(epollfd,EPOLL_CTL_ADD,listenfd,ev)；
// 最大监听1000个连接
MAX_EVENTS = 1000;
listenEvents = new event[MAX_EVENTS];
while(true){
    // 同步阻塞监听事件
    // 最多返回MAX_EVENTS个事件响应结果
    // (超时时间1000ms，标识在超时时间内没有任何事件就绪则当前线程被唤醒，返回值nfd将为0)
    nfds = epoll_wait(epollfd, listenEvents, MAX_EVENTS, 1 * 1000);
    for(n = 0; n < nfds; ++n){
        if(events[n].data.fd == listenfd){
            // 当发现服务器监听套接字存在可读事件，建立新的套接字连接
            clientfd = accept(listenfd);
            ev.events = EPOLLIN | EPOLLET;
            ev.data.fd = clientfd;
            // 新建立的套接字连接也加入当前epoll的监听(监听读(EPOLLIN)/写(EPOLLET)事件)
            epoll_ctl(epollfd,EPOLL_CTL_ADD,clientfd,ev);
        } else{
            // 否则是其它连接的I/O事件就绪,进行对应的操作
            ... do_something
        }
    }
}
```

**epoll的优点：**

　　**epoll是目前性能最好的I/O多路复用器之一，具有I/O多路复用优点的情况下很好的解决了select/poll的缺陷。目前linux平台中，像nginx、redis、netty等高性能服务器都是首选epoll作为基础来实现网络I/O功能的。**

**epoll的缺点：**

**1. 常规情况下闲置连接占比很大，epoll的性能表现的很好。但是也有少部分场景中，绝大多数连接都是活跃的，那么其性能与select/poll这种基于位图、数组等简单结构的I/O多路复用器相比，就不那么有优势了。因为select/poll被诟病的一点就是通常情况下进行了无谓的全量检查，而当活跃连接数占比一直超过90%甚至更高时，就不再是浪费了;相反的，由于epoll内部结构比较复杂，在这种情况下其性能比select/poll还要低一点。**

　　**2. epoll是linux操作系统下独有的，使得基于epoll实现的应用程序的跨平台兼容性受到了一定影响。**

### 3.8.异步非阻塞I/O(Asynchronous I/O AIO)

windows和linux都支持了select系统调用，但linux内核在之后又实现了epoll这一更高性能的I/O多路复用器来改进select。

　　windows没有模仿linux，而是提供了被称为**IOCP**(**Input/Output Completion Port 输入输出完成端口**)的功能解决select性能的问题。IOCP采用异步非阻塞IO(AIO)的模型，其与epoll同步非阻塞IO的最大区别在于，epoll调用完成后，仅仅返回了就绪的文件描述符集合；而IOCP则在内核中自动的完成了epoll中原本应该由应用程序主动发起的I/O操作。

**举个例子，当监听到就绪事件开始读取某一网络连接的请求报文时，epoll依然需要通过程序主动的发起读取请求，将数据从内核中读入用户空间。而windows下的IOCP则是通过注册回调事件的方式工作，由内核自动的将数据放入指定的用户空间，当处理完毕后会调度激活注册的回调事件，被唤醒的线程能直接访问到所需要的数据。**

**这也是为什么BIO/NIO/IO多路复用被称为同步I/O，而IOCP被称为异步I/O的原因。**

　　**同步I/O与异步I/O的主要区别就在于站在应用程序的视角看，真正读取/写入数据时是否是由应用程序主导的。如果需要用户程序主动发起最终的I/O请求就被称为同步I/O；而如果是内核自动完成I/O后通知用户程序，则被称为异步I/O。(可以类比在前面硬件I/O模型中，站在CPU视角的同步、异步I/O模型，只不过这里CPU变成了应用程序，而外设/DMA变成了操作系统内核)**

**AIO的优点：**

　　**AIO作为异步I/O，由内核自动的完成了底层一整套的I/O操作，应用程序在事件回调通知中能直接获取到所需数据。内核中可以实现非常高效的调度、通知框架。拥有前面NIO高性能的优点，又简化了应用程序的开发。**

**AIO的缺点：**

　　**由内核全盘控制的全自动I/O虽然能够做到足够高效，但是在一些特定场景下性能并不一定能超过由应用程序主导的，经过深度优化的代码。像epoll在支持了和select/poll一样的水平触发I/O的同时，还支持了更加细致的边缘触发I/O，允许用户自主的决定当I/O就绪时，是否需要立即处理或是缓存起来等待稍后再处理。(就像java等支持自动内存垃圾回收的语言，即使其垃圾收集器经过持续的优化，在大多数情况下性能都很不错，但却依然无法达到和经过开发人员反复调优，手动回收内存的C、C++等语言实现的程序一样的性能)**

![img](https://pic3.zhimg.com/80/v2-84ad1c036576026b78bdfdd924d75252_720w.webp)

**（截图自《Unix网络编程 卷1》）**

### 3.9.操作系统I/O模型小结

　　**1. 同步I/O包括了同步阻塞I/O和同步非阻塞I/O，而异步I/O中由于异步阻塞I/O模型没有太大价值，因此提到异步I/O(AIO)时，默认指的就是异步非阻塞I/O。**

![img](https://pic1.zhimg.com/80/v2-907336a77288cdd739329a23df0d1408_720w.webp)

**2. 在I/O多路复用器的工作中，当监听到对应文件描述符I/O事件就绪时，后续进行的读/写操作既可以是阻塞的，也可以是非阻塞的。如果是都以阻塞的方式进行读/写，虽然实现简单，但如果某一文件描述符需要读写的数据量很大时将耗时较多，可能会导致事件循环中的其它事件得不到及时处理。因此截图中的阻塞读写数据部分并不准确，需要辩证的看待。**

## 4.非阻塞I/O是银弹吗？

计算机技术的发展看似日新月异，但本质上有两类目标指引着其前进。一是尽可能的增强、压榨硬件的性能，提高机器效率；二是尽可能的通过持续的抽象、封装简化软件复杂度，提高程序员的开发效率。计算机软件的发展方向必须至少满足其中的一种目标。

　　从上面关于操作系统内核I/O模型的发展中可以看到，最初被广泛使用的是易理解、开发简单的BIO模型；但由于互联网时代的到来，web服务器系统面临着C10K问题，需要能支持海量的并发客户端连接，因此出现了包括NIO、I/O多路复用、AIO等技术，利用一个内核线程管理成百上千的并发连接，来解决BIO模型中一个内核线程对应一个网络连接的工作模式中，由于处理大量连接导致内核线程上下文频繁切换，造成CPU资源耗尽的问题。上述的第一条原则指引着内核I/O模型的发展，使得web服务器能够获得更大的连接服务吞吐量，提高了机器效率。

**但非阻塞I/O真的是完美无缺的吗？**

**有着非阻塞I/O模型开发经验的程序员都知道，正是由于一个内核线程管理着成百上千个客户端连接，因此在整个线程的执行流中不能出现耗时、阻塞的操作(比如同步阻塞的数据库查询、rpc接口调用等)。如果这种操作不可避免，则需要单独使用另外的线程异步的处理，而不能阻塞当前的整个事件循环，否则将会导致其它连接的请求得不到及时的处理，造成饥饿。**

　　**对于多数互联网分布式架构下处理业务逻辑的应用程序服务器来说，在一个网络请求服务中，可能需要频繁的访问数据库或者通过网络远程调用其它服务的接口。如果使用的是基于NIO模型进行工作的话，则要求rpc库以及数据库、中间件等连接的库是支持异步非阻塞的。如果由于同步阻塞库的存在，在每次接受连接进行服务时依然被迫通过另外的线程处理以避免阻塞，则NIO服务器的性能将退化到和使用传统的BIO模型一样的地步。**

　　所幸的是随着非阻塞I/O的逐渐流行，上述问题得到了很大的改善，越来越的框架/库都提供了异步非阻塞的api接口。

### 4.1.非阻塞I/O带来的新问题

　　异步非阻塞库改变了同步阻塞库下程序员习以为常的，线性的思维方式，在编码时被迫的以事件驱动的方式思考。逻辑上连贯的业务代码为了适应异步非阻塞的库程序，被迫分隔成多个独立片段嵌套在各个不同层次的回调函数中。对于复杂的业务而言，很容易出现嵌套为一层层的回调函数调用链，形成臭名昭著的**callback hell(回调地狱)**。

　　最早被callback hell折磨的可能是客户端程序的开发人员，因为客户端程序需要时刻监听着用户操作事件的产生，通常以基于事件驱动的方式组织异步处理代码。

**callback hell伪代码示例：**

```
// 由于互相之间有前后的数据依赖，按照顺序异步的调用A、B、C、D
A.dosomething((res)->{
    data = res.xxx;
    B.dosomething(data,(res)->{
        data = res.xxx;
        C.dosomething(data,(res)->{
            data = res.xxx
            D.dosomething(data,(res)->{
                // 。。。 有依赖的同步业务越复杂，层次越深，就像一个无底洞
            })
        })
    })
})
```

　　异步非阻塞库的使用割裂了代码的连贯结构，使得程序变得难以理解、调试，这一缺陷在堆积着复杂晦涩业务逻辑的web应用程序服务器程序中显得难以忍受。这也是为什么如今web服务器仍然有很大一部分依然使用传统的同步阻塞的BIO模型进行开发的主要原因。通过分布式、集群的方式分摊大量并发的连接，而只在业务相对简单的API网关、消息队列等I/O密集型的中间件程序中NIO才被广泛使用(实在不行，业务服务器集群可以加机器，保证开发效率也同样重要)。

　　**那么就没有什么办法既能够拥有非阻塞I/O支撑海量并发、高吞吐量的性能优势；又能够令程序员以同步方式思考、编写程序，以提高开发效率吗？**

解决办法当然是存在的，且相关技术依然在不断发展。上述计算机技术发展的第二个原则指导着这些技术发展，目的是为了简化代码复杂性，提高程序员的效率。

### 4.2.优化语法、语言库以简化异步编程的难度

　　在函数式编程的领域，就一直有着诸多晦涩的“黑科技”(CPS变换、monad等)，能够简化callback hell，使得可以以几乎是同步的方式编写实质上是异步执行的代码。例如EcmaScript便在EcmaScript6、EcmaScript7中分别引入了promise和async/await来解决这一问题。

### 4.3.在语言级别支持用户级线程(协程)

　　前面提到，传统的基于BIO模型的工作模式最大的优点在于可以同步的编写代码，遇到需要等待的耗时操作时能够被阻塞，使用起来简单易懂。但由于1：1的维护内核线程在处理海量连接时由于频繁的内核线程上下文切换而力不从心，催生了非阻塞I/O。

　　而由于上述非阻塞I/O引起的代码复杂度增加的问题，计算机科学家们想到了很早之前就在操作系统概念中提出，但一直没有被广泛使用的另一种线程实现方式：用户级线程。

**用户级线程顾名思义，就是在用户级实现的线程，操作系统内核对其是无感知的。用户级线程在许多方面与大家所熟知的内核级线程相似，都有着自己独立的执行流，和进程中的其它线程共享内存空间。**

　　**用户级线程与内核级线程最大的一个区别就在于由于操作系统对其无感知，因此无法对用户级线程进行基于中断的抢占式调度。要使得同一进程下的不同用户级线程能够协调工作，必须小心的编写执行逻辑，以互相之间主动让渡CPU的形式工作，否则将会导致一个用户级线程持续不断的占用CPU，而令其它用户级线程处于饥饿状态，因此用户级线程也被称为协程，即互相协作的线程。**

　　**用户级线程无论如何是基于至少一个内核线程/进程的，多个用户级线程可以挂载在一个内核线程/进程中被内核统一的调度管理。**

![img](https://pic1.zhimg.com/80/v2-8bec2d1647126b7e04d49cd1db7c10e4_720w.webp)

**（截图自《现代操作系统》）**

**协程可以在遇到I/O等耗时操作时选择主动的让出CPU，以实现同步阻塞的效果，令程序执行流转移到另一个协程中。由于多个协程可以复用一个内核线程，每个协程所占用的开销相对内核级线程来说非常小；且协程上下文切换时由于不需要陷入内核，其切换效率也远比内核线程的上下文切换高(开销近似于一个函数调用)。**

　　最近很流行的Go语言就是由于其支持语言层面的协程而备受推崇。程序员可以利用一些语言层面提供的协程机制编写高效的web服务器程序(例如在语句中添加控制协程同步的关键字)。通过在编译后的最终代码中加入对应的协程调度指令，由协程调度器接手，控制协程同步时在耗时I/O操作发生时主动的让出CPU，并在处理完毕后能被调度回来接着执行。Go语言通过语言层面上对协程的支持，降低了编写正确、协调工作的协程代码的难度。

　　Go编写的高性能web服务器如果运行在多核CPU的linux操作系统中，一般会创建m个内核线程和n个协程(m正比与CPU核心数，n远大于m且正比于并发连接数)，底层每个内核线程依然可以利用epoll IO多路复用器处理并发的网络连接，并将业务逻辑处理的任务转交给用户态的协程(gorountine)。每个协程可以在不同的内核线程(CPU核心)中被来回调度，以获得最大的CPU吞吐量。

　　**使用协程，程序员在开发时能够编写同步阻塞的耗时I/O代码，又不用担心高并发情况下BIO模型中的性能问题。可以说协程兼顾了程序开发效率与机器执行效率，因此越来越多的语言也在语言层面或是在函数库中提供协程机制。顺便一提，上述js中的async/await机制其底层也是协程实现的。**

### 4.4. 实现用户透明的协程

　　在通过虚拟机作为中间媒介，操作系统平台无关的语言中(比如java)，虚拟机作为应用程序与操作系统内核的中间层，可以对应用程序进行各方面的优化，令程序员可以轻松编写出高效的代码。

　　有大牛在知乎的一篇回答中提到过，其曾经领导团队在阿里巴巴工作时在java中实现了透明的协程。但似乎没有和官方标准达成统一因此并没有对外开放。

　　如果能够在虚拟机中提供高效、用户透明的协程机制，使得原本基于BIO多线程的服务器程序无需改造便自动的获得了支持海量并发的能力，那真是太强了Orz。

## 5.总结

　　通过对ucore操作系统源码级的研究学习，加深了我对操作系统原理书中各种抽象概念的理解，也渐渐理解了一些关于各种I/O模型的问题。

　　一方面，通过对操作系统I/O模型的总结，使得我对于上层应用程序如java中的nio和netty中的非阻塞的编程风格有了更深的理解，不再像之前只习惯于BIO编程那样感到奇怪，而是觉得非常自然。另一方面，又意识到了自己还有太多的不足。

　　站在操作系统I/O模型这一层面，向上看，依然对基于nio的各种中间件不太熟悉，不了解在具体实践中如何利用好NIO这一利器，写出鲁棒、高效的代码；向下看，由于ucore为了尽可能的简化实验课的难度，省略了很多的功能没有实现，导致我对于操作系统底层是如何实现网络协议栈、如何实现nio和io多路复用器的原理知之甚少，暂时只能将其当作黑盒子看待，很多地方可能理解的有偏差。令我在拓宽知识面的同时，感叹知道的越多就越感觉自己无知。但人总是要向前走的，在学习中希望尽量能做到知其然而知其所以然。通过对ucore操作系统的学习，使得我对于操作系统内核的学习不再感到恐惧，在认知学习概念中就是从恐惧区转为了学习区。以后有机会的话，可以通过研究早期的linux内核源码来解答我关于I/O模型底层实现的一系列问题。

　　这篇博客是这一段时间来对操作系统学习的一个阶段性总结，直接或间接的回答了博客开头的几个问题，希望能帮到对操作系统、I/O模型感兴趣的人。这篇文章中还存在许多理解不到位的地方，请多多指教。

原文链接：https://zhuanlan.zhihu.com/p/401867380

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.378】Redis与Memcache对比

## **1.文章简介：**

1、 Redis和Memcache都是将数据存放在内存中，都是内存数据库。不过memcache还可用于缓存其他东西，例如图片、视频等等。
2、Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，hash等数据结构的存储。
3、虚拟内存–Redis当物理内存用完时，可以将一些很久没用到的value 交换到磁盘
4、过期策略–memcache在set时就指定，例如set key1 0 0 8,即永不过期。Redis可以通过例如expire 设定，例如expire name 10
5、分布式–设定memcache集群，利用magent做一主多从;redis可以做一主多从。都可以一主一从
6、存储数据安全–memcache挂掉后，数据没了；redis可以定期保存到磁盘（持久化）
7、灾难恢复–memcache挂掉后，数据不可恢复; redis数据丢失后可以通过aof恢复
8、Redis支持数据的备份，即master-slave模式的数据备份。

## 2.关于redis和memcache的不同，下面罗列了一些相关说法，供记录：

redis和memecache的不同在于[2]：
1、存储方式：
memecache 把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小
redis有部份存在硬盘上，这样能保证数据的持久性，支持数据的持久化（笔者注：有快照和AOF日志两种持久化方式，在实际应用的时候，要特别注意配置文件快照参数，要不就很有可能服务器频繁满载做dump）。
2、数据支持类型：
redis在数据支持上要比memecache多的多。
3、使用底层模型不同：
新版本的redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。
4、运行环境不同：
redis目前官方只支持LINUX 上去行，从而省去了对于其它系统的支持，这样的话可以更好的把精力用于本系统 环境上的优化，虽然后来微软有一个小组为其写了补丁。但是没有放到主干上

个人总结一下，有持久化需求或者对数据结构和处理有高级要求的应用，选择redis，其他简单的key/value存储，选择memcache。

## **3.下面重点分析Memcached和Redis两种方案：**

Memcached介绍
Memcached 是一个高性能的分布式内存对象缓存系统，用于动态Web应用以减轻数据库负载。它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提供动态、数据库驱动网站的速度，现在已被LiveJournal、hatena、Facebook、Vox、LiveJournal等公司所使用。

Memcached工作方式分析
许多Web应用都将数据保存到 RDBMS中，应用服务器从中读取数据并在浏览器中显示。 但随着数据量的增大、访问的集中，就会出现RDBMS的负担加重、数据库响应恶化、 网站显示延迟等重大影响。Memcached是高性能的分布式内存缓存服务器,通过缓存数据库查询结果，减少数据库访问次数，以提高动态Web等应用的速度、 提高可扩展性。下图展示了memcache与数据库端协同工作情况：

![img](https://pic4.zhimg.com/80/v2-87a2993f8be6061085cde91baa8db493_720w.webp)

其中的过程是这样的：
1.检查用户请求的数据是缓存中是否有存在，如果有存在的话，只需要直接把请求的数据返回，无需查询数据库。
2.如果请求的数据在缓存中找不到，这时候再去查询数据库。返回请求数据的同时，把数据存储到缓存中一份。
3.保持缓存的“新鲜性”，每当数据发生变化的时候（比如，数据有被修改，或被删除的情况下），要同步的更新缓存信息，确保用户不会在缓存取到旧的数据。

## 4.Memcached作为高速运行的分布式缓存服务器，具有以下的特点：

1.协议简单
2.基于libevent的事件处理
3.内置内存存储方式
4.memcached不互相通信的分布式

## 5.如何实现分布式可拓展性？

Memcached的分布式不是在服务器端实现的，而是在客户端应用中实现的，即通过内置算法制定目标数据的节点，如下图所示：

![img](https://pic1.zhimg.com/80/v2-ff54837916fd9245c5dcc3dc5eff36d8_720w.webp)

## 6.Redis 介绍

Redis是一个key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、 list(链表)、set(集合)和zset(有序集合)。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步,当前 Redis的应用已经非常广泛，国内像新浪、淘宝，国外像 Flickr、Github等均在使用Redis的缓存服务。

## 7.Redis 工作方式分析

Redis作为一个高性能的key-value数据库具有以下特征：
1.多样的数据模型
2.持久化
3.主从同步
Redis支持丰富的数据类型，最为常用的数据类型主要由五种：String、Hash、List、Set和Sorted Set。Redis通常将数据存储于内存中，或被配置为使用虚拟内存。Redis有一个很重要的特点就是它可以实现持久化数据，通过两种方式可以实现数据持久化：使用RDB快照的方式，将内存中的数据不断写入磁盘；或使用类似MySQL的AOF日志方式，记录每次更新的日志。前者性能较高，但是可能会引起一定程度的数据丢失；后者相反。 Redis支持将数据同步到多台从数据库上，这种特性对提高读取性能非常有益。

## 8.Redis如何实现分布式可拓展性？

2.8以前的版本：与Memcached一致，可以在客户端实现，也可以使用代理，twitter已开发出用于Redis和Memcached的代理Twemproxy 。
3.0 以后的版本：相较于Memcached只能采用客户端实现分布式存储，Redis则在服务器端构建分布式存储。Redis Cluster是一个实现了分布式且允许单点故障的Redis高级版本，它没有中心节点，各个节点地位一致，具有线性可伸缩的功能。如图给出Redis Cluster的分布式存储架构，其中节点与节点之间通过二进制协议进行通信，节点与客户端之间通过ascii协议进行通信。在数据的放置策略上，Redis Cluster将整个 key的数值域分成16384个哈希槽，每个节点上可以存储一个或多个哈希槽，也就是说当前Redis Cluster支持的最大节点数就是16384。

![img](https://pic4.zhimg.com/80/v2-d961c47244df61d1babcaca7c3c87ff7_720w.webp)

## **9.综合结论**

应该说Memcached和Redis都能很好的满足解决我们的问题，它们性能都很高，总的来说，可以把Redis理解为是对Memcached的拓展，是更加重量级的实现，提供了更多更强大的功能。具体来说：

1.性能上：
性能上都很出色，具体到细节，由于Redis只使用单核，而Memcached可以使用多核，所以平均每一个核上Redis在存储小数据时比
Memcached性能更高。而在100k以上的数据中，Memcached性能要高于Redis，虽然Redis最近也在存储大数据的性能上进行优化，但是比起 Memcached，还是稍有逊色。

2.内存空间和数据量大小：
MemCached可以修改最大内存，采用LRU算法。Redis增加了VM的特性，突破了物理内存的限制。

3.操作便利上：
MemCached数据结构单一，仅用来缓存数据，而Redis支持更加丰富的数据类型，也可以在服务器端直接对数据进行丰富的操作,这样可以减少网络IO次数和数据体积。

4.可靠性上：
MemCached不支持数据持久化，断电或重启后数据消失，但其稳定性是有保证的。Redis支持数据持久化和数据恢复，允许单点故障，但是同时也会付出性能的代价。

5.应用场景：
Memcached：动态系统中减轻数据库负载，提升性能；做缓存，适合多读少写，大数据量的情况（如人人网大量查询用户信息、好友信息、文章信息等）。
Redis：适用于对读写效率要求都很高，数据处理业务复杂和对安全性要求较高的系统（如新浪微博的计数和微博发布部分系统，对数据安全性、读写要求都很高）。

## **10.需要慎重考虑的部分**

1.Memcached单个key-value大小有限，一个value最大只支持1MB，而Redis最大支持512MB
2.Memcached只是个内存缓存，对可靠性无要求；而Redis更倾向于内存数据库，因此对对可靠性方面要求比较高
3.从本质上讲，Memcached只是一个单一key-value内存Cache；而Redis则是一个数据结构内存数据库，支持五种数据类型，因此Redis除单纯缓存作用外，还可以处理一些简单的逻辑运算，Redis不仅可以缓存，而且还可以作为数据库用
4.新版本（3.0）的Redis是指集群分布式，也就是说集群本身均衡客户端请求，各个节点可以交流，可拓展行、可维护性更强大。

原文链接：https://zhuanlan.zhihu.com/p/407429579

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.379】Nginx高效的原因，你都了解了吗

**导读**

研发朋友们对应用过载就扩容这个动作已是非常的熟悉，但大家有听说过业务流量突增，nginx也随之需要扩容嘛？我相信很少听过，为什么呢？主要是nginx高效工作起作用，那接下来我们来讲讲我们nginx为何高效。

## 1.Nginx 为何神物

Nginx 是一种异步框架的web服务器，它可以用作反向代理、负载均衡器和 HTTP 缓存或邮件服务器。Nginx 是免费且开源的，根据类 BSD 许可证的条款发布，意味任何人都可以下载甚至修改源码。Nginx 软件由伊戈尔·赛索耶夫创建并于 2004 年首次公开发布。2011 年成立同名公司以提供支持，现大部分互联网公司都在使用。

## 2.Nginx 为何高效

一个高性能web服务器典型特点是处理速度快且消耗资源少.尤其在上万连接同时在线的时候.若要做到处理速度快,并发模型的设计尤其关键.服务器并发量取决于两个因素:一是服务器连接的进程数量, 二是每个进程可同时处理的并发请求数量,因而web服务器并发模型由两部分构成,服务的提供方式和连接处理机制, 这两种别具一格的方式使得Nginx在同类型的web服务器中表现优秀。

### 2.1. 服务提供方式

一般Web服务器并发处理请求有以下三种方式：多进程方式、多线程方式、异步方式。

**多进程** 需要内存复制等额外开销,客户端较多时候，服务器性能会降低，典型应用如Apache的prefork模块；

**多线程** 使用进程中多个线程提供服务，多线程开销较小,典型应用如Apache的worker模块；

**异步** 则采用非阻塞的方式与每个客户通信，服务器用一个进程进行轮询，典型应用如Nginx的work process进程；

其中效率最高是异步，最稳定是多进程、占用资源少是多线程。

|    类型    |                             优点                             |                             缺点                             |
| :--------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 多进程方式 | 稳定性高：由于采用独立进程处理独立请求，而进程之间是独立的，单个进程问题不会影响其他进程，因此稳定性最好 | 资源占用率高：当请求过大时，需要大量的进程处理请求，进程生成、切换开销很大，而且进程间资源是独立的，造成内存重复利用。 |
| 多线程方式 | 开销较小：线程间部分数据是共享的，且线程生成与线程间的切换所需资源开销比进程间切换小得多 | 线程切换过快可能造成线程抖动，且线程过多会造成服务器不稳定。 |
|  异步方式  | 性能最好：一个进程或线程处理多个请求，不需要额外开销，性能最好，资源占用最低。 | 某个进程或线程出错，可能导致大量请求无法处理，甚至导致整个服务宕机。 |

Nginx基于事件模型（异步方式）提供服务，更适合每秒连接数和请求数同时非线性增长的情况。即使负载增加了，内存和CPU使用事件数量始终保持可预期。Nginx使用普通的硬件就能在一个服务器上处理数万的并发连接。这就是Nginx高性能的第一个原因。

### 2.2.连接处理机制

谈到连接处理机制，关键是LINUX的I/O模型，同步阻塞io，同步非阻塞io，异步阻塞io（io复用），异步非阻塞io（信号驱动或者异步io）

**阻塞和非阻塞：**

那什么是阻塞和非阻塞呢,这里用点菜传菜举例来说明阻塞和非阻塞：
第一种：就在出菜窗口等待，直到厨师炒完菜后将菜送到窗口，然后服务员再将菜送到用户手中；
第二种：等一会再到窗口来问厨师，某个菜好了没？如果没有先处理其他事情，等会再去问一次；

第一种为阻塞方式，第二种为非阻塞的。从上面看，明显第二种非阻塞的效率更高；

**同步和异步：**

那什么是同步和异步呢，这里用传菜来举例下同步和异步 ：

同步：客户点菜，服务员直接跟厨师打交道，菜出来没出来，服务员直接指导，但只有当厨师将菜送到服务员手上，这个过程才算正常完成，这就是同步的事件。
异步：同样是点菜，有些餐馆有专门的传菜人员，当厨师炒好菜后，传菜员将菜送到传菜窗口，并通知服务员，这就变成异步的了。

对于同步的事件，你只能以阻塞的方式去做。而对于异步的事件，阻塞和非阻塞都是可以的，

传统的服务器采用的就是同步阻塞的多进程模型，而Nginx服务器使用多进程机制能够保证不增长对系统资源的压力,同时使用异步非阻塞方式减少了工作进程在I/O调用上的阻塞延迟，保证了不降低对请求的处理能力，这就是高效原因之一。

**I/O多路复用：**

select 和 poll 一般所有的操作系统都会支持，但是每次等待都要设置需要等待的套接口，并且内部的实现不够高效，很难支持监听高并发量的套接口集。不同的操作系统使用了不同的高级轮询技术来支持高性能的监听，一般这些方式都不是可移植的，比如freebsd上实现了 kqueue，solaris实现了/dev/poll，linux实现了epoll等。我们使用的 Centos 6.x 系统的epoll库。这也就是Nginx高性能的第二大原因。

### 2.3. Tengine 丰富插件

Tengine 是 nginx衍生版本，性能和稳定性已经在大型的网站如淘宝网，天猫商城等得到了很好的体验，其一个重要特点就是非常多的丰富插件，以下就来介绍下我们乐信使用了哪些 Tengine 插件模块：

\- –with-http_concat_module \ ## 用于合并多个文件在一个响应报文中
\- –with-http_flv_module \ ## 供服务端伪流媒体支持
\- –with-http_stub_status_module \ ## nginx 性能监控
\- –with-http_gzip_static_module \ ## gzip 支持各类数据压缩传输，降低网络
\- –with-http_sysguard_module \ ## 过载保护模块
\- –with-http_lua_module \ ## lua 模块
\- –with-http_v2_module \ ## http2支持
\- –add-module=../nginx_upstream_check_module-master \ ## upstream 健康检测和自动剔除模块
\- –add-module=../nginx-upsync-module-1.8.x ## 实现配置动态更新等等；

正是以上tengine丰富的插件，给我们的业务带来了强稳定性，高并发性能噢；

### 2.4. tengine 其它特性

除了以上，内部有大量可见的细节优化，采用多进程单线程的工作方式，并且利用cpu和进程的亲缘性将进程和特定cpu绑定（tengine默认支持），避免了进程上下文切换的开销，从而减少了cpu占用。另外它实现了高效的内存池，将内存占用降到最低等，这个依靠于nginx.cong 主配置进行优化等等；

## 3. tengine 扩展 lua

ngx_lua 模块是 nginx 第三方模块,它能将lua语言嵌入到nginx配置中,从而使用lua 极大增强了nginx的能力，nginx以高并发而知名,lua脚本轻便,两者的搭配堪称完美。

我们使用的 tengine+lua 大致可以分为2类作用，第一类是核心接入服务器，第二类是业务承载服务器，可能在我们这里都统称在一个大集群里面：

核心接入服务器：
1.如动态负载均衡（后续我们需要做的，因为我们业务需更高级别隔离）；
2.根据请求特征将流量分配到不同分组不同set或者机房，我们叫它流量调度（已具备）；
3.防DDOS攻击限流：可以将请求日志推送到实时计算集群，然后将需要限流的IP推送到核心Nginx进行限流（我们利用TGW防DDOS攻击）；
4.缓存服务，使用Nginx Proxy Cache实现内容页面的缓存等等；
业务承载服务器：
1.缓存，直接把图片等资源链接存储在redis,通过lua+nginx+redis 进行第一层请求返回（如tab页缓存）；
2.AB测试/灰度发布：比如要上一个新的接口，可以通过在业务Nginx通过Lua写复杂的业务规则实现不同的人看到不同的版本。（已具备）；
3.服务质量监控：我们可以记录请求响应时间、缓存响应时间、反向代理服务响应时间等达到实时监控，另外记录非200状态码错误来了监控服务可用率（已具备）等功能；

## 4.总结

1. nginx 它是一个高性能web服务器，它能同时提供上万的连接请求，而且速度快，占用资源少，部署在普通服务器就ok；
2. nginx 高效是因为异步且采用非阻塞的方式与每个客户通信，通过使用了操作系统IO复用方式（select和poll、epoll）达到高性能；
3. tengine 丰富的插件可提供我们插拔式选择使用，并且在主配置文件也具有大量的优化性能参数，如内存池，cpu亲缘性绑定等等；
4. lua_ngx 现在在我们业务方面实践，比如有waf防火墙，流量调度，流量管控，页面缓存，质量监控，统一日志等多个模块应用业务上。

原文链接：https://zhuanlan.zhihu.com/p/406797738

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.380】深入理解 ProtoBuf 原理与工程实践

ProtoBuf 作为一种跨平台、语言无关、可扩展的序列化结构数据的方法，已广泛应用于网络数据交换及存储。随着互联网的发展，系统的异构性会愈发突出，跨语言的需求会愈加明显，同时 gRPC 也大有取代Restful之势，而 ProtoBuf 作为g RPC 跨语言、高性能的法宝，我们技术人有必要

深入理解 ProtoBuf 原理，为以后的技术更新和选型打下基础。

我将过去的学习过程以及实践经验，总结成系列文章，与大家一起探讨学习，希望大家能有所收获，当然其中有不正确的地方也欢迎大家批评指正。

本系列文章主要包含：

1. 深入理解 ProtoBuf 原理与工程实践（概述）
2. 深入理解 ProtoBuf 原理与工程实践（编码）
3. 深入理解 ProtoBuf 原理与工程实践（序列化）
4. 深入理解 ProtoBuf 原理与工程实践（工程实践）

## 1.什么是ProtoBuf

ProtoBuf(Protocol Buffers)是一种跨平台、语言无关、可扩展的序列化结构数据的方法，可用于网络数据交换及存储。

在序列化结构化数据的机制中，ProtoBuf是灵活、高效、自动化的，相对常见的XML、JSON，描述同样的信息，ProtoBuf序列化后数据量更小、序列化/反序列化速度更快、更简单。

一旦定义了要处理的数据的数据结构之后，就可以利用ProtoBuf的代码生成工具生成相关的代码。只需使用 Protobuf 对数据结构进行一次描述，即可利用各种不同语言(proto3支持C++, Java, Python, Go, Ruby, Objective-C, C#)或从各种不同流中对你的结构化数据轻松读写。

## 2.为什么是 ProtoBuf

大家可能会觉得 Google 发明 ProtoBuf 是为了解决序列化速度的，其实真实的原因并不是这样的。

ProtoBuf最先开始是 Google用来解决索引服务器 request/response 协议的。没有ProtoBuf之前，Google 已经存在了一种 request/response 格式，用于手动处理 request/response 的编解码。它也能支持多版本协议，不过代码不够优雅：

```
if (protocolVersion=1) {
    doSomething();
} else if (protocolVersion=2) {
    doOtherThing();
} ...
```

如果是非常明确的格式化协议，会使新协议变得非常复杂。因为开发人员必须确保请求发起者与处理请求的实际服务器之间的所有服务器都能理解新协议，然后才能切换开关以开始使用新协议。

这也就是每个服务器开发人员都遇到过的低版本兼容、新旧协议兼容相关的问题。

为了解决这些问题，于是ProtoBuf就诞生了。

ProtoBuf 最初被寄予以下 2 个特点：

- **更容易引入新的字段**，并且不需要检查数据的中间服务器可以简单地解析并传递数据，而无需了解所有字段。
- **数据格式更加具有自我描述性**，可以用各种语言来处理(C++, Java 等各种语言)。

这个版本的 ProtoBuf 仍需要自己手写解析的代码。

不过随着系统慢慢发展，演进，ProtoBuf具有了更多的特性：

- 自动生成的序列化和反序列化代码避免了手动解析的需要。（官方提供自动生成代码工具，各个语言平台的基本都有）。
- 除了用于数据交换之外，ProtoBuf被用作持久化数据的便捷自描述格式。

ProtoBuf 现在是 Google 用于数据交换和存储的通用语言。谷歌代码树中定义了 48162 种不同的消息类型，包括 12183 个 .proto 文件。它们既用于 RPC 系统，也用于在各种存储系统中持久存储数据。

ProtoBuf 诞生之初是为了解决服务器端新旧协议(高低版本)兼容性问题，名字也很体贴，“协议缓冲区”。只不过后期慢慢发展成用于传输数据。

Protocol Buffers 命名由来：

> Why the name “Protocol Buffers”?
> The name originates from the early days of the format, before we had the protocol buffer compiler to generate classes for us. At the time, there was a class called ProtocolBuffer which actually acted as a buffer for an individual method. Users would add tag/value pairs to this buffer individually by calling methods like AddValue(tag, value). The raw bytes were stored in a buffer which could then be written out once the message had been constructed.
> Since that time, the “buffers” part of the name has lost its meaning, but it is still the name we use. Today, people usually use the term “protocol message” to refer to a message in an abstract sense, “protocol buffer” to refer to a serialized copy of a message, and “protocol message object” to refer to an in-memory object representing the parsed message.

## 3.如何使用 ProtoBuf

### 3.1. ProtoBuf 协议的工作流程

![img](https://pic4.zhimg.com/80/v2-6bd90593f1d04ef0c50be6f503ae4f0b_720w.webp)

可以看到，对于序列化协议来说，使用方只需要关注业务对象本身，即 idl 定义，序列化和反序列化的代码只需要通过工具生成即可。

### 3.2 .ProtoBuf 消息定义

ProtoBuf 的消息是在idl文件(.proto)中描述的。下面是本次样例中使用到的消息描述符customer.proto：

```
syntax = "proto3";
package domain;
option java_package = "com.protobuf.generated.domain";
option java_outer_classname = "CustomerProtos";
message Customers {
    repeated Customer customer = 1;
}
message Customer {
    int32 id = 1;
    string firstName = 2;
    string lastName = 3;
    enum EmailType {
        PRIVATE = 0;
        PROFESSIONAL = 1;
    }
    message EmailAddress {
        string email = 1;
        EmailType type = 2;
    }
    repeated EmailAddress email = 5;
}
```

上面的消息比较简单，Customers包含多个Customer，Customer包含一个id字段，一个firstName字段，一个lastName字段以及一个email的集合。

除了这些定义外，文件顶部还有三行可帮助代码生成器：

1. 首先，syntax = “proto3”用于idl语法版本，目前有两个版本proto2和proto3，两个版本语法不兼容，如果不指定，默认语法是proto2。由于proto3比proto2支持的语言更多，语法更简洁，本文使用的是proto3。
2. 其次有一个package domain;定义。此配置用于嵌套生成的类/对象。
3. 有一个option java_package定义。生成器还使用此配置来嵌套生成的源。此处的区别在于这仅适用于Java。在使用Java创建代码和使用JavaScript创建代码时，使用了两种配置来使生成器的行为有所不同。也就是说，Java类是在包com.protobuf.generated.domain下创建的，而JavaScript对象是在包domain下创建的。

ProtoBuf 提供了更多选项和数据类型，本文不做详细介绍，感兴趣可以参考这里。

### 3.3 .代码生成

首先安装 ProtoBuf 编译器 protoc，这里有详细的安装教程，安装完成后，可以使用以下命令生成 Java 源代码：

```
protoc --java_out=./src/main/java ./src/main/idl/customer.proto
```

从项目的根路径执行该命令，并添加了两个参数：java_out，定义./src/main/java/为Java代码的输出目录；而./src/main/idl/customer.proto是.proto文件所在目录。

生成的代码非常复杂，但是幸运的是它的用法却非常简单。

```
CustomerProtos.Customer.EmailAddress email = CustomerProtos.Customer.EmailAddress.newBuilder()
                .setType(CustomerProtos.Customer.EmailType.PROFESSIONAL)
                .setEmail("crichardson@email.com").build();
        CustomerProtos.Customer customer = CustomerProtos.Customer.newBuilder()
                .setId(1)
                .setFirstName("Lee")
                .setLastName("Richardson")
                .addEmail(email)
                .build();
        // 序列化
        byte[] binaryInfo = customer.toByteArray();
        System.out.println(bytes_String16(binaryInfo));
        System.out.println(customer.toByteArray().length);
        // 反序列化
        CustomerProtos.Customer anotherCustomer = CustomerProtos.Customer.parseFrom(binaryInfo);
        System.out.println(anotherCustomer.toString());
```

### 3.4. 性能数据

我们简单地以Customers为模型，分别构造、选取小对象、普通对象、大对象进行性能对比。

序列化耗时以及序列化后数据大小对比

![img](https://pic2.zhimg.com/80/v2-6b3bca2954b3717861c50ada727cd9c9_720w.webp)

反序列化耗时

![img](https://pic2.zhimg.com/80/v2-97d226a1fce4f599183f115aab7db929_720w.webp)

## 4.总结

上面介绍了 ProtoBuf 是什么、产生的背景、基本用法，我们再总结下。

**优点：**

> \1. 效率高
> 从序列化后的数据体积角度，与XML、JSON这类文本协议相比，ProtoBuf通过T-(L)-V（TAG-LENGTH-VALUE）方式编码，不需要”, {, }, :等分隔符来结构化信息，同时在编码层面使用varint压缩，所以描述同样的信息，ProtoBuf序列化后的体积要小很多，在网络中传输消耗的网络流量更少，进而对于网络资源紧张、性能要求非常高的场景，ProtoBuf协议是不错的选择。

```
// 我们简单做个对比// 要描述如下JSON数据{"id":1,"firstName":"Chris","lastName":"Richardson","email":[{"type":"PROFESSIONAL","email":"crichardson@email.com"}]}# 使用JSON序列化后的数据大小为118byte7b226964223a312c2266697273744e616d65223a224368726973222c226c6173744e616d65223a2252696368617264736f6e222c22656d61696c223a5b7b22747970<br>65223a2250524f46455353494f4e414c222c22656d61696c223a226372696368617264736f6e40656d61696c2e636f6d227d5d7d# 而使用ProtoBuf序列化后的数据大小为48byte0801120543687269731a0a52696368617264736f6e2a190a156372696368617264736f6e40656d61696c2e636f6d1001
```

> 从序列化/反序列化速度角度，与XML、JSON相比，ProtoBuf序列化/反序列化的速度更快，比XML要快20-100倍。
> \2. 支持跨平台、多语言
> ProtoBuf是平台无关的，无论是Android与PC，还是C#与Java都可以利用ProtoBuf进行无障碍通讯。
> proto3支持C++, Java, Python, Go, Ruby, Objective-C, C#。
> \3. 扩展性、兼容性好
> 具有向后兼容的特性，更新数据结构以后，老版本依旧可以兼容，这也是ProtoBuf诞生之初被寄予解决的问题。因为编译器对不识别的新增字段会跳过不处理。
> \4. 使用简单
> ProtoBuf 提供了一套编译工具，可以自动生成序列化、反序列化的样板代码，这样开发者只要关注业务数据idl，简化了编码解码工作以及多语言交互的复杂度。

**缺点：**

> 可读性差，缺乏自描述
> XML，JSON是自描述的，而ProtoBuf则不是。
> ProtoBuf是二进制协议，编码后的数据可读性差，如果没有idl文件，就无法理解二进制数据流，对调试不友好。

不过Charles已经支持ProtoBuf协议，导入数据的描述文件即可。

此外，由于没有idl文件无法解析二进制数据流，ProtoBuf在一定程度上可以保护数据，提升核心数据被破解的门槛，降低核心数据被盗爬的风险。

原文链接：https://zhuanlan.zhihu.com/p/407874981

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)