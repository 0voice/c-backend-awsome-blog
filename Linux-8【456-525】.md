# 【NO.456】音视频开发技术的基本知识

互联网信息的传播与娱乐方式经历了从文字到图片再到音视频的转变，现如今抖音、快手等短视频更是如日中天，特别是5G时代的到来，笔者相信互联网对音视频开发者的需求会迎来更大的增长需求，何况音视频开发者因为其稀缺性薪酬本来就比较高。

在学习音视频开发之前，我们先来了解一下音视频的基本知识。

## 1.音频

声波的三要素：频率、振幅和波形。频率代表音阶的高低，振幅代表响度，波形代表音色。

频率越高，波长就越短。低频声响的波长则较长，所以其可以更容易地绕过障碍物，因此能量衰减就小，声音就会传得远。人的听力有一个频率范围，大约是20Hz~20kHz。

音频数据的承载方式最常用的是脉冲编码调制，即PCM。

### 1.1 数字信号

在自然界中，声音是连续不断的，是一种模拟信号，那怎样才能把声音保存下来呢？那就是把声音数字化，即转换为数字信号。

将模拟信号数字化，要经过采样、量化和编码三个步骤。

音频数字化有一个问题：数字信号并不能连续保存所有时间点的振幅。

实际上，音频数字化并不需要保存连续的信号，就可以还原到人耳可接受的声音。

### 1.2 采样率

每一秒钟所采样的数目称为采样频率或采率，单位为HZ（赫兹）。采样频率越高所能描述的声波频率就越高。

> 根据奈奎斯特定理，按比声音最高频率高2倍以上的频率对声音进行采样，经过数字化处理之后，人耳听到的声音质量不会被降低。所以采样频率一般为44.1kHz。

- 8khz：电话等使用，对于记录人声已经足够使用。
- 22.05khz：广播使用频率。
- 44.1khz：音频CD。
- 48khz：DVD、数字电视中使用。
- 96khz-192khz：DVD-Audio、蓝光高清等使用。

### 1.3 量化格式

量化格式也叫采样大小或采样精度，指的是一个采样使用多少bit存放，一般是16bit，一个字节是8bit。

### 1.4 编解码

描述一段PCM数据一般需要以下几个概念：量化格式(sampleFormat)、采样率(sampleRate)、声道数(channel)。
量化格式和采样率上面提到过了，声道数是指支持能不同发声的音响的个数。不难理解，立体声道的声道数默认为2个声道。

数据比特率，即1秒时间内的比特数目。

\>

> 以CD的音质为例，量化格式（位深度）为16比特，样率为44100，声道数为2。
> 比特率:
> 比特率 = 采样率 × 采样深度 × 通道数
> 44100 * 16 * 2 = 1378.123kbps **注意这里的k表示1024**
> 一分钟音频数据的大小：1378.125 * 60 / 8 / 1024 = 10.09MB
> 注意：1个字节等于8位，也就是8比特

这不对啊，我们平时听的阴影四五分钟也才四五兆，到了你这里怎么一秒钟就十多兆了呢？这就是编码的功劳了。

我们先来看看比特率中的数字和字母到底是什么意思？

> 首先128k的全称“128kbps”，我们分解一下：128是数字，k是千位符，b是单位，s是秒，ps其实就是“/s”。这样来看，128kbps就是128kb/s。也就是每秒128kb。请注意，这里的b是小写的b，也就是位。
> 知道了这个，我们就能算出来128kb的文件大概占用多少的存储空间：
> 128*1000=128000b/s÷8=16000B/s÷1024=15.625KB/s
> 15.625KB/s*60=937.5KB/分钟÷1024=0.9155MB/分钟
> 所以，128kb的音频文件，大概每分钟长度的大小都在0.92M或者916kb左右。
> **注意b和B是不同的概念**

编码就是一个压缩的过程，而压缩又分为有损压缩和无损压缩：

有损压缩就是去掉冗余信号，冗余信号是指不能被人耳感知到的信号，包含人耳听觉范围之外的音频信号以及被掩蔽掉的音频信号等

无损压缩就是通过优化排列方式来达到压缩目的。

常见的音频编码格式：

1. WAV(无损)

WAV编码就是在PCM数据格式的前面加上44字节，分别用来描述PCM的采样率、声道数、数据格式等信息。

特点：音质非常好，大量软件都支持。
缺点：因为没用经过压缩，所以文件占用的储存空间会特别大。

适用场合：多媒体开发的中间文件、保存音乐和音效素材。

1. MP3(有损)

MP3具有不错的压缩比，使用LAME编码（MP3编码格式的一种实现）的中高码率的MP3文件，听感上非常接近源WAV文件。现如今市面上的音乐大多是这种编码格式。

特点：音质在128Kbit/s以上表现还不错，压缩比比较高，大量软件和硬件都支持，兼容性好。
缺点：由于技术比较落后，同样码率下音质会比AAC、OGG差一些。

1. AAC(有损)

AAC是新一代的音频有损压缩技术，它通过一些附加的编码技术（比如PS、SBR等），衍生出了LC-AAC、HE-AAC、HE-AAC v2三种主要的编码格式。

LC-AAC是比较传统的AAC，相对而言，其主要应用于中高码率场景的编码（≥80Kbit/s）；

HE-AAC（相当于AAC+SBR）主要应用于中低码率场景的编码（≤80Kbit/s）；

而新近推出的HE-AAC v2（相当于AAC+SBR+PS）主要应用于低码率场景的编码（≤48Kbit/s）。事实上大部分编码器都设置为≤48Kbit/s自动启用PS技术，而>48Kbit/s则不加PS，相当于普通的HE-AAC。

特点：在小于128Kbit/s的码率下表现优异，并且多用于视频中的音频编码。
不足：虽然在低码率上表现比MP3好一些，但是还没有达到全面碾压的地步。

适用场合：128Kbit/s以下的音频编码，多用于视频中音频轨的编码。

AAC格式主要分为两种：ADIF、ADTS。

ADIF：Audio Data Interchange Format。音频数据交换格式。这种格式的特征是可以确定的找到这个音频数据的开始，不能在音频数据流中间开始的解码，即它的解码必须在明确定义的开始处进行。这种格式常用在磁盘文件中。

ADTS：Audio Data Transport Stream。音频数据传输流。这种格式的特征是它是一个有同步字的比特流，解码可以在这个流中任何位置开始。它的特征类似于mp3数据流格式。

ADTS可以在任意帧解码，它每一帧都有头信息。ADIF只有一个统一的头，所以必须得到所有的数据后解码。且这两种的header的格式也是不同的，目前一般编码后的都是ADTS格式的音频流。

1. Ogg(有损)

Ogg是一种非常有潜力的编码，在各种码率下都有比较优秀的表现，尤其是在中低码率场景下。Ogg除了音质好之外，还是完全免费的，这为Ogg获得更多的支持打好了基础。Ogg有着非常出色的算法，可以用更小的码率达到更好的音质，128Kbit/s的Ogg比192Kbit/s甚至更高码率的MP3还要出色。但目前因为还没有媒体服务软件的支持，因此基于Ogg的数字广播还无法实现。Ogg目前受支持的情况还不够好，无论是软件上的还是硬件上的支持，都无法和MP3相提并论。

特点：可以用比MP3更小的码率实现比MP3更好的音质，高中低码率下均有良好的表现。
缺点：兼容性不够好，流媒体特性不支持。

适用场合：语音聊天的音频消息场景。

1. FLAC(无损)

FLAC是一套著名的自由音频压缩编码，其特点是无损压缩。不同于其他有损压缩编码如MP3 及AAC，它不会破坏任何原有的音频资讯，所以可以还原音乐光盘音质。2012年以来它已被很多软件及硬件音频产品（如CD等）所支持.

FLAC与MP3不同，MP3是音频压缩编码，但FLAC是无损压缩，也就是说音频以FLAC编码压缩后不会丢失任何信息，将FLAC文件还原为WAV文件后，与压缩前的WAV文件内容相同。这种压缩与ZIP的方式类似，但FLAC的压缩比率大于ZIP和RAR，因为FLAC是专门针对PCM音频的特点设计的压缩方式。而且可以使用播放器直接播放FLAC压缩的文件，就象通常播放你的MP3文件一样.

## 2.视频

所谓视频其实就是由很多的静态图片组成的。由于人类眼睛的特殊结构，画面快速切换时，画面会有残留，所以静态图片快速切换的时候感觉起来就是连贯的动作。这就是视频的原理。

视频帧：
既然视频是由许多静态图片组成的，那么视频的每一张静态图片就叫一帧。

视频帧又分为I帧、B帧和P帧：

I帧：帧内编码帧，大多数情况下I帧就是关键帧，就是一个完整帧，无需任何辅助就能独立完整显示的画面。

B帧：帧是双向预测帧。参考前后图像帧编码生成。需要前面的 I/P 帧或者后面的 P 帧来协助形成一个画面。

P帧：前向预测编码帧。是一个非完整帧，通过参考前面的I帧或P帧生成画面。

> 所以 I 帧是很关键的存在，压缩 I 帧就可以很容易压制掉空间的大小，而压缩P帧和B帧可以压缩掉时间上的冗余信息 。所以在视频 seek 的时候，I 帧很关键，如果视频 seek 之后发生往前的跳动，有可能就是你要seek到的位置没用关键帧，这就需要处理了。好像Android自带的播放器就会有这个问题，有时候无法精确地seek到某个位置。

还有一个叫 IDR 帧的概念，IDR都是I帧，可以防止一帧解码出错，导致后面所有帧解码出错的问题。
因为 H264 采用的是多帧预测，导致 I 帧不能作为独立的观察条件，所以多出一个叫 IDR 帧的特殊 I 帧用于参考，IDR 帧最关键的概念就是：在解码器过程中一旦收到 IDR 帧，就会立即清空参考帧缓冲区，并将IDR帧作为被参考帧。这样，即便前面一帧解码出现重大错误，也不会蔓延到后面的数据中。

**注：关键帧都是I帧，但是I帧不一定是关键帧** 这是为什么？求高人指点！！！

DTS全称：Decoding Time Stamp。标示读入内存中数据流在什么时候开始送入解码器中进行解码。也就是解码顺序的时间戳。

PTS全称：Presentation Time Stamp。用于标示解码后的视频帧什么时候被显示出来。

> **在没有B帧的情况下，DTS和PTS的输出顺序是一样的，一旦存在B帧，PTS和DTS则会不同。** 因为解码的顺序和播放的顺序可能是不一致的。

GOP（Group Of Picture）就是两个 I 帧之间的距离，一般 GOP 设置得越大，画面的效果就会越好，到那时需要解码的时间就会越长。所以如果码率固定而 GOP 值越大，P/B帧 数量会越多，画面质量就会越高。

帧率：
帧率，即单位时间内帧的数量，单位为：帧/秒 或fps（frames per second）。帧率越高，每秒切换的图片就越多，画面越顺滑，过渡越自然。
帧率的一般以下几个典型值：

- 24/25 fps：1秒 24/25 帧，一般的电影帧率。
- 30/60 fps：1秒 30/60 帧，游戏的帧率，30帧可以接受，60帧会感觉更加流畅逼真。Android系统的高性能渲染就是以60帧为标准。
- 85 fps以上人眼基本无法察觉出来了，所以更高的帧率在视频里没有太大意义。

色彩空间：

我们都知道RGB是三原色，通过RGB三种基础色，可以混合出所有的颜色。

还有一张是YUV，这种色彩空间并不是我们熟悉的。这是一种亮度与色度分离的色彩格式。
早期的电视都是黑白的，即只有亮度值，即Y。有了彩色电视以后，加入了UV两种色度，形成现在的YUV，也叫YCbCr。
Y：亮度，就是灰度值。除了表示亮度信号外，还含有较多的绿色通道量。
U：蓝色通道与亮度的差值。
V：红色通道与亮度的差值。

因为人眼对亮度敏感，对色度不敏感，因此减少部分UV的数据量，人眼却无法感知出来，这样可以通过压缩UV的分辨率，在不影响观感的前提下，减小视频的体积。大大提高传输的效率和节省带宽。
关于YUV这里我就不多说了，后面我会专门写一篇文章介绍。比如YUV444，YUV422，YUV420和YUV420sp以及YUV和RGB是如何转换的等等。

视频编码格式：

视频编码格式有很多，比如H26x系列和MPEG系列的编码，这些编码格式都是为了适应时代发展而出现的。
其中，H26x（1/2/3/4/5）系列由ITU（International Telecommunication Union）国际电传视讯联盟主导的。
MPEG（1/2/3/4）系列由MPEG（Moving Picture Experts Group, ISO旗下的组织）主导。
H264是目前最主流的视频编码标准，目前大多数的视频和流媒体都是使用这种编码格式。

> H264编码算法是十分复杂，不是三言两语能够讲清楚的，也不在我的能力范围只能，我们要做到的就是知道怎么使用就好了。

## 3.编解码

编码：编码就是将原始音频数据也就是PCM压缩的一个过程；或者是将原始的视频数据RGB或YUV压缩的一个过程。

解码：解码就是编码一个逆过程，比如将编码后的数据AAC解码成PCM给播放器播放；或者将编码后的H264数据解码成YUV或RGB给播放器渲染的过程。

编解码又分为硬件编解码和软件编解码。

软件编解码就是指利用CPU的计算能力来进行编解码码，通常如果CPU的能力不是很强的时候，一则编解码速度会比较慢，二则手机可能出现发热现象。但是，由于使用统一的算法，兼容性会很好。

硬件编解码解码，指的是利用手机上专门的解码芯片来加速解码。通常硬解码的解码速度会快很多，但是由于硬解码由各个厂家实现，质量参差不齐，非常容易出现兼容性问题。

## 4.封装格式

封装格式业界也有人称音视频容器，比如我们经常看到的视频后缀名：`mp4、rmvb、avi、mkv、mov`等就是音视频的容器，它们将音频和视频甚至是字幕一起打包进去，封装成一个文件。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/423200248

# 【NO.457】用WinDbg断点调试FFmpeg

本文主要讲解 WinDbg 调试器的使用。WinDbg 在 Windows 里面的地位，就跟 GDB 在 Linux 的地位一样。可以通过 微软的官方网站 下载 安装 WinDbg。

WinDbg 是比较轻量级的调试工具，在一些场景下比较实用，例如不方便安装 vs2019。

只要有 符号信息表（symbols） 跟 调试信息表（debug info），一样能用 WinDbg 进行 源码级调试 跟 各种断点调试。这些信息，windows 是放在 pdb 文件里面的，在源码目录，可以看到 ffmpeg_g.pdb 这个文件。

FFmpeg 的编译过程跟 前面文章 《用msys2与msvc编译FFmpeg 》一样的，都是使用 msys2 + msvc。请按照之前的教程编译出来 ffmpeg.exe 文件，如果已经编译出来就可以直接用之前的 ffmpeg.exe。

![img](https://pic4.zhimg.com/80/v2-b05e74c959ab3fef0175cf1bb5cfedab_720w.webp)

WinDbg 有 32 位跟64位，我们的 ffmpeg.exe 是 64 位的，所以选用 WinDbg 64位来调试。如下：

![img](https://pic4.zhimg.com/80/v2-9f76a503925633095ed906d9057bbd03_720w.webp)

打开 windbg.exe ，界面如下：

![img](https://pic2.zhimg.com/80/v2-9c42a0f6de1b828180ea6fec7b8dd93d_720w.webp)

然后点击 菜单栏的 File → Open Executable，会弹出窗口，如下：

![img](https://pic3.zhimg.com/80/v2-a38e871146e272a6b1b9ad45da9e585e_720w.webp)

上图中 设置了 Arguments 参数 跟工作目录，参数如下：

Arguments ：-i walking-dead.mp4 -c copy walking-dead.flv -y

Start directory：C:\msys64\home\loken\ffmpeg\build64\ffmepg-4.4-msvc

walking-dead.mp4 是本书经常用到的视频素材，请下载保存好。

打开之后，界面如下：

![img](https://pic3.zhimg.com/80/v2-eccb1d758c5297d383251388c03b9962_720w.webp)

这里简单讲解一些 WinDbg 的界面，底部是命令输入框，上图中，我输入了一个 k， 查看当前断点的调用栈。

可以看到，WinDbg 会默认停在 ntdll 模块 的 LdrpDoDebuggerBreak 函数，这是 WinDbg 的默认断点，现在还没有跑进去 ffmpeg.exe 的main函数，所以我们需要加一个断点，如下：

```text
# 设置断点
bu ffmpeg_g!main
# 继续执行
g
```

注意，是 ffmpeg_g ，后面有个 _g 。 设置完断点之后，再敲入一个命令 g，g 代表 go。代码就会执行到 main 那里停下来。如下：

![img](https://pic1.zhimg.com/80/v2-2985ec40e7acdf455683d470eaa67bfc_720w.webp)

现在讲一下 WinDbg 常用的一些命令。

1，k ：查看函数调用栈。

2，bu ：根据符号进行断点，例如 bu ffmpeg_g.exe!main ，前面要有模块名，跟gdb 有点不一样。

3，bl：查看所有断点。

4，p：单步步进。

5，g：代码继续执行，go 的意思，快捷键 F5

WinDbg 的更多命令，请看 《微软WinDbg 文档》，《WinDbg 调试器文档》 。

------

WinDbg 还有更多的调试窗口可以调出来，这些窗口都在菜单栏的 View 里面，这里简单介绍一下这些窗口。

1，Watch，观察窗口。点击可以添加自己想观察的全局变量或者局部变量。

2，Locals，局部变量窗口，运行到某个函数，这个窗口就是显示这个函数的局部变量信息。

3，Registers，寄存器窗口。

4，Memory，内存窗口。

5，Call Stack ，函数调用栈。

6，Disassembly，汇编代码窗口。

下面我把一些窗口调出来看看效果，如下：

![img](https://pic4.zhimg.com/80/v2-7c84aa515b3d4ebde2690de196a2daef_720w.webp)

调试器的功能都是类似的，常用的功能无非就是 数据断点，函数断点，然后可以观察变量之类的。

WinDbg 这个调试器的具体架构实现跟用法，墙裂推荐 《软件调试》卷二 windows 下册第30章 ，通过这本书，你可以了解到如何实现一个调试器。

参考资料：

1，《软件调试》卷二 windows 下册第30章 - 张银奎

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/529621196

# 【NO.458】FFplay源码分析-nobuffer

在使用 FFplay 播放 RTMP 流的时候，如果 不开启 nobuffer 选项，画面延迟会高达 7 秒左右，开启了，局域网延迟可降低到100毫秒左右。

因此本文主要研究 nobuffer 的具体实现，以及播放端 缓存 7 秒的数据有何作用。

fflags 的定义在 libavformat/options_table.h，如下图代码，这是一个通用选项，所有的 解复用器都有这个选项。

![img](https://pic1.zhimg.com/80/v2-d0ecf694be9557a0f4f2f50a7a6d57fc_720w.webp)

也就是说，这个命令行参数，是在调 avformat_open_input() 函数的时候丢进去的，我为什么知道是在这个地方丢进去的？请看之前的专栏《FFplay源码分析》，所有的解复用参数，也叫格式参数，都是在 avformat_open_input() 丢进去的。可以看下图证明一下：

![img](https://pic4.zhimg.com/80/v2-6b4c1e0d67bb35945a555c72eea3b99b_720w.webp)

记得改 Clion 的调试参数，把 `-fflags nobuffer` 加上去。

------

在 `avformat_open_input()` 函数内部，会把 `fflags` 这个 AVOption 丢给 AVClass，如下图所示，AVClass 里面存储了好几个 AVOption ，fflags 这个 AVOption 的下标是 5 ，前面的是 默认的选项，自动加进去的。

![img](https://pic1.zhimg.com/80/v2-235344190b78c850b0d5fbdef4b38750_720w.webp)

注意 ，`av_opt_set_dict()` 这个函数会改变 `tmp` 的值，把能用 选项应用之后剔除。



https://link.zhihu.com/?target=https%3A//docs.qq.com/doc/DYXlQZVZoRkRBRlFk)



------

`nobuffer` 这个参数 传递过程中的函数调用有点长，推荐看之前的《[FFmpeg源码分析-参数解析篇](https://link.zhihu.com/?target=https%3A//www.xianwaizhiyin.net/%3Fcat%3D14)》，原理类似，我知道这个 参数最后会赋值到 `struct AVFormatContext` 的 `flags` 字段里面，如下图：

![img](https://pic2.zhimg.com/80/v2-fc9816793f1a3a3d0c8443347118cc99_720w.webp)

如上图所示，所以 `avformat_open_input()` 函数执行完之后 `AVFormatContext::flags` 的第7位应该会被置为1，因为 0x40 的二进制是 1000000。请看下图：

![img](https://pic1.zhimg.com/80/v2-5ea510ed6cca9e7c93220dc1b653988c_720w.webp)

从上图可以看出， ic->flags 直接就是 64 ，也就是 16 进制的 0x40。所以上面的分析没错。 avformat_open_input() 函数只是把 命令行参数解析 到这个 flags 字段，但是真正使用这个字段 是在 avformat_find_stream_info() 里面。直接搜 AVFMT_FLAG_NOBUFFER 就能找到使用的位置。

avformat_find_stream_info() 函数的内部逻辑实际上非常复杂，我直接讲重点代码，如下:

```text
if (!(ic->flags & AVFMT_FLAG_NOBUFFER)) {
    ret = avpriv_packet_list_put(&ic->internal->packet_buffer,
                                        &ic->internal->packet_buffer_end,
    pkt1, NULL, 0);
    if (ret < 0)
        goto unref_then_goto_end;

    pkt = &ic->internal->packet_buffer_end->pkt;
} else {
    pkt = pkt1;
}
```

AVFMT_FLAG_NOBUFFER 标记 如果没设置，就会导致 探测的数据包丢进去队列，我们知道 avformat_find_stream_info() 会先读一段数据包分析出流是什么编码器之类的，为了重用这个 探测的数据包，这里就会丢进去队列，播放的时候，就从这些数据包开始，但是整个探测过程，长达 5秒，也就是 ffplay 大概会读 5秒的数据，来分析输入流的情况。如果开启 nobuffer，就不会重用这些探测数据，ffpaly 探测完输入流之后，就会重新读取新的数据包来播放。不用缓存的，所以延迟就低了。

如下图，我在 ffpaly.c 的 avformat_find_stream_info() 前后输出了个时间，正好相差5秒。

```text
double start_time3 = av_gettime_relative() / 1000000.0;
if (find_stream_info) {
    ...省略代码...
    err = avformat_find_stream_info(ic, opts);
    ...省略代码...
}
double end_time3 = av_gettime_relative() / 1000000.0;
printf("start is %f , end is %f \r\n",start_time3,end_time3);
```

![img](https://pic4.zhimg.com/80/v2-aa55afd51e3011dd6d7fa7e947bd7703_720w.webp)

所以实际上 ffplay 在 实时的场景下，缓存是个鸡肋，本来这个 buffer 功能是为了分析本地文件，避免重复读取，但是影响到了实时的场景。实时场景必须把 buffer 关掉。

补充，因为我是虚拟机做服务器，所以都是本机通信，不启用 buffer 也能很流畅，但是如果我把 SRS 部署在局域网另一台机器，不开启 buffer ，视频有卡顿，估计是还没来得及解码丢进去队列，所以 ffplay 不断丢弃视频帧。因为视频比音频慢了，得丢弃。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/492176676

# 【NO.459】RTSP直播延时的深度优化（干货）

每日专注分享音视频技术点，ffmpeg，webRTC技术，推流拉流，流媒体服务器，SRS，sfu模型，H264码率，等等技术栈文章视频。

------

现在ijkPlayer是许多播放器、直播平台的首选，相信很多开发者都接触过ijkPlayer，无论是Android工程师还是iOS工程师。我曾经在Github上的ijkPlayer开源项目上提问过：视频流为1080P、30fps，如何优化RTSP直播的延时为大约100ms呢？发现大家对RTSP直播延时优化非常感兴趣，纷纷提问或者给出自己的观点。本文主要是总结，也是与大家探讨RTSP直播的延时优化。

## 1.修改编译脚本支持RTSP

ijkPlayer默认是没有把RTSP协议编译进去，所以我们得修改编译脚本，原来的disable改为enable：

```text
export COMMON_FF_CFG_FLAGS="$COMMON_FF_CFG_FLAGS --enable-protocol=rtp"
export COMMON_FF_CFG_FLAGS="$COMMON_FF_CFG_FLAGS --enable-protocol=tcp"
export COMMON_FF_CFG_FLAGS="$COMMON_FF_CFG_FLAGS --enable-demuxer=rtsp"
export COMMON_FF_CFG_FLAGS="$COMMON_FF_CFG_FLAGS --enable-demuxer=sdp"
export COMMON_FF_CFG_FLAGS="$COMMON_FF_CFG_FLAGS --enable-demuxer=rtp"
```

## 

## **2.修改播放器的option参数**

```text
//丢帧阈值
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_PLAYER, "framedrop", 30);
//视频帧率
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_PLAYER, "fps", 30);
//环路滤波
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_CODEC, "skip_loop_filter", 48);
//设置无packet缓存
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_PLAYER, "packet-buffering", 0);
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_FORMAT, "fflags", "nobuffer");
//不限制拉流缓存大小
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_PLAYER, "infbuf", 1);
//设置最大缓存数量
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_FORMAT, "max-buffer-size", 1024);
//设置最小解码帧数
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_PLAYER, "min-frames", 3);
//启动预加载
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_PLAYER, "start-on-prepared", 1);
//设置探测包数量
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_FORMAT, "probsize", "4096");
//设置分析流时长
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_FORMAT, "analyzeduration", "2000000");
```

值得注意的是，ijkPlayer默认使用udp拉流，因为速度比较快。如果需要可靠且减少丢包，可以改为tcp协议：

```text
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_FORMAT, "rtsp_transport", "tcp");
```

另外，可以这样开启硬解码，如果打开硬解码失败，再自动切换到软解码：

```text
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_PLAYER, "mediacodec", 0);
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_PLAYER, "mediacodec-auto-rotate", 0);
mediaPlayer.setOption(IjkMediaPlayer.OPT_CATEGORY_PLAYER, "mediacodec-handle-resolution-change", 0);
```

## 3.网络抖动的丢包

在拉流时，音频流、视频流是单独保存到缓冲队列的。如果发生网络抖动，就会引起缓冲抖动（JitBuffer），可以总结为网络卡顿导致音视频缓冲队列增大，从而导致解码滞后、播放滞后。此时，我们需要主动丢包来跟进当前时间戳。因为音视频同步一般以音频时钟为基准，人们对音频更加敏感，所以我们优先丢掉视频队列的包。但是，丢视频数据包时，需要丢掉整个GOP的数据包，因为B帧、P帧依赖I帧来解码，否则会引起花屏。有一位开发者叫做暴走大牙，他的一篇关于ijkPlayer直播延时的文章写得很好：ijkplay播放直播流延时控制小结

## 4.解码器设为零延时

大家应该听过编码器的零延时（zerolatency），但可能没听过解码器零延时。其实解码器内部默认会缓存几帧数据，用于后续关联帧的解码，大概是3-5帧。经过反复测试，发现解码器的缓存帧会带来100多ms延时。也就是说，假如能够去掉缓存帧，就可以减少100多ms的延时。而在avcodec.h文件的AVCodecContext结构体有一个参数（flags）用来设置解码器延时：

```text
typedef struct AVCodecContext {
......
int flags;
......
}
```

为了去掉解码器缓存帧，我们可以把flags设置为CODEC_FLAG_LOW_DELAY。在初始化解码器时进行设置：

```text
//set decoder as low deday
codec_ctx->flags |= CODEC_FLAG_LOW_DELAY;
```

## 5.减少FFmpeg拆帧等待延时

FFmpeg拆帧是根据下一帧的起始码来作为当前帧结束符，起始码一般是：0x00 0x00 0x00 0x01或者0x00 0x00 0x01。这样就会带来一帧的延时，这一帧延时能不能去掉呢？如果有帧结束符，我们以帧结束符来拆帧，这样做就能解决一帧延时。现在，问题变成找到帧结束符，然后替换成下一帧起始码来拆帧。整个调用流程是：read_frame—>read_frame_internal—>parse_packet—>av_parser_parse2—>parser_parse—>ff_combine_frame. 流程图如下：

![img](https://pic1.zhimg.com/80/v2-70f418b9043cce3d058320aa666c80f0_720w.webp)

### 5.1 找到当前帧结束符

在rtpdec.c文件的rtp_parse_packet_internal方法里，有获取帧结束符，也就是mark标志位，我们在这里设一个全局变量：

```text
static int rtp_parse_packet_internal(RTPDemuxContext *s, AVPacket *pkt,
                                     const uint8_t *buf, int len)
{
    ......
 
    if (buf[1] & 0x80)
        flags |= RTP_FLAG_MARKER;
    //the end of a frame
    mark_flag = flags;
 
    ......
}
```

### 5.2 去掉parse_packet的while循环

我们在外部调用libavformat模块的utils.c文件的read_frame读取一帧数据，而read_frame调用内部方法read_frame_internal，read_frame_internal接着调用parse_packet方法，在该方法里有一个while循环体。现在把循环体去掉，并且释放申请的内存：

```text
static int parse_packet(AVFormatContext *s, AVPacket *pkt, int stream_index)
{
    ......
 
//    while (size > 0 || (pkt == &flush_pkt && got_output)) {
        int len;
        int64_t next_pts = pkt->pts;
        int64_t next_dts = pkt->dts;
 
        av_init_packet(&out_pkt);
        len = av_parser_parse2(st->parser, st->internal->avctx,
                               &out_pkt.data, &out_pkt.size, data, size,
                               pkt->pts, pkt->dts, pkt->pos);
        pkt->pts = pkt->dts = AV_NOPTS_VALUE;
        pkt->pos = -1;
        /* increment read pointer */
        data += len;
        size -= len;
 
        got_output = !!out_pkt.size;
 
        if (!out_pkt.size){
            av_packet_unref(&out_pkt);//release current packet
            av_packet_unref(pkt);//release current packet
            return 0;
//            continue;
        }
    ......        
   
        ret = add_to_pktbuf(&s->internal->parse_queue, &out_pkt,
                            &s->internal->parse_queue_end, 1);
        av_packet_unref(&out_pkt);
        if (ret < 0)
            goto fail;
//    }
 
    /* end of the stream => close and free the parser */
    if (pkt == &flush_pkt) {
        av_parser_close(st->parser);
        st->parser = NULL;
    }
 
fail:
    av_packet_unref(pkt);
    return ret;
}
```

### 5.3 修改av_parser_parse2的帧偏移量

在libavcodec模块的parser.c文件中，parse_packet调用到av_parser_parse2来解释数据包，该方法内部有记录帧偏移量。原先是等待下一帧的起始码，现在改为当前帧结束符，所以要把下一帧起始码这个偏移量长度去掉：

```text
int av_parser_parse2(AVCodecParserContext *s, AVCodecContext *avctx,
                     uint8_t **poutbuf, int *poutbuf_size,
                     const uint8_t *buf, int buf_size,
                     int64_t pts, int64_t dts, int64_t pos)
{
    ......
 
    /* WARNING: the returned index can be negative */
    index = s->parser->parser_parse(s, avctx, (const uint8_t **) poutbuf,
                                    poutbuf_size, buf, buf_size);
    av_assert0(index > -0x20000000); // The API does not allow returning AVERROR codes
#define FILL(name) if(s->name > 0 && avctx->name <= 0) avctx->name = s->name
    if (avctx->codec_type == AVMEDIA_TYPE_VIDEO) {
        FILL(field_order);
    }
 
    /* update the file pointer */
    if (*poutbuf_size) {
        /* fill the data for the current frame */
        s->frame_offset = s->next_frame_offset;
 
        /* offset of the next frame */
//        s->next_frame_offset = s->cur_offset + index;
        //video frame don't plus index
        if (avctx->codec_type == AVMEDIA_TYPE_VIDEO) {
            s->next_frame_offset = s->cur_offset;
        }else{
            s->next_frame_offset = s->cur_offset + index;
        }
        s->fetch_timestamp   = 1;
    }
    if (index < 0)
        index = 0;
    s->cur_offset += index;
    return index;
}
```

### 5.4 去掉parser_parse的寻找帧起始码

av_parser_parse2调用到parser_parse方法，而我们这里使用的是h264解码，所以在libavcodec模块的h264_parser.c有一个结构体ff_h264_parser，把h264_parse赋值给parser_parse：

```text
AVCodecParser ff_h264_parser = {
    .codec_ids      = { AV_CODEC_ID_H264 },
    .priv_data_size = sizeof(H264ParseContext),
    .parser_init    = init,
    .parser_parse   = h264_parse,
    .parser_close   = h264_close,
    .split          = h264_split,
};
```

现在我们需要h264_parser.c文件的h264_parse方法，去掉寻找下一帧起始码作为当前帧结束符的过程：

```text
static int h264_parse(AVCodecParserContext *s,
                      AVCodecContext *avctx,
                      const uint8_t **poutbuf, int *poutbuf_size,
                      const uint8_t *buf, int buf_size)
{
    ......
 
    if (s->flags & PARSER_FLAG_COMPLETE_FRAMES) {
        next = buf_size;
    } else {
//TODO:don't use next frame start code, modify by xufulong
//        next = h264_find_frame_end(p, buf, buf_size, avctx);
 
        if (ff_combine_frame(pc, next, &buf, &buf_size) < 0) {
            *poutbuf      = NULL;
            *poutbuf_size = 0;
            return buf_size;
        }
 
/*        if (next < 0 && next != END_NOT_FOUND) {
            av_assert1(pc->last_index + next >= 0);
            h264_find_frame_end(p, &pc->buffer[pc->last_index + next], -next, avctx); // update state
        }*/
    }
 
    ......
}
```

### 5.5 修改parser.c的组帧方法

h264_parse又调用parser.c的ff_combine_frame组帧方法，我们在这里把mark替换起始码作为帧结束符：

```text
external int mark_flag;//引用全局变量
 
int ff_combine_frame(ParseContext *pc, int next,const uint8_t **buf, int *buf_size)
{
    ......
 
    /* copy into buffer end return */
//    if (next == END_NOT_FOUND) {
        void *new_buffer = av_fast_realloc(pc->buffer, &pc->buffer_size,
                                           *buf_size + pc->index +
                                           AV_INPUT_BUFFER_PADDING_SIZE);
 
        if (!new_buffer) {
          
            pc->index = 0;
            return AVERROR(ENOMEM);
        }
        pc->buffer = new_buffer;
        memcpy(&pc->buffer[pc->index], *buf, *buf_size);
        pc->index += *buf_size;
//        return -1;
          if(!mark_flag)
            return -1;
        next = 0;
//    }
 
    ......
 
}
```

经过以上修改，局域网用电脑推送1080P、30fps的视频流，Android设备拉流解码播放，整体延时可优化至130ms左右。而手机推流，延时可达到86ms。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/457790444

# 【NO.460】H264解码之FFmepg解码ES数据

## 1.**前言**

在项目开发中，我们收到的数据流，一般是RTP数据流，所以在解码过程中，我们分为以下几个步骤：

解析RTP数据包，去RTP头，得到PS数据

解析PS数据，去掉PS头，得到PES数据

解析PES数据，去掉PES头，得到ES数据

解析ES数据，如解析出：PPS、SPS、IDR、P等

本文，我们就来探究怎么用FFmepg解析ES数据。

## 2.解析ES数据

在解析ES数据之前，我们会解析PES，在解析PES过程中，会解析出PTS、DTS、ES数据包、ES数据包长度。

接下来就开始正式解码了。

现在很多人或者说网上很多讲的还是用的FFmepg3.X.X以前的接口，如avcodec_decode_video2()，甚至更早的。

我们这里两种方法都说一说。

首先使用avcodec_decode_video2()方式：

**代码片段：**

```text
void decode(unsigned char * stream, long len, unsigned __int64 pts, long pts_ref_distance, unsigned __int64 dts)
{
	int ret;
 	m_avpacket.data = stream;
 	m_avpacket.size = len;
	
	//这里需要前面pes解析出的pts、dts进行显示控制，要不然，视频帧会时快时慢 
 	m_avpacket.pts = pts;
 	m_avpacket.dts = dts;
 	int got_picture = 0;
 	long diff = 0;
 
 	int re = avcodec_decode_video2(m_avcodeccontext, m_avframe, &got_picture, &m_avpacket);
	if (got_picture > 0 && re > 0)
	{
		__int64 the_pts = 0;
		the_pts = av_frame_get_best_effort_timestamp(m_avframe);
		if (0 == m_last_pts)
		{
			m_last_pts = the_pts;
		}
		diff = (long)(abs(the_pts - m_last_pts) *1000.0 / 90000);
		m_last_pts = the_pts;
		long lStoreLen = m_avframe->width * m_avframe->height * 3 >> 1;

		if (m_avframe->width > 0 && m_avframe->height > 0 && lStoreLen > YUV_BUF)
		{
			if (lStoreLen > YUV_BUF_MAX )
			{
				OutputDebugStringA("yuv buf overflow 20M!");
				return;
			}
			delete[] m_dest;
			m_dest = new unsigned char[lStoreLen];
		}
		//这里解析出来的数据通过av_image_copy_to_buffer拷贝为YUV数据
		AVPixelFormat fmt = (AVPixelFormat)m_avframe->format;
        av_image_copy_to_buffer(m_dest, lStoreLen, 
            m_avframe->data, m_avframe->linesize, 
            fmt, m_avframe->width, m_avframe->height, 1);
		/*................................
		//这里就可以将YUV数据行进操作了
		................................*/
	}
	else
	{
		OutputDebugStringA("****decode failed\n");
	}
}
```

接下来我们说说新方式，就是代替官方推荐的代替avcodec_decode_video2方式：

![img](https://pic2.zhimg.com/80/v2-5ad76c28f7ff19c6125766607e830d5d_720w.webp)

**代码片段：**

```text
void decode(unsigned char * stream, long len, unsigned __int64 pts, long pts_ref_distance, unsigned __int64 dts)
{
	int ret;
	long diff = 0;
	while (len > 0)
	{
		ret = av_parser_parse2(m_avCodePCparser, m_avcodeccontext, &m_avPpkt->data, &m_avPpkt->size,stream, len,pts, dts, 0);
		if (ret < 0)
		{
			OutputDebugStringA("Error while parsing\n");
		}

		stream += ret;
		len -= ret;

		if (m_avPpkt->size)
		{
			m_avPpkt->pts = pts;
			m_avPpkt->dts = dts;

			ret = avcodec_send_packet(m_avcodeccontext, m_avPpkt);
			if (ret < 0)
			{
				OutputDebugStringA("Error submitting the packet to the decoder\n");
			}

			while (ret == 0) {
				ret = avcodec_receive_frame(m_avcodeccontext, m_avframe);
				if (ret == AVERROR(EAGAIN) || ret == AVERROR_EOF)
					continue;
				else if (ret < 0)
				{
					OutputDebugStringA("Error during decoding\n");
				}

				if (ret >= 0)
				{
					__int64 the_pts = 0;
					the_pts = av_frame_get_best_effort_timestamp(m_avframe);
					if (0 == m_last_pts)
					{
						m_last_pts = the_pts;
					}
					diff = (long)(abs(the_pts - m_last_pts) *1000.0 / 90000);
					m_last_pts = the_pts;

					int data_size = av_image_get_buffer_size((AVPixelFormat)m_avframe->format, m_avframe->width, m_avframe->height, 1);

					if (m_avframe->width > 0 && m_avframe->height > 0 && data_size > YUV_BUF)
					{
						delete[] m_dest;
						m_dest = new unsigned char[data_size];
					}

					AVPixelFormat fmt = (AVPixelFormat)m_avframe->format;
					av_image_copy_to_buffer(m_dest, data_size,
						m_avframe->data, m_avframe->linesize,
						fmt, m_avframe->width, m_avframe->height, 1);
					/*................................
					//这里就可以将YUV数据行进操作了
					................................*/
				}
			}
		}
	}
	
	return;
}
```

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/457349444

# 【NO.461】YUV与RGB的格式/采样方式/存储方式

## 1.YUV与RGB

YUV,RGB是色彩空间模型,而BMP,PNG,JPEG,GIF是图像的文件存储格式。

RGB(RED GREEN BLUE),我们都知道任何颜色都可以通过原色 red green blue通过不同的比例混合出来,三个色基都为零,混合为黑色, 三个色基都最强,混合为白色.这种简单的色彩空间模型广泛应用与计算机.

YUV, Y表示亮度,U V表示色差信息(分别表示blue 和 Red的色差信息), 因为科学发现,人类的研究对色彩的亮度信息比较敏感,对色差信息相对比没那么敏感.亮度信号是强度的感觉，它和色度信号断开，这样的话强度就可以在不影 响颜色的情况下改变。YUV 格式通常用于 PAL制，即欧洲的电视传输标准，而且缺省情况下是图像和视频压缩的标准。YUV 使用RGB的信息，但它从全彩色图像中产生一个黑白图像，然后提取出三个主要的颜色变成两个额外的信号来描述颜色。把这三个信号组合回来就可以产生一个全 彩色图像。

YCbCr 是在世界数字组织视频标准研制过程中作为ITU - R BT1601 建议的一部分, 其实是YUV经过缩放和偏移的翻版。其中Y与YUV 中的Y含义一致, Cb , Cr 同样都指色彩, 只是在表示方法上不同而已。在YUV 家族中, YCbCr 是在计算机系统中应用最多的成员, 其应用领域很广泛,JPEG、MPEG均采用此格式。一般人们所讲的YUV大多是指YCbCr。

现在很多人都将YCbCr跟YUV混为一谈,因为YCbCr是根据YUV演变出来的,而目前YCbCr应用更加普遍,很多时候说的YUV指的是YCbCr.

## 2.RGB的格式

常用的RGB存储格式有如下

- RGB565 每个像素用16位表示，RGB分量分别使用5位、6位、5位
- RGB555 每个像素用16位表示，RGB分量都使用5位（剩下1位不用）
- RGB24 每个像素用24位表示，RGB分量各使用8位
- RGB32 每个像素用32位表示，RGB分量各使用8位（剩下8位不用）
- ARGB32 每个像素用32位表示，RGB分量各使用8位（剩下的8位用于表示Alpha通道值）

## 3.YUV的格式

YUV格式有两大类：planar和packed。

- 对于planar的YUV格式，先连续存储所有像素点的Y，紧接着存储所有像素点的U，随后是所有像素点的V。
- 对于packed的YUV格式，每个像素点的Y,U,V是连续交叉存储的。

## 4.YUV的采样方式

YUV码流的存储格式其实与其采样的方式密切相关，主流的采样方式有三种，YUV4:4:4，YUV4:2:2，YUV4:2:0，如何根据其采样格式来从码流中还原每个像素点的YUV值，因为只有正确地还原了每个像素点的YUV值，才能通过YUV与RGB的转换公式提取出每个像素点的RGB值，然后显示出来。

用三个图来直观地表示采集的方式吧，以黑点表示采样该像素点的Y分量，以空心圆圈表示采用该像素点的UV分量。

![img](https://pic2.zhimg.com/80/v2-6fe7bd6c885fd6eaa768c5b87e8c15a5_720w.webp)

- YUV 4:4:4采样，每一个Y对应一组UV分量，每像素32位
- YUV 4:2:2采样，每两个Y共用一组UV分量，每像素16位
- YUV 4:2:0采样，每四个Y共用一组UV分量，每像素16位

平常所讲的YUV A:B:C的意思一般是指基于4个象素来讲,其中Y采样了A次，U采样了B次,V采样了C次.

YUV 格式可以分为打包格式packed format和平面格式planar format。打包格式将YUV分量存放在同一个数组中，通常是几个相邻的像素组成一个宏像素（macro-pixel）；而平面格使用三个数组分开存放YUV三个分量，就像是一个三维平面一样。Packed format和planner format的区别在于，packed format中的YUV是混合在一起的，因此就有了UYVY、YUYV等等，他们在码流中排列的方式有所不同。而对于planner format每一个Y分量，U分量和V分量都是以独立的平面组织的，也就是说所有的U分量都在Y分量之后出现，而V分量在所有的U分量之后。就像三个大色块一样。

## 5.YUV的存储方式

下面用图的形式给出常见的YUV码流的存储方式，并在存储方式后面附有取样每个像素点的YUV数据的方法，其中，Cb、Cr的含义等同于U、V。YUV的存储格式实在是太多了,这里我简单列举几个,其他都类似.

YUYV 格式 （属于YUV422）

![img](https://pic3.zhimg.com/80/v2-46489d8be9aeb08bb62f0225a84d5132_720w.webp)

### 5.1 YUV422P（属于YUV422）

![img](https://pic1.zhimg.com/80/v2-ac35307cc2c78820234fb4f2bac27fe8_720w.webp)

YUV422P也属于YUV422的一种，它是一种Plane模式，即打包模式，并不是将YUV数据交错存储，而是先存放所有的Y分量，然后存储所有的U（Cb）分量，最后存储所有的V（Cr）分量，如上图所示。其每一个像素点的YUV值提取方法也是遵循YUV422格式的最基本提取方法，即两个Y共用一个UV。比如，对于像素点Y’00、Y’01 而言，其Cb、Cr的值均为 Cb00、Cr00。

以YUV420 planar数据为例， 以720×480大小图象YUV420 planar为例，

其存储格式是： 共大小为(720×480×3>>1)字节，

分为三个部分:Y,U和V

Y分量： (720×480)个字节

U(Cb)分量：(720×480>>2)个字节

V(Cr)分量：(720×480>>2)个字节

三个部分内部均是行优先存储，三个部分之间是Y,U,V 顺序存储。

即YUV数据的0－－720×480字节是Y分量值，

720×480－－720×480×5/4字节是U分量

720×480×5/4 －－720×480×3/2字节是V分量。

NV21(属于YUV420)

![img](https://pic1.zhimg.com/80/v2-b0188009afdec90db2f3960f08e792a4_720w.webp)

NV21属于YUV420格式，是一种two-plane模式，即Y和UV分为两个Plane，但是UV（CbCr）为交错存储，而不是分为三个plane。其提取方式与上一种类似，即Y’00、Y’01、Y’10、Y’11共用Cr00、Cb00

YUV420 planar数据存储， 以720×488大小图象YUV420 planar为例，

其存储格式是： 共大小为(720×480×3>>1)字节，

分为三个部分: Y分量：　　 　(720×480)个字节 U(Cb)分量：　　(720×480>>2)个字节　　　　 V(Cr)分量：　　 (720×480>>2)个字节

三个部分内部均是行优先存储，三个部分之间是Y,U,V 顺序存储。

即YUV数据的0－－720×480字节是Y分量值， 720×480－－720×480×5/4字节是U分量 720×480×5/4 －－720×480×3/2字节是V分量。

## 6.YV12

![img](https://pic4.zhimg.com/80/v2-a79ba08deec44d518347c0fe6cee62ef_720w.webp)

YV12属于YUV420格 式，也是一种Plane模式，将Y、U、V分量分别打包，依次存储。其每一个像素点的YUV数据提取遵循YUV420格式的提取方式，即4个Y分量共用一 组UV。注意，上图中，Y’00、Y’01、Y’10、Y’11共用Cr00、Cb00，其他依次类推。

RGB 到 YUV的转换

Y=0.30R+0.59G+0.11B ， U=0.493(B－Y) ， V=0.877(R－Y) , 从公式中，我们关键要理解的一点是，UV / CbCr信号实际上就是蓝色差信号和红色差信号，进而言之，实际上一定程度上间接的代表了蓝色和红色的强度，理解这一点对于我们理解各种颜色变换处理的过程会有很大的帮助。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/455441075

# 【NO.462】【音视频技术】播放器架构设计

## 1.概述

首先，我们了解一下播放器的定义是什么 ？

> “播放器，是指能播放以数字信号形式存储的视频或音频文件的软件，也指具有播放视频或音频文件功能的电子器件产品。” —— 《百度百科》

我的解读如下：“播放器，是指能读取、解析、渲染存储在本地或者服务器上的音视频文件的软件，或者电子产品。”

归纳起来，它主要有如下 3 个方面的功能特性：

读取（IO）：“获取” 内容 -> 从 “本地” or “服务器” 上获取

解析（Parser）：“理解” 内容 -> 参考 “格式&协议” 来 “理解” 内容

渲染（Render）：“展示” 内容 -> 通过扬声器/屏幕来 “展示” 内容

把这 3 个方面的功能串起来，就构成了整个播放器的数据流，如图所示：

![img](https://pic4.zhimg.com/80/v2-a8538ca68d150e575bbf825458c7412b_720w.webp)

IO：负责数据的读取。从数据源读取数据有多种标准协议，比如常见的有：File，HTTP(s)，RTMP，RTSP 等

Parser & Demuxer：负责数据的解析。音视频数据的封装格式，都有着各种业界标准，只需要参考这些行业标准文档，即可解析各种封装格式，比如常见的格式：mp4，flv，m3u8，avi 等

Decoder：其实也属于数据解析的一种，只不过更多的是负责对压缩的音视频数据进行解码，拿到原始的 YUV 和 PCM 数据，常见的视频压缩格式如：H.264、MPEG4、VP8/VP9，音频压缩格式如 G.711、AAC、Speex 等

Render：负责视频数据的绘制和渲染，是一个平台相关的特性，不同的平台有不同的渲染 API 和方法，比如：Windows 的 DDraw/DirectSound，Android 的 SurfaceView/AudioTrack，跨平台的如：OpenGL 和 ALSA 等

下面我们逐一剖析一下播放器整个数据流的每一个模块的输入和输出，并一起设计一下每一个模块的接口 API。



## 2.模块设计

### 2.1 IO 模块

IO 模块的输入：数据源的地址（URL），这个 URL 可以是一个本地的文件路径，也可以是一个网络的流地址。

IO 模块的输出：二进制的数据，即通过 IO 协议读取的音视频二进制数据。

视频数据源的 URL 示例如下：

file:///c:/WINDOWS/clock.avi rtmp://[http://live.hkstv.hk.lxdns.com/live/hks](https://link.zhihu.com/?target=http%3A//live.hkstv.hk.lxdns.com/live/hks) [http://www.w3school.com.cn/i/movie.mp4](https://link.zhihu.com/?target=http%3A//www.w3school.com.cn/i/movie.mp4) [http://devimages.apple.com/iphone/samples/bipbop/bipbopall.m3u8](https://link.zhihu.com/?target=http%3A//devimages.apple.com/iphone/samples/bipbop/bipbopall.m3u8)

综上，播放器 IO 模块的接口设计如下所示：

![img](https://pic1.zhimg.com/80/v2-865d7ae6fa7017dad9445c2b6913fb20_720w.webp)

Open/Close 方法主要是用于打开/关闭视频流，播放器内核可以通过 URL 的头（Schemes）知道需要采用哪一种 IO 协议来拉流（如：FILE/RTMP/HTTP），然后通过继承本接口的子类去完成实际的协议解析和数据读取。

IO 模块读取数据，则定义了 2 个方法，Read 方法用于顺序读取数据，ReadAt 用于从指定的 Offset 偏移的位置读取数据，后者主要用于文件或者视频点播，为播放器提供 Seek 能力。

对于网络流，可能出现断线的情况，因此独立出一个 Reconnect 接口，用于提供重连的能力。

### 2.2 解析模块

![img](https://pic4.zhimg.com/80/v2-220a675c0f8bdafa56e4647c09fe0adf_720w.webp)

从 IO 模块读到的音视频二进制数据，其实都是用如 mp4、flv、avi 等格式封装起来的，如果想分离出音频包和视频包，则需要通过一个 Parser & Demuxer 模块进行解析。

解析模块的输入：由 IO 模块读取出来的 bytes 二进制数据

解析模块的输出：音视频的媒体信息，未解码的音频数据包，未解码的视频数据包

音视频的媒体信息主要包括如下内容：

视频时长、码率、帧率等 

音频的格式：编码算法，采样率，通道数等 

视频的格式：编码算法，宽高，长宽比等

综上，解析模块的接口设计如下图所示：

![img](https://pic2.zhimg.com/80/v2-2f7542a333d90f18c2113971b22dcac5_720w.webp)

创建好解析对象后，通过 Parse 函数输入音视频数据解析出基本的音视频媒体信息，通过 Read 函数读取分离的音视频数据包，然后分别送入音频和视频×××，通过 Get 方法获取各种音视频参数信息。

### 2.3 解码模块

![img](https://pic4.zhimg.com/80/v2-b02300d4dbe4a3fa179ad3bb32cd9197_720w.webp)

解析模块分离好音频和视频包以后，就可以分配送入到音频×××和视频×××了

解码模块的输入：未解压的音频/视频包

解码模块的输出：解压好的音频/图像的原始数据，即 PCM 和 YUV

由于音视频的解码，往往不是每送入×××一帧数据就一定能输出一帧数据，而是经常需要缓存几帧参考帧才能拿到输出，所以编码器的接口设计常常采用一种 “生产者-消费者” 模型，通过一个公共的 buffer 队列来串联 “生产者-消费者”，如下图所述（截取自 Android MediaCodec 编解码库的设计）：

![img](https://pic2.zhimg.com/80/v2-8ef9428b526b22a8e56a2e63be58a291_720w.webp)

综上，解码模块的接口设计如下所示：

![img](https://pic3.zhimg.com/80/v2-3e5e75853576950582ffd569af3546c2_720w.webp)

解析模块输出的媒体信息，包含有该使用什么类型的音频/视频×××，可利用该信息完成×××的初始化。剩下的过程，就是通过 Queue 和 Dequeue 不断跟×××交互，送入未解码的数据，拿到解码后的数据了。

### 2.4 渲染模块

![img](https://pic3.zhimg.com/80/v2-3e025b35d11de92fddd9f115fe5f9b36_720w.webp)

×××输出原始的图像和音频数据后，下一步就是送入到渲染模块进行图像的渲染和音频的播放了。

一般视频数据渲染是输出到显卡展示在窗口上，音频数据则是送入声卡利用扬声器播放出来。虽然不同平台的窗口绘制和扬声器播放的系统层 API 都不太一样，但是接口层面的流程也都差不多，如图所示：

![img](https://pic2.zhimg.com/80/v2-5af3cf28412e6c8394b11b21915a57a9_720w.webp)

对于视频渲染而言，流程则是：Init 初始化 -> SetView 设置窗口对象 -> SetParam 设置渲染参数 -> Render 执行渲染/绘制

![img](https://pic4.zhimg.com/80/v2-e77544143a2fea3a0de929c093c815bf_720w.webp)

对于音频播放而言，流程则是：Init 初始化 -> SetParam 设置播放参数 -> Render 执行播放操作

### 2.5 把模块串起来

![img](https://pic3.zhimg.com/80/v2-738da61d14e2ec35976a7aa5648933de_720w.webp)

如图所示，把各个模块这样串起来后，就是播放器的整个数据流走向了，但这是一个单线程的结构，从 IO 读到数据后，立马送入解析 -> 解码 -> 渲染，这样的单线程结构的播放器设计，会存在如下几个问题：

1. 音视频分离后 -> 解码 -> 播放，中间无法插入逻辑进行音画同步
2. 无数据缓冲区，一旦网络/解码抖动 -> 导致频繁的卡顿
3. 单线程运行，没有充分利用 CPU 多核

要想解决单线程结构的问题，可以以数据的 “生产者 - 消费者” 为边界，添加数据缓冲区，将单线程模型，改造为多线程模型（IO 线程、解码线程、渲染线程），如图所示：

![img](https://pic1.zhimg.com/80/v2-291f1897527db0182911ff7e7ccfbcb4_720w.webp)

改造为多线程模型后，其优势如下：

帧队列（Packet Queue）：可抵抗网络抖动

显示队列（Frame Queue）：可抵抗解码/渲染的抖动

渲染线程：添加 AV Sync 逻辑，可支持音画同步的处理

并行工作，高效，充分利用多核 CPU

注：我们将在下一篇文章专门来聊一聊这 2 个新增的缓冲区该如何设计和管理。

## 3.播放器 SDK 接口设计

前面详细介绍了播放器内涵的关键架构设计和数据流，如果期望以该播放器内核作为 SDK 给 APP 提供底层能力的话，还需要设计一套易用的 API 接口，这套 API 接口，其实可抽象为如下 5 大部分：

创建/销毁播放器

配置参数（如：窗口句柄、视频 URL、循环播放等）

发送命令（如：初始化，开始播放，暂停播放，拖动，停止等）

音视频数据回调（如：解码后的音视频数据回调）

消息/状态消息回调（如：缓冲开始/结束、播放完成等）

综上，播放器常见接口列表如下：

Create/Release/Reset

SetDataSource/SetOptions/SetView/SetVolume

Prepare/Start/Pause/Stop/SeekTo

SetXXXListener/OnXXXCallback

## 4.播放器的状态模型

总体来说，播放器其实是一个状态机，被创建出来了以后，会根据应用层发送给它的命令以及自身产生的事件在各个状态之间切换，可以用如下这张图来展示：

![img](https://pic4.zhimg.com/80/v2-be98c67242f06757613dd883b5513cc3_720w.webp)

播放器一共有 9 种状态，其中，Idle 是创建后/重置后的到达的初始状态，End 和 Error 分别是主动销毁播放器和发生错误后进入的最终状态（通过 reset 重置后可恢复 Idle 状态）

其他的状态切换和达到方式，图中已经标注得比较清楚了，这里就不再赘述了。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/454178003

# 【NO.463】Nginx搭建RTMP推拉流服务器

## 1.安装Nginx

在Mac上有一个很好用的包管理插件，名为homebrew。 具体的安装可以自行去搜索下。下面就借助Homebrew来安装Nginx。

### 1.1 首先是拉取Nginx

```text
$ brew tap home/nginx
```

### **1.2 执行安装**

```text
$ brew install nginx-full --with-rtmp-module
```

这里需要注意的就是后面的–with-rtmp-module参数，其意思就是带上rtmp的模块，这样我们才能借助Nginx实现一个rtmp的推拉流服务器。

安装过程中，homebrew或帮我们自动的安装如pcre,openssl等模块。因此相对于其他平台的安装方式或者源码安装方式，homebrew贼省心。

经过稍长的等待时间，带有rtmp模块的Nginx就安装好了。查看安装详情的命令为：

```text
brew info nginx-full
```

就可以看到具体的安装信息了，比如配置文件在哪里，可执行文件又在哪里。

我这里有如下路径：

\- 配置文件路径 /usr/local/etc/nginx/

\- web容器路径 /usr/local/var/www

\- 可执行文件路径/usr/local/Ceallar/nginx/

配置rtmp

在nginx.conf的HTTP节点后面添加一个同级别的rtmp接单。具体内容如下：

```text
#user  nobody;
worker_processes  1;

#error_log  logs/error.log;
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;

error_log   /usr/local/var/logs/nginx/error.log debug;
pid         /usr/local/var/run/nginx.pid;

#pid        logs/nginx.pid;


events {
    worker_connections  256;
}


http {
    include       mime.types;
    default_type  application/octet-stream;

    log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for"';

    access_log   /usr/local/var/logs/access.log  main;
    #access_log  logs/access.log  main;

    sendfile        on;
    port_in_redirect off;
    #tcp_nopush     on;

    #keepalive_timeout  0;
    keepalive_timeout  65;

    #gzip  on;

    server {
        listen       8080;
        server_name  localhost;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location / {
            #root   html;
            root    /usr/local/var/www;
            index  index.html index.htm index.php;
        }

        #error_page  404              /404.html;

        # redirect server error pages to the static page /50x.html
        #
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   /usr/local/var/www;
        }

        # proxy the PHP scripts to Apache listening on 127.0.0.1:80
        #
        location ~ \.php$ {
            proxy_pass   http://127.0.0.1;
        }

        # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000
        #
        location ~ \.php$ {
            fastcgi_intercept_errors on;
            #root           html;
            root            /usr/local/var/www;
            fastcgi_pass   127.0.0.1:9000;
            #fastcgi_pass unix:/run/php/php7.0-fpm.sock;
            fastcgi_index  index.php;
            fastcgi_param  SCRIPT_FILENAME  $document_root$fastcgi_script_name;
            include        fastcgi_params;
        }

        # deny access to .htaccess files, if Apache's document root
        # concurs with nginx's one
        #
        #location ~ /\.ht {
        #    deny  all;
        #}
    }


    # another virtual host using mix of IP-, name-, and port-based configuration
    #
    #server {
    #    listen       8000;
    #    listen       somename:8080;
    #    server_name  somename  alias  another.alias;

    #    location / {
    #        root   html;
    #        index  index.html index.htm;
    #    }
    #}


    # HTTPS server
    #
    #server {
    #    listen       443 ssl;
    #    server_name  localhost;

    #    ssl_certificate      cert.pem;
    #    ssl_certificate_key  cert.key;

    #    ssl_session_cache    shared:SSL:1m;
    #    ssl_session_timeout  5m;

    #    ssl_ciphers  HIGH:!aNULL:!MD5;
    #    ssl_prefer_server_ciphers  on;

    #    location / {
    #        root   html;
    #        index  index.html index.htm;
    #    }
    #}
    include servers/*;
}

rtmp {
server {
  listen 1935;
  chunk_size 4000;
  application rtmplive {
    live on;
    max_connections 1024;
  }

  application hls {
    live on;
    hls on;
    hls_path /usr/local/var/www/hls;
    hls_fragment 1s;
  }
}
}
```

最后面hls_path就是待会要用到的推流文件目录了。一般来说不必创建，如果出现文件夹权限问题的话，手动添加下可读可写权限就可以了。

## 2.安装ffmpeg

安装它在其他的平台上可能会超级费劲，但是在Mac上，有了homebrew，那就真的不是事了。

```text
➜  $/opt nginx brew install ffmpeg
Updating Homebrew...
==> Auto-updated Homebrew!
Updated 1 tap (caskroom/cask).

==> Installing dependencies for ffmpeg: lame, x264, xvid
==> Installing ffmpeg dependency: lame
==> Downloading https://homebrew.bintray.com/bottles/lame-3.99.5.high_sierra.bottle.1.tar.gz
######################################################################## 100.0%
==> Pouring lame-3.99.5.high_sierra.bottle.1.tar.gz
��  /usr/local/Cellar/lame/3.99.5: 27 files, 2MB
==> Installing ffmpeg dependency: x264
==> Downloading https://homebrew.bintray.com/bottles/x264-r2795.high_sierra.bottle.tar.gz
######################################################################## 100.0%
==> Pouring x264-r2795.high_sierra.bottle.tar.gz
��  /usr/local/Cellar/x264/r2795: 11 files, 3.2MB
==> Installing ffmpeg dependency: xvid
==> Downloading https://homebrew.bintray.com/bottles/xvid-1.3.4.high_sierra.bottle.tar.gz
######################################################################## 100.0%
==> Pouring xvid-1.3.4.high_sierra.bottle.tar.gz
��  /usr/local/Cellar/xvid/1.3.4: 10 files, 1.2MB
==> Installing ffmpeg
==> Downloading https://homebrew.bintray.com/bottles/ffmpeg-3.4.high_sierra.bottle.tar.gz
######################################################################## 100.0%
==> Pouring ffmpeg-3.4.high_sierra.bottle.tar.gz
��  /usr/local/Cellar/ffmpeg/3.4: 248 files, 50.9MB
```

## 3.安装VLC客户端

VLC客户端是一个很好用的可以拉流并进行读取的软件，Mac上挺好用的。

## 4.开始推流，拉流

### 4.1 推流

推流之前，先准备一个视频软件。我就直接用QQ的录屏来录制了一个视频，放在桌面上，名为demo.mp4

然后在命令行里面输入：

```text
ffmpeg -re -i 本地视频路径如(如/Users/changba/Desktop/Player/demo.mp4)  -vcodec copy -f flv rtmp://localhost:1935/rtmplive/home
```

> 这里rtmplive是上面的配置文件中,配置的应用的路径名称;后面的room可以随便写，待会使用拉流软件的时候把地址对应上就可以了。

### 4.2 rtmp的配置

输入完之后，就可以打开VLC客户端了。

### 4.3 推流效果

### 4.4 拉流

具体操作为：file–>>Open Network
然后在弹出的URL框中输入如下链接。

```text
rtmp://localhost:1935/rtmplive/home
```

记得对应上名字就可以了,大致的拉流效果大家可以自己试下

## 5.总结

至此，基于rtmp的推拉流的Nginx服务器就算是完成了。如果感兴趣不妨来尝试一下，其实还是挺有意思的。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/451814135

# 【NO.464】FFMPEG 之 AVDevice

## 1.综述

在使用FFMPEG 作为编码时，可以采用FFMPEG 采集本地的音视频设备的数据，然后进行编码，封装，传输等操作。 在不同的平台上，音视频数据采集驱动并不一致。 在linux 平台上有fbdex,v4l2,x11grab ，在ios 上有avfoundation. 在window 平台上有dshow,vfwcap.gdigrab。

![img](https://pic4.zhimg.com/80/v2-3880ea1dc2ebf76fe33b0f144d4ccf83_720w.webp)

## 2.使用流程

对于FFMPEG 来说，device 也是一个文件， 在使用device 进行录屏，摄像时， 与普通播放一个片源差不多。 就初始化处稍有点区别。

播放普通片源，初始化方式：

```text
AVFormatContext *pFormatCtx = avformat_alloc_context();
avformat_open_input(&pFormatCtx, "test.h265",NULL,NULL);
```

而使用device 去录屏时：

```text
AVFormatContext *pFormatCtx = avformat_alloc_context();
AVInputFormat *ifmt=av_find_input_format("vfwcap");
avformat_open_input(&pFormatCtx, 0, ifmt,NULL);
```

可以看出录屏与播放普通视频，就多一个av_find_input_forma() 的调用。

此处仅讲关于FFMPEG 有哪些API ， 关于如何使用FFMPEG 操作device 录像，录屏请参考雷神csdn ，写的非常详细。

## 3.AVDevice 相关API

如下是AVDevice 库开出的API ， 很清晰， 比如查当前平台有哪些可用的device, 可用avdevice_list_devices()

![img](https://pic2.zhimg.com/80/v2-162eeda0c10a15856115719e8f0e5371_720w.webp)

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/451806536

# 【NO.465】【网络通信 -- WebRTC】WebRTC 源码分析 -- 线程相关(线程切换分析 -- MethodCall / ConstMethodCall 类分析)

## **1.MethodCall 类分析**

```text
// 类模板
// C    : 包含方法的类类型
// R    : 方法的返回值类型
// Args : 方法的参数类型
template <typename C, typename R, typename... Args>
// QueuedTask : 是一个接口类型，提供了 Run() 接口函数
class MethodCall : public QueuedTask {
 public:
  // 声明函数对象类型
  typedef R (C::*Method)(Args...);
  // MethodCall 对象构造函数
  MethodCall(C* c, Method m, Args&&... args)
      : c_(c),
        m_(m),
        args_(std::forward_as_tuple(std::forward<Args>(args)...)) {}
 
  // 方法运行管理
  R Marshal(const rtc::Location& posted_from, rtc::Thread* t) {
    // 判断给定线程是否是当前线程
    if (t->IsCurrent()) {
      // 执行给定的方法
      Invoke(std::index_sequence_for<Args...>());
    } else {
      // 将任务加入任务队列
      t->PostTask(std::unique_ptr<QueuedTask>(this));
      // 等待异步任务处理完毕
      event_.Wait(rtc::Event::kForever);
    }
    // 返回处理结果
    return r_.moved_result();
  }
 
 private:
  // 任务的 Run 方法覆写
  bool Run() override {
    // 在任务中执行给定的方法
    Invoke(std::index_sequence_for<Args...>());
    // 触发 event 激活等待的线程
    event_.Set();
    return false;
  }
 
  // 模板函数，执行给定类型 C 的方法 M
  // 实际调用 ReturnType 模板类中的 Invoke 方法
  template <size_t... Is>
  void Invoke(std::index_sequence<Is...>) {
    r_.Invoke(c_, m_, std::move(std::get<Is>(args_))...);
  }
 
  // 类实例
  C* c_;
  // 类中的方法
  Method m_;
  // 返回值类型
  ReturnType<R> r_;
  // 方法的参数
  std::tuple<Args&&...> args_;
  // 事件实例，用于等待异步处理结果
  rtc::Event event_;
};
```

## **2.ConstMethodCall 类分析**

```text
// 类模板
// C    : 包含方法的类类型
// R    : 方法的返回值类型
// Args : 方法的参数类型
template <typename C, typename R, typename... Args>
// QueuedTask : 是一个接口类型，提供了 Run() 接口函数
// 执行类 C 中的 const 方法，不会改变类 C
class ConstMethodCall : public QueuedTask {
 public:
  // 声明函数对象类型，const 方法
  typedef R (C::*Method)(Args...) const;
  // ConstMethodCall 对象构造函数
  ConstMethodCall(const C* c, Method m, Args&&... args)
      : c_(c),
        m_(m),
        args_(std::forward_as_tuple(std::forward<Args>(args)...)) {}
 
  // 方法运行管理
  R Marshal(const rtc::Location& posted_from, rtc::Thread* t) {
    // 判断给定线程是否是当前线程
    if (t->IsCurrent()) {
      // 执行给定的方法
      Invoke(std::index_sequence_for<Args...>());
    } else {
      // 将任务加入任务队列
      t->PostTask(std::unique_ptr<QueuedTask>(this));
      // 等待异步任务处理完毕
      event_.Wait(rtc::Event::kForever);
    }
    // 返回处理结果
    return r_.moved_result();
  }
 
 private:
 // 任务的 Run 方法覆写
  bool Run() override {
    // 在任务中执行给定的方法
    Invoke(std::index_sequence_for<Args...>());
    // 触发 event 激活等待的线程
    event_.Set();
    return false;
  }
 
  // 模板函数，执行给定类型 C 的方法 M
  // 实际调用 ReturnType 模板类中的 Invoke 方法
  template <size_t... Is>
  void Invoke(std::index_sequence<Is...>) {
    r_.Invoke(c_, m_, std::move(std::get<Is>(args_))...);
  }
 
  // 类实例，该类是常量
  const C* c_;
  // 类中的方法
  Method m_;
  // 返回值类型
  ReturnType<R> r_;
  // 方法的参数
  std::tuple<Args&&...> args_;
  // 事件实例，用于等待异步处理结果
  rtc::Event event_;
};
```

## **3.ReturnType 类分析**

```text
// 类模板 -- 包装方法执行的返回值
template <typename R>
class ReturnType {
 public:
  // 模板函数，执行给定类型的对应的方法
  template <typename C, typename M, typename... Args>
  void Invoke(C* c, M m, Args&&... args) {
    // 实际执行类型 C 中的方法 M
    r_ = (c->*m)(std::forward<Args>(args)...);
  }
 
  // 获取方法运行的返回值
  R moved_result() { return std::move(r_); }
 
 private:
  R r_;
};
 
// 类模板 -- 包装方法执行的返回值
// 特化模板，返回值为 void 的情况
template <>
class ReturnType<void> {
 public:
  // 模板函数，执行给定类型的对应的方法
  template <typename C, typename M, typename... Args>
  void Invoke(C* c, M m, Args&&... args) {
    // 实际执行类型 C 中的方法 M
    (c->*m)(std::forward<Args>(args)...);
  }
 
  void moved_result() {}
};
```

**附录**

**附录一、方法异步处理的实现**

- 说明，方法的异步处理涉及的知识点总结如下

```text
1. 线程发布任务
webrtc/src/rtc_base/thread.cc
Thread::PostTask(std::unique_ptr<webrtc::QueuedTask> task)
 
2. 事件的等待与激活
webrtc/src/rtc_base/event.h
webrtc/src/rtc_base/event.cc
```

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/450505246

# 【NO.466】【网络通信 -- WebRTC】WebRTC 基础知识 -- 基础知识总结【1】WebRTC 简介

## 1.WebRTC 简介

WebRTC (Web Real­Time Communication，网页实时通信) 是 Google 于 2010 以 6829 万美元从 Global IP Solutions 公司购买，并于 2011 年将其开源，旨在建立一个互联网浏览器间的实时通信的平台，让 WebRTC 技术成为 H5 标准之一；

WebRTC 是一个基于浏览器的实时多媒体通信技术，该项技术旨在使 Web 浏览器具备实时通信能力；同时，通过将这些能力封装并以 JavaScript API 的方式开放给 Web 应用开发人员，使得 Web 应用开发人员能够通过 HTML 标签和 JavaScript API 快速地开发出基于 Web 浏览器的实时音视频应用，而无需依赖任何第三方插件；

WebRTC 由 IETF (Internet Engineering Task Force，互联网工程任务组) 和 W3C (World Wide Web Consortium，万维网联盟) 联合负责其标准化工作；

- IETF 定制 WebRTC 的互联网基础协议标准，该标准也被称为 RTC Web (Real-Time Communication in Web-browsers)
- W3C 则负责定制 WebRTC 的客户端 JavaScript API 接口的标准

文章福利：[分享一个每晚8-10点都有的免费技术直播的链接，订阅即可学习~宝藏地址！！](https://link.zhihu.com/?target=https%3A//ke.qq.com/course/3202131%3FflowToken%3D1040743)

下面收藏的一些视频可以加我群领取

![img](https://pic4.zhimg.com/80/v2-f1fddd71350ab3261823c3defb1d3263_720w.webp)

## 2.WebRTC 框架简介

**WebRTC 整体框架图示**

![img](https://pic4.zhimg.com/80/v2-2a02ae4a6a0df5193bcde0f64f3b723b_720w.webp)

### 2.1 Your Web App

Web 开发者开发的程序，Web 开发者可以基于集成 WebRTC 的浏览器提供的 Web API 开发基于视频、音频的实时通信应用

### 2.2 Web API

面向第三方开发者的 WebRTC 标准 API(Javascript)，使开发者能够容易地开发出类似于网络视频聊天的 Web 应用，这些 API 提供三个功能接口，分别是 MediaStream、RTCPeerConnection 和 RTCDataChannel；

MediaStream 接口用于捕获和存储客户端的实时音视频流，便于客户端进行音视频采集和渲染

RTCPeerConnection 接口是 WebRTC 的核心接口，封装了 WebRTC 连接的管理，负责 WebRTC 连接机制的接口

RTCDataChannel 接口是进行 WebRTC 连接数据传输的数据通道接口

### 2.3 WebRTC Native C++ API

本地 C++ API 提供给浏览器厂商、平台 SDK 开发者使用的 C++ API，不同的平台可以通过各自的 C++ 接口调用能力，对其进行上层封装以满足跨平台的需求

Session management/Abstract signaling

会话管理 / 抽象信令，WebRTC 的会话层，主要用于进行信令交互和管理 RTCPeerConnection 的连接状态

### 2.4 VoiceEngine

音频处理引擎，包含一系列音频多媒体处理的框架，包括 Audio Codecs、NetEQ for voice、Acoustic Echo Canceller (AEC) 和 Noise Reduction (NR)

Audio Codecs 是音频编解码器，当前 WebRTC 支持 ilbc、isac、G711、G722 和 opus 等

NetEQ for voice 是自适应抖动控制算法以及语音包丢失隐藏算法，用于适应不断变化的网络环境，从而保持尽可能低的延迟，同时保持最高的语音质量

AEC 是回声消除器，用于实时消除麦克风采集到的回声

NR 是噪声抑制器，用于消除与相关 VoIP 的某些类型的背景噪音

### 2.5 VideoEngine

视频处理引擎，包含一系列视频处理的整体框架，从摄像头采集视频到视频信息网络传输再到视频显示整个完整过程的解决方案，包括 Video Codec、Video Jitter Buffer 和 Image Enhancement

Video Codec 是视频编解码器，当前 WebRTC 支持 VP8、VP9 和 H.264 编解码；

Video Jitter Buffer 是视频抖动缓冲器，用于降低由于视频抖动和视频信息包丢失带来的不良影响；

Image Enhancement 是图像质量增强模块，用于对摄像头采集回来的图像进行处理，包括明暗度检测、颜色增强、降噪处理等；

### 2.6 Transport

数据传输模块，WebRTC 对音视频进行 P2P 传输的核心模块，包括 SRTP、Multiplexing 和 P2P；

SRTP 是基于 UDP 的安全实时传输协议，为 WebRTC 中音视频数据提供安全单播和多播功能

Multiplexing 是多路复用技术，采用多路复用技术能把多个信号组合在一条物理信道上进行传输，减少对传输线路的数量消耗

P2P 是端对端传输技术，WebRTC 的 P2P 技术集成了 STUN、TURN 和 ICE，这些都是针对 UDP 的 NAT 的防火墙穿越方法，是连接有效性的保障

## 3.WebRTC 一对一通话

**一对一通话结构图示**

![img](https://pic1.zhimg.com/80/v2-42f00c6d92e3de214b7ceb310bc08610_720w.webp)

- 两个 WebRTC 终端，负责音视频采集、编解码、NAT 穿越、音视频数据传输；
- 一个 Signal（信令）服务器，负责信令处理，如加入房间、离开房间、媒体协商消息的传递等；
- 一个 STUN/TURN 服务器，负责获取 WebRTC 终端在公网的 IP 地址，以及 NAT 穿越失败后的数据中转；

### 3.1 **一对一通话流程图示**

![img](https://pic2.zhimg.com/80/v2-bc16270cf4fe167a950e3efa15bf8311_720w.webp)

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/445233381

# 【NO.467】神器 ffmpeg——操作视频，极度舒适

现在短视频很流行，有很多视频编辑软件，功能丰富，而我们需要的只是裁剪功能，而且需要用编程的方式调用，那么最合适的莫过于 **ffmpeg[1]** 了。

ffmpeg 是一个命令行工具，功能强大，可以编程调用。

从 ffmpeg 官网上下载对应操作系统的版本，我下的是 **Windows 版[2]**。

下载后解压到一个目录，然后将目录下的 bin，配置到环境变量里。然后打开一个命令行，输入：

```text
> ffmpeg -version
ffmpeg version 2021-10-07-git-b6aeee2d8b-full_build- ...
```

测试一下，能显示出版本信息，说明配置好了。

现在读一下文档，发现拆分视频文件的命令是：

```text
ffmpeg -i [filename] -ss [starttime] -t [length] -c copy [newfilename]
```

- i 为需要裁剪的文件
- ss 为裁剪开始时间
- t 为裁剪结束时间或者长度
- c 为裁剪好的文件存放

好了，用 Python 写一个调用：

```text
import subprocess as sp

def cut_video(filename, outfile, start, length=90):
    cmd = "ffmpeg -i %s -ss %d -t %d -c copy %s" % (filename, start, length, outfile)
    p = sp.Popen(cmd, shell=True)
    p.wait()
    return
```

- 定义了一个函数，通过参数传入 ffmpeg 需要的信息
- 将裁剪命令写成一个字符串模板，将参数替换到其中
- 用 subprocess 的 Popen 执行命令，其中参数 shell=True 表示将命令作为一个整体执行
- p.wait() 很重要，因为裁剪需要一会儿，而且是另起进程执行的，所以需要等执行完成再做后续工作，否则可能找不到裁剪好的文件

这样视频裁剪工作就完成了，然后再看看什么是最重要的。

## 1.计算分段

视频裁剪时，需要一些参数，特别是开始时间，如何确定呢？如果这件事做不好，裁剪工作就很麻烦。

所以看看如何计算裁剪分段。

我需要将视频裁剪成一分半的小段，那么将需要知道目标视频文件的时间长度。

## 2.获取视频长度

如何获得长度呢？ffmpeg 提供了另一个命令 —— ffprobe。

找了一下，可以合成一个命令来获取：

```text
> ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 -i a.flv

920.667
```

命令比较复杂哈，可以先不用管其他参数，只要将要分析的视频文件传入就好了。命令的结果是显示一行视频文件的长度。

于是可以编写一个函数：

```text
import subprocess as sp

def get_video_duration(filename):
    cmd = "ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 -i %s" % filename
    p = sp.Popen(cmd, stdout=sp.PIPE, stderr=sp.PIPE)
    p.wait()
    strout, strerr = p.communicate() # 去掉最后的回车
    ret = strout.decode("utf-8").split("\n")[0]
    return ret
```

- 函数只有一个参数，就是视频文件路径
- 合成命令语句，将视频文件路径替换进去
- 用 subprocess 来执行，注意这里需要设置一下命令执行后的输出
- 用 wait 等待命令执行完成
- 通过 communicate 提取输出结果
- 从结果中提取视频文件的长度，返回

## 3.分段

得到了视频长度，确定好每个分段的长度，就可以计算出需要多少分段了。

代码很简单：

```text
import math
duration = math.floor(float(get_video_duration(filename)))
part = math.ceil(duration / length)
```

注意，计算分段时，需要进行向上取整，即用 ceil，以包含最后的一点尾巴。

得到了需要的分段数，用一个循环就可以计算出每一段的起始时间了。

## 4.获取文件

因为处理的文件很多，所以需要自动获取需要处理的文件。

方法很简单，也很常用，一般可以用 os.walk 递归获取文件，还可以自己写，具体根据实际情况。

```text
for fname in os.listdir(dir):
    fname = os.path.join(dir, os.path.join(dir, fname))
    basenames = os.path.basename(fname).split('.')
    mainname = basenames[0].split("_")[0]
    ...
```

提供视频文件所在的目录，通过 os.listdir 获取目录中的文件，然后，合成文件的绝对路径，因为调用裁剪命令时需要绝对路径比较方便。

获取文件名，是为了在后续对裁剪好的文件进行命名。

## 5.代码集成

现在每个部分都写好了，可以将代码集成起来了：

```text
def main(dir):
    outdir = os.path.join(dir, "output")
    if not os.path.exists(outdir):
        os.mkdir(outdir)

    for fname in os.listdir(dir):
        fname = os.path.join(dir, os.path.join(dir, fname))
        if os.path.isfile(fname):
            split_video(fname, outdir)
```

- main 方法是集成后的方法
- 先创建一个裁剪好的存储目录，放在视频文件目录中的 output 目录里
- 通过 listdir 获取到文件后，对每个文件进行处理，其中判断了一下是否为文件
- 调用 split_video 方法开始对一个视频文件进行裁剪

## 6.总结

总体而言，这是个很简单的应用，核心功能就是调用了一个 ffmpeg 命令。

相对于技术，更重要的是如何对一个项目进行分析和分解，以及从什么地方开始。

这里的方式起始时，不断地找最重要地事情，以最重要的事情为线索不断地推进，最终以自下而上地方式解决整个问题。

期望这篇文章对你有所启发，比心。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/429558315

# 【NO.468】音视频面试问题|面试技巧

从事音视频四五年，面试官都会问的面试题都有哪些？
在这里要告诉大家的是对于自己特别熟悉的，拿手的，一定要重点多说一些，其他的，不熟悉的，一带而过。而且自己做的项目，一定要特别熟悉，内部啥原理，甚至代码细节，都会被问到。 还要养成一个好习惯：多做笔记！！！俗话说，好记性不如烂笔头！所以做笔记的习惯一定要有！下面给一些示例给大家讲解一下！！

## **1.常见的音视频格式有哪些？**

**参考答案**

1. MPEG（运动图像专家组）是Motion Picture Experts Group 的缩写。这类格式包括了MPEG-1,MPEG-2和MPEG-4在内的多种视频格式。
2. AVI，音频视频交错(Audio Video Interleaved)的英文缩写。AVI这个由微软公司发布的视频格式，在视频领域可以说是最悠久的格式之一。
3. MOV，使用过Mac机的朋友应该多少接触过QuickTime。QuickTime原本是Apple公司用于Mac计算机上的一种图像视频处理软件。
4. ASF(Advanced Streaming format高级流格式)。ASF 是MICROSOFT 为了和的Real player 竞争而发展出来的一种可以直接在网上观看视频节目的文件压缩格式。
5. WMV，一种独立于编码方式的在Internet上实时传播多媒体的技术标准，Microsoft公司希望用其取代QuickTime之类的技术标准以及WAV、AVI之类的文件扩展名。
6. NAVI，如果发现原来的播放软件突然打不开此类格式的AVI文件，那你就要考虑是不是碰到了n AVI。n AVI是New AVI 的缩写，是一个名为Shadow Realm 的地下组织发展起来的一种新视频格式。
7. 3GP是一种3G流媒体的视频编码格式，主要是为了配合3G网络的高传输速度而开发的，也是目前手机中最为常见的一种视频格式。
8. REAL VIDEO（RA、RAM）格式由一开始就是定位在视频流应用方面的，也可以说是视频流技术的始创者。
9. MKV，一种后缀为MKV的视频文件频频出现在网络上，它可在一个文件中集成多条不同类型的音轨和字幕轨，而且其视频编码的自由度也非常大，可以是常见的DivX、XviD、3IVX，甚至可以是RealVideo、QuickTime、WMV 这类流式视频。
10. FLV是FLASH VIDEO的简称，FLV流媒体格式是一种新的视频格式。由于它形成的文件极小、加载速度极快，使得网络观看视频文件成为可能，它的出现有效地解决了视频文件导入Flash后，使导出的SWF文件体积庞大，不能在网络上很好的使用等缺点。
11. F4V，作为一种更小更清晰，更利于在网络传播的格式，F4V已经逐渐取代了传统FLV，也已经被大多数主流播放器兼容播放，而不需要通过转换等复杂的方式。

## **2.列举一些音频编解码常用的实现方案？**

**参考答案**

- 第一种就是采用专用的音频芯片对 语音信号进行采集和处理，音频编解码算法集成在硬件内部，如 MP3 编解码芯片、语音合成 分析芯片等。使用这种方案的优点就是处理速度块，设计周期短；缺点是局限性比较大，不灵活，难以进行系统升级。
- 第二种方案就是利用 A/D 采集卡加上计算机组成硬件平台，音频编解码算法由计算机上的软件来实现。使用这种方案的优点是价格便 宜，开发灵活并且利于系统的升级；缺点是处理速度较慢，开发难度较大。
- 第三种方案是使用高精度、高速度 的 A/D 采集芯片来完成语音信号的采集，使用可编程的数据处理能力强的芯片来实现语音信号处理的算法，然后 用 ARM 进行控制。采用这种方案的优点是系统升级能力强，可以兼容多种音频压缩格式甚至未来的音频压缩格 式，系统成本较低；缺点是开发难度较大，设计者需要移植音频的解码算法到相应的 ARM 芯片中去。

## **3.请叙述AMR基本码流结构？**

**参考答案**

AMR文件由文件头和数据帧组成，文件头标识占6个字节，后面紧跟着就是音频帧；

格式如下所示：

文件头（占 6 字节）| ：--- | 语音帧1 | 语音帧2 | … |

- 文件头： 单声道和多声道情况下文件的头部是不一致的，单声道情况下的文件头只包括一个Magic number，而多声道情况下文件头既包含Magic number，在其之后还包含一个32位的Chanel description field。多声道情况下的32位通道描述字符，前28位都是保留字符，必须设置成0，最后4位说明使用的声道个数。

语音数据： 文件头之后就是时间上连续的语音帧块了，每个帧块包含若干个8位组对齐的语音帧，相对于若干个声道，从第一个声道开始依次排列。每一个语音帧都是从一个8位的帧头开始：其中P为填充位必须设为0，每个帧都是8位组对齐的。

除此之外还很多**面试题**需要去掌握的，这里我就不一一列举啦！

![img](https://pic2.zhimg.com/80/v2-91f19bb5dca86b219cb1117c868c8541_720w.webp)

![img](https://pic3.zhimg.com/80/v2-57c8d3973ded64074499f3d5799445ce_720w.webp)

![img](https://pic2.zhimg.com/80/v2-223d78688da065a9c060746c48651a59_720w.webp)

![img](https://pic3.zhimg.com/80/v2-2777a6e0e7104730035c6124e0ba4f66_720w.webp)

![img](https://pic1.zhimg.com/80/v2-bd4c021d09a6712c067ed9ebb1ee3904_720w.webp)

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/428517343

# 【NO.469】什么是码率控制? 在视频编码中，码率控制的概念是什么，它是通过什么实现的

码率控制是指视频编码中决定输出码率的过程。首先介绍一下 X264 中使用到的与码率控制相关的几个概念：

CQP(Constant QP) 恒 定QP（Quantization Parameter）,追求量化失真的恒定，瞬时码率会随场景 复杂度而波动，该模式基本被淘汰(被 CRF 取代)，只有用”-pq 0”来进行无损编码还有价值。

CRF(Constant Rate Factor),恒定质量因子，与恒定 QP 类似，但追求主观感知到的质量恒定，瞬时码率也 会随场景复杂度波动。对于快速运动或细节丰富的场景会适当增大量化失真（因为人眼不易注意到），反之 对于静止或平坦区域则减少量化失真。

ABR(Average Bitrate),平均码率，追求整个文件的码率平均达到指定值（对于流媒体有何特殊之处？）。瞬时码率也会随着场景复杂度波动，但最终要受平均值的约束。

CBR(Constant Bitrate),恒定码率。前面几个模式都属于可变码率（瞬时码率在波动），即VBR（Variable Bitrate）；恒定码率与之相对，即码率保持不变。

x264 并没有直接提供 CBR 这种模式，但可以通过在 VBR 模式的基础上做进一步限制来达到恒定码率的目标。CRF 和 ABR 模式都能通过--vbv-maxrate --vbv-bufsize来限制码率波动。

> 关于这几个概念的参考如下：

- 1.Waht are CBR,VBV and CPB?
- 2.FFmpeg and H.264 Encoding Guide
- 3.CRF Guide(Constant Rate Factor in X264 and X265)
- 4.MeGUI/x264 setting

## 1.X264 中码率控制

X264 中对于码率控制方法有三种：X264_RC_CQP、X264_RC_CRF、X264_RC_ABR。默认情况是选择 CRF 方法，设置是在 x264_param_default函数里设置的

```text
param->rc.i_rc_method = X264_RC_CRF;
param->rc.f_rf_constant = 23;
```

关于这三种方法，网上有提到优先级是ABR>CQP>CRF的，但分析 X264 的源码，并没有看出有优先级顺序，关于码率控制方法的设置代码如下：

```text
OPT("bitrate")
{
    p->rc.i_bitrate = atoi(value);
    p->rc.i_rc_method = X264_RC_ABR;
}
OPT2("qp", "qp_constant")
{
    p->rc.i_qp_constant = atoi(value);
    p->rc.i_rc_method = X264_RC_CQP;
}
OPT("crf")
{
    p->rc.f_rf_constant = atof(value);
    p->rc.i_rc_method = X264_RC_CRF;
}
```

## 2.X264 中关于 QP设置

首先看一段 X264 中关于 QP 值的代码，该段代码在x264_ratecontrol_new：

```text
rc->ip_offset = 6.0 * log2f(h->param.rc.f_ip_factor);
rc->pb_offset = 6.0 * log2f(h->param.rc.f_pb_factor);
rc->qp_constant[SLICE_TYPE_P] = h->param.rc.i_qp_constant;
rc->qp_constant[SLICE_TYPE_I] = x264_clip3(h->param.rc.i_qp_constant - rc->ip_offset + 0.5, 0, QP_MAX);
rc->qp_constant[SLICE_TYPE_B] = x264_clip3(h->param.rc.i_qp_constant + rc->pb_offset + 0.5, 0, QP_MAX);
```

从上面的代码可以看出，默认的i_qp_constant或者通过命令行传入的qp qp_constant实际设置的是 P 帧的 QP。I 帧和 B 帧的 QP 设置是根据f_ip_factor f_pb_factor计算得到。

在研究编码算法的时候，一般会选用 CQP 方法，设定 QP 为 24、28、32、36、40等（一般选 4 个 QP 值），然后比较算法优劣。在 X264 中，关于QPmin、QPmax、QPstep的默认设置如下：

```text
param->rc.i_qp_min = 0;
param->rc.i_qp_max = QP_MAX;
param->rc.i_qp_step = 4;
```

### 2.1 QPmin,默认值

\0. 定义 X264 可以使用的最小量化值，量化值越小，输出视频质量越好。当 QP 小于某一个值后， 编码输出的宏块质量与原始块极为相近，此时没必要继续降低 QP。如果开启了自适应量化器（默认开启），不建议 提高 QPmin 的值，因为这会降低平滑背景区域的视觉质量。

### 2.2 QPmax，默认值

\51. 定义 X264 可以使用的最大量化值。默认值 51 是 H.264 规格中可供使用的最大量化值。如果 想要控制 X264 输出的最低品质，可以将此值设置的小一些。QPmin 和 QPmax 在CRF，ABR方法下是有效的，过低的设置 QPmax，可能造成 ABR 码率控制失败。不建议调整该参数。

### 2.3 QPstep，默认值

4.设置两帧间量化值的最大变化幅度。

. 比较三种码率控制方式如下：

| 码率控制方法 | 视觉质量稳定性 | 即时输出码率 | 输出文件大小 |
| ------------ | -------------- | ------------ | ------------ |
|              |                |              |              |

帧间 QP 变化，帧内宏块 QP 不变，输出码率未知，各帧输出视觉质量有变化（高 QP 低码率的情况下会更明显）。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/428259095

# 【NO.470】FFmpeg命令行格式和转码过程

## 1.命令行基本格式为：

```text
ffmpeg [global_options] {[input_file_options] -i input_url} ... {[output_file_options] output_url} ...
```

格式分解如下：

```text
ffmpeg
    global_options
        input1_options -i input1
        input2_options -i input2
        ...
            output1_options output1
            output2_options output2
            ...
```

“ffmpeg” 读取任意数量的输入 “文件”(可以是常规文件、管道、网络流、录制设备等，由 “-i” 选项指定)，写入任意数量的输出 “文件”。命令行中无法被解释为选项(option)的任何元素都会被当作输出文件。

每个输入或输出文件，原则上都可以包含任意数量的流。FFmpeg 中流的类型有五种：视频(video)、音频(audio)、字幕(subtitle)、附加数据(attachment)、普通数据(data)。文件中流的数量和(或)流类型种数的极限值由文件封装格式决定。选择哪一路输入文件的哪一路流输出到哪一路输出，这个选择过程既可以由 FFmpeg 自动完成，也可以通过 “-map” 选项手动指定(后面第 6 节 “流选择” 章节会深入描述)。

注：关于附加数据(attachment)和普通数据(data)的说明如下：

> Attachments could be liner notes, related images, metadata files, fonts, etc. Data tracks would be for things like timecode, navigation items, cmml, streaming tracks. 参考资料[3] “What are the the data and attachment stream type?”

命令行中的输入文件及输入文件中的流都可以通过对应的索引引用，文件、流的索引都是从 0 开始。例如，2:4 表示第 3 个输入文件中的第 5 个流。(后面 6.3 节 “stream specifier” 章节会详细介绍)。

一个通用规则是：**输入/输出选项(options)作用于跟随此选项后的第一个文件。因此，顺序很重要，并且可以在命令行中多次指定同一选项。每个选项仅作用于离此选项最近的下一输入或输出文件。全局选项不受此规则限制。**

不要把输入文件和输出文件混在一起———应该先将输入文件写完，再写输出文件。也不要把不同文件的选项混在一起，各选项仅对其下一输入或输出文件有效，一个选项不能跨越一个文件传递到后续文件。

举几个命令行例子：

■ 设置输出文件码率为 64 kbit/s：

```text
ffmpeg -i input.avi -b:v 64k -bufsize 64k output.avi
```

其中 “-b:v 64k” 和 “-bufsize 64k” 是输出选项。

■ 强制输入文件帧率(仅对 raw 格式有效)是 1 fps，输出文件帧率为 24 fps：

```text
ffmpeg -r 1 -i input.m2v -r 24 output.avi
```

其中 “-r 1” 是输入选项，“-r 24” 是输出选项。

■ 转封装：将 avi 格式转为 mp4 格式，并将视频缩放为 vga 分辨率：

```text
ffmpeg -y -i video.avi -s vga video.mp4
```

其中 “-y” 是全局选项，“-s vga” 是输出选项。

## 2. 转码过程

```text
_______              ______________
|       |            |              |
| input |  demuxer   | encoded data |   decoder
| file  | ---------> | packets      | -----+
|_______|            |______________|      |
                                           v
                                       _________
                                      |         |
                                      | decoded |
                                      | frames  |
                                      |_________|
 ________             ______________       |
|        |           |              |      |
| output | <-------- | encoded data | <----+
| file   |   muxer   | packets      |   encoder
|________|           |______________|
```

“ffmpeg” 调用 libavformat 库(包含解复用器 demuxer)，从输入文件中读取到包含编码数据的包(packet)。如果有多个输入文件，“ffmpeg” 尝试追踪多个有效输入流的最小时间戳(timestamp)，用这种方式实现多个输入文件的同步。

然后编码包(packet)被传递到解码器(decoder)，解码器解码后生成原始帧(frame)，原始帧可以被滤镜(filter)处理(图中未画滤镜)，经滤镜处理后的帧送给编码器，编码器将之编码后输出编码包。最终，由复用器(muxex)将编码包写入特定封装格式的输出文件。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/427312103

# 【NO.471】进程原理及系统调用

# 1.进程概念

操作系统作为硬件的使用层，提供使用硬件资源的能力，进程作为操作系统使用层， 提供使用操作系统抽象出的资源层的能力。
进程：是指计算机中已运行的程序。进程本身不是基本的运行单位，而是线程的容器。 程序本身只是指令、数据及其组织形式的描述，进程才是程序（那些指令和数据）的真正运行实例。

- 进程是处于执行时期的程序。
- 内核调度的对象是线程，不是进程。

## 1.1 进程四要素与线程区别

1. 有一段程序代其执行;
2. 有进程专用的系统堆栈空间;
3. 在内核有task_struct数据结构;
4. **进程有独立的存储空间,拥有专有的用户空间;**
   如果具备前面三条而缺少第四条,那就称为"线程"。如果完全没有用户空间,那就称为"内核线程";如果共享用户空间则就称为"用户线程"。

# 2.进程生命周期

Linux操作系统属于多任务操作系统，系统中的每个进程能够分时复用CPU时间片，通过有效的进程调度策略实现多任务并行执行。而进程在被CPU调度运行，等待CPU资源分配以及等待外部事件时会属于不同的状态。进程之间的状态关系：
运行 ：该进程此刻正在执行。
等待：进程能够运行，但没有得到许可，因为CPU分配给另一个进程。调度器可以在 下一次任务切换时选择该进程。
睡眠 ：进程正在睡眠无法运行，因为它在等待一个外部事件。调度器无法在下一次任 务切换时选择该进程。

【进程状态之间转换】如下:

![在这里插入图片描述](https://img-blog.csdnimg.cn/a643215a5be440bd9e09c201745b2d93.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_19,color_FFFFFF,t_70,g_se,x_16)



# 3.task_struct数据结构

Linux内核涉及进程和程序的所有算法都围绕一个名为task_struct的数据结构建立，该结构定义在include/linux/sched.h中。这是系统中主要的一个结构。在阐述调度器的实现之前，了解一下Linux管理进程的方式是很有必要的。 task_struct包含很多成员，将进程与各个内核子系统联系，task_struct定义如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/b7337c7fd3f942d3b61bfdc705501e0e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)





![请添加图片描述](https://img-blog.csdnimg.cn/33bb87ab5efa43d09170ccbbdaf2ea2c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



# 4.进程优先级

并非所有进程都具有相同的重要性。除了大多数我们所熟悉的进程优先级之外，进程 还有不同的关键度类别，以满足不同需求。首先进行比较粗糙的划分，进程可以分为实时进程 和非实时进程（普通进程）。 实时进程优先级（0-99）都比普通 进程的优先级（100-139）高。当系统中有实时进程 运行时，普通进程几乎无法分到赶时间片（只能分到5%的CPU时间）。

# 5.进程系统调用

讨论fork和exec系列系统调用的实现。通常这些调用不是由应用程序直接发出的，而是通过一个中间层调用，即负责与内核通信的C标准库。从用户状态切换到核心态的方法，依不同的体系结构而各有不同。

![在这里插入图片描述](https://img-blog.csdnimg.cn/d007403518c0471387834decddb72619.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



## 5.1 进程复制

传统的UNIX中用于复制进程的系统调用是fork。但它并不是Linux为此实现的唯一调用， 实际上Linux实现了3个
(1) fork是重量级调用，因为它建立了父进程的一个完整副本，然后作为子进程执行。 为减少与该调用相关的工作量，Linux使用了写时复制（copy-on-write）技术。
(2) vfork类似于fork，但并不创建父进程数据的副本。相反，父子进程之间共享数据。 这节省了大量CPU时间（如果一个进程操纵共享数据，则另一个会自动注意到）。
(3) clone产生线程，可以对父子进程之间的共享、复制进行精确控制。

![在这里插入图片描述](https://img-blog.csdnimg.cn/87c83d42042847ce9293862a0656ee08.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


只有在不得不复制数据内容时才去复制数据内容,这就是写时复制核心思想,可以看到因为修改页Z导致子进程不得不去复制原页Z来保证父子进程互不干扰
**内核只为新生成的子进程创建虚拟空间结构,它们来复制于父进程的虚拟结构,但是不为这些分配物理内存,它们共享父进程的物理空间,当父进程中有更改相应段的行为发生时,再为子进程相应段分配物理空间**



**【写时复制】**
内核使用了写时复制（Copy-On-Write，COW）技术，以防止在fork执行时将父进程的所有数据 复制到子进程。在调用fork时，内核通常对父进程的每个内存页，都为子进程创建一个相同的副本。
**【执行系统调用】**
fork、vfork和clone系统调用的入口点分别是sys_fork、sys_vfork和sys_clone函数。其定义依赖于 具体的体系结构，因为在用户空间和内核空间之间传递参数的方法因体系结构而异。
**【do_fork实现】**
所有3个fork机制最终都调用kernel/fork.c中的do_fork（一个体系结构无关的函数），其代码流程 如图所示。

![在这里插入图片描述](https://img-blog.csdnimg.cn/4a513dbefc3847c4ae089f6c0ba0f15f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)





![在这里插入图片描述](https://img-blog.csdnimg.cn/f4bbd2d0977b4182be7bcb4bf42f5d03.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



## 5.2 内核线程

内核线程是直接由内核本身启动的进程。内核线程实际上是将内核函数委托给独立的进程，与系统中其他进程“并行”执行（实际上，也并行于内核自身的执行）。内核线程经常称之为（内核）守护进程。它们用于执行下列任务。
• 周期性地将修改的内存页与页来源块设备同步（例如，使用mmap的文件映射）。
• 如果内存页很少使用，则写入交换区。
• 管理延时动作（deferred action）。
• 实现文件系统的事务日志。

## 5.3 退出进程

进程必须用exit系统调用终止。这使得内核有机会将该进程使用的资源释放回系统。见kernel/exit.c------>do_exit。简而言之， 该函数的实现就是将各个引用计数器减1，如果引用计数器归0而 没有进程再使用对应的结构，那么将相应的内存区域返还给内存管理模块。

![在这里插入图片描述](https://img-blog.csdnimg.cn/252e58f205ac4a2cbf87bca9ca078a9f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)

原文作者：[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605746202

# 【NO.472】posix API与网络协议栈的实现原理

**目录**

(posix API与网络协议栈的实现原理)



网络一共有8类技术文章:

![在这里插入图片描述](https://img-blog.csdnimg.cn/d056c177e96246b7be7873fb7da55536.png)



本文是这系列文章最后一篇
下一专题写池式结构:

![在这里插入图片描述](https://img-blog.csdnimg.cn/c3724dfaedb74c749c8e926736d75eb1.png)



写完池式结构就写底层组件是怎么做的，比如libevent

做网络编程的时候，所接触到所有的网络编程，往底层走，往底层去看一看的时候，会发现到头来走的全是这些API

以客户端和服务器分开来讲，服务端有哪些？



![在这里插入图片描述](https://img-blog.csdnimg.cn/9c4ff3e0cdb8440aa0cee4a863fd1efa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_8,color_FFFFFF,t_70,g_se,x_16)



客户端这边也同样如此

![在这里插入图片描述](https://img-blog.csdnimg.cn/d085d9bda2fe4af29773b4eb461d7521.png)



可以看到这就是所有的API另外再加一个对API的管理，就是epoll

![在这里插入图片描述](https://img-blog.csdnimg.cn/a9cde2d5b485432d8151628f9b023d41.png)



网络编程底层的时候就这11个API

几个不是很常用的



![在这里插入图片描述](https://img-blog.csdnimg.cn/2725b324b5f44c87b91838416cfc6e4f.png)



fcntl()就是设置fd阻塞和非阻塞的,这个东西其实更多意义来讲的话，它是跟文件系统有关系的，它跟文件有关系

那以TCP为例，TCP总共过程分三个阶段，不讲具体的代码,这些API怎么实现的跟协议栈上的一些关联

![在这里插入图片描述](https://img-blog.csdnimg.cn/bed4b47838d641b898b6dce77e231878.png)



其实TCP它本身来说他是个非常固定的东西

这是非常常见以及常用的问题。

![在这里插入图片描述](https://img-blog.csdnimg.cn/31d4ceb24ec349c3815613b0a46eb23b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_10,color_FFFFFF,t_70,g_se,x_16)



先从TCP和udp这个概念上来讲，就是TCP和udp
哪些场景？**既然 TCP这么常用，那为什么还会有一些场景会用到udp,udp主要用在哪里？**
有这么两种应用场景



![在这里插入图片描述](https://img-blog.csdnimg.cn/9e43aa38f13648acb5a134ec8022ce8d.png)



大量数据传输,比如我们传输文件，你打开迅雷会员的时候，你就应该可以感受到,打开迅雷整个局域网就你一个人下，其他人上网速特别慢，**那为什么出现这些现象？**
就是因为udp它中间不带这个拥塞控制，不带这种流量控制所以它在下载的时候，它的传输效率会比tcp要快，这是第一个场景在下载的时候。

第二个是说的这种实时性很强的，比如在游戏领域做竞技类游戏对应的协议，在这种弱网的环境下面传输的时候，

还有一个半个,就是在已有的一些RFC文件里面，比如像DNS协议传输分两部分里面既有TCP也有udp，*udp用在哪呢?*

比如说我们打开浏览器去请求一个域名的IP的时候，**从我们浏览器先获取到这个 IP地址的时候，这一步它是采用的udp协议**,那对于TCP呢就是域名服务器节点和节点之间的数据同步，它采用是tcp

对于这个地方为什么用UDP很难说清楚，可能就是因为他不需要去建立这个连接，你只需要发送请求反馈数据就可以了，根本就不需要建立这个连接，所以这这种是依赖于以前已经有的东西

udp相比较TCP而言，
**它有哪些缺点以及有哪些优点，它的缺点有哪些？**

![在这里插入图片描述](https://img-blog.csdnimg.cn/4fa559232a194bd2b1a3ac752a8fe25d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_13,color_FFFFFF,t_70,g_se,x_16)



UDP和TCP是属于传输层的，然后arp,ip,acmp协议是属于网络层的。

第一个udp的缺点,就是第一个不稳定，**那为什么udp不稳定？**
**你的不稳定是从哪里来的？**
**不可靠不稳定，为什么还要进行文件传输？**

udp不稳定，应该从这么几个维度来讲，

**udp的不稳定是相比较TCP而言**，那不稳定,
TCP总共一起三个过程，



![在这里插入图片描述](https://img-blog.csdnimg.cn/b297216d42784b4782b292e260e648f6.png)



UDP的不稳定，从这三个维度来说，

第一个有建立连接上面稳定性是否，第二个传输数据上面，第三个在断开连接上面
TCP既然是这三个过程，那 udp应该对比来说它也有这三个过程,那么不稳定，应该从这三个方面来说。

第一个在建立连接的这个过程，
**建立连接的这个过程上面udp和tcp的区别在哪里？**
udp是什么样的，udp压根就没有连接，就是我们udp在发送数据的时候,在准备一个socket的时候，就是我们调用sendto这个函数，

比如一个客户端和一个服务器，在客户端这边我们建立一个socket，紧接着我们调用send to就ok了，这个函数很有意思，第一个fd第二个buf第三个buf长度，第四个目的的IP地址。
相当于是什么意思，相当于你只要调用sendto，你根本中间就不要在乎这个数据它有没有连接，就是说我们只管调用sendto,对端就能收到，这个能不能收到我没法确定，只管sendto

**大家有没有见过在udp这个基础上面调用connect函数的？**

udp的这个connect它没有发送具体的数据，仅此就是尝试一下这条链路就是通的，对方有没有存在，对方的IP的端口有没有存在，如果把这个成功的话，那后面sendto失败的可能性会要低;

那 Tcp是怎么样的？

就是在TCP建立连接发送数据之前，我们一定要调用connect，服务器这端它是需要建立一个特定的状态的，我们才能够给对方发，比如说它需要进入一个listen的状态，

这里面有一个问题，这也是之前
**三次握手是发生在哪一个函数里面?**
如果你把这当做是哪个函数里面，那对TCP知识的理解不透彻，

![在这里插入图片描述](https://img-blog.csdnimg.cn/d79e1296876949049e15e00f05b9fa6e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



就是在我们调用这些API这都是在应用程序里面调用的，**那三次握手是发生在哪？是发生在协议栈和协议栈之间，就是在TCP 协议和TCP协议之间,这是协议栈帮忙实现的，跟我们的应用程序它是没有关系的。**

3次握手，4次挥手，都是在协议栈里面实现的。
但是这个过程要有一个发起者，就是客户那边当我们调connect的时候此时准备三次握手，
再讲三次握手之前还有一个问题要解释清楚，叫TCP头。

![在这里插入图片描述](https://img-blog.csdnimg.cn/15618aa2ee464d66836b24728604eaf2.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)



就是每一帧数据，每一帧的TCP的包前面这一段都带这个东西，**就是把这三次握手中间每一次都会带有这样一个数据包**
然后再给大家讲一下三次握手。

在调用connect这个函数客户端先发起，在应用程序上面，我们调用这个函数，就把它IP地址和端口copy到协议栈里面，**协议栈自己会准备一个包，也就是我们所说的syn包**，那syn的包什么意思？就是在TCP头里面，那8个状态里面有一个叫做syn的状态位，把那一位置一，然后把这个数据发给对方。**至于这个发送是什么时候发的?**就是在connect的时候，对应的IP地址和端口发到协议栈上面，协议栈上组一个包，这时候把它发出来。

然后服务器收到这个包之后，第一次返回一个ack的包和一个syn的包,ack等于刚刚那个syn加一

协议栈里面具体在发送的时候那个代码的具体实现，有个叫commit_skb，

去万达广场里面吃饭的时候，那餐馆第一个你去到餐馆里，在这服务员的地方领一个号，然后你自己走开一点，然后等这个号开始服务员叫你，然后你再过来，过来之后你把号提交给服务员,服务员就拿这个号跟之前对比是不是自己发的，如果是就允许进去这个问题,

站在服务端而言有两次数据是需要存储起来的,

第一个tcp建立连接的时候，这时候在服务端需要保存客户端的信息,**那为什么需要保存客户端信息呢?**服务端只有一个,客户端可以多个同时建立三次握手。

服务端而言，它会存在有比如现在有三个在这个队列里面，



![在这里插入图片描述](https://img-blog.csdnimg.cn/87bba3a19f294e10a0e1410b7a1c9bdf.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)



但是这个节点请注意,这个节点里面每一个框代表了一个客户端信息，也就是一个节点，这个节点它还不能用，它是一个半成品的状态，三次握手还没有完成，三次握手还只进行了第一次，这是一个半完成的状态，
就是一个半成品，等最后这一次过来之后，拿到之前的这里面首先拿到客户端的信息，
来对比这个半状态的结果里面有还是没有，如果有再把对应的数据对应的节点拿到下面。
这个节点,不是拷贝，是移动，拿了之前的这个节点的信息改变状态信息再加入进来。

![在这里插入图片描述](https://img-blog.csdnimg.cn/3ed2fb83d76c42aeb6b4759a378edeac.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)



**前面这个半成品队列叫做半链接队列,下面这个队列叫做全连接队列。
这个半链接队列，我们也叫做syn队列
下面这个叫做accept队列。**

**怎么进入到全连接队列里面，要进入这个全连接队列的这个状态机里面，**

只能通过三次握手的最后一次，这是一个前提条件，三次握手的最后一次过来，这是一个前提条件并且到半连接里面有,这两个条件同时满足，才进入全链接队列

**会提到一个问题，就是两个队列是共享的吗?**两个队列不是共享的，它的节点是共享的
节点是一样的

进入这个状态之后，accept函数才得以处理。
**他总共一起做两个事情，第一个从全连接队列里面取出一个节点出来，第二个分配fd**

![在这里插入图片描述](https://img-blog.csdnimg.cn/20ae19815d33453ca37781690f7dba44.png)



如果这个全连接这里面一个节点没有，我们调用accept这个函数怎么处理？
这时候会有一个条件等待，就是条件等待阻塞的过程，等待的这个全连接队列里面有一个节点
**如果我们这个 fd设置为非阻塞的**那也就是判断完这个条件队里面为空，那就直接返回了，返回一个-1

还是那句话，**三次握手发生在哪一个函数里面？**
如果这个半连接队列存在的话，这个东西它已经进入到一个listen的状态。

三次握手是发生在哪个函数里？
**广义上三次握手，就发生在客户端connect里面**，而服务端注意，服务端既不在listen，也不是accept，
**是进入listen之后，三次握手完成之后，在调用accept函数**
3次握手是说的协议栈和协议栈之间的操作，协议栈被动去实现它

有两个小疑问。

**从这个半连接队列里面三次握手，最后这一次过来的时候，从这个半连接队列里面怎么找到这个客户端特定的节点,这是第一个问题?**

**第二个小疑问，就是这里的这个节点，他在什么时候释放，他的生命周期有多长?**

通过什么找到对应客户端，就是这个节点就等同于跟客户端它是一对一的，也就等于客户端

这个问题应当从包头里面解析出来，没错就是5元组

![在这里插入图片描述](https://img-blog.csdnimg.cn/f582da5104b744fda1642e3b7924fa12.png)



我们的端口只有6535个，那为什么我们那个连接能做到100万？那怎么做的？
也是跟那个5元组也有关系，就是 fd它对应的就是5元组，**那 fd怎么对应这个五元组呢?**

就在accept()这个过程中间，它返回了一个fd,是accept()从这个节点里面拿出来一个节点,是通过这个五元组来找到它
源端口不同,这是第一个问题，组合到一起情况就很多
每一个TCP的包协议头里面都有，原端口号和目的端口号，IP地址里面有原IP和目的ID，所以就可以解析出来这个5元组。5元组至关重要，网络连接每一个包每一个连接操作系统怎么找到对应的fd，就是通过5元组
seq它是验证双方是否合法，不是验证那个节点是否合法，如果现在双方不合法，那这个节点根本就不把它丢进来

好，**第二个问题就这个节点的生命周期有多长？**

第一，就是到底有没有听过TCP的那11个状态，那个状态存在哪里？
存到这个节点里面，这里的这个节点它是伴随着整个一个连接的生命周期，
它不是fd的生命周期，fd的生命周期是在accept()里面创建的,
但是这个节点是先于fd的，这个节点已经有了，fd还没有，fd是在accept之后分配的，但是这个节点早就已有了

这个节点，也有另外一个高大上的名字叫做TCP控制块，英文名叫做tcb。
listen这个函数就是把对应这个 fd置为一个listen的状态，可以进行三次握手

**udp的并发怎么做?**

![在这里插入图片描述](https://img-blog.csdnimg.cn/58bebec75d5a4d51aebd82968683913b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)



很有可能一帧数据夹杂的其他的数据
你在发送的这个过程中间肯定有一些分包的问题，
第一种策略，我们通过udp上面设计一些协议，设定一些传输的控制性，我们来做这个事情。
就是在sendto的buffer里面我们加入一些协议能不能解决问题？



![在这里插入图片描述](https://img-blog.csdnimg.cn/a65978c53c624eefa8c393cbe3769409.png)


是不可以的。



如果只通过send to发送在同一个端口上面，刚刚说到这个问题，是没办法去做一个包把它区分开来的。
也就是a的包里面发b的包，接收这个数据的时候，你很难区分清楚，
从这么多个客户端接收数据进来，它很有可能出现数据的乱序

**那我们下一节课再给大家解释清楚这个 Udp的并发是怎么做的。**

发送数据都会在这一个recvbuffer里面，所以就出现一种脏数据现象，通过我们buffer加协议头这个问题他是解不了的,因为你多个数据只在这一个recvbuffer里面解出来

![在这里插入图片描述](https://img-blog.csdnimg.cn/d8a9719a1b4d4c3b8c73b6741d9723f5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)



可以采用TCP的方式，用udp去模拟TCP的方式。
只做第一次，也就是说在客户端跟服务器建立连接的时候，不叫建立连接,发送数据包的第一次的时候这个也要配合的应用协议，发送第一帧数据的时候，
这个 fd-->recvbuffer接收第一帧数据的时候，接收到一个新的客户端来了，然后对应应用程序上面分配一个新的fd，以及一个对应的端口号，再send出来，也就出现了一种现象，一个fd会对应一个客户端

第一步
就是recvfrom接收到一个客户端信息，然后拿出它的IP地址和端口。
第二步
新建一个fd，然后调用sendto，
相比较而言，此时可能从一个微观的角度来说，从这一次session来说，你就可以在这个过程中把这一次当做一个客户端，那这个过程中间就出现了一个fd对应一个客户端这样来解,这是在底层上面一定要做到这样才能够去解决这个 udp处理并发的问题

重新走一个端口也可以走我们之前的端口发出去也是ok的，但是我们要走另外一个fd

kcp只是在TCP这个框架上面，做了一层应用协议
就在传输数据包是怎么样，传输数据包是怎么样这个过程。 kcp是纯算法的

有两个问题，
一个是客户端fd=connect()成功,但是服务端一直不accept(),调不调用accept()这条道路都是通的,客户端是可以写,只是服务端不能收到而已,
第二就是这个半链接的节点对象,除了保存11个状态以外,还保留send/recv缓存

**第二个就是数据传输**

tcp的传输过程
第一个问题。
**TCP它是如何保证顺序？**
TCP是一个流式套接字，什么叫流呢?就是先发的先到,后发的后到就是保证顺序的
**那这个顺序的过程中间， TCP它又是如何保证顺序？**
在讲这个之前先举一个例子，
比如说
现在有一个蓝框，有一个球场有一个框，这个框里面呢放了100个球，标号从0~100，然后这100个球呢把它放到另外一个100米远的地方，
把它放过去，然后这么把它发过去，那怎么保证它顺序？

这里有这么几个问题，
第一个问题，
在发送的过程中间，这是一个最简单最简单的方法,
第一个方法就是把1号包发完之后，对方确认接收了，发送第二个,第二号包发送完之后，对方确认,发送第三个，这样他肯定保证是顺序的，

比如说我们现在先为每个包编一个号，就是比如说这100个球，我们先把第一个球送给对方对方接收到了，然后返回一个确认值，然后这边才发送二号包。
就是第二个球接收到了再发第三个球这样一个过程，这样一个过程接收的肯定是正确的，

这种方法它保证了顺序，但是它的效率极低,它的效率是不高的，那怎么办？
就会出现一种现象，就第一个包发出去之后，第二个包
这是第一号发出去，
可能没有返回确认。
第二个
也发了，第三个也发了，
第四个包也在路上，
然后再来去确认,应该是这样一个状态，这才是一个高效的，这才是一个有那么一点点智能的，它才能够保证这个数据的这个高效性，
**这里面就有一个问题，问题在哪？**
既然多个包同时在网络上传输的时候，这时候我们可能在网络上在公网上传输我们是很难保证，
先发的先到后发的后到，

刚刚说的TCP里面先发的先到后发的后到，这是在我们应用程序已经接收到数据的时候，它是先receive的是先发,后receive的后到，但我没接收之前在网络上面传输的时候，网络上面是一个极其复杂的情况，那这个过程我们是很难保证，先发的数据先到后发的数据后到。

没办法确定哪个包先到达对方这个机器上面的。

![在这里插入图片描述](https://img-blog.csdnimg.cn/07b8b1f3cb7f44efb2fa0d1455cabcd8.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_19,color_FFFFFF,t_70,g_se,x_16)



这个顺序是没法确定的，
这就是一个重排的过程。
比如这边是1234。
这边1号2号4号，3号没有收到，
这时候有一个机制叫做超时重传，
每接收到一个包，这时候协议栈会启动一个定时器，启动一个200毫秒的定时器，
在接收一号包的时候重置这个定时器，再次启动200毫秒的定时器，在他接收4号的时候再次重置，一旦超时一旦超时就开始检测这些包的顺序到底哪个没到，
第一个词超时重传就是从小到大依次验证，
找到比如说这个3号包没有的话，那就重新从3号包以后全部重发，4号包也进行重发，就这么一个情况。

这就是tcp能保证，最小的接收的是顺序的,至少1和2是顺序的3,4重发的时候再次保证顺序

那这个过程中间大家可以看到TCP的一些缺点，

第一个确认时间周期长，

第二个重发的时候，比如说3号没有收到4号包也会进行重发，即使收到也会进行重发，那其实这个过程当中应该理解重发的比例，已经收到的就是重发的这个次数有点多。

为什么4号包也要重发?
TCP给已经接收到的这个对端，
就是这个头
这个确认消息ack它只有一个数字，
这个 Ack的意思可以确认多少号包以前的全部收到了，后面的包没有收到，是这个意思，这个 ack这个值
它只是一个数字，在这个设计协议本身的时候就已经决定了，那是没办法，
比如说我们再多几个包，
我们很难表示3号和5号收到,4号和6号没有收到,协议头我们是表示不出来。

这就是由于TCP协议本身的一些缺陷，
那也就是udp在这个基础上面，所以大家所理解的kcp也是在这个基础上面解决这些问题，就比如说它的确认时间，那我们可以做到收到一个包就确认一次，这样的话它的实时性会更强。
第二个就是关于这个重发的次数，我们可以
比如说在一次发送中我们告诉对方我们有哪些包中间可以没有收到，这也可以做，那也是可以从一定的线路上面去优化TCP。

**TCP在传输的过程中间只要理解这一个点就是tcp如何保证顺序的**，解理解这一点就ok，

然后还有一个过程，

![在这里插入图片描述](https://img-blog.csdnimg.cn/f14a2233c24c4e0ebb7d879a375a6055.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_19,color_FFFFFF,t_70,g_se,x_16)



这一个包一个包的发送，我们怎么能够进行
我们一次发送，我们可以传出4个或传出更多的怎么做，它是一种什么样的规律，那我们怎么能够做到？
刚刚发一个包，我们刚刚那个状态怎么就可以变到在网上发多个包以及这多个包是怎么计算的？

这里要跟大家讲到的一个概念就是所说到的**慢启动**，
比如第一次从一开始的时候我们先发一个包，
第二次
我们在我们第一个包发送出去，我们能够准确的在规定的时间里面收到了ACK，
那我们第二次就发2个包，第三次就发4个包，
第四次8个包



![在这里插入图片描述](https://img-blog.csdnimg.cn/856694320fea4bdc90b40dd0c15e7220.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



这个慢启动并不慢，这个慢启动是它的初始值比较小，然后需要从一点点一点点的加起来，
并不是一开始设立一个很大的数没有，然后在这个过程中间到底涨到多少合适

线性增长这个过程中间，一旦超出要把它降一半，
**前面那段叫做慢启动，后面这一段叫做拥塞控制**

这个函数send
**只做到一点，他跟我们数据成不成功一点关系都没有，send成功了压根就不能够去决定我们数据有没有成功**

![在这里插入图片描述](https://img-blog.csdnimg.cn/2d5b6594060b40ab9dfdf9e4f87b6cd8.png)



他只能去决定这个应用程序的数据把它放在协议栈里面，

![在这里插入图片描述](https://img-blog.csdnimg.cn/7bcc524756144ff0ad58b2dc2bf7c66f.png)



如何保证顺序的?数据包怎么发的?
send这个函数跟write这个函数一样的，他唯一还做的一个事情就是copy_from_user
从用户空间里面把这个数据copy到协议栈里面，放到sendbuffer里面

**第三个讲的是断开连接**

断开连接的过程对于应用程序而言，两个接口可以做
一个是close
另外一个是shutdown

在这一堆函数里面，

站在一个网络传输的角度接和收，
要么接要么收

我们调用哪一些接口，协议栈它会发送数据出去。
相当于一个广义上的send

![在这里插入图片描述](https://img-blog.csdnimg.cn/1e59218180164cf7828f3647078a456a.png)



就这4个它都有1个发送的过程。

调用close时候，应用程序上调用close看似只是一个fd，
他没有发送，不需要copy任何数据，也就是在这sendbuffer里面，我们准备一帧里面带有fin结束标志的这个包把它发送出去，

那这4次挥手的过程
因为他是个比较复杂东西，
是TCP里面就是状态比较多的地方

![在这里插入图片描述](https://img-blog.csdnimg.cn/c0b8f71f18bd4035a8138fd0c6587b64.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



这是我们调用close，会发送一帧fin，接收完之后这里有ACK，
然后在这个阶段里面我们再调用cloes，
两边是对称的，TCP是双向通信的，
这个fd即可接又可收，
调用close这个过程，就是关闭了它的发送,就是调用close之后它这边是不能发送

**但是在广义上来说，另外这边它不能收了，没有数据可收了。**
**所以这里面有两个东西epollhup,epollrdhup**

这两个状态里面有这个问题
**为什么没有一个 epollwhup为什么没有写？**
写这个东西在这个逻辑里面它是多余的

**为什么没有epollwrhup这个东西。**而是有epollhup和epollrdhup

![在这里插入图片描述](https://img-blog.csdnimg.cn/000fe2b0d1344e57a104a425bded2648.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_13,color_FFFFFF,t_70,g_se,x_16)



这边是读关闭了，站在这一端而言，接收到了fin,

**也就是对应如果两端都关闭了，也就是对应来说自己也发送出去close，并且接收到这个 ACK这一步完了之后，这个 fd如果还在里面，它就会epollhup这个状态**



![在这里插入图片描述](https://img-blog.csdnimg.cn/2212f9e28a3449e4b7e633829a93abfa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_10,color_FFFFFF,t_70,g_se,x_16)





![在这里插入图片描述](https://img-blog.csdnimg.cn/a61e0ac8e0c849eb9e64c4d77dab88bd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



第一个就是大量的time_wait,大量的close_wait
第二个就是大量的fin_wait1,fin_wait2

这也是经常在面试中经常会问到的，**第一个出现大量time_wait怎么办？**

主动方调用close才有time_wait,
是最先调用close的时候他才有time_wait

其实这个服务端而言出现了大量time_wait，其实这是一个非常不正常的现象，
也就是说你的服务器逻辑代码里面应该是出现了主动调用close的现象。

第一个原因，自己代码里面主动调用了close
首先第一查自己逻辑，自己调用close地方是不是正常的，
如果是正常的，
**可以通过setsockopt()把它设置为一个reuse，就把它设置为重用。**

第二个如果出现大量close_wait
0=recv()再去close
其实在这个现象是因为调用close延迟
可能在这个过程中，就是知道对方已经关闭了，知道一个客户端主动断开连接之后，需要去释放服务,这个客户端对于一些业务一些逻辑信息。
所以在这个逻辑处理的时间上面可能有点长
所以就会把这个close延迟之后

**就是看你调用close的地方，是不是在正确的时机**
我怎么知道是不是正确时机
就是recv返回0之后
这时候
应该是立即调用close
**对客户业务代码的解析以及释放可以抛给另外一个线程，这里边另外一个线程去释放业务代码，不应该在这个流程处理异步处理**

这个fin_wait2有没有方法终止，
就是现在进入了一个fin_wait2的状态，
**能不能够终止？**

**等到对方,对方一直都不关闭，所以这个中间有没有方法去中断？**
**没有**
你能做的事情都已经做完了,剩下的只有一个事情，
**你能强行禁止，只能通过kill**

从TCP的状态迁移图上
压根就没有进入到这个close的状态，也就是这个过程中间，

你就不会再次重新去分配，
那要我重新再去分配它，而是拿着之前用的东西拿来用。

提供出来重用，能够从一大一定限度上面减少这个time_wait

原文作者：[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605811495

# 【NO.473】常使用的网络IO管理

  网络 IO，会涉及到两个系统对象，一个是用户空间调用 IO 的进程或者线程，另一个是内核空间的内核系统，比如发生 IO 操作 read 时，它会经历两个阶段：

1. 等待数据准备就绪

2. 将数据从内核拷贝到进程或者线程中。

​    因为在以上两个阶段上各有不同的情况，所以出现了五种网络 IO 模型。

## 1.阻塞IO（blocking IO）

1. ***\*定义\****：在 linux 中，默认情况下所有的 socket 都是 blocking，例如listen()、send()、recv()等都是阻塞的。所谓阻塞型接口是指系统调用（一般是 IO 接口）不返回调用结果并让当前线程一直阻塞，只有当该系统调用获得结果或者超时出错时才返回。
   ![img](https://img-blog.csdnimg.cn/0e323315ed994b0ca48b55847d9ee45b.png)
2. ***\*改进\****：在服务器端使用多线程（或多进程），但没有特定的模式。进程的开销要远远大于线程，所以如果需要同时为较多的客户机提供服务，则不推荐使用多进程；如果单个服务执行体需要消耗较多的 CPU 资源，譬如需要进行大规模或长时间的数据运算或文件访问，则进程较为安全。使用 pthread_create ()创建新线程，fork()创建新进程。
3. ***\*问题：\****上述多线程、多进程都会严重占据系统资源，降低系统对外界响应效率，而线程与进程本身也更容易进入假死状态。此时，很多人用线程池或连接池：
   “线程池”旨在减少创建和销毁线程的频率，其维持一定合理数量的线程，并让空闲的线程重新承担新的执行任务。
   “连接池”维持连接的缓存池，尽量重用已有的连接、减少创建和关闭连接的频率。
   但是，“线程池”和“连接池”技术也只是在一定程度上缓解了频繁调用 IO 接口带来的资源占用。而且，***\*所谓“池”始终有其上限，当请求大大超过上限时，“池”构成的系统对外界的响应并不比没有池的时候效果好多少。所以使用“池”必须考虑其面临的响应规模，并根据响应规模调整“池”的大小。\****

## 2.非阻塞 IO（non-blocking IO）

​    多线程模型可以方便高效的解决小规模的服务请求，但面对大规模的服务请求，多线程模型也会遇到瓶颈，可以用非阻塞接口来尝试解决这个问题。
![img](https://img-blog.csdnimg.cn/9df1c5fc90094240aaf75a55e31b081d.png)

 

***\*1. 定义：\****

从用户进程角度讲 ，它发起一个read 操作后，并不需要等待，而是马上就得到了一个结果。所以，在非阻塞式 IO 中，用户进程其实是需要不断的主动询问 kernel数据准备好了没有。

***\*recv返回值：\****
在非阻塞状态下，recv() 接口在被调用后立即返回，返回值代表了不同的含义。如在本例中：

\* recv() 返回值大于 0，表示接受数据完毕，返回值即是接受到的字节数；

\* recv() 返回 0，表示连接已经正常断开；

\* recv() 返回 -1，且 errno 等于 EAGAIN，表示 recv 操作还没执行完成；

\* recv() 返回 -1，且 errno 不等于 EAGAIN，表示 recv 操作遇到系统错误 errno。

***\*2. 设置非阻塞：\****
    非阻塞的接口相比于阻塞型接口的显著差异在于，在被调用之后立即返回。使用如下的函数可以将某句柄 fd 设为非阻塞状态。
fcntl( fd, F_SETFL, O_NONBLOCK );

***\*3. 问题：\****
    上述非阻塞模型绝不被推荐。因为，循环调用 recv()将大幅度推高CPU占用率；此外，在这个方案中 recv()更多的是起到检测“操作是否完成”的作用，实际操作系统提供了更为高效的检测“操作是否完成“作用的接口，例如 select()多路复用模式，可以一次检测多个连接是否活跃。

## 3.多路复用 IO（IO multiplexing）

***\*（1）原理\****：是 select/epoll 这个 function会不断的轮询所负责的所有 socket，当某个 socket 有数据到达了，就通知用户进程。
***\*（2）流程\****：

![img](https://img-blog.csdnimg.cn/881aa7a86f21484191aaac44eb3bb5c4.png)

​    这个图和 blocking IO 的图其实并没有太大的不同，事实上还更差一些。因为这里需要使用两个系统调用(select 和 read)，而 blocking IO 只调用了一个系统调用(read)。但是使用 select 以后最大的优势是用户可以在一个线程内同时处理多个 socket 的 IO 请求。
所以，如果处理的连接数不是很高的话，使用select/epoll 的 web server 不一定比使用 multi-threading + blocking IO 的 web server 性能更好，可能延迟还更大。select/epoll 的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。
***\*（3）问题\****：

首先 select()接口并不是实现“事件驱动”的最好选择。因为当需要探测的句柄值较大时，select()接口本身需要消耗大量时间去轮询各个句柄。

其次，该模型将事件探测和事件响应夹杂在一起，一旦事件响应的执行体庞大，则对整个模型是灾难性的。

***\*（4）解决方案\****：

有很多高效的事件驱动库可以屏蔽上述的困难，常见的事件驱动库有libevent 库，还有作为 libevent 替代者的 libev 库。这些库会根据操作系统的特点选择最合适的事件探测接口，并且加入了信号(signal) 等技术以支持异步响应，这使得这些库成为构建事件驱动模型的不二选择。

实际上，Linux 内核从 2.6 开始，也引入了支持异步响应的 IO 操作，如 aio_read, aio_write，这就是异步 IO。

## **4.异步 IO（Asynchronous I/O）**

​    Linux 下的 asynchronous IO 用在磁盘 IO 读写操作，不用于网络 IO，从内核 2.6 版本才开始引入。

***\*1. 流程：\****
![img](https://img-blog.csdnimg.cn/9aa2853567f447a28c4dfa367ba57158.png)

​    用户进程发起 read 操作之后，立刻就可以开始去做其它的事。而另一方面，从 kernel的角度，当它收到一个 asynchronous read 之后，首先它会立刻返回，所以不会对用户进程产生任何 block。然后，kernel 会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel 会给用户进程发送一个 signal，告诉它 read 操作完成了。

​    ***\*异步 IO 是真正非阻塞的，它不会对请求进程产生任何的阻塞，因此对高并发的网络服务器实现至关重要。\****

## **5.信号驱动 IO（signal driven I/O，SIGIO）**

![img](https://img-blog.csdnimg.cn/1873e0411c2f47978e39b2827ffb1977.png)

首先我们允许套接口进行信号驱动 I/O,并安装一个信号处理函数，进程继续运行并不阻塞。当数据准备好时，进程会收到一个 SIGIO 信号，可以在信号处理函数中调用 I/O 操作函数处理数据。当数据报准备好读取时，内核就为该进程产生一个 SIGIO 信号。我们随后既可以在信号处理函数中调用 read 读取数据报，并通知主循环数据已准备好待处理，也可以立即通知主循环，让它来读取数据报。

***\*优势\****：在于等待数据报到达(第一阶段)期间，进程可以继续执行，不被阻塞。免去了 select 的阻塞与轮询，当有活跃套接字时，由注册的 handler 处理。

 

--------------------------------------------------------分割线---------------------------------------------------------------

 

## 6.总结

1. ***\*blocking 和 non-blocking 的区别在哪，synchronous IO 和 asynchronous IO 的区别在哪\*******\*？\*******\*non-blocking\*******\*和\*******\*asynchronous IO 的区别在哪\*******\*？\****
   （1）blocking 和 non-blocking：调用 blocking IO 会一直 block 住对应的进程直到操作完成，而non-blocking IO 在 kernel 还在准备数据的情况下会立刻返回。
   （2）synchronous IO 做”IO operation”的时候会将 process 阻塞。按照这个定义，之前所述的 blocking IO，non-blocking IO，IO multiplexing 都属于synchronous IO。但要注意：
       有人可能会说，non-blocking IO 并没有被 block 啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的 IO 操作，就是例子中的 read 这个系统调用。non-blocking IO 在执行 read 这个系统调用的时候，如果 kernel 的数据没有准备好，这时候不会 block 进程。但是当 kernel 中数据准备好的时候，read 会将数据从 kernel 拷贝到用户内存中，这个时候进程是被 block 了，在这段时间内进程是被 block的。而 asynchronous IO 则不一样，当进程发起 IO 操作之后，就直接返回再也不理睬了，直到 kernel 发送一个信号，告诉进程说 IO 完成。在这整个过程中，进程完全没有被 block。
   （3）在non-blocking IO 中，虽然进程大部分时间都不会被 block，但是它仍然要求进程去主动的 check，并且当数据准备完成以后，也需要进程主动的再次调用 recvfrom 来将数据拷贝到用户内存。而 asynchronous IO 则完全不同。它就像是用户进程将整个 IO 操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查 IO 操作的状态，也不需要主动的去拷贝数据。

原文作者：[[当当响](https://blog.csdn.net/zhpcsdn921011)

原文链接：https://bbs.csdn.net/topics/606367359

# 【NO.474】服务器模型reactor

 对高并发编程，网络连接上的消息处理，可以分为两个阶段：等待消息准备好、消息处理。当使用默认的阻塞套接字时，往往是把这两个阶段合而为一，这样操作套接字的代码所在的线程就得睡眠来等待消息准备好，这导致了高并发下线程会频繁的睡眠、唤醒，从而影响了 CPU 的使用效率。

​    **高并发编程方法当然就是把两个阶段分开处理。即：等待消息准备好的代码段，与处理消息的代码段是分离的**。当然，这也要求套接字必须是非阻塞的，否则，处理消息的代码段很容易导致条件不满足时，所在线程又进入了睡眠等待阶段。那么问题来了，等待消息准备好这个阶段怎么实现？它毕竟还是等待，这意味着线程还是要睡眠的！**解决办法就是，线程主动查询，或者让 1 个线程为所有连接而等待！**这就是 IO 多路复用了。多路复用就是处理等待消息准备好这件事的，但它可以同时处理多个连接！它也可能“等待”，所以它也会导致线程睡眠，然而这不要紧，因为它一对多、它可以监控所有连接。这样，当我们的线程被唤醒执行时，就一定是有一些连接准备好被我们的代码执行了。

​    作为一个高性能服务器程序通常需要考虑处理三类事件： I/O 事件、定时事件及信号。

**reactor是什么？怎么理解？**

​    reactor是一种设计模式, 是服务器的重要模型, 是一种事件驱动的反应堆模式, 高效的事件处理模型。

​    reactor 反应堆:  事件来了才执行，事件类型可能不尽相同，所以我们需要提前注册好不同的事件处理函数。事件到来就由 epoll_wait  获取同时到来的多个事件，并且根据数据的不同类型将事件分发给事件处理机制 (事件处理器)， 也就是提前注册的哪些接口函数。
​    reactor模型的设计思想和思维方式：它需要的是事件驱动，相应的事件发生，根据事件自动的调用相应的函数，所以需要提前注册好处理函数的接口到reactor中, 函数是由reactor去调用的，而不是再主函数中直接进行调用的, 需要使用回调函数。
​    reactor中的 IO 使用的是select poll  epoll 多路复用IO, 以便提高 IO 事件的处理能力，提高IO事件处理效率，支持更高的并发 。

​    和普通函数调用的不同之处在于：***\*应用程序不是主动的调用某个 API 完成处理，而是恰恰相反，Reactor 逆置了事件处理流程，应用程序需要提供相应的接口并注册到 Reactor 上，如果相应的时间发生，Reactor 将主动调用应用程序注册的接口，这些接口又称为“回调函数”\****。

​    Reactor 模式是处理并发 I/O 比较常见的一种模式，***\*用于同步 I/O\****，***\*中心思想\****是：***\*将所有要处理的 I/O 事件注册到一个中心 I/O 多路复用器上，同时主线程/进程阻塞在多路复用器上；一旦有 I/O 事件到来或是准备就绪(文件描述符或 socket 可读、写)，多路复用器返回并将事先注册的相应 I/O 事件分发到对应的处理器中\****。

Reactor 模型有三个重要的组件：

​    多路复用器：由操作系统提供，在 linux 上一般是 select, poll, epoll 等系统调用。

​    事件分发器：将多路复用器中返回的就绪事件分到对应的处理函数中。

​    事件处理器：负责处理特定事件的处理函数。

代码层面主要涉及以下几个方面：

reactor结构体、以及事件结构体的封装：

```
struct ntyevent



{



    int fd;



    int events;



    void *arg;



    int (*callback)(int fd, int events, void *arg);



 



    int status;



    char buffer[BUFFER_LENGTH];



    int length;



    long last_active;



};



 



 



struct ntyreactor



{



    int epfd;



    struct ntyevent *events;



};
```

reactor对应事件fd的注册、新增和删除：

```
void nty_event_set(struct ntyevent *ev, int fd, NCALLBACK callback, void *arg)



{



    ev->fd = fd;



    ev->callback = callback;



    ev->events = 0;



    ev->arg = arg;



    ev->last_active = time(NULL);



 



    return ;



}



 



int nty_event_add(int epfd, int events, struct ntyevent *ev)



{



    struct epoll_event ep_ev = {0, {0}};



    ep_ev.data.ptr = ev;



    ep_ev.events = ev->events = events;



 



    int op;



    if (ev->status == 1)



    {



        op = EPOLL_CTL_MOD;



    } else



    {



        op = EPOLL_CTL_ADD;



        ev->status = 1;



    }



    



    if (epoll_ctl(epfd, op, ev->fd, &ep_ev) < 0) 



    {



        printf("event add failed [fd=%d], events[%d]\n", ev->fd, events);



		return -1;  



    }



 



    return 0;



}



 



int nty_event_del(int epfd, struct ntyevent *ev)



{



    struct epoll_event ep_ev = {0, {0}};



 



    if (ev->status != 1)



    {



        return -1;



    }



 



    ep_ev.data.ptr = ev;



    ev->status = 0;



    epoll_ctl(epfd, EPOLL_CTL_DEL, ev->fd, &ep_ev);



    



    return 0;



}
```

回调函数：

```
int recv_cb (int fd, int events, void *arg)



{



    struct ntyreactor *reactor = (struct ntyreactor*)arg;



	struct ntyevent *ev = reactor->events+fd;



 



    int len = recv(fd, ev->buffer, BUFFER_LENGTH, 0);



    nty_event_del(reactor->epfd, ev);



 



    if (len > 0)



    {



	    ev->length = len;



		ev->buffer[len] = '\0';



 



		printf("C[%d]:%s\n", fd, ev->buffer);



 



		nty_event_set(ev, fd, send_cb, reactor);



		nty_event_add(reactor->epfd, EPOLLOUT, ev);



    } else if (len == 0)



    {



        close(ev->fd);



        printf("[fd=%d] pos[%ld], closed\n", fd, ev-reactor->events);



    } else



    {



        close(ev->fd);



		printf("recv[fd=%d] error[%d]:%s\n", fd, errno, strerror(errno));



    }  



    return len;



}



 



int send_cb(int fd, int events, void *arg) {



 



	struct ntyreactor *reactor = (struct ntyreactor*)arg;



	struct ntyevent *ev = reactor->events+fd;



 



	int len = send(fd, ev->buffer, ev->length, 0);



	if (len > 0) {



		printf("send[fd=%d], [%d]%s\n", fd, len, ev->buffer);



 



		nty_event_del(reactor->epfd, ev);



		nty_event_set(ev, fd, recv_cb, reactor);



		nty_event_add(reactor->epfd, EPOLLIN, ev);



		



	} else {



 



		close(ev->fd);



 



		nty_event_del(reactor->epfd, ev);



		printf("send[fd=%d] error %s\n", fd, strerror(errno));



 



	}



 



	return len;



}



 



int accept_cb(int fd, int events, void *arg)



{



    struct ntyreactor *reactor = (struct ntyreactor*)arg;



	if (reactor == NULL) return -1;



 



    struct sockaddr_in client_addr;



	socklen_t len = sizeof(client_addr);



 



	int clientfd;



 



    if ((clientfd = accept(fd, (struct sockaddr*)&client_addr, &len)) == -1) {



		if (errno != EAGAIN && errno != EINTR) {



			



		}



		printf("accept: %s\n", strerror(errno));



		return -1;



	}



 



    int i = 0;



    do



    {



		for (i = 3;i < MAX_EPOLL_EVENTS;i ++) {



			if (reactor->events[i].status == 0) {



				break;



			}



		}



		if (i == MAX_EPOLL_EVENTS) {



			printf("%s: max connect limit[%d]\n", __func__, MAX_EPOLL_EVENTS);



			break;



		}        



 



        int flag = 0;



		if ((flag = fcntl(clientfd, F_SETFL, O_NONBLOCK)) < 0) {



			printf("%s: fcntl nonblocking failed, %d\n", __func__, MAX_EPOLL_EVENTS);



			break;



		}



        



        nty_event_set(&reactor->events[clientfd], clientfd, recv_cb, reactor);



		nty_event_add(reactor->epfd, EPOLLIN, &reactor->events[clientfd]);



    } while (0);



    



    printf(new connect [%s:%d][time:%ld], pos[%d]\n", 



		inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), reactor->events[i].last_active, i);







    return 0;



}
```

 原文作者：[当当响](https://blog.csdn.net/zhpcsdn921011)

原文链接：https://bbs.csdn.net/topics/606367428

# 【NO.475】nginx 中数据结构讲解

nginx为了做到跨平台， 定义、封装了一些基本的数据结构。由于nginx 对内存分配比较“吝啬”（当然咯，只有保证低内存消耗，才可能实现十万甚至百万级别的同时并发连接数），所以nginx 的数据结构天生都是尽可能少占用内存。学习优秀的代码，未来才能设计好整个系统平台。
可重点看ngx_pool_t 的设计思想

ngx_array_t 数据结构
src/core

在 Nginx 数组中，内存分配是基于内存池的，并不是固定不变的，也不是需要多少内存就申请多少，若当前内存不足以存储所需元素时，按照当前数组的两倍内存大小进行申请，这样做减少内存分配的次数，提高效率。

数据结构定义：

```
typedef struct {
    void        *elts;      // 指向数组数据区域的首地址
    ngx_uint_t   nelts;     // 数组实际数据的个数
    size_t       size;      // 单个元素所占据的字节大小
    ngx_uint_t   nalloc;    // 数组容量 
    ngx_pool_t  *pool;      // 数组对象所在的内存池 
} ngx_array_t;
```


数据结构图：

```
基本操作
/* 创建新的动态数组 */
ngx_array_t *ngx_array_create(ngx_pool_t *p, ngx_uint_t n, size_t size);
/* 销毁数组对象，内存被内存池回收 */
void ngx_array_destroy(ngx_array_t *a);
/* 在现有数组中增加一个新的元素 */
void *ngx_array_push(ngx_array_t *a);
/* 在现有数组中增加 n 个新的元素 */
void *ngx_array_push_n(ngx_array_t *a, ngx_uint_t n);
```


示例代码：

```
#include <stdio.h>
#include <string.h>
#include "ngx_config.h"
#include "ngx_core.h"
#include "ngx_list.h"
#include "ngx_palloc.h"
#include "ngx_string.h"

#define N 10

typedef struct Key {
    int id;
    char name[32];
}Key;

volatile ngx_cycle_t *ngx_cycle;
void ngx_log_error_core(ngx_uint_t level, ngx_log_t *log,
			ngx_err_t err, const char *fmt, ...)
{
}

void print_array(ngx_array_t* arr)
{
    Key* key = arr->elts;

    int i = 0;
    for(i = 0; i < arr->nelts; i ++)
    {
        printf("%s.\n", key[i].name);
    }

}

int main()
{
    printf("Key = %d\n", sizeof(Key));   // 36
    printf("ngx_variable_value_t = %d\n", sizeof(ngx_variable_value_t));   // 16 = 8 + 8
    ngx_pool_t* pool = ngx_create_pool(1024, NULL);
    ngx_array_t* array = ngx_array_create(pool, N, sizeof(Key));

    int i = 0;
    Key* key = NULL;
    for(i = 0; i < 24; i ++) 
    {
        key = ngx_array_push(array);
        key->id = i + 1;
        sprintf(key->name, "Test %d", key->id);
    }
    
    key = ngx_array_push_n(array, 10);
    for(i = 0; i < 10; i ++)
    {
        key[i].id = 24 + i + 1;
        sprintf(key[i].name, "Other Test %d", key[i].id);
    }
    print_array(array);
    return 0;

}

// 编译命令
// gcc -o ngx_array_main ngx_array_main.c  -I ../nginx_folds/nginx/src/core/  -I ../nginx_folds/nginx/objs/ -I ../nginx_folds/nginx/src/os/unix/ -I ../nginx_folds/pcre-8.41/ -I ../nginx_folds/nginx/src/event/  ../nginx_folds/nginx/objs/src/core/ngx_list.o ../nginx_folds/nginx/objs/src/core/ngx_string.o ../nginx_folds/nginx/objs/src/core/ngx_palloc.o ../nginx_folds/nginx/objs/src/os/unix/ngx_alloc.o ../nginx_folds/nginx/objs/src/core/ngx_array.o
```

ngx_list_t 数据结构
ngx_list_t 是 Nginx 封装的链表容器，链表容器内存分配是基于内存池进行的，操作方便，效率高。Nginx 链表容器和普通链表类似，均有链表表头和链表节点，通过节点指针组成链表。

数据结构定义如下

```
/* 链表结构 */
typedef struct ngx_list_part_s  ngx_list_part_t;

/* 链表中的节点结构 */
struct ngx_list_part_s {
    void             *elts; /* 指向该节点数据区的首地址 */
    ngx_uint_t        nelts;/* 该节点数据区实际存放的元素个数 */
    ngx_list_part_t  *next; /* 指向链表的下一个节点 */
};

/* 链表表头结构 */
typedef struct {
    ngx_list_part_t  *last; /* 指向链表中最后一个节点 */
    ngx_list_part_t   part; /* 链表中表头包含的第一个节点 */
    size_t            size; /* 元素的字节大小 */
    ngx_uint_t        nalloc;/* 链表中每个节点所能容纳元素的个数 */
    ngx_pool_t       *pool; /* 该链表节点空间的内存池对象 */
} ngx_list_t;
```

数据结构图：

```
基本操作
/* 创建链表 */
ngx_list_t * ngx_list_create(ngx_pool_t *pool, ngx_uint_t n, size_t size);

/* 初始化链表 */
static ngx_inline ngx_int_t ngx_list_init(ngx_list_t *list, ngx_pool_t *pool, ngx_uint_t n, size_t size);

/* 添加一个元素 */
void *ngx_list_push(ngx_list_t *l);
```



```
示例代码
#include <stdio.h>
#include <string.h>
#include "ngx_config.h"
#include "ngx_core.h"
#include "ngx_list.h"
#include "ngx_palloc.h"
#include "ngx_string.h"

#define N	10
volatile ngx_cycle_t *ngx_cycle;

void ngx_log_error_core(ngx_uint_t level, ngx_log_t *log,
			ngx_err_t err, const char *fmt, ...)
{

}


void print_list(ngx_list_t *l) {
	ngx_list_part_t *p = &(l->part);
	

	while (p) {
	
		int i = 0;
		for (i = 0;i < p->nelts;i ++) {
			printf("%s\n", (char*)(((ngx_str_t*)p->elts + i)->data));
		}
		p = p->next;
		printf(" -------------------------- \n");
	}

}

// typedef struct {
//     size_t      len;  // long unsigned int
//     u_char     *data;
// } ngx_str_t;

int main() {

	// 4, 4, 8, 8, 16
	printf("int = %ld\n", sizeof(int));
	printf("unsigned int = %ld\n", sizeof(unsigned int));
	printf("long = %ld\n", sizeof(long));
	printf("long unsigned int = %ld\n", sizeof(long unsigned int));
	printf("sizeof(ngx_str_t) = %d\n", sizeof(ngx_str_t));   // 16
	
	ngx_pool_t *pool = ngx_create_pool(1024, NULL);
	
	ngx_list_t *l = ngx_list_create(pool, N, sizeof(ngx_str_t));
	
	int i = 0;
	for (i = 0;i < 24;i ++) {
	
		ngx_str_t *ptr = ngx_list_push(l);
		
		char *buf = ngx_palloc(pool, 32);
		sprintf(buf, "MyList %d node", i+1);
	
		ptr->len = strlen(buf);
		ptr->data = buf;
	}
	
	print_list(l);
	return 0;

}

// 编译命令
// gcc -o ngx_list_main ngx_list_main.c  -I ../nginx_folds/nginx/src/core/  -I ../nginx_folds/nginx/objs/ -I ../nginx_folds/nginx/src/os/unix/ -I ../nginx_folds/pcre-8.41/ -I ../nginx_folds/nginx/src/event/  ../nginx_folds/nginx/objs/src/core/ngx_list.o ../nginx_folds/nginx/objs/src/core/ngx_string.o ../nginx_folds/nginx/objs/src/core/ngx_palloc.o ../nginx_folds/nginx/objs/src/os/unix/ngx_alloc.o 
```

首先在说明ngx_pool_t内存池前，先介绍相关的15个方法

内存池操作
ngx_create_pool
ngx_destroy_pool
ngx_reset_pool

基于内存池的分配、释放内存操作
ngx_palloc : 分配地址对齐的内存。按总线的长度（例sizeof（unsigned long）对齐地址后，可以减少CPU读取内存的次数，当然代价是有一些内存浪费）
ngx_pnalloc ： 分配内存时不进行地址对齐
ngx_pcalloc ： 分配出地址对齐的内存后，在调用memset 将这些内存全部清0
ngx_pmemalign ： 按照alignment进行地址对齐来分配内存。注意，这样分配出的内存不管申请的size大小，都是不会使用小块内存池的，它会从进程的堆中分配内存，并挂在大块内存组成的large单链表中
ngx_pfree ： 提前释放大块内存。它的效率不高，其实就是遍历large链表，寻找ngx_pool_large_t 的alloc 成员等于待释放地址，找到后释放内存给操作系统，将ngx_pool_large_t 移出链表并删除

随着内存池释放同步释放资源的操作
ngx_pool_cleanup_add : 添加一个需要在内存池释放时同步释放的资源。
ngx_pool_run_cleanup_file ： 在内存池释放前，如果需要提前关闭文件（当然是调用过ngx_pool_cleanup_add 添加的文件，同时ngx_pool_cleanup_t 的handle成员被设为ngx_pool_cleanup_file）, 则调用该方法
ngx_pool_cleanup_file ： 以关闭文件来释放资源的方法，可以设置到ngx_pool_cleanup_t 的handle 成员
ngx_pool_delete_file ： 以删除文件来释放资源的方法，可以设置到ngx_pool_cleanup_t 的handle 成员

与内存池无关的分配、释放操作
ngx_alloc ： 从操作系统中分配内存
ngx_calloc ： 从操作系统中分配内存，在调用memset 把内存清0
ngx_free ： 释放内存到操作系统

Nginx 已经提供封装了malloc、free的ngx_alloc 、ngx_free 方法，那为什么还需要一个复杂的内存池呢？对于没有垃圾回收机制的c语言编写引用来说，最容易犯的错就是内存泄露。当分配内存与释放内存的逻辑相距遥远时，还很容易发生同一块内存被释放两次。内存池就是来降低程序员犯错误的机率的。模块开发者只需要关心内存的分配，而释放内存则交个内存池来释放。

ngx_pool_t内存池的设计上还考虑了小块内存的频繁分配在效率上有提升空间，以及内存碎片还可以在减少些。不过在这里需要定义一下什么叫小块内存，NGX_MAX_ALLOC_FROM_POOL宏是一个很重要的标准：
#NGX_MAX_ALLOC_FROM_POOL (ngx_pageisze -1)
可见，在x86架构上就是4095 字节，通常，小于等于NGX_MAX_ALLOC_FROM_POOL 就意味这小块内存。这也不是绝对的，当调用ngx_create_pool 创建内存池时，如果传递的size参数小于NGX_MAX_ALLOC_FROM_POOL + sizeof(ngx_pool_t)， 则对于这个内存池来说，size - sizeof(ngx_pool_t) 字节就是小块内存的标准。大块内存和小块内存的处理规则不同，这个在源码中看处理逻辑就知道了。

下面是有关ngx_pool_t 结构的一些数据结构：

```
// src/score/ngx_core.h
// 首先说一下这个变量问题，这里_s 和 _t 为什么这样定义，我搜到的答案给我的解释其中合理的是：
// _s 指的是struct 变量;  _t 指的是 某一个type类型
#include <ngx_config.h>

typedef struct ngx_module_s          ngx_module_t;
typedef struct ngx_conf_s            ngx_conf_t;
typedef struct ngx_cycle_s           ngx_cycle_t;
typedef struct ngx_pool_s            ngx_pool_t;
typedef struct ngx_chain_s           ngx_chain_t;
typedef struct ngx_log_s             ngx_log_t;
typedef struct ngx_open_file_s       ngx_open_file_t;
typedef struct ngx_command_s         ngx_command_t;
typedef struct ngx_file_s            ngx_file_t;
typedef struct ngx_event_s           ngx_event_t;
```



```
src/core/ngx_palloc.{h,c}

typedef struct {
	// 当前内存分配结束位置，即下一段可分配内存的位置 (指向未分配的空闲内存的首地址)
    u_char               *last;	
    u_char               *end;	// 小块内存池结束位置
    ngx_pool_t           *next;	// 指向下一内存的指针，内存池是通过链表连接的
    ngx_uint_t            failed;	//记录内存池分配失败的次数（4.4之后移向下一个小块内存池）
} ngx_pool_data_t;   // 内存池的数据结构模块
```



```
struct ngx_pool_s {
	// 内存池的数据块，结构如上，描述是小块内存池。当分配小块内存，剩余的空间不足时，会再分配1个ngx_pool_t， 它们会通过d中next成员构成单链表
    ngx_pool_data_t       d;		
    size_t                max;		// 评估申请内存属于小块内存还是大块内存的标准（=小块内存的最大值）
	

	// 多个小块内存池构成链表时，current 指向分配内存时遍历的第1个小块内存池
	ngx_pool_t           *current;
	
	// 与内存池关系不大
	ngx_chain_t          *chain;	// 指针指向chain结构
	
	// 大块内存都直接从进程的堆中分配，为了能够在销毁内存池时同时释放大块内存，就把
	// 每一次分配的大块内存通过ngx_pool_large_t 组成单链表挂在large成员上
	ngx_pool_large_t     *large;	// 指向大块内存链表，超过max的内存分配是不同规则的
	
	// 所有待清理资源（例如需要关闭或者删除的文件）以ngx_pool_cleanup_t 对象构成单链表挂在cleanup 成员上
	ngx_pool_cleanup_t   *cleanup;	// 释放内存池指针，内部包含函数指针结构，类似析构函数
	// 内存池执行中输出日志的对象
	ngx_log_t            *log;		// 内存分配的日志指针

};

// src/core/ngx_buf.h
struct ngx_chain_s {
    ngx_buf_t    *buf;
    ngx_chain_t  *next;
};

// ngx_palloc.h
struct ngx_pool_large_s {
	// 所有大块内存通过next 指针连在一起
    ngx_pool_large_t     *next;
    void                 *alloc;
};


struct ngx_pool_cleanup_s {
    ngx_pool_cleanup_pt   handler;
    void                 *data;
    ngx_pool_cleanup_t   *next;
};
```


从ngx_pool_s 结构中，可以知道，当申请的内存算是大块内存时(大于ngx_pool_t 的max成员)，是直接调用ngx_alloc 从进程的堆中分配的，同时会再分配一个ngx_pool_large_t 结构体挂在large链表中，其定义如上面的 ngx_pool_large_s ：

对于非常大的内存，如果它的生命周期远远的短于所述的内存池，那么在内存池销毁前提前的释放它就变得有意义了。而ngx_free 方法就是提前释放大块内存的，需要注意，它的实现是遍历large 链表，找打alloc 等于待师范地址的ngx_pool_large_t 后，调用ngx_free释放大块内存，但不是房ngx_pool_large_t结构体，而是把alloc 置为NULL。如此实现的意义是下次分配大块内存时，能复用这个结构体。所以可以想见，如果large 链表中的元素很多，那么ngx_free 的遍历损耗是很大，因此，最好不要调用ngx_pfree。

在看看小块内存，通过从进程的堆中预分配更过的内存（ngx_create_pool 的size参数决定分配的大小），而后直接使用这块内存的一部分作为小块内存返回给申请者，以此实现减少碎片和调用malloc 的次数。它们是放在成员d中维护管理的，看看ngx_pool_data_t 是如何定义。

```
//src/core/ngx_palloc.h  
ngx_pool_t *
ngx_create_pool(size_t size, ngx_log_t *log)
{
    ngx_pool_t  *p; // 这里就占80字节，里面有8个指针，两个long int 变量

    p = ngx_memalign(NGX_POOL_ALIGNMENT, size, log);
    if (p == NULL) {
        return NULL;
    }
    
    p->d.last = (u_char *) p + sizeof(ngx_pool_t);
    p->d.end = (u_char *) p + size;
    p->d.next = NULL;
    p->d.failed = 0;
    
    size = size - sizeof(ngx_pool_t);
    p->max = (size < NGX_MAX_ALLOC_FROM_POOL) ? size : NGX_MAX_ALLOC_FROM_POOL;
    
    p->current = p;
    p->chain = NULL;
    p->large = NULL;
    p->cleanup = NULL;
    p->log = log;
    
    return p;

}
```



```
void
ngx_destroy_pool(ngx_pool_t *pool)
{
    ngx_pool_t          *p, *n;
    ngx_pool_large_t    *l;
    ngx_pool_cleanup_t  *c;

    for (c = pool->cleanup; c; c = c->next) {
        if (c->handler) {
            ngx_log_debug1(NGX_LOG_DEBUG_ALLOC, pool->log, 0,
                           "run cleanup: %p", c);
            c->handler(c->data);
        }
    }

#if (NGX_DEBUG)

    /*
     * we could allocate the pool->log from this pool
     * so we cannot use this log while free()ing the pool
     */
    
    for (l = pool->large; l; l = l->next) {
        ngx_log_debug1(NGX_LOG_DEBUG_ALLOC, pool->log, 0, "free: %p", l->alloc);
    }
    
    for (p = pool, n = pool->d.next; /* void */; p = n, n = n->d.next) {
        ngx_log_debug2(NGX_LOG_DEBUG_ALLOC, pool->log, 0,
                       "free: %p, unused: %uz", p, p->d.end - p->d.last);
    
        if (n == NULL) {
            break;
        }
    }

#endif

    for (l = pool->large; l; l = l->next) {
        if (l->alloc) {
            ngx_free(l->alloc);
        }
    }
    
    for (p = pool, n = pool->d.next; /* void */; p = n, n = n->d.next) {
        ngx_free(p);
    
        if (n == NULL) {
            break;
        }
    }

}
```


下图是将ngx_pool_t 的内存逻辑画出来，可以直观地参考下图进行学习：

下图是将ngx_pool_t 的内存逻辑画出来，可以直观地参考下图进行学习：

下面写了一个演示代码：
附上运行结果，可结合上面的逻辑图来看，还是比较清楚的。

```
pool_test.cpp


#include <iostream>
#include <string>

extern "C" {
    #include "ngx_config.h"
    #include "ngx_conf_file.h"
    #include "nginx.h"
    #include "ngx_core.h"
    #include "ngx_string.h"
    #include "ngx_palloc.h"
}
using namespace std;

volatile ngx_cycle_t  *ngx_cycle;

void ngx_log_error_core(ngx_uint_t level, ngx_log_t *log, ngx_err_t err,
            const char *fmt, ...)
{
}

void dump_pool(ngx_pool_t* pool)
{
    printf("------------start--------------\n");
    while (pool)
    {
        printf("pool = 0x%x\n", pool);
        printf("  .d\n");
        printf("    .last = 0x%x\n", pool->d.last);
        printf("    .end = 0x%x\n", pool->d.end);
        printf("    .next = 0x%x\n", pool->d.next);
        printf("    .failed = %d\n", pool->d.failed);
        printf("  .max = %d\n", pool->max);
        printf("  .current = 0x%x\n", pool->current);
        printf("  .chain = 0x%x\n", pool->chain);
        printf("  .large = 0x%x\n", pool->large);
        printf("  .cleanup = 0x%x\n", pool->cleanup);
        printf("  .log = 0x%x\n", pool->log);
        printf("available pool memory = %d\n", pool->d.end - pool->d.last);
        pool = pool->d.next;
    }
    printf("------------end--------------\n");
}

int main()
{
    ngx_pool_t *pool;

    printf("--------------------------------\n");
    printf("create a new pool:\n");
    pool = ngx_create_pool(1024, NULL);
    dump_pool(pool);


​    

    printf("alloc block 1 from the pool:\n");
    // printf("--------------------------------\n");
    ngx_palloc(pool, 512);
    dump_pool(pool);
     
    // printf("--------------------------------\n");
    printf("alloc block 2 from the pool:\n");
    // printf("--------------------------------\n");
    ngx_palloc(pool, 512);
    dump_pool(pool);
     
    printf("alloc block 3 from the pool :\n");
    ngx_palloc(pool, 512);
    dump_pool(pool);
    
    printf("alloc block 4 from the pool :\n");
    ngx_palloc(pool, 512);
    dump_pool(pool);
     
    ngx_destroy_pool(pool);
    return 0;

}
```




--------------------------------

```
create a new pool:
------------start--------------
pool = 0x3f1da2c0
  .d
    .last = 0x3f1da310
    .end = 0x3f1da6c0
    .next = 0x0
    .failed = 0
  .max = 944
  .current = 0x3f1da2c0
  .chain = 0x0
  .large = 0x0
  .cleanup = 0x0
  .log = 0x0
available pool memory = 944

------------end--------------
alloc block 1 from the pool:
------------start--------------
pool = 0x3f1da2c0
  .d
    .last = 0x3f1da510
    .end = 0x3f1da6c0
    .next = 0x0
    .failed = 0
  .max = 944
  .current = 0x3f1da2c0
  .chain = 0x0
  .large = 0x0
  .cleanup = 0x0
  .log = 0x0
available pool memory = 432

------------end--------------
alloc block 2 from the pool:
------------start--------------
pool = 0x3f1da2c0
  .d
    .last = 0x3f1da510
    .end = 0x3f1da6c0
    .next = 0x3f1da6d0
    .failed = 0
  .max = 944
  .current = 0x3f1da2c0
  .chain = 0x0
  .large = 0x0
  .cleanup = 0x0
  .log = 0x0
available pool memory = 432

pool = 0x3f1da6d0
  .d
    .last = 0x3f1da8f0
    .end = 0x3f1daad0
    .next = 0x0
    .failed = 0
  .max = 0
  .current = 0x0
  .chain = 0x0
  .large = 0x0
  .cleanup = 0x0
  .log = 0x0
available pool memory = 480

------------end--------------
alloc block 3 from the pool :
------------start--------------
pool = 0x3f1da2c0
  .d
    .last = 0x3f1da510
    .end = 0x3f1da6c0
    .next = 0x3f1da6d0
    .failed = 1                 // 开始只分配了1024个B空间， 分了两次512B空间后，再次分配的就会失败
  .max = 944
  .current = 0x3f1da2c0
  .chain = 0x0
  .large = 0x0
  .cleanup = 0x0
  .log = 0x0
available pool memory = 432     // 需要减去pool 表头占的前80 B, 1024 - 512 - 80 

pool = 0x3f1da6d0
  .d
    .last = 0x3f1da8f0
    .end = 0x3f1daad0
    .next = 0x3f1daae0
    .failed = 0
  .max = 0
  .current = 0x0
  .chain = 0x0
  .large = 0x0
  .cleanup = 0x0
  .log = 0x0
available pool memory = 480   // 第二个节点只用减去4 * 8 B,  1024 - 512 - 32

pool = 0x3f1daae0
  .d
    .last = 0x3f1dad00
    .end = 0x3f1daee0
    .next = 0x0
    .failed = 0
  .max = 0
  .current = 0x0
  .chain = 0x0
  .large = 0x0
  .cleanup = 0x0
  .log = 0x0
available pool memory = 480     // 1024 - 512 - 32 , 后面每次都是这个数

------------end--------------
```


最后我制作了一个里面包含了我学习nginx的一个镜像， 现在你们只需要执行下面命令就能拉取学习需要的环境了

最后我制作了一个里面包含了我学习nginx的一个镜像， 现在你们只需要执行下面命令就能拉取学习需要的环境了

docker pull registry.cn-hangzhou.aliyuncs.com/aclj/nginx:1.0.1

如果需要项目环境源代码，可以参考此github， 后续可根据需求增加东西，欢迎star~
————————————————
版权声明：本文为CSDN博主「_刘小雨」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_39486027/article/details/124728540

# 【NO.476】nginx自定义实现一个计量模块

## 1.意义

HTTP 过滤模块与普通模块的功能是i完全不同的，下面先回顾一下HTTP模块有哪些功能。
首先HTTP框架为HTTP请求的处理过程定义了11个阶段，相关代码如下：

```
typedef enum {
    // server 读完之后怎么处理
    NGX_HTTP_POST_READ_PHASE = 0,   // ngx_http_init_phases

    // 读完之后对中间数据修改
    NGX_HTTP_SERVER_REWRITE_PHASE,  // ngx_http_init_phases
    
    // load 配置文件
    // 重写
    // 重写之后的操作
    NGX_HTTP_FIND_CONFIG_PHASE,
    NGX_HTTP_REWRITE_PHASE,         // ngx_http_init_phases
    NGX_HTTP_POST_REWRITE_PHASE,
    
    // 读取文件之前、之中、之后怎么操作
    NGX_HTTP_PREACCESS_PHASE,       // ngx_http_init_phases
    NGX_HTTP_ACCESS_PHASE,          // ngx_http_init_phases
    NGX_HTTP_POST_ACCESS_PHASE,
    
    // 回复内容之前、之后的操作
    NGX_HTTP_PRECONTENT_PHASE,      // ngx_http_init_phases
    NGX_HTTP_CONTENT_PHASE,         // ngx_http_init_phases
    
    // log信息 
    NGX_HTTP_LOG_PHASE              // ngx_http_init_phases

} ngx_http_phases;
```


HTTP 框架允许普通的HTTP处理模块介入其中的7个阶段处理请求，但是通常大部分HTTP模块（官方模块或者第三方模块）都只在NGX_HTTP_CONTENT_PHASE 阶段处理请求。在这个阶段处理请求有一个特点，即HTTP模块有两种介入方法，第一种方法是，任一个HTTP模块会对所有的用户请求产生作用，第二种方法是，只对请求的URI 匹配了nginx.conf 中的某些location 表达式下的HTTP模块起作用。也就是说如果多个HTTP模块共同处理一个请求，则多半是由subrequest功能完成，即将原始请求分为多个子请求，每个子请求在由一个HTTP模块在NGX_HTTP_CONTENT_PHASE 阶段处理。

HTTP 框架允许普通的HTTP处理模块介入其中的7个阶段处理请求，但是通常大部分HTTP模块（官方模块或者第三方模块）都只在NGX_HTTP_CONTENT_PHASE 阶段处理请求。在这个阶段处理请求有一个特点，即HTTP模块有两种介入方法，第一种方法是，任一个HTTP模块会对所有的用户请求产生作用，第二种方法是，只对请求的URI 匹配了nginx.conf 中的某些location 表达式下的HTTP模块起作用。也就是说如果多个HTTP模块共同处理一个请求，则多半是由subrequest功能完成，即将原始请求分为多个子请求，每个子请求在由一个HTTP模块在NGX_HTTP_CONTENT_PHASE 阶段处理。

然而,HTTP过滤模块则不同于此，一个请求可以被任意多个HTTP 过滤模块处理。

## 2.实现一个自己的模块能用在哪些方面呢

黑白名单ip, 固定的ip 能访问
特定的请求不响应
流量控制
转发的功能
源ip地址修改
对具体的请求加一些信息
…
其实我们会发现上面实现模块对应的功能是对应http 对应的11个阶段， 对其中的某一个阶段或者多个阶段进行修改就是需要实现自己模块的功能了。

## 3.自定义模块

下面是自定义实现的一个ip 计量的HTTP 模块， 在N个客户端进行访问时候统计访问次数

### 3.1 首先需要明确下面几个问题

如何快速查找对应的客户端
工作在http 的哪一个阶段
客户端在多次请求时不同进程上，如何进行统计（多进程间如何共享）

### 3.2 用到的技术

数据结构 （nginx内部的红黑树 rbtree）
slab (共享内存)

### 3.3 nginx 中配置文件如何用代码结合

```
ngx_command_t ngx_http_location_count_cmd[] = {
    {
        ngx_string("count"),
        NGX_HTTP_LOC_CONF | NGX_CONF_NOARGS,   // 写在location里面， 不带参数
        ngx_http_location_count_create_cmd_set,
        NGX_HTTP_LOC_CONF_OFFSET,
        0,
        NULL
    }, 
    ngx_null_command                        // 结尾标识符, 类似字符串中的 \0
};
```

### 3.4 nginx 解析http 的顺序和对应的函数

```
static ngx_http_module_t ngx_http_location_count_ctx = {
    // 提供8个方法，解析配置文件的
    // 解析 http {}
    // 1 8 2 7 3 6 4 5 这种顺序解析的
    NULL,   // preconfiguration
    NULL,   // postconfiguration

    NULL,   // create_main_conf
    NULL,   //init_main_conf
    
    NULL,   // create_srv_conf
    NULL,   // merge_srv_conf
    
    ngx_http_location_count_create_loc_conf,   // create_loc_conf
    NULL,   // merge_loc_conf

};
```



### 3.5节点（客户端）的统计

节点（客户端）的统计

```
// 这里添加红黑树的初始化
ngx_rbtree_init(&conf->lcshm->rbtree, &conf->lcshm->sentinel,  \
gx_rbtree_insert_value);

// 节点的插入
node->key = key;
node->data = 1;
ngx_rbtree_insert(&conf->lcshm->rbtree, node);
    
// 遍历节点
ngx_rbtree_node_t *node = ngx_rbtree_min(conf->lcshm->rbtree.root, \
conf->lcshm->rbtree.sentinel);

do {
	// 提取node 信息
	// ...


	node = ngx_rbtree_next(&conf->lcshm->rbtree, node);

} while (node);
```

### 3.6 全部代码如下

```
#include <ngx_http.h>
#include <ngx_config.h>
#include <ngx_core.h>

typedef struct {
    ngx_rbtree_t rbtree;
    ngx_rbtree_node_t sentinel;
}ngx_http_location_count_shm_t;   // 共享内存

typedef struct  {

	ssize_t shmsize;
	ngx_slab_pool_t *pool;
	
	ngx_http_location_count_shm_t* lcshm;
	
	//ngx_uint_t interval;    // 后面参数的属性
	//ngx_uint_t client_count;

} ngx_http_location_conf_t;


static void* ngx_http_location_count_create_loc_conf(ngx_conf_t *cf);
static char *ngx_http_location_count_create_cmd_set(ngx_conf_t *cf, ngx_command_t *cmd, void *conf);
static ngx_int_t ngx_http_location_count_shm_zone_init(ngx_shm_zone_t *zone, void *data);
static ngx_int_t ngx_http_location_count_handler(ngx_http_request_t *r);

// static void ngx_http_location_count_rbtree_insert (ngx_rbtree_node_t *root,
//     ngx_rbtree_node_t *node, ngx_rbtree_node_t *sentinel);

// 红黑树的遍历   方便html的显示各个客户端访问次数
static int ngx_encode_http_page_rb(ngx_http_location_conf_t *conf, char *html);

// 查找函数，在共享内存中找到相应的信息
// 查找    插入
static ngx_int_t ngx_http_pagecount_lookup(ngx_http_request_t *r, ngx_http_location_conf_t *conf, ngx_uint_t key);


ngx_command_t ngx_http_location_count_cmd[] = {
    {
        ngx_string("count"),
        NGX_HTTP_LOC_CONF | NGX_CONF_NOARGS,   // 写在location里面， 不带参数
        ngx_http_location_count_create_cmd_set,
        NGX_HTTP_LOC_CONF_OFFSET,
        0,
        NULL
    }, 
    ngx_null_command                        // 结尾标识符
};


static ngx_http_module_t ngx_http_location_count_ctx = {
    // 提供8个方法，解析配置文件的
    // 解析 http {}
    // 1 8 2 7 3 6 4 5 这种顺序解析的
    NULL,   // preconfiguration
    NULL,   // postconfiguration

    NULL,   // create_main_conf
    NULL,   //init_main_conf
    
    NULL,   // create_srv_conf
    NULL,   // merge_srv_conf
    
    ngx_http_location_count_create_loc_conf,   // create_loc_conf
    NULL,   // merge_loc_conf

};

ngx_module_t ngx_http_location_count_module = {
    NGX_MODULE_V1,                  // 前面7个参数
    &ngx_http_location_count_ctx,
    ngx_http_location_count_cmd,
    NGX_HTTP_MODULE,
    NULL,
    NULL,
    NULL,
    NULL,
    NULL,
    NULL,
    NULL,
    NGX_MODULE_V1_PADDING  // 全部是填充 
};

static void* ngx_http_location_count_create_loc_conf(ngx_conf_t *cf)
{
    ngx_http_location_conf_t* conf = ngx_palloc(cf->pool, sizeof(ngx_http_location_conf_t));
    if (conf == NULL)
        return NULL;

    // ngx_log_error(NGX_LOG_EMERG, cf->log, ngx_errno, "ngx_http_location_count_create_loc_conf");
    return conf;

}

// nginx.conf --> count
// curl -I http://localhost/test
static char *ngx_http_location_count_create_cmd_set(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
{
    // count 配置文件中的
    // 如果设置的多个参数是放在这里的  cf->args->elts

    // 由上面的带入进来
    ngx_http_location_conf_t* lconf = (ngx_http_location_conf_t*)conf;
    ngx_str_t name = ngx_string("location_count_slab");
    lconf->shmsize = 128 * 1024;
    
    // 向系统提交一个我们分配这么大的内存给我们自定义模块使用
    // 这里是提交申请
    ngx_shm_zone_t* zone = ngx_shared_memory_add(cf, &name, lconf->shmsize, &ngx_http_location_count_module);
    
    if(zone == NULL) // 分配失败
    {
        return NGX_CONF_ERROR;
    }
    
    // 真正申请
    zone->init =  ngx_http_location_count_shm_zone_init;
    zone->data = lconf;
    
    // handler 
    ngx_http_core_loc_conf_t *corecf = ngx_http_conf_get_module_loc_conf(cf, ngx_http_core_module);
    corecf->handler = ngx_http_location_count_handler;


    // ngx_log_error(NGX_LOG_EMERG, cf->log, ngx_errno, "ngx_http_location_count_create_cmd_set");
    
    return NULL;

}


static ngx_int_t ngx_http_location_count_shm_zone_init(ngx_shm_zone_t *zone, void *data)
{

    ngx_http_location_conf_t* conf;
    
    //这里需要添加东西
    ngx_http_location_conf_t* oconf = data;
    conf = (ngx_http_location_conf_t*)zone->data;
    if(oconf) 
    {
        conf->lcshm = oconf->lcshm;
        conf->pool = oconf->pool;
        return NGX_OK;
    }
    // printf("ngx_http_location_count_shm_zone_init 00000\n");
    conf->pool = (ngx_slab_pool_t*)zone->shm.addr;
    conf->lcshm = ngx_slab_alloc(conf->pool, sizeof(ngx_http_location_count_shm_t));
    if(conf->lcshm == NULL)
    {
        return NGX_ERROR;
    }
    
    conf->pool->data = conf->lcshm;
    // printf("ngx_http_location_count_shm_zone_init 11111\n");


    // 这里添加红黑树的初始化
    ngx_rbtree_init(&conf->lcshm->rbtree, &conf->lcshm->sentinel, ngx_rbtree_insert_value);
    // ngx_rbtree_init(&conf->lcshm->rbtree, &conf->lcshm->sentinel, ngx_http_location_count_rbtree_insert);


    return NGX_OK;

}

ngx_int_t ngx_http_location_count_handler(ngx_http_request_t *r)
{
    u_char html[1024] = {0};
	int len = sizeof(html);
    ngx_uint_t key = 0;


    struct sockaddr_in* client_addr = (struct sockaddr_in*)r->connection->sockaddr;
    
    ngx_http_location_conf_t* conf = ngx_http_get_module_loc_conf(r, ngx_http_location_count_module);
    key = client_addr->sin_addr.s_addr;
    
    // key , value          // ip地址和访问次数
    ngx_shmtx_lock(&conf->pool->mutex);
    ngx_http_pagecount_lookup(r, conf, key);
    ngx_shmtx_unlock(&conf->pool->mutex);
    
    ngx_encode_http_page_rb(conf, (char*)html);
    
    // ngx_log_error(NGX_LOG_EMERG, r->connection->log, ngx_errno, "ngx_http_location_count_handler");
    
    // 返回显示
    
    // 发送http头
    r->headers_out.status = 200;
    ngx_str_set(&r->headers_out.content_type, "text/html");
    ngx_http_send_header(r);  
    
    // body  body中可能返回多个客户端访问次数的列表，是一个前端显示的过程
    ngx_buf_t* b = ngx_pcalloc(r->pool, sizeof(ngx_buf_t));
    
    ngx_chain_t out;
    out.buf = b;
    out.next = NULL;
    
    b->pos = html;
    b->last = html + len;
    b->memory = 1;
    b->last_buf = 1;
    
    return ngx_http_output_filter(r, &out);

}

// 红黑树插入方法  ngx_rbtree_insert_value (直接用nginx中的函数)
#if 0
void ngx_http_location_count_rbtree_insert (ngx_rbtree_node_t *temp,
    ngx_rbtree_node_t *node, ngx_rbtree_node_t *sentinel)
{
    ngx_rbtree_node_t **p;
   //ngx_http_testslab_node_t *lrn, *lrnt;

    for (;;)
    {
        if (node->key < temp->key)
        {
            p = &temp->left;
        }
        else if (node->key > temp->key) {
           	p = &temp->right;
        }
        else
        {
          	return ;
        }
     
        if (*p == sentinel)
        {
            break;
        }
     
        temp = *p;
    }
     
    *p = node;
     
    node->parent = temp;
    node->left = sentinel;
    node->right = sentinel;
    ngx_rbt_red(node);

}
#endif

// 红黑树的查找，插入
// 参数r 方便日志的打印
ngx_int_t ngx_http_pagecount_lookup(ngx_http_request_t *r, ngx_http_location_conf_t *conf, ngx_uint_t key)
{
    ngx_rbtree_node_t *node, *sentinel;

	node = conf->lcshm->rbtree.root;
	sentinel = conf->lcshm->rbtree.sentinel;
	
	while(node != sentinel)   //  node == sentinel 需要进行下面操作，在slab中分配节点
	{
	    if(key < node->key)
	    {
	        node = node->left;
	        continue;
	    } 
	    else if (key > node->key)
	    {
	        node = node->right;
	        continue;
	    }
	    else
	    {
	        node->data ++;   // 找到了,  需要做一个原子操作
	        return NGX_OK;
	    }
	}
	
	// 添加之前 需要在slab中分配一个节点
	node = ngx_slab_alloc_locked(conf->pool, sizeof(ngx_rbtree_node_t));
	if (NULL == node) {
		return NGX_ERROR;
	}
	
	node->key = key;
	node->data = 1;
	ngx_rbtree_insert(&conf->lcshm->rbtree, node);
	
	return NGX_OK;

}

static int ngx_encode_http_page_rb(ngx_http_location_conf_t *conf, char *html)
{

    sprintf(html, "<h1>Ip Access Count</h1>");
    strcat(html, "<h2>");
    
    ngx_rbtree_node_t *node = ngx_rbtree_min(conf->lcshm->rbtree.root, conf->lcshm->rbtree.sentinel);
    
    do {
    
    	char str[INET_ADDRSTRLEN] = {0};
    	char buffer[128] = {0};
    
    	sprintf(buffer, "req from : %s, count: %d <br/>",
    		inet_ntop(AF_INET, &node->key, str, sizeof(str)), node->data);
    
    	strcat(html, buffer);
    
    	node = ngx_rbtree_next(&conf->lcshm->rbtree, node);
    
    } while (node);
    
    strcat(html, "</h2>");
    
    return NGX_OK;

}
```


上述的编译命令，环境我全部放在自定义中的镜像中了， 现在可以动动小手就可以拉取学习的环境了~

上述的编译命令，环境我全部放在自定义中的镜像中了， 现在可以动动小手就可以拉取学习的环境了~

docker pull registry.cn-hangzhou.aliyuncs.com/aclj/nginx:1.0.1

如果需要项目环境源代码，可以参考此github， 后续可根据需求增加东西，欢迎star~

参考：
后台服务器：https://course.0voice.com/v1/course/intro?courseId=5&agentId=0
————————————————
版权声明：本文为CSDN博主「_刘小雨」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_39486027/article/details/124733639

# 【NO.477】协程的调度实现与性能测试

那我怎么在简历里面写协程，协程这个东西实在真的太好用了，你可以跟很多东西结合到一起，比如说你们对文件，做文件操作可不可以用？
好对文件操作，比如说你做日志落盘的时候，可不可以用协程来操作它也是可以的，比如说你对数据库的操作，对数据库的操作，还有包括像一些网络io的处理，这个文件的操作和网络io都是针对文件io来处理。

然后我们尽量知道他用到哪里，协程怎么用到数据库？这是我们今天等一下跟大家讲到的讲到的就是关于携程的API的封装。

**一个线程里面多个协程是怎么运行的？**
好比多个线程在一个进程中是怎么运行的，
每一个协程当它遇到io操作的时候，它就会去判断io是否就绪，如果没有就绪就会让出，**这个让出它让给谁了？**
站在应用层：让给其它协程这个说法是对的，
站在协程设计者的角度，这个协程是让给调度器了。
由调度器判断哪一个协程准备就绪了，resume该协程
大量的时间运行在哪？是在调度器上面



![在这里插入图片描述](https://img-blog.csdnimg.cn/d51d5ee535d044b390117e2d51f0ff88.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)



这里一个问题我们先留在这个地方，吧就是这个协程如果一直不让出。

![在这里插入图片描述](https://img-blog.csdnimg.cn/97160158a09347808747b3aaf7752e67.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)



![在这里插入图片描述](https://img-blog.csdnimg.cn/91df0fe37f434dc3b7e9b9159f8305c0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_13,color_FFFFFF,t_70,g_se,x_16)



![在这里插入图片描述](https://img-blog.csdnimg.cn/f05553234cc04dd0ad2a7ad728141264.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)



```cpp
#include "nty_coroutine.h"



 



#include <arpa/inet.h>



 



#define MAX_CLIENT_NUM            1000000



#define TIME_SUB_MS(tv1, tv2)  ((tv1.tv_sec - tv2.tv_sec) * 1000 + (tv1.tv_usec - tv2.tv_usec) / 1000)



 



 



void server_reader(void *arg) {



    int fd = *(int *)arg;



    int ret = 0;



 



 



    struct pollfd fds;



    fds.fd = fd;



    fds.events = POLLIN;



 



    while (1) {



        



        char buf[1024] = {0};



        ret = nty_recv(fd, buf, 1024, 0);



        if (ret > 0) {



            if(fd > MAX_CLIENT_NUM) 



            printf("read from server: %.*s\n", ret, buf);



 



            ret = nty_send(fd, buf, strlen(buf), 0);



            if (ret == -1) {



                nty_close(fd);



                break;



            }



        } else if (ret == 0) {    



            nty_close(fd);



            break;



        }



 



    }



}



 



 



void server(void *arg) {



 



    unsigned short port = *(unsigned short *)arg;



    free(arg);



 



    int fd = nty_socket(AF_INET, SOCK_STREAM, 0);



    if (fd < 0) return ;



 



    struct sockaddr_in local, remote;



    local.sin_family = AF_INET;



    local.sin_port = htons(port);



    local.sin_addr.s_addr = INADDR_ANY;



    bind(fd, (struct sockaddr*)&local, sizeof(struct sockaddr_in));



 



    listen(fd, 20);



    printf("listen port : %d\n", port);



 



    



    struct timeval tv_begin;



    gettimeofday(&tv_begin, NULL);



 



    while (1) {



        socklen_t len = sizeof(struct sockaddr_in);



        int cli_fd = nty_accept(fd, (struct sockaddr*)&remote, &len);



        if (cli_fd % 1000 == 999) {



 



            struct timeval tv_cur;



            memcpy(&tv_cur, &tv_begin, sizeof(struct timeval));



            



            gettimeofday(&tv_begin, NULL);



            int time_used = TIME_SUB_MS(tv_begin, tv_cur);



            



            printf("client fd : %d, time_used: %d\n", cli_fd, time_used);



        }



        printf("new client comming\n");



 



        nty_coroutine *read_co;



        nty_coroutine_create(&read_co, server_reader, &cli_fd);



 



    }



    



}



 



 



 



int main(int argc, char *argv[]) {



    nty_coroutine *co = NULL;



 



    int i = 0;



    unsigned short base_port = 9096;



    for (i = 0;i < 100;i ++) {



        unsigned short *port = calloc(1, sizeof(unsigned short));



        *port = base_port + i;



        nty_coroutine_create(&co, server, port); ////////no run



    }



 



    nty_schedule_run(); //run



 



    return 0;



}



 



 



 
```

首先如果这个没有准备就绪，就把这个 io加入到epoll里面,这个epoll是schedule的epoll，也就是说对着这个图，
每一个协程他对io的操作，做的一件事情判端io有没有就绪，没有就绪把它加入到epoll里面。
请大家注意这epoll是一个全局的是所有协程都共用这一个，它是管理的所有的io的，把它加入进去之后，剩下的一个事情就是nty_coroutine_yield,
然后这个地方把它让出,让出请大家注意这里有1个点，就是回到了我们调度器的地方，回到调度器的地方，nty_schedule_search_wait在这个地方。
好，这里大家看到就是在前面是判断，如果这个循环里面往下面走nty_coroutine_resume，是返回到协程里面，请注意这里有个过程，
如果单独一个协调看不出来，用多个的时候，你才能够体现这个调度器的作用，
也就是说a让出之后执行到底这个点，请记住这个点就是我们要的这个点，然后在调度器往下走的时候再次返回。
这个resume的过程是走到了他最初yield的地方。
好，这样他就完成了整个时间片，

就是说这个让出的点，为什么io没有准备就绪，它能够切换出去。

就是一开始我们进行io操作之前把他加入epoll里面，加入之后，我们再让出，调度器在开始运行，调度器然后里面去判断哪些准备就绪了，然后resume到另一个协程，就这样一个过程

这就是协程在运行的时候，它底下就这样的，n多个协程，然后每一个都是从
调度器恢复到协程里面去，这就是调度器它的核心的原理就这样，很多朋友跟我聊的时候，就是这个协程的调度这让出和恢复，它怎么切换的这个点不懂。

关于现在协程一直不让出，
**就是这协程首先进行是为了解决io等待挂起的问题，**挂起的问题，啊解决io等待挂起的问题，
那如果这个协程它本身里面没有这种io操作的话，那么用协程的意义不大，
那就可以直接单独的你用线程去处理是一样的是一样的，那这个处理它就跟同步差不多，如果没有io操作，就不用东西。如果在跟这些朋友在讨论这种问题的时候，也时刻记录，如果没io操作，用协程干什么，可以用rpc底层的框架。

调度器里面在实现的时候，大家可以看到代码上面只做了三部分集合，只做了三部分集合：一部分是说的我们睡眠等待这个时间，第二部分就是就绪，第三部分就是io的等待。

这里只做了三个集合，第一个是说的我们做休眠的时候，这个时间超时的时候，那这里面有一个过程有一个情况就是关于这个时间就是sleep睡眠，
就是比如说我休眠两毫秒，现在我把当前的这个协程加入到一个sleep_tree里面，那对应说我们下次接下来再有一个休眠的时间，
又加入这个树里面，又有一个休眠我再加入树里面，那各位朋友就不太理解，**就比如说如果中间有一些时间相同的怎么办？**

是这样构建这样一个树，
构成这样一个数，这个数里面它的key就是时间，它的内容树在操作的时候是以key，value,那这个key是我们的时间，value对应的就是这个协程，那这个 key有没有冲突呢？

大家肯定会想到这个问题，就如果他说这个 key冲突的话怎么办？

就是如果当时间冲突的时候，各位朋友们在这里比如说我们一次性把它插入的时候，发现时间有一点点有冲突的话，会判断插入失败，
那插入失败了怎么办？请大家注意再次插入。
那再次插入在这边做了一个事情，就是对这个时间加上这么一丢丢，你再加上这么1纳秒加上一丢丢，

那我们可以看到
在这地方比如说我们调度nty_schedule_sched_sleepdown就是再去找到，我们这个协程里面，我们插入一个key的时候，如果时间冲突了，大家就可以看到
插入的时候如果插入失败，我们把这个sleep_usecs ++

![在这里插入图片描述](https://img-blog.csdnimg.cn/4575ca85802e4af09ef7bff2ab5ee3c8.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


，请大家注意这是一个很小单位的加加就加一丢，丢那加完之后再次继续continue插入，
这样即使这里面多那么一丢丢，比如多那么一纳秒或者少了一纳秒，其实它不影响不影响这个调度，
因为对协程来说本身它的实时性它要求没那么高，
**关于时间冲突，我们就给他再加上一丢丢，就是定时器的冲突也是这么解。**



这是关于这个插入的时间。
再紧接着第二个方案，就是这是第一块判断这个时间哪些时间到期了，哪些时间到期了，我们就可以开始运行，哪些时间到期了，我们从中找到一个时间到期了，我们开始 resume恢复它的运行。

![在这里插入图片描述](https://img-blog.csdnimg.cn/322db1d494a545dcbbd5db1c63219de1.png)



然后第二部分就是就绪，就是新建的也是加入到就绪里面，然后再去判断。这是采用一个就绪队列，队列里面拿出第一个点开始运行。

![在这里插入图片描述](https://img-blog.csdnimg.cn/be177c1971f84531a84b032cc5d13f54.png)



第三个就给大家讲到的就是 io等待，其实在大量的io的时候，他会是在第三个步骤，是在第三个步骤里面的数量是很多的，而前面的两者第一个的它是很少的，第二个的会比第一个要多也不是很多，但是第三个是最多的，
或者你也可以想象，比如说我们现在有1000个协程现在创建完，其实你要发现每一个协程里面去加入一个sleep，这种情况是很少很少，它是主要是解决一个new，刚开始创建的时候，这个状态其实大量的是在处理io等待的时候这样一个情况。

就是一个红黑树只是把它写成宏定义了，是为了让我们多次去定义红黑树，也就是说我们在这个里面我们可能不止定义一颗红黑树，要定义多个红黑树，所以我在这里写就把它用了一个宏定义的方法，

**再跟大家讲讲就是关于这个协程多核的问题，**
解决形成多核的问题有这么几种方式，
有这么三种方式，第一种我们可以采用多进程，多进程的好处，实现起来很容易，就是我每一个进程里面，进程是单线程的，每一个进程里面都有一个调度器，那我们这个协程代码本身我们是不需要去改的。
第二个是采用的多线程的模式，
多线程会比多进程复杂很多，就是个多线程和多进程怎么联系一起的联系的，
为什么利用多核，我们是利用多CPU并行的去执行，是利用CPU的计算能力，那请注意多核来用的时候我们采用多线程和多进程。
我们可以每一个线程或者每个进程做亲缘性，
比如说每一个线程绑定一个CPU，亲和一个CPU，每一个进程亲和一个CPU，就是这样来做到多核的支持。
那有同学有没有一种不用多线程或者多进程呢？也可以采用叉86体系结构提供出来的指令实现

![在这里插入图片描述](https://img-blog.csdnimg.cn/72e2b2439d7840d79a710298cb6d94eb.png)



关于ntyco里面做的呢,是采用多进程做的，
因为这个很容易因为这个好做，然后呢这个多线程这个它不是很好写，什么意思？这个多线程中间就需要对我们的调度器进行加锁，
这个调度器加锁怎么加呢大家，你现在也想一想，

这里main函数进来之后，然后for循环监听100个端口，
0~99，然后创建协程100个，
然后里头做的事情就是监听，我们把它改成多线程也是ok，怎么改呢？

我们在这里直接把它创建多个线程，关于这个协程这个过程中间在调度的时候，
因为协程就出现一个现象，就是协程a在一个线程里面，协程b在一个线程里面，协程c在一个线程里面，协程d在一个线程里面，
他每一个协程都在不同的线程里面，**那么在调度的时候，请问你它核心加锁地方加在哪了？**

就是在调度器调度的时候加锁，
协程本身是没有关系的，是不会影响，就是对于应用层而言我们是不用管，我们压根就是代码还是这么去写，代码还是这么去写，但是请大家注意，
就是关于这个协程调度的时候，比如我们加入sleep这种树的时候，我们要对它进行加锁，
再往下面走这一层，它里面拿出一个已经超时的节点的时候，在里面
我们要对它进行一个加锁，也就是对于红黑树里面我们去找出一个节点出来，这个点要加锁，**如果我们要多线程支持这个锁我们定义在哪里？**

这个锁定义在调度器里面，全局的没错，在调度器里面，在调度器的这个结构体里面可以引入一个概念，我们在这里面我们可以加上一个，定义一个COROUTINE_MP他是否支持这个东西，这个里面就跟在这里面进行加锁，**那加这个锁的过程中间有哪些锁可以加？**。

那对于红黑树的话，就以sleep这个为例，
这个地方用红黑树的话，我们采用互斥锁是ok的。

![在这里插入图片描述](https://img-blog.csdnimg.cn/9c8430c919584bb8873d375a9126150a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



那下面这个关于对列里面移出一个节点，我们可以采用自旋索，

![在这里插入图片描述](https://img-blog.csdnimg.cn/60185ab4eb534bfcbec99585e9a89d0f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



那还有下面这个里面，搜索一个等待的，我们也可以采用互斥锁，

也就是说这个如果对全局的对多线程的支持，我们可以加锁，可以加这两种锁，第一种是互斥锁，还有就是自旋锁。

现在再回忆一下，
这个协程对多线程的支持它是很复杂吗？它不是，但是难度有难度有。
为了使这个代码如果你是为了尽快的它以上线为主，以拿到实用为主，其实用多进程的做法，他不会去影响协程本身，但如果采用多线程的话，那我们需要对携程的代码本身进行一定的修改，

在回忆一下协程如何对多线程支持，对多线程支持，就是对协程的调度器进行加锁，它里面分为几个状态，每个状态的集合进行加锁，在调度resume这个过程中间，不会使得它出现一些副作用，

那大家可能会在想这里面会不会出现死锁的问题，会不会出现一些死锁问题，请大家注意。如果对于这锁本身，因为大家可以看到
内部分配，我们只是对这一个全局的这一块，它是一个全局的这一块它是全局的，因为这个协程的调度器是多个线程共用的一个，那这关于调度器它是一个共同那大家各位你仔细去回忆一下，其实如果你不参照业务代码进来的话，
线程死锁的概率是很低的，因为它是多个线程共用一个东西

协程的接口，如果你再去使用现成的时候，或者你自己封装这些的时候，携程应该定义哪些接口就可以？
第一个coroutine_create协程的创建都要有，好创建这个要有，
第二个scheduler_run(),就跟那个event_loop一样；
yield 和resume这个没必要提出来，lua提供出来了
**很不一样的答案很不一样的答案，就是accept()**

有朋友能够回到这个点上面，我认为是值得给他一些鲜花的。

就是关于网络io既然我们是对io进行操作，请大家注意协程所有io的操作不能用操作系统，而需要用我们自己再去封装，因为我们每一次对于IO操作，比如我们调用reed()，或者我们调用一个write()的时候，如果我们调用系统的read()的话，那这个 fd，**那这个协程，这个fd怎么能够到达协程的调度器里面，所以这里面我们需要在操作系统的read，write，这个系统的调用上面我们再封装这么一层来做。**

也就是说在这个 read和write这些做法的时候是这么做的，就是我们自己实现一个read或者write这个函数里面是什么?
就是这样判断的。
先判断这个io是否就绪，我们可以非常粗暴的一种方式，
就是只要此函数,读之前我们把所有fd全部加到epoll里面去，然后再把它让出，让出完之后我们再去调用我们系统的read这么一个流程。

![在这里插入图片描述](https://img-blog.csdnimg.cn/ff9e5e5d678d4ce0af1d524e7f7ada20.png)



那这个流程请注意它就改成了一个什么样的read，nty_read改完之后就变成了这么一个效果。

先加epoll，然后再切换,回来之后read，你就想一下这个read的这个过程中间，它现在变成变成了一个异步的read，这个read已经不在是我们最初系统调用的那个read，而是已经改为了一个异步的read。

这里面就分了一个上半部和下半部，也就是大家可能有没有接触到内核的时候，有个底半部和顶半部，这里也有一部分叫做上半部下半部，上半部就在切换之前，下半步呢就是在切换回来之后，就是以yeild为界,上面的为上半部，下面的为下半部，也就是他们两个的执行顺序，执行流程不在一起，
不是一个流程执行的，是先执行一段它，让出去再执行一个其他的read，再执行另外一个协程的read，然后执行完之后再resume回来，

**协程核心的封装关键就在这地方，这就是很多朋友聊到的这个read怎么把它封装成一个异步的read，就是这么做。**

了解清楚这个之后，我们再来把我们所有对于系统IO操作的这些，我们都需要把它剔出来，
大家可以考虑一下它有没有必要做成异步的，有这么些接口，第一个就是socket，第二个就是这个 close，这里有两个，其实不止两个，以这两个为例，那我们想一下这两个有没有必要把它做成异步？有还是没有没有？

大家看到socket他会不会引起阻塞，会不会引起不正常的关系，还有就是close的函数，它会不会引起？

**可以想一下就是这些我们在操作之前不需要介入去判断这个 fd是否就绪的时候，我们是不是就可以不用去封装？**

除了这一些之外，还有fcntl，还有包括我们的setsockopt,这些好这些请大家注意，如果单独是为了这个同步的操作变成异步的话，那这些接口它是没有必要的，

就是在这个同步的封装成的异步的这个角度上面，它是没有必要的，但是也有人为了一套我们还是封装好一点，请大家注意不是为了一套，我等一下还会跟大家讲就是关于hook，

需要封装的：

![在这里插入图片描述](https://img-blog.csdnimg.cn/771dc71de2bd41828e551bb8b9f4ca43.png)


不需要封装的：

![在这里插入图片描述](https://img-blog.csdnimg.cn/c0437aa4adb846d79e2b864c59dbf75a.png)



这些接口封装就可以统一按照这么一个方式，左边的那一排左边的一排在操作之前，是会引起这个阻塞的状态，就是在没有成功的时候它是不会返回的，它是会影响我们代码，也就是我们的accept没有获取到对应的连接的话，我们的程序就挂起，connect也是连接不成功，他就在等待，send，如果这个fd没有准备就绪的话，它也是等待；

左边的这一排我们在实现的时候，采用这么一个策略，从accept之前先加入epoll里面，然后让进去，然后交由调度器的那个大的epoll统一的去调度，
触发一次，看哪些等待就绪了，然后如果就绪了，然后在执行的下一步，然后再回到这么一个流程里面，请大家注意那所有的都是这么流程，左边的这一排所有的都是这个流程，

![在这里插入图片描述](https://img-blog.csdnimg.cn/bc3cd3bff0274064a70bc4fab55cbda9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_10,color_FFFFFF,t_70,g_se,x_16)



右边的这一盘其实我们也可以封装，我们也可以封装，比如说我们创建一个socket的时候，我们默认把这个 socket的封装变成一个非阻塞的封装，一个非阻塞，或者调用close的时候，我们自动的把这个协程关掉

怎么封装我们可以有这么两种策略，
第一种呢我们可以自己定一个，比如说我们可以这个read，就是凡是用我们这个框架都走掉这么一个read，就是自己独立成一套接口，
这是第一种方法，但这种方法呢你比如说你要去跟MySQL或者跟ridis，或者说不去修改redis源码的时候，这客户端提供的一些开发包的源码，会做不了；

**第二种策略呢就是我们直接做成跟系统一模一样的接口，**

第二个做成跟系统一模一样的，跟系统都有一样的结果，
那一样的接口我们在定义的时候这里就会引起一些冲突，请大家注意这个冲突，
介绍一个东西叫做hook，先实现一个案例，把这个钩子本身的原理给大家解释一下，有这么几个接口。

dlsym()是针对于系统的，我们提供了系统的一些调度，我们dlsym，如果说我们open就是用一些第三方的库,我们就用dllopen，我们操作mysql，用这个hook来做，大家可以看一下它什么一个效果。



![在这里插入图片描述](https://img-blog.csdnimg.cn/3cddd4b4fa564c71bc8a29abb4b41faa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_16,color_FFFFFF,t_70,g_se,x_16)



好，这里一起写了123456实现6个，当然还有2个就是resv_from和sendto这2个我们不写。
然后在初始化的时候，dlsym它是有了标准固定的写法的，



![在这里插入图片描述](https://img-blog.csdnimg.cn/2a5f6b8667664f3aafedbe4943be19dd.png)



但是在实现的时候，请大家注意这个函数是我们拿到系统调用的API这里面具体实现的把它赋值给他，然后这里面我们要加上一个，如果在检测的时候，当系统调用这个函数的时候，在我们应用程序中间的这个入口，他是哪个入口我们要重新定义一下。

![在这里插入图片描述](https://img-blog.csdnimg.cn/c0595271aa054918a00d2b13060a5e97.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_17,color_FFFFFF,t_70,g_se,x_16)



稍等一下给我们等一下看了之后，等一下我再给大家解释一下，包括内存泄露的一些方法我们怎么解的，我们怎么去判断对于线上的一些系统我们怎么去做，比如我们用到内存池的时候，我们怎么处理，也可以用这个hook把它解决。
好这里写了几个，然后初始化完了之后我们就这样，我们就实现我们对MySQL的操作，
然后就是MySQL初始化，然后再执行mysql_real_connect就可以了。



![在这里插入图片描述](https://img-blog.csdnimg.cn/02b8ffefcdd548d5a901773c231efe87.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_17,color_FFFFFF,t_70,g_se,x_16)



正常的理解，这里面我们要做的是
我们运行一下不加打印的话，我们感觉看不出什么差别。
这个打印就是区别我们现在调动的哪个函数，我们打印的函数调用哪个函数我们打印的函数。好，我们再编一下，就可以看到。

这原因大概有这个流程是这样的，现在大家记住了这一个入口，我们调用了hook之后，这里我们编译的时候，我们把动态库给链接进来了，请大家注意生成的这个点 o的文件，这个可执行程序是把这个库一起在运行的时候是把它拿来的，不管是动态库也好还是静态库也好，在我们调用connect的时候也就在这个库里面，如果有调用connect，请大家注意这个初始化这个接口这个hook就把这个contact调用的时候，就采用我们系统现在默认的调用的是这一部分，**就把它给截获了，也就现在出现了我们调用的库点，so或者动态库静态库也好，调用的是我们自己的实现接口**，这个hook的作用就是这样。

也就是说我们每调用这个，就把系统的connect,或者在我们库里面哪个调用的也好，把这个connect调用成我们自己实现这个connect,静态动态都是可以的，那大家可以看到如果有这个东西存在，如果有这个东西存在的话，各位朋友们你就想一想，我们所有的系统调用跟静态库，就是我们刚刚从一开始跟大家讲，**我们在做协程的时候，这些API我们采用方法，我们是不是就可以直接由hook来做？**
比如说我们的协程库要和我们的mysql放到一起，如果我们不去修改mysql-dev之内，一点都去改，我们只要加上hook我们就可以对应的来用。

其实这个原理啊很容易的，就是把原先的我在库里面调用的那个 connect的那个函数，名字换成了connect_f,就是我们提供的这些动态接口，我们换成了connect_f,而我们现在代码要调用的就变成我们现在的这个

比如现在我们有一个代码，就是我们现在的库已经实现的，不管是动态库也好，点a也好，还是点so也好，这里面比如说我们这个库里面我们调用到connect函数或者我们调用的read函数，我们现在呢通过dlsym()这一个我们把里面的找寻出来，找到这里面的connect我们找到他之后，然后用connect_f,也就是变成一个情况，我们系统调用的时候这样把它截获了，调用现在在我们代码里面所实现的这个connect。那就这样一个原理，就是把系统里面的connect重新截获了，那有了这个原理之后，各位朋友们那有很多东西我们可以做出很多不一样的东西。

我们把它改一下，connect_f改成connect_m，再编一下我们再跑一下，会出现什么情况？
上面这个这地方就是我们现在没有改 M之前,他是走了connect，如果我们改完m之后它是没有走connect的，对不对？也就是说这里面我就找去系统有没有定义都没有了，就不走，有的话就走好。

有这个存在我有很多东西做，所以这个刚才讲协程这个东西实现阶段，比如我们的malloc调用,free时候我们可以都是可以的。

当你发现一些内存泄露的时候，**大家知道一旦发生内存泄漏，肯定想的一个问题，你肯定能够知道就是有malloc没有free**

**我们把它用hook来做,用hook，我们在每一次malloc,我们加个打印，然后挂出来的东西哪些地方有malloc没有free，我们就能够清楚那地方，这也是一个做法。**

jemalloc还有tcmalloc也是这个原理，
这种内存池提供出来的这一部分，要去使用的时候，我们压根就不用改代码，就是我们写好代码之后可能就引入一个宏定义就ok了，什么都不用改，就直接会调用我们开发的库里面，我们只要在编译的时候把这个库给列进去，然后在我们调的时候那头文件里面定一个宏，就发现我们调的malloc不再是我们系统调的，而是jemalloc，没错，**就是把nginx运行在dpdk上面也是这样做的**。包括你后面做代码移植的时候或者是跨平台都是好的可以运用的。
再讲协程的这个API封装上面。
ntyco里面是支持MySQL的

```cpp
/*



 *  Author : WangBoJing , email : 1989wangbojing@gmail.com



 * 



 *  Copyright Statement:



 *  --------------------



 *  This software is protected by Copyright and the information contained



 *  herein is confidential. The software may not be copied and the information



 *  contained herein may not be used or disclosed except with the written



 *  permission of Author. (C) 2017



 * 



 *



 



****       *****                                      *****



  ***        *                                       **    ***



  ***        *         *                            *       **



  * **       *         *                           **        **



  * **       *         *                          **          *



  *  **      *        **                          **          *



  *  **      *       ***                          **



  *   **     *    ***********    *****    *****  **                   ****



  *   **     *        **           **      **    **                 **    **



  *    **    *        **           **      *     **                 *      **



  *    **    *        **            *      *     **                **      **



  *     **   *        **            **     *     **                *        **



  *     **   *        **             *    *      **               **        **



  *      **  *        **             **   *      **               **        **



  *      **  *        **             **   *      **               **        **



  *       ** *        **              *  *       **               **        **



  *       ** *        **              ** *        **          *   **        **



  *        ***        **               * *        **          *   **        **



  *        ***        **     *         **          *         *     **      **



  *         **        **     *         **          **       *      **      **



  *         **         **   *          *            **     *        **    **



*****        *          ****           *              *****           ****



                                       *



                                      *



                                  *****



                                  ****















 *



 */



 







#include "nty_coroutine.h"







#include <arpa/inet.h>







#define MAX_CLIENT_NUM            1000000



#define TIME_SUB_MS(tv1, tv2)  ((tv1.tv_sec - tv2.tv_sec) * 1000 + (tv1.tv_usec - tv2.tv_usec) / 1000)











void server_reader(void *arg) {



    int fd = *(int *)arg;



    int ret = 0;







 



    struct pollfd fds;



    fds.fd = fd;



    fds.events = POLLIN;







    while (1) {



        



        char buf[1024] = {0};



        ret = nty_recv(fd, buf, 1024, 0);



        if (ret > 0) {



            if(fd > MAX_CLIENT_NUM) 



            printf("read from server: %.*s\n", ret, buf);







            ret = nty_send(fd, buf, strlen(buf), 0);



            if (ret == -1) {



                nty_close(fd);



                break;



            }



        } else if (ret == 0) {    



            nty_close(fd);



            break;



        }







    }



}











void server(void *arg) {







    unsigned short port = *(unsigned short *)arg;



    free(arg);







    int fd = nty_socket(AF_INET, SOCK_STREAM, 0);



    if (fd < 0) return ;







    struct sockaddr_in local, remote;



    local.sin_family = AF_INET;



    local.sin_port = htons(port);



    local.sin_addr.s_addr = INADDR_ANY;



    bind(fd, (struct sockaddr*)&local, sizeof(struct sockaddr_in));







    listen(fd, 20);



    printf("listen port : %d\n", port);







    



    struct timeval tv_begin;



    gettimeofday(&tv_begin, NULL);







    while (1) {



        socklen_t len = sizeof(struct sockaddr_in);



        int cli_fd = nty_accept(fd, (struct sockaddr*)&remote, &len);



        if (cli_fd % 1000 == 999) {







            struct timeval tv_cur;



            memcpy(&tv_cur, &tv_begin, sizeof(struct timeval));



            



            gettimeofday(&tv_begin, NULL);



            int time_used = TIME_SUB_MS(tv_begin, tv_cur);



            



            printf("client fd : %d, time_used: %d\n", cli_fd, time_used);



        }



        printf("new client comming\n");







        nty_coroutine *read_co;



        nty_coroutine_create(&read_co, server_reader, &cli_fd);







    }



    



}















int main(int argc, char *argv[]) {



    nty_coroutine *co = NULL;







    int i = 0;



    unsigned short base_port = 9096;



    for (i = 0;i < 100;i ++) {



        unsigned short *port = calloc(1, sizeof(unsigned short));



        *port = base_port + i;



        nty_coroutine_create(&co, server, port); ////////no run



    }







    nty_schedule_run(); //run







    return 0;



}
```

**总结**

好，我们接下来再给大家捋一下，就是关于协程的过程中间，我们应该要了解到那些东西，首先来说请大家确定一点协程并不复杂，
那请大家注意第一个点协程核心的是为了去解决io读取的时候没有准备就绪的时候，这个阻塞的过程中间把这种同步的读改成异步的读，**核心用一句话用以前同步的读改成异步读。**

在这个核心的同步的读怎么改成异步的读？把对应的read，write 或者receive和send，这些把它改成里面由原来的对系统操作的read改成重定义的变成一个异步的，就是刚异步的就这么一个方式改的。
好，这是核心原本上阻塞改为非阻塞，其实阻塞和非阻塞改为非阻塞的读或者写部分，它本质上面你有没有发现它还会会出现我们在读写的时候逻辑不清晰，就是还是会发现它会出现那种逻逻没有这个同步这个效果，这个逻辑可能符合人的思考，就是你感觉非阻塞读它性能会跟协程一样，它的性能跟其实是一样的，只是说在代码读的方面，在代码的可读性上面没有形成这样符合人的逻辑

对于文件
这是服务器中间一个一个文件，对于磁盘操作这里有两个过程，这里有两个过程，第一个就是对于文件读写，
就是在协程里面我们去读写文件，还有一个就是我们读完之后能够把这个文件发送到客户端，
这个请求也就是说这里面有两个fd，一个是我们socketfd一个是我们操作文件的fd,这两个在读文件的时候这个也可以用协程，在发送完之后网络io这个地方也可以用协程来理解，读完之后从文件里面读取完之后再去把send出来。

怎么改成异步读，是这么一种方法，然后这个yeild他让到哪里去了，以及怎么回来。
好第三个就是说yeild让到哪？就让到协程的调度器，然后回来在哪？通过调度器，如果判断他IO准备就绪了，然后resume回来，
那关于这个调度器如何去判断？底层核心的，还是用的是一个epoll去管理所有的io,那这里面还有就是关于协调这个切换的事情，这里面就跟大家讲到了，比如说把寄存器替换掉这种方法

第一个解决协调到底解决什么问题，这是第一个问题。
再说同步改为异步这个怎么做的，
还有就是同步比异步的性能差异，
咱们就说这个问题，还有就是在这个过程yeild和resume关于调度这个过程中间切换怎么做的，
关于协程的API

其实
一定有一个差异的点，多线程也好，你看异步的时候异步的有的时候你自己去准备一下，你要跟你讲，比如说你这一次读出来的数据三者在哪，呢三者是在下一次才能算的数据，也就是说有时候有时候你准备好了被其他线程所关闭了，
比如说你还没遇到多个线程共有一个fd的情况，啊比如说线程 A线程a读取数据，吧然后把它关闭了，可能线程b还拿这个fd加send这种情况，这是多线程经常会出现一个,典型就是一个serve加上一个线程池，
这个很多没有写服务器就会这么写，可能也跟我最初讲服务器的时候那个版本有关系，啊前面一个一半一半管理完成把fd抛给现在只有现实进行明细，明白就是一个情况，这个fd很有可能会被多个线程同时共用，
同时共用的读那就会出现一个情况，那线程a对它比如说关闭了线程b所以它会出现很多的一些错误值
这种情况下面，所以多线程去操作fd为什么要加锁，而且等等不是很推荐这个方法去做，这还是有点考虑。

![在这里插入图片描述](https://img-blog.csdnimg.cn/87dd8ff462024dbe892fe40e04345676.png)



用它进来就是用它计量，现在相比较线程，进程有什么优势？
就是轻量级轻量级啊它的优势的话，就是
不像现在那么就是对io操作简单，如果是在线程里面一个io如果准备就绪了，如果准备就绪，那我们是线程的切换，线程的计划
就是由a线程切换到b线程，但是协程遇到io的话，它的切变清，就是它切换的不会像协程那么大，这就是协程相比的线程好处。

**切换的小在哪里？**线程切换你要想一下，包括线程的栈，包括线程的上下文，它会比协程都会要多很多的，

好，这里跟大家介绍协程的快，协程的快是针对于线程对比，针对于网络io而言,它跟reactor的差不多。
它是对比线程的切换会比快，但是对比io操作而言，其实差不多，这样两个对比跟线程对比切换快并且代码易读。

原文作者：[也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605065308

# 【NO.478】tcp服务器epoll的多种实现







![在这里插入图片描述](https://img-blog.csdnimg.cn/293806a5504c4302ac756493524007d6.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)


我们在读写文件的时候，这是一款服务器，CS，这是一个服务器，这个客户端去连接服务器的时候，中间大家知道从连接的这个过程中间产生通过三次握手连接，服务器先进行监听一个端口，监听的时候是用调用listen进行监听，



TCP网络编程模型就好比这样一个模型，大家去酒店吃饭，走到那个饭店的门口门口有一个迎宾的人，有迎宾的小姐姐，然后你跟她说吃饭，她把你带进餐馆里面，然后给你介绍一个真正的服务员，后面你点菜买单，然后包括像夹菜点酒都是由这个服务员为你去服务。

这里面这个酒店出现两个人，一个是对你的迎宾的人，第二个为你服务吃饭的服务员，就这个模式。

大家可以看到客户端和服务通信也是这样一个情况，客户端就是跑到酒店去吃饭，
走到服务器这地方，服务器这里有一个listen的端口，就是迎宾的人，然后介绍一个服务员，后面客户端进行通信的时候就是介绍的这个服务员进行通信。

就是这个迎宾的人怎么让他迎宾，怎么让他开始工作？
迎宾的人上哪里去找？首先记清楚他是一个人，那这个人怎么来呢？
这里面所提到的人就是 fd，我们可以通过socket创建，创建完之后，还有比如说一个酒店特别大，比如说这个酒店特别大，它有几个门那这个迎宾的小姐姐他去那个门？他只能做一件事情，他去哪个门，所以这里我们需要绑定他在哪个门，bind()一个端口，绑定完之后，他才开始工作，真正开始工作是进入listen状态。

我想问一下这迎宾的小姐姐在listen的过程中间，他跟酒店这个管理制度有没有关系，跟这个酒店的工作流程有没有关系？没用，所以刚才讲个迎宾的小姐姐，她是一个被动的，她是一个被动的工作，就是他的工作就是listen，他不影响酒店的内部的整个工作流程，首先这点一定要记住，listen一旦进入之后，他不影响其它流程，

然后当来了一个客人之后，这时候要创建一个新的，accept才是真正接待的人，listen这个过程是指定这个迎宾的小姐姐在这个门口迎宾的工作。真的来一个客人的话，accept()介绍一个新的服务员,后面的动作点菜也好，买单也好，recv和send都是跟这个服务员之间沟通,包括close买单的时候也是。

服务器它写代码的就这么写，它就是这些接口，这个listen这个过程他就是让这个小姐姐在门口迎宾，然后剩下的就是小姐姐把这个人带进去介绍一个新的服务员

**这个过程，三次握手是服务员在迎宾的这个过程中间发生的，三次握手他是被动发生的，**

三次握手是在这个迎宾的人在这等待的过程中间完成，三次握手之后才把他介绍了这个新的服务员，
有人吃饭你们有人问到你是在这吃饭都说是的，这个过程叫三次握手，收到客户说是的，再把他带进去介绍服务员,

客户端这边首先吃饭，客户端也是一个人，这个人也通过socket创建的，创建完对应的他去哪里吃饭，他去哪里吃饭？准备一个地址，准备一个地址然后connect。

这个过程就是客户端连接服务器，这个过程可以绑定也可以不绑定，这个绑定的就相当于这个他出去吃饭，他的的每个门主他可以从大门走也可以小门，所以这个都okay，如果只允许他从正门走，那这时候就给他绑定一下

它是去连接的，你可以绑定也可以不绑定，所以我认为在客户当中出现了两个地址，一个是绑定的，一个是不绑定的，绑定和绑定它是一个可选项，它是一个可选项

客户端调用contact函数就是决定了它去连接这个服务器，每一个服务器IP地址端口都是多少，去找哪个酒店迎宾的小姐姐，这是确定的是在connect确定的，
这地方就连接完成之后再调用的是send和recv这是网络编程的整个流程。

这是网络编程核心的，以这些东西为基础就衍生出来了很多的做法，客户端和服务器就是这个流程

![在这里插入图片描述](https://img-blog.csdnimg.cn/ce8a5cbd9ea7476d9af1bb00b5fff962.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



udp的服务器该怎么做？
**udp就好比你去一个小餐馆，那个迎宾的人跟服务员他是一个人**，就是你去了一个小饭店，没有连接在这方面，这个客户端和服务器之间不涉及到任何的连接，这个应该不用去争议的时候，

我们在设计函数的时候它的功能要做到单一讲究的一个功能，就是还是单一功能单一越单一越好。可利用可复用性就越强，就是一个函数设计功能越单一，它的可复用性就会越强，一个函数它越符号它的可复用性就越低。

recvfrom函数就有点意思，它不单只接收数据，并且还把哪个客户端给谁发都带出来了，就是recvfrom函数数据是谁发的都知道。

那现在考虑一下，比如说现在有三个udp的客户端，就是三个函数，同样都是都用sendto都是发给这一个端口，都是recvfrom由他来一个接受，

就好比一个小的餐馆，现在这个餐馆只有一个迎宾的人，现在过去了多批人多活人同时走到这个门口，我就想问一下，那这个迎宾的人，**他能分清楚这三户人？他不可以，**

这个服务端我们只有1个，然后如果10个1000个2000个同时调用sendoto，
我就想问一下那这个服务呢它能区分区分不了，区分不了也就是换一个说法，udp的服务器如何做多个客户端并发？

那也就是我们在udp的这个基础上面recvfrom，可能很多人会想一个办法，就是buffer里面想办法，也就是说从客户端发到服务器的时候，这个数据上面想办法，
就是我定义一种本地的协议，就是说我发送的数据包我知道是购买a发的，购完b发的数据我能知道它是b发的，然后c发的我就知道那可能同学在想的，**就是我能不能通过这个数据包括应用协议来解决这个问题**。

就是给buffer加客户端头，但是客户端口端加客户端头是有一个巨大的前提？

我们现在讲的第一个方法就是在我们发送的数据包上面加一层协议能不能行？
好很多朋友认为可以，好，
我跟大家讲一下不可以，为什么？

其它是有很大的前提，就是需要顺序的接收，就是需要顺序接收这是可以的。
就是我们知道a发的先到先发的先到后，发的后到那这种情情况下面它是ok的，那你可以在协议里面加一协议，这是可以的，在公网的情况下，面udp是很难保证先发的先到后发的后报可能有可能先发的数据后到后发的，数据先到。

第二种方法
其实原理也很简单，请他注意那我们用udp来做，其他注意我们也可以仿造了刚刚TCP这个迎宾的这个模式迎宾的这个人的模式，那么也我们也可以仿照这个模式，
好这样一个模式做，那怎么做呢你怎么做对吧？

我们也是这样的，创建一个迎宾的人在这里等着，
也就客户他先sendto给服务器这个迎宾的人创建一个，然后迎宾的接收到一个数据包之后，再为他准备一个服务员跟TCP一样，与其实那个就是一模一样，也准备一个服务员，也就是在那个为了真的区分好每一个客户每一个群体每个群体每个客户是谁，那个火车站前面那个小饭馆也采用了酒店这种模式，一个迎宾者和一个点赞的人，一个迎宾者一个点赞的，在各方面就采用这种模式，做一连接之后，sendto先给个迎宾的人，然后服务端分配一个独有的fd给这个客户端进行服务，然后后面sendto就回来的数据，就是从这个新的fd开始发送出来

就是说新的这个 fd对它进行发送。
这就是刚才讲的用udp模拟TCP的三次过程实现udp并发

socket是什么？

很难理解很难理解这个为什么叫做插座，fd与五元组配套

![在这里插入图片描述](https://img-blog.csdnimg.cn/9ccae4f9bf3c414ea434005f7afa5d53.png)



为什么这个fd它只对应到这个五元组里面，只有这个tcp和udp？

比如说我想去抓以太网的包那怎么做，其实相当于那时候我们使用的是一个叫原生的socket

好，我讲就这一份关于fd我们既可以把它当做文件描述，主要理解也可以把它当做网络通信的一个观点，一个通信的一个句名就是我们拿着它，就相当于拿到一个客户端一样，这个fd给代表来理解说我们可以把它当文件描述，也可以当做一个客户端，

先跟大家解释一下它关于文件描述的一些事情，文件描述的一些事情那就是io的一些事情。

讲到这里之后要给大家介绍一个很老很老的东西，为什么跟大家去讲这些非常老的东西，啊大家才能够知道这个技术它迭代的这个分数它怎么迭代的一个非常非常老的东西，大家可能清楚。
就是叫sigio有可能听过，有些没听过叫sigio



![在这里插入图片描述](https://img-blog.csdnimg.cn/d8fade7951594d9d8631a3a305cb4246.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



现在一个客户端给他发送一个数据，我们调一个sendto给他发一点数据，请大家注意请大家注意，这是一个socket，我们在一个端口进行接受recvfrom。
由于当发过去的时候，这个 io他现在有数据了，他就会去触犯这个 singio信号 也就是说
这个东西就有意思，他就跟我们进程通信，就好比我们常用的一个命令叫kill-9，比如在后面加一个进程ID1234一样，
要那也就是说出现一个信息一个信息，大家看到这个kill-9什么意思？9是信号那个 ID给这个进程1234这个进程发一个-9的信号

数据先由操作系统接收的数据，然后再由操作系统去通知对应的这个进程来接触这个数据，sigio是由操作系统把这个信号抛给了对应的这个进程，然后这个进程捕获到这个 sigio的信号，捕获到这个 sigio的信号，所以它才能正常接收数据

```cpp
#include <stdio.h>



#include <sys/types.h>



#include <sys/socket.h>



#include <netinet/in.h>



 



#include <string.h>



#include <unistd.h>



#include <signal.h>



#include <fcntl.h>



 



 



int sockfd = 0;



 



void do_sigio(int sig) {



 



    struct sockaddr_in cli_addr;



    int clilen = sizeof(struct sockaddr_in);



    int clifd = 0;



#if 0



    clifd = accept(sockfd, (struct sockaddr*)&cli_addr, &clilen);



 



    char buffer[256] = {0};



    int len = read(clifd, buffer, 256);



    printf("Listen Message : %s\r\n", buffer);



 



    int slen = write(clifd, buffer, len);



#else



 



    char buffer[256] = {0};



    int len = recvfrom(sockfd, buffer, 256, 0, (struct sockaddr*)&cli_addr, (socklen_t*)&clilen);



    printf("Listen Message : %s\r\n", buffer);



 



    int slen = sendto(sockfd, buffer, len, 0, (struct sockaddr*)&cli_addr, clilen);



#endif



}



 



 



int main(int argc, char *argv[]) {



 



    sockfd = socket(AF_INET, SOCK_DGRAM, 0);



 



    struct sigaction sigio_action;



    sigio_action.sa_flags = 0;



    sigio_action.sa_handler = do_sigio;



    sigaction(SIGIO, &sigio_action, NULL);



 



 



    struct sockaddr_in serv_addr;



    memset(&serv_addr, 0, sizeof(serv_addr));



    



    serv_addr.sin_family = AF_INET;



    serv_addr.sin_port = htons(9096);



    serv_addr.sin_addr.s_addr = INADDR_ANY;



 



    fcntl(sockfd, F_SETOWN, getpid());



 



    int flags = fcntl(sockfd, F_GETFL, 0);



    flags |= O_ASYNC | O_NONBLOCK;



 



    fcntl(sockfd, F_SETFL, flags);



 



    bind(sockfd, (struct sockaddr*)&serv_addr, sizeof(serv_addr));



 



    while(1) sleep(1);



 



    close(sockfd);



 



    return 0;



 



}



 



 



 



 
```



![在这里插入图片描述](https://img-blog.csdnimg.cn/b741eedf0ed94718a7cc5b9518a5d7d5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_17,color_FFFFFF,t_70,g_se,x_16)



![在这里插入图片描述](https://img-blog.csdnimg.cn/49a86fb056bf43809f02dbac20f4a6aa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



我们接着来分析一下这个代码意味着什么？

那我们现在问一下那个sigio是谁发出来的？
也就是这个进程他能接受的信号，这里有两个点要跟大家讲，第一个点是说的信号接收的流程是
第二个要跟大家讲的就是这个 sigio它意味着什么？
第一个就是关于信号接收是怎么接受?第二个CIO意味着什么？

操作系统发的，首先他解释一下那这个操作系统，他就是当接受那个数据的时候，在网卡接受那个数据的时候，既然是网卡发出来的，就是网卡接收一些数据通知，
操作系统发出sigio消息，那既然它是网卡接收到数据就会发出sigio，那各位我问大家有问题，TCP可不可以？不行，为什么？

因为TCP这个过程中间，TCP的过程中间第一个网络io的消息会很多，
好，那各位部门你可以想象一下，如果我们用TCP来做的话，它的single的信号会比udp要多很多的会要多很多很多，
对吧？
因为大家可以看到这个sigio的时候，它没有区分，是哪个fd没有区分，

为什么tcb不行，是因为tcp处理的时候，
这个情况太多，

那这东西怎么处理？
TCP不可以，那现在问一下大家如果，**现在操作系统要把TCP不去响应tcp的话**，那各位们你应该在哪个地方实现？
你看或者用哪一种形式来屏蔽到来，忽略到这个，那 TCP这过程我们应该在哪个地方去忽略掉这个sigio？

在协议栈这个地方TCP接收到一针数据的时候，不去处理sigio

sigio 与poll select是怎么联系的？

关于信号我跟大家解释三个点，
第一个点对这个进程而言这些信号如何保存，也就是我们注册的就是在我们这个代码上面，就是关于sigio的这个信号这个回调函数它怎么保存的？

就是大家可以看到这个进程，在我这里执行的时候，它是一个进程，在这个进程里面它关于这个回调函数它怎么保存的，
**第一点就是进程内部如何保存
第二个就是当我们调用signal这个函数的时候，signal这个函数的时候如何把这个信号保存到进程里？
第三个就是当我们在什么时候发，比如我们kill-9的时候这个信号如何发送？**

这三点从这三点各方面来理解一下，所以各方面如果他有被问到后面他面试的时候有被问到信号是怎么工作，也是这三个方面回答的。

就相当于这么一个进程，中间有一块空间，
他专门用了存信号，这个信号他也可以思考一下，但现在也可以思考一下这个信号的集合，我们总共一起有那些信号？比如刚刚那个kill-9，sigint,sigterm,sigio这些信号总共一起信号大概是有这么多有这么多，我找一下还有这么多，31种信号，那这31种信号我们怎么去存？

第二个这是一个进程，然后我们在应用程序的时候，我们调用signal的这个函数的时候，如何把这个信号保存到进程里面，还有一个就是我们如何通过其他的进程，比如说kill-9或者操作系统sigio如何触发到这个里面，

第一个就是说进程集合怎么存的，这一块怎么存的。
第二个是说信号signal的函数，如何把这信号保存到结构里面。
第三个就是kill-9，以及这个信号怎么发送到进程，进程怎么捕获这东西的。

首先来看一下这个进程里面保存它，
首先第一个想到的是task_struct

![在这里插入图片描述](https://img-blog.csdnimg.cn/9facb7f2131140c1b5aa17304eeb2b68.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



就是存到这个地方就这个 sighand_struct



![在这里插入图片描述](https://img-blog.csdnimg.cn/02e34b859eec4e37bf174ffb3b7e1ada.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



像这个sighand_struct从这里面大家看到这里有一个k_sigaction
大家看到这里有个k_signaction，我们点进去之后，
可以看到里面对应的是有一个signaction，

![在这里插入图片描述](https://img-blog.csdnimg.cn/f5d805b439ef4330bb975d5237e3ee1c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



这跟用户空间的是一样的，这里面action这里面它就以这个集合就是对应的这个 action的集合，

![在这里插入图片描述](https://img-blog.csdnimg.cn/aa5287c7e2684fe1a5fd5ba591ca3fe0.png)



这有个_NSIGl对应的就是这个数组,这个数组呢大小是64，也就是在每一个进程里面在每一个进程里面有一个空间sigaction的一个结构体，一个对应的数组叫action，这里面这个数组大小有64，它用来去存储所有的信号集合，

这个数字64存储信号怎么存的呢？

比如说有的时候如果一下触发大于64的信号怎么办？
比如说我一下触发128个，他是不是就溢出来了呢？
关于这signal他是怎么存的，信号只有31个，

再谈一个函数，
可以看到这是一个系统调用，这是内核里面内核里面这个系统调用SYSCALL_DEFINE2

![在这里插入图片描述](https://img-blog.csdnimg.cn/0b518c880f4f43a4b4ebfaa3b5417c01.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



这个代码里面我们调用的这个signal，它是一个系统调用，它是一个系统调用,

![在这里插入图片描述](https://img-blog.csdnimg.cn/f4538307954b4e0285318b8b00665c46.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_13,color_FFFFFF,t_70,g_se,x_16)



它调用的这个函数是哪个呢？

系统调用他调到内核里面的哪个函数，就是当我们在调用signal的函数的时候，他走到了内核里面这个地方

![在这里插入图片描述](https://img-blog.csdnimg.cn/cd8bfa8f16f745e682074a22498f7233.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_13,color_FFFFFF,t_70,g_se,x_16)



从这里我们往下走就可以了，走到这里之后大家可以看到这个sighand的设置,

![在这里插入图片描述](https://img-blog.csdnimg.cn/7d9a54905caa474a8312b9abe54acf11.png)



这一行什么意思的，大家看看这个p就是进程,进程对应的这个就是我们sighand，然后对应的action然后sig-1，可以记住也就是刚刚这个数组64，

就是这个信号每一个信号只有一个，也就是说大家在对应这个信号的值，也就是如果我们一个sigio信号，他就被存储到这个数组里面的第sigio位，

sigio的值是多少呢？
sigio是29，也就这个信号，
它存储到28从零开始存储到28的位置，也是把之前的就进行覆盖，没错，把前面的就进行覆盖，没有触发的最后一次设置的把它覆盖，

这就是我们在调用sin的这个函数，
基本这个函数就是对应的把这个函数把我们对应的x设置那个数组里面这个内容。

kill这个函数在调用的时候带两个参数，第一个对应是PID就是进程ID。
第二个对应就是发送了什么信号

给哪个进程发什么信号，PID就是进程的ID，就这个 Id大于0的话，拿到对应的进程，然后往下再send_signal，就生产在发送的时候1层1层剥到这方面了。
直接给大家看最后一个东西怎么就是所谓的激活

![在这里插入图片描述](https://img-blog.csdnimg.cn/5780bc9452b540d8bc4c80a647e12a3a.png)



就是对应的大家可以看到这里有个signalfd_notify，大家可能看到这个中断其实对应来说就是一个信号等待的意思，

epoll,poll
他为什么要这么做？

![在这里插入图片描述](https://img-blog.csdnimg.cn/e48f83170bf144b8a0aead50c30e9ade.png)



就是不同的信号通过不同的位置那个安全的那个下标的位置一个信号，它是个整形的数字对应的数组的不同的位置。

就还是以服务器为例子，还是以一个酒店为例子？
就是酒店没调好，酒店刚刚跟大家讲了一个饭，一个饭馆一个一个很大的酒店吃饭没错没错，这是一个迎宾的小姐姐来了一个客人之后
来了一个客人，然后这个客人给他介绍了一个新的服务员，好请来注意就是每一个客户端对应一个服务员，来一桌客户就有一个服务员，
随着这个酒店规模越来越大，所以这个酒店一天可能是几十桌几百桌都有，好，那也就是对于这些服务员，**对这些服务员我怎么进行管理**，那各位朋友们可能在想一个问题，这还有什么管理嘛？有事情就向上面反馈，没错，就是有什么事情对外
反馈的这个过程
比如说还要点个菜，你去下餐，你是要下到后面厨房里面，那也就说一个客户端他点菜的时候还有他下到厨房里面去，在这个厨房在处理的时候，他知道是哪个服务员吗？
**io多路复用就这意思，为了管理多个服务员来处理的。**

这里每一个 fd有这么几种方式可以处理，第一种就是派一个人随着这个酒店可能有一两百个服务员的话，专门派一个人，这个酒店服务员根本就不动，站在他们面前，专门找一个人挨一个问，你好了没有你好了没有？你有没有要点菜的？然后再派个人往后面走，然后拿到对应的把他由厨房来派？

![在这里插入图片描述](https://img-blog.csdnimg.cn/b4d45f97e6dc46049f5a64e46c0272eb.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



你看这种好太麻烦，
你派一个人专门去做，而且你们这个中间还会出现一些现象，比如说我们为了管理，管理这个酒店所有的服务员，一桌一个服务员，酒店为了管理这个服务员，首先想到第一个方法，一个非常非常老套的方法啊就是，找一个人专门去问专门去咨询，那咨询哪些内容，只咨询三个状态，只咨询三个功能，
第一个状态要不要点菜？
第二个要不要点酒？
第三个就是要不要结账？
只问这三个状态，
就是每一桌一个服务员去服务一桌的时候，就是这3个轮训，感觉相当于正跑堂的，就相当于大堂经理就是这边问他那要不要？
注意这三个状态，**结合了刚刚这个人做的事情，找一个人专门去问这些服务员，那这个专门这个人是什么？他就是select。**

那大家想一下这个select它有几个参数？
第一个参数就是酒店有多少服务员，酒店的服务员的数量有多少，他要问多少次，问的次数
第二个参数，哪一些桌
可能刚开始带着一种目的看到了一个菜吃的差不多了，然后可能就说要要不要结账，可能看到有些菜刚刚点完，就它主动去问要不要加酒，刚坐下来可能就主动去问他有没有点菜，
这里面有三个集合主动的问一些桌里面需要点餐，首先进行保证这是查询的态度。
第三个就是点酒的集合，第四个结账的集合，第五个参数多长时间一次

![在这里插入图片描述](https://img-blog.csdnimg.cn/6ad172d25d514b8d9c331f2339feaa70.png)



就是这个专门去问的这个人就是相当于大堂的一个经理，
每一桌都跑过去，然后得出这么一个结果
这是他的工作是这样的，整个一起
有个前提，就是你可以想象到一个酒店里面，比如说一个酒店里面有几百桌好几百座，然后每一个桌前面站在一个服务员，
然后这个大堂的经理也好，还是助理也好，还是秘书也好，他就专门为了去传达信号去问站在桌子旁边的那个服务员，
你问一下这个你问一下这桌有没有，你问一下这个要不要不点菜要不点酒要不结账，这么一个工作

然后这5个参数就是这个点菜的集合，我们现在不知道他要不点菜，select返回的时候，他会带着哪些桌要点菜，刚开始是我们主观的认为他点菜，但是后面可能有一些要点菜，有些桌不要点菜，好返回的时候才是真正的结果

结合我们的socket理解，它也是这样，也是5个参数，
第一个我们有多少个io？第二个就是可读的集合，我们认为他需要的读，第三个就是可写的集合，第四个是出错的集合，第五个就是多长时间轮询一次。

这个应该叫中间有一个点要跟大家解释的，就是这个服务员的数量以及这个 io的数量怎么判断，这是很难理解，就是io的数量

这里是巧合的地方，
最大fd+1那为什么会采用最大的fd+1？两个原因这种方式，第一个因为我们的fd是int型的，
int型的就出现这个情况，其实这个 fd+1或者不加都可以，你就是不加1，加1是为了我们把它放大一点，那个就采用一种方式是采用这个 max fd其实ok，

比如说这个酒店有200个服务员
正在工作，有200个服务员，今天正在工作的有150位，好，正在工作的有100个人，
对于这个助理而言，我们压根就不用去管另外50位多少请假了，

我们在这地方查询的时候，我们就直接以200为计算，因为这里面还有一部分就是那些桌点菜了，我们不知道，那其实在这块我们做的一件事情就是最大的fd进行加一，最大的fd当做数量就可以，那我们就可以等同为
这个 max fd的这个值，它最大的值就相当于这个 fd总共一起有多少个数量

这个酒店工作了一段时间之后呢，发现这个专门做事的人要做一个事情，就是需要带三个小本本，第一个点菜的集合，第二点点酒的集合，第三个结账的集合，

这3个小本本记录有点麻烦，我们就进行1个小本本，就把这三个分别记在一个小本上面，大家发现也是跑也是跑，**但只是记录在一个小本本上面，这就是poll带三个参数**
一个是pfd,第二个参数是它有多大length,第三个参数是说的timeout，

可以想象一下一个酒店里面几百多的人，然后每一桌旁边带一个小姐姐，就是服务员为他倒酒倒点菜，然后在大堂里面就专门安排一个人挨桌问，他每次开始工作的这个工作的人他就是select，
他一开始工作就带三个小本本，然后里面哪些需要点酒，哪些需要点菜，然后问一次
然后在里面那些画勾，然后再带回来，隔一段时间在继续，他就取决于timeout多长时间跑，
**select翻译到io里面就是哪一些可读，哪一些可写，哪一些出错了，然后多长时间培训一次，每一次需要把这个集合进行更新，每一次把数据带进去，**

select五个参数,还有就是poll这东西每次记三个小本本有点麻烦，然后后面改成一个把这三个集合记到一个本本里面，记在一个本里面，那对应就变成了poll模式。
同样poll他还是需要定时的去跑一次，多长时间跑一次，多长时间跑一次，但是只是由三个本变成一个本，由三个数组变成一个数，
其他的没有改变，其他的没有改变，他还是多长时间轮训一次，多长时间去整个大堂里跑圈 ，他不是一直在轮训，它是根据时间定时的，

这个定时谁定的这个时间，我想问一下这个秘书这个专门跑这个人，**这个时间期间他在干什么？**

他是在哪个地方休息，在这个中间时间空档的时候这个专门跑堂这个人他在哪个地方休息，他休息在哪里？

请大家注意这select的timeout它的时间是在哪里休息，请大家注意它阻塞在哪里，也就是说这个时间比如说我们3秒钟,也就说这io三秒钟之内，他阻塞是阻塞在select函数的内部，三秒钟没有我就返回，如果有的话就低于三秒，最长等待三秒，没有的话我就最长等待三秒，有的话我就提前返回没有，然后poll也是这样

剩下一个主角，一个非常重要的东西，就是epoll，epoll是怎么样的，epoll就比较高级

每一桌上面放一个按钮，现在有些酒店点菜，你按那个按钮就可以，就变成这种模式，服务员还在那里，但是每个服务员手里捧着一个小按钮要点菜了然后按一下，要点酒了按一下，要结账了按一下。这个人只有遇到按钮的时候，他就不需要每一桌去问，而是直接按按钮的那一桌，我们才开始把这信息收回来，就这样。

![在这里插入图片描述](https://img-blog.csdnimg.cn/194fa65b083e4beba8e3f5637c48b640.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_19,color_FFFFFF,t_70,g_se,x_16)



这epoll是这样的，比如一个小区有n多的住户，现在在考虑这个快递怎么记的问题，怎么记录，就在楼下做一个东西就是蜂巢。

![在这里插入图片描述](https://img-blog.csdnimg.cn/ae360552fbac40eb911e1a8d181ea0c7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_10,color_FFFFFF,t_70,g_se,x_16)



那这个快递员他去工作的时候什么样？
送快递的时候就把那个快递放在蜂巢里面，收的时候也是要从这个里面拿，这样呢他就跟那个住户进行了一个隔离，
隔离那大家可以节省了很多时间，以前快递员去跑整个小区的情况，都不需要挨家去敲，直接从这个蜂巢里面拿取，

这里有前提什么？
中间可能有一部分住户，不允许快递员拿？
首先第一个快递员，我们通过epoll_create()创建一个快递员，
第二个比如说这个住户里面新搬了一个，我们通过epoll_ctl(ADD/DEL/MOD)增加一个或者说搬走一个，或者说有一个人从5楼搬到4楼，从4楼搬到3楼房进行mod修改。
第三个函数是epoll_wait()多长时间轮询一次，epoll总归一起由这三个函数所组成。

epoll_create()你就可以理解，聘请一个快递员，然后为小区收快递这个集体里面增加一个减一个，然后修改一个，然后epoll_wait()是多长时间跑一次，

epoll_wait()总归一起带4个参数，一个是epfd是哪个快递员，第二个就是他收快递的那个袋子events，第三个是他那个袋子有多大length，第四个是多长时间跑一次timeout。

epoll_create(1)这里面有个参数，它带这个参数只有0和1的区别，不管你写多大，不管你填多大，你就是填1万与填10万，它的效果是一样的,那么这个参数有什么用？

这个参数只有0和1的区别，就是为什么带这个参数？就是epoll最早的版本
一次性最多有多少，就是后面这个快递员收快递的时候，

它最大一次性能够收多少？最多最多，这是一个预估的值，最多最多最初是这样一个值，随着后面这个一点一点的修改，其实在前期实现的时候，用的是一些数组或者用的是一个固定的空间或者是用一个list分配的，

创建的快递员在蜂巢里面最多能收多少个，就类似，但是随着后面的版本的修改，相当于这个蜂巢的这个盒子就变成了无线大，就变成了所谓我们的一个链表来处理，
所以后面这个值就没有太大的意义，所以它只有0合1的区别，0就是失败，1就成功，大于0的数都成功就可以了，其实这个参数它没有什么意义，现在来说已经没什么意义，只要大于0就 ok。

第二个就是epoll_ctl()这个函数，它是为小区里面，我们添加io，删除io，修改io，就这个意思。

**总结**

第一个跟TCP编程的方式。
第二个udp并发udt并发
第三个就是关于socket是怎么回事，这个插座怎么理解，然后解释sigio就是从网络最古老的版本这个sigio接受数据，然后解释了这个网络sigio的是从哪里接收的数据，这个信号从哪里来的，然后解释了信号的整个数据流程，
然后讲select,epoll.

原文作者：[也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605102147

# 【NO.479】C++面试常问基础总结梳理

本文对C++在使用中、面试中经常会被问到的点做了梳理，可当作日常开发过程中的工具书，亦可当作面试准备资料，欢迎大家交流~~

## 1. 友元函数

(1) 友元概念？

友元是C++提供的一种破坏数据封装和数据隐藏的机制。

通过将一个模块声明为另一个模块的友元，一个模块能够引用到另一个模块中本是被隐藏的信息。

可以使用友元函数和友元类。

为了确保数据的完整性，及数据封装与隐藏的原则，建议尽量不使用或少使用友元。

(2) 如何使用？

友元函数是在类声明中由关键字friend修饰说明的非成员函数，在它的函数体中能够通过对象名访问 private 和 protected成员。

(3) 作用

增加灵活性，使程序员可以在封装和快速性方面做合理选择。

访问对象中的成员必须通过对象名。

 

##  2. const用法

常类型的对象必须进行初始化，而且不能被更新。

常引用：被引用的对象不能被更新。

const  类型说明符  &引用名

常对象：必须进行初始化,不能被更新。

类名  const  对象名

常数组：数组元素不能被更新。

类型说明符  const  数组名[大小]...

常指针：指向常量的指针。

 

## 3. 调用规则

构造函数和析构函数的构造规则：

① 派生类可以不定义构造函数的情况 

当具有下述情况之一时，派生类可以不定义构造函数：

基类没有定义任何构造函数。

基类具有缺省参数的构造函数。

基类具有无参构造函数。

② 派生类必须定义构造函数的情况 

当基类或成员对象所属类只含有带参数的构造函数时，即使派生类本身没有数据成员要初始化，它也必须定义构造函数，并以构造函数初始化列表的方式向基类和成员对象的构造函数传递参数，以实现基类子对象和成员对象的初始化。 

③ 派生类的构造函数只负责直接基类的初始化 

 

C++语言标准有一条规则：***\*如果派生类的基类同时也是另外一个类的派生类，则每个派生类只负责它的直接基类的构造函数调用\****。

这条规则表明当派生类的直接基类只有带参数的构造函数，但没有默认构造函数时（包括缺省参数和无参构造函数），它必须在构造函数的初始化列表中调用其直接基类的构造函数，并向基类的构造函数传递参数，以实现派生类对象中的基类子对象的初始化。

这条规则有一个例外情况，当派生类存在虚基类时，所有虚基类都由最后的派生类负责初始化。

 

总结：

（1）当有多个基类时，将***\*按照\****它们在继承方式中的***\*声明次序调用\****，与它们在构造函数初始化列表中的次序无关。当基类A本身又是另一个类B的派生类时，则先调用基类B的构造函数，再调用基类A的构造函数。

（2）当有多个对象成员时，将按它们在派生类中的***\*声明次序调用\****，与它们在构造函数初始化列表中的次序无关。

（3）当构造函数初始化列表中的基类和对象成员的构造函数调用完成之后，才执行派生类构造函数体中的程序代码。

 

## 4. 构造函数与析构函数

(1) 类对象成员的构造

先构造成员、再构造自身（调用构造函数）

(2) 派生类构造函数

派生类可能有多个基类，也可能包括多个成员对象，在创建派生类对象时，派生类的构造函数除了要负责本类成员的初始化外，还要调用基类和成员对象的构造函数，并向它们传递参数，以完成基类子对象和成员对象的建立和初始化。

 

***\*派生类只能采用构造函数初始化列表的方式向基类或成员对象的构造函数传递参数\****，形式如下：

\```c++
派生类构造函数名(参数表):基类构造函数名(参数表),成员对象名1(参数表),…{

 //……

}
\```

(3) 构造函数和析构函数调用次序

派生类对象的构造: 先构造基类、再构造成员、最后构造自身（调用构造函数）。 

基类构造顺序由派生层次决定：***\*最远的基类最先构造\****。

成员构造顺序和定义顺序符合。

析构函数的析构顺序与构造相反。

 

## 5. 基类与派生类关系

基类对象与派生类对象之间存在赋值相容性。包括以下几种情况：

(1) 把派生类对象赋值给基类对象；

(2) 把派生类对象的地址赋值给基类指针；

(3) 用派生类对象初始化基类对象的引用。

反之则不行，即***\*不能把基类对象赋值给派生类对象\****；不能把基类对象的地址赋值给派生类对象的指针；也不能把基类对象作为派生对象的引用。

 

## 6. 继承访问权限

(1) 公有继承

①　基类中protected的成员

类内部：可以访问

类的使用者：不能访问

类的派生类成员：可以访问

②　派生类不可访问基类的private成员

③　派生类可访问基类的protected成员

④　派生类可访问基类的public成员

 

(2) 私有继承

派生类不可访问基类的任何成员与函数。

 

(3) 保护继承

派生方式为protected的继承称为保护继承，在这种继承方式下，基类的public成员在派生类中会变成protected成员，基类的protected和private成员在派生类中保持原来的访问权限。

注意点：当采用保护继承的时候，由于public成员变为protected成员，因此类的使用者不可访问！而派生类可访问！通俗点，即：基类中protected的成员：

类内部：可以访问；

类的使用者：不能访问；

类的派生类成员：可以访问。

 

(4) 派生类对基类成员的访问形式

①　通过派生类对象直接访问基类成员 

②　在派生类成员函数中直接访问基类成员 

③　通过基类名字限定访问被重载的基类成员名  

 

##  7. 虚拟继承

多继承下的二义性：在多继承方式下，派生类继承了多个基类的成员，当两个不同基类拥有同名成员时，容易产生名字冲突问题。

 

虚拟继承引入的原因：重复基类，派生类间接继承同一基类使得间接基类（Person）在派生类中有多份拷贝，引发二义性。

 

(1) 虚拟继承virtual inheritance的定义

语法

\```c++
class derived_class : virtual […] base_class
\```

虚基类virtual base class

被虚拟继承的基类

在其所有的派生类中，仅出现一次

 

(2) 虚拟继承的构造次序

虚基类的初始化与一般的多重继承的初始化在语法上是一样的，但构造函数的调用顺序不同；若基类由虚基类派生而来，则派生类必须提供对间接基类的构造（即在构造函数初始列表中构造虚基类，无论此虚基类是直接还是间接基类）

 调用顺序的规定：

先调用虚基类的构造函数，再调用非虚基类的构造函数。若同一层次中包含多个虚基类,这些虚基类的构造函数按它们的说明的次序调用；若虚基类由非基类派生而来,则仍然先调用基类构造函数,再调用派生类构造函数。

 

(3) 虚基类由最终派生类初始化 

在没有虚拟继承的情况下，每个派生类的构造函数只负责其直接基类的初始化。但在虚拟继承方式下，虚基类则由最终派生类的构造函数负责初始化。

在虚拟继承方式下，若最终派生类的构造函数没有明确调用虚基类的构造函数，编译器就会尝试调用虚基类不需要参数的构造函数（包括缺省、无参和缺省参数的构造函数），如果没找到就会产生编译错误。

 

## 8. 虚函数、纯虚函数和抽象类

(1) 多态性

多态性：多态就是在同一个类或继承体系结构的基类与派生类中，用同名函数来实现各种不同的功能。

***\*静态绑定又称静态联编\****，是指在编译程序时就根据调用函数提供的信息，把它所对应的具体函数确定下来，即在编译时就把调用函数名与具体函数绑定在一起。 

***\*动态绑定又称动态联编\****，是指在编译程序时还不能确定函数调用所对应的具体函数，只有在程序运行过程中才能够确定函数调用所对应的具体函数，即在程序运行时才把调用函数名与具体函数绑定在一起。

编译时多态性： ---静态联编(连接)----系统在编译时就决定如何实现某一动作,即对某一消息如何处理.静态联编具有执行速度快的优点.在C++中的编译时多态性是通过函数重载和运算符重载实现的。

运行时多态性： ---动态联编(连接)----系统在运行时动态实现某一动作,即对某一消息在运行过程实现其如何响应.动态联编为系统提供了灵活和高度问题抽象的优点,在C++中的运行时多态性是通过继承和虚函数实现的。

 

(2) 虚函数

虚函数的意义

①　基类与派生类的赋值相容

派生类对象可以赋值给基类对象。

派生类对象的地址可以赋值给指向基类对象的指针。

派生类对象可以作为基类对象的引用。

赋值相容的问题：

不论哪种赋值方式，都只能通过基类对象（或基类对象的指针或引用）访问到派生类对象从基类中继承到的成员， 不能借此访问派生类定义的成员。

②　虚函数使得可以通过基类对象的指针或引用访问派生类定义的成员。

③　Virtual关键字其实质是告知编译系统，被指定为virtual的函数采用动态联编的形式编译。

④　虚函数的虚特征：基类指针指向派生类的对象时，通过该指针访问其虚函数将调用派生类的版本。

 

\- 一旦将某个成员函数声明为虚函数后，它在继承体系中就永远为虚函数了 

\- 如果基类定义了虚函数，当通过基类指针或引用调用派生类对象时，将访问到它们实际所指对象中的虚函数版本。

\- 只有通过基类对象的指针和引用访问派生类对象的虚函数时，才能体现虚函数的特性。

\- 派生类中的虚函数要保持其虚特征，必须与基类虚函数的函数原型完全相同，否则就是普通的重载函数，与基类的虚函数无关。

\- 派生类通过从基类继承的成员函数调用虚函数时，将访问到派生类中的版本。

\- 只有类的非静态成员函数才能被定义为虚函数，类的构造函数和静态成员函数不能定义为虚函数。原因是虚函数在继承层次结构中才能够发生作用，而构造函数、静态成员是不能够被继承的。

\- 内联函数也不能是虚函数。因为内联函数采用的是静态联编的方式，而虚函数是在程序运行时才与具体函数动态绑定的，采用的是动态联编的方式，即使虚函数在类体内被定义，C++编译器也将它视为非内联函数。

⑤　5.基类析构函数几乎总是为虚析构函数。

why?

假定使用delete和一个指向派生类的基类指针来销毁派生类对象，如果基类析构函数不为虚,就如一个普通成员函数，delete函数调用的就是基类析构函数。在通过基类对象的引用或指针调用派生类对象时，将致使对象析构不彻底！

 

(3) 纯虚函数和抽象类 

①　纯虚函数概念？

仅定义函数原型而不定义其实现的虚函数。

Why pure function?

实用角度：占位手段place-holder

方法学：接口定义手段，抽象表达手段

How?

\```c++
class X
{
   virtual ret_type func_name (param) = 0;
}
\```

②　抽象类概念？

What is an abstract class?

包含一个或多个纯虚函数的类

Using abstract class

***\*不能实例化抽象类\****

但是可以定义抽象类的指针和引用

Converting abstract class to concrete class

定义一个抽象类的派生类

定义所有纯虚函数

③　C++对抽象类具有以下限定

\- 抽象类中含有纯虚函数，由于纯虚函数没有实现代码，所以不能建立抽象类的对象。

\- 抽象类只能作为其他类的基类，可以通过抽象类对象的指针或引用访问到它的派生类对象，实现运行时的多态性。

\- 如果派生类只是简单地继承了抽象类的纯虚函数，而没有重新定义基类的纯虚函数，则派生类也是一个抽象类。

 

##  9. 运算符重载

运算符重载是C++的一项强大功能。通过重载，可以扩展C++运算符的功能，使它们能够操作用户自定义的数据类型，增加程序代码的直观性和可读性。

本章主要介绍 类成员运算符重载与友元运算符重载,  二元运算符与一元运算符重载,  运算符++、--、[]、（）重载,  this指针与运算符重载及 流运算符<<和>>的重载。

(1) 重载二元运算符

①　二元运算符的调用形式与解析

aa@bb  可解释成  aa.operator@(bb)、或解释成 operator@(aa,bb)

如果两者都有定义,就按照重载解析。

\```c++
class X{
public:
   void operator+(int);
   X(int);
};
void operator+(X,X);
void operator+(X,double);
\```

②　类运算符重载形式

a. 非静态成员运算符重载

以类成员形式重载的运算符参数比实际参数少一个，第1个参数是以this指针隐式传递的。 

\```c++
class Complex{
     double real,image;
public:
     Complex operator+(Complex b){……}
......
};
\```

b. 友元运算符重载

如果将运算符函数作为类的友元重载，它需要的参数个数就与运算符实际需要的参数个数相同。比如，若用友元函数重载Complex类的加法运算符，则形式如下：

\```c++
class Complex{
……
   friend Complex operator+(Complex a,Complex b);     //声明
//......
};
Complex  operator+(Complex a,Complex b){……}       //定义
\```

(2) 重载一元运算符 

①　一元运算符

一元运算符只需要一个运算参数，如取地址运算符（&）、负数（?）、自增加（++）等。

②　一元运算符常见调用形式为：

@a  或  a@      //隐式调用形式

a.operator@()        // 显式调用一元运算符@

其中的@代表一元运算符，a代表操作数。

@a代表前缀一元运算，如“++a”；

a@表示后缀运算，如“a++”。

③　@a将被C++解释为下面的形式之一

a.operator@()、operator@(a) 

④　一元运算符作为类成员函数重载时不需要参数，其形式如下：

\```c++
class X{
……
     T operator@(){……};
}
\```

T是运算符@的返回类型。从形式上看，作为类成员函数重载的一元运算符没有参数，但实际上它包含了一个隐含参数，即调用对象的this指针。

⑤　前自增(减)与后自增(减)

C++编译器可以通过在运算符函数参数表中是否插入关键字int 来区分这两种方式

//前缀

operator -- ();

operator -- (X & x);

//后缀

operator -- (int);

operator -- (X & x, int);

 

(3) 重载赋值运算符=

赋值运算符“=”的重载特殊性：

赋值运算进行时将调用此运算符；

只能用成员函数重载；

如果需要而没有定义时，编译器自动生成，该版本进行bit-by-bit拷贝。

 

(4) 重载赋值运算符[]

①　[ ]是一个二元运算符，其重载形式如下：

\```c++
class X{
……
     X& operator[](int n);
};
\```

②　重载[]需要注意的问题

\- []是一个二元运算符，其第1个参数是通过对象的this指针传递的，第2个参数代表数组的下标。

\- 由于[]既可以出现在赋值符“=”的左边，也可以出现在赋值符“=”的右边，所以重载运算符[]时常返回引用。

\- []只能被重载为类的非静态成员函数，不能被重载为友元和普通函数。

 

(5) 重载( ) 

①　运算符( )是函数调用运算符，也能被重载。且只能被重载为类的成员函数。

②　运算符( )的重载形式如下：

\```c++
class X{
……
     X& operator( )(参数表);
}；
\```

其中的参数表可以包括任意多个参数。

③　运算符( )的调用形式如下：

X Obj;       //对象定义

Obj()(参数表);    //调用形式1

Obj(参数表);       //调用形式2

 

## 10.模板

模板（template）是C++实现代码重用机制的重要工具，是泛型技术（即与数据类型无关的通用程序设计技术）的基础。

模板是C++中相对较新的语言机制，它实现了与具体数据类型无关的通用算法程序设计，能够提高软件开发的效率，是程序代码复用的强有力工具。 

 

本章主要介绍了函数模板和类模板两类，以及STL库中的几个常用模板数据类型。

(1) 模板 

①　模板概念

模板是对具有相同特性的函数或类的再抽象，模板是一种参数多态性的工具，可以为逻辑功能相同而类型不同的程序提供一种代码共享的机制。

一个模板并非一个实实在在的函数或类，仅仅是一个函数或类的描述，是参数化的函数和类。

②　模板分类

函数模板、类模板。

③　函数模板与模板函数

函数模板提供了一种通用的函数行为，该函数行为可以用多种不同的数据类型进行调用，编译器会据调用类型自动将它实例化为具体数据类型的函数代码，也就是说函数模板代表了一个函数家族。

与普通函数相比，函数模板中某些函数元素的数据类型是未确定的，这些元素的类型将在使用时被参数化；与重载函数相比，函数模板不需要程序员重复编写函数代码，它可以自动生成许多功能相同但参数和返回值类型不同的函数。

\1) 函数模板

a. 函数模板的定义

\```c++
template <class T1, class T2,…>返回类型 函数名(参数表){

 ……                  //函数模板定义体

}
\```

template是定义模板的关键字；写在一对<>中的T1，T2，…是模板参数，其中的class表示其后的参数可以是任意类型。

模板参数常称为类型参数或类属参数，在模板实例化（即调用模板函数时）时需要传递的实参是一种数据类型，如int或double之类。

函数模板的参数表中常常出现模板参数，如T1，T2

 

b. 使用函数模板的注意事项

① 在定义模板时，不允许template语句与函数模板定义之间有任何其他语句。

\```c++
template <class T>
int x;     //错误，不允许在此位置有任何语句
T min(T a,T b){…}
\```

② 函数模板可以有多个类型参数，但每个类型参数都必须用关键字class或typename限定。此外，模板参数中还可以出现确定类型参数，称为非类型参数。例：

\```c++
template <class T1,class T2,class T3,int T4>
T1 fx(T1 a, T 2 b, T3 c){…}
\```

在传递实参时，非类型参数T4只能使用常量

③ 不要把这里的class与类的声明关键字class混淆在一起，虽然它们由相同的字母组成，但含义是不同的。这里的class表示T是一个类型参数，可以是任何数据类型，如int、float、char等，或者用户定义的struct、enum或class等自定义数据类型。

④ 为了区别类与模板参数中的类型关键字class，标准C++提出?了用typename作为模板参数的类型关键字，同时也支持使用class。比如，把min定义的template <class T>写成下面的形式是完全等价的：

\```c++
template <typename T> 
T min(T a,T b){…}
\```

 

c. 函数模板的实例化

\- 实例化发生的时机

模板实例化发生在调用模板函数时。当编译器遇到程序中对函数模板的调用时，它才会根据调用语句中实参的具体类型，确定模板参数的数据类型，并用此类型替换函数模板中的模板参数，生成能够处理该类型的函数代码，即模板函数。

\- 当多次发生类型相同的参数调用时，只在第1次进行实例化。假设有下面的函数调用：

\```c++
int x=min(2,3);  
int y=min(3,9);
int z=min(8.5);
\```

编译器只在第1次调用时生成模板函数，当之后遇到相同类型的参数调用时，不再生成其他模板函数，它将调用第1次实例化生成的模板函数。

 

\- 实例化的方式：

隐式实例化：

编译器能够判断模板参数类型时，自动实例化函数模板为模板函数

\```c++
template <typename T> T max (T, T);
…
int i = max (1, 2); 
float f = max (1.0, 2.0);
char ch = max (‘a’, ‘A’);
…
\```

隐式实例化，表面上是在调用模板，实际上是调用其实例。

 

显示实例化explicit instantiation：

编译器不能判断模板参数类型或常量值，需要使用特定数据类型实例化

语法形式::

模板名称<数据类型,…,常量值,…> (参数)

\```C++
template <class T> T max (T, T);
…
int i = max (1, ‘2’); 
// error: data type can’t be deduced
int i = max<int> (1, ‘2’);
…
\```

 

d. 函数模板的特化

\- 特化的原因

但在某些情况下，模板描述的通用算法不适合特定的场合（数据类型等）

比如：如max函数

\```c++
char * cp = max (“abcd”, “1234”);
\```

实例化为：char * max (char * a, char * b){return a > b ? a : b;}

这肯定是有问题的，因为字符串的比较为：

\```c++
char * max (char * a, char * b)
{   return strcmp(a, b)>0 ? a : b;  }
\```

\- 特化

所谓特化，就是针对模板不能处理的特殊数据类型，编写与模板同名的特殊函数专门处理这些数据类型。

模板特化的定义形式：

template <> 返回类型 函数名<特化的数据类型>(参数表) {

​     ……                

}

说明：

① template < >是模板特化的关键字，< >中不需要任何内容；

② 函数名后的< >中是需要特化处理的数据类型。

 

e. 说明

① 当程序中同时存在模板和它的特化时，特化将被优先调用；

② 在同一个程序中，除了函数模板和它的特化外，还可以有同名的普通函数。其区别在于C++会对普通函数的调用实参进行隐式的类型转换，但不会对模板函数及特化函数的参数进行任何形式的类型转换。

 

f. 调用顺序

当同一程序中具有模板与普通函数时，其匹配顺序如下：

完全匹配的非模板函数

完全匹配的模板函数

类型相容的非模板函数

 

\2) 类模板

a. 类模板的概念

类模板可用来设计结构和成员函数完全相同，但所处理的数据类型不同的通用类。

如栈，存在：

双精度栈：

\```c++
class doubleStack{
 private:
 double data[size];
 ……
};
\```

字符栈：

\```c++
class charStack{
 private:
 char data[size];
 ……
};
\```

这些栈除了数据类型之外，操作完全相同，就可用类模板实现。

 

b. 类模板的声明

template<class T1,class T2,…>

class 类名{

​    ……                 // 类成员的声明与定义

}

其中T1、T2是类型参数

类模板中可以有多个模板参数，包括类型参数和非类型参数

 

c. 非类型参数

非类型参数是指某种具体的数据类型，在调用模板时只能为其提供用相应类型的常数值。非类型参数是受限制的，通常可以是整型、枚举型、对象或函数的引用，以及对象、函数或类成员的指针，但不允许用浮点型（或双精度型）、类对象或void作为非类型参数。

 

在下面的模板参数表中，T1、T2是类型参数，T3是非类型参数。

template<class T1,class T2,int T3>

在实例化时，必须为T1、T2提供一种数据类型，为T3指定一个整常数（如10），该模板才能被正确地实例化。

 

d. 类模板的成员函数的定义

方法1：在类模板外定义，语法：

template <模板参数列表> 
返回值类型 类模板名<模板参数名表>::成员函数名 (参数列表)
{
   ……
};

方法2：成员函数定义，与常规成员函数的定义类似，另外

“模板参数列表”引入的“类型标识符”作为数据类型使用

“模板参数列表”引入的“普通数据类型常量”作为常量使用

 

e. 类模板特化

特化，即用与该模板相同的名字为某种数据类型专门重写一个模板类。 

类模板有两种特化方式：一种是特化整个类模板，另一种是特化个别成员函数 

特化成员函数的方法：

template <> 返回类型 类模板名<特化的数据类型>::特化成员函数名(参数表){

 ……                  //函数定义体

}

\```c++
template<>  void Array<char *>::Sort(){
 for(int i=0;i<Size-1;i++){
  int p=i;
  for(int j=i+1;j<Size;j++)
   if(strcmp(a[p],a[j])<0)
    p=j;

  char* t=a[p];
  a[p]=a[i];
  a[i]=t;
 }
}
\```

 

##  11. 异常处理

(1) catch捕获异常时，不会进行数据类型的默认转换。

(2) 限制异常的方法

\- 当一个函数声明中不带任何异常描述时，它可以抛出任何异常。例如：

\```c++
int f(int,char);     //函数f可以抛出任何异常
\```

\- 在函数声明的后面添加一个throw参数表，在其中指定函数可以抛出的异常类型。例如：

\```c
int g(int,char)  throw(int,char);  //只允许抛出int和char异常。
\```

\- 指定throw限制表为不包括任何类型的空表，不允许函数抛出任何异常。如：

\```c++
int h(int,char) throw();//不允许抛出任何异常
\```

(3) 捕获所有异常

在多数情况下，catch都只用于捕获某种特定类型的异常，但它也具有捕获全部异常的能力。其形式如下：

\```c++
catch(…) {
 ……       //异常处理代码
}
\```

(4) 再次抛出异常

如是catch块无法处理捕获的异常，它可以将该异常再次抛出，使异常能够在恰当的地方被处理。再次抛出的异常不会再被同一个catch块所捕获，它将被传递给外部的catch块处理。要在catch块中再次抛出同一异常，只需在该catch块中添加不带任何参数的throw语句即可。

(5) 异常的嵌套调用

try块可以嵌套，即一个try块中可以包括另一个try块，这种嵌套可能形成一个异常处理的调用链。

 原文作者：[当当响](https://blog.csdn.net/zhpcsdn921011)

原文链接：https://bbs.csdn.net/topics/606578746

# 【NO.480】Nginx数据结构

就是nginx源码里面该怎么去看里面有哪些东西？

核心的第一点就是把基础组建这一块，就是把我们在nginx源码里面的一些数据结构，得需要捋一遍。

数据结构这里面包含哪些东西，，就现在凭你自己现在自己，你认为做一款开源的项目，这里面有哪些组件是我们有必要要了解的，

第一个大家可以想象，首先对于字符串的处理字符的处理，这个肯定是有的。
第二个对于内存的处理我们也是需要有的，
我们对于文件的操作也是需要有的，就是对于这索引hash它是需要有的，还有类似于红黑树也是需要有，还有呢比如共享内存也是要有的，链表也是要有的，消息队列也是要有的。
还有呢比如说进程间通信的组件也是要有

可以看到在我们任何1个项目里面，就是在任何1个项目里面，你会发现这些组件它是可以通通拿过来又可以去复用的，
应用到其他的项目里面也是可以的，因为其他项目里面这些组件也会有，你经历过几个项目之后，如果你自己的技术有点追求，这里面有很多标准，但是很多时候你换其他地方读也是ok，因为这做法不同做法就会有些差异，
就是这些小组件，然后对应到我们的项目中间，它就是整个一个一个的轮子

nginx所有的数据结构划分出来，有这么几个部分所组成，



![在这里插入图片描述](https://img-blog.csdnimg.cn/860557320f104605ad5173c370db9d7c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



第一个部分是说的基础的数据结构，这里面呢大家可以看到什么叫基础，就对于数据类型这种叫基础，比如int的定义，

然后还有高级点的，比如队列数组红黑树哈希
一些进程间通信的组件，包括共享内存，原子操作，自旋锁然后包括通道，信号，
但是在面试的时候它不仅仅局限于要你会用，而很多的时候会问到里面它是怎么实现的，其他的方法是怎么实现的，
这叫分为4类，还有最后一类就是基础组件式的，内存池，线程池，slab

sizeof(int)这个地方它就会有点小差异，这个差异在哪？

就很容易比如说我们在都是以32位这种心态去写的，这时候他数据不会丢失，请注意这时候这个long型占了8个字节，这8个字节中间它就会有一些空档的区域，有一些空隙的空间，从而造成一些数据我们是没有意义的，造成一些占的空间会浪费，这个时候出于这么一个问题，
所以在这种情况里面有了这么一个定义intptr，这个东西是为了去兼容这个32位和64位这个int

![在这里插入图片描述](https://img-blog.csdnimg.cn/dc8de6d261b94eacb322a7b2fc783e8c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



第二个概念就是个string，
这个东西啊我想问一下大家有没有对字符号进行处理过，有没有对字符号进行操作过没有？在做字符串操作的时候，你认为做这个最不爽的一点是？

比如说第一个问题是越界，越界的问题就是因为它不带长度，所以越界，结尾标识符的它也是由于我们不带长度所引起的，还有内存分配，就这两点是非常不爽的问题，
一个就是对内存分配，比如说我们在数据中间进行一些操作的时候，各位你要发现我们在操作之前，我们可能对它先得需要分配一块内存。

第一个对于越界的问题我们可以加长度，这是第一个。
第二个对于内存分配的时候，我们可以这样做，但是这是有一个前提的，你有这个客观条件才可以操作，你每次拿数据的时候，对于字符串操作的时候，
首先我们引入一个内存池，也是我们分配这种数据直接在内存池里面，

就是在满足这两个条件的基础上面，第一个内存先分配，就是这个内存我们已经分配好了，我们不用担心内存越界问题。
第二个至于它使用多长，我们是有一个长度的一个计算的，满足这两点我们可以使用柔性数组，也叫做零长数组，什么意思？我们就可以这么定义，

![在这里插入图片描述](https://img-blog.csdnimg.cn/cc27fb587bd44be49cccefba92cbf499.png)



加了一个零长数组，每一次分配的时候这个 a,我们只是打个标签
这个结构体是4个字节，
4个字节就是我们定义的这个结构体，然后再紧接着这个 a我们这个标识指向的是这个地方，那紧接着它下面开始就可以存储我们字符。

![在这里插入图片描述](https://img-blog.csdnimg.cn/60793923812f40cdb987eb58d9d03aaa.png)



那这个字符有多长呢我们通过len来控制，那是不是发现这个做法是挺好的？
但是它中间它有一个缺陷，我们这个计算是可以的，但是请注意不利于我们的printf（），
适合我们的数据把它存储起来，不能把它打印出来，这个东西在使用的时候它有一个局限性，这是一个版本。
第二个版本，

![在这里插入图片描述](https://img-blog.csdnimg.cn/1de7d920ee824acb8ac1d3ef1d207a08.png)



这个我们用的是一个字符，用的是一个指针指向的位置，
多占了一个指针的长度，这个指针不利于我们去修改这个数据，但我们可以指向另外的空间，
就什么意思？
我们是可以不连续我们也是ok的，我们是可以把这个data任意的指向任意一个地方，就是说我们在使用的时候，我们可以指向下一块内存不连到一起，这个也是ok的，我们可以指向任意的地方，这两者之间的差异

所以这时候，**这个问题为什么nginx这地方不用柔性数组？**

![在这里插入图片描述](https://img-blog.csdnimg.cn/a91b9d9f4b9a45b4b109c6ee1effc5c0.png)



可以看到，比如说看到这个宏，

![在这里插入图片描述](https://img-blog.csdnimg.cn/13984f9ef45141ddb1afe3e7a439144d.png)



这个东西这个做法上面可能在一开始看的时候，不是那么能理解，就举个例子，比如说我们一个函数

![在这里插入图片描述](https://img-blog.csdnimg.cn/9dfd7f22ec104fd0950f843961fcf047.png)



这个操作上面可不可以？
这个当他一返回的时候他是没有用的，在这里跟大家讲了一个前提，首先这里面要么直接传一个常量进去，要么就是直接在内存池中间去分配的一个string，在内存池中间去分配的，也就在堆上分配，然后一起去释放，那两种情况才是ok的，**所以对于一些常用的数据结构，是有自己的一些场景。**

首先第一个点就是这个地方为什么既然它是在内存中间的，**为什么这东西它不能用柔性数组，**之前说过柔性数组用在它的内存是先于分配的，第二的话呢长度是我们可以通过另外的渠道拿到的，它使用时候，它的常量改变就不那么方便。

那接着再给大家讲一个比较复杂的东西叫list。

![在这里插入图片描述](https://img-blog.csdnimg.cn/51c92a11c1064d58bf8df0bd8cf41781.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_11,color_FFFFFF,t_70,g_se,x_16)



有这么一个场景，GPS上传，有一针一传，一针一传，一针一次，就会发现这时候很容易耗电，所以就出现一个现象，在终端的时候在设备上的时候就把数据给存起来，可能到5分钟的时候再发一次，或者10分钟的时候再发一次，这个数据先在本地先存n帧，然后存完之后5分钟再全部发一次，中间多一些少一些无所谓，

那我们再来分析一下，就是把n个GPS的数据我们要传到服务端，
这边比如说我们这个 n等于20，一次传20帧，那在这传的时候相当于是一个数组，相当于有20个数据，**问一下在服务端我们去接收这20个数据的时候，我们用什么数据结构比较好？**

就是一个经度一个维度，再加上一个再加上一个n乘以它，这个接收完了之后只要解析这个n我们就知道它有多少帧
那现在这个数组我们用什么数组？
这 N它是一个变化的值，首先这个数组我们是不能够直接去定一个固定的值

我大方我直接定一个1024的数字，然后要他一直存是不是？可以，这个当然没问题，如果他有一次超过1024，我就问一下你这个数字怎么搞？

第一个数组长度固定，因为这个n它是不固定的，
这个数组不合适，

还有一个前提，我们如果有了内存池的前提，对于这个问题我们该怎么做？
这里面它前提是有一个内存池的情况下面，vector这个东西可能我们就不太利于在我们已有的这个内存池的情况再去往里面去分配，也就是这个 vector里面定义是我们很难把它分配到内存池里面。

我们每一帧每一帧数据，这个要给大家引入一个东西，我们可以这样子在一开始的时候我们定一个链表，这个链表有点特殊

跟大家讲的这个场景，我们就比较的适合ngx_list_t。



![在这里插入图片描述](https://img-blog.csdnimg.cn/a14d4777a23240cda012e70488c68822.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)

![在这里插入图片描述](https://img-blog.csdnimg.cn/2a79e2b0d1a6447799bd2f27f1a0ee5e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_16,color_FFFFFF,t_70,g_se,x_16)



第一个就说的这个链表本身
就是它，在紧接着是每个链表里面的节点，请注意这个链表和链表的节点不是一个东西，

它总归一起有两个东西组成，一个就是链表本身，第二个就是每个链表的节点是什么。
还有一个可选的东西，就是中间的内存这节点*elts，指向的内存块是什么地方，

nelts这个值是用来做什么的？
这项就是用来描述这个分配的这个 n里面，这中间比如说这个123456块里面中间有多少个已经被使用。
比如说这里面已经使用了4个，那这个n那就等于4，这就是关于这个结构体的组成。
这个传输的过程这是网络做的，也就是当我们去读出来的这个数据，我们存到哪里，我们如何去把更好的存储

接下再问到一个问题，既然这个里面使用了一个池使用了一个内存池，作用就是防止这些数据会有碎片的出现，那想问他这个问题，那这里面碎片多少，这里还是个碎片。这个池它的作用在哪里？既然它这么多的碎片。

在这里只是图，这个图是这么画的，核心来说还是一个整块，这么多小块其实就是一整块的内存。

![在这里插入图片描述](https://img-blog.csdnimg.cn/8a286d96e7274411b8048f5cde79e099.png)



这还只是一个轮廓，他要提供多少个方法，我们如何去把它用起来，
提供方法，首先第一个就是刚刚的那个结构体的定义，然后还有一些我们需要实现一些函数来操作这些
我们到底要不要改，以及我们到底要不要删，或者我们到底要不要查，或者说我们该怎么去
增加

核心来说就提供了一个方法
我们先来看一下这里面到底需不需要增删改查

首先第一个
我们增加一个节点，或者我们有没有删除一个节点，或者说我们有没有去修改的情况，删除比较麻烦好
第一个增加肯定是要有的，不然怎么添这个节点。
第二个对于这个数据结构，我们有没有必要去删除？

这个删除我们有没有意义，我们可以用整个内存池一起去回收。
它的使用场景就是，每一个连接在连接的时候，我们建立一个内存池，在连接释放的时候，整个内存是销毁，那这个删除有没有必要？
第三个有没有可能修改，有没有可能修改？
以及修改我们该怎么去修改，那还有一个就是查找有没有要查找的可能性

它是不带索引的功能，也就是在这个茫茫的这种n多的这种数据存储进去之后，我们的查找方法只能通过遍历，而且这种查找方法效率极低，它的查找是没有意义的
**只要一个增加就可以了**

对于这个链表操作的方法只有一个就是ngx_list_push,

![在这里插入图片描述](https://img-blog.csdnimg.cn/c5ca5754f3c84e3197b843f1bc558dd3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



关于这个push才真的能够领略它设计的整个优势，

```cpp
//gcc -o ngx_list_main ngx_list_main.c -I ./nginx-1.14.2/src/core/ -I ./nginx-1.14.2/objs/  -I ./nginx-1.14.2/src/os/unix/ -I ./pcre-8.41/ -I ./nginx-1.14.2/src/event/ ./nginx-1.14.2/objs/src/core/ngx_list.o ./nginx-1.14.2/objs/src/core/ngx_string.o ./nginx-1.14.2/objs/src/core/ngx_palloc.o ./nginx-1.14.2/objs/src/os/unix/ngx_alloc.o



 



 



#include <stdio.h>



#include <string.h>



#include "ngx_config.h"



#include "ngx_core.h"



#include "ngx_list.h"



#include "ngx_palloc.h"



#include "ngx_string.h"



 



#define N    10



 



volatile ngx_cycle_t *ngx_cycle;



 



void ngx_log_error_core(ngx_uint_t level, ngx_log_t *log,



            ngx_err_t err, const char *fmt, ...)



{



}



 



void print_list(ngx_list_t *l) {



    ngx_list_part_t *p = &(l->part);



    while (p) {



        int i = 0;



        for (i = 0;i < p->nelts;i ++) {



            printf("%s\n", (char*)(((ngx_str_t*)p->elts + i)->data));



        }



        p = p->next;



        printf(" -------------------------- \n");



    }



}



 



int main() {



 



    ngx_pool_t *pool = ngx_create_pool(1024, NULL);



 



    ngx_list_t *l = ngx_list_create(pool, N, sizeof(ngx_str_t));



 



    int i = 0;



    for (i = 0;i < 24;i ++) {



        ngx_str_t *ptr = ngx_list_push(l);//返回是精髓，返回待写的



        



        char *buf = ngx_palloc(pool, 32);



        sprintf(buf, "King %d", i+1);



        ptr->len = strlen(buf);



        ptr->data = buf;



    }



    print_list(l);



}
```

这个 table的东西呢它就4项，它是一个非常非常简单的东西

![在这里插入图片描述](https://img-blog.csdnimg.cn/7c608de5bd274d9b94ba51c5454a1a8d.png)



可以看到第一项哈希,第二项key,第三项value，第四项lowcase_key就是小写的key，这个什么意思？

这个数据结构呢就是单纯的就是为了这个而生的,就是为了去避免这种发小写的这种，比如说我们这小写的也认了，要注意这个哈希是什么意思,把我们这个 key那我们只对它做一个计算，方便我们去找到
哈希是对这个 k做了一个简单的这个哈希值，然后方便我们对于这个数组的位置，方便我们查找就这个意思。

纯内存操作，
那关于文件操作，我们如果对这个文件操作要实现一个组件的话，或者实现一个数据结构的话，我们该怎么做，
第一个我们不需要担心数据的存储，我们不需要去考虑布局在内存中的存储，因为它有内存池，不需要考虑具体数据的程度在内存中的存储，**那我们对这如果已经知道了一块内存的数据，我们如何去把它同步到磁盘中间？**

所以现在考虑一下现在大家考虑一下，我们假设已经有了这一块内存所有的数据我们需要同步到这个磁盘中间去，同步的方法就是write和read，那么现在对于这一块内存的数据，我们怎么把它同步进去，这个要考虑几个原因，考虑几个问题，
第一个一次性能不能写完?第二个就是我们一次性没有写完，下一次在哪个地方开始写这几个问题，
那我们怎么去定义对这个内对这块内存我们做一个结构体来标识，让我们更好的把这个数据同步到磁盘里面，就用一个结构体来表述把它同步进去，
就是头在哪个地方，尾在哪个地方，读到哪个地方

首先第一个它开始的位置，
首先它是有个开始的位置，还有一个结束的位置。文件在不停的增加哪个目录？。
他就是做这个事情，他就是记录我们对数据文件同步的时候用了这个ngx_buf_t，
这里只是对一个文件进行同步，如果我们要对多个文件进行同步，那对多个文件进行统一的操作那怎么做？这个链ngx_chain_t



![在这里插入图片描述](https://img-blog.csdnimg.cn/23291a3927fc4a92b05e7d9be4686f59.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/f04b21fec4e0415195b5db9d4f201e9e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



为什么这里有开始的位置，这里有个结束位置，这里为什么还要那个文件的结束位置，这个是怎么理解？
这些东西关于对于文件操作的几个东西，就是我们对于文件定位的时候，就是我们从哪个地方，这首先前面6项，
就是我们在同步的文件里面就记录了这个所有的位置，
后面它不是核心，只是为了去辅助我们数据更好做。

我们在读的时候或者我们再往里面写的时候，在里面写的时候，首先这个buff从哪个地方开始到哪个地方结束，写到文件的哪个位置，再到文件的哪个位置，
u_char这里面的数据我们可以拿出来的，

不像是我们刚刚讲的那个例子，
历史里面它是指向的是有具体的某个类型值的，但是请注意这个buff它分配完了之后，它也是在那些池里面，我们没办法知道从buff里面是哪个内存池，但是我们可以从内存池里面
去分配
list可以知道这是分配在哪一个内存池的

这一项start是从buff里面哪个地方开始到哪个地方结束，然后写到这个文件在哪个定位开始到哪个地方结束，然后写进去的时候之前这个内容是什么，到了结尾这个内容是什么？
这个返回能够返回回来，就是这么6个字**对于这个 buff操作时候我们需要引几个函数？**其实核心的是这两个读和写

所有的业务需求是基于这个基础组建做的，它也是基于这些技术组建做的，全部都是基于这些技术组件在上面做的，

进程间通信最好的一个方法可以采用共享内存。
那共享内存的做法，关于函数接口上面有这么两种做法，
一个是采用的mmap
第二个就采用shmat，这两个都是可取的，而这两个的思路在实现的时候又是很相近的，各位朋友你去打开那个进程，虚拟内存它的共享图的时候，那个内存的Top图的时候，你要发现现在中间它多一项叫做mmap项，这个当我们使用共享内存的时候，这个内存多个进程之间共享的内存地址就是说的mmap这个段



![在这里插入图片描述](https://img-blog.csdnimg.cn/55a88a9c3d414205a5d19ded9aa526ba.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_9,color_FFFFFF,t_70,g_se,x_16)



总共一起提供出来三个方法，这三个方法大家可以看到可以看到
这三个这三个中间到底是采用宏地分隔出来的，他到底是选择哪个？



![在这里插入图片描述](https://img-blog.csdnimg.cn/e2f4501e00bb49368124ca1c4ed3dd9a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



关于共享内存本身来说，第一个就是分配，第二个就是释放

 原文作者：[[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605194135

# 【NO.481】Linux服务器开发,libevent/libev框架实战那些坑

## 1.前言

libevent、libev和libuv都是c语言实现的异步事件库。注册异步事件，检测异步事件，根据事件的触发先后顺序调用相对应的函数处理事件。
处理的时间包括：**网络IO事件，定时事件以及信号事件。**这三个是驱动服务器逻辑的三个重要事件。
libevent和libev解决了跨平台的问题，封装了异步事件库与操作系统的交互。
libevent使用了大量的全局变量，很难安全得在多线程环境中运行；event的数据结构太大，包含了io、时间以及信号 处理全封装在一个结构体中，额外的组件如http、dns、openssl等实现质量差，计时器采用最小二叉堆(libuv改为最小四叉堆)不能很好的处理时间事件。



![在这里插入图片描述](https://img-blog.csdnimg.cn/c4fa6a6ac20b4fcbb337ac0cf4e35a7f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)



## 2.如何阅读网络库

记住两个线索：

### 2.1 网络封装

- IO检测
- IO操作

### 2.2 事件操作

- 连接建立的问题 （限制最大连接数，设置黑白名单，创建用户的对象）
- 连接断开
- 数据到达 （具体消息的分发，解密）
- 数据发送

不注重效率的都会使用getdateoftime()，会对我们的系统有较大的考验，因为它会读一个文件，把时间取出来。如果我们要经常获取时间，我可以运用缓存时间。
systime_mono() 机器启动到现在的时间。
systime_wall() 现在的时间。
wall+mono2-mono1为什么要这样计算时间？担心有的人会更好系统时间，导致我们服务器出现错乱。
更新时间缓存的目的是避免过多的调用系统调用，影响性能。
最小堆的顶端是最近要触发的任务。
收集网络事件，收集定时事件，将事件进行包装，根据事件优先级不同放入不同队列中，并没有急着处理。
event_asign()相当于epoll_mod，是个增强版，没有还会增加。

### 2.3 巧妙设计

linux协议栈中每一个fd在内核态中都有一个读写缓冲区，而在内核态也同样有一个读写缓冲区，为什么用户态需要实现一个对写缓冲区？
能不能确保一次int n=read(fd,buf,size)读出内核态所有数据？答案肯定是不能，因为数据包的界定是有固定长度和特殊字符两种，size参数的填写是盲猜的。有了用户态缓冲区就更容易读出完整的数据。
而写的时候int n=write(fd,buf,size)，size是打算写的值，而n表示实际写入的值，如果n<size说明数据没有全部写出去，剩下的数据要放到缓冲区。我们要缓存数据，然后注册写事件。
常用的解决方案：

- fix buffer char buf[16*1024*1024] char buf[16*1024*1024] 容纳的最大值，还要进行错误判断，数据移动频繁，存在空间浪费。

- ringbuf 多线程、网卡，逻辑上连续但是物理上上是数组不连续。

- chainbuf 最大的缺点是内存不连续，会带来多次IO

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/1ffdbb87f6a54eff9386f1a9cce20ff9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16)

用不同的回调函数对不同的事件进行解耦。

## 3.总结

今天通过Mark老师细心的讲解，我又被深深的上了一课，脱离libevent这个框架之外的习惯我觉得更值得我去学习。Mark老师反复的强调，写代码之前要思路清晰，只有思路清晰代码写起来才能更加流畅。将事件进行归类，读起代码来也会变得得心应手。虽然小生目前依然才疏学浅，相信在不远的将来一定会成为栋梁之才。

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/605018548

# 【NO.482】tcp支持浏览器websocket协议

一个io它是怎么一种情况，一个客户端连接一个服务器，一个客户端一个连接，大家时刻在做服务器，都是时刻抓住这样一个点，
就是说一个客户端在服务端会有一个网络io，一个客户端在服务端会有一个网络io，之前用epoll来管理这些io我们写了一个版本写了一个版本是怎么做的。

![在这里插入图片描述](https://img-blog.csdnimg.cn/b360ccda8974401fb2b5b2f606c3d905.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



网络io到epoll的实现,epoll在实现的中间它有哪些问题以及如何去封装，然后再一层一层的跟大家实现的这个反应堆模式，就是以大量的网络io，然后每一个网络io它对应的事件会有对应的回调函数，
网络io对应的事件有对应的回调函数，

**那这个网络io对应的事件是说的哪些事件？**
就是所说的epollin/可读,epollout可写，
对应的回调函数怎么理解呢，就是epollin的时候我们调用recv_cb
然后epollout我们调send_cb，
当然还有与之对应的listen,我们可以调用accept_cb，封装完了之后，它的封装性更强，
它的封装性更强好吧，这是跟大家讲的reactor就这么一个模式，

在reactor的基础上面我们看看协议怎么做，基于websocket,就是因为协议很简单，它大体上的东西,核心的元素都会有

一个客户端对应一个连接，一个连接是怎么管理的呢就是通过epoll管理，一个连接我们对应的数据存储在哪呢？

服务器所生成的一个fd我们放到sockitem

![在这里插入图片描述](https://img-blog.csdnimg.cn/f146f1c9fac74abeb820d85d0cc25c83.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



如果客户端给服务器发数据，epoll检测到io可读，就会调用sockitem里面对应回调函数。

![在这里插入图片描述](https://img-blog.csdnimg.cn/dd2fb07dcbab4f5abfd1d8b3589e7712.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_9,color_FFFFFF,t_70,g_se,x_16)


如果是收到数据那么有两种情况：一个io对应可读或者可写
读的数据发送的数据都放到buffer里面





![在这里插入图片描述](https://img-blog.csdnimg.cn/53849ead2c1e496fbbee348b8a90d5fd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_14,color_FFFFFF,t_70,g_se,x_16)


接收

![在这里插入图片描述](https://img-blog.csdnimg.cn/fd8edb1cdcc64d53be9f49b0f40692f5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_12,color_FFFFFF,t_70,g_se,x_16)


发送

![在这里插入图片描述](https://img-blog.csdnimg.cn/c508497d888d41ceb4b2eb51c51f9ffe.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_12,color_FFFFFF,t_70,g_se,x_16)



那么分析这个代码的时候逻辑就会有很大的改变了

![在这里插入图片描述](https://img-blog.csdnimg.cn/ae3dab0845b24be3a00fc0da4c86ed7c.png)



recv_cb里面我们接收到的数据我们该怎么去发送，比如回声服务器接收数据就返回什么数据的话我们该怎么做？

我们不需要去关注在什么时候调用send，我们只要关注一点这个sendbuffer里面有没有数据就ok了，只要关注这样一点就ok了，它会自动发送，

![在这里插入图片描述](https://img-blog.csdnimg.cn/c706a1d3b2b9401aa0d3cc543f95d935.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_13,color_FFFFFF,t_70,g_se,x_16)





![在这里插入图片描述](https://img-blog.csdnimg.cn/f603b030d4574959a594519a6a1e032b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



发送是怎么发送的呢？
两步就ok
我们现在根本就不用关注他的发送，只要把send_cb这一步做好了

![在这里插入图片描述](https://img-blog.csdnimg.cn/fb5c90ab6f35443a9c96ef9d0fefbc91.png)


libevent源码也是这样设计的有一个buffer，把数据放到里面就ok了，



我们只是做到了接收与发送，要是有协议呢？
**接下来我们如何把websocket协议加进去？**



![在这里插入图片描述](https://img-blog.csdnimg.cn/6d2ebe902d414d3d978c9e72e370ea57.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)



首先第一步要考虑的是websocket的握手，

关于websocket握手是怎么一回事
**websocket使用在哪？**
为什么会有一个握手？

主要它是用来浏览器跟服务器做一个长链接，什么意思？我们打开CSDN有个登录，我们看到这边我们点击登录，现在这时候出来一个微信二维码，我们现在通过微信扫描二维码登录

那么这个功能与我们的websocket有什么关系？
前端页面，二维码是前端的，现在我们通过微信的那个微信的客户端，我们扫码扫一下这个二维码，扫完之后把对应的二维码，传到微信的服务器，然后微信的服务器，进行回调到csdn的服务器，然后前端扫码完之后为什么会有一个跳转？
就是这步，在csdn的服务器会主动推送一个数据，csdn的服务器，主动发数据给网页前端。
那在这一步是服务器主动发的，主动发的过程中间，就是采用了websocket协议

服务器主动发数据给浏览器的时候，可以选择websocket，但是websocket不是唯一的解决方案，
1.网页聊天，即时通讯
2.网页弹幕
3.股票
单独使用tcp没有那么好做，websocket是基于tcp，

**了解websocket的使用场景后思考一下websocket是怎么建立的？**

weisocket协议和客户端之间是怎么一种反思？

![在这里插入图片描述](https://img-blog.csdnimg.cn/9c4b529653ac4f2596b64481b0ae19b9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_15,color_FFFFFF,t_70,g_se,x_16)



这个连接请大家注意，这个连接是在TCP建立后连接的基础上面，客户端和服务器已经有一个连接了，然后客户端给服务器发送一段应用数据，这个数据叫做握手数据，

相当于是这样一个客户端和服务器之间建立好的连接，现在这个客户端发送一段数据
首先发的第一步数据验证双方是否合法，这个数据叫做握手数据，



![在这里插入图片描述](https://img-blog.csdnimg.cn/63250e89d8934365b6f257327f0479df.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


与http协议如此相似，
客户端先握手成功之后在服务器发送消息
**握手过程**

![在这里插入图片描述](https://img-blog.csdnimg.cn/308e2a38843f4888b74d252ad7cccca9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_13,color_FFFFFF,t_70,g_se,x_16)



websocket协议由两部分组成一部分握手，另一部分通信。

**就是在recv_cb里面我们怎么接收？我们怎么去区分他是握手数据还是通信数据？**
需要引入一个状态机，

![在这里插入图片描述](https://img-blog.csdnimg.cn/69ea33c0379e497691070392ece473a0.png)



那么这个状态机的状态我们保存在哪里？每一个连接里面应该都有一个状态机

![在这里插入图片描述](https://img-blog.csdnimg.cn/35852a8788e0483ba1f77decc234c4a0.png)





![在这里插入图片描述](https://img-blog.csdnimg.cn/2b0e2650f2034e8aa0d5b559b038d93f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_11,color_FFFFFF,t_70,g_se,x_16)



![在这里插入图片描述](https://img-blog.csdnimg.cn/22062475ad114c908f5f8e781e85cc90.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)



![在这里插入图片描述](https://img-blog.csdnimg.cn/c3213ae1069e45edaadb5e8a28ccd178.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_18,color_FFFFFF,t_70,g_se,x_16)


区分accept_cb和recv_cb

![在这里插入图片描述](https://img-blog.csdnimg.cn/9a673125a23045d79e2bd01e51c9de92.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_14,color_FFFFFF,t_70,g_se,x_16)


握手的状态怎么进入通信的状态？



刚刚讲了状态机，横向思考一下，其实HTTP协议也需要有一个状态机
在http协议接收的时候它也有握手，同样它有它的header还有它的body
header和body里面都有自己对应的资源，就是方便理解为什么nginx里面有一个状态机的实现？

实现recv_cb的时候我们不能通过具体的数据去判断它是不是头。
第一个状态处理完了之后去处理下一个状态，状态机就是这样。

websocket通信的时候它的协议头是什么样的？

![在这里插入图片描述](https://img-blog.csdnimg.cn/53e16a3c6d334d4fa91e2046de10c71e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5oiR5Lmf6KaB5b2T5piP5ZCb,size_20,color_FFFFFF,t_70,g_se,x_16)


如果以后自己基于TCP做协议的时候，可以看到有三个核心的点
1.操作码，fin是不是终止的包是不是数据包是不是握手包
2.包长度，分包粘包怎么解：可以选择包长度或者分隔符，这里websocket选择的就是包长度，
3.不想传输明文可以加一个mask-->key 主要与payload做一个可逆的计算得出data
4.payload data数据是纯应用层的数据，就可以采用json/xml，





![在这里插入图片描述](https://img-blog.csdnimg.cn/ea19d68a9d0e4a71855ee44eff6455f3.png)


长连接是客户端和服务器维持的一个连接，通过心跳包去维持，短连接就是一次请求不用管了，发送短信，长连接计算完需要回数据



tcp的keepalive有这么几个特点，不要去代替应用层的心跳包
1.一旦超时之后tcp会自动的去回收keepalive
2.超时之后应用层得不到可控制的反馈，没办法去判断他超时我们该做什么策略性的东西

```cpp
#include <stdio.h>



#include <stdlib.h>



#include <string.h>



 



#include <unistd.h>



#include <netinet/tcp.h>



#include <arpa/inet.h>



#include <pthread.h>



 



#include <fcntl.h>



#include <errno.h>



#include <sys/epoll.h>



 



#include <openssl/sha.h>



#include <openssl/pem.h>



#include <openssl/bio.h>



#include <openssl/evp.h>



 



#define BUFFER_LENGTH            1024



#define GUID                     "258EAFA5-E914-47DA-95CA-C5AB0DC85B11"



 



 



enum  WEBSOCKET_STATUS {



    WS_HANDSHARK,



    WS_DATATRANSFORM,



    WS_DATAEND,



};



 



 



struct sockitem { //



    int sockfd;



    int (*callback)(int fd, int events, void *arg);



 



    char recvbuffer[BUFFER_LENGTH]; //



    char sendbuffer[BUFFER_LENGTH];



 



    int rlength;



    int slength;



 



    int status;



 



};



 



// mainloop / eventloop --> epoll -->  



struct reactor {



 



    int epfd;



    struct epoll_event events[512];



    



    



 



};



 



 



struct reactor *eventloop = NULL;



 



int recv_cb(int fd, int events, void *arg);



int send_cb(int fd, int events, void *arg);



 



#if 1  // websocket



 



char* decode_packet(char *stream, char *mask, int length, int *ret);



int encode_packet(char *buffer,char *mask, char *stream, int length);



 



struct _nty_ophdr {



 



    unsigned char opcode:4,



         rsv3:1,



         rsv2:1,



         rsv1:1,



         fin:1;



    unsigned char payload_length:7,



        mask:1;



 



} __attribute__ ((packed));



 



struct _nty_websocket_head_126 {



    unsigned short payload_length;



    char mask_key[4];



    unsigned char data[8];



} __attribute__ ((packed));



 



struct _nty_websocket_head_127 {



 



    unsigned long long payload_length;



    char mask_key[4];



 



    unsigned char data[8];



    



} __attribute__ ((packed));



 



typedef struct _nty_websocket_head_127 nty_websocket_head_127;



typedef struct _nty_websocket_head_126 nty_websocket_head_126;



typedef struct _nty_ophdr nty_ophdr;



 



 



int base64_encode(char *in_str, int in_len, char *out_str) {    



    BIO *b64, *bio;    



    BUF_MEM *bptr = NULL;    



    size_t size = 0;    



 



    if (in_str == NULL || out_str == NULL)        



        return -1;    



 



    b64 = BIO_new(BIO_f_base64());    



    bio = BIO_new(BIO_s_mem());    



    bio = BIO_push(b64, bio);



    



    BIO_write(bio, in_str, in_len);    



    BIO_flush(bio);    



 



    BIO_get_mem_ptr(bio, &bptr);    



    memcpy(out_str, bptr->data, bptr->length);    



    out_str[bptr->length-1] = '\0';    



    size = bptr->length;    



 



    BIO_free_all(bio);    



    return size;



}



 



 



int readline(char* allbuf,int level,char* linebuf) {    



    int len = strlen(allbuf);    



 



    for (;level < len; ++level)    {        



        if(allbuf[level]=='\r' && allbuf[level+1]=='\n')            



            return level+2;        



        else            



            *(linebuf++) = allbuf[level];    



    }    



 



    return -1;



}



 



int handshark(struct sockitem *si, struct reactor *mainloop) {



 



    char linebuf[256];



    char sec_accept[32]; 



    int level = 0;



    unsigned char sha1_data[SHA_DIGEST_LENGTH+1] = {0};



    char head[BUFFER_LENGTH] = {0};  



 



    do {        



        memset(linebuf, 0, sizeof(linebuf));        



        level = readline(si->recvbuffer, level, linebuf); 



 



        if (strstr(linebuf,"Sec-WebSocket-Key") != NULL)        {   



            



            strcat(linebuf, GUID);    



            



            SHA1((unsigned char*)&linebuf+19,strlen(linebuf+19),(unsigned char*)&sha1_data);  



            



            base64_encode(sha1_data,strlen(sha1_data),sec_accept);           



            sprintf(head, "HTTP/1.1 101 Switching Protocols\r\n" \



                "Upgrade: websocket\r\n" \



                "Connection: Upgrade\r\n" \



                "Sec-WebSocket-Accept: %s\r\n" \



                "\r\n", sec_accept);            



 



            printf("response\n");            



            printf("%s\n\n\n", head);            



#if 0



            if (write(cli_fd, head, strlen(head)) < 0)     //write ---> send            



                perror("write");            



#else



            memset(si->recvbuffer, 0, BUFFER_LENGTH);



 



            memcpy(si->sendbuffer, head, strlen(head)); // to send 



            si->slength = strlen(head);



 



            // to set epollout events



            struct epoll_event ev;



            ev.events = EPOLLOUT | EPOLLET;



            //ev.data.fd = clientfd;



            si->sockfd = si->sockfd;



            si->callback = send_cb;



            si->status = WS_DATATRANSFORM;



            ev.data.ptr = si;



 



            epoll_ctl(mainloop->epfd, EPOLL_CTL_MOD, si->sockfd, &ev);



 



#endif



            break;        



        }    



 



    } while((si->recvbuffer[level] != '\r' || si->recvbuffer[level+1] != '\n') && level != -1);    



 



    return 0;



}



 



int transform(struct sockitem *si, struct reactor *mainloop) {



 



    int ret = 0;



    char mask[4] = {0};



    char *data = decode_packet(si->recvbuffer, mask, si->rlength, &ret);



 



 



    printf("data : %s , length : %d\n", data, ret);



 



    ret = encode_packet(si->sendbuffer, mask, data, ret);



    si->slength = ret;



 



    memset(si->recvbuffer, 0, BUFFER_LENGTH);



 



    struct epoll_event ev;



    ev.events = EPOLLOUT | EPOLLET;



    //ev.data.fd = clientfd;



    si->sockfd = si->sockfd;



    si->callback = send_cb;



    si->status = WS_DATATRANSFORM;



    ev.data.ptr = si;



 



    epoll_ctl(mainloop->epfd, EPOLL_CTL_MOD, si->sockfd, &ev);



 



    return 0;



}



 



void umask(char *data,int len,char *mask) {    



    int i;    



    for (i = 0;i < len;i ++)        



        *(data+i) ^= *(mask+(i%4));



}



 



char* decode_packet(char *stream, char *mask, int length, int *ret) {



 



    nty_ophdr *hdr =  (nty_ophdr*)stream;



    unsigned char *data = stream + sizeof(nty_ophdr);



    int size = 0;



    int start = 0;



    //char mask[4] = {0};



    int i = 0;



 



    //if (hdr->fin == 1) return NULL;



 



    if ((hdr->mask & 0x7F) == 126) {



 



        nty_websocket_head_126 *hdr126 = (nty_websocket_head_126*)data;



        size = hdr126->payload_length;



        



        for (i = 0;i < 4;i ++) {



            mask[i] = hdr126->mask_key[i];



        }



        



        start = 8;



        



    } else if ((hdr->mask & 0x7F) == 127) {



 



        nty_websocket_head_127 *hdr127 = (nty_websocket_head_127*)data;



        size = hdr127->payload_length;



        



        for (i = 0;i < 4;i ++) {



            mask[i] = hdr127->mask_key[i];



        }



        



        start = 14;



 



    } else {



        size = hdr->payload_length;



 



        memcpy(mask, data, 4);



        start = 6;



    }



 



    *ret = size;



    umask(stream+start, size, mask);



 



    return stream + start;



    



}



 



 



int encode_packet(char *buffer,char *mask, char *stream, int length) {



 



    nty_ophdr head = {0};



    head.fin = 1;



    head.opcode = 1;



    int size = 0;



 



    if (length < 126) {



        head.payload_length = length;



        memcpy(buffer, &head, sizeof(nty_ophdr));



        size = 2;



    } else if (length < 0xffff) {



        nty_websocket_head_126 hdr = {0};



        hdr.payload_length = length;



        memcpy(hdr.mask_key, mask, 4);



 



        memcpy(buffer, &head, sizeof(nty_ophdr));



        memcpy(buffer+sizeof(nty_ophdr), &hdr, sizeof(nty_websocket_head_126));



        size = sizeof(nty_websocket_head_126);



        



    } else {



        



        nty_websocket_head_127 hdr = {0};



        hdr.payload_length = length;



        memcpy(hdr.mask_key, mask, 4);



        



        memcpy(buffer, &head, sizeof(nty_ophdr));



        memcpy(buffer+sizeof(nty_ophdr), &hdr, sizeof(nty_websocket_head_127));



 



        size = sizeof(nty_websocket_head_127);



        



    }



 



    memcpy(buffer+2, stream, length);



 



    return length + 2;



}



 



 



#endif 



 



 



 



static int set_nonblock(int fd) {



    int flags;



 



    flags = fcntl(fd, F_GETFL, 0);



    if (flags < 0) return flags;



    flags |= O_NONBLOCK;



    if (fcntl(fd, F_SETFL, flags) < 0) return -1;



    return 0;



}



 



 



int send_cb(int fd, int events, void *arg) {



 



    struct sockitem *si = (struct sockitem*)arg;



 



    send(fd, si->sendbuffer, si->slength, 0); //



 



    struct epoll_event ev;



    ev.events = EPOLLIN | EPOLLET;



    //ev.data.fd = clientfd;



    si->sockfd = fd;



    si->callback = recv_cb;



    ev.data.ptr = si;



    



    memset(si->sendbuffer, 0, BUFFER_LENGTH);



 



    epoll_ctl(eventloop->epfd, EPOLL_CTL_MOD, fd, &ev);



 



}



 



//  ./epoll 8080



 



int recv_cb(int fd, int events, void *arg) {



 



    //int clientfd = events[i].data.fd;



    struct sockitem *si = (struct sockitem*)arg;



    struct epoll_event ev;



 



    int ret = recv(fd, si->recvbuffer, BUFFER_LENGTH, 0);



    if (ret < 0) {



 



        if (errno == EAGAIN || errno == EWOULDBLOCK) { //



            return -1;



        } else {



            



        }



 



        ev.events = EPOLLIN;



        //ev.data.fd = fd;



        epoll_ctl(eventloop->epfd, EPOLL_CTL_DEL, fd, &ev);



        close(fd);



        free(si);



        



 



    } else if (ret == 0) { //



 



        // 



        printf("disconnect %d\n", fd);



        ev.events = EPOLLIN;



        //ev.data.fd = fd;



        epoll_ctl(eventloop->epfd, EPOLL_CTL_DEL, fd, &ev);



        close(fd);



        free(si);



        



    } else {



 



        //printf("Recv: %s, %d Bytes\n", si->recvbuffer, ret);



        si->rlength = 0;



 



        if (si->status == WS_HANDSHARK) {



            printf("request\n");    



            printf("%s\n", si->recvbuffer);   



 



            handshark(si, eventloop);



        } else if (si->status == WS_DATATRANSFORM) {



            transform(si, eventloop);



        } else if (si->status == WS_DATAEND) {



 



        }



 



    }



 



}



 



 



int accept_cb(int fd, int events, void *arg) {



 



    struct sockaddr_in client_addr;



    memset(&client_addr, 0, sizeof(struct sockaddr_in));



    socklen_t client_len = sizeof(client_addr);



    



    int clientfd = accept(fd, (struct sockaddr*)&client_addr, &client_len);



    if (clientfd <= 0) return -1;



 



    set_nonblock(clientfd);



 



    char str[INET_ADDRSTRLEN] = {0};



    printf("recv from %s at port %d\n", inet_ntop(AF_INET, &client_addr.sin_addr, str, sizeof(str)),



        ntohs(client_addr.sin_port));



 



    struct epoll_event ev;



    ev.events = EPOLLIN | EPOLLET;



    //ev.data.fd = clientfd;



 



    struct sockitem *si = (struct sockitem*)malloc(sizeof(struct sockitem));



    si->sockfd = clientfd;



    si->callback = recv_cb;



    si->status = WS_HANDSHARK;



    ev.data.ptr = si;



    



    epoll_ctl(eventloop->epfd, EPOLL_CTL_ADD, clientfd, &ev);



    



    return clientfd;



}



 



int main(int argc, char *argv[]) {



 



    if (argc < 2) {



        return -1;



    }



 



    int port = atoi(argv[1]);



 



    



 



    int sockfd = socket(AF_INET, SOCK_STREAM, 0);



    if (sockfd < 0) {



        return -1;



    }



 



    set_nonblock(sockfd);



 



    struct sockaddr_in addr;



    memset(&addr, 0, sizeof(struct sockaddr_in));



 



    addr.sin_family = AF_INET;



    addr.sin_port = htons(port);



    addr.sin_addr.s_addr = INADDR_ANY;



 



    if (bind(sockfd, (struct sockaddr*)&addr, sizeof(struct sockaddr_in)) < 0) {



        return -2;



    }



 



    if (listen(sockfd, 5) < 0) {



        return -3;



    }



 



    eventloop = (struct reactor*)malloc(sizeof(struct reactor));



    // epoll opera



 



    eventloop->epfd = epoll_create(1);



    



    struct epoll_event ev;



    ev.events = EPOLLIN;



    



    struct sockitem *si = (struct sockitem*)malloc(sizeof(struct sockitem));



    si->sockfd = sockfd;



    si->callback = accept_cb;



    ev.data.ptr = si;



    



    epoll_ctl(eventloop->epfd, EPOLL_CTL_ADD, sockfd, &ev);



 



    while (1) {



 



        int nready = epoll_wait(eventloop->epfd, eventloop->events, 512, -1);



        if (nready < -1) {



            break;



        }



 



        int i = 0;



        for (i = 0;i < nready;i ++) {



 



 



 



            if (eventloop->events[i].events & EPOLLIN) {



                //printf("sockitem\n");



                struct sockitem *si = (struct sockitem*)eventloop->events[i].data.ptr;



                si->callback(si->sockfd, eventloop->events[i].events, si);



            }



 



            if (eventloop->events[i].events & EPOLLOUT) {



                struct sockitem *si = (struct sockitem*)eventloop->events[i].data.ptr;



                si->callback(si->sockfd, eventloop->events[i].events, si);



            }



        }



 



    }



 



}



 



 



 
```

原文作者：[[我也要当昏君](https://blog.csdn.net/qq_46118239)

原文链接：https://bbs.csdn.net/topics/605160175

# 【NO.483】Linux服务器开发,手写分布式锁

## 0.前言

分布式锁需要考虑很多事情，第一网络是否正常，第二个提供分布式锁这台机器的高可用性。



![在这里插入图片描述](https://img-blog.csdnimg.cn/066dce11917f4a9b8aaacbef6690cedf.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)



## 1.网络模块封装

### 1.1 io检测部分的接口：

- 添加读事件是因为当有客户端连接事件来临时，我们好去处理这个事件。

- 写事件这是因为当我们服务端作为客户端去连接第三方服务时候，需要注册写事件。

- 删除事件就比如关闭连接时，我们需要将事件从检测模块中删除。

- 修改事件就比如当客户端发来事件我们需要检测读事件，但是接收失败了我们要把读事件修改为注册写事件进行反馈。

- 检测事件肯定是必须的啦！

  ### io操作：

  绑定监听，接收连接，建立连接，关闭fd，读，写，fd属性。



需要注意的是：fd类型中clientfd写事件触发的情况，服务器收到客户端的数据可以直接调用write()函数，**只有当写入失败时才要注册写事件，检测何时可写。**应该将发送失败的数据缓存起来，等事件可写了，再将缓冲区的数据进行发送。
断开连接或者错误，都是交给读写操作。

## 2.协程调度

没有协程之前，当成功接收到一个数据后，要调用多个回调函数,(解析数据、查库拿数据、返回给客户端)等等。现在我们考虑的是，将三个序列利用协程的方式进行粘合，三个回调变成一个协程回调执行序列。
为每一个fd执行一个序列，每个协程是一个执行序列。
lua虚拟机不支持协程，并且没有类似于pthread_create()函数，也就是说主协程被自动创建。**lua虚拟机同时只能有一个协程在运行。**主协程负责调度其他协程。
主协程会进行事件检测，不断地从epoll中去取事件，根据事件去唤醒其他协程。

![在这里插入图片描述](https://img-blog.csdnimg.cn/76fef6d536174cd590cc6ca23038ee1a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)



- lua和go语言中的协程方案是最完善的。
- epoll注册写事件触发，说明三次握手后同步包可以发送了，表示连接已经建立成功了。
- 异步的执行逻辑，同步的写法。
- 当连接建立成功后，connectfd和clientfd的流程变成一样。

## 3.异步连接池

### 3.1 为什么需要异步连接池？

现在所有的连接已经变成一个执行序列，连接由异步变为同步。此时，一个连接同时只能在一个协程中运行，是并发不是并行，也就是说只有一个执行序多个对列，依次执行。
pool_size是尺子最大连接数，backlog堆积的操作数。

![在这里插入图片描述](https://img-blog.csdnimg.cn/05edc8699a0847829c504fe0bdfe566b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)



## 4.缓冲池设计 用户读写缓冲区

Mark老师举例，4个协程在使用，4个协程在等待，超出的协程会报错。cache记录的是随时可用连接，free记录的是正在使用的连接。connect>8是要报错，小于pool_size要创建或是从cache里去取，cache没有说明都在free里。
给连接池起名字，默认是ip：端口格式，比如127.0.0.1:8888。

![在这里插入图片描述](https://img-blog.csdnimg.cn/d72c6b45546748718c40a650321c00e7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)



## 5.定时器设计

**定时器是在lua层实现了一个最小堆，每一个任务生成一个协程，但是要考虑回收协程，尤其是在删除的时候。**这块做的还不完善，Mark老师也希望大家一起帮忙搞一搞，先记下这件事吧！

## 6.总结

Mrk老师说道:做任何事情，拆分的思想很重要，联想起以前一个数学老师的话，“老太太吃柿子，要捡软的捏。”以后遇到问题和困难，也应该先按照这个思路去处理问题。作为开发还有一点是非常重的，就是**测试**的能力，这方面得想办法学习提升一下。通过本节课，我初步了解了分布式锁，感觉对于skynet也有了更好的认识。在成长的路上，有Mark老师陪伴，好幸福啊~

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/604975373

# 【NO.484】Linux服务器开发,手写内存检测组件

## 1.前言

导致内存泄漏的根本原因是因为C++没有内存回收机制gc，内存分配和内存释放次数不对等，是一个很常见的问题，常用的工具有valgrind、mtrace等等，不过这些工具的问题点是我们已经发现了内存泄漏。危害是堆上内存被耗尽，当我们想使用的时候就分配不出来，代码运行不下去被系统强行回收。

- 如何预防内存泄漏
- 如何解决内存泄漏
- 如何发现内存泄漏

分配时候计数器加1，释放时候计数器减1。当我们看到计数器不为0，正常退出的话计数器不等于0，可以大胆的推测出可能存在内存泄漏。
如何定位哪一行内存泄漏

- _FILE,*FUNCTION*,_LINE宏定位代码执行情况
- built_return_address() 返回函数被哪里调用的_libc_malloc()。
  破除递归的方法的代码需要注意。



![在这里插入图片描述](https://img-blog.csdnimg.cn/97bee5b5ada6427cae59f313420b606d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)


**通过addr2line贡酒对指定地址进行代码行数定位，这个做法也是我第一次见，感觉开启了新世界的大门。**



## 2.第一种方法 文件做法

malloc时创建文件，free时删除文件，看程序最后剩下什么文件就可以直观的看到内存泄漏情况。

![在这里插入图片描述](https://img-blog.csdnimg.cn/23065953cd9641d29359f3277257635a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)


可以通过cat命令，将内存泄漏的地址进行查看，再通过addr2line工具对泄漏位置进行定位。



## 3.第二种方法 宏定义

**别人的库有内存泄漏怎么办？**

只要它底层调用的malloc也是可行的。

## 4.第三种方法 指针方案（好用，不推荐）

在/usr/local/malloc.h
__malloc_hook是一个函数指针，当调用malloc时会调用这个函数指针。相对应的也有__free_hook这个函数指针。我们重新将指针赋一个新的地址，这样就有点类似于钩子的感觉，调用malloc/free时将被我们截获。

## 5.第四种方法mtrace工具 版本 纯操作

导入库 export MALLOC_TRACE=./test.log 否则日志文件无法生成。需要把分配释放的地址进行对应分析，然后找出落单的，再用add2line工具计算出具体的泄漏行数，和上面三种方法有异曲同工之妙。

![在这里插入图片描述](https://img-blog.csdnimg.cn/01b642f5577c45dc94f92714bf44b9ae.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)



## 6.总结

首先，不要认为内存泄漏这个问题复杂，核心主要两个问题。如果发现内存泄漏，发现泄漏找到具体出错位置，对错误进行排查。虽然认真学了一上午，感觉还是有些收获，但是在代码层面上还需要进一步加强。从另一个角度来说，尽然C++11封装了智能指针，还是应该充分的使用智能指针，要不然内存泄漏真的是让人头痛的一个事啊！

## 7.补充

**共享内存mmap shmget**

用户空间内专门有一个地方所谓的共享内存，在task_struct结构体中的mm_struct结构体中，堆栈，代码段数据段，参数环境变量等等都整洁的排布着。进程初始化时将各个数值进行赋，mmap_base是共享内存的初始值。值得注意的是，共享内存不是在堆栈上，是有一块虚拟的空间，不同的进程映射在相同的内存空间。有点匿名管道的意思。

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/604975478

# 【NO.485】Linux服务器开发,mysql连接池的实现

## 0.前言

连接池和线程池的的关系，当线程数量小于连接数量时就需要等到连接释放再去争夺连接资源。线程池是主动连接执行任务，连接池和内存池相似都是被动获取，执行任务后归还。

## 1.池化技术

池化技术的作用是减少资源创建次数，提高程序的响应性能。

![在这里插入图片描述](https://img-blog.csdnimg.cn/379c7c1fce8b48fdb4a221929b248d43.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5bGv6Zeo5bGx6bih5Y-r5oiR5bCP6bih,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)


可以看到，在sql执行部分的执行效率是非常低的，只有执行sql语句才是真正干活的时间。



## 2.什么是数据库连接池

用一个连接容器进行存储，7个线程请求到7个连接，握手挥手的时间都可以节省。



![img](https://img-community.csdnimg.cn/images/32b5323c679d41808ed8e632a49689d9.png)



## 3.为什么使用数据库连接池

优点：

- 降低网络开销
- 连接复用，有效减少连接数
- 提升性能，避免频繁的新建连接，新建连接开销比较大
- 没有TIME_WAIT状态问题
  缺点:
- 设计优点复杂
  假如每个线程绑定一个连接，这会导致代码的耦合性比较高。有些任务不需要连接数据库，代码冗余。
  测试发现，数据库在同一台电脑上使用连接池性要比不使用连接池快一倍，如果在不同电脑上可能会差更多。

## 4.数据库连接池运行机制

当需要访问数据库时去连接池取，使用完毕后进行归还。系统关闭之前，要断开所有连接并释放连接占用的系统资源，个人感觉这个挺好理解，符合人类的常规逻辑。上图：



![img](https://img-community.csdnimg.cn/images/51e449a371ac4ec1b3d1e6da0e09ea7d.png)



## 5.连接池和线程池的关系

首先，连接池的连接对象和线程池的线程数量是相对应的。其次线程执行完任务时要关闭连接对象。
最后，线程池是主动调用任务，而线程池是被动的接受，感觉更像是一个小受。

## 6.线程池设计要点

1、连接数据库：涉及数据库ip、端口、用户名、密码、数据库名字

- 独立的连接通道
- 配置最小连接数和最大的连接数。
  2、需要一个管理连接的队列：管理连接，获取连接。list、queue
  3、获取连接对象
  4、归还连接对象
  5、连接池的名字

## 7.连接池的具体实现

### 7.1 编译步骤

```bash
 rm -rf build



 mkdir build



 cd build



 cmake ..



 make



 ./test_dbpool 4 4 1 2000   



 



![img](https://img-community.csdnimg.cn/images/97d1f1f8540948508a944aa2cdb1a14f.png "#left")



 



 



## (4是线程数量 4 是连接数量 1是开启连接池 2000线程数量) 
```

构造函数中初始化必要参数，先创建最小连接数。

```c
GetDBConn(int timeout_ms)；//线程数量多而连接数量不够时，进行超时判断防止等待。
```

- 当当前连接数小于最大连接数时，说明可以新建连接，将连接插入空闲连接，然后再去给予连接句柄。
- 当当前连接数大于最大连接数时，就不能再去创建连接，等待超时时间，可以设置的比较大。过了超时时间仍然拿不到，那很遗憾就返回空啦！有空闲连接就能给予句柄了。
- 决不允许两个任务共用一个连接。

如果真的要销毁连接池：

- 1、先销毁线程池，确保所有任务退出；
- 2、再去销毁连接池。

连接需要归还了我们才去销毁：

- 1、资源申请释放的顺序非常重要
- 2、异步编程是比如容易崩，资源释放异步函数还在使用

数据库重连机制：

- 每次操作之前先去测试链路是否通。
- 先去执行任务，再去处理连接不同的问题。

## 8.连接池连接设置数量

根据测试结果，线程数连接数开到32的时候性能不再提高、
开启多少线程是最快的：N为核心数

- 计算密集型 N+1

- IO密集型 根据实际阻塞的时间去测试，2N+2 这一块感觉接下来要去思考。

  

  ## 9.总结

  通过今天听了零声学院Darren老师的课程，真的是如醍醐灌顶般，瞬间明白了mysql异步连接池的运行原理和实际开发中遇到性能问题该如何解决。自己觉得收获颇丰，对异步连接池有了新的认识，自己有信心有能力对老师的连接池进行改造。从最初的懵懂少年，通过不断地学习和努力，我已经逐渐成为了翩翩少年。是零声学院的老师助我成长，伴我入眠。

  

原文作者：[屯门山鸡叫我小鸡](https://blog.csdn.net/sinat_28294665)

原文链接：https://bbs.csdn.net/topics/605005724

# 【NO.486】后端开发【一大波干货知识】数据库之mysql索引原理详解

## 1.索引

索引分类：主键索引、唯一索引、普通索引、组合索引、以及全文索引（elasticsearch）；

### 1.1.主键索引

非空唯一索引，一个表只有一个主键索引；在innodb中，主键索引的B+树包含表格信息

```
PRIMARY KEY(key) // 括号中的key为列名
```

### 1.2.唯一索引

不可以出现相同的值，可以有NULL值（NULL值也不能重复）

```
UNIQUE(key) // 括号中的key为列名
```

### 1.3.普通索引

允许出现相同的索引内容

```
INDEX(key)
-- OR
KEY(key[,...])
```

### 1.4.组合索引

对表上的多个列进行索引

```
INDEX idx(key1,key2[,...]);
UNIQUE(key1,key2[,....]);
PRIMARY KEY(key1,key2[,....]);
```

### 1.5.全文索引

将存储在数据库当中的整本书和整篇文章中的任意内容信息查找出来的技术；

关键词 FULLTEXT;

在短字符串中用 LIKE % ；在全文索引中用 match 和 against ；

### 1.6.主键选择

innodb 中表是索引组织表，每张表有且仅有一个主键；

1.如果显示设置 PRIMARY KEY ，则该设置的key为该表的主键；

2.如果没有显示设置，则从非空唯一索引中选择；

\1. 只有一个非空唯一索引，则选择该索引为主键；

\2. 有多个非空唯一索引，则选择声明的第一个为主键；

没有非空唯一索引，则自动生成一个 6 字节的 _rowid 作为主键；

## 2.约束

为了实现数据的完整性，对于innodb，提供了以下几种约束，primary key，unique key，

foreign key， default, not null；

### 2.1.外键约束（不建议使用）

外键用来关联两个表，来保证参照完整性；MyISAM存储引擎本身并不支持外键，只起到注释作用；而innodb完整支持外键；

外键约束具有事务性。

```
create table parent ( 
    id int not null, 
    primary key(id) 
) engine=innodb; 
create table child ( 
    id int, 
    parent_id int, 
    foreign key(parent_id) references parent(id) ON DELETE CASCADE ON UPDATE CASCADE 
) engine=innodb; 
-- 被引用的表为父表，引用的表称为子表，上面例子中child为子表，parent为父表； 
-- 外键定义时，可以设置行为 ON DELETE 和 ON UPDATE，行为发生时的操作可选择： 
-- CASCADE 子表做同样的行为 ，如父表删除添加等，子表也会有相同变化
-- SET NULL 更新子表相应字段为 NULL ，如父表删除对应行，子表会设为null
-- NO ACTION 父类做相应行为报错 ，如父表做出改变，子表报错
-- RESTRICT 同 NO ACTION
```

### 2.2.约束与索引的区别

创建主键索引或者唯一索引的时候同时创建了相应的约束；但是约束时逻辑上的概念；索引是一个数据结构既包含逻辑的概念也包含物理的存储方式；

## 3.索引实现

### 3.1.B+树

索引是基于B+树实现的

B+树全称：多路平衡搜索树，减少磁盘访问次数；用来组织磁盘数据，以页为单位，物理磁盘页一般为4K，innodb 默认页大小为 16K；对页的访问是一次磁盘io，缓存中会缓存常访问的页；

特征：非叶子节点只存储索引信息，叶子节点存储具体数据信息；叶子节点之间互相连接，方便范围查询；

每个索引对应着一个 B+ 树；

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173557_81077.jpg)

为了支持范围查询：

所有的非父节点都会存储一个p，用于存储同一层中上一个节点与下一个节点的磁盘地址。比如查找索引值在1~22之间的数据，就可以通过页5->页6->页7，页2->页3的顺序来找数据，这样能够减少回溯，避免每一次都要有这样的过程（访问完叶节点 -> 返回父节点 -> 找下一个叶节点）

### 3.2.B+树层高

B+树的一个节点对应一个数据页；B+树的层越高，那么要读取到内存的数据页越多，io次数越多；

innodb一个节点16kB；

假设：

key为10byte且指针大小6byte，假设一行记录的大小为1kB；

那么一个非叶子节点可存下16kB/16byte=1024个（key+point）；每个叶子节点可存储1024行数据；

结论：

2层B+树叶子节点1024个，可容纳最大记录数为： 1024 * 16 = 16384；

3层B+树叶子节点1024 * 1024，可容纳最大记录数为：1024 * 1024 * 16 = 16777216；

4层B+数叶子节点1024 * 1024 * 1024，可容纳最大记录数为：1024 * 1024 * 1024 * 16 =

17179869184;

注意： 若数据超过500w行的时候，需要进行分表分库，不要将数据都放在一个表中

### 3.3.关于自增id

如果自增id超过了最大值会报错；

若担心自增值会过大，可以使用bigint类型，bigint的范围为(-263, 263-1)。

### 3.4.聚集索引

定义：按照主键构造的 B+ 树；叶子节点中存放数据页；数据也是索引的一部分；

innodb是这样子的

```
# table id name 
select * from user where id >= 18 and id < 40;
```

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173557_52476.jpg)

使用myisam创建数据库，会在mysql对应路径下生成*.frm, *.myd, *.myi文件。

*.frm：表信息文件

*.myd：数据文件（堆构建的）

*.myi：索引文件 （B+树构建的，叶子节点里面存储的是索引值+对应数据的磁盘地址）

通过索引文件找到对应数据的地址，再到数据文件中查找的过程称为回表查询

使用myisam创建数据库，会在mysql对应路径下生成*.frm和*.idb文件，其中*.idb存放的是索引和数据。

### 3.5. 辅助索引

定义：叶子节点不包含行记录的全部数据；辅助索引的叶子节点中，除了用来排序的 key 还包含一个bookmark ；该书签存储了聚集索引的 key；

辅助索引是按照非主键索引构造的。

```
-- 某个表 包含 id name lockyNum; id是主键，lockyNum存储辅助索引; 
select * from user where lockyNum = 33;
```

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173558_66946.jpg)

**解析：**

有两棵B+树。

一颗是根据id创建的聚集索引，叶子节点数据由多个主键索引+行数据组成

一颗是根据lockyNum创建的辅助索引，叶子节点数据由多个辅助索引+bookmark所组成。（bookmark对应一个主键）

根据lockyNum查找数据时，找到对应的辅助索引，再通过对应的bookmark去聚集索引中找具体的数据。从辅助索引跳到聚集索引也称为回表查询。

### 3.6.索引存储方式

innodb由段、区、页组成；段分为数据段、索引段、回滚段等；区大小为 1 MB（一个区由64个连续页构成）；页的默认值为16k；页为逻辑页，磁盘物理页大小一般为 4K 或者 8K；为了保证区中的页的连续，存储引擎一般一次从磁盘中申请 4~5 个区；

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173559_83765.jpg)

### 3.7.最左匹配原则

对于组合索引，从左到右依次匹配，遇到 > < between like 就停止匹配；

有where，group by，having，order by， join on的时候会尝试通过索引来查找

如k1，k2构成组合索引：

当单独使用k2时，会全表查询；

当同时使用k1，k2时并且没有 > < between like ，会用辅助索引查找；

### 3.8.覆盖索引

从辅助索引中就能找到数据，而不需通过聚集索引查找；利用辅助索引树高度一般低于聚集索引树；较少磁盘 io；

即避免回表查询，select的时候不要全表展示出来，可以只展示出辅助索引中的数据，这样就不需要回表查询，或者只用主键进行匹配。

### 3.9.索引失效

1、select … where A and B 若 A 和 B 中有一个不包含索引，则索引失效；

2、索引字段参与运算，则索引失效；例如： from_unixtime(idx) = ‘2021-04-30’;

3、索引字段发生隐式转换，则索引失效；例如： ‘1’ 隐式转换为 1 ；

4、LIKE 模糊查询，通配符 % 开头，则索引失效；例如： select * from user where name like ‘%Mark’;

5、在索引字段上使用 NOT <> != 索引失效；如果判断 id <> 0 则修改为 idx > 0 or idx < 0 ；

6、组合索引中，没使用第一列索引，索引失效；

7、in + or 索引失效；单独的in 是不会失效的；not in 肯定失效的；

### 3.10.索引原则

1、查询频次较高且数据量大的表建立索引；索引选择使用频次较高，过滤效果好的列或者组合；

2、使用短索引；节点包含的信息多，较少磁盘io操作；比如：smallint，tinyint；

3、对于很长的动态字符串，考虑使用前缀索引；

4、对于组合索引，考虑最左侧匹配原则和覆盖索引；

5、尽量选择区分度高的列作为索引；该列的值相同的越少越好；

6、尽量扩展索引，在现有索引的基础上，添加复合索引；最多6个索引

7、不要 select * ； 尽量只列出需要的列字段；方便使用覆盖索引；

8、索引列，列尽量设置为非空；

9、可选：开启自适应 hash 索引或者调整 change buffer；

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173559_23943.jpg)

## 4.innodb体系结构

### 4.1. Buffer Pool

Buffer pool 缓存表和索引数据；采用 LRU 算法（原理如下图）让 Buffer pool 只缓存比较热的数据 ；

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173600_16342.jpg)

### 4.2.Change buffer

Change buffer 缓存非唯一索引的数据变更（DML操作），Change buffer 中的数据将会异步merge 到磁盘当中；

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173601_69195.jpg)

原文链接：https://zhuanlan.zhihu.com/p/499640513

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.487】TCP三次握手、四次挥手以及TIME_WAIT详解

### 1.前提概述

TCP网络编程中常用的api函数有：

socket、bind、listen、accept、recv、send、close、connect

其中socket函数返回一个文件描述符fd，这个fd并不单纯，而是对应着内核创建的TCB（transport control block），可以理解为一个下标索引，而不同TCB则是根据不同的五元组（源ip、源port、目的ip、目的port、协议类型）来进行区分。

bind函数则是将socket函数创建的fd和本机ip联系起来。

listen的功能是通知协议进程准备接收socketfd 上的连接请求，套接字也将从CLOSED转换到LISTEN状态，它同时也指定socket上可以排队等待的连接数的门限值。超过门限值时，socket将拒绝进入的连接请求排队等待。当这种情况出现时，TCP将忽略进入的连接请求，进程可以通过调用accept来得到队列中的连接。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173232_27144.jpg)

### 2.三次握手过程

在服务端调用listen函数进行监听时，客户端就可以准备通信了，而在通信之前自然离不开一些准备工作了，也就是常说的三次握手。

当客户端调用connect函数时，三次握手也随之发生，如图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173232_91736.jpg)

1、客户端首先会发送一个SYN分节（SYN位置位），它将告诉服务器客户端将在连接中发送的数据初始序列号，通常该数据包只有包头。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173233_68098.jpg)

当服务端收到客户端的第一个SYN分节数据包时，服务端将为本次连接建立对应的TCB并存入一个所谓的半连接队列当中，此时半连接队列中的套接字都将处于SYN_RCVD状态。

2、服务器必须确认客户端发送的SYN，置位SYN和ACK位。如图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173234_43101.jpg)

客户端在收到服务器的确认之后，connect将返回0，即建立连接成功。

3、客户端最后再对服务器发送的SYN进行确认，如图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173234_77736.jpg)

当服务器收到来自客户端的确认时，处于半连接队列中的套接字将被移到已完成连接队列的队尾，当进程调用accept时，将从已完成连接队列中的队头项返回给进程。此外，listen函数中的第二个参数backlog在一些系统中规定为两个队列之和的最大值，而在有些系统中则是已完成连接队列和的最大值。当队列已满时，服务端将会忽略客户端发送的SYN分节。

此时已连接队列中的套接字都将处于ESTABLISHED状态，即双方连接建立。

### 3.三次握手过程中的问题

**为什么需要三次握手？**

三次握手其实是为了建立双方之间的通信，即双方都要确认通信状态，而不是单向的通信。如果变成两次握手，那么服务器不能确认客户端的接收状态，此外，服务器发送的ACK数据包若丢失，客户端将拒绝后续的数据，则服务器一直在发送，而客户端一直再拒绝。

### 4.四次挥手

挥手过程可以理解为一对情侣的分手场景，如图：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173235_76269.jpg)

结合三次握手过程，我们能很快清楚四次挥手的过程，继续情侣分手的场景，当一方提出分手时，另一方会做出回应，当然后续会有一些争执或者收拾彼此的东西，然后另一方觉得确实没办法继续在一起了，于是也说出了分手，此时当初提出分手的一方会很干脆的进行确认，此后两方再无瓜葛。

### 5.四次挥手过程中的问题

**1、两端可能同时close吗，此时什么场景？**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215173235_99373.jpg)

**2、主动方出现大量的TIME_WAIT？TIME_WAIT作用？**

这种情况一般发生在高并发短连接的场景，一般可以通过设置reuseaddr的方法来解决。

TIME_WAIT状态作用：

主动关闭放发送最后一个ack包后，可能出现丢失（这时对方还会重发FIN，收到两个FIN的时间间隔一定小于2MSL），使对方没有收到最后一个ACK包时有时间可以重发ACK包；

防止前一个连接中老的分组在新连接中再现，TIME_WAIT存在时间为2MSL，而某个连接上的分组最多存活1MSL就会被丢弃。

**3、出现大量的CLOSE_WAIT状态？**

被动方处于在收到FIN分节时，处于CLOSE_WAIT，接着就需要进行程序收尾工作，一旦耗时操作比较多，对应的CLOSE_WAIT时间就越长。此时需要调整代码逻辑，即时的调用close函数。

**4、出现大量的FIN_WAIT1/2状态？**

此时对端没有即时发送ACK，而主动方也没有其他办法过渡到TIME_WAIT状态，只能选择kill掉进程。

**5、socket描述符和对应的TCB控制块回收的时间点？**

调用close函数后fd被立即回收；

而TCB的话，被动方在接收到ack后被回收，主动方则在TIME_WAIT时间到之后再进行回收。

原文链接：https://zhuanlan.zhihu.com/p/499127830

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.488】Linux内核必懂知识—调度器分析及完全公平调度器CFS

## 1.**调度器分析**

### 1.1.**调度器**

- 内核中安排进程执行的模块，用以切换进程状态。
- 做两件事：选择某些就绪进程来执行；打断某些执行的进程让其变为就绪状态。
- 分配CPU时间的基本依据：进程优先级。
  上下文切换（context switch）：将进程在CPU中切换执行的过程，内核承担 此任务，负责重建和存储被切换掉之前的CPU状态。

### **1.2.调度类sched_class结构体与调度类**

sched_class结构体表示调度类，定义在kernel/sched/sched.h。

- 成员解析
  ecqueue_task:向就绪队列添加一个进程，某个任务进入可运行状态时，该函数将会被调用，它将调度实体放入到红黑树中。
  dequeue_task:将一个进程从就绪队列中进行删除，当某个任务退出可运行状态时调用该函数，它将从红黑树中去掉对应调度实体。
  yield_task:在进程想要资源放弃对处理器的控制权时，可食用在sched_yield系统调用，会调用内核API去处理操作。
  check_preempt_curr:检查当前运行任务是否被抢占。
  pick_next_task:选在接下来要运行的最合适的实体（进程）。
  put_prev_task:用于另一个进程代替当前运行的进程。
  set_curr_task:当任务修改它的调用类或修改它的任务组时，将调用这个函数。
  task_tick:在每次激活周期调度器时，由周期性调度器调用。
- 源码注释

```
// 系统当中有多个调度类，按照调度优先级排成一个链表，下一个优先级调度类
    const struct sched_class *next;
    // 将进程加入到执行队列当中，即将调度实体（进程）存放到红黑树中，并对nr_running变量自动加1
    void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
    // 从执行队列当中删除进程，并对nr_running变量自动减1
    void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
    // 放弃CPU执行权,实际上该函数执行先出队后入队,在这种情况下,它直接将调度实体放在红黑树的最右端
    void (*yield_task) (struct rq *rq);
    bool (*yield_to_task) (struct rq *rq, struct task_struct *p, bool preempt);
    // 用于检查当前进程是否可以被新的进程抢占
    void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);
    /*
     * It is the responsibility of the pick_next_task() method that will
     * return the next task to call put_prev_task() on the @prev task or
     * something equivalent.
     *
     * May return RETRY_TASK when it finds a higher prio class has runnable
     * tasks.
     */
    // 选择下一个应该要运行的进程
    struct task_struct * (*pick_next_task) (struct rq *rq,
                        struct task_struct *prev);
    // 将进程放回到运行队列当中 
    void (*put_prev_task) (struct rq *rq, struct task_struct *p);
#ifdef CONFIG_SMP
    // 为进程选择一个合适的CPU
    int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
    // 迁移任务到另一个CPU
    void (*migrate_task_rq)(struct task_struct *p);
    // 专门用户唤醒进程    
    void (*task_waking) (struct task_struct *task);
    void (*task_woken) (struct rq *this_rq, struct task_struct *task);
    // 修改进程在CPU的亲和力
    void (*set_cpus_allowed)(struct task_struct *p,
                 const struct cpumask *newmask);
    // 启动运行队列
    void (*rq_online)(struct rq *rq);
    // 禁止运行队列
    void (*rq_offline)(struct rq *rq);
#endif
    // 当进程改变它的调度类或进程组时被调用
    void (*set_curr_task) (struct rq *rq);
    // 调用自己time_tick函数，可能引起进程切换，将驱动运行时抢占
    void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
    // 当进程创建时候调用，不同调度策略的进程初始化不一样
    void (*task_fork) (struct task_struct *p);
    // 进程退出时会使用
    void (*task_dead) (struct task_struct *p);
    /*
     * The switched_from() call is allowed to drop rq->lock, therefore we
     * cannot assume the switched_from/switched_to pair is serliazed by
     * rq->lock. They are however serialized by p->pi_lock.
     */
    // 专门用于进程切换操作
    void (*switched_from) (struct rq *this_rq, struct task_struct *task);
    void (*switched_to) (struct rq *this_rq, struct task_struct *task);
    // 更改进程优先级 
    void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
                 int oldprio);
    unsigned int (*get_rr_interval) (struct rq *rq,
                     struct task_struct *task);
    void (*update_curr) (struct rq *rq);
#ifdef CONFIG_FAIR_GROUP_SCHED
    void (*task_move_group) (struct task_struct *p);
#endif
};
```struct sched_class {
    // 系统当中有多个调度类，按照调度优先级排成一个链表，下一个优先级调度类
    const struct sched_class *next;
    // 江金城加入到执行队列当中，即将调度实体（进程）存放到红黑树中，并对nr_running变量自动加1
    void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
    // 从执行队列当中删除进程，并对nr_running变量自动减1
    void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
    // 放弃CPU执行权,实际上该函数执行先出队后入队,在这种情况下,它直接将调度实体放在红黑树的最右端
    void (*yield_task) (struct rq *rq);
    bool (*yield_to_task) (struct rq *rq, struct task_struct *p, bool preempt);
    // 用于检查当前进程是否可以被新的进程抢占
    void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);
    /*
     * It is the responsibility of the pick_next_task() method that will
     * return the next task to call put_prev_task() on the @prev task or
     * something equivalent.
     *
     * May return RETRY_TASK when it finds a higher prio class has runnable
     * tasks.
     */
    // 选择下一个应该要运行的进程
    struct task_struct * (*pick_next_task) (struct rq *rq,
                        struct task_struct *prev);
    // 将进程放回到运行队列当中 
    void (*put_prev_task) (struct rq *rq, struct task_struct *p);
#ifdef CONFIG_SMP
    // 为进程选择一个合适的CPU
    int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
    // 迁移任务到另一个CPU
    void (*migrate_task_rq)(struct task_struct *p);
    // 专门用户唤醒进程    
    void (*task_waking) (struct task_struct *task);
    void (*task_woken) (struct rq *this_rq, struct task_struct *task);
    // 修改进程在CPU的亲和力
    void (*set_cpus_allowed)(struct task_struct *p,
                 const struct cpumask *newmask);
    // 启动运行队列
    void (*rq_online)(struct rq *rq);
    // 禁止运行队列
    void (*rq_offline)(struct rq *rq);
#endif
    // 当进程改变它的调度类或进程组时被调用
    void (*set_curr_task) (struct rq *rq);
    // 调用自己time_tick函数，可能引起进程切换，将驱动运行时抢占
    void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
    // 当进程创建时候调用，不同调度策略的进程初始化不一样
    void (*task_fork) (struct task_struct *p);
    // 进程退出时会使用
    void (*task_dead) (struct task_struct *p);
    /*
     * The switched_from() call is allowed to drop rq->lock, therefore we
     * cannot assume the switched_from/switched_to pair is serliazed by
     * rq->lock. They are however serialized by p->pi_lock.
     */
    // 专门用于进程切换操作
    void (*switched_from) (struct rq *this_rq, struct task_struct *task);
    void (*switched_to) (struct rq *this_rq, struct task_struct *task);
    // 更改进程优先级 
    void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
                 int oldprio);
    unsigned int (*get_rr_interval) (struct rq *rq,
                     struct task_struct *task);
    void (*update_curr) (struct rq *rq);
#ifdef CONFIG_FAIR_GROUP_SCHED
    void (*task_move_group) (struct task_struct *p);
#endif
};
```

### **1.3.Linux调度类**

dl_sched_class、rt_sched_class、fair_sched_class及idle_sched_class等。每个进程都有对应一种调度策略，每一种调度策略又对应一种调度类（每一个调度类可以对应多种调度策略）。

```
extern const struct sched_class stop_sched_class;
extern const struct sched_class dl_sched_class;
extern const struct sched_class rt_sched_class;
extern const struct sched_class fair_sched_class;
extern const struct sched_class idle_sched_class;
```

rt_sched_class类 实时调度器（调度策略:SCHED_FIFO SCHED_RR）
fair_sched_class类 完全公平调度器（调度策略：SCHED_NORMAL、SCHED_BATCH）
SCHED_FIFO调度策略的实时进程永远比SCHED_NORMAL调度策略的普通进程优先运行。eg:pick_next_task函数。
调度类优先级顺序：stop_sched_class > dl_sched_class > rt_sched_class > fair_sched_class > idle_sched_class.
stop_sched_class：优先级最高的线程，会中断所有其他线程，并且不会被其他任务打断。
dl_sched_class
rt_sched_class：作用于实时线程。
fair_sched_class： 公平调度器CFS，作用： 一般常用线程。
idle_sched_class： 每个CPU的第一个PID=0线程，swapper，是一个静态线程。每个调度类属于idle_sched_class。一般运行在开机过程和CPU异常时会做dump。
SCHED_NORMAL,SCHED_BATCH,SCHED_IDLE直接被映射到fair_sched_class；
SCHED_RR,SCHED_FIFO与rt_sched_class相关联。

```
* Simple, special scheduling class for the per-CPU stop tasks:
 */
const struct sched_class stop_sched_class = {
    .next            = &dl_sched_class,
    .enqueue_task        = enqueue_task_stop,
    .dequeue_task        = dequeue_task_stop,
    .yield_task        = yield_task_stop,
    .check_preempt_curr    = check_preempt_curr_stop,
    .pick_next_task        = pick_next_task_stop,
    .put_prev_task        = put_prev_task_stop,
#ifdef CONFIG_SMP
    .select_task_rq        = select_task_rq_stop,
    .set_cpus_allowed    = set_cpus_allowed_common,
#endif
    .set_curr_task          = set_curr_task_stop,
    .task_tick        = task_tick_stop,
    .get_rr_interval    = get_rr_interval_stop,
    .prio_changed        = prio_changed_stop,
    .switched_to        = switched_to_stop,
    .update_curr        = update_curr_stop,
};
```

Linux调度核心选择下一个合适的task运行时，会按照优先级顺序遍历调度类的pick_next_task函数。

## **2.优先级**

- task_struct结构体中采用三个成员表示进程的优先级：prio和normal_prio表示动态优先级, static_prio表示进程的静态优先级。
- 内核将任务优先级划分，实时优先级范围是0到MAX_RT_PRIO-1（即99），而普通进 程的静态优先级范围是从MAX_RT_PRIO到MAX_PRIO-1（即100到139）。数字越小优先级越高。

```
/*
 * Priority of a process goes from 0..MAX_PRIO-1, valid RT
 * priority is 0..MAX_RT_PRIO-1, and SCHED_NORMAL/SCHED_BATCH
 * tasks are in the range MAX_RT_PRIO..MAX_PRIO-1. Priority
 * values are inverted: lower p->prio value means higher priority.
 *
 * The MAX_USER_RT_PRIO value allows the actual maximum
 * RT priority to be separate from the value exported to
 * user-space.  This allows kernel threads to set their
 * priority to a value higher than any user task. Note:
 * MAX_RT_PRIO must not be smaller than MAX_USER_RT_PRIO.
 */
#define MAX_USER_RT_PRIO    100
#define MAX_RT_PRIO        MAX_USER_RT_PRIO
#define MAX_PRIO        (MAX_RT_PRIO + NICE_WIDTH)
#define DEFAULT_PRIO        (MAX_RT_PRIO + NICE_WIDTH / 2)
```

- 进程分类
  实时进程（Real-Time Process）：优先级高、需要立即被执行的进程。
  普通进程（Normal Process）:优先级低、更长执行时间的进程。

## **3.调度策略**

unsigned int policy:保存进程的调度策略（5种）

- SCHED_NORMAL：用于普通进程，通过CFS调度器实现。
- SCHED_BATCH: 相当于SCHED_NORMAL分化版本，采用分时策略，根据动态优先级，分配CPU运行需要资源。用于非交互处理器消耗型进程。
- SCHED_IDLE:优先级最低，在系统空闲时才执行这类进程。系统负载很低时使用CFS。
- SCHED_FIFO:先进先出调度算法（实时调度策略），相同优先级任务先到先服务，高优先级任务可以抢占低优先级任务。
- SCHED_RR：轮流调度算法（实时调度策略）。
- SCHED_DEADLINE:新支持实时进程调度策略，针对突发性计算。

```
/*
 * Scheduling policies
 */
#define SCHED_NORMAL        0
#define SCHED_FIFO        1
#define SCHED_RR        2
#define SCHED_BATCH        3
/* SCHED_ISO: reserved but not implemented yet */
#define SCHED_IDLE        5
#define SCHED_DEADLINE        6
```

## 4.**完全公平调度算法（时间片轮询调度算法）**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216161658_67188.jpg)

实现完全公平调度算法，要为进程定义两个时间：

1. 实际运行时间
   实际运行时间=调度周期*进程权重/所有进程权重之和。*
   *调度周期：指所有进程运行一遍所需要的时间。*
   *进程权重：根据进程的重要性，分配给每个进程不同的权重。*
   *2.虚拟运行时间*
   *虚拟运行时间=实际运行时间*1024/进程权重=（调度周期*进程权重/所有进程权重之和）* 1024/进程权重=调度周期*1024/所有进程总权重。
   一个调度周期里，所有进程的虚拟运行时间相同。

- 基本原理
  CFS定义一种新调度模型，它给cfs_rq（cfs的run queue）中的每一个进程都设置一个虚 拟时钟-virtual runtime(vruntime)。如果一个进程得以执行，随着执行时间的不断增长，其 vruntime也将不断增大，没有得到执行的进程vruntime将保持不变。

## 5.**调度器结构分析**

任务：合理分配CPU时间给运行进程。
目标：有效分配CPU是时间片。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216161658_84329.jpg)

主调度器：通过schedule()函数来完成进程的选择和切换。
周期调度器：根据频率自动调用scheduler_tick函数，作用：根据进程运行时间触发调度。
上下文切换：主要做两件事：切换地址空间；切换寄存器和栈空间。

## 6.**CFS就绪队列**

调度管理是各个调度器的职责。CFS的顶级调度就队列struct cfs_rq。

```
/* CFS-related fields in a runqueue */
// CFS调度运行队列，每个CPU的rq都会包含一个cfs_rq,每个组调度的sched_entity也会有一个cfs_rq队列
struct cfs_rq {
    // CFS运行队列中所有进程总负载
    struct load_weight load;
    // nr_running::cfs_rq中调度实体数量
    // h_nr_running 只对进程有效
    unsigned int nr_running, h_nr_running;
    u64 exec_clock;
        //跟踪记录队列所有进程最小虚拟运行时间
    u64 min_vruntime;
#ifndef CONFIG_64BIT
    u64 min_vruntime_copy;
#endif
    // 红黑树的root  用于在按时间排序的红黑树中管理所有进程
    struct rb_root tasks_timeline;
    // 下一个调度结点（红黑树最左边结点就是下个调度实体）
    struct rb_node *rb_leftmost;
    /*
     * 'curr' points to currently running entity on this cfs_rq.
     * It is set to NULL otherwise (i.e when none are currently running).
     */
    struct sched_entity *curr, *next, *last, *skip;
#ifdef    CONFIG_SCHED_DEBUG
    unsigned int nr_spread_over;
#endif
#ifdef CONFIG_SMP
    /*
     * CFS load tracking
     */
    struct sched_avg avg;
    u64 runnable_load_sum;
    unsigned long runnable_load_avg;
#ifdef CONFIG_FAIR_GROUP_SCHED
    unsigned long tg_load_avg_contrib;
#endif
    atomic_long_t removed_load_avg, removed_util_avg;
#ifndef CONFIG_64BIT
    u64 load_last_update_time_copy;
#endif
#ifdef CONFIG_FAIR_GROUP_SCHED
    /*
     *   h_load = weight * f(tg)
     *
     * Where f(tg) is the recursive weight fraction assigned to
     * this group.
     */
    unsigned long h_load;
    u64 last_h_load_update;
    struct sched_entity *h_load_next;
#endif /* CONFIG_FAIR_GROUP_SCHED */
#endif /* CONFIG_SMP */
#ifdef CONFIG_FAIR_GROUP_SCHED
    struct rq *rq;    /* cpu runqueue to which this cfs_rq is attached */
    /*
     * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in
     * a hierarchy). Non-leaf lrqs hold other higher schedulable entities
     * (like users, containers etc.)
     *
     * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This
     * list is used during load balance.
     */
    int on_list;
    struct list_head leaf_cfs_rq_list;
    struct task_group *tg;    /* group that "owns" this runqueue */
#ifdef CONFIG_CFS_BANDWIDTH
    int runtime_enabled;
    u64 runtime_expires;
    s64 runtime_remaining;
    u64 throttled_clock, throttled_clock_task;
    u64 throttled_clock_task_time;
    int throttled, throttle_count;
    struct list_head throttled_list;
#endif /* CONFIG_CFS_BANDWIDTH */
#endif /* CONFIG_FAIR_GROUP_SCHED */
};
```

## 7.**总结**

本文主要介绍了调度器分析：功能，调度类sched_class结构体与调度类；Linux调度类（5种）及其优先级，调度策略（5种）；完全公平调度算法，包括实际运行时间、虚拟运行时间，调度器结构分析，CFS就绪队列等内容。

原文链接：https://zhuanlan.zhihu.com/p/500583213

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.489】一文彻底掌握用户态协议栈，一看就懂的

## 1.用户态协议栈

那我们先跟大家解释这个协议栈这个东西啊协议栈这个东西呢或多或少啊各个朋友应该都听过，我们站在一个设计者的角度，站在一个设计者的角度，站在tcpip的个人的角度，我们怎么去设计这个协议的？
设计队的角度来设计这个网络协议战。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162730_92965.jpg)

就是各位朋友们你想一下这个网络协议战，
有很多朋友就会想到一个点，那为什么我们还需要去设计一个网络协议栈，我们不是去学这个东西就可以吗？其实我也跟大家解释一下，
其实你在那把网络协议上理解的很透彻的话，你一定要站在一个设计者的角度，
就是你抛开所有的东西，抛开所有的那些框架性的东西，就是你自己去想这个两台PC机之间他们如何通信的，他们通信是a发一段数据b如何收到？
收到完了之后如何去想这个过程是怎么样的，以及发送的数据格式是怎么样的，啊那我们今天就站在这么一个角度来跟大家讲。
当然在讲这个的时候，
我们还会引入一个概念，站在一个设计者的角度去设计一个协议栈的话那怎么去设计？呢那当然我们就跟大家讲到这个用户态协议栈，
因为用户态的协议栈呢他是把协议账当做一个应用程序
来运行，就好比我们很多时候我们写了一个服务器，我们写了一个代码，好，我们跑的时候我们调用网络的接口是调用send,recive，
这个我们在之前讲网络编程的时候给大家讲过，就是我们调用的调用的connect Listen accept
啊这些接口我们是调用的这些接口，这些接口呢是我们系统早就已经帮忙完成的这些接口。
那如果做一个用户态的协议栈呢请大家注意，就是把网络的这一层把我们网络协商对于网络数据解析的这一层，
把它重新拎出来，跟我们的应用程序坐在一起，就把这个网络协议的解析放到进程里面的一部分。

就这么个意思好吧？
就是

好，把协议栈这是协议栈这是我们的应用程序，如果不是这么做，呢
本来的做法呢是这样的，是把网络协议，但它是在操作系统的，把这一部分跟我们的应用程序分开，
放到操作系统里面就这么一种情况，现在用户态协议栈就是把这个协议栈放到应用程序跟应用程序放在一起，这么明显的这个能理解吧。
好，那我们现在再来分析一下，
为什么会要有这个用户态协议栈，呢啊为什么会要有这个用户态协议栈？呢
好，我在这里问一下大家，好减少拷贝，我在这里问一下大家大家有没有接触过或者有用过用户态协议栈的

好，这里有朋友说应该是很多朋友是没有用的，没有用过是很正常的，啊串口通信串口通信不是走的网络吧，应该绝大多数朋友是没有用过的。
那我们接着来跟大家讲一讲，如果没有在这种场景下面，但是你是很难用得上的，就是跟大家解释一下，为什么会有用户态协议栈。

就是

啊各公司私有协议算了，
呃私有协议这是应该是属于用户的协议，就是类似于在TCP的上面去定义的协议，好吧？
好减少CPU上下文切换，那好，

我们来给大家解解释一下为什么是这么个说法，我们这里要跟大家解释一下，首先第一个这里是一个网卡，这里是一个网卡，然后在对应上
中间这一层是我们的协议栈。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162731_59261.jpg)

好，还有这边是我们的应用程序，应用程序就是我们自己编译出来的进程这么一个概念。
这三个点
第一个数据是从哪里来的？这三个这三个我们用一个框把它框加入，就是我们
在服务端也好或者在客户端也好，就是我们能够进行网络通信的也好，就这样一个一个应用程序就是把它放到了一起，就这一段。
好，现在比如说我们应用程序通过一个客户端一个应用程序就是一个进程，没错，应用程序就是一个进程进程是运行阶段，
现在一个客户端也好，然后我们通过来通信就是一个客户端，就是我pc机现在给百度访问百度这个过程也好，或者去访问淘宝这个过程也好，就这样的。
访问先数据是先到达网卡，先到达网卡，这边是客户端也好，就是对端的一台对端的机器，然后发送数据先经过网卡网卡先接收到这个数据，
然后网卡把它处理完了之后，再把这个数据然后copy到协议栈里面，然后协议栈再把这个数据
我们通过系统调用都从协议栈里面copy到我们应用程序上面来。
好，这个过程能不能理解？这个过程应该能理解，就是

我们先把数据先到了网卡那这里，**我们问一下网卡的作用用来做什么**？
好，
网卡资源大家打开这个7层模型啊打开这个7层模型，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162731_67001.jpg)

**网卡是属于哪一层的**，大家看吧打开这7层模型，大家可能很多朋友说的网卡属于哪一层，其实我跟大家讲一下
就是在光纤里面传的时候，在光纤里面传的时候传的是
光信号在双绞线里面传的是电信号，这个物理层的什么物理层就是所说的我们光信号或者电信号
网卡的作用呢就是把这个光电信号转换为数字信号，
转化为数字信号，也就是说一个AD在三个的过程中呢就是把数字信号转化为
我们的模拟信号转化为光电信号，光电就是模拟信号，那也就是说一个AD转化和da转换作用。
好，那地方它所以网卡它不是在任何一层，
它既不是在物理层，也不是在数据链路层，它是在物理层数据链数据链路层之间做这个物理层转化为数据链路层这么一个概念，等于说就是把这个模拟信号转化为数字信号数字信号转化为模拟信号这么一个概念
来理解，如果大家能理解这个网卡的作用

那我们网卡对象在这一版对端机器接收完之后，我们不管是网卡，不管是光纤还是双绞线
网卡接收完这个数据之后，通过AD转化
把模拟信号转为数字信号，然后把这个数据放到协议上，怎么把这个数据从网卡迁移到协议上，请大家注意这里有个东西也要跟大家解释一下，这有个工具
不是叫工具，有一个东西叫做 SK Buffer叫做SK buffer。
好sk_buff这个东西就是就是用来从网卡里面
数据运到协议栈里面，协议栈里面主要对网卡数据进行解析的。

这就是网卡的数据，这就是协议栈
协议栈把网卡数据解析完之后，把SK包的数据解析完之后，对应的这一帧一帧的数据，
然后放到这个recv缓冲区里面，然后我们通过系统调用调用receive这个函数
要用receive，好，然后从协议栈里面把数据从协议栈上发到我们的应用程序，所以我们就能够读到这个数据来理解，这就是中间经过这么两个方面，
就是从网卡copy到协议栈，再从协议站copy到我们的应用程序，这是我们现在操作系统，他工作方式就是这样的，

来理解一个网卡对应一个Mac地址没错，一个网卡对应一个Mac。
好，就是以我们现在的linux我们为例，他就是这么工作的，
就是我们现在每接收一堆数据，就是你现在写的服务器也好，你写的客户端也好，你把数据发送出去接收数据也好，
它都是这么一个过程，每一帧从网卡里面需要copy到协议栈上，再从协议栈上copy再用程序每一次都用，所以很多朋友就在想一个方法，想一个方法，
就是这里面从网卡里面copy到协议栈再从协议栈copy到应用程序，这个过程它有两次拷贝，两次copy，
两次拷贝来理解，这两次拷贝分别是从网卡copy到协议栈，再从协议栈copy到应用程序，其他的我们还没算，就是大家你是因为里面我们copy多，
消息队里面他的消消息对列把copy数据库它的数据库里面拿出来，那这个东西没算，就是每一次系统调用都需要经过这么两次，所以很多问题就在考虑一个问题，
就是我们能不能简化一下，好，后面就出现了一个新的方法，就是这样的。
好，这里是一块内存，这是一块内存，然后通过这么一个方法，
就是通过网卡的这个 dma的方式，就是将网卡映射到内存中间。

好，将网卡映射到内存中间，就是网卡里面解析完的数据这里有块存储，把这块存储的空间映射到内存中间，跟内存的空间是一一对应的，

也就是说接收数据网卡解析完之后，数据就直接映射到内存中间这一个方法来理解，这个方法呢就是跟大家讲到的一个叫做内存映射叫m map的方式，
它底层是走的一种叫做dma的方式，叫做内存直接从直接通道这么一个方法。

这里跟大家讲的大家可以看到从网卡里面的数据到达内存中间，然后应用程序是直接在内存中间可以直接读取这个数据的直接读取
映射过来的这一块数据了，所以在这个过程中间就减少了这么一次拷贝，但是过程中间有可能会，说这不是减少一次吗？有这么斤斤计较吗？请大家注意。
这一次dma的方式，他从严格意义上来说，它不叫copy，什么叫做拷贝
好，首先我们把这数据
数据的通道大家能够理解，就是现在通过网卡映射需要到内存中间，如果大家能理解这个方式，那我们接着再跟大家讲一讲。
就是第一个上面这一条路是有两次拷贝，
下面这一次是采用一种DMA的方式，也就是说这个 DNA的方式什么意思？可以跟大家讲它是没有拷贝的。
没有copy，那很多朋友说那这个网格数据怎么到内存里难道没有拷贝吗？请**大家注意拷贝是什么？就是复制什么意思？**

或者是通过我们CPU执行的这，要通过 CPU指令的啊是通过CPU指令才能够做得到的，
其他注意那这个 dma的方式它是自己操作的，是CPU是不需要去干预的，
CPU不需要干预的，啊就是说网卡的数据直接到达内存中间，
能理解这一个请大家注意，所以这里两次拷贝，但是这个网卡的数据到达内存中间它是没有拷贝的，我们应用程序是可以直接去取这个数据的，所以在这个过程它是没有拷贝的，也叫做零拷贝，

这个地方不是有一次拷贝的，这个零拷贝是怎么理解？就是我刚才讲零拷贝就是利用dma CPU是没有干预的，CPU是没有操作的，所以它叫做零拷贝。
好，如果大家能理解这个零拷贝这个词没有呢

那我们在这里跟大家延伸一下延伸一下，有很多朋友问到这个 mmap的原理好，m map的原理
没错，有那么问到mmap里面的那 mmap，不对应，啊我可以跟大家解释一下，mmap我们可以这么做，我们可以从磁盘中间对
对磁盘中的一个文件我们可以映射，我们同样也可以对网卡进行映射，
也可以同样大家可以对一个USB或者对一个U盘，你也可以叫做dma叫做mmap你也可以把它映射出来，包括有一些蓝牙的设备，包括WiFi的设备，
都是可以直接通过m map映射到内存中间来进行操作的，能理解**那 m map它的原理是怎么样？**
好这一步也跟大家解释一下，
也跟他解释讲要依赖dma对应来说它是一个总线，它是一个总线，请大家注意啊这一步我们就没有深入进去跟大家讲计算机系统体系结构了，因为这东西就比较多了，请大家注意。
像这种dma的方式，mmap包括对于磁盘操作，它也是一个dma的方式，也是绕开了这个 CPU去复制，
包括包括我们对于蓝牙操作或者外设或者USB操作它都是这样的，通过一个DNA的方式，然后直接把对应的存储映射到内存中间，
也就是数据是直接过来的，请大家注意这里有一个前提是需要有一条总线的，
好，请大家注意这一点就可以了，也就是mmap它底层实现的东西，它是由于底层是有DMA的这种方式的支持才可以做到的，好吧？

好CPU如何知道就这样吧这里有个情况就是当做数据，
映射完了之后，这时候会给CPU触发一个中断，就是数据已经就绪了，这一步这是体系结构里面已经讲过，这个这是DMA的方式，数据已经运行，转过来之后，dma
这边传输完了之后会给CPU引发一个中断，好吧？接着跟大家讲网卡上面网卡上面是直接芯片的，好，这个我给大家解释一下。

好，讲到这里，我相信大家可能还有一些概念上的原理，概念上的原理在这里我也问大家一下，就是既然大家有问到这个问题，啊我问一下大家就是关于这个网卡，
网卡的驱动它是运行在哪里好？
答案一是运行在网卡上面的，
第二运行在CPU上面的，运行到我们的操作系统里面的，运行到CPU，就跟我们操作系统运行在一起的，内核里面的啊是选择一还是选择二？
没错，请大家注意，请大家注意。
这里有一个概念都有吧，请大家注意这里就有一个概念的问题，请大家注意，这里有一步就以CPU

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162732_29498.jpg)

好，这个我们把它简化一下我们这两部分，一部分这两个框这一部分是操作系统，也就是我们内核，好，这是网卡。
好，这是网卡。
好，请大家注意啊这个网卡中间它也有芯片，请大家注意这个芯片上面运行的东西**叫做物检**？，它是本来在芯片出厂的时候，在网卡出厂的时候早就已经做好了，

那这往返驱动请大家注意它是内核里面的一部分是去驱动
使得这个网卡进行正常工作的，所以很多时候跟网卡的驱动，包括我们后面跟大家讲的这个 Nic系统的这种
它是运行在内核里面的，它是去兼容使网卡能够正常工作的人，那就它是取使网卡正常工作的，
这么一个东西叫做驱动叫做网卡驱动，能理解这个概念，一定跟大家讲的网卡驱动是运行在内核里面的，它是使得网卡能够正常工作的，以及能够去接受网卡的数据，使得网卡能够发送数据这样一个正常工作，这是网卡系统

好，网卡上面是，芯片芯片上面它也是需要有程序的，以及包括这个网卡上面跟大家解释一下，比如这个网卡上面它接收数据的处理，它的怎么一种处理方法，也就是说对于这个
模模数转换这高低电瓶怎么去以多少作为一个参数，它也是需要有代码的，也是需要有程序收入进去的。
好，
这是关于底层的原理这一部分，我们就关于网络底层这个东西给大家讲到这里。

因为这里面还有一部分就是关于这个网卡它的作用还有很大一部分的作用，包括我们对于这个网卡可以做虚拟化，对于它的一些功能我们还可以做，对于有用户态协议栈这个东西之后，
我们的对于网卡的想象空间就会更大，有了用户态协议栈，我们对网卡的想象空间会更大。
举个例子跟大家解释一下举个例子跟大家解释一下，第一个比如说比如说沟通，比如说我们现在一台PC机，
一台pc机，如果我们能够把网卡自己能够通过代码去控制的话，那我们是不是可以把我们的做成一个交换机也可以，或者说我们做了一个路由器也是可以，
或者说我们包括像一些数据的过滤我们也是ok的，就是一些数据我们不去处理也是ok的，所以对于这个网卡我们的可操作性它就会变得更强。
好，

讲这个讲这里那我们核心的原理还是跟大家来讲，这个协议上这底层原理是依赖这样一个动作，
那对于网卡我们接收完一帧完整的数据，我们怎么处理好吧？
那我们接着来跟大家讲，假设现在假设有一个前提，
假设我们能够取到一帧完整的网络数据，一个完整的数据包。
取得一个完整的数据包，这个完整的数据包呢就是包括网卡里面接收什么数据，我们就能够用什么数据。
好那在这个基础上面我们就有的去实现这个协议栈的这个前提，有了实现就有了实现这个协议上的最基础的东西。
好，
在这里我也跟大家分享，我们怎么去取到一张网络的数据，啊得到一个完整的网络数据包的数据，这里有这么几个方法，
第一个方法，
第一个方法我们可以利用原生的sock的好如何取到一定完整数据，这是第一个方法。
第二个方法，
我们可以利用一些开源的框架比如netmap。第三个我们也可以用一些成熟的，啊我们可以用比如说一些商业的框架比如dpdk，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162733_52754.jpg)

大概是这么三种方法，当然还有一些其他的，比如像这种pfl这种方向都是可以的，大概能够在网卡中间取一点完整的数据的，有这么三种方法。
好吧，那我们今天在这个基础上面，
我们来跟大家先来封装一下，今天的代码量会比较多，代码量会比较多，我们今天要跟大家封装几个协议，为了我们后面就跟大家去实现TCP有关系。
现在我们在我们实现的这个前提下面，在这个底层的框架的基础上面我们实现一个协议栈，那也就是说我们应用的这个框架利用netmap，因为netmap开源的

dbdk我们后面会有专门的主题，专门的内容来给大家讲这一部分。
好，如果各部门现在没有，那地方可以现在在github里面搜一下这个框架，然后把它放下来，然后编译一下然后就可以了，我们在这就跟大家实现这个协议栈

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162733_22443.jpg)

这里是个服务器，这里是个客户端，现在客户端给服务器发送一份数据好。
还是，我们还是采用用一个udp的系统，先从udp开始封装起，比如客户端现在发一些数据到服务器，服务器接收这个数据接上一个完整的udp的数据包，然后怎么处理呢？
我们对一个udp的数据包啊这里要从头开始跟大家讲，就是以一个udp的数据包为例，啊 udp的数据包大概分为这几个方面
第一个
以太网头，第二个IP头，第三个udp头，第四部分才是我们的用户数据，**才是我们接收到，我们调用recvfrom接收到的那一段的数据。**
也就是说一针udp的数据包分为这么几个层次，几个包，第一个以太网的头，第二个IP头，第三个udp头，第四个是我们的用户数据。
也就是说以太网络头是对应的在数据链路层的，然后IP头是在网络层的传输层，udp是在传输层的，然后以及用户层的数据，
每一个我们对应的来跟大家来封装一下，大家可以大家就可以对应的代码，并且我们要把它跑起来。

这里我有必要再打开这个情节给大家解释一下，就是本身这个题目的话，今天我们是讲这个
滑动窗口，我认为在讲滑动窗口之前，啊
然后包括像TCP协议上的实现之前，我们先把这个环境先把它跑起来，先在后面的时候我们再去跟大家去讲到TCP协议具体实现的时候，再去跟大家讲这个关于滑动窗口具体是怎么做的。

第一个就是以太网头怎么封装，呢
以太网的头包括14个字节的以太网的头，
前面6个字节是目的地址，后面6个字节是原地址以及2个字节的类型，请大家注意这个目的地址六个字节什么意思？
就是所谓的那个
MAC地址
请他认为这个MAC地址这个东西
每个网卡都有一个MAC地址，网卡出场时的那个MAC地址那你可以改的，因为我们在每发送一帧数据包的时候，在软件上面我们是可以对这个数据进行修改的，那就是关于这个 MAC地址，
原厂出的mac地址是可以改的，也就是说大家你所接到的MAC地址也好，IP地址也好，端口也好，请大家注意，
我们没有在计算机没有哪个物件，它叫做MAC地址，没有哪个物件叫IP地址，也没有哪个固体的东西叫做端口都没有，请大家注意。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162734_39061.jpg)

所谓的MAC地址也好，ID地址要端口也好，全是协议栈里面一个字段名，那地方全是这样一个字段名而已，它并不是一个固体的部件。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162735_67141.jpg)

好6个字节目的地址，再加上6个字体的原地址，再加上1个协议，
这就是关于以太网的头
6个字节的目的地址，6个字节原地址，请大家注意这1个数组包
大家等一下我们去取的时候
这个我们取出来这么一个一帧数据包，这个数据包按照我们这里应该用数组吧 ；注意这个内存啊这个内存在排布的时候，
结构体的使用它跟数组是一样的，是一样，就是对于一块内存的使用，比如同样是8个字节，
同样是14个字节，那我们是用数组去存也是可以的，我们用结构体去存它也是ok的，好会不会有对齐的问题肯定有。
好，这是关于以太网头，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162735_30640.jpg)

然后第二个就是IP地址

好在这呢关于这个长度我有必要再跟大家聊一聊，就是关于这个长度，啊大家可以看到这16位的这个长度它有多少？
16位，一个udp的是4，节数，16位总长度它有多少？65535，好。
65535，也就是从理论上面来讲，一个IP包它有多长，一个IP包最长它能够有多长？各位最长它能有多长？

也就是说最大的传输单元是1500，最大的传输单元是1500，那一IP包它最大可以传65535，
但是很多人就不太理解，那653这里面不是已经规定了最大只能传1500，嘛为什么这个IP波还能做64k还能做64k?

这 Mtu那个东西它是以太网的限制，你比如说我们一个64k的数据，
一个IP的64k的包，就一个流媒体的数据包，我们发过去了，一个大64k满包发过去，发过去之后请他注意在以太网这层在网卡传出去的时候，它会分片
分成。

这么大一个1500一个一个包把它发出去，连续发多个把它发出去

我们都会有问过这个 mtu和这个最大传输是不是会有些冲突？
比如说如果避免分包分包的过程是避免不了的比如说你去访问百度的过程中间或者访问一个公共IP，你中间要访问的时候，你要经过那些路由器那些路由器或者网关，它也是一个网卡的设备，它也是要协商进行解析，对数据进行分析，请大家注意，
他也会去分包，它在关于分包的这个避免的情况它是避免不了，除非你传输的包特别小，然后每一个包中间的时间间隔足够长，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162736_63145.jpg)

然后就是协议。
可以看到以太网这里有一个proto，这呢关于IP里面呢也有一个协议，
为什么每一层都会有一个协议，
这是标志着让数据链来传输的时候，从这头里面能够解析出来网络层是用的什么协议，通过网络层我们能够解析出来传输层用的什么协议对不对？IP头里面的这一个proto是用来去形容传输层我们用什么协议

ID包里面哪有端口，IP包没有端口

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162736_94055.jpg)

UDP包里面对于每一个包它有没有一个ID？
IP包里面没有，
它没有这个每一个包它是哪一个包发的它是没有的，那这就造成了一个现象，请大家注意就是这个 ud p在发送的时候，
他的协议上面是不可能去实现，从协议本身它不可能去实现。
对于包的定义的就是一个udp，发数据它是没有边界的，你是很难去给udp这个数据包定义一个包的，就是udp协议本身它就没有这个包的概念能理解。
所以各位在这里讲的是udp研发店所有的udp的包，你发现它的包的头
唯一的就是通过这个 check能够效去检验这个包对不对，有没有丢失，但是我们很难去把它去看出来，这里总管一起我们发了多少个包，从它的定义的格式上面，从这个逻辑上面它就应该体会出来，
udp它是没有数据包的概念，所以说啊所以说我们后面会去以一个ud的包给他发送出去，但是请到你udp的头在定义的时候，他压根就没有这个包的概念。
那有一个包肯你知道这个包括有个ID吗？它至少会有一个分割会知道的，包括是有个ID，我知道这个包收到了，UDP是没有

udp只有8个字节协议头。

得出这样一个结论，就是关于IP层为什么没有为什么没有这个端口，

MAC地址它是以太网是数据电路层的产物，
IP地址它是网络层的产物，端口号它是传输层的产物，所以各方面在IP层在网络上它是没有端口这个概念的，所以在另外一个层次我们也可以帮助大家更好去理解就是
如果没有MAC没有IP地址的话，换一句话说换句话说没有IP地址，也就是说哪些如果路由器路由器它是工作在网络层的，
如果没有路由器没有交换机的话，从一定程度上交换机本身就是二层的交换机，二层交机它只适合在局域网内工作，那就它只适合在局域网内工作，
如果要跨网络的话，那就需要借助路由器来调节，或者三层调换器三层调节三层调换器就是它能够工作在网络上来理解，
就是交换机他只能工作在局域网内，但是如果要跨网络，从a网的话请不引这里一定要引入一个路由器或者三层交换机在内。
网好。对于这个端口后呢

很多时候再来问大家一个问题，再问大家一个问题，大家有没有听过NAT的东西？网络地址映射网络地址映射它是什么？将端口
和IP地址做映射的，它是需要工作的传输层，它是要对传输层进行解析的工作，
他需要对传输层的协议进行解析，所以很多时候我们听到一些东西，工作上2层3层4层5层，
工作在哪一层，请大家注意，你就可以看到它是对哪一层产物进行解析的，你就能够判断出来它能够在哪一层，比如说交换机它只对MAC地址进行处理，
所以交换机是二层的产物

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162737_21359.jpg)

nginx工作到应用层，他是对应用层协议进行解析的东西。
haproxy它是对TCP端口他是传输层的，
lvs它是对IP地址也是网络层的，
f5数据链路层

关于负载均衡每一层工作的概念，能理解他工作在哪一层，

这是跟大家讲那个分层的时候，他每一层工作在哪个产品上面，大家能够理解上他，你也能够从他这个工作在哪个层面，你自己应该也能想明白他原理。

第一部分它是需要有一个以太网的，第二个就是IP，
第三个就是udp的，还有一段用户数据就是data，那这个这个用户数据我们怎么定义？
那个用户数据我们怎么地，因为这里面有几个概念，刚才说我想到用数据，
好，那用数组可以，那是我的长度怎么定，第二个有什么数组好像这个长度不那么低，就想到我们用指针指针可不可以用？指针好像不太对，那我来给大家解释一下。

大家看到这里面我们定一下这个长度我们怎么定？好这个 data，

大家呢有两个情况，我们第一个用数组，数组的长度不太好定义，因为我们不知道用户数据有多少，第二个用指针的话就出现一个现象，我比如用指针去定义，你会发现这4个字节的指针会在哪，，
也就是说这个数据包是在这里截止的，后面这个是没有也是这4个字节，它是1个指针指向另外一块内存的，指针肯定是不合适的，

这里给大家介绍叫做一个柔性数组，也叫做零长数组。

柔性速度用在两个方面，有两种前提它是可以用的，
第一个内存是已经分配好的，
第二个跟这个柔性数组它的长度我们是可以通过其他方法来计算出来的，这两种条件下面我们是可以用柔性数组

我们怎么去抓到udp数据包？

我们就用netmap方案来跟大家讲。

**柔性数组仅此它是有一个标签而已，**
**大家看到就是对于这样一个数据包，**
**前面这4个字节是以太网头，20个字节是IP头，8个字节是udp的头那这个payload就是1个标签，指向的**
**是这个udp8个字节头后面的这个位置，就是指向这个位置，至于后面多长，拿着payload这个标签加1，**
**这里有两个前提就在手里头，第一个内存是先分配好的，**
**第二个就是我们可以通过一某种计算能够得出这个数组的长度我们才可以用，不然的话很容易会造成这个类型越界。**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162737_15698.jpg)

第一个这里画三个东西，第一个这里是一个网卡，这里是个CPU，CPU上面在这里有一个不等同的概念，就是可以在这里理解为那个内核
被理解为我们所有在CPU上面执行的东西需要通过CPU，这里面，包括内部包括我们的应用程序，包括我们自己写的代码应用程序就在CPU上执行的。
好还有一个板块就是内存将网卡映射到内存中间，内存将网卡映射到内存中间，
现在我们的应用程序是直接在网卡里面取数据的，直接在网卡里面取数据，所以在这里面我们写的第一个，
首先第一个先让neymap先工作起来，那就是直接发出去没错，直接接收数据和发送数据，
有的网卡安装出来之后叫eth0也有的叫ens33，

ens33是虚拟机的网卡，然后eth0是物理网卡

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162738_30057.png)

这个网卡接下来它的数据就被接管了，**请大家注意这一行之后，这个网卡的所有数据就被映射到内存中间，**

用你的虚拟机开始启动两个网卡来做，那例如也就是说我们用SSH连接的是eth0，
但是我们接管的是eth1来理解这个做法就不会去影响，有时候你发现一开始工作的时候，如果你只有一个网卡，工作之后你会发现断网了，发现SSH连不上了，为什么？
就是因为你这个所有的数据都被让他们去接管了

好，现在跟大家再解决一个问题，**就是我们怎么知道这个网卡来数据，怎么知道这个网卡里面有数据来了？**
好，今天表这里也有一个方法，就这个网卡数据来，它会有个中断，这里有一个东西这是那个 map实现的东西，就是他把这个数据接收完的这个方法，他把它跟我们的io多路复用，也是我们open出来之后。
我们借助一个poll能够去知道，有一个fd能够去知道这里面有数据来了，如果映射完之后tcpdump是抓不到数据。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162739_95584.jpg)

接下来我们就开始去处理它对应的数据。
这里有一个东西要给大家解释一下，大家看到这个摆完之后写完之后，这个poll就判断它对应有没有数据了，就是对应它就是这里有个fd，
natmap在工作的时候，他把这个网卡比较做到一好的一点，他就直接把这个网卡做成了/dev下的一个文件，就做成一个设备文件，
比如说这个设备文件里面，网卡里面所有的数据，所有数据都会到这设备文件里面，也就是natmap里面，这个fd是监测这个设备文件，
我们判断它里面有数据就有数据，这个fd里面有数据来了，我们就可以去读取它。

这里也有一个情况，
读这个动作大家有没有去想过这个关于read或者write什么叫读？
读这个动作，那一定是从外存读到内存，这从包括我们读文件，
读文件或者说我们去读一个设备，包括我们读数据库这个也是ok的，他从外面读到内存中间，从外层读到内存中间叫读，
内存的操作，我们不叫内存叫操作内存，请大家注意这个读这个动作，它叫做这里面一旦我们检测有数据中，这里面我们去操作，那时候我们不叫做读，叫做nm_nextpkt()

叫做去操作获取下一包，这个怎么理解呢？
这个给大家讲这也是netmap实现的，就是网卡里面过来的数据的时候，网卡里面处理完数据之后，把数据映射到内存中间，这个映射的过程中间，
一个包一个包的映射，映射的这个包叫package，如果你再来一个包再来一包， n个客户端连接的网络，n个客户端给这个网卡发送数据的话，
那就有n个包，**那内存中间这是n个包是如何组织起来了？**

其他的东西这里用的一个东西叫做循环队列，叫做ring_buffer。
就是来一个包映射过来的时候，把这包加入这个循环队列里面，所以我们在取的时候，**只要记住它头在哪个地方**，就是取下一包，拿出下一包我们再使用，
就是过来的包这里有一个循环的队列，
好，映射一个包到我们在这里，读的时候这里叫做nm_nextpkt，读出下一个包出来。
这个地方为什么叫做nm_nextpkt,为什么这个地方不叫read。

**就是零拷贝应用在哪些场景？**其实大家可以看到，包括大家你能够想到去做持久化的部分，做日志操作，我们调用的都是像fwrite或者调用write这两个函数，我们在操作日志的时候能够操作文件，那我们如果用mmap这个方法零拷贝就可以使用，我们可以去open一个文件，把它映射到内存中间，然后我们的日志在落盘的时候，我们直接写到这个内存区域中间，然后由他同步过去，不用经过文件，映射到内存就同步过去

知道长度不用担心“/0”

这里还有一个前提也跟大家讲一下就是关于这个
这一步强转可能很多朋友也不太理解，就是我们针对这个数组针对于这一块，stream，你不要简单的把它理解为字符数组，
请你把它理解为单一的一块内存，一块内存的一个开口位置，我们需要对它单独的每一个字节进行操作，所以我们利用的是这个无符号的数组，请注意理解它是指向一块内存，而这个内存多少我们是可以自己去计算的。

**有人说这个方法是不是阻塞的，这个nm_nextpkt是不是阻塞的？**
这个nm_nextpkt是在内存中间，它是内存中间一个循环的
一个环形队列，每一个快给每个包是在内存中间的，但内存操作的时候，我们去操作内存的时候，就在这个点上面，我们没有所谓的阻塞和非阻塞，就是因为这个数据包早就已经准备就绪了，已经有这个数据了。

现在我们调用的是一个poll，他通知到我们应用程序，现在这里已经有包了，我们现在再来取，再来把那个nm_nextpkt包取出来，请注意它是取，但是我们没有做copy。

这个方法,它不能够阻塞和非阻塞来讲，它是存内存操作，

接着我们看一下定义几个东西，这些东西呢在各方面这不是我要定义的，而是这个文档里面有

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162739_36541.jpg)

标准的定义

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162740_63305.jpg)

我们先把这些数据包给抓起来，
它能够执行而已，我们也可以采用循环啊我们可以采用循环，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162740_81439.jpg)

对于这个 Udp，这里我们取出的是一个以太网的帧，以太网的帧，这里当然会有一些特殊情况的存在，就比如说你如果发这个包很大的话，
他不一定那个就是有一些数据是在后面的，就是你如果挖这个数据包很大，那这个nm_nextpkt取得这种包，它可能还有一部分在后面，
这个以太网的帧接受这个数据包，也就是说在我们小于1500的数据的时候，你不用担心，
就是我们如果发送数据小于这个1460的时候，我们根本就不用去担心啊它的数据，比如一半到前面，一半到后面，你的担心是多余的，它都不会存在的好吧？
如果超过1640，那就可能它就会有一个分包现象，就有可能我们一个包在下一个包后面。

接着我们跟大家讲这里面还有一个字节对齐问题

这里这个以太网的头是多少？14个字节，然后 IP头现在是多少？20个字节，然后udp的头是多少？8个字节。
**但是现在这个 sizeof(udppkt)有多少？**
在我们现在这个情况下面等于多少？
等于44号，为什么？
这里会有自己对齐的问题，这前面是4字节,14字节我们一块内存以4个字节
对齐为例，前面4个字节再4个字节，再4个字节，再加上前面2个字节，这里面合到一起14个字节，也就是这一块。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162741_99283.jpg)

这里面在紧接IP包的时候它是顶齐，从这里开始再分配，这里就留下了两个字节的一个空窗期，这中间会有小窗口，这两个字节里面是没有数据的，所以在 sizeof(udppkt)后
里面是等于44的，不是42，
因为这地方有一个小窗口，为了保证就是我们接收的是一个完整的数据包，，一个packet它中间是没有这两个之间的空空格期的，所以在这里我们要对它加上一个对齐，以一个字节的方式对齐，netmap编译的时候，请代表要加上这个东西，就是

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162742_95767.jpg)

刚开始是可以运行的，是可以接收数据的，
但是你要过一段时间，这不行了，好这是第一个问题。
第二个问题刚刚我们是可以拼这个 IP地址，我们现在拼一下，这个我把它看到了我们再跑一下，刚开始我们跑起来时候我们再比较有没有发现这个点？
就是我们刚刚我们在没启动之前它是可以拼的，但你发现我们现在启动了之后现在是不可以拼的，这是第二个问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162742_26458.jpg)

这里请大家注意第一个问题为什么它不行？
这里要给大家引入一个概念叫做arp，
因为我们现在做的这个协议栈跑的这个东西压根就没有去实现arp的协议，
只是简单的把这个 udp的数据包能够接受而已，**那为什么最开始它又可以？**

arp的工作是这样的，arp的工作是在局域网内全部进行广播，
比如每一台机器都会对局域网内从
一段一开始1~255中间每一个区某每一台机器都会去广播，
就是我是192.168点多少，你的MAC地址是多少？

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162743_60624.jpg)

紧接着这台机器接收到这个数据包之后接收到这个arp请求之后，
这台机器就会返回我是某某某我的MAC地址是多少，然后收到响应会在本地建立一个叫做**ARP的表，这里面包含一下IP地址是多少，MAC地址是多少，**每一台都有，我们可以看一下，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162744_83626.jpg)

我们可以看一下在这里就没有192.168.2.217的地址

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162744_84660.jpg)

过一段时间是没了，是因为我们现在这台物理机的这个 arp表里面已经丧失了已经没有了，它已经超时了，这个 IP地址和MAC地址它的这个 arp表的这条信息已经超时了，

好，还有几个问题就是为什么不能这里面也有一个协议，叫做acmp协议。

这里面除了arp和acmp，还有一些其他的协议，比如一些广播上其他的给他发过来，我们这里识别不了你，可是乱码请注意这里面一个数据包都是有自己协议的，
它的表实效是因为现在windows电脑的arp表失效，
他发出的请求还在给他发，但是我现在时间接管网卡的这个应用程序接收完之后没有给他回响应，来理解这个打印

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162745_70313.jpg)

就在这地方挺重要，这一层是说的网络层的协议，IP层如果我们在这个 Ip的这个协议地方，我们引入一个else if这个包等于arp的协议，好我们可以对它进行arp的处理，这是这第一个情况。
第二个情况，如果在这里面判断它是不是acmp协议

**arp是跟IP是一层的，icmp是跟udp一层的，它是传输层的，**也就是大家可以看到这里面所谓的网络分层，其实对于我们来说是先发的那一层协议，是一个谁先谁后的问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162745_30697.jpg)

**ICMP它是传输层的协议**，就是在这个定义的时候是在IP头里面定义的，是在IP头里面。

udp有这么几个特点，
第一个它的实时性比较强，
第二个就是他不带拥塞控制。
传输速度要比TCP快。

是在udp的基础上面，我们是需要封装一层应用协议的，如果不封装应用协议的话，那 udp它是没办法用的对吧？那封装的观点它也是个一对一的传承也是一个一对一的传输好吧？
udp协议它是用于哪一些场景？
nginx

原文链接：https://zhuanlan.zhihu.com/p/501791822

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.490】分布式缓存--缓存与数据库强一致场景下的方案

## **1. 概述**

缓存与数据库的强一致性，也称线性一致性，核心要求是：数据库中的值发生变更，缓存数据要实现同步复制，并且一旦操作完成，随后任意客户端的查询都必须返回这一新值。以下图为例，一旦`写入b`完成，必须保证读到；而写入过程中，认为值的跳变可能发生在某一瞬间，因此读到a或b都是可能的。数据库与缓存作为一个整体，在向外提供服务的过程中，无论数据是否变更过，都时刻保持数据一致，因为它内部的数据`仿佛`只有一份，即使并发访问不同节点。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162214_88937.jpg)

## **2. 场景**

秒杀是一个比较典型的强一致场景，一般秒杀系统的库存同时保持在数据库与缓存中，如果查询缓存有数据，直接可以走秒杀流程，将数据库中的库存数量进行扣减，同时将最新的数据更新到缓存，使缓存中数据与数据库中数据保持强一致，这里只是拿秒杀的场景来举例，类似秒杀的场景有很多，像抢门票系统、12306抢火车票等，资源比较少用户比较多，需要在特定时间内进行抢购的业务场景。真实秒杀场景的设计，是在缓存中扣库存，不会直接在数据库中进行扣库存，因为数据库的性能远远比缓存差，所以本篇也只是拿类似秒杀这样的场景，来阐述强一致下的设计思想与相关实现。

## **3. 方案**

分布式系统里面，有个众所周知的理论，就是`CAP理论`，CAP即：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162215_49747.jpg)

Consistency（一致性）
Availability（可用性）
Partition tolerance（分区容忍性）
这三个性质对应了分布式系统的三个指标。
而CAP理论说的就是一个分布式系统，不可能同时做到这三点。如果默认是分区的，那么只能选择P的情况下，出现了两种选择组合，AP与CP，AP保证可用性则牺牲了一致性，CP保证了一致性则牺牲了可用性，所以我们在讲缓存与数据库强一致的同时，不可避免牺牲了系统可用性的指标，所以看到12306网站这种体验不好，总是抢不到票，或者在一直提示排队中这种情况，就是系统可用性不佳的表现，因为火车站的票源是个稀缺资源，而且在各个站点之间查到的数量又是动态的，在这种强一致性下的业务场景，可用性必然会出现问题。这里不深入讨论12306网站具体是如何实现的，只是拿该场景做个引入。

假设现有一般抢购系统，某些商品搞促销活动，库存也就1000，抢完为止，在开枪时间未到来前，页面显示初始库存，在抢购过程中，只要刷新页面库存还有，按钮就不会置灰，还可以接着点击抢购，直到页面显示库存为0，活动结束。

这是个比较典型的`读多写少`场景，大量请求来集中访问，少部分请求能真正完成下单，我们很容易想到做`读写分离`，将商品的库存提前从数据库预加载到缓存，用户读的时候，从缓存读取数量，只要能看到数量，也就可以直接下单，至于用户能否抢到，得看用户运气了，让真正下单成功的用户去走后续付款操作。注意，这里对于某个用户下单成功后，后台要做的操作是先扣数据库库存数量，随后`实时同步`更新库存到缓存中。如果这一步更新不及时，很有可能数据库与缓存库存不一致，导致缓存中的数量比实际数据库库存还多，最终缓存库存减为零，而数据库已经是负数，结果导致超卖。

### **3.1 数据库与缓存双写+读取操作异步串行化**

当库存发生变化后，更新数据库，同时更新缓存，如果在读并发高的情况下，更新数据库与更新缓存的时间间隔中，被读操作打断，那么读到的将是缓存中旧的库存，数据库已经是新库存，此时会出现不一致；

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162215_70124.jpg)

为了解决这种问题，应先更新数据库后，立即删除缓存

更新数据的时候，根据数据的唯一标识，将操作路由之后，发送到一个jvm内部的内存队列中，同时删除缓存。
读取数据的时候，那么将重新读取数据，并更新缓存的操作，根据唯一标识路由之后，也发送同一个jvm内部的队列中。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162216_83444.jpg)

一个队列对应一个工作线程，每个工作线程串行拿到对应的操作，然后一条一条的执行，这样的话，一个数据变更的操作先执行，删除缓存。如果一个读请求过来，读到了空的缓存，就从数据库将更新后的值加载到缓存。如果并发高的情况下，会出现多个读操作并发的读数据库并加载缓存，可以先将缓存更新的请求发送到队列中，此时会在队列中积压，然后同步等待缓存更新完成。

这里有一个优化点，一个队列中，其实多个更新缓存请求串在一起是没意义的，因此可以做过滤，如果发现队列中已经有一个更新缓存的请求了，那么就不用再放个更新请求操作进去了，直接等待前面的更新操作请求完成即可；

如果请求还在等待时间范围内，不断轮询发现可以取到值了，那么就直接返回； 如果请求等待的时间超过一定时长，那么直接尝试从数据库中读取数据，并写入缓存。

实现代码如下：
`step1`: 注册监听器，初始化工作线程池和内存队列
在SpringBoot的启动类中注册如下监听器类InitListener

```
@EnableAutoConfiguration
@SpringBootApplication
@ComponentScan
@MapperScan("com.roncoo.eshop.inventory.mapper")
public class Application {
    /**
     * 注册监听器
     * @return
     */
    @SuppressWarnings({ "rawtypes", "unchecked" })
    @Bean
    public ServletListenerRegistrationBean servletListenerRegistrationBean() {
        ServletListenerRegistrationBean servletListenerRegistrationBean = 
                new ServletListenerRegistrationBean();
        servletListenerRegistrationBean.setListener(new InitListener());  
        return servletListenerRegistrationBean;
    }
    public static void main(String[] args) {
        SpringApplication.run(Application.class, args);
    }
}
```

监听器的实现类如下如下：

```
/**
 * 系统初始化监听器
 *
 */
public class InitListener implements ServletContextListener {
    @Override
    public void contextInitialized(ServletContextEvent sce) {
        // 初始化工作线程池和内存队列
        RequestProcessorThreadPool.init();
    }
}
```

请求处理线程池的类如下，完成线程池与内存队列的初始化：

```
/**
 * 请求处理线程池：单例
 *
 */
public class RequestProcessorThreadPool {
    /**
     * 线程池
     */
    private ExecutorService threadPool = Executors.newFixedThreadPool(10);
    public RequestProcessorThreadPool() {
        RequestQueue requestQueue = RequestQueue.getInstance();
        for(int i = 0; i < 10; i++) {
            ArrayBlockingQueue<Request> queue = new ArrayBlockingQueue<Request>(100);
            requestQueue.addQueue(queue);  
            threadPool.submit(new RequestProcessorThread(queue));  
        }
    }
    /**
     * 
     * 静态内部类的方式，去初始化单例
     * 
     *
     */
    private static class Singleton {
        private static RequestProcessorThreadPool instance;
        static {
            instance = new RequestProcessorThreadPool();
        }
        public static RequestProcessorThreadPool getInstance() {
            return instance;
        }
    }
    public static RequestProcessorThreadPool getInstance() {
        return Singleton.getInstance();
    }
    /**
     * 初始化方法
     */
    public static void init() {
        getInstance();
    }
}
```

请求内存队列

```
/**
 * 请求内存队列
 *
 */
public class RequestQueue {
    /**
     * 内存队列
     */
    private List<ArrayBlockingQueue<Request>> queues = 
            new ArrayList<ArrayBlockingQueue<Request>>();
    /**
     * 标识位map
     */
    private Map<Integer, Boolean> flagMap = new ConcurrentHashMap<Integer, Boolean>();
    /**
     * 
     * 静态内部类的方式，去初始化单例
     * 
     *
     */
    private static class Singleton {
        private static RequestQueue instance;
        static {
            instance = new RequestQueue();
        }
        public static RequestQueue getInstance() {
            return instance;
        }
    }
    public static RequestQueue getInstance() {
        return Singleton.getInstance();
    }
    /**
     * 添加一个内存队列
     * @param queue
     */
    public void addQueue(ArrayBlockingQueue<Request> queue) {
        this.queues.add(queue);
    }
    /**
     * 获取内存队列的数量
     * @return
     */
    public int queueSize() {
        return queues.size();
    }
    /**
     * 获取内存队列
     * @param index
     * @return
     */
    public ArrayBlockingQueue<Request> getQueue(int index) {
        return queues.get(index);
    }
    public Map<Integer, Boolean> getFlagMap() {
        return flagMap;
    }
}
```

每个工作线程如下：

```
/**
 * 执行请求的工作线程
 *
 */
public class RequestProcessorThread implements Callable<Boolean> {
    /**
     * 自己监控的内存队列
     */
    private ArrayBlockingQueue<Request> queue;
    public RequestProcessorThread(ArrayBlockingQueue<Request> queue) {
        this.queue = queue;
    }
    @Override
    public Boolean call() throws Exception {
        try {
            while(true) {
                // ArrayBlockingQueue，线程安全的内存队列
                // Blocking就是说明，如果队列满了，或者是空的，那么都会在执行操作的时候，阻塞住
                Request request = queue.take();
                boolean forceRfresh = request.isForceRefresh();
                // 先做读请求的去重
                if(!forceRfresh) {
                    RequestQueue requestQueue = RequestQueue.getInstance();
                    Map<Integer, Boolean> flagMap = requestQueue.getFlagMap();
                    if(request instanceof ProductInventoryDBUpdateRequest) {
                        // 如果是一个更新数据库的请求，那么就将那个productId对应的标识设置为true
                        flagMap.put(request.getProductId(), true);
                    } else if(request instanceof ProductInventoryCacheRefreshRequest) {
                        Boolean flag = flagMap.get(request.getProductId());
                        // 如果flag是null
                        if(flag == null) {
                            flagMap.put(request.getProductId(), false);
                        }
                        // 如果是缓存刷新的请求，那么就判断，如果标识不为空，而且是true，就说明之前有一个这个商品的数据库更新请求
                        if(flag != null && flag) {
                            flagMap.put(request.getProductId(), false);
                        }
                        // 如果是缓存刷新的请求，而且发现标识不为空，但是标识是false
                        // 说明前面已经有一个数据库更新请求与一个缓存刷新请求了
                        if(flag != null && !flag) {
                            // 对于这种读请求，直接就过滤掉，不要放到后面的内存队列里面去了
                            return true;
                        }
                    }
                }
                System.out.println("===========日志===========: 工作线程处理请求，商品id=" + request.getProductId()); 
                // 执行这个request操作
                request.process();
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
        return true;
    }
}
```

`step2`: 封装两种请求接口

```
/**
 * 请求接口
 *
 */
public interface Request {
    void process();
    Integer getProductId();
    boolean isForceRefresh();
}
```

更新数据库请求实现类

```
public class ProductInventoryDBUpdateRequest implements Request {
    /**
     * 商品库存
     */
    private ProductInventory productInventory;
    /**
     * 商品库存Service
     */
    private ProductInventoryService productInventoryService;
    public ProductInventoryDBUpdateRequest(ProductInventory productInventory,
            ProductInventoryService productInventoryService) {
        this.productInventory = productInventory;
        this.productInventoryService = productInventoryService;
    }
    @Override
    public void process() {
        System.out.println("===========日志===========: 数据库更新请求开始执行，商品id=" + productInventory.getProductId() + ", 商品库存数量=" + productInventory.getInventoryCnt());  
        // 修改数据库中的库存
        productInventoryService.updateProductInventory(productInventory);  
                // 删除redis中的缓存
        productInventoryService.removeProductInventoryCache(productInventory);
    }
    /**
     * 获取商品id
     */
    public Integer getProductId() {
        return productInventory.getProductId();
    }
    @Override
    public boolean isForceRefresh() {
        return false;
    }
}
```

更新缓存类请求类

```
/**
 * 重新加载商品库存的缓存
 * @author Administrator
 *
 */
public class ProductInventoryCacheRefreshRequest implements Request {
    /**
     * 商品id
     */
    private Integer productId;
    /**
     * 商品库存Service
     */
    private ProductInventoryService productInventoryService;
    /**
     * 是否强制刷新缓存
     */
    private boolean forceRefresh;
    public ProductInventoryCacheRefreshRequest(Integer productId,
            ProductInventoryService productInventoryService,
            boolean forceRefresh) {
        this.productId = productId;
        this.productInventoryService = productInventoryService;
        this.forceRefresh = forceRefresh;
    }
    @Override
    public void process() {
        // 从数据库中查询最新的商品库存数量
        ProductInventory productInventory = productInventoryService.findProductInventory(productId);
        System.out.println("===========日志===========: 已查询到商品最新的库存数量，商品id=" + productId + ", 商品库存数量=" + productInventory.getInventoryCnt());  
        // 将最新的商品库存数量，刷新到redis缓存中去
        productInventoryService.setProductInventoryCache(productInventory); 
    }
    public Integer getProductId() {
        return productId;
    }
    public boolean isForceRefresh() {
        return forceRefresh;
    }
}
```

`step3`: 请求异步执行Service封装

```
/**
 * 请求异步执行的service
 *
 */
public interface RequestAsyncProcessService {
    void process(Request request);
}
```

实现类：

```
/**
 * 请求异步处理的service实现
 * @author Administrator
 *
 */
@Service("requestAsyncProcessService")  
public class RequestAsyncProcessServiceImpl implements RequestAsyncProcessService {
    @Override
    public void process(Request request) {
        try {
            // 做请求的路由，根据每个请求的商品id，路由到对应的内存队列中去
            ArrayBlockingQueue<Request> queue = getRoutingQueue(request.getProductId());
            // 将请求放入对应的队列中，完成路由操作
            queue.put(request);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
    /**
     * 获取路由到的内存队列
     * @param productId 商品id
     * @return 内存队列
     */
    private ArrayBlockingQueue<Request> getRoutingQueue(Integer productId) {
        RequestQueue requestQueue = RequestQueue.getInstance();
        // 先获取productId的hash值
        String key = String.valueOf(productId);
        int h;
        int hash = (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
        // 对hash值取模，将hash值路由到指定的内存队列中，比如内存队列大小8
        // 用内存队列的数量对hash值取模之后，结果一定是在0~7之间
        // 所以任何一个商品id都会被固定路由到同样的一个内存队列中去的
        int index = (requestQueue.queueSize() - 1) & hash;
        System.out.println("===========日志===========: 路由内存队列，商品id=" + productId + ", 队列索引=" + index);  
        return requestQueue.getQueue(index);
    }
}
```

`step4`: 两种请求controller封装

```
@Controller
public class ProductInventoryController {
    @Resource
    private RequestAsyncProcessService requestAsyncProcessService;
    @Resource
    private ProductInventoryService productInventoryService;
    /**
     * 更新商品库存
     */
    @RequestMapping("/updateProductInventory")
    @ResponseBody
    public Response updateProductInventory(ProductInventory productInventory) {
        System.out.println("===========日志===========: 接收到更新商品库存的请求，商品id=" + productInventory.getProductId() + ", 商品库存数量=" + productInventory.getInventoryCnt());
        Response response = null;
        try {
            Request request = new ProductInventoryDBUpdateRequest(
                    productInventory, productInventoryService);
            requestAsyncProcessService.process(request);
            response = new Response(Response.SUCCESS);
        } catch (Exception e) {
            e.printStackTrace();
            response = new Response(Response.FAILURE);
        }
        return response;
    }
    /**
     * 获取商品库存
     */
    @RequestMapping("/getProductInventory")
    @ResponseBody
    public ProductInventory getProductInventory(Integer productId) {
        System.out.println("===========日志===========: 接收到一个商品库存的读请求，商品id=" + productId);  
        ProductInventory productInventory = null;
        try {
            Request request = new ProductInventoryCacheRefreshRequest(
                    productId, productInventoryService, false);
            requestAsyncProcessService.process(request);
            // 将请求扔给service异步去处理以后，就需要while(true)一会儿，在这里hang住
            // 去尝试等待前面有商品库存更新的操作，同时缓存刷新的操作，将最新的数据刷新到缓存中
            long startTime = System.currentTimeMillis();
            long endTime = 0L;
            long waitTime = 0L;
            // 等待超过200ms没有从缓存中获取到结果
            while(true) {
                // 一般公司里面，面向用户的读请求控制在200ms就可以了
                if(waitTime > 200) {
                    break;
                }
                // 尝试去redis中读取一次商品库存的缓存数据
                productInventory = productInventoryService.getProductInventoryCache(productId);
                // 如果读取到了结果，那么就返回
                if(productInventory != null) {
                    System.out.println("===========日志===========: 在200ms内读取到了redis中的库存缓存，商品id=" + productInventory.getProductId() + ", 商品库存数量=" + productInventory.getInventoryCnt());  
                    return productInventory;
                }
                // 如果没有读取到结果，那么等待一段时间
                else {
                    Thread.sleep(20);
                    endTime = System.currentTimeMillis();
                    waitTime = endTime - startTime;
                }
            }
            // 直接尝试从数据库中读取数据
            productInventory = productInventoryService.findProductInventory(productId);
            if(productInventory != null) {
                // 将缓存刷新一下
                // 这个过程，实际上是一个读操作的过程，但是没有放在队列中串行去处理，还是有数据不一致的问题
                request = new ProductInventoryCacheRefreshRequest(
                        productId, productInventoryService, true);
                requestAsyncProcessService.process(request);
                // 代码会运行到这里，只有三种情况：
                // 1、就是说，上一次也是读请求，数据刷入了redis，但是redis LRU算法给清理掉了，标志位还是false
                // 所以此时下一个读请求是从缓存中拿不到数据的，再放一个读Request进队列，让数据去刷新一下
                // 2、可能在200ms内，就是读请求在队列中一直积压着，没有等待到它执行
                // 所以就直接查一次库，然后给队列里塞进去一个刷新缓存的请求
                // 3、数据库里本身就没有，缓存穿透，穿透redis，请求到达mysql库
                return productInventory;
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
        return new ProductInventory(productId, -1L);  
    }
}
```

上述实现过程中有两个优化点
`优化点1`: 读请求去重优化
因为那个写请求肯定会更新数据库，然后那个读请求肯定会从数据库中读取最新数据，然后刷新到缓存中，自己只要hang一会儿就可以从缓存中读到数据了。

`优化点2`: 空数据读请求过滤优化
可能某个数据，在数据库里面压根儿就没有，那么那个读请求是不需要放入内存队列的，而且读请求在controller那一层，直接就可以返回了，不需要等待。

如果缓存里没数据，就两个情况，第一个是数据库里就没数据，缓存肯定也没数据；第二个是数据库更新操作过来了，先删除了缓存，此时缓存是空的，但是数据库里是有的。我们做了之前的读请求去重优化，用了一个flag map，只要前面有数据库更新操作，flag就肯定是存在的，你只不过可以根据true或false，判断你前面执行的是写请求还是读请求。但是如果flag压根儿就没有呢，就说明这个数据，无论是写请求，还是读请求，都没有过。那这个时候过来的读请求，发现flag是null，就可以认为数据库里肯定也是空的，那就不会去读取了。或者说，我们也可以认为每个商品有一个最初始的库存，但是因为最初始的库存肯定会同步到缓存中去的，有一种特殊的情况，就是说，商品库存本来在redis中是有缓存的，但是因为redis内存满了，就给干掉了，但是此时数据库中是有值的，那么在这种情况下，可能就是之前没有任何的写请求和读请求的flag的值，此时还是需要从数据库中重新加载一次数据到缓存中的。

### **3.2 方案改进**

上述方案是笔者的朋友在互联网大厂的经验总结，在思路上是没有问题的，但是在工业级项目的落地过程中，会有不少问题。
比如机器突然挂了，那内存队列就会`丢数据`，再比如，如果并发读的数量很大，那么`内存队列积压`的数据为会越来越多，导致后面的请求也有可能hang在那很长时间，一直读不到数据。

`问题1: 如果内存队列丢数据，怎么办？`
这种情况比较常见，比如机器突然挂了，内存队列数据丢了，该如何处理？首先明确一点，这里的请求都是`同步阻塞`的，如果业务系统挂了，那上游的路由网关会出现请求异常或者超时，外部系统或者外部用户请求也会异常或者超时，那调用端会重试请求，直到机器重启ok，请求会再次进队列，数据只不过重新进入队列。

`问题2：数据积压如何处理？`
这里确实会存在内存队列积压大量的读请求，导致后续的请求hang在那几秒甚至十几秒都没有得到处理。

```
问题3：业务系统需要完成强一致的需求，需要引入内存队列，路由网关，导致大量的开发成本，并且稍微控制不好，就会出现隐藏的bug。
```

针对以上问题，作如下改进：

```
改进点1：封装缓存代理客户端与缓存服务端，引入RocketMQ，将消息写入带上时间戳版本。
```

封装缓存客户端，一方面是省去路由网关，另一方面是充当消息写入的角色。RocketMQ替换原来的内存队列，因为消息本身按照key分区写入，就能保证相同的key会写到相同的分区队列里面。然后换成代理服务端按照消息有序消费，再写入缓存集群，架构设计如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162216_72617.jpg)

上图中，首先事务写入保证消息不会丢，其次写入时带上时间戳作为版本号，防止读取的旧值后写入 ，更新的新值先写入，当写入缓存集群中时，比较时间戳是否是较新的，防止旧值覆盖新值。

`改进点2：解决MQ吞吐问量问题，缓存代理服务端使用内存队列。`
针对改点1中的情形，为保证消息绝对有序，只有一个线程消费MQ中一个分区的消息，再写入缓存，会带来吞吐量的下降，因此在缓存代理服务端使用多个内存队列，让多个线程依次消费多个队列，增加吞吐量。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216162217_39531.jpg)

`改进点3: 增加手动ack，增加消息进入重试队列与死信队列的机制。`
如果Redis缓存挂了，此时需要通过更新缓存后拿到结果，并手动通知ack到消息中间件，确保消息消费者处理完后，才丢弃该消息，防止消息在消费者端丢失，时间戳来保证更新缓存幂等性，此外一直更新缓存失败的消息进入消息中间件进入重试队列死信队列，待下次发消息后再消费。

### **3.3 终极方案**

通过上述改进，一套思想完备，可落地到生产级的方案基本完成，有人会说这不是`分布式锁`的思想么？说对了，多年前，分布式锁还没有发展成熟的时候，就是通过类似的这种消息正中间将写入分区的操作串行化，进行消费，再通过幂等性保证最终写入不会被乱序覆盖，现在分布式锁的实现已经比较成熟，完全可以用分布式锁来解决，比如用Redis的提供的客户端Redission来实现，不但简化流程，而且保证只有抢到锁的线程才可以更新数据库与缓存，再释放锁，当然加锁与释放锁的占用时间也是较快的，因为更新数据与写一条缓存也就几毫秒或者是十几毫秒的时间，可以保证后续更新的操作在很快时间内可以再次抢到锁。

## **4. 总结**

本篇讲述了数据库与缓存双写在强一致下的实现思路与方案，从一开始的方案设计到落地，再到落地后的优化改进，最后到比较可行又简单的方式，你会发现，好的架构不是一步到位，而是逐步`演进`而来；其次几年前的方案，也许比较`合适`，但是现在看起来就会显得过于复杂，原因是解决特定问题的专业技术已经出现，专门解决了该类问题，最终通过重构来解决之前过于复杂又容易出问题的方案，由此也不难发现，好的架构是比较`简单`的。

原文链接：https://zhuanlan.zhihu.com/p/501195720

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.491】手写内存池以及原理代码分析【C语言】

### 1.内存池是对堆进行管理

当进程执行时，操作系统会分出0~4G的虚拟内存空间给进程，程序员可以自行管理（分配、释放）的部分就是mmap映射区、heap堆区，而内存池管理的部分就是用户进程的堆区。

### 2.为什么要用内存池？

内存池就是用来避免堆区出现碎片化

- **避免频繁地分配和释放内存（防止堆区出现碎片化）**

当客户端连接上服务端的时候，服务端会准备一部分的堆区用来做消息保留。当一个连接成功之后，服务器会在堆区为其分配一段属于这个连接的内存，当连接关闭之后，所分配的内存也随之释放。但是当连接量较大且过于频繁时，不可避免地对内存进行频繁的分配和释放。这会导致堆区出现小窗口，也就是堆区碎片化。

堆区出现碎片化会怎么样？

- 长时间工作会出现不可查的BUG
- 无法分配较大且整块的内存，malloc会返回NULL

### 3.内存池设计

场景：在一段很干净的堆区上，如何实现能避免内存碎片化的内存池？

第一：使用链表管理内存

使用链表将分配出来的一块一块内存在堆区连接起来，设置flag(是否被使用)，让链表节点上的各段内存慢慢各自扩张。

单独使用链表会出现什么问题？

- 内存块会被划分得越来越小，链表会变得越来越长，知道不能划分出更大得内存。

所以加入了固定内存的想法的设计

**对于分配小段内存时，将小段内存进行固定划分，如下**

1. 16bytes
2. 32bytes
3. 64bytes
4. 128bytes
5. 256byts
6. 512bytes

**但是不同的固定小内存的分配的话就会出现以下问题:**

1.查找慢

分配时查找

释放时候查找

2.块与块之间也会出现间隙

无法将块合并

小块回收麻烦

所以最后得出自定义固定小块和随机大块的内存池模型

### 4.内存池

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216163108_67685.jpg)

### 5.内存池工作流程

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216163109_62428.jpg)

### 6.内存池结构体

```
//内存池
struct mp_pool_s{
    size_t max;                    
    struct mp_node_s *current;        //当前指向的小块内存区，链表结构
    struct mp_large_s *large;         //大块内存
    struct mp_node_s *head[0];        //小块内存的头部
};
```

小块内存用单向链表串起来

```
//小块内存
struct mp_node_s{
    unsigned char *last;    //当前的
    unsigned char *end;   //最后
    struct mp_node_s *next; //下一个4k
    size_t failed;
};
```

大块内存也是用单向链表串起来

```
//大块内存
struct mp_large_s{
    struct mp_large_s *next;
    void *alloc;
};
```

### 7.API

创建内存池

1. 确定以size大小作为小块内存的固定大小
2. 一来是last指向所在节点的第一个字节
3. end指向最后

```
//创建内存池
struct mp_pool_s *mp_create_pool(int size){
    struct mp_pool_s *pool;
    int ret = posix_memalign((void**)&pool, ALIGNMENT, size + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s));
    if(ret)return NULL;
    pool->max = (size<MP_MAX_ALLOC_FORM_POOL) ? size : MP_MAX_ALLOC_FORM_POOL;
    pool->current = pool->head;
    pool->head->last = (unsigned char*)pool + sizeof(struct mp_pool_s) + sizeof(struct mp_node_s);
    pool->head->end = pool->head->last + size;
    pool->head->failed = 0;
    pool->large = NULL;
    return pool;
}
```

销毁内存池

1. 先释放大块内存，遍历链表
2. 再释放小块，遍历链表

```
//销毁内存池
void mp_destroy_pool(struct mp_pool_s *pool){
    struct mp_large_s *large;
    for(large = pool->large; large!=NULL; large = large->next){ //释放大块
        if(large->alloc)free(large->alloc);
    }
    struct mp_node_s *h = pool->head->next;
    while(h){    //释放小块
        struct mp_pool_s *next = h->next;
        free(h);
        h = h->next;
    }
    free(pool);
}
```

**重置内存池**

1. 先释放大块内存
2. 再将小块内存的last指向内存的第一个位置

```
//重置内存池
void mp_reset_pool(struct mp_pool_s *pool){
    struct mp_node_s *head;
    struct mp_large_s *large;
    for(large = pool->large; large; large=large->next){ //将释放大块
        if(large->alloc)free(large->alloc);
    }
    pool->large = NULL;
    for(head = pool->head; head; head = head->next){    //将各个节点的头位置指向刚创建的位置
        head->last = (unsigned char*)head + sizeof(struct mp_node_s);
    }
}
```

给内存池分配一个小块内存

1. 首先分配出一整块（大小为psize）小块内存，并创建节点指向这块内存
2. 内存对齐，利用尾插法插入链表最后的位置
3. 重新调整内存池的current的指向

```
/*分配psize大小的小块内存，开始指向的位置head->last = memblk + size，链表尾插法*/
static void *mp_alloc_block(struct mp_pool_s *pool, size_t size){
    unsigned char *memblk;
    struct mp_node_s *head = pool->head;
    size_t psize = (size_t)(head->end - (unsigned char*)head);  //psize == 创建内存池时输入的参数
    int ret = posix_memalign((void*)&memblk, ALIGNMENT, psize);  //分配内存     24字节对齐
    if(ret)return NULL;
    struct mp_node_s *p, *new_node, *current;
    new_node = (struct mp_node_s*)memblk;
    new_node->end = memblk + psize;
    new_node->next = NULL;
    new_node->failed = 0;
    memblk += sizeof(struct mp_node_s);                 //跳过节点结构体做内存对齐
    memblk = mp_align_ptr(memblk, ALIGNMENT);            //内存对齐
    new_node->last = memblk + size;
    current = pool->current;
    for(p = current; p->next; p = p->next){             //尾插法
        if(p->failed++>4)current = p->next;
    }
    p->next = new_node;                            
    pool->current = current ? current : new_node;
    return memblk;
}
```

在小块内存中取出一块size大小的内存（不做字节对齐）

1. 首先获取当前内存池所指向的小块内存
2. 跟着一个节点一个节点往下面查找小块内存中是否有足够size大小的内存分配
3. 如果有就将内存做字节对齐并返回
4. 如果没有合适的小块内存则分配多一整块小块内存
5. 如果小块内存分配失败，则以创建大块内存的方式对size分配

```
//在小块内存区上取一块size大小的内存
void *mp_nalloc(struct mp_pool_s *pool, size_t size){
    unsigned char *m;
    struct mp_node_s  *p;
    if(size<=pool->max){
        p = pool->current;  //当前小块指向
        do{
            m = p->last;        
            if((size_t)(p->end - m)>=size){ //如果当前节点剩余内存比size大的话就在该节点分配
                p->last = m + size;
                return m;
            }
            p = p->last;
            //如果没有一个节的剩余大小大于size就重新分配一个block
        }while(p);
        return mp_alloc_block(pool, size);
    }
    //如果这个size超过了小块内存的限制，就以大块内容分配方式来分配
    return mp_alloc_large(pool, size);
}
```

在小块内存中取出一块size大小的内存（做字节对齐）

```
//在小块内存区上取一块size大小的内存  字节对齐
void *mp_alloc(struct mp_pool_s *pool, size_t size){
    unsigned char *memblk;
    struct mp_pool_s *p;
    if(size<=pool->max){
        p = pool->current;         //当前小块指向
        do{
            memblk = mp_align_ptr(p->last, ALIGNMENT);//做字节对齐
            if((size_t)(p->end-memblk) >= size){  //如果当前节点剩余内存比size大的话就在该节点分配
                p->last = memblk + size;
                return memblk;
            }
            p = p->next;            
        }while(p);
        //如果没有一个节的剩余大小大于size就重新分配一个block
        return mp_alloc_block(pool, size);
    }
    //如果这个size超过了小块内存的限制，就以大块内容分配方式来分配
    return mp_alloc_large(pool, size);
}
```

在小块内存中取出一段size大小的内存并初始化为0

```
//分配内存且初始化为0
void *mp_calloc(struct mp_pool_s *pool, size_t size){
    void *p = mp_alloc(pool, size);
    if(p){
        memset(p, 0 , size);
    }
    return p;
}
```

**分配size大小的大块内存（大块内存的节点放在小块内存里）**

1. 首先malloc得到内存首地址，然后将大内存挂到内存池上，如果大块内存的结构体没有创建就在小块内存中创建
2. 创建好之后挂接好并返回大块内存首地址

```
//创建大块内存
static void *mp_alloc_large(struct mp_pool_s *pool, size_t size){
    void *p = malloc(size);                //分配
    if(p==NULL)return NULL;
    size_t n = 0;
    struct mp_large_s  *large;
    for(large=pool->large; large; large = large->next){
        if(large->alloc == NULL){  
            large->alloc = p;
            return p;
        }
        if(n++>3)break;
    }
    //大内存挂接不成
    large = mp_alloc(pool, sizeof(struct mp_large_s));
    if(large==NULL){
        free(large);
        return NULL;
    }
    large->alloc = p;
    large->next = pool->large;
    pool->large = large;
    return p;
}
```

释放指定大块内存p

- 先查找再释放

```
//释放内存p
void mp_free(struct mp_pool_s *pool, void *p){
    struct mp_large_s * large;
    for(large = pool->large; large; large = large->next){
        if(p==large->alloc){
            free(large->alloc);
            large->alloc = NULL;    
            return;
        }
    }
```

重载posix_memalign

```
//重构mem_memalign
void *mp_memalign(struct mp_pool_s *pool, size_t size, size_t alignment){
    void *p;
    int ret = posix_memalign(&p, alignment, size);
    if(ret){
        return NULL;
    }
    struct mp_large_s *large = mp_alloc(pool, sizeof(struct mp_large_s ));
    if(ret)return NULL;
    struct mp_large_s *large = mp_alloc(pool, sizeof(struct mp_large_s));
    if(large==NULL){
        free(p);
        return NULL;
    }
    large->alloc = p;
    large->next = pool->large;
    pool->large = large;
    return p;
}
```

内存对齐公式

```
#define MP_ALIGNMENT               32
#define MP_PAGE_SIZE            4096
#define MP_MAX_ALLOC_FROM_POOL    (MP_PAGE_SIZE-1)
#define mp_align(n, alignment) (((n)+(alignment-1)) & ~(alignment-1))
#define mp_align_ptr(p, alignment) (void *)((((size_t)p)+(alignment-1)) & ~(alignment-1))
```

至此，内存池全部内容都在这

原文链接：https://zhuanlan.zhihu.com/p/502486049

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.492】tcp协议栈实现，tcp定时器与滑动窗口实现

要实现用户态协议栈，必须要搞懂TCP，TCP 11个状态、滑动窗口、拥塞控制、定时器等等。

要使用用户态协议栈，内核提供的epoll就不起作用了，我们需要自己实现用户态的epoll。epoll内部涉及到一个回调的时机，回调的作用是将红黑树中的节点添加进就绪队列，具体在epoll原理里面会具体讲解。搞清楚TCP的11个状态，我们就明白应该在什么时机进行回调了。

### 1.TCP状态转换图

在前面的[posix与网络协议栈](Build software better, together api和网络协议栈.md)中，已经介绍了tcp的状态转换。可以结合tcp状态转换图一起看。

TCP状态保存在哪里？保存在TCB中，即TCP PCB，协议控制块。里面包含了socket信息，以及sendbuffer，recvbuffer。TCB保存了从listen到time_wait的所有状态。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216163719_81035.jpg)

### 2.用户态TCP协议栈实现

前面实现了UDP协议栈，TCP协议栈实现也是类似的，但是比UDP要复杂很多。

### 3.TCP头定义

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216163720_17869.jpg)

seq num初始值是多少，到达最大值(2^32 - 1)后怎么样, 会越界吗？

seq num初始值是一个随机值，之后累加。到达最大值后又从0开始计算，不会越界。

seq num指的是包的数量，还是字节数量？

计算的时候，使用的都是字节数。

TCP的包是什么意思？TCP头为什么没有包长？

TCP前后两个包都有序号，就可以计算出包的长度。

ack num = seq num + 包长。

header length是4bit，最大值是15，单位是4个字节，所以TCP头最大时15*4 = 60字节。没有option的话，TCP头是20字节，header length值就是5。

window size，能够接收数据的最大容量。

urgent pointer, 如果URG位置1，就是告诉对端从这个位置开始的数据，要马上处理。

```
struct tcphdr {
    unsigned short sport;
    unsigned short dport;
    unsigned int seqnum;
    unsigned int acknum;
    unsigned char hdrlen_resv;
    unsigned char flag; 
    unsigned short window;
    unsigned short checksum;
    unsigned short urgent_pointer;
    unsigned int options[0];
};
```

### 4.定义TCP flag

```
#define TCP_CWR_FLAG        0x80
#define TCP_ECE_FLAG        0x40
#define TCP_URG_FLAG        0x20
#define TCP_ACK_FLAG        0x10
#define TCP_PSH_FLAG        0x08
#define TCP_RST_FLAG        0x04
#define TCP_SYN_FLAG        0x02
#define TCP_FIN_FLAG        0x01
```

后面5个flag比较重要

ACK 是用来确认的

PSH 告诉对端赶紧通知应用程序把数据包处理了，在数据传输过程都可以设置成PSH。

RST，收到的ack num，或者seq num、widow size非法，或者数据不对了，就给对端回一个RST。三次握手发送第一次后，超时没收到对端的第二次握手，也会发送一个RST。

SYN只是在连接开始的时候，用于告诉对端seq num，也就是发送的第一个包的序号。

FIN，终止。

### 5.定义TCP包

```
struct tcppkt {
    struct ethhdr eh; // 14
    struct iphdr ip;  // 20 
    struct tcphdr tcp; // 8
    unsigned char data[0];
};
```

### 6.定义TCP状态

```
typedef enum _tcp_status {    TCP_STATUS_CLOSED,    TCP_STATUS_LISTEN,    TCP_STATUS_SYN_REVD,    TCP_STATUS_SYN_SENT,    TCP_STATUS_ESTABLISHED,    TCP_STATUS_FIN_WAIT_1,    TCP_STATUS_FIN_WAIT_2,    TCP_STATUS_CLOSING,    TCP_STATUS_TIME_WAIT,    TCP_STATUS_CLOSE_WAIT,    TCP_STATUS_LAST_ACK,};
```

### 7.定义TCB

```
typedef enum _tcp_status {
    TCP_STATUS_CLOSED,
    TCP_STATUS_LISTEN,
    TCP_STATUS_SYN_REVD,
    TCP_STATUS_SYN_SENT,
    TCP_STATUS_ESTABLISHED,
    TCP_STATUS_FIN_WAIT_1,
    TCP_STATUS_FIN_WAIT_2,
    TCP_STATUS_CLOSING,
    TCP_STATUS_TIME_WAIT,
    TCP_STATUS_CLOSE_WAIT,
    TCP_STATUS_LAST_ACK,
};
```

### 8.实现TCP三次握手

服务端处理好三次握手的状态转换，客户端就能与服务器建立连接。

```
int main() {
    struct nm_pkthdr h;
    struct nm_desc *nmr = nm_open("netmap:eth0", NULL, 0, NULL);
    if (nmr == NULL) return -1;
    struct pollfd pfd = {0};
    pfd.fd = nmr->fd;
    pfd.events = POLLIN;
    struct ntcb tcb;
    while (1) {
        int ret = poll(&pfd, 1, -1);
        if (ret < 0) continue;
        if (pfd.revents & POLLIN) {
            unsigned char *stream = nm_nextpkt(nmr, &h);
            struct ethhdr *eh = (struct ethhdr *)stream;
            if (ntohs(eh->h_proto) ==  PROTO_IP) {
                struct udppkt *tcp = (struct udppkt *)stream;
                if (tcp->ip.type == PROTO_TCP) {
                    struct tcppkt *tcp = (struct tcppkt *)stream;
                    unsigned int sip = tcp->ip.sip;
                    unsigned int dip = tcp->ip.dip;
                    unsigned short sport = tcp->tcp.sport;
                    unsigned short dport = tcp->tcp.dport;
                    tcb = search_tcb();
                    if (tcb->status == TCP_STATUS_LISTEN) { //
                        if (tcp->tcp.flag & TCP_SYN_FLAG) {
                            client_tcb = create_tcb()；
                            client_tcb->status = TCP_STATUS_SYN_REVD;
                            // 将sip，sport，smac与dip，dport，dmac互换
                            // send syn, ack pkt
                            // seqnum, ack 
                        } 
                    } else if (tcb->status == TCP_STATUS_SYN_REVD) {
                        if (tcp->tcp.flag & TCP_ACK_FLAG) {
                            client_tcb->status = TCP_STATUS_ESTABLISHED;
                        }
                    }
                }
            }
        }
    }
}
```

### 9.数据发送过程

MSS（Maximum Segment Size，最大报文长度），是TCP协议定义的一个选项，MSS选项用于在TCP连接建立时，收发双方协商通信时每一个报文段所能承载的最大数据长度。

MTU，是对数据链路层的限制。

客户端到服务器

1. 发送1M的文件
2. sendbuffer = 2k
3. mss = 512
4. mtu = 1500

```
while (1) {
    poll(fd)
    send(fd, buffer, 1k, 0);
}
```

如果客户端sendbuff = 2k, mss = 512。要分4个包发送

客户端能否发出去这4个包呢？

不一定，取决于服务器的接收窗口window size大小。如果window size是1024。客户端如果发送两个包，每个包大小512，则如果服务器的应用程序没有取，那么回的ack包里面的window size就会是0，那么客户端的sendbuffer里面就会剩下1k数据发送不了。就会等服务器数据处理完了再发送。

如果每发送一个包，就等待ack，这种效率太慢。我们需要能够同时发送多个包，就是慢启动的过程。

### 10.慢启动的过程

第一次发送 1 * mss

第二次发送 2 * mss

第三次发送 4 * mss

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216163721_51370.jpg)

慢启动的过程，发送1mss，2mss，4*mss，…

如何判断数据包超出网络负载？

通过判断超时。超时时间又怎么算呢？

拥塞避免，从客户端往服务器发送数据包，网络上的数据越来越多，造成网络拥塞，致使服务器没有办法正确接收数据。

如何判断数据包超出网络负载？

rtt， round trip time， 数据包往返一次的时间。

进入电梯这种弱网的环境下，rtt突然变大，叫做抖动。

当前rtt计算方法

rtt = 0.1 * rtt(new) + 0.9 * rtt(old), 是一个消抖的过程。

用于判断当前这一次有没有超时，一旦出现超时，判断在发送包的数量上是否需要减一减，超出网络负载。

如果服务器的window size是0，没有接收的空间了，客户端就不能再发送了。如果服务器处理完数据，有空间了，客户端怎么能知道服务器有空间了呢？

服务器端window是0，等到服务端将数据处理完，window不为0的时候，客户端怎么能知道服务器已经有接收空间了呢？

1. 服务器主动告诉客户端。– 不好的地方，如果通知包在网络中丢失了怎么办？
2. 客户端定时查询 – TCP是这种做法，当收到对端window为0，定时发送探测包, 就是探测定时器。

客户端定时查询更好。

### 11.滑动窗口

滑动窗口也是以mss作为单位的。

滑动窗口。

在接收的过程中间，准备好指针。一根指针对应已经发送确认的，另一根指针对应允许接收的最大位置。两根指针之间的长度表示window size。

回ack的表示前面的数据都已经收到，都可以调用recv进行处理；未发送ack先不用管，表示数据还没有组织好，还不能调用recv处理。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216163722_93980.jpg)

window大小和recvbuff的关系？

window size和recvbuff是有关系的，但是是两个概念。

看起来window size = recvbuff / 2， 这个没有找到具体的说明。

### 12.定时器

重传定时器、探测定时器(坚持定时器)、keepalive、TIME_WAIT定时器、延迟ack定时器

重传定时器，发送端发送一个包后，启动重传定时器，RTT超时重传，如果在规定时间内收到ack包，则撤销定时器；

探测定时器，如果对端window size 是0，则启动探测定时器；

TCP已经有keepalive，应用层为什么还要提供心跳包？

TCP keepalive也是心跳包，超时主动回收TCB，应用层感知不到。应用层心跳包可控制性更强。

TIME_WAIT定时器，time_wait时间是2msl，防止4次挥手的最后一次ack丢失。

延迟ack定时器，接收端收到TCP包，启动200ms定时器，后面再次收到数据，则重置定时器，超时后发送ack。

原文链接：https://zhuanlan.zhihu.com/p/503077683

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.493】如何更有效的使用 Redis 缓存

## **1.前言**

对于 Redis 来讲，作为缓存使用，是我们在业务中经常使用的，这里总结下，Redis 作为缓存在业务中的使用。

## 2.**旁路缓存**

Cache Aside（旁路缓存）策略以数据库中的数据为准，缓存中的数据是按需加载的。它可以分为读策略和写策略。

## 3.**只读缓存**

只读缓存 从缓存中读取数据；如果缓存命中，则直接返回数据；如果缓存不命中，则从数据库中查询数据；查询到数据后，将数据写入到缓存中，并且返回给用户。

如果需要对数据进行修改的时候，直接修改数据库中的数据，然后删除缓存中的旧数据。

只读缓存的优点：

所有最新的数据都在数据库中，数据不存在丢失的风险。

缺点：

每次修改数据，都会删除缓冲，之后的请求会发生一次缓存缺失。

## **4.读写缓存**

除了进行读操作外，数据的修改操作也会发送到缓存中，直接在缓存中对数据进行修改。此时，得益于Redis的高性能访问特性，数据的增删改操作可以在缓存中快速完成，处理结果也会快速返回给业务应用，这就可以提升业务应用的响应速度。

当然 Redis 是内存数据库，一旦掉电或宕机，内存中的数据就有可能存在丢失。

针对这种情况，一般会有两种回写策略：

- 1、同步回写；

写请求发给缓存的同时，也会发给后端数据库进行处理，等到缓存和数据库都写完数据，才给客户端返回。这样，即使缓存宕机或发生故障，最新的数据仍然保存在数据库中，这就提供了数据可靠性保证。

不过，同步直写会降低缓存的访问性能。这是因为缓存中处理写请求的速度是很快的，而数据库处理写请求的速度较慢。即使缓存很快地处理了写请求，也需要等待数据库处理完所有的写请求，才能给应用返回结果，这就增加了缓存的响应延迟。

- 2、异步回写。

所有写请求都先在缓存中处理。可以定时将缓存写入到内存中，然后等到这些增改的数据要被从缓存中淘汰出来时，再次将它们写回后端数据库。这样一来，处理这些数据的操作是在缓存中进行的，很快就能完成。只不过，如果发生了掉电，而它们还没有被写回数据库，就会有丢失的风险了。

优点：

被修改的数据永远在缓存中，不会发生缓存缺失，下次可以直接访问，不在需要向数据库中进行一次查询。

缺点：

数据可能存在丢失的风险。

## 5.**设置多大的缓存合适**

缓存能够提高响应速度，但是缓存的数量也不是越多越好？

1、大容量缓存是能带来性能加速的收益，但是成本也会更高；

2、在一些场景中，比如秒杀，少量的缓存承担的就是绝大部分的流量访问。

系统的设计选择是一个权衡的过程：大容量缓存是能带来性能加速的收益，但是成本也会更高，而小容量缓存不一定就起不到加速访问的效果。一般来说，建议把缓存容量设置为总数据量的15%到30%，兼顾访问性能和内存空间开销。

## **6.内存被写满了如何处理**

Redis 中的内存被写满了，就会触发内存淘汰机制了

具体参加内存淘汰机制

## **7.缓存经常遇到的问题**

Redis 作为缓存，经常遇到的几种情况：缓存中的数据和数据库中的不一致；缓存雪崩；缓存击穿和缓存穿透。

下面一一来探讨下

## **8.缓存中的数据和数据库中的不一致**

数据一致性，通俗的理解就是，数据库中的数据和缓冲中的数据完全一致就满足一致性。不过对于只读缓存，如果缓冲中没有就去数据库中查询，这样如果缓存中没有数据，但是数据库中的数据是最新的，最终也能满足数据一致性。

所以总结下，一致性大致分成下面的两种情况：

1、缓存中有数据，缓存中的数据和数据库中的数据一样；

2、缓存中没有数据，数据库中记录了最新的数据。

下面分析下只读缓存和读写缓存中的数据不一致情况

## **9.读写缓存**

读写缓存有同步写回和异步写回两种策略

同步写回：缓存在新增修改的时候，也会同步数据到数据库中，这样总能保持缓存中的数据和数据库中的一致；

异步写回：缓存新增修改时候，先不写回到数据库中，定时或者缓存中数据淘汰的时候，再写回到数据库中。这种，如果 Redis 故障宕机了，没有及时写回数据到数据库中，就会造成数据的不一致。

对于读写缓存，使用同步写回的策略，能保证数据数据的一致性。不过，需要在业务应用中使用事务机制，来保证缓存和数据库的更新具有原子性，也就是说，两者要不一起更新，要不都不更新，返回错误信息，进行重试。否则，我们就无法实现同步直写。

如果系统没宕机，redis 系统正常的情况下，因为读写缓存，缓存中的数据是一直存在的，所以当修改数据的时候先修改缓存中的数据，这样就算并发很大的情况下，因为缓存中的数据都是最新的，并且一直存在，这样数据总能读取到最新的数据。

### **9.1.只读缓存**

只读缓存，如果数据新增，直接写入到数据库中，如果有数据修改删除，也是直接操作数据库不过缓存中的数据不会更新，而是直接删除缓存中的数据。

这样数据的更新操作之后，数据库中的数据总是最新的，缓存中就会发生缓存缺失，此时就会从数据库中读取数据，然后再加载到缓存中，这样缓存中的数据总能和数据库中的数据一致。

只读缓存在数据新增的时候，缓存中是没有数据的，所以肯定是要从数据库中加载，这种情况不存在数据不一致的情况。

在只读缓存中，数据不一致的情况,发生在数据的更新删除操作中，下面来一一分析下

删改操作既要修改数据库，同时还要删除对应的缓存，如果这两个操作的原子性无法得到保证，(一起操作成功，或者一起操作失败)，那么数据的一致性就得不到保证了。

### 9.2.**来个异常的栗子**

1、先修改数据库，然后删除缓存，但是删除缓存失败了；

删除缓存失败了，那么缓存中存在的就是旧值，这时候用户的请求过来了，首先去缓存中查询，这时候拿到的就是老旧的数据。

2、先删除缓存，在修改数据库，修改数据库失败了；

缓存删除成功，数据库修改失败了，那么数据库中存在的就是旧值，因为缓存已经被删除了，这时候去缓存中查询，发生了缓存的缺失，数据就会从数据库中加载到缓存中，这时候读取到也是老旧的数据。

**针对这种问题如何解决呢？**

上面出现异常的两种场景，归根到底，就是两者操作的原子性没有得到保证，所以可以借助于消息队列实现最终的一致性。

使用 mq 解决分布式事务可参见分布式事务

这里的操作场景相对简单一点，只要借助于 mq 的重试机制，保证第二步的操成功就可以了。

栗如：

1、先修改数据库；

2、发送删除缓存的消息到 mq 中；

3、下游收到删除的消息，操作删除缓存，如果失败，借助于 mq 的重试机制，就能进行重试操作，直到成功。当然如果，重试多次还是失败，我们需要记录错误原因，然后通知业务方。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216164630_20609.jpg)

那到底应该先删除缓存还是先修改数据库呢？这里我们再探讨一下

### **9.3.先删除缓存后修改数据库**

先删除缓存，然后修改数据库

如果数据库的更新有延迟，那么这时候一个线程过来查询该数据，因为缓存中已经删除了，这时候发生了缓存的缺失，然后就回去数据库中查询，数据库可能还没有更新成功，就可能获取到旧值。

如何解决呢

使用 延迟双删 策略

当数据库被修改之后，线程 sleep 一段时间，然后再次删除缓存，然缓存发生一次缺失，这样下次的请求，就能把数据库中最新的数据加载到缓存中。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216164631_61886.jpg)

比如上面的这种情况，因为数据库的更新可能存在延迟，所以时候线程2读取到了数据库的旧值，然后加载到了缓存中，这样接下来的所有的查询就都会读取旧值

所以 线程1，通过延迟双删来处理这种情况

线程1，在 sleep 一段时间之后，删除缓存，这样就能使后续的缓存缺失，后续的查询就能加载数据库中最新的数据到缓存中。

不过 sleep 的时间需要大于，线程2，读数据并且写入数据到内存的时间，如果 sleep 时间过小，这时候线程2，的旧值还没有写入到缓存中，线程1，已经再次删除了缓存，然后这时候线程2把旧值写入，导致缓存中依然是旧数据。

```
redis.delKey(X)db.update(X)Thread.sleep(N)redis.delKey(X)
```

当然，这在 sleep 的时间内，还是有一部分请求会读取到旧值

### **9.4.先修改数据库然后删除缓存**

先修改数据库，然后删除缓存

如果缓存删除有延迟，那么这时候过来的请求，就会读取到缓存中老旧的数据，不过缓存会马上被删除，只会有少部分的数据读取到老旧的数据，对业务影响比较小。

经过对比，发现先修改数据库然后在删除缓存，对我们业务的影响比较小，同时也跟容易处理。

### 9.5.**只读缓存和读写缓存如何选择**

读写缓存对比只读缓存

优点：缓存中一直会有数据，如果更新操作后会立即再次访问，可以直接命中缓存，能够降低读请求对于数据库的压力。

缺点：如果更新后的数据，之后很少再被访问到，会导致缓存中保留的不是最热的数据，缓存利用率不高（只读缓存中保留的都是热数据）。

所以读写缓存比较适合用于读写相当的业务场景。

## **10.缓存雪崩**

### 10.1.**什么是缓存雪崩**

缓存雪崩是指大量的应用请求无法在Redis缓存中进行处理，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增。

缓存雪崩有两种场景

### **10.2.大量缓存同时过期**

如果有大量的缓存 key 设置了同样的过期时间，如果这些缓存 key 过期了，同时有大量的请求，进来了，这些请求就会直接打到数据库中，数据库可能因为这些请求，导致数据库压力增大，严重的时候数据库宕机。

如何解决呢？

1、避免给大量的过期键设置相同的过期时间，设计过期时间的时候，可以考虑加入一个业务上允许的过期随机值；

2、服务降级，只有部分核心业务的请求，才会流转到数据库中，数据库的压力就会被大大减轻了；

- 当业务应用访问的是非核心数据（例如电商商品属性）时，暂时停止从缓存中查询这些数据，而是直接返回预定义信息、空值或是错误信息；
- 当业务应用访问的是核心数据（例如电商商品库存）时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。

### **10.3.Redis 实例发生宕机**

Redis 实例的宕机，缓存层就不能处理数据，最总流量都会流入到数据库中

如何解决呢？

1、业务中实现服务熔断或者请求限流机制；

- 服务熔断：如果监听到发生了缓存雪崩，直接暂停对缓存服务的请求，但是这种对业务的影响比较大；
- 服务限流：可以在入口做限流，不要让所有的请求都流入到后端的服务中；

2、提前预防，搭建 Redis 的高可用集群；

- 尝试构建 Redis 的高可用集群，比如当某主节点挂掉了，集群能够马上重新选出新的主节点。例如哨兵机制

### **10.4.缓存击穿**

其实跟缓存雪崩有点类似，缓存雪崩是大规模的key失效，而缓存击穿是一个热点的Key，有大并发集中对其进行访问，突然间这个Key失效了，导致大并发全部打在数据库上，导致数据库压力剧增。这种现象就叫做缓存击穿。

如何解决？

对于热点 key 可以不设置过期时间，或者设置一个超过使用周期的过期时间，保证这个 key 在业务使用期间永远存在。

### **10.5.缓存穿透**

如果业务请求的缓存，既不在缓存中，也不再数据库中，那么缓存将没有用，所有的请求都会流入到数据库中。

那么，缓存穿透会发生在什么时候呢？一般来说，有两种情况。

1、业务层误操作：缓存中的数据和数据库中的数据被误删除了，所以缓存和数据库中都没有数据；

2、恶意攻击：专门访问数据库中没有的数据。

如何解决？

1、缓存空值或缺省值；

一旦发生缓存穿透，在缓存中写入一个业务中允许的空值，这样缓存中有数据了，就避免了缓存穿透。

2、使用布隆过滤器；

使用布隆过滤器判断下数据是否存在，数据如果不存在，就不向数据库发起请求了。

布隆过滤器

3、在请求入口的前端进行请求检测；

缓存穿透的一个原因是有大量的恶意请求访问不存在的数据，所以，一个有效的应对方案是在请求入口前端，对业务系统接收到的请求进行合法性检测，把恶意的请求（例如请求参数不合理、请求参数是非法值、请求字段不存在）直接过滤掉，不让它们访问后端缓存和数据库。这样一来，也就不会出现缓存穿透问题了。

### 10.6.**缓存中的 hot key 和 big key**

这两种的处理方式可参见

Hot Key 和 big key

## 11.**总结**

对于缓存的使用，我们经常用到的有两种1、只读缓存；2、读写缓存；

只读缓存，对比读写缓存

优点：缓存中一直会有数据，如果更新操作后会立即再次访问，可以直接命中缓存，能够降低读请求对于数据库的压力。

缺点：如果更新后的数据，之后很少再被访问到，会导致缓存中保留的不是最热的数据，缓存利用率不高（只读缓存中保留的都是热数据）。

所以读写缓存比较适合用于读写相当的业务场景。

缓存在使用的过程中，会面临缓存中的数据和数据库中的不一致；缓存雪崩；缓存击穿和缓存穿透，这些我们需要弄明白这些情况发生的额场景，然后再业务中一一去避免。

原文链接：https://zhuanlan.zhihu.com/p/505318288

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.494】Redis之最细命令介绍

## 1.Redis

Redis是Remote Dirctionary Service的简称，即远程字典服务；

Redis是内存数据库（数据都存储在内存中，mysql中主要数据存储在磁盘）、KV数据库（key-value）、数据结构数据库（value提供了丰富的数据结构）；

Redis应用非常广泛，如Twitter、暴雪娱乐、Github、Stack Overflow、腾讯、阿里、京东等等，很多中小型公司也在使用；

Redis有16个数据库（字典），并且是单线程的，所以使用时只使用一个数据库。一个key只对应一个value；

## 2.使用Redis步骤

1、connect，客户端连接Redis；

2、auth，输入登录信息，用户名与密码；未设置密码则不需要输入密码；

3、select，选择要使用的数据库（Redis有16个数据库），不选择则默认使用第0个数据库

## 3.value

注意：Redis中的数字是从1开始，而不是从0开始的。负数代表倒数，如-1为倒数第一个、-2为倒数第二个

## 4.Redis中value编码

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216164215_75759.jpg)

## 5.string

字符数组，该字符串是动态字符串 raw，字符串长度小于1M 时，加倍扩容；超过 1M 每次只多扩1M；字符串最大长度为 512M；

注意：redis 字符串是二进制安全字符串；可以存储图片，二进制协议等二进制数据；

### 5.1.基础命令

```
# 设置 key 的 value 值 
SET key val 
# 获取 key 的 value 
GET key 
# 执行原子加一的操作 
INCR key 
# 执行原子加一个整数的操作 
INCRBY key increment 
# 执行原子减一的操作 
DECR key 
# 执行原子减一个整数的操作 
DECRBY key decrement 
# 如果key不存在，这种情况下等同SET命令。 当key存在时，什么也不做 
SETNX key value 
# 删除 key val 键值对 
DEL key 
# 设置或者清空key的value(字符串)在offset处的bit值。 
SETBIT key offset value 
# 返回key对应的string在offset处的bit值 
GETBIT key offset 
# 统计字符串被设置为1的bit数. 
BITCOUNT key
```

### 5.2.存储结构

字符串长度小于等于 20 且能转成整数，则使用 int 存储；

字符串长度小于等于 44，则使用 embstr 存储；

字符串长度大于 44，则使用 raw 存储；

### 5.3.应用

```
# 对象存储
SET role:10001 '{["name"]:"mark",["sex"]:"male",["age"]:30}' 
GET role:10001
# 一般Redis客户端可以识别':'，role:10001是指10001行的数据
# 这个value是json格式，如果经常需要修改还是用hash来存储比较好
# 累加器
# 统计阅读数 累计加1 
incr reads 
# 累计加100 
incrby reads 100
# 分布式锁 ，本例简单展示一下，后序文章中会具体讲解
# 加锁 
setnx lock 1 
# 释放锁 
del lock 
# 1. 排他功能 2. 加锁行为定义 3. 释放行为定义
# 位运算
# 月签到功能 10001 用户id 202106 2021年6月份的签到 6月份的第1天 
setbit sign:10001:202106 1 1 
# 计算 2021年6月份 的签到情况 
bitcount sign:10001:202106 
# 获取 2021年6月份 第二天的签到情况 1 已签到 0 没有签到 
getbit sign:10001:202106 2
```

## 6.list

双向链表实现，列表首尾操作（删除和增加）时间复杂度 O(1) ；查找中间元素时间复杂度为O(n) ；

列表中数据是否压缩的依据：

1. 元素长度小于 48，不压缩；
2. 元素压缩前后长度差不超过 8，不压缩；

### 6.1.基础命令

```
# 从队列的左侧入队一个或多个元素 
LPUSH key value [value ...] 
# 从队列的左侧弹出一个元素 
LPOP key 
# 从队列的右侧入队一个或多个元素 
RPUSH key value [value ...] 
# 从队列的右侧弹出一个元素 
RPOP key 
# 返回从队列的 start 和 end 之间的元素 0, 1 2 
LRANGE key start end 
# 从存于 key 的列表里移除前 count 次出现的值为 value 的元素
LREM key count value 
# 它是 RPOP 的阻塞版本，因为这个命令会在给定list无法弹出任何元素的时候阻塞连接 
BRPOP key timeout # 超时时间 + 延时队列
```

### 6.2.存储结构

```
/* Minimum ziplist size in bytes for attempting compression. */ 
#define MIN_COMPRESS_BYTES 48 
/* quicklistNode is a 32 byte struct describing a ziplist for a quicklist. 
* We use bit fields keep the quicklistNode at 32 bytes. 
* count: 16 bits, 
* max 65536 (max zl bytes is 65k, so max count actually < 32k). 
* encoding: 2 bits, RAW=1, LZF=2. 
* container: 2 bits, NONE=1, ZIPLIST=2. 
* recompress: 1 bit, bool, true if node is temporary decompressed for usage. 
* attempted_compress: 1 bit, boolean, used for verifying during testing. 
* extra: 10 bits, free for future use; pads out the remainder of 32 bits */ 
typedef struct quicklistNode { 
    struct quicklistNode *prev; 
    struct quicklistNode *next; 
    unsigned char *zl; 
    unsigned int sz; /* ziplist size in bytes */ 
    unsigned int count : 16; /* count of items in ziplist */ 
    unsigned int encoding : 2; /* RAW==1 or LZF==2 */ 
    unsigned int container : 2; /* NONE==1 or ZIPLIST==2 */ 
    unsigned int recompress : 1; /* was this node previous compressed? */ 
    unsigned int attempted_compress : 1; /* node can't compress; too small */                 
    unsigned int extra : 10; /* more bits to steal for future usage */ 
} quicklistNode; 
typedef struct quicklist { 
    quicklistNode *head; quicklistNode *tail; 
    unsigned long count; /* total count of all entries in all ziplists */ 
    unsigned long len; /* number of quicklistNodes */ 
    int fill : QL_FILL_BITS; /* fill factor for individual nodes */ 
    unsigned int compress : QL_COMP_BITS; /* depth of end nodes not to compress;0=off */ 
    unsigned int bookmark_count: QL_BM_BITS; quicklistBookmark bookmarks[]; 
} quicklist;
```

### 6.3.应用

```
# 栈（先进后出 FILO）
LPUSH + LPOP 
# 或者 
RPUSH + RPOP
# 队列（先进先出 FIFO）
LPUSH + RPOP 
# 或者 
RPUSH + LPOP
# 阻塞队列（blocking queue）
LPUSH + BRPOP 
# 或者 
RPUSH + BLPOP
```

异步消息队列：

操作与队列一样，但是在不同系统间；

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216164216_47102.jpg)

## 7.hash

散列表，在很多高级语言当中包含这种数据结构；c++ unordered_map 通过 key 快速索引value；

### 7.1.基础命令

```
# 获取 key 对应 hash 中的 field 对应的值 
HGET key field 
# 设置 key 对应 hash 中的 field 对应的值 
HSET key field value 
# 设置多个hash键值对 
HMSET key field1 value1 field2 value2 ... fieldn valuen 
# 获取多个field的值 
HMGET key field1 field2 ... fieldn 
# 给 key 对应 hash 中的 field 对应的值加一个整数值 
HINCRBY key field increment 
# 获取 key 对应的 hash 有多少个键值对 
HLEN key 
# 删除 key 对应的 hash 的键值对，该键为field 
HDEL key field
```

### 7.2.存储结构

节点数量大于 512（hash-max-ziplist-entries） 或所有字符串长度大于 64（hash-max-ziplistvalue），则使用 dict 实现；

节点数量小于等于 512 且有一个字符串长度小于 64，则使用 ziplist 实现；

### 7.3.应用

```
# 存储对象
{
    hmset hash:10001 name mark age 18 sex male 
# 与 string 比较 
    set hash:10001 '{["name"]:"mark",["sex"]:"male",["age"]:18}' 
# 假设现在修改 mark的年龄为19岁 
# hash： 
    hset hash:10001 age 19 
# string: 
    get role:10001 
    # 将得到的字符串调用json解密，取出字段，修改 age 值 
    # 再调用json加密 
    set role:10001 '{["name"]:"mark",["sex"]:"male",["age"]:19}'
}
# 购物车
{
# 将用户id作为 key 
# 商品id作为 field 
# 商品数量作为 value 
# 注意：这些物品是按照我们添加顺序来显示的； 
# 添加商品： 
    hset MyCart:10001 40001 1 
    lpush MyItem:10001 40001 
# 增加数量： 
    hincrby MyCart:10001 40001 1 
    hincrby MyCart:10001 40001 -1 # 减少数量1 
# 显示所有物品数量： 
    hlen MyCart:10001 
# 删除商品： 
    hdel MyCart:10001 40001 
    lrem MyItem:10001 1 40001 
# 获取所有物品： 
    lrange MyItem:10001 
# 40001 40002 40003 
    hget MyCart:10001 40001 
    hget MyCart:10001 40002 
    hget MyCart:10001 40003
}
```

## 8.set

集合；用来存储唯一性字段，不要求有序；

### 8.1.基础命令

```
# 添加一个或多个指定的member元素到集合的 key中 
SADD key member [member ...] 
# 计算集合元素个数 
SCARD key 
# 返回key中所有value
SMEMBERS key 
# 返回成员 member 是否是存储的集合 key的成员 
SISMEMBER key member 
# 随机返回key集合中的一个或者多个元素，不删除这些元素 
SRANDMEMBER key [count] 
# 从存储在key的集合中移除并返回一个或多个随机元素 
SPOP key [count] 
# 返回一个集合与给定集合的差集的元素 
SDIFF key [key ...] 
# 返回指定所有的集合的成员的交集 
SINTER key [key ...] 
# 返回给定的多个集合的并集中的所有成员 
SUNION key [key ...
```

### 8.2.存储结构

元素都为整数且节点数量小于等于 512（set-max-intset-entries），则使用整数数组存储；

元素当中有一个不是整数或者节点数量大于 512，则使用字典存储；

### 8.3.应用

```
# 抽奖
{
# 添加抽奖用户 
    sadd Award:1 10001 10002 10003 10004 10005 10006 
    sadd Award:1 10009 
# 查看所有抽奖用户 
    smembers Award:1 
# 抽取多名幸运用户 
    srandmember Award:1 10 
}
# 共同关注
{
    sadd follow:A mark king darren mole vico 
    sadd follow:C mark king darren 
    sinter follow:A follow:C
}
# 推荐好友
{
    sadd follow:A mark king darren mole vico 
    sadd follow:C mark king darren 
    # C可能认识的人： 
    sdiff follow:A follow:C
}
```

## 9.zset

有序集合；用来实现排行榜；它是一个有序唯一；

### 9.1.基础命令

```
# 添加到键为key有序集合（sorted set）里面 
ZADD key [NX|XX] [CH] [INCR] score member [score member ...] 
# 从键为key有序集合中删除 member 的键值对 
ZREM key member [member ...] 
# 返回有序集key中，成员member的score值 
ZSCORE key member 
# 为有序集key的成员member的score值加上增量increment 
ZINCRBY key increment member 
# 返回key的有序集元素个数 
ZCARD key 
# 返回有序集key中成员member的排名 
ZRANK key member 
# 返回存储在有序集合key中的指定范围的元素 order by id limit 1,100 
ZRANGE key start stop [WITHSCORES] 
# 返回有序集key中，指定区间内的成员(逆序) 
ZREVRANGE key start stop [WITHSCORES]
```

### 9.2.存储结构

节点数量大于 128或者有一个字符串长度大于64，则使用跳表（skiplist）；

节点数量小于等于128（zset-max-ziplist-entries）且所有字符串长度小于等于64（zset-maxziplist-value），则使用 ziplist 存储；

数据少的时候，节省空间； O(n)

数量多的时候，访问性能；O（1） o(logn)

### 9.3.应用

```
# 热榜
{
# 点击新闻： 
    zincrby hot:20210601 1 10001 
    zincrby hot:20210601 1 10002 
    zincrby hot:20210601 1 10003 
    zincrby hot:20210601 1 10004 
    zincrby hot:20210601 1 10005 
    zincrby hot:20210601 1 10006 
    zincrby hot:20210601 1 10007 
    zincrby hot:20210601 1 10008 
    zincrby hot:20210601 1 10009 
    zincrby hot:20210601 1 10010 
# 获取排行榜： 
    zrevrange hot:20210601 0 9 withscores
}
# 延时队列
#       将消息序列化成一个字符串作为 zset 的 member；这个消息的到期处理时间作为 score，
# 然后用多个线程轮询 zset 获取到期的任务进行处理。
{
def delay(msg): 
    msg.id = str(uuid.uuid4()) #保证 member 唯一 
    value = json.dumps(msg) 
    retry_ts = time.time() + 5 # 5s后重试 
    redis.zadd("delay-queue", retry_ts, value)
# 使用连接池 
def loop(): 
    while True: 
        values = redis.zrangebyscore("delay-queue", 0, time.time(), start=0, num=1)
        if not values: 
            time.sleep(1) 
            continue 
        value = values[0] 
        success = redis.zrem("delay-queue", value) 
        if success: 
            msg = json.loads(value) 
            handle_msg(msg) 
# 缺点：loop 是多线程竞争，两个线程都从zrangebyscore获取到数据，但是zrem一个成功一个失 败
# 优化：为了避免多余的操作，可以使用lua脚本原子执行这两个命令 
# 解决：漏斗限流
}
# 时间窗口限流
#     系统限定用户的某个行为在指定的时间范围内（动态）只能发生N次；
{
# 指定用户 user_id 的某个行为 action 在特定时间内 period 只允许发生做多的次数 max_count
local function is_action_allowed(red, userid, action, period, max_count) 
    local key = tab_concat({"hist", userid, action}, ":") 
    local now = zv.time() 
    red:init_pipeline() 
    # 记录行为 
    red:zadd(key, now, now) 
    # 移除时间窗口之前的行为记录，剩下的都是时间窗口内的记录 
    red:zremrangebyscore(key, 0, now - period *100) 
    # 获取时间窗口内的行为数量 
    red:zcard(key) 
    # 设置过期时间，避免冷用户持续占用内存 时间窗口的长度+1秒 
    red:expire(key, period + 1) 
    local res = red:commit_pipeline() 
    return res[3] <= max_count
end
# 维护一次时间窗口，将窗口外的记录全部清理掉，只保留窗口内的记录； 
# 缺点：记录了所有时间窗口内的数据，如果这个量很大，不适合做这样的限流；漏斗限流 
# 注意：如果用 key + expire 操作也能实现，但是实现的是熔断，维护时间窗口是限流的功能；
}
```

**分布式定时器：**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216164217_58963.jpg)

生产者将定时任务 hash 到不同的 redis 实体中，为每一个 redis 实体分配一个 dispatcher 进程，用来定时获取 redis 中超时事件并发布到不同的消费者中。

原文链接：https://zhuanlan.zhihu.com/p/504803441

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.495】Linux C/C++ 并发下的技术方案（互斥锁、自旋锁、原子操作）

前言

线程：是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。在Unix System V及SunOS中也被称为轻量进程，但轻量进程更多指内核线程，而把用户线程称为线程。

## 1.为什么要使用锁？

下面是一个主线程count计数的方案，分别通过它的10个子线程进行count的自增。

```
mkdir LOCK    //创建LOCK文件夹
cd LOCK        //转到LOCK目录下
touch Lock.c    //创建Lock.c文件
gcc -o lock Lock.c -lpthread//编译的时候需要用pthread动态库
```

然后这里是Lock.c的代码

```
#include<stdio.h>
#include<pthread.h>
#define THREAD_COUNT   10
void* thread_callback(void* arg) {
    int* pcount = (int*)arg;
    int i = 0;
    while (i++ < 1000000) {
        (*pcount)++;
        usleep(1);
    }
}
int main() {
    pthread_t threadid[THREAD_COUNT] = { 0 };
    int i = 0;
    int count = 0;
    for (i = 0; i < THREAD_COUNT; i++) {
        pthread_create(&threadid[i], NULL, thread_callback, &count);//创建线程（线程id，线程属性，线程入口函数，主线程往子线程传的参数）
    }
    for (i = 0; i < 100; i++) {
        printf("count: %d\n", count);
        sleep(1);
    }
    getchar();
}
```

因为电脑配置不同，所以CPU的处理不同。

如果每个子线程使count自增10万次

最后count值可能等于100万（或者小于100w），

由于我的电脑每个子线程使count自增100万次，才会小于1000万，所以我以每个子线程自增100万次作为演示。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216165028_37903.jpg)

在这里可以发现，明明count应该达到1000万才对，但是由于操作系统进程的切换导致count++这条语句出现了问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216165028_74288.jpg)

这是count++转化为汇编的语句，分为三条，在正常情况下，线程一执行完这三条语句再执行线程二.

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216165029_69604.jpg)

但是异常的情况下，线程一可能三条语句执行不完，就会进行线程二的执行。

导致的结果就是明明应该使count自增2次，但实践上只自增了1次，这样的结果就会导致1000万条数据有所衰减。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216165030_54870.jpg)

所以为了避免这种情况的出现，我们可以使用，互斥锁、自旋锁、原子操作等方法解决这个问题。

## 2.解决方法（互斥锁、自旋锁、原子操作）

### 2.1.互斥锁

代码如下（示例）：

```
#include<stdio.h>
#include<pthread.h>
#define THREAD_COUNT   10
pthread_mutex_t mutex;//添加一个互斥锁
void* thread_callback(void* arg) {
    int* pcount = (int*)arg;
    int i = 0;
    while (i++ < 1000000) {
#if 0
#else
        pthread_mutex_lock(&mutex);//线程调用函数让互斥锁上锁
        (*pcount)++;
        pthread_mutex_unlock(&mutex);//解除互斥锁的锁定
#endif
        usleep(1);
    }
}
int main() {
    pthread_t threadid[THREAD_COUNT] = { 0 };
    pthread_mutex_init(&mutex, NULL);//线程初始化
    int i = 0;
    int count = 0;
    for (i = 0; i < THREAD_COUNT; i++) {
        pthread_create(&threadid[i], NULL, thread_callback, &count);//创建线程（线程id，线程属性，线程入口函数，主线程往子线程传的参数）
    }
    for (i = 0; i < 100; i++) {
        printf("count: %d\n", count);
        sleep(1);
    }
    getchar();
}
```

### 2.2.自旋锁

代码如下（示例）：

```
#include<stdio.h>
#include<pthread.h>
#define THREAD_COUNT   10
pthread_spinlock_t spinlock;//添加一个自旋锁
void* thread_callback(void* arg) {
    int* pcount = (int*)arg;
    int i = 0;
    while (i++ < 1000000) {
#if 0
#else
        pthread_spin_lock(&spinlock);//线程调用函数
        (*pcount)++;
        pthread_spin_unlock(&spinlock);
#endif
        usleep(1);
    }
}
int main() {
    pthread_t threadid[THREAD_COUNT] = { 0 };
    pthread_spin_init(&spinlock, PTHREAD_PROCESS_SHARED);
    int i = 0;
    int count = 0;
    for (i = 0; i < THREAD_COUNT; i++) {
        pthread_create(&threadid[i], NULL, thread_callback, &count);//创建线程（线程id，线程属性，线程入口函数，主线程往子线程传的参数）
    }
    for (i = 0; i < 100; i++) {
        printf("count: %d\n", count);
        sleep(1);
    }
    getchar();
}
```

### 2.3.原子操作

代码如下（示例）：

```
#include<stdio.h>
#include<pthread.h>
#define THREAD_COUNT   10
int inc(int* value, int add) {
    int old;
    __asm__ volatile(        //汇编语句
        "lock; xaddl %2,%1;"    //使 %1的结果为%2+%1
        : "=a" (old)
        : "m" (*value), "a"(add)
        : "cc", "memory"
    );
    return old;
}
void* thread_callback(void* arg) {
    int* pcount = (int*)arg;
    int i = 0;
    while (i++ < 1000000) {
#if 0
#else
        inc(pcount, 1);
#endif
        usleep(1);
    }
}
int main() {
    pthread_t threadid[THREAD_COUNT] = { 0 };
    int i = 0;
    int count = 0;
    for (i = 0; i < THREAD_COUNT; i++) {
        pthread_create(&threadid[i], NULL, thread_callback, &count);//创建线程（线程id，线程属性，线程入口函数，主线程往子线程传的参数）
    }
    for (i = 0; i < 100; i++) {
        printf("count: %d\n", count);
        sleep(1);
    }
    getchar();
}
```

### 2.4.三种方法的比较（个人理解）

自旋锁：线程调用时相当于执行while（1）语句，直到获取锁内内容执行完，才进行下一个线程。

互斥锁：如果当前线程没有执行完，引起线程切换，就会执行下一条线程，当再次回来的时候重新执行。

原子操作：把多条语句合并成一条语句。

自旋锁适合锁的内容很少的时候使用，而互斥锁适合锁的内容较多的时候使用。

## 3.总结

今天通过这个例子，我让大家理解了，主线程和子线程之间可能会发生一些事情，导致程序最后不能达到我们预期的想法，因此就需要通过锁定语句的方法来使程序正常执行。

原文链接：https://zhuanlan.zhihu.com/p/505964220

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.496】Linux服务器开发【干货知识】—MySQL事务原理分析

前言

今天的目标是学习MySQL事务原理分析，但是却似乎总是非常不顺利，概念和实操实在多到令人发指，故干脆轻松学完一节课，等到时机到了再重新刷一遍吧！

### 1.事务是什么？

将数据库从一致性状态转化成另一种一致性状态。

单条语句是隐含得事务，多条语句需要手动开启事务。

### 2.ACID特性是什么？

原子性依靠undolog(共享表空间)实现，记录进行操作，然后进行反向操作。

一致性最难理解，换句话说在事务执行前后，数据库完整性约束不能被破坏。

### 3.隔离级别

mvcc提供了一种快照读的方式提升读的并发性能，通过读历史版本的方式。

个人理解：各个级别其实就像是一种胆大策略，没有对错之分。只要不担心错读数据，就可以选用读不加锁而写加排他锁，依次类推。串行化读取虽然保险，但是效率也是最低，也许是我天生性格的原因，似乎对这种方式情有独钟。人生路慢慢，欲速则不达也。

- 快照读就是不加任何参数。
- 如果是当前读，需要加上lock in share mode 或 for update 加读写锁。
- 查看锁信息select * from information_schema.innodb_locks;

myisam不会发生并发读异常、并发死锁，因为是表锁。所以说当出现问题，试着换一下引擎也许是一个很好的选择。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216165913_23211.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216165914_56724.jpg)

删除、更改需要增加排他锁，而增加插入需要将插入位置加入位置意向锁，然后间接的加入增加排他锁。三个事项加在一起是完整的写操作。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216165914_88571.jpg)

表级别的对写锁才会和意向锁冲突，行级别咱就都不考虑啦。

### 4.Record Lock

记录单个行上的锁。

### 5.Gap Lock

间隙锁，锁定一个范围。只有在可重复读上面才会有间隙锁。

插入意向锁容易造成死锁，如果没有这个锁要一个一个的插入，有了它是为了提升效率。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216165915_27939.jpg)

死锁80%都是上图第二行造成的！

### 6.小计

更改mysql5.7的字符集

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216165916_17067.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216165917_86738.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/16/20221216165917_95247.jpg)

原文链接：https://zhuanlan.zhihu.com/p/507239547

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.497】UDP的可靠性传输详解

### 1.文章目录

**UDP和TCP的区别**

**TCP**

**UDP**

**为什么要使用UDP传输可靠性数据**

**如何使用UDP传输可靠性数据**

**KCP的使用方式**

**kcp配置模式**

**kcp的协议头**

### 2.UDP和TCP的区别

Tcp和udp都是属于TCP/IP协议(传输层协议)。

### 2.TCP

TCP（Transmission Control Protocol，传输控制协议）是面向连接的协议，也就是说，在收发数据前，必须和对方建立可靠的连接。 一个TCP连接必须要经过三次握手，断开连接时需要四次挥手。

TCP的可靠性主要体现在什么方面呢？

**1. 应用数据被分割成TCP认为最合适发送的数据块。**

这个和UDP完全不同，应用程序将产生的数据报长度将保持不变。由TCP传递给IP的信息单位称为报文段或段。最大报文段(MSS)表示TCP传往另一端的最大块数据的长度。连接建立时，双方都需要通告自己的MSS。默认情况下MSS的值为536字节(可以加上20字节的IP首部和20字节的TCP首部)。对于一个以太网，最大的MSS可达到1460字节(1500(MTU) - 20(IP) - 20(TCP))。

**2. 当TCP发出一个段之后，它启动一个定时器，等待目的端确认收到这个报文段，如果不能及时收到一个确认，将重发这个报文段。**

**3. 当TCP收到另一端的数据，他将发送一个确认。这个确认不是立即发送，而是延迟几分之一秒。**

**4. TCP将保持它首部和数据的校验和。这个是一个端到端的检测，目的是检测数据在传输时的任何变化，如果有收到段的检验和有差错，tcp将丢弃这个报文段和不确认收到此报文段。**

**5. 既然tcp报文段作为ip数据报来传输，而ip数据报的到达可能会失序，因此tcp报文段的到达可能会失序。如果必要，tcp将对收到的数据进行重新排序。**

**6. ip数据报会发生重复，tcp的接收端必须丢弃重复的数据。**

**7. TCP提供流量控制。**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217155917_39358.jpg)

### 4.UDP

UDP（UserDatagramProtocol）是一个简单的面向消息的传输层协议，尽管UDP提供标头和有效负载的完整性验证（通过校验和），但它不保证向上层协议提供消息传递，并且UDP层在发送后不会保留UDP 消息的状态。因此，UDP有时被称为不可靠的数据报协议。如果需要传输可靠性，则必须在用户应用程序中实现。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217155917_64794.jpg)

### 5.为什么要使用UDP传输可靠性数据

UDP(User Datagram Protocol)传输与IP传输非常类似，它的传输方式也是”Best Effort”的，所以UDP协议也是不可靠的。我们知道TCP就是为了解决IP层不可靠的传输层协议，既然UDP是不可靠的，为什么不直接使用IP协议而要额外增加一个UDP协议呢？

1. 一个重要的原因是IP协议中并没有端口(port)的概念。IP协议进行的是IP地址到IP地址的传输，这意味者两台计算机之间的对话。但每台计算机中需要有多个通信通道，并将多个通信通道分配给不同的进程使用。一个端口就代表了这样的一个通信通道。UDP协议实现了端口，从而让数据包可以在送到IP地址的基础上，进一步可以送到某个端口。
2. 对于一些简单的通信，我们只需要“Best Effort”式的IP传输就可以了，而不需要TCP协议复杂的建立连接的方式(特别是在早期网络环境中，如果过多的建立TCP连接，会造成很大的网络负担，而UDP协议可以相对快速的处理这些简单通信）
3. 在使用TCP协议传输数据时，如果一个数据段丢失或者接收端对某个数据段没有确认，发送端会重新发送该数据段。TCP重新发送数据会带来传输延迟和重复数据，降低了用户的体验。对于迟延敏感的应用，少量的数据丢失一般可以被忽略，这时使用UDP传输将能够提升用户的体验。

UDP将数据从源端发送到目的端时，无需事先建立连接，没有使用TCP中的确认技术或滑动窗口机制，因此UDP不能保证数据传输的可靠性，也无法避免接收到重复数据的情况。

UDP传输的可靠性由应用层负责，由应用程序根据需要提供报文ACK机制、重传机制、序号机制、重排机制和窗口机制。这些TCP已经都具备了。

### 6.如何使用UDP传输可靠性数据

**KCP主要的优势体现在以下方面:**

1. 以10%-20%带宽浪费的代价换取了比 TCP快30%-40%的传输速度。
2. RTO翻倍vs不翻倍：TCP超时计算是RTOx2，这样连续丢三次包就变成RTOx8了，十分恐怖，而KCP启动快速模式后不x2，只是x1.5（实验证明1.5这个值相对比较好），提高了传输速度。
3. 选择性重传 vs 全部重传: TCP丢包时会全部重传从丢的那个包开始以后的数据， KCP是选择性重传，只重传真正丢失的数据包。
4. 快速重传（跳过多少个包马上重传）（如果使用了快速重传，可以不考虑RTO））
5. 发送端发送了1,2,3,4,5几个包，然后收到远端的ACK: 1, 3, 4, 5，当收到ACK3时， KCP知道2被跳过1次，收到ACK4时，知道2被跳过了2次，此时可以认为2号丢失，不用等超时，直接重传2号包，大大改善了丢包时的传输速度。
6. UNA vs ACK+UNA：ARQ模型响应有两种， UNA（此编号前所有包已收到，如TCP）和ACK（该编号包已收到），光用UNA将导致全部重传，光用ACK则丢失成本太高，以往协议都是二选其一，而 KCP协议中， 除去单独的 ACK包外，所有包都有UNA信息。
7. 非退让流控：KCP正常模式同TCP一样使用公平退让法则，即发送窗口大小由：发送缓存大小、接收端剩余接收缓存大小、丢包退让及慢启动这四要素决定。但传送及时性要求很高的小数据时，可选择通过配置跳过后两步，仅用前两项来控制发送频率。以牺牲部分公平性及带宽利用率之代价，换取了开着BT都能流畅传输的效果。

**名词说明：**

用户数据：应用层发送的数据，如一张图片2Kb的数据

MTU：最大传输单元。即每次发送的最大数据

RTO： Retransmission TimeOut，重传超时时间。

cwnd:congestion window，拥塞窗口，表示发送方可发送多少个KCP数据包。

与接收方窗口有关，与网络状况（拥塞控制）有关，与发送窗口大小有关。

rwnd:receiver window,接收方窗口大小，表示接收方还可接收多少个KCP数据包

snd_queue:待发送KCP数据包队列

snd_nxt:下一个即将发送的kcp数据包序列号

snd_una:下一个待确认的序列号

### 7.KCP的使用方式

1. 创建 KCP对象： ikcpcb *kcp = ikcp_create(conv, user);
2. 设置传输回调函数（如UDP的send函数）： kcp->output = udp_output;
3. 真正发送数据需要调用sendto
4. 循环调用 update： ikcp_update(kcp, millisec);
5. 输入一个应用层数据包（如UDP收到的数据包） ：ikcp_input(kcp,received_udp_packet,received_udp_size);
6. 我们要使用recvfrom接收，然后扔到kcp里面做解析
7. 发送数据： ikcp_send(kcp1, buffer, 8); 用户层接口
8. 接收数据： hr = ikcp_recv(kcp2, buffer, 10);

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217155918_97012.jpg)

### 8.kcp配置模式

1.工作模式： int ikcp_nodelay(ikcpcb *kcp, int nodelay, int interval, int resend, int nc)

- nodelay ：是否启用 nodelay模式， 0不启用； 1启用。
- interval ：协议内部工作的 interval，单位毫秒，比如 10ms或者 20ms
- resend ：快速重传模式，默认0关闭，可以设置2（2次ACK跨越将会直接重传）
- nc ：是否关闭流控，默认是0代表不关闭， 1代表关闭。
- 普通模式： ikcp_nodelay(kcp, 0, 40, 0, 0);
- 极速模式： ikcp_nodelay(kcp, 1, 10, 2, 1)

2.最大窗口： int ikcp_wndsize(ikcpcb *kcp, int sndwnd, int rcvwnd);

该调用将会设置协议的最大发送窗口和最大接收窗口大小，默认为32，单位为包。

3.最大传输单元： int ikcp_setmtu(ikcpcb *kcp, int mtu);

kcp协议并不负责探测 MTU，默认 mtu是1400字节

4.最小RTO：不管是 TCP还是 KCP计算 RTO时都有最小 RTO的限制，即便计算出来RTO为

40ms，由于默认的 RTO是100ms，协议只有在100ms后才能检测到丢包，快速模式下为

30ms，可以手动更改该值： kcp->rx_minrto = 10;

### 9.kcp的协议头

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217155919_81231.jpg)

conv:连接号。 UDP是无连接的， conv用于表示来自于哪个客户端。对连接的一种替代

cmd:命令字。如， IKCP_CMD_ACK确认命令，

IKCP_CMD_WASK接收窗口大小询问命令，

IKCP_CMD_WINS接收窗口大小告知命令，

frg:分片，用户数据可能会被分成多个KCP包，发送出去

wnd:接收窗口大小，发送方的发送窗口不能超过接收方给出的数值

ts:时间序列

sn:序列号

una:下一个可接收的序列号。其实就是确认号，收到

sn=10的包， una为11

len：数据长度

data:用户数据

原文链接：https://zhuanlan.zhihu.com/p/510221511

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.498】DPDK系统学习—DPDK的虚拟交换机框架 OvS

## 1.目录：

**多队列网卡**

1. **多队列网卡硬件实现**
2. **内核对多队列网卡的支持**
3. **多队列网卡的结构**
4. **DPDK 与多队列网卡**

**虚拟化**

1. **CPU 虚拟化**
2. **内存虚拟化**
3. **I/O 虚拟化**

**Virtio**

1. **为什么是 virtio?**

## 2.多队列网卡

### 2.1.多队列网卡硬件实现

有四个硬件队列(Queue0, Queue1, Queue2, Queue3),当收到报文时,通过 hash 包头

的(sip, sport, dip, dport)四元组,将一条流总是收到相同队列,同时触发与该队列绑定的中断。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160918_30622.jpg)

### 2.2.内核对多队列网卡的支持

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160919_25103.jpg)

Linux 内核中, RPS ( Receive Packet Steering )在接收端提供了这样的机制。RPS 主要是把软中断的负载均衡到 CPU 的各个 core 上,网卡驱动对每个流生成一个 hash 标识,这个 hash值可以通过四元组(源 IP 地址 SIP ,源四层端口 SPOR T,目的 IP 地址 DIP ,目的四层端口DPORT )来计算,然后由中断处理的地方根据这个 hash 标识分配到相应的 core 上去,这样就可以比较充分地发挥多核的能力了。

NAPI 是 linux 上采用一种提高网络处理效率的技术,其核心概念就是不采用中断的方式读取数据,取而代之是采用中断唤醒数据接收的服务程序,然后 POLL 的方式轮询数据。

### 2.3.NAPI 的优点:

1. 中断缓和,在日常使用中,网卡产生的高达几 k/s,每次中断都需要系统来处理,是一个很大的压力,而 NAPI 使用轮询是禁止了网卡接收中断,减小处理中断的压力。
2. 数据包节流,NAPI 之前的 NIC 总在接收数据包之后产生一个 IRQ,接着在中断服务函数将 skb 加入本地 softnet,然后触发本地 NET_RX_SOFTIRQ 软中断后续处理。如果包速过高,因为 IRQ 的优先级高于 SoftIRQ,导致系统的大部分资源都在响应中断,但 softnet 的队列大小有限,接收到的超额数据包也只能丢掉,所以这时这个模型是在用宝贵的系统资源做无用功。而 NAPI 则在这样的情况下,直接把包丢掉,不会继续将需要丢掉的数据包扔给内核去处理,这样,网卡将需要丢掉的数据包尽可能的早丢弃掉,内核将不可见需要丢掉的数据包,这样也减少了内核的压力。

### 2.4.NAPI 的缺点:

1. 对于上层的应用程序而言,系统不能在每个数据包接收到的时候都可以及时地去处理它,而且随着传输速度增加,累计的数据包将会耗费大量的内存。
2. 另外一个问题是对于大的数据包处理比较困难,原因是大的数据包传送到网络层上的时候耗费的时间比短数据包长很多(即使是采用 DMA 方式)。所以,NAPI 技术适用于对高速率的短长度数据包的处理。

QDisc 是 queueing discipline 的简写,它是理解流量(traffic control)的基础。无论何时,内核如果需要通过某个网络接口发送数据包,需要按照这个接口配置的 qdisc(排队规则)把数据包加入队列。然后,内核会尽可能多的从 qdisc 里面取出数据包,把它们交给网络适配器模块。最简单的 QDisc 是 pfifo,他不对进入的数据包做任何处理,数据包采用先入先出的方式。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160919_37166.jpg)

### 2.5.多队列网卡的结构

Linux 内核的网卡结构体是由 net_device 表示,数据包是由 sk_buff 表示的

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160920_94455.jpg)

### 2.6.接收端

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160921_58741.jpg)

### 2.7.发送端

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160922_42057.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160922_38344.jpg)

### 2.8.DPDK 与多队列网卡

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160923_88685.jpg)

## 3.虚拟化

虚拟化是资源的逻辑表示,不受物理设备的约束。虚拟化技术的实现形式是在系统中加入一个虚拟化层,虚拟化层将下层的资源抽象成另一种形式的资源,提供给上层使用。

### 3.1.CPU 虚拟化

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160924_53198.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160925_55526.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160925_90325.jpg)

### 3.2.内存虚拟化

虚拟机本质上是 Host 机上的一个进程,按理说应该可以使用 Host 机的虚拟地址空间,但由于在虚拟化模式下,虚拟机处于非 Root 模式,无法直接访问 Root 模式下的 Host 机上的内存。

这个时候就需要 VMM 的介入, VMM 需要 intercept (截获)虚拟机的内存访问指令,然后 virtualize(模拟)Host 上的内存,相当于 VMM 在虚拟机的虚拟地址空间和 Host 机的虚拟地址空间中间增加了一层,即虚拟机的物理地址空间,也可以看作是 Qemu 的虚拟地址空间(稍微有点绕,但记住一点,虚拟机是由 Qemu 模拟生成的就比较清楚了)。所以,内存软件虚拟化的目标就是要将虚拟机的虚拟地址(Guest Virtual Address, GVA)转化

为 Host 的物理地址(Host Physical Address, HPA),中间要经过虚拟机的物理地址(GuestPhysical Address, GPA)和 Host 虚拟地址(Host Virtual Address)的转化,即:

GVA -> GPA -> HVA -> HPA

其中前两步由虚拟机的系统页表完成,中间两步由 VMM 定义的映射表(由数据结构kvm_memory_slot 记录)完成,它可以将连续的虚拟机物理地址映射成非连续的 Host 机虚拟地址,后面两步则由 Host 机的系统页表完成。如下图所示。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160926_17253.jpg)

### 3.3.**这样做得目的有两个:**

1. 提供给虚拟机一个从零开始的连续的物理内存空间。
2. 在各虚拟机之间有效隔离、调度以及共享内存资源。

### 3.4.影子页表技术

接上图,我们可以看到,传统的内存虚拟化方式,虚拟机的每次内存访问都需要 VMM 介入,并由软件进行多次地址转换,其效率是非常低的。因此才有了影子页表技术和 EPT 技术。

影子页表简化了地址转换的过程,实现了 Guest 虚拟地址空间到 Host 物理地址空间的直接映射。要实现这样的映射,必须为 Guest 的系统页表设计一套对应的影子页表,然后将影子页表装入 Host 的 MMU 中,这样当 Guest 访问 Host 内存时,就可以根据 MMU 中的影子页表映射关系,完成 GVA 到 HPA 的直接映射。而维护这套影子页表的工作则由 VMM 来完成。

由于 Guest 中的每个进程都有自己的虚拟地址空间,这就意味着 VMM 要为 Guest 中的每个进程页表都维护一套对应的影子页表,当 Guest 进程访问内存时,才将该进程的影子页表装入 Host 的 MMU 中,完成地址转 换。

我们也看到,这种方式虽然减少了地址转换的次数,但本质上还是纯软件实现的,效率还是不高,而且 VMM 承担了太多影子页表的维护工作,设计不好。

为了改善这个问题,就提出了基于硬件的内存虚拟化方式,将这些繁琐的工作都交给硬件来完成,从而大大提高了效率。

### 3.5.EPT 技术

这方面 Intel 和 AMD 走在了最前面,Intel 的 EPT 和 AMD 的 NPT 是硬件辅助内存虚拟化的代表,两者在原理上类似,本文重点介绍一下 EPT 技术。

如下图是 EPT 的基本原理图示,EPT 在原有 CR3 页表地址映射的基础上,引入了 EPT 页表来实现另一层映射,这样,GVA->GPA->HPA 的两次地址转换都由硬件来完成

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160927_53150.jpg)

这里举一个小例子来说明整个地址转换的过程。假设现在 Guest 中某个进程需要访问内存,CPU 首先会访问 Guest 中的 CR3 页表来完成 GVA 到 GPA 的转换,如果 GPA 不为空,则 CPU 接着通过 EPT 页表来实现 GPA 到 HPA 的转换(实际上,CPU 会首先查看硬件 EPT TLB 或者缓存,如果没有对应的转换,才会进一步查看 EPT 页表),如果 HPA 为空呢,则 CPU 会抛出 EPT Violation 异常由 VMM 来处理。如果 GPA 地址为空,即缺页,则 CPU 产生缺页异常,注意,这里,如果是软件实现的方式,则会产生 VM-exit,但是硬件实现方式,并不会发生 VM-exit,而是按照一般的缺页中断处理,这种情况下,也就 是 交 给 Guest 内 核 的 中 断 处 理 程 序 处 理 。 在 中 断 处 理 程 序 中 会 产 生EXIT_REASON_EPT_VIOLATION,Guest 退出,VMM 截获到该异常后,分配物理地址并建立GVA 到 HPA 的映射,并保存到 EPT 中,这样在下次访问的时候就可以完成从 GVA 到HPA 的转换了。

### 3.6.I/O 虚拟化

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160928_42439.jpg)

**I/O 全虚拟化(上图中间部分)**

这种方式比较好理解,简单来说,就是通过纯软件的形式来模拟虚拟机的 I/O 请求。以qemu-kvm 来举例,内核中的 kvm 模块负责截获 I/O 请求,然后通过事件通知告知给用户空间的设备模型 qemu,qemu 负责完成本次 I/O 请求的模拟。

优点:不需要对操作系统做修改,也不需要改驱动程序,因此这种方式对于多种虚拟化技术的「可移植性」和「兼容性」比较好。

缺点:纯软件形式模拟,自然性能不高,另外,虚拟机发出的 I/O 请求需要虚拟机和 VMM之间的多次交互,产生大量的上下文切换,造成巨大的开销。

**I/O 半虚拟化(上图左侧)**

针对 I/O 全虚拟化纯软件模拟性能不高这一点, I/O 半虚拟化前进了一步。它提供了一种机制,使得 Guest 端与 Host 端可以建立连接,直接通信,摒弃了截获模拟这种方式,从而获得较高的性能。

值得注意的有两点:1)采用 I/O 环机制,使得 Guest 端和 Host 端可以共享内存,减少了虚拟机与 VMM 之间的交互;2)采用事件和回调的机制来实现 Guest 与 Host VMM 之间的通信。这样,在进行中断处理时,就可以直接采用事件和回调机制,无需进行上下文切换,减少了开销。

要实现这种方式, Guest 端和 Host 端需要采用类似于 C/S 的通信方式建立连接,这也就意味着要修改 Guest 和 Host 端操作系统内核相应的代码,使之满足这样的要求。为了描述方便,我们统称 Guest 端为前端,Host 端为后端。

前后端通常采用的实现方式是驱动的方式,即前后端分别构建通信的驱动模块,前端实现在内核的驱动程序中,后端实现在 qemu 中,然后前后端之间采用共享内存的方式传递数据。关于这方面一个比较好的开源实现是 virtio

**优点:**性能较 I/O 全虚拟化有了较大的提升

**缺点:**要修改操作系统内核以及驱动程序,因此会存在移植性和适用性方面的问题,导致其使用受限。

**I/O 直通或透传技术(上图右侧)**

上面两种虚拟化方式,还是从软件层面上来实现,性能自然不会太高。最好的提高性能的方式还是从硬件上来解决。如果让虚拟机独占一个物理设备,像宿主机一样使用物理设备,那无疑性能是最好的。

I/O 直通技术就是提出来完成这样一件事的。它通过硬件的辅助可以让虚拟机直接访问物理设备,而不需要通过 VMM 或被 VMM 所截获。

由于多个虚拟机直接访问物理设备,会涉及到内存的访问,而内存又是共享的,那怎么来隔离各个虚拟机对内存的访问呢,这里就要用到一门技术——IOMMU,简单说,IOMMU就是用来隔离虚拟机对内存资源访问的。

I/O 直通技术需要硬件支持才能完成,这方面首选是 Intel 的 VT-d 技术,它通过对芯片级的改造来达到这样的要求,这种方式固然对性能有着质的提升,不需要修改操作系统,移植性也好。但该方式也是有一定限制的,这种方式仅限于物理资源丰富的机器,因为这种方式仅仅能满足一个设备分配给一个虚拟机,一旦一个设备被虚拟机占用了,其他虚拟机时无法使用该设备的。

## 4.Virtio

virtio 是一种 I/O 半虚拟化解决方案,是一套通用 I/O 设备虚拟化的程序,是对半虚拟 化 Hypervisor 中 的 一 组 通 用 I/O 设 备 的 抽 象 。 提 供 了 一 套 上 层 应 用 与 各Hypervisor 虚拟化设备(KVM,Xen,VMware 等)之间的通信框架和编程接口,减少跨平台所带来的兼容性问题,大大提高驱动程序开发效率。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160928_83325.jpg)

### 4.1.为什么是 virtio?

在完全虚拟化的解决方案中,guest VM 要使用底层 host 资源,需要 Hypervisor 来截获所有的请求指令,然后模拟出这些指令的行为,这样势必会带来很多性能上的开销。半虚拟化通过底层硬件辅助的方式,将部分没必要虚拟化的指令通过硬件来完成,Hypervisor只负责完成部分指令的虚拟化,要做到这点,需要 guest 来配合,guest 完成不同设备的前端驱动程序,Hypervisor 配合 guest 完成相应的后端驱动程序,这样两者之间通过某种交互机制就可以实现高效的虚拟化过程。

由于不同 guest 前端设备其工作逻辑大同小异(如块设备、网络设备、 PCI 设备、 balloon驱动等),单独为每个设备定义一套接口实属没有必要,而且还要考虑扩平台的兼容性问题,另外,不同后端 Hypervisor 的实现方式也大同小异(如 KVM、Xen 等),这个时候,就需要一套通用框架和标准接口(协议)来完成两者之间的交互过程,virtio 就是这样一套标准,它极大地解决了这些不通用的问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160929_64507.jpg)

### 4.2.virtio 的架构

从总体上看,virtio 可以分为四层,包括前端 guest 中各种驱动程序模块,后端 Hypervisor(实现在 Qemu 上)上的处理程序模块,中间用于前后端通信的 virtio 层和 virtio-ring 层,virtio 这一层实现的是虚拟队列接口,算是前后端通信的桥梁,而 virtio-ring 则是该桥梁的具体实现,它实现了两个环形缓冲区,分别用于保存前端驱动程序和后端处理程序执行的信息。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160930_43552.jpg)

严格来说, virtio 和 virtio-ring 可以看做是一层, virtio-ring 实现了 virtio 的具体通信机制和数据流程。或者这么理解可能更好, virtio 层属于控制层,负责前后端之间的通知机制(kick,notify)和控制流程,而 virtio-vring 则负责具体数据流转发。

### 4.3.virtio 数据流交互机制

vring 主要通过两个环形缓冲区来完成数据流的转发,如下图所示。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160930_77223.jpg)

vring 包含三个部分,描述符数组 desc,可用的 available ring 和使用过的 usedring。

desc 用于存储一些关联的描述符,每个描述符记录一个对 buffer 的描述,available ring 则用于 guest 端表示当前有哪些描述符是可用的,而 used ring 则表示 host 端哪些描述符已经被使用。

Virtio 使用 virtqueue 来实现 I/O 机制,每个 virtqueue 就是一个承载大量数据的队列,具体使用多少个队列取决于需求,例如,virtio 网络驱动程序(virtio-net)使用两个队列(一个用于接受,另一个用于发送),而 virtio 块驱动程序(virtio-blk)仅使用一个队列。

具体的,假设 guest 要向 host 发送数据,首先,guest 通过函数 virtqueue_add_buf将存有数据的 buffer 添加到 virtqueue 中,然后调用 virtqueue_kick 函数,virtqueue_kick 调用 virtqueue_notify 函数,通过写入寄存器的方式来通知到 host。

host 调用 virtqueue_get_buf 来获取 virtqueue 中收到的数据。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160931_68216.jpg)

存放数据的 buffer 是一种分散-聚集的数组,由 desc 结构来承载,如下是一种常用的 desc的结构:

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160932_12199.jpg)

当 guest 向 virtqueue 中写数据时,实际上是向 desc 结构指向的 buffer 中填充数据,完了会更新 available ring,然后再通知 host。当 host 收到接收数据的通知时,首先从 desc 指向的 buffer 中找到 available ring 中添加的 buffer,映射内存,同时更新 used ring,并通知 guest 接收数据完毕。

virtio 是 guest 与 host 之间通信的润滑剂,提供了一套通用框架和标准接口或协议来完成两者之间的交互过程,极大地解决了各种驱动程序和不同虚拟化解决方案之间的适配问题。virtio 抽象了一套 vring 接口来完成 guest 和 host 之间的数据收发过程,结构新颖,接口清晰。

### 4.4.Vhost

vhost 是 virtio 的一种后端实现方案,在 virtio 简介中,我们已经提到 virtio 是一种半虚拟化的实现方案,需要虚拟机端和主机端都提供驱动才能完成通信,通常, virtio主机端的驱动是实现在用户空间的 qemu 中,而 vhost 是实现在内核中,是内核的一个模块 vhost-net.ko。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160932_14819.jpg)

### 4.5.为什么要用 vhost

在 virtio 的机制中,guest 与 用户空间的 Hypervisor 通信,会造成多次的数据拷贝和 CPU 特权级的上下文切换。例如 guest 发包给外部网络,首先,guest 需要切换到host kernel,然后 host kernel 会切换到 qemu 来处理 guest 的请求, Hypervisor通过系统调用将数据包发送到外部网络后,会切换回 host kernel , 最后再切换回guest。这样漫长的路径无疑会带来性能上的损失。

vhost 正是在这样的背景下提出的一种改善方案,它是位于 host kernel 的一个模块,用于和 guest 直接通信,数据交换直接在 guest 和 host kernel 之间通过 virtqueue来进行,qemu 不参与通信,但也没有完全退出舞台,它还要负责一些控制层面的事情,比如和 KVM 之间的控制指令的下发等。

### 4.6.vhost 的数据流程

下图左半部分是 vhost 负责将数据发往外部网络的过程, 右半部分是 vhost 大概的数据交互流程图。其中,qemu 还是需要负责 virtio 设备的适配模拟,负责用户空间某些管理控制事件的处理,而 vhost 实现较为纯净,以一个独立的模块完成 guest 和 host kernel 的数据交换过程。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217160933_18122.jpg)

vhost 与 virtio 前端的通信主要采用一种事件驱动 eventfd 的机制来实现,guest 通知 vhost 的事件要借助 kvm.ko 模块来完成,vhost 初始化期间,会启动一个工作线程work 来监听 eventfd,一旦 guest 发出对 vhost 的 kick event,kvm.ko 触发ioeventfd 通知到 vhost,vhost 通过 virtqueue 的 avail ring 获取数据,并设置used ring。同样,从 vhost 工作线程向 guest 通信时,也采用同样的机制,只不过这种情况发的是一个回调的 call envent,kvm.ko 触发 irqfd 通知 guest。

原文链接：https://zhuanlan.zhihu.com/p/510739584

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.499】后台开发【一大波干货知识】Nginx数据结构剖析

### 1.Nginx数据结构

就是nginx源码里面该怎么去看里面有哪些东西？

核心的第一点就是把基础组建这一块，就是把我们在nginx源码里面的一些数据结构，得需要捋一遍。

数据结构这里面包含哪些东西，，就现在凭你自己现在自己，你认为做一款开源的项目，这里面有哪些组件是我们有必要要了解的，

第一个大家可以想象，首先对于字符串的处理字符的处理，这个肯定是有的。

第二个对于内存的处理我们也是需要有的，

我们对于文件的操作也是需要有的，就是对于这索引hash它是需要有的，还有类似于红黑树也是需要有，还有呢比如共享内存也是要有的，链表也是要有的，消息队列也是要有的。

还有呢比如说进程间通信的组件也是要有

可以看到在我们任何1个项目里面，就是在任何1个项目里面，你会发现这些组件它是可以通通拿过来又可以去复用的，

应用到其他的项目里面也是可以的，因为其他项目里面这些组件也会有，你经历过几个项目之后，如果你自己的技术有点追求，这里面有很多标准，但是很多时候你换其他地方读也是ok，因为这做法不同做法就会有些差异，

就是这些小组件，然后对应到我们的项目中间，它就是整个一个一个的轮子

nginx所有的数据结构划分出来，有这么几个部分所组成，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161601_24749.jpg)

第一个部分是说的基础的数据结构，这里面呢大家可以看到什么叫基础，就对于数据类型这种叫基础，比如int的定义，

然后还有高级点的，比如队列数组红黑树哈希

一些进程间通信的组件，包括共享内存，原子操作，自旋锁然后包括通道，信号，

但是在面试的时候它不仅仅局限于要你会用，而很多的时候会问到里面它是怎么实现的，其他的方法是怎么实现的，

这叫分为4类，还有最后一类就是基础组件式的，内存池，线程池，slab

sizeof(int)这个地方它就会有点小差异，这个差异在哪？

就很容易比如说我们在都是以32位这种心态去写的，这时候他数据不会丢失，请注意这时候这个long型占了8个字节，这8个字节中间它就会有一些空档的区域，有一些空隙的空间，从而造成一些数据我们是没有意义的，造成一些占的空间会浪费，这个时候出于这么一个问题，

所以在这种情况里面有了这么一个定义intptr，这个东西是为了去兼容这个32位和64位这个int

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161602_21793.jpg)

第二个概念就是个string，

这个东西啊我想问一下大家有没有对字符号进行处理过，有没有对字符号进行操作过没有？在做字符串操作的时候，你认为做这个最不爽的一点是？

比如说第一个问题是越界，越界的问题就是因为它不带长度，所以越界，结尾标识符的它也是由于我们不带长度所引起的，还有内存分配，就这两点是非常不爽的问题，

一个就是对内存分配，比如说我们在数据中间进行一些操作的时候，各位你要发现我们在操作之前，我们可能对它先得需要分配一块内存。

第一个对于越界的问题我们可以加长度，这是第一个。

第二个对于内存分配的时候，我们可以这样做，但是这是有一个前提的，你有这个客观条件才可以操作，你每次拿数据的时候，对于字符串操作的时候，

首先我们引入一个内存池，也是我们分配这种数据直接在内存池里面，

就是在满足这两个条件的基础上面，第一个内存先分配，就是这个内存我们已经分配好了，我们不用担心内存越界问题。

第二个至于它使用多长，我们是有一个长度的一个计算的，满足这两点我们可以使用柔性数组，也叫做零长数组，什么意思？我们就可以这么定义，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161602_21452.png)

加了一个零长数组，每一次分配的时候这个 a,我们只是打个标签

这个结构体是4个字节，

4个字节就是我们定义的这个结构体，然后再紧接着这个 a我们这个标识指向的是这个地方，那紧接着它下面开始就可以存储我们字符。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161603_30208.jpg)

那这个字符有多长呢我们通过len来控制，那是不是发现这个做法是挺好的？

但是它中间它有一个缺陷，我们这个计算是可以的，但是请注意不利于我们的printf（），

适合我们的数据把它存储起来，不能把它打印出来，这个东西在使用的时候它有一个局限性，这是一个版本。

第二个版本，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161603_23986.jpg)

这个我们用的是一个字符，用的是一个指针指向的位置，

多占了一个指针的长度，这个指针不利于我们去修改这个数据，但我们可以指向另外的空间，

就什么意思？

我们是可以不连续我们也是ok的，我们是可以把这个data任意的指向任意一个地方，就是说我们在使用的时候，我们可以指向下一块内存不连到一起，这个也是ok的，我们可以指向任意的地方，这两者之间的差异

所以这时候，这个问题为什么nginx这地方不用柔性数组？

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161604_78331.jpg)

可以看到，比如说看到这个宏，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161605_18211.png)

这个东西这个做法上面可能在一开始看的时候，不是那么能理解，就举个例子，比如说我们一个函数

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161605_79386.jpg)

这个操作上面可不可以？

这个当他一返回的时候他是没有用的，在这里跟大家讲了一个前提，首先这里面要么直接传一个常量进去，要么就是直接在内存池中间去分配的一个string，在内存池中间去分配的，也就在堆上分配，然后一起去释放，那两种情况才是ok的，所以对于一些常用的数据结构，是有自己的一些场景。

首先第一个点就是这个地方为什么既然它是在内存中间的，**为什么这东西它不能用柔性数组，**之前说过柔性数组用在它的内存是先于分配的，第二的话呢长度是我们可以通过另外的渠道拿到的，它使用时候，它的常量改变就不那么方便。

那接着再给大家讲一个比较复杂的东西叫list。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161606_88398.jpg)

有这么一个场景，GPS上传，有一针一传，一针一传，一针一次，就会发现这时候很容易耗电，所以就出现一个现象，在终端的时候在设备上的时候就把数据给存起来，可能到5分钟的时候再发一次，或者10分钟的时候再发一次，这个数据先在本地先存n帧，然后存完之后5分钟再全部发一次，中间多一些少一些无所谓，

那我们再来分析一下，就是把n个GPS的数据我们要传到服务端，

这边比如说我们这个 n等于20，一次传20帧，那在这传的时候相当于是一个数组，相当于有20个数据，问一下在服务端我们去接收这20个数据的时候，我们用什么数据结构比较好？

就是一个经度一个维度，再加上一个再加上一个n乘以它，这个接收完了之后只要解析这个n我们就知道它有多少帧

那现在这个数组我们用什么数组？

这 N它是一个变化的值，首先这个数组我们是不能够直接去定一个固定的值

我大方我直接定一个1024的数字，然后要他一直存是不是？可以，这个当然没问题，如果他有一次超过1024，我就问一下你这个数字怎么搞？

第一个数组长度固定，因为这个n它是不固定的，

这个数组不合适，

还有一个前提，我们如果有了内存池的前提，对于这个问题我们该怎么做？

这里面它前提是有一个内存池的情况下面，vector这个东西可能我们就不太利于在我们已有的这个内存池的情况再去往里面去分配，也就是这个 vector里面定义是我们很难把它分配到内存池里面。

我们每一帧每一帧数据，这个要给大家引入一个东西，我们可以这样子在一开始的时候我们定一个链表，这个链表有点特殊

跟大家讲的这个场景，我们就比较的适合ngx_list_t。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161607_70753.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161608_49876.jpg)

第一个就说的这个链表本身

就是它，在紧接着是每个链表里面的节点，请注意这个链表和链表的节点不是一个东西，

它总归一起有两个东西组成，一个就是链表本身，第二个就是每个链表的节点是什么。

还有一个可选的东西，就是中间的内存这节点*elts，指向的内存块是什么地方，

nelts这个值是用来做什么的？

这项就是用来描述这个分配的这个 n里面，这中间比如说这个123456块里面中间有多少个已经被使用。

比如说这里面已经使用了4个，那这个n那就等于4，这就是关于这个结构体的组成。

这个传输的过程这是网络做的，也就是当我们去读出来的这个数据，我们存到哪里，我们如何去把更好的存储

接下再问到一个问题，既然这个里面使用了一个池使用了一个内存池，作用就是防止这些数据会有碎片的出现，那想问他这个问题，那这里面碎片多少，这里还是个碎片。这个池它的作用在哪里？既然它这么多的碎片。

在这里只是图，这个图是这么画的，核心来说还是一个整块，这么多小块其实就是一整块的内存。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161608_70269.jpg)

这还只是一个轮廓，他要提供多少个方法，我们如何去把它用起来，

提供方法，首先第一个就是刚刚的那个结构体的定义，然后还有一些我们需要实现一些函数来操作这些

我们到底要不要改，以及我们到底要不要删，或者我们到底要不要查，或者说我们该怎么去增加

核心来说就提供了一个方法

我们先来看一下这里面到底需不需要增删改查

首先第一个

我们增加一个节点，或者我们有没有删除一个节点，或者说我们有没有去修改的情况，删除比较麻烦好

第一个增加肯定是要有的，不然怎么添这个节点。

第二个对于这个数据结构，我们有没有必要去删除？

这个删除我们有没有意义，我们可以用整个内存池一起去回收。

它的使用场景就是，每一个连接在连接的时候，我们建立一个内存池，在连接释放的时候，整个内存是销毁，那这个删除有没有必要？

第三个有没有可能修改，有没有可能修改？

以及修改我们该怎么去修改，那还有一个就是查找有没有要查找的可能性

它是不带索引的功能，也就是在这个茫茫的这种n多的这种数据存储进去之后，我们的查找方法只能通过遍历，而且这种查找方法效率极低，它的查找是没有意义的

只要一个增加就可以了

对于这个链表操作的方法只有一个就是ngx_list_push,

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161609_74029.jpg)

关于这个push才真的能够领略它设计的整个优势，

```
//gcc -o ngx_list_main ngx_list_main.c -I ./nginx-1.14.2/src/core/ -I ./nginx-1.14.2/objs/  -I ./nginx-1.14.2/src/os/unix/ -I ./pcre-8.41/ -I ./nginx-1.14.2/src/event/ ./nginx-1.14.2/objs/src/core/ngx_list.o ./nginx-1.14.2/objs/src/core/ngx_string.o ./nginx-1.14.2/objs/src/core/ngx_palloc.o ./nginx-1.14.2/objs/src/os/unix/ngx_alloc.o
#include <stdio.h>
#include <string.h>
#include "ngx_config.h"
#include "ngx_core.h"
#include "ngx_list.h"
#include "ngx_palloc.h"
#include "ngx_string.h"
#define N    10
volatile ngx_cycle_t *ngx_cycle;
void ngx_log_error_core(ngx_uint_t level, ngx_log_t *log,
            ngx_err_t err, const char *fmt, ...)
{
}
void print_list(ngx_list_t *l) {
    ngx_list_part_t *p = &(l->part);
    while (p) {
        int i = 0;
        for (i = 0;i < p->nelts;i ++) {
            printf("%s\n", (char*)(((ngx_str_t*)p->elts + i)->data));
        }
        p = p->next;
        printf(" -------------------------- \n");
    }
}
int main() {
    ngx_pool_t *pool = ngx_create_pool(1024, NULL);
    ngx_list_t *l = ngx_list_create(pool, N, sizeof(ngx_str_t));
    int i = 0;
    for (i = 0;i < 24;i ++) {
        ngx_str_t *ptr = ngx_list_push(l);//返回是精髓，返回待写的
        char *buf = ngx_palloc(pool, 32);
        sprintf(buf, "King %d", i+1);
        ptr->len = strlen(buf);
        ptr->data = buf;
    }
    print_list(l);
}
```

这个 table的东西呢它就4项，它是一个非常非常简单的东西

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161610_72257.jpg)

可以看到第一项哈希,第二项key,第三项value，第四项lowcase_key就是小写的key，这个什么意思？

这个数据结构呢就是单纯的就是为了这个而生的,就是为了去避免这种发小写的这种，比如说我们这小写的也认了，要注意这个哈希是什么意思,把我们这个 key那我们只对它做一个计算，方便我们去找到

哈希是对这个 k做了一个简单的这个哈希值，然后方便我们对于这个数组的位置，方便我们查找就这个意思。

纯内存操作，

那关于文件操作，我们如果对这个文件操作要实现一个组件的话，或者实现一个数据结构的话，我们该怎么做，

第一个我们不需要担心数据的存储，我们不需要去考虑布局在内存中的存储，因为它有内存池，不需要考虑具体数据的程度在内存中的存储，那我们对这如果已经知道了一块内存的数据，我们如何去把它同步到磁盘中间？

所以现在考虑一下现在大家考虑一下，我们假设已经有了这一块内存所有的数据我们需要同步到这个磁盘中间去，同步的方法就是write和read，那么现在对于这一块内存的数据，我们怎么把它同步进去，这个要考虑几个原因，考虑几个问题，

第一个一次性能不能写完?第二个就是我们一次性没有写完，下一次在哪个地方开始写这几个问题，

那我们怎么去定义对这个内对这块内存我们做一个结构体来标识，让我们更好的把这个数据同步到磁盘里面，就用一个结构体来表述把它同步进去，

就是头在哪个地方，尾在哪个地方，读到哪个地方

首先第一个它开始的位置，

首先它是有个开始的位置，还有一个结束的位置。文件在不停的增加哪个目录？。

他就是做这个事情，他就是记录我们对数据文件同步的时候用了这个ngx_buf_t，

这里只是对一个文件进行同步，如果我们要对多个文件进行同步，那对多个文件进行统一的操作那怎么做？这个链ngx_chain_t

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161611_56432.jpg)

为什么这里有开始的位置，这里有个结束位置，这里为什么还要那个文件的结束位置，这个是怎么理解？

这些东西关于对于文件操作的几个东西，就是我们对于文件定位的时候，就是我们从哪个地方，这首先前面6项，

就是我们在同步的文件里面就记录了这个所有的位置，

后面它不是核心，只是为了去辅助我们数据更好做。

我们在读的时候或者我们再往里面写的时候，在里面写的时候，首先这个buff从哪个地方开始到哪个地方结束，写到文件的哪个位置，再到文件的哪个位置，

u_char这里面的数据我们可以拿出来的，

不像是我们刚刚讲的那个例子，

历史里面它是指向的是有具体的某个类型值的，但是请注意这个buff它分配完了之后，它也是在那些池里面，我们没办法知道从buff里面是哪个内存池，但是我们可以从内存池里面去分配

list可以知道这是分配在哪一个内存池的

这一项start是从buff里面哪个地方开始到哪个地方结束，然后写到这个文件在哪个定位开始到哪个地方结束，然后写进去的时候之前这个内容是什么，到了结尾这个内容是什么？

这个返回能够返回回来，就是这么6个字**对于这个 buff操作时候我们需要引几个函数？**其实核心的是这两个读和写

所有的业务需求是基于这个基础组建做的，它也是基于这些技术组建做的，全部都是基于这些技术组件在上面做的，

进程间通信最好的一个方法可以采用共享内存。

那共享内存的做法，关于函数接口上面有这么两种做法，

一个是采用的mmap

第二个就采用shmat，这两个都是可取的，而这两个的思路在实现的时候又是很相近的，各位朋友你去打开那个进程，虚拟内存它的共享图的时候，那个内存的Top图的时候，你要发现现在中间它多一项叫做mmap项，这个当我们使用共享内存的时候，这个内存多个进程之间共享的内存地址就是说的mmap这个段

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161611_16263.jpg)

总共一起提供出来三个方法，这三个方法大家可以看到可以看到

这三个这三个中间到底是采用宏地分隔出来的，他到底是选择哪个？

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217161612_93449.jpg)

关于共享内存本身来说，第一个就是分配，第二个就是释放

原文链接：https://zhuanlan.zhihu.com/p/511652302

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.500】后端开发【一大波干货知识】Redis的线程模型和异步机制

### 1.文章目录

- Redis 6.0引入多线程
- 异步机制
- Redis pipeline技术
- Redis 事务
- ACID特性分析
- redis 发布订阅

我们通常说，Redis 是单线程，主要是指 Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程。

但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。

### 2.为什么使用单线程:

多线程并发开销大，访问共享资源时，要确保资源的正确性，需要额外的机制保证正确性，额外的操作增加了系统开销。

在Redis 6.0之前，Redis 在处理客户端的请求时，包括获取 (socket 读)、解析、执行、内容返回 (socket 写) 等都由一个顺序串行的主线程处理，这就是所谓的「单线程」。其中执行命令阶段，由于 Redis 是单线程来处理命令的，所有每一条到达服务端的命令不会立刻执行，所有的命令都会进入一个 Socket 队列中，当 socket 可读则交给单线程事件分发器逐个被执行。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217162223_61041.jpg)

官方曾做过类似问题的回复：使用Redis时，几乎不存在CPU成为瓶颈的情况， Redis主要受限于内存和网络。例如在一个普通的Linux系统上，Redis通过使用pipelining每秒可以处理100万个请求，所以如果应用程序主要使用O(N)或O(log(N))的命令，它几乎不会占用太多CPU。

使用了单线程后，可维护性高。多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗。Redis通过AE事件模型以及IO多路复用等技术，处理性能非常高，因此没有必要使用多线程。单线程机制使得 Redis 内部实现的复杂度大大降低，Hash 的惰性 Rehash、Lpush 等等 “线程不安全” 的命令都可以无锁进行。

Redis绝大部分操作是基于内存的，而且是纯kv（key-value）操作，所以命令执行速度非常快。我们可以大概理解成，redis中的数据存储在一张大HashMap中，HashMap的优势就是查找和写入的时间复杂度都是O(1)。Redis内部采用这种结构存储数据，就奠定了Redis高性能的基础。根据Redis官网描述，在理想情况下Redis每秒可以提交一百万次请求，每次请求提交所需的时间在纳秒的时间量级。既然每次的Redis操作都这么快，单线程就可以完全搞定了，那还何必要用多线程呢！

### 3.Redis 6.0引入多线程

我们知道，单线程主要在一个CPU核进行工作，但是随着我们硬件的快速升级和大量业务的需求，单个线程处理网络读写的速度跟不上底层网络硬件的速度， 读写网络的read/write系统调用占用了redis执行期间大部分CPU时间，瓶颈主要在于网络的IO消耗，优化主要有两个方向:

- 提高网络 IO 性能，典型的实现比如使用 DPDK来替代内核网络栈的方式、零拷贝技术。
- 使用多线程充分利用多核，提高网络请求读写的并行度，典型的实现比如 Memcached。

零拷贝技术有其局限性，无法完全适配redis这一复杂的网络IO模型。而DPDK技术通过旁路网卡IO绕过内核协议栈的方式又太过于复杂以及需要内核甚至硬件的支持，所以我们只能从后者下手啦。主要注意的是，redis多IO线程模型只用来处理网络读写请求，对于redis的读写命令，依然是单线程处理。这是因为：

- 网络处理经常是瓶颈，需要通过多线程并行处理可提高性能
- 继续使用单线程执行读写命令，不需要为了保证LUA脚本、事务等开发多线程安全机制，实现更简单

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217162224_88688.jpg)

client： 客户端对象，Redis 是典型的 CS 架构（Client <—> Server），客户端通过 socket 与服务端建立网络通道然后发送请求命令，服务端执行请求的命令并回复。Redis 使用结构体 client 存储客户端的所有相关信息，包括但不限于封装的套接字连接 – *conn，当前选择的数据库指针 – *db，读入缓冲区 – querybuf，写出缓冲区 – buf，写出数据链表 – reply等。

aeApiPoll：I/O 多路复用 API，是基于 epoll_wait/select/kevent 等系统调用的封装，监听等待读写事件触发，然后处理，它是事件循环（Event Loop）中的核心函数，是事件驱动得以运行的基础。

acceptTcpHandler： 连接应答处理器，底层使用系统调用 accept 接受来自客户端的新连接，并为新连接注册绑定命令读取处理器，以备后续处理新的客户端 TCP 连接；除了这个处理器，还有对应的 acceptUnixHandler 负责处理 Unix Domain Socket 以及 acceptTLSHandler 负责处理 TLS 加密连接。

readQueryFromClient：命令读取处理器，解析并执行客户端的请求命令。

beforeSleep： 事件循环中进入 aeApiPoll 等待事件到来之前会执行的函数，其中包含一些日常的任务，比如把 client->buf 或者 client->reply （后面会解释为什么这里需要两个缓冲区）中的响应写回到客户端，持久化 AOF 缓冲区的数据到磁盘等，相对应的还有一个 afterSleep 函数，在 aeApiPoll 之后执行。

sendReplyToClient： 命令回复处理器，当一次事件循环之后写出缓冲区中还有数据残留，则这个处理器会被注册绑定到相应的连接上，等连接触发写就绪事件时，它会将写出缓冲区剩余的数据回写到客户端。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217162224_32644.jpg)

### 4.流程简述如下：

1、主线程负责接收建立连接请求，获取 socket 放入全局等待读处理队列

2、主线程处理完读事件之后，通过 RR(Round Robin) 将这些连接分配给这些 IO 线程

3、主线程阻塞等待 IO 线程读取 socket 完毕

4、主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行

5、主线程阻塞等待 IO 线程将数据回写 socket 完毕

6、解除绑定，清空等待队列

### 5.该设计有如下特点：

- IO 线程要么同时在读 socket，要么同时在写，不会同时读或写
- IO 线程只负责读写 socket 解析命令，不负责命令处理

### 6.开启多线程后，是否会存在线程并发安全问题？

不存在。从实现机制可以看出，redis的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程顺序执行所以我们不需要去考虑控制 Key、Lua、事务，LPUSH/LPOP 等等的并发及线程安全问题。

### 7.Redis6.0与Memcached多线程模型对比：

相同点：都采用了master线程—worker线程的模型

### 8.不同点：

- memcached执行主逻辑也是在worker线程里面，模型更进简单，实现了真正的线程隔离
- redis把处理逻辑交还给了master线程，虽然在一定程序上增加了模型复杂度，但也解决了线程并发安全等问题

### 9.Redis 6.0 默认是否开启了多线程?

redis 6.0 的多线程默认是禁用的，只使用主线程。如需开启在redis.conf文件进行配置

```
# 在 redis.conf 中
# if you have a four cores boxes, try to use 2 or 3 I/O threads, if you have
a 8 cores, try to use 6 threads.
io-threads 4
# 默认只开启 encode 也就是redis发送给客户端的协议压缩工作；也可开启io-threads-do-reads
yes来实现 decode;
# 一般发送给redis的命令数据包都比较少，所以不需要开启 decode 功能；
# io-threads-do-reads no
```

### 10.异步机制

**创建线程:**

Redis 主线程启动后，会使用操作系统提供的 pthread_create 函数创建 3 个子线程，分别由它们负责 AOF 日志写操作、key-value删除以及文件关闭的异步执行。

主线程通过一个链表形式的任务队列和子线程进行交互。

当Redis实例收到key-value删除和清空数据库的操作时，主线程会把这个操作封装成一个任务，放入到任务队列中，然后给客户端返回一个完成信息，表明删除已经完成。

这个时候删除操作还没有执行，等到后台子线程从任务队列中读取任务后，才开始实际删除键值对，并释放相应的内存空间。因此，我们把这种异步删除也称为惰性删除（lazy free）。此时，删除或清空操作不会阻塞主线程，这就避免了对主线程的性能影响。

同步连接方案采用阻塞io来实现；优点是代码书写是同步的，业务逻辑没有割裂；缺点是阻塞当前线程，直至redis返回结果；通常用多个线程来实现线程池来解决效率问题；异步连接方案采用非阻塞io来实现；优点是没有阻塞当前线程，redis 没有返回，依然可以往redis发送命令；缺点是代码书写是异步的（回调函数），业务逻辑割裂，可以通过协程解决

（openresty，skynet）；配合redis6.0以后的io多线程（前提是有大量并发请求），异步连接池，能更好解决应用层的数据访问性能；

### 11.Redis pipeline技术

**redis pipeline 是一个客户端提供的，而不是服务端提供的；**

当我们使用客户端对 Redis 进行一次操作时，如下图所示，客户端将请求传送给服务器，服务器处理完毕后，再将响应回复给客户端。这要花费一个网络数据包来回的时间。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217162225_88477.jpg)

如果连续执行多条指令，那就会花费多个网络数据包来回的时间。

如下图所示。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217162226_14744.jpg)

回到客户端代码层面，客户端是经历了读-写-读-写四个操作才完整地执行了两条指令。

现在如果我们调整读写顺序，改成写—写-读-读，这两个指令同样可以正常完成。

两个连续的写操作和两个连续的读操作总共只会花费一次网络来回，就好比连续的 write 操作合并了，连续的 read 操作也合并了一样。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/17/20221217162226_56659.jpg)

这便是管道操作的本质， 服务器根本没有任何区别对待， 还是收到一条消息， 执行一条消息， 回复一条消息的正常的流程。 客户端通过对管道中的指令列表改变读写顺序就可以大幅节省 IO 时间。 管道中指令越多，效果越好。对于request操作，只是将数据写到fd对应的写缓冲区，时间非常快，真正耗时操作在读取response。

### 12.Redis 事务

MULTI 开启事务，事务执行过程中，单个命令是入队列操作，直到调用 EXEC 才会一起执行；

> MULTI 开启事务
> EXEC 提交事务
> DISCARD 取消事务
> WATCH 检测key的变动，若在事务执行中，key变动则取消事务；在事务开启前调用，乐观锁实现（cas）;
> 若被取消则事务返回 nil ;

但是事务在执行的过程中，却不是原子性的，所以我们可以使用lua脚本来实现redis的原子性。

> redis中加载了一个lua虚拟机；用来执行redis lua脚本；redis lua 脚本的执行是原子性的；当某个
> 脚本正在执行的时候，不会有其他命令或者脚本被执行；
> lua脚本当中的命令会直接修改数据状态；
> cat test1.lua | redis-cli script load –pipe
> \# 加载 lua脚本字符串 生成 sha1
> \> script load ‘local val = KEYS[1]; return val’
> “b8059ba43af6ffe8bed3db65bac35d452f8115d8”
> \# 检查脚本缓存中，是否有该 sha1 散列值的lua脚本
> \> script exists “b8059ba43af6ffe8bed3db65bac35d452f8115d8”
> \1) (integer) 1
> \# 清除所有脚本缓存
> \> script flush
> OK
> \# 如果当前脚本运行时间过长，可以通过 script kill 杀死当前运行的脚本
> \> script kill
> (error) NOTBUSY No scripts in execution right now.

### 13.EVAL

> \# 测试使用
> EVAL script numkeys key [key …] arg [arg …]

### 14.EVALSHA

> \# 线上使用
> EVALSHA sha1 numkeys key [key …] arg [arg …]

### 15.ACID特性分析

A 原子性；事务是一个不可分割的工作单位，事务中的操作要么全部成功，要么全部失败；redis

> 不支持回滚；即使事务队列中的某个命令在执行期间出现了错误，整个事务也会继续执行下去，直
> 到将事务队列中的所有命令都执行完毕为止。
> C 一致性；事务使数据库从一个一致性状态到另外一个一致性状态；这里的一致性是指预期的一
> 致性而不是异常后的一致性；所以redis也不满足；
> I 隔离性；事务的操作不被其他用户操作所打断；redis命令执行是串行的，redis事务天然具备隔
> 离性；
> D 持久性；redis只有在 aof 持久化策略的时候，并且需要在 redis.conf 中
> appendfsync=always 才具备持久性；实际项目中几乎不会使用 aof 持久化策略

### 16.redis 发布订阅

> 为了支持消息的多播机制，redis引入了发布订阅模块；disque 消息队列
> \# 订阅频道
> subscribe 频道
> \# 订阅模式频道
> psubscribe 频道
> \# 取消订阅频道
> unsubscribe 频道
> \# 取消订阅模式频道
> punsubscribe 频道
> \# 发布具体频道或模式频道的内容
> publish 频道 内容
> \# 客户端收到具体频道内容
> message 具体频道 内容
> \# 客户端收到模式频道内容
> pmessage 模式频道 具体频道 内容

发布订阅功能一般要区别命令连接重新开启一个连接；因为命令连接严格遵循请求回应模式；而pubsub能收到redis主动推送的内容；所以实际项目中如果支持pubsub的话，需要另开一条连接用于处理发布订阅 。

### 17.缺点:

> 发布订阅的生产者传递过来一个消息，redis会直接找到相应的消费者并传递过去；假如没有消费者，消息直接丢弃；假如开始有2个消费者，一个消费者突然挂掉了，另外一个消费者依然能收到消息，但是如果刚挂掉的消费者重新连上后，在断开连接期间的消息对于该消费者来说彻底丢失了；
> 另外，redis停机重启，pubsub的消息是不会持久化的，所有的消息被直接丢弃；

原文链接：https://zhuanlan.zhihu.com/p/512118401

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.501】Linux的虚拟内存详解（MMU、页表结构）

内存是程序得以运行的重要物质基础。如何在有限的内存空间运行较大的应用程序，曾是困扰人们的一个难题。为解决这个问题，人们设计了许多的方案，其中最成功的当属虚拟内存技术。Linux作为一个以通用为目的的现代大型操作系统，当然也毫不例外的采用了优点甚多的虚拟内存技术。

## 1.虚拟内存

为了运行比实际物理内存容量还要大的程序，包括Linux在内的所有现代操作系统几乎毫无例外的都采用了虚拟内存技术。虚拟内存技术，可让系统看上去具有比实际物理意义内存大的多的内存空间，并为实现多道程序的执行创造了条件。

## 2.虚拟内存的概念

总所周知，**为了对内存中的存储单元进行识别，内存中的每一个存储单元都必须有一个确切的地址。而一台计算机的处理器能访问多大的内存空间就取决于处理器的程序计数器，该计数器的字长越长，能访问的空间就越大。**

例如：对于程序计数器位数为32位的处理器来说，他的地址发生器所能发出的地址数目为2^32=4G个，于是这个处理器所能访问的最大内存空间就是4G。在计算机技术中，这个值就叫做处理器的寻址空间或寻址能力。

照理说，为了充分利用处理器的寻址空间，就应按照处理器的最大寻址来为其分配系统的内存。如果处理器具有32位程序计数器，那么就应该按照下图的方式，为其配备4G的内存：

![img](https://pic3.zhimg.com/80/v2-44262f44c7a5893677a292a13fbb94ca_720w.webp)

这样，处理器所发出的每一个地址都会有一个真实的物理存储单元与之对应；同时，每一个物理存储单元都有唯一的地址与之对应。这显然是一种最理想的情况。

但遗憾的是，**实际上计算机所配置内存的实际空间常常小于处理器的寻址范围**，这是就会因处理器的一部分寻址空间没有对应的物理存储单元，从而导致处理器寻址能力的浪费。例如：如下图的系统中，具有32位寻址能力的处理器只配置了256M的内存储器，这就会造成大量的浪费：

![img](https://pic4.zhimg.com/80/v2-f971f03e06e7b41ec3be7425819ba153_720w.webp)

另外，**还有一些处理器因外部地址线的根数小于处理器程序计数器的位数**，而使地址总线的根数不满足处理器的寻址范围，从而处理器的其余寻址能力也就被浪费了。例如：Intel8086处理器的程序计数器位32位，而处理器芯片的外部地址总线只有20根，所以它所能配置的最大内存为1MB：

![img](https://pic1.zhimg.com/80/v2-19bd139e798b17dfb6d23d781719d4d4_720w.webp)

在实际的应用中，如果需要运行的应用程序比较小，所需内存容量小于计算机实际所配置的内存空间，自然不会出什么问题。但是，目前很多的应用程序都比较大，计算机实际所配置的内存空间无法满足。

**实践和研究都证明：一个应用程序总是逐段被运行的，而且在一段时间内会稳定运行在某一段程序里。**

**这也就出现了一个方法：如下图所示，把要运行的那一段程序自辅存复制到内存中来运行，而其他暂时不运行的程序段就让它仍然留在辅存。**

![img](https://pic3.zhimg.com/80/v2-0f30a829aabda439818996b9759ff0c6_720w.webp)

当需要执行另一端尚未在内存的程序段（如程序段2），如下图所示，就可以把内存中程序段1的副本复制回辅存，在内存腾出必要的空间后，再把辅存中的程序段2复制到内存空间来执行即可：

![img](https://pic3.zhimg.com/80/v2-98aa35aea34da8a0390fb977104150a6_720w.webp)

**在计算机技术中，把内存中的程序段复制回辅存的做法叫做“换出”，而把辅存中程序段映射到内存的做法叫做“换入”。经过不断有目的的换入和换出，处理器就可以运行一个大于实际物理内存的应用程序了。**或者说，处理器似乎是拥有了一个大于实际物理内存的内存空间。于是，**这个存储空间叫做虚拟内存空间，而把真正的内存叫做实际物理内存，或简称为物理内存。**

那么对于一台真实的计算机来说，它的虚拟内存空间又有多大呢？**计算机虚拟内存空间的大小是由程序计数器的寻址能力来决定的。**例如：在程序计数器的位数为32的处理器中，它的虚拟内存空间就为4GB。

可见，如果一个系统采用了虚拟内存技术，那么它就存在着两个内存空间：**虚拟内存空间和物理内存空间。虚拟内存空间中的地址叫做“虚拟地址”；而实际物理内存空间中的地址叫做“实际物理地址”或“物理地址”。处理器运算器和应用程序设计人员看到的只是虚拟内存空间和虚拟地址，而处理器片外的地址总线看到的只是物理地址空间和物理地址。**

由于存在两个内存地址，因此一个应用程序从编写到被执行，需要进行两次映射。**第一次是映射到虚拟内存空间，第二次时映射到物理内存空间。在计算机系统中，第两次映射的工作是由硬件和软件共同来完成的。承担这个任务的硬件部分叫做存储管理单元MMU，软件部分就是操作系统的内存管理模块了。**

**在映射工作中，为了记录程序段占用物理内存的情况，操作系统的内存管理模块需要建立一个表格，该表格以虚拟地址为索引，记录了程序段所占用的物理内存的物理地址。这个虚拟地址/物理地址记录表便是存储管理单元MMU把虚拟地址转化为实际物理地址的依据，**记录表与存储管理单元MMU的作用如下图所示：

![img](https://pic4.zhimg.com/80/v2-b0c337737637235d66f2e6e911e783b7_720w.webp)

综上所述，虚拟内存技术的实现，是建立在应用程序可以分成段，并且具有“在任何时候正在使用的信息总是所有存储信息的一小部分”的局部特性基础上的。它是通过用辅存空间模拟RAM来实现的一种使机器的作业地址空间大于实际内存的技术。

**从处理器运算装置和程序设计人员的角度来看，它面对的是一个用MMU、映射记录表和物理内存封装起来的一个虚拟内存空间，这个存储空间的大小取决于处理器程序计数器的寻址空间。**

可见，程序映射表是实现虚拟内存的技术关键，它可给系统带来如下特点：

- 系统中每一个程序各自都有一个大小与处理器寻址空间相等的虚拟内存空间；
- 在一个具体时刻，处理器只能使用其中一个程序的映射记录表，因此它只看到多个程序虚存空间中的一个，这样就保证了各个程序的虚存空间时互不相扰、各自独立的；
- 使用程序映射表可方便地实现物理内存的共享。

## 3.Linux的虚拟内存技术

**以存储单元为单位来管理显然不现实，因此Linux把虚存空间分成若干个大小相等的存储分区，Linux把这样的分区叫做页。**为了换入、换出的方便，物理内存也就按也得大小分成若干个块。**由于物理内存中的块空间是用来容纳虚存页的容器，所以物理内存中的块叫做页框。**页与页框是Linux实现虚拟内存技术的基础。

### 3.1 虚拟内存的页、物理内存的页框及页表

**在Linux中，页与页框的大小一般为4KB。**当然，根据系统和应用的不同，页与页框的大小也可有所变化。

**物理内存和虚拟内存被分成了页框与页之后，其存储单元原来的地址都被自然地分成了两段，并且这两段各自代表着不同的意义：高位段分别叫做页框码和页码，它们是识别页框和页的编码；低位段分别叫做页框偏移量和页内偏移量，它们是存储单元在页框和页内的地址编码。**下图就是两段虚拟内存和物理内存分页之后的情况：

![img](https://pic4.zhimg.com/80/v2-c7493986b2c8ed3024130bc6bf1c067f_720w.webp)

为了使系统可以正确的访问虚存页在对应页框中的映像，在把一个页映射到某个页框上的同时，就**必须把页码和存放该页映像的页框码填入一个叫做页表的表项中。**这个页表就是之前提到的映射记录表。一个页表的示意图如下所示：

![img](https://pic1.zhimg.com/80/v2-a514e3ff4c10836505c4b7f9d1954a60_720w.webp)

页模式下，虚拟地址、物理地址转换关系的示意图如下所示：

![img](https://pic2.zhimg.com/80/v2-6a70993c5df0f304fb93514c182d71a5_720w.webp)

也就是说：**处理器遇到的地址都是虚拟地址。虚拟地址和物理地址都分成页码（页框码）和偏移值两部分。在由虚拟地址转化成物理地址的过程中，偏移值不变。而页码和页框码之间的映射就在一个映射记录表——页表中。**

### 3.2 请页与交换

虚存页面到物理页框的映射叫做页面的加载。

**当处理器试图访问一个虚存页面时，首先到页表中去查询该页是否已映射到物理页框中，并记录在页表中。如果在，则MMU会把页码转换成页框码，并加上虚拟地址提供的页内偏移量形成物理地址后去访问物理内存；如果不在，则意味着该虚存页面还没有被载入内存，这时MMU就会通知操作系统：发生了一个页面访问错误（页面错误），接下来系统会启动所谓的“请页”机制，即调用相应的系统操作函数，判断该虚拟地址是否为有效地址。**

**如果是有效的地址，就从虚拟内存中将该地址指向的页面读入到内存中的一个空闲页框中，并在页表中添加上相对应的表项，最后处理器将从发生页面错误的地方重新开始运行；如果是无效的地址，则表明进程在试图访问一个不存在的虚拟地址，此时操作系统将终止此次访问。**

当然，也存在这样的情况：**在请页成功之后，内存中已没有空闲物理页框了。这是，系统必须启动所谓地“交换”机制，即调用相应的内核操作函数，在物理页框中寻找一个当前不再使用或者近期可能不会用到的页面所占据的页框。找到后，就把其中的页移出，以装载新的页面。对移出页面根据两种情况来处理：如果该页未被修改过，则删除它；如果该页曾经被修改过，则系统必须将该页写回辅存。**

系统请页的处理过程如下所示：

![img](https://pic1.zhimg.com/80/v2-55ceca2388a8caae1df66c97d3bcb27c_720w.webp)

为了公平地选择将要从系统中抛弃的页面，**Linux系统使用最近最少使用（LRU）页面的衰老算法。这种策略根据系统中每个页面被访问的频率，为物理页框中的页面设置了一个叫做年龄的属性。页面被访问的次数越多，则页面的年龄最小；相反，则越大。而年龄较大的页面就是待换出页面的最佳候选者。**

### 3.3 快表

在系统每次访问虚存页时，都要在内存的所有页表中寻找该页的页框，这是一个很费时间的工作。但是，**人们发现，系统一旦访问了某一个页，那么系统就会在一段时间内稳定地工作在这个页上。所以，为了提高访问页表的速度，系统还配备了一组正好能容纳一个页表的硬件寄存器，这样当系统再访问虚存时，就首先到这组硬件寄存器中去访问，系统速度就快多了。这组存放当前页表的寄存器叫做快表。**

总之，使用虚拟存储技术时，处理器必须配备一些硬件来承担内存管理的一部分任务。承担内存管理任务的硬件部分叫做存储管理单元MMU。存储管理单元MMU的工作过程如下图所示：

![img](https://pic1.zhimg.com/80/v2-5f0e4d44860ccb00a7a69893f5a8a8d0_720w.webp)

### 3.4 页的共享

在多程序系统中，常常有多个程序需要共享同一段代码或数据的情况。在分页管理的存储器中，这个事情很好办：让多个程序共享同一个页面即可。

具体的方法是：使这些相关程序的虚拟空间的页面在页表中指向内存中的同一个页框。这样，当程序运行并访问这些相关页面时，就都是对同一个页框中的页面进行访问，而该页框中的页就被这些程序所共享。下图是3个程序共享一个页面的例子：

![img](https://pic1.zhimg.com/80/v2-290350f98633da8a0b27142b10448b8c_720w.webp)

### 3.5 页的保护

由上可知，**页表实际上是由虚拟空间转到物理空间的入口。因此，为了保护页面内容不被没有该页面访问权限的程序所破坏，就应在页表的表项中设置一些访问控制字段，用于指明对应页面中的内容允许何种操作，从而禁止非法访问。**

下图是页表项中存放控制信息的一种可能的形式：

![img](https://pic4.zhimg.com/80/v2-b76ab42f9bc87b9890246b9d52358f03_720w.webp)

注意：**其中的PCD位表示着是否允许高速缓存（cache）**。

如果程序对一个页试图进行一个该页控制字段所不允许的操作，则会引起操作系统的一次中断——非法访问中断，并拒绝这种操作，从而保护该页的内容不被破坏。

### 3.6 多级页表

需要注意的是，页表是操作系统创建的用于内存管理的表格。因此，**一个程序在运行时，其页表也要存放到内存空间。如果一个程序只需要一个页表，则不会有什么问题。但如果，程序的虚拟空间很大的话，就会出现一个比较大的问题。**

比如：一个程序的虚拟空间为4GB，页表以4KB为一页，那么这个程序空间就是1M页。为了存储这1M页的页指针，那么这个页表的长度就相当大了，对内存的负担也很大了。所以，**最好对页表也进行分页存储，在程序运行时只把需要的页复制到内存，而暂时不需要的页就让它留在辅存中。为了管理这些页表页，还要建立一个记录页表页首地址的页目录表，**于是单级页表就变成了二级页表。二级页表的地址转换如下图所示：

![img](https://pic3.zhimg.com/80/v2-e70e5879939f02331bba362f7a7fc14e_720w.webp)

当然，如果程序的虚拟空间更大，那么也可以用三级页表来管理。为了具有通用性，**Linux系统使用了三级页表结构：页目录（Page Directory，PGD）、中间页目录（Page Middle Directory，PMD）、页表（Page Table，PTE）。**

### 3.7 Linux的页表结构

为了通用，Linux系统使用了三级页表结构：页目录、中间页目录和页表。**PGD为顶级页表，是一个pgd_t数据类型（定义在文件linux/include/page.h中）的数组，每个数组元素指向一个中间页目录；PMD为二级页表，是一个pmd_t数据结构的数组，每个数组元素指向一个页表；PTE则是页表，是一个pte_t数据类型的数组，每个元素中含有物理地址。**

![img](https://pic3.zhimg.com/80/v2-9dbc08baf5d58f4891f1b14f94d6552e_720w.webp)

为了应用上的灵活，Linux使用一系列的宏来掩盖各种平台的细节。用户可以在配置文件config中根据自己的需要对页表进行配置，以决定是使用三级页表还是使用二级页表。

在系统编译时，会根据配置文件config中的配置，把目录include/asm符号连接到具体CPU专用的文件目录中。例如，对于i386CPU，该目录符号会连接到include/asm-i386，并在文件pgable-2level-defs.h中定义了二级页表的基本结构，如下图：

![img](https://pic2.zhimg.com/80/v2-2062a18a572016ecce2d3ec5258b64f9_720w.webp)

其中还定义了：

```text
#define PGDIR_SHIFT 22                        //PGD在线性地址中的起始地址为bit22
#define PTRS_PER_PGD 1024                     //PGD共有1024个表项
#define PTRS_PER_PTE 1024                     //PTE共有1024个表项
#endif
```

在文件
include/asm-i386/pgtable.h中定义了页目录和页表项的数据结构，如下：

```text
typedof struct { unsigned long pte_low; } pte_t;                    //页表中的物理地址，页框码
typedof struct { unsigned long pgd; } pgd_t;                        //指向一个页表
typedof struct { unsigned long pgprot; } pgprot_t;                  //页表中的各个状态信息和访问权限
```

从定义可知，它们都是只有一个长整型类型（32位）的结构体。

注意：如上文的“页的保护”部分，页框码代表物理地址，只需要高20位就够了（**因为页框的长度为4KB，因此页内偏移12位**）。而后12位可以存放各个状态信息和访问权限。但是Linux并没有这样做，反而重新定义了一个结构体来存放，通过“或”运算来将两者结合

原文地址：https://zhuanlan.zhihu.com/p/352366788

作者：linux

# 【NO.502】各大厂c++ linux后端开发岗位要求汇总

## 1.前言

今天的主题是主要整理一下后端开发岗位招聘要求和需要掌握的技能，相信今年有很多想要更换工作的同猿们，被突如其来的疫情打乱了计划，希望来年的你们能得到你们想要的offer！祝好~

这份要求可能没有大家常见有数据分析那种，有很多的饼图、柱状图等等，这里只有单纯的文字，不过相信大家看完后，也知道自己该做什么样的准备~

## 2.招聘网站

首先介绍一下获取这些岗位和技能的来源：

互联网招聘网站，想必大家都知道一些，这里只列举我最常用的几个，也同样是最有效的几个：

拉勾网

拉勾网目前应该是互联网招聘职位最多、活跃HR也最多的网站，这里不接受反驳哈~

BOSS直聘

BOSS直聘，在我印象中是2017年左后火起来，或者更早，我是那个时候才使用的，这里有一个最好的地方是可以直接和HR去聊一下岗位和公司，也可以直接将简历发送给HR，当然，拉钩也有同样的功能~

脉脉

脉脉是一款职场人社交的APP，这里会有大量的公司职员，而且有大量的猎头穿梭在人海中，只为寻找到你哈！

上面三个渠道，就是我找工作使用最多的网站，同样，也是在互联网招聘中占据最多的，一般有这三个渠道就足够使用了，不比花费更多的精力去使用其他的招聘网站去投简历了~！

## 3.岗位要求汇总

下面这些岗位要求，是我浏览了大量岗位进行的一个总结，这些技能要求基本都是一线互联网大厂的要求，同样这些技能也是最基础的技能，需要我们掌握~

C++后台高级工程师（腾讯）

> 岗位要求：
> 具备良好的分析解决问题能力，能独立承担任务，有系统进度把控能力；
> 责任心强，具有良好的对外沟通和团队协作能力，主动，好学；
> 熟练Unix/Linux下C/C++开发和程序性能调优；
> 熟悉Unix/Linux操作系统原理及常用工具；
> 熟悉TCP/IP协议、进程间通讯编程，熟悉Unix/Linux下常用架构设计方法；
> 具备全面的软件知识结构认知（操作系统、软件工程、设计模式、数据结构、数据库系统、网络安全）优先；
> 熟悉分布式架构的主要架构方法，熟悉Mysql等数据库，熟悉NoSQL存储，熟悉面向对象设计。
> 熟悉rpc协议编程

C++开发（高级/资深）工程师（头条）

> 职位要求：
> 1、本科及以上学历，计算机或相关专业，良好的英文表达能力；
> 2、精通C++ 语言开发，有扎实的代码编写能力和良好的设计能力；
> 3、热爱编程，有较强的学习能力，有强烈的求知欲、好奇心 ，能及时关注和学习业界最新技术；
> 4、具有Android平台应用程序开发和Linux环境开发经验优先，熟悉前端开发优先。

后台开发工程师（UCoud）

> 任职资格：
> 2年以上后台开发经验，计算机技术基础扎实
> 掌握 Golang/C/C++/Java/Python 等一门主流编程语言
> 熟悉 Linux，熟悉 Docker
> 熟悉 HTTP，TCP/IP 等网络协议
> 熟悉后端服务高可用方案
> 积极主动、学习能力强

软件研发工程师（C++）（百度）

> 【任职要求】
> -熟练掌握C/C++ -熟悉数据结构、算法设计 -熟悉多线程、异步编程、网络编程技术 -熟悉消息队列、文件系统、Linux操作系统原理、Linux IO性能调优方法
> -熟悉高吞吐批量数据处理系统的特点和技术方案 -富有激情和创造力，学习能力强，良好的团队合作能力
> 具有以下条件者优先：
> -有大规模分布式系统开发经验 -熟悉分布式系统理论 -有数据传输系统、消息队列、流式计算开发经验
> -有raft，mysql、rocksdb调优经验

C++开发工程师（喜马拉雅）

> 技能要求：
> 本科及以上学历，C/C++三年工作经验以上，计算机相关专业；
> 熟悉linux环境编程，熟练掌握C/C++、多线程、多进程、内存共享、网络通信编程技术，熟悉linux下的调试工具（如GDB）使用。
> 深入理解TCP/IP、HTTP等协议及网络编程，并有完整的项⽬目经验
> 有关系数据库使用经验，精通SQL语句句，能查找SQL语句句性能问题并进行行调优；
> 对数据结构和算法设计具有深刻的理理解，有多年年系统分析和设计的实践经验
> 具备优秀的逻辑思维能力力，对解决挑战性问题充满热情，善于解决问题和分析问题。
> 有大规模，高并发分布式系统开发经验者优先
> 有网络安全知识或开发经验者优先9.有移动端（iOS/Android）开发经验者优

C++高级开发工程师（京东）

> 1、本科及以上学历，计算机、自动化相关专业；
> 2、3年以上Linux平台开发经验；
> 3、精通C/C++开发，如熟悉其他编程方式如javascript, python等的更佳；
> 4、了解常用算法和数据结构；
> 5、有网络编程、多线程编程、多进程编程经验；
> 6、有高并发后台系统开发经历。

C++开发工程师（喜马拉雅）

> 岗位要求：
> 精通 C/C++编程语言，具有良好的面向对象思想，至少精通一门脚本语言
> 熟悉STL 数据结构及算法基础；
> 熟悉多种网络协议，精通高并发网络编程和多线程编程；
> 熟悉MySql数据库,Redis数据库,并且有相关开发经验；
> 具有较强的学习能力及解决 Bug 能力；
> 有游戏服务端开发经验优先。

C++开发工程师/架构师（小红书）

> 精通C/C++编程，熟悉Linux平台、shell脚本，对数据结构和算法设计有较为深刻的理解；
> 熟悉多线程/多进程程序开发，对异步、并发技术有深入理解；
> 熟悉TCP/IP HTTP等网络协议，具备海量服务器开发经验者优先；
> 熟悉mysql、nosql等数据库，了解存储引擎；
> 有数据库相关中间件，存储等基础架构服务项目优先；
> 本科及以上相关专业学历，5年及以上工作经验；
> 具有良好的沟通能力，团队合作能力。

c++开发工程师（华为）

> 1、 精通C++开发语言。计算机、软件工程等相关专业优先。
> 2、 熟悉linux下编译、开发、调试，如cmake、gdb等。
> 3、 深入掌握数据结构、算法和操作系统知识，对软件架构、软件编码有强烈的兴趣。
> 4、 学习能力强，沟通能力强，责任心强，具有良好的团队合作精神。

服务器C++开发工程师（今日头条）

> 职位要求：
> 1、本科及以上学历，计算机相关专业；
> 2、3~7年游戏服务器工作经验，有1款及以上的游戏项目上线经验；
> 3、掌握C++，有良好的面向对象设计思想并熟练运用设计模式；
> 4、掌握LINUX下开发调试等相关工具链 CLANG\GCC\CMAKE\GDB ；
> 5、熟悉网络编程、操作系统，熟练掌握REDIS数据库应用；
> 6、有扎实的网络基础，熟悉TCP/UDP等网络传输协议协议；
> 7、有良好的数据结构和算法基础；
> 8、有RTC相关项目开发经验优先；

C++开发工程师（欢聚时代）

> 职位描述：
> 岗位职责：负责直播SDK的开发工作。
> 任职资格：
> 本科及以上学历，计算机相关专业，两年以上开发经验。
> 熟练使用C++，STL，熟悉常用的数据结构与算法。
> 熟悉TCP/IP，熟悉网络编程，多线程编程。
> 熟悉音视频相关技术加分。
> 有跨平台开发经验加分。
> 有良好的沟通能力，有较强的自我驱动力和快速学习能力。

C++开发工程师（微博）

> 职位要求：
> 计算机相关专业统招本科以上学历；
> 良好的沟通与表达能力、思路清晰，较强的动手能力与逻辑分析能力
> 2+年后端系统研发经验或者基础架构开发经验，熟练掌握 C/C++，熟悉一种以上脚本语言，如Shell、Python等，具备扎实的算法和数据结构功底
> 参与过高并发分布式在线系统的研发，解决过相关性能问题
> 精通 NoSQL 数据库技术和内存数据库技术（如 redis, memcache）
> 扎实的编程能力，熟悉算法和数据结构，熟悉计算机基础理论
> 熟悉大流量、高并发、高性能的分布式系统的设计及应用，擅长性能调优者优先
> 有搜索相关功能性能调优优先

Linux C/C++ 后台研发工程师（小米）

> 招聘要求：
> 1.熟练掌握C/C++，有良好的代码风格和编程习惯
> 2.熟悉Linux下多线程/进程及网络开发, 对数据结构和算法有深刻理解；熟悉shell、python等脚本编程；
> 3.熟悉分布式系统原理,网络编程原理,熟悉TCP/IP协议栈
> 4.熟悉nginx,mysql,redis,memcached,thrift,grpc等开源软件优先;
> 5.扎实的计算机基础，具备优秀的学习能力和逻辑思维能力，善于分析问题和解决问题，有互联网行业经验优先。

高级C++开发工程师（美团）

> 职位要求：
> 计算机相关专业，本科及以上学历。
> 扎实的 C++ 基础，熟悉常用的数据结构和算法。
> 熟悉 Linux 开发环境及工具，熟悉 bash, python 等常用脚本语言。
> 熟悉并行计算、高性能计算、网络编程者优先。
> 工作积极主动，认真负责

以上这些岗位想必大家看完后，会发现很多共有的技能要求，我这里也不仅仅是搬运工，我自己总结了下这些大厂需要的技能要求：

> 1、熟练Unix/Linux下C/C++开发和程序性能调优，以及如cmake、gdb等
> 2、熟悉Unix/Linux操作系统原理及常用工具
> 3、熟悉TCP/IP、HTTP协议、进程间通讯编程，熟悉Unix/Linux下常用架构设计方法；
> 4、熟悉分布式架构的主要架构方法，熟悉Mysql等数据库，熟悉NoSQL存储，熟悉面向对象设计。
> 5、熟悉rpc协议编程
> 6、熟悉 Linux，熟悉 Docker
> 7、熟悉消息队列、文件系统、Linux操作系统原理、Linux IO性能调优方法
> 8、熟悉多线程、异步编程、网络编程技术
> 9、数据结构、算法、设计模式
> 10、分布式系统
> 11、熟悉一种以上脚本语言，如Shell、Python、js等
> 12、熟悉nginx,mysql,redis,memcached,thrift,grpc等开源软件优先;

总结为以上12点，我们在日常学习和工作中，遇到以上的知识点，我们需要进行发散，举一反三并且进行实践，总之就是一句话——多总结，多实践 才是学习程序的正确的道路呀~

原文地址：https://zhuanlan.zhihu.com/p/420731832

作者：linux

# 【NO.503】内存优化-如何使用tcmalloc来提升内存性能？提升的结果太不可思议

**无论是在后台开发，或者其他长期运行的服务开发中，对内存的使用一直是架构师或者主程序在最初就要关注的point，如果内存使用不当，频繁申请释放内存造成系统负担过大，性能降低，到最后产生大量内存碎片，无法申请可利用内存，最终宕机，给广大程序员同学造成长期加班的痛苦。**

**在讲到tcmalloc之前，这里不得不说GLIBC的资源释放机制：**

\1. glibc在多线程内存分配的场景下为了减少lock contention，会new出很多arena出来，每个线程都有自己默认的arena，但是内存申请时如果默认arena被占用，则round-robin到下一个arena。

\2. 每个arena的空间不可直接共享和互相借用，除非通过主arena释放给操作系统然后被各个辅助arena重新申请。

\3. glibc归还内存给OS有一个很苛刻的条件就是top chunk必须是free的，否则，即使应用程序已经释放了大片内存，glibc也不会将这些内存归还给OS。

这里我引入tcmalloc，相当于常见的内存池，tcmalloc的优势体现在：

（1）分配内存页的时候，直接跟OS打交道，而常用的内存池一般是基于别的内存管理器上分配，如果完全一样的内存管理策略，明显tcmalloc在性能及内存利用率上要省掉第三方内存管理的开销。之所以会出现这种情况，是因为大部分写内存池的coder都不太了解OS

（2）大部分的内存池只负责分配，不管回收。

## 1.**为什么要使用TCmalloc**

TCMalloc要比glibc 2.3的malloc（可以从一个叫作ptmalloc2的独立库获得）和其他我测试过的malloc都快。ptmalloc在一台2.8GHz的P4机器上执行一次小对象malloc及free大约需要300纳秒，而TCMalloc的版本同样的操作大约只需要50纳秒。malloc版本的速度是至关重要的，因为如果malloc不够快，应用程序的作者就倾向于在malloc之上写一个自己的**内存释放列表**。这就可能导致额外的代码复杂度，以及更多的内存占用――除非作者本身非常仔细地划分释放列表的大小并经常从中清除空闲的对象。

TCMalloc也减少了**多线程程序中的锁竞争**情况。对于小对象，已经基本上达到了零竞争。对于大对象，TCMalloc尝试使用恰当粒度和有效的自旋锁。ptmalloc同样是通过使用每线程各自的空间来减少锁的竞争，但是ptmalloc2使用每线程空间有一个很大的问题。在ptmalloc2中，内存不可能会从一个空间移动到另一个空间。这有可能导致大量内存被浪费。例如，在一个Google的应用中，第一阶段可能会为其URL标准化的数据结构分配大约300MB内存。当第一阶段结束后，第二阶段将从同样的地址空间开始。如果第二个阶段被安排到了与第一阶段不同的空间内，这个阶段不会复用任何第一阶段留下的的内存，并会给地址空间添加另外一个300MB。类似的内存爆炸问题也可以在其他的应用中看到。

TCMalloc的另一个好处表现在**小对象的空间效率**。例如，分配N个8字节对象可能要使用大约`8N * 1.01`字节的空间，即多用百分之一的空间。而ptmalloc2中每个对象都使用了一个四字节的头，我认为并将最终的尺寸圆整为8字节的倍数，最后使用了`16N`字节。

## 2.**如何使用TCmalloc**

### **2.1 下载**

google-perftools：[http://code.google.com/p/google-perftools/gperftools-2.1.tar.gz](https://link.zhihu.com/?target=http%3A//code.google.com/p/google-perftools/gperftools-2.1.tar.gz)

libunwind：[http://download.savannah.gnu.org/releases/libunwind/libunwind-1.1.tar.gz](https://link.zhihu.com/?target=http%3A//download.savannah.gnu.org/releases/libunwind/libunwind-1.1.tar.gz)

### **2.2 libunwind安装**

64位操作系统请先安装 libunwind库，32位操作系统不要安装。libunwind库为基于64位CPU和操作系统的程序提供了主要的堆栈辗转开解功能。当中包含用于输出堆栈跟踪的API、用于以编程方式辗转开解堆栈的API以及支持C++异常处理机制的API。

1: #tar zxvf libunwind-1.1.tar.gz

2: #cd libunwind-1.1

3: #./configure

4: #make

5: #make install

### 2.3 安装google-perftools：

1: #tar zxvf tar zxvf gperftools-2.1.tar.gz

2: #cd gperftools-2.1

3: #./configure

4: #make

5: #make install

### **2.4 TCMalloc库载入到Linux系统中：**

1: echo "/usr/local/lib" > /etc/ld.so.conf.d/usr_local_lib.conf

2: /sbin/ldconfig

### **2.5 TCMalloc库链接到你的程序中：**

要使用TCMalloc，只要将tcmalloc通过“-ltcmalloc”链接器标志接入你的应用即可。

你也可以通过使用LD_PRELOAD在不是你自己编译的应用中使用tcmalloc：

```text
$ LD_PRELOAD="/usr/lib/libtcmalloc.so" 
```

　　LD_PRELOAD比较麻烦，我们也不十分推荐这种用法。

　　TCMalloc还包含了一个**堆检查器**以及一个**堆测量器**。

如果你更想链接不包含堆测量器和检查器的TCMalloc版本（比如可能为了减少静态二进制文件的大小），你应该链接`libtcmalloc_minimal`。

**測试代码：**

```text
#include <iostream>
 using namespace std; int main()
 {
         int *p = new int();
 return 0;
 }
```

编译：g++ main.cpp -o main -ltcmalloc -g -O0

内存泄漏检查： env HEAPCHECK=normal ./main

结果：

WARNING: Perftools heap leak checker is active -- Performance may sufferHave memory regions w/o callers: might report false leaksLeak check _main_ detected leaks of 4 bytes in 1 objectsThe 1 largest leaks:Using local file ./main.Leak of 4 bytes in 1 objects allocated from: @ 4007a6 main @ 7f1734263d1d __libc_start_main @ 4006d9 _startIf the preceding stack traces are not enough to find the leaks, try running THIS shell command:pprof ./main "/tmp/main.54616._main_-end.heap" --inuse_objects --lines --heapcheck --edgefraction=1e-10 --nodefraction=1e-10 --gvIf you are still puzzled about why the leaks are there, try rerunning this program with HEAP_CHECK_TEST_POINTER_ALIGNMENT=1 and/or with HEAP_CHECK_MAX_POINTER_OFFSET=-1If the leak report occurs in a small fraction of runs, try running with TCMALLOC_MAX_FREE_QUEUE_SIZE of few hundred MB or with TCMALLOC_RECLAIM_MEMORY=false, it might help find leaks more repeatablyExiting with error code (instead of crashing) because of whole-program memory leaks

这里不细讲内存泄漏检测，我将专门分享一篇文章来分析内存泄漏。

### 2.6 **TCmalloc的一些环境变量和参数**

我们可以通过环境变量来控制tcmalloc的行为，通常有用的标志。

| 标志                                  | 默认值     | 作用                         |
| ------------------------------------- | ---------- | ---------------------------- |
| TCMALLOC_SAMPLE_PARAMETER             | 0          | 采样时间间隔                 |
| TCMALLOC_RELEASE_RATE                 | 1.0        | 释放未使用内存的概率         |
| TCMALLOC_LARGE_ALLOC_REPORT_THRESHOLD | 1073741824 | 内存最大分配阈值             |
| TCMALLOC_MAX_TOTAL_THREAD_CACHE_BYTES | 16777216   | 分配给线程缓冲的最大内存上限 |

微调参数:

| TCMALLOC_SKIP_MMAP              | default: false    | If true, do not try to use mmap to obtain memory from the kernel. |
| ------------------------------- | ----------------- | ------------------------------------------------------------ |
| TCMALLOC_SKIP_SBRK              | default: false    | If true, do not try to use sbrk to obtain memory from the kernel. |
| TCMALLOC_DEVMEM_START           | default: 0        | Physical memory starting location in MB for /dev/mem allocation. Setting this to 0 disables/dev/mem allocation. |
| TCMALLOC_DEVMEM_LIMIT           | default: 0        | Physical memory limit location in MB for /dev/mem allocation. Setting this to 0 means no limit. |
| TCMALLOC_DEVMEM_DEVICE          | default: /dev/mem | Device to use for allocating unmanaged memory.               |
| TCMALLOC_MEMFS_MALLOC_PATH      | default: ""       | If set, specify a path where hugetlbfs or tmpfs is mounted. This may allow for speedier allocations. |
| TCMALLOC_MEMFS_LIMIT_MB         | default: 0        | Limit total memfs allocation size to specified number of MB. 0 means "no limit". |
| TCMALLOC_MEMFS_ABORT_ON_FAIL    | default: false    | If true, abort() whenever memfs_malloc fails to satisfy an allocation. |
| TCMALLOC_MEMFS_IGNORE_MMAP_FAIL | default: false    | If true, ignore failures from mmap.                          |
| TCMALLOC_MEMFS_MAP_PRVIATE      | default: false    | If true, use MAP_PRIVATE when mapping via memfs, not MAP_SHARED. |

### 2.7 **修改TCmalloc的一些行为**

我们可以通过包含malloc_extension.h头文件中的MallocExtension类提供了一些微调的接口来修改tcmalloc的行为来使得你的程序达到更高的效率。

默认情况下，tcmalloc将逐渐的释放长时间未使用的内存给内核。tcmalloc_release_rate标志控制归还给操作系统内存的速度大，你也可以长治释放内存通过执行如下操作：

MallocExtension::instance()->ReleaseFreeMemory();

你同样可以调用SetMemoryReleaseRate()来在运行时修改tcmalloc_release_rate的值，或者调用GetMemoryReleaseRate来查看当前释放的概率值。

MallocExtension::instance()->SetMemoryReleaseRate(10.0);

// 【0-10】数值越大，回收速度越快，这个需要根据自己项目情况测试给一个最合理的参数。

当然你也可以通过以下接口来获取tcmalloc的相关堆栈使用情况:

```text
MallocExtension::instance()->GetStats(buffer, buffer_length);
```



```text
MallocExtension::instance()->GetHeapSample(&string);
```

MallocExtension::instance()->GetHeapGrowthStacks(&string);

### 2.8 **TCmalloc和PTMalloc的性能参数对比**

PTMalloc2包（现在已经是glibc的一部分了）包含了一个单元测试程序`t-test1.c`。它会产生一定数量的线程并在每个线程中进行一系列分配和解除分配；线程之间没有任何通信除了在内存分配器中同步。

`t-test1`（放在`tests/tcmalloc/`中，编译为`ptmalloc_unittest1`）用一系列不同的线程数量（1～20）和最大分配尺寸（64B～32KB）运行。这些测试运行在一个2.4GHz 双核心Xeon的RedHat 9系统上，并启用了超线程技术， 使用了Linux glibc-2.3.2，每个测试中进行一百万次操作。在每个案例中，一次正常运行，一次使用`LD_PRELOAD=libtcmalloc.so`。

下面的图像显示了TCMalloc对比PTMalloc2在不同的衡量指标下的性能。首先，现实每秒操作次数（百万）以及最大分配尺寸，针对不同数量的线程。用来生产这些图像的原始数据（`time`工具的输出）可以在`t-test1.times.txt`中找到。

![img](https://pic2.zhimg.com/80/v2-6ada46383930160544c6bbd25e7548a1_720w.webp)

![img](https://pic2.zhimg.com/80/v2-165083d6365d014585c5fd8b55890bb1_720w.webp)

- TCMalloc要比PTMalloc2更具有一致地伸缩性——对于所有线程数量>1的测试，小分配达到了约7～9百万操作每秒，大分配降到了约2百万操作每秒。单线程的案例则明显是要被剔除的，因为他只能保持单个处理器繁忙因此只能获得较少的每秒操作数。PTMalloc2在每秒操作数上有更高的方差——某些地方峰值可以在小分配上达到4百万操作每秒，而在大分配上降到了<1百万操作每秒。
- TCMalloc在绝大多数情况下要比PTMalloc2快，并且特别是小分配上。线程间的争用在TCMalloc中问题不大。
- TCMalloc的性能随着分配尺寸的增加而降低。这是因为每线程缓存当它达到了阈值（默认是2MB）的时候会被垃圾收集。对于更大的分配尺寸，在垃圾收集之前只能在缓存中存储更少的对象。
- TCMalloc性能在约32K最大分配尺寸附件有一个明显的下降。这是因为在每线程缓存中的32K对象的最大尺寸；对于大于这个值得对象TCMalloc会从中央页面堆中进行分配。

下面是每秒CPU时间的操作数（百万）以及线程数量的图像，最大分配尺寸64B～128KB。

![img](https://pic3.zhimg.com/80/v2-31cac8fa4a5819158957e4bcbda12292_720w.webp)

![img](https://pic1.zhimg.com/80/v2-796bfdeff224ec5ccaa1215b8f8b7ab0_720w.webp)

这次我们再一次看到TCMalloc要比PTMalloc2更连续也更高效。对于<32K的最大分配尺寸，TCMalloc在大线程数的情况下典型地达到了CPU时间每秒约0.5～1百万操作，同时PTMalloc通常达到了CPU时间每秒约0.5～1百万，还有很多情况下要比这个数字小很多。在32K最大分配尺寸之上，TCMalloc下降到了每CPU时间秒1～1.5百万操作，同时PTMalloc对于大线程数降到几乎只有零（也就是，使用PTMalloc，在高度多线程的情况下，很多CPU时间被浪费在轮流等待锁定上了）。

### 2.9 **关于TCmalloc的一些说明**

对于某些系统，TCMalloc可能无法与没有链接`libpthread.so`（或者你的系统上同等的东西）的应用程序正常工作。它应该能正常工作于使用glibc 2.3的Linux上，但是其他OS/libc的组合方式尚未经过任何测试。

TCMalloc可能要比其他malloc版本在某种程度上更吃内存，（但是倾向于不会有其他malloc版本中可能出现的爆发性增长。）尤其是在启动时TCMalloc会分配大约240KB的内部内存。

不要试图将TCMalloc载入到一个运行中的二进制程序中（例如，在Java中使用JNI）。**二进制程序已经使用系统malloc分配了一些对象，并会尝试将它们传递到TCMalloc进行解除分配**。TCMalloc是无法处理这种对象的。

原文地址：https://zhuanlan.zhihu.com/p/499660314

作者：Linux

# 【NO.504】一文搞懂Linux进程调度原理

## 1.Linux进程调度的目标

1.高效性：高效意味着在相同的时间下要完成更多的任务。调度程序会被频繁的执行，所以调度程序要尽可能的高效；

2.加强交互性能:在系统相当的负载下，也要保证系统的响应时间；

3.保证公平和避免饥渴；

4.SMP调度：调度程序必须支持多处理系统；

5.软实时调度：系统必须有效的调用实时进程，但不保证一定满足其要求；

## **2.Linux进程优先级**

进程提供了两种优先级，一种是普通的进程优先级，第二个是实时优先级。前者适用SCHED_NORMAL调度策略，后者可选SCHED_FIFO或SCHED_RR调度策略。**任何时候，实时进程的优先级都高于普通进程**，实时进程只会被更高级的实时进程抢占，同级实时进程之间是按照FIFO（一次机会做完）或者RR（多次轮转）规则调度的。

### 2.1 **首先，说下实时进程的调度**

实时进程，只有静态优先级，因为内核不会再根据休眠等因素对其静态优先级做调整，其范围在0~MAX_RT_PRIO-1间。默认MAX_RT_PRIO配置为100，也即，默认的实时优先级范围是0~99。而nice值，影响的是优先级在MAX_RT_PRIO~MAX_RT_PRIO+40范围内的进程。

不同与普通进程，系统调度时，实时优先级高的进程总是先于优先级低的进程执行。知道实时优先级高的实时进程无法执行。实时进程总是被认为处于活动状态。如果有数个 优先级相同的实时进程，那么系统就会按照进程出现在队列上的顺序选择进程。假设当前CPU运行的实时进程A的优先级为a，而此时有个优先级为b的实时进程B进入可运行状态，那么只要b<a，系统将中断A的执行，而优先执行B，直到B无法执行（无论A，B为何种实时进程）。

不同调度策略的实时进程只有在相同优先级时才有可比性：

\1. 对于FIFO的进程，意味着只有当前进程执行完毕才会轮到其他进程执行。由此可见相当霸道。

\2. 对于RR的进程。一旦时间片消耗完毕，则会将该进程置于队列的末尾，然后运行其他相同优先级的进程，如果没有其他相同优先级的进程，则该进程会继续执行。

总而言之，对于实时进程，高优先级的进程就是大爷。它执行到没法执行了，才轮到低优先级的进程执行。等级制度相当森严啊。

### 2.2 **重头戏，说下非实时进程调度**

**引子**

> 将当前目录下的documents目录打包，但不希望tar占用太多CPU：
> nice -19 tar zcf pack.tar.gz documents
> 这个“-19”中的“-”仅表示参数前缀；所以，如果希望赋予tar进程最高的优先级，则执行：
> nice --19 tar zcf pack.tar.gz documents
> 也可修改已经存在的进程的优先级：
> 将PID为1799的进程优先级设置为最低：
> renice 19 1799
> renice命令与nice命令的优先级参数的形式是相反的，直接以优先级值作为参数即可，无“-”前缀说法。

**言归正传**

Linux对普通的进程，根据动态优先级进行调度。而动态优先级是由静态优先级（static_prio）调整而来。Linux下，静态优先级是用户不可见的，隐藏在内核中。而内核提供给用户一个可以影响静态优先级的接口，那就是nice值，两者关系如下：

static_prio=MAX_RT_PRIO +nice+ 20

nice值的范围是-20~19，因而静态优先级范围在100~139之间。nice数值越大就使得static_prio越大，最终进程优先级就越低。

ps -el 命令执行结果：NI列显示的每个进程的nice值，PRI是进程的优先级（如果是实时进程就是静态优先级，如果是非实时进程，就是动态优先级）

而进程的时间片就是完全依赖 static_prio 定制的，见下图

![img](https://pic4.zhimg.com/80/v2-176a49440ae5d1cce96ccc25f0184707_720w.webp)

我们前面也说了，系统调度时，还会考虑其他因素，因而会计算出一个叫进程动态优先级的东西，根据此来实施调度。因为，不仅要考虑静态优先级，也要考虑进程的属性。例如如果进程属于交互式进程，那么可以适当的调高它的优先级，使得界面反应地更加迅速，从而使用户得到更好的体验。Linux2.6 在这方面有了较大的提高。Linux2.6认为，交互式进程可以从平均睡眠时间这样一个measurement进行判断。进程过去的睡眠时间越多，则越有可能属于交互式进程。则系统调度时，会给该进程更多的奖励（bonus），以便该进程有更多的机会能够执行。奖励（bonus）从0到10不等。

系统会严格按照动态优先级高低的顺序安排进程执行。动态优先级高的进程进入非运行状态，或者时间片消耗完毕才会轮到动态优先级较低的进程执行。动态优先级的计算主要考虑两个因素：静态优先级，进程的平均睡眠时间也即bonus。计算公式如下，

dynamic_prio = max (100, min (static_prio - bonus + 5, 139))

在调度时，Linux2.6 使用了一个小小的trick，就是算法中经典的空间换时间的思想[**还没对照源码确认**]，使得计算最优进程能够在O(1)的时间内完成。

**为什么根据睡眠和运行时间确定奖惩分数是合理的**

睡眠和CPU耗时反应了进程IO密集和CPU密集两大瞬时特点，不同时期，一个进程可能即是CPU密集型也是IO密集型进程。对于表现为IO密集的进程，应该经常运行，但每次时间片不要太长。对于表现为CPU密集的进程，CPU不应该让其经常运行，但每次运行时间片要长。交互进程为例，假如之前其其大部分时间在于等待CPU，这时为了调高相应速度，就需要增加奖励分。另一方面，如果此进程总是耗尽每次分配给它的时间片，为了对其他进程公平，就要增加这个进程的惩罚分数。

### 2.3 **现代方法CFS**

不再单纯依靠进程优先级绝对值，而是参考其绝对值，综合考虑所有进程的时间，给出当前调度时间单位内其应有的权重，也就是，每个进程的权重X单位时间=应获cpu时间，但是这个应得的cpu时间不应太小（假设阈值为1ms），否则会因为切换得不偿失。但是，当进程足够多时候，肯定有很多不同权重的进程获得相同的时间——最低阈值1ms，所以，CFS只是近似完全公平。

## 3.**Linux进程状态机**

![img](https://pic2.zhimg.com/80/v2-8dc62822cec84f5ee1e9920fdd2e1b59_720w.webp)

进程是通过fork系列的系统调用（fork、clone、vfork）来创建的，内核（或内核模块）也可以通过kernel_thread函数创建内核进程。这些创建子进程的函数本质上都完成了相同的功能——将调用进程复制一份，得到子进程。（可以通过选项参数来决定各种资源是共享、还是私有。）
那么既然调用进程处于TASK_RUNNING状态（否则，它若不是正在运行，又怎么进行调用？），则子进程默认也处于TASK_RUNNING状态。
另外，在系统调用clone和内核函数kernel_thread也接受CLONE_STOPPED选项，从而将子进程的初始状态置为 TASK_STOPPED。

进程创建后，状态可能发生一系列的变化，直到进程退出。而尽管进程状态有好几种，但是进程状态的变迁却只有两个方向——从TASK_RUNNING状态变为非TASK_RUNNING状态、或者从非TASK_RUNNING状态变为TASK_RUNNING状态。总之，TASK_RUNNING是必经之路，不可能两个非RUN状态直接转换。

也就是说，如果给一个TASK_INTERRUPTIBLE状态的进程发送SIGKILL信号，这个进程将先被唤醒（进入TASK_RUNNING状态），然后再响应SIGKILL信号而退出（变为TASK_DEAD状态）。并不会从TASK_INTERRUPTIBLE状态直接退出。

进程从非TASK_RUNNING状态变为TASK_RUNNING状态，是由别的进程（也可能是中断处理程序）执行唤醒操作来实现的。执行唤醒的进程设置被唤醒进程的状态为TASK_RUNNING，然后将其task_struct结构加入到某个CPU的可执行队列中。于是被唤醒的进程将有机会被调度执行。

而进程从TASK_RUNNING状态变为非TASK_RUNNING状态，则有两种途径：

1、响应信号而进入TASK_STOPED状态、或TASK_DEAD状态；
2、执行系统调用主动进入TASK_INTERRUPTIBLE状态（如nanosleep系统调用）、或TASK_DEAD状态（如exit系统调用）；或由于执行系统调用需要的资源得不到满　　　　 足，而进入TASK_INTERRUPTIBLE状态或TASK_UNINTERRUPTIBLE状态（如select系统调用）。
显然，这两种情况都只能发生在进程正在CPU上执行的情况下。

通过ps命令我们能够查看到系统中存在的进程，以及它们的状态：

R(TASK_RUNNING)，可执行状态。

只有在该状态的进程才可能在CPU上运行。而同一时刻可能有多个进程处于可执行状态，这些进程的task_struct结构（进程控制块）被放入对应CPU的可执行队列中（一个进程最多只能出现在一个CPU的可执行队列中）。进程调度器的任务就是从各个CPU的可执行队列中分别选择一个进程在该CPU上运行。
只要可执行队列不为空，其对应的CPU就不能偷懒，就要执行其中某个进程。一般称此时的CPU“忙碌”。对应的，CPU“空闲”就是指其对应的可执行队列为空，以致于CPU无事可做。
有人问，为什么死循环程序会导致CPU占用高呢？因为死循环程序基本上总是处于TASK_RUNNING状态（进程处于可执行队列中）。除非一些非常极端情况（比如系统内存严重紧缺，导致进程的某些需要使用的页面被换出，并且在页面需要换入时又无法分配到内存……），否则这个进程不会睡眠。所以CPU的可执行队列总是不为空（至少有这么个进程存在），CPU也就不会“空闲”。

很多操作系统教科书将正在CPU上执行的进程定义为RUNNING状态、而将可执行但是尚未被调度执行的进程定义为READY状态，这两种状态在linux下统一为 TASK_RUNNING状态。

S(TASK_INTERRUPTIBLE)，可中断的睡眠状态。

处于这个状态的进程因为等待某某事件的发生（比如等待socket连接、等待信号量），而被挂起。这些进程的task_struct结构被放入对应事件的等待队列中。当这些事件发生时（由外部中断触发、或由其他进程触发），对应的等待队列中的一个或多个进程将被唤醒。

通过ps命令我们会看到，一般情况下，进程列表中的绝大多数进程都处于TASK_INTERRUPTIBLE状态（除非机器的负载很高）。毕竟CPU就这么一两个，进程动辄几十上百个，如果不是绝大多数进程都在睡眠，CPU又怎么响应得过来。

D(TASK_UNINTERRUPTIBLE)，不可中断的睡眠状态。

与TASK_INTERRUPTIBLE状态类似，进程处于睡眠状态，但是此刻进程是不可中断的。不可中断，指的并不是CPU不响应外部硬件的中断，而是指进程不响应异步信号。
绝大多数情况下，进程处在睡眠状态时，总是应该能够响应异步信号的。否则你将惊奇的发现，kill -9竟然杀不死一个正在睡眠的进程了！于是我们也很好理解，为什么ps命令看到的进程几乎不会出现TASK_UNINTERRUPTIBLE状态，而总是TASK_INTERRUPTIBLE状态。

而TASK_UNINTERRUPTIBLE状态存在的意义就在于，内核的某些处理流程是不能被打断的。如果响应异步信号，程序的执行流程中就会被插入一段用于处理异步信号的流程（这个插入的流程可能只存在于内核态，也可能延伸到用户态），于是原有的流程就被中断了（参见《linux异步信号handle浅析》）。
在进程对某些硬件进行操作时（比如进程调用read系统调用对某个设备文件进行读操作，而read系统调用最终执行到对应设备驱动的代码，并与对应的物理设备进行交互），可能需要使用TASK_UNINTERRUPTIBLE状态对进程进行保护，以避免进程与设备交互的过程被打断，造成设备陷入不可控的状态。（比如read系统调用触发了一次磁盘到用户空间的内存的DMA，如果DMA进行过程中，进程由于响应信号而退出了，那么DMA正在访问的内存可能就要被释放了。）这种情况下的TASK_UNINTERRUPTIBLE状态总是非常短暂的，通过ps命令基本上不可能捕捉到。

linux系统中也存在容易捕捉的TASK_UNINTERRUPTIBLE状态。执行vfork系统调用后，父进程将进入TASK_UNINTERRUPTIBLE状态，直到子进程调用exit或exec。
通过下面的代码就能得到处于TASK_UNINTERRUPTIBLE状态的进程：

```text
#include <unistd.h>
void main() {
if (!vfork()) sleep(100);
}
```

编译运行，然后ps一下：

```text
kouu@kouu-one:~/test$ ps -ax | grep a\.out
4371 pts/0 D+ 0:00 ./a.out
4372 pts/0 S+ 0:00 ./a.out
4374 pts/1 S+ 0:00 grep a.out
```

然后我们可以试验一下TASK_UNINTERRUPTIBLE状态的威力。不管kill还是kill -9，这个TASK_UNINTERRUPTIBLE状态的父进程依然屹立不倒。

T(TASK_STOPPED or TASK_TRACED)，暂停状态或跟踪状态。

向进程发送一个SIGSTOP信号，它就会因响应该信号而进入TASK_STOPPED状态（除非该进程本身处于TASK_UNINTERRUPTIBLE状态而不响应信号）。（SIGSTOP与SIGKILL信号一样，是非常强制的。不允许用户进程通过signal系列的系统调用重新设置对应的信号处理函数。）
向进程发送一个SIGCONT信号，可以让其从TASK_STOPPED状态恢复到TASK_RUNNING状态。

当进程正在被跟踪时，它处于TASK_TRACED这个特殊的状态。“正在被跟踪”指的是进程暂停下来，等待跟踪它的进程对它进行操作。比如在gdb中对被跟踪的进程下一个断点，进程在断点处停下来的时候就处于TASK_TRACED状态。而在其他时候，被跟踪的进程还是处于前面提到的那些状态。
对于进程本身来说，TASK_STOPPED和TASK_TRACED状态很类似，都是表示进程暂停下来。
而TASK_TRACED状态相当于在TASK_STOPPED之上多了一层保护，处于TASK_TRACED状态的进程不能响应SIGCONT信号而被唤醒。只能等到调试进程通过ptrace系统调用执行PTRACE_CONT、PTRACE_DETACH等操作（通过ptrace系统调用的参数指定操作），或调试进程退出，被调试的进程才能恢复TASK_RUNNING状态。

Z(TASK_DEAD - EXIT_ZOMBIE)，退出状态，进程成为僵尸进程。

进程在退出的过程中，处于TASK_DEAD状态。

在这个退出过程中，进程占有的所有资源将被回收，除了task_struct结构（以及少数资源）以外。于是进程就只剩下task_struct这么个空壳，故称为僵尸。
之所以保留task_struct，是因为task_struct里面保存了进程的退出码、以及一些统计信息。而其父进程很可能会关心这些信息。比如在shell中，$?变量就保存了最后一个退出的前台进程的退出码，而这个退出码往往被作为if语句的判断条件。
当然，内核也可以将这些信息保存在别的地方，而将task_struct结构释放掉，以节省一些空间。但是使用task_struct结构更为方便，因为在内核中已经建立了从pid到task_struct查找关系，还有进程间的父子关系。释放掉task_struct，则需要建立一些新的数据结构，以便让父进程找到它的子进程的退出信息。

父进程可以通过wait系列的系统调用（如wait4、waitid）来等待某个或某些子进程的退出，并获取它的退出信息。然后wait系列的系统调用会顺便将子进程的尸体（task_struct）也释放掉。
子进程在退出的过程中，内核会给其父进程发送一个信号，通知父进程来“收尸”。这个信号默认是SIGCHLD，但是在通过clone系统调用创建子进程时，可以设置这个信号。

通过下面的代码能够制造一个EXIT_ZOMBIE状态的进程：

```text
#include <unistd.h>
void main() {
if (fork())
while(1) sleep(100);
}
```

编译运行，然后ps一下：

```text
kouu@kouu-one:~/test$ ps -ax | grep a\.out
10410 pts/0 S+ 0:00 ./a.out
10411 pts/0 Z+ 0:00 [a.out] <defunct>
10413 pts/1 S+ 0:00 grep a.out
```

只要父进程不退出，这个僵尸状态的子进程就一直存在。那么如果父进程退出了呢，谁又来给子进程“收尸”？
当进程退出的时候，会将它的所有子进程都托管给别的进程（使之成为别的进程的子进程）。托管给谁呢？可能是退出进程所在进程组的下一个进程（如果存在的话），或者是1号进程。所以每个进程、每时每刻都有父进程存在。除非它是1号进程。

1号进程，pid为1的进程，又称init进程。
linux系统启动后，第一个被创建的用户态进程就是init进程。它有两项使命：
1、执行系统初始化脚本，创建一系列的进程（它们都是init进程的子孙）；
2、在一个死循环中等待其子进程的退出事件，并调用waitid系统调用来完成“收尸”工作；
init进程不会被暂停、也不会被杀死（这是由内核来保证的）。它在等待子进程退出的过程中处于TASK_INTERRUPTIBLE状态，“收尸”过程中则处于TASK_RUNNING状态。

X(TASK_DEAD - EXIT_DEAD)，退出状态，进程即将被销毁。

而进程在退出过程中也可能不会保留它的task_struct。比如这个进程是多线程程序中被detach过的进程（进程？线程？参见《linux线程浅析》）。或者父进程通过设置SIGCHLD信号的handler为SIG_IGN，显式的忽略了SIGCHLD信号。（这是posix的规定，尽管子进程的退出信号可以被设置为SIGCHLD以外的其他信号。）
此时，进程将被置于EXIT_DEAD退出状态，这意味着接下来的代码立即就会将该进程彻底释放。所以EXIT_DEAD状态是非常短暂的，几乎不可能通过ps命令捕捉到。



## 4.**一些重要的杂项**

### 4.1 **调度程序的效率**

“优先级”明确了哪个进程应该被调度执行，而调度程序还必须要关心效率问题。调度程序跟内核中的很多过程一样会频繁被执行，如果效率不济就会浪费很多CPU时间，导致系统性能下降。
在linux 2.4时，可执行状态的进程被挂在一个链表中。每次调度，调度程序需要扫描整个链表，以找出最优的那个进程来运行。复杂度为O(n)；
在linux 2.6早期，可执行状态的进程被挂在N(N=140)个链表中，每一个链表代表一个优先级，系统中支持多少个优先级就有多少个链表。每次调度，调度程序只需要从第一个不为空的链表中取出位于链表头的进程即可。这样就大大提高了调度程序的效率，复杂度为O(1)；
在linux 2.6近期的版本中，可执行状态的进程按照优先级顺序被挂在一个红黑树（可以想象成平衡二叉树）中。每次调度，调度程序需要从树中找出优先级最高的进程。复杂度为O(logN)。
那么，为什么从linux 2.6早期到近期linux 2.6版本，调度程序选择进程时的复杂度反而增加了呢？
这是因为，与此同时，调度程序对公平性的实现从上面提到的第一种思路改变为第二种思路（通过动态调整优先级实现）。而O(1)的算法是基于一组数目不大的链表来实现的，按我的理解，这使得优先级的取值范围很小（区分度很低），不能满足公平性的需求。而使用红黑树则对优先级的取值没有限制（可以用32位、64位、或更多位来表示优先级的值），并且O(logN)的复杂度也还是很高效的。

### 4.2 **调度触发的时机**

调度的触发主要有如下几种情况：
1、当前进程（正在CPU上运行的进程）状态变为非可执行状态。
进程执行系统调用主动变为非可执行状态。比如执行nanosleep进入睡眠、执行exit退出、等等；
进程请求的资源得不到满足而被迫进入睡眠状态。比如执行read系统调用时，磁盘高速缓存里没有所需要的数据，从而睡眠等待磁盘IO；
进程响应信号而变为非可执行状态。比如响应SIGSTOP进入暂停状态、响应SIGKILL退出、等等；
2、抢占。进程运行时，非预期地被剥夺CPU的使用权。这又分两种情况：进程用完了时间片、或出现了优先级更高的进程。
优先级更高的进程受正在CPU上运行的进程的影响而被唤醒。如发送信号主动唤醒，或因为释放互斥对象（如释放锁）而被唤醒；
内核在响应时钟中断的过程中，发现当前进程的时间片用完；
内核在响应中断的过程中，发现优先级更高的进程所等待的外部资源的变为可用，从而将其唤醒。比如CPU收到网卡中断，内核处理该中断，发现某个socket可读，于是唤醒正在等待读这个socket的进程；再比如内核在处理时钟中断的过程中，触发了定时器，从而唤醒对应的正在nanosleep系统调用中睡眠的进程；

### **4.3 内核抢占**

理想情况下，只要满足“出现了优先级更高的进程”这个条件，当前进程就应该被立刻抢占。但是，就像多线程程序需要用锁来保护临界区资源一样，内核中也存在很多这样的临界区，不大可能随时随地都能接收抢占。
linux 2.4时的设计就非常简单，内核不支持抢占。进程运行在内核态时（比如正在执行系统调用、正处于异常处理函数中），是不允许抢占的。必须等到返回用户态时才会触发调度（确切的说，是在返回用户态之前，内核会专门检查一下是否需要调度）；
linux 2.6则实现了内核抢占，但是在很多地方还是为了保护临界区资源而需要临时性的禁用内核抢占。
也有一些地方是出于效率考虑而禁用抢占，比较典型的是spin_lock。spin_lock是这样一种锁，如果请求加锁得不到满足（锁已被别的进程占有），则当前进程在一个死循环中不断检测锁的状态，直到锁被释放。
为什么要这样忙等待呢？因为临界区很小，比如只保护“i+=j++;”这么一句。如果因为加锁失败而形成“睡眠-唤醒”这么个过程，就有些得不偿失了。
那么既然当前进程忙等待（不睡眠），谁又来释放锁呢？其实已得到锁的进程是运行在另一个CPU上的，并且是禁用了内核抢占的。这个进程不会被其他进程抢占，所以等待锁的进程只有可能运行在别的CPU上。（如果只有一个CPU呢？那么就不可能存在等待锁的进程了。）
而如果不禁用内核抢占呢？那么得到锁的进程将可能被抢占，于是可能很久都不会释放锁。于是，等待锁的进程可能就不知何年何月得偿所望了。
对于一些实时性要求更高的系统，则不能容忍spin_lock这样的东西。宁可改用更费劲的“睡眠-唤醒”过程，也不能因为禁用抢占而让更高优先级的进程等待。比如，嵌入式实时linux montavista就是这么干的。
由此可见，实时并不代表高效。很多时候为了实现“实时”，还是需要对性能做一定让步的。

### 4.4 **多处理器下的负载均衡**

前面我们并没有专门讨论多处理器对调度程序的影响，其实也没有什么特别的，就是在同一时刻能有多个进程并行地运行而已。那么，为什么会有“多处理器负载均衡”这个事情呢？
如果系统中只有一个可执行队列，哪个CPU空闲了就去队列中找一个最合适的进程来执行。这样不是很好很均衡吗？
的确如此，但是多处理器共用一个可执行队列会有一些问题。显然，每个CPU在执行调度程序时都需要把队列锁起来，这会使得调度程序难以并行，可能导致系统性能下降。而如果每个CPU对应一个可执行队列则不存在这样的问题。
另外，多个可执行队列还有一个好处。这使得一个进程在一段时间内总是在同一个CPU上执行，那么很可能这个CPU的各级cache中都缓存着这个进程的数据，很有利于系统性能的提升。
所以，在linux下，每个CPU都有着对应的可执行队列，而一个可执行状态的进程在同一时刻只能处于一个可执行队列中。
于是，“多处理器负载均衡”这个麻烦事情就来了。内核需要关注各个CPU可执行队列中的进程数目，在数目不均衡时做出适当调整。什么时候需要调整，以多大力度进程调整，这些都是内核需要关心的。当然，尽量不要调整最好，毕竟调整起来又要耗CPU、又要锁可执行队列，代价还是不小的。
另外，内核还得关心各个CPU的关系。两个CPU之间，可能是相互独立的、可能是共享cache的、甚至可能是由同一个物理CPU通过超线程技术虚拟出来的……CPU之间的关系也是实现负载均衡的重要依据。关系越紧密，进程在它们之间迁移的代价就越小。参见《linux内核SMP负载均衡浅析》。

### 4.5 **优先级继承**

由于互斥，一个进程（设为A）可能因为等待进入临界区而睡眠。直到正在占有相应资源的进程（设为B）退出临界区，进程A才被唤醒。
可能存在这样的情况：A的优先级非常高，B的优先级非常低。B进入了临界区，但是却被其他优先级较高的进程（设为C）抢占了，而得不到运行，也就无法退出临界区。于是A也就无法被唤醒。
A有着很高的优先级，但是现在却沦落到跟B一起，被优先级并不太高的C抢占，导致执行被推迟。这种现象就叫做优先级反转。
出现这种现象是很不合理的。较好的应对措施是：当A开始等待B退出临界区时，B临时得到A的优先级（还是假设A的优先级高于B），以便顺利完成处理过程，退出临界区。之后B的优先级恢复。这就是优先级继承的方法。

### 4.6 **中断处理线程化**

在linux下，中断处理程序运行于一个不可调度的上下文中。从CPU响应硬件中断自动跳转到内核设定的中断处理程序去执行，到中断处理程序退出，整个过程是不能被抢占的。
一个进程如果被抢占了，可以通过保存在它的进程控制块（task_struct）中的信息，在之后的某个时间恢复它的运行。而中断上下文则没有task_struct，被抢占了就没法恢复了。
中断处理程序不能被抢占，也就意味着中断处理程序的“优先级”比任何进程都高（必须等中断处理程序完成了，进程才能被执行）。但是在实际的应用场景中，可能某些实时进程应该得到比中断处理程序更高的优先级。
于是，一些实时性要求更高的系统就给中断处理程序赋予了task_struct以及优先级，使得它们在必要的时候能够被高优先级的进程抢占。但是显然，做这些工作是会给系统造成一定开销的，这也是为了实现“实时”而对性能做出的一种让步。

原文地址：https://zhuanlan.zhihu.com/p/348813914

作者：Linux

# 【NO.505】盘点后端开发那些值得学习的优秀开源项目

今天给大家推荐一些值得学习的**开源项目**，包括**C, C++，Golang，Java**等后台开发主流语言的项目，大家工作之余，可以花点时间学习和研究这些项目的优秀**设计和实现，提高自己**。

![img](https://pic2.zhimg.com/80/v2-20533bd51d4ff1c43f9e023562a757e9_720w.webp)

## 1.**学习开源项目好处**

**首先是提升编程技能。**对计算机专业相关的学生而言，在学习编程后，能验证能力的只是一些简单项目，但通过阅读开源项目的源码，你不仅可以学习顶级项目的设计思路，还可以学习顶级开发者的编程思路，比如通过学习提升代码的可读性和简洁性。同时，你也可以提交PR、注释，而社区里的资深工程师会给出直接反馈，这比你自己摸索要成长得更快。

**其次能帮你找到满意的工作。**如果你在开源项目上留下印记，无论是贡献代码、技术文档、应用案例等等，这些都能证明个人能力，甚至，有时你的简历只需放上GitHub个人账号链接就已足够：）

**最后，开源也许会成为你热衷的事业。**处在这样一个开源崛起的时代，尤其在国内很多顶级项目不断催生，现在正是那些热爱开源理念和开源软件的开发者大展鸿图的时候，他们有的在学生时代就已学习和贡献开源，开源世界为他们带来了荣誉和快乐，而他们在未来也致力于开发和运营开源软件。

**总之**，对于高校学生或者已经工作几年同学，只要你能通过开源项目的代码证明自己的实力，这无疑像是拿到了观看球赛的前排门票，你不会再因为“内卷”而发愁，因为你的前方视野足够辽阔。

## 2.**如何学习开源项目**

### 2.1 **首先了解整体架构**

查找和阅读该项目的博客和资料，通过google你能找到某个项目大体介绍的博客，快速阅读一下就能对项目的目的、功能、基本使用有个大概的了解。

### 2.2 **先把项目跑起来**

如果该项目有提供现成的example工程，首先尝试按照开始文档的介绍运行example，如果运行顺利，那么恭喜你顺利开了个好头;如果遇到问题，首先尝试在项目的FAQ等文档里查找答案，再次，可以将问题（例如异常信息）当成关键词去搜索，查找相关的解决办法，你遇到了，别人一般也会遇到，热心的朋友会记录下解决的过程;最后，可以将问题提交到项目的邮件列表，请大家帮你看看。在没有成功运行example之前，不要尝试修改example。

如果时间允许，尝试从源码构建该项目。通常开源项目都会提供一份构建指南，指导你如何搭建一个用于开发、调试和构建的环境。尝试构建一个版本。

### 2.3 **阅读源码建议**

（1）阅读源码之前，查看该项目是否提供架构和设计文档，阅读这些文档可以了解该项目的大体设计和结构，读源码的时候不会无从下手。
（2）阅读源码之前，一定要能构建并运行该项目，有个直观感受。
（3）阅读源码的第一步是抓主干，尝试理清一次正常运行的代码调用路径，这可以通过debug来观察运行时的变量和行为。修改源码加入日志和打印可以帮助你更好的理解源码。
（4）适当画图来帮助你理解源码，在理清主干后，可以将整个流程画成一张流程图或者标准的UML图，帮助记忆和下一步的阅读。
（5）挑选感兴趣的“枝干”代码来阅读，比如你对网络通讯感兴趣，就阅读网络层的代码，深入到实现细节，如它用了什么库，采用了什么设计模式，为什么这样做等。如果可以，debug细节代码。
（6）阅读源码的时候，重视单元测试，尝试去运行单元测试，基本上一个好的单元测试会将该代码的功能和边界描述清楚。
（7）在熟悉源码后，发现有可以改进的地方，有精力、有意愿可以向该项目的开发者提出改进的意见或者issue，甚至帮他修复和实现，参与该项目的发展。

### 2.4 **开启自己的开源项目**

通常在阅读文档和源码之后，你能对该项目有比较深入的了解了，但是该项目所在领域，你可能还想搜索相关的项目和资料，看看有没有其他的更好的项目或者解决方案。在广度和深度之间权衡。

## **3.C经典开源项目**

### 3.1 Libev**

**libev**是一个全功能和高性能的事件驱动库，基于epoll，kqueue等OS提供的基础设施。其以高效出名，它可以将IO事件，定时器，和信号统一起来，统一放在事件处理这一套框架下处理。基于Reactor模式，效率较高，并且代码精简（4.15版本8000多行），是学习事件驱动编程的很好的资源。

**特点**

- 不使用全局变量，而是每个函数都有一个循环上下文。
- 对每种事件类型使用小的观察器(一个I/O观察器在x86_64机器上使用56字节，而用libevent的话使用136字节)。
- 没有http库等组件。libev的功能非常少。
- 允许更多事件类型，例如基于wall clock或者单调时间的定时器、线程间中断等等。

更简单地说，libev的设计遵循UNIX工具箱的哲学，尽可能好地只做一件事。

**整体架构：**

![img](https://pic2.zhimg.com/80/v2-41c7ed2c6fcef13c6291b546fd7d8945_720w.webp)

开源地址：

[https://github.com/enki/libev](https://link.zhihu.com/?target=https%3A//github.com/enki/libev)

### 3.2**** **Redis**

![img](https://pic4.zhimg.com/80/v2-bb085ee23e9cfe4ac1057d9f1d27859f_720w.webp)

**Redis** 是一种经典的开源内存**Key-Value**数据结构存储，用作数据库、缓存和消息代理。Redis 提供了数据结构，例如字符串、散列、列表、集合、带有范围查询的排序集合、位图、超级日志、地理空间索引和流。Redis 内置复制、Lua 脚本、LRU 驱逐、事务和不同级别的磁盘持久化，并通过 Redis Sentinel 和 Redis Cluster 自动分区提供高可用性。

代码架构：

![img](https://pic1.zhimg.com/80/v2-d122b32903c0dc0e3fd85d9ecaff396c_720w.webp)

开源地址：

[https://github.com/redis/redis](https://link.zhihu.com/?target=https%3A//github.com/redis/redis)

### **3.3 Nginx**

![img](https://pic1.zhimg.com/80/v2-9b2265bd95e8663e05142b0b0dddb3e0_720w.webp)

**Nginx**是一款轻量级的Web服务器、反向代理服务器，由于它的内存占用少，启动极快，高并发能力强，在互联网项目中广泛应用。

**特点：**

- Nginx可以部署在网络上使用FastCGI脚本、SCGI处理程序、WSGI应用服务器或Phusion Passenger模块的动态HTTP内容，并可作为软件负载均衡器。
- Nginx使用异步事件驱动的方法来处理请求。Nginx的模块化事件驱动架构可以在高负载下提供更可预测的性能。
- Nginx是一款面向性能设计的HTTP服务器，相较于Apache、lighttpd具有占有内存少，稳定性高等优势。与旧版本（≤2.2）的Apache不同，Nginx不采用每客户机一线程的设计模型，而是充分使用异步逻辑从而削减了上下文调度开销，所以并发服务能力更强。整体采用模块化设计，有丰富的模块库和第三方模块库，配置灵活。在Linux操作系统下，Nginx使用epoll事件模型，得益于此，Nginx在Linux操作系统下效率相当高。同时Nginx在OpenBSD或FreeBSD操作系统上采用类似于epoll的高效事件模型kqueue。

**整体架构:**

![img](https://pic4.zhimg.com/80/v2-bf7b9175b368e0f44fa52d6230252957_720w.webp)

开源地址：

[https://github.com/nginx/nginx](https://link.zhihu.com/?target=https%3A//github.com/nginx/nginx)

### **3.4 SQLite**

**SQLite**是一个开源的嵌入式关系数据库，实现自包容、零配置、支持事务的SQL数据库引擎。其特点是高度便携、使用方便、结构紧凑、高效、可靠。足够小，大致3万行C代码，250K。

**整体架构：**

![img](https://pic2.zhimg.com/80/v2-eb085a37bd856b4c597a76384dd60941_720w.webp)

开源地址：

[http://www.sqlite.org/](https://link.zhihu.com/?target=http%3A//www.sqlite.org/)

### **3.5 Linux**

**Linux** 是一套免费使用和自由传播的类 Unix 操作系统，是一个基于 POSIX 和 UNIX 的多用户、多任务、支持多线程和多 CPU 的操作系统。Linux 能运行主要的 UNIX 工具软件、应用程序和网络协议。它支持 32 位和 64 位硬件。Linux 继承了 Unix 以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统，目前最为流行后台服务器操作系统。

**整体架构：**

![img](https://pic3.zhimg.com/80/v2-126df77e94e6c6aca494cd9f4432839a_720w.webp)

Linux内核学习分为四个阶段。

- 首先，了解操作系统基本概念。
- 其次，了解Linux内核机制（大的框架和架构，不要在乎细节）。
- 其次，研读内核源码（选择自己感兴趣的方向，比如调度（计算），虚拟化，网络，内存，存储等）。

最后，确定个人的发展方向

- 设备驱动开发方向（嵌入式）
- 云网络开发方向（云计算）
- 虚拟化方向（云计算）
- 云存储方向（云计算）
- Linux应用开发方向（Linux后台开发）

开源地址：

[https://www.kernel.org/](https://link.zhihu.com/?target=https%3A//www.kernel.org/)

## 4.**C++开源项目**

### **4.1 TinyWebServer（初学者）**

这是一个帮助初学者快速实现网络编程、搭建属于自己的轻量级Web服务器的小项目。

**项目虽小但真的五脏俱全：**

- 使用线程池、非阻塞Socket、epoll（ET/LT均实现）、事件处理（Reactor及模拟Proactor）的并发模型。
- 使用状态机解析HTTP请求报文，支持解析GET和POST请求
- 访问服务器数据库实现web端用户注册、登录功能，可以请求服务器图片和视频文件
- 实现同步/异步日志系统，记录服务器运行状态
- 经Webbench压力测试可以实现上万的并发连接数据交换

代码地址：

[https://github.com/qinguoyi/TinyWebServer](https://link.zhihu.com/?target=https%3A//github.com/qinguoyi/TinyWebServer)

### **4.2 sylar**

C++高性能分布式服务器框架,功能最全webserver/websocket server,自定义tcp_server（包含日志模块，配置模块，线程模块，协程模块，协程调度模块，io协程调度模块，hook模块，socket模块，bytearray序列化，http模块，TcpServer模块，Websocket模块，Https模块等, Smtp邮件模块, MySQL, SQLite3, ORM,Redis,Zookeeper)。

**优点：**

- 基于epoll的IO复用机制实现Reactor模式，采用边缘触发（ET）模式，和非阻塞模式
- 由于采用ET模式，read、write和accept的时候必须采用循环的方式，直到error==EAGAIN为止，防止漏读等清况，这样的效率会比LT模式高很多，减少了触发次数
- Version-0.1.0基于单线程实现，Version-0.2.0利用线程池实现多IO线程，Version-0.3.0实现通用worker线程池，基于one loop per thread的IO模式，Version-0.4.0增加定时器，Version-0.5.0增加简易协程实现和异步日志实现
- 线程模型将划分为主线程、IO线程和worker线程，主线程接收客户端连接（accept），并通过Round-Robin策略分发给IO线程，IO线程负责连接管理（即事件监听和读写操作），worker线程负责业务计算任务（即对数据进行处理，应用层处理复杂的时候可以开启）基于时间轮实现定时器功能，定时剔除不活跃连接，时间轮的插入、删除复杂度为O(1)，执行复杂度取决于每个桶上的链表长
- 采用智能指针管理多线程下的对象资源增加简易协程实现，目前版本基于ucontext.h（供了解学习，尚未应用到本项目中）From:
- simple-coroutine增加简易C++异步日志库 From: simple-log
- 支持HTTP长连接
- 支持优雅关闭连接
- 通常情况下，由客户端主动发起FIN关闭连接客户端发送FIN关闭连接后，服务器把数据发完才close，而不是直接暴力close,如果连接出错，则服务器可以直接close.

代码地址：

[https://github.com/sylar-yin/sylar](https://link.zhihu.com/?target=https%3A//github.com/sylar-yin/sylar)

### **4.3 OpenSSL**

一个强大的安全套接字层密码库，加密HTTPS，加密SSH都贼好用，同时它还可以用于跨平台密码工具。

**OpenSSL实现了以下功能：**

- 数据保密性：信息加密就是把明码的输入文件用加密算法转换成加密的文件以实现数据的保密。加密的过程需要用到密钥来加密数据然后再解密。
- 数据完整性：加密也能保证数据的一致性。例如：消息验证码（MAC），能够校验用户提供的加密信息，接收者可以用MAC来校验加密数据，保证数据在传输过程中没有被篡改过。
- 安全验证：加密的另外一个用途是用来作为个人的标识，用户的密钥可以作为他的安全验证的标识。SSL是利用公开密钥的加密技术（RSA）来作为用户端与服务器端在传送机密资料时的加密通讯协定。

代码地址：

[https://www.openssl.org/source](https://link.zhihu.com/?target=https%3A//www.openssl.org/source)

### **4.4 LevelDB**

**LevelDB** 是一个由 Google 编写的快速键值存储库，它提供了从字符串键到字符串值的有序映射。

**LevelDB 有以下优点：**

- 提供应用程序运行上下文，方便跟踪调试
- 可扩展的、多种方式记录日志，包括命令行、文件、回卷文件、内存、syslog服务器、Win事件日志等
- 可以动态控制日志记录级别，在效率和功能中进行调整
- 所有配置可以通过配置文件进行动态调整
- 支持Java、C++、C、python等多种语言

整体架构：

![img](https://pic2.zhimg.com/80/v2-631342e43a7a38060fafdead35125505_720w.webp)

- MemTable：内存数据结构，具体实现是 SkipList。接受用户的读写请求，新的数据修改会首先在这里写入。
- Immutable MemTable：当 MemTable 的大小达到设定的阈值时，会变成 Immutable MemTable，只接受读操作，不再接受写操作，后续由后台线程 Flush 到磁盘上。
- SST Files：Sorted String Table Files，磁盘数据存储文件。分为 Level0 到 LevelN 多层，每一层包含多个 SST 文件，文件内数据有序。Level0 直接由 Immutable Memtable Flush 得到，其它每一层的数据由上一层进行 Compaction 得到。
- Manifest Files：Manifest 文件中记录 SST 文件在不同 Level 的分布，单个 SST 文件的最大、最小 key，以及其他一些 LevelDB 需要的元信息。由于 LevelDB 支持 snapshot，需要维护多版本，因此可能同时存在多个 Manifest 文件。
- Current File：由于 Manifest 文件可能存在多个，Current 记录的是当前的 Manifest 文件名。
- Log Files (WAL)：用于防止 MemTable 丢数据的日志文件。

开源地址：

[https://github.com/google/leveldb](https://link.zhihu.com/?target=https%3A//github.com/google/leveldb)

### **4.5 Chromium**

Chromium是由Google主导开发的网页浏览器。以BSD许可证等多重自由版权发行并开放源代码，Chromium的开发可能早自2006年即开始. Chromium 是Google 的Chrome浏览器背后的引擎，其目的是为了创建一个安全、稳定和快速的通用浏览器.

整体架构：

![img](https://pic2.zhimg.com/80/v2-d511fbc9f53f0f4e587926a21b5c1665_720w.webp)

chromium的代码目录包含这些模块：

base：通用代码集和基础组件实现库，包含字符串、文件、线程、消息队列等工具类集合。

cc：负责渲染绘制，chrome为什么高效就是因为有它。chrome：浏览器界面模块，大量调用了cc提供的接口。

content：多进程沙盒浏览器莫款，管理多进程和多线程。

gpu，OpenGL封装实现：CommandBuffer和OpenGL的兼容支持模块。

net：网络功能实现模块。

media：多媒体封装代码，实现视频播放等功能。

mojo：跨语言（C++ / Java / JavaScript）跨平台的进程间对象通信模块，类似AIDL的功能。

skia：图形库。

third_party：排版引擎。

ui：UI库。

ipc: 网络进程通信模块。

v8，V8 JavaScript 引擎库。

以上每一个模块要想真正理解，都得花很大的功夫，简单用一张图来说明以上模块的关系：

![img](https://pic2.zhimg.com/80/v2-8106af5a2699ee24fcc09d7109f2c311_720w.webp)

开源地址：

[https://chromium.googlesource.com/chromium/src.git](https://link.zhihu.com/?target=https%3A//chromium.googlesource.com/chromium/src.git)

## 5.**Go经典开源项目**

Golang有哪些好像优秀的项目呢？列举一下我收集到的golang开发的优秀项目。

### **5.1 docker**

golang头号优秀项目，通过虚拟化技术实现的操作系统与应用的隔离，也称为容器。

**特点：**

- Docker是世界领先的软件容器平台。
- Docker使用Google公司推出的Go语言进行开发实现，基于Linux内核的cgroup，namespace，以及AUFS类的UnionFS等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。Docke最初实现是基于LXC。
- Docker能够自动执行重复性任务，例如搭建和配置开发环境，从而解放了开发人员以便他们专注在真正重要的事情上：构建杰出的软件。
- 用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。

整体架构：

![img](https://pic1.zhimg.com/80/v2-66c993a90b61237a774f4014f391ec38_720w.webp)

开源地址：

[https://github.com/docker](https://link.zhihu.com/?target=https%3A//github.com/docker)

### 5.2 kubernetes

**Kubernetes**（常简称为**K8s**）是用于自动部署、扩展和管理“容器化（containerized）应用程序”的开源系统。

**特点：**

- 跨主机编排容器
- 更充分地利用硬件资源来最大化地满足企业应用的需求
- 可移植 : 支持公有云,私有云,混合云,多重云
- 可扩展 : 模块化,插件化,可挂载,可组合,支持各种形式的扩展
- 自动化 : 自动部署,自动重启,自动复制,自动伸缩/扩展,通过声明式语法提供了

**整体架构:**

![img](https://pic1.zhimg.com/80/v2-93c0f887c54ffa08f1ddeb8f51a1d164_720w.webp)

- etcd保存了整个集群的状态；
- apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制；
- controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；
- scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上；
- kubelet负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理；
- Container runtime负责镜像管理以及Pod和容器的真正运行（CRI）；
- kube-proxy负责为Service提供cluster内部的服务发现和负载均衡；

**Kubernetes**设计理念和功能其实就是一个类似Linux的**分层架构**，如下图所示：

![img](https://pic3.zhimg.com/80/v2-321f4e84ed5f66b96d3ea39b8dacac4e_720w.webp)

- 核心层：Kubernetes最核心的功能，对外提供API构建高层的应用，对内提供插件式应用执行环境
- 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS解析等）
- 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态Provision等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy等）
- 接口层：kubectl命令行工具、客户端SDK以及集群联邦
- 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴Kubernetes外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS应用、ChatOps等Kubernetes内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等

开源地址：

[https://github.com/kubernetes/kubernetes](https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes)

### **5.3 etcd**

etcd 是 CoreOS 团队于 2013 年 6 月发起的开源项目，它的目标是构建一个高可用的分布式键值(key-value)数据库。

**特点：**

- 简单：定义明确、面向用户的 API (gRPC)
- 安全：具有可选客户端证书身份验证的自动 TLS
- 快速：基准测试为 10,000 次写入/秒
- 可靠：使用 Raft 正确分布
- etcd 是用 Go 编写的，使用Raft共识算法来管理高可用的复制日志。
- 许多公司在生产中使用 etcd ，在关键部署场景中，开发团队支持它，在这些场景中，etcd 经常与Kubernetes、locksmith、vulcand、Doorman等应用程序合作。严格的测试进一步确保了可靠性。

整体架构：

![img](https://pic2.zhimg.com/80/v2-81803900b67e866fc5e5885a0faffb41_720w.webp)

- httpserver
  etcd node之间进行通信，接收来自其他node的消息；

- raft
  实现分布式一致性raft协议, raft模块与server模块的通信采用了四个channel：

- - propc：处理client来的命令
  - recvc：处理http消息
  - readyc: 消息经过raft处理之后封装成Ready交给server处理
  - advanceC：server处理一条消息之后通知raft

- WAL
  server为了防止数据丢失而实现的write ahead log，与很多数据库的实现类似

- snapshotter防止wal的无限制增长，定期生成snap文件仅保留 term，index以及key value data；

- mvcc实现多版本的并发控制，使用revision（main和sub）来描述一个key的整个过程，从创建到删除。mvcc中还包含了watcher，用于实现监听key，prefix， range的变化。

- backend & boltdb持久化key value到boltdb数据库

- raftlograftlog模块包含unstable和raft的snapshot，unstable保存log entries，但是entries数量比较多的时候，就需要compact，创建一个snapshot，这里的snapshot还是保存在memory中的。raft模块会定时收集entries交给server处理。

开源地址：

[https://github.com/etcd-io/etcd](https://link.zhihu.com/?target=https%3A//github.com/etcd-io/etcd)

### **5.4 Tidb**

TiDB（“Ti”代表 Titanium）是一个开源的 NewSQL 数据库，支持混合事务和分析处理 (HTAP) 工作负载。它兼容 MySQL，具有水平可扩展性、强一致性和高可用性。

**特点：**

- 水平可扩展性TiDB 通过简单地添加新节点来扩展 SQL 处理和存储。这使得基础设施容量规划比仅垂直扩展的传统关系数据库更容易且更具成本效益。
- MySQL 兼容语法TiDB 就像是您的应用程序的 MySQL 5.7 服务器。您可以继续使用所有现有的 MySQL 客户端库，并且在许多情况下，您不需要更改应用程序中的任何一行代码。由于 TiDB 是从头开始构建的，而不是 MySQL 的 fork，请查看已知兼容性差异列表。
- 分布式事务TiDB 在内部将表分片成基于范围的小块，我们称之为“区域”。每个 Region 默认大小约为 100 MiB，TiDB 使用优化的两阶段提交来确保 Region 以事务一致的方式维护。
- 云原生TiDB 旨在在云中工作——公共、私有或混合——使部署、供应、操作和维护变得简单。TiDB 的存储层，称为 TiKV，是一个Cloud Native Computing Foundation (CNCF) 毕业项目。TiDB 平台的架构还允许 SQL 处理和存储以非常云友好的方式相互独立扩展。
- 最小化 ETLTiDB 旨在支持事务处理 (OLTP) 和分析处理 (OLAP) 工作负载。这意味着，虽然传统上您可能在 MySQL 上进行交易，然后将 (ETL) 数据提取、转换和加载到列存储中以进行分析处理，但不再需要此步骤。
- 高可用性TiDB 使用 Raft 共识算法来确保数据在 Raft 组中的整个存储中的高可用和安全复制。如果发生故障，Raft 组会自动为故障成员选举新的领导者，并在无需任何人工干预的情况下自愈 TiDB 集群。故障和自愈操作对应用程序也是透明的。

**整体架构：**

![img](https://pic4.zhimg.com/80/v2-79ad76aeaaab221a7ecec93b582da97b_720w.webp)

开源地址：

[https://github.com/pingcap/tidb](https://link.zhihu.com/?target=https%3A//github.com/pingcap/tidb)

### **5.5 Netpoll vs gnet**

**Netpoll**是字节跳动内部的 Golang 高性能、I/O 非阻塞的网络库，专注于 RPC 场景。

开源社区目前缺少专注于 RPC 方案的 Go 网络库。类似的项目如：evio、gnet 等，均面向 Redis、Haproxy 这样的场景。因此 Netpoll 应运而生，它借鉴了 evio 和 Netty 的优秀设计，具有出色的性能，更适用于微服务架构。

整体架构：

![img](https://pic3.zhimg.com/80/v2-3a16bb3fd275e33a5cba7e1001248df6_720w.webp)

- netpoll 将 Reactor 以 1:N 的形式组合成主从模式。
- MainReactor 主要管理 Listener，负责监听端口，建立新连接；
- SubReactor 负责管理 Connection，监听分配到的所有连接，并将所有触发的事件提交到协程池里进行处理。
- netpoll 在 I/O Task 中引入了主动的内存管理，向上层提供 NoCopy 的调用接口，由此支持 NoCopy RPC。
- 使用协程池集中处理 I/O Task，减少 goroutine 数量和调度开销。

开源地址：

[https://github.com/cloudwego/netpoll](https://link.zhihu.com/?target=https%3A//github.com/cloudwego/netpoll)

### **5.6 gnet**

![img](https://pic4.zhimg.com/80/v2-0ae62978df369cdf9d0cadd4a0ce29bf_720w.webp)

gnet的卖点在于它是一个高性能、轻量级、非阻塞的纯 Go 实现的传输层（TCP/UDP/Unix Domain Socket）网络框架，开发者可以使用 gnet 来实现自己的应用层网络协议(HTTP、RPC、Redis、WebSocket 等等)，从而构建出自己的应用层网络应用：比如在 gnet 上实现 HTTP 协议就可以创建出一个 HTTP 服务器 或者 Web 开发框架，实现 Redis 协议就可以创建出自己的 Redis 服务器等等。

gnet，在某些极端的网络业务场景，比如海量连接、高频短连接、网络小包等等场景，gnet 在性能和资源占用上都远超 Go 原生的 net 包（基于 netpoller）。

![img](https://pic1.zhimg.com/80/v2-6c1ce8f5f1a3044ff36ca14d62942d60_720w.webp)

主从 Reactors + Goroutine Pool 模型

**功能**

- [x] 高性能的基于多线程/Go程网络模型的 event-loop 事件驱动
- [x] 内置 goroutine 池，由开源库 ants 提供支持
- [x] 内置 bytes 内存池，由开源库 bytebufferpool 提供支持
- [x] 整个生命周期是无锁的
- [x] 简单易用的 APIs
- [x] 基于 Ring-Buffer 的高效且可重用的内存 buffer
- [x] 支持多种网络协议/IPC 机制：TCP、UDP 和 Unix Domain Socket
- [x] 支持多种负载均衡算法：Round-Robin(轮询)、Source-Addr-Hash(源地址哈希) 和 Least-Connections(最少连接数)
- [x] 支持两种事件驱动机制：**「Linux」** 里的 epoll 以及 **「FreeBSD/DragonFly/Darwin」** 里的 kqueue
- [x] 支持异步写操作
- [x] 灵活的事件定时器
- [x] SO_REUSEPORT 端口重用
- [x] 内置多种编解码器，支持对 TCP 数据流分包：LineBasedFrameCodec, DelimiterBasedFrameCodec,FixedLengthFrameCodec和LengthFieldBasedFrameCodec，参考自 netty codec，而且支持自定制编解码器
- [x] 支持 Windows 平台，Go 标准网络库
- [ ] 实现 gnet 客户端

Github:

[https://github.com/panjf2000/gnet](https://link.zhihu.com/?target=https%3A//github.com/panjf2000/gnet)

## **6.Java经典开源项目**

这里推荐一些最值得阅读优秀的Java开源项目。

### 6.1 Netty

**Netty**是一个Java NIO技术的开源异步事件驱动的网络编程框架，用于快速开发可维护的高性能协议服务器和客户端。

![img](https://pic4.zhimg.com/80/v2-0cd55546f651d87c4ad757515225a1ff_720w.webp)

往通俗了讲，可以将Netty理解为：一个将Java NIO进行了大量封装，并大大降低Java NIO使用难度和上手门槛的超牛逼框架。

**特点：**

**设计**

- 各种传输类型的统一 API - 阻塞和非阻塞套接字
- 基于灵活和可扩展的事件模型，允许清晰的关注点分离
- 高度可定制的线程模型——单线程、一个或多个线程池，如 SEDA
- 真正的无连接数据报套接字支持（自 3.1 起）

**便于使用**

- 有据可查的 Javadoc、用户指南和示例

- 没有额外的依赖，JDK 5 (Netty 3.x) 或 6 (Netty 4.x) 就足够了

- - 注意：某些组件（例如 HTTP/2）可能有更多要求。 有关更多信息，请参阅 要求页面。

**表现**

- 更高的吞吐量，更低的延迟
- 更少的资源消耗
- 最小化不必要的内存复制

**安全**

- 完整的 SSL/TLS 和 StartTLS 支

开源地址：

[https://github.com/netty/netty](https://link.zhihu.com/?target=https%3A//github.com/netty/netty)

### **6.2 J2EE框架 Spring**

star:45.1k; fork:31.8k

![img](https://pic2.zhimg.com/80/v2-2dede4cf851558edd585e1c7d4354251_720w.webp)

**Spring Framework** 是一个开源的Java/Java EE全功能栈（full-stack）的应用程序框架，以Apache许可证形式发布，也有.NET平台上的移植版本。该框架基于 Expert One-on-One Java EE Design and Development（ISBN 0-7645-4385-7）一书中的代码，最初由 Rod Johnson 和 Juergen Hoeller等开发。Spring Framework 提供了一个简易的开发方式，这种开发方式，将避免那些可能致使底层代码变得繁杂混乱的大量的属性文件和帮助类。

**Spring 中包含的关键特性：**

- 强大的基于 JavaBeans 的采用控制翻转（Inversion of Control，IoC）原则的配置管理，使得应用程序的组建更加快捷简易。
- 一个可用于从 applet 到 Java EE 等不同运行环境的核心 Bean 工厂。
- 数据库事务的一般化抽象层，允许宣告式(Declarative)事务管理器，简化事务的划分使之与底层无关。
- 内建的针对 JTA 和 单个 JDBC 数据源的一般化策略，使 Spring 的事务支持不要求 Java EE 环境，这与一般的 JTA 或者 EJB CMT 相反。
- JDBC 抽象层提供了有针对性的异常等级(不再从SQL异常中提取原始代码), 简化了错误处理, 大大减少了程序员的编码量. 再次利用JDBC时，你无需再写出另一个 '终止' (finally) 模块. 并且面向JDBC的异常与Spring 通用数据访问对象 (Data Access Object) 异常等级相一致.
- 以资源容器，DAO 实现和事务策略等形式与 Hibernate，JDO 和 iBATIS SQL Maps 集成。利用众多的翻转控制方便特性来全面支持, 解决了许多典型的Hibernate集成问题. 所有这些全部遵从Spring通用事务处理和通用数据访问对象异常等级规范.
- 灵活的基于核心 Spring 功能的 MVC 网页应用程序框架。开发者通过策略接口将拥有对该框架的高度控制，因而该框架将适应于多种呈现(View)技术，例如 JSP，FreeMarker，Velocity，Tiles，iText 以及 POI。值得注意的是，Spring 中间层可以轻易地结合于任何基于 MVC 框架的网页层，例如 Struts，WebWork，或 Tapestry。
- 提供诸如事务管理等服务的面向方面编程框架。

开源地址:

[https://github.com/spring-projects/spring-framework](https://link.zhihu.com/?target=https%3A//github.com/spring-projects/spring-framework)

### 6.3 Android 开源框架 EventBus Android

star:23.1k; fork:4.6k

![img](https://pic2.zhimg.com/80/v2-c2e2778ddcad9f8256594f73f760ddf5_720w.webp)

如果你学习过设计模式，那么当想通知其他组件某些事情发生时你一定会使用观察者模式。好了，既然能想到这个设计模式，那么就来看一个屌爆天的Android开源框架EventBus。主要功能是替代Intent、Handler、BroadCast在Fragment、Activity、Service、线程之间传递消息。他的最牛逼优点是开销小，代码简洁，解耦代码。

**特点：**

- 简化组件之间的通信
- 分离事件发送者和接收者
- 在 UI 工件（例如活动、片段）和后台线程中表现良好
- 避免复杂且容易出错的依赖关系和生命周期问题
- 很快；专为高性能而优化
- 很小（~60k jar）
- 是在实践中被证明通过应用与1,000,000,000+安装
- 具有交付线程、订阅者优先级等高级功能。

开源地址：

[https://github.com/greenrobot/EventBus](https://link.zhihu.com/?target=https%3A//github.com/greenrobot/EventBus)

### **6.4 Java 设计模式 java-design-patterns**

star:71.4k;fork:22.2k

![img](https://pic2.zhimg.com/80/v2-719c53087e9405a05c9544f951261d81_720w.webp)

设计模式是程序员在设计应用程序或系统时解决常见问题的最佳实践，重用设计模式有助于防止可能导致重大问题的细微问题，同时熟悉模式的程序员和架构师的代码也更具可读性。

开源地址：

[https://github.com/iluwatar/java-design-pattern](https://link.zhihu.com/?target=https%3A//github.com/iluwatar/java-design-pattern)

# 【NO.506】关于linux进程间的close-on-exec机制

## **1. 引子**

事情是这样的，最近我们线上一个基于nginx的http服务经常报警，具体如下：

```text
accept() failed (24: Too many open files) while accepting new connection on 0.0.0.0:80
```

## **2. 分析和重现问题**

第一时间怀疑是不是流量太大、tcp连接过多导致文件描述符耗光了？

待我们仔细分析流量已经用netstat查看具体的连接数，离我们设置的上限还差很远。这个时候开始怀疑我们的程序是不是有bug导致文件描述符泄露了。

于是用valgrind一顿狂测：

```text
valgrind --tool=memcheck \--leak-check=full \--show-below-main=yes \--leak-resolution=med \--track-fds=yes \--time-stamp=yes \--trace-children=yes \--show-reachable=yes \/usr/local/nginx/sbin/nginx
```

仔细核对valgrind的输出log日志，插，居然没有发现任何疑点。狂汗！！！问题又回到原点了。

错误日志是nginx输出的，那我们还是从nginx进程运行状态开始研究吧。

首先想到的是看看nginx进程到底打开了多少个文件描述符，具体如下：

```text
[cloud@w-nwdkill9 ~]$ ps aux|grep nginxroot      5501  0.4  0.2 544204 284676 ?       S    16:31   0:02 nginx: master process /usr/local/nginx/sbin/nginxcloud     5560 13.8  0.2 552780 287152 ?       S    16:31   1:19 nginx: worker processcloud     5561 11.6  0.2 550476 285748 ?       S    16:31   1:07 nginx: worker processcloud     5562 11.1  0.2 550820 285888 ?       S    16:31   1:04 nginx: worker processcloud     5564 10.5  0.2 550388 285464 ?       S    16:31   1:00 nginx: worker processcloud     5565 11.7  0.2 550408 285768 ?       S    16:31   1:07 nginx: worker processcloud     5566 12.0  0.2 550868 285908 ?       S    16:31   1:09 nginx: worker processcloud     5567 12.3  0.2 550732 285936 ?       R    16:31   1:11 nginx: worker processcloud     5569 12.8  0.2 550600 285912 ?       S    16:31   1:14 nginx: worker processcloud     5570 10.5  0.2 550848 285880 ?       S    16:31   1:00 nginx: worker processcloud     5571 12.4  0.2 550548 285804 ?       S    16:31   1:11 nginx: worker processcloud     5572 11.7  0.2 550664 285968 ?       S    16:31   1:07 nginx: worker processcloud     5573 10.6  0.2 550376 285540 ?       R    16:31   1:01 nginx: worker processcloud     5574  8.7  0.2 550288 285056 ?       S    16:31   0:50 nginx: worker processcloud     5575  9.6  0.2 550656 285688 ?       S    16:31   0:55 nginx: worker processcloud     5576  9.9  0.2 550436 285408 ?       S    16:31   0:57 nginx: worker processcloud     5577 12.1  0.2 550532 285720 ?       S    16:31   1:10 nginx: worker processcloud     5578 11.3  0.2 550400 285660 ?       S    16:31   1:05 nginx: worker processcloud     5579 10.6  0.2 550588 285428 ?       S    16:31   1:01 nginx: worker processcloud    17834  0.0  0.0 103304   888 pts/1    S+   16:40   0:00 grep nginx[cloud@w-nwdkill9 ~]$ ls -lhst /proc/5571/fd|wc -l55190
```

一个nginx进程打开的文件描述符就是5万多，

而我们设置的进程最大能打开的文件描述符个数为65535：

```text
[cloud@w-nwdkill9 ~]$ ulimit -acore file size          (blocks, -c) unlimiteddata seg size           (kbytes, -d) unlimitedscheduling priority             (-e) 0file size               (blocks, -f) unlimitedpending signals                 (-i) 65535max locked memory       (kbytes, -l) 65535max memory size         (kbytes, -m) unlimitedopen files                      (-n) 65535pipe size            (512 bytes, -p) 8POSIX message queues     (bytes, -q) 819200real-time priority              (-r) 0stack size              (kbytes, -s) 65535cpu time               (seconds, -t) unlimitedmax user processes              (-u) unlimitedvirtual memory          (kbytes, -v) unlimitedfile locks                      (-x) unlimited
```

的确是这样的，流量稍稍变化，可能就没有文件描述符可用了，

**因此nginx accept函数**返回错误。

再具体看看进程到底打开了什么文件描述符:

```text
[cloud@w-nwdkill9 ~]$ ls -lhst /proc/5571/fd...0 lrwx------ 1 cloud cloud 64 Apr 26 16:34 54786 -> /usr/local/nginx/logs/qlog_missing_8888_20140414.log (deleted)0 lrwx------ 1 cloud cloud 64 Apr 26 16:34 54787 -> socket:[1350821086]0 lrwx------ 1 cloud cloud 64 Apr 26 16:34 54788 -> socket:[1350821317]0 lrwx------ 1 cloud cloud 64 Apr 26 16:34 54789 -> socket:[1350821321]0 lrwx------ 1 cloud cloud 64 Apr 26 16:34 54790 -> socket:[1350821338]0 lrwx------ 1 cloud cloud 64 Apr 26 16:34 54791 -> /usr/local/nginx/logs/qlog_missing_8888_20140413.log (deleted)...
```

我们发现很多这种文件描述符“54791 -> /usr/local/nginx/logs/qlog_missing_8888_20140413.log (deleted)”，再到文件系统上ls一下，发现“ /usr/local/nginx/logs/qlog_missing_8888_20140413.log ”文件的确已经不在了。

然后我们想到会不是qlog这个日志公共库的问题呢？

我们随即联系了维护qlog库的同事，仔细聊起这个问题，他说qlog库本身会打开两种文件描述符：

- 写网络日志的时候，需要打开socket
- 写本地文件日志的时候，需要打开本地文件

然后又聊到，我们使用qlog的nginx程序的运行和[运维](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/solution/operation%3Ffrom%3D10680)方式。

我们nginx在不定期reload，

根据场景不同选择的reload方式主要有三种：

- 重新加载配置：kill -HUP nginx-master.pid
- 二进制文件替换的平滑重启热加载：kill -USR2 nginx-master.pid
- 日志滚动：kill -USR1 nginx-master.pid

经过离线实际测试和验证，

我们发现第二种方式“kill -USR2 nginx-master.pid”的确会有文件描述符泄露。验证过程就是，我们使用一个离线的差不多的环境，在另一个端口(例如16888)发启动nginx，然后对master进程发送USR2信号，即可以通过命令kill -USR2 `cat /usr/local/nginx/logs/nginx.pid`来操作，这个时候会有两个nginx的master进程，然后对比两个master打开的文件描述符个数，发现新的master进程比原来老的master进程多出3个来：

```text
[weizili@build11 ~]$ ./nginx -c nginx.conf[weizili@build11 ~]$ ps aux|grep nginx|grep weiziliweizili   7432  0.0  0.0 129964  3140 ?        Ss   17:10   0:00 nginx: master process ./nginx -c nginx.confweizili   7433  0.0  0.0 129964  3124 ?        S    17:10   0:00 nginx: worker processweizili   7462  0.0  0.0   6436   680 pts/15   S+   17:10   0:00 grep -n --color nginx[weizili@build11 ~]$ kill -USR2 7432[weizili@build11 ~]$ ps aux|grep nginx|grep weiziliweizili   7432  0.0  0.0 129964  3288 ?        Ss   17:10   0:00 nginx: master process ./nginx -c nginx.confweizili   7433  0.0  0.0 129964  3124 ?        S    17:10   0:00 nginx: worker processweizili   7503  0.3  0.2 129964  9004 ?        S    17:10   0:00 nginx: master process ./nginx -c nginx.confweizili   7504  0.0  0.0 129964  3124 ?        S    17:10   0:00 nginx: worker processweizili   7515  0.0  0.0   6436   680 pts/15   S+   17:10   0:00 grep -n --color nginx[weizili@build11 ~]$ ls -l /proc/7432/fd | wc -l            <----  老master进程打开的文件句柄数25[weizili@build11 ~]$ ls -l /proc/7503/fd | wc -l             <----  新master进程打开的文件句柄数28
```

再仔细分析新master进程所有打开的文件句柄，发现日志文件都打开了两次。

```text
[weizili@build11 ~]$ ls -l /proc/7503/fd | grep tutorial | sort -k 11l-wx------ 1 weizili weizili 64 Apr 26 17:10 17 -> /home/s/tutorial/logs/frameworktrace.2014-04-26-17l-wx------ 1 weizili weizili 64 Apr 26 17:10 20 -> /home/s/tutorial/logs/frameworktrace.2014-04-26-17l-wx------ 1 weizili weizili 64 Apr 26 17:10 4 -> /home/s/tutorial/logs/info.2014-04-26-17l-wx------ 1 weizili weizili 64 Apr 26 17:10 5 -> /home/s/tutorial/logs/info.2014-04-26-17lrwx------ 1 weizili weizili 64 Apr 26 17:10 18 -> /home/s/tutorial/logs/qlog_missing_20140426.logl-wx------ 1 weizili weizili 64 Apr 26 17:10 16 -> /home/s/tutorial/logs/stat.2014-04-26-17l-wx------ 1 weizili weizili 64 Apr 26 17:10 19 -> /home/s/tutorial/logs/stat.2014-04-26-17
```

我们把老的master进程kill之后，再对新的master又发一次USR2信号，发现日志文件现在都被打开了三次，进程文件描述又多了3个。

```text
[weizili@build11 ~]$ kill -QUIT 7432[weizili@build11 ~]$ kill -USR2 7503[weizili@build11 ~]$ ps aux|grep nginx|grep weiziliweizili   7503  0.0  0.2 129964  9008 ?        S    17:10   0:00 nginx: master process ./nginx -c nginx.confweizili   7504  0.0  0.0 129964  3124 ?        S    17:10   0:00 nginx: worker processweizili   8374  1.2  0.2 129960  9000 ?        S    17:16   0:00 nginx: master process ./nginx -c nginx.confweizili   8375  0.0  0.0 129960  3120 ?        S    17:16   0:00 nginx: worker processweizili   8378  0.0  0.0   6436   676 pts/15   S+   17:16   0:00 grep -n --color nginx[weizili@build11 ~]$ ls -l /proc/8374/fd |grep tutorial  | sort -k 11l-wx------ 1 weizili weizili 64 Apr 26 17:16 17 -> /home/s/tutorial/logs/frameworktrace.2014-04-26-17l-wx------ 1 weizili weizili 64 Apr 26 17:16 20 -> /home/s/tutorial/logs/frameworktrace.2014-04-26-17l-wx------ 1 weizili weizili 64 Apr 26 17:16 24 -> /home/s/tutorial/logs/frameworktrace.2014-04-26-17l-wx------ 1 weizili weizili 64 Apr 26 17:16 4 -> /home/s/tutorial/logs/info.2014-04-26-17l-wx------ 1 weizili weizili 64 Apr 26 17:16 5 -> /home/s/tutorial/logs/info.2014-04-26-17l-wx------ 1 weizili weizili 64 Apr 26 17:16 6 -> /home/s/tutorial/logs/info.2014-04-26-17lrwx------ 1 weizili weizili 64 Apr 26 17:16 22 -> /home/s/tutorial/logs/qlog_missing_20140426.logl-wx------ 1 weizili weizili 64 Apr 26 17:16 16 -> /home/s/tutorial/logs/stat.2014-04-26-17l-wx------ 1 weizili weizili 64 Apr 26 17:16 19 -> /home/s/tutorial/logs/stat.2014-04-26-17l-wx------ 1 weizili weizili 64 Apr 26 17:16 23 -> /home/s/tutorial/logs/stat.2014-04-26-17
```

## **3. 解决问题**

上面已经可以稳定重现问题了，现在就着手解决问题。

### **3.1 nginx相关源码分析**

先来分析一下nginx在处理USR2信号的过程。当nginx收到USR2信号时，会走到下列代码处：

```text
case ngx_signal_value(NGX_CHANGEBIN_SIGNAL):if (getppid() > 1 || ngx_new_binary > 0) {
/** Ignore the signal in the new binary if its parent is* not the init process, i.e. the old binary's process* is still running.  Or ignore the signal in the old binary's* process if the new binary's process is already running.*/
action = ", ignoring";ignore = 1;break;}
ngx_change_binary = 1;action = ", changing binary";break;
```

上述代码将变量ngx_change_binary置为1。然后会在master cycle中实际去处理。

最总会走到下列代码处：

```text
if (ngx_change_binary) {ngx_change_binary = 0;ngx_log_error(NGX_LOG_NOTICE, cycle->log, 0, "changing binary");// 函数ngx_exec_new_binary是核心ngx_new_binary = ngx_exec_new_binary(cycle, ngx_argv);}
```

关于函数 ngx_exec_new_binary，代码如下：

```text
ngx_pid_tngx_exec_new_binary(ngx_cycle_t *cycle, char *const *argv){char             **env, *var;u_char            *p;ngx_uint_t         i, n;ngx_pid_t          pid;ngx_exec_ctx_t     ctx;ngx_core_conf_t   *ccf;ngx_listening_t   *ls;
ctx.path = argv[0];ctx.name = "new binary process";ctx.argv = argv;
n = 2;env = ngx_set_environment(cycle, &n);if (env == NULL) {return NGX_INVALID_PID;}
var = ngx_alloc(sizeof(NGINX_VAR)+ cycle->listening.nelts * (NGX_INT32_LEN + 1) + 2,cycle->log);
p = ngx_cpymem(var, NGINX_VAR "=", sizeof(NGINX_VAR));
ls = cycle->listening.elts;for (i = 0; i < cycle->listening.nelts; i++) {p = ngx_sprintf(p, "%ud;", ls[i].fd);}
*p = '';
env[n++] = var;
#if (NGX_SETPROCTITLE_USES_ENV)
/* allocate the spare 300 bytes for the new binary process title */
env[n++] = "SPARE=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX""XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX";
#endif
env[n] = NULL;
#if (NGX_DEBUG){char  **e;for (e = env; *e; e++) {ngx_log_debug1(NGX_LOG_DEBUG_CORE, cycle->log, 0, "env: %s", *e);}}#endif
ctx.envp = (char *const *) env;
ccf = (ngx_core_conf_t *) ngx_get_conf(cycle->conf_ctx, ngx_core_module);
if (ngx_rename_file(ccf->pid.data, ccf->oldpid.data) != NGX_OK) {ngx_log_error(NGX_LOG_ALERT, cycle->log, ngx_errno,ngx_rename_file_n " %s to %s failed ""before executing new binary process \"%s\"",ccf->pid.data, ccf->oldpid.data, argv[0]);
ngx_free(env);ngx_free(var);
return NGX_INVALID_PID;}
pid = ngx_execute(cycle, &ctx);
if (pid == NGX_INVALID_PID) {if (ngx_rename_file(ccf->oldpid.data, ccf->pid.data) != NGX_OK) {ngx_log_error(NGX_LOG_ALERT, cycle->log, ngx_errno,ngx_rename_file_n " %s back to %s failed after ""the try to execute the new binary process \"%s\"",ccf->oldpid.data, ccf->pid.data, argv[0]);}}
ngx_free(env);ngx_free(var);
return pid;}
```

上述函数 解释起来其实说来也简单，

**nginx就是通过fork+execve这种经典的处理方式来实现的。**

不过在函数的开始部分有一些设置环境变量的处理，它有什么作用呢？

设想一下，如果新的二进制文件在启动时必然要涉及bind端口的动作，而此时旧进程已经做了绑定，我们知道多个进程是不能同时绑定同一个地址和端口的，所以新的进程要避免这种情况发生。

nginx的做法是将原来的绑定得到的listen fd保存在名为”NGINX”(宏定义NGINX_VAR)环境变量中，这样在新进程初始化的过程中，通过函数ngx_add_inherited_sockets就可以获取listen fd来使用了，不必再次绑定。关于listen fd如何在环境变量中设置和获取，这里不再详细列举。

至此，到达该文的高潮部分了。

我们发现nginx在新的master进程起来之后，并没有将不用的文件描述符关闭。

### **3.2文件描述符与exec()**

我们知道，默认情况下，由exec()的调用程序（这里指老的nginx master进程）所打开的所有文件描述符在exec()的执行过程中会保持打开状态，且在新的程序(这里指新的nginx master进程)中依然有效。

这种通常情况下，是一个很实用的特性，因为调用程序可能会以特定的文件描述符来打开文件，而在新程序中这些文件会保持有效，无需在去了解文件名或重新打开。**shell就是利用这一特性为其所执行的程序处理IO重定向**。

分析到这里，我们就知道为什么上述正对nginx的USR2信息处理过程中，新的master进程会多出一些看起来重复的文件描述符。

怎么解决这问题呢？我们需要用到这个标记：**FD_CLOEXEC**

### **3.3 执行时关闭(close-on-exec)标记：FD_CLOEXEC**

在执行exec()之前，程序有时需要确保关闭某些特定的文件描述符。尤其是在特权进程中来调用exec()来启动一个未知程序，亦或是新的程序并不需要这些已经打开的文件描述符。我们这个场景，qlog作为一个基础库，他是不知道应用场景的，因此需要检查qlog库的文件描述符打开时是否设置了FD_CLOEXEC这个标记，经过源码阅读确认没有设置这个标记；另外，nginx作为一个server程序，而且作为一个http框架，支持插件式的扩展模块，理应处理好这中文件句柄泄露的问题。但是我们看到目前双方都把这个标记FD_CLOEXEC忘记了。

从安全编程的角度出发，nginx应该在做热加载(ngx_exec_new_binary)之前确保关闭那些不必要的文件描述符。

对所有的此类文件描述符调用close()函数即可达到这一目的，然而nginx没有这么做，是有他的道理的，

因为这一做法存在如下局限性：

- 某些描述符可能是由库函数打开的(例如我们当前这种情况下，qlog会打开一些文件描述符)。但库函数无法使nginx在执行exec()之前关闭相应的文件描述符。作为基本原则，库函数应该总是为其打开的文件描述符设置FD_CLOEXEC标记。稍后介绍这种做法。
- 如果exec()因某种原因失败，可能还需要使这些描述符保持打开状态。如果这些描述符依然关闭，将他们重新打开并执行相同的文件的难度是可想而知，是相当大的，基本不可能。

基于上述原因，nginx把这个问题留给了他的使用者解决。

### **3.4 closeonexec测试程序**

FD_CLOEXEC，这是fcntl的一个Flag标志，用来设置文件的close-on-exec状态标志。在exec()调用后，close-on-exec标志为0的情况，此文件不被关闭；非零则在exec()后自动关闭。默认close-on-exec状态为0，需要通过FD_CLOEXEC设置。

下面的测试程序说明了FD_CLOEXEC标记的用法。

程序执行时，如果带了命令行参数(可以是任意字符串参数)，该程序首先为标准输出设置FD_CLOEXEC标记，随后再执行ls外部命令。程序如下。

```text
#include <stdio.h>#include <fcntl.h>#include <unistd.h>
int main(int argc, char* argv[]){if (argc > 1) {int flags = fcntl(STDOUT_FILENO, F_GETFD);if (flags == -1) {perror("fctnl(STDOUT_FILENO, F_GETFD) ERROR:");return -1;}
flags |= FD_CLOEXEC;if (fcntl(STDOUT_FILENO, F_SETFD, flags) == -1) {perror("fctnl(STDOUT_FILENO, F_SETFD) ERROR:");return -1;}}
execlp("ls", "ls", "-l", argv[0], (char*)NULL);return 0;}
```

程序执行效果如下：

```text
[weizili@build11 ~]$ ./closeonexec-rwxrwxr-x 1 weizili weizili 45752 Apr 26 15:52 ./closeonexec[weizili@build11 ~]$ ./closeonexec 1ls: write error: Bad file descriptor[weizili@build11 ~]$
```

实际上FD_CLOEXEC是文件描述符标志中唯一可以操作的一位。包括Linux在内的许多UNIX实现，还允许另外一种非标准的ioctl调用来修改该标记：

- 以ioctl(fd, FIOCLEX)为fd设置此标志
- 以ioctl(fd, FIONCLEX)来清除此标志

### **3.5 修复上述文件描述符泄露bug**

修改nginx模块代码，在模块main_conf的初始化函数开始处调用 closeonexec，即可解决上述问题。

```text
/** porting code from libdaemon-0.14/libdaemon/dfork.c:daemon_close_allv */static int daemon_close_allv(const std::set<int> except_fds) {struct rlimit rl;int fd, maxfd;
#ifdef __linux__
DIR *d;
if ((d = opendir("/proc/self/fd"))) {
struct dirent *de;
while ((de = readdir(d))) {int found;long l;char *e = NULL;
if (de->d_name[0] == '.')continue;
errno = 0;l = strtol(de->d_name, &e, 10);if (errno != 0 || !e || *e) {closedir(d);errno = EINVAL;return -1;}
fd = (int) l;
if ((long) fd != l) {closedir(d);errno = EINVAL;return -1;}
if (fd < 3)continue;
if (fd == dirfd(d))continue;
found = 0;if (except_fds.find(fd) != except_fds.end()) {found = 1;}#if 0for (int i = 0;  i < (int)except_fds.size() && except_fds[i] >= 0; i++)if (except_fds[i] == fd) {found = 1;break;}#endif
if (found)continue;
if (close(fd) < 0) {int saved_errno = errno;closedir(d);errno = saved_errno;
return -1;}
}
closedir(d);return 0;}
#endif
if (getrlimit(RLIMIT_NOFILE, &rl) > 0)maxfd = (int) rl.rlim_max;elsemaxfd = sysconf(_SC_OPEN_MAX);
for (fd = 3; fd < maxfd; fd++) {int found = 0;if (except_fds.find(fd) != except_fds.end()) {found = 1;}#if 0for (int i = 0; except_fds[i] >= 0; i++)if (except_fds[i] == fd) {found = 1;break;}#endif
if (found)continue;
if (close(fd) < 0 && errno != EBADF)return -1;
}
return 0;}
static void convert_except_fds(const char* listening_fds, std::set<int>& except_fds) {std::vector<std::string> string_fds;osl::StringUtil::split(string_fds, listening_fds, ";");std::vector<std::string>::iterator it (string_fds.begin());std::vector<std::string>::iterator ite(string_fds.end());for (; it != ite; ++it) {if (it->empty()) {return;}except_fds.insert(atoi(it->data()));}}
static void append_errorlog_fds(std::set<int>& except_fds) {except_fds.insert(3);// fd of the opened file : /usr/local/nginx/logs/error.log//TODO unknown case ??}
/*** If we use 'kill -USR2 nginx.pid' to restart a new nginx binary,* we have a potential risk of file descriptor leak.** Why? Please see the manual of system call 'execve' 'open', and pay attention of O_CLOEXEC**/static bool closeonexec(){char* listening_fds = getenv(NGINX_VAR);if (!listening_fds) {//cold startreturn true;}
pid_t ppid = getppid();if (ppid == 1) {//fprintf(stderr, "%s:%d reload nginx by kill -HUP\n", __func__, `fk_line` );return true;}
#if 0//TODO Add logic code to acquire//  : pid_t ngx_master_pid = get_ngx_master_pid();// to do a double checkassert(ppid == ngx_master_pid);#endifstd::set<int> except_fds;convert_except_fds(listening_fds, except_fds);append_errorlog_fds(except_fds);daemon_close_allv(except_fds);
return true;}
```

## **4. 总结**

再次把FD_CLOEXEC的含义简单的总结一下：

close on exec, 从字面意思即可理解为：如果对描述符设置了FD_CLOEXEC，在使用execl调用执行的程序里，此描述符将在子进程中会被自动关闭，不能使用了。

但是在父进程中仍然可以使用。

Linux系统的open函数，其中flags参数可以传入O_CLOEXEC标记，即可自动设置上FD_CLOEXEC标记，但Linux内核版本2.6.23才开始支持此标记。

**扩展问题：**

就是父子进程中的端口占用情况。

父进程监听一个端口后，fork出一个子进程，然后kill掉父进程，再重启父进程，

这个时候提示端口占用，

用netstat查看，子进程占用了父进程监听的端口。

原文地址：https://zhuanlan.zhihu.com/p/602267181

作者：linux

# 【NO.507】网络编程:手绘TCP状态机

**知识卡**

![img](https://pic1.zhimg.com/80/v2-6323a56fff5913e7191bbd5b1f982430_720w.webp)

**情景对话**

老王：小王，最近工作注意力不集中呀！

小王：我在等面试结果呢！

老王：你感觉如何呢？

小王：

当时情况是这样的！

------

大王：你擅长window，还是liunx?

小王：Linux（这年头谁还在写window程序）

大王；那你对网络编程一定很熟悉 吧？

小王：那是当然（都是小菜一碟）。

大王：请绘制TCP状态转换过程？

小王：。。。。（这个谁能记住他，绞尽脑汁想，5分钟过去了）

大王：还有什么要补充的吗？（耐心等待）

小王：不会写，有几个记不清楚（5分钟过去了）

大王：好，回去等通知。

老王：我来讲一讲，需要解决下面几个问题

![img](https://pic1.zhimg.com/80/v2-0016e314a66c36d46c4a4fb85b37e0dc_720w.webp)

自我提问

## 1.**问题1 socket通讯过程，和抓包格式 时间限时在2分钟**

小王：

> socket常用接口 accept,read,write close，我经常用很熟悉呀，没什么可学的了， 还有tcp协议那个图 我看多少遍？

（老王）我这里提示一下，不做深入讨论，时间限时在2分钟。

![img](https://pic3.zhimg.com/80/v2-e92b8becaba806a66065ed514770a4ee_720w.webp)

完整通讯过程

![img](https://pic1.zhimg.com/80/v2-a8106991780add30c0a10528341a522c_720w.webp)

![img](https://pic1.zhimg.com/80/v2-5c57d674a72349e63ce31fbf32862d24_720w.webp)

用户态

## 2.**问题2 ：三次握手和四次挥手过程 ,时间限时在5 分钟**

![img](https://pic1.zhimg.com/80/v2-40eaa4976148abb927c14ac5e802da8c_720w.webp)

客户端和[服务器](https://link.zhihu.com/?target=https%3A//cloud.tencent.com/product/cvm%3Ffrom%3D10680)同时发现异常，都进行关闭这个连接

![img](https://pic2.zhimg.com/80/v2-1c9271a8f9661e129fa0cae4cee72c11_720w.webp)

我没遇到过

## 3.**问题3 在机器重启，服务重启，网络断开等情况下呢？**

小王：

> 遇到这个情况，不是epoll, SO_KEEPALIVE read返回 0代表接受。一般都是这么处理的

闲话少说，从四次挥手最有一步异常说起。

老王：RichardStevens说过这样2句话

There are two reasons for the TIME_WAIT state:

一、保证TCP协议的全双工连接能够可靠关闭

To implement TCP's full-duplex connection termination reliably

二、保证这次连接的重复数据段从网络中消失

To allow old duplicate segments to expire in the network

小王：

表示不理解，上面不是同一个意思吗，如果达到不了就消失？

（老王）错误，继续看

根据第三版《UNIX网络编程 卷1》2.7节，TIME_WAIT状态的主要目的有两个：

- 优雅的关闭TCP连接，也就是尽量保证被动关闭的一端收到它自己发出去的FIN报文的ACK确认报文；
- 处理延迟的重复报文，这主要是为了避免前后两个使用相同四元组的连接中的前一个连接的报文干扰后一个连接。 **保证TCP协议的全双工连接能够可靠关闭：（解释）**

> ACK is lost. The server will resend its final FIN, so the client must maintain state information, allowing it to resend the final ACK. If it did not maintain this information, it would respond with an RST (a different type of TCP segment), which would be interpreted by the server as an error

发生条件：

服务正常，网络正常。

- 服务正常，网络正常：

B发送FIN，进入LAST_ACK状态，A收到这个FIN包后发送ACK包，**B收到这个ACK包**，然后进入CLOSED状态

- 服务正常，网络拥塞，网络连接良好

B发送FIN，进入LAST_ACK状态，A收到这个FIN包后发送ACK包，由于某种原因，这个ACK包丢失了，**B没有收到ACK包**，然后B等待ACK包超时，又向A发送了一个FIN包 a) **假如这个时候，A还是处于TIME_WAIT状态(也就是TIME_WAIT持续的时间在2MSL内)**A收到这个FIN包后向B发送了一个ACK包，B收到这个ACK包进入CLOSED状态

b) **假如这个时候，A已经从TIME_WAIT状态变成了CLOSED状态** A收到这个FIN包后，认为这是一个错误的连接，向B发送一个**RST**包，当B收到这个RST包，进入CLOSED状态

- 服务不正常 或者网络断开 c) **假如这个时候，A挂了（假如这台机器炸掉了）【第四种情况，不在参考链接里】** B没有收到A的回应，那么会继续发送FIN包，也就是触发了TCP的重传机制，如果A还是没有回应，B还会继续 发送FIN包，直到重传超时(至于这个时间是多长需要仔细研究)，B重置这个连接，进入CLOSED状态，

小王：原来是这样

画外音

网络断了，节点重启了，是无法处理的。只能依靠Rst解决。

下面情况如果ack，不能按时到达，阻止建立新的连接。

小王：原来是这样

画外音：

> TCP连接中的一端发送了FIN报文之后如果收不到对端针对该FIN的ACK，则会反复多次重传FIN报文. 处于TIME_WAIT状态的一端在收到重传的FIN时会重新计时(rfc793 以及 linux kernel源代码tcp_timewait_state_process函数

**保证这次连接的重复数据段从网络中消失（解释）**

发生条件：

Note that it is *very* unlikely that delayed segments will cause problems like this.

Firstly the address and port of each end point needs to be the same; which is normally unlikely as the client's port is usually selected for you by the operating system from the ephemeral port range and thus changes between connections.

Secondly, the sequence numbers for the delayed segments need to be valid in the new connection which is also unlikely. However, should both of these things occur then `TIME_WAIT` will prevent the new connection's data from being corrupted.

画外音：

必须原来的ip，原来的端口发起的连接，想想一个服务器连接多个客户端，四元组 是唯一的。

![img](https://pic4.zhimg.com/80/v2-e621f23c9f23dbac8ccf1419fba682f3_720w.webp)

*Due to a shortened TIME-WAIT state, a delayed TCP segment has been accepted in an unrelated connection.*

![img](https://pic4.zhimg.com/80/v2-7503e41fdcab0719781b228847c5c7c7_720w.webp)

image.png

小王：原来是这样！

**画外音：**

**四次挥手已经完成，最有一个ack顺利达到对方，一方进入closed状态（假如3秒内完成）**

**对方依然要等待2MSL（剩余28秒），这个等待不是多余等待，而是防止**

**这个时候双方如果马上同时closed（是允许建立新的连接。这是正常通讯过程）、**

**还有延迟重发的数据包。对同一个pair连接，新老数据造成混乱**。

> tcp协议提到内核接受数据是根据port区分是那个，而不是fd。

## 4.**小王偷偷写这么几句话**

time_wait 存在的意义有2点

（1） TCP 可靠传输，保证四次挥手最后一个ack 顺利到达对方。

采用方式是：如果获取到对方重新发送fin请求，需要重新计时间，维持TIme_wait状态。

保障每次发送出去ack都最终结果（收到或者消失）

如果在网络出断网，或者服务节点重启，或者对方不启tcp重传机制上面方法是无法处理的

应该超时或者返回Rst包出路 结束last_ack状态。

（2 ） TCP基于四元组建立连接， 假如客户端端口 不随机产生，而是相同ip，相同的

端口，再次连接的话。可能出现，虽然old 连接已经消失，但是在网络中数据可能存在。

以tcp 内核中断处理 网络消息是根据 端口划分的。会造成新旧数据混乱。

> TCP不能给处于TIME_WAIT状态的连接启动新的连接。 TIME_WAIT的持续时间是2MSL，保证在建立新的连接之前老的重复分组在网络中消逝。 这个规则有一个例外：如果到达的SYN的序列号大于前一个连接的结束序列号， 源自Berkeley的实现将给当前处于TIME_WAIT状态的连接启动新的化身。 do_time_wait

![img](https://pic2.zhimg.com/80/v2-783f051280100e17186da109edb4595d_720w.webp)

tcp状态机

原文地址：https://zhuanlan.zhihu.com/p/602356909

作者：linux

# 【NO.508】从进程和线程的创建过程来看进程和线程的区别

在用户空间我们可以通过fork（）函数来创建一个新的进程。fork()是一个glibc标准库函数，在内核里边会有一个系统调用与之对应--sys_fork（）。同样的，我们是通过pthread_create（）来创建一个线程，内核中对应的系统调用是clone（）。现在通过分析sys_fork（）和clone（）的异同来看进程和线程的区别。本文是基于5.0版本的Linux内核和2.21版本的glibc。

**fork**

首先看一下sys_fork（），定义在kernel\fork.c文件中：

```text
SYSCALL_DEFINE0(fork)
{
#ifdef CONFIG_MMU
	return _do_fork(SIGCHLD, 0, 0, NULL, NULL, 0);
#else
	/* can not support in nommu mode */
	return -EINVAL;
#endif
}
```

可以看到sys_fork 会调用 _do_fork。

**pthread_create**

对于线程的创建，我们先来看glibc源码（glibc-2.21\sysdeps\unix\sysv\linux\createthread.c）：

```text
static int
create_thread (struct pthread *pd, const struct pthread_attr *attr,
	       bool stopped_start, STACK_VARIABLES_PARMS, bool *thread_ran)
{
  ......
 
  const int clone_flags = (CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SYSVSEM
			   | CLONE_SIGHAND | CLONE_THREAD
			   | CLONE_SETTLS | CLONE_PARENT_SETTID
			   | CLONE_CHILD_CLEARTID
			   | 0);
 TLS_DEFINE_INIT_TP (tp, pd);
 
  if (__glibc_unlikely (ARCH_CLONE (&start_thread, STACK_VARIABLES_ARGS,
				    clone_flags, pd, &pd->tid, tp, &pd->tid)
			== -1))
    return errno;
  ........
  
}
```

我们这里可以看到，在glibc中先设置了一个clone_flag（这个flag标记很重要，最后和进程的一个很大的区别就在这里），然后陷入到的clone（）系统调用：

```text
SYSCALL_DEFINE5(clone, unsigned long, clone_flags, unsigned long, newsp,
     int __user *, parent_tidptr,
     int __user *, child_tidptr,
     unsigned long, tls)
{
  return _do_fork(clone_flags, newsp, 0, parent_tidptr, child_tidptr, tls);
```

这里我们发现，创建线程时最后也会调用到 _do_fork函数！那么在_do_fork（）中，创建进程和创建线程有什么区别呢？我们继续往下看。

```text
long _do_fork(unsigned long clone_flags,
        unsigned long stack_start,
        unsigned long stack_size,
        int __user *parent_tidptr,
        int __user *child_tidptr,
        unsigned long tls)
{
  struct task_struct *p;
  int trace = 0;
  long nr;
 
 
......
  p = copy_process(clone_flags, stack_start, stack_size,
       child_tidptr, NULL, trace, tls, NUMA_NO_NODE);
......
  if (!IS_ERR(p)) {
    struct pid *pid;
    pid = get_task_pid(p, PIDTYPE_PID);
    nr = pid_vnr(pid);
 
 
    if (clone_flags & CLONE_PARENT_SETTID)
      put_user(nr, parent_tidptr);
 
 
......
    wake_up_new_task(p);
......
    put_pid(pid);
  } 
......
```

在_do_fork这个函数之中最重要的步骤就是 copy_process， copy_process函数中最重要的则是创建新任务的task_struct结构体。对于内核来讲进程和线程都是一个task任务，而内核用task_struct来封装一个任务。 task_struct 的结构示意图如下所示（图来源于极客时间趣谈Linux操作系统）：

![img](https://pic3.zhimg.com/80/v2-f955045e2611396e8dcf9e910bef1872_720w.webp)

```text
static __latent_entropy struct task_struct *copy_process(
          unsigned long clone_flags,
          unsigned long stack_start,
          unsigned long stack_size,
          int __user *child_tidptr,
          struct pid *pid,
          int trace,
          unsigned long tls,
          int node)
{
  int retval;
  struct task_struct *p;
......
  p = dup_task_struct(current, node);
```

dup_task_struct 主要做了下面几件事情：

- 调用 alloc_task_struct_node 分配一个 task_struct 结构；
- 调用 alloc_thread_stack_node 来创建内核栈，这里面调用 __vmalloc_node_range 分配一个连续的 THREAD_SIZE 的内存空间，赋值给 task_struct 的 void *stack 成员变量；
- 调用 arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)，将 task_struct 进行复制，其实就是调用 memcpy；
- 调用 setup_thread_stack 设置 thread_info。

到这里，整个 task_struct 复制了一份，而且内核栈也创建好了。 继续看copy_process:

```text
/* copy all the process information */
	shm_init_task(p);
	retval = security_task_alloc(p, clone_flags);
	if (retval)
		goto bad_fork_cleanup_audit;
	retval = copy_semundo(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_security;
	retval = copy_files(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_semundo;
	retval = copy_fs(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_files;
	retval = copy_sighand(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_fs;
	retval = copy_signal(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_sighand;
	retval = copy_mm(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_signal;
	retval = copy_namespaces(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_mm;
	retval = copy_io(clone_flags, p);
	if (retval)
		goto bad_fork_cleanup_namespaces;
	retval = copy_thread_tls(clone_flags, stack_start, stack_size, p, tls);
	if (retval)
		goto bad_fork_cleanup_io;
```

这一段代码主要是用来复制任务的信息，内容比较多，我们可以从函数名称就大概猜出具体复制些什么，这里主要挑五个比较重要的来讲。

| copy_files   | 主要用于复制一个进程打开的文件信息。这些信息用一个结构 files_struct 来维护，每个打开的文件都有一个文件描述符。 |
| ------------ | ------------------------------------------------------------ |
| copy_fs      | 主要用于复制一个进程的目录信息。这些信息用一个结构 fs_struct 来维护。 |
| copy_sighand | 分配一个新的 sighand_struct。这里最主要的是维护信号处理函数  |
| copy_signal  | 复制用于维护发给这个进程的信号的数据结构                     |
| copy_mm      | 进程都有自己的内存空间，用 mm_struct 结构来表示。copy_mm 函数中调用 dup_mm，分配一个新的 mm_struct 结构，调用 memcpy 复制这个结构。 |

接下来一个个分析这几个函数

**copy_files**

```text
static int copy_files(unsigned long clone_flags, struct task_struct *tsk)
{
  struct files_struct *oldf, *newf;
  oldf = current->files;
  if (clone_flags & CLONE_FILES) {
    atomic_inc(&oldf->count);
    goto out;
  }
  newf = dup_fd(oldf, &error);
  tsk->files = newf;
out:
  return error;
}
```

这个函数中我们看到了在glibc create_thread中设置的clone_flags,在上面的代码中可以看出，如果设置了CLONE_FLES这个位，那么只是给主线程的task_struct的files_struct的引用计数count原子地加1，然后就返回了。而如果是进程的创建，CLONE_FLES位没有被标记，则会在dup_fd里边创建一个新的 files_struct，然后将所有的文件描述符数组 fdtable 拷贝一份。dup_fd函数定义在linux-5.0\fs\file.c文件中，这里就不贴代码了。

**copy_fs**

```text
static int copy_fs(unsigned long clone_flags, struct task_struct *tsk)
{
  struct fs_struct *fs = current->fs;
  if (clone_flags & CLONE_FS) {
    fs->users++;
    return 0;
  }
  tsk->fs = copy_fs_struct(fs);
  return 0;
}
```

对于 copy_fs，在创建进程时是调用 copy_fs_struct 复制一个 fs_struct。如果是创建线程则由于 CLONE_FS 标识位已经被设置，所以也是将原来的 fs_struct 的用户数加一。这个逻辑和copy_files是一样的。

**copy_sighand**

对于和信号相关的的两个结构体，sighand_struct、signal_struct，处理过程也是类似，如果是进程创建则会复制一分，而线程创建也仅仅是将将原来的 sighand_struct 引用计数加一。

```text
static int copy_sighand(unsigned long clone_flags, struct task_struct *tsk)
{
  struct sighand_struct *sig;
 
 
  if (clone_flags & CLONE_SIGHAND) {
    atomic_inc(&current->sighand->count);
    return 0;
  }
  sig = kmem_cache_alloc(sighand_cachep, GFP_KERNEL);
  atomic_set(&sig->count, 1);
  memcpy(sig->action, current->sighand->action, sizeof(sig->action));
  return 0;
```

**copy_signal**

对于 copy_signal，进程的创建是创建一个新的 signal_struct，而线程的创建则因为 CLONE_THREAD 直接返回了。

```text
static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)
{
  struct signal_struct *sig;
  if (clone_flags & CLONE_THREAD)
    return 0;
  sig = kmem_cache_zalloc(signal_cachep, GFP_KERNEL);
  tsk->signal = sig;
    init_sigpending(&sig->shared_pending);
......
}
```

**copy_mm**

对于 copy_mm，进程创建是是调用 dup_mm 复制一个 mm_struct，线程的创建则因为 CLONE_VM 标识位而直接指向了原来的 mm_struct。

```text
static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)
{
  struct mm_struct *mm, *oldmm;
  oldmm = current->mm;
  if (clone_flags & CLONE_VM) {
    mmget(oldmm);
    mm = oldmm;
    goto good_mm;
  }
  mm = dup_mm(tsk);
good_mm:
  tsk->mm = mm;
  tsk->active_mm = mm;
  return 0;
}
```

**总结**

综上所述，创建进程的时候调用的系统调用是 fork，在 copy_process 函数里面，会将五个最重要的结构体 files_struct、fs_struct、sighand_struct、signal_struct、mm_struct 都复制一遍，从此父进程和子进程各用各的数据结构，各有一个独立的内存空间。而创建线程的话，调用的是系统调用 clone，在 copy_process 函数里面， 五大结构仅仅是引用计数加一，也即线程共享进程的数据结构。

这里在次引用极客时间趣谈Linux操作系统的一张图总结一下：

![img](https://pic1.zhimg.com/80/v2-125357aa3a2f27d50a1d7eb5ce16e960_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/602672787

作者：linux

# 【NO.509】超详细讲解Linux中的基础IO

## 1.系统文件IO

操作文件除了使用C语言、C++以及其他的语言的接口之外，还可以使用操作系统提供的接口

### 1.1 open

![img](https://pic2.zhimg.com/80/v2-5cc02b24372d541d1c31bb940e8efd41_720w.webp)

#### 1.1.1 open的第一个参数

open函数的第一个参数是pathname，表示要打开或创建的目标文件

- 若pathname以**路径**的方式给出，则需要创建该文件时，在pathname路径下进行创建
- 若pathname以**文件名**的方式给出，则需要创建该文件时，默认在当前路径下进行创建

#### 1.1.2 open的第二个参数

open函数的第二个参数是flags，表示打开文件的方式

![img](https://pic4.zhimg.com/80/v2-b73b60dec6d0784c2732b415b460049f_720w.webp)

上面提供的参数选项仅仅是较为常用的，具体可以man 2 open命令进行查看

打开文件时可以传入多个参数选项，当有多个选项传入时，将这些选项用"按位或"隔开

**传参原理:**

系统接口open的第二个参数flags为整型类型，在32位平台上有32个bit位。若将一个bit位作为一个标志位，则理论上flags可以传递32种不同的标志位。而实际上flags的参数选项都是宏

![img](https://pic3.zhimg.com/80/v2-d0fca16077f4dbbcf406e5790b200866_720w.webp)

在oen函数内部就可以通过使用一系列的位运算来判断是否设置了某一选项。

#### 1.1.3 open的第三个参数

open函数的第三个参数是mode，表示创建文件的默认权限

传入0666参数进行文件创建，按理应得到-rw-rw-rw-权限的文件，但却得到了如下图的文件

![img](https://pic3.zhimg.com/80/v2-2418212ab7397e9fd89bc6ff15a749f6_720w.webp)

这是因为权限掩码的存在(默认为0002)，可以在代码中设置权限掩码来避免上述情况发生。

```text
#include <stdio.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
int main()
{
    umask(0000);//设置文件掩码                                                                     
    int fd = open("./log.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    close(fd);
    return 0;
}
```

![img](https://pic3.zhimg.com/80/v2-1af93c9f6743f9ca9c778338c162394a_720w.webp)

**注意:** 当不需创建文件时，可以不用设置第三个参数

#### 1.1.4 open的返回值

open函数成功调用后返回打开文件的文件描述符，若调用失败则返回-1。文件描述符具体在后面进行讲解。

### 1.2 close

使用close函数时传入需要关闭文件的**文件描述符**（即调用open函数的返回值）即可。若关闭文件成功则返回0；若关闭文件失败则返回-1。

![img](https://pic2.zhimg.com/80/v2-cc9dd1b05665d898b51c10f3c71ee6dd_720w.webp)

### 1.3 write

Linux系统接口中使用write函数向文件写入信息

![img](https://pic4.zhimg.com/80/v2-ab9ddb2f6d5a17491faa0c27950ef4cb_720w.webp)

将buf位置开始向后count字节的数据写入文件描述符为fd的文件当中。

- 若数据写入成功，则返回实际写入数据的字节数。
- 若数据写入失败，则返回-1。

```text
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
int main()
{
    int fd = open("./log.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    if(fd < 0){
        perror("open:");
        exit(1);
    }
 
    const char* buf = "hello Linux!\n";
    write(fd, buf, strlen(buf));                                                     
    close(fd);
    return 0;
}
```

![img](https://pic4.zhimg.com/80/v2-269e28789497b0e6f930f29028306e17_720w.webp)

Linux系统接口中使用write函数向文件写入信息

![img](https://pic4.zhimg.com/80/v2-a43f26ab65567a7c3481b988850d56ef_720w.webp)

从文件描述符为fd的文件读取count字节的数据到buf位置当中

- 若数据读取成功，则返回实际读取数据的字节数
- 若数据读取失败，则返回-1

```text
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
int main()
{
    int fd = open("./log.txt",O_RDONLY);
    if(fd < 0){
        perror("open:");
        exit(1);
    }
 
    char buf[64] = {0};
    while(read(fd, buf, 63)){
        printf("%s",buf);                                                                                                                                                        
        memset(buf,0x00,64);
    }
 
    close(fd);
    return 0;
}
```

![img](https://pic1.zhimg.com/80/v2-5b5856f82fa7eef26dc493130bf48cc4_720w.webp)

## 2.文件描述符

### 2.1 进程与文件描述符

文件被访问的前提是被加载到内存中打开，一个进程可以打开多个文件，而系统当中又存在大量进程。即在系统中任何时刻都可能存在大量已经打开的文件，所以操作系统内部提供了struct file结构体用于描述文件(先描述，再组织)，再使用双链表进行管理。

而为了区分已经打开的文件哪些属于特定的某一个进程，我们就还需要建立进程和文件之间的对应关系。

**进程与文件之间的对应关系是如何确定的呢？**

task_struct（PCB）中有一个指针，该指针指向一个名为files_struct的结构体。在该结构体中就有一个名为fd_array的指针数组，该数组的下标就是我们所谓的文件描述符。

当一个进程打开其第一个文件时，先将该文件从磁盘当中加载到内存，形成对应的struct file变量，将该struct file变量链入文件双链表中，并将该结构体的首地址填入到fd_array数组中下标为3的位置，最后返回该文件的文件描述符给调用进程。

![img](https://pic4.zhimg.com/80/v2-c3b5a32da1f0fb730adeea3c0aeb4cab_720w.webp)

因此只需要有某一文件的文件描述符，就可以找到与该文件相关的文件信息，进而对文件进行一系列输入输出操作。

**内存文件 VS 磁盘文件**

当文件存储在磁盘当中时，被称为磁盘文件；当磁盘文件被加载到内存中后，此时的文件被称为内存文件。磁盘文件和内存文件之间的关系就像程序和进程的关系一样，当程序加载到内存运行起来后便成了进程，而当磁盘文件加载到内存后便成了内存文件。

磁盘文件由两部分构成，分别是文件内容和文件属性。文件内容就是文件当中存储的数据，文件属性就是文件的一些基本信息，例如文件名、文件大小以及文件创建时间等信息都是文件属性，文件属性又被称为元信息。

文件加载到内存时，一般先加载文件的属性信息，当需要对文件内容进行读取、输入或输出等操作时，再延后式的加载文件数据。

### 2.2 文件描述符的分配规则

Linux进程会默认打开3个文件描述符，分别是标准输入0、标准输出1、标准错误2

分别对应的物理设备一般为：键盘、显示器、显示器

键盘和显示器都属于硬件，且操作系统能够识别。当某一进程创建时，操作系统就会根据键盘、显示器、显示器形成各自的struct file变量，将这3个struct file变量连入文件双链表当中，并将这3个struct file变量的地址分别填入fd_array数组下标为0、1、2的位置，至此就默认打开了标准输入流、标准输出流和标准错误流。

```text
#include <stdio.h>                                                                                                                                                               
#include <string.h>
#include <stdlib.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
int main()
{
    int fd1 = open("./log1.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    int fd2 = open("./log2.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    int fd3 = open("./log3.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    int fd4 = open("./log4.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    int fd5 = open("./log5.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    printf("fd1: %d\n",fd1);
    printf("fd2: %d\n",fd2);
    printf("fd3: %d\n",fd3);
    printf("fd4: %d\n",fd4);
    printf("fd5: %d\n",fd5);
    close(fd1);
    close(fd2);
    close(fd3);
    close(fd4);
    close(fd5);
    return 0;
}
```

![img](https://pic2.zhimg.com/80/v2-600354a4c318311996b0439f9d07ccd5_720w.webp)

则后续打开的文件的文件描述符，从3开始逐个增加1。

在打开文件之前将文件描述符为0和2的文件关闭又会发生什么呢？

```text
#include <stdio.h>
#include <string.h>
#include <stdlib.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
int main()
{
    close(0);
    close(2);                                                                                                                                                                    
    int fd1 = open("./log1.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    int fd2 = open("./log2.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    int fd3 = open("./log3.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    int fd4 = open("./log4.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    int fd5 = open("./log5.txt",O_RDWR | O_CREAT | O_TRUNC,0666);
    printf("fd1: %d\n",fd1);
    printf("fd2: %d\n",fd2);
    printf("fd3: %d\n",fd3);
    printf("fd4: %d\n",fd4);
    printf("fd5: %d\n",fd5);
    close(fd1);
    close(fd2);
    close(fd3);
    close(fd4);
    close(fd5);
    return 0;
}
```

![img](https://pic4.zhimg.com/80/v2-b023236ad3d07e7db7b591c571039033_720w.webp)

**结论：** 文件描述符是从最小但是没有被使用的fd_array数组下标开始进行分配的

## 3.重定向

### 3.1 自实现重定向原理

#### 3.1.1 输出重定向

输出重定向就是，将本应该输出到一个文件的数据重定向输出到另一个文件中

![img](https://pic2.zhimg.com/80/v2-01d4eb5d8782d641ff0ae14c3deed671_720w.webp)

若想让本应该输出到"显示器文件"的数据输出到log.txt文件当中，可以在打开log.txt文件之前将文件描述符为1的文件关闭（即将“显示器文件”关闭）。当我们后续打开log.txt文件时所分配到的文件描述符就是1。

```text
#include <stdio.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
int main()
{
	close(1);
	int fd = open("./log.txt",O_RDWR | O_CREAT | O_TRUNC, 0666);
    if(fd < 0){
        perror("opern error:");
        return 1;
    }
    printf("hello world\n");
	fflush(stdout);//为什么需要刷新？阅读后续章节《缓冲区》
	close(fd);
	return 0;
}
```

![img](https://pic2.zhimg.com/80/v2-042054fbdc8ef28fd3a9278f873b0d79_720w.webp)

#### 3.1.2 追加重定向

输出重定向是覆盖式输出数据，而追加重定向是追加式输出数据，并无其他区别

```text
#include <stdio.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
int main()
{
    close(1);
    int fd = open("./log.txt",O_WRONLY | O_APPEND);
    if(fd < 0){
        perror("opern error:");
        return 1;
    }
    printf("hello world\n");                                                         
    fflush(stdout);
    return 0;
}
```

![img](https://pic4.zhimg.com/80/v2-4734750c4a2e9417ad5e2e5263381593_720w.webp)

#### 3.1.3 输入重定向

输入重定向，将本应该从一个文件读取数据，现在重定向为从另一个文件读取数据

![img](https://pic3.zhimg.com/80/v2-bfd71feb3c84b87c4c9479a9c5de9d46_720w.webp)

```text
#include <stdio.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
int main()
{
    close(0);
    int fd = open("./log.txt",O_RDONLY);
    if(fd < 0){
        perror("opern error:");
        return 1;
    }
    char buf[64] = {0};
    while(scanf("%s",buf) != EOF){
        printf("%s\n",buf);                                                          
    }
    return 0;
}
```

![img](https://pic4.zhimg.com/80/v2-c25b407eb80f017bbf332e40093534c7_720w.webp)

### 3.2 dup2函数

在Linux环境下还可以使用dup2()系统调用来实现重定向。dup2()本质上是通过fd_array数组中地址元素的拷贝完成重定向的。

![img](https://pic1.zhimg.com/80/v2-53977ff6cc642357ef07ba3d5b984594_720w.webp)

```text
#include <unistd.h>
int dup2(int oldfd, int newfd);
```

函数功能： 将fd_array[oldfd]的内容拷贝到fd_array[newfd]当中

函数返回值： 若调用成功则返回newfd，否则返回-1

- 若oldfd不是有效的文件描述符，则dup2调用失败，并且此时文件描述符为newfd的文件没有被关闭。
- 若oldfd是一个有效的文件描述符，但是newfd和oldfd具有相同的值，则dup2不做任何操作并直接返回newfd。

```text
#include <stdio.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <unistd.h>
#include <fcntl.h>
int main()
{
	int fd = open("log.txt", O_WRONLY | O_CREAT, 0666);
	if (fd < 0){
		perror("open");
		return 1;
	}
	close(1);
	dup2(fd, 1);
	printf("hello printf\n");
	fprintf(stdout, "hello fprintf\n");
	return 0;
}
```

![img](https://pic2.zhimg.com/80/v2-7392e31b82c2002297bc2c9e5499a65d_720w.webp)

## 4.C语言IO深入底层

相较于C库函数或其他语言的库函数而言，系统调用接口更为底层，语言层的库函数实际上都是对系统接口进行的封装。

在Linux环境下的C库底层使用的就是Linux操作系统提供的接口，在Windows环境下的C库底层使用的便是Windows操作系统给提供的接口，再通过条件编译即可实现跨平台

### 4.1 C语言中默认打开的三个流

前面提到任何进程在运行的时候都会默认打开三个流，即标准输入流、标准输出流以及标准错误流，对应到C语言当中就是stdin、stdout以及stderr。

```text
extern FILE *stdin;
extern FILE *stdout;
extern FILE *stderr;
```

stdin、stdout以及stderr都是属于FILE*类型的，与使用fopen()打开文件时获得的文件指针相同

注意： 不止是C语言当中stdin、stdout和stderr，C++当中也有对应的cin、cout和cerr，其他语言中也有类似的概念。这种特性并不是某种语言所特有的，而是由操作系统所支持的。

**标准输出流和标准错误流对应的都是显示器，它们有什么区别？**

```text
#include <stdio.h>
int main()
{
	printf("hello printf\n"); //stdout
	perror("perror"); //stderr
 
	fprintf(stdout, "stdout:hello fprintf\n"); //stdout
	fprintf(stderr, "stderr:hello fprintf\n"); //stderr
	return 0;
}
```

![img](https://pic3.zhimg.com/80/v2-f4b9daafb77052a1b3603860dff2d206_720w.webp)

从上面的结果来看，好像并没有什么区别，但是将程序运行结果重定向输出到文件log.txt当中后，可以发现log.txt文件当中只有向标准输出流输出的两行字符串，而向标准错误流输出的两行数据并没有重定向到文件当中，而是仍然输出到了显示器上。

![img](https://pic2.zhimg.com/80/v2-e8d1fdb55fd1dae7b793234aef28f4bd_720w.webp)

使用命令进行重定向时，默认重定向的是文件描述符为1的标准输出流，而并不会对文件描述符为2的标准错误流进行重定向。当然也可以显示指定重定向标准错误流。

![img](https://pic2.zhimg.com/80/v2-523b537f56cdcc2710799993b0805f61_720w.webp)

### 4.2 FILE

因为库函数是对系统调用接口的封装，本质上访问文件都是通过文件描述符fd进行访问的，所以C库当中的FILE结构体内部必定封装了文件描述符fd。

在/usr/include/stdio.h头文件中可以看到下面这句代码，也就是说FILE实际上就是struct _IO_FILE结构体的一个别名。

```text
typedef struct _IO_FILE FILE;
```

而在`/usr/include/libio.h`头文件中可以找到`struct _IO_FILE`结构体的定义，在该结构体的众多成员当中，我们可以看到一个名为`_fileno`的成员，这个成员实际上就是封装的文件描述符。

```text
struct _IO_FILE {
	int _flags;       /* High-order word is _IO_MAGIC; rest is flags. */
#define _IO_file_flags _flags
 
	//缓冲区相关
	/* The following pointers correspond to the C++ streambuf protocol. */
	/* Note:  Tk uses the _IO_read_ptr and _IO_read_end fields directly. */
	char* _IO_read_ptr;   /* Current read pointer */
	char* _IO_read_end;   /* End of get area. */
	char* _IO_read_base;  /* Start of putback+get area. */
	char* _IO_write_base; /* Start of put area. */
	char* _IO_write_ptr;  /* Current put pointer. */
	char* _IO_write_end;  /* End of put area. */
	char* _IO_buf_base;   /* Start of reserve area. */
	char* _IO_buf_end;    /* End of reserve area. */
	/* The following fields are used to support backing up and undo. */
	char *_IO_save_base; /* Pointer to start of non-current get area. */
	char *_IO_backup_base;  /* Pointer to first valid character of backup area */
	char *_IO_save_end; /* Pointer to end of non-current get area. */
 
	struct _IO_marker *_markers;
 
	struct _IO_FILE *_chain;
 
	int _fileno; //封装的文件描述符
#if 0
	int _blksize;
#else
	int _flags2;
#endif
	_IO_off_t _old_offset; /* This used to be _offset but it's too small.  */
 
#define __HAVE_COLUMN /* temporary */
	/* 1+column number of pbase(); 0 is unknown. */
	unsigned short _cur_column;
	signed char _vtable_offset;
	char _shortbuf[1];
 
	/*  char* _save_gptr;  char* _save_egptr; */
 
	_IO_lock_t *_lock;
#ifdef _IO_USE_OLD_IO_FILE
};
```

**C库文件IO函数的具体流程理解**

例：fopen函数在上层为用户申请FILE结构体变量，并返回该结构体的地址(FILE*)。在底层通过系统接口open打开对应的文件，得到文件描述符fd，并把fd填充到FILE结构体当中的_fileno变量中，至此便完成了文件的打开操作。

而C语言当中的其他文件操作函数，比如fread、fwrite、fputs、fgets等，都是先根据我们传入的文件指针找到对应的FILE结构体，然后在FILE结构体当中找到文件描述符，最后通过文件描述符对文件进行的一系列操作。

### 4.3 用户层缓冲区

```text
#include <stdio.h>
#include <unistd.h>
int main()
{
	printf("hello printf\n");
	fputs("hello fputs\n", stdout);
	write(1, "hello write\n", 12);
	fork();
	return 0;
}
```

![img](https://pic3.zhimg.com/80/v2-7a7e3bf34c8ea9ff1f0c8ce9bbe3f09a_720w.webp)

查看上述代码，貌似并没有什么奇特的地方，甚至这个fork()十分的多余。但是将程序执行结果重定向到log.txt之后会得到这种情况。

![img](https://pic4.zhimg.com/80/v2-b19eb5180d1ede77f8cb80175199527f_720w.webp)

用户层的缓冲区，通常有以下三种缓冲策略:

- 无缓冲
- 行缓冲。（常见于对显示器进行刷新数据）
- 全缓冲。（常见于对磁盘文件写入数据）

当直接执行可执行程序，将数据打印到显示器时所采用的就是行缓冲，又因为代码当中每个字符串后都有'\n'，所以当执行完对应代码后就立即将数据刷新到了显示器上。

但是将运行结果重定向到log.txt文件后，用户层缓冲区的刷新策略就变为了全缓冲，此时使用printf和fputs函数打印的数据都进入了C语言的缓冲区中，之后当我们使用fork函数创建子进程时，由于进程间具有独立性，而之后当父进程或是子进程对要刷新缓冲区内容时，本质就是对父子进程共享的数据进行了修改，此时就需要对数据进行写时拷贝，至此缓冲区当中的数据就变成了两份，一份父进程的，一份子进程的，所以重定向到log.txt文件当中printf和puts函数打印的数据就有两份。但由于write函数是系统接口，不存在用户层缓冲区，因此write函数打印的数据就只有一份。

**用户层缓冲区由谁提供？在哪？**

由C语言库实现提供。用户层的输出缓冲区就存在stdout中，stdout是一个FILE*的指针。在FILE结构体当中还有一大部分成员是用于记录缓冲区相关的信息的。

```text
//缓冲区相关
/* The following pointers correspond to the C++ streambuf protocol. */
/* Note:  Tk uses the _IO_read_ptr and _IO_read_end fields directly. */
char* _IO_read_ptr;   /* Current read pointer */
char* _IO_read_end;   /* End of get area. */
char* _IO_read_base;  /* Start of putback+get area. */
char* _IO_write_base; /* Start of put area. */
char* _IO_write_ptr;  /* Current put pointer. */
char* _IO_write_end;  /* End of put area. */
char* _IO_buf_base;   /* Start of reserve area. */
char* _IO_buf_end;    /* End of reserve area. */
```

**操作系统是否存在缓冲区？**

操作系统实际上也是存在缓冲区的，当刷新用户缓冲区的数据时，并不是直接将用户缓冲区的数据刷新到磁盘或是显示器上，而是先将数据刷新到操作系统缓冲区，然后再由操作系统将数据刷新到磁盘或是显示器上。（操作系统有自己的刷新机制，我们不必关系操作系统缓冲区的刷新规则）

## 5.理解"一切皆文件"思想

"一切皆文件"是Linux的设计哲学，体现在os的软件设计层面。

Linux是C语言编写的，但是如何使用C语言实现面向对象？

面向对象语言中的类大致由成员属性和成员方法构成，C语言中可以包含成员属性但是并不可以包含成员方法，但是可以使用函数指针指向函数来代替成员方法。

![img](https://pic3.zhimg.com/80/v2-85e21378b5830bb8d411bf78d6a96756_720w.webp)

通过这个设计方式，消除了上层处理硬件的差异，统一了看待文件的方式，一切皆文件。

## 6.文件系统

文件可以分为内存文件和磁盘文件，之前谈论的都是内存文件，接下开始讲解磁盘文件

### 6.1 认识inode

磁盘文件由两部分构成，分别是文件内容和文件属性（都存储在磁盘中）。文件内容就是文件当中存储的数据，文件属性就是文件的一些基本信息，例如文件名、文件大小以及文件创建时间等信息都是文件属性，文件属性又被称为元信息。

![img](https://pic1.zhimg.com/80/v2-99d0577506b6bc4259fc5e64891e33ac_720w.webp)

在Linux操作系统中，文件的元信息和内容是分离存储的，其中保存元信息的结构被称之为inode。因为系统当中存在大量的文件，所以需要给每个文件的属性集起一个唯一的编号，即inode号。即inode是一个文件的属性集合，Linux中每个文件都对应一个inode，为了区分系统中大量inode，我们为每个inode设置了inode编号。

![img](https://pic3.zhimg.com/80/v2-ac113825fcd80ff72dd73723cf046be6_720w.webp)

**ls**命令添加 **-i** 选项即可查看文件的inode编号

### 6.2 磁盘概念

内存是掉电易失存储介质，而磁盘是一种永久性存储介质。在计算机中，磁盘几乎是唯一的机械设备。磁盘在冯诺依曼体系结构当中既可以充当输入设备，又可以充当输出设备。

![img](https://pic2.zhimg.com/80/v2-53e8cd228ca9d0f242a7a5c4edb7caf5_720w.webp)

**磁盘寻址方案(CHS寻址)**

1. 确定读写信息在磁盘的哪个盘面
2. 确定读写信息在磁盘的哪个柱面(磁道)
3. 确定读写信息在磁盘的哪个扇区

### 6.3 磁盘分区与格式化

若要理解文件系统，先将磁盘想象成一个线性的存储介质。譬如磁带，当磁带被卷起来时就像磁盘一样是圆形的，但当将磁带拉直后就是线性的。

#### **6.3.1 磁盘分区**

磁盘通常被称为块设备，一般以扇区为单位，一个扇区的大小通常为512字节。计算机为了更好的管理磁盘，对磁盘进行了分区。磁盘分区就是使用分区编辑器在磁盘上划分几个逻辑部分，盘片一旦划分成数个分区，不同的目录与文件就可以存储进不同的分区。分区越多，就可以将文件的性质区分得越细，按照更为细分的性质，存储在不同的地方以管理文件。譬如在Windows操作系统下磁盘一般被分为C盘和D盘两个区域。

![img](https://pic1.zhimg.com/80/v2-8d83839aa4735187c0ba142af893b0cc_720w.webp)

#### 6.3.2 磁盘格式化

当磁盘完成分区后，还需对磁盘进行格式化。磁盘格式化就是对磁盘中的分区进行初始化的一种操作，这种操作通常会导致现有的磁盘或分区中所有的文件被清除。其实磁盘格式化就是对分区后的各个区域写入对应的管理信息。

写入的管理信息是什么是由文件系统决定的，不同的文件系统格式化时写入的管理信息是不同的，常见的文件系统有EXT2、EXT3、XFS、NTFS等。

### 6.4 EXT2文件系统的存储方案

对于每一个分区来说，分区的头部会包括一个启动块(Boot Block)，对于该分区的其余区域，EXT2文件系统会根据分区的大小将其划分为一个个的块组(Block Group)

注意: 启动块的大小是确定的，而块组的大小是由格式化的时候确定的，且不可随意更改

![img](https://pic4.zhimg.com/80/v2-27fa159d83e14d2007c0aed09af12c97_720w.webp)

而每个组块都有着相同的组成结构，每个组块都由超级块(Super Block)、块组描述符表(Group Descriptor Table)、块位图(Block Bitmap)、inode位图(inode Bitmap)、inode表(inode Table)以及数据表(Data Block)组成

![img](https://pic1.zhimg.com/80/v2-0434039f59fa7664f0c09fc3fee473b0_720w.webp)

1. Super Block： 存放文件系统本身的结构信息。记录的信息主要有：Data Block和inode的总量、未使用的Data Block和inode的数量、一个Data Block和inode的大小、最近一次挂载的时间、最近一次写入数据的时间、最近一次检验磁盘的时间等其他文件系统的相关信息。Super Block的信息被破坏，可以说整个文件系统结构就被破坏了。
2. Group Descriptor Table： 块组描述符表，描述该分区当中块组的属性信息
3. Block Bitmap： 块位图当中记录着Data Block中数据块是否被占用
4. inode Bitmap： inode位图当中记录着每个inode是否空闲可用
5. inode Table： 存放文件属性，即每个文件的inode
6. Data Blocks： 存放文件内容

**注意：**

1. 组块中会存在冗余的Super Block，当某个Super Block被破坏后可以通过该块组中的其他Super Block进行恢复
2. 磁盘分区并格式化后，每个分区的inode个数就确定了
3. 一个文件使用的数据块和inode结构的对应关系，通过一个数组进行维护。该数组一般可以存储15个元素，其中前12个元素分别对应该文件使用的12个数据块，剩余的三个元素分别是一级索引、二级索引和三级索引。当该文件使用数据块的个数超过12个时，可以用这三个索引进行数据块扩充

**如何创建一个文件？**

1. 通过遍历inode位图的方式，找到一个空闲的inode
2. 在inode表当中找到对应的inode，并将文件的属性信息填充进inode结构中
3. 将该文件的文件名和inode指针添加到目录文件的数据块中

**如何对文件写入信息？**

1. 通过文件的inode编号找到对应的inode结构
2. 通过inode结构找到存储该文件内容的数据块，并将数据写入数据块
3. 若不存在数据块或申请的数据块已被写满，则通过遍历块位图的方式找到一个空闲的块号，并在数据区当中找到对应的空闲块，再将数据写入数据块，最后还需要建立数据块和inode结构的映射关系

**如何删除文件？**

1. 将该文件对应的inode在inode位图当中置为无效。
2. 将该文件申请过的数据块在块位图当中置为无效。

此操作并不会真正将文件对应的信息删除，而只是将其inode号和数据块号置为了无效，所以当删除文件后短时间内是可以恢复的。但后续创建其他文件或是对其他文件进行写入操作申请数据块号时，可能会将该置为无效了的inode号和数据块号分配出去，此时删除文件的数据就会被覆盖，就无法恢复文件了。

为什么拷贝文件较慢，而删除文件较快呢？

因为拷贝文件需要先创建文件，然后再对该文件进行写入操作，该过程需要先申请inode号并填入文件的属性信息，之后还需要再申请数据块号，最后才能进行文件内容的数据拷贝，而删除文件只需将对应文件的inode号和数据块号置为无效即可，无需真正的删除文件。

对目录的认识

1. Linux中一切皆文件，目录也被看作是文件
2. 目录有自己的属性信息，目录的inode结构当中存储的就是目录的属性信息
3. 目录的数据块当中存储的就是该目录下的文件名以及对应文件的inode指针

### 6.5 软硬链接

#### 6.5.1 软链接

Linux环境下的软链接就类似与Windows环境下的快捷方式

![img](https://pic3.zhimg.com/80/v2-bf0d7acacbc52d78b472356a7b848cf6_720w.webp)

软链接又称符号链接，软链接文件相对于源文件来说是一个独立的文件，该文件有自己的inode号，但是该文件包含源文件的路径名。

但软链接文件只是其源文件的一个标记，当删除了源文件后，链接文件不能独立存在，虽然仍保留文件名，但却不能执行或是查看软链接的内容了。

#### 6.5.2 硬链接

硬链接文件的inode号与源文件的inode号是相同的，并且硬链接文件的大小与源文件的大小也是相同的。且当创建了一个硬链接文件后，该硬链接文件和源文件的硬链接数都变成了2。

![img](https://pic1.zhimg.com/80/v2-795aa1858ae3e334172813af9e6cd258_720w.webp)

硬链接文件就是源文件的一个别名，一个文件有几个文件名，该文件的硬链接数就是几。且当硬链接的源文件被删除后，硬链接文件仍能正常执行，只是文件的链接数减少了一个，因为此时该文件的文件名少了一个。

**为什么新建目录的的硬链接数是2？**

![img](https://pic4.zhimg.com/80/v2-49e81007a20825a6beee1b308d380537_720w.webp)

因为每个目录创建后，该目录下默认会有两个隐含文件.和..，分别代表当前目录和上级目录。因此这里创建的目录有两个名字，一个是dir另一个就是该目录下的.，所以刚创建的目录硬链接数是2。

![img](https://pic1.zhimg.com/80/v2-417c418f50fca1216d6935fc60720130_720w.webp)

通过上图也可以看出dir和该目录下的`.`的inode号相同，也可以说明它们代表的实际是同一个文件

原文地址：https://zhuanlan.zhihu.com/p/602871471

作者：linux

# 【NO.510】操作系统：文件系统的实现

## 1.文件系统结构

磁盘的逻辑单元为块，内存和磁盘之间的I/O传输以块为单位执行。

磁盘的特点

1. 1可以原地重写，可以从磁盘上读一块儿，修改该块，并将它写回到原来的位置
2. 可以直接访问磁盘上的任意一块。因此，可以方便地按顺序或随机访问文件

文件系统需要提供高效快捷磁盘访问，以便轻松存储、定位、提取数据。即存储文件、访问文件

文件系统有两个不同的设计问题

1. 访问问题：如何定义文件系统对用户的接口
2. 存储问题：创建数据结构和算法，把逻辑文件系统映射到物理外存设备

文件系统本身通常由许多不同层组成。每层实际利用更低层功能，创建新的功能，以用于更高层的服务。

![img](https://pic4.zhimg.com/80/v2-eb83a3d6c8f88ed77463323c8ee1bdeb_720w.webp)

设备驱动程序可以作为翻译器，他的输入作为高级指令，输出由底层的、硬件特定指令组成。

基础文件系统只需向适当设备驱动程序发送命令。

逻辑文件系统通过文件控制块维护文件结构。

文件控制块（FCB）包含有关文件的信息，包括所有者、权限、文件内容的位置等。

大多数操作系统支持多种不同的文件系统，举例：

- CD-ROM ISO9660 文件格式
- Unix 文件系统（Unix File System）
- Windows文件系统：FAT（File Allocation Table），FAT32, FAT64,NTFS（Windows NT File System）
- Linux 文件系统：可扩展文件系统（Extended file system），分布式文件系统（Distributed File System）

## 2.文件系统实现

### 2.1 概述

在磁盘上，文件系统包括的信息有

1. 如何启动存储在那里操作系统
2. 总的块数
3. 空闲块的数目和位置
4. 目录结构
5. 各个具体文件 等

上述许多结构会在之后详细讲述。这里简述如下：

- 引导控制块（每个卷）：可以包含从该卷引导操作系统的所需信息。如果磁盘不包括操作系统，则这块的内容为空。UFS称为引导块（boot block），NFS称为分区引导扇区（partition boot sector）
- 卷控制块（每个卷）：包括卷的详细信息（分区的块数、块的大小、空闲块的数量和指针、空闲
- FCB 的数量和指针等）。UFS称为超级块儿（super block）,NTFS主控文件表（master boot sector）
- 每个文件的FCB包含该文件的许多详细信息。他有一个唯一的标识号，以便与目录条目相关联
- 每个文件系统的目录结构用于组织文件

内存中的信息用于管理文件系统并通过缓存来提高性能，这些数据在安装文件装系统时被加载，在文件系统操作期间被更新，在卸载是被卸载。这些结构类型包括：

1. 每个进程的打开文件表：包括一个指向系统的打开文件表中合适条目的指针和其他信息
2. 整个系统的打开文件表：包括每个打开文件的FCB副本和其他信息

创建一个新文件

1. 应用程序调用逻辑文件系统。逻辑文件系统指导目录结构的格式，它会分配一个新的FCB
2. 系统将相应的目录信息读入内存
3. 更新目录结构和FCB
4. 将结果写回磁盘

一旦文件被创建，就能用于I/O，不过，首先他要被打开。系统调用open()将文件名传到逻辑文件系统，系统调用open()：

1. 首先搜索整个系统的打开文件表，查看是否已经被打开，如果是，则在该进程的打开文件表创建一个条目，并指向现有整个系统的打开文件表。
2. 否则，根据文件名搜索目录结构
3. 找到后，它的FCB会复制到内存的整个系统的开放文件表中（该表还存放着打开该文件的进程数量） ，接下来，在该进程的打开文件表创建一个条目，并指向现有整个系统的打开文件表。

Open() 返回值：文件描述符是一个非负整数。它是一进程打开文件表的索引值，指向系统范围内打开文件表相应条目

![img](https://pic4.zhimg.com/80/v2-684bb1515e1a671d2384b84166b4452b_720w.webp)

![img](https://pic1.zhimg.com/80/v2-69c3f8d2824448421a282ff88e8e2220_720w.webp)

### 2.2 虚拟文件系统

操作系统如何才能将多个类型的文件系统集成到目录结构中？用户如何在访问文件系统空间时，可以无缝地在文件系统类型间迁移？大多数操作系统采用面向对象的技术来简化、组织、模块化实现。

数据结构和程序用于隔离基本的操作系统调用的功能与实现细节。因此，文件系统的实现有三个主要层构成。

第一层为文件系统接口。

第二层为虚拟文件系统（VFS），把文件系统的通用操作和具体实现分开，虚拟文件系统提供了在唯一标识一个文件的机制。VFS基于vnode 的文件表示结构，它包含了一个数值标识符来唯一表示网络上的一个文件。

1. VFS能区分不同本地文件系统
2. VFS能区分本地文件系统和远程文件系统

![img](https://pic2.zhimg.com/80/v2-c4b200a1a02d96d22a7f1cc3c4fbea6d_720w.webp)

![img](https://pic1.zhimg.com/80/v2-8a630645ce4c80e79d48f613fc47d00c_720w.webp)

## 3.目录实现

### 3.1 线性列表

采用文件名称和数据块指针的线性列表

- 优点：编程简单
- 缺点：因为需要搜索，运行较为费时

### 3.2 哈希表

哈希表根据文件名得到一个值，并返回一个指向线性列表中元素的指针

- 优点：减少目录搜索时间
- 缺点：两个文件名哈希到相同的位置时可能发生冲突；因哈希表固定大小，创建文件需要哈希表重建时，比较麻烦。

## 4.磁盘空间的分配方法

### 4.1 连续分配

每个文件在磁盘上占有一组连续的块。 文件的连续分配可以用文件第一块的磁盘地址和连续块的数量（即长度）来定义

![img](https://pic1.zhimg.com/80/v2-3f776398d60d9c408c1f1aae7cf839ac_720w.webp)

连续分配支持顺序访问和直接访问

问题：当文件需要扩展，文件大小变大时会无法扩展

解决：找更大的连续空间，复制过去

**基于扩展的连续分配方案**
用以下参数来定义文件

1. 开始地址
2. 块儿数
3. 指向下一个扩展块儿的指针（扩展块儿可以是多个）

定义格式：

文件【开始地址，块儿数，指向下一个扩展块的指针】

### 4.2 链接分配

每个文件是磁盘块儿的链表，磁盘块分布在磁盘的任何地方，文件有起始块和结束块来定义

定义格式：【起始块，结束块】

同时，每个磁盘块都有指向下一个磁盘块的地址。

![img](https://pic4.zhimg.com/80/v2-e8242794f80d4a3c060a4ee64da9a163_720w.webp)

优点：没有磁盘空间浪费

缺点：

1. 不支持文件的直接访问
2. 需要更多的磁盘空间（来记录指针）

链接分配的一个重要变种是`文件分配表`

每个卷的开始部分用于存储文件分配表(File Allocation Table)，表中每个磁盘块都有一个FAT条目，并可通过块号索引。（未使用的块为0，使用的块包含下一个块儿号）

![img](https://pic1.zhimg.com/80/v2-bcbdc5e94fefc57e35f36b400540b8ec_720w.webp)

目录条目含有文件首块号码，通过这个块号索引的FAT条目包含文件下一块的号码，这个链会继续下去，直到最后一块，最后一块的表条目值为文件结束值。

![img](https://pic3.zhimg.com/80/v2-3dd9af7de7b66cdae25db4f0964012d6_720w.webp)

### 4.3 索引分配

通过将所有指针放在一起，即`索引块`

文件用索引块来定义， 每个文件有其索引块。

![img](https://pic4.zhimg.com/80/v2-8eefea9c07926893b23dfbf5f14f58e7_720w.webp)

这里有一个问题，`索引块应为多大`？

每个文件必须有一个索引块，因此索引块应尽可能小，然而不能太小，否则放不下足够多的指针，为处理这个问题，有如下一些机制：

1. 链接方案：为了处理大文件，可以将多个索引块链接起来
2. 多层次索引：用第一层索引块指向一组第二层的索引块，第二层索引块再指向文件块
3. 组合方案：用于基于UNIX的文件系统，将索引块的前15个指针存储在文件的i-node中。其中，前12个指针指向直接块，剩下3个指针指向间接块

![img](https://pic4.zhimg.com/80/v2-65d28e8a454642ec2c43de8f394afc9f_720w.webp)

## 5.磁盘空闲空间的管理

### 5.1 位向量

空闲空间表实现为位图, 或位向量，每块用一位（bit）表示。1表示块空闲；0表示块已分配

### 5.2 链表

所有空闲块用链表链接起来，并将指向第一个空闲块儿的指针保存在特殊位置，同时缓存在内存。

每个块儿含有下一个块儿的指针

### 5.3 组

将n个空闲块的地址保存在第一个空闲块中。

这些空闲块中的前n－1个为空，而最后一块包含另外n个空闲块的地址。

`比链表好的是`空闲块的地址可以很快找到，而且可以明确一段`连续`空闲块空间

例：n=3

![img](https://pic3.zhimg.com/80/v2-543d7be45e9670008a47026bbb85fd22_720w.webp)

### 5.4 计数

基于以下事实：

通常有多个连续块需要同时分配或释放，尤其是在使用连续分配时。因此记录

- 记录第一块的地址和紧跟第一块的连续的空闲块的数量。
- 空闲空间表的每个条目包括`磁盘地址`和`数量`

例：

![img](https://pic3.zhimg.com/80/v2-2e2d5a94a4167db84fb3ec8178e8d2be_720w.webp)

## 6.文件系统的性能和效率

磁盘空间的有效使用（效率），取决于

- 磁盘分配和目录管理算法
- 保留在文件目录条目中的数据类型

改善性能的方法：`缓存`

1. 缓冲区缓存：一块独立内存，位于其中的块是马上需要使用的
2. 页面缓存：将文件数据作为页而不是块来缓存。页面缓存使用虚拟内存技术，将文件数据作为页来缓存，比采用物理磁盘块来缓存更高效
3. 板载高速缓存

![img](https://pic1.zhimg.com/80/v2-d55559911413be535b8dbe257290957c_720w.webp)

如果没有统一缓存，则会由下图情况发生：

![img](https://pic2.zhimg.com/80/v2-67cc8f5ef9a30e8e435541f83e0f0775_720w.webp)

系统调用read()和write()会通过缓冲区缓存，然而，内存映射调用需要使用两个缓存，即页面缓存和缓冲区缓存。内存映射先从文件系统中读入磁盘块,并放入缓冲区缓存，由于虚拟内存系统没有缓冲区缓存接口，缓冲缓存内的文件必须复制到页面缓存中。

采用统一缓冲缓存

统一缓冲缓存：统一使用缓冲器缓存来缓存进程页和文件数据。

![img](https://pic1.zhimg.com/80/v2-11ca2fd2cc3d5cdf62f56ee4ec534e90_720w.webp)

无论是缓存块还是页面都有`置换问题`，

文件的读入或写出一般是`按顺序进行`。所以，不适合采用LRU算法，因为最近使用的页面最后才会用甚至根本不会再用。

顺序访问可以通过`马上释放`和`预先读取`来加以优化

1. 马上释放（free-behind）：请求下一页时，马上释放上一页
2. 预先读取（read-ahead）：请求页之后的下一个页也一起读入

## 7.文件系统的恢复

目录信息一般事先保存在内存中以加快访问，有时会导致目录结构中的数据和磁盘块中的数据不一致。

解决：

1. 一致性检查：比较目录结构中的数据和磁盘块中的数据，尝试着去修正不一致
2. 备份&恢复：
   I. 备份（backup）：利用系统程序来备份数据到其他的存储设备。软盘，磁带
   II. 恢复（recovery）：通过从备份来恢复丢失的文件或磁盘

基于日志结构的文件系统

- 文件创建涉及到目录结构修改，FCB分配，数据块分配等
- 所有元数据（meta data）的变化写入日志上，一旦这些修改写到日志，就认为已经提交了。
- 提交了的事务，并不一定马上完成操作
- 当整个提交的事务已经完成时，就从日志中删除事务条目
- 如果系统崩溃，日志文件可能还存在事务，它包含的任何事务虽然已经由操作系统提交了，但还没有完成到文件系统，必须重新执行。

原文地址：https://zhuanlan.zhihu.com/p/603445185

作者：linux

# 【NO.511】Linux网络分析必备技能：tcpdump实战详解

`tcpdump`，它是 Linux 系统中特别有用的网络工具，通常用于故障诊断、网络分析，功能非常的强大。

相对于其它 Linux 工具而言，`tcpdump` 是复杂的。当然我也不推荐你去学习它的全部，学以致用，能够解决工作中的问题才是关键。

本文会从*应用场景*和*基础原理*出发，提供丰富的*实践案例*，让你快速的掌握 `tcpdump` 的核心使用方法，足以应对日常工作的需求。

## 1.**应用场景**

在日常工作中遇到的很多网络问题都可以通过 tcpdump 优雅的解决：

*1.* 相信大多数同学都遇到过 SSH 连接服务器缓慢，通过 tcpdump 抓包，可以快速定位到具体原因，一般都是因为 DNS 解析速度太慢。

*2.* 当我们工程师与用户面对网络问题争执不下时，通过 tcpdump 抓包，可以快速定位故障原因，轻松甩锅，毫无压力。

*3.* 当我们新开发的网络程序，没有按照预期工作时，通过 tcpdump 收集相关数据包，从包层面分析具体原因，让问题迎刃而解。

*4.* 当我们的网络程序性能比较低时，通过 tcpdump 分析数据流特征，结合相关协议来进行网络参数优化，提高系统网络性能。

*5.* 当我们学习网络协议时，通过 tcpdump 抓包，分析协议格式，帮助我们更直观、有效、快速的学习网络协议。

上述只是简单罗列几种常见的应用场景，而 tcpdump 在网络诊断、网络优化、协议学习方面，确实是一款非常强大的网络工具，只要存在网络问题的地方，总能看到它的身影。

熟练的运用 `tcpdump`，可以帮助我们解决工作中各种网络问题，下边我们先简单学习下它的工作原理。

## 2.**工作原理**

tcpdump 是 Linux 系统中非常有用的网络工具，运行在用户态，本质上是通过调用 `libpcap` 库的各种 `api` 来实现数据包的抓取功能。

![img](https://pic4.zhimg.com/80/v2-ffd675508f1cee9bff0a0e30fede9dbb_720w.webp)

通过上图，我们可以很直观的看到，数据包到达网卡后，经过数据包过滤器（BPF）筛选后，拷贝至用户态的 tcpdump 程序，以供 tcpdump 工具进行后续的处理工作，输出或保存到 pcap 文件。

数据包过滤器（BPF）主要作用，就是根据用户输入的过滤规则，只将用户关心的数据包拷贝至 tcpdump，这样能够减少不必要的数据包拷贝，降低抓包带来的性能损耗。

**思考**：这里分享一个真实的面试题

> 面试官：如果某些数据包被 iptables 封禁，是否可以通过 tcpdump 抓到包？

通过上图，我们可以很轻易的回答此问题。

因为 Linux 系统中 `netfilter` 是工作在协议栈阶段的，tcpdump 的过滤器（BPF）工作位置在协议栈之前，所以当然是可以抓到包了！

我们理解了 tcpdump 基本原理之后，下边直接进入实战！

## 3.**实战：基础用法**

我们先通过几个简单的示例来介绍 tcpdump 基本用法。

*1.* 不加任何参数，默认情况下将抓取第一个非 lo 网卡上所有的数据包

```text
$ tcpdump 
```

*2.* 抓取 eth0 网卡上的所有数据包

```text
$ tcpdump -i eth0
```

*3.* 抓包时指定 `-n` 选项，不解析主机和端口名。这个参数很关键，会影响抓包的性能，一般抓包时都需要指定该选项。

```text
$ tcpdump -n -i eth0
```

*4.* 抓取指定主机 `192.168.1.100` 的所有数据包

```text
$ tcpdump -ni eth0 host 192.168.1.100
```

*5.* 抓取指定主机 `10.1.1.2` 发送的数据包

```text
$ tcpdump -ni eth0 src host 10.1.1.2
```

*6.* 抓取发送给 `10.1.1.2` 的所有数据包

```text
$ tcpdump -ni eth0 dst host 10.1.1.2
```

*7.* 抓取 eth0 网卡上发往指定主机的数据包，抓到 10 个包就停止，这个参数也比较常用

```text
$ tcpdump -ni eth0 -c 10 dst host 192.168.1.200
```

*8.* 抓取 eth0 网卡上所有 SSH 请求数据包，SSH 默认端口是 22

```text
$ tcpdump -ni eth0 dst port 22
```

*9.* 抓取 eth0 网卡上 5 个 ping 数据包

```text
$ tcpdump -ni eth0 -c 5 icmp
```

*10.* 抓取 eth0 网卡上所有的 arp 数据包

```text
$ tcpdump -ni eth0 arp
```

*11.* 使用十六进制输出，当你想检查数据包内容是否有问题时，十六进制输出会很有帮助。

```text
$ tcpdump -ni eth0 -c 1 arp -X
listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes
12:13:31.602995 ARP, Request who-has 172.17.92.133 tell 172.17.95.253, length 28
    0x0000:  0001 0800 0604 0001 eeff ffff ffff ac11  ................
    0x0010:  5ffd 0000 0000 0000 ac11 5c85            _.........\.
```

*12.* 只抓取 eth0 网卡上 IPv6 的流量

```text
$ tcpdump -ni eth0 ip6
```

*13.* 抓取指定端口范围的流量

```text
$ tcpdump -ni eth0 portrange 80-9000
```

*14.* 抓取指定网段的流量

```text
$ tcpdump -ni eth0 net 192.168.1.0/24
```



## **4.实战：高级进阶**

tcpdump 强大的功能和灵活的策略，主要体现在过滤器（BPF）强大的表达式组合能力。

本节主要分享一些常见的所谓高级用法，希望读者能够举一反三，根据自己实际需求，来灵活使用它。

*1.* 抓取指定客户端访问 ssh 的数据包

```text
$ tcpdump -ni eth0 src 192.168.1.100 and dst port 22
```

*2.* 抓取从某个网段来，到某个网段去的流量

```text
$ tcpdump -ni eth0 src net 192.168.1.0/16 and dst net 10.0.0.0/8 or 172.16.0.0/16
```

*3.* 抓取来自某个主机，发往非 ssh 端口的流量

```text
$ tcpdump -ni eth0 src 10.0.2.4 and not dst port 22
```

*4.* 当构建复杂查询的时候，你可能需要使用引号，单引号告诉 tcpdump 忽略特定的特殊字符，这里的 `()` 就是特殊符号，如果不用引号的话，就需要使用转义字符

```text
$ tcpdump -ni eth0 'src 10.0.2.4 and (dst port 3389 or 22)'
```

*5.* 基于包大小进行筛选，如果你正在查看特定的包大小，可以使用这个参数

小于等于 64 字节：

```text
$ tcpdump -ni less 64
```

大于等于 64 字节：

```text
$ tcpdump -ni eth0 greater 64
```

等于 64 字节：

```text
$ tcpdump -ni eth0 length == 64
```

*6.* 过滤 TCP 特殊标记的数据包

抓取某主机发送的 `RST` 数据包：

```text
$ tcpdump -ni eth0 src host 192.168.1.100 and 'tcp[tcpflags] & (tcp-rst) != 0'
```

抓取某主机发送的 `SYN` 数据包：

```text
$ tcpdump -ni eth0 src host 192.168.1.100 and 'tcp[tcpflags] & (tcp-syn) != 0'
```

抓取某主机发送的 `FIN` 数据包：

```text
$ tcpdump -ni eth0 src host 192.168.1.100 and 'tcp[tcpflags] & (tcp-fin) != 0'
```

抓取 TCP 连接中的 `SYN` 或 `FIN` 包

```text
$ tcpdump 'tcp[tcpflags] & (tcp-syn|tcp-fin) != 0'
```

*7.* 抓取所有非 ping 类型的 `ICMP` 包

```text
$ tcpdump 'icmp[icmptype] != icmp-echo and icmp[icmptype] != icmp-echoreply'
```

*8.* 抓取端口是 80，网络层协议为 IPv4， 并且含有数据，而不是 SYN、FIN 以及 ACK 等不含数据的数据包

```text
$ tcpdump 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)'
```

解释一下这个复杂的表达式，具体含义就是，整个 IP 数据包长度减去 IP 头长度，再减去 TCP 头的长度，结果不为 0，就表示数据包有 `data`，如果还不是很理解，需要自行补一下 `tcp/ip` 协议

*9.* 抓取 HTTP 报文，`0x4754` 是 `GET` 前两字符的值，`0x4854` 是 `HTTP` 前两个字符的值

```text
$ tcpdump  -ni eth0 'tcp[20:2]=0x4745 or tcp[20:2]=0x4854'
```

## 5.**常用选项**

通过上述的实战案例，相信大家已经掌握的 `tcpdump` 基本用法，在这里来详细总结一下常用的选项参数。

**（一）基础选项**

- `-i`：指定接口
- `-D`：列出可用于抓包的接口
- `-s`：指定数据包抓取的长度
- `-c`：指定要抓取的数据包的数量
- `-w`：将抓包数据保存在文件中
- `-r`：从文件中读取数据
- `-C`：指定文件大小，与 `-w` 配合使用
- `-F`：从文件中读取抓包的表达式
- `-n`：不解析主机和端口号，这个参数很重要，一般都需要加上
- `-P`：指定要抓取的包是流入还是流出的包，可以指定的值 `in`、`out`、`inout`

**（二）输出选项**

- `-e`：输出信息中包含数据链路层头部信息
- `-t`：显示时间戳，`tttt` 显示更详细的时间
- `-X`：显示十六进制格式
- `-v`：显示详细的报文信息，尝试 `-vvv`，`v` 越多显示越详细

## 6.**过滤表达式**

tcpdump 强大的功能和灵活的策略，主要体现在过滤器（BPF）强大的表达式组合能力。

**（一）操作对象**

表达式中可以操作的对象有如下几种：

- `type`，表示对象的类型，比如：`host`、`net`、`port`、`portrange`，如果不指定 type 的话，默认是 host
- `dir`：表示传输的方向，可取的方式为：`src`、`dst`。
- `proto`：表示协议，可选的协议有：`ether`、`ip`、`ip6`、`arp`、`icmp`、`tcp`、`udp`。

**（二）条件组合**

表达对象之间还可以通过关键字 `and`、`or`、`not` 进行连接，组成功能更强大的表达式。

- `or`：表示或操作
- `and`：表示与操作
- `not`：表示非操作

建议看到这里后，再回头去看实战篇章的示例，相信必定会有更深的理解。如果是这样，那就达到了我预期的效果了！

## 7.**经验**

到这里就不再加新知识点了，分享一些工作中总结的经验：

*1.* 我们要知道 `tcpdump` 不是万能药，并不能解决所有的网络问题。

*2.* 在高流量场景下，抓包可能会影响系统性能，如果是在生产环境，请谨慎使用！

*3.* 在高流量场景下，`tcpdump` 并不适合做流量统计，如果需要，可以使用交换机镜像的方式去分析统计。

*4.* 在 Linux 上使用 `tcpdump` 抓包，结合 `wireshark` 工具进行数据分析，能事半功倍。

*5.* 抓包时，尽可能不要使用 `any` 接口来抓包。

*6.* 抓包时，尽可能指定详细的数据包过滤表达式，减少无用数据包的拷贝。

*7.* 抓包时，尽量指定 `-n` 选项，减少解析主机和端口带来的性能开销。

## 8.**最后**

通过上述内容，我们知道 tcpdump 是一款功能强大的故障诊断、网络分析工具。在我们的日常工作中，遇到的网络问题总是能够通过 tcpdump 来解决。

不过 tcpdump 相对于其它 Linux 命令来说，会复杂很多，但鉴于它强大功能的诱惑力，我们多花一些时间是值得的。要想很好地掌握 tcpdump，需要对网络报文（`TCP/IP`协议）有一定的了解。

当然，对于简单的使用来说，只要有网络基础概念就行，掌握了 tcpdump 常用方法，就足以应付工作中大部分网络相关的疑难杂症了。

原文地址：https://zhuanlan.zhihu.com/p/413937603

作者：linux

# 【NO.512】大厂面试题之计算机网络重点篇(附答案)

## 1.ISO七层模型中表示层和会话层功能是什么？

- 表示层：图像、视频编码解，数据加密。
- 会话层：建立会话，如session认证、断点续传。

## 2.三次握手四次挥手的变迁图

《TCP/IP详解 卷1:协议》有一张TCP状态变迁图，很具有代表性，有助于大家理解三次握手和四次挥手的状态变化。如下图所示，粗的实线箭头表示正常的客户端状态变迁，粗的虚线箭头表示正常的服务器状态变迁。

![img](https://pic3.zhimg.com/80/v2-3c861e281f780ae2c4b4602d4cf5f556_720w.webp)

## 3.对称密钥加密的优点缺点？

对称密钥加密（Symmetric-Key Encryption），加密和解密使用同一密钥。

- 优点：运算速度快
- 缺点：无法安全地将密钥传输给通信方

## 4.非对称密钥加密你了解吗？优缺点？

非对称密钥加密，又称公开密钥加密（Public-Key Encryption），加密和解密使用不同的密钥。

公开密钥所有人都可以获得，通信发送方获得接收方的公开密钥之后，就可以使用公开密钥进行加密，接收方收到通信内容后使用私有密钥解密。

非对称密钥除了用来加密，还可以用来进行签名。因为私有密钥无法被其他人获取，因此通信发送方使用其私有密钥进行签名，通信接收方使用发送方的公开密钥对签名进行解密，就能判断这个签名是否正确。

- 优点：可以更安全地将公开密钥传输给通信发送方；
- 缺点：运算速度慢。

## 5.HTTPS是什么

HTTPS 并不是新协议，而是让 **HTTP 先和 SSL（Secure Sockets Layer）通信，再由 SSL 和 TCP 通信，也就是说 HTTPS 使用了隧道进行通信**。通过使用 SSL，HTTPS 具有了加密（防窃听）、认证（防伪装）和完整性保护（防篡改）。

## 6.HTTP的缺点有哪些？

- 使用明文进行通信，内容可能会被窃听；
- 不验证通信方的身份，通信方的身份有可能遭遇伪装；
- 无法证明报文的完整性，报文有可能遭篡改。

## 7.HTTPS采用的加密方式有哪些？是对称还是非对称？

HTTPS 采用混合的加密机制，使用非对称密钥加密用于传输对称密钥来保证传输过程的安全性，之后使用对称密钥加密进行通信来保证通信过程的效率。

![image-20230216165637386](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230216165637386.png)

确保传输安全过程（其实就是rsa原理）：

1. Client给出协议版本号、一个客户端生成的随机数（Client random），以及客户端支持的加密方法。
2. Server确认双方使用的加密方法，并给出数字证书、以及一个服务器生成的随机数（Server random）。
3. Client确认数字证书有效，然后生成呀一个新的随机数（Premaster secret），并使用数字证书中的公钥，加密这个随机数，发给Server。
4. Server使用自己的私钥，获取Client发来的随机数（Premaster secret）。
5. Client和Server根据约定的加密方法，使用前面的三个随机数，生成”对话密钥”（session key），用来加密接下来的整个对话过程。

## 8.为什么有的时候刷新页面不需要重新建立 SSL 连接？

TCP 连接有的时候会被浏览器和服务端维持一段时间，TCP 不需要重新建立，SSL 自然也会用之前的。

## 9.SSL中的认证中的证书是什么？了解过吗？

通过使用 **证书** 来对通信方进行认证。

数字证书认证机构（CA，Certificate Authority）是客户端与服务器双方都可信赖的第三方机构。

服务器的运营人员向 CA 提出公开密钥的申请，CA 在判明提出申请者的身份之后，会对已申请的公开密钥做数字签名，然后分配这个已签名的公开密钥，并将该公开密钥放入公开密钥证书后绑定在一起。

进行 HTTPS 通信时，服务器会把证书发送给客户端。客户端取得其中的公开密钥之后，先使用数字签名进行验证，如果验证通过，就可以开始通信了。

## 10.HTTP如何禁用缓存？如何确认缓存？

HTTP/1.1 通过 Cache-Control 首部字段来控制缓存。

禁止进行缓存

no-store 指令规定不能对请求或响应的任何一部分进行缓存。

```text
Cache-Control: no-store
```

强制确认缓存

no-cache 指令规定缓存服务器需要先向源服务器验证缓存资源的有效性，只有当缓存资源有效时才能使用该缓存对客户端的请求进行响应。

```text
Cache-Control: no-cache
```

## 11.GET与POST传递数据的最大长度能够达到多少呢？

get 是通过URL提交数据，因此GET可提交的数据量就跟URL所能达到的最大长度有直接关系。

很多文章都说GET方式提交的数据**最多只能是1024字节**，而实际上，URL不存在参数上限的问题，HTTP协议规范也没有对URL长度进行限制。

**这个限制是特定的浏览器及服务器对它的限制**，比如IE对URL长度的限制是2083字节(2K+35字节)。对于其他浏览器，如FireFox，Netscape等，则没有长度限制，这个时候其限制取决于服务器的操作系统；即如果url太长，服务器可能会因为安全方面的设置从而拒绝请求或者发生不完整的数据请求。

post 理论上讲是没有大小限制的，HTTP协议规范也没有进行大小限制，但实际上post所能传递的数据量大小取决于服务器的设置和内存大小。

因为我们一般post的数据量很少超过MB的，所以我们很少能感觉的到post的数据量限制，但实际中如果你上传文件的过程中可能会发现这样一个问题，即上传个头比较大的文件到服务器时候，可能上传不上去。

以php语言来说，查原因的时候你也许会看到有说PHP上传文件涉及到的参数PHP默认的上传有限定，一般这个值是2MB，更改这个值需要更改php.conf的post_max_size这个值。这就很明白的说明了这个问题了。

## 12.网络层常见协议？可以说一下吗？

![img](https://pic3.zhimg.com/80/v2-0236ac5821ff02bb2cca7a2466fc2bda_720w.webp)

## 13.TCP四大拥塞控制算法总结？（极其重要）

**四大算法**

拥塞控制主要是四个算法：1）慢启动，2）拥塞避免，3）拥塞发生，4）快速恢复。这四个算法不是一天都搞出来的，这个四算法的发展经历了很多时间，到今天都还在优化中。

**慢热启动算法 – Slow Start**

所谓慢启动，也就是TCP连接刚建立，一点一点地提速，试探一下网络的承受能力，以免直接扰乱了网络通道的秩序。

慢启动算法：

1. 连接建好的开始先初始化拥塞窗口cwnd大小为1，表明可以传一个MSS大小的数据。
2. 每当收到一个ACK，cwnd大小加一，呈线性上升。
3. 每当过了一个往返延迟时间RTT(Round-Trip Time)，cwnd大小直接翻倍，乘以2，呈指数让升。
4. 还有一个ssthresh（slow start threshold），是一个上限，当cwnd >= ssthresh时，就会进入“拥塞避免算法”（后面会说这个算法）

**拥塞避免算法 – Congestion Avoidance**

如同前边说的，当拥塞窗口大小cwnd大于等于慢启动阈值ssthresh后，就进入拥塞避免算法。算法如下：

1. 收到一个ACK，则cwnd = cwnd + 1 / cwnd
2. 每当过了一个往返延迟时间RTT，cwnd大小加一。

过了慢启动阈值后，拥塞避免算法可以避免窗口增长过快导致窗口拥塞，而是缓慢的增加调整到网络的最佳值。

**拥塞发生状态时的算法**

一般来说，TCP拥塞控制默认认为网络丢包是由于网络拥塞导致的，所以一般的TCP拥塞控制算法以丢包为网络进入拥塞状态的信号。对于丢包有两种判定方式，一种是超时重传RTO[Retransmission Timeout]超时，另一个是收到三个重复确认ACK。

超时重传是TCP协议保证数据可靠性的一个重要机制，其原理是在发送一个数据以后就开启一个计时器，在一定时间内如果没有得到发送数据报的ACK报文，那么就重新发送数据，直到发送成功为止。

但是如果发送端接收到3个以上的重复ACK，TCP就意识到数据发生丢失，需要重传。这个机制不需要等到重传定时器超时，所以叫 做快速重传，而快速重传后没有使用慢启动算法，而是拥塞避免算法，所以这又叫做快速恢复算法。

超时重传RTO[Retransmission Timeout]超时，TCP会重传数据包。TCP认为这种情况比较糟糕，反应也比较强烈：

- 由于发生丢包，将慢启动阈值ssthresh设置为当前cwnd的一半，即ssthresh = cwnd / 2.
- cwnd重置为1
- 进入慢启动过程

最为早期的TCP Tahoe算法就只使用上述处理办法，但是由于一丢包就一切重来，导致cwnd又重置为1，十分不利于网络数据的稳定传递。

所以，TCP Reno算法进行了优化。当收到三个重复确认ACK时，TCP开启快速重传Fast Retransmit算法，而不用等到RTO超时再进行重传：

- cwnd大小缩小为当前的一半
- ssthresh设置为缩小后的cwnd大小
- 然后进入快速恢复算法Fast Recovery。

![img](https://pic4.zhimg.com/80/v2-c47542f694b02a32d6fc29684467605f_720w.webp)

**快速恢复算法 – Fast Recovery**

TCP Tahoe是早期的算法，所以没有快速恢复算法，而Reno算法有。在进入快速恢复之前，cwnd和ssthresh已经被更改为原有cwnd的一半。快速恢复算法的逻辑如下：

- cwnd = cwnd + 3 MSS，加3 MSS的原因是因为收到3个重复的ACK。
- 重传DACKs指定的数据包。
- 如果再收到DACKs，那么cwnd大小增加一。
- 如果收到新的ACK，表明重传的包成功了，那么退出快速恢复算法。将cwnd设置为ssthresh，然后进入拥塞避免算法。

如图所示，第五个包发生了丢失，所以导致接收方接收到三次重复ACK，也就是ACK5。所以将ssthresh设置当当时cwnd的一半，也就是6/2 = 3，cwnd设置为3 + 3 = 6。然后重传第五个包。当收到新的ACK时，也就是ACK11，则退出快速恢复阶段，将cwnd重新设置为当前的ssthresh，也就是3，然后进入拥塞避免算法阶段。

## 14.为何快速重传是选择3次ACK？

主要的考虑还是要区分包的丢失是由于链路故障还是乱序等其他因素引发。

两次duplicated ACK时很可能是乱序造成的！三次duplicated ACK时很可能是丢包造成的！四次duplicated ACK更更更可能是丢包造成的，但是这样的响应策略太慢。丢包肯定会造成三次duplicated ACK!综上是选择收到三个重复确认时窗口减半效果最好，这是实践经验。

在没有fast retransmit / recovery 算法之前，重传依靠发送方的retransmit timeout，就是在timeout内如果没有接收到对方的ACK，默认包丢了，发送方就重传，包的丢失原因

1）包checksum 出错

2）网络拥塞

3）网络断，包括路由重收敛，但是发送方无法判断是哪一种情况，于是采用最笨的办法，就是将自己的发送速率减半，即CWND 减为1/2，这样的方法对2是有效的，可以缓解网络拥塞，3则无所谓，反正网络断了，无论发快发慢都会被丢；但对于1来说，丢包是因为偶尔的出错引起，一丢包就对半减速不合理。

于是有了fast retransmit 算法，基于在反向还可以接收到ACK，可以认为网络并没有断，否则也接收不到ACK，如果在timeout 时间内没有接收到> 2 的duplicated ACK，则概率大事件为乱序，乱序无需重传，接收方会进行排序工作；

而如果接收到三个或三个以上的duplicated ACK，则大概率是丢包，可以逻辑推理，发送方可以接收ACK，则网络是通的，可能是1、2造成的，先不降速，重传一次，如果接收到正确的ACK，则一切OK，流速依然（包出错被丢）。

而如果依然接收到duplicated ACK，则认为是网络拥塞造成的，此时降速则比较合理。

## 15.对于FIN_WAIT_2，CLOSE_WAIT状态和TIME_WAIT状态？你知道多少?

FIN_WAIT_2：

- - 半关闭状态。
  - 发送断开请求一方还有接收数据能力，但已经没有发送数据能力。

CLOSE_WAIT状态：

- - 被动关闭连接一方接收到FIN包会立即回应ACK包表示已接收到断开请求。
  - 被动关闭连接一方如果还有剩余数据要发送就会进入CLOSED_WAIT状态。

TIME_WAIT状态：

- - 又叫2MSL等待状态。
  - 如果客户端直接进入CLOSED状态，如果服务端没有接收到最后一次ACK包会在超时之后重新再发FIN包，此时因为客户端已经CLOSED，所以服务端就不会收到ACK而是收到RST。所以TIME_WAIT状态目的是防止最后一次握手数据没有到达对方而触发重传FIN准备的。
  - 在2MSL时间内，同一个socket不能再被使用，否则有可能会和旧连接数据混淆（如果新连接和旧连接的socket相同的话）。

## 16.你了解流量控制原理吗？

目的是接收方通过TCP头窗口字段告知发送方本方可接收的最大数据量，用以解决发送速率过快导致接收方不能接收的问题。所以流量控制是点对点控制。

TCP是双工协议，双方可以同时通信，所以发送方接收方各自维护一个发送窗和接收窗。

- - 发送窗：用来限制发送方可以发送的数据大小，其中发送窗口的大小由接收端返回的TCP报文段中窗口字段来控制，接收方通过此字段告知发送方自己的缓冲（受系统、硬件等限制）大小。
  - 接收窗：用来标记可以接收的数据大小。

TCP是流数据，发送出去的数据流可以被分为以下四部分：已发送且被确认部分 | 已发送未被确认部分 | 未发送但可发送部分 | 不可发送部分，其中发送窗 = 已发送未确认部分 + 未发但可发送部分。接收到的数据流可分为：已接收 | 未接收但准备接收 | 未接收不准备接收。接收窗 = 未接收但准备接收部分。

发送窗内数据只有当接收到接收端某段发送数据的ACK响应时才移动发送窗，左边缘紧贴刚被确认的数据。接收窗也只有接收到数据且最左侧连续时才移动接收窗口。

## 17.建立TCP服务器的各个系统调用过程是怎样的？

![img](https://pic3.zhimg.com/80/v2-454e8297f173ca5d05b031609c40a33e_720w.webp)

![img](https://pic3.zhimg.com/80/v2-a67a9deff7ba5ae1da98f8e5504bf6be_720w.webp)

服务器：

- - fd：accept返回的连接描述字，每个连接有一个，生命周期为连接周期。
  - 注：sockfd是监听描述字，一个服务器只有一个，用于监听是否有连接；fd是连接描述字，用于每个连接的操作。
  - fd：连接描述字。
  - buf：缓冲区buf。
  - count：缓冲区长度。
  - 注：大于0表示读取的字节数，返回0表示文件读取结束，小于0表示发生错误。
  - sockfd：服务器socket描述字。
  - addr：指向地址结构指针。
  - addrlen：协议地址长度。
  - 注：一旦accept某个客户机请求成功将返回一个全新的描述符用于标识具体客户的TCP连接。
  - sockfd：要监听的sock描述字。
  - backlog：socket可以排队的最大连接数。
  - sockfd：socket返回的套接字描述符，类似于文件描述符fd。
  - addr：有个sockaddr类型数据的指针，指向的是被绑定结构变量。
  - addrlen：地址长度。
  - domain：协议域，决定了socket的地址类型，IPv4为AF_INET。
  - type：指定socket类型，SOCK_STREAM为TCP连接。
  - protocol：指定协议。IPPROTO_TCP表示TCP协议，为0时自动选择type默认协议。
  - 创建socket -> int socket(int domain, int type, int protocol);
  - 绑定socket和端口号 -> int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

```text
    // IPv4的sockaddr地址结构
    struct sockaddr_in {
        sa_family_t sin_family;    // 协议类型，AF_INET
        in_port_t sin_port;    // 端口号
        struct in_addr sin_addr;    // IP地址
    };
    struct in_addr {
        uint32_t s_addr;
    }
```

- - 监听端口号 -> int listen(int sockfd, int backlog);
  - 接收用户请求 -> int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
  - 从socket中读取字符 -> ssize_t read(int fd, void *buf, size_t count);
  - 关闭socket -> int close(int fd);

客户机：

- - fd：同服务器端fd。
  - fd、buf、count：同read中意义。
  - 大于0表示写了部分或全部数据，小于0表示出错。
  - sockfd客户端的sock描述字。
  - addr：服务器的地址。
  - addrlen：socket地址长度。
  - 创建socket -> int socket(int domain, int type, int protocol);
  - 连接指定计算机 -> int connect(int sockfd, struct sockaddr* addr, socklen_t addrlen);
  - 向socket写入信息 -> ssize_t write(int fd, const void *buf, size_t count);
  - 关闭oscket -> int close(int fd);

## 18.TCP 协议如何保证可靠传输？

**第一种回答**

- **确认和重传：**接收方收到报文就会确认，发送方发送一段时间后没有收到确认就会重传。
- **数据校验：**TCP报文头有校验和，用于校验报文是否损坏。
- **数据合理分片和排序**：tcp会按最大传输单元(MTU)合理分片，接收方会缓存未按序到达的数据，重新排序后交给应用层。而UDP：IP数据报大于1500字节，大于MTU。这个时候发送方的IP层就需要分片，把数据报分成若干片，是的每一片都小于MTU。而接收方IP层则需要进行数据报的重组。由于UDP的特性，某一片数据丢失时，接收方便无法重组数据报，导致丢弃整个UDP数据报。
- **流量控制：**当接收方来不及处理发送方的数据，能通过滑动窗口，提示发送方降低发送的速率，防止包丢失。
- **拥塞控制：**当网络拥塞时，通过拥塞窗口，减少数据的发送，防止包丢失。

**第二种回答**

- **建立连接**（标志位）：通信前确认通信实体存在。
- **序号机制**（序号、确认号）：确保了数据是按序、完整到达。
- **数据校验**（校验和）：CRC校验全部数据。
- **超时重传**（定时器）：保证因链路故障未能到达数据能够被多次重发。
- **窗口机制**（窗口）：提供流量控制，避免过量发送。
- **拥塞控制**：同上。

**第三种回答**

首部校验这个校验机制能够确保数据传输不会出错吗？答案是不能。

**原因**

TCP协议中规定，TCP的首部字段中有一个字段是校验和，发送方将伪首部、TCP首部、TCP数据使用累加和校验的方式计算出一个数字，然后存放在首部的校验和字段里，接收者收到TCP包后重复这个过程，然后将计算出的校验和和接收到的首部中的校验和比较，如果不一致则说明数据在传输过程中出错。

这就是TCP的数据校验机制。但是这个机制能够保证检查出一切错误吗？**显然不能。**

因为这种校验方式是累加和，也就是将一系列的数字（TCP协议规定的是数据中的每16个比特位数据作为一个数字）求和后取末位。但是小学生都知道A+B=B+A，假如在传输的过程中有前后两个16比特位的数据前后颠倒了（至于为什么这么巧合？我不知道，也许路由器有bug？也许是宇宙中的高能粒子击中了电缆？反正这个事情的概率不为零，就有可能会发生），那么校验和的计算结果和颠倒之前是一样的，那么接收端肯定无法检查出这是错误的数据。

**解决方案**

传输之前先使用MD5加密数据获得摘要，跟数据一起发送到服务端，服务端接收之后对数据也进行MD5加密，如果加密结果和摘要一致，则认为没有问题

## 19.UDP是什么

提供**无连接**的，尽最大努力的数据传输服务（**不保证数据传输的可靠性**）。

## 20.TCP和UDP的区别

1、TCP面向连接（如打电话要先拨号建立连接）;UDP是无连接的，即发送数据之前不需要建立连接

2、TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP尽最大努力交付，即不保证可靠交付

3、TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流;UDP是面向报文的

UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）

4、每一条TCP连接只能是点到点的;UDP支持一对一，一对多，多对一和多对多的交互通信

5、TCP首部开销20字节;UDP的首部开销小，只有8个字节

6、TCP的逻辑通信信道是全双工的可靠信道，UDP则是不可靠信道

7、UDP是面向报文的，发送方的UDP对应用层交下来的报文，不合并，不拆分，只是在其上面加上首部后就交给了下面的网络层，论应用层交给UDP多长的报文，它统统发送，一次发送一个。而对接收方，接到后直接去除首部，交给上面的应用层就完成任务了。因此，它需要应用层控制报文的大小

TCP是面向字节流的，它把上面应用层交下来的数据看成无结构的字节流会发送，可以想象成流水形式的，发送方TCP会将数据放入“蓄水池”（缓存区），等到可以发送的时候就发送，不能发送就等着TCP会根据当前网络的拥塞状态来确定每个报文段的大小。

## 21.UDP的特点有哪些（附赠TCP的特点）？

- UDP是无连接的；
- UDP使用尽最大努力交付，即不保证可靠交付，因此主机不需要维持复杂的链接状态（这里面有许多参数）；
- UDP是面向报文的；
- UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）；
- UDP支持一对一、一对多、多对一和多对多的交互通信；
- UDP的首部开销小，只有8个字节，比TCP的20个字节的首部要短。

那么，再说一次TCP的特点：

- TCP是面向连接的。（就好像打电话一样，通话前需要先拨号建立连接，通话结束后要挂机释放连接）；
- 每一条TCP连接只能有两个端点，每一条TCP连接只能是点对点的（一对一）；
- TCP提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、并且按序到达；
- TCP提供全双工通信。TCP允许通信双方的应用进程在任何时候都能发送数据。TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双方通信的数据；
- 面向字节流。TCP中的“流”（stream）指的是流入进程或从进程流出的字节序列。“面向字节流”的含义是：虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序交下来的数据仅仅看成是一连串的无结构的字节流。

## 22.TCP对应的应用层协议

FTP：定义了文件传输协议，使用21端口. Telnet：它是一种用于远程登陆的端口,23端口 SMTP：定义了简单邮件传送协议，服务器开放的是25号端口。POP3：它是和SMTP对应，POP3用于接收邮件。

## 23.UDP对应的应用层协议

DNS：用于域名解析服务，用的是53号端口 SNMP：简单网络管理协议，使用161号端口 TFTP(Trival File Transfer Protocal)：简单文件传输协议，69

## 24.数据链路层常见协议？可以说一下吗？

![img](https://pic4.zhimg.com/80/v2-44fb8c86a97d54135d49f0518e38ae5b_720w.webp)

## 25.Ping命令基于哪一层协议的原理是什么？

ping命令基于网络层的命令，是基于ICMP协议工作的。

## 26.在进行UDP编程的时候，一次发送多少bytes好?

当然,这个没有唯一答案，相对于不同的系统,不同的要求,其得到的答案是不一样的。

我这里仅对像ICQ一类的发送聊天消息的情况作分析，对于其他情况，你或许也能得到一点帮助:首先,我们知道,TCP/IP通常被认为是一个四层协议系统,包括链路层,网络层,运输层,应用层.UDP属于运输层,

下面我们由下至上一步一步来看:以太网(Ethernet)数据帧的长度必须在46-1500字节之间,这是由以太网的物理特性决定的.这个1500字节被称为链路层的MTU(最大传输单元).但这并不是指链路层的长度被限制在1500字节,其实这这个MTU指的是链路层的数据区.并不包括链路层的首部和尾部的18个字节.

所以,事实上,这个1500字节就是网络层IP数据报的长度限制。因为IP数据报的首部为20字节,所以IP数据报的数据区长度最大为1480字节.而这个1480字节就是用来放TCP传来的TCP报文段或UDP传来的UDP数据报的.又因为UDP数据报的首部8字节,所以UDP数据报的数据区最大长度为1472字节.这个1472字节就是我们可以使用的字节数。

当我们发送的UDP数据大于1472的时候会怎样呢？这也就是说IP数据报大于1500字节,大于MTU.这个时候发送方IP层就需要分片(fragmentation). 把数据报分成若干片,使每一片都小于MTU.而接收方IP层则需要进行数据报的重组. 这样就会多做许多事情,而更严重的是,由于UDP的特性,当某一片数据传送中丢失时,接收方便 无法重组数据报.将导致丢弃整个UDP数据报。

因此,在普通的局域网环境下，我建议将UDP的数据控制在1472字节以下为好.

进行Internet编程时则不同,因为Internet上的路由器可能会将MTU设为不同的值. 如果我们假定MTU为1500来发送数据的,而途经的某个网络的MTU值小于1500字节,那么系统将会使用一系列的机 制来调整MTU值,使数据报能够顺利到达目的地,这样就会做许多不必要的操作.

鉴于Internet上的标准MTU值为576字节,所以我建议在进行Internet的UDP编程时. 最好将UDP的数据长度控件在548字节(576-8-20)以内

## 27.TCP 利用滑动窗口实现流量控制的机制？

> “流量控制是为了控制发送方发送速率，保证接收方来得及接收。TCP 利用滑动窗口实现流量控制。

TCP 中采用滑动窗口来进行传输控制，滑动窗口的大小意味着**接收方还有多大的缓冲区可以用于接收数据**。发送方可以通过滑动窗口的大小来确定应该发送多少字节的数据。当滑动窗口为 0 时，发送方一般不能再发送数据报，但有两种情况除外，一种情况是可以发送紧急数据。

> “例如，允许用户终止在远端机上的运行进程。另一种情况是发送方可以发送一个 1 字节的数据报来通知接收方重新声明它希望接收的下一字节及发送方的滑动窗口大小。

## 28.可以解释一下RTO，RTT和超时重传分别是什么吗？

- 超时重传：发送端发送报文后若长时间未收到确认的报文则需要重发该报文。可能有以下几种情况：

- - 发送的数据没能到达接收端，所以对方没有响应。
  - 接收端接收到数据，但是ACK报文在返回过程中丢失。
  - 接收端拒绝或丢弃数据。

- RTO：从上一次发送数据，因为长期没有收到ACK响应，到下一次重发之间的时间。就是重传间隔。

- - 通常每次重传RTO是前一次重传间隔的两倍，计量单位通常是RTT。例：1RTT，2RTT，4RTT，8RTT......
  - 重传次数到达上限之后停止重传。

- RTT：数据从发送到接收到对方响应之间的时间间隔，即数据报在网络中一个往返用时。大小不稳定。

## 29.XSS攻击是什么？（低频）

跨站点脚本攻击，指攻击者通过篡改网页，嵌入恶意脚本程序，在用户浏览网页时，控制用户浏览器进行恶意操作的一种攻击方式。如何防范XSS攻击 1）前端，服务端，同时需要字符串输入的长度限制。2）前端，服务端，同时需要对HTML转义处理。将其中的”<”,”>”等特殊字符进行转义编码。防 XSS 的核心是必须对输入的数据做过滤处理。

## 30.CSRF攻击？你知道吗？

跨站点请求伪造，指攻击者通过跨站请求，以合法的用户的身份进行非法操作。可以这么理解CSRF攻击：攻击者盗用你的身份，以你的名义向第三方网站发送恶意请求。CRSF能做的事情包括利用你的身份发邮件，发短信，进行交易转账，甚至盗取账号信息。

**如何防范CSRF攻击？**

**安全框架**，例如Spring Security。

**token机制**。在HTTP请求中进行token验证，如果请求中没有token或者token内容不正确，则认为CSRF攻击而拒绝该请求。

**验证码**。通常情况下，验证码能够很好的遏制CSRF攻击，但是很多情况下，出于用户体验考虑，验证码只能作为一种辅助手段，而不是最主要的解决方案。

**referer识别**。在HTTP Header中有一个字段Referer，它记录了HTTP请求的来源地址。如果Referer是其他网站，就有可能是CSRF攻击，则拒绝该请求。但是，服务器并非都能取到Referer。很多用户出于隐私保护的考虑，限制了Referer的发送。在某些情况下，浏览器也不会发送Referer，例如HTTPS跳转到HTTP。

1）验证请求来源地址；

2）关键操作添加验证码；

3）在请求地址添加 token 并验证。

## 31.文件上传漏洞是如何发生的？你有经历过吗？

文件上传漏洞，指的是用户上传一个可执行的脚本文件，并通过此脚本文件获得了执行服务端命令的能力。许多第三方框架、服务，都曾经被爆出文件上传漏洞，比如很早之前的Struts2，以及富文本编辑器等等，可被攻击者上传恶意代码，有可能服务端就被人黑了。

**如何防范文件上传漏洞？**

文件上传的目录设置为不可执行。

1）判断文件类型。在判断文件类型的时候，可以结合使用MIME Type，后缀检查等方式。因为对于上传文件，不能简单地通过后缀名称来判断文件的类型，因为攻击者可以将可执行文件的后缀名称改为图片或其他后缀类型，诱导用户执行。2）对上传的文件类型进行白名单校验，只允许上传可靠类型。

3）上传的文件需要进行重新命名，使攻击者无法猜想上传文件的访问路径，将极大地增加攻击成本，同时向shell.php.rar.ara这种文件，因为重命名而无法成功实施攻击。

4）限制上传文件的大小。

5）单独设置文件服务器的域名。

## 32.拥塞控制原理听说过吗？

- 拥塞控制目的是防止数据被过多注网络中导致网络资源（路由器、交换机等）过载。因为拥塞控制涉及网络链路全局，所以属于全局控制。控制拥塞使用拥塞窗口。

- TCP拥塞控制算法：

- - 慢开始 & 拥塞避免：先试探网络拥塞程度再逐渐增大拥塞窗口。每次收到确认后拥塞窗口翻倍，直到达到阀值ssthresh，这部分是慢开始过程。达到阀值后每次以一个MSS为单位增长拥塞窗口大小，当发生拥塞（超时未收到确认），将阀值减为原先一半，继续执行线性增加，这个过程为拥塞避免。
  - 快速重传 & 快速恢复：略。
  - 最终拥塞窗口会收敛于稳定值。

## 33.如何区分流量控制和拥塞控制？

- 流量控制属于通信双方协商；拥塞控制涉及通信链路全局。
- 流量控制需要通信双方各维护一个发送窗、一个接收窗，对任意一方，接收窗大小由自身决定，发送窗大小由接收方响应的TCP报文段中窗口值确定；拥塞控制的拥塞窗口大小变化由试探性发送一定数据量数据探查网络状况后而自适应调整。
- 实际最终发送窗口 = min{流控发送窗口，拥塞窗口}。

## 34.常见的HTTP状态码有哪些？



![img](https://pic1.zhimg.com/80/v2-dafc02e04fc28aa1a5d384e60cf73e78_720w.webp)



**1xx 信息**

100 Continue ：表明到目前为止都很正常，客户端可以继续发送请求或者忽略这个响应。

**2xx 成功**

- 200 OK
- 204 No Content ：请求已经成功处理，但是返回的响应报文不包含实体的主体部分。一般在只需要从客户端往服务器发送信息，而不需要返回数据时使用。
- 206 Partial Content ：表示客户端进行了范围请求，响应报文包含由 Content-Range 指定范围的实体内容。

**3xx 重定向**

- 301 Moved Permanently ：永久性重定向
- 302 Found ：临时性重定向
- 303 See Other ：和 302 有着相同的功能，但是 303 明确要求客户端应该采用 GET 方法获取资源。
- 304 Not Modified ：如果请求报文首部包含一些条件，例如：If-Match，If-Modified-Since，If-None-Match，If-Range，If-Unmodified-Since，如果不满足条件，则服务器会返回 304 状态码。
- 307 Temporary Redirect ：临时重定向，与 302 的含义类似，但是 307 要求浏览器不会把重定向请求的 POST 方法改成 GET 方法。

**4xx 客户端错误**

- 400 Bad Request ：请求报文中存在语法错误。
- 401 Unauthorized ：该状态码表示发送的请求需要有认证信息（BASIC 认证、DIGEST 认证）。如果之前已进行过一次请求，则表示用户认证失败。
- 403 Forbidden ：请求被拒绝。
- 404 Not Found

**5xx 服务器错误**

- 500 Internal Server Error ：服务器正在执行请求时发生错误。
- 503 Service Unavailable ：服务器暂时处于超负载或正在进行停机维护，现在无法处理请求。

原文地址：https://zhuanlan.zhihu.com/p/364194368

作者：linux

# 【NO.513】深入 malloc 函数，带你真正理解内存分配！

内存是计算机中必不可少的资源，因为 CPU 只能直接读取内存中的数据，所以当 CPU 需要读取外部设备（如硬盘）的数据时，必须先把数据加载到内存中。

我们来看看可爱的内存长什么样子的吧，如图所示：

![img](https://pic4.zhimg.com/80/v2-80b864b5445fae9e7ea50b33e32df383_720w.webp)

## **1.内存申请**

通常使用高级语言（如Go、Java 或 Python 等）都不需要自己管理内存（因为有垃圾回收机制），但 C/C++ 程序员就经常要与内存打交道。

当我们使用 C/C++ 编写程序时，如果需要使用内存，就必须先调用 `malloc` 函数来申请一块内存。但是，`malloc` 真的是申请了内存吗？

我们通过下面例子来观察 `malloc` 到底是不是真的申请了内存：

```text
 1#include <stdlib.h>
 2
 3int main(int argc, char const *argv[])
 4{
 5   void *ptr;
 6
 7   ptr = malloc(1024 * 1024 * 1024); // 申请 1GB 内存
 8
 9   sleep(3600); // 睡眠3600秒, 方便调试
10
11   return 0;
12}
```

上面的程序主要通过调用 `malloc` 函数来申请了 1GB 的内存，然后睡眠 3600 秒，方便我们查看其内存使用情况。

现在，我们编译上面的程序并且运行，如下：

```text
1$ gcc malloc.c -o malloc
2$ ./malloc
```

并且我们打开一个新的终端，然后查看其内存使用情况，如图所示：

![img](https://pic1.zhimg.com/80/v2-306ed2fabc3c52ca8b99903b2fa40134_720w.webp)

图中的 `VmRSS` 表示进程使用的物理内存大小，但我们明明申请了 1GB 的内存，为什么只显示使用 404KB 的内存呢？这里就涉及到 `虚拟内存` 和 `物理内存` 的概念了。

## **2.物理内存与虚拟内存**

下面先来介绍一下 `物理内存` 与 `虚拟内存` 的概念：

- `物理内存`：也就是安装在计算机中的内存条，比如安装了 2GB 大小的内存条，那么物理内存地址的范围就是 0 ~ 2GB。
- `虚拟内存`：虚拟的内存地址。由于 CPU 只能使用物理内存地址，所以需要将虚拟内存地址转换为物理内存地址才能被 CPU 使用，这个转换过程由 `MMU（Memory Management Unit，内存管理单元）` 来完成。`虚拟内存` 大小不受 `物理内存` 大小的限制，在 32 位的操作系统中，每个进程的虚拟内存空间大小为 0 ~ 4GB。

程序中使用的内存地址都是虚拟内存地址，也就是说，我们通过 `malloc` 函数申请的内存都是虚拟内存。实际上，内核会为每个进程管理其虚拟内存空间，并且会把虚拟内存空间划分为多个区域，如图所示：

![img](https://pic4.zhimg.com/80/v2-4710dcd77e35523345965f9d3aba0dab_720w.webp)

我们来分析一下这些区域的作用：

- `代码段`：用于存放程序的可执行代码。
- `数据段`：用于存放程序的全局变量和静态变量。
- `堆空间`：用于存放由 `malloc` 申请的内存。
- `栈空间`：用于存放函数的参数和局部变量。
- `内核空间`：存放 Linux 内核代码和数据。

## **3.brk指针**

由此可知，通过 `malloc` 函数申请的内存地址是由 `堆空间` 分配的（其实还有可能从 `mmap` 区分配，这种情况暂时忽略）。在内核中，使用一个名为 `brk` 的指针来表示进程的 `堆空间` 的顶部，如图所示：

![img](https://pic3.zhimg.com/80/v2-f84c20dd688accf3608532a5904dbf76_720w.webp)

所以，通过移动 `brk` 指针就可以达到申请（向上移动）和释放（向下移动）堆空间的内存。例如申请 1024 字节时，只需要把 `brk` 向上移动 1024 字节即可，如图所示：

![img](https://pic4.zhimg.com/80/v2-9e3484ac80904eafbf0cfb4758767a4f_720w.webp)

事实上，`malloc` 函数就是通过移动 `brk` 指针来实现申请和释放内存的，Linux 提供了一个名为 `brk()` 的系统调用来移动 `brk` 指针。

## **4.内存映射**

现在我们知道，`malloc` 函数只是移动 `brk` 指针，但并没有申请物理内存。前面我们介绍虚拟内存和物理内存的时候介绍过，虚拟内存地址必须映射到物理内存地址才能被使用。如 图所示：

![img](https://pic2.zhimg.com/80/v2-51615028ceb171463172ff50c8ec0d0d_720w.webp)

如果对没有进行映射的虚拟内存地址进行读写操作，那么将会发生 `缺页异常`。Linux 内核会对 `缺页异常` 进行修复，修复过程如下：

- 获取触发 `缺页异常` 的虚拟内存地址（读写哪个虚拟内存地址导致的）。
- 查看此虚拟内存地址是否被申请（是否在 `brk` 指针内），如果不在 `brk` 指针内，将会导致 Segmention Fault 错误（也就是常见的coredump），进程将会异常退出。
- 如果虚拟内存地址在 `brk` 指针内，那么将此虚拟内存地址映射到物理内存地址上，完成 `缺页异常` 修复过程，并且返回到触发异常的地方进行运行。

从上面的过程可以看出，不对申请的虚拟内存地址进行读写操作是不会触发申请新的物理内存。所以，这就解释了为什么申请 1GB 的内存，但实际上只使用了 404 KB 的物理内存。

## **5.总结**

本文主要解释了内存申请的原理，并且了解到 `malloc` 申请的只是虚拟内存，而且物理内存的申请延迟到对虚拟内存进行读写的时候，这样做可以减轻进程对物理内存使用的压力

原文地址：https://zhuanlan.zhihu.com/p/423692007

作者：linux

# 【NO.514】面试必问的epoll技术，从内核源码出发彻底搞懂epoll

## 1.epoll概述

epoll是linux中IO多路复用的一种机制，I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。当然linux中IO多路复用不仅仅是epoll，其他多路复用机制还有select、poll，但是接下来介绍epoll的内核实现。

网上关于epoll接口的介绍非常多，这个不是我关注的重点，但是还是有必要了解。该接口非常简单，一共就三个函数，这里我摘抄了网上关于该接口的介绍：

1. int epoll_create(int size);
   创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大。这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值。需要注意的是，当创建好epoll句柄后，它就是会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。
2. int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
   epoll的事件注册函数，它不同与select()是在监听事件时告诉内核要监听什么类型的事件，而是在这里先注册要监听的事件类型。第一个参数是epoll_create()的返回值，第二个参数表示动作，用三个宏来表示：
   EPOLL_CTL_ADD：注册新的fd到epfd中；
   EPOLL_CTL_MOD：修改已经注册的fd的监听事件；
   EPOLL_CTL_DEL：从epfd中删除一个fd；
   第三个参数是需要监听的fd，第四个参数是告诉内核需要监听什么事，struct epoll_event结构如下：

```text
struct epoll_event {
 __uint32_t events;  /* Epoll events */
 epoll_data_t data;  /* User data variable */
};
```

events可以是以下几个宏的集合：

> EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
> EPOLLOUT：表示对应的文件描述符可以写；
> EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
> EPOLLERR：表示对应的文件描述符发生错误；
> EPOLLHUP：表示对应的文件描述符被挂断；
> EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。
> EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里

1. int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
   等待事件的产生，类似于select()调用。参数events用来从内核得到事件的集合，**maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size**(备注：在4.1.2内核里面，epoll_create的size没有什么用），参数timeout是超时时间（毫秒，0会立即返回，小于0时将是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时

**epoll相比select/poll的优势**：

- select/poll每次调用都要传递所要监控的所有fd给select/poll系统调用（这意味着每次调用都要将fd列表从用户态拷贝到内核态，当fd数目很多时，这会造成低效）。而每次调用epoll_wait时（作用相当于调用select/poll），不需要再传递fd列表给内核，因为已经在epoll_ctl中将需要监控的fd告诉了内核（epoll_ctl不需要每次都拷贝所有的fd，只需要进行增量式操作）。所以，在调用epoll_create之后，内核已经在内核态开始准备数据结构存放要监控的fd了。每次epoll_ctl只是对这个数据结构进行简单的维护。
- select/poll一个致命弱点就是当你拥有一个很大的socket集合，不过由于网络延时，任一时间只有部分的socket是"活跃"的，但是select/poll每次调用都会线性扫描全部的集合，导致效率呈现线性下降。但是epoll不存在这个问题，它只会对"活跃"的socket进行操作---这是因为在内核实现中epoll是根据每个fd上面的callback函数实现的。
- 当我们调用epoll_ctl往里塞入百万个fd时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的fd给我们用户。这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的fd外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。而且，通常情况下即使我们要监控百万计的fd，大多一次也只返回很少量的准备就绪fd而已，所以，epoll_wait仅需要从内核态copy少量的fd到用户态而已。那么，这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时，除了把fd放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个fd的中断到了，就把它放到准备就绪list链表里。所以，当一个fd（例如socket）上有数据到了，内核在把设备（例如网卡）上的数据copy到内核中后就来把fd（socket）插入到准备就绪list链表里了。

## 2.源码分析

epoll相关的内核代码在fs/eventpoll.c文件中，下面分别分析epoll_create、epoll_ctl和epoll_wait三个函数在内核中的实现，分析所用linux内核源码为4.1.2版本。

### 2.1 epoll_create

epoll_create用于创建一个epoll的句柄，其在内核的系统实现如下：

sys_epoll_create:

```text
SYSCALL_DEFINE1(epoll_create, int, size)
{
 if (size <= 0)
 return -EINVAL;
 return sys_epoll_create1(0);
}
```

可见，我们在调用epoll_create时，传入的size参数，仅仅是用来判断是否小于等于0，之后再也没有其他用处。
整个函数就3行代码，真正的工作还是放在sys_epoll_create1函数中。

sys_epoll_create -> sys_epoll_create1:

```text
/*
 * Open an eventpoll file descriptor.
 */
SYSCALL_DEFINE1(epoll_create1, int, flags)
{
 int error, fd;
 struct eventpoll *ep = NULL;
 struct file *file;

 /* Check the EPOLL_* constant for consistency.  */
    BUILD_BUG_ON(EPOLL_CLOEXEC != O_CLOEXEC);

 if (flags & ~EPOLL_CLOEXEC)
 return -EINVAL;
 /*
     * Create the internal data structure ("struct eventpoll").
     */
    error = ep_alloc(&ep);
 if (error < 0)
 return error;
 /*
     * Creates all the items needed to setup an eventpoll file. That is,
     * a file structure and a free file descriptor.
     */
    fd = get_unused_fd_flags(O_RDWR | (flags & O_CLOEXEC));
 if (fd < 0) {
        error = fd;
 goto out_free_ep;
    }
    file = anon_inode_getfile("[eventpoll]", &eventpoll_fops, ep,
                 O_RDWR | (flags & O_CLOEXEC));
 if (IS_ERR(file)) {
        error = PTR_ERR(file);
 goto out_free_fd;
    }
    ep->file = file;
    fd_install(fd, file);
 return fd;

out_free_fd:
    put_unused_fd(fd);
out_free_ep:
    ep_free(ep);
 return error;
}
```

sys_epoll_create1 函数流程如下：

- 首先调用ep_alloc函数申请一个eventpoll结构，并且初始化该结构的成员，这里没什么好说的，代码如下：

sys_epoll_create -> sys_epoll_create1 -> ep_alloc:

```text
static int ep_alloc(struct eventpoll **pep)
{
    int error;
 struct user_struct *user;
 struct eventpoll *ep;
    user = get_current_user();
    error = -ENOMEM;
    ep = kzalloc(sizeof(*ep), GFP_KERNEL);
 if (unlikely(!ep))
        goto free_uid; 

    spin_lock_init(&ep->lock);
    mutex_init(&ep->mtx);
    init_waitqueue_head(&ep->wq);
    init_waitqueue_head(&ep->poll_wait);
    INIT_LIST_HEAD(&ep->rdllist);
    ep->rbr = RB_ROOT;
    ep->ovflist = EP_UNACTIVE_PTR;
    ep->user = user;

    *pep = ep;

 return 0;

free_uid:
    free_uid(user);
 return error;
}
```



- 接下来调用get_unused_fd_flags函数，在本进程中申请一个未使用的fd文件描述符。

sys_epoll_create -> sys_epoll_create1 -> ep_alloc -> get_unused_fd_flags:

```text
int get_unused_fd_flags(unsigned flags)
{
 return __alloc_fd(current->files, 0, rlimit(RLIMIT_NOFILE), flags);
}
```

linux内核中，current是个宏，返回的是一个task_struct结构（我们称之为进程描述符）的变量，表示的是当前进程，进程打开的文件资源保存在进程描述符的files成员里面，所以current->files返回的当前进程打开的文件资源。rlimit(RLIMIT_NOFILE) 函数获取的是当前进程可以打开的最大文件描述符数，这个值可以设置，默认是1024。

__alloc_fd的工作是为进程在[start,end)之间(备注：这里start为0， end为进程可以打开的最大文件描述符数)分配一个可用的文件描述符,这里就不继续深入下去了，代码如下：

sys_epoll_create -> sys_epoll_create1 -> ep_alloc -> get_unused_fd_flags -> __alloc_fd:

```text
/*
 * allocate a file descriptor, mark it busy.
 */
int __alloc_fd(struct files_struct *files,
 unsigned start, unsigned end, unsigned flags)
{
 unsigned int fd;
 int error;
 struct fdtable *fdt;

    spin_lock(&files->file_lock);
repeat:
    fdt = files_fdtable(files);
    fd = start;
 if (fd < files->next_fd)
        fd = files->next_fd;

 if (fd < fdt->max_fds)
        fd = find_next_fd(fdt, fd);

 /*
     * N.B. For clone tasks sharing a files structure, this test
     * will limit the total number of files that can be opened.
     */
    error = -EMFILE;
 if (fd >= end)
 goto out;

    error = expand_files(files, fd);
 if (error < 0)
 goto out;

 /*
     * If we needed to expand the fs array we
     * might have blocked - try again.
     */
 if (error)
 goto repeat;

 if (start <= files->next_fd)
        files->next_fd = fd + 1;

    __set_open_fd(fd, fdt);
 if (flags & O_CLOEXEC)
        __set_close_on_exec(fd, fdt);
 else
        __clear_close_on_exec(fd, fdt);
    error = fd;
#if 1
 /* Sanity check */
 if (rcu_access_pointer(fdt->fd[fd]) != NULL) {
        printk(KERN_WARNING "alloc_fd: slot %d not NULL!\n", fd);
        rcu_assign_pointer(fdt->fd[fd], NULL);
    }
#endif

out:
    spin_unlock(&files->file_lock);****
 return error;
}
```

然后，epoll_create1会调用anon_inode_getfile，创建一个file结构，如下：

sys_epoll_create -> sys_epoll_create1 -> anon_inode_getfile:

```text
/**
 * anon_inode_getfile - creates a new file instance by hooking it up to an
 *                      anonymous inode, and a dentry that describe the "class"
 *                      of the file
 *
 * @name:    [in]    name of the "class" of the new file
 * @fops:    [in]    file operations for the new file
 * @priv:    [in]    private data for the new file (will be file's private_data)
 * @flags:   [in]    flags
 *
 * Creates a new file by hooking it on a single inode. This is useful for files
 * that do not need to have a full-fledged inode in order to operate correctly.
 * All the files created with anon_inode_getfile() will share a single inode,
 * hence saving memory and avoiding code duplication for the file/inode/dentry
 * setup.  Returns the newly created file* or an error pointer.
 */
struct file *anon_inode_getfile(const char *name,
 const struct file_operations *fops,
                void *priv, int flags)
{
 struct qstr this;
 struct path path;
 struct file *file;

 if (IS_ERR(anon_inode_inode))
 return ERR_PTR(-ENODEV);

 if (fops->owner && !try_module_get(fops->owner))
 return ERR_PTR(-ENOENT);

 /*
     * Link the inode to a directory entry by creating a unique name
     * using the inode sequence number.
     */
    file = ERR_PTR(-ENOMEM);
    this.name = name;
    this.len = strlen(name);
    this.hash = 0;
    path.dentry = d_alloc_pseudo(anon_inode_mnt->mnt_sb, &this);
 if (!path.dentry)
        goto err_module;
    path.mnt = mntget(anon_inode_mnt);
 /*
     * We know the anon_inode inode count is always greater than zero,
     * so ihold() is safe.
     */
    ihold(anon_inode_inode);

    d_instantiate(path.dentry, anon_inode_inode);

    file = alloc_file(&path, OPEN_FMODE(flags), fops);
 if (IS_ERR(file))
        goto err_dput;
    file->f_mapping = anon_inode_inode->i_mapping;

    file->f_flags = flags & (O_ACCMODE | O_NONBLOCK);
    file->private_data = priv;

 return file;

err_dput:
    path_put(&path);
err_module:
    module_put(fops->owner);
 return file;
}
```

anon_inode_getfile函数中首先会alloc一个file结构和一个dentry结构，然后将该file结构与一个匿名inode节点anon_inode_inode挂钩在一起，这里要注意的是，在调用anon_inode_getfile函数申请file结构时，传入了前面申请的eventpoll结构的ep变量，申请的file->private_data会指向这个ep变量，同时，在anon_inode_getfile函数返回来后，ep->file会指向该函数申请的file结构变量。

简要说一下file/dentry/inode，当进程打开一个文件时，内核就会为该进程分配一个file结构，表示打开的文件在进程的上下文，然后应用程序会通过一个int类型的文件描述符来访问这个结构，实际上内核的进程里面维护一个file结构的数组，而文件描述符就是相应的file结构在数组中的下标。

dentry结构（称之为“目录项”）记录着文件的各种属性，比如文件名、访问权限等，每个文件都只有一个dentry结构，然后一个进程可以多次打开一个文件，多个进程也可以打开同一个文件，这些情况，内核都会申请多个file结构，建立多个文件上下文。但是，对同一个文件来说，无论打开多少次，内核只会为该文件分配一个dentry。所以，file结构与dentry结构的关系是多对一的。

同时，每个文件除了有一个dentry目录项结构外，还有一个索引节点inode结构，里面记录文件在存储介质上的位置和分布等信息，每个文件在内核中只分配一个inode。 dentry与inode描述的目标是不同的，一个文件可能会有好几个文件名（比如链接文件），通过不同文件名访问同一个文件的权限也可能不同。dentry文件所代表的是逻辑意义上的文件，记录的是其逻辑上的属性，而inode结构所代表的是其物理意义上的文件，记录的是其物理上的属性。dentry与inode结构的关系是多对一的关系。

- 最后，epoll_create1调用fd_install函数，将fd与file交给关联在一起，之后，内核可以通过应用传入的fd参数访问file结构,本段代码比较简单，不继续深入下去了。

sys_epoll_create -> sys_epoll_create1 -> fd_install:

```text
/*
 * Install a file pointer in the fd array.
 *
 * The VFS is full of places where we drop the files lock between
 * setting the open_fds bitmap and installing the file in the file
 * array.  At any such point, we are vulnerable to a dup2() race
 * installing a file in the array before us.  We need to detect this and
 * fput() the struct file we are about to overwrite in this case.
 *
 * It should never happen - if we allow dup2() do it, _really_ bad things
 * will follow.
 *
 * NOTE: __fd_install() variant is really, really low-level; don't
 * use it unless you are forced to by truly lousy API shoved down
 * your throat.  'files' *MUST* be either current->files or obtained
 * by get_files_struct(current) done by whoever had given it to you,
 * or really bad things will happen.  Normally you want to use
 * fd_install() instead.
 */

void __fd_install(struct files_struct *files, unsigned int fd,
 struct file *file)
{
 struct fdtable *fdt;

    might_sleep();
    rcu_read_lock_sched();
 while (unlikely(files->resize_in_progress)) {
        rcu_read_unlock_sched();
        wait_event(files->resize_wait, !files->resize_in_progress);
        rcu_read_lock_sched();
    }
 /* coupled with smp_wmb() in expand_fdtable() */
    smp_rmb();
    fdt = rcu_dereference_sched(files->fdt);
    BUG_ON(fdt->fd[fd] != NULL);
    rcu_assign_pointer(fdt->fd[fd], file);
    rcu_read_unlock_sched();
}

void fd_install(unsigned int fd, struct file *file)
{
    __fd_install(current->files, fd, file);
}
```

总结epoll_create函数所做的事：调用epoll_create后，在内核中分配一个eventpoll结构和代表epoll文件的file结构，并且将这两个结构关联在一块，同时，返回一个也与file结构相关联的epoll文件描述符fd。当应用程序操作epoll时，需要传入一个epoll文件描述符fd，内核根据这个fd，找到epoll的file结构，然后通过file，获取之前epoll_create申请eventpoll结构变量，epoll相关的重要信息都存储在这个结构里面。接下来，所有epoll接口函数的操作，都是在eventpoll结构变量上进行的。

所以，epoll_create的作用就是为进程在内核中建立一个从epoll文件描述符到eventpoll结构变量的通道。

### 2.2 epoll_ctl

epoll_ctl接口的作用是添加/修改/删除文件的监听事件，内核代码如下：

sys_epoll_ctl:

```text
/*
 * The following function implements the controller interface for
 * the eventpoll file that enables the insertion/removal/change of
 * file descriptors inside the interest set.
 */
SYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd,
        struct epoll_event __user *, event)
{
 int error;
 int full_check = 0;
    struct fd f, tf;
    struct eventpoll *ep;
    struct epitem *epi;
    struct epoll_event epds;
    struct eventpoll *tep = NULL;

 error = -EFAULT;
 if (ep_op_has_event(op) &&
        copy_from_user(&epds, event, sizeof(struct epoll_event)))
 goto error_return;

 error = -EBADF;
    f = fdget(epfd);
 if (!f.file)
 goto error_return;

 /* Get the "struct file *" for the target file */
    tf = fdget(fd);
 if (!tf.file)
 goto error_fput;

 /* The target file descriptor must support poll */
 error = -EPERM;
 if (!tf.file->f_op->poll)
 goto error_tgt_fput;

 /* Check if EPOLLWAKEUP is allowed */
 if (ep_op_has_event(op))
        ep_take_care_of_epollwakeup(&epds);
 

 /*
     * We have to check that the file structure underneath the file descriptor
     * the user passed to us _is_ an eventpoll file. And also we do not permit
     * adding an epoll file descriptor inside itself.
     */
 error = -EINVAL;
 if (f.file == tf.file || !is_file_epoll(f.file))
 goto error_tgt_fput;

 /*
     * epoll adds to the wakeup queue at EPOLL_CTL_ADD time only,
     * so EPOLLEXCLUSIVE is not allowed for a EPOLL_CTL_MOD operation.
     * Also, we do not currently supported nested exclusive wakeups.
     */
 if (ep_op_has_event(op) && (epds.events & EPOLLEXCLUSIVE)) {
 if (op == EPOLL_CTL_MOD)
 goto error_tgt_fput;
 if (op == EPOLL_CTL_ADD && (is_file_epoll(tf.file) ||
                (epds.events & ~EPOLLEXCLUSIVE_OK_BITS)))
 goto error_tgt_fput;
    }
 

 /*
     * At this point it is safe to assume that the "private_data" contains
     * our own data structure.
     */
    ep = f.file->private_data;

 /*
     * When we insert an epoll file descriptor, inside another epoll file
     * descriptor, there is the change of creating closed loops, which are
     * better be handled here, than in more critical paths. While we are
     * checking for loops we also determine the list of files reachable
     * and hang them on the tfile_check_list, so we can check that we
     * haven't created too many possible wakeup paths.
     *
     * We do not need to take the global 'epumutex' on EPOLL_CTL_ADD when
     * the epoll file descriptor is attaching directly to a wakeup source,
     * unless the epoll file descriptor is nested. The purpose of taking the
     * 'epmutex' on add is to prevent complex toplogies such as loops and
     * deep wakeup paths from forming in parallel through multiple
     * EPOLL_CTL_ADD operations.
     */
    mutex_lock_nested(&ep->mtx, 0);
 if (op == EPOLL_CTL_ADD) {
 if (!list_empty(&f.file->f_ep_links) ||
                        is_file_epoll(tf.file)) {
            full_check = 1;
            mutex_unlock(&ep->mtx);
            mutex_lock(&epmutex);
 if (is_file_epoll(tf.file)) {
 error = -ELOOP;
 if (ep_loop_check(ep, tf.file) != 0) {
                    clear_tfile_check_list();
 goto error_tgt_fput;
                }
            } else
                list_add(&tf.file->f_tfile_llink,
                            &tfile_check_list);
            mutex_lock_nested(&ep->mtx, 0);
 if (is_file_epoll(tf.file)) {
                tep = tf.file->private_data;
                mutex_lock_nested(&tep->mtx, 1);
           }
        }
    }

 /*
     * Try to lookup the file inside our RB tree, Since we grabbed "mtx"
     * above, we can be sure to be able to use the item looked up by
     * ep_find() till we release the mutex.
     */
    epi = ep_find(ep, tf.file, fd);

 error = -EINVAL;
 switch (op) {
 case EPOLL_CTL_ADD:
 if (!epi) {
            epds.events |= POLLERR | POLLHUP;
 error = ep_insert(ep, &epds, tf.file, fd, full_check);
        } else
 error = -EEXIST;
 if (full_check)
            clear_tfile_check_list();
 break;
 case EPOLL_CTL_DEL:
 if (epi)
 error = ep_remove(ep, epi);
 else
 error = -ENOENT;
 break;
 case EPOLL_CTL_MOD:
 if (epi) {
 if (!(epi->event.events & EPOLLEXCLUSIVE)) {
                epds.events |= POLLERR | POLLHUP;
 error = ep_modify(ep, epi, &epds);
            }
        } else
 error = -ENOENT;
 break;
    }
 if (tep != NULL)
        mutex_unlock(&tep->mtx);
    mutex_unlock(&ep->mtx);

error_tgt_fput:
 if (full_check)
        mutex_unlock(&epmutex);

    fdput(tf);
error_fput:
    fdput(f);
error_return:

 return error;
}
```

根据前面对epoll_ctl接口的介绍，op是对epoll操作的动作（添加/修改/删除事件），ep_op_has_event(op)判断是否不是删除操作，如果op != EPOLL_CTL_DEL为true，则需要调用copy_from_user函数将用户空间传过来的event事件拷贝到内核的epds变量中。因为，只有删除操作，内核不需要使用进程传入的event事件。

接着连续调用两次fdget分别获取epoll文件和被监听文件（以下称为目标文件）的file结构变量（备注：该函数返回fd结构变量，fd结构包含file结构）。

接下来就是对参数的一些检查，出现如下情况，就可以认为传入的参数有问题，直接返回出错：

1. 目标文件不支持poll操作(!tf.file->f_op->poll)；
2. 监听的目标文件就是epoll文件本身(f.file == tf.file)；
3. 用户传入的epoll文件(epfd代表的文件）并不是一个真正的epoll的文件(!is_file_epoll(f.file));
4. 如果操作动作是修改操作，并且事件类型为EPOLLEXCLUSIVE，返回出错等等。

当然下面还有一些关于操作动作如果是添加操作的判断，这里不做解释，比较简单，自行阅读。

在ep里面，维护着一个红黑树，每次添加注册事件时，都会申请一个epitem结构的变量表示事件的监听项，然后插入ep的红黑树里面。在epoll_ctl里面，会调用ep_find函数从ep的红黑树里面查找目标文件表示的监听项，返回的监听项可能为空。

接下来switch这块区域的代码就是整个epoll_ctl函数的核心，对op进行switch出来的有添加(EPOLL_CTL_ADD)、删除(EPOLL_CTL_DEL)和修改(EPOLL_CTL_MOD)三种情况，这里我以添加为例讲解，其他两种情况类似，知道了如何添加监听事件，其他删除和修改监听事件都可以举一反三。

为目标文件添加监控事件时，首先要保证当前ep里面还没有对该目标文件进行监听，如果存在(epi不为空)，就返回-EEXIST错误。否则说明参数正常，然后先默认设置对目标文件的POLLERR和POLLHUP监听事件，然后调用ep_insert函数，将对目标文件的监听事件插入到ep维护的红黑树里面：

sys_epoll_ctl -> ep_insert:

```text
/*
 * Must be called with "mtx" held.
 */
static int ep_insert(struct eventpoll *ep, struct epoll_event *event,
 struct file *tfile, int fd, int full_check)
{
    int error, revents, pwake = 0;
    unsigned long flags;
    long user_watches;
 struct epitem *epi;
 struct ep_pqueue epq;

    user_watches = atomic_long_read(&ep->user->epoll_watches);
 if (unlikely(user_watches >= max_user_watches))
 return -ENOSPC;
 if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL)))
 return -ENOMEM;
 

 /* Item initialization follow here ... */
    INIT_LIST_HEAD(&epi->rdllink);
    INIT_LIST_HEAD(&epi->fllink);
    INIT_LIST_HEAD(&epi->pwqlist);
    epi->ep = ep;
    ep_set_ffd(&epi->ffd, tfile, fd);
    epi->event = *event;
    epi->nwait = 0;
    epi->next = EP_UNACTIVE_PTR;
 if (epi->event.events & EPOLLWAKEUP) {
        error = ep_create_wakeup_source(epi);
 if (error)
            goto error_create_wakeup_source;
    } else {
        RCU_INIT_POINTER(epi->ws, NULL);
    }

 /* Initialize the poll table using the queue callback */
    epq.epi = epi;
    init_poll_funcptr(&epq.pt, ep_ptable_queue_proc);

 /*
     * Attach the item to the poll hooks and get current event bits.
     * We can safely use the file* here because its usage count has
     * been increased by the caller of this function. Note that after
     * this operation completes, the poll callback can start hitting
     * the new item.
     */
    revents = ep_item_poll(epi, &epq.pt);

 /*
     * We have to check if something went wrong during the poll wait queue
     * install process. Namely an allocation for a wait queue failed due
     * high memory pressure.
     */
    error = -ENOMEM;
 if (epi->nwait < 0)
        goto error_unregister;

 /* Add the current item to the list of active epoll hook for this file */
    spin_lock(&tfile->f_lock);
    list_add_tail_rcu(&epi->fllink, &tfile->f_ep_links);
    spin_unlock(&tfile->f_lock);

 /*
     * Add the current item to the RB tree. All RB tree operations are
     * protected by "mtx", and ep_insert() is called with "mtx" held.
     */
    ep_rbtree_insert(ep, epi);

 /* now check if we've created too many backpaths */
    error = -EINVAL;
 if (full_check && reverse_path_check())
        goto error_remove_epi;

 /* We have to drop the new item inside our item list to keep track of it */
    spin_lock_irqsave(&ep->lock, flags);

 /* record NAPI ID of new item if present */
    ep_set_busy_poll_napi_id(epi);

 /* If the file is already "ready" we drop it inside the ready list */
 if ((revents & event->events) && !ep_is_linked(&epi->rdllink)) {
        list_add_tail(&epi->rdllink, &ep->rdllist);
        ep_pm_stay_awake(epi);

 /* Notify waiting tasks that events are available */
 if (waitqueue_active(&ep->wq))
            wake_up_locked(&ep->wq);
 if (waitqueue_active(&ep->poll_wait))
            pwake++;
    }

    spin_unlock_irqrestore(&ep->lock, flags);

    atomic_long_inc(&ep->user->epoll_watches);

 /* We have to call this outside the lock */
 if (pwake)
        ep_poll_safewake(&ep->poll_wait);

 return 0;

error_remove_epi:
    spin_lock(&tfile->f_lock);
    list_del_rcu(&epi->fllink);
    spin_unlock(&tfile->f_lock);
    rb_erase(&epi->rbn, &ep->rbr);

error_unregister:
    ep_unregister_pollwait(ep, epi);

 /*
     * We need to do this because an event could have been arrived on some
     * allocated wait queue. Note that we don't care about the ep->ovflist
     * list, since that is used/cleaned only inside a section bound by "mtx".
     * And ep_insert() is called with "mtx" held.
     */
    spin_lock_irqsave(&ep->lock, flags);
 if (ep_is_linked(&epi->rdllink))
        list_del_init(&epi->rdllink);
    spin_unlock_irqrestore(&ep->lock, flags);

    wakeup_source_unregister(ep_wakeup_source(epi));

error_create_wakeup_source:
    kmem_cache_free(epi_cache, epi);

 return error;
}
```

前面说过，对目标文件的监听是由一个epitem结构的监听项变量维护的，所以在ep_insert函数里面，首先调用kmem_cache_alloc函数，从slab分配器里面分配一个epitem结构监听项，然后对该结构进行初始化，这里也没有什么好说的。我们接下来看ep_item_poll这个函数调用：

sys_epoll_ctl -> ep_insert -> ep_item_poll:

```text
static inline unsigned int ep_item_poll(struct epitem *epi, poll_table *pt)
{
    pt->_key = epi->event.events;

 return epi->ffd.file->f_op->poll(epi->ffd.file, pt) & epi->event.events;
}
```

ep_item_poll函数里面，调用目标文件的poll函数，这个函数针对不同的目标文件而指向不同的函数，如果目标文件为套接字的话，这个poll就指向sock_poll，而如果目标文件为tcp套接字来说，这个poll就是tcp_poll函数。虽然poll指向的函数可能会不同，但是其作用都是一样的，就是获取目标文件当前产生的事件位，并且将监听项绑定到目标文件的poll钩子里面（最重要的是注册ep_ptable_queue_proc这个poll callback回调函数），这步操作完成后，以后目标文件产生事件就会调用ep_ptable_queue_proc回调函数。

接下来，调用list_add_tail_rcu将当前监听项添加到目标文件的f_ep_links链表里面，该链表是目标文件的epoll钩子链表，所有对该目标文件进行监听的监听项都会加入到该链表里面。

然后就是调用ep_rbtree_insert，将epi监听项添加到ep维护的红黑树里面,这里不做解释，代码如下：

sys_epoll_ctl -> ep_insert -> ep_rbtree_insert:

```text
static void ep_rbtree_insert(struct eventpoll *ep, struct epitem *epi)
{
    int kcmp;
 struct rb_node **p = &ep->rbr.rb_node, *parent = NULL;
 struct epitem *epic;

 while (*p) {
        parent = *p;
        epic = rb_entry(parent, struct epitem, rbn);
        kcmp = ep_cmp_ffd(&epi->ffd, &epic->ffd);
 if (kcmp > 0)
            p = &parent->rb_right;
 else
            p = &parent->rb_left;
    }
    rb_link_node(&epi->rbn, parent, p);
    rb_insert_color(&epi->rbn, &ep->rbr);
}
```

前面提到，ep_insert有调用ep_item_poll去获取目标文件产生的事件位，在调用epoll_ctl前这段时间，可能会产生相关进程需要监听的事件，如果有监听的事件产生，(revents & event->events 为 true)，并且目标文件相关的监听项没有链接到ep的准备链表rdlist里面的话，就将该监听项添加到ep的rdlist准备链表里面，rdlist链接的是该epoll描述符监听的所有已经就绪的目标文件的监听项。并且，如果有任务在等待产生事件时，就调用wake_up_locked函数唤醒所有正在等待的任务，处理相应的事件。当进程调用epoll_wait时，该进程就出现在ep的wq等待队列里面。接下来讲解epoll_wait函数。

总结epoll_ctl函数：该函数根据监听的事件，为目标文件申请一个监听项，并将该监听项挂人到eventpoll结构的红黑树里面。

### 2.3 epoll_wait

epoll_wait等待事件的产生，内核代码如下：

sys_epoll_wait:

```text
/*
 * Implement the event wait interface for the eventpoll file. It is the kernel
 * part of the user space epoll_wait(2).
 */
SYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events,
 int, maxevents, int, timeout)
{
 int error;
 struct fd f;
 struct eventpoll *ep;

 /* The maximum number of event must be greater than zero */
 if (maxevents <= 0 || maxevents > EP_MAX_EVENTS)
 return -EINVAL;

 /* Verify that the area passed by the user is writeable */
 if (!access_ok(VERIFY_WRITE, events, maxevents * sizeof(struct epoll_event)))
 return -EFAULT;

 /* Get the "struct file *" for the eventpoll file */
    f = fdget(epfd);
 if (!f.file)
 return -EBADF;

 /*
     * We have to check that the file structure underneath the fd
     * the user passed to us _is_ an eventpoll file.
     */
    error = -EINVAL;
 if (!is_file_epoll(f.file))
 goto error_fput;
 

 /*
     * At this point it is safe to assume that the "private_data" contains
     * our own data structure.
     */
    ep = f.file->private_data;

 /* Time to fish for events ... */
    error = ep_poll(ep, events, maxevents, timeout);

error_fput:
    fdput(f);
 return error;
}
```

首先是对进程传进来的一些参数的检查：

- maxevents必须大于0并且小于EP_MAX_EVENTS，否则就返回-EINVAL；
- 内核必须有对events变量写文件的权限，否则返回-EFAULT；
- epfd代表的文件必须是个真正的epoll文件，否则返回-EBADF。

参数全部检查合格后，接下来就调用ep_poll函数进行真正的处理：

sys_epoll_wait -> ep_poll:

```text
/**
 * ep_poll - Retrieves ready events, and delivers them to the caller supplied
 *           event buffer.
 *
 * @ep: Pointer to the eventpoll context.
 * @events: Pointer to the userspace buffer where the ready events should be
 *          stored.
 * @maxevents: Size (in terms of number of events) of the caller event buffer.
 * @timeout: Maximum timeout for the ready events fetch operation, in
 *           milliseconds. If the @timeout is zero, the function will not block,
 *           while if the @timeout is less than zero, the function will block
 *           until at least one event has been retrieved (or an error
 *           occurred).
 *
 * Returns: Returns the number of ready events which have been fetched, or an
 *          error code, in case of error.
 */
static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,
 int maxevents, long timeout)
{
 int res = 0, eavail, timed_out = 0;
    unsigned long flags;
    u64 slack = 0;
    wait_queue_t wait;
    ktime_t expires, *to = NULL;

 if (timeout > 0) {
        struct timespec64 end_time = ep_set_mstimeout(timeout);

        slack = select_estimate_accuracy(&end_time);
        to = &expires;
        *to = timespec64_to_ktime(end_time);
    } else if (timeout == 0) {
 /*
         * Avoid the unnecessary trip to the wait queue loop, if the
         * caller specified a non blocking operation.
         */
        timed_out = 1;
        spin_lock_irqsave(&ep->lock, flags);
 goto check_events;
    }

fetch_events:

 if (!ep_events_available(ep))
        ep_busy_loop(ep, timed_out);

    spin_lock_irqsave(&ep->lock, flags); 

 if (!ep_events_available(ep)) {
 /*
         * Busy poll timed out.  Drop NAPI ID for now, we can add
         * it back in when we have moved a socket with a valid NAPI
         * ID onto the ready list.
         */
        ep_reset_busy_poll_napi_id(ep);

 /*
         * We don't have any available event to return to the caller.
         * We need to sleep here, and we will be wake up by
         * ep_poll_callback() when events will become available.
         */
        init_waitqueue_entry(&wait, current);
        __add_wait_queue_exclusive(&ep->wq, &wait); 

 for (;;) {
 /*
             * We don't want to sleep if the ep_poll_callback() sends us
             * a wakeup in between. That's why we set the task state
             * to TASK_INTERRUPTIBLE before doing the checks.
             */
            set_current_state(TASK_INTERRUPTIBLE);
 if (ep_events_available(ep) || timed_out)
 break;
 if (signal_pending(current)) {
                res = -EINTR;
 break;
            }

            spin_unlock_irqrestore(&ep->lock, flags);
 if (!schedule_hrtimeout_range(to, slack, HRTIMER_MODE_ABS))
                timed_out = 1;

            spin_lock_irqsave(&ep->lock, flags);
        }

        __remove_wait_queue(&ep->wq, &wait);
        __set_current_state(TASK_RUNNING);
    }
check_events:
 /* Is it worth to try to dig for events ? */
    eavail = ep_events_available(ep);

    spin_unlock_irqrestore(&ep->lock, flags);

 /*
     * Try to transfer events to user space. In case we get 0 events and
     * there's still timeout left over, we go trying again in search of
     * more luck.
     */
 if (!res && eavail &&
        !(res = ep_send_events(ep, events, maxevents)) && !timed_out)
 goto fetch_events;

 return res;
}
```

ep_poll中首先是对等待时间的处理，timeout超时时间以ms为单位，timeout大于0，说明等待timeout时间后超时，如果timeout等于0，函数不阻塞，直接返回，小于0的情况，是永久阻塞，直到有事件产生才返回。

当没有事件产生时（(!ep_events_available(ep))为true）,调用__add_wait_queue_exclusive函数将当前进程加入到ep->wq等待队列里面，然后在一个无限for循环里面，首先调用set_current_state(TASK_INTERRUPTIBLE)，将当前进程设置为可中断的睡眠状态，然后当前进程就让出cpu，进入睡眠，直到有其他进程调用wake_up或者有中断信号进来唤醒本进程，它才会去执行接下来的代码。

如果进程被唤醒后，首先检查是否有事件产生，或者是否出现超时还是被其他信号唤醒的。如果出现这些情况，就跳出循环，将当前进程从ep->wp的等待队列里面移除，并且将当前进程设置为TASK_RUNNING就绪状态。

如果真的有事件产生，就调用ep_send_events函数，将events事件转移到用户空间里面。

sys_epoll_wait -> ep_poll -> ep_send_events:

```text
static int ep_send_events(struct eventpoll *ep,
 struct epoll_event __user *events, int maxevents)
{
 struct ep_send_events_data esed; 

    esed.maxevents = maxevents;
    esed.events = events;

 return ep_scan_ready_list(ep, ep_send_events_proc, &esed, 0, false);
}
```

ep_send_events没有什么工作，真正的工作是在ep_scan_ready_list函数里面：

sys_epoll_wait -> ep_poll -> ep_send_events -> ep_scan_ready_list:

```text
/**
 * ep_scan_ready_list - Scans the ready list in a way that makes possible for
 *                      the scan code, to call f_op->poll(). Also allows for
 *                      O(NumReady) performance.
 *
 * @ep: Pointer to the epoll private data structure.
 * @sproc: Pointer to the scan callback.
 * @priv: Private opaque data passed to the @sproc callback.
 * @depth: The current depth of recursive f_op->poll calls.
 * @ep_locked: caller already holds ep->mtx
 *
 * Returns: The same integer error code returned by the @sproc callback.
 */
static int ep_scan_ready_list(struct eventpoll *ep,
 int (*sproc)(struct eventpoll *,
                       struct list_head *, void *),
 void *priv, int depth, bool ep_locked)
{
 int error, pwake = 0;
    unsigned long flags;
    struct epitem *epi, *nepi;
    LIST_HEAD(txlist);

 /*
    * We need to lock this because we could be hit by
     * eventpoll_release_file() and epoll_ctl().
     */

 if (!ep_locked)
        mutex_lock_nested(&ep->mtx, depth);

 /*
     * Steal the ready list, and re-init the original one to the
     * empty list. Also, set ep->ovflist to NULL so that events
     * happening while looping w/out locks, are not lost. We cannot
     * have the poll callback to queue directly on ep->rdllist,
     * because we want the "sproc" callback to be able to do it
     * in a lockless way.
     */
    spin_lock_irqsave(&ep->lock, flags);
    list_splice_init(&ep->rdllist, &txlist);
    ep->ovflist = NULL;
    spin_unlock_irqrestore(&ep->lock, flags);

 /*
     * Now call the callback function.
     */
 error = (*sproc)(ep, &txlist, priv);

    spin_lock_irqsave(&ep->lock, flags);
 /*
     * During the time we spent inside the "sproc" callback, some
     * other events might have been queued by the poll callback.
     * We re-insert them inside the main ready-list here.
     */
 for (nepi = ep->ovflist; (epi = nepi) != NULL;
         nepi = epi->next, epi->next = EP_UNACTIVE_PTR) {
 /*
         * We need to check if the item is already in the list.
         * During the "sproc" callback execution time, items are
         * queued into ->ovflist but the "txlist" might already
         * contain them, and the list_splice() below takes care of them.
         */
 if (!ep_is_linked(&epi->rdllink)) {
            list_add_tail(&epi->rdllink, &ep->rdllist);
            ep_pm_stay_awake(epi);
        }
    }
 /*
     * We need to set back ep->ovflist to EP_UNACTIVE_PTR, so that after
     * releasing the lock, events will be queued in the normal way inside
     * ep->rdllist.
     */
    ep->ovflist = EP_UNACTIVE_PTR;

 /*
     * Quickly re-inject items left on "txlist".
     */
    list_splice(&txlist, &ep->rdllist);
    __pm_relax(ep->ws);

 if (!list_empty(&ep->rdllist)) {
 /*
         * Wake up (if active) both the eventpoll wait list and
         * the ->poll() wait list (delayed after we release the lock).
         */
 if (waitqueue_active(&ep->wq))
            wake_up_locked(&ep->wq);
 if (waitqueue_active(&ep->poll_wait))
            pwake++;
    }
    spin_unlock_irqrestore(&ep->lock, flags);

 if (!ep_locked)
        mutex_unlock(&ep->mtx); 

 /* We have to call this outside the lock */
 if (pwake)
        ep_poll_safewake(&ep->poll_wait);

 return error;
}
```

ep_scan_ready_list首先将ep就绪链表里面的数据链接到一个全局的txlist里面，然后清空ep的就绪链表，同时还将ep的ovflist链表设置为NULL，ovflist是用单链表，是一个接受就绪事件的备份链表，当内核进程将事件从内核拷贝到用户空间时，这段时间目标文件可能会产生新的事件，这个时候，就需要将新的时间链入到ovlist里面。

仅接着，调用sproc回调函数(这里将调用ep_send_events_proc函数)将事件数据从内核拷贝到用户空间。

sys_epoll_wait -> ep_poll -> ep_send_events -> ep_scan_ready_list -> ep_send_events_proc:

```text
static int ep_send_events_proc(struct eventpoll *ep, struct list_head *head,
                   void *priv)
{
 struct ep_send_events_data *esed = priv;
    int eventcnt;
    unsigned int revents;
 struct epitem *epi;
 struct epoll_event __user *uevent;
 struct wakeup_source *ws;
    poll_table pt;

    init_poll_funcptr(&pt, NULL);

 /*
     * We can loop without lock because we are passed a task private list.
     * Items cannot vanish during the loop because ep_scan_ready_list() is
     * holding "mtx" during this call.
     */
 for (eventcnt = 0, uevent = esed->events;
         !list_empty(head) && eventcnt < esed->maxevents;) {
        epi = list_first_entry(head, struct epitem, rdllink);

 /*
         * Activate ep->ws before deactivating epi->ws to prevent
         * triggering auto-suspend here (in case we reactive epi->ws
         * below).
         *
         * This could be rearranged to delay the deactivation of epi->ws
         * instead, but then epi->ws would temporarily be out of sync
         * with ep_is_linked().
         */
        ws = ep_wakeup_source(epi);
 if (ws) {
 if (ws->active)
                __pm_stay_awake(ep->ws);
            __pm_relax(ws);
        }
        list_del_init(&epi->rdllink);

        revents = ep_item_poll(epi, &pt);

 /*
         * If the event mask intersect the caller-requested one,
         * deliver the event to userspace. Again, ep_scan_ready_list()
         * is holding "mtx", so no operations coming from userspace
         * can change the item.
         */
 if (revents) {
 if (__put_user(revents, &uevent->events) ||
                __put_user(epi->event.data, &uevent->data)) {
               list_add(&epi->rdllink, head);
                ep_pm_stay_awake(epi);
 return eventcnt ? eventcnt : -EFAULT;
            }
            eventcnt++;
            uevent++;
 if (epi->event.events & EPOLLONESHOT)
                epi->event.events &= EP_PRIVATE_BITS;
 else if (!(epi->event.events & EPOLLET)) {
 /*
                 * If this file has been added with Level
                 * Trigger mode, we need to insert back inside
                 * the ready list, so that the next call to
                 * epoll_wait() will check again the events
                 * availability. At this point, no one can insert
                 * into ep->rdllist besides us. The epoll_ctl()
                 * callers are locked out by
                 * ep_scan_ready_list() holding "mtx" and the
                 * poll callback will queue them in ep->ovflist.
                 */
                list_add_tail(&epi->rdllink, &ep->rdllist);
                ep_pm_stay_awake(epi);
           }
        }
    }
 return eventcnt;
}
```

ep_send_events_proc回调函数循环获取监听项的事件数据，对每个监听项，调用ep_item_poll获取监听到的目标文件的事件，如果获取到事件，就调用__put_user函数将数据拷贝到用户空间。

回到ep_scan_ready_list函数，上面说到，在sproc回调函数执行期间，目标文件可能会产生新的事件链入ovlist链表里面，所以，在回调结束后，需要重新将ovlist链表里面的事件添加到rdllist就绪事件链表里面。

同时在最后，如果rdlist不为空（表示是否有就绪事件），并且由进程等待该事件，就调用wake_up_locked再一次唤醒内核进程处理事件的到达（流程跟前面一样，也就是将事件拷贝到用户空间）。

到这，epoll_wait的流程是结束了，但是有一个问题，就是前面提到的进程调用epoll_wait后会睡眠，但是这个进程什么时候被唤醒呢？在调用epoll_ctl为目标文件注册监听项时，对目标文件的监听项注册一个ep_ptable_queue_proc回调函数，ep_ptable_queue_proc回调函数将进程添加到目标文件的wakeup链表里面，并且注册ep_poll_callbak回调，当目标文件产生事件时，ep_poll_callbak回调就去唤醒等待队列里面的进程。

总结一下epoll该函数： epoll_wait函数会使调用它的进程进入睡眠（timeout为0时除外），如果有监听的事件产生，该进程就被唤醒，同时将事件从内核里面拷贝到用户空间返回给该进程。

原文地址：https://zhuanlan.zhihu.com/p/451235847

作者：linux

# 【NO.515】从进入内核态看内存管理

知乎上一个比较有意思的话题：如何理解「进入内核态」，要回答好这个问题需要对内存管理及程序的运行机制有比较深刻的了解，比如你需要了解内存的分段，分页，中断，特权级等机制，信息量比较大，本文将会从 Intel CPU 的发展历史讲起，循序渐近地帮助大家彻底掌握这一概念，相信大家看了肯定有帮助，本文目录如下

- CPU 运行机制

- Intel CPU 历史发展史

- - 分段
  - 保护模式

- 特权级

- - 系统调用
  - 中断

- 分段内存的优缺点

- 内存分页

- 总结

## 1.**CPU 运行机制**

我们先简单地回顾一下 CPU 的工作机制，重新温习一下一些基本概念，因为我在查阅资料的过程发现一些网友对寻址，CPU 是几位的概念理解得有些模糊，理解了这些概念再去看 CPU 的发展史就不会再困惑

CPU 是如何工作的呢？它是根据一条条的机器指令来执行的，而机器指令= 操作码+操作数，操作数主要有三类：寄存器地址、内存地址或立即数（即常量）。

我们所熟悉的程序就是一堆指令和数据的集合，当打开程序时，装载器把程序中的指令和数据加载到内存中，然后 CPU 到内存中一条条地取指令，然后再译码，执行。

在内存中是以字节为基本单位来读写数据的，我们可以把内存看作是一个个的小格子（一般我们称其为内存单元），而每个小格子是一个字节，那么对于 B8 0123H 这条指令来说，它在内存中占三字节，如下，CPU 该怎么找到这些格子呢，我们需要给这些格子编号，这些编号也就是我们说的**内存地址**，根据内存地址就是可以定位指令所在位置，从而取出里面的数据

![img](https://pic3.zhimg.com/80/v2-f6f463a43d44875665f6929ac0f63e66_720w.webp)

**如图示**：内存被分成了一个个的格子，每个格子一个字节，20000~20002 分别为对应格子的编号（即内存地址）

CPU 执行指令主要分为以下几个步骤

1. **取指令**，CPU 怎么知道要去取哪条指令呢，它里面有一个 IP 寄存器指向了对应要取的指令的内存地址， 然后这个内存地址会通过**地址总线**找到对应的格子，我们把这个过程称为**寻址**，不难发现**寻址能力决定于地址总线的位宽**，假设地址总线位数为 20 位，那么内存的可寻址空间为 2^20 * 1Byte = 1M，将格子（内存单元）里面的数据（指令）取出来后，再通过**数据总线**发往 CPU 中的指令缓存区（指令寄存器），那么一次能传多少数据呢，**取决于数据总线的位宽**，如果数据总线为 16 位，那么一次可以传 16 bit 也就是两个字节。
2. **译码**：指令缓冲区中的指令经过译码以确定该进行什么操作
3. **执行**：译码后会由控制单元向运算器发送控制指令进行操作（比如执行加减乘除等），执行是由运算器操纵数据也就是操作数进行计算，而操作数保存在存储单元（即片内的缓存和寄存器组）中，由于操作数有可能是内存地址，所以执行中可能需要到内存中获取数据（这个过程称为**访存**），执行后的结果保存在寄存器或**写回**内存中

![img](https://pic1.zhimg.com/80/v2-948bc3d6e1cd7dd11f31c155fe8c409c_720w.webp)

以指令 mov ax, 0123H 为例，它表示将数据 0123H 存到寄存器 AX 中，在此例中 AX 为 16 位寄存器，一次可以操作 16 位也就是 2 Byte 的数据，所以我们将其称为 16 位 CPU，**CPU 是多少位取决于它一次执行指令的数据带宽，而数据带宽又取决于通用寄存 器的位宽**

1. **更新 IP**：执行完一条指令后,更新 IP 中的值，将其指向下一条指令的起始地址，然后重复步骤 1

由以上总结可知**寻址能力与寄存器位数有关**

接下来我们以执行四条指令为例再来仔细看下 CPU 是如何执行指令的，动图如下：

![动图封面](https://pic1.zhimg.com/v2-0658cbabdab9b04787ebebd256fed160_b.jpg)



看到上面这个动图，细心地你可能会发现两个问题

1. 前文说指令地址是根据 IP 来获取的吗，但上图显示指令地址却是由「CS 左移四位 + IP」计算而来的，与我们所阐述的指令保存在 IP 寄存器中似乎有些出入，这是怎么回事呢？
2. 动图显示的地址是真实物理地址，这样进程之间可以互相访问/改写对方的物理地址，显然是不安全的，那如何才能做到安全访问或者说进程间内存的隔离呢

以上两点其实只要我们了解一下 CPU 的发展历史就明白解决方案了，有了以上的铺垫，在明白了**寻址**，**16/32/64 位 CPU** 等术语的含义后，再去了解 CPU 的发展故事会更容易得多，话不多说，发车

## 2.**Intel CPU 历史发展史**

1971 年世界上第一块 4 位 CPU-4004 微处理器横空出世，1974 年 Intel 研发成功了 8 位 CPU-8080，这两款 CPU 都是使用的绝对物理地址来寻址的，指令地址只存在于 IP 寄存器中（即只使用 IP 寄存器即可确定内存地址）。由于是使用绝对物理地址寻址，也就意味着进程之间的内存数据可能会互相覆盖，很不安全，所以这两者只支持单进程

### **2.1 分段**

1978 年英特尔又研究成功了第一款 16 位 CPU - 8086，这款 CPU 可以说是 x86 系列的鼻祖了，设计了 16 位的寄存器和 20 位的地址总线，所以内存地址可以达到 2^20 Byte 即 1M，极大地扩展了地址空间，但是问题来了，由于寄存器只有 16 位，那么 16 位的 IP 寄存器如何能寻址 20 位的地址呢，首先 Intel 工程师设计了一种分段的方法：1M 内存可以分为 16 个大小为 64 K 的段，那么内存地址就可以由「段的起始地址（也叫**段基址**） + **段内偏移**（IP 寄存器中的值）」组成，对于进程说只需要关心 4 个段 ，`代码段`，`数据段`，`堆栈段`，`附加段`，这几个段的段基址分别保存在 CS，DS，SS，ES 这四个寄存器中

![img](https://pic2.zhimg.com/80/v2-a767e55a3c58d445f7a1baa4a2134f41_720w.webp)

这四个寄存器也是 16 位，那怎么访问 20 位的内存地址呢，实现也很简单，将每个寄存器的值左移四位，然后再加上段内偏移即为寻址地址，CPU 都是取代码段 中的指令来执行的，我们以代码段内的寻址为例来计算内存地址，指令的地址 = CS << 4 + IP ，这种方式做到了 20 位的寻址，只要改变 CS，IP 的值，即可实现在 0 到最大地址 0xFFFFF 全部 20 位地址的寻址

举个例子：假设 CS 存的数据为 0x2000,IP 为 0x0003,那么对应的指令地址为

![img](https://pic2.zhimg.com/80/v2-80216f1528b4e7452a265f0c0e05d559_720w.webp)

图示为真实的物理地址计算方式，从中可知， CS 其实保存的是真实物理地址的高 16 位

分段的初衷是为了解决寻址问题，但本质上段寄存器中保存的还是真实物理地址的段基础，且可以随意指定，所以它也无法支持多进程，因为这意味着进程可以随意修改 CS：IP 将其指向任意地址，很可能会覆盖正在运行的其他进程的内存，造成灾难性后果。

我们把这种使用真实物理地址且未加任何限制的寻址方式称为**实模式**（real mode，即实际地址模式）

### 2.2 **保护模式**

实模式上的物理地址由段寄存器中的段基址:IP 计算而来，而段基址可由用户随意指定，显然非常不安全，于是 Intel 在之后推出了 80286 中启用了保护模式，这个保护是怎么做的呢

首先段寄存器保存的不再是段基址了，而是段选择子（Selector），其结构如下

![img](https://pic4.zhimg.com/80/v2-87607804ad7b2fe0088027d166d2622b_720w.webp)

其中第 3 到 15 位保存的是描述符索引，此索引会根据 TI 的值是 0 还是 1 来选择是到 GDT（全局描述符表，一般也称为段表）还是 LDT 来找段描述符，段描述符保存的是段基址和段长度，找到段基址后再加上保存在 IP 寄存器中的段偏移量即为物理地址，段描述符的长度统一为 8 个字节，而 GDT/LDT 表的基地址保存在 gdtr/ldtr 寄存器中，以 GDT （此时 TI 值为 0）为例来看看此时 CPU 是如何寻址的

![img](https://pic2.zhimg.com/80/v2-3a0617cd956e4fd53518360093b76f15_720w.webp)

可以看到程序中的地址是由段选择子：段内偏移量组成的，也叫**逻辑地址**，在只有分段内存管理的情况下它也被称为**虚拟内存**

GDT 及段描述符的分配都是由操作系统管理的，进程也无法更新 CS 等寄存器中值，这样就避免了直接操作其他进程以及自身的物理地址，达到了保护内存的效果，从而为多进程运行提供了可能，我们把这种寻址方式称为**保护模式**

那么保护模式是如何实现的呢，细心的你可能发现了上图中在段选择子和段描述符中里出现了 **RPL** 和 **DPL** 这两个新名词，这两个表示啥意思呢？这就涉及到一个概念：**特权级**

## 3.**特权级**

我们知道 CPU 是根据机器指令来执行的，但这些指令有些是非常危险的，比如**清内存**，**置时钟**，**分配系统资源**等，这些指令显然不能让普通的进程随意执行，应该始终控制在操作系统中执行，所以要把操作系统和普通的用户进程区分开来

我们把一个进程的虚拟地址划分为两个空间，**用户空间**和**内核空间**，用户空间即普通进程所处空间，内核空间即操作系统所处空间

![img](https://pic1.zhimg.com/80/v2-e15e7f3c1dea16083dba2b3e9f9d1418_720w.webp)

当 CPU 运行于用户空间（执行用户空间的指令）时，它处于用户态，只能执行普通的 CPU 指令 ，当 CPU 运行于内核空间（执行内核空间的指令）时，它处于**内核态**，可以执行清内存，置时钟，读写文件等特权指令，那怎么区分 CPU 是在用户态还是内核态呢，CPU 定义了四个特权等级，如下，从 0 到 3，特权等级依次递减，当特权级为 0 时，CPU 处于内核态，可以执行任何指令，当特权级为 3 时，CPU 处于用户态，**在 Linux 中只用了 Ring 0，Ring 3 两个特权等级**

![img](https://pic1.zhimg.com/80/v2-4ddae844372b21a4fa5d5cd2882c4d84_720w.webp)

那么问题来了，怎么知道 CPU 处于哪一个特权等级呢，还记得上文中我们提到的段选择子吗

![img](https://pic4.zhimg.com/80/v2-380456be98301eb98224c3259abe9503_720w.webp)

其中的 RPL 表示请求特权（(Requested privilege level)）我们把当前保存于 CS 段寄存器的段选择子中的 RPL 称为 CPL（current priviledge level），即当前特权等级，可以看到 RPL 有两位，刚好对应着 0,1,2,3 四个特权级，而上文提到的 DPL 表示段描述符中的特权等级（Descriptor privilege level）知道了这两个概念也就知道保护模式的实现原理了，CPU 会在两个关键点上对内存进行保护

1. 目标段选择子被加载时
2. 当通过线性地址（在只有段式内存情况下，线性地址为物理地址）访问一个内存页时。由此可见，保护也反映在内存地址转换的过程之中，既包括分段又包括分页（后文分提到分页）

CPU 是怎么保护内存的呢，它会对 CPL，RPL，DPL 进行如下检查

![img](https://pic3.zhimg.com/80/v2-ab6dff39e6d8d0bfbc8cebcac8fa3402_720w.webp)

只有 CPL <= DPL 且 RPL <= DPL 时，才会加载目标代码段执行，否则会报一般保护异常 （General-protection exception）

那么特权等级（也就是 CPL）是怎么变化的呢，我们之前说了 CPU 运行于用户空间时，处于用户态，特权等级为 3，运行于内核空间时，处于内核态，特权等级为 0，所以也可以换个问法 CPU 是如何从用户空间切换到内核空间或者从内核空间切换到用户空间的，这就涉及到一个概念：**系统调用**

### 3.1 **系统调用**

我们知道用户进程虽然不能执行特权指令，但有时候也需要执行一些读写文件，发送网络包等操作，而这些操作又只能让操作系统来执行，那该怎么办呢，可以让操作系统提供接口，让用户进程来调用即可，我们把这种方式叫做**系统调用**，系统调用可以直接由应用程序调用，或者通过调用一些公用函数库或 shell（这些函数库或 shell 都封装了系统调用接口）等也可以达到间接调用系统调用的目的。通过系统调用，应用程序实现了**陷入（trap）内核态**的目的，这样就从用户态切换到了内核态中，如下

![img](https://pic3.zhimg.com/80/v2-4d9db473e02d4fffbe4fe78367cb0a06_720w.webp)

应用程序通过系统调用陷入内核态

那么系统调用又是怎么实现的呢，主要是靠**中断**实现的，接下来我们就来了解一下什么是中断

### 3.2 **中断**

陷入内核态的系统调用主要是通过一种 **trap gate**（陷阱门）来实现的，它其实是软件中断的一种，由 CPU 主动触发给自己一个中断向量号，然后 CPU 根据此中断向量号就可以去中断向量表找到对应的**门描述符**，门描述符与 GDT 中的段描述符相似，也是 8 个字节，门描述符中包含段选择子，段内偏移，DPL 等字段 ，然后再根据段选择子去 GDT（或者 LDT，下图以 GDT 为例） 中查找对应的段描述符，再找到段基地址，然后根据中断描述符表的段内偏移即可找到中断处理例程的入口点,整个中断处理流程如下

![img](https://pic4.zhimg.com/80/v2-366e353b58c263cfe1d74c7d5de8cc27_720w.webp)

**画外音**：上图中门描述符和段描述符只画出了关键的几个字段，省略了其它次要字段

当然了，不是随便发一个中断向量都能被执行，只有满足一定条件的中断才允许被普通的应用程序调用，从发出软件中断再到执行中断对应的代码段会做如下的检查

![img](https://pic4.zhimg.com/80/v2-96ee238ba675b970d27860fb2782728f_720w.webp)

一般应用程序发出软件中断对应的向量号是大家熟悉的 int 0x80（int 代表 interrupt），它的门描述符中的 DPL 为 3,所以能被所有的用户程序调用，而它对应的目标代码段描述符中的 DPL 为 0，所以当通过中断门检查后（即 CPL <= 门描述符中的 DPL 成立），CPU 就会将 CS 寄存器中的 RPL（3） 替换为目标代码段描述符的 DPL（0），替换后的 CPL 也就变成了 0，通过这种方式完成了从用户态到内核态的替换，当中断代码执行后执行 iret 指令又会切换回用户态

另外当执行中断程序时，还需要首先把当前用户进程中对应的堆栈，返回地址等信息，以便切回到用户态时能恢复现场

可以看到 int 80h 这种软件中断的执行又是检查特权级，又是从用户态切换到内核态，又是保存寄存器的值，可谓是非常的耗时，光看一下以下图示就知道像 int 0x80 这样的软件中断开销是有多大了

![img](https://pic1.zhimg.com/80/v2-1f02dd51825cd75b4bc9394f042a26e0_720w.webp)

系统调用

所以后来又开发出了 SYSENTER`/`SYSCALL 这样快速系统调用的指令，它们取消了权限检查，也不需要在中断描述表（Interrupt Descriptor Table、IDT）中查找系统调用对应的执行过程，也不需要保存堆栈和返回地址等信息，而是直接进入*CPL 0*，并将新值加载到与代码和堆栈有关的寄存器当中（cs，eip，ss 和 esp），所以极大地提升了性能

## 4.**分段内存的优缺点**

使用了保护模式后，程序员就可以在代码中使用了**段选择子：段偏移量**的方式来寻址，这不仅让多进程运行成为了可能，而且也解放了程序员的生产力，我们完全可以认为程序拥有所有的内存空间（虚拟空间），因为段选择子是由操作系统分配的，只要操作系统保证不同进程的段的虚拟空间映射到不同的物理空间上，不要重叠即可，也就是说虽然各个程序的虚拟空间是一样的，但由于它们映射的物理地址是不同且不重叠的，所以是能正常工作的，但是为了方便映射，一般要求在物理空间中分配的段是连续的（这样只要维护映射关系的起始地址和对应的空间大小即可）

![img](https://pic1.zhimg.com/80/v2-f85af2b41f4dbaba65d1340cde849df8_720w.webp)

段式内存管理-虚拟空间与实际物理内存的映射

但段式内存管理缺点也很明显：内存碎片可能很大，举个例子

![img](https://pic1.zhimg.com/80/v2-13efae555312c8f873143e9d89b59670_720w.webp)

如上图示，连续加载了三个程序到内存中，如果把 Chrome 关闭了，此时内存中有两段 128 M的空闲内存，但如果此时要加载一个 192 M 的程序 X 却有心无力了 ，因为段式内存需要划分出一块**连续的**内存空间，此时你可以选择把占 256 M 的 Python 程序先 swap 到磁盘中，然后紧跟着 512 M 内存的后面划分出 256 M 内存，再给 Python 程序 swap 到这块物理内存中，这样就腾出了连续的 256 M 内存，从而可以加载程序 X 了，但这种频繁地将几十上百兆内存与硬盘进行 swap 显然会对性能造成严重的影响，毕竟谁都知道内存和硬盘的读写速度可是一个天上一个地上，如果一定要交换，能否每次 swap 得能少一点，比如只有几 K，这样就能满足我们的需求，分页内存管理就诞生了

## 5.**内存分页**

1985 年 intel 推出了 32 位处理器 80386，也是首款支持分页内存的 CPU

和分段这样连续分配一整段的空间给程序相比，分页是把整个物理空间切成一段段固定尺寸的大小，当然为了映射，虚拟地址也需要切成一段段固定尺寸的大小，这种固定尺寸的大小我们一般称其为页，在 LInux 中一般每页的大小为 4KB，这样虚拟地址和物理地址就通过页来映射起来了

![img](https://pic4.zhimg.com/80/v2-15cd86c78debe98b2bd2a769c3a4698f_720w.webp)

当然了这种映射关系是需要一个映射表来记录的，这样才能把虚拟地址映射到物理内存中，给定一个虚拟地址，它最终肯定在某个物理页内，所以虚拟地址一般由「页号+页内偏移」组成，而映射表项需要包含物理内存的页号，这样只要将页号对应起来，再加上页内偏移，即可获取最终的物理内存

![img](https://pic3.zhimg.com/80/v2-c15d7f741f112db2867918683e3f43fa_720w.webp)

于是问题来了，映射表（也称页表）该怎么设计呢,我们以 32 位虚拟地址位置来看看，假设页大小为 4K（2^12），那么至少需要 2^20 也就是 100 多万个页表项才能完全覆盖所有的虚拟地址，假设每一个页表项 4 个字节，那就意味着为一个进程的虚拟地址就需要准备 2^20 * 4 B = 4 M 的页表大小，如果有 100 个进程，就意味着光是页表就要占用 400M 的空间了，这显然是非常巨大的开销，那该怎么解决这个页表空间占用巨大的问题呢

我们注意到现在的做法是一次性为进程分配了占用其所有虚拟空间的页表项，但实际上一个进程根本用不到这么巨大的虚拟空间，所以这种分配方式无疑导致很多分配的页表项白白浪费了，那该怎么办，答案是**分级管理，等真正需要分配物理空间的时候再分配**，其实大家可以想想我们熟悉的 windows 是怎么分配的，是不是一开始只分配了 C 盘，D盘，E盘，等要存储的时候，先确定是哪个盘，再在这个盘下分配目录，然后再把文件存到这个目录下，并不会一开始就把所有盘的空间给分配完的

![img](https://pic4.zhimg.com/80/v2-87502935e04b9a9717000361aae85617_720w.webp)

同样的道理，以 32 位虚拟地址为例，我们也可以对页表进行分级管理, 页表项 2^20 = 2^10 * 2^10 = 1024 * 1024，我们把一个页表分成两级页表，第一级页表 1024 项，每一项都指向一个包含有 1024 个页表项的二级页表

![img](https://pic2.zhimg.com/80/v2-46bb9e8bb0005f2ce5ffd1c091324f5d_720w.webp)

图片来自《图解系统》

这样只有在一级页表中的页表项被分配的时候才会分配二级页表，极大的节省了空间，我们简单算下，假设 4G 的虚拟空间进程只用了 20%（已经很大了，大部分用不到这么多），那么由于一级页表空间为 1024 *4 = 4K，总的页表空间为 4K+ 0.2 * 4M = 0.804M，相比于原来的 4M 是个巨大的提升！

那么对于分页保护模式又是如何起作用的呢，同样以 32 位为例，它的二级页表项（也称 page table entry）其实是以下结构

![img](https://pic4.zhimg.com/80/v2-d66413adbe4158a5e49b595e597e442f_720w.webp)

注意第三位（也就是 2 对应的位置）有个 U/S，它其实就是代表特权级，表示的是用户/超级用户标志。为 1 时，允许所有特权级别的程序访问；为 0 时，仅允许特权级为0、1、2（Linux 中没有 1，2）的程序（也就是内核）访问。页目录中的这个位对其所映射的所有页面起作用

既然分页这么好，那么分段是不是可以去掉了呢，理论上确实可以，但 Intel 的 CPU 严格执行了 backward compatibility（回溯兼容），也就是说最新的 CPU 永远可以运行针对早期 CPU 开发的程序，否则早期的程序就得针对新 CPU 架构重新开发了（早期程序针对的是 CPU 的段式管理进行开发），这无论对用户还是开发者都是不能接受的（别忘了安腾死亡的一大原因就是由于不兼容之前版本的指令），兼容性虽然意味着每款新的 CPU 都得兼容老的指令，所背的历史包袱越来越重，但对程序来说能运行肯定比重新开发好，所以既然早期的 CPU 支持段，那么自从 80386 开始的所有 CPU 也都得支持段，而分页反而是可选的，也就意味着这些 CPU 的内存管理都是段页式管理，逻辑地址要先经过段式管理单元转成线性地址（也称虚拟地址），然后再经过页式管理单元转成物理内存，如下

![img](https://pic1.zhimg.com/80/v2-ad2df8cd742651f9220d2dacf16f40e4_720w.webp)

分页是可选项

在 Linux 中，虽然也是段页式内存管理，但它统一把 CS，DS，SS，ES 的段基址设置为了 0，段界限也设置为了整个虚拟内存的长度，所有段都分布在同一个地址空间，这种内存模式也叫平坦内存模型（flat memory model）

![img](https://pic4.zhimg.com/80/v2-e5e27bea90bc299a9c9efe283cf5bcf3_720w.webp)

平坦内存模型

我们知道逻辑地址由段选择子：段内偏移地址组成，既然段选择子指向的段基地址为 0，那也就意味着段内偏移地址即为即为线性地址（也就是虚拟地址），由此可知 Linux 中所有程序的代码都使用了虚拟地址，通过这种方式巧妙地绕开了分段管理，分段只起到了访问控制和权限的作用（别忘了各种权限检查依赖 DPL，RPL 等特权字段，特权极转移也依赖于段选择子中的 DPL 来切换的）

## 6.**总结**

看完本文相信大家对实模式，保护模式，特权级转换，分段，分页等概念应该有了比较清晰的认识。

我们简单总结一下，CPU 诞生之间，使用的绝对物理内存来寻址（也就是实模式），随后随着 8086 的诞生，由于工艺的原因，虽然地址总线是 20 位，但寄存器却只有 16 位，一个难题出现了，16 位的寄存器该怎么寻址 20 位的内存地址呢，于是段的概念被提出了，段的出现虽然解决了寻址问题，但本质上 CS << 4 + IP 的寻址方式依然还是绝对物理地址，这样的话由于地址会互相覆盖，显然无法做到多进程运行，于是保护模式被提出了，保护就是为了物理内存免受非法访问，于是用户空间，内核空间，特权级也被提出来了，段寄存器里保存的不再是段基址，而是段选择子，由操作系统分配，用户也无法随意修改段选择子，必须通过中断的形式才能从用户态陷入内核态，中断执行的过程也需要经历特权级的检查，检查通过之后特权级从 3 切换到了 0，于是就可以放心合法的执行特权指令了。可以看到，通过操作系统分配段选择子+中断的方式内存得到了有效保护，但是分段可能造成内存碎片过大以致频繁 swap 会影响性能的问题，于是分页出现了，保护模式+分页终于可以让多进程，高效调度成为了可能

原文地址：https://zhuanlan.zhihu.com/p/536026548

作者：linux

# 【NO.516】「Linux」多线程详解，一篇文章彻底搞懂多线程中各个难点

## 1.什么是线程？

linux内核中是没有线程这个概念的，而是轻量级进程的概念：LWP。一般我们所说的线程概念是C库当中的概念。

### 1.1线程是怎样描述的？

线程实际上也是一个task_struct，工作线程拷贝主线程的task_struct，然后共用主线程的mm_struct。线程ID是在用task_struct中pid描述的，而task_struct中tgid是线程组ID，表示线程属于该线程组，对于主线程而言，其pid和tgid是相同的，我们一般看到的进程ID就是tgid。

即:

![img](https://pic4.zhimg.com/80/v2-42239c9b5aca18ac66aa9a78bb8921eb_720w.webp)

获取线程ID和主线程ID的值：

![img](https://pic3.zhimg.com/80/v2-1d7656a1fea373626911b29e60e28afe_720w.webp)

但是获取该gettid系统调用接口并没有被封装起来，如果确实需要获取线程ID，可使用：

```text
#include <sys/syscall.h>
int TID = syscall(SYS_gettid);
```

则对线程组而言，所有的tgid一定是一样的，所有的pid一定是不一样的。主线程pid和tgid一样，工作线程pid和tgid一定不一样。

### 1.2如何查看一个线程的ID

命令：ps -eLf

![img](https://pic1.zhimg.com/80/v2-ce6286777e5867197ad60f6cbf6f6cc4_720w.webp)

上述polkitd进程是多线程的，进程ID为731，进程内有6个线程，线程ID为731，764，765，768，781，791。

![img](https://pic1.zhimg.com/80/v2-5ac5511da2bad62578cfbf3d29d146f8_720w.webp)

### 1.3多线程如何避免调用栈混乱的问题？

工作线程和主线程共用一个mm_struct，如果都向栈中压栈，必然会导致调用栈出错。

实际上工作线程压栈是压了共享区，该共享区包含了许多线程独有的资源。如图：

![img](https://pic1.zhimg.com/80/v2-6ad430eaa460730b01d8c833f1530e48_720w.webp)

每一个线程，默认在共享区中占有的空间为8M，可以使用ulimit -s修改。

进程是资源分配的基本单位，线程是调度的基本单位。

**1.3.1线程独有资源**

- 线程ID
- 一组寄存器
- errno
- 信号屏蔽字
- 调度优先级

**1.3.2线程共享资源和环境**

- 文件描述符表
- 信号的处理方式
- 当前工作目录
- 用户id和组id

### 1.4为什么要有多线程？

举个生活中的例子， 这就好比去银行办理业务。 到达银行后， 首先取一个号码， 然后坐下来安心等待。 这时候你一定希望， 办理业务的窗口越多越好。 如果把整个营业大厅当成一个进程的话， 那么每一个窗口就是一个工作线程。

**1.4.1线程带来的优势**

1、线程会共享内存地址空间。

2、创建线程花费的时间要少于创建进程花费的时间。

3、终止线程花费的时间要少于终止进程花费的时间。

4、线程之间上下文切换的开销， 要小于进程之间的上下文切换。

5、线程之间数据的共享比进程之间的共享要简单。

6、充分利用多处理器的可并行数量。（线程会提高运行效率，但当线程多到一定程度后，可能会导致效率下降，因为会有线程调度切换。）

**1.4.2线程带来的缺点**

健壮性降低：多个线程之中， 只要有一个线程不够健壮存在bug（如访问了非法地址引发的段错误） ， 就会导致进程内的所有线程一起完蛋。

线程模型作为一种并发的编程模型， 效率并没有想象的那么高， 会出现复杂度高、 易出错、 难以测试和定位的问题。

### 1.5注意

1、并不是只有主线程才能创建线程， 被创建出来的线程同样可以创建线程。

2、不存在类似于fork函数那样的父子关系， 大家都归属于同一个线程组， 进程ID都相等， group_leader都指向主线程， 而且各有各的线程ID。

> 通过group_leader指针， 每个线程都能找到主线程。 主线程存在一个链表头，后面创建的每一个线程都会链入到该双向链表中。

3、并非只有主线程才能调用pthread_join连接其他线程， 同一线程组内的任意线程都可以对某线程执行pthread_join函数。

4、并非只有主线程才能调用pthread_detach函数， 其实任意线程都可以对同一线程组内的线程执行分离操作。

线程的对等关系：

![img](https://pic3.zhimg.com/80/v2-730890e52179ce98154ba36b39f50f6e_720w.webp)

## 2.线程创建

> 接口：int pthread_create(pthread_t *thread, const pthread_attr_t *attr,void *(*start_routine) (void *), void *arg);
> 参数解释
> 1、thread：线程标识符，是一个出参
> 2、attr：线程属性
> 3、star_routine：函数指针，保存线程入口函数的地址
> 4、arg：给线程入口函数传参
> 返回值：成功返回0，失败返回error number

详解：

第一个参数是pthread_t类型的指针， 线程创建成功的话，会将分配的线程ID填入该指针指向的地址。 线程的后续操作将使用该值作为线程的唯一标识。

第二个参数是pthread_attr_t类型， 通过该参数可以定制线程的属性， 比如可以指定新建线程栈的大小、 调度策略等。 如果创建线程无特殊的要求， 该值也可以是NULL， 表示采用默认属性。

第三个参数是线程需要执行的函数。 创建线程， 是为了让线程执行一定的任务。 线程创建成功之后， 该线程就会执行start_routine函数， 该函数之于线程， 就如同main函数之于主线程。

第四个参数是新建线程执行的start_routine函数的入参。

pthread_create错误码及描述：

![img](https://pic3.zhimg.com/80/v2-12fb1014e55a6338a270e154644393ee_720w.webp)

### 2.1传入参数arg的选择

![img](https://pic2.zhimg.com/80/v2-b2448de22126e5c43e8aa732739368d9_720w.webp)

不要使用临时变量传参，使用堆上开辟的变量可以。

例：

```text
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <unistd.h>
void *ThreadWork(void *arg)
{
  int *p = (int*)arg;
  printf("i am work thread:%p,   data:%d\n",pthread_self(),*p);
  pthread_exit(NULL);
}
int main()
{
  int i = 1;
  pthread_t tid;
  int ret = pthread_create(&tid,NULL,ThreadWork,(void*)&i);//不要传临时变量，这里是示范
  if(ret != 0)
  {
    perror("pthread_create");
    return -1;
  }
  while(1)
  {
    printf("i am main work thread\n");
    sleep(1);
  }
  return 0;
}
```

### 2.2线程ID以及进程地址空间

线程获取自身的ID：

```text
#include <pthread.h>
pthread_t pthread_self(void);
```

判断两个线程ID是否对应着同一个线程：

```text
#include <pthread.h>
int pthread_equal(pthread_t t1, pthread_t t2);
```

返回为0时，则表示两个线程为同一个线程，非0时，表示不是同一个线程。

用户调用pthread_create函数时， 首先要为线程分配线程栈， 而线程栈的位置就落在共享区。 调用mmap函数为线程分配栈空间。 pthread_create函数分配的pthread_t类型的线程ID， 不过是分配出来的空间里的一个地址， 更确切地说是一个结构体的指针。

![img](https://pic4.zhimg.com/80/v2-45e85ef60e7c502b2aaff4c61062f4ab_720w.webp)

即：

![img](https://pic4.zhimg.com/80/v2-f359fdf490a2ac0e09f3ea969674c5d7_720w.webp)

### 2.3线程注意点

1、线程ID是进程地址空间内的一个地址， 要在同一个线程组内进行线程之间的比较才有意义。 不同线程组内的两个线程， 哪怕两者的pthread_t值是一样的， 也不是同一个线程。

2、线程ID就有可能会被复用：

> 1、线程退出。
> 2、线程组的其他线程对该线程执行了pthread_join， 或者线程退出前将分离状态设置为已分离。
> 3、再次调用pthread_create创建线程。

### 2.4线程创建出来的默认值

线程创建的第二个参数是pthread_attr_t类型的指针， pthread_attr_init函数会将线程的属性重置成默认值。

线程属性及默认值：

![img](https://pic3.zhimg.com/80/v2-929b34ff679edc7ab04c6a4805f1f2c2_720w.webp)

如果确实需要很多的线程， 可以调用接口来调整线程栈的大小：

```text
#include <pthread.h>
int pthread_attr_setstacksize(pthread_attr_t *attr,size_t stacksize);
int pthread_attr_getstacksize(pthread_attr_t *attr,size_t *stacksize);
```

## 3.线程终止

线程终止，但进程不会终止的方法：

1、入口函数的return返回，线程就退出了

2、线程调用pthread_exit(NULL)，谁调用谁退出

> \#include <pthread.h>
> void pthread_exit(void *retval);
> 参数：retval是返回信息，”临终遗言“，可以给可以不给
> 该变量不能使用临时变量。
> 可使用：全局变量、堆上开辟的空间、字符串常量。

pthread_exit和线程启动函数（start_routine） 执行return是有区别的。 在start_routine中调用的任何层级的函数执行pthread_exit（） 都会引发线程退出， 而return， 只能是在start_routine函数内执行才能导致线程退出。

3、其它线程调用了pthread_cancel函数取消了该线程

> int pthread_cancel(pthread_t thread);
> thread：线程标识符
> 调用该函数的执行流可以取消其它线程，但是需要知道其它线程的线程标识符，也可以执行流自己取消自己，传入自己的线程标识符。

如果线程组中的任何一个线程调用了exit函数， 或者主线程在main函数中执行了return语句， 那么整个线程组内的所有线程都会终止。

## 4.线程等待

### 4.1线程等待接口

```text
#include <pthread.h>
int pthread_join(pthread_t thread, void **retval);
```

![img](https://pic2.zhimg.com/80/v2-af6806899576fa7e7dc672830659aa25_720w.webp)

调用该函数，该执行流在等待线程退出的时候，该执行流是阻塞在pthread_joind当中的。

### 4.2线程等待和进程等待的不同

第一点不同之处是进程之间的等待只能是父进程等待子进程， 而线程则不然。线程组内的成员是对等的关系， 只要是在一个线程组内， 就可以对另外一个线程执行连接（join） 操作。

第二点不同之处是进程可以等待任一子进程的退出 ， 但是线程的连接操作没有类似的接口， 即不能连接线程组内的任一线程， 必须明确指明要连接的线程的线程ID。

pthread_join()错误码：

![img](https://pic3.zhimg.com/80/v2-305dba3a6243881444f5e677940b07d6_720w.webp)

### 4.3为什么要等待退出的线程？

如果不连接已经退出的线程， 会导致资源无法释放。 所谓资源指的又是什么呢？

1、已经退出的线程， 其空间没有被释放， 仍然在进程的地址空间之内。

2、新创建的线程， 没有复用刚才退出的线程的地址空间。

如果不执行连接操作， 线程的资源就不能被释放， 也不能被复用， 这就造成了资源的泄漏。

纵然调用了pthread_join， 也并没有立即调用munmap来释放掉退出线程的栈， 它们是被后建的线程复用了。 释放线程资源的时候， 若进程可能再次创建线程， 而频繁地munmap和mmap会影响性能， 所以将该栈缓存起来， 放到一个链表之中， 如果有新的创建线程的请求， 会首先在栈缓存链表中寻找空间合适的栈， 有的话， 直接将该栈分配给新创建的线程。

例：

```text
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/syscall.h>


void *ThreadWork(void *arg)
{
  int *p = (int*)arg;
  printf("pid :  %d\n",syscall(SYS_gettid));
  printf("i am work thread:%p,   data:%d\n",pthread_self(),*p);
  sleep(3);
  pthread_exit(NULL);
}

int main()
{
  int i = 1;
  pthread_t tid;
  int ret = pthread_create(&tid,NULL,ThreadWork,(void*)&i);//不要传临时变量，这里是示范
  if(ret != 0)
  {
    perror("pthread_create");
    return -1;
  }
  pthread_join(tid,NULL);//线程等待
  while(1)
  {
    printf("i am main work thread\n");
    sleep(1);
  }
  return 0;
}
```

## 5.线程分离

```text
接口：#include <pthread.h>
int pthread_detach(pthread_t thread);
```

默认情况下， 新创建的线程处于可连接（Joinable） 的状态， 可连接状态的线程退出后， 需要对其执行连接操作， 否则线程资源无法释放， 从而造成资源泄漏。

如果其他线程并不关心线程的返回值， 那么连接操作就会变成一种负担： 你不需要它， 但是你不去执行连接操作又会造成资源泄漏。 这时候你需要的东西只是：线程退出时， 系统自动将线程相关的资源释放掉， 无须等待连接。

可以是线程组内其他线程对目标线程进行分离， 也可以是线程自己执行pthread_detach函数。

线程的状态之中， 可连接状态和已分离状态是冲突的， 一个线程不能既是可连接的， 又是已分离的。 因此， 如果线程处于已分离的状态， 其他线程尝试连接线程时， 会返回EINVAL错误。

pthread_detach错误码：

![img](https://pic1.zhimg.com/80/v2-0b138e4679b2e1a102f4aa30a393c2dc_720w.webp)

注意：这里的已分离不是指线程失去控制，不归线程组管，而是指线程退出后，系统会自动释放线程资源。若是线程组内的任意线程执行了exit函数，即使是已分离的线程，也仍会收到影响，一并退出。

## 6.线程安全

线程安全中涉及到的概念：

```text
临界资源：多线程中都能访问到的资源
临界区：每个线程内部，访问临界资源的代码，就叫临界区
```

### 6.1什么是线程不安全？

多个线程访问同一块临界资源，导致资源产生二义性的现象。

**6.1.1举一个例子**

- 假设现在有两个线程A和B，单核CPU的情况下，此时有一个int类型的全局变量为100，A和B的入口函数都要对这个全局变量进行–操作。
- 线程A先拿到CPU资源后，对全局变量进行–操作并不是原子性操作，也就是意味着，A在执行–的过程中有可能会被打断。假设A刚刚将全局变量的值读到寄存器当中，就被切换出去了，此时程序计数器保存了下一条执行的指令，上下文信息保存寄存器中的值，这两个东西是用来线程A再次拿到CPU资源后，恢复现场使用的。
- 此时，线程B拿到了CPU资源，对全局变量进行了–操作，并且将100减为了99，回写到了内存中。
- A再次拥有了CPU资源后，恢复现场，继续往下执行，从寄存器中读到的值仍为100，减完之后为99，回写到内存中为99。

上述例子中，线程A和B都对全局变量进行了–操作，全局变量的值应该变为98，但程序现在实际的结果为99，所以这就导致了线程不安全。

### 6.2如何解决线程不安全现象?

解决方案只需做到下述三点即可：

1、代码必须要有互斥的行为： 当一个线程正在临界区中执行时， 不允许其他线程进入该临界区中。

2、如果多个线程同时要求执行临界区的代码， 并且当前临界区并没有线程在执行， 那么只能允许一个线程进入该临界区。

3、如果线程不在临界区中执行， 那么该线程不能阻止其他线程进入临界区。

则本质上，我们需要对该临界区加一把锁：

![img](https://pic4.zhimg.com/80/v2-e270fae0677754a8f7c902b3e1b5d6f3_720w.webp)

锁是一个很普遍的需求， 当然用户可以自行实现锁来保护临界区。 但是实现一个正确并且高效的锁非常困难。 纵然抛下高效不谈， 让用户从零开始实现一个正确的锁也并不容易。 正是因为这种需求具有普遍性， 所以Linux提供了互斥量。

### 6.3互斥量接口

**6.3.1互斥量的初始化**

1、静态分配：

```text
#include <pthread.h>
pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;
```

2、动态分配：

```text
int pthread_mutex_init(pthread_mutex_t *restrict mutex,const pthread_mutexattr_t *restrict attr);
```

![img](https://pic3.zhimg.com/80/v2-224580e4d8cec26b47aa880ef556a516_720w.webp)

调用int pthread_mutex_init（）函数后，互斥量是处于没有加锁的状态。

**6.3.2互斥量的销毁**

```text
int pthread_mutex_destroy(pthread_mutex_t *mutex);
```

注意：

1、使用PTHREAD_MUTEX_INITIALIZER初始化的互斥量无须销毁。

2、不要销毁一个已加锁的互斥量， 或者是真正配合条件变量使用的互斥量。

3、已经销毁的互斥量， 要确保后面不会有线程再尝试加锁。

当互斥量处于已加锁的状态， 或者正在和条件变量配合使用， 调用pthread_mutex_destroy函数会返回EBUSY错误码。

**6.3.3互斥量的加锁**

```text
int pthread_mutex_lock(pthread_mutex_t *mutex);
int pthread_mutex_trylock(pthread_mutex_t *mutex);
int pthread_mutex_timedlock(pthread_mutex_t *restrict mutex, const struct timespec *restrict abs_timeout);
```

第一个接口：int pthread_mutex_lock(pthread_mutex_t *mutex);

1、该接口是阻塞加锁接口。

2、mutex为传入互斥锁变量的地址

3、如果mutex当中的计数器为1，pthread_mutex_lock接口就返回了，表示加锁成功，同时计数器当中的值会被更改为0.

4、如果mutex当中的计数器为0，pthread_mutex_lock接口就阻塞了，pthread_mutex_lock接口没有返回了，阻塞在函数内部，直到加锁成功

第二个接口：int pthread_mutex_trylock(pthread_mutex_t *mutex);

1、该接口为非阻塞接口

2、mutex中计数器为1时，加锁成功，计数器置为0，然后返回

3、mutex中计数器为0时，加锁失败，但也会返回，此时加锁是失败状态，一定不要去访问临界资源

4、非阻塞接口一般都需要搭配循环来使用。

第三个接口：int pthread_mutex_timedlock(pthread_mutex_t *restrict mutex, const struct timespec *restrict abs_timeout);

1、带有超时时间的加锁接口

2、不能直接获取互斥锁的时候，会等待abs_timeout时间

3、如果在这个时间内加锁成功了，直接返回，不需要再继续等待剩余的时间，并且表示加锁成功

4、如果超出了该时间，也返回了，但是加锁失败了，需要循环加锁

上述三个加锁接口，第一个接口用的最多。

**6.3.4互斥量的解锁**

```text
int pthread_mutex_unlock(pthread_mutex_t *mutex);
```

1. 对上述所有的加锁接口，都可使用该函数解锁
2. 解锁的时候，会将互斥锁当中计数器的值从0变为1，表示其它线程可以获取互斥量

### 6.4互斥锁的本质

1、在互斥锁内部有一个计数器，其实就是互斥量，计数器的值只能为0或者为1

2、当线程获取互斥锁的时候，如果计数器当前值为0，表示当前线程不能获取到互斥锁，也就是没有获取到互斥锁，就不要去访问临界资源

3、当前线程获取互斥锁的时候，如果计数器当前值为1，表示当前线程可以获取到互斥锁，也就是意味着可以访问临界资源

### 6.5互斥锁中的计数器如何保证了原子性？

获取锁资源的时候（加锁）：

1、寄存器当中值直接赋值为0

2、将寄存器当中的值和计数器当中的值进行交换

3、判断寄存器当中的值，得出加锁结果

两种情况：

![img](https://pic1.zhimg.com/80/v2-2954e6bfb9307a54931c667b7e8faa34_720w.webp)

例：4个线程，对同一个全局变量进行减减操作

```text
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/syscall.h>
#define NUMBER 4
int g_val = 100;

pthread_mutex_t mutex;//定义互斥锁


void *ThreadWork(void *arg)
{
  int *p = (int*)arg;
  pthread_detach(pthread_self());//自己分离自己，不用主线程回收它的资源了
  while(1)
  {
    pthread_mutex_lock(&mutex);//加锁
    if(g_val > 0)
    {
      printf("i am pid : %d,i get g_val : %d\n",(int)syscall(SYS_gettid),g_val);
      --g_val;
      usleep(2);
    }
    else{
      pthread_mutex_unlock(&mutex);//在所有可能退出的地方，进行解锁
      break;
    }
    pthread_mutex_unlock(&mutex);//解锁

  }
  pthread_exit(NULL);
}

int main()
{
  pthread_t tid[NUMBER];
  pthread_mutex_init(&mutex,NULL);//互斥锁初始化
  int i = 0;
  for(;i < NUMBER;++i)
  {
    int ret = pthread_create(&tid[i],NULL,ThreadWork,(void*)&g_val);//不要传临时变量，这里是示范
    if(ret != 0)
    {
      perror("pthread_create");
      return -1;
     }
  }
  //pthread_join(tid,NULL);//线程等待
  //pthread_detach(tid);//线程分离
  pthread_mutex_destroy(&mutex);//销毁互斥锁
  while(1)
  {
    printf("i am main work thread\n");
    sleep(1);
  }
  return 0;
}
```

### 6.6互斥锁公平嘛？

互斥锁是不公平的。

内核维护等待队列， 互斥量实现了大体上的公平； 由于等待线程被唤醒后， 并不自动持有互斥量， 需要和刚进入临界区的线程竞争（抢锁）， 所以互斥量并没有做到先来先服务。

### 6.7互斥锁的类型

1、PTHREAD_MUTEX_NORMAL： 最普通的一种互斥锁。 它不具备死锁检测功能， 如线程对自己锁定的互斥量再次加锁， 则会发生死锁。

2、
PTHREAD_MUTEX_RECURSIVE_NP： 支持递归的一种互斥锁， 该互斥量的内部维护有互斥锁的所有者和一个锁计数器。 当线程第一次取到互斥锁时， 会将锁计数器置1， 后续同一个线程再次执行加锁操作时， 会递增该锁计数器的值。 解锁则递减该锁计数器的值， 直到降至0， 才会真正释放该互斥量， 此时其他线程才能获取到该互斥量。 解锁时， 如果互斥量的所有者不是调用解锁的线程， 则会返回EPERM。

3、
PTHREAD_MUTEX_ERRORCHECK_NP： 支持死锁检测的互斥锁。 互斥量的内部会记录互斥锁的当前所有者的线程ID（调度域的线程ID） 。 如果互斥量的持有线程再次调用加锁操作， 则会返回EDEADLK。 解锁时， 如果发现调用解锁操作的线程并不是互斥锁的持有者， 则会返回EPERM。

4、自旋锁，自旋锁采用了和互斥量完全不同的策略， 自旋锁加锁失败， 并不会让出CPU， 而是不停地尝试加锁， 直到成功为止。 这种机制在临界区非常小且对临界区的争夺并不激烈的场景下， 效果非常好。自旋锁的效果好， 但是副作用也大， 如果使用不当， 自旋锁的持有者迟迟无法释放锁， 那么， 自旋接近于死循环， 会消耗大量的CPU资源， 造成CPU使用率飙高。 因此， 使用自旋锁时， 一定要确保临界区尽可能地小， 不要有系统调用， 不要调用sleep。 使用strcpy/memcpy等函数也需要谨慎判断操作内存的大小， 以及是否会引起缺页中断。

5、PTHREAD_MUTEX_ADAPTIVE_NP：自适应锁，首先与自旋锁一样， 持续尝试获取， 但过了一定时间仍然不能申请到锁， 就放弃尝试， 让出CPU并等待。 PTHREAD_MUTEX_ADAPTIVE_NP类型的互斥量， 采用的就是这种机制。

### 6.8死锁和活锁

![img](https://pic2.zhimg.com/80/v2-aff262e422876ccf53db6bd423b0be29_720w.webp)

线程1已经成功拿到了互斥量1， 正在申请互斥量2， 而同时在另一个CPU上，线程2已经拿到了互斥量2， 正在申请互斥量1。 彼此占有对方正在申请的互斥量，结局就是谁也没办法拿到想要的互斥量， 于是死锁就发生了。

**6.8.1死锁概念**

死锁是指在一组进程中的各个进程均占有不会释放的资源，但因互相申请被其它进程所占有不会释放的资源而处于一种永久等待的状态。

**6.8.2死锁的四个必要条件**

1、互斥条件：一个资源只能被一个执行流使用

2、请求与保持条件：一个执行流因请求资源而阻塞时，对已获得的资源不会释放

3、不剥夺条件：一个执行流已获得的资源，在未使用完之前，不能强行剥夺

4、循环等待条件：若干执行流之间形成一种头尾相接的循环等待资源的关系

**6.8.3避免死锁**

1、破坏死锁的四个必要条件（实际上只能破坏条件2和4）

2、加锁顺序一致（按照先后顺序申请互斥锁）

3、避免未释放锁的情况

4、资源一次性分配

**6.8.4活锁**

避免死锁的另一种方式是尝试一下，如果取不到锁就返回。

```text
int pthread_mutex_trylock(pthread_mutex_t *mutex);
int pthread_mutex_timedlock(pthread_mutex_t *restrict mutex,const struct timespec *restrict abs_timeout);
```

这两个函数反映了一种，不行就算了的思想。

trylock不行就回退的思想有可能会引发活锁（live lock） 。 生活中也经常遇到两个人迎面走来， 双方都想给对方让路， 但是让的方向却不协调， 反而互相堵住的情况 。 活锁现象与这种场景有点类似。

![img](https://pic4.zhimg.com/80/v2-2c2ce9846311e996ae6f7da8b0ec64c3_720w.webp)

线程1首先申请锁mutex_a后， 之后尝试申请mutex_b， 失败以后， 释放mutex_a进入下一轮循环， 同时线程2会因为尝试申请mutex_a失败，而释放mutex_b， 如果两个线程恰好一直保持这种节奏， 就可能在很长的时间内两者都一次次地擦肩而过。 当然这毕竟不是死锁， 终究会有一个线程同时持有两把锁而结束这种情况。 尽管如此， 活锁的确会降低性能。

**6.8.5死锁调试**

```text
查看多个线程堆栈：thread apply all bt
跳转到线程中：t 线程号
查看具体的调用堆栈：f 堆栈号
直接从pid号用gdb调试：gdb attach pid

#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/syscall.h>
#define NUMBER 2

pthread_mutex_t mutex1;//定义互斥锁
pthread_mutex_t mutex2;


void *ThreadWork1(void *arg)
{
  int *p = (int*)arg;
  pthread_mutex_lock(&mutex1);
  
  sleep(2);
  
  pthread_mutex_lock(&mutex2);
  pthread_mutex_unlock(&mutex2);
  pthread_mutex_unlock(&mutex1);
  return NULL;
}

void *ThreadWork2(void *arg)
{
  int *p = (int*)arg;
  pthread_mutex_lock(&mutex2);
  
  sleep(2);
  
  pthread_mutex_lock(&mutex1);
  pthread_mutex_unlock(&mutex1);
  pthread_mutex_unlock(&mutex2);
  return NULL;
}
int main()
{
  pthread_t tid[NUMBER];
  pthread_mutex_init(&mutex1,NULL);//互斥锁初始化
  pthread_mutex_init(&mutex2,NULL);//互斥锁初始化
  int i = 0;
  int ret = pthread_create(&tid[0],NULL,ThreadWork1,(void*)&i);
  if(ret != 0)
  {
    perror("pthread_create");
    return -1;
  }
  ret = pthread_create(&tid[1],NULL,ThreadWork2,(void*)&i);
  if(ret != 0)
  {
    perror("pthread_create");
    return -1;
  }
  //pthread_join(tid,NULL);//线程等待
  //pthread_join(tid,NULL);//线程等待
  //pthread_detach(tid);//线程分离
  pthread_join(tid[0],NULL);
  pthread_join(tid[1],NULL);
  pthread_mutex_destroy(&mutex1);//销毁互斥锁
  pthread_mutex_destroy(&mutex2);//销毁互斥锁
  while(1)
  {
    printf("i am main work thread\n");
    sleep(1);
  }
  return 0;
}
```

在上述代码中，一定会出现死锁，线程1拿到了互斥锁1，又再去申请线程2的互斥锁2，线程2拿到了互斥锁2又再去申请线程1的互斥锁1。

开始调试：

1、找到进程号

![img](https://pic2.zhimg.com/80/v2-8e1eb2ea447981df2212fa2622282591_720w.webp)

2、开始调试

![img](https://pic3.zhimg.com/80/v2-78fdb318eb2acf7c4915c5c586cdbcd6_720w.webp)

3、查看多个线程堆栈

![img](https://pic4.zhimg.com/80/v2-e03c95c8739e8139d23850d381c692e7_720w.webp)

4、跳转到线程中

![img](https://pic3.zhimg.com/80/v2-8c20d0e5574b37001d9e52e48bdbaf3e_720w.webp)

5、查看具体调用堆栈

![img](https://pic3.zhimg.com/80/v2-79c77f49cd2151503280b64586c699c2_720w.webp)

6、查看互斥锁1和互斥锁2，分别被谁拿着

![img](https://pic2.zhimg.com/80/v2-a94db56ae96a52658911c94b2fd7c4b9_720w.webp)

### 6.9读写锁

**6.9.1什么是读写锁？**

大部分情况下，对于共享变量的访问特点：只是读取共享变量的值，而不是修改，只有在少数情况下，才会真正的修改共享变量的值。

在这种情况下，读请求之间是同步的，它们之间的并发访问是安全的。然而写请求必须锁住读请求和其它写请求。

即读线程可多个同时读，而写线程只允许同一时间内一个线程去写。

**6.9.2读写锁接口**

```text
#include <pthread.h>
//销毁
int pthread_rwlock_destroy(pthread_rwlock_t *rwlock);

//初始化
int pthread_rwlock_init(pthread_rwlock_t *restrict rwlock,
              const pthread_rwlockattr_t *restrict attr);
```

![img](https://pic3.zhimg.com/80/v2-890207d7d114d64e436ec1c0284aa92a_720w.webp)

读写锁的默认属性：

![img](https://pic4.zhimg.com/80/v2-62306a2d0fb1d844d61f69b9be243c47_720w.webp)

对于调用pthread_rwlock_init初始化的读写锁，在不需要读写锁的时候，需要调用pthread_rwlock_destroy销毁。

**6.9.3读者加锁**

```text
#include <pthread.h>

int pthread_rwlock_rdlock(pthread_rwlock_t *rwlock); //阻塞类型的读加锁接口
int pthread_rwlock_tryrdlock(pthread_rwlock_t *rwlock); //非阻塞类型的读加锁接口
```

最大的好处就是，允许多个线程以只读加锁的方式获取到读写锁；

本质上，读写锁的内部维护了一个引用计数，每当线程以读方式获取读写锁时，该引用计数+1；

当释放以读加锁的方式的读写锁时，会先对引用计数进行-1，直到引用计数的值为0的时候，才真正释放了这把读写锁。

**6.9.4写者加锁**

```text
#include <pthread.h>
int pthread_rwlock_trywrlock(pthread_rwlock_t *rwlock);// 非阻塞写
int pthread_rwlock_wrlock(pthread_rwlock_t *rwlock);//阻塞写
```

写锁用的是独占模式，如果当前读写锁被某写线程占用着，则不允许任何读锁通过请求，也不允许任何写锁请求通过，读锁请求和写锁请求都要陷入阻塞，直到线程释放写锁。

**6.9.5 解锁**

```text
#include <pthread.h>
int pthread_rwlock_unlock(pthread_rwlock_t *rwlock);
```

不论是读者加锁还是写者加锁，都采用该接口进行解释。

读者解锁，只有当引用计数为0的时候，才真正释放了读写锁。

**6.9.6读写锁的竞争策略**

对于读写锁而言，目前有两种策略，读者优先和携着优先；

读写锁的类型有如下几种：

```text
PTHREAD_RWLOCK_PREFER_READER_NP, //读者优先
PTHREAD_RWLOCK_PREFER_WRITER_NP, //很唬人， 但是也是读者优先
PTHREAD_RWLOCK_PREFER_WRITER_NONRECURSIVE_NP, //写者优先
PTHREAD_RWLOCK_DEFAULT_NP = PTHREAD_RWLOCK_PREFER_READER_NP
```

读者优先：读锁来请求可以立即响应，只要有一个读锁没完成，那么写锁就无法写。这种策略是不公平的，极端情况下，写现场很可能被饿死，即线程总是拿不到锁资源。

写者优先：只要线程申请了写锁，那么在写锁后面到来的读锁请求就会统统被阻塞，不能先于写锁拿到锁。

读写锁实现中的变量及含义

![img](https://pic1.zhimg.com/80/v2-d2af269cba4c6ddeb8e63811fa945d58_720w.webp)

对于读请求而言：如果

> \1. 无线程持有写锁，即_writer = 0.
> \2. 采用读者优先策略或者当前没有写锁申请请求，即 _nr_writers_queue = 0
> \3. 当满足这两个条件时，读锁请求立即获得读锁，返回之前执行_nr_readers++，表示多了一个线程正在读
> \4. 不满足这两个条件时，执行_nr_readers_queued++，表示增加了一个读锁等待者，然后调用futex，陷入阻塞。醒来之后，执行_nr_readers_queued- -，再次判断是否满足条件1，2

对于写请求而言：如果

> \1. 无线程持有写锁，即_writer = 0.
> \2. 没有线程持有读锁，即_nr_readers = 0.
> \3. 如果上述条件满足，就会立即拿到锁，将_writer 置为当前线程的ID
> \4. 如果不满足，则执行_nr_writers_queue++， 表示增加了一个写锁等待者线程，然后执行futex陷入等待。醒来后，先执行_nr_writers_queue- -，再继续判断条件1，2

对于解锁，如果当前是写锁：

> \1. 执行_writer = 0.，表示释放写锁。
> \2. 根据_nr_writers_queue判断有没有写锁，如果有则唤醒一个写锁，如果没有写锁等待者，则唤醒所有的读锁等待者。

对于解锁，如果当前是读锁：

> \1. 执行_nr_readers- -，表示读锁占有者少了一个。
> \2. 判断_nr_readers是否等于0，是的话则表示当前线程是最后一个读锁占有者，需要唤醒写锁等待者或读锁等待者
> \3. 根据_nr_writers_queue判断是否存在写锁等待者，若有，则唤醒一个写锁等待线程
> \4. 如果没有写锁等待者，判断是否存在读锁等待者，若有，则唤醒全部的读锁等待者

读写锁很容易造成，读者饿死或者写者饿死。

也可以设计公平的读写锁。

代码：

```text
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <sys/syscall.h>
#include <unistd.h>
#include <fcntl.h>

#define THREADCOUNT 100

static int count = 0;
static pthread_rwlock_t lock;

void* Read(void* i)
{
  while(1)
  {
    pthread_rwlock_rdlock(&lock);
    printf("i am 读线程 : %d, 现在的count是%d\n", (int)syscall(SYS_gettid), count);
    pthread_rwlock_unlock(&lock);
    //sleep(1);
  }
}

void* Write(void* i)
{
  while(1)
  {
    pthread_rwlock_wrlock(&lock);
    ++count;
    printf("i am 写线程 : %d, 现在的count是: %d\n", (int)syscall(SYS_gettid), count);
    pthread_rwlock_unlock(&lock);
    sleep(1);
  }
}


int main()
{
  //close(1);
  //int fd = open("./dup2_result.txt", O_CREAT | O_RDWR);
  //dup2(fd, 1);
  pthread_t tid[THREADCOUNT];
  pthread_rwlock_init(&lock, NULL);
  for(int i = 0; i < THREADCOUNT; ++i)
  {
    if(i % 2 == 0)
    {
      pthread_create(&tid[i], NULL, Read, (void*)&i);
    }
    else
    {
      pthread_create(&tid[i], NULL, Write, (void*)&i);
    }
  }

  for(int i = 0; i < THREADCOUNT; ++i)
  {
    pthread_join(tid[i], NULL);
  }

  pthread_rwlock_destroy(&lock);
  return 0;
}
```

上述代码很容易触发线程饿死。
读饿死或者写饿死。

## 7.线程间同步

### 7.1为什么需要线程同步？

线程同步是为了对临界资源访问的合理性。

例如：

> 就像工厂里生产车间没有原料了， 所有生产车间都停工了， 工人们都在车间睡觉。 突然进来一批原料， 如果原料充足， 你会发广播给所有车间， 原料来了， 快来开工吧。 如果进来的原料很少， 只够一个车间开工的， 你可能只会通知一个车间开工。

### 7.2如何做到线程间同步？

条件等待是线程间同步的另一种方法。

如果条件不满足， 它能做的事情就是等待， 等到条件满足为止。 通常条件的达成， 很可能取决于另一个线程， 比如生产者-消费者模型。 当另外一个线程发现条件符合的时候， 它会选择一个时机去通知等待在这个条件上的线程。 有两种可能性， 一种是唤醒一个线程， 一种是广播， 唤醒其他线程。

则在这个情况下，需要做到：

1、线程在条件不满足的情况下， 主动让出互斥量， 让其他线程去折腾， 线程在此处等待， 等待条件的满足；

2、一旦条件满足， 线程就可以立刻被唤醒。

3、线程之所以可以安心等待， 依赖的是其他线程的协作， 它确信会有一个线程在发现条件满足以后， 将向它发送信号， 并且让出互斥量。

### 7.3条件变量

本质上是PCB等待队列 + 等待接口 + 唤醒接口。

**7.3.1条件变量的初始化**

静态初始化

```text
pthread_cond_t cond = PTHREAD_COND_INITIALIZER;
```

动态初始化

```text
pthread_cond_init(pthread_cond_t *cond,const pthread_condattr_t *attr);
```

![img](https://pic1.zhimg.com/80/v2-c898a8dc634598dd27f7303ce080aa10_720w.webp)

**7.3.2条件变量的等待**

```text
int pthread_cond_wait(pthread_cond_t *restrict cond,pthread_mutex_t *restrict mutex);
int pthread_cond_timedwait(pthread_cond_t *restrict conpthread_mutex_t *restrict mutex,const struct timespec *restrict abstime);
```

为什么这两个接口中有互斥锁？

条件不会无缘无故地突然变得满足了， 必然会牵扯到共享数据的变化。 所以一定要有互斥锁来保护。 没有互斥锁， 就无法安全地获取和修改共享数据。

同步并没有保证互斥，而保证互斥是使用到了互斥锁。

```text
pthread_mutex_lock(&m)
while(condition_is_false)
{
	pthread_mutex_unlock(&m);
	//解锁之后， 等待之前， 可能条件已经满足， 信号已经发出， 但是该信号可能会被错过
	cond_wait(&cv);
	pthread_mutex_lock(&m);
}
```

上面的解锁和等待不是原子操作。 解锁以后， 调用cond_wait之前，如果已经有其他线程获取到了互斥量， 并且满足了条件， 同时发出了通知信号， 那么cond_wait将错过这个信号， 可能会导致线程永远处于阻塞状态。 所以解锁加等待必须是一个原子性的操作， 以确保已经注册到事件的等待队列之前， 不会有其他线程可以获得互斥量。

那先注册等待事件， 后释放锁不行吗？ 注意， 条件等待是个阻塞型的接口， 不单单是注册在事件的等待队列上， 线程也会因此阻塞于此， 从而导致互斥量无法释放， 其他线程获取不到互斥量， 也就无法通过改变共享数据使等待的条件得到满足， 因此这就造成了死锁。

```text
pthread_mutex_lock(&m);
while(condition_is_false)
	pthread_cond_wait(&v,&m);//此处会阻塞
/*如果代码运行到此处， 则表示我们等待的条件已经满足了，
*并且在此持有了互斥量
*/
/*在满足条件的情况下， 做你想做的事情。
*/
pthread_mutex_unlock(&m);
```

pthread_cond_wait函数只能由拥有互斥量的线程来调用， 当该函数返回的时候， 系统会确保该线程再次持有互斥量， 所以这个接口容易给人一种误解， 就是该线程一直在持有互斥量。 事实上并不是这样的。 这个接口向系统声明了我在PCB等待序列中之后， 就把互斥量给释放了。 这样其他线程就有机会持有互斥量，操作共享数据， 触发变化， 使线程等待的条件得到满足。

```text
pthread_cond_wait内部会进行解锁逻辑，则一定要先放到PCB等待序列中，再进行解锁。
while(condition_is_false)
	pthread_cond_wait(&v,&m);//此处会阻塞
if(condition_is_false)
	pthread_cond_wait(&v,&m);//此处会阻塞
```

唤醒以后， 再次检查条件是否满足， 是不是多此一举？

因为唤醒中存在虚假唤醒（spurious wakeup） ， 换言之，条件尚未满足， pthread_cond_wait就返了。 在一些实现中， 即使没有其他线程向条件变量发送信号， 等待此条件变量的线程也有可能会醒来。

条件满足了发送信号， 但等到调用pthread_cond_wait的线程得到CPU资源时， 条件又再次不满足了。 好在无论是哪种情况， 醒来之后再次测试条件是否满足就可以解决虚假等待的问题。

pthread_cond_wait内部实现逻辑：

1. 将调用pthread_cond_wait函数的执行流放入到PCB等待队列当中
2. 解锁
3. 等待被唤醒
4. 被唤醒之后：

> 1、从PCB等待队列中移除出来
> 2、抢占互斥锁
> 情况1：拿到互斥锁，pthread_cond_wait就返回了
> 情况2：没有拿到互斥锁，阻塞在pthread_cond_wait内部抢锁的逻辑中
> 当阻塞在pthread_cond_wait函数抢锁逻辑中时，一旦执行流时间耗尽，意味着线程就被切换出来了，程序计数器就保存的是抢锁的指令，上下文信息保存的就是寄存器的值
> 当再次拥有CPU资源后，恢复抢锁逻辑
> 直到抢锁成功，pthread_cond_wait函数才会返回

**7.3.3条件变量的唤醒**

```text
int pthread_cond_signal(pthread_cond_t *cond);
int pthread_cond_broadcast(pthread_cond_t *cond);
```

pthread_cond_signal负责唤醒等待在条件变量上的一个线程。

pthread_cond_broadcast，就是广播唤醒等待在条件变量上的所有线程。

先发送信号，然后解锁互斥量，这个顺序是必须的嘛？

先通知条件变量、 后解锁互斥量， 效率会比先解锁、 后通知条件变量低。 因为先通知后解锁， 执行pthread_cond_wait的线程可能在互斥量已然处于加锁状态的时候醒来， 发现互斥量仍然没有解锁， 就会再次休眠， 从而导致了多余的上下文切换。

**7.3.4条件变量的销毁**

```text
int pthread_cond_destroy(pthread_cond_t *cond);
```

注意：

1、永远不要用一个条件变量对另一个条件变量赋值， 即pthread_cond_t cond_b = cond_a不合法， 这种行为是未定义的。

2、使用PTHREAD_COND_INITIALIZE静态初始化的条件变量， 不需要被销毁。

3、要调用pthread_cond_destroy销毁的条件变量可以调用pthread_cond_init重新进行初始化。

4、不要引用已经销毁的条件变量， 这种行为是未定义的。

例：

```text
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/syscall.h>
#define NUMBER 2

int g_bowl = 0;
pthread_mutex_t mutex;//定义互斥锁
pthread_cond_t cond1;//条件变量
pthread_cond_t cond2;//条件变量


void *WorkProduct(void *arg)
{
  int *p = (int*)arg;
  while(1)
  {
    pthread_mutex_lock(&mutex);
    while(*p > 0)
    {
      pthread_cond_wait(&cond2,&mutex);//条件等待，条件不满足，陷入阻塞
    }
    ++(*p);
    printf("i am workproduct :%d,i product %d\n",(int)syscall(SYS_gettid),*p);
    pthread_cond_signal(&cond1);//通知消费者
    pthread_mutex_unlock(&mutex);//释放锁
  }
  return NULL;
}

void *WorkConsume(void *arg)
{
  int *p = (int*)arg;
  while(1)
  {
    pthread_mutex_lock(&mutex);
    while(*p <= 0)
    {
      pthread_cond_wait(&cond1,&mutex);//条件等待，条件不满足，陷入阻塞
    }
    printf("i am workconsume :%d,i consume %d\n",(int)syscall(SYS_gettid),*p);
    --(*p);
    pthread_cond_signal(&cond2);//通知生产者
    pthread_mutex_unlock(&mutex);//释放锁
  }
  return NULL;
}
int main()
{
  pthread_t cons[NUMBER],prod[NUMBER];
  pthread_mutex_init(&mutex,NULL);//互斥锁初始化
  pthread_cond_init(&cond1,NULL);//条件变量初始化
  pthread_cond_init(&cond2,NULL);//条件变量初始化
  int i = 0;
  for(;i < NUMBER;++i)
  {
    int ret = pthread_create(&prod[i],NULL,WorkProduct,(void*)&g_bowl);
    if(ret != 0)
    {
      perror("pthread_create");
      return -1;
    }
    ret = pthread_create(&cons[i],NULL,WorkConsume,(void*)&g_bowl);
    if(ret != 0)
    {
      perror("pthread_create");
      return -1;
    }
  }
  for(i = 0;i < NUMBER;++i)
  {
    pthread_join(cons[i],NULL);//线程等待
    pthread_join(prod[i],NULL);
  }
  pthread_mutex_destroy(&mutex);//销毁互斥锁
  pthread_cond_destroy(&cond1);
  pthread_cond_destroy(&cond2);
  while(1)
  {
    printf("i am main work thread\n");
    sleep(1);
  }
  return 0;
}
```

在这里为什么有两个条件变量呢？
若所有的线程只使用一个条件变量，会导致所有线程最后都进入PCB等待队列。

thread apply all bt查看：

![img](https://pic3.zhimg.com/80/v2-4e79bc47098cfaea872545f74686d0c2_720w.webp)

**7.3.5情况分析：两个生产者，两个消费者，一个PCB等待队列**

1、最开始的情况，两个消费者抢到了锁，此时生产者未生产，则都放入PCB等待队列中

![img](https://pic3.zhimg.com/80/v2-40e5d68635788d8b8b93badd576a1dae_720w.webp)

2、一个生产者抢到了锁，生产了一份材料，唤醒一个消费者，此时三者抢锁，若两个生产者分别先后抢到了锁，则都进入PCB等待队列中

![img](https://pic3.zhimg.com/80/v2-fd2d33e7cd5015d17d6d6ccf0149c69e_720w.webp)

3、只有一个消费者，则必会抢到锁，消费材料，唤醒PCB等待队列，若此时唤醒的是，消费者，则现在是这样一个情况：

![img](https://pic1.zhimg.com/80/v2-d80be0c043fbdd85c226ae3f33caec8c_720w.webp)

4、两个消费者在外边抢锁，一定都会进入PCB等待队列中

![img](https://pic4.zhimg.com/80/v2-ee9eb3e45aac17a3e894307f136c56cf_720w.webp)

解决上述问题可采用两种方法：

1、使用int pthread_cond_broadcast(pthread_cond_t *cond);，唤醒PCB等待队列中所有的线程。此时所有线程都会同时执行抢锁逻辑，太消费资源了。此方法不妥

2、采用两个PCB等待序列，一个放生产者，一个放消费者，生产者唤醒消费者，消费者唤醒生产者。

## 8.线程取消

### 8.1线程取消函数接口

```text
int pthread_cancel(pthread_t thread);
```

一个线程可以通过调用该函数向另一个线程发送取消请求。 这不是个阻塞型接口， 发出请求后， 函数就立刻返回了， 而不会等待目标线程退出之后才返回。

调用pthread_cancel时， 会向目标线程发送一个SIGCANCEL的信号， 该信号就是kill -l中消失的32号信号。

线程的默认取消状态是PTHREAD_CANCEL_ENABLE。即是可被取消的。

![img](https://pic2.zhimg.com/80/v2-32d5fbd4479866807f26a3e4a9f826a5_720w.webp)

什么是取消点？ 可通过man pthreads查看取消点
就是对于某些函数， 如果线程允许取消且取消类型是延迟取消， 并且线程也收到了取消请求， 那么当执行到这些函数的时候， 线程就可以退出了。

### 8.2线程取消带来的弊端

目标线程可能会持有互斥量、 信号量或其他类型的锁， 这时候如果收到取消请求， 并且取消类型是异步取消， 那么可能目标线程掌握的资源还没有来得及释放就被迫退出了， 这可能会给其他线程带来不可恢复的后果， 比如死锁（其他线程再也无法获得资源） 。

注意：

轻易不要调用pthread_cancel函数， 在外部杀死线程是很糟糕的做法，毕竟如果想通知目标线程退出， 还可以采取其他方法。

如果不得不允许线程取消， 那么在某些非常关键不容有失的代码区域， 暂时将线程设置成不可取消状态， 退出关键区域之后， 再恢复成可以取消的状态。

在非关键的区域， 也要将线程设置成延迟取消， 永远不要设置成异步取消。

### 8.3线程清理函数

假设遇到取消请求， 线程执行到了取消点， 却没有来得及做清理动作（如动态申请的内存没有释放， 申请的互斥量没有解锁等） ， 可能会导致错误的产生， 比如死锁， 甚至是进程崩溃。

为了避免这种情况， 线程可以设置一个或多个清理函数， 线程取消或退出时，会自动执行这些清理函数， 以确保资源处于一致的状态。

如果线程被取消， 清理函数则会负责解锁操作。

```text
void pthread_cleanup_push(void (*routine)(void *),void *arg);
void pthread_cleanup_pop(int execute);
```

这两个函数必须同时出现， 并且属于同一个语法块。

何时会触发注册的清理函数：?

1、当线程的主函数是调用pthread_exit返回的， 清理函数总是会被执行。

2、当线程是被其他线程调用pthread_cancel取消的， 清理函数总是会被执行。

3、当线程的主函数是通过return返回的， 并且pthread_cleanup_pop的唯一参数execute是0时， 清理函数不会被执行.

4、线程的主函数是通过return返回的， 并且pthread_cleanup_pop的唯一参数execute是非零值时， 清理函数会执行一次。

代码：

```text
#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include <pthread.h>
#include <unistd.h>
#include <sys/syscall.h>
#define NUMBER 2
int g_bowl = 0;

pthread_mutex_t mutex;//定义互斥锁

void clean(void *arg)
{
  printf("Clean up:%s\n",(char*)arg);
  pthread_mutex_unlock(&mutex);//释放锁
}

void *WorkCancel(void *arg)
{
  pthread_mutex_lock(&mutex);
  pthread_cleanup_push(clean,"clean up handler");//清除函数的push
  struct timespec t = {3,0};//取消点
  nanosleep(&t,0);
  pthread_cleanup_pop(0);//清除
  pthread_mutex_unlock(&mutex);
}

void *WorkWhile(void *arg)
{
  sleep(5);
  pthread_mutex_lock(&mutex);
  printf("i get the mutex\n");//若能拿到资源，则表示取消清理函数成功！
  pthread_mutex_unlock(&mutex);
  return NULL;
}
int main()
{
  pthread_t cons,prod;
  pthread_mutex_init(&mutex,NULL);//互斥锁初始化
  int ret = pthread_create(&prod,NULL,WorkCancel,(void*)&g_bowl);//该线程拿到锁，然后挂掉
  if(ret != 0)
  {
    perror("pthread_create");
    return -1;
  }
  int ret1 = pthread_create(&cons,NULL,WorkWhile,(void*)&ret);//测试该线程是否可以拿到锁
  if(ret1 != 0)
  {
    perror("pthread_create");
    return -1;
  }

  pthread_cancel(prod);//取消该线程
  pthread_join(prod,NULL);//线程等待
  pthread_join(cons,NULL);//线程等待
  pthread_mutex_destroy(&mutex);//销毁互斥锁
  while(1)
  {
    sleep(1);
  }
  return 0;
}
```

结果：只要拿到锁，就表明线程清理函数成功了。

![img](https://pic4.zhimg.com/80/v2-75bd5eacc35c2d6a73e1e6abbda4e29b_720w.webp)

## 9.多线程与fork()

永远不要在多线程程序里面调用fork。

Linux的fork函数， 会复制一个进程， 对于多线程程序而言， fork函数复制的是用fork的那个线程， 而并不复制其他的线程。 fork之后其他线程都不见了。 Linux存在forkall语义的系统调用， 无法做到将多线程全部复制。

多线程程序在fork之前， 其他线程可能正持有互斥量处理临界区的代码。 fork之后， 其他线程都不见了， 那么互斥量的值可能处于不可用的状态， 也不会有其他线程来将互斥量解锁。

## 10.生产者与消费者模型

### 10.1生产者与消费者模型的本质

本质上是一个线程安全的队列，和两种角色的线程（生产者和消费者）

存在三种关系：

1、生产者与生产者互斥

2、消费者与消费者互斥

3、生产者与消费者同步+互斥

### 10.2为什么需要生产者与消费者模型？

生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生成完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列中取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。这个阻塞队列就是用来给生产者和消费解耦的。

### 10.3优点

1、解耦

2、支持高并发

3、支持忙闲不均

### 10.4实现两个消费者线程，两个生产者线程的生产者消费者模型

生产者生成时用的同一个全局变量，故对该全局变量进行了加锁。

```text
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>
#include <pthread.h>
#include <queue>
#include <sys/syscall.h>

#define PTHREAD_COUNT 2
int data = 0;//全局变量作为插入数据
pthread_mutex_t mutex1;
class ModelOfConProd{
  public:
    ModelOfConProd()//构造
    {
      _capacity = 10;
      pthread_mutex_init(&_mutex,NULL);
      pthread_cond_init(&_cons,NULL);
      pthread_cond_init(&_prod,NULL);
    }
    ~ModelOfConProd()//析构
    {
      _capacity = 0;
      pthread_mutex_destroy(&_mutex);
      pthread_cond_destroy(&_cons);
      pthread_cond_destroy(&_prod);
    }

    void Push(int data)//push数据，生产者线程使用的
    {
      pthread_mutex_lock(&_mutex);
      while((int)_queue.size() >= _capacity)
      {
        pthread_cond_wait(&_prod,&_mutex);
      }
      _queue.push(data);
      pthread_mutex_unlock(&_mutex);
      pthread_cond_signal(&_cons);
    }


    void Pop(int& data)//pop数据，消费者线程使用的
    {
      pthread_mutex_lock(&_mutex);
      while(_queue.empty())
      {
        pthread_cond_wait(&_cons,&_mutex);
      }
      data = _queue.front();
      _queue.pop();
      pthread_mutex_unlock(&_mutex);
      pthread_cond_signal(&_prod);
    }

  private:
    int _capacity;//容量大小，限制容量大小
    std::queue<int> _queue;//队列
    pthread_mutex_t _mutex;//互斥锁
    pthread_cond_t _cons;//消费者条件变量
    pthread_cond_t _prod;//生产者条件变量
};

void *ConsumerStart(void *arg)//消费者入口函数
{
  ModelOfConProd *cp = (ModelOfConProd *)arg;
  while(1)
  {
    cp->Push(data);
    printf("i am pid : %d,i push :%d\n",(int)syscall(SYS_gettid),data);
    pthread_mutex_lock(&mutex1);//++的时候，给该全局变量加锁
    ++data;
    pthread_mutex_unlock(&mutex1);
  }
}


void *ProductsStart(void *arg)//生产者入口函数
{
  ModelOfConProd *cp = (ModelOfConProd *)arg;
  int data = 0;
  while(1)
  {
    cp->Pop(data);
    printf("i am pid : %d,i pop :%d\n",(int)syscall(SYS_gettid),data);
  }
}


int main()
{
  ModelOfConProd *cp = new ModelOfConProd;
  pthread_mutex_init(&mutex1,NULL);
  pthread_t cons[PTHREAD_COUNT],prod[PTHREAD_COUNT];
  for(int i = 0;i < PTHREAD_COUNT; ++i)
  {
    int ret = pthread_create(&cons[i],NULL,ConsumerStart,(void*)cp);
    if(ret < 0)
    {
      perror("pthread_create");
      return -1;
    }
    ret = pthread_create(&prod[i],NULL,ProductsStart,(void*)cp);
    if(ret < 0)
    {
      perror("pthread_create");
      return -1;
    }
  }


  for(int i = 0;i < PTHREAD_COUNT;++i)
  {
    pthread_join(cons[i],NULL);
    pthread_join(prod[i],NULL);
  }


  pthread_mutex_destroy(&mutex1);

  return 0;
}
```

## 11.写多线程时应注意

1. 先考虑代码的核心逻辑（先实现）
2. 考虑核心逻辑中是否访问临界资源或者说执行临界区代码，如果有就需要保持互斥
3. 考虑线程之间是否需要同步

原文地址：https://zhuanlan.zhihu.com/p/381191748

作者：linux

# 【NO.517】百度 C++ 工程师的那些极限优化（内存篇）

## 1.背景

在百度看似简简单单的界面后面，是遍布全国的各个数据中心里，运转着的海量 C++服务。对 C++的重度应用是百度的一把双刃剑，学习成本陡峭，指针类错误定位难、扩散性广另很多开发者望而却步。然而在另一方面，语言层引入的额外开销低，对底层能力可操作性强，又能够为追求极致性能提供优异的实践环境。

因此，对百度的 C++工程师来说，掌握底层特性并加以利用来指导应用的性能优化，就成了一门必要而且必须的技能。久而久之，百度工程师就将这种追求极致的性能优化，逐渐沉淀成了习惯，甚至形成了对技术的信仰。下面我们就来盘点和分享一些，在性能优化的征途上，百度 C++工程师积累下来的理论和实践，以及那些为了追求极致，所发掘的『奇技淫巧』。

## 2.重新认识性能优化

作为程序员，大家或多或少都会和性能打交道，尤其是以 C++为主的后端服务工程师，但是每个工程师对性能优化概念的理解在细节上又是千差万别的。下面先从几个优化案例入手，建立一个性能优化相关的感性认识，之后再从原理角度，描述一下本文所讲的性能优化的切入角度和方法依据。

### 2.1 从字符串处理开始

#### 2.1.1 string as a buffer

为了调用底层接口和集成一些第三方库能力，在调用界面层，会存在对 C++字符串和 C 风格字符串的交互场景，典型是这样的：

```text
size_t some_c_style_api(char* buffer, size_t size);
void some_cxx_style_function(std::string& result) {
    // 首先扩展到充足大小
    result.resize(estimate_size);
    // 从c++17开始，string类型支持通过data取得非常量指针
    auto acture_size = some_c_style_api(result.data(), result.size());
    // 最终调整到实际大小
    result.resize(acture_size);
}
```

这个方法存在一个问题，就是在首次 resize 时，string 对 estimate_size 内的存储区域全部进行了 0 初始化。但是这个场景中，实际的有效数据其实是在 some_c_style_api 内部被写入的，所以 resize 时的初始化动作其实是冗余的。在交互 buffer 的 size 较大的场景，例如典型的编码转换和压缩等操作，这次冗余的初始化引入的开销还是相当可观的。

可以这样编码：

```text
size_t some_c_style_api(char* buffer, size_t size);
void some_cxx_style_function(std::string& result) {
    auto* buffer = babylon::resize_uninitialized(result, estimate_size);
    auto acture_size = some_c_style_api(buffer, result.size());
    result.resize(acture_size);
}
```

#### 2.1.2 split string

实际业务中，有一个典型场景是一些轻 schema 数据的解析，比如一些标准分隔符，典型是'_'或者'\t'，简单分割的分列数据（这在日志等信息的粗加工处理中格外常见）。由于场景极其单纯，可能的算法层面优化空间一般认为较小，而实际实现中，这样的代码是广为存在的：

```text
std::vector<std::string> tokens;
// boost::split
boost::split(token, str, [] (char c) {return c == '\t';});
// absl::StrSplit
for (std::string_view sv : absl::StrSplit(str, '\t')) {
    tokens.emplace_back(sv);
}
// absl::StrSplit no copy
for (std::string_view sv : absl::StrSplit(str, '\t')) {
    direct_work_on_segment(sv);
}
```

boost 版本广泛出现在新工程师的代码中，接口灵活，流传度高，但是实际业务中效率其实并不优秀，例如和 google 优化过的 absl 相比，其实有倍数级的差距。尤其如果工程师没有注意进行单字符优化的时候（直接使用了官方例子中的 is_any_of），甚至达到了数量级的差距。进一步地，如果联动思考业务形态，一般典型的分割后处理是可以做到零拷贝的，这也可以进一步降低冗余拷贝和大量临时对象的创建开销。

![img](https://pic4.zhimg.com/80/v2-60dbb2c6f474db3f25990d16c3461d33_720w.webp)

最后，再考虑到百度当前的内部硬件环境有多代不同型号的 CPU，进一步改造 spilt 显式使用 SIMD 优化，并自适应多代向量指令集，可以取得进一步的性能提升。尤其是 bmi 指令加速后，对于一个 SIMD 步长内的连续分隔符探测，比如密集短串场景，甚至可以取得数量级的性能提升。

最终在百度，我们可以这样编码实现：

```text
babylon::split([] (std::string_view sv) {
  direct_work_on_segment(sv);
}, str, '\t'};
```

#### 2.1.3 magic of protobuf

随着 brpc 在百度内部的广泛应用，protobuf 成为了百度内部数据交换的主流方式，解析、改写、组装 protobuf 的代码在每个服务中几乎都会有一定的占比。尤其是近几年，进一步叠加了微服务化的发展趋势之后，这层数据交换边界就变得更加显著起来。

在有些场景下，例如传递并增加一个字段，或者从多个后端存储获取分列表达的数据合并后返回，利用标准的 C++API 进行反序列化、修改、再序列化的成本，相对于实际要执行的业务来说，额外带来的性能开销会显著体现出来。

举例来说，比如我们定义了这样的 message：

```text
message Field {


    bytes column = 1;
    bytes value = 2;
};
message Record {
    bytes key = 1;
    repeated Field field = 2;
};
message Response {
    repeated Record record = 1;
    bytes error_message = 2;
};
```

我们设想一个场景，一个逻辑的 record 分散于多个子系统，那么我们需要引入一个 proxy 层，完成多个 partial record 的 merge 操作，常规意义上，这个 merge 动作一般是这样的：

```text
one_sub_service.query(&one_controller, &request, &one_sub_response, nullptr);
another_sub_service.query(&another_controller, &request, &another_sub_response, nullptr);
...
for (size_t i = 0; i < record_size; ++i) {
    final_response.mutable_record(i).MergeFrom(one_sub_response.record(i));
    final_response.mutable_record(i).MergeFrom(another_sub_response.record(i));
    ...
}
```

对于一个轻量级 proxy 来说，这一层反复对后端的解析、合并、再序列化引入的成本，就会相对凸现出来了。进一步的消除，就要先从 protobuf 的本质入手。

![img](https://pic4.zhimg.com/80/v2-49573124c965abc4b5503b6a79ca2157_720w.webp)

protobuf 的根基先是一套公开标准的 wire format，其上才是支持多语言构造和解析的 SDK，因此尝试降低对解析和合并等操作的进一步优化，绕过 c++api，深入 wire format 层来尝试是一种可行且有效的手段。那么我们先来看一下一些 wire format 层的特性。

![img](https://pic2.zhimg.com/80/v2-312116bd7e35bd450a78b6cc47edef85_720w.webp)

即 message 的构成直接由内部包含的 field 的序列化结果堆砌而成，field 之间不存在分割点，在整个 message 外部，也不存在框架信息。基于这个特性，一些合并和修改操作可以在序列化的 bytes 结果上被低成本且安全地操作。而另一方面，message field 的格式和 string 又是完全一致的，也就是定义一个 message field，或者定义一个 string field 而把对应 message 序列化后存入，结果是等价的（而且这两个特性是被官方承诺的）。

结合这些特性，之前的合并操作在实现上我们改造为：

```text
message ProxyResponse {
    // 修改proxy侧的message定义，做到不深层解析
    repeated string record = 1;
    bytes error_message = 2;
};


one_sub_service.query(&one_controller, &request, &one_sub_response, nullptr);
another_sub_service.query(&another_controller, &request, &another_sub_response, nullptr);
...
for (size_t i = 0; i < record_size; ++i) {
    // 利用string追加替换message操作
    final_response.mutable_record(i).append(one_sub_response.record(i));
    final_response.mutable_record(i).append(another_sub_response.record(i));
    ...
}
```

在微服务搭的环境下，类似的操作可以很好地控制额外成本的增加。

### 2.2 回头来再看看性能优化

一般来讲，一个程序的性能构成要件大概有三个，即算法复杂度、IO 开销和并发能力。

![img](https://pic3.zhimg.com/80/v2-3966b2f19c7ff6f0bacd9ebab126ed22_720w.webp)

首要的影响因素是大家都熟悉的算法复杂度。一次核心算法优化和调整，可以对应用性能产生的影响甚至是代差级别的。例如 LSM Tree 对 No-SQL 吞吐的提升，又例如事件触发对 epoll 大并发能力的提升。然而正因为关注度高，在实际工程实现层面，无论是犯错几率，还是留下的优化空间，反而会大为下降。甚至极端情况下，可能作为非科研主导的工程师，在进行性能优化时鲜少遇到改良算法的场景，分析问题选择合适算法会有一定占比，但是可能范围也有限。

更多情况下需要工程师解决的性能问题，借用一句算法竞赛用语，用『卡常数』来形容可能更贴切。也就是采用了正确的合适的算法，但是因为没有采用体系结构下更优的实现方案，导致在 O(X)上附加了过大的常数项，进而造成的整体性能不足。虽然在算法竞赛中，卡常数和常数优化是出题人和解题人都不愿意大量出现的干扰项（因为毕竟是以核心算法本身为目标），但是转换到实际项目背景下，常数优化却往往是性能优化领域的重要手段。

现在我们再来回顾一下上面引出问题的三个优化案例。可以看到，其中都不包含算法逻辑本身的改进，但是通过深入利用底层（比如依赖库或指令集）特性，依然能够取得倍数甚至数量级的优化效果。这和近些年体系结构变得越发复杂有很大关联，而这些变化，典型的体现场景就是 IO 和并发。并发的优化，对于工程经验比较丰富的同学应该已经不陌生了，但是关于 IO，尤其是『内存 IO』可能需要特别说明一下。

与代码中显示写出的 read/write/socket 等设备 IO 操作不同，存储系统的 IO 很容易被忽略，因为这些 IO 透明地发生在普通 CPU 指令的背后。先列举 2009 年 Jeff Dean 的一个经典讲座中的一页数字。

![img](https://pic2.zhimg.com/80/v2-0f5bcd67409924f0d030ef4debd347cd_720w.webp)

虽然已经是十多年前的数据，但是依然可以看出，最快速的 L1 cache 命中和 Main memory 访问之间，已经拉开了多个数量级的差距。这些操作并不是在代码中被显式控制的，而是 CPU 帮助我们透明完成的，在简化编程难度的同时，却也引入了问题。也就是，如果不能良好地适应体系结构的特性，那么看似同样的算法，在常数项上就可能产生数量级的差异。而这种差异因为隐蔽性，恰恰是最容易被新工程师所忽略的。下面，我们就围绕内存访问这个话题，来盘点一下百度 C++工程师的一些『常数优化』。

## 3.从内存分配开始

要使用内存，首先就要进行内存分配。进入了 c++时代后，随着生命周期管理的便捷化，以及基于 class 封装的各种便捷容器封装的诞生，运行时的内存申请和释放变得越来越频繁。但是因为地址空间是整个进程所共享的一种资源，在多核心系统中就不得不考虑竞争问题。有相关经验的工程师应该会很快联想到两个著名的内存分配器，tcmalloc 和 jemalloc，分别来自 google 和 facebook。下面先来对比一下两者的大致原理。

### 3.1 先看看 tcmalloc 和 jemalloc

![img](https://pic1.zhimg.com/80/v2-310bd7d9a333c825a647c728905466d0_720w.webp)

针对多线程竞争的角度，tcmalloc 和 jemalloc 共同的思路是引入了线程缓存机制。通过一次从后端获取大块内存，放入缓存供线程多次申请，降低对后端的实际竞争强度。而典型的不同点是，当线程缓存被击穿后，tcmalloc 采用了单一的 page heap（简化了中间的 transfer cache 和 central cache，他们也是全局唯一的）来承载，而 jemalloc 采用了多个 arena（默认甚至超过了服务器核心数）来承载。因此和网上流传的主流评测推导原理一致，在线程数较少，或释放强度较低的情况下，较为简洁的 tcmalloc 性能稍胜过 jemalloc。而在核心数较多、申请释放强度较高的情况下，jemalloc 因为锁竞争强度远小于 tcmalloc，会表现出较强的性能优势。

一般的评测到这里大致就结束了，不过我们可以再想一步，如果我们愿意付出更多的内存到 cache 层，将后端竞争压力降下来，那么是否 tcmalloc 依然可以回到更优的状态呢？如果从上面的分析看，应该是可以有这样的推论的，而且近代服务端程序的瓶颈也往往并不在内存占用上，似乎是一个可行的方案。

不过实际调整过后，工程师就会发现，大多数情况下，可能并不能达到预期效果。甚至明明从 perf 分析表现上看已经观测到竞争开销和申请释放动作占比很小了，但是整个程序表现依然不尽如人意。

这实际上是内存分配连续性的对性能影响的体现，即线程缓存核心的加速点在于将申请批量化，而非单纯的降低后端压力。缓存过大后，就会导致持续反复的申请和释放都由缓存承担，结果是缓存中存放的内存块地址空间分布越来越零散，呈现一种洗牌效果。

![img](https://pic3.zhimg.com/80/v2-68f9cb81e39980fcb10e0fdd782bc4ca_720w.webp)

体系结构的缓存优化，一般都是以局部性为标准，也就是说，程序近期访问的内存附近，大概率是后续可能被访问的热点。因此，如果程序连续申请和访问的内存呈跳跃变化，那么底层就很难正确进行缓存优化。体现到程序性能上，就会发现，虽然分配和释放动作都变得开销很低了，但是程序整体性能却并未优化（因为真正运行的算法的访存操作常数项增大）。

### 3.2 那么理想的 malloc 模型是什么？

通过前面的分析，我们大概得到了两条关于 malloc 的核心要素，也就是竞争性和连续性。那么是否 jemalloc 是做到极致了呢？要回答这个问题，还是要先从实际的内存使用模型分析开始。

![img](https://pic2.zhimg.com/80/v2-7d6d05ae568b722f8011ca5ab4ac895d_720w.webp)

这是一个很典型的程序，核心是一组持续运行的线程池，当任务到来后，每个线程各司其职，完成一个个的任务。在 malloc 看来，就是多个长生命周期的线程，随机的在各个时点发射 malloc 和 free 请求。如果只是基于这样的视图，其实 malloc 并没有办法做其他假定了，只能也按照基础局部性原理，给一个线程临近的多次 malloc，尽量分配连续的地址空间出来。同时利用线程这一概念，将内存分区隔离，减少竞争。这也就是 tcmalloc 和 jemalloc 在做的事情了。

但是内存分配这件事和程序的边界就只能在这里了吗？没有更多的业务层输入，可以让 malloc 做的更好了吗？那么我们再从业务视角来看一下内存分配。

![img](https://pic2.zhimg.com/80/v2-19db76bc403808003d77608819ed6841_720w.webp)

微服务、流式计算、缓存，这几种业务模型几乎涵盖了所有主流的后端服务场景。而这几种业务对内存的应用有一个重要的特征，就是拥有边界明确的生命周期。回退到早期的程序设计年代，其实 server 设计中每个请求单独一个启动线程处理，处理完整体销毁是一个典型的方案。即使是使用线程池，一个请求接受后从头到尾一个线程跟进完成也是持续了相当长时间的成熟设计。

而针对这种早期的业务模型，其实 malloc 是可以利用到更多业务信息的，例如线程动态申请的内存，大概率后续某个时点会全部归还，从 tcmalloc 的线程缓存调整算法中也可以看出对这样那个的额外信息其实是专门优化过的。

但是随着新型的子任务级线程池并发技术的广泛应用，即请求细分为多个子任务充分利用多核并发来提升计算性能，到 malloc 可见界面，业务特性几乎已经不复存在。只能看到持续运行的线程在随机 malloc 和 free，以及大量内存的 malloc 和 free 漂移在多个线程之间。

那么在这样 job 化的背景下，怎样的内存分配和释放策略能够在竞争性和局部性角度工作的更好呢？下面我们列举两个方案。

#### 3.2.1 job arena

![img](https://pic1.zhimg.com/80/v2-4b9c52ed1cb4611f5968a31562028520_720w.webp)

第一种是基础的 job arena 方案，也就是每个 job 有一个独立的内存分配器，job 中使用的动态内存注册到 job 的 arena 中。因为 job 生命周期明确，中途释放的动态内存被认为无需立即回收，也不会显著增大内存占用。在无需考虑回收的情况下，内存分配不用再考虑分块对齐，每个线程内可以完全连续。最终 job 结束后，整块内存直接全部释放掉，大幅减少实际的竞争发生。

显而易见，因为需要感知业务生命周期，malloc 接口是无法获得这些信息并进行支持的，因此实际会依赖运行时使用的容器能够单独暴露内存分配接口出来。幸运的是，在 STL 的带动下，现实的主流容器库一般都实现了 allocator 的概念，尽管细节并不统一。

例如重度使用的容器之一 protobuf，从 protobuf 3.x 开始引入了 Arena 的概念，可以允许 Message 将内存结构分配通过 Arena 完成。可惜直到最新的 3.15 版本，string field 的 arena 分配依然没有被官方支持。[https://github.com/protocolbuffers/protobuf/issues/4327](https://link.zhihu.com/?target=https%3A//github.com/protocolbuffers/protobuf/issues/4327)

但是，因为 string/bytes 是业务广为使用的类型，如果脱离 Arena 的话，实际对连续性的提升就会大打折扣。因此在百度，我们内部维护了一个 ArenaString 的 patch，重现了 issue 和注释中的表达，也就是在 Arena 上分配一个『看起来像』string 的结构。对于读接口，因为和 string 的内存表达一致，可以直接通过 const string&呈现。对于 mutable 接口，会返回一个替代的 ArenaString 包装类型，在使用了 auto 技术的情况下，几乎可以保持无缝切换。

另外一个重度使用的容器就是 STL 系列了，包括 STL 自身实现的容器，以及 boost/tbb/absl 中按照同类接口实现的高级容器。从 C++17 开始，STL 尝试将之前混合在 allocator 中的内存分配和实例构造两大功能进行拆分，结果就是产生了 PMR（Polymorphic Memory Resouce）的概念。在解耦了构造器和分配器之后，程序就不再需要通过修改模板参数中的类型，来适应自己的内存分配方法了。其实 PMR 自身也给出了一种连续申请，整体释放的分配器实现，即 monotonic_buffer_resource，但是这个实现是非线程安全的。

结合上面两个内存分配器的概念，在实际应用中，我们利用线程缓存和无锁循环队列（降低竞争），整页获取零散供给（提升连续）实现了一个 SwissMemoryResource，通过接口适配统一支持 STL 和 protobuf 的分配器接口。最终通过 protocol 插件集成到 brpc 中，在百度，我们可以如下使用：

```text
babylon::ReusableRPCProtocol::register_protocol();
::baidu::rpc::ServerOptions options;
options.enabled_protocols = "baidu_std_reuse";


class SomeServiceImpl : public SomeService {
public:
    // request和response本身采用了请求级别的memory resource来分配
    virtual void some_method(google::protobuf::RpcController* controller,
            const SomeRequest* request,
            SomeResponse* response,
            google::protobuf::Closure* done) {
        baidu::rpc::ClosureGuard guard(done);
        // 通过转换到具体实现来取得MemoryResource
        auto* closure = static_cast<babylon::ReusableRPCProtocol::Closure*>(done);
        auto& resource = closure->memory_resource();
        // 做一些请求级别的动态容器
        std::pmr::vector<std::pmr::string> tmp_vector(&resource);
        google::protobuf::Arena::CreateMessage<SomeOtherMessage>(&(Arena&)resource);
        ...
        // done->Run时请求级内存整体释放
    }
};
```

#### 3.2.2 job reserve

![img](https://pic2.zhimg.com/80/v2-ffa508ef2aadc2fa4ed775e3d09cf441_720w.webp)

更复杂一些的是 job reserve 方案，在 job arena 的基础上，结合了 job 结束后不析构中间结构，也不释放内存，转而定期进行紧凑重整。这就进一步要求了中间结构是可以在保留内存的情况下完成重置动作的，并且能够进行容量提取，以及带容量重新构建的功能。这里用 vector<string>为例解释一下：

![img](https://pic4.zhimg.com/80/v2-a785dea5afadfdd13963efae0795b86b_720w.webp)

和典型的 vector 处理主要不同点是，在 clear 或者 pop_back 等操作缩减大小之后，内容对象并没有实际析构，只是清空重置。

因此，再一次用到这个槽位的时候，可以直接拿到已经构造好的元素，而且其 capacity 之内的内存也依然持有。可以看到反复使用同一个实例，容器内存和每个元素自身的 capacity 都会逐渐趋向于饱和值，反复的分配和构造需求都被减少了。了解过 protobuf 实现原理的工程师可以对照参考，这种保留实例的 clear 动作，也是 protobuf 的 message 锁采用的方法。

不过关注到之前提过局部性的工程师可能会发现，尽管分配需求降低了，但是最终饱和态的内存分布在连续性上仍不理想，毕竟中途的动态分配是按需进行，而未能参考局部性了。因此容器还需要支持一个动作，也就是重建。



![img](https://pic2.zhimg.com/80/v2-2c1c3b61805e4ae1e0467dfd9aa33c89_720w.webp)



也就是，当重复利用多次，发生了较多临时申请之后，需要能够提取出当前的容量 schema，在新的连续空间中做一次原样重建，让内存块重新回归连续。

### 3.3 总结一下内存分配

通过分析 malloc 的性能原理，引入这两种细粒度的内存分配和管理方案，可以在更小的竞争下，得到更好的内存连续性。

在实测中，简单应用做到 job arena 一般就可以取得大部分性能收益，一般能够达到倍数级提升，在整体服务角度也能够产生可观测的性能节省。而 job reserve，虽然可以获得进一步地性能提升，但一方面是因为如果涉及非 protobuf 容器，需要实现自定义的 schema 提取和重建接口，另一方面趋于饱和的 capacity 也会让内存使用增大一些。引入成本会提高不少，因此实际只会在性能更为紧要的环节进行使用。

## 4.再来看看内存访问

内存分配完成后，就到了实际进行内存访问的阶段了。一般我们可以将访存需求拆解到两个维度，一个是单线程的连续访问，另一个是多个线程的共享访问。下面就分拆到两个部分来分别谈谈各个维度的性能优化方法。

### 4.1 顺序访存优化

一般来说，当我们要执行大段访存操作时，如果访问地址连续，那么实际效率可以获得提升。典型例如对于容器遍历访问操作，数组组织的数据，相比于比链表组织的数据，一般会有显著的性能优势。其实在内存分配的环节，我们引入的让连续分配（基本也会是连续访问）的空间地址连续性更强，也是出于这一目的。

那么下面我们先来看一看，连续性的访问产生性能差异的原理是什么。

![img](https://pic2.zhimg.com/80/v2-e3b1b8973c5d5e1f16e2fbb47a9ab189_720w.webp)

这里以 Intel CPU 为例来简要描述一下预取过程。详见：[https://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf](https://link.zhihu.com/?target=https%3A//www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf)当硬件监测到连续地址访问模式出现时，会激活多层预取器开始执行，参考当前负载等因素，将预测将要访问的数据加载到合适的缓存层级当中。这样，当后续访问真实到来的时候，能够从更近的缓存层级中获取到数据，从而加速访问速度。因为 L1 容量有限，L1 的硬件预取步长较短，加速目标主要为了提升 L2 到 L1，而 L2 和 LLC 的预取步长相对较长，用于将主存提升到 cache。

在这里局部性概念其实充当了软硬件交互的某种约定，因为程序天然的访问模式总有一些局部性，硬件厂商就通过预测程序设计的局部性，来尽力加速这种模式的访问请求，力求做到通用提升性能。而软件设计师，则通过尽力让设计呈现更多的局部性，来激活硬件厂商设计的优化路径，使具体程序性能得到进一步优化。某种意义上讲，z 不失为一个相生相伴的循环促进。

这里通过一个样例来验证体现一下如何尊重局部性，以及局部性对程序的影响。

```text
// 设计一块很大的存储区域用于存储固定维度的浮点向量
// vecs中存储每个浮点向量的起始地址
std::vector<float> large_memory_buffer;
std::vector<float*> vecs;


std::shuffle(vecs.begin(), vecs.end(), random_engine);


for (size_t i = 0; i < vecs.size(); ++i) {
    __builtin_prefetch(vecs[i + step]);
    dot_product(vecs[i], input);
}
```

这是一个推荐/搜索系统中常见的内积打分场景，即通过向量计算来进行大规模打分。同样的代码，依据 shuffle 和 prefetch 存在与否，产生类似如下的表现：

1. shuffle & no prefetch：44ms
2. shuffle & prefetch：36ms
3. shuffle & no prefetch：13ms
4. shuffle & prefetch：12ms

从 1 和 3 的区别可以看出，完全相同的指令，在不同的访存顺序下存在的性能差距可以达到倍数级。而从 1 和 2 的区别可以看出，手动添加预取操作后，对性能有一定改善，预期更精细地指导预取步长和以及 L1 和 L2 的分布还有改善空间。不过指令执行周期和硬件效率很难完备匹配，手动预取一般用在无法改造成物理连续的场景，但调参往往是一门玄学。最后 3 和 4 可以看出，即使连续访存下，预取依然有一些有限的收益，推测和硬件预取无法跨越页边界造成的多次预测冷启动有关，但是影响已经比较微弱了。

最具备指导意义的可能就是类似这个内积打分的场景，有时为了节省空间，工程师会将程序设计为，从零散的空间取到向量指针，并组成一个数组或链表系统来管理。天然来讲，这样节省了内存的冗余，都引用向一份数据。但是如果引入一些冗余，将所需要的向量数据一同拷贝构成连续空间，对于检索时的遍历计算会带来明显的性能提升。

### 4.2 并发访问优化

提到并发访问，可能要先从一个概念，缓存行（cache line）说起。

为了避免频繁的主存交互，其实缓存体系采用了类似 malloc 的方法，即划分一个最小单元，叫做缓存行（主流 CPU 上一般 64B），所有内存到缓存的操作，以缓存行为单位整块完成。

例如对于连续访问来说第一个 B 的访问就会触发全部 64B 数据都进入 L1，后续的 63B 访问就可以直接由 L1 提供服务了。所以并发访问中的第一个问题就是要考虑缓存行隔离，也就是一般可以认为，位于不同的两个缓存行的数据，是可以被真正独立加载/淘汰和转移的（因为 cache 间流转的最小单位是一个 cache line）。

典型的问题一般叫做 false share 现象，也就是不慎将两个本无竞争的数据，放置在一个缓存行内，导致因为体系结构的原因，引入了『本不存在的竞争』。这点在网上资料比较充足，例如 brpc 和 disruptor 的设计文档都比较详细地讲解了这一点，因此这里就不再做进一步的展开了。

### 4.3 那先来聊聊缓存一致性

排除了 false share 现象之后，其余就是真正的共享问题了，也就是确实需要位于同一个缓存行内的数据（往往就是同一个数据），多个核心都要修改的场景。由于在多核心系统中 cache 存在多份，因此就需要考虑这多个副本间一致性的问题。这个一致性一般由一套状态机协议保证（MESI 及其变体）。

![img](https://pic1.zhimg.com/80/v2-e9637ff99f2b2995960e9337205178e8_720w.webp)

大体是，当竞争写入发生时，需要竞争所有权，未获得所有权的核心，只能等待同步到修改的最新结果之后，才能继续自己的修改。这里要提一下的是有个流传甚广的说法是，因为缓存系统的引入，带来了不一致，所以引发了各种多线程可见性问题。

这么说其实有失偏颇，MESI 本质上是一个『一致性』协议，也就是遵守协议的缓存系统，其实对上层 CPU 多个核心做到了顺序一致性。比如对比一下就能发现，缓存在竞争时表现出来的处理动作，其实和只有主存时是一致的。

![img](https://pic4.zhimg.com/80/v2-cca6d1c6fbcfb006f0d51a784c43636b_720w.webp)

只是阻塞点从竞争一个物理主存单元的写入，转移到了虽然是多个缓存物理单元，但是通过协议竞争独占上。不过正因为竞争阻塞情况并没有缓解，所以 cache 的引入其实搭配了另一个部件也就是写缓冲（store buffer）。

注：写缓存本身引入其实同时收到乱序执行的驱动。

![img](https://pic4.zhimg.com/80/v2-1f7757f8e937cef91f8c707ab87c0c2f_720w.webp)

写缓冲的引入，真正开始带来的可见性问题。

以 x86 为例，当多核发生写竞争时，未取得所有权的写操作虽然无法生效到缓存层，但是可以在改为等待在写缓冲中。而 CPU 在一般情况下可以避免等待而先开始后续指令的执行，也就是虽然 CPU 看来是先进行了写指令，后进行读指令，但是对缓存而言，先进行的是读指令，而写指令被阻塞到缓存重新同步之后才能进行。要注意，如果聚焦到缓存交互界面，整体依然是保证了顺序一致，但是在指令交互界面，顺序发生了颠倒。这就是典型的 StoreLoad 乱序成了 LoadStore，也是 x86 上唯一的一个乱序场景。而针对典型的 RISC 系统来说（arm/power），为了流水线并行度更高，一般不承诺写缓冲 FIFO，当一个写操作卡在写缓冲之后，后续的写操作也可能被先处理，进一步造成 StoreStore 乱序。

写缓冲的引入，让竞争出现后不会立即阻塞指令流，可以容忍直到缓冲写满。但因为缓存写入完成需要周知所有 L1 执行作废操作完成，随着核心增多，会出现部分 L1 作废长尾阻塞写缓冲的情况。因此一些 RISC 系统引入了进一步的缓冲机制。

![img](https://pic3.zhimg.com/80/v2-1f00fa2e9a97fbebf67567549ce78fca_720w.webp)

进一步的缓冲机制一般叫做失效队列，也就是当一个写操作只要将失效消息投递到每个 L1 的失效队列即视为完成，失效操作长尾不再影响写入。这一步改动甚至确实地部分破坏了缓存一致性，也就是除非一个核心等待当前失效消息排空，否则可能读取到过期数据。

到这里已经可以感受到，为了对大量常规操作进行优化，近代体系结构设计中引入了多个影响一致性的机制。但是为了能够构建正确的跨线程同步，某些关键节点上的一致性又是必不可少的。

因此，配套的功能指令应运而生，例如 x86 下 mfence 用于指导后续 load 等待写缓冲全部生效，armv8 的 lda 用于确保后续 load 等待 invalid 生效完成等。这一层因为和机型与指令设计强相关，而且指令的配套使用又能带来多种不同的内存可见性效果。这就大幅增加了工程师编写正确一致性程序的成本，而且难以保证跨平台可移植。于是就到了标准化发挥作用的时候了，这个关于内存一致性领域的标准化规范，就是内存序（memory order）。

### 4.4 再谈一谈 memory order

作为一种协议机制，内存序和其他协议类似，主要承担了明确定义接口层功能的作用。体系结构专家从物理层面的优化手段中，抽象总结出了多个不同层级的逻辑一致性等级来进行刻画表达。这种抽象成为了公用边界标准之后，硬件和软件研发者就可以独立开展各自的优化工作，而最终形成跨平台通用解决方案。

对于硬件研发者来说，只要能够最终设计一些特定的指令或指令组合，支持能够实现这些内存序规范的功能，那么任意的设计扩展原理上都是可行的，不用考虑有软件兼容性风险。同样，对于软件研发者来说，只要按照标准的逻辑层来理解一致性，并使用正确的内存序，就可以不用关注底层平台细节，写出跨平台兼容的多线程程序。

内存序在官方定义里，是洋洋洒洒一大篇内容，为了便于理解，下面从开发程序须知角度，抽出一些简洁精炼的概念（虽不是理论完备的）来辅助记忆和理解。

首先来看看，内存序背后到底发生了啥。

```text
int payload = 0;
int flag = 0;
void normal_writer(int i) {
  payload = flag + i;
  flag = 1;
}
int normal_reader() {
  while (flag == 0) {
  }
  return payload;
}
```

![img](https://pic3.zhimg.com/80/v2-adfa511ffb7785f845d260699253ad8a_720w.webp)

在这个样例中可以看到，在编译层，默认对于无关指令，会进行一定程度的顺序调整（不影响正确性的前提下）。另一方面，编译器默认可以假定不受其他线程影响，因此同一个数据连续的多次内存访问可以省略。

下面看一下最基础的内存序等级，relaxed。

```text
int payload = 0;
std::atomic<int> flag {0};
void relaxed_writer(int i) {
    payload = flag.load(std::memory_order_relaxed) + i;
    flag.store(1, std::memory_order_relaxed);
}
int relaxed_reader() {
    while (flag.load(std::memory_order_relaxed) == 0) {
    }
    return payload;
}
```

![img](https://pic2.zhimg.com/80/v2-bd2b61bbc56bfa84b405c45a65d393f5_720w.webp)

在使用了基础的内存序等级 relaxed 之后，编译器不再假设不受其他线程影响，每个循环都会重新加载 flag。另外可以观测到 flag 和 payload 的乱序被恢复了，不过原理上 relaxed 并不保证顺序，也就是这个顺序并不是一个编译器的保证承诺。总体来说，relaxed 等级和普通的读写操作区别不大，只是保证了对应的内存访问不可省略。

更进一步的内存序等级是 consume-release，不过当前没有对应的实现案例，一般都被默认提升到了下一个等级，也就是第一个真实有意义的内存序等级 acquire-release。先从原理上讲，一般可以按照满足条件/给出承诺的方式来简化理解，即：

要求：对同一变量 M 分别进行写（release）A 和读（acquire）B，B 读到了 A 写入的值。承诺：A 之前的所有其他写操作，对 B 之后的读操作可见。实际影响：涉及到的操作不会发生穿越 A/B 操作的重排；X86：无额外指令；ARMv8：A 之前排空 store buffer，B 之后排空 invalid queue，A/B 保序；ARMv7&Power：A 之前全屏障，B 之后全屏障。

```text
int payload = 0;
std::atomic<int> flag {0};
void release_writer(int i) {
    payload = flag.load(std::memory_order_relaxed) + i;
    flag.store(1, std::memory_order_release);
}
int acquire_reader() {
    while (flag.load(std::memory_order_acquire) == 0) {
    }
    return payload;
}
```

![img](https://pic3.zhimg.com/80/v2-d82cedbdac7a0214a3ab8b2465d5862e_720w.webp)



![img](https://pic4.zhimg.com/80/v2-f7af0cea9723e975c2e073f1b3022bc7_720w.webp)

由于 x86 默认内存序不低于 acquire-release，这里用 ARMv8 汇编来演示效果。可以看出对应指令发生了替换，从 st/ld 变更到了 stl/lda，从而利用 armv8 的体系结构实现了相应的内存序语义。

再进一步的内存序，就是最强的一级 sequentially-consistent，其实就是恢复到了 MESI 的承诺等级，即顺序一致。同样可以按照满足条件/给出承诺的方式来简化理解，即：

- 要求：对两个变量 M，N 的（Sequentially Consistent）写操作 Am，An。在任意线程中，通过（Sequentially Consistent）的读操作观测到 Am 先于 An。
- 承诺：在其他线程通过（Sequentially Consistent）的读操作 B 也会观测到 Am 先于 An。
- 实际影响：

1. X86：Am 和 An 之后清空 store buffer，读操作 B 无额外指令；
2. ARMv8：Am 和 An 之前排空 store buffer， B 之后排空 invalid queue，A/B 保序；
3. ARMv7：Am 和 An 前后全屏障，B 之后全屏障；
4. POWER：Am 和 An 前全屏障，B 前后全屏障。

值得注意的是，ARMv8 开始，特意优化了 sequentially-consistent 等级，省略了全屏障成本。推测是因为顺序一致在 std::atomic 实现中作为默认等级提供，为了通用意义上提升性能做了专门的优化。

### 4.5 理解 memory order 如何帮助我们

先给出一个基本测试的结论，看一下一组对比数据：

1. 多线程竞争写入近邻地址 sequentially-consistent：0.71 单位时间
2. 多线程竞争写入近邻地址 release：0.006 单位时间
3. 多线程竞争写入 cache line 隔离地址 sequentially-consistent：0.38 单位时间
4. 多线程竞争写入 cache line 隔离地址 release：0.02 单位时间

这里可以看出，做 cache line 隔离，对于 sequentially-consistent 内存序下，有一定的收益，但是对 release 内存序，反而有负效果。这是由于 release 内存序下，因为没有强内存屏障，写缓冲起到了竞争缓解的作用。而在充分缓解了竞争之后，因为 cache line 隔离引入了相同吞吐下更多 cache line 的传输交互，反而开销变大。

在这个信息指导下，我们在实现无锁队列时，采用了循环数组 + 分槽位版本号的模式来实现。因为队列操作只需要 acquire-release 等级，分槽位版本号间无需采用 cache line 隔离模式设计，整体达到了比较高的并发性能。

![img](https://pic1.zhimg.com/80/v2-0ca0d16328b63c0fab806ed85fb9cc00_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/375742507

作者：linux

# 【NO.518】malloc内存分配过程详解

起malloc，但凡对C/C++有点基础的人在编写代码的时候都用过。我们调用malloc接口分配一段连续的内存空间，不使用时使用free可以释放这段内存空间。这些我们都已经比较的熟悉了。但是你知道malloc背后的调用机制吗？

C语言程序员都知道，malloc只是C语言库标准提供的一个普通函数，我们实现的malloc和库函数比起来效率要低很多，但是可以通过编写一个简单的malloc来体现C库的精髓，我们实现的malloc和库的实现原理上市一致的。

## 1.malloc的定义：

根据标准C的定义，malloc的函数原型是这样的：

```text
<span style="font-family:Microsoft YaHei;font-size:14px;">void* malloc(size_t size);</span>
```

函数要求如下：

- malloc函数分配的内存大小至少为size参数所指定的字节数。
- malloc的返回值是一个void类型的指针，我们必须强制转化为我们需要的类型的指针。
- 多次调用malloc所分配的地址不能有重叠部分，除非某次molloc所分配的地址被free释放掉了。
- malloc应该尽快的完成内存额分配并且返回。
- 实现malloc的同时实现calloc和realloc和free。

如果是子啊Linux环境下，可以使用

```text
<span style="font-size:14px;">man malloc
</span>
```

查看malloc的具体定义。

## 2.Linux的内存管理

**1.虚拟内存地址与物理内存地址的关系：**

现代操作系统在处理内存地址时普遍的采用虚拟内存地址技术，什么事虚拟内存技术呢？

这种技术使每个进程“仿佛独享”一块2N字节的内存（N是机器的位数），例如在64位的操作系统下，每个进程的虚拟内存空间是264B。这种虚拟内存空间的作用是简化程序的编写并且方便操作系统对进程之间的隔离管理。

虚拟内存技术是由MMU和页表构成的，MMU是一种映射算法，它从虚拟内存地址映射到物理内存地址上，单位是页

**2.什么是页表？**

在现代操作系统中，不管是虚拟内存地址还是物理内存地址，都是以页尾单位管理的，而不是大家以为的字节。（一个内存页是一段固定的地址，典型的内存页的大小是4K）。所以内存地址可以分为页号和页内偏移量

![img](https://pic1.zhimg.com/80/v2-b40413c04633b6ae417da1418b699414_720w.webp)

## 3.Linux进程级的内存管理

首先，我们可以了解一下一个进程的内核空间：

![img](https://pic2.zhimg.com/80/v2-fd6e55efd5e96b07e925b97ead89e729_720w.webp)

可以看到一个进程地址空间的主要成分为：

- 正文：这是整个用户空间的最低地址部分，存放的是指令（也就是程序所编译成的可执行机器码）
- 初始化数据段：这里存放的是初始化过的全局变量
- 未初始化数据段：这里存放的是未初始化的全局变量
- Heap：堆，这是我们本文重点关注的地方，堆自低地址向高地址增长，后面要讲到的brk相关的系统调用就是从这里分配内存
- Stack：这是栈区域，自高地址向低地址增长
- 命令行参数和环境变量：用户调用的最底层。

我们都知道，在malloc分配空间时是在Heap上分配的，实质上， Linux维护一个break指针，这个指针指向堆空间的某个地址。从堆起始地址到break之间的地址空间为映射好的，可以供进程访问；而从break往上，是未映射的地址空间，如果访问这段空间则程序会报错。

![img](https://pic4.zhimg.com/80/v2-35221ddb47bb148a1ffbbc914e84ea87_720w.webp)

由上文知道，要增加一个进程实际的可用堆大小，就需要将break指针向高地址移动。Linux通过brk和sbrk系统调用操作break指针。两个系统调用的原型如下：

```text
int brk(void *addr);
void *sbrk(intptr_t increment);
```

brk将break指针直接设置为某个地址，而sbrk将break从当前位置移动increment所指定的增量。brk在执行成功时返回0，否则返回-1并设置errno为ENOMEM；sbrk成功时返回break移动之前所指向的地址，否则返回(void *)-1。

一个小技巧是，如果将increment设置为0，则可以获得当前break的地址。

另外需要注意的是，由于Linux是按页进行内存映射的，所以如果break被设置为没有按页大小对齐，则系统实际上会在最后映射一个完整的页，从而实际已映射的内存空间比break指向的地方要大一些。但是使用break之后的地址是很危险的（尽管也许break之后确实有一小块可用内存地址）。



有了上面的知识，我们可以实现一个最简单的malloc（没什么用，像个玩具）

```text
/* 一个玩具malloc */
#include <sys/types.h>
#include <unistd.h>
void *malloc(size_t size)
{
    void *p;
    p = sbrk(0);
    if (sbrk(size) == (void *)-1)
        return NULL;
    return p;
}
```

这个malloc由于对所分配的内存缺乏记录，不便于内存释放，所以无法用于真实场景。

## 4.开始实现正式的malloc

一个方案是将堆内存空间以块（Block）的形式组织起来，每个块由meta区和数据区组成，meta区记录数据块的元信息（数据区大小、空闲标志位、指针等等），数据区是真实分配的内存区域，并且数据区的第一个字节地址即为malloc返回的地址。

以下是一个块的结构：

```text
typedef struct s_block *t_block;
struct s_block {
    size_t size;  /* 数据区大小 */
    t_block next; /* 指向下个块的指针 */
    int free;     /* 是否是空闲块 */
    int padding;  /* 填充4字节，保证meta块长度为8的倍数 */
    char data[1]  /* 这是一个虚拟字段，表示数据块的第一个字节，长度不应计入meta */
};
```

现在考虑如何在block链中查找合适的block。一般来说有两种查找算法：

- First fit：从头开始，使用第一个数据区大小大于要求size的块所谓此次分配的块
- Best fit：从头开始，遍历所有块，使用数据区大小大于size且差值最小的块作为此次分配的块

两种方法各有千秋，best fit具有较高的内存使用率（payload较高），而first fit具有更好的运行效率。这里我们采用first fit算法。

```text
* First fit */
t_block find_block(t_block *last, size_t size) {
    t_block b = first_block;
    while(b && !(b->free && b->size >= size)) {
        *last = b;
        b = b->next;
    }
    return b;
}
```

ind_block从frist_block开始，查找第一个符合要求的block并返回block起始地址，如果找不到这返回NULL。这里在遍历时会更新一个叫last的指针，这个指针始终指向当前遍历的block。这是为了如果找不到合适的block而开辟新block使用的。

如果现有block都不能满足size的要求，则需要在链表最后开辟一个新的block。这里关键是如何只使用sbrk创建一个struct：

```text
#define BLOCK_SIZE 24 /* 由于存在虚拟的data字段，sizeof不能正确计算meta长度，这里手工设置 */
 
t_block extend_heap(t_block last, size_t s) {
    t_block b;
    b = sbrk(0);
    if(sbrk(BLOCK_SIZE + s) == (void *)-1)
        return NULL;
    b->size = s;
    b->next = NULL;
    if(last)
        last->next = b;
    b->free = 0;
    return b;
}
```

First fit有一个比较致命的缺点，就是可能会让很小的size占据很大的一块block，此时，为了提高payload，应该在剩余数据区足够大的情况下，将其分裂为一个新的block：

```text
void split_block(t_block b, size_t s) {
t_block new;
new = b->data + s;
new->size = b->size - s - BLOCK_SIZE ;
new->next = b->next;
new->free = 1;
b->size = s;
b->next = new;
}
```

有了上面的代码，我们可以利用它们整合成一个简单但初步可用的malloc。注意首先我们要定义个block链表的头first_block，初始化为NULL；另外，我们需要剩余空间至少有BLOCK_SIZE + 8才执行分裂操作。

由于我们希望malloc分配的数据区是按8字节对齐，所以在size不为8的倍数时，我们需要将size调整为大于size的最小的8的倍数：

```text
size_t align8(size_t s) {
    if(s & 0x7 == 0)
        return s;
    return ((s >> 3) + 1) << 3;
}
#define BLOCK_SIZE 24
void *first_block=NULL;
```



```text
void *malloc(size_t size) {
    t_block b, last;
    size_t s;
    /* 对齐地址 */
    s = align8(size);
    if(first_block) {
        /* 查找合适的block */
        last = first_block;
        b = find_block(&last, s);
        if(b) {<pre name="code" class="cpp">         /* 如果可以，则分裂 */
            if ((b->size - s) >= ( BLOCK_SIZE + 8))
                split_block(b, s);
            b->free = 0;
        } else {
            /* 没有合适的block，开辟一个新的 */
            b = extend_heap(last, s);
            if(!b)
                return NULL;
        }
    } else {
        b = extend_heap(NULL, s);
        if(!b)
            return NULL;
        first_block = b;
    }
    return b->data;
}
```

原文地址：https://zhuanlan.zhihu.com/p/502710223

作者：linux

# 【NO.519】TCP BBR拥塞控制算法深度解析

正文之前，给出本文的图例：

![img](https://pic2.zhimg.com/80/v2-8d1be1e42cdc609b0084af3a592c0b81_720w.webp)

**BBR的组成**

bbr算法实际上非常简单，在实现上它由5部分组成：

![img](https://pic2.zhimg.com/80/v2-818364011901aa6f4c3ce80b67a110b1_720w.webp)

## 1.**BBR的组成**

**1.即时速率的计算**

计算一个即时的带宽bw，该带宽是bbr一切计算的基准，bbr将会根据当前的即时带宽以及其所处的pipe状态来计算pacing rate以及cwnd(见下文)，后面我们会看到，这个即时带宽计算方法的突破式改进是bbr之所以简单且高效的根源。计算方案按照标量计算，不再关注数据的含义。在bbr运行过程中，系统会跟踪当前为止最大的即时带宽。

**2.RTT的跟踪**

bbr之所以可以获取非常高的带宽利用率，是因为它可以非常安全且豪放地探测到带宽的最大值以及rtt的最小值，这样计算出来的BDP就是目前为止TCP管道的最大容量。bbr的目标就是达到这个最大的容量！这个目标最终驱动了cwnd的计算。在bbr运行过程中，系统会跟踪当前为止最小RTT。

**3.BBR状态机的维持**

bbr算法根据互联网的拥塞行为有针对性地定义了4中状态，即STARTUP，DRAIN，PROBE_BW，PROBE_RTT。bbr通过对上述计算的即时带宽bw以及rtt的持续观察，在这4个状态之间自由切换，相比之前的所有拥塞控制算法，其革命性的改进在于bbr拥塞算法不再跟踪系统的TCP拥塞状态机，而旨在用统一的方式来应对pacing rate和cwnd的计算，不管当前TCP是处在Open状态还是处在Disorder状态，抑或已经在Recovery状态，换句话说，bbr算法感觉不到丢包，它能看到的就是bw和rtt！

**4.结果输出-pacing rate和cwnd**

首先必须要说一下，bbr的输出并不仅仅是一个cwnd，更重要的是pacing rate。在传统意义上，cwnd是TCP拥塞控制算法的唯一输出，但是它仅仅规定了当前的TCP最多可以发送多少数据，它并没有规定怎么把这么多数据发出去，在Linux的实现中，如果发出去这么多数据呢？简单而粗暴，突发！忽略接收端通告窗口的前提下，Linux会把cwnd一窗数据全部突发出去，而这往往会造成路由器的排队，在深队列的情况下，会测量出rtt剧烈地抖动。

bbr在计算cwnd的同时，还计算了一个与之适配的pacing rate，该pacing rate规定cwnd指示的一窗数据的数据包之间，以多大的时间间隔发送出去。

**5.其它外部机制的利用-fq，rack等**

bbr之所以可以高效地运行且如此简单，是因为很多机制并不是它本身实现的，而是利用了外部的已有机制，比如下一节中将要阐述的它为什么在计算带宽bw时能如此放心地将重传数据也计算在内...

## 2.**带宽计算细节以及状态机**

**1.即时带宽的计算**

bbr作为一个纯粹的拥塞控制算法，完全忽略了系统层面的TCP状态，计算带宽时它仅仅需要两个值就够了：

**1).应答了多少数据，记为delivered；**

**2).应答1)中的delivered这么多数据所用的时间，记为interval_us。**

**将上述二者相除，就能得到带宽：**

**bw = delivered/interval_us**

非常简单！以上的计算完全是标量计算，只关注数据的大小，不关注数据的含义，比如delivered的采集中，bbr根本不管某一个应答是重传后的ACK确认的，正常ACK确认的，还是说SACK确认的。bbr只关心被应答了多少！

这和TCP/IP网络模型是一致的，因为在中间链路上，路由器交换机们也不会去管这些数据包是重传的还是乱序的，然而拥塞也是在这些地方发生的，既然拥塞点都不关心数据的意义，TCP为什么要关注呢？反过来，我们看一下拥塞发生的原因，即数据量超过了路由器的带宽限制，利用这一点，只需要精心地控制发送的数据量就好了，完全不用管什么乱序，重传之类的。当然我的意思是说，拥塞控制算法中不用管这些，但这并不意味着它们是被放弃的，其它的机制会关注的，比如SACK机制，RACK机制，RTO机制等。

接下来我们看一下这个delivered以及interval_us的采集是如何实现的。还是像往常一样，我不准备分析源码，因为如果分析源码的话，往往难以抓住重点，过一段时间自己也看不懂了，相反，画图的话，就可以过滤掉很多诸如unlikely等异常流或者当前无需关注的东西：

![img](https://pic4.zhimg.com/80/v2-7ae9c190f3cb9ea812fb76bea483b3f7_720w.webp)

上图中，我故意用了一个极端点的例子，在该例子中，我几乎都是使用的SACK，当X被SACK时，我们可以根据图示很容易算出从Delivered为7时的数据包被确认到X被确认为止，一共有 12-7=5个数据包被确认，即这段时间网络上清空了5个数据包！我们便很容易算出带宽值了。我的这个图示在解释带宽计算方法之外，还有一个目的，即说明bbr在计算带宽时是不关注数据包是否按序确认的，它只关注数量，即数据包被网络清空的数量。实实在在的计算，不猜Lost，不猜乱序，这些东西，你再怎么猜也猜不准！

计算所得的bw就是bbr此后一切计算的基准。

**2.状态机**

bbr的状态机转换图以及注释如下图所示：

![img](https://pic2.zhimg.com/80/v2-a82f3f8058b52e335fbe4a5849420ca9_720w.webp)

通过上述的状态机以及上一节的带宽计算方式，我们知道了bbr的工作方式：不断地基于当前带宽以及当前的增益系数计算pacing rate以及cwnd，以此2个结果作为拥塞控制算法的输出，在TCP连接的持续过程中，每收到一个ACK，都会计算即时的带宽，然后将结果反馈给bbr的pipe状态机，不断地调节增益系数，这就是bbr的全部，我们发现它是一个典型的封闭反馈系统，与TCP当前处于什么拥塞状态完全无关，其简图如下：

![img](https://pic3.zhimg.com/80/v2-e24d7e2d4b3efb143915eee083cbcd3e_720w.webp)

这非常不同于之前的所有拥塞控制算法，在之前的算法中，我们发现拥塞算法内部是受外部的拥塞状态影响的，比如说在Recovery状态下，甚至都不会进入拥塞控制算法，在bbr进入内核之前，Linux使用PRR算法控制了Recovery状态的窗口调整，即便说这个时候网络已经恢复，TCP也无法发现，因为TCP的Recovery状态还未恢复到Open，这就是根源！

**pacing rate以及cwnd的计算**

这一节好像是重点中的重点，但是我觉得如果理解了bbr的带宽计算，状态机以及其增益系数的概念，这里就不是重点了，这里只是一个公式化的结论。

pacing rate怎么计算？很简单，就是是使用时间窗口内(默认10轮采样)最大BW。上一次采样的即时BW，用它来在可能的情况下更新时间窗口内的BW采样值集合。这次能否按照这个时间窗口内最大BW发送数据呢？这样看当前的增益系数的值，设为G，那么BW*G就是pacing rate的值，是不是很简单呢？！

至于说cwnd的计算可能要稍微复杂一点，但是也是可以理解的，我们知道，cwnd其实描述了一条网络管道(rwnd描述了接收端缓冲区)，因此cwnd其实就是这个管道的容量，也就是BDP！

BW我们已经有了，缺少的是D，也就是RTT，不过别忘了，bbr一直在持续搜集最小的RTT值，注意，bbr并没有采用什么移动指数平均算法来“猜测”RTT(我用猜测而不是预测的原因是，猜测的结果往往更加不可信！)，而是直接冒泡采集最小的RTT(注意这个RTT是TCP系统层面移动指数平均的结果，即SRTT，但brr并不会对此结果再次做平均！)。我们用这个最小RTT干什么呢？

当前是计算BDP了！这里bbr取的RTT就是这个最小RTT。最小RTT表示一个曾经达到的最佳RTT，既然曾经达到过，说明这是客观的可以再次达到的RTT，这样有益于网络管道利用率最大化！

我们采用BDP*G'就算出了cwnd，这里的G'是cwnd的增益系数，与带宽增益系数含义一样，根据bbr的状态机来获取！

**BBR的细节浅述**

该节的题目比较怪异，既然是细节为什么又要浅述？？

这是我的风格，一方面，说是细节是因为这些东西还真的很少有人注意到，另一方面，说是浅述，是因为我一般都不会去分析代码以及代码里每一个异常流，我认为那些对于理解原理帮助不大，那些东西只是在研发和优化时才是有用的，所以说，像往常一样，我这里的这个小节还是一如既往地去谈及一些“细节”。

**1.豪放且大胆的安全探测**

在看到bbr之后，我觉得之前的TCP拥塞控制算法都错了，并不是思想错了，而是实现的问题。

bbr之所以敢大胆的去探测预估带宽是因为TCP把更多的权力交给了它！在bbr之前，很多本应该由拥塞控制算法去处理的细节并不归拥塞控制算法管。在详述之前，我们必须分清两件事：

1).传输多少数据？

2).传输哪些数据？

按照“上帝的事情上帝管，凯撒的事情凯撒管”的原则，这两件事本来就该由不同的机制来完成，不考虑对端接收窗口的情况下，拥塞窗口是唯一的主导因素，“传输多少数据”这件事应该由拥塞算法来回答，而“传输哪些数据”这个问题应该由TCP拥塞状态机以及SACK分布来决定，诚然这两个问题是不同的问题，不应该杂糅在一起。

然而，在bbr进入内核之前的Linux TCP实现中，以上两个问题并不是分得特别清。TCP的拥塞状态只有在Open时才是上述的职责分离的完美样子，一旦进入Lost或者Recovery，那么拥塞控制算法即便对“问题1)：传输多少数据”都无能为力，在Linux的现有实现中，PRR算法将接管一切，一直把窗口下降到ssthresh，在Lost状态则反应更加激烈，直接cwnd硬着陆！随后等丢失数据传输成功后再执行慢启动....在重新进入Open状态之前，拥塞控制算法几乎不会起作用，这并不是一种高速公路上的模式(小碰擦，拍照后停靠路边，自行解决)，更像是闹市区的交通事故处理方式(无论怎样，保持现场，直到交警和保险公司的人来现场处置)。

bbr算法逃离了这一切错误的做法，在bbr的patch中，并非只是完成了一个tcp_bbr.c，而是对整个TCP拥塞状态控制框架进行了大手术，我们可以从以下的拥塞控制核心函数中可见一斑：

```text
static void tcp_cong_control(struct sock *sk, u32 ack, u32 acked_sacked,
                 int flag, const struct rate_sample *rs)
{
    const struct inet_connection_sock *icsk = inet_csk(sk);
    if (icsk->icsk_ca_ops->cong_control) {
        // 如果是bbr，则完全被bbr接管，不管现在处在什么状态！
        /* 目前而言，只有bbr使用了这个机制，但我相信，不久的将来，
         * 会有越来越多的拥塞控制算法使用这个统一的完全接管机制！
         * 就我个人而言，在几个月前就写过一个patch，接管了tcp_cwnd_reduction
         * 这个prr的降窗过程。如果当时有了这个框架，我就有福了！
         */
        icsk->icsk_ca_ops->cong_control(sk, rs);
        return;
    }
    // 否则继续以往的错误方法！
    if (tcp_in_cwnd_reduction(sk)) {
        /* Reduce cwnd if state mandates */
       // 非Open状态中拥塞算法不受理窗口调整
        tcp_cwnd_reduction(sk, acked_sacked, flag);
    } else if (tcp_may_raise_cwnd(sk, flag)) {
        /* Advance cwnd if state allows */
        tcp_cong_avoid(sk, ack, acked_sacked);
    }
    tcp_update_pacing_rate(sk);
}
```

在这个框架下，无论处在哪个状态(Open，Disorder，Recovery，Lost...)，如果拥塞控制算法自己声明有这个能力，那么具体可以传输多少数据，完全由拥塞控制算法自行决定，TCP拥塞状态控制机制不再干预！

**2.为什么bbr可以忽略Recovery和Lost状态**

看懂了以上第1点，这一点就很容易理解了。

在第1点中，我描述了bbr确实忽略了Recovery等非Open的拥塞状态，但是为什么可以忽略呢？一般而言，很多人都会质疑，会说bbr采用这么鲁莽的方式，最终一定会让窗口卡住不再滑动，但是我要反驳，你难道不知道cwnd只是个标量吗？我画一个图来分析：

![img](https://pic2.zhimg.com/80/v2-c4cb407e0ba1e7c368620fc59d8a8d2d_720w.webp)

看懂了吗？不存在任何问题！基本上，我们在讨论拥塞控制算法的时候，会忽略流量控制，因为不想让rwnd和cwnd杂糅起来，但是在这里，它们相遇了，幸运的是，并没有引发冲突！

然而，这并不是全部，本节旨在“浅析”，因此就不会关注代码处理的细节。在bbr的实现中，如果算法外部的TCP拥塞状态已经进入了Lost，那么cwnd该是多少呢？在bbr之前的拥塞算法中，包括cubic在内的所有算法中，当TCP核心实现从将cwnd调整到1或者prr到ssthresh一直到恢复到Open状态，拥塞算法无权干预流程，然而bbr不。虽然说进入Lost状态后，cwnd会硬着陆到1，然而由于bbr的接管，在Lost期间，cwnd还是可以根据即时带宽调整的！

这意味着什么？

这意味着bbr可以区别噪声丢包和拥塞丢包了！

**a).噪声丢包**

如果是噪声丢包，在收到reordering个重复ACK后，由于bbr并不区分一个确认是ACK还是SACK引起的，所以在bbr看来，即时带宽并没有降低，可能还有所增加，所以一个数据包的丢失并不会引发什么，bbr依旧会给出一个比较大的cwnd配额，此时虽然TCP可能已经进入了Recovery状态，但bbr依旧按照自己的bw以及调整后的增益系数来计算cwnd的新值，过程中并不会受到任何TCP拥塞状态的影响。

如此一来，所有的噪声丢包就被区别开来了！bbr的宗旨是：“首先，在我的bw计算指示我发生拥塞之前，任何传统的TCP拥塞判断-丢包/时延增加，均全部失效，我并不care丢包和RTT增加”，随后brr又会说：“但是我比较care的是，RTT在一段时间内(随你怎么配，但我个人倾向于自学习)都没有达到我所采集到的最小值或者更小的值！这也许意味着着链路真的发生拥塞了！”...

**b).拥塞丢包**

将a)的论述反过来，我们就会得到奇妙的封闭性结论。这样，bbr不光是消除了吞吐曲线的锯齿(ssthresh所致，bbr并不使用ssthresh！)，而且还消除了传统拥塞控制算法(指bbr以及封闭的傻逼Appex之前)的判断滞后性问题。在cubic发现丢包进而判断为拥塞时，拥塞可能已经缓解了，但是cubic无法发现这一点。为什么？原因在于cubic在计算新的cwnd的时候，并没有把当前的网络状态(比如bw)当作参数，而只是一味的按照数学意义上的三次方程去计算，这是错误的，这不是一个正确的反馈系统的做法！

基于a)和b)，看到了吧，这就是新的拥塞判断机制！综合考虑丢包和RTT的增加：

**b-1).如果丢包时真的发生了拥塞，那么测量的即时带宽肯定会减少，否则，丢包即拥塞就是谎言。**

**b-2).如果RTT增加时真的发生了拥塞，那么测量的即时带宽肯定会减少，否则，时延增加即拥塞就是谎言。**

bbr测量了即时带宽，这个统一cwnd和rtt的计量，完全忽略了丢包，因此bbr的算法思想是TCP拥塞控制的正轨！事实上，丢包本就不应该作为一种拥塞的标志，它只是拥塞的表现。

**3.状态机的点点滴滴**

我在上文已经呈现了关于STARTUP，DRAIN，PROBE_BW，PROBE_RTT的状态图以及些许细节，当时我指出这个状态图的目标是为了完成bbr的目标，即填满整个网络！在这个状态图看来，所有已知的东西就是当前的即时带宽，所有可以计算的东西就是增益系数，然后根据这两个元素就可以轻易计算出pacing rate和cwnd，是不是很简单呢？整体看来就是就是这么简单，但是从细节上看，不同的pipe状态中的增益系数的计算却是值得推敲的，以下是bbr处在各个状态时的增益系数：

**STARTUP：2~3**

**DRAIN：pacing rate的增益系数为1000/2885，cwnd的增益系数为1000/2005+1。**

**PROBE_BW：5/4，1，3/4，bbr在PROBE_BW期间会随机在这些增益系数之间选择当前的增益系数。**

**PROBE_RTT：1。但是在探测RTT期间，为了防止丢包，cwnd会强制cut到最小值，即4个MSS。**

我们可以看到，bbr并没有明确的所谓“降窗时刻”，一切都是按照状态机来的，期间丝毫不会理会TCP是否处在Open，Recovery等状态。在此前的拥塞控制算法中，除了Vegas等基于延时的算法会在计算得到的target cwnd小于当前cwnd时视为拥塞而在算法中降窗外，其它的所有基于丢包的算法中均是检测到丢包(RTO或者reordering个重复ACK)时降窗的，可悲的是，这个降窗过程并不受拥塞算法的控制，拥塞算法只能消极地给出一个ssthresh值，即降窗的目标，这显然是令人无助的！

bbr不再关注丢包事件，它并不把丢包当成很严重的事，这事也不归它管，只要TCP拥塞状态机控制机制可以合理地将一些包标记为LOST，然后重传它们便是了，bbr能做的仅仅是告诉TCP一共可以发出去多少数据，仅此而已！然而，如果TCP并没有把LOST数据包合理标记好，bbr并不care，它只是根据当前的bw和增益系数给出下一个pacing rate以及cwnd而已！

**4.关于Sched FQ**

这里涉及的是bbr之外的东西，Fair queue！在bbr的patch最后，会发现几行注释：

**NOTE: BBR \*must\* be used with the fq qdisc ("man tc-fq") with pacing enabled, since pacing is integral to the BBR design andimplementation. BBR without pacing would not function properly, and may incur unnecessary high packet loss rates.**

记住这几行文字并理解它们。

这是bbr最为重要的一方面。虽然说Linux的TCP实现早就支持的pacing rate，但直到4.8版本都没有在TCP层面支持它，很大的一部分原因是因为借助已有的FQ可以很完美地实现pacing rate！TCP可以借助FQ来实现平缓而非突发的数据发送！

关于FQ的详细内容可以去看相关的manual和源码，这里要说的仅仅是，FQ可以根据bbr设置的pacing rate将一个cwnd内的数据的发送从“突发到网络”这种行为变换到“平缓发送到网路”的行为，所谓的平缓发送指的就是数据包是按照带宽速率计算的间隔一个个发送到网络的，而不是突发进网络的！

这样一来，就给了网络缓存以缓解的机会！记住，关键问题是bbr会在每收到ACK/SACK时计算bw，这个精确的测量不会漏掉任何可乘之机，即便当前网络拥塞了，它只要能在下一时刻恢复，bbr就可以发现，因此即时带宽通常可以表现这一点！

**5.其它**

还有关于令牌桶监管发现(lt policed)的主题，long term采样的主题，留到后面的文章具体阐述吧，本文已经足够长了。

**6.bufferbloat问题**

关于深队列，数据包如何如何长时间排队但不丢包却引发RTO，对于浅队列，数据包如何如何频繁丢包...谈起这个话题我一开始想滔滔不绝，后来想骂人，现在我三缄其口！任何人都知道端到端的QoS是一个典型的反馈系统，但是任何人都只是夸夸其谈，我选择的是闭口不说，如果非要我说，我的回答就是：不知道！

这是一个怎么说都能对又怎么说都能错的话题，就像股票预测那样，所以我选择闭嘴。

bbr算法到来后，单单从公共测试结果上看，貌似解决了bufferbloat问题，也许吧，也许。bbr好像真的开始在高速公路上飚车了...最后给出一个测试图

![img](https://pic2.zhimg.com/80/v2-9727ac506711b8b3135eee2e416f7825_720w.webp)

**bbr代码的简单性和复杂性**

我一向觉得TCP拥塞控制算法太过复杂，而复杂的东西基本上就是用来装逼的垃圾，直到遇到了bbr。

Neal Cardwell提供的patch简单而又直接，大家可以从该bbr的patch上一看究竟！在bbr模块之外，Neal Cardwell主要更改了tcp_ack函数里面关于delivered计数的部分以及拥塞控制主函数，这一切都十分显然，只要patch代码就可以一目了然。在数据包被发送的时候-不管是初次发送还是重传，均会被当前TCP的连接状况记录在该数据包的tcp_skb_cb中，在数据包被应答的时候-不管是被ACK还是被SACK，均会根据当前的状态和其tcp_skb_cb中状态计算出一个带宽，这些显而易见的逻辑相比任何人都应该知道哪里的代码被修改了！

然而，这种查找和确认的工作太令人感到悲哀，读懂代码是容易的，移植代码是无聊的，因为时间卡的太紧！我必须要说的是，如果一件感兴趣的事情变成了必须要完成的工作，那么做它的激情起码减少了1/4，OK，还不算太坏，然而如果这个必须完成的工作有了deadline，那么激情就会再减少1/4，最后，如果有人在背后一直催，那么完蛋，这件事可以瞬间完成，但是我可以郑重说明这是凑合的结果！但是实际上，这件事本应该可以立即快速有高质量的完成并验收！

**写在最后**

我比较喜欢工匠精神，一种时间打磨精品的精神，一种自由引导创造的精神。

原文地址：https://zhuanlan.zhihu.com/p/383910510

作者：linux

# 【NO.520】Linux完全公平调度算法原理与实现

Linux 进程调度算法经历了以下几个版本的发展：

- 基于时间片轮询调度算法。(2.6之前的版本)
- O(1) 调度算法。(2.6.23之前的版本)
- 完全公平调度算法。(2.6.23以及之后的版本)

这篇主要分析 Linux 现在所使用的 完全公平调度算法。分析完全公平调度算法 前，我们先了解下 完全公平调度算法 的基本原理。

## 1.完全公平调度算法基本原理

完全公平调度算法 体现在对待每个进程都是公平的，那么怎么才能做到完全公平呢？有一个比较简单的方法就是：让每个进程都运行一段相同的时间片，这就是 基于时间片轮询调度算法。如下图所示：

![img](https://pic2.zhimg.com/80/v2-6ac0c3c0f89cb744d2cb85d83d9c64cd_720w.webp)

如上图所示，开始时进程1获得 time0 ~ time1 的CPU运行时间。当进程1的时间片使用完后，进程2获得 time1 ~ time2 的CPU运行时间。而当进程2的时间片使用完后，进程3获得 time2 ~ time3 的CPU运行时间。

如此类推，由于每个时间片都是相等的，所以理论上每个进程都能获得相同的CPU运行时间。这个算法看起来很不错，但存在两个问题：

- 不能按权重分配不同的运行时间，例如有些进程权重大的应该获得更多的运行时间。
- 每次调度时都需要遍历运行队列中的所有进程，找到优先级最大的进程运行，时间复杂度为 O(n)。对于一个高性能的操作系统来说，这是不能接受的。

为了解决上面两个问题，Linux内核的开发者创造了完全公平调度算法。

**完全公平调度的两个时间**

为了实现 完全公平调度算法，为进程定义两个时间:

**1、实际运行时间：**

> 实际运行时间 = 调度周期 * 进程权重 / 所有进程权重之和

- 调度周期：是指所有可进程运行一遍所需要的时间。
- 进程权重：依据进程的重要性，分配给每个进程不同的权重。

例如，调度周期为 30ms，进程A的权重为 1，而进程B的权重为 2。那么：

进程A的实际运行时间为：30ms * 1 / (1 + 2) = 10ms

进程B的实际运行时间为：30ms * 2 / (1 + 2) = 20ms

**2、虚拟运行时间：**

> 虚拟运行时间 = 实际运行时间 * 1024 / 进程权重 = (调度周期 * 进程权重 / 所有进程权重之和) * 1024 / 进程权重

= 调度周期 * 1024 / 所有进程总权重

从上面的公式可以看出，在一个调度周期里，所有进程的 虚拟运行时间 是相同的。所以在进程调度时，只需要找到 虚拟运行时间 最小的进程调度运行即可。

为了能够快速找到 虚拟运行时间 最小的进程，Linux 内核使用 红黑树 来保存可运行的进程，而比较的键值就是进程的 虚拟运行时间。

如果不了解 红黑树 的话，可以把它看成一个自动排序的容器即可。如下图所示：

![img](https://pic1.zhimg.com/80/v2-8e2ddac2a1783bce76975c6495b06b04_720w.webp)

如上图所示，红黑树 的左节点比父节点小，而右节点比父节点大。所以查找最小节点时，只需要获取 红黑树 的最左节点即可，时间复杂度为 O(logN)。

## 2.完全公平调度的两个对象

Linux 内核为了实现 完全公平调度算法，定义两个对象：cfs_rq (可运行进程队列) 和 sched_entity (调度实体)。

- cfs_rq (可运行进程队列)：使用 红黑树 结构来保存内核中可以运行的进程。
- sched_entity (调度实体)：可被内核调度的实体，如果忽略组调度（本文也不涉及组调度），可以把它当成是进程。

cfs_rq 对象定义

```text
struct cfs_rq {
    struct load_weight load;
    unsigned long nr_running;       // 运行队列中的进程数
    u64 exec_clock;                 // 当前时钟
    u64 min_vruntime;               // 用于修证虚拟运行时间
    struct rb_root tasks_timeline; // 红黑树的根节点
    struct rb_node *rb_leftmost;   // 缓存红黑树最左端节点, 用于快速获取最小节点
    ...
};
```

对于 cfs_rq 对象，现在主要关注的是 tasks_timeline 成员，其保存了可运行进程队列的根节点（红黑树的根节点）。

sched_entity 对象定义

```text
struct sched_entity {
  struct load_weight  load;
  struct rb_node    run_node;         // 用于连接到运行队列的红黑树中
  struct list_head  group_node;
  unsigned int    on_rq;              // 是否已经在运行队列中
  u64          exec_start;            // 开始统计运行时间的时间点
  u64          sum_exec_runtime;      // 总共运行的实际时间
  u64          vruntime;              // 虚拟运行时间(用于红黑树的键值对比)
  u64          prev_sum_exec_runtime; // 总共运行的虚拟运行时间
  u64          last_wakeup;
  u64          avg_overlap;
    ...
};
```

对于 sched_entity 对象，现在主要关注的是 run_node 成员，其主要用于把调度实体连接到可运行进程的队列中。

另外，进程描述符 task_struct 对象中，定义了一个类型为 sched_entity 的成员变量 se，如下：

```text
struct task_struct {
    ...
    struct sched_entity se;
    ...
}
```

所以，他们之间的关系如下图：

![img](https://pic2.zhimg.com/80/v2-82faf3d00ec1cff5987296f4b7ac0a89_720w.webp)

从上图可以看出，所有 sched_entity 对象通过其 run_node 成员组成了一颗红黑树，这颗红黑树的根结点保存在 cfs_rq 对象的 task_timeline 成员中。

另外，进程描述符 task_sturct 定义了一个类型为 sched_entity 的成员变量 se，所以通过进程描述符的 se 字段就可以把进程保存到可运行队列中。

## 3.完全公平调度算法实现

有了上面的基础，现在可以开始分析 Linux 内核中怎么实现 完全公平调度算法 了。

我们先来看看怎么更新一个进程的虚拟运行时间。

**1. 更新进程虚拟运行时间**

更新一个进程的虚拟运行时间是通过 __update_curr() 函数完成的，其代码如下：

/src/kernel/sched_fair.c

```text
static inline void
__update_curr(struct cfs_rq *cfs_rq, struct sched_entity *curr,
              unsigned long delta_exec)
{
    unsigned long delta_exec_weighted;
    curr->sum_exec_runtime += delta_exec; // 增加进程总实际运行的时间
    delta_exec_weighted = delta_exec;     // 初始化进程使用的虚拟运行时间
    // 根据进程的权重计算其使用的虚拟运行时间
    if (unlikely(curr->load.weight != NICE_0_LOAD)) {
        delta_exec_weighted = calc_delta_fair(delta_exec_weighted, &curr->load);
    }
    curr->vruntime += delta_exec_weighted; // 更新进程的虚拟运行时间
}
```

__update_curr() 函数各个参数意义如下：

- cfs_rq：可运行队列对象。
- curr：当前进程调度实体。
- delta_exec：实际运行的时间。

__update_curr() 函数主要完成以下几个工作：

1. 更新进程调度实体的总实际运行时间。
2. 根据进程调度实体的权重值，计算其使用的虚拟运行时间。
3. 把计算虚拟运行时间的结果添加到进程调度实体的 vruntime 字段。

我们接着分析怎么把进程添加到运行队列中。

**2. 把进程调度实体添加到运行队列中**

要将进程调度实体添加到运行队列中，可以调用 __enqueue_entity() 函数，其实现如下：

/src/kernel/sched_fair.c

```text
static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
{
    struct rb_node **link = &cfs_rq->tasks_timeline.rb_node; // 红黑树根节点
    struct rb_node *parent = NULL;
    struct sched_entity *entry;
    s64 key = entity_key(cfs_rq, se); // 当前进程调度实体的虚拟运行时间
    int leftmost = 1;
    while (*link) { // 把当前调度实体插入到运行队列的红黑树中
        parent = *link;
        entry = rb_entry(parent, struct sched_entity, run_node);
        if (key < entity_key(cfs_rq, entry)) { // 比较虚拟运行时间
            link = &parent->rb_left;
        } else {
            link = &parent->rb_right;
            leftmost = 0;
        }
    }
    if (leftmost) {
        cfs_rq->rb_leftmost = &se->run_node; // 缓存红黑树最左节点
        cfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, se->vruntime);
    }
    // 这里是红黑树的平衡过程(参考红黑树数据结构的实现)
    rb_link_node(&se->run_node, parent, link);
    rb_insert_color(&se->run_node, &cfs_rq->tasks_timeline);
}
```

__enqueue_entity() 函数的主要工作如下：

1. 获取运行队列红黑树的根节点。
2. 获取当前进程调度实体的虚拟运行时间。
3. 把当前进程调度实体添加到红黑树中（可参考红黑树的插入算法）。
4. 缓存红黑树最左端节点。
5. 对红黑树进行平衡操作（可参考红黑树的平衡算法）。

调用 __enqueue_entity() 函数后，就可以把进程调度实体插入到运行队列的红黑树中。同时会把红黑树最左端的节点缓存到运行队列的 rb_leftmost 字段中，用于快速获取下一个可运行的进程。

**3. 从可运行队列中获取下一个可运行的进程**

要获取运行队列中下一个可运行的进程可以通过调用 __pick_next_entity() 函数，其实现如下：

/src/kernel/sched_fair.c

```text
static struct sched_entity *__pick_next_entity(struct cfs_rq *cfs_rq)
{
    // 1. 先调用 first_fair() 获取红黑树最左端节点
    // 2. 调用 rb_entry() 返回节点对应的调度实体
    return rb_entry(first_fair(cfs_rq), struct sched_entity, run_node);
}
static inline struct rb_node *first_fair(struct cfs_rq *cfs_rq)
{
    return cfs_rq->rb_leftmost; // 获取红黑树最左端节点
}
```

前面介绍过，红黑树的最左端节点就是虚拟运行时间最少的进程，所以 __pick_next_entity() 函数的流程就非常简单了，如下：

- 首先调用 first_fair() 获取红黑树最左端节点。
- 然后再调用 rb_entry() 返回节点对应的调度实体。

## 4.调度时机

前面介绍了 完全公平调度算法 怎么向可运行队列添加调度的进程和怎么从可运行队列中获取下一个可运行的进程，那么 Linux 内核在什么时候会进行进程调度呢？

> 答案是由 Linux 内核的时钟中断触发的。

时钟中断 是什么？如果没了解过操作系统原理的可能不了解这个东西，但是没关系，不了解可以把他当成是定时器就好了，就是每隔固定一段时间会调用一个回调函数来处理这个事件。

时钟中断 犹如 Linux 的心脏一样，每隔一定的时间就会触发调用 scheduler_tick() 函数，其实现如下：

```text
void scheduler_tick(void)
{
    ...
    curr->sched_class->task_tick(rq, curr, 0); // 这里会调用 task_tick_fair() 函数
    ...
}
```

scheduler_tick() 函数会调用 task_tick_fair() 函数处理调度相关的工作，而 task_tick_fair() 主要通过调用 entity_tick() 来完成调度工作的，调用链如下：

```text
scheduler_tick() -> task_tick_fair() -> entity_tick()
```

entity_tick() 函数实现如下：

```text
static void
entity_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr, int queued)
{
    // 更新当前进程的虚拟运行时间
    update_curr(cfs_rq);
    ...
    if (cfs_rq->nr_running > 1 || !sched_feat(WAKEUP_PREEMPT))
        check_preempt_tick(cfs_rq, curr); // 判断是否需要进行进程调度
}
```

entity_tick() 函数主要完成以下工作：

- 调用 update_curr() 函数更新进程的虚拟运行时间，这个前面已经介绍过。
- 调用 check_preempt_tick() 函数判断是否需要进行进程调度。

我们接着分析 check_preempt_tick() 函数的实现：

```text
static void
check_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)
{
    unsigned long ideal_runtime, delta_exec;
    // 计算当前进程可用的时间片
    ideal_runtime = sched_slice(cfs_rq, curr); 
    // 进程运行的实际时间
    delta_exec = curr->sum_exec_runtime - curr->prev_sum_exec_runtime;
    // 如果进程运行的实际时间大于其可用时间片, 那么进行调度
    if (delta_exec > ideal_runtime)
        resched_task(rq_of(cfs_rq)->curr);
}
```

check_preempt_tick() 函数主要完成以下工作：

- 通过调用 sched_slice() 计算当前进程可用的时间片。
- 获取当前进程在当前调度周期实际已运行的时间。
- 如果进程实际运行的时间大于其可用时间片, 那么调用 resched_task() 函数进行进程调度。

可以看出，在 时钟中断 的处理中，有可能会进行进程调度。除了 时钟中断 外，一些主动让出 CPU 的操作也会进行进程调度（如一些 I/O 操作），这里就不详细分析了，有兴趣可以自己研究。

原文地址：https://zhuanlan.zhihu.com/p/354028574

作者：linux

# 【NO.521】如何快速地进出——C++ 读写数据 I/O 性能优化

I 即 Input 表示进，O 即 Output 表示出，任何的进出，如果你没有掌握正确的姿势，就容易让工具累趴，如上图所示。比如说，读写数据，如果没有掌握正确的 IO 姿势，你的计算机很容易就累了。

在多媒体技术高速发展的今天，数据的积累成指数级别的增长。数据的挖掘要求我们对海量的数据进行输入，进而处理与分析之后，将结果输出。很多时候，在算法处理数据耗时较短的情况的，数据的 I/O 成了减少时间、提高程序性能的瓶颈。

要改进 I/O 性能，一个最简单的方法就是使用不同的方法。下面着重比较 ifstream、ofstream、fread、fwrite、mmap 在读写方面的时间差异。

不谈数据处理只看是否到内存中的性能比较，就是耍流氓。譬如，ifstream 在读入的时候，天然就可以对数据进行处理，而 fread/mmap 本质上只是做了一个内存的映射，我们仍需进一步将映射入的数据进行结构化。

为了进行比较，我们假设构造了一个数据，有多行的数据，行内数据用逗号隔开。最后一列是二分类标签数据。数据格式示例如下所示（一行）

![img](https://pic4.zhimg.com/80/v2-b905e6d45aaf84d500e9fd6d25e277ef_720w.webp)

我们要做的，就是把这个数据读入到对应的结构中，之后再原封不动地写入到新文件中，进行计时。为了无误差的计时，计时我采用 chrono。

```text
auto s1 = chrono::steady_clock::now();
auto s2 = chrono::steady_clock::now();
some code
cout << "read data time: " << chrono::duration_cast<chrono::duration<double>>(s2 - s1).count() << " s" << endl;
```

## 1.ifstream/ofstream 读写

人狠话不多，直接上代码。直接读代码，写那么多汉字来解读代码，毫无意义，又不是小孩子了。读代码永远是学习编程最快的方式。

```text
#include<bits/stdc++.h>//万能头文件 
using namespace std;

struct Data {
	vector<double> features;
	int label;
	Data(vector<double> f, int l) : features(f), label(l) {
	}
};

string trainFile = "train_data.txt";
int featuresNum;
vector<Data> trainDataSet;

bool loadTrainData() {


	ifstream infile(trainFile.c_str());
	string line;

	if (!infile) {
		cout << "打开训练文件失败" << endl;
		exit(0);
	}

	while (infile) {
		getline(infile, line);
		if (line.size() > featuresNum) {
			stringstream sin(line);
			char ch;
			double dataV;
			int i;
			vector<double> feature;
			i = 0;

			while (sin) {
				char c = sin.peek();
				if (int(c) != -1) {
					sin >> dataV;
					feature.push_back(dataV);
					sin >> ch;
					i++;
				} else {
					cout << "训练文件数据格式不正确，出错行为" << (trainDataSet.size() + 1) << "行" << endl;
					return false;
				}
			}
			int ftf;
			ftf = (int)feature.back();
			feature.pop_back();
			trainDataSet.push_back(Data(feature, ftf));
		}
	}
	infile.close();
	return true;
}

string outputFile = "output_data.txt";
int storeData() {
	featuresNum = trainDataSet[0].features.size();
	string line;
	int i;
	ofstream fout(outputFile.c_str());
	if (!fout.is_open()) {
		cout << "打开输出文件失败" << endl;
	}
	for (i = 0; i < trainDataSet.size(); i++) {
		for (int j = 0; j < featuresNum; j++) {
			fout << trainDataSet[i].features[j] << ",";
		}
		fout << trainDataSet[i].label <<"\n";
	}
	fout.close();
	return 0;
}


int main(int argc, char* argv[]) {
	auto s1 = chrono::steady_clock::now();
	loadTrainData();
	auto s2 = chrono::steady_clock::now();
	cout << "time elapsed: " << chrono::duration_cast<chrono::duration<double>>(s2 - s1).count() << " s" << endl;
	storeData();
	auto s3 = chrono::steady_clock::now();
	cout << "time elapsed: " << chrono::duration_cast<chrono::duration<double>>(s3 - s2).count() << " s" << endl;

}
```

结果表明，在我这个机子上，50M 的文件，使用 ifstream 读和结构化数据用了 25 秒。ofstream 写数据用了 15 秒。好，下面，我们再来看一下，fread 和 fwrite 需要多少时间。

## 2.fread/fwrite(or sprintf) 读写

同时，我也使用 fread 和 fwrite 进行了测试。为了和 fwrite 的不可读结果做对比，同时我还测试了 sprintf。所用的代码如下。

```text
#include<bits/stdc++.h>//万能头文件 
#define FEATURE_NUM 1000
#define TRAIN_DATA_NUM 10000
using namespace std;

string trainFile = "train_data.txt";
int featuresNum;
const int MAXS = 8 * (FEATURE_NUM + 1)*TRAIN_DATA_NUM;
char buf[MAXS];
double features[TRAIN_DATA_NUM][FEATURE_NUM];
int labels[TRAIN_DATA_NUM];

void loadTrainData() {

	FILE *fp;
	fp = fopen(trainFile.c_str(), "rb");
	int len = fread(buf, 1, MAXS, fp);
	fclose(fp);

	buf[len] = '\0';
	double position = 1.0;
	int symbol = 0;
	int point = 0;
	int i, col, row = 0;
	double line[FEATURE_NUM + 2] = { 0 };
	i = 0;
	for (char *p = buf; *p && p - buf < len; p++) {
		if (row >= TRAIN_DATA_NUM) {
			break;
		}

		if (*p == ',') {
			if (symbol == 1)
				line[i] = -line[i];
			i++;
			point = 0;
			position = 1.0;
			symbol = 0;
			line[i] = 0.0;
		} else if (*p == '\n') {
			if (symbol == 1)
				line[i] = -line[i];
			int ftf = (int)line[FEATURE_NUM];
			for (col = 0; col < FEATURE_NUM; col++) {
				features[row][col] = line[col];
			}
			labels[row] = ftf;
			row = row + 1;
			i = 0;
			point = 0;
			position = 1.0;
			symbol = 0;
			line[i] = 0.0;
		} else if (*p == '.') {
			point = 1;
		} else if (*p == '-') {
			symbol = 1;
		} else {
			if (point == 0)
				line[i] = line[i] * 10 + *p - '0';
			else {
				position = position*0.1;
				line[i] += position*(*p - '0');
			}
		}

	}
}

/* 拷贝为编码格式*/ 
char outputSet[MAXS];
string output_data = "output_data.txt";
int storeData() {
	char *output = outputSet;
	char *output0 = output;
	featuresNum = 1000;	
	for (int i = 0; i < 8000; i++) {
		for (int j = 0; j < featuresNum; j++) {
			memcpy(output, &(features[i][j]), sizeof(double));
			output += sizeof(double);
			*output = ',';
			output++;
		}
		memcpy(output, &(labels[i]), sizeof(int));
		output += sizeof(int);
		*output = '\n';
		output++;
	}
	FILE *pf = fopen(output_data.c_str(), "w+");
	fwrite(output0, 1, output-output0, pf);
	fclose(pf);
	return 0;
}

int storeData1() {
	char *output = outputSet;
	char *output0 = output;
	int n;
	featuresNum = 1000;	
	for (int i = 0; i < 8000; i++) {
		for (int j = 0; j < featuresNum; j++) {
			n = sprintf(output, "%0.3f,",features[i][j]);
			output += n;
		}
		n = sprintf(output, "%d\n",labels[i]);
		output += n;
	}
	FILE *pf = fopen(output_data.c_str(), "w+");
	fwrite(output0, 1, output-output0, pf);
	fclose(pf);
	return 0;
}

int main(int argc, char* argv[]) {
	auto s1 = chrono::steady_clock::now();
	loadTrainData();
	auto s2 = chrono::steady_clock::now();
	cout << "time elapsed: " << chrono::duration_cast<chrono::duration<double>>(s2 - s1).count() << " s" << endl;
	storeData1();
	auto s3 = chrono::steady_clock::now();
	cout << "time elapsed: " << chrono::duration_cast<chrono::duration<double>>(s3 - s2).count() << " s" << endl;

}
```

fread 读入数据并进行处理，所用的时间为 0.3 s。fwrite 写入数据时间为 0.2 秒。sprintf 写入数据时间为 12 s。

## 3.mmap 读写

mmap 需要用到如下的头文件。

```text
#include <sys/stat.h>
#include <sys/types.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/mman.h>
```

mmap 和 fread（fwrite）本质上几乎不耗时的，耗时的是字符串的处理。但是，如果不加字符串处理，拿内存映射的时间和 fstream 函数比，显然对于 fstream 是不公平的。

```text
#include<bits/stdc++.h>

//mmap 需要用到如下头文件
#include <sys/stat.h>
#include <sys/types.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/mman.h>

#define FEATURE_NUM 1000
#define TRAIN_DATA_NUM 10000
using namespace std;

string trainFile = "train_data.txt";
int featuresNum;
const int MAXS = 8 * (FEATURE_NUM + 1)*TRAIN_DATA_NUM;
char buf[MAXS];
double features[TRAIN_DATA_NUM][FEATURE_NUM];
int labels[TRAIN_DATA_NUM];

void loadTrainData() {

	int fd = open("train_data.txt", O_RDONLY);
	const int data_sz = lseek(fd, 0, SEEK_END);
	char* buf = (char*)mmap(
	                NULL, data_sz, PROT_READ, MAP_PRIVATE, fd, 0);

	//FILE *fp;
	//fp = fopen(trainFile.c_str(), "rb");
	//int  = fread(buf, 1, MAXS, fp);
	//fclose(fp);

	int len = data_sz;

	//buf[len] = '\0';
	double position = 1.0;
	int symbol = 0;
	int point = 0;
	int i, col, row = 0;
	double line[FEATURE_NUM + 2] = { 0 };
	i = 0;
	for (char *p = buf; *p && p - buf < len; p++) {
		if (row >= TRAIN_DATA_NUM) {
			break;
		}

		if (*p == ',') {
			if (symbol == 1)
				line[i] = -line[i];
			i++;
			point = 0;
			position = 1.0;
			symbol = 0;
			line[i] = 0.0;
		} else if (*p == '\n') {
			if (symbol == 1)
				line[i] = -line[i];
			int ftf = (int)line[FEATURE_NUM];
			for (col = 0; col < FEATURE_NUM; col++) {
				features[row][col] = line[col];
			}
			labels[row] = ftf;
			row = row + 1;
			i = 0;
			point = 0;
			position = 1.0;
			symbol = 0;
			line[i] = 0.0;
		} else if (*p == '.') {
			point = 1;
		} else if (*p == '-') {
			symbol = 1;
		} else {
			if (point == 0)
				line[i] = line[i] * 10 + *p - '0';
			else {
				position = position*0.1;
				line[i] += position*(*p - '0');
			}
		}

	}
}

char outputSet[MAXS];
string output_data = "output_data.txt";
int storeData() {
	char *output = outputSet;
	char *output0 = output;
	featuresNum = 1000;
	for (int i = 0; i < 8000; i++) {
		for (int j = 0; j < featuresNum; j++) {
			memcpy(output, &(features[i][j]), sizeof(double));
			output += sizeof(double);
			*output = ',';
			output++;
		}
		memcpy(output, &(labels[i]), sizeof(int));
		output += sizeof(int);
		*output = '\n';
		output++;
	}
	int total = output-output0;
	//FILE *pf = fopen(output_data.c_str(), "w+");
	//fwrite(output0, 1, output-output0, pf);
	//fclose(pf);

	int fd = open(output_data.c_str(),O_RDWR|O_CREAT|O_TRUNC,0666);
	truncate(output_data.c_str(),total);
	void *dst_ptr=mmap(NULL,total,PROT_WRITE|PROT_READ,MAP_SHARED,fd,0);
	memcpy(dst_ptr,output0,total);
	return 0;
}

/*
int storeData1() {
	char *output = outputSet;
	char *output0 = output;
	int n;
	featuresNum = 1000;
	for (int i = 0; i < 8000; i++) {
		for (int j = 0; j < featuresNum; j++) {
			n = sprintf(output, "%0.3f,",features[i][j]);
			output += n;
		}
		n = sprintf(output, "%d\n",labels[i]);
		output += n;
	}
	FILE *pf = fopen(output_data.c_str(), "w+");
	fwrite(output0, 1, output-output0, pf);
	fclose(pf);
	return 0;
}
*/

int main(int argc, char* argv[]) {
	auto s1 = chrono::steady_clock::now();
	loadTrainData();
	auto s2 = chrono::steady_clock::now();
	cout << "time elapsed: " << chrono::duration_cast<chrono::duration<double>>(s2 - s1).count() << " s" << endl;
	storeData();
	auto s3 = chrono::steady_clock::now();
	cout << "time elapsed: " << chrono::duration_cast<chrono::duration<double>>(s3 - s2).count() << " s" << endl;
	cout << features[7999][999]<<endl;

}
```

mmap 只能在 linux 下使用，它读取并处理数据所用的时间为 0.15 s，写入数据到 txt 所用的时间为 0.1 s。

## 4.结论

综上所述，ifstream/ofstream 一秒钟可以处理 3 M的数据。

他们所用的时间大概是 fread/fwrite 的 100 倍，是 mmap 的 150 倍，这里多出来的 50 倍大概是我找的 linux 机器算得比较快。事实上，fread 和 mmap 差距不大，但是 mmap 只能在 linux 下使用，所以，一般没什么特殊要求，用 fread 就行了。

![img](https://pic1.zhimg.com/80/v2-493ce7676c4bb87291184633c944ba84_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/391123571

作者：linux

# 【NO.522】如何解决tcp通信中的粘包问题？

## 1. 粘包问题概述

### **1.1 描述背景**

采用TCP协议进行网络数据传送的软件设计中，普遍存在粘包问题。这主要是由于现代操作系统的网络传输机制所产生的。我们知道，网络通信采用的套接字(socket)技术，其实现实际是由系统内核提供一片连续缓存(流缓冲)来实现应用层程序与网卡接口之间的中转功能。多个数据包被连续存储于连续的缓存中，在对数据包进行读取时由于无法确定发生方的发送边界，而采用某一估测值大小来进行数据读出，若双方的size不一致时就会使数据包的边界发生错位，导致读出错误的数据分包，进而曲解原始数据含义。

### **1.2 粘包的概念**

粘包问题的本质就是数据读取边界错误所致，通过下图可以形象地理解其现象。

![img](https://pic2.zhimg.com/80/v2-270c5a2cabc679d311c67d77aafa811d_720w.webp)

如图1所示，当前的socket缓存中已经有6个数据分组到达，其大小如图中数字。而应用程序在对数据进行收取时(如图2)，采用了300字节的要求去读取，则会误将pkg1和pkg2一起收走当做一个包来处理。而实际上，很可能pkg1是一个文本文件的内容，而pkg2则可能是一个音频内容，这风马牛不相及的两个数据包却被揉进一个包进行处理，显然有失妥当。严重时可能因为丢了pkg2而导致软件陷入异常分支产生乌龙事件。

因此，粘包问题必须引起所有软件设计者（项目经理）的高度重视！

那么，或许会有读者发问，为何不让接收程序按照100字节来读取呢？我想如果您了解一些TCP编程的话就不会有这样的问题。网络通信程序中，数据包通常是不能确定大小的，尤其在软件设计阶段无法真的做到确定为一个固定值。比如聊天软件客户端若采用TCP传输一个用户名和密码到服务端进行验证登陆，我想这个数据包不过是几十字节，至多几百字节即可发送完毕，而有时候要传输一个很大的视频文件，即使分包发送也应该一个包在几千字节吧。（据说，某国电信平台的MW中见到过一次发送1.5万字节的电话数据）这种情况下，发送数据的分包大小无法固定，接收端也就无法固定。所以一般采用一个较为合理的预估值进行轮询接收。（网卡的MTU都是1500字节，因此这个预估值一般为MTU的1~3倍）。

相信读者对粘包问题应该有了初步认识了。

## 2.粘包回避设计

### **2.1 设计方案一：定长发送**

在进行数据发送时采用固定长度的设计，也就是无论多大数据发送都分包为固定长度（为便于描述，此处定长为记为LEN），也就是发送端在发送数据时都以LEN为长度进行分包。这样接收方都以固定的LEN进行接收，如此一来发送和接收就能一一对应了。分包的时候不一定能完整的恰好分成多个完整的LEN的包，最后一个包一般都会小于LEN，这时候最后一个包可以在不足的部分填充空白字节。

当然，这种方法会有缺陷。1.最后一个包的不足长度被填充为空白部分，也即无效字节序。那么接收方可能难以辨别这无效的部分，它本身就是为了补位的，并无实际含义。这就为接收端处理其含义带来了麻烦。当然也有解决办法，可以通过增添标志位的方法来弥补，即在每一个数据包的最前面增加一个定长的报头，然后将该数据包的末尾标记一并发送。接收方根据这个标记确认无效字节序列，从而实现数据的完整接收。2.在发送包长度随机分布的情况下，会造成带宽浪费。比如发送长度可能为 1,100,1000,4000字节等等，则都需要按照定长最大值即4000来发送，数据包小于4000字节的其他包也会被填充至4000，造成网络负载的无效浪费。

综上，此方案适在发送数据包长度较为稳定(趋于某一固定值)的情况下有较好的效果。

### 2.2 **设计方案二：尾部标记序列**

在每个要发送的数据包的尾部设置一个特殊的字节序列，此序列带有特殊含义，跟字符串的结束符标识”\0”一样的含义，用来标示这个数据包的末尾，接收方可对接收的数据进行分析，通过尾部序列确认数据包的边界。

这种方法的缺陷较为明显：1.接收方需要对数据进行分析，甄别尾部序列。2.尾部序列的确定本身是一个问题。什么样的序列可以向”\0”一样来做一个结束符呢？这个序列必须是不具备通常任何人类或者程序可识别的带含义的数据序列，就像“\0”是一个无效字符串内容，因而可以作为字符串的结束标记。那普通的网络通信中，这个序列是什么呢？我想一时间很难找到恰当的答案。

### 2.3 **设计方案三：头部标记分步接收**

这个方法是作者有限学识里最好的办法了。它既不损失效率，还完美解决了任何大小的数据包的边界问题。

这个方法的实现是这样的，定义一个用户报头，在报头中注明每次发送的数据包大小。接收方每次接收时先以报头的size进行数据读取，这必然只能读到一个报头的数据，从报头中得到该数据包的数据大小，然后再按照此大小进行再次读取，就能读到数据的内容了。这样一来，每个数据包发送时都封装一个报头，然后接收方分两次接收一个包，第一次接收报头，根据报头大小第二次才接收数据内容。（此处的data[0]的本质是一个指针，指向数据的正文部分，也可以是一篇连续数据区的起始位置。因此可以设计成data[user_size]，这样的话。）

下面通过一个图来展现设计思想。

![img](https://pic2.zhimg.com/80/v2-2a8a4e08d266e6565e2869d01404bcc9_720w.webp)

由图看出，数据发送多了封装报头的动作；接收方将每个包的接收拆分成了两次。

这方案看似精妙，实则也有缺陷：1.报头虽小，但每个包都需要多封装sizeof(_data_head)的数据，积累效应也不可完全忽略。2.接收方的接收动作分成了两次，也就是进行数据读取的操作被增加了一倍，而数据读取操作的recv或者read都是系统调用，这对内核而言的开销是一个不能完全忽略的影响，对程序而言性能影响可忽略（系统调用的速度非常快）。

优点：避免了程序设计的复杂性，其有效性便于验证，对软件设计的稳定性要求来说更容易达标。综上，方案三乃上策！

## 3.**补充：**

### **3.1 什么时候需要考虑粘包问题?**

1:如果利用tcp每次发送数据，就与对方建立连接，然后双方发送完一段数据后，就关闭连接，这样就不会出现粘包问题（因为只有一种包结构,类似于http协议）。关闭连接主要要双方都发送close连接（参考tcp关闭协议）。如：A需要发送一段字符串给B，那么A与B建立连接，然后发送双方都默认好的协议字符如"hello give me sth abour yourself"，然后B收到报文后，就将缓冲区数据接收,然后关闭连接，这样粘包问题不用考虑到，因为大家都知道是发送一段字符。

2：如果发送数据无结构，如文件传输，这样发送方只管发送，接收方只管接收存储就ok，也不用考虑粘包

3：如果双方建立连接，需要在连接后一段时间内发送不同结构数据，如连接后，有好几种结构：

1)"hello give me sth abour yourself"

2)"Don't give me sth abour yourself"

那这样的话，如果发送方连续发送这个两个包出去，接收方一次接收可能会是"hello give me sth abour yourselfDon't give me sth abour yourself" 这样接收方就傻了，到底是要干嘛？不知道，因为协议没有规定这么诡异的字符串，所以要处理把它分包，怎么分也需要双方组织一个比较好的包结构，所以一般可能会在头加一个数据长度之类的包，以确保接收。

### 3.2 **粘包出现原因：在流传输中出现，UDP不会出现粘包，因为它有消息边界**

1 发送端需要等缓冲区满才发送出去，造成粘包
2 接收方不及时接收缓冲区的包，造成多个包接收

解决办法：

为了避免粘包现象，可采取以下几种措施。

- 一是对于发送方引起的粘包现象，用户可通过编程设置来避免，TCP提供了强制数据立即传送的操作指令push，TCP软件收到该操作指令后，就立即将本段数据发送出去，而不必等待发送缓冲区满；
- 二是对于接收方引起的粘包，则可通过优化程序设计、精简接收进程工作量、提高接收进程优先级等措施，使其及时接收数据，从而尽量避免出现粘包现象；
- 三是由接收方控制，将一包数据按结构字段，人为控制分多次接收，然后合并，通过这种手段来避免粘包。

以上提到的三种措施，都有其不足之处。第一种编程设置方法虽然可以避免发送方引起的粘包，但它关闭了优化[算法](https://link.zhihu.com/?target=http%3A//lib.csdn.net/base/datastructure)，降低了网络发送效率，影响应用程序的性能，一般不建议使用。第二种方法只能减少出现粘包的可能性，但并不能完全避免粘包，当发送频率较高时，或由于网络突发可能使某个时间段数据包到达接收方较快，接收方还是有可能来不及接收，从而导致粘包。第三种方法虽然避免了粘包，但应用程序的效率较低，对实时应用的场合不适合。

### 3.3 **为什么基于TCP的通讯程序需要进行封包和拆包**

TCP是个"流"协议,所谓流,就是没有界限的一串数据.大家可以想想河里的流水,是连成一片的,其间是没有分界线的.但一般通讯程序开发是需要定义一个个相互独立的数据包的,比如用于登陆的数据包,用于注销的数据包.由于TCP"流"的特性以及网络状况,在进行数据传输时会出现以下几种情况.

假设我们连续调用两次send分别发送两段数据data1和data2,在接收端有以下几种接收情况(当然不止这几种情况,这里只列出了有代表性的情况).
A.先接收到data1,然后接收到data2.
B.先接收到data1的部分数据,然后接收到data1余下的部分以及data2的全部.
C.先接收到了data1的全部数据和data2的部分数据,然后接收到了data2的余下的数据.
D.一次性接收到了data1和data2的全部数据.

对于A这种情况正是我们需要的,不再做讨论.对于B,C,D的情况就是大家经常说的"粘包",就需要我们把接收到的数据进行拆包,拆成一个个独立的数据包.为了拆包就必须在发送端进行封包.

另:对于UDP来说就不存在拆包的问题,因为UDP是个"数据包"协议,也就是两段数据间是有界限的,在接收端要么接收不到数据要么就是接收一个完整的一段数据,不会少接收也不会多接收.

### 3.4 **为什么会出现B.C.D的情况**

"粘包"可发生在发送端也可发生在接收端.

1.由Nagle算法造成的发送端的粘包:Nagle算法是一种改善网络传输效率的算法.简单的说,当我们提交一段数据给TCP发送时,TCP并不立刻发送此段数据,而是等待一小段时间,看看在等待期间是否还有要发送的数据,若有则会一次把这两段数据发送出去.这是对Nagle算法一个简单的解释,详细的请看相关书籍.象C和D的情况就有可能是Nagle算法造成的.

2.接收端接收不及时造成的接收端粘包:TCP会把接收到的数据存在自己的缓冲区中,然后通知应用层取数据.当应用层由于某些原因不能及时的把TCP的数据取出来,就会造成TCP缓冲区中存放了几段数据.

### 3.5 **怎样封包和拆包**

最初遇到"粘包"的问题时,我是通过在两次send之间调用sleep来休眠一小段时间来解决.这个解决方法的缺点是显而易见的,使传输效率大大降低,而且也并不可靠.后来就是通过应答的方式来解决,尽管在大多数时候是可行的,但是不能解决象B的那种情况,而且采用应答方式增加了通讯量,加重了网络负荷. 再后来就是对数据包进行封包和拆包的操作.

封包:
封包就是给一段数据加上包头,这样一来数据包就分为包头和包体两部分内容了(以后讲过滤非法包时封包会加入"包尾"内容).包头其实上是个大小固定的结构体,其中有个结构体成员变量表示包体的长度,这是个很重要的变量,其他的结构体成员可根据需要自己定义.根据包头长度固定以及包头中含有包体长度的变量就能正确的拆分出一个完整的数据包.

对于拆包目前我最常用的是以下两种方式.
1.动态缓冲区暂存方式.之所以说缓冲区是动态的是因为当需要缓冲的数据长度超出缓冲区的长度时会增大缓冲区长度.
大概过程描述如下:
A,为每一个连接动态分配一个缓冲区,同时把此缓冲区和SOCKET关联,常用的是通过结构体关联.
B,当接收到数据时首先把此段数据存放在缓冲区中.
C,判断缓存区中的数据长度是否够一个包头的长度,如不够,则不进行拆包操作.
D,根据包头数据解析出里面代表包体长度的变量.
E,判断缓存区中除包头外的数据长度是否够一个包体的长度,如不够,则不进行拆包操作.
F,取出整个数据包.这里的"取"的意思是不光从缓冲区中拷贝出数据包,而且要把此数据包从缓存区中删除掉.删除的办法就是把此包后面的数据移动到缓冲区的起始地址.

这种方法有两个缺点.1.为每个连接动态分配一个缓冲区增大了内存的使用.2.有三个地方需要拷贝数据,一个地方是把数据存放在缓冲区,一个地方是把完整的数据包从缓冲区取出来,一个地方是把数据包从缓冲区中删除.第二种拆包的方法会解决和完善这些缺点.

前面提到过这种方法的缺点.下面给出一个改进办法, 即采用环形缓冲.但是这种改进方法还是不能解决第一个缺点以及第一个数据拷贝,只能解决第三个地方的数据拷贝(这个地方是拷贝数据最多的地方).第2种拆包方式会解决这两个问题.
环形缓冲实现方案是定义两个指针,分别指向有效数据的头和尾.在存放数据和删除数据时只是进行头尾指针的移动.

2.利用底层的缓冲区来进行拆包
由于TCP也维护了一个缓冲区,所以我们完全可以利用TCP的缓冲区来缓存我们的数据,这样一来就不需要为每一个连接分配一个缓冲区了.另一方面我们知道recv或者wsarecv都有一个参数,用来表示我们要接收多长长度的数据.利用这两个条件我们就可以对第一种方法进行优化.
对于阻塞SOCKET来说,我们可以利用一个循环来接收包头长度的数据,然后解析出代表包体长度的那个变量,再用一个循环来接收包体长度的数据.
相关代码如下:

```text
char PackageHead[1024];
char PackageContext[1024*20];

int len;
PACKAGE_HEAD *pPackageHead;
while( m_bClose == false )
{
memset(PackageHead,0,sizeof(PACKAGE_HEAD));
len = m_TcpSock.ReceiveSize((char*)PackageHead,sizeof(PACKAGE_HEAD));
if( len == SOCKET_ERROR )
{
    break;
}
if(len == 0)
{
    break;
}
pPackageHead = (PACKAGE_HEAD *)PackageHead;
memset(PackageContext,0,sizeof(PackageContext));
if(pPackageHead->nDataLen>0)
{
len = m_TcpSock.ReceiveSize((char*)PackageContext,pPackageHead->nDataLen);
}
        }
```

m_TcpSock是一个封装了SOCKET的类的变量,其中的ReceiveSize用于接收一定长度的数据,直到接收了一定长度的数据或者网络出错才返回.

```text
int winSocket::ReceiveSize( char* strData, int iLen )
{
if( strData == NULL )
return ERR_BADPARAM;
char *p = strData;
int len = iLen;
int ret = 0;
int returnlen = 0;
while( len > 0)
{
ret = recv( m_hSocket, p+(iLen-len), iLen-returnlen, 0 );
if ( ret == SOCKET_ERROR || ret == 0 )
{
return ret;
}

len -= ret;
returnlen += ret;
}

return returnlen;
}
```

对于非阻塞的SOCKET,比如完成端口,我们可以提交接收包头长度的数据的请求,当 GetQueuedCompletionStatus返回时,我们判断接收的数据长度是否等于包头长度,若等于,则提交接收包体长度的数据的请求,若不等于则提交接收剩余数据的请求.当接收包体时,采用类似的方法.

原文地址：https://zhuanlan.zhihu.com/p/394695773

作者：linux

# 【NO.523】多线程还是多进程的选择及区别

**鱼还是熊掌：浅谈多进程多线程的选择**

关于多进程和多线程，教科书上最经典的一句话是“进程是资源分配的最小单位，线程是CPU调度的最小单位”，这句话应付考试基本上够了，但如果在工作中遇到类似的选择问题，那就没有这么简单了，选的不好，会让你深受其害。

经常在网络上看到有的XDJM问“多进程好还是多线程好？”、“Linux下用多进程还是多线程？”等等期望一劳永逸的问题，我只能说：没有最好，只有更好。根据实际情况来判断，哪个更加合适就是哪个好。

我们按照多个不同的维度，来看看多线程和多进程的对比（注：因为是感性的比较，因此都是相对的，不是说一个好得不得了，另外一个差的无法忍受）。

![img](https://pic4.zhimg.com/80/v2-55ed14ca36f32e693f441de0a7cd225f_720w.webp)

看起来比较简单，优势对比上是“线程 3.5 v 2.5 进程”，我们只管选线程就是了？

呵呵，有这么简单我就不用在这里浪费口舌了，还是那句话，没有绝对的好与坏，只有哪个更加合适的问题。我们来看实际应用中究竟如何判断更加合适。

**1）需要频繁创建销毁的优先用线程**

原因请看上面的对比。

这种原则最常见的应用就是Web服务器了，来一个连接建立一个线程，断了就销毁线程，要是用进程，创建和销毁的代价是很难承受的

**2）需要进行大量计算的优先使用线程**

所谓大量计算，当然就是要耗费很多CPU，切换频繁了，这种情况下线程是最合适的。

这种原则最常见的是图像处理、算法处理。

**3）强相关的处理用线程，弱相关的处理用进程**

什么叫强相关、弱相关？理论上很难定义，给个简单的例子就明白了。

一般的Server需要完成如下任务：消息收发、消息处理。“消息收发”和“消息处理”就是弱相关的任务，而“消息处理”里面可能又分为“消息解码”、“业务处理”，这两个任务相对来说相关性就要强多了。因此“消息收发”和“消息处理”可以分进程设计，“消息解码”、“业务处理”可以分线程设计。

当然这种划分方式不是一成不变的，也可以根据实际情况进行调整。

**4）可能要扩展到多机分布的用进程，多核分布的用线程**

原因请看上面对比。

**5）都满足需求的情况下，用你最熟悉、最拿手的方式**

至于“数据共享、同步”、“编程、调试”、“可靠性”这几个维度的所谓的“复杂、简单”应该怎么取舍，我只能说：没有明确的选择方法。但我可以告诉你一个选择原则：如果多进程和多线程都能够满足要求，那么选择你最熟悉、最拿手的那个。

需要提醒的是：虽然我给了这么多的选择原则，但实际应用中基本上都是“进程+线程”的结合方式，千万不要真的陷入一种非此即彼的误区。

**进程与线程**

进程是程序执行时的一个实例，即它是程序已经执行到课中程度的数据结构的汇集。从内核的观点看，进程的目的就是担当分配系统资源（CPU时间、内存等）的基本单位。

线程是进程的一个执行流，是CPU调度和分派的基本单位，它是比进程更小的能独立运行的基本单位。一个进程由几个线程组成（拥有很多相对独立的执行流的用户程序共享应用程序的大部分数据结构），线程与同属一个进程的其他的线程共享进程所拥有的全部资源。

> "进程——资源分配的最小单位，线程——程序执行的最小单位"

进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。

总的来说就是：进程有独立的地址空间，线程没有单独的地址空间（同一进程内的线程共享进程的地址空间）。

使用多线程的**理由之一**是和进程相比，它是一种非常"节俭"的多任务操作方式。我们知道，在Linux系统下，启动一个新的进程必须分配给它独立的地址空间，建立众多的数据表来维护它的代码段、堆栈段和数据段，这是一种"昂贵"的多任务工作方式。而运行于一个进程中的多个线程，它们彼此之间使用相同的地址空间，共享大部分数据，启动一个线程所花费的空间远远小于启动一个进程所花费的空间，而且，线程间彼此切换所需的时间也远远小于进程间切换所需要的时间。据统计，总的说来，一个进程的开销大约是一个线程开销的30倍左右，当然，在具体的系统上，这个数据可能会有较大的区别。

使用多线程的**理由之二**是线程间方便的通信机制。对不同进程来说，它们具有独立的数据空间，要进行数据的传递只能通过通信的方式进行，这种方式不仅费时，而且很不方便。线程则不然，由于同一进程下的线程之间共享数据空间，所以一个线程的数据可以直接为其它线程所用，这不仅快捷，而且方便。当然，数据的共享也带来其他一些问题，有的变量不能同时被两个线程所修改，有的子程序中声明为static的数据更有可能给多线程程序带来灾难性的打击，这些正是编写多线程程序时最需要注意的地方。

除了以上所说的优点外，不和进程比较，多线程程序作为一种多任务、并发的工作方式，当然有以下的优点：

- 提高应用程序响应。这对图形界面的程序尤其有意义，当一个操作耗时很长时，整个系统都会等待这个操作，此时程序不会响应键盘、鼠标、菜单的操作，而使用多线程技术，将耗时长的操作（time consuming）置于一个新的线程，可以避免这种尴尬的情况。
- 使多CPU系统更加有效。操作系统会保证当线程数不大于CPU数目时，不同的线程运行于不同的CPU上。
- 改善程序结构。一个既长又复杂的进程可以考虑分为多个线程，成为几个独立或半独立的运行部分，这样的程序会利于理解和修改。

在linux上编程采用多线程还是多进程的争执由来已久，这种争执最常见到在B/S通讯中服务端并发技术 的选型上，比如WEB服务器技术中，Apache是采用多进程的（perfork模式，每客户连接对应一个进程，每进程中只存在唯一一个执行线 程）。

从Unix发展历史看，伴随着Unix的诞生多进程就出现了，而多线程很晚才被系统支持，例如Linux直到内核2.6，才支持符合Posix规范的NPTL线程库。进程和线程的特点，也就是各自的优缺点如下：

> 进程优点：编程、调试简单，可靠性较高。
> 进程缺点：创建、销毁、切换速度慢，内存、资源占用大。
> 线程优点：创建、销毁、切换速度快，内存、资源占用小。
> 线程缺点：编程、调试复杂，可靠性较差。

上面的对比可以归结为一句话：“线程快而进程可靠性高”。线程有个别名叫“轻量级进程”，在有的书籍资料上介绍线程可以十倍、百倍的效率快于进程； 而进程之间不共享数据，没有锁问题，结构简单，一个进程崩溃不像线程那样影响全局，因此比较可靠。我相信这个观点可以被大部分人所接受，因为和我们所接受的知识概念是相符的。

在写这篇文章前，我也属于这“大部分人”，这两年在用C语言编写的几个C/S通讯程序中，因时间紧总是采用多进程并发技术，而且是比较简单的现场为 每客户fork()一个进程，当时总是担心并发量增大时负荷能否承受，盘算着等时间充裕了将它改为多线程形式，或者改为预先创建进程的形式，直到最近在网上看到了一篇论文《Linux系统下多线程与多进程性能分析》才认真思考这个问题，我自己也做了实验。

下面是得出结论的实验步骤和过程，结论究竟是怎样的？ 感兴趣就一起看看吧。

实验代码使用论文中的代码样例，做了少量修改，值得注意的是这样的区别：

> 论文实验和我的实验时间不同，论文所处的年代linux内核是2.4，我的实验linux内核是2.6，2.6使用的线程库是NPTL，2.4使用的是老的Linux线程库（用进程模拟线程的那个LinuxThread）。
> 论文实验和我用的机器不同，论文描述了使用的环境：单cpu 机器基本配置为:celeron 2.0 GZ, 256M, Linux 9.2,内核 2.4.8。我的环境是：双核 Intel(R) Xeon(R) CPU 5130 @ 2.00GHz（做实验时，禁掉了一核），512MG内存，Red Hat Enterprise Linux ES release 4 (Nahant Update 4)，内核2.6.9-42。

进程实验代码（fork.c）：

```text
#include <stdlib.h>
#include <stdio.h>
#include <signal.h>

#define P_NUMBER 255 //并发进程数量
#define COUNT 5 //每次进程打印字符串数
#define TEST_LOGFILE "logFile.log"
FILE *logFile=NULL;

char *s="hello linux\0";

int main()
{
    int i=0,j=0;
    logFile=fopen(TEST_LOGFILE,"a+");//打开日志文件
    for(i=0;i<P_NUMBER;i++)
    {
        if(fork()==0)//创建子进程，if(fork()==0){}这段代码是子进程运行区间
        {
            for(j=0;j<COUNT;j++)
            {
                printf("[%d]%s\n",j,s);//向控制台输出
                /*当你频繁读写文件的时候，Linux内核为了提高读写性能与速度，会将文件在内存中进行缓存，这部分内存就是Cache Memory(缓存内存)。可能导致测试结果不准，所以在此注释*/
                //fprintf(logFile,"[%d]%s\n",j,s);//向日志文件输出，
            }
            exit(0);//子进程结束
        }
    }
    
    for(i=0;i<P_NUMBER;i++)//回收子进程
    {
        wait(0);
    }
    
    printf("Okay\n");
    return 0;
}
```

线程实验代码（thread.c）：

```text
#include <pthread.h>
#include <unistd.h>
#include <stdlib.h>
#include <stdio.h>

#define P_NUMBER 255//并发线程数量
#define COUNT 5 //每线程打印字符串数
#define TEST_LOG "logFile.log"
FILE *logFile=NULL;

char *s="hello linux\0";

print_hello_linux()//线程执行的函数
{
    int i=0;
    for(i=0;i<COUNT;i++)
    {
        printf("[%d]%s\n",i,s);//想控制台输出
        /*当你频繁读写文件的时候，Linux内核为了提高读写性能与速度，会将文件在内存中进行缓存，这部分内存就是Cache Memory(缓存内存)。可能导致测试结果不准，所以在此注释*/
        //fprintf(logFile,"[%d]%s\n",i,s);//向日志文件输出
    }
    pthread_exit(0);//线程结束
}

int main()
{
    int i=0;
    pthread_t pid[P_NUMBER];//线程数组
    logFile=fopen(TEST_LOG,"a+");//打开日志文件
    
    for(i=0;i<P_NUMBER;i++)
        pthread_create(&pid[i],NULL,(void *)print_hello_linux,NULL);//创建线程
        
    for(i=0;i<P_NUMBER;i++)
        pthread_join(pid[i],NULL);//回收线程
        
    printf("Okay\n");
    return 0;
}
```

两段程序做的事情是一样的，都是创建“若干”个进程/线程，每个创建出的进程/线程打印“若干”条“hello linux”字符串到控制台和日志文件，两个“若干”由两个宏 P_NUMBER和COUNT分别定义，程序编译指令如下：

> gcc -o fork fork.c
> gcc -lpthread -o thread thread.c

实验通过time指令执行两个程序，抄录time输出的挂钟时间（real时间）：

> time ./fork
> time ./thread

每批次的实验通过改动宏 P_NUMBER和COUNT来调整进程/线程数量和打印次数，每批次测试五轮，得到的结果如下：

**一、重复论文实验步骤**

**(注：本文平均值算法采用的是去掉一个最大值去掉一个最小值，然后平均)**

![img](https://pic1.zhimg.com/80/v2-1db70ff34894c2842deaff54536ed21c_720w.webp)

![img](https://pic1.zhimg.com/80/v2-62696a994debe07dd598ba9ee37f6968_720w.webp)

![img](https://pic3.zhimg.com/80/v2-8b8fa5f61dbf81323c1f60b475e3c672_720w.webp)

![img](https://pic4.zhimg.com/80/v2-d738d0aa346fdf93633d108d5c2b0837_720w.webp)

![img](https://pic4.zhimg.com/80/v2-7151bfc65aa35bc9d17ee37261edcf7b_720w.webp)

![img](https://pic2.zhimg.com/80/v2-2d34d995d9dad1e895cc591c7128cf21_720w.webp)

![img](https://pic2.zhimg.com/80/v2-09b47cce7a97e2eb133f06084ff99399_720w.webp)

![img](https://pic2.zhimg.com/80/v2-ac9a47de97b930896e8e94562cc14df9_720w.webp)

本轮实验是为了和论文作对比，因此将进程/线程数量限制在255个，论文也是测试了255个进程/线程分别进行5次，10 次,50 次,100 次,500 次……10000 次打印的用时，论文得出的结果是：**任务量较大时,多进程比多线程效率高;而完成的任务量较小时,多线程比多进程要快，重复打印 600 次时,多进程与多线程所耗费的时间相同。**

虽然我的实验直到1000打印次数时，多进程才开始领先，但考虑到使用的是NPTL线程库的缘故，从而可以证实了论文的观点。从我的实验数据看，多线程和多进程两组数据非常接近，考虑到数据的提取具有瞬间性，因此可以认为他们的速度是相同的。

是不是可以得出这样的结论：**多线程创建、销毁速度快，而多线程切换速度快，**这个结论我们会在第二个试验中继续试图验证

当前的网络环境中，我们更看中高并发、高负荷下的性能，纵观前面的实验步骤，最长的实验周期不过2分钟多一点，因此下面的实验将向两个方向延伸，第一，增加并发数量，第二，增加每进程/线程的工作强度。

**二、增加并发数量的实验**

下面的实验打印次数不变，而进程/线程数量逐渐增加。在实验过程中多线程程序在后四组（线程数350，500，800,1000）的测试中都出现了“段错误”，出现错误的原因和多线程预分配线程栈有关。

实验中的计算机CPU是32位，寻址最大范围是4GB（2的32次方），Linux是按照3GB/1GB的方式来分配内存，其中1GB属于所有进程共享的内核空间，3GB属于用户空间（进程虚拟内存空间）。Linux2.6的默认线程栈大小是8M（通过ulimit -a查看），对于多线程，在创建线程的时候系统会为每一个线程预分配线程栈地址空间，也就是8M的虚拟内存空间。线程数量太多时，线程栈累计的大小将超过进程虚拟内存空间大小（计算时需要排除程序文本、数据、共享库等占用的空间），这就是实验中出现的“段错误”的原因。

Linux2.6的默认线程栈大小可以通过 ulimit -s 命令查看或修改，我们可以计算出线程数的最大上线: (1024*1024*1024*3) / (1024*1024*8) = 384，实际数字应该略小与384，因为还要计算程序文本、数据、共享库等占用的空间。在当今的稍显繁忙的WEB服务器上，突破384的并发访问并不是稀 罕的事情，要继续下面的实验需要将默认线程栈的大小减小，但这样做有一定的风险，比如线程中的函数分配了大量的自动变量或者函数涉及很深的栈帧（典型的是 递归调用），线程栈就可能不够用了。可以配合使用POSIX.1规定的两个线程属性guardsize和stackaddr来解决线程栈溢出问 题，guardsize控制着线程栈末尾之后的一篇内存区域，一旦线程栈在使用中溢出并到达了这片内存，程序可以捕获系统内核发出的告警信号，然后使用 malloc获取另外的内存，并通过stackaddr改变线程栈的位置，以获得额外的栈空间，这个动态扩展栈空间办法需要手工编程，而且非常麻烦。

有两种方法可以改变线程栈的大小，使用 ulimit -s 命令改变系统默认线程栈的大小，或者在代码中创建线程时通过pthread_attr_setstacksize函数改变栈尺寸，在实验中使用的是第一种，在程序运行前先执行ulimit指令将默认线程栈大小改为1M：

> ulimit -s 1024
> time ./thread

![img](https://pic4.zhimg.com/80/v2-f12f33e6a39a0eaa3caa0f1a8f071db7_720w.webp)

![img](https://pic4.zhimg.com/80/v2-8c04b599c9129cec1076c9be7d81db03_720w.webp)

![img](https://pic3.zhimg.com/80/v2-f946486c7f0d8b44dfaef9924ad0b1ca_720w.webp)

![img](https://pic1.zhimg.com/80/v2-da5d1f70d93ce95aab159441f1bf74cc_720w.webp)

![img](https://pic2.zhimg.com/80/v2-b9e63fcd9f7a3802eecc431202ce9099_720w.webp)

![img](https://pic2.zhimg.com/80/v2-7a7123c645e2162a3b2bcaa2448a4b75_720w.webp)

【实验结论】

当线程/进程逐渐增多时，执行相同任务时，线程所花费时间相对于进程有下降的趋势（本人怀疑后两组数据受系统其他瓶颈的影响），这是不是进一步验证了多线程创建、销毁速度快，而多进程切换速度快。

**三、增加每进程/线程的工作强度的实验**

这次将程序打印数据增大，原来打印字符串为：

```text
char *s = "hello linux\0";
```

现在修改为每次打印256个字节数据:

```text
char *s = "1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\
    1234567890abcdef\0";
```

![img](https://pic3.zhimg.com/80/v2-37068d10e2dfca1130b11ab371d6b7f2_720w.webp)

![img](https://pic1.zhimg.com/80/v2-318a7eeeee8e8aff464fde1332b1765c_720w.webp)

![img](https://pic3.zhimg.com/80/v2-46d699f94f23150dc996d1587e77fdce_720w.webp)

【实验结论】

从上面的实验比对结果看，即使Linux2.6使用了NPTL线程库，多线程比较多进程在效率上没有任何的优势，在线程数增大时多线程程序还出现了运行错误，实验可以得出下面的结论：

> 在Linux2.6上，多线程并不比多进程速度快，考虑到线程栈的问题，多进程在并发上有优势。

**四、多进程和多线程在创建和销毁上的效率比较**

预先创建进程或线程可以节省进程或线程的创建、销毁时间，在实际的应用中很多程序使用了这样的策略，比如Apapche预先创建进程、Tomcat 预先创建线程，通常叫做进程池或线程池。在大部分人的概念中，进程或线程的创建、销毁是比较耗时的，在stevesn的著作《Unix网络编程》中有这样 的对比图（第一卷 第三版 30章 客户/服务器程序设计范式）：

![img](https://pic1.zhimg.com/80/v2-419f80771bd6bf01603e9dfb70115c68_720w.webp)

stevens已驾鹤西去多年，但《Unix网络编程》一书仍具有巨大的影响力，上表中stevens比较了三种服务器上多进程和多线程的执行效 率，因为三种服务器所用计算机不同，表中数据只能纵向比较，而横向无可比性，stevens在书中提供了这些测试程序的源码（也可以在网上下载）。书中介 绍了测试环境，两台与服务器处于同一子网的客户机，每个客户并发5个进程（服务器同一时间最多10个连接），每个客户请求从服务器获取4000字节数据， 预先派生子进程或线程的数量是15个。

第0行是迭代模式的基准测试程序，服务器程序只有一个进程在运行（同一时间只能处理一个客户请求），因为没有进程或线程的调度切换，因此它的速度是 最快的，表中其他服务模式的运行数值是比迭代模式多出的差值。迭代模式很少用到，在现有的互联网服务中，DNS、NTP服务有它的影子。第1～5行是多进 程服务模式，期中第1行使用现场fork子进程，2～5行都是预先创建15个子进程模式，在多进程程序中套接字传递不太容易（相对于多线 程），stevens在这里提供了4个不同的处理accept的方法。6～8行是多线程服务模式，第6行是现场为客户请求创建子线程，7～8行是预先创建 15个线程。表中有的格子是空白的，是因为这个系统不支持此种模式，比如当年的BSD不支持线程，因此BSD上多线程的数据都是空白的。

从数据的比对看，现场为每客户fork一个进程的方式是最慢的，差不多有20倍的速度差异，Solaris上的现场fork和预先创建子进程的最大差别是504.2 ：21.5，但我们不能理解为预先创建模式比现场fork快20倍，原因有两个：

\1. stevens的测试已是十几年前的了，现在的OS和CPU已起了翻天覆地的变化，表中的数值需要重新测试。

\2. stevens没有提供服务器程序整体的运行计时，我们无法理解504.2 ：21.5的实际运行效率，有可能是1504.2 : 1021.5，也可能是100503.2 : 100021.5，20倍的差异可能很大，也可能可以忽略。

因此我写了下面的实验程序，来计算在Linux2.6上创建、销毁10万个进程/线程的绝对用时。

创建10万个进程（forkcreat.c）:

```text
#include <stdio.h>
#include <signal.h>
#include <stdio.h>
#include <unistd.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <sys/types.h>
#include <sys/wait.h>

int count;//子进程创建成功数量 
int fcount;//子进程创建失败数量 
int scount;//子进程回收数量 

/*信号处理函数–子进程关闭收集*/
void sig_chld(int signo)
{
    pid_t chldpid;//子进程id
    int stat;//子进程的终止状态
    
    //子进程回收，避免出现僵尸进程
    while((chldpid=wait(&stat)>0))
    {
        scount++;
    }
}

int main()
{
    //注册子进程回收信号处理函数
    signal(SIGCHLD,sig_chld);
    
    int i;
    for(i=0;i<100000;i++)//fork()10万个子进程
    {
        pid_t pid=fork();
        if(pid==-1)//子进程创建失败
        {
            fcount++;
        }
        else if(pid>0)//子进程创建成功
        {
            count++;
        }
        else if(pid==0)//子进程执行过程
        {
            exit(0);
        }
    }
    
    printf("count:%d fount:%d scount:%d\n",count,fcount,scount);
}
```

创建10万个线程（pthreadcreat.c）:

```text
#include <stdio.h>
#include <pthread.h>

int count=0;//成功创建线程数量

void thread(void)
{
    //啥也不做
}

int main(void)
{
    pthread_t id;//线程id
    int i,ret;
    
    for(i=0;i<100000;i++)//创建10万个线程
    {
        ret=pthread_create(&id,NULL,(void *)thread,NULL);
        if(ret!=0)
        {
            printf("Create pthread error!\n");
            return(1);
        }
        count++;
        pthread_join(id,NULL);
    }
    
    printf("count:%d\n",count);
}
```

![img](https://pic2.zhimg.com/80/v2-e4afb6ab7e86dc046d3d7bda65bb65a1_720w.webp)

从数据可以看出，多线程比多进程在效率上有10多倍的优势，但不能让我们在使用哪种并发模式上定性，这让我想起多年前政治课上的一个场景：在讲到优越性时，面对着几个对此发表质疑评论的调皮男生，我们的政治老师发表了高见，“不能只横向地和当今的发达国家比，你应该纵向地和过去中国几十年的发展历史 比”。政治老师的话套用在当前简直就是真理，我们看看，即使是在赛扬CPU上，创建、销毁进程/线程的速度都是空前的，可以说是有质的飞跃的，平均创建销毁一个进程的速度是0.18毫秒，对于当前服务器几百、几千的并发量，还有预先派生子进程/线程的必要吗？

预先派生子进程/线程比现场创建子进程/线程要复杂很多，不仅要对池中进程/线程数量进行动态管理，还要解决多进程/多线程对accept的“抢” 问题，在stevens的测试程序中，使用了“惊群”和“锁”技术。即使stevens的数据表格中，预先派生线程也不见得比现场创建线程快，在 《Unix网络编程》第三版中，新作者参照stevens的测试也提供了一组数据，在这组数据中，现场创建线程模式比预先派生线程模式已有了效率上的优势。因此我对这一节实验下的结论是：

> 预先派生进程/线程的模式（进程池、线程池）技术，不仅复杂，在效率上也无优势，在新的应用中可以放心大胆地为客户连接请求去现场创建进程和线程。

我想，这是fork迷们最愿意看到的结论了。

**五、双核系统重复论文实验步骤**

![img](https://pic1.zhimg.com/80/v2-8ca64c8c326094b7ea2522e43b8aad54_720w.webp)

![img](https://pic1.zhimg.com/80/v2-05f1fa1f9963c72bc7e16ebdb68175b0_720w.webp)

![img](https://pic2.zhimg.com/80/v2-3c49863261a69b3e630de25ec60cbaf9_720w.webp)

![img](https://pic4.zhimg.com/80/v2-cf1a25b4c71b469a2fce9ac09c9b405f_720w.webp)

【实验结论】

双核处理器在完成任务量较少时，没有系统其他瓶颈因素影响时基本上是单核的两倍，在任务量较多时，受系统其他瓶颈因素的影响，速度明显趋近于单核的速度。

**六、并发服务的不可测性**

看到这里，你会感觉到我有挺进程、贬线程的论调，实际上对于现实中的并发服务具有不可测性，前面的实验和结论只可做参考，而不可定性。

**结束语**

本篇文章比较了Linux系统上多线程和多进程的运行效率，在实际应用时还有其他因素的影响，比如网络通讯时采用长连接还是短连接，是否采用 select、poll，这些都可能影响到并发模式的选型。

原文地址：https://zhuanlan.zhihu.com/p/381901463

作者：linux

# 【NO.524】最常见的linux网络编程面试题【好文收藏】

## 1.什么是IO多路复用

I/O多路复用的本质是使用select,poll或者epoll函数，挂起进程，当一个或者多个I/O事件发生之后，将控制返回给用户进程。以服务器编程为例，传统的多进程(多线程)并发模型，在处理用户连接时都是开启一个新的线程或者进程去处理一个新的连接，而I/O多路复用则可以在一个进程(线程)当中同时监听多个网络I/O事件，也就是多个文件描述符。select、poll 和 epoll 都是 Linux API 提供的 IO 复用方式。

## 2.epool中et和lt的区别与实现原理

LT：水平触发，效率会低于ET触发，尤其在大并发，大流量的情况下。但是LT对代码编写要求比较低，不容易出现问题。LT模式服务编写上的表现是：只要有数据没有被获取，内核就不断通知你，因此不用担心事件丢失的情况。
ET：边缘触发，效率非常高，在并发，大流量的情况下，会比LT少很多epoll的系统调用，因此效率高。但是对编程要求高，需要细致的处理每个请求，否则容易发生丢失事件的情况。

## 3.tcp连接建立的时候3次握手，断开连接的4次握手的具体过程

三次握手 --- 第一次握手是客户端connect连接到server，server accept client的请求之后，向client端发送一个消息，相当于说我都准备好了，你连接上我了，这是第二次握手，第3次握手就是client向server发送的，就是对第二次握手消息的确认。之后client和server就开始通讯了。

四次握手 --- 断开连接的一端发送close请求是第一次握手，另外一端接收到断开连接的请求之后需要对close进行确认，发送一个消息，这是第二次握手，发送了确认消息之后还要向对端发送close消息，要关闭对对端的连接，这是第3次握手，而在最初发送断开连接的一端接收到消息之后，进入到一个很重要的状态time_wait状态，这个状态也是面试官经常问道的问题，最后一次握手是最初发送断开连接的一端接收到消息之后。对消息的确认。

## 4.connect方法会阻塞，请问有什么方法可以避免其长时间阻塞？

最通常的方法最有效的是加定时器；也可以采用非阻塞模式。
或者考虑采用异步传输机制，同步传输与异步传输的主要区别在于同步传输中，如果调用recvfrom后会一致阻塞运行，从而导致调用线程暂停运行；异步传输机制则不然，会立即返回。

## 5.网络中，如果客户端突然掉线或者重启，服务器端怎么样才能立刻知道？

若客户端掉线或者重新启动，服务器端会收到复位信号，每一种tcp/ip得实现不一样，控制机制也不一样。

## 6.在子网210.27.48.21/30种有多少个可用地址？分别是什么？

简: 30表示的是网络号(network number)是30位，剩下2位中11是广播(broadcast)地址，00是multicast地址，只有01和10可以作为host address。
详: 210.27.48.21/30代表的子网的网络号是30位，即网络号是210.27.48.21 & 255.255.255.251=210.27.48.20，此子网的地址空间是2位，即可以有4个地址：210.27.48.20, 210.27.48.21, 210.27.48.22, 210.27.48.23。第一个地址的主机号(host number/id)是0，而主机号0代表的是multicast地址。最后一个地址的最后两位是11，主机号每一位都为1代表的是广播(broadcast)地址。所以只有中间两个地址可以给host使用。其实那个问题本身不准确，广播或multicast地止也是可以使用的地址，所以回答4也应该正确，当然问的人也可能是想要你回答2。我个人觉得最好的回答是一个广播地址，一个multicast地址，2个unicast地址。

## 7.TTL是什么？有什么用处，通常那些工具会用到它？（ping? traceroute? ifconfig? netstat?）

简: TTL是Time To Live，一般是hup count，每经过一个路由就会被减去一，如果它变成0，包会被丢掉。它的主要目的是防止包在有回路的网络上死转，浪费网络资源。ping和traceroute用到它。
详: TTL是Time To Live，目前是hup count，当包每经过一个路由器它就会被减去一，如果它变成0，路由器就会把包丢掉。IP网络往往带有环(loop)，比如子网A和子网B有两个路由器相连，它就是一个loop。TTL的主要目的是防止包在有回路的网络上死转，因为包的TTL最终后变成0而使得此包从网上消失(此时往往路由器会送一个ICMP包回来，traceroute就是根据这个做的)。ping会送包出去，所以里面有它，但是ping不一定非要不可它。traceroute则是完全因为有它才能成的。ifconfig是用来配置网卡的，netstat -rn 是用来列路由表的，所以都用不着它

## 8.路由表示做什么用的？在linux环境中怎么来配置一条默认路由？

简: 路由表是用来决定如何将包从一个子网传送到另一个子网的，换局话说就是用来决定从一个网卡接收到的包应该送的哪一张网卡上的。在Linux上可以用“route add default gw <默认路由器IP>”来配置一条默认路由。
详: 路由表是用来决定如何将包从一个子网传送到另一个子网的，换局话说就是用来决定从一个网卡接收到的包应该送的哪一张网卡上的。路由表的每一行至少有目标网络号、netmask、到这个子网应该使用的网卡。当路由器从一个网卡接收到一个包时，它扫描路由表的每一行，用里面的netmask和包里的目标IP地址做并逻辑运算(&)找出目标网络号，如果此网络号和这一行里的网络号相同就将这条路由保留下来做为备用路由，如果已经有备用路由了就在这两条路由里将网络号最长的留下来，另一条丢掉，如此接着扫描下一行直到结束。如果扫描结束任没有找到任何路由，就用默认路由。确定路由后，直接将包送到对应的网卡上去。在具体的实现中，路由表可能包含更多的信息为选路由算法的细节所用。题外话：路由算法其实效率很差，而且不scalable，解决办法是使用IP交换机，比如MPLS。
在Linux上可以用“route add default gw <默认路由器IP>”来配置一条默认路由。

## 9.在网络中有两台主机A和B，并通过路由器和其他交换设备连接起来，已经确认物理连接正确无误，怎么来测试这两台机器是否连通？如果不通，怎么来判断故障点？怎么排除故障？

测试这两台机器是否连通：从一台机器ping另一台机器，如果ping不通，用traceroute可以确定是哪个路由器不能连通，然后再找问题是在交换设备/hup/cable等。

## 10.网络编程中设计并发服务器，使用多进程 与 多线程 ，请问有什么区别？

答案一:

1）进程：子进程是父进程的复制品。子进程获得父进程数据空间、堆和栈的复制品。
2）线程：相对与进程而言，线程是一个更加接近与执行体的概念，它可以与同进程的其他线程共享数据，但拥有自己的栈空间，拥有独立的执行序列。
两者都可以提高程序的并发度，提高程序运行效率和响应时间。
线程和进程在使用上各有优缺点：线程执行开销小，但不利于资源管理和保护；而进程正相反。同时，线程适合于在SMP机器上运行，而进程则可以跨机器迁移。

答案二:

根本区别就一点：用多进程每个进程有自己的地址空间(address space)，线程则共享地址空间。所有其它区别都是由此而来的：
1）速度：线程产生的速度快，线程间的通讯快、切换快等，因为他们在同一个地址空间内。
2）资源利用率：线程的资源利用率比较好也是因为他们在同一个地址空间内。
3）同步问题：线程使用公共变量/内存时需要使用同步机制还是因为他们在同一个地址空间内。

## 11.网络编程的一般步骤

对于TCP连接：

1.服务器端1）创建套接字create；2）绑定端口号bind；3）监听连接listen；4）接受连接请求accept，并返回新的套接字；5）用新返回的套接字recv/send；6）关闭套接字。
2.客户端1）创建套接字create; 2）发起建立连接请求connect; 3）发送/接收数据send/recv；4）关闭套接字。

TCP总结：

Server端：create -- bind -- listen-- accept-- recv/send-- close
Client端：create------- conncet------send/recv------close.

对于UDP连接：

1.服务器端:1）创建套接字create；2）绑定端口号bind；3）接收/发送消息recvfrom/sendto；4）关闭套接字。
2.客户端:1）创建套接字create；2）发送/接收消息sendto/recvfrom；3）关闭套接字.

UDP总结:

Server端：create----bind ----recvfrom/sendto----close
Client端：create---- sendto/recvfrom----close.

## 12.TCP的重发机制是怎么实现的？

1）滑动窗口机制，确立收发的边界，能让发送方知道已经发送了多少（已确认）、尚未确认的字节数、尚待发送的字节数；让接收方知道（已经确认收到的字节数）。
2）选择重传，用于对传输出错的序列进行重传。

## 13.TCP为什么不是两次连接？而是三次握手？

如果A与B两个进程通信，如果仅是两次连接。可能出现的一种情况就是：A发送完请报文以后，由于网络情况不好，出现了网络拥塞，即B延时很长时间后收到报文，即此时A将此报文认定为失效的报文。B收到报文后，会向A发起连接。此时两次握手完毕，B会认为已经建立了连接可以通信，B会一直等到A发送的连接请求，而A对失效的报文回复自然不会处理。依次会陷入B忙等的僵局，造成资源的浪费。

## 14.socket编程，如果client断电了，服务器如何快速知道？

使用定时器（适合有数据流动的情况）； 使用socket选项SO_KEEPALIVE（适合没有数据流动的情况）;

## 15.fork()一子进程程后 父进程癿全局变量能不能使用？

fork后子进程将会拥有父进程的几乎一切资源，父子进程的都各自有自己的全局变量。不能通用，不同于线程。对于线程，各个线程共享全局变量。

## 16.4G的long型整数中找到一个最大的，如何做？

要找到最大的肯定要遍历所有的数的，而且不能将数据全部读入内存，可能不足。算法的时间复杂度肯定是O（n）
感觉就是遍历，比较。。。。还能怎么改进呢？？？？
可以改进的地方，就是读入内存的时候，一次多读些。。。。
需要注意的就是每次从磁盘上尽量多读一些数到内存区，然后处理完之后再读入一批。减少IO次数，自然能够提高效率。而对于类快速排序方法，稍微要麻烦一些： 分批读入，假设是M个数，然后从这M个数中选出n个最大的数缓存起来，直到所有的N个数都分批处理完之后，再将各批次缓存的n个数合并起来再进行一次类快 速排序得到最终的n个最大的数就可以了。在运行过程中，如果缓存数太多，可以不断地将多个缓存合并，保留这些缓存中最大的n个数即可。由于类快速排序的时 间复杂度是O（N），这样分批处理再合并的办法，依然有极大的可能会比堆和败者树更优。当然，在空间上会占用较多的内存。

此题还有个变种，就是寻找K个最大或者最小的数。有以下几种算法：
容量为K的最大堆/最小堆，假设K可以装入内存；
如果N个数可以装入内存，且都小于MAX，那么可以开辟一个MAX大的数组，类似计数排序。。。从数组尾部扫描K个最大的数，头部扫描K个最小的数。

## 17.tcp三次握手的过程，accept发生在三次握手哪个阶段？

client 的 connect 引起3次握手
server 在socket， bind， listen后，阻塞在accept，三次握手完成后，accept返回一个fd，因此accept发生在三次握手之后。

## 18.tcp流， udp的数据报，之间有什么区别，为什么TCP要叫做数据流？

TCP本身是面向连接的协议，S和C之间要使用TCP，必须先建立连接，数据就在该连接上流动，可以是双向的，没有边界。所以叫数据流 ，占系统资源多
UDP不是面向连接的，不存在建立连接，释放连接，每个数据包都是独立的包，有边界，一般不会合并。
TCP保证数据正确性，UDP可能丢包，TCP保证数据顺序，UDP不保证

## 19.socket在什么情况下可读?

1. 接收缓冲区有数据，一定可读
2. 对方正常关闭socket，也是可读
3. 对于侦听socket，有新连接到达也可读
4. socket有错误发生，且pending

## 20.TCP通讯中，select到读事件，但是读到的数据量是0，为什么，如何解决?

select 返回0代表超时。select出错返回-1。
select到读事件，但是读到的数据量为0，说明对方已经关闭了socket的读端。本端关闭读即可。
当select出错时，会将接口置为可读又可写。这时就要通过判断select的返回值为-1来区分。

## 21.说说IO多路复用优缺点？

IO多路复用优点：

1.相比基于进程的模型给程序员更多的程序行为控制。
[http://2.IO](https://link.zhihu.com/?target=http%3A//2.IO)多路复用只需要一个进程就可以处理多个事件，单个进程内数据共享变得容易，调试也更容易。 
3.因为在单一的进程上下文当中，所以不会有多进程多线程模型的切换开销。

IO多路复用缺点：

1.业务逻辑处理困难，编程思维不符合人类正常思维。 
2.不能充分利用多核处理器。

## 22.说说select机制的缺点

每次调用select，都需要把监听的文件描述符集合fd_set从用户态拷贝到内核态，从算法角度来说就是O(n)的时间开销。

每次调用select调用返回之后都需要遍历所有文件描述符，判断哪些文件描述符有读写事件发生，这也是O(n)的时间开销。

内核对被监控的文件描述符集合大小做了限制，并且这个是通过宏控制的，大小不可改变(限制为1024)。

## 23.说一下epoll的好处

epoll解决了select和poll在文件描述符集合拷贝和遍历上的问题，能够在一个进程当中监听多个文件描述符，并且十分高效。

## 24.epoll需要在用户态和内核态拷贝数据么？

在注册监听事件时从用户态将数据传入内核态；当返回时需要将就绪队列的内容拷贝到用户空间。

## 25.epoll的实现知道么？在内核当中是什么样的数据结构进行存储，每个操作的时间复杂度是多少？

在内核当中是以红黑树的方式组织监听的事件，查询开销是O(logn)。采用回调的方式检测就绪事件，时间复杂的位O(1);

原文地址：https://zhuanlan.zhihu.com/p/387831204

作者：linux

# 【NO.525】内存优化-使用tcmalloc分析解决内存泄漏和内存暴涨问题

## 1.下载安装tcmalloc

\#1、到google下载代码：

当然你最好下载最新或者最稳定版本，这里比如下载2.1版本：

wget [https://gperftools.googlecode.com/files/gperftools-2.1.tar.gz](https://link.zhihu.com/?target=https%3A//gperftools.googlecode.com/files/gperftools-2.1.tar.gz)

\#解压

tar -zxvf google-perftools-2.1.tar.gz

\#看看说明

cd google-perftools-2.1

./configure -h

./configure

make && make install

## 2.代码中使用tcmalloc替换malloc

我们如何使用tcmalloc来替换glibc的malloc呢？

在链接tcmalloc的时候我们可以使用以下任意一种方式：

1.启动程序之前，预先加载tcmalloc动态库的环境变量设置： exportLD_PRELOAD="
/usr/local/lib/libtcmalloc.so"

2.在你的动态库链接的地方加入：-ltcmalloc

## 3.检测内存泄漏

**測试代码1：**

```text
#include <iostream>
using namespace std;
int main()
{
        int *p = new int();
        return 0;
}
```

编译：g++ t.cpp -o main -ltcmalloc -g -O0

内存泄漏检查： env HEAPCHECK=normal ./main

结果：

```text
root@ubuntu:/home/gaoke/test# env HEAPCHECK=normal ./main
WARNING: Perftools heap leak checker is active -- Performance may suffer
Have memory regions w/o callers: might report false leaks
Leak check _main_ detected leaks of 4 bytes in 1 objects
The 1 largest leaks:
*** WARNING: Cannot convert addresses to symbols in output below.
*** Reason: Cannot find 'pprof' (is PPROF_PATH set correctly?)
*** If you cannot fix this, try running pprof directly.
Leak of 4 bytes in 1 objects allocated from:
  @ 4007ef 
  @ 7f7895a64f45 
  @ 400719 


If the preceding stack traces are not enough to find the leaks, try running THIS shell command:

pprof ./main "/tmp/main.6712._main_-end.heap" --inuse_objects --lines --heapcheck  --edgefraction=1e-10 --nodefraction=1e-10 --gv

If you are still puzzled about why the leaks are there, try rerunning this program with HEAP_CHECK_TEST_POINTER_ALIGNMENT=1 and/or with HEAP_CHECK_MAX_POINTER_OFFSET=-1
If the leak report occurs in a small fraction of runs, try running with TCMALLOC_MAX_FREE_QUEUE_SIZE of few hundred MB or with TCMALLOC_RECLAIM_MEMORY=false, it might help find leaks more repeatably
Exiting with error code (instead of crashing) because of whole-program memory leaks
```

大家注意，这里有关键字Leak，你就得当心这里可能存在内存泄漏，提示

Leak of 4 bytes in 1 objects allocated from

对，是有四字节的内存泄漏，虽然你看代码能看到指针p未释放，但是这里你需要掌握的是在你无法直观的通过阅读代码来找到内存泄漏点的情况下，如何用tcmalloc工具来分析问题。

相信细心的你会注意到运行输出的这一行

pprof ./main "/tmp/main.6712._main_-end.heap" --inuse_objects --lines --heapcheck --edgefraction=1e-10 --nodefraction=1e-10 --gv

这里就是我要重点讲的**pprof工具**

google-perftool提供了一个叫pprof的工具，它是一个perl的脚本，通过这个工具，可以将google-perftool的输出结果分析得更为直观，输出为text、图片、pdf等格式。

这里我们把结果通过text的方式输出：你只需要把刚才的--gv换成--text

pprof ./main "/tmp/main.6712._main_-end.heap" --inuse_objects --lines --heapcheck --edgefraction=1e-10 --nodefraction=1e-10 --text

![img](https://pic3.zhimg.com/80/v2-2c76472bc6570da8926d7ec55bd43472_720w.webp)

好了，你可以看到这里已经很明显了，给你提示了t.cpp文件的第五行代码存在内存泄漏（当然你也可以输出其他格式，raw，png，pdf等等，whatever，只要可以帮助你去分析问题解决问题）。

实际上项目中遇到的内存泄漏问题是异常复杂的，我给的这个示例只是小试牛刀。项目常见的内存泄漏点大家都清楚，new了但是没有得到delete，但是要根据pprof工具对应的函数，代码行找到对应的泄漏点你可能需要花费点功夫。

实际上你的大多数应用都是以服务的方式启动，长时间处于作业/工作状态。你需要定期来检测下内存泄漏情况，那么这时你需要显示的调用接口来输出leak情况，

**示例代码2**

```text
bool memory_check(void* arg)
{
    HeapLeakChecker::NoGlobalLeaks();
    return TRUE;
}
```

将上面的代码加到你的定时检测逻辑里，或者需要观察的点，那么他就会输出示例1中的内容，动态的帮助你分析内存泄漏点。

## 4.分析使用tcmalloc后内存暴涨不降问题

记得几年前我开始推广大家使用tcmalloc后，一些同事做压测过程中也遇到了不少麻烦。比如当有大量数据过来，new出来很多的大块内存，突然发现有时候内存增长到几个G，开始以为是内存泄露的问题。

![img](https://pic1.zhimg.com/80/v2-34354b03b8124b88c621dba3749b2e58_720w.webp)

先是用tcmalloc环境变量来检查内存泄漏没有找到泄漏的报告，用valgrind也做了大量的测试，但是valgrind显示没有内存泄露。 实际上遇到这种问题不要慌，基本上是对tcmalloc使用上的问题，你要知道默认情况下，tcmalloc会将长时间未用的内存交还系统。tcmalloc_release_rate这个flag控制了这个交回频率。你可以在运行时通过这个语句强制这个release发生：

MallocExtension::instance()->ReleaseFreeMemory();

当然了，你可以通过 SetMemoryReleaseRate() 来设置这个tcmalloc_release_rate. 如果设置为0，代表永远不交回。数字越大代表交回的频率越大。一般合理的值就是设置一个0 - 10 之间的一个数。也可以通过设置环境变量 TCMALLOC_RELEASE_RATE来设置这个rate。

实际上我估计很多人看了官网说的

MallocExtension::instance()->SetMemoryReleaseRate(7.0);

很疑惑，我曾经带着疑惑做了测试，发现SetMemoryReleaseRate设置9，10回收的内存仍然是很慢的，所以后来我索性在进程启动的开始设置SetMemoryReleaseRate为9，然后在new对象的时候ReleaseFreeMemory，在new对象析构的时候ReleaseFreeMemory一次 （new出来的对象可能从new到delete的生命周期是不确定的，可能存在1天？4小时？30分钟都有可能，而且不是频繁的释放和销毁），因此这种情况下，内存就比较及时的回收了，所以大家可以根据自己的项目逻辑来选择ReleaseFreeMemory的时机，最好不要频繁的申请和释放，这对tcmalloc来说也是难受。

![img](https://pic1.zhimg.com/80/v2-99350e63cdd4b97cf6758449d50e5cbc_720w.webp)

所以你不仅仅要关注tcmalloc申请大小内存块，还要关注内存块的在合适的时间及时回收，否则造成内存占用过高。

原文地址：https://zhuanlan.zhihu.com/p/501249551

作者：linux