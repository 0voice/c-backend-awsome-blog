# 【NO.381】从抖音到火山引擎——看流媒体技术演进和机会

**编者按：** 8月5日上午，LiveVideoStackCon 2022 音视频技术大会上海站邀请到了火山引擎RTC负责人宋慎义老师，为我们从实时性、沉浸式、跨地区和开发者等四个方向，来看从抖音到火山引擎，流媒体技术演进过程和机会。在宋慎义老师的演讲中，我们看到了火山引擎一路走来的历程，也了解到通过结合不同的场景，火山引擎对外来探索的坚持。

*文/宋慎义
整理/LiveVideoStack*

![图片](https://oscimg.oschina.net/oscnet/up-344a0d46e97f5585d003a7cfcebfe04de5e.png)

LiveVideoStack是2017年开始创办的，我也恰好在那一年加入字节跳动，与字节跳动一同高速成长，从抖音到飞书，再到现在把这些成熟能力通过火山引擎对外输出，为更多的开发者与企业客户提供服务。

![图片](https://oscimg.oschina.net/oscnet/up-05bb6fdb2f17bf29f35b53d31f7203233a2.png)

首先介绍一下过去几年，在流媒体技术上，我们遇到了哪些场景，解决了哪些难题以及从长远来看，未来几年我们需要持续关注的未来的方向和机会。

字节跳动大约是在2016年开始做流媒体相关的技术，之前只有点播。一开始我们是做单向的直播，后来有了实时互动。2020年年初疫情爆发，更复杂的互动模式开始产生。我们在2020年推出了火山引擎，整个公司包括云基础、视频云以及大数据都开始向火山引擎上迁移。从那之后，字节跳动的技术能力开始慢慢成为火山引擎上的一些服务对外输出。之后，我们又利用火山引擎支撑游戏、沉浸式音视频等多种应用，火山引擎也帮助这些应用获得更好实现增长。

![图片](https://oscimg.oschina.net/oscnet/up-08f2abbc02e9a715b836940a59d86ddd1c9.png)

我们在做直播连麦业务的时候，遇到第一个问题是：如何在高清、实时和流畅这三个维度上做好平衡。想完全兼顾这三个维度是不可能的，虽然不能完全100%的实现高清、流畅和实时，但是可以90%实现，随着技术的演进，90%可以提升到99%，甚至更高。我们更多技术的发展也在不断地提高我们在高清、流畅和实时上的能力，不断提升用户体验。

![图片](https://oscimg.oschina.net/oscnet/up-a2b9d381cd75eafadd633a8622f1ff73f14.png)

疫情爆发以后，在线教育和视频会议也大爆发，诞生了很多非常复杂、有挑战的新场景，比如大班小组课、网络研讨会以及游戏的对战语音等，它需要一些新的突破，同时要解决全球化、多地区的问题，还要解决人和设备的问题。现在，除了人会加入互动，设备也会加入互动，所以通过架构的优化，我们提出多中心的分布式的信令的架构，能够支撑超大规模的互动。

![图片](https://oscimg.oschina.net/oscnet/up-9bcaa75f049e25a4d96868ce84501dbb99e.png)

近两年，沉浸式音视频忽然火爆起来，迎来音视频技术的新增长曲线，产生新的潮流，包括VR的视频、直播互动以及超低延时的直播和云端的渲染等，给技术方案提出了新的挑战。

![图片](https://oscimg.oschina.net/oscnet/up-f28b55f4a999101db3521ec938f68826285.png)

今天，我从四个维度给大家介绍一下，我们在流媒体技术演进上重点关注的几个方向。第一个是实时性。实时性是流媒体技术发展最大的限制条件，如果没有实时性的限制，音视频技术复杂度至少能降低一半；第二个是沉浸式，这是现在需求比较大的一个演进的方向，近几年的技术进步也要开始沉浸式地、慢慢地走入千家万户，走入每个人的生活；第三个是全球化，目前是中国流量红利见顶的一个阶段，在这个前提下，不得不面临的一个问题是，我们既想继续发展技术，又想获得更多的用户，获得更多的增长，那就必须去面对全球化的问题；最后我讲一下我们在开发者生态上的思考，广义上的开发者不仅仅是开发者，还包括内容创作者。我们看看，通过技术进步能够提供给开发者，怎样的工具，怎样的服务。

## **1、实时性**

![图片](https://oscimg.oschina.net/oscnet/up-4340b2e6a23732ef73c6f43e86599839a3e.png)

提到实时互动，我发现同行们在信道传输上的讨论比较多，但怎样把信源跟信道结合起来，一起去做优化，是实时互动的一个关键的技术点。

![图片](https://oscimg.oschina.net/oscnet/up-2d41f3c6d21ad397a3ef232823bfb59051c.png)

实时性的核心理念就是信源分级与信道分级，用有限的信道去传输最重要的信息。展开来讲，我们可以把信源或者说要传输的信息，按照实时性和可靠性这两个维度进行拆分。有些信息非常重要，它对实时性和可靠性的要求非常高，比如说信令消息；有些信息它对实时性要求高，对可靠性的要求也许能低一些，例如音频；对于视频而言，可靠性要求就更低了；同时也有一些信息，它对可靠性要求很高，对实时性要求没有那么高，比如说文件的传输、直播等；也有一些信息对可靠性和实时性要求都不高，比如说日志。那么信源可以分级，信道其实也是可以分级的。信道看起来是一个单一的信道，但是毕竟这么多的信源是要在统一的一个信道里去传输的，所以我们要把信道划分成很多个不同的能力，要把一些重要的信道、传输方式、传输容量预留出来，给信源来使用。调节的方法就是通过调优先级、FEC或者是调重传来实现这个信道的划分和信道的隔离。

![图片](https://oscimg.oschina.net/oscnet/up-c9713140e5112137e8d07227d85226a9398.png)

前面的偏理论，现在讲一下如何落地。首先是信源分级的落地。实际场景中，音频跟视频这些信源会按照重要性继续进行拆分。常见的是低频的信号或低清的信号肯定更重要一些，高清的信号关键时刻是可以舍弃的。在视频的信源分级上相对是比较成熟的，我们常见的有Simulcast这样的技术，包括在直播上经常使用的HLS、CMAF,本质上都是做视频的信源分级。不同清晰度之间也可以互相参考，比如SVC的技术，它有时域跟空域上的机制的划分以及长期参考帧的技术，可以让我们在一定程度上降低一些带宽。高清的部分实在传不过去也没关系，现在也有很多超分辨率重建的技术，短时间内顶一下还是可以的。

我们在音频上的关注更多一些，因为在真正信道变化的时候，音频产生的问题对感官的影响更大。这里介绍一下我们使用的一个性价比比较高的技术是MDC多描述编码。简单来讲，就是可以把一个音频的序列拆成多个子序列，让它们「1、2、3，1、2、3」地报数，所有报1的站一队，所有报2的站一队，所有报3的站一队。这样的好处就是这三段编码、三段序列可以分开地去编码、传输，只要有任何一个序列能够到达对面，相当于音频的低频部分、低频信号就可以到达对面了。到达对面的分片越多，音频的清晰度就会越高。如果只到达一两个，它的清晰度会有一些影响，但是基本不会影响沟通，这是一种性价比比较高的方式，它是一种原生的抗丢包的Codec。再把这种技术和FEC、AI-Codec结合起来，基本就可以实现：只要网是通的，只要有一点数据能够传到对面去，就能达到比较流畅的效果。实在传不过去，还有一些NetEQ、PLC的方式可以作补充。

![图片](https://oscimg.oschina.net/oscnet/up-957839d04462752e071090ca2afea303f9a.png)

经常有人问我，什么传输协议最好？

其实传输协议没有好与不好之分，只有合适与不合适的区别。

什么样的传输协议是合适的？

主要看应用场景、现实条件里有没有实时性的要求。如果没有实时性的要求，那一个能够有效利用信道与带宽的传输协议，就是最合适的。比如没有实时性要求，那TCP就是一个非常优秀的传输协议。如果有实时性要求，在传输协议的设计上就必须要实现三个目的：首先要能够调节重传实现可靠；有了一定可靠性之后，还要通过调节FEC让它实现一定的实时性；高优先级保证重要数据，丢弃/暂缓不重要数据。

一个很重要的传输协议的设计功能就是优先级。一个优秀的传输协议，实时传输协议一定要具备优先级的区别，这样能保证重要的数据优先传过去，不重要的数据就果断放弃。比如，一些非常重要的数据对实时性和可靠性的要求非常高，那就可以加很高倍的冗余，并且用很快速的重传方法，然后能高优先级地传输。虽然这样子的数据可能带宽浪费比较严重，没有办法传大量高码率的数据，但这样的数据用这样的传输协议就能实现很好的可靠性与实时性。对可靠性要求不是很高的数据，就可以进行低倍冗余，也可以进行有限重传，比如只重传关键帧，对于非关键帧不重传，这种情况下就可以实现信道的分级。这几个能力可以组合起来使用，但也可以是相对的，对于一些长肥网络可能最理智的方式就是关掉所有重传，让FEC去起作用。

![图片](https://oscimg.oschina.net/oscnet/up-c59f5f5128ecd86328bf24e1767879465a6.png)

信道分级以后一个重要的工作就是如何对信道进行建模。信道建模简单来讲就是要准确的表述网络状态。以前，信道建模的方式比较简单，都是用单一的变量，比如用丢包、带宽来描述一个信道，后来大家发现这些还是有一定的缺陷的，于是就有了相对组合的一些模型，比如延时带宽积。现在，随着算力的增加，信道建模的复杂度也相对越来越高，有的模型会有很多变量加进来参考。但是信道建模去描述网络状态是一方面，一个更重要的方式是，如何快速地感知网络的变化。有的模型其实是用统计类的指标，再加上滤波的方法来实现网络变化的感知。它可能会比较准，但是有一个很大的问题，它没有办法实现快速。有时候，我们的网络环境已经变了，但几秒钟之后它才感知出来。这样的话，对实时性的要求就会比较高，所以在实践中除了统计类的指标，比如丢包率、带宽这种需要好几秒才能统计出来指标，我们往往会加入一些瞬时指标去做修正，比如乱序、延时这些指标，就能够很快地去检测出来。

那么能够描述网络状态，感知网络的变化之后，剩下的就是如何快速地兼容弱网。大家很喜欢用这个弱网对抗这个词，但我不是很喜欢用弱网对抗，因为很多时候弱网是没有办法对抗的，只能去兼容。有些时候，动态变化是网络自身固有条件的、固有属性的动态变化。有些时候，是因为我们的信息传输发生了拥塞，那么这种时候需要改变的是信源自己。我们前面讲到信源分级，信源分级以后，通过快速地感知网络的变化，调整信源，然后果断舍弃不重要的数据，才能够实现更好的实时性。所以最终怎么去评价一个信道建模是否好，就看当网络快速变化的时候，我们缓冲区能不能快速地排空，快速地跟着变，然后把最重要的信息传到对面去。

## **2、沉浸式**

![图片](https://oscimg.oschina.net/oscnet/up-c667bf16a7e7577d4366e5ddecf5e8e9a4f.png)

可能会有人问，我们研究得那么细致到底有什么用？

随着我们现在需求越来越丰富、越来越膨胀，很多沉浸式的场景，它对实时性的要求也非常高。如果没有前面那么细致的信道分级、信源分级的技术，经常就会产生一些很大的拥塞，比如我们的实时性经常有几十兆甚至上百兆这样的码率，如果我们不去做很好的拥塞控制的话，可能一下就会拥塞几十秒。

![图片](https://oscimg.oschina.net/oscnet/up-f4b4b6b870cb1a77ea5ade3186288be7813.png)

我们在沉浸式音视频方面，也做了一些探索。下面给大家介绍一下，一个是自由视角，它整体的挑战还是很多的，比如实时视角的差值。我们在流媒体方面遇到了更多、更难的技术，就是如何快速的频道切换 、快速的视角切换。简单的方式是可以用一个低清的流进行预加载，但是这样依然无法解决快速视角切换时，应该怎样去进行预加载？有人提出全I帧的方案，也有人提出低GOP的方案，但是它对清晰度的损伤是非常大的。我们实践下来相对靠谱一点的方案，是用Simulcast加长期参考帧的方式。视角切换时，只需传一个I帧加一个长期参考帧和少量几个P帧，就可以实现快速的频道切换。这样的话GOP可以做得比较大，也可以实现在实时性和清晰度上达到相对不错的平衡。

![图片](https://oscimg.oschina.net/oscnet/up-a4fffc502dceed6e971e310378f1609ee65.png)

另外一个广泛使用的是全景视频，这个方案大家也都研究过。全景视频的核心理念就是信源分级。首先会有一个低清的540p的全景图来进行兜底，这个我们就不细讲了。在高清部分，怎样把一个8K的视频传到对面去？我们的做法是分片去编码，这样可以实现一次编码，很多个人同时超低延时地观看。我们可以用一个六面体的投影，分成640×640的分片，这样就可以用高端显卡进行并行编码，一块8K的显卡大约能编出来60帧，需要编120帧的话，可以用两块显卡并行地编。在这个demo上，如果我们的头动视角是往右上方移动的话，就可以很快地把右上方的几个新的分片挪下来，然后把左边几个分片排除掉，实现快速的头动延时。当然还有一些合并编码和合并解码的问题，比如264、265都是有相关的能力的，264有Slice，265有Tile。编码标准的定义是比较完善的，但在实践上还是有很多工程问题需要解决，对码流格式的要求也是比较高。最终这个方案可以实现上行能压到大约100兆，下行能做到10兆以内，并且在这种场景下我们依然可以达到400毫秒的延时，也可以做到150毫秒的头动延时。如果想要更快的话，还可以用边缘云和私有云的方式去加速。

![图片](https://oscimg.oschina.net/oscnet/up-23033ff981273cb0502c60b7f23a1147008.png)

再讲一下点云传输，就是体积视频。点云传输相比于全景传输是一个体验性更好、更终极的方案，但目前大家的通用做法还是伪3D的方式，就是用2D的背景加前面的3D对象，其实也是在2D的环境下进行了投影，最后叠加上去了，然后重新贴到一个二维平面上，用二维技术把投影和深度数据进行重新编码，复用二维视频的编码和传输通道。我希望未来几年大家能够有真正的3D压缩能力，把它做出来。

![图片](https://oscimg.oschina.net/oscnet/up-7039a26829044aa76233e6ec67821e46cdd.png)

前面讲的都是视频，接下来介绍音频。一个比较常用的音频沉浸式的方式就是双声道的空间音效，它可以基本复用现有的音频编码和传输链路，只在渲染端做一些模拟处理，但是这样的效果肯定不是最好的，更好的方式就是我们正在关注的这个方式——多声道的采集。用多个麦克风去采集，然后经过变换抽离出多个音源，把多个音源分别进行编码然后传输。这里也用到了高精度采样技术，我们也可以把16bit的采样数据提升到24bit，然后采样频率一并提升。

不过这样就带来一个问题，它的码率会膨胀，音频码率有时也能到好几百K甚至好几兆，那这种情况下如何实现实时性的全景声？一个是我们刚刚提到MDC多描述编码，另一个就是在音乐中经常用到的高低频段，分段编码的SBR技术以及参数立体声的技术，也可以跟现在的音频编码传输结合起来，保证重要的数据能够传过去，来保证实时性。

## **3、跨地区**

![图片](https://oscimg.oschina.net/oscnet/up-051780db95f51484eaafe6715e57e940175.png)

另一个重点关注的问题就是全球化了，全球化的问题分为几块，一个比较重要的就是音视频的基础设施的能力，然后就是全球化的数据一致性的问题。

![图片](https://oscimg.oschina.net/oscnet/up-e6585d2320f9cc117db1c526e76707cb534.png)

全球的节点是非常多的，面对成千上万的节点，难免会遇到节点与链路坏掉的情况，每天有几十次上百次的链路劣化。公网的路由检测往往只能检测通和断两种情况，它并不能检测质量下降的情况，而且缺少对路径综合能力的判断，毕竟检测的时效性较差，检测切换的速度也比较慢，还有一个缺点就是DSCP在广域网上是失效的，所以我们的做法就是把SDN技术搬到广域网上，建一层overlay的网络。在传输面就完全使用SDN的传输能力，在控制面上我们又重写了控制层，可以让广域网上实现SDN的能力。

这样有几点好处：一个是DSCP可以兼容，如果我们去改DSCP，它在这个传输网络上还是可以生效的，能够实现包级别的QS控制，而且能够实现秒级的检测和切换，最终我们大概就能实现在公网上电信级的可能性。所以音视频已经慢慢地成为了基础设施，我们开发者就不用太关注体验、成本、稳定性这些东西，可以把精力放在自己的产品开发上。

另外讲一下，分布式场景下的数据一致性的问题。因为我们的实际用户是遍布在全球各地的，用中心的房间控制器和中心的信令是不合适的。如果你有一个中心信令的话，地球另一端的用户加入房间和推拉流的首帧延时就会非常长，有时候能到将近一秒。所以我们这里就不能做强一致性的保证，至少这个房间与用户的关系，这个是不能够做强一致性保证的，只能是做最终一致性的保证。

![图片](https://oscimg.oschina.net/oscnet/up-8493e990d84c32b13ad7b98bfcb0c39f4d1.png)

在真正人数多，也不能做强一致性保证的时候，如何保证大规模的互动呢？

我们最大的挑战，就是来自于大房间的架构上的挑战，比如说这个大班课、网络研讨会还有多人的游戏对战，需要非常多的人上台互动，并且有很多的用户来观看，有几十万甚至好几十万的用户要同时观看，有几百上千的用户在台上互动。这种情况下，信令跟传输层都需要做特定的优化。首先信令要做分布式信令，要去中心化，一定不能做单一的信令，因为它扛不住这么多用户的冲击。这种时候信令就要尽量地下沉，分散到全球的多个数据中心里，甚至是下沉到离用户最近的地方，我们只需要把活跃用户的状态一步步更新到其他的信令，而不需要把所有的用户状态分布出去。这样的话，理论上才能够实现无限的扩展，最终就可以实现一个RTT的进房，一个RTT的推拉流。

另外就是整个传输拓扑图的分发问题。在全球化的时候，传输拓扑图是并行去进行计算和更新的。因为串行的计算速度比较慢，会面临很多问题。这个传输拓扑需要一个大致平衡的分发树，它整体的树的高度应该是比较统一的，这样才能实现比较低的延时。并行计算的时候还要应对海量的用户快速进房、快速出房的问题，所以还需要进行节点的动态分裂与合并。有时候做一些容灾的和紧急修复的时候，它的QPS也非常高，同时在这种情况下就很容易引发回环的问题，所以还要做回环的检测与解除，这里面的限制也很多。

## **4、开发者**

![图片](https://oscimg.oschina.net/oscnet/up-15ba115c47cc140970c06ce41016d2fd9f2.png)

讲完全球化，我最后再讲一下开发者生态。整体上，我们随着全球化的业务越来越多，开发者生态的建设，包括全球的开发者一起参与进来，也是我们需要去探索的问题。我们以前觉得的流媒体技术商业化，最大的成本来自于资源、带宽，现在我渐渐发现流媒体技术商业化，最大的成本来自于开发者的开发成本，这里的开发者不仅仅包括代码的开发者，也包括内容的创作者。创作者实在是太难了，他们要想做出一个优秀的应用，想做出一些酷炫的内容，是需要非常高的学习成本的，如果我们能想办法一起能给他们提供一些非常方便的工具、非常方便的能力，这就是很大的价值，也是火山引擎持续关注的方向。

![图片](https://oscimg.oschina.net/oscnet/up-bbc0e70cb3dcb88043ef0fe811ff3649a65.png)

我们关注的开发者的工具，就是平民化的创作工具。现在，很多创作者是被很专业、复杂、昂贵的创作工具所限制想象力。一个新的机会，一个平民化的创作工具，一个好的技术要想走进千家万户，它的创作工具一定是非常简单的。比如说，我们最早的时候修图都用Photoshop，但是真正把修图带进千家万户的其实是美图秀秀。我们最早去做视频、剪辑用Premiere和Final Cut，但是真正把视频制作带进千家万户的其实是剪映。抖音这么火，推荐系统是肯定很重要的，但源源不断的内容供给也是最重要的因素之一，源源不断内容供给靠的是平民化、简单化的创作工具。我们现在有很多新技术，比如特效技术、流媒体技术，还有动作捕捉、数字人以及实时自由视角，但是真的要使用起来难度很大。如果我们大家一起去做出很多能让自己用手机就能做出来的工具、就能使用的一些场景，我相信将给整个行业带来更多的想象力。

![图片](https://oscimg.oschina.net/oscnet/up-308f5c79d8aa2ae9f28546853abdd2e154b.png)

过去几年，开发者的工具变化也很大。最早我们有SaaS，SaaS的应用形态相对比较单一，所以大家都觉得不够灵活。过了几年，出现了PaaS。PaaS是灵活了，但对开发者的使用门槛也越来越高。于是又出来了新的aPaaS。我觉得很难讲，哪一个是未来的趋势，但是有这么多可能性，我们给开发者提供更多的选择，总归是更好的。

![图片](https://oscimg.oschina.net/oscnet/up-d927fe0360c083d900541731d21665fb144.png)

最后提一下，我们最近在重点思考和关注的一个事情就是多模块之间的协同。所有的方案都会涉及到多个模块的协同，大家往往喜欢做一个大而全的一个APP，把所有的功能都装进去，安装包一下子就100多兆，直播、点播、RTC、网页、消息、小程序全都在里面，很多时候他们是一起工作的。所有的模块包括这个APP是共用带宽、共用所有的采集和播放的设备，并且也共用CPU、GPU的内存。但是很多模块在设计时不会去考虑，其他的模块是怎么去使用资源、带宽的。希望我们以后在设计这些模块时要考虑一下，如何去共享资源，甚至是出让资源而不是抢占资源。大家在设计的时候应该去思考如何去共享、协同，把更多的选择权留给开发者。

火山引擎在这方面也做了一些探索：一是我们所有模块都支持动态的性能的降级、动态的带宽降级。不但支持我们自己的降级，还支持自定义的降级。比如用我们的SDK的时候，可以把我们的SDK限制在最高的一个带宽使用量上，例如：500K上，这样SDK就不会使用更高的带宽，也可以支持把目前你所能探索到的带宽预留出来100K给别的模块用。无论现在探测出来多少带宽，就只用现在的已有的带宽减100K，剩下的预留给其他的模块，这样能够实现更好的协同。同时火山引擎也可以支持让不同的SDK之间设置优先级，让这个SDK比其他SDK更重要，这在很多场景下是有用的。比如在直播做互动的时候，虽然直播很重要，音视频很重要，但是消息更重要，有时用户送的特效礼物可能是一个点播文件，但有些时候这些东西甚至会比直播和RTC更重要。在教育、会议的场景下，这种现象就更多，比如我们的音视频共享，可能就比音频信号更重要，这种时候就要求我们的每个模块都能具备自定义的升降级以及自定义的优先级的能力，同样除了对带宽，对性能也可以做这样的上限和预留的操作。这个目的就是把更多的选择权留给开发者。

![图片](https://oscimg.oschina.net/oscnet/up-32124b261804db8c7809e70094938647172.png)

基于以上的设计理念，火山引擎推出了音视频云端一体解决方案veVOS，这个方案有很多的优点，欢迎大家去火山引擎的官网上去看。这里专注讲一下协同，因为我觉得其他的优点大家之前能够关注到，但是协同这一点也是希望所有人、所有业内伙伴未来一起去关注的点。要做云和端的协同，要做模块与模块之间的协同，甚至要做我们自己的模块和宿主的APP应用之间的协同，防止这些模块之间的资源冲突，我们光把它们打成同一个包，打成一个SDK直接给客户和开发者是远远不够的，要提供更多的能力，让不同的模块之间、不同公司的SDK之间和应用之间，防止资源冲突，做到更好的协同。

![图片](https://oscimg.oschina.net/oscnet/up-ceadc93c6bd2550345632e3372f5a49df0d.png)

流媒体行业已经发展几十年了，大家对互动性、沉浸式、全球化还有开发者生态方面的要求越来越高，所以我觉得未来还是有很多机会的。

![图片](https://oscimg.oschina.net/oscnet/up-1421dfb808c41ca8acb74c4e6b4c5394005.png)

从抖音到火山引擎，我们持续关注的流媒体行业中的几个重要的机会：一是沉浸式的实时音视频，希望不久之后能有真正的沉浸式实时音视频的产生，并且能够真正地融入到我们的生活。我们在科幻片里看到的那些实时互动，那种沉浸式的场景，我相信很快就会变成现实；另一个是平民化的创作工具，能够带给我们丰富的内容。我觉得平民化的创作工具其实是这个行业持续的发展、持续产生新动力的动力源泉；最后一个是音视频的基础设施，它让我们能够帮开发者做好很多事情，让开发者聚焦到做更多的、更好的产品，能促进这个行业更快的进步。所以希望在座的各位一起去关注这些机会，通过技术的进步和工具的建设，给流媒体行业带来更加丰富的可能性。

谢谢大家，谢谢LiveVideoStack，欢迎大家持续关注火山引擎！



原文作者：*宋慎义*

原文链接：https://www.livevideostack.cn/news/from-tiktok-to-volcano-engine-the-evolution-and-opportunities-of-streaming-media-technology/

# 【NO.382】阿里云全球实时传输网络GRTN—QOE优化实践

**编者按：** 直播已深入每家每户，以淘宝的直播为例，在粉丝与主播的连麦互动中如何实现无感合屏或切屏？LiveVideoStackCon 2022音视频技术大会上海站邀请到了阿里云GRTN核心网技术负责人肖凯，为我们分享GRTN核心网的运作机制、运用方面以及QOE的网络模型在业务板块的实践优化。

文/肖凯

整理/LiveVideoStack

![图片](https://oscimg.oschina.net/oscnet/up-d1f11bbd428ddad9ef2a50efd5553123888.png)

大家好，欢迎大家来到 LiveVideoStackCon 2022音视频技术大会上海站，我是来自阿里云的肖凯，现在负责阿里云的GRTN的传输引擎的开发以及组网架构。今天讲解主要分两个版块，一方面简单介绍一下GRTN的理念和提供的能力。另一块就是阿里云的GRTN在接待客户的过程中，是怎样去优化QOE的指标。

![图片](https://oscimg.oschina.net/oscnet/up-40eeecade209c248713945b1a5e76cf2b9a.png)

今天的分享主要分为几块：GRTN简介、阿里云做QoE的优化经验、赛马系统、和阿里云的一些可编程的能力。

### **1、GRTN简介**

![图片](https://oscimg.oschina.net/oscnet/up-ea8dbf349aed08b3e3f8f73454ec913cdd6.png)

GRTN实际上现在是一张全SFU的网络，我是从 15年开始做直播这一块，伴随阿里云直播系统一路做到现在的通信级的传输分发网络。

现在的阿里云的GRTN基于覆盖全球的2800多个边缘节点，我们把这些节点和网络资源运用起来，做成了一张通信级的SFU的传输网络。

这些节点，包括解决跨洲的网络问题，都有专门的线路，整个系统都是从直播演进过来，过去很多的 CDN直播网络一般都是树状的结构。但阿里云的GRTN是一张树状和网状结合的动态网络，目前阿里云GRTN支撑的屏到屏延迟是100毫秒左右，满足云游戏或者云渲染这样的场景。

GRTN的能力很简单，它提供的是内容的传输和分发。任何一个用户使用RTP协议，把媒体推到阿里云GRTN的节点，它就可以在全球的任何地方就近地从GRTN把内容拉出去，GRTN会解决动态组网、就近接入等问题。

### **2、GRTN当前业务模式**

![图片](https://oscimg.oschina.net/oscnet/up-50cb34d218b17ed06e6824e2257bb65452d.png)

GRTN的当前的业务模式，目前很多客户接的都是阿里云的RTS 1.0，即在阿里云官网能够看到的RTS业务。

RTS 1.0是阿里云从18年左右开始研发的，它的核心理念是为了帮助客户在有限改造的前提下，接入GRTN，把延迟降下去。传统的直播FLV延迟大概在5秒， HLS更多，延迟达到20s 左右。RTS就是对推流侧或者播放侧进行改造，最重要的还是播放侧协议换成RTP，能够做到延迟在1秒左右，这个技术在19年左右淘宝直播已经全量落地。

![图片](https://oscimg.oschina.net/oscnet/up-55fac0c7303e804643f0bf317fe06c68a5c.png)

RTS 1.0结束之后，阿里云就进入到了RTS 2.0的时代。RTS 2.0里，我们对实时流媒体这个场景的预期是没有RTC和直播的区分，可以让所有的业务都建立在全链路RTP的协议上。全链路使用通信级的传输，是GRTN的技术理念。目前的RTS 2.0，它是具有通信级的服务能力的。

RTS 2.0的传输延迟在国内基本是在100毫秒左右，即为节点的传输耗时，剩下的延迟就可以放在编码侧或者放在播放侧，用来抗抖动。这样的场景一般用在一对一的通视频通信，或者多人会议，包括连麦直播一体化。

![图片](https://oscimg.oschina.net/oscnet/up-724c8cfb2918b6a970b058c3db536981386.png)

那在GRTN上怎么把一对一通信做出来呢？

阿里云GRTN的对外服务包括两种模式，一种是阿里云的SDK，通过使用GRTN的私有协议，另一方面，阿里云也支持浏览器，GRTN的生态是完全开放。用户可以使用浏览器，以标准的SDP信令交互的方式与GRTN的对接，把媒体推进来，再通过GRTN选择性地把媒体拉出去。两个客户端跟GRTN可以选择通过单PC或者多PC的模式交换音频、视频或自定义的消息，通过GRTN实现通信级的传输，这就是一对一通信。

这个模型并不仅限于通信，还包括云渲染，云游戏的模型。

![图片](https://oscimg.oschina.net/oscnet/up-de4aab110aa0c9ac6fd07d4a3d0f290931d.png)

在一对一通信的基础上，GRTN支持多人会议，如图所示，这里有4个参会方，这里会讲解多人会议在GRTN上需要怎样的能力。

在参会人比较多的时候，通常而言选择性的订阅对端的视频、音频是一个很麻烦的问题，因为涉及到Audio Ranking。很多业务方为了做这种多人会议，不得不把音频放到一个专门的Ranking Server上去做。GRTN提供了大规模的Audio Ranking能力，也就是说任何一个端在GRTN上消费音频，都可以做到为它进行Audio Ranking。这个人订阅了什么，GRTN就在这个人订阅的音频中进行Audio Ranking，不涉及Ranking server, 不增加延迟。

GRTN的另一个重要能力是切流。GRTN可以为任何观众实现他的媒体的替换，在云合流的连麦场景，这是一个很核心的能力，在一个浏览器上，观众通过GRTN在看一个人的画面，然后通过切流的指令，就让这个观众在完全无感的情况下实现画面的切换。

这就是GRTN的切流能力，这个能力可以为GRTN上某一个主播的所有观众实现媒体画面的实时切换，可以从a画面切到b画面，从a主播切到b主播，观众是完全无感的。

![图片](https://oscimg.oschina.net/oscnet/up-d55fb24fa6aaa6f18cd4a4a7d96f4def42c.png)

接下来我们看如何用切流能力实现云端连麦合流？在连麦这个场景上，如果是客户端的连麦，那就是ab两个主播进行连麦，观众在看a主播的过程中他们一连麦，观众看的画面就实时变成了a和b合屏的画面。这种场景能够简单的实现，通过端合流，即a主播在端上直接把自己的画面更改，观众看的内容相应进行变化。但是存在一些场景端合流是无法做到的，例如端的性能不够，这样场景下就需要通过云合流。

如图所示，一个主播流的画面推送到GRTN之后，有一个观众在看主播的画面，当这个主播和别的粉丝发生了连麦，连麦之后有一个业务方的合屏服务器，合屏服务器会把两个媒体合成一个。在这个时候就需要实现客户端的画面切换，而且全部都要切过去，这个时候我们提供的能力是切流指令，即前面所讲的切流的能力。切流指令传输到GRTN之后，GRTN将主播所有观众的画面无感地切换成合屏流的画面。

这个能力目前是实现淘宝直播在GRTN上直播连麦完全一体化的基础解决方案。

这是一个通用的方案，在后面随着GRTN和后续RTS 2.0服务的对外输出，这个能力会直接对外开放。

在这里和大家简单介绍一下淘宝直播的情况，淘宝直播实际上已经实现全量在通过GRTN进行，任何一场直播里观众和主播之间的延迟基本上都在1秒以内的。这个目前是GRTN在 RTS  2.0上的一个典型的场景。

### **3、QOE概述及优化难点**

![图片](https://oscimg.oschina.net/oscnet/up-edc73e684a0f5f3ac1ebfbcaeb82142b065.png)

QOE的一些优化实际上就是基于阿里云的外部客户的数据，为什么讲QOE而不是QOS？因为我们在接待客户的过程中发现，QOE通常都是客户本身制定的一系列的指标，比如说渗透率、观播时长、业务转换率，这些指标不是把QOS某个指标做好了，QOE就能变好。

例如GRTN在接客户时，发现我们的首帧卡顿、百秒卡顿时长、延迟、画质全方位的领先，RTS的QOS一定是全方位的比FLV要好，也就不用说比HLS了。但在面对不同的客户的时候，有的客户他说他的QOE正了，有的客户说他的QOE有问题，因为在客户从传统的FLV过渡到RTS以及RTS 2.0之后，他们会因为客户端的适配没有做好，或者说业务场景的磨合没有做好，遇到了一些问题。例如 WebRTC来进行通信，播放器的buffer的机制可以做得非常的激进，但是当在直播场景时，观众的体验可能比你的激进的延迟控制更加重要，所以在直播场景下更多的是要去做一个平衡。

![图片](https://oscimg.oschina.net/oscnet/up-0f79094d31720bfa72d89d3e1115e56616f.png)

在这个过程中，我们发现有时候客户把QOS全做正了，但是QOE却还需要花很多的时间去处理，所以在把QOE做正的过程中，要用的什么方法？

这是在QOE里阿里云要持续投入的。想要做好QOE一定要有业务输入，没有业务的输入，没有业务的反馈，QOE肯定是做不正的，所以阿里云有一个持续的基于业务的数据驱动技术投入这个板块。

这里最重要的一点就是客户端的数据，在做QOE的过程中，我认为服务端是没有资格说QOE的，只有客户端和业务才有资格说自己的QOE这么正。所以在这个过程中，GRTN的方法是先得到业务方的脱敏数据，然后去做QOE（最后会有一个数据的展示）。

### **4、GRTN QOE 优化理念**

![图片](https://oscimg.oschina.net/oscnet/up-b9daf32a06a0a5f34f00e848a3f3038202a.png)

GRTN优化QOE的一个理念是，GRTN做到了无感的链路切换。

GRTN内部是一个全SFU网络，上游的网络随时切换，对观众来说是完全无感的。同时还有强实时的主备链路。在很多直播、通信场景下，会有重保的概念，或是强实时的双路保障。如果节点之间出现问题，能够立马把它切到另外的节点链路上，这样观众完全无感。

还有GRTN节点和客户端之间的mobility的方案，例如某个节点可能网络有问题，或者客户端的网络发生了WiFi到4G的切换，那么使用一个mobility的方案瞬间能够切换节点，同时GRTN的下游消费者完全不受影响。

![图片](https://oscimg.oschina.net/oscnet/up-08a80fb1ca219472231a31190e95bc1bd72.png)

GRTN另一个优化QOE的方法，就是可编程策略。可编程实际上是我们近一年做出来的一个成果。传统的QOS优化能力，例如启用BBR还是启用GCC或者是别的拥塞控制算法，会发一堆的配置下去，配置里面全是开关。但是现在GRTN，可以在边缘直接用可编程的策略执行模块，类似CDN有可编程的能力，包括边缘脚本之类，GRTN也类似，但是做的比较彻底。现在的能力是可以在节点直接下发策略，运行语言，可以直接对发帧和发包逻辑做控制，可以介入到重传逻辑中，直接编程GRTN的对每一个客户端的行为，即通过策略配置系统直接把代码发下来。无需软件发版升级，因为像2800多个节点，是无法高频升级软件版本的，但是利用GRTN可编程能力可以实现一天几个策略迭代，结合客户端的数据，能够实现数据的打通。这样发策略下来，客户端拿到QOE的数据反馈给GRTN，GRTN的调优人员就知道如何去进一步的优化。

![图片](https://oscimg.oschina.net/oscnet/up-6f1c8135b1838769a9ec16f71bbe3292647.png)

如图是GRTN的一个多场景的随机配置，也是基于阿里云线上海量的业务数据来进行的。例如阿里云线上的配置管理系统会把配置集下发，这是做AB的基础能力。后面配置管理系统会将n组配置实时发到全网所有的边缘节点，针对的是某一个域名。针对这个域名，同时给他发出三组配置下去进行随机，可能会配一定的权重。例如阿里云认为conf_1 是个高风险的配置，一个高风险的新型的功能，发出去之后，把conf_1指配全网1%的业务量去做 AB。发到节点之后，当任何一个消费者来到GRTN消费内容时，将对它进行一个随机加权的选择，它有一定的概率使用conf_1，也有一定的概率使用后面两种。

第一步的请求完成之后，我们让多组配置同时在线上运行，但是运行完后怎么拿到结果呢？

简单的方法就是客户记录我们的trace_id，GRTN有一个trace_id的理念，这个ID对应客户端的这一次播放，任何两次播放的ID都不一样。

另一种方法是客户端把一个session ID带在它的请求参数里面，这样一个客户端就在GRTN有一个session ID跟trace_id对应，这次播放用的什么conf ，我们也能够给它记录到。同时这次播放，根据session ID，我们就可以从客户端的埋点查到它的QOE结果。

### **5、GRTN 赛马系统**

![图片](https://oscimg.oschina.net/oscnet/up-b4eec240231c1ade25594eb282731e664ed.png)

接下来对它做关联，播放器在GRTN上完成播放之后，播放器这边开始埋日志，他们埋的核心日志就包括首帧耗时、百秒渲染卡顿，也包括任何一个播放端的播放时长。在业务方记下来的日志中，它知道这个session id对应的这一次播放播了多久，它的各项指标怎样。在GRTN就知道发的trace_id是哪个，然后针对这一次播放，缓冲深度配了多少，以及丢包率目前统计下来是什么情况。

这两个数据（服务端日志和客户端日志）把客户的日志收上来，抛送给我们之后，这边就把session ID和trace_id在GRTN的数据分析体系里面做一个综合，就得到了一个结果：任何一次播放它对应的服务端的网络情况是什么，它对应的客户端的首帧耗时、百秒渲染卡顿、播放时长是什么。GRTN就通过这两种数据综合把客户端的数据和服务端的一个行为做到了关联。

![图片](https://oscimg.oschina.net/oscnet/up-6831c40355ec61cfc3ad42e1493ae7dec7e.png)

关联做到之后，下一步就做赛马系统。在任何一次配置的时候，就像现在阿里云给客户做调优的时候，我们会事先跟客户说一下要为你做调优。

例如说在这样一次配置中，以客户线上的业务为例，conf_1是一个高风险的功能，conf_2是对现有功能比如BBR的参数的调优，conf_3启用的可能是GCC。把配置发到节点，客户在进行播放之后，针对上两步把他的客户端和服务端的数据拿到之后，采集到GRTN这边，数据上传来之后，再对AB的结果做一个综合的分析。这个时候在研发人员的眼里就已经明确的知道下发的各组配置它的效果到底如何，区别是什么。研发调优人员就能够知道怎么去做进一步的调优，同时反馈哪一组配置可以被淘汰，再基于好的配置对它进行进一步的调优。所以这也就是赛马系统的价值——能够基于客户端的数据和服务端的数据进行综合的持续的迭代。

![图片](https://oscimg.oschina.net/oscnet/up-fc2dc8075e951a1edcd230661a2066063e9.png)

如图是赛马系统，它作为一个整体，有GRTN的节点网，服务客户端上报数据和GRTN的日志系统打通，做到相互配合。

### **6、GRTN QOE 优化案例**

![图片](https://oscimg.oschina.net/oscnet/up-6567c97e60059001d03a969d8180ba22952.png)

这是GRTN的一个优化样例，也就是赛马系统的评分。当时我们做实验有4组，normal就是平时日常运行常量的配置，radical就是一组非常激进的配置，reference就是用来跟radical进行对比的参照。如图做了一个六维的展示，也按照我们的想法对它进行了综合打分。

![图片](https://oscimg.oschina.net/oscnet/up-4c3772443a3b8652da20b2c9c3ead702c8e.png)

更详细的结果是这个表，刚才提到的conf_id配下去之后，运行完之后，接下来得到成功率、秒开这样的一些数据。这就是GRTN目前展示出来的赛马系统能够看到的数据。

成功率、秒开、都属于QOS的范畴，最后的平均播放时长，是属于QOE的范畴。我们测试下来得到的radical这一组的数据是最好的，它在播放时长上可能有1秒钟左右的优势，积累了24小时的数据，大概几十万的量级，我们认为这个量级的播放是可以用于支撑AB的数据。GRTN最开始在手淘场景做这个系统，手淘的业务量比较大的，所以我们从一开始拿手淘的线上的全部量级去运行。现在是直接可以拿外部客户的数据去运行，做成赛马系统，将阿里云可编程的能力，客户端的数据采集，包括赛马，做成一个闭环。

现在优化的方法，想要优化某种策略，就发一组配置下去。例如发一组配置，运行一个晚高峰，到了第二天就能拿到数据结果，这样的一个过程实际上对迭代的优势是非常大的。

例如今年3月份左右，我们给某个客户在调优播放时长的时候，通过分析客户端的一些行为，包括通过测试对数据进行分析，发现客户的音视频同步可能有点问题。怎么去解决这个问题呢？我们认为通过服务端的发帧策略的调整能够帮助客户端更好地实现音视频同步。我们用可编程把这个策略做好发出去，在第二天这个效果是非常好的。我们发现发下去之后，这组配置的观众播放时长升高了，这其实就是QOE的一个优化。

在这个基础上就完成了第一轮的迭代，我们认为这个路线是对的。接下来就是在这条路线上，怎么把参数进一步的调优。在最开始对发帧的策略进行调整之后，我们只是做了一个粗调，觉得大概可以弥补客户端的某些缺陷。实现了之后，接下来做进一步的不同的配置，不同的参数之间去做调优。

![图片](https://oscimg.oschina.net/oscnet/up-2168914ae159acd79e740fa887887a10fa2.png)

以上就是我的分享，谢谢大家。

原文作者：肖凯

原文链接：https://www.livevideostack.cn/news/aliyun-global-real-time-transmission-network-grtn-qoe-optimization-practice/



# 【NO.383】腾讯云实时音视频出海技术实践及落地

**编者按：** 互联网出海热潮涌动，社交娱乐、跨境电商、在线教育等产品与音视频技术密切相关，如何在纷繁复杂的基础设施和网络条件下提供高质量的音视频服务，是出海产品和云服务商面临的共同挑战。

本次分享将详细介绍腾讯云音视频客户在出海过程中所遇到的挑战，以及RT-ONE™网络的应对策略，为出海应用的音视频技术实践提供参考借鉴。

*文/崔立鹏
整理/LiveVideoStack*

![图片](https://oscimg.oschina.net/oscnet/up-b2c03aaa9767376373c6dad69832521792d.png)

今天跟大家分享的内容是腾讯云音视频在出海方面的一些技术实践，在最开始，我想分享下一个问题，为什么要讲出海？其实，这两年我们可以感受到国内的消费互联网到了平稳发展的阶段，像前几年那样创新应用层出不穷，某些应用一下子就能爆火的情况越来越少出现了。在这种情况下，国内很多公司都选择出海，去海外发展自己的业务，这是我们观察到整个行业的一个趋势。腾讯云也是顺应这个潮流，来支撑这些客户的业务发展，在此之外，腾讯云在全球都有布局，海外一些本土的客户也是有音视频需求的。虽然原来我们以国内业务为主，被行业不同的客户磨炼到今天，我们可以看到在产品实力上并不比国外云厂商差，有些方面的竞争力反而是领先的，这也是我们出海的一个比较好的信心来源。

![图片](https://oscimg.oschina.net/oscnet/up-51478b18297c80158209d8d9c0cd80a66f0.png)

然后再来看一下行业的情况，从整体上来看，海外音视频的应用是在蓬勃发展的。最近两年，国内市场抖音，快手，视频号增长还不错，但是新的应用比较少。但整个海外的情况，2020到2021年，娱乐出海的收入指数翻了两倍多，在全球社交应用推广榜Top20里，有十几家都是国内出海的客户，除了TikTok大家还比较熟悉之外，有些本地人都不知道是中国的应用，比如BIGO，LiveMe。我们C端产品的竞争力其实是很不错的，能够和欧美厂商PK。

![图片](https://oscimg.oschina.net/oscnet/up-fc29d0476cc87e2f2d7b66a4dcd5354100f.png)

在整个泛娱乐出海大潮下，音视频是重头戏，但音视频出海还是面临着很大的挑战。这可能和其他应用的出海不同，比如图文类或者阅读类应用的出海不会有特别多的技术相关的挑战，包括国内基础设施比较完备在音视频方面也都能比较好支撑，但海外环境对音视频技术挑战巨大：

1. 海外的用户网络很不稳定的，带宽受限，丢包比较多这种情况经常存在；
2. 一些国家的运营商可能会封禁UDP流量；
3. 用户的机型也比较复杂，国内只有几大手机厂商，但是国外有很多很老的手机厂商也在使用，这部分用户的终端性能问题突出；
4. 应用升级的覆盖周期很长，国内网络比较流畅，大家应用升级比较快，但是海外的很多用户可能几个月都达不到很高的版本覆盖，这导致在升级一些功能的时候出现了很大的挑战；
5. 安全合规在国内一直在收紧，海外的安全合规要求更高，像GDPR这些，这对企业来说也是红线，这对我们也提出了一些要求；
6. 创新玩法也是不断要更新迭代的。

为了应对这些挑战，作为基础设施厂商，我们还是要想办法解决这些问题。我接下来会分两块来讲解，一块是产品上的优化，一块是玩法上的创新。

## **1.产品优化**

![图片](https://oscimg.oschina.net/oscnet/up-e90702a5ed5d9d21c0ea4c513a3260960b5.png)

去年在LVS上海站大会上，我们发布了云端一体的方案：云上的RT-ONE™音视频通信网络和端上的视立方SDK RT-cube™方案。这是基于我们多年的音视频服务经验，发现很多开发者想要一个综合的解决方案，比如，在做音视频通话的时候是需要发文字消息的，做直播的时候是需要弹幕或者美颜的。因此我们做了一个决定：通过云端一体的方案来满足行业需求。推出市场一年之后，我们可以看到融合之后效果还是不错的，举一个简单的例子：我们之前的直播主要是RTMP推流CDN拉流, 端到端延迟在3到5秒左右，如果客户想要更低延迟的直播，那么可以用1秒左右延迟的快直播，还有客户需要主播间连麦互动时，300毫秒以内有RTC。但是，这么一个个的产品的SDK是比较分裂的，每个能力都需要单独对接。通过融合之后，开发者可以很方便地在这三者间做平滑的切换，比如主播要去互动的时候就用RTC，观众需要观看的时候就可以走高延迟的CDN直播或者快直播，如果主播想要互动的话，直接再切成RTC。通过这种融合，可以帮助开发者大幅度降低开发门槛，以及产品体验上的提升。All in one SDK包含了采集、美颜、传输、直播/点播的播放还有短视频内容剪辑，这些产品能力的组合，可以帮助行业内开发者更快实现一些产品功能。

![图片](https://oscimg.oschina.net/oscnet/up-be154df1896f229129b23702c578009ef20.png)

全球的网络部署方面也是很重要的。实际上，我们在很多国家发现网络覆盖还不好，比如在南美和非洲，我们的布点是不够的，经过过去几年的发展，我们在全球部署了2800多个加速节点，有200T的带宽储备，我们整个腾讯的27个区域，71个可用区都部署了RT-ONE™的服务，这保证了我们在全球能有一个比较好的网络覆盖。在比较好的网络覆盖的基础上，去做一些技术方面的优化就会变得更容易一些。

![图片](https://oscimg.oschina.net/oscnet/up-7ff1f4c22151726249cbbb0204a63097c80.png)

接下来就是QoS优化的问题。整个RTC传输里，大家遇到最重要的问题就是要怎么去对抗拥塞、丢包、抖动、延时这些问题。我们通过云端智能流控引擎，去应对海外的极端网络环境，比如限带宽，有些地区用户的下行网络只有100kbps左右；还有高丢包，突发抖动等场景。这里是RTC流控的整个架构图，我们可以看到在客户端上，在本地做音视频采集之后，会把CPU等相关网络参数实时上报，还有网络模块，它会对本地抖动做一个统计，并进行初步的带宽评估，与QoE相关的音频卡顿，视频卡顿统计之后，上传到智能流控引擎，智能流控引擎会根据实时数据做一个决策，去决定最终用一个怎样的码率进行传输。在极端的情况下，我们可以把视频的码率降到100k以内，来保证用户有最基础的通话质量。在最开始设计的时候，我们进行了一个模块化的设计，状态上报系统和云端配置系统是独立的，这保证了后期升级的时候更快速便利，例如，我们之前用大数据和人工智能去处理之前一些地区的数据去实现一个更好的QoS算法。另外，针对udp封禁的情况，我们降级至tcp去进行传输，为了保证用户能够实现一个基础通话。

![图片](https://oscimg.oschina.net/oscnet/up-39bd823bd129b541a7a127f2b75e6d92bb1.png)

另外一块就是音频的优化，我们使用了天籁实验室开发的信源信道联合编码技术。音频端到端传输包括从采集到前处理、编码、再到传输，再到解码、增强、回放。传统的音频传输优化集中在信道环节，比如FEC实际上是通过增加带宽消耗来降低丢包率，但FEC加多反而会导致更大的网络浪费。我们在这个基础上把优化聚焦到了信源这一环节，希望在信源侧淡弱FEC，实现更高效的带宽利用。除了本身信源侧的FEC之外，我们还结合了信道的FEC来联合调控，实现更高效率的带宽利用。大家可以看一下右图，这是一个高码率应用传输的比例，我们新的算法生成之后，FEC后高码率流下降30%，卡顿率下降50%，这是信源信道联合编码技术带来的优化。

![图片](https://oscimg.oschina.net/oscnet/up-d840fffc5b730d167c47dbf4624dfdaba38.png)

此外我们也通过音频混流来对下行带宽进行了优化。海外市场很多应用都是用来做语聊的，比如房间内有8个主播聊天，有几十个观众在收听，如果按正常的50k码率的音频来计算，每个人去拉流，就有400k码率的下行带宽需求。如果使用刚才的技术，其实能优化的也是有限的。所以我们选择在房间内把这些音频混在一起之后，再推回房间，然后用户拉流的时候只需拉一路，就能收到8个人的声音，这可以直接把下行带宽的占用从400k降到50k，这对用户的下行网络是有极大改善的。除此之外，比如语聊房中需要知道哪个主播在说话，主播音量的波动需要能体现出来，这在常规的单流拉流上很容易判断，但混流之后，就比较麻烦了，所以我们在做云端混流的时候加入每个发言人的身份和音量的信息到SEI里进行下发，同时我们也针对发言人比较多的情况进行一个能量竞争选路，来确保最终的产品能力和不混流是一样的。

![图片](https://oscimg.oschina.net/oscnet/up-46c98fff945d89c4ba3d22e0c174aa6578e.png)

在视频方面，我们也做了ROI编码优化的工作，我们把更多的码率分配至用户关注的区域上。通过观察左边原始的编码方案以及右边的ROI编码方案，我们可以看到主播脸部的清晰度明显提升，这是码率分配的策略，这个策略使得整个视频的码率降低20%-30%左右。

![图片](https://oscimg.oschina.net/oscnet/up-eedfa6185f239eeaea5668ee4d176eb5918.png)

另外就是之前提及的安全合规，这方面随着各国监管的逐渐规范，挑战也越来越严峻。如果提供基础技术的厂商无法通过这些认证，就会给客户带来很多合规风险，所以我们也做了大量的工作，比如我们需要独立部署的环境，国内外的隔离，来实现符合当地法律法规，也通过了一些权威认证比如ISO27017/27018/27701/29151、CSA STAR、NIST CSF等。对出海应用来说，合规是生命线，一定不能掉以轻心。

## **2.玩法创新**

下面，我分享一下与玩法创新相关的内容，之前也提及过，海外的创新玩法是层出不穷的。我们观察到，在线社交娱乐在向虚拟化、沉浸式发展演进。

![图片](https://oscimg.oschina.net/oscnet/up-2faa18a1b6de958ceb3309768f16a5d9485.png)

通过左边的曲线图，我们可以看出虚拟形象社交App的全球下载量增速非常迅猛，这也是为什么行业内都在关注的元宇宙等相关概念。受风俗文化影响，海外一些区域的用户习惯与国内用户不同，比如国内直播比较普遍，但是中东反而语聊更多，用户不愿意视频的情况下，虚拟形象可以用来代替露脸，能够传达的信息也比语聊更多一些。在虚拟形象社交App中，我们可以看到卡拉OK、语音聊天和儿童早教类App占比较高。

![图片](https://oscimg.oschina.net/oscnet/up-023e3738e2c42ae6da7754b2bc9fa137030.png)

针对这些情况，我们也提供了相应的方案。比如给QQ音乐开发的演唱会场景，当用户参与虚拟演唱会时，每个用户可以通过在聊天框内输入弹幕或者特定指令操作虚拟世界里对应人物。还有给斗鱼做的云蹦迪，还有下面的云端虚拟会展类场景都比较有意思，越来越多的社交应用也往这方面发展。

![图片](https://oscimg.oschina.net/oscnet/up-f98d0aa456b8b3d3bbf36269bc78138eb97.png)

针对这些行业需求，我们同样开发了云+端方案：

云渲染指的是画面是在云端渲染出来的，并不需要在客户端集成渲染引擎，只需要打开一个网页或者一个播放器就可以加载播放视频。在需要互动的情况下，通过信令也能传到云端。

端渲染是指在客户端通过SDK，通过Unity等渲染引擎实现的虚拟场景渲染，比如可以支持用户自定义“捏脸”，表情随动或者语音驱动。

除了视频渲染方面，元宇宙里音频方面的需求也可能不同，比如在沉浸实时互动中，可能需要无限用户上麦。在一个比较大的虚拟世界里，可能有上千人在互动，这些人都是开麦的，你走进一个区域，你能听见附近的人说话，听不见较远的人说话。之前，业内一般只能做到50人上限，这极大地限制了虚拟世界的广度，所以我们就开发了无限上麦的功能。还有3D空间音频，在一个虚拟世界里，我们希望听到的声音是有方位感的。上述功能构成了整个沉浸式实时互动方案。

![图片](https://oscimg.oschina.net/oscnet/up-949cfd9c0704f6c8e92925adef70a29875a.png)

这里也展开介绍一下基于云渲染的虚拟直播。之前提及QQ音乐的云端演唱会是一个比较典型的场景。我们底层提供了云渲染PaaS平台，这是基于腾讯的IaaS去部署的，可以渲染设计好的虚拟世界的场景，比如演唱会的场景，其中虚拟角色的场景，还有沟通软件等都可以在上面运行。在此基础上，我们可以实现多人的互动房间，动捕或者弹幕驱动虚拟人，在云渲染出来之后，可以通过快直播来拉流播放，例如QQ音乐的云端演唱会和斗鱼的云蹦迪。这项技术支持多人会议、线上会展、偏游戏类偏互动类的娱乐场景以及虚拟客服等。

![图片](https://oscimg.oschina.net/oscnet/up-68620a341892948436c9a21b25e5f3e896a.png)

这是云渲染虚拟直播方案的架构图。从观众端来看，通过网页或者播放器拉云端渲染出的直播流进行播放，用户操作通过弹幕或者鼠标按键采集后，通过信令通道发送到云端的云渲染引擎。云渲染会结合用户输入把视频模型渲染出来。同时，也有一些活动控制插件，比如角色、场景、镜头和控场。主播端的权限会多一些，云渲染为他渲染出独立视角，供全网用户直播观看，如果有观众想加入到活动现场看到自己视角的画面，云渲染也可以为他单独渲染一幅个人视角的画面。

![图片](https://oscimg.oschina.net/oscnet/up-73013ffae6adfc8b820500642b625f3a9f8.png)

其次是基于端渲染的虚拟直播方案。刚才提到的云渲染在一些场景下还是存在限制，比如用户的下行网络不流畅的情况下，视频还是会出现卡顿，其次云渲染的画面成本较高，更适合多人直播的场景，不适合每个人都去渲染画面。在此情况下，端渲染还是有其独一无二的优势。它不需要太多的网络带宽，只用传很小的信令数据就可以实现驱动。我们开发了一套Avatar的虚拟形象和3D虚拟世界的套件，让用户可以通过拍照自动生成自定义形象，用户可以用声音、面部表情进行驱动。在虚拟世界里，面部可以有不同的视角，第一人称或者第三人称的视角，用户还可以做动作，比如移动、打招呼等。我们也开发了一些常见的场景如会议类、KTV类、蹦迪类等场景。

![图片](https://oscimg.oschina.net/oscnet/up-3381d4c82dada3e7d66c2ffecbc14f39778.png)

再者是我们刚才提及的无限上麦，最初我们称其为千人上麦，然后我们发现也不止千人，更多人数也是可以支持的。它最典型的场景是右下角的场景，每个人在一个场景里，每个人都能听到附近的人说话。整个虚拟世界里可能有上千人，所有人都是可以开麦开摄像头的，我们只需要在拉流的时候只拉附近的流就可以，我们可以输入他的位置去进行声音的选录，确保他只听到一部分与他相关的人的声音。

![图片](https://oscimg.oschina.net/oscnet/up-7c38c27a5706eb2bba7c66f4bba6800b167.png)

**3D音效体验** 音频： 进度条 *00:00* *00:51* 后退15秒 倍速 快进15秒

↑↑3D音频请使用耳机收听↑↑

最后是虚拟世界里的空间音频：基于位置的3D实时语音。大家都能够容易的分辨出声音是从哪个方向来的，是因为如果一个声音在不同的位置，我们两只耳朵听到的声音不一样。基于这个原理，我们只需把两只耳朵听到的声音记录下来，回放出来，我们就会觉得这个声音就是在某个位置的。HRTF技术就是实现声音从无方位感到有3D位置声音的转化，实现空间音频的效果。在虚拟世界里，比如在虚拟会议和娱乐类的场景中，如果我们希望更有沉浸感的体验，可以通过这个技术来实现。比如在吃鸡类的游戏中，大家可以通过这个技术去锁定对方的位置。

## **3.未来的挑战与思考**

刚才提到了一些我们具体的技术优化和玩法创新，截至目前为止，我们服务了国内大多数的出海企业，比如在海外很火的Weplay、SoulChill 等游戏或社交类应用。腾讯云音视频作为国内音视频云服务的代表，在支撑国内应用出海方面责无旁贷。

![图片](https://oscimg.oschina.net/oscnet/up-f780e970f409e9886549da7996b2a84ba47.png)

最后，我也分享一下我们对于未来的挑战和思考。国内的大环境越来越内卷，市场竞争特别激烈，新业务发展特别困难。经过多年激烈市场竞争的洗礼，很多国内互联网公司的产品还是很有竞争力的，我们可以借着现在的机会走出去，把优秀的产品输出到更多国家，寻找更广阔的市场。当然出海的过程可能会面临种种挑战，比如要做好本地化运营和跨国团队的管理，要从零建立生态链，但总的来看当前是最好的时机。腾讯云作为底层音视频技术提供方，也希望能和大家一起共创，把腾讯积累的成熟技术分享给业内更多从业者，让大家出海更顺畅一些。也期待跟更多从业者交流，一起把中国企业出海做大做强！

原文作者：*崔立鹏*

原文链接：https://www.livevideostack.cn/news/tencent-cloud-real-time-audio-and-video-technology-practice-and-landing/



# 【NO.384】MPEG音频编码三十年

**前言**

很明显，声音信息的电子格式要早于视觉信息的电子格式，用电子格式分发声音信息的服务也是如此。同样，音频的数字格式与视频数字格式的出现时间也不同。在上世纪80年代初，唱片公司可以通过CD（Compact Disc）向消费者市场发行数字音频，而在80年代后期出现的D1数字磁带则专用于录音室的专业应用。但压缩技术却颠倒了它们的出现顺序：压缩数字视频比压缩数字音频要早10年。所以和之前我所写的文章《视频编码四十年》[1]不同，本文的标题是《音频编码三十年》。

如果**音频**缺乏恰当的定义，那么这篇文章有可能成为争议来源。在本文中，我们所指的音频是人类可听范围内的声音，而非人类发声系统所产生的声音或者发声模型不可用的其他音源。实际上，早在CD出现之前的20年，数字语音就已经存在于专业应用（trunk network，主干网络）中了。ITU-T G.721的“32 kbit/s ADPCM（adaptive differential pulse code modulation，自适应差分脉冲编码调制）”可以追溯到1984年，同一年H.120被批准为推荐标准。

因此，本文的标题也可以是《音频编码四十年》。不过如果以此为标题的话，本文将充斥大量的语音压缩标准，而使真正的音频标准失去了关注。因此，本文将只关注音频压缩标准，其中的音频并不包括语音。但其中有一个例外，我会在下文提到。

与视频压缩中ITU-T（国际电信联盟电信标准化部门）这个非MPEG组织开发了视频编码标不同，音频压缩中MPEG占据主导地位。需要音频压缩用于数字音频广播标准的ITU-R（国际电信联盟无线通信部门）也倾向于依赖外部资源，其中就包括MPEG。

**MPEG-1 Audio**

关于MPEG（专注于视频压缩领域的组织)如何以及为何会研究音频压缩，感兴趣的人可以阅读这篇文章（其中还包括许多其他内容）：https://ride.chiariglione.org/the-1st-mpeg-project/。 MPEG音频工作组成立活动于1998年12月1-2日举行，当时最多元化的音频编码专家组在汉诺威（德国城市）会面（当时这一传统还没有完全确立）并启动了这项工作，在大家的不懈努力下，最终MPEG在1992年11月推出了MPEG-1音频标准。

MPEG中的音频组常常是未来创新的先行者。比如，其一，广播世界还在回避低分辨率的MPEG-1视频压缩标准时，它却非常重视MPEG-1音频压缩标准；其二，与视频主要依赖相同的编码架构不同，音频征集提案需要产出两类算法：一类充分成熟、易于实现、但是效果稍差；另一类算法更新、在当时却比较难实现，但效果更好。将两类算法合并的工作非常辛苦，但终于实现了使用这两种技术的三层（3 Layers）标准。

Layer 1用于数字压缩磁带（DCC），这个产品在几年以后停用了；Layer 2用于音频广播，以及作为Video CD（VCD）的音频组件使用；第三层（MP3）就不需要特别介绍了。MP3在接下来的MPEG-2中进行修订后，它在听感上与原CD信号几乎没有差异的同时，提供128 kbit/s的码率。这与原本CD信号1.44 Mbit/s的码率相比，实现了11:1的压缩。

**MPEG-2 Audio**

该标准于1994年批准，主要目的是用于多声道音频，其中的关键要求是MPEG-1音频解码器应能解码MPEG-2音频码流的立体声组件。向后兼容在广播领域十分有用，因为运营商可以升级到多声道服务，而不会失去只拥有MPEG-1音频解码器的客户。

**MPEG-2 AAC**

人们希望提供最佳质量音频而不受向后兼容约束，这种需求推动了MPEG-2 AAC的工作。这意味着Layer 2必须同时解码Layer 1和Layer 2，而Layer 3必须解码所有Layer。1997年4月所发布的MPEG-2 AAC就是构建在MP3技术之上，可以提供128 kbit/s 码率听感透明的立体声信号和320 kbit/s 的5.1声道信号（例如数字电视中的信号）。

**MPEG-4 AAC**

1998年，MPEG-4 AAC与其他两个MPEG-4组件（Systems and Visual，系统和视觉）一同发布。MPEG-4 AAC也是构建在MPEG-2 AAC之上。2003年，Apple宣布旗下的iTunes和iPod将使用MPEG-4 AAC作为主要的压缩格式，这一举动动摇了MP3在音乐发行中的主导地位。大部分PC、智能手机和之后的平板都可以播放AAC歌曲。Apple没有把AAC当作纯粹的播放器技术使用，它推出的iTunes服务提供以MPEG-4 文件格式打包的AAC格式的歌曲，文件扩展名为“.m4a”。

**AAC-LD**

1999年，MPEG发布了包含AAC低延迟版本的MPEG-4 Amendment 1，被称为AAC-LD（Low Delay AAC，低延迟AAC）。AAC编码器/解码器通常需要约55毫秒的单向延迟（转换延迟+look-ahead处理），而AAC-LD通过简化和替换某些AAC工具（更低延迟的新转换和look-ahead处理的移除）达到了仅21毫秒的单向延迟。AAC可以作为通话编解码器使用，同时具有音乐编码器的信号带宽和感知音质，并且可以对单声道信号在64 kb/s码率时实现卓越的音质。

**MPEG-4 HE-AAC**

2003年，作为MPEG-4的第一次修订，MPEG发布了MPEG-4 HE-AAC（High Efficiency Advanced Audio Coding）。HE-AAC巩固了移动手持设备作为高音质音频获取工具的地位，能够以48 kbit/s接收高音质立体声音乐（压缩能力比AAC高出2.5倍）。与CD信号相比，它的压缩比几乎达到了30:1。

HE-AAC向核心AAC引擎添加了SBR（Spectral Bandwidth Replication，频谱带宽复制）。由于AAC已被广泛部署，因此允许通过仅将SBR工具添加到现有AAC实现中而扩展为HE-AAC。

**MPEG HE-AAC v2**

同样在2003年，9个月之后，MPEG 推出了MPEG HE-AAC v2。它源于MPEG-4二次修订（Parametric coding for high-quality audio，高质量音频的参数编码）中所包含的工具。虽然核心参数编码器并没有被广泛接受，修订中的PS（Parametric Stereo，参数立体声）工具可以将立体声音乐高效编码为单声道信号加少量辅助信息（side information）。 HE-AAC v2——PS工具和HE-AAC的结合，能够以32 kb/s传输具有优质音频质量的立体声信号。

3GPP也采用了HE-AAC v2，并将其命名为Enhanced aacPlus。3GPP的采用为HE-AAC v2技术集成到移动电话创造了条件。今天，超过百亿的移动设备支持HE-AAC v2格式歌曲的传输和播放。由于HE-AAC构建于AAC之上，所以这些设备也支持AAC格式歌曲的传输和播放。

**ALS 和SLS**

2005年，MPEG发布了两种用于无损压缩音频的算法——ALS（Audio Lossless coding）和SLS（Scalable to Lossless coding）。这两种算法都可以完美（即无损）重建标准的CD音频信号，压缩比接近2:1。具有可变的压缩比是SLS的一个重要特性：它可以将立体声信号压缩到128 kb/s（11:1压缩比），并达到和AAC编解码器一样的出色质量，但它是通过持续增加编码码率（即降低压缩比）实现压缩比为2:1的无损重建。

**MPEG Surround（MPEG环绕声）**

ALS/SLS是MPEG-4中最后一批重要的标准，它们也是MPEG中“最长寿”的音频标准。第一个版本于1999年发布，20年以后（2019年），MPEG才发布了第五个版本。

在MPEG-4退出历史舞台之后，MPEG开发了MPEG-D系列音频编码标准。其中第一个就是MPEG Surround（MPEG环绕声），于2007年发布。在某种意义上，这项技术是HE-AAC v2工具的通用PS（Parametric Stereo，参数立体声），MPEG Surround可以被用作5-2声道压缩工具，或者M-N声道压缩工具。这个通用的PS工具其后就是HE-AAC编解码器。因此，MPEG Surround构建在HE-AAC之上，正如HE-AAC构建在AAC之上。MPEG Surround为低码率应用的立体声和多声道之间提供了连接。它具备良好的压缩效果，同时保持了非常清晰的音质，且算力消耗较低。虽然HE-AAC能够以48 kbit/s传输立体声，但MPEG Surround可以在相同的48 kbit/s传输预算下传输5.1声道音频，且复杂度不超过立体声 HE-AAC。因此，想要将立体声服务拓展到5.1声道音频，可以使用MPEG Surround直接替代。

**AAC-ELD**

2007年，MPEG推出了AAC-ELD（Enhanced Low Delay AAC）技术。这项技术结合了来自其他标准的工具：HE-AAC v2和AAC-LD的SBR和PS。这个新的编解码器在适度增加延迟的同时提供了更加强大的信号压缩：AAC-ELD以48 kb/s的速度为单声道信号提供出色的音频质量（单向延迟仅为32毫秒）。

**SAOC**

2010年，MPEG发布了MPEG-D SAOC（Spatial Audio Object Coding，空间音频对象编码），它可以高效编码多声道信号（多个对象的混合，比如将各种单一乐器混合）。SAOC将多声道信号下行混音（例如立体声到单声道），先将单声道信号和一些辅助信息进行编码和传输，然后将接收到的单声道信号和辅助信息进行解码然后上行混音到立体声信号，以便用户感知到乐器被放置在正确位置，且最终立体声信号与原始信号相同。这是通过以下方法实现的：在任何时刻任何频率区域，其中一种乐器将更倾向于主导其他乐器，所以此时/此频率区域，其他信号（如果存在的话）将更难被感知到。SAOC分析输入信号，将每个声道分为时间“块”和频率“块”，然后再确定每个“块”的主导程度。这些信息都被编码为辅助信息。

SAOC的一个应用是远程会议，其中多个地点的会议电话可以在会议连接时被混合为单一声道，并和SAOC的辅助信息一起传输给每个会议参与者。在用户终端，单声道被上行混音为立体声（或者三声道：左-中-右）并进行呈现，使得每个远程会议参与者都处于前音场的不同位置。

**USAC**

2011年，MPEG推出了USAC（Unified Speech and Audio Coding），它将语音编码和音频编码工具合并到一个算法中。USAC结合了MPEG AAC工具（利用人类感知音频方法）和最先进的语音编码工具（利用人类产生语音的方法）。因此，编码器同时具备感知模型和语音激励模型/声道模型，可以每隔20毫秒动态地选择音乐/语音编码工具。使用这种方法，USAC在压缩任何输入信号（无论是音乐、语音还是音乐和语音的混合信号）时都达到了高水平的性能。

按照MPEG标准的传统，USAC将立体声信号的“良好”性能扩展到低至16 kb/s，并随着码率的提升提供更高的音质。128 kbit/s的立体声信号质量稍好于MPEG-4 AAC，因此USAC可以代替AAC，因为在任何码率下，USAC在性能上都能与AAC匹敌，或者优于AAC。类似地，USAC能够编码多声道音频信号，并且能够以最佳方式编码语音内容。

**DRC**

MPEG-D DRC（Dynamic Range Control，动态范围控制）技术可以让听者能够控制音频水平。它可以作为每种MPEG音频编码技术的后处理器使用，并在播放时修改解码信号的动态范围。DRC可用来降低电影中音量最大的部分，这样就不会打扰到你的邻居。它可以在恶劣音频环境下（汽车、公交车、房间里有很多人）放大音频中的安静部分，还可以将音频的动态与智能手机扬声器的输出（其动态范围通常非常有限）匹配。在规范音频输出信号音量方面（在一些监管环境中可能会强制执行），DRC标准也发挥了非常重要的作用。DRC发布于2015年，并在2017年扩展为Amendment 1参数型DRC，增益映射和均衡工具。

**3D Audio**

MPEG-H 3D音频发布于2015年，一如既往，它也是MPEG系列工具（系统、视频和音频）中的一部分。它提供了沉浸式音频内容的高效编码：编码范围通常在11~22个内容声道。3D音频算法可以处理任何声道组合、对象以及HOA（Higher Order Ambisonics）内容。HOA中对象为单声道音频，具有动态的时间位置，HOA能够将整个声场编码为多声道“HOA系数”信号。

由于3D音频内容是沉浸式的，所以可以将其看作是一部360度“电影（即视频加音频）”。用户坐在360度球体中心（最佳聆听地点），音频被解码和呈现给用户，这样用户就会感知到来自周围环境中某个地方的音频。MPEG-H 3D音频也可以通过耳机呈现，因为并不是每个用户都拥有11或者22声道的聆听空间。除此之外，MPEG-H 3D音频支持HRTF（Head Related Transfer Function，头相关变换函数）的默认和个性化使用，以便使听者感知音频内容（仿佛来自听者周围的音源），就像使用扬声器一样。3D音频在耳机中播放所添加的一个功能是：无论听者如何转动头部，听者所听到的音频可以一直保持在“正确位置”。也就是说，当用户看向前方时，声音是来自“前方”，而当用户转动头部看向右侧时候，声音会被感知为来自用户左侧。因此，对于360度视频应用来说，MPEG-H 3D音频已经是一个接近完整的解决方案了。

**结语**

在这三十年中，MPEG的音频组向市场输入了一代又一代的音频编码标准。作为MPEG最佳传统，这些标准在某种意义上是通用的，可用于纯音频或者音频+视频应用程序。随着新一代音频编码标准构建在前代编码标准之上，这些编码标准通常可进行扩展。

下图展示了这三十年的音频编码之路。

![图片](https://oscimg.oschina.net/oscnet/up-8b45373170944c378ddcac1e21621fcc099.png)

令人遗憾的是，MPEG组织已经不复存在了，但是关于MPEG的记忆都保存在了我的博客上：

https://mpeg.chiariglione.org/，欢迎大家阅读。

**注释：**

[1] https://blog.chiariglione.org/forty-years-of-video-coding-and-counting/

**致谢：**

本文已获得作者Leonardo Chiariglione授权翻译和发布，特此感谢。

**原文链接：**

https://blog.chiariglione.org/thirty-years-of-audio-coding-and-counting/

**特别说明：**

作者在原文基础上进行了修改，并审阅了本篇中文译文。

原文作者：Leonardo Chiariglione

原文链接：https://www.livevideostack.cn/news/thirty-years-of-audio-coding-and-counting/



# 【NO.385】利用WebTransport进行现场视频流注入

编者按：通过网络支持的实时音视频通话已成为人们日常生活和办公中必不可少的一部分，对于音视频领域的网络技术要求也越来越高。对此，LiveVideoStack 特别邀请到了来自美国 Paramount Global 的张博老师，他以《利用 WebTransport 进行现场视频流注入》为题来进行相关内容分享。

文 / 张博

整理 / LiveVideoStack

 

![图片](https://oscimg.oschina.net/oscnet/up-a2aa309b88a16464f6b6893f133acc9bfa9.jpg)

大家好，我叫张博，目前在美国波士顿，供职于美国 Paramount Global 公司。Paramount 是美国五大电影制片公司之一，国内叫派拉蒙影视。Pluto TV 是它旗下的一个 streaming service 流媒体。我是负责视频编码和播放系统设计的架构师。在此之前，我还供职于其它的视频技术公司，包括 Fubo TV，Brightcove，Ericsson。在供职于 Brightcove 公司期间，我曾担任过多个国际视频制定标准委员会的委员，包括 MPEG，INCITS L3.1，DASH Industry 工业论坛和 CTA-WAVE，并且曾经参与过 MPEG-DASH 和 MPEG-CMAF 标准的制定工作，我还曾经参与 Brightcove 公司著名的 Zencoder 编码系统和开源视频播放器 Video.js 的开发工作。

今天我要讲的话题是利用 WebTransport 进行现场视频流注入，英文叫 Live Video Ingest via WebTransport。Pluto TV 是 Paramount 旗下的一个流媒体服务。Paramount 公司有自己的院线、电影院和 streaming service，因此我们线上线下都有放送的平台。Pluto TV 不需要交会员费，我们是完全通过广告的营收来支持营运。Pluto TV 大概有几百个频道，其中包括 Paramount 下属的其它传统电视频道（比如 CBS 新闻网络，Nickelodean，Showtime），另外也包括一些由众多单个 VoD 内容串联起来的虚拟直播频道。我们基本上都是靠广告营收，在广告上有很多创新。不过今天我要讲的话题跟我的工作其实没有关系。我们也有一部分的现场直播的频道应用，但是现在还没有运用到 WebTransport，因为 WebTransport 是一个比较新的技术，2019 年才正式制定出版协议上线，现在还是在一个定稿阶段。

![图片](https://oscimg.oschina.net/oscnet/up-2f4882e80cc94af6268b92f58b0dd054e79.jpg)

我今天演讲分为三个部分：首先是对 WebTransport 的简单介绍；接下来会分享我提出的一个新的方法：利用 WebTransport 进行 Live Video Ingest 现场视频流的注入；最后我会做一个概念证明，这个 idea 提出来以后需要去证明它真的可以被做出来。

**01 WebTransport 简介**

![图片](https://oscimg.oschina.net/oscnet/up-a669a7be44b2474db83ef2a31da991257b4.jpg)

首先是来简单介绍一下 WebTransport。WebTransport 是一种基于 HTTP/3 的新型网络传输协议，它支持以下功能：包括双向通讯（就通讯双方可以给对方发送 message 和 datagram）、安全传输（所有数据传输都是经过 TLS 加密的），它有两种数据传输模式：一种是基于 stream 的，类似于 TCP 的可靠传输；还有一种是基于 Datagram 的快速低延迟传输，有点类似于 UDP。它还有一个功能是 NAT and firewall traversal，它可以穿透 NAT 和防火墙，支持跨互联网的传输。通常人们把 WebTransport 跟另外两个协议进行对比，一个是 Websocket，一个是 WebRTC。那么 Websocket 是基于 HTTP/2 (第二代 HTTP 协议），WebTransport 是基于 HTTP/3（第三代的 HTTP 协议）。一般认为未来 WebTransport 会取代 Websocket 用在很多游戏和交互比较多的应用上。WebTransport 有很大的发展前景，因为 WebTransport 基于 HTTP/3，所以它比基于 HTTP/2 的 Websocket 拥有更快的传输速度和更低的延迟。另外一个经常对比的协议就是 WebRTC，WebRTC 必须要依靠 ICE（Interactive Connectivity Establishments）协议来让通讯双方知道对方的 IP 地址和网络端口，如果通讯双方没有直接的网络连接的话，它还需要通过中间的通信的基础设施 communication infrastructure 来建立连接。那么在这一点上 Websocket 和 WebRTC 就不如 WebTransport，因为它是直接运行在 443 网络端口上的，所以它天然具备穿透 NAT 和防火墙的能力，现有的 Web Infrastructure 就可以无障碍的支持 WebTransport，所以它相较于 WebRTC 更简单一些，也更易于部署，不需要额外的基础设施投资。

![图片](https://oscimg.oschina.net/oscnet/up-0f11bf5e544886f2cf359bcb0e36ce15a6b.jpg)

WebTransport 是运行在 HTTP/3 和 443 网络端口之上的一种 Client server 协议，和 Websocket 一样，每一个连接双方都会有一个 HTTP/3 的 connection。比如一个传统 Client、一个 browser 浏览器和一个 HTTP 服务器之间，现有的服务器和客户端原本就支持 HTTP，但是我们现在只需要让它额外支持 WebTransport，通讯双方就可以具备 Websocket 和 WebTransport 的能力，它会先建立一个 HTTP/3 的 connection 然后在 HTTP 的 connection 里它会允许建立多个 WebTransport 的 session，每一个 session 都是独立的传输单元，都有自己独立的 session ID。包里面会有一个 session ID 在 header 里，这样的话它就可以区分不同的 session。

连接的建立是由连接的发起方通过 extended CONNECT method 来发起连接的请求，跟 Websocket 是一样的。双方都需要支持 WebTransport 连接才可以建立。在创建 HTTP 连接的时候就需要在 setting frame 里将 SETTINGS_ENABLE_ WEBTRANSPORT 的参数设置为 1。当它看见对方的 setting frame 里参数被设为 1 以后，它就知道对方是支持 WebTransport 的，然后它才会允许连接的建立。流量控制和拥塞控制是由底层的 QUIC 协议来负责，就是 flow control 和 congesting control。

![图片](https://oscimg.oschina.net/oscnet/up-0c18107b28b455a8a8cd58d4a858870d5b1.jpg)

WebTransport 支持单向，也支持双向数据传输，基于 TLS 的安全数据传输可以无障碍穿越 NAT 和防火墙。它有两种传输模式，一个是 stream，一个是 datagram。stream 支持可靠的、有序的数据传输，而 Datagram 就只管发给对方，它不会重发，也不会流量控制的数据传输，所以它的速度会快一些。stream 是比较可靠、有序的传输。

**02 利用 WebTransport 进行现场视频流注入**

第二部分是 WebTransport 在视频方面有哪些应用。首先我想到的就是把 WebTransport 用来进行现场视频流注入 Live Ingest。

![图片](https://oscimg.oschina.net/oscnet/up-0995d4b8b54e4895b67d2c1145539485a24.jpg)

图片是现有传统的现场视频流注入方法，有上、中、下三种现有的模式，第一种是最上面的，用 RTMP 作为视频注入的媒体，从左到右是视频源 video source，把原始视频以 RTMP 流的方式发给 ingester 注入端，注入端拿到后，把数据给转码器和封装器，封装器拿到以后把视频流封装成 DASH 和 HLS 的 stream，然后发给 origin server，再发给 CDN，一直到播放器 player。我们知道 RTNP 是基于 TCP 的，所以它的延迟会比较大，因为它中间需要做一些 buffering，连接的建立也会更费时，一般会有 2 到 3 秒的延迟。

WebRTC 我不知道国内用的多不多，是只用作 live ingest，还是直接对终端用户进行视频直播。但是，美国的一些公司把 WebRTC 作为视频注入协议，它跟 RTMP 类似，把原始的视频流与 WebRTC 流的格式发给 ingester。但是 WebRTC 比较依赖 ICE 和底层的 infrastructure，所以它的协议更复杂一些，需要额外的基础设施部署。

最下面的第三种方法在传统的广电网络里面应用比较多，美国传统的 cable broadcast 公司是直接使用 mpeg-ts 和 multicast，然后直接把数据从视频源发给注入端，这个方法的速度很快而且也很稳定，但这个是基于 broadcaster 也就是广电的电视网，它的网络是独享的，不是共享的公共互联网，它没有任何的网络的 jitter，所以它很快。但是 OTT 的 streaming service 是不可以享受到红利的，所以在互联网中它无法使用，因为互联网一般是不允许 multicast 工作的。

![图片](https://oscimg.oschina.net/oscnet/up-21396e9675b51bde24ad88184b1da72db98.jpg)

基于刚才说的三种模式，我发现它们都有一些各自的问题，那么我想到 WebTransport 来进行现场直播流注入的基本思路就如上图所示这样，首先是视频源 Client serve，注入端就是 server，视频源相对于注入端它是 Client，输入端是 server。Client 和注入端 server 建立一个 WebTransport 的连接，就像中间这样一个管道，然后 Client 通过 WebTransport 管道把 mpeg-ts 的流或其它格式的视频流通过管道传输给 server。WebTransport 本身并不对原始的视频流做任何的修改和优化，它仅仅只是一个管道而已，因为管道具备安全性、较低的延迟，还可以跨越互联网。所以我们就把视频源直接通过管道发给注入端，可以让它更安全、更低延迟地、更及时地传送到另一端。

![图片](https://oscimg.oschina.net/oscnet/up-610faafb6d7c54628055d2c82accb3859cf.jpg)

WebTransport 相对于几个现有的传输方法的优势是易于部署。因为它是基于 HTTP 之上的，现有的 Web Infrastructure 就可以支持，视频的注入无需额外基础设施的投资，然后它是基于 443 端口，所以可以无障碍穿越 NAT、防火墙。低延迟是它一个最大的优势，因为 WebTransport 有 datagram 的传输方式，而现有的 Websocket 是没有的，WebRTC 是有的，但它都是 UDP、RTP 的，需要依赖 ICE，所以它可以支持高效、低延迟的传输，相对于 RTMP 和 HTTP 这两个基于 TCP 的协议要具备优势，因为低延迟对于视频直播尤为重要。

**03 概念证明**

概念是很简单的，也没有很多复杂的概念，但是我需要对我的方法做一个概念证明 Proof of Concept。

![图片](https://oscimg.oschina.net/oscnet/up-3b4947ad65a86c9023af47d90dead475fe0.jpg)

现有对 WebTransport 的支持其实并不多，因为第一版协议 2019 年才提出，那么现在客户端这边支持它的浏览器只有 Chromium，Chromium 是 Chrome 的实验版本产品，通用版的 Chrome 都不支持 WebTransport。Google 有自己的一个简单的 Client 和 server 的 WebTransport 的实现。Client 是用 Javascript 写的，它调用 Chromium 提供的 WebTransport API 来进行连接的建立和传输，然后 server 是用 Python 写的，它调用 AIOQUIC 库，是一个 Python 的 QUIC 的 library，然后我利用 Google 提供的 Client 和 server 的实现做了一个自己的 PoC，程序在 Github 上面可以找到 Live Video Ingest via WebTransport。

![图片](https://oscimg.oschina.net/oscnet/up-87090c1a05ae7360bd5d67f3a81443f7bf2.jpg)

![图片](https://oscimg.oschina.net/oscnet/up-7bd76441169b7078f4e8d5167499b258865.png)

我的实现 PoC 的思路是这样的，首先让 Client 程序和 server 程序建立一个 WebTransport 的连接然后我会让 Client（一个 Javascript 的程序，它是基于 Google 的 WebTransport Client）每隔数秒钟抓取一次摄像头的视频，然后我把摄像头的视频，封装成 WebM 格式，然后 Client 会将 WebM 文件通过 WebTransport 管道发送到 server 那边，server 拿到 WebM 文件后把它用 FFmpeg 转格式成为 MP4 文件，然后存到一个 webserver 目录下，比如说 EngineX 的目录下，然后提供给 video player 进行下载播放。

那么为什么传输格式可以是 mpeg-ts，但我要用 WebM 呢？因为有一些技术上的限制。WebTransport 的客户端仅仅只被浏览器支持，那么 Client 只能是一个 Javascript 程序，我们无法将 FFmpeg 生成的 mpeg-ts 的视频流发给运行在浏览器中的 Client，我没有找到合适的方法来做这件事情，所以我只能用 WebM 格式进行，流传输在我的 PoC 里面是这样的，但是我相信将来 WebTransport 会有更多的本地的 native 的支持，将来我们可以直接把 Web mpeg-ts 流直接通过 WebRTC，而不需要通过浏览器发送到 server 那端。

![图片](https://oscimg.oschina.net/oscnet/up-e44ec6b3fab6a53c6e4384030c8bb96e81b.jpg)

具体的实现是我们让 Client 调用 Chromium 提供的 API 来建立一个与 server 之间的 WebTransport 的 datagram 连接，我之所以采用 datagram 而不采用 stream 是因为 datagram 可以快速地传输数据，然后连接建立以后，Client 会调用 getUserMedia API 来抓取摄像头的视频，并创建一个 video 窗口进行本地的视频播放，然后 Client 会每隔 4 秒钟调用 MediaRecorder API 抓取的视频录制成 WebM 文件，然后将 WebM 文件以 datagram 的形式分段通过 WebTransport 发给 server，每一个 datagram 的长度是 1,200 个字节，这由底层协议的最大报文长度决定的，server 在收到 WebM 文件后用 FFmpeg 把 WebM 文件转格式成为 MP4 格式，然后把它存到 server 的下载链接目录下，并且 server 把新文件的 HTTP 下载链接回复给 Client，Client 拿到链接以后就下载 MP4 文件，然后在另外一个 video 窗口进行播放，如果我把这两个 video 播放窗口并列摆放的话，我们就可以看到整个流程的延迟，即本地视频是直接播放的，下载的视频是经过 WebTransport 和 FFmpeg 的转码再经过 HTTP 的下载，这三个步骤才可以播放，那么这两个摆在一起的话就可以看到整个流程的延迟。

![图片](https://oscimg.oschina.net/oscnet/up-06b19ca84bf55489424a9491357d07c2248.jpg)

下面是一个简单演示 demo。我把 server 部署在 AWS EC2 的机器上，Client 运行在本地的 Chromium 浏览器上。那么我需要打开 443 端口并且允许 UDP traffic 通过。如果以前都是 443 的话，我们只需要打开 TCP。对于 QUIC 和 WebTransport 这种协议来说，我们还需要打开 UDP 的端口以便让 WebTransport datagram 能够到达 server 那边。

![图片](https://oscimg.oschina.net/oscnet/up-2e969e12fa3e766d4074dbcddd7ac2130d5.jpg)

本来的 demo 应该运行在 AWS 上面，但我只有一个免费的那个账户，今天的时间已经到了，所以今天我的 demo 只能运行在本地的机器上面，这是 demo 的截屏。

![图片](https://oscimg.oschina.net/oscnet/up-6f1578584a05d0365dcbd08a18ea6c6c3a2.png)

我现在就要运行一下 demo，窗口是 WebTransport 的 server 的 Python 程序。WebTransport server 要读取本地的 certificate 就是 TLS 的 certificate 和 key，我是让它跑在 443 端口上面，这本身没有区别，然后 chromium 浏览器它需要允许特定的 IP 和端口，然后把 Client 拿出来，本地窗口抓取的视频直接播放，然后我点 connect server 那边就拿到了连接，然后我这边开始抓取视频，ingest 注入到 server 那边 start，然后如果我们等几秒钟就能够看见两个视频可以同时收到。那么我们看一下手机上面的时间，延迟大概是有 4 秒钟左右，有的时候是 4 秒，有的时候是 3 秒，有的时候是 2 秒因为 API 时间并不稳定，每一个 segment 的时间并不稳定。

![图片](https://oscimg.oschina.net/oscnet/up-9b7ae6f6334b824113ad59f38c3e9331bd9.jpg)

因为时间有限，我只完成了一个 PoC 的实现，那么未来我还需要将我的新方法跟其它现有的方法进行比较。那么我很关心的是两个比较参数，一个是它的延迟，一个是丢包率。因为我知道，我用的是 datagram 进行数据传输，用视频流注入的延迟会比较低，但是低多少我还得具体去衡量。再一个就是丢包率，因为用 datagram 的话会有一些丢包，它不保证可靠的传输，那么对视频播放的质量也会有些影响，我要去看具体的影响有多大，然后我再进行一些改进和优化。

![图片](https://oscimg.oschina.net/oscnet/up-608c41f3da5e073aaccae99ad7e25c6df65.jpg)

以上就是本次的所有分享内容，上图是相关的 reference，谢谢大家！

原文作者：张博

原文链接：https://www.livevideostack.cn/news/%e5%88%a9%e7%94%a8webtransport%e8%bf%9b%e8%a1%8c%e7%8e%b0%e5%9c%ba%e8%a7%86%e9%a2%91%e6%b5%81%e6%b3%a8%e5%85%a5/

# 【NO.386】WEBRTC 笔记

## 1.WEBRTC 是什么

它的全称是WEB Real-time communication。一开始我还觉得是一种通讯技术。这里的communication主要是人与人之间的，因此它解决了在网页视频、音频的播放和获取的问题。它的目标是但愿用户之间直接通讯，而不是经过服务器来进行交互。简单地说就是在浏览器上实现视频通话，并且最好不须要中央服务器。html

你们应该仔细看看这个[教程](http://www.noobyard.com/link?url=https://www.html5rocks.com/en/tutorials/webrtc/basics/) ，我但愿这篇笔记能够更快地帮助你们理解，说明一下比较容易困惑的点，少走一些弯路，而不是取代这篇教程。html5

## 2.核心技术

1. [getUserMedia()](http://www.noobyard.com/link?url=https://webrtc.github.io/samples/src/content/getusermedia/gum/) : 获取视频和音频。git
2. [MediaRecorder](http://www.noobyard.com/link?url=https://webrtc.github.io/samples/src/content/getusermedia/record/) : 记录视频和音频。github
3. [RTCPeerConnection](http://www.noobyard.com/link?url=https://webrtc.github.io/samples/src/content/peerconnection/pc1/) : 创建视频流。web
4. [RTCDataChannel](http://www.noobyard.com/link?url=https://webrtc.github.io/samples/src/content/datachannel/basic/) : 创建数据流。浏览器

## 3.实际问题

然而在现实中网络是不通畅的，2个浏览器之间没法直接创建链接，甚至都没法发现对方。为此须要额外的技术来完成链接。服务器

1. [ICE](http://www.noobyard.com/link?url=https://www.html5rocks.com/en/tutorials/webrtc/basics/#ice) 这个框架应该是嵌入浏览器内部的，咱们并不须要了解太多的细节。网络
2. [signaling](http://www.noobyard.com/link?url=https://www.html5rocks.com/en/tutorials/webrtc/basics/#toc-signaling) 就个人理解，这个至关于媒人，来帮助2个浏览器来创建链接。框架

## 4.创建链接

[JSEP](http://www.noobyard.com/link?url=http://tools.ietf.org/html/draft-ietf-rtcweb-jsep-00)：
![JavaScript Session Establishment Protocol](https://ewr1.vultrobjects.com/imgur2/000/004/556/742_592_7b5.jpg)socket

1. 首先建立 RTCPeerConnection 对象，仅仅是初始化。
2. 使用 createOffer/createAnswer 来交换[SDP](http://www.noobyard.com/link?url=http://en.wikipedia.org/wiki/Session_Description_Protocol)，sdp中包含网络信息，RTCPeerConnection 对象得以创建链接。
3. 激活onicecandidate完成链接。

WEBRTC没有规定createOffer/createAnswer时使用的协议，所以signaling server 只要能够与浏览器交换SDP便可。能够用socket.io/wensocket等通讯技术把createOffer/createAnswer中的SDP给送到对方手里就行了。

------

下面我将用一个简单的例子来讲明链接是如何创建的。
为了更好地说明信号服务器的做用，我把它直接给拿掉了。取而代之的是一块公告牌。
在`sendMessage`和`receiveMsg`中，将要发送的信息写在页面的msg下方。没错，人工复制便可。

1. 首先打开2个页面，一个主动方点击call，另外一个被动方点击recv
2. 将caller的消息复制到receiver的answer按钮边上的文本框内，再点击answer。
3. 将receiver的消息复制到caller的answer按钮边上的文本框内，再点击answer。
4. 点击send将send左边的文本发送到对方send右侧的文本框内。

[demo code，人工信号服务器](http://www.noobyard.com/link?url=https://github.com/erow/webrtc_demo.git)

## 5.概述

1. 建立对象 。

2. 绑定回调函数。

   ```
   peerConn = new RTCPeerConnection(pcConfig);
   peerConn.onicecandidate = handleIceCandidate;
   
   dataChannel = peerConn.createDataChannel('1');
   channel.onopen = function() {
   console.log('CHANNEL opened!!!');
     };
   
     channel.onmessage = function(event){
     remoteText.innerText = event.data;
   };
   ```

3. 提供服务：createOffer。

   ```
   这期间要发送offer,candidate消息。
   `peerConn.createOffer(onLocalSessionCreated, logError);`
   在`onLocalSessionCreated`中调用`sendMessage`。
   随后会触发`handleIceCandidate`调用`sendMessage`。
   ```

4. 建立应答： createAnswer。

   ```
   peerConn.setRemoteDescription(new RTCSessionDescription(message), function() {},
                                 logError);
   peerConn.createAnswer(onLocalSessionCreated, logError);
   ```

   ```
   注意，这一步是在receiver端进行的。
   跟createOffer相似，createAnswer会发送一个answer消息，随后发送candidate消息。
   ```

5. 添加candidate

   ```
   peerConn.addIceCandidate(new RTCIceCandidate({
     candidate: message.candidate
   }));
   ```

6. 链接创建

原文作者：[菜鸟学院](http://www.noobyard.com/)

原文链接：http://www.noobyard.com/article/p-bvixwrsr-mn.html

# 【NO.387】想学习音视频开发，感觉网上的资料很少？

最近有读者留言，说“想转行音视频开发，怎么做”，正巧，前几天我还在本乎上，看到有人在问音视频的学习资料，还是个大一的学生。

想说一句：真有眼光。

![img](https://pic2.zhimg.com/80/v2-9827214bec98cc58532de80f564ee385_720w.webp)

如今这个时代，想赚钱，一个共识是，**得先选对赛道**。有些行业和领域，终其一生的天花板也不过如此。但有的朝阳行业，你一进去就可以获得大量的机会，就是“ROI（投入产出比）”很高。那就聊聊为啥从事音视频技术，未来会很赚钱。要说音视频技术，在这两年迎来爆发期。首先 5G 的发展提供了硬件条件，又受疫情的影响，**生活场景线上化**，大量的线上办公、线上教育、线上娱乐等需求，让几亿人涌入各类线上互动平台。比如：

- 抖音和快手的**短视频**，需要应用图像处理和视频编码技术，如何在保持高画质的情况下，尽量减少视频文件的大小；
- **连麦直播**需要 RTC 和直播技术，如何能够保证在各种网络状况下实现超低延时、降低卡顿率；
- **视频会议**需要 RTC 和转码合流服务等技术，几十上百人的大型视频会议，如何保证流畅度、卡顿率、画质等指标等；
- 随着线上体验的增加，人们对互动中的**音频体验**要求也在提升。除了听得到、听得清，还得好听、音质还原度高等。例如，Facebook 改名 Meta 进军元宇宙，TWS 耳机支持了空间音频渲染和主动降噪等等。

可以说，**音视频技术就如同空气和水**，无处不在，未来充满无限可能。而且这些真实场景都强调**实时互动**，延迟必须控制在毫秒级别内，如果在这个过程中，出现延时高、卡顿、画面模糊、杂音大等情况，你可以想象会出现什么样的体验吗？别说李佳琪双 11 直播带货 100 亿了，正常打一把沟通流畅的王者荣耀都不一定。其实，早在疫情初期，很多 SaaS 平台甚至大厂都出现过卡顿问题，主要对突如其来的流量没有做好充分准备，而如今大家都看到了线上的市场，自然对音视频技术人才的需求就多了。所以不夸张地说，音视频开发是一片**蓝海，人少，钱多**。

而且未来，会更加炙手可热。

![img](https://pic4.zhimg.com/80/v2-9a4ad28ad35e62dd411fbef69e5bcb37_720w.webp)

这张图供参考。但也能看出来，整体薪资待遇比普通开发者要优厚很多。不仅如此，从网上随手一搜，就能看到某某安卓转音视频，真香了、突破就业危机等等。

![img](https://pic2.zhimg.com/80/v2-26e1119763afd22a9229e72fc65e7295_720w.webp)

如今除了大厂，很多小公司也在寻找音视频的人才，稍微好点的音视频人才可能同时 3～4 家公司抢着要。就是因为从业人才基数低，高端人才缺乏，最重要的是，音视频技术有开发门槛，不好培养，也很难自学。**但这也意味着，你跟别人相比有技术上的核心竞争优势，有分水岭。**

快速入门音视频技术的方法，有吗？

音视频技术学起来并不容易，要懂的东西太多：音视频的采集、编码、传输、解码、渲染...等等，网上也少见体系化的资料。但就像左耳朵耗子所说，**“要去知识的源头学习”**。对学习者来说，找到优质的**信息源**可以让你事半功倍，不是二手加工的，也不会有信息损失或有误。

想学好并从事音视频开发大致需要6个阶段，分别是

1.音视频基础→2.ffmpeg实战→3.流媒体客户端→4.SRS流媒体服务器→5.webRTC实战→6.AndroidNDK



![img](https://pic1.zhimg.com/80/v2-3b5f3ce305df88ff629bd1251b8cb984_720w.webp)



![img](https://pic4.zhimg.com/80/v2-557eb3e37a4d46b82ce74930c8c2051b_720w.webp)

![img](https://pic3.zhimg.com/80/v2-968d41bf822cbf85bfaedd1bef665442_720w.webp)

![img](https://pic1.zhimg.com/80/v2-7070fef089d53ad822a9006db2ca9b54_720w.webp)

![img](https://pic3.zhimg.com/80/v2-860c658e52a28f8141c6d00926fd4522_720w.webp)

![img](https://pic4.zhimg.com/80/v2-3b3be57c41ca17643050d1106dd7f92f_720w.webp)

![img](https://pic2.zhimg.com/80/v2-57d781872161a36f4debc88c6c0c7075_720w.webp)

![img](https://pic3.zhimg.com/80/v2-5711cc848f8322bf53cc5a05612fa43e_720w.webp)

![img](https://pic4.zhimg.com/80/v2-3a7f80e1f93c100b3754a61dc9343a8f_720w.webp)

![img](https://pic2.zhimg.com/80/v2-8503b9be650ade46b7432e280e8b845d_720w.webp)

![img](https://pic1.zhimg.com/80/v2-dfdc0c221289af4c5bb8c7a8b489f154_720w.webp)

![img](https://pic1.zhimg.com/80/v2-728dee14839e77e0f2c26c6dc5fdba8c_720w.webp)

![img](https://pic4.zhimg.com/80/v2-63707c58dc8d3b903c8b9f651c5f9bc7_720w.webp)

![img](https://pic2.zhimg.com/80/v2-ed81762a6e6f6e4282b921d68db9264d_720w.webp)

现如今的音视频技术可以说无处不在。未来，也将作为一种基础技术应用到更广泛的的场景中，音视频技术人才也会成为新宠儿。虽然很难精通，但这个领域知识更新慢，**学的东西不容易淘汰**，积累的经验将会是撬动你更大未来的一个支点。所有的伟大都来源于一个勇敢的开始。

无论是现在从事音视频技术，还是后期转岗，都是很多人为数不多的机遇，能不能抓住就看个人了。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/574750538

# 【NO.388】WebRTC开源项目-手把手教你搭建AppRTC

## 1. 搭建AppRTC

### 搭建环境ubuntu 16.04server版本

\1. 服务器组成

AppRTC 房间+Web服务器 [https://github.com/webrtc/apprtc](https://link.zhihu.com/?target=https%3A//github.com/webrtc/apprtc)

Collider 信令服务器，在AppRTC源码里

CoTurn coturn打洞+中继服务器

Nginx 服务器，用于Web访问代理和Websocket代理。

AppRTC组成图如下所示。

![img](https://pic1.zhimg.com/80/v2-521d6239f25981729a2d576216010d48_720w.webp)

AppRTC 房间+Web服务器使用python+js语言

AppRTC Collider信令服务器采用go语言

Coturn 采用C语言

在部署到公网时需要通过Nginx做Web和Websocket的代理连接

实际开发：把信令+房间管理 都是写到一个服务器

AppRTC的的价值：

（1）js代码；apprtc/out/chrome_app/js/apprtc.debug.js

![img](https://pic2.zhimg.com/80/v2-0a5b223c8d709d3910d8997660929e41_720w.webp)

（2）Collider信令服务器原型。



## **2. 准备工作** 在一台全新的[ubuntu](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3Dubuntu) 16.04 server版本安装AppRTC，前期准备工作。

安装vim
安装ssh
安装ifconfig工具
更新源
安装git

###  **2.1 安装vim**

```text
sudo apt-get install vim
```

### **2.2 安装ssh**

```text
sudo apt-get install openssh-server
```

输入 “sudo ps -e | grep ssh” --> 回车 --> 有 sshd，说明 ssh 服务已经启动，如果没有启动，输入 “sudo service ssh start” --> 回车 --> ssh 服务就会启动。

### **2.3 安装ifconfig工具**

```text
sudo apt-get install net-tools
2udo apt-get install iputils-ping
```

### **2.4 更新源**

将源更新为阿里源，否则apt-get install安装软件较慢。

```text
# 1 在修改source.list前，最好先备份一份
sudo mv /etc/apt/sources.list /etc/apt/sources.list.old
# 2 执行命令打开sourcse.list文件
sudo vim /etc/apt/sources.list
# 3 复制更新源
```

复制以下源到sources.list

```text
# deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted
deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties
deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted
deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties
deb http://mirrors.aliyun.com/ubuntu/ xenial universe
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe
deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse
deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties
deb http://archive.canonical.com/ubuntu xenial partner
deb-src http://archive.canonical.com/ubuntu xenial partner
deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties
deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe
deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse
```

保存后更新源

```text
# 4 update命令
sudo apt-get update
```

### **2.5 安装git**

```text
sudo apt-get install git
```

## 3. 安装AppRTC必须的软件

### **3.0 创建目录**

```text
mkdir ~/webrtc
cd ~/webrtc
```

注意后续的目录，因为我的[webrtc](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3Dwebrtc)全路径为/home/lqf/webrtc，所以后续的目录都是采用该目录。

安装需要的各种工具(除了apt之外还可以下载安装包或者源码自己编译安装)：

### **3.1 安装JDK**

```text
# 先安装add-apt-repository命令支持
sudo apt-get install python-software-properties
sudo apt-get install software-properties-common
# 第一步：打开终端，添加ppa源
sudo add-apt-repository ppa:openjdk-r/ppa
# 第二步：更新源
sudo apt-get update
# 第三步：安装openjdk 8
sudo apt-get install openjdk-8-jdk# 第四步：配置openjdk 8为默认java环境
sudo update-alternatives --config java
sudo update-alternatives --config javac
# 最后，验证一下是否成功
java -version
# 返回
openjdk version "1.8.0_222"
OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~16.04.1-b10)
OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)
```

### **3.2 安装node.js**

```text
cd ~/webrtc
wget https://nodejs.org/dist/v10.16.0/node-v10.16.0-linux-x64.tar.xz
# 解压
tar -xvf node-v10.16.0-linux-x64.tar.xz
# 进入目录
cd node-v10.16.0-linux-x64/
# 查看当前的目录
pwd

# 确认一下nodejs下bin目录是否有node 和npm文件，如果有就可以执行软连接，比如
sudo ln -s /home/lqf/webrtc/node-v10.16.0-linux-x64/bin/npm /usr/local/bin/
sudo ln -s /home/lqf/webrtc/node-v10.16.0-linux-x64/bin/node /usr/local/bin/


# 看清楚，这个路径是你自己创建的路径，我的路径是/home/lqf/webrtc/node-v10.16.0-linux-x64


# 查看是否安装，安装正常则打印版本号
node -v 
npm -v 


# 有版本信息后安装 grunt-cli,先进到nodejs的bin目录, 要和自己的目录匹配
cd /home/lqf/webrtc/node-v10.16.0-linux-x64/bin
sudo npm -g install grunt-cli
sudo ln -s /home/lqf/webrtc/node-v10.16.0-linux-x64/bin/grunt /usr/local/bin/

grunt --version
# 显示grunt-cli v1.3.2

#使用淘宝源替换npm，后续要执行npm时执行cnpm
sudo npm install -g cnpm --registry=https://registry.npm.taobao.org
sudo ln -s /home/lqf/webrtc/node-v10.16.0-linux-x64/lib/node_modules/cnpm/bin/cnpm /usr/local/bin/
```

### **3.3 安装Python和Python-webtest （python2.7）**

ubuntu 一般都会自带python 2.7 输入 python -v 输出版本则已经有了

![img](https://pic2.zhimg.com/80/v2-5c5982ca203ffdcd712dcc3295528069_720w.webp)

如果没有则安装

```text
# 没有则输入下命令
sudo apt-get install python
# python 安装之后安装 Python-webtest
sudo apt-get install python-webtest


python -V
#Python 2.x
```

### **3.4 安装google_appengine**

```text
# 回到webrtc目录
cd ~/webrtc/
# 下载google_appengine
wget https://storage.googleapis.com/appengine-sdks/featured/google_appengine_1.9.40.zip
unzip google_appengine_1.9.40.zip

#配置环境变量：在/etc/profile文件最后增加一行，和自己路径保持一致
export PATH=$PATH:/home/lqf/webrtc/google_appengine
# 生效
source /etc/profile
```

### **3.5 安装go**

安装go

```text
sudo apt-get install golang-go
#检测go 版本 
go version
#go version go1.6.2 linux/amd64
```

创建go工作目录

```text
#创建go工作目录
mkdir -p ~/webrtc/goworkspace/src
#配置环境变量：在/etc/profile文件最后增加一行：
sudo vim /etc/profile
```

添加

```text
export GOPATH=/home/lqf/webrtc/goworkspace
# 然后执行
source /etc/profile
```

### **3.6 安装apprtc**

```text
cd ~/webrtc/
# 先尝试使用github的下载连接
git clone https://github.com/webrtc/apprtc.git
# 如果github的连接下不了就用码云的链接
git clone https://gitee.com/sabergithub/apprtc.git
#将collider的源码软连接到go的工作目录下
ln -s /home/lqf/webrtc/apprtc/src/collider/collider $GOPATH/src
ln -s /home/lqf/webrtc/apprtc/src/collider/collidermain $GOPATH/src
ln -s /home/lqf/webrtc/apprtc/src/collider/collidertest $GOPATH/src

#下一步在编译 go get collidermain: 被墙
#报错: package golang.org/x/net/websocket: unrecognized import path "golang.org/x/net/websocket"
#所以先执行:
mkdir -p $GOPATH/src/golang.org/x/
cd $GOPATH/src/golang.org/x/
git clone https://github.com/golang/net.git net
go install net

#编译collidermain
cd ~/webrtc/goworkspace/src
go get collidermain
go install collidermain
```

### **3.7 安装coturn**

```text
sudo apt-get install libssl-dev
sudo apt-get install libevent-dev

# 返回webrtc目录
cd ~/webrtc
#git clone https://github.com/coturn/coturn 
#cd coturn
#提供另一种安装方式turnserver是coturn的升级版本
wget http://coturn.net/turnserver/v4.5.0.7/turnserver-4.5.0.7.tar.gz
tar xfz turnserver-4.5.0.7.tar.gz
cd turnserver-4.5.0.7
 
./configure 
make 
sudo make install
```

### **3.8 安装Nginx**

注意安装的时候要带ssl

```text
sudo apt-get update
#安装依赖：gcc、g++依赖库
sudo apt-get install build-essential libtool
#安装 pcre依赖库（http://www.pcre.org/）
sudo apt-get install libpcre3 libpcre3-dev
#安装 zlib依赖库（http://www.zlib.net）
sudo apt-get install zlib1g-dev
#安装ssl依赖库
sudo apt-get install openssl


#下载nginx 1.15.8版本
wget http://nginx.org/download/nginx-1.15.8.tar.gz
tar xvzf nginx-1.15.8.tar.gz
cd nginx-1.15.8/


# 配置，一定要支持https
./configure --with-http_ssl_module 

# 编译
make

#安装
sudo make install `在这里插入代码片`
```

默认安装目录：/usr/local/nginx

启动：sudo /usr/local/nginx/sbin/nginx

停止：sudo /usr/local/nginx/sbin/nginx -s stop

重新加载配置文件：sudo /usr/local/nginx/sbin/nginx -s reload

## 4. 配置与运行

![img](https://pic4.zhimg.com/80/v2-379a57af5f41668bed47c4b81109bd93_720w.webp)

### 4.1 coturn 打洞+中继服务器

配置防火墙，允许访问3478端口（含tcp和udp，此端口用于nat穿透）

可以前台执行：sudo turnserver -L 0.0.0.0 -a -u lqf:123456 -v -f -r [http://nort.gov](https://link.zhihu.com/?target=http%3A//nort.gov) 然后退出，再到后台去执行。

```text
sudo nohup turnserver -L 0.0.0.0 -a -u lqf:123456 -v -f -r nort.gov &
#账号 lqf 密码:123456 这一步随便给，但是后面配置apprtc时需要用到
#命令后加 & ,执行起来后按 ctr+c,不会停止



#开启新窗口 执行
lsof -i:3478`
#输出大致这样的成功
COMMAND     PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
turnserve 30299 root   16u  IPv4  61868      0t0  UDP *:3478 
turnserve 30299 root   17u  IPv4  61869      0t0  UDP *:3478 
turnserve 30299 root   32u  IPv4  61879      0t0  TCP *:3478 (LISTEN)
turnserve 30299 root   33u  IPv4  61880      0t0  TCP *:3478 (LISTEN)
```

### **4.2 collider 信令服务器**

配置防火墙，允许访问8089端口（tcp，用于客户端和collider建立websocket信令通信）

```text
# 执行collider 信令服务器
sudo nohup $GOPATH/bin/collidermain -port=8089 -tls=false -room-server="http://192.168.2.143:8090" &
```

-room-server=“[http://192.168.2.143:8090](https://link.zhihu.com/?target=http%3A//192.168.2.143%3A8090)” 实际是连接房间服务器的地址

```text
#同样检查是否成功
sudo lsof -i:8089
#显示
COMMAND     PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
colliderm 30354 root    3u  IPv6  62696      0t0  TCP *:8089 (LISTEN)
```

### **4.3 apprtc 房间服务器**

先安装python的request模块

#### **4.3.1 安装pip** 下载setup-python工具

```text
cd /home/lqf/webrtc
wget https://pypi.python.org/packages/2.7/s/setuptools/setuptools-0.6c11-py2.7.egg  --no-check-certificate
chmod +x setuptools-0.6c11-py2.7.egg
sudo ./setuptools-0.6c11-py2.7.egg
wget https://pypi.python.org/packages/source/p/pip/pip-1.5.4.tar.gz
tar -xf pip-1.5.4.tar.gz
cd pip-1.5.4/
sudo python setup.py install
sudo pip install requests
```

#### 4.3.2 修改配置文件

配置防火墙，允许访问8090端口（tcp，此端口用于web访问）

配置文件修改（主要是配置apprtc对应的conturn和collider相关参数）

vim /home/lqf/webrtc/apprtc/src/app_engine/constants.py

修改后（填的都是外网IP，为了适合更多数朋友测试，我这里用的是内网的环境，在公网部署填入公网IP即可）

```text
# ICE_SERVER_OVERRIDE = None 注释掉



# Turn/Stun server override. This allows AppRTC to connect to turn servers
# directly rather than retrieving them from an ICE server provider.
# ICE_SERVER_OVERRIDE = None
# Enable by uncomment below and comment out above, then specify turn and stun
ICE_SERVER_OVERRIDE = [
  {
 "urls": [
 "turn:192.168.2.143:3478?transport=udp",
 "turn:192.168.2.143:3478?transport=tcp"
     ],
 "username": "lqf",
 "credential": "123456"
   },
   {
 "urls": [
 "stun:192.168.2.143:3478"
     ]
   }
 ]

ICE_SERVER_BASE_URL = 'https:192.168.2.143'
ICE_SERVER_URL_TEMPLATE = '%s/v1alpha/iceconfig?key=%s'
ICE_SERVER_API_KEY = os.environ.get('ICE_SERVER_API_KEY')

# Dictionary keys in the collider instance info constant.
WSS_INSTANCE_HOST_KEY = '192.168.2.143:8088'
WSS_INSTANCE_NAME_KEY = 'vm_name'
WSS_INSTANCE_ZONE_KEY = 'zone'
WSS_INSTANCES = [{
 WSS_INSTANCE_HOST_KEY: '192.168.2.143:8088',
 WSS_INSTANCE_NAME_KEY: 'wsserver-std',
 WSS_INSTANCE_ZONE_KEY: 'us-central1-a'
}]
```

进到apprtc目录

```text
#编译
cd /home/lqf/webrtc/apprtc
sudo cnpm install
sudo grunt build
```

启动:

sudo /home/lqf/webrtc/google_appengine/dev_appserver.py --host=0.0.0.0 --port=8090 /home/lqf/webrtc/apprtc/out/app_engine --skip_sdk_update_check

```text
# 默认端口是8080， 可以自己指定端口，我们这里指定为8090
sudo nohup  /home/lqf/webrtc/google_appengine/dev_appserver.py --host=0.0.0.0 --port=8090 /home/lqf/webrtc/apprtc/out/app_engine --skip_sdk_update_check &
#如果提示更新选择: n
```

此时可以通过谷歌浏览器访问测试：

[http://192.168.2.143:8090/](https://link.zhihu.com/?target=http%3A//192.168.2.143%3A8090/)

```text
#检查
sudo lsof -i:8090
#输出下列内容
```

### **4.4 nginx代理**

#### **4.4.1 生成证书**

```text
mkdir -p ~/cert
cd ~/cert
# CA私钥
openssl genrsa -out key.pem 2048
# 自签名证书
openssl req -new -x509 -key key.pem -out cert.pem -days 1095
```

#### 4.4.2 Web代理

反向代理apprtc，使之支持https访问，如果http直接访问apprtc，则客户端无法启动视频音频采集（必须得用https访问)

配置web服务器

（1）配置自己的证书

ssl_certificate /home/lqf/cert/cert.pem; // 注意证书所在的路径

ssl_certificate_key /home/lqf/cert/key.pem;

（2）配置主机域名或者主机IP

server_name 192.168.2.143;

完整配置文件：/usr/local/nginx/conf/conf.d/apprtc-https-proxy.conf

```text
upstream roomserver {
   server 192.168.2.143:8090;
}
server {
    listen 443 ssl;
    ssl_certificate /home/lqf/cert/cert.pem;
    ssl_certificate_key /home/lqf/cert/key.pem; 
    charset utf-8;
    # ip地址或者域名
    server_name 192.168.221.134;
    location / {
        # 转向代理的地址
        proxy_pass http://roomserver$request_uri;
        proxy_set_header Host $host;
    }
}
```

编辑nginx.conf文件，在末尾}之前添加包含文件

```text
  include /usr/local/nginx/conf/conf.d/*.conf;
}
```

#### **4.4.3 配置websocket代理**

ws 不安全的连接 类似http

wss是安全的连接，类似https

完整配置文件：/usr/local/nginx/conf/conf.d/apprtc-websocket-proxy.conf

```text
map $http_upgrade $connection_upgrade {
    default upgrade;
    '' close;
}
upstream websocket {
    server 192.168.2.143:8089;
}

server {
    listen 8088 ssl;
    ssl_certificate /home/lqf/cert/cert.pem;
    ssl_certificate_key /home/lqf/cert/key.pem;

    server_name 192.168.2.143;
    location /ws {
        proxy_pass http://websocket;
        proxy_http_version 1.1;
        proxy_connect_timeout 4s; #配置点1
        proxy_read_timeout 6000s; #配置点2，如果没效，可以考虑这个时间配置长一点
        proxy_send_timeout 6000s; #配置点3
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection $connection_upgrade;
    }
}
```

### 4.5 解决跨域问题

浏览器通话跨域问题 :pushState

Messages:Failed to start signaling: Failed to execute ‘pushState’ on ‘History’

解决方法

修改文件apprtc/src/web_app/js/appcontroller.js

```text
vim /home/lqf/webrtc/apprtc/src/web_app/js/appcontroller.js
#搜索  displaySharingInfo_ 大概是445行displaySharingInfo_函数第一行添加

roomLink=roomLink.replace("http","https");
```

最终结果（大概446行的修改）

```text
AppController.prototype.displaySharingInfo_ = function(roomId, roomLink) {
 roomLink=roomLink.replace("http","https");
 this.roomLinkHref_.href = roomLink;
 this.roomLinkHref_.text = roomLink;
 this.roomLink_ = roomLink;
 this.pushCallNavigation_(roomId, roomLink);
 this.activate_(this.sharingDiv_);
};
```

然后重新build

```text
cd ~/webrtc/apprtc
sudo grunt build 
```

在重新在前台运行

```text
sudo  /home/lqf/webrtc/google_appengine/dev_appserver.py --host=0.0.0.0 --port=8090 /home/lqf/webrtc/apprtc/out/app_engine --skip_sdk_update_check
```

## 5. 总结

（1）目录/home/lqf/webrtc

（2）IP： 192.168.2.143

（3）前后台执行的问题

（4）防火墙开发端口的问题

（5）端口规划的问题

![img](https://pic3.zhimg.com/80/v2-c0a38fbf05ae3f82f6a7e0786f73248e_720w.webp)

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/454470942

# 【NO.389】最全的MSVC编译参数，收藏备用，MinGW与MSVC编译的区别

## 1.**msvc的命令行编译链接命令**

### 1.1 **cl命令格式**


CL [option…] file… [option | file]… [lib…] [@command-file] [/link link-opt…]

选项→用途
option→参数可以使用/或者-，具体含义可以使用/HELP option看到解释。
file→一个或者多个源文件，.obj文件或者。lib文件，CL编译源文件传递.obj和.lib给linker
lib→一个或多个库文件，cl将传送给linker
command-file→一个保存多个选项的文件
link-opt→一个或多个链接操作，cl将传递给linkercl用到的环境变量

变量INCLUDE→指定vc的头文件位置，windows sdk的头文件位置。等等，中间用；分割。
LIB→指定vc的库，windows sdk的库路径。中间用;分割。

##  2.**优化参数**


选项→用途
/O1→目标尺寸最小
/O2→目标速度最快
/Ob→控制inline扩展，/Ob{0/1/2}
/Od→关闭优化
/Og→使用全局优化
/Oi→产生固定函数
/Os→偏向尺寸优化
/Ot→偏向速度优化
/Ox→最佳优化
/Oy→忽略帧指针（仅x86)
/favor→对特定架构优化。/favor:{blend/ATOM/AMD64/INTEL64}

##  3.**产生代码**


选项→用途
/arch→产生代码时使用SSE或者SSE2指令（仅x86）
/clr→产生运行在the common language runtime上的输出文件
/EH→指定异常处理模型/EH{s/a}[c][r][-]
/fp→指定浮点指针行为/fp:[precise/except[-]/fast/strict]
/GA→对windows应用进行优化
/Gd→使用_cdecl调用转换(仅x86)
/Ge→激活堆栈探测
/GF→打开字符串池
/Gh→调用钩子函数_penter
/GH→调用钩子函数_pexit
/GL→打开整个程序优化
/Gm→打开最小重建造
/GR→打开运行时类型信息(RTII)
/Gr→使用_fastcall调用转换（仅x86)
/GS→检查缓冲区安全
/Gs→控制堆栈探测
/GT→使用静态线程本地存储时保证数据分配安全
/guard:cf→加入控制流安全检查
/Gv→使用_vectorcall调用转换(仅x86)
/Gw→打开整个程序的全局数据优化
/GX→打开同步异常处理
/Gy→打开函数级链接
/GZ→打开快速检查，等同于/RTC1
/Gz→使用_stdcall调用转换(仅x86)
/homeparams→强制参数通过寄存器传递，仅x64编译
/hotpatch→创建一个补丁镜像
/Qfast_transcendentals→Generates fast transcendentals.
/QIfist→禁止从浮点转换为整数是调用函数_ftol(仅x86)
/Qimprecise_fwaits→在try块内部移除fwait命令
/Qpar→打开自动并行循环
/Gpar-report→打开一个并行循环的报告级
/Gsafe_fp_loads→对浮点值使用整数移动指令，禁止某些浮点指令的装入优化
/RTC)→打开运行时错误检查
/volatile)→解释执行选择怎样的volatile关键字

##  4.**输出文件**


选项→用途
/doc→处理注释文档到一个XML文件
/FA→配置汇编列表文件
/Fa→创建汇编列表文件
/Fd→删除程序数据库文件
/Fe→重命名执行文件
/Fi→指定预处理输出文件名
/Fm→创建mapfile
/Fo→创建object文件
/Fp→指定预编译头文件名
/FR /Fr→参数浏览文件


**预处理**


选项→用途
/AI→Specifies a directory to search to resolve file references passed to the #using directive.
/C→Preserves comments during preprocessing.
/D→定预处理宏
/E→复制预处理到标准输出
/EP→复制预处理到标准输出
/FI→预处理指定的include文件
/FU→Forces the use of a file name, as if it had been passed to the #using directive.
/Fx→合并注入代码和源代码
/I→指定include文件搜索路径
/P→写预处理到一个输出文件
/U→删除预定义宏
/u→和/U相同
/X→忽略标准include路径

##  5.**语言**


选项→用途
/openmp→打开#pragma omp在源代码中
/vd→禁止或者打开隐藏vtordisp类的成员
/vmb→Uses best base for pointers to members.
/vmg→Uses full generality for pointers to members.
/vmm→申明多继承
/vms→申明单继承
/vmv→申明虚拟继承
/Z7→产生和C 7.0兼容的调试信息
/Za→禁用语音扩展
/Zc→指定一个标准行为在/Ze下
/Ze→打开语音扩展
/Zg→产生函数原型
/ZI→在程序数据库中包括调试信息（仅x86)
/Zi→产生完整的调试信息
/Zl→从.obj文件中删除默认的库名
/Zpn→打包结构成员
/Zs→仅做语法检查
/ZW→产生一个输出文件能运行在windows运行环境

## 6.**链接**


选项→用途
/F→设置堆栈尺寸
/LD→创建动态链接库
/LDd→创建一个调试动态链接库
/link→传输指定的参数给link
/LN→创建一个MSIL模型
/MD→编译创建一个[多线程](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Ffrom%3Dpc_blog_highlight%26q%3D%E5%A4%9A%E7%BA%BF%E7%A8%8B) DLL,使用msvcrt.lib
/MDd→编译创建一个调试多线程 DLL，使用msvcrtd.lib
/MT→编译创建一个多线程执行程序，使用libcmt.lib
/MTd→编译场景一个调试多线程执行程序，使用libcmtd.lib

## 7.**预编译头**


选项→用途
/Y-→在当前建造中忽略其他全部预处理头编译选项
/Yc→创建一个预编译头文件
/Yd→在全部的object文件中放置完整的调试信息
/Yu→在编译期间使用预编译头文件

##  8.**杂项**


选项→用途
/?→列出编译选项
@→指定一个响应文件
/analyze→打开代码分析
/bigobj→Increases the number of addressable sections in an .obj file.
/c→编译但不链接
/cgthreads→给cl.exe指定一个线程数用来优化在建造过程中的性能
/errorReport→打开在vc++终端中提供内部编译错误信息(ICE)
/FC→显示传递给cl.exe的源代码的完整路径到一个文件中
/FS→强制写入一个程序数据库文件(PDB)
/H→现在扩展名的长度
/HELP→列出编译选项
/J→改变默认char类型
/kernel→编译器和链接器将创建一个可以在windows内核中执行的执行程序
/MP→同时建造多源代码文件
/nologo→禁止显示启动版权标志
/sdl→打开一些附加的安全功能和警告
/showIncludes→在编译期间显示全部include文件的列表
/Tc/TC→指定C源代码
/Tp/TP→指定C++源代码
/V→版本信息
/Wall→打开全部警告，包括默认关闭的警告
/W→警告级别
/w→关闭全部警告
/WL→打开在用命令行编译C++源代码时使用一行显示错误和警告信息
/Wp64→侦测可能的64-bit问题
/Yd→在对象文件中放置完整的调试信息
/Yl→当创建一个调试库时植入PCH引用
/Zm→指定一个预编译头分配限制

## 9.**MinGW与MSVC编译的区别**

本人使用的是QT5.6，当时我们选择下载的是第一个VS2015版本，也就是通过MSVC方式编译，我们来对比一下这两个编译器的区别：

1. MSVC是指微软的VC编译器
2. MinGW是指是Minimalist GNU on Windows的缩写。它是一个可自由使用和自由发布的Windows特定头文件和使用GNU工具集导入库的集合，允许你在GNU/Linux和Windows平台生成本地的Windows程序而不需要第三方C运行时库。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/434454915

# 【NO.390】针对初学者的 20 多个 FFmpeg 命令

在这个指南中，我将用示例来阐明如何使用 FFmpeg 媒体框架来做各种各样的音频、视频转码和转换的操作。我已经为初学者汇集了最常用的 20 多个 FFmpeg 命令，我将不时地添加更多的示例来保持更新这个指南。请给这个指南加书签，以后回来检查更新。

- 在 Linux 中安装 FFmpeg

## 0.针对初学者的 20 多个 FFmpeg 命令

FFmpeg 命令的典型语法是：

```text
ffmpeg [全局选项] {[输入文件选项] -i 输入_url_地址} ...
 {[输出文件选项] 输出_url_地址} ...
复制代码
```

现在我们将查看一些重要的和有用的 FFmpeg 命令。

## 1.获取音频/视频文件信息

为显示你的媒体文件细节，运行：

```text
$ ffmpeg -i video.mp4
复制代码
```

样本输出：

```text
ffmpeg version n4.1.3 Copyright (c) 2000-2019 the FFmpeg developers
built with gcc 8.2.1 (GCC) 20181127
configuration: --prefix=/usr --disable-debug --disable-static --disable-stripping --enable-fontconfig --enable-gmp --enable-gnutls --enable-gpl --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libdrm --enable-libfreetype --enable-libfribidi --enable-libgsm --enable-libiec61883 --enable-libjack --enable-libmodplug --enable-libmp3lame --enable-libopencore_amrnb --enable-libopencore_amrwb --enable-libopenjpeg --enable-libopus --enable-libpulse --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libv4l2 --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxcb --enable-libxml2 --enable-libxvid --enable-nvdec --enable-nvenc --enable-omx --enable-shared --enable-version3
libavutil 56. 22.100 / 56. 22.100
libavcodec 58. 35.100 / 58. 35.100
libavformat 58. 20.100 / 58. 20.100
libavdevice 58. 5.100 / 58. 5.100
libavfilter 7. 40.101 / 7. 40.101
libswscale 5. 3.100 / 5. 3.100
libswresample 3. 3.100 / 3. 3.100
libpostproc 55. 3.100 / 55. 3.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'video.mp4':
Metadata:
major_brand : isom
minor_version : 512
compatible_brands: isomiso2avc1mp41
encoder : Lavf58.20.100
Duration: 00:00:28.79, start: 0.000000, bitrate: 454 kb/s
Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, smpte170m/bt470bg/smpte170m), 1920x1080 [SAR 1:1 DAR 16:9], 318 kb/s, 30 fps, 30 tbr, 15360 tbn, 60 tbc (default)
Metadata:
handler_name : ISO Media file produced by Google Inc. Created on: 04/08/2019.
Stream #0:1(eng): Audio: aac (LC) (mp4a / 0x6134706D), 44100 Hz, stereo, fltp, 128 kb/s (default)
Metadata:
handler_name : ISO Media file produced by Google Inc. Created on: 04/08/2019.
At least one output file must be specified
复制代码
```

如你在上面的输出中看到的，FFmpeg 显示该媒体文件信息，以及 FFmpeg 细节，例如版本、配置细节、版权标记、构建参数和库选项等等。

如果你不想看 FFmpeg 标语和其它细节，而仅仅想看媒体文件信息，使用 -hide_banner 标志，像下面。

```text
$ ffmpeg -i video.mp4 -hide_banner
复制代码
```

样本输出：



![img](https://pic1.zhimg.com/80/v2-021a22f1d27e74fe9fbd3b5573701b08_720w.webp)

技术业务背景Technology business background

*使用 FFMpeg 查看音频、视频文件信息。*

看见了吗？现在，它仅显示媒体文件细节。

## 2.转换视频文件到不同的格式

FFmpeg 是强有力的音频和视频转换器，因此，它能在不同格式之间转换媒体文件。举个例子，要转换 mp4 文件到 avi 文件，运行：

```text
$ ffmpeg -i video.mp4 video.avi
复制代码
```

类似地，你可以转换媒体文件到你选择的任何格式。

例如，为转换 YouTube flv 格式视频为 mpeg 格式，运行：

```text
$ ffmpeg -i video.flv video.mpeg
复制代码
```

如果你想维持你的源视频文件的质量，使用 -qscale 0 参数：

```text
$ ffmpeg -i input.webm -qscale 0 output.mp4
复制代码
```

为检查 FFmpeg 的支持格式的列表，运行：

```text
$ ffmpeg -formats
复制代码
```

## 3.转换视频文件到音频文件

我转换一个视频文件到音频文件，只需具体指明输出格式，像 .mp3，或 .ogg，或其它任意音频格式。

上面的命令将转换 input.mp4 视频文件到 output.mp3 音频文件。

```text
$ ffmpeg -i input.mp4 -vn output.mp3
复制代码
```

此外，你也可以对输出文件使用各种各样的音频转换编码选项，像下面演示。

```text
$ ffmpeg -i input.mp4 -vn -ar 44100 -ac 2 -ab 320 -f mp3 output.mp3
复制代码
```

在这里，

- -vn – 表明我们已经在输出文件中禁用视频录制。
- -ar – 设置输出文件的音频频率。通常使用的值是22050 Hz、44100 Hz、48000 Hz。
- -ac – 设置音频通道的数目。
- -ab – 表明音频比特率。
- -f – 输出文件格式。在我们的实例中，它是 mp3 格式。

## 4.更改视频文件的分辨率

如果你想设置一个视频文件为指定的分辨率，你可以使用下面的命令：

```text
$ ffmpeg -i input.mp4 -filter:v scale=1280:720 -c:a copy output.mp4
复制代码
```

或，

```text
$ ffmpeg -i input.mp4 -s 1280x720 -c:a copy output.mp4
复制代码
```

上面的命令将设置所给定视频文件的分辨率到 1280×720。

类似地，为转换上面的文件到 640×480 大小，运行：

```text
$ ffmpeg -i input.mp4 -filter:v scale=640:480 -c:a copy output.mp4
复制代码
```

或者，

```text
$ ffmpeg -i input.mp4 -s 640x480 -c:a copy output.mp4
复制代码
```

这个技巧将帮助你缩放你的视频文件到较小的显示设备上，例如平板电脑和手机。

## 5.压缩视频文件

减小媒体文件的大小到较小来节省硬件的空间总是一个好主意.

下面的命令将压缩并减少输出文件的大小。

```text
$ ffmpeg -i input.mp4 -vf scale=1280:-1 -c:v libx264 -preset veryslow -crf 24 output.mp4
复制代码
```

请注意，如果你尝试减小视频文件的大小，你将损失视频质量。如果 24 太有侵略性，你可以降低 -crf 值到或更低值。

你也可以通过下面的选项来转换编码音频降低比特率，使其有立体声感，从而减小大小。

```text
-ac 2 -c:a aac -strict -2 -b:a 128k
复制代码
```

## 6.压缩音频文件

正像压缩视频文件一样，为节省一些磁盘空间，你也可以使用 -ab 标志压缩音频文件。

例如，你有一个 320 kbps 比特率的音频文件。你想通过更改比特率到任意较低的值来压缩它，像下面。

```text
$ ffmpeg -i input.mp3 -ab 128 output.mp3
复制代码
```

各种各样可用的音频比特率列表是：

1. 96kbps
2. 112kbps
3. 128kbps
4. 160kbps
5. 192kbps
6. 256kbps
7. 320kbps

## 7.从一个视频文件移除音频流

如果你不想要一个视频文件中的音频，使用 -an 标志。

```text
$ ffmpeg -i input.mp4 -an output.mp4
复制代码
```

在这里，-an 表示没有音频录制。

上面的命令会撤销所有音频相关的标志，因为我们不要来自 input.mp4 的音频。

## 8.从一个媒体文件移除视频流

类似地，如果你不想要视频流，你可以使用 -vn 标志从媒体文件中简单地移除它。-vn 代表没有视频录制。换句话说，这个命令转换所给定媒体文件为音频文件。

下面的命令将从所给定媒体文件中移除视频。

```text
$ ffmpeg -i input.mp4 -vn output.mp3
复制代码
```

你也可以使用 -ab 标志来指出输出文件的比特率，如下面的示例所示。

```text
$ ffmpeg -i input.mp4 -vn -ab 320 output.mp3
复制代码
```

## 9.从视频中提取图像

FFmpeg 的另一个有用的特色是我们可以从一个视频文件中轻松地提取图像。如果你想从一个视频文件中创建一个相册，这可能是非常有用的。

为从一个视频文件中提取图像，使用下面的命令：

```text
$ ffmpeg -i input.mp4 -r 1 -f image2 image-%2d.png
复制代码
```

在这里，

- -r – 设置帧速度。即，每秒提取帧到图像的数字。默认值是 25。
- -f – 表示输出格式，即，在我们的实例中是图像。
- image-%2d.png – 表明我们如何想命名提取的图像。在这个实例中，命名应该像这样image-01.png、image-02.png、image-03.png 等等开始。如果你使用 %3d，那么图像的命名像 image-001.png、image-002.png 等等开始。

## 10.裁剪视频

FFMpeg 允许以我们选择的任何范围裁剪一个给定的媒体文件。

裁剪一个视频文件的语法如下给定：

```text
ffmpeg -i input.mp4 -filter:v "crop=w:h:x:y" output.mp4
复制代码
```

在这里，

- input.mp4 – 源视频文件。
- -filter:v – 表示视频过滤器。
- crop – 表示裁剪过滤器。
- w – 我们想自源视频中裁剪的矩形的宽度。
- h – 矩形的高度。
- x – 我们想自源视频中裁剪的矩形的 x 坐标 。
- y – 矩形的 y 坐标。

比如说你想要一个来自视频的位置 (200,150)，且具有 640 像素宽度和 480 像素高度的视频，命令应该是：

```text
$ ffmpeg -i input.mp4 -filter:v "crop=640:480:200:150" output.mp4
复制代码
```

请注意，剪切视频将影响质量。除非必要，请勿剪切。

## 11.转换一个视频的具体的部分

有时，你可能想仅转换视频文件的一个具体的部分到不同的格式。以示例说明，下面的命令将转换所给定视频input.mp4 文件的开始 10 秒到视频 .avi 格式。

```text
$ ffmpeg -i input.mp4 -t 10 output.avi
复制代码
```

在这里，我们以秒具体说明时间。此外，以 hh.mm.ss 格式具体说明时间也是可以的。

## 12.设置视频的屏幕高宽比

你可以使用 -aspect 标志设置一个视频文件的屏幕高宽比，像下面。

```text
$ ffmpeg -i input.mp4 -aspect 16:9 output.mp4
复制代码
```

通常使用的高宽比是：

- 16:9
- 4:3
- 16:10
- 5:4
- 2:21:1
- 2:35:1
- 2:39:1

## 13.添加海报图像到音频文件

你可以添加海报图像到你的文件，以便图像将在播放音频文件时显示。这对托管在视频托管主机或共享网站中的音频文件是有用的。

```text
$ ffmpeg -loop 1 -i inputimage.jpg -i inputaudio.mp3 -c:v libx264 -c:a aac -strict experimental -b:a 192k -shortest output.mp4
复制代码
```

## 14.使用开始和停止时间剪下一段媒体文件

可以使用开始和停止时间来剪下一段视频为小段剪辑，我们可以使用下面的命令。

```text
$ ffmpeg -i input.mp4 -ss 00:00:50 -codec copy -t 50 output.mp4
复制代码
```

在这里，

- –s – 表示视频剪辑的开始时间。在我们的示例中，开始时间是第 50 秒。
- -t – 表示总的持续时间。

当你想使用开始和结束时间从一个音频或视频文件剪切一部分时，它是非常有用的。

类似地，我们可以像下面剪下音频。

```text
$ ffmpeg -i audio.mp3 -ss 00:01:54 -to 00:06:53 -c copy output.mp3
复制代码
```

## 15.切分视频文件为多个部分

一些网站将仅允许你上传具体指定大小的视频。在这样的情况下，你可以切分大的视频文件到多个较小的部分，像下面。

```text
$ ffmpeg -i input.mp4 -t 00:00:30 -c copy part1.mp4 -ss 00:00:30 -codec copy part2.mp4
复制代码
```

在这里，

- -t 00:00:30 表示从视频的开始到视频的第 30 秒创建一部分视频。
- -ss 00:00:30 为视频的下一部分显示开始时间戳。它意味着第 2 部分将从第 30 秒开始，并将持续到原始视频文件的结尾。

## 16.接合或合并多个视频部分到一个

FFmpeg 也可以接合多个视频部分，并创建一个单个视频文件。

创建包含你想接合文件的准确的路径的 join.txt。所有的文件都应该是相同的格式（相同的编码格式）。所有文件的路径应该逐个列出，像下面。

```text
file /home/sk/myvideos/part1.mp4
file /home/sk/myvideos/part2.mp4
file /home/sk/myvideos/part3.mp4
file /home/sk/myvideos/part4.mp4
复制代码
```

现在，接合所有文件，使用命令：

```text
$ ffmpeg -f concat -i join.txt -c copy output.mp4
复制代码
```

如果你得到一些像下面的错误；

```text
[concat @ 0x555fed174cc0] Unsafe file name '/path/to/mp4'
join.txt: Operation not permitted
复制代码
```

添加 -safe 0 :

```text
$ ffmpeg -f concat -safe 0 -i join.txt -c copy output.mp4
复制代码
```

上面的命令将接合 part1.mp4、part2.mp4、part3.mp4 和 part4.mp4 文件到一个称为 output.mp4 的单个文件中。

## 17.添加字幕到一个视频文件

我们可以使用 FFmpeg 来添加字幕到视频文件。为你的视频下载正确的字幕，并如下所示添加它到你的视频。

```text
$ fmpeg -i input.mp4 -i subtitle.srt -map 0 -map 1 -c copy -c:v libx264 -crf 23 -preset veryfast output.mp4
复制代码
```

## 18.预览或测试视频或音频文件

你可能希望通过预览来验证或测试输出的文件是否已经被恰当地转码编码。为完成预览，你可以从你的终端播放它，用命令：

```text
$ ffplay video.mp4
复制代码
```



![img](https://pic1.zhimg.com/80/v2-f4b6c93884bdef80fe91927440c8a6dc_720w.webp)

夏季野生茂密的森林，有美丽的大树Summer wild thick forest with

类似地，你可以测试音频文件，像下面所示。

```text
$ ffplay audio.mp3
复制代码
```



![img](https://pic4.zhimg.com/80/v2-1ab35c143235d9d3a28a229cd0248783_720w.webp)

带有数字数据信息的高科技用户界面抬头显示Hi-tech user interface head up

## 19.增加/减少视频播放速度

FFmpeg 允许你调整视频播放速度。

为增加视频播放速度，运行：

```text
$ ffmpeg -i input.mp4 -vf "setpts=0.5*PTS" output.mp4
复制代码
```

该命令将双倍视频的速度。

为降低你的视频速度，你需要使用一个大于 1 的倍数。为减少播放速度，运行：

```text
$ ffmpeg -i input.mp4 -vf "setpts=4.0*PTS" output.mp4
复制代码
```

## 20.创建动画的 GIF

出于各种目的，我们在几乎所有的社交和专业网络上使用 GIF 图像。使用 FFmpeg，我们可以简单地和快速地创建动画的视频文件。下面的指南阐释了如何在类 Unix 系统中使用 FFmpeg 和 ImageMagick 创建一个动画的 GIF 文件。

- 在 Linux 中如何创建动画的 GIF

## 21.从 PDF 文件中创建视频

我长年累月的收集了很多 PDF 文件，大多数是 Linux 教程，保存在我的平板电脑中。有时我懒得从平板电脑中阅读它们。因此，我决定从 PDF 文件中创建一个视频，在一个大屏幕设备（像一台电视机或一台电脑）中观看它们。如果你想知道如何从一批 PDF 文件中制作一个电影，下面的指南将帮助你。

- 在 Linux 中如何从 PDF 文件中创建一个视频

## 22.获取帮助

在这个指南中，我已经覆盖大多数常常使用的 FFmpeg 命令。它有很多不同的选项来做各种各样的高级功能。要学习更多用法，请参考手册页。

```text
$ man ffmpeg
复制代码
```

我希望这个指南将帮助你入门 FFmpeg。如果你发现这个指南有用，可以分享它。更多的好东西将要来。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/426415645

# 【NO.391】谷歌开源、高性能RPC框架：gRPC 使用体验

> 在广告系统实践中，精排服务基于 gRPC 协议调用 TF-Serving 在线推理服务。相信很多业务已经使用过 gRPC 相关语言的框架进行服务调用，尤其是基于谷歌云的出海业务的服务调用更绕不开 gRPC，所以很有必要理解 gRPC 的原理。本文通过简要介绍抓包分析一次 gRPC 的调用过程，逐步认识 gRPC。

### 1.**概述**

gRPC 是谷歌推出的一个开源、高性能的 RPC 框架。默认情况下使用 protoBuf 进行序列化和反序列化，并基于 HTTP/2 传输报文，带来诸如多请求复用一个 TCP 连接(所谓的多路复用)、双向流、流控、头部压缩等特性。gRPC 目前提供 C、Go 和 JAVA 等语言版本，对应 gRPC、gRPC-Go 和 gRPC-JAVA 等开发框架。

在 gRPC 中，开发者可以像调用本地方法一样，通过 gRPC 的客户端调用远程机器上 gRPC 服务的方法，gRPC 客户端封装了 HTTP/2 协议数据帧的打包、以及网络层的通信细节，把复杂留给框架自己，把便捷提供给用户。gRPC 基于这样的一个设计理念：定义一个服务，及其被远程调用的方法(方法名称、入参、出参)。在 gRPC 服务端实现这个方法的业务逻辑，并在 gRPC 服务端处理来着远程客户端对这个 RPC 方法的调用。在 gRPC 客户端也拥有这个 RPC 方法的存根(stub)。gRPC 的客户端和服务端都可以用任何支持 gRPC 的语言来实现，例如一个 gRPC 服务端可以是 C++语言编写的，以供 Ruby 语言的 gRPC 客户端和 JAVA 语言的 gRPC 客户端调用，如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtOhjEJJB8LfMW2KvDRgvTdfYVhTf2HFaB2FemtsfsymPZdLNGt0wj7g/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

gRPC 默认使用 ProtoBuf 对请求/响应进行序列化和反序列化，这使得传输的请求体和响应体比 JSON 等序列化方式包体更小、更轻量。

gRPC 基于 HTTP/2 协议传输报文，HTTP/2 具有多路复用、头部压缩等特性，基于 HTTP/2 的帧设计，实现了多个请求复用一个 TCP 连接，基本解决了 HTTP/1.1 的队头阻塞问题，相对 HTTP/1.1 带来了巨大的性能提升。下面对 HTTP/2 进行简介。



### 2.**HTTP/2 简介**

HTTP 是一个成功的应用层协议。但是由于 HTTP 的队头阻塞等特性导致基于 HTTP 的应用程序性能有较大影响。队头阻塞是指顺序请求的一个请求必须处理完才能处理后续的其他请求，当一个请求被阻塞时会给应用程序带来延迟。虽然 HTTP/1.1 提供了流水线(request pipeline)的请求操作，但是由于受到 HTTP 自身协议的限制，无法消除 HTTP 的队头阻塞带来的延迟。为了减少延迟，需要 HTTP 的客户端与服务器建立多个连接实现并发处理请求，降低延迟。然而，在高并发情况下，大量的网络连接可能耗尽系统资源，可以使用连接池模式只维持固定的连接数可以防止服务的资源耗尽。连接池连接数的设置在对性能要求极高的应用程序也是一个挑战，需要根据实际机器配置的压测情况确定。

另外，HTTP 头字段重复且冗长，导致网络传输不必要的冗余报文，以及初始 TCP 拥塞窗口很快被填满。一个 TCP 连接处理大量请求是会导致较大的延迟。

HTTP/2 通过优化 HTTP 的报文定义，允许同一个网络连接上并发交错的处理请求和响应，并通过减少 HTTP 头字段的重复传输、压缩 HTTP 头，提高了处理性能。

HTTP 每次网络传输会携带通信的资源、浏览器属性等大量冗余头信息，为了减少这些重复传输的开销，HTTP/2 会压缩这些头部字段：

1. 基于 HTTP/2 协议的客户端和服务器使用"头部表"来跟踪与存储发送的键值对，对于相同的键值对数据，不用每次请求和响应都发送；
2. 头部表在 HTTP/2 的连接有效期内一直存在，由客户端和服务器共同维护更新；
3. 每个新的 HTTP 头键值对要么追加，要么替换头部表原来的值。

举个例子，有两个请求，在 HTTP/1.x 中，请求 1 和请求 2 都要发送全部的头数据；在 HTTP/2 中，请求 1 发送全部的头数据，请求 2 仅仅发送变更的头数据，这样就可以减少冗余的数据，降低网络开销。如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtAs9kyiblXlAUlKATmEKVpA5Rd4rB6MxrojfD2mGcuba2nibpyWKIyk2g/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

这里再举个例子说明 HTTP/1.x 和 HTTP/2 处理请求的差异，浏览器打开网络要请求/index.html、styles.css 和/scripts.js 三个文件，基于 HTTP/1.x 建立的连接只能先请求/index.html,得到响应后再请求 styles.css，得到响应后再获取/scripts.js。而基于 HTTP/2 一个网络连接在获取/index.html 后,可以同时获取 styles.css 和/scripts.js 文件，如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZt4mia8JYicHXib9X5467J2nvR51Em6scUKeXVJPHOc6Ufriau1DrOibfDoAQ/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

HTTP/2 对服务资源和网络更友好，相对与 HTTP/1.x，处理同样量级的请求，HTTP/2 的需要建立的 TCP 连接数更少。这主要得益于 HTTP/2 使用二进制数据帧来传输数据，使得一个 TCP 连接可以同时处理多个请求而不用等待一个请求处理完成再处理下一个。从而充分发掘了 TCP 的并发能力。

#### 2.1 HTTP/2 帧

在 HTTP/2 中，帧是网络通信的基本单位，HTTP/2 主要定义了 10 种不同的帧类型，每种帧类型在建立和管理连接或者单个 stream 流有不同的作用。不同的帧类型都有公共字段：Length(3 字节),Type(1 字节), Flags(1 字节), Stream Identifier(4 字节) 和 Frame Payload(变长)。

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtmEGvRG5XO9PyYkvGK7vaOADM8FsMP1sXiczpx5k7dYdCzPxW8F7PkLA/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

HTTP/2 帧都以固定的 9 字节大小作为帧头，后面跟着变长的包体 Paylload。如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtrLiajgfZaCq5M6Mric5aGHjOs5TWiaGyBibgkCHd9vTA9zOGmE6YxWSREQ/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

帧头字段说明：

1. **Length** 帧的数据(Frame Payload)长度，*不包括帧头长度*，3 个字节(24bit), 帧最大长度为 1<<24 - 1(16383);
2. **Type** 帧类型，1 个字节(8bit), 目前 HTTP/2 定义了 10 中帧类型，常见的帧类型有 DATA 帧、HEADERs 帧、SETTINGS 帧等，10 种帧类型如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtQAbUOlOB3uogsQbZy8fepYc24qpHebQfiaFQowbazS9kz6q1ic8YYMuA/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

1. **Flags** 帧标志，1 个字节(8bit)，没有特定帧类型的帧标志应该被忽略，在发送时帧标志需要保持未设置(0x0).常见的标志位有 END_HEADERS 表示 HTTP/2 数据头结束，相当于 HTTP 头后的空行（“\r\n”），END_STREAM 表示单方向数据发送结束（即 EOS，End of Stream），相当于 HTTP/1.x 里 Chunked 分块结束标志（“0\r\n\r\n”）；
2. **R** 保留字段 1bit,发送时保持未设置(0x0),接收时忽略；
3. **Stream Identifier** 流标识符，31bit. 一个无符号整数。由客户端发起的 Stream 数据流用奇数编号 ID 的流标识符；由服务器发起的数据流使用偶数编号 ID 的流标识符。流标识符零(0x0)用于连接控制消息；零流标识符不能用于建立新的 stream 流。

![图片](data:image/svg+xml,<%3Fxml version='1.0' encoding='UTF-8'%3F><svg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'><title></title><g stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'><g transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'><rect x='249' y='126' width='1' height='1'></rect></g></g></svg>)

#### 2.2 HTTP/2 请求模型

HTTP/2 的请求模型如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtr1VGARFDS481wj1eZEDpzqzkiacA1WAMOmnhjCstlgPWshChGJRj5ZA/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

**Connection 连接**：对应一个 TCP 连接，可以承载一个或者多个 Stream。

**Stream 流**：对应一个双向通信的数据流，可以承载一个或者多个 Message。每个数据流都有一个唯一的流标识符和可选的优先级信息，用于承载双向消息。

Stream 流有几个重要特性：

1. 单个 HTTP/2 连接可以承载多个并发的 stream 流，通信双方都可能交叉地收到多个 stream 流的数据帧；
2. stream 流可以单方面建立与使用，也可以由客户端和服务器双方共享消息通道;
3. 客户端或者服务器都可以关闭 stream 流;
4. 发送方在 stream 流按顺序发送数据帧，接收到按照顺序接收数据帧。特别地，HEADS 帧和 DATA 帧的顺序在语言上是较为重要的；
5. stream 流由无符号整数标识。stream 流标识符是由发起流的端点分配给 stream 流的。

**Message 消息**：对应 HTTP/1.x 的请求 Request 或响应 response.包含一个或者多个 Frame 数据帧。

**Frame 数据帧：**HTTP/2 网络通信的基本单位，承载的是压缩和编码后的二进制流，不同 Stream 数据流的帧可以交错发送，并根据帧头的流 ID(数据流标识符)进行区分和组装。

关于 HTTP/2 主要介绍这些，更多参考：https://github.com/halfrost/Halfrost-Field/blob/master/contents/Protocol/HTTP:2-HTTP-Frames-Definitions.md



### 3.**gRPC 协议**

前面对 HTTP/2 帧作了简要说明，这节开始介绍 gRPC 协议，gRPC 基于 HTTP/2/协议进行通信，使用 ProtoBuf 组织二进制数据流，gRPC 的协议格式如下图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtXibqTP5VmmxZ4bqToibJ5HeNPl4OtC4Ez4DuQg8Im0AeBb904tqnBaPg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

从以上图可知，gRPC 协议在 HTTP 协议的基础上，对 HTTP/2 的帧的有效包体(Frame Payload)做了进一步编码：gRPC 包头(5 字节)+gRPC 变长包头，其中：

1. 5 字节的 gRPC 包头由：1 字节的压缩标志(compress flag)和 4 字节的 gRPC 包头长度组成；
2. gRPC 包体长度是变长的，其是这样的一串二进制流：使用指定序列化方式(通常是 ProtoBuf)序列化成字节流，再使用指定的压缩算法对序列化的字节流压缩而成的。如果对序列化字节流进行了压缩，gRPC 包头的压缩标志为 1。
3. 对比 tRPC 协议可知，gRPC 的帧头和包头比 tRPC 协议帧头和包头要小，当然 HTTP/2 的帧类型更复杂一些。tRPC 协议帧定义如下图：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtHpWBEicBCf7rBEg3L7VHcTIEUL4Yicia3MQoCOibkZpjMWO0g6hfUjoA4Q/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

### 4.**gRPC 调用抓包分析**

下面基于官方提供的 gRPC-Go helloword 例子，使用 Wireshark 分析通过 tcpdump 抓包 gRPC 调用的报文，加深对 gRPC 协议的理解。

#### 4.1 抓包准备

1. 下载 Wireshark 抓包工具，下载地址：https://www.wireshark.org/；
2. 安装 Go 环境；
3. 安装 protoc-gen-go: go get -u github.com/golang/protobuf/protoc-gen-go；
4. 下载 g[rpc-go/examples/helloworld](https://github.com/grpc/grpc-go/tree/master/examples/helloworld) gRPC-Go 的 helloword Go 工程。

#### 4.2 抓包

1. 运行 helloword 的服务端 greeter_server：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtK2XFnQKgOUjtF2EicsVctCLYv2MavEG0Xtt5qbssJic4zfC1oB55JNsw/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

1. 使用 tcpdump 命令准备抓一次 helloword 的调用：

sudo tcpdump -iany port 50051 -w grpc.cap

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtMTnxYc54tf2ZY0KFNiaib0jrWibBSqjRcE3HxlcMib3DicGHCHPiaSXfTPGg/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

1. 运行 helloword 的客户端 greeter_client：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZt7oFMzSL0cMIb0KlPb585dkSyHib1KAL8eD9COXpZG2gPWSmCnuCq0qw/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

完成一次调用，tcpdump 抓到一次调用的报文，保存为 grpc.cap。

#### 4.3 Wireshark 配置

打开 Wireshark 主面板，选择 ProtoBuf 文件路径：Wireshark-->Preferences-->Protocols-->Protobuf-->Protobuf Search Paths。

选择 helloworld 的 proto 文件地址。

![图片](data:image/svg+xml,<%3Fxml version='1.0' encoding='UTF-8'%3F><svg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'><title></title><g stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'><g transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'><rect x='249' y='126' width='1' height='1'></rect></g></g></svg>)

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZthER7zqYmn85PHsftxbnvgTW97pTRrqnKCtnLtbyicjaOwiaE2P9wo47A/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

Wireshark 打开 grpc.cap 文件，选中 greeter_client 发送端口号和 greeter_server 发送端口号的报文记录，右键 Decode As...为 HTTP/2:

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtm3ibW4MR5ibqmgG2jJkHUjOic5e3greqAf6LMmdAAJHxLtNxuv64CEyIQ/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

Wireshark 过滤框输入 HTTP2 就可得到一次完整的 gRPC 调用细节：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtaWzhEhsD9wYyzfibm2tibLz7t2PMicy9QpfWwicb6Pdkpias3mjMde6tJmA/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

#### 4.4 gRPC 调用分析

从以上抓包得到的 gRPC 调用图可知，gRPC客户端(port:62880)一次调用服务端(port:50051)的RPC方法通常会包括多次HTTP/2帧的发送，本文分析中抓包的一个帧序列例子：Magic-->SETTINGS(双向四个)-->HEADERS-->DATA(GRPC-PROTOBUF)-->WINDOW_UPDATE,PING-->PING-->HEADERS,DATA,HEADERS-->WINDOW_UPDATE,PING-->PING。

下面对调用过程中的每个帧做简要分析。

**1）客户端发送 Magic 帧****Magic 帧的为固定内容：PRI \* HTTP/2.0\r\n\r\nSM\r\n\r\n。如下图所示：客户端发送 Magic 帧后双方就会使用 HTTP/2 相关协议进行通信。**

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtiarQOiciaUOam0Dj8icdiaB5qJz93NnGUbTCM7rtRoAtAlibAxvK2rUicP8pw/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

**2）客户端和服务端发 SETTINGS 帧**

接着 Magic 帧后，接下来就是发送 SETTINGS 帧，[SETTINGS 帧](https://github.com/halfrost/Halfrost-Field/blob/master/contents/Protocol/HTTP:2-HTTP-Frames-Definitions.md#五-settings-帧)主要用于传递影响两端网络通信的配置参数，以及确认收到这些参数。

客户端和服务器首先发两个 SETTINGS 帧传递配置参数信息，接着服务端发了一个确认的 SETTINGS 帧后，客户端也发出了一个确认的 SETTINGS 帧：

a.客户端发第一个 SETTINGS， 帧类型 = 0x4，帧标志为 0x00, 流标识符为 0：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtNvlsJuxmbv7k05KTXbEOR9fdrP7ruYck0sntBib0oMvYykOQF0VibH6g/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

b.服务端向客户端回了一个 SETTINGS 帧，帧类型 = 0x4，帧标志为 0x00, 流标识符为 0，同时告诉客户端，服务端愿意接收的最大帧大小为 16384 bytes。同时我们看到，SETTINGS 帧的参数类型为 SETTINGS_MAX_FRAME_SIZE(0x5)，参数类型表示服务端愿意接受的包体大小，初始值 为 16364 个字节。此外，SETTINGS 帧长度为 6：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZt3wcOqibX2BViciaaYHZL3WNs0VoOjcxE5unwCqhUw5e3m5sc9kv54zKfw/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

c.随后服务端再发出一个确认的 SETTINGS 帧，帧类型 = 0x4，帧标志为 0x01, 流标识符为 0：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZt3lhYbjBTdxGT2JPt52IMKcX1fwibgQnmdWgnPBaqEPqDAMoGiazMr9Ew/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

d.客户端收到服务端的确认 SETTINGS 帧后，也发出一个 SETTINGS 帧进行确认，帧类型 = 0x4，帧标志为 0x01, 流标识符为 0，双方进行确认后下面就可以开始传输头帧(HEADERS)和数据帧(DATA)了：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZt2SJRnicRsK7ZUBFRs8LvV8syWxopozwexicLSIRkUaxJIxhAIQ8yRaxQ/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

**3）客户端发送 HEADERS 帧**

客户端和服务器双方发送 SETTINGS 帧进行双方参数确认后，下一步客户端向服务端发送一个[HEADERS 帧](https://github.com/halfrost/Halfrost-Field/blob/master/contents/Protocol/HTTP:2-HTTP-Frames-Definitions.md#二-headers-帧)， 当前 HEADERS 帧长度为 92，帧类型 = 0x1，帧标志为 0x04(End Headers，0=End Stream:False,1=End Headers:True,0=Padded:False,0=Priority:False)，流标识符为 1，HEADERS 帧还额外带有 Head Block Fragment 头块片段(header 列表是零个或多个字段的集合。当通过网络连接传输时，使用 HTTP 头压缩[COMPRESSION] 将 header 列表序列化为 header block 块。然后将序列化的 header block 块分成字节流，称为 header 块片段)；

同时还可以看到一些 HTTP 请求头(8 个)信息，比如:method:POST，:scheme:http，:path:/helloworld.Greeter/SayHello 等等，如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZthicyO0F2vpkCbZ99NDxOf8SlrUibKQKB5586kJuw86C3ZUbTGsMvTZeg/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

**4）客户端发送 DATA 帧**

HEADERS 帧发送完之后，接下来客户端给服务器发送[DATA 帧](https://github.com/halfrost/Halfrost-Field/blob/master/contents/Protocol/HTTP:2-HTTP-Frames-Definitions.md#一-data-帧)，当前数据帧的长度为 18 字节(不包含 HTTP/2 帧头)，帧标识为 0x01:End Stream，流标识符为 1，然后是 HTTP/2 的有效包体数据信息(18 字节)，也就是经过 protobuf 序列化的字节流的 gRPC 数据；当前的 gRPC 数据由 gRPC 包头(5 字节)+gRPC 包体(13 字节)组成，gRPC 包头的压缩标志为 Not Compressed(未压缩)，gRPC 包头长度为 13 字节，gRPC 的包体内容为"who are you", 对应的 protobuf 中 message 的 Name 字段承载的信息=WireType<本身占 1 个字节>枚举值为 2[string,编码 0a]+value 长度<本身占 1 个字节>[string 需要显式的告知 value 长度]11 个字节(编码 0b)+字段 value 信息"who are you"<11 个字节>，如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtf78BouaBDfP0hY4XWh8CvKMBghPqvBwANWlZv23mdD0TflbiaQ8WfnQ/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

**5）服务端发送 WINDOW_UPDATE 帧和 PING 帧**

客户端发完 DATA 帧后，服务器先回复了两个帧，分别是 WINDOW_UPDATE 帧和 PING 帧，

[WINDOW_UPDATE 帧](https://github.com/halfrost/Halfrost-Field/blob/master/contents/Protocol/HTTP:2-HTTP-Frames-Definitions.md#九-window_update-帧) 主要用于流量控制。当前的 WINDOW_UPDATE 帧的长度为 4，帧类型为 WINDOW_UPDATE(8)，帧标志为 0x00，流标志符为 0，Window Size Increment（流量窗口增量）为 18(收到客户端发送的 DATA 帧长度 18)。

[PING 帧](https://github.com/halfrost/Halfrost-Field/blob/master/contents/Protocol/HTTP:2-HTTP-Frames-Definitions.md#七-ping-帧) 用于测量最小往返时间(RTT)以及确定连接是否存活。当前 PING 帧的长度为 8，帧类型为 PING(6)，帧标志为 0x00(ACK=False)，流标志符为 0。

此次 WINDOW_UPDATE 帧和 PING 帧的发送情况如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZthP94Z6Rd16eg4I6aaq2hFdCQvIicOcxyRMabV9pB5geyv23BjO9l5eA/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

**6）客户端回复 PING 帧**

客户端收到服务器的 PING 帧后，会回一个 PING 帧确认(ACK=True)以及回复 Pong 信息，当前 PING 帧的长度为 8，帧类型为 PING(6)，帧标志为 0x01(ACK=True)，流标志符为 0，Pong 信息为一串 16 位的 UUID 串，如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtG0OkIuaDOpfS7PVe2PwCoYpGdEJiakxyV5edc4YRMXwOJPaX7A1O86g/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

**7）服务端回复 HEADERS 帧+DATA 帧(gRPC)+HEADERS 帧(终止流)**

服务端收到客户端的 PING 帧确认客户端存活状态后，

a. 首先是一个 HEADERS 帧，该帧的帧长度为 14，帧类型 Type 为 HEADS(1)，帧标志 Flags 为 End Headers(0x04)，流标志符为 1，

HEAD 长度为 54，head 数量为 2，分别为 status: 200 OK、content-type:application/grpc;

b. 然后是一个 DATA 帧，该帧的帧长度为 20，帧类型 Type 为 DATA(0)，帧标志 Flags 为 0x00，流标志符为 1，HTTP/2 的有效包体数据信息，也即是 gRPC 数据信息为 15 个字节(5 字节的 gRPC 包头+15 字节的 gRPC 包体内容(”I am datumhu“))；

c. 最后是一个终止流的 HEADERS 帧，该 HEADERS 帧的帧长度为 24，帧类型 Type 为 HEADS(1)，帧标志 Flags 为 End Headers,End Stream(0x05)，流标志符为 1，HEAD 长度为 40 字节，head 数量为 2，分别为 grpc-status: 0、grpc-message:;

如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZtjADPhj3aleJ53H3ZRictkKoyu7e0hR4yU91oTZ0cq4vfrlictPGkJHzQ/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

**8）客户端回复 WINDOW_UPDATE 帧和 PING 帧**

客户端收到服务端的 DATA 响应后，给服务器发送一个 WINDOW_UPDATE 帧和 PING 帧，其中 WINDOW_UPDATE 的窗口大小增量为 20(收到服务端响应的 DATA 帧长度)，如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZt81JrQS3fg1WzQjpLEv4oVcldxOXmX5hdUG5S11ufmnN0jULPFNmsFw/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

**9）服务端回 PING 帧**

最后服务器收到客户端的 PING 帧后，回复一个 PING 帧确认(ACK=1)，如下图所示：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZt6T4t3ZwVbSBrfAz0OMZqia80EqweQF8yu83EJnmOtPw6IBbOZKaepMQ/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

以上一次 gRPC 调用的数据流图概括为如下：

![图片](https://mmbiz.qpic.cn/mmbiz/j3gficicyOvavKkbzEXh8bBOTrlaYZEoZt3QcFWCzMdJJ7mQproOhEibFW1E3UbztzISs6xKJ3k3eX0tiauLQGU3nA/640?wx_fmt=other&wxfrom=5&wx_lazy=1&wx_co=1)

### 5.**总结**

本文首先概述了 gRPC 的原理，由于 gRPC 是基于 HTTP/2 协议进行网络传输，随后简介了 HTTP/2 通过多路复用和头部压缩等优化措施，基本解决了 HTTP/1.x 包头阻塞的问题，相对 HTTP/1.1 带来了性能提升。HTTP/2 多路复用和头部压缩的关键在于 HTTP/2 通过帧的设计优化了 HTTP 协议语义。所以接着介绍了 HTTP/2 的帧结构和 gRPC 的协议。最后通过抓包一次完整的 gRPC 调用，分析了 GRPC-HTTP2 的数据流过程，希望能够加深对 gRPC 的理解。

原文作者：datumhu，腾讯 IEG 后开开发工程师

原文链接：https://mp.weixin.qq.com/s/6XXJfbnIaKzSFtXyDDB72g

# 【NO.392】C++音视频开发的技术要点

每日专注分享音视频技术点，ffmpeg，webRTC技术，流媒体服务器，SRS，sfu模型，H264码率，等等技术栈文章视频。

------

总体来讲，音视频开发是有一定的技术门槛的，我觉得至少需要在这个领域踏踏实实积累个3-5年，才能对音视频相关的开发知识有一个整体、深刻的理解。

从技术上来讲，需要从如下两个大类知识点上去积累：

## 1. C/C++通用开发知识

音视频开发的主要编程语言就是C和C++。

这块的专业知识积累是通用的，并不局限于某个特定的行业，属于程序员的技术功底。

可以重点关注如下几个方面：

计算机系统的底层工作原理

操作系统原理

程序的编译、链接和加载机制

C/C++语言特性背后蕴含的思想，底层工作原理，适用场景，存在什么样的问题

软件设计原则和设计模式

数据结构和算法

多线程并发编程原理

网络编程

跨平台

操作系统API

软件调试

## 2. 音视频领域专业知识

这块属于从事音视频行业的专业知识。

这块的专业知识是非常多的，每个功能模块背后涉及很多专业的知识。

音视频的开发可以分为两大块，涉及的内容大致如下：

音视频客户端开发

客户端应用开发

音视频引擎开发

音视频引擎SDK

音视频引擎框架

音视频引擎功能模块

音/视频采集

音/视频渲染

音/视频数据处理

音/视频编/解码

录制

串流

音视频同步

流媒体服务器开发

通用服务器开发知识，需要关注如下几个点

高稳定性

高性能

高并发

高可用

流媒体服务器开发

SFU vs MCU

流媒体协议转换

弱网下的音视频传输协议

录制 & 转码

…

上述内容中，客户端应用开发、音视频引擎SDK、音视频引擎框架、通用服务器开发等主要涉及C/C++通用开发知识，但要设计好这些部分必须对音视频相关的知识和产品业务有比较深刻的理解才能做到。

通常，音视频架构师比较关注这些部分。

而音视频引擎底层功能模块和SFU/MCU流媒体服务器的开发，则和音视频的专业知识密切相关。

音视频的采集模块

视频数据可以通过如下方式获得：

USB摄像头

专业的硬件视频采集卡（有软压卡和硬压卡之分）

网络摄像机（支持RTSP协议）

操作系统提供的屏幕录制API

读取音视频文件并解码

订阅流媒体服务器上的流

音频数据可以通过如下方式获得：

声卡

扬声器播放声音的回环采集（依赖操作系统的API）

读取音视频文件并解码

订阅流媒体服务器上的流

支持音频输入的网络摄像机（支持RTSP协议）

支持音频输入的视频采集卡

在手机上，操作系统的SDK会提供相关的音视频采集接口

音/视频渲染

视频渲染一般需要了解OpenGL，而音频渲染需要了解OpenAL

可以通过开源库SDL来快速实现渲染模块

在Windows下使用DirectShow框架，操作系统提供了对应的视频和音频渲染模块（通过GraphEdit可以看到）

在DirectShow中渲染器会涉及到音视频同步的策略，当然，也完全可以自己去实现音视频同步模块

音/视频数据处理

这些模块基本是在编码前或解码后，对视频或音频的原始数据进行某种算法上的处理

视频处理主要包括分辨率转换、色彩空间转换、帧率转换、图像增强、多路视频拼接、添加字幕、添加LOGO图片等，这块对整体的性能影响比较大，往往需要使用SIMD指令进行汇编优化或使用GPU算法进行加速

音频处理主要包括回声消除、噪声抑制、自动增益、混音等，这块往往会涉及比较多的信号处理和数学知识，是音频中比较复杂的一块

音/视频编/解码

视频编/解码

要理解视频的基本编码原理，熟悉视频编码的关键参数和码流格式

目前使用比较多的是H.264，H.265开始逐步在使用，其他的视频编码也有很多，如AVS、VP8、VP9等

视频编码对音视频引擎的性能影响比较大，这块基本都是需要使用GPU加速的，目前的Intel集显对H.264和H.265支持还是比较好的，NVIDIA的独立显卡在编码上存在路数的限制；手机上一般都有对应的硬件加速模块；在性能较好的硬件上，可以考虑开源的X264

音频编/解码

要理解音频的基本编码原理，熟悉音频的关键参数和码流格式

目前使用比较多的是AAC，其他的音频编码也有很多，如G7.11、G.722、OPUS等

在PC上，一般音频的相关模块对性能的影响不明显，但在海思嵌入式系统上，音频模块对性能的影响就不能忽略，因为海思基本没有提供音频的硬件加速模块，而ARM CPU性能也有点弱

录制

需要理解FLV、MP4、TS等容器格式

对于特殊的录制方式要注意软件的处理方式，例如，加片头和片尾的录制功能，追加录制

MP4录制要注意moov box放在文件开始或结束对录制文件的写入和点播的影响

录制时音视频均匀混合的策略

串流

理解视频互动、直播和点播的工作原理

关键评价指标

延迟

首屏时间

同步

流畅性

画质/音质

理解下述的几种音视频传输协议

RTMP

HTTP + FLV / Websocket + FLV

HLS

RTP & RTCP

RTSP

SIP

WebRTC

H.323

弱网下的音视频传输协议

理解TCP协议栈原理

可靠的UDP传输协议

KCP

SRT

QUIC

FEC + 丢包重传机制（如NACK）

音视频的开发并不是完全从零开始，而是有许多可以依赖的开源库，但要用好这些库，需要对上述的音视频专业知识有深刻的理解。

**比较常见的音视频开源库，如下：**

ffmpeg

可以直接使用ffmpeg的命令行实现转码、切片等常见功能

可以基于FFmpeg API封装开发自己的音视频模块

live555

比较完善的RTSP库

x264

比较常用的H.264编码库

fdkaac

比较常用的AAC编解码库

librtmp

支持rtmp协议，产品化时需要自己进一步完善

pjsip

支持sip协议

webrtc

google开源的webrtc库，有比较好的音/视频引擎，对网络状态的实时评估可以借鉴，回声消除模块也是比较有名的

SDL

比较有名的音视频渲染库

SRS

国内比较知名的RTMP流媒体服务器，支持HLS、HTTP+FLV，4.0版本开始支持WebRTC

OWT

Intel开源的WebRTC套件，支持了WebRTC客户端SDK和分布式的WebRTC MCU服务器

OpenCV

著名的视频算法库

另外，视频的编码和解码可以基于Intel Media SDK和NVIDIA的NVENC来实现。

在海思嵌入式上，海思芯片（如Hi3531D等）提供了硬件的音视频采集、音视频渲染、视频编/解码、视频图像处理等核心功能，这就需要借助于海思提供的SDK进行开发了。

-完-



![img](https://pic4.zhimg.com/80/v2-05fd788d5924a2ea37fb9efc77802a8b_720w.webp)

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/454809166

# 【NO.393】FFmpeg使用小结

## 1.视频播放器的原理

**封装格式**

作用：视频码流和音频码流按照一定的格式储存在一个文件汇总

**视频编码数据**

作用：将视频像素数据（RGB，YUV等）压缩成为视频码流，从而降低视频的数据量

**音频编码数据**

作用：将音频采样数据（PCM等）压缩成为音频码流，从而降低音频的数据量

**视频像素数据**

作用：保存了屏幕上每一个像素点的像素值

格式：常见的像素数据格式有RGB24, RGB32, YUV420P,YUV422P,YUV444P等。压缩编码中一般使用的是YUV格式的像素数据,最为常见的格式为YUV420P。

特点：视频像素数据体积很大，一般情况下一小时高清视频的RGB24格式的数据体积为:3600*25*1920*1080*3=559.9GB（PS：这里假定帧率为25HZ，取样精度8bit）

**音频采样数据**

作用：保存了音频中每个采样点的值。

特点：音频采样数据体积很大,一般情况下一首4分钟的PCM格式的歌曲体积为:4*60*44100*2*2=42.3MByte
PS:这里假定采样率为44100Hz,采样精度为16bit

1 术语：

什么是影片？其实就是一组（很多张）图片，时间间隔很小的连续展示出来，人们就觉得画面中的人物在动，这就是影片。那电影的实质就是N多张图片的集合。那 每张图片和帧又有什么关系呢？事实上，如果一部影片里面的图片，我们原封不动的全部存起来，空间会很大很大很大，但是如果通过一定的算法（这里不讲相关算 法），把每一张图片压缩（编码_encode）一下，变成 帧。再把帧连起来变成流，再把不同的流放到某个容器里面，这就是我们平常看见的电影文件了，文件 碟中谍4.H264.ACC.mkv，他为什么要这样命名呢？ mkv表达了它的容器是.mkv的，且包含至少两个流，h264的视频流，ACC的音频流。这是一种典型的 牺牲时间来换取空间的做法。

容器(Container)——容器就是一种文件格式，比如flv，mkv等。包含下面5种流以及文件头信息。

流(Stream)——是一种视频数据信息的传输方式，5种流：音频，视频，字幕，附件，数据。

帧(Frame)——帧代表一幅静止的图像，分为I帧，P帧，B帧。

编解码器(Codec)——是对视频进行压缩或者解压缩，CODEC =COde （编码） +DECode（解码）

复用/解复用(mux/demux)——把不同的流按照某种容器的规则放入容器，这种行为叫做复用（mux）

把不同的流从某种容器中解析出来，这种行为叫做解复用(demux)

视频压缩中，每帧代表一幅静止的图像。而在实际压缩时，会采取各种算法减少数据的容量，其中IPB就是最常见的。

（1）I帧表示关键帧，你可以理解为这一帧画面的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画面）

（2）P帧表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧的画面差别的数据）

（3）B帧是双向差别帧，也就是B帧记录的是本帧与前后帧的差别（具体比较复杂，有4种情况），换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码时CPU会比较累。

从上面的解释看，我们知道I和P的解码算法比较简单，资源占用也比较少，I只要自己完成就行了，P呢，也只需要解码器把前一个画面缓存一下，遇到P时就使用之前缓存的画面就好了，如果视频流只有I和P，解码器可以不管后面的数据，边读边解码，线性前进，大家很舒服。但网络上的电影很多都采用了B帧，因为B帧记录的是前后帧的差别，比P帧能节约更多的空间，但这样一来，文件小了，解码器就麻烦了，因为在解码时，不仅要 用之前缓存的画面，还要知道下一个I或者P的画面（也就是说要预读预解码），而且，B帧不能简单地丢掉，因为B帧其实也包含了画面信息，如果简单丢掉，并 用之前的画面简单重复，就会造成画面卡（其实就是丢帧了），并且由于网络上的电影为了节约空间，往往使用相当多的B帧，B帧用的多，对不支持B帧的播放器 就造成更大的困扰，画面也就越卡。 一般平均来说，I的压缩率是7（跟JPG差不多），P是20，B可以达到50，可见使用B帧能节省大量空间，节省出来的空间可以用来保存多一些I帧，这样在相同码率下，可以提供更好的画质。

附: [编解码过程](https://link.zhihu.com/?target=http%3A//blog.csdn.net/abcjennifer/article/details/6577934)

![img](https://pic4.zhimg.com/80/v2-d5de0a11ab01fa5ed2f3b99b41f73beb_720w.webp)

\1. 注册所有容器格式和CODEC:av_register_all()

\2. 打开文件:av_open_input_file()

\3. 从文件中提取流信息:av_find_stream_info()

\4. 穷举所有的流，查找其中种类为CODEC_TYPE_VIDEO

\5. 查找对应的解码器:avcodec_find_decoder()

\6. 打开编解码器:avcodec_open()

\7. 为解码帧分配内存:avcodec_alloc_frame()

\8. 不停地从码流中提取出帧数据:av_read_frame()

\9. 判断帧的类型，对于视频帧调用:avcodec_decode_video()

\10. 解码完后，释放解码器:avcodec_close()

\11. 关闭输入文件:av_close_input_file()

------

## 2. 简介（[http://derekzhan.iteye.com/blog/1989274](https://link.zhihu.com/?target=http%3A//derekzhan.iteye.com/blog/1989274)）

**资料**

FFmpeg官网： [http://www.ffmpeg.org](https://link.zhihu.com/?target=http%3A//www.ffmpeg.org)

FFmpeg doc : [http://www.ffmpeg.org/documentation.html](https://link.zhihu.com/?target=http%3A//www.ffmpeg.org/documentation.html)

FFmpeg wiki : [https://trac.ffmpeg.org/wiki](https://link.zhihu.com/?target=https%3A//trac.ffmpeg.org/wiki)

FFmpeg基础: [http://wenku.baidu.com/view/296eefcaf90f76c661371af1.html](https://link.zhihu.com/?target=http%3A//wenku.baidu.com/view/296eefcaf90f76c661371af1.html)

FFmpeg的名称来自MPEG视频编码标准，前面的“FF”代表“Fast Forward”，FFmpeg是一套可以用来记录、转换数字音频、视频，并能将其转化为流的开源计算机程序。可以轻易地实现多种视频格式之间的相互转换。FFmpeg的用户有Google，Facebook，Youtube，优酷，爱奇艺，土豆等。

**组成**

1、libavformat：用于各种音视频封装格式的生成和解析，包括获取解码所需信息以生成解码上下文结构和读取音视频帧等功能，包含demuxers和muxer库；

2、libavcodec：用于各种类型声音/图像编解码；

3、libavutil：包含一些公共的工具函数；

4、libswscale：用于视频场景比例缩放、色彩映射转换；

5、libpostproc：用于后期效果处理；

6、ffmpeg：是一个命令行工具，用来对视频文件转换格式，也支持对电视卡实时编码；

7、ffsever：是一个HTTP多媒体实时广播流服务器，支持时光平移；

8、ffplay：是一个简单的播放器，使用ffmpeg 库解析和解码，通过SDL显示；

**2.1 过滤器(Filter)**

在多媒体处理中，filter的意思是被编码到输出文件之前用来修改输入文件内容的一个软件工具。如：视频翻转，旋转，缩放等。

语法：[input_link_label1][input_link_label2]… filter_name=parameters [output_link_label1][output_link_label2]…

过滤器图link label ：是标记过滤器的输入或输出的名称

（1）.视频过滤器 -vf

如testsrc视频按顺时针方向旋转90度　　ffplay -f lavfi -i testsrc -vf transpose=1

如testsrc视频水平翻转(左右翻转)　　ffplay -f lavfi -i testsrc -vf hflip

（2）.音频过滤器 -af

实现慢速播放，声音速度是原始速度的50%　　ffplay p629100.mp3 -af atempo=0.5

（3）如何实现顺时针旋转90度并水平翻转？

过滤器链（Filterchain）

基本语法 Filterchain = 逗号分隔的一组filter

语法：“filter1,filter2,filter3,…filterN-2,filterN-1,filterN”

顺时针旋转90度并水平翻转 ffplay -f lavfi -i testsrc -vf transpose=1,hflip

（4）如何实现水平翻转视频和源视频进行比较？

方法一: 过滤器链（Filterchain）

第一步： 源视频宽度扩大两倍　　ffmpeg -i jidu.mp4 -t 10 -vf pad=2*iw output.mp4

第二步：源视频水平翻转　　ffmpeg -i jidu.mp4 -t 10 -vf hflip output2.mp4

第三步：水平翻转视频覆盖output.mp4　　ffmpeg -i output.mp4 -i output2.mp4 -filter_complex overlay=w compare.mp4

方法二：过滤器图（Filtergraph）

基本语法 :Filtergraph = 分号分隔的一组filterchain

“filterchain1;filterchain2;…filterchainN-1;filterchainN”

用ffplay直接观看结果：fplay -f lavfi -i testsrc -vf split[a][b];[a]pad=2*iw[1];[b]hflip[2];[1][2]overlay=w

F1: split过滤器创建两个输入文件的拷贝并标记为[a],[b]

F2: [a]作为pad过滤器的输入，pad过滤器产生2倍宽度并输出到[1].

F3: [b]作为hflip过滤器的输入，vflip过滤器水平翻转视频并输出到[2].

F4: 用overlay过滤器把 [2]覆盖到[1]的旁边.

**2.2 选择媒体流**

一些多媒体容器比如AVI，mkv，mp4等，可以包含不同种类的多个流，如何从容器中抽取各种流呢？

语法：-map file_number:stream_type[:stream_number]

这有一些特别流符号的说明：

1、-map 0 选择第一个文件的所有流

2、-map i:v 从文件序号i(index)中获取所有视频流， -map i:a 获取所有音频流，-map i:s 获取所有字幕流等等。

3、特殊参数-an,-vn,-sn分别排除所有的音频，视频，字幕流。注意:文件序号和流序号从0开始计数。

**2.3 查看帮助**

可用的bit流 ：ffmpeg –bsfs

可用的编解码器：ffmpeg –codecs

可用的解码器：ffmpeg –decoders

可用的编码器：ffmpeg –encoders

可用的过滤器：ffmpeg –filters

可用的视频格式：ffmpeg –formats

可用的声道布局：ffmpeg –layouts

可用的license：ffmpeg –L

可用的像素格式：ffmpeg –pix_fmts

可用的协议：ffmpeg -protocals

**2.4 码率、帧率和文件大小**

码率和帧率是视频文件的最重要的基本特征，对于他们的特有设置会决定视频质量。如果我们知道码率和时长那么可以很容易计算出输出文件的大小。

帧率：帧率也叫帧频率，帧率是视频文件中每一秒的帧数，肉眼想看到连续移动图像至少需要15帧。 fps

码率：比特率(也叫码率，数据率)是一个确定整体视频/音频质量的参数，秒为单位处理的字节数，码率和视频质量成正比，在视频文件中中比特率用bps来表达

**帧率**

1、用 -r 参数设置帧率 （fps 每秒传输帧数(Frames Per Second)）

ffmpeg –i input –r fps output

2、用fps filter设置帧率

ffmpeg -i clip.mpg -vf fps=fps=25 clip.webm

例如设置帧率为29.97fps，下面三种方式具有相同的结果：

ffmpeg -i input.avi -r 29.97 output.mpg

ffmpeg -i input.avi -r 30000/1001 output.mpg

ffmpeg -i input.avi -r netsc output.mpg



**码率、文件大小**

设置码率 –b 参数　　ffmpeg -i film.avi -b 1.5M film.mp4

音频：-b:a 视频： - b:v　　设置视频码率为1500kbps　　ffmpeg -i input.avi -b:v 1500k output.mp4

**控制输出文件大小**

-fs (file size首字母缩写) 　　ffmpeg -i input.avi -fs 1024K output.mp4

计算输出文件大小　　(视频码率+音频码率) * 时长 /8 = 文件大小K

------

## 3. 调整视频分辨率

**3.1 调整视频分辨率**

用-s参数设置视频分辨率，参数值wxh，w宽度单位是像素，h高度单位是像素　　　　ffmpeg -i input_file -s 320x240 output_file

**3.2 预定义的视频尺寸**

下面两条命令有相同效果

ffmpeg -i input.avi -s 640x480 output.avi

ffmpeg -i input.avi -s vga output.avi

Scale filter调整分辨率 ，Scale filter的优点是可以使用一些额外的参数 ， 语法： Scale=width:height[:interl={1|-1}]

下面两条命令有相同效果

ffmpeg -i input.mpg -s 320x240 output.mp4

ffmpeg -i input.mpg -vf scale=320:240 output.mp4

对输入视频成比例缩放

改变为源视频一半大小　 　ffmpeg -i input.mpg -vf scale=iw/2:ih/2 output.mp4

改变为原视频的90%大小 ffmpeg -i input.mpg -vf scale=iw*0.9:ih*0.9 output.mp4

在未知视频的分辨率时，保证调整的分辨率与源视频有相同的横纵比。

例如宽度固定400，高度成比例：

ffmpeg -i input.avi -vf scale=400:400/a

ffmpeg -i input.avi -vf scale=400:-1

相反地，高度固定300，宽度成比例：

ffmpeg -i input.avi -vf scale=-1:300

ffmpeg -i input.avi -vf scale=300*a:300

------

## 4. 裁剪/填充视频

**4.1 裁剪视频crop filter**

从输入文件中选取你想要的矩形区域到输出文件中,常见用来去视频黑边。 语法：crop:ow[:oh[:x[:y:[:keep_aspect]]]]

裁剪输入视频的左三分之一，中间三分之一，右三分之一:

ffmpeg -i input -vf crop=iw/3:ih :0:0 output

ffmpeg -i input -vf crop=iw/3:ih :iw/3:0 output

ffmpeg -i input -vf crop=iw/3:ih :iw/3*2:0 output

裁剪帧的中心, 当我们想裁剪区域在帧的中间时，裁剪filter可以跳过输入x和y值，他们的默认值是

Xdefault = ( input width - output width)/2 , Ydefault = ( input height - output height)/2

ffmpeg -i input_file -v crop=w:h output_file

裁剪中间一半区域： ffmpeg -i input.avi -vf crop=iw/2:ih/2 output.avi

比较裁剪后的视频和源视频比较

ffplay -i jidu.mp4 -vf split[a][b];[a]drawbox=x=(iw-300)/2:(ih-300)/2:w=300:h=300:c=yellow[A];[A]pad=2*iw[C];[b]crop=300:300:(iw-300)/2:(ih-300)/2[B];　　

[C][B]overlay=w*2.4:40

自动检测裁剪区域 , cropdetect filter 自动检测黑边区域

ffplay jidu.mp4 -vf cropdetect

然后用检测到的值来裁剪视频

ffplay jidu.mp4 –vf crop=672:272:0:54

填充视频(pad) , 在视频帧上增加一快额外额区域，经常用在播放的时候显示不同的横纵比　　语法：pad=width[:height:[:x[:y:[:color]]]]

创建一个30个像素的粉色宽度来包围一个SVGA尺寸的图片： ffmpeg -i photo.jpg -vf pad=860:660:30:30:pink framed_photo.jpg

同理可以制作testsrc视频用30个像素粉色包围视频:　　ffplay -f lavfi -i testsrc -vf pad=iw+60:ih+60:30:30:pink

4:3到16:9---------- 一些设备只能播放16:9的横纵比，4:3的横纵比必须在水平方向的两边填充成16:9，高度被保持，宽度等于高度乘以16/9，x（输入文件水平位移）值由表达式(output_width - input_width)/2来计算。

4：3到16:9的通用命令是：ffmpeg -i input -vf pad=ih*16/9:ih :(ow-iw)/2:0:color output

举例： ffplay -f lavfi -i testsrc -vf pad=ih*16/9:ih:(ow-iw)/2:0:pink

16:9到4:3----------为了用4:3的横纵比来显示16:9的横纵比，填充输入文件的垂直两边，宽度保持不变，高度是宽度的3/4，y值（输入文件的垂直偏移量）是由一个表达式（output_height-input_height）/2计算出来的。

16:9到4:3的通用命令：ffmpeg -i input -vf pad=iw :iw*3/4:0:(oh-ih)/2:color output

举例：ffplay -f lavfi -i testsrc=size=320x180 -vf pad=iw:iw*3/4:0:(oh-ih)/2:pink

------

## 5. 翻转和旋转

**翻转**

水平翻转语法： -vf hflip：ffplay -f lavfi -i testsrc -vf hflip

垂直翻转语法：-vf vflip：ffplay -f lavfi -i testsrc -vf vflip

**旋转**

语法：transpose={0,1,2,3}

　　0:逆时针旋转90°然后垂直翻转

　　1:顺时针旋转90°

　　2:逆时针旋转90°

　　3:顺时针旋转90°然后水平翻转

------

## 6. 模糊，锐化

模糊, 语法：boxblur=luma_r:luma_p[:chroma_r:chram_p[:alpha_r:alpha_p]]　, fplay -f lavfi -i testsrc -vf boxblur=1:10:4:10

注意：luma_r和alpha_r半径取值范围是0~min(w,h)/2, chroma_r半径的取值范围是0~min(cw/ch)/2

锐化, 语法：-vf unsharp=l_msize_x:l_msize_y:l_amount:c_msize_x:c_msize_y:c_amount, 所有的参数是可选的，默认值是5:5:1.0:5:5:0.0

l_msize_x:水平亮度矩阵，取值范围3-13，默认值为5

l_msize_y:垂直亮度矩阵，取值范围3-13，默认值为5

l_amount:亮度强度，取值范围-2.0-5.0，负数为模糊效果，默认值1.0

c_msize_x:水平色彩矩阵，取值范围3-13，默认值5

c_msize_y:垂直色彩矩阵，取值范围3-13，默认值5

c_amount:色彩强度，取值范围-2.0-5.0，负数为模糊效果，默认值0.0

**举例**

使用默认值，亮度矩阵为5x5和亮度值为1.0　　ffmpeg -i input -vf unsharp output.mp4

高斯模糊效果(比较强的模糊)：　　ffplay -f lavfi -i testsrc -vf unsharp=13:13:-2

------

## **7. 覆盖（画中画）**

覆盖, 语法：overlay[=x[:y], 所有的参数都是可选，默认值都是0

**举例**

Logo在左上角:ffmpeg -i pair.mp4 -i logo.png -filter_complex overlay pair1.mp4

右上角：ffmpeg -i pair.mp4 -i logo.png -filter_complex overlay=W-w pair2.mp4

左下角：ffmpeg -i pair.mp4 -i logo.png -filter_complex overlay=0:H-h pair2.mp4

右下角： ffmpeg -i pair.mp4 -i logo.png -filter_complex overlay=W-w:H-h pair2.mp4

删除logo,语法：-vf delogo=x:y:w:h[:t[:show]]

x:y 离左上角的坐标

w:h logo的宽和高

t: 矩形边缘的厚度默认值4

show：若设置为1有一个绿色的矩形，默认值0.

ffplay -i jidu.mp4 -vf delogo=50:51:60:60:100:0

------

## 8. 添加文本

语法：drawtext=fontfile=font_f:text=text1[:p3=v3[:p4=v4[…]]]

**常用的参数值**

x：离左上角的横坐标，y: 离左上角的纵坐标，fontcolor：字体颜色，fontsize：字体大小，text:文本内容

textfile:文本文件，t：时间戳，单位秒，n:帧数开始位置为0，draw/enable:控制文件显示，若值为0不显示，1显示，可以使用函数

**简单用法**

1、在左上角添加Welcome文字

ffplay -f lavfi -i color=c=white -vf drawtext=fontfile=arial.ttf:text=Welcom

2、在中央添加Good day

ffplay -f lavfi -i color=c=white -vf drawtext="fontfile=arial.ttf:text='Goodday':x=(w-tw)/2:y=(h-th)/2"

3、设置字体颜色和大小

ffplay -f lavfi -i color=c=white -vf drawtext="fontfile=arial.ttf:text='Happy Holidays':x=(w-tw)/2:y=(h-th)/2:fontcolor=green:fontsize=30"

**动态文本**

用 t (时间秒)变量实现动态文本

1、顶部水平滚动

ffplay -i jidu.mp4 -vf drawtext="fontfile=arial.ttf:text='Dynamic RTL text':x=w-t*50:fontcolor=darkorange:fontsize=30"

2、底部水平滚动

ffplay -i jidu.mp4 -vf drawtext="fontfile=arial.ttf:textfile=textfile.txt:x=w-t*50:y=h-th:fontcolor=darkorange:fontsize=30"

3、垂直从下往上滚动

ffplay jidu.mp4 -vf drawtext="textfile=textfile:fontfile=arial.ttf:x=(w-tw)/2:y=h-t*100:fontcolor=white:fontsize=30“

4.实现右上角显示当前时间？

**动态文本**

在右上角显示当前时间 localtime

ffplay jidu.mp4 -vf drawtext="fontfile=arial.ttf:x=w-tw:fontcolor=white:fontsize=30:text='%{localtime\:%H\\\:%M\\\:%S}'“

每隔3秒显示一次当前时间

ffplay jidu.mp4 -vf drawtext="fontfile=arial.ttf:x=w-tw:fontcolor=white:fontsize=30:text='%{localtime\:%H\\\:%M\\\:%S}':enable=lt(mod(t\,3)\,1)"

------

## **9. 图片处理**

图片支持: FFmpeg支持绝大多数图片处理, 除LJPEG（无损JPEG）之外，其他都能被解码，除了EXR,PIC,PTX之外，所有的都能被编码。

截取一张图片使用 –ss(seek from start)参数:ffmpeg -ss 01:23:45 -i jidu.mp4 image.jpg

从视频中生成GIF图片:ffmpeg -i jidu.mp4 -t 10 -pix_fmt rgb24 jidu.gif

转换视频为图片（每帧一张图):ffmpeg -i clip.avi frame%4d.jpg

图片转换为视频:ffmpeg -f image2 -i img%4d.jpg -r 25 video.mp4

裁剪:ffmpeg -f lavfi -i rgbtestsrc -vf crop=150:150 crop_rg.png

填充:ffmpeg -f lavfi -i smptebars -vf pad=360:280:20:20:orange pad_smpte.jpg

翻转:ffmpeg -i orange.jpg -vf hflip orange_hfilp.jpg

　　ffmpeg -i orange.jpg -vf vflip orange_vfilp.jpg

旋转:ffmpeg -i image.png -vf transpose=1 image_rotated.png

覆盖:ffmpeg -f lavfi -i rgbtestsrc -s 400x300 rgb .png

　　ffmpeg -f lavfi -i smptebars smpte.png

　　 ffmpeg -i rgb .png -i smpte.png -filter_complex overlay= (W-w)/2:(H-h)/2 rgb_smpte.png

------

## **10.其他高级技巧**

**屏幕录像**

显示设备名称:ffmpeg -list_devices 1 -f dshow -i dummy

调用摄像头:ffplay -f dshow -i video="Integrated Camera"

保存为文件:ffmpeg -y -f dshow -s 320x240 -r 25 -i video="Integrated Camera" -b:v 800K -vcodec mpeg4 new.mp4

添加字幕subtitles:语法 –vf subtitles=file, ffmpeg -i jidu.mp4 -vf subtitles=rgb.srt output.mp4

视频颤抖:ffplay –i jidu.mp4 -vf crop=in_w/2:in_h/2:(in_w-out_w)/2+((in_w-out_w)/2)*sin(n/10):(in_h-out_h)/2 +((in_h-out_h)/2)*sin(n/7)

色彩平衡:ffplay -i jidu.mp4 -vf curves=vintage

色彩变幻:fplay -i jidu.mp4 -vf hue="H=2*PI*t: s=sin(2*PI*t)+1“

彩色转换黑白:ffplay -i jidu.mp4 -vf lutyuv="u=128:v=128"

**设置音频视频播放速度:**

3倍视频播放视频: ffplay -i jidu.mp4 -vf setpts=PTS/3

?速度播放视频:ffplay -i jidu.mp4 -vf setpts=PTS/(3/4)

2倍速度播放音频:ffplay -i speech.mp3 -af atempo=2

**截图**

每隔一秒截一张图:　　ffmpeg -i input.flv -f image2 -vf fps=fps=1 out%d.png

每隔20秒截一张图:　　ffmpeg -i input.flv -f image2 -vf fps=fps=1/20 out%d.png

多张截图合并到一个文件里（2x3） ?每隔一千帧(秒数=1000/fps25)即40s截一张图

ffmpeg? -i jidu.mp4 -frames 3 -vf "select=not(mod(n\,1000)),scale=320:240,tile=2x3" out.png

**马赛克视频**

用多个输入文件创建一个马赛克视频：

ffmpeg -i jidu.mp4 -i jidu.flv -i "Day By Day SBS.mp4" -i "Dangerous.mp4" -filter_complex "nullsrc=size=640x480 [base]; [0:v] setpts=PTS-STARTPTS, scale=320x240 [upperleft]; [1:v] setpts=PTS-STARTPTS, scale=320x240 [upperright]; [2:v] setpts=PTS-STARTPTS, scale=320x240 [lowerleft]; [3:v] setpts=PTS-STARTPTS, scale=320x240 [lowerright]; [base][upperleft] overlay=shortest=1 [tmp1]; [tmp1][upperright] overlay=shortest=1:x=320 [tmp2]; [tmp2][lowerleft] overlay=shortest=1:y=240 [tmp3]; [tmp3][lowerright] overlay=shortest=1:x=320:y=240" -c:v libx264 output.mkv

**Logo动态移动**

2秒后logo从左到右移动：ffplay -i jidu.mp4 -vf movie=logo.png[logo];[in][logo]overlay=x='if(gte(t\,2)\,((t-2)*80)-w\,NAN)':y=0

2秒后logo从左到右移动后停止在左上角:ffplay -i jidu.mp4 -vf movie=logo.png[logo];[in][logo]overlay=x='if(gte(((t-2)*80)-w\,W)\,0\,((t-2)*80)-w)':y=0

每隔10秒交替出现logo:ffmpeg -y -t 60 -i jidu.mp4 -i logo.png -i logo2.png -filter_complex "overlay=x=if(lt(mod(t\,20)\,10)\,10\,NAN ):y=10,overlay=x=if(gt(mod(t\,20)\,10)\,W-w-10\,NAN ) :y=10" overlay.mp4

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/451247703

# 【NO.394】2022技术展望｜开源十年，WebRTC 的现状与未来

2020 年，WebRTC 发生了很多变化。WebRTC 其实就是一个客户端库。大家都知道它是开源的。尽管 Google 大力地在支持 WebRTC，但社区的力量同样功不可没。

WebRTC 对于桌面平台、浏览器端实现音视频交互来讲十分重要。因为在你可以再浏览器上运行任何一种服务，并进行安全检查，无需安装任何应用。这是此前开发者使用该开源库的主要方式。

但 2020 年，浏览器的发展方向变了。首先讲讲 Microsoft，它将自研的浏览器引擎替换为基于 Chromium 的引擎，同时它们也成为了 WebRTC 的积极贡献者。Microsoft 贡献之一是 perfect negotiation，它使得两端以更稳妥的形式协商。而且，它们还改良了屏幕捕获，使其效率更高。

另一方面，还有 Safari。苹果的 Safari 还在继续改进他们 WebRTC API。激动人心的是，最新一版的 Safari Tech Preview 中已支持了 VP9，而且还支持硬件加速，大家可以在 Safari 的“开发者设置”中启用它。

火狐浏览器增加了重传以及 transport-cc，这有助于更好地估计可用带宽，从而改善媒体质量。

另一方面，Project Zero——Google 负责产品安全性的团队，通过寻找漏洞，帮助提高 WebRTC 的安全性。这意味着如果你的库不基于浏览器，及时更新 WebRTC 库、遵守说明就更加重要了。

------

另一件激动人心的事情就是，2020 年，云游戏已经上线了。它的实现有赖于 WebRTC。Stadia（Google 的云游戏平台）已于 2019 年底推出，但 2020 年初才正式在浏览器得以支持。其云游戏搭载 VP9，提供 4k、HDR 图像和环绕声体验。这些都会通过 WebRTC 进行传输。

数月前，NVIDIA 也发布了适用于 Chromebook 的 GeForce Now，同样使用了 WebRTC。最近，Microsoft 和亚马逊也宣布支持基于浏览器的云游戏开发。这确实促使 WebRTC 从数百毫秒延迟降低到了数十毫秒延迟，同时开启了全新的应用场景。但最重要的是， 2020 年，实时通讯（RTC）对于每个人来说都是必不可少的一部分。因此，许多网络服务的使用率暴涨，涨幅从十倍到几百倍不等。大家打语音电话的次数更多了，时间更久了，群组数量和成员人数也增加了， 线上交流越来越多。所以我们需要更丰富的互动方式。

从 Google 的角度来看， 在疫情爆发的头 2、3 个月内，我们的最大需求容量增长了 30 倍。所以即使是 Google，要确保后端和所有系统功能都可以应对这么大的增长，我们也付出了很多努力。

在变化面前， WebRTC 和实时通信使用量激增。大众的日常习惯也在变化。现在不只在公司能工作， 自己的卧室、厨房里都是工作场所了。由于“社交距离”，面对面交流变得不再现实，我们需要其它与他人社交的方法。我们只能通过视频，依据别人的表情猜测他的意图，此时高清的视频质量就显得更加重要了。

每个人协作的方式不同，可能是因为我们用的设备不一样。如果你在公司， 你用的可能是台式机，那你可能会用它在会议室里开会。而下班之后，你可能会带你的笔记本电脑回家。但现在人们都在用笔记本处理各种事宜，比如同时运行应用、视频会议和文字聊天。这种场景下，电脑的使用率非常高。我们看到学校里的孩子们也在用笔记本电脑，比如 Chromebook， 但他们电脑的性能相对差一点。社交、学习线上化之后，电脑的任务处理量突然增大， 所以开展该 WebRTC 项目的意义在于我们需要帮助扩展 WebRTC，确保其运行良好。

其次，我们需要为 Web 开发者和实时通讯开发者提供更大的灵活度，让他们可以在当下开发出新的互动体验。当疫情爆发时，它阻碍我们了在 Chrome 中开展的所有实验，于是我们所做的一件事情就是专注于服务的扩展、维护。但这远远不够，特别是在提高性能方面，我们需要做得更好。

------

大家可以猜一猜，如果你要在任何使用 WebRTC 的浏览器中开展实时服务， 最耗性能的部分会是什么呢？是视频编码？音频编码？网络适配？（因为你会考虑到可能会有丢包和网络变化）又或者是渲染？

当你想在屏幕显示摄像头采集的画面时，我们可以来看看浏览器中发生了什么。我们假设你有一个通过 USB 驱动程序输入的摄像头， 驱动运行，开始处理，摄像头可能还会进行人脸检测、亮度调整等操作。这要经过浏览器内的系统，Chrome 和其它浏览器都是多进程的。多进程有助于浏览器的稳定性和安全性，比如一个组件或一个页面崩溃，或存在安全漏洞，那么它就会与其他沙盒中的组件隔离。但这也意味着进程间有大量的通信。所以如果你有一帧视频数据从摄像头被采集，它可能是 MJPEG 格式。当它开始渲染你定义媒体流的页面时， 格式可能为 I420。当从渲染进程转到 GPU 进程（需要实际在屏幕上绘制）时，需要提供最高质量的数据，此时数据可能是 RGB 格式。当它再次进入操作系统，在屏幕上进行合成时， 可能需要一个 alpha 层， 格式又要变。这中间涉及到大量转换和复制步骤。由此可见， 无论内容来自摄像头还是某一终端，仅仅把它放到屏幕上的视频帧中就要花费大量的处理时间。所以这就是 WebRTC 服务中最复杂的部分——渲染。

![img](https://pic3.zhimg.com/80/v2-f7abd945ef89a1a43b6987accb9cad8a_720w.webp)

这也是我们做了很多优化的地方。渲染变得更加高效了，可以确保我们不会在每次更新视频帧时都重新绘制。如果同时有多个视频，我们会把他们同步，再做其他操作。Chrome 团队优化了内存分配，确保每个视频帧都以最有效的方式得到分配。我们还改进了 Chrome OS 上的操作系统调度，以确保视频服务即使负载过重也能保证交互和响应。接下来的几个月里，我们将致力于从摄像头采集到视频帧渲染到屏幕这个过程的“零拷贝”。我们希望不会出现一次拷贝或转换，但所有信息都会以最高效的方式保存在图片内存里的。

同时，我们也致力于使刷新率适应视频帧率。所以在没有任何变化的情况下，我们不需要 60Hz 的屏幕刷新率，但要适应视频的帧速率，例如 25 秒一次。以下是我们觉得有用的建议：

1、避免耗时耗力的扩展操作，在 incongnito 模式下进行测试。

避免耗时耗力的扩展操作很难，它可以干扰你的服务进程，减缓你的服务速度。

2、避免安全程序干扰浏览器运行

杀毒软件若要做深度数据包检查或阻止数据包，会占用大量 CPU。

3、通过 Intel Power Gadgets 来测试

我们建议你用 Intel Power Gadgets 看看你的服务用了多少能耗。它会比只看 CPU 百分比直观的多。

4、花哨的视频效果会占用更多性能

如果你用一些花哨的动画， 比如会动的圆点来装饰你的视频帧，就会占用更多性能。尽管看起来不错，但它可能会导致视频帧卡顿一番才能渲染在屏幕上。

5、摄像头分辨率设置与传输分辨率一致

如果你使用摄像头采集，请确保打开摄像头时将其分辨率的设置，与你调用 getUserMedia 时的设置保持一致。如果你打开摄像头，设置了高清画质，格式为 VGA，那么势必需要转换很多字节的信息都会被扔掉。

6、要留意 WebAudio 的使用

WebAudio 可能比预期需要更多 CPU 来处理。

------

**关于视频编解码**

视频编解码器可用于构建更高性能服务器。因为不仅 CPU 资源很重要， 若你构建网络相关的服务，视频编解码器就显得重要起来了。如果你要把业务拓展一百倍， Google 提供一款免费的编解码器，VP8、VP9、AV1，并且他在所有浏览器中都可用。

![img](https://pic4.zhimg.com/80/v2-d81aa327ba0c930e4fcae985c889116b_720w.webp)

VP8 是目前为止浏览器内部使用最多的版本，所有浏览器都支持它。VP9 同样在市场中流通很多年了，也一直在改进。它具备 30%-50% 的节约率，以及如支持 HDR 和 4K 的高质量功能。同时，它广泛应用于谷歌内部，支持 Stadia 及其他内部服务。因为它有 VP8 中没有的功能，即能帮助你更好地适应高低带宽连接的转换。然后是 AV1。AV1 也即将在 WebRTC、一些开源实现和浏览器中使用。大多数浏览器已经可以使用它进行流式传输。希望明年能正式启用它。实际上，微软刚刚宣布他们的操作系统将支持硬件加速 AV1。性能的提升给予了开发者更大空间。

------

**WebRTC NV（Next Version）**

发布 WebRTC 1.0 之后，我们就和社区一起研发下一个版本, 该版本叫“NV”。该版本意在支持当前 WebRTC API 不可能或很难实现的新用例，比如虚拟现实。对于虚拟现实特效，就像前面提到过的笔记本电脑和机器学习的例子一样， 为了能够使用 WebRTC API 运行，我们需要更好地掌握媒体处理的技能， 比如更好控制传输和拥塞，使用编解码器进行更多自定义操作等等。

在以上这些目标上，WebRTC NV 的思路是不定义全新 API。目前已经有两个 API 和 WebRTC，PeerConnetion 和 getUserMedia 了。我们不会重新定义它们，从头开始研发。相反，我们正在做的是：允许我们使用称为“HTML 流”的接口访问端对 peer connection 内部，以及允许访问浏览器中的编解码器的接口。再加上诸如 Web Assembly 和 workers threads 的新技术，你可以在浏览器，以及集成的端对端连接中使用 Javascript 进行实时处理。

如果看一下现在的 WebRTC 的内部，你会发现媒体流就像是从网络传入时一样被拆包（depacketized）。这里会有一些丢失或延迟的适配。因此，我们对此进行了重构。

另一方面， 摄像头输入或麦克风输入已经经过编解码器如 Opus 或 VP8，去除了回声。比特率已经根据网络情况进行了适配，然后将其打包为 RTP 数据包并通过网络发送。我们想做到在 WebRTC NV 中拦截该管道，所以要从媒体框架开始。因此，我们希望能够在媒体帧从网络到达显示器，以及从摄像机麦克风到网络回到媒体帧时对其进行监听。我们希望能够更好地管理这些流。目前我们提出两个流方案，也正是我致力研究的两个 API。

![img](https://pic3.zhimg.com/80/v2-6247cd4c566d30aec7f1484bfca8df3e_720w.webp)

第一个是可插入媒体流（Insertable Media Stream）。当前的 Chrome 浏览器 86 中已提供此功能。Google 服务和其他外部服务已使用了此功能。你可以使用它来实现端到端加密，或者可以使用它向框架中添加自定义元数据（meta-data）。你要做的是在 PeerConnection 中定义此编码的可插入媒体流，并且你也可以创建流。之后，当你从摄像头获取视频帧时，它首先被编码，比如 VP8 格式，之后你可以访问它并进行流式处理。你还可以对其进行加密或标记其中一些元数据。

另一个是原始媒体流 API（Raw Media Stream）。这是标准委员会正在讨论的标准工作。目前已经有一些确切的建议了。从 Google 的角度来说，我们正在尝试这种实现。该 API 允许我们访问原始帧。它意味着，当原始帧从摄像头采集后，在还未进行视频编码前，你就可以访问这些数据了。然后你可以对其进行处理，比如实现 AR 效果。你还可以运行多个滤镜来删除背景，然后应用一些效果。比如我想把我现在的视频背景设成一个热带岛屿。这还可以应用到自定义的编解码器中，比如你此前使用的一些编解码器与现在的浏览器不兼容，那么你可以利用这个接口将数据直接传给编解码器来处理。原始媒体流 API 可以提供一种非常有效的方式来访问此原始媒体。

总结一下。虽然 WebRTC 作为 W3C 正式标准已经发布，但仍在继续改进。新的视频编解码器 AV1 可节省多达 50% 的带宽，正在 WebRTC 和网络浏览器中使用。开源代码的持续改进有望进一步减少延迟，提高视频流的质量。WebRTC NV 收集了创建补充 API 的倡议，以实现新的用例。这些 API 包括对现有 API 的扩展，以提供更多对现有功能的控制，例如可扩展视频编码，以及提供对 low-level 组件的访问的 API。后者通过集成高性能的定制 WebAssembly 组件，为网络开发者提供了更多的创新灵活性。随着新兴的 5G 网络和对更多交互式服务的需求，我们预计在未来一年内，持续增强在 WebRTC 的服务端建设。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/448159960



# 【NO.395】FFmpeg入门宝典，音视频流媒体开发学习，一篇看到就要收藏的文章（附20个视频资料）

2G打开了了移动互联网天下，3G带来了即时通信，诞生了QQ 微信等巨头，4G 带来了短视频兴起。字节跳动等公司崛起。2 3 4G的出现促成了移动互联网10年繁荣。而5G的出现，也会促成至少10年音视频行业的繁荣。所以，做音视频研发的前景是广阔的，对于很早看出音视频前景的伙伴来说，已经开始学习，想要尽早的投入音视频研发的队伍，掌握这个技术，跳槽涨薪，未来发展提升是必然。

**音视频行业行业现状-**核心竞争力：定义音视频是程序届的皇冠，掌握音视频意味着拿到通往未来的船票，不用担心会被其他人替代。音视频是有门槛的。是与其他人拉开差距的分水岭

高端人才相关缺乏：Boss直聘中，北上广深很多年限上50w-70w的音视频岗位，常年还招不到人，月薪2-3万大多是刚从事音视频入门级开发者

技术迭代慢：就H264编码从95年成为标准至今，都在使用。比较偏底层技术，底层技术几十年不会有太大的改变。

想要入门音视频开发必须要从FFmpeg学起，本篇介绍了一个基础的学习资料，附带视频讲解，建议码住收藏，关注我，持续输出音视频开发技术文章！

## 1.FFmpeg入门宝典

## **2.播放器框架**

![img](https://pic2.zhimg.com/80/v2-acb7e5e4a6dddc6aa0211c13616049a5_720w.webp)

------

## 3.**常见音视频概念**

![img](https://pic4.zhimg.com/80/v2-616d2509ce9eecf2de7938b52512015b_720w.webp)

**容器／文件（Conainer/File）：**即特定格式的多媒体文件， 比如mp4、flv、mkv等。

• **媒体流（Stream）：**表示时间轴上的一段连续数据，如一 段声音数据、一段视频数据或一段字幕数据，可以是压缩的，也可以是非压缩的，压缩的数据需要关联特定的编解码器。

• **数据帧／数据包（Frame/Packet）：**通常，一个媒体流是由大量的数据帧组成的，对于压缩数据，帧对应着编解码器的最小处理单元，分属于不同媒体流的数据帧交错存储于容器之中。

• **编解码器：**编解码器是以帧为单位实现压缩数据和原始数据之间的相互转换的

------

## 4.**常用概念-复用器**

![img](https://pic2.zhimg.com/80/v2-e7686000de2ede9f3d7c3f50aec09ca1_720w.webp)

## 5.**常用概念-编解码器**

![img](https://pic2.zhimg.com/80/v2-74e6fb3f1789dda608f6f4220776711d_720w.webp)



## 6.**FFMPEG入门**

## **7.FFmpeg8个常用库简介**

![img](https://pic3.zhimg.com/80/v2-2957b34da10d9b7902f2117a8e1f790e_720w.webp)

**FFMPEG有8个常用库**：

• AVUtil：核心工具库，下面的许多其他模块都会依赖该库做一些基本的音视频处理操作。

• **AVFormat**：文件格式和协议库，该模块是最重要的模块之一，封装了Protocol层和Demuxer、Muxer层，使得协议和格式对于开发者来说是透明的。

• **AVCodec**：编解码库，封装了Codec层，但是有一些Codec是具备自己的License的，FFmpeg是不会默认添加像libx264、FDK-AAC等库的，但是FFmpeg就像一个平台一样，可以将其他的第三方的Codec以插件的方式添加进来，然后为开发者提供统一的接口。

• AVFilter：音视频滤镜库，该模块提供了包括音频特效和视频特效的处理，在使用FFmpeg的API进行编解码的过程中，直接使用该模块为音视频数据做特效处理是非常方便同时也非常高效的一种方式。

**AVDevice**：输入输出设备库，比如，需要编译出播放声音或者视频的工具ffplay，就需要确保该模块是打开的，同时也需要SDL的预先编译，因为该设备模块播放声音与播放视频使用的都是SDL库。

• **SwrRessample**：该模块可用于**音频重采样**，可以对数字音频进行声道数、数据格式、采样率等多种基本信息的转换。

• **SWScale**：该模块是将图像进行格式转换的模块，比如，可以将YUV的数据转换为RGB的数据，缩放尺寸由1280*720变为800*480。

• **PostProc**：该模块可用于进行后期处理，当我们使用AVFilter的时候需要打开该模块的开关，因为Filter中会使用到该模块的一些基础函数。

◼ av_register_all()：注册所有组件,4.0已经弃用

◼ avdevice_register_all()对设备进行注册，比如V4L2等。

◼ avformat_network_init();初始化网络库以及网络加密协议相关的库（比如openssl）

------

**FFmpeg函数简介**

◼ avformat_alloc_context();负责申请一个AVFormatContext结构的内存,并进行简单初始化

◼ avformat_free_context();释放该结构里的所有东西以及该结构本身

◼ avformat_close_input();关闭解复用器。关闭后就不再需要使用avformat_free_context 进行释放。

◼ avformat_open_input();打开输入视频文件

◼ avformat_find_stream_info()：获取视频文件信息

◼ av_read_frame(); 读取音视频包

◼ avformat_seek_file(); 定位文件

◼ av_seek_frame():定位文件

![img](https://pic1.zhimg.com/80/v2-2b667e31292012695860503595554184_720w.webp)

------

**编解码相关**

• avcodec_alloc_context3():分配解码器上下文

• avcodec_find_decoder()：根据ID查找解码器

• avcodec_find_decoder_by_name():根据解码器名字

• avcodec_open2()：打开编解码器

• avcodec_decode_video2()：解码一帧视频数据

• avcodec_decode_audio4()：解码一帧音频数据

• avcodec_send_packet():发送编码数据包

• avcodec_receive_frame():接收解码后数据

• avcodec_free_context():释放解码器上下文，包含了avcodec_close()

• avcodec_close():关闭解码器

![img](https://pic1.zhimg.com/80/v2-585fdf9938d7a51ca02da6e013b1c8bc_720w.webp)

**FFmpeg3.3 组件注册方式**

我们使用ffmpeg，首先要执行av_register_all，把全局的解码器、编码器等结构体注册到**各自全局的对象链表里**，以便后面查找调用。

![img](https://pic3.zhimg.com/80/v2-6f776c9920e353aa66d1f91c0d620c12_720w.webp)

**FFmpeg 4.0.2 组件注册方式**

FFMPEG内部去做，不需要用户调用API去注册。

**以codec编解码器为例**：

\1. 在configure的时候生成要注册的组件./configure:7203:print_enabled_components libavcodec/codec_list.c AVCodec codec_list $CODEC_LIST

这里会生成一个**codec_list.c** 文件，里面只有static const AVCodec * const **codec_list[]**数组。

\2. 在**libavcodec/allcodecs.c**将static const AVCodec * const codec_list[]的编解码器用链表的方式组织起来。

FFMPEG内部去做，不需要用户调用API去注册。 对于demuxer/muxer（解复用器，也称容器）则对应

\1. libavformat/muxer_list.c libavformat/demuxer_list.c 这两个文件也是在configure的时 候生成，也就是说直接下载源码是没有这两个文件的。

\2. 在libavformat/allformats.c将demuxer_list[]和muexr_list[] 以链表的方式组织。

**其他组件也是类似的方式**

**FFmpeg数据结构简介**

AVFormat**Context**

封装格式上下文结构体，也是统领全局的结构体，保存了视频文件封装格式相关信息。

AVInputFormat demuxer

每种封装格式（例如FLV, MKV, MP4, AVI）对应一个该结构体。

AVOutputFormat muxer AVStream

视频文件中每个视频（音频）流对应一个该结构体。

AVCodec**Context**

编解码器上下文结构体，保存了视频（音频）编解码相关信息。

AVCodec

每种视频（音频）编解码器(例如H.264解码器)对应一个该结构体。

AVPacket

存储一帧压缩编码数据。

AVFrame

存储一帧解码后像素（采样）数据。

------

## **8.FFmpeg数据结构之间的关系**

**AVFormatContext和AVInputFormat之间的关系**

**AVFormatContext API调用**

**AVInputFormat 主要是FFMPEG内部调用**

数据：AVFormatContext 封装格式上下文结构体

struct **AVInputFormat** *iformat;

所有方法可重入的

方法：AVInputFormat 每种封装格式（例如FLV, MKV, MP4）

int (*read_header)(struct **AVFormatContext** * );

int (*read_packet)(struct **AVFormatContext** *, AVPacket *pkt);

面向对象的封装？

int avformat_open_input(AVFormatContext **ps, const char *filename, AVInputFormat *fmt, AVDictionary **options)

**AVCodecContext和AVCodec之间的关系**

**数据：**AVCodecContext 编码器上下文结构体

struct **AVCodec** *codec;

**方法**：**AVCodec** 每种视频（音频）编解码器

int (*decode)(**AVCodecContext** *, void *outdata, int *outdata_size, AVPacket *avpkt);

int (*encode2)(**AVCodecContext** *avctx, AVPacket *avpkt, const AVFrame *frame, int *got_packet_ptr);

**AVFormatContext, AVStream和AVCodecContext之间的关系**

![img](https://pic4.zhimg.com/80/v2-5f546884eb74641d28dbbb47eeb496c7_720w.webp)

**区分不同的码流**

AVMEDIA_TYPE_VIDEO视频流

video_index = av_find_best_stream(ic, AVMEDIA_TYPE_VIDEO,

-1,-1, NULL, 0)

AVMEDIA_TYPE_AUDIO音频流

audio_index = av_find_best_stream(ic, AVMEDIA_TYPE_AUDIO,

-1,-1, NULL, 0)

**AVPacket和AVFrame之间的关系**

![img](https://pic1.zhimg.com/80/v2-ebf69498ebb7745807e2df6cc09b6b38_720w.webp)

**AVFormatContext**

• iformat：输入媒体的AVInputFormat，比如指向AVInputFormat ff_flv_demuxer

• nb_streams：输入媒体的AVStream 个数

• streams：输入媒体的AVStream []数组

• duration：输入媒体的时长（以微秒为单位），计算方式可以参考**av_dump_format()**函数。

• bit_rate：输入媒体的码率

**AVInputFormat**

• name：封装格式名称

• extensions：封装格式的扩展名

• id：封装格式ID

• 一些封装格式处理的接口函数,比如read_packet()

**AVStream**

• index：标识该视频/音频流

• time_base：该流的时基，PTS*time_base=真正的时间（秒）

• avg_frame_rate： 该流的帧率

• duration：该视频/音频流长度

• codecpar：编解码器参数属性

**AVCodecParameters**

• codec_type：媒体类型AVMEDIA_TYPE_VIDEO/AVMEDIA_TYPE_AUDIO等

• codec_id：编解码器类型， AV_CODEC_ID_H264/AV_CODEC_ID_AAC等。

**AVCodecContext**

• codec：编解码器的AVCodec，比如指向AVCodec ff_aac_latm_decoder

• width, height：图像的宽高（只针对视频）

• pix_fmt：像素格式（只针对视频）

• sample_rate：采样率（只针对音频）

• channels：声道数（只针对音频）

• sample_fmt：采样格式（只针对音频）

**AVCodec**

• name：编解码器名称

• type：编解码器类型

• id：编解码器ID

• 一些编解码的接口函数，比如int (*decode)()

**AVPacket**

• pts：显示时间戳

• dts：解码时间戳

• data：压缩编码数据

• size：压缩编码数据大小

• pos:数据的偏移地址

• stream_index：所属的AVStream

**AVFrame**

• data：解码后的图像像素数据（音频采样数据）

• linesize：对视频来说是图像中一行像素的大小；对音频来说是整个音频帧的大小

• width, height：图像的宽高（只针对视频）

• key_frame：是否为关键帧（只针对视频） 。

• pict_type：帧类型（只针对视频） 。例如I， P， B

• sample_rate：音频采样率（只针对音频）

• nb_samples：音频每通道采样数（只针对音频）

• pts：显示时间戳

------

## 9.**SDL简介作用**

SDL(Simple DirectMedia Layer)库的作用主要是封装了复杂的视音频底层交互工作， 简化了视音频处理的难度。

本次课程我们重点不在SDL，只因为涉及到一些函数的调用，所以稍微做一个简介。

SDL结构如下所示。可以看出它实际上还是调用了DirectX等底层的API完成了和硬件的交互

![img](https://pic1.zhimg.com/80/v2-179ec6305981767a4ae01d5c1db843b4_720w.webp)

**SDL视频显示函数简介**

• SDL_Init()：初始化SDL系统

• SDL_CreateWindow()：创建窗口SDL_Window

• SDL_CreateRenderer()：创建渲染器SDL_Renderer

• SDL_CreateTexture()：创建纹理SDL_Texture

• SDL_UpdateTexture()：设置纹理的数据

• SDL_RenderCopy()：将纹理的数据拷贝给渲染器

• SDL_RenderPresent()：显示

• SDL_Delay()：工具函数，用于延时。

• SDL_Quit()：退出SDL系统

**SDL中事件和多线程**

**SDL多线程**

• 函数

• SDL_CreateThread()：创建一个线程

• SDL_LockMutex(), SDL_UnlockMutex()：互斥量操作

• 数据结构

• SDL_Thread：线程的句柄

**SDL事件**

• 函数

• SDL_WaitEvent()：等待一个事件

• SDL_PushEvent()：发送一个事件

• SDL_PumpEvents()：将硬件设备产生的事件放入事件队列

• SDL_PeepEvents()：从事件队列提取一个事件

• 数据结构

• SDL_Event：代表一个事件



原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/432586853

# 【NO.396】FFmpeg的结构和命令行工具（在线介绍）

## 1.FFmpeg介绍以及编译

音视频开发，首先不得不提到FFmpeg。该框架为开发者们提供了非常大的帮助，它是一套可以用来采集、处理、编码、传输的开源框架。可以用在各种PC端（Linux、Windows、macOS）和移动端（iOS、Android）等平台。本节会从编译开始，介绍一下FFmpeg的框架，以及在iOS平台上的使用。

## 2.FFmpeg编译选项

我们可以到FFmpeg官网下载稳定版本的源码（一般来说4.3的版本比4.3.1的版本稳定）。然后将下载的源码解压，FFmpeg与大部分GNU软件的编译方式类似，都是通过configure脚本来实现编译前定制的，这种方式可以让用户在编译前对软件进行裁剪，同时通过对运行系统以及目标平台的指定，实现满足需求的最小包体积编译。我们可以利用help命令来查看它有哪些编译选项。

```text
./configure -help
复制代码
```

1. 标准选项：GNU软件配置项，例如安装路径、-- prefix=...等。
2. 专利使用协议选项：一些开源协议。
3. 编译、链接选项：生成静态库还是动态库这些。
4. 可执行程序选项：决定是否生成FFmpeg、ffplay、ffprobe 和ffserver等。
5. 文档选项：文档的类型。
6. 模块选项：需要开启或者关闭里面哪些模块。
7. Toolchain选项：CPU架构、交叉编译，操作系统。C、C++的一些配置等。
8. 其他：一些其他深度定制编译选项。

## 3.FFmpeg结构

默认的编译会生成4个可执行文件和8个静态库。可执行文件分别是用于转码、裁剪文件的ffmpeg、用于播放媒体文件的ffplay、用于获取文件信息的ffprobe、以及作为简单流媒体服务器的ffserver。8个静态库就是FFmpeg的8个模块，具体包括以下内容：

- AVUtil：核心工具库，该模块是最基础的模块之一，许多其他的模块都会依赖这个工具做一些操作。
- AVFormat：文件格式和协议库，该模块是用来做传输层数据的封装和解封装的，使得协议和格式转换处理更加方便。
- AVCodec：编解码库，FFmpeg有一些默认的编解码库，也可以集成如H264、FDK-AAC等库。其他第三方库以插件的形式添加在FFmpeg中，API调用符合默认接口风格，为开发者提供了统一的接口。
- AVFilter：音视频滤镜库，这个模块提供了音视频中的各种特效处理，非常方便。
- AVDevice：输入输出设备库，获取系统上的输入输出设备，从而实现采集或者播放音视频。
- SWResample：音频重采样，音频的格式转换、混音。
- SWScale：提供图像的裁剪、旋转、格式转换等功能。
- PostProc：视频后期处理。

## 4.iOS平台的编译

对于iOS平台来说，需要在configure的时候加入以下内容：

```text
./configure \
--target-os=darwin \
--arch="arm64" \
--cc="xcrun -sdk iphoneos clang" \
--extra-cflags="-arch arm64 -mios-version-min=8.0 -Ixx" \
--extra-ldflags="-arch arm64 -mios-version-min=8.0 -Lxx" \
复制代码
```

这里推荐一个编译脚本FFmpeg-iOS-build-scriptt，它可以帮我下载指定的FFmpeg版本，选择编译的平台，配置链接的第三方库，非常好用。

## 5.在macOS上安装FFmpeg命令行工具

```text
brew install ffmpeg
复制代码
```

如果安装好没有ffplay工具，那么在安装时加上--with-ffplay就有了。

```text
brew install ffmpeg --with-ffplay
```

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/429550853

# 【NO.397】音视频编解码常用知识点

## 1. 前言

ffplay是ffmpeg的一个子工具，它具有强大的音视频解码播放能力，目前它广泛被各种流行播放器（QQ影音、暴风影音……）集成应用。作为一款开源软件，ffplay囊括Linux、Windows、Ios、Android等众多主流系统平台，十分适合进行二次开发。这里有必要介绍一下它常用的技巧。首先下载ffmpeg代码包，里面有免编译版、源代码百、静态库版、动态库版，具体怎么下载安装请参考我的博文《FFmpeg简介、功能入门、源码下载安装、常规应用》。接下来以Windows平台为例子讲述一下具体用法。

## 2. 使用技巧

Win+r组合键运行cmd进入Windows命令行控制界面，使用cd命令进入ffplay.exe的可执行目录（当然也可以使用环境变量等手段使ffplay.exe命令全局可用），其他平台如linux的操作也类似。ffplay的基本用法很简单，其一般形式如下：

```text
ffplay [option] file
ffplay [option] URL
```

总结起来ffplay的用法就是option项加上资源路径，option项是用来指定播放时的一些参数的，如指定连接的协议、视频画面的大小，音视频解码器选用、传输码率设定等，一般这些参数我们很少会设置，使用默认就OK，此时option项可以直接忽略，ffplay会帮我们选择，这也是它功能强大的体现，option的更多具体选项可以参考其官方文档；资源路径则包括文件资源路径和网络资源路径，文件资源路径是指定需要播放的音视频文件，如*.mp3、*.mp4、*,avi、*.mkv、*.rmvb等等类型的文件，网络资源路径根据协议可以分为RTSP、RTMP、HTTP流资源，心情好，来个直播，如：

```text
ffplay rtmp://live.hkstv.hk.lxdns.com/live/hks
```

再或者，用http浏览一下视频，如：

```text
ffplay http://live.hkstv.hk.lxdns.com/live/hks/playlist.m3u8
```

RTSP播放也了解一下（对于RTSP播放有个坑，请参考《ffplay播放rtsp网络串流失败问题》），如：

```text
rtsp://184.72.239.149/vod/mp4://BigBuckBunny_175k.mov
```

音视频文件指定分辨率播放

```text
ffplay -vfscale=1920:1080 xxxx.avi
```



![img](https://pic3.zhimg.com/80/v2-4c1fb9a29803126870077cf5cd729406_720w.webp)

## 3. 番外篇

在安防等视频流媒体数据处理领域，我们可能更关注的是用ffplay播放RTSP音视频流，其实国内各大厂商的VMS（video management system）平台也是基于此设计的。每每使用它们的IPC、NVR时都需要下载它们，但是有了ffplay神器，一个就够了，它可以播放诸如海康、大华、长视等厂商IPC、NVR的RTSP流，视频监控就变得如此简单。这里很有必要介绍一下RTSP链接的格式。

RTSP链接格式与HTTP链接格式类似，也是由URL（Uniform Resource Locator）发展继承而来。URL由三部分组成：资源类型、存放资源的主机域名、资源文件名，一般语法格式为(带方括号[]的为可选项)：

```text
protocol :// hostname[:port] / path / [;parameters][?query]#fragment
```

这里就不一一解释其各项的含义了，我们重点关注RTSP链接的格式，相比于URL，RTSP由于参数表列是嵌入RTSP报文中的，格式上会少了parameters等参数选项，其一般格式如下：

```text
rtsp://[username]:[password]@[ip]:[port]/path
```

## 4. 总结

运用ffplay播放小技巧可以轻松应对各种文件资源和网络资源的播放，特别是在安防监控领域，使用它播放个监控资源那简直太方便了，而且还可以用它来检查验证音视频格式封包是否异常，在调试优化过程ffplay总能带给你惊喜。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/429125576

# 【NO.398】WebRTC 发送方码率预估实现解析

## 1.WebRTC使用的是Google Congestion Control (简称GCC)拥塞控制，目前有两种实现：

\* 旧的实现是接收方根据收到的音视频RTP报文, 预估码率，并使用REMB RTCP报文反馈回发送方。 * 新的实现是在发送方根据接收方反馈的TransportFeedback RTCP报文，预估码率。

## 2.基于延迟的拥塞控制原理

先来看下Google Congestion Control（GCC）的标准草案：
[https://tools.ietf.org/html/draft-ietf-rmcat-gcc-02](https://link.zhihu.com/?target=https%3A//tools.ietf.org/html/draft-ietf-rmcat-gcc-02) 结合草案，可以得知GCC是基于网络延迟梯度的拥塞控制算法，判断的依据如下图：



![img](https://pic3.zhimg.com/80/v2-dc6cd2e07d1017bd738fcd60c44f721e_720w.webp)



发送方发送两个包组的间隔为 ： Ds = Ts1 - Ts0

接收方接收两个包组的间隔为： Dr = Tr1 - Tr0

如果网络没有任何抖动，那么 [ delta = Dr - Ds ] 应该是一个恒定不变的值，但是现实中网络有抖动、拥塞、丢包等情况，所以delta也是一个抖动的值。 GCC通过测量delta，来判断当前网络的使用情况，分为 OverUse (过载)，Normal(正常)，UnderUse(轻载) 这三种情况。 有同学可能会问，发送方和接收方时钟不统一，怎么计算差值呢，需要做时间对齐或者NTP同步吗?

不需要，因为我们对比的是delta，只需要单位一致即可，举个例子: seq1的包 发送时间为 16000ms(发送方时钟)，接收时间为 900ms(接收方时钟) seq2的包 发送时间为 16001ms(发送方时钟)，接收时间为 905ms(接收方时钟) 那么延迟梯度delta=(905-900) - (16001-16000) = 4

## 3.Pacing和包组

值得注意的是，延迟梯度的判断是以包组为单位的，而且必须在发送方开启pacing发送, 有以下几点原因： 单个包测量误差会过大，基于包组的测量更能反应网络的情况。

burst发送容易冲击网络，影响测量的精度。 那么怎么判断哪几个包属于一个包组呢，非常简单，按发送方的pacing速率分包组。 WebRTC pacing默认是5ms一个包组，如下图所示



![img](https://pic3.zhimg.com/80/v2-3d7459412e7dd4d0a9db8d3141809946_720w.webp)



## 4.TransportFeedback RTCP报文

再来看看transport-feedback的包结构：
[https://tools.ietf.org/html/draft-holmer-rmcat-transport-wide-cc-extensions-01](https://link.zhihu.com/?target=https%3A//tools.ietf.org/html/draft-holmer-rmcat-transport-wide-cc-extensions-01)



![img](https://pic3.zhimg.com/80/v2-dc372427de5196fc5db1a876165c432a_720w.webp)



解析这个报文，我们可以得到下面的信息：

- 接收到的包seq和包的接收时间
- 丢失的包seq
- 可以看到本质上transport-feedback是接收方对数据的ACK，并且捎带了接收的延迟梯度。

## 5.发送方码率预估

收到transport-feedback报文后需要怎么处理，结合GCC的算法来看，分为以下几步： 1.计算接收方ack了多少个字节, 统计在采样的时间窗口内接收方的接收速率 看看GCC怎么说：



![img](https://pic3.zhimg.com/80/v2-078e84d9027540881304d33298f45d96_720w.webp)



按照这个算法实现acked_bitrate_estimate，可以计算出接收方在当前时间窗口内的接收速率。

2.将包按包组归类, 计算包组的发送时间 接收时间的差值 在前面的【Pacing和包组】中已经讲过，这里不再赘述

3.按包组的delta, 进行网络状态评估 GCC的标准草案里面使用的是卡尔曼滤波器(接收方评估)，发送方评估默认的实现是Trendline Filter。

基本的原理是最小二乘法, 将多个时间点的网络抖动(delta)拟合成一条直线，如下图所示：



![img](https://pic4.zhimg.com/80/v2-0f1ab31588ffbd5094a4791eb50ca6bf_720w.webp)



根据直线斜率的变化趋势判断网络的负载情况。 上面已经得到了包组的delta，对delta做平滑计算后，按照(时间点, 平滑后的delta)， 可以在坐标系上绘制出散点图，使用最小二乘法拟合出delta随时间变化的直线，根据直线斜率计算出变化趋势。 来看看GCC里面的说法：



![img](https://pic2.zhimg.com/80/v2-f848914ce7bc259be7ec342cd4d23e91_720w.webp)



- m(i)为i时刻的包组delta，del_var_th(i)为当前判断是否过载的门槛
- m(i) < -del_var_th(i)，判断为under-use(低载)
- m(i) > del_var_th(i) 且持续至少overuse_time_th时长，判断为over-use(过载)





![img](https://pic1.zhimg.com/80/v2-29a99dc1a7f8c0dba1cfabe9785dce14_720w.webp)



del_var_th必须动态调整，不然可能会在跟TCP的竞争中被饿死（出于公平性考虑）。过大的del_var_th会对延迟变化不敏感，过小的del_var_th则会过于敏感，抖动容易被频繁误判为过载，必须动态调整，才能和基于丢包的连接（比如TCP）竞争。

## 6.根据探测的网络情况, 预估码率

总体的思想是根据当前接收方的接收码率，结合当前的网络负载情况，进行AIMD码率调整：

- 在接近收敛前，使用乘性增，接近收敛时使，用加性增。
- 当网络过载时，使用乘性减。





![img](https://pic1.zhimg.com/80/v2-9972a991b8ffc24011b86ed5f9993f6c_720w.webp)



在Decrease状态下，会不停地计算平均最大码率（average max bitrate），当前预估码率和平均最大码率差值在3个标准差以内时，进行乘性增，否则进行加性增。如果包到达速率超过了平均最大码率的3个标准差，那么需要重新计算平均最大码率。





![img](https://pic1.zhimg.com/80/v2-5e5ac8001eb28c0a8f6ab860954e7894_720w.webp)



乘性增期间，每秒最多增加8%的码率





![img](https://pic2.zhimg.com/80/v2-d7104231726f50718721e54255d1c385_720w.webp)



加性增期间，每个rtt增加“半个”包大小



![img](https://pic1.zhimg.com/80/v2-6840db97de907d629bebfecdb0e765f4_720w.webp)



评估出的码率不能超过接收速率的1.5倍



![img](https://pic1.zhimg.com/80/v2-9f0de0363116fe86672b4b3eb6ca6514_720w.webp)



当探测到网络过载时，按照0.85的速率降低码率

## 7.发送方码率预估的算法流程

将上面的几步结合起来，可以得到一个大致的算法框架

```text
struct FeedbackResultVector {
    int64_t send_time_ms; // 包发送时间(发送时记录)
    int64_t recv_time_ms; // 包接收时间(从TransportFeedback RTCP报文解析得到)
    int packet_size; // 包大小
};

// 解析TransportFeedback RTCP报文
FeedbackResultVector feedback_result_vec = 
    TransportFeedbackRtcp.Parse(rtcp_feedback);
// 遍历每个包, 进行处理
for (feedback_result : feedback_result_vec) {
    double delta = 0.0;
    // 计算ack速率(接收方接收速率)
    AckBitrateEstimate.Update(now_ms, feedback_result.packet_size);
    // 把接收反馈包按照包组分类,计算包组delta
    bool compute_delta = PacketGroup.AddPacket(feedback_result, delta);
    if (comupute_delta) {
        // 探测网络状态
        TrendLineFilter.Detect(delta);
        // 根据GCC状态机,进行AIMD码率调整
        AimdRateControl.Update(
            TrendLineFilter.NetState(), 
            AckBitrateEstimate.Bitrate()
        );
    }
}
```

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/428681596

# 【NO.399】C++ 开发者的机会在哪里？盘点 2022 好的C/C++ 就业方向

引用一个校招脉友的提问：

![img](https://pic1.zhimg.com/80/v2-e05234d58bd10df607bc4f3d7d2aaa7c_720w.webp)

现在c++的机会是不是越来越少了？这个问题还是老生常谈，看看大家怎么说？

![img](https://pic4.zhimg.com/80/v2-fe65bb242f3c1f1f9ba9fb33ce1c272f_720w.webp)

![img](https://pic4.zhimg.com/80/v2-d11ee7f9a3477f242ff1b612deb34ecf_720w.webp)

![img](https://pic1.zhimg.com/80/v2-3cade2960ac7c3093481e76ea8706e78_720w.webp)

不难发现，c++很多的岗位很多都存在两个共性。

第一个，岗位比较高端，任职要求高；

第二个，部分在传统行业，流动性不大但薪资较其他的语言薪资较低。

所以显得c++在2022这个铜三铁四的环境下，行情比较稳定。比如嵌入式，数据库内核，信息安全，音视频，QT客户端等等。

那有没有这样一个方向，相对自动驾驶，AI底层算法任职要求要对较低，但是对于嵌入式，QT客户端又薪资相对较高的c/c++开发技术方向呢？

这里，推荐给大家考虑的是c/c++Linux服务器开发岗位。从技术上说，有技术深度，从岗位上来说，大厂又缺乏专业人才。从学习难易程度来说，又适合又c/c++基础的各个方向的工程师跨行学习。

主要从八个维度了解学习c/c++Linux后台开发技术：

## 1.精进基石

数据结构与算法

![img](https://pic3.zhimg.com/80/v2-9372615acfc5169ff507b3a412adcf0e_720w.webp)

![img](https://pic2.zhimg.com/80/v2-bce3d6095f098f773ccebab61a5e5e51_720w.webp)

![img](https://pic2.zhimg.com/80/v2-6118318530b6377f16d85747c3e34fb1_720w.webp)

![img](https://pic2.zhimg.com/80/v2-311a11e9ff4e9f4d116cbe791c38c4c5_720w.webp)

## 2.高性能网络设计

网络编程

![img](https://pic3.zhimg.com/80/v2-d6d290c7f9a4cc0ab4430411ea2c497e_720w.webp)

![img](https://pic4.zhimg.com/80/v2-03afe9f12f31d485d9bb6c2ad9dda4d3_720w.webp)

![img](https://pic3.zhimg.com/80/v2-503f6eaf95eda506bc048d9a5122f48a_720w.webp)

![img](https://pic2.zhimg.com/80/v2-0e13ef2840bace00e2edbf8b9ea27289_720w.webp)

## 3.基础组件设计

池式组件

![img](https://pic3.zhimg.com/80/v2-14789cab1c230c0256cffe639a80967e_720w.webp)

![img](https://pic3.zhimg.com/80/v2-cd5e638db40faefb9d98bb3af4662ad2_720w.webp)

![img](https://pic2.zhimg.com/80/v2-1913e03600904f4d077707cf12338a2d_720w.webp)

## 4.中间件开发

MySQL

![img](https://pic2.zhimg.com/80/v2-7efd73cd49ef24fb800649cd699ae241_720w.webp)

TiDB

![img](https://pic4.zhimg.com/80/v2-1a1ff8bd5a41365b87c113c3a5bc2e4f_720w.webp)

Redis

![img](https://pic3.zhimg.com/80/v2-9a0d2e75a3bcf0a5115e2620205bdd2e_720w.webp)

RocksDB

![img](https://pic2.zhimg.com/80/v2-3aedc756f1563538f5a9a0e5650e4089_720w.webp)

Nginx

![img](https://pic2.zhimg.com/80/v2-7af62906ca305ce5f288402e451d5561_720w.webp)

MongoDB

![img](https://pic1.zhimg.com/80/v2-1189f7c57f6a0e3d66ce72f8f7af30e4_720w.webp)

## 5.开源框架

skynet

![img](https://pic4.zhimg.com/80/v2-8166765892e111347166f701ce8a952f_720w.webp)

tars

![img](https://pic2.zhimg.com/80/v2-dccab9f71c2f6ab11b6068f8ecf84da5_720w.webp)

dpdk

![img](https://pic1.zhimg.com/80/v2-38c419028e4fa4dd343107845c215db8_720w.webp)

## 6.RUST

Rust特性

![img](https://pic2.zhimg.com/80/v2-b053644adc58afc3aad78dbe5653fe0d_720w.webp)

![img](https://pic4.zhimg.com/80/v2-5491a56a44821417caba923737a040b3_720w.webp)

第三方库

![img](https://pic4.zhimg.com/80/v2-6a94895d87a33eeb0b148d7337e3d73f_720w.webp)

![img](https://pic3.zhimg.com/80/v2-b83ce80f7705c4e893f926d76c1a1716_720w.webp)

rust项目

![img](https://pic4.zhimg.com/80/v2-19120247207bf8dfe9e066f9540543bf_720w.webp)

## 7.性能分析

![img](https://pic4.zhimg.com/80/v2-6d3f10a17016f8856e3c90a65ea4b21b_720w.webp)

火焰图的生成原理与构建方式

![img](https://pic4.zhimg.com/80/v2-689642c783da0633e13617664221d22b_720w.webp)

## 8.分布式架构

分布式消息队列

![img](https://pic2.zhimg.com/80/v2-3ed2a59cb16c6e661edfe85b27938ed1_720w.webp)

分布式服务

![img](https://pic1.zhimg.com/80/v2-b5114cc856ef4780f076d8e46b5bc714_720w.webp)

分布式API网关

![img](https://pic3.zhimg.com/80/v2-0613c653159bf2d09c6697d6ca7273da_720w.webp)

分布存储与容器

![img](https://pic3.zhimg.com/80/v2-3c01702d38d30cf58ec7eb8c860655a6_720w.webp)



除了掌握一定的技术能力之外呢，需要了解和实战的项目也是必不可少的。

这里给大家推荐两个的实战项目：

## 9.图床共享云存储项目

![img](https://pic3.zhimg.com/80/v2-aa2880e9c1ecbc908b21b4c651e7f922_720w.webp)

![img](https://pic1.zhimg.com/80/v2-ccce6da51663608e453c1236f5e24184_720w.webp)

通过项目的实操，能够将自己对于前面八大模块的技术学以致用，将技术落地到项目中去，更好的吸收技术解法。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/503595616

# 【NO.400】WebRTC 源码分析 -- RTC_CHECK

**【1】RTC_DCHECK(1 != 1) << "hello world " << 100 << 3.14；的执行过程**

```text
RTC_DCHECK(1 != 1) << "hello world " << 100 << 3.14；的执行过程
宏展开结果
while (!(1 != 1)) FatalLogCall<false>("main.cc", 7, "1 != 1") 
    & LogStreamer<>() << "hello world " << 100 << 3.14；
注，根据运算符的优先级，<< 运算符比 & 运算符优先级高，所以先算 << 运算符
```

**<< 运算符的运算过程**

- \1. 先计算 LogStreamer<>() << "hello world "，LogStreamer<>() 生成临时对象，临时对象会调用 operator<<() 函数，operator<<() 函数会把 "hello world" 和临时对象作为参数，生成 LogStreamer<T, Ts…> 对象，该对象存储着 "hello world" 和临时对象
- \2. 上一步生成的 LogStreamer<T, Ts…> 对象会继续调用 operator<<() 函数，同时把自己和 100 传入，生成一个新的 LogStreamer<T, Ts…> 对象，从而递归下去，直到所有的 << 运算符处理完毕

![img](https://pic3.zhimg.com/80/v2-4ee47dc77f2d995e55ae78a1c0bb7472_720w.webp)

**& 运算符的运算过程**

- \1. 上一步最后会返回 LogStreamer<T, Ts…> 对象，组成了新的表达式，FatalLogCall<false>(“main.cc”, 7, “1 != 1”) & LogStreamer<T, Ts…>
- FatalLogCall<false>(“main.cc”, 7, “1 != 1”) 会生成临时对象，临时对象会继续调用 operator&() 函数
- \2. 在 operator&() 函数中会使用 LogStreamer<T, Ts…> 对象调用 Call() 函数，会产生递归调用，每次调用都会把本类保存的日志数据往下层传递
- \3. 递归到最后，LogStreamer<>() 生成临时对象会调用 FatalLog() 函数，将所有日志数据打印出来

![img](https://pic1.zhimg.com/80/v2-ea2a1738bb7d9a287be5478d006e5258_720w.webp)

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/485451406

# 【NO.401】RTMP推流及协议学习（全代码）

每日专注分享音视频技术点，ffmpeg，webRTC技术，推流拉流，流媒体服务器，SRS，sfu模型，H264码率，等等技术栈文章视频。

------

## 1.推流工作

### 1.1 整体框架图

[Stream](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3DStream)ing from RTSP -> Get Audio/Video frame -> Convert frame -> RTMP push

使用libtrmp提供的API

lib[rtmp](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3Drtmp)提供了推流的API，可以在rtmp.h文件中查看所有API。我们只需要使用常用的几个API就可以将streaming推送到服务器。
\- RTMP_Init()//初始化结构体
\- RTMP_Free()
\- RTMP_Alloc()
\- RTMP_SetupURL()//设置rtmp server地址
\- RTMP_EnableWrite()//打开可写选项，设定为推流状态
\- RTMP_Connect()//建立NetConnection
\- RTMP_Close()//关闭连接
\- RTMP_ConnectStream()//建立NetStream
\- RTMP_DeleteStream()//删除NetStream
\- RTMP_SendPacket()//发送数据

Start -> RTMP_Init() -> RTMP_Alloc() -> RTMP_Setup() -> RTMP_EnableWrite() -> RTMP_Connect()

-> RTMP_ConnectStream() -> RTMP_SendPacket() -> End

### 1.2 将streaming封装成为RTMP格式

在发送第一帧Audio和Video的时候，需要将Audio和Video的信息封装成为RTMP header，发送给rtmp server。
Audio头有4字节，包含：头部标记0xaf 0x00、 profile、channel、bitrate 信息。
Video头有16字节，包含I[Frame](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3DFrame)、PFrame、AVC标识，除此之外，还需要将sps和pps放在header 里面。
RTMP协议定义了message Type，其中Type ID为8，9的消息分别用于传输音频和视频数据：

```text
#define RTMP_PACKET_TYPE_AUDIO 0x08
#define RTMP_PACKET_TYPE_VIDEO 0x09
```

### 1.3 Audio 格式封装的源码：

AAC header packet:

```text
body = (unsigned char *)malloc(4 + size);
memset(body, 0, 4);
body[0] = 0xaf;
body[1] = 0x00;
 
switch (profile){
 case 0:
    body[2]|=(1<<3);//main
    break;
 case 1:
    body[2]|=(1<<4);//LC
    break;
 case 2:
    body[2]|=(1<<3);//SSR
    body[2]|=(1<<4);
    break;
 default:
    ;
}
switch(this->channel){
 case 1:
    body[3]|=(1<<3);//channel1
    break;
 case 2:
    body[3]|=(1<<4);//channel2
    break;
 default:
    ;
}
switch(this->rate){
 case 48000:
    body[2]|=(1);
    body[3]|=(1<<7);
    break;
 case 44100:
    body[2]|=(1<<1);
    break;
 case 32000:
    body[2]|=(1<<1);
    body[3]|=(1<<7);
    break;
 default:
    ;
}
sendPacket(RTMP_PACKET_TYPE_AUDIO, body, 4, 0);
free(body);
```

### 1.4 Video 格式封装的源码：

H264 header packet:

```text
body = (unsigned char *)malloc(16 + sps_len + pps_len);
this->videoFist = false;
 
memset(body, 0, 16 + sps_len + pps_len);
body[i++] = 0x17;   // 1: IFrame, 7: AVC
                    // AVC Sequence Header
body[i++] = 0x00;
body[i++] = 0x00;
body[i++] = 0x00;
body[i++] = 0x00;
 
// AVCDecoderConfigurationRecord
body[i++] = 0x01;
body[i++] = sps[1];
body[i++] = sps[2];
body[i++] = sps[3];
body[i++] = 0xff;
body[i++] = 0xe1;
body[i++] = (sps_len >> 8) & 0xff;
body[i++] = sps_len & 0xff;
for (size_t j = 0; j < sps_len; j++)
{
    body[i++] = sps[j];
}
body[i++] = 0x01;
body[i++] = (pps_len >> 8) & 0xff;
body[i++] = pps_len & 0xff;
for (size_t j = 0; j < pps_len; j++)
{
    body[i++] = pps[j];
}
sendPacket(RTMP_PACKET_TYPE_VIDEO, body, i, nTimeStamp);
 
free(body);
```

只有第一帧Audio和第一帧video才需要发送header信息。之后就直接发送帧数据。

发送Audio的时候，只需要在数据帧前面加上2 byte的header信息：

```text
spec_info[0] = 0xAF;
spec_info[1] = 0x01;
```

发送Video的时候，需要在header里面标识出I P帧的信息，以及视频帧的长度信息：

```text
body = (unsigned char *)malloc(9 + size);
memset(body, 0, 9);
i = 0;
if (bIsKeyFrame== 0) {
    body[i++] = 0x17;   // 1: IFrame, 7: AVC
}
else {
    body[i++] = 0x27;   // 2: PFrame, 7: AVC
}
// AVCVIDEOPACKET
body[i++] = 0x01;
body[i++] = 0x00;
body[i++] = 0x00;
body[i++] = 0x00;
 
// NALUs
body[i++] = size >> 24 & 0xff;
body[i++] = size >> 16 & 0xff;
body[i++] = size >> 8 & 0xff;
body[i++] = size & 0xff;
memcpy(&body[i], data, size);
```



## 2.进阶

### 2.1 RTMP client与RTMP server交互流程

1 简要介绍

播放一个RTMP协议的流媒体需要经过以下几个步骤：握手，建立网络连接，建立网络流，播放。RTMP连接都是以握手作为开始的。建立连接阶段用于建立客户端与服务器之间的“网络连接”；建立流阶段用于建立客户端与服务器之间的“网络流”；播放阶段用于传输视音频数据。其中，网络连接代表服务器端应用程序和客户端之间基础的连通关系。网络流代表了发送多媒体数据的通道。服务器和客户端之间只能建立一个网络连接，但是基于该连接可以创建很多网络流。他们的关系如图所示：

![img](https://pic4.zhimg.com/80/v2-bd45a654f7b69e2ad420c201b00566a3_720w.webp)

2 握手（HandShake）

一个RTMP连接以握手开始，双方分别发送大小固定的三个数据块

a) 握手开始于客户端发送C0、C1块。服务器收到C0或C1后发送S0和S1。

b) 当客户端收齐S0和S1后，开始发送C2。当服务器收齐C0和C1后，开始发送S2。

c) 当客户端和服务器分别收到S2和C2后，握手完成。

![img](https://pic2.zhimg.com/80/v2-baf261ffdfd5222fc0026e337c6b9041_720w.webp)

3建立网络连接（NetConnection）

a) 客户端发送命令消息中的“连接”(connect)到服务器，请求与一个服务应用实例建立连接。

b) 服务器接收到连接命令消息后，发送确认窗口大小(Window Acknowledgement Size)协议消息到客户端，同时连接到连接命令中提到的应用程序。

c) 服务器发送设置带宽()协议消息到客户端。

d) 客户端处理设置带宽协议消息后，发送确认窗口大小(Window Acknowledgement Size)协议消息到服务器端。

e) 服务器发送用户控制消息中的“流开始”(Stream Begin)消息到客户端。

f) 服务器发送命令消息中的“结果”(_result)，通知客户端连接的状态。

![img](https://pic2.zhimg.com/80/v2-e187603466694e275144cfb3a0b41ffd_720w.webp)

4建立网络流（NetStream）

a) 客户端发送命令消息中的“创建流”（createStream）命令到服务器端。

b) 服务器端接收到“创建流”命令后，发送命令消息中的“结果”(_result)，通知客户端流的状态。

![img](https://pic3.zhimg.com/80/v2-62afea6f5ec5bee2f615c2fca03ab11a_720w.webp)

5 播放（Play）

a) 客户端发送命令消息中的“播放”（play）命令到服务器。

b) 接收到播放命令后，服务器发送设置块大小（ChunkSize）协议消息。

c) 服务器发送用户控制消息中的“streambegin”，告知客户端流ID。

d) 播放命令成功的话，服务器发送命令消息中的“响应状态” NetStream.Play.Start & NetStream.Play.reset，告知客户端“播放”命令执行成功。

e) 在此之后服务器发送客户端要播放的音频和视频数据。

![img](https://pic2.zhimg.com/80/v2-8a810b0cf89ade864c040fac0ce444a1_720w.webp)

## 3.RTMP协议解析

### 3.1 消息

消息是RTMP协议中基本的数据单元。不同种类的消息包含不同的Message Type ID，代表不同的功能。RTMP协议中一共规定了十多种消息类型，分别发挥着不同的作用。例如，Message Type ID在1-7的消息用于协议控制，这些消息一般是RTMP协议自身管理要使用的消息，用户一般情况下无需操作其中的数据。Message Type ID为8，9的消息分别用于传输音频和视频数据。Message Type ID为15-20的消息用于发送AMF编码的命令，负责用户与服务器之间的交互，比如播放，暂停等等。消息首部（Message Header）有四部分组成：标志消息类型的Message Type ID，标志消息长度的Payload Length，标识时间戳的Timestamp，标识消息所属媒体流的Stream ID。消息的报文结构如图3所示。

![img](https://pic2.zhimg.com/80/v2-a3fa1e48e5c7fa4cceda182b94cde491_720w.webp)

消息

### 3.2 消息块

在网络上传输数据时，消息需要被拆分成较小的数据块，才适合在相应的网络环境上传输。RTMP协议中规定，消息在网络上传输时被拆分成消息块（Chunk）。消息块首部（Chunk Header）有三部分组成：用于标识本块的Chunk Basic Header，用于标识本块负载所属消息的Chunk Message Header，以及当时间戳溢出时才出现的Extended Timestamp。消息块的报文结构如图4所示。

![img](https://pic1.zhimg.com/80/v2-c6c72b4fa6186f83d4cb87ef87b50248_720w.webp)

消息块

### 3.3 消息分块

在消息被分割成几个消息块的过程中，消息负载部分（Message Body）被分割成大小固定的数据块（默认是128字节，最后一个数据块可以小于该固定长度），并在其首部加上消息块首部（Chunk Header），就组成了相应的消息块。消息分块过程如图5所示，一个大小为307字节的消息被分割成128字节的消息块（除了最后一个）。

![img](https://pic4.zhimg.com/80/v2-54b6e7abb5c24c3706c02f0ff3305fd7_720w.webp)

RTMP分块

RTMP传输媒体数据的过程中，发送端首先把媒体数据封装成消息，然后把消息分割成消息块，最后将分割后的消息块通过TCP协议发送出去。接收端在通过TCP协议收到数据后，首先把消息块重新组合成消息，然后通过对消息进行解封装处理就可以恢复出媒体数据。

RTMPDump源码分析

握手(HandsShake)

static int HandShake(RTMP * r, int FP9HandShake);

HandShake函数在：/rtmp/rtmplib/handshack.h中。
./rtmp.c:69:#define RTMP_SIG_SIZE 1536

```text
/*client HandShake*/
 695 static int HandShake(RTMP * r, int FP9HandShake){
 709 uint8_t clientbuf[RTMP_SIG_SIZE + 4], *clientsig=clientbuf+4;
 
/*C0 字段已经写入clientsig*/
 721 if (encrypted){  
 722      clientsig[-1] = 0x06; /* 0x08 is RTMPE as well */  
 723      offalg = 1;  
 724 }else  
    //0x03代表RTMP协议的版本（客户端要求的）  
    //数组竟然能有“-1”下标,因为clientsig指向的是clientbuf+4,所以不存在非法地址
    //C0中的字段(1B)  
 725   clientsig[-1] = 0x03;
 /*准备C1字段过程略去，C1字段的数据写入clientsig中, clientsig的大小为1536个字节*/
 
/*1st part of shakehand .......*/
/*C ------- S*/
/*c0 C1-->   */
/*  <-- S0 S1*/
/*C2 -->     */
/*send clientsig C0 和 C1一起发送*/
 814   if (!WriteN(r, (char *)clientsig-1, RTMP_SIG_SIZE + 1))
 815     return FALSE;
 
/*get server response->read type, if get response type not match handshake failed*/
 817   if (ReadN(r, (char *)&type, 1) != 1)  /* 0x03 or 0x06 */
 818     return FALSE; /*encrypt type = 0x06*/
 
/*get server response->read serversig*/
 826 if (ReadN(r, (char *)serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE)
 827     return FALSE;
 
/*如果是加密协议，则需要校验收到的serversig是否和发送的匹配，如果没有加密则直接发送收到的serversig*/
 968   if (!WriteN(r, (char *)reply, RTMP_SIG_SIZE))
 969     return FALSE;
 
/*2nd part of shakehand .....*/
/*C ----- S*/
/*   <-- S2*/
 972   if (ReadN(r, (char *)serversig, RTMP_SIG_SIZE) != RTMP_SIG_SIZE)
 973     return FALSE;
/* compare info between serversig and clientsig*/
 1060  if (memcmp(serversig, clientsig, RTMP_SIG_SIZE) != 0)
/*如果相等，则握手成功*/
}
```

建立链接（NetConnnet）

RTMP_Connect(RTMP *r, RTMPPacket *cp);

建立连接的代码位于：librtmp/rtmp.c中，定义函数：RTMP_Connect()。RTMP_Conncet()里面又分别调用了两个函数：RTMP_Connect0(), RTMP_Connect1()。RTMP_Connect0()主要进行的是socket的连接，RTMP_Connct1()进行的是RTMP相关的连接动作。

```text
1031 int RTMP_Connect(RTMP *r, RTMPPacket *cp)
1032 {
1033   struct sockaddr_in service;
1034   if (!r->Link.hostname.av_len)
1035     return FALSE;
1036 
1037   memset(&service, 0, sizeof(struct sockaddr_in));
1038   service.sin_family = AF_INET;
1039 
1040   if (r->Link.socksport)
1041     {
1042       /* Connect via SOCKS */
1043       if (!add_addr_info(&service, &r->Link.sockshost, r->Link.socksport))
1044   return FALSE;
1045     }
1046   else
1047     {
1048       /* Connect directly */
1049       if (!add_addr_info(&service, &r->Link.hostname, r->Link.port))
1050   return FALSE;
1051     }
1052 
1053   if (!RTMP_Connect0(r, (struct sockaddr *)&service))
1054     return FALSE;
1055 
1056   r->m_bSendCounter = TRUE;
1057 
1058   return RTMP_Connect1(r, cp);
1059 }
```

int RTMP_Connect0(RTMP *r, struct sockaddr* service);

RTMP_Connect0函数分析：

```text
905 int RTMP_Connect0(RTMP *r, struct sockaddr * service){
     /*创建socket*/
 913   r->m_sb.sb_socket = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
     /*通过socket连接到服务器地址*/
 916   if (connect(r->m_sb.sb_socket, service, sizeof(struct sockaddr)) < 0)
     /*如果指定了socket端口到，则进行socks Negotiate*/
 928     if (!SocksNegotiate(r)){}
     /*连接成功之后，返回TRUE*/
 956   return TRUE;
    }
```

int RTMP_Connect1(RTMP *r, RTMPPacket *cp);

RTMP_Connect1函数分析：
根据不同的传输协议，选择传送数据的方式。之后进行HandShake，最后调用SendConnectPacket()送Connect packet

```text
int
RTMP_Connect1(RTMP *r, RTMPPacket *cp)
{
/*if crypto use tls_conncet*/
  if (r->Link.protocol & RTMP_FEATURE_SSL){
#if defined(CRYPTO) && !defined(NO_SSL)
      TLS_client(RTMP_TLS_ctx, r->m_sb.sb_ssl);
      TLS_setfd(r->m_sb.sb_ssl, r->m_sb.sb_socket);
      if (TLS_connect(r->m_sb.sb_ssl) < 0){...}
#else
      return FALSE;
#endif
    }
  /*if no crypto, use http post*/  
  if (r->Link.protocol & RTMP_FEATURE_HTTP){
      HTTP_Post(r, RTMPT_OPEN, "", 1);
      if (HTTP_read(r, 1) != 0){...}
        ...
    }
    /*进行HandShake*/
  if (!HandShake(r, TRUE)){...}
    /*握手成功之后，发送Connect Packet*/
  if (!SendConnectPacket(r, cp)){...}
  return TRUE;
}
```

SendConnectPacket() 里面主要对RTMP信息进行打包，然后调用RTMP_SendPacket函数，将内容发送出去。

```text
static int
SendConnectPacket(RTMP *r, RTMPPacket *cp)
{
  RTMPPacket packet;
  char pbuf[4096], *pend = pbuf + sizeof(pbuf);
  char *enc;
 
  if (cp)
    return RTMP_SendPacket(r, cp, TRUE);
 
  packet.m_nChannel = 0x03; /* control channel (invoke) */
  packet.m_headerType = RTMP_PACKET_SIZE_LARGE;
  packet.m_packetType = RTMP_PACKET_TYPE_INVOKE;
  packet.m_nTimeStamp = 0;
  packet.m_nInfoField2 = 0;
  packet.m_hasAbsTimestamp = 0;
  packet.m_body = pbuf + RTMP_MAX_HEADER_SIZE;
 
  enc = packet.m_body;
  enc = AMF_EncodeString(enc, pend, &av_connect);
  enc = AMF_EncodeNumber(enc, pend, ++r->m_numInvokes);
  *enc++ = AMF_OBJECT;
 
  /*encrypto 部分省略 主要就是调用AMF函数进行*/
  ...
 
  packet.m_nBodySize = enc - packet.m_body;
 
  return RTMP_SendPacket(r, &packet, TRUE);
}
```

建立流（NetStream）

RTMP_ConnectStream()函数主要用于在NetConnection基础上面建立一个NetStream。

int RTMP_ConnectStream(RTMP *r, int seekTime);

```text
int RTMP_ConnectStream(RTMP *r, int seekTime)
{
  RTMPPacket packet = { 0 };
  /* seekTime was already set by SetupStream / SetupURL.
   * This is only needed by ReconnectStream.
   */
  if (seekTime > 0)
    r->Link.seekTime = seekTime;
 
  r->m_mediaChannel = 0;
 
  // 接收到的实际上是块(Chunk)，而不是消息(Message)，因为消息在网上传输的时候要分割成块.
  while (!r->m_bPlaying && RTMP_IsConnected(r) && RTMP_ReadPacket(r, &packet)){
      // 一个消息可能被封装成多个块(Chunk)，只有当所有块读取完才处理这个消息包
      if (RTMPPacket_IsReady(&packet)){
        if (!packet.m_nBodySize)
          continue;
        // 读取到flv数据包，则继续读取下一个包
        if ((packet.m_packetType == RTMP_PACKET_TYPE_AUDIO) ||
            (packet.m_packetType == RTMP_PACKET_TYPE_VIDEO) ||
            (packet.m_packetType == RTMP_PACKET_TYPE_INFO)){
            RTMP_Log(RTMP_LOGWARNING, "Received FLV packet before play()! Ignoring.");
            RTMPPacket_Free(&packet);
            continue;
          }
        RTMP_ClientPacket(r, &packet);// 处理收到的数据包
        RTMPPacket_Free(&packet);// 处理完毕，清除数据
      }
    }
  return r->m_bPlaying;
}
```

简单的一个逻辑判断，重点在while循环里。首先，必须要满足三个条件。其次，进入循环以后只有出错或者建立流（NetStream）完成后，才能退出循环。
有两个重要的函数：

int RTMP_ReadPacket(RTMP *r, RTMPPacket *packet);

块格式：

basic header(1-3字节） chunk msg header(0/3/7/11字节) Extended Timestamp(0/4字节) chunk data
消息格式：

timestamp(3字节) msg length(3字节) msg type id(1字节，小端) msg stream id(4字节)

```text
/**
 * @brief 读取接收到的消息块(Chunk)，存放在packet中. 对接收到的消息不做任何处理。 块的格式为：
 *
 *   | basic header(1-3字节）| chunk msg header(0/3/7/11字节) | Extended Timestamp(0/4字节) | chunk data |
 *
 *   其中 basic header还可以分解为：| fmt(2位) | cs id (3 <= id <= 65599) |
 *   RTMP协议支持65597种流，ID从3-65599。ID 0、1、2作为保留。
 *      id = 0，表示ID的范围是64-319（第二个字节 + 64）；
 *      id = 1，表示ID范围是64-65599（第三个字节*256 + 第二个字节 + 64）；
 *      id = 2，表示低层协议消息。
 *   没有其他的字节来表示流ID。3 -- 63表示完整的流ID。
 *
 *    一个完整的chunk msg header 还可以分解为 ：
 *     | timestamp(3字节) | msg length(3字节) | msg type id(1字节，小端) | msg stream id(4字节) |
 */
int  
RTMP_ReadPacket(RTMP *r, RTMPPacket *packet)  
{
  uint8_t hbuf[RTMP_MAX_HEADER_SIZE] = { 0 };
  // Chunk Header长度最大值为3 + 11 + 4 = 18
  char *header = (char *)hbuf;
  // header指向从socket接收到的数据
  int   nSize, hSize, nToRead, nChunk;
  // nSize是块消息头长度，hSize是块头长度
  int   didAlloc = FALSE;
 
  RTMP_Log(RTMP_LOGDEBUG2, "%s: fd=%d", __FUNCTION__, r->m_sb.sb_socket);
 
  // 读取1个字节存入 hbuf[0]
  if (ReadN(r, (char *)hbuf, 1) == 0)
  {
    RTMP_Log(RTMP_LOGERROR, "%s, failed to read RTMP packet header", __FUNCTION__);
    return FALSE;
  }
 
  packet->m_headerType = (hbuf[0] & 0xc0) >> 6;
  // 块类型fmt
  packet->m_nChannel    = (hbuf[0] & 0x3f);
  // 块流ID（2 - 63）
  header++;
 
  // 块流ID第一个字节为0，表示块流ID占2个字节，表示ID的范围是64-319（第二个字节 + 64）
  if (packet->m_nChannel == 0)
  {
    // 读取接下来的1个字节存放在hbuf[1]中
    if (ReadN(r, (char *)&hbuf[1], 1) != 1)
    {
      RTMP_Log(RTMP_LOGERROR, "%s, failed to read RTMP packet header 2nd byte", __FUNCTION__);
      return FALSE;
    }
 
    // 块流ID = 第二个字节 + 64 = hbuf[1] + 64
    packet->m_nChannel = hbuf[1];
    packet->m_nChannel += 64;
    header++;
  }
  // 块流ID第一个字节为1，表示块流ID占3个字节，表示ID范围是64 -- 65599（第三个字节*256 + 第二个字节 + 64）
  else if (packet->m_nChannel == 1){
    int tmp;
    // 读取2个字节存放在hbuf[1]和hbuf[2]中
    if (ReadN(r, (char *)&hbuf[1], 2) != 2)
    {
      RTMP_Log(RTMP_LOGERROR, "%s, failed to read RTMP packet header 3nd byte", __FUNCTION__);
      return FALSE;
    }
 
    // 块流ID = 第三个字节*256 + 第二个字节 + 64
    tmp = (hbuf[2] << 8) + hbuf[1];
    packet->m_nChannel = tmp + 64;
    RTMP_Log(RTMP_LOGDEBUG, "%s, m_nChannel: %0x", __FUNCTION__, packet->m_nChannel);
    header += 2;
  }
 
  // 块消息头(ChunkMsgHeader)有四种类型，大小分别为11、7、3、0,每个值加1 就得到该数组的值
  // 块头 = BasicHeader(1-3字节) + ChunkMsgHeader + ExtendTimestamp(0或4字节)
  nSize = packetSize[packet->m_headerType];
 
  // 块类型fmt为0的块，在一个块流的开始和时间戳返回的时候必须有这种块
  // 块类型fmt为1、2、3的块使用与先前块相同的数据
  // 关于块类型的定义，可参考官方协议：流的分块 --- 6.1.2节
  if (nSize == RTMP_LARGE_HEADER_SIZE)
  /* if we get a full header the timestamp is absolute */
  {
    packet->m_hasAbsTimestamp = TRUE;
    // 11个字节的完整ChunkMsgHeader的TimeStamp是绝对时间戳
  }else if (nSize < RTMP_LARGE_HEADER_SIZE){
    /* using values from the last message of this channel */
    if (r->m_vecChannelsIn[packet->m_nChannel])
    memcpy(packet, r->m_vecChannelsIn[packet->m_nChannel], sizeof(RTMPPacket));
  }
 
  nSize--;
  // 真实的ChunkMsgHeader的大小，此处减1是因为前面获取包类型的时候多加了1
 
  // 读取nSize个字节存入header
  if (nSize > 0 && ReadN(r, header, nSize) != nSize){
    RTMP_Log(RTMP_LOGERROR, "%s, failed to read RTMP packet header. type: %x", 
     __FUNCTION__, (unsigned int)hbuf[0]);
    return FALSE;
  }
 
  // 目前已经读取的字节数 = chunk msg header + basic header
  hSize = nSize + (header - (char *)hbuf);
  // chunk msg header为11、7、3字节，fmt类型值为0、1、2
  if (nSize >= 3){
    // 首部前3个字节为timestamp
    packet->m_nTimeStamp = AMF_DecodeInt24(header);
 
    /* RTMP_Log(RTMP_LOGDEBUG, "%s, reading RTMP packet chunk on channel %x, 
    headersz %i, timestamp %i, abs timestamp %i", __FUNCTION__, 
    packet.m_nChannel, nSize, packet.m_nTimeStamp, packet.m_hasAbsTimestamp); */
 
    // chunk msg header为11或7字节，fmt类型值为0或1
    if (nSize >= 6)
    {
      packet->m_nBodySize = AMF_DecodeInt24(header + 3);
      packet->m_nBytesRead = 0;
      RTMPPacket_Free(packet);
 
      if (nSize > 6)
      {
        packet->m_packetType = header[6];
        // msg type id
        if (nSize == 11)
          packet->m_nInfoField2 = DecodeInt32LE(header + 7); // msg stream id，小端字节序
      }
    }
 
  // Extend Tiemstamp,占4个字节
    if (packet->m_nTimeStamp == 0xffffff){
      if (ReadN(r, header + nSize, 4) != 4)
      {
        RTMP_Log(RTMP_LOGERROR, "%s, failed to read extended timestamp", __FUNCTION__);
        return FALSE;
      }
      packet->m_nTimeStamp = AMF_DecodeInt32(header + nSize);
      hSize += 4;
    }
  }
 
  RTMP_LogHexString(RTMP_LOGDEBUG2, (uint8_t *)hbuf, hSize);
 
  // 如果消息长度非0，且消息数据缓冲区为空，则为之申请空间
  if (packet->m_nBodySize > 0 && packet->m_body == NULL){
    if (!RTMPPacket_Alloc(packet, packet->m_nBodySize)){
      RTMP_Log(RTMP_LOGDEBUG, "%s, failed to allocate packet", __FUNCTION__);
      return FALSE;
    }
    didAlloc = TRUE;
    packet->m_headerType = (hbuf[0] & 0xc0) >> 6;
  }
 
  // 剩下的消息数据长度如果比块尺寸大，则需要分块,否则块尺寸就等于剩下的消息数据长度
  nToRead = packet->m_nBodySize - packet->m_nBytesRead;
  nChunk = r->m_inChunkSize;
  if (nToRead < nChunk)
  nChunk = nToRead;
 
  /* Does the caller want the raw chunk? */
  if (packet->m_chunk){
    packet->m_chunk->c_headerSize = hSize;
    // 块头大小
    memcpy(packet->m_chunk->c_header, hbuf, hSize);
    // 填充块头数据
    packet->m_chunk->c_chunk = packet->m_body + packet->m_nBytesRead;
    // 块消息数据缓冲区指针
    packet->m_chunk->c_chunkSize = nChunk;
    // 块大小
  }
 
  // 读取一个块大小的数据存入块消息数据缓冲区
  if (ReadN(r, packet->m_body + packet->m_nBytesRead, nChunk) != nChunk){
    RTMP_Log(RTMP_LOGERROR, "%s, failed to read RTMP packet body. len: %u",
     __FUNCTION__, packet->m_nBodySize);
    return FALSE;
  }
 
  RTMP_LogHexString(RTMP_LOGDEBUG2, (uint8_t *)packet->m_body + packet->m_nBytesRead, nChunk);
 
  // 更新已读数据字节个数
  packet->m_nBytesRead += nChunk;
 
  /* keep the packet as ref for other packets on this channel */
  // 将这个包作为通道中其他包的参考
  if (!r->m_vecChannelsIn[packet->m_nChannel])
   r->m_vecChannelsIn[packet->m_nChannel] = malloc(sizeof(RTMPPacket));
  memcpy(r->m_vecChannelsIn[packet->m_nChannel], packet, sizeof(RTMPPacket));
 
  // 包读取完毕
  if (RTMPPacket_IsReady(packet)){
    /* make packet's timestamp absolute，绝对时间戳 = 上一次绝对时间戳 + 时间戳增量 */
    if (!packet->m_hasAbsTimestamp)
      /* timestamps seem to be always relative!! */
      packet->m_nTimeStamp += r->m_channelTimestamp[packet->m_nChannel];
 
    // 当前绝对时间戳保存起来，供下一个包转换时间戳使用
    r->m_channelTimestamp[packet->m_nChannel] = packet->m_nTimeStamp;
 
    /* reset the data from the stored packet.  we keep the header since we may use it later if 
                       a new packet for this channel arrives and requests to re-use some info (small packet header) */
    // 重置保存的包。保留块头数据，因为通道中新到来的包(更短的块头)可能需要使用前面块头的信息.
    r->m_vecChannelsIn[packet->m_nChannel]->m_body = NULL;
    r->m_vecChannelsIn[packet->m_nChannel]->m_nBytesRead = 0;
    r->m_vecChannelsIn[packet->m_nChannel]->m_hasAbsTimestamp = FALSE; // can only be false if we reuse header
  }
  else{
  packet->m_body = NULL;
  /* so it won't be erased on free */
  }
 
  return TRUE;
}
```

int ReadN(RTMP *r, char *buffer, int n);

```text
/**
 * @brief 从HTTP或SOCKET中读取n个数据存放在buffer中.
 */
static int ReadN(RTMP *r, char *buffer, int n)
{
    int  nOriginalSize = n;
    int  avail;
    char *ptr;
 
    r->m_sb.sb_timedout = FALSE;
    #ifdef _DEBUG
    memset(buffer, 0, n);
    #endif
 
    ptr = buffer;
    while (n > 0){
    int nBytes = 0, nRead;
    if (r->Link.protocol & RTMP_FEATURE_HTTP)
                   {
    while (!r->m_resplen)
    {
        if (r->m_sb.sb_size < 144)
        {
            if (!r->m_unackd)
            HTTP_Post(r, RTMPT_IDLE, "", 1);
            if (RTMPSockBuf_Fill(r, &r->m_sb) < 1){
                if (!r->m_sb.sb_timedout)
                RTMP_Close(r);
                return 0;
            }
        }
 
        if (HTTP_read(r, 0) == -1){
            RTMP_Log(RTMP_LOGDEBUG, "%s, No valid HTTP response found", __FUNCTION__);
            RTMP_Close(r);
            return 0;
        }
    }
 
    if (r->m_resplen && !r->m_sb.sb_size)
    RTMPSockBuf_Fill(r, &r->m_sb);
 
    avail = r->m_sb.sb_size;
    if (avail > r->m_resplen)
        avail = r->m_resplen;
    }else{
        avail = r->m_sb.sb_size;
        if (avail == 0){
            if (RTMPSockBuf_Fill(r, &r->m_sb) < 1){
                if (!r->m_sb.sb_timedout)
                    RTMP_Close(r);
                return 0;
            }
            avail = r->m_sb.sb_size;
        }
    }
 
    nRead = ((n < avail) ? n : avail);
    if (nRead > 0){
        memcpy(ptr, r->m_sb.sb_start, nRead);
        r->m_sb.sb_start += nRead;
        r->m_sb.sb_size -= nRead;
        nBytes = nRead;
        r->m_nBytesIn += nRead;
        if (r->m_bSendCounter && r->m_nBytesIn > ( r->m_nBytesInSent + r->m_nClientBW / 10))
        if (!SendBytesReceived(r))
        return FALSE;
    }
    /*RTMP_Log(RTMP_LOGDEBUG, "%s: %d bytes\n", __FUNCTION__, nBytes); */
    //#ifdef _DEBUG
    //      fwrite(ptr, 1, nBytes, netstackdump_read);
    //#endif
 
    if (nBytes == 0){
        RTMP_Log(RTMP_LOGDEBUG, "%s, RTMP socket closed by peer", __FUNCTION__);
        /*goto again; */
        RTMP_Close(r);
        break;
    }
 
    if (r->Link.protocol & RTMP_FEATURE_HTTP){
        r->m_resplen -= nBytes;
        n -= nBytes;
        ptr += nBytes;
    }
 
    return nOriginalSize - n;
}
```

int RTMPSockBuf_Fill(RTMP *r, RTMPSockBuf *sb);

```text
/**
 * @brief 调用Socket编程中的recv()函数，接收数据
 */
int RTMPSockBuf_Fill(RTMP *r, RTMPSockBuf *sb)
{
    int nBytes;
    if  (!sb->sb_size)
        sb->sb_start = sb->sb_buf;
 
    while (1)
    {
        // 缓冲区长度：总长-未处理字节-已处理字节  
        // |-----已处理--------|-----未处理--------|---------缓冲区----------|  
        // sb_buf        sb_start    sb_size  
        nBytes = sizeof(sb->sb_buf) - sb->sb_size - (sb->sb_start - sb->sb_buf);
        {
            // int recv( SOCKET s, char * buf, int len, int flags);  
            // s    ：一个标识已连接套接口的描述字。  
            // buf  ：用于接收数据的缓冲区。   
            // len  ：缓冲区长度。  
            // flags：指定调用方式。  
            // 从sb_start（待处理的下一字节） + sb_size（）还未处理的字节开始buffer为空，可以存储
            nBytes = r->m_sock.recv(sb->sb_socket, sb->sb_start + sb->sb_size, nBytes, 0);
        }
 
        if (nBytes != -1){
            // 未处理的字节又多了
            sb->sb_size += nBytes;
        }else{
            int sockerr = r->m_sock.getsockerr();
            RTMP_Log(RTMP_LOGDEBUG, "%s, recv returned %d. GetSockError(): %d (%s)", 
            __FUNCTION__, nBytes, sockerr, strerror(sockerr));
            if (sockerr == EINTR && !RTMP_ctrlC)
                continue;
 
            if (sockerr == EWOULDBLOCK || sockerr == EAGAIN){
                sb->sb_timedout = TRUE;
                nBytes = 0;
            }
        }
        break;
    }
    return nBytes;
}
```

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/456832095

# 【NO.402】【网络通信 -- 直播】流媒体协议中的时间戳理解与音视频同步,RTP/RTCP/RTMP推流拉流/音视频同步

------

## 1.音视频同步的概念示例

音视频同步即视频同步到音频，视频在渲染的时候，每一帧视频与音频的时间戳对比，以判定是立即渲染还是延迟渲染；

比如有一个音频序列，其时间戳是 A(0，20，40，60，80，100，120 ...)，视频序列 V(0，40，80，120 ... )

音画同步的步骤如下 :

\1) 取一帧音频 A(0) 播放；取一帧视频 V(0)，视频帧时间戳与音频相等，视频立即渲染

\2) 取一帧音频 A(20) 播放；取一帧视频 V(40)，视频帧时间戳大于音频，视频延迟渲染

\3) 取一帧音频 A(40) 播放；取出视频，还是上面的 V(40)，视频帧时间戳与音频相等，视频立即渲染

说明

真实场景中音视频时间戳不一定完全相等，只要音视频时间戳差值的绝对值在一个帧间隔时间内，即可认为是相同的时间戳

------

## 2.RTP 协议中的时间戳与音视频同步

### 2.1 RTP 协议中时间戳相关的基本概念

时间戳单位，时间戳的单位是由采样频率所代替的单位，目的是为了使时间戳单位更为精准；

比如一个音频的采样频率为 8000 Hz，则时间戳单位为 1/8000

时间戳增量，相邻两个 RTP 包之间的时间差 (以时间戳单位为基准)

### 2.2 RTP 包的时间戳

净荷中第一个采样数据的采样时间 (在一次会话开始时的时间戳初值是随机选择的)，每一个采样数据的采样时间通过一个单调且线性递增的时钟获得，时钟频率取决于净荷数据的编码格式，相邻两个 RTP 分组的时间戳差值，取决于前一个 RTP 分组净荷中包含的采样数据数目；

\1. 时间戳就是一个值，用来反映某个数据块的产生(采集)时间点，后采集数据块的时间戳肯定大于先采集数据块的时间戳，从而可以标记数据块的先后顺序

\2. 在实时流传输中，数据采集后立刻传递到 RTP 模块进行发送，那么，数据块的采集时间戳就直接作为 RTP 包的时间戳

\3. 时间戳的单位为采样频率的倒数，例如采样频率为 8000 Hz 时，时间戳的单位为 1 / 8000

\4. 时间戳增量是指两个 RTP 包之间的时间间隔，即发送第二个 RTP 包相距发送第一个 RTP 包时的时间间隔(单位是时间戳单位)

总结

在采用 RTP 协议进行媒体控制传输时，音频和视频作为不同的会话传输，音视频在 RTP 层没有直接的关联，RTP 包头携带的时间戳信息的基点(起始时间)，因此，时间戳的信息只能在同一个音频或视频会话内，确定收到的 RTP 包的时间先后的相对顺序，但该时间戳信息无法确定不同媒体会话之间的时间关系

### 2.3 音视频流间同步实现

发送端在发送音视频数据的同时会发送 SR 包，从而使接收方能够正确使音视频同步播放，在 RTCP 的 SR 包中，包含有 <NTP 时间，RTP 时间戳> 对，音频帧 RTP 时间戳和视频帧 RTP 时间戳通过 <NTP 时间，RTP 时间戳> 对，可以准确定位到绝对时间轴 NTP 上，即可确定音频帧和视频帧的相对时间关系；

计算方式

变量描述

![img](https://pic4.zhimg.com/80/v2-ec1bec101e89bdd58ff2f0487a8b4617_720w.webp)

RTP 时间戳频率计算公式

```text
Audio_Fre = (AudioSRRTPtime2 - AudioSRRTPtime1)/(AudioSRNTPtime2 - AudioSRNTPtime1)
Video_Fre = (VideoSRRTPtime2 - VideoSRRTPtime1)/(VideoSRNTPtime2 - VideoSRNTPtime1)
```

以 AUDIO 为基准，在某个 Audio_SRNTP 时刻，对应视频 RTP 时间计算方式

```text
Video_RTPTime = Video_SRRTPTime + (Audio_SRNTP - Video_SRNTP) × Video_Fre
```

对于任何播放时刻，以 Audio 作为主轴，假设 AUDIO 的播放时间为 PLAY_AUDIO_TIME，视频的播放时间为 PLAY_VIDEO_TIME，计算方式

```text
(PLAY_VIDEO_TIME - Video_RTPTime) / Video_Fre = PLAY_AUDIO_TIME - AudioSRRTPtime) / Audio_Fre
```

------

## 3.RTMP 协议中的时间戳与音视频同步

### 3.1 RTMP Chunk 数据包中的时间戳

Chunk Type(fmt) = 0，timestamp 字段表示绝对时间，(最多能表示到 16777215 = 0xFFFFFF = 2^24 - 1)，当超过最大值时，使用 Extended Timestamp 字段表示绝对时间

Chunk Type(fmt) = 1，timestamp 字段表示与上一个 Chunk 数据的时间差值，(最多能表示到 16777215 = 0xFFFFFF = 2^24 - 1)，当超过最大值时，使用 Extended Timestamp 字段表示与上一个 Chunk 数据的时间差值

Chunk Type(fmt) = 2，timestamp 字段表示与上一个 Chunk 数据的时间差值，(最多能表示到 16777215 = 0xFFFFFF = 2^24 - 1)，当超过最大值时，使用 Extended Timestamp 字段表示与上一个 Chunk 数据的时间差值

Chunk Type(fmt) = 3，其消息头与上一个 Chunk 数据包的消息头完全相同

### 3.2 RTMP 时间戳与 PTS、DTS 的关系

RTMP 数据包时间值与 DTS 的关系

\1. 若当前 RTMP Chunk 数据包 Chunk Type(fmt) = 0，则可以直接解析 timestamp 字段获取当前 RTMP 数据包时间值，该值即为 DTS；

\2. 若当前 RTMP Chunk 数据包 Chunk Type(fmt) = 1 或 2，则根据解析出的 timestamp 字段值加上上一个 RTMP 数据包的时间值，可以获取当前 RTMP 数据包的时间值，该值即为 DTS；

\3. 若 RTMP Chunk 数据包 Chunk Type(fmt) = 3

上一个 RTMP Chunk 数据包 Chunk Type(fmt) = 0，则当前 RTMP 数据包的时间值为上一个 RTMP 数据包时间值，该值即为 DTS

上一个 RTMP Chunk 数据包 Chunk Type(fmt) = 1 或 2，则根据解析出的 timestamp 字段值加上上一个 RTMP 数据包的时间值，可以获取当前 RTMP 数据包的时间值，该值即为 DTS

基于 DTS 确定 PTS

对于 H264 视频数据，FLV 的 Video Tag Data 存在 CTS 字段，该字段表示 DTS 与 PTS 的差值，并有 PTS = DTS + CTS，从而可以获取 PTS 的值

对于 AAC 音频数据，其 DTS 与 PTS 的值一致

------

## 4.RTP 与 RTMP 协议之间的时间戳转换

\1. 根据收到的 sr rtcp 数据包，将音视频的时间戳转成绝对时间，然后取音视频绝对时间的最小值作为基准

后续的时间戳 - 基准时间 -> 得到 rtmp 的时间戳

该同步方式比较准确

\2. 不需要 sr rtcp 数据包，直接取当前时间点的音频时间戳、视频时间戳，分别作为各自的音频、视频时间戳基准

后续的音频时间戳 - 音频基准 -> rtmp 音频时间戳

后续的视频时间戳 - 视频基准 -> rtmp 视频时间戳

该同步方式由于音频和视频的基准不同，当偏差较大的时候则音视频没有那么同步。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/455991811

# 【NO.403】webrtc搭建视频通话、视频会议 （亲测半个小时搭建成功）

## 1. 搭建前置条件

- 首先你需要有一台linux服务器，windows的也可以，请自行搞定
- 一些 简单工具应该先装好
  如：git、make、gcc之类的

## 2. 安装node和npm

下载官网最新nodejs：[https://nodejs.org/en/download/](https://link.zhihu.com/?target=https%3A//nodejs.org/en/download/)

```text
wget https://nodejs.org/dist/v10.16.0/node-v10.16.0-linux-x64.tar.xz
```

安装

```text
# 解压
tar -xvf node-v10.16.0-linux-x64.tar.xz
# 改名
mv node-v10.16.0-linux-x64 nodejs
# 进入目录
cd nodejs/

# 确认一下nodejs下bin目录是否有node和npm文件，如果有就可以执行软连接
sudo ln -s /home/dds/webrtc/nodejs/bin/npm /usr/local/bin/
sudo ln -s /home/dds/webrtc/nodejs/bin/node /usr/local/bin/

**看清楚，上面软链接的路径是你自己创建的路径，我的路径是/home/dds/webrtc/nodejs**

#查看是否安装
node -v 
npm -v 

# 注意，ubuntu 有的是需要sudo,如果不想sudo,可以软链接到当前用户目录
sudo ln -s /home/dds/webrtc/nodejs/bin/node /usr/bin/
```

## 3. coturn穿透和转发服务器

### 3.1 ubuntu安装

ubuntu的话直接用apt安装就行了

```text
sudo apt install coturn 
```

### 3.2 centos安装

centos或者其他的系统根据下面的方式进行安装

安装依赖

```text
Ubuntu, Debian, Mint:		
		$ sudo apt-get install libssl-dev（必须）
		$ sudo apt-get install libsqlite3 (or sqlite3)
		$ sudo apt-get install libsqlite3-dev (or sqlite3-dev)
		$ sudo apt-get install libevent-dev（必须）
		$ sudo apt-get install libpq-dev 
		$ sudo apt-get install mysql-client
		$ sudo apt-get install libmysqlclient-dev
		$ sudo apt-get install libhiredis-dev

Fedora:		
    	$ sudo yum install openssl-devel
		$ sudo yum install sqlite
		$ sudo yum install sqlite-devel
		$ sudo yum install libevent
		$ sudo yum install libevent-devel
		$ sudo yum install postgresql-devel
		$ sudo yum install postgresql-server
		$ sudo yum install mysql-devel
		$ sudo yum install mysql-server
		$ sudo yum install hiredis
		$ sudo yum install hiredis-devel		
```

编译安装coturn

```text
git clone https://github.com/coturn/coturn 
cd coturn 
./configure 
make 
sudo make install
```

### 3.3 配置相关

查看是否安装成功

```text
## 如果能够找到就说明已经安装成功
which turnserver
```

根据自己的安装目录，配置文件`/usr/local/etc/turnserver.conf` 或者`/etc/turnserver.conf`

我的目录是 `/usr/local/etc/turnserver.conf`
配置 如下

```text
verbose
fingerprint
lt-cred-mech
realm=test 
user=ddssingsong:123456
stale-nonce
no-loopback-peers
no-multicast-peers
mobility
no-cli
```

或者下面这个配置，只配置stun（stun-only）

```text
listening-ip=本地ip
listening-port=3478
#relay-ip=0.0.0.0
external-ip=外网ip
min-port=59000
max-port=65000
Verbose
fingerprint
no-stdout-log
syslog
user=ddssingsong:123456
no-tcp
no-tls
no-tcp-relay
stun-only
# 下面是配置证书，不懂就问后端人员怎么用openssl生成这个
cert=pem/turn_server_cert.pem 
pkey=pem/turn_server_pkey.pem 
#secure-stun
```

### 3.4 启动相关

```text
# 如果按照上面的配置直接运行
turnserver

# 如果没有配置上述配置文件，可采用其他运行方法
/usr/local/bin/turnserver --syslog -a -f --min-port=32355 --max-port=65535 --user=dds:123456 -r dds --cert=turn_server_cert.pem --pkey=turn_server_pkey.pem --log-file=stdout -v

--syslog 使用系统日志
-a 长期验证机制
-f 使用指纹
--min-port   起始用的最小端口
--max-port   最大端口号
--user=dds:123456  turn用户名和密码
-r realm组别
--cert PEM格式的证书
--pkey PEM格式的私钥文件
-l, --log-file,<filename> 指定日志文件
-v verbose


#请根据需要选择
```

测试地址，请分别测试stun和turn，relay代表turn

https://[webrtc](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3Dwebrtc).github.io/samples/src/content/peerconnection/trickle-ice/

![img](https://pic4.zhimg.com/80/v2-abfd35c48a156569f2be20d3d8e3832f_720w.webp)

## 4. 安装webrtc服务器和浏览器端

1. 下载 代码

```text
# 代码检出来
git clone https://github.com/ddssingsong/webrtc_server_node.git  
cd webrtc_server
```

2.修改`/public/dist/js/SkyRTC-client.js`，设置穿透服务器

```text
   var iceServer = {
        "iceServers": [
          {
            "url": "stun:stun.l.google.com:19302"
          },
          {
            "url": "stun:118.25.25.147:3478"
          },
          {
             "url": "turn:118.25.25.147:3478",
             "username":"ddssingsong",
             "credential":"123456"
          }
        ]
    };
```

3.修改`/public/dist/js/conn.js`

```text
## 最后一行

##  如果没有配wss代理

rtc.connect("ws:" + window.location.href.substring(window.location.protocol.length).split('#')[0], window.location.hash.slice(1));

如果配了nginx wss代理
rtc.connect("wss:" + window.location.href.substring(window.location.protocol.length).split('#')[0]+"/wss", window.location.hash.slice(1));

# 后面的那个“/wss”是根据自己配的代理路径
```

4.运行

```text
# cd到项目路径

# 安装依赖
npm install

# 运行
node server.js
```

**其实到了这一步就可以测试客户端了，往下看获取线上部署详情**

客户端测试可以不使用nginx配置代理，只需要使用ws即可

## 5. 安装nginx

如果是ubuntu的话还是可以使用apt安装

```text
sudo apt-get install nginx
```

centos按照下面的方式进行

1. 安装依赖

```text
yum install -y gcc gcc-c++ autoconf automake make zlib zlib-devel openssl openssl-devel pcre pcre-devel
```

2.编译安装nginx

```text
wget -C http://nginx.org/download/nginx-1.12.0.tar.gz
tar xvf nginx-1.12.0.tar.gz
cd nginx-1.12.0

./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_module

make 

sudo make install 
```

3.生成证书，这个只是简单的生成，请慎重对待

```text
# 移动到目录，下面会用到
cd /
sudo mkdir cert
ce cert

# 生成服务器证书key
sudo openssl genrsa -out cert.pem 1024

# 生成证书请求,需要你输入信息，一路回车就行，不要输入内容
sudo openssl req -new -key cert.pem -out cert.csr

# 生成crt证书
sudo openssl x509 -req -days 3650 -in cert.csr -signkey cert.pem -out cert.crt
```

4.修改 配置文件`/usr/local/nginx/conf/nginx.conf`或者`/etc/nginx/nginx.conf`,没有的话自己找一下

将下面的内容帖进去就行了

```text
user www-data;
worker_processes auto;
pid /run/nginx.pid;
include /etc/nginx/modules-enabled/*.conf;

events {
	worker_connections 768;
	# multi_accept on;
}

http {
	sendfile on;
	tcp_nopush on;
	tcp_nodelay on;
	keepalive_timeout 65;
	types_hash_max_size 2048;

	include /etc/nginx/mime.types;
	default_type application/octet-stream;


	ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE
	ssl_prefer_server_ciphers on;

	access_log /var/log/nginx/access.log;
	error_log /var/log/nginx/error.log;

	gzip on;

	include /etc/nginx/conf.d/*.conf;
	include /etc/nginx/sites-enabled/*;
	
	 #代理https
	upstream web {
			server 0.0.0.0:3000;      
        }
	#代理websocket
	upstream websocket {
			server 0.0.0.0:3000;   
        }
        
	server { 
		listen       443; 
		server_name  localhost;
		ssl          on;

		ssl_certificate     /cert/cert.crt;#配置证书
		ssl_certificate_key  /cert/cert.key;#配置密钥

		ssl_session_cache    shared:SSL:1m;
		ssl_session_timeout  50m;
		ssl_protocols TLSv1 TLSv1.1 TLSv1.2 SSLv2 SSLv3;
		ssl_ciphers  HIGH:!aNULL:!MD5;
		ssl_prefer_server_ciphers  on;

    
	#wss 反向代理  
	location /wss {
		proxy_pass http://websocket/; # 代理到上面的地址去
		proxy_read_timeout 300s;
		proxy_set_header Host $host;
		proxy_set_header X-Real_IP $remote_addr;
		proxy_set_header X-Forwarded-for $remote_addr;
		proxy_set_header Upgrade $http_upgrade;
		proxy_set_header Connection 'Upgrade';	
  }
	#https 反向代理
	location / {
		proxy_pass         http://web/;
		proxy_set_header   Host             $host;
		proxy_set_header   X-Real-IP        $remote_addr;
		proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
  }
 }
}
```

5.开启nginx

```text
#查看是否开启
ps -ef|grep nginx

#改变配置文件重启nginx
sudo nginx -s reload
```

## 6. 测试浏览器

```text
#访问

https://serverIp#roomName

如：
外网：https://192.168.1.123/#123
内网：http:192.168.1.123:3000#123

# 查看效果，其中roomName为进入的房间名，不同房间的用户无法互相通话
```

## 7. 测试客户端

将这个项目下下来使用 android studio 编译并安装

```text
## 看清楚分支，项目一直在开发中，所以请使用固定分支测试，一般使用branch_nodejs分支测试，master和dev是最新代码
https://github.com/ddssingsong/webrtc_android
```

修改`WebrtcUtil.java`，要去掉界面上的地址哦

```text
    // turn and stun
    // 外网测试才需要
    private static MyIceServer[] iceServers = {
            new MyIceServer("stun:stun.l.google.com:19302"),
            new MyIceServer("118.25.25.147:3478?transport=udp"),
            new MyIceServer("118.25.25.147:3478?transport=udp",
                    "ddssingsong",
                    "123456"),
            new MyIceServer("118.25.25.147:3478?transport=tcp",
                    "ddssingsong",
                    "123456"),

    };

    // 外网测试
    private static String WSS = "wss://47.254.34.146/wss";

    //本地内网信令地址
    private static String WSS = "ws://192.168.1.122:3000";
```

## 8. 到这里，基本完成

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/455942818

# 【NO.404】x264码率控制

问题：在做视频编码时，当我们给定编码器一个目标码率的时候，编码器内部是怎么达到码率要求的那？

概况：关于码率控制有两个目的，第一：兼容传输，播放条件。第二：获取更高的视频质量。

码率控制分为两类：CBR：constant bit rate，固定码率。 VBR：variable bit rate 可变码率。

VBR：可变码率是一类码率控制算法的统称，他们的特点是局部的码率可变的，常用的可变码率子类包括如下：

1：abr：average bit rate，控制整个文件的平均码率。

2：crf：constant refactor ，恒定质量。总码率不可控

3：cqp：constatnt qp，恒定量化参数。关闭一切码率控制算法，与crf的区别在于，crf允许x264对每一帧，每一个宏块进行选取qp，从而产生一个恒定的质量。

对应的x264参数如下：

```text
#define X264_RC_CQP                  0
#define X264_RC_CRF                  1
#define X264_RC_ABR                  2
 
//恒定QP
int         i_qp_constant;  /* 0 to (51 + 6*(x264_bit_depth-8)). 0=lossless */
int         i_qp_min;       /* min allowed QP value */
int         i_qp_max;       /* max allowed QP value */
int         i_qp_step;      /* max QP step between frames */
 
//恒定质量
float       f_rf_constant;  /* 1pass VBR, nominal QP */
float       f_rf_constant_max;  /* In CRF mode, maximum CRF as caused by VBV */
 
//平均码率
 int         i_bitrate;
```

恒定码率CBR：

并不是每个瞬间码率都相同，也不是每一秒码率相同。固定码率指的是固定信道容量。此时就涉及到了VBV（video buffer verifier）视频缓冲区校验器。vbv模型：编码码率通过一个容量受限的信道传输到解码设备，解码设备在解码前有一个缓存，解码器实时从缓存区读取数据解码，保证即不上溢也不下溢(即拿取速度过快或过慢)。

对应参数如下：最终生成的mp4文件可以看出码率为147kbps.，buffer_size的带下取决于容忍的延迟以及播放器的硬件内存限制。

```text
int         i_vbv_max_bitrate;//缓冲区最大填充速度
int         i_vbv_buffer_size;//缓冲区大小.
 
FFmpeg.exe -i q.mp4 -crf 21 -maxrate 150k -bufsize 450k -codec：v：0 libx264 -s 320x240 -r 15 out.
```

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/454882492

# 【NO.405】FFmpeg源码分析：内存管理系统

FFmpeg有专门的内存管理系统，包括：内存分配、内存拷贝、内存释放。其中内存分配包含分配内存与对齐、内存分配与清零、分配指定大小的内存块、重新分配内存块、快速分配内存、分配指定最大值的内存、分配数组内存、快速分配数组内存、重新分配数组内存。

FFmpeg的内存管理位于libavutil/mem.c，相关函数如下图所示：

![img](https://pic4.zhimg.com/80/v2-fad0b0128d7c6776305bf526da87d37b_720w.webp)

## 1.内存分配

### **1.1 av_malloc**

av_malloc()内存分配，并且内存对齐，方便系统快速访问内存。代码如下：

```text
void *av_malloc(size_t size)
{
    void *ptr = NULL;
 
    if (size > max_alloc_size)
        return NULL;
 
#if HAVE_POSIX_MEMALIGN
    if (size)
    if (posix_memalign(&ptr, ALIGN, size))
        ptr = NULL;
#elif HAVE_ALIGNED_MALLOC
    ptr = _aligned_malloc(size, ALIGN);
#elif HAVE_MEMALIGN
#ifndef __DJGPP__
    ptr = memalign(ALIGN, size);
#else
    ptr = memalign(size, ALIGN);
#endif
    /* Why 64?
     * Indeed, we should align it:
     *   on  4 for 386
     *   on 16 for 486
     *   on 32 for 586, PPro - K6-III
     *   on 64 for K7 (maybe for P3 too).
     * Because L1 and L2 caches are aligned on those values.
     * But I don't want to code such logic here!
     */
    /* Why 32?
     * For AVX ASM. SSE / NEON needs only 16.
     * Why not larger? Because I did not see a difference in benchmarks ...
     */
    /* benchmarks with P3
     * memalign(64) + 1          3071, 3051, 3032
     * memalign(64) + 2          3051, 3032, 3041
     * memalign(64) + 4          2911, 2896, 2915
     * memalign(64) + 8          2545, 2554, 2550
     * memalign(64) + 16         2543, 2572, 2563
     * memalign(64) + 32         2546, 2545, 2571
     * memalign(64) + 64         2570, 2533, 2558
     *
     * BTW, malloc seems to do 8-byte alignment by default here.
     */
#else
    ptr = malloc(size);
#endif
    if(!ptr && !size) {
        size = 1;
        ptr= av_malloc(1);
    }
#if CONFIG_MEMORY_POISONING
    if (ptr)
        memset(ptr, FF_MEMORY_POISON, size);
#endif
    return ptr;
}
```

### **1.2 av_mallocz**

av_mallocz()是在av_malloc()基础上，调用memset()进行内存清零：

```text
void *av_mallocz(size_t size)
{
    void *ptr = av_malloc(size);
    if (ptr)
        memset(ptr, 0, size);
    return ptr;
}
```

### **1.3 av_malloc_array**

av_malloc_array()先计算数组所需要内存块大小，然后用av_malloc()分配数组内存：

```text
void *av_malloc_array(size_t nmemb, size_t size)
{
    size_t result;
    if (av_size_mult(nmemb, size, &result) < 0)
        return NULL;
    return av_malloc(result);
}
```

### **1.4 av_mallocz_array**

av_mallocz_array()先计算数组所需要内存块大小，然后用av_mallocz()分配数组内存：

```text
void *av_mallocz_array(size_t nmemb, size_t size)
{
    size_t result;
    if (av_size_mult(nmemb, size, &result) < 0)
        return NULL;
    return av_mallocz(result);
}
```

### **1.5 av_calloc**

av_calloc()操作与av_mallocz_array()，先计算内存大小再用av_mallocz()分配内存：

```text
void *av_calloc(size_t nmemb, size_t size)
{
    size_t result;
    if (av_size_mult(nmemb, size, &result) < 0)
        return NULL;
    return av_mallocz(result);
}
```

### **1.6 av_max_alloc**

av_max_alloc()主要是指定分配内存的最大值：

```text
static size_t max_alloc_size= INT_MAX;
 
void av_max_alloc(size_t max)
{
    max_alloc_size = max;
}
```

在av_malloc()用于判断size是否超出最大值：

```text
void *av_malloc(size_t size)
{
    void *ptr = NULL;
 
    if (size > max_alloc_size)
        return NULL;
 
    ......
}
```

### **1.7 av_realloc**

av_realloc()是对系统的realloc函数进行封装，重新分配内存块：

```text
void *av_realloc(void *ptr, size_t size)
{
    if (size > max_alloc_size)
        return NULL;
 
#if HAVE_ALIGNED_MALLOC
    return _aligned_realloc(ptr, size + !size, ALIGN);
#else
    return realloc(ptr, size + !size);
#endif
}
```

### **1.8 av_realloc_array**

av_realloc_array()先计算内存块大小，然后用av_realloc()重新分配数组内存：

```text
void *av_realloc_array(void *ptr, size_t nmemb, size_t size)
{
    size_t result;
    if (av_size_mult(nmemb, size, &result) < 0)
        return NULL;
    return av_realloc(ptr, result);
}
```

### **1.9 av_fast_realloc**

av_fast_realloc()快速重新分配内存，如果原始内存块足够大直接复用：

```text
void *av_fast_realloc(void *ptr, unsigned int *size, size_t min_size)
{
    if (min_size <= *size)
        return ptr;
 
    if (min_size > max_alloc_size) {
        *size = 0;
        return NULL;
    }
 
    min_size = FFMIN(max_alloc_size, FFMAX(min_size + min_size / 16 + 32, min_size));
 
    ptr = av_realloc(ptr, min_size);
    /* we could set this to the unmodified min_size but this is safer
     * if the user lost the ptr and uses NULL now
     */
    if (!ptr)
        min_size = 0;
 
    *size = min_size;
 
    return ptr;
}
```

### **1.10 av_fast_malloc**

av_fast_malloc()快速分配内存：

```text
void av_fast_malloc(void *ptr, unsigned int *size, size_t min_size)
{
    ff_fast_malloc(ptr, size, min_size, 0);
}
```

其中ff_fast_malloc()代码位于libavutil/mem_internal.h:

```text
static inline int ff_fast_malloc(void *ptr, unsigned int *size, size_t min_size, int zero_realloc)
{
    void *val;
 
    memcpy(&val, ptr, sizeof(val));
    if (min_size <= *size) {
        av_assert0(val || !min_size);
        return 0;
    }
    min_size = FFMAX(min_size + min_size / 16 + 32, min_size);
    av_freep(ptr);
    val = zero_realloc ? av_mallocz(min_size) : av_malloc(min_size);
    memcpy(ptr, &val, sizeof(val));
    if (!val)
        min_size = 0;
    *size = min_size;
    return 1;
}
```

### **1.11 av_fast_mallocz**

av_fast_mallocz()快速分配内存，并且内存清零：

```text
void av_fast_mallocz(void *ptr, unsigned int *size, size_t min_size)
{
    ff_fast_malloc(ptr, size, min_size, 1);
}
```

## 2.内存拷贝

### **2.1 av_strdup**

av_strdup()用于重新分配内存与拷贝字符串：

```text
char *av_strdup(const char *s)
{
    char *ptr = NULL;
    if (s) {
        size_t len = strlen(s) + 1;
        ptr = av_realloc(NULL, len);
        if (ptr)
            memcpy(ptr, s, len);
    }
    return ptr;
}
```

### **2.2 av_strndup**

av_strndup()用于分配指定大小内存与拷贝字符串，先用memchr()获取有效字符串长度，然后使用av_realloc()重新分配内存，再用memcpy()拷贝字符串：

```text
char *av_strndup(const char *s, size_t len)
{
    char *ret = NULL, *end;
 
    if (!s)
        return NULL;
 
    end = memchr(s, 0, len);
    if (end)
        len = end - s;
 
    ret = av_realloc(NULL, len + 1);
    if (!ret)
        return NULL;
 
    memcpy(ret, s, len);
    ret[len] = 0;
    return ret;
}
```

### **2.3 av_memdup**

av_memdup()用于内存分配与内存拷贝，先用av_malloc()分配内存，再用memcpy()拷贝内存：

```text
void *av_memdup(const void *p, size_t size)
{
    void *ptr = NULL;
    if (p) {
        ptr = av_malloc(size);
        if (ptr)
            memcpy(ptr, p, size);
    }
    return ptr;
}
```

### **2.4 av_memcpy_backptr**

av_memcpy_backptr()用于内存拷贝，与系统提供的memcpy()类似，并且考虑16位、24位、32位内存对齐：

```text
void av_memcpy_backptr(uint8_t *dst, int back, int cnt)
{
    const uint8_t *src = &dst[-back];
    if (!back)
        return;
 
    if (back == 1) {
        memset(dst, *src, cnt);
    } else if (back == 2) {
        fill16(dst, cnt);
    } else if (back == 3) {
        fill24(dst, cnt);
    } else if (back == 4) {
        fill32(dst, cnt);
    } else {
        if (cnt >= 16) {
            int blocklen = back;
            while (cnt > blocklen) {
                memcpy(dst, src, blocklen);
                dst       += blocklen;
                cnt       -= blocklen;
                blocklen <<= 1;
            }
            memcpy(dst, src, cnt);
            return;
        }
        if (cnt >= 8) {
            AV_COPY32U(dst,     src);
            AV_COPY32U(dst + 4, src + 4);
            src += 8;
            dst += 8;
            cnt -= 8;
        }
        if (cnt >= 4) {
            AV_COPY32U(dst, src);
            src += 4;
            dst += 4;
            cnt -= 4;
        }
        if (cnt >= 2) {
            AV_COPY16U(dst, src);
            src += 2;
            dst += 2;
            cnt -= 2;
        }
        if (cnt)
            *dst = *src;
    }
}
```

## 3.内存释放

### **3.1 av_free**

av_free()用于释放内存块，主要是调用系统free()进行释放。如果宏定义了对齐分配，那么要对齐释放：

```text
void av_free(void *ptr)
{
#if HAVE_ALIGNED_MALLOC
    _aligned_free(ptr);
#else
    free(ptr);
#endif
}
```

### **3.2 av_freep**

av_freep()用于释放内存指针，先备份内存指针，然后把指针地址清空，再释放内存：

```text
void av_freep(void *arg)
{
    void *val;
 
    memcpy(&val, arg, sizeof(val));
    memcpy(arg, &(void *){ NULL }, sizeof(val));
    av_free(val);
}
```

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/450437498

# 【NO.406】WebRTC 传输安全机制第二话：深入显出 SRTP 协议

## 1.**要解决的问题**

`RTP/RTCP` 协议并没有对它的负载数据进行任何保护。因此，如果攻击者通过抓包工具，如 Wireshark，将音视频数据抓取到后，通过该工具就可以直接将音视频流播放出来，这是非常恐怖的事情。

在 WebRTC 中，为了防止这类事情发生，没有直接使用 `RTP/RTCP` 协议，而是使用了 `SRTP/SRTCP` 协议 ，即安全的 `RTP/RTCP` 协议。WebRTC 使用了非常有名的 libsrtp 库将原来的 `RTP/RTCP` 协议数据转换成 `SRTP/SRTCP` 协议数据。

**[SRTP](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/u7eStMiCGxNyEWsYkG8UYg) 要解决的问题：**

• 对 `RTP/RTCP` 的负载 (payload) 进行加密，保证数据安全；

• 保证 `RTP/RTCP` 包的完整性，同时防重放攻击。

------

## 2.**SRTP/SRTCP结构**

SRTP 结构

![img](https://pic2.zhimg.com/80/v2-e873f532d97e3d969b5be37a01888435_720w.webp)

从 SRTP 结构图中可以看到：

\1. 加密部分 `Encrypted Portion`，由 `payload`, `RTP padding`和`RTP pad count` 部分组成。也就是我们通常所说的仅对 RTP 负载数据加密。

\2. 需要校验部分 `Authenticated Portion`，由 `RTP Header`, `RTP Header extension` 和 `Encrypted Portion` 部分组成。

通常情况下只需要对 RTP 负载数据进行加密，如果需要对 RTP header extension 进行加密，[RFC6904](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/u7eStMiCGxNyEWsYkG8UYg) 给出了详细方案，在 libsrtp 中也完成了实现。

**SRTCP 结构**

![img](https://pic4.zhimg.com/80/v2-3892e2d974e4a830a6c2f741f7b5eda3_720w.webp)

从`SRTCP` 结构图中可以看到：1. 加密部分`Encrypted Portion`，为`RTCP Header`之后的部分，对`Compound RTCP`也是同样。

\2. E-flag 显式给出了 RTCP 包是否加密。（PS：一个 RTP 包怎么判断是加密的？）

3.`SRTCP index`显示给出了 RTCP 包的序列号，用来防重放攻击。（PS：一个 RTP 包的 16-bits 的序列号可以防重放攻击吗？）

\4. 待校验部分`Authenticated Portion`，由`RTCP Header`和`Encrypted Portion`部分组成。

在初步认识了 `SRTP` 和 `SRTCP`的结构后，接下来介绍`Encrypted Portion` 和`Authenticated Portion`如何得到了。

------

## 3.**Key管理**

在 `SRTP/SRTCP` 协议中，使用二元组 `<SRTP目的IP地址，SRTP/SRTCP目的端口>` 的方式来标识一个通信参与者的 `SRTP/SRTCP` 会话，称为 `SRTP/SRTCP Session`。

在 `SRTP` 协议中使用三元组 `<SSRC, RTP/RTCP目的地址，RTP/RTCP目的端口>` 来标识一个 stream，一个 `SRTP/SRTCP Session` 由多个 stream 组成。对每个 stream 的加解密相关参数的描述，称为 `Cryptographic Context`。

每个 stream 的 `Cryptographic Context`中 中的包含如下参数：

• SSRC: Stream 使用的 SSRC。

• Cipher Parameter：加解密使用的 key, salt，算法描述 (类型，参数等)。

• Authentication Parameter: 完整性使用的 Key, salt，算法描述 (类型，参数等)。

• Anti-Replay Data: 防止重放攻击缓存的数据信息，例如，ROC，最大序号等。

在 `SRTP/SRTCP Session` 中，每个 Stream 都会使用到属于自己的，加解密 Key，Authentication Key。这些 Key 都是在同一个 Session 中使用到的，称为 `Session Key`。这些 `Session Key` 是通过对 `Master Key` 使用 KDF(Key Derivation Function) 导出的。

`KDF` 是用于导出 `Session Key` 函数，KDF 默认使用是加解密函数。例如，在完成 DTLS 后，协商得到的 SRTP 加密算法的 Profile 为：

```text
SRTP_AES128_CM_HMAC_SHA1_80
         cipher: AES_128_CM
         cipher_key_length: 128
         cipher_salt_length: 112
         maximum_lifetime: 2^31
         auth_function: HMAC-SHA1
         auth_key_length: 160
         auth_tag_length: 80
```

对应的`KDF` 为 `AES128_CM`。`Session Key` 的导出流程如下图所示：

![img](https://pic1.zhimg.com/80/v2-ac35d7513512fc926df1094a3e917934_720w.webp)

`Session Key`的导出依赖于如下参数：

•`key_label`: 根据导出的 Key 的类型不同，`key_label`取值如下：

![img](https://pic4.zhimg.com/80/v2-6395cf2b24bb55d74182c9294cc05a8f_720w.webp)

• master_key: DTLS 完成后，协商得到的 Key。

• master_salt: DTLS 完成后，协商得到的 Salt。

• packet_index: RTP/RTCP 的包序号。SRTP 使用 48-bits 的隐式包需要，SRTCP 使用 31-bits 包序号。参考[序号管理](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/u7eStMiCGxNyEWsYkG8UYg)。

• key_derivation_rate: key 导出速率 , 记为 kdr。默认取值为 0，执行 1 次 Key 导出。取值范围 `{{1,2,4,...,2^24}`。在 `key_derivation_rate>0` 的情况下，在加密之前，执行一次 key 导出，后续在 packet_index/key_derivation_rate > 0 时，执行 key 导出。

```text
r = packet_index / kdr
key_id = label || r
x = key_id XOR master_salt
key = KDF(master_key, x)
```

> '/'：表示整除，B=0时，C = A/B=0。

`||`：表示连接的含义。A,B,C使用网络字节序表示，C = A||B, 则C的高字节为A，低字节位为B。

`XOR`：是异或运算，计算时按照低字节位对齐。

以下使用`AES128_CM`，举例说明`Session Key`的导出过程，假设`DTLS`协商得到：

```text
master_key:  E1F97A0D3E018BE0D64FA32C06DE4139   // 128-bits
master_salt: 0EC675AD498AFEEBB6960B3AABE6           // 112-bits
```

导出加密 Key(cipher key):

```text
packet_index/kdr:              000000000000
label:                       00
master_salt:   0EC675AD498AFEEBB6960B3AABE6
-----------------------------------------------
xor:           0EC675AD498AFEEBB6960B3AABE6     (x, KDF input)
x*2^16:        0EC675AD498AFEEBB6960B3AABE60000 (AES-CM input)
cipher key:    C61E7A93744F39EE10734AFE3FF7A087 (AES-CM output)
```

导出 SALT Key(cipher salt):

```text
packet_index/kdr:              000000000000
label:                       02
master_salt:   0EC675AD498AFEEBB6960B3AABE6
----------------------------------------------
xor:           0EC675AD498AFEE9B6960B3AABE6     (x, KDF input)
x*2^16:        0EC675AD498AFEE9B6960B3AABE60000 (AES-CM input)
               30CBBC08863D8C85D49DB34A9AE17AC6 (AES-CM ouptut)
cipher salt:   30CBBC08863D8C85D49DB34A9AE1
```

导出校验 Key(auth key)，需要 `auth key`长度为 94 字节：

```text
packet_index/kdr:                000000000000
label:                         01
master salt:     0EC675AD498AFEEBB6960B3AABE6
-----------------------------------------------
xor:             0EC675AD498AFEEAB6960B3AABE6     (x, KDF input)
x*2^16:          0EC675AD498AFEEAB6960B3AABE60000 (AES-CM input)

auth key                           AES input blocks
CEBE321F6FF7716B6FD4AB49AF256A15   0EC675AD498AFEEAB6960B3AABE60000
6D38BAA48F0A0ACF3C34E2359E6CDBCE   0EC675AD498AFEEAB6960B3AABE60001
E049646C43D9327AD175578EF7227098   0EC675AD498AFEEAB6960B3AABE60002
6371C10C9A369AC2F94A8C5FBCDDDC25   0EC675AD498AFEEAB6960B3AABE60003
6D6E919A48B610EF17C2041E47403576   0EC675AD498AFEEAB6960B3AABE60004
6B68642C59BBFC2F34DB60DBDFB2       0EC675AD498AFEEAB6960B3AABE60005
```

至此，我们得到了`SRTP/SRTCP`加密和认证需要的`Session Key`：cipher key，auth key，salt key。

------

## 4.**序列号管理**

**SRTP 序列号管理**

在 `RTP` 包结构定义中使用 `16-bit` 来描述序列号。考虑到防重放攻击，消息完整性校验，加密数据，导出 SessionKey 的需要，在 `SRTP` 协议中，SRTP 包的序列号，使用隐式方式来记录包序列号 `packet_index`，使用 i 标识 packet_index。

对于发送端来说，i 的计算方式如下：

```text
i = 2^16 * ROC + SEQ
```

其中，SEQ 是 RTP 包中描述的 16-bit 包序号。ROC(rollover couter) 是 RTP 包序号 (SEQ) 翻转计数，也就是每当 `SEQ/2^16=0`, ROC 计数加 1。ROC 初始值为 0。

对于接收端来说，考虑到丢包和乱序因素的影响，除了维护 `ROC`，还需要维护一个当前收到的最大包序号 `s_l`，当一个新的包到来时候，接收端需要估计出当前包所对应的实际 SRTP 包的序号。ROC 的初始值为 0，s_l 的初始值为收到第一个 SRTP 包的 SEQ。后续通过如下公式，估计接收到的 SRTP 序号 i：

```text
i = 2^16 * v + SEQ
```

其中，`v` 可能的取值 `{ ROC-1, ROC, ROC+1 }`，ROC 是接收端本地维护的 ROC，SEQ 是收到 SRTP 的序号。v 分别取 ROC-1，ROC，ROC+1 计算出 i，与 `2^16*ROC + s_l` 进行比较，那个更接近，v 就取对应的值。完成 SRTP 解密和完整性校验后，更新 ROC 和 s_l，分如下 3 种情况：



\1. v = ROC - 1， ROC 和 s_l 不更新。

\2. v = ROC，如果 SEQ > s_1，则更新 s_l = SEQ。

\3. v = ROC + 1, ROC = v = ROC + 1，s_l = SEQ。

更直观的代码描述：

```text
if (s_l < 32768)
    if (SEQ - s_l > 32768)
        set v to (ROC-1) mod 2^32
    else
        set v to ROC
    endif
else
    if (s_l - 32768 > SEQ)
        set v to (ROC+1) mod 2^32
    else
        set v to ROC
    endif
endif
return SEQ + v*65536
```

**SRTCP 序列号管理**

`RTCP` 中没有描述序号的字段，`SRTCP` 的序号在 SRTCP 包，使用 `31-bits` 中显示描述，详见[SRTCP格式](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/u7eStMiCGxNyEWsYkG8UYg)，也就是说在 SRTCP 的最大序列号为 2^31。



**序列号与通信时长**

可以看到 SRTP 的序列号最大值为 2^48, SRTCP 的序列号最大值为 2^16。在大多数应用中（假设每 128000 个 RTP 数据包至少有一个 RTCP 数据包），SRTCP 序号将首先达到上限。以 200 SRTCP 数据包 / 秒的速度， SRTCP 的 2^31 序列号空间足以确保大约 4 个月的通信。

------

## 5.**防重放攻击**

攻击者将截获的 SRTP/SRTCP 包保存下来，然后重新发送到网络中，实现了包的重放。SRTP 接收者通过维护一个重放列表 (ReplayList) 来防止这种攻击。理论上 Replay List 应该保存所有接收到并完成校验的包的序列号 index。在实际情况下 ReplayList 使用滑动窗口（sliding window）来实现防重放攻击。使用 `SRTP-WINDOW-SIZE` 来描述滑动窗口的大小。



**SRTP 防重放攻击**

在序列号管理部分，我们详述了接收者，根据接收到的 SRTP 包的 SEQ，ROC，s_l 估算出 SRTP 包的 `packet_index` 的方法。同时，将接收者已经接收到 SRTP 包的最大序列号，记为 `local_packet_index`。计算差值 `delta`：

```text
delta =  packet_index - local_packet_index
```

分如下 3 种情况说明：

\1. delta > 0：表示收到了新的包。

\2. delta < -(SRTP-WINDOW-SIZE - 1) < 0：表示收到的包的序列号，小于重放窗口要求的最小序号。libSRTP 收到这样的包时，会返回 `srtp_err_status_replay_old=10`, 表示收到旧的重放包。

\3. delta < 0, delta >= -(SRTP-WINDOW-SIZE - 1): 表示收到了重放窗口之内的包。如果在 ReplayList 找到对应的包，则是一个 index 重复的重放包。libSRTP 收到这样的包时，会返回 `srtp_err_status_replay_fail=9`。否则表示收到一个乱序包。

下图更加直观说明防重放攻击的三个区域：

![img](https://pic3.zhimg.com/80/v2-f3cd78475cae56582dc9f07dd61fd31a_720w.webp)

> SRTP-WINDOW-SIZE 的取值，最小是 64。应用可以根据需要设置成较大的值，libsrtp 会向上取整为 32 的整数倍。例如，在 WebRTC 中`SRTP-WINDOW-SIZE`= 1024。使用者可以根据需要进行调整，但要达到防重放攻击的目的。

**SRTCP 防重放攻击**

在 SRTCP 中，packet index 显式给出。在 libsrtp 中，SRTCP 的防重放攻击的窗口大小为 128。使用 `window_start`记录防重放攻击的起始序列号。SRTCP 防重放攻击的检查步骤如下：

\1. index > window_start + 128: 收到新的 SRTCP 包。

\2. index < window_start: 收到包的序列号在重放窗口的左侧，可以认为我们收到了比较老的包。libsrtp 收到这样的包之后，会返回到 `srtp_err_status_replay_old=10`。

\3. replay_list_index = index - windwo_start：在 ReplayList 中 replay_list_index 对应的标识位为 1，表示已经收到包，libsrtp 返回`srtp_err_status_replay_fail=9`。对应的标识位为 0，表示收到乱序包。

------

## 6.**加密和校验算法**

在 SRTP 中，使用了 CTR（Counter mode）模式的 AES 加密算法，CTR 模式通过递增一个加密计数器以产生连续的密钥流，计数器可以是任意保证长时间不产生重复输出的密钥。根据计数方式的不同，分为以下两种类型：

• `AES-ICM`: ICM 模式（Integer Counter Mode，整数计数模式），使用整数计数运算。

• `AES-GCM`: GCM 模式（Galois Counter Mode，基于伽罗瓦域计数模式），计数运算定义在伽罗瓦域。

在 SRTP 中，使用 `AES-ICM` 完成加密算法，同时使用 `HMAC-SHA1` 完成 `MAC` 计算，对数据进行完整性校验，加密和 MAC 计算需要分两步完成。`AES-GCM` 基于 AEAD（Authenticated-Encryption with Associated-Data，关联数据的认证加密）的思想，在对数据进行加密的同时计算 `MAC` 值，实现了一个步骤，完成加密和校验信息的计算。下面分别对这个 `AES-ICM` 和 `AES_GSM` 的用法进行介绍。

**AEC—ICM**

![img](https://pic4.zhimg.com/80/v2-e4623087c3221ec1513345d65ff6d99f_720w.webp)

上图描述了`AES-ICM`的加密和解密过程，图中的 K 是通过 KDF 导出的`SessionKey`。加密和加密都是通过对 Counter 进行加密，与明文 P 异或运算得到加密数据 C，反之，与密文 C 异或运算得到明文数据 P。考虑到安全性，Counter 生成依赖于`Session Salt`, 包的索引（packet index）和包的 SSRC。Counter 是 128-bits 的计数，生成方式如下定义：

```text
one byte
<-->
0  1  2  3  4  5  6  7  8  9  10 11 12 13 14 15
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
|00|00|00|00|   SSRC    |   packet index  | b_c |---+
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+   |
                                                    |
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+   v
|                  salt (k_s)             |00|00|->(+)
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+   |
                                                    |
                                                    v
                                            +-------------+
                    encryption key (k_e) -> | AES encrypt |
                                            +-------------+
                                                    |
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+   |
|                keystream block                |<--+
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
```

**HMAC—SHA1**

散列消息认证码（Hash-based message authentication code，缩写为 HMAC），是一种通过特别计算方式之后产生的消息认证码（MAC），使用密码散列函数，同时结合一个加密密钥，它可以用来保证数据的完整性，同时可以用来作某个消息的身份验证。HMAC 通过一个标准算法，在计算哈希的过程中，把 key 混入计算过程中。HMAC 的加密实现如下：

```text
HMAC(K,M) = H ( (K XOR opad ) + H( (K XOR ipad ) + M ) )
```

• H：hash 算法，比如，MD5，SHA-1，SHA-256。

• B：块字节的长度，块是 hash 操作的基本单位。这里 B=64。

• L：hash 算法计算出来的字节长度。(L=16 for MD5, L=20 for SHA-1)。

• K：共享密钥，K 的长度可以是任意的，但是为了安全考虑，还是推荐 K 的长度>B。

当 K 长度大于 B 时候，会先在 K 上面执行 hash 算法，将得到的 L 长度结果作为新的共享密钥。如果 K 的长度 <B, 那么会在 K 后面填充 0x00 一直到等于长度 B。

• M：要认证的内容。

• opad：外部填充常量，是 0x5C 重复 B 次。

• ipad：内部填充常量，是 0x36 重复 B 次。

• XOR：异或运算。

• +：代表 " 连接 " 运算。

计算步骤如下：

\1. 将 0x00 填充到 K 的后面，直到其长度等于 B。

\2. 将步骤 1 的结果跟 ipad 做异或。

\3. 将要加密的信息附在步骤 2 的结果后面。

\4. 调用 H 方法。

\5. 将步骤 1 的结果跟 opad 做异或。

\6. 将步骤 4 的结果附在步骤 5 的结果后面。

\7. 调用 H 方法。

`SRTP` 和 `SRTCP` 计算 `Authentication tag`，使用的 `K` 对应 Key 管理部分描述的 `RTP auth key` 和 `RTCP auth key`，使用的 Hash 算法为 `SHA-1`，`Authentication tag` 的长度为 80-bits。

在计算 SRTP 的，要认证的内容 M 为：

```text
M = Authenticated Portion + ROC
```

其中，`+` 代表 " 连接 " 运算，`Authenticated Portion` 在 `SRTP` 的结构图中给出。

在计算 `SRTCP` 时，要认证的内容 M 为：

```text
M=Authenticated Portion
```

其中，`Authenticated Portion` 在 `SRTCP` 的结构图中给出。

通过使用 `Authenticated Portion` 算法，计算得到 SRTP/SRTCP 的 `Encrypted Portion Portion` 部分。

**AES—GCM**

`AES-GCM` 使用计数器模式来加密数据，该操作可以有效地流水线化，GCM 身份验证使用的操作特别适合于硬件中的有效实现。在 [GCM-SPEC ](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/u7eStMiCGxNyEWsYkG8UYg)详述了 GCM 的理论知识， [Section4.2 Hardware ](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/u7eStMiCGxNyEWsYkG8UYg)详述了硬件实现。

`AES-GCM` 在 `SRTP` 加密中的应用，在[RFC7714](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/u7eStMiCGxNyEWsYkG8UYg) 进行了详细描述。Key 管理和序列号管理与本文中描述的相同，需要注意的是：

\1. `AES-GCM` 作为一种 AEAD（Authenticated Encryption with Associated Data）加密算法，输入和输出是什么，对应到 `SRTP/SRTCP` 的包结构中理解。



\2. `Counter` 的是计算方式和 AES-ICM 中描述的计算方式不同，需要重点关注。

[libsrtp](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/u7eStMiCGxNyEWsYkG8UYg) 已经实现了 `AES-GCM`，有兴趣的同学，可以结合代码进行研读。

------

## 7.**libsrtp的使用**

[libsrtp](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/u7eStMiCGxNyEWsYkG8UYg) 是被广泛使用的 SRTP/SRTCP 加密的开源项目。经常用到的 api 如下：

\1. `srtp_init`，初始化 srtp 库，初始化内部加密算法，在使用 srtp 前，必须要调用了。

\2. `srtp_create`, 创建 srtp_session，可以结合本文中介绍的 session，session key 等概念一起理解。

\3. `srtp_unprotect/srtp_protect`，RTP 包加解密接口。

\4. `srtp_protect_rtcp/srtp_unprotect_rtcp`，RTCP 包的加解密接口。

\5. `srtp_set_stream_roc/srtp_get_stream_roc`, 设置和获取 stream 的 ROC，这两个接口在最新的 2.3 版本加入。

重要的结构 `srtp_policy_t`，用来初始化加解密参数，在 `srtp_create` 中使用这个结构。以下参数需要关注：

\1. DTLS 协商后得到的 `MasterKey` 和 `MasterSalt` 通过这个结构传递给 libsrtp，用于 session key 的生成。

\2. `window_size`，对应我们之前描述的 srtp 防重放攻击的窗口大小。

\3. `allow_repeat_tx`，是否允许重传相同序号的包。

[SRS ](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/u7eStMiCGxNyEWsYkG8UYg)是一个新生代实时通信服务器，对 libsrtp 感兴趣的同学，可以快速在本机搭起调试环境，进行相关测试，更加深入理解相关的算法。

------

## 8.**总结**

本文通过对 `SRTP/SRTCP` 相关原理的深入详细解读，对 libsrtp 使用遇到的问题进行解答，希望能够给实时音视频通信的相关领域的同学以帮助。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/449453756

# 【NO.407】WebRTC能给我带来什么？

WebRTC现在已经成为了W3C的正式标准，提供具有NAT遍历功能的次秒级的点对点视频和音频流。次秒级延迟已经被广泛应用于视频会议之中，也一直是视频流公司的焦点，如Millicast和Limelight（仅举两个例子），这些公司旨在将这种点对点技术交付给成千上万的人。在不到一秒钟的时间内便实现了交互式视频、游戏流、拍卖和超低延迟的体育运动。

针对直接使用其他流媒体协议的用户，Pion的创建者肖恩•杜布瓦（Sean DuBois）在SF Video Tech上谈到了WebRTC带来的RTMP、SRT和RIST等协议。它的核心是WebRTC（如SRT和RIST）创建一个连接，通过它可以发送各种数据。虽然我们期望媒体被发送，但是实际上，文件传输可以很容易地被实现——让我们不要忘记整个SRT是建立在UDT之上的，而UDT是一个专门用于文件传输的实用程序。在可以实现文件传输的地方，实时数据和元数据传输也可以实现

Sean很快将WebRTC概括为（典型）浏览器之间的协议，这是一种点对点的安全连接，多个音频和视频流可以在其连接上流动。与RIST和其他最新的协议一样，它基于许多已有的协议：SRTP、DTLS、ICE和SDP等技术来提供信令、连接管理、加密和通信。

![img](https://pic2.zhimg.com/80/v2-f72b74a2bcb6c4faeefb5c3d1e023511_720w.webp)

对于RTMP非常长的改进列表，它们都在视频中被简明扼要地展现出来了，所以我们在这里只突出几个要点。重要的是，低延迟是其中的关键。RTMP在当时是属于低延迟的，但并不是以今天的低延迟标准。Sean解释说，谷歌的Stadia可以为按键提供125毫秒的视频延迟。DTLS和SRTP对于安全性来说是必不可少的，但是它们是众所周知便于理解和可靠的保护数据的方法。DTLS与TLS几乎完全相同，TLS保护您的银行转账，只是将其改为UDP而不是TCP中。但是，WebRTC可以通过交换“指纹”（DTLS-SRTP）而不是支持web上TLS的完全可信的证书基础结构来工作。只要您有信心可以提前安全地交换指纹，那么取消对证书的要求对于灵活性和敏捷性是一个很大的提升。

NAT遍历也是一大福音，即使两个端点都在防火墙后面，端点也总能找到通信的方法，尽管这确实意味着需要ICE服务器来促进连接。然而，在广播中，你更有可能控制一端，这样就不太需要这样做了。Sean强调了使用WebRTC的“同步广播”功能在同一流中发送多个质量级别的能力。

之后Sean着眼于SRT和RIST。这两种协议都是低延迟流协议，它们都可以提供次秒级的流传输，以实现RTT相对较低的良好连接。Sean强调了SRT和RIST在协商使用中的编解码器及其可选安全性方面的不足。由于更注重提供贡献源，它们往往具有更静态的配置，通常是在测试程序之后创建的，以确保其质量能够被广播商/流媒体提供商所接受。

最后，Sean重点介绍了WebRTC的一系列有趣的创新用途，从非正式的群组流媒体到无人机、共享在线游戏到文件传输等等

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/449083890

# 【NO.408】FFmpeg学习笔记——重采样demo解析

## **1.重采样**

### **1.1 什么是重采样？**

通俗的讲，重采样就是改变音频的采样率、sample format(采样格式)、声道数(channel)等参数，使之按照我们期望的参数输出。

### **1.2 为什么需要重采样？**

做什么事情之前，我们都要问一个为什么，也就是知道原理！那么为什么需要重采样呢？那是因为当原有的音频参数不满足我们实际要求时，比如说在FFmpeg解码音频的时候，不同的音源有不同的格式和采样率等，所以在解码后的数据中的这些参数也会不一致(最新的FFmpeg解码音频后，音频格式为AV_SAMPLE_FMT_TLTP);如果我们接下来需要使用解码后的音频数据做其它操作的话，然而这些参数的不一致会导致有很多额外工作，此时直接对其进行重采样的话，获取我们制定的音频参数，就会方便很多。

再比如说，在将音频进行SDL播放的时候，因为当前的SDL2.0不支持plannar格式，也不支持浮点型的，而最新的FFpemg会将音频解码为AV_SAMPLE_FMT_FLTP，这个时候进行对它重采样的话，就可以在SDL2.0上进行播放这个音频了。

**加入音视频流媒体学习沟通交流群**[1023370945](https://link.zhihu.com/?target=https%3A//jq.qq.com/%3F_wv%3D1027%26k%3DIeqmzvrN)

### **1.3 重采样参数解析：**

- sample rate(采样率)：采样设备每秒抽取样本的次数
- sample format(采样格式)和量化精度：这个应该好理解，就是采用什么格式进行采集数据；每种⾳频格式有不同的量化精度（位宽），位数越多，表示值就越精确，声⾳表现⾃然就越精准。
- channel layout（通道布局，也就是声道数）：这个就是采样的声道数

这里多补充一下:

在FFmpeg里面主要有两种采样格式:floating-point formats 和 planar sample formats；具体采样参数如下(在（libavutil/samplefmt.h头文件里面)：

```text
enum AVSampleFormat {
    AV_SAMPLE_FMT_NONE = -1,
    AV_SAMPLE_FMT_U8,          ///< unsigned 8 bits
    AV_SAMPLE_FMT_S16,         ///< signed 16 bits
    AV_SAMPLE_FMT_S32,         ///< signed 32 bits
    AV_SAMPLE_FMT_FLT,         ///< float
    AV_SAMPLE_FMT_DBL,         ///< double

    AV_SAMPLE_FMT_U8P,         ///< unsigned 8 bits, planar
    AV_SAMPLE_FMT_S16P,        ///< signed 16 bits, planar
    AV_SAMPLE_FMT_S32P,        ///< signed 32 bits, planar
    AV_SAMPLE_FMT_FLTP,        ///< float, planar
    AV_SAMPLE_FMT_DBLP,        ///< double, planar
    AV_SAMPLE_FMT_S64,         ///< signed 64 bits
    AV_SAMPLE_FMT_S64P,        ///< signed 64 bits, planar

    AV_SAMPLE_FMT_NB           ///< Number of sample formats. DO NOT USE if linking dynamically
};
```

上半部分就是floating-point formats,下半部分就是planar格式，关于这两种格式的介绍：

- floating-point formats：

```text
* - The floating-point formats are based on full volume being in the range

 *   [-1.0, 1.0]. Any values outside this range are beyond full volume level.
```

- planar sample formats：

```text
For planar sample formats, each audio channel is in a separate data plane,
 * and linesize is the buffer size, in bytes, for a single plane. All data
 * planes must be the same size. For packed sample formats, only the first data
 * plane is used, and samples for each channel are interleaved. In this case,
 * linesize is the buffer size, in bytes, for the 1 plane.
```

还有就是声道分布参数，这个在FFmpeg也有说明(声道分布在FFmpeg\libavutil\channel_layout.h中有定义，⼀般来说⽤的⽐较多的是 AV_CH_LAYOUT_STEREO（双声道）和AV_CH_LAYOUT_SURROUND（三声道），这两者的定义如下)：

```text
#define AV_CH_LAYOUT_STEREO (AV_CH_FRONT_LEFT|AV_CH_FRONT_RIGHT) 

#define AV_CH_LAYOUT_SURROUND (AV_CH_LAYOUT_STEREO|AV_CH_FRONT_CENTER)
```

下面是其他声道数参数：

![img](https://pic2.zhimg.com/80/v2-fa5360c694819f5e464f9fc5d485cbf1_720w.webp)

### **1.4 分⽚（plane）和打包（packed）：**

以双声道为例，带P（plane）的数据格式在存储时，其左声道和右声道的数据是分开存储的，左声道的 数据存储在data[0]，右声道的数据存储在data[1]，每个声道的所占⽤的字节数为linesize[0]和 linesize[1]；

不带P（packed）的⾳频数据在存储时，是按照LRLRLR...的格式交替存储在data[0]中，linesize[0] 表示总的数据量。

这个可以在下面的代码里面可以看到用法，这里简单提一下。

### **1.5 ⾳频帧的数据量计算：**

⼀帧⾳频的数据量（字节）=channel数 * nb_samples样本数 * 每个样本占⽤的字节数 如果该⾳频帧是FLTP格式的PCM数据，包含1024个样本，双声道，那么该⾳频帧包含的⾳频数据量是：

```text
2*1024*4=8192字节
```

### **1.6 ⾳频播放时间计算：**

以采样率44100Hz来计算，每秒44100个sample，⽽正常⼀帧为1024个sample，可知每帧播放时 间/1024=1000ms/44100，得到每帧播放时间=1024*1000/44100=23.2ms （更精确的是 23.21995464852608）

但是要注意：

- ⼀帧播放时间（毫秒） = nb_samples样本数 *1000/采样率 = 1024*1000/44100=23.21995464852608ms；约等于 23.2ms，精度损失了 0.011995464852608ms，如果累计10万帧，误差>1199毫秒，如果有视频⼀起的就会有⾳视频同步的问题。这个误差太可怕了。。。。

------

## **2.FFmpeg之api讲解：**

- 分配⾳频重采样的上下⽂：

```text
av_cold struct SwrContext *swr_alloc(void){
    SwrContext *s= av_mallocz(sizeof(SwrContext));
    if(s){
        s->av_class= &av_class;
        av_opt_set_defaults(s);
    }
    return s;
}



struct SwrContext {
    const AVClass *av_class;                        ///< AVClass used for AVOption and av_log()
    int log_level_offset;                           ///< logging level offset
    void *log_ctx;                                  ///< parent logging context
    enum AVSampleFormat  in_sample_fmt;             ///< input sample format
    enum AVSampleFormat int_sample_fmt;             ///< internal sample format (AV_SAMPLE_FMT_FLTP or AV_SAMPLE_FMT_S16P)
    enum AVSampleFormat out_sample_fmt;             ///< output sample format
    int64_t  in_ch_layout;                          ///< input channel layout
    int64_t out_ch_layout;                          ///< output channel layout
    int      in_sample_rate;                        ///< input sample rate
    int     out_sample_rate;                        ///< output sample rate
    int flags;                                      ///< miscellaneous flags such as SWR_FLAG_RESAMPLE
    float slev;                                     ///< surround mixing level
    float clev;                                     ///< center mixing level
    float lfe_mix_level;                            ///< LFE mixing level
    float rematrix_volume;                          ///< rematrixing volume coefficient
    float rematrix_maxval;                          ///< maximum value for rematrixing output
    int matrix_encoding;                            /**< matrixed stereo encoding */
    const int *channel_map;                         ///< channel index (or -1 if muted channel) map
    int used_ch_count;                              ///< number of used input channels (mapped channel count if channel_map, otherwise in.ch_count)
    int engine;

    int user_in_ch_count;                           ///< User set input channel count
    int user_out_ch_count;                          ///< User set output channel count
    int user_used_ch_count;                         ///< User set used channel count
    int64_t user_in_ch_layout;                      ///< User set input channel layout
    int64_t user_out_ch_layout;                     ///< User set output channel layout
    enum AVSampleFormat user_int_sample_fmt;        ///< User set internal sample format
    int user_dither_method;                         ///< User set dither method

    struct DitherContext dither;

    int filter_size;                                /**< length of each FIR filter in the resampling filterbank relative to the cutoff frequency */
    int phase_shift;                                /**< log2 of the number of entries in the resampling polyphase filterbank */
    int linear_interp;                              /**< if 1 then the resampling FIR filter will be linearly interpolated */
    int exact_rational;                             /**< if 1 then enable non power of 2 phase_count */
    double cutoff;                                  /**< resampling cutoff frequency (swr: 6dB point; soxr: 0dB point). 1.0 corresponds to half the output sample rate */
    int filter_type;                                /**< swr resampling filter type */
    double kaiser_beta;                                /**< swr beta value for Kaiser window (only applicable if filter_type == AV_FILTER_TYPE_KAISER) */
    double precision;                               /**< soxr resampling precision (in bits) */
    int cheby;                                      /**< soxr: if 1 then passband rolloff will be none (Chebyshev) & irrational ratio approximation precision will be higher */

    float min_compensation;                         ///< swr minimum below which no compensation will happen
    float min_hard_compensation;                    ///< swr minimum below which no silence inject / sample drop will happen
    float soft_compensation_duration;               ///< swr duration over which soft compensation is applied
    float max_soft_compensation;                    ///< swr maximum soft compensation in seconds over soft_compensation_duration
    float async;                                    ///< swr simple 1 parameter async, similar to ffmpegs -async
    int64_t firstpts_in_samples;                    ///< swr first pts in samples

    int resample_first;                             ///< 1 if resampling must come first, 0 if rematrixing
    int rematrix;                                   ///< flag to indicate if rematrixing is needed (basically if input and output layouts mismatch)
    int rematrix_custom;                            ///< flag to indicate that a custom matrix has been defined

    AudioData in;                                   ///< input audio data
    AudioData postin;                               ///< post-input audio data: used for rematrix/resample
    AudioData midbuf;                               ///< intermediate audio data (postin/preout)
    AudioData preout;                               ///< pre-output audio data: used for rematrix/resample
    AudioData out;                                  ///< converted output audio data
    AudioData in_buffer;                            ///< cached audio data (convert and resample purpose)
    AudioData silence;                              ///< temporary with silence
    AudioData drop_temp;                            ///< temporary used to discard output
    int in_buffer_index;                            ///< cached buffer position
    int in_buffer_count;                            ///< cached buffer length
    int resample_in_constraint;                     ///< 1 if the input end was reach before the output end, 0 otherwise
    int flushed;                                    ///< 1 if data is to be flushed and no further input is expected
    int64_t outpts;                                 ///< output PTS
    int64_t firstpts;                               ///< first PTS
    int drop_output;                                ///< number of output samples to drop
    double delayed_samples_fixup;                   ///< soxr 0.1.1: needed to fixup delayed_samples after flush has been called.

    struct AudioConvert *in_convert;                ///< input conversion context
    struct AudioConvert *out_convert;               ///< output conversion context
    struct AudioConvert *full_convert;              ///< full conversion context (single conversion for input and output)
    struct ResampleContext *resample;               ///< resampling context
    struct Resampler const *resampler;              ///< resampler virtual function table

    double matrix[SWR_CH_MAX][SWR_CH_MAX];          ///< floating point rematrixing coefficients
    float matrix_flt[SWR_CH_MAX][SWR_CH_MAX];       ///< single precision floating point rematrixing coefficients
    uint8_t *native_matrix;
    uint8_t *native_one;
    uint8_t *native_simd_one;
    uint8_t *native_simd_matrix;
    int32_t matrix32[SWR_CH_MAX][SWR_CH_MAX];       ///< 17.15 fixed point rematrixing coefficients
    uint8_t matrix_ch[SWR_CH_MAX][SWR_CH_MAX+1];    ///< Lists of input channels per output channel that have non zero rematrixing coefficients
    mix_1_1_func_type *mix_1_1_f;
    mix_1_1_func_type *mix_1_1_simd;

    mix_2_1_func_type *mix_2_1_f;
    mix_2_1_func_type *mix_2_1_simd;

    mix_any_func_type *mix_any_f;

    /* TODO: callbacks for ASM optimizations */
};
```

- 初始化SwrContext结构体：

```text
int swr_init(struct SwrContext *s);
```

- 分配SwrContext并设置/重置常⽤的参数：

```text
struct SwrContext *swr_alloc_set_opts(struct SwrContext *s, // ⾳频重采样上下⽂ 
int64_t out_ch_layout, // 输出的layout, 如：5.1声道 
enum AVSampleFormat out_sample_fmt, // 输出的采样格式。
Float, S16,⼀般 选⽤是s16 绝⼤部分声卡⽀持 
int out_sample_rate, //输出采样率 
int64_t in_ch_layout, // 输⼊的layout enum AVSampleFormat in_sample_fmt, // 输⼊的采样格式 
int in_sample_rate, // 输⼊的采样率 
int log_offset, // ⽇志相关，不⽤管先，直接为0 void *log_ctx // ⽇志相关，不⽤管先，直接为NULL 
);
```

- 将输⼊的⾳频按照定义的参数进⾏转换并输出：

```text
int swr_convert(struct SwrContext *s, // ⾳频重采样的上下⽂ 
uint8_t **out, // 输出的指针。传递的输出的数组 int out_count, //输出的样本数量，不是字节数。单通道的样本数量。 
const uint8_t **in , //输⼊的数组，AVFrame解码出来的DATA 
int in_count // 输⼊的单通道的样本数量。
);
```

- 释放掉SwrContext结构体并将此结构体置为NULL：

```text
void swr_free(struct SwrContext **s)
```

- ⾳频重采样，采样格式转换和混合库：

与lswr的交互是通过SwrContext完成的，SwrContext被分配给swr_alloc（）或 swr_alloc_set_opts（）。它是不透明的，所以所有参数必须使⽤AVOptions API设置。为了使⽤lswr，你需要做的第⼀件事就是分配SwrContext。这可以使⽤swr_alloc（）或 swr_alloc_set_opts（）来完成。如果您使⽤前者，则必须通过AVOptions API设置选项。后⼀个函数 提供了相同的功能，但它允许您在同⼀语句中设置⼀些常⽤选项。例如，以下代码将设置从平⾯浮动样本格式到交织的带符号16位整数的转换，从48kHz到44.1kHz的下采 样，以及从5.1声道到⽴体声的下混合（使⽤默认混合矩阵）。这是使⽤swr_alloc（）函数：

```text
1 SwrContext *swr = swr_alloc(); 
2 av_opt_set_channel_layout(swr, "in_channel_layout", AV_CH_LAYOUT_ 5POINT1, 0); 
3 av_opt_set_channel_layout(swr, "out_channel_layout", AV_CH_LAYOUT_ STEREO, 0); 
4 av_opt_set_int(swr, "in_sample_rate", 48000, 0) ; 
5 av_opt_set_int(swr, "out_sample_rate", 44100, 0) ;
6 av_opt_set_sample_fmt(swr, "in_sample_fmt", AV_SAMPLE_FMT_FLTP, 0 ); 
7 av_opt_set_sample_fmt(swr, "out_sample_fmt", AV_SAMPLE_FMT_S16, 0 );
```

同样的⼯作也可以使⽤swr_alloc_set_opts（）：

```text
SwrContext *swr = swr_alloc_set_opts(NULL, // we're allocating a new context 
2 AV_CH_LAYOUT_STEREO, // out_ch_layout
3 AV_SAMPLE_FMT_S16, // out_sample_fmt 
4 44100, // out_sample_rate
5 AV_CH_LAYOUT_5POINT1, // in_ch_layout
6 AV_SAMPLE_FMT_FLTP, // in_sample_fmt 
7 48000, // in_sample_rate 
8 0, // log_offset
9 NULL
); // log_ctx
```

⼀旦设置了所有值，它必须⽤swr_init（）初始化。如果需要更改转换参数，可以使⽤ AVOptions来更改参数，如上⾯第⼀个例⼦所述; 或者使⽤swr_alloc_set_opts（），但是第 ⼀个参数是分配的上下⽂。您必须再次调⽤swr_init（）。

转换本身通过重复调⽤swr_convert（）来完成。请注意，如果提供的输出空间不⾜或采样率转换完成 后，样本可能会在swr中缓冲，这需要“未来”样本。可以随时通过使⽤swr_convert（）（in_count可以 设置为0）来检索不需要将来输⼊的样本。在转换结束时，可以通过调⽤具有NULL in和in incount的 swr_convert（）来刷新重采样缓冲区。

下面是一般重采样的流程：

- 分配：swr_alloc()
- 设置参数:av_opt_set_channel_layout();av_opt_set_int();av_opt_set_sample_fmt()
- 初始化:swr_init()
- 转换: swr_convert()

## **3.重采样demo:**

说明一下，这里代码有参考FFmpeg给的demo哈：

![img](https://pic2.zhimg.com/80/v2-1ffb2d0df14b642e99da07a234df3a99_720w.webp)

```text
/*
 * Copyright (c) 2012 Stefano Sabatini
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to deal
 * in the Software without restriction, including without limitation the rights
 * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 * copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
 * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
 * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
 * THE SOFTWARE.
 */

/**
 * @example resampling_audio.c
 * libswresample API use example.
 */

#include <libavutil/opt.h>
#include <libavutil/channel_layout.h>
#include <libavutil/samplefmt.h>
#include <libswresample/swresample.h>

static int get_format_from_sample_fmt(const char **fmt,
                                      enum AVSampleFormat sample_fmt)
{
    int i;
    struct sample_fmt_entry {
        enum AVSampleFormat sample_fmt; const char *fmt_be, *fmt_le;
    } sample_fmt_entries[] = {
    { AV_SAMPLE_FMT_U8,  "u8",    "u8"    },
    { AV_SAMPLE_FMT_S16, "s16be", "s16le" },
    { AV_SAMPLE_FMT_S32, "s32be", "s32le" },
    { AV_SAMPLE_FMT_FLT, "f32be", "f32le" },
    { AV_SAMPLE_FMT_DBL, "f64be", "f64le" },
};
    *fmt = NULL;

    for (i = 0; i < FF_ARRAY_ELEMS(sample_fmt_entries); i++) {
        struct sample_fmt_entry *entry = &sample_fmt_entries[i];
        if (sample_fmt == entry->sample_fmt) {
            *fmt = AV_NE(entry->fmt_be, entry->fmt_le);
            return 0;
        }
    }

    fprintf(stderr,
            "Sample format %s not supported as output format\n",
            av_get_sample_fmt_name(sample_fmt));
    return AVERROR(EINVAL);
}

/**
 * Fill dst buffer with nb_samples, generated starting from t. 交错模式的
 */
static void fill_samples(double *dst, int nb_samples, int nb_channels, int sample_rate, double *t)
{
    int i, j;
    double tincr = 1.0 / sample_rate, *dstp = dst;
    const double c = 2 * M_PI * 440.0;

    /* generate sin tone with 440Hz frequency and duplicated channels */
    for (i = 0; i < nb_samples; i++) {
        *dstp = sin(c * *t);
        for (j = 1; j < nb_channels; j++)
            dstp[j] = dstp[0];
        dstp += nb_channels;
        *t += tincr;
    }
}

int main(int argc, char **argv)
{
    // 输入参数
    int64_t src_ch_layout = AV_CH_LAYOUT_STEREO;
    int src_rate = 48000;
    enum AVSampleFormat src_sample_fmt = AV_SAMPLE_FMT_DBL;
    int src_nb_channels = 0;
    uint8_t **src_data = NULL;  // 二级指针
    int src_linesize;
    int src_nb_samples = 1024;


    // 输出参数
    int64_t dst_ch_layout = AV_CH_LAYOUT_STEREO;
    int dst_rate = 44100;
    enum AVSampleFormat dst_sample_fmt = AV_SAMPLE_FMT_S16;
    int dst_nb_channels = 0;
    uint8_t **dst_data = NULL;  //二级指针
    int dst_linesize;
    int dst_nb_samples;
    int max_dst_nb_samples;

    // 输出文件
    const char *dst_filename = NULL;    // 保存输出的pcm到本地，然后播放验证
    FILE *dst_file;


    int dst_bufsize;
    const char *fmt;

    // 重采样实例
    struct SwrContext *swr_ctx;

    double t;
    int ret;

    if (argc != 2) {
        fprintf(stderr, "Usage: %s output_file\n"
                        "API example program to show how to resample an audio stream with libswresample.\n"
                        "This program generates a series of audio frames, resamples them to a specified "
                        "output format and rate and saves them to an output file named output_file.\n",
                argv[0]);
        exit(1);
    }
    dst_filename = argv[1];

    dst_file = fopen(dst_filename, "wb");
    if (!dst_file) {
        fprintf(stderr, "Could not open destination file %s\n", dst_filename);
        exit(1);
    }

    // 创建重采样器
    /* create resampler context */
    swr_ctx = swr_alloc();
    if (!swr_ctx) {
        fprintf(stderr, "Could not allocate resampler context\n");
        ret = AVERROR(ENOMEM);
        goto end;
    }

    // 设置重采样参数
    /* set options */
    // 输入参数
    av_opt_set_int(swr_ctx, "in_channel_layout",    src_ch_layout, 0);
    av_opt_set_int(swr_ctx, "in_sample_rate",       src_rate, 0);
    av_opt_set_sample_fmt(swr_ctx, "in_sample_fmt", src_sample_fmt, 0);
    // 输出参数
    av_opt_set_int(swr_ctx, "out_channel_layout",    dst_ch_layout, 0);
    av_opt_set_int(swr_ctx, "out_sample_rate",       dst_rate, 0);
    av_opt_set_sample_fmt(swr_ctx, "out_sample_fmt", dst_sample_fmt, 0);

    // 初始化重采样
    /* initialize the resampling context */
    if ((ret = swr_init(swr_ctx)) < 0) {
        fprintf(stderr, "Failed to initialize the resampling context\n");
        goto end;
    }

    /* allocate source and destination samples buffers */
    // 计算出输入源的通道数量
    src_nb_channels = av_get_channel_layout_nb_channels(src_ch_layout);
    // 给输入源分配内存空间
    ret = av_samples_alloc_array_and_samples(&src_data, &src_linesize, src_nb_channels,
                                             src_nb_samples, src_sample_fmt, 0);
    if (ret < 0) {
        fprintf(stderr, "Could not allocate source samples\n");
        goto end;
    }

    /* compute the number of converted samples: buffering is avoided
     * ensuring that the output buffer will contain at least all the
     * converted input samples */
    // 计算输出采样数量
    max_dst_nb_samples = dst_nb_samples =
            av_rescale_rnd(src_nb_samples, dst_rate, src_rate, AV_ROUND_UP);

    /* buffer is going to be directly written to a rawaudio file, no alignment */
    dst_nb_channels = av_get_channel_layout_nb_channels(dst_ch_layout);
    // 分配输出缓存内存
    ret = av_samples_alloc_array_and_samples(&dst_data, &dst_linesize, dst_nb_channels,
                                             dst_nb_samples, dst_sample_fmt, 0);
    if (ret < 0) {
        fprintf(stderr, "Could not allocate destination samples\n");
        goto end;
    }

    t = 0;
    do {
        /* generate synthetic audio */
        // 生成输入源
        fill_samples((double *)src_data[0], src_nb_samples, src_nb_channels, src_rate, &t);

        /* compute destination number of samples */
        int64_t delay = swr_get_delay(swr_ctx, src_rate);
        dst_nb_samples = av_rescale_rnd(delay + src_nb_samples, dst_rate, src_rate, AV_ROUND_UP);
        if (dst_nb_samples > max_dst_nb_samples) {
            av_freep(&dst_data[0]);
            ret = av_samples_alloc(dst_data, &dst_linesize, dst_nb_channels,
                                   dst_nb_samples, dst_sample_fmt, 1);
            if (ret < 0)
                break;
            max_dst_nb_samples = dst_nb_samples;
        }
        //        int fifo_size = swr_get_out_samples(swr_ctx,src_nb_samples);
        //        printf("fifo_size:%d\n", fifo_size);
        //        if(fifo_size < 1024)
        //            continue;

        /* convert to destination format */
        //        ret = swr_convert(swr_ctx, dst_data, dst_nb_samples, (const uint8_t **)src_data, src_nb_samples);
        ret = swr_convert(swr_ctx, dst_data, dst_nb_samples, (const uint8_t **)src_data, src_nb_samples);
        if (ret < 0) {
            fprintf(stderr, "Error while converting\n");
            goto end;
        }
        dst_bufsize = av_samples_get_buffer_size(&dst_linesize, dst_nb_channels,
                                                 ret, dst_sample_fmt, 1);
        if (dst_bufsize < 0) {
            fprintf(stderr, "Could not get sample buffer size\n");
            goto end;
        }
        printf("t:%f in:%d out:%d\n", t, src_nb_samples, ret);
        fwrite(dst_data[0], 1, dst_bufsize, dst_file);
    } while (t < 10);

    ret = swr_convert(swr_ctx, dst_data, dst_nb_samples, NULL, 0);
    if (ret < 0) {
        fprintf(stderr, "Error while converting\n");
        goto end;
    }
    dst_bufsize = av_samples_get_buffer_size(&dst_linesize, dst_nb_channels,
                                             ret, dst_sample_fmt, 1);
    if (dst_bufsize < 0) {
        fprintf(stderr, "Could not get sample buffer size\n");
        goto end;
    }
    printf("flush in:%d out:%d\n", 0, ret);
    fwrite(dst_data[0], 1, dst_bufsize, dst_file);


    if ((ret = get_format_from_sample_fmt(&fmt, dst_sample_fmt)) < 0)
        goto end;
    fprintf(stderr, "Resampling succeeded. Play the output file with the command:\n"
                    "ffplay -f %s -channel_layout %"PRId64" -channels %d -ar %d %s\n",
            fmt, dst_ch_layout, dst_nb_channels, dst_rate, dst_filename);

end:
    fclose(dst_file);

    if (src_data)
        av_freep(&src_data[0]);
    av_freep(&src_data);

    if (dst_data)
        av_freep(&dst_data[0]);
    av_freep(&dst_data);

    swr_free(&swr_ctx);
    return ret < 0;
}
```

输出out.pcm:

![img](https://pic1.zhimg.com/80/v2-1983fff456a2beccbf31836aaaafab8c_720w.webp)

![img](https://pic3.zhimg.com/80/v2-267cb9ad25a507c5c031df74cad19a3a_720w.webp)

![img](https://pic1.zhimg.com/80/v2-11799572fcdf4fdf573460b8c737de9c_720w.webp)

然后进行播放一下：

```text
ffplay -f s16le -channel_layout 3 -channels 2 -ar 44100 out.pcm
```

![img](https://pic3.zhimg.com/80/v2-a97c9790677ecab7baa1ba6cb03ab8fe_720w.webp)

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/431679639

# 【NO.409】Linux ubuntu FFmpeg开发环境搭建(保姆式搭建教程)

## **1.创建目录**


在home目录下创建
ffmpeg_sources：用于下载源文件
ffmpeg_build： 存储编译后的库文件
bin：存储二进制文件（ffmpeg，ffplay，ffprobe，X264，X265等）
cd ~
mkdir ffmpeg_sources ffmpeg_build bin

------

##  2.**安装依赖**


更新

```text
sudo apt-get update
```


安装需要的组件

```text
sudo apt-get -y install \
autoconf \
automake \
build-essential \
cmake \
git-core \
libass-dev \
libfreetype6-dev \
libsdl2-dev \
libtool \
libva-dev \
libvdpau-dev \
libvorbis-dev \
libxcb1-dev \
libxcb-shm0-dev \
libxcb-xfixes0-dev \
pkg-config \
texinfo \
wget \
zlib1g-dev
```

------


**编译与安装**


本指南假定您要安装一些最常见的第三方库。每个小节为您提供安装该库所需的命令。
如果不需要某些功能，则可以跳过相关小节（如果不需要），然后在FFmpeg中删除相应的./configure选项。例如，如果不需要libvpx，请跳过该小节，然后从“ [安装FFmpeg”](https://link.zhihu.com/?target=https%3A//trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu%23FFmpeg)部分中删除--enable-libvpx。
提示：如果要在多核系统中加快编译速度，可以在每个make命令（例如make -j4）中使用-j选项。
建议都使用源码进行安装。


**NASM**
部分库使用到汇编程序。
如果你系统提供的库nasm版本>=2.13则可以直接使用命令进行安装

```text
sudo apt-get install nasm
```


否则使用源码进行安装

```text
cd ~/ffmpeg_sources && \
wget https://www.nasm.us/pub/nasm/releasebuilds/2.14.02/nasm-2.14.02.tar.bz2 && \
tar xjvf nasm-2.14.02.tar.bz2 && \
cd nasm-2.14.02 && \
./autogen.sh && \
PATH="$HOME/bin:$PATH" ./configure --prefix="$HOME/ffmpeg_build" --bindir="$HOME/bin" && \
make && \
make install
```


**Yasm**
部分库使用到该汇编库
如果你系统提供的库yasm版本 ≥ 1.2.0则可以直接使用命令进行安装:

```text
sudo apt-get install yasm
```


否则使用源码进行安装:

```text
cd ~/ffmpeg_sources && \
wget -O yasm-1.3.0.tar.gz https://www.tortall.net/projects/yasm/releases/yasm-1.3.0.tar.gz && \
tar xzvf yasm-1.3.0.tar.gz && \
cd yasm-1.3.0 && \
./configure --prefix="$HOME/ffmpeg_build" --bindir="$HOME/bin" && \
make && \
make install
```


**libx264**
H.264视频编码器。更多信息和使用范例参考[H.264 Encoding Guide](https://link.zhihu.com/?target=https%3A//trac.ffmpeg.org/wiki/Encode/H.264)
要求编译ffmpeg时配置：--enable-gpl --enable-libx264.
如果你系统提供的 libx264-dev版本 ≥ 118 则可以使用命令直接安装:

```text
sudo apt-get install libx264-dev
```


否则使用源码进行编译：

```text
cd ~/ffmpeg_sources && \
git -C x264 pull 2> /dev/null || git clone --depth 1 https://gitee.com/mirrors_addons/x264.git && \
cd x264 && \
PATH="$HOME/bin:$PATH" PKG_CONFIG_PATH="$HOME/ffmpeg_build/lib/pkgconfig" ./configure --prefix="$HOME/ffmpeg_build" --bindir="$HOME/bin" --enable-static --enable-pic && \
PATH="$HOME/bin:$PATH" make && \
make install
```


**libx265**
H.265/HEVC 视频编码器， 更多信息和使用范例参考[H.265 Encoding Guide](https://link.zhihu.com/?target=https%3A//trac.ffmpeg.org/wiki/Encode/H.265)。
要求编译ffmpeg时配置：--enable-gpl --enable-libx265.
如果你系统提供的 libx265-dev版本≥ 68 则可以使用命令直接安装:

```text
sudo apt-get install libx265-dev libnuma-dev
```


否则使用源码进行编译：

```text
sudo apt-get install mercurial libnuma-dev && \
cd ~/ffmpeg_sources && \

if cd x265 2> /dev/null; then git pull && cd ..; else git clone https://gitee.com/mirrors_videolan/x265.git; fi && \
cd x265/build/linux && \
PATH="$HOME/bin:$PATH" cmake -G "Unix Makefiles" -DCMAKE_INSTALL_PREFIX="$HOME/ffmpeg_build" -DENABLE_SHARED=off ../../source && \
PATH="$HOME/bin:$PATH" make && \
make install
```


**libvpx**
VP8/VP9视频编解码器。 更多信息和使用范例参考[VP9 Video Encoding Guide](https://link.zhihu.com/?target=https%3A//trac.ffmpeg.org/wiki/Encode/VP9) 。
要求编译ffmpeg时配置： --enable-libvpx.
如果你系统提供的libvpx-dev version ≥ 1.4.0则可以使用命令直接安装:

```text
sudo apt-get install libvpx-dev
```


否则使用源码进行编译：

```text
cd ~/ffmpeg_sources && \
git -C libvpx pull 2> /dev/null || git clone --depth 1 https://github.com/webmproject/libvpx.git && \
cd libvpx && \
PATH="$HOME/bin:$PATH" ./configure --prefix="$HOME/ffmpeg_build" --disable-examples --disable-unit-tests --enable-vp9-highbitdepth --as=yasm --enable-pic && \
PATH="$HOME/bin:$PATH" make && \
make install
```


**libfdk-aac**
AAC音频编码器. 更多信息和使用范例参考[AAC Audio Encoding Guide](https://link.zhihu.com/?target=https%3A//trac.ffmpeg.org/wiki/Encode/AAC)。
要求编译ffmpeg时配置：--enable-libfdk-aac ( 如果你已经配置了 --enable-gpl则需要加上--enable-nonfree).
如果你系统提供的libfdk-aac-dev则可以使用命令直接安装:

```text
sudo apt-get install libfdk-aac-dev
```


否则使用源码进行编译：

```text
cd ~/ffmpeg_sources && \
git -C fdk-aac pull 2> /dev/null || git clone --depth 1 https://github.com/mstorsjo/fdk-aac && \
cd fdk-aac && \
autoreconf -fiv && \
./configure CFLAGS=-fPIC --prefix="$HOME/ffmpeg_build"   && \
make && \
make install
```


**libmp3lame**
MP3音频编码器.
要求编译ffmpeg时配置：--enable-libmp3lame.
如果你系统提供的libmp3lame-dev版本≥ 3.98.3则可以使用命令直接安装:

```text
sudo apt-get install libmp3lame-dev
```


否则使用源码进行编译：

```text
cd ~/ffmpeg_sources && \
git clone  --depth 1 https://gitee.com/hqiu/lame.git && \
cd lame && \
PATH="$HOME/bin:$PATH" ./configure --prefix="$HOME/ffmpeg_build" --bindir="$HOME/bin" --enable-nasm --with-pic && \
PATH="$HOME/bin:$PATH" make && \
make install
```


**libopus**
Opus音频编解码器.
要求编译ffmpeg时配置：--enable-libopus.
如果你系统提供的libopus-dev 版本≥ 1.1则可以使用命令直接安装:

```text
sudo apt-get install libopus-dev
```


否则使用源码进行编译：

```text
cd ~/ffmpeg_sources && \
git -C opus pull 2> /dev/null || git clone --depth 1 https://github.com/xiph/opus.git && \
cd opus && \
./autogen.sh && \
./configure --prefix="$HOME/ffmpeg_build" -with-pic&& \
make && \
make install
```


**libaom**
AV1 视频编解码器:
Warning:libaom does not yet appear to have a stable API, so compilation of libavcodec/libaomenc.c may occasionally fail. Just wait a day or two for us to catch up with these annoying changes, re-download ffmpeg-snapshot.tar.bz2, and try again. Or skip libaom altogether.
要求编译ffmpeg时配置：--enable-libaom.
先不支持AV1 ，编译有问题。

```text
cd ~/ffmpeg_sources && \
git -C aom pull 2> /dev/null || git clone --depth 1 https://github.com/mozilla/aom.git && \
mkdir -p aom_build && \
cd aom_build && \
PATH="$HOME/bin:$PATH" cmake -G "Unix Makefiles" -DCMAKE_INSTALL_PREFIX="$HOME/ffmpeg_build" -DENABLE_SHARED=off -DENABLE_NASM=on ../aom && \
PATH="$HOME/bin:$PATH" make && \
make install
```


**FFmpeg**

```text
cd ~/ffmpeg_sources && \
wget -O ffmpeg-4.2.1.tar.bz2 https://ffmpeg.org/releases/ffmpeg-4.2.1.tar.bz2 && \
tar xjvf ffmpeg-4.2.1.tar.bz2 && \
cd ffmpeg-4.2.1 && \
PATH="$HOME/bin:$PATH" PKG_CONFIG_PATH="$HOME/ffmpeg_build/lib/pkgconfig" CFLAGS="-O3 -fPIC" ./configure \
 --prefix="$HOME/ffmpeg_build" \
 --pkg-config-flags="--static" \
 --extra-cflags="-I$HOME/ffmpeg_build/include" \
 --extra-ldflags="-L$HOME/ffmpeg_build/lib" \
 --extra-libs="-lpthread -lm" \
 --bindir="$HOME/bin" \
 --enable-gpl \
 --enable-libass \
 --enable-libfdk-aac \
 --enable-libfreetype \
 --enable-libmp3lame \
 --enable-libopus \
 --enable-libvorbis \
 --enable-libvpx \
 --enable-libx264 \
 --enable-libx265 \
 --enable-pic \
  --enable-shared   \
 --enable-nonfree && \
PATH="$HOME/bin:$PATH" make && \
make install && \
hash -r
```


然后重新登录系统或者在当前shell会话执行如下命令以识别新安装ffmpeg的位置：
source ~/.profile
现在已经完成编译和安装ffmpeg (also ffplay, ffprobe, lame, x264, & x265) 。该文档剩余章节主要讲如何更新和删除ffmepg。
编译完成后，ffmpeg_build ffmpeg_sources bin目录的大体情况

```text
lqf@ubuntu:~/ffmpeg_build$ ls
bin  include  lib  share

lqf@ubuntu:~/ffmpeg_sources$ ls
fdk-aac               lame-3.100.tar.gz     opus        yasm-1.3.0.tar.gz
ffmpeg-4.2.1          libvpx                x264
ffmpeg-4.2.1.tar.bz2  nasm-2.14.02          x265
lame                  nasm-2.14.02.tar.bz2  yasm-1.3.0

lqf@ubuntu:~/bin$ ls
ffmpeg  ffplay  ffprobe  lame  nasm  ndisasm  vsyasm  x264  yasm  ytasm
```

------

## 3. **使用**


现在，您可以打开一个终端，输入ffmpeg命令，它应该执行新的ffmpeg。
如果你需要多个用户能同时使用你新编译的ffmpeg，则可以移动或者拷贝ffmpeg二进制文件从~/bin到/usr/local/bin。
测试ffplay是否可以使用（需要在图形方式进行测试）
ffplay rtmp://202.69.69.180:443/webcast/bshdlive-pc
如果能够正常播放则说明 ffplay能够编译成功使用。

------

##  4.**文档**


你可以使用 man ffmpeg以本地的方式访问文档:
echo"MANPATH_MAP $HOME/bin $HOME/ffmpeg_build/share/man" >> ~/.manpath
你可能必须注销系统然后重新登录man ffmpeg才生效。
HTML 格式的文档位于 ~/ffmpeg_build/share/doc/ffmpeg.
你也可以参考在线文档 [online FFmpeg documentation](https://link.zhihu.com/?target=https%3A//ffmpeg.org/documentation.html),

支持FFmpeg 代码 Debug
刚才的工程可以运行，但不能debug。解决此问题，首先认定一点，生成的可执行程序中，ffmpeg 不包含调试信息，调试信息在 ffmpeg_g 中,debug 要选择 ffmpeg_g。
另外，./config选项也是确定包含调试信息的核心，需要在config中添加：

-–enable-debug=3：开启debug调试

-–disable-asm：禁用 asm 优化

--disable-optimizations：禁用优化，以便调试时按函数顺序执行。

–-disable-stripping：禁用剥离可执行程序和共享库，即调试时可以进入到某个函数进行单独调试。

采用以下命令重新config:
先clean

```text
make clean
```


再重新config

```text
./configure \
  --prefix="$HOME/ffmpeg_build" \
  --pkg-config-flags="--static" \
  --extra-cflags="-I$HOME/ffmpeg_build/include" \
  --extra-ldflags="-L$HOME/ffmpeg_build/lib" \
  --extra-libs="-lpthread -lm" \
  --bindir="$HOME/bin" \
  --enable-gpl \
  --enable-libass \
  --enable-libfdk-aac \
  --enable-libfreetype \
  --enable-libmp3lame \
  --enable-libopus \
  --enable-libvorbis \
  --enable-libvpx \
  --enable-libx264 \
  --enable-libx265 \
  --enable-pic \
  --enable-shared   \
  --enable-nonfree \
  --enable-debug=3 \
  --disable-optimizations \
  --disable-asm \
  --disable-stripping && \
PATH="$HOME/bin:$PATH" make && \
make install && \
hash -r
```

##  5.**一些注意事项**


在使用 ffplay 播放生成 h264 格式的视频时，播放速度会加快，解决方式：不要使用 FFmpeg 转码生成纯 h264 格式的视频，要使用一种容器包含 h264 视频，即生成一种音视频流格式，也就是不要生成纯粹的 h264 码流，而是生成诸如 mkv 等格式的文件。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/431679639

# 【NO.410】音视频开发技术的基本知识

互联网信息的传播与娱乐方式经历了从文字到图片再到音视频的转变，现如今抖音、快手等短视频更是如日中天，特别是5G时代的到来，笔者相信互联网对音视频开发者的需求会迎来更大的增长需求，何况音视频开发者因为其稀缺性薪酬本来就比较高。

在学习音视频开发之前，我们先来了解一下音视频的基本知识。

## 1.音频

声波的三要素：频率、振幅和波形。频率代表音阶的高低，振幅代表响度，波形代表音色。

频率越高，波长就越短。低频声响的波长则较长，所以其可以更容易地绕过障碍物，因此能量衰减就小，声音就会传得远。人的听力有一个频率范围，大约是20Hz~20kHz。

音频数据的承载方式最常用的是脉冲编码调制，即PCM。

### 1.1 数字信号

在自然界中，声音是连续不断的，是一种模拟信号，那怎样才能把声音保存下来呢？那就是把声音数字化，即转换为数字信号。

将模拟信号数字化，要经过采样、量化和编码三个步骤。

音频数字化有一个问题：数字信号并不能连续保存所有时间点的振幅。

实际上，音频数字化并不需要保存连续的信号，就可以还原到人耳可接受的声音。

### 1.2 采样率

每一秒钟所采样的数目称为采样频率或采率，单位为HZ（赫兹）。采样频率越高所能描述的声波频率就越高。

> 根据奈奎斯特定理，按比声音最高频率高2倍以上的频率对声音进行采样，经过数字化处理之后，人耳听到的声音质量不会被降低。所以采样频率一般为44.1kHz。

- 8khz：电话等使用，对于记录人声已经足够使用。
- 22.05khz：广播使用频率。
- 44.1khz：音频CD。
- 48khz：DVD、数字电视中使用。
- 96khz-192khz：DVD-Audio、蓝光高清等使用。

### 1.3 量化格式

量化格式也叫采样大小或采样精度，指的是一个采样使用多少bit存放，一般是16bit，一个字节是8bit。

### 1.4 编解码

描述一段PCM数据一般需要以下几个概念：量化格式(sampleFormat)、采样率(sampleRate)、声道数(channel)。
量化格式和采样率上面提到过了，声道数是指支持能不同发声的音响的个数。不难理解，立体声道的声道数默认为2个声道。

数据比特率，即1秒时间内的比特数目。

\>

> 以CD的音质为例，量化格式（位深度）为16比特，样率为44100，声道数为2。
> 比特率:
> 比特率 = 采样率 × 采样深度 × 通道数
> 44100 * 16 * 2 = 1378.123kbps **注意这里的k表示1024**
> 一分钟音频数据的大小：1378.125 * 60 / 8 / 1024 = 10.09MB
> 注意：1个字节等于8位，也就是8比特

这不对啊，我们平时听的阴影四五分钟也才四五兆，到了你这里怎么一秒钟就十多兆了呢？这就是编码的功劳了。

我们先来看看比特率中的数字和字母到底是什么意思？

> 首先128k的全称“128kbps”，我们分解一下：128是数字，k是千位符，b是单位，s是秒，ps其实就是“/s”。这样来看，128kbps就是128kb/s。也就是每秒128kb。请注意，这里的b是小写的b，也就是位。
> 知道了这个，我们就能算出来128kb的文件大概占用多少的存储空间：
> 128*1000=128000b/s÷8=16000B/s÷1024=15.625KB/s
> 15.625KB/s*60=937.5KB/分钟÷1024=0.9155MB/分钟
> 所以，128kb的音频文件，大概每分钟长度的大小都在0.92M或者916kb左右。
> **注意b和B是不同的概念**

编码就是一个压缩的过程，而压缩又分为有损压缩和无损压缩：

有损压缩就是去掉冗余信号，冗余信号是指不能被人耳感知到的信号，包含人耳听觉范围之外的音频信号以及被掩蔽掉的音频信号等

无损压缩就是通过优化排列方式来达到压缩目的。

常见的音频编码格式：

1. WAV(无损)

WAV编码就是在PCM数据格式的前面加上44字节，分别用来描述PCM的采样率、声道数、数据格式等信息。

特点：音质非常好，大量软件都支持。
缺点：因为没用经过压缩，所以文件占用的储存空间会特别大。

适用场合：多媒体开发的中间文件、保存音乐和音效素材。

1. MP3(有损)

MP3具有不错的压缩比，使用LAME编码（MP3编码格式的一种实现）的中高码率的MP3文件，听感上非常接近源WAV文件。现如今市面上的音乐大多是这种编码格式。

特点：音质在128Kbit/s以上表现还不错，压缩比比较高，大量软件和硬件都支持，兼容性好。
缺点：由于技术比较落后，同样码率下音质会比AAC、OGG差一些。

1. AAC(有损)

AAC是新一代的音频有损压缩技术，它通过一些附加的编码技术（比如PS、SBR等），衍生出了LC-AAC、HE-AAC、HE-AAC v2三种主要的编码格式。

LC-AAC是比较传统的AAC，相对而言，其主要应用于中高码率场景的编码（≥80Kbit/s）；

HE-AAC（相当于AAC+SBR）主要应用于中低码率场景的编码（≤80Kbit/s）；

而新近推出的HE-AAC v2（相当于AAC+SBR+PS）主要应用于低码率场景的编码（≤48Kbit/s）。事实上大部分编码器都设置为≤48Kbit/s自动启用PS技术，而>48Kbit/s则不加PS，相当于普通的HE-AAC。

特点：在小于128Kbit/s的码率下表现优异，并且多用于视频中的音频编码。
不足：虽然在低码率上表现比MP3好一些，但是还没有达到全面碾压的地步。

适用场合：128Kbit/s以下的音频编码，多用于视频中音频轨的编码。

AAC格式主要分为两种：ADIF、ADTS。

ADIF：Audio Data Interchange Format。音频数据交换格式。这种格式的特征是可以确定的找到这个音频数据的开始，不能在音频数据流中间开始的解码，即它的解码必须在明确定义的开始处进行。这种格式常用在磁盘文件中。

ADTS：Audio Data Transport Stream。音频数据传输流。这种格式的特征是它是一个有同步字的比特流，解码可以在这个流中任何位置开始。它的特征类似于mp3数据流格式。

ADTS可以在任意帧解码，它每一帧都有头信息。ADIF只有一个统一的头，所以必须得到所有的数据后解码。且这两种的header的格式也是不同的，目前一般编码后的都是ADTS格式的音频流。

1. Ogg(有损)

Ogg是一种非常有潜力的编码，在各种码率下都有比较优秀的表现，尤其是在中低码率场景下。Ogg除了音质好之外，还是完全免费的，这为Ogg获得更多的支持打好了基础。Ogg有着非常出色的算法，可以用更小的码率达到更好的音质，128Kbit/s的Ogg比192Kbit/s甚至更高码率的MP3还要出色。但目前因为还没有媒体服务软件的支持，因此基于Ogg的数字广播还无法实现。Ogg目前受支持的情况还不够好，无论是软件上的还是硬件上的支持，都无法和MP3相提并论。

特点：可以用比MP3更小的码率实现比MP3更好的音质，高中低码率下均有良好的表现。
缺点：兼容性不够好，流媒体特性不支持。

适用场合：语音聊天的音频消息场景。

1. FLAC(无损)

FLAC是一套著名的自由音频压缩编码，其特点是无损压缩。不同于其他有损压缩编码如MP3 及AAC，它不会破坏任何原有的音频资讯，所以可以还原音乐光盘音质。2012年以来它已被很多软件及硬件音频产品（如CD等）所支持.

FLAC与MP3不同，MP3是音频压缩编码，但FLAC是无损压缩，也就是说音频以FLAC编码压缩后不会丢失任何信息，将FLAC文件还原为WAV文件后，与压缩前的WAV文件内容相同。这种压缩与ZIP的方式类似，但FLAC的压缩比率大于ZIP和RAR，因为FLAC是专门针对PCM音频的特点设计的压缩方式。而且可以使用播放器直接播放FLAC压缩的文件，就象通常播放你的MP3文件一样.

## 2.视频

所谓视频其实就是由很多的静态图片组成的。由于人类眼睛的特殊结构，画面快速切换时，画面会有残留，所以静态图片快速切换的时候感觉起来就是连贯的动作。这就是视频的原理。

视频帧：
既然视频是由许多静态图片组成的，那么视频的每一张静态图片就叫一帧。

视频帧又分为I帧、B帧和P帧：

I帧：帧内编码帧，大多数情况下I帧就是关键帧，就是一个完整帧，无需任何辅助就能独立完整显示的画面。

B帧：帧是双向预测帧。参考前后图像帧编码生成。需要前面的 I/P 帧或者后面的 P 帧来协助形成一个画面。

P帧：前向预测编码帧。是一个非完整帧，通过参考前面的I帧或P帧生成画面。

> 所以 I 帧是很关键的存在，压缩 I 帧就可以很容易压制掉空间的大小，而压缩P帧和B帧可以压缩掉时间上的冗余信息 。所以在视频 seek 的时候，I 帧很关键，如果视频 seek 之后发生往前的跳动，有可能就是你要seek到的位置没用关键帧，这就需要处理了。好像Android自带的播放器就会有这个问题，有时候无法精确地seek到某个位置。

还有一个叫 IDR 帧的概念，IDR都是I帧，可以防止一帧解码出错，导致后面所有帧解码出错的问题。
因为 H264 采用的是多帧预测，导致 I 帧不能作为独立的观察条件，所以多出一个叫 IDR 帧的特殊 I 帧用于参考，IDR 帧最关键的概念就是：在解码器过程中一旦收到 IDR 帧，就会立即清空参考帧缓冲区，并将IDR帧作为被参考帧。这样，即便前面一帧解码出现重大错误，也不会蔓延到后面的数据中。

**注：关键帧都是I帧，但是I帧不一定是关键帧** 这是为什么？求高人指点！！！

DTS全称：Decoding Time Stamp。标示读入内存中数据流在什么时候开始送入解码器中进行解码。也就是解码顺序的时间戳。

PTS全称：Presentation Time Stamp。用于标示解码后的视频帧什么时候被显示出来。

> **在没有B帧的情况下，DTS和PTS的输出顺序是一样的，一旦存在B帧，PTS和DTS则会不同。** 因为解码的顺序和播放的顺序可能是不一致的。

GOP（Group Of Picture）就是两个 I 帧之间的距离，一般 GOP 设置得越大，画面的效果就会越好，到那时需要解码的时间就会越长。所以如果码率固定而 GOP 值越大，P/B帧 数量会越多，画面质量就会越高。

帧率：
帧率，即单位时间内帧的数量，单位为：帧/秒 或fps（frames per second）。帧率越高，每秒切换的图片就越多，画面越顺滑，过渡越自然。
帧率的一般以下几个典型值：

- 24/25 fps：1秒 24/25 帧，一般的电影帧率。
- 30/60 fps：1秒 30/60 帧，游戏的帧率，30帧可以接受，60帧会感觉更加流畅逼真。Android系统的高性能渲染就是以60帧为标准。
- 85 fps以上人眼基本无法察觉出来了，所以更高的帧率在视频里没有太大意义。

色彩空间：

我们都知道RGB是三原色，通过RGB三种基础色，可以混合出所有的颜色。

还有一张是YUV，这种色彩空间并不是我们熟悉的。这是一种亮度与色度分离的色彩格式。
早期的电视都是黑白的，即只有亮度值，即Y。有了彩色电视以后，加入了UV两种色度，形成现在的YUV，也叫YCbCr。
Y：亮度，就是灰度值。除了表示亮度信号外，还含有较多的绿色通道量。
U：蓝色通道与亮度的差值。
V：红色通道与亮度的差值。

因为人眼对亮度敏感，对色度不敏感，因此减少部分UV的数据量，人眼却无法感知出来，这样可以通过压缩UV的分辨率，在不影响观感的前提下，减小视频的体积。大大提高传输的效率和节省带宽。
关于YUV这里我就不多说了，后面我会专门写一篇文章介绍。比如YUV444，YUV422，YUV420和YUV420sp以及YUV和RGB是如何转换的等等。

视频编码格式：

视频编码格式有很多，比如H26x系列和MPEG系列的编码，这些编码格式都是为了适应时代发展而出现的。
其中，H26x（1/2/3/4/5）系列由ITU（International Telecommunication Union）国际电传视讯联盟主导的。
MPEG（1/2/3/4）系列由MPEG（Moving Picture Experts Group, ISO旗下的组织）主导。
H264是目前最主流的视频编码标准，目前大多数的视频和流媒体都是使用这种编码格式。

> H264编码算法是十分复杂，不是三言两语能够讲清楚的，也不在我的能力范围只能，我们要做到的就是知道怎么使用就好了。

## 3.编解码

编码：编码就是将原始音频数据也就是PCM压缩的一个过程；或者是将原始的视频数据RGB或YUV压缩的一个过程。

解码：解码就是编码一个逆过程，比如将编码后的数据AAC解码成PCM给播放器播放；或者将编码后的H264数据解码成YUV或RGB给播放器渲染的过程。

编解码又分为硬件编解码和软件编解码。

软件编解码就是指利用CPU的计算能力来进行编解码码，通常如果CPU的能力不是很强的时候，一则编解码速度会比较慢，二则手机可能出现发热现象。但是，由于使用统一的算法，兼容性会很好。

硬件编解码解码，指的是利用手机上专门的解码芯片来加速解码。通常硬解码的解码速度会快很多，但是由于硬解码由各个厂家实现，质量参差不齐，非常容易出现兼容性问题。

## 4.封装格式

封装格式业界也有人称音视频容器，比如我们经常看到的视频后缀名：`mp4、rmvb、avi、mkv、mov`等就是音视频的容器，它们将音频和视频甚至是字幕一起打包进去，封装成一个文件。

原文作者：零声音视频开发

原文链接：https://zhuanlan.zhihu.com/p/431679639

# 【NO.411】Linux操作系统原理—内核网络协议栈

## 1.前言

本文主要记录 Linux 内核网络协议栈的运行原理

## 2.数据报文的封装与分用

![img](https://pic4.zhimg.com/80/v2-28505bf69ef2e76b47a2c31e5d3481fb_720w.webp)

封装：当应用程序用 TCP 协议传送数据时，数据首先进入内核网络协议栈中，然后逐一通过 TCP/IP 协议族的每层直到被当作一串比特流送入网络。对于每一层而言，对收到的数据都会封装相应的协议首部信息（有时还会增加尾部信息）。TCP 协议传给 IP 协议的数据单元称作 TCP 报文段，或简称 TCP 段（TCP segment）。IP 传给数据链路层的数据单元称作 IP 数据报（IP datagram），最后通过以太网传输的比特流称作帧（Frame）。

![img](https://pic3.zhimg.com/80/v2-f45af88eace4ffafb979790fbc483886_720w.webp)

**分用**：当目的主机收到一个以太网数据帧时，数据就开始从内核网络协议栈中由底向上升，同时去掉各层协议加上的报文首部。每层协议都会检查报文首部中的协议标识，以确定接收数据的上层协议。这个过程称作分用。

![img](https://pic3.zhimg.com/80/v2-684f266a135e0f5953aa937bd13b3daa_720w.webp)

## 3.Linux 内核网络协议栈

**协议栈的全景图**

![img](https://pic4.zhimg.com/80/v2-90880724df8d8361eeeb7f7ec4ed3967_720w.webp)

**协议栈的分层结构**

![img](https://pic1.zhimg.com/80/v2-be83ffcae8fac5e1a23a49f91dc0d234_720w.webp)

![img](https://pic3.zhimg.com/80/v2-0b2e1ea0b0237237565bc368e8a55e56_720w.webp)

逻辑抽象层级：

物理层：主要提供各种连接的物理设备，如各种网卡，串口卡等。

链路层：主要提供对物理层进行访问的各种接口卡的驱动程序，如网卡驱动等。

网路层：是负责将网络数据包传输到正确的位置，最重要的网络层协议是 IP 协议，此外还有如 ICMP，ARP，RARP 等协议。

传输层：为应用程序之间提供端到端连接，主要为 TCP 和 UDP 协议。

应用层：顾名思义，主要由应用程序提供，用来对传输数据进行语义解释的 “人机交互界面层”，比如 HTTP，SMTP，FTP 等协议。

协议栈实现层级：

硬件层（Physical device hardware）：又称驱动程序层，提供连接硬件设备的接口。

设备无关层（Device agnostic interface）：又称设备接口层，提供与具体设备无关的驱动程序抽象接口。这一层的目的主要是为了统一不同的接口卡的驱动程序与网络协议层的接口，它将各种不同的驱动程序的功能统一抽象为几个特殊的动作，如 open，close，init 等，这一层可以屏蔽底层不同的驱动程序。

网络协议层（Network protocols）：对应 IP layer 和 Transport layer。毫无疑问，这是整个内核网络协议栈的核心。这一层主要实现了各种网络协议，最主要的当然是 IP，ICMP，ARP，RARP，TCP，UDP 等。

协议无关层（Protocol agnostic interface），又称协议接口层，本质就是 SOCKET 层。这一层的目的是屏蔽网络协议层中诸多类型的网络协议（主要是 TCP 与 UDP 协议，当然也包括 RAW IP， SCTP 等等），以便提供简单而同一的接口给上面的系统调用层调用。简单的说，不管我们应用层使用什么协议，都要通过系统调用接口来建立一个 SOCKET，这个 SOCKET 其实是一个巨大的 sock 结构体，它和下面的网络协议层联系起来，屏蔽了不同的网络协议，通过系统调用接口只把数据部分呈献给应用层。

BSD（Berkeley Software Distribution）socket：BSD Socket 层，提供统一的 SOCKET 操作接口，与 socket 结构体关系紧密。

INET（指一切支持 IP 协议的网络） socket：INET socket 层，调用 IP 层协议的统一接口，与 sock 结构体关系紧密。

系统调用接口层（System call interface），实质是一个面向用户空间（User Space）应用程序的接口调用库，向用户空间应用程序提供使用网络服务的接口。

![img](https://pic3.zhimg.com/80/v2-ac0f6a75fde2252703893159d47a847e_720w.webp)

**协议栈的数据结构**

![img](https://pic3.zhimg.com/80/v2-0d7918808a5162845d51dbd1c5a49a46_720w.webp)

msghdr：描述了从应用层传递下来的消息格式，包含有用户空间地址，消息标记等重要信息。

iovec：描述了用户空间地址的起始位置。

file：描述文件属性的结构体，与文件描述符一一对应。

file_operations：文件操作相关结构体，包括 read()、write()、open()、ioctl() 等。

socket：向应用层提供的 BSD socket 操作结构体，协议无关，主要作用为应用层提供统一的 Socket 操作。

sock：网络层 sock，定义与协议无关操作，是网络层的统一的结构，传输层在此基础上实现了 inet_sock。

sock_common：最小网络层表示结构体。

inet_sock：表示层结构体，在 sock 上做的扩展，用于在网络层之上表示 inet 协议族的的传输层公共结构体。

udp_sock：传输层 UDP 协议专用 sock 结构，在传输层 inet_sock 上扩展。

proto_ops：BSD socket 层到 inet_sock 层接口，主要用于操作 socket 结构。

proto：inet_sock 层到传输层操作的统一接口，主要用于操作 sock 结构。

net_proto_family：用于标识和注册协议族，常见的协议族有 IPv4、IPv6。

softnet_data：内核为每个 CPU 都分配一个这样的 softnet_data 数据空间。每个 CPU 都有一个这样的队列，用于接收数据包。

sk_buff：描述一个帧结构的属性，包含 socket、到达时间、到达设备、各层首部大小、下一站路由入口、帧长度、校验和等等。

sk_buff_head：数据包队列结构。

net_device：这个巨大的结构体描述一个网络设备的所有属性，数据等信息。

inet_protosw：向 IP 层注册 socket 层的调用操作接口。

inetsw_array：socket 层调用 IP 层操作接口都在这个数组中注册。

sock_type：socket 类型。

IPPROTO：传输层协议类型 ID。

net_protocol：用于传输层协议向 IP 层注册收包的接口。

packet_type：以太网数据帧的结构，包括了以太网帧类型、处理方法等。

rtable：路由表结构，描述一个路由表的完整形态。

rt_hash_bucket：路由表缓存。

dst_entry：包的去向接口，描述了包的去留，下一跳等路由关键信息。

napi_struct：NAPI 调度的结构。NAPI 是 Linux 上采用的一种提高网络处理效率的技术，它的核心概念就是不采用中断的方式读取数据，而代之以首先采用中断唤醒数据接收服务，然后采用 poll 的方法来轮询数据。NAPI 技术适用于高速率的短长度数据包的处理。

网络协议栈初始化流程

这需要从内核启动流程说起。当内核完成自解压过程后进入内核启动流程，这一过程先在 arch/mips/kernel/head.S 程序中，这个程序负责数据区（BBS）、中断描述表（IDT）、段描述表（GDT）、页表和寄存器的初始化，程序中定义了内核的入口函数 kernel_entry()、kernel_entry() 函数是体系结构相关的汇编代码，它首先初始化内核堆栈段为创建系统中的第一过程进行准备，接着用一段循环将内核映像的未初始化的数据段清零，最后跳到 start_kernel() 函数中初始化硬件相关的代码，完成 Linux Kernel 环境的建立。

start_kenrel() 定义在 init/main.c 中，真正的内核初始化过程就是从这里才开始。函数 start_kerenl() 将会调用一系列的初始化函数，如：平台初始化，内存初始化，陷阱初始化，中断初始化，进程调度初始化，缓冲区初始化，完成内核本身的各方面设置，目的是最终建立起基本完整的 Linux 内核环境。

start_kernel() 中主要函数及调用关系如下：

![img](https://pic1.zhimg.com/80/v2-bd6296e70355f4019c8a5380fae5b420_720w.webp)

start_kernel() 的过程中会执行 socket_init() 来完成协议栈的初始化，实现如下：

```text
void sock_init(void)//网络栈初始化
{
	int i;
 
	printk("Swansea University Computer Society NET3.019\n");
 
	/*
	 *	Initialize all address (protocol) families. 
	 */
	 
	for (i = 0; i < NPROTO; ++i) pops[i] = NULL;
 
	/*
	 *	Initialize the protocols module. 
	 */
 
	proto_init();
 
#ifdef CONFIG_NET
	/* 
	 *	Initialize the DEV module. 
	 */
 
	dev_init();
  
	/*
	 *	And the bottom half handler 
	 */
 
	bh_base[NET_BH].routine= net_bh;
	enable_bh(NET_BH);
#endif  
}
```

![img](https://pic3.zhimg.com/80/v2-557a082f52c0cfe6bb2ba825889bf502_720w.webp)

sock_init() 包含了内核协议栈的初始化工作：

sock_init：Initialize sk_buff SLAB cache，注册 SOCKET 文件系统。

net_inuse_init：为每个 CPU 分配缓存。

proto_init：在 /proc/net 域下建立 protocols 文件，注册相关文件操作函数。

net_dev_init：建立 netdevice 在 /proc/sys 相关的数据结构，并且开启网卡收发中断；为每个 CPU 初始化一个数据包接收队列（softnet_data），包接收的回调；注册本地回环操作，注册默认网络设备操作。

inet_init：注册 INET 协议族的 SOCKET 创建方法，注册 TCP、UDP、ICMP、IGMP 接口基本的收包方法。为 IPv4 协议族创建 proc 文件。此函数为协议栈主要的注册函数：

rc = proto_register(&udp_prot, 1);：注册 INET 层 UDP 协议，为其分配快速缓存。

(void)sock_register(&inet_family_ops);：向 static const struct net_proto_family *net_families[NPROTO] 结构体注册 INET 协议族的操作集合（主要是 INET socket 的创建操作）。

inet_add_protocol(&udp_protocol, IPPROTO_UDP) < 0;：向 externconst struct net_protocol *inet_protos[MAX_INET_PROTOS] 结构体注册传输层 UDP 的操作集合。

static struct list_head inetsw[SOCK_MAX]; for (r = &inetsw[0]; r < &inetsw[SOCK_MAX];++r) INIT_LIST_HEAD(r);：初始化 SOCKET 类型数组，其中保存了这是个链表数组，每个元素是一个链表，连接使用同种 SOCKET 类型的协议和操作集合。

for (q = inetsw_array; q < &inetsw_array[INETSW_ARRAY_LEN]; ++q)：

inet_register_protosw(q);：向 sock 注册协议的的调用操作集合。

arp_init();：启动 ARP 协议支持。

ip_init();：启动 IP 协议支持。

udp_init();：启动 UDP 协议支持。

dev_add_pack(&ip_packet_type);：向 ptype_base[PTYPE_HASH_SIZE]; 注册 IP 协议的操作集合。

socket.c 提供的系统调用接口。

![img](https://pic4.zhimg.com/80/v2-9c2f95b6325c5d96d6d7a8445c92c6d7_720w.webp)

![img](https://pic2.zhimg.com/80/v2-1868dd3069373777f1f1e8817b8d6349_720w.webp)

协议栈初始化完成后再执行 dev_init()，继续设备的初始化。

**Socket 创建流程**

![img](https://pic1.zhimg.com/80/v2-e2f67e80e2d7cff73c00c8b9b58d78dc_720w.webp)

**协议栈收包流程概述**

硬件层与设备无关层：硬件监听物理介质，进行数据的接收，当接收的数据填满了缓冲区，硬件就会产生中断，中断产生后，系统会转向中断服务子程序。在中断服务子程序中，数据会从硬件的缓冲区复制到内核的空间缓冲区，并包装成一个数据结构（sk_buff），然后调用对驱动层的接口函数 netif_rx() 将数据包发送给设备无关层。该函数的实现在 net/inet/dev.c 中，采用了 bootom half 技术，该技术的原理是将中断处理程序人为的分为两部分，上半部分是实时性要求较高的任务，后半部分可以稍后完成，这样就可以节省中断程序的处理时间，整体提高了系统的性能。

NOTE：在整个协议栈实现中 dev.c 文件的作用重大，它衔接了其下的硬件层和其上的网络协议层，可以称它为链路层模块，或者设备无关层的实现。

网络协议层：就以 IP 数据报为例，从设备无关层向网络协议层传递时会调用 ip_rcv()。该函数会根据 IP 首部中使用的传输层协议来调用相应协议的处理函数。UDP 对应 udp_rcv()、TCP 对应 tcp_rcv()、ICMP 对应 icmp_rcv()、IGMP 对应 igmp_rcv()。以 tcp_rcv() 为例，所有使用 TCP 协议的套接字对应的 sock 结构体都被挂入 tcp_prot 全局变量表示的 proto 结构之 sock_array 数组中，采用以本地端口号为索引的插入方式。所以，当 tcp_rcv() 接收到一个数据包，在完成必要的检查和处理后，其将以 TCP 协议首部中目的端口号为索引，在 tcp_prot 对应的 sock 结构体之 sock_array 数组中得到正确的 sock 结构体队列，再辅之以其他条件遍历该队列进行对应 sock 结构体的查询，在得到匹配的 sock 结构体后，将数据包挂入该 sock 结构体中的缓存队列中（由 sock 结构体中的 receive_queue 字段指向），从而完成数据包的最终接收。

NOTE：虽然这里的 ICMP、IGMP 通常被划分为网络层协议，但是实际上他们都封装在 IP 协议里面，作为传输层对待。

协议无关层和系统调用接口层：当用户需要接收数据时，首先根据文件描述符 inode 得到 socket 结构体和 sock 结构体，然后从 sock 结构体中指向的队列 recieve_queue 中读取数据包，将数据包 copy 到用户空间缓冲区。数据就完整的从硬件中传输到用户空间。这样也完成了一次完整的从下到上的传输。

**协议栈发包流程概述**

1、应用层可以通过系统调用接口层或文件操作来调用内核函数，BSD socket 层的 sock_write() 会调用 INET socket 层的 inet_wirte()。INET socket 层会调用具体传输层协议的 write 函数，该函数是通过调用本层的 inet_send() 来实现的，inet_send() 的 UDP 协议对应的函数为 udp_write()。

2、在传输层 udp_write() 调用本层的 udp_sendto() 完成功能。udp_sendto() 完成 sk_buff 结构体相应的设置和报头的填写后会调用 udp_send() 来发送数据。而在 udp_send() 中，最后会调用 ip_queue_xmit() 将数据包下放的网络层。

3、在网络层，函数 ip_queue_xmit() 的功能是将数据包进行一系列复杂的操作，比如是检查数据包是否需要分片，是否是多播等一系列检查，最后调用 dev_queue_xmit() 发送数据。

4、在链路层中，函数调用会调用具体设备提供的发送函数来发送数据包，e.g. dev->hard_start_xmit(skb, dev);。具体设备的发送函数在协议栈初始化的时候已经设置了。这里以 8390 网卡为例来说明驱动层的工作原理，在 net/drivers/8390.c 中函数 ethdev_init() 的设置如下：

```text
/* Initialize the rest of the 8390 device structure. */  
int ethdev_init(struct device *dev)  
{  
    if (ei_debug > 1)  
        printk(version);  
      
    if (dev->priv == NULL) { //申请私有空间  
        struct ei_device *ei_local; //8390 网卡设备的结构体  
          
        dev->priv = kmalloc(sizeof(struct ei_device), GFP_KERNEL); //申请内核内存空间  
        memset(dev->priv, 0, sizeof(struct ei_device));  
        ei_local = (struct ei_device *)dev->priv;  
#ifndef NO_PINGPONG  
        ei_local->pingpong = 1;  
#endif  
    }  
      
    /* The open call may be overridden by the card-specific code. */  
    if (dev->open == NULL)  
        dev->open = &ei_open; // 设备的打开函数  
    /* We should have a dev->stop entry also. */  
    dev->hard_start_xmit = &ei_start_xmit; // 设备的发送函数，定义在 8390.c 中  
    dev->get_stats   = get_stats;  
#ifdef HAVE_MULTICAST  
    dev->set_multicast_list = &set_multicast_list;  
#endif  
  
    ether_setup(dev);  
          
    return 0;  
}
```

**UDP 的收发包流程总览**

![img](https://pic3.zhimg.com/80/v2-754f589893dd553d85c7f1fdc157f146_720w.webp)

**内核中断收包流程**

![img](https://pic4.zhimg.com/80/v2-bcb33d3a3e788a1bfb9f758e9bc46e9f_720w.webp)

**UDP 收包流程**

![img](https://pic4.zhimg.com/80/v2-1f2c1515723d09144ab5b02f4b8121a3_720w.webp)

**UDP 发包流程**

![img](https://pic2.zhimg.com/80/v2-5fa444542cbdecf157242c9e0525eae9_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/379915285

作者：linux

# 【NO.412】从linux内核出发彻底弄懂socket底层的来龙去脉

## **1.socket与inode**

socket在Linux中对应的文件系统叫Sockfs,每创建一个socket,就在sockfs中创建了一个特殊的文件，同时创建了sockfs文件系统中的inode，该inode唯一标识当前socket的通信。

如下图所示，左侧窗口使用nc工具创建一个TCP连接；右侧找到该进程id(3384)，通过查看该进程下的描述符，可以看到"3 ->socket:[86851]",socket表示这是一个socket类型的fd，[86851]表示这个一个inode号，能够唯一标识当前的这个socket通信连接，进一步在该inode下查看"grep -i "86851" /proc/net/tcp”可以看到该TCP连接的所有信息(连接状态、IP地址等)，只不过是16进制显示。

![img](https://pic1.zhimg.com/80/v2-ff2f1d386cad6573dee7df7df11e46d8_720w.webp)

**在分析socket与inode之前，先通过ext4文件系统举例：**

在VFS层，即抽象层，所有的文件系统都使用struct inode结构体描述indoe，然而分配inode的方式都不同，如ext4文件系统的分配inode函数是ext4_alloc_inode，如下所示：

```text
static struct inode *ext4_alloc_inode(struct super_block *sb)
{
 struct ext4_inode_info *ei;

 ei = kmem_cache_alloc(ext4_inode_cachep, GFP_NOFS);
 if (!ei)
  return NULL;

 ei->vfs_inode.i_version = 1;
 spin_lock_init(&ei->i_raw_lock);
 INIT_LIST_HEAD(&ei->i_prealloc_list);
 spin_lock_init(&ei->i_prealloc_lock);
 ext4_es_init_tree(&ei->i_es_tree);
 rwlock_init(&ei->i_es_lock);
 INIT_LIST_HEAD(&ei->i_es_list);
 ei->i_es_all_nr = 0;
 ei->i_es_shk_nr = 0;
 ei->i_es_shrink_lblk = 0;
 ei->i_reserved_data_blocks = 0;
 ei->i_da_metadata_calc_len = 0;
 ei->i_da_metadata_calc_last_lblock = 0;
 spin_lock_init(&(ei->i_block_reservation_lock));
#ifdef CONFIG_QUOTA
 ei->i_reserved_quota = 0;
 memset(&ei->i_dquot, 0, sizeof(ei->i_dquot));
#endif
 ei->jinode = NULL;
 INIT_LIST_HEAD(&ei->i_rsv_conversion_list);
 spin_lock_init(&ei->i_completed_io_lock);
 ei->i_sync_tid = 0;
 ei->i_datasync_tid = 0;
 atomic_set(&ei->i_unwritten, 0);
 INIT_WORK(&ei->i_rsv_conversion_work, ext4_end_io_rsv_work);
 return &ei->vfs_inode;
}
```

从函数中可以看出来，函数其实是调用kmem_cache_alloc分配了 ext4_inode_info结构体（结构体如下所示），然后进行了一系列的初始化，最后返回的却是struct inode结构体（如上面代码的return &ei->vfs_inode）。如下结构体ext4_inode_info(ei)所示，vfs_inode是其struct inode结构体成员。

```text
struct ext4_inode_info {
 __le32 i_data[15]; /* unconverted */
 __u32 i_dtime;
 ext4_fsblk_t i_file_acl;

 ......
 struct rw_semaphore i_data_sem;
 struct rw_semaphore i_mmap_sem;
 struct inode vfs_inode;
 struct jbd2_inode *jinode;
  ......
};
```

![img](https://pic1.zhimg.com/80/v2-45d7b2caae1170cee18e1c1bf38f6a80_720w.webp)

再看一下：ext4_inode、ext4_inode_info、inode之间的关联，

ext4_inode如下所示，是磁盘上inode的结构

```text
struct ext4_inode {
 __le16 i_mode;  /* File mode */
 __le16 i_uid;  /* Low 16 bits of Owner Uid */
 __le32 i_size_lo; /* Size in bytes */
 __le32 i_atime; /* Access time */
 __le32 i_ctime; /* Inode Change time */
 __le32 i_mtime; /* Modification time */
 __le32 i_dtime; /* Deletion Time */
 __le16 i_gid;  /* Low 16 bits of Group Id */
 __le16 i_links_count; /* Links count */
 __le32 i_blocks_lo; /* Blocks count */
 __le32 i_flags; /* File flags */
 ......
}
```

ext4_inode_info是ext4文件系统的inode在内存中管理结构体：

```text
struct ext4_inode_info {
 __le32 i_data[15]; /* unconverted */
 __u32 i_dtime;
 ext4_fsblk_t i_file_acl;
 ......
};
```

inode是文件系统抽象层：

```text
struct inode {
    umode_t                 i_mode;
    unsigned short          i_opflags;
    kuid_t                  i_uid;
    kgid_t                  i_gid;
    unsigned int            i_flags;
 
    /* 对inode操作的具体方法
     * 不同的文件系统会注册不同的函数方法即可
     */
    const struct inode_operations   *i_op;
    struct super_block      *i_sb;
    struct address_space    *i_mapping;
 
    unsigned long           i_ino;
    
    union {
        const unsigned int i_nlink;
        unsigned int __i_nlink;
    };
    dev_t                   i_rdev;
    /* 文件大小 */
    loff_t                  i_size;
    /* 文件最后访问时间 */
    struct timespec         i_atime;
    /* 文件最后修改时间 */
    struct timespec         i_mtime;
    /* 文件创建时间 */
    struct timespec         i_ctime;
    spinlock_t              i_lock; /* i_blocks, i_bytes, maybe i_size */
    unsigned short          i_bytes;
    unsigned int            i_blkbits;
    enum rw_hint            i_write_hint;
    blkcnt_t                i_blocks;

    /* Misc */
    unsigned long           i_state;
    struct rw_semaphore     i_rwsem;
    unsigned long           dirtied_when;   /* jiffies of first dirtying */
    unsigned long           dirtied_time_when;
 
    /* inode通过以下结构被加入到的各种链表 */
    struct hlist_node       i_hash;
    struct list_head        i_io_list;      /* backing dev IO list */
 
    struct list_head        i_lru;          /* inode LRU list */
    struct list_head        i_sb_list;
    struct list_head        i_wb_list;      /* backing dev writeback list */
    union {
        struct hlist_head       i_dentry;
        struct rcu_head         i_rcu;
    };
    atomic64_t              i_version;
    atomic_t                i_count;
    atomic_t                i_dio_count;
    atomic_t                i_writecount;
 
    /* 对文件操作(如文件读写等)的具体方法
     * 实现虚拟文件系统的核心结构
     * 不同的文件系统只需要注册不同的函数方法即可
     */
    const struct file_operations    *i_fop; /* former ->i_op->default_file_ops */
    struct file_lock_context        *i_flctx;
    struct address_space    i_data;
    struct list_head        i_devices;
    union {
        struct pipe_inode_info  *i_pipe;
        struct block_device     *i_bdev;
        struct cdev             *i_cdev;
        char                    *i_link;
        unsigned                i_dir_seq;
    };
    __u32                   i_generation;

    void                    *i_private; /* fs or device private pointer */
} __randomize_layout;
```

三者的关系如下图，struct inode是VFS抽象层的表示，ext4_inode_info是ext4文件系统inode在内存中的表示，struct ext4_inode是文件系统inode在磁盘中的表示。

![img](https://pic1.zhimg.com/80/v2-1f9934c8b31e190e4b7dfd17f8c3a2b4_720w.webp)

**VFS采用C语言的方式实现了struct inode和struct ext4_inode_info继承关系，inode与ext4_inode_info是父类与子类的关系**，并且Linux内核实现了inode与ext4_inode_info父子类的互相转换，如下EXT4_I所示：

```text
static inline struct ext4_inode_info *EXT4_I(struct inode *inode)
{
 return container_of(inode, struct ext4_inode_info, vfs_inode);
}
```

**以上是以ext4为例进行了分析，下面将开始从socket与inode进行分析：**

sockfs是虚拟文件系统，所以在磁盘上不存在inode的表示，在内核中有struct socket_alloc来表示内存中sockfs文件系统inode的相关结构体：

```text
struct socket_alloc {
 struct socket socket;
 struct inode vfs_inode;
};
```

**struct socket与struct inode的关系如下图，正如ext4文件系统中struct ext4_inode_info与struct inode的关系类似，inode和socket_alloc结构体是父类与子类的关系。**

![img](https://pic1.zhimg.com/80/v2-624c904338f5ee441997bdcac8233458_720w.webp)

从上面分析ext4文件系统分配inode时，是通过ext4_alloc_inode函数分配了ext4_inode_info结构体，并初始化结构体成员，函数最后返回的是ext4_inode_info中的struct inode成员。sockfs文件系统也类似，sockfs文件系统分配inode时，创建的是socket_alloc结构体，在函数最后返回的是struct inode。

如下所示alloc_inode是分配inode结构体的回调函数接口。

```text
static const struct super_operations sockfs_ops = {
 .alloc_inode = sock_alloc_inode,
 .destroy_inode = sock_destroy_inode,
 .statfs  = simple_statfs,
}
```

sockfs文件系统的inode分配函数是sock_alloc_inode，如下所示：

```text
static struct inode *sock_alloc_inode(struct super_block *sb)
{
 struct socket_alloc *ei;
 struct socket_wq *wq;

 ei = kmem_cache_alloc(sock_inode_cachep, GFP_KERNEL);
 if (!ei)
  return NULL;
 wq = kmalloc(sizeof(*wq), GFP_KERNEL);
 if (!wq) {
  kmem_cache_free(sock_inode_cachep, ei);
  return NULL;
 }
 init_waitqueue_head(&wq->wait);
 wq->fasync_list = NULL;
 wq->flags = 0;
 RCU_INIT_POINTER(ei->socket.wq, wq);

 ei->socket.state = SS_UNCONNECTED;
 ei->socket.flags = 0;
 ei->socket.ops = NULL;
 ei->socket.sk = NULL;
 ei->socket.file = NULL;

 return &ei->vfs_inode;
}
```

sock_alloc_inode函数分配了socket_alloc结构体，也就意味着分配了struct socket和struct inode,并最终返回了socket_alloc结构体成员inode。

故struct socket这个字段出生的时候其实就和一个struct inode结构体伴生出来的，它们俩共同封装在struct socket_alloc中，由sockfs的sock_alloc_inode函数分配的,函数返回的是struct inode结构体.和ext4文件系统类型类似。**sockfs文件系统也实现了struct inode与struct socket的转换:**

```text
static inline struct socket *SOCKET_I(struct inode *inode)
{
 return &container_of(inode, struct socket_alloc, vfs_inode)->socket;
}
```

## **2.socket的创建与初始化**

首先看一下struct socket在内核中的定义：

```text
struct socket {
 socket_state  state;//socket状态

 short   type; //socket类型

 unsigned long  flags;//socket的标志位

 struct socket_wq __rcu *wq;

 struct file  *file;//与socket关联的文件指针
 struct sock  *sk;//套接字的核心，面向底层网络具体协议
 const struct proto_ops *ops;//socket函数操作集
};
```

**在内核中还有struct sock结构体，在struct socket中可以看到那么它们的关系是什么？**

1、socket面向上层，sock面向下层的具体协议

2、socket是内核抽象出的一个通用结构体，主要是设置了一些跟fs相关的字段，而真正跟网络通信相关的字段结构体是struct sock

3、struct sock是套接字的核心，是对底层具体协议做的一层抽象封装，比如TCP协议，struct sock结构体中的成员sk_prot会赋值为tcp_prot,UDP协议会赋值为udp_prot。

(关于更多struct sock的分析将在以后的文章中分析)

**创建socket的系统调用：**在用户空间创建了一个socket后，返回值是一个文件描述符。在SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)最后调用sock_map_fd进行关联，其中返回的就是用户空间获取的文件描述符fd，sock就是调用sock_create创建成功的socket.

```text
SYSCALL_DEFINE3(socket, int, family, int, type, int, protocol)
{
 int retval;
 struct socket *sock;
 int flags;

 /* Check the SOCK_* constants for consistency.  */
 BUILD_BUG_ON(SOCK_CLOEXEC != O_CLOEXEC);
 BUILD_BUG_ON((SOCK_MAX | SOCK_TYPE_MASK) != SOCK_TYPE_MASK);
 BUILD_BUG_ON(SOCK_CLOEXEC & SOCK_TYPE_MASK);
 BUILD_BUG_ON(SOCK_NONBLOCK & SOCK_TYPE_MASK);

 flags = type & ~SOCK_TYPE_MASK;
 if (flags & ~(SOCK_CLOEXEC | SOCK_NONBLOCK))
  return -EINVAL;
 type &= SOCK_TYPE_MASK;

 if (SOCK_NONBLOCK != O_NONBLOCK && (flags & SOCK_NONBLOCK))
  flags = (flags & ~SOCK_NONBLOCK) | O_NONBLOCK;

 retval = sock_create(family, type, protocol, &sock);
 if (retval < 0)
  return retval;

 return sock_map_fd(sock, flags & (O_CLOEXEC | O_NONBLOCK));
}
```

socket的创建将调用sock_create函数：

```text
int sock_create(int family, int type, int protocol, struct socket **res)
{
 return __sock_create(current->nsproxy->net_ns, family, type, protocol, res, 0);
}
```

__sock_create函数调用sock_alloc函数分配socket结构和文件节点：

```text
int __sock_create(struct net *net, int family, int type, int protocol,
    struct socket **res, int kern)
{
 int err;
 struct socket *sock;
 const struct net_proto_family *pf;
  //检查family的字段范围
 if (family < 0 || family >= NPROTO)
  return -EAFNOSUPPORT;
 if (type < 0 || type >= SOCK_MAX)
  return -EINVAL;

 ......
 sock = sock_alloc();//分配socket和inode，返回sock
 if (!sock) {
  net_warn_ratelimited("socket: no more sockets\n");
  return -ENFILE; /* Not exactly a match, but its the
       closest posix thing */
 }

 sock->type = type;
  ......
 rcu_read_lock();
 pf = rcu_dereference(net_families[family]);//获取协议族family对应的操作表
 err = -EAFNOSUPPORT;
 if (!pf)
  goto out_release;

 if (!try_module_get(pf->owner))
  goto out_release;

 /* Now protected by module ref count */
 rcu_read_unlock();

 err = pf->create(net, sock, protocol, kern);//调用family协议族的socket创建函数
 if (err < 0)
  goto out_module_put;


 if (!try_module_get(sock->ops->owner))
  goto out_module_busy;

 ......
}
```

socket结构体的创建在sock_alloc()函数中：

```text
struct socket *sock_alloc(void)
{
 struct inode *inode;
 struct socket *sock;

 inode = new_inode_pseudo(sock_mnt->mnt_sb);
 if (!inode)
  return NULL;

 sock = SOCKET_I(inode);

 inode->i_ino = get_next_ino();
 inode->i_mode = S_IFSOCK | S_IRWXUGO;
 inode->i_uid = current_fsuid();
 inode->i_gid = current_fsgid();
 inode->i_op = &sockfs_inode_ops;

 this_cpu_add(sockets_in_use, 1);
 return sock;
}
```

new_inode_pseudo中通过继续调用sockfs文件系统中的sock_alloc_inode函数完成struct socket_alloc的创建并返回其结构体成员struct inode。

然后调用SOCKT_I函数返回对应的struct socket。

在_sock_create中：pf->create(net, sock, protocol, kern);

通过相应的协议族，进一步调用不同的socket创建函数。pf是struct net_proto_family结构体，如下所示：

```text
struct net_proto_family {
 int  family;
 int  (*create)(struct net *net, struct socket *sock,
      int protocol, int kern);
 struct module *owner;
};
```

net_families[]数组里存放的是各个协议族的信息，以family字段作为下标，对应的值为net_pro_family结构体。此处我们针对TCP协议分析，因此我们family字段是AF_INET，pf->create将调用inet_create函数继续完成底层struct sock等创建和初始化。

inet_create函数完成struct socket、struct inode、struct sock的创建与初始化后，**调用sock_map_fd(sock, flags & (O_CLOEXEC | O_NONBLOCK));完成socket与文件系统的关联，负责分配文件，并与socket进行绑定：**

1、调用sock_alloc_file，分配一个struct file，并将私有数据指针指向socket结构

2、fd_install 对应文件描述符和file

```text
static int sock_map_fd(struct socket *sock, int flags)
{
 struct file *newfile;
 int fd = get_unused_fd_flags(flags);//为socket分配文件号和文件结构
 if (unlikely(fd < 0)) {
  sock_release(sock);
  return fd;
 }

 newfile = sock_alloc_file(sock, flags, NULL);//分配file对象
 if (likely(!IS_ERR(newfile))) {
  fd_install(fd, newfile);//使文件号与文件结构挂钩
  return fd;
 }

 put_unused_fd(fd);
 return PTR_ERR(newfile);
}
```

get_unused_fd_flags(flags)继续调用alloc_fd完成文件描述符的分配。

sock_alloc_file(sock, flags, NULL)分配一个struct file结构体

```text
struct file *sock_alloc_file(struct socket *sock, int flags, const char *dname)
{
 ......
 file = alloc_file(&path, FMODE_READ | FMODE_WRITE,
    &socket_file_ops);//分配struct file结构体
 if (IS_ERR(file)) {
  /* drop dentry, keep inode for a bit */
  ihold(d_inode(path.dentry));
  path_put(&path);
  /* ... and now kill it properly */
  sock_release(sock);
  return file;
 }

 sock->file = file; //socket通过其file字段进行关联
 file->f_flags = O_RDWR | (flags & O_NONBLOCK);
 file->private_data = sock;//file通过private_data与socket关联
 return file; //返回初始化、关联后的file结构体
}
```

其中file = alloc_file(&path, FMODE_READ | FMODE_WRITE,
&socket_file_ops);分配了file结构体并进行初始化：

```text
struct file *alloc_file(const struct path *path, fmode_t mode,
  const struct file_operations *fop)
{
 struct file *file;

 file = get_empty_filp();
 if (IS_ERR(file))
  return file;

 file->f_path = *path;
 file->f_inode = path->dentry->d_inode;
 file->f_mapping = path->dentry->d_inode->i_mapping;
 file->f_wb_err = filemap_sample_wb_err(file->f_mapping);
 if ((mode & FMODE_READ) &&
      likely(fop->read || fop->read_iter))
  mode |= FMODE_CAN_READ;
 if ((mode & FMODE_WRITE) &&
      likely(fop->write || fop->write_iter))
  mode |= FMODE_CAN_WRITE;
 file->f_mode = mode;
 file->f_op = fop;
 if ((mode & (FMODE_READ | FMODE_WRITE)) == FMODE_READ)
  i_readcount_inc(path->dentry->d_inode);
 return file;
}
```

其中file->f_op = fop，将socket_file_ops传递给文件操作表

```text
static const struct file_operations socket_file_ops = {
 .owner = THIS_MODULE,
 .llseek = no_llseek,
 .read_iter = sock_read_iter,
 .write_iter = sock_write_iter,
 .poll =  sock_poll,
 .unlocked_ioctl = sock_ioctl,
#ifdef CONFIG_COMPAT
 .compat_ioctl = compat_sock_ioctl,
#endif
 .mmap =  sock_mmap,
 .release = sock_close,
 .fasync = sock_fasync,
 .sendpage = sock_sendpage,
 .splice_write = generic_splice_sendpage,
 .splice_read = sock_splice_read,
};
```

以上操作完成了struct socket、struct sock、struct file等的创建、初始化、关联，并最终返回socket描述符fd

![img](https://pic2.zhimg.com/80/v2-e2b5ff5271eb567013ba3ab973bf8055_720w.webp)

**socket描述符fd和我们平时操作文件的文件描述符相同，那么会有一个疑问，可以看到struct file_operations socket_file_ops函数表中并没有提供write()和read()接口,只是看到read_iter，write_iter等接口，那么系统是如何处理的呢？**

以write()为例：

sys_write()->__vfs_write()

```text
ssize_t __vfs_write(struct file *file, const char __user *p, size_t count,
      loff_t *pos)
{
 if (file->f_op->write)//如果文件函数表结构体提供了write接口函数
  return file->f_op->write(file, p, count, pos);//调用它的write函数
 else if (file->f_op->write_iter)
  return new_sync_write(file, p, count, pos);//否则调用new_sync_write函数
 else
  return -EINVAL;
}
```

从__vfs_write函数中可以看出来，如果socket函数表中没有提供write接口函数，则调用new_sync_write:

```text
static ssize_t new_sync_write(struct file *filp, const char __user *buf, size_t len, loff_t *ppos)
{
 ......

 ret = call_write_iter(filp, &kiocb, &iter);
 ......
}
```

call_write_iter:

```text
static inline ssize_t call_write_iter(struct file *file, struct kiocb *kio,struct iov_iter *iter)
{
 return file->f_op->write_iter(kio, iter);//调用socket文件函数表的aio_write函数
}
```

从以上__vfs_write()分析，如果文件函数表结构提供了write接口函数则调用write函数，如果文件函数表结构没有提供write接口函数(如socket操作函数表中没有提供write接口)，则调用write_iter接口，即调用socket操作函数表中的sock_write_iter。就这样通过socket fd进行普通文件系统那样通过描述符进行读写等。

用户得到socket fd,可以进行地址绑定、发送以及接收数据等操作，在Linux内核中有相关的函数完成从socket fd到struct socket、struct file的转换：

```text
static struct socket *sockfd_lookup_light(int fd, int *err, int *fput_needed)
{
 struct fd f = fdget(fd);//通过socket fd获取struct fd结构体，struct fd结构体中有struct file结构
 struct socket *sock;

 *err = -EBADF;
 if (f.file) {
  sock = sock_from_file(f.file, err);//通过获取的struct file结构体获取相应的struct socket指针
  if (likely(sock)) {
   *fput_needed = f.flags;
   return sock;
  }
  fdput(f);
 }
 return NULL;
}
```

fdget()函数从当前进程的files_struct结构中找到网络文件系统中的file文件指针，并封装在struct fd结构体中。sock_from函数通过得到的file结构体得到对应的socket结构指针。sock_from函数如下：

```text
struct socket *sock_from_file(struct file *file, int *err)
{
 if (file->f_op == &socket_file_ops)
  return file->private_data; /* set in sock_map_fd */

 *err = -ENOTSOCK;
 return NULL;
}
```

至此，socket底层来龙去脉的大体结构大概就分析到这。

原文地址：https://zhuanlan.zhihu.com/p/456895712

作者：linux

# 【NO.413】深入理解epoll背后的原理

## **1.简介**

Epoll 是个很老的知识点，是后端工程师的经典必修课。这种知识具备的特点就是研究的人多，所以研究的趋势就会越来越深。当然分享的人也多，由于分享者水平参差不齐，也产生的大量错误理解。

今天我再次分享 epoll，肯定不会列个表格，对比一下差异，那就太无聊了。我将从线程阻塞的原理，中断优化，网卡处理数据过程出发，深入的介绍 epoll 背后的原理，最后还会 diss 一些流行的观点。相信无论你是否已经熟悉 epoll，本文都会对你有价值。

## **2.引言**

正文开始前，先问大家几个问题。

1、epoll 性能到底有多高。很多文章介绍 epoll 可以轻松处理几十万个连接。而传统 IO 只能处理几百个连接 是不是说 epoll 的性能就是传统 IO 的千倍呢？

2、很多文章把网络 IO 划分为阻塞，非阻塞，同步，异步。并表示：非阻塞的性能比阻塞性能好，异步的性能比同步性能好。

- 如果说阻塞导致性能低，那传统 IO 为什么要阻塞呢？
- epoll 是否需要阻塞呢？
- Java 的 NIO 和 AIO 底层都是 epoll 实现的，这又怎么理解同步和异步的区别？

3、都是 IO 多路复用。

- 既生瑜何生亮，为什么会有 select，poll 和 epoll 呢？
- 为什么 epoll 比 select 性能高？

PS：

本文共包含三大部分：**初识 epoll、epoll 背后的原理 、Diss 环节**。

本文的重点是介绍原理，建议读者的关注点尽量放在：“为什么”。

Linux 下进程和线程的区别其实并不大，尤其是在讨论原理和性能问题时，因此本文中“进程”和“线程”两个词是混用的。

## **3.初识 epoll**

epoll 是 Linux 内核的可扩展 I/O 事件通知机制，其最大的特点就是性能优异。下图是 **libevent**(一个知名的异步事件处理软件库)对 select，poll，epoll ，kqueue 这几个 I/O 多路复用技术做的性能测试。

![img](https://pic1.zhimg.com/80/v2-9237027e633e254420048083b984f8b0_720w.webp)

很多文章在描述 epoll 性能时都引用了这个基准测试，但少有文章能够清晰的解释这个测试结果。

这是一个限制了100个活跃连接的基准测试，每个连接发生1000次读写操作为止。纵轴是请求的响应时间，横轴是持有的 socket 句柄数量。随着句柄数量的增加，epoll 和 kqueue 响应时间几乎无变化，而 poll 和 select 的响应时间却增长了非常多。

可以看出来，epoll 性能是很高的，并且随着监听的文件描述符的增加，epoll 的优势更加明显。

不过，这里限制的100个连接很重要。epoll 在应对大量网络连接时，只有活跃连接很少的情况下才能表现的性能优异。换句话说，epoll 在处理大量非活跃的连接时性能才会表现的优异。如果15000个 socket 都是活跃的，epoll 和 select 其实差不了太多。

**为什么 epoll 的高性能有这样的局限性？**

问题好像越来越多了，看来我们需要更深入的研究了。

## **4.epoll背后的原理**

### **4.1 阻塞**

**4.1.1 为什么阻塞**我们以网卡接收数据举例，回顾一下之前我分享过的网卡接收数据的过程。

![img](https://pic2.zhimg.com/80/v2-ffcc372996e299c87318ae4a99e74f45_720w.webp)

为了方便理解，我尽量简化技术细节，可以把接收数据的过程分为4步：

1. NIC（网卡） 接收到数据，通过 DMA 方式写入内存(Ring Buffer 和 sk_buff)。
2. NIC 发出中断请求（IRQ），告诉内核有新的数据过来了。
3. Linux 内核响应中断，系统切换为内核态，处理 Interrupt Handler，从RingBuffer 拿出一个 Packet， 并处理协议栈，填充 Socket 并交给用户进程。
4. 系统切换为用户态，用户进程处理数据内容。

网卡何时接收到数据是依赖发送方和传输路径的，这个延迟通常都很高，是毫秒(ms)级别的。而应用程序处理数据是纳秒(ns)级别的。也就是说整个过程中，内核态等待数据，处理协议栈是个相对很慢的过程。这么长的时间里，用户态的进程是无事可做的，因此用到了“阻塞（挂起）”。

**4.1.2 阻塞不占用 cpu**

阻塞是进程调度的关键一环，指的是进程在等待某事件发生之前的等待状态。请看下表，在 Linux 中，进程状态大致有7种（在 include/linux/sched.h 中有更多状态）：

![img](https://pic3.zhimg.com/80/v2-3f2761991013d3e802a5cd145ba32b5e_720w.webp)

从说明中其实就可以发现，“可运行状态”会占用 CPU 资源，另外创建和销毁进程也需要占用 CPU 资源（内核）。重点是，当进程被"阻塞/挂起"时，是不会占用 CPU 资源的。

换个角度来讲。为了支持多任务，Linux 实现了进程调度的功能（CPU 时间片的调度）。而这个时间片的切换，只会在“可运行状态”的进程间进行。因此“阻塞/挂起”的进程是不占用 CPU 资源的。

另外讲个知识点，为了方便时间片的调度，所有“可运行状态”状态的进程，会组成一个队列，就叫**“工作队列”**。

**4.1.3 阻塞的恢复**

内核当然可以很容易的修改一个进程的状态，问题是网络 IO 中，内核该修改那个进程的状态。

![img](https://pic3.zhimg.com/80/v2-065e5286f90b7f43d06bd22708348c7e_720w.webp)

socket 结构体，包含了两个重要数据：进程 ID 和端口号。进程 ID 存放的就是执行 connect，send，read 函数，被挂起的进程。在 socket 创建之初，端口号就被确定了下来，操作系统会维护一个端口号到 socket 的数据结构。

当网卡接收到数据时，数据中一定会带着端口号，内核就可以找到对应的 socket，并从中取得“挂起”进程的 ID。将进程的状态修改为“可运行状态”（加入到工作队列）。此时内核代码执行完毕，将控制权交还给用户态。通过正常的“CPU 时间片的调度”，用户进程得以处理数据。

**4.1.4 进程模型**

上面介绍的整个过程，基本就是 BIO（阻塞 IO）的基本原理了。用户进程都是独立的处理自己的业务，这其实是一种符合进程模型的处理方式。

### **4.2 上下文切换的优化**

上面介绍的过程中，有两个地方会造成频繁的上下文切换，效率可能会很低。

1. 如果频繁的收到数据包，NIC 可能频繁发出中断请求（IRQ）。CPU 也许在用户态，也许在内核态，也许还在处理上一条数据的协议栈。但无论如何，CPU 都要尽快的响应中断。这么做实际上非常低效，造成了大量的上下文切换，也可能导致用户进程长时间无法获得数据。（即使是多核，每次协议栈都没有处理完，自然无法交给用户进程）
2. 每个 Packet 对应一个 socket，每个 socket 对应一个用户态的进程。这些用户态进程转为“可运行状态”，必然要引起进程间的上下文切换。

**4.2.1 网卡驱动的 NAPI 机制**

在 NIC 上，解决频繁 IRQ 的技术叫做 New API(NAPI) 。原理其实特别简单，把 Interrupt Handler 分为两部分。

1. 函数名为 napi_schedule，专门快速响应 IRQ，只记录必要信息，并在合适的时机发出软中断 softirq。
2. 函数名为 netrxaction，在另一个进程中执行，专门响应 napi_schedule 发出的软中断，批量的处理 RingBuffer 中的数据。

所以使用了 NAPI 的驱动，接收数据过程可以简化描述为：

![img](https://pic1.zhimg.com/80/v2-6d98f27db58708143b3cc8503c30c360_720w.webp)

1. NIC 接收到数据，通过 DMA 方式写入内存(Ring Buffer 和 sk_buff)。
2. NIC 发出中断请求（IRQ），告诉内核有新的数据过来了。
3. driver 的 napi_schedule 函数响应 IRQ，并在合适的时机发出软中断（NET_RX_SOFTIRQ）
4. driver 的 net_rx_action 函数响应软中断，从 Ring Buffer 中批量拉取收到的数据。并处理协议栈，填充 Socket 并交给用户进程。
5. 系统切换为用户态，多个用户进程切换为“可运行状态”，按 CPU 时间片调度，处理数据内容。

一句话概括就是：等着收到一批数据，再一次批量的处理数据。

**4.2.2 单线程的 IO 多路复用**

内核优化“进程间上下文切换”的技术叫的“IO 多路复用”，思路和 NAPI 是很接近的。

每个 socket 不再阻塞读写它的进程，而是用一个专门的线程，批量的处理用户态数据，这样就减少了线程间的上下文切换。

![img](https://pic2.zhimg.com/80/v2-881b760e8afec9f0162a89aac48b74a9_720w.webp)

作为 IO 多路复用的一个实现，select 的原理也很简单。所有的 socket 统一保存执行 select 函数的（监视进程）进程 ID。任何一个 socket 接收了数据，都会唤醒“监视进程”。内核只要告诉“监视进程”，那些 socket 已经就绪，监视进程就可以批量处理了。

### **4.3 IO 多路复用的进化**

**4.3.1 对比 epoll 与 select**

select，poll 和 epoll 都是“IO 多路复用”，那为什么还会有性能差距呢？篇幅限制，这里我们只简单对比 select 和 epoll 的基本原理差异。

对于内核，同时处理的 socket 可能有很多，监视进程也可能有多个。所以监视进程每次“批量处理数据”，都需要告诉内核它“关心的 socket”。内核在唤醒监视进程时，就可以把“关心的 socket”中，就绪的 socket 传给监视进程。

换句话说，在执行系统调用 select 或 epoll_create 时，入参是“关心的 socket”，出参是“就绪的 socket”。

而 select 与 epoll 的区别在于：

- **select （一次O(n)查找）**

1. 每次传给内核一个用户空间分配的 fd_set 用于表示“关心的 socket”。其结构（相当于 bitset）限制了只能保存1024个 socket。
2. 每次 socket 状态变化，内核利用 fd_set 查询O(1)，就能知道监视进程是否关心这个 socket。
3. 内核是复用了 fd_set 作为出参，返还给监视进程（所以每次 select 入参需要重置）。
   然而监视进程必须遍历一遍 socket 数组O(n)，才知道哪些 socket 就绪了。

- **epoll （全是O(1)查找）**

1. 每次传给内核一个实例句柄。这个句柄是在内核分配的红黑树 rbr+双向链表 rdllist。只要句柄不变，内核就能复用上次计算的结果。
2. 每次 socket 状态变化，内核就可以快速从 rbr 查询O(1)，监视进程是否关心这个 socket。同时修改 rdllist，所以 rdllist 实际上是“就绪的 socket”的一个缓存。
3. 内核复制 rdllist 的一部分或者全部（LT 和 ET），到专门的 epoll_event 作为出参。
   所以监视进程，可以直接一个个处理数据，无需再遍历确认。

**Select 示例代码**

![img](https://pic3.zhimg.com/80/v2-e908826c9106431e4b199e116de01c5e_720w.webp)

**Epoll 示例代码**

![img](https://pic2.zhimg.com/80/v2-c6a66af317f4f90428973ebb44b987e1_720w.webp)

另外，epoll_create 底层实现，到底是不是红黑树，其实也不太重要（完全可以换成 hashtable）。重要的是 efd 是个指针，其数据结构完全可以对外透明的修改成任意其他数据结构。

**4.3.2 API 发布的时间线**

另外，我们再来看看网络 IO 中，各个 api 的发布时间线。就可以得到两个有意思的结论。

> 1983，socket 发布在 Unix(4.2 BSD) 
> 1983，select 发布在 Unix(4.2 BSD) 
> 1994，Linux的1.0，已经支持socket和select 
> 1997，poll 发布在 Linux 2.1.23 
> 2002，epoll发布在 Linux 2.5.44

1、socket 和 select 是同时发布的。这说明了，select 不是用来代替传统 IO 的。这是两种不同的用法(或模型)，适用于不同的场景。

2、select、poll 和 epoll，这三个“IO 多路复用 API”是相继发布的。这说明了，它们是 IO 多路复用的3个进化版本。因为 API 设计缺陷，无法在不改变 API 的前提下优化内部逻辑。所以用 poll 替代 select，再用 epoll 替代 poll。

### **4.4 总结**

我们花了三个章节，阐述 Epoll 背后的原理，现在用三句话总结一下。

1. 基于数据收发的基本原理，系统利用阻塞提高了 CPU 利用率。
2. 为了优化上线文切换，设计了“IO 多路复用”（和 NAPI）。
3. 为了优化“内核与监视进程的交互”，设计了三个版本的 API(select,poll,epoll)。

## **5.Diss 环节**

讲完“Epoll 背后的原理”，已经可以回答最初的几个问题。这已经是一个完整的文章，很多人劝我删掉下面的 diss 环节。

我的观点是：学习就是个研究+理解的过程。上面是研究，下面再讲一下我的个人“理解”，欢迎指正。

### **5.1 关于 IO 模型的分类**

关于阻塞，非阻塞，同步，异步的分类，这么分自然有其道理。但是在操作系统的角度来看**“这样分类，容易产生误解，并不好”**。

![img](https://pic3.zhimg.com/80/v2-1f97a729b85d23648d4e566a9b0bdd72_720w.webp)

**5.1.1 阻塞和非阻塞**

Linux 下所有的 IO 模型都是阻塞的，这是收发数据的基本原理导致的。阻塞用户线程是一种高效的方式。

你当然可以写一个程序，socket 设置成非阻塞模式，在不使用监视器的情况下，依靠死循环完成一次 IO 操作。但是这样做的效率实在是太低了，完全没有实际意义。

换句话说，阻塞不是问题，运行才是问题，运行才会消耗 CPU。IO 多路复用不是减少了阻塞，是减少了运行。上下文切换才是问题，IO 多路复用，通过减少运行的进程，有效的减少了上下文切换。

**5.1.2 同步和异步**

Linux 下所有的 IO 模型都是同步的。BIO 是同步的，select 同步的，poll 同步的，epoll 还是同步的。

Java 提供的 AIO，也许可以称作“异步”的。但是 JVM 是运行在用户态的，Linux 没有提供任何的异步支持。因此 JVM 提供的异步支持，和你自己封装成“异步”的框架是没有本质区别的（你完全可以使用 BIO 封装成异步框架）。

所谓的“同步“和”异步”只是两种事件分发器（event dispatcher）或者说是两个设计模式（Reactor 和 Proactor）。都是运行在用户态的，两个设计模式能有多少性能差异呢？

- Reactor 对应 java 的 NIO，也就是 Channel，Buffer 和 Selector 构成的核心的 API。
- Proactor对应 java 的 AIO，也就是 Async 组件和 Future 或 Callback 构成的核心的 API。

**5.1.3 我的分类**

我认为 IO 模型只分两类：

1. 更加符合程序员理解和使用的，进程模型；
2. 更加符合操作系统处理逻辑的，IO 多路复用模型。

对于“IO多路复用”的事件分发，又分为两类：Reactor 和 Proactor。

### **5.2 关于 mmap**

epoll 到底用没用到 mmap？

**答案：没有！**

这是个以讹传讹的谣言。其实很容易证明的，用 epoll 写个 demo。strace 一下就清楚了。

原文地址：https://zhuanlan.zhihu.com/p/448034877

作者：Linux

# 【NO.414】Linux下全新的异步I/O：io_uring详解

## 1.基本原理

io_uring 是 2019 年 5 月发布的 Linux 5.1 加入的一个重大特性 —— Linux 下的全新的异步 I/O 支持，希望能彻底解决长期以来 Linux AIO 的各种不足。

io_uring 实现异步 I/O 的方式其实是一个生产者-消费者模型：

1. 用户进程生产 I/O 请求，放入提交队列（Submission Queue，后续简称 SQ）。
2. 内核消费 SQ 中的 I/O 请求，完成后将结果放入完成队列（Completion Queue，后续简称 CQ）。
3. 用户进程从 CQ 中收割 I/O 结果。

SQ 和 CQ 是内核初始化 io_uring 实例的时候创建的。为了减少系统调用和减少用户进程与内核之间的数据拷贝，io_uring 使用 mmap 的方式让用户进程和内核共享 SQ 和 CQ 的内存空间。

另外，由于先提交的 I/O 请求不一定先完成，SQ 保存的其实是一个数组索引（数据类型 uint32），真正的 SQE（Submission Queue Entry）保存在一个独立的数组（SQ Array）。所以要提交一个 I/O 请求，得先在 SQ Array 中找到一个空闲的 SQE，设置好之后，将其数组索引放到 SQ 中。

用户进程、内核、SQ、CQ 和 SQ Array 之间的基本关系如下：

![img](https://pic2.zhimg.com/80/v2-4953b41578cfde16524d96a917846659_720w.webp)

## 2.初始化

```text
int io_uring_setup(int entries, struct io_uring_params *params);
```

内核提供了 io_uring_setup 系统调用来初始化一个 io_uring 实例。

io_uring_setup 的返回值是一个文件描述符，暂且称为 ring_fd，用于后续的 mmap 内存映射和其它相关系统调用的参数。

io_uring 会创建 SQ、CQ 和 SQ Array， entries 参数表示的是 SQ 和 SQ Array 的大小，CQ 的大小默认是 2 * entries。

params 参数既是输入参数，也是输出参数，其定义如下：

```text
struct io_uring_params {
    __u32 sq_entries;
    __u32 cq_entries;
    __u32 flags;
    __u32 sq_thread_cpu;
    __u32 sq_thread_idle;
    __u32 features;
    __u32 resv[4];
    struct io_sqring_offsets sq_off;
    struct io_cqring_offsets cq_off;
};

struct io_sqring_offsets {
    __u32 head;
    __u32 tail;
    __u32 ring_mask;
    __u32 ring_entries;
    __u32 flags;
    __u32 dropped;
    __u32 array;
    __u32 resv[3];
};

struct io_cqring_offsets {
    __u32 head;
    __u32 tail;
    __u32 ring_mask;
    __u32 ring_entries;
    __u32 overflow;
    __u32 cqes;
    __u32 flags;
    __u32 resv[3];
};
```

- flags、 sq_thread_cpu、 sq_thread_idle 是输入参数，用于设置 io_uring 的一些特性。

- resv[4] 是保留字段，我们暂且忽略。

- 其它参数都是由内核设置的输出参数，用户进程可能需要使用这些参数进行一些初始化和判断：

- - sq_entries 是提交队列的大小。
  - cq_entries 是完成队列的大小。
  - features 描述当前内核版本支持的 io_uring 特性。其中，IORING_FEAT_SINGLE_MMAP 是 io_uring 一个比较重要的特性：内核支持通过一次 mmap 完成 SQ 和 CQ 的内存映射，具体可以参考 liburing 的 io_uring_mmap。
  - sq_off 描述了 SQ 的一些属性的 offset。
  - cq_off 描述了 CQ 的一些属性的 offset。

用户进程需要根据相关参数对 SQ、CQ 和 SQ Array 进行 mmap 之后，才能和内核共享使用。下面是示例代码：

```text
int io_uring_mmap(int ring_fd, struct io_uring_params *p) {
  unsigned sq_ring_sz = p->sq_off.array + p->sq_entries * sizeof(unsigned);
  unsigned cq_ring_sz =
      p->cq_off.cqes + p->cq_entries * sizeof(struct io_uring_cqe);
  unsigned sq_array_size = p->sq_entries * sizeof(struct io_uring_sqe);

  // 建立 SQ 和 CQ 的内存映射
  if (p->features & IORING_FEAT_SINGLE_MMAP) {
    if (cq_ring_sz > sq_ring_sz) {
      sq_ring_sz = cq_ring_sz;
    }
    cq_ring_sz = sq_ring_sz;
  }

  void *sq_ring_ptr =
      mmap(nullptr, sq_ring_sz, PROT_READ | PROT_WRITE,
           MAP_SHARED | MAP_POPULATE, ring_fd, IORING_OFF_SQ_RING);
  if (sq_ring_ptr == MAP_FAILED) {
    return -errno;
  }

  void *cq_ring_ptr = nullptr;
  if (p->features & IORING_FEAT_SINGLE_MMAP) {
    cq_ring_ptr = sq_ring_ptr;
  } else {
    cq_ring_ptr = mmap(nullptr, cq_ring_sz, PROT_READ | PROT_WRITE,
                       MAP_SHARED | MAP_POPULATE, ring_fd, IORING_OFF_CQ_RING);
    if (cq_ring_ptr == MAP_FAILED) {
      return -errno; // FIXME: unmap sq_ring_ptr
    }
  }
  // 建立 SQ Array 的内存映射
  void *sq_array_ptr =
      mmap(nullptr, sq_array_size, PROT_READ | PROT_WRITE,
           MAP_SHARED | MAP_POPULATE, ring_fd, IORING_OFF_SQES);
  if (sq_array_ptr == MAP_FAILED) {
    return -errno; // FIXME: unmap sq_ring_ptr and cp_range_ptr
  }
  
  // ......
}
```

## 3.如何提交 I/O 请求

初始化完成之后，我们需要向 io_uring 提交 I/O 请求。默认情况下，使用 io_uring 提交 I/O 请求需要：

1. 从 SQ Arrary 中找到一个空闲的 SQE。
2. 根据具体的 I/O 请求设置这个 SQE。
3. 将 SQE 的数组索引放到 SQ 中。
4. 调用系统调用 io_uring_enter 提交 SQ 中的 I/O 请求。

为了进一步提升性能，io_uring 提供了内核轮询的方式来提交 I/O 请求（设置 params.flags 的 IORING_SETUP_SQPOLL 位）：创建一个内核线程（简称 SQ 线程）对 SQ 进行轮询（polling），发现有未提交的 I/O 请求就自动进行提交：

- 如果 I/O 请求源源不断，SQ 线程会一直轮询并提交 I/O 请求给内核，这个过程不需要经过系统调用。
- 如果 SQ 线程的空闲时间超过 sq_thread_idle 毫秒，会自动睡眠，并设置 io_sq_ring 的 flags（ sq_ring_ptr + p.sq_off.flags ）的 IORING_SQ_NEED_WAKEUP 位。
- 用户进程需要根据 flags 是否设置了 IORING_SQ_NEED_WAKEUP 来决定是否需要调用 io_uring_enter 来唤醒 SQ 线程：

```text
/* fills in new sqe entries */
add_more_io();
/*
* need to call io_uring_enter() to make the kernel notice the new IO
* if polled and the thread is now sleeping.
*/
if ((*sqring→flags) & IORING_SQ_NEED_WAKEUP)
    io_uring_enter(ring_fd, to_submit, min_complete, IORING_ENTER_SQ_WAKEUP, NULL);
```

## 4.如何收割 I/O 结果

默认情况下，调用 io_uring_enter 来收割 I/O：

```text
io_uring_enter(ring_fd, to_submit, min_complete, IORING_ENTER_GETEVENTS, NULL);
```

如果已完成的 I/O 数量小于 min_complete，请求会阻塞。

当调用 io_uring_setup 时， params.flags 的 IORING_SETUP_IOPOLL 位被设置时，调用 io_uring_enter 收割 I/O，如果已完成的 I/O 的数量不为 0，则不阻塞，立刻返回。

用户进程可以通过遍历 CQ 的 [head, tail) 区间获取已完成的 CQE 并进行处理，然后移动 head 指针到 tail，完成 I/O 收割。

如此，io_uring 的 I/O 提交和收割都可以做到不经过系统调用。

io_uring_register

```text
int io_uring_register(unsigned int fd, unsigned int opcode,
                      void *arg, unsigned int nr_args);
```

io_uring_register 可以将一个文件描述符数组、iovec 数组注册到某个 io_uring 实例，提升 I/O 操作的性能。

- IORING_REGISTER_FILES

每次提交 I/O 请求的时候，内核需要将文件 fd 对应的文件增加引用计数。每次 I/O 完成之后，内核需要将文件 fd 对应的文件减少引用计数。而引用计数的修改是原子的，在高 IOPS 的场景下会严重影响性能。

用户进程可以先将文件 fd 数组注册到 io_uring，注册的时候内核会将这些文件的引用计数加 1，只有当取消注册或 io_uring 销毁时，才会将引用计数减 1。

之后提交 I/O 请求的时候将 SQE 的 flags 的 IOSQE_FIXED_FILE 设置上，将 fd 参数设置为注册到 io_uring 的 fd 数组的索引。这样，便可以避免每次 I/O 都会导致引用计数原子地加 1 减 1 的开销。

- IORING_REGISTER_BUFFERS

当使用 O_DIRECT 的时候：提交 I/O 操作时，内核需要将用户进程的内存映射到内核；完成 I/O 操作后，内核需要将用户进程的内存在内核的映射取消。

在高 IOPS 的时候，频繁的建立、取消内存映射会造成比较大的开销。用户进程可以提前将一个 iovec 数组注册到某个 io_uring 实例，建立相关的内存映射，只有当主动取消注册或 io_uring 实例销毁时，才会取消内存映射。

之后提交 I/O 请求的时候可以使用 IORING_OP_READ_FIXED 、 IORING_OP_WRITE_FIXED 来配合这些已注册的 buffer （将 io_uring_sqe.addr 指向 iovec 数组相关的 buffer）完成 I/O 操作。

## 5.小结

io_uring 通过用户进程和内核共享两个 ring queue 的方式，来减少系统调用和内存拷贝，解决了 Linux AIO 的一些槽点。

原文地址：https://zhuanlan.zhihu.com/p/413523052

作者：linux

# 【NO.415】epoll源码剖析：为什么使用红黑树以及如何使用红黑树

![image-20230207170509923](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230207170509923.png)

我们知道epoll的底层使用了红黑树来管理文件描述符，为什么会选择红黑树这种结构呢？

以下是个人理解：

epoll和poll的一个很大的区别在于，poll每次调用时都会存在一个将pollfd结构体数组中的每个结构体元素从用户态向内核态中的一个链表节点拷贝的过程，而内核中的这个链表并不会一直保存，当poll运行一次就会重新执行一次上述的拷贝过程，这说明一个问题：poll并不会在内核中为要监听的文件描述符长久的维护一个数据结构来存放他们，而epoll内核中维护了一个内核事件表，它是将所有的文件描述符全部都存放在内核中，系统去检测有事件发生的时候触发回调，当你要添加新的文件描述符的时候也是调用epoll_ctl函数使用EPOLL_CTL_ADD宏来插入，epoll_wait也不是每次调用时都会重新拷贝一遍所有的文件描述符到内核态。当我现在要在内核中长久的维护一个数据结构来存放文件描述符，并且时常会有插入，查找和删除的操作发生，这对内核的效率会产生不小的影响，因此需要一种插入，查找和删除效率都不错的数据结构来存放这些文件描述符，那么红黑树当然是不二的人选。

接下来我们来看看epoll底层是如何使用红黑树的

我们知道epoll在添加一个文件描述符进行监听或者删除一个文件描述符时使用的是epoll_ctl函数，该函数底层调用的是sys_epoll_ctl函数，下面给出该函数的部分源码

```text
/*
 * The following function implements the controller interface for
 * the eventpoll file that enables the insertion/removal/change of
 * file descriptors inside the interest set.  It represents
 * the kernel part of the user space epoll_ctl(2).
 */
asmlinkage long
sys_epoll_ctl(int epfd, int op, int fd, struct epoll_event __user *event)
{
	int error;
	struct file *file, *tfile;
	struct eventpoll *ep;
	struct epitem *epi;
	struct epoll_event epds;
 
	DNPRINTK(3, (KERN_INFO "[%p] eventpoll: sys_epoll_ctl(%d, %d, %d, %p)\n",
		     current, epfd, op, fd, event));
 
	error = -EFAULT;
	if (EP_OP_HASH_EVENT(op) &&
	    copy_from_user(&epds, event, sizeof(struct epoll_event)))
		goto eexit_1;
 
	/* Get the "struct file *" for the eventpoll file */
	error = -EBADF;
	file = fget(epfd);
	if (!file)
		goto eexit_1;
 
	/* Get the "struct file *" for the target file */
	tfile = fget(fd);
	if (!tfile)
		goto eexit_2;
 
	/* The target file descriptor must support poll */
	error = -EPERM;
	if (!tfile->f_op || !tfile->f_op->poll)
		goto eexit_3;
 
	/*
	 * We have to check that the file structure underneath the file descriptor
	 * the user passed to us _is_ an eventpoll file. And also we do not permit
	 * adding an epoll file descriptor inside itself.
	 */
	error = -EINVAL;
	if (file == tfile || !IS_FILE_EPOLL(file))
		goto eexit_3;
 
	/*
	 * At this point it is safe to assume that the "private_data" contains
	 * our own data structure.
	 */
	ep = file->private_data;
 
	down_write(&ep->sem);
 
	/* Try to lookup the file inside our hash table */
	epi = ep_find(ep, tfile, fd);
```

在sys_epoll_ctl的参数中，op代表要进行的操作，fd表示要被操作的文件描述符。操作类型定义在下面着三个宏中

```text
/* Valid opcodes to issue to sys_epoll_ctl() */
#define EPOLL_CTL_ADD 1
#define EPOLL_CTL_DEL 2
#define EPOLL_CTL_MOD 3
```

首先呢，会调用ep_find函数在内核事件表也就是红黑树中查找该fd是否已经存在，这里的结果会先保存在epi中，ep_find函数做了什么操作呢？这里就是我们第一个用到红黑树的地方：查找

先来看一下ep_find的实现：

```text
/*
 * Search the file inside the eventpoll hash. It add usage count to
 * the returned item, so the caller must call ep_release_epitem()
 * after finished using the "struct epitem".
 */
static struct epitem *ep_find(struct eventpoll *ep, struct file *file, int fd)
{
	int kcmp;
	unsigned long flags;
	struct rb_node *rbp;
	struct epitem *epi, *epir = NULL;
	struct epoll_filefd ffd;
 
	EP_SET_FFD(&ffd, file, fd);
	read_lock_irqsave(&ep->lock, flags);
	for (rbp = ep->rbr.rb_node; rbp; ) {
		epi = rb_entry(rbp, struct epitem, rbn);
		kcmp = EP_CMP_FFD(&ffd, &epi->ffd);
		if (kcmp > 0)
			rbp = rbp->rb_right;
		else if (kcmp < 0)
			rbp = rbp->rb_left;
		else {
			ep_use_epitem(epi);
			epir = epi;
			break;
		}
	}
	read_unlock_irqrestore(&ep->lock, flags);
 
	DNPRINTK(3, (KERN_INFO "[%p] eventpoll: ep_find(%p) -> %p\n",
		     current, file, epir));
 
	return epir;
}
```

这里的for循环就是一个红黑树的查找过程，我们可以看到这里查找的时候用到的一个变量是kcmp，这个kcmp的值就是我们的fd在红黑树中所用来排序的值。而且我们可以看到这个kcmp的值来源于宏函数EP_CMP_FFD我们来看一下这个宏函数的实现

```text
/* Compare rb-tree keys */
#define EP_CMP_FFD(p1, p2) ((p1)->file > (p2)->file ? +1: \
			    ((p1)->file < (p2)->file ? -1: (p1)->fd - (p2)->fd))
```

根据该宏函数的实现我们看到在比较时其实使用的是一个epoll_filefd的结构体中的file成员来比较的，那么我们再进入epoll_filefd中查看一下

![img](https://pic4.zhimg.com/80/v2-8a397a57324ce8c64ff02e87d3d65cff_720w.webp)

我们看到这里的file是一个struct file类型的指针，当我们比较两个file类型的指针时比较的是他们的指针的值，也就是file结构体的地址。

根据源码判断，在红黑树中排序的根据是file的地址大小。至于为什么，目前还并不是很清楚，也存在我理解错误的可能，这里不是很确定。

查找完毕后，就要开始进行具体的操作了，这里会根据宏的值判断应该进行的操作是插入，删除，还是修改。这里给出sys_epoll_ctl的剩余源码（和文章开头给出的前半部分刚好衔接）

```text
error = -EINVAL;
	switch (op) {
	case EPOLL_CTL_ADD:
		if (!epi) {
			epds.events |= POLLERR | POLLHUP;
 
			error = ep_insert(ep, &epds, tfile, fd);
		} else
			error = -EEXIST;
		break;
	case EPOLL_CTL_DEL:
		if (epi)
			error = ep_remove(ep, epi);
		else
			error = -ENOENT;
		break;
	case EPOLL_CTL_MOD:
		if (epi) {
			epds.events |= POLLERR | POLLHUP;
			error = ep_modify(ep, epi, &epds);
		} else
			error = -ENOENT;
		break;
	}
 
	/*
	 * The function ep_find() increments the usage count of the structure
	 * so, if this is not NULL, we need to release it.
	 */
	if (epi)
		ep_release_epitem(epi);
 
	up_write(&ep->sem);
 
eexit_3:
	fput(tfile);
eexit_2:
	fput(file);
eexit_1:
	DNPRINTK(3, (KERN_INFO "[%p] eventpoll: sys_epoll_ctl(%d, %d, %d, %p) = %d\n",
		     current, epfd, op, fd, event, error));
 
	return error;
}
```

我们看到这部分代码里最主要的工作就是进行这个switch，case语句所做的判断工作了，这里sys_epoll_ctl函数根据参数op的不同而调用不同的函数进行处理，我们以EPOLL_CTL_ADD宏举例，该宏要进行的操作是插入一个新的文件描述符。

epoll底层的红黑树插入是调用ep_insert插入的，而ep_insert函数里面调用了ep_rbtree_insert来进行对红黑树中一个节点的插入。这两个函数的声明如下：

```text
static void ep_rbtree_insert(struct eventpoll *ep, struct epitem *epi);
static int ep_insert(struct eventpoll *ep, struct epoll_event *event,
		     struct file *tfile, int fd);
```

我们忽略ep_insert函数其他的实现要点，直接查看它所调用的函数ep_retree_insert的实现

```text
static void ep_rbtree_insert(struct eventpoll *ep, struct epitem *epi)
{
	int kcmp;
	struct rb_node **p = &ep->rbr.rb_node, *parent = NULL;
	struct epitem *epic;
 
	while (*p) {
		parent = *p;
		epic = rb_entry(parent, struct epitem, rbn);
		kcmp = EP_CMP_FFD(&epi->ffd, &epic->ffd);
		if (kcmp > 0)
			p = &parent->rb_right;
		else
			p = &parent->rb_left;
	}
	rb_link_node(&epi->rbn, parent, p);
	rb_insert_color(&epi->rbn, &ep->rbr);
}
```

可以看到这里在插入一个新节点时对于其在红黑树中的位置的选择过程是用一个while循环来实现的，当该while循环退出后，说明我们已经找到了该节点应在的位置，接下来调用rb_link_node函数将该节点插入到红黑树中，该函数的实现很简单，就是往一颗二叉树中插入一个新的节点，实现如下

```text
static inline void rb_link_node(struct rb_node * node, struct rb_node * parent,
				struct rb_node ** rb_link)
{
	node->rb_parent = parent;
	node->rb_color = RB_RED;
	node->rb_left = node->rb_right = NULL;
 
	*rb_link = node;
}
```

然后再调用rb_insert_color函数，这个函数实现的是对插入一个新节点之后的整个红黑树进行调整的过程，这里牵扯到红黑树的旋转，不是我们本文的重点，只把代码贴上，有兴趣的同学可以下去自习。

```text
void rb_insert_color(struct rb_node *node, struct rb_root *root)
{
	struct rb_node *parent, *gparent;
 
	while ((parent = node->rb_parent) && parent->rb_color == RB_RED)
	{
		gparent = parent->rb_parent;
 
		if (parent == gparent->rb_left)
		{
			{
				register struct rb_node *uncle = gparent->rb_right;
				if (uncle && uncle->rb_color == RB_RED)
				{
					uncle->rb_color = RB_BLACK;
					parent->rb_color = RB_BLACK;
					gparent->rb_color = RB_RED;
					node = gparent;
					continue;
				}
			}
 
			if (parent->rb_right == node)
			{
				register struct rb_node *tmp;
				__rb_rotate_left(parent, root);
				tmp = parent;
				parent = node;
				node = tmp;
			}
 
			parent->rb_color = RB_BLACK;
			gparent->rb_color = RB_RED;
			__rb_rotate_right(gparent, root);
		} else {
			{
				register struct rb_node *uncle = gparent->rb_left;
				if (uncle && uncle->rb_color == RB_RED)
				{
					uncle->rb_color = RB_BLACK;
					parent->rb_color = RB_BLACK;
					gparent->rb_color = RB_RED;
					node = gparent;
					continue;
				}
			}
 
			if (parent->rb_left == node)
			{
				register struct rb_node *tmp;
				__rb_rotate_right(parent, root);
				tmp = parent;
				parent = node;
				node = tmp;
			}
 
			parent->rb_color = RB_BLACK;
			gparent->rb_color = RB_RED;
			__rb_rotate_left(gparent, root);
		}
	}
 
	root->rb_node->rb_color = RB_BLACK;
}
```

原文地址：https://zhuanlan.zhihu.com/p/366955699

作者：linux



# 【NO.416】盘点腾讯linux C++后台开发面试题

鹅厂是cpp的主战场，而以cpp为背景的工程师大都对os，network这块要求特别高，不像是Java这种偏重业务层的语言，之前面试Java的公司侧重还是在数据结构、网络、框架、数据库和分布式。所以OS这块吃的亏比较大。

面试分为以下几大块

- C/C++
- 网络
- 操作系统
- Linux系统
- MongoDB
- Redis
- mysql
- 算法
- 设计模式
- 分布式架构
- 系统设计
- 等等，未完待续

## 1.C/C++

1. const
2. 多态
3. 什么类不能被继承（这个题目非常经典，我当时答出了private但是他说不好，我就没想到final我以为那个是java的）

## 2.网络

1. 网络的字节序
2. 网络知识 tcp三次握手 各种细节 timewait状态
3. tcp 与 udp 区别 概念 适用范围
4. TCP四次挥手讲一下过程，最后一次ack如果客户端没收到怎么办，为什么挥手不能只有三次，为什么time_wait。
5. 对于socket编程，accept方法是干什么的，在三次握手中属于第几次，可以猜一下，为什么这么觉得。
6. tcp怎么保证有序传输的，讲下tcp的快速重传和拥塞机制，知不知道time_wait状态，这个状态出现在什么地方，有什么用？
7. 知道udp是不可靠的传输，如果你来设计一个基于udp差不多可靠的算法，怎么设计？
8. http与https有啥区别？说下https解决了什么问题，怎么解决的？说下https的握手过程。
9. tcp 粘包半包问题怎么处理？
10. keepalive 是什么东东？如何使用？
11. 列举你所知道的tcp选项，并说明其作用。
12. socket什么情况下可读？
13. nginx的epoll模型的介绍以及io多路复用模型
14. SYN Flood攻击
15. 流量控制，拥塞控制
16. TCP和UDP区别，TCP如何保证可靠性，对方是否存活(心跳检测)
17. tcpdump抓包，如何分析数据包
18. tcp如何设定超时时间
19. 基于socket网络编程和tcp/ip协议栈，讲讲从客户端send()开始，到服务端recv()结束的过程，越细越好
20. http报文格式
21. http1.1与http1.0区别，http2.0特性
22. http3了解吗
23. http1.1长连接时，发送一个请求阻塞了，返回什么状态码？
24. udp调用connect有什么作用？

## 3.操作系统

1. 进程和线程-分别的概念 区别 适用范围 它们分别的通讯方式 不同通讯方式的区别优缺点
2. 僵尸进程
3. 死锁是怎么产生的
4. CPU的执行方式
5. 代码中遇到进程阻塞，进程僵死，内存泄漏等情况怎么排查。
6. 有没有了解过协程？说下协程和线程的区别？
7. 堆是线程共有还是私有，堆是进程共有还是私有，栈呢
8. 了解过协程吗（我：携程？？？不了解呜呜呜）
9. 共享内存的使用实现原理（必考必问，然后共享内存段被映射进进程空间之后，存在于进程空间的什么位置？共享内存段最大限制是多少？）
10. c++进程内存空间分布（注意各部分的内存地址谁高谁低，注意栈从高道低分配，堆从低到高分配）
11. ELF是什么？其大小与程序中全局变量的是否初始化有什么关系（注意.bss段）
12. 使用过哪些进程间通讯机制，并详细说明（重点）
13. 多线程和多进程的区别（重点 面试官最最关心的一个问题，必须从cpu调度，上下文切换，数据共享，多核cup利用率，资源占用，等等各方面回答，然后有一个问题必须会被问到：哪些东西是一个线程私有的？答案中必须包含寄存器，否则悲催）
14. 信号：列出常见的信号，信号怎么处理？
15. i++是否原子操作？并解释为什么？？？？？？？
16. 说出你所知道的各类linux系统的各类同步机制（重点），什么是死锁？如何避免死锁（每个技术面试官必问）
17. 列举说明linux系统的各类异步机制
18. exit() _exit()的区别？
19. 如何实现守护进程？
20. linux的内存管理机制是什么？
21. linux的任务调度机制是什么？
22. 标准库函数和系统调用的区别？
23. 补充一个坑爹坑爹坑爹坑爹的问题：系统如何将一个信号通知到进程？（这一题哥没有答出来）

## 4.Linux系统

1. linux的各种命令 给你场景让你解决
2. Linux了解么，查看进程状态ps，查看cpu状态 top。查看占用端口的进程号netstat grep
3. Linux的cpu 100怎么排查，top jstack，日志，gui工具
4. Linux操作系统了解么
5. 怎么查看CPU负载，怎么查看一个客户下有多少进程
6. Linux内核是怎么实现定时器的
7. gdb怎么查看某个线程
8. core dump有没有遇到过，gdb怎么调试
9. linux如何设置core文件生成
10. linux如何设置开机自启动
11. linux用过哪些命令、工具
12. 用过哪些工具检测程序性能，如何定位性能瓶颈的地方
13. netstat tcpdump ipcs ipcrm （如果这四个命令没听说过或者不能熟练使用，基本上可以回家，通过的概率较小 ^_^ ，这四个命令的熟练掌握程度基本上能体现面试者实际开发和调试程序的经验)
14. cpu 内存 硬盘 等等与系统性能调试相关的命令必须熟练掌握，设置修改权限 tcp网络状态查看 各进程状态 抓包相关等相关命令 必须熟练掌握
15. awk sed需掌握
16. gdb调试相关的经验，会被问到

## 5.MongoDB

1. 关于大数据存储的（mongodb hadoop）各种原理 mongodb又问的深入很多

## 6.Redis

1. Redis内存数据库的内存指的是共享内存么
2. Redis的持久化方式
3. Redis和MySQL有什么区别，用于什么场景。
4. redis有没有用过，常用的数据结构以及在业务中使用的场景，redis的hash怎么实现的
5. 问了下缓存更新的模式，以及会出现的问题和应对思路？
6. redis的sentinel上投票选举的问题 raft算法
7. redis单线程结构有什么优势？有什么问题？ 主要优势单线程，避免线程切换产生静态消耗，缺点是容易阻塞，虽然redis使用io复用epoll和输入缓冲区把命令按照队列先进先出输入等等
8. 你觉得针对redis这些缺点那些命令在redis上不可使用？ 比如keys、hgetall等等这些命令 建议用scan等等 这方面阐述
9. 你觉得为什么项目中没有用mysql而用了es，redis在这里到底起到了什么作用？因为架构上这里理解不清楚，最后回答自己都觉得有漏洞了
10. 你觉得redis什么算有用？ 有用？ 是说存进去了还是说命中缓存？最后把缓存命中率是什么说了一遍
11. 你们这边redis集群是怎么样子的
12. 平常redis用的多的数据结构是什么，跳表实现，怎么维护索引，当时我说是一个简单的二分，手写二分算法，并且时间复杂度是怎么计算出来的 （2的k次方等于n k等于logn）

## 7.MySQL

1. 你们后端用什么数据库做持久化的？有没有用到分库分表，怎么做的？
2. 索引的常见实现方式有哪些，有哪些区别?MySQL的存储引擎有哪些，有哪些区别？InnoDB使用的是什么方式实现索引，怎么实现的？说下聚簇索引和非聚簇索引的区别?
3. mysql查询优化
4. MySQL的索引，B+树性质。
5. B+树和B树，联合索引等原理
6. mysql的悲观锁和乐观锁区别和应用，ABA问题的解决
7. 项目性能瓶颈在哪，数据库表怎么设计
8. 假设项目的性能瓶颈出现在写数据库上，应该怎么解决峰值时写速度慢的问题
9. 假设数据库需要保存一年的数据，每天一百万条数据，一张表最多存一千万条数据，应该怎么设计表
10. 数据库自增索引。100台服务器，每台服务器有若干个用户，用户有id，同时会有新用户加入。实现id自增，统计用户个数？不能重复，好像是这样的。
11. mysql，会考sql语言，服务器数据库大规模数据怎么设计，db各种性能指标

## 8.算法

1. 堆栈
2. 有序数组排序，二分，复杂度
3. 常见排序算法，说下快排过程，时间复杂度
4. 有N个节点的满二叉树的高度。1+logN
5. 如何实现关键字输入提示，使用字典树，复杂度多少，有没有其他方案，答哈希，如果是中文呢，分词后建立字典树？
6. hashmap的实现讲一下吧，讲的很详细了。讲一下红黑树的结构，查询性能等。
7. 快排的时间复杂度，冒泡时间复杂度，快排是否稳定，快排的过程
8. 100w个数，怎么找到前1000个最大的，堆排序，怎么构造，怎么调整，时间复杂度。
9. 一个矩阵，从左上角到右下角，每个位置有一个权值。可以上下左右走，到达右下角的路径权值最小怎么走。
10. 四辆小车，每辆车加满油可以走一公里，问怎么能让一辆小车走最远。说了好几种方案，面试官引导我优化了一下，但是还是不满意，最后他说跳过。
11. MySQL的索引，B+树性质。
12. 十亿和数找到前100个最大的，堆排序，怎么实现，怎么调整。
13. 布隆过滤器
14. hash表解决冲突的方法
15. 跳表插入删除过程
16. 让你实现一个哈希表，怎么做（当时按照Redis中哈希表的实现原理回答）

## 9.设计模式

1. 对于单例模式，有什么使用场景了，讲了全局id生成器，他问我分布式id生成器怎么实现，说了zk，问我zk了解原理不，讲了zab，然后就没问啦。
2. 除了单例模式，知道适配器模式怎么实现么，有什么用

## 10.分布式架构

1. CAP BASE理论
2. 看你项目里面用了etcd，讲解下etcd干什么用的，怎么保证高可用和一致性？
3. 既然你提到了raft算法，讲下raft算法的基本流程？raft算法里面如果出现脑裂怎么处理？有没有了解过paxos和zookeeper的zab算法，他们之前有啥区别？
4. rpc有没有了解

## 11.系统设计

1. 朋友之间的点对点关系用图维护，怎么判断两人是否是朋友，并查集，时间复杂度，过程。
2. 10g文件，只有2g内存，怎么查找文件中指定的字符串出现位置。
3. Linux大文件怎么查某一行的内容。
4. 秒杀系统的架构设计
5. 十亿个数的集合和10w个数的集合，如何求它们的交集。
6. 回到网络，刚才你说到直播场景，知道直播的架构怎么设计么，要点是什么，说了几个不太对，他说要避免广播风暴，答不会。
7. 针对自己最熟悉的项目，画出项目的架构图，主要的数据表结构，项目中使用到的技术点，项目的总峰值qps，时延，以及有没有分析过时延出现的耗时分别出现在什么地方，项目有啥改进的地方没有？
8. 如果请求出现问题没有响应，如何定位问题，说下思路？
9. 除了公司项目之外，业务有没有研究过知名项目或做出过贡献？
10. go程和线程有什么区别？ 答：1 起一个go程大概只需要4kb的内存，起一个Java线程需要1.5MB的内存；go程的调度在用户态非常轻量，Java线程的切换成本比较高。接着问为啥成本比较高？因为Java线程的调度需要在用户态和内核态切换所以成本高？为啥在用户态和内核态之间切换调度成本比较高？简单说了下内核态和用户态的定义。接着问，还是没有明白为啥成本高？心里瞬间崩溃，没完没了了呀，OS这块依旧是痛呀，支支吾吾半天放弃了。
11. 服务器CPU 100%怎么定位？可能是由于平时定位业务问题的思维定势，加之处于蒙蔽状态，随口就是：先查看监控面板看有无突发流量异常，接着查看业务日志是否有异常，针对CPU100%那个时间段，取一个典型业务流程的日志查看。最后才提到使用top命令来监控看是哪个进程占用到100%。果然阵脚大乱，张口就来，捂脸。。。 本来正确的思路应该是先用top定位出问题的进程，再用top定位到出问题的线程，再打印线程堆栈查看运行情况，这个流程换平时肯定能答出来，但是，但是没有但是。还是得好好总结。
12. 最后问了一个系统设计题目（朋友圈的设计），白板上面画出系统的架构图，主要的表结构和讲解主要的业务流程，如果用户变多流量变大，架构将怎么扩展，怎样应对？ 这个答的也有点乱，直接上来自顾自的用了一个通用的架构，感觉毫无亮点。后面反思应该先定位业务的特点，这个业务明显是读多写少，然后和面试官沟通一期刚开始的方案的用户量，性能要求，单机目标qps是什么等等？在明确系统的特点和约束之后再来设计，而不是一开始就是用典型互联网的那种通用架构自顾自己搞自己的方案。
13. 设计一个限流的算法
14. 定时器除了小根堆，还可以怎么做
15. 项目性能瓶颈在哪，数据库表怎么设计
16. .在高并发的生产环境中（非调试场景下），如果出现数据包的丢失，如何定位问题
17. 补充一个最最重要，最最坑爹，最最有难度的一个题目：一个每秒百万级访问量的互联网服务器，每个访问都有数据计算和I/O操作，如果让你设计，你怎么设计？

## 12.道友总结

1. tcp/udp，http和https还有网络这块（各种网络模型，已经select，poll和epoll）一定要非常熟悉
2. 一定要有拿的出手的项目经验，而且要能够讲清楚，讲清楚项目中取舍，设计模型和数据表
3. 分布式要非常熟悉
4. 常见问题定位一定要有思路
5. 操作系统，还是操作系统，重要的事情说三遍
6. 系统设计，思路，思路，思路，一定要思路清晰，一定要总结下系统设计的流程
7. 一点很重要的心得，平时blog和专栏看的再多，如果没有自己的思考不过是过眼云烟，根本不会成为自己的东西，就像内核态和用户态，平常也看过，但是没细想，突然要自己说，还真说不出来，这就很尴尬了。勿以浮沙筑高台，基础这种东西还是需要时间去慢慢打牢，多去思考和总结。

原文地址：https://zhuanlan.zhihu.com/p/103027724

作者：linux

# 【NO.417】60道30K+C++工程师面试必问面试题

**1、在C++ 程序中调用被C 编译器编译后的函数，为什么要加extern “C”？**

答：首先，extern是C/C++语言中表明函数和全局变量作用范围的关键字，该关键字告诉编译器，其声明的函数和变量可以在本模块或其它模块中使用。

通常，在模块的头文件中对本模块提供给其它模块引用的函数和全局变量以关键字extern声明。extern "C"是连接申明(linkage declaration),被extern "C"修饰的变量和函数是按照C语言方式编译和连接的。作为一种面向对象的语言，C++支持函数重载，而过程式语言C则不支持。函数被C++编译后在符号库中的名字与C语言的不同。例如，假设某个函数的原型为：void foo( int x, int y );该函数被C编译器编译后在符号库中的名字为_foo，而C++编译器则会产生像_foo_int_int之类的名字。这样的名字包含了函数名、函数参数数量及类型信息，C++就是靠这种机制来实现函数重载的。

所以，可以用一句话概括extern “C”这个声明的真实目的:解决名字匹配问题，实现C++与C的混合编程。

**2、头文件中的ifndef/define/endif有什么作用？**

答：这是C++预编译头文件保护符，保证即使文件被多次包含，头文件也只定义一次。

**3、＃include<file.h> 与 ＃include "file.h"的区别？**

答：前者是从标准库路径寻找和引用file.h，而后者是从当前工作路径搜寻并引用file.h。

**4、评价一下C/C++各自的特点**

答：C语言是一种结构化语言，面向过程，基于算法和数据结构，所考虑的是如何通过一个过程或者函数从输入得到输出；

C++是面向对象，基于类、对象和继承，所考虑的是如何构造一个对象模型，让这个模型能够契合与之对应的问题，通过获取对象的状态信息得到输出或实现过程控制。

**5、const 有什么用途**

答：在C/C++中，（1）可？以定义const常量，（2）修饰函数的返回值和形参；

在C++中，还可以修饰函数的定义体，定义类的const成员函数。被const修饰的东西受到强制保护，可以预防意外的变动，提高了程序的健壮性。

**6、const和#define有什么区别？**

答：（1）const和#define都可以定义常量，但是const用途更广。

（2）const 常量有数据类型，而宏常量没有数据类型。编译器可以对前者进行类型安全检查。而对后者只进行字符替换，没有类型安全检查，并且在字符替换可能会产生意料不到的错误。

（3） 有些集成化的调试工具可以对const 常量进行调试，但是不能对宏常量进行调试。

**7、关于sizeof小结的。**

答：sizeof计算的是在栈中分配的内存大小。

（1） sizeof不计算static变量占得内存；

（2） 32位系统的指针的大小是4个字节，64位系统的指针是8字节，而不用管指针类型；

（3） char型占1个字节，int占4个字节，short int占2个字节

long int占4个字节，float占4字节，double占8字节，string占4字节

一个空类占1个字节，单一继承的空类占1个字节，虚继承涉及到虚指针所以占4个字节

（4） 数组的长度：

若指定了数组长度，则不看元素个数，总字节数=数组长度*sizeof（元素类型）

若没有指定长度，则按实际元素个数类确定

Ps：若是字符数组，则应考虑末尾的空字符。

（5） 结构体对象的长度

在默认情况下，为方便对结构体内元素的访问和管理，当结构体内元素长度小于处理器位数的时候，便以结构体内最长的数据元素的长度为对齐单位，即为其整数倍。若结构体内元素长度大于处理器位数则以处理器位数为单位对齐。

（6） unsigned影响的只是最高位的意义，数据长度不会改变，所以sizeof（unsigned int）=4

（7） 自定义类型的sizeof取值等于它的类型原型取sizeof

（8） 对函数使用sizeof，在编译阶段会被函数的返回值的类型代替

（9） sizeof后如果是类型名则必须加括号，如果是变量名可以不加括号，这是因为sizeof是运算符

（10） 当使用结构类型或者变量时，sizeof返回实际的大小。当使用静态数组时返回数组的全部大小，sizeof不能返回动态数组或者外部数组的尺寸

**8、sizeof与strlen的区别？**

答： （1）sizeof的返回值类型为size_t（unsigned int）；

（2）sizeof是运算符，而strlen是函数；

（3）sizeof可以用类型做参数，其参数可以是任意类型的或者是变量、函数，而strlen只能用char*做参数，且必须是以’\0’结尾；

（4）数组作sizeof的参数时不会退化为指针，而传递给strlen是就退化为指针；

（5）sizeo是编译时的常量，而strlen要到运行时才会计算出来，且是字符串中字符的个数而不是内存大小；

**9、指针和引用的区别？**

答：指针和引用都提供了间接操作对象的功能。

（1） 指针定义时可以不初始化，而引用在定义时就要初始化，和一个对象绑定，而且一经绑定，只要引用存在，就会一直保持和该对象的绑定；

（2） 赋值行为的差异：指针赋值是将指针重新指向另外一个对象，而引用赋值则是修改对象本身；

（3） 指针之间存在类型转换，而引用分const引用和非const应用，非const引用只能和同类型的对象绑定，const引用可以绑定到不同但相关类型的对象或者右值

**10、数组和指针的区别？**

答：（1）数组要么在全局数据区被创建，要么在栈上被创建；指针可以随时指向任意类型的内存块；

（2）修改内容上的差别：

char a[] = “hello”;

a[0] = ‘X’;

char *p = “world”; // 注意p 指向常量字符串

p[0] = ‘X’; // 编译器不能发现该错误，运行时错误

(3)用运算符sizeof 可以计算出数组的容量（字节数）。sizeof(p),p 为指针得到的是一个指针变量的字节数，而不是p 所指的内存容量。C++/C 语言没有办法知道指针所指的内存容量，除非在申请内存时记住它。注意当数组作为函数的参数进行传递时，该数组自动退化为同类型的指针。

**11、空指针和悬垂指针的区别？**

答：空指针是指被赋值为NULL的指针；delete指向动态分配对象的指针将会产生悬垂指针。

（1） 空指针可以被多次delete，而悬垂指针再次删除时程序会变得非常不稳定；

（2） 使用空指针和悬垂指针都是非法的，而且有可能造成程序崩溃，如果指针是空指针，尽管同样是崩溃，但和悬垂指针相比是一种可预料的崩溃。

**12、C++中有malloc/free，为什么还有new/delete？**

答：malloc/free是C/C++标准库函数，new/delete是C++运算符。他们都可以用于动态申请和释放内存。

对于内置类型数据而言，二者没有多大区别。malloc申请内存的时候要制定分配内存的字节数，而且不会做初始化；new申请的时候有默认的初始化，同时可以指定初始化；

对于类类型的对象而言，用malloc/free无法满足要求的。对象在创建的时候要自动执行构造函数，消亡之前要调用析构函数。由于malloc/free是库函数而不是运算符，不在编译器控制之内，不能把执行构造函数和析构函数的任务强加给它，因此，C++还需要new/delete。

**13、什么是智能指针？**

答：当类中有指针成员时，一般有两种方式来管理指针成员：一是采用值型的方式管理，每个类对象都保留一份指针指向的对象的拷贝；另一种更优雅的方式是使用智能指针，从而实现指针指向的对象的共享。

智能指针的一种通用实现技术是使用引用计数。智能指针类将一个计数器与类指向的对象相关联，引用计数跟踪该类有多少个对象共享同一指针。

每次创建类的新对象时，初始化指针并将引用计数置为1；当对象作为另一对象的副本而创建时，拷贝构造函数拷贝指针并增加与之相应的引用计数；对一个对象进行赋值时，赋值操作符减少左操作数所指对象的引用计数（如果引用计数为减至0，则删除对象），并增加右操作数所指对象的引用计数；调用析构函数时，构造函数减少引用计数（如果引用计数减至0，则删除基础对象）。

**14、面向对象技术的基本概念是什么，三个基本特征是什么？**

答：基本概念：类、对象、继承； 基本特征：封装、继承、多态。

封装：将低层次的元素组合起来形成新的、更高实体的技术；

继承：广义的继承有三种实现形式：实现继承、可视继承、接口继承。

多态：允许将子类类型的指针赋值给父类类型的指针

**15、C++空类默认有哪些成员函数？**

答：默认构造函数、析构函数、复制构造函数、赋值函数

**16、哪一种成员变量可以在一个类的实例之间共享？**

答：static静态成员变量

**17、继承层次中，为什么基类析构函数是虚函数？**

答：编译器总是根据类型来调用类成员函数。但是一个派生类的指针可以安全地转化为一个基类的指针。这样删除一个基类的指针的时候，C++不管这个指针指向一个基类对象还是一个派生类的对象，调用的都是基类的析构函数而不是派生类的。如果你依赖于派生类的析构函数的代码来释放资源，而没有重载析构函数，那么会有资源泄漏。

**18、为什么构造函数不能为虚函数？**

答：虚函数采用一种虚调用的方法。需调用是一种可以在只有部分信息的情况下工作的机制。如果创建一个对象，则需要知道对象的准确类型，因此构造函数不能为虚函数。

**19.如果虚函数是有效的，那为什么不把所有函数设为虚函数？**

答：不行。首先，虚函数是有代价的，由于每个虚函数的对象都要维护一个虚函数表，因此在使用虚函数的时候都会产生一定的系统开销，这是没有必要的。

**20、构造函数可以是内联函数**

**21、什么是多态？多态有什么作用？**

答：多态就是将基类类型的指针或者引用指向派生类型的对象。多态通过虚函数机制实现。

多态的作用是接口重用。

**22、重载和覆盖有什么区别？**

答：虚函数是基类希望派生类重新定义的函数，派生类重新定义基类虚函数的做法叫做覆盖；

重载就在允许在相同作用域中存在多个同名的函数，这些函数的参数表不同。重载的概念不属于面向对象编程，编译器根据函数不同的形参表对同名函数的名称做修饰，然后这些同名函数就成了不同的函数。

重载的确定是在编译时确定，是静态的；虚函数则是在运行时动态确定。

**23、公有继承、受保护继承、私有继承**

答：（1）公有继承时，派生类对象可以访问基类中的公有成员，派生类的成员函数可以访问基类中的公有和受保护成员；

（2）私有继承时，基类的成员只能被直接派生类的成员访问，无法再往下继承；

（3）受保护继承时，基类的成员也只被直接派生类的成员访问，无法再往下继承。

**24、公有继承时基类受保护的成员，可以通过派生类对象访问但不能修改。**

**25、有哪几种情况只能用构造函数初始化列表而不能用赋值初始化？**

答：const成员，引用成员

**26、什么是虚指针？**

答：虚指针或虚函数指针是虚函数的实现细节。带有虚函数的每一个对象都有一个虚指针指向该类的虚函数表。

**27、C++如何阻止一个类被实例化？一般在什么时候将构造函数声明为private？**

答：（1）将类定义为抽象基类或者将构造函数声明为private；

（2）不允许类外部创建类对象，只能在类内部创建对象

**28、main函数执行之前会执行什么？执行之后还能执行代码吗？**

答：（1）全局对象的构造函数会在main函数之前执行；

（2）可以，可以用_onexit 注册一个函数，它会在main 之后执行;

如果你需要加入一段在main退出后执行的代码，可以使用atexit()函数，注册一个函数。

语法：

```text
#include <stdlib.h>
#include <stdio.h>
int atexit(void (*function")(void));
void fn1( void ), fn2( void ), fn3( void );
int main( void )
{
atexit(fn1);
atexit( fn2 );
printf( "This is executed first.\n" );
}
void fn1()
{
printf( " This is\n" );
}
void fn2()
{
printf( " executed next." );
}
```

结果：

This is executed first.

This is executed next.

**29、请描述进程和线程的区别？**

答：（1）进程是程序的一次执行，线程是进程中的执行单元；

（2）进程间是独立的，这表现在内存空间、上下文环境上，线程运行在进程中；

（3）一般来讲，进程无法突破进程边界存取其他进程内的存储空间；而同一进程所产生的线程共享内存空间；

（4）同一进程中的两段代码不能同时执行，除非引入多线程。

**30、进程间如何通信？**

答：信号、信号量、消息队列、共享内存

**31、在网络编程中涉及并发服务器，使用多进程与多线程的区别？**

答：（1）线程执行开销小，但不利于资源管理和保护；进程则相反，进程可跨越机器迁移。

（2）多进程时每个进程都有自己的内存空间，而多线程间共享内存空间；

（3）线程产生的速度快，线程间通信快、切换快；

（4）线程的资源利用率比较好；

（5）线程使用公共变量或者资源时需要同步机制。

**32、说一下TCP 3次握手、4次挥手的全过程。**

**33、TCP和UDP有什么区别。**

答：

TCP——传输控制协议,提供的是面向连接、可靠的字节流服务。

当客户和服务器彼此交换数据前，必须先在双方之间建立一个TCP连接，之后才能传输数据。TCP提供超时重发，丢弃重复数据，检验数据，流量控制等功能，保证数据能从一端传到另一端。

UDP——用户数据报协议，是一个简单的面向数据报的传输层协议。UDP不提供可靠性，它只是把应用程序传给IP层的数据报发送出去，但是并不能保证它们能到达目的地。由于UDP在传输数据报前不用在客户和服务器之间建立一个连接，且没有超时重发等机制，故而传输速度很快.

TCP协议和UDP协议的一些特性区别如下：

1.TCP协议在传送数据段的时候要给段标号；UDP 协议不需要。

2.TCP协议可靠；UDP协议不可靠。

3.TCP协议是面向连接；UDP协议采用无连接。

4.TCP协议负载较高,采用虚电路；UDP协议低负载。

5.TCP协议的发送方要确认接受方是否收到数据段(3次握手协议)。

6.TCP协议采用窗口技术和流控制。

**34、如何编写套接字？**

**35、调用函数时要进行参数压栈，一般情况下顺序是从最右边参数往左压栈。**

**36、经常要操作的内存分为那几个类别？**

答：（1）栈区：由编译器自动分配和释放，存放函数的参数值、局部变量的值等；

（2）堆：一般由程序员分配和释放，存放动态分配的变量；

（3）全局区（静态区）：全局变量和静态变量存放在这一块，初始化的和未初始化的分开放；

（4）文字常量区：常量字符串就放在这里，程序结束自动释放；

（5）程序代码区：参访函数体的二进制代码。

**37、请讲述堆和栈的区别。**

答：（1）申请方式不同。栈上有系统自动分配和释放；堆上有程序员自己申请并指明大小；

（2）栈是向低地址扩展的数据结构，大小很有限；堆是向高地址扩展，是不连续的内存区域，空间相对大且灵活；

（3）栈由系统分配和释放速度快；堆由程序员控制，一般较慢，且容易产生碎片；

**38、全局变量放在数据段，内部变量static int count；放在数据段，内部变量char \*p=“AAA”，p的位置在堆栈上，指向的空间的位置数据段，内部变量char \*p=new char；p的位置堆，指向的空间的位置数据段**

**39、字符数组与字符串的比较：最明显的区别是字符串会在末尾自动添加空字符。**

**40、函数指针相关概念（C++学习笔记）**

**41、类使用static成员的优点，如何访问？**

答：优点：

（1）static 成员的名字是在类的作用域中，因此可以避免与其他类的成员或全局对象名字冲突；

（2）可以实施封装。static 成员可以是私有成员，而全局对象不可以；

（3） static 成员是与特定类关联的，可清晰地显示程序员的意图。

static 数据成员必须在类定义体的外部定义(正好一次)，static 关键字只能用于类定义体内部的声明中，定义不能标示为static. 不像普通数据成员，static成员不是通过类构造函数进行初始化，也不能在类的声明中初始化，而是应该在定义时进行初始化.保证对象正好定义一次的最好办法，就是将static 数据成员的定义放在包含类非内联成员函数定义的文件中。

静态数据成员初始化的格式为：

＜数据类型＞＜类名＞::＜静态数据成员名＞=＜值＞

类的静态数据成员有两种访问形式：

＜类对象名＞.＜静态数据成员名＞ 或 ＜类类型名＞::＜静态数据成员名＞

**42、static数据成员和static成员函数**

答：（1）static数据成员：

static数据成员独立于该类的任意对象而存在；每个static数据成员是与类关联的对象，并不与该类的对象相关联。Static数据成员（const

static数据成员除外）必须在类定义体的外部定义。不像普通数据成员，static成员不是通过类的构造函数进行初始化，而是应该在定义时进行初始化。

（2）static成员函数：

Static成员函数没有this形参，它可以直接访问所属类的static成员，不能直接使用非static成员。因为static成员不是任何对象的组成部分，所以static成员不能被声明为const。同时，static成员函数也不能被声明为虚函数。

**43、static成员变量定义放在cpp文件中，不能放在初始化列表中。Const static成员可就地初始化。**

**44、如何引用一个已经定义过的全局变量？**

答：可以用引用头文件的方式，也可以用extern关键字，如果用引用头文件方式来引用某个在头文件中声明的全局变量，假定你将那个变量写错了，那么在编译期间会报错，如果你用extern方式引用时，假定你犯了同样的错误，那么在编译期间不会报错，而在连接期间报错。

**44、static关键字的作用。**

答：static总是使得变量或对象的存储形式变成静态存储，连接方式变成内部连接，对于局部变量（已经是内部连接了），它仅改变其存储方式；对于全局变量（已经是静态存储了），它仅改变其连接类型。

**45、奈奎斯特定理**

**46、香农定理**

**47、多态类中的虚函数表是 Compile-Time，还是 Run-Time时建立的?**

答案：虚拟函数表是在编译期就建立了,各个虚拟函数这时被组织成了一个虚拟函数的入口地址的数组。而对象的隐藏成员--虚拟函数表指针是在运行期--也就是构造函数被调用时进行初始化的，这是实现多态的关键。

**48、一个父类写了一个 virtual 函数，如果子类覆盖它的函数不加 virtual ,也能实现多态?**

**在子类的空间里，有没有父类的这个函数，或者父类的私有变量? (华为笔试题）**

答案：只要基类在定义成员函数时已经声明了 virtue关键字，在派生类实现的时候覆盖该函数时，virtue关键字可加可不加，不影响多态的实现。子类的空间里有父类的所有变量(static除外)。

**49、完成字符串拷贝可以使用 sprintf、strcpy 及 memcpy 函数，请问这些函数有什么区别**

**，你喜欢使用哪个，为什么？**

答案：这些函数的区别在于 实现功能以及操作对象不同。

（1）strcpy 函数操作的对象是字符串，完成从源字符串到目的字符串的拷贝功能。

（2）sprintf 函数操作的对象不限于字符串：虽然目的对象是字符串，但是源对象可以是字符串、也可以是任意基本类型的数据。这个函数主要用来实现（字符串或基本数据类型）向字符串的转换功能。如果源对象是字符串，并且指定 %s 格式符，也可实现字符串拷贝功能。

（3）memcpy 函数顾名思义就是内存拷贝，实现将一个内存块的内容复制到另一个内存块这一功能。内存块由其首地址以及长度确定。程序中出现的实体对象，不论是什么类型，其最终表现就是在内存中占据一席之地（一个内存区间或块）。因此，memcpy

的操作对象不局限于某一类数据类型，或者说可适用于任意数据类型，只要能给出对象的起始地址和内存长度信息、并且对象具有可操作性即可。鉴于memcpy 函数等长拷贝的特点以及数据类型代表的物理意义，memcpy 函数通常限于同种类型数据或对象之间的拷贝，其中当然也包括字符串拷贝以及基本数据类型的拷贝。

对于字符串拷贝来说，用上述三个函数都可以实现，但是其实现的效率和使用的方便程度不同：

• strcpy 无疑是最合适的选择：效率高且调用方便。

• sprintf 要额外指定格式符并且进行格式转化，麻烦且效率不高。

• memcpy 虽然高效，但是需要额外提供拷贝的内存长度这一参数，易错且使用不便；并且如果长度指定过大的话（最优长度是源字符串长度 + 1），还会带来性能的下降。其实 strcpy 函数一般是在内部调用 memcpy 函数或者用汇编直接实现的，以达到高效的目的。因此，使用 memcpy 和 strcpy 拷贝字符串在性能上应该没有什么大的差别。

对于非字符串类型的数据的复制来说，strcpy 和 snprintf 一般就无能为力了，可是对 memcpy 却没有什么影响。但是，对于基本数据类型来说，尽管可以用 memcpy 进行拷贝，由于有赋值运算符可以方便且高效地进行同种或兼容类型的数据之间的拷贝，所以这种情况下 memcpy 几乎不被使用 。memcpy 的长处是用来实现（通常是内部实现居多）对结构或者数组的拷贝，其目的是或者高效，或者使用方便，甚或两者兼有。

**50、应用程序在运行时的内存包括代码区和数据区，其中数据区又包括哪些部分？**

答：对于一个进程的内存空间而言，可以在逻辑上分成 3个部份：代码区，静态数据区和动态数据区。

动态数据区一般就是“堆栈”。 栈是一种线性结构，堆是一种链式结构。进程的每个线程都有私有的“栈”。

全局变量和静态变量分配在静态数据区，本地变量分配在动态数据区，即堆栈中。程序通过堆栈的基地址和偏移量来访问本地变量。

**51、C++函数中值的传递方式有哪几种?**

答：三种传递方式为：值传递、指针传递和引用传递。

**52、C++里面是不是所有的动作都是main()引起的？如果不是，请举例**.

比如全局变量的初始化，就不是由main函数引起的

举例： class A{};

A a; //a的构造函数限执行

int main() {}

**53、下列哪两个是等同的**

int b;

A const int* a = &b;

B const* int a = &b;

C const int* const a = &b;

D int const* const a = &b;

**54、内联函数在编译时是否做参数类型检查？**

答：内联函数要做参数类型检查, 这是内联函数跟宏相比的优势。

**55、全局变量和局部变量有什么区别？实怎么实现的？操作系统和编译器是怎么知道的？**

（1）生命周期不同：

全局变量随主程序创建和创建，随主程序销毁而销毁

局部变量在局部函数内部，甚至局部循环体等内部存在，退出就不存在； 内存中

分配在全局数据区

（2）使用方式不同：通过声明后全局变量程序的各个部分都可以用到；局部变量只能在局部使用，分配在栈区

操作系统和编译器通过内存分配的位置来知道的，全局变量分配在全局数据段并且在程序开始运行的时候被加载。局部变量则分配在堆栈里面 。

**56、有 A 、 B 、 C 、 D 四个人，要在夜里过一座桥。他们通过这座桥分别需要耗时 1 、 2 、 5 、 10 分钟，只有一支手电，并且同时最多只能两个人一起过桥。请问，如何安排，能够在 17 分钟内这四个人都过桥？**

Solution:关键是时间最长的两个人必须同时过桥

The First Time ： A(1) 和 B(2) 过桥， A(1) 返回 Cost ： 1+2

The Second Time ： C(5) 和 D(10) 过桥， B(2) 返回 Cost ： 10+2

The Third Time A(1) 和 B(2) 过桥 Cost ： 2

Total Time Cost ： (1+2)+(10+2)+2=17 minutes

**57、static全局变量与普通的全局变量有什么区别？static局部变量和普通局部变量有什么区别？static函数与普通函数有什么区别？**

答：static全局变量与普通全局变量区别：static全局变量只初使化一次，防止在其他文件单元中被引用;

static局部变量和普通局部变量区别：static局部变量只被初始化一次，下一次依据上一次结果值；

static函数与普通函数区别：static函数在内存中只有一份，普通函数在每个被调用中维持一份拷贝。

**58、程序的局部变量存在于（堆栈）中，全局变量存在于（静态区 ）中，动态申请数据存在于（ 堆）中。**

**59、对于一个频繁使用的短小函数,在C语言中应用什么实现,在C++中应用什么实现?**

c用宏定义，c++用inline

**60、有1,2,....一直到n的无序数组,求排序算法,并且要求时间复杂度为O(n),空间复杂度O(1),使用交换,而且一次只能交换两个数。**

```text
#include<iostream.h>

Using namespace std；

int main(){

int a[] = {10,6,9,5,2,8,4,7,1,3};

int len = sizeof(a) / sizeof(int);

int temp;

for(int i = 0; i < len; )

{

temp = a[a[i] - 1];

a[a[i] - 1] = a[i];

a[i] = temp;

if ( a[i] == i + 1)

i++;

}

for (int j = 0; j < len; j++)

cout<<a[j]<<",";

return 0;

}
```

原文地址：https://zhuanlan.zhihu.com/p/470972286

作者：linux

# 【NO.418】C++高并发内存池的设计和实现

## 1.整体设计

### **1.1 需求分析**

池化技术是计算机中的一种设计模式，内存池是常见的池化技术之一，它能够有效的提高内存的申请和释放效率以及内存碎片等问题，但是传统的内存池也存在一定的缺陷，高并发内存池相对于普通的内存池它有自己的独特之处，解决了传统内存池存在的一些问题。

1）直接使用new/delete、malloc/free存在的问题

new/delete用于c++中动态内存管理而malloc/free在c++和c中都可以使用，本质上new/delete底层封装了malloc/free。无论是上面的那种内存管理方式，都存在以下两个问题：

效率问题：频繁的在堆上申请和释放内存必然需要大量时间，降低了程序的运行效率。对于一个需要频繁申请和释放内存的程序来说，频繁调用new/malloc申请内存,delete/free释放内存都需要花费系统时间，频繁的调用必然会降低程序的运行效率。

内存碎片：经常申请小块内存，会将物理内存“切”得很碎，导致内存碎片。申请内存的顺序并不是释放内存的顺序，因此频繁申请小块内存必然会导致内存碎片，造成“有内存但是申请不到大块内存”的现象。

2）普通内存池的优点和缺点

针对直接使用new/delete、malloc/free存在的问题，普通内存池的设计思路是：预先开辟一块大内存，程序需要内存时直接从该大块内存中“拿”一块，提高申请和释放内存的效率，同时直接分配大块内存还减少了内存碎片问题。

优点：申请和释放内存的效率有所提高；一定程度上解决了内存碎片问题。

缺点：多线程并发场景下申请和释放内存存在锁竞争问题造成申请和释放内存的效率降低。

3）高并发内存池要解决的问题

基于以上原因，设计高并发内存池需要解决以下三个问题：

- 效率问题
- 内存碎片问题
- 多线程并发场景下的内存释放和申请的锁竞争问题。

### **1.2 总体设计思路**

高并发内存池整体框架由以下三部分组成，各部分的功能如下：

- 线程缓存（thread cache）：每个线程独有线程缓存，主要解决多线程下高并发运行场景线程之间的锁竞争问题。线程缓存模块可以为线程提供小于64k内存的分配，并且多个线程并发运行不需要加锁。
- 中心控制缓存（central control cache）：中心控制缓存顾名思义，是高并发内存池的中心结构主要用来控制内存的调度问题。负责大块内存切割分配给线程缓存以及回收线程缓存中多余的内存进行合并归还给页缓存，达到内存分配在多个线程中更均衡的按需调度的目的，它在整个项目中起着承上启下的作用。（注意：这里需要加锁，当多个线程同时向中心控制缓存申请或归还内存时就存在线程安全问题，但是这种情况是极少发生的，并不会对程序的效率产生较大的影响，总体来说利大于弊）
- 页缓存（page cache）：以页为单位申请内存，为中心控制缓存提供大块内存。当中心控制缓存中没有内存对象时，可以从page cache中以页为单位按需获取大块内存，同时page cache还会回收central control cache的内存进行合并缓解内存碎片问题。

![img](https://pic1.zhimg.com/80/v2-628fc6858e8a2a1b89e64da8b66dec04_720w.webp)

### **1.3 申请内存流程图**

![img](https://pic2.zhimg.com/80/v2-c3900354c553e3f7c274128920e5f5e1_720w.webp)

## 2.详细设计

### **2.1 各个模块内部结构详细剖析**

1）thread cache

逻辑结构设计

thread cache的主要功能就是为每一个线程提供64K以下大小内存的申请。为了方便管理，需要提供一种特定的管理模式，来保存未分配的内存以及被释放回来的内存，以方便内存的二次利用。这里的管理通常采用将不同大小的内存映射在哈希表中，链接起来。而内存分配的最小单位是字节，64k = 1024*64Byte如果按照一个字节一个字节的管理方式进行管理，至少也得需要1024*64大小的哈希表对不同大小的内存进行映射。为了减少哈希表长度，这里采用按一定数字对齐的方式进行内存分配，将浪费率保持在1%~12%之间。具体结构如下：

![img](https://pic2.zhimg.com/80/v2-2043a11ccafd756ab3c578ef87f6ce91_720w.webp)

具体说明如下：

- 使用数组进行哈希映射，每一个位置存放的是一个链表freelists，该链表的作用是将相同大小的内存对象链接起来方便管理。
- 每个数组元素链接的都是不同大小的内存对象。
- 第一个元素表示对齐数是8，第2个是16....依次类推。对齐数表示在上一个对齐数和这个对齐数之间的大小的内存都映射在这个位置，即要申请1字节或者7字节的内存都在索引位置为0出找8字节大小的内存，要申请9~16字节大小的内存都在索引为1的位置找16字节的内存对象。
- 通过上面的分析，可以看出如果进行8字节对齐，最多会浪费7字节的内存（实际申请1字节内存，返回的是8字节大小的内存对象）,将这种现象称为内存碎片浪费。
- 为了将内存碎片浪费保持在12%以下，也就是说最多容忍有12%的内存浪费，这里使用不同的对齐数进行对齐。
- 0~128采用8字节对齐，129~1024采用16字节对齐，1025~8*1024采用128字节对齐，8*1024~64*1024采用1024字节对齐；内存碎片浪费率分别为：1/8，129/136，1025/1032，8102/8199均在12%左右。同时，8字节对齐时需要[0,15]共16个哈希映射；16字节对齐需要[16,71]共56个哈希映射；128字节对齐需要[72,127]共56个哈希映射;1024字节对齐需要[128,184]共56个哈希映射。
- 哈希映射的结构如下：

![img](https://pic4.zhimg.com/80/v2-d404d653a18a6cb2dc20687fc052d627_720w.webp)

如何保证每个线程独有？

大于64k的内存如何申请？

当thread cache中申请的内存大于64K时，直接向page cache申请。但是page cache中最大也只能申请128页的内存，所以当thread cache申请的内存大于128页时page cache中会自动给thread cache在系统内存中申请。

2）central control cache

central control cache作为thread cache和page cache的沟通桥梁，起到承上启下的作用。它需要向thread cache提供切割好的小块内存，同时他还需要回收thread cache中的多余内存进行合并，在分配给其他其他thread cache使用，起到资源调度的作用。它的结构如下：

![img](https://pic2.zhimg.com/80/v2-29c6886f3dcf57c1acd2143d1bb29709_720w.webp)

具体说明如下：

- central control cache的结构依然是一个数组，他保存的是span类型的对象。
- span是用来管理一块内存的，它里边包含了一个freelist链表，用于将大块内存切割成指定大小的小块内存链接到freelist中，当thread cache需要内存时直接将切割好的内存给thread cache。
- 开始时，每个数组索引位置都是空的，当thread cache申请内存时，spanList数组会向page cache申请一大块内存进行切割后挂在list中。当该快内存使用完，会继续申请新的内存，因此就存在多个span链接的情况。前边span存在对象是因为有可能后边已经申请好内存了前边的内存也释放回来了。
- 当某一个span的全部内存都还回来时，central control cache会再次将这块内存合并，在归还到page cache中。
- 当central control cache为空时，向page cache申请内存，每次至少申请一页，并且必须以页为单位进行申请（这里的页大小由我们自己决定，这里采用4K）。

这里需要注意的是，thread cache可能会有多个，但是central control cache只有一个，要让多个thread cache对象访问一个central control cache对象，这里的central control cache需要设计成单例模式。

3）page cache

page cache是以页为单位进行内存管理的，它是将不同页数的内存利用哈希进行映射，最多映射128页内存，具体结构如下：

![img](https://pic2.zhimg.com/80/v2-ef6ef4f51a86de058ece30b01247c7f5_720w.webp)

page Cache申请和释放内存流程：

- 当central control cache向page cache申请内存时，比如要申请8页的内存，它会先在span大小为8的位置找，如果没有就继续找9 10...128，那个有就从那个中切割8页。
- 例如，走到54时才有内存，就从54处切8页返回给central control cache，将剩余的54-846页挂在46页处。
- 当page cache中没有内存时，它直接申请一个128页的内存挂在128位置。当central control cache申请内存时再从128页切。

### **2.2 设计细节**

1）thread cache

根据申请内存大小计算对应的_freelists索引

- 1~8都映射在索引为0处，9~16都在索引为2处......
- 因此以8字节对齐时，可以表示为：((size + (2^3 - 1)) >> 3) - 1;
- 如果申请的内存为129，索引如何计算？
- 首先前128字节是按照8字节对齐的，因此：((129-128）+（2^4-1))>>4)-1 + 16
- 上式中16表示索引为0~15的16个位置以8字节对齐。

代码实现：

```text
//根据内存大小和对齐数计算对应下标
static inline size_t _Intex(size_t size, size_t alignmentShift)
{
	//alignmentShift表示对齐数的位数，例如对齐数为8 = 2^3时，aligmentShift = 3
	//这样可以将除法转化成>>运算，提高运算效率
	return ((size + (1 << alignmentShift) - 1) >> alignmentShift) - 1;
}
//根据内存大小，计算对应的下标
static inline size_t Index(size_t size)
{
	assert(size <= THREAD_MAX_SIZE);
 
	//每个对齐数对应的索引个数，分别表示8 16 128 1024字节对齐
	int groupArray[4] = {16,56,56,56};
 
	if (size <= 128)
	{
		//8字节对齐
		return _Intex(size, 3) + groupArray[0];
	}
	else if (size <= 1024)
	{
		//16字节对齐
		return _Intex(size, 4) + groupArray[1];
	}
	else if (size <= 8192)
	{
		//128字节对齐
		return _Intex(size, 7) + groupArray[2];
	}
	else if (size <= 65536)
	{
		//1024字节对齐
		return _Intex(size, 10) + groupArray[3];
	}
 
	assert(false);
	return -1;
}
```

freelist向中心缓存申请内存时需要对申请的内存大小进行对齐

首先，需要申请的内存大小不够对齐数时都需要进行向上对齐。即，要申请的内存大小为1字节时需要对齐到8字节。如何对齐？不进行对齐可以吗？

首先，不进行对齐也可以计算出freelist索引，当第一次申请内存时，freelist的索引位置切割后的内存大小就是实际申请的内存大小，并没有进行对齐，造成内存管理混乱。对齐方式如下：

- 对齐数分别为8 = 2^3; 16 = 2^4 ; 128 = 2^7 ; 1024 = 2^10,转化成二进制后只有1个1.
- 在对齐区间内，所有数+对齐数-1后一定是大于等于当前区间的最大值且小于下一个相邻区间的最大值。
- 因此，size + 对齐数 - 1如果是8字节对齐只需将低3位变为0，如果是16字节对齐将低3位变为0......
- 例如：size = 2时，对齐数为8；则size + 8 - 1 = 9，转为而进制位1001，将低三位变为0后为1000，转为十进制就是对齐数8.

> 代码表示如下：alignment表示对齐数
> (size + alignment - 1) & ~(alignment - 1);

注意：向这些小函数，定义成inline可以减少压栈开销。 ‘

如何将小块内存对象“挂在”freelist链表中

哈哈，前边已经为这里做好铺垫了。前边规定单个对象大小最小为8字节，32位系统下一个指针的大小为4字节，64位机器下一个指针的大小为8字节。前边我们规定单个对象最小大小为8字节就是为了无论是在32位系统下还是在64位系统下，都可以保存一个指针将小块对象链接起来。那么，如何使用一小块内存保存指针？

直接在内存的前4/8个字节将下一块内存的地址保存，取内存时直接对该内存解引用就可以取出地址。

> 访问：*(void**)(mem)

每次从freelist中取内存或者归还内存时，直接进行头插或头删即可。

从central control cache中申请内存，一次申请多少合适呢？

这里的思路是采用“慢启动”的方式申请，即第一次申请申请一个，第二次申请2个....当达到一定大小（512个）时不再增加。这样做的好处是，第一次申请给的数量少可以防止某些线程只需要一个多给造成浪费，后边给的多可以减少从central control cache的次数从而提高效率。

当使用慢启动得到的期望内存对象个数大于当前central control cache中内存对象的个数时，有多少给多少。因为，实际上目前只需要一个，我们多申请了不够，那就有多少给多少。当一个都没有的时候才会去page cache申请。

什么时候thread cache将内存还给central controlcache？

当一个线程将内存还给thread cache时，会去判断对应的_freelist的对应位置是否有太多的内存还回来（thread cache中内存对象的大小大于等于最个数的时候，就向central control cache还）。

2）Central Control Cache

SpanList结构

SpanList在central control cache中最重要的作用就是对大块内存管理，它存储的是一个个span类的对象，使用链表进行管理。结构如下：

![img](https://pic1.zhimg.com/80/v2-4c4a4faff188ba0443cba763223943dc_720w.webp)

也就是说，SpanList本质上就是一个span链表。这里考虑到后边归还内存需要找到对应页归还，方便插入，这里将spanlist设置成双向带头循环链表。

Span结构

Span存储的是大块内存的信息，陪SpanList共同管理大块内存，它的内存单位是页（4K）。它的结构实际上就是一个个size大小的对象链接起来的链表。它同时也作为SpanList的节点，spanList是双向循环链表，因此span中还有next和prev指针。

![img](https://pic2.zhimg.com/80/v2-4bfb9c366c319885a613a490adff6831_720w.webp)

```text
struct Span
{
    PageID _pageId = 0;   // 页号
    size_t _n = 0;        // 页的数量
    Span* _next = nullptr;
    Span* _prev = nullptr;
    void* _list = nullptr;  // 大块内存切小链接起来，这样回收回来的内存也方便链接
    size_t _usecount = 0;    // 使用计数，==0 说明所有对象都回来了
    size_t _objsize = 0;    // 切出来的单个对象的大小
};
```

当spanList中没有内存时需要向PageCache申请内存，一次申请多少合适呢？

根据申请的对象的大小分配内存，也就是说单个对象大小越小分配的页数越少，单个对象的大小越大分配到的内存越多。如何衡量多少？

这里我们是通过thread cache中从central control cache中获取的内存对象的个数的上限来确定。也就是说，个数的上限*内存对象的大小就是我们要申请的内存的大小。在右移12位（1页）就是需要申请的页数。

```text
//计算申请多少页内存
static inline size_t NumMovePage(size_t memSize)
{
	//计算thread cache最多申请多少个对象，这里就给多少个对象
	size_t num = NumMoveSize(memSize);
	//此时的nPage表示的是获取的内存大小
	size_t nPage = num*memSize;
	//当npage右移是PAGE_SHIFT时表示除2的PAGE_SHIFT次方，表示的就是页数
	nPage >>= PAGE_SHIFT;
 
	//最少给一页（体现了按页申请的原则）
	if (nPage == 0)
		nPage = 1;
 
	return nPage;
}
```

向central control cache申请一块内存，切割时如果最后产生一个碎片（不够一个对象大小的内存）如何处理？

一旦产生这种情况，最后的碎片内存只能丢弃不使用。但是对于我们的程序来说是不会产生的，因为我们每次申请至少一页，4096可以整除我们所对应的任何一个大小的对象。

central control cache何时将内存还给page cache？

thread cache将多余的内存会还给central control cache中的spanlist对应的span，span中有一个usecount用来统计该span中有多少个对象被申请走了，当usecount为0时，表示所有对象都还回来了，则将该span还给page cache，合并成更大的span。

3）Page Cache

当从一个大页切出一个小页内存时，剩余的内存如何挂在对应位置？

在Page cache中的span它是没有切割的，都是一个整页，也就是说这里的Span的list并没有使用到。这里计算内存的地址都是按照页号计算的，当一个Span中有多页内存时保存的是第一页的内存，那么就可以计算出剩余内存和切走内存的页号，设置相应的页号进行映射即可。

从一个大的Span中切时，采用头切还是尾切？

Span中如何通过页号计算地址？

每一页大小都是固定的，当我们从系统申请一块内存会返回该内存的首地址，申请内存时返回的都是一块连续的内存，所以我们可以使用内存首地址/页大小的方式计算出页号，通过这种方式计算出来的一大块内存的多个页的页号都是连续的。

![img](https://pic2.zhimg.com/80/v2-74916881fbcff5ed1f31d1701fcffe75_720w.webp)

Page Cache向系统申请内存

Page Cache向系统申请内存时，前边我们说过每次直接申请128页的内存。这里需要说明的是，我们的项目中不能出现任和STL中的数据结构和库函数，因此这里申请内存直接采用系统调用VirtualAlloc。下面对VirtualAlloc详细解释：

VirtualAlloc是一个Windows API函数，该函数的功能是在调用进程的虚地址空间,预定或者提交一部分页。简单点的意思就是申请内存空间。

函数声明如下：

```text
LPVOID VirtualAlloc{
LPVOID lpAddress, // 要分配的内存区域的地址
DWORD dwSize, // 分配的大小
DWORD flAllocationType, // 分配的类型
DWORD flProtect // 该内存的初始保护属性
};
```

参数说明：

- LPVOID lpAddress, 分配内存区域的地址。当你使用VirtualAlloc来提交一块以前保留的内存块的时候，lpAddress参数可以用来识别以前保留的内存块。如果这个参数是NULL，系统将会决定分配内存区域的位置，并且按64-KB向上取整(roundup)。
- SIZE_T dwSize, 要分配或者保留的区域的大小。这个参数以字节为单位，而不是页，系统会根据这个大小一直分配到下页的边界DWORD
- flAllocationType, 分配类型 ,你可以指定或者合并以下标志：MEM_COMMIT，MEM_RESERVE和MEM_TOP_DOWN。
- DWORD flProtect 指定了被分配区域的访问保护方式

注：PageCache中有一个map用来存储pageId和Span的映射。在释放内存时，通过memSize计算出pageId，在通过PageId在map中查找对应的Span从而就可以获得单个对象的大小，在根据单个对象的大小确定是要将内存还给page cache还是还给central control cache。

central control cache释放回来的内存如何合并成大内存？

通过span中的页号查找前一页和后一页，判断前一页和后一页是否空闲（没有被申请的内存），如果空闲就进行和并，合并完后重新在map中进行映射。

注意：将PageCache和CentralControlCache设置成单例模式，因为多个线程对同时使用一个page cache和central control cache进行内存管理。

单例模式简单介绍

- 单例模式，顾名思义只能创建一个实例。
- 有两种实现方式：懒汉实现和饿汉实现
- 做法：将构造函数和拷贝构造函数定义成私有且不能默认生成，防止在类外构造对象；定义一个本身类型的成员，在类中构造一个对象，提供接口供外部调用。

4）加锁问题

- 在central control cache和page cache中都存在多个线程访问同一临界资源的情况，因此需要加锁。
- 在central control cache中，不同线程只要访问的不是同一个大小的内存对象，则就不需要加锁，可以提高程序的运行效率（加锁后就有可能导致线程挂起等待），也就是说central control cache中是“桶锁”。需要改freelist那个位置的内存，就对那个加锁。
- page cache中，需要对申请和合并内存进行加锁。
- 这里我们统一使用互斥锁。

注意：使用map进行映射，虽然说我们对pagecache进行了加锁，不会早成写数据的冲突，但是我们还向外提供了查找的接口，就有可能导致一个线程在向map中写而另一个线程又查找，出现线程安全问题，但是如果给查找位置加锁，这个接口会被频繁的调用，造成性能的损失。而在tcmalloc中采用基数树来存储pageId和span的映射关系，从而提高效率。

## 3.测试

### 3.1 单元测试

```text
void func1()
{
	for (size_t i = 0; i < 10; ++i)
	{
		hcAlloc(17);
	}
}
 
void func2()
{
	for (size_t i = 0; i < 20; ++i)
	{
		hcAlloc(5);
	}
}
 
//测试多线程
void TestThreads()
{
	std::thread t1(func1);
	std::thread t2(func2);
 
 
	t1.join();
	t2.join();
}
 
//计算索引
void TestSizeClass()
{
	cout << SizeClass::Index(1035) << endl;
	cout << SizeClass::Index(1025) << endl;
	cout << SizeClass::Index(1024) << endl;
}
 
//申请内存
void TestConcurrentAlloc()
{
	void* ptr0 = hcAlloc(5);
	void* ptr1 = hcAlloc(8);
	void* ptr2 = hcAlloc(8);
	void* ptr3 = hcAlloc(8);
 
	hcFree(ptr1);
	hcFree(ptr2);
	hcFree(ptr3);
}
 
//大块内存的申请
void TestBigMemory()
{
	void* ptr1 = hcAlloc(65 * 1024);
	hcFree(ptr1);
 
	void* ptr2 = hcAlloc(129 * 4 * 1024);
	hcFree(ptr2);
}
 
//int main()
//{
//	//TestBigMemory();
//
//	//TestObjectPool();
//	//TestThreads();
//	//TestSizeClass();
//	//TestConcurrentAlloc();
//
//	return 0;
//}
```

### 3.2 性能测试

```text
void BenchmarkMalloc(size_t ntimes, size_t nworks, size_t rounds)
{
	//创建nworks个线程
	std::vector<std::thread> vthread(nworks);
	size_t malloc_costtime = 0;
	size_t free_costtime = 0;
 
	//每个线程循环依次
	for (size_t k = 0; k < nworks; ++k)
	{
		//铺货k
		vthread[k] = std::thread([&, k]() {
			std::vector<void*> v;
			v.reserve(ntimes);
 
			//执行rounds轮次
			for (size_t j = 0; j < rounds; ++j)
			{
				size_t begin1 = clock();
				//每轮次执行ntimes次
				for (size_t i = 0; i < ntimes; i++)
				{
					v.push_back(malloc(16));
				}
				size_t end1 = clock();
 
				size_t begin2 = clock();
				for (size_t i = 0; i < ntimes; i++)
				{
					free(v[i]);
				}
				size_t end2 = clock();
				v.clear();
 
				malloc_costtime += end1 - begin1;
				free_costtime += end2 - begin2;
			}
		});
	}
 
	for (auto& t : vthread)
	{
		t.join();
	}
 
	printf("%u个线程并发执行%u轮次，每轮次malloc %u次: 花费：%u ms\n",
		nworks, rounds, ntimes, malloc_costtime);
 
	printf("%u个线程并发执行%u轮次，每轮次free %u次: 花费：%u ms\n",
		nworks, rounds, ntimes, free_costtime);
 
	printf("%u个线程并发malloc&free %u次，总计花费：%u ms\n",
		nworks, nworks*rounds*ntimes, malloc_costtime + free_costtime);
}
 
 
// 单轮次申请释放次数 线程数 轮次
void BenchmarkConcurrentMalloc(size_t ntimes, size_t nworks, size_t rounds)
{
	std::vector<std::thread> vthread(nworks);
	size_t malloc_costtime = 0;
	size_t free_costtime = 0;
 
	for (size_t k = 0; k < nworks; ++k)
	{
		vthread[k] = std::thread([&]() {
			std::vector<void*> v;
			v.reserve(ntimes);
 
			for (size_t j = 0; j < rounds; ++j)
			{
				size_t begin1 = clock();
				for (size_t i = 0; i < ntimes; i++)
				{
					v.push_back(hcAlloc(16));
				}
				size_t end1 = clock();
 
				size_t begin2 = clock();
				for (size_t i = 0; i < ntimes; i++)
				{
					hcFree(v[i]);
				}
				size_t end2 = clock();
				v.clear();
 
				malloc_costtime += end1 - begin1;
				free_costtime += end2 - begin2;
			}
		});
	}
 
	for (auto& t : vthread)
	{
		t.join();
	}
 
	printf("%u个线程并发执行%u轮次，每轮次concurrent alloc %u次: 花费：%u ms\n",
		nworks, rounds, ntimes, malloc_costtime);
 
	printf("%u个线程并发执行%u轮次，每轮次concurrent dealloc %u次: 花费：%u ms\n",
		nworks, rounds, ntimes, free_costtime);
 
	printf("%u个线程并发concurrent alloc&dealloc %u次，总计花费：%u ms\n",
		nworks, nworks*rounds*ntimes, malloc_costtime + free_costtime);
}
 
int main()
{
	cout << "==========================================================" << endl;
	BenchmarkMalloc(100000, 4, 10);
	cout << endl << endl;
 
	BenchmarkConcurrentMalloc(100000, 4, 10);
	cout << "==========================================================" << endl;
 
	return 0;
}
```

结果比较

![img](https://pic2.zhimg.com/80/v2-bc38cb0b9978c69b6136767090a6182d_720w.webp)

附：[完整代码](https://link.zhihu.com/?target=https%3A//github.com/MouCoder/cpp_Code/commit/a4d2cd3676b0a866a14e3cf17a90998503208d8a)

原文地址：https://zhuanlan.zhihu.com/p/389658892

作者：linux

# 【NO.419】不懂并行和并发？一文彻底搞懂并行和并发的区

## 1.理解并发、并行的例子

先举例子来理解这2个概念的区别。

老师让两个同学去办公室谈话。如果这两同学(进程)是并列跨过办公室门(CPU)的，那么就是并行。如果同学A先进同学B后进入(或者先B后A)，或者两人并列同时进入，但是在办公室外的路人甲(用户)看来，同学A和同学B同时都在办公室内，这是并发。

![img](https://pic4.zhimg.com/80/v2-e00672e8303340ad3494648d17c5a43f_720w.webp)

其实这个例子不合理，因为真正的并行是多核CPU下的概念，但上面这个简单的例子非常有助于理解。

如果举例要精确一点，那么大概是这样的：进办公室有两个门(两CPU)，如果两同学分别从不同的门进入，不管先后性，两者互相独立，那么是并行；如果两同学不管以什么方式进入，在路人甲看来，他两同时都在办公室内，就是并发。

![img](https://pic1.zhimg.com/80/v2-442a60e3fc8c029f680f374d44d5eaf8_720w.webp)

我不信到现在还不理解并发和并行。

## 2.并发和并行的理论性解释

为什么操作系统上可以同时运行多个程序而用户感觉不出来？

这是因为无论是单CPU还是多CPU，操作系统都营造出了可以同时运行多个程序的**假象**。实际的过程操作系统对进程的调度以及CPU的快速上下文切换实现的：**每个进程执行一会就先停下来，然后CPU切换到下个被操作系统调度到的进程上使之运行**。因为切换的很快，使得用户认为操作系统一直在服务自己的程序。

再来解释并发就容易理解多了。

并发(concurrent)指的是多个程序可以同时运行的现象，更细化的是多进程可以同时运行或者多指令可以同时运行。但这不是重点，在描述并发的时候也不会去扣这种字眼是否精确，并发的重点在于它是一种现象。并发描述的是多进程同时运行的现象。但实际上，对于单核心CPU来说，同一时刻只能运行一个进程。所以，这里的"同时运行"表示的不是真的同一时刻有多个进程运行的现象，这是并行的概念，而是提供一种功能让用户看来多个程序同时运行起来了，但实际上这些程序中的进程不是一直霸占CPU的，而是执行一会停一会。

所以，并发和并行的区别就很明显了。**它们虽然都说是"多个进程同时运行"，但是它们的"同时"不是一个概念。并行的"同时"是同一时刻可以多个进程在运行(处于running)，并发的"同时"是经过上下文快速切换，使得看上去多个进程同时都在运行的现象，是一种OS欺骗用户的现象**。

实际上，**当程序中写下多进程或多线程代码时，这意味着的是并发而不是并行**。并发是因为多进程/多线程都是需要去完成的任务，不并行是因为并行与否由操作系统的调度器决定，可能会让多个进程/线程被调度到同一个CPU核心上。只不过调度算法会尽量让不同进程/线程使用不同的CPU核心，所以在实际使用中几乎总是会并行，但却不能以100%的角度去保证会并行。也就是说，**并行与否程序员无法控制，只能让操作系统决定**。

再次注明，并发是一种现象，之所以能有这种现象的存在，和CPU的多少无关，而是和进程调度以及上下文切换有关的。

理解了概念，再来深入扩展下。

## 3.串行、并行和并发

**任务描述**

如图：

![img](https://pic2.zhimg.com/80/v2-e23dd12c084fbd3bbd403ea4e3469455_720w.webp)

任务是将左边的一堆柴全部搬到右边烧掉，每个任务包括三个过程：取柴，运柴，放柴烧火。

这三个过程分别对应一个函数：

```text
func get { geting }
func carry { carrying }
func unload { unloading }
```

**串行模式**

串行表示所有任务都一一按先后顺序进行。串行意味着必须先装完一车柴才能运送这车柴，只有运送到了，才能卸下这车柴，并且只有完成了这整个三个步骤，才能进行下一个步骤。

和稍后所解释的并行相对比，串行是一次只能取得一个任务，并执行这个任务。

假设这堆柴需要运送4次才能运完，那么当写下的代码类似于下面这种时，那么就是串行非并发的模式：

```text
for(i=0;i<4;i++){
    get()
    carry()
    unload()
}
```

或者，将三个过程的代码全部集中到一个函数中也是如此：

```text
func task {
    geting
    carrying
    unloading
}

for(i=0;i<4;i++){
    task()
}
```

这两种都是串行的代码模式。画图描述：

![img](https://pic1.zhimg.com/80/v2-41ae30f3545e2e42ef6f754c4fac9724_720w.webp)

**并行模式**

并行意味着可以同时取得多个任务，并同时去执行所取得的这些任务。并行模式相当于将长长的一条队列，划分成了多条短队列，所以并行缩短了任务队列的长度。

正如前面所举的两同学进办公室的例子，串行的方式下，必须1个同学进入后第二个同学才进入，队列长度为2，而并行方式下可以同时进入，队列长度减半了。

并行的效率从代码层次上强依赖于多进程/多线程代码，从硬件角度上则依赖于多核CPU。

对于单进程/单线程，由于只有一个进程/线程在执行，所以尽管同时执行所取得的多个任务，但实际上这个进程/线程是不断的在多任务之间切换，一会执行一下这个，一会执行一下那个，就像是一个人在不同地方之间来回奔波。所以，单进程/线程的并行，效率比串行更低。

对于多进程/多线程，各进程/线程都可以执行各自所取得的任务，这是真正的并行。

但是，还需要考虑硬件层次上CPU核心数，如果只有单核CPU，那么在硬件角度上这单核CPU一次也只能执行一个任务，上面多进程/多线程的并行也并非真正意义上的并行。只有多核CPU，并且多进程/多线程并行，才是真正意义上的并行。

如下图，是多进程/多线程(2个工作者)的并行：

![img](https://pic1.zhimg.com/80/v2-2c5da2416dabd0116c076fd5c4c48a84_720w.webp)

**并发**

并发表示多个任务同时都要执行的现象，更详细的概念前面已经说面的够具体了。

其实，很多场景下都会使用并发的概念。比如同时500个http请求涌向了web服务器，比如有时候说并发数是1000等。

有时候也将并发当成任务，比如500并发数意味着500个任务，表示的是在一个特定的时间段内(约定俗成的大家认为是1秒)可以完成500个任务。这500个任务可以是单进程/单线程方式处理的，这时表示的是并发不并行的模式(coroutine就是典型的并发不并行)，即先执行完一个任务后才执行另一个任务，也可以是多进程/多线程方式处理的，这时表示的是并发且并行模式。

要解决大并发问题，通常是将大任务分解成多个小任务。很典型的一个例子是处理客户端的请求任务，这个大任务里面包含了监听并建立客户端的连接、处理客户端的请求、响应客户端。但基本上所有这类程序，都将这3部分任务分开了：在执行任何一个小任务的时候，都可以通过一些手段使得可以执行其它小任务，比如在处理请求的时候，可以继续保持监听状态。

由于操作系统对进程的调度是随机的，所以切分成多个小任务后，可能会从任一小任务处执行。这可能会出现一些现象：

- 可能出现一个小任务执行了多次，还没开始下个任务的情况。这时一般会采用队列或类似的数据结构来存放各个小任务的成果。比如负责监听的进程已经执行了多次，建立了多个连接，但是还没有调度到处理请求的进程去处理任何一个请求。
- 可能出现还没准备好第一步就执行第二步的可能。这时，一般采用多路复用或异步的方式，比如只有准备好产生了事件通知才执行某个任务。比如还没有和任何一个客户端建立连接时，就去执行了处理请求的进程。
- 可以多进程/多线程的方式并行执行这些小任务。也可以单进程/单线程执行这些小任务，这时很可能要配合多路复用才能达到较高的效率

看图非常容易理解：

![img](https://pic3.zhimg.com/80/v2-fb4d8719454a53723fe0f25ba077c81a_720w.webp)

上图中将一个任务中的三个步骤取柴、运柴、卸柴划分成了独立的小任务，有取柴的老鼠，有运柴的老鼠，有卸柴烧火的老鼠。

如果上图中所有的老鼠都是同一只，那么是串行并发的，如果是不同的多只老鼠，那么是并行并发的。

## 4.总结

并行和串行：

- 串行：一次只能取得一个任务并执行这一个任务

- 并行：可以同时通过多进程/多线程的方式取得多个任务，并以多进程或多线程的方式同时执行这些任务

- 注意点：

- - 如果是单进程/单线程的并行，那么效率比串行更差
  - 如果只有单核cpu，多进程并行并没有提高效率
  - 从任务队列上看，由于同时从队列中取得多个任务并执行，相当于将一个长任务队列变成了短队列

并发：

- 并发是一种现象：同时运行多个程序或多个任务需要被处理的现象

- 这些任务可能是并行执行的，也可能是串行执行的，和CPU核心数无关，是操作系统进程调度和CPU上下文切换达到的结果

- 解决大并发的一个思路是将大任务分解成多个小任务：

- - 可能要使用一些数据结构来避免切分成多个小任务带来的问题
  - 可以多进程/多线程并行的方式去执行这些小任务达到高效率
  - 或者以单进程/单线程配合多路复用执行这些小任务来达到高效率

原文地址：https://zhuanlan.zhihu.com/p/383279972

作者：linux

# 【NO.420】etcd：etcd的原理和应用场景全面解析

## 1.etcd介绍

### 1.1 etcd是什么

etcd 是云原生架构中重要的基础组件，由 CNCF 孵化托管。etcd 在微服务和 Kubernates 集群中不仅可以作为服务注册于发现，还可以作为 key-value 存储的中间件。

![img](https://pic1.zhimg.com/80/v2-87d935ce9618ce5a840aaf7acba6bf60_720w.webp)

http server：用于处理用户发送的API请求及其他etcd节点的同步与心跳信息请求

store：用于处理etcd支持的各类功能的事务，包括：数据索引、节点状态变更、监控与反馈、事件处理与执行等等，是etcd对用户提供大多数API功能的具体实现

raft：强一致性算法，是etcd的核心

wal（write ahead log）：预写式日志，是etcd的数据存储方式。除了在内存中存有所有数据的状态及节点的索引外，还通过wal进行持久化存储。

- 在wal中，所有的数据提交前都会事先记录日志
- entry是存储的具体日志内容
- snapshot是为了防止数据过多而进行的状态快照

### 1.2 etcd的特点

etcd的目标是构建一个高可用的分布式键值(key-value)数据库。具有以下特点：

- 简单：安装配置简单，而且提供了 HTTP API 进行交互，使用也很简单
- 键值对存储：将数据存储在分层组织的目录中，如同在标准文件系统中
- 监测变更：监测特定的键或目录以进行更改，并对值的更改做出反应
- 安全：支持 SSL 证书验证
- 快速：根据官方提供的 benchmark 数据，单实例支持每秒 2k+ 读操作
- 可靠：采用 raft 算法，实现分布式系统数据的可用性和一致性

etcd 采用 Go 语言编写，它具有出色的跨平台支持，很小的二进制文件和强大的社区。 etcd 机器之间的通信通过 raft 算法处理。

### 1.3 etcd的功能

etcd 是一个高度一致的分布式键值存储，它提供了一种可靠的方式来存储需要由分布式系统或机器集群访问的数据。它可以优雅地处理网络分区期间的 leader 选举，以应对机器的故障，即使是在 leader 节点发生故障时。

从简单的 Web 应用程序到 Kubernetes 集群，任何复杂的应用程序都可以从 etcd 中读取数据或将数据写入 etcd。

### 1.4 etcd的应用场景

最常用于服务注册与发现，作为集群管理的组件使用

也可以用于K-V存储，作为数据库使用

### 1.5 关于etcd的存储

etcd 是一个键值存储的组件，其他的应用都是基于其键值存储的功能展开。

etcd 的存储有如下特点：

1、采用kv型数据存储，一般情况下比关系型数据库快。

2、支持动态存储(内存)以及静态存储(磁盘)。

3、分布式存储，可集成为多节点集群。

4、存储方式，采用类似目录结构。

- 只有叶子节点才能真正存储数据，相当于文件。
- 叶子节点的父节点一定是目录，目录不能存储数据。

## 2.服务注册与发现

分布式系统中最常见的问题之一：在同一个分布式集群中的进程或服务如何才能找到对方并建立连接

服务发现就可以解决此问题。

从本质上说，服务发现就是要了解集群中是否有进程在监听 UDP 或者 TCP 端口，并且通过名字就可以进行查找和链接

### 2.1 服务发现的三大支柱

![img](https://pic3.zhimg.com/80/v2-43893de6b0d95b7c341edd67fb7250e2_720w.webp)

强一致性、高可用的服务存储目录。基于 Raft 算法的 etcd 天生就是这样一个强一致性、高可用的服务存储目录。

一种注册服务和服务健康状况的机制。用户可以在 etcd 中注册服务，并且对注册的服务配置 key TTL，定时保持服务的心跳以达到监控健康状态的效果。

一种查找和连接服务的机制。通过在 etcd 指定的主题下注册的服务业能在对应的主题下查找到。为了确保连接，我们可以在每个服务机器上都部署一个 Proxy 模式的 etcd，这样就可以确保访问 etcd 集群的服务都能够互相连接。

etcd2 中引入的 etcd/raft 库，是目前最稳定、功能丰富的开源一致性协议之一。作为 etcd、TiKV、CockcorachDB、Dgraph 等知名分布式数据库的核心数据复制引擎，etcd/raft 驱动了超过十万个集群，是被最为广泛采用一致性协议实现之一

### 2.2 消息发布与订阅

在多节点、分布式系统中，最适用的一种组件间通信方式就是消息发布与订阅,即：

- 即构建一个配置共享中心，数据提供者在这个配置中心发布消息，而消息使用者则订阅他们关心的主题，一旦主题有消息发布，就会实时通知订阅者。通过这种方式可以做到分布式系统配置的集中式管理与动态更新

![img](https://pic3.zhimg.com/80/v2-05dd6e8a2f51fb0a0a93bdb920a39f32_720w.webp)

应用中用到的一些配置信息放到etcd上进行集中管理。这类场景的使用方式通常是：

- 应用在启动的时候主动从etcd获取一次配置信息。
- 同时，在etcd节点上注册一个Watcher并等待
- 以后每次配置有更新的时候，etcd都会实时通知订阅者，以此达到获取最新配置信息的目的

分布式搜索服务中，索引的元信息和服务器集群机器的节点状态存放在etcd中，供各个客户端订阅使用。使用etcd的key TTL功能可以确保机器状态是实时更新的。

etcd中使用了Watcher机制，通过注册与异步通知机制，实现分布式环境下不同系统之间的通知与协调，从而对数据变更做到实时处理。实现方式：

- 不同系统都在etcd上对同一个目录进行注册，同时设置Watcher观测该目录的变化（如果对子目录的变化也有需要，可以设置递归模式）
- 当某个系统更新了etcd的目录，那么设置了Watcher的系统就会收到通知，并作出相应处理。

![img](https://pic2.zhimg.com/80/v2-8ad6899eba236db40249fac23d934cfd_720w.webp)

## 3.etcd集群的部署

为了整个集群的高可用，etcd一般都会进行集群部署，以避免单点故障。

引导etcd集群的启动有以下三种机制：

- 静态
- etcd动态发现
- DNS发现

**静态启动etcd集群**

单机安装

如果想要在一台机器上启动etcd集群，可以使用 goreman 工具（go语言编写的多进程管理工具，是对Ruby下官方使用的foreman的重写）

操作步骤：

- 安装Go运行环境
- 安装goreman：go get [http://github.com/mattn/goreman](https://link.zhihu.com/?target=http%3A//github.com/mattn/goreman)
- 配置goreman的配置加农本 Procfile：

![img](https://pic2.zhimg.com/80/v2-72370b1df8893388341c10ed26b47ea9_720w.webp)



```text
etcd1: etcd --name infra1 --listen-client-urls http://127.0.0.1:12379 --advertise-client-urls http://127.0.0.1:12379 --listen-peer-urls http://127.0.0.1:12380 --initial-advertise-peer-urls http://127.0.0.1:12380 --initial-cluster-token etcd-cluster-1 --initial-cluster 'infra1=http://127.0.0.1:12380,infra2=http://127.0.0.1:22380,infra3=http://127.0.0.1:32380' --initial-cluster-state new --enable-pprof --logger=zap --log-outputs=stderr
	etcd2: etcd --name infra2 --listen-client-urls http://127.0.0.1:22379 --advertise-client-urls http://127.0.0.1:22379 --listen-peer-urls http://127.0.0.1:22380 --initial-advertise-peer-urls http://127.0.0.1:22380 --initial-cluster-token etcd-cluster-1 --initial-cluster 'infra1=http://127.0.0.1:12380,infra2=http://127.0.0.1:22380,infra3=http://127.0.0.1:32380' --initial-cluster-state new --enable-pprof --logger=zap --log-outputs=stderr
	etcd3: etcd --name infra3 --listen-client-urls http://127.0.0.1:32379 --advertise-client-urls http://127.0.0.1:32379 --listen-peer-urls http://127.0.0.1:32380 --initial-advertise-peer-urls http://127.0.0.1:32380 --initial-cluster-token etcd-cluster-1 --initial-cluster 'infra1=http://127.0.0.1:12380,infra2=http://127.0.0.1:22380,infra3=http://127.0.0.1:32380' --initial-cluster-state new --enable-pprof --logger=zap --log-outputs=stderr
```

参数说明：

- –name：etcd集群中的节点名，这里可以随意，可区分且不重复即可
- –listen-peer-urls：监听的用于节点之间通信的url，可监听多个，集群内部将通过这些url进行数据交互(如选举，数据同步等)
- –initial-advertise-peer-urls：建议用于节点之间通信的url，节点间将以该值进行通信。
- –listen-client-urls：监听的用于客户端通信的url，同样可以监听多个。
- –advertise-client-urls：建议使用的客户端通信 url，该值用于 etcd 代理或 etcd 成员与 etcd 节点通信。
- –initial-cluster-token： etcd-cluster-1，节点的 token 值，设置该值后集群将生成唯一 id，并为每个节点也生成唯一 id，当使用相同配置文件再启动一个集群时，只要该 token 值不一样，etcd 集群就不会相互影响。
- –initial-cluster：也就是集群中所有的 initial-advertise-peer-urls 的合集。
- –initial-cluster-state：new，新建集群的标志

启动：goreman -f /opt/etcd/etc/procfile start

**docker启动集群**

etcd 使用 
[http://gcr.io/etcd-development/etcd](https://link.zhihu.com/?target=http%3A//gcr.io/etcd-development/etcd) 作为容器的主要加速器， [http://quay.io/coreos/etcd](https://link.zhihu.com/?target=http%3A//quay.io/coreos/etcd) 作为辅助的加速器：

- docker pull bitnami/etcd:3.4.7
- docker image tag bitnami/etcd:3.4.7 [http://quay.io/coreos/etcd:3.4.7](https://link.zhihu.com/?target=http%3A//quay.io/coreos/etcd%3A3.4.7)

镜像设置好之后，启动 3 个节点的 etcd 集群，脚本命令如下：

- 该脚本是部署在三台机器上，每台机器置行对应的脚本即可。

```text
REGISTRY=quay.io/coreos/etcd
    # For each machine
    ETCD_VERSION=3.4.7
    TOKEN=my-etcd-token
    CLUSTER_STATE=new
    NAME_1=etcd-node-0
    NAME_2=etcd-node-1
    NAME_3=etcd-node-2
    HOST_1= 192.168.202.128
    HOST_2= 192.168.202.129
    HOST_3= 192.168.202.130
    CLUSTER=${NAME_1}=http://${HOST_1}:2380,${NAME_2}=http://${HOST_2}:2380,${NAME_3}=http://${HOST_3}:2380
    DATA_DIR=/var/lib/etcd
    # For node 1
    THIS_NAME=${NAME_1}
    THIS_IP=${HOST_1}
    docker run \
      -p 2379:2379 \
      -p 2380:2380 \
      --volume=${DATA_DIR}:/etcd-data \
      --name etcd ${REGISTRY}:${ETCD_VERSION} \
      /usr/local/bin/etcd \
      --data-dir=/etcd-data --name ${THIS_NAME} \
      --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \
      --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \
      --initial-cluster ${CLUSTER} \
      --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}
    # For node 2
    THIS_NAME=${NAME_2}
    THIS_IP=${HOST_2}
    docker run \
      -p 2379:2379 \
      -p 2380:2380 \
      --volume=${DATA_DIR}:/etcd-data \
      --name etcd ${REGISTRY}:${ETCD_VERSION} \
      /usr/local/bin/etcd \
      --data-dir=/etcd-data --name ${THIS_NAME} \
      --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \
      --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \
      --initial-cluster ${CLUSTER} \
      --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}
    # For node 3
    THIS_NAME=${NAME_3}
    THIS_IP=${HOST_3}
    docker run \
      -p 2379:2379 \
      -p 2380:2380 \
      --volume=${DATA_DIR}:/etcd-data \
      --name etcd ${REGISTRY}:${ETCD_VERSION} \
      /usr/local/bin/etcd \
      --data-dir=/etcd-data --name ${THIS_NAME} \
      --initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://0.0.0.0:2380 \
      --advertise-client-urls http://${THIS_IP}:2379 --listen-client-urls http://0.0.0.0:2379 \
      --initial-cluster ${CLUSTER} \
      --initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN}
```

**动态发现启动etcd集群**

在实际环境中，集群成员的ip可能不会提前知道。这种情况下需要使用自动发现来引导etcd集群，而不是事先指定静态配置

协议原理

discovery service protocol 帮助新的 etcd 成员使用共享 URL 在集群引导阶段发现所有其他成员。

该协议使用新的发现令牌来引导一个唯一的 etcd 集群。一个发现令牌只能代表一个 etcd 集群。只要此令牌上的发现协议启动，即使它中途失败，也不能用于引导另一个 etcd 集群。

获取 discovery 的 token：

- 生成将标识新集群的唯一令牌：UUID=$(uuidgen)
- 指定集群的大小：curl -X PUT http://<\etcd_ip>:2379/v2/keys/discovery/6c007a14875d53d9bf0ef5a6fc0257c817f0fb83/_config/size -d value=3
- 将该url地址，作为 --discovery 参数来启动etcd，节点会自动使用该url目录进行etcd的注册和发现服务。

在完成了集群的初始化后，当再需要增加节点时，需要使用etcdctl进行操作，每次启动新的etcd集群时，都使用新的token进行注册。

**DNS自发现模式**

etcd核心API

![img](https://pic2.zhimg.com/80/v2-9f54519e12477fde8899faca9a33af59_720w.webp)

- KV 服务，创建，更新，获取和删除键值对。
- 监视，监视键的更改。
- 租约，消耗客户端保持活动消息的基元。
- 锁，etcd 提供分布式共享锁的支持。
- 选举，暴露客户端选举机制。

## 4.etcd典型应用场景（K8s）

### 4.1 **什么是k8s？**

开源的，用于管理云平台中多个主机上的容器化应用。

与传统应用部署方式的区别：

传统部署：

- 通过插件或脚本的方式安装应用。这样做的缺点是应用的运行、配置、管理、所有生存周期将与当前操作系统绑定，不利于应用的升级更新、回滚等操作。
- 由于资源利用不足而无法扩展，并且组织维护大量物理服务器的成本很高。

虚拟化部署：

- 虚拟化功能，允许在单个物理服务器的cpu上运行多个虚拟机（VM）
- 应用程序在VM之间隔离，提供安全级别
- 虚拟机非常重，可移植性、扩展性差

容器化部署：

- 通过容器的方式实现部署，每个容器之间相互隔离，每个容器有自己的文件系统，容器之间进程不会相互影响，能区分计算资源。
- 相对于虚拟机，容器能快速部署，由于容器与底层设施、机器文件系统是解耦的，所以它能在不同的云、不同版本操作系统之间进行迁移。
- 容器占用资源少、部署快，每个应用都可以被打包成一个容器镜像，每个应用与容器之间形成一对一的关系。每个应用不需要与其余的应用堆栈组合，也不依赖于生产环境的基础结构，这使得从研发–>测试–>生产能提供一致的环境。

[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(
img-fm4iUxNG-1607939778399)([https://d33wubrfki0l68.cloudfront.net/26a177ede4d7b032362289c6fccd448fc4a91174/eb693/images/docs/container_evolution.svg](https://link.zhihu.com/?target=https%3A//d33wubrfki0l68.cloudfront.net/26a177ede4d7b032362289c6fccd448fc4a91174/eb693/images/docs/container_evolution.svg))]

K8s提供了一个可弹性运行分布式系统的框架，可以满足实际生产环境所需要的扩展要求、故障转移、部署模式等

K8s提供如下功能：

- 服务发现与负载均衡
- 存储编排
- 自动部署和回滚
- 自动二进制打包：K8s允许指定每个容器所需CPU和内存（RAM），当容器制定了资源请求时，K8s可以做出更好的决策来管理容器的资源。
- 自我修复：K8s重新启动失败的容器，替换容器，杀死不响应用户定义的运行状况检查的容器，并且在准备好服务之前将其通告给客户端
- 密钥与配置管理

### 4.2 K8s的特点

可移植: 支持公有云，私有云，混合云，多重云（multi-cloud）

可扩展: 模块化，插件化，可挂载，可组合

自动化: 自动部署，自动重启，自动复制，自动伸缩/扩展

### 4.3 K8s组件

**master（主节点）组件：**

kube-apiserver：对外提供调用的开放接口服务

ETCD：提供默认的存储系统，保存所有集群数据

kube-controller-manager：运行管理控制器，是集群中处理常规任务的后台线程，包括：

- 节点控制器：
- 副本控制器：负责维护系统中每个副本中的pod（pod是最小的，管理，创建，计划的最小单元）
- 端点控制器：填充endpoints对象（连接service 和 pods）
- service account和token控制器：

cloud-controller-manager：云控制器管理器负责与底层云提供商的平台交互

kube-scheduler：监视新创建没有分配到node的pod，为pod选择一个node

插件 addons：实现集群pod和service功能

- DNS
- 用户界面
- 容器资源监测
- Cluster-level Logging

**node（计算节点）组件：**

kubelet：主要的节点代理，它会监视已分配给节点的pod

kube-proxy：通过在主机上维护网络规则并执行连接转发来实现k8s服务抽象

docker：运行容器

RKT：运行容器，作为docker工具的替代方案

supervisord：一个轻量级监控系统，用于保障kubelet和docker的运行

fluentd：守护进程，可提供cluster-level logging

![img](https://pic4.zhimg.com/80/v2-32fb98085d54bdb84644be6fd241d24f_720w.webp)

![img](https://pic3.zhimg.com/80/v2-21e0d33200cf328e4fb33b73f2be843e_720w.webp)

### 4.4 K8s典型架构图

![img](https://pic2.zhimg.com/80/v2-027b3819dc8adab8e6858eb3e010c6cd_720w.webp)

说明：

CNI：CNI（容器网络接口）是Cloud Native Computing Foundation项目，由一个规范和库（用于编写用于在Linux容器中配置网络接口的插件）以及许多受支持的插件组成。 CNI仅涉及容器的网络连接以及删除容器时删除分配的资源，通过json的语法定义了CNI插件所需要的输入和输出。

CRI：容器运行时接口，一个能让kubelet无需编译就可以支持多种容器运行时的插件接口。CRI包含了一组protocol buffer。gRPC API相关的库。

OCI：主要负责是容器的生命周期管理，OCI的runtime spec标准中对于容器状态的描述，以及对容器的创建、删除、查看等操作进行了定义。runc是对OCI标准的一个参考实现

原文地址：https://zhuanlan.zhihu.com/p/422184303

作者：linux

# 【NO.421】腾讯面试：linux内存性能优化总结

## **1.内存映射**

Linux 内核给每个进程都提供了一个独立且连续的虚拟地址空间，以便进程可以方便地访问虚拟内存；虚拟地址空间的内部又被分为内核空间和用户空间两部分，不同字长的处理器，地址空间的范围也不同；图示为 32 位和 64 位系统的虚拟地址空间；

![img](https://pic2.zhimg.com/80/v2-ab1833d585eda9af7e4bb47e032ff469_720w.webp)

内存映射是将虚拟内存地址映射到物理内存地址，内核为每个进程都维护了一张页表，记录虚拟地址与物理地址的映射关系；

![img](https://pic1.zhimg.com/80/v2-f07715157891438f24777269ac166fc8_720w.webp)

页表存储在 CPU 的内存管理单元 MMU 中，正常情况下，处理器就可以直接通过硬件，找出要访问的内存；当进程访问的虚拟地址在页表中不存在时，系统会产生一个缺页异常，进入内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行；

TLB (Translation Lookaside Buffer，转译后备缓冲器) 是 MMU 中页表的高速缓存，由于进程的虚拟地址空间是独立的，而 TLB 的访问速度又比 MMU 快得多，因此通过减少进程的上下文切换，减少 TLB 的刷新次数，可以提高 TLB 缓存的使用率，进而提高 CPU 的内存访问性能；

MMU 规定了内存映射的最小单位，即页，通常是 4 KB 大小，每一次内存映射，都需要关联 4KB 或者 4KB 整数倍的内存空间；

多级页表就是把内存分成区块来管理，将原来的映射关系改成区块索引和区块内的偏移；由于虚拟内存空间通常只用了很少一部分，多级页表就只保存这些使用中的区块，从而大大地减少页表的项数，Linux 四级页表管理内存页图示

![img](https://pic1.zhimg.com/80/v2-ff1e9eeae280ec21775d10ff7295fa38_720w.webp)

大页，即比普通页更大的内存块，常见的大小有 2MB 和 1GB；



## 2.虚拟内存空间分布

32 位系统中用户空间的分段示意图

![img](https://pic4.zhimg.com/80/v2-3a9cb107f5f54eb9c6c6bd7b1909e96f_720w.webp)

- \1. 只读段，包括代码和常量等
- \2. 数据段，包括全局变量等
- \3. 堆，包括动态分配的内存，从低地址开始向上增长
- \4. 文件映射段，包括动态库、共享内存等，从高地址开始向下增长
- \5. 栈，包括局部变量和函数调用的上下文等，栈的大小是固定的，一般是 8 MB

## 3.内存的分配与回收

**内存分配**

小块内存 (小于 128K)，使用 brk() 来分配，即通过移动堆顶的位置来分配内存，这些内存释放后并不会立刻归还系统，而是被缓存起来，以便重复使用；

- brk() 方式分配内存，可以减少缺页异常的发生，提高内存访问效率；由于这些内存没有归还系统，在内存工作繁忙时，频繁的内存分配和释放会造成内存碎片；

大块内存 (大于 128K)，使用内存映射 mmap() 来分配，即在文件映射段找一块空闲内存分配出去

- mmap() 方式分配内存，会在释放时直接归还系统，因此 mmap 会发生缺页异常；在内存工作繁忙时，频繁的内存分配会导致大量的缺页异常，使内核的管理负担增大；

**内存回收**

回收缓存，如使用 LRU (Least Recently Used) 算法，回收最近使用最少的内存页面；

回收不常访问的内存，把不常用的内存通过交换分区直接写到磁盘中；

- 交换分区 (Swap) 即把一块磁盘空间当成内存来用；把进程暂时不用的数据存储到磁盘中(换出)，当进程访问这些内存时，再从磁盘读取这些数据到内存中 (换入)

杀死进程，内存紧张时系统还会通过 OOM (Out of Memory)，直接杀掉占用大量内存的进程；

内核的一种保护机制，监控进程的内存使用情况，并且使用 oom_score 为每个进程的内存使用情况进行评分；

- 一个进程消耗的内存越大，oom_score 就越大；
- 一个进程运行占用的 CPU 越多，oom_score 就越小；
- 管理员可以通过 /proc 文件系统，手动设置进程的 oom_adj，从而调整进程的 oom_score；

oom_adj 的范围是 [-17, 15]，数值越大，表示进程越容易被 OOM 杀死；数值越小，表示进程越不容易被 OOM 杀死，其中 -17 表示禁止 OOM；

## 4.buffer/cache

Buffer 是内核缓冲区用到的内存，对应的是 /proc/meminfo 中的 Buffers 值；

Cache 是内核页缓存和 Slab 用到的内存，对应的是 /proc/meminfo 中的 Cached 与 SReclaimable 之和；

- Buffers 是对原始磁盘块的临时存储，即用来缓存磁盘的数据，通常不会特别大 (20MB 左右)；从而内核可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写等等；

Buffer 既可以用作 “将要写入磁盘数据的缓存”，也可以用作 “从磁盘读取数据的缓存”

- Cached 是从磁盘读取文件的页缓存，即用来缓存从文件读取的数据；从而，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘；

实际上，Cache 也会缓存写文件时的数据

Cache 既可以用作 “从文件读取数据的页缓存”，也可以用作 “写文件的页缓存”

- SReclaimable 是 Slab 的一部分，Slab 包括两部分，其中的可回收部分，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录；

## 5.内存泄漏

栈内存由系统自动分配和管理，一旦程序运行超出了这个局部变量的作用域，栈内存就会被系统自动回收，不会产生内存泄漏的问题；

堆内存由应用程序分配和管理，除非程序退出，这些堆内存并不会被系统自动释放，而是需要应用程序明确调用库函数 free() 释放，如果应用程序没有正确释放堆内存，就会造成内存泄漏；

只读段，包括程序的代码和常量，由于是只读的，不会再去分配新的内存，不会产生内存泄漏；

数据段，包括全局变量和静态变量，这些变量在定义时就已经确定了大小，不会产生内存泄漏；

内存映射段，包括动态链接库和共享内存，其中共享内存由程序动态分配和管理，若程序在分配后忘了回收，就会导致泄漏问题；

## 6.Swap 知识点

Swap 即把一块磁盘空间或者一个本地文件当成内存来使用，包括换出和换入两个过程；

- 换出，即把进程暂时不用的内存数据存储到磁盘中，并释放这些数据占用的内存；
- 换入，即在进程再次访问这些内存的时候，把它们从磁盘读到内存中来；

**NUMA 与 Swap**

NUMA (Non-Uniform Memory Access) 架构，在 NUMA 架构下，多个处理器被划分到不同 Node 上，且每个 Node 都拥有自己的本地内存空间，而同一个 Node 内部的内存空间，又可以进一步分为不同的内存域(Zone)；

某个 Node 内存不足时，系统可以从其他 Node 寻找空闲内存，也可以从本地内存中回收内存；可以通过 
/proc/sys/vm/zone_reclaim_mode 来选择模式，支持以下几个选项；

- 默认的 0，表示既可以从其他 Node 寻找空闲内存，也可以从本地回收内存；
- 1、2、4 都表示只回收本地内存，2 表示可以回写脏数据回收内存，4 表示可以用 Swap 方式回收内存；

**swappiness**

- 对文件页的回收，即直接回收缓存，或者把脏页写回磁盘后再回收；
- 对匿名页的回收，即通过 Swap 机制，把它们写入磁盘后再释放内存；

Linux 提供了 /proc/sys/vm/swappiness 选项，用来调整使用 Swap 的积极程度，swappiness 的范围是 0-100，数值越大，越积极使用 Swap，即更倾向于回收匿名页；数值越小，越消极使用 Swap，即更倾向于回收文件页；

降低 Swap 的使用，可以提高系统的整体性能

\1. 禁止 Swap，现在服务器的内存足够大，所以除非有必要，禁用 Swap 即可，随着云计算的普及，大部分云平台中的虚拟机都默认禁止 Swap；

\2. 若实在需要用到 Swap，可以尝试降低 swappiness 的值，减少内存回收时 Swap 的使用倾向；

\3. 响应延迟敏感的应用，如果它们可能在开启 Swap 的服务器中运行，你还可以用库函数 mlock() 或者 mlockall() 锁定内存，阻止它们的内存换出；

## 7.Linux 回收内存的时机

在内存资源紧张时，Linux 通过直接内存回收和定期扫描的方式，释放文件页和匿名页，以便把内存分配给更需要的进程使用；

直接内存回收，存在新的大块内存分配请求，但是剩余内存不足，此时系统就需要回收一部分内存，进而尽可能地满足新内存请求；

kswapd0 内核线程，用于定期回收内存，kswapd0 定义了三个内存阈值，分别是页最小阈值 (pages_min)、页低阈值(pages_low) 和页高阈值(pages_high)；pages_free 表示剩余内存；

![img](https://pic3.zhimg.com/80/v2-df2a2063fda8e7f5852ba0a9435febb2_720w.webp)

- 剩余内存小于页最小阈值，说明进程可用内存都耗尽了，只有内核才可以分配内存；
- 剩余内存落在页最小阈值和页低阈值中间，说明内存压力比较大，剩余内存不多了；这时 kswapd0 会执行内存回收，直到剩余内存大于高阈值为止；
- 剩余内存落在页低阈值和页高阈值中间，说明内存有一定压力，但还可以满足新内存请求；
- 剩余内存大于页高阈值，说明剩余内存比较多，没有内存压力；

页低阈值可以通过内核选项 
/proc/sys/vm/min_free_kbytes 来间接设置，min_free_kbytes 设置了页最小阈值，而其他两个阈值，都是根据页最小阈值计算生成，如下

```text
pages_low  = pages_min * 5 / 4
pages_high = pages_min * 3 / 2
```

## 8.性能指标与工具总结

![img](https://pic4.zhimg.com/80/v2-84440bba3c1a4c7e8e27985a7a6398db_720w.webp)

![img](https://pic3.zhimg.com/80/v2-b20f785c7cd311a1ce26c71045e04a36_720w.webp)

## 9.实战记录

**【1】free 命令**

![img](https://pic2.zhimg.com/80/v2-244e6595b82838de53571f34b5c25765_720w.webp)

- total 总内存大小；
- used 已使用内存的大小，包含了共享内存；
- free 未使用内存的大小；
- shared 共享内存的大小；
- buff/cache 缓存和缓冲区的大小；
- available 新进程可用内存的大小；

**【2】top 命令**

![img](https://pic4.zhimg.com/80/v2-be84fec554997abc962eac015e1e4103_720w.webp)

VIRT 是进程虚拟内存的大小，只要是进程申请过的内存，即便还没有真正分配物理内存，也会计算在内；

RES 是常驻内存的大小，即进程实际使用的物理内存大小，但不包括 Swap 和共享内存；

SHR 是共享内存的大小，比如与其他进程共同使用的共享内存、加载的动态链接库以及程序的代码段等；

%MEM 是进程使用物理内存占系统总内存的百分比；

注意

- \1. 虚拟内存通常并不会全部分配物理内存；
- \2. 共享内存 SHR 并不一定是共享的，如程序的代码段、非共享的动态链接库，也都算在 SHR 里；

**【3】磁盘和文件写案例**

【3.1】案例一，写文件

```text
测试命令
$ dd if=/dev/urandom of=/tmp/file bs=1M count=500
 
监控命令
vmstat 2
```

![img](https://pic3.zhimg.com/80/v2-f64b38b129cde37e5a1d61cf29b2f796_720w.webp)

- \1. 在 Cache 刚开始增长时，块设备 I/O 很少，而过一段时间后，才会出现大量的块设备写；
- \2. 当 dd 命令结束后，Cache 不再增长，但块设备写还会持续一段时间，并且多次 I/O 写的结果加起来是 dd 要写的 500M 的数据；

**【3.2】案例二，写磁盘**

```text
测试命令
# 运行dd命令向磁盘分区/dev/sdb1写入2G数据
$ dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048
 
监控命令
vmstat 2
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
1  0      0 7584780 153592  97436    0    0   684     0   31  423  1 48 50  2  0
 1  0      0 7418580 315384 101668    0    0     0     0   32  144  0 50 50  0  0
 1  0      0 7253664 475844 106208    0    0     0     0   20  137  0 50 50  0  0
 1  0      0 7093352 631800 110520    0    0     0     0   23  223  0 50 50  0  0
 1  1      0 6930056 790520 114980    0    0     0 12804   23  168  0 50 42  9  0
 1  0      0 6757204 949240 119396    0    0     0 183804   24  191  0 53 26 21  0
 1  1      0 6591516 1107960 123840    0    0     0 77316   22  232  0 52 16 33  0
```

- \1. 写磁盘时 (即 bo 大于 0 时)，Buffer 和 Cache 都在增长，但显然 Buffer 的增长快得多；

**【4】磁盘和文件读案例**

【4.1】案例一、读文件

```text
测试命令
# 运行dd命令读取文件数据
$ dd if=/tmp/file of=/dev/null
 
监控命令
vmstat 2
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  1      0 7724164   2380 110844    0    0 16576     0   62  360  2  2 76 21  0
 0  1      0 7691544   2380 143472    0    0 32640     0   46  439  1  3 50 46  0
 0  1      0 7658736   2380 176204    0    0 32640     0   54  407  1  4 50 46  0
 0  1      0 7626052   2380 208908    0    0 32640    40   44  422  2  2 50 46  0
```

- 读取文件时 (即 bi 大于 0 时)，Buffer 保持不变，而 Cache 则在不停增长；

【4.2】案例二、读磁盘

```text
测试命令
# 运行dd命令读取文件
$ dd if=/dev/sda1 of=/dev/null bs=1M count=1024
 
监控命令
vmstat 2
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
0  0      0 7225880   2716 608184    0    0     0     0   48  159  0  0 100  0  0
 0  1      0 7199420  28644 608228    0    0 25928     0   60  252  0  1 65 35  0
 0  1      0 7167092  60900 608312    0    0 32256     0   54  269  0  1 50 49  0
 0  1      0 7134416  93572 608376    0    0 32672     0   53  253  0  0 51 49  0
 0  1      0 7101484 126320 608480    0    0 32748     0   80  414  0  1 50 49  0
```

- 读磁盘时 (也就是 bi 大于 0 时)，Buffer 和 Cache 都在增长，但显然 Buffer 的增长快很多；

**附录**

【1】文件系统与磁盘的区别

磁盘是一个存储设备(块设备)，可以被划分为不同的磁盘分区，而在磁盘或者磁盘分区上，还可以再创建文件系统，并挂载到系统的某个目录中，这样，系统就可以通过这个挂载目录，来读写文件；即磁盘是存储数据的块设备，也是文件系统的载体；文件系统需要通过磁盘，来保证数据的持久化存储；

在读写普通文件时，I/O 请求会首先经过文件系统，然后由文件系统负责，来与磁盘进行交互；在读写块设备文件时，会跳过文件系统，直接与磁盘交互，也就是所谓的 “裸 I/O”；

【2】统计所有进程的物理内存使用量

每个进程的 PSS ，是指把共享内存平分到各个进程后，再加上进程本身的非共享内存大小的和；

```text
# 使用grep查找Pss指标后，再用awk计算累加值
$ grep Pss /proc/[1-9]*/smaps | awk '{total+=$2}; END {printf "%d kB\n", total }'
391266 kB
```

原文地址：https://zhuanlan.zhihu.com/p/431297893

作者：linux

# 【NO.422】linux内核vmalloc原理与实现

在 Linux 系统中的每个进程都有独立 4GB 内存空间，而 Linux 把这 4GB 内存空间划分为用户内存空间（0 ~ 3GB）和内核内存空间（3GB ~ 4GB），而内核内存空间由划分为直接内存映射区和动态内存映射区（vmalloc区）。

直接内存映射区从 3GB 开始到 3GB+896MB 处结束，直接内存映射区的特点就是物理地址与虚拟地址的关系为：虚拟地址 = 物理地址 + 3GB。而动态内存映射区不能通过这种简单的关系关联，而是需要访问动态内存映射区时，由内核动态申请物理内存并且映射到动态内存映射区中。下图是动态内存映射区在内存空间的位置：

![img](https://pic1.zhimg.com/80/v2-33c4056be2df5d21a827bff76dab9290_720w.webp)

## 1.为什么需要vmalloc区

由于直接内存映射区（3GB ~ 3GB+896MB）是直接映射到物理地址（0 ~ 896MB）的，所以内核不能通过直接内存映射区使用到超过 896MB 之外的物理内存。这时候就需要提供一个机制能够让内核使用 896MB 之外的物理内存，所以 Linux 就实现了一个 vmalloc 机制。vmalloc 机制的目的是在内核内存空间提供一个内存区，能够让这个内存区映射到 896MB 之外的物理内存。如下图：

![img](https://pic4.zhimg.com/80/v2-c3bf096397e6e70d8b648d13d5f9b123_720w.webp)

那么什么时候使用 vmalloc 呢？一般来说，如果要申请大块的内存就可以用vmalloc。

## 2.vmalloc实现

可以通过 vmalloc() 函数向内核申请一块内存，其原型如下：

```text
void * vmalloc(unsigned long size);
```

参数 size 表示要申请的内存块大小。

我们看看看 vmalloc() 函数的实现，代码如下：

```text
static inline void * vmalloc(unsigned long size)
{
    return __vmalloc(size, GFP_KERNEL|__GFP_HIGHMEM, PAGE_KERNEL);
}
```

从上面代码可以看出，vmalloc() 函数直接调用了 __vmalloc() 函数，而 __vmalloc() 函数的实现如下：

```text
void * __vmalloc(unsigned long size, int gfp_mask, pgprot_t prot)
{
    void * addr;
    struct vm_struct *area;

    size = PAGE_ALIGN(size); // 内存对齐
    if (!size || (size >> PAGE_SHIFT) > num_physpages) {
        BUG();
        return NULL;
    }

    area = get_vm_area(size, VM_ALLOC); // 申请一个合法的虚拟地址
    if (!area)
        return NULL;

    addr = area->addr;
    // 映射物理内存地址
    if (vmalloc_area_pages(VMALLOC_VMADDR(addr), size, gfp_mask, prot)) {
        vfree(addr);
        return NULL;
    }

    return addr;
}
```

__vmalloc() 函数主要工作有两点：

- 调用 get_vm_area() 函数申请一个合法的虚拟内存地址。
- 调用 vmalloc_area_pages() 函数把虚拟内存地址映射到物理内存地址。

接下来，我们看看 get_vm_area() 函数的实现，代码如下：

```text
struct vm_struct * get_vm_area(unsigned long size, unsigned long flags)
{
    unsigned long addr;
    struct vm_struct **p, *tmp, *area;

    area = (struct vm_struct *) kmalloc(sizeof(*area), GFP_KERNEL);
    if (!area)
        return NULL;
    size += PAGE_SIZE;
    addr = VMALLOC_START;
    write_lock(&vmlist_lock);
    for (p = &vmlist; (tmp = *p) ; p = &tmp->next) {
        if ((size + addr) < addr)
            goto out;
        if (size + addr <= (unsigned long) tmp->addr)
            break;
        addr = tmp->size + (unsigned long) tmp->addr;
        if (addr > VMALLOC_END-size)
            goto out;
    }
    area->flags = flags;
    area->addr = (void *)addr;
    area->size = size;
    area->next = *p;
    *p = area;
    write_unlock(&vmlist_lock);
    return area;

out:
    write_unlock(&vmlist_lock);
    kfree(area);
    return NULL;
}
```

get_vm_area() 函数比较简单，首先申请一个类型为 vm_struct 的结构 area 用于保存申请到的虚拟内存地址。然后查找可用的虚拟内存地址，如果找到，就把虚拟内存到虚拟内存地址保存到 area 变量中。最后把 area 连接到 vmalloc 虚拟内存地址管理链表 vmlist 中。vmlist 链表最终结果如下图：



![img](https://pic2.zhimg.com/80/v2-174c58ee01a475d497ae8294c7c4d839_720w.webp)



申请到虚拟内存地址后，__vmalloc() 函数会调用 vmalloc_area_pages() 函数来对虚拟内存地址与物理内存地址进行映射。

我们知道，映射过程就是对进程的 页表 进行映射。但每个进程都有一个独立 页表（内核线程除外），并且我们知道内核空间是所有进程共享的，那么就有个问题：如果只映射当前进程 页表 的内核空间，那么怎么同步到其他进程的内核空间呢？

为了解决内核空间同步问题，Linux 并不是直接对当前进程的内核空间映射的，而是对 init 进程的内核空间（init_mm）进行映射，我们来看看 vmalloc_area_pages() 函数的实现：

```text
inline int vmalloc_area_pages (unsigned long address, unsigned long size,
                               int gfp_mask, pgprot_t prot)
{
    pgd_t * dir;
    unsigned long end = address + size;
    int ret;

    dir = pgd_offset_k(address);         // 获取 address 地址在 init 进程对应的页目录项
    spin_lock(&init_mm.page_table_lock); // 对 init_mm 上锁
    do {
        pmd_t *pmd;

        pmd = pmd_alloc(&init_mm, dir, address);
        ret = -ENOMEM;
        if (!pmd)
            break;

        ret = -ENOMEM;
        if (alloc_area_pmd(pmd, address, end - address, gfp_mask, prot)) // 对页目录项进行映射
            break;

        address = (address + PGDIR_SIZE) & PGDIR_MASK;
        dir++;

        ret = 0;
    } while (address && (address < end));
    spin_unlock(&init_mm.page_table_lock);
    return ret;
}
```

从上面代码可以看出，vmalloc_area_pages() 函数映射的主体是 init 进程的内存空间。因为映射的 init 进程的内存空间，所以当前进程访问 vmalloc() 函数申请的内存时，由于没有对虚拟内存进行映射，所以会发生 缺页异常 而触发内核调用 do_page_fault() 函数来修复。我们看看 do_page_fault() 函数对 vmalloc() 申请的内存异常处理：

```text
void do_page_fault(struct pt_regs *regs, unsigned long error_code)
{
    ...
    __asm__("movl %%cr2,%0":"=r" (address));  // 获取出错的虚拟地址
    ...

    if (address >= TASK_SIZE && !(error_code & 5))
        goto vmalloc_fault;

    ...

vmalloc_fault:
    {
        int offset = __pgd_offset(address);
        pgd_t *pgd, *pgd_k;
        pmd_t *pmd, *pmd_k;
        pte_t *pte_k;

        asm("movl %%cr3,%0":"=r" (pgd));
        pgd = offset + (pgd_t *)__va(pgd);
        pgd_k = init_mm.pgd + offset;

        if (!pgd_present(*pgd_k))
            goto no_context;
        set_pgd(pgd, *pgd_k);

        pmd = pmd_offset(pgd, address);
        pmd_k = pmd_offset(pgd_k, address);
        if (!pmd_present(*pmd_k))
            goto no_context;
        set_pmd(pmd, *pmd_k);

        pte_k = pte_offset(pmd_k, address);
        if (!pte_present(*pte_k))
            goto no_context;
        return;
    }
}
```

上面的代码就是当进程访问 vmalloc() 函数申请到的内存时，发生 缺页异常 而进行的异常修复，主要的修复过程就是把 init 进程的 页表项 复制到当前进程的 页表项 中，这样就可以实现所有进程的内核内存地址空间同步。

原文地址：https://zhuanlan.zhihu.com/p/346892153

作者：linux

# 【NO.423】高性能服务器开发十大必须掌握的核心技术

这篇文章，我们循序渐进，从内存、磁盘I/O、网络I/O、CPU、缓存、架构、算法等多层次递进，串联起高性能开发十大必须掌握的核心技术。

- \- I/O优化：零拷贝技术
- \- I/O优化：多路复用技术
- \- 线程池技术
- \- 无锁编程技术
- \- 进程间通信技术
- \- RPC && 序列化技术
- \- 数据库索引技术
- \- 缓存技术 && 布隆过滤器
- \- 全文搜索技术
- \- 负载均衡技术

准备好了吗，坐稳了，发车！

首先，我们从最简单的模型开始。

老板告诉你，开发一个静态web服务器，把磁盘文件（网页、图片）通过网络发出去，怎么做？

你花了两天时间，撸了一个1.0版本：

- 主线程进入一个循环，等待连接
- 来一个连接就启动一个工作线程来处理
- 工作线程中，等待对方请求，然后从磁盘读文件、往套接口发送数据，完事儿

上线一天，老板发现太慢了，大一点的图片加载都有卡顿感。让你优化，这个时候，你需要：

## 1.**I/O优化：零拷贝技术**

上面的工作线程，从磁盘读文件、再通过网络发送数据，数据从磁盘到网络，兜兜转转需要拷贝四次，其中CPU亲自搬运都需要两次。

![img](https://pic2.zhimg.com/80/v2-7c3a4d016ce673c741e7a30ca2634a09_720w.webp)

**零拷贝技术**，解放CPU，文件数据直接从内核发送出去，无需再拷贝到应用程序缓冲区，白白浪费资源。

![img](https://pic4.zhimg.com/80/v2-d599c8873e9421da083c00c1af57a0ef_720w.webp)

Linux API：

```text
ssize_t sendfile(
  int out_fd, 
  int in_fd, 
  off_t *offset, 
  size_t count
  );
```

函数名字已经把函数的功能解释的很明显了：发送文件。指定要发送的文件描述符和网络套接字描述符，一个函数搞定！

用上了零拷贝技术后开发了2.0版本，图片加载速度明显有了提升。不过老板发现同时访问的人变多了以后，又变慢了，又让你继续优化。这个时候，你需要：

## 2.**I/O优化：多路复用技术**

前面的版本中，每个线程都要阻塞在recv等待对方的请求，这来访问的人多了，线程开的就多了，大量线程都在阻塞，系统运转速度也随之下降。

这个时候，你需要多路复用技术，使用**select**模型，将所有等待（accept、recv）都放在主线程里，工作线程不需要再等待。

![img](https://pic4.zhimg.com/80/v2-fd6ee0dc26643afa3071cec5e983da0b_720w.webp)

过了一段时间之后，网站访问的人越来越多了，就连select也开始有点应接不暇，老板继续让你优化性能。

这个时候，你需要升级多路复用模型为**epoll**。

**select有三弊，epoll有三优。**

- select底层采用数组来管理套接字描述符，同时管理的数量有上限，一般不超过几千个，epoll使用树和链表来管理，同时管理数量可以很大。
- select不会告诉你到底哪个套接字来了消息，你需要一个个去询问。epoll直接告诉你谁来了消息，不用轮询。
- select进行系统调用时还需要把套接字列表在用户空间和内核空间来回拷贝，循环中调用select时简直浪费。epoll统一在内核管理套接字描述符，无需来回拷贝。

用上了epoll多路复用技术，开发了3.0版本，你的网站能同时处理很多用户请求了。

但是贪心的老板还不满足，不舍得升级硬件服务器，却让你进一步提高服务器的吞吐量。你研究后发现，之前的方案中，工作线程总是用到才创建，用完就关闭，大量请求来的时候，线程不断创建、关闭、创建、关闭，开销挺大的。这个时候，你需要：

## 3.**线程池技术**

我们可以在程序一开始启动后就批量启动一波工作线程，而不是在有请求来的时候才去创建，使用一个公共的任务队列，请求来临时，向队列中投递任务，各个工作线程统一从队列中不断取出任务来处理，这就是**线程池技术**。

![img](https://pic2.zhimg.com/80/v2-5cf3faf8a6fb60778afb2f7845c5f059_720w.webp)

多线程技术的使用一定程度提升了服务器的并发能力，但同时，多个线程之间为了数据同步，常常需要使用互斥体、信号、条件变量等手段来同步多个线程。这些重量级的同步手段往往会导致线程在用户态/内核态多次切换，系统调用，线程切换都是不小的开销。

在线程池技术中，提到了一个公共的任务队列，各个工作线程需要从中提取任务进行处理，这里就涉及到多个工作线程对这个公共队列的同步操作。

有没有一些轻量级的方案来实现多线程安全的访问数据呢？这个时候，你需要：

## 4.**无锁编程技术**

多线程并发编程中，遇到公共数据时就需要进行线程同步。而这里的同步又可以分为**阻塞型同步**和**非阻塞型同步**。

阻塞型同步好理解，我们常用的互斥体、信号、条件变量等这些操作系统提供的机制都属于阻塞型同步，其本质都是要加“锁”。

![img](https://pic1.zhimg.com/80/v2-12d241b66ff35d3f17c82af7b1cc3af4_720w.webp)

与之对应的非阻塞型同步就是在无锁的情况下实现同步，目前有三类技术方案：

- Wait-free
- Lock-free
- Obstruction-free



三类技术方案都是通过一定的算法和技术手段来实现不用阻塞等待而实现同步，这其中又以Lock-free最为应用广泛。

Lock-free能够广泛应用得益于目前主流的CPU都提供了原子级别的read-modify-write原语，这就是著名的**CAS(Compare-And-Swap)**操作。在Intel x86系列处理器上，就是**cmpxchg**系列指令。

```text
// 通过CAS操作实现Lock-free
do {
  ...
} while(!CAS(ptr，old_data，new_data ))
```

我们常常见到的无锁队列、无锁链表、无锁HashMap等数据结构，其无锁的核心大都来源于此。在日常开发中，恰当的运用无锁化编程技术，可以有效地降低多线程阻塞和切换带来的额外开销，提升性能。

服务器上线了一段时间，发现服务经常崩溃异常，排查发现是工作线程代码bug，一崩溃整个服务都不可用了。于是你决定把工作线程和主线程拆开到不同的进程中，工作线程崩溃不能影响整体的服务。这个时候出现了多进程，你需要：

## 5.**进程间通信技术**

提起进程间通信，你能想到的是什么？

- 管道
- 命名管道
- socket
- 消息队列
- 信号
- 信号量
- 共享内存

以上各种进程间通信的方式详细介绍和比较，推荐一篇文章[浅析进程间通信的几种方式（含实例源码）](https://zhuanlan.zhihu.com/p/94856678)，这里不再赘述。

对于本地进程间需要高频次的大量数据交互，首推**共享内存**这种方案。

现代操作系统普遍采用了基于虚拟内存的管理方案，在这种内存管理方式之下，各个进程之间进行了强制隔离。程序代码中使用的内存地址均是一个**虚拟地址**，由操作系统的内存管理算法提前分配映射到对应的物理内存页面，CPU在执行代码指令时，对访问到的内存地址再进行实时的转换翻译。

![img](https://pic3.zhimg.com/80/v2-1ea2f52c32f1d330465e19337a4c29c2_720w.webp)

从上图可以看出，不同进程之中，虽然是同一个内存地址，最终在操作系统和CPU的配合下，实际存储数据的内存页面却是不同的。

而共享内存这种进程间通信方案的核心在于：*如果让同一个物理内存页面映射到两个进程地址空间中，双方不是就可以直接读写，而无需拷贝了吗？*

![img](https://pic1.zhimg.com/80/v2-0a008191ed8eb2d84e075048c3b0716c_720w.webp)

当然，共享内存只是最终的数据传输载体，双方要实现通信还得借助信号、信号量等其他通知机制。

用上了高性能的共享内存通信机制，多个服务进程之间就可以愉快的工作了，即便有工作进程出现Crash，整个服务也不至于瘫痪。

不久，老板增加需求了，不再满足于只能提供静态网页浏览了，需要能够实现动态交互。这一次老板还算良心，给你加了一台硬件服务器。

于是你用Java/PHP/Python等语言搞了一套web开发框架，单独起了一个服务，用来提供动态网页支持，和原来等静态内容服务器配合工作。

这个时候你发现，静态服务和动态服务之间经常需要通信。

一开始你用基于HTTP的RESTful接口在服务器之间通信，后来发现用JSON格式传输数据效率低下，你需要更高效的通信方案。

这个时候你需要：

## 6.**RPC && 序列化技术**

什么是RPC技术？

**RPC全称Remote Procedure Call**，远程过程调用。我们平时编程中，随时都在调用函数，这些函数基本上都位于本地，也就是当前进程某一个位置的代码块。但如果要调用的函数不在本地，而在网络上的某个服务器上呢？这就是远程过程调用的来源。

![img](https://pic1.zhimg.com/80/v2-99caeb44c3289ebf327ef7bf7428b878_720w.webp)

从图中可以看出，通过网络进行功能调用，涉及参数的打包解包、网络的传输、结果的打包解包等工作。而其中对数据进行打包和解包就需要依赖序列化技术来完成。

什么是序列化技术？

![img](https://pic2.zhimg.com/80/v2-1597f9450f079cd8628766c06c087451_720w.webp)

序列化简单来说，是将内存中的对象转换成可以传输和存储的数据，而这个过程的逆向操作就是反序列化。序列化 && 反序列化技术可以实现将内存对象在本地和远程计算机上搬运。好比把大象关进冰箱门分三步：

- 将本地内存对象编码成数据流
- 通过网络传输上述数据流
- 将收到的数据流在内存中构建出对象

序列化技术有很多免费开源的框架，衡量一个序列化框架的指标有这么几个：

- 是否支持跨语言使用，能支持哪些语言
- 是否只是单纯的序列化功能，包不包含RPC框架
- 序列化传输性能
- 扩展支持能力（数据对象增删字段后，前后的兼容性）
- 是否支持动态解析（动态解析是指不需要提前编译，根据拿到的数据格式定义文件立即就能解析）

下面流行的三大序列化框架protobuf、thrift、avro的对比：

### 6.1 **ProtoBuf：**

`厂商`：Google

`支持语言`：C++、Java、Python等

`动态性支持`：较差，一般需要提前编译

`是否包含RPC`：否

`简介`：ProtoBuf是谷歌出品的序列化框架，成熟稳定，性能强劲，很多大厂都在使用。自身只是一个序列化框架，不包含RPC功能，不过可以与同是Google出品的GPRC框架一起配套使用，作为后端RPC服务开发的黄金搭档。

![img](https://pic2.zhimg.com/80/v2-6388825c1614d6f5ae2d4cde1e2579b1_720w.webp)

缺点是对动态性支持较弱，不过在更新版本中这一现象有待改善。总体来说，ProtoBuf都是一款非常值得推荐的序列化框架。

### 6.2 **Thrift**

`厂商`：Facebook

`支持语言`：C++、Java、Python、PHP、C#、Go、JavaScript等

`动态性支持`：差

`是否包含RPC`：是

`简介`：这是一个由Facebook出品的RPC框架，本身内含二进制序列化方案，但Thrift本身的RPC和数据序列化是解耦的，你甚至可以选择XML、JSON等自定义的数据格式。在国内同样有一批大厂在使用，性能方面和ProtoBuf不分伯仲。缺点和ProtoBuf一样，对动态解析的支持不太友好。

### 6.3 **Avro**

`支持语言`：C、C++、Java、Python、C#等

`动态性支持`：好

`是否包含RPC`：是

`简介`：这是一个源自于Hadoop生态中的序列化框架，自带RPC框架，也可独立使用。相比前两位最大的优势就是支持动态数据解析。

![img](https://pic4.zhimg.com/80/v2-92b20ad41e41633f4bd2660c29e73d6f_720w.webp)

为什么我一直在说这个动态解析功能呢？在之前的一段项目经历中，轩辕就遇到了三种技术的选型，摆在我们面前的就是这三种方案。需要一个C++开发的服务和一个Java开发的服务能够进行RPC。

Protobuf和Thrift都需要通过“编译”将对应的数据协议定义文件编译成对应的C++/Java源代码，然后合入项目中一起编译，从而进行解析。

当时，Java项目组同学非常强硬的拒绝了这一做法，其理由是这样编译出来的强业务型代码融入他们的业务无关的框架服务，而业务是常变的，这样做不够优雅。

最后，经过测试，最终选择了AVRO作为我们的方案。Java一侧只需要动态加载对应的数据格式文件，就能对拿到的数据进行解析，并且性能上还不错。（当然，对于C++一侧还是选择了提前编译的做法）

自从你的网站支持了动态能力，免不了要和数据库打交道，但随着用户的增长，你发现数据库的查询速度越来越慢。

这个时候，你需要：

## 7.**数据库索引技术**

想想你手上有一本数学教材，但是目录被人给撕掉了，现在要你翻到讲三角函数的那一页，你该怎么办？

没有了目录，你只有两种办法，要么一页一页的翻，要么随机翻，直到找到三角函数的那一页。

对于数据库也是一样的道理，如果我们的数据表没有“目录”，那要查询满足条件的记录行，就得全表扫描，那可就恼火了。所以为了加快查询速度，得给数据表也设置目录，在数据库领域中，这就是**索引**。

一般情况下，数据表都会有多个字段，那根据不同的字段也就可以设立不同的索引。

### 7.1 **索引的分类**

- 主键索引
- 聚集索引
- 非聚集索引

主键我们都知道，是唯一标识一条数据记录的字段（也存在多个字段一起来唯一标识数据记录的**联合主键**），那与之对应的就是主键索引了。

聚集索引是指索引的逻辑顺序与表记录的物理存储顺序一致的索引，一般情况下主键索引就符合这个定义，所以一般来说主键索引也是聚集索引。但是，这不是绝对的，在不同的数据库中，或者在同一个数据库下的不同存储引擎中还是有不同。

聚集索引的叶子节点直接存储了数据，也是数据节点，而非聚集索引的叶子节点没有存储实际的数据，需要二次查询。

### 7.2 **索引的实现原理**

索引的实现主要有三种：

- B+树
- 哈希表
- 位图

其中，B+树用的最多，其特点是树的节点众多，相较于二叉树，这是一棵多叉树，是一个扁平的胖树，减少树的深度有利于减少磁盘I/O次数，适宜数据库的存储特点。

![img](https://pic1.zhimg.com/80/v2-917ed845e490a397f9a5a1d939b4d038_720w.webp)

哈希表实现的索引也叫散列索引，通过哈希函数来实现数据的定位。哈希算法的特点是速度快，常数阶的时间复杂度，但缺点是只适合准确匹配，不适合模糊匹配和范围搜索。

![img](https://pic4.zhimg.com/80/v2-483a1f39930e227d398970283dcc81f3_720w.webp)

位图索引相对就少见了。想象这么一个场景，如果某个字段的取值只有有限的少数几种可能，比如性别、省份、血型等等，针对这样的字段如果用B+树作为索引的话会出现什么情况？会出现大量索引值相同的叶子节点，这实际上是一种存储浪费。

位图索引正是基于这一点进行优化，针对字段取值只有少量有限项，数据表中该列字段出现大量重复时，就是位图索引一展身手的时机。

所谓位图，就是Bitmap，其基本思想是对该字段每一个取值建立一个二进制位图来标记数据表的每一条记录的该列字段是否是对应取值。

![img](https://pic2.zhimg.com/80/v2-757c0de25e57af7e8016b72c367cd021_720w.webp)

索引虽好，但也不可滥用，一方面索引最终是要存储到磁盘上的，无疑会增加存储开销。另外更重要的是，数据表的增删操作一般会伴随对索引的更新，因此对数据库的写入速度也是会有一定影响。

你的网站现在访问量越来越大了，同时在线人数大大增长。然而，大量用户的请求带来了后端程序对数据库大量的访问。渐渐的，数据库的瓶颈开始出现，无法再支持日益增长的用户量。老板再一次给你下达了性能提升的任务。

## 8.**缓存技术 && 布隆过滤器**

从物理CPU对内存数据的缓存到浏览器对网页内容的缓存，**缓存**技术遍布于计算机世界的每一个角落。

面对当前出现的数据库瓶颈，同样可以用缓存技术来解决。

每次访问数据库都需要数据库进行查表（当然，数据库自身也有优化措施），反映到底层就是进行一次或多次的磁盘I/O，但凡涉及I/O的就会慢下来。如果是一些频繁用到但又不会经常变化的数据，何不将其缓存在内存中，不必每一次都要找数据库要，从而减轻对数据库对压力呢？

![img](https://pic3.zhimg.com/80/v2-7c8d509624e1a1f4b282668fdb470c8e_720w.webp)

有需求就有市场，有市场就会有产品，以**memcached**和**Redis**为代表的内存对象缓存系统应运而生。

缓存系统有三个著名的问题：

- `缓存穿透`: 缓存设立的目的是为了一定层面上截获到数据库存储层的请求。穿透的意思就在于这个截获没有成功，请求最终还是去到了数据库，缓存没有产生应有的价值。
- `缓存击穿`: 如果把缓存理解成一面挡在数据库面前的墙壁，为数据库“抵御”查询请求，所谓击穿，就是在这面墙壁上打出了一个洞。一般发生在某个热点数据缓存到期，而此时针对该数据的大量查询请求来临，大家一股脑的怼到了数据库。
- `缓存雪崩`: 理解了击穿，那雪崩就更好理解了。俗话说得好，击穿是一个人的雪崩，雪崩是一群人的击穿。如果缓存这堵墙上处处都是洞，那这面墙还如何屹立？吃枣药丸。

关于这三个问题这里不详细讲述。

有了缓存系统，我们就可以在向数据库请求之前，先询问缓存系统是否有我们需要的数据，如果有且满足需要，我们就可以省去一次数据库的查询，如果没有，我们再向数据库请求。

注意，这里有一个关键的问题，如何判断我们要的数据是不是在缓存系统中呢？

进一步，我们把这个问题抽象出来：**如何快速判断一个数据量很大的集合中是否包含我们指定的数据？**

![img](https://pic2.zhimg.com/80/v2-b85390a444e20dd5c8d8ac8c7b1aaa21_720w.webp)

这个时候，就是**布隆过滤器**大显身手的时候了，它就是为了解决这个问题而诞生的。那布隆过滤器是如何解决这个问题的呢？

先回到上面的问题中来，这其实是一个查找问题，对于查找问题，最常用的解决方案是搜索树和哈希表两种方案。

因为这个问题有两个关键点：快速、数据量很大。树结构首先得排除，哈希表倒是可以做到常数阶的性能，但数据量大了以后，一方面对哈希表的容量要求巨大，另一方面如何设计一个好的哈希算法能够做到如此大量数据的哈希映射也是一个难题。

对于容量的问题，考虑到只需要判断对象是否存在，而并非拿到对象，我们可以将哈希表的表项大小设置为1个bit，1表示存在，0表示不存在，这样大大缩小哈希表的容量。

而对于哈希算法的问题，如果我们对哈希算法要求低一些，那哈希碰撞的机率就会增加。那一个哈希算法容易冲突，那就多弄几个，多个哈希函数同时冲突的概率就小的多。

布隆过滤器就是基于这样的设计思路：

![img](https://pic1.zhimg.com/80/v2-002abe289e3fc8254431cecc266cc334_720w.webp)

当设置对应的key-value时，按照一组哈希算法的计算，将对应比特位置1。

但当对应的key-value删除时，却不能将对应的比特位置0，因为保不准其他某个key的某个哈希算法也映射到了同一个位置。

也正是因为这样，引出了布隆过滤器的另外一个重要特点：**布隆过滤器判定存在的实际上不一定存在，但判定不存在的则一定不存在。**

你们公司网站的内容越来越多了，用户对于快速全站搜索的需求日益强烈。这个时候，你需要：

## 9.**全文搜索技术**

对于一些简单的查询需求，传统的关系型数据库尚且可以应付。但搜索需求一旦变得复杂起来，比如根据文章内容关键字、多个搜索条件但逻辑组合等情况下，数据库就捉襟见肘了，这个时候就需要单独的索引系统来进行支持。

![img](https://pic3.zhimg.com/80/v2-ecf7c332af697d9816f066576b17e6f6_720w.webp)

如今行业内广泛使用的**ElasticSearch**（简称ES）就是一套强大的搜索引擎。集全文检索、数据分析、分布式部署等优点于一身，成为企业级搜索技术的首选。

![img](https://pic4.zhimg.com/80/v2-ddac25bd033023aea1eda30928176ab7_720w.webp)

ES使用RESTful接口，使用JSON作为数据传输格式，支持多种查询匹配,为各主流语言都提供了SDK，易于上手。

另外，ES常常和另外两个开源软件Logstash、Kibana一起，形成一套日志收集、分析、展示的完整解决方案：**ELK架构**。

![img](https://pic1.zhimg.com/80/v2-de982082deb6128455e10b37e90a84b4_720w.webp)

其中，Logstash负责数据的收集、解析，ElasticSearch负责搜索，Kibana负责可视化交互，成为不少企业级日志分析管理的铁三角。

无论我们怎么优化，一台服务器的力量终究是有限的。公司业务发展迅猛，原来的服务器已经不堪重负，于是公司采购了多台服务器，将原有的服务都部署了多份，以应对日益增长的业务需求。

现在，同一个服务有多个服务器在提供服务了，需要将用户的请求均衡的分摊到各个服务器上，这个时候，你需要：

## 10.**负载均衡技术**

顾名思义，**负载均衡**意为将负载均匀平衡分配到多个业务节点上去。

![img](https://pic2.zhimg.com/80/v2-100a37a149f3786e632d40528731bdb1_720w.webp)

和缓存技术一样，负载均衡技术同样存在于计算机世界到各个角落。

**按照均衡实现实体**，可以分为软件负载均衡（如LVS、Nginx、HAProxy）和硬件负载均衡（如A10、F5）。

**按照网络层次**，可以分为四层负载均衡（基于网络连接）和七层负载均衡（基于应用内容）。

**按照均衡策略算法**，可以分为轮询均衡、哈希均衡、权重均衡、随机均衡或者这几种算法相结合的均衡。

而对于现在遇到等问题，可以使用nginx来实现负载均衡，nginx支持轮询、权重、IP哈希、最少连接数目、最短响应时间等多种方式的负载均衡配置。

### 10.1 **轮询**

```text
upstream web-server {
    server 192.168.1.100;
    server 192.168.1.101;
}
```

### 10.2 **权重**

```text
upstream web-server {
    server 192.168.1.100 weight=1;
    server 192.168.1.101 weight=2;
}
```

### 10.3 **IP哈希值**

```text
upstream web-server {
    ip_hash;
    server 192.168.1.100 weight=1;
    server 192.168.1.101 weight=2;
}
```

### 10.4 **最少连接数目**

```text
upstream web-server {
    least_conn;
    server 192.168.1.100 weight=1;
    server 192.168.1.101 weight=2;
}
```

### 10.5 **最短响应时间**

```text
upstream web-server {
    server 192.168.1.100 weight=1;
    server 192.168.1.101 weight=2;
    fair;  
}
```

## 11.**总结**

高性能是一个永恒的话题，其涉及的技术和知识面其实远不止上面列出的这些。

从物理硬件CPU、内存、硬盘、网卡到软件层面的通信、缓存、算法、架构每一个环节的优化都是通往高性能的道路。

原文地址：https://zhuanlan.zhihu.com/p/409011314

作者：linux

# 【NO.424】百行代码实现基于C++11的线程池threadpool , 简洁且可带任意多参数

C++11 加入了线程库，从此告别了标准库不支持并发的历史。然而 c++ 对于多线程的支持还是比较低级，稍微高级一点的用法都需要自己去实现，譬如线程池、信号量等。

线程池(thread pool)这个东西，在面试上多次被问到，一般的回答都是：“**管理一个任务队列，一个线程队列，然后每次取一个任务分配给一个线程去做，循环往复。**” 貌似没有问题吧。但是写起程序来的时候就出问题了。

废话不多说，先上实现，然后再啰嗦。(dont talk, show me ur code !)

## 1.代码实现

```text
#pragma once
#ifndef THREAD_POOL_H
#define THREAD_POOL_H

#include <vector>
#include <queue>
#include <atomic>
#include <future>
//#include <condition_variable>
//#include <thread>
//#include <functional>
#include <stdexcept>

namespace std
{
//线程池最大容量,应尽量设小一点
#define THREADPOOL_MAX_NUM 16
//#define THREADPOOL_AUTO_GROW

//线程池,可以提交变参函数或拉姆达表达式的匿名函数执行,可以获取执行返回值
//不直接支持类成员函数, 支持类静态成员函数或全局函数,Opteron()函数等
class threadpool
{
using Task = function<void()>; //定义类型
vector<thread> _pool; //线程池
queue<Task> _tasks; //任务队列
mutex _lock; //同步
condition_variable _task_cv; //条件阻塞
atomic<bool> _run{ true }; //线程池是否执行
atomic<int> _idlThrNum{ 0 }; //空闲线程数量

public:
inline threadpool(unsigned short size = 4) { addThread(size); }
inline ~threadpool()
{
_run=false;
_task_cv.notify_all(); // 唤醒所有线程执行
for (thread& thread : _pool) {
//thread.detach(); // 让线程“自生自灭”
if(thread.joinable())
thread.join(); // 等待任务结束， 前提：线程一定会执行完
}
}

public:
// 提交一个任务
// 调用.get()获取返回值会等待任务执行完,获取返回值
// 有两种方法可以实现调用类成员，
// 一种是使用 bind：.commit(std::bind(&Dog::sayHello, &dog));
// 一种是用 mem_fn：.commit(std::mem_fn(&Dog::sayHello), this)
template<class F, class... Args>
auto commit(F&& f, Args&&... args) ->future<decltype(f(args...))>
{
if (!_run) // stoped ??
throw runtime_error("commit on ThreadPool is stopped.");

using RetType = decltype(f(args...)); // typename std::result_of<F(Args...)>::type, 函数 f 的返回值类型
auto task = make_shared<packaged_task<RetType()>>(
bind(forward<F>(f), forward<Args>(args)...)
); // 把函数入口及参数,打包(绑定)
future<RetType> future = task->get_future();
{ // 添加任务到队列
lock_guard<mutex> lock{ _lock };//对当前块的语句加锁 lock_guard 是 mutex 的 stack 封装类，构造的时候 lock()，析构的时候 unlock()
_tasks.emplace([task](){ // push(Task{...}) 放到队列后面
(*task)();
});
}
#ifdef THREADPOOL_AUTO_GROW
if (_idlThrNum < 1 && _pool.size() < THREADPOOL_MAX_NUM)
addThread(1);
#endif // !THREADPOOL_AUTO_GROW
_task_cv.notify_one(); // 唤醒一个线程执行

return future;
}

//空闲线程数量
int idlCount() { return _idlThrNum; }
//线程数量
int thrCount() { return _pool.size(); }
#ifndef THREADPOOL_AUTO_GROW
private:
#endif // !THREADPOOL_AUTO_GROW
//添加指定数量的线程
void addThread(unsigned short size)
{
for (; _pool.size() < THREADPOOL_MAX_NUM && size > 0; --size)
{ //增加线程数量,但不超过 预定义数量 THREADPOOL_MAX_NUM
_pool.emplace_back( [this]{ //工作线程函数
while (_run)
{
Task task; // 获取一个待执行的 task
{
// unique_lock 相比 lock_guard 的好处是：可以随时 unlock() 和 lock()
unique_lock<mutex> lock{ _lock };
_task_cv.wait(lock, [this]{
return !_run || !_tasks.empty();
}); // wait 直到有 task
if (!_run && _tasks.empty())
return;
task = move(_tasks.front()); // 按先进先出从队列取一个 task
_tasks.pop();
}
_idlThrNum--;
task();//执行任务
_idlThrNum++;
}
});
_idlThrNum++;
}
}
};

}

#endif 
```

代码不多吧,上百行代码就完成了 线程池, 并且, 看看 commit, 哈, 不是固定参数的, 无参数数量限制! 这得益于可变参数模板.

## 2.怎么使用?

```text
#include "threadpool.h"
#include <iostream>

void fun1(int slp)
{
printf(" hello, fun1 ! %d\n" ,std::this_thread::get_id());
if (slp>0) {
printf(" ======= fun1 sleep %d ========= %d\n",slp, std::this_thread::get_id());
std::this_thread::sleep_for(std::chrono::milliseconds(slp));
}
}

struct gfun {
int operator()(int n) {
printf("%d hello, gfun ! %d\n" ,n, std::this_thread::get_id() );
return 42;
}
};

class A {
public:
static int Afun(int n = 0) { //函数必须是 static 的才能直接使用线程池
std::cout << n << " hello, Afun ! " << std::this_thread::get_id() << std::endl;
return n;
}

static std::string Bfun(int n, std::string str, char c) {
std::cout << n << " hello, Bfun ! "<< str.c_str() <<" " << (int)c <<" " << std::this_thread::get_id() << std::endl;
return str;
}
};

int main()
try {
std::threadpool executor{ 50 };
A a;
std::future<void> ff = executor.commit(fun1,0);
std::future<int> fg = executor.commit(gfun{},0);
std::future<int> gg = executor.commit(a.Afun, 9999); //IDE提示错误,但可以编译运行
std::future<std::string> gh = executor.commit(A::Bfun, 9998,"mult args", 123);
std::future<std::string> fh = executor.commit([]()->std::string { std::cout << "hello, fh ! " << std::this_thread::get_id() << std::endl; return "hello,fh ret !"; });

std::cout << " ======= sleep ========= " << std::this_thread::get_id() << std::endl;
std::this_thread::sleep_for(std::chrono::microseconds(900));

for (int i = 0; i < 50; i++) {
executor.commit(fun1,i*100 );
}
std::cout << " ======= commit all ========= " << std::this_thread::get_id()<< " idlsize="<<executor.idlCount() << std::endl;

std::cout << " ======= sleep ========= " << std::this_thread::get_id() << std::endl;
std::this_thread::sleep_for(std::chrono::seconds(3));

ff.get(); //调用.get()获取返回值会等待线程执行完,获取返回值
std::cout << fg.get() << " " << fh.get().c_str()<< " " << std::this_thread::get_id() << std::endl;

std::cout << " ======= sleep ========= " << std::this_thread::get_id() << std::endl;
std::this_thread::sleep_for(std::chrono::seconds(3));

std::cout << " ======= fun1,55 ========= " << std::this_thread::get_id() << std::endl;
executor.commit(fun1,55).get(); //调用.get()获取返回值会等待线程执行完

std::cout << "end... " << std::this_thread::get_id() << std::endl;


std::threadpool pool(4);
std::vector< std::future<int> > results;

for (int i = 0; i < 8; ++i) {
results.emplace_back(
pool.commit([i] {
std::cout << "hello " << i << std::endl;
std::this_thread::sleep_for(std::chrono::seconds(1));
std::cout << "world " << i << std::endl;
return i*i;
})
);
}
std::cout << " ======= commit all2 ========= " << std::this_thread::get_id() << std::endl;

for (auto && result : results)
std::cout << result.get() << ' ';
std::cout << std::endl;
return 0;
}
catch (std::exception& e) {
std::cout << "some unhappy happened... " << std::this_thread::get_id() << e.what() << std::endl;
}
```

为了避嫌，先进行一下版权说明：代码是 me “写”的，但是思路来自 Internet， 特别是这个线程池实现。

## 3.实现原理

接着前面的废话说。“管理一个任务队列，一个线程队列，然后每次取一个任务分配给一个线程去做，循环往复。” 这个思路有神马问题？

线程池一般要复用线程，所以如果是取一个 task 分配给某一个 thread，执行完之后再重新分配，在语言层面基本都是不支持的：**一般语言的 thread 都是执行一个固定的 task 函数，执行完毕线程也就结束了(至少 c++ 是这样)**。

so 要如何实现 task 和 thread 的分配呢？让每一个 thread 都去执行调度函数：循环获取一个 task，然后执行之。idea 是不是很赞！保证了 thread 函数的唯一性，而且复用线程执行 task 。

即使理解了 idea，代码还是需要详细解释一下的。

- 一个线程 pool，一个任务队列 queue ，应该没有意见；
- **任务队列是典型的生产者-消费者模型**，本模型至少需要两个工具：一个 mutex + 一个条件变量，或是一个 mutex + 一个信号量。mutex 实际上就是锁，保证任务的添加和移除(获取)的互斥性，一个条件变量是保证获取 task 的同步性：一个 empty 的队列，线程应该等待(阻塞)；
- atomic<bool> 本身是原子类型，从名字上就懂：**它们的操作 load()/store() 是原子操作，所以不需要再加 mutex。**

## 4.c++语言细节

即使懂原理也不代表能写出程序，上面用了众多c++11的“奇技淫巧”，下面简单描述之。

- using Task = function<void()> 是类型别名，简化了 typedef 的用法。function<void()> 可以认为是一个函数类型，接受任意原型是 void() 的函数，或是函数对象，或是匿名函数。void() 意思是不带参数，没有返回值。
- pool.emplace_back([this]{...}) 和 pool.push_back([this]{...}) 功能一样，只不过前者性能会更好；
- pool.emplace_back([this]{...}) 是构造了一个线程对象，执行函数是拉姆达匿名函数 ；
- 所有对象的初始化方式均采用了 {}，而不再使用 () 方式，因为风格不够一致且容易出错；
- 匿名函数：[this]{...} 不多说。[] 是捕捉器，this 是引用域外的变量 this指针， 内部使用死循环, 由cv_task.wait(lock,[this]{...}) 来阻塞线程；
- delctype(expr) 用来推断 expr 的类型，和 auto 是类似的，相当于类型占位符，占据一个类型的位置；auto f(A a, B b) -> decltype(a+b) 是一种用法，不能写作 decltype(a+b) f(A a, B b)，为啥？！c++ 就是这么规定的！
- commit 方法是不是略奇葩！可以带任意多的参数，第一个参数是 f，后面依次是函数 f 的参数！(注意:参数要传struct/class的话,建议用pointer,小心变量的作用域) 可变参数模板是 c++11 的一大亮点，够亮！至于为什么是 Arg... 和 arg... ，因为规定就是这么用的！
- commit 直接使用只能调用stdcall函数，但有两种方法可以实现调用类成员，一种是使用 bind：.commit(std::bind(&Dog::sayHello, &dog))；一种是用 mem_fn：.commit(std::mem_fn(&Dog::sayHello), &dog)；
- **make_shared 用来构造 shared_ptr 智能指针。**用法大体是 shared_ptr<int> p = make_shared<int>(4) 然后 *p == 4 。智能指针的好处就是， 自动 delete ！
- bind 函数，接受函数 f 和部分参数，返回currying后的匿名函数，譬如 bind(add, 4) 可以实现类似 add4 的函数！
- forward() 函数，类似于 move() 函数，后者是将参数右值化，前者是... 肿么说呢？大概意思就是：不改变最初传入的类型的引用类型(左值还是左值，右值还是右值)；
- packaged_task 就是任务函数的封装类，通过 get_future 获取 future ， 然后通过 future 可以获取函数的返回值(future.get())；packaged_task 本身可以像函数一样调用 () ；
- queue 是队列类， front() 获取头部元素， pop() 移除头部元素；back() 获取尾部元素，push() 尾部添加元素；
- **lock_guard 是 mutex 的 stack 封装类**，构造的时候 lock()，析构的时候 unlock()，是 c++ RAII 的 idea；
- condition_variable cv; 条件变量， 需要配合 unique_lock 使用；unique_lock 相比 lock_guard 的好处是：可以随时 unlock() 和 lock()。cv.wait() 之前需要持有 mutex，wait 本身会 unlock() mutex，如果条件满足则会重新持有 mutex。
- 最后**线程池析构的时候,join() 可以等待任务都执行完在结束**,很安全!

原文地址：https://zhuanlan.zhihu.com/p/472021998

作者：linux

# 【NO.425】全网最透彻的五种linux IO模型分析「值得收藏」

## 1.基础

在引入IO模型前，先对io等待时某一段数据的"经历"做一番解释。如图：

![img](https://pic1.zhimg.com/80/v2-49c248370dec7aa98f8cbf5bac893a74_720w.webp)

当某个程序或已存在的进程/线程(后文将不加区分的只认为是进程)需要某段数据时，它只能在用户空间中属于它自己的内存中访问、修改，这段内存暂且称之为app buffer。假设需要的数据在磁盘上，那么进程首先得发起相关系统调用，通知内核去加载磁盘上的文件。但正常情况下，数据只能加载到内核的缓冲区，暂且称之为kernel buffer。数据加载到kernel buffer之后，还需将数据复制到app buffer。到了这里，进程就可以对数据进行访问、修改了。

现在有几个需要说明的问题。

**(1).为什么不能直接将数据加载到app buffer呢**？

实际上是可以的，有些程序或者硬件为了提高效率和性能，可以实现内核旁路的功能，避过内核的参与，直接在存储设备和app buffer之间进行数据传输，例如RDMA技术就需要实现这样的内核旁路功能。

但是，最普通也是绝大多数的情况下，为了安全和稳定性，数据必须先拷入内核空间的kernel buffer，再复制到app buffer，以防止进程串进内核空间进行破坏。

**(2).上面提到的数据几次拷贝过程，拷贝方式是一样的吗**？

不一样。现在的存储设备(包括网卡)基本上都支持DMA操作。什么是DMA(direct memory access，直接内存访问)？简单地说，就是内存和设备之间的数据交互可以直接传输，不再需要计算机的CPU参与，而是通过硬件上的芯片(可以简单地认为是一个小cpu)进行控制。

假设，存储设备不支持DMA，那么数据在内存和存储设备之间的传输，必须由内核线程占用CPU去完成数据拷贝(比如网卡不支持DMA时，内核负责将数据从网卡拷贝到kernel buffer)。而DMA就释放了计算机的CPU，让它可以去处理其他任务，DMA也释放了从用户进程切换到内核的过程，从而避免了用户进程在这个拷贝阶段被阻塞。

再说kernel buffer和app buffer之间的复制方式，这是两段内存空间的数据传输，只能由内核占用CPU来完成拷贝。

所以，在加载硬盘数据到kernel buffer的过程是DMA拷贝方式，而从kernel buffer到app buffer的过程是CPU参与的拷贝方式。

**(3).如果数据要通过TCP连接传输出去要怎么办**？

例如，web服务对客户端的响应数据，需要通过TCP连接传输给客户端。

TCP/IP协议栈维护着两个缓冲区：send buffer和recv buffer，它们合称为socket buffer。需要通过TCP连接传输出去的数据，需要先复制到send buffer，再复制给网卡通过网络传输出去。如果通过TCP连接接收到数据，数据首先通过网卡进入recv buffer，再被复制到用户空间的app buffer。

同样，在数据复制到send buffer或从recv buffer复制到app buffer时，是内核占用CPU来完成的数据拷贝。从send buffer复制到网卡或从网卡复制到recv buffer时，是DMA方式的拷贝，这个阶段不需要切换到内核，也不需要计算机自身的CPU。

如下图所示，是通过TCP连接传输数据时的过程。

![img](https://pic4.zhimg.com/80/v2-bcebb7a60ec1533d6afb0d7edcc67d37_720w.webp)

**(4).网络数据一定要从kernel buffer复制到app buffer再复制到send buffer吗**？

不是。如果进程不需要修改数据，就直接发送给TCP连接的另一端，可以不用从kernel buffer复制到app buffer，而是直接复制到send buffer。这就是**零复制**技术。

例如，如果httpd进程不需要访问和修改任何数据，那么将数据原原本本地复制到app buffer再原原本本地复制到send buffer然后传输出去的过程中，从kernel buffer复制到app buffer的过程是可以省略的。使用零复制技术，就可以减少一次拷贝过程，提升效率。

当然，实现零复制技术的方法有多种。

以下是以httpd进程处理文件类请求时比较完整的数据操作流程。

![img](https://pic2.zhimg.com/80/v2-042ee67f06794c65ddd16e1d6d4e9469_720w.webp)

大致解释下：客户端发起对某个文件的请求，通过TCP连接，请求数据进入TCP的recv buffer，再通过recv()函数将数据读入到app buffer，此时httpd工作进程对数据进行一番解析，知道请求的是某个文件，于是发起read系统调用，于是内核加载该文件，数据从磁盘复制到kernel buffer再复制到app buffer，此时httpd就要开始构建响应数据了，可能会对数据进行一番修改，例如在响应首部中加一个字段，最后将修改或未修改的数据复制(例如send()函数)到send buffer中，再通过TCP连接传输给客户端。

## 2. I/O模型

所谓的IO模型，描述的是出现I/O等待时进程的状态以及处理数据的方式。围绕着进程的状态、数据准备到kernel buffer再到app buffer的两个阶段展开。其中数据复制到kernel buffer的过程称为**数据准备**阶段，数据从kernel buffer复制到app buffer的过程称为**数据复制**阶段。请记住这两个概念，后面描述I/O模型时会一直用这两个概念。

本文某些地方以httpd进程的TCP连接方式处理本地文件为例，请无视httpd是否真的实现了如此、那般的功能，也请无视TCP连接处理数据的细节，这里仅仅只是作为方便解释的示例而已。

再次说明，从硬件设备到内存的数据传输过程是不需要CPU参与的，而内存间传输数据是需要内核线程占用CPU来参与的。

### 2.1 Blocking I/O模型

如图：

![img](https://pic4.zhimg.com/80/v2-53b82873827f8caa5ee2aba0fbce47d3_720w.webp)

假设客户端发起index.html的文件请求，httpd需要将index.html的数据从磁盘中加载到自己的httpd app buffer中，然后复制到send buffer中发送出去。

但是在httpd想要加载index.html时，它首先检查自己的app buffer中是否有index.html对应的数据，没有就发起系统调用让内核去加载数据，例如read()，内核会先检查自己的kernel buffer中是否有index.html对应的数据，如果没有，则从磁盘中加载，然后将数据准备到kernel buffer，再复制到app buffer中，最后被httpd进程处理。

如果使用Blocking I/O模型：

(1).当设置为blocking i/o模型，httpd从1到3都是被阻塞的。
(2).只有当数据复制到app buffer完成后，或者发生了错误，httpd才被唤醒处理它app buffer中的数据。
(3).cpu会经过两次上下文切换：用户空间到内核空间再到用户空间，第一次是发起系统调用的切换，第二次是内核将数据拷贝到app buffer完成后的切换。
(4).由于2阶段的拷贝是不需要CPU参与的，所以在2阶段准备数据的过程中，cpu可以去处理其它进程的任务。
(5).3阶段的数据复制需要CPU参与，将httpd阻塞。
(6).这是最省事、最简单的IO模式。

如下图：

![img](https://pic2.zhimg.com/80/v2-908c4d9f7d1a5cb420fd930b5dd4f789_720w.webp)

### 2.2 Non-Blocking I/O模型

(1).当设置为non-blocking时，httpd第一次发起系统调用(如read())后，立即返回一个错误值EWOULDBLOCK，而不是让httpd进入睡眠状态。UNP中也正是这么描述的。

```text
When we set a socket to be nonblocking, we are telling the kernel "when an I/O operation that I request cannot be completed without putting the process to sleep, do not put the process to sleep, but return an error instead.
```

(2).虽然read()立即返回了，但httpd还要不断地去发送read()检查内核：数据是否已经成功拷贝到kernel buffer了？这称为轮询(polling)。每次轮询时，只要内核没有把数据准备好，read()就返回错误信息EWOULDBLOCK。
(3).直到kernel buffer中数据准备完成，再去轮询时不再返回EWOULDBLOCK，而是将httpd阻塞，以等待数据复制到app buffer。
(4).httpd在1到2阶段不被阻塞，但是会不断去发送read()轮询。在3被阻塞，将cpu交给内核把数据copy到app buffer。

如下图：

![img](https://pic1.zhimg.com/80/v2-1a7edad4ad21f1cd2af28dd0138dc6f8_720w.webp)

### 2.3 I/O Multiplexing模型

称为多路IO模型或IO复用，意思是可以检查多个IO等待的状态。有三种IO复用模型：select、poll和epoll。其实它们都是一种函数，用于监控指定文件描述符的数据是否就绪。

就绪指的是对某个系统调用不再阻塞了，可以直接执行IO。例如对于read()来说，数据准备好了就是就绪状态，此时read()可以直接去读取数据且能立即读取到数据，对write()来说，就是有空间可以写入数据了(比如缓冲区未满)，此时write()可以直接写入。

就绪种类包括是否可读、是否可写以及是否异常，其中可读条件中就包括了数据是否准备好，也即数据是否已经在kernel buffer中。当就绪之后，将通知进程，进程再发送对数据操作的系统调用，如read()。

所以，这三个函数仅仅只是处理了数据是否准备好以及如何通知进程的问题。可以将这几个函数结合阻塞和非阻塞IO模式使用，但通常IO复用都会结合非阻塞IO模式。

select()和poll()差不多，它们的监控和通知手段是类似的，只不过poll()要更聪明一点，某些时候效率也更高些，此处仅以select()监控单个文件请求为例简单介绍IO复用，至于更具体的、监控多个文件以及epoll的方式，在本文的最后专门解释。

(1).当想要加载某个文件时，假如httpd要发起read()系统调用，如果是阻塞或者非阻塞情形，那么read()会根据数据是否准备好而决定是否返回。是否可以主动去监控这个数据是否准备到了kernel buffer中呢，亦或者是否可以监控send buffer中是否有新数据进入呢？这就是select()/poll()/epoll的作用。

(2).当使用select()时，进程被select()所『阻塞』，之所以阻塞要加上引号，是因为select()有时间间隔选项可用控制阻塞时长，如果该选项设置为0，则select不阻塞而是立即返回，还可以设置为永久阻塞。

(3).当select()的监控对象就绪时，httpd进程通过轮询判断知道可以执行read()了，于是httpd再发起read()系统调用，此时数据会从kernel buffer复制到app buffer中并read()成功。

(4).httpd发起read()系统调用后切换到内核，由内核占用CPU来复制数据到app buffer，所以httpd进程被阻塞。

上面的描述可能还太过抽象，这里用shell伪代码来简单描述select()的工作方式(细节并非准确，但易于理解)。假设有一个select命令，作用和select()函数相同。伪代码如下：

```text
# select监控指定的文件描述符，并返回已就绪的描述符数量给x
 # 进程将阻塞在select命令上，直到select返回
 x=$(select fd1 fd2 fd3)
 
 # 如果x大于0，说明有文件描述符数据就绪，于是遍历所有fd，
 # 并分别使用read去读取这些fd，但并不知道具体是哪个fd已
 # 就绪，所以read时最好是非阻塞的读取，否则read每一个未
 # 就绪的fd时都会阻塞
 if [ x -gt 0 ];then
   for fd in fd1 fd2 fd3;do
     read -t 0 -u $fd  # read操作最好是非阻塞的
   done
 fi
```

所以，在使用IO复用模型时，真正的IO操作(比如read)最好是非阻塞方式的，但并非必须。比如只监控一个文件描述符时，select能返回意味着这个文件描述符一定是就绪的(select还有其它返回值，但这里不考虑其它返回值。

IO多路复用时，模型如图：

![img](https://pic4.zhimg.com/80/v2-79a54eaeeb688935d793569cbf4d612b_720w.webp)

**select/poll的性能问题**

select()/poll()的性能会随着监控的文件描述符数量增多而快速下降。其原因是多方面的。

其中一个原因是它们所监控的文件描述符会以某种数据结构全部传送给内核，让内核负责监控这些文件描述符，当内核发现某个文件描述符已经就绪时，会修改数据结构，然后将修改后的数据结构传回给进程。所以涉及了两次数据传输的过程。

对于select()来说，每次传递的数据结构的大小是固定的，都是1024个描述符的大小。对于poll()来说，只会传递被监控的文件描述符，所以文件描述符少的时候，poll()的性能是可以的，此外poll()可以超出1024个文件描述符的监控数量限制，但随着描述符数量的增多，来回传递的数据量也是非常大的。

基于这方面的性能考虑，更建议使用信号驱动IO或epoll模型，它们都是直接告诉内核要监控哪些文件描述符，内核会以合适的数据结构安排这些待监控的文件描述符(如epoll，内核采用红黑树的方式)，换句话说，它们不会传递一大片的文件描述符数据结构，效率更高。

使用IO复用还有很多细节，本文在此仅只是对其作最基本的功能性描述，在本文末还会多做一些扩展，至于更多的内容需自行了解。

### 2.4 Signal-driven I/O模型

即信号驱动IO模型。当文件描述符上设置了O_ASYNC标记时，就表示该文件描述符是信号驱动的IO。

> 注：可能你觉得O_ASYNC应该表示的是异步IO，但并非如此。
> 在历史上，信号驱动IO也称为异步IO，比如这个标记就暗含了这一点历史。
> 如今常说的术语【异步】，是由POSIX AIO规范所提供的功能，这个异步表示某进程发起IO操作时，立即返回，当IO完成或有错误时，该进程会收到通知，于是该进程可以去处理这个通知，比如执行回调函数。

当某个文件描述符使用信号驱动IO模型时，要求进程配置信号SIGIO的信号处理程序，然后进程就可以做其他任何事情。当该文件描述符就绪时，内核会向该进程发送SIGIO信号。该进程收到SIGIO信号后，就会去执行已经配置号的信号处理程序。

通常来说，SIGIO的信号处理程序中会编写read()类的读取代码，这表示在收到SIGIO时在信号处理程序中执行read操作，另一种常见的作法是在SIGIO的信号处理程序中设置某变量标记，然后在外部判断该标记是否为true，如果标记为true，则执行read类的操作。

使用Shell伪代码如下：

```text
# 第一种模式：在信号处理程序中执行IO操作
 trap 'read...' SIGIO  # 设置SIGIO的信号处理程序
 # 然后就可以执行其它任意任务
 # 当在执行其它任务过程中，内核发送了SIGIO，进程会
 # 立即去执行SIGIO信号处理程序
 ...other codes...
 
 # 第二种模式：在信号处理程序中设置变量标记，在外部执行IO操作
 trap 'a=1' SIGIO
 ... other codes...
 # 
 while [ $a -eq 1 ];do
   read...
 done
```

很明显，使用信号驱动IO模型时，进程对该描述符的读取是被动的，进程不会主动在描述符就绪前执行读取操作。

其实信号驱动IO模型就像是小姐姐在闲逛，小姐姐本没有想过要买什么东西，但如果发现有合适的，也会去买下来，在逛了一段时间后，一件超短裙闪现到小姐姐的视线，小姐姐很喜欢这个款式，于是立即决定买下来。

这里可以做出大胆的推测，**并非所有文件描述符类型都能使用信号驱动的IO模型**。如果某文件描述符想要开启信号驱动IO，要求有某个另一端会主动向该描述符发送数据，比如管道、套接字、终端等都符合这种要求。显然，普通文件系统上的文件IO是无法使用信号驱动IO的。

回到信号驱动IO模型，由于进程没有主动执行IO操作，所以不会阻塞，当数据就绪后，进程收到内核发送的SIGIO信号，进程会去执行SIGIO的信号处理程序，当进程执行read()时，由于数据已经就绪，所以可以直接将数据从kernel buffer复制到app buffer，read()过程中，进程将被阻塞。

> 注意：sigio信号只是通知了数据就绪，但并不知道有多少数据已经就绪。

如图：

![img](https://pic4.zhimg.com/80/v2-c7750b186b84def68fd99c0fb2e0f073_720w.webp)

### 2.5 Asynchronous I/O模型

即异步IO模型。

异步IO来自于POSIX AIO规范，它专门提供了能异步IO的读写类函数，如aio_read()，aio_write()等。

使用异步IO函数时，要求指定IO完成时或IO出现错误时的通知方式，通知方式主要分两类：

- 发送指定的信号来通知
- 在另一个线程中执行指定的回调函数

为了帮助理解，这里假设aio_read()的语法如下(真实的语法要复杂的多)：

```text
aio_read(x,y,z,notify_mode,notify_value)
```

其中nofity_mode允许的值有两种：

- 当notify_mode参数的值为SIGEV_SIGNAL时，notify_value参数的值为一个信号
- 当notify_mode参数的值为SIGEV_THREAD，notify_value参数的值为一个函数，这个函数称为回调函数

当使用异步IO函数时，进程不会因为要执行IO操作而阻塞，而是立即返回。

例如，当进程执行异步IO函数aio_read()时，它会请求内核执行具体的IO操作，当数据已经就绪**且从kernel buffer拷贝到app buffer**后，内核认为IO操作已经完成，于是内核会根据调用异步IO函数时指定的通知方式来执行对应的操作：

- 如果通知模式是信号通知方式(SIGEV_SIGNAL)，则在IO完成时，内核会向进程发送notify_value指定的信号
- 如果通知模式是信号回调方式(SIGEV_THREAD)，则在IO完成时，内核会在一个独立的线程中执行notify_value指定的回调函数

回顾一下信号驱动IO，信号驱动IO要求有另一端主动向文件描述符写入数据，所以它支持像socket、pipe、terminal这类文件描述符，但不支持普通文件IO的文件描述符。

而异步IO则没有这个限制，异步IO操作借助的是那些具有神力的异步函数，只要文件描述符能读写，就能使用异步IO函数来实现异步IO。

所以，异步IO在整个过程中都不会被阻塞。如图：

![img](https://pic2.zhimg.com/80/v2-87cdf2de5aacc47f2d23b574c15739ed_720w.webp)

看上去异步很好，但是注意，在复制kernel buffer数据到app buffer中时是需要CPU参与的，这意味着不受阻的进程会和异步调用函数争用CPU。以httpd为例，如果并发量比较大，httpd接入的连接数可能就越多，CPU争用情况就越严重，异步函数返回成功信号的速度就越慢。如果不能很好地处理这个问题，异步IO模型也不一定就好。

### 2.6 同步IO和异步IO、阻塞和非阻塞的区分

阻塞和非阻塞，体现在当前进程是否可执行，是否能获取到CPU。

当阻塞和非阻塞的概念体现在IO模型上：

- 阻塞IO：从开始发起IO操作开始就阻塞，直到IO完成才返回，所以进程会立即进入睡眠态
- 非阻塞IO：发起IO操作时，如果当前数据已就绪，则切换到内核态由内核完成数据拷贝(从kernel buffer拷贝到app buffer)，此时进程被阻塞，因为它的CPU已经被内核抢走了。如果发起IO操作时数据未就绪，则立即返回而不阻塞，即进程继续享有CPU，可以继续任务。但进程不知道数据何时就绪，所以通常会采用轮循代码(比如while循环)不断判断数据是否就绪，当数据最终就绪后，切换到内核态，进程仍然被阻塞

同步和异步，考虑的是两边数据是否同步(比如kernel buffer和app buffer之间数据是否同步)。同步和异步的区别体现在两边数据尚未完成同步时的行为：

- 同步：在保持两边数据同步的过程中，进程被阻塞，由内核抢占其CPU去完成数据同步，直到两边数据同步，进程才被唤醒
- 异步：在保持两边数据同步的过程中，由内核默默地在后台完成数据同步(如果不理解，可认为是单独开了一个内核线程负责数据同步)，内核不会抢占进程的CPU，所以进程自身不被阻塞，当内核完成两端数据同步时，通知进程已同步完成

这里阻塞和非阻塞、同步和异步都是广义的概念，上面所做的解释适用于所有使用这些术语的情况，而不仅仅是本文所专注的IO模型。

回到阻塞、非阻塞、同步、异步的IO模型，再对它们啰嗦啰嗦。

阻塞、非阻塞、IO复用、信号驱动都是同步IO模型。需注意，虽然不同IO模型在加载数据到kernel buffer的数据准备过程中可能阻塞、可能不阻塞，**但kernel buffer才是read()函数读取数据时的对象，同步的意思是让kernel buffer和app buffer数据同步**。在保持kernel buffer和app buffer同步的过程中，CPU将从执行read()操作的进程切换到内核态，内核获取CPU拷贝数据到app buffer，所以执行read()操作的进程在这个同步的阶段中是被阻塞的。

只有异步IO模型才是异步的，因为它调用的是具有【神力】的异步IO函数(如aio_read())，调用这些函数时会请求内核，当数据已经拷贝到app buffer后，通知进程并执行指定的操作。

需要注意的是，无论是哪种IO模型，在将数据从kernel buffer拷贝到app buffer的这个阶段，都是需要CPU参与的。只不过，同步IO模型和异步IO模型中，CPU参与的方式不一样:

- 同步IO模型中，调用read()的进程会切换到内核，由内核占用CPU来执行数据拷贝，所以原进程在此阶段一直被阻塞
- 异步IO模型中，由内核在后台默默的执行数据拷贝，所以原进程在此阶段不被阻塞

如图：

![img](https://pic4.zhimg.com/80/v2-ff109c9806c3d767a965f1e323db0627_720w.webp)

### 2.7 信号驱动IO和异步IO的区别

很多人都不理解信号驱动IO和异步IO之间的区别，一方面是因为它们都立即返回，另一方面是因为它们看似都是被动的或后台的。

但其实在前文已经分析清楚了它们的区别，这里仅做总结性分析。在此之前，还是借用前文使用过的类比。

信号驱动IO模型：小姐姐在逛街，小姐姐本没有想过要买什么东西，但如果发现有合适的，也会去买下来，在逛了一段时间后，一件超短裙闪现到小姐姐的视线，小姐姐很喜欢这个款式，于是立即决定买下来，买的时候小姐姐不能再干其它事情。

异步IO模型：小姐姐在逛街，她这次带上了男朋友，只要想买东西，都可以让男朋友去帮忙买，而小姐姐可以继续自己逛自己的，男朋友买好后通知小姐姐即可。

#### 2.7.1 异步IO

异步IO通过调用具有异步IO能力的函数来实现。在调用异步函数时，要求指定IO完成时的通知方式。

当IO完成后，内核(这里的内核是广义的，不再局限于操作系统内核，它也可以是浏览器内核，或语言的解释器，或语言的虚拟机)要么通知进程，要么执行回调函数。

这里所谓的IO完成，表示的是已经保持了两边数据的同步(比如kernel buffer和app buffer之间)。而异步之所以称为异步，就体现在完成两边数据同步的阶段中，它表示由内核在后台默默完成数据的同步任务。

对于异步IO来说，它不在乎什么类型的文件描述符，socket、pipe、fifo、terminal以及普通文件都可以执行异步IO。

#### 2.7.2 信号驱动IO

信号驱动IO是同步IO模型。

当某个文件描述符设置了O_ASYNC标记时(前文说过，称呼为O_ASYNC是历史原因)，表示该文件描述符开启信号驱动IO的功能。

使用信号驱动IO，要求进程注册SIGIO的信号处理程序，注册之后，进程就可以做其他任务。

当有另一端向该描述符写入数据时，就意味着该文件描述符已经就绪，内核会发送SIGIO信号给进程，于是进程会去执行已经注册的SIGIO信号处理程序。一般来说，信号处理程序中，要么是read()类的读取函数，要么是为后面是否读取做判断的变量标记。

但是，内核发送SIGIO信号只是通知进程数据已经就绪，但就绪了多少数据量，进程并不知道。

而且，进程因为收到通知而认为可以数据已就绪，于是执行read()，进程在执行read()的时候，CPU将从用户态切换到内核态，由内核获取CPU来执行数据同步操作，所以在这个阶段中，进程的read()是被阻塞的。

因为信号驱动要求有另一端主动写入数据，所以socket、pipe、fifo、terminal等文件描述符类型是可以信号驱动IO 的，但是不支持对普通文件使用信号驱动IO。

## 3.select()、poll()和epoll

前面说了，这三个函数是文件描述符状态监控的函数，它们可以监控一系列文件的一系列事件，当出现满足条件的事件后，就认为是就绪或者错误。事件大致分为3类：可读事件、可写事件和异常事件。它们通常都放在循环结构中进行循环监控。

select()和poll()函数处理方式的本质类似，只不过poll()稍微先进一点，而epoll处理方式就比这两个函数先进多了。当然，就算是先进分子，在某些情况下性能也不一定就比老家伙们强。

### 3.1 select() & poll()

首先，通过FD_SET宏函数创建待监控的描述符集合，并将此描述符集合作为select()函数的参数，可以在指定select()函数阻塞时间间隔，于是select()就创建了一个监控对象。

除了普通文件描述符，还可以监控套接字，因为套接字也是文件，所以select()也可以监控套接字文件描述符，例如recv buffer中是否收到了数据，也即监控套接字的可读性，send buffer中是否满了，也即监控套接字的可写性。select()默认最大可监控1024个文件描述符。而poll()则没有此限制。

select()的时间间隔参数分3种：
(1).设置为指定时间间隔内阻塞，除非之前有就绪事件发生。
(2).设置为永久阻塞，除非有就绪事件发生。
(3).设置为完全不阻塞，即立即返回。但因为select()通常在循环结构中，所以这是轮询监控的方式。

当创建了监控对象后，由内核监控这些描述符集合，于此同时调用select()的进程被阻塞(或轮询)。当监控到满足就绪条件时(监控事件发生)，select()将被唤醒(或暂停轮询)，于是select()返回**满足就绪条件的描述符数量**，之所以是数量而不仅仅是一个，是因为多个文件描述符可能在同一时间满足就绪条件。由于只是返回数量，并没有返回哪一个或哪几个文件描述符，所以通常在使用select()之后，还会在循环结构中的if语句中使用宏函数FD_ISSET进行遍历，直到找出所有的满足就绪条件的描述符。最后将描述符集合通过指定函数拷贝回用户空间，以便被进程处理。

监听描述符集合的大致过程如下图所示，其中select()只是其中的一个环节：

![img](https://pic3.zhimg.com/80/v2-254645205436c68e4f00a2bc29696a72_720w.webp)

大概描述下这个循环监控的过程：

(1).首先通过FD_ZERO宏函数初始化描述符集合。图中每个小方格表示一个文件描述符。
(2).通过FD_SET宏函数创建描述符集合，此时集合中的文件描述符都被打开，也就是稍后要被select()监控的对象。
(3).使用select()函数监控描述符集合。当某个文件描述符满足就绪条件时，select()函数返回集合中满足条件的数量。图中标黄色的小方块表示满足就绪条件的描述符。
(4).通过FD_ISSET宏函数遍历整个描述符集合，并将满足就绪条件的描述符发送给进程。同时，使用FD_CLR宏函数将满足就绪条件的描述符从集合中移除。
(5).进入下一个循环，继续使用FD_SET宏函数向描述符集合中添加新的待监控描述符。然后重复(3)、(4)两个步骤。

如果使用简单的伪代码来描述：

```text
FD_ZERO
for() {
    FD_SET()
    select()
    if(){
        FD_ISSET()
        FD_CLR()
    }
    writen()
}
```

以上所说只是一种需要循环监控的示例，具体如何做却是不一定的。不过从中也能看出这一系列的流程。

### 3.2 epoll

epoll比poll()、select()先进，考虑以下几点，自然能看出它的优势所在：

(1).epoll_create()创建的epoll实例可以随时通过epoll_ctl()来新增和删除感兴趣的文件描述符，不用再和select()每个循环后都要使用FD_SET更新描述符集合的数据结构。
(2).在epoll_create()创建epoll实例时，还创建了一个epoll就绪链表list。而epoll_ctl()每次向epoll实例添加描述符时，还会注册该描述符的回调函数。当epoll实例中的描述符满足就绪条件时将触发回调函数，被移入到就绪链表list中。
(3).当调用epoll_wait()进行监控时，它只需确定就绪链表中是否有数据即可，如果有，将复制到用户空间以被进程处理，如果没有，它将被阻塞。当然，如果监控的对象设置为非阻塞模式，它将不会被阻塞，而是不断地去检查。

也就是说，epoll的处理方式中，根本就无需遍历描述符集合。

原文地址：https://zhuanlan.zhihu.com/p/393635611

作者：linux

# 【NO.426】TCP连接中TIME_WAIT状态的作用及优化

![img](https://pic1.zhimg.com/80/v2-f5bbc0108ffc20afb4af87cb8cf3a960_720w.webp)

## 1. 为什么需要TIME_WAIT状态？为什么TIME_WAIT的时长是2*MSL？

原因1：防止连接关闭时四次挥手中的最后一次ACK丢失：

TCP需要保证每一包数据都可靠的到达对端，包括正常连接状态下的业务数据报文，以及用于连接管理的握手、挥手报文，这其中在四次挥手中的最后一次ACK报文比较特殊，TIME_WAIT状态就是为了应对最后一条ACK丢失的情况。

TCP保证可靠传输的前提是收发两端分别维护关于这条连接的状态信息（TCB控制块），当发生丢包时进行ARQ重传。如果连接释放了，就无法进行重传，也就无法保证发生丢包时的可靠传输。

对于最后一条ACK，如果没有TIME_WAIT状态，主动关闭一方（客户端）就会在收到对端（服务器）的FIN并回复ACK后 直接从FIN_WAIT_2 进入 CLOSED状态，并释放连接，销毁TCB实例。此时如果最后一条ACK丢失，那么服务器重传的FIN将无人处理，最后导致服务器长时间的处于 LAST_ACK状态而无法正常关闭（服务器只能等到达到FIN的最大重传次数后关闭）。

至于将TIME_WAIT的时长设置为 2MSL，是因为报文在链路中的最大生存时间为MSL（Maximum Segment Lifetime），超过这个时长后报文就会被丢弃。TIME_WAIT的时长则是：最后一次ACK传输到服务器的时间 + 服务器重传FIN 的时间，即为 2MSL。

原因2：防止新连接收到旧链接的TCP报文：

TCP使用四元组区分一个连接（源端口、目的端口、源IP、目的IP），如果新、旧连接的IP与端口号完全一致，则内核协议栈无法区分这两条连接。

2*MSL 的时间足以保证两个方向上的数据都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定是新连接上产生的。

MSL（Maximum Segment Lifetime） 报文最大生存时间在不同操作系统中的具体值：

```text
Windows			:	2 	min
Linux(Ubuntu)	:	60	s
Unix			:	30	s
```

## 2. TIME_WAIT对连接并发数的影响（TIME_WAIT过多的危害）：

在Linux系统中，MSL = 60 s， 2 * MSL = 120 s，所以一条待关闭的TCP连接会在 TIME_WAIT 状态等待 120秒（2分钟）。

当连接处于TIME_WAIT状态时仍会占用系统资源（fd、端口、内存），当系统的并发连接数很大时，过多的TIME_WAIT状态的连接会对系统的并发量造成影响。

（1）对服务器的影响：

由于服务器一般只需要监听一个固定的端口，所以服务器所能支持的最大并发出数的上限取决于系统套接字描述符fd的大小，以及服务器的内存大小。

fd：

Linux中一个进程 所能打开的fd的最大数量默认为 1024 个，可通过 "ulimit -n (+指定数量)" 进行修改。

Linux系统所能支持的fd最大值在 /proc/sys/fs/fd-max 文件中可以查看，系统当前的fd使用情况可以通过 /proc/sys/fs/fd-nr 查看。（本机上的fd-max的值为：1221842，即100W级。实际上fd的最大值同样也取决于系统内存的大小）

内存：

假设每一个TCP连接需要开辟 “4k的接收缓冲区 + 4k的发送缓冲区 = 8k”，1W的并发连接需要80M内存，10W并发需要800M，100W并发需要8G内存。

综上，服务器的并发数主要受限于系统内存的大小，当 TIME_WAIT 状态的连接过多时，会导致消耗的内存增加，这一点可以通过扩展服务器的内存来解决。

（2）对客户端的影响：

客户端的并发数主要受限于端口数量。

一种典型的场景是：高并发短连接（“短连接”表示“业务处理+传输数据”的时间远远小于TIME_WAIT超时的时间）。

在这种场景下，客户端可能会消耗大量的端口（例如取一个Web网页，1秒钟的HTTP短连接处理完业务数据，却需要 2分钟的TIME_WAIT等待时间，在这段时间内客户端上的这个端口是无法被其他连接使用的，如果新建连接则需要使用另外的端口号），Linux系统的最大端口为65535，除去系统使用的端口号，假设网络进程可使用的端口有 6W个，由于TIME_WAIT状态下在 2*MSL（120秒）内无法再被使用，这就限制了客户端的连接速率为 60000 / 120秒 = 500 次/秒， 这是一个非常低的并发率。

同时，大量的TIME_WAIT连接同样会消耗客户端的内存，所以客户端的最大并发数取决于 端口号与内存 二者中的最小值。

## 3. 优化TIME_WAIT的方法：

**方法1：修改内核参数 tcp_tw_reuse：**

```text
net.ipv4.tcp_tw_reuse = 1;
net.ipv4.tcp_timestamp = 1;
```

注意： tcp_tw_reuse 内核参数只在调用 connect() 函数时起作用，所以只能用于客户端（主动连接的一端）。

tcp_tw_reuse 的作用是：在调用connect()函数时，内核会随机找一个处于TIME_WAIT状态 超过1秒 的连接给新连接复用。（超时时间由 tcp_timestamp设置，默认为 1秒）

这种方式可以缩短 TIME_WAIT 的等待时间。

**方法2：修改内核参数 tcp_max_tw_buckets：**

net.ipv4.tcp_max_tw_buckets 参数的默认值为18000，当系统中处于 TIME_WAIT 状态的连接数量超过阈值，系统会将后面的TIME_WAIT连接重置。

由于这种方法会直接重置连接，因此需要谨慎使用。

**方法3：设置套接字选项 SO_LINGER：**

SO_LINGER选项用于设置 调用close() 关闭TCP连接时的行为，注意 SO_LINGER选项会使用RST复位报文段取代 FIN-ACK四次挥手的过程，设置了SO_LINGER选项的一方在调用close() 时会直接发送一个RST，接收端收到后复位连接，不会回复任何响应。

这样做的弊端是导致TCP缓冲区中的数据被丢弃。

正常情况下，调用close后的缺省行为是：如果有待发送的数据残留在发送缓冲区中，内核协议栈将继续将这些数据发送给接收端后才关闭连接，走正常的四次挥手流程；

设置SO_LINGER后，立即关闭连接，通过RST分组，发送缓冲区如果有未发送的数据，将会被丢弃，主动关闭的一方跳过TIME_WAIT状态，直接进入CLOSED（也跳过了FIN_WAIT_1 和 FIN_WAIT_2）。

SO_LINGER的另一种更温和的实现方式是设置一个超时时间（so_linger.l_linger），而不是直接关闭。

应用程序调用close后进入睡眠，内核协议栈负责发送缓冲中残留的待发送数据，如果在 l_linger 超时时间内发送完毕，则走正常的四次挥手流程；如果超时未发送完，则发送RST强制关闭连接，并丢弃发送缓冲区中其余的数据（接收端收到RST后也会丢弃接收缓冲区中的数据），并且发送端close()函数返回 EWOULDBLOCK。

综上，如果只是单纯为了规避 TIME_WAIT 状态，使用 SO_LINGER并不是一个好主意，因为它会在调用close关闭连接时 使用RST强制关闭连接，这可能会导致 发送缓冲区、接收缓冲区 中还未处理完的数据被丢弃。

```text
struct linger so_linger;
so_linger.l_onoff = 1;		//0表示关闭，忽略l_linger的值；非0表示打开
so_linger.l_linger = 0;		//设置等待时间，等于0则表示立即关闭

setsockopt(fd, SOL_SOCKET, SO_LINGER, &so_linger, sizeof(so_linger));
```

**方法4：设置套接字选项 SO_REUSEADDR：**

SO_REUSEADDR 选项用于通知内核：

如果端口忙，并且端口对应的TCP连接状态为TIME_WAIT，则可以重用端口；

如果端口忙，并且端口对应的TCP连接处于其他状态（非TIME_WAIT），则返回 “Address already in use” 的错误信息。

设置SO_REUSEADDR的风险是可能会导致新连接上收到旧连接的数据（复用了旧连接的端口，导致新旧连接的四元组完全一致，内核协议栈无法区分这两个连接）。

SO_REUSEADDR 选项并没像 tcp_tw_reuse 那样同时提供一个 tcp_timestamp 参数可以设置 TIME_WAIT的等待时长。

综上，对TIME_WAIT状态的优化思路是尽量缩小等待时长，而不是暴力的直接关闭（可能会引起新连接收到旧连接数据的风险），也不要直接发送RST复位连接（可能会引起发送、接收缓冲区中的数据丢失），所以使用修改内核参数 tcp_tw_reuse 参数是最保险的方式，通过根据实际网络情况和应用场景适当的调节 tcp_timestamp 的值，可以达到缩小 TIME_WAIT 等待时长，进而减少系统中同一时刻处于 TIME_WAIT 状态的连接数量的目的。

原文地址：https://zhuanlan.zhihu.com/p/374135294

作者：linux

# 【NO.427】深入浅出DPDK学习笔记——认识DPDK

什么是DPDK？ 对于用户来说， 它可能是一个性能出色的包数据处理加速软件库； 对于开发者来说， 它可能是一个实践包处理新想法的创新工场； 对于性能调优者来说， 它可能又是一个绝佳的成果分享平台。当下火热的网络功能虚拟化， 则将DPDK放在一个重要的基石位置。 虽然很难用短短几语就勾勒出DPDK的完整轮廓， 但随着认识的深入， 我们相信你一定能够认可它传播的那些最佳实践方法， 从而将这些理念带到更广泛的多核数据包处理的生产实践中去。DPDK最初的动机很简单， 就是证明IA多核处理器能够支撑高性能数据包处理。 随着早期目标的达成和更多通用处理器体系的加入，DPDK逐渐成为通用多核处理器高性能数据包处理的业界标杆。

## 1.主流包处理硬件平台

DPDK用软件的方式在通用多核处理器上演绎着数据包处理的新篇章， 而对于数据包处理， 多核处理器显然不是唯一的平台。 支撑包处理的主流硬件平台大致可分为三个方向：硬件加速器、网络处理器、多核处理器。根据处理内容、 复杂度、 成本、 量产规模等因素的不同， 这些平台在各自特定的领域都有一定的优势。 硬件加速器对于本身规模化的固化功能具有高性能低成本的特点， 网络处理器提供了包处理逻辑软件可编程的能力， 在获得灵活性的同时兼顾了高性能的硬件包处理， 多核处理器在更为复杂多变的高层包处理上拥有优势， 随着包处理的开源生态系

统逐渐丰富， 以及近年来性能的不断提升， 其为软件定义的包处理提供了快速迭代的平台。

随着现代处理器的创新与发展（如异构化） ， 开始集成新的加速处理与高速IO单元， 它们互相之间不断地融合。 在一些多核处理器中， 已能看到硬件加速单元的身影。 从软件包处理的角度， 可以卸载部分功能到那些硬件加速单元进一步提升性能瓶颈； 从硬件包处理的流水线来看， 多核上运行的软件完成了难以固化的上层多变逻辑的任务； 二者相得益彰。

## 2.硬件加速器

硬件加速器被广泛应用于包处理领域， ASIC和FPGA是其中最广为采用的器件。ASIC（Application-Specific Integrated Circuit） 是一种应特定用户要求和特定电子系统的需要而设计、 制造的集成电路。 ASIC的优点是面向特定用户的需求， 在批量生产时与通用集成电路相比体积更小、 功耗更低、 可靠性提高、 性能提高、 保密性增强、成本降低等。 但ASIC的缺点也很明显， 它的灵活性和扩展性不够、 开发费用高、 开发周期长。为了弥补本身的一些缺点， ASIC越来越多地按照加速引擎的思路来构建， 结合通用处理器的特点， 融合成片上系统（SoC） 提供异构处理能力， 使得ASIC带上了智能（Smart） 的标签。

FPGA（Field-Programmable Gate Array） 即现场可编程门阵列。 它作为ASIC领域中的一种半定制电路而出现， 与ASIC的区别是用户不需要介入芯片的布局布线和工艺问题， 而且可以随时改变其逻辑功能， 使用灵活。 FPGA以并行运算为主， 其开发相对于传统PC、 单片机的开发有很大不同， 以硬件描述语言（Verilog或VHDL） 来实现。 相比于PC或单片机（无论是冯·诺依曼结构还是哈佛结构） 的顺序操作有很大区别。全可编程FPGA概念的提出， 使FPGA朝着进一步软化的方向持续发展， 其并行化整数运算的能力将进一步在通用计算定制化领域得到挖掘， 近年来在数据中心中取得了很大进展， 比如应用于机器学习场合。我们预计FPGA在包处理的应用场景将会从通信领域（CT） 越来越多地走向数据中心和云计算领域。

## 3.网络处理器

网络处理器（Network Processer Unit， NPU） 是专门为处理数据包而设计的可编程通用处理器， 采用多内核并行处理结构， 其常被应用于通信领域的各种任务， 比如包处理、 协议分析、 路由查找、 声音/数据的汇聚、 防火墙、 QoS等。 其通用性表现在执行逻辑由运行时加载的软件决定， 用户使用专用指令集即微码（microcode） 进行开发。 其硬件体系结构大多采用高速的接口技术和总线规范， 具有较高的I/O能力，使得包处理能力得到很大提升。 除了这些特点外， NPU一般还包含多种不同性能的存储结构， 对数据进行分类存储以适应不同的应用目的。NPU中也越来越多地集成进了一些专用硬件协处理器， 可进一步提高片内系统性能。

图1-1是NP-5处理器架构框图， 以EZCHIP公司的NP-5处理器架构为例， TOP部分为可编程部分， 根据需要通过编写微码快速实现业务相关的包处理逻辑。 NPU拥有高性能和高可编程性等诸多优点， 但其成本和特定领域的特性限制了它的市场规模（一般应用于专用通信设备） 。 而不同厂商不同架构的NPU遵循的微码规范不尽相同， 开发人员的成长以及生态系统的构建都比较困难。 虽然一些NPU的微码也开始支持由高级语言（例如C） 编译生成， 但由于结构化语言本身原语并未面向包处理， 使得转换后的效率并不理想。

随着SDN对于可编程网络， 特别是可编程数据面的要求， 网络处理器也可能会迎来新的发展机遇， 但依然需要解决好不同架构的底层抽象以及上层业务的语义抽象。

![img](https://pic1.zhimg.com/80/v2-bcd88ccef8f47cd6b895e5a6d20187c8_720w.webp)

## 4.多核处理器

现代CPU性能的扩展主要通过多核的方式进行演进。 这样利用通用处理器同样可以在一定程度上并行地处理网络负载。 由于多核处理器在逻辑负载复杂的协议及应用层面上的处理优势， 以及越来越强劲的数据面的支持能力， 它在多种业务领域得到广泛的采用。 再加上多年来围绕CPU已经建立起的大量成熟软件生态， 多核处理器发展的活力和热度也是其他形态很难比拟的。 图1-2是Intel双路服务器平台框图， 描述了一个典型的双路服务器平台的多个模块， CPU、 芯片组C612、 内存和以太网控制器XL710构成了主要的数据处理通道。 基于PCIe总线的I/O接口提供了大量的系统接口， 为服务器平台引入了差异化的设计。当前的多核处理器也正在走向SoC化， 针对网络的SoC往往集成内存控制器、 网络控制器， 甚至是一些硬件加速处理引擎。这里列出了一些主流厂商的多核处理器的SoC平台：IA multi-core Xeon、Tilear-TILE-Gx、Cavium Network-OCTEON&OCTEON II、Freescale-QorIQ、NetLogic Microsystem-XLP。

![img](https://pic4.zhimg.com/80/v2-8f789e7134dbfd5c4fae9e98ddd74307_720w.webp)

图1-3的Cavium OCTEON处理器框图以Cavium OCTEON多核处理器为例， 它集成多个CPU核以及众多加速单元和网络接口， 组成了一个片上系统（SoC） 。 在这些SoC上， 对于可固化的处理（例如， 流分类， QoS） 交由加速单元完成， 而对于灵活的业务逻辑则由众多的通用处理器完成， 这种方式有效地融合了软硬件各自的优势。 随着软件（例如， DPDK） 在I/O性能提升上的不断创新， 将多核处理器的竞争力提升到一个前所未有的高度， 网络负载与虚拟化的融合又催生了NFV的潮流。

![img](https://pic3.zhimg.com/80/v2-96abbb6abda342cb604754e64c7089c6_720w.webp)

## 5.初识DPDK

本书介绍DPDK， 主要以IA（Intel Architecture） 多核处理器为目标平台。 在IA上， 网络数据包处理远早于DPDK而存在。 从商业版的Windows到开源的Linux操作系统， 所有跨主机通信几乎都会涉及网络协议栈以及底层网卡驱动对于数据包的处理。 然而， 低速网络与高速网络处理对系统的要求完全不一样。

### 5.1 IA不适合进行数据包处理吗

以Linux为例， 传统网络设备驱动包处理的动作可以概括如下：数据包到达网卡设备、网卡设备依据配置进行DMA操作、网卡发送中断， 唤醒处理器、驱动软件填充读写缓冲区数据结构、数据报文达到内核协议栈， 进行高层处理、如果最终应用在用户态， 数据从内核搬移到用户态、如果最终应用在内核态， 在内核继续进行。

随着网络接口带宽从千兆向万兆迈进， 原先每个报文就会触发一个中断， 中断带来的开销变得突出， 大量数据到来会触发频繁的中断开销， 导致系统无法承受， 因此有人在Linux内核中引入了NAPI机制， 其策略是系统被中断唤醒后， 尽量使用轮询的方式一次处理多个数据包，直到网络再次空闲重新转入中断等待。 NAPI策略用于高吞吐的场景，效率提升明显。一个二层以太网包经过网络设备驱动的处理后， 最终大多要交给用户态的应用， 图1-4的典型网络协议层次OSI与TCP/IP模型， 是一个基础的网络模型与层次， 左侧是OSI定义的7层模型， 右侧是TCP/IP的具体实现。 网络包进入计算机大多需要经过协议处理， 在Linux系统中TCP/IP由Linux内核处理。 即使在不需要协议处理的场景下， 大多数场景下也需要把包从内核的缓冲区复制到用户缓冲区， 系统调用以及数据包复制的开销， 会直接影响用户态应用从设备直接获得包的能力。 而对于多样的网络功能节点来说， TCP/IP协议栈并不是数据转发节点所必需的。

![img](https://pic2.zhimg.com/80/v2-1def3a85ba7769ea1e3a6f0287a6e385_720w.webp)

以无线网为例， 图1-5的无线4G/LTE数据面网络协议展示了从基站、 基站控制器到无线核心网关的协议层次， 可以看到大量处理是在网络二、 三、 四层进行的。 如何让Linux这样的面向控制面原生设计的操作系统在包处理上减少不必要的开销一直是一大热点。 有个著名的高性能网络I/O框架Netmap， 它就是采用共享数据包池的方式， 减少内核到用户空间的包复制。

![img](https://pic1.zhimg.com/80/v2-1eaf340f78ce9d1ea1cf984f22300214_720w.webp)

NAPI与Netmap两方面的努力其实已经明显改善了传统Linux系统上的包处理能力， 那是否还有空间去做得更好呢？ 作为分时操作系统，Linux要将CPU的执行时间合理地调度给需要运行的任务。 相对于公平分时， 不可避免的就是适时调度。 早些年CPU核数比较少， 为了每个任务都得到响应处理， 进行充分分时， 用效率换响应， 是一个理想的策略。 现今CPU核数越来越多， 性能越来越强， 为了追求极端的高性能高效率， 分时就不一定总是上佳的策略。 以Netmap为例， 即便其减少了核到用户空间的内存复制， 但内核驱动的收发包处理和用户态线程依旧由操作系统调度执行， 除去任务切换本身的开销， 由切换导致的后续cache替换（不同任务内存热点不同） ， 对性能也会产生负面的影响。如果再往实时性方面考虑， 传统上， 事件从中断发生到应用感知，也是要经过长长的软件处理路径。 所以， 在2010年前采用IA处理器的用户会得出这样一个结论， 那就是IA不适合做包处理。真的是这样么？ 在IA硬件基础上， 包处理能力到底能做到多好， 有没有更好的方法评估和优化包处理性能， 怎样的软件设计方法能最充分地释放多核IA的包处理能力， 这些问题都是在DPDK出现之前， 实实在在地摆在Intel工程师面前的原始挑战。

### 5.2 DPDK最佳实践

如今， DPDK应该已经很好地回答了IA多核处理器是否可以应对高性能数据包处理这个问题。 而解决好这样一个问题， 也不是用了什么凭空产生的特殊技术， 更多的是从工程优化角度的迭代和最佳实践的融合。 如果要简单地盘点一下这些技术， 大致可以归纳如下。

轮询， 这一点很直接， 可避免中断上下文切换的开销。 之前提到Linux也采用该方法改进对大吞吐数据的处理， 效果很好。 在第7章， 我们会详细讨论轮询与中断的权衡。

用户态驱动， 在这种工作方式下， 既规避了不必要的内存拷贝又避免了系统调用。 一个间接的影响在于， 用户态驱动不受限于内核现有的数据格式和行为定义。 对mbuf头格式的重定义、 对网卡DMA操作的重新优化可以获得更好的性能。 而用户态驱动也便于快速地迭代优化， 甚至对不同场景进行不同的优化组合。 在第6章中， 我们将探讨用户态网卡收发包优化。

亲和性与独占， DPDK工作在用户态， 线程的调度仍然依赖内核。利用线程的CPU亲和绑定的方式， 特定任务可以被指定只在某个核上工作。 好处是可避免线程在不同核间频繁切换， 核间线程切换容易导致因cache miss和cache write back造成的大量性能损失。 如果更进一步地限定某些核不参与Linux系统调度， 就可能使线程独占该核， 保证更多cache hit的同时， 也避免了同一个核内的多任务切换开销。 在第3章， 我们会再展开讨论。

降低访存开销， 网络数据包处理是一种典型的I/O密集型（I/O bound） 工作负载。 无论是CPU指令还是DMA， 对于内存子系统（Cache+DRAM） 都会访问频繁。 利用一些已知的高效方法来减少访存的开销能够有效地提升性能。 比如利用内存大页能有效降低TLB miss，比如利用内存多通道的交错访问能有效提高内存访问的有效带宽， 再比如利用对于内存非对称性的感知可以避免额外的访存延迟。 而cache更是几乎所有优化的核心地带， 这些有意思而且对性能有直接影响的部分， 将在第2章进行更细致的介绍。

软件调优， 调优本身并不能说是最佳实践。 这里其实指代的是一系列调优实践， 比如结构的cache line对齐， 比如数据在多核间访问避免跨cache line共享， 比如适时地预取数据， 再如多元数据批量操作。 这些具体的优化策略散布在DPDK各个角落。 在第2章、 第6章、 第7章都会具体涉及。

利用IA新硬件技术， IA的最新指令集以及其他新功能一直是DPDK致力挖掘数据包处理性能的源泉。 拿Intel® DDIO技术来讲， 这个cache子系统对DMA访存的硬件创新直接助推了性能跨越式的增长。 有效利用SIMD（Single Instruction Multiple Data） 并结合超标量技术（Superscalar） 对数据层面或者对指令层面进行深度并行化， 在性能的进一步提升上也行之有效。 另外一些指令（比如cmpxchg） ， 本身就是lockless数据结构的基石， 而crc32指令对与4Byte Key的哈希计算也是改善明显。 这些内容， 在第2章、 第4章、 第5章、 第6章都会有涉及。

充分挖掘网卡的潜能， 经过DPDK I/O加速的数据包通过PCIe网卡进入系统内存， PCIe外设到系统内存之间的带宽利用效率、 数据传送方式（coalesce操作） 等都是直接影响I/O性能的因素。 在现代网卡中， 往往还支持一些分流（如RSS， FDIR等） 和卸载（如Chksum， TSO等）功能。 DPDK充分利用这些硬件加速特性， 帮助应用更好地获得直接的性能提升。

除了这些基础的最佳实践， 本书还会用比较多的篇幅带领大家进入DPDK I/O虚拟化的世界。 在那里， 我们依然从I/O的视角， 介绍业界广泛使用的两种主流方式， SR-IOV和Virtio， 帮助大家理解I/O硬件虚拟化的支撑技术以及I/O软件半虚拟化的技术演进和革新。随着DPDK不断丰满成熟， 也将自身逐步拓展到更多的平台和场景。 从Linux到FreeBSD， 从物理机到虚拟机， 从加速网络I/O到加速存储I/O， DPDK在不同纬度发芽生长。 在NFV大潮下， 无论是NFVI（例如， virtual switch） 还是VNF， DPDK都用坚实有力的性能来提供基础设施保障。

### 5.3 DPDK框架简介

DPDK为IA上的高速包处理而设计。 图1-6所示的DPDK主要模块分解展示了以基础软件库的形式， 为上层应用的开发提供一个高性能的基础I/O开发包。 它大量利用了有助于包处理的软硬件特性， 如大页、 缓存行对齐、 线程绑定、 预取、 NUMA、 IA最新指令的利用、 Intel DDIO、 内存交叉访问等。

核心库Core Libs， 提供系统抽象、 大页内存、 缓存池、 定时器及无锁环等基础组件。PMD库， 提供全用户态的驱动， 以便通过轮询和线程绑定得到极高的网络吞吐， 支持各种本地和虚拟的网卡。Classify库， 支持精确匹配（Exact Match） 、 最长匹配（LPM） 和通配符匹配（ACL） ， 提供常用包处理的查表操作。QoS库， 提供网络服务质量相关组件， 如限速（Meter） 和调度（Sched） 。

![img](https://pic1.zhimg.com/80/v2-7f82a524723100c399ec9db2c4325968_720w.webp)

除了这些组件， DPDK还提供了几个平台特性， 比如节能考虑的运行时频率调整（POWER） ， 与Linux kernel stack建立快速通道的KNI（Kernel Network Interface） 。 而Packet Framework和DISTRIB为搭建更复杂的多核流水线处理模型提供了基础的组件。

### 5.4 寻找性能优化的天花板

首先就看看数据包转发速率是否有天花板。 其实包转发的天花板就是理论物理线路上能够传送的最大速率， 即线速。 那数据包经过网络接口进入内存， 会经过I/O总线（例如， PCIe bus） ， I/O总线也有天花板， 实际事务传输不可能超过总线最大带宽。CPU从cache里加载/存储cache line有没有天花板呢， 当然也有， 比如Haswell处理器能在一个周期加载64字节和保存32字节。 同样内存控制器也有内存读写带宽。 这些不同纬度的边界把工作负载包裹起来， 而优化就是在这个边界里吹皮球， 不断地去接近甚至触碰这样的边界。

由于天花板是理论上的， 因此对于前面介绍的一些可量化的天花板， 总是能够指导并反映性能优化的优劣。 而有些天花板可能很难量化， 比如在某个特定频率的CPU下每个包所消耗的周期最小能做到多少。 对于这样的天花板， 可能只能用不断尝试实践的方式， 当然不同的方法可能带来不同程度的突破， 总的增益越来越少时， 就可能是接近天花板的时候。

那DPDK在IA上提供网络处理能力有多优秀呢？ 它是否已经能触及一些系统的天花板？ 在这些天花板中， 最难触碰的是哪一个呢？ 要真正理解这一点， 首先要明白在IA上包处理终极挑战的问题是什么， 在这之前我们需要先来回顾一下衡量包处理能力的一些常见能力指标。

### 5.5 解读数据包处理能力

不管什么样的硬件平台， 对于包处理都有最基本的性能诉求。 一般常被提到的有吞吐、 延迟、 丢包率、 抖动等。 对于转发， 常会以包转发率（pps， 每秒包转发率） 而不是比特率（bit/s， 每秒比特转发率） 来衡量转发能力， 这跟包在网络中传输的方式有关。 不同大小的包对存储转发的能力要求不尽相同。

线速（Wire Speed） 是线缆中流过的帧理论上支持的最大帧数。以太网（Ethernet） 为例， 一般所说的接口带宽， 1Gbit/s、10Gbit/s、 25Gbit/s、 40Gbit/s、 100Gbit/s， 代表以太接口线路上所能承载的最高传输比特率， 其单位是bit/s（bit per second， 位/秒） 。 实际上， 不可能每个比特都传输有效数据。 以太网每个帧之间会有帧间距（Inter-Packet Gap， IPG） ， 默认帧间距大小为12字节。 每个帧还有7个字节的前导（Preamble） ， 和1个字节的帧首定界符（Start FrameDelimiter， SFD） 。 有效内容主要是以太网的目的地址、 源地址、 以太网类型、 负载。 报文尾部是校验码。所以， 通常意义上的满速带宽能跑有效数据的吞吐可以由如下公式得到理论帧转发率：

![img](https://pic3.zhimg.com/80/v2-cfa7aaf2f6d0539290507f6edba2f516_720w.webp)

最大理论帧转发率的倒数表示了线速情况下先后两个包到达的时间间隔。按照这个公式， 将不同包长按照特定的速率计算可得到一个以太帧转发率。

![img](https://pic2.zhimg.com/80/v2-75ff56dac6e29d8424648f17166d4b21_720w.webp)

满足什么条件才能达到无阻塞转发的理论上限呢？ 如果我们把处理一个数据包的整个生命周期看做是工厂的生产流水线， 那么就要保证在这个流水线上， 不能有任何一级流水处理的延迟超过此时间间隔。 理解了这一点， 对照表1-1， 就很容易发现， 对任何一个数据包处理流水线来说， 越小的数据包， 挑战总是越大。 这样的红线对任何一个硬件平台， 对任何一个在硬件平台上设计整体流水线的设计师来说都是无法逃避并需要积极面对的。

### 5.6 探索IA处理器上最艰巨的任务

在通用处理器上处理包的最大挑战是什么？ 为什么以往通用处理器很少在数据面中扮演重要的角色？ 如果我们带着这些问题来看数据面上的负载， 就会有一个比较直观的理解。 这里拿40Gbit/s的速率作为考察包转发能力的样本。 如图1-8所示， 曲线为不同大小的包的最大理论转发能力。

![img](https://pic4.zhimg.com/80/v2-47817b9d32e92bf80b844f5fb1d5e93f_720w.webp)

分别截取64B和1024B数据包长， 图1-8所示的线速情况下的报文的指令成本能明显地说明不同报文大小给系统带来的巨大差异。 就如我们在包转发率那一节中理解的， 对于越小的包， 相邻包到达的时间间隔就越小， 16.8ns vs 208.8ns。 假设CPU的主频率是2GHz， 要达到理论最大的转发能力， 对于64B和1024B软件分别允许消耗33和417个时钟周期。在存储转发（store-forward） 模型下， 报文收发以及查表都需要访存。那就对比一下访存的时钟周期， 一次LLC命中需要大约40个时钟周期，如果LLC未命中， 一次内存的读就需要70ns。 换句话说， 对于64B大小的包， 即使每次都能命中LLC， 40个时钟周期依然离33有距离。 显然，小包处理时延对于通用CPU系统架构的挑战是巨大的。那是否说明IA就完全不适合高性能的网络负载呢？ 答案是否定的。证明这样的结论我们从两个方面入手， 一个是IA平台实际能提供的最大能力， 另一个是这个能力是否足以应对一定领域的高性能网络负载。

DPDK的出现充分释放了IA平台对包处理的吞吐能力。 我们知道，随着吞吐率的上升， 中断触发的开销是不能忍受的， DPDK通过一系列软件优化方法（大页利用， cache对齐， 线程绑定， NUMA感知， 内存通道交叉访问， 无锁化数据结构， 预取， SIMD指令利用等） 利用IA平台硬件特性， 提供完整的底层开发支持库。 使得单核三层转发可以轻松地突破小包30Mpps， 随着CPU封装的核数越来越多， 支持的PCIe通道数越来越多， 整系统的三层转发吞吐在2路CPU的Xeon E5-2658v3上可以达到300Mpps。 这已经是一个相当可观的转发吞吐能力了。虽然这个能力不足以覆盖网络中所有端到端的设备场景， 但无论在核心网接入侧， 还是在数据中心网络中， 都已经可以覆盖相当多的场景。

随着数据面可软化的发生， 数据面的设计、 开发、 验证乃至部署会发生一系列的变化。 首先， 可以采用通用服务器平台， 降低专门硬件设计成本； 其次， 基于C语言的开发， 就程序员数量以及整个生态都要比专门硬件开发更丰富； 另外， 灵活可编程的数据面部署也给网络功能虚拟化（NFV） 带来了可能， 更会进一步推进软件定义网络（SDN） 的全面展开。

## 6.软件包处理的潜力——再识DPDK

### 6.1 DPDK加速网络节点

随着处理器的每一代更新， 在IA上的性能提升以很高的斜率不断发酵。 当千兆、 万兆接口全速转发已不再是问题时， DPDK已将目标伸向百万兆的接口。DPDK软件包内有一个最基本的三层转发实例（l3fwd） ， 可用于测试双路服务器整系统的吞吐能力， 实验表明可以达到220Gbit/s的数据报文吞吐能力。 值得注意的是， 除了通过硬件或者软件提升性能之外， 如今DPDK整系统报文吞吐能力上限已经不再受限于CPU的核数， 当前瓶颈在于PCIe（IO总线） 的LANE数。 换句话说， 系统性能的整体I/O天花板不再是CPU， 而是系统所提供的所有PCIe LANE的带宽， 能插入多少个高速以太网接口卡。

在这样的性能基础上， 网络节点的软化就成为可能。 对于网络节点上运转的不同形态的网络功能， 一旦软化并适配到一个通用的硬件平台， 随之一个自然的诉求可能就是软硬件解耦。 解耦正是网络功能虚拟化（NFV） 的一个核心思想， 而硬件解耦的多个网络功能在单一通用节点上的隔离共生问题， 是另一个核心思想虚拟化诠释的。 当然这个虚拟化是广义的， 在不同层面可以有不同的支撑技术。

NFV有很多诉求， 业务面高性能， 控制面高可用、 高可靠、 易运维、 易管理等。 但没有业务面的高性能， 后续的便无从谈起。 DPDK始终为高性能业务面提供坚实的支撑， 除此以外， DPDK立足IA的CPU虚拟化技术和IO的虚拟化技术， 对各种通道做持续优化改进的同时， 也对虚拟交换（vswitch） 的转发面进化做出积极贡献。 应对绝对高吞吐能力的要求， DPDK支持各种I/O的SR-IOV接口； 应对高性能虚拟主机网络的要求， DPDK支持标准virtio接口； 对虚拟化平台的支撑， DPDK从KVM、 VMWARE、 XEN的hypervisor到容器技术， 可谓全平台覆盖。

### 6.2 DPDK加速计算节点

C10K是IT界的一个著名命题， 甚至后续衍生出了关于C1M和C10M的讨论。 其阐述的一个核心问题就是， 随着互联网发展， 随着数据中心接口带宽不断提升， 计算节点上各种互联网服务对于高并发下的高吞吐有着越来越高的要求。但是单一接口带宽的提高并不能直接导致高并发、 高吞吐服务的发生， 即使用到了一系列系统方法（异步非阻塞， 线程等） ， 但网络服务受限于内核协议栈多核水平扩展上的不足以及建立拆除连接的高开销，开始逐渐阻碍进一步高并发下高带宽的要求。 另一方面， 内核协议栈需要考虑更广泛的支持， 并不能为特定的应用做特殊优化， 一般只能使用系统参数进行调优。

当然， 内核协议栈也在不断改进， 而以应用为中心的趋势也会不断推动用户态协议栈的涌现。 有基于BSD协议栈移植的， 有基于多核模型重写的原型设计， 也有将整个Linux内核包装成库的。 它们大多支持以DPDK作为I/O引擎， 有些也将DPDK的一些优化想法加入到协议栈的优化中， 取得了比较好的效果。

### 6.3 DPDK加速存储节点

除了在网络、 计算节点的应用机会之外， DPDK的足迹还渗透到存储领域。 Intel®最近开源了SPDK（Storage Performance Development Kit） ， 一款存储加速开发套件， 其主要的应用场景是iSCSI性能加速。目前iSCSI系统包括前端和后端两个部分， 在前端， DPDK提供网络I/O加速， 加上一套用户态TCP/IP协议栈（ 目前还不包含在开源包中） ， 以流水线的工作方式支撑起基于iSCSI的应用； 在后端， 将DPDK用户态轮询驱动的方式实践在NVMe上， PMD的NVMe驱动加速了后端存储访问。 这样一个端到端的整体方案， 用数据证明了卓有成效的IOPS性能提升。

### 6.4 DPDK的方法论

专用负载下的针对性软件优化：专用处理器通过硬件架构专用优化来达到高性能， DPDK则利用通用处理器， 通过优化的专用化底层软件来达到期望的高性能。 这要求DPDK尽可能利用一切平台（ CPU， 芯片组， PCIe以及网卡） 特性， 并针对网络负载的特点， 做针对性的优化， 以发掘通用平台在某一专用领域的最大能力。

追求可水平扩展的性能：利用多核并行计算技术， 提高性能和水平扩展能力。 对于产生的并发干扰， 遵循临界区越薄越好、 临界区碰撞越少越好的指导原则。 数据尽可能本地化和无锁化， 追求吞吐率随核数增加而线性增长。

向Cache索求极致的实现优化性能：相比于系统优化和算法优化， 实现优化往往较少被提及。 实现优化对开发者的要求体现在需要对处理器体系结构有所了解。 DPDK可谓集大量的实现优化之大成， 而这些方法多数围绕着Cache进行， 可以说能娴熟地驾驭好Cache， 在追求极致性能的路上就已经成功了一半。

理论分析结合实践推导：性能的天花板在哪， 调优是否还有空间， 是否值得花更多的功夫继续深入， 这些问题有时很难直接找到答案。 分析、 推测、 做原型、 跑数据、 再分析， 通过这样的螺旋式上升， 慢慢逼近最优解， 往往是实践道路上的导航明灯。 条件允许下， 有依据的理论量化计算， 可以更可靠地明确优化目标。

### 6.5 从融合的角度看DPDK

随着云计算的推进， ICT这个词逐渐在各类技术研讨会上被提及。云计算的定义虽然有各种版本， 但大体都包含了对网络基础设施以及对大数据处理的基本要求， 这也是IT与CT技术融合的推动力。那这和DPDK有关系吗？ 还真有！ 我们知道云计算的对象是数据，数据在云上加工， 可还是要通过各种载体落到地上。 在各种载体中最广泛使用的当属IP， 它是整个互联网蓬勃发展的基石。 高效的数据处理总是离不开高效的数据承载网络。

教科书说到网络总会讲到那经典的7层模型， 最低层是物理层， 最高层是应用层。 名副其实的是， 纵观各类能联网的设备， 从终端设备到网络设备再到数据中心服务器， 还真是越靠近物理层的处理以硬件为主， 越靠近应用层的处理以软件为主。 这当然不是巧合， 其中深谙了一个原则， 越是能标准化的， 越要追求极简极速， 所以硬件当仁不让， 一旦进入多样性可变性强的领域， 软件往往能发挥作用。 但没有绝对和一成不变， 因为很多中间地带更多的是权衡。

DPDK是一个软件优化库， 目标是在通用处理器上发挥极致的包能力， 以媲美硬件级的性能。 当然软件是跑在硬件上的， 如果看整个包处理的硬件平台， 软硬件融合的趋势也相当明显。 各类硬件加速引擎逐渐融入CPU构成异构SoC（System On-Chip） ， 随着Intel®对Altera®收购的完成， CPU+FPGA这一对组合也给足了我们想象的空间， 可以说包处理正处在一个快速变革的时代。

原文地址：https://zhuanlan.zhihu.com/p/415818071

作者：linux

# 【NO.428】最强阿里巴巴历年经典面试题汇总：C++研发岗

![img](https://pic1.zhimg.com/80/v2-8c0605f70d3c3cc20b6a6407525943d0_720w.webp)

（1）、B树、存储模型

（2）、字典树构造及其优化与应用

（3）、持久化数据结构，序列化与反序列化时机（4）、在无序数组中找最大的K个数?

（4）、大规模文本文件，全是单词，求前10词频的单词

（5）、堆排序与其在求10词频问题中的应用

（6）、字典树与其在统计词频上的应用

（7）、红黑树的特性与其在C++ STL中的应用

（8）、红黑树的调整

（9）、贪心算法与其弊端

（10）、能取得全局最优解的算法

（11）、动态规划的原理与本质

（12）、01背包问题的详细解释

（13）、进程间通信方式

（14）、数据库中join的类型与区别

（15）、数据库的ACID

（16）、实现bitmap数据结构，包括数据的存储与插入方式

（17）、实现unordered_map，键为string，value不限

（18）、实现unordered_map过程中的冲突解决办法

（19）、一串int型整数存放磁盘上的压缩存储方式，包括写入与读取及内存无法一次性读取时的解决办法

（20）、对Java的了解

（21）、Bloom过滤器处理大规模问题时的持久化，包括内存大小受限、磁盘换入换出问题

（22）、线程池的了解、优点、调度处理方式和保护任务队列的方式

（23）、对象复用的了解

（24）、零拷贝的了解

（25）、Linux的I/O模型

（26）、异步I/O的详细解释

（27）、线程池对线程的管理方式，包括初始化线程的方法、线程创建后的管理、指派任务的方式

（28）、同步I/O与异步I/O的区别

（29）、Direct I/O 和其与异步I/O的区别

（30）、Linux内核如何调用Direct I/O

（31）、Bloom过滤器的优点与原理

（32）、字符串hash成状态位的具体实现方式

（33）、hash函数如何保证冲突最小

（34）、文件读写使用的系统调用

（35）、文件读写中涉及的磁盘缓冲区与其手动flush问题

（36）、数据库join的具体含义

（37）、struct与class的区别

（38）、STL库的介绍

（39）、vector使用的注意点及其原因

（40）、频繁对vector调用push_back()对性能的影响和原因

（41）、vector重新分配内存的大小与方式

（42）、hashmap的实现方式

（43）、map的实现方式

（44）、C++虚函数的具体实现原理

（45）、实现编译器处理虚函数表应该如何处理

（46）、析构函数一般写成虚函数的原因

（47）、解释哲学家进餐问题

（48）、描述银行家算法

（49）、实现一种算法解决哲学家进餐问题

（50）、大数量整数的去重问题

（51）、如果用bitmap解决大数量整数去重问题，计算当全为int型整数时需要消耗的内存

（52）、算法题：环形公路上加油站算法问题

现有一圆环形路，路上有n个加油站，第i个加油站储存有N[i]升容量的油，与下一个加油站之间有一定的距离g[i]，一汽车初始无油，假设该车每公里消耗1升油，请问该车从哪个加油站出发可以绕该环形路行驶一圈。

（53）、多个服务器通信，线程池的设定

（54）、哈希表的冲突解决方式

（55）、哈希表在桶固定的情况下，时间复杂度。怎么优化？

（56）、多线程中哈希表保证线程安全

（57）、哈希表特别大，桶特别多的时候怎么加锁

（58）、C语言变量存放位置

（59）、栈上的分配内存快还是堆上快

（60）、http的长连接和短连接是什么，各有什么优缺点，然后使用场景

（61）、在一个浏览器里面输入一个网址，后回车，在这后面发生了什么？

（62）、进程线程的区别，多进程与多线程的区别

（63）、什么是生产者消费者模型？如果一个人洗碗，另一个人马上用碗，是生产者消费者模型吗？

（64）、GET/POST的区别，GET/POST的安全性问题，假如你来实现，你怎么实现GET/POST的安全性

（65）、你做服务器压力测试时，用什么测试，如何配置参数，吞吐量大小，并发量大小

（66）、类似Nginx这种web服务器是用什么数据结构实现定时器事件的，四叉堆知道是什么吗，与二叉堆有什么区别？

（67）、动态规划与贪心算法的区别，什么情况下，动态规划可以转换为贪心算法

（68）、说一下快排，快排是稳定的吗?为什么？哪些排序算法稳定？哪些不稳定？

（69）、数据库有哪些索引，你知道哪些索引引擎，这些索引引擎有什么区别

（70）、epoll与select的区别，epoll在什么情况下吞吐率比较高？

（71）、非阻塞与异步的区别？

（72）、HTTP1.0和HTTP1.1的区别，服务器端如何判断是长连接还是短连接？

（73）、HTTP2.0的 新特性，它是如何实现共用一个长连接？

（74）、tcp如何连接到服务器，你如何判断tcp连接到服务器，你服务器的输入是什么？

（75）、epoll的底层实现

**完整的linux c/c++高级开发技术路线图**

![img](https://pic2.zhimg.com/80/v2-dd61a798de3ebe741df54e9eb6da1a7d_720w.webp)

![img](https://pic2.zhimg.com/80/v2-b7bf75892f81a3ee5a9ad3b9c0f9f4e1_720w.webp)

![img](https://pic1.zhimg.com/80/v2-6a3a9de5f37c74528accdb5b84728034_720w.webp)

![img](https://pic4.zhimg.com/80/v2-522e78c886f5121c08990e57e9cd2b3b_720w.webp)

![img](https://pic3.zhimg.com/80/v2-82b0ac03ee96fa62235d85bbc6fbb242_720w.webp)

![img](https://pic2.zhimg.com/80/v2-d84edf93cf10529900617d85c19eda49_720w.webp)

![img](https://pic4.zhimg.com/80/v2-50cc437d64513556a447ad9b64591c83_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/372020083

作者：linux

# 【NO.429】超硬核，进程在内存中的样子！以及进程的一生

## 1. 什么是进程

简单来讲，进程就是**运行中**的程序。

进一步讲，进程是在用户空间中，加载器根据程序头提供的信息，将程序**加载到内存并运行的实体**。

### **1.1 进程的虚拟空间**

![img](https://pic1.zhimg.com/80/v2-f0f493070306e22efa6a0b983625a638_720w.webp)

ELF 文件头中指定的程序入口地址，以及各个节区在程序运行时的内存排布地址等，指的都是在进程虚拟空间中的地址。

虚拟空间可以认为是操作系统给每个进程准备的沙盒，每个进程只存活在自己的虚拟世界里，却感觉自己独占了所有的系统资源（内存）。

![img](https://pic2.zhimg.com/80/v2-caf70a97f2a00d6257193b444d5482ad_720w.webp)

当一个进程要使用某块内存时，它会将自己世界里的一个内存地址告诉操作系统，剩下的事情就由操作系统接管了。操作系统中的内存管理策略将决定映射哪块真实的物理内存，供其使用。操作系统会竭尽全力满足所有进程合法的内存访问请求。一旦发现进程试图访问非法内存，操作系统会把进程杀死，防止它做“坏事”影响到系统或其它进程。

![img](https://pic3.zhimg.com/80/v2-12deb56e501f76dd66c899fb5362e586_720w.webp)

如上图，每个进程都有自己的堆栈、可读可写的数据段等。

### **1.2 虚拟空间的好处**

一方面为了安全，防止进程操作其它进程或者系统内核的数据；

另一方面为了保证系统可同时运行多个进程，且单个进程使用的内存空间可以超过实际的物理内存容量。

该做法的另一个结果则是降低了每个进程内存管理的复杂度，进程只需关心如何使用自己线性排列的虚拟地址，而不需关心物理内存的实际容量，以及如何使用真实的物理内存。

### **1.3 虚拟空间地址排布**

在 32 位系统下，进程的虚拟地址空间有 4G （2^32 Bytes），其中的 1G 分配给了内核空间，用户可以使用剩余的 3G。在 64 位的 Linux 系统上，进程的虚拟地址空间可以达到 256TB，内核和应用分别占用 128TB。目前来看，这样的地址空间范围足够用了。

一个典型的内存排布结构如下图所示：

![img](https://pic1.zhimg.com/80/v2-06aba8af8b0e9151e11151f7dc26bf78_720w.webp)

其中，#1 部分是按照 ELF 文件中的程序头信息，加载文件内容所得到的。除此之外，加载器还会**为每个应用分配栈区（Stack）、堆区（Heap）和动态链接库加载区**。

栈和堆分别向相对的方向增长，系统会有相应的保护措施，阻止越界行为发生。

在 Linux 系统中，使用如下命令可查看一个运行中的进程的内存排布。

```text
cat /proc/PID/maps
```

**实例**

```text
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

static char static_data[16] = "I'm Static Data";
static char raw_static_data[40960];
static const char const_data[16] = "I'm Const Data";

int main(int args, char **argv)
{
	printf("Message In Main\n");
	while (1) {
		sleep(1);
	}

	return 0;
}
```



```text
liyongjun@Box:~/project/c/C_study/tmp$ ps -ef | grep elfdemo
liyongj+   19786   19785  0 12:23 pts/0    00:00:00 ./tmp/elfdemo.out
liyongjun@Box:~/project/c/C_study/tmp$ cat /proc/19786/maps
560ca27b1000-560ca27b2000 r--p 00000000 08:05 3014892                    /home/liyongjun/project/c/C_study/tmp/elfdemo.out
560ca27b2000-560ca27b3000 r-xp 00001000 08:05 3014892                    /home/liyongjun/project/c/C_study/tmp/elfdemo.out
560ca27b3000-560ca27b4000 r--p 00002000 08:05 3014892                    /home/liyongjun/project/c/C_study/tmp/elfdemo.out
560ca27b4000-560ca27b5000 r--p 00002000 08:05 3014892                    /home/liyongjun/project/c/C_study/tmp/elfdemo.out
560ca27b5000-560ca27b6000 rw-p 00003000 08:05 3014892                    /home/liyongjun/project/c/C_study/tmp/elfdemo.out
560ca27b6000-560ca27c0000 rw-p 00000000 00:00 0 
560ca37a0000-560ca37c1000 rw-p 00000000 00:00 0                          [heap]
7f139d870000-7f139d895000 r--p 00000000 08:05 657544                     /usr/lib/x86_64-linux-gnu/libc-2.31.so
7f139d895000-7f139da0d000 r-xp 00025000 08:05 657544                     /usr/lib/x86_64-linux-gnu/libc-2.31.so
7f139da0d000-7f139da57000 r--p 0019d000 08:05 657544                     /usr/lib/x86_64-linux-gnu/libc-2.31.so
7f139da57000-7f139da58000 ---p 001e7000 08:05 657544                     /usr/lib/x86_64-linux-gnu/libc-2.31.so
7f139da58000-7f139da5b000 r--p 001e7000 08:05 657544                     /usr/lib/x86_64-linux-gnu/libc-2.31.so
7f139da5b000-7f139da5e000 rw-p 001ea000 08:05 657544                     /usr/lib/x86_64-linux-gnu/libc-2.31.so
7f139da5e000-7f139da64000 rw-p 00000000 00:00 0 
7f139da76000-7f139da77000 r--p 00000000 08:05 657540                     /usr/lib/x86_64-linux-gnu/ld-2.31.so
7f139da77000-7f139da9a000 r-xp 00001000 08:05 657540                     /usr/lib/x86_64-linux-gnu/ld-2.31.so
7f139da9a000-7f139daa2000 r--p 00024000 08:05 657540                     /usr/lib/x86_64-linux-gnu/ld-2.31.so
7f139daa3000-7f139daa4000 r--p 0002c000 08:05 657540                     /usr/lib/x86_64-linux-gnu/ld-2.31.so
7f139daa4000-7f139daa5000 rw-p 0002d000 08:05 657540                     /usr/lib/x86_64-linux-gnu/ld-2.31.so
7f139daa5000-7f139daa6000 rw-p 00000000 00:00 0 
7ffe3e71b000-7ffe3e73c000 rw-p 00000000 00:00 0                          [stack]
7ffe3e7e1000-7ffe3e7e4000 r--p 00000000 00:00 0                          [vvar]
7ffe3e7e4000-7ffe3e7e5000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 --xp 00000000 00:00 0                  [vsyscall]
```

## 2. 进程的启动

从用户角度来看，启动一个进程有许多种方式，可以配置开机自启动，可以在 shell 中手动运行，也可以从脚本或其它进程中启动。

而从开发人员角度看，无非就是**两个系统调用，即 fork() 和 execve()**。下面就来探究下这两个系统调用的行为细节。

### **2.1 fork() 系统调用**

![img](https://pic4.zhimg.com/80/v2-43a5796b9d5ba2b756cfc68671ba3d37_720w.webp)

fork() 系统调用将创建一个与父进程几乎一样的新进程，之后继续执行下面的指令。程序可以根据 fork() 的返回值，确定当前处于父进程中，还是子进程中——在父进程中，返回值为新创建子进程的进程 ID，在子进程中，返回值是 0。一些使用多进程模型的服务器程序（比如 sshd），就是通过 fork() 系统调用来实现的，每当新用户接入时，系统就会专门创建一个新进程，来服务该用户。

fork() 系统调用所创建的新进程，与其父进程的内存布局和数据几乎一模一样。在内核中，它们的代码段所在的只读存储区会共享相同的物理内存页；而可读可写的数据段、堆及栈等内存，内核会使用写时拷贝技术，为每个进程独立创建一份。

在 fork() 系统调用刚刚执行完的那一刻，子进程即可拥有一份与父进程完全一样的数据拷贝。对于已打开的文件，内核会增加每个文件描述符的引用计数，每个进程都可以用相同的文件句柄访问同一个文件。

深入理解了这些底层行为细节，就可以顺理成章地理解 fork() 的一些行为表现和正确使用规范，无需死记硬背，也可获得一些别人踩过坑后才能获得的经验。

比如，使用多进程模型的网络服务程序中，为什么要在子进程中关闭监听套接字，同时要在父进程中关闭新连接的套接字呢？

原因在于 fork() 执行之后，所有已经打开的套接字都被增加了引用计数，在其中任一个进程中都无法彻底关闭套接字，只能减少该文件的引用计数。因此，在 fork() 之后，每个进程立即关闭不再需要的文件是个好的策略，否则很容易导致大量没有正确关闭的文件一直占用系统资源的现象。这让我想到了管道，父子进程各自关闭自己不使用的一端，当时很疑惑，干嘛要关啊，不用就不用呗，放那碍你啥事了，现在想想，出于安全考虑，关闭确实是个好习惯。

### **2.2 execve() 系统调用**

execve() 系统调用的作用是运行另外一个指定的程序。它会把新程序加载到当前进程的内存空间内，当前的进程会被丢弃，它的堆、栈和所有的段数据都会被新进程相应的部分代替，然后会从新程序的初始化代码和 main 函数开始运行。同时，进程的 ID 将保持不变。

execve() 系统调用通常与 fork() 系统调用配合使用。从一个进程中启动另一个程序时，通常是先 fork() 一个子进程，然后在子进程中使用 execve() 变身为运行指定程序的进程。 例如，当用户在 Shell 下输入一条命令启动指定程序时，Shell 就是先 fork() 了自身进程，然后在子进程中使用 execve() 来运行指定的程序。

需要注意的是，exec 系列函数的返回值只在遇到错误的时候才有意义。如果新程序成功地被执行，那么当前进程的所有数据就都被新进程替换掉了，所以永远也不会有任何返回值。

对于已打开文件的处理，在 exec() 系列函数执行之前，应该确保全部关闭。因为 exec() 调用之后，当前进程就完全变身成另外一个进程了，老进程的所有数据都不存在了。如果 exec() 调用失败，当前打开的文件状态应该被保留下来。让应用层处理这种情况会非常棘手，而且有些文件可能是在某个库函数内部打开的，应用对此并不知情，更谈不上正确地维护它们的状态了。

所以，对于执行 exec() 函数的应用，应该总是使用内核为文件提供的执行时关闭标志（FD_CLOEXEC）。设置了该标志之后，如果 exec() 执行成功，文件就会被自动关闭；如果 exec() 执行失败，那么文件会继续保持打开状态。使用系统调用 fcntl() 可以设置该标志。

## 3. 监控子进程状态

在 Linux 应用中，父进程需要监控其创建的所有子进程的退出状态，可以通过如下几个系统调用来实现。

- pid_t wait(int * statua)

一直阻塞地等待任意一个子进程退出，返回值为退出的子进程的 ID，status 中包含子进程设置的退出标志。

- pid_t waitpid(pid_t pid, int * status, int options)

可以用 pid 参数指定要等待的进程或进程组的 ID，options 可以控制是否阻塞，以及是否监控因信号而停止的子进程等。

- int waittid(idtype_t idtype, id_t id, siginfo_t *infop, int options)

提供比 waitpid 更加精细的控制选项来监控指定子进程的运行状态。

- wait3() 和 wait4() 系统调用

可以在子进程退出时，获取到子进程的资源使用数据。

更详细的信息请参考帮助手册。

重点讨论：即使父进程在业务逻辑上不关心子进程的终止状态，也需要使用 wait 类系统调用，原因如下：

在 Linux 的内核实现中，允许父进程在子进程创建之后的任意时刻用 wait() 系列系统调用来确定子进程的状态。

也就是说，如果子进程在父进程调用 wait() 之前就终止了，内核需要保留该子进程的终止状态和资源使用等数据，直到父进程执行 wait() 把这些数据取走。

在子进程终止到父进程获取退出状态之间的这段时间，这个进程会变成所谓的僵尸状态，在该状态下，任何信号都无法结束它。如果系统中存在大量此类僵尸进程，势必会占用大量内核资源，甚至会导致新进程创建失败。

如果父进程也终止，那么 init 进程会接管这些僵尸进程并自动调用 wait ，从而把它们从系统中移除。但是对于长期运行的服务器程序，这一定不是开发者希望看到的结果。所以，父进程一定要仔细维护好它创建的所有子进程的状态，防止僵尸进程的产生。

## 4. 进程的终止

正常终止一个进程可以用 _exit 系统调用来实现，原型为：

```text
void _exit(int status);
```

其中 status 会返回 wait() 类的系统调用。进程退出时会清理掉该进程占用的所有系统资源，包括关闭打开的文件描述符、释放持有的文件锁和内存锁、取消内存映射等，还会给一些子进程发送信号。该系统调用一定会成功，永远不会返回。

在退出之前，还希望做一些个性化的清理操作，可以使用库函数 exit() 。函数原型为：

```text
void exit(int status);
```

这个库函数先调用退出处理程序，然后再利用 status 参数调用 _exit() 系统调用。这里的退出处理程序可以通过 atexit() 或 on_exit() 函数注册。其中 atexit() 只能注册返回值和参数都为空的回调函数，而 on_exit() 可以注册带参数的回调函数。退出处理函数的执行顺序与注册顺序相反。它们的函数原型如下所示：

```text
int atexit(void (*func)(void));
int on_exit(void (*func)(int, void *), void *arg);
```

通常情况下，个性化的退出处理函数只会在主进程中执行一次，所以 exit() 函数一般在主进程中使用，而在子进程中只使用 _exit() 系统调用结束当前进程。

原文地址：https://zhuanlan.zhihu.com/p/401087855

作者：linux

# 【NO.430】Linux原生异步IO原理与实现（Native AIO）

## 1.什么是异步 IO？

> 异步 IO：当应用程序发起一个 IO 操作后，调用者不能立刻得到结果，而是在内核完成 IO 操作后，通过信号或回调来通知调用者。

异步 IO 与同步 IO 的区别如图所示：

![img](https://pic2.zhimg.com/80/v2-7747749a0c0f4f6ab9901032450bbd7d_720w.webp)

从上图可知，同步 IO 必须等待内核把 IO 操作处理完成后才返回。而异步 IO 不必等待 IO 操作完成，而是向内核发起一个 IO 操作就立刻返回，当内核完成 IO 操作后，会通过信号的方式通知应用程序。

## 2.Linux 原生 AIO 原理

Linux Native AIO 是 Linux 支持的原生 AIO，为什么要加原生这个词呢？因为Linux存在很多第三方的异步 IO 库，如 libeio 和 glibc AIO。所以为了加以区别，Linux 的内核提供的异步 IO 就称为原生异步 IO。

很多第三方的异步 IO 库都不是真正的异步 IO，而是使用多线程来模拟异步 IO，如 libeio 就是使用多线程来模拟异步 IO 的。

本文主要介绍 Linux 原生 AIO 的原理和实现，所以不会对其他第三方的异步 IO 库进行分析，下面我们先来介绍 Linux 原生 AIO 的原理。

如图所示：

![img](https://pic4.zhimg.com/80/v2-9b8ddcee57127d96f1b87b0c9d93a68f_720w.webp)

Linux 原生 AIO 处理流程：

- 当应用程序调用 io_submit 系统调用发起一个异步 IO 操作后，会向内核的 IO 任务队列中添加一个 IO 任务，并且返回成功。
- 内核会在后台处理 IO 任务队列中的 IO 任务，然后把处理结果存储在 IO 任务中。
- 应用程序可以调用 io_getevents 系统调用来获取异步 IO 的处理结果，如果 IO 操作还没完成，那么返回失败信息，否则会返回 IO 处理结果。

从上面的流程可以看出，Linux 的异步 IO 操作主要由两个步骤组成：

- \1) 调用 io_submit 函数发起一个异步 IO 操作。
- \2) 调用 io_getevents 函数获取异步 IO 的结果。

## 3.Linux原生AIO实现

一般来说，使用 Linux 原生 AIO 需要 3 个步骤：

- \1) 调用 io_setup 函数创建一个一般 IO 上下文。
- \2) 调用 io_submit 函数向内核提交一个异步 IO 操作。
- \3) 调用 io_getevents 函数获取异步 IO 操作结果。

所以，我们可以通过分析这三个函数的实现来理解 Linux 原生 AIO 的实现。

> Linux 原生 AIO 实现在源码文件 /fs/aio.c 中。

### 3.1 **创建异步IO上下文**

要使用 Linux 原生 AIO，首先需要创建一个异步 IO 上下文，在内核中，异步 IO 上下文使用 kioctx 结构表示，定义如下：

```text
struct kioctx {
    atomic_t                users;    // 引用计数器
    int                     dead;     // 是否已经关闭
    struct mm_struct        *mm;      // 对应的内存管理对象

    unsigned long           user_id;  // 唯一的ID，用于标识当前上下文, 返回给用户
    struct kioctx           *next;

    wait_queue_head_t       wait;     // 等待队列
    spinlock_t              ctx_lock; // 锁

    int                     reqs_active; // 正在进行的异步IO请求数
    struct list_head        active_reqs; // 正在进行的异步IO请求对象
    struct list_head        run_list;

    unsigned                max_reqs;  // 最大IO请求数

    struct aio_ring_info    ring_info; // 环形缓冲区

    struct work_struct      wq;
};
```

在 kioctx 结构中，比较重要的成员为 active_reqs 和 ring_info。active_reqs 保存了所有正在进行的异步 IO 操作，而 ring_info 成员用于存放异步 IO 操作的结果。

kioctx 结构如图所示：

![img](https://pic1.zhimg.com/80/v2-36876f378b7685d59cff93f06631edd0_720w.webp)

如上图所示，active_reqs 成员保存的异步 IO 操作队列是以 kiocb 结构为单元的，而 ring_info 成员指向一个类型为 aio_ring_info 结构的环形缓冲区（Ring Buffer）。

所以我们先来看看 kiocb 结构和 aio_ring_info 结构的定义：

```text
struct kiocb {
    ...
    struct file         *ki_filp;      // 异步IO操作的文件对象
    struct kioctx       *ki_ctx;       // 指向所属的异步IO上下文
    ...
    struct list_head    ki_list;       // 用于连接所有正在进行的异步IO操作对象
    __u64               ki_user_data;  // 用户提供的数据指针(可用于区分异步IO操作)
    loff_t              ki_pos;        // 异步IO操作的文件偏移量
    ...
};
```

kiocb 结构比较简单，主要用于保存异步 IO 操作的一些信息，如：

- ki_filp：用于保存进行异步 IO 的文件对象。
- ki_ctx：指向所属的异步 IO 上下文对象。
- ki_list：用于连接当前异步 IO 上下文中的所有 IO 操作对象。
- ki_user_data：这个字段主要提供给用户自定义使用，比如区分异步 IO 操作，或者设置一个回调函数等。
- ki_pos：用于保存异步 IO 操作的文件偏移量。

而 aio_ring_info 结构是一个环形缓冲区的实现，其定义如下：

```text
struct aio_ring_info {
    unsigned long       mmap_base;     // 环形缓冲区的虚拟内存地址
    unsigned long       mmap_size;     // 环形缓冲区的大小

    struct page         **ring_pages;  // 环形缓冲区所使用的内存页数组
    spinlock_t          ring_lock;     // 保护环形缓冲区的自旋锁
    long                nr_pages;      // 环形缓冲区所占用的内存页数

    unsigned            nr, tail;

    // 如果环形缓冲区不大于 8 个内存页时
    // ring_pages 就指向 internal_pages 字段
#define AIO_RING_PAGES  8
    struct page         *internal_pages[AIO_RING_PAGES]; 
};
```

这个环形缓冲区主要用于保存已经完成的异步 IO 操作的结果，异步 IO 操作的结果使用 io_event 结构表示。如图所示：



![img](https://pic4.zhimg.com/80/v2-1b1be61b97844059de263fa471d3b217_720w.webp)



图中的 head 代表环形缓冲区的开始位置，而 tail 代表环形缓冲区的结束位置，如果 tail 大于 head，则表示有完成的异步 IO 操作结果可以获取。如果 head 等于 tail，则表示没有完成的异步 IO 操作。

环形缓冲区的 head 和 tail 位置保存在 aio_ring 的结构中，其定义如下：

```text
struct aio_ring {
    unsigned    id;
    unsigned    nr;    // 环形缓冲区可容纳的 io_event 数
    unsigned    head;  // 环形缓冲区的开始位置
    unsigned    tail;  // 环形缓冲区的结束位置
    ...
};
```

上面介绍了那么多数据结构，只是为了接下来的源码分析更加容易明白。

现在，我们开始分析异步 IO 上下文的创建过程，异步 IO 上下文的创建通过调用 io_setup 函数完成，而 io_setup 函数会调用内核函数 sys_io_setup，其实现如下：

```text
asmlinkage long sys_io_setup(unsigned nr_events, aio_context_t *ctxp)
{
    struct kioctx *ioctx = NULL;
    unsigned long ctx;
    long ret;
    ...
    ioctx = ioctx_alloc(nr_events);  // 调用 ioctx_alloc 函数创建异步IO上下文
    ret = PTR_ERR(ioctx);
    if (!IS_ERR(ioctx)) {
        ret = put_user(ioctx->user_id, ctxp); // 把异步IO上下文的标识符返回给调用者
        if (!ret)
            return 0;
        io_destroy(ioctx);
    }
out:
    return ret;
}
```

sys_io_setup 函数的实现比较简单，首先调用 ioctx_alloc 申请一个异步 IO 上下文对象，然后把异步 IO 上下文对象的标识符返回给调用者。

所以，sys_io_setup 函数的核心过程是调用 ioctx_alloc 函数，我们继续分析 ioctx_alloc 函数的实现：

```text
static struct kioctx *ioctx_alloc(unsigned nr_events)
{
    struct mm_struct *mm;
    struct kioctx *ctx;
    ...
    ctx = kmem_cache_alloc(kioctx_cachep, GFP_KERNEL); // 申请一个 kioctx 对象
    ...
    INIT_LIST_HEAD(&ctx->active_reqs);                 // 初始化异步 IO 操作队列
    ...
    if (aio_setup_ring(ctx) < 0)                       // 初始化环形缓冲区
        goto out_freectx;
    ...
    return ctx;
    ...
}
```

ioctx_alloc 函数主要完成以下工作：

- 调用 kmem_cache_alloc 函数向内核申请一个异步 IO 上下文对象。
- 初始化异步 IO 上下文各个成员变量，如初始化异步 IO 操作队列。
- 调用 aio_setup_ring 函数初始化环形缓冲区。

环形缓冲区初始化函数 aio_setup_ring 的实现有点小复杂，主要涉及内存管理的知识点，所以这里跳过这部分的分析。

### 3.2 **提交异步 IO 操作**

提交异步 IO 操作是通过 io_submit 函数完成的，io_submit 需要提供一个类型为 iocb 结构的数组，表示要进行的异步 IO 操作相关的信息，我们先来看看 iocb 结构的定义：

```text
struct iocb {
    __u64   aio_data;       // 用户自定义数据, 可用于标识IO操作或者设置回调函数
    ...
    __u16   aio_lio_opcode; // IO操作类型, 如读(IOCB_CMD_PREAD)或者写(IOCB_CMD_PWRITE)操作
    __s16   aio_reqprio;
    __u32   aio_fildes;     // 进行IO操作的文件句柄
    __u64   aio_buf;        // 进行IO操作的缓冲区(如写操作的话就是写到文件的数据)
    __u64   aio_nbytes;     // 缓冲区的大小
    __s64   aio_offset;     // IO操作的文件偏移量
    ...
};
```

io_submit 函数最终会调用内核函数 sys_io_submit 来实现提供异步 IO 操作，我们来分析 sys_io_submit 函数的实现：

```text
asmlinkage long
sys_io_submit(aio_context_t ctx_id, long nr, 
              struct iocb __user **iocbpp)
{
    struct kioctx *ctx;
    long ret = 0;
    int i;
    ...
    ctx = lookup_ioctx(ctx_id); // 通过异步IO上下文标识符获取异步IO上下文对象
    ...
    for (i = 0; i < nr; i++) {
        struct iocb __user *user_iocb;
        struct iocb tmp;

        if (unlikely(__get_user(user_iocb, iocbpp+i))) {
            ret = -EFAULT;
            break;
        }

        // 从用户空间复制异步IO操作到内核空间
        if (unlikely(copy_from_user(&tmp, user_iocb, sizeof(tmp)))) {
            ret = -EFAULT;
            break;
        }

        // 调用 io_submit_one 函数提交异步IO操作
        ret = io_submit_one(ctx, user_iocb, &tmp);
        if (ret)
            break;
    }

    put_ioctx(ctx);
    return i ? i : ret;
}
```

sys_io_submit 函数的实现比较简单，主要从用户空间复制异步 IO 操作信息到内核空间，然后调用 io_submit_one 函数提交异步 IO 操作。我们重点分析 io_submit_one 函数的实现：

```text
int io_submit_one(struct kioctx *ctx, 
                  struct iocb __user *user_iocb,
                  struct iocb *iocb)
{
    struct kiocb *req;
    struct file *file;
    ssize_t ret;
    char *buf;
    ...
    file = fget(iocb->aio_fildes);      // 通过文件句柄获取文件对象
    ...
    req = aio_get_req(ctx);             // 获取一个异步IO操作对象
    ...
    req->ki_filp = file;                // 要进行异步IO的文件对象
    req->ki_user_obj = user_iocb;       // 指向用户空间的iocb对象
    req->ki_user_data = iocb->aio_data; // 设置用户自定义数据
    req->ki_pos = iocb->aio_offset;     // 设置异步IO操作的文件偏移量

    buf = (char *)(unsigned long)iocb->aio_buf; // 要进行异步IO操作的数据缓冲区

    // 根据不同的异步IO操作类型来进行不同的处理
    switch (iocb->aio_lio_opcode) {
    case IOCB_CMD_PREAD: // 异步读操作
        ...
        ret = -EINVAL;
        // 发起异步IO操作, 会根据不同的文件系统调用不同的函数:
        // 如ext3文件系统会调用 generic_file_aio_read 函数
        if (file->f_op->aio_read)
            ret = file->f_op->aio_read(req, buf, iocb->aio_nbytes, req->ki_pos);
        break;
    ...
    }
    ...
    // 异步IO操作或许会在调用 aio_read 时已经完成, 或者会被添加到IO请求队列中。
    // 所以, 如果异步IO操作被提交到IO请求队列中, 直接返回
    if (likely(-EIOCBQUEUED == ret)) return 0;

    aio_complete(req, ret, 0); // 如果IO操作已经完成, 调用 aio_complete 函数完成收尾工作
    return 0;
}
```

上面代码已经对 io_submit_one 函数进行了详细的注释，这里总结一下 io_submit_one 函数主要完成的工作：

- 通过调用 fget 函数获取文件句柄对应的文件对象。
- 调用 aio_get_req 函数获取一个类型为 kiocb 结构的异步 IO 操作对象，这个结构前面已经分析过。另外，aio_get_req 函数还会把异步 IO 操作对象添加到异步 IO 上下文的 active_reqs 队列中。
- 根据不同的异步 IO 操作类型来进行不同的处理，如 异步读操作 会调用文件对象的 aio_read 方法来进行处理。不同的文件系统，其 aio_read 方法的实现不一样，如 Ext3 文件系统的 aio_read 方法会指向 generic_file_aio_read 函数。
- 如果异步 IO 操作被添加到内核的 IO 请求队列中，那么就直接返回。否则就代表 IO 操作已经完成，那么就调用 aio_complete 函数完成收尾工作。

io_submit_one 函数的操作过程如图所示：



![img](https://pic2.zhimg.com/80/v2-6716a020070e9dc47ac324124ee1e5b5_720w.webp)



所以，io_submit_one 函数的主要任务就是向内核提交 IO 请求。

### 3.3 异步 IO 操作完成

当异步 IO 操作完成后，内核会调用 aio_complete 函数来把处理结果放进异步 IO 上下文的环形缓冲区 ring_info 中，我们来分析一下 aio_complete 函数的实现：

```text
int aio_complete(struct kiocb *iocb, long res, long res2)
{
    struct kioctx *ctx = iocb->ki_ctx;
    struct aio_ring_info *info;
    struct aio_ring *ring;
    struct io_event *event;
    unsigned long flags;
    unsigned long tail;
    int ret;
    ...
    info = &ctx->ring_info; // 环形缓冲区对象

    spin_lock_irqsave(&ctx->ctx_lock, flags);         // 对异步IO上下文进行上锁
    ring = kmap_atomic(info->ring_pages[0], KM_IRQ1); // 对内存页进行虚拟内存地址映射

    tail = info->tail;                           // 环形缓冲区下一个空闲的位置
    event = aio_ring_event(info, tail, KM_IRQ0); // 从环形缓冲区获取空闲的位置保存结果
    tail = (tail + 1) % info->nr;                // 更新下一个空闲的位置

    // 保存异步IO结果到环形缓冲区中
    event->obj = (u64)(unsigned long)iocb->ki_user_obj;
    event->data = iocb->ki_user_data;
    event->res = res;
    event->res2 = res2;
    ...
    info->tail = tail;
    ring->tail = tail; // 更新环形缓冲区下一个空闲的位置

    put_aio_ring_event(event, KM_IRQ0); // 解除虚拟内存地址映射
    kunmap_atomic(ring, KM_IRQ1);       // 解除虚拟内存地址映射

    // 释放异步IO对象
    ret = __aio_put_req(ctx, iocb);
    spin_unlock_irqrestore(&ctx->ctx_lock, flags);
    ...
    return ret;
}
```

aio_complete 函数的 iocb 参数是我们通过调用 io_submit_once 函数提交的异步 IO 对象，而参数 res 和 res2 是用内核进行 IO 操作完成后返回的结果。

aio_complete 函数的主要工作如下：

- 根据环形缓冲区的 tail 指针获取一个空闲的 io_event 对象来保存 IO 操作的结果。
- 对环形缓冲区的 tail 指针进行加一操作，指向下一个空闲的位置。

当把异步 IO 操作的结果保存到环形缓冲区后，用户层就可以通过调用 io_getevents 函数来读取 IO 操作的结果，io_getevents 函数最终会调用 sys_io_getevents 函数。

我们来分析 sys_io_getevents 函数的实现：

```text
asmlinkage long sys_io_getevents(aio_context_t ctx_id,
                                 long min_nr,
                                 long nr,
                                 struct io_event *events,
                                 struct timespec *timeout)
{
    struct kioctx *ioctx = lookup_ioctx(ctx_id);
    long ret = -EINVAL;
    ...
    if (likely(NULL != ioctx)) {
        // 调用 read_events 函数读取IO操作的结果
        ret = read_events(ioctx, min_nr, nr, events, timeout);
        put_ioctx(ioctx);
    }
    return ret;
}
```

从上面的代码可以看出，sys_io_getevents 函数主要调用 read_events 函数来读取异步 IO 操作的结果，我们接着分析 read_events 函数：

```text
static int read_events(struct kioctx *ctx,
                      long min_nr, long nr,
                      struct io_event *event,
                      struct timespec *timeout)
{
    long start_jiffies = jiffies;
    struct task_struct *tsk = current;
    DECLARE_WAITQUEUE(wait, tsk);
    int ret;
    int i = 0;
    struct io_event ent;
    struct timeout to;

    memset(&ent, 0, sizeof(ent));
    ret = 0;

    while (likely(i < nr)) {
        ret = aio_read_evt(ctx, &ent); // 从环形缓冲区中读取一个IO处理结果
        if (unlikely(ret <= 0))        // 如果环形缓冲区没有IO处理结果, 退出循环
            break;

        ret = -EFAULT;
        // 把IO处理结果复制到用户空间
        if (unlikely(copy_to_user(event, &ent, sizeof(ent)))) {
            break;
        }

        ret = 0;
        event++;
        i++;
    }

    if (min_nr <= i)
        return i;
    if (ret)
        return ret;
    ...
}
```

read_events 函数主要还是调用 aio_read_evt 函数来从环形缓冲区中读取异步 IO 操作的结果，如果读取成功，就把结果复制到用户空间中。

aio_read_evt 函数是从环形缓冲区中读取异步 IO 操作的结果，其实现如下：

```text
static int aio_read_evt(struct kioctx *ioctx, struct io_event *ent)
{
    struct aio_ring_info *info = &ioctx->ring_info;
    struct aio_ring *ring;
    unsigned long head;
    int ret = 0;

    ring = kmap_atomic(info->ring_pages[0], KM_USER0);

    // 如果环形缓冲区的head指针与tail指针相等, 代表环形缓冲区为空, 所以直接返回
    if (ring->head == ring->tail) 
        goto out;

    spin_lock(&info->ring_lock);

    head = ring->head % info->nr;
    if (head != ring->tail) {
        // 根据环形缓冲区的head指针从环形缓冲区中读取结果
        struct io_event *evp = aio_ring_event(info, head, KM_USER1);

        *ent = *evp;                  // 将结果保存到ent参数中
        head = (head + 1) % info->nr; // 移动环形缓冲区的head指针到下一个位置
        ring->head = head;            // 保存环形缓冲区的head指针
        ret = 1;
        put_aio_ring_event(evp, KM_USER1);
    }

    spin_unlock(&info->ring_lock);

out:
    kunmap_atomic(ring, KM_USER0);
    return ret;
}
```

aio_read_evt 函数的主要工作就是判断环形缓冲区是否为空，如果不为空就从环形缓冲区中读取异步 IO 操作的结果，并且保存到参数 ent 中，并且移动环形缓冲区的 head 指针到下一个位置。

## 4.总结

本文主要分析了 Linux 原生 AIO 的原理及实现，但为了不陷入太多的实现细节中，本文并没有涉及到磁盘 IO 相关的知识点。然而磁盘 IO 也是 AIO 实现中不可或缺的一部分，所以有兴趣的朋友可以通过阅读 Linux 的源码来分析其实现原理。

原文地址：https://zhuanlan.zhihu.com/p/364819119

作者：linux



# 【NO.431】如何减少频繁分配内存（malloc或new）造成的内存碎片

高性能之内存池（频繁使用malloc和new会降低性能）

内存池(Memory Pool)是一种内存分配方式。通常我们习惯直接使用new、malloc等API申请分配内存，这样做的缺点在于：由于所申请内存块的大小不定，当频繁使用时会造成大量的内存碎片并进而降低性能。内存池则是在真正使用内存之前，先申请分配一定数量的、大小相等(一般情况下)的内存块留作备用。当有新的内存需求时，就从内存池中分出一部分内存块，若内存块不够再继续申请新的内存。这样做的一个显著优点是尽量避免了内存碎片，使得内存分配效率得到提升。
（1）针对特殊情况，例如需要频繁分配释放固定大小的内存对象时，不需要复杂的分配算法和多线程保护。也不需要维护内存空闲表的额外开销，从而获得较高的性能。

（2）由于开辟一定数量的连续内存空间作为内存池块，因而一定程度上提高了程序局部性，提升了程序性能。

（3）比较容易控制页边界对齐和内存字节对齐，没有内存碎片的问题。

（4）当需要分配管理的内存在100M一下的时候，采用内存池会节省大量的时间，否则会耗费更多的时间。

（5）内存池可以防止更多的内存碎片的产生

（6）更方便于管理内存

利用C/C++开发大型应用程序中，内存的管理与分配是一个需要认真考虑的部分。

本文描述了内存池设计原理并给出内存池的实现代码。

内存池设计过程中需要考虑好内存的分配与释放问题，其实也就是空间和时间的矛盾。

有的内存池设计得很巧妙，内存分配与需求相当，但是会浪费过多的时间去查找分配与释放，这就得不偿失；

实际使用中，我们更多的是关心内存分配的速度，而不是内存的使用效率。基于此，本文按照如下思想设计实现内存池。

主要包含三个结构：StiaticMemory, MemoryChunk和MemoryBlock，三者之间的关系如下图所示：

![img](https://pic4.zhimg.com/80/v2-0a23bcbbb3b713ecc7fe9ae7886ec8ff_720w.webp)

1.内存的分配：

（1）如果分配大小超过1024，直接采用malloc分配，分配的时候多分配sizeof(size_t)字节，用于保存该块的大小；

（2）否则根据分配大小，查找到容纳该大小的最小size的MemoryChunk；

（3）查找MemoryChunk的链表指针pList，找到空闲的MemoryBlock返回；

（4）如果pList为NULL，临时创建MemoryBlock返回；

（5）MemoryBlock头部包含两个成员，pChunk指向的所属的MemoryChunk对象，size表明大小，其后才是给用户使用的空间；

2.内存的释放：

（1）根据释放的指针，查找器size头部，即减去sizeof(size_t)字节，判断该块的大小；

（2）如果大小超过1024，直接free;

（3）否则交给MemoryChunk处理，而块的头部保存了该指针，因此直接利用该指针就可以收回该内存。

注意的问题：

上述设计的内存池通过冗余的头部来实现内存块的分配与释放，减少了内存池的操作时间，速度上要优于原始的malloc和free操作，同时减少了内存碎片的增加。

但是该设计中没有去验证释放的块冗余头部的正确性，因此故意释放不属于内存池中的块或者修改头部信息都会导致内存池操作失败，当然这些可以由程序员来控制。

此外，内存池中分配出去的内存块如果不主动释放，内存池没有保留信息，不会自动释放，但是在退出的时候会验证验证是否完全释放，其实这个在系统测试时候就可以检测出来，我想这个缺陷也是可以弥补的，在此提出，希望使用者注意。

下面贴上源码。

MemoryChunk.h 文件，线程安全

```text
#ifndef MEMORY_CHUNK_H  
#define MEMORY_CHUNK_H  
#include <cstdio>  
#include <cassert>  
#include <cstdlib>  
  
#ifdef WIN32  
#include <windows.h>  
typedef CRITICAL_SECTION MUTEXTYPE;  
#define INITMUTEX(hMutex) InitializeCriticalSection(&hMutex)  
#define DELMUTEX(hMutex) DeleteCriticalSection(&hMutex)  
#define LOCK(hMutex) EnterCriticalSection(&hMutex)  
#define UNLOCK(hMutex) LeaveCriticalSection(&hMutex)  
#else  
#include <pthread.h>  
typedef pthread_mutex_t MUTEXTYPE;  
#define INITMUTEX(hMutex) pthread_mutex_init(&hMutex,NULL)  
#define DELMUTEX(hMutex) pthread_mutex_destroy(&hMutex)  
#define LOCK(hMutex) pthread_mutex_lock(&hMutex)  
#define UNLOCK(hMutex) pthread_mutex_unlock(&hMutex)  
#endif  
  
class MemoryChunk;  
  
/** @struct MemoryBlock  
 *  
 */  
struct BlockHeader  
{  
    MemoryChunk* pChunk;  
    size_t len;  
};  
struct MemoryBlock;  
struct BlockData  
{  
    union{  
        MemoryBlock* pNext;  
        char pBuffer;  
    };  
};  
struct MemoryBlock  
{  
    BlockHeader header;  
    BlockData data;  
};  
  
/** @class MemoryChunk  
 *  
 */  
  
class MemoryChunk  
{  
public:  
    MemoryChunk(size_t size, int count)  
    {  
        INITMUTEX(hMutex);  
        this->pFreeList=NULL;  
        this->size=size;  
        this->count=0;  
        MemoryBlock* pBlock;  
        while(count--){  
            pBlock=CreateBlock();  
            if(!pBlock)break;  
            pBlock->data.pNext=pFreeList;  
            pFreeList=pBlock;  
        }  
    }  
    ~MemoryChunk()  
    {  
        int tempcount=0;  
        MemoryBlock* pBlock;  
        while(pBlock=pFreeList){  
            pFreeList=pBlock->data.pNext;  
            DeleteBlock(pBlock);  
            ++tempcount;  
        }  
        assert(tempcount==count);//!确保释放完全  
        DELMUTEX(hMutex);  
    }  
    void* malloc()  
    {  
        MemoryBlock* pBlock;  
        LOCK(hMutex);  
        if(pFreeList){  
            pBlock=pFreeList;  
            pFreeList=pBlock->data.pNext;  
        }  
        else{  
            if(!(pBlock=CreateBlock())){  
                UNLOCK(hMutex);  
                return NULL;  
            }  
        }  
        UNLOCK(hMutex);  
        return &pBlock->data.pBuffer;  
    }  
    static void free(void* pMem)  
    {  
        MemoryBlock* pBlock=(MemoryBlock*)((char*)pMem-sizeof(BlockHeader));  
        pBlock->header.pChunk->free(pBlock);  
    }  
    void free(MemoryBlock* pBlock)  
    {  
        LOCK(hMutex);  
        pBlock->data.pNext=pFreeList;  
        pFreeList=pBlock;  
        UNLOCK(hMutex);  
    }  
      
    MemoryChunk* Next(){return pNext;}  
  
protected:  
    MemoryBlock* CreateBlock()  
    {  
        MemoryBlock* pBlock=(MemoryBlock*)::malloc(sizeof(BlockHeader)+size);  
  
        if(pBlock){  
  
            pBlock->header.pChunk=this;  
            pBlock->header.len=size;  
              
            ++count;  
        }  
        return pBlock;  
    }  
    void DeleteBlock(MemoryBlock* pBlock)  
    {  
        ::free(pBlock);  
    }  
private:  
    MemoryBlock* pFreeList;  
    size_t size;//!Block大小  
    int count;//!Block数目  
    MemoryChunk* pNext;  
    MUTEXTYPE hMutex;  
};  
#endif  
```

StaticMemory.h文件，内存池对象

```text
#ifndef STATIC_MEMORY_H  
#define STATIC_MEMORY_H  
#include "MemoryChunk.h"  
/** @ StaticMemory.h  
 * 定义实现内存池  
 * 采用固定大小策略进行内存管理与分配  
 * 减少因大量小内存分配导致的内存碎片增加  
 */  
struct HeapHeader  
{  
    size_t size;  
};  
struct MemoryHeap  
{  
    HeapHeader header;  
    char pBuffer;  
};  
  
class StaticMemory  
{  
public:  
    typedef enum{MAX_SIZE=1024,MIN_SIZE=sizeof(MemoryChunk*)};  
    StaticMemory()  
    {  
        chunkcount=0;  
        for(size_t size=MIN_SIZE; size<=MAX_SIZE; size*=2)++chunkcount;  
        //pChunkList=(MemoryChunk**)malloc(sizeof(MemoryChunk*)*chunkcount);  
        pChunkList=new MemoryChunk*[chunkcount];  
        int index=0;  
        for(size_t size=MIN_SIZE; size<=MAX_SIZE; size*=2)  
        {  
            pChunkList[index++]=new MemoryChunk(size,1000);  
        }  
    }  
    ~StaticMemory()  
    {  
        for(int index=0; index<chunkcount; ++index)  
        {  
            delete pChunkList[index];  
        }  
        //free(pChunkList);  
        delete[] pChunkList;  
    }  
    void* Malloc(size_t size)  
    {  
        if(size>MAX_SIZE){  
            return malloc(size);  
        }  
        int index=0;  
        for(size_t tsize=MIN_SIZE; tsize<=MAX_SIZE; tsize*=2){  
            if(tsize>=size)break;  
            ++index;  
        }  
        return pChunkList[index]->malloc();  
    }  
    void Free(void* pMem)  
    {  
        if(!free(pMem))MemoryChunk::free(pMem);  
    }  
protected:  
    void* malloc(size_t size)  
    {  
        MemoryHeap* pHeap=(MemoryHeap*)::malloc(sizeof(HeapHeader)+size);  
        if(pHeap){  
            pHeap->header.size=size;  
            return &pHeap->pBuffer;  
        }  
        return NULL;  
    }  
    bool free(void* pMem)  
    {  
        MemoryHeap* pHeap=(MemoryHeap*)((char*)pMem-sizeof(HeapHeader));  
        if(pHeap->header.size>MAX_SIZE){  
            ::free(pHeap);  
            return true;  
        }  
        return false;  
    }  
private:  
    MemoryChunk** pChunkList;  
    int chunkcount;  
};  
#endif  
```

ObejctManager.h文件，用于实现对象的创建与管理，比较简易。

```text
#ifndef OBJECT_MANAGER_H  
#define OBJECT_MANAGER_H  
#include "StaticMemory.h"  
/** @class ObjectManager  
 * 实现利用内存池创建对象  
 * 要求对象具有缺省构造函数  
 */  
template<typename T>  
class ObjectManager  
{  
public:  
    typedef T ObjectType;  
  
    static ObjectType* Create(StaticMemory* pool)  
    {  
        void* pobject=pool->Malloc(sizeof(T));  
        new(pobject) ObjectType();  
        return static_cast<ObjectType*>(pobject);  
    }  
    static void Delete(StaticMemory* pool, ObjectType* pobject)  
    {  
        pobject->~ObjectType();  
        pool->Free(pobject);  
    }  
};  
#endif  
```

测试结果：

分单线程和多线程进行测试，重复的内存分配与释放在实际使用中是不太可能的，为了模拟实际使用，通过随机数来确定分配内存大小，同时也通过随机数来确定分配与释放操作。在测试过程中限制最大分配大小为1024，目的是为了测试小内存块的分配情况对比。

内存池单线程测试结果

![img](https://pic3.zhimg.com/80/v2-fe9bf6eadbdbe47ddb72a26394c9d156_720w.webp)

内存池多线程测试结果

![img](https://pic4.zhimg.com/80/v2-f99b1559b4bb1efd3e259394699df237_720w.webp)

进行多线程测试主要是测试多线程运行下，加锁给内存分配带来的影响，因此为了排除CPU的影响，测试采用的机器为16盒，16G内存的Linux服务器。

具体配置如下：

Intel(R) Xeon(R) CPU E5630 @ 2.53GHz

stepping : 2
cpu MHz : 2527.084

cache size : 12288 KB

原文地址：https://zhuanlan.zhihu.com/p/375447849

作者：linux

# 【NO.432】深入理解Linux 的Page Cache

## **1.Page Cache**

### **1.1 Page Cache 是什么？**

为了理解 Page Cache，我们不妨先看一下 Linux 的文件 I/O 系统，如下图所示：

![img](https://pic2.zhimg.com/80/v2-1fff8af2d8099698f96977884087d251_720w.webp)

> Figure1. Linux 文件 I/O 系统

上图中，红色部分为 Page Cache。可见 Page Cache 的本质是由 Linux 内核管理的内存区域。我们通过 mmap 以及 buffered I/O 将文件读取到内存空间实际上都是读取到 Page Cache 中。

### **1.2 如何查看系统的 Page Cache？**

通过读取 `/proc/meminfo` 文件，能够实时获取系统内存情况：

```text
$ cat /proc/meminfo
...
Buffers:            1224 kB
Cached:           111472 kB
SwapCached:        36364 kB
Active:          6224232 kB
Inactive:         979432 kB
Active(anon):    6173036 kB
Inactive(anon):   927932 kB
Active(file):      51196 kB
Inactive(file):    51500 kB
...
Shmem:             10000 kB
...
SReclaimable:      43532 kB
...
```

根据上面的数据，你可以简单得出这样的公式（等式两边之和都是 112696 KB）：

```text
Buffers + Cached + SwapCached = Active(file) + Inactive(file) + Shmem + SwapCached
```

两边等式都是 Page Cache，即：

```text
Page Cache = Buffers + Cached + SwapCached
```

通过阅读 1.4 以及 1.5 小节，就能够理解为什么 SwapCached 与 Buffers 也是 Page Cache 的一部分。

内核计算源码（linux 2.6.19）：

![img](https://pic3.zhimg.com/80/v2-8fcd958af4f30767a007130ec3c42902_720w.webp)

内核算法：Cached = files - SwapCached - Buffers；

Buffers + Cached + SwapCached = Active(file) + Inactive(file) + Shmem + SwapCached

公式推出来的

Cached = Active(file) + Inactive(file) + Shmem - Buffers ；

由此可见，这个Cached 并不等于Active(file) + Inactive(file) ；

这个cache包含很多 ：

1. 含有普通文件数据的页‘；
2. 含有目录的页；
3. 含有直接从块设备文件(跳过文件系统)读出的数据的页；
4. 含有用户态进程数据的页；
5. 属于特殊文件系统文件的页，如shm；

### **1.3 page 与 Page Cache**

page 是内存管理分配的基本单位， Page Cache 由多个 page 构成。page 在操作系统中通常为 4KB 大小（32bits/64bits），而 Page Cache 的大小则为 4KB 的整数倍。

**另一方面，并不是所有 page 都被组织为 Page Cache**。

Linux 系统上供用户可访问的内存分为两个类型[2]，即：

- File-backed pages：文件备份页也就是 Page Cache 中的 page，对应于磁盘上的若干数据块；对于这些页最大的问题是脏页回盘；
- Anonymous pages：匿名页不对应磁盘上的任何磁盘数据块，它们是进程的运行是内存空间（例如方法栈、局部变量表等属性）；

**为什么 Linux 不把 Page Cache 称为 block cache，这不是更好吗？**

这是因为从磁盘中加载到内存的数据不仅仅放在 Page Cache 中，还放在 buffer cache 中。例如通过 Direct I/O 技术的磁盘文件就不会进入 Page Cache 中。当然，这个问题也有 Linux 历史设计的原因，毕竟这只是一个称呼，含义随着 Linux 系统的演进也逐渐不同。

下面比较一下 File-backed pages 与 Anonymous pages 在 Swap 机制下的性能。

内存是一种珍惜资源，当内存不够用时，内存管理单元（Memory Mangament Unit）需要提供调度算法来回收相关内存空间。内存空间回收的方式通常就是 swap，即交换到持久化存储设备上。

File-backed pages（Page Cache）的内存回收代价较低。Page Cache 通常对应于一个文件上的若干顺序块，因此可以通过顺序 I/O 的方式落盘。另一方面，如果 Page Cache 上没有进行写操作（所谓的没有脏页），甚至不会将 Page Cache 回盘，因为数据的内容完全可以通过再次读取磁盘文件得到。

Page Cache 的主要难点在于脏页回盘，这个内容会在第二节进行详细说明。

Anonymous pages 的内存回收代价较高。这是因为 Anonymous pages 通常随机地写入持久化交换设备。另一方面，无论是否有更操作，为了确保数据不丢失，Anonymous pages 在 swap 时必须持久化到磁盘。

### **1.4 Swap 与缺页中断**

Swap 机制指的是当物理内存不够用，内存管理单元（Memory Mangament Unit）需要提供调度算法来回收相关内存空间，然后将清理出来的内存空间给当前内存申请方。

Swap 机制存在的本质原因是 Linux 系统提供了虚拟内存管理机制，每一个进程认为其独占内存空间，因此所有进程的内存空间之和远远大于物理内存。所有进程的内存空间之和超过物理内存的部分就需要交换到磁盘上。

操作系统以 page 为单位管理内存，当进程发现需要访问的数据不在内存时，操作系统可能会将数据以页的方式加载到内存中。上述过程被称为缺页中断，当操作系统发生缺页中断时，就会通过系统调用将 page 再次读到内存中。

但主内存的空间是有限的，当主内存中不包含可以使用的空间时，操作系统会从选择合适的物理内存页驱逐回磁盘，为新的内存页让出位置，**选择待驱逐页的过程在操作系统中叫做页面替换（Page Replacement）**，替换操作又会触发 swap 机制。

如果物理内存足够大，那么可能不需要 Swap 机制，但是 Swap 在这种情况下还是有一定优势：对于有发生内存泄漏几率的应用程序（进程），Swap 交换分区更是重要，这可以确保内存泄露不至于导致物理内存不够用，最终导致系统崩溃。但内存泄露会引起频繁的 swap，此时非常影响操作系统的性能。

Linux 通过一个 swappiness 参数来控制 Swap 机制[2]：这个参数值可为 0-100，控制系统 swap 的优先级：

- 高数值：较高频率的 swap，进程不活跃时主动将其转换出物理内存。
- 低数值：较低频率的 swap，这可以确保交互式不因为内存空间频繁地交换到磁盘而提高响应延迟。

**最后，为什么 Buffers 也是 Page Cache 的一部分？**

这是因为当匿名页（Inactive(anon) 以及 Active(anon)）先被交换（swap out）到磁盘上后，然后再加载回（swap in）内存中，由于读入到内存后原来的 Swap File 还在，所以 SwapCached 也可以认为是 File-backed page，即属于 Page Cache。这个过程如 Figure 2 所示。

![img](https://pic4.zhimg.com/80/v2-45dd833446be8b4b94ea842c409be457_720w.webp)

> Figure2. 匿名页的被交换后也是 Page Cache

### **1.5 Page Cache 与 buffer cache**

执行 free 命令，注意到会有两列名为 buffers 和 cached，也有一行名为 “-/+ buffers/cache”。

```text
~ free -m
             total       used       free     shared    buffers     cached
Mem:        128956      96440      32515          0       5368      39900
-/+ buffers/cache:      51172      77784
Swap:        16002          0      16001
```

其中，cached 列表示当前的页缓存（Page Cache）占用量，buffers 列表示当前的块缓存（buffer cache）占用量。用一句话来解释：**Page Cache 用于缓存文件的页数据，buffer cache 用于缓存块设备（如磁盘）的块数据。**页是逻辑上的概念，因此 Page Cache 是与文件系统同级的；块是物理上的概念，因此 buffer cache 是与块设备驱动程序同级的。

Page Cache）占用量，buffers 列表示当前的块缓存（buffer cache）占用量。用一句话来解释：**Page Cache 用于缓存文件的页数据，buffer cache 用于缓存块设备（如磁盘）的块数据。**页是逻辑上的概念，因此 Page Cache 是与文件系统同级的；块是物理上的概念，因此 buffer cache 是与块设备驱动程序同级的。

Page Cache 与 buffer cache 的共同目的都是加速数据 I/O：写数据时首先写到缓存，将写入的页标记为 dirty，然后向外部存储 flush，也就是缓存写机制中的 write-back（另一种是 write-through，Linux 默认情况下不采用）；读数据时首先读取缓存，如果未命中，再去外部存储读取，并且将读取来的数据也加入缓存。操作系统总是积极地将所有空闲内存都用作 Page Cache 和 buffer cache，当内存不够用时也会用 LRU 等算法淘汰缓存页。

在 Linux 2.4 版本的内核之前，Page Cache 与 buffer cache 是完全分离的。但是，块设备大多是磁盘，磁盘上的数据又大多通过文件系统来组织，这种设计导致很多数据被缓存了两次，浪费内存。**所以在 2.4 版本内核之后，两块缓存近似融合在了一起：如果一个文件的页加载到了 Page Cache，那么同时 buffer cache 只需要维护块指向页的指针就可以了**。只有那些没有文件表示的块，或者绕过了文件系统直接操作（如dd命令）的块，才会真正放到 buffer cache 里。因此，我们现在提起 Page Cache，基本上都同时指 Page Cache 和 buffer cache 两者，本文之后也不再区分，直接统称为 Page Cache。

下图近似地示出 32-bit Linux 系统中可能的一种 Page Cache 结构，其中 block size 大小为 1KB，page size 大小为 4KB。

![img](https://pic4.zhimg.com/80/v2-07387a93aafd63fd84d5c5c8307067b3_720w.webp)

Page Cache 中的每个文件都是一棵基数树（radix tree，本质上是多叉搜索树），树的每个节点都是一个页。根据文件内的偏移量就可以快速定位到所在的页，如下图所示。关于基数树的原理可以参见英文维基，这里就不细说了。

![img](https://pic1.zhimg.com/80/v2-1f66bb107ad132267476b3d3167ad7f4_720w.webp)

### **1.6 Page Cache 与预读** 

操作系统为基于 Page Cache 的读缓存机制提供预读机制（PAGE_READAHEAD），一个例子是：

- 用户线程仅仅请求读取磁盘上文件 A 的 offset 为 0-3KB 范围内的数据，由于磁盘的基本读写单位为 block（4KB），于是操作系统至少会读 0-4KB 的内容，这恰好可以在一个 page 中装下。
- 但是操作系统出于局部性原理[3]会选择将磁盘块 offset [4KB,8KB)、[8KB,12KB) 以及 [12KB,16KB) 都加载到内存，于是额外在内存中申请了 3 个 page；

下图代表了操作系统的预读机制：

![img](https://pic2.zhimg.com/80/v2-170fb78bc43c47249468b78eebc83875_720w.webp)

操作系统的预读机制

上图中，应用程序利用 read 系统调动读取 4KB 数据，实际上内核使用 readahead 机制完成了 16KB 数据的读取。

## **2.Page Cache 与文件持久化的一致性&可靠性**

现代 Linux 的 Page Cache 正如其名，是对磁盘上 page（页）的内存缓存，同时可以用于读/写操作。一切内存缓存都存在一致性问题：内存中的数据与磁盘中的数据不一致，例如用作分布式中间件缓存的 Redis 就与 MySQL 等数据库中的数据存在不一致。

Linux 提供多种机制来保证数据一致性，但无论是单机上的内存与磁盘一致性，还是分布式组件中节点 1 与节点 2 、节点 3 的数据一致性问题，理解的关键是 trade-off：吞吐量与数据一致性保证是一对矛盾。

首先，需要我们理解一下文件的数据。**文件 = 数据 + 元数据**。元数据用来描述文件的各种属性，也必须存储在磁盘上。因此，我们说保证文件一致性其实包含了两个方面：数据一致+元数据一致。

> 文件的元数据包括：文件大小、创建时间、访问时间、属主属组等信息。

我们考虑如下一致性问题：如果发生写操作并且对应的数据在 Page Cache 中，那么写操作就会直接作用于 Page Cache 中，此时如果数据还没刷新到磁盘，那么内存中的数据就领先于磁盘，此时对应 page 就被称为 Dirty page。

当前 Linux 下以两种方式实现文件一致性：

1. **Write Through（写穿）**：向用户层提供特定接口，应用程序可主动调用接口来保证文件一致性；
2. **Write back（写回）**：系统中存在定期任务（表现形式为内核线程），周期性地同步文件系统中文件脏数据块，这是默认的 Linux 一致性方案；

上述两种方式最终都依赖于系统调用，主要分为如下三种系统调用：

![img](https://pic3.zhimg.com/80/v2-b93833fcea11f844c7657c78f49b9dea_720w.webp)

上述三种系统调用可以分别由用户进程与内核进程发起。下面我们研究一下内核线程的相关特性。

1. 创建的针对回写任务的内核线程数由系统中持久存储设备决定，为每个存储设备创建单独的刷新线程；

2. 关于多线程的架构问题，Linux 内核采取了 Lighthttp 的做法，即系统中存在一个管理线程和多个刷新线程（每个持久存储设备对应一个刷新线程）。管理线程监控设备上的脏页面情况，若设备一段时间内没有产生脏页面，就销毁设备上的刷新线程；若监测到设备上有脏页面需要回写且尚未为该设备创建刷新线程，那么创建刷新线程处理脏页面回写。而刷新线程的任务较为单调，只负责将设备中的脏页面回写至持久存储设备中。

3. 刷新线程刷新设备上脏页面大致设计如下：

4. 1. 每个设备保存脏文件链表，保存的是该设备上存储的脏文件的 inode 节点。所谓的回写文件脏页面即回写该 inode 链表上的某些文件的脏页面；
   2. 系统中存在多个回写时机，第一是应用程序主动调用回写接口（fsync，fdatasync 以及 sync 等），第二管理线程周期性地唤醒设备上的回写线程进行回写，第三是某些应用程序/内核任务发现内存不足时要回收部分缓存页面而事先进行脏页面回写，设计一个统一的框架来管理这些回写任务非常有必要。

Write Through 与 Write back 在持久化的可靠性上有所不同：

- Write Through 以牺牲系统 I/O 吞吐量作为代价，向上层应用确保一旦写入，数据就已经落盘，不会丢失；
- Write back 在系统发生宕机的情况下无法确保数据已经落盘，因此存在数据丢失的问题。不过，在程序挂了，例如被 kill -9，Page Cache 中的数据操作系统还是会确保落盘；

## **3.为什么使用 Page Cache 与为什么不使用 Page Cache?**

### **3.1 Page Cache 的优势**

**1.加快数据访问**

如果数据能够在内存中进行缓存，那么下一次访问就不需要通过磁盘 I/O 了，直接命中内存缓存即可。

由于内存访问比磁盘访问快很多，因此加快数据访问是 Page Cache 的一大优势。

**2.减少 I/O 次数，提高系统磁盘 I/O 吞吐量**

得益于 Page Cache 的缓存以及预读能力，而程序又往往符合局部性原理，因此通过一次 I/O 将多个 page 装入 Page Cache 能够减少磁盘 I/O 次数， 进而提高系统磁盘 I/O 吞吐量。

### **3.2 Page Cache 的劣势**

page cache 也有其劣势，最直接的缺点是需要占用额外物理内存空间，物理内存在比较紧俏的时候可能会导致频繁的 swap 操作，最终导致系统的磁盘 I/O 负载的上升。

Page Cache 的另一个缺陷是对于应用层并没有提供很好的管理 API，几乎是透明管理。应用层即使想优化 Page Cache 的使用策略也很难进行。因此一些应用选择在用户空间实现自己的 page 管理，例如 MySQL InnoDB 存储引擎以 16KB 的页进行管理。

Page Cache 最后一个缺陷是在某些应用场景下比 Direct I/O 多一次磁盘读 I/O 以及磁盘写 I/O。

原文地址：https://zhuanlan.zhihu.com/p/436313908

作者:linux

# 【NO.433】高并发高吞吐IO秘密武器——epoll池化技术

## 1.epoll函数详解

epoll是Linux特有的IO复用函数，使用一组函数来完成任务，而不是单个函数。

epoll把用户关心的文件描述符上的事件放在内核的一个事件表中，不需要像select、poll那样每次调用都要重复传入文件描述符集或事件集。

epoll需要使用一个额外的文件描述符，来唯一标识内核中的时间表，由epoll_create创建。

函数原型

```text
#include <sys/epoll.h>
 
    int epoll_create(int size);
    int epoll_create1(int flags);
 
    int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
 
    int epoll_wait(int epfd, struct epoll_event *events,
                int maxevents, int timeout);
    int epoll_pwait(int epfd, struct epoll_event *events,
                int maxevents, int timeout,
                const sigset_t *sigmask);
```

- epoll_create：创建一个epoll实例，size参数给内核一个提示，标识事件表的大小。函数返回的文件描述符将作用其他所有epoll系统调用的第一个参数，以指定要访问的内核事件表。
- epoll_ctl：操作文件描述符。fd表示要操作的文件描述符，op指定操作类型，event指定事件。
- epoll_wait：在一段超时时间内等待一组文件描述符上的事件。如果监测到事件，就将所有就绪的事件从内核事件表(epfd参数指定)中复制到第二个参数events指向的数组中。因为events数组只用于输出epoll_wait监测到的就绪事件，而不像select、poll那样就用于传入用户注册的事件，又用于输出内核检测到的就绪事件。这样极大提高了应用程序索引就绪文件描述符的效率。

**函数返回**

特别注意epoll_wait函数成功时返回就绪的文件描述符总数。select和poll返回文件描述符总数。

以寻找已经就绪的文件描述符，举个例子如下：

epoll_wait只需要遍历返回的文件描述符，但是poll和select需要遍历所有文件描述符

```text
//  poll
int ret = poll(fds, MAX_EVENT_NUMBER, -1);
// 必须遍历所有已注册的文件描述符
for (int i = 0; i < MAX_EVENT_NUMBER; i++) {
    if (fds[i].revents & POLLIN) {
        int sockfd = fds[i].fd;
    }
}
 
// epoll_wait
int ret = epoll_wait(epollfd, events, MAX_EVENT_NUMBER, -1);
// 仅需要遍历就绪的ret个文件描述符
for (int i = 0; i < ret; i++) {
    int sockfd = events[i].data.fd;
}
```

**LT水平触发模式和ET边沿触发模式**

epoll监控多个文件描述符的I/O事件。epoll支持边缘触发(edge trigger，ET)或水平触发（level trigger，LT)，通过epoll_wait等待I/O事件，如果当前没有可用的事件则阻塞调用线程。

select和poll只支持LT工作模式，epoll的默认的工作模式是LT模式。

水平触发：

- 当epoll_wait检测到其上有事件发生并将此事件通知应用程序后，应用程序可以不立即处理此事件。这样应用程序下一次调用epoll_wait的时候，epoll_wait还会再次向应用程序通告此事件，直到事件被处理。

边沿触发：

- 当epoll_wait检测到其上有事件发生并将此事件通知应用程序后，应用程序必须立即处理此事件，后续的epoll_wait调用将不再向应用程序通知这一事件。

所以，边沿触发模式很大程度上降低了同一个epoll事件被重复触发的次数，所以效率更高。

## 2.三组IO复用函数对比

\1. 用户态将文件描述符传入内核的方式

- select：创建3个文件描述符集并拷贝到内核中，分别监听读、写、异常动作。这里受到单个进程可以打开的fd数量限制，默认是1024。
- poll：将传入的struct pollfd结构体数组拷贝到内核中进行监听。
- epoll：执行epoll_create会在内核的高速cache区中建立一颗红黑树以及就绪链表(该链表存储已经就绪的文件描述符)。接着用户执行的epoll_ctl函数添加文件描述符会在红黑树上增加相应的结点。

\2. 内核态检测文件描述符读写状态的方式

- select：采用轮询方式，遍历所有fd，最后返回一个描述符读写操作是否就绪的mask掩码，根据这个掩码给fd_set赋值。
- poll：同样采用轮询方式，查询每个fd的状态，如果就绪则在等待队列中加入一项并继续遍历。
- epoll：采用回调机制。在执行epoll_ctl的add操作时，不仅将文件描述符放到红黑树上，而且也注册了回调函数，内核在检测到某文件描述符可读/可写时会调用回调函数，该回调函数将文件描述符放在就绪链表中。

\3. 找到就绪的文件描述符并传递给用户态的方式

- select：将之前传入的fd_set拷贝传出到用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。
- poll：将之前传入的fd数组拷贝传出用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。
- epoll：epoll_wait只用观察就绪链表中有无数据即可，最后将链表的数据返回给数组并返回就绪的数量。内核将就绪的文件描述符放在传入的数组中，所以只用遍历依次处理即可。

\4. 重复监听的处理方式

- select：将新的监听文件描述符集合拷贝传入内核中，继续以上步骤。
- poll：将新的struct pollfd结构体数组拷贝传入内核中，继续以上步骤。
- epoll：无需重新构建红黑树，直接沿用已存在的即可。

## 3.epoll池化技术使用步骤

（1）epoll_create创建epoll池

```text
int epoll_create(int size);
```

size标识内核事件表的大小，返回的文件描述符将作用于其他所有epoll系统调用的第一个参数。

加上异常处理，一般的写法：

```text
epollfd = epoll_create(1024);
if (epollfd == -1) {
    perror("epoll_create");
    exit(EXIT_FAILURE);
}
```

这里epollfd来唯一标识epoll池，等于-1时表示出现了异常。

（2）在epoll池中添加fd

```text
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
```

epfd就是刚才创建epoll池的fd，op代表操作类型，常见三种操作类型如下：

1. EPOLL_CTL_ADD：在事件表中注册fd上的事件
2. EPOLL_CTL_MOD：修改fd上的注册事件
3. EPOLL_CTL_DEL：删除fd上的注册事件

所以我们这里使用EPOLL_CTL_ADD在epoll池中添加注册事件：

```text
if (epoll_ctl(epollfd, EPOLL_CTL_ADD, 11, &ev) == -1) {
    perror("epoll_ctl: listen_sock");
    exit(EXIT_FAILURE);
}
```

（3）返回就绪文件描述符

主要通过epoll_wait来实现：

```text
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

在一段超时时间内等待一组文件描述符上的事件。如果监测到事件，就将所有就绪的事件从内核事件表(epfd参数指定)中复制到第二个参数events指向的数组中。因为events数组只用于输出epoll_wait监测到的就绪事件，而不像select、poll那样就用于传入用户注册的事件，又用于输出内核检测到的就绪事件。这样极大提高了应用程序索引就绪文件描述符的效率。

举个例子：

```text
int ret = epoll_wait(epollfd, events, MAX_EVENT_NUM, -1);
for (int i = 0; i < ret; i++) {
    int sockfd = events[i].data.fd;
    // sockfd肯定已经就绪，直接处理
}
```

## 4.隐藏在epoll高效背后的秘密：红黑树和双向链表

- 为什么poll、select需要遍历所有已注册的文件描述符才能找到就绪fd，但是epoll不需要？
- 为什么在epoll池中fd频繁增删查改的过程中保持高效？
- 为什么epoll能通过事件通知的形式，做到最高效的运行？

要弄懂这些问题，需要深入了解epoll背后的数据结构。

**红黑树**

Linux 内核对于 epoll 池的内部实现就是用红黑树的结构体来管理这些注册进程来的句柄 fd。红黑树是一种平衡二叉树，时间复杂度为 O(log n)，就算这个池子就算不断的增删改，也能保持非常稳定的查找性能。

关于红黑树为什么可以实现高效的增删查改，就是另一个故事了，可以简单地概括如下：

![img](https://pic1.zhimg.com/80/v2-0f17dbba501e31df7bc62fbfd7697070_720w.webp)

（1）每个节点或者是黑色，或者是红色。

（2）根节点是黑色。

（3）每个叶子节点（NIL）是黑色。 [注意：这里叶子节点，是指为空(NIL或NULL)的叶子节点！]

（4）如果一个节点是红色的，则它的子节点必须是黑色的。

（5）从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。

**双向链表**

epoll_ctl内部实现中，通过poll回调，poll 机制让上层能直接告诉底层，如果这个 fd 一旦读写就绪了，请底层硬件（比如网卡）回调的时候自动把这个 fd 相关的结构体放到指定队列中，并且唤醒操作系统。

poll内部挂了个钩子，设置了唤醒的回调路径。这个路径存放在哪里？放到一个特定的队列（就绪队列，ready list），这个就绪队列其实是一个双向链表。放到就绪队列中，就可以唤醒epoll啦！

放到就绪队列中的epoll结构体的名字叫做epitem，每个注册到 epoll 池的 fd 都会对应一个。

![img](https://pic1.zhimg.com/80/v2-84d05a1effc8ee139a25a0641d1618b8_720w.webp)

## 5.epoll天生为网络编程而生

为什么说为网络编程而生呢？因为文件系统不能使用epoll

1. ext2，ext4，xfs 等这种真正的文件系统的 fd ，无法使用 epoll 管理；
2. socket fd，eventfd，timerfd 这些实现了 poll 调用的可以放到 epoll 池进行管理；

## 6.代码实例

```text
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <netdb.h>
#include <unistd.h>
#include <fcntl.h>
#include <sys/epoll.h>
#include <errno.h>
 
#define MAXEVENTS 64
 
static int make_socket_non_blocking (int sfd)
{
  int flags, s;
 
  flags = fcntl (sfd, F_GETFL, 0);
  if (flags == -1)
    {
      perror ("fcntl");
      return -1;
    }
 
  flags |= O_NONBLOCK;
  s = fcntl (sfd, F_SETFL, flags);
  if (s == -1)
    {
      perror ("fcntl");
      return -1;
    }
 
  return 0;
}
 
static int create_and_bind (char *port)
{
  struct addrinfo hints;
  struct addrinfo *result, *rp;
  int s, sfd;
 
  memset (&hints, 0, sizeof (struct addrinfo));
  hints.ai_family = AF_UNSPEC;     /* Return IPv4 and IPv6 choices */
  hints.ai_socktype = SOCK_STREAM; /* We want a TCP socket */
  hints.ai_flags = AI_PASSIVE;     /* All interfaces */
 
  s = getaddrinfo (NULL, port, &hints, &result);
  if (s != 0)
    {
      fprintf (stderr, "getaddrinfo: %s\n", gai_strerror (s));
      return -1;
    }
 
  for (rp = result; rp != NULL; rp = rp->ai_next)
    {
      sfd = socket (rp->ai_family, rp->ai_socktype, rp->ai_protocol);
      if (sfd == -1)
        continue;
 
      s = bind (sfd, rp->ai_addr, rp->ai_addrlen);
      if (s == 0)
        {
          /* We managed to bind successfully! */
          break;
        }
 
      close (sfd);
    }
 
  if (rp == NULL)
    {
      fprintf (stderr, "Could not bind\n");
      return -1;
    }
 
  freeaddrinfo (result);
 
  return sfd;
}
 
int main (int argc, char *argv[])
{
  int sfd, s;
  int efd;
  struct epoll_event event;
  struct epoll_event *events;
 
  if (argc != 2)
    {
      fprintf (stderr, "Usage: %s [port]\n", argv[0]);
      exit (EXIT_FAILURE);
    }
 
  sfd = create_and_bind (argv[1]);
  if (sfd == -1)
    abort ();
 
  s = make_socket_non_blocking (sfd);
  if (s == -1)
    abort ();
 
  s = listen (sfd, SOMAXCONN);
  if (s == -1)
    {
      perror ("listen");
      abort ();
    }
 
  efd = epoll_create1 (0);
  if (efd == -1)
    {
      perror ("epoll_create");
      abort ();
    }
 
  event.data.fd = sfd;
  event.events = EPOLLIN | EPOLLET;
  s = epoll_ctl (efd, EPOLL_CTL_ADD, sfd, &event);
  if (s == -1)
    {
      perror ("epoll_ctl");
      abort ();
    }
 
  /* Buffer where events are returned */
  events = calloc (MAXEVENTS, sizeof event);
 
  /* The event loop */
  while (1)
    {
      int n, i;
 
      n = epoll_wait (efd, events, MAXEVENTS, -1);
      for (i = 0; i < n; i++)
	{
	  if ((events[i].events & EPOLLERR) ||
              (events[i].events & EPOLLHUP) ||
              (!(events[i].events & EPOLLIN)))
	    {
              /* An error has occured on this fd, or the socket is not
                 ready for reading (why were we notified then?) */
	      fprintf (stderr, "epoll error\n");
	      close (events[i].data.fd);
	      continue;
	    }
 
	  else if (sfd == events[i].data.fd)
	    {
              /* We have a notification on the listening socket, which
                 means one or more incoming connections. */
              while (1)
                {
                  struct sockaddr in_addr;
                  socklen_t in_len;
                  int infd;
                  char hbuf[NI_MAXHOST], sbuf[NI_MAXSERV];
 
                  in_len = sizeof in_addr;
                  infd = accept (sfd, &in_addr, &in_len);
                  if (infd == -1)
                    {
                      if ((errno == EAGAIN) ||
                          (errno == EWOULDBLOCK))
                        {
                          /* We have processed all incoming
                             connections. */
                          break;
                        }
                      else
                        {
                          perror ("accept");
                          break;
                        }
                    }
 
                  s = getnameinfo (&in_addr, in_len,
                                   hbuf, sizeof hbuf,
                                   sbuf, sizeof sbuf,
                                   NI_NUMERICHOST | NI_NUMERICSERV);
                  if (s == 0)
                    {
                      printf("Accepted connection on descriptor %d "
                             "(host=%s, port=%s)\n", infd, hbuf, sbuf);
                    }
 
                  /* Make the incoming socket non-blocking and add it to the
                     list of fds to monitor. */
                  s = make_socket_non_blocking (infd);
                  if (s == -1)
                    abort ();
 
                  event.data.fd = infd;
                  event.events = EPOLLIN | EPOLLET;
                  s = epoll_ctl (efd, EPOLL_CTL_ADD, infd, &event);
                  if (s == -1)
                    {
                      perror ("epoll_ctl");
                      abort ();
                    }
                }
              continue;
            }
          else
            {
              /* We have data on the fd waiting to be read. Read and
                 display it. We must read whatever data is available
                 completely, as we are running in edge-triggered mode
                 and won't get a notification again for the same
                 data. */
              int done = 0;
 
              while (1)
                {
                  ssize_t count;
                  char buf[512];
 
                  count = read (events[i].data.fd, buf, sizeof buf);
                  if (count == -1)
                    {
                      /* If errno == EAGAIN, that means we have read all
                         data. So go back to the main loop. */
                      if (errno != EAGAIN)
                        {
                          perror ("read");
                          done = 1;
                        }
                      break;
                    }
                  else if (count == 0)
                    {
                      /* End of file. The remote has closed the
                         connection. */
                      done = 1;
                      break;
                    }
 
                  /* Write the buffer to standard output */
                  s = write (1, buf, count);
                  if (s == -1)
                    {
                      perror ("write");
                      abort ();
                    }
                }
 
              if (done)
                {
                  printf ("Closed connection on descriptor %d\n",
                          events[i].data.fd);
 
                  /* Closing the descriptor will make epoll remove it
                     from the set of descriptors which are monitored. */
                  close (events[i].data.fd);
                }
            }
        }
    }
 
  free (events);
 
  close (sfd);
 
  return EXIT_SUCCESS;
}
```

原文地址：https://zhuanlan.zhihu.com/p/411809420

作者：linux

# 【NO.434】面试必备：计算机网络常问的六十二个问题（建议收藏）

这次给大家带来了计算机网络六十二问，三万字，七十图详解，大概是全网最全的网络面试题。

建议大家**收藏**了慢慢看，新的一年一定能够跳槽加薪，虎年“豹”富！

## 1.基础

**1.说下计算机网络体系结构**

计算机网络体系结构，一般有三种：OSI 七层模型、TCP/IP 四层模型、五层结构。

![img](https://pic2.zhimg.com/80/v2-52bf13466cb25c6daffaa580251c37b1_720w.webp)

简单说，OSI是一个理论上的网络通信模型，TCP/IP是实际上的网络通信模型，五层结构就是为了介绍网络原理而折中的网络通信模型。

> **OSI 七层模型**

OSI 七层模型是国际标准化组织（International Organization for Standardization）制定的一个用于计算机或通信系统间互联的标准体系。

- 应用层：通过应用进程之间的交互来完成特定网络应用，应用层协议定义的是应用进程间通信和交互的规则，常见的协议有：**HTTP FTP SMTP SNMP DNS**.
- 表示层：数据的表示、安全、压缩。确保一个系统的应用层所发送的信息可以被另一个系统的应用层读取。
- 会话层：建立、管理、终止会话，是用户应用程序和网络之间的接口。
- 运输层：提供源端与目的端之间提供可靠的透明数据传输，传输层协议为不同主机上运行的进程提供逻辑通信。
- 网络层：将网络地址翻译成对应的物理地址，实现不同网络之间的路径选择, 协议有 **ICMP IGMP IP 等**.
- 数据链路层：在物理层提供比特流服务的基础上，建立相邻结点之间的数据链路。
- 物理层：建立、维护、断开物理连接。

> **TCP/IP 四层模型**

- 应用层：对应于 OSI 参考模型的（应用层、表示层、会话层）。
- 传输层: 对应 OSI 的传输层，为应用层实体提供端到端的通信功能，保证了数据包的顺序传送及数据的完整性。
- 网际层：对应于 OSI 参考模型的网络层，主要解决主机到主机的通信问题。
- 网络接口层：与 OSI 参考模型的数据链路层、物理层对应。

> **五层体系结构**

- 应用层：对应于 OSI 参考模型的（应用层、表示层、会话层）。
- 传输层：对应 OSI 参考模型的的传输层
- 网络层：对应 OSI 参考模型的的网络层
- 数据链路层：对应 OSI 参考模型的的数据链路层
- 物理层：对应 OSI 参考模型的的物理层。

**2.说一下每一层对应的网络协议有哪些？**

一张表格总结常见网络协议：

![img](https://pic1.zhimg.com/80/v2-34adf7053ed62a0c12f869d263f05860_720w.webp)

**3.那么数据在各层之间是怎么传输的呢？**

对于发送方而言，从上层到下层层层包装，对于接收方而言，从下层到上层，层层解开包装。

- 发送方的应用进程向接收方的应用进程传送数据
- AP先将数据交给本主机的应用层，应用层加上本层的控制信息H5就变成了下一层的数据单元
- 传输层收到这个数据单元后，加上本层的控制信息H4，再交给网络层，成为网络层的数据单元
- 到了数据链路层，控制信息被分成两部分，分别加到本层数据单元的首部（H2）和尾部（T2）
- 最后的物理层，进行比特流的传输

![img](https://pic4.zhimg.com/80/v2-2d5340fc9c36759d6a42ff0a08b56e77_720w.webp)

这个过程类似写信，写一封信，每到一层，就加一个信封，写一些地址的信息。到了目的地之后，又一层层解封，传向下一个目的地。

## 2.网络综合

**4.从浏览器地址栏输入 url 到显示主页的过程？**

这道题，大概的过程比较简单，但是有很多点可以细挖：DNS解析、TCP三次握手、HTTP报文格式、TCP四次挥手等等。

1. DNS 解析：将域名解析成对应的 IP 地址。
2. TCP连接：与服务器通过三次握手，建立 TCP 连接
3. 向服务器发送 HTTP 请求
4. 服务器处理请求，返回HTTp响应
5. 浏览器解析并渲染页面
6. 断开连接：TCP 四次挥手，连接结束

我们以输入[http://www.baidu.com](https://link.zhihu.com/?target=http%3A//www.baidu.com) 为例：

![img](https://pic4.zhimg.com/80/v2-16edbd62c9c922c74fa3ff278379e02f_720w.webp)

> 各个过程都使用了哪些协议？

![img](https://pic2.zhimg.com/80/v2-fb154bade380c621402cad84344b9d45_720w.webp)

**5.说说 DNS 的解析过程？**

DNS，英文全称是 **domain name system**，域名解析系统，它的作用也很明确，就是域名和 IP 相互映射。

DNS 的解析过程如下图：

![img](https://pic3.zhimg.com/80/v2-ce9f2898f6d5588c9dfdcc209d617e96_720w.webp)

假设你要查询 **[http://www.baidu.com](https://link.zhihu.com/?target=http%3A//www.baidu.com)** 的 IP 地址:

- 首先会查找浏览器的缓存,看看是否能找到**[http://www.baidu.com](https://link.zhihu.com/?target=http%3A//www.baidu.com)**对应的IP地址，找到就直接返回；否则进行下一步。
- 将请求发往给本地DNS服务器，如果查找到也直接返回，否则继续进行下一步；

![img](https://pic4.zhimg.com/80/v2-edd932c1d733ee9297763763d533b8df_720w.webp)

- 本地DNS服务器向**根域名服务器**发送请求，根域名服务器返回负责com的顶级域名服务器的IP地址的列表。
- 本地DNS服务器再向其中一个负责com的顶级域名服务器发送一个请求，返回负责[http://baidu.com](https://link.zhihu.com/?target=http%3A//baidu.com)的权限域名服务器的IP地址列表。
- 本地DNS服务器再向其中一个权限域名服务器发送一个请求，返回**[http://www.baidu.com](https://link.zhihu.com/?target=http%3A//www.baidu.com)**所对应的IP地址。

**6.说说 WebSocket 与 Socket 的区别？**

- Socket 其实就是等于 **IP 地址 + 端口 + 协议**。

> 具体来说，Socket 是一套标准，它完成了对 TCP/IP 的高度封装，屏蔽网络细节，以方便开发者更好地进行网络编程。

- WebSocket 是一个持久化的协议，它是伴随 H5 而出的协议，用来解决 **http 不支持持久化连接**的问题。
- Socket 一个是**网编编程的标准接口**，而 WebSocket 则是应用层通信协议。

**7.说一下你了解的端口及对应的服务？**

![img](https://pic3.zhimg.com/80/v2-d2d81799bcb8300f9e9bd1bf9b0a3d3e_720w.webp)

## 3.HTTP

**8.说说 HTTP 常用的状态码及其含义？**

HTTP状态码首先应该知道个大概的分类：

- 1XX：信息性状态码
- 2XX：成功状态码
- 3XX：重定向状态码
- 4XX：客户端错误状态码
- 5XX：服务端错误状态码

几个常用的，面试之外，也应该记住：

![img](https://pic1.zhimg.com/80/v2-520cc4dc9508392b586f218c4f14f72c_720w.webp)

之前写过一篇：程序员五一被拉去相亲，结果彻底搞懂了HTTP常用状态码，还比较有意思，可以看看。

> **说一下301和302的区别？**

- 301：永久性移动，请求的资源已被永久移动到新位置。服务器返回此响应时，会返回新的资源地址。
- 302：临时性性移动，服务器从另外的地址响应资源，但是客户端还应该使用这个地址。

用一个比喻，301就是嫁人的新垣结衣，302就是有男朋友的长泽雅美。

**9.HTTP 有哪些请求方式？**

![img](https://pic1.zhimg.com/80/v2-71fb0342ab79021acb56c73e2072ca1c_720w.webp)



其中，POST、DELETE、PUT、GET的含义分别对应我们最熟悉的增、删、改、查。

**10.说⼀下 GET 和 POST 的区别？**

可以从以下几个方面来说明GET和POST的区别：

![img](https://pic4.zhimg.com/80/v2-dc49b5448c5a0fdb9e17c8add6f237ef_720w.webp)

1. 从 HTTP 报文层面来看，GET 请求将信息放在 URL，POST 将请求信息放在请求体中。这一点使得 GET 请求携带的数据量有限，因为 URL 本身是有长度限制的，而 POST 请求的数据存放在报文体中，因此对大小没有限制。而且从形式上看，GET 请求把数据放 URL 上不太安全，而 POST 请求把数据放在请求体里想比较而言安全一些。
2. 从数据库层面来看，GET 符合幂等性和安全性，而 POST 请求不符合。这个其实和 GET/POST 请求的作用有关。按照 HTTP 的约定，GET 请求用于查看信息，不会改变服务器上的信息；而 POST 请求用来改变服务器上的信息。正因为 GET 请求只查看信息，不改变信息，对数据库的一次或多次操作获得的结果是一致的，认为它符合幂等性。安全性是指对数据库操作没有改变数据库中的数据。
3. 从其他层面来看，GET 请求能够被缓存，GET 请求能够保存在浏览器的浏览记录里，GET 请求的 URL 能够保存为浏览器书签。这些都是 POST 请求所不具备的。缓存是 GET 请求被广泛应用的根本，他能够被缓存也是因为它的幂等性和安全性，除了返回结果没有其他多余的动作，因此绝大部分的 GET 请求都被 CDN 缓存起来了，大大减少了 Web 服务器的负担。

**11.GET 的长度限制是多少？**

HTTP中的GET方法是通过URL传递数据的，但是URL本身其实并没有对数据的长度进行限制，真正限制GET长度的是浏览器。

例如IE浏览器对URL的最大限制是2000多个字符，大概2kb左右，像Chrome、Firefox等浏览器支持的URL字符数更多，其中FireFox中URL的最大长度限制是65536个字符，Chrome则是8182个字符。

这个长度限制也不是针对数据部分，而是针对整个URL。

**12.HTTP 请求的过程与原理？**

HTTP协议定义了浏览器怎么向服务器请求文档，以及服务器怎么把文档传给浏览器。

![img](https://pic4.zhimg.com/80/v2-c36de165065548f8b94d2877a5e7d537_720w.webp)



- 每个服务器都有一个进程，它不断监听TCP的端口80，以便发现是否有浏览器向它发出连接建立请求
- 监听到连接请求，就会建立TCP连接
- 浏览器向服务器发出浏览某个页面的请求，服务器接着就返回所请求的页面作为响应
- 最后，释放TCP连接

在浏览器和服务器之间的请求和响应的交互，必须按照规定的格式和遵循一定的规则，这些格式和规则就是超文本传输协议HTTP。

PS:这道题和上面浏览器输入网址发生了什么那道题大差不差。

**13.说一下HTTP的报文结构？**

HTTP报文有两种，HTTP请求报文和HTTP响应报文：

![img](https://pic4.zhimg.com/80/v2-97dbda7508b490321351538a97f99cd7_720w.webp)

**HTTP请求报文**

HTTP 请求报文的格式如下：

复制代码

```text
GET / HTTP/1.1
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5)
Accept: */*
```

HTTP 请求报文的第一行叫做请求行，后面的行叫做首部行，首部行后还可以跟一个实体主体。请求首部之后有一个空行，这个空行不能省略，它用来划分首部与实体。

请求行包含三个字段：

- 方法字段：包括POST、GET等请方法。
- URL 字段
- HTTP 版本字段。

**HTTP 响应报文**

HTTP 响应报文的格式如下：

复制代码

```text
HTTP/1.0 200 OK
Content-Type: text/plain
Content-Length: 137582
Expires: Thu, 05 Dec 1997 16:00:00 GMT
Last-Modified: Wed, 5 August 1996 15:55:28 GMT
Server: Apache 0.84
<html>
  <body>Hello World</body>
</html>
```

HTTP 响应报文的第一行叫做**状态行**，后面的行是**首部行**，最后是**实体主体**。

- **状态行**包含了三个字段：协议版本字段、状态码和相应的状态信息。
- **实体部分**是报文的主要部分，它包含了所请求的对象。
- **首部行**首部可以分为四种首部，请求首部、响应首部、通用首部和实体首部。通用首部和实体首部在请求报文和响应报文中都可以设置，区别在于请求首部和响应首部。常见的请求首部有 Accept 可接收媒体资源的类型、Accept-Charset 可接收的字符集、Host 请求的主机名。常见的响应首部有 ETag 资源的匹配信息，Location 客户端重定向的 URI。常见的通用首部有 Cache-Control 控制缓存策略、Connection 管理持久连接。常见的实体首部有 Content-Length 实体主体的大小、Expires 实体主体的过期时间、Last-Modified 资源的最后修改时间。

**14.URI 和 URL 有什么区别?**



![img](https://pic1.zhimg.com/80/v2-982df6018e5e4c49cd64c79ea5da32fc_720w.webp)



- URI，统一资源标识符(Uniform Resource Identifier， URI)，标识的是Web上每一种可用的资源，如 HTML文档、图像、视频片段、程序等都是由一个URI进行标识的。
- URL，统一资源定位符（Uniform Resource Location)，它是URI的一种子集，主要作用是提供资源的路径。

它们的主要区别在于，URL除了提供了资源的标识，还提供了资源访问的方式。这么比喻，URI 像是身份证，可以唯一标识一个人，而 URL 更像一个住址，可以通过 URL 找到这个人——人类住址协议://地球/中国/北京市/海淀区/xx职业技术学院/14号宿舍楼/525号寝/张三.男。

**15.说下 HTTP/1.0，1.1，2.0 的区别？**

关键需要记住 **HTTP/1.0** 默认是短连接，可以强制开启，HTTP/1.1 默认长连接，HTTP/2.0 采用**多路复用**。

**HTTP/1.0**

- 默认使用**短连接**，每次请求都需要建立一个 TCP 连接。它可以设置Connection: keep-alive 这个字段，强制开启长连接。

**HTTP/1.1**

- 引入了持久连接，即 TCP 连接默认不关闭，可以被多个请求复用。
- 分块传输编码，即服务端每产生一块数据，就发送一块，用” 流模式” 取代” 缓存模式”。
- 管道机制，即在同一个 TCP 连接里面，客户端可以同时发送多个请求。

**HTTP/2.0**

- 二进制协议，1.1 版本的头信息是文本（ASCII 编码），数据体可以是文本或者二进制；2.0 中，头信息和数据体都是二进制。
- 完全多路复用，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应。
- 报头压缩，HTTP 协议不带有状态，每次请求都必须附上所有信息。Http/2.0 引入了头信息压缩机制，使用 gzip 或 compress 压缩后再发送。
- 服务端推送，允许服务器未经请求，主动向客户端发送资源。

**16.HTTP/3了解吗？**

HTTP/3主要有两大变化，**传输层基于UDP**、使用**QUIC保证UDP可靠性**。

HTTP/2存在的一些问题，比如重传等等，都是由于TCP本身的特性导致的，所以HTTP/3在QUIC的基础上进行发展而来，QUIC（Quick UDP Connections）直译为快速UDP网络连接，底层使用UDP进行数据传输。

HTTP/3主要有这些特点：

- 使用UDP作为传输层进行通信
- 在UDP的基础上QUIC协议保证了HTTP/3的安全性，在传输的过程中就完成了TLS加密握手
- HTTPS 要建⽴⼀个连接，要花费 6 次交互，先是建⽴三次握⼿，然后是 TLS/1.3 的三次握⼿。QUIC 直接把以往的 TCP 和 TLS/1.3 的 6 次交互合并成了 **3** 次，减少了交互次数。
- QUIC 有⾃⼰的⼀套机制可以保证传输的可靠性的。当某个流发⽣丢包时，只会阻塞这个流，其他流不会受到影响。

我们拿一张图看一下HTTP协议的变迁：

![img](https://pic1.zhimg.com/80/v2-93bf45e92acebbbde71074883815126c_720w.webp)

**17.HTTP 如何实现长连接？在什么时候会超时？**

> **什么是 HTTP 的长连接？**

1. HTTP 分为长连接和短连接，**本质上说的是 TCP 的长短连接**。TCP 连接是一个双向的通道，它是可以保持一段时间不关闭的，因此 TCP 连接才具有真正的长连接和短连接这一说法。
2. TCP 长连接可以复用一个 TCP 连接，来发起多次的 HTTP 请求，这样就可以减少资源消耗，比如一次请求 HTML，如果是短连接的话，可能还需要请求后续的 JS/CSS。

> **如何设置长连接？**

通过在头部（请求和响应头）设置 **Connection** 字段指定为keep-alive，HTTP/1.0 协议支持，但是是默认关闭的，从 HTTP/1.1 以后，连接默认都是长连接。

> **在什么时候会超时呢？**

- HTTP 一般会有 httpd 守护进程，里面可以设置 **keep-alive timeout**，当 tcp 连接闲置超过这个时间就会关闭，也可以在 HTTP 的 header 里面设置超时时间
- TCP 的 **keep-alive** 包含三个参数，支持在系统内核的 net.ipv4 里面设置；当 TCP 连接之后，闲置了 **tcp_keepalive_time**，则会发生侦测包，如果没有收到对方的 ACK，那么会每隔 tcp_keepalive_intvl 再发一次，直到发送了 **tcp_keepalive_probes**，就会丢弃该连接。

复制代码

```text
1. tcp_keepalive_intvl = 15
2. tcp_keepalive_probes = 5
3. tcp_keepalive_time = 1800
```

**18.说说HTTP 与 HTTPS 有哪些区别？**

1. HTTP 是超⽂本传输协议，信息是明⽂传输，存在安全⻛险的问题。HTTPS 则解决 HTTP 不安全的缺陷，在TCP 和 HTTP ⽹络层之间加⼊了 SSL/TLS 安全协议，使得报⽂能够加密传输。
2. HTTP 连接建⽴相对简单， TCP 三次握⼿之后便可进⾏ HTTP 的报⽂传输。⽽ HTTPS 在 TCP 三次握⼿之后，还需进⾏ SSL/TLS 的握⼿过程，才可进⼊加密报⽂传输。
3. HTTP 的端⼝号是 80，HTTPS 的端⼝号是 443。
4. HTTPS 协议需要向 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的。

**19.为什么要用HTTPS？解决了哪些问题？**

因为HTTP 是明⽂传输，存在安全上的风险：

**窃听⻛险**，⽐如通信链路上可以获取通信内容，用户账号被盗。

**篡改⻛险**，⽐如强制植⼊垃圾⼴告，视觉污染。

**冒充⻛险**，⽐如冒充淘宝⽹站，用户金钱损失。

![img](https://pic3.zhimg.com/80/v2-611d56ca4aed8903d75e14e6579a6252_720w.webp)

所以引入了HTTPS，HTTPS 在 HTTP 与 TCP 层之间加⼊了 SSL/TLS 协议，可以很好的解决了这些风险：

- **信息加密**：交互信息⽆法被窃取。
- **校验机制**：⽆法篡改通信内容，篡改了就不能正常显示。
- **身份证书**：能证明淘宝是真淘宝。

所以SSL/TLS 协议是能保证通信是安全的。

**20.HTTPS工作流程是怎样的？**

这道题有几个要点：**公私钥、数字证书、加密、对称加密、非对称加密**。

HTTPS 主要工作流程：

1. 客户端发起 HTTPS 请求，连接到服务端的 443 端口。
2. 服务端有一套数字证书（证书内容有公钥、证书颁发机构、失效日期等）。
3. 服务端将自己的数字证书发送给客户端（公钥在证书里面，私钥由服务器持有）。
4. 客户端收到数字证书之后，会验证证书的合法性。如果证书验证通过，就会生成一个随机的对称密钥，用证书的公钥加密。
5. 客户端将公钥加密后的密钥发送到服务器。
6. 服务器接收到客户端发来的密文密钥之后，用自己之前保留的私钥对其进行非对称解密，解密之后就得到客户端的密钥，然后用客户端密钥对返回数据进行对称加密，酱紫传输的数据都是密文啦。
7. 服务器将加密后的密文返回到客户端。
8. 客户端收到后，用自己的密钥对其进行对称解密，得到服务器返回的数据。

![img](https://pic1.zhimg.com/80/v2-a1eeae5d51eee66c069aca8e10ec3884_720w.webp)

这里还画了一张更详尽的图：



![img](https://pic2.zhimg.com/80/v2-92b976456c55c25c2de1d570b7c97d15_720w.webp)



**21.客户端怎么去校验证书的合法性？**

首先，服务端的证书从哪来的呢？

为了让服务端的公钥被⼤家信任，服务端的证书都是由 CA （*Certificate Authority*，证书认证机构）签名的，CA就是⽹络世界⾥的公安局、公证中⼼，具有极⾼的可信度，所以由它来给各个公钥签名，信任的⼀⽅签发的证书，那必然证书也是被信任的。

![img](https://pic4.zhimg.com/80/v2-53e8d6a28dfb1de2227b3a62e17c86d3_720w.webp)

CA 签发证书的过程，如上图左边部分：

- ⾸先 CA 会把持有者的公钥、⽤途、颁发者、有效时间等信息打成⼀个包，然后对这些信息进⾏ Hash 计算，得到⼀个 Hash 值；
- 然后 CA 会使⽤⾃⼰的私钥将该 Hash 值加密，⽣成 Certificate Signature，也就是 CA 对证书做了签名；
- 最后将 Certificate Signature 添加在⽂件证书上，形成数字证书；

客户端校验服务端的数字证书的过程，如上图右边部分：

- ⾸先客户端会使⽤同样的 Hash 算法获取该证书的 Hash 值 H1；
- 通常浏览器和操作系统中集成了 CA 的公钥信息，浏览器收到证书后可以使⽤ CA 的公钥解密 Certificate
- Signature 内容，得到⼀个 Hash 值 H2 ；
- 最后⽐较 H1 和 H2，如果值相同，则为可信赖的证书，否则则认为证书不可信。

假如在HTTPS的通信过程中，中间人篡改了证书原文，由于他没有CA机构的私钥，所以CA公钥解密的内容就不一致。

**22.如何理解 HTTP 协议是无状态的？**

这个无状态的的状态值的是什么？是客户端的状态，所以字面意思，就是HTTP协议中服务端不会保存客户端的任何信息。

比如当浏览器第一次发送请求给服务器时，服务器响应了；如果同个浏览器发起第二次请求给服务器时，它还是会响应，但是呢，服务器不知道你就是刚才的那个浏览器。

> **那有什么办法记录状态呢？**

主要有两个办法，Session和Cookie。

**23.说说Session 和 Cookie 有什么联系和区别?**

先来看看什么是 Session 和 Cookie ：

- Cookie 是保存在客户端的一小块文本串的数据。客户端向服务器发起请求时，服务端会向客户端发送一个 Cookie，客户端就把 Cookie 保存起来。在客户端下次向同一服务器再发起请求时，Cookie 被携带发送到服务器。服务端可以根据这个Cookie判断用户的身份和状态。
- Session 指的就是服务器和客户端一次会话的过程。它是另一种记录客户状态的机制。不同的是cookie保存在客户端浏览器中，而session保存在服务器上。客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上，这就是session。客户端浏览器再次访问时只需要从该session中查找用户的状态。

![img](https://pic2.zhimg.com/80/v2-de4669512e62eb72e59c9d520420cff5_720w.webp)

> Session 和 Cookie 到底有什么不同呢？

- 存储位置不一样，Cookie 保存在客户端，Session 保存在服务器端。
- 存储数据类型不一样，Cookie 只能保存ASCII，Session可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息，比如说 UserId 等。
- 有效期不同，Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般有效时间较短，客户端关闭或者 Session 超时都会失效。
- 隐私策略不同，Cookie 存储在客户端，比较容易遭到不法获取，早期有人将用户的登录名和密码存储在 Cookie 中导致信息被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。
- 存储大小不同， 单个Cookie保存的数据不能超过4K，Session可存储数据远高于 Cookie。

> Session 和 Cookie有什么关联呢？

可以使用Cookie记录Session的标识。

![img](https://pic1.zhimg.com/80/v2-6ac9d2c103a7875d66d75bb74b9be1c0_720w.webp)

- 用户第一次请求服务器时，服务器根据用户提交的信息，创建对应的 Session，请求返回时将此 Session 的唯一标识信息 SessionID 返回给浏览器，浏览器接收到服务器返回的 SessionID 信息后，会将此信息存入 Cookie 中，同时 Cookie 记录此 SessionID 是属于哪个域名。
- 当用户第二次访问服务器时，请求会自动判断此域名下是否存在 Cookie 信息，如果存在，则自动将 Cookie 信息也发送给服务端，服务端会从 Cookie 中获取 SessionID，再根据 SessionID 查找对应的 Session 信息，如果没有找到，说明用户没有登录或者登录失效，如果找到 Session 证明用户已经登录可执行后面操作。

> **分布式环境下Session怎么处理呢？**

分布式环境下，客户端请求经过负载均衡，可能会分配到不同的服务器上，假如一个用户的请求两次没有落到同一台服务器上，那么在新的服务器上就没有记录用户状态的Session。

这时候怎么办呢？

可以使用Redis等分布式缓存来存储Session，在多台服务器之间共享。

![img](https://pic2.zhimg.com/80/v2-76cd66da168adaa7c8927bdce9c82405_720w.webp)

> **客户端无法使用Cookie怎么办？**

有可能客户端无法使用Cookie，比如浏览器禁用Cookie，或者客户端是安卓、IOS等等。

这时候怎么办？SessionID怎么存？怎么传给服务端呢？

首先是SessionID的存储，可以使用客户端的本地存储，比如浏览器的sessionStorage。

接下来怎么传呢？

- 拼接到URL里：直接把SessionID作为URL的请求参数
- 放到请求头里：把SessionID放到请求的Header里，比较常用。

## 4.TCP

**24.详细说一下 TCP 的三次握手机制**

PS:TCP三次握手是最重要的知识点，一定要熟悉到问到即送分。

TCP提供面向连接的服务，在传送数据前必须建立连接，TCP连接是通过三次握手建立的。

![img](https://pic1.zhimg.com/80/v2-6cdbb794d9b1545aa1c995955572aba4_720w.webp)



三次握手的过程：

- 最开始，客户端和服务端都处于CLOSE状态，服务端监听客户端的请求，进入LISTEN状态
- 客户端端发送连接请求，**第一次握手** (SYN=1, seq=x)，发送完毕后，客户端就进入 SYN_SENT 状态
- 服务端确认连接，**第二次握手** (SYN=1, ACK=1, seq=y, ACKnum=x+1)， 发送完毕后，服务器端就进入 SYN_RCV 状态。
- 客户端收到服务端的确认之后，再次向服务端确认，这就是**第三次握手 **(ACK=1，ACKnum=y+1)，发送完毕后，客户端进入 ESTABLISHED 状态，当服务器端接收到这个包时，也进入 ESTABLISHED 状态。

TCP三次握手通俗比喻：

在二十年前的农村，电话没有普及，手机就更不用说了，所以，通信基本靠吼。

老张和老王是邻居，这天老张下地了，结果家里有事，热心的邻居老王赶紧跑到村口，开始叫唤老王。

- 老王：老张唉！我是老王，你能听到吗？
- 老张一听，是老王的声音：老王老王，我是老张，我能听到，你能听到吗？
- 老王一听，嗯，没错，是老张：老张，我听到了，我有事要跟你说。"你老婆要生了，赶紧回家吧！"

老张风风火火地赶回家，老婆顺利地生了个带把的大胖小子。握手的故事充满了幸福和美满。

![img](https://pic4.zhimg.com/80/v2-2cb2acca9bb24e956ce2bcd580b56b9f_720w.webp)



**25.TCP 握手为什么是三次，为什么不能是两次？不能是四次？**

> **为什么不能是两次？**

- 为了防止服务器端开启一些无用的连接增加服务器开销
- 防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误。

由于网络传输是有延时的(要通过网络光纤和各种中间代理服务器)，在传输的过程中，比如客户端发起了 SYN=1 的第一次握手。

如果服务器端就直接创建了这个连接并返回包含 SYN、ACK 和 Seq 等内容的数据包给客户端，这个数据包因为网络传输的原因丢失了，丢失之后客户端就一直没有接收到服务器返回的数据包。

如果没有第三次握手告诉服务器端客户端收的到服务器端传输的数据的话，服务器端是不知道客户端有没有接收到服务器端返回的信息的。

服务端就认为这个连接是可用的，端口就一直开着，等到客户端因超时重新发出请求时，服务器就会重新开启一个端口连接。这样一来，就会有很多无效的连接端口白白地开着，导致资源的浪费。

![img](https://pic1.zhimg.com/80/v2-e6f582a4faa640030849baef6df0c428_720w.webp)

还有一种情况是已经失效的客户端发出的请求信息，由于某种原因传输到了服务器端，服务器端以为是客户端发出的有效请求，接收后产生错误。

![img](https://pic1.zhimg.com/80/v2-c83d12979ef785dc0251e43f29532164_720w.webp)

所以我们需要“第三次握手”来确认这个过程：

通过第三次握手的数据告诉服务端，客户端有没有收到服务器“第二次握手”时传过去的数据，以及这个连接的序号是不是有效的。若发送的这个数据是“收到且没有问题”的信息，接收后服务器就正常建立 TCP 连接，否则建立 TCP 连接失败，服务器关闭连接端口。由此减少服务器开销和接收到失效请求发生的错误。

> **为什么不是四次？**

简单说，就是三次挥手已经足够创建可靠的连接，没有必要再多一次握手导致花费更多的时间建立连接。

**26.三次握手中每一次没收到报文会发生什么情况？**

- 第一次握手服务端未收到SYN报文服务端不会进行任何的动作，而客户端由于一段时间内没有收到服务端发来的确认报文，等待一段时间后会重新发送SYN报文，如果仍然没有回应，会重复这个过程，直到发送次数超过最大重传次数限制，就会返回连接建立失败。
- 第二次握手客户端未收到服务端响应的ACK报文客户端会继续重传，直到次数限制；而服务端此时会阻塞在accept()处，等待客户端发送ACK报文
- 第三次握手服务端为收到客户端发送过来的ACK报文服务端同样会采用类似客户端的超时重传机制，如果重试次数超过限制，则accept()调用返回-1，服务端建立连接失败；而此时客户端认为自己已经建立连接成功，因此开始向服务端发送数据，但是服务端的accept()系统调用已经返回，此时不在监听状态，因此服务端接收到客户端发送来的数据时会发送RST报文给客户端，消除客户端单方面建立连接的状态。

**27.第二次握手传回了 ACK，为什么还要传回 SYN？**

ACK是为了告诉客户端传来的数据已经接收无误。

而传回SYN是为了告诉客户端，服务端响应的确实是客户端发送的报文。

**28.第3次握手可以携带数据吗？**

第3次握手是可以携带数据的。

此时客户端已经处于ESTABLISHED状态。对于客户端来说，它已经建立连接成功，并且确认服务端的接收和发送能力是正常的。

第一次握手不能携带数据是出于安全的考虑，因为如果允许携带数据，攻击者每次在SYN报文中携带大量数据，就会导致服务端消耗更多的时间和空间去处理这些报文，会造成CPU和内存的消耗。

**29.说说半连接队列和 SYN Flood 攻击的关系？**

> **什么是半连接队列？**

TCP 进入三次握手前，服务端会从 **CLOSED** 状态变为 **LISTEN** 状态, 同时在内部创建了两个队列：半连接队列（SYN 队列）和全连接队列（ACCEPT 队列）。

![img](https://pic2.zhimg.com/80/v2-cde46b1e90fb949bede0c794d5a8749d_720w.webp)

顾名思义，半连接队列存放的是三次握手未完成的连接，全连接队列存放的是完成三次握手的连接。

- TCP 三次握手时，客户端发送 SYN 到服务端，服务端收到之后，便回复 **ACK 和 SYN**，状态由 **LISTEN 变为 SYN_RCVD**，此时这个连接就被推入了 **SYN 队列**，即半连接队列。
- 当客户端回复 ACK, 服务端接收后，三次握手就完成了。这时连接会等待被具体的应用取走，在被取走之前，它被推入 ACCEPT 队列，即全连接队列。

> **什么是SYN Flood ？**

SYN Flood 是一种典型的 DDos 攻击，它在短时间内，伪造**不存在的 IP 地址**, 向服务器发送大量SYN 报文。当服务器回复 SYN+ACK 报文后，不会收到 ACK 回应报文，那么SYN队列里的连接旧不会出对队，久⽽久之就会占满服务端的 **SYN** 接收队列（半连接队列），使得服务器不能为正常⽤户服务。

![img](https://pic4.zhimg.com/80/v2-dbbd99911fc5347084e740f4189a3773_720w.webp)

> **那有什么应对方案呢？**

主要有 **syn cookie** 和 **SYN Proxy 防火墙**等。

- **syn cookie**：在收到 SYN 包后，服务器根据一定的方法，以数据包的源地址、端口等信息为参数计算出一个 cookie 值作为自己的 SYNACK 包的序列号，回复 SYN+ACK 后，服务器并不立即分配资源进行处理，等收到发送方的 ACK 包后，重新根据数据包的源地址、端口计算该包中的确认序列号是否正确，如果正确则建立连接，否则丢弃该包。
- **SYN Proxy 防火墙**：服务器防火墙会对收到的每一个 SYN 报文进行代理和回应，并保持半连接。等发送方将 ACK 包返回后，再重新构造 SYN 包发到服务器，建立真正的 TCP 连接。

**30.说说 TCP 四次挥手的过程？**

PS：问完三次握手，常常也会顺道问问四次挥手，所以也是必须掌握知识点。

![img](https://pic3.zhimg.com/80/v2-52e96ad9efa5b8748a8c2965cc774e06_720w.webp)

TCP 四次挥手过程：

- 数据传输结束之后，通信双方都可以主动发起断开连接请求，这里假定客户端发起
- 客户端发送释放连接报文，**第一次挥手** (FIN=1，seq=u)，发送完毕后，客户端进入 **FIN_WAIT_1** 状态。
- 服务端发送确认报文，**第二次挥手** (ACK=1，ack=u+1,seq =v)，发送完毕后，服务器端进入 **CLOSE_WAIT** 状态，客户端接收到这个确认包之后，进入 **FIN_WAIT_2** 状态。
- 服务端发送释放连接报文，**第三次挥手** (FIN=1，ACK1,seq=w,ack=u+1)，发送完毕后，服务器端进入 **LAST_ACK**状态，等待来自客户端的最后一个 ACK。
- 客户端发送确认报文，**第四次挥手** (ACK=1，seq=u+1,ack=w+1)，客户端接收到来自服务器端的关闭请求，发送一个确认包，并进入 TIME_WAIT 状态，**等待了某个固定时间（两个最大段生命周期，2MSL，2 Maximum Segment Lifetime）之后**，没有收到服务器端的 ACK ，认为服务器端已经正常关闭连接，于是自己也关闭连接，进入 CLOSED 状态。服务器端接收到这个确认包之后，关闭连接，进入 CLOSED 状态。

大白话说四次挥手：

假如单身狗博主有一个女朋友—由于博主上班九九六，下班肝博客，导致没有时间陪女朋友，女朋友忍无可忍。

- 女朋友：狗男人，最近你都不理我，你是不是不爱我了？你是不是外面有别的狗子了？我要和你分手？
- 沙雕博主一愣，怒火攻心：分手就分手，不陪你闹了，等我把东西收拾收拾。

沙雕博主小心翼翼地装起了自己的青轴机械键盘。

- 哼，蠢女人，我已经收拾完了，我先滚为敬，再见！
- 女朋友：滚，滚的远远的，越远越好，我一辈子都不想再见到你。

挥手的故事总充满了悲伤和遗憾！

![img](https://pic3.zhimg.com/80/v2-5a5bf991f72fe675666ee8a873c95072_720w.webp)

**31.TCP 挥手为什么需要四次呢？**

再来回顾下四次挥手双方发 FIN 包的过程，就能理解为什么需要四次了。

- 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。
- 服务端收到客户端的 FIN 报文时，先回一个 ACK 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报文给客户端来表示同意现在关闭连接。

从上面过程可知，服务端通常需要等待完成数据的发送和处理，所以服务端的 ACK和 FIN 一般都会分开发送，从而比三次握手导致多了一次。

**32.TCP 四次挥手过程中，为什么需要等待 2MSL, 才进入 CLOSED 关闭状态？**

> **为什么需要等待？**

**1. 为了保证客户端发送的最后一个 ACK 报文段能够到达服务端。** 这个 ACK 报文段有可能丢失，因而使处在 **LAST-ACK** 状态的服务端就收不到对已发送的 **FIN + ACK**报文段的确认。服务端会超时重传这个 FIN+ACK 报文段，而客户端就能在 2MSL 时间内（**超时 + 1MSL 传输**）收到这个重传的 FIN+ACK 报文段。接着客户端重传一次确认，重新启动 2MSL 计时器。最后，客户端和服务器都正常进入到 **CLOSED** 状态。

**2. 防止已失效的连接请求报文段出现在本连接中**。客户端在发送完最后一个 ACK 报文段后，再经过时间 2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失。这样就可以使下一个连接中不会出现这种旧的连接请求报文段。

> **为什么等待的时间是2MSL？**

MSL 是 Maximum Segment Lifetime，报⽂最⼤⽣存时间，它是任何报⽂在⽹络上存在的最⻓时间，超过这个时间报⽂将被丢弃。

TIME_WAIT 等待 2 倍的 MSL，⽐较合理的解释是： ⽹络中可能存在来⾃发送⽅的数据包，当这些发送⽅的数据包被接收⽅处理后⼜会向对⽅发送响应，所以⼀来⼀回需要等待 **2** 倍的时间。

![img](https://pic1.zhimg.com/80/v2-9dcc89e3ad207d4833c830c45a5fb154_720w.webp)

⽐如如果被动关闭⽅没有收到断开连接的最后的 ACK 报⽂，就会触发超时重发 Fin 报⽂，另⼀⽅接收到 FIN 后，会重发 ACK 给被动关闭⽅， ⼀来⼀去正好 2 个 MSL。

**33.保活计时器有什么用？**

除时间等待计时器外，TCP 还有一个保活计时器（keepalive timer）。

设想这样的场景：客户已主动与服务器建立了 TCP 连接。但后来客户端的主机突然发生故障。显然，服务器以后就不能再收到客户端发来的数据。因此，应当有措施使服务器不要再白白等待下去。这就需要使用保活计时器了。

服务器每收到一次客户端的数据，就重新设置保活计时器，时间的设置通常是两个小时。若两个小时都没有收到客户端的数据，服务端就发送一个探测报文段，以后则每隔 75 秒钟发送一次。若连续发送 10 个探测报文段后仍然无客户端的响应，服务端就认为客户端出了故障，接着就关闭这个连接。

**34.CLOSE-WAIT 和 TIME-WAIT 的状态和意义？**

> **CLOSE-WAIT状态有什么意义？**

服务端收到客户端关闭连接的请求并确认之后，就会进入CLOSE-WAIT状态。此时服务端可能还有一些数据没有传输完成，因此不能立即关闭连接，而CLOSE-WAIT状态就是为了保证服务端在关闭连接之前将待发送的数据处理完。

> **TIME-WAIT有什么意义？**

TIME-WAIT状态发生在第四次挥手，当客户端向服务端发送ACK确认报文后进入TIME-WAIT状态。

它存在的意义主要是两个：

![img](https://pic1.zhimg.com/80/v2-8ada1088284e8f3c2f2312920df697ec_720w.webp)

- **防⽌旧连接的数据包**如果客户端收到服务端的FIN报文之后立即关闭连接，但是此时服务端对应的端口并没有关闭，如果客户端在相同端口建立新的连接，可能会导致新连接收到旧连接残留的数据包，导致不可预料的异常发生。
- **保证连接正确关闭**假设客户端最后一次发送的ACK包在传输的时候丢失了，由于TCP协议的超时重传机制，服务端将重发FIN报文，如果客户端没有维持TIME-WAIT状态而直接关闭的话，当收到服务端重新发送的FIN包时，客户端就会使用RST包来响应服务端，导致服务端以为有错误发生，然而实际关闭连接过程是正常的。

**35.TIME_WAIT 状态过多会导致什么问题？怎么解决？**

> **TIME_WAIT 状态过多会导致什么问题?**

如果服务器有处于 TIME-WAIT 状态的 TCP，则说明是由服务器⽅主动发起的断开请求。

过多的 TIME-WAIT 状态主要的危害有两种：

第⼀是内存资源占⽤；

第⼆是对端⼝资源的占⽤，⼀个 TCP 连接⾄少消耗⼀个本地端⼝；

> **怎么解决TIME_WAIT 状态过多？**

- 服务器可以设置SO_REUSEADDR套接字来通知内核，如果端口被占用，但是TCP连接位于TIME_WAIT 状态时可以重用端口。
- 还可以使用长连接的方式来减少TCP的连接和断开，在长连接的业务里往往不需要考虑TIME_WAIT状态。

**36.说说 TCP 报文首部的格式？**

看一下TCP报文首部的格式：

![img](https://pic4.zhimg.com/80/v2-0d1a76f6602578c27056693027f2958b_720w.webp)

- **16 位端口号**：源端口号，主机该报文段是来自哪里；目标端口号，要传给哪个上层协议或应用程序
- **32 位序号**：一次 TCP 通信（从 TCP 连接建立到断开）过程中某一个传输方向上的字节流的每个字节的编号。
- **32 位确认号**：用作对另一方发送的 tcp 报文段的响应。其值是收到的 TCP 报文段的序号值加 1。
- **4 位首部长度**：表示 tcp 头部有多少个 32bit 字（4 字节）。因为 4 位最大能标识 15，所以 TCP 头部最长是 60 字节。
- **6 位标志位**：URG(紧急指针是否有效)，ACk（表示确认号是否有效），PST（缓冲区尚未填满），RST（表示要求对方重新建立连接），SYN（建立连接消息标志接），FIN（表示告知对方本端要关闭连接了）
- **16 位窗口大小**：是 TCP 流量控制的一个手段。这里说的窗口，指的是接收通告窗口。它告诉对方本端的 TCP 接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度。
- **16 位校验和**：由发送端填充，接收端对 TCP 报文段执行 CRC 算法以检验 TCP 报文段在传输过程中是否损坏。注意，这个校验不仅包括 TCP 头部，也包括数据部分。这也是 TCP 可靠传输的一个重要保障。
- **16 位紧急指针**：一个正的偏移量。它和序号字段的值相加表示最后一个紧急数据的下一字节的序号。因此，确切地说，这个字段是紧急指针相对当前序号的偏移，不妨称之为紧急偏移。TCP 的紧急指针是发送端向接收端发送紧急数据的方法。

**37.TCP 是如何保证可靠性的？**

TCP主要提供了检验和、序列号/确认应答、超时重传、最大消息长度、滑动窗口控制等方法实现了可靠性传输。

![img](https://pic4.zhimg.com/80/v2-ee2c1d1b796c983e84117be715cb296b_720w.webp)



1. **连接管理**：TCP使用三次握手和四次挥手保证可靠地建立连接和释放连接，这里就不用多说了。
2. **校验和**：TCP 将保持它首部和数据的检验和。这是一个端到端的检验和，目的是检测数据在传输过程中的任何变化。如果接收端的检验和有差错，TCP 将丢弃这个报文段和不确认收到此报文段。

![img](https://pic3.zhimg.com/80/v2-91b15e816a8c5eec9d4fef3cebf4c216_720w.webp)

1. **序列号/确认应答**：TCP 给发送的每一个包进行编号，接收方会对收到的包进行应答，发送方就会知道接收方是否收到对应的包，如果发现没有收到，就会重发，这样就能保证数据的完整性。就像老师上课，会问一句，这一章听懂了吗？没听懂再讲一遍。

![img](https://pic1.zhimg.com/80/v2-741f259fc00096e4998c4bad15fbd6f8_720w.webp)

1. **流量控制：**TCP 连接的每一方都有固定大小的缓冲空间，TCP的接收端只允许发送端发送接收端缓冲区能接纳的数据。当接收方来不及处理发送方的数据，能提示发送方降低发送的速率，防止包丢失。TCP 使用的流量控制协议是可变大小的滑动窗口协议。 （TCP 利用滑动窗口实现流量控制）

![img](https://pic4.zhimg.com/80/v2-f0b80b5e505788a912fc54662820c5fb_720w.webp)

1. **最大消息长度**：在建立TCP连接的时候，双方约定一个最大的长度（MSS）作为发送的单位，重传的时候也是以这个单位来进行重传。理想的情况下是该长度的数据刚好不被网络层分块。

![img](https://pic4.zhimg.com/80/v2-ff174d7c2ca9167e46e829a2019e9ac7_720w.webp)

1. **超时重传：**超时重传是指发送出去的数据包到接收到确认包之间的时间，如果超过了这个时间会被认为是丢包了，需要重传。

![img](https://pic1.zhimg.com/80/v2-19117b691d0dc146e9c03d4362b1c6a8_720w.webp)

1. **拥塞控制：**如果网络非常拥堵，此时再发送数据就会加重网络负担，那么发送的数据段很可能超过了最大生存时间也没有到达接收方，就会产生丢包问题。为此TCP引入慢启动机制，先发出少量数据，就像探路一样，先摸清当前的网络拥堵状态后，再决定按照多大的速度传送数据。

![img](https://pic1.zhimg.com/80/v2-ab2a1f07bfb93c2a9702f18eae8751ec_720w.webp)

**38.说说 TCP 的流量控制？**

TCP 提供了一种机制，可以让发送端根据接收端的实际接收能力控制发送的数据量，这就是**流量控制**。

TCP 通过**滑动窗口**来控制流量，我们看下简要流程：

- 首先双方三次握手，初始化各自的窗口大小，均为 400 个字节。

![img](https://pic2.zhimg.com/80/v2-f98d25375175da0901848feb950472dd_720w.webp)

- 假如当前发送方给接收方发送了 200 个字节，那么，发送方的SND.NXT会右移 200 个字节，也就是说当前的可用窗口减少了 200 个字节。
- 接受方收到后，放到缓冲队列里面，REV.WND =400-200=200 字节，所以 win=200 字节返回给发送方。接收方会在 ACK 的报文首部带上缩小后的滑动窗口 200 字节
- 发送方又发送 200 字节过来，200 字节到达，继续放到缓冲队列。不过这时候，由于大量负载的原因，接受方处理不了这么多字节，只能处理 100 字节，剩余的 100 字节继续放到缓冲队列。这时候，REV.WND = 400-200-100=100 字节，即 win=100 返回发送方。
- 发送方继续发送 100 字节过来，这时候，接收窗口 win 变为 0。
- 发送方停止发送，开启一个定时任务，每隔一段时间，就去询问接受方，直到 win 大于 0，才继续开始发送。

**39.详细说说 TCP 的滑动窗口？**

TCP 发送一个数据，如果需要收到确认应答，才会发送下一个数据。这样的话就会有个缺点：效率会比较低。

“用一个比喻，我们在微信上聊天，你打完一句话，我回复一句之后，你才能打下一句。假如我没有及时回复呢？你是把话憋着不说吗？然后傻傻等到我回复之后再接着发下一句？”

为了解决这个问题，TCP 引入了**窗口**，它是操作系统开辟的一个缓存空间。窗口大小值表示无需等待确认应答，而可以继续发送数据的最大值。

TCP 头部有个字段叫 win，也即那个 **16 位的窗口大小**，它告诉对方本端的 TCP 接收缓冲区还能容纳多少字节的数据，这样对方就可以控制发送数据的速度，从而达到**流量控制**的目的。

“通俗点讲，就是接受方每次收到数据包，在发送确认报文的时候，同时告诉发送方，自己的缓存区还有多少空余空间，缓冲区的空余空间，我们就称之为接受窗口大小。这就是 win。”

TCP 滑动窗口分为两种: 发送窗口和接收窗口。**发送端的滑动窗口**包含四大部分，如下：

- 已发送且已收到 ACK 确认
- 已发送但未收到 ACK 确认
- 未发送但可以发送
- 未发送也不可以发送

![img](https://pic4.zhimg.com/80/v2-c8f773271ff555339df7bdc6c03f7af3_720w.webp)

- 深蓝色框里就是发送窗口。
- SND.WND: 表示发送窗口的大小, 上图虚线框的格子数是 10个，即发送窗口大小是 10。
- SND.NXT：下一个发送的位置，它指向未发送但可以发送的第一个字节的序列号。
- SND.UNA: 一个绝对指针，它指向的是已发送但未确认的第一个字节的序列号。

接收方的滑动窗口包含三大部分，如下：

- 已成功接收并确认
- 未收到数据但可以接收
- 未收到数据并不可以接收的数据

![img](https://pic3.zhimg.com/80/v2-414f9286f97434dc0a66908b1523061e_720w.webp)

- 蓝色框内，就是接收窗口。
- REV.WND: 表示接收窗口的大小, 上图虚线框的格子就是 9 个。
- REV.NXT: 下一个接收的位置，它指向未收到但可以接收的第一个字节的序列号。

**40.了解Nagle 算法和延迟确认吗？**

> **Nagle 算法和延迟确认是干什么的？**

当我们 TCP 报⽂的承载的数据⾮常⼩的时候，例如⼏个字节，那么整个⽹络的效率是很低的，因为每个 TCP 报⽂中都会有 20 个字节的 TCP 头部，也会有 20 个字节的 IP 头部，⽽数据只有⼏个字节，所以在整个报⽂中有效数据占有的比例就会⾮常低。



![img](https://pic1.zhimg.com/80/v2-4207c0c8258207a7915a23e7afe1048c_720w.webp)



这就好像快递员开着⼤货⻋送⼀个⼩包裹⼀样浪费。

那么就出现了常⻅的两种策略，来减少⼩报⽂的传输，分别是：

- Nagle 算法
- 延迟确认

> **Nagle 算法**

Nagle 算法：**任意时刻，最多只能有一个未被确认的小段**。所谓 “小段”，指的是小于 MSS 尺寸的数据块，所谓 “未被确认”，是指一个数据块发送出去后，没有收到对方发送的 ACK 确认该数据已收到。

Nagle 算法的策略：

- 没有已发送未确认报⽂时，⽴刻发送数据。
- 存在未确认报⽂时，直到「没有已发送未确认报⽂」或「数据⻓度达到 MSS ⼤⼩」时，再发送数据。

只要没满⾜上⾯条件中的⼀条，发送⽅⼀直在囤积数据，直到满⾜上⾯的发送条件。

> **延迟确认**

事实上当没有携带数据的 ACK，它的⽹络效率也是很低的，因为它也有 40 个字节的 IP 头 和 TCP 头，但却没有携带数据报⽂。

为了解决 ACK 传输效率低问题，所以就衍⽣出了 **TCP** 延迟确认。

TCP 延迟确认的策略：

- 当有响应数据要发送时，ACK 会随着响应数据⼀起⽴刻发送给对⽅
- 当没有响应数据要发送时，ACK 将会延迟⼀段时间，以等待是否有响应数据可以⼀起发送
- 如果在延迟等待发送 ACK 期间，对⽅的第⼆个数据报⽂⼜到达了，这时就会⽴刻发送 ACK

一般情况下，**Nagle 算法和延迟确认**不能一起使用，Nagle 算法意味着延迟发，**延迟确认**意味着延迟接收，两个凑在一起就会造成更大的延迟，会产生性能问题。

**41.说说TCP 的拥塞控制？**

> **什么是拥塞控制？不是有了流量控制吗？**

前⾯的流量控制是避免发送⽅的数据填满接收⽅的缓存，但是并不知道整个⽹络之中发⽣了什么。

⼀般来说，计算机⽹络都处在⼀个共享的环境。因此也有可能会因为其他主机之间的通信使得⽹络拥堵。

在⽹络出现拥堵时，如果继续发送⼤量数据包，可能会导致数据包时延、丢失等，这时 **TCP** 就会重传数据，但是⼀重传就会导致⽹络的负担更重，于是会导致更⼤的延迟以及更多的丢包，这个情况就会进⼊恶性循环被不断地放⼤....

所以，TCP 不能忽略整个网络中发⽣的事，它被设计成⼀个⽆私的协议，当⽹络发送拥塞时，TCP 会⾃我牺牲，降低发送的数据流。

于是，就有了拥塞控制，控制的⽬的就是避免发送⽅的数据填满整个⽹络。

就像是一个水管，不能让太多的水（数据流）流入水管，如果超过水管的承受能力，水管会被撑爆（丢包）。

![img](https://pic3.zhimg.com/80/v2-47a890255ba275dfffb5fbaf78529dce_720w.webp)

发送方维护一个**拥塞窗口 cwnd（congestion window）** 的变量，调节所要发送数据的量。

> **什么是拥塞窗⼝？和发送窗⼝有什么关系呢？**

拥塞窗⼝ **cwnd**是发送⽅维护的⼀个的状态变量，它会根据⽹络的拥塞程度动态变化的。

发送窗⼝ swnd 和接收窗⼝ rwnd 是约等于的关系，那么由于加⼊了拥塞窗⼝的概念后，此时发送窗⼝的值是swnd = min(cwnd, rwnd)，也就是拥塞窗⼝和接收窗⼝中的最⼩值。

拥塞窗⼝ cwnd 变化的规则：

- 只要⽹络中没有出现拥塞， cwnd 就会增⼤；
- 但⽹络中出现了拥塞， cwnd 就减少；

> **拥塞控制有哪些常用算法？**

拥塞控制主要有这几种常用算法：

![img](https://pic2.zhimg.com/80/v2-83294358548cf597296024aee1f2b029_720w.webp)

- 慢启动
- 拥塞避免
- 拥塞发生
- 快速恢复

**慢启动算法**

慢启动算法，慢慢启动。

它表示 TCP 建立连接完成后，一开始不要发送大量的数据，而是先探测一下网络的拥塞程度。由小到大逐渐增加拥塞窗口的大小，如果没有出现丢包，**每收到一个 ACK，就将拥塞窗口 cwnd 大小就加 1（单位是 MSS）**。**每轮次**发送窗口增加一倍，呈指数增长，如果出现丢包，拥塞窗口就减半，进入拥塞避免阶段。

举个例子：

- 连接建⽴完成后，⼀开始初始化 cwnd = 1 ，表示可以传⼀个 MSS ⼤⼩的数据。
- 当收到⼀个 ACK 确认应答后，cwnd 增加 1，于是⼀次能够发送 2 个
- 当收到 2 个的 ACK 确认应答后， cwnd 增加 2，于是就可以⽐之前多发2 个，所以这⼀次能够发送 4 个
- 当这 4 个的 ACK 确认到来的时候，每个确认 cwnd 增加 1， 4 个确认 cwnd 增加 4，于是就可以⽐之前多发4 个，所以这⼀次能够发送 8 个。

![img](https://pic3.zhimg.com/80/v2-9dbecfc0e9a00da61a8933853bccdd6e_720w.webp)

发包的个数是指数性的增⻓。

![img](https://pic4.zhimg.com/80/v2-d091fb47acc9d85a320e81accba143df_720w.webp)

为了防止 cwnd 增长过大引起网络拥塞，还需设置一个**慢启动阀值 ssthresh**（slow start threshold）状态变量。当cwnd到达该阀值后，就好像水管被关小了水龙头一样，减少拥塞状态。即当 **cwnd >ssthresh** 时，进入了**拥塞避免**算法。

**拥塞避免算法**

一般来说，慢启动阀值 ssthresh 是 65535 字节，cwnd到达**慢启动阀值**后

- 每收到一个 ACK 时，cwnd = cwnd + 1/cwnd
- 当每过一个 RTT 时，cwnd = cwnd + 1

显然这是一个线性上升的算法，避免过快导致网络拥塞问题。

接着上面慢启动的例子，假定 ssthresh 为 8 ： ：

- 当 8 个 ACK 应答确认到来时，每个确认增加 1/8，8 个 ACK 确认 cwnd ⼀共增加 1，于是这⼀次能够发送 9个 MSS ⼤⼩的数据，变成了线性增⻓。

![img](https://pic3.zhimg.com/80/v2-325cd338e513307e6926de94f390523a_720w.webp)

**拥塞发生**

当网络拥塞发生**丢包**时，会有两种情况：

- RTO 超时重传
- 快速重传

如果是发生了 **RTO 超时重传**，就会使用拥塞发生算法

- 慢启动阀值 sshthresh = cwnd /2
- cwnd 重置为 1
- 进入新的慢启动过程

![img](https://pic2.zhimg.com/80/v2-105cd21b816a7e0691c753e7d90a0fa5_720w.webp)

这种方式就像是飙车的时候急刹车，还飞速倒车，这。。。

其实还有更好的处理方式，就是**快速重传**。发送方收到 3 个连续重复的 ACK 时，就会快速地重传，不必等待 **RTO 超时**再重传。

发⽣快速重传的拥塞发⽣算法：

- 拥塞窗口大小 cwnd = cwnd/2
- 慢启动阀值 ssthresh = cwnd
- 进入快速恢复算法

**快速恢复**

快速重传和快速恢复算法一般同时使用。快速恢复算法认为，还有 3 个重复 ACK 收到，说明网络也没那么糟糕，所以没有必要像 RTO 超时那么强烈。

正如前面所说，进入快速恢复之前，cwnd 和 sshthresh 已被更新：

- cwnd = cwnd /2

\- sshthresh = cwnd

然后，进⼊快速恢复算法如下：

- cwnd = sshthresh + 3
- 重传重复的那几个 ACK（即丢失的那几个数据包）
- 如果再收到重复的 ACK，那么 cwnd = cwnd +1
- 如果收到新数据的 ACK 后, cwnd = sshthresh。因为收到新数据的 ACK，表明恢复过程已经结束，可以再次进入了拥塞避免的算法了。

![img](https://pic4.zhimg.com/80/v2-61b735cc7d094a2fc72a78e3a591332b_720w.webp)

**42.说说 TCP 的重传机制？**

重传包括**超时重传、快速重传、带选择确认的重传（SACK）、重复 SACK 四种**。

![img](https://pic4.zhimg.com/80/v2-c23cf058a4b2a584590fa47794a6d417_720w.webp)

**超时重传**

超时重传，是 TCP 协议保证数据可靠性的另一个重要机制，其原理是在发送某一个数据以后就开启一个计时器，在一定时间内如果没有得到发送的数据报的 ACK 报文，那么就重新发送数据，直到发送成功为止。

> **超时时间应该设置为多少呢？**

先来看下什么叫 **RTT（Round-Trip Time，往返时间）**。

![img](https://pic4.zhimg.com/80/v2-1ae8d6aa66479140d559465720f0723b_720w.webp)

RTT 就是数据完全发送完，到收到确认信号的时间，即数据包的一次往返时间。

超时重传时间，就是 RTO（Retransmission Timeout)。那么，**RTO 到底设置多大呢？**

- 如果 RTO 设置很大，等了很久都没重发，这样肯定就不行。
- 如果 RTO 设置很小，那很可能数据都没有丢失，就开始重发了，这会导致网络阻塞，从而恶性循环，导致更多的超时出现。

一般来说，RTO 略微大于 RTT，效果是最佳的。

其实，RTO 有个标准方法的计算公式，也叫 **Jacobson / Karels 算法**。

1. 首先计算 SRTT（即计算平滑的 RTT）

复制代码

```text
SRTT = (1 - α) * SRTT + α * RTT  //求 SRTT 的加权平均
```

1. 其次，计算 RTTVAR (round-trip time variation)

复制代码

```text
RTTVAR = (1 - β) * RTTVAR + β * (|RTT - SRTT|) //计算 SRTT 与真实值的差距
```

1. 最后，得出最终的 RTO

复制代码

```text
RTO = µ * SRTT + ∂ * RTTVAR  =  SRTT + 4·RTTVAR
```

在 Linux 下，**α = 0.125**，**β = 0.25**， **μ = 1**，**∂ = 4**。别问这些参数是怎么来的，它们是大量实践，调出的最优参数。

超时重传不是十分完美的重传方案，它有这些缺点：

- 当一个报文丢失时，会等待一定的超时周期，才重传分组，增加了端到端的时延。
- 当一个报文丢失时，在其等待超时的过程中，可能会出现这种情况：其后的报文段已经被接收端接收但却迟迟得不到确认，发送端会认为也丢失了，从而引起不必要的重传，既浪费资源也浪费时间。

并且，对于 TCP，如果发生一次超时重传，时间间隔下次就会加倍。

**快速重传**

TCP 还有另外⼀种快速重传（**Fast Retransmit**）机制，它不以时间为驱动，⽽是以数据驱动重传。

它不以时间驱动，而是以数据驱动。它是基于接收端的反馈信息来引发重传的。

可以用它来解决超时重发的时间等待问题，快速重传流程如下：

![img](https://pic3.zhimg.com/80/v2-94c0630679ae083c93944a17cbe2600a_720w.webp)

在上图，发送⽅发出了 1，2，3，4，5 份数据：

- 第⼀份 Seq1 先送到了，于是就 Ack 回 2；
- 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2；
- 后⾯的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到；
- 发送端收到了三个 **Ack = 2** 的确认，知道了 **Seq2** 还没有收到，就会在定时器过期之前，重传丢失的 **Seq2**。
- 最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。

快速重传机制只解决了⼀个问题，就是超时时间的问题，但是它依然⾯临着另外⼀个问题。就是重传的时候，是重传之前的⼀个，还是重传所有的问题。

⽐如对于上⾯的例⼦，是重传 Seq2 呢？还是重传 Seq2、Seq3、Seq4、Seq5 呢？因为发送端并不清楚这连续的三个 Ack 2 是谁传回来的。

根据 TCP 不同的实现，以上两种情况都是有可能的。可⻅，这是⼀把双刃剑。

为了解决不知道该重传哪些 TCP 报⽂，于是就有 SACK ⽅法。

**带选择确认的重传（SACK）**

为了解决应该重传多少个包的问题? TCP 提供了**带选择确认的重传**（即 SACK，Selective Acknowledgment）。

**SACK 机制**就是，在快速重传的基础上，接收方返回最近收到报文段的序列号范围，这样发送方就知道接收方哪些数据包是没收到的。这样就很清楚应该重传哪些数据包。

![img](https://pic3.zhimg.com/80/v2-10836be338620dee33ff3bd7208adfc2_720w.webp)

如上图中，发送⽅收到了三次同样的 ACK 确认报⽂，于是就会触发快速重发机制，通过 SACK 信息发现只有200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进⾏重发。

**重复 SACK（D-SACK）**

D-SACK，英文是 Duplicate SACK，是在 SACK 的基础上做了一些扩展，主要用来告诉发送方，有哪些数据包，自己重复接受了。

DSACK 的目的是帮助发送方判断，是否发生了包失序、ACK 丢失、包重复或伪重传。让 TCP 可以更好的做网络流控。

例如ACK丢包导致的数据包重复：

![img](https://pic2.zhimg.com/80/v2-305ad381a36e313fe9b391b1c6f65e2d_720w.webp)

- 接收⽅发给发送⽅的两个 ACK 确认应答都丢失了，所以发送⽅超时后，重传第⼀个数据包（3000 ~

3499）

- 于是接收⽅发现数据是重复收到的，于是回了⼀个 **SACK = 3000~3500**，告诉「发送⽅」 3000~3500的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个SACK 就代表着 D-SACK 。这样发送⽅就知道了，数据没有丢，是接收⽅的 ACK 确认报⽂丢了。

**43.说说TCP 的粘包和拆包？**

TCP 的粘包和拆包更多的是业务上的概念！

> **什么是TCP粘包和拆包？**

TCP 是面向流，没有界限的一串数据。TCP 底层并不了解上层业务数据的具体含义，它会根据 TCP 缓冲区的实际情况进行包的划分，所以在业务上认为，一**个完整的包可能会被 TCP 拆分成多个包进行发送**，**也有可能把多个小的包封装成一个大的数据包发送**，这就是所谓的 TCP 粘包和拆包问题。

![img](https://pic3.zhimg.com/80/v2-84a79fb4107198918389ef279dca23e6_720w.webp)

> **为什么会产生粘包和拆包呢?**

- 要发送的数据小于 TCP 发送缓冲区的大小，TCP 将多次写入缓冲区的数据一次发送出去，将会发生粘包；
- 接收数据端的应用层没有及时读取接收缓冲区中的数据，将发生粘包；
- 要发送的数据大于 TCP 发送缓冲区剩余空间大小，将会发生拆包；
- 待发送数据大于 MSS（最大报文长度），TCP 在传输前将进行拆包。即 TCP 报文长度 - TCP 头部长度 > MSS。

> **那怎么解决呢？**

- 发送端将每个数据包封装为固定长度
- 在数据尾部增加特殊字符进行分割
- 将数据分为两部分，一部分是头部，一部分是内容体；其中头部结构大小固定，且有一个字段声明内容体的大小。

## 5.UDP

UDP问的不多，基本上是被拿来和TCP比较。

**44.说说 TCP 和 UDP 的区别？**

最根本区别：**TCP 是面向连接，而 UDP 是无连接**。

![img](https://pic1.zhimg.com/80/v2-91480d36bad1b1ca8a099855a87a8670_720w.webp)

可以这么形容：TCP是打电话，UDP是大喇叭。

![img](https://pic2.zhimg.com/80/v2-b0e23a8ad4b843f3c82823cbd3f6c115_720w.webp)

> **说说TCP和UDP的应用场景？**

- **TCP应用场景：** 效率要求相对低，但对准确性要求相对高的场景。因为传输中需要对数据确认、重发、排序等操作，相比之下效率没有UDP高。例如：文件传输（准确高要求高、但是速度可以相对慢）、收发邮件、远程登录。
- **UDP应用场景：** 效率要求相对高，对准确性要求相对低的场景。例如：QQ聊天、在线视频、网络语音电话（即时通讯，速度要求高，但是出现偶尔断续不是太大问题，并且此处完全不可以使用重发机制）、广播通信（广播、多播）。

**45.为什么QQ采用UDP协议？**

PS：这是多年前的老题了，拉出来怀怀旧。

![img](https://pic4.zhimg.com/80/v2-bd6368ef49c0401e96f787b451005f7f_720w.webp)

- 首先，QQ并不是完全基于UDP实现。比如在使用QQ进行文件传输等活动的时候，就会使用TCP作为可靠传输的保证。
- 使用UDP进行交互通信的好处在于，延迟较短，对数据丢失的处理比较简单。同时，TCP是一个全双工协议，需要建立连接，所以网络开销也会相对大。
- 如果使用QQ语音和QQ视频的话，UDP的优势就更为突出了，首先延迟较小。最重要的一点是不可靠传输，这意味着如果数据丢失的话，不会有重传。因为用户一般来说可以接受图像稍微模糊一点，声音稍微不清晰一点，但是如果在几秒钟以后再出现之前丢失的画面和声音，这恐怕是很难接受的。
- 由于QQ的服务器设计容量是海量级的应用，一台服务器要同时容纳十几万的并发连接，因此服务器端只有采用UDP协议与客户端进行通讯才能保证这种超大规模的服务

简单总结一下：UDP协议是无连接方式的协议，它的效率高，速度快，占资源少，对服务器的压力比较小。但是其传输机制为不可靠传送，必须依靠辅助的算法来完成传输控制。QQ采用的通信协议以UDP为主，辅以TCP协议。

**46.UDP协议为什么不可靠？**

UDP在传输数据之前不需要先建立连接，远地主机的运输层在接收到UDP报文后，不需要确认，提供不可靠交付。总结就以下四点：

- 不保证消息交付：不确认，不重传，无超时
- 不保证交付顺序：不设置包序号，不重排，不会发生队首阻塞
- 不跟踪连接状态：不必建立连接或重启状态机
- 不进行拥塞控制：不内置客户端或网络反馈机制

**47.DNS为什么要用UDP?**

更准确地说，DNS既使用TCP又使用UDP。

当进行区域传送（主域名服务器向辅助域名服务器传送变化的那部分数据）时会使用TCP，因为数据同步传送的数据量比一个请求和应答的数据量要多，而TCP允许的报文长度更长，因此为了保证数据的正确性，会使用基于可靠连接的TCP。

当客户端想DNS服务器查询域名（域名解析）的时候，一般返回的内容不会超过UDP报文的最大长度，即512字节，用UDP传输时，不需要创建连接，从而大大提高了响应速度，但这要求域名解析服务器和域名服务器都必须自己处理超时和重传从而保证可靠性。

## 6.IP

**48.IP 协议的定义和作用？**

> **IP协议是什么？**

IP协议（Internet Protocol）又被称为互联网协议，是支持网间互联的数据包协议，工作在**网际层**，主要目的就是为了提高网络的可扩展性。

通过**网际协议IP**，可以把参与互联的，性能各异的网络**看作一个统一的网络**。

![img](https://pic2.zhimg.com/80/v2-a5ee62dd82bee7aa40bc90a2af0e6599_720w.webp)

和传输层TCP相比，IP协议是一种无连接/不可靠、尽力而为的数据包传输服务，和TCP协议一起构成了TCP/IP协议的核心。

> **IP协议有哪些作用？**

IP协议主要有以下几个作用：

- **寻址和路由**：在IP数据报中携带源IP地址和目的IP地址来表示该数据包的源主机和目标主机。IP数据报在传输过程中，每个中间节点（IP网关、路由器）只根据网络地址来进行转发，如果中间节点是路由器，则路由器会根据路由表选择合适的路径。IP协议根据路由选择协议提供的路由信息对IP数据报进行转发，直至目标主机。
- **分段和重组**：IP数据报在传输过程中可能会经过不同的网络，在不同的网络中数据报的最大长度限制是不同的，IP协议通过给每个IP数据报分配一个标识符以及分段与组装的相关信息，使得数据报在不同的网络中能够被传输，被分段后的IP数据报可以独立地在网络中进行转发，在达到目标主机后由目标主机完成重组工作，恢复出原来的IP数据报。

> **传输层协议和网络层协议有什么区别？**

网络层协议负责提供主机间的逻辑通信；传输层协议负责提供进程间的逻辑通信。

**49.IP 地址有哪些分类？**

一个IP地址在这鞥个互联网范围内是惟一的，一般可以这么认为，IP 地址 = {<网络号>，<主机号>}。

1. **网络号**：它标志主机所连接的网络地址表示属于互联网的哪一个网络。
2. **主机号**：它标志主机地址表示其属于该网络中的哪一台主机。

IP 地址分为 A，B，C，D，E 五大类：

- A 类地址 (1~126)：以 0 开头，网络号占前 8 位，主机号占后面 24 位。
- B 类地址 (128~191)：以 10 开头，网络号占前 16 位，主机号占后面 16 位。
- C 类地址 (192~223)：以 110 开头，网络号占前 24 位，主机号占后面 8 位。
- D 类地址 (224~239)：以 1110 开头，保留为多播地址。
- E 类地址 (240~255)：以 1111开头，保留位为将来使用

![img](https://pic2.zhimg.com/80/v2-a2fb55653526be2c16289c425a8974e9_720w.webp)

**50.域名和 IP 的关系？一个 IP 可以对应多个域名吗？**

- IP地址在同一个网络中是惟一的，用来标识每一个网络上的设备，其相当于一个人的身份证号
- 域名在同一个网络中也是惟一的，就像是一个人的名字、绰号

假如你有多个不用的绰号，你的朋友可以用其中任何一个绰号叫你，但你的身份证号码却是惟一的。但同时你的绰号也可能和别人重复，假如你不在，有人叫你的绰号，其它人可能就答应了。

一个域名可以对应多个IP，但这种情况DNS做负载均衡的，在用户访问过程中，一个域名只能对应一个IP。

而一个IP却可以对应多个域名，是一对多的关系。

**51.IPV4 地址不够如何解决？**

我们知道，IP地址有32位，可以标记2的32次方个地址，听起来很多，但是全球的网络设备数量已经远远超过这个数字，所以IPV4地址已经不够用了，那怎么解决呢？

![img](https://pic1.zhimg.com/80/v2-15533f300ebdb8a8070864fa5abb5cac_720w.webp)

- DHCP：动态主机配置协议，动态分配IP地址，只给接入网络的设备分配IP地址，因此同一个MAC地址的设备，每次接入互联网时，得到的IP地址不一定是相同的，该协议使得空闲的IP地址可以得到充分利用。
- CIDR：无类别域间路由。CIDR消除了传统的A类、B类、C类地址以及划分子网的概念，因而更加有效地分配IPv4的地址空间，但无法从根本上解决地址耗尽的问题。
- NAT：网络地址转换协议，我们知道属于不同局域网的主机可以使用相同的IP地址，从而一定程度上缓解了IP资源枯竭的问题，然而主机在局域网中使用的IP地址是不能在公网中使用的，当局域网主机想要与公网主机进行通信时，NAT方法可以将该主机IP地址转换为全球IP地址。该协议能够有效解决IP地址不足的问题。
- IPv6：作为接替IPv4的下一代互联网协议，其可以实现2的128次方个地址，而这个数量级，即使给地球上每一粒沙子都分配一个IP地址也够用，该协议能够从根本上解决IPv4地址不够用的问题。

**52.说下 ARP 协议的工作过程？**

ARP 协议，**Address Resolution Protocol**，地址解析协议，它是用于实现 IP 地址到 MAC 地址的映射。

![img](https://pic4.zhimg.com/80/v2-cfbe609535e3f3ebb14fd698651dae4f_720w.webp)

1. 首先，每台主机都会在自己的 ARP 缓冲区中建立一个 ARP 列表，以表示 IP 地址和 MAC 地址的对应关系。
2. 当源主机需要将一个数据包要发送到目的主机时，会首先检查自己的 ARP 列表，是否存在该 IP 地址对应的 MAC 地址；如果有﹐就直接将数据包发送到这个 MAC 地址；如果没有，就向本地网段发起一个 ARP 请求的广播包，查询此目的主机对应的 MAC 地址。此 ARP 请求的数据包里，包括源主机的 IP 地址、硬件地址、以及目的主机的 IP 地址。
3. 网络中所有的主机收到这个 ARP 请求后，会检查数据包中的目的 IP 是否和自己的 IP 地址一致。如果不相同，就会忽略此数据包；如果相同，该主机首先将发送端的 MAC 地址和 IP 地址添加到自己的 ARP 列表中，如果 ARP 表中已经存在该 IP 的信息，则将其覆盖，然后给源主机发送一个 ARP 响应数据包，告诉对方自己是它需要查找的 MAC 地址。
4. 源主机收到这个 ARP 响应数据包后，将得到的目的主机的 IP 地址和 MAC 地址添加到自己的 ARP 列表中，并利用此信息开始数据的传输。如果源主机一直没有收到 ARP 响应数据包，表示 ARP 查询失败。

**53.为什么既有IP地址，又有MAC 地址？**

> **MAC地址和IP地址都有什么作用？**

- MAC地址是数据链路层和物理层使用的地址，是写在网卡上的物理地址，用来定义网络设备的位置，不可变更。
- IP地址是网络层和以上各层使用的地址，是一种逻辑地址。IP地址用来区别网络上的计算机。

> **为什么有了MAC地址还需要IP地址？**

如果我们只使用MAC地址进行寻址的话，我们需要路由器记住每个MAC地址属于哪个子网，不然一次路由器收到数据包都要满世界寻找目的MAC地址。而我们知道MAC地址的长度为48位，也就是最多共有2的48次方个MAC地址，这就意味着每个路由器需要256T的内存，显然是不现实的。

和MAC地址不同，IP地址是和地域相关的，在一个子网中的设备，我们给其分配的IP地址前缀都是一样的，这样路由器就能根据IP地址的前缀知道这个设备属于哪个子网，剩下的寻址就交给子网内部实现，从而大大减少了路由器所需要的内存。

> **为什么有了IP地址还需要MAC地址？**

![img](https://pic2.zhimg.com/80/v2-07b65276ff46978db0e865fe34dff88d_720w.webp)

- 只有当设备连入网络时，才能根据他进入了哪个子网来为其分配IP地址，在设备还没有IP地址的时候，或者在分配IP的过程中。我们需要MAC地址来区分不同的设备。
- IP 地址可以比作为地址，MAC 地址为收件人，在一次通信过程中，两者是缺一不可的。

**54.ICMP 协议的功能？**

ICMP（Internet Control Message Protocol） ，网际控制报文协议。

- ICMP 协议是一种面向无连接的协议，用于传输出错报告控制信息。
- 它是一个非常重要的协议，它对于网络安全具有极其重要的意义。它属于网络层协议，主要用于在主机与路由器之间传递控制信息，包括**报告错误、交换受限控制和状态信息**等。
- 当遇到 IP 数据无法访问目标、IP 路由器无法按当前的传输速率转发数据包等情况时，会自动发送 ICMP 消息。

比如我们日常使用得比较多的 **ping**，就是基于 ICMP 的。

**55.说下 ping 的原理？**

ping，**Packet Internet Groper**，是一种因特网包探索器，用于测试网络连接量的程序。Ping 是工作在 TCP/IP 网络体系结构中应用层的一个服务命令， 主要是向特定的目的主机发送 ICMP（Internet Control Message Protocol 因特网报文控制协议） 请求报文，测试目的站是否可达及了解其有关状态。

![img](https://pic3.zhimg.com/80/v2-c7746b59a97875bf648f0235a9280e86_720w.webp)

一般来说，ping 可以用来检测网络通不通。它是基于ICMP协议工作的。假设**机器 A**ping **机器 B**，工作过程如下：

1. ping 通知系统，新建一个固定格式的 ICMP 请求数据包
2. ICMP 协议，将该数据包和目标机器 B 的 IP 地址打包，一起转交给 IP 协议层
3. IP 层协议将本机 IP 地址为源地址，机器 B 的 IP 地址为目标地址，加上一些其他的控制信息，构建一个 IP 数据包
4. 先获取目标机器 B 的 MAC 地址。
5. 数据链路层构建一个数据帧，目的地址是 IP 层传过来的 **MAC 地址**，源地址是本机的 **MAC 地址**
6. 机器 B 收到后，对比目标地址，和自己本机的 MAC 地址是否一致，符合就处理返回，不符合就丢弃。
7. 根据目的主机返回的 ICMP 回送回答报文中的时间戳，从而计算出往返时间
8. 最终显示结果有这几项：发送到目的主机的 IP 地址、发送 & 收到 & 丢失的分组数、往返时间的最小、最大 & 平均值

## 7.网络安全

**56.说说有哪些安全攻击？**

网络安全攻击主要分为两种类型，**被动攻击**和**主动攻击**：

![img](https://pic3.zhimg.com/80/v2-64853b1f1cc9099f3823e29dc4f0ee76_720w.webp)

- **被动攻击**：是指攻击者从网络上窃听他人的通信内容，通常把这类攻击称为截获，被动攻击主要有两种形式：消息内容泄露攻击和流量分析攻击。由于攻击者没有修改数据，使得这种攻击很难被检测到。
- **主动攻击**：直接对现有的数据和服务造成影响，常见的主动攻击类型有：**篡改**：攻击者故意篡改网络上送的报文，甚至把完全伪造的报文传送给接收方。**恶意程序**：恶意程序种类繁多，包括计算机病毒、计算机蠕虫、特洛伊木马、后门入侵、流氓软件等等。**拒绝服务Dos**：攻击者向服务器不停地发送分组，使服务器无法提供正常服务。

**57.DNS劫持了解吗？**

DNS劫持即域名劫持，是通过将原域名对应的IP地址进行替换，从而使用户访问到错误的网站，或者使用户无法正常访问网站的一种攻击方式。

![img](https://pic3.zhimg.com/80/v2-84c0f3303de9d7175530caa9dc8d21d2_720w.webp)

域名劫持往往只能在特定的网络范围内进行，范围外的DNS服务器能够返回正常的IP地址。攻击者可以冒充原域名所属机构，通过电子邮件的方式修改组织机构的域名注册信息，或者将域名转让给其它主持，并将新的域名信息保存在所指定的DNS服务器中，从而使用户无法对原域名来进行解析以访问目标地址。

> **DNS劫持的步骤是什么样的？**

1. 获取要劫持的域名信息：攻击者会首先访问域名查询要劫持的站点的域名信息。
2. 控制域名响应的E-Mail账号：在获取到域名信息后，攻击者通过暴力破解或者专门的方法破解公司注册域名时使用的E-mail账号所对应的密码，更高级的攻击者甚至能够直接对E-Mail进行信息窃取。
3. 修改注册信息：当攻击者破解了E-Mail后，会利用相关的更改功能修改该域名的注册信息，包括域名拥有者信息，DNS服务器信息等。
4. 使用E-Mail收发确认函：在修改完注册信息后，攻击者E-Mail在真正拥有者之前收到修改域名注册信息的相关确认信息，并回复确认修改文件，待网络公司恢复已成功修改信件后，攻击者便成功完成DNS劫持。

> **怎么应对DNS劫持？**

- 直接通过IP地址访问网站，避开DNS劫持
- 由于域名劫持往往只能在特定的网络范围内进行，因此一些高级用户可以通过网络设置让DNS指向正常的域名服务器以实现对目标网址的正常访问，例如计算机首选DNS服务器的地址固定为8.8.8.8。

**58.什么是 CSRF 攻击？如何避免？**

> **什么是 CSRF 攻击？**

CSRF，跨站请求伪造（英文全称是 Cross-site request forgery），是一种挟持用户在当前已登录的 Web 应用程序上执行非本意的操作的攻击方法。

> **CSRF 是如何攻击的呢？**

来看一个例子：

![img](https://pic1.zhimg.com/80/v2-fed4a7f9c2c188be810348ae537bb9a0_720w.webp)

1. 用户登陆银行，没有退出，浏览器包含了 用户 在银行的身份认证信息。
2. 攻击者将伪造的转账请求，包含在在帖子
3. 用户在银行网站保持登陆的情况下，浏览帖子
4. 将伪造的转账请求连同身份认证信息，发送到银行网站
5. 银行网站看到身份认证信息，以为就是 用户的合法操作，最后造成用户资金损失。

> **怎么应对 CSRF 攻击呢？**

- **检查 Referer 字段**HTTP头中的Referer字段记录了该 HTTP 请求的来源地址。在通常情况下，访问一个安全受限页面的请求来自于同一个网站，而如果黑客要对其实施 CSRF攻击，他一般只能在他自己的网站构造请求。因此，可以通过验证Referer值来防御CSRF 攻击。
- **添加校验 token**以在 HTTP 请求中以参数的形式加入一个随机产生的 token，并在服务器端建立一个拦截器来验证这个 token，如果请求中没有token或者 token 内容不正确，则认为可能是 CSRF 攻击而拒绝该请求。
- **敏感操作多重校验**对一些敏感的操作，除了需要校验用户的认证信息，还可以通过邮箱确认、验证码确认这样的方式多重校验。

**59.什么是 DoS、DDoS、DRDoS 攻击？**

![img](https://pic3.zhimg.com/80/v2-c079e15016da60c568e9c5ad8e18071a_720w.webp)

- **DOS**: (Denial of Service), 翻译过来就是拒绝服务, 一切能引起拒绝 行为的攻击都被称为 DOS 攻击。最常见的 DoS 攻击就有**计算机网络宽带攻击**、**连通性攻击**。
- **DDoS**: (Distributed Denial of Service)，翻译过来是分布式拒绝服务。是指处于不同位置的多个攻击者同时向一个或几个目标发动攻击，或者一个攻击者控制了位于不同位置的多台机器，并利用这些机器对受害者同时实施攻击。主要形式有流量攻击和资源耗尽攻击，常见的 DDoS攻击有： **SYN Flood、Ping of Death、ACK Flood、UDP Flood** 等。
- **DRDoS**: (Distributed Reflection Denial of Service)，中文是分布式反射拒绝服务，该方式靠的是发送大量带有被害者 IP 地址的数据包给攻击主机，然后攻击主机对 IP 地址源做出大量回应，从而形成拒绝服务攻击。

> **如何防范DDoS?**

针对DDoS中的流量攻击，最直接的方法是增加带宽，理论上只要带宽大于攻击流量就可以了，但是这种方法成本非常高。在有充足带宽的前提下，我们应该尽量提升路由器、网卡、交换机等硬件设施的配置。

针对资源耗尽攻击，我们可以升级主机服务器硬件，在网络带宽得到保证的前提下，使得服务器能够有效对抗海量的SYN攻击包。我们也可以安装专业的抗DDoS防火墙，从而对抗SYN Flood等流量型攻击。瓷碗，负载均衡，CDN等技术都能有效对抗DDos攻击。

**60.什么是 XSS 攻击，如何避免?**

XSS 攻击也是比较常见，XSS，叫**跨站脚本攻击（Cross-Site Scripting）**，因为会与层叠样式表 (Cascading Style Sheets, CSS) 的缩写混淆，因此有人将跨站脚本攻击缩写为 XSS。它指的是恶意攻击者往 Web 页面里插入恶意 html 代码，当用户浏览网页的时候，嵌入其中 Web 里面的 html 代码会被执行，从而达到恶意攻击用户的特殊目的。

XSS 攻击一般分三种类型：**存储型 、反射型 、DOM 型 XSS**

> **XSS 是如何攻击的呢？**

简单说，XSS的攻击方式就是想办法“教唆”用户的浏览器去执行一些这个网页中原本不存在的前端代码。

拿反射型举个例子吧，流程图如下：

1. 攻击者构造出特殊的 URL，其中包含恶意代码。
2. 用户打开带有恶意代码的 URL 时，访问正常网站服务器
3. 网站服务端将恶意代码从 URL 中取出，拼接在 HTML 中返回给浏览器。
4. 用户浏览器接收到响应后解析执行，混在其中的恶意代码也被执行，请求恶意服务器，发送用户数据
5. 攻击者就可以窃取用户的数据，以此冒充用户的行为，调用目标网站接口执行攻击者指定的操作。

![img](https://pic1.zhimg.com/80/v2-e9fef7770c0d0a42e66607b3fefc6a68_720w.webp)

> **如何应对 XSS 攻击？**

- 对输入进行过滤，过滤标签等，只允许合法值。
- HTML 转义
- 对于链接跳转，如<a href="xxx" 等，要校验内容，禁止以 script 开头的非法链接。
- 限制输入长度

**61.对称加密与非对称加密有什么区别？**

**对称加密**：指加密和解密使用同一密钥，优点是运算速度较快，缺点是如何安全将密钥传输给另一方。常见的对称加密算法有：DES、AES 等。

![img](https://pic3.zhimg.com/80/v2-f4e23be07172ce8ff1d202dab0948bde_720w.webp)

**非对称加密**：指的是加密和解密使用不同的密钥（即公钥和私钥）。公钥与私钥是成对存在的，如果用公钥对数据进行加密，只有对应的私钥才能解密。常见的非对称加密算法有 RSA。

![img](https://pic2.zhimg.com/80/v2-b775a590eb303857dc8517ac243de7d1_720w.webp)

**62.RSA和AES算法有什么区别？**

- **RSA**采用非对称加密的方式，采用公钥进行加密，私钥解密的形式。其私钥长度一般较长，由于需要大数的乘幂求模等运算，其运算速度较慢，不合适大量数据文件加密。
- **AES**采用对称加密的方式，其秘钥长度最长只有256个比特，加密和解密速度较快，易于硬件实现。由于是对称加密，通信双方在进行数据传输前需要获知加密密钥。

原文地址：https://zhuanlan.zhihu.com/p/466239718

作者：linux

# 【NO.435】深入剖析阻塞式socket的timeout

## 1.前言

**网络编程中超时时间是一个重要但又容易被忽略的问题,对其的设置需要仔细斟酌。**

本文讨论的是socket设置为阻塞模式，如果socket处于阻塞模式运行时，就需要考虑处理socket操作超时的问题。
所谓阻塞模式，是指其完成指定的操作之前阻塞当前的进程或线程，直到操作有结果返回.
在我们直接调用socket操作函数时，如果不进行特意声明的话，它们都是工作在阻塞模式的，
如 connect, send, recv等.

**简单分类的话，可以将超时处理分成两类：**

**连接(connect)超时;**
**发送(send), 接收(recv)超时;**

## 2.连接超时

从字面上看，连接超时就是在一定时间内还是连接不上目标主机。你所建立的socket连接其实最终都要进行系统调用进入内核态，剩下的就是等待内核通知连接建立。所以自行在代码中设置了超时时间（一般是叫connectTimeout或者socketTimeout），那么这个超时时间一到如果内核还没成功建立连接，那就认为是连接超时了。如果他们没设置超时时间，那么这个connectTimeout就取决于内核什么时候抛出超时异常了。

因此，我们需要分析一下内核是怎么来判断连接超时的。

**内核层的超时分析**

我们都知道一个连接的建立需要经过3次握手，所以连接超时简单的说是是客户端往服务端发的SYN报文没有得到响应（服务端没有返回ACK报文）。

由于网络本身是不稳定的，丢包是很常见的事情（或者对方主机因为某些原因丢弃了该包），因此内核在发送SYN报文没有得到响应后，往往还是进行多次重试。同时，为了避免发送太多的包影响网络，重试的时间间隔还会不断增加。

在linux中，重试的时间间隔会呈指数型增长，为2的N次方，即：

第一次发送SYN报文后等待1s（2的0次幂）后再重试

第二次发送SYN报文后等待2s（2的1次幂）后再重试

第三次发送SYN报文后等待4s（2的2次幂）后再重试

第四次发送SYN报文后等待8s（2的3次幂）后再重试

第五次发送SYN报文后等待16s（2的4次幂）后再重试

第六次发送SYN报文后等待32s（2的5次幂）后再重试

第七次发送SYN报文后等待64s（2的6次幂）后再重试

对于重试次数，由linux的net.ipv4.tcp_syn_retries来确定，默认值一般是6（有些linux发行版可能不太一样），我们可以通过sysctl net.ipv4.tcp_syn_retries查看。比如重试次数是6次，那么我们可以得出超时时间应该是 1+2+4+8+16+32+64=127秒 （上面的第一条是第一次发送SYN报文，不算重试）。

如果我们想修改重试次数，可以输入命令sysctl -w net.ipv4.tcp_syn_retries=5来修改（需要root权限）。如果希望重启后生效，将net.ipv4.tcp_syn_retries = 5放入/etc/sysctl.conf中，之后执行sysctl -p 即可生效。

在一些linux发行版中，重试时间可能会变动。如果想确定操作系统具体的超时时间，可以通过下面这条命令来判断：

```text
gaoke@ubuntu:~$ date; telnet 10.16.15.15 5000; date
Sat Apr  2 14:27:33 CST 2022
Trying 10.16.15.15...
telnet: Unable to connect to remote host: Connection timed out
Sat Apr  2 14:29:40 CST 2022
```

**综合分析**

如果应用层面设置了自己的超时时间，同时内核也有自己的超时时间，那么应该以哪个为准呢？答案是哪个超时时间小以哪个为准。

个人认为，在我们的实际应用中，这个超时时间不宜设置的太长，通常建议2-10s。比如在分布式系统中，我们通常会在多台节点中根据一定策略选择一台进行连接。在有机器宕机的情况下，如果连接超时时间设置的比较长，而我们客户端的线程池又比较小，就很可能大多数的线程都在等待建立连接，过了较长时间才发现连接不上，影响应用的整体吞吐量。

**connect系统调用**

我们观察一下此系统调用的kernel源码，调用栈如下所示:

```text
connect[用户态]
    |->SYSCALL_DEFINE3(connect)[内核态]
            |->sock->ops->connect
```

![img](https://pic2.zhimg.com/80/v2-1f2b925d0c0cf1e582a940e12cfd9c21_720w.webp)

最终调用的tcp_connect源码如下：

```text
int tcp_connect(struct sock *sk) {
    ......
    // 发送SYN
    err = tcp_transmit_skb(sk, buff, 1, sk->sk_allocation);
    ...
    /* Timer for repeating the SYN until an answer. */
    // 由于是刚建立连接，所以其rto是TCP_TIMEOUT_INIT
    inet_csk_reset_xmit_timer(sk, ICSK_TIME_RETRANS,
                inet_csk(sk)->icsk_rto, TCP_RTO_MAX);
    return 0;    
}
```

又上面代码可知，在tcp_connect设置了重传定时器之后return回了tcp_v4_connect再return到inet_stream_connect。

我们可以采用设置SO_SNDTIMEO来控制connect系统调用的超时,如下所示:

```text
setsockopt(sockfd, SOL_SOCKET, SO_SNDTIMEO, &timeout, len);
```

**不设置SO_SNDTIMEO**

如果不设置SO_SNDTIMEO,那么会由tcp重传定时器在重传超过设置的时候后超时,如下图所示:

![img](https://pic4.zhimg.com/80/v2-b63b820c7acc39c804edc6169a1e8cff_720w.webp)

我们如何查看syn重传次数？:

```text
cat /proc/sys/net/ipv4/tcp_syn_retries
```

![img](https://pic1.zhimg.com/80/v2-a7ecd3ce874b2ae2c0d6a582d78cf524_720w.webp)

对于系统调用，connect的超时时间为:

| tcp_syn_retries | timeout              |
| --------------- | -------------------- |
| 1               | min(so_sndtimeo,3s)  |
| 2               | min(so_sndtimeo,7s)  |
| 3               | min(so_sndtimeo,15s) |
| 4               | min(so_sndtimeo,31s) |
| 5               | min(so_sndtimeo,63s) |

kernel代码版本细微变化

值得注意的是，linux本身官方发布的2.6.32源码对于tcp_syn_retries2的解释和RFC并不一致,不同内核小版本上的实验会有不同的connect timeout表现的原因(有的抓包到的重传SYN时间间隔为3,6,12......)。

以下为代码对比:

```text
========================>linux 内核版本2.6.32-431<========================
#define TCP_TIMEOUT_INIT ((unsigned)(1*HZ))    /* RFC2988bis initial RTO value    */

static inline bool retransmits_timed_out(struct sock *sk,
                     unsigned int boundary,
                     unsigned int timeout,
                     bool syn_set)
{
    ......
    unsigned int rto_base = syn_set ? TCP_TIMEOUT_INIT : TCP_RTO_MIN;
    ......
    timeout = ((2 << boundary) - 1) * rto_base;
    ......

}
========================>linux 内核版本2.6.32.63<========================
#define TCP_TIMEOUT_INIT ((unsigned)(3*HZ))    /* RFC 1122 initial RTO value    */

static inline bool retransmits_timed_out(struct sock *sk,
                     unsigned int boundary
{
    ......
    timeout = ((2 << boundary) - 1) * TCP_RTO_MIN;
    ......
}
```

另外，tcp_syn_retries重传次数可以在单个socket中通过setsockopt设置。

## 3.发送超时

**在tcp连接建立之后，写操作可以理解为向对端发送tcp报文的过程。在tcp的实现中，每一段报文都需要有对端的回应，即ACK报文。和连接时发送SYN报文一样，如果超过一定时间没有收到响应，内核会再次重发该报文。和SYN报文的重试不同的是，linux有另外的参数来控制这个重试次数，即net.ipv4.tcp_retries2，可以通过sysctl net.ipv4.tcp_retries2查看其值。**

另外，这个数据报文重试时间间隔的计算方式也和SYN报文不一样，由于计算方式比较复杂，这里就不详细介绍。

一般linux发行版的net.ipv4.tcp_retries2的默认值为5或者15，对应的超时时间如下表：

tcp_retries2对端无响应

525.6s-51.2s,根据动态rto定

15924.6s-1044.6s,根据动态rto定

和SYN报文的超时时间一样，如果应用层设置了超时时间，哪么具体的超时时间以内核和应用层的超时时间的最小值为准。

socket的write系统调用最后调用的是tcp_sendmsg,源码如下所示:

```text
int tcp_sendmsg(struct kiocb *iocb, struct socket *sock, struct msghdr *msg,
        size_t size){
    ......
    timeo = sock_sndtimeo(sk, flags & MSG_DONTWAIT);
    ......
    while (--iovlen >= 0) {
        ......
        // 此种情况是buffer不够了
        if (copy <= 0) {
    new_segment:
          ......
          if (!sk_stream_memory_free(sk))
              goto wait_for_sndbuf;

          skb = sk_stream_alloc_skb(sk, select_size(sk),sk->sk_allocation);
          if (!skb)
              goto wait_for_memory;
        }
        ......
    }
    ......
    // 这边等待write buffer有空间
wait_for_sndbuf:
        set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
wait_for_memory:
        if (copied)
            tcp_push(sk, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);
            // 这边等待timeo长的时间
        if ((err = sk_stream_wait_memory(sk, &timeo)) != 0)
            goto do_error;
        ......
out:
    // 如果拷贝了数据，则返回
    if (copied)
        tcp_push(sk, flags, mss_now, tp->nonagle);
    TCP_CHECK_TIMER(sk);
    release_sock(sk);
    return copied;        
out_err:
    // error的处理
    err = sk_stream_error(sk, flags, err);
    TCP_CHECK_TIMER(sk);
    release_sock(sk);
    return err;        
}
```

从上面的内核代码看出，如果socket的write buffer依旧有空间的时候，会立马返回，并不会有timeout。但是write buffer不够的时候，会等待SO_SNDTIMEO的时间(nonblock时候为0)。但是如果SO_SNDTIMEO没有设置的时候,默认初始化为MAX_SCHEDULE_TIMEOUT,可以认为其超时时间为无限。那么其超时时间会有另一个条件来决定，我们看下sk_stream_wait_memory的源码:

```text
int sk_stream_wait_memory(struct sock *sk, long *timeo_p){
        // 等待socket shutdown或者socket出现err
        sk_wait_event(sk, ¤t_timeo, sk->sk_err ||
                          (sk->sk_shutdown & SEND_SHUTDOWN) ||
                          (sk_stream_memory_free(sk) &&
                          !vm_wait));
}
```

在write等待的时候，如果出现socket被shutdown或者socket出现错误的时候，则会跳出wait进而返回错误。在不考虑对端shutdown的情况下,出现sk_err的时间其实就是其write的timeout时间,那么我们看下什么时候出现sk->sk_err。

**SO_SNDTIMEO不设置,write buffer满之后ack一直不返回的情况(例如，物理机宕机)**

物理机宕机后，tcp发送msg的时候,ack不会返回，则会在重传定时器tcp_retransmit_timer到期后timeout,其重传到期时间通过tcp_retries2以及TCP_RTO_MIN计算出来。

tcp_retries2的设置位置为:

```text
cat /proc/sys/net/ipv4/tcp_retries2
```

**SO_SNDTIMEO不设置,write buffer满之后对端不消费，导致buffer一直满的情况**

和上面ack超时有些许不一样的是，一个逻辑是用TCP_RTO_MIN通过tcp_retries2计算出来的时间。另一个是真的通过重传超过tcp_retries2次数来time_out，两者的区别和rto的动态计算有关。但是可以大致认为是一致的。

**上述逻辑如下图所示:**

![img](https://pic2.zhimg.com/80/v2-7dfb1c3a893d4a66f0f8f775eb838f1d_720w.webp)

**write_timeout表格**

| tcp_retries2 | buffer未满 | buffer满                                      |
| ------------ | ---------- | --------------------------------------------- |
| 5            | 立即返回   | min(SO_SNDTIMEO,(25.6s-51.2s)根据动态rto定    |
| 15           | 立即返回   | min(SO_SNDTIMEO,(924.6s-1044.6s)根据动态rto定 |

## 4.接收超时

**在tcp协议中，读的操作和写操作的逻辑是相通的。**

tcp连接建立后，两边的通信无非就是报文的互传。写操作是将数据放到tcp报文中发送给对端，然后等待对端响应，一定时间没有得到响应就是超时。而读操作其实就是发送一个读取数据的报文给对端，然后对端返回带有数据的报文，一定时间没有收到对端的报文则认为超时。对于tcp协议而言，其实不会分辨他们发送的报文具体是要干嘛，因此readTimeout的判断逻辑和writeTimeout基本一样。它的重传次数也是由参数net.ipv4.tcp_retries2控制。在应用层面也一般是统一叫socketTimeout。

**read系统调用**

socket的read系统调用最终调用的是tcp_recvmsg, 其源码如下:

```text
int tcp_recvmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
        size_t len, int nonblock, int flags, int *addr_len)
{
    ......
    // 这边timeo=SO_RCVTIMEO
    timeo = sock_rcvtimeo(sk, nonblock);
    ......
    do{
        ......
        // 下面这一堆判断表明，如果出现错误,或者已经被CLOSE/SHUTDOWN则跳出循环
        if(copied) {
            if (sk->sk_err ||
                sk->sk_state == TCP_CLOSE ||
                (sk->sk_shutdown & RCV_SHUTDOWN) ||
                !timeo ||
                signal_pending(current))
                break;
        } else {
            if (sock_flag(sk, SOCK_DONE))
                break;

            if (sk->sk_err) {
                copied = sock_error(sk);
                break;
            }
            // 如果socket shudown跳出
            if (sk->sk_shutdown & RCV_SHUTDOWN)
                break;
            // 如果socket close跳出
            if (sk->sk_state == TCP_CLOSE) {
                if (!sock_flag(sk, SOCK_DONE)) {
                    /* This occurs when user tries to read
                     * from never connected socket.
                     */
                    copied = -ENOTCONN;
                    break;
                }
                break;
            }
            .......
        }
        .......

        if (copied >= target) {
            /* Do not sleep, just process backlog. */
            release_sock(sk);
            lock_sock(sk);
        } else /* 如果没有读到target自己数(和水位有关,可以暂认为是1)，则等待SO_RCVTIMEO的时间 */
            sk_wait_data(sk, &timeo);    
    } while (len > 0);
    ......
}
```

上面的逻辑如下图所示:

![img](https://pic1.zhimg.com/80/v2-5670ad1fb88cb0dec2c45c31e0abd03c_720w.webp)

重传以及探测定时器timeout事件的触发时机如下图所示:

![img](https://pic3.zhimg.com/80/v2-0323bbdba2c5e5f382e541e01b84e386_720w.webp)

如果内核层面ack正常返回而且对端窗口不为0，仅仅应用层不返回任何数据,那么就会无限等待，直到对端有数据或者socket close/shutdown为止，如下图所示:

![img](https://pic4.zhimg.com/80/v2-e7edb495529db43528b4e217ccf0d20f_720w.webp)

很多应用就是基于这个无限超时来设计的,例如activemq的消费者逻辑。

**ReadTimeout超时表格**

C系统调用:

| tcp_retries2 | 对端无响应                                    | 对端内核响应正常                 |
| ------------ | --------------------------------------------- | -------------------------------- |
| 5            | min(SO_RCVTIMEO,(25.6s-51.2s)根据动态rto定    | SO_RCVTIMEO==0?无限,SO_RCVTIMEO) |
| 15           | min(SO_RCVTIMEO,(924.6s-1044.6s)根据动态rto定 | SO_RCVTIMEO==0?无限,SO_RCVTIMEO) |

Java系统调用

| tcp_retries2 | 对端无响应                                   | 对端内核响应正常               |      |
| ------------ | -------------------------------------------- | ------------------------------ | ---- |
| 5            | min(SO_TIMEOUT,(25.6s-51.2s)根据动态rto定    | SO_TIMEOUT==0?无限,SO_RCVTIMEO |      |
| 15           | min(SO_TIMEOUT,(924.6s-1044.6s)根据动态rto定 | SO_TIMEOUT==0?无限,SO_RCVTIMEO |      |

## 5.对端物理机宕机之后的超时

**对端物理机宕机后还依旧有数据发送**

对端物理机宕机时对端内核也gg了(不会发出任何包通知宕机)，那么本端发送任何数据给对端都不会有响应。其超时时间就由上面讨论的
min(设置的socket超时[例如SO_TIMEOUT],内核内部的定时器超时来决定)。

**对端物理机宕机后没有数据发送，但在read等待**

这时候如果设置了超时时间timeout，则在timeout后返回。但是，如果仅仅是在read等待，由于底层没有数据交互，那么其无法知道对端是否宕机，所以会一直等待。但是，内核会在一个socket两个小时都没有数据交互情况下(可设置)启动keepalive定时器来探测对端的socket。如下图所示:

![img](https://pic3.zhimg.com/80/v2-b4cbb2fff7bff2ff37dfb5fac067b86e_720w.webp)

大概是2小时11分钟之后会超时返回。keepalive的设置由内核参数指定：

```text
cat /proc/sys/net/ipv4/tcp_keepalive_time 7200 即两个小时后开始探测
cat /proc/sys/net/ipv4/tcp_keepalive_intvl 75 即每次探测间隔为75s
cat /proc/sys/net/ipv4/tcp_keepalve_probes 9 即一共探测9次
```

可以在setsockops中对单独的socket指定是否启用keepalive定时器。

**对端物理机宕机后没有数据发送，也没有read等待**

和上面同理，也是在keepalive定时器超时之后，将连接close。所以我们可以看到一个不活跃的socket在对端物理机突然宕机之后,依旧是ESTABLISHED状态，过很长一段时间之后才会关闭。

## 6.进程宕机后的超时

物理机突然宕机和进程宕掉的表现不一样。一个tcp连接建立后，如果一端的物理机突然宕机，另外一端是完全不知情的，它会像往常一样继续发送相关报文，直到超时时间到了才返回。另外，一般操作系统会有机制检测来释放该tcp连接。而如果只是进程宕掉，在进程退出的时候，操作会负责回收这个进程所属的所有tcp连接，在这时会向这些tcp连接的对端发送FIN报文，表示要关闭连接了，这时候对端是可以知道连接已经关闭的。（如果进程退出后还收到来自对端的报文，那么内核会立马发送reset给对端，从而不会卡住对端的线程资源）

所以如果仅仅是对端进程宕机的话(进程所在内核会close其所拥有的所有socket)，由于fin包的发送，本端内核可以立刻知道当前socket的状态。如果socket是阻塞的，那么将会在当前或者下一次write/read系统调用的时候返回给应用层相应的错误。如果是nonblock，那么会在select/epoll中触发出对应的事件通知应用层去处理。
如果fin包没发送到对端，那么在下一次write/read的时候内核会发送reset包作为回应。

**nonblock**

设置为nonblock=true后，由于read/write都是立刻返回，且通过select/epoll等处理重传超时/probe超时/keep alive超时/socket close等事件，所以根据应用层代码决定其超时特性。定时器超时事件发生的时间如上面几小节所述，和是否nonblock无关。nonblock的编程模式可以让应用层对这些事件做出响应。

## 7.总结

网络编程中超时时间是个重要但又容易被忽略的问题，这个问题只有在遇到物理机宕机，偶尔的网络抖动等平时遇不到的现象时候才会凸显。希望本篇文章可以对读者在以后遇到类似超时问题时有所帮助,只要涉及到阻塞式的网络请求，就一定要加超时，否则你将进入自己曾经种下的苦果，加班，掉的不是汗水，是头发。

原文地址：https://zhuanlan.zhihu.com/p/535405145

作者：linux

# 【NO.436】深入理解 Linux 的 epoll 机制及epoll原理

![img](https://pic1.zhimg.com/80/v2-bd16c336192cdde813971183e864e334_720w.webp)

在 Linux 系统之中有一个核心武器：epoll 池，在高并发的，高吞吐的 IO 系统中常常见到 epoll 的身影。

## 1.IO 多路复用

在 Go 里最核心的是 Goroutine ，也就是所谓的协程，协程最妙的一个实现就是异步的代码长的跟同步代码一样。比如在 Go 中，网络 IO 的 read，write 看似都是同步代码，其实底下都是异步调用，一般流程是：

```
write ( /* IO 参数 */ )
请求入队
等待完成
后台 loop 程序
发送网络请求
唤醒业务方
```

Go 配合协程在网络 IO 上实现了异步流程的同步代码化。核心就是用 epoll 池来管理网络 fd 。

实现形式上，后台的程序只需要 1 个就可以负责管理多个 fd 句柄，负责应对所有的业务方的 IO 请求。这种一对多的 IO 模式我们就叫做 IO 多路复用。

**多路是指**？多个业务方（句柄）并发下来的 IO 。

**复用是指**？复用这一个后台处理程序。

站在 IO 系统设计人员的角度，业务方咱们没办法提要求，因为业务是上帝，只有你服从的份，他们要创建多个 fd，那么你就需要负责这些 fd 的处理，并且最好还要并发起来。

业务方没法提要求，那么只能要求后台 loop 程序了！

要求什么呢？快！快！快！这就是最核心的要求，处理一定要快，要给每一个 fd 通道最快的感受，要让每一个 fd 觉得，你只在给他一个人跑腿。

那有人又问了，那我一个 IO 请求（比如 write ）对应一个线程来处理，这样所有的 IO 不都并发了吗？是可以，但是有瓶颈，线程数一旦多了，性能是反倒会差的。

> 这里不再对比多线程和 IO 多路复用实现高并发之间的区别，详细的可以去了解下 nginx 和 redis 高并发的秘密。

## 2.最朴实的实现方式？

我不用任何其他系统调用，能否实现 IO 多路复用？

可以的。那么写个 for 循环，每次都尝试 IO 一下，读/写到了就处理，读/写不到就 sleep 下。这样我们不就实现了 1 对多的 IO 多路复用嘛。

```
while True:
for each 句柄数组 {
read/write(fd, /* 参数 */)
}
sleep(1s)
```

慢着，有个问题，上面的程序可能会被卡死在第三行，使得整个系统不得运行，为什么？

默认情况下，我们没有加任何参数 create 出的句柄是阻塞类型的。我们读数据的时候，如果数据还没准备好，是会需要等待的，当我们写数据的时候，如果还没准备好，默认也会卡住等待。所以，在上面伪代码**第三行**是可能被直接卡死，而导致整个线程都得到不到运行。

举个例子，现在有 11，12，13 这 3 个句柄，现在 11 读写都没有准备好，只要 read/write(11, /*参数*/) 就会被卡住，但 12，13 这两个句柄都准备好了，那遍历句柄数组 11，12，13 的时候就会卡死在前面，后面 12，13 则得不到运行。这不符合我们的预期，因为我们 IO 多路复用的 loop 线程是公共服务，不能因为一个 fd 就直接瘫痪。

那这个问题怎么解决？

**只需要把 fd 都设置成非阻塞模式**。这样 read/write 的时候，如果数据没准备好，返回 EAGIN 的错误即可，不会卡住线程，从而整个系统就运转起来了。比如上面句柄 11 还未就绪，那么 read/write(11, /*参数*/) 不会阻塞，只会报个 EAGIN 的错误，这种错误需要特殊处理，然后 loop 线程可以继续执行 12，13 的读写。

以上就是**最朴实的 IO 多路复用**的实现了。但是好像在生产环境没见过这种 IO 多路复用的实现？为什么？

因为还不够高级。for 循环每次要定期 sleep 1s，这个会导致吞吐能力极差，因为很可能在刚好要 sleep 的时候，所有的 fd 都准备好 IO 数据，而这个时候却要硬生生的等待 1s，可想而知。。。

那有同学又要质疑了，那 for 循环里面就不 sleep 嘛，这样不就能及时处理了吗？

及时是及时了，但是 CPU 估计要跑飞了。不加 sleep ，那在没有 fd 需要处理的时候，估计 CPU 都要跑到 100% 了。这个也是无法接受的。

纠结了，那 sleep 吞吐不行，不 sleep 浪费 cpu，怎么办？

这种情况用户态很难有所作为，只能求助内核来提供机制协助来。因为内核才能及时的管理这些通知和调度。

我们再梳理下 IO 多路复用的需求和原理。IO 多路复用就是 1 个线程处理 多个 fd 的模式。我们的要求是：这个 “1” 就要尽可能的快，避免一切无效工作，**要把所有的时间都用在处理句柄的 IO 上，不能有任何空转，sleep 的时间浪费。**

有没有一种工具，我们把一箩筐的 fd 放到里面，只要有一个 fd 能够读写数据，后台 loop 线程就要立马唤醒，全部马力跑起来。其他时间要把 cpu 让出去。

能做到吗？能，这种需求只能内核提供机制满足你。

## 3.这事 Linux 内核必须要给个说法？

是的，想要不用 sleep 这种辣眼睛的实现，Linux 内核必须出手了，毕竟 IO 的处理都是内核之中，数据好没好内核最清楚。

内核一口气提供了 3 种工具 select，poll，epoll 。

为什么有 3 种？

历史不断改进，矬 -> 较矬 -> 卧槽、高效 的演变而已。

![img](https://pic3.zhimg.com/80/v2-735efb2999f75ac206e822b3b1c94cae_720w.webp)

Linux 还有其他方式可以实现 IO 多路复用吗？

好像没有了！

这 3 种到底是做啥的？

这 3 种都能够管理 fd 的可读可写事件，在所有 fd 不可读不可写无所事事的时候，可以阻塞线程，切走 cpu 。fd 有情况的时候，都要线程能够要能被唤醒。

而这三种方式以 epoll 池的效率最高。为什么效率最高？

其实很简单，这里不详说，其实无非就是 epoll 做的无用功最少，select 和 poll 或多或少都要多余的拷贝，盲猜（遍历才知道）fd ，所以效率自然就低了。

举个例子，以 select 和 epoll 来对比举例，池子里管理了 1024 个句柄，loop 线程被唤醒的时候，select 都是蒙的，都不知道这 1024 个 fd 里谁 IO 准备好了。这种情况怎么办？只能遍历这 1024 个 fd ，一个个测试。假如只有一个句柄准备好了，那相当于做了 1 千多倍的无效功。

epoll 则不同，从 epoll_wait 醒来的时候就能精确的拿到就绪的 fd 数组，不需要任何测试，拿到的就是要处理的。

## 4.epoll 池原理

下面我们看一下 epoll 池的使用和原理。

## 5.epoll 涉及的系统调用

epoll 的使用非常简单，只有下面 3 个系统调用。

```
epoll_create
epollctl
epollwait
```

就这？是的，就这么简单。

- epollcreate 负责创建一个池子，一个监控和管理句柄 fd 的池子；
- epollctl 负责管理这个池子里的 fd 增、删、改；
- epollwait 就是负责打盹的，让出 CPU 调度，但是只要有“事”，立马会从这里唤醒；

## 6.epoll 高效的原理

Linux 下，epoll 一直被吹爆，作为高并发 IO 实现的秘密武器。其中原理其实非常朴实：**epoll 的实现几乎没有做任何无效功。** 我们从使用的角度切入来一步步分析下。

首先，epoll 的第一步是创建一个池子。这个使用 epoll_create 来做：

原型：

```
int epoll_create(int size);
```

示例：

```
epollfd = epoll_create(1024);
if (epollfd == -1) {
perror("epoll_create");
exit(EXIT_FAILURE);
}
```

这个池子对我们来说是黑盒，这个黑盒是用来装 fd 的，我们暂不纠结其中细节。我们拿到了一个 epollfd ，这个 epollfd 就能唯一代表这个 epoll 池。

然后，我们就要往这个 epoll 池里放 fd 了，这就要用到 epoll_ctl 了

原型：

```
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
```

示例：

```
if (epoll_ctl(epollfd, EPOLL_CTL_ADD, 11, &ev) == -1) {
perror("epoll_ctl: listen_sock");
exit(EXIT_FAILURE);
}
```

上面，我们就把句柄 11 放到这个池子里了，op（EPOLL_CTL_ADD）表明操作是增加、修改、删除，event 结构体可以指定监听事件类型，可读、可写。

**第一个跟高效相关的问题来了，添加 fd 进池子也就算了，如果是修改、删除呢？怎么做到时间快？**

这里就涉及到你怎么管理 fd 的数据结构了。

最常见的思路：用 list ，可以吗？功能上可以，但是性能上拉垮。list 的结构来管理元素，时间复杂度都太高 O(n)，每次要一次次遍历链表才能找到位置。池子越大，性能会越慢。

那有简单高效的数据结构吗？

有，红黑树。Linux 内核对于 epoll 池的内部实现就是用红黑树的结构体来管理这些注册进程来的句柄 fd。红黑树是一种平衡二叉树，时间复杂度为 O(log n)，就算这个池子就算不断的增删改，也能保持非常稳定的查找性能。

**现在思考第二个高效的秘密：怎么才能保证数据准备好之后，立马感知呢？**

epoll_ctl 这里会涉及到一点。**秘密就是：回调的设置**。在 epoll_ctl 的内部实现中，除了把句柄结构用红黑树管理，另一个核心步骤就是设置 poll 回调。

**思考来了：poll 回调是什么？怎么设置？**

先说说 file_operations->poll 是什么？

在 fd 篇 说过，Linux 设计成一切皆是文件的架构，这个不是说说而已，而是随处可见。实现一个文件系统的时候，就要实现这个文件调用，这个结构体用 struct file_operations 来表示。这个结构体有非常多的函数，我精简了一些，如下：

```
struct file_operations {
ssize_t (*read) (struct file *, char __user *, size_t, loff_t *);
ssize_t (*write) (struct file *, const char __user *, size_t, loff_t *);
__poll_t (*poll) (struct file *, struct poll_table_struct *);
int (*open) (struct inode *, struct file *);
int (*fsync) (struct file *, loff_t, loff_t, int datasync);
// ....
};
```

你看到了 read，write，open，fsync，poll 等等，这些都是对文件的定制处理操作，对于文件的操作其实都是在这个框架内实现逻辑而已，比如 ext2 如果有对 read/write 做定制化，那么就会是 ext2_read，ext2_write，ext4 就会是 ext4_read，ext4_write。在 open 具体“文件”的时候会赋值对应文件系统的 file_operations 给到 file 结构体。

那我们很容易知道 read 是文件系统定制 fd 读的行为调用，write 是文件系统定制 fd 写的行为调用，file_operations->poll 呢？

这个是定制监听事件的机制实现。通过 poll 机制让上层能直接告诉底层，我这个 fd 一旦读写就绪了，请底层硬件（比如网卡）回调的时候自动把这个 fd 相关的结构体放到指定队列中，并且唤醒操作系统。

举个例子：网卡收发包其实走的异步流程，操作系统把数据丢到一个指定地点，网卡不断的从这个指定地点掏数据处理。请求响应通过中断回调来处理，中断一般拆分成两部分：硬中断和软中断。poll 函数就是把这个软中断回来的路上再加点料，只要读写事件触发的时候，就会立马通知到上层，采用这种事件通知的形式就能把浪费的时间窗就完全消失了。

**划重点：这个 poll 事件回调机制则是 epoll 池高效最核心原理。**

**划重点：epoll 池管理的句柄只能是支持了 file_operations->poll 的文件 fd。换句话说，如果一个“文件”所在的文件系统没有实现 poll 接口，那么就用不了 epoll 机制。**

**第二个问题：poll 怎么设置？**

在 epoll_ctl 下来的实现中，有一步是调用 vfs_poll 这个里面就会有个判断，如果 fd 所在的文件系统的 file_operations 实现了 poll ，那么就会直接调用，如果没有，那么就会报告响应的错误码。

```
static inline __poll_t vfs_poll(struct file *file, struct poll_table_struct *pt)
{
if (unlikely(!file->f_op->poll))
return DEFAULT_POLLMASK;
return file->f_op->poll(file, pt);
}
```

你肯定好奇 poll 调用里面究竟是实现了什么？

总结概括来说：挂了个钩子，设置了唤醒的回调路径。epoll 跟底层对接的回调函数是：ep_poll_callback，这个函数其实很简单，做两件事情：

1. 把事件就绪的 fd 对应的结构体放到一个特定的队列（就绪队列，ready list）；
2. 唤醒 epoll ，活来啦！

当 fd 满足可读可写的时候就会经过层层回调，最终调用到这个回调函数，把**对应 fd 的结构体**放入就绪队列中，从而把 epoll 从 epoll_wait 出唤醒。

这个对应结构体是什么？

结构体叫做 epitem ，每个注册到 epoll 池的 fd 都会对应一个。

就绪队列很高级吗？

就绪队列就简单了，因为没有查找的需求了呀，只要是在就绪队列中的 epitem ，都是事件就绪的，必须处理的。所以就绪队列就是一个最简单的双指针链表。

**小结下：epoll 之所以做到了高效，最关键的两点：**

1. 内部管理 fd 使用了高效的红黑树结构管理，做到了增删改之后性能的优化和平衡；
2. epoll 池添加 fd 的时候，调用 file_operations->poll ，把这个 fd 就绪之后的回调路径安排好。通过事件通知的形式，做到最高效的运行；
3. epoll 池核心的两个数据结构：红黑树和就绪列表。红黑树是为了应对用户的增删改需求，就绪列表是 fd 事件就绪之后放置的特殊地点，epoll 池只需要遍历这个就绪链表，就能给用户返回所有已经就绪的 fd 数组；

![img](https://pic2.zhimg.com/80/v2-3b97ee5434014b8f0db2c76296c0f195_720w.webp)

## 7.哪些 fd 可以用 epoll 来管理？

再来思考另外一个问题：由于并不是所有的 fd 对应的文件系统都实现了 poll 接口，所以自然并不是所有的 fd 都可以放进 epoll 池，那么有哪些文件系统的 file_operations 实现了 poll 接口？

首先说，类似 ext2，ext4，xfs 这种常规的文件系统是没有实现的，换句话说，这些你最常见的、真的是文件的文件系统反倒是用不了 epoll 机制的。

那谁支持呢？

最常见的就是网络套接字：socket 。网络也是 epoll 池最常见的应用地点。Linux 下万物皆文件，socket 实现了一套 socket_file_operations 的逻辑（ net/socket.c ）：

```
static const struct file_operations socket_file_ops = {
.read_iter = sock_read_iter,
.write_iter = sock_write_iter,
.poll = sock_poll,
// ...
};
```

我们看到 socket 实现了 poll 调用，所以 socket fd 是天然可以放到 epoll 池管理的。

还有吗？

有的，其实 Linux 下还有两个很典型的 fd ，常常也会放到 epoll 池里。

- eventfd：eventfd 实现非常简单，故名思义就是专门用来做事件通知用的。使用系统调用 eventfd 创建，这种文件 fd 无法传输数据，只用来传输事件，常常用于生产消费者模式的事件实现；
- timerfd：这是一种定时器 fd，使用 timerfd_create 创建，到时间点触发可读事件；

**小结一下**：

1. ext2，ext4，xfs 等这种真正的文件系统的 fd ，无法使用 epoll 管理；
2. socket fd，eventfd，timerfd 这些实现了 poll 调用的可以放到 epoll 池进行管理；

其实，在 Linux 的模块划分中，eventfd，timerfd，epoll 池都是文件系统的一种模块实现。

![img](https://pic4.zhimg.com/80/v2-ba39e534ed4d8a6811521eea743db32f_720w.webp)

## 8.思考

前面我们已经思考了很多知识点，有一些简单有趣的知识点，提示给读者朋友，这里只抛砖引玉。

问题：单核 CPU 能实现并行吗？

不行。

问题：单线程能实现高并发吗？

可以。

问题：那并发和并行的区别是？

一个看的是**时间段内**的执行情况，一个看的是时间**时刻**的执行情况。

问题：单线程如何做到高并发？

IO 多路复用呗，今天讲的 epoll 池就是了。

问题：单线程实现并发的有开源的例子吗？

redis，nginx 都是非常好的学习例子。当然还有我们 Golang 的 runtime 实现也尽显高并发的设计思想。

## 9.总结

1. IO 多路复用的原始实现很简单，就是一个 1 对多的服务模式，一个 loop 对应处理多个 fd ；
2. IO 多路复用想要做到真正的高效，必须要内核机制提供。因为 IO 的处理和完成是在内核，如果内核不帮忙，用户态的程序根本无法精确的抓到处理时机；
3. fd 记得要设置成非阻塞的哦，切记；
4. epoll 池通过高效的内部管理结构，并且结合操作系统提供的 poll 事件注册机制，实现了高效的 fd 事件管理，为高并发的 IO 处理提供了前提条件；
5. epoll 全名 eventpoll，在 Linux 内核下以一个文件系统模块的形式实现，所以有人常说 epoll 其实本身就是文件系统也是对的；
6. socketfd，eventfd，timerfd 这三种”文件“fd 实现了 poll 接口，所以网络 fd，事件fd，定时器fd 都可以使用 epoll_ctl 注册到池子里。我们最常见的就是网络fd的多路复用；
7. ext2，ext4，xfs 这种真正意义的文件系统反倒没有提供 poll 接口实现，所以不能用 epoll 池来管理其句柄。那文件就无法使用 epoll 机制了吗？不是的，有一个库叫做 libaio ，通过这个库我们可以间接的让文件使用 epoll 通知事件，以后详说，此处不表。

原文链接：https://zhuanlan.zhihu.com/p/410316787

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.437】Linux中的消息队列、共享内存，你确定都掌握了吗？

## 1.消息队列(message queue)

> 消息队列是消息的链表，存放在内存中，由内核维护

***消息队列的特点\***
1、消息队列中的消息是有类型的。
2、消息队列中的消息是有格式的。
3、消息队列可以实现消息的随机查询。消息不一定要以先进先出的次序读取，编程时可以按消息的类型读取。
4、消息队列允许一个或多个进程向它写入或者读取消息。
5、与无名管道、命名管道一样，从消息队列中读出消息，消息队列中对应的数据都会被删除。
6、每个消息队列都有消息队列标识符，消息队列的标识符在整个系统中是唯一的。
7、只有内核重启或人工删除消息队列时，该消息队列才会被删除。若不人工删除消息队列，消息队列会一直存在于系统中。
8、*消息队列可以独立于进程存在，可以无阻塞收发，可以选择性的接收消息
》 在ubuntu 12.04中消息队列限制值如下:

- 每个消息内容最多为8K字节
- 每个消息队列容量最多为16K字节
- 系统中消息队列个数最多为1609个
- 系统中消息个数最多为16384个

System V提供的IPC通信机制需要一个key值，通过key值就可在系统内获得一个唯一的消息队列标识符。
key值可以是人为指定的，也可以通过ftok函数获得。

```
#include <sys/types.h>
#include <sys/ipc.h>
key_t ftok(const char *pathname, int proj_id)；
功能：
    获得项目相关的唯一的IPC键值。
参数：
    pathname：路径名
    proj_id：项目ID，非0整数(只有低8位有效)
返回值：
    成功返回key值
    失败返回 -1
```

## 2.创建消息队列：

```
#include <sys/msg.h>
int msgget(key_t key, int msgflg)；
功能：
创建一个新的或打开一个已经存在的消息队列。不同的进程调用此函数，
只要用相同的key值就能得到同一个消息队列的标识符。
参数：
key：IPC键值。
msgflg：标识函数的行为及消息队列的权限。
返回值：
成功：消息队列的标识符，失败：返回-1。
》msgflg的取值：
IPC_CREAT：创建消息队列。
IPC_EXCL：检测消息队列是否存在。
位或权限位：消息队列位或权限位后可以设置消息队列的访问权限，
格式和open函数的mode_t一样，但可执行权限未使用。
```

> 使用shell命令操作消息队列:
> 查看消息队列
> ipcs -q
> 删除消息队列
> ipcrm -q msqid

## 3.消息队列的消息的格式：

```
typedef struct _msg
{
    long mtype; /*消息类型*/
    char mtext[100]; /*消息正文*/
    ... /*消息的正文可以有多个成员*/
}MSG;
消息类型必须是长整型的，而且必须是结构体类型的第一个成员，
类型下面是消息正文，正文可以有多个成员（正文成员可以是任意数据类型的）。
```

## 4.发送消息：

```
#include <sys/msg.h>
int msgsnd(int msqid, const void *msgp,size_t msgsz, int msgflg);
功能：
    将新消息添加到消息队列。
参数：
    msqid：消息队列的标识符。
    msgp：待发送消息结构体的地址。
    msgsz：消息正文的字节数。
    msgflg：函数的控制属性
        0：msgsnd调用阻塞直到条件满足为止。
        IPC_NOWAIT: 若消息没有立即发送则调用该函数的进程会立即返回。
返回值：
    成功：0
    失败：返回-1
```

## 5.接收消息：

```
#include <sys/msg.h>
ssize_t msgrcv(int msqid, void *msgp, size_t msgsz,
long msgtyp, int msgflg);
功能：
从标识符为msqid的消息队列中接收一个消息。一旦接收消息成功，
则消息在消息队列中被删除。
参数：
msqid：消息队列的标识符，代表要从哪个消息列中获取消息。
msgp： 存放消息结构体的地址。
msgsz：消息正文的字节数。
msgtyp：消息的类型、可以有以下几种类型
    msgtyp = 0：返回队列中的第一个消息
    msgtyp > 0：返回队列中消息类型为msgtyp的消息
    msgtyp < 0：返回队列中消息类型值小于或等于
    msgtyp绝对值的消息，如果这种消息有若干个，则取类型值最小的消息。
    注意：
    若消息队列中有多种类型的消息，msgrcv获取消息的时候按消息类型获取，
    不是先进先出的。在获取某类型消息的时候，若队列中有多条此类型的消息，
    则获取最先添加的消息，即先进先出原则。
msgflg：函数的控制属性
    0：msgrcv调用阻塞直到接收消息成功为止。
    MSG_NOERROR:若返回的消息字节数比nbytes字节数多,则消息就会截短到nbytes字节,且不通知消息发送进程。
    IPC_NOWAIT:调用进程会立即返回。若没有收到消息则立即返回-1。
返回值：
    成功返回读取消息的长度
    失败返回-1。
```

## 6.消息队列的控制：

```
#include <sys/msg.h>
int msgctl(int msqid, int cmd, struct msqid_ds *buf);
功能：
    对消息队列进行各种控制，如修改消息队列的属性，或删除消息消息队列。
参数：
    msqid：消息队列的标识符。
    cmd：函数功能的控制。
    buf：msqid_ds数据类型的地址，用来存放或更改消息队列的属性。
返回值：
    成功：返回 0
    失败：返回 -1
》cmd：函数功能的控制
    IPC_RMID：删除由msqid指示的消息队列，将它从系统中删除并破坏相关数据结构。
    IPC_STAT：将msqid相关的数据结构中各个元素的当前值存入到由buf指向的结构中。
    IPC_SET：将msqid相关的数据结构中的元素设置为由buf指向的结构中的对应值。
```

例：01_message_queue_**write**.c 01_message_queue_**read**.c

![img](https://pic4.zhimg.com/80/v2-b5fb7ba4aff8b1afcb9bb42c57513f47_720w.webp)

![img](https://pic1.zhimg.com/80/v2-f8eb15b8b8a3aaf56faa85a44aa95718_720w.webp)

## 7.总结：

```
》每个程序都有两个任务，一个任务是负责接收消息，一个任务是负责发送消息，
通过fork创建-子进程实现多任务。
》一个进程负责接收信息，它只接收某种类型的消息，只要别的进程发送此类型的消息，
此进程-就能收到。收到后通过消息的name成员就可知道是谁发送的消息。
》另一个进程负责发信息，可以通过输入来决定发送消息的类型。
》设计程序的时候，接收消息的进程接收消息的类型不一样，这样就实现了发送的消息
只被接收-此类型消息的人收到，其它人收不到。这样就是实现了给特定的人发送消息。
```

## 8.共享内存(shared memory)

> 共享内存允许两个或者多个进程共享给定的存储区域。

**共享内存的特点**
1、共享内存是进程间共享数据的一种最快的方法。一个进程向共享的内存区域写入了数据，共享这个内存区域的所有进程就可以立刻看到其中的内容。
2、使用共享内存要注意的是多个进程之间对一个给定存储区访问的互斥。若一个进程正在向共享内存区写数据，则在它做完这一步操作前，别的进程不应当去读、写这些数据。

![img](https://pic2.zhimg.com/80/v2-7fd8e629e2de2d1855b068d4f389e141_720w.webp)

在ubuntu 12.04中共享内存限制值如下：
-共享存储区的最小字节数：1
-共享存储区的最大字节数：32M
-共享存储区的最大个数：4096
-每个进程最多能映射的共享存储区的个数：4096

## 9.获得一个共享存储标识符

```
#include <sys/ipc.h>
#include <sys/shm.h>
int shmget(key_t key, size_t size,int shmflg);
功能:创建或打开一块共享内存区
参数：
key：IPC键值
size：该共享存储段的长度(字节)
shmflg：标识函数的行为及共享内存的权限。
返回值:
成功：返回共享内存标识符。
失败：返回－1。
》参数属性shmflg：
IPC_CREAT：如果不存在就创建
IPC_EXCL：如果已经存在则返回失败
位或权限位：共享内存位或权限位后可以设置共享内存的访问权限，格式和open函数的mode_t一样，但可执行权限未使用。
```

> 使用shell命令操作共享内存:
> 查看共享内存
> ipcs -m
> 删除共享内存
> ipcrm -m shmid

## 10.共享内存映射(attach)

```
#include <sys/types.h>
#include <sys/shm.h>
void *shmat(int shmid, const void *shmaddr,int shmflg);
功能：
将一个共享内存段映射到调用进程的数据段中。
参数：
shmid：共享内存标识符。
shmaddr：共享内存映射地址(若为NULL则由系统自动指定)，推荐使用NULL。
shmflg：共享内存段的访问权限和映射条件。
返回值：
成功：返回共享内存段映射地址
失败：返回 -1
》shmflg：共享内存段的访问权限和映射条件
    0：共享内存具有可读可写权限。
    SHM_RDONLY：只读。
    SHM_RND：（shmaddr非空时才有效）
    没有指定SHM_RND则此段连接到shmaddr所指定的地址上(shmaddr必需页对齐)。
    指定了SHM_RND则此段连接到shmaddr-shmaddr%SHMLBA 所表示的地址上。
》注：
shmat函数使用的时候第二个和第三个参数一般设为NULL和0，
即系统自动指定共享内存地址，并且共享内存可读可写。
```

## 11.解除共享内存映射(detach)

```
#include <sys/types.h>
#include <sys/shm.h>
int shmdt(const void *shmaddr);
功能：
将共享内存和当前进程分离(仅仅是断开联系并不删除共享内存)。
参数：
shmaddr：共享内存映射地址。
返回值：
成功返回 0，
失败返回 -1。
```

## 12.共享内存控制

```
#include <sys/ipc.h>
#include <sys/shm.h>
int shmctl(int shmid, int cmd,struct shmid_ds *buf);
功能：共享内存空间的控制。
参数：
shmid：共享内存标识符。
cmd：函数功能的控制。
buf：shmid_ds数据类型的地址，用来存放或修改共享内存的属性。
返回值：
成功返回 0，
失败返回 -1。
》cmd：函数功能的控制
    IPC_RMID：删除。
    IPC_SET：设置shmid_ds参数。
    IPC_STAT：保存shmid_ds参数。
    SHM_LOCK：锁定共享内存段(超级用户)。
    SHM_UNLOCK：解锁共享内存段。
```

例：02_shared_memory_**write**.c 02_shared_memory_**read**.c

![img](https://pic3.zhimg.com/80/v2-2102704c6fb5ded6b4d73ad30841d9ce_720w.webp)

![img](https://pic1.zhimg.com/80/v2-de902402d906112813abd946c5e20540_720w.webp)

注意：
SHM_LOCK用于锁定内存，禁止内存交换。并不代表共享内存被锁定后禁止其它进程访问。其真正的意义是：被锁定的内存不允许被交换到虚拟内存中。这样做的优势在于让共享内存一直处于内存中，从而提高程序性能。

原文链接：https://zhuanlan.zhihu.com/p/411043847

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.438】关于高性能服务器底层网络通信模块的设计方法

在对I/O完成端口进行底层封装的基础上，本文提出一种具有高性能的、可扩展性的通用网络通信模块设计方案。该方案采用多种系统性能优化技术，如线程池、对象池和环形缓存区等。该模块在Win32平台上用c++开发完成，经过严格的压力和性能测试后，实验结果表明该模块能够支持海量并发连接，具有较高的数据吞吐量，在实际项目应用中也取得了良好的表现。

## **1.概述**

　　要设计与开发出一款高性能的服务器(如网游服务器、Web服务器和代理服务器等)，一般都采用高效率的网络I/O模型。Linux平台上经常会采用epoll模型，而在Win32平台上完成端口(以下简称IOCP)模型是设计与开发高性能的、具有可伸缩性的服务器的最佳选择，它可以支持海量并发客户端请求。多线程编程是服务器端开发常用技术，多线程必然涉及线程间的通信与同步。如果使用不当，也会影响到系统的性能，必须谨慎设计才能保证系统良好运行。减少数据拷贝以及小对象频繁创建与销毁是一种很重要的提高系统性能手段，可通过设计内存池或对象池加以解决。这几个问题的提出，说明实际的高性能服务器研发比较复杂，尤其是采用高效I/O模型来架构服务器时，更是增加了开发的难度，原因是这些模型的机制比较复杂。

　　底层网络通信模块是服务器应用程序的核心模块，也是高性能服务器的最基础模块之一。它主要的功能是接收海量并发连接、接收网络数据包、暂存和发送应用逻辑层的逻辑数据包，所以，它也是上次应用逻辑和底层网络之间通信的媒介。

## **2.IOCP机制**

　　要实现一个并发的网络服务器，比较简单的模型是：每当一个请求到达就创建一个新线程，然后在新线程中为请求服务。这种模型减轻了实际开发的复杂度，在并发连接较少的情况下可以考虑使用，然而在高并发需求下并不适用。高并发环境中，创建和销毁大量线程所花费的时间和消耗的系统资源是巨大的，而且会加重线程调度的负担，同时线程上下文切换(context switch)也会浪费许多宝贵的CPU时间。

　　为了提高系统性能，首先必须有足够的可运行线程来充分利用CPU资源，但线程的数量不能太多。事实上，具体工作线程的数量和并发连接数量不是直接相关联的。在Win32平台下开发高效的服务器端应用程序，最理想的模型是IOCP模型，该模型解决了一系列系统性能瓶颈问题。

　　IOCP提供了最好的可伸缩性，而且其执行效率比较高，采用这种网络模型可能会加大开发的复杂度，但却是Windows平台上唯一适用于开发高负载服务器的技术。IOCPWindows系统的一种内核对象，也是Win32下最复杂的一种I/O模型，它通过一定数量的工作线程对重叠I/O请求进行处理，以便为已经完成的I/O请求提供服务，相对其他I/O模型，它可以管理任意数量套接字句柄。它主要由等待线程队列和I/O完成队列2个部分组成。一个完成端口对象可以和多个套接字句柄相关联，当针对某个套接字句柄发起的异步I/O操作完成时，系统向该完成端口的I/O完成队列加入一个I/O完成包。于此同时，工作线程调用GetQueuedcompIetionstatus(以下简称GQCS)时，如果I/O完成队列中有完成包，当前调用就会返回，取得数据进行后续的处理。

　　成功创建一个完成端口后，便可开始将套接字句柄与对象关联到一起。但在关联套接字之前，首先必须创建一个或多个工作者线程为完成端口提供服务。

## **3.模块设计方案**

### **3.1.架构设计**

　　在充分考虑服务器性能和扩展性的基础上，本文提出了基于三层结构的系统设计方案，该模块的架构如图1所示。

![img](https://pic1.zhimg.com/80/v2-934513cb028e8b1f156660cec16a772c_720w.webp)

　　图1看上去并不复杂，但却是一个兼顾可扩展性和高性能的架构，在实际项目应用中也取得了很好的表现。下面是对图1主要类的功能分析。

　　CIocpServer类足完成端口服务器基本通信类，它使用Windows平台特有的IOCP机制，对网络通信模型进行底层封装。提供了基本的服务器端网络通信功能，这些功能主要有开启服务器、关闭服务器、管理客户端连接列表、管理未决的接受请求列表、发出异步操作等。同时通过多态机制向它的派生类提供以下基本扩展接口：

1. 1. 新连接确立的处理接口。
   2. 客户端断开连接时的处理接口。
   3. 连接出现错误时的处理接口。
   4. 从客户端接收完数据后的处理接口。
   5. 向客户端发送完数据后的处理接口。
   6. 拼包处理接口。

　　CUserServer类继承CIocpServer，在ClocpServer的基础上，CUserServer加入了一些服务器逻辑处理功能，并且封装了3类数据队列和3类处理线程，分别如下：

***(1)接收数据包队列及接收线程：\***接收队列用于存放接收到的数据包，此数据包还没有进行逻辑意义上的拼包，接收线程从此队列中取出数据包，并将其拼装成逻辑意义上完整的数据包加入到逻辑数据包队列中。

***(2)逻辑数据包队列及逻辑处理线程：\***逻辑队列用于存放已经拼包成了逻辑意义上的数据包，逻辑处理线程对此类数据包进行逻辑解析，这里就是服务器的主要逻辑部分，有的数据包在处理完成后，可能是需要向客户端返回处理结果，此时就需要逻辑线程将处理完成的数据包放入发送数据包队列中。

　***(3)发送数据包队列及发送线程：\***发送队列存放待发送的数据包，发送线程根据数据包里的客户端套接字发送给特定客户端。

　　CTestServer类是一个测试类，主要用于演示如何在CUserServer的基础上派生一个真正的应用服务器，并用于说明它需要重载实现CUserServer的哪些重要虚函数。

### **3.2.资源管理**

　　用IOCP开发服务器时，当I/O发生错误时需要有效地释放与套接字相关的缓存区，如果对同一缓存区释放多次，就会导致内存释放的错误。当投递的异步I/O请求返回了非WSA_IO_PENDING错误时，要对此错误进行处理，通常执行2步操作：释放此次操作使用的缓冲区数据；关闭当前操作所使用的套接字句柄。同时GQCS调用会返回FALSE，也要做上面2步相同的操作，这样就可能产生对同一缓存区进行重复释放的错误。解决的办法可以有2种：

1. 1. 通过引用计数机制控制缓存区释放；
   2. 使缓存区释放操作线性化。

　　该系统的没计采用了第(2)种解决方案，所谓的释放操作线性化是指把可能引起2次释放同一缓存区的操作合并为一次释放。如果在执行异步I/O操作过程中发生了非WSA_IO_PENDING错误，可以让GQCS返回时得知这个错误和发生错误时的缓存区指针，而不对该错误进行处理。通知的方式是，使用PostQueuedCompletionStatus(1扶下简称Post)函数抛出一个特殊标志的消息，这个特殊标志可以通过GQCS函数的第2个参数，即传送字节数来表示，可以选择任何一个不可能出现的值，比如一个负数。当然，如果通过单句柄数据或单I/O数据来传递也是可以的。而发生错误时的缓冲区指针，必须要通过单句柄数据或单I/O数据来传递。

　　把释放操作全放在GQCS函数里以后，对释放操作的处理就比较统一了。当然，为了实现真正的线性化和原子化，在释放操作的执行逻辑上需要对释放代码加锁以实现线程互斥(多线程情况下)。

### **3.3.包的乱序解决方案**

　　**如果在同一个套接字上一次提交多个异步I/O请求，肯定会按照它们提交的次序完成，但在多线程环境下，完成包处理次序可能和提交次序不一致。该问题的一个简单的解决方法足一次只投递一个异步I/O请求，当工作线程处理完该请求的完成数据包后，再投递下一个异步I/O请求。但这样做会降低服务器的处理性能。为了保证完成包处理次序和提交次序相一致，可以为每个连接上投递的请求都分配一个序号，单句柄数据中记录当前需要读取的单I/O数据的序号，如果工作线程获得的单I/O数据的序号与单句柄数据中记录的序号一致的话，就处理该数据。如果不相等，则把这个单I/O数据保存到该连接的pOutOfOrderReads列表中。**

## **4.性能优化**

　　在网络服务器的开发过程中，池(Pool)技术已经被广泛应用。使用池技术在一定层度上可以明显优化服务器应用程序的性能，提高程序执行效率和降低系统资源开销。这里所说的池是一种广义上的池，比如数据库连接池、线程池、内存池、对象池等。其中，对象池可以看成保存对象的容器，在进程初始化时创建一定数量的对象。需要时直接从池中取出一个空闲对象，用完后并不直接释放掉对象，而是再放到对象池中以方便下一次对象请求可以直接复用。其他几种池的设计思想也是如此，池技术的优势是，可以消除对象创建所带来的延迟，从而提高系统的性能。

### **4.1.线程池**

　　线程池是提高服务器程序性能的一种很好技术，在Win32乎台下开发的网络服务器程序使用的线程池可分为两类：一类是由完成端口对象负责维护的工作线程池，主要负责网络层相关处理(比如投递异步读或写操作等)；另一类是负责逻辑处理的线程池，它是专门提供给应用层来使用的。

　　本文提出了一种逻辑线程池的设计方案，线程池框架结构主要分为以下几个部分：

1. 1. 线程池管理器：用于创建并管理线程，往任务队列添加数据包等，并可以动态增加工作线程。
   2. 工作线程：线程池中的线程，执行实际的逻辑处理。
   3. 任务接口：每个任务必须实现的接口，以供工作线程调度任务使用。
   4. 任务队列：提供一种缓存机制，用于存放从网络层接收的数据包。

　　该通信模块使用了上述线程池的设计方案，从测试结果来看，当并发连接数很大时，线程池对服务器的性能改善是显著的。

　　该设计方案有个很好的特性，就是可以创建工作线程数量固定的线程池，也可以创建动态线程池。如果有大量的客户要求服务器为其服务，但由于线程池的工作线程是有限的话，服务器只能为部分客户端服务，客户端提交的任务只能在任务队列中等待处理。动态改变的工作线程数目的线程池，可以以适应突发性的请求。一旦请求变少了将逐步减少线程池中工作线程的数目。当然线程增加可以采用一种超前方式，即批量增加一批工作线程，而不是来一个请求才建立创建一个线程。批量创建是更加有效的方式，而且该方案还限制了线程池中工作线程数目的上限和下限，确保线程池技术能提高系统整体性能。

### **4.2.对象池**

　　对象池是针对特定应用程序而设计的内存管理方式，在某种场合下内存的分配和释放性能会大大提升。默认的内存管理函数(new/delete或malloc/free)有其不足之处，如果应用程序频繁地在堆上分配和释放内存，那么就会导致性能损失，并且会使系统中出现大量的内存碎片，降低内存的利用率。

　　所谓对象池就是应用程序可以通过系统的内存分配调用预先一次性申请适当大小的内存块，然后可以根据特定对象的大小，把该块内存分割成一个个大小相同的对象。如果对象池中没有空闲对象使用时，可以再向系统申请同样大小的内存块。如果对象使用完毕后直接放到对象池中，这种内存管理策略能有效地提升程序性能。

#### 4.2.1.对象池的应用

　　当服务器接受一个客户端请求后，会创建成功返回一个客户端套接字句柄。如果出现大量并发客户端连接请求时，就会出现频繁地分配和释放对象的情况，这个过程可能会消耗大量的系统资源，有损系统性能。WinSock2还提供一个接受扩展函数AcceptEx，它允许在接受连接之前就事先创建一个套接字句柄，使之与接受连接相关联。在调用AcceptEx时，可以直接把该句柄作为参数传递给AcceptEx。有了这个保证，可以通过采用对象池技术来提升系统性能，可以在接受连接之前就创建一定数量的套接字句柄，随着新连接请求的到来将句柄分配出去，当客户端断开连接后，把相应句柄重新放入套接字对象池中。

　　另外需要用到对象池的地方是，在每一次投递WSASend或WSARecv操作时，都要传进一个重叠结构体参数。可以提前创建一个蕈叠结构体对象池，当发起异步I/O操作时，先从池中取一个结构体对象，用完之后并不直接销毁，而是再放回对象池以便以后蘑复利用。创建的结构体数量取决于完成端口的处理效率，如果处理效率比较高，则数量可能就少些，反之，就需要多创建些对象。

　　该系统所设计的对象池足线程安全的，可以被多个线程共享，在获得和释放对象时都需要加锁，从而保证线程问互斥访问对象池。

#### 4.2.2.对象池的优点

　　与系统直接管理内存相比，对象池在系统性能优化方面主要有如下优点：

1. 1. 针对特殊情况，例如需要频繁分配和释放固定大小的对象时，不需要复杂的分配算法和线程同步。也不需要维护内存空闲表的额外开销，从而获得较好的性能。
   2. 由于直接分配一定数量的连续内存空问作为内存块，因此一定程度上提高了程序局部性能，提升了应用程序整体性能。
   3. 比较容易控制页边界对齐和内存字节对齐，基本没有内存碎片问题。

### **4.3环形缓存区**

　　基于TCP协议的服务器应用程序，拼包处理过程必不可少。由于要从接收缓存中分解出一个个逻辑数据包，因此一般都要涉及内存拷贝操作，过多的内存拷贝必然降低系统性能。

　　当然，就逻辑数据包的拼装问题而言，也完全可以避免数据拷贝操作，方法是使用环形缓冲区。本文所说的环形缓冲区足具体这种特征的接收缓冲区，在服务器的接收事件里，当处理完了一次从缓冲区里取走所有完整逻辑包的操作后，可能会在缓冲区里遗留下来新的不完整数据包。使用了环形缓冲区后，就可以不将数据重新复制到缓冲区首郎以等待后续数据的拼装，可以根据记录下的队列首部和队列尾部指针进行下一次的拼包操作。

　　环形缓冲区在IOCP的处理中，甚至在其他需要高效率处理数据收发的网络模型的接收事件处理中，是一种被广泛采用的优化方案。

## **5.实验结果**

　　为了证明论文中系统优化的方法能获得预期的性能优势，对内存池和系统整体性能进行了实验测试。测试硬件是：CPU：AMD Turion 64，内存：1 024 MB，网络：100 MB局域网，操作系统：Windows XP Professional SP2。

　　**测试1：对象池性能测试**

![img](https://pic1.zhimg.com/80/v2-ad3f45a036d1fcd6b4a2cd5f94a73454_720w.webp)

　　由表1可以看出，由于使用对象池来分配小对象的内存，速度提高了52.48%，使得内存分配获得了显著的效率提升。速度提高的原闪可以归结为以下几点：

　　(1)除了偶尔的内存申请和销毁会导致从进程堆中分配和销毁内存块外，绝大多数的内存申请和销毁都由对象池在已经申请到的内存块中进行，而没有直接与进程打交道，而直接与进程打交道足很耗时的操作。

　　(2)这足在单线程环境的对象池，在多线程环境下，由于加锁，因此速度提高的会少些。

　　**测试2：系统压力测试**

　　根据上述设计，采用Visual Studio 2005开发实现的测试服务器在压力测试中取得很好的结果。在3 000个模拟客户端的长时间不问断连续信息传输过程中，服务器处理吞吐能力始终保持在1200条/s左右，并且所在服务器操作系统状况良好，系统资源消耗正常，占用率稳定。

## **6.结束语**

　　根据高性能的、可扩展性的服务器实际应用需求，本文提出了基于三层结构的底层网络通信模块设计方案，并采用多种系统优化技术来实现该模块在实际应用中的高性能和高效率。其中，线程池和对象池优化技术不仅在服务器端开发上有很好的应用，也可以用于其他对性能要求较高的应用程序中。经过严格的性能测试，结果表明该模块在实际应用中，有非常好的表现，这也达到了笔者设计的初衷。

原文链接：https://zhuanlan.zhihu.com/p/411416355

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.439】你真的了解Redis单线程为什么如此之快吗？

## 1.概述

Redis的高并发和快简单可以归结为一下几点：

1.Redis是基于内存的；

2.Redis是单线程的；

3.Redis使用多路复用技术。

4.高效的数据结构

但具体怎么做的呢，下面来详细看下每一点的具体实现吧～

## 2.基于内存实现

Redis 是基于内存的数据库，那不可避免的就要与磁盘数据库做对比。对于磁盘数据库来说，是需要将数据读取到内存里的，这个过程会受到磁盘 I/O 的限制。

而对于内存数据库来说，本身数据就存在于内存里，也就没有了这方面的开销。

## 3.合适的线程模型

Redis 快的原因还有一个是因为使用了合适的线程模型：

![img](https://pic1.zhimg.com/80/v2-0ed12e375749ff897e582e06d1379bfc_720w.webp)

**1、I/O多路复用模型**

Redis 采用网络IO多路复用技术来保证在多连接的时候， 系统的高吞吐量。

多路-指的是多个socket连接，复用-指的是复用一个线程。多路复用主要有三种技术：select，poll，epoll。epoll是最新的也是目前最好的多路复用技术。

![img](https://pic2.zhimg.com/80/v2-8d2eb47943b346a1c30ed48f59d55361_720w.webp)

- **I/O** ：网络 I/O
- **多路**：多个 TCP 连接
- **复用**：共用一个线程或进程

生产环境中的使用，通常是多个客户端连接 Redis，然后各自发送命令至 Redis 服务器，最后服务端处理这些请求返回结果。

![img](https://pic4.zhimg.com/80/v2-1e372efe9deb971b9494947367cd1fdf_720w.webp)

应对大量的请求，Redis 中使用 I/O 多路复用程序同时监听多个套接字，并将这些事件推送到一个队列里，然后逐个被执行。最终将结果返回给客户端。

**2、单线程模型，避免上下文切换**

你一定听说过，Redis 是单线程的。那么单线程的 Redis 为什么会快呢？

因为多线程在执行过程中需要进行 CPU 的上下文切换，这个操作比较耗时。Redis 又是基于内存实现的，对于内存来说，没有上下文切换效率就是最高的。多次读写都在一个CPU 上，对于内存来说就是最佳方案。

**3、为什么Redis是单线程的**

**<1>.官方答案**

因为Redis是基于内存的操作，CPU不是Redis的瓶颈，Redis的瓶颈最有可能是机器内存的大小或者网络带宽。既然单线程容易实现，而且CPU不会成为瓶颈，那就顺理成章地采用单线程的方案了。

**<2>.性能指标**

关于Redis的性能，官方网站也有，普通笔记本轻松处理每秒几十万的请求。

**<3>.详细原因**

**1）不需要各种锁的性能消耗**

Redis的数据结构并不全是简单的Key-Value，还有list，hash等复杂的结构，这些结构有可能会进行很细粒度的操作，比如在很长的列表后面添加一个元素，在hash当中添加或者删除

一个对象。这些操作可能就需要加非常多的锁，导致的结果是同步开销大大增加。

**总之，在单线程的情况下，就不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗。**

**2）单线程多进程集群方案**

单线程的威力实际上非常强大，每核心效率也非常高，多线程自然是可以比单线程有更高的性能上限，但是在今天的计算环境中，即使是单机多线程的上限也往往不能满足需要了，需要进一步摸索的是多服务器集群化的方案，这些方案中多线程的技术照样是用不上的。

**所以单线程、多进程的集群不失为一个时髦的解决方案。**

**3）CPU消耗**

采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU。

但是如果CPU成为Redis瓶颈，或者不想让服务器其他CUP核闲置，那怎么办？

可以考虑多起几个Redis进程，Redis是key-value数据库，不是关系数据库，数据之间没有约束。只要客户端分清哪些key放在哪个Redis进程上就可以了。

## 4.高效的数据结构

Redis 中有多种数据类型，每种数据类型的底层都由一种或多种数据结构来支持。正是因为有了这些数据结构，Redis 在存储与读取上的速度才不受阻碍。这些数据结构有什么特别的地方，各位看官接着往下看：

![img](https://pic4.zhimg.com/80/v2-44d1f197aa9a8aebccef576a1b2e3e03_720w.webp)

**1、简单动态字符串**

这个名词可能你不熟悉，换成 **SDS** 肯定就知道了。这是用来处理字符串的。了解 C 语言的都知道，它是有处理字符串方法的。而 Redis 就是 C 语言实现的，那为什么还要重复造轮子？我们从以下几点来看：

**（1）字符串长度处理**

这个图是字符串在 C 语言中的存储方式，想要获取 「Redis」的长度，需要从头开始遍历，直到遇到 ‘\0’ 为止。

![img](https://pic1.zhimg.com/80/v2-ffcf8f59aa48f3b5eadc88c5404afbbc_720w.webp)

Redis 中怎么操作呢？用一个 len 字段记录当前字符串的长度。想要获取长度只需要获取 len 字段即可。你看，差距不言自明。前者遍历的时间复杂度为 O(n)，Redis 中 O(1) 就能拿到，速度明显提升。

**（2）内存重新分配**

C 语言中涉及到修改字符串的时候会重新分配内存。修改地越频繁，内存分配也就越频繁。而内存分配是会消耗性能的，那么性能下降在所难免。

而 Redis 中会涉及到字符串频繁的修改操作，这种内存分配方式显然就不适合了。于是 SDS 实现了两种优化策略：

- **空间预分配**

对 SDS 修改及空间扩充时，除了分配所必须的空间外，还会额外分配未使用的空间。

具体分配规则是这样的：SDS 修改后，len 长度小于 1M，那么将会额外分配与 len 相同长度的未使用空间。如果修改后长度大于 1M，那么将分配1M的使用空间。

- **惰性空间释放**

当然，有空间分配对应的就有空间释放。

SDS 缩短时，并不会回收多余的内存空间，而是使用 free 字段将多出来的空间记录下来。如果后续有变更操作，直接使用 free 中记录的空间，减少了内存的分配。

**（3）二进制安全**

你已经知道了 Redis 可以存储各种数据类型，那么二进制数据肯定也不例外。但二进制数据并不是规则的字符串格式，可能会包含一些特殊的字符，比如 ‘\0’ 等。

前面我们提到过，C 中字符串遇到 ‘\0’ 会结束，那 ‘\0’ 之后的数据就读取不上了。但在 SDS 中，是根据 len 长度来判断字符串结束的。

看，二进制安全的问题就解决了。

**2、双端链表**

列表 List 更多是被当作队列或栈来使用的。队列和栈的特性一个先进先出，一个先进后出。双端链表很好的支持了这些特性。

![img](https://pic3.zhimg.com/80/v2-ea59a4cf3e7db97d7dd58d8e42f643fe_720w.webp)

**（1）前后节点**

![img](https://pic3.zhimg.com/80/v2-0b0fedaafa5f0e57619985071bb728ae_720w.webp)

链表里每个节点都带有两个指针，prev 指向前节点，next 指向后节点。这样在时间复杂度为 O(1) 内就能获取到前后节点。

**（2）头尾节点**

![img](https://pic3.zhimg.com/80/v2-edd4daf731d7b60d9db6b3a4630b1be2_720w.webp)

你可能注意到了，头节点里有 head 和 tail 两个参数，分别指向头节点和尾节点。这样的设计能够对双端节点的处理时间复杂度降至 O(1) ，对于队列和栈来说再适合不过。同时链表迭代时从两端都可以进行。

**（3）链表长度**

头节点里同时还有一个参数 len，和上边提到的 SDS 里类似，这里是用来记录链表长度的。因此获取链表长度时不用再遍历整个链表，直接拿到 len 值就可以了，这个时间复杂度是 O(1)。

你看，这些特性都降低了 List 使用时的时间开销。

**3、压缩列表**

双端链表我们已经熟悉了。不知道你有没有注意到一个问题：如果在一个链表节点中存储一个小数据，比如一个字节。那么对应的就要保存头节点，前后指针等额外的数据。

这样就浪费了空间，同时由于反复申请与释放也容易导致内存碎片化。这样内存的使用效率就太低了。

于是，压缩列表上场了！

![img](https://pic4.zhimg.com/80/v2-74ad4f7579907ea850ab1e445d63069f_720w.webp)

它是经过特殊编码，专门为了提升内存使用效率设计的。所有的操作都是通过指针与解码出来的偏移量进行的。

并且压缩列表的内存是连续分配的，遍历的速度很快。

**4、字典**

Redis 作为 K-V 型数据库，所有的键值都是用字典来存储的。

日常学习中使用的字典你应该不会陌生，想查找某个词通过某个字就可以直接定位到，速度非常快。这里所说的字典原理上是一样的，通过某个 key 可以直接获取到对应的value。

字典又称为哈希表，这点没什么可说的。哈希表的特性大家都很清楚，能够在 O(1) 时间复杂度内取出和插入关联的值。

**5、跳跃表**

作为 Redis 中特有的数据结构-跳跃表，其在链表的基础上增加了多级索引来提升查找效率。

![img](https://pic1.zhimg.com/80/v2-4905351369910fe775265bc969312ec4_720w.webp)

这是跳跃表的简单原理图，每一层都有一条有序的链表，最底层的链表包含了所有的元素。这样跳跃表就可以支持在 O(logN) 的时间复杂度里查找到对应的节点。

下面这张是跳表真实的存储结构，和其它数据结构一样，都在头节点里记录了相应的信息，减少了一些不必要的系统开销。

![img](https://pic2.zhimg.com/80/v2-6cf647e653c637ecdd1c5a13d6708b71_720w.webp)

**6、合理的数据编码**

对于每一种数据类型来说，底层的支持可能是多种数据结构，什么时候使用哪种数据结构，这就涉及到了编码转化的问题。

那我们就来看看，不同的数据类型是如何进行编码转化的：

**String**：存储数字的话，采用int类型的编码，如果是非数字的话，采用 raw 编码；

**List**：字符串长度及元素个数小于一定范围使用 ziplist 编码，任意条件不满足，则转化为 linkedlist 编码；

**Hash**：hash 对象保存的键值对内的键和值字符串长度小于一定值及键值对；

**Set**：保存元素为整数及元素个数小于一定范围使用 intset 编码，任意条件不满足，则使用 hashtable 编码；

**Zset**：zset 对象中保存的元素个数小于及成员长度小于一定值使用 ziplist 编码，任意条件不满足，则使用 skiplist 编码。

## 5.Redis高并发快总结

1. **基于内存实现。**数据都存储在内存里，减少了一些不必要的 I/O 操作，操作速率很快。

2. **合适的线程模型。**

- I/O 多路复用模型同时监听客户端连接；
- 单线程在执行过程中不需要进行上下文切换和竞争，减少了耗时。

**3.高效的数据结构。**

- 底层多种数据结构支持不同的数据类型，支持 Redis 存储不同的数据；
- 不同数据结构的设计，使得数据存储时间复杂度降到最低。

**4.合理的数据编码。**

- 根据字符串的长度及元素的个数适配不同的编码格式。

原文链接：https://zhuanlan.zhihu.com/p/411767232

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.440】并发与多线程之线程安全篇

并发是指某个时间段内，多个任务交替执行的能力。 CPU 把可执行时间均匀地分成若干份，每个进程执行一段时间后，记录当前的工作状态，释放当前的执行资源并进入等待状态，让其他进程抢占 CPU 资源。并行是指同时处理多任务的能力。目前， CPU 已经发展为多核，可以同时执行多个互不依赖的指令及执行块。

并发和并行的目标都是尽可能快执行完所有的任务，**两者区别核心在于进程是否同时执行**，并发环境有着以下几个特点：

1. 并发程序之间有相互制约的关系。
2. 并发程序的执行过程是断断续续的。
3. 当并发设置合理并且 CPU 拥有足够的处理能力时，并发会提高程序的运行效率。

## 1.**线程安全**

> 我们都知道，进程是操作系统进行资源分配的独立单位，而线程是CPU调度和分派的基本单位，为了更充分地利用CPU资源，一般都会使用多线程进行处理。多线程的作用是**提高任务的平均执行速度**。

线程可以拥有自己的操作栈、程序计数器、局部变量表等资源，它与同一进程内的其他线程共享该进程的所有资源。线程在生命周期内存在多种状态，分别为**NEW（新建状态）**、**RUNNABLE（就绪状态）**、**RUNNING（运行状态）**、**BLOCKED（堵塞状态）**、**DEAD（死亡状态）** 五种状态。线程状态图如下

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213160114_21905.jpg)

## 2.**NEW-新建状态**

New 是线程被创建且未启动的状态。线程被创建的方式有三种：第一种继承 Thread 类，第二种实现 Runnable 接口，第三种实现 Callable 接口。由于 Java 单继承限制，所以推荐第二种，实现 Runnable 接口可以使编程更加灵活，对外暴露的细节比较少，开发者只需专注于具体的实现，即 run 方法的实现。第三种方式是实现 Callable 接口，代码如下：

```
@FunctionalInterface
public interface Callable<V> {
    //    Computes a result, or throws an exception if unable to do so.
    //    Returns:computed result
    //    Throws:Exception – if unable to compute a result
    V call() throws Exception;
}
```

从注释中看出，当出现无法计算的结果就会抛出异常，并且Callable接口是存在返回值的。这是 Callable 跟 Runnable 的本质区别。

## **3.RUNNABLE-就绪状态**

Runnable 是调用 start() 之后并且在运行之前的状态。线程的 start 方法不能多次调用，否则将抛出 IllegalThreadStateException 异常。

## **4.RUNNING-运行状态**

Running 是 run() 正在执行时线程的状态。线程可能会由于某些原因而退出 RUNNING ，如时间、异常、锁、调度等。

## **5.BLOCKED-堵塞状态**

Blocked 是线程已经发生堵塞的状态，具体发生堵塞的有以下几种情况：

- 同步堵塞：锁被其他线程占用。
- 主动堵塞：调用 Thread 的某些方法，主动让出 CPU 执行权，比如 sleep()、join() 等。
- 等待堵塞：执行了wait() 。

## 6.**DEAD-死亡状态**

Dead 是线程已经执行完 run 方法，或因异常错误导致退出的状态，该状态无法逆转，即无法回到就绪状态。

在计算机的线程处理过程当中，因为每个线程轮流占用 CPU 的计算资源，可能会出现某个线程尚未执行完就不得不中断的情况，容易导致线程不安全。例如，在服务端某个高并发业务共享某用户数据，首先 A 线程执行用户的查询任务，但是查询出来的数据还未返回就退出 CPU 时间片；然后后面进来的 B 线程抢占了 CPU 资源并覆盖了该用户数据，最后 A 线程重新执行，将 B 线程修改过后的数据返回给前端，导致页面出现数据异常。所以，为了保证线程安全，在多个线程并发地竞争共享资源时，通常采用同步机制协调各个线程的执行，确保得到正确的结果。

线程安全问题只在多线程环境下出现，单线程串行执行并不会存在此问题。为了保证高并发场景下的线程安全，可以从以下四个维度来探讨：

（1）**数据单线程内可见**。单线程总是安全的。通过限制数据仅在单线程内可见，可以避免数据被其他线程篡改。最典型的就是线程局部变量，它存储在独立虚拟机栈帧的局部变量表中，与其他线程毫无瓜葛。ThreadLocal 就是采用这种方式来实现线程安全的。

（2）**只读对象**。只读对象总是安全的。它的特性是允许复制、拒绝写入。最典型的只读对象有 String、Integer 等。一个对象想要拒绝任何写入，必须要满足以下条件：使用 final 关键字修饰类，避免被继承；使用 private final 关键字避免属性被中途修改；没有任何更新方法；返回值不能为可变对象。

（3）**线程安全类**。某些线程安全类的内部有非常明确的线程安全机制。比如 StringBuffer 就是一个线程安全类，它采用 synchronized 关键字来修饰相关方法。

（4）**同步与锁机制**。如果想要对某个对象进行并发更新操作，但又不属于上述三类，需要开发者在代码中自定义实现相关的安全同步机制。

线程安全的核心理念就是“**要不只读，要不加锁**”。JDK 提供的并发包，主要分成以下几个类族：

（1）**线程同步类**。这些类使线程间的协调更加容易，支持了更加丰富的线程协调场景，逐步淘汰了使用 Object 的 wait() 和 notify() 进行同步的方式。主要代表有 CountDownLatch、Semaphore、CyclicBarrier 等。

（2）**并发集合类**。集合并发操作的要求是执行速度快，提取数据准。最典型的莫过于 ConcurrentHashMap ，经过不断的优化，有刚开始的分段式锁到后来的 CAS ，不断的提高并发性能。除此之外，还有 ConcurrentSkipListMap 、 CopyOnWriteArrayList 、BlockingQueue 等。

（3）**线程管理类**。虽然 Thread 和 ThreadLocal 在 JDK1.0 就已经引入，但是真正把 Thread 的作用发挥到极致的是线程池。根据实际场景的需要，提供了多种创建线程池的快捷方式，如使用 Executors 静态工厂或者使用 ThreadPoolExecutors 等。另外，通过 ScheduledExecutorService 来执行定时任务。

（4）**锁相关类**。锁以 Lock 接口为核心，派生出一些实际场景中进行互斥操作的锁相关类。最有名的是 ReentrantLock 。

原文链接：https://zhuanlan.zhihu.com/p/488001949

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.441】设计模式—代理模式以及动态代理的实现

代理模式（Proxy Design Pattern）是为一个对象提供一个替身，以控制对这个对象的访问。即通过代理对象访问目标对象。被代理的对象可以是远程对象、创建开销大的对象或需要安全控制的对象。

## **1.代理模式介绍**

在结束创建型模式的讲解后，从这一篇开始就进入到了结构型模式，结构型模式主要是总结一些类和或对象组合在一起的结构。代理模式在不改变原始代理类的情况下，通过引入代理类来给原始类附加功能。

代理模式的主要结构如下：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213160501_14222.jpg)

1. `Subject`：抽象主题类，通过接口或抽象类声明主题和代理对象实现的业务方法
2. `RealSubject`：真实主题类，实现`Subject`中的具体业务，是代理对象所代表的真实对象
3. `Proxy`：代理类，其内部含有对真实主题的引用，它可以访问、控制或扩展`RealSubject`的功能
4. `Client`：客户端，通过使用代理类来访问真实的主题类

按照上面的类图，可以实现如下代码：

```
//主题类接口
public interface Subject {
    void Request();
}
//真实的主题类
public class RealSubject implements Subject{
    @Override
    public void Request() {
        System.out.println("我是真实的主题类");
    }
}
//代理类
public class Proxy implements Subject{
    private RealSubject realSubject;
    @Override
    public void Request() {
        if (realSubject == null) {
            realSubject = new RealSubject();
        }
        realSubject.Request();
    }
}
//客户端
public class Client {
    public static void main(String[] args) {
        Proxy proxy = new Proxy();
        proxy.Request();
    }
}
```

代理模式有比较广泛的使用，比如`Spring AOP`、`RPC`、缓存等。在 Java 中，根据代理的创建时期，可以将代理模式分为静态代理和动态代理，下面就来分别阐述。

## **2.代理模式实现**

动态代理和静态代理的区分就是语言类型是在运行时检查还是在编译期检查。

### **2.1 .静态代理**

静态代理是指在编译期，也就是在JVM运行之前就已经获取到了代理类的字节码信息。即Java源码生成`.class`文件时期：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213160502_90225.jpg)

由于在JVM运行前代理类和真实主题类已经是确定的，因此也被称为静态代理。

在实际使用中，通常需要定义一个公共接口及其方法，被代理对象（目标对象）与代理对象一起实现相同的接口或继承相同的父类。其实现代码就是第一节中的代码。

### **2.2.动态代理**

动态代理，也就是在JVM运行时期动态构建对象和动态调用代理方法。

常用的实现方式是反射。**反射机制**是指程序在运行期间可以访问、检测和修改其本身状态或行为的一种能力，使用反射我们可以调用任意一个类对象，以及其中包含的属性及方法。比如JDK Proxy。

此外动态代理也可以通过ASM(Java 字节码操作框架)来实现。比如CGLib。

#### **2.2.1.JDK Proxy**

这种方式是JDK自身提供的一种方式，它的实现不需要引用第三方类，只需要实现`InvocationHandler`接口，重写`invoke()`方法即可。代码实现如下所示：

```
public class ProxyExample {
    static interface Car {
        void running();
    }
    static class Bus implements Car {
        @Override
        public void running() {
            System.out.println("bus is running");
        }
    }
    static class Taxi implements Car {
        @Override
        public void running() {
            System.out.println("taxi is runnig");
        }
    }
    //核心部分 JDK Proxy 代理类
    static class JDKProxy implements InvocationHandler {
        private Object target;
        public Object getInstance(Object target) {
            this.target = target;
            //获得代理对象
            return Proxy.newProxyInstance(target.getClass().getClassLoader(), target.getClass().getInterfaces(), this);
         }
        @Override
        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
            Object result = method.invoke(target, args);
            return result;
        }
    }
    public static void main(String[] args) {
        JDKProxy jdkProxy = new JDKProxy();
        Car instance = (Car) jdkProxy.getInstance(new Taxi());
        instance.running();
    }
}
```

动态代理的核心是实现`Invocation`接口，我们再看看`Invocation`接口的源码：

```
public interface InvocationHandler {
    public Object invoke(Object proxy, Method method, Object[] args)
        throws Throwable;
}
```

实际上是通过`invoke()`方法来触发代理的执行方法。最终使得实现`Invocation`接口的类具有动态代理的能力。

动态代理的好处在于不需要和静态代理一样提前写好公共的代理接口，只需要实现`Invocation`接口就可拥有动态代理能力。下面我们再来看看 CGLib 是如何实现的

#### **2.2.2. CGLib**

CGLib 动态代理采取的是创建目标类的子类的方式，通过子类化，我们可以达到近似使用被调用者本身的效果。其实现代码如下所示：

```
public class CGLibExample {
    static class car {
        public void running() {
            System.out.println("car is running");
        }
    }
    static class CGLibProxy implements MethodInterceptor {
        private Object target;
        public Object getInstance(Object target) {
            this.target = target;
            Enhancer enhancer = new Enhancer();
            //设置父类为实例类
            enhancer.setSuperclass(this.target.getClass());
            //回调方法
            enhancer.setCallback(this);
            //创建代理对象
            return enhancer.create();
        }
        @Override
        public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable {
            Object result = methodProxy.invokeSuper(o, objects);
            return result;
        }
    }
    public static void main(String[] args) {
        CGLibProxy cgLibProxy = new CGLibProxy();
        car instance = (car) cgLibProxy.getInstance(new car());
        instance.running();
    }
}
```

从代码可以看出CGLib 也是通过实现代理器的接口，然后再调用某个方法完成动态代理，不同的是CGLib在初始化被代理类时，是通过`Enhancer`对象把代理对象设置为被代理类的子类来实现动态代理：

```
Enhancer enhancer = new Enhancer();
//设置父类为实例类
enhancer.setSuperclass(this.target.getClass());
//回调方法
enhancer.setCallback(this);
//创建代理对象
return enhancer.create();
```

#### **2.2.3. JDK Proxy 和 CGLib 的区别**

1. 来源：JDK Proxy 是JDK 自带的功能，CGLib 是第三方提供的工具
2. 实现：JDK Proxy 通过拦截器加反射的方式实现；CGLib 基于ASM实现，性能比较高
3. 接口：JDK Proxy 只能代理继承接口的类，CGLib 不需要通过接口来实现，它是通过实现子类的方式来完成调用

## **3.代理模式的应用场景**

### **3.1. MapperProxyFactory**

在MyBatis 中，也存在着代理模式的使用，比如`MapperProxyFactory`。其中的`newInstance()`方法就是生成一个具体的代理来实现功能，代码如下：

```
public class MapperProxyFactory<T> {
  private final Class<T> mapperInterface;
  private final Map<Method, MapperMethodInvoker> methodCache = new ConcurrentHashMap<>();
  public MapperProxyFactory(Class<T> mapperInterface) {
    this.mapperInterface = mapperInterface;
  }
  public Class<T> getMapperInterface() {
    return mapperInterface;
  }
  public Map<Method, MapperMethodInvoker> getMethodCache() {
    return methodCache;
  }
  // 创建代理类
  @SuppressWarnings("unchecked")
  protected T newInstance(MapperProxy<T> mapperProxy) {
    return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] { mapperInterface }, mapperProxy);
  }
  public T newInstance(SqlSession sqlSession) {
    final MapperProxy<T> mapperProxy = new MapperProxy<>(sqlSession, mapperInterface, methodCache);
    return newInstance(mapperProxy);
  }
}
```

### **3.2. Spring AOP**

代理模式最常使用的一个应用场景就是在业务系统中开发一些非功能性需求，比如监控、统计、鉴权、限流、事务、日志等。将这些附加功能与业务功能解耦，放在代理类中统一处理，让程序员只需要关注业务方面的开发。而Spring AOP 的切面实现原理就是基于动态代理

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213160502_63197.jpg)

Spring AOP 的底层通过上面提到的 JDK Proxy 和 CGLib动态代理机制，为目标对象执行横向织入。当Bean实现了接口时， Spring就会使用JDK Proxy，在没有实现接口时就会使用 CGLib。也可以在配置中强制使用 CGLib：

```
<aop:aspectj-autoproxy proxy-target-class="true"/>
```

### **3.3. RPC 框架的封装**

RPC 框架的实现可以看作成是一种代理模式，通过远程代理、将网络同步、数据编解码等细节隐藏起来，让客户端在使用 RPC 服务时，不必考虑这些细节。

原文链接：https://zhuanlan.zhihu.com/p/489003709

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.442】后端开发—一文详解网络IO模型

### 1.网络IO：

网络IO会涉及到两个系统对象 一个是 用户空间 调用 IO 的进程或者线程，另一个是 内核

空间的 内核系统， 比如 发生 IO 操作 read 时，它会经历两个阶段

1. 等待数据准备就绪

2. 将数据 从内核拷贝到进程 或者线程 中。

### 2.服务器模型 Reactor 与 Proactor

　　 对高并发编程网络连接上的消息处理，可以分为两个阶段：等待消息准备好、消息 处理。当使用默认的阻塞套接字时（例如上面提到的 1 个线程捆绑处理 1 个连接），往往是把这两个阶段合而为一，这样操作套接字的代码所在的线程就得睡眠来等待消息准备好，这导致了高并发下线程会频繁的睡眠、唤醒，从而影响了 CPU 的使用效率。高并发编程方法当然就是把两个阶段分开处理。即，等待消息准备好的代码段，与处理消息的代码段是分离的。当然，这也要求套接字必须是非阻塞的，否则，处理消息的代码段很容易导致条件不满足时，所在线程又进入了睡眠等待阶段。那 么问题来了，等待消息准备好这个阶段怎么实现？它毕竟还是等待，这意味着线程还是要睡眠的！解决办法就是，线程主动查询，或者让 1 个线程为所有连接而等待！这就是 IO 多路复用了。多路复用就是处理　　等待消息准备好这件事的，但它可以同时处理多个连接！它也可能 等待 ””，所以它也会导致线程睡眠，然而这不要紧，因为它一对多、它可以监控所有连接。这样，当我们的线程被唤醒执行时，就一定是有一些连接准备好被我们的代码执行了。

作为一个高性能服务器程序通常需要考虑处理三类事件： I/O 事件，定时事件及信号。两种 高效 的事件处理模型： Reactor 和 Proactor。

### 3.Reactor模型

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213160712_29194.jpg)

Reactor 释义 反应堆 ””，是一种事件驱动机制。和普通函数调用的不同之处在于：应用程序不是主动的调用某个 API 完成处理，而是恰恰相反， Reactor 逆置了事件处理流程，应用程序需要提供相应的接口并注册到 Reactor 上，如果相应的时间发生， Reactor 将主动调用应用程序注册的接口，这些接口又称为 回调函数 。

Reactor 模式是处理并发I/O 比较常见的一种模式，用于同步I/O，中心思想是将所有要处理的I/O 事件注册到一个中心I/O 多路复用器上，同时主线程/进程阻塞在多路复用器上；一旦有I/O 事件到来或是准备就绪(文件描述符或socket 可读、写)，多路复用器返回并将事先注册的相应I/O 事件分发到对应的处理器中。

### 4.Reactor 模型有三个重要的组件：

多路复用器：由操作系统提供，在linux 上一般是select, poll, epoll 等系统调用。

事件分发器：将多路复用器中返回的就绪事件分到对应的处理函数中。

事件处理器：负责处理特定事件的处理函数。

### 5.具体流程如下：

注册读就绪事件和相应的事件处理器；

事件分离器等待事件；

事件到来，激活分离器，分离器调用事件对应的处理器；

事件处理器完成实际的读操作，处理读到的数据，注册新的事件，然后返还控制权；

### 6.Reactor 模式是编写高性能网络服务器的必备技术之一，它具有如下的优点：

响应快；编程相对简单，可以最大程度的避免复杂的多线程及同步问题，并且 避免了多线程进程的切换开销可扩展性，可以方便的通过增加 Reactor 实例个数来充分利用 CPU 资源；可复用性， reactor 框架本身与具体事件处理逻辑无关，具有很高的复用性；

### 7.实际Reactor 模型可能是这样：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213160712_74383.jpg)

### 8.Proactor模型

具体流程如下：

处理器发起异步操作，并关注I/O 完成事件

事件分离器等待操作完成事件

分离器等待过程中，内核并行执行实际的I/O 操作，并将结果数据存入用户自定义缓冲区，最后通知事件分离器读操作完成I/O 完成后，通过事件分离器呼唤处理器事件处理器处理用户自定义缓冲区中的数据

　　我们可以发现proactor 模型最大的特点就是Proactor 最大的特点是使用异步I/O。所有的I/O 操作都交由系统提供的异步I/O 接口去执行。工作线程仅仅负责业务逻辑。在Proactor 中，用户函数启动一个异步的文件操作。同时将这个操作注册到多路复用器上。多路复用器并不关心文件是否可读或可写而是关心这个异步读操作是否完成。异步操作是操作系统完成，用户程序不需要关心。多路复用器等待直到有完成通知到来。当操作系统完成了读文件操作——将读到的数据复制到了用户先前提供的缓冲区之后，通知多路作系统完成了读文件操作——将读到的数据复制到了用户先前提供的缓冲区之后，通知多路复用器相关操作已完成。多路复用器再调用相应的处理程序，处理数据。

　　　　Proactor 增加了编程的复杂度，但给工作线程带来了更高的效率。Proactor 可以在系统态将读写优化，利用I/O 并行能力，提供一个高性能单线程模型。在windows 上，由于没有epoll 这样的机制，因此提供了IOCP 来支持高并发， 由于操作系统做了较好的优化，windows 较常采用Proactor 的模型利用完成端口来实现服务器。在linux 上，在2.6 内核出现了aio 接口，但aio 实际效果并不理想，它的出现，主要是解决poll 性能不佳的问题，但实际上经过测试，epoll 的性能高于poll+aio，并且aio 不能处理accept，因此linux 主要还是以Reactor 模型为主。

### 9.Libevent libev libuv

libevent

名气最大，应用最广泛，历史悠久的跨平台事件库；

libev :

较较libevent而言，设计更简练，性能更好，但对而言，设计更简练，性能更好，但对Windows支持不够好

libuv :

开发nodenode的过程中需要一个跨平台的事件库，他们首选了的过程中需要一个跨平台的事件库，他们首选了libev，但又要支持Windows，故重新封装了一套，linux下用libev实现，Windows下用IOCP实现；

### 10.总结:

　　掌握了Reactor原理，为以后学习其他网络框架底层代码，打下了基础。

原文链接：https://zhuanlan.zhihu.com/p/489567205

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.443】【技术干货分享】一文了解Nginx反向代理与conf原理

## 1.Nginx主要功能

Webservice， 反向代理， 负载均衡。

逻辑上，nginx和server的关系是这样的:

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213161644_44931.jpg)

## 2.Nginx和路由器/交换机有什么区别？

路由器是物理网关，nginx是应用层网关。

物理上，他们的关系是下图这样的。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213161644_28618.jpg)

Nginx、haproxy、lvs、F5，都可以做负载均衡，有什么区别？

他们处于tcp/ip协议不同层。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213161645_14681.jpg)

## 3.Nginx安装

所需要的安装包nginx、openssl、pcre、zlib

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213161646_23751.png)

安装命令

./configure –prefix=/usr/local/nginx –with-http_realip_module –with-http_addition_module –with-http_gzip_static_module –with-http_secure_link_module –with-http_stub_status_module –with-stream –with-pcre=/root/nginx/pcre-8.39 –with-zlib=/root/nginx/zlib-1.2.11 –with-openssl=/root/nginx/openssl-1.1.1g

启动nginx

cd /usr/local/nginx

./sbin/nginx -c ./conf/nginx.conf

使用默认配置启动nginx

之后通过浏览器访问nginx。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213161646_38602.jpg)

## 4.conf文件使用

最基本的使用如下：

```
worker_processes 4;
events {
        worker_connections 1024;
}
http {
        server {
                listen 9001;
                location / {
                        proxy_pass http://172.17.0.2:9004;
                }
        }
        server {
                listen 9002;
        }
        server {
                listen 9003;
        }
        server {
                listen 9004;
                location / {
                        root /html;
                }
        }
}
```

### 5.配置worker进程数

```
worker_processes 4;
```

master进程listen端口，网络io由worker进程处理；对于浏览器发送的请求，会产生两个tcp连接，一个是http请求，一个是keep-alive。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213161647_89498.png)

## 6.配置服务

nginx支持http，smtp，websocket等多种应用层协议；也可以listen不同的端口；

```
http {
  server {
      listen 9100;
  }
}xxxxxxxxxx http {  server {      listen 9100;  }}http {  server {      listen 9100;  }}
```

### 7.配置代理

```
http {
  server {
    listen 9001;
    location / {
      proxy_pass http://172.17.0.2:9004;
    }
  }
}
```

### 8.重定向

```
http {
  server {
    listen 9001;
    location / {
      rewrite ^/(.*) https://github.com/congchp/Linux-server redirect;
    }
  }
}
```

### 9.负载均衡

```
        upstream backend {
            server 172.17.0.2:9000 weight=2;
            server 172.17.0.2:9001 weight=1;
        }
        location / {
            proxy_pass http://backend;
        }
```

## 10.提供静态服务内容

图片

```
        location /images/ {
             root /share/;
        }
```

视频

```
        location /vedio/ {
             root /share/;
        }
```

## 11.支持CGI

common gateway interface, 通用网关接口。nginx通过stdin、stdout也cgi程序进行通信。

主要用在哪里？在线编译工具。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213161648_15722.jpg)

编写一个cgi程序

```
// gcc -o hello_cgi hello_cgi.h -lfcgi
#include <stdio.h>
#include <fcgi_stdio.h>
int main() {
    while (FCGI_Accept() >= 0) {
        printf(" Content-type: text/html\r\n");
        printf("\r\n");
        printf("<title>Fast CGI Hello! </title>");
        printf("<h1>Congchp cgi</h1>");
        printf("Thank you cgi\n");
    }
}
```

nginx使用cgi，需要安装fastcgi和spawn-fcgi。cgi是一请求一进程；fastcgi是一个进程池，通过进行管理器对cgi进行管理。spawn-fcgi是fastcgi进行管理器，管理cgi。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213161648_97319.png)

spawn-fcgi编译

```
./configure
make
#将编译后的spawn-fcgi程序copy到/usr/local/nginx目录下
cp ./src/spwan-fcgi /usr/local/nginx
```

fastcgi编译安装

```
./configure
make
make install
```

通过spawn-fcgi启动cgi程序。

```
./spawn-fcgi -a 127.0.0.1 -p 9101 -f /share/hello_cgi
```

nginx配置

```
    server {
        listen 9102;
        location / {
            fastcgi_pass 127.0.0.1:9101;
            fastcgi_index index.cgi;
            fastcgi_param SCRIPT_FILENAME cgi$fastcgi_script_name;
            include ../conf/fastcgi_params;
        }
    }
```

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213161649_91770.jpg)

## 12.什么是惊群？

惊群分为3种：

1. accept
2. epoll_wait
3. pthead_cond_wait

accept惊群和pthead_cond_wait的惊群，内核已经解决了。

nginx是存在epoll_wait惊群的，为什么呢？

所有workder进程都是从master进程fork出来的，都对同一个端口进行listen，如果在所有worker进程中使用epoll对listenfd进行检测，必然会出现epoll_wait的惊群。

惊群并不会导致业务问题，只会造成很多无效的唤醒，高性能的服务器是不能接受这一点的。

nginx是如何解决epoll_wait惊群的？

通过加accept锁，通过shmem共享内存实现进程锁。

为保证只有一个worker进程对新连接进行处理，所有worker进程在向epoll注册listenfd读事件前抢accept_mutex, 抢到互斥锁的那个进程注册listenfd读事件，在读事件里面调用accept接受该连接。当一个worker进程在accept这个连接之后，就开始读取请求，解析请求，处理请求，产生response后，再返回给客户端，最后才断开连接。一个请求，完全由一个worker进程来处理，而且只在一个worker进程中处理。

其他没有抢到accept锁的worker进程，可以处理其他event。

```
while (1) {
    ret = try_lock(lock);
    if (ret == 0)
        epll_ctl(epfd, EPOLL_ADD, listenfd, EPOLL_IN);
    epoll_wait();
        if (events[i].data.fd == listenfd)
            unlock(lock);
}
```

原文链接：https://zhuanlan.zhihu.com/p/490362306

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.444】Linux环境，C/C++语言手写代码实现线程池

前言

在我们日常生活中会遇到许许多多的问题，如果一个服务端要接受很多客户端的数据，该怎么办？多线程并发内存不够怎么办？所以我们需要了解线程池的相关知识。

### 1.线程池是什么？

1.线程池的简介

线程池是一种多线程处理形式，处理过程中将任务添加到队列，然后在创建线程后自动启动这些任务。线程池线程都是后台线程。每个线程都使用默认的堆栈大小，以默认的优先级运行，并处于多线程单元中。如果某个线程在托管代码中空闲（如正在等待某个事件）,则线程池将插入另一个辅助线程来使所有处理器保持繁忙。如果所有线程池线程都始终保持繁忙，但队列中包含挂起的工作，则线程池将在一段时间后创建另一个辅助线程但线程的数目永远不会超过最大值。超过最大值的线程可以排队，但他们要等到其他线程完成后才启动。

### 2.线程池的组成

1、线程池管理器（ThreadPoolManager）:用于创建并管理线程池

2、工作线程（WorkThread）: 线程池中线程

3、任务接口（Task）:每个任务必须实现的接口，以供工作线程调度任务的执行。

4、任务队列:用于存放没有处理的任务。提供一种缓冲机制。

### 3.线程池的主要优点

1.避免线程太多，使得内存耗尽

2.避免创建与销毁线程的代价

3.任务与执行分离

### 4.代码实现

1.线程池结构体定义

代码如下（示例）：

```
struct nTask {
    void (*task_func)(struct nTask* task);
    void* user_data;
    struct nTask* prev;
    struct nTask* next;
};
struct nWorker {
    pthread_t threadid;
    int terminate;
    struct nManager* manager;
    struct nWorker* prev;
    struct nWorker* next;
};
typedef struct nManager {
    struct nTask* tasks;
    struct nWorker* workers;
    pthread_mutex_t mutex;//定义互斥锁
    pthread_cond_t cond;//条件变量
}ThreadPool;
```

## 5.接口定义

代码如下（示例）：

```
//API
int nThreadPoolCreate(ThreadPool* pool, int numWorkers) {
    if (pool == NULL) return -1;
    if (numWorkers < 1) numWorkers = 1;
    memset(pool, 0, sizeof(ThreadPool));
    pthread_cond_t blank_cond = PTHREAD_COND_INITIALIZER;
    memcpy(&pool->cond, &blank_cond, sizeof(pthread_cond_t));
    pthread_mutex_t blank_mutex = PTHREAD_MUTEX_INITIALIZER;
    memcpy(&pool->mutex, &blank_mutex, sizeof(pthread_mutex_t));
    int i = 0;
    for (i = 0; i < numWorkers; i++) {
        struct nWorker* worker = (struct nWorker*)malloc(sizeof(struct nWorker));
        if (worker == NULL) {
            perror("malloc");
            return -2;
        }
        memset(worker, 0, sizeof(struct nWorker));
        worker->manager = pool;
        int ret = pthread_create(&worker->threadid, NULL, nThreadPoolCallback, worker);
        if (ret) {
            perror("pthread_create");
            free(worker);
            return -3;
        }
        LIST_INSERT(worker, pool->workers);
    }
    return 0;
}
//API
int nThreadPoolDestory(ThreadPool* pool, int nWorker) {
    struct nWorker* worker = NULL;
    for (worker = pool->workers; worker != NULL; worker = worker->next) {
        worker->terminate;
    }
    pthread_mutex_lock(&pool->mutex);
    pthread_cond_broadcast(&pool->cond);//唤醒所有线程
    pthread_mutex_unlock(&pool->mutex);
    pool->workers = NULL;
    pool->tasks = NULL;
    return 0;
}
//API
int nThreadPoolPushTask(ThreadPool* pool, struct nTask* task) {
    pthread_mutex_lock(&pool->mutex);
    LIST_INSERT(task, pool->tasks);
    pthread_cond_signal(&pool->cond);//唤醒一个线程
    pthread_mutex_unlock(&pool->mutex);
}
```

## 6.回调函数

代码如下（示例）：

```
static void* nThreadPoolCallback(void* arg) {
    struct nWorker* worker = (struct nWorker*)arg;
    while (1)
    {
        pthread_mutex_lock(&worker->manager->mutex);
        while (worker->manager->tasks == NULL) {
            if (worker->terminate)break;
            pthread_cond_wait(&worker->manager->cond, &worker->manager->mutex);
        }
        if (worker->terminate) {
            pthread_mutex_unlock(&worker->manager->mutex);
            break;
        }
        struct nTask* task = worker->manager->tasks;
        LIST_REMOVE(task, worker->manager->tasks);
        pthread_mutex_unlock(&worker->manager->mutex);
        task->task_func(task);
    }
    free(worker);
    return;
}
```

## 7.全部代码（加注释）

代码如下（示例）：

```
#include<stdio.h>
#include<string.h>
#include<stdlib.h>
#include<pthread.h>//Linux编程使用线程相关的就会用到这个，编译的时候还要加-lpthread，用pthread动态库
#define LIST_INSERT(item,list) do{    \            //链表的增加
        item->prev = NULL;            \
        item->next = list;            \
    if((list)!=NULL)(list)->prev=item;    \
        (list) = item;                \
}while(0)
#define LIST_REMOVE(item,list)do{    \            //链表的删除
        if (item->prev != NULL) item->prev->next = item->next;    \
        if (item->next != NULL) item->next->prev = item->prev;    \
        if (item == list) list = item->next;                    \
        item->prev = item->next = NULL;                            \
}while(0)
struct nTask {        //任务
    void (*task_func)(struct nTask* task);
    void* user_data;
    struct nTask* prev;
    struct nTask* next;
};
struct nWorker {    //工作
    pthread_t threadid;
    int terminate;    //用来判断工作是否结束，以便于终止
    struct nManager* manager;    //可以用来查看任务
    struct nWorker* prev;
    struct nWorker* next;
};
typedef struct nManager {    //管理
    struct nTask* tasks;
    struct nWorker* workers;
    pthread_mutex_t mutex;//定义互斥锁，也可以用自旋锁
    pthread_cond_t cond;//条件变量
}ThreadPool;
//callback!=task
static void* nThreadPoolCallback(void* arg) {
    struct nWorker* worker = (struct nWorker*)arg;    //接受pthread_create传来的参数
    while (1)    //默认一直执行
    {
        pthread_mutex_lock(&worker->manager->mutex);//加锁
        while (worker->manager->tasks == NULL) {
            if (worker->terminate)break;
            pthread_cond_wait(&worker->manager->cond, &worker->manager->mutex);//如果没有任务就等待
        }
        if (worker->terminate) {
            pthread_mutex_unlock(&worker->manager->mutex);//解锁
            break;
        }
        struct nTask* task = worker->manager->tasks;
        LIST_REMOVE(task, worker->manager->tasks);
        pthread_mutex_unlock(&worker->manager->mutex);
        task->task_func(task);
    }
    free(worker);
    return;
}
//API
int nThreadPoolCreate(ThreadPool* pool, int numWorkers) {
    if (pool == NULL) return -1;
    if (numWorkers < 1) numWorkers = 1;//如果任务数量小于1，就默认为1
    memset(pool, 0, sizeof(ThreadPool));
    pthread_cond_t blank_cond = PTHREAD_COND_INITIALIZER;//创建互斥量cond，静态全局
    memcpy(&pool->cond, &blank_cond, sizeof(pthread_cond_t));
    //pthread_mutex_init(&pool->mutex, NULL);
    pthread_mutex_t blank_mutex = PTHREAD_MUTEX_INITIALIZER;//创建互斥锁，静态全局
    memcpy(&pool->mutex, &blank_mutex, sizeof(pthread_mutex_t));
    int i = 0;
    for (i = 0; i < numWorkers; i++) {
        struct nWorker* worker = (struct nWorker*)malloc(sizeof(struct nWorker));
        if (worker == NULL) {
            perror("malloc");
            return -2;
        }
        memset(worker, 0, sizeof(struct nWorker));
        worker->manager = pool;
        int ret = pthread_create(&worker->threadid, NULL, nThreadPoolCallback, worker);
        if (ret) {
            perror("pthread_create");
            free(worker);
            return -3;
        }
        LIST_INSERT(worker, pool->workers);
    }
    return 0;
}
//API
int nThreadPoolDestory(ThreadPool* pool, int nWorker) {
    struct nWorker* worker = NULL;
    for (worker = pool->workers; worker != NULL; worker = worker->next) {
        worker->terminate;
    }
    pthread_mutex_lock(&pool->mutex);
    pthread_cond_broadcast(&pool->cond);//唤醒所有线程
    pthread_mutex_unlock(&pool->mutex);
    pool->workers = NULL;//置空
    pool->tasks = NULL;//置空
    return 0;
}
//API
int nThreadPoolPushTask(ThreadPool* pool, struct nTask* task) {
    pthread_mutex_lock(&pool->mutex);
    LIST_INSERT(task, pool->tasks);
    pthread_cond_signal(&pool->cond);//唤醒一个线程
    pthread_mutex_unlock(&pool->mutex);
}
#if 1
#define THREADPOOL_INIT_COUNT    20
#define TASK_INIT_SIZE            1000
void task_entry(struct nTask* task) {
    int idx = *(int*)task->user_data;
    printf("idx: %d\n", idx);
    free(task->user_data);
    free(task);
}
int main(void) {
    ThreadPool pool = { 0 };
    nThreadPoolCreate(&pool, THREADPOOL_INIT_COUNT);
    int i = 0;
    for (i = 0; i < TASK_INIT_SIZE; i++) {
        struct nTask* task = (struct nTask*)malloc(sizeof(struct nTask));
        if (task == NULL) {
            perror("malloc");
            exit(1);
        }
        memset(task, 0, sizeof(struct nTask));
        task->task_func = task_entry;
        task->user_data = malloc(sizeof(int));
        *(int*)task->user_data = i;
        nThreadPoolPushTask(&pool, task);
    }
    getchar();//防止主线程提前结束任务
}
#endif
```

## 8.总结

关于线程池是基本代码就在上面了，关于编程这一部分内容，我建议大家还是要自己去动手实现，如果只是单纯的看了一遍，知识这块可能会记住，但是操作起来可能就比较吃力，万事开头难，只要坚持下去，总会有结果的。

原文链接：https://zhuanlan.zhihu.com/p/490878260

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.445】一文掌握tcp服务器epoll的多种实现

## 1.tcp服务器epoll的多种实现

- 总结

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162353_73187.jpg)

我们在读写文件的时候，这是一款服务器，CS，这是一个服务器，这个客户端去连接服务器的时候，中间大家知道从连接的这个过程中间产生通过三次握手连接，服务器先进行监听一个端口，监听的时候是用调用listen进行监听，

TCP网络编程模型就好比这样一个模型，大家去酒店吃饭，走到那个饭店的门口门口有一个迎宾的人，有迎宾的小姐姐，然后你跟她说吃饭，她把你带进餐馆里面，然后给你介绍一个真正的服务员，后面你点菜买单，然后包括像夹菜点酒都是由这个服务员为你去服务。

这里面这个酒店出现两个人，一个是对你的迎宾的人，第二个为你服务吃饭的服务员，就这个模式。

大家可以看到客户端和服务通信也是这样一个情况，客户端就是跑到酒店去吃饭，
走到服务器这地方，服务器这里有一个listen的端口，就是迎宾的人，然后介绍一个服务员，后面客户端进行通信的时候就是介绍的这个服务员进行通信。

就是这个迎宾的人怎么让他迎宾，怎么让他开始工作？
迎宾的人上哪里去找？首先记清楚他是一个人，那这个人怎么来呢？
这里面所提到的人就是 fd，我们可以通过socket创建，创建完之后，还有比如说一个酒店特别大，比如说这个酒店特别大，它有几个门那这个迎宾的小姐姐他去那个门？他只能做一件事情，他去哪个门，所以这里我们需要绑定他在哪个门，bind()一个端口，绑定完之后，他才开始工作，真正开始工作是进入listen状态。

我想问一下这迎宾的小姐姐在listen的过程中间，他跟酒店这个管理制度有没有关系，跟这个酒店的工作流程有没有关系？没用，所以刚才讲个迎宾的小姐姐，她是一个被动的，她是一个被动的工作，就是他的工作就是listen，他不影响酒店的内部的整个工作流程，首先这点一定要记住，listen一旦进入之后，他不影响其它流程，

然后当来了一个客人之后，这时候要创建一个新的，accept才是真正接待的人，listen这个过程是指定这个迎宾的小姐姐在这个门口迎宾的工作。真的来一个客人的话，accept()介绍一个新的服务员,后面的动作点菜也好，买单也好，recv和send都是跟这个服务员之间沟通,包括close买单的时候也是。

服务器它写代码的就这么写，它就是这些接口，这个listen这个过程他就是让这个小姐姐在门口迎宾，然后剩下的就是小姐姐把这个人带进去介绍一个新的服务员

**这个过程，三次握手是服务员在迎宾的这个过程中间发生的，三次握手他是被动发生的，**

三次握手是在这个迎宾的人在这等待的过程中间完成，三次握手之后才把他介绍了这个新的服务员，
有人吃饭你们有人问到你是在这吃饭都说是的，这个过程叫三次握手，收到客户说是的，再把他带进去介绍服务员,

客户端这边首先吃饭，客户端也是一个人，这个人也通过socket创建的，创建完对应的他去哪里吃饭，他去哪里吃饭？准备一个地址，准备一个地址然后connect。

这个过程就是客户端连接服务器，这个过程可以绑定也可以不绑定，这个绑定的就相当于这个他出去吃饭，他的的每个门主他可以从大门走也可以小门，所以这个都okay，如果只允许他从正门走，那这时候就给他绑定一下

它是去连接的，你可以绑定也可以不绑定，所以我认为在客户当中出现了两个地址，一个是绑定的，一个是不绑定的，绑定和绑定它是一个可选项，它是一个可选项

客户端调用contact函数就是决定了它去连接这个服务器，每一个服务器IP地址端口都是多少，去找哪个酒店迎宾的小姐姐，这是确定的是在connect确定的，
这地方就连接完成之后再调用的是send和recv这是网络编程的整个流程。

这是网络编程核心的，以这些东西为基础就衍生出来了很多的做法，客户端和服务器就是这个流程

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162354_80518.jpg)

## 2.**udp的服务器该怎么做？**

**udp就好比你去一个小餐馆，那个迎宾的人跟服务员他是一个人**，就是你去了一个小饭店，没有连接在这方面，这个客户端和服务器之间不涉及到任何的连接，这个应该不用去争议的时候，

我们在设计函数的时候它的功能要做到单一讲究的一个功能，就是还是单一功能单一越单一越好。可利用可复用性就越强，就是一个函数设计功能越单一，它的可复用性就会越强，一个函数它越符号它的可复用性就越低。

recvfrom函数就有点意思，它不单只接收数据，并且还把哪个客户端给谁发都带出来了，就是recvfrom函数数据是谁发的都知道。

那现在考虑一下，比如说现在有三个udp的客户端，就是三个函数，同样都是都用sendto都是发给这一个端口，都是recvfrom由他来一个接受，

就好比一个小的餐馆，现在这个餐馆只有一个迎宾的人，现在过去了多批人多活人同时走到这个门口，我就想问一下，那这个迎宾的人，**他能分清楚这三户人？他不可以，**

这个服务端我们只有1个，然后如果10个1000个2000个同时调用sendoto，
我就想问一下那这个服务呢它能区分区分不了，区分不了也就是换一个说法，udp的服务器如何做多个客户端并发？

那也就是我们在udp的这个基础上面recvfrom，可能很多人会想一个办法，就是buffer里面想办法，也就是说从客户端发到服务器的时候，这个数据上面想办法，
就是我定义一种本地的协议，就是说我发送的数据包我知道是购买a发的，购完b发的数据我能知道它是b发的，然后c发的我就知道那可能同学在想的，**就是我能不能通过这个数据包括应用协议来解决这个问题**。

就是给buffer加客户端头，但是客户端口端加客户端头是有一个巨大的前提？

我们现在讲的第一个方法就是在我们发送的数据包上面加一层协议能不能行？
好很多朋友认为可以，好，
我跟大家讲一下不可以，为什么？

其它是有很大的前提，就是需要顺序的接收，就是需要顺序接收这是可以的。
就是我们知道a发的先到先发的先到后，发的后到那这种情情况下面它是ok的，那你可以在协议里面加一协议，这是可以的，在公网的情况下，面udp是很难保证先发的先到后发的后报可能有可能先发的数据后到后发的，数据先到。

第二种方法
其实原理也很简单，请他注意那我们用udp来做，其他注意我们也可以仿造了刚刚TCP这个迎宾的这个模式迎宾的这个人的模式，那么也我们也可以仿照这个模式，
好这样一个模式做，那怎么做呢你怎么做对吧？

我们也是这样的，创建一个迎宾的人在这里等着，
也就客户他先sendto给服务器这个迎宾的人创建一个，然后迎宾的接收到一个数据包之后，再为他准备一个服务员跟TCP一样，与其实那个就是一模一样，也准备一个服务员，也就是在那个为了真的区分好每一个客户每一个群体每个群体每个客户是谁，那个火车站前面那个小饭馆也采用了酒店这种模式，一个迎宾者和一个点赞的人，一个迎宾者一个点赞的，在各方面就采用这种模式，做一连接之后，sendto先给个迎宾的人，然后服务端分配一个独有的fd给这个客户端进行服务，然后后面sendto就回来的数据，就是从这个新的fd开始发送出来

就是说新的这个 fd对它进行发送。
这就是刚才讲的用udp模拟TCP的三次过程实现udp并发

socket是什么？

很难理解很难理解这个为什么叫做插座，fd与五元组配套

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162355_46943.jpg)

在这里插入图片描述

为什么这个fd它只对应到这个五元组里面，只有这个tcp和udp？

比如说我想去抓以太网的包那怎么做，其实相当于那时候我们使用的是一个叫原生的socket

好，我讲就这一份关于fd我们既可以把它当做文件描述，主要理解也可以把它当做网络通信的一个观点，一个通信的一个句名就是我们拿着它，就相当于拿到一个客户端一样，这个fd给代表来理解说我们可以把它当文件描述，也可以当做一个客户端，

先跟大家解释一下它关于文件描述的一些事情，文件描述的一些事情那就是io的一些事情。

讲到这里之后要给大家介绍一个很老很老的东西，为什么跟大家去讲这些非常老的东西，啊大家才能够知道这个技术它迭代的这个分数它怎么迭代的一个非常非常老的东西，大家可能清楚。
就是叫sigio有可能听过，有些没听过叫sigio

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162356_99388.jpg)

现在一个客户端给他发送一个数据，我们调一个sendto给他发一点数据，请大家注意请大家注意，这是一个socket，我们在一个端口进行接受recvfrom。
由于当发过去的时候，这个 io他现在有数据了，他就会去触犯这个 singio信号 也就是说
这个东西就有意思，他就跟我们进程通信，就好比我们常用的一个命令叫kill-9，比如在后面加一个进程ID1234一样，
要那也就是说出现一个信息一个信息，大家看到这个kill-9什么意思？9是信号那个 ID给这个进程1234这个进程发一个-9的信号

数据先由操作系统接收的数据，然后再由操作系统去通知对应的这个进程来接触这个数据，sigio是由操作系统把这个信号抛给了对应的这个进程，然后这个进程捕获到这个 sigio的信号，捕获到这个 sigio的信号，所以它才能正常接收数据

```
#include <stdio.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <string.h>
#include <unistd.h>
#include <signal.h>
#include <fcntl.h>
int sockfd = 0;
void do_sigio(int sig) {
    struct sockaddr_in cli_addr;
    int clilen = sizeof(struct sockaddr_in);
    int clifd = 0;
#if 0
    clifd = accept(sockfd, (struct sockaddr*)&cli_addr, &clilen);
    char buffer[256] = {0};
    int len = read(clifd, buffer, 256);
    printf("Listen Message : %s\r\n", buffer);
    int slen = write(clifd, buffer, len);
#else
    char buffer[256] = {0};
    int len = recvfrom(sockfd, buffer, 256, 0, (struct sockaddr*)&cli_addr, (socklen_t*)&clilen);
    printf("Listen Message : %s\r\n", buffer);
    int slen = sendto(sockfd, buffer, len, 0, (struct sockaddr*)&cli_addr, clilen);
#endif
}
int main(int argc, char *argv[]) {
    sockfd = socket(AF_INET, SOCK_DGRAM, 0);
    struct sigaction sigio_action;
    sigio_action.sa_flags = 0;
    sigio_action.sa_handler = do_sigio;
    sigaction(SIGIO, &sigio_action, NULL);
    struct sockaddr_in serv_addr;
    memset(&serv_addr, 0, sizeof(serv_addr));
    serv_addr.sin_family = AF_INET;
    serv_addr.sin_port = htons(9096);
    serv_addr.sin_addr.s_addr = INADDR_ANY;
    fcntl(sockfd, F_SETOWN, getpid());
    int flags = fcntl(sockfd, F_GETFL, 0);
    flags |= O_ASYNC | O_NONBLOCK;
    fcntl(sockfd, F_SETFL, flags);
    bind(sockfd, (struct sockaddr*)&serv_addr, sizeof(serv_addr));
    while(1) sleep(1);
    close(sockfd);
    return 0;
}

```

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162357_66065.jpg)

我们接着来分析一下这个代码意味着什么？

那我们现在问一下那个sigio是谁发出来的？
也就是这个进程他能接受的信号，这里有两个点要跟大家讲，第一个点是说的信号接收的流程是
第二个要跟大家讲的就是这个 sigio它意味着什么？
第一个就是关于信号接收是怎么接受?第二个CIO意味着什么？

操作系统发的，首先他解释一下那这个操作系统，他就是当接受那个数据的时候，在网卡接受那个数据的时候，既然是网卡发出来的，就是网卡接收一些数据通知，
操作系统发出sigio消息，那既然它是网卡接收到数据就会发出sigio，那各位我问大家有问题，TCP可不可以？不行，为什么？

因为TCP这个过程中间，TCP的过程中间第一个网络io的消息会很多，
好，那各位部门你可以想象一下，如果我们用TCP来做的话，它的single的信号会比udp要多很多的会要多很多很多，
对吧？
因为大家可以看到这个sigio的时候，它没有区分，是哪个fd没有区分，

为什么tcb不行，是因为tcp处理的时候，
这个情况太多，

那这东西怎么处理？
TCP不可以，那现在问一下大家如果，**现在操作系统要把TCP不去响应tcp的话**，那各位们你应该在哪个地方实现？
你看或者用哪一种形式来屏蔽到来，忽略到这个，那 TCP这过程我们应该在哪个地方去忽略掉这个sigio？

在协议栈这个地方TCP接收到一针数据的时候，不去处理sigio

## 3.sigio 与poll select是怎么联系的？

关于信号我跟大家解释三个点，
第一个点对这个进程而言这些信号如何保存，也就是我们注册的就是在我们这个代码上面，就是关于sigio的这个信号这个回调函数它怎么保存的？

就是大家可以看到这个进程，在我这里执行的时候，它是一个进程，在这个进程里面它关于这个回调函数它怎么保存的，
**第一点就是进程内部如何保存**
**第二个就是当我们调用signal这个函数的时候，signal这个函数的时候如何把这个信号保存到进程里？**
**第三个就是当我们在什么时候发，比如我们kill-9的时候这个信号如何发送？**

这三点从这三点各方面来理解一下，所以各方面如果他有被问到后面他面试的时候有被问到信号是怎么工作，也是这三个方面回答的。

就相当于这么一个进程，中间有一块空间，
他专门用了存信号，这个信号他也可以思考一下，但现在也可以思考一下这个信号的集合，我们总共一起有那些信号？比如刚刚那个kill-9，sigint,sigterm,sigio这些信号总共一起信号大概是有这么多有这么多，我找一下还有这么多，31种信号，那这31种信号我们怎么去存？

第二个这是一个进程，然后我们在应用程序的时候，我们调用signal的这个函数的时候，如何把这个信号保存到进程里面，还有一个就是我们如何通过其他的进程，比如说kill-9或者操作系统sigio如何触发到这个里面，

第一个就是说进程集合怎么存的，这一块怎么存的。
第二个是说信号signal的函数，如何把这信号保存到结构里面。
第三个就是kill-9，以及这个信号怎么发送到进程，进程怎么捕获这东西的。

首先来看一下这个进程里面保存它，
首先第一个想到的是task_struct

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162357_16960.jpg)

就是存到这个地方就这个 sighand_struct

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162358_74744.jpg)

像这个sighand_struct从这里面大家看到这里有一个k_sigaction
大家看到这里有个k_signaction，我们点进去之后，
可以看到里面对应的是有一个signaction，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162359_27991.jpg)

这跟用户空间的是一样的，这里面action这里面它就以这个集合就是对应的这个 action的集合，

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162400_30927.jpg)

在这里插入图片描述

这有个_NSIGl对应的就是这个数组,这个数组呢大小是64，也就是在每一个进程里面在每一个进程里面有一个空间sigaction的一个结构体，一个对应的数组叫action，这里面这个数组大小有64，它用来去存储所有的信号集合，

这个数字64存储信号怎么存的呢？

比如说有的时候如果一下触发大于64的信号怎么办？
比如说我一下触发128个，他是不是就溢出来了呢？
关于这signal他是怎么存的，信号只有31个，

再谈一个函数，
可以看到这是一个系统调用，这是内核里面内核里面这个系统调用SYSCALL_DEFINE2

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162401_35679.jpg)

这个代码里面我们调用的这个signal，它是一个系统调用，它是一个系统调用,

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162402_47433.jpg)

它调用的这个函数是哪个呢？

系统调用他调到内核里面的哪个函数，就是当我们在调用signal的函数的时候，他走到了内核里面这个地方

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162402_67834.jpg)

从这里我们往下走就可以了，走到这里之后大家可以看到这个sighand的设置,

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162403_90379.png)

在这里插入图片描述

这一行什么意思的，大家看看这个p就是进程,进程对应的这个就是我们sighand，然后对应的action然后sig-1，可以记住也就是刚刚这个数组64，

就是这个信号每一个信号只有一个，也就是说大家在对应这个信号的值，也就是如果我们一个sigio信号，他就被存储到这个数组里面的第sigio位，

sigio的值是多少呢？
sigio是29，也就这个信号，
它存储到28从零开始存储到28的位置，也是把之前的就进行覆盖，没错，把前面的就进行覆盖，没有触发的最后一次设置的把它覆盖，

这就是我们在调用sin的这个函数，
基本这个函数就是对应的把这个函数把我们对应的x设置那个数组里面这个内容。

kill这个函数在调用的时候带两个参数，第一个对应是PID就是进程ID。
第二个对应就是发送了什么信号

给哪个进程发什么信号，PID就是进程的ID，就这个 Id大于0的话，拿到对应的进程，然后往下再send_signal，就生产在发送的时候1层1层剥到这方面了。
直接给大家看最后一个东西怎么就是所谓的激活

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162404_30493.jpg)

在这里插入图片描述

就是对应的大家可以看到这里有个signalfd_notify，大家可能看到这个中断其实对应来说就是一个信号等待的意思，

epoll,poll
他为什么要这么做？

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162405_87816.png)

在这里插入图片描述

就是不同的信号通过不同的位置那个安全的那个下标的位置一个信号，它是个整形的数字对应的数组的不同的位置。

就还是以服务器为例子，还是以一个酒店为例子？
就是酒店没调好，酒店刚刚跟大家讲了一个饭，一个饭馆一个一个很大的酒店吃饭没错没错，这是一个迎宾的小姐姐来了一个客人之后
来了一个客人，然后这个客人给他介绍了一个新的服务员，好请来注意就是每一个客户端对应一个服务员，来一桌客户就有一个服务员，
随着这个酒店规模越来越大，所以这个酒店一天可能是几十桌几百桌都有，好，那也就是对于这些服务员，**对这些服务员我怎么进行管理**，那各位朋友们可能在想一个问题，这还有什么管理嘛？有事情就向上面反馈，没错，就是有什么事情对外
反馈的这个过程
比如说还要点个菜，你去下餐，你是要下到后面厨房里面，那也就说一个客户端他点菜的时候还有他下到厨房里面去，在这个厨房在处理的时候，他知道是哪个服务员吗？
**io多路复用就这意思，为了管理多个服务员来处理的。**

这里每一个 fd有这么几种方式可以处理，第一种就是派一个人随着这个酒店可能有一两百个服务员的话，专门派一个人，这个酒店服务员根本就不动，站在他们面前，专门找一个人挨一个问，你好了没有你好了没有？你有没有要点菜的？然后再派个人往后面走，然后拿到对应的把他由厨房来派？

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162406_12518.jpg)

你看这种好太麻烦，
你派一个人专门去做，而且你们这个中间还会出现一些现象，比如说我们为了管理，管理这个酒店所有的服务员，一桌一个服务员，酒店为了管理这个服务员，首先想到第一个方法，一个非常非常老套的方法啊就是，找一个人专门去问专门去咨询，那咨询哪些内容，只咨询三个状态，只咨询三个功能，
第一个状态要不要点菜？
第二个要不要点酒？
第三个就是要不要结账？
只问这三个状态，
就是每一桌一个服务员去服务一桌的时候，就是这3个轮训，感觉相当于正跑堂的，就相当于大堂经理就是这边问他那要不要？
注意这三个状态，**结合了刚刚这个人做的事情，找一个人专门去问这些服务员，那这个专门这个人是什么？他就是select。**

那大家想一下这个select它有几个参数？
第一个参数就是酒店有多少服务员，酒店的服务员的数量有多少，他要问多少次，问的次数
第二个参数，哪一些桌
可能刚开始带着一种目的看到了一个菜吃的差不多了，然后可能就说要要不要结账，可能看到有些菜刚刚点完，就它主动去问要不要加酒，刚坐下来可能就主动去问他有没有点菜，
这里面有三个集合主动的问一些桌里面需要点餐，首先进行保证这是查询的态度。
第三个就是点酒的集合，第四个结账的集合，第五个参数多长时间一次

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162406_62757.jpg)

在这里插入图片描述

就是这个专门去问的这个人就是相当于大堂的一个经理，
每一桌都跑过去，然后得出这么一个结果
这是他的工作是这样的，整个一起
有个前提，就是你可以想象到一个酒店里面，比如说一个酒店里面有几百桌好几百座，然后每一个桌前面站在一个服务员，
然后这个大堂的经理也好，还是助理也好，还是秘书也好，他就专门为了去传达信号去问站在桌子旁边的那个服务员，
你问一下这个你问一下这桌有没有，你问一下这个要不要不点菜要不点酒要不结账，这么一个工作

然后这5个参数就是这个点菜的集合，我们现在不知道他要不点菜，select返回的时候，他会带着哪些桌要点菜，刚开始是我们主观的认为他点菜，但是后面可能有一些要点菜，有些桌不要点菜，好返回的时候才是真正的结果

结合我们的socket理解，它也是这样，也是5个参数，
第一个我们有多少个io？第二个就是可读的集合，我们认为他需要的读，第三个就是可写的集合，第四个是出错的集合，第五个就是多长时间轮询一次。

这个应该叫中间有一个点要跟大家解释的，就是这个服务员的数量以及这个 io的数量怎么判断，这是很难理解，就是io的数量

这里是巧合的地方，
最大fd+1那为什么会采用最大的fd+1？两个原因这种方式，第一个因为我们的fd是int型的，
int型的就出现这个情况，其实这个 fd+1或者不加都可以，你就是不加1，加1是为了我们把它放大一点，那个就采用一种方式是采用这个 max fd其实ok，

比如说这个酒店有200个服务员
正在工作，有200个服务员，今天正在工作的有150位，好，正在工作的有100个人，
对于这个助理而言，我们压根就不用去管另外50位多少请假了，

我们在这地方查询的时候，我们就直接以200为计算，因为这里面还有一部分就是那些桌点菜了，我们不知道，那其实在这块我们做的一件事情就是最大的fd进行加一，最大的fd当做数量就可以，那我们就可以等同为
这个 max fd的这个值，它最大的值就相当于这个 fd总共一起有多少个数量

这个酒店工作了一段时间之后呢，发现这个专门做事的人要做一个事情，就是需要带三个小本本，第一个点菜的集合，第二点点酒的集合，第三个结账的集合，

这3个小本本记录有点麻烦，我们就进行1个小本本，就把这三个分别记在一个小本上面，大家发现也是跑也是跑，**但只是记录在一个小本本上面，这就是poll带三个参数**
一个是pfd,第二个参数是它有多大length,第三个参数是说的timeout，

可以想象一下一个酒店里面几百多的人，然后每一桌旁边带一个小姐姐，就是服务员为他倒酒倒点菜，然后在大堂里面就专门安排一个人挨桌问，他每次开始工作的这个工作的人他就是select，
他一开始工作就带三个小本本，然后里面哪些需要点酒，哪些需要点菜，然后问一次
然后在里面那些画勾，然后再带回来，隔一段时间在继续，他就取决于timeout多长时间跑，
**select翻译到io里面就是哪一些可读，哪一些可写，哪一些出错了，然后多长时间培训一次，每一次需要把这个集合进行更新，每一次把数据带进去，**

select五个参数,还有就是poll这东西每次记三个小本本有点麻烦，然后后面改成一个把这三个集合记到一个本本里面，记在一个本里面，那对应就变成了poll模式。
同样poll他还是需要定时的去跑一次，多长时间跑一次，多长时间跑一次，但是只是由三个本变成一个本，由三个数组变成一个数，
其他的没有改变，其他的没有改变，他还是多长时间轮训一次，多长时间去整个大堂里跑圈 ，他不是一直在轮训，它是根据时间定时的，

这个定时谁定的这个时间，我想问一下这个秘书这个专门跑这个人，**这个时间期间他在干什么？**

他是在哪个地方休息，在这个中间时间空档的时候这个专门跑堂这个人他在哪个地方休息，他休息在哪里？

请大家注意这select的timeout它的时间是在哪里休息，请大家注意它阻塞在哪里，也就是说这个时间比如说我们3秒钟,也就说这io三秒钟之内，他阻塞是阻塞在select函数的内部，三秒钟没有我就返回，如果有的话就低于三秒，最长等待三秒，没有的话我就最长等待三秒，有的话我就提前返回没有，然后poll也是这样

剩下一个主角，一个非常重要的东西，就是epoll，epoll是怎么样的，epoll就比较高级

每一桌上面放一个按钮，现在有些酒店点菜，你按那个按钮就可以，就变成这种模式，服务员还在那里，但是每个服务员手里捧着一个小按钮要点菜了然后按一下，要点酒了按一下，要结账了按一下。这个人只有遇到按钮的时候，他就不需要每一桌去问，而是直接按按钮的那一桌，我们才开始把这信息收回来，就这样。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162407_47171.jpg)

这epoll是这样的，比如一个小区有n多的住户，现在在考虑这个快递怎么记的问题，怎么记录，就在楼下做一个东西就是蜂巢。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213162408_23597.jpg)

那这个快递员他去工作的时候什么样？
送快递的时候就把那个快递放在蜂巢里面，收的时候也是要从这个里面拿，这样呢他就跟那个住户进行了一个隔离，
隔离那大家可以节省了很多时间，以前快递员去跑整个小区的情况，都不需要挨家去敲，直接从这个蜂巢里面拿取，

这里有前提什么？
中间可能有一部分住户，不允许快递员拿？
首先第一个快递员，我们通过epoll_create()创建一个快递员，
第二个比如说这个住户里面新搬了一个，我们通过epoll_ctl(ADD/DEL/MOD)增加一个或者说搬走一个，或者说有一个人从5楼搬到4楼，从4楼搬到3楼房进行mod修改。
第三个函数是epoll_wait()多长时间轮询一次，epoll总归一起由这三个函数所组成。

epoll_create()你就可以理解，聘请一个快递员，然后为小区收快递这个集体里面增加一个减一个，然后修改一个，然后epoll_wait()是多长时间跑一次，

epoll_wait()总归一起带4个参数，一个是epfd是哪个快递员，第二个就是他收快递的那个袋子events，第三个是他那个袋子有多大length，第四个是多长时间跑一次timeout。

epoll_create(1)这里面有个参数，它带这个参数只有0和1的区别，不管你写多大，不管你填多大，你就是填1万与填10万，它的效果是一样的,那么这个参数有什么用？

这个参数只有0和1的区别，就是为什么带这个参数？就是epoll最早的版本
一次性最多有多少，就是后面这个快递员收快递的时候，

它最大一次性能够收多少？最多最多，这是一个预估的值，最多最多最初是这样一个值，随着后面这个一点一点的修改，其实在前期实现的时候，用的是一些数组或者用的是一个固定的空间或者是用一个list分配的，

创建的快递员在蜂巢里面最多能收多少个，就类似，但是随着后面的版本的修改，相当于这个蜂巢的这个盒子就变成了无线大，就变成了所谓我们的一个链表来处理，
所以后面这个值就没有太大的意义，所以它只有0合1的区别，0就是失败，1就成功，大于0的数都成功就可以了，其实这个参数它没有什么意义，现在来说已经没什么意义，只要大于0就 ok。

第二个就是epoll_ctl()这个函数，它是为小区里面，我们添加io，删除io，修改io，就这个意思。

## 4.总结

第一个跟TCP编程的方式。
第二个udp并发udt并发
第三个就是关于socket是怎么回事，这个插座怎么理解，然后解释sigio就是从网络最古老的版本这个sigio接受数据，然后解释了这个网络sigio的是从哪里接收的数据，这个信号从哪里来的，然后解释了信号的整个数据流程，
然后讲select,epoll.

原文链接：https://zhuanlan.zhihu.com/p/491520467

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.446】后端开发【一大波干货知识】tcp/ip定时器与滑动窗口详解

## 1.为什么udp有包长，而tcp没有包长。

首先，send()发送一次发送1k，发送一次缓冲区满了就会返回-1。2k发送出去后缓冲区被清空，send()才会被再次调用。最大传输片会打印四个包发送。而最大传输单元是在数据链路层对网卡的一些限制，如果mss大于mtu时候就会被分割，而mss小于mtu时候就会直接发送。所以udp需要包长，而tcp不需要因为mss+tcp包头就可以完成不需要包长。具体点说，就是帧需要，加上512，可以将包长进行计算出，这里不做重复的事减轻了很多后续工作。

- 发送1m的文件
- sendbuf 2k
- mss（最大传输片）=512，mss是在包头的option中设置的。
- mtu（最大传输单元）=1500

## 2.协议头分析

- ACK表示确认
- PSH表示应用程序发来数据赶紧处理呀宝贝
- RST表示告诉对端你的数据不合法，重置
- SYN表示同步头
- FIN表示古德拜啦~断开前兆

## 3.慢启动的问题

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163738_92368.jpg)

对数增长慢启动后，线性增拥塞控制，最后是要快速重传。

进入弱网环境，rtt（数据包往返）出现很长时间，这就叫抖动。

rtt=0.1rtt(new)+0.9rtt(old)这是一个消抖的过程，通信专业知识，有时间应该去学学一下研究通信啦！得出的值就可以判断是否超时。当然了，拥塞避免增长速度肯定是没有慢启动指数增长的快，但是面积大，面积就是传输的数量，所以拥塞避免是为了保证传输的量更加大。

## 4.如果接收端缓冲区buffer满了，但是没有丢失数据，还会发吗？会

就不会发了呀宝贝。回应上写的windows为0。服务端的recvbuf和回应客户端返回的window完全是两个不同的概念，但是两个值会一直接近。

## 5.如果服务器端recvbuf从无到有，如何告诉客户端我有空间了呢？

服务器主动告诉客户端，我的recvbuf不为0

客户端轮询

第一种服务器主动告诉客户端，优点是实时性比较好，但是缺点是发生丢包怎么办？将会陷入死锁的状态。如果做一个定时器会不会好一点？可是客户端关机了怎么办？

**TCP的做法是什么呢？

当服务器recvbuf为0的时候，客户端主动轮训发送探测包，服务器的有空间吗？服务器被动的回。

**

如果服务器主动发送回复客户端，客户端不回服务器多次发送可以不？做法是可行的，但是违背了原则，毕竟服务器也不愿意当舔狗，主动向全世界客户端宣布爱你。

## 6.滑动窗口的运行机制

两个指针表示收尾，第三根指针作为成功接收否的标记指针，尾指针后面的接收的指针不处理。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163738_30203.jpg)

TCPkeepalive和应用层keepalive，应用层的keepalive可控性更强，而TCPkeepalive很笨，有点像小刘同学嘎嘎幸亏她不知道要不然肯定会打我，应用层你感知不到好难受。

## 7.问答环节

Q1：tcp利用send函数发送数据发现数据丢失了，需要在应用层对丢包重传处理吗？

答:利用tcp传输数据，发送数据是一定会发送到对端网卡。如果你发生丢包，那要注意send()返回值是为-1，应用层处理业务逻辑有问题。往往错误都是一些低端的错误。

Q2:滑动窗口的尾指针后面组织准备接受数据的状态，这是哪里在组织，操作系统吗？

答：是tcp协议组织的。

Q3：如图，当客户端有多个数据包同时发送给服务器，发送中途状态服务器的recvbuf满了，服务器需要立刻通知客户端吗？还是要等一个阶段的数据发送的差不多了再告诉客户端有一部分数据未接收成功？目前看来，这个状况是服务器性能问题，所以不会进入快速重传状态对吧？

答：要发送多少，是接收端返回数据。接收端接收数据数据以后，会回ack，ack里面就会确定，还能接受大小。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163739_22687.jpg)

原文链接：https://zhuanlan.zhihu.com/p/494470170

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.447】网络IO管理-简单一问一答、多线程方式

## 1.思考：

1. 那网络中进程之间如何通信，浏览器的进程怎么与web服务器通信的？

2. 什么时候用一请求一线程的方式？

3. 什么时候用select/poll？

4. 什么时候用epoll?

## 2.准备工作

下面展示socket几个常用的函数`listenfd, bind, listen, accept具体作用`。

```
// 聘请迎宾的小姐姐
if ((listenfd = socket(AF_INET, SOCK_STREAM, 0)) == -1) {
    printf("create socket error: %s(errno: %d)\n", strerror(errno), errno);
    return 0;
}
// 迎宾的小姐姐在哪个门口工作
if (bind(listenfd, (struct sockaddr *)&servaddr, sizeof(servaddr)) == -1) {
        printf("bind socket error: %s(errno: %d)\n", strerror(errno), errno);
        return 0;
}
// 小姐姐正式走到门口开始迎宾
if (listen(listenfd, 10) == -1) {
       printf("listen socket error: %s(errno: %d)\n", strerror(errno), errno);
       return 0;
}
// 把客户带到大厅介绍一个服务员,coonfd就是服务员
if ((connfd = accept(listenfd, (struct sockaddr *)&client, &len)) == -1) {
    printf("accept socket error: %s(errno: %d)\n", strerror(errno), errno);
    return 0;
}
```

**网络连接的简单过程**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163030_58013.jpg)

## 3.网络IO模型

### 3.1.阻塞IO

#### 3.1.1. 简单一问一答服务器/客户机模型

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163030_75124.jpg)

当用户进程调用了 read 这个系统调用，kernel 就开始了 IO 的第一个阶段：准备数据。对于network io 来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的数据包），这个时候 kernel 就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞。当 kernel一直等到数据准备好了它就会将数据从 kernel 中拷贝到用户内存，然后 kernel 返回结果，用户进程才解除 block 的状态，重新运行起来。所以，blocking IO 的特点就是在 IO 执行的两个阶段（等待数据和拷贝数据两个阶段）都被block 了。

几乎所有的程序员第一次接触到的网络编程都是从 listen()、send()、recv() 等接口开始的，这些接口都是阻塞型的。使用这些接口可以很方便的构建服务器/客户机的模型。

**代码展示**

```
#include <errno.h>
#include <netinet/in.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/types.h>
#include <unistd.h>
#define MAXLNE  4096
int main(int argc, char **argv) 
{
    int listenfd, connfd, n;
    struct sockaddr_in servaddr;
    char buff[MAXLNE];
    if ((listenfd = socket(AF_INET, SOCK_STREAM, 0)) == -1) {
        printf("create socket error: %s(errno: %d)\n", strerror(errno), errno);
        return 0;
    }
    memset(&servaddr, 0, sizeof(servaddr));
    servaddr.sin_family = AF_INET;
    servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
    servaddr.sin_port = htons(9999);
    if (bind(listenfd, (struct sockaddr *)&servaddr, sizeof(servaddr)) == -1) {
        printf("bind socket error: %s(errno: %d)\n", strerror(errno), errno);
        return 0;
    }
    if (listen(listenfd, 10) == -1) {
        printf("listen socket error: %s(errno: %d)\n", strerror(errno), errno);
        return 0;
    }
    struct sockaddr_in client;
    socklen_t len = sizeof(client);
    if ((connfd = accept(listenfd, (struct sockaddr *)&client, &len)) == -1) {
        printf("accept socket error: %s(errno: %d)\n", strerror(errno), errno);
        return 0;
    }
    printf("========waiting for client's request========\n");
    while (1) {
        n = recv(connfd, buff, MAXLNE, 0);
        if (n > 0) {
            buff[n] = '\0';
            printf("recv msg from client: %s\n", buff);
        send(connfd, buff, n, 0);
        } else if (n == 0) {
            close(connfd);
        }
        //close(connfd);
    }
    close(listenfd);
    return 0;
}
```

#### 3.2.2. 多线程方式

**提出问题**

实际上，除非特别指定，几乎所有的 IO 接口 ( 包括 socket 接口 ) 都是阻塞型的。这给网络编程带来了一个很大的问题，如在调用 send()的同时，线程将被阻塞，在此期间，线程将无法执行任何运算或响应任何的网络请求。

**解决方案**

改进方案是在服务器端使用多线程（或多进程）。多线程（或多进程）的目的是让每个连接都拥有独立的线程（或进程），这样任何一个连接的阻塞都不会影响其他的连接。具体使用多进程还是多线程并没有一个特定的模式。

**适用场景与注意的问题**

户端不多的情况下，在一个局域网内，做几个人的会议这类的，理论上客户端很难突破C10k，使用 pthread_create ()创建新线程。

多线程的服务器模型似乎完美的解决了为多个客户机提供问答服务的要求，但其实并不尽然。如果要同时响应成百上千路的连接请求，则无论多线程还是多进程都会严重占据系统资源，降低系统对外界响应效率，而线程与进程本身也更容易进入假死状态。

这时候很多人会考虑使用“线程池”或“连接池”。这里就需要深刻理解一下其概念，“线程池”旨在减少创建和销毁线程的频率，其维持一定合理数量的线程，并让空闲的线程重新承担新的执行任务。“连接池”维持连接的缓存池，尽量重用已有的连接、减少创建和关闭连接的频率。所谓“池”始终有其上限，当请求大大超过上限时，“池”构成的系统对外界的响应并不比没有池的时候效果好多少。所以使用“池”必须考虑其面临的响应规模，并根据响应规模调整“池”的大小。

**代码展示**

```
#include <errno.h>
#include <netinet/in.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/types.h>
#include <unistd.h>
#include <pthread.h>
#define MAXLNE  4096
//8m * 4G = 128 , 512
//C10k
void *client_routine(void *arg) { //
    int connfd = *(int *)arg;
    char buff[MAXLNE];
    while (1) {
        int n = recv(connfd, buff, MAXLNE, 0);
        if (n > 0) {
            buff[n] = '\0';
            printf("recv msg from client: %s\n", buff);
            send(connfd, buff, n, 0);
        } else if (n == 0) {
            close(connfd);
            break;
        }
    }
    return NULL;
}
int main(int argc, char **argv) 
{
    int listenfd, connfd, n;
    struct sockaddr_in servaddr;
    char buff[MAXLNE];
    if ((listenfd = socket(AF_INET, SOCK_STREAM, 0)) == -1) {
        printf("create socket error: %s(errno: %d)\n", strerror(errno), errno);
        return 0;
    }
    memset(&servaddr, 0, sizeof(servaddr));
    servaddr.sin_family = AF_INET;
    servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
    servaddr.sin_port = htons(9999);
    if (bind(listenfd, (struct sockaddr *)&servaddr, sizeof(servaddr)) == -1) {
        printf("bind socket error: %s(errno: %d)\n", strerror(errno), errno);
        return 0;
    }
    if (listen(listenfd, 10) == -1) {
        printf("listen socket error: %s(errno: %d)\n", strerror(errno), errno);
        return 0;
    }
    printf("========waiting for client's request========\n");
    while (1) {
        struct sockaddr_in client;
        socklen_t len = sizeof(client);
        if ((connfd = accept(listenfd, (struct sockaddr *)&client, &len)) == -1) {
            printf("accept socket error: %s(errno: %d)\n", strerror(errno), errno);
            return 0;
        }
        pthread_t threadid;
        pthread_create(&threadid, NULL, client_routine, (void*)&connfd);
    }
    close(listenfd);
    return 0;
}
```

**理解多线程方式不足**

假设有A, B, C三个客户端连接服务器，其中A, B发送数据，C连接进来。A, B服务员正在服务，C迎宾的小姐姐迎接。这时候服务器没办法准确的预测哪个fd会有数据来临。大量的客户端链接进来，需处理多个客户端，我们不知道哪个客户端需要我们处理。这时，如果有一个组件把所有的fd放在一起，准确的知道哪个客户端需要我们处理。

原文链接：https://zhuanlan.zhihu.com/p/492069172

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.448】后端开发【一大波干货知识】定时器方案红黑树，时间轮，最小堆

### 1.目录：

- 一、如何组织定时任务？
- 定时器收网络IO处理造成误差特别大，该怎么处理？
- 用何种数据机构存储定时器？
- 红黑树如何解决相同时间的key值的？
- 最小堆
- 时间轮
- 一个帮助理解单层级时间轮的例子
- 如何解决空推进的问题？
- 为什么多线程使用时间轮
- 设计哪些接口，如何设计？
- 满足哪些条件才能作为定时器的数据结构？
- 二、定时的方法有哪些？
- 究竟什么是定时？
- 三、总结

### 2.如何组织定时任务？

首先，定时器组件通常和网络组件一起工作。举个最简单的例子来说：

```
int event=epoll_wait(epfd,ev,nev,timeout);
```

timeout作为参数值，<0为一直阻塞，=0相当于非阻塞检测双端队列就绪情况0代表立刻返回,>0数值表示最长阻塞的时间。在时间上需要应对的问题填入epoll_wait函数，就可以兼顾网络事件的处理和定时任务的处理。

### 3.定时器收网络IO处理造成误差特别大，该怎么处理？

- **nginx,redis中通过定时信号去处理+epoll_wait解决**，nginx_timer_revolusion()函数定时发送时间信号会打断epoll_wait()的处理，让它尽快的处理定时事件，从而解决了误差较大的问题。很明显，如果网络IO处理事件时，会造成一定的误差。定时任务事件处理=epoll_wait处理时间+网络事件处理时间。

```
//第一种:通过发送定时信号去打断epoll_wait函数
while(!quit）
{
  int now=get_now_time();//单位:ms
  int timeout=get_nearest_time()-now;
  if(timeout<0)
      timeout=0;
  int nevent=epoll_wait(epfd,ev,nev,timeout);
  for(int i=0;i<nevent;i++)
  {
      //网络事件处理
  }
  update_timer();、//时间事件处理
}
```

单独开启一个线程，去处理定时任务。

```
//第二种：再其他线程添加定时任务
void* thread_timer(void * thread_param)
{
  init_timer();
  while(!quit)
  {
      update_timer();
      sleep(t);
  }
  clear_timer();
  return nullptr;
}
```

### 4.用何种数据机构存储定时器？

考虑这个问题要清楚，**越近要触发的事件优先级越高**。
定时器存在的意义是处理延时任务，具体来说比如：**定期检测客户连接状态，心跳检测，技能冷却CD，倒计时，定时广播，界面实时刷新**等等。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213164055_43230.jpg)

### 5.红黑树如何解决相同时间的key值的？

大家知道，stl::map和stl::set是内部都是采用红黑树实现的，并没有要求key值一定要不同、

```
int find_nares_expire_timer(){
        ngx_rbtree_node_t *node;
        if(timer.root==&sentinel){
            return -1;
    }
    node=ngx_rbtree_min(timer.root,timer.sentinel);
    int diff=(int)node->key-(int)current_time();
    return diff>0 ? diff :0;
}
```

### 6.最小堆

最小堆是堆排序中一个子的流程，最小堆是一个完全二叉树（其他叶子节点都是满的，而最深的节点叶子都是靠在最左侧），用数组组织其中的元素。与用链表表示堆相比，数组表示堆不仅节省空间，而且更容易实现堆的插入、删除等操作。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213164056_21381.jpg)

- 某个节点:x
- 左子树索引:2x+1
- 右子树索引:2x+2
- 父节点索引:floor((x-1)/2)

**需要注意的是：最小堆只关注父子的大小，不关注兄弟的大小。**
增加节点只有上升操作，删除节点有上升和下降，个人理解不管是删除还是增加，都是要先对树进行操作，而后维护树的正常秩序。
红黑树和最小堆的增删都是O(logn),在查找最小的就节点方面红黑树是O(logh)，最小堆是O(log1)，很明显最小堆比红黑树更加我稳定一些。

### 7.时间轮

crontab是linux的定时服务，它就是用时间轮实现的。与红黑树和最小堆不同的是，时间轮是多线程环境下使用的。
tcp的滑动窗口就是时间窗口的概念，这就是是单层级时间轮的实现。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213164057_72003.jpg)

- 限流:动态的，5秒内有100次操作，换句话说一秒一秒的移动，限制在100次的操作。单层级时间轮就是实现的限流这个操作。更精确，但是
- 熔断:静态的，5秒测一次，5秒测一次的感觉。

### 8.一个帮助理解单层级时间轮的例子

在客户端给服务器发送心跳的这个过程中，老师举例客户端可能五秒发一次服务器十秒检测一次，如果没有收到心跳就会踢除连接。检测是否有过期连接的方法有两种：一种是每一个连接启动一个定时器，另一种是将所有的连接存放在map<int,connect *>中，用一个定时器去检测上万个连接，因为有很多是新上来的连接，每秒去检测肯定是有很多次浪费的检测。
要设计单层级时间轮要从两个方面考虑:一个是检测间隔时间的大小，另一个是时间轮的精度。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213164058_33031.jpg)

公式:(5+10)/8=7

- 5表示当前时间。
- 10表示检测周期。
- 8表示连接数。

计算机内部取余操作m%n=m-n*floor(m/n)，x%16=x &(16-1)。
时间轮根本就不怕任务多，任务越多越好。

### 9.如何解决空推进的问题？

- kafka利用的是最小堆+单层级时间轮
- 多层级时间轮

### 10.为什么多线程使用时间轮

多线程要加锁，条件反射第一反应是锁的粒度。增加删除节点的时间复杂度都是O(log1)，所以锁的粒度是最小的。上面也说到，红黑树和最小堆的时间复杂度都是O(logn),所以都使用时间轮。
设计哪些接口，如何设计？
笔者认为，分析定时器该如何设计，当有新设计需求时完全可以借用此模式，所以是有必要用心学的。

```
//初始化定时器
void init)timer();
//添加定时器
Node * add_timer(int expire,callback cb);
//删除定时器
bool del_timer(Node* node);
//找到最近发生的定时任务
Node* find_nearest_timer();
//清除定时器
void clear_timer();
```

### 11.满足哪些条件才能作为定时器的数据结构？

- 能够快速找到这个节点，增加和删除。
- 能够快速找到最小节点。

### 12.定时的方法有哪些？

#### 12.1.究竟什么是定时？

定时是指在一段时间后欻某段代码的机制，我们可以利用这段代码有条不紊的处理所有到期的定时器。定时机制是定时器得以比处理的原动力。Linux有三种定时方法:

- socket选项SO_RCVTIMEO和SO_SNDTIMEO。
- SIGALARM信号。
- I/O复用系统调用的超时参数。（上文提到的epoll_wait()）。

socket选项的SO_RCVTIMEO和SO_SNDTIMEO是用来设置socket几首数据和发送数据超时时间的。send,sendmsg,rcv,recvmsg,accept和connect等系统调用都可以设置这个选项，根据这些系统调用的返回值以及errno来判断超时时间是否一道，然后去处理超时任务。
SIGALARM信号是当alarm()和settimer()函数设置实时闹钟时被触发，然后我们利用这个信号的信号处理函数来处理定时任务。定时周期T反应了定时的精度，如果某个定时任务的超时时间不是T的整数倍，那么它实际被执行的时间和恶预期的时间将略有偏差。
Linux下的三组I/O复用系统调用都带有超时参数，因此不仅能统一处理信号和I/事件，也能同意处理定时事件。值得注意的是：I/O复用系统调用可能在超时时间到期内就返回(I/O事件发生)。如果我们要利用它们来定时，就需要不断的更新定时参数以反映剩余的时间。
更多的细节内容还要细致的研读游双先生的《Linux高性能服务器编程》。

### 13.总结

通过零声学院Mark老师精彩的讲述定时器，让小生在定时器这个方面有了新的认识和突破，给鄙人领入了新的世界，开启了新的大门。
想想曾经，小可只沉溺于Qt中的QTimer类，就像一个烂醉如泥的懒汉，终日沉迷于花天酒地，只知开(颠)开(鸾)心(倒)心(凤)的酣畅，却不知阳春白雪正在向自己告别。虽然能够完成简单的任务，但是却从来没有发现其中藏着如此多的秘密。
现在的我，更像是一个满身泥泞，刚刚从大海中登陆抢滩的战士。
在Mark老师吹响嘹亮的冲锋号声中，尽管前进的道路上充满荆棘坎坷，但是为了胜利和尊严，不惜一切代价，奋勇争先。
**生命不息，战斗不止**

原文链接：https://zhuanlan.zhihu.com/p/495173977

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.449】后端开发【一大波干货知识】—Redis，Memcached，Nginx网络组件

## 1.reator网络编程

epoll被称为事件管理器，利用管理器去管理多个连接。

```
int clientfd=accept(listenfd,addr,sz);
clientfd ==-1 && erro==EWOLDBLOCK //表示全连接中连接为空
int connect(int sockfd, const struct sockaddr *addr,socklen_t addrlen);
error == EINPROGRESS //正在建立连接
error == EISCONN  //连接建立成功    
```

- 关闭读端 read = 0
- 关闭写端 write = -1 && errno =EPIPE

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163416_21695.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163417_68931.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163418_80973.jpg)

- io函数像read只能检测一个fd对应的状态，可以检测具体的状态。
- io多路复用可以检测多个fd对应的状态，只能检测可读，可写，错误，断开笼统等信息。
- getsockopt 也可以检测错误。

### 2.阻塞IO 和 非阻塞IO

- 阻塞在网络线程
- 连接的fd阻塞属性决定了io函数是否阻塞
- 具体差异在：io函数在数据未到达时是否立刻返回。

```
//默认情况下，fd时阻塞的，设置非阻塞的方法如下：
int flag=fcntl(fd,F_GETFL,0);
fcntl(fd,F_SETFL,flag | O_NONBLOCK)；
```

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163418_42728.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163419_88279.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163420_39848.jpg)

- timeout == 0 是非阻塞效果，检测一下立即返回。
- timeout == -1 是永久阻塞
- timeout == 1000

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163420_99925.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163422_19459.jpg)

epoll_create 会去创建红黑树和就绪队列。

epoll_ctl 会去注册事件，会建立回调关系。当事件被触发，epoll_ctl 会将fd从红黑树中放到就绪队列。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163422_58852.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163423_87320.png)

问：代码第9行能不能监听写事件？

答：不能，因为刚开始的时候，写缓冲区是空的，会被一直触发可写。

### 3.编程细节，返回值以及错误码

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163424_77964.jpg)

读端关闭了。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163424_39067.jpg)

建议read()函数使用非阻塞io，因为出现错误会立刻返回，不会卡在这里影响别人。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163425_67893.jpg)

将数据写到缓冲区，协议栈会将数据发送到对端。

## 4.redis、nginx、memcached reactor具体使用

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163426_74357.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163427_45667.jpg)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163427_92045.jpg)

redis-6.0支持IO多线程，封装在networking.cz中。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/13/20221213163428_84292.jpg)

原文链接：https://zhuanlan.zhihu.com/p/493857761

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.450】手写实现分布式锁

前言

分布式锁需要考虑很多事情，第一网络是否正常，第二个提供分布式锁这台机器的高可用性。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215165657_60281.jpg)

## 1.网络模块封装

### 1.1.明线

#### 1.1.1io检测部分的接口：

- 添加读事件是因为当有客户端连接事件来临时，我们好去处理这个事件。
- 写事件这是因为当我们服务端作为客户端去连接第三方服务时候，需要注册写事件。
- 删除事件就比如关闭连接时，我们需要将事件从检测模块中删除。
- 修改事件就比如当客户端发来事件我们需要检测读事件，但是接收失败了我们要把读事件修改为注册写事件进行反馈。
- 检测事件肯定是必须的啦！io操作：
  绑定监听，接收连接，建立连接，关闭fd，读，写，fd属性。

### 1.2.暗线

需要注意的是：fd类型中clientfd写事件触发的情况，服务器收到客户端的数据可以直接调用write()函数，**只有当写入失败时才要注册写事件，检测何时可写。**应该将发送失败的数据缓存起来，等事件可写了，再将缓冲区的数据进行发送。
断开连接或者错误，都是交给读写操作。

## 2.协程调度

没有协程之前，当成功接收到一个数据后，要调用多个回调函数,(解析数据、查库拿数据、返回给客户端)等等。现在我们考虑的是，将三个序列利用协程的方式进行粘合，三个回调变成一个协程回调执行序列。
为每一个fd执行一个序列，每个协程是一个执行序列。
lua虚拟机不支持协程，并且没有类似于pthread_create()函数，也就是说主协程被自动创建。**lua虚拟机同时只能有一个协程在运行。**主协程负责调度其他协程。
主协程会进行事件检测，不断地从epoll中去取事件，根据事件去唤醒其他协程。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215165658_75077.jpg)

- lua和go语言中的协程方案是最完善的。
- epoll注册写事件触发，说明三次握手后同步包可以发送了，表示连接已经建立成功了。
- 异步的执行逻辑，同步的写法。
- 当连接建立成功后，connectfd和clientfd的流程变成一样。

## 3.异步连接池

### 3.1.为什么需要异步连接池？

现在所有的连接已经变成一个执行序列，连接由异步变为同步。此时，一个连接同时只能在一个协程中运行，是并发不是并行，也就是说只有一个执行序多个对列，依次执行。
pool_size是尺子最大连接数，backlog堆积的操作数。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215165658_16897.jpg)

## 4.缓冲池设计 用户读写缓冲区

Mark老师举例，4个协程在使用，4个协程在等待，超出的协程会报错。cache记录的是随时可用连接，free记录的是正在使用的连接。connect>8是要报错，小于pool_size要创建或是从cache里去取，cache没有说明都在free里。
给连接池起名字，默认是ip：端口格式，比如127.0.0.1:8888。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215165659_27152.jpg)

## 5.定时器设计

**定时器是在lua层实现了一个最小堆，每一个任务生成一个协程，但是要考虑回收协程，尤其是在删除的时候。**这块做的还不完善，Mark老师也希望大家一起帮忙搞一搞，先记下这件事吧！

## 6.总结

Mrk老师说道:做任何事情，拆分的思想很重要，联想起以前一个数学老师的话，“老太太吃柿子，要捡软的捏。”以后遇到问题和困难，也应该先按照这个思路去处理问题。作为开发还有一点是非常重的，就是**测试**的能力，这方面得想办法学习提升一下。通过本节课，我初步了解了分布式锁，感觉对于skynet也有了更好的认识。在成长的路上，有Mark老师陪伴，好幸福啊~

原文链接：https://zhuanlan.zhihu.com/p/495651179

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)



# 【NO.451】后端开发【一大波干货知识】定时器方案红黑树，时间轮，最小堆

### 1.目录：

- 一、如何组织定时任务？
- 定时器收网络IO处理造成误差特别大，该怎么处理？
- 用何种数据机构存储定时器？
- 红黑树如何解决相同时间的key值的？
- 最小堆
- 时间轮
- 一个帮助理解单层级时间轮的例子
- 如何解决空推进的问题？
- 为什么多线程使用时间轮
- 设计哪些接口，如何设计？
- 满足哪些条件才能作为定时器的数据结构？
- 二、定时的方法有哪些？
- 究竟什么是定时？
- 三、总结

### 2.如何组织定时任务？

首先，定时器组件通常和网络组件一起工作。举个最简单的例子来说：

```
int event=epoll_wait(epfd,ev,nev,timeout);
```

timeout作为参数值，<0为一直阻塞，=0相当于非阻塞检测双端队列就绪情况0代表立刻返回,>0数值表示最长阻塞的时间。在时间上需要应对的问题填入epoll_wait函数，就可以兼顾网络事件的处理和定时任务的处理。

### 3.定时器收网络IO处理造成误差特别大，该怎么处理？

- **nginx,redis中通过定时信号去处理+epoll_wait解决**，nginx_timer_revolusion()函数定时发送时间信号会打断epoll_wait()的处理，让它尽快的处理定时事件，从而解决了误差较大的问题。很明显，如果网络IO处理事件时，会造成一定的误差。定时任务事件处理=epoll_wait处理时间+网络事件处理时间。

```
//第一种:通过发送定时信号去打断epoll_wait函数
while(!quit）
{
  int now=get_now_time();//单位:ms
  int timeout=get_nearest_time()-now;
  if(timeout<0)
      timeout=0;
  int nevent=epoll_wait(epfd,ev,nev,timeout);
  for(int i=0;i<nevent;i++)
  {
      //网络事件处理
  }
  update_timer();、//时间事件处理
}
```

单独开启一个线程，去处理定时任务。

```
//第二种：再其他线程添加定时任务
void* thread_timer(void * thread_param)
{
  init_timer();
  while(!quit)
  {
      update_timer();
      sleep(t);
  }
  clear_timer();
  return nullptr;
}
```

### 4.用何种数据机构存储定时器？

考虑这个问题要清楚，**越近要触发的事件优先级越高**。
定时器存在的意义是处理延时任务，具体来说比如：**定期检测客户连接状态，心跳检测，技能冷却CD，倒计时，定时广播，界面实时刷新**等等。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215165314_38616.jpg)

### 5.红黑树如何解决相同时间的key值的？

大家知道，stl::map和stl::set是内部都是采用红黑树实现的，并没有要求key值一定要不同、

```
int find_nares_expire_timer(){
        ngx_rbtree_node_t *node;
        if(timer.root==&sentinel){
            return -1;
    }
    node=ngx_rbtree_min(timer.root,timer.sentinel);
    int diff=(int)node->key-(int)current_time();
    return diff>0 ? diff :0;
}
```

### 6.最小堆

最小堆是堆排序中一个子的流程，最小堆是一个完全二叉树（其他叶子节点都是满的，而最深的节点叶子都是靠在最左侧），用数组组织其中的元素。与用链表表示堆相比，数组表示堆不仅节省空间，而且更容易实现堆的插入、删除等操作。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215165315_76657.jpg)

- 某个节点:x
- 左子树索引:2x+1
- 右子树索引:2x+2
- 父节点索引:floor((x-1)/2)

**需要注意的是：最小堆只关注父子的大小，不关注兄弟的大小。**
增加节点只有上升操作，删除节点有上升和下降，个人理解不管是删除还是增加，都是要先对树进行操作，而后维护树的正常秩序。
红黑树和最小堆的增删都是O(logn),在查找最小的就节点方面红黑树是O(logh)，最小堆是O(log1)，很明显最小堆比红黑树更加我稳定一些。

### 7.时间轮

crontab是linux的定时服务，它就是用时间轮实现的。与红黑树和最小堆不同的是，时间轮是多线程环境下使用的。
tcp的滑动窗口就是时间窗口的概念，这就是是单层级时间轮的实现。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215165316_76587.jpg)

- 限流:动态的，5秒内有100次操作，换句话说一秒一秒的移动，限制在100次的操作。单层级时间轮就是实现的限流这个操作。更精确，但是
- 熔断:静态的，5秒测一次，5秒测一次的感觉。

### 8.一个帮助理解单层级时间轮的例子

在客户端给服务器发送心跳的这个过程中，老师举例客户端可能五秒发一次服务器十秒检测一次，如果没有收到心跳就会踢除连接。检测是否有过期连接的方法有两种：一种是每一个连接启动一个定时器，另一种是将所有的连接存放在map<int,connect *>中，用一个定时器去检测上万个连接，因为有很多是新上来的连接，每秒去检测肯定是有很多次浪费的检测。
要设计单层级时间轮要从两个方面考虑:一个是检测间隔时间的大小，另一个是时间轮的精度。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215165316_49767.jpg)

公式:(5+10)/8=7

- 5表示当前时间。
- 10表示检测周期。
- 8表示连接数。

计算机内部取余操作m%n=m-n*floor(m/n)，x%16=x &(16-1)。
时间轮根本就不怕任务多，任务越多越好。

### 9.如何解决空推进的问题？

- kafka利用的是最小堆+单层级时间轮
- 多层级时间轮

### 10.为什么多线程使用时间轮

多线程要加锁，条件反射第一反应是锁的粒度。增加删除节点的时间复杂度都是O(log1)，所以锁的粒度是最小的。上面也说到，红黑树和最小堆的时间复杂度都是O(logn),所以都使用时间轮。
设计哪些接口，如何设计？
笔者认为，分析定时器该如何设计，当有新设计需求时完全可以借用此模式，所以是有必要用心学的。

```
//初始化定时器
void init)timer();
//添加定时器
Node * add_timer(int expire,callback cb);
//删除定时器
bool del_timer(Node* node);
//找到最近发生的定时任务
Node* find_nearest_timer();
//清除定时器
void clear_timer();
```

### 11.满足哪些条件才能作为定时器的数据结构？

- 能够快速找到这个节点，增加和删除。
- 能够快速找到最小节点。

### 12.定时的方法有哪些？

#### 12.1究竟什么是定时？

定时是指在一段时间后欻某段代码的机制，我们可以利用这段代码有条不紊的处理所有到期的定时器。定时机制是定时器得以比处理的原动力。Linux有三种定时方法:

- socket选项SO_RCVTIMEO和SO_SNDTIMEO。
- SIGALARM信号。
- I/O复用系统调用的超时参数。（上文提到的epoll_wait()）。

socket选项的SO_RCVTIMEO和SO_SNDTIMEO是用来设置socket几首数据和发送数据超时时间的。send,sendmsg,rcv,recvmsg,accept和connect等系统调用都可以设置这个选项，根据这些系统调用的返回值以及errno来判断超时时间是否一道，然后去处理超时任务。
SIGALARM信号是当alarm()和settimer()函数设置实时闹钟时被触发，然后我们利用这个信号的信号处理函数来处理定时任务。定时周期T反应了定时的精度，如果某个定时任务的超时时间不是T的整数倍，那么它实际被执行的时间和恶预期的时间将略有偏差。
Linux下的三组I/O复用系统调用都带有超时参数，因此不仅能统一处理信号和I/事件，也能同意处理定时事件。值得注意的是：I/O复用系统调用可能在超时时间到期内就返回(I/O事件发生)。如果我们要利用它们来定时，就需要不断的更新定时参数以反映剩余的时间。
更多的细节内容还要细致的研读游双先生的《Linux高性能服务器编程》。

### 13.总结

通过零声学院Mark老师精彩的讲述定时器，让小生在定时器这个方面有了新的认识和突破，给鄙人领入了新的世界，开启了新的大门。
想想曾经，小可只沉溺于Qt中的QTimer类，就像一个烂醉如泥的懒汉，终日沉迷于花天酒地，只知开(颠)开(鸾)心(倒)心(凤)的酣畅，却不知阳春白雪正在向自己告别。虽然能够完成简单的任务，但是却从来没有发现其中藏着如此多的秘密。
现在的我，更像是一个满身泥泞，刚刚从大海中登陆抢滩的战士。
在Mark老师吹响嘹亮的冲锋号声中，尽管前进的道路上充满荆棘坎坷，但是为了胜利和尊严，不惜一切代价，奋勇争先。
**生命不息，战斗不止**

原文链接：https://zhuanlan.zhihu.com/p/495173977

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.452】Reactor实现http服务器，附完整代码

如何在reactor的基础上实现业务？就是怎么利用reactor做服务器，并实现服务器的业务。

本文基于reactor，实现简单的http协议封装。只是为了说明reactor如何做业务，真正的http服务器业务逻辑是很复杂的。

服务器网络这一层，如nginx、redis，核心是epoll，实现使用的是reactor。

## 1.http协议封装与reactor的关系

**reactor**包含内容

- reactor_run，event loop，通过epoll进行io检测；
- accept_cb，处理网络连接；
- recv_cb 接收客户端http请求
- send_cb 发送http响应

**应用层什么场景使用accept_cb?**

1. 对特定ip的访问进行限制；
2. 负载均衡的功能，就是将请求转发到哪台服务器进行处理。

如果http服务器没有特殊的需求，是不需要修改reactor中的accept_cb的。

## 2.如何在reactor基础上实现http

http使用的是reqeust-reply的模型进行通讯，客户端发送http请求，服务器处理请求并返回应答。http的传输层是基于TCP的。

http的tcp链接的生命周期：

1. accept_cb
2. recv_cb
3. send_cb

对于服务器，首先需要使用recv_cb接收http数据，通过拆包粘包得到一帧完整的http数据；之后对http数据进行解析处理；最后通过send_cb向客户端发送response。

## 3.实现简单的response

先实现一个最简单的response，对于所有http请求，直接回复一个response，内容是一段html。返回给客户端后，浏览器可以直接显示。只在send_cb中调用发送数据就可以了。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215170719_80342.jpg)

```
int http_response(struct ntyevent *ev) {
    if (ev == NULL) return -1;
    memset(ev->buffer, 0, BUFFER_LENGTH);
    const char *html = "<html><head><title>hello http</title></head><body><H1>Cong</H1></body></html>\r\n\r\n";
    ev->length = sprintf(ev->buffer, 
        "HTTP/1.1 200 OK\r\n\
         Date: Sun, 30 Jan 2022 05:55:32 GMT\r\n\
         Content-Type: text/html;charset=UTF-8\r\n\
         Content-Length: 81\r\n\r\n%s", 
         html);
    return ev->length;
}
int send_cb(int fd, int events, void *arg) {
    struct ntyreactor *reactor = (struct ntyreactor*)arg;
    struct ntyevent *ev = ntyreactor_idx(reactor, fd);
    http_response(ev);
    //
    int len = send(fd, ev->buffer, ev->length, 0);
    if (len > 0) {
        printf("send[fd=%d], [%d]%s\n", fd, len, ev->buffer);
        nty_event_del(reactor->epfd, ev);
        nty_event_set(ev, fd, recv_cb, reactor);
        nty_event_add(reactor->epfd, EPOLLIN, ev);
    } else {
        close(ev->fd);
        nty_event_del(reactor->epfd, ev);
        printf("send[fd=%d] error %s\n", fd, strerror(errno));
    }
    return len;
}
```

HTTP响应由四个部分组成，分别是：状态行、消息报头、空行和响应正文。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215170720_41247.jpg)

## 4.http get请求实现

实现http get请求获取静态资源。

客户端发送一个HTTP 请求到服务器的请求消息包括以下格式：请求行（ request line ）、请求头部（ header ）、空行和请求数据四个部分组成，下图给出了请求报文的一般格式。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215170720_79818.jpg)

## 5.读请求行

需要对http请求进行解析，判断请求类型(GET、POST等)，获取资源(uri)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215170721_85399.jpg)

```
int http_request(struct ntyevent *ev) {
    // GET, POST
    char linebuf[1024] = {0};
    int idx = readline(ev->buffer, 0, linebuf);
    if (strstr(linebuf, "GET")) {
        ev->method = HTTP_METHOD_GET;
        //uri
        int i = 0;
        while (linebuf[sizeof("GET ") + i] != ' ') i++;
        linebuf[sizeof("GET ")+i] = '\0';
        sprintf(ev->resource, "./%s/%s", HTTP_WEBSERVER_HTML_ROOT, linebuf+sizeof("GET "));
    } else if (strstr(linebuf, "POST")) {
    }
}
```

在读取http请求头的时候，需要注意，http是以\r\n对每一行进行分隔的，第一行就是请求行，里面有请求类型(GET、POST)、uri、协议版本等。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215170721_54382.jpg)

自定义协议，如何界定包的完整性：

1. 使用分隔符，比如http使用\r\n；
2. 协议头中加包的长度。

### 6.http response

根据上面的uri找到静态资源，并准备好response状态行和消息头。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215170722_66982.jpg)

发送不同类型的静态资源的时候，可以使用相应的Content-Type。

**发送response**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215170724_56050.jpg)

### 7.mmap

发送静态资源文件的时候，需要先将文件读入内存，再将内存中的数据send到相应的网络fd。通过使用sendfile完成文件的发送，不再需要两步操作。

**sendfile使用mmap**

零拷贝，使用的是mmap方式，本质是DMA的方式，不需要CPU参与。普通copy，从磁盘copy数据到内存，需要CPU的move指令。

在进程中有一块区域叫内存分配区，当调用mmap的时候，会把文件映射到对应的区域，操作文件就跟操作内存一样。

### 8.完整代码

```
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <arpa/inet.h>
#include <fcntl.h>
#include <unistd.h>
#include <errno.h>
#include <time.h>
#include <sys/stat.h>
#include <sys/sendfile.h>
#define BUFFER_LENGTH        4096
#define MAX_EPOLL_EVENTS    1024
#define SERVER_PORT            9105
#define PORT_COUNT            1
#define HTTP_WEBSERVER_HTML_ROOT    "html"
#define HTTP_METHOD_GET        0
#define HTTP_METHOD_POST    1
typedef int NCALLBACK(int ,int, void*);
struct ntyevent {
    int fd;
    int events;
    void *arg;
    int (*callback)(int fd, int events, void *arg);
    int status;
    char buffer[BUFFER_LENGTH];
    int length;
    long last_active;
    // http param
    int method; //
    char resource[BUFFER_LENGTH];
    int ret_code;
};
struct eventblock {
    struct eventblock *next;
    struct ntyevent *events;
};
struct ntyreactor {
    int epfd;
    int blkcnt;
    struct eventblock *evblk; //fd --> 100w
};
int recv_cb(int fd, int events, void *arg);
int send_cb(int fd, int events, void *arg);
struct ntyevent *ntyreactor_idx(struct ntyreactor *reactor, int sockfd);
void nty_event_set(struct ntyevent *ev, int fd, NCALLBACK callback, void *arg) {
    ev->fd = fd;
    ev->callback = callback;
    ev->events = 0;
    ev->arg = arg;
    ev->last_active = time(NULL);
    return ;
}
int nty_event_add(int epfd, int events, struct ntyevent *ev) {
    struct epoll_event ep_ev = {0, {0}};
    ep_ev.data.ptr = ev;
    ep_ev.events = ev->events = events;
    int op;
    if (ev->status == 1) {
        op = EPOLL_CTL_MOD;
    } else {
        op = EPOLL_CTL_ADD;
        ev->status = 1;
    }
    if (epoll_ctl(epfd, op, ev->fd, &ep_ev) < 0) {
        printf("event add failed [fd=%d], events[%d]\n", ev->fd, events);
        return -1;
    }
    return 0;
}
int nty_event_del(int epfd, struct ntyevent *ev) {
    struct epoll_event ep_ev = {0, {0}};
    if (ev->status != 1) {
        return -1;
    }
    ep_ev.data.ptr = ev;
    ev->status = 0;
    epoll_ctl(epfd, EPOLL_CTL_DEL, ev->fd, &ep_ev);
    return 0;
}
int readline(char *allbuf, int idx, char *linebuf) {
    int len = strlen(allbuf);
    for(;idx < len;idx ++) {
        if (allbuf[idx] == '\r' && allbuf[idx+1] == '\n') {
            return idx+2;
        } else {
            *(linebuf++) = allbuf[idx];
        }
    }
    return -1;
}
int http_request(struct ntyevent *ev) {
    // GET, POST
    char linebuf[1024] = {0};
    int idx = readline(ev->buffer, 0, linebuf);
    if (strstr(linebuf, "GET")) {
        ev->method = HTTP_METHOD_GET;
        //uri
        int i = 0;
        while (linebuf[sizeof("GET ") + i] != ' ') i++;
        linebuf[sizeof("GET ")+i] = '\0';
        sprintf(ev->resource, "./%s/%s", HTTP_WEBSERVER_HTML_ROOT, linebuf+sizeof("GET "));
    } else if (strstr(linebuf, "POST")) {
    }
}
int recv_cb(int fd, int events, void *arg) {
    struct ntyreactor *reactor = (struct ntyreactor*)arg;
    struct ntyevent *ev = ntyreactor_idx(reactor, fd);
    int len = recv(fd, ev->buffer, BUFFER_LENGTH, 0); // 
    if (len > 0) {
        ev->length = len;
        ev->buffer[len] = '\0';
        printf("C[%d]:%s\n", fd, ev->buffer); //http
        http_request(ev);
        //send();
        nty_event_del(reactor->epfd, ev);
        nty_event_set(ev, fd, send_cb, reactor);
        nty_event_add(reactor->epfd, EPOLLOUT, ev);
    } else if (len == 0) {
        nty_event_del(reactor->epfd, ev);
        close(ev->fd);
        //printf("[fd=%d] pos[%ld], closed\n", fd, ev-reactor->events);
    } else {
        nty_event_del(reactor->epfd, ev);
        close(ev->fd);
        printf("recv[fd=%d] error[%d]:%s\n", fd, errno, strerror(errno));
    }
    return len;
}
int http_response(struct ntyevent *ev) {
    if (ev == NULL) return -1;
    memset(ev->buffer, 0, BUFFER_LENGTH);
#if 0
    const char *html = "<html><head><title>hello http</title></head><body><H1>Cong</H1></body></html>\r\n\r\n";
    ev->length = sprintf(ev->buffer, 
        "HTTP/1.1 200 OK\r\n\
         Date: Thu, 11 Nov 2021 12:28:52 GMT\r\n\
         Content-Type: text/html;charset=ISO-8859-1\r\n\
         Content-Length: 83\r\n\r\n%s", 
         html);
#else
    printf("resource: %s\n", ev->resource);
    int filefd = open(ev->resource, O_RDONLY);
    if (filefd == -1) { // return 404
        ev->ret_code = 404;
        ev->length = sprintf(ev->buffer, 
            "HTTP/1.1 404 Not Found\r\n"
             "Date: Thu, 11 Nov 2021 12:28:52 GMT\r\n"
             "Content-Type: text/html;charset=ISO-8859-1\r\n"
            "Content-Length: 85\r\n\r\n"
             "<html><head><title>404 Not Found</title></head><body><H1>404</H1></body></html>\r\n\r\n" );
    } else {
        struct stat stat_buf;
        fstat(filefd, &stat_buf);
        close(filefd);
        if (S_ISDIR(stat_buf.st_mode)) {
            ev->ret_code = 404;
            ev->length = sprintf(ev->buffer, 
                "HTTP/1.1 404 Not Found\r\n"
                "Date: Sun, 30 Jan 2022 05:55:32 GMT\r\n"
                "Content-Type: text/html;charset=ISO-8859-1\r\n"
                "Content-Length: 85\r\n\r\n"
                "<html><head><title>404 Not Found</title></head><body><H1>404</H1></body></html>\r\n\r\n" );
        } else if (S_ISREG(stat_buf.st_mode)) {
            ev->ret_code = 200;
            ev->length = sprintf(ev->buffer, 
                "HTTP/1.1 200 OK\r\n"
                 "Date: Sun, 30 Jan 2022 05:55:32 GMT\r\n"
                 // "Content-Type: text/html;charset=ISO-8859-1\r\n"
                // "Content-Type: application/pdf\r\n"
                "Content-Type: image/jpeg\r\n"
                "Content-Length: %ld\r\n\r\n", 
                     stat_buf.st_size );
        }
    }
#endif
    return ev->length;
}
int send_cb(int fd, int events, void *arg) {
    struct ntyreactor *reactor = (struct ntyreactor*)arg;
    struct ntyevent *ev = ntyreactor_idx(reactor, fd);
    http_response(ev);
    //
    int len = send(fd, ev->buffer, ev->length, 0);
    if (len > 0) { // http header和body分两次发送
        printf("send[fd=%d], [%d]%s\n", fd, len, ev->buffer);
        if (ev->ret_code == 200) {
            int filefd = open(ev->resource, O_RDONLY);
            struct stat stat_buf;
            fstat(filefd, &stat_buf);
            // sendfile使用mmap
            sendfile(fd, filefd, NULL, stat_buf.st_size);
            close(filefd);
        }
        nty_event_del(reactor->epfd, ev);
        nty_event_set(ev, fd, recv_cb, reactor);
        nty_event_add(reactor->epfd, EPOLLIN, ev);
    } else {
        close(ev->fd);
        nty_event_del(reactor->epfd, ev);
        printf("send[fd=%d] error %s\n", fd, strerror(errno));
    }
    return len;
}
int accept_cb(int fd, int events, void *arg) {
    struct ntyreactor *reactor = (struct ntyreactor*)arg;
    if (reactor == NULL) return -1;
    struct sockaddr_in client_addr;
    socklen_t len = sizeof(client_addr);
    int clientfd;
    if ((clientfd = accept(fd, (struct sockaddr*)&client_addr, &len)) == -1) {
        if (errno != EAGAIN && errno != EINTR) {
        }
        printf("accept: %s\n", strerror(errno));
        return -1;
    }
    int flag = 0;
    if ((flag = fcntl(clientfd, F_SETFL, O_NONBLOCK)) < 0) {
        printf("%s: fcntl nonblocking failed, %d\n", __func__, MAX_EPOLL_EVENTS);
        return -1;
    }
    struct ntyevent *event = ntyreactor_idx(reactor, clientfd);
    nty_event_set(event, clientfd, recv_cb, reactor);
    nty_event_add(reactor->epfd, EPOLLIN, event);
    printf("new connect [%s:%d], pos[%d]\n", 
        inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), clientfd);
    return 0;
}
int init_sock(short port) {
    int fd = socket(AF_INET, SOCK_STREAM, 0);
    fcntl(fd, F_SETFL, O_NONBLOCK);
    struct sockaddr_in server_addr;
    memset(&server_addr, 0, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(port);
    bind(fd, (struct sockaddr*)&server_addr, sizeof(server_addr));
    if (listen(fd, 20) < 0) {
        printf("listen failed : %s\n", strerror(errno));
    }
    return fd;
}
int ntyreactor_alloc(struct ntyreactor *reactor) {
    if (reactor == NULL) return -1;
    if (reactor->evblk == NULL) return -1;
    struct eventblock *blk = reactor->evblk;
    while (blk->next != NULL) {
        blk = blk->next;
    }
    struct ntyevent *evs = (struct ntyevent*)malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    if (evs == NULL) {
        printf("ntyreactor_alloc ntyevents failed\n");
        return -2;
    }
    memset(evs, 0, (MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    struct eventblock *block = (struct eventblock *)malloc(sizeof(struct eventblock));
    if (block == NULL) {
        printf("ntyreactor_alloc eventblock failed\n");
        return -2;
    }
    memset(block, 0, sizeof(struct eventblock));
    block->events = evs;
    block->next = NULL;
    blk->next = block;
    reactor->blkcnt ++; //
    return 0;
}
struct ntyevent *ntyreactor_idx(struct ntyreactor *reactor, int sockfd) {
    int blkidx = sockfd / MAX_EPOLL_EVENTS;
    while (blkidx >= reactor->blkcnt) {
        ntyreactor_alloc(reactor);
    }
    int i = 0;
    struct eventblock *blk = reactor->evblk;
    while(i ++ < blkidx && blk != NULL) {
        blk = blk->next;
    }
    return &blk->events[sockfd % MAX_EPOLL_EVENTS];
}
int ntyreactor_init(struct ntyreactor *reactor) {
    if (reactor == NULL) return -1;
    memset(reactor, 0, sizeof(struct ntyreactor));
    reactor->epfd = epoll_create(1);
    if (reactor->epfd <= 0) {
        printf("create epfd in %s err %s\n", __func__, strerror(errno));
        return -2;
    }
    struct ntyevent *evs = (struct ntyevent*)malloc((MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    if (evs == NULL) {
        printf("ntyreactor_alloc ntyevents failed\n");
        return -2;
    }
    memset(evs, 0, (MAX_EPOLL_EVENTS) * sizeof(struct ntyevent));
    struct eventblock *block = (struct eventblock *)malloc(sizeof(struct eventblock));
    if (block == NULL) {
        printf("ntyreactor_alloc eventblock failed\n");
        return -2;
    }
    memset(block, 0, sizeof(struct eventblock));
    block->events = evs;
    block->next = NULL;
    reactor->evblk = block;
    reactor->blkcnt = 1;
    return 0;
}
int ntyreactor_destory(struct ntyreactor *reactor) {
    close(reactor->epfd);
    //free(reactor->events);
    struct eventblock *blk = reactor->evblk;
    struct eventblock *blk_next = NULL;
    while (blk != NULL) {
        blk_next = blk->next;
        free(blk->events);
        free(blk);
        blk = blk_next;
    }
    return 0;
}
int ntyreactor_addlistener(struct ntyreactor *reactor, int sockfd, NCALLBACK *acceptor) {
    if (reactor == NULL) return -1;
    if (reactor->evblk == NULL) return -1;
    //reactor->evblk->events[sockfd];
    struct ntyevent *event = ntyreactor_idx(reactor, sockfd);
    nty_event_set(event, sockfd, acceptor, reactor);
    nty_event_add(reactor->epfd, EPOLLIN, event);
    return 0;
}
int ntyreactor_run(struct ntyreactor *reactor) {
    if (reactor == NULL) return -1;
    if (reactor->epfd < 0) return -1;
    if (reactor->evblk == NULL) return -1;
    struct epoll_event events[MAX_EPOLL_EVENTS+1];
    int checkpos = 0, i;
    while (1) {
/*
        long now = time(NULL);
        for (i = 0;i < 100;i ++, checkpos ++) {
            if (checkpos == MAX_EPOLL_EVENTS) {
                checkpos = 0;
            }
            if (reactor->events[checkpos].status != 1) {
                continue;
            }
            long duration = now - reactor->events[checkpos].last_active;
            if (duration >= 60) {
                close(reactor->events[checkpos].fd);
                printf("[fd=%d] timeout\n", reactor->events[checkpos].fd);
                nty_event_del(reactor->epfd, &reactor->events[checkpos]);
            }
        }
*/
        int nready = epoll_wait(reactor->epfd, events, MAX_EPOLL_EVENTS, 1000);
        if (nready < 0) {
            printf("epoll_wait error, exit\n");
            continue;
        }
        for (i = 0;i < nready;i ++) {
            struct ntyevent *ev = (struct ntyevent*)events[i].data.ptr;
            if ((events[i].events & EPOLLIN) && (ev->events & EPOLLIN)) {
                ev->callback(ev->fd, events[i].events, ev->arg);
            }
            if ((events[i].events & EPOLLOUT) && (ev->events & EPOLLOUT)) {
                ev->callback(ev->fd, events[i].events, ev->arg);
            }
        }
    }
}
// 3, 6w, 1, 100 == 
// <remoteip, remoteport, localip, localport>
int main(int argc, char *argv[]) {
    unsigned short port = SERVER_PORT; // listen 8888
    if (argc == 2) {
        port = atoi(argv[1]);
    }
    struct ntyreactor *reactor = (struct ntyreactor*)malloc(sizeof(struct ntyreactor));
    ntyreactor_init(reactor);
    int i = 0;
    int sockfds[PORT_COUNT] = {0};
    for (i = 0;i < PORT_COUNT;i ++) {
        sockfds[i] = init_sock(port+i);
        ntyreactor_addlistener(reactor, sockfds[i], accept_cb);
    }
    //dup2(sockfd, STDIN);
    ntyreactor_run(reactor);
    ntyreactor_destory(reactor);
    for (i = 0;i < PORT_COUNT;i ++) {
        close(sockfds[i]);
    }
    free(reactor);
    return 0;
}
nts[i].events, ev->arg);
            }
        }
    }
}
// 3, 6w, 1, 100 == 
// <remoteip, remoteport, localip, localport>
int main(int argc, char *argv[]) {
    unsigned short port = SERVER_PORT; // listen 8888
    if (argc == 2) {
        port = atoi(argv[1]);
    }
    struct ntyreactor *reactor = (struct ntyreactor*)malloc(sizeof(struct ntyreactor));
    ntyreactor_init(reactor);
    int i = 0;
    int sockfds[PORT_COUNT] = {0};
    for (i = 0;i < PORT_COUNT;i ++) {
        sockfds[i] = init_sock(port+i);
        ntyreactor_addlistener(reactor, sockfds[i], accept_cb);
    }
    //dup2(sockfd, STDIN);
    ntyreactor_run(reactor);
    ntyreactor_destory(reactor);
    for (i = 0;i < PORT_COUNT;i ++) {
        close(sockfds[i]);
    }
    free(reactor);
    return 0;
}
```

原文链接：https://zhuanlan.zhihu.com/p/496676483

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.453】hash，bloomfilter，分布式一致性hash

## 1.场景

- 使用word 文档时，判断某个单词是否拼写正确
- 垃圾邮件过滤算法
- Redis缓存穿透
- bitcoin core中交易校验

## 2.需求

从海量数据中查询某个字符串是否存在？

## 3.平衡二叉搜索树

增删改查时间复杂度是O(log2n)

平衡的目的是保证操作的时间复杂度稳定，保证下次搜索能够稳定排除一半的数据。

O(log2n)的直观理解：100万个节点，最多比较20次；10亿个节点，最多比较30次。

总结：通过比较保证有序，使用二分查找，通过每次排除一般的元素达到快速索引的目的。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171241_86764.jpg)

## 4.散列表(HashMap)

根据key计算key在hash map数组中的位置的数据结构。

## 5.hash函数

映射函数 Hash(key)=addr；hash 函数可能会把两个或两个以上的不同 key 映射到同一地址，这种情况称之为冲突（或者 hash 碰撞）；

hash 函数实现过程当中 为什么 会出现 i*31?

i * 31 = i * (32-1) = i * (1<<5 -1) = i << 5 - i；可以将乘法转换为位运算，速度快。

31是质数，hash 随机分布性是最好的。

## 6.选择hash函数

计算速度快

强随机分布（等概率、均匀地分布在整个地址空间）

murmurhash1，murmurhash2，murmurhash3，siphash（redis6.0当中使⽤，rust等大多数语言选用的hash算法来实现hashmap），cityhash 都具备强随机分布性

siphash 主要解决字符串接近的强随机分布性

## 7.负载因子

负载因子 = 数组存储元素的个数 / 数据长度

用来形容散列表的存储密度；负载因子越小，冲突越小，负载因子越大，冲突越大；

## 8.冲突处理

**链表法**

引用链表来处理哈希冲突；也就是将冲突元素用链表链接起来；这也是常用的处理冲突的⽅式；但是可能出现一种极端情况，冲突元素比较多，该冲突链表过长，这个时候可以将这个链表转换为红黑树；由原来链表时间复杂度O(n) 转换为红黑树时间复杂度O(log2n)；那么判断该链表过长的依据是多少？可以采⽤超过 256（经验值）个节点的时候将链表结构转换为红黑树结构；

**开放寻址法**

将所有的元素都存放在哈希表的数组中，不使用额外的数据结构；一般使用线性探查的思路解决。

## 9.**布隆过滤器**

是一种数据结构，比较巧妙的概率型数据结构（probabilistic data structure），特点是高效地插入和查询，能确定某个字符串一定不存在或者可能存在。

布隆过滤器不存储具体数据，所以占用空间小，查询结果存在误差，但是误差可控，同时不支持删除操作。

## 10.为什么不用HashMap

key映射到HashMap 的数组，然后可以在 O(1) 的时间复杂度内返回结果，效率也非常高。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171242_18189.jpg)

## 11.HashMap的问题：size增加，内存占用比较大

**测试结果：**

unordered_map<string, bool> mp;

main_key=“[https://www.alibaba.com/”](https://link.zhihu.com/?target=https%3A//www.alibaba.com/”)

sub_key=to_string(i);

Key = main_key+sub_key;

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171242_30915.jpg)

二叉树也跟hashmap类似，而且还需要比较字符串，不适用于海量数据中字符串的查找。

而bloomfilter，不比较字符串，不存储具体的元素，可以查询海量数据。

## 12.Bloomfilter构成

布隆过滤器是一个bit 数组(bitmap) + n个hash函数

m % 2n = m & (2n - 1)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171243_68453.jpg)

## 13.原理

布隆过滤器的原理是，当一个元素被加入bitmap时，通过K 个 Hash 函数将这个元素映射成bitmap中的 K 个点，把它们置为 1；检索时，再通过 k 个 hash 函数运算检测bitmap的 k 个点是否都为1; 如果这些点有任何一个不为1，则该key一定不存在; 如果全部为 1 ，则该key 很可能（我们期望存在的概率是多少可以设置） 存在。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171244_37438.png)

(1) 例如针对值 baidu 和三个不同的哈希函数分别生成了哈希值 1 、 4 、 7 ，则上图转变为

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171244_90782.jpg)

(2) 再存一个值 tencent，如果哈希函数返回 3 、 4 、 8 的话，图继续变为：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171245_53677.jpg)

4这个 bit 位由于两个哈希函数都返回了这个 bit 位，因此它被覆盖了。现在我们如果想查询 alibaba 这个值是否存在，哈希函数返回了 1 、 5 、 8 三个值，结果我们发现 5 这个 bit 位上的值为 0 说明没有任何一个值映射到这个 bit 位上 ，因此我们可以很确定地说 alibaba 这个值不存在。而当我们需要查询 baidu 这个值是否存在的话，那么哈希函数必然会返回 1 、 4 、 7 ，然后我们检查发现这三个 bit 位上的值均为 1 ，那么我们可以说 baidu存在了么？答案是不可以，只能是 baidu 这个值可能存在。会存在误判。

## 14.Bloomfilter为什么不支持删除元素？

在bitmap中每个bit只有两种状态（0 或者 1），一个bit被设置为 1 状态，但不确定它被设置了多少次；也就是不知道被多少个 key 哈希映射而来以及是被具体哪个 hash 函数映射而来；

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171245_45027.jpg)

### 15.应用场景

布隆过滤器通常用于判断某个 key 一定不存在的场景，同时允许判断存在时有误差的情况；

常用于解决数据库的缓存穿透问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171246_54754.jpg)

描述缓存场景，为了减轻数据库（mysql）的访问压力，在 server 端与数据库（mysql）之间加入缓存用来存储热点数据；

描述缓存穿透，server端请求数据时，缓存和数据库都不包含该数据，最终请求压力全部涌向数据库；

数据请求步骤，如图中 2 所示；

发生原因：黑客利用漏洞伪造数据攻击或者内部业务 bug 造成大量重复请求不存在的数据；

解决方案：如图中 3 所示；

## 16.应用分析

在实际应用中，该选择多少个 hash 函数？要分配多少空间的位图？预期存储多少元素？如何控制误差？

可以参考[https://hur.st/bloomfilter](https://link.zhihu.com/?target=https%3A//hur.st/bloomfilter)

## 17.分布式一致性hash

### 17.1.应用场景

分布式一致性hash最先是用来解决分布式缓存问题的。将数据均衡的分散在不同的服务器中，用来分摊缓存服务器的压力；解决缓存服务器数量变化引起的缓存失效问题。

### 17.2.普通的hash算法在分布式应用中的不足

以前memcached不支持集群，不能够横向扩展，就使用hash将数据均衡分布到多个memcached中。

但是，对于增加节点，就会有问题。

比如，在分布式的存储系统中，要将数据存储到具体的节点上，如果我们采用普通的hash算法进行路由，将数据映射到具体的节点上，如key%N，key是数据的key，N是机器节点数，如果有一个机器加入或退出这个集群，则所有的数据映射都无效了，如果是持久化存储则要做数据迁移，如果是分布式缓存，则其他缓存就失效了。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171247_78751.jpg)

### 17.3.分布式一致性hash解决数据映射失效问题

分布式一致性hash能解决增加节点，数据映射失效的问题。

把算法固定住，不因为增加节点而变化。不是对节点数量取余，而是对固定的数 2 {32} 取余。

分布式一致性 hash 算法将哈希空间组织成一个虚拟的圆环，圆环的大小是 2 {32} ；

将各个节点使用hash函数进行一个哈希，具体可以选择节点的ip + port作为关键字进行hash，这样每个节点就能确定其在哈希环上的位置。存放数据的时候，使用相同的hash函数，得到key在hash环上的具体位置。根据数据的hash值，去hash环找到第一个大于等于数据hash值的节点，数据存储到该节点。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171247_37177.jpg)

分布式一致性hash，对于增加节点，只会影响部分数据映射。对于这个问题，需要通过数据迁移来解决。

### 17.4.hash偏移

hash 算法得到的结果是随机的，不能保证服务器节点均匀分布在哈希环上；分布不均匀造成请求访问不均匀，服务器承受的压力不均匀。导致了数据倾斜问题。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171248_11194.jpg)

节点足够多的话， hash具有强随机分布性，数据就会均匀分布。

解决方法：增加虚拟节点。为每一个节点生成多个虚拟节点。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171249_16951.jpg)

## 18.总结

hashmap、bloomfilter和分布式一致性hash都是基于hash函数的。

hashmap用于key-value数据的存储，如C++ stl的unordered_map，redis数据组织等；bloomfilter是一种概率行数据结构，可以确定某个key一定不存在或者可能存在；分布式一致性hash用于分布式缓存系统的负载均衡，并且解决节点变化带来的缓存失效问题。

原文链接：https://zhuanlan.zhihu.com/p/497190019

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.454】DPDK技术系统学习一（接收，发送，arp，icmp功能测试）

***\*如何技术不去手动做练习实践，就总有一种无从下手的感觉\****

## 1.准备环境并启动，使用dpdk接管其中一个网卡。

ubuntu虚拟机环境配置多队列网卡，安装dpdk。

在环境已经配置ok的前提下，每次重启环境后需要重新配置环境变量，并且绑定网卡。

```
export RTE_SDK=/home/hlp/dpdk/dpdk-stable-19.08.2
export RTE_TARGET=x86_64-native-linux-gcc
ifconfig    #注意保存要绑定的网卡的ip和mac地址，理解是mac地址比较重要
#这里我dpdk要绑定eth0网卡，其对应的ip和mac为  192.168.50.59和00-0c-29-4d-f0-d3
sudo ifconfig eth0 down  #关闭要绑定的网卡
./usertools/dpdk-setup.sh #通过脚本绑定网卡，使dpdk接管网卡数据。 这里用49

```

## 2.测试dpdk接管网卡数据，测试对udp数据的接收。

### 2.1.描述预计准备

通过第0步，dpdk已经接管了网卡，个人理解是这里与mac地址。==》dpdk接管网卡

获取老师提供的已有的基于dpdk实现的测试接收功能的demo代码。==》准备demo

demo实现原理 ==》通过dpdk提供的接口获取到网卡数据，对数据进行过滤，观察udp数据

参考dpdk examples目录，用makefile进行编译。 ===》编译测试代码，使用make命令

查看生成的可执行文件，目录如下：

```
root@ubuntu:/home/hlp/dpdk/dpdk-stable-19.08.2/examples/01_recv# tree
├── build                    #这个目录都是编译生成的相关文件
│   ├── app
│   │   ├── dpdk_recv
│   │   └── dpdk_recv.map
│   ├── dpdk_recv            #生成的可执行文件
│   ├── dpdk_recv.map
│   ├── _install
│   ├── _postbuild
│   ├── _postinstall
│   ├── _preinstall
│   └── recv.o
├── Makefile                #编译makefile配置文件
└── recv.c                    #我们的demo代码
2 directories, 12 files
```

**运行测试进行查看，**

===》网卡接收到的数据过多

===》使用测试代码对接收到数据进行过滤，解析udp的相关数据，通过打印观察现象，

===》运行测试demo，使用串口调试工具模拟udp的发送，观察demo打印信息。

### 2.2.正确测试结果如下：

**1：测试demo运行如下：**

```
root@ubuntu:/home/hlp/dpdk/dpdk-stable-19.08.2/examples/01_recv# ./build/dpdk_recv EAL: Detected 8 lcore(s)EAL: Detected 1 NUMA nodesEAL: Multi-process socket /var/run/dpdk/rte/mp_socketEAL: Selected IOVA mode 'PA'EAL: No available hugepages reported in hugepages-1048576kBEAL: Probing VFIO support...EAL: VFIO support initializedEAL: PCI device 0000:02:06.0 on NUMA socket -1EAL:   Invalid NUMA socket, default to 0EAL:   probe driver: 8086:100f net_e1000_emEAL: PCI device 0000:03:00.0 on NUMA socket -1EAL:   Invalid NUMA socket, default to 0EAL:   probe driver: 15ad:7b0 net_vmxnet3
```

**2：启动网络调试助手进行数据发送测试：==》中间可能有发送不成功，下文分析**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171904_81123.jpg)

**描述：**

====》从图中可以看到，测试发送后，基于dpdk实现的测试demo运行ok

====》demo可以正常接收到我们的数据，并正常分析出我们的报文中的原ip，目的ip，以及发送内容

**遗留问题：**

====》 这里的端口打印可能有问题，后期通过自己实现解决

====》这里除了接收我们的消息外，还会接收到相关其他的udp数据，为什么？

**3：测试中流程分析：**

我的测试场景是：使用物理机+虚拟机（linux环境进行测试）

在物理机上用串口模拟工具下发，目标ip填写的是上文保存的dpdk 绑定网卡前的ip，端口随机。

使用串口工具进行测试时，会发现必然无法发送成功的场景，这是因为这里的发送ip没有找到对应的arp表。

**分析：**

===》要想在物理机发送给虚拟机的链路ok，需要arp表的支持。

===》ip其实是可变的，mac地址（唯一）是寻址的关键，需要配置arp表。

===》配置arp表需要关注，arp -a查出的arp表是多个接口有对应关系，需要配置arp表在对应的接口上。

**配置arp表的相关命令如下：**

```
#注意  这里一定要保存对dpdk绑定的网卡的mac地址# 查看arp 表 arp -a#使用arp命令进行添加 ==》还是不生效，没添加在对应接口中arp -s 192.168.50.59 00-0c-29-4d-f0-d3arp -d 192.168.50.59 #使用netsh进行arp的绑定#1：找到 网线或者网卡对应的idx netsh i i show in#2： 绑定网卡和解绑命令（这里使用有线网或者以太网） 16是查找到的网卡对应的idx值netsh -c i i add neighbors 16 192.168.50.59 00-0c-29-4d-f0-d3#配置后可以通过arp -a查看，发现多了一个静态arp#192.168.50.59         00-0c-29-85-2e-88     静态netsh  i i delete neighbors 16
```

## **3.基于dpdk实现通过网卡发送数据。**

### **3.1.思路描述**

在dpdk 接管网卡数据，解析udp数据的基础上，实现udp数据的回复发送。

通过dpdk相关接口，初始化时要获取到dpdk绑定的本端网卡的mac地址

需要根据获取到的udp的数据报，按照ethhdr协议栈，ip协议栈，udp协议栈依次解析，获取原端口，目的端口，原ip，目的ip，来源数据的mac地址等必要数据

根据相关协议栈规则，构造附后udp报文的结构的数据 eth头，ip头，udp头+body数据的结构进行发送。

### **3.2.测试**

这里直接用老师的demo，02_send.c，编译和运行同上。

参考上文，同样需要手动配置绑定arp，使通路可以识别。

观察：串口通信工具能正常接收到dpdk回复的原数据。

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171905_79049.jpg)

## **4.实现arp的探测回复功能。**

每一次测试时，都需要手动配置arp才能使链路通，需要arp功能支持。

**arp业务处理有三个点：**

===》1：收到arp探测报文，对探测报文的回复（arp response）

===》2：广播发送arp探测报文（arp request）

===》3：arp表的保存

arp主要实现和获取了网络ip和mac地址的对应映射，是局域网网络互通的关键。

### **4.1 .arp描述及demo梳理**

dpdk可以接管网卡接收到的所有数据。

对数据进行解析，过滤arp报文，根据协议类型（0x0806）。

对所有的arp报文进行解析（根据本机ip），因为会收到局域网所有节点的arp探测报文。

已经过滤出是本机ip对应探测的arp报文，根据arp协议（eth头+arp头），构造arp报文进行发送

### **4.2 .测试demo的操作及运行**

获取老师提供的arp.c文件，修改文件中设定的本机ip。

删除测试物理机上已经配置的arp映射关系（netsh i i delete neighbors 16）。

编译并进行运行测试。

===》在没有对物理机arp表做dpdk绑定的网卡映射信息前提下，使用串口工具进行udp发送测试

===》启动后发现，可以收到并过滤出局域网多个环境发来的arp探测请求

===》使用串口工具进行udp发送测试，观察到现象

```
#1： 使用arp -a查物理机arp表，新增了dpdk接收网卡对应mac的一条信息  192.168.50.59         00-0c-29-4d-f0-d3     动态#2: 不用手动配置arp  上文udp的发送和接收都正常。
```

**理解：**在发送udp/tcp等数据前，会查看对应的arp缓存表，如果没有找到映射信息，会先发arp探测报文。

**现象：**可以尝试观察日志/抓包，针对物理机给dpdk发送udp数据的场景，分析这里arp 探测报文的发送时机。（物理机可以抓包查看，dpdk测试机可以通过日志查看，应该是在接收udp报文前有一个物理机ip的arp探测报文）

## 5.icmp协议报文的实现

### **5.1.概述**

icmp协议是ip层的附属协议，

ICMP 相当于网络世界的侦察兵。常用的有两种类型，主动探查的查询报文和异常报告的差错报文。

ping 命令使用查询报文，Traceroute 命令使用差错报文。

这里实现基础的ping命令的功能，

### **5.2.测试**

1：获取已有的测试demo，同上，修改源码中设定本机的ip值，获取网卡mac地址与该ip绑定，提供arp支持（这里的ip设置要注意，注意和测试机同一网段）。

2：对网卡接收到的报文，根据ip头部协议类型解析提取icmp相关的报文，根据icmp协议头部类型，提取为ping命令对应报文，解析后获取必要信息构造icmp报文进行回复。

3：运行该demo的情况下，在测试机器上运行ping 网卡配置的ip，观察返回报文以及我们的运行日志。

### **5.3.测试结果**

可以看到，在没有启动测试代码前，无法ping通，启动后，已经实现ping命令功能，日志上可以看到一些日志：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215171906_30011.jpg)



原文链接：https://zhuanlan.zhihu.com/p/497811718

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.455】后端开发【一大波干货知识】网络通信模型和网络IO管理

## 1.简单的C/S通信模型（accept阻塞的话，就只能一个客户端接进来）

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215172810_93612.jpg)

## 2.socket()函数

```
//函数原型。返回：若是成功则为非负数，如出错则为 -1
int socket(int domin, int type, int protocol);
//调用 参数1：使用多少位地址AF_INET 为32，参数2：数据报的类型。 参数3：TCP/UDP 协议
int clientfd = socket(AF_INET, SOCK_STREAM, 0);
```

**connect() 函数,用来向服务器建立连接**

```
//函数原型。 返回：成功为 0， 出错为 -1
int connect(int clientfd, const struct sockaddr *addr, socklen_t addrlen );
```

**bind()函数，绑定地址**

```
//函数原型 返回：成功为 0，出错为 -1
int     bind(int, const struct sockaddr *, socklen_t) __DARWIN_ALIAS(bind);
//使用：
    struct sockaddr_in _addr;
    _addr.sin_family = AF_INET;
    _addr.sin_port = htons(8888);
    _addr.sin_addr.s_addr = INADDR_ANY;
    bind(sockfd, (sockaddr *)&_addr, sizeof(_addr) );
```

listen() 函数，将sock从一个主动套接字转为一个监听套接字

```
//函数原型,返回一个非负数的连接描述符
int  listen(int sockfd, int backlog);
```

accept()函数，等待来自客户端的连接请求，到达监听描述符，然后在adr中填写客户端的套接字地址，并返回一个已连接描述符

```
int accept(int listenfd, struct sockaddr *addr, int *addrlen);
```

一个类似以上模型的小故事：

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215172811_81817.jpg)

```
#include <netinet/tcp.h>
#include <arpa/inet.h>
#include <errno.h>
#include <fcntl.h>
#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
using namespace std;
int main()
{
    int sockfd = socket(AF_INET, SOCK_STREAM, 0);//创建socket
    struct sockaddr_in _addr;
    _addr.sin_family = AF_INET;
    _addr.sin_port = htons(8888);
    _addr.sin_addr.s_addr = INADDR_ANY;
    socklen_t socklen = sizeof(_addr);
    bind(sockfd, (sockaddr *)&_addr, socklen );
    int listenfd = listen(sockfd, 0);
    struct sockaddr_in clientAddr;
    socklen_t clientlen = sizeof(clientAddr); 
    for (;;){
        int client_sockfd = accept(sockfd,(sockaddr *)&clientAddr, &clientlen);
        char buffer[1024] = {0};
        //等待用户发信息
        ssize_t len_recv = recv(client_sockfd,buffer,1024,0);
        //不做错误处理
        printf("Recv:%s, %d Bytes\n", buffer, len_recv);
        //给用户发信息
        send(client_sockfd,buffer,len_recv,0);
        //关闭这个连接
        close(client_sockfd);
    }
    close(sockfd);
    return 0;
}
```

socket 五元组标识 ：<客户端地址，客户端端口，服务器地址，服务器端口，使用的协议tcp/udp>，五元组中的任何一个元素发生变化都表示一个新的客户端socket连接。

## 3.网络IO

网络IO会涉及到两个系统对象，一个是用户空间调用IO的进程或线程，另一个是内核空间的内核系统，比如发生IO操作read时，它会经历两个阶段：

> 等待数据准备就绪(数据准备)
> 将数据从内核拷贝到进程或者线程中。(数据读写)

因为在以上两个阶段上各有不同的情况，所以出现了多种网络IO模型

**阻塞，非阻塞，同步，异步**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215172812_52174.jpg)

## 4.五种IO网络模型

### 4.1.阻塞IO（blocking IO）linux默认情况下都是阻塞IO

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215172812_72821.jpg)

> 当用户进程调用了read系统函数，内核就开始第一阶段数据的准备。对于网路IO来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的数据包），这个时候 内核 就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞（数据一直没准备好，就会一直空占CPU）。当 内核一直等到数据准备好了，它就会将数据从 内核 中拷贝到用户内存，然后 内核 返回结果，用户进程才解除 阻塞 的状态，重新运行起来。

### 4.2.非阻塞 (non-blocking IO)

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215172813_34094.jpg)

> 从上图可以看出，应用进程调用了read函数之后，立刻就有结果返回，这个结果给用户判断结果是个error时，它就知道数据没有准备好，于是它再次发送read函数。直到内核中的数据准备好，并且又再次收到用户进程的system call，那么它马上就将数据拷贝到用户内存，然后返回。所以非阻塞模式IO中，用户进程其实就是不断的主动询问内核数据准备好了没有。
> recv() 返回值大于0时，表示接受数据完毕，返回值既是接收到的字节数
> recv() 返回0，表示连接已正常断开；
> recv() 返回-1，且errno等于EAGAIN，表示recv操作还没执行完成；如果errno不等于EAGAIN，表示recv操作遇到系统错误errno
> 设置非阻塞： fcntl( fd, F_SETFL, O_NONBLOCK ); 不推荐使用，因为循环调用read(),会占用大量资源

### 4.3.IO多路复用

> 当用户进程调用了 select，那么整个进程会被 block，而同时，kernel 会“监视”所 有 select 负责的 socket，当任何一个 socket 中的数据准备好了，select 就会返回。这个时候用户进程再调用 read 操作，将数据从 kernel 拷贝到用户进程。这个图和 blocking IO 的图其实并没有太大的不同，事实上还更差一些。因为这里需要使用两个系统调用(select 和 read)，而 blocking IO 只调用了一个系统调用(read)。但是使用 select 以后最大的优势是用户可以在一个线程内同时处理多个 socket 的 IO 请求。用户可以注册多个 socket，然后不断地调用 select 读取被激活的 socket，即可达到在同一个线程内同时处理多个 IO 请求的目的。而在同步阻塞模型中，必须通过多线程的方式才能达到这个目的。**（所以，如果处理的连接数不是很高的话，使用select/epoll 的 web server 不一定比使用 multi-threading + blocking IO 的 web server 性能更好，可能延迟还更大。select/epoll 的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。）**

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215172814_16482.jpg)

### 4.4.信号驱动 （signal-driven）

内核在第一阶段是异步，在第二个阶段是同步；于非阻塞IO的区别在于它提供了消息通知机制，不需要用户进程不断的轮询检查，减少了系统API的调用次数，提高效率

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215172814_56702.jpg)

### 4.5.异步（asynchronous）

![img](https://linuxcpp.0voice.com/zb_users/upload/2022/12/15/20221215172815_87456.jpg)

```
struct aiocb { int aio_fildes off_t aio_offset volatile void *aio_buf size_t aio_nbytes int aio_reqprio struct sigevent aio_sigevent int aio_lio_opcode }
```

> 用户进程发起 read 操作之后，立刻就可以开始去做其它的事。而另一方面，从 kernel的角度，当它受到一个 asynchronous read 之后，首先它会立刻返回，所以不会对用户进程产生任何 block。然后，kernel 会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel 会给用户进程发送一个 signal，告诉它 read 操作完成了。

**总结：经上面的介绍，会发现non-blocking IO 和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。**
**而asynchronous IO则完全不同。它就像是用户进程将整个IO操作都交给了内核去完成，然后内核做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。**

原文链接：https://zhuanlan.zhihu.com/p/498435536

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)