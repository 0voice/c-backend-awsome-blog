# 【NO.176】Redis 多线程网络模型全面揭秘

## 0.**导言**

在目前的技术选型中，Redis 俨然已经成为了系统高性能缓存方案的事实标准，因此现在 Redis 也成为了后端开发的基本技能树之一，Redis 的底层原理也顺理成章地成为了必须学习的知识。

Redis 从本质上来讲是一个网络服务器，而对于一个网络服务器来说，网络模型是它的精华，搞懂了一个网络服务器的网络模型，你也就搞懂了它的本质。

本文通过层层递进的方式，介绍了 Redis 网络模型的版本变更历程，剖析了其从单线程进化到多线程的工作原理，此外，还一并分析并解答了 Redis 的网络模型的很多抉择背后的思考，帮助读者能更深刻地理解 Redis 网络模型的设计。

## 1.**Redis 有多快？**

根据官方的 benchmark，通常来说，在一台普通硬件配置的 Linux 机器上跑单个 Redis 实例，处理简单命令（时间复杂度 O(N) 或者 O(log(N))），QPS 可以达到 8w+，而如果使用 pipeline 批处理功能，则 QPS 至高能达到 100w。

仅从性能层面进行评判，Redis 完全可以被称之为高性能缓存方案。

## 2.**Redis 为什么快？**

Redis 的高性能得益于以下几个基础：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRv4vcI9ozcWceomicZtwbhElPzboBJvwicvBh6GwHQ8phib2PkeLjQhBUQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

- **C 语言实现**，虽然 C 对 Redis 的性能有助力，但语言并不是最核心因素。
- **纯内存 I/O**，相较于其他基于磁盘的 DB，Redis 的纯内存操作有着天然的性能优势。
- **I/O 多路复用**，基于 epoll/select/kqueue 等 I/O 多路复用技术，实现高吞吐的网络 I/O。
- **单线程模型**，单线程无法利用多核，但是从另一个层面来说则避免了多线程频繁上下文切换，以及同步机制如锁带来的开销。

## 3.**Redis 为何选择单线程？**

Redis 的核心网络模型选择用单线程来实现，这在一开始就引起了很多人的不解，Redis 官方的对于此的回答是：

> It's not very frequent that CPU becomes your bottleneck with Redis, as usually Redis is either memory or network bound. For instance, using pipelining Redis running on an average Linux system can deliver even 1 million requests per second, so if your application mainly uses O(N) or O(log(N)) commands, it is hardly going to use too much CPU.

核心意思就是，对于一个 DB 来说，CPU 通常不会是瓶颈，因为大多数请求不会是 CPU 密集型的，而是 I/O 密集型。具体到 Redis 的话，如果不考虑 RDB/AOF 等持久化方案，Redis 是完全的纯内存操作，执行速度是非常快的，因此这部分操作通常不会是性能瓶颈，Redis 真正的性能瓶颈在于网络 I/O，也就是客户端和服务端之间的网络传输延迟，因此 Redis 选择了单线程的 I/O 多路复用来实现它的核心网络模型。

上面是比较笼统的官方答案，实际上更加具体的选择单线程的原因可以归纳如下：

### 3.1 避免过多的上下文切换开销

多线程调度过程中必然需要在 CPU 之间切换线程上下文 context，而上下文的切换又涉及程序计数器、堆栈指针和程序状态字等一系列的寄存器置换、程序堆栈重置甚至是 CPU 高速缓存、TLB 快表的汰换，如果是进程内的多线程切换还好一些，因为单一进程内多线程共享进程地址空间，因此线程上下文比之进程上下文要小得多，如果是跨进程调度，则需要切换掉整个进程地址空间。

如果是单线程则可以规避进程内频繁的线程切换开销，因为程序始终运行在进程中单个线程内，没有多线程切换的场景。

### 3.2 避免同步机制的开销

如果 Redis 选择多线程模型，又因为 Redis 是一个数据库，那么势必涉及到底层数据同步的问题，则必然会引入某些同步机制，比如锁，而我们知道 Redis 不仅仅提供了简单的 key-value 数据结构，还有 list、set 和 hash 等等其他丰富的数据结构，而不同的数据结构对同步访问的加锁粒度又不尽相同，可能会导致在操作数据过程中带来很多加锁解锁的开销，增加程序复杂度的同时还会降低性能。

### 3.3 简单可维护

Redis 的作者 Salvatore Sanfilippo (别称 antirez) 对 Redis 的设计和代码有着近乎偏执的简洁性理念，你可以在阅读 Redis 的源码或者给 Redis 提交 PR 的之时感受到这份偏执。因此代码的简单可维护性必然是 Redis 早期的核心准则之一，而引入多线程必然会导致代码的复杂度上升和可维护性下降。

事实上，多线程编程也不是那么尽善尽美，首先多线程的引入会使得程序不再保持代码逻辑上的串行性，代码执行的顺序将变成不可预测的，稍不注意就会导致程序出现各种并发编程的问题；其次，多线程模式也使得程序调试更加复杂和麻烦。网络上有一幅很有意思的图片，生动形象地描述了并发编程面临的窘境。

你期望的多线程编程 **VS** 实际上的多线程编程：

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvat1HcdK6LgF6qcXcENK8AcR4VVq0ITnfMdVzgm2mSYNFSzicDxz0bDBEB8Wzkwnu1hKRTYXK06SuLQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)你期望的多线程VS实际上的多线程

前面我们提到引入多线程必须的同步机制，如果 Redis 使用多线程模式，那么所有的底层数据结构都必须实现成线程安全的，这无疑又使得 Redis 的实现变得更加复杂。

总而言之，Redis 选择单线程可以说是多方博弈之后的一种权衡：在保证足够的性能表现之下，使用单线程保持代码的简单和可维护性。

## 4.**Redis 真的是单线程？**

在讨论这个问题之前，我们要先明确『单线程』这个概念的边界：它的覆盖范围是核心网络模型，抑或是整个 Redis？如果是前者，那么答案是肯定的，在 Redis 的 v6.0 版本正式引入多线程之前，其网络模型一直是单线程模式的；如果是后者，那么答案则是否定的，Redis 早在 v4.0 就已经引入了多线程。

因此，当我们讨论 Redis 的多线程之时，有必要对 Redis 的版本划出两个重要的节点：

1. Redis v4.0（引入多线程处理异步任务）
2. Redis v6.0（正式在网络模型中实现 I/O 多线程）

### 4.1 单线程事件循环

我们首先来剖析一下 Redis 的核心网络模型，从 Redis 的 v1.0 到 v6.0 版本之前，Redis 的核心网络模型一直是一个典型的单 Reactor 模型：利用 epoll/select/kqueue 等多路复用技术，在单线程的事件循环中不断去处理事件（客户端请求），最后回写响应数据到客户端：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRQjOP938GeNx5AUv8ibY0Yvpzn9g3g5AfImJqLqv5EQ5bAI27hZia4gug/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

这里有几个核心的概念需要学习：

- **client**：客户端对象，Redis 是典型的 CS 架构（Client <---> Server），客户端通过 **socket** 与服务端建立网络通道然后发送请求命令，服务端执行请求的命令并回复。**Redis** 使用结构体 **client** 存储客户端的所有相关信息，包括但不限于`封装的套接字连接 -- *conn`，`当前选择的数据库指针 -- *db`，`读入缓冲区 -- querybuf`，`写出缓冲区 -- buf`，`写出数据链表 -- reply`等。
- **aeApiPoll**：I/O 多路复用 API，是基于 epoll_wait/select/kevent 等系统调用的封装，监听等待读写事件触发，然后处理，它是事件循环（Event Loop）中的核心函数，是事件驱动得以运行的基础。
- **acceptTcpHandler**：连接应答处理器，底层使用系统调用 `accept` 接受来自客户端的新连接，并为新连接注册绑定命令读取处理器，以备后续处理新的客户端 TCP 连接；除了这个处理器，还有对应的 `acceptUnixHandler` 负责处理 Unix Domain Socket 以及 `acceptTLSHandler` 负责处理 TLS 加密连接。
- **readQueryFromClient**：命令读取处理器，解析并执行客户端的请求命令。
- **beforeSleep**：事件循环中进入 aeApiPoll 等待事件到来之前会执行的函数，其中包含一些日常的任务，比如把 `client->buf` 或者 `client->reply` （后面会解释为什么这里需要两个缓冲区）中的响应写回到客户端，持久化 AOF 缓冲区的数据到磁盘等，相对应的还有一个 afterSleep 函数，在 aeApiPoll 之后执行。
- **sendReplyToClient**：命令回复处理器，当一次事件循环之后写出缓冲区中还有数据残留，则这个处理器会被注册绑定到相应的连接上，等连接触发写就绪事件时，它会将写出缓冲区剩余的数据回写到客户端。

Redis 内部实现了一个高性能的事件库 --- AE，基于 epoll/select/kqueue/evport 四种事件驱动技术，实现 Linux/MacOS/FreeBSD/Solaris 多平台的高性能事件循环模型。Redis 的核心网络模型正式构筑在 AE 之上，包括 I/O 多路复用、各类处理器的注册绑定，都是基于此才得以运行。

至此，我们可以描绘出客户端向 Redis 发起请求命令的工作原理：

1. Redis 服务器启动，开启主线程事件循环（Event Loop），注册 `acceptTcpHandler` 连接应答处理器到用户配置的监听端口对应的文件描述符，等待新连接到来；
2. 客户端和服务端建立网络连接；
3. `acceptTcpHandler` 被调用，主线程使用 AE 的 API 将 `readQueryFromClient` 命令读取处理器绑定到新连接对应的文件描述符上，并初始化一个 `client` 绑定这个客户端连接；
4. 客户端发送请求命令，触发读就绪事件，主线程调用 `readQueryFromClient` 通过 socket 读取客户端发送过来的命令存入 `client->querybuf` 读入缓冲区；
5. 接着调用 `processInputBuffer`，在其中使用 `processInlineBuffer` 或者 `processMultibulkBuffer` 根据 Redis 协议解析命令，最后调用 `processCommand` 执行命令；
6. 根据请求命令的类型（SET, GET, DEL, EXEC 等），分配相应的命令执行器去执行，最后调用 `addReply` 函数族的一系列函数将响应数据写入到对应 `client` 的写出缓冲区：`client->buf` 或者 `client->reply` ，`client->buf` 是首选的写出缓冲区，固定大小 16KB，一般来说可以缓冲足够多的响应数据，但是如果客户端在时间窗口内需要响应的数据非常大，那么则会自动切换到 `client->reply` 链表上去，使用链表理论上能够保存无限大的数据（受限于机器的物理内存），最后把 `client` 添加进一个 LIFO 队列 `clients_pending_write`；
7. 在事件循环（Event Loop）中，主线程执行 `beforeSleep` --> `handleClientsWithPendingWrites`，遍历 `clients_pending_write` 队列，调用 `writeToClient` 把 `client` 的写出缓冲区里的数据回写到客户端，如果写出缓冲区还有数据遗留，则注册 `sendReplyToClient` 命令回复处理器到该连接的写就绪事件，等待客户端可写时在事件循环中再继续回写残余的响应数据。

对于那些想利用多核优势提升性能的用户来说，Redis 官方给出的解决方案也非常简单粗暴：在同一个机器上多跑几个 Redis 实例。事实上，为了保证高可用，线上业务一般不太可能会是单机模式，更加常见的是利用 Redis 分布式集群多节点和数据分片负载均衡来提升性能和保证高可用。

### 4.2 多线程异步任务

以上便是 Redis 的核心网络模型，这个单线程网络模型一直到 Redis v6.0 才改造成多线程模式，但这并不意味着整个 Redis 一直都只是单线程。

Redis 在 v4.0 版本的时候就已经引入了的多线程来做一些异步操作，此举主要针对的是那些非常耗时的命令，通过将这些命令的执行进行异步化，避免阻塞单线程的事件循环。

我们知道 Redis 的 `DEL` 命令是用来删除掉一个或多个 key 储存的值，它是一个阻塞的命令，大多数情况下你要删除的 key 里存的值不会特别多，最多也就几十上百个对象，所以可以很快执行完，但是如果你要删的是一个超大的键值对，里面有几百万个对象，那么这条命令可能会阻塞至少好几秒，又因为事件循环是单线程的，所以会阻塞后面的其他事件，导致吞吐量下降。

Redis 的作者 antirez 为了解决这个问题进行了很多思考，一开始他想的办法是一种渐进式的方案：利用定时器和数据游标，每次只删除一小部分的数据，比如 1000 个对象，最终清除掉所有的数据，但是这种方案有个致命的缺陷，如果同时还有其他客户端往某个正在被渐进式删除的 key 里继续写入数据，而且删除的速度跟不上写入的数据，那么将会无止境地消耗内存，虽然后来通过一个巧妙的办法解决了，但是这种实现使 Redis 变得更加复杂，而多线程看起来似乎是一个水到渠成的解决方案：简单、易理解。于是，最终 antirez 选择引入多线程来实现这一类非阻塞的命令。更多 antirez 在这方面的思考可以阅读一下他发表的博客：[Lazy Redis is better Redis](http://antirez.com/news/93)。

于是，在 Redis v4.0 之后增加了一些的非阻塞命令如 `UNLINK`、`FLUSHALL ASYNC`、`FLUSHDB ASYNC`。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRplfowbOAzxBrakDhbibteUuBibSRRfF3oiau3LamJnnsMCLCcyEzDU5ag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

`UNLINK` 命令其实就是 `DEL` 的异步版本，它不会同步删除数据，而只是把 key 从 keyspace 中暂时移除掉，然后将任务添加到一个异步队列，最后由后台线程去删除，不过这里需要考虑一种情况是如果用 `UNLINK` 去删除一个很小的 key，用异步的方式去做反而开销更大，所以它会先计算一个开销的阀值，只有当这个值大于 64 才会使用异步的方式去删除 key，对于基本的数据类型如 List、Set、Hash 这些，阀值就是其中存储的对象数量。

## 5.**Redis 多线程网络模型**

前面提到 Redis 最初选择单线程网络模型的理由是：CPU 通常不会成为性能瓶颈，瓶颈往往是**内存**和**网络**，因此单线程足够了。那么为什么现在 Redis 又要引入多线程呢？很简单，就是 Redis 的网络 I/O 瓶颈已经越来越明显了。

随着互联网的飞速发展，互联网业务系统所要处理的线上流量越来越大，Redis 的单线程模式会导致系统消耗很多 CPU 时间在网络 I/O 上从而降低吞吐量，要提升 Redis 的性能有两个方向：

- 优化网络 I/O 模块
- 提高机器内存读写的速度

后者依赖于硬件的发展，暂时无解。所以只能从前者下手，网络 I/O 的优化又可以分为两个方向：

- 零拷贝技术或者 DPDK 技术
- 利用多核优势

零拷贝技术有其局限性，无法完全适配 Redis 这一类复杂的网络 I/O 场景，更多网络 I/O 对 CPU 时间的消耗和 Linux 零拷贝技术，可以阅读我的另一篇文章：[Linux I/O 原理和 Zero-copy 技术全面揭秘](https://strikefreedom.top/linux-io-and-zero-copy)。而 DPDK 技术通过旁路网卡 I/O 绕过内核协议栈的方式又太过于复杂以及需要内核甚至是硬件的支持。

因此，利用多核优势成为了优化网络 I/O 性价比最高的方案。

6.0 版本之后，Redis 正式在核心网络模型中引入了多线程，也就是所谓的 *I/O threading*，至此 Redis 真正拥有了多线程模型。前一小节，我们了解了 Redis 在 6.0 版本之前的单线程事件循环模型，实际上就是一个非常经典的 Reactor 模型：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcR5VNqhl8TpY2iaCiaED7ic8ZkgMaNfUAfYa6onXs6cRsoREiblYL3icf6pog/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

目前 Linux 平台上主流的高性能网络库/框架中，大都采用 Reactor 模式，比如 netty、libevent、libuv、POE(Perl)、Twisted(Python)等。

Reactor 模式本质上指的是使用 `I/O 多路复用(I/O multiplexing) + 非阻塞 I/O(non-blocking I/O)` 的模式。

更多关于 Reactor 模式的细节可以参考我之前的文章：[Go netpoller 原生网络模型之源码全面揭秘](https://strikefreedom.top/go-netpoll-io-multiplexing-reactor)，Reactor 网络模型那一小节，这里不再赘述。

Redis 的核心网络模型在 6.0 版本之前，一直是单 Reactor 模式：所有事件的处理都在单个线程内完成，虽然在 4.0 版本中引入了多线程，但是那个更像是针对特定场景（删除超大 key 值等）而打的补丁，并不能被视作核心网络模型的多线程。

通常来说，单 Reactor 模式，引入多线程之后会进化为 Multi-Reactors 模式，基本工作模式如下：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRJLN1JHqxe4n2jQvvLYDSAeH1iax9Bsb5VqC0ZAATlE6y97xoOe4ibtiaw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

区别于单 Reactor 模式，这种模式不再是单线程的事件循环，而是有多个线程（Sub Reactors）各自维护一个独立的事件循环，由 Main Reactor 负责接收新连接并分发给 Sub Reactors 去独立处理，最后 Sub Reactors 回写响应给客户端。

Multiple Reactors 模式通常也可以等同于 Master-Workers 模式，比如 Nginx 和 Memcached 等就是采用这种多线程模型，虽然不同的项目实现细节略有区别，但总体来说模式是一致的。

### 5.1 设计思路

Redis 虽然也实现了多线程，但是却不是标准的 Multi-Reactors/Master-Workers 模式，这其中的缘由我们后面会分析，现在我们先看一下 Redis 多线程网络模型的总体设计：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRGtQ2jB57bdFbiawdd4krQVfNibfYlicxyYkLjkMPdTgH8ep6Av8jniaSsA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

1. Redis 服务器启动，开启主线程事件循环（Event Loop），注册 `acceptTcpHandler` 连接应答处理器到用户配置的监听端口对应的文件描述符，等待新连接到来；
2. 客户端和服务端建立网络连接；
3. `acceptTcpHandler` 被调用，主线程使用 AE 的 API 将 `readQueryFromClient` 命令读取处理器绑定到新连接对应的文件描述符上，并初始化一个 `client` 绑定这个客户端连接；
4. 客户端发送请求命令，触发读就绪事件，服务端主线程不会通过 socket 去读取客户端的请求命令，而是先将 `client` 放入一个 LIFO 队列 `clients_pending_read`；
5. 在事件循环（Event Loop）中，主线程执行 `beforeSleep` -->`handleClientsWithPendingReadsUsingThreads`，利用 Round-Robin 轮询负载均衡策略，把 `clients_pending_read`队列中的连接均匀地分配给 I/O 线程各自的本地 FIFO 任务队列 `io_threads_list[id]` 和主线程自己，I/O 线程通过 socket 读取客户端的请求命令，存入 `client->querybuf` 并解析第一个命令，**但不执行命令**，主线程忙轮询，等待所有 I/O 线程完成读取任务；
6. 主线程和所有 I/O 线程都完成了读取任务，主线程结束忙轮询，遍历 `clients_pending_read` 队列，**执行所有客户端连接的请求命令**，先调用 `processCommandAndResetClient` 执行第一条已经解析好的命令，然后调用 `processInputBuffer` 解析并执行客户端连接的所有命令，在其中使用 `processInlineBuffer` 或者 `processMultibulkBuffer` 根据 Redis 协议解析命令，最后调用 `processCommand` 执行命令；
7. 根据请求命令的类型（SET, GET, DEL, EXEC 等），分配相应的命令执行器去执行，最后调用 `addReply` 函数族的一系列函数将响应数据写入到对应 `client` 的写出缓冲区：`client->buf` 或者 `client->reply` ，`client->buf` 是首选的写出缓冲区，固定大小 16KB，一般来说可以缓冲足够多的响应数据，但是如果客户端在时间窗口内需要响应的数据非常大，那么则会自动切换到 `client->reply` 链表上去，使用链表理论上能够保存无限大的数据（受限于机器的物理内存），最后把 `client` 添加进一个 LIFO 队列 `clients_pending_write`；
8. 在事件循环（Event Loop）中，主线程执行 `beforeSleep` --> `handleClientsWithPendingWritesUsingThreads`，利用 Round-Robin 轮询负载均衡策略，把 `clients_pending_write` 队列中的连接均匀地分配给 I/O 线程各自的本地 FIFO 任务队列 `io_threads_list[id]` 和主线程自己，I/O 线程通过调用 `writeToClient` 把 `client` 的写出缓冲区里的数据回写到客户端，主线程忙轮询，等待所有 I/O 线程完成写出任务；
9. 主线程和所有 I/O 线程都完成了写出任务， 主线程结束忙轮询，遍历 `clients_pending_write` 队列，如果 `client` 的写出缓冲区还有数据遗留，则注册 `sendReplyToClient` 到该连接的写就绪事件，等待客户端可写时在事件循环中再继续回写残余的响应数据。

这里大部分逻辑和之前的单线程模型是一致的，变动的地方仅仅是把读取客户端请求命令和回写响应数据的逻辑异步化了，交给 I/O 线程去完成，这里需要特别注意的一点是：**I/O 线程仅仅是读取和解析客户端命令而不会真正去执行命令，客户端命令的执行最终还是要在主线程上完成**。

### 5.2 源码剖析

> 以下所有代码基于目前最新的 [Redis v6.0.10](https://github.com/redis/redis/tree/6.0.10) 版本。

#### 5.2.1 **多线程初始化**

```
void initThreadedIO(void) {
    server.io_threads_active = 0; /* We start with threads not active. */

    // 如果用户只配置了一个 I/O 线程，则不会创建新线程（效率低），直接在主线程里处理 I/O。
    if (server.io_threads_num == 1) return;

    if (server.io_threads_num > IO_THREADS_MAX_NUM) {
        serverLog(LL_WARNING,"Fatal: too many I/O threads configured. "
                             "The maximum number is %d.", IO_THREADS_MAX_NUM);
        exit(1);
    }

    // 根据用户配置的 I/O 线程数，启动线程。
    for (int i = 0; i < server.io_threads_num; i++) {
        // 初始化 I/O 线程的本地任务队列。
        io_threads_list[i] = listCreate();
        if (i == 0) continue; // 线程 0 是主线程。

        // 初始化 I/O 线程并启动。
        pthread_t tid;
        // 每个 I/O 线程会分配一个本地锁，用来休眠和唤醒线程。
        pthread_mutex_init(&io_threads_mutex[i],NULL);
        // 每个 I/O 线程分配一个原子计数器，用来记录当前遗留的任务数量。
        io_threads_pending[i] = 0;
        // 主线程在启动 I/O 线程的时候会默认先锁住它，直到有 I/O 任务才唤醒它。
        pthread_mutex_lock(&io_threads_mutex[i]);
        // 启动线程，进入 I/O 线程的主逻辑函数 IOThreadMain。
        if (pthread_create(&tid,NULL,IOThreadMain,(void*)(long)i) != 0) {
            serverLog(LL_WARNING,"Fatal: Can't initialize IO thread.");
            exit(1);
        }
        io_threads[i] = tid;
    }
}
```

`initThreadedIO` 会在 Redis 服务器启动时的初始化工作的末尾被调用，初始化 I/O 多线程并启动。

Redis 的多线程模式默认是关闭的，需要用户在 `redis.conf` 配置文件中开启：

```
io-threads 4
io-threads-do-reads yes
```

#### 5.2.2 **读取请求**

当客户端发送请求命令之后，会触发 Redis 主线程的事件循环，命令处理器 `readQueryFromClient` 被回调，在以前的单线程模型下，这个方法会直接读取解析客户端命令并执行，但是多线程模式下，则会把 `client` 加入到 `clients_pending_read` 任务队列中去，后面主线程再分配到 I/O 线程去读取客户端请求命令：

```
void readQueryFromClient(connection *conn) {
    client *c = connGetPrivateData(conn);
    int nread, readlen;
    size_t qblen;

    // 检查是否开启了多线程，如果是则把 client 加入异步队列之后返回。
    if (postponeClientRead(c)) return;
    
    // 省略代码，下面的代码逻辑和单线程版本几乎是一样的。
    ... 
}

int postponeClientRead(client *c) {
    // 当多线程 I/O 模式开启、主线程没有在处理阻塞任务时，将 client 加入异步队列。
    if (server.io_threads_active &&
        server.io_threads_do_reads &&
        !ProcessingEventsWhileBlocked &&
        !(c->flags & (CLIENT_MASTER|CLIENT_SLAVE|CLIENT_PENDING_READ)))
    {
        // 给 client 打上 CLIENT_PENDING_READ 标识，表示该 client 需要被多线程处理，
        // 后续在 I/O 线程中会在读取和解析完客户端命令之后判断该标识并放弃执行命令，让主线程去执行。
        c->flags |= CLIENT_PENDING_READ;
        listAddNodeHead(server.clients_pending_read,c);
        return 1;
    } else {
        return 0;
    }
}
```

接着主线程会在事件循环的 `beforeSleep()` 方法中，调用 `handleClientsWithPendingReadsUsingThreads`：

```
int handleClientsWithPendingReadsUsingThreads(void) {
    if (!server.io_threads_active || !server.io_threads_do_reads) return 0;
    int processed = listLength(server.clients_pending_read);
    if (processed == 0) return 0;

    if (tio_debug) printf("%d TOTAL READ pending clients\n", processed);

    // 遍历待读取的 client 队列 clients_pending_read，
    // 通过 RR 轮询均匀地分配给 I/O 线程和主线程自己（编号 0）。
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_read,&li);
    int item_id = 0;
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;
    }

    // 设置当前 I/O 操作为读取操作，给每个 I/O 线程的计数器设置分配的任务数量，
    // 让 I/O 线程可以开始工作：只读取和解析命令，不执行。
    io_threads_op = IO_THREADS_OP_READ;
    for (int j = 1; j < server.io_threads_num; j++) {
        int count = listLength(io_threads_list[j]);
        io_threads_pending[j] = count;
    }

    // 主线程自己也会去执行读取客户端请求命令的任务，以达到最大限度利用 CPU。
    listRewind(io_threads_list[0],&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        readQueryFromClient(c->conn);
    }
    listEmpty(io_threads_list[0]);

    // 忙轮询，累加所有 I/O 线程的原子任务计数器，直到所有计数器的遗留任务数量都是 0，
    // 表示所有任务都已经执行完成，结束轮询。
    while(1) {
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += io_threads_pending[j];
        if (pending == 0) break;
    }
    if (tio_debug) printf("I/O READ All threads finshed\n");

    // 遍历待读取的 client 队列，清除 CLIENT_PENDING_READ 和 CLIENT_PENDING_COMMAND 标记，
    // 然后解析并执行所有 client 的命令。
    while(listLength(server.clients_pending_read)) {
        ln = listFirst(server.clients_pending_read);
        client *c = listNodeValue(ln);
        c->flags &= ~CLIENT_PENDING_READ;
        listDelNode(server.clients_pending_read,ln);

        if (c->flags & CLIENT_PENDING_COMMAND) {
            c->flags &= ~CLIENT_PENDING_COMMAND;
            // client 的第一条命令已经被解析好了，直接尝试执行。
            if (processCommandAndResetClient(c) == C_ERR) {
                /* If the client is no longer valid, we avoid
                 * processing the client later. So we just go
                 * to the next. */
                continue;
            }
        }
        processInputBuffer(c); // 继续解析并执行 client 命令。

        // 命令执行完成之后，如果 client 中有响应数据需要回写到客户端，则将 client 加入到待写出队列 clients_pending_write
        if (!(c->flags & CLIENT_PENDING_WRITE) && clientHasPendingReplies(c))
            clientInstallWriteHandler(c);
    }

    /* Update processed count on server */
    server.stat_io_reads_processed += processed;

    return processed;
}
```

这里的核心工作是：

- 遍历待读取的 `client` 队列 `clients_pending_read`，通过 RR 策略把所有任务分配给 I/O 线程和主线程去读取和解析客户端命令。
- 忙轮询等待所有 I/O 线程完成任务。
- 最后再遍历 `clients_pending_read`，执行所有 `client` 的命令。

#### 5.2.3 **写回响应**

完成命令的读取、解析以及执行之后，客户端命令的响应数据已经存入 `client->buf` 或者 `client->reply` 中了，接下来就需要把响应数据回写到客户端了，还是在 `beforeSleep` 中， 主线程调用 `handleClientsWithPendingWritesUsingThreads`：

```
int handleClientsWithPendingWritesUsingThreads(void) {
    int processed = listLength(server.clients_pending_write);
    if (processed == 0) return 0; /* Return ASAP if there are no clients. */

    // 如果用户设置的 I/O 线程数等于 1 或者当前 clients_pending_write 队列中待写出的 client
    // 数量不足 I/O 线程数的两倍，则不用多线程的逻辑，让所有 I/O 线程进入休眠，
    // 直接在主线程把所有 client 的相应数据回写到客户端。
    if (server.io_threads_num == 1 || stopThreadedIOIfNeeded()) {
        return handleClientsWithPendingWrites();
    }

    // 唤醒正在休眠的 I/O 线程（如果有的话）。
    if (!server.io_threads_active) startThreadedIO();

    if (tio_debug) printf("%d TOTAL WRITE pending clients\n", processed);

    // 遍历待写出的 client 队列 clients_pending_write，
    // 通过 RR 轮询均匀地分配给 I/O 线程和主线程自己（编号 0）。
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_write,&li);
    int item_id = 0;
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        c->flags &= ~CLIENT_PENDING_WRITE;

        /* Remove clients from the list of pending writes since
         * they are going to be closed ASAP. */
        if (c->flags & CLIENT_CLOSE_ASAP) {
            listDelNode(server.clients_pending_write, ln);
            continue;
        }

        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;
    }

    // 设置当前 I/O 操作为写出操作，给每个 I/O 线程的计数器设置分配的任务数量，
    // 让 I/O 线程可以开始工作，把写出缓冲区（client->buf 或 c->reply）中的响应数据回写到客户端。
    io_threads_op = IO_THREADS_OP_WRITE;
    for (int j = 1; j < server.io_threads_num; j++) {
        int count = listLength(io_threads_list[j]);
        io_threads_pending[j] = count;
    }

    // 主线程自己也会去执行读取客户端请求命令的任务，以达到最大限度利用 CPU。
    listRewind(io_threads_list[0],&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        writeToClient(c,0);
    }
    listEmpty(io_threads_list[0]);

    // 忙轮询，累加所有 I/O 线程的原子任务计数器，直到所有计数器的遗留任务数量都是 0。
    // 表示所有任务都已经执行完成，结束轮询。
    while(1) {
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += io_threads_pending[j];
        if (pending == 0) break;
    }
    if (tio_debug) printf("I/O WRITE All threads finshed\n");

    // 最后再遍历一次 clients_pending_write 队列，检查是否还有 client 的写出缓冲区中有残留数据，
    // 如果有，那就为 client 注册一个命令回复器 sendReplyToClient，等待客户端写就绪再继续把数据回写。
    listRewind(server.clients_pending_write,&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);

        // 检查 client 的写出缓冲区是否还有遗留数据。
        if (clientHasPendingReplies(c) &&
                connSetWriteHandler(c->conn, sendReplyToClient) == AE_ERR)
        {
            freeClientAsync(c);
        }
    }
    listEmpty(server.clients_pending_write);

    /* Update processed count on server */
    server.stat_io_writes_processed += processed;

    return processed;
}
```

这里的核心工作是：

- 检查当前任务负载，如果当前的任务数量不足以用多线程模式处理的话，则休眠 I/O 线程并且直接同步将响应数据回写到客户端。
- 唤醒正在休眠的 I/O 线程（如果有的话）。
- 遍历待写出的 `client` 队列 `clients_pending_write`，通过 RR 策略把所有任务分配给 I/O 线程和主线程去将响应数据写回到客户端。
- 忙轮询等待所有 I/O 线程完成任务。
- 最后再遍历 `clients_pending_write`，为那些还残留有响应数据的 `client` 注册命令回复处理器 `sendReplyToClient`，等待客户端可写之后在事件循环中继续回写残余的响应数据。

#### 5.2.4 **I/O 线程主逻辑**

```
void *IOThreadMain(void *myid) {
    /* The ID is the thread number (from 0 to server.iothreads_num-1), and is
     * used by the thread to just manipulate a single sub-array of clients. */
    long id = (unsigned long)myid;
    char thdname[16];

    snprintf(thdname, sizeof(thdname), "io_thd_%ld", id);
    redis_set_thread_title(thdname);
    // 设置 I/O 线程的 CPU 亲和性，尽可能将 I/O 线程（以及主线程，不在这里设置）绑定到用户配置的
    // CPU 列表上。
    redisSetCpuAffinity(server.server_cpulist);
    makeThreadKillable();

    while(1) {
        // 忙轮询，100w 次循环，等待主线程分配 I/O 任务。
        for (int j = 0; j < 1000000; j++) {
            if (io_threads_pending[id] != 0) break;
        }

        // 如果 100w 次忙轮询之后如果还是没有任务分配给它，则通过尝试加锁进入休眠，
        // 等待主线程分配任务之后调用 startThreadedIO 解锁，唤醒 I/O 线程去执行。
        if (io_threads_pending[id] == 0) {
            pthread_mutex_lock(&io_threads_mutex[id]);
            pthread_mutex_unlock(&io_threads_mutex[id]);
            continue;
        }

        serverAssert(io_threads_pending[id] != 0);

        if (tio_debug) printf("[%ld] %d to handle\n", id, (int)listLength(io_threads_list[id]));


        // 注意：主线程分配任务给 I/O 线程之时，
        // 会把任务加入每个线程的本地任务队列 io_threads_list[id]，
        // 但是当 I/O 线程开始执行任务之后，主线程就不会再去访问这些任务队列，避免数据竞争。
        listIter li;
        listNode *ln;
        listRewind(io_threads_list[id],&li);
        while((ln = listNext(&li))) {
            client *c = listNodeValue(ln);
            // 如果当前是写出操作，则把 client 的写出缓冲区中的数据回写到客户端。
            if (io_threads_op == IO_THREADS_OP_WRITE) {
                writeToClient(c,0);
              // 如果当前是读取操作，则socket 读取客户端的请求命令并解析第一条命令。
            } else if (io_threads_op == IO_THREADS_OP_READ) {
                readQueryFromClient(c->conn);
            } else {
                serverPanic("io_threads_op value is unknown");
            }
        }
        listEmpty(io_threads_list[id]);
        // 所有任务执行完之后把自己的计数器置 0，主线程通过累加所有 I/O 线程的计数器
        // 判断是否所有 I/O 线程都已经完成工作。
        io_threads_pending[id] = 0;

        if (tio_debug) printf("[%ld] Done\n", id);
    }
}
```

I/O 线程启动之后，会先进入忙轮询，判断原子计数器中的任务数量，如果是非 0 则表示主线程已经给它分配了任务，开始执行任务，否则就一直忙轮询一百万次等待，忙轮询结束之后再查看计数器，如果还是 0，则尝试加本地锁，因为主线程在启动 I/O 线程之时就已经提前锁住了所有 I/O 线程的本地锁，因此 I/O 线程会进行休眠，等待主线程唤醒。

主线程会在每次事件循环中尝试调用 `startThreadedIO` 唤醒 I/O 线程去执行任务，如果接收到客户端请求命令，则 I/O 线程会被唤醒开始工作，根据主线程设置的 `io_threads_op` 标识去执行命令读取和解析或者回写响应数据的任务，I/O 线程在收到主线程通知之后，会遍历自己的本地任务队列 `io_threads_list[id]`，取出一个个 `client` 执行任务：

- 如果当前是写出操作，则调用 `writeToClient`，通过 socket 把 `client->buf` 或者 `client->reply` 里的响应数据回写到客户端。
- 如果当前是读取操作，则调用 `readQueryFromClient`，通过 socket 读取客户端命令，存入 `client->querybuf`，然后调用 `processInputBuffer` 去解析命令，这里最终只会解析到第一条命令，然后就结束，不会去执行命令。
- 在全部任务执行完之后把自己的原子计数器置 0，以告知主线程自己已经完成了工作。

```
void processInputBuffer(client *c) {
// 省略代码
...

    while(c->qb_pos < sdslen(c->querybuf)) {
        /* Return if clients are paused. */
        if (!(c->flags & CLIENT_SLAVE) && clientsArePaused()) break;

        /* Immediately abort if the client is in the middle of something. */
        if (c->flags & CLIENT_BLOCKED) break;

        /* Don't process more buffers from clients that have already pending
         * commands to execute in c->argv. */
        if (c->flags & CLIENT_PENDING_COMMAND) break;
        /* Multibulk processing could see a <= 0 length. */
        if (c->argc == 0) {
            resetClient(c);
        } else {
            // 判断 client 是否具有 CLIENT_PENDING_READ 标识，如果是处于多线程 I/O 的模式下，
            // 那么此前已经在 readQueryFromClient -> postponeClientRead 中为 client 打上该标识，
            // 则立刻跳出循环结束，此时第一条命令已经解析完成，但是不执行命令。
            if (c->flags & CLIENT_PENDING_READ) {
                c->flags |= CLIENT_PENDING_COMMAND;
                break;
            }

            // 执行客户端命令
            if (processCommandAndResetClient(c) == C_ERR) {
                /* If the client is no longer valid, we avoid exiting this
                 * loop and trimming the client buffer later. So we return
                 * ASAP in that case. */
                return;
            }
        }
    }

...
}
```

这里需要额外关注 I/O 线程初次启动时会设置当前线程的 CPU 亲和性，也就是绑定当前线程到用户配置的 CPU 上，在启动 Redis 服务器主线程的时候同样会设置 CPU 亲和性，Redis 的核心网络模型引入多线程之后，加上之前的多线程异步任务、多进程（BGSAVE、AOF、BIO、Sentinel 脚本任务等），Redis 现如今的系统并发度已经很大了，而 Redis 本身又是一个对吞吐量和延迟极度敏感的系统，所以用户需要 Redis 对 CPU 资源有更细粒度的控制，这里主要考虑的是两方面：CPU 高速缓存和 NUMA 架构。

首先是 CPU 高速缓存（这里讨论的是 L1 Cache 和 L2 Cache 都集成在 CPU 中的硬件架构），这里想象一种场景：Redis 主进程正在 CPU-1 上运行，给客户端提供数据服务，此时 Redis 启动了子进程进行数据持久化（BGSAVE 或者 AOF），系统调度之后子进程抢占了主进程的 CPU-1，主进程被调度到 CPU-2 上去运行，导致之前 CPU-1 的高速缓存里的相关指令和数据被汰换掉，CPU-2 需要重新加载指令和数据到自己的本地高速缓存里，浪费 CPU 资源，降低性能。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRIXxGMfcYjfnic2crvbVO6SZiaGCibLNIiaX6Cld5QNZQibCgw4mlQFNT0eA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

因此，Redis 通过设置 CPU 亲和性，可以将主进程/线程和子进程/线程绑定到不同的核隔离开来，使之互不干扰，能有效地提升系统性能。

其次是基于 NUMA 架构的考虑，在 NUMA 体系下，内存控制器芯片被集成到处理器内部，形成 CPU 本地内存，访问本地内存只需通过内存通道而无需经过系统总线，访问时延大大降低，而多个处理器之间通过 QPI 数据链路互联，跨 NUMA 节点的内存访问开销远大于本地内存的访问：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRoERwicIEI7HcLazMZnWibb1PEichu4ddOfospnUjrO0QkTV8egOzVt0MQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

因此，Redis 通过设置 CPU 亲和性，让主进程/线程尽可能在固定的 NUMA 节点上的 CPU 上运行，更多地使用本地内存而不需要跨节点访问数据，同样也能大大地提升性能。

关于 NUMA 相关知识请读者自行查阅，篇幅所限这里就不再展开，以后有时间我再单独写一篇文章介绍。

最后还有一点，阅读过源码的读者可能会有疑问，Redis 的多线程模式下，似乎并没有对数据进行锁保护，事实上 Redis 的多线程模型是全程无锁（Lock-free）的，这是通过原子操作+交错访问来实现的，主线程和 I/O 线程之间共享的变量有三个：`io_threads_pending` 计数器、`io_threads_op` I/O 标识符和 `io_threads_list` 线程本地任务队列。

`io_threads_pending` 是原子变量，不需要加锁保护，`io_threads_op` 和 `io_threads_list` 这两个变量则是通过控制主线程和 I/O 线程交错访问来规避共享数据竞争问题：I/O 线程启动之后会通过忙轮询和锁休眠等待主线程的信号，在这之前它不会去访问自己的本地任务队列 `io_threads_list[id]`，而主线程会在分配完所有任务到各个 I/O 线程的本地队列之后才去唤醒 I/O 线程开始工作，并且主线程之后在 I/O 线程运行期间只会访问自己的本地任务队列 `io_threads_list[0]` 而不会再去访问 I/O 线程的本地队列，这也就保证了主线程永远会在 I/O 线程之前访问 `io_threads_list` 并且之后不再访问，保证了交错访问。`io_threads_op` 同理，主线程会在唤醒 I/O 线程之前先设置好 `io_threads_op` 的值，并且在 I/O 线程运行期间不会再去访问这个变量。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRJBRrpB3m53MVSxWicicdVRVrmKbTbqiaj2UlnLyxBgdGct0qjDAUlGMvg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 5.3 性能提升

Redis 将核心网络模型改造成多线程模式追求的当然是最终性能上的提升，所以最终还是要以 benchmark 数据见真章：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRiaQSN3kHrmK0gzlT6Mt2AJAibfRVTy1wo7WUVYVWZYfMn9VzFkM8VxJA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

测试数据表明，Redis 在使用多线程模式之后性能大幅提升，达到了一倍。更详细的性能压测数据可以参阅这篇文章：[Benchmarking the experimental Redis Multi-Threaded I/O](https://itnext.io/benchmarking-the-experimental-redis-multi-threaded-i-o-1bb28b69a314)。

以下是美图技术团队实测的新旧 Redis 版本性能对比图，仅供参考：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRmiaMAFBWiaEJTp8LzYyPNKibsIbictEh6icgIVgODElCW0TkX9PgLDLZWAw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvat1HcdK6LgF6qcXcENK8AcRoyRxiaUZEYmMNicQCEXtAicEC3Kgu0eLTA9NS4keQna51zrmmxicxJ1rKA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 5.4 模型缺陷

首先第一个就是我前面提到过的，Redis 的多线程网络模型实际上并不是一个标准的 Multi-Reactors/Master-Workers 模型，和其他主流的开源网络服务器的模式有所区别，最大的不同就是在标准的 Multi-Reactors/Master-Workers 模式下，Sub Reactors/Workers 会完成 `网络读 -> 数据解析 -> 命令执行 -> 网络写` 整套流程，Main Reactor/Master 只负责分派任务，而在 Redis 的多线程方案中，I/O 线程任务仅仅是通过 socket 读取客户端请求命令并解析，却没有真正去执行命令，所有客户端命令最后还需要回到主线程去执行，因此对多核的利用率并不算高，而且每次主线程都必须在分配完任务之后忙轮询等待所有 I/O 线程完成任务之后才能继续执行其他逻辑。

Redis 之所以如此设计它的多线程网络模型，我认为主要的原因是为了保持兼容性，因为以前 Redis 是单线程的，所有的客户端命令都是在单线程的事件循环里执行的，也因此 Redis 里所有的数据结构都是非线程安全的，现在引入多线程，如果按照标准的 Multi-Reactors/Master-Workers 模式来实现，则所有内置的数据结构都必须重构成线程安全的，这个工作量无疑是巨大且麻烦的。

所以，在我看来，Redis 目前的多线程方案更像是一个折中的选择：既保持了原系统的兼容性，又能利用多核提升 I/O 性能。

其次，目前 Redis 的多线程模型中，主线程和 I/O 线程的通信过于简单粗暴：忙轮询和锁，因为通过自旋忙轮询进行等待，导致 Redis 在启动的时候以及运行期间偶尔会有短暂的 CPU 空转引起的高占用率，而且这个通信机制的最终实现看起来非常不直观和不简洁，希望后面 Redis 能对目前的方案加以改进。

## 6.**总结**

Redis 作为缓存系统的事实标准，它的底层原理值得开发者去深入学习，Redis 自 2009 年发布第一版之后，其单线程网络模型的选择在社区中从未停止过讨论，多年来一直有呼声希望 Redis 能引入多线程从而利用多核优势，但是作者 antirez 是一个追求大道至简的开发者，对 Redis 加入任何新功能都异常谨慎，所以在 Redis 初版发布的十年后才最终将 Redis 的核心网络模型改造成多线程模式，这期间甚至诞生了一些 Redis 多线程的替代项目。虽然 antirez 一直在推迟多线程的方案，但却从未停止思考多线程的可行性，Redis 多线程网络模型的改造不是一朝一夕的事情，这其中牵扯到项目的方方面面，所以我们可以看到 Redis 的最终方案也并不完美，没有采用主流的多线程模式设计。

让我们来回顾一下 Redis 多线程网络模型的设计方案：

- 使用 I/O 线程实现网络 I/O 多线程化，I/O 线程只负责网络 I/O 和命令解析，不执行客户端命令。
- 利用原子操作+交错访问实现无锁的多线程模型。
- 通过设置 CPU 亲和性，隔离主进程和其他子进程，让多线程网络模型能发挥最大的性能。

通读本文之后，相信读者们应该能够了解到一个优秀的网络系统的实现所涉及到的计算机领域的各种技术：设计模式、网络 I/O、并发编程、操作系统底层，甚至是计算机硬件。另外还需要对项目迭代和重构的谨慎，对技术方案的深入思考，绝不仅仅是写好代码这一个难点。

## 7.参考&延伸阅读

- [Redis v5.0.10](https://github.com/redis/redis/tree/5.0.10)
- [Redis v6.0.10](https://github.com/redis/redis/tree/6.0.10)
- [Lazy Redis is better Redis](http://antirez.com/news/93)
- [An update about Redis developments in 2019](http://antirez.com/news/126)
- [How fast is Redis?](https://redis.io/topics/benchmarks)
- [Go netpoller 原生网络模型之源码全面揭秘](https://strikefreedom.top/go-netpoll-io-multiplexing-reactor)
- [Linux I/O 原理和 Zero-copy 技术全面揭秘](https://strikefreedom.top/linux-io-and-zero-copy)
- [Benchmarking the experimental Redis Multi-Threaded I/O](https://itnext.io/benchmarking-the-experimental-redis-multi-threaded-i-o-1bb28b69a314)
- [NUMA DEEP DIVE PART 1: FROM UMA TO NUMA](https://frankdenneman.nl/2016/07/07/numa-deep-dive-part-1-uma-numa/)

原文作者：allanpan，腾讯 IEG 后台开发工程师，公众号：远赴星辰。

原文链接：https://mp.weixin.qq.com/s/-op5WR1wSkgAuP7JYZWP8g

# 【NO.177】网络 IO 演变发展过程和模型介绍

在互联网中提起网络，我们都会避免不了讨论高并发、百万连接。而此处的百万连接的实现，脱离不了网络 IO 的选择，因此本文作为一篇个人学习的笔记，特此进行记录一下整个网络 IO 的发展演变过程。以及目前广泛使用的网络模型。

## **1.网络 IO 的发展**

在本节内容中，我们将一步一步介绍网络 IO 的演变发展过程。介绍完发展过程后，再对网络 IO 中几组容易混淆的概念进行对比、分析。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HETu42nzJ6nvmrltaxMIlZJdXr2TaY9pPbZoSMASuG5NhCh4sQzIXPDA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 1.1 网络 IO 的各个发展阶段

通常，我们在此讨论的网络 IO 一般都是针对 linux 操作系统而言。网络 IO 的发展过程是随着 linux 的内核演变而变化，因此网络 IO 大致可以分为如下几个阶段：

**1. 阻塞 IO(BIO)**
**2. 非阻塞 IO(NIO)**
**3. IO 多路复用第一版(select/poll)**
**4. IO 多路复用第二版(epoll)**
**5. 异步 IO(AIO)**

而每一个阶段，都是因为当前的网络有一些缺陷，因此又在不断改进该缺陷。这是**网络 IO 一直演变过程中的本质**。下面将对上述几个阶段进行介绍，并对每个阶段的网络 IO 解决了哪些问题、优点、缺点进行剖析。

### **1.2 网络的两个阶段**

在网络中，我们通常可以将其广义上划分为以下两个阶段：

**第一阶段：硬件接口到内核态**
**第二阶段：内核态到用户态**

本人理解：我们通常上网，大部分数据都是通过网线传递的。因此对于两台计算机而言，要进行网络通信，其数据都是先从应用程序传递到传输层(TCP/UDP)到达内核态，然后再到网络层、数据链路层、物理层，接着数据传递到硬件网卡，最后通过网络传输介质传递到对端机器的网卡，然后再一步一步数据从网卡传递到内核态，最后再拷贝到用户态。

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEZDwwibVibDL8RCj0Vhia4xMMbO5FqvRcdIEicjYyjQSpBe4kanbfFbRrRg/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 1.3 阻塞 IO 和非阻塞 IO 的区别

根据 1.2 节的内容，我们可以知道，网络中的数据传输从网络传输介质到达目的机器，需要如上两个阶段。此处我们把从**硬件到内核态**这一阶段，是否发生阻塞等待，可以将网络分为**阻塞 IO**和**非阻塞 IO**。如果用户发起了读写请求，但内核态数据还未准备就绪，该阶段不会阻塞用户操作，内核立马返回，则称为非阻塞 IO。如果该阶段一直阻塞用户操作。直到内核态数据准备就绪，才返回。这种方式称为阻塞 IO。

因此，区分阻塞 IO 和非阻塞 IO 主要看第一阶段是否阻塞用户操作。

### 1.4 同步 IO 和异步 IO 的区别

从前面我们知道了，数据的传递需要两个阶段，在此处只要任何一个阶段会阻塞用户请求，都将其称为同步 IO，两个阶段都不阻塞，则称为异步 IO。

在目前所有的操作系统中，linux 中的 epoll、mac 的 kqueue 都属于同步 IO，因为其在第二阶段(数据从内核态到用户态)都会发生拷贝阻塞。而只有 windows 中的 IOCP 才真正属于异步 IO，即 AIO。

## **2.阻塞 IO**

在本节，我们将介绍最初的阻塞 IO，阻塞 IO 英文为 blocking IO，又称为 BIO。根据前面的介绍，阻塞 IO 主要指的是第一阶段(硬件网卡到内核态)。

### 2.1 阻塞 IO 的概念

阻塞 IO，顾名思义当用户发生了系统调用后，如果数据未从网卡到达内核态，内核态数据未准备好，此时会一直阻塞。直到数据就绪，然后从内核态拷贝到用户态再返回。具体过程可以参考 2.2 的图示。

### 2.2 阻塞 IO 的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEGibuyiacHXVXK7gdQ95jEUAwSuy3s6G5v61Tms5R368FXesWOhiajiaq4A/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 2.3 阻塞 IO 的缺点

在一般使用阻塞 IO 时，都需要配置多线程来使用，最常见的模型是**阻塞 IO+多线程**，每个连接一个单独的线程进行处理。

**我们知道，一般一个程序可以开辟的线程是有限的，而且开辟线程的开销也是比较大的。也正是这种方式，会导致一个应用程序可以处理的客户端请求受限。面对百万连接的情况，是无法处理。**

既然发现了问题，分析了问题，那就得解决问题。既然阻塞 IO 有问题，本质是由于其阻塞导致的，因此自然而然引出了下面即将介绍的主角：**非阻塞 IO**

## **3.非阻塞 IO**

非阻塞 IO 是为了解决前面提到的阻塞 IO 的缺陷而引出的，下面我们将介绍非阻塞 IO 的过程。

### 3.1 非阻塞 IO 的概念

非阻塞 IO：见名知意，就是在第一阶段(网卡-内核态)数据未到达时不等待，然后直接返回。因此非阻塞 IO 需要不断的用户发起请求，询问内核数据好了没，好了没。

### 3.2 非阻塞 IO 的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HESdhj8nSKbRFuZ6U6GuoT5gnyttj1uic3YcC0LQy2kHC45s3jSC82aXA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

非阻塞 IO 是需要系统内核支持的，在创建了连接后，可以调用 setsockop 设置 noblocking

### 3.3 非阻塞 IO 的优点

正如前面提到的，非阻塞 IO 解决了阻塞 IO**每个连接一个线程处理的问题**，所以其最大的优点就是 **一个线程可以处理多个连接**，这也是其非阻塞决定的。

### 3.4 非阻塞 IO 的缺点

但这种模式，也有一个问题，就是需要用户多次发起系统调用。**频繁的系统调用**是比较消耗系统资源的。

因此，既然存在这样的问题，那么自然而然我们就需要解决该问题：**保留非阻塞 IO 的优点的前提下，减少系统调用**

## **4.IO 多路复用第一版**

为了解决非阻塞 IO 存在的频繁的系统调用这个问题，随着内核的发展，出现了 IO 多路复用模型。那么我们就需要搞懂几个问题：

1. IO 多路复用到底复用什么？
2. IO 多路复用如何复用？

**IO 多路复用：** 很多人都说，IO 多路复用是用一个线程来管理多个网络连接，但本人不太认可，因为在非阻塞 IO 时，就已经可以实现一个线程处理多个网络连接了，这个是由于其非阻塞而决定的。

**在此处，个人观点，多路复用主要复用的是通过有限次的系统调用来实现管理多个网络连接。最简单来说，我目前有 10 个连接，我可以通过一次系统调用将这 10 个连接都丢给内核，让内核告诉我，哪些连接上面数据准备好了，然后我再去读取每个就绪的连接上的数据。因此，IO 多路复用，复用的是系统调用。通过有限次系统调用判断海量连接是否数据准备好了**

**无论下面的 select、poll、epoll，其都是这种思想实现的，不过在实现上，select/poll 可以看做是第一版，而 epoll 是第二版**

### 4.1IO 多路复用第一版的概念

**IO 多路复用第一版，这个概念是本人想出来的，主要是方便将 select/poll 和 epoll 进行区分**

所以此处 IO 多路复用第一版，主要特指 select 和 poll 这两个。

select 的 api

```
// readfds:关心读的fd集合；writefds：关心写的fd集合；excepttfds：异常的fd集合
int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

> select 函数监视的文件描述符分 3 类，分别是 writefds、readfds、和 exceptfds。调用后 select 函数会阻塞，直到有描述副就绪（有数据 可读、可写、或者有 except），或者超时（timeout 指定等待时间，如果立即返回设为 null 即可），函数返回。当 select 函数返回后，可以 通过遍历 fdset，来找到就绪的描述符。

> select 目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个优点。select 的一 个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在 Linux 上一般为 1024，可以通过修改宏定义甚至重新编译内核的方式提升这一限制，但 是这样也会造成效率的降低。

poll 的 api

```
int poll (struct pollfd *fds, unsigned int nfds, int timeout);

struct pollfd {
    int fd; /* file descriptor */
    short events; /* requested events to watch */
    short revents; /* returned events witnessed */
};
```

> pollfd 结构包含了要监视的 event 和发生的 event，不再使用 select“参数-值”传递的方式。同时，pollfd 并没有最大数量限制（但是数量过大后性能也是会下降）。和 select 函数一样，poll 返回后，需要轮询 pollfd 来获取就绪的描述符。

> 从上面看，select 和 poll 都需要在返回后，通过遍历文件描述符来获取已经就绪的 socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。

**从本质来说：IO 多路复用中，select()/poll()/epoll_wait()这几个函数对应第一阶段；read()/recvfrom()对应第二阶段**

### 4.2IO 多路复用第一版的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEbPaUbbrbibQsCw9xPib0FfZXfOKQ6OdekEicBSyr5micL6iaUAIQqRrDqAA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 4.3IO 多路复用第一版的优点

**IO 多路复用，主要在于复用，通过 select()或者 poll()将多个 socket fds 批量通过系统调用传递给内核，由内核进行循环遍历判断哪些 fd 上数据就绪了，然后将就绪的 readyfds 返回给用户。再由用户进行挨个遍历就绪好的 fd，读取或者写入数据。**

**所以通过 IO 多路复用+非阻塞 IO，一方面降低了系统调用次数，另一方面可以用极少的线程来处理多个网络连接。**

### 4.4IO 多路复用第一版的缺点

虽然第一版 IO 多路复用解决了之前提到的频繁的系统调用次数，但同时引入了新的问题：**用户需要每次将海量的 socket fds 集合从用户态传递到内核态，让内核态去检测哪些网络连接数据就绪了**

**但这个地方会出现频繁的将海量 fd 集合从用户态传递到内核态，再从内核态拷贝到用户态。所以，这个地方开销也挺大。**

既然还有这个问题，那我们继续开始解决这个问题，因此就引出了第二版的 IO 多路复用。

**其实思路也挺简单，既然需要拷贝，那就想办法，不拷贝。既然不拷贝，那就在内核开辟一段区域咯**

### 4.5IO 多路复用第一版的区别

**select 和 poll 的区别**

1. select 能处理的最大连接，默认是 1024 个，可以通过修改配置来改变，但终究是有限个；而 poll 理论上可以支持无限个
2. select 和 poll 在管理海量的连接时，会频繁的从用户态拷贝到内核态，比较消耗资源。

## **5.IO 多路复用第二版**

IO 多路复用第二版主要指 epoll，epoll 的出现也是随着内核版本迭代才诞生的，在网上到处看到，epoll 是内核 2.6 以后开始支持的

**epoll 的出现是为了解决前面提到的 IO 多路复用第一版的问题**

### 5.1IO 多路复用第二版的概念

epoll 提供的 api

```
//创建epollFd，底层是在内核态分配一段区域，底层数据结构红黑树+双向链表
int epoll_create(int size)；//创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大

//往红黑树中增加、删除、更新管理的socket fd
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；

//这个api是用来在第一阶段阻塞，等待就绪的fd。
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
1. int epoll_create(int size);
创建一个epoll的句柄，size用来告诉内核这个监听的数目一共有多大，这个参数不同于select()中的第一个参数，给出最大监听的fd+1的值，参数size并不是限制了epoll所能监听的描述符最大个数，只是对内核初始分配内部数据结构的一个建议。
当创建好epoll句柄后，它就会占用一个fd值，在linux下如果查看/proc/进程id/fd/，是能够看到这个fd的，所以在使用完epoll后，必须调用close()关闭，否则可能导致fd被耗尽。

2. int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；
函数是对指定描述符fd执行op操作。
- epfd：是epoll_create()的返回值。
- op：表示op操作，用三个宏来表示：添加EPOLL_CTL_ADD，删除EPOLL_CTL_DEL，修改EPOLL_CTL_MOD。分别添加、删除和修改对fd的监听事件。
- fd：是需要监听的fd（文件描述符）
- epoll_event：是告诉内核需要监听什么事，struct epoll_event结构如下：

struct epoll_event {
  __uint32_t events;  /* Epoll events */
  epoll_data_t data;  /* User data variable */
};

//events可以是以下几个宏的集合：
EPOLLIN ：表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
EPOLLOUT：表示对应的文件描述符可以写；
EPOLLPRI：表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
EPOLLERR：表示对应的文件描述符发生错误；
EPOLLHUP：表示对应的文件描述符被挂断；
EPOLLET： 将EPOLL设为边缘触发(Edge Triggered)模式，这是相对于水平触发(Level Triggered)来说的。
EPOLLONESHOT：只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里

3. int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
等待epfd上的io事件，最多返回maxevents个事件。
参数events用来从内核得到事件的集合，maxevents告之内核这个events有多大，这个maxevents的值不能大于创建epoll_create()时的size，参数timeout是超时时间（毫秒，0会立即返回，-1将不确定，也有说法说是永久阻塞）。该函数返回需要处理的事件数目，如返回0表示已超时。
```

二 工作模式

epoll 对文件描述符的操作有两种模式：LT（level trigger）和 ET（edge trigger）。LT 模式是默认模式，LT 模式与 ET 模式的区别如下：　　 LT 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序可以不立即处理该事件。下次调用 epoll_wait 时，会再次响应应用程序并通知此事件。　　 ET 模式：当 epoll_wait 检测到描述符事件发生并将此事件通知应用程序，应用程序必须立即处理该事件。如果不处理，下次调用 epoll_wait 时，不会再次响应应用程序并通知此事件。

1. LT 模式

LT(level triggered)是缺省的工作方式，并且同时支持 block 和 no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的 fd 进行 IO 操作。如果你不作任何操作，内核还是会继续通知你的。

1. ET 模式

ET(edge-triggered)是高速工作方式，只支持 no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过 epoll 告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了(比如，你在发送，接收或者接收请求，或者发送接收的数据少于一定量时导致了一个 EWOULDBLOCK 错误）。但是请注意，如果一直不对这个 fd 作 IO 操作(从而导致它再次变成未就绪)，内核不会发送更多的通知(only once)

ET 模式在很大程度上减少了 epoll 事件被重复触发的次数，因此效率要比 LT 模式高。epoll 工作在 ET 模式的时候，必须使用非阻塞套接口，以避免由于一个文件句柄的阻塞读/阻塞写操作把处理多个文件描述符的任务饿死。

### 5.2IO 多路复用第二版的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEbPaUbbrbibQsCw9xPib0FfZXfOKQ6OdekEicBSyr5micL6iaUAIQqRrDqAA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

当 epoll_wait()调用后会阻塞，然后完了当返回时，会返回了哪些 fd 的数据就绪了，用户只需要遍历就绪的 fd 进行读写即可。

### 5.3IO 多路复用第二版的优点

**IO 多路复用第二版 epoll 的优点在于：**

一开始就在内核态分配了一段空间，来存放管理的 fd,所以在每次连接建立后，交给 epoll 管理时，需要将其添加到原先分配的空间中，后面再管理时就不需要频繁的从用户态拷贝管理的 fd 集合。通通过这种方式大大的提升了性能。

所以现在的 IO 多路复用主要指 epoll

### 5.4IO 多路复用第二版的缺点

**个人猜想：** 如何降低占用的空间

## **6.异步 IO**

### 6.1 异步 IO 的过程

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEXGYke8ibPkbXEkaQ861CSg8YvQ3qFdvGcXicAicyrD9Vaicx77liakCLBnA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

前面介绍的所有网络 IO 都是同步 IO，因为当数据在内核态就绪时，在内核态拷贝用用户态的过程中，仍然会有短暂时间的阻塞等待。而异步 IO 指：**内核态拷贝数据到用户态这种方式也是交给系统线程来实现，不由用户线程完成**，目前只有 windows 系统的 IOCP 是属于异步 IO。

## **7.网络 IO 各种模型**

### 7.1 reactor 模型

目前 reactor 模型有以下几种实现方案：

**1. 单 reactor 单线程模型**
**2. 单 reactor 多线程模型**
**3. multi-reactor 多线程模型**
**4. multi-reactor 多进程模型**

> 下文网络模型的图，均摘自[这篇文章](https://zhuanlan.zhihu.com/p/95662364)

#### **7.1.1 单 reactor 单线程模型**

此种模型，通常是只有一个 epoll 对象，所有的**接收客户端连接**、**客户端读取**、**客户端写入**操作都包含在一个线程内。该种模型也有一些中间件在用，比如 redis

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEibnlbqiclVmzSkNpIym8ibYaXOAAnn2cZSibDr7fiaHXGjypWk0iacO7kkdA/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> 但在目前的单线程 Reactor 模式中，不仅 I/O 操作在该 Reactor 线程上，连非 I/O 的业务操作也在该线程上进行处理了，这可能会大大延迟 I/O 请求的响应。所以我们应该将非 I/O 的业务逻辑操作从 Reactor 线程上卸载，以此来加速 Reactor 线程对 I/O 请求的响应。

#### **7.1.2 单 reactor 多线程模型**

该模型主要是通过将，前面的模型进行改造，将读写的业务逻辑交给具体的线程池来实现，这样可以显示 reactor 线程对 IO 的响应，以此提升系统性能![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEvzbIZnWXH0HUWLgUZTRzHGzNsRK0F2JC0bxHmia6XG02wocUUvianoIQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> 在工作者线程池模式中，虽然非 I/O 操作交给了线程池来处理，但是所有的 I/O 操作依然由 Reactor 单线程执行，在高负载、高并发或大数据量的应用场景，依然较容易成为瓶颈。所以，对于 Reactor 的优化，又产生出下面的多线程模式。

#### **7.1.3 multi-reactor 多线程模型**

在这种模型中，主要分为两个部分：mainReactor、subReactors。mainReactor 主要负责接收客户端的连接，然后将建立的客户端连接通过负载均衡的方式分发给 subReactors，

subReactors 来负责具体的每个连接的读写

对于非 IO 的操作，依然交给工作线程池去做，对逻辑进行解耦

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEJ1BVQvZoIJB0xqBLxNvCOcOWkTknoMP4t1XkUgHfsoN7J2jzClJuaw/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

> mainReactor 对应 Netty 中配置的 BossGroup 线程组，主要负责接受客户端连接的建立。一般只暴露一个服务端口，BossGroup 线程组一般一个线程工作即可 subReactor 对应 Netty 中配置的 WorkerGroup 线程组，BossGroup 线程组接受并建立完客户端的连接后，将网络 socket 转交给 WorkerGroup 线程组，然后在 WorkerGroup 线程组内选择一个线程，进行 I/O 的处理。WorkerGroup 线程组主要处理 I/O，一般设置 2*CPU 核数个线程

### 7.2 proactor 模型

proactor 主要是通过对异步 IO 的封装的一种模型，它需要底层操作系统的支持，目前只有 windows 的 IOCP 支持的比较好。详细的介绍可以参考[这篇文章](https://zhuanlan.zhihu.com/p/95662364)

### 7.3 主流的中间件所采用的网络模型

![图片](https://mmbiz.qpic.cn/mmbiz_jpg/j3gficicyOvauhvQxpibL7I3CeEZPcaA1HEiaTnSvliaMcYCEDmUiaDlbRQTlURhaKHY8N0qwsgLuXZpkOgeJ3UtXMlQ/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

### 7.4 主流网络框架

- netty
- gnet
- libevent
- evio(golang)
- ACE(c++)
- boost::asio(c++)
- muduo （linux only)

关于c++和c的上述几个库对比，感兴趣的话大家可以自行搜索资料或者阅读[这篇文章](https://www.cnblogs.com/leijiangtao/p/5197566.html)。



## **8.参考资料**

1. [IO 模式和 IO 多路复用](https://juejin.im/post/5bf7b89e518825369c564059)
2. [Linux IO 模式及 select、poll、epoll 详解](https://segmentfault.com/a/1190000003063859)
3. [Chapter 6. I/O Multiplexing: The select and poll Functions](http://www.masterraghu.com/subjects/np/introduction/unix_network_programming_v1.3/ch06.html)
4. [高性能 IO 模型分析-Reactor 模式和 Proactor 模式（二）](https://zhuanlan.zhihu.com/p/95662364)

原文作者：jaydenwen，腾讯 pcg 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/EDzFOo3gcivOe_RgipkTkQ

# 【NO.178】操作系统与存储：解析Linux内核全新异步IO引擎io_uring设计与实现

## **0.引言**

存储场景中，我们对性能的要求非常高。在存储引擎底层的IO技术选型时，可能会有如下讨论关于IO的讨论。

> http://davmac.org/davpage/linux/async-io.html
> So from the above documentation, it seems that Linux doesn't have a true async file I/O that is not blocking (AIO, Epoll or POSIX AIO are all broken in some ways). I wonder if tlinux has any remedy. We should reach out to tlinux experts to get their opinions.

看完这段话，读者可能会有如下的问题。

1. 这是在讨论什么，为何会有此番讨论？
2. 有没有更好的解决方案？
3. 更好的解决方案是通过怎样的设计和实现解决问题？
4. ...

2019年，Linux Kernel正式进入5.x时代，众多新特性中，与存储领域相关度最高的便是最新的IO引擎——io_uring。从一些性能测试的结论来看，io_uring性能远高于native AIO方式，带来了巨大的性能提升，这对当前异步IO领域也是一个big news。

1. 对于问题1，本文简述了Linux过往的的IO发展历程，同步IO接口、原生异步IO接口AIO的缺陷，为何原有方式存在缺陷。
2. 对于问题2，本文从设计的角度出发，介绍了最新的IO引擎io_uring的相关内容。
3. 对于问题3，本文深入最新版内核linux-5.10中解析了io_uring的大体实现（关键数据结构、流程、特性实现等）。
4. ...

## 1.**一切过往，皆为序章**

以史为镜，可以知兴替。我们先看看现存过往IO接口的缺陷。

### 1.1 过往同步IO接口

当今Linux对文件的操作有很多种方式，过往同步IO接口从功能上划分，大体分为如下几种。

- 原始版本
- offset版本
- 向量版本
- offset+向量版本

#### 1.1.1 **read，write**

最原始的文件IO系统调用就是read，write

read系统调用从文件描述符所指代的打开文件中读取数据。

read简单介绍：

```
NAME
    read - read from a file descriptor
SYNOPSIS
    #include <unistd.h>

    ssize_t read(int fd, void *buf, size_t count);
DESCRIPTION
    read() attempts to read up to count bytes from file descriptor fd
    into the buffer starting at buf.
    
    On files that support seeking, the read operation commences at the
    file offset, and the file offset is incremented by the number of
    bytes read.  If the file offset is at or past the end of file, no
    bytes are read, and read() returns zero.
    
    If count is zero, read() may detect the errors described below.  In
    the absence of any errors, or if read() does not check for errors, a
    read() with a count of 0 returns zero and has no other effects.
    
    According to POSIX.1, if count is greater than SSIZE_MAX, the result
    is implementation-defined; see NOTES for the upper limit on Linux.
```

write系统调用将数据写入一个已打开的文件中。

write简单介绍：

```
NAME
    write - write to a file descriptor
SYNOPSIS
    #include <unistd.h>
    
    ssize_t write(int fd, const void *buf, size_t count);
DESCRIPTION
    write() writes up to count bytes from the buffer starting at buf to
    the file referred to by the file descriptor fd.
    
    The number of bytes written may be less than count if, for example,
    there is insufficient space on the underlying physical medium, or the
    RLIMIT_FSIZE resource limit is encountered (see setrlimit(2)), or the
    call was interrupted by a signal handler after having written less
    than count bytes.  (See also pipe(7).)
    
    For a seekable file (i.e., one to which lseek(2) may be applied, for
    example, a regular file) writing takes place at the file offset, and
    the file offset is incremented by the number of bytes actually
    written.  If the file was open(2)ed with O_APPEND, the file offset is
    first set to the end of the file before writing.  The adjustment of
    the file offset and the write operation are performed as an atomic
    step.
    
    POSIX requires that a read(2) that can be proved to occur after a
    write() has returned will return the new data.  Note that not all
    filesystems are POSIX conforming.
    
    According to POSIX.1, if count is greater than SSIZE_MAX, the result
    is implementation-defined; see NOTES for the upper limit on Linux.
```

#### 1.1.2 **在文件特定偏移处的IO：pread，pwrite**

在多线程环境下，为了保证线程安全，需要保证下列操作的原子性。

```
    off_t orig;
    orig = lseek(fd, 0, SEEK_CUR); // Save current offset
    lseek(fd, offset, SEEK_SET);
    s = read(fd, buf, len);
    lseek(fd, orig, SEEK_SET); // Restore original file offset
```

让使用者来保证原子性较繁，从接口上就有保证是一个好的选择，后来出现的pread便实现了这一点。

与read, write类似，pread, pwrite调用时可以指定位置进行文件IO操作，而非始于文件的当前偏移处，且他们不会改变文件的当前偏移量。这种方式，减少了编码，并提高了代码的健壮性。

pread、pwrite简单介绍：

```
NAME
       pread,  pwrite  -  read from or write to a file descriptor at a given
       offset
SYNOPSIS
       #include <unistd.h>

       ssize_t pread(int fd, void *buf, size_t count, off_t offset);

       ssize_t pwrite(int fd, const void *buf, size_t count, off_t offset);
       
DESCRIPTION
       pread() reads up to count bytes from file descriptor fd at offset
       offset (from the start of the file) into the buffer starting at buf.
       The file offset is not changed.

       pwrite() writes up to count bytes from the buffer starting at buf to
       the file descriptor fd at offset offset.  The file offset is not
       changed.

       The file referenced by fd must be capable of seeking.
```

当然，往read，write接口参数的标志位集合中加入新标志，用以表征新逻辑，可能达到相同的效果，但是这可能不够优雅——如果某个参数有多种可能的值，而函数内又以条件表达式检查这些参数值，并根据不同参数值做出不同的行为，那么以明确函数取代参数（Replace Parameter with Explicit Methods）也是一种合适的重构手法。

如果需要反复执行lseek，并伴之以文件IO，那么pread和pwrite系统调用在某些情况下是具有性能优势的。这是因为执行单个pread或pwrite系统调用的成本要低于执行lseek和read/write两个系统调用（当然，相对地，执行实际IO的开销通常要远大于执行系统调用，系统调用的性能优势作用有限）。历史上，一些数据库，通过使用kernel的这一新接口，获得了不菲的收益。如PostgreSQL：[*[PATCH\] Using pread instead of lseek (with analysis)*](https://www.postgresql-archive.org/PATCH-Using-pread-instead-of-lseek-with-analysis-td2215257.html)

#### 1.1.3 **分散输入和集中输出（Scatter-Gather IO）：readv, writev**

“物质的组成与结构决定物质的性质，性质决定用途，用途体现性质。”是自然科学的重要思想，在计算机科学中也是如此。现有计算机体系结构下，数据存储由一个或多个基本单元组成，物理、逻辑上的结构，决定了数据存储的性质——可能是连续的，也可能是不连续的。

对于不连续的数据的处理相对较繁，例如，使用read将数据读到不连续的内存，使用write将不连续的内存发送出去。更具体地看，如果要从文件中读一片连续的数据至进程的不同区域，有两种方案：

1. 使用read一次将它们读至一个较大的缓冲区中，然后将它们分成若干部分复制到不同的区域。
2. 调用read若干次分批将它们读至不同区域。

同样地，如果想将程序中不同区域的数据块连续地写至文件，也必须进行类似的处理。而且这种方案需要多次调用read、write系统调用，有损性能。

那么如何简化编程，如何解决这种开销呢？一种有效的解法就是使用特定的数据结构对非连续的数据进行管理，批量传输数据。从接口上就有此保证是一个好的选择，后来出现的readv，writev便实现了这一点。

这种基于向量的，分散输入和集中输出的系统调用并非只对单个缓冲区进行读写操作，而是一次即可传输多个缓冲区的数据，免除了多次系统调用的开销。该机制使用一个数组iov定义了一组用来传输数据的缓冲区，一个整形数iovcnt指定iov的成员个数，其中，iov中的每个成员都是如下形式的数据结构。

```
struct iovec {
   void  *iov_base;    /* Starting address */
   size_t iov_len;     /* Number of bytes to transfer */
};
```

**功能交集：preadv，pwritev**

上述两种功能都是一种进步，不过似乎格格不入，那么是否能合二为一，进两步呢？

数学上，集合是指具有某种特定性质的具体的或抽象的对象汇总而成的集体。其中，构成集合的这些对象则称为该集合的元素。我这里将接口定义成一种集合，一种特定功能就是其中的一个元素。根据已知有限集构造一个子集，该子集对于每一个元素要么包含要么不包含，那么根据乘法原理，这个子集共有2^N 种构造方式，即有2^N个子集。这么多可能的集合，显然较繁。基于场景对于功能子集的需求、元素之间的容斥、集合中元素是否需要有序（接口层面对功能的表现）、简约性等因素，我们会确立一些优雅的接口，这也是函数接口设计的一个哲学话题。

后来出现的preadv，pwritev，便是偏移和向量的交集，也是一种在排列组合的巨大可能性下确立的少部分简约的接口。

**带标志位集合的IO：preadv2，pwritev2**

再后来，还出现了变种函数preadv2和pwritev2，相比较preadv，pwritev，v2版本还能设置本次IO的标志，比如RWF_DSYNC、RWF_HIPRI、RWF_SYNC、RWF_NOWAIT、RWF_APPEND。

readv、preadv、preadv2系列简单介绍：

```
NAME
    readv,  writev,  preadv,  pwritev,  preadv2, pwritev2 - read or write
       data into multiple buffers

SYNOPSIS
    #include <sys/uio.h>

   ssize_t readv(int fd, const struct iovec *iov, int iovcnt);

   ssize_t writev(int fd, const struct iovec *iov, int iovcnt);

   ssize_t preadv(int fd, const struct iovec *iov, int iovcnt,
                  off_t offset);

   ssize_t pwritev(int fd, const struct iovec *iov, int iovcnt,
                   off_t offset);

   ssize_t preadv2(int fd, const struct iovec *iov, int iovcnt,
                   off_t offset, int flags);

   ssize_t pwritev2(int fd, const struct iovec *iov, int iovcnt,
                    off_t offset, int flags);

DESCRIPTION
   The readv() system call reads iovcnt buffers from the file associated
       with the file descriptor fd into the buffers described by iov
       ("scatter input").

       The writev() system call writes iovcnt buffers of data described by
       iov to the file associated with the file descriptor fd ("gather
       output").

       The pointer iov points to an array of iovec structures, defined in
       <sys/uio.h> as:

           struct iovec {
               void  *iov_base;    /* Starting address */
               size_t iov_len;     /* Number of bytes to transfer */
           };

       The readv() system call works just like read(2) except that multiple
       buffers are filled.

       The writev() system call works just like write(2) except that multi‐
       ple buffers are written out.

       Buffers are processed in array order.  This means that readv() com‐
       pletely fills iov[0] before proceeding to iov[1], and so on.  (If
       there is insufficient data, then not all buffers pointed to by iov
       may be filled.)  Similarly, writev() writes out the entire contents
       of iov[0] before proceeding to iov[1], and so on.

       The data transfers performed by readv() and writev() are atomic: the
       data written by writev() is written as a single block that is not in‐
       termingled with output from writes in other processes (but see
       pipe(7) for an exception); analogously, readv() is guaranteed to read
       a contiguous block of data from the file, regardless of read opera‐
       tions performed in other threads or processes that have file descrip‐
       tors referring to the same open file description (see open(2)).

   preadv() and pwritev()
       The preadv() system call combines the functionality of readv() and
       pread(2).  It performs the same task as readv(), but adds a fourth
       argument, offset, which specifies the file offset at which the input
       operation is to be performed.

       The pwritev() system call combines the functionality of writev() and
       pwrite(2).  It performs the same task as writev(), but adds a fourth
       argument, offset, which specifies the file offset at which the output
       operation is to be performed.

       The file offset is not changed by these system calls.  The file re‐
       ferred to by fd must be capable of seeking.

   preadv2() and pwritev2()
       These system calls are similar to preadv() and pwritev() calls, but
       add a fifth argument, flags, which modifies the behavior on a per-
       call basis.

       Unlike preadv() and pwritev(), if the offset argument is -1, then the
       current file offset is used and updated.

       The flags argument contains a bitwise OR of zero or more of the fol‐
       lowing flags:

       RWF_DSYNC (since Linux 4.7)
              Provide a per-write equivalent of the O_DSYNC open(2) flag.
              This flag is meaningful only for pwritev2(), and its effect
              applies only to the data range written by the system call.

       RWF_HIPRI (since Linux 4.6)
              High priority read/write.  Allows block-based filesystems to
              use polling of the device, which provides lower latency, but
              may use additional resources.  (Currently, this feature is us‐
              able only on a file descriptor opened using the O_DIRECT
              flag.)

       RWF_SYNC (since Linux 4.7)
              Provide a per-write equivalent of the O_SYNC open(2) flag.
              This flag is meaningful only for pwritev2(), and its effect
              applies only to the data range written by the system call.

       RWF_NOWAIT (since Linux 4.14)
              Do not wait for data which is not immediately available.  If
              this flag is specified, the preadv2() system call will return
              instantly if it would have to read data from the backing stor‐
              age or wait for a lock.  If some data was successfully read,
              it will return the number of bytes read.  If no bytes were
              read, it will return -1 and set errno to EAGAIN.  Currently,
              this flag is meaningful only for preadv2().

       RWF_APPEND (since Linux 4.16)
              Provide a per-write equivalent of the O_APPEND open(2) flag.
              This flag is meaningful only for pwritev2(), and its effect
              applies only to the data range written by the system call.
              The offset argument does not affect the write operation; the
              data is always appended to the end of the file.  However, if
              the offset argument is -1, the current file offset is updated.
```

### 1.2 同步IO接口的缺陷

上述接口，尽管形式多样，但它们都有一个共同的特征，就是同步，即在读写IO时，系统调用会阻塞住等待，在数据读取或写入后才返回结果。

对于传统、普通的编程模型，这类同步接口编程简单，结果可预测，倒也无妨，但是在要求高效的场景下，同步导致的后果就是caller在阻塞的同时无法继续执行其他的操作，只能等待IO结果返回，其实caller本可以利用这段时间继续往后执行。例如，一个FTP server，接收到客户机上传的文件，然后将文件写入到本机的过程中，若FTP服务程序忙于等待文件读写结果的返回，则会拒绝其他此刻正需要连接的客户机请求。在这种场景下，更好的方式是采用异步编程模型，就上述例子而言，当服务器接收到某个客户机上传文件后，直接、无阻塞地将写入IO的buffer提交给内核，然后caller继续接受下一个客户请求，内核处理完IO之后，主动调用某种通知机制，告诉caller该IO已完成，完成状态保存在某位置。

存储场景中，我们对性能的要求非常高，所以我们需要异步IO。

### 1.3 AIO

后来，应这类诉求，产生了异步IO接口，即Linux Native异步IO——AIO。

AIO接口简单介绍（表格引用自*Understanding Nginx Modules Development and Architecture Resolving(Second Edition)*）：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16GKicX3CuSeI3AKVeqH4VnILf1PJWuHIQMhsovNDRib0bbED8cBDQwFYrg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

类似地，如同前文所提PostgreSQL——历史上，也有一些项目通过使用kernel的新接口，获得了不菲的收益。

例如，高性能服务器nginx就使用了这样的机制，nginx把读取文件的操作异步地提交给内核后，内核会通知IO设备独立地执行操作，这样，nginx进程可以继续充分地占用CPU，而且，当大量读事件堆积到IO设备的队列中时，将会发挥出内核中“电梯算法”的优势，从而降低随机读取磁盘扇区的成本。

### 1.4 AIO的缺陷

但是，AIO仍然不够完美，同样存在很多缺陷，同样以nginx为例，目前，nginx仅支持在读取文件时使用AIO，因为正常写入文件往往是写入内存就立刻返回，效率很高，而如果替换成AIO写入速度会明显下降。

这是因为AIO不支持缓存操作，即使需要操作的文件块在linux文件缓存中存在，也不会通过操作缓存中的文件块来代替实际对磁盘的操作，这可能降低实际处理的性能。需要看具体的使用场景，如果大部分用户请求对文件操作都会落到文件缓存中，那么使用AIO可能不是一个好的选择。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16GkJPRD77tIhdrIRp6DNEqU3uyCv1Dbw9V8dVMBlCfBBJJYoOYA2fcxw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

以上是AIO的不足之一，分析AIO缘何不足，需要较大的篇幅，这里按下不表直接总结一下其他不足之处。

- **仅支持direct IO**。在采用AIO的时候，只能使用O_DIRECT，不能借助文件系统缓存来缓存当前的IO请求，还存在size对齐（直接操作磁盘，所有写入内存块数量必须是文件系统块大小的倍数，而且要与内存页大小对齐。）等限制，这直接影响了aio在很多场景的使用。
- **仍然可能被阻塞。语义不完备**。即使应用层主观上，希望系统层采用异步IO，但是客观上，有时候还是可能会被阻塞。io_getevents(2)调用read_events读取AIO的完成events，read_events中的wait_event_interruptible_hrtimeout等待aio_read_events，如果条件不成立（events未完成）则调用__wait_event_hrtimeout进入睡眠（当然，支持用户态设置最大等待时间）。
- **拷贝开销大**。每个IO提交需要拷贝64+8字节，每个IO完成需要拷贝32字节，总共104字节的拷贝。这个拷贝开销是否可以承受，和单次IO大小有关：如果需要发送的IO本身就很大，相较之下，这点消耗可以忽略，而在大量小IO的场景下，这样的拷贝影响比较大。
- **API不友好**。每一个IO至少需要两次系统调用才能完成（submit和wait-for-completion)，需要非常小心地使用完成事件以避免丢事件。
- **系统调用开销大**。也正是因为上一条，io_submit/io_getevents造成了较大的系统调用开销，在存在spectre/meltdown（CPU熔断幽灵漏洞，CVE-2017-5754）的机器上，若要避免漏洞问题，则系统调用性能会大幅下降。所以在存储场景下，高频系统调用的性能影响较大。

在过去的数年间，针对上述限制的很多改进努力都未尽如人意。

终于，全新的异步IO引擎io_uring就在这样的环境下诞生了。

## 2.**设计——应该是什么样子**

既然是全新实现，我们是否可以不囿于现状，思考它应该是什么样子？

关于“应该是什么样子”，我曾听智超兄说过这样的一句话：“Linux应该是什么样子，它现在就是什么样子。”，这并不是类似于“存在即合理”这样的谬传，而是对Linux系统优雅哲学的高度概括，同时也是对开源自由软件精神的肯定——因为自始至终都是自由的，所以大家觉得应该是什么样子（哪里有缺陷，哪里不够优雅），大家就会自由地去修改它，所以，经过时代的发展，它的面貌与大家所期望的最相符，即众人拾柴，众望所归。

以后世上可能会有无数文章讲述io_uring是什么样子，我们先看看它应该是什么样子。

### 2.1 设计原则

如上所述，历史实现在一定场景下，会有一定问题，新实现理应反思问题、解决问题。与此同时，需要遵循一定设计原则，如下是若干原则。

- 简单：接口需要足够简单，这一点不言自明。
- 易用：同时需要足够克制，保持易于理解，就不容易误用，对于使用者来说，这是一种有效的助推（之所以如上没有采用“简单易用”这样的惯用语，是因为简单并不一定意味着易用。我们尽量避免这种不合逻辑的隐喻）。
- 可扩展：接口要有足够的扩展性，尽管某个接口是为了某种场景（如存储）而建立，但是我们需要面向未来，若有朝一日需要支持非阻塞设备（非块存储）以及网络I/O时，这里不应是桎梏。
- 特性丰富：当然，接口需要支持足够丰富的功能。
- 高效：在存储场景下，高效率始终是关键目标。
- 可伸缩性：满足峰值场景的性能需要（高效和低延迟很重要，但是峰值速率对于存储设备来讲也很重要）底层软件是基于硬件建构的，为了适应新硬件的要求，接口还需要考虑到伸缩性。

另外，因为我们的部分目标之间，本质上往往是存在一定互斥性的（如可伸缩与足够简单互斥、特性丰富与高效互斥）很难同时满足，所以，我们设计时也需要权衡。其中，io_uring始终需要围绕高效进行设计。

### 2.2 实现思路

#### 2.2.1 **解决“系统调用开销大”的问题**

针对这个问题，考虑是否每次都需要系统调用。如果能将多次系统调用中的逻辑放到有限次数中来，就能将消耗降为常数时间复杂度。

#### 2.2.2 **解决“拷贝开销大”的问题**

之所以在提交和完成事件中存在大量的内存拷贝，是因为应用程序和内核之间的通信需要拷贝数据，所以为了避免这个问题，需要重新考量应用与内核间的通信方式。我们发现，两者通信，不是必须要拷贝，通过现有技术，可以让应用与内核共享内存，用于彼此通信，需要生产者-消费者模型。

要实现核外与内核的零拷贝，最佳方式就是实现一块内存映射区域，两者共享一段内存，核外往这段内存写数据，然后通知内核使用这段内存数据，或者内核填写这段数据，核外使用这部分数据。因此，我们需要一对共享的ring buffer用于应用程序和内核之间的通信。

共享ring buffer的设计主要带来以下几个好处：

- 提交、完成请求时节省应用和内核之间的内存拷贝
- 使用SQPOLL高级特性时，应用程序无需调用系统调用
- 无锁操作，用memory ordering实现同步，通过几个简单的头尾指针的移动就可以实现快速交互。

一块用于核外传递数据给内核，一块是内核传递数据给核外，一方只读，一方只写。

- 提交队列SQ(submission queue)中，应用是IO提交的生产者，内核是消费者。
- 完成队列CQ(completion queue)中，内核是IO完成的生产者，应用是消费者。

内核控制SQ ring的head和CQ ring的tail，应用程序控制SQ ring的tail和CQ ring的head

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16GAAbF0n5q3joFXXS6dUonrrXyZVqqEeVfcHwYIZdCia80NLzbMeUia2RQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

那么他们分别需要保存的是什么数据呢？

假设A缓存区为核外写，内核读，就是将IO数据写到这个缓存区，然后通知内核来读；再假设B缓存区为内核写，核外读，他所承担的责任就是返回完成状态，标记A缓存区的其中一个entry的完成状态为成功或者失败等信息。

#### 2.2.3 **解决“API不友好”的问题**

问题在于需要多个系统调用才能完成，考虑是否可以把多个系统调用合而为一。

你可能会想到，这与上文所说的重构手法相悖，即以明确函数取代参数（Replace Parameter with Explicit Methods）——如果某个参数有多种可能的值，而函数内又以条件表达式检查这些参数值，并根据不同参数值做出不同的行为。

然而，手法只是手法，选择具体的重构手法需要遵循重构原则。在不同场景下，可能事实恰恰相反——令函数携带参数（Parameterize Method）可能是一个好的选择。

话说天下大势，分久必合，合久必分。你可能会发现这样的两个函数，它们做着类似的工作，但因少数几个值致使行为略有不同。在这种情况下，你可以将这些各自分离的函数统一起来，并通过参数来处理那些变化情况，用以简化问题。这样的修改可以去除重复代码，并提高灵活性，因为你可以用这个参数处理更多的变化情况。

也许你会发现，你无法用这种办法处理整个函数，但可以处理函数中的一部分代码。这种情况下，你应该首先将这部分代码提炼到一个独立函数中，然后再对那个提炼所得的函数使用令函数携带参数（Parameterize Method）。

2.3 实现——现在是什么样子

推导完了应该是什么样子，解析一下现在是什么样子。

#### 2.2.4 **关键数据结构**

程序等于数据结构加算法，这里先解析io_uring有哪些关键数据结构。

**io_uring、io_rings结构**

结构前面是一些标志位集合和掩码，尾部是一个柔性数组。这两个数据在前面使用mmap分配内存的时候，对应到了不同的offset，即前面IORING_OFF_SQ_RING、IORING_OFF_CQ_RING和IORING_OFF_SQES的预定于的值。

其中io_rings结构中sq, cq成员，分别代表了提交的请求的ring和已经完成的请求返回结构的ring。io_uring结构中是head和tail，用于控制队列中的头尾索引。即前文提到的，内核控制SQ ring的head和CQ ring的tail，应用程序控制SQ ring的tail和CQ ring的head。

```
struct io_uring {
 u32 head ____cacheline_aligned_in_smp;
 u32 tail ____cacheline_aligned_in_smp;
};

/*
 * This data is shared with the application through the mmap at offsets
 * IORING_OFF_SQ_RING and IORING_OFF_CQ_RING.
 *
 * The offsets to the member fields are published through struct
 * io_sqring_offsets when calling io_uring_setup.
 */
struct io_rings {
 /*
  * Head and tail offsets into the ring; the offsets need to be
  * masked to get valid indices.
  *
  * The kernel controls head of the sq ring and the tail of the cq ring,
  * and the application controls tail of the sq ring and the head of the
  * cq ring.
  */
 struct io_uring  sq, cq;
 /*
  * Bitmasks to apply to head and tail offsets (constant, equals
  * ring_entries - 1)
  */
 u32   sq_ring_mask, cq_ring_mask;
 /* Ring sizes (constant, power of 2) */
 u32   sq_ring_entries, cq_ring_entries;
 /*
  * Number of invalid entries dropped by the kernel due to
  * invalid index stored in array
  *
  * Written by the kernel, shouldn't be modified by the
  * application (i.e. get number of "new events" by comparing to
  * cached value).
  *
  * After a new SQ head value was read by the application this
  * counter includes all submissions that were dropped reaching
  * the new SQ head (and possibly more).
  */
 u32   sq_dropped;
 /*
  * Runtime SQ flags
  *
  * Written by the kernel, shouldn't be modified by the
  * application.
  *
  * The application needs a full memory barrier before checking
  * for IORING_SQ_NEED_WAKEUP after updating the sq tail.
  */
 u32   sq_flags;
 /*
  * Runtime CQ flags
  *
  * Written by the application, shouldn't be modified by the
  * kernel.
  */
 u32                     cq_flags;
 /*
  * Number of completion events lost because the queue was full;
  * this should be avoided by the application by making sure
  * there are not more requests pending than there is space in
  * the completion queue.
  *
  * Written by the kernel, shouldn't be modified by the
  * application (i.e. get number of "new events" by comparing to
  * cached value).
  *
  * As completion events come in out of order this counter is not
  * ordered with any other data.
  */
 u32   cq_overflow;
 /*
  * Ring buffer of completion events.
  *
  * The kernel writes completion events fresh every time they are
  * produced, so the application is allowed to modify pending
  * entries.
  */
 struct io_uring_cqe cqes[] ____cacheline_aligned_in_smp;
};
```

**Submission Queue Entry单元数据结构**

Submission Queue（下称SQ）是提交队列，核外写内核读的地方。Submission Queue Entry（下称SQE），即提交队列中的条目，队列由一个个条目组成。

描述一个SQE会复杂很多，不仅是因为要描述更多的信息，也是因为可扩展性这一设计原则。

我们需要操作码、标志集合、关联文件描述符、地址、偏移量，另外地，可能还需要表示优先级。

```
/*
 * IO submission data structure (Submission Queue Entry)
 */
struct io_uring_sqe {
 __u8 opcode;  /* type of operation for this sqe */
 __u8 flags;  /* IOSQE_ flags */
 __u16 ioprio;  /* ioprio for the request */
 __s32 fd;  /* file descriptor to do IO on */
 union {
  __u64 off; /* offset into file */
  __u64 addr2;
 };
 union {
  __u64 addr; /* pointer to buffer or iovecs */
  __u64 splice_off_in;
 };
 __u32 len;  /* buffer size or number of iovecs */
 union {
  __kernel_rwf_t rw_flags;
  __u32  fsync_flags;
  __u16  poll_events; /* compatibility */
  __u32  poll32_events; /* word-reversed for BE */
  __u32  sync_range_flags;
  __u32  msg_flags;
  __u32  timeout_flags;
  __u32  accept_flags;
  __u32  cancel_flags;
  __u32  open_flags;
  __u32  statx_flags;
  __u32  fadvise_advice;
  __u32  splice_flags;
 };
 __u64 user_data; /* data to be passed back at completion time */
 union {
  struct {
   /* pack this to avoid bogus arm OABI complaints */
   union {
    /* index into fixed buffers, if used */
    __u16 buf_index;
    /* for grouped buffer selection */
    __u16 buf_group;
   } __attribute__((packed));
   /* personality to use, if used */
   __u16 personality;
   __s32 splice_fd_in;
  };
  __u64 __pad2[3];
 };
};
```

- opcode是操作码，例如IORING_OP_READV，代表向量读。
- flags是标志位集合。
- ioprio是请求的优先级，对于普通的读写，具体定义可以参照ioprio_set(2)，
- fd是这个请求相关的文件描述符
- off是操作的偏移量
- addr表示这次IO操作执行的地址，如果操作码opcode描述了一个传输数据的操作，这个操作是基于向量的，addr就指向struct iovec的数组首地址，这和前文所说的preadv系统调用是一样的用法；如果不是基于向量的，那么addr必须直接包含一个地址，len这里（非向量场景）就表示这段buffer的长度，而向量场景就表示iovec的数量。
- 接下来的是一个union，表示一系列针对特定操作码opcode的一些flag。例如，对于上文所提的IORING_OP_READV，随后的flags就遵循preadv2系统调用。
- user_data是各操作码opcode通用的，内核并未染指，仅仅只是拷贝给完成事件completion event
- 结构的最后用于内存对齐，对齐到64字节，为了更丰富的特性，未来这个请求结构应该会包含更多的内容。

这就是核外往内核填写的Submission Queue Entry的数据结构，准备好这样的一个数据结构，将它写到对应的sqes所在的内存位置，然后再通知内核去对应的位置取数据，这样就完成了一次数据交接。

**Completion Queue Entry单元数据结构**

Completion Queue（下称CQ）是完成队列，内核写核外读的地方。Completion Queue Entry（下称CQE），即完成队列中的条目，队列由一个个条目组成。

描述一个CQE就简单得多。

```
/*
 * IO completion data structure (Completion Queue Entry)
 */
struct io_uring_cqe {
 __u64 user_data; /* sqe->data submission passed back */
 __s32 res;  /* result code for this event */
 __u32 flags;
};
```

- user_data就是sqe发送时核外填写的，只不过在完成时回传而已，一个常见的用例就是作为一个指针，指向原始请求。从submission queue到completion queue，内核不会动这里面的数据。
- res用来保存最终的这个sqe的执行结果，就是这个event的返回码，可以认为是系统调用的返回值，表示成功或失败等。如果接口成功的话返回传输的字节数，如果失败的话，就是错误码。如果错误发生，res就等于-EIO。
- flags是标志位集合。如果flags设置为IORING_CQE_F_BUFFER，则前16位是buffer ID（调用链：io_uring_enter -> io_iopoll_check -> io_iopoll_getevents -> io_do_iopoll -> io_iopoll_complete -> io_put_rw_kbuf -> io_put_kbuf，最终会调用io_put_kbuf，如代码所示）。

```
/*
 * cqe->flags
 *
 * IORING_CQE_F_BUFFER If set, the upper 16 bits are the buffer ID
 */
#define IORING_CQE_F_BUFFER  (1U << 0)

enum {
 IORING_CQE_BUFFER_SHIFT  = 16,
};
static unsigned int io_put_kbuf(struct io_kiocb *req, struct io_buffer *kbuf)
{
 unsigned int cflags;

 cflags = kbuf->bid << IORING_CQE_BUFFER_SHIFT;
 cflags |= IORING_CQE_F_BUFFER;
 req->flags &= ~REQ_F_BUFFER_SELECTED;
 kfree(kbuf);
 return cflags;
}
```

**上下文结构io_ring_ctx**

前面介绍了SQE/CQE等关键的数据结构，他们是用来承载数据流的关键部分，有了数据流的关键数据结构我们还需要一个上下文数据结构，用于整个io_uring控制流。这就是io_ring_ctx，贯穿整个io_uring所有过程的数据结构，基本上在任何位置只需要你能持有该结构就可以找到任何数据所在的位置，例如，sq_sqes就是指向io_uring_sqe结构的指针，指向SQEs的首地址。

```
struct io_ring_ctx {
 struct {
  struct percpu_ref refs;
 } ____cacheline_aligned_in_smp;

 struct {
  unsigned int  flags;
  unsigned int  compat: 1;
  unsigned int  limit_mem: 1;
  unsigned int  cq_overflow_flushed: 1;
  unsigned int  drain_next: 1;
  unsigned int  eventfd_async: 1;
  unsigned int  restricted: 1;

  /*
   * Ring buffer of indices into array of io_uring_sqe, which is
   * mmapped by the application using the IORING_OFF_SQES offset.
   *
   * This indirection could e.g. be used to assign fixed
   * io_uring_sqe entries to operations and only submit them to
   * the queue when needed.
   *
   * The kernel modifies neither the indices array nor the entries
   * array.
   */
  u32   *sq_array;
  unsigned  cached_sq_head;
  unsigned  sq_entries;
  unsigned  sq_mask;
  unsigned  sq_thread_idle;
  unsigned  cached_sq_dropped;
  unsigned  cached_cq_overflow;
  unsigned long  sq_check_overflow;

  struct list_head defer_list;
  struct list_head timeout_list;
  struct list_head cq_overflow_list;

  wait_queue_head_t inflight_wait;
  struct io_uring_sqe *sq_sqes;
 } ____cacheline_aligned_in_smp;

 struct io_rings *rings;

 /* IO offload */
 struct io_wq  *io_wq;

 /*
  * For SQPOLL usage - we hold a reference to the parent task, so we
  * have access to the ->files
  */
 struct task_struct *sqo_task;

 /* Only used for accounting purposes */
 struct mm_struct *mm_account;

#ifdef CONFIG_BLK_CGROUP
 struct cgroup_subsys_state *sqo_blkcg_css;
#endif

 struct io_sq_data *sq_data; /* if using sq thread polling */

 struct wait_queue_head sqo_sq_wait;
 struct wait_queue_entry sqo_wait_entry;
 struct list_head sqd_list;

 /*
  * If used, fixed file set. Writers must ensure that ->refs is dead,
  * readers must ensure that ->refs is alive as long as the file* is
  * used. Only updated through io_uring_register(2).
  */
 struct fixed_file_data *file_data;
 unsigned  nr_user_files;

 /* if used, fixed mapped user buffers */
 unsigned  nr_user_bufs;
 struct io_mapped_ubuf *user_bufs;

 struct user_struct *user;

 const struct cred *creds;

#ifdef CONFIG_AUDIT
 kuid_t   loginuid;
 unsigned int  sessionid;
#endif

 struct completion ref_comp;
 struct completion sq_thread_comp;

 /* if all else fails... */
 struct io_kiocb  *fallback_req;

#if defined(CONFIG_UNIX)
 struct socket  *ring_sock;
#endif

 struct idr  io_buffer_idr;

 struct idr  personality_idr;

 struct {
  unsigned  cached_cq_tail;
  unsigned  cq_entries;
  unsigned  cq_mask;
  atomic_t  cq_timeouts;
  unsigned long  cq_check_overflow;
  struct wait_queue_head cq_wait;
  struct fasync_struct *cq_fasync;
  struct eventfd_ctx *cq_ev_fd;
 } ____cacheline_aligned_in_smp;

 struct {
  struct mutex  uring_lock;
  wait_queue_head_t wait;
 } ____cacheline_aligned_in_smp;

 struct {
  spinlock_t  completion_lock;

  /*
   * ->iopoll_list is protected by the ctx->uring_lock for
   * io_uring instances that don't use IORING_SETUP_SQPOLL.
   * For SQPOLL, only the single threaded io_sq_thread() will
   * manipulate the list, hence no extra locking is needed there.
   */
  struct list_head iopoll_list;
  struct hlist_head *cancel_hash;
  unsigned  cancel_hash_bits;
  bool   poll_multi_file;

  spinlock_t  inflight_lock;
  struct list_head inflight_list;
 } ____cacheline_aligned_in_smp;

 struct delayed_work  file_put_work;
 struct llist_head  file_put_llist;

 struct work_struct  exit_work;
 struct io_restriction  restrictions;
};
```

#### **2.2.5  关键流程**

数据结构定义好了，逻辑实现具体是如何驱动这些数据结构的呢？使用上，大体分为准备、提交、收割过程。

有几个io_uring相关的系统调用：

```
#include <linux/io_uring.h>

int io_uring_setup(u32 entries, struct io_uring_params *p);

int io_uring_enter(unsigned int fd, unsigned int to_submit,
                   unsigned int min_complete, unsigned int flags,
                   sigset_t *sig);
                   
int io_uring_register(unsigned int fd, unsigned int opcode,
                      void *arg, unsigned int nr_args);
```

下面分析关键流程。

**io_uring准备阶段**

io_uring通过io_uring_setup完成准备阶段。

```
int io_uring_setup(u32 entries, struct io_uring_params *p);
```

io_uring_setup系统调用的过程就是初始化相关数据结构，建立好对应的缓存区，然后通过系统调用的参数io_uring_params结构传递回去，告诉核外环内存地址在哪，起始指针的地址在哪等关键的信息。

需要初始化内存的内存分为三个区域，分别是SQ，CQ，SQEs。内核初始化SQ和CQ，此外，提交请求在SQ，CQ之间有一个间接数组，即内核提供了一个Submission Queue Entries（SQEs）数组。之所以额外采用了一个数组保存SQEs，是为了方便通过环形缓冲区提交内存上不连续的请求。SQ和CQ中每个节点保存的都是SQEs数组的索引，而不是实际的请求，实际的请求只保存在SQEs数组中。这样在提交请求时，就可以批量提交一组SQEs上不连续的请求。

通常，SQE被独立地使用，意味着它的执行不影响在ring中的连续SQE条目。它允许全面、灵活的操作，并且使它们最高性能地并行执行完成。一个顺序的使用案例就是数据的整体写入。它的一个通常的例子就是一系列写，随之的是fsync/fdatasync，应用通常转变成程序同步-等待操作。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16GCgEc4Twct3f5E0OaWI2gCwjc00chtKHu8JrsW8YYfkIvclSNUKvQeA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

先从参数上来解析

- 核外需要告诉io_uring_setup提交的整个缓存区数组的大小。（代表 queue depth？），这里就是entries参数。

- params这个参数从IO的角度看有两种，一种是输入参数，一种是输出参数。

- - sq_entries是输出参数，由内核填充，让应用程序知道这个ring支持多少SQE。
  - 类似地，cq_entries告诉应用程序，CQ ring有多大。
  - sq_off和cq_off分别是io_sqring_offsets和io_cqring_offsets结构，是内核与核外的约定，分别描述了SQ和CQ的指针在mmap中的offset
  - 其他的结构成员涉及到高级用法，暂时按下不表。
  - 比如params->flags，这个成员变量是用来设置当前整个io_uring 的标志的，它将决定是否启动sq_thread，是否采用iopoll模式等等
  - sq_thread_cpu、sq_thread_idle也由用户设置，用来指定io_sq_thread内核线程CPU、idle时间。
  - 一部分属于输入参数，是用户设置、核外传递给核外的，用于定义io_uring在内核中的行为，这些都是在创建阶段就决定了的。
  - 还有一部分属于输出参数，由内核设置（io_uring_create）、传递数据到核外的，核外根据这些数据来使用mmap分配内存，初始化一些数据结构。

```
/*
 * Passed in for io_uring_setup(2). Copied back with updated info on success
 */
struct io_uring_params {
 __u32 sq_entries;
 __u32 cq_entries;
 __u32 flags;
 __u32 sq_thread_cpu;
 __u32 sq_thread_idle;
 __u32 features;
 __u32 wq_fd;
 __u32 resv[3];
 struct io_sqring_offsets sq_off;
 struct io_cqring_offsets cq_off;
};

/*
 * io_uring_params->features flags
 */
#define IORING_FEAT_SINGLE_MMAP  (1U << 0)
#define IORING_FEAT_NODROP  (1U << 1)
#define IORING_FEAT_SUBMIT_STABLE (1U << 2)
#define IORING_FEAT_RW_CUR_POS  (1U << 3)
#define IORING_FEAT_CUR_PERSONALITY (1U << 4)
#define IORING_FEAT_FAST_POLL  (1U << 5)
#define IORING_FEAT_POLL_32BITS  (1U << 6)
```

再从实现上来解析，如下为io_uring_setup代码。

```
/*
 * Sets up an aio uring context, and returns the fd. Applications asks for a
 * ring size, we return the actual sq/cq ring sizes (among other things) in the
 * params structure passed in.
 */
static long io_uring_setup(u32 entries, struct io_uring_params __user *params)
{
 struct io_uring_params p;
 int i;

 if (copy_from_user(&p, params, sizeof(p)))
  return -EFAULT;
 for (i = 0; i < ARRAY_SIZE(p.resv); i++) {
  if (p.resv[i])
   return -EINVAL;
 }

 if (p.flags & ~(IORING_SETUP_IOPOLL | IORING_SETUP_SQPOLL |
   IORING_SETUP_SQ_AFF | IORING_SETUP_CQSIZE |
   IORING_SETUP_CLAMP | IORING_SETUP_ATTACH_WQ |
   IORING_SETUP_R_DISABLED))
  return -EINVAL;

 return  io_uring_create(entries, &p, params);
}
```

经过标志位非法检查之后，关键是调用内部函数io_uring_create实现实例创建过程。

- 首先需要创建一个上下文结构io_ring_ctx用来管理整个会话。
- 随后实现SQ和CQ内存区的映射，使用IORING_OFF_CQ_RING偏移量，使用io_cqring_offsets结构的实例，即io_uring_params中cq_off这个成员，SQ使用IORING_OFF_SQES这个偏移量。
- 其余的是一些错误检查、权限检查、资源配额检查等检查逻辑。

```
static int io_uring_create(unsigned entriesstatic int io_uring_create(unsigned entries, struct io_uring_params *p,
      struct io_uring_params __user *params)
{
 struct user_struct *user = NULL;
 struct io_ring_ctx *ctx;
 bool limit_mem;
 int ret;

 if (!entries)
  return -EINVAL;
 if (entries > IORING_MAX_ENTRIES) {
  if (!(p->flags & IORING_SETUP_CLAMP))
   return -EINVAL;
  entries = IORING_MAX_ENTRIES;
 }

 /*
  * Use twice as many entries for the CQ ring. It's possible for the
  * application to drive a higher depth than the size of the SQ ring,
  * since the sqes are only used at submission time. This allows for
  * some flexibility in overcommitting a bit. If the application has
  * set IORING_SETUP_CQSIZE, it will have passed in the desired number
  * of CQ ring entries manually.
  */
 p->sq_entries = roundup_pow_of_two(entries);
 if (p->flags & IORING_SETUP_CQSIZE) {
  /*
   * If IORING_SETUP_CQSIZE is set, we do the same roundup
   * to a power-of-two, if it isn't already. We do NOT impose
   * any cq vs sq ring sizing.
   */
  if (!p->cq_entries)
   return -EINVAL;
  if (p->cq_entries > IORING_MAX_CQ_ENTRIES) {
   if (!(p->flags & IORING_SETUP_CLAMP))
    return -EINVAL;
   p->cq_entries = IORING_MAX_CQ_ENTRIES;
  }
  p->cq_entries = roundup_pow_of_two(p->cq_entries);
  if (p->cq_entries < p->sq_entries)
   return -EINVAL;
 } else {
  p->cq_entries = 2 * p->sq_entries;
 }

 user = get_uid(current_user());
 limit_mem = !capable(CAP_IPC_LOCK);

 if (limit_mem) {
  ret = __io_account_mem(user,
    ring_pages(p->sq_entries, p->cq_entries));
  if (ret) {
   free_uid(user);
   return ret;
  }
 }

 ctx = io_ring_ctx_alloc(p);
 if (!ctx) {
  if (limit_mem)
   __io_unaccount_mem(user, ring_pages(p->sq_entries,
        p->cq_entries));
  free_uid(user);
  return -ENOMEM;
 }
 ctx->compat = in_compat_syscall();
 ctx->user = user;
 ctx->creds = get_current_cred();
#ifdef CONFIG_AUDIT
 ctx->loginuid = current->loginuid;
 ctx->sessionid = current->sessionid;
#endif
 ctx->sqo_task = get_task_struct(current);

 /*
  * This is just grabbed for accounting purposes. When a process exits,
  * the mm is exited and dropped before the files, hence we need to hang
  * on to this mm purely for the purposes of being able to unaccount
  * memory (locked/pinned vm). It's not used for anything else.
  */
 mmgrab(current->mm);
 ctx->mm_account = current->mm;

#ifdef CONFIG_BLK_CGROUP
 /*
  * The sq thread will belong to the original cgroup it was inited in.
  * If the cgroup goes offline (e.g. disabling the io controller), then
  * issued bios will be associated with the closest cgroup later in the
  * block layer.
  */
 rcu_read_lock();
 ctx->sqo_blkcg_css = blkcg_css();
 ret = css_tryget_online(ctx->sqo_blkcg_css);
 rcu_read_unlock();
 if (!ret) {
  /* don't init against a dying cgroup, have the user try again */
  ctx->sqo_blkcg_css = NULL;
  ret = -ENODEV;
  goto err;
 }
#endif

 /*
  * Account memory _before_ installing the file descriptor. Once
  * the descriptor is installed, it can get closed at any time. Also
  * do this before hitting the general error path, as ring freeing
  * will un-account as well.
  */
 io_account_mem(ctx, ring_pages(p->sq_entries, p->cq_entries),
         ACCT_LOCKED);
 ctx->limit_mem = limit_mem;

 ret = io_allocate_scq_urings(ctx, p);
 if (ret)
  goto err;

 ret = io_sq_offload_create(ctx, p);
 if (ret)
  goto err;

 if (!(p->flags & IORING_SETUP_R_DISABLED))
  io_sq_offload_start(ctx);

 memset(&p->sq_off, 0, sizeof(p->sq_off));
 p->sq_off.head = offsetof(struct io_rings, sq.head);
 p->sq_off.tail = offsetof(struct io_rings, sq.tail);
 p->sq_off.ring_mask = offsetof(struct io_rings, sq_ring_mask);
 p->sq_off.ring_entries = offsetof(struct io_rings, sq_ring_entries);
 p->sq_off.flags = offsetof(struct io_rings, sq_flags);
 p->sq_off.dropped = offsetof(struct io_rings, sq_dropped);
 p->sq_off.array = (char *)ctx->sq_array - (char *)ctx->rings;

 memset(&p->cq_off, 0, sizeof(p->cq_off));
 p->cq_off.head = offsetof(struct io_rings, cq.head);
 p->cq_off.tail = offsetof(struct io_rings, cq.tail);
 p->cq_off.ring_mask = offsetof(struct io_rings, cq_ring_mask);
 p->cq_off.ring_entries = offsetof(struct io_rings, cq_ring_entries);
 p->cq_off.overflow = offsetof(struct io_rings, cq_overflow);
 p->cq_off.cqes = offsetof(struct io_rings, cqes);
 p->cq_off.flags = offsetof(struct io_rings, cq_flags);

 p->features = IORING_FEAT_SINGLE_MMAP | IORING_FEAT_NODROP |
   IORING_FEAT_SUBMIT_STABLE | IORING_FEAT_RW_CUR_POS |
   IORING_FEAT_CUR_PERSONALITY | IORING_FEAT_FAST_POLL |
   IORING_FEAT_POLL_32BITS;

 if (copy_to_user(params, p, sizeof(*p))) {
  ret = -EFAULT;
  goto err;
 }

 /*
  * Install ring fd as the very last thing, so we don't risk someone
  * having closed it before we finish setup
  */
 ret = io_uring_get_fd(ctx);
 if (ret < 0)
  goto err;

 trace_io_uring_create(ret, ctx, p->sq_entries, p->cq_entries, p->flags);
 return ret;
err:
 io_ring_ctx_wait_and_kill(ctx);
 return ret;
}
```

io_sqring_offsets、io_cqring_offsets等相关结构、标志位集合。

预定义offset
如果要表征分配的是io uring相关的一些内存，就需要预定义一些offset，如IORING_OFF_SQ_RING、IORING_OFF_SQES和IORING_OFF_CQ_RING，这些offset值定义了保存到这个三个结构保存到位置。这里mmap的时候，就使用了这些offset。

```
/*
 * Magic offsets for the application to mmap the data it needs
 */
#define IORING_OFF_SQ_RING  0ULL
#define IORING_OFF_CQ_RING  0x8000000ULL
#define IORING_OFF_SQES   0x10000000ULL

/*
 * Filled with the offset for mmap(2)
 */
struct io_sqring_offsets {
 __u32 head;
 __u32 tail;
 __u32 ring_mask;
 __u32 ring_entries;
 __u32 flags;
 __u32 dropped;
 __u32 array;
 __u32 resv1;
 __u64 resv2;
};

/*
 * sq_ring->flags
 */
#define IORING_SQ_NEED_WAKEUP (1U << 0) /* needs io_uring_enter wakeup */
#define IORING_SQ_CQ_OVERFLOW (1U << 1) /* CQ ring is overflown */

struct io_cqring_offsets {
 __u32 head;
 __u32 tail;
 __u32 ring_mask;
 __u32 ring_entries;
 __u32 overflow;
 __u32 cqes;
 __u32 flags;
 __u32 resv1;
 __u64 resv2;
};

/*
 * cq_ring->flags
 */

/* disable eventfd notifications */
#define IORING_CQ_EVENTFD_DISABLED (1U << 0)

/*
 * io_uring_enter(2) flags
 */
#define IORING_ENTER_GETEVENTS (1U << 0)
#define IORING_ENTER_SQ_WAKEUP (1U << 1)
#define IORING_ENTER_SQ_WAIT (1U << 2)

/*
 * io_uring_register(2) opcodes and arguments
 */
enum {
 IORING_REGISTER_BUFFERS   = 0,
 IORING_UNREGISTER_BUFFERS  = 1,
 IORING_REGISTER_FILES   = 2,
 IORING_UNREGISTER_FILES   = 3,
 IORING_REGISTER_EVENTFD   = 4,
 IORING_UNREGISTER_EVENTFD  = 5,
 IORING_REGISTER_FILES_UPDATE  = 6,
 IORING_REGISTER_EVENTFD_ASYNC  = 7,
 IORING_REGISTER_PROBE   = 8,
 IORING_REGISTER_PERSONALITY  = 9,
 IORING_UNREGISTER_PERSONALITY  = 10,
 IORING_REGISTER_RESTRICTIONS  = 11,
 IORING_REGISTER_ENABLE_RINGS  = 12,

 /* this goes last */
 IORING_REGISTER_LAST
};
```

具体的实践，可以参考如下liburing中的初始化函数io_uring_queue_init中对io_uring_setup的使用（http://git.kernel.dk/cgit/liburing/tree/src/setup.c）。

liburing中使用io_uring_setup的部分代码

```
/*
 * Returns -1 on error, or zero on success. On success, 'ring'
 * contains the necessary information to read/write to the rings.
 */
int io_uring_queue_init(unsigned entries, struct io_uring *ring, unsigned flags)
{
 struct io_uring_params p;
 int fd, ret;

 memset(&p, 0, sizeof(p));
 p.flags = flags;

 fd = io_uring_setup(entries, &p);
 if (fd < 0)
  return fd;

 ret = io_uring_queue_mmap(fd, &p, ring);
 if (ret)
  close(fd);

 return ret;
}
```

注意mmap的时候需要传入MAP_POPULATE参数，为文件映射通过预读的方式准备好页表，随后对映射区的访问不会被page fault。

**IO提交**

在初始化完成之后，应用程序就可以使用这些队列来添加IO请求，即填充SQE。当请求都加入SQ后，应用程序还需要某种方式告诉内核，生产的请求待消费，这就是提交IO请求，可以通过io_uring_enter系统调用。

```
int io_uring_enter(unsigned int fd, unsigned int to_submit,
                   unsigned int min_complete, unsigned int flags,
                   sigset_t *sig);
```

内核将SQ中的请求提交给Block层。这个系统调用既能提交，也能等待。

具体的实现是找到一个空闲的SQE，根据请求设置SQE，并将这个SQE的索引放到SQ中。SQ是一个典型的ring buffer，有head，tail两个成员，如果head == tail，意味着队列为空。SQE设置完成后，需要修改SQ的tail，以表示向ring buffer中插入了一个请求。

先从参数上来解析

- fd即由io_uring_setup(2)返回的文件描述符，

- to_submit告诉内核待消费和提交的SQE的数量，表示一次提交多少个 IO，

- min_complete请求完成请求的个数。

- flags是修饰接口行为的标志集合，这里主要例举两个flags

- - 如果在io_uring_setup的时候flag设置了IORING_SETUP_SQPOLL，内核会额外启动一个特定的内核线程来执行轮询的操作，称作SQ线程，这里使用的轮询结构会最终对应到struct file_operations中的iopoll操作，这个操作作为一个新的接口在最近才添加到这里，Linux native aio的新功能也使用了这个iopoll。这里io _uring实际上只有vfs层的改动，其它的都是使用已经存在的东西，而且几个核心的东西和aio使用的相同/类似。直接通过访问相关的队列就可以获取到执行完的任务，不需要经过系统调用。关于这个线程，通过io_uring_params结构中的sq_thread_cpu配置，这个内核线程可以运行在某个指定的 CPU核心 上。这个内核线程会不停的 Poll SQ，直到在通过sq_thread_idle配置的时间内没有Poll到任何请求为止。
  - 如果flags中设置了IORING_ENTER_GETEVENTS，并且min_complete > 0，这个系统调用会一直 block，直到 min_complete 个 IO 已经完成才返回。这个系统调用会同时处理 IO 收割。
  - 另外的，IORING_SQ_NEED_WAKEUP可以表示在一些时候唤醒休眠中的轮询线程。

static int io_sq_thread(void *data)即内核轮询线程。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16GgXCtyc2gibXmpBJAjCiaRfDI0XHanoP7RK9yGsk72TF2kWlzfm3Dr3lg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

同样地，可以用这个系统调用等待完成。除非应用程序，内核会直接修改CQ，因此调用io_uring_enter系统调用时不必使用IORING_ENTER_GETEVENTS，完成就可以被应用程序消费。

io_uring提供了submission offload模式，使得提交过程完全不需要进行系统调用。当程序在用户态设置完SQE，并通过修改SQ的tail完成一次插入时，如果此时SQ线程处于唤醒状态，那么可以立刻捕获到这次提交，这样就避免了用户程序调用io_uring_enter。如上所说，如果SQ线程处于休眠状态，则需要通过使用IORING_SQ_NEED_WAKEUP标志位调用io_uring_enter来唤醒SQ线程。

以io_iopoll_check为例，正常情况下执行路线是io_iopoll_check -> io_iopoll_getevents -> io_do_iopoll -> (kiocb->ki_filp->f_op->iopoll). 在完成请求的操作之后，会调用下面这个函数提交结果到cqe数组中，这样应用就能看到结果了。这里的io_cqring_fill_event就是获取一个目前可以写入到cqe，写入数据。这里最终调用的会是io_get_cqring，可以见就是返回目前tail的后面的一个。

更详细的内容可以直接参考io_uring_enter(2)的man page。

内核中io_uring_enter的相关代码如下。

```
SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
  u32, min_complete, u32, flags, const sigset_t __user *, sig,
  size_t, sigsz)
{
 struct io_ring_ctx *ctx;
 long ret = -EBADF;
 int submitted = 0;
 struct fd f;

 io_run_task_work();

 if (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |
   IORING_ENTER_SQ_WAIT))
  return -EINVAL;

 f = fdget(fd);
 if (!f.file)
  return -EBADF;

 ret = -EOPNOTSUPP;
 if (f.file->f_op != &io_uring_fops)
  goto out_fput;

 ret = -ENXIO;
 ctx = f.file->private_data;
 if (!percpu_ref_tryget(&ctx->refs))
  goto out_fput;

 ret = -EBADFD;
 if (ctx->flags & IORING_SETUP_R_DISABLED)
  goto out;

 /*
  * For SQ polling, the thread will do all submissions and completions.
  * Just return the requested submit count, and wake the thread if
  * we were asked to.
  */
 ret = 0;
 if (ctx->flags & IORING_SETUP_SQPOLL) {
  if (!list_empty_careful(&ctx->cq_overflow_list))
   io_cqring_overflow_flush(ctx, false, NULL, NULL);
  if (flags & IORING_ENTER_SQ_WAKEUP)
   wake_up(&ctx->sq_data->wait);
  if (flags & IORING_ENTER_SQ_WAIT)
   io_sqpoll_wait_sq(ctx);
  submitted = to_submit;
 } else if (to_submit) {
  ret = io_uring_add_task_file(ctx, f.file);
  if (unlikely(ret))
   goto out;
  mutex_lock(&ctx->uring_lock);
  submitted = io_submit_sqes(ctx, to_submit);
  mutex_unlock(&ctx->uring_lock);

  if (submitted != to_submit)
   goto out;
 }
 if (flags & IORING_ENTER_GETEVENTS) {
  min_complete = min(min_complete, ctx->cq_entries);

  /*
   * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user
   * space applications don't need to do io completion events
   * polling again, they can rely on io_sq_thread to do polling
   * work, which can reduce cpu usage and uring_lock contention.
   */
  if (ctx->flags & IORING_SETUP_IOPOLL &&
      !(ctx->flags & IORING_SETUP_SQPOLL)) {
   ret = io_iopoll_check(ctx, min_complete);
  } else {
   ret = io_cqring_wait(ctx, min_complete, sig, sigsz);
  }
 }

out:
 percpu_ref_put(&ctx->refs);
out_fput:
 fdput(f);
 return submitted ? submitted : ret;
}
```

io_iopoll_complete实现

```
/*
 * Find and free completed poll iocbs
 */
static void io_iopoll_complete(struct io_ring_ctx *ctx, unsigned int *nr_events,
          struct list_head *done)
{
 struct req_batch rb;
 struct io_kiocb *req;
 LIST_HEAD(again);

 /* order with ->result store in io_complete_rw_iopoll() */
 smp_rmb();

 io_init_req_batch(&rb);
 while (!list_empty(done)) {
  int cflags = 0;

  req = list_first_entry(done, struct io_kiocb, inflight_entry);
  if (READ_ONCE(req->result) == -EAGAIN) {
   req->result = 0;
   req->iopoll_completed = 0;
   list_move_tail(&req->inflight_entry, &again);
   continue;
  }
  list_del(&req->inflight_entry);

  if (req->flags & REQ_F_BUFFER_SELECTED)
   cflags = io_put_rw_kbuf(req);

  __io_cqring_fill_event(req, req->result, cflags);
  (*nr_events)++;

  if (refcount_dec_and_test(&req->refs))
   io_req_free_batch(&rb, req);
 }

 io_commit_cqring(ctx);
 if (ctx->flags & IORING_SETUP_SQPOLL)
  io_cqring_ev_posted(ctx);
 io_req_free_batch_finish(ctx, &rb);

 if (!list_empty(&again))
  io_iopoll_queue(&again);
}
```

io_get_cqring实现

```
static struct io_uring_cqe *io_get_cqring(struct io_ring_ctx *ctx)
{
 struct io_rings *rings = ctx->rings;
 unsigned tail;

 tail = ctx->cached_cq_tail;
 /*
  * writes to the cq entry need to come after reading head; the
  * control dependency is enough as we're using WRITE_ONCE to
  * fill the cq entry
  */
 if (tail - READ_ONCE(rings->cq.head) == rings->cq_ring_entries)
  return NULL;

 ctx->cached_cq_tail++;
 return &rings->cqes[tail & ctx->cq_mask];
}
```

**IO收割**

来都来了，搞点事情吧，在我们提交IO的同时，使用同一个io_uring_enter系统调用就可以回收完成状态，这样的好处就是一次系统调用接口就完成了原本需要两次系统调用的工作，大大的减少了系统调用的次数，也就是减少了内核核外的切换，这是一个很明显的优化，内核与核外的切换极其耗时。

当IO完成时，内核负责将完成IO在SQEs中的index放到CQ中。由于IO在提交的时候可以顺便返回完成的IO，所以收割IO不需要额外系统调用。

如果使用了IORING_SETUP_SQPOLL参数，IO收割也不需要系统调用的参与。由于内核和用户态共享内存，所以收割的时候，用户态遍历[cring->head, cring->tail)区间，即已经完成的IO队列，然后找到相应的CQE并进行处理，最后移动head指针到tail，IO收割至此而终。

所以，在最理想的情况下，IO提交和收割都不需要使用系统调用。

### 2.3 高级特性

此外，我们可以使用一些优化思想，进行更进一步的优化，这些优化，以一种可选的方式成为io_uring的其它一些高级特性。

## 3.**Fixed Files模式**

### 3.1 优化思想

非关键逻辑上提至循环外，简化关键路径。

### 3.2 优化实现

可以调用io_uring_register系统调用，使用IORING_REGISTER_FILES操作码，将一组file注册到内核，最终调用io_sqe_files_register，这样内核在注册阶段就批量完成文件的一些基本操作（对于这组文件填充相应的数据结构fixed_file_data，其中fixed_file_table是维护的file表。内核态下，如何获得文件描述符获取相关的信息呢，就需要通过fget，根据fd号获得指向文件的struct file），之后的再次批量IO时就不需要重复地进行此类基本信息设置（更具体地，例如对文件进行fget/fput操作）。如果需要进行IO操作的文件相对固定（比如数据库日志），这会节省一定量的IO时间。

### 3.3 fixed_file_data结构

```
struct fixed_file_data {
 struct fixed_file_table  *table;
 struct io_ring_ctx  *ctx;

 struct fixed_file_ref_node *node;
 struct percpu_ref  refs;
 struct completion  done;
 struct list_head  ref_list;
 spinlock_t   lock;
};
```

### 3.4 io_sqe_files_register实现Fixed Files操作

```
static int io_sqe_files_register(struct io_ring_ctx *ctx, void __user *arg,
     unsigned nr_args)
{
 __s32 __user *fds = (__s32 __user *) arg;
 unsigned nr_tables, i;
 struct file *file;
 int fd, ret = -ENOMEM;
 struct fixed_file_ref_node *ref_node;
 struct fixed_file_data *file_data;

 if (ctx->file_data)
  return -EBUSY;
 if (!nr_args)
  return -EINVAL;
 if (nr_args > IORING_MAX_FIXED_FILES)
  return -EMFILE;

 file_data = kzalloc(sizeof(*ctx->file_data), GFP_KERNEL);
 if (!file_data)
  return -ENOMEM;
 file_data->ctx = ctx;
 init_completion(&file_data->done);
 INIT_LIST_HEAD(&file_data->ref_list);
 spin_lock_init(&file_data->lock);

 nr_tables = DIV_ROUND_UP(nr_args, IORING_MAX_FILES_TABLE);
 file_data->table = kcalloc(nr_tables, sizeof(*file_data->table),
       GFP_KERNEL);
 if (!file_data->table)
  goto out_free;

 if (percpu_ref_init(&file_data->refs, io_file_ref_kill,
    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL))
  goto out_free;

 if (io_sqe_alloc_file_tables(file_data, nr_tables, nr_args))
  goto out_ref;
 ctx->file_data = file_data;

 for (i = 0; i < nr_args; i++, ctx->nr_user_files++) {
  struct fixed_file_table *table;
  unsigned index;

  if (copy_from_user(&fd, &fds[i], sizeof(fd))) {
   ret = -EFAULT;
   goto out_fput;
  }
  /* allow sparse sets */
  if (fd == -1)
   continue;

  file = fget(fd);
  ret = -EBADF;
  if (!file)
   goto out_fput;

  /*
   * Don't allow io_uring instances to be registered. If UNIX
   * isn't enabled, then this causes a reference cycle and this
   * instance can never get freed. If UNIX is enabled we'll
   * handle it just fine, but there's still no point in allowing
   * a ring fd as it doesn't support regular read/write anyway.
   */
  if (file->f_op == &io_uring_fops) {
   fput(file);
   goto out_fput;
  }
  table = &file_data->table[i >> IORING_FILE_TABLE_SHIFT];
  index = i & IORING_FILE_TABLE_MASK;
  table->files[index] = file;
 }

 ret = io_sqe_files_scm(ctx);
 if (ret) {
  io_sqe_files_unregister(ctx);
  return ret;
 }

 ref_node = alloc_fixed_file_ref_node(ctx);
 if (IS_ERR(ref_node)) {
  io_sqe_files_unregister(ctx);
  return PTR_ERR(ref_node);
 }

 file_data->node = ref_node;
 spin_lock(&file_data->lock);
 list_add_tail(&ref_node->node, &file_data->ref_list);
 spin_unlock(&file_data->lock);
 percpu_ref_get(&file_data->refs);
 return ret;
out_fput:
 for (i = 0; i < ctx->nr_user_files; i++) {
  file = io_file_from_index(ctx, i);
  if (file)
   fput(file);
 }
 for (i = 0; i < nr_tables; i++)
  kfree(file_data->table[i].files);
 ctx->nr_user_files = 0;
out_ref:
 percpu_ref_exit(&file_data->refs);
out_free:
 kfree(file_data->table);
 kfree(file_data);
 ctx->file_data = NULL;
 return ret;
}
```

## **4.Fixed Buffers模式**

### 4.1 优化思想

优化思想也是将非关键逻辑上提至循环外，简化关键路径。

### 4.2 优化实现

如果应用提交到内核的虚拟内存地址是固定的，那么可以提前完成虚拟地址到物理pages的映射，将这个并不是每次都要做的非关键路径从关键的IO 路径中剥离，避免每次I/O都进行转换，从而优化性能。可以在io_uring_setup之后，调用io_uring_register，使用IORING_REGISTER_BUFFERS 操作码，将一组buffer注册到内核（参数是一个指向iovec的数组，表示这些地址需要map到内核），最终调用io_sqe_buffer_register，这样内核在注册阶段就批量完成buffer的一些基本操作（减小get_user_pages、put_page开销，提前使用get_user_pages来获得userspace虚拟地址对应的物理pages，初始化在io_ring_ctx上下文中用于管理用户态buffer的io_mapped_ubuf数据结构，map/unmap，传递IOV的地址和长度等），之后的再次批量IO时就不需要重复地进行此类内存拷贝和基础信息检测。

在操作IO的时，如果需要进行IO操作的buffer相对固定，提交的虚拟地址曾经被注册过，那么可以使用带FIXED系列的opcode（IORING_OP_READ_FIXED/IORING_OP_WRITE_FIXED）IO，可以看到底层调用链：io_issue_sqe->io_read->io_import_iovec->__io_import_iovec->io_import_fixed，会直接使用已经完成的“成果”，如此就免去了虚拟地址到pages的转换，这会节省一定量的IO时间。

#### 4.2.1 **io_mapped_ubuf结构**

```
struct io_mapped_ubuf {
 u64  ubuf;
 size_t  len;
 struct  bio_vec *bvec;
 unsigned int nr_bvecs;
 unsigned long acct_pages;
};
```

#### **4.2.2 io_sqe_buffer_register实现Fixed Buffers操作**

```
static int io_sqe_buffer_register(struct io_ring_ctx *ctx, void __user *arg,
      unsigned nr_args)
{
 struct vm_area_struct **vmas = NULL;
 struct page **pages = NULL;
 struct page *last_hpage = NULL;
 int i, j, got_pages = 0;
 int ret = -EINVAL;

 if (ctx->user_bufs)
  return -EBUSY;
 if (!nr_args || nr_args > UIO_MAXIOV)
  return -EINVAL;

 ctx->user_bufs = kcalloc(nr_args, sizeof(struct io_mapped_ubuf),
     GFP_KERNEL);
 if (!ctx->user_bufs)
  return -ENOMEM;

 for (i = 0; i < nr_args; i++) {
  struct io_mapped_ubuf *imu = &ctx->user_bufs[i];
  unsigned long off, start, end, ubuf;
  int pret, nr_pages;
  struct iovec iov;
  size_t size;

  ret = io_copy_iov(ctx, &iov, arg, i);
  if (ret)
   goto err;

  /*
   * Don't impose further limits on the size and buffer
   * constraints here, we'll -EINVAL later when IO is
   * submitted if they are wrong.
   */
  ret = -EFAULT;
  if (!iov.iov_base || !iov.iov_len)
   goto err;

  /* arbitrary limit, but we need something */
  if (iov.iov_len > SZ_1G)
   goto err;

  ubuf = (unsigned long) iov.iov_base;
  end = (ubuf + iov.iov_len + PAGE_SIZE - 1) >> PAGE_SHIFT;
  start = ubuf >> PAGE_SHIFT;
  nr_pages = end - start;

  ret = 0;
  if (!pages || nr_pages > got_pages) {
   kvfree(vmas);
   kvfree(pages);
   pages = kvmalloc_array(nr_pages, sizeof(struct page *),
      GFP_KERNEL);
   vmas = kvmalloc_array(nr_pages,
     sizeof(struct vm_area_struct *),
     GFP_KERNEL);
   if (!pages || !vmas) {
    ret = -ENOMEM;
    goto err;
   }
   got_pages = nr_pages;
  }

  imu->bvec = kvmalloc_array(nr_pages, sizeof(struct bio_vec),
      GFP_KERNEL);
  ret = -ENOMEM;
  if (!imu->bvec)
   goto err;

  ret = 0;
  mmap_read_lock(current->mm);
  pret = pin_user_pages(ubuf, nr_pages,
          FOLL_WRITE | FOLL_LONGTERM,
          pages, vmas);
  if (pret == nr_pages) {
   /* don't support file backed memory */
   for (j = 0; j < nr_pages; j++) {
    struct vm_area_struct *vma = vmas[j];

    if (vma->vm_file &&
        !is_file_hugepages(vma->vm_file)) {
     ret = -EOPNOTSUPP;
     break;
    }
   }
  } else {
   ret = pret < 0 ? pret : -EFAULT;
  }
  mmap_read_unlock(current->mm);
  if (ret) {
   /*
    * if we did partial map, or found file backed vmas,
    * release any pages we did get
    */
   if (pret > 0)
    unpin_user_pages(pages, pret);
   kvfree(imu->bvec);
   goto err;
  }

  ret = io_buffer_account_pin(ctx, pages, pret, imu, &last_hpage);
  if (ret) {
   unpin_user_pages(pages, pret);
   kvfree(imu->bvec);
   goto err;
  }

  off = ubuf & ~PAGE_MASK;
  size = iov.iov_len;
  for (j = 0; j < nr_pages; j++) {
   size_t vec_len;

   vec_len = min_t(size_t, size, PAGE_SIZE - off);
   imu->bvec[j].bv_page = pages[j];
   imu->bvec[j].bv_len = vec_len;
   imu->bvec[j].bv_offset = off;
   off = 0;
   size -= vec_len;
  }
  /* store original address for later verification */
  imu->ubuf = ubuf;
  imu->len = iov.iov_len;
  imu->nr_bvecs = nr_pages;

  ctx->nr_user_bufs++;
 }
 kvfree(pages);
 kvfree(vmas);
 return 0;
err:
 kvfree(pages);
 kvfree(vmas);
 io_sqe_buffer_unregister(ctx);
 return ret;
}
```

## 5.**Polled IO模式**

### 5.1 优化思想

将较多的CPU时间放到重要的事情上，全速完成关键路径。

状态从未完成变成已完成，就需要对完成状态进行探测，很多时候，可以使用中断模型，也就是等待后端数据处理完毕之后，内核会发起一个SIGIO或eventfd的EPOLLIN状态提醒核外有数据已经完成了，可以开始处理。但是，中断其实是比较耗时的，如果是高IOPS的场景，就会不停地中断，中断开销就得不偿失。

我们可以更激进一些，让内核采用Polled IO模式收割块设备层请求。这在一定的程度上加速了IO，这在追求低延时和高IOPS的应用场景非常有用。

### 5.2 优化实现

io_uring_enter通过正确设置IORING_ENTER_GETEVENTS，IORING_SETUP_IOPOLL等flag（如下代码设置IORING_SETUP_IOPOLL并且不设置IORING_SETUP_SQPOLL，即没有使用SQ线程）调用io_iopoll_check。

```
SYSCALL_DEFINE6(io_uring_enter, unsigned int, fd, u32, to_submit,
  u32, min_complete, u32, flags, const sigset_t __user *, sig,
  size_t, sigsz)
{
 struct io_ring_ctx *ctx;
 long ret = -EBADF;
 int submitted = 0;
 struct fd f;

 io_run_task_work();

 if (flags & ~(IORING_ENTER_GETEVENTS | IORING_ENTER_SQ_WAKEUP |
   IORING_ENTER_SQ_WAIT))
  return -EINVAL;

 f = fdget(fd);
 if (!f.file)
  return -EBADF;

 ret = -EOPNOTSUPP;
 if (f.file->f_op != &io_uring_fops)
  goto out_fput;

 ret = -ENXIO;
 ctx = f.file->private_data;
 if (!percpu_ref_tryget(&ctx->refs))
  goto out_fput;

 ret = -EBADFD;
 if (ctx->flags & IORING_SETUP_R_DISABLED)
  goto out;

 /*
  * For SQ polling, the thread will do all submissions and completions.
  * Just return the requested submit count, and wake the thread if
  * we were asked to.
  */
 ret = 0;
 if (ctx->flags & IORING_SETUP_SQPOLL) {
  if (!list_empty_careful(&ctx->cq_overflow_list))
   io_cqring_overflow_flush(ctx, false, NULL, NULL);
  if (flags & IORING_ENTER_SQ_WAKEUP)
   wake_up(&ctx->sq_data->wait);
  if (flags & IORING_ENTER_SQ_WAIT)
   io_sqpoll_wait_sq(ctx);
  submitted = to_submit;
 } else if (to_submit) {
  ret = io_uring_add_task_file(ctx, f.file);
  if (unlikely(ret))
   goto out;
  mutex_lock(&ctx->uring_lock);
  submitted = io_submit_sqes(ctx, to_submit);
  mutex_unlock(&ctx->uring_lock);

  if (submitted != to_submit)
   goto out;
 }
 if (flags & IORING_ENTER_GETEVENTS) {
  min_complete = min(min_complete, ctx->cq_entries);

  /*
   * When SETUP_IOPOLL and SETUP_SQPOLL are both enabled, user
   * space applications don't need to do io completion events
   * polling again, they can rely on io_sq_thread to do polling
   * work, which can reduce cpu usage and uring_lock contention.
   */
  if (ctx->flags & IORING_SETUP_IOPOLL &&
      !(ctx->flags & IORING_SETUP_SQPOLL)) {
   ret = io_iopoll_check(ctx, min_complete);
  } else {
   ret = io_cqring_wait(ctx, min_complete, sig, sigsz);
  }
 }

out:
 percpu_ref_put(&ctx->refs);
out_fput:
 fdput(f);
 return submitted ? submitted : ret;
}
```

io_iopoll_check开始poll核外程序可以不停的轮询需要的完成事件数量min_complete，循环内主要调用io_iopoll_getevents。

```
static int io_iopoll_check(struct io_ring_ctx *ctx, long min)
{
 unsigned int nr_events = 0;
 int iters = 0, ret = 0;

 /*
  * We disallow the app entering submit/complete with polling, but we
  * still need to lock the ring to prevent racing with polled issue
  * that got punted to a workqueue.
  */
 mutex_lock(&ctx->uring_lock);
 do {
  /*
   * Don't enter poll loop if we already have events pending.
   * If we do, we can potentially be spinning for commands that
   * already triggered a CQE (eg in error).
   */
  if (io_cqring_events(ctx, false))
   break;

  /*
   * If a submit got punted to a workqueue, we can have the
   * application entering polling for a command before it gets
   * issued. That app will hold the uring_lock for the duration
   * of the poll right here, so we need to take a breather every
   * now and then to ensure that the issue has a chance to add
   * the poll to the issued list. Otherwise we can spin here
   * forever, while the workqueue is stuck trying to acquire the
   * very same mutex.
   */
  if (!(++iters & 7)) {
   mutex_unlock(&ctx->uring_lock);
   io_run_task_work();
   mutex_lock(&ctx->uring_lock);
  }

  ret = io_iopoll_getevents(ctx, &nr_events, min);
  if (ret <= 0)
   break;
  ret = 0;
 } while (min && !nr_events && !need_resched());

 mutex_unlock(&ctx->uring_lock);
 return ret;
}
```

io_iopoll_getevents调用io_do_iopoll。

```
/*
 * Poll for a minimum of 'min' events. Note that if min == 0 we consider that a
 * non-spinning poll check - we'll still enter the driver poll loop, but only
 * as a non-spinning completion check.
 */
static int io_iopoll_getevents(struct io_ring_ctx *ctx, unsigned int *nr_events,
    long min)
{
 while (!list_empty(&ctx->iopoll_list) && !need_resched()) {
  int ret;

  ret = io_do_iopoll(ctx, nr_events, min);
  if (ret < 0)
   return ret;
  if (*nr_events >= min)
   return 0;
 }

 return 1;
}
```

io_do_iopoll中的kiocb->ki_filp->f_op->iopoll，即blkdev_iopoll，不断地轮询探测确认提交给Block层的请求的完成状态，直到足够数量的IO完成。

```
static int io_do_iopoll(struct io_ring_ctx *ctx, unsigned int *nr_events,
   long min)
{
 struct io_kiocb *req, *tmp;
 LIST_HEAD(done);
 bool spin;
 int ret;

 /*
  * Only spin for completions if we don't have multiple devices hanging
  * off our complete list, and we're under the requested amount.
  */
 spin = !ctx->poll_multi_file && *nr_events < min;

 ret = 0;
 list_for_each_entry_safe(req, tmp, &ctx->iopoll_list, inflight_entry) {
  struct kiocb *kiocb = &req->rw.kiocb;

  /*
   * Move completed and retryable entries to our local lists.
   * If we find a request that requires polling, break out
   * and complete those lists first, if we have entries there.
   */
  if (READ_ONCE(req->iopoll_completed)) {
   list_move_tail(&req->inflight_entry, &done);
   continue;
  }
  if (!list_empty(&done))
   break;

  ret = kiocb->ki_filp->f_op->iopoll(kiocb, spin);
  if (ret < 0)
   break;

  /* iopoll may have completed current req */
  if (READ_ONCE(req->iopoll_completed))
   list_move_tail(&req->inflight_entry, &done);

  if (ret && spin)
   spin = false;
  ret = 0;
 }

 if (!list_empty(&done))
  io_iopoll_complete(ctx, nr_events, &done);

 return ret;
}
```

块设备层相关file_operations。

```
const struct file_operations def_blk_fops = {
 .open  = blkdev_open,
 .release = blkdev_close,
 .llseek  = block_llseek,
 .read_iter = blkdev_read_iter,
 .write_iter = blkdev_write_iter,
 .iopoll  = blkdev_iopoll,
 .mmap  = generic_file_mmap,
 .fsync  = blkdev_fsync,
 .unlocked_ioctl = block_ioctl,
#ifdef CONFIG_COMPAT
 .compat_ioctl = compat_blkdev_ioctl,
#endif
 .splice_read = generic_file_splice_read,
 .splice_write = iter_file_splice_write,
 .fallocate = blkdev_fallocate,
};
```

当使用POLL IO时，大多数CPU时间花费在blkdev_iopoll上。即全速完成关键路径。

```
static int blkdev_iopoll(struct kiocb *kiocb, bool wait)
{
 struct block_device *bdev = I_BDEV(kiocb->ki_filp->f_mapping->host);
 struct request_queue *q = bdev_get_queue(bdev);

 return blk_poll(q, READ_ONCE(kiocb->ki_cookie), wait);
}
```

### 5.3 Kernel Side Polling

IORING_SETUP_SQPOLL，当前应用更新SQ并填充一个新的SQE，内核线程sq_thread会自动完成提交，这样应用无需每次调用io_uring_enter系统调用来提交IO。应用可通过IORING_SETUP_SQ_AFF和sq_thread_cpu绑定特定的CPU。

实际机器上，不仅有高IOPS场景，还有些场景的IOPS有些时间段会非常低。为了节省无IO场景的CPU开销，一段时间空闲，该内核线程可以自动睡眠。核外在下发新的IO时，通过IORING_ENTER_SQ_WAKEUP唤醒该内核线程。

### 5.4 小结

如上可见，内核提供了足够多的选择，不同的方案有着不同角度的优化方向，这些优化方案可以自行组合。通过合理地使用，可以使io_uring 全速运转。

## 6.**io_uring用户态库liburing**

正如前文所说，简单并不一定意味着易用——io_uring的接口足够简单，但是相对于这种简单，操作上需要手动mmap来映射内存，稍显复杂。为了更方便地使用io_uring，原作者Jens Axboe还开发了一套liburing库。liburing库提供了一组辅助函数实现设置和内存映射，应用不必了解诸多io_uring的细节就可以简单地使用起来。例如，无需担心memory barrier，或者是ring buffer管理之类等。上文所提的一些高级特性，在liburing中也有封装。

### 6.1 核心数据结构

liburing中，核心的结构有io_uring、io_uring_sq、io_uring_cq

```
/*
 * Library interface to io_uring
 */
struct io_uring_sq {
 unsigned *khead;
 unsigned *ktail;
 unsigned *kring_mask;
 unsigned *kring_entries;
 unsigned *kflags;
 unsigned *kdropped;
 unsigned *array;
 struct io_uring_sqe *sqes;

 unsigned sqe_head;
 unsigned sqe_tail;

 size_t ring_sz;
};

struct io_uring_cq {
 unsigned *khead;
 unsigned *ktail;
 unsigned *kring_mask;
 unsigned *kring_entries;
 unsigned *koverflow;
 struct io_uring_cqe *cqes;

 size_t ring_sz;
};

struct io_uring {
 struct io_uring_sq sq;
 struct io_uring_cq cq;
 int ring_fd;
};
```

### 6.2 核心接口

相关接口在头文件linux/tools/io_uring/liburing.h，如果是通过标准方式安装的liburing，则在/usr/include/下。

```
/*
 * System calls
 */
extern int io_uring_setup(unsigned entries, struct io_uring_params *p);
extern int io_uring_enter(int fd, unsigned to_submit,
 unsigned min_complete, unsigned flags, sigset_t *sig);
extern int io_uring_register(int fd, unsigned int opcode, void *arg,
 unsigned int nr_args);

/*
 * Library interface
 */
extern int io_uring_queue_init(unsigned entries, struct io_uring *ring,
 unsigned flags);
extern int io_uring_queue_mmap(int fd, struct io_uring_params *p,
 struct io_uring *ring);
extern void io_uring_queue_exit(struct io_uring *ring);
extern int io_uring_peek_cqe(struct io_uring *ring,
 struct io_uring_cqe **cqe_ptr);
extern int io_uring_wait_cqe(struct io_uring *ring,
 struct io_uring_cqe **cqe_ptr);
extern int io_uring_submit(struct io_uring *ring);
extern struct io_uring_sqe *io_uring_get_sqe(struct io_uring *ring);
```

### 6.3 主要流程

- 使用io_uring_queue_init，完成io_uring相关结构的初始化。在这个函数的实现中，会调用多个mmap来初始化一些内存。

- 初始化完成之后，为了提交IO请求，需要获取里面queue的一个项，使用io_uring_get_sqe。

- - 获取到了空闲项之后，使用io_uring_prep_readv、io_uring_prep_writev初始化读、写请求。和前文所提preadv、pwritev的思想差不多，这里直接以不同的操作码委托io_uring_prep_rw，io_uring_prep_rw只是简单地初始化io_uring_sqe。

- 准备完成之后，使用io_uring_submit提交请求。

- 提交了IO请求时，可以通过非阻塞式函数io_uring_peek_cqe、阻塞式函数io_uring_wait_cqe获取请求完成的情况。默认情况下，完成的IO请求还会存在内部的队列中，需要通过io_uring_cqe_seen表标记完成操作。

- 使用完成之后要通过io_uring_queue_exit来完成资源清理的工作。

### 6.4 核心实现

io_uring_queue_init的实现，前文已略有提及。其中的操作主要就是io_uring_setup和io_uring_queue_mmap，io_uring_setup前文已解析过，这里主要看io_uring_queue_mmap。

```
/*
 * Returns -1 on error, or zero on success. On success, 'ring'
 * contains the necessary information to read/write to the rings.
 */
int io_uring_queue_init(unsigned entries, struct io_uring *ring, unsigned flags)
{
 struct io_uring_params p;
 int fd, ret;

 memset(&p, 0, sizeof(p));
 p.flags = flags;

 fd = io_uring_setup(entries, &p);
 if (fd < 0)
  return fd;

 ret = io_uring_queue_mmap(fd, &p, ring);
 if (ret)
  close(fd);

 return ret;
}
```

io_uring_queue_mmap初始化io_uring结构，然后主要调用io_uring_mmap。

```
/*
 * For users that want to specify sq_thread_cpu or sq_thread_idle, this
 * interface is a convenient helper for mmap()ing the rings.
 * Returns -1 on error, or zero on success.  On success, 'ring'
 * contains the necessary information to read/write to the rings.
 */
int io_uring_queue_mmap(int fd, struct io_uring_params *p, struct io_uring *ring)
{
 int ret;

 memset(ring, 0, sizeof(*ring));
 ret = io_uring_mmap(fd, p, &ring->sq, &ring->cq);
 if (!ret)
  ring->ring_fd = fd;
 return ret;
}
```

io_uring_mmap初始化io_uring_sq结构和io_uring_cq结构的内存，另外还会分配一个io_uring_sqe结构的数组。

```
static int io_uring_mmap(int fd, struct io_uring_params *p,
    struct io_uring_sq *sq, struct io_uring_cq *cq)
{
 size_t size;
 void *ptr;
 int ret;

 sq->ring_sz = p->sq_off.array + p->sq_entries * sizeof(unsigned);
 ptr = mmap(0, sq->ring_sz, PROT_READ | PROT_WRITE,
   MAP_SHARED | MAP_POPULATE, fd, IORING_OFF_SQ_RING);
 if (ptr == MAP_FAILED)
  return -errno;
 sq->khead = ptr + p->sq_off.head;
 sq->ktail = ptr + p->sq_off.tail;
 sq->kring_mask = ptr + p->sq_off.ring_mask;
 sq->kring_entries = ptr + p->sq_off.ring_entries;
 sq->kflags = ptr + p->sq_off.flags;
 sq->kdropped = ptr + p->sq_off.dropped;
 sq->array = ptr + p->sq_off.array;

 size = p->sq_entries * sizeof(struct io_uring_sqe);
 sq->sqes = mmap(0, size, PROT_READ | PROT_WRITE,
    MAP_SHARED | MAP_POPULATE, fd,
    IORING_OFF_SQES);
 if (sq->sqes == MAP_FAILED) {
  ret = -errno;
err:
  munmap(sq->khead, sq->ring_sz);
  return ret;
 }

 cq->ring_sz = p->cq_off.cqes + p->cq_entries * sizeof(struct io_uring_cqe);
 ptr = mmap(0, cq->ring_sz, PROT_READ | PROT_WRITE,
   MAP_SHARED | MAP_POPULATE, fd, IORING_OFF_CQ_RING);
 if (ptr == MAP_FAILED) {
  ret = -errno;
  munmap(sq->sqes, p->sq_entries * sizeof(struct io_uring_sqe));
  goto err;
 }
 cq->khead = ptr + p->cq_off.head;
 cq->ktail = ptr + p->cq_off.tail;
 cq->kring_mask = ptr + p->cq_off.ring_mask;
 cq->kring_entries = ptr + p->cq_off.ring_entries;
 cq->koverflow = ptr + p->cq_off.overflow;
 cq->cqes = ptr + p->cq_off.cqes;
 return 0;
}
```

### 6.5 具体例程

如下是一个基于liburing的helloworld示例。

```
#include <unistd.h>
#include <fcntl.h>
#include <string.h>
#include <stdio.h>
#include <liburing.h>

#define ENTRIES 4

int main(int argc, char *argv[])
{
    struct io_uring ring;
    struct io_uring_sqe *sqe;
    struct io_uring_cqe *cqe;
    struct iovec iov = {
        .iov_base = "Hello World",
        .iov_len = strlen("Hello World"),
    };
    int fd, ret;
    if (argc != 2) {
        printf("%s: <testfile>\n", argv[0]);
        return 1;
    }
    /* setup io_uring and do mmap */
    ret = io_uring_queue_init(ENTRIES, &ring, 0);
    if (ret < 0) {
        printf("io_uring_queue_init: %s\n", strerror(-ret));
        return 1;
    }
    fd = open("testfile", O_WRONLY | O_CREAT);
    if (fd < 0) {
        printf("open failed\n");
        ret = 1;
        goto exit;
    }
    /* get an sqe and fill in a WRITEV operation */
    sqe = io_uring_get_sqe(&ring);
    if (!sqe) {
        printf("io_uring_get_sqe failed\n");
        ret = 1;
        goto out;
    }
    io_uring_prep_writev(sqe, fd, &iov, 1, 0);
    /* tell the kernel we have an sqe ready for consumption */
    ret = io_uring_submit(&ring);
    if (ret < 0) {
        printf("io_uring_submit: %s\n", strerror(-ret));
        goto out;
    }
    /* wait for the sqe to complete */
    ret = io_uring_wait_cqe(&ring, &cqe);
    if (ret < 0) {
        printf("io_uring_wait_cqe: %s\n", strerror(-ret));
        goto out;
    }
    /* read and process cqe event */
    io_uring_cqe_seen(&ring, cqe);
out:
    close(fd);
exit:
    /* tear down */
    io_uring_queue_exit(&ring);
    return ret;
}
```

更多的示例可参考：
http://git.kernel.dk/cgit/liburing/tree/examples
https://git.kernel.dk/cgit/liburing/tree/test

## 7.**性能**

如上，推演过了设计与实现，回归到存储的需求上来，io_uring子系统是否能满足我们对高性能的极致需求呢？这一切还是需要profile。

### 7.1 测试方法

io_uring原作者Jens Axboe在fio中提供了ioengine=io_uring的支持，可以使用fio进行测试，使用ioengine选项指定异步IO引擎。

可以基于不同的IO栈：

- libaio
- kernel+io_uring
- kernel+io_uring polling mode

可以基于一些硬件之上：

- NVMe SSD
- ...

测试过程中主要4k数据的顺序读、顺序写、随机读、随机写，对比几种IO引擎的性能及QoS等指标

io_uring polling mode测试实例：

```
fio -name=testname -filename=/mnt/vdd/testfilename -iodepth=64 -thread -rw=randread -ioengine=io_uring -sqthread_poll=1 -direct=1 -bs=4k -size=10G -numjobs=1 -runtime=600 -group_reporting
```

### 7.2 测试结果

网上可以找到一些关于io uring的性能测试，这里列出部分供参考：

- [*Im**proved Flash Performance Using the New Linux Kernel I/O Interface*](https://www.flashmemorysummit.com/Proceedings2019/08-07-Wednesday/20190807_SOFT-202-1_Verma.pdf)
- [*io_uring echo server benchs*](https://github.com/frevib/io_uring-echo-server/blob/io-uring-feat-fast-poll/benchs/benchs.md)
- [*[PATCHSET v5\] io_uring IO interface*](https://lore.kernel.org/linux-block/20190116175003.17880-1-axboe@kernel.dk/)
- ...

主要有以下几个测试结果

- io_uring在非polling模式下，相比libaio，性能提升不是非常显著。
- io_uring在polling模式下，性能提升显著，与spdk接近，在队列深度较高时性能更好。
- 在meltdown和spectre漏洞没有修复的场景下，io_uring的提升并不太高。虽然减少了大量的用户态到内核态的上下文切换，在meldown和spectre漏洞没有修复的场景下，用户态到内核态的切换开销本比较小，所以提升不太高。
- 在某些场景下使用io_uring + Kernel NVMe的驱动，效果甚至要比使用SPDK 用户态NVMe 驱动更好

从测试中，我们可以得出结论，在存储中使用io_uring，相比使用libaio，应用的性能会有显著的提升。

在同样的硬件平台上，仅仅更换IO引擎，就可以带来较大的提升，是很难得的，对于存储这种延时敏感的应用而言十分宝贵。

### 7.3 io_uring的优势

综合前文和测试，io_uring有如此出众的性能，主要来源于以下几个方面：

- 用户态和内核态共享提交队列SQ和完成队列CQ实现零拷贝。
- IO提交和收割可以offload给Kernel，不需要经过系统调用。
- 支持块设备层的Polling模式。
- 可以提前注册用户态内存地址，从而减少地址映射的开销。
- 相比libaio，支持buffered IO

## 8.**发展方向**

事物的发展是一个哲学话题。前文阐述了io_uring作为一个新事物，发展的根本动力、内因和外因，谨此简述一些可预见的未来的发展方向。

### 8.1 普及

应用层多使用。目前主要应用在存储的场景中，这是一个不仅需要高性能，也需要稳定的场景，而一般来说，新事物并不具备“稳定”的属性。但是io_uring同样也是稳定的，因为虽然io_uring使用到了若干新概念，但是这些新的东西已经有了实践的检验，如eventfd通知机制，SIGIO信号机制，与AIO基本相似。它是一个质变的新事物。

就我们腾讯而言，内核使用tlinux，tlinux3基于4.14.99主线；tlinux4基于5.4.23主线。

所以，tlinux3可以用native aio，tlinux4之后已经可以用native io_uring。

相信通过大家的努力，正如前文所说的PostgreSQL使用彼时新接口pread，Nginx使用彼时的新接口AIO一样，通过使用新街口，我们的工程也能获得巨大收益。

### 8.2 优化方向

#### 8.2.1 **降低本身的工作负载**

持续降低系统调用开销、拷贝开销、框架本身的负载。

#### 8.2.2 **重构**

> "Politics are for the moment. An equation is for eternity.
> 　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　——Albert Einstein

追求真理的人不可避免地追求永恒。“政治只是一时，方程却是永恒。”——爱因斯坦如是说，时值以色列的第一任总统魏兹曼于1952年逝世，继任首相古理安建议邀请爱因斯坦担任第二任总统。

我们说折衷权衡、精益求精，字里行间都是永恒，然而软件应该持续重构，这实际上并不只是io_uring需要做的，有机会我会写一篇关于重构的文章。

## 9.**总结**

首先，本文简述了Linux过往的的IO发展历程，同步IO接口、原生异步IO接口AIO的缺陷，为何原有方式存在缺陷。其次，再从设计的角度出发，介绍了最新的IO引擎io_uring的相关内容。最后，深入最新版内核linux-5.10中解析了io_uring的大体实现（关键数据结构、流程、特性实现等）。

## 10.**关于**

难免纰漏，欢迎交流，可以通过以下网址找到本文。

- 知乎：https://www.zhihu.com/people/linkerist-61
- Github: https://github.com/Linkerist/blog/issues

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvasa9lQclibbU8QD0Gq8Jib16Gl18nIYG3o715BRWUw0uqkv90xxBM3F7xYrBjjvmXGFa4AnsV0EHIMg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

内容会更新，可以关注我的公众号，欢迎交流。

## 11.**参考**

[*PATCH 12/19\]io_uring: add support for pre-mapped user IO buffers*](https://lore.kernel.org/linux-block/20190211190049.7888-14-axboe@kernel.dk/)

[*Add pread/pwrite support bits to match the lseek bit*](https://lwn.net/Articles/97178/)

[*Toward non-blocking asynchronous I/O*](https://lwn.net/Articles/724198/)

[*A new kernel polling interface*](https://lwn.net/Articles/743714/)

[*The rapid growth of io_uring*](https://lwn.net/Articles/810414/)

[*Ringing in a new asynchronous I/O API*](https://lwn.net/Articles/776703/)

[*Efficient IO with io_uring*](http://kernel.dk/io_uring.pdf)

[*The current state of kernel page-table isolation*](https://lwn.net/Articles/741878)

[The Linux man-pages project](https://www.kernel.org/doc/man-pages/)

https://zhuanlan.zhihu.com/p/62682475

[*why we need io_uring? by byteisland*](https://www.byteisland.com/io_uring（1）-我们为什么会需要-io_uring/)

*Computer Systems: A Programmer's Perspective, Third Edition*

*Advanced Programming in the UNIX Environment, Third Edition*

*The Linux Programming Interface: A Linux and UNIX System Programming Handbook*

[*Introduction to io_uring*](http://kernel.taobao.org/2020/08/Introduction_to_IO_uring/)

*Understanding Nginx Modules Development and Architecture Resolving(Second Edition)*

原文作者：子晨

原文链接：https://mp.weixin.qq.com/s/QshDG-nbmBcF1OBZbBFwjg

# 【NO.179】云时代，我们需要怎样的数据库？

本文作者腾讯云TDSQL负责人潘安群。潘安群主要负责腾讯云分布式数据库研发，拥有超过13年分布式数据库研发经验，研发成果多次入选国际顶会VLDB、SIGMOD等。他带领团队打造的安全可控分布式数据库TDSQL，是业内首个应用于互联网银行核心交易系统、首个进入银行传统核心系统、首个助力传统大型银行实现银行业首例“大型机”下移分布式平台的国产企业级分布式数据库。

## **0.引言**

数据库技术发展已达半个世纪之久，数据库图灵奖得主Michael Stonebraker曾在Readings in Database Systems中将数据库模型技术分为9个不同的时代与类型，而云时代开始以后，我们可以从全新的视角审视数据库等基础技术的过去和未来。

基于云计算，包括数据库在内的IT基础技术发生从技术形态到线上线下整个市场结合的大幅变化，数据库技术呈现从传统集中式到云时代分布式迁移替换的趋势，这也给国产数据库赋予机遇与挑战。

在2020年11月，Gartner发布了2020年度的数据库厂商评估报告，国数据库厂商占据三席，标志着国内数据库进入全新发展阶段。

同时，Gartner预测，到2022年，世界上3/4的数据库都会跑在云上，而我们认为，云数据库的发展，目前正在经历从第一阶段“数据库上云，即从数据库到云数据库”，到第二阶段“从云数据库到云原生数据库”的变革。

归根结底，云数据库做了什么得到了业内的认可？未来数据库发展趋势是什么？我们可以如何在新机遇下的云融合时代把握技术创新的脉搏？在当前国产数据库也成为一个热门话题之际，我们谈一谈我们的理解和思考，与大家共勉。

## **1.云时代的IT基础技术形态演变**

随着云计算的发展，整个IT基础技术翻天覆地的变化体现在几个方面：

IT设施部署，从过去的零散化走向今天的集中化、规模化。过去，每一个企业自建各自的数据中心等IT基础设施，包括服务器、网络到操作系统、数据库等，形成企业市场上零散化的IT设施模式。而今天基于云计算服务，企业IT设施呈现集中化、规模化效应，对效率、性能、成本的要求提升。

IT服务交付，从过去的软件交付模式走向服务交付模式。过去购买商业化软件，或者是使用开源软件产品，基本是通过商业化或开源的方式进行分发，而现在完全变成一个个服务的形式进行交付。这带来的变化是，用户不需要再盘算该购买几台服务器，而是在具有数据库使用需求时，直接云上使用即可。

开发方式，将呈现从过去业务进行非常底层的开发以及调用底层API等操作的模式，转向SaaS化、Severless模式的服务。在云上，开发者可以使用各种各样的SaaS服务。无论从效率、基础技术能力等方面来说，这都是一个巨大的变化。

而数据形式及应用场景领域而言，事实上过去的数据形式或应用场景相对单一，以传统数据库为例，场景主要集中在了比如金融、运营商、政务等传统行业领域。随着互联网、移动互联网、产业互联网的发展，各个行业也正逐步加速其电子化、信息化发展趋势，应用服务形式呈多样化发展，使得当前行业的数据形式及应用场景也越来越多样化，并对底层数据库能力提出更多的要求和挑战。过去，行业场景中更多以结构化的数据为主，关系型数据库可以支撑极大部分场景需求，现在我们可以看到涌现出了许多如NoSQL、Graph图数据库等各种类型的数据库，NoSQL下属同时还可以细分KV型、文档型等多种类别，而且整体数据库类型还有持续增加的趋势。这是非常合理的现象。也就是说，对于未来数据库来说，其自身发展也会呈现多样化的、而且是融合、创新的趋势。我们知道，按照传统经验来说，如果一个技术产品是单一的形态，那么追求的是尽量做到通用化，然而，在当前多样化需求的趋势下，技术应用层面需要进行各种权衡和取舍。

因此可以说，这是云时代的发展变化，对数据库带来的新的挑战和要求。在当前云数据库成为大势所趋的同时，我们认为，国产云数据库要发展好，需要持续在基础能力、成本效率、产品化、未来技术融合等各个层面进行探索突破。

## **2.云数据库技术演进的挑战**

结合云计算的特点，国产云数据库发展面临着需要持续探索可用性与一致性、高并发性能、弹性可扩展等基础能力突破，同时面向云时代的多样化趋势打造新一代分布式数据库产品的挑战和要求。

### 2.1 可用性与一致性。

作为数据库，高可用性、数据一致性是最基础的挑战。高可用性，要求达到99.999%以上；数据强一致性，意味着数据不出错，数据库高度可靠。云计算时代，技术设施的升级换代对技术实现方式带来变革。过去，比如金融行业，系统基于稳定性较高的传统集中式大型机或小型机来保障系统的可用性与一致性。然而，传统集中式结构存在明显的技术边界，包括性能和吞吐量的边界，今天它们已然面临较大的吞吐和性能瓶颈，无法满足云时代的产业需求。自然地，当前产业趋势是向分布式架构转型升级，转向基于x86等的分布式、开放式平台。传统架构系统依赖于大型机或小型机在硬件层面进行的大量的冗余设计，在硬件层面实现可用性与一致性保障。而相对来说，基于x86机器部署的新一代分布式架构系统，则在如何实现性能、无限水平扩展的基础上保证数据一致以及系统高可用提出新的挑战要求。

### 2.2 性能成本。

云计算时代，如果实现了规模化以后，还不能实现成本降低的话，是不可接受的。云计算要帮助提升整个社会资源利用率，性能成本需要控制到最低。

对于腾讯云的服务来说，我们需要考虑的是如何能够保证客户以最便宜的价格买到最高级的服务——比如花最少的钱买到最大的磁盘空间、以及最好的TPS等产品表现。而在这个过程中，最核心的就是资源利用率。举个例子，云计算服务商如果把资源利用率提升20%，对客户、服务商本身而言将能极大地降低一部分成本。

### 2.3 云原生意味着一定是弹性伸缩的。

弹性伸缩，也就是可以根据用户的实际需求进行资源分配与使用，而不再是过去通过预采购或预分配的方式。过去，客户大部分都是先预估，然后采购，所以资源利用率一直被诟病；现在则不需要用户再预估自己未来可能会用到多少资源，而是可以根据实时的使用需求实现弹性伸缩。也因为这样，通过提高资源利用率，云数据库才可以实现成本上的优势。但是，极致弹性伸缩对数据库在更高程度的SQL支持、分布式事务能力方面，提出了更高的要求。

### 2.4 云数据库产品化服务化程度。

国内数据库发展历程也经历多个阶段，但正是云计算、互联网的时代兴起，国内诸多腾讯这类云厂商得以抓住机会，基于自身业务场景特点和需求，发展新一代数据库等基础软件技术。在过去很多年的时间当中，腾讯非常重看重的一点就是，如何打磨提升整个数据库的产品化程度，提升用户体验，包括技术产品化、服务完善等方面。互联网厂商基于内部业务场景发展自己的技术体系，这是优势的一面，而在to B开放的过程中，同时也面临产品标准化、通用性、使用体验等挑战。面向行业客户提供技术产品，其要求比支撑内部使用高得多。对于传统企业客户而言，腾讯云希望提供给到客户的是一个完整的产品，而不是一个半成品。因此，产品化程度，是腾讯一直持续强调的能力。

### 2.5 海量场景验证。

最后关键的一点是，对于云数据库而言，包括稳定性、特性需求等基础能力的发展，核心条件是需要有足够的应用场景进行打磨。数据库系统的研发、完善是一个非常复杂的过程，如何让数据库得到实践、得到应用？走到今天，我们认为，持续的、海量的场景打磨，是产品发展的关键条件。得益于腾讯自身以及云上各行各业的应用，超过百万开发者的使用，腾讯云数据库能够有足够的空间打磨产品。这是我们的挑战，也是推动我们发展的土壤。

这些挑战是云数据库发展过程中的必经之路，也是我们在云计算时代创造出新一代分布式数据库产品的机遇。

## **3.云数据库未来关键趋势**

基于这些挑战以及云计算时代赋予的机会，我们认为未来云数据库发展将包括几大趋势要求：

**弹性伸缩：解决成本核心问题——资源利用率**

前面提到，成本与性能是核心的要素。这里引申出来一个云计算时代的差异，那就是我们需要实现对CPU、内存和磁盘等基础设施资源的灵活调度。

云数据库时代我们将通过对极致的弹性伸缩架构探索，来综合解决性能、效率和成本问题。针对不同的场景侧重，云原生分布式数据库可分为两种架构：一种是Shared Nothing，一种是Shared Storage，两者都可以通过实现计算与存储分离架构来整体获得更优秀的弹性伸缩能力，克服传统架构下的存储量受限、扩展难、主从延迟高等缺点，同时也能够帮助我们将成本控制得更低，充分释放领先技术的成本效益。

而计算与存储全Serverless架构的数据库服务也是未来可重点关注的方向，它在可自动无感扩缩容的基础上，同时实现可按实际使用计费，不用不付费，提升云数据库效用。

**多模多引擎趋势下的数据库底层与服务超融合**

新基建、产业互联网快速发展，各行各业数字化进程加速，数据形式越来越多样化，越来越海量，如何能最高效地解决数据库在性能、成本、服务等方面问题，超融合是必然趋势。

当下我们处于各行各业都在推进电子化、信息化建设和数字化转型的趋势浪潮之下，行业不断涌现出大量的新兴场景。数据库作为支撑各类IT系统架构的基础软件技术，其整个技术形态也随之出现各类新的应用实现，包括大量的NoSQL实践，以及存储领域有传统的B+ Tree、现在的LSM Tree，和行存、列存等架构形态产品；而根据workload类型区分的话，则涌现出包括OLTP、OLAP，或者两者混合形成的HTAP型数据库等。

而多种多样的引擎产品，在大多数情况下不会独立存在来服务于一个企业或系统。One size fits none。从技术角度看，极致的性能成本与通用性有着天然的矛盾，因此，在多样化场景下，一定会是多引擎共存，充分发挥各种引擎的特点与优势，才能实现极致与通用的兼得。

但是不是作为云数据库服务厂商，我们把这些各类引擎产品都暴露给客户、开发者自行选择呢？从产品服务体验的角度看，必然不是。多模态技术引擎的现状必然对开发者选型带来选型、开发应用上的困难——即如何能够在保证适应不同的场景下，同时获得足够高的性能表现，这也是当前数据库发展面临的一个困境。为了解决这个问题，未来我们希望是不需要用户来进行这些复杂的选择，而是系统基于AI智能调度、serverless等解决方案，来彻底实现多引擎的统一标准化服务。从底层的角度看，未来开发者无需感知具体的产品选型，而比如在做数据分析的时候，系统能够自动帮助调度性能最好、事务交易一致性得到保障的方案。

在此基础上，未来云数据库服务的趋势还是交付方式的融合，包括软硬件一体化、私有云与公有云平台融合等多种产品和服务交付方案，能够让客户在敏感业务和运营成本之间实现更加精细化管理。

### 3.1 **智能化：AI+DB**

智能化技术等底层技术生态融合变革，实现数据库自治与智能管理也是未来数据库趋势之一。过去，对于一个企业，也许几个DBA来管理几十套实例就足够了，但比如对腾讯来说，数十万的数据库实例，难以通过配置人力来维持运营，因此倒逼我们必须要通过工具或平台来解决运营效率的问题。此外，当前分布式微服务改造的趋势下，未来企业IT运营也将具有越来越强烈的自治需求。智能化技术与数据库底层的融合，能实现对数据库进行全生命周期智能管理。

### **3.2 加速释放新硬件红利**

过去一个新硬件的推广周期很长，很多传统企业在采购新硬件方面相对非常保守。而对于云厂商来说，相对有条件逐步率先探索新硬件的应用，比如先在非关键性应用，同时也具备海量的场景验证，来实现稳步规模化的推广。这个角度来说，基于云计算服务，云原生数据库相对更加容易探索、释放到新硬件带来的红利。

当前我们也处于新硬件创新层出不穷的时代，包括SSD、NVM、RDMA+SPDK、千核服务器、异构处理器等，基于云数据库服务，广大的客户、普通开发者也能够更快速地享受到新硬件带来的加持。

因此，融合、自治、效用是未来企业级分布式数据库基本特点。腾讯云数据库将从实践层面对以上趋势进行落地推进，来满足各行各业客户未来对数据库的多样性需求。

原文作者：潘安群

原文链接：https://mp.weixin.qq.com/s/2uG1WjCjqOJWJceCTj5zSA

# 【NO.180】STGW 下一代互联网标准传输协议QUIC大规模运营之路

## **0.前言**

QUIC 作为互联网下一代标准传输协议，能够明显提升业务访问速度，提升弱网请求成功率以及改善网络变化场景下的平滑体验。

STGW 作为公司级的 7 层接入网关以及腾讯云 CLB（负载均衡器）的底层支撑框架，每天都为公司内部业务和腾讯云外部客户提供数万亿次的请求服务，对请求处理的性能、传输效率、运营的可靠性都有非常严苛的要求。

本文主要介绍 STGW 大规模运营 QUIC 过程中的一些经验和开发工作。

## 1.**QUIC 简介**

### 1.1 QUIC 的诞生和发展

> 在 QUIC 诞生之前，HTTP 协议经历了几次重要的升级：
>
> HTTP1.0 -> HTTP1.1：增加了长连接支持，大大提升了长连接场景下的性能。
>
> HTTP -> HTTPS：增加安全特性，对请求的性能综合来看会有一定的影响。
>
> HTTP1.1 -> HTTP2：主要特性是多路复用与头部压缩，提高了单连接的并发能力。

这些重要变化都是围绕安全与性能展开，对 HTTP 协议的应用和发展起到了很重要的作用。但是，它没有绕开内核 TCP 的限制，导致其协议的发展终究存在瓶颈。

GOOGLE 在引领业界从 HTTP1.1 迈向 HTTP2（GOOGLE SPDY 协议的标准版）后，再一次走在了前头，在 2012 年提出了实验性的 QUIC 协议，首次使用 UDP 重构了 TLS 和 TCP 协议。QUIC 协议不仅仅只应用于 HTTP，QUIC 在设计时除了考虑 HTTP 外，更是设计作为一个通用的传输层协议。在安全性上，GOOGLE 设计了 QUIC 加密协议作为握手协议以解决 QUIC 协议上的安全问题。一般来说，QUIC 握手协议+QUIC 传输层+HTTP2 就是我们常说的 GQUIC（这里指 web 部分）。GQUIC 协议的版本不断演化，从 Q46 开始，GQUIC 协议也不断向 IETF QUIC 和 HTTP3 靠拢。

2015 年，QUIC 的网络草案被正式提交至互联网工程任务组，这意味着新的 QUIC 协议标准将要诞生。在标准 QUIC 协议起草过程中，QUIC 协议上的标准 HTTP 协议作为 HTTP3 也同时被起草。而作为 QUIC 的标准握手协议，IETF 将 TLS1.3 应用其中。TLS1.3+QUIC+HTTP3，这就是我们常说的 IETF QUIC（这里指 web 部分）。截止目前，QUIC 标准的草案已经更新到 34 版，仍没形成正式的 RFC。但是，QUIC 已进入 IETF 最后征求意见，预计标准 QUIC/HTTP3 协议会很快问世。

### 1.2 QUIC 的关键特性

关于 QUIC 的原理，相关介绍的文章很多，这里再列举一下 QUIC 的重要特性。这些特性是 QUIC 得以被广泛应用的关键。不同业务也可以根据业务特点利用 QUIC 的特性去做一些优化。同时，这些特性也是我们去提供 QUIC 服务的切入点。

1. 低连接延时：QUIC 由于基于 UDP，无需 TCP 连接，在最好情况下，短连接下 QUIC 可以做到 0RTT 开启数据传输。而基于 TCP 的 HTTPS，即使在最好的 TLS1.3 的 early data 下仍然需要 1RTT 开启数据传输。而对于目前线上常见的 TLS1.2 完全握手的情况，则需要 3RTT 开启数据传输。对于 RTT 敏感的业务，QUIC 可以有效的降低连接建立延迟。
2. 可自定义的拥塞控制：QUIC 的传输控制不再依赖内核的拥塞控制算法，而是实现在应用层上，这意味着我们根据不同的业务场景，实现和配置不同的拥塞控制算法以及参数。GOOGLE 提出的 BBR 拥塞控制算法与 CUBIC 是思路完全不一样的算法，在弱网和一定丢包场景，BBR 比 CUBIC 更不敏感，性能也更好。在 QUIC 下我们可以根据业务随意指定拥塞控制算法和参数，甚至同一个业务的不同连接也可以使用不同的拥塞控制算法。
3. 无队头阻塞：虽然 HTTP2 实现了多路复用，但是因为其基于面向字节流的 TCP，因此一旦丢包，将会影响多路复用下的所有请求流。QUIC 基于 UDP，在设计上就解决了队头阻塞问题。同时，IETF 设计了 QPACK 编码替换 HPACK 编码，在一定程度上也减轻了 HTTP 请求头的队头阻塞问题。无队头阻塞使得 QUIC 相比 TCP 在弱网和一定丢包环境上有更强大的性能。
4. 连接迁移：当用户的地址发生变化时，如 WIFI 切换到 4G 场景，基于 TCP 的 HTTP 协议无法保持连接的存活。QUIC 基于连接 ID 唯一识别连接。当源地址发生改变时，QUIC 仍然可以保证连接存活和数据正常收发。

## 2.**QUIC 协议栈的选择**

对于协议的实现，STGW 与 CDN 业务团队在 LEGO（STGW 与 CDN 自研的高性能转发框架）上实现过完整的 HTTP2 协议。同时，STGW 也在业界最早实现了 TLS 异步代理计算的方案。对于 HTTP1.1/2 和 TLS 协议有不少工程和优化经验。QUIC 协议栈的自研目前也在按计划展开，但尚不成熟。

本文就基于开源方案，给大家简单介绍一下 QUIC 协议栈的深度定制和优化工作。

关于 QUIC 协议栈的实现，当前功能广泛，协议支持齐全的实现并不多。NGINX 官方目前实现了一个实验版本，但是该实现很多问题没解决，同时，其仅支持 IETF 最新的 DRAFT，甚至连一个完整的拥塞控制算法也没有实现。CLOUDFLARE 的 QUIC 基于 RUST 实现，性能从公开数据来看并不强。

其它的很多实现诸如 MSQUIC, NGHTTP3 等都只支持 IETF QUIC，并不支持 GQUIC。

GOOGLE 是 QUIC 协议的开创者，其基于 CHROME 的 QUIC 协议栈实现最早，功能最齐，实现上也最为标准。

不论是哪种 QUIC 协议栈，其接入都需要我们对 QUIC 的基本特性和概念有较深的理解。比如常见的连接，流，QUIC 连接 ID，QUIC 定时器，统一的调度器等等。这些概念与 QUIC 协议的内容息息相关。

下面以 CHROMIUM QUIC 为例的将 QUIC 协议栈与高性能转发框架 NGINX 与 LEGO 融合的架构图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxGiaMqAupt74SpwpWHicrD3ZkGhtvX7gGn0TTsRkNykYBAzfibJBC4elmg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)CHROMIUM QUIC接入高性能框架NGINX/LEGO

## **3.STGW 的工作**

STGW 作为公司级的 7 层接入网关以及腾讯云 CLB（负载均衡器）的底层支撑框架，为公司内部业务和腾讯云外部客户提供数万亿次请求的服务，对请求处理的性能、传输效率、运营质量的高可靠性都有非常严苟的要求。

为此，我们对 QUIC 协议栈做了大量优化和深度定制，以满足大规模运营和业务需求。主要的工作有：

1. 单机与传输性能优化

2. 1. QUIC 协议单机性能/成本优化：QUIC 将协议栈搬到应用层来，从目前一些公开的实现看，性能比 TCP 差不少，优化 QUIC 协议的性能是大规模推广 QUIC 协议很重要的一环。
   2. 与高性能转发框架融合：当前开源的 QUIC 协议栈的实现仅仅提供单核支持，并仅提供简单的 demo。要想大规模应用，需要将 QUIC 协议栈接入到我们使用的高性能网络转发框架中来，如 NGINX,LEGO 等。
   3. 传输性能，拥塞控制定制化：可以允许不同业务根据业务特性选择不同的拥塞控制算法。
   4. 做到安全高比例的 0RTT，以降低业务的连接延迟。

3. 功能特性的定制和增强

4. 1. 如何让 QUIC 连接迁移从理论走向应用：QUIC 的连接 ID 是 QUIC 协议的特性，但是实际应用中，要做到连接迁移并不容易，需要充分考虑 QUIC 包的各个路径。即使在同一台机器上，也需要正确转发到对应的核上去。
   2. QUIC 私有协议的支持：QUIC 不仅仅用于 HTTP，作为通用的传输层协议，除了支持 GQUIC,IETF HTTP3 外，QUIC 的私有协议也需要我们提供给用户。
   3. QUIC 定制化 SDK：除了高性能 QUIC 服务端外，要想使用 QUIC 需要客户端 SDK 的支持。对此我们也开发了 QUIC 的 SDK，并针对不同的场景做了定制化。
   4. 满足业务各种定制化需求：如有些业务需要 QUIC 明文传输，一些业务需要 QUIC 回源等。

5. 高可用运营

6. 1. 日常变更与平滑升级：在配置频繁变更和模块升级时，我们需要做到对 QUIC 连接无损。
   2. 抓包分析工具：分析定位为更方便。
   3. 统计监控：QUIC 的关键统计指标，需做到可视化运营。

我们围绕着这些问题展开了 QUIC 的相关工作，力求将 QUIC 特性，QUIC 运营，QUIC 性能，QUIC 定制化需求等做到最好。

## 4.**QUIC 处理性能优化**

QUIC 协议基于 UDP 将 TCP 的特性从内核移到了应用层，从当前各种 QUIC 实现来看，性能相比 TCP 差不少。TCP 长期以来使用非常广泛，这也使得其从协议栈到网卡已经经过了非常多的优化，与之相比，UDP 的优化则少了很多。除了内核和硬件外，QUIC 协议的性能也与实现有关，不同的实现版本可能也会有很大的差别。

我们对 QUIC 的性能利用火焰图等各种工具进行了详细分析，找出了一些影响 QUIC 性能的关键点：

1. 密码相关算法的开销：对于小包来说，RSA 的计算占比很高，对于大包来说，对称加解密也会占到 15%左右的比例。

2. UDP 收发包的开销：特别是对于大文件下载来说，sendmsg 占比很高，可以达到 35%-40%以上。

3. 协议栈开销：主要受协议栈实现，如 ACK 的处理，MTU 探测和发包大小，内存管理和拷贝等。

   我们基于影响 QUIC 性能的关键点进行了优化。

### 4.1 QUIC 的 RSA 硬件 OFFLOAD

在小文件请求场景中，RSA 的计算在 QUIC 请求同 HTTPS 一样，仍然是最消耗 CPU 的开销。RSA 在 HTTPS 请求可以利用硬件 offload，在 QUIC 握手过程中，RSA 同样可以利用硬件进行 offload。

使用硬件做 RSA 卸载一个很重要原因是，CPU 计算 RSA 性能较差，而专门做加解密的加速卡性能则很强。一般来说，单块 RSA 加解密卡的成本差不多是一台服务器的 5%-7%，但是其对 RSA 签名的操作性能是服务器的 2-3 倍左右，一台机器插入 2 块卡就可以带来 5 倍的 RSA 性能提升。

将 QUIC 的 RSA 计算进行硬件卸载在不同的 QUIC 协议栈上方法并不相同，下面介绍一种 RSA HOOK + ASYNC JOB 通用的 RSA 卸载方案。其特点是代码侵入性小，不需要额外修改太多 quic 协议栈或者 openssl 的代码。

Openssl1.1.0 之后，开始支持 Async Job。Async job 机制利用类协程方式切换上下文方式实现异步任务，并且提供了一个通知机制通知异步任务的返回结果。

Async Job 里的 2 个重要函数是：

> async_fibre_makecontext
>
> async_fibre_swapcontext

它们利用 setjmp，longjmp，setcontext，makecontext 这些调用，保存和切换当前上下文，从而达到状态保留和代码跳转的能力。

使用 RSA callback 将握手过程中的 RSA 进行拦截，并在 RSA 的 HOOK 函数中本地或者远程向加速卡请求 RSA 操作。同时使用 Async job 方式将同步方式异步化，从而实现 RSA 操作的异步卸载。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwx5OJYmTec2NQibibjU6tl3MuHRPch4ibVibMylWanO38Uh0IAexFvoqRjcA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

在 QUIC TLS1.3 上 RSA HOOK + Async Job 进行 RSA offload。

QUIC 在进行了 RSA 的硬件 OFFLOAD 后，对于小包短连接，性能得到了很大的提升。

以 CHROMIUM QUIC 为例，在 1RTT 场景，QUIC 在使用了 RSA OFFLOAD 后，性能为原来的 256%；0RTT 场景，QUIC 在使用了 RSA OFFLOAD 后，性能为原来的 205%。在 QUIC 协议栈开销更小的实现上，这个性能提升会更加明显。

### 4.2 QUIC 发包的 GSO 优化

在大文件下载中，QUIC 发包的逻辑占比很大，通常在 35%-40%以上。因此优化发包逻辑可以提升大文件传输的性能。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxKdF7e0EY2DLr1hYKFCbW3JVCFTtPANkJtMjvt50CW8sqcbPHdiczDuA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)QUIC大文件请求火焰图

GSO(Generic Segmentation Offload)在内核 4.8 之后开始支持 UDP，其原理是让数据跨过 IP 层，链路层，在数据离开协议栈，进入网卡驱动前进行分段，不论是 TCP 还是 UDP，都是分段(每个包都附加 TCP/UDP 头部)，这样，当一个段丢失，不需要发送整个 TCP/UDP 报文。同时，路径上的 CPU 消耗也会减少。

若网卡硬件本身支持 UDP 分段，则称为 GSO OFFLOAD，其将分段工作放在网卡硬件上做，可以进一步节省 CPU。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwxz3pue6TaVib8YPeQpfStexliaTsfERae7mpNCUlhOKCW721UR5dhxVtw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)GSO原理示意图

QUIC 协议在实现上，一般为了不进行 MTU 分片，通常会在发包前就将发送数据进行分段，从而无需再进行 MTU 分片。对于大包来说，QUIC 会将每个包控制在 1400 字节左右，然后通过 sendmsg 发送出去。大文件发送场景，这种性能是很低的。如果在 sendmsg 时发送大包不做分段，然后利用内核 GSO 延迟分段，会减少路径的 CPU 消耗。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwx5d0kRjv9nTHaXDkKxWtqUciaFictdCskdz16TNIfZicYQQuXOQkF5uEUg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)使用GSO发不同大小包时的吞吐

从表中可以看出，使用 GSO 连续 20 个包进行 sendmsg 发送，相比于 1400 个包单独发送，性能提升了 2-3 倍。

在实际 QUIC 场景中，发包并不是 QUIC 的全部逻辑。同时，并不是每次发包正好都可以凑齐 20 个连续包。我们对大文件下使用 GSO 进行 QUIC 压力测试，相同 CPU 使用情况下，吞吐提高了大约在 15%-20%。

### 4.3 **QUIC 协议栈的优化**

QUIC 协议栈的性能与 QUIC 协议栈实现有关。对于一些常见的协议栈实现，其优化空间主要有：

1. 一些实现如 CHROMIUM 在 0RTT 和 1RTT 请求中分别多了一次 RSA 计算，这个多余的 RSA 计算是可以去掉的。优化后，0RTT 和 1RTT 的 RSA 计算分别为 0 次和 1 次。
2. 大文件下载中服务端会收到并处理大量的 ACK。在 ACK 处理上，并不需要接收一个处理一个。可以将一轮中所有的 ACK 解析后再同时进行处理。
3. 一次发包大小尽可能接近 MTU，QUIC 协议本身也提供了 MTU 探测的特性。
4. 尽可能减少协议栈的内存拷贝。

下图是小文件 0RTT 请求场景中，协议栈优化前和优化后的性能对比：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxQ64bRvaMOhYqTUrSlXSNcLryvJnEbtsIib6hsSjtia9ZQVQqKunbbiaMQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)优化前

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxSFicTVxv7TfQIXYE10VWSibgOVsBEEibEDicfcmY0soDWd3wL9icJ4eWAaQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

优化后

**4.4 QUIC 性能优化小结**

目前，我们将 QUIC 协议栈无缝接入到了高性能转发模块 NGINX 与 LEGO。在小包请求上，QUIC 的性能开销基本可以达到 HTTPS 的 90%以上。如果 QUIC 使用加速卡做 RSA OFFLOAD，性能甚至比原生的 HTTPS 强。在大包请求上，优化过后的 QUIC CPU 性能可以达到 HTTPS 70%，但在大部分机型中，大文件请求通常都是网卡先到达瓶颈。总的来说，QUIC 目前的性能问题做大规模部署已经不在是大问题。当然，这里面仍然存在优化空间，我们也为此做继续的优化之中。

## 5.**QUIC 的 0RTT 优化**

下图展示了一个 HTTPS 请求与一个 QUIC 请求的对比。可以看出，一个完全握手的 HTTPS 请求，在 HTTP 请求正式发出时经历了 3 个 RTT。而 QUIC 请求可以做到发 HTTP 请求之前的 0RTT 消耗。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwx91RaNRWQic9mCxO4PGDQ4MCwq2icwbDDXCKTX90xa9GyOuhwfjnGmB8g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

为什么 QUIC 可以做到 0RTT 呢？这里分为 QUIC 握手协议和 IETF QUIC 的 TLS1.3 协议。我们以 GQUIC 握手为例，如下图：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwx6QsGdNF2Ortdw0wicNVvskEWzwJuhQQL0CJ5G5plMobPcJLDibXcCylQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

用户第一个 QUIC 包时发送一个没有带 server config 的 client hello，这个时候 server 回一个 REJ 的包，包含 server config 等信息。用户后续带上 server config 继续发 client hello，服务端会回 server hello。此时握手建立成功。

QUIC 加密握手基于 DH 的非对称秘钥交换算法进行秘钥交换，Server config 包含服务端的算法与公钥等信息，客户端利用服务端的公钥和自己的公私钥计算协商出连接的对称秘钥。

因此，第一次请求，客户端在没有保存服务端 server config 信息时，需要 1RTT 请求来完成第一次 QUIC 请求。而在后续请求中，客户端可以直接带上之前的 server config 来完成 0RTT 请求。

所以，这里的关键是：如何提升 0RTT 的比例。一种典型的场景就是，同一个用户在第一次 1RTT 请求获取到的 server config 信息，在后续多次请求中，不论路由到哪台 7 层 STGW 服务器，都能够尽可能的处理对应的 server config 信息。对此，我们尝试过很多方案，主要有：

1）4 层通过会话保持将同一个 IP 尽可能转发到同一个 7 层 STGW 服务器。这样的缺点是：1 用户的 ip 可能发生变化，2 四层基于 IP 的会话保持和基于连接 ID 的会话保持冲突，这可能导致 0RTT 提升的同时，连接迁移特性可能无法使用。

2）类似于 HTTPS 的分布式 session cache，同一个集群通过远端模块共享 server config 信息。这需要额外引进新的模块，并且会带来一定的延时。

3）类似于 session ticket，支持分布式无状态的 server config 生成。实际过程中可以根据日期和参数生成多组 SCFG，进一步提高可用性和安全性。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxNDfFofsXqvgdaYsbRVAW93lbxmgCPUwicS7KliaMrRbnx5Nk96wiaFLrg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

关于 0RTT 的优化目前我们做了不少工作，对于一些不敏感的数据传输，我们可以做到 100% 0RTT。

## 6.**QUIC 连接迁移实现**

QUIC 连接迁移是 QUIC 协议一个很重要的特点。QUIC 使用连接 ID 唯一区分连接，以应对用户的网络突然发生变化。一种典型的场景是 4G 与 wifi 之间的切换，之后用户的地址发生变化，原始的客户端 fd 已无法使用。这时只需要在客户端使用 QUIC SDK 重新创建新的 fd，并继续之前连接的发包，即可发出相同连接 ID 的包出去。

用户的 QUIC 包可能经过中间很多路径最后到达实际的业务服务器。我们以典型的腾讯业务走网关的场景分析：

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxJqXSKh6Xiap0LK787MSC3tavDvYQC1ba42CZib3qhqaAUt4ypD4aZyqQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

一次 QUIC 请求经过外网后会先到四层 TGW 集群，然后转发给七层 STGW 集群的一台服务器。到达 STGW 服务器后，包会到达一个特定的 worker 进程处理。Worker 进行 QUIC 协议卸载后使用 TCP 或 UDP 转发给具体业务的一个 RS。如果用户的 QUIC 连接在处理过程中突然源地址发生了变化，我们如何继续正确的响应和维持这个 QUIC 连接？另一个场景是：用户的源地址没有发生变化，但是 7 层 STGW 服务器需要做配置变更和升级，这时 QUIC 连接是否可以维持？

### 6.1 四层基于 QUIC 连接 ID 的会话保持

当用户网络地址发生变化时，虽然源地址变化，但是 QUIC 连接 ID 仍可保持一致。包经过中间网络后首先会到 TGW 集群。为了保证用户的地址发生变化时，QUIC 连接得以维持，TGW 集群需要做正确的转发。

TGW 集群对 QUIC 的会话保持需要考虑 GQUIC 和 IETF QUIC 不同的情况。对于 GQUIC（Q043 以下）实现起来较为简单，因为 GQUIC 协议里的连接 ID 由客户产生，并在整个连接保持不变。对于 IETF QUIC，连接 ID 由客户端和服务端协商产生，需要考虑 long header 包和 short header 包等不同的场景。下图为 IETF 连接 ID 的协商过程以及 GQUIC 和 IETF QUIC 不同类型包的抓包分析。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwxc84LTFDJJ5kEZGIr99wkuiat6y2W1fLSvpdoPPytm1DeUgIrriar6RyQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)IETF 下连接 ID 的协商过程

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxYEwrSdHqfLJnQ0Ne7s5MWEfDf5uznkYO7PfF6NcFfHyrjpNG4Fyy8g/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)GQUIC包

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxnZq9OdCpLau3elV1J6MHicAu6LzKqE4LOLKwX0MGZFbNmKSdiaKG0bMQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxtN5SOzicKicwMy3n2P83CnfLWibPObZByKEHW1mbIiccTD07EPqGyZicr7Q/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)IETF不同类型包

目前 TGW 集群已经支持 QUIC 的会话保持，基本原理是在同集群不同的 TGW 服务器之间同步 QUIC 连接信息，同时能够区分不同 QUIC 协议，将相同 QUIC 连接 ID 的包转发到相同的 7 层 STGW 服务器去。

### 6.2 七层单机多核的连接迁移

当包到达 STGW 服务器时，由于 STGW 服务器多核转发，此时还需要将 QUIC 包转发到同一个进程(或线程)去处理。当前，7 层网络框架一般使用多核+REUSEPORT 模型来提供高性能转发能力。对于 QUIC 服务，上层不同的进程在同一个 UDP 端口使用 REUSEPORT 监听。LINUX 内核默认基于 4 元组 hash，因此原生情况下，不同源地址的包是无法保证到达同一个进程的。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwx9855WHzJnibCTcUeBqjevl0p1tYq6GMpVoIKlfSUFEZVIcqJhVlQj2w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

EBPF 在内核 4.19 引入 BPF_PROG_TYPE_SK_REUSEPORT 这个 HOOK，可以策略性的控制一个包到达后发往到哪一个 accept 队列。这使得使用 EBPF 可以实现因为用户源地址变化引起的 QUIC 连接迁移。具体方法是在 EBPF 的 REUSEPORT 钩子处，解析 QUIC 包以及连接 ID，根据 QUIC 连接 ID 将包转发到对应的 worker 去。

### 6.3 配置加载和热升级连接保持

QUIC 连接迁移还有一种典型的场景是配置加载和热升级：当 STGW 服务器进程配置变更或者进行模块升级时，原生 NGINX 对 TCP 是可以保持连接不中断的。但是对于基于 UDP 的 QUIC，在未经过优化的情况下，我们无法在配置变更和模块升级过程中保持包的正常转发。

以 NGINX 的配置和热升级变更为例，NGINX 在配置变更和热升级时，会产生新的一组 worker，同时老的 worker 进入 shutting down 状态，而老的连接状态都在老的 worker 中。此时新老 worker 共用一组 fd。若老的 worker 关闭 fd 监听，则对于老的请求的连接都会超时。若老的 worker 继续监听 fd，则存在新老 worker 惊群读同一个 fd 的问题，这使得任意新老连接的包可能会到任意一个 worker，对新老连接都存在影响。

STGW 作为一个平台，每天的配置变更需求非常多，某些集群甚至达到了几秒一次配置变更。虽然我们实现了动态配置加载可以做到绝大部分场景不需要 reload 程序，但是仍然有少部分程序要 reload。同时，热升级这种场景也是比较常见的。如果配置 reload 或者模块升级就导致存量 QUIC 连接超时或中断，必然会对业务产生很大影响。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxuCdLzQkOhhoAyoiayViaOvujZU0J8L1LZLfuaLkMPXKuzhKScJXCf6oQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)NGINX配置变更和热升级时worker的收包

那么，如何解决这个问题呢？

基于内核的 EBPF 方案可以较好的处理 4G 与 WIFI 切换的场景，但是对于 STGW 服务器配置变更和模块升级的场景，却很难实现。

为此，STGW 使用了基于共享内存的 QUIC 连接迁移方案，使用共享内存管理不同进程的所有连接信息。同时为每个 worker 设定了一个 packet queue 用于接收来自别的进程连接迁移的包的转发。

可以说，目前 STGW 完全支持 4G 与 WIFI 互切的 QUIC 连接迁移场景。同时对于线上大规模运营来说，持续的配置变更和模块升级也不会影响 QUIC 连接的保持。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxdNoAVhtdb0EbIKcCWUZ2OXKNDKEjvDCdYXIibGQN3hZhicNolGT9AQfw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)STGW基于共享内存的连接迁移和连接保持方案

### 6.4 连接迁移的应用场景

一切重连开销很大的场景可以说都是 QUIC 连接迁移的使用场景。

例如游戏，视频以及业务信道传输就是比较典型的场景。当用户在 WIFI 网络切换到 4G 时，使用原始的 TCP 方案，在网络切换后会有一个连接重建过程。一般重连后，业务会有一些初始化操作，这会消耗几个甚至十几个 RTT，现象就是应用的卡顿或者菊花旋转。在使用 QUIC 连接迁移功能后，可以保证在 WIFI 与 4G 网络切换过程中，连接的正确迁移和存活，无需建立新的连接，从而使得业务的流畅度在网络切换时会得到很大提升。。

## 7.**灵活的拥塞算法与 TCP 重定义**

TCP 拥塞控制算法的目的可以简单概括为：充分利用网络带宽、降低网络延时、优化用户体验。然而就目前而言要实现这些目标就难免有权衡和取舍。LINUX 的拥塞控制算法经过很多次迭代，主流都是使用的 CUBIC 算法。在 Linux4.19 内核后，拥塞控制算法从 CUBIC 改为了 BBR。

BBR 算法相比之前拥塞控制算法，进行了非常重大的改变。BBR 通过实时计算带宽和最小 RTT 来决定发送速率 pacing rate 和窗口大小 cwnd。BBR 主要致力于：

1）在有一定丢包率的网络链路上充分利用带宽。

2）降低网络链路上的 buffer 占用率，从而降低延迟。

BBR 完全摒弃丢包作为拥塞控制的直接反馈因素，这也导致其对丢包并不是非常敏感。通过测试我们得出，在模拟一定概率丢包的网络情况下，对 QUIC 大文件的请求，BBR 的下载性能会比 CUBIC 更好。

QUIC 将拥塞控制做在了应用层，这也使得我们能够灵活的选择不同的拥塞控制算法。目前我们在 QUIC 上支持常见的 CUBIC,BBR 等算法，并实现了业务的自主配置，根据不同业务，针对请求的不同 VIP 使用不同的拥塞控制算法。同时，我们也支持针对同一个业务不同的用户的 RTT 动态的选择拥塞控制算法。另外，我们也同 CDN 的拥塞控制算法团队密切合作，以优化拥塞控制算法在不同场景下的业务体验。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41BwxAibXTooLzSyeqslED3SiaPvb9icXgU5MEFrxiatnKxGH4YIo4IzNSCEutw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)业务可配的拥塞控制算法

除了拥塞控制，基于 QUIC 也可以在传输层针对特定的应用场景去不同的定制化。QUIC 将 TCP 的特性带到了应用层，这也使得在传输层上，我们有更多的操作可能性。例如，参照音视频领域一些常见的用法，发送数据时发送冗余数据，在一定丢包情况下，QUIC 传输层可以自动恢复数据，而不需要等待数据包的重传，以降低音视频的卡顿率。这些基于 TCP 是很难做到的。业务如果需要重定义 TCP 的一些功能或特性，来提升业务体验，QUIC 将会有很大的发挥空间。

## 8.**支持 QUIC 私有协议**

STGW 作为 7 层网关，提供通用 WEB 协议卸载和转发。因此，支持 QUIC 的 WEB 协议如 GQUIC,HTTP/3 是我们的基本能力。

但是，如前面所说，QUIC 作为通用传输层协议，不仅仅应用于 WEB，任何私有协议都可以改造到 QUIC 下。使用 QUIC 握手协议之后，客户端就可以根据自己的业务需求，发送 GQUIC,HTTP/3 等 WEB 请求，或者可以发送任意自己的私有协议。

STGW 基于 NGINX 的 STREAM 模块，对其进行深度改造，使得任意私有协议都可以跑在 QUIC 协议之下。这也大大增加了 QUIC 的应用场景。

![图片](https://mmbiz.qpic.cn/mmbiz_png/j3gficicyOvauGdmAUPqq2d1a1wDD41Bwxp0ibxxI1J8hnw7Goz8685YlKn0micnaQGicswKIwo30ehXdyib6CwvEcPg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

所以，不要觉得不是 HTTP 协议就用不了 QUIC。只要你理解了 QUIC 的特性，并且觉得 QUIC 的特性能够优化业务体验，那么，来试试 QUIC 吧。

## 9.**QUIC 定制化 SDK**

由于 QUIC 尚未标准化，当前使用 QUIC 相对来说门槛较高。

客户端方面，由于 Google 将其浏览器以 Chromium 项目开源出来，其网络协议栈 Cronet 成为业界 QUIC 客户端的主要参考对象。但 Cronet 因为 API 支持有限，代码复杂，难以满足个性化需求等，不适合直接用在我们的移动客户端上。同时，QUIC 作为一个还在高速发展的协议，服务端和客户端都在快速迭代，需要保持紧密的跟进。

基于上述痛点以及 QUIC 渐渐流行的趋势，我们提供比 Google QUIC 更定制化的 TQUIC SDK。TQUIC SDK 相比 Cronet，有体积更加轻量，简单易用，支持私有协议，连接迁移等诸多优点。目前，TQUIC SDK 已应用于公司内部多个业务之中。

## 10.**总结**

本文综合介绍了 STGW 在大规模应用 QUIC 协议过程中做的一些优化和成果。当前：

1. 我们将 QUIC 协议栈与高性能网络框架做了深度融合，并支持 QUIC WEB 协议，QUIC 私有协议，带外拥塞控制配置等大部分 QUIC 功能和特性。满足 QUIC 大规模部署与运营。
2. 我们对 QUIC 协议栈 0RTT，1RTT，小包，高带宽等多场景做了大量的性能优化，解决了 QUIC 严重消耗 CPU 资源的几个瓶颈。在小包请求上性能基本可以达到 HTTPS 的 90%。
3. 针对 RTT 敏感的短连接业务，我们大大提升了 0RTT 的比例，某些场景可以做到 100% 0RTT。
4. 更全面的连接迁移，解决了 4 层，7 层，多集群、多机器、多进程以及进程重启、重加载，模块升级等各种场景下的连接迁移问题。
5. 我们提供了定制化的 QUIC SDK，以用于客户端满足定制化 QUIC 的各种特性。

QUIC 仍然有很多特性需要充分挖掘，如 QUIC 本身基于 UDP 没有队头阻塞特性以及 QPACK 编码在 HTTP/3 的 HTTP 头部上对队头阻塞的优化等。这些特性在弱网环境下对业务都会有较好的性能提升和卡顿率降低，特别是多路复用场景。目前我们也在结合业务积累更多的实际数据，并期望在这块能够有更多的优化。

QUIC 以及相关的 HTTP/3 等协议即将形成最终的标准，我们也在不断跟进 QUIC 协议的演进。

STGW 将持续为自研业务和腾讯云 CLB 客户提供 QUIC 的统一接入和优化，帮助业务更好的提升用户体验。

原文作者：wentaomao，腾讯 TEG 后台开发工程师

原文链接：https://mp.weixin.qq.com/s/ciR-1N4z0zvGOJSoyrvMUA

# 【NO.181】linux：零拷贝（ zero-copy ）技术原理详解

## 1.引言

传统的 Linux 操作系统的标准 I/O 接口是基于数据拷贝操作的，即 I/O 操作会导致数据在操作系统内核地址空间的缓冲区和应用程序地址空间定义的缓冲区之间进行传输。这样做最大的好处是可以减少磁盘 I/O 的操作，因为如果所请求的数据已经存放在操作系统的高速缓冲存储器中，那么就不需要再进行实际的物理磁盘 I/O 操作。但是数据传输过程中的数据拷贝操作却导致了极大的 CPU 开销，限制了操作系统有效进行数据传输操作的能力。

零拷贝（ zero-copy ）技术可以有效地改善数据传输的性能，在内核驱动程序（比如网络堆栈或者磁盘存储驱动程序）处理 I/O 数据的时候，零拷贝技术可以在某种程度上减少甚至完全避免不必要 CPU 数据拷贝操作。

## 2.什么是零拷贝？

零拷贝就是一种避免 CPU 将数据从一块存储拷贝到另外一块存储的技术。针对操作系统中的设备驱动程序、文件系统以及网络协议堆栈而出现的各种零拷贝技术极大地提升了特定应用程序的性能，并且使得这些应用程序可以更加有效地利用系统资源。这种性能的提升就是通过在数据拷贝进行的同时，允许 CPU 执行其他的任务来实现的。

零拷贝技术可以减少数据拷贝和共享总线操作的次数，消除传输数据在存储器之间不必要的中间拷贝次数，从而有效地提高数据传输效率。而且，零拷贝技术减少了用户应用程序地址空间和操作系统内核地址空间之间因为上下文切换而带来的开销。进行大量的数据拷贝操作其实是一件简单的任务，从操作系统的角度来说，如果 CPU 一直被占用着去执行这项简单的任务，那么这将会是很浪费资源的；如果有其他比较简单的系统部件可以代劳这件事情，从而使得 CPU 解脱出来可以做别的事情，那么系统资源的利用则会更加有效。综上所述，零拷贝技术的目标可以概括如下：

避免数据拷贝

①避免操作系统内核缓冲区之间进行数据拷贝操作。
②避免操作系统内核和用户应用程序地址空间这两者之间进行数据拷贝操作。
③用户应用程序可以避开操作系统直接访问硬件存储。
④数据传输尽量让 DMA 来做。

将多种操作结合在一起

①避免不必要的系统调用和上下文切换。
②需要拷贝的数据可以先被缓存起来。
③对数据进行处理尽量让硬件来做。

接下来就探讨Linux中**主要的几种零拷贝技术**以及零拷贝技术**适用的场景**。为了迅速建立起零拷贝的概念，我们拿一个常用的场景进行引入：

## 3.引文

在写一个服务端程序时（Web Server或者文件服务器），文件下载是一个基本功能。这时候服务端的任务是：**将服务端主机磁盘中的文件不做修改地从已连接的socket发出去**，我们通常用下面的代码完成：

```text
while((n = read(diskfd, buf, BUF_SIZE)) > 0)
    write(sockfd, buf , n);
```

基本操作就是循环的从磁盘读入文件内容到缓冲区，再将缓冲区的内容发送到`socket`。但是由于Linux的`I/O`操作默认是缓冲`I/O`。这里面主要使用的也就是`read`和`write`两个系统调用，我们并不知道操作系统在其中做了什么。实际上在以上`I/O`操作中，发生了多次的数据拷贝。

当应用程序访问某块数据时，操作系统首先会检查，是不是最近访问过此文件，文件内容是否缓存在内核缓冲区，如果是，操作系统则直接根据`read`系统调用提供的`buf`地址，将内核缓冲区的内容拷贝到`buf`所指定的用户空间缓冲区中去。如果不是，操作系统则首先将磁盘上的数据拷贝的内核缓冲区，这一步目前主要依靠`DMA`来传输，然后再把内核缓冲区上的内容拷贝到用户缓冲区中。

接下来，`write`系统调用再把用户缓冲区的内容拷贝到网络堆栈相关的内核缓冲区中，最后`socket`再把内核缓冲区的内容发送到网卡上。

说了这么多，不如看图清楚：

![img](https://pic2.zhimg.com/80/v2-636202ea28493cb1790f08fd7d14a1c1_720w.webp)

数据拷贝

从上图中可以看出，共产生了四次数据拷贝，即使使用了`DMA`来处理了与硬件的通讯，CPU仍然需要处理两次数据拷贝，与此同时，在用户态与内核态也发生了多次上下文切换，无疑也加重了CPU负担。

在此过程中，我们没有对文件内容做任何修改，那么在内核空间和用户空间来回拷贝数据无疑就是一种浪费，而零拷贝主要就是为了解决这种低效性。

## 4.什么是零拷贝技术（zero-copy）？

零拷贝主要的任务就是**避免**CPU将数据从一块存储拷贝到另外一块存储，主要就是利用各种零拷贝技术，避免让CPU做大量的数据拷贝任务，减少不必要的拷贝，或者让别的组件来做这一类简单的数据传输任务，让CPU解脱出来专注于别的任务。这样就可以让系统资源的利用更加有效。

我们继续回到引文中的例子，我们如何减少数据拷贝的次数呢？一个很明显的着力点就是减少数据在内核空间和用户空间来回拷贝，这也引入了零拷贝的一个类型：

让数据传输不需要经过user space

### 4.1使用mmap

我们减少拷贝次数的一种方法是调用mmap()来代替read调用：

```text
buf = mmap(diskfd, len);
write(sockfd, buf, len);
```

应用程序调用`mmap()`，磁盘上的数据会通过`DMA`被拷贝的内核缓冲区，接着操作系统会把这段内核缓冲区与应用程序共享，这样就不需要把内核缓冲区的内容往用户空间拷贝。应用程序再调用`write()`,操作系统直接将内核缓冲区的内容拷贝到`socket`缓冲区中，这一切都发生在内核态，最后，`socket`缓冲区再把数据发到网卡去。

同样的，看图很简单：

![img](https://pic3.zhimg.com/80/v2-fc408a7d628df3a5ef0d29a4b97316ee_720w.webp)



### 4.2mmap

使用mmap替代read很明显减少了一次拷贝，当拷贝数据量很大时，无疑提升了效率。但是使用`mmap`是有代价的。当你使用`mmap`时，你可能会遇到一些隐藏的陷阱。例如，当你的程序`map`了一个文件，但是当这个文件被另一个进程截断(truncate)时, write系统调用会因为访问非法地址而被`SIGBUS`信号终止。`SIGBUS`信号默认会杀死你的进程并产生一个`coredump`,如果你的服务器这样被中止了，那会产生一笔损失。

通常我们使用以下解决方案避免这种问题：

1. **为SIGBUS信号建立信号处理程序**
   当遇到`SIGBUS`信号时，信号处理程序简单地返回，`write`系统调用在被中断之前会返回已经写入的字节数，并且`errno`会被设置成success,但是这是一种糟糕的处理办法，因为你并没有解决问题的实质核心。
2. **使用文件租借锁**
   通常我们使用这种方法，在文件描述符上使用租借锁，我们为文件向内核申请一个租借锁，当其它进程想要截断这个文件时，内核会向我们发送一个实时的`RT_SIGNAL_LEASE`信号，告诉我们内核正在破坏你加持在文件上的读写锁。这样在程序访问非法内存并且被`SIGBUS`杀死之前，你的`write`系统调用会被中断。`write`会返回已经写入的字节数，并且置`errno`为success。

我们应该在`mmap`文件之前加锁，并且在操作完文件后解锁：

```text
if(fcntl(diskfd, F_SETSIG, RT_SIGNAL_LEASE) == -1) {
    perror("kernel lease set signal");
    return -1;
}
/* l_type can be F_RDLCK F_WRLCK  加锁*/
/* l_type can be  F_UNLCK 解锁*/
if(fcntl(diskfd, F_SETLEASE, l_type)){
    perror("kernel lease set type");
    return -1;
}
```

### 4.3使用sendfile

从2.1版内核开始，Linux引入了`sendfile`来简化操作:

```text
#include<sys/sendfile.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

系统调用`sendfile()`在代表输入文件的描述符`in_fd`和代表输出文件的描述符`out_fd`之间传送文件内容（字节）。描述符`out_fd`必须指向一个套接字，而`in_fd`指向的文件必须是可以`mmap`的。这些局限限制了`sendfile`的使用，使`sendfile`只能将数据从文件传递到套接字上，反之则不行。

使用`sendfile`不仅减少了数据拷贝的次数，还减少了上下文切换，数据传送始终只发生在`kernel space`。

![img](https://pic4.zhimg.com/80/v2-303b963b10d4e907c82ed5e0203310af_720w.webp)

### 4.4sendfile系统调用过程

在我们调用`sendfile`时，如果有其它进程截断了文件会发生什么呢？假设我们没有设置任何信号处理程序，`sendfile`调用仅仅返回它在被中断之前已经传输的字节数，`errno`会被置为success。如果我们在调用sendfile之前给文件加了锁，`sendfile`的行为仍然和之前相同，我们还会收到RT_SIGNAL_LEASE的信号。

目前为止，我们已经减少了数据拷贝的次数了，但是仍然存在一次拷贝，就是页缓存到socket缓存的拷贝。那么能不能把这个拷贝也省略呢？

借助于硬件上的帮助，我们是可以办到的。之前我们是把页缓存的数据拷贝到socket缓存中，实际上，我们仅仅需要把缓冲区描述符传到`socket`缓冲区，再把数据长度传过去，这样`DMA`控制器直接将页缓存中的数据打包发送到网络中就可以了。

总结一下，`sendfile`系统调用利用`DMA`引擎将文件内容拷贝到内核缓冲区去，然后将带有文件位置和长度信息的缓冲区描述符添加socket缓冲区去，这一步不会将内核中的数据拷贝到socket缓冲区中，`DMA`引擎会将内核缓冲区的数据拷贝到协议引擎中去，避免了最后一次拷贝。

![img](https://pic1.zhimg.com/80/v2-6cf7009e5788af6d9b80ecc15faf5cbc_720w.webp)

**带DMA的sendfile**

不过这一种收集拷贝功能是需要硬件以及驱动程序支持的。

### 4.5使用splice

sendfile只适用于将数据从文件拷贝到套接字上，限定了它的使用范围。Linux在`2.6.17`版本引入`splice`系统调用，用于在两个文件描述符中移动数据：

```text
#define _GNU_SOURCE         /* See feature_test_macros(7) */
#include <fcntl.h>
ssize_t splice(int fd_in, loff_t *off_in, int fd_out, loff_t *off_out, size_t len, unsigned int flags);
```

splice调用在两个文件描述符之间移动数据，而不需要数据在内核空间和用户空间来回拷贝。他从`fd_in`拷贝`len`长度的数据到`fd_out`，但是有一方必须是管道设备，这也是目前`splice`的一些局限性。`flags`参数有以下几种取值：

- **SPLICE_F_MOVE** ：尝试去移动数据而不是拷贝数据。这仅仅是对内核的一个小提示：如果内核不能从`pipe`移动数据或者`pipe`的缓存不是一个整页面，仍然需要拷贝数据。Linux最初的实现有些问题，所以从`2.6.21`开始这个选项不起作用，后面的Linux版本应该会实现。
- ** SPLICE_F_NONBLOCK** ：`splice` 操作不会被阻塞。然而，如果文件描述符没有被设置为不可被阻塞方式的 I/O ，那么调用 splice 有可能仍然被阻塞。
- ** SPLICE_F_MORE**： 后面的`splice`调用会有更多的数据。

splice调用利用了Linux提出的管道缓冲区机制， 所以至少一个描述符要为管道。

以上几种零拷贝技术都是减少数据在用户空间和内核空间拷贝技术实现的，但是有些时候，数据必须在用户空间和内核空间之间拷贝。这时候，我们只能针对数据在用户空间和内核空间拷贝的时机上下功夫了。Linux通常利用**写时复制(copy on write)**来减少系统开销，这个技术又时常称作`COW`。

由于篇幅原因，本文不详细介绍写时复制。大概描述下就是：如果多个程序同时访问同一块数据，那么每个程序都拥有指向这块数据的指针，在每个程序看来，自己都是独立拥有这块数据的，只有当程序需要对数据内容进行修改时，才会把数据内容拷贝到程序自己的应用空间里去，这时候，数据才成为该程序的私有数据。如果程序不需要对数据进行修改，那么永远都不需要拷贝数据到自己的应用空间里。这样就减少了数据的拷贝。写时复制的内容可以再写一篇文章了。。。

除此之外，还有一些零拷贝技术，比如传统的Linux I/O中加上`O_DIRECT`标记可以直接`I/O`，避免了自动缓存，还有尚未成熟的`fbufs`技术，本文尚未覆盖所有零拷贝技术，只是介绍常见的一些，如有兴趣，可以自行研究，一般成熟的服务端项目也会自己改造内核中有关I/O的部分，提高自己的数据传输速率。

## 5.零拷贝原理

[http://1.io](https://link.zhihu.com/?target=http%3A//1.io)读写的方式

1.1中断

1.2DMA

2.中断方式

2.1中断方式的流程图如下：

![img](https://pic2.zhimg.com/80/v2-68f188488d6583878afcb220591cdd9d_720w.webp)

①用户进程发起数据读取请求

②系统调度为该进程分配cpu

③cpu向io控制器(ide,scsi)发送io请求

④用户进程等待io完成，让出cpu

⑤系统调度cpu执行其他任务

⑥数据写入至io控制器的缓冲寄存器

⑦缓冲寄存器满了向cpu发出中断信号

⑧cpu读取数据至内存

2.2缺点：中断次数取决于缓冲寄存器的大小

3.DMA ： 直接内存存取

3.1DMA方式的流程图如下：

![img](https://pic3.zhimg.com/80/v2-bfb79b2982aa6e491d71e582f578f79e_720w.webp)

①用户进程发起数据读取请求

②系统调度为该进程分配cpu

③cpu向DMA发送io请求

④用户进程等待io完成，让出cpu

⑤系统调度cpu执行其他任务

⑥数据写入至io控制器的缓冲寄存器

⑦DMA不断获取缓冲寄存器中的数据（需要cpu时钟）

⑧传输至内存（需要cpu时钟）

⑨所需的全部数据获取完毕后向cpu发出中断信号

3.2优点：减少cpu中断次数，不用cpu拷贝数据

4.数据拷贝

4.1下面展示了 传统方式读取数据后并通过网络发送 所发生的数据拷贝：

![img](https://pic4.zhimg.com/80/v2-a6e56cdfb0ff9444301fbbc96d131b13_720w.webp)

①一个read系统调用后，DMA执行了一次数据拷贝，从磁盘到内核空间

②read结束后，发生第二次数据拷贝，由cpu将数据从内核空间拷贝至用户空间

③send系统调用，cpu发生第三次数据拷贝，由cpu将数据从用户空间拷贝至内核空间(socket缓冲区)

④send系统调用结束后，DMA执行第四次数据拷贝，将数据从内核拷贝至协议引擎

⑤另外，这四个过程中，每个过程都发生一次上下文切换

4.2内存缓冲数据，主要是为了提高性能，内核可以预读部分数据，当所需数据小于内存缓冲区大小时，将极大的提高性能。

4.3零拷贝是为了消除这个过程中冗余的拷贝

5.零拷贝-sendfile 对应到java中

FileChannel.transferTo(long position, long count, WritableByteChannel target)//将数据从文件通道传输到了给定的可写字节通道

5.1避免了第2，3步的数据拷贝，参考下图：

![img](https://pic1.zhimg.com/80/v2-0fb110b4e3d2fa1637bc231683d46b70_720w.webp)

①DMA从拷贝至内核缓冲区

②cpu将数据从内核缓冲区拷贝至内核空间(socket缓冲区)

③DMA将数据从内核拷贝至协议引擎

④这三个过程中共发生2次上下文切换，分别为发起读取文件和发送数据

5.2以上过程发生了三次数据拷贝，其中有一次为cpu完成

5.3linux内核2.4以后，socket缓冲区做了调整，DMA带收集功能，如下图：

![img](https://pic2.zhimg.com/80/v2-a041c34c1f094d2911bf09556a060d45_720w.webp)

①DMA从拷贝至内核缓冲区

②将数据的位置和长度的信息的描述符增加至内核空间(socket缓冲区)

③DMA将数据从内核拷贝至协议引擎

6.零拷贝-mmap 对应到java中

MappedByteBuffer//文件内存映射

6.1数据不会复制到用户空间，只在内核空间，与sendfile类似，但是应用程序可以直接操作该内存。



原文地址：https://zhuanlan.zhihu.com/p/558465064

作者：linux

# 【NO.182】【源码剖析】MemoryPool —— 简单高效的内存池 allocator 实现

## 1.什么是内存池？什么是 C++ 的 allocator？

内存池简单说，是为了减少频繁使用 malloc/free new/delete 等系统调用而造成的性能损耗而设计的。当我们的程序需要频繁地申请和释放内存时，频繁地使用内存管理的系统调用可能会造成性能的瓶颈，嗯，是可能，毕竟操作系统的设计也不是盖的（麻麻说把话说太满会被打脸的(⊙v⊙)）。内存池的思想是申请较大的一块内存（不够时继续申请），之后把内存管理放在应用层执行，减少系统调用的开销。

那么，allocator 呢？它默默的工作在 C++ 所有容器的内存分配上。默默贴几个链接吧：

- [http://www.cnblogs.com/wpcockroach/archive/2012/05/10/2493564.html](https://link.zhihu.com/?target=http%3A//www.cnblogs.com/wpcockroach/archive/2012/05/10/2493564.html)
- [http://blog.csdn.net/justaipanda/article/details/7790355](https://link.zhihu.com/?target=http%3A//blog.csdn.net/justaipanda/article/details/7790355)
- [http://www.cplusplus.com/reference/memory/allocator/](https://link.zhihu.com/?target=http%3A//www.cplusplus.com/reference/memory/allocator/)
- [http://www.cplusplus.com/reference/memory/allocator_traits/](https://link.zhihu.com/?target=http%3A//www.cplusplus.com/reference/memory/allocator_traits/)

当你对 allocator 有基本的了解之后，再看这个项目应该会有恍然大悟的感觉，因为这个内存池是以一个 allocator 的标准来实现的。一开始不明白项目里很多函数的定义是为了什么，结果初步了解了 allocator 后才知道大部分是标准接口。这样一个 memory pool allocator 可以与大多数 STL 容器兼容，也可以应用于你自定义的类。像作者给出的例子 —— test.cpp， 是用一个基于自己写的 stack 来做 memory pool allocator 和 std::allocator 性能的对比 —— 最后当然是 memory pool allocator 更优。

## 2.项目

Github：[MemoryPool](https://link.zhihu.com/?target=https%3A//github.com/cacay/MemoryPool)

## 3.基本使用

因为这是一个 allocator 类，所以所有使用 std::allocator 的地方都可以使用这个 MemoryPool。在项目的 test.cpp 中，MemoryPool 作为 allocator 用于 StackAlloc（作者实现的 demo 类） 的内存管理类。定义如下：

```text
StackAlloc<int, MemoryPool<int> > stackPool;
```

其次，你也可以将其直接作为任一类型的内存池，用 newElement 创建新元素，deleteElement 释放元素，就像 new/delete 一样。用下面的例子和 new/delete 做对比：

```text
#include <iostream>
#include <cassert>
#include <time.h>
#include <vector>
#include <stack>

#include "MemoryPool.h"

using namespace std;

/* Adjust these values depending on how much you trust your computer */
#define ELEMS 1000000
#define REPS 50

int main()
{

    clock_t start;

    MemoryPool<size_t> pool;
    start = clock();
    for(int i = 0;i < REPS;++i)
    {
        for(int j = 0;j< ELEMS;++j)
        {
            // 创建元素
            size_t* x = pool.newElement();

            // 释放元素
            pool.deleteElement(x);
        }
    }
    std::cout << "MemoryPool Time: ";
    std::cout << (((double)clock() - start) / CLOCKS_PER_SEC) << "\n\n";


    start = clock();
    for(int i = 0;i < REPS;++i)
    {
        for(int j = 0;j< ELEMS;++j)
        {
            size_t* x = new size_t;

            delete x;
        }
    }
    std::cout << "new/delete Time: ";
    std::cout << (((double)clock() - start) / CLOCKS_PER_SEC) << "\n\n";

    return 0;

}
```

运行的结果是：

```text
MemoryPool Time: 1.93389

new/delete Time: 4.64903
```

嗯，内存池快了一倍多。如果是自定义的类的话这个差距应该还会更大一点。

## 4.代码分析：

项目的实现有 C++11 和 C++98 两个版本，C++11 版本似乎更加高效，不过个人 C++11 了解不多，就以 C++98 版本来分析吧。

### 4.1 主要函数

- allocate 分配一个对象所需的内存空间
- deallocate 释放一个对象的内存（归还给内存池，不是给操作系统）
- construct 在已申请的内存空间上构造对象
- destroy 析构对象
- newElement 从内存池申请一个对象所需空间，并调用对象的构造函数
- deleteElement 析构对象，将内存空间归还给内存池
- allocateBlock 从操作系统申请一整块内存放入内存池

### 4.2 关键知识点

理解项目的关键在于理解 placement new 和 union 的用法.

placement new: [http://blog.csdn.net/zhangxinrun/article/details/5940019](https://link.zhihu.com/?target=http%3A//blog.csdn.net/zhangxinrun/article/details/5940019)

union：[http://www.cnblogs.com/BeyondTechnology/archive/2010/09/19/1831293.html](https://link.zhihu.com/?target=http%3A//www.cnblogs.com/BeyondTechnology/archive/2010/09/19/1831293.html)

关于 union 的使用觉得好巧妙，这是相应的定义：

```text
    union Slot_ {
      value_type element;
      Slot_* next;
    };
```

Slot_ 在创建对象的时候存放对象的值，当这个对象被释放时这块内存作为一个 Slot_* 指针放入 free 的链表。所以 Slot_ 既可以用来存放对象，又可以用来构造链表。

### 4.3 工作原理

内存池是一个一个的 block 以链表的形式连接起来，每一个 block 是一块大的内存，当内存池的内存不足的时候，就会向操作系统申请新的 block 加入链表。还有一个 freeSlots_ 的链表，链表里面的每一项都是对象被释放后归还给内存池的空间，内存池刚创建时 freeSlots_ 是空的，之后随着用户创建对象，再将对象释放掉，这时候要把内存归还给内存池，怎么归还呢？就是把指向这个对象的内存的指针加到 freeSlots_ 链表的前面（前插）。

用户在创建对象的时候，先检查 freeSlots_ 是否为空，不为空的时候直接取出一项作为分配出的空间。否则就在当前 block 内取出一个 Slot_ 大小的内存分配出去，如果 block 里面的内存已经使用完了呢？就向操作系统申请一个新的 block。

内存池工作期间的内存只会增长，不释放给操作系统。直到内存池销毁的时候，才把所有的 block delete 掉。

### 4.4 注释源码

[点我到 Github](https://link.zhihu.com/?target=https%3A//github.com/AngryHacker/code-with-comments/tree/master/memorypool)

 **头文件**

```text
/*-
 * Copyright (c) 2013 Cosku Acay, http://www.coskuacay.com
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 */

#ifndef MEMORY_POOL_H
#define MEMORY_POOL_H

#include <limits.h>
#include <stddef.h>

template <typename T, size_t BlockSize = 4096>
class MemoryPool
{
  public:
    /* Member types */
    typedef T               value_type;       // T 的 value 类型
    typedef T*              pointer;          // T 的 指针类型
    typedef T&              reference;        // T 的引用类型
    typedef const T*        const_pointer;    // T 的 const 指针类型
    typedef const T&        const_reference;  // T 的 const 引用类型
    typedef size_t          size_type;        // size_t 类型
    typedef ptrdiff_t       difference_type;  // 指针减法结果类型

    template <typename U> struct rebind {
      typedef MemoryPool<U> other;
    };

    /* Member functions */
    /* 构造函数 */
    MemoryPool() throw();
    MemoryPool(const MemoryPool& memoryPool) throw();
    template <class U> MemoryPool(const MemoryPool<U>& memoryPool) throw();

    /* 析构函数 */
    ~MemoryPool() throw();

    /* 元素取址 */
    pointer address(reference x) const throw();
    const_pointer address(const_reference x) const throw();

    // Can only allocate one object at a time. n and hint are ignored
    // 分配和收回一个元素的内存空间
    pointer allocate(size_type n = 1, const_pointer hint = 0);
    void deallocate(pointer p, size_type n = 1);

    // 可达到的最多元素数
    size_type max_size() const throw();

    // 基于内存池的元素构造和析构
    void construct(pointer p, const_reference val);
    void destroy(pointer p);

    // 自带申请内存和释放内存的构造和析构
    pointer newElement(const_reference val);
    void deleteElement(pointer p);

  private:
    // union 结构体,用于存放元素或 next 指针
    union Slot_ {
      value_type element;
      Slot_* next;
    };

    typedef char* data_pointer_;  // char* 指针，主要用于指向内存首地址
    typedef Slot_ slot_type_;     // Slot_ 值类型
    typedef Slot_* slot_pointer_; // Slot_* 指针类型

    slot_pointer_ currentBlock_;  // 内存块链表的头指针
    slot_pointer_ currentSlot_;   // 元素链表的头指针
    slot_pointer_ lastSlot_;      // 可存放元素的最后指针
    slot_pointer_ freeSlots_;     // 元素构造后释放掉的内存链表头指针

    size_type padPointer(data_pointer_ p, size_type align) const throw();  // 计算对齐所需空间
    void allocateBlock();  // 申请内存块放进内存池
   /*
    static_assert(BlockSize >= 2 * sizeof(slot_type_), "BlockSize too small.");
    */
};

#include "MemoryPool.tcc"

#endif // MEMORY_POOL_H
```

**实现文件**

```text
/*-
 * Copyright (c) 2013 Cosku Acay, http://www.coskuacay.com
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and associated documentation files (the "Software"),
 * to deal in the Software without restriction, including without limitation
 * the rights to use, copy, modify, merge, publish, distribute, sublicense,
 * and/or sell copies of the Software, and to permit persons to whom the
 * Software is furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 */

#ifndef MEMORY_BLOCK_TCC
#define MEMORY_BLOCK_TCC

// 计算对齐所需补的空间
template <typename T, size_t BlockSize>
inline typename MemoryPool<T, BlockSize>::size_type
MemoryPool<T, BlockSize>::padPointer(data_pointer_ p, size_type align)
const throw()
{
  size_t result = reinterpret_cast<size_t>(p);
  return ((align - result) % align);
}

/* 构造函数，所有成员初始化 */
template <typename T, size_t BlockSize>
MemoryPool<T, BlockSize>::MemoryPool()
throw()
{
  currentBlock_ = 0;
  currentSlot_ = 0;
  lastSlot_ = 0;
  freeSlots_ = 0;
}

/* 复制构造函数,调用 MemoryPool 初始化*/
template <typename T, size_t BlockSize>
MemoryPool<T, BlockSize>::MemoryPool(const MemoryPool& memoryPool)
throw()
{
  MemoryPool();
}

/* 复制构造函数,调用 MemoryPool 初始化*/
template <typename T, size_t BlockSize>
template<class U>
MemoryPool<T, BlockSize>::MemoryPool(const MemoryPool<U>& memoryPool)
throw()
{
  MemoryPool();
}

/* 析构函数，把内存池中所有 block delete 掉 */
template <typename T, size_t BlockSize>
MemoryPool<T, BlockSize>::~MemoryPool()
throw()
{
  slot_pointer_ curr = currentBlock_;
  while (curr != 0) {
    slot_pointer_ prev = curr->next;
    // 转化为 void 指针，是因为 void 类型不需要调用析构函数,只释放空间
    operator delete(reinterpret_cast<void*>(curr));
    curr = prev;
  }
}

/* 返回地址 */
template <typename T, size_t BlockSize>
inline typename MemoryPool<T, BlockSize>::pointer
MemoryPool<T, BlockSize>::address(reference x)
const throw()
{
  return &x;
}

/* 返回地址的 const 重载*/
template <typename T, size_t BlockSize>
inline typename MemoryPool<T, BlockSize>::const_pointer
MemoryPool<T, BlockSize>::address(const_reference x)
const throw()
{
  return &x;
}

// 申请一块空闲的 block 放进内存池
template <typename T, size_t BlockSize>
void
MemoryPool<T, BlockSize>::allocateBlock()
{
  // Allocate space for the new block and store a pointer to the previous one
  // operator new 申请对应大小内存，返回 void* 指针
  data_pointer_ newBlock = reinterpret_cast<data_pointer_>
                           (operator new(BlockSize));
  // 原来的 block 链头接到 newblock
  reinterpret_cast<slot_pointer_>(newBlock)->next = currentBlock_;
  // 新的 currentblock_
  currentBlock_ = reinterpret_cast<slot_pointer_>(newBlock);
  // Pad block body to staisfy the alignment requirements for elements
  data_pointer_ body = newBlock + sizeof(slot_pointer_);
  // 计算为了对齐应该空出多少位置
  size_type bodyPadding = padPointer(body, sizeof(slot_type_));
  // currentslot_ 为该 block 开始的地方加上 bodypadding 个 char* 空间
  currentSlot_ = reinterpret_cast<slot_pointer_>(body + bodyPadding);
  // 计算最后一个能放置 slot_type_ 的位置
  lastSlot_ = reinterpret_cast<slot_pointer_>
              (newBlock + BlockSize - sizeof(slot_type_) + 1);
}

// 返回指向分配新元素所需内存的指针
template <typename T, size_t BlockSize>
inline typename MemoryPool<T, BlockSize>::pointer
MemoryPool<T, BlockSize>::allocate(size_type, const_pointer)
{
  // 如果 freeSlots_ 非空，就在 freeSlots_ 中取内存
  if (freeSlots_ != 0) {
    pointer result = reinterpret_cast<pointer>(freeSlots_);
    // 更新 freeSlots_
    freeSlots_ = freeSlots_->next;
    return result;
  }
  else {
    if (currentSlot_ >= lastSlot_)
      // 之前申请的内存用完了，分配新的 block
      allocateBlock();
    // 从分配的 block 中划分出去
    return reinterpret_cast<pointer>(currentSlot_++);
  }
}

// 将元素内存归还给 free 内存链表
template <typename T, size_t BlockSize>
inline void
MemoryPool<T, BlockSize>::deallocate(pointer p, size_type)
{
  if (p != 0) {
    // 转换成 slot_pointer_ 指针，next 指向 freeSlots_ 链表
    reinterpret_cast<slot_pointer_>(p)->next = freeSlots_;
    // 新的 freeSlots_ 头为 p
    freeSlots_ = reinterpret_cast<slot_pointer_>(p);
  }
}

// 计算可达到的最大元素上限数
template <typename T, size_t BlockSize>
inline typename MemoryPool<T, BlockSize>::size_type
MemoryPool<T, BlockSize>::max_size()
const throw()
{
  size_type maxBlocks = -1 / BlockSize;
  return (BlockSize - sizeof(data_pointer_)) / sizeof(slot_type_) * maxBlocks;
}

// 在已分配内存上构造对象
template <typename T, size_t BlockSize>
inline void
MemoryPool<T, BlockSize>::construct(pointer p, const_reference val)
{
  // placement new 用法，在已有内存上构造对象，调用 T 的复制构造函数，
  new (p) value_type (val);
}

// 销毁对象
template <typename T, size_t BlockSize>
inline void
MemoryPool<T, BlockSize>::destroy(pointer p)
{
  // placement new 中需要手动调用元素 T 的析构函数
  p->~value_type();
}

// 创建新元素
template <typename T, size_t BlockSize>
inline typename MemoryPool<T, BlockSize>::pointer
MemoryPool<T, BlockSize>::newElement(const_reference val)
{
  // 申请内存
  pointer result = allocate();
  // 在内存上构造对象
  construct(result, val);
  return result;
}

// 删除元素
template <typename T, size_t BlockSize>
inline void
MemoryPool<T, BlockSize>::deleteElement(pointer p)
{
  if (p != 0) {
    // placement new 中需要手动调用元素 T 的析构函数
    p->~value_type();
    // 归还内存
    deallocate(p);
  }
}

#endif // MEMORY_BLOCK_TCC
```

原文地址：https://zhuanlan.zhihu.com/p/558263613

作者：linux

# 【NO.183】STL 红黑树源码分析

最近我们项目上因为STL的使用出了很多问题，尤其是对map的使用。map的底层与set一样同为STL红黑树，所以抽空将STL红黑树的源代码再学习学习。

## 1. STL红黑树的节点

STL的红黑树也属于红黑树（红黑树是一种自平衡的二叉查找树），所以具备普通红黑树的5个特性：

1. **每个节点或是红色的或是黑色的**
2. **根节点是黑色的**
3. **每个叶节点（NULL）是黑色的**
4. **如果一个节点是红色的，则它的两个孩子节点都是黑色的（从每个叶子到根的所有路径上不能有两个连续的红色节点）**
5. **对每个节点，从该节点到其所有后代叶节点的简单路径上，均包含相同数目的黑色节点**

STL的红黑树和普通的红黑树相比较，多了以下两点特性：

1. **header不仅指向root，也指向红黑树的最左节点，以便用常数时间实现begin()，并且也指向红黑树的最右边节点，以便 set相关泛型算法（如set_union等等）可以有线性时间表现**
2. **当要删除的节点有两个子节点时，其后继节点连接到其位置，而不是被复制，因此，唯一使无效的迭代器是引用已删除节点的迭代器**

可以看出，**相比于普通的红黑树多了一个header节点，并且为红色**。如下图所示，普通的红黑树是以100节点开始的，而STL的红黑树以header节点开始。迭代器的begin指向红黑树根节点，也就是header节点的父亲，而end指向header节点。

![img](https://pic1.zhimg.com/80/v2-147ee77160c2d36dc9006c5faff7313c_720w.webp)

**注：此处的begin()和end()并不是指内置函数，而是指迭代器。begin()指向最左节点，图中标注的有错**

STL红黑树的数据存储类型为链式存储，因此存储单元为节点。因此，我们先看一下节点的定义。

如下所示为红黑树节点基类_Rb_tree_node_base的源代码，可以看出定义了颜色标记_Rb_tree_color。基类中分别声明了_M_left、_M_right、_M_parent三个指针成员同时声明了一个颜色标记常量_M_color成员。

```text
// 颜色标记
enum _Rb_tree_color { _S_red = false, _S_black = true };

// 基类，用来定义一个节点的属性
struct _Rb_tree_node_base
{
    // typedef重命名
    typedef _Rb_tree_node_base* _Base_ptr;
    typedef const _Rb_tree_node_base* _Const_Base_ptr;

    // 节点的属性，颜色、指向父节点的指针、指向左右孩子节点的指针
    _Rb_tree_color  _M_color;  // 颜色
    _Base_ptr       _M_parent; // 指向父亲
    _Base_ptr       _M_left;   // 指向左孩子
    _Base_ptr       _M_right;  // 指向右孩子
    
    // 求节点__x的最小节点
    static _Base_ptr
    _S_minimum(_Base_ptr __x) _GLIBCXX_NOEXCEPT
    {
        while (__x->_M_left != 0) __x = __x->_M_left;
        return __x;
    }

    // 求节点__x的最大节点
    static _Base_ptr
    _S_maximum(_Base_ptr __x) _GLIBCXX_NOEXCEPT
    {
        while (__x->_M_right != 0) __x = __x->_M_right;
        return __x;
    }
};
```

同时，节点基类里面还定义了两个重要的函数，分别是获取红黑树中__x节点的最小节点的函数_S_minimum与最大节点的函数_S_maximum。

由于STL红黑树是有序的，所以需要一个比较函数对象，该对象类型定义如下

```text
template<typename _Key_compare>
  struct _Rb_tree_key_compare
  {
      // 成员变量就是我们提供给红黑树的仿函数对象
      _Key_compare    _M_key_compare;
  }
```

当我们初始化一个空的红黑树对象的时候，也会初始化一个节点出来，该节点就是header节点，header节点的定义如下：

```text
// Helper type to manage
// default initialization of node count and header.
struct _Rb_tree_header
{
    _Rb_tree_node_base  _M_header; // 不存储数据
    size_t    _M_node_count; // 用来统计红黑树中节点的数量
    _Rb_tree_header()
    {
        _M_header._M_color = _S_red; // header节点一定是红色的
        _M_reset();
    }
    void _M_reset()
    {
        _M_header._M_parent = 0;
        _M_header._M_left = &_M_header;
        _M_header._M_right = &_M_header;
        _M_node_count = 0;
    }
}
```

红黑树节点_Rb_tree_node继承自红黑树基类_Rb_tree_node_base。可以计算一下一个红黑树节点的大小为：**4个指针+一个枚举+_Val**。

```text
template<typename _Val>
struct _Rb_tree_node : public _Rb_tree_node_base
{
    // 定义了一个节点类型的指针
    typedef _Rb_tree_node<_Value>* _Link_type;
    // 节点数据域，即节点中存储的值
    _Val _M_value_field;
};
```

## **2. STL红黑树**

STL红黑树的源代码如下所示，是一个模板类，_Key是key的类型，_Val是value的类型，_KeyOfValue是<key, value>对的类型，_Campare是比较函数对象类型，_Alloc是空间配置器的类型，默认为标准的allocator分配器。

包含了一个_Rb_tree_impl类型的成员变量_M_impl，对红黑树进行初始化操作与内存管理操作。_Rb_tree_impl继承了header节点_Rb_tree_header。

STL红黑树包含两种迭代器分别为_Rb_tree_iterator<value_type>和std::reverse_iterator<iterator>，说明STL红黑树支持rbegin和rend操作。

下面我们对rb-tree的源代码逐步进行解析。

```text
template<typename _Key, typename _Val, typename _KeyOfValue,
           typename _Compare, typename _Alloc = allocator<_Val> >
class _Rb_tree
{
public:
    typedef _Key                  key_type;
    typedef _Val                  value_type;
    typedef value_type*           pointer;
    typedef const value_type*     const_pointer;
    typedef value_type&           reference;
    typedef const value_type&     const_reference;
    typedef size_t                size_type;
    typedef ptrdiff_t             difference_type;
    typedef _Alloc                allocator_type;
    // 将节点类型_Rb_tree_node<_Val>与空间配置器_Alloc绑定
    // 空间配置器具体为配置节点空间的_Node_allocator
    typedef typename __gnu_cxx::__alloc_traits<_Alloc>::template
    rebind<_Rb_tree_node<_Val> >::other _Node_allocator;
    // 空间配置器萃取机为具体配置器_Node_allocator的萃取机
    typedef __gnu_cxx::__alloc_traits<_Node_allocator> _Alloc_traits;
protected:
    typedef _Rb_tree_node_base*     _Base_ptr;  // 节点基类指针
    typedef _Rb_tree_node<_Val>*    _Link_type; // 节点指针
   
protected:
    // 包含一个_Rb_tree_impl对象，因此一个红黑树对象至少包含
    // 一个header节点对象
    // 一个具体配置器_Node_allocator的对象
    _Rb_tree_impl<_Compare> _M_impl;

    // _Rb_tree_impl的定义如下，包含一个header节点
    template<typename _Key_compare,
       bool _Is_pod_comparator = __is_pod(_Key_compare)>
    struct _Rb_tree_impl : public _Node_allocator
                         , public _Rb_tree_key_compare<_Key_compare>
                         , public _Rb_tree_header
    {
        typedef _Rb_tree_key_compare<_Key_compare> _Base_key_compare;
        // 构造函数如下
        _Rb_tree_impl() : _Node_allocator()
        {}
        
        _Rb_tree_impl(const _Rb_tree_impl& __x)
        : _Node_allocator(_Alloc_traits::_S_select_on_copy(__x))
        , _Base_key_compare(__x._M_key_compare)
        {}
    };
public:
    typedef _Rb_tree_iterator<value_type>       iterator;
    typedef std::reverse_iterator<iterator>     reverse_iterator;
    
    // 重要的函数
    // _M_get_Node_allocator用来获取红黑树节点对象的空间配置器对象
    _Node_allocator& _M_get_Node_allocator()
    { return this->_M_impl; }
    // 为节点对象分配空间，返回值为内存地址，每次分配一个空间
    _Link_type _M_get_node()
    { return _Alloc_traits::allocate(_M_get_Node_allocator(), 1); }
    // 空间释放函数，将节点指针传进去
    void _M_put_node(_Link_type __p)
    { _Alloc_traits::deallocate(_M_get_Node_allocator(), __p, 1); }
    // 构造节点的底层函数
    template<typename... _Args>
    void _M_construct_node(_Link_type __node, _Args&&... __args)
    {
        ::new(__node) _Rb_tree_node<_Val>;
        _Alloc_traits::construct(_M_get_Node_allocator(),
                     __node->_M_valptr(),
                     std::forward<_Args>(__args)...);
    }
    // 构造节点对象，调用_M_construct_node
    template<typename... _Args>
    _Link_type _M_create_node(_Args&&... __args)
    {
        _Link_type __tmp = _M_get_node();
        _M_construct_node(__tmp, std::forward<_Args>(__args)...);
        return __tmp;
    }
    // 节点对象的析构函数
    void _M_destroy_node(_Link_type __p)
    {
        // 释放内容
        _Alloc_traits::destroy(_M_get_Node_allocator(), __p->_M_valptr());
        // 释放内存
        __p->~_Rb_tree_node<_Val>();
    }
    // 红黑树的构造函数
    _Rb_tree() = default;
    _Rb_tree(const _Compare& __comp
            , const allocator_type& __a = allocator_type())
    : _M_impl(__comp, _Node_allocator(__a)) { }
    // 红黑树的析构函数
    ~_Rb_tree()
    { _M_erase(_M_begin()); }
};
```

还有如下操作函数：

```text
// 图中100 节点
 _Base_ptr&
_M_root() _GLIBCXX_NOEXCEPT
{ return this->_M_impl._M_header._M_parent; }

// 图中most left标记
_Base_ptr&
_M_leftmost() _GLIBCXX_NOEXCEPT
{ return this->_M_impl._M_header._M_left; }

// 图中most right标记
_Base_ptr&
_M_rightmost() _GLIBCXX_NOEXCEPT
{ return this->_M_impl._M_header._M_right; }

// 图中begin()标记
_Link_type
_M_begin() _GLIBCXX_NOEXCEPT
{ return static_cast<_Link_type>(this->_M_impl._M_header._M_parent); }

// 图中end()标记
_Link_type
_M_end() _GLIBCXX_NOEXCEPT
{ return reinterpret_cast<_Link_type>(&this->_M_impl._M_header);
```

![img](https://pic1.zhimg.com/80/v2-d4cb199bf2556854440e963148e981f8_720w.webp)

## **3. STL红黑树迭代器**

STL红黑树也自定义了一个迭代器_Rb_tree_iterator。

```text
template<typename _Tp>
struct _Rb_tree_iterator
{
    typedef _Tp  value_type;
    typedef _Tp& reference;
    typedef _Tp* pointer;
    typedef bidirectional_iterator_tag iterator_category;
    typedef ptrdiff_t                  difference_type;

    typedef _Rb_tree_iterator<_Tp>        _Self;
    typedef _Rb_tree_node_base::_Base_ptr _Base_ptr;
    typedef _Rb_tree_node<_Tp>*           _Link_type;
    // 迭代器是一个指向红黑树节点的指针
    _Base_ptr _M_node;
};
```

重载了*操作符和->操作符来获取节点中存储的数据

```text
reference
operator*() const _GLIBCXX_NOEXCEPT
{ return *static_cast<_Link_type>(_M_node)->_M_valptr(); }

pointer
operator->() const _GLIBCXX_NOEXCEPT
{ return static_cast<_Link_type> (_M_node)->_M_valptr(); }
```

迭代器类重载的++操作符，自增（自底而上）的时候调用。本质是调用函数_Rb_tree_increment

```text
_Self&
operator++() _GLIBCXX_NOEXCEPT
{
    _M_node = _Rb_tree_increment(_M_node);
    return *this;
}
```

而_Rb_tree_increment函数的底层是local_Rb_tree_increment，源代码如下，可以看出采用的是前序遍历。

```text
static _Rb_tree_node_base *
local_Rb_tree_increment( _Rb_tree_node_base* __x ) throw ()
{
  /* 存在右子树,那么下一个节点为右子树的最小节点 */
  if ( __x->_M_right != 0 )
  {
    __x = __x->_M_right;
    while ( __x->_M_left != 0 )
      __x = __x->_M_left;
  }else  {
/* 不存在右子树,那么分为两种情况：自底往上搜索,当前节点为父节点的左孩子的时候,父节点就是后继节点；*/
/* 第二种情况:_x为header节点了,那么_x就是最后的后继节点. 简言之_x为最小节点且往上回溯,一直为父节点的右孩子,直到_x变为父节点,_y为其右孩子 */
    _Rb_tree_node_base *__y = __x->_M_parent;
    while ( __x == __y->_M_right )
    {
      __x  = __y;
      __y  = __y->_M_parent;
    }
    if ( __x->_M_right != __y )
      __x = __y;
  }
  return (__x);
}
```

同理，也重载了--操作符，调用_Rb_tree_decrement函数

```text
_Self&
operator--() _GLIBCXX_NOEXCEPT
{
  _M_node = _Rb_tree_decrement(_M_node);
    return *this;
}
```

同理，local_Rb_tree_decrement的源代码如下

```text
static _Rb_tree_node_base *
local_Rb_tree_decrement( _Rb_tree_node_base * __x )
throw ()
{
/* header节点 */
  if ( __x->_M_color ==
       _S_red
       && __x
       ->_M_parent->_M_parent == __x )
    __x = __x->_M_right;
  /* 左节点不为空,返回左子树中最大的节点 */
  else if ( __x->_M_left != 0 )
  {
    _Rb_tree_node_base *__y = __x->_M_left;
    while ( __y->_M_right != 0 )
      __y = __y->_M_right;
    __x = __y;
  }else  {
/* 自底向上找到当前节点为其父节点的右孩子,那么父节点就是前驱节点 */
    _Rb_tree_node_base *__y = __x->_M_parent;
    while ( __x == __y->_M_left )
    {
      __x  = __y;
      __y  = __y->_M_parent;
    }
    __x = __y;
  }
  return
    (__x);
}
```

也重载了==与!=操作符，用来判断节点指针是否相等。

```text
bool
operator==(const _Self& __x) const _GLIBCXX_NOEXCEPT
{ return _M_node == __x._M_node; }

bool
operator!=(const _Self& __x) const _GLIBCXX_NOEXCEPT
{ return _M_node != __x._M_node; }
```

黑节点的统计函数：

```text
unsigned int
_Rb_tree_black_count(const _Rb_tree_node_base *__node,
                     const _Rb_tree_node_base *__root) throw() {
    if (__node == 0)
        return 0;
    unsigned int __sum = 0;
    do {
        if (__node->_M_color == _S_black)
            ++__sum;
        if (__node == __root)
            break;
        __node = __node->_M_parent;
    } while (1);
    return __sum;
}
```

## **4.STL红黑树的自平衡**

我们在一开始就说明了STL红黑树也要满足普通红黑树的规则，这些规则才保证了红黑树的自平衡。红黑树从根到叶子的最长路径不会超过最短路径的2倍。

当插入节点或者删除节点的时候，红黑树的规则可能会被打破，这时候就需要做出一些调整，调整的方法有**变色**和**旋转**两种。旋转又包含左旋转和右旋转两种形式。

**变色**

为了重新符合红黑树的规则，尝试把红色节点变为黑色，或者把黑色节点变为红色。

如下所示为变色的场景：

![img](https://pic4.zhimg.com/80/v2-b896a65ec8035fe978432c7d8ca72fc7_720w.webp)

![img](https://pic1.zhimg.com/80/v2-6d0216a179b17163b6672022fecd1198_720w.webp)

![img](https://pic3.zhimg.com/80/v2-724d12ea47fbee58584b21c72445218a_720w.webp)

STL的源代码如下所示：

```text
_Rb_tree_node_base *const __y = __xpp->_M_right;    // 得到叔叔节点
if (__y && __y->_M_color == _S_red)     // case1: 叔叔节点存在，且为红色
{
    /**
     * 解决办法是：颜色翻转，父亲与叔叔的颜色都变为黑色,祖父节点变为红色,然后当前节点设为祖父，依次网上来判断是否破坏了红黑树性质
    */
    __x->_M_parent->_M_color = _S_black;    // 将其父节点改为黑色
    __y->_M_color = _S_black;               // 将其叔叔节点改为黑色
    __xpp->_M_color = _S_red;               // 将其祖父节点改为红色
    __x = __xpp;                            // 修改_x,往上回溯
} else {        // 无叔叔或者叔叔为黑色
    if (__x == __x->_M_parent->_M_right) {          // 当前节点为父亲节点的右孩子
        __x = __x->_M_parent;
        local_Rb_tree_rotate_left(__x, __root);     // 以父节点进行左旋转
    }
    // 旋转之后,节点x变成其父节点的左孩子
    __x->_M_parent->_M_color = _S_black;            // 将其父亲节点改为黑色
    __xpp->_M_color = _S_red;                       // 将其祖父节点改为红色
    local_Rb_tree_rotate_right(__xpp, __root);      // 以祖父节点右旋转
}
```

![img](https://pic2.zhimg.com/80/v2-a9e6d721c41a15941ef47283027292e9_720w.webp)

![img](https://pic3.zhimg.com/80/v2-7ba4aaad6b38b6858028f309a3c846d6_720w.webp)

![img](https://pic4.zhimg.com/80/v2-1812c199e724145d244ce484bdbaf26b_720w.webp)

代码如下：

```text
_Rb_tree_node_base *const __y = __xpp->_M_left; // 保存叔叔节点
if (__y && __y->_M_color == _S_red) {       // 叔叔节点存在且为红色
    __x->_M_parent->_M_color = _S_black;    // 父亲节点改为黑色
    __y->_M_color = _S_black;               // 祖父节点改为红色
    __xpp->_M_color = _S_red;
    __x = __xpp;
} else {        // 若无叔叔节点或者其叔叔节点为黑色
    if (__x == __x->_M_parent->_M_left) {   // 当前节点为父亲节点的左孩子
        __x = __x->_M_parent;
        local_Rb_tree_rotate_right(__x, __root);    // 以父节点右旋转
    }
    __x->_M_parent->_M_color = _S_black;        // 父节点置为黑色
    __xpp->_M_color = _S_red;                   // 祖父节点置为红色
    local_Rb_tree_rotate_left(__xpp, __root);   // 左旋转
}
```

**左旋转**

逆时针旋转红黑树的两个节点，使得父节点被自己的右孩子取代，而自己成为自己的左孩子。

```text
/**
* 当前节点的左旋转过程
* 将该节点的右节点设置为它的父节点，该节点将变成刚才右节点的左孩子
* 该节点的右节点的左孩子变成该节点的右孩子
* @param _x
*/
//    _x                      _y
//  /   \     左旋转         /  \
// T1   _y   --------->   _x    T3
//     / \              /   \
//    T2 T3            T1   T2
void leftRotate(Node *_x) {
    // step1 处理_x的右孩子
    // 右节点变为_x节点的父亲节点,先保存一下右节点
    Node *_y = _x->right;
    // T2变为node的右节点
    _x->right = _y->left;
    if (NULL != _y->left)
        _y->left->parent = _x;

    // step2 处理_y与父亲节点关系
    _y->parent = _x->parent;      // 原来_x的父亲变为_y的父亲
    // 说明原来_x为root节点,此时需要将_y设为新root节点
    // 或者判断NULL == _y->parent
    if (_x == root)
        root = _y;
    else if (_x == _x->parent->left)    // 原_x的父节点的左孩子连接新节点_y
        _x->parent->left = _y;
    else // 原_x的父节点的右孩子连接新节点_y
        _x->parent->right = _y;

    // step3 处理_x与_y关系
    _y->left = _x;      // _y的左孩子为_x
    _x->parent = _y;    // _x的父亲是_y
}
```

**右旋转**

顺时针旋转红黑树的两个节点，使得父节点被自己的左孩子取代，而自己成为自己的右孩子。

```text
//        _x                      _y
//      /   \     右旋转         /  \
//     _y    T2 ------------->  T0  _x
//    /  \                         /  \
//   T0  T1                       T1  T2
void rightRotate(Node *_x) {
    // step1 处理_x的左孩子
    // 左节点变为_x节点的父亲节点,先保存一下左节点
    Node *_y = _x->left;
    // T1变为_x的左孩子
    _x->left = _y->right;
    if (NULL != _y->right)
        _y->right->parent = _x;

    // step2 处理_y与父节点之间的关系
    // 或者判断_x->parent==NULL
    if (_x == root)
        root = _y;
    else if (_x == _x->parent->right)
        _x->parent->right = _y;
    else
        _x->parent->left = _y;

    // step3 处理_x与_y关系
    _y->right = _x;     // _y的右孩子为_x
    _x->parent = _y;    // _x的父亲是_y
}
```

STL红黑树会在插入和删除节点的时候进行自平衡操作，底层调用的是_Rb_tree_insert_and_rebalance函数，该函数的源码分析如下：

```text
void
_Rb_tree_insert_and_rebalance(const bool __insert_left,
                              _Rb_tree_node_base *__x,
                              _Rb_tree_node_base *__p,
                              _Rb_tree_node_base &__header) throw() {
    _Rb_tree_node_base * &__root = __header._M_parent;

    // Initialize fields in new node to insert.
    __x->_M_parent = __p;
    __x->_M_left = 0;
    __x->_M_right = 0;
    __x->_M_color = _S_red;

    // 处理__header部分
    // Insert.
    // Make new node child of parent and maintain root, leftmost and
    // rightmost nodes.
    // N.B. First node is always inserted left.
    if (__insert_left) {
        __p->_M_left = __x; // also makes leftmost = __x when __p == &__header

        if (__p == &__header) {
            __header._M_parent = __x;
            __header._M_right = __x;
        } else if (__p == __header._M_left)
            __header._M_left = __x; // maintain leftmost pointing to min node
    } else {
        __p->_M_right = __x;

        if (__p == __header._M_right)
            __header._M_right = __x; // maintain rightmost pointing to max node
    }

 // Rebalance.
    while (__x != __root
           && __x->_M_parent->_M_color == _S_red)   // 若新插入节点不是为RB-Tree的根节点，且其父节点color属性也是红色,即违反了性质4.
    {
        _Rb_tree_node_base *const __xpp = __x->_M_parent->_M_parent;        // 祖父节点

        if (__x->_M_parent == __xpp->_M_left)   // 父亲是祖父节点的左孩子
        {
            _Rb_tree_node_base *const __y = __xpp->_M_right;    // 得到叔叔节点
            if (__y && __y->_M_color == _S_red)     // case1: 叔叔节点存在，且为红色
            {
                /**
                 * 解决办法是：颜色翻转，父亲与叔叔的颜色都变为黑色,祖父节点变为红色,然后当前节点设为祖父，依次网上来判断是否破坏了红黑树性质
                 */
                __x->_M_parent->_M_color = _S_black;    // 将其父节点改为黑色
                __y->_M_color = _S_black;               // 将其叔叔节点改为黑色
                __xpp->_M_color = _S_red;               // 将其祖父节点改为红色
                __x = __xpp;                            // 修改_x,往上回溯
            } else {        // 无叔叔或者叔叔为黑色
                if (__x == __x->_M_parent->_M_right) {          // 当前节点为父亲节点的右孩子
                    __x = __x->_M_parent;
                    local_Rb_tree_rotate_left(__x, __root);     // 以父节点进行左旋转
                }
                // 旋转之后,节点x变成其父节点的左孩子
                __x->_M_parent->_M_color = _S_black;            // 将其父亲节点改为黑色
                __xpp->_M_color = _S_red;                       // 将其祖父节点改为红色
                local_Rb_tree_rotate_right(__xpp, __root);      // 以祖父节点右旋转
            }
        } else {        // 父亲是祖父节点的右孩子
            _Rb_tree_node_base *const __y = __xpp->_M_left; // 保存叔叔节点
            if (__y && __y->_M_color == _S_red) {       // 叔叔节点存在且为红色
                __x->_M_parent->_M_color = _S_black;    // 父亲节点改为黑色
                __y->_M_color = _S_black;               // 祖父节点改为红色
                __xpp->_M_color = _S_red;
                __x = __xpp;
            } else {        // 若无叔叔节点或者其叔叔节点为黑色
                if (__x == __x->_M_parent->_M_left) {   // 当前节点为父亲节点的左孩子
                    __x = __x->_M_parent;
                    local_Rb_tree_rotate_right(__x, __root);    // 以父节点右旋转
                }
                __x->_M_parent->_M_color = _S_black;        // 父节点置为黑色
                __xpp->_M_color = _S_red;                   // 祖父节点置为红色
                local_Rb_tree_rotate_left(__xpp, __root);   // 左旋转
            }
        }
    }
    //若新插入节点为根节点,则违反性质2
    //只需将其重新赋值为黑色即可
    __root->_M_color = _S_black;
}
```

map和set其实都是对rb-tree的包装，操作函数最终都是调用rb-tree提供的操作。map和set的增删改查，其实就是rb-tree的增删改查，所以掌握rb-tree的底层原理即可。这篇文章的内容已经足够多了，下一篇我会仔细分析一下rb-tree的增删改查，如果不理解这其中的细节，会导致我们在使用map和set时踩到很多陷阱。

原文地址：https://zhuanlan.zhihu.com/p/557734821

作者：linux

# 【NO.184】C++数据结构与算法：布隆过滤器（Bloom Filter）原理与实现

文本代码下载地址：Github：[https://github.com/dongyusheng/csdn-code/tree/master/BloomFilter](https://link.zhihu.com/?target=https%3A//github.com/dongyusheng/csdn-code/tree/master/BloomFilter)

## 1.什么是布隆过滤器

布隆过滤器（Bloom Filter）是1970年由布隆提出的

它实际上是一个**很长的二进制向量和一系列随机映射函数**。布隆过滤器可以用于检索一个元素是否在一个集合中

**优点：**

- 可以高效地进行查询，可以用来告诉你“某样东西一定不存在或者可能存在”
- 可以高效的进行插入
- 相比于传统的List、Set、Map等数据结构，它占用空间更少，因为其本身并不存储任何数据（重点）

**缺点：**

- 其返回的结果是概率性（存在误差）的
- 一般不提供删除操作

布隆过滤器一般使用在数据量特别大的场景下，一般不会使用

用的使用场景：

- 使用word文档时，判断某个单词是否拼写正确。例如我们在编写word时，某个单词错误那么就会在单词下面显示红色波浪线
- 网络爬虫程序，不去爬相同的url页面
- 垃圾邮件的过滤算法
- 缓存崩溃后造成的缓存击穿
- 集合重复元素的判别
- 查询加速（比如基于key-value的存储系统，如redis等）

## 2.什么时候选择布隆过滤器，而不使用其他数据结构

如果想要判断一个元素是不是在一个集合里，一般想到的是将所有元素保存起来，然后通过比较确定。链表，树、哈希表等数据结构都是这种思路（如下图所示）

![img](https://pic3.zhimg.com/80/v2-fd393023f85161cf69394abfa297357e_720w.webp)

上面这些数据结构面对数据量特别大的时候显现的缺点：

- 存储容量占比高，考虑到负载因子的存在，通常空间是不能被用满的
- 当数据量特别大时，会占用大量的内存空间。如果存储了类似于URL这样的key，那么内存消费太严重
- 如果使用hashmap，如果已有元素超过了总容量的一半之后，一般就需要考虑扩容了，因为元素多了之后哈希冲突就会增加，退化为链表存储的效率了

**下面是两个测试程序，分别测试hashmap和红黑树**，当元素特别多时，其查询和占用的内存会非常大

**测试map（内部使用红黑树）**

```text
#include <iostream>
#include <map>
#include <string>
#include <sys/time.h>
#include <utility>
#include <iomanip>
 
#define MAP_ITEMS 100000
 
using namespace std;
 
int main()
{
	std::map<std::string, bool> mp;
	
	timeval startTime, endTime;
	
	//1.插入MAP_ITEMS个元素到map中
	gettimeofday(&startTime, NULL);
	std::string key = "https://blog.csdn.net/qq_41453285";
	for(int i = 0; i < MAP_ITEMS; ++i){
		string sub_key = to_string(i);
		mp.insert(std::make_pair(key + sub_key, 1));
	}
	
	gettimeofday(&endTime, NULL);
	long insert_time = (endTime.tv_sec - startTime.tv_sec)*1000 + (endTime.tv_usec-startTime.tv_usec)/1000;
	
	//2.在map中查找一个元素
	gettimeofday(&startTime, NULL);
	if( mp.find(key + "10000") == mp.end())
		std::cout << "not found!" << std::endl;
	
	gettimeofday(&endTime, NULL);
	long find_time = endTime.tv_usec - startTime.tv_usec;
	
	//3.估算当前key的平均大小
	double key_size = key.size() + to_string(MAP_ITEMS).size()/2;
	
	//4.打印相关信息
	std::cout << "Number of members  " << "key size  " << "insert time(ms)  " << "find time(us)  " << std::endl;
	std::cout << left << setw(19) << MAP_ITEMS;
	std::cout << left << setw(10) << key_size;
	std::cout << left << setw(17) << insert_time;
	std::cout << left << setw(15) << find_time << std::endl;
}
```

**代码中的MAP_ITEMS常量代表当前map中存储的元素的个数**

当MAP_ITEMS为100000时，结果如下：

![img](https://pic2.zhimg.com/80/v2-a5926459485bfcd78e034acb212ce005_720w.webp)

当MAP_ITEMS为1000000时，结果如下：

![img](https://pic1.zhimg.com/80/v2-cd276da5ad69a134e5d0cb6b8dbc2048_720w.webp)

当MAP_ITEMS为10000000时，结果如下：

![img](https://pic3.zhimg.com/80/v2-c0ba50d58bbea8f735933a6355b6e776_720w.webp)



**测试unordered_map（内部使用hashmap）**

```text
#include <iostream>
#include <unordered_map>
#include <string>
#include <sys/time.h>
#include <utility>
#include <iomanip>
 
#define MAP_ITEMS 100000
 
using namespace std;
 
int main()
{
	unordered_map<string, bool> unordermp;
	
	timeval startTime, endTime;
	
	//1.插入MAP_ITEMS个元素到map中
	gettimeofday(&startTime, NULL);
	std::string key = "https://blog.csdn.net/qq_41453285";
	for(int i = 0; i < MAP_ITEMS; ++i){
		string sub_key = to_string(i);
		unordermp.insert(std::make_pair(key + sub_key, 1));
	}
	
	gettimeofday(&endTime, NULL);
	long insert_time = (endTime.tv_sec - startTime.tv_sec)*1000 + (endTime.tv_usec-startTime.tv_usec)/1000;
	
	//2.在map中查找一个元素
	gettimeofday(&startTime, NULL);
	if( unordermp.find(key + "10000") == unordermp.end())
		std::cout << "not found!" << std::endl;
	
	gettimeofday(&endTime, NULL);
	long find_time = endTime.tv_usec - startTime.tv_usec;
	
	//3.估算当前key的平均大小
	double key_size = key.size() + to_string(MAP_ITEMS).size()/2;
	
	//4.打印相关信息
	std::cout << "Number of members  " << "key size  " << "insert time(ms)  " << "find time(us)  " << std::endl;
	std::cout << left << setw(19) << MAP_ITEMS;
	std::cout << left << setw(10) << key_size;
	std::cout << left << setw(17) << insert_time;
	std::cout << left << setw(15) << find_time << std::endl;
}
```

**代码中的MAP_ITEMS常量代表当前unordered_map中存储的元素的个数**

当MAP_ITEMS为100000时，结果如下：

![img](https://pic1.zhimg.com/80/v2-b4d98c4bd16631555829f927933dc8f0_720w.webp)

当MAP_ITEMS为1000000时，结果如下：

![img](https://pic2.zhimg.com/80/v2-34e57284cb7efe4d3cc977a2a4c218a1_720w.webp)

当MAP_ITEMS为10000000时，结果如下：

![img](https://pic2.zhimg.com/80/v2-175057b0fdd6c5dab4c9083eab9c7acd_720w.webp)

## 3.布隆过滤器的数据结构与实现原理

### 3.1 数据结构

**布隆过滤器是一个bit向量或者说是一个bit数组（下面的数字为索引）**。如下所示：

![img](https://pic2.zhimg.com/80/v2-4edd9b19517d3af7cbfb6bcdf81a3bd1_720w.webp)

**其最小单位为bit，初始化时全部置为0**

添加、查询原理

布隆过滤器添加原理：利用K个Hash函数，将元素传入到这K个Hash函数中，并且映射到bit向量的K个点中，并且将映射到的K个点置为1

布隆过滤器查询原理：

- 利用K个Hash函数，将元素传入到这K个Hash函数中，并且映射到bit向量的K个点中
- 如果这些点中有任何一个为0，则被检测的元素一定不存在
- 如果这些点都返回1，则被检测的元素很可能（因为布隆过滤器存在误差）存在，但是不一定百分百存在

上面添加、查询使用的Hash函数一般都是相同的，实现设计好的

为什么布隆过滤器要使用多个Hash函数？

- Hash面临的问题就是冲突。假设Hash函数是良好的，如果我们的位阵列长度为m个点，那么如果我们想将冲突率降低到例如 1%，这个散列表就只能容纳 m/100个元素
- 解决方法较简单，使用K>1的布隆过滤器，即K个函数将每个元素改为对应于K个bits，因为误判度会降低很多，并且如果参数k和m选取得好，一半的m可被置为1

**一个重要的概念：**针对于一个特定的哈希函数和一个特定的值，那么该哈希函数返回的值每次都是固定的，不可能出现多次调用之间出现哈希函数返回值不同的情况

### 3.2 演示说明

假设我们的布隆过滤器有三个哈希函数，分别名为hash1、hash2、hash3

**①添加元素：**针对于“baidu”这个元素，我们调用三个哈希函数，将其映射到bit向量的三个位置（分别为1、4、7），并且将对应的位置置为1

![img](https://pic4.zhimg.com/80/v2-65dafedd27d1744c9d27a00297648dff_720w.webp)

**②添加元素：**现在针对于“tencent”这个元素，我们也调用三个哈希函数，将其映射到bit向量的三个位置（分别为3、4、8），并且将对应的位置置为1

![img](https://pic2.zhimg.com/80/v2-33e7aa1cebfeff849a6a67f16faebd11_720w.webp)

③此时，整个bit向量的1、3、4、7、8这几个位置被置为1了。其中4这个索引被覆盖了，因为“baidu”和“tencent”都将其置为1，覆盖的索引与误判率有关，详情见下面的介绍

④去查询一个不存在的元素，并且确定其肯定不存在：例如现在我们去查询“dongshao”这个元素，假设调用上面的三个哈希函数返回的索引是1、5、8，通过上图我们知道5这个索引处为0，因此“dongshao”这个元素一定不存在，因为如果存在的话，那么5这个位置应该被置为1才对（见上面的“一个重要概念”）

⑤去查询“baidu”这个元素，不能判断其百分百存在：我们将“baidu”传入上面的三个哈希函数中，哈希返回的对应索引值为1、4、7，发现1、4、7这几个索引处都为1，因此我们判断“baidu”这个元素可能存在。为什么不是百分百确定呢？见下面的误判率介绍

### 3.3 误判率

布隆过滤器允许存在一定的误判断，误判率也称为“假阳”

误判率一般是出现在查询的时候

例如上面我们去查询“baidu”的时候，由于“baidu”之前被我们插入过，为什么还不能百分百确定它一定存在呢？

- 因为“tencent”这个元素在插入的时候，将4这个索引置为1了
- 假设我们查询“baidu”的时候实际返回的是1、7索引为1，4索引为0。而4索引又被tencent覆盖为1，所以最终“baidu”最终看到的是1、4、7索引都为1，我们不能百分百确定“baidu”这个元素存在

因此，当随着增加的值越来越多时，bit向量被置为1的数量也就会越来越多，因此误判率会越来越大。例如，当查询“taobao”时，万一所有的哈希函数返回的对应bit都为1，那么布隆过滤器可能也认为“taobao”这个元素存在

### 3.4 布隆过滤器一般不拥有删除的功能

我们一般不能从布隆过滤器中删除元素。考虑下面几种情况：

- 因为要删除该元素，我们必须百分百确保该元素存在于布隆过滤器中，而布隆过滤器由于存在误判率，无法确定该元素百分百存在于布隆过滤器内
- 另外计数器回绕也会造成问题
- 如果我们因为某一个元素而将其对应的bit位删除变为0，那么如果这些bit位也是其他元素正在使用的，那么其他元素在查询时就会返回0，从而认为元素不存在而造成误判

## 4.误判概率的相关证明和计算

**证明①（哈希函数越多、插入元素越少，误判率越低）**

假设布隆过滤器中的hash函数满足simple uniform hashing(简单一致散列)假设：每个元素都等概率地hash到m个slot中的任何一个，与其它元素被hash到哪个slot无关

若m为bit数（向量表的长度）， 则对某一特定bit位在一个元素由某特定hash函数插入时没有被置位为1的概率为：

![img](https://pic1.zhimg.com/80/v2-ce557415a3d9b72aea102c6c61448d5c_720w.webp)

则k个hash函数中没有一个对其置位的概率为，随着k的增加，概率会变小：

![img](https://pic4.zhimg.com/80/v2-b11ecd2ae4acc82001f71525e6c0790b_720w.webp)

如果插入了n个元素，但都没有将其置位的概率为：

![img](https://pic3.zhimg.com/80/v2-9c6d65ceec6429c9a23b9e53ead31f8e_720w.webp)

现在考虑查询阶段，若对应某个待查询元素的k bits全部置位为1，则可判定其在集合中。 因此将某元素误判的概率p为：

![img](https://pic4.zhimg.com/80/v2-26b24706ea5cd3423718f07f90d92ec3_720w.webp)

现在考虑查询阶段，若对应某个待query元素的k bits全部置位为1，则可判定其在集合中。 因此将某元素误判的概率p为：

![img](https://pic3.zhimg.com/80/v2-95ca5d633d777673905ee4e682d68b96_720w.webp)

由于

![img](https://pic2.zhimg.com/80/v2-32ae1714c379b872c4596b2bb86f33f5_720w.webp)

当x→0时，并且

![img](https://pic4.zhimg.com/80/v2-88118a3e86be84a225dcd50a7bc11337_720w.webp)

当m很大时趋近于0，所以：

![img](https://pic4.zhimg.com/80/v2-3fe256b2518a3ffedea8dbd90bcec71f_720w.webp)

从上式中可以看出，当m增大或n减小时，都会使得误判率减小

**证明②（何时误判率最低？）**

现在计算对于给定的m和n，k为何值时可以使得误判率最低。设误判率为k的函数为：

![img](https://pic2.zhimg.com/80/v2-0059ace8559a63813b887480094472b9_720w.webp)

下面求最值，即是误差趋近于0

![img](https://pic3.zhimg.com/80/v2-d190387f891fa67ed8e3a5ea71ab712e_720w.webp)

- 因此，即当

![img](https://pic1.zhimg.com/80/v2-195d7838a78207b9b354eab77289a6dc_720w.webp)

时误判率最低，此时误判率为：

![img](https://pic2.zhimg.com/80/v2-f6fb2ec070e6077474c5559acb963cc1_720w.webp)

可以看出若要使得误判率≤1/2，则：

![img](https://pic2.zhimg.com/80/v2-c3256ebcc421b286ef031d10aa66515d_720w.webp)

这说明了若想保持某固定误判率不变，布隆过滤器的bit数m与被增加的元素数n应该是线性同步增加的

## 5.Hash函数的选择

**常见的应用比较广的hash函数有MD5， SHA1， SHA256，**一般用于信息安全方面，比如签名认证和加密等。比如我们传输文件时习惯用对原文件内容计算它的MD5值，生成128 bit的整数，通 常我们说的32位MD5值，是转换为HEX格式后的32个字符

**MurmurHash：**

- MurmurHash是2008年发明的，相比较MD5， MurMurhash不太安全（当然MD5也被破译了， sha也可以被破译），但是性能是MD5的几十倍
- MurmurHash有很多个版本， MurmurHash3修复了MurmurHash2的一些缺陷同时速度还要快一些，因此很多开源项目有用，比如nginx、 redis、 memcashed、 Hadoop等，比如用于计算一致性hash等
- MurmurHash被比较好的测试过了，测试方法见[https://github.com/aappleby/smhasher](https://link.zhihu.com/?target=https%3A//github.com/aappleby/smhasher)
- MurMurhash的实现也可以参考smhasher，或者参考[https://sites.google.com/site/murmurhash](https://link.zhihu.com/?target=https%3A//sites.google.com/site/murmurhash)
- 我们演示的布隆过滤器中的hash函数选择MurmurHash2算法

**补充：双重散列**

双重散列是线性开型寻址散列（开放寻址法）中的冲突解决技术。双重散列使用在发生冲突时将第二个散列函数应用于键的想法

此算法使用下面的公式来进行双哈希处理。hash1() 和 hash2() 是哈希函数，而 TABLE_SIZE 是哈希表的大小。 当发生碰撞时，我们通过重复增加步长i 来寻找键

![img](https://pic1.zhimg.com/80/v2-0f6e2dad0677d50d3ce33721c14e0638_720w.webp)

## 6.布隆过滤器的实现

### 6.1 **布隆过滤器在实现时一般设计考虑下面几样东西：**

- n：布隆过滤器最大处理的元素的个数
- P：希望的误差率
- m：布隆过滤器的bit位数目
- k：哈希函数的个数

应用时首先要先由用户决定要增加的最多元素个数n和希望的误差率P。这也是一个设计完整的布隆过滤器需要用户输入的仅有的两个参数（加入hash种子则为3个），之后的所有参数将由系统计算，并由此建立布隆过滤器

**①首先根据传入的n和p计算需要的内存大小m bits:**

![img](https://pic4.zhimg.com/80/v2-42c635e06efdf792fc95f5c35b7cc8df_720w.webp)

**②再由m，n得到hash function的个数：**

![img](https://pic2.zhimg.com/80/v2-df9999dd891b87d003e296de6882b1e5_720w.webp)

至此系统所需的参数已经备齐，后面就可以添加n个元素到布隆过滤器中，进行查询

### 6.2 布隆过滤器空间利用率问题

根据公式，当k最优时：

![img](https://pic4.zhimg.com/80/v2-15022fd076028771b9dfb4160b6fc917_720w.webp)

因此可验证当P=1%时，存储每个元素需要9.6 bits：

![img](https://pic4.zhimg.com/80/v2-1759583c141698a7de75f9f83cae257f_720w.webp)

而每当想将误判率降低为原来的1/10，则存储每个元素需要增加4.8 bits：

![img](https://pic3.zhimg.com/80/v2-081d15e3fd6d01b6d19806040915eb7a_720w.webp)

### 6.3 布隆过滤器误判率对比表

如果方便知道需要使用多少位才能降低错误概率，可以从下表所示的存储项目和位数 比率估计布隆过滤器的误判率

![img](https://pic1.zhimg.com/80/v2-9037055a3dd167e47ae0d48a59d523c8_720w.webp)

为每个URL分配两个字节就可以达到千分之几的冲突。比较保守的实现是，为每个URL 分配4个字节，项目和位数比是1∶32，误判率是0.00000021167340。对于5000万数量级的URL，布隆过滤器只占用200MB的空间

## 7.在线验证公式

**测试网址：**[https://hur.st/bloomfilter/](https://link.zhihu.com/?target=https%3A//hur.st/bloomfilter/)

**下面是一个测试网址，可以根据你输入的数值返回对应的数据：**

- n：布隆过滤器最大处理的元素的个数
- P：希望的误差率
- m：布隆过滤器的bit位数目
- k：哈希函数的个数

![img](https://pic3.zhimg.com/80/v2-ac94c9562fed3ee4845265bcedb3a6e6_720w.webp)

## 8.编码实现

**bloomfilter.h**

这个代码是布隆过滤器的实现代码

```text
#ifndef __MICRO_BLOOMFILTER_H__
#define __MICRO_BLOOMFILTER_H__
 
/**
 *
 *  仿照Cassandra中的BloomFilter实现，Hash选用MurmurHash2，通过双重散列公式生成散列函数，参考：http://hur.st/bloomfilter
 *    Hash(key, i) = (H1(key) + i * H2(key)) % m
 *
**/
 
#include <stdio.h>
#include <stdlib.h>
#include <stdint.h>
#include <string.h>
#include <math.h>
 
#define __BLOOMFILTER_VERSION__ "1.1"
#define __MGAIC_CODE__          (0x01464C42)
 
/**
 *  BloomFilter使用例子：
 *  static BaseBloomFilter stBloomFilter = {0};
 *
 *  初始化BloomFilter(最大100000元素，不超过0.00001的错误率)：
 *      InitBloomFilter(&stBloomFilter, 0, 100000, 0.00001);
 *  重置BloomFilter：
 *      ResetBloomFilter(&stBloomFilter);
 *  释放BloomFilter:
 *      FreeBloomFilter(&stBloomFilter);
 *
 *  向BloomFilter中新增一个数值（0-正常，1-加入数值过多）：
 *      uint32_t dwValue;
 *      iRet = BloomFilter_Add(&stBloomFilter, &dwValue, sizeof(uint32_t));
 *  检查数值是否在BloomFilter内（0-存在，1-不存在）：
 *      iRet = BloomFilter_Check(&stBloomFilter, &dwValue, sizeof(uint32_t));
 *
 *  (1.1新增) 将生成好的BloomFilter写入文件:
 *      iRet = SaveBloomFilterToFile(&stBloomFilter, "dump.bin")
 *  (1.1新增) 从文件读取生成好的BloomFilter:
 *      iRet = LoadBloomFilterFromFile(&stBloomFilter, "dump.bin")
**/
 
// 注意，要让Add/Check函数内联，必须使用 -O2 或以上的优化等级
#define FORCE_INLINE __attribute__((always_inline))
 
#define BYTE_BITS           (8)
#define MIX_UINT64(v)       ((uint32_t)((v>>32)^(v)))
 
#define SETBIT(filter, n)   (filter->pstFilter[n/BYTE_BITS] |= (1 << (n%BYTE_BITS)))
#define GETBIT(filter, n)   (filter->pstFilter[n/BYTE_BITS] & (1 << (n%BYTE_BITS)))
 
#pragma pack(1)
 
// BloomFilter结构定义
typedef struct
{
    uint8_t cInitFlag;                              // 初始化标志，为0时的第一次Add()会对stFilter[]做初始化
    uint8_t cResv[3];
 
    uint32_t dwMaxItems;                            // n - BloomFilter中最大元素个数 (输入量)
    double dProbFalse;                              // p - 假阳概率(误判率) (输入量，比如万分之一：0.00001)
    uint32_t dwFilterBits;                          // m =  ; - BloomFilter的比特数
    uint32_t dwHashFuncs;                           // k = round(log(2.0) * m / n); - 哈希函数个数
 
    uint32_t dwSeed;                                // MurmurHash的种子偏移量
    uint32_t dwCount;                               // Add()的计数，超过MAX_BLOOMFILTER_N则返回失败
 
    uint32_t dwFilterSize;                          // dwFilterBits / BYTE_BITS
    unsigned char *pstFilter;                       // BloomFilter存储指针，使用malloc分配
    uint32_t *pdwHashPos;                           // 存储上次hash得到的K个bit位置数组(由bloom_hash填充)
} BaseBloomFilter;
 
// BloomFilter文件头部定义
typedef struct
{
    uint32_t dwMagicCode;                           // 文件头部标识，填充 __MGAIC_CODE__
    uint32_t dwSeed;
    uint32_t dwCount;
 
    uint32_t dwMaxItems;                            // n - BloomFilter中最大元素个数 (输入量)
    double dProbFalse;                              // p - 假阳概率 (输入量，比如万分之一：0.00001)
    uint32_t dwFilterBits;                          // m = ceil((n * log(p)) / log(1.0 / (pow(2.0, log(2.0))))); - BloomFilter的比特数
    uint32_t dwHashFuncs;                           // k = round(log(2.0) * m / n); - 哈希函数个数
 
    uint32_t dwResv[6];
    uint32_t dwFileCrc;                             // (未使用)整个文件的校验和
    uint32_t dwFilterSize;                          // 后面Filter的Buffer长度
} BloomFileHead;
 
#pragma pack()
 
 
// 计算BloomFilter的参数m,k
static inline void _CalcBloomFilterParam(uint32_t n, double p, uint32_t *pm, uint32_t *pk)
{
    /**
     *  n - Number of items in the filter
     *  p - Probability of false positives, float between 0 and 1 or a number indicating 1-in-p
     *  m - Number of bits in the filter
     *  k - Number of hash functions
     *
     *  f = ln(2) × ln(1/2) × m / n = (0.6185) ^ (m/n)
     *  m = -1 * ln(p) × n / 0.6185 , 这里有错误
     *  k = ln(2) × m / n = 0.6931 * m / n
     * darren修正：
     * m = -1*n*ln(p)/((ln(2))^2) = -1*n*ln(p)/(ln(2)*ln(2)) = -1*n*ln(p)/(0.69314718055995*0.69314718055995))
     *   = -1*n*ln(p)/0.4804530139182079271955440025
     * k = ln(2)*m/n
    **/
 
    uint32_t m, k, m2;
 
    //    printf("ln(2):%lf, ln(p):%lf\n", log(2), log(p)); // 用来验证函数正确性
 
    // 计算指定假阳(误差)概率下需要的比特数
    m =(uint32_t) ceil(-1.0 * n * log(p) / 0.480453); //darren 修正
	//m2 =(uint32_t) ceil(-1 * n * log(p) / 0.480453); //错误写法
    
	m = (m - m % 64) + 64;                              // 8字节对齐
 
    // 计算哈希函数个数
    double double_k = (0.69314 * m / n); // ln(2)*m/n // 这里只是为了debug出来看看具体的浮点数值
    k = round(double_k);    // 返回x的四舍五入整数值。
    printf("orig_k:%lf, k:%u\n", double_k, k);
 
    *pm = m;
    *pk = k;
    return;
}
 
 
// 根据目标精度和数据个数，初始化BloomFilter结构
/**
 * @brief 初始化布隆过滤器
 * @param pstBloomfilter 布隆过滤器实例
 * @param dwSeed    hash种子
 * @param dwMaxItems 存储容量
 * @param dProbFalse 允许的误判率
 * @return 返回值
 *      -1 传入的布隆过滤器为空
 *      -2 hash种子错误或误差>=1
 */
inline int InitBloomFilter(BaseBloomFilter *pstBloomfilter,
                           uint32_t dwSeed,
                           uint32_t dwMaxItems,  double dProbFalse)
{
    if (pstBloomfilter == NULL)
        return -1;
    if ((dProbFalse <= 0) || (dProbFalse >= 1))
        return -2;
 
    // 先检查是否重复Init，释放内存
    if (pstBloomfilter->pstFilter != NULL)
        free(pstBloomfilter->pstFilter);
    if (pstBloomfilter->pdwHashPos != NULL)
        free(pstBloomfilter->pdwHashPos);
 
    memset(pstBloomfilter, 0, sizeof(BaseBloomFilter));
 
    // 初始化内存结构，并计算BloomFilter需要的空间
    pstBloomfilter->dwMaxItems = dwMaxItems;    // 最大存储
    pstBloomfilter->dProbFalse = dProbFalse;    // 误差
    pstBloomfilter->dwSeed = dwSeed;            // hash种子
 
    // 计算 m, k
    _CalcBloomFilterParam(pstBloomfilter->dwMaxItems, pstBloomfilter->dProbFalse,
                          &pstBloomfilter->dwFilterBits, &pstBloomfilter->dwHashFuncs);
 
    // 分配BloomFilter的存储空间
    pstBloomfilter->dwFilterSize = pstBloomfilter->dwFilterBits / BYTE_BITS;
    pstBloomfilter->pstFilter = (unsigned char *) malloc(pstBloomfilter->dwFilterSize);
    if (NULL == pstBloomfilter->pstFilter)
        return -100;
 
    // 哈希结果数组，每个哈希函数一个
    pstBloomfilter->pdwHashPos = (uint32_t*) malloc(pstBloomfilter->dwHashFuncs * sizeof(uint32_t));
    if (NULL == pstBloomfilter->pdwHashPos)
        return -200;
 
    printf(">>> Init BloomFilter(n=%u, p=%e, m=%u, k=%d), malloc() size=%.6fMB, items:bits=1:%0.1lf\n",
           pstBloomfilter->dwMaxItems, pstBloomfilter->dProbFalse, pstBloomfilter->dwFilterBits,
           pstBloomfilter->dwHashFuncs, (double)pstBloomfilter->dwFilterSize/1024/1024,
           pstBloomfilter->dwFilterBits*1.0/pstBloomfilter->dwMaxItems);
 
    // 初始化BloomFilter的内存
    memset(pstBloomfilter->pstFilter, 0, pstBloomfilter->dwFilterSize);
    pstBloomfilter->cInitFlag = 1;
    return 0;
}
 
// 释放BloomFilter
inline int FreeBloomFilter(BaseBloomFilter *pstBloomfilter)
{
    if (pstBloomfilter == NULL)
        return -1;
 
    pstBloomfilter->cInitFlag = 0;
    pstBloomfilter->dwCount = 0;
 
    free(pstBloomfilter->pstFilter);
    pstBloomfilter->pstFilter = NULL;
    free(pstBloomfilter->pdwHashPos);
    pstBloomfilter->pdwHashPos = NULL;
    return 0;
}
 
// 重置BloomFilter
// 注意: Reset()函数不会立即初始化stFilter，而是当一次Add()时去memset
inline int ResetBloomFilter(BaseBloomFilter *pstBloomfilter)
{
    if (pstBloomfilter == NULL)
        return -1;
 
    pstBloomfilter->cInitFlag = 0;
    pstBloomfilter->dwCount = 0;
    return 0;
}
 
// 和ResetBloomFilter不同，调用后立即memset内存
inline int RealResetBloomFilter(BaseBloomFilter *pstBloomfilter)
{
    if (pstBloomfilter == NULL)
        return -1;
 
    memset(pstBloomfilter->pstFilter, 0, pstBloomfilter->dwFilterSize);
    pstBloomfilter->cInitFlag = 1;
    pstBloomfilter->dwCount = 0;
    return 0;
}
 
///
///  函数FORCE_INLINE，加速执行
///
// MurmurHash2, 64-bit versions, by Austin Appleby
// https://sites.google.com/site/murmurhash/
FORCE_INLINE uint64_t MurmurHash2_x64 ( const void * key, int len, uint32_t seed )
{
    const uint64_t m = 0xc6a4a7935bd1e995;
    const int r = 47;
 
    uint64_t h = seed ^ (len * m);
 
    const uint64_t * data = (const uint64_t *)key;
    const uint64_t * end = data + (len/8);
 
    while(data != end)
    {
        uint64_t k = *data++;
 
        k *= m;
        k ^= k >> r;
        k *= m;
 
        h ^= k;
        h *= m;
    }
 
    const uint8_t * data2 = (const uint8_t*)data;
 
    switch(len & 7)
    {
    case 7: h ^= ((uint64_t)data2[6]) << 48;
    case 6: h ^= ((uint64_t)data2[5]) << 40;
    case 5: h ^= ((uint64_t)data2[4]) << 32;
    case 4: h ^= ((uint64_t)data2[3]) << 24;
    case 3: h ^= ((uint64_t)data2[2]) << 16;
    case 2: h ^= ((uint64_t)data2[1]) << 8;
    case 1: h ^= ((uint64_t)data2[0]);
        h *= m;
    };
 
    h ^= h >> r;
    h *= m;
    h ^= h >> r;
 
    return h;
}
 
// 双重散列封装，k个函数函数, 比如要20个
FORCE_INLINE void bloom_hash(BaseBloomFilter *pstBloomfilter, const void * key, int len)
{
    //if (pstBloomfilter == NULL) return;
    int i;
    uint32_t dwFilterBits = pstBloomfilter->dwFilterBits;
    uint64_t hash1 = MurmurHash2_x64(key, len, pstBloomfilter->dwSeed);
    uint64_t hash2 = MurmurHash2_x64(key, len, MIX_UINT64(hash1));
 
    for (i = 0; i < (int)pstBloomfilter->dwHashFuncs; i++)
    {
        // k0 = (hash1 + 0*hash2) % dwFilterBits; // dwFilterBits bit向量的长度
        // k1 = (hash1 + 1*hash2) % dwFilterBits;
        pstBloomfilter->pdwHashPos[i] = (hash1 + i*hash2) % dwFilterBits;
    }
 
    return;
}
 
// 向BloomFilter中新增一个元素
// 成功返回0，当添加数据超过限制值时返回1提示用户
FORCE_INLINE int BloomFilter_Add(BaseBloomFilter *pstBloomfilter, const void * key, int len)
{
    if ((pstBloomfilter == NULL) || (key == NULL) || (len <= 0))
        return -1;
 
    int i;
 
    if (pstBloomfilter->cInitFlag != 1)
    {
        // Reset后没有初始化，使用前需要memset
        memset(pstBloomfilter->pstFilter, 0, pstBloomfilter->dwFilterSize);
        pstBloomfilter->cInitFlag = 1;
    }
 
    // hash key到bloomfilter中, 为了计算不同hash命中的位置，保存pdwHashPos数组
    bloom_hash(pstBloomfilter, key, len);
    for (i = 0; i < (int)pstBloomfilter->dwHashFuncs; i++)
    {
        // dwHashFuncs[0] = hash0(key)
        // dwHashFuncs[1] = hash1(key)
        // dwHashFuncs[k-1] = hashk-1(key)
        SETBIT(pstBloomfilter, pstBloomfilter->pdwHashPos[i]);
    }
 
    // 增加count数
    pstBloomfilter->dwCount++;
    if (pstBloomfilter->dwCount <= pstBloomfilter->dwMaxItems)
        return 0;
    else
        return 1;       // 超过N最大值，可能出现准确率下降等情况
}
 
// 检查一个元素是否在bloomfilter中
// 返回：0-存在，1-不存在，负数表示失败
FORCE_INLINE int BloomFilter_Check(BaseBloomFilter *pstBloomfilter, const void * key, int len)
{
    if ((pstBloomfilter == NULL) || (key == NULL) || (len <= 0))
        return -1;
 
    int i;
 
    bloom_hash(pstBloomfilter, key, len);
    for (i = 0; i < (int)pstBloomfilter->dwHashFuncs; i++)
    {
        // 如果有任意bit不为1，说明key不在bloomfilter中
        // 注意: GETBIT()返回不是0|1，高位可能出现128之类的情况
        if (GETBIT(pstBloomfilter, pstBloomfilter->pdwHashPos[i]) == 0)
            return 1;
    }
 
    return 0;
}
 
 
/* 文件相关封装 */
// 将生成好的BloomFilter写入文件
inline int SaveBloomFilterToFile(BaseBloomFilter *pstBloomfilter, char *szFileName)
{
    if ((pstBloomfilter == NULL) || (szFileName == NULL))
        return -1;
 
    int iRet;
    FILE *pFile;
    static BloomFileHead stFileHeader = {0};
 
    pFile = fopen(szFileName, "wb");
    if (pFile == NULL)
    {
        perror("fopen");
        return -11;
    }
 
    // 先写入文件头
    stFileHeader.dwMagicCode = __MGAIC_CODE__;
    stFileHeader.dwSeed = pstBloomfilter->dwSeed;
    stFileHeader.dwCount = pstBloomfilter->dwCount;
    stFileHeader.dwMaxItems = pstBloomfilter->dwMaxItems;
    stFileHeader.dProbFalse = pstBloomfilter->dProbFalse;
    stFileHeader.dwFilterBits = pstBloomfilter->dwFilterBits;
    stFileHeader.dwHashFuncs = pstBloomfilter->dwHashFuncs;
    stFileHeader.dwFilterSize = pstBloomfilter->dwFilterSize;
 
    iRet = fwrite((const void*)&stFileHeader, sizeof(stFileHeader), 1, pFile);
    if (iRet != 1)
    {
        perror("fwrite(head)");
        return -21;
    }
 
    // 接着写入BloomFilter的内容
    iRet = fwrite(pstBloomfilter->pstFilter, 1, pstBloomfilter->dwFilterSize, pFile);
    if ((uint32_t)iRet != pstBloomfilter->dwFilterSize)
    {
        perror("fwrite(data)");
        return -31;
    }
 
    fclose(pFile);
    return 0;
}
 
// 从文件读取生成好的BloomFilter
inline int LoadBloomFilterFromFile(BaseBloomFilter *pstBloomfilter, char *szFileName)
{
    if ((pstBloomfilter == NULL) || (szFileName == NULL))
        return -1;
 
    int iRet;
    FILE *pFile;
    static BloomFileHead stFileHeader = {0};
 
    if (pstBloomfilter->pstFilter != NULL)
        free(pstBloomfilter->pstFilter);
    if (pstBloomfilter->pdwHashPos != NULL)
        free(pstBloomfilter->pdwHashPos);
 
    //
    pFile = fopen(szFileName, "rb");
    if (pFile == NULL)
    {
        perror("fopen");
        return -11;
    }
 
    // 读取并检查文件头
    iRet = fread((void*)&stFileHeader, sizeof(stFileHeader), 1, pFile);
    if (iRet != 1)
    {
        perror("fread(head)");
        return -21;
    }
 
    if ((stFileHeader.dwMagicCode != __MGAIC_CODE__)
            || (stFileHeader.dwFilterBits != stFileHeader.dwFilterSize*BYTE_BITS))
        return -50;
 
    // 初始化传入的 BaseBloomFilter 结构
    pstBloomfilter->dwMaxItems = stFileHeader.dwMaxItems;
    pstBloomfilter->dProbFalse = stFileHeader.dProbFalse;
    pstBloomfilter->dwFilterBits = stFileHeader.dwFilterBits;
    pstBloomfilter->dwHashFuncs = stFileHeader.dwHashFuncs;
    pstBloomfilter->dwSeed = stFileHeader.dwSeed;
    pstBloomfilter->dwCount = stFileHeader.dwCount;
    pstBloomfilter->dwFilterSize = stFileHeader.dwFilterSize;
 
    pstBloomfilter->pstFilter = (unsigned char *) malloc(pstBloomfilter->dwFilterSize);
    if (NULL == pstBloomfilter->pstFilter)
        return -100;
    pstBloomfilter->pdwHashPos = (uint32_t*) malloc(pstBloomfilter->dwHashFuncs * sizeof(uint32_t));
    if (NULL == pstBloomfilter->pdwHashPos)
        return -200;
 
 
    // 将后面的Data部分读入 pstFilter
    iRet = fread((void*)(pstBloomfilter->pstFilter), 1, pstBloomfilter->dwFilterSize, pFile);
    if ((uint32_t)iRet != pstBloomfilter->dwFilterSize)
    {
        perror("fread(data)");
        return -31;
    }
    pstBloomfilter->cInitFlag = 1;
 
    printf(">>> Load BloomFilter(n=%u, p=%f, m=%u, k=%d), malloc() size=%.2fMB\n",
           pstBloomfilter->dwMaxItems, pstBloomfilter->dProbFalse, pstBloomfilter->dwFilterBits,
           pstBloomfilter->dwHashFuncs, (double)pstBloomfilter->dwFilterSize/1024/1024);
 
    fclose(pFile);
    return 0;
}
 
#endif
```

**bloomfilter.cpp**

这个是布隆过滤器的测试代码

```text
#include "bloomfilter.h"
#include <stdio.h>
 
#define MAX_ITEMS 6000000      // 设置最大元素个数
#define ADD_ITEMS 1000      // 添加测试元素
#define P_ERROR 0.0001// 设置误差
 
//
int main(int argc, char** argv)
{
 
    printf(" test bloomfilter\n");
 
    // 1. 定义BaseBloomFilter
    static BaseBloomFilter stBloomFilter = {0};
 
    // 2. 初始化stBloomFilter，调用时传入hash种子，存储容量，以及允许的误判率
    InitBloomFilter(&stBloomFilter, 0, MAX_ITEMS, P_ERROR);
 
    // 3. 向BloomFilter中新增数值
    char url[128] = {0};
    for(int i = 0; i < ADD_ITEMS; i++){
        sprintf(url, "https://blog.csdn.net/qq_41453285/%d.html", i);
        if(0 == BloomFilter_Add(&stBloomFilter, (const void*)url, strlen(url))){
            // printf("add %s success", url);
        }else{
            printf("add %s failed", url);
        }
        memset(url, 0, sizeof(url));
    }
 
    // 4. check url exist or not
    char* str = "https://blog.csdn.net/qq_41453285/0.html";
    if (0 == BloomFilter_Check(&stBloomFilter, (const void*)str, strlen(str)) ){
        printf("https://blog.csdn.net/qq_41453285/0.html exist\n");
    }
 
    char* str2 = "https://blog.csdn.net/qq_41453285/10001.html";
    if (0 != BloomFilter_Check(&stBloomFilter, (const void*)str2, strlen(str2)) ){
          printf("https://blog.csdn.net/qq_41453285/10001.html not exist\n");
    }
 
    // 5. free bloomfilter
    FreeBloomFilter(&stBloomFilter);
    getchar();
    return 0;
}
```

**结果图下图所示：**

- n：布隆过滤器最大处理的元素的个数
- P：希望的误差率
- m：布隆过滤器的bit位数目
- k：哈希函数的个数

![img](https://pic3.zhimg.com/80/v2-46bc8f881346482402baf60c261d2e26_720w.webp)

![img](https://pic3.zhimg.com/80/v2-bea13254fcb5d65149c2ff44d60f7d4e_720w.webp)



原文地址：https://zhuanlan.zhihu.com/p/557308262

作者：linux

# 【NO.185】ringbuffer 消息队列 内存池 性能优化利器

## 1.简约而不简单的ringbuffer

最近在研究srsLTE的代码,其中就发现一个有意思的数据结构------ringbuffer。

虽然，这是一个很基本的数据结构，但时，它在LTE这种通信协议栈系统中却大行其道，也是很容易被协议开发人员忽略的。在整个通信协议的开发团队中，一般会有一个平台中间件的团队，他们的任务是给业务部门提供高性能、高可靠性的中间件代码，如内存池、线程池、消息通信机制、日志系统等等。这篇文章就来讨论下这个简约而不简单的ringbuffer。

## 2.ringbuffer数据结构

环形缓冲器（ringr buffer），也称作圆形队列（circular queue），循环缓冲区（cyclic buffer），圆形缓冲区（circula buffer），是一种用于表示一个固定尺寸、头尾相连的缓冲区的数据结构，适合缓存数据流。

在通信程序中，经常使用环形缓冲器作为数据结构来存放通信中发送和接收的数据。环形缓冲区是一个先进先出的循环缓冲区，可以向通信程序提供对缓冲区的互斥访问。

![img](https://pic4.zhimg.com/80/v2-e62501b9e35913bf4af8340ffde589a3_720w.webp)

## 3.用法

圆形缓冲区的一个有用特性是：当一个数据元素被用掉后，其余数据元素不需要移动其存储位置。相反，一个非圆形缓冲区（例如一个普通的队列）在用掉一个数据元素后，其余数据元素需要向前搬移。换句话说，圆形缓冲区适合实现先进先出缓冲区，而非圆形缓冲区适合后进先出缓冲区。

圆形缓冲区适合于事先明确了缓冲区的最大容量的情形。扩展一个圆形缓冲区的容量，需要搬移其中的数据。因此一个缓冲区如果需要经常调整其容量，用链表实现更为合适。

写操作覆盖圆形缓冲区中未被处理的数据在某些情况下是允许的。特别是在多媒体处理时。例如，音频的生产者可以覆盖掉声卡尚未来得及处理的音频数据。

## 4.工作机制

一般的，圆形缓冲区需要4个指针 ：

- 在内存中实际开始位置；
- 在内存中实际结束位置，也可以用缓冲区长度代替；
- 存储在缓冲区中的有效数据的开始位置（读指针）；
- 存储在缓冲区中的有效数据的结尾位置（写指针）。
  读指针、写指针可以用整型值来表示。下例为一个未满的缓冲区的读写指针：

![img](https://pic1.zhimg.com/80/v2-386e64896fd987c1c8fae13f7d766f20_720w.webp)

下例为一个满的缓冲区的读写指针:

![img](https://pic2.zhimg.com/80/v2-54ff4d3d06cbe882d4ecb0d053d1b219_720w.webp)

## 5.区分缓冲区满或者空

缓冲区是满、或是空，都有可能出现读指针与写指针指向同一位置，有多种策略用于检测缓冲区是满、或是空。常用的做法是总是保持一个存储单元为空，缓冲区中总是有一个存储单元保持未使用状态。缓冲区最多存入 size-1个数据。如果读写指针指向同一位置，则缓冲区为空。如果写指针位于读指针的相邻后一个位置，则缓冲区为满。这种策略的优点是简单、鲁棒；缺点是语义上实际可存数据量与缓冲区容量不一致，测试缓冲区是否满需要做取余数计算。

出色的KFIFO

kfifo是一种"First In First Out “数据结构，它采用了前面提到的环形缓冲区来实现，提供一个无边界的字节流服务。采用环形缓冲区的好处为，当一个数据元素被用掉后，其余数据元素不需要移动其存储位置，从而减少拷贝提高效率。更重要的是，kfifo采用了并行无锁技术，kfifo实现的单生产/单消费模式的共享队列是不需要加锁同步的。

熟悉Linux内核的读者应该对kfifo.c和kfifo.h并不陌生.kfifo经过简单改进就可以在用户态进行使用，笔者在实际项目中多次使用，经过实践，代码是稳定、可靠、高效的。

## 6.ringbuffer蕴藏的巨大能量

### 6.1 **消息队列**

ringbuffer的一个天生的高性能的消息队列，特别是在单生产/单消费的模式下，它是无锁的，这点非常重要。之前的文章曾介绍过，LTE的协议栈实现对时序是敏感的，这意味着代码的执行不能有阻塞的风险，而线程间的通信几乎是协议栈中必须的基本功能。因此，用ringbuffer去实现一个高性能的消息队列是一种非常理想的方案。当然，由于不同的线程的运行模型不同，例如PDCP线程属于包驱动的线程，大部分时间它是属于阻塞的，当有数据到达，如RRC可以通过消息队列给PDCP发送一个消息，这个时候需要唤醒PDCP进行处理，这个是属于线程同步的技术范畴，可以通过MUTEX、信号量等方案去实现。如果你的系统的Linux（rt-patch）,eventfd也是不错的选择，eventfd优势是可以使用poll、select、epoll等操作，这样协议栈的线程实现的方式上较为简洁，关键是eventfd性能也非常的快。

当然这里需要划一个重点，不同线程间需要独立的消息队列，来保证FIFO的无锁特性，当然缺点是会浪费一些内容，但是这在协议栈的开发中往往不是什么大的问题，性能和稳定永远是第一位的。由于FIFO通常是固定大小的数据结构不太适合可变消息的发送，这里的技巧是队列里面只放消息的指针，消息的内容通常是在内存池中申请不同大小的结构。

srsLTE代码的实现PDCP和RLC并不一定是以单独的线程运行的，但是在实际项目中，为了性能的考虑，通常是需要线程化的，且上下行也要线程化，且绑定不同的CPU核，来保证性能。

下图是PDCP和RLC线程的消息队列实例：

![img](https://pic1.zhimg.com/80/v2-6588c70906a33414dd03cfdd9dc33f20_720w.webp)

### 6.2 内存池

内存池在通信协议栈和很多的软件中都是常用的技术，它的好处是除了可以避免内存碎片，更重要的一点是，内存是预先申请的，并且自我管理，在申请和释放的效率更快，这对协议栈的实现是十分重要的。

内存池的实现在方式都是大同小异的，通常将内存分为8字节、16字节、32字节… 1K等大小不同的内存块，并通过链表的方式进行管理。具体的实现方式可以自行到github上搜索，实现方式都是类似的。

那么，ringbuffer和内存池有什么关系呢？实际上，ringbuffer和内存池的实现并无直接的关系，但是内存池在实现上有个至关重要的问题，那就内存的申请和释放可能不是在同一个线程中。简单的说就是，内存的申请和释放可能存在竞争的情况，通常的做法是进行加锁进行保护。但是加锁的操作可能对协议的时序产生不确定的影响，这对时序要求较高的协议实现（如CMAC）是无法接受的。

ringbuffer的优秀的特性又一次被应用的淋漓尽致，做法也是相当的简单，就是使用ringbuffer单生产/单消费的模式的无锁特性，释放的线程可以将需要释放的地址使用ringbuffer发送给申请的线程，由申请的线程进行内存的释放，这就就不需要加锁的操作，因为同一个线程不会出现并发的链表操作。

下图是结合了消息队列和内存池技术的一次应用，该方案是十分经典和有效的，在很多大型的通信系统中都能看到这种方案的影子：

![img](https://pic2.zhimg.com/80/v2-72f19fb8b1433a9d9ae3c527f549784d_720w.webp)

## 7.总结

本文是结合笔者的实际项目经验，介绍了ringbuffer在协议栈软件开发中的一些应用和技巧，主要是ringbuffer单生产/单消费的模式的无锁特性在内存池内存释放和消息队列中的应用技巧。如果读者也有类似的性能方面的系统需求，可以不妨试试 ringbuffer，性能超乎你的想象，且没有特别复杂的算法和CPU指令集的限制。

原文地址：https://zhuanlan.zhihu.com/p/556524313

作者：linux

# 【NO.186】ZeroMQ无锁队列的原理与实现

## 1.前言

无锁队列用在什么地方？每秒几十万的元素时再考虑使用无锁队列，比如股票行情这种。如果队列里一秒就几千几万的元素，那就不需要使用无锁队列，性能没有太大的提高。

源码：[ypipe.hpp](https://link.zhihu.com/?target=https%3A//github.com/gopherWxf/c-c-linux-LearningCode/tree/master/3.2.4%E6%97%A0%E9%94%81%E9%98%9F%E5%88%97freequeue)

## 2. 为什么需要无锁队列

锁是解决并发问题的万能钥匙，可是并发问题只有锁能解决吗？锁引起的问题：

- Cache损坏(Cache trashing)

线程间频繁切换的时候会导致 Cache 中数据的丢失，Cache中的数据会失效,因为它缓存的是将被换出任务的数据,这些数据对于新换进的任务是没⽤的。处理器的运⾏速度⽐主存快N倍,所以⼤量的处理器时间被浪费在处理器与主存的数据传输上。这就是在处理器和主存之间引⼊Cache的原因。Cache是⼀种速度更快但容量更⼩的内存(也更加昂贵),当处理器要访问主存中的数据时,这些数据⾸先被拷⻉到Cache中，因为这些数据在不久的将来可能⼜会被处理器访问。Cache misses对性能有⾮常⼤的影响,因为处理器访问Cache中的数据将⽐直接访问主存快得多。在保存和恢复上下⽂的过程中还隐藏了额外的开销。

- 在同步机制上争抢队列

阻塞不是微不⾜道的操作。它导致操作系统暂停当前的任务或使其进⼊睡眠状态(等待，不占⽤任何的处理器)。直到资源(例如互斥锁)可⽤，被阻塞的任务才可以解除阻塞状态(唤醒)。在⼀个负载较重的应⽤程序中使⽤这样的阻塞队列来在线程之间传递消息会导致严重的争⽤问题。也就是说，任务将⼤量的时间(睡眠，等待，唤醒)浪费在获得保护队列数据的互斥锁，⽽不是处理队列中的数据上。

⾮阻塞机制⼤展伸⼿的机会到了。任务之间不争抢任何资源，在队列中预定⼀个位置，然后在这个位置上插⼊或提取数据。这中机制使⽤了⼀种被称之为CAS(⽐较和交换)的特殊操作，这个特殊操作是⼀种特殊的指令，它可以原⼦的完成以下操作:它需要3个操作数m，A，B，其中m是⼀个内存地址，操作将m指向的内存中的内容与A⽐较，如果相等则将B写⼊到m指向的内存中并返回true，如果不相等则什么也不做返回false。简而言之非阻塞的机制使用了 CAS 的特殊操作，使得任务之间可以不争抢任何资源，然后在队列中预定的位置上，插入或者提取数据。[CAS底层实现](https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_42956653/article/details/126141284)

- 多线程动态内存分配malloc性能下降

在多线程系统中,需要仔细的考虑动态内存分配。当⼀个任务从堆中分配内存时，标准的内存分配机制会阻塞所有与这个任务共享地址空间的其它任务(进程中的所有线程)。这样做的原因是让处理更简单，且它⼯作得很好。两个线程不会被分配到⼀块相同的地址的内存，因为它们没办法同时执⾏分配请求。显然线程频繁分配内存会导致应⽤程序性能下降(必须注意,向标准队列或map插⼊数据的时候都会导致堆上的动态内存分配)

## 3. 无锁队列的实现(参考zmq，只支持一写一读的场景)

### 3.1 无锁队列前言

//TODO git地址补充 源码的ypipe.hpp、yqueue.hpp，这些源码可以在⼯程项⽬使⽤，但要注意，这⾥只⽀持单写单读的场景。 其中yqueue 是用来设计队列，ypipe 用来设计队列的写入/读取时机、回滚以及 flush，首先我们来看 yqueue 的设计。

### 3.2 原子操作函数介绍

```text
template<typename T>
class atomic_ptr_t {
public:
    void set(T *ptr_); //⾮原⼦操作
    T *xchg(T *val_); //原⼦操作，设置⼀个新的值，然后返回旧的值
    T *cas(T *cmp_, T *val_);//原⼦操作
private:
    volatile T *ptr;
};
```

- set函数，把私有成员ptr指针设置成参数ptr_的值，不是⼀个原⼦操作，需要使⽤者确保执⾏set过程没有其他线程使⽤ptr的值。
- xchg函数，把私有成员ptr指针设置成参数val_的值，并返回ptr设置之前的值。原⼦操作，线程安全。
- cas函数，原⼦操作，线程安全，把私有成员ptr指针与参数cmp_指针⽐较：如果相等返回ptr设置之前的值，并把ptr更新为参数val_的值，如果不相等直接返回ptr值。

### 3.3 yqueue_t的chunk块机制

#### **3.3.1 chunk块机制 一次分配多个元素**

首先我们需要考虑元素的分配，元素存在哪里？yqueue 中的数据结构使用的 chunk 块机制，每次批量分配一批元素，这样可以减少内存的分配和释放yqueue_t内部由⼀个⼀个chunk组成，每个chunk保存N个元素：spare_chunk⾥⾯，当再次需要分配chunk_t的时候从spare_chunk中获取。

当队列空间不⾜时每次分配⼀个chunk_t，每个chunk_t能存储N个元素。在数据出队列后，队列有多余空间的时候，回收的chunk也不是⻢上释放，⽽是根据局部性原理先回收到

```text
struct chunk_t {
   T values[N]; //每个chunk_t可以容纳N个T类型的元素，以后就以一个chunk_t为单位申请内存
   chunk_t *prev;
   chunk_t *next;
};
```

![img](https://pic3.zhimg.com/80/v2-490a9cc9ec8a235c769115da7e5158ea_720w.webp)

#### **3.3.2 chunk块机制 局部性原理**

程序局部性原理：是指程序在执行时呈现出局部性规律，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域，具体来说，局部性通常有两种形式：时间局部性和空间局部性。

时间局部性：被引用过一次的存储器位置在未来会被多次引用（通常在循环中）。

空间局部性：如果一个存储器的位置被引用，那么将来他附近的位置也会被引用。

在yqueue_t类中有一个spare_chunk用于保存最近的空闲块 。也就是说，在将一个chunk中的所有元素都pop掉了，那么我们可以free这个chunk。但是我们可以保存一块最近的空闲块，以后如果chunk不够用时，扩容chunk就不用malloc，直接复用该spare_chunk即可。根据局部性原理，这个spare_chunk的地址或者内存页很有可能还在cache里，那么这样的机制就可以减少一次malloc以及存入cache的操作。

```text
//  class yqueue_t
//  People are likely to produce and consume at similar rates.  In
//  this scenario holding onto the most recently freed chunk saves
//  us from having to call malloc/free.
atomic_ptr_t<chunk_t> spare_chunk; //空闲块（把所有元素都已经出队的块称为空闲块），读写线程的共享变量
```

可以看到在pop的时候，如果删除满一格chunk，就把这个chunk放到spare_chunk里。

```text
//  Removes an element from the front end of the queue.
inline void pop() {
    if (++begin_pos == N) // 删除满一个chunk才回收chunk
    {
        chunk_t *o = begin_chunk;
        begin_chunk = begin_chunk->next;
        begin_chunk->prev = NULL;
        begin_pos = 0;

        //  'o' has been more recently used than spare_chunk,
        //  so for cache reasons we'll get rid of the spare and
        //  use 'o' as the spare.
        chunk_t *cs = spare_chunk.xchg(o); //由于局部性原理，总是保存最新的空闲块而释放先前的空闲快
        free(cs);
    }
}
```

在push的时候，如果chunk满了，说明要发生扩容，那么我们优先从spare_chunk取出最近的空闲块当新的chunk来使用

```text
//  Adds an element to the back end of the queue.
inline void push() {
    back_chunk = end_chunk;
    back_pos = end_pos; //

    if (++end_pos != N) //end_pos!=N表明这个chunk节点还没有满
        return;

    chunk_t *sc = spare_chunk.xchg(NULL); // 为什么设置为NULL？ 因为如果把之前值取出来了则没有spare chunk了，所以设置为NULL
    if (sc)                               // 如果有spare chunk则继续复用它
    {
        end_chunk->next = sc;
        sc->prev = end_chunk;
    }
    else // 没有则重新分配
    {
        // static int s_cout = 0;
        // printf("s_cout:%d\n", ++s_cout);
        end_chunk->next = (chunk_t *) malloc(sizeof(chunk_t)); // 分配一个chunk
        alloc_assert(end_chunk->next);
        end_chunk->next->prev = end_chunk;
    }
    end_chunk = end_chunk->next;
    end_pos = 0;
}
```

![img](https://pic1.zhimg.com/80/v2-f1ffe0391fd90f4ff72a5807704ccbb0_720w.webp)

### 3.4 yqueue_t成员和接口介绍

yqueue_t的作用就是管理元素、管理chunk。chunk和spare_chunk上文已经说过了，这里不再赘述。

```text
// T is the type of the object in the queue.队列中元素的类型
// N is granularity(粒度) of the queue，简单来说就是chunk_t ⼀个结点可以装载N个T类型的元素
template<typename T, int N>
class yqueue_t {
public:
    inline yqueue_t();// 创建队列.
    inline ~yqueue_t();// 销毁队列.
    inline T &front();// Returns reference to the front element of the queue. If the queue is empty, behaviour is undefined.
    inline T &back();// Returns reference to the back element of the queue.If the queue is empty, behaviour is undefined.
    inline void push();// Adds an element to the back end of the queue.
    inline void pop();// Removes an element from the front of the queue.
    inline void unpush()// Removes element from the back end of the queue。 回滚时使⽤
private:
// Individual memory chunk to hold N elements.
    struct chunk_t {
        T values[N];
        chunk_t *prev;
        chunk_t *next;
    };
    chunk_t *begin_chunk;
    int begin_pos;
    chunk_t *back_chunk;
    int back_pos;
    chunk_t *end_chunk;
    int end_pos;
    atomic_ptr_t<chunk_t> spare_chunk; //空闲块（我把所有元素都已经出队的块称为空闲块），读写线程的共享变量
};
```

**2.4.1 begin/back/end_chunk 与 begin/back/end_pos 成员介绍**

```text
chunk_t *begin_chunk;
int begin_pos;
chunk_t *back_chunk;
int back_pos;
chunk_t *end_chunk;
int end_pos;
```

yqueue_t内部有三个chunk_t类型指针以及对应的索引位置：

- begin_chunk/begin_pos：begin_chunk用于指向队列的第一个chunk，begin_pos用于指向第一个chunk的第一个元素的索引位置，因为pop()，所以第一个元素不可能永远是0，会随着pop而改变。同理第一个chunk也会被回收，也需要记录第一个chunk的位置。
- back_chunk/back_pos：begin_chunk用于指向队列的最后一个chunk，back_pos用于指向最后一个chunk的最后一个元素的索引位置。
- end_chunk/end_pos：在最后一个chunk未满的情况下，end_chunk和back_chunk是相同的，back_pos的下一个就是end_pos。在最后一个chunk满的情况下，end_chunk指向新分配的chunk，end_pos=0。也就是说end_chunk和end_pos是辅助back_chunk/back_pos的，可以理解为探测。

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='720' height='528'></svg>)

**2.4.2 函数介绍**

frount和pop连用。back和push连用。

**构造函数yqueue_t**

预先分配⼀个chunk。

```text
//  创建队列.
inline yqueue_t() {
    begin_chunk = (chunk_t *) malloc(sizeof(chunk_t));
    alloc_assert(begin_chunk);
    begin_pos = 0;
    back_chunk = NULL; //back_chunk总是指向队列中最后一个元素所在的chunk，现在还没有元素，所以初始为空
    back_pos = 0;
    end_chunk = begin_chunk; //end_chunk总是指向链表的最后一个chunk
    end_pos = 0;
}
```

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='301' height='283'></svg>)

**稀构函数~yqueue_t**

销毁所有的chunk

```text
//  销毁队列.
inline ~yqueue_t() {
    while (true) {
        if (begin_chunk == end_chunk) {
            free(begin_chunk);
            break;
        }
        chunk_t *o = begin_chunk;
        begin_chunk = begin_chunk->next;
        free(o);
    }

    chunk_t *sc = spare_chunk.xchg(NULL);
    free(sc);
}
```

**front、back函数**

这⾥的front()或者back()函数，需要注意的返回的是左值引⽤，我们可以修改其值。

对于先进后出队列⽽⾔：

- begin_chunk->values[begin_pos]代表队列头可读元素， 读取队列头元素即是读取begin_pos位置的元素；
- back_chunk->values[back_pos]代表队列尾可写元素，写⼊元素时则是更新back_pos位置的元素，要确保元素真正⽣效，还需要调⽤push函数更新back_pos的位置，避免下次更新的时候⼜是更新当前back_pos位置对应的元素。

```text
//  Returns reference to the front element of the queue.
//  If the queue is empty, behaviour is undefined.
// 返回队列头部元素的引用，调用者可以通过该引用更新元素，结合pop实现出队列操作。
inline T &front() // 返回的是引用，是个左值，调用者可以通过其修改容器的值
{
    return begin_chunk->values[begin_pos];
}

//  Returns reference to the back element of the queue.
//  If the queue is empty, behaviour is undefined.
// 返回队列尾部元素的引用，调用者可以通过该引用更新元素，结合push实现插入操作。
// 如果队列为空，该函数是不允许被调用。
inline T &back() // 返回的是引用，是个左值，调用者可以通过其修改容器的值
{
    return back_chunk->values[back_pos];
}
```

**push函数**

- 当++end_pos != N 时，说明当前的chunk还有空余位置可以插入，则不需要扩容
- 当++end_pos == N时，说明当前的chunk已经插入满了，下一次插入就要插入到新的chunk了，所以需要发生扩容

需要新分配chunk时，先尝试从spare_chunk获取，如果获取到则直接使⽤，如果spare_chunk为NULL则需要重新分配chunk。最终都是要更新end_chunk和end_pos。

```text
//  Adds an element to the back end of the queue.
    inline void push() {
        back_chunk = end_chunk;
        back_pos = end_pos; //

        if (++end_pos != N) //end_pos!=N表明这个chunk节点还没有满
            return;

        chunk_t *sc = spare_chunk.xchg(NULL); // 为什么设置为NULL？ 因为如果把之前值取出来了则没有spare chunk了，所以设置为NULL
        if (sc)                               // 如果有spare chunk则继续复用它
        {
            end_chunk->next = sc;
            sc->prev = end_chunk;
        }
        else // 没有则重新分配
        {
            // static int s_cout = 0;
            // printf("s_cout:%d\n", ++s_cout);
            end_chunk->next = (chunk_t *) malloc(sizeof(chunk_t)); // 分配一个chunk
            alloc_assert(end_chunk->next);
            end_chunk->next->prev = end_chunk;
        }
        end_chunk = end_chunk->next;
        end_pos = 0;
    }
```

![img](https://pic1.zhimg.com/80/v2-23afa5ab083f07310f0832ae9d5b8568_720w.webp)

**unpush函数**

unpush函数没什么好说的，也是考虑有没有发生扩容的情况，然后分两种情况回退即可。

```text
//  Removes element from the back end of the queue. In other words
 //  it rollbacks last push to the queue. Take care: Caller is
 //  responsible for destroying the object being unpushed.
 //  The caller must also guarantee that the queue isn't empty when
 //  unpush is called. It cannot be done automatically as the read
 //  side of the queue can be managed by different, completely
 //  unsynchronised thread.
 // 必须要保证队列不为空，参考ypipe_t的uwrite
 inline void unpush() {
     //  First, move 'back' one position backwards.
     if (back_pos) // 从尾部删除元素
         --back_pos;
     else {
         back_pos = N - 1; // 回退到前一个chunk
         back_chunk = back_chunk->prev;
     }

     //  Now, move 'end' position backwards. Note that obsolete end chunk
     //  is not used as a spare chunk. The analysis shows that doing so
     //  would require free and atomic operation per chunk deallocated
     //  instead of a simple free.
     if (end_pos) // 意味着当前的chunk还有其他元素占有
         --end_pos;
     else {
         end_pos = N - 1; // 当前chunk没有元素占用，则需要将整个chunk释放
         end_chunk = end_chunk->prev;
         free(end_chunk->next);
         end_chunk->next = NULL;
     }
 }
```

**pop函数**

- ++begin_pos != N，说明当前chunk还有元素没被取出，该chunk还要继续被使⽤；
- ++end_pos == N，说明该chunk的所有元素已经被取出，所以该chunk要被回收。把最后回收的chunk保存到spare_chunk，然后释放之前spare_chunk保存的chunk。

这⾥有两个点需要注意：

1. pop掉的元素，其销毁⼯作交给调⽤者完成，即是pop前调⽤者需要通过front()接⼝读取并进⾏销毁
2. 空闲块的保存，要求是原⼦操作。因为闲块是读写线程的共享变量，因为在push中也使⽤了spare_chunk。

```text
//  Removes an element from the front end of the queue.
inline void pop() {
    if (++begin_pos == N) // 删除满一个chunk才回收chunk
    {
        chunk_t *o = begin_chunk;
        begin_chunk = begin_chunk->next;
        begin_chunk->prev = NULL;
        begin_pos = 0;

        //  'o' has been more recently used than spare_chunk,
        //  so for cache reasons we'll get rid of the spare and
        //  use 'o' as the spare.
        chunk_t *cs = spare_chunk.xchg(o); //由于局部性原理，总是保存最新的空闲块而释放先前的空闲快
        free(cs);
    }
}
```

### 3.5 ypipe—> yqueue的封装

yqueue 负责元素内存的分配与释放，入队以及出队列；ypipe 负责 yqueue 读写指针的变化。ypipe_t在yqueue_t的基础上构建⼀个单写单读的⽆锁队列

```text
template<typename T, int N>
class ypipe_t {
public:
    // Initialises the pipe.
    inline ypipe_t();

    // The destructor doesn't have to be virtual. It is mad virtual
    // just to keep ICC and code checking tools from complaining.
    inline virtual ~ypipe_t();

    // Write an item to the pipe. Don't flush it yet. If incomplete is
    // set to true the item is assumed to be continued by items
    // subsequently written to the pipe. Incomplete items are never flushed down the stream.
    // 写⼊数据，incomplete参数表示写⼊是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。
    inline void write(const T &value_, bool incomplete_);

    // Pop an incomplete item from the pipe. Returns true is such
    // item exists, false otherwise.
    inline bool unwrite(T *value_);

    // Flush all the completed items into the pipe. Returns false if
    // the reader thread is sleeping. In that case, caller is obliged to
    // wake the reader up before using the pipe again.
    // 刷新所有已经完成的数据到管道，返回false意味着读线程在休眠，在这种情况下调⽤者需要唤醒读线程。
    inline bool flush();

    // Check whether item is available for reading.
    // 这⾥⾯有两个点，⼀个是检查是否有数据可读，⼀个是预取
    inline bool check_read();

    // Reads an item from the pipe. Returns false if there is no value.
    // available.
    inline bool read(T *value_);

    // Applies the function fn to the first elemenent in the pipe
    // and returns the value returned by the fn.
    // The pipe mustn't be empty or the function crashes.
    inline bool probe(bool (*fn)(T &));

protected:
    // Allocation-efficient queue to store pipe items.
    // Front of the queue points to the first prefetched item, back of
    // the pipe points to last un-flushed item. Front is used only by
    // reader thread, while back is used only by writer thread.
    yqueue_t<T, N> queue;
    // Points to the first un-flushed item. This variable is used
    // exclusively by writer thread.
    T *w;//指向第⼀个未刷新的元素,只被写线程使⽤
    // Points to the first un-prefetched item. This variable is used
    // exclusively by reader thread.
    T *r;//指向第⼀个还没预提取的元素，只被读线程使⽤
    // Points to the first item to be flushed in the future.
    T *f;//指向下⼀轮要被刷新的⼀批元素中的第⼀个
    // The single point of contention between writer and reader thread.
    // Points past the last flushed item. If it is NULL,
    // reader is asleep. This pointer should be always accessed using
    // atomic operations.
    atomic_ptr_t<T> c;//读写线程共享的指针，指向每⼀轮刷新的起点（看代码的时候会详细说）。当c为空时，表示读线程睡眠（只会在读线程中被设置为空）
    // Disable copying of ypipe object.
    ypipe_t(const ypipe_t &);
    const ypipe_t &operator=(const ypipe_t &);
};
```

#### **3.5.1 如何写入和读取**

这一节的目的是找出改变wrfc这四个指针的的函数，至于函数的具体作用会放下下面写。

写入可以单独写，也可以批量写，先来看看write函数。可以看到如果incomplete_=true，则说明在批量写，直到incomplete_=false时，进行写提交。

```text
//  Write an item to the pipe.  Don't flush it yet. If incomplete is
//  set to true the item is assumed to be continued by items
//  subsequently written to the pipe. Incomplete items are never flushed down the stream.
// 写入数据，incomplete参数表示写入是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。
inline void write(const T &value_, bool incomplete_) {
    //  Place the value to the queue, add new terminator element.
    queue.back() = value_;
    queue.push();

    //  Move the "flush up to here" poiter.
    if (!incomplete_) {
        f = &queue.back(); // 记录要刷新的位置
    }
}
```



```text
//1. 单次写
yquque.write(count,false);
yquque.flush();
//2. 批量写
yquque.write(count,true);
yquque.write(count,true);
yquque.write(count,false);
yquque.flush();
```

上面两种方式最后都用到了flush，下面来看看flush。

```text
//  Flush all the completed items into the pipe. Returns false if
//  the reader thread is sleeping. In that case, caller is obliged to
//  wake the reader up before using the pipe again.
// 刷新所有已经完成的数据到管道，返回false意味着读线程在休眠，在这种情况下调用者需要唤醒读线程。
// 批量刷新的机制， 写入批量后唤醒读线程；
// 反悔机制 unwrite
inline bool flush() {
    //  If there are no un-flushed items, do nothing.
    if (w == f) // 不需要刷新，即是还没有新元素加入
        return true;

    //  Try to set 'c' to 'f'.
    // read时如果没有数据可以读取则c的值会被置为NULL
    if (c.cas(w, f) != w) // 尝试将c设置为f，即是准备更新w的位置
    {

        //  Compare-and-swap was unseccessful because 'c' is NULL.
        //  This means that the reader is asleep. Therefore we don't
        //  care about thread-safeness and update c in non-atomic
        //  manner. We'll return false to let the caller know
        //  that reader is sleeping.
        c.set(f); // 更新为新的f位置
        w = f;
        return false; //线程看到flush返回false之后会发送一个消息给读线程，这需要写业务去做处理
    }
    else  // 读端还有数据可读取
    {
        //  Reader is alive. Nothing special to do now. Just move
        //  the 'first un-flushed item' pointer to 'f'.
        w = f;             // 更新f的位置
        return true;
    }
}
```

写入分析完了，来看看如何读。

```text
//  Check whether item is available for reading.
// 这里面有两个点，一个是检查是否有数据可读，一个是预取
inline bool check_read() {
    //  Was the value prefetched already? If so, return.
    if (&queue.front() != r && r) //判断是否在前几次调用read函数时已经预取数据了return true;
        return true;

    //  There's no prefetched value, so let us prefetch more values.
    //  Prefetching is to simply retrieve the
    //  pointer from c in atomic fashion. If there are no
    //  items to prefetch, set c to NULL (using compare-and-swap).
    // 两种情况
    // 1. 如果c值和queue.front()， 返回c值并将c值置为NULL，此时没有数据可读
    // 2. 如果c值和queue.front()， 返回c值，此时可能有数据度的去
    r = c.cas(&queue.front(), NULL); //尝试预取数据

    //  If there are no elements prefetched, exit.
    //  During pipe's lifetime r should never be NULL, however,
    //  it can happen during pipe shutdown when items are being deallocated.
    if (&queue.front() == r || !r) //判断是否成功预取数据
        return false;

    //  There was at least one value prefetched.
    return true;
}

//  Reads an item from the pipe. Returns false if there is no value.
//  available.
inline bool read(T *value_) {
    //  Try to prefetch a value.
    if (!check_read())
        return false;

    //  There was at least one value prefetched.
    //  Return it to the caller.
    *value_ = queue.front();
    queue.pop();
    return true;
}
```

下面来多分析一下，如果read返回false，那么我们应该怎么做？读失败意味着管道内没有可读的数据，所以我们可以休眠，可以让出cpu，也可以条件等待。

这里最正确的做法是用条件等待。上面的flush返回false代表着读端在等待，那么flush返回false后我们应该通知读端。

```text
//读端
if (yqueue.read(&value)) {
    //数据处理
}
else {
    // usleep(100);
    std::unique_lock<std::mutex> lock(ypipe_mutex_);
    ypipe_cond_.wait(lock);
    // sched_yield();
}

//写端
yqueue.write(count, false);
if (!yqueue.flush()) {
    // printf("notify_one\n");
    std::unique_lock<std::mutex> lock(ypipe_mutex_);
    ypipe_cond_.notify_one();
}
```

其实我们初略的观察这些函数，就能发现，这几个函数改变的是w,r,f,c这四个指针，下面来看看这四个指针的具体作用吧。

#### **3.5.2 w,r,f,c图文结合详解（重点理解）**

这里这几个变量非常抽象，要结合着函数来讲

- T *f：指向下一轮要被刷新的一批元素的第一个。
- T *w：指向第一个未刷新的元素，只被写线程使用；
- T *r：指向第一个没有被预提取的元素，只被读线程使用；
- atomic_ptr_t c：读写线程共享的指针，指向每⼀轮刷新的起点。当c为空时，表示读线程睡眠（只会在读线程中被设置为空）
- write()：写⼊数据，incomplete参数表示写⼊是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。完成后会将f = &queue.back();
- unwrite()：在数据没有flush之前可以运⾏反悔 Pop an incomplete item from the pipe. Returns true is such item exists, false otherwise.
- bool flush()：将write的元素真正刷新到队列，使读端可以访问对应的数据。返回false意味着读线程在休眠，在这种情况下调⽤者需要唤醒读线程。如果读端阻塞，则c=f;w=f;否则w=f;
- bool check_read()：检测是否有数据可读，如果c==queue.front则c=NULL,否则r=c
- bool read (T *value_)：读数据，将读出的数据写⼊value指针中，返回false意味着没有数据可读

这样写感觉还是非常抽象，下面结合着函数和图来讲这些函数与四个变量的关系吧。

**构造函数ypipe_t()**

在构造函数里面，下一轮要被刷新的元素的第一个(f)，必然是第一个位置；第一个未刷新的元素(w)，也是第一个位置；第一个没有被预读取的元素( r )，也是第一个位置；每一轮刷新的起点，也是第一个位置( c );

![img](https://pic1.zhimg.com/80/v2-516e4a2a7d857674e813c7f8c8e05c98_720w.webp)

```text
inline ypipe_t() {
	//  Insert terminator element into the queue.
	queue.push(); //yqueue_t的尾指针加1，开始back_chunk为空，现在back_chunk指向第一个chunk_t块的第一个位置
	
	//  Let all the pointers to point to the terminator.
	//  (unless pipe is dead, in which case c is set to NULL).
	r = w = f = &queue.back(); //就是让r、w、f、c四个指针都指向这个end迭代器
	c.set(&queue.back());
}
```

**写入函数write(const T &value_, bool incomplete_)**

第二个参数决定是否要刷新一批元素，false时，刷新一批元素，那么下一轮要被刷新的元素的第一个( f ) 就要改变了。

![img](https://pic3.zhimg.com/80/v2-d71f7eb1deaf536d9bfd74e107edfbb6_720w.webp)

~~~text
```cpp
//  Write an item to the pipe.  Don't flush it yet. If incomplete is
//  set to true the item is assumed to be continued by items
//  subsequently written to the pipe. Incomplete items are never flushed down the stream.
// 写入数据，incomplete参数表示写入是否还没完成，在没完成的时候不会修改flush指针，即这部分数据不会让读线程看到。
inline void write(const T &value_, bool incomplete_) {
    //  Place the value to the queue, add new terminator element.
    queue.back() = value_;
    queue.push();

    //  Move the "flush up to here" poiter.
    if (!incomplete_) {
        f = &queue.back(); // 记录要刷新的位置
    }
}
~~~

**刷新元素使元素对读线程可见 bool flush()**

还记得c吗？指向每一轮刷新的起点。如果c和w一样，则尝试将c置为f。刷新元素，指向第一个未刷新的元素( w )，那么必然w=f了。此时前面的元素都可以被读线程可见。

我们来看看什么情况下c != w。

在未更新前队列没有数据可读，没有数据可读的时候，check_read将c⾥⾯的ptr置为NULL。所以会走下面的流程。返回false的⽬的是告诉调⽤者数据读端(接收端)没有数据可读，可能处于休眠的状态，可以结合condition机制，发送⼀个notify唤醒读端继续读取数据。

```text
//  Try to set 'c' to 'f'.
// read时如果没有数据可以读取则c的值会被置为NULL
if (c.cas(w, f) != w) // 尝试将c设置为f，即是准备更新w的位置
{

    //  Compare-and-swap was unseccessful because 'c' is NULL.
    //  This means that the reader is asleep. Therefore we don't
    //  care about thread-safeness and update c in non-atomic
    //  manner. We'll return false to let the caller know
    //  that reader is sleeping.
    c.set(f); // 更新为新的f位置
    w = f;
    return false; //线程看到flush返回false之后会发送一个消息给读线程，这需要写业务去做处理
}
```

未更新前队列有数据可读，此时只需要更新w即可，但此时c值不去更新。

```text
else  // 读端还有数据可读取
{
    //  Reader is alive. Nothing special to do now. Just move
    //  the 'first un-flushed item' pointer to 'f'.
    w = f;             // 更新f的位置
    return true;
}
```

![img](https://pic4.zhimg.com/80/v2-b50779c840bba5a4fe0c72ab66eefd1b_720w.webp)

从write和flush我们也可以看出来，在更新w和f的时候并没有互斥的保护，所以此程序插⼊数据的时候不适合⽤于多线程场景。

flush函数主要是将w更新到f的位置，说明已经写到的位置。

```text
//  Flush all the completed items into the pipe. Returns false if
//  the reader thread is sleeping. In that case, caller is obliged to
//  wake the reader up before using the pipe again.
// 刷新所有已经完成的数据到管道，返回false意味着读线程在休眠，在这种情况下调用者需要唤醒读线程。
// 批量刷新的机制， 写入批量后唤醒读线程；
// 反悔机制 unwrite
inline bool flush() {
    //  If there are no un-flushed items, do nothing.
    if (w == f) // 不需要刷新，即是还没有新元素加入
        return true;

    //  Try to set 'c' to 'f'.
    // read时如果没有数据可以读取则c的值会被置为NULL
    if (c.cas(w, f) != w) // 尝试将c设置为f，即是准备更新w的位置
    {

        //  Compare-and-swap was unseccessful because 'c' is NULL.
        //  This means that the reader is asleep. Therefore we don't
        //  care about thread-safeness and update c in non-atomic
        //  manner. We'll return false to let the caller know
        //  that reader is sleeping.
        c.set(f); // 更新为新的f位置
        w = f;
        return false; //线程看到flush返回false之后会发送一个消息给读线程，这需要写业务去做处理
    }
    else  // 读端还有数据可读取
    {
        //  Reader is alive. Nothing special to do now. Just move
        //  the 'first un-flushed item' pointer to 'f'.
        w = f;             // 更新f的位置
        return true;
    }
}
```

**预取读取函数ckeck_read()**

如果指针r指向的是队头元素（r==&queue.front()）或者r没有指向任何元素（NULL）则说明队列中并没有可读的数据，这个时候check_read尝试去预取数据。所谓的预取就是令 r=c (cas函数就是返回c本身的值，看上⾯关于cas的实现)， ⽽c在write中被指向f（⻅上图），这时从queue.front()到f这个位置的数据都被预取出来了，然后每次调⽤read都能取出⼀段。

![img](https://pic2.zhimg.com/80/v2-a22ff17566ec1087aab1910b08724f35_720w.webp)

值得注意的是，当c==&queue.front()时，代表数据被取完了，这时把c指向NULL，接着读线程会睡眠，这也是给写线程 检查 读线程是否睡眠的标志（c指向NULL）。

继续上⾯写⼊AB数据的场景，第⼀次调⽤read时，会先check_read，把指针r指向指针c的位置（所谓的预取），这时r,c,w,f的关系如下：

![img](https://pic2.zhimg.com/80/v2-40f88f65c5ac33c430209c51eca71475_720w.webp)

为什么要预读取？当front()和r相等时：

- r = c.cas(&queue.front(), NULL);执行之前，如果写端没有flush，那么c置为NULL，说明没有数据可读，返回false。
- r = c.cas(&queue.front(), NULL);执行之前，如果写端调用flush，那么c就不等于front()，则r返回了新的f值，最终返回true。

```text
//  Check whether item is available for reading.
// 这里面有两个点，一个是检查是否有数据可读，一个是预取
inline bool check_read() {
    //  Was the value prefetched already? If so, return.
    if (&queue.front() != r && r) //判断是否在前几次调用read函数时已经预取数据了return true;
        return true;

    //  There's no prefetched value, so let us prefetch more values.
    //  Prefetching is to simply retrieve the
    //  pointer from c in atomic fashion. If there are no
    //  items to prefetch, set c to NULL (using compare-and-swap).
    // 两种情况
    // 1. 如果c值和queue.front()， 返回c值并将c值置为NULL，此时没有数据可读
    // 2. 如果c值和queue.front()， 返回c值，此时可能有数据度的去
    r = c.cas(&queue.front(), NULL); //尝试预取数据

    //  If there are no elements prefetched, exit.
    //  During pipe's lifetime r should never be NULL, however,
    //  it can happen during pipe shutdown when items are being deallocated.
    if (&queue.front() == r || !r) //判断是否成功预取数据
        return false;

    //  There was at least one value prefetched.
    return true;
}

//  Reads an item from the pipe. Returns false if there is no value.
//  available.
inline bool read(T *value_) {
    //  Try to prefetch a value.
    if (!check_read())
        return false;

    //  There was at least one value prefetched.
    //  Return it to the caller.
    *value_ = queue.front();
    queue.pop();
    return true;
}
```

### 3.6总结

_c指针，则是读写线程都可以操作，因此需要使⽤原⼦的CAS操作来修改，它的可能值有以下⼏种：

- NULL：读线程设置，此时意味着已经没有数据可读，读线程在休眠。
- ⾮零：写线程设置，这⾥⼜区分两种情况：

```text
- 旧值为_w的情况下，cas(_w,_f)操作修改为_f，意味着如果原先的值为_w，则原⼦性的修改为_f，表示有更多已被刷新的数据可读。
- 在旧值为NULL的情况下，此时读线程休眠，因此可以安全的设置为当前_f指针的位置。
```



```text
- 写端yquque.write(count,false)；将f = &queue.back();
- 写端yquque.flush();如果c==w，则c=f;w=f;否则w=f;
- 读端check_read();如果c==queue.front则c=NULL否则r更新为f。
```

## 4. ZMQ无锁队列1写1读性能测试

这里分三种测试情况：

- 一次写就提交，read失败就usleep
- 10次写才提交，read失败就yield
- flush失败就notify，read失败就wait

可以看到用cond是效率是最高的，usleep的情况和yield的情况类似，实时性没有cond高。并且按照道理来说，正确的使用方法也是用cond

![img](https://pic3.zhimg.com/80/v2-c3e7b784caf442341fabf01e254c86da_720w.webp)

下面来看一看互斥锁队列 vs 互斥锁+条件变量队列 vs 内存屏障链表 vs RingBuffer CAS 实现。可以看到在一个写线程一个读线程的情况下，我们的ZMQ无锁队列是最快的。

![img](https://pic3.zhimg.com/80/v2-38f5ea4a32847da786d66b318501b13e_720w.webp)

那么在一写一读的场景下，我们就优先选用ZMQ无锁队列即可

![img](https://pic3.zhimg.com/80/v2-e3264df82fa2d8474023a83964599962_720w.webp)

## 5. 如何实现多写多读的无锁队列？

后续的多写多读的无锁队列由下一篇文章再来介绍。

原文地址：https://zhuanlan.zhihu.com/p/552982779

作者：linux

# 【NO.187】网络不通？服务丢包？这篇 TCP 连接状态详解及故障排查，收好了~

我们通过了解TCP各个状态，可以排除和定位网络或系统故障时大有帮助。

### **1.TCP状态**

了解TCP之前，先了解几个命令：

**linux查看tcp的状态命令**：
\1) **`netstat -nat`** 查看TCP各个状态的数量
2)**`lsof -i:port`** 可以检测到打开套接字的状况
\3) **`sar -n SOCK`** 查看tcp创建的连接数
\4) **`tcpdump -iany tcp port 9000`** 对tcp端口为9000的进行抓包

网络测试常用命令;
1）ping:检测网络连接的正常与否,主要是测试延时、抖动、丢包率。

但是很多服务器为了防止攻击，一般会关闭对ping的响应。所以ping一般作为测试连通性使用。

ping命令后，会接收到对方发送的回馈信息，其中记录着对方的IP地址和TTL。TTL是该字段指定IP包被路由器丢弃之前允许通过的最大网段数量。

TTL是IPv4包头的一个8 bit字段。例如IP包在服务器中发送前设置的TTL是64，你使用ping命令后，得到服务器反馈的信息，其中的TTL为56，说明途中一共经过了8道路由器的转发，每经过一个路由，TTL减1。

2）traceroute：raceroute 跟踪数据包到达网络主机所经过的路由工具
**`traceroute hostname`**

3）pathping：是一个路由跟踪工具，它将 ping 和 tracert 命令的功能与这两个工具所不提供的其他信息结合起来，综合了二者的功能

**`pathping www.baidu.com`**

4）mtr：以结合ping nslookup tracert 来判断网络的相关特性

\5) nslookup:用于解析域名，一般用来检测本机的DNS设置是否配置正确。

LISTENING：侦听来自远方的TCP端口的连接请求.
首先服务端需要打开一个socket进行监听，状态为LISTEN。

有提供某种服务才会处于LISTENING状态，**TCP状态变化就是某个端口的状态变化**，提供一个服务就打开一个端口。

例如：提供www服务默认开的是80端口，提供ftp服务默认的端口为21，当提供的服务没有被连接时就处于LISTENING状态。

FTP服务启动后首先处于侦听（LISTENING）状态。处于侦听LISTENING状态时，该端口是开放的，等待连接，但还没有被连接。就像你房子的门已经敞开的，但还没有人进来。

看LISTENING状态最主要的是看本机开了哪些端口，这些端口都是哪个程序开的，关闭不必要的端口是保证安全的一个非常重要的方面，服务端口都对应一个服务（应用程序），停止该服务就关闭了该端口，例如要关闭21端口只要停止IIS服务中的FTP服务即可。关于这方面的知识请参阅其它文章。

如果你不幸中了服务端口的木马，木马也开个端口处于LISTENING状态。

**SYN-SENT：客户端SYN_SENT状态**：
再发送连接请求后等待匹配的连接请求:客户端通过应用程序调用connect进行active open.

于是客户端tcp发送一个SYN以请求建立一个连接.之后状态置为SYN_SENT.

> The socket is actively attempting to establish a connection. 在发送连接请求后等待匹配的连接请求

当请求连接时客户端首先要发送同步信号给要访问的机器，此时状态为SYN_SENT，如果连接成功了就变为ESTABLISHED，正常情况下SYN_SENT状态非常短暂。

例如要访问网站[http://www.baidu.com](https://link.zhihu.com/?target=http%3A//www.baidu.com),如果是正常连接的话，用TCPView观察IEXPLORE.EXE（IE）建立的连接会发现很快从SYN_SENT变为ESTABLISHED，表示连接成功。SYN_SENT状态快的也许看不到。

如果发现有很多SYN_SENT出现，那一般有这么几种情况，一是你要访问的网站不存在或线路不好。

二是用扫描软件扫描一个网段的机器，也会出出现很多SYN_SENT，另外就是可能中了病毒了，例如中了”冲击波”，病毒发作时会扫描其它机器，这样会有很多SYN_SENT出现。

**SYN-RECEIVED：服务器端状态SYN_RCVD**
再收到和发送一个连接请求后等待对方对连接请求的确认
当服务器收到客户端发送的同步信号时，将标志位ACK和SYN置1发送给客户端，此时服务器端处于SYN_RCVD状态，如果连接成功了就变为ESTABLISHED，正常情况下SYN_RCVD状态非常短暂。

**如果发现有很多SYN_RCVD状态，那你的机器有可能被SYN Flood的DoS(拒绝服务攻击)攻击了。**

SYN Flood的攻击原理是：
在进行三次握手时，攻击软件向被攻击的服务器发送SYN连接请求（握手的第一步），但是这个地址是伪造的，如攻击软件随机伪造了51.133.163.104、65.158.99.152等等地址。

服务器在收到连接请求时将标志位ACK和SYN置1发送给客户端（握手的第二步），但是这些客户端的IP地址都是伪造的，服务器根本找不到客户机，也就是说握手的第三步不可能完成。

这种情况下服务器端一般会重试（再次发送SYN+ACK给客户端）并等待一段时间后丢弃这个未完成的连接，这段时间的长度我们称为SYN Timeout，一般来说这个时间是分钟的数量级（大约为30秒-2分钟）；

一个用户出现异常导致服务器的一个线程等待1分钟并不是什么很大的问题，但如果有一个恶意的攻击者大量模拟这种情况，服务器端将为了维护一个非常大的半连接列表而消耗非常多的资源——数以万计的半连接。

即使是简单的保存并遍历也会消耗非常多的CPU时间和内存，何况还要不断对这个列表中的IP进行SYN+ACK的重试。

此时从正常客户的角度看来，服务器失去响应，这种情况我们称做：服务器端受到了**SYN Flood攻击（SYN洪水攻击）**

**ESTABLISHED：代表一个打开的连接。**
ESTABLISHED状态是表示两台机器正在传输数据，观察这个状态最主要的就是看哪个程序正在处于ESTABLISHED状态。

服务器出现很多**ESTABLISHED状态：netstat -nat |grep 9502或者使用lsof -i:9502可以检测到。**

**当客户端未主动close的时候就断开连接：即客户端发送的FIN丢失或未发送。**
这时候若客户端断开的时候发送了FIN包，则服务端将会处于CLOSE_WAIT状态；

这时候若客户端断开的时候未发送FIN包，则服务端处还是显示ESTABLISHED状态；

结果客户端重新连接服务器。

而新连接上来的客户端（也就是刚才断掉的重新连上来了）在服务端肯定是ESTABLISHED; 如果客户端重复的上演这种情况，那么服务端将会出现大量的假的ESTABLISHED连接和CLOSE_WAIT连接。

最终结果就是新的其他客户端无法连接上来，但是利用netstat还是能看到一条连接已经建立，并显示ESTABLISHED，但始终无法进入程序代码。

**FIN-WAIT-1**：等待远程TCP连接中断请求，或先前的连接中断请求的确认
主动关闭(active close)端应用程序调用close，于是其TCP发出FIN请求主动关闭连接，之后进入FIN_WAIT1状态./ *The socket is closed, and the connection is shutting down. 等待远程TCP的连接中断请求，或先前的连接中断请求的确认* /

如果服务器出现shutdown再重启，使用netstat -nat查看，就会看到很多FIN-WAIT-1的状态。就是因为服务器当前有很多客户端连接，直接关闭服务器后，无法接收到客户端的ACK。

**FIN-WAIT-2**：从远程TCP等待连接中断请求
主动关闭端接到ACK后，就进入了FIN-WAIT-2

> Connection is closed, and the socket is waiting for a shutdown from the remote end. 从远程TCP等待连接中断请求

这就是著名的半关闭的状态了，这是在关闭连接时，客户端和服务器两次握手之后的状态。

在这个状态下，应用程序还有接受数据的能力，但是已经无法发送数据，但是也有一种可能是，客户端一直处于FIN_WAIT_2状态，而服务器则一直处于WAIT_CLOSE状态，而直到应用层来决定关闭这个状态。

**CLOSE-WAIT：等待从本地用户发来的连接中断请求**
被动关闭(passive close)端TCP接到FIN后，就发出ACK以回应FIN请求(它的接收也作为文件结束符传递给上层应用程序),并进入CLOSE_WAIT.

> The remote end has shut down, waiting for the socket to close. 等待从本地用户发来的连接中断请求

**CLOSING：等待远程TCP对连接中断的确认**
比较少见

> Both sockets are shut down but we still don’t have all our data sent. 等待远程TCP对连接中断的确认

**LAST-ACK：等待原来的发向远程TCP的连接中断请求的确认**
被动关闭端一段时间后，接收到文件结束符的应用程序将调用CLOSE关闭连接。这导致它的TCP也发送一个
FIN,等待对方的ACK.就进入了LAST-ACK .

> The remote end has shut down, and the socket is closed. Waiting for acknowledgement. 等待原来发向远程TCP的连接中断请求的确认

使用并发压力测试的时候，突然断开压力测试客户端，服务器会看到很多LAST-ACK。

**TIME-WAIT：等待足够的时间以确保远程TCP接收到连接中断请求的确认**
在主动关闭端接收到FIN后，TCP就发送ACK包，并进入TIME-WAIT状态。

> The socket is waiting after close to handle
> packets still in the network.等待足够的时间以确保远程TCP接收到连接中断请求的确认

TIME_WAIT等待状态，这个状态又叫做2MSL状态，说的是在TIME_WAIT2发送了最后一个ACK数据报以后，要进入TIME_WAIT状态，这个状态是防止最后一次握手的数据报没有传送到对方那里而准备的（注意这不是四次握手，这是第四次握手的保险状态）。

这个状态在很大程度上保证了双方都可以正常结束，但是，问题也来了。

由于插口的2MSL状态（插口是IP和端口对的意思，socket），使得应用程序在2MSL时间内是无法再次使用同一个插口的，对于客户程序还好一些，但是对于服务程序，例如httpd，它总是要使用同一个端口来进行服务，而在2MSL时间内，启动httpd就会出现错误（插口被使用）。

为了避免这个错误，服务器给出了一个平静时间的概念，这是说在2MSL时间内，虽然可以重新启动服务器，但是这个服务器还是要平静的等待2MSL时间的过去才能进行下一次连接。

详情请看：TIME_WAIT引起Cannot assign requested address报错

**CLOSED：没有任何连接状态**
被动关闭端在接受到ACK包后，就进入了closed的状态。连接结束

> The socket is not being used. 没有任何连接状态

### **2.TCP状态迁移路线图**

client/server两条路线讲述TCP状态迁移路线图：

![img](https://pic4.zhimg.com/80/v2-7f4750683e6f671cc5b05bdef90f0193_720w.webp)

这是一个看起来比较复杂的状态迁移图，因为它包含了两个部分—-服务器的状态迁移和客户端的状态迁移，如果从某一个角度出发来看这个图，就会清晰许多，这里面的服务器和客户端都不是绝对的，发送数据的就是客户端，接受数据的就是服务器。

客户端应用程序的状态迁移图
客户端的状态可以用如下的流程来表示：

CLOSED->SYN_SENT->ESTABLISHED->FIN_WAIT_1->FIN_WAIT_2->TIME_WAIT->CLOSED

以上流程是在程序正常的情况下应该有的流程，从书中的图中可以看到，在建立连接时，当客户端收到SYN报文的ACK以后，客户端就打开了数据交互地连接。

而结束连接则通常是客户端主动结束的，客户端结束应用程序以后，需要经历FIN_WAIT_1，FIN_WAIT_2等状态，这些状态的迁移就是前面提到的结束连接的四次握手。

服务器的状态迁移图
服务器的状态可以用如下的流程来表示：

CLOSED->LISTEN->SYN收到->ESTABLISHED->CLOSE_WAIT->LAST_ACK->CLOSED

在建立连接的时候，服务器端是在第三次握手之后才进入数据交互状态，而关闭连接则是在关闭连接的第二次握手以后（注意不是第四次）。而关闭以后还要等待客户端给出最后的ACK包才能进入初始的状态。

**其他状态迁移**
还有一些其他的状态迁移，这些状态迁移针对服务器和客户端两方面的总结如下

LISTEN->SYN*SENT，对于这个解释就很简单了，服务器有时候也要打开连接的嘛。*

*SYN_SENT->SYN收到，服务器和客户端在SYN_SENT状态下如果收到SYN数据报，则都需要发送SYN的ACK数据报并把自己的状态调整到SYN收到状态，准备进入ESTABLISHED*
*SYN_SENT->CLOSED，在发送超时的情况下，会返回到CLOSED状态。*

*SYN*收到->LISTEN，如果受到RST包，会返回到LISTEN状态。

SYN_收到->FIN_WAIT_1，这个迁移是说，可以不用到ESTABLISHED状态，而可以直接跳转到FIN_WAIT_1状态并等待关闭。

![img](https://pic1.zhimg.com/80/v2-eda25767646980fc29d9ac7aef1860cc_720w.webp)

怎样牢牢地将这张图刻在脑中呢？那么你就一定要对这张图的每一个状态，及转换的过程有深刻的认识，不能只停留在一知半解之中。

下面对这张图的11种状态详细解析一下，以便加强记忆！不过在这之前，先回顾一下TCP建立连接的三次握手过程，以及关闭连接的四次握手过程。

### **3.TCP连接建立三次握手**

TCP是一个面向连接的协议，所以在连接双方发送数据之前，都需要首先建立一条连接。

**Client连接Server**：
当Client端调用socket函数调用时，相当于Client端产生了一个处于Closed状态的套接字。

**(1)第一次握手**：Client端又调用connect函数调用，系统为Client随机分配一个端口，连同传入connect中的参数(Server的IP和端口)，这就形成了一个连接四元组，客户端发送一个带SYN标志的TCP报文到服务器。

这是三次握手过程中的报文1。connect调用让Client端的socket处于SYN_SENT状态，等待服务器确认；SYN：同步序列编号(Synchronize Sequence Numbers)。

**(2)第二次握手**：服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包，此时服务器进入SYN_RECV状态；

**(3) 第三次握手**：客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1)，此包发送完毕，客户器和客务器进入ESTABLISHED状态，完成三次握手。连接已经可以进行读写操作。

一个完整的三次握手也就是：请求—-应答—-再次确认。
TCP协议通过三个报文段完成连接的建立，这个过程称为三次握手(three-way handshake)，过程如下图所示。

对应的函数接口：

![img](https://pic4.zhimg.com/80/v2-c9c0c9354295d05a8bc178c45f8d28bb_720w.webp)

2)Server
当Server端调用socket函数调用时，相当于Server端产生了一个处于Closed状态的监听套接字，Server端调用bind操作，将监听套接字与指定的地址和端口关联，然后又调用listen函数，系统会为其分配未完成队列和完成队列，此时的监听套接字可以接受Client的连接，监听套接字状态处于LISTEN状态。

当Server端调用accept操作时，会从完成队列中取出一个已经完成的client连接，同时在server这段会产生一个会话套接字，用于和client端套接字的通信，这个会话套接字的状态是ESTABLISH。

从图中可以看出，当客户端调用connect时，触发了连接请求，向服务器发送了SYN J包，这时connect进入阻塞状态；

服务器监听到连接请求，即收到SYN J包，调用accept函数接收请求向客户端发送SYN K ，ACK J+1，这时accept进入阻塞状态；客户端收到服务器的SYN K ，ACK J+1之后，这时connect返回，并对SYN K进行确认；服务器收到ACK K+1时，accept返回，至此三次握手完毕，连接建立。

我们可以通过网络抓包的查看具体的流程：

比如我们服务器开启9502的端口。使用tcpdump来抓包：tcpdump -iany tcp port 9502

然后我们使用telnet 127.0.0.1 9502开连接:

![img](https://pic1.zhimg.com/80/v2-a6d9516c6e81ad45ded6cde751f75060_720w.webp)

我们看到 （1）（2）（3）三步是建立tcp：
第一次握手：
**`14:12:45.104687 IP localhost.39870 > localhost.9502: Flags [S], seq 2927179378`**
客户端**`IP localhost.39870`** (客户端的端口一般是自动分配的) 向服务器**`localhost.9502`** 发送syn包(syn=j)到服务器》

syn的seq=**`2927179378`**

第二次握手：
**`14:12:45.104701 IP localhost.9502 > localhost.39870: Flags`** [S.], seq 1721825043, ack 2927179379,
服务器收到syn包，必须确认客户的SYN（ack=j+1），同时自己也发送一个SYN包（syn=k），即SYN+ACK包
SYN（ack=j+1）=ack 2927179379 服务器主机SYN包（syn=seq 1721825043）

第三次握手：
**`14:12:45.104711 IP localhost.39870 > localhost.9502: Flags [.], ack 1,`**
客户端收到服务器的SYN+ACK包，向服务器发送确认包ACK(ack=k+1)
客户端和服务器进入ESTABLISHED状态后，可以进行通信数据交互。此时和accept接口没有关系，即使没有accepte，也进行3次握手完成。

连接出现连接不上的问题，一般是网路出现问题或者网卡超负荷或者是连接数已经满啦。

紫色背景的部分：

IP localhost.39870 > localhost.9502: Flags [P.], seq 1:8, ack 1, win 4099, options [nop,nop,TS val 255478182 ecr 255474104], length 7
客户端向服务器发送长度为7个字节的数据，

IP localhost.9502 > localhost.39870: Flags [.], ack 8, win 4096, options [nop,nop,TS val 255478182 ecr 255478182], length 0
服务器向客户确认已经收到数据

IP localhost.9502 > localhost.39870: Flags [P.], seq 1:19, ack 8, win 4096, options [nop,nop,TS val 255478182 ecr 255478182], length 18
然后服务器同时向客户端写入数据。

IP localhost.39870 > localhost.9502: Flags [.], ack 19, win 4097, options [nop,nop,TS val 255478182 ecr 255478182], length 0
客户端向服务器确认已经收到数据

这个就是tcp可靠的连接，每次通信都需要对方来确认。

### **4. TCP连接的终止（四次握手释放）**

由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个 FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。

首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。

建立一个连接需要三次握手，而终止一个连接要经过四次握手，这是由TCP的半关闭(half-close)造成的，如图：

![img](https://pic1.zhimg.com/80/v2-21dbe7a57efc7f7725cd3850d69c4674_720w.webp)

（1）客户端A发送一个FIN，用来关闭客户A到服务器B的数据传送（报文段4）。
（2）服务器B收到这个FIN，它发回一个ACK，确认序号为收到的序号加1（报文段5）。和SYN一样，一个FIN将占用一个序号。
（3）服务器B关闭与客户端A的连接，发送一个FIN给客户端A（报文段6）。
（4）客户端A发回ACK报文确认，并将确认序号设置为收到序号加1（报文段7）。

对应函数接口如图：

![img](https://pic1.zhimg.com/80/v2-0dad8dda95961a85d94ca68c897ff330_720w.webp)

调用过程如下：

\1) 当client想要关闭它与server之间的连接。client（某个应用进程）首先调用close主动关闭连接，这时TCP发送一个FIN M；client端处于FIN_WAIT1状态。

\2) 当server端接收到FIN M之后，执行被动关闭。对这个FIN进行确认，返回给client ACK。

当server端返回给client ACK后，client处于FIN_WAIT2状态，server处于CLOSE_WAIT状态。它的接收也作为文件结束符传递给应用进程，因为FIN的接收 意味着应用进程在相应的连接上再也接收不到额外数据；

\3) 一段时间之后，当server端检测到client端的关闭操作(read返回为0)。接收到文件结束符的server端调用close关闭它的socket。这导致server端的TCP也发送一个FIN N；此时server的状态为LAST_ACK。

\4) 当client收到来自server的FIN后 。client端的套接字处于TIME_WAIT状态，它会向server端再发送一个ack确认，此时server端收到ack确认后，此套接字处于CLOSED状态。

这样每个方向上都有一个FIN和ACK。

1．为什么建立连接协议是三次握手，而关闭连接却是四次握手呢？

这是因为服务端的LISTEN状态下的SOCKET当收到SYN报文的建连请求后，它可以把ACK和SYN（ACK起应答作用，而SYN起同步作用）放在一个报文里来发送。但关闭连接时，当收到对方的FIN报文通知时，它仅仅表示对方没有数据发送给你了；

但未必你所有的数据都全部发送给对方了，所以你可以未必会马上会关闭SOCKET,也即你可能还需要发送一些数据给对方之后，再发送FIN报文给对方来表示你同意现在可以关闭连接了，所以它这里的ACK报文和FIN报文多数情况下都是分开发送的。

2．为什么TIME_WAIT状态还需要等2MSL后才能返回到CLOSED状态？

这是因为虽然双方都同意关闭连接了，而且握手的4个报文也都协调和发送完毕，按理可以直接回到CLOSED状态（就好比从SYN_SEND状态到ESTABLISH状态那样）：

一方面是可靠的实现TCP全双工连接的终止，也就是当最后的ACK丢失后，被动关闭端会重发FIN，因此主动关闭端需要维持状态信息，以允许它重新发送最终的ACK。

另一方面，但是因为我们必须要假想网络是不可靠的，你无法保证你最后发送的ACK报文会一定被对方收到，因此对方处于LAST_ACK状态下的SOCKET可能会因为超时未收到ACK报文，而重发FIN报文，所以这个TIME_WAIT状态的作用就是用来重发可能丢失的ACK报文。

TCP在2MSL等待期间，定义这个连接(4元组)不能再使用，任何迟到的报文都会丢弃。设想如果没有2MSL的限制，恰好新到的连接正好满足原先的4元组，这时候连接就可能接收到网络上的延迟报文就可能干扰最新建立的连接。

3、发现系统存在大量TIME_WAIT状态的连接，可以通过调整内核参数解决：vi /etc/sysctl.conf 加入以下内容：
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_fin_timeout = 30
然后执行 /sbin/sysctl -p 让参数生效。
net.ipv4.tcp_syncookies = 1 表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭；
net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；
net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。
net.ipv4.tcp_fin_timeout 修改系統默认的 TIMEOUT 时间

### **5.同时打开**

两个应用程序同时执行主动打开的情况是可能的，虽然发生的可能性较低。每一端都发送一个SYN,并传递给对方，且每一端都使用对端所知的端口作为本地端口。例如：

主机a中一应用程序使用7777作为本地端口，并连接到主机b 8888端口做主动打开。

主机b中一应用程序使用8888作为本地端口，并连接到主机a 7777端口做主动打开。

tcp协议在遇到这种情况时，只会打开一条连接。

这个连接的建立过程需要4次数据交换，而一个典型的连接建立只需要3次交换（即3次握手）

但多数伯克利版的tcp/ip实现并不支持同时打开。

![img](https://pic1.zhimg.com/80/v2-76b1093a6224a27b3b00107d7dcab3b8_720w.webp)

### **6.同时关闭**

如果应用程序同时发送FIN，则在发送后会首先进入FIN_WAIT_1状态。在收到对端的FIN后，回复一个ACK，会进入CLOSING状态。在收到对端的ACK后，进入TIME_WAIT状态。这种情况称为同时关闭。

同时关闭也需要有4次报文交换，与典型的关闭相同。

### **7. TCP的FLAGS说明**

在TCP层，有个FLAGS字段，这个字段有以下几个标识：SYN, FIN, ACK, PSH, RST, URG.
其中，对于我们日常的分析有用的就是前面的五个字段。

**一、字段含义**：
**1、SYN表示建立连接**：
步序列编号(Synchronize Sequence Numbers)栏有效。该标志仅在三次握手建立TCP连接时有效。它提示TCP连接的服务端检查序列编号，该序列编号为TCP连接初始端(一般是客户端)的初始序列编号。在这里，可以把TCP序列编号看作是一个范围从0到4，294，967，295的32位计数器。通过TCP连接交换的数据中每一个字节都经过序列编号。在TCP报头中的序列编号栏包括了TCP分段中第一个字节的序列编号。

**2、FIN表示关闭连接**：

**3、ACK表示响应**：
确认编号(Acknowledgement Number)栏有效。大多数情况下该标志位是置位的。TCP报头内的确认编号栏内包含的确认编号(w+1，Figure-1)为下一个预期的序列编号，同时提示远端系统已经成功接收所有数据。

**4、PSH表示有DATA数据传输**：

5、RST表示连接重置：复位标志有效。用于复位相应的TCP连接。

**二、字段组合含义**：

![img](https://pic1.zhimg.com/80/v2-9a6b838c32270ca3255100a78d022ea8_720w.webp)

其中，ACK是可能与SYN，FIN等同时使用的，比如SYN和ACK可能同时为1，它表示的就是建立连接之后的响应，
如果只是单个的一个SYN，它表示的只是建立连接。
TCP的几次握手就是通过这样的ACK表现出来的。
但SYN与FIN是不会同时为1的，因为前者表示的是建立连接，而后者表示的是断开连接。

RST一般是在FIN之后才会出现为1的情况，表示的是连接重置。
一般地，当出现FIN包或RST包时，我们便认为客户端与服务器端断开了连接；

![img](https://pic4.zhimg.com/80/v2-c1b1a6f08bc39ba2cb18557cb3ed98a3_720w.webp)

**RST与ACK标志位都置一了，并且具有ACK number，非常明显，这个报文在释放TCP连接的同时，完成了对前面已接收报文的确认。**

而当出现SYN和SYN＋ACK包时，我们认为客户端与服务器建立了一个连接。

PSH为1的情况，一般只出现在 DATA内容不为0的包中，也就是说PSH为1表示的是有真正的TCP数据包内容被传递。

TCP的连接建立和连接关闭，都是通过请求－响应的模式完成的。

### **8. TCP通信中服务器处理客户端意外断开**

如果TCP连接被对方正常关闭，也就是说，对方是正确地调用了closesocket(s)或者shutdown(s)的话，那么上面的Recv或Send调用就能马上返回，并且报错。这是由于close socket(s)或者shutdown(s)有个正常的关闭过程，会告诉对方“TCP连接已经关闭，你不需要再发送或者接受消息了”。

但是，如果意外断开，客户端（3g的移动设备）并没有正常关闭socket。双方并未按照协议上的四次挥手去断开连接。

那么这时候正在执行Recv或Send操作的一方就会因为没有任何连接中断的通知而一直等待下去，也就是会被长时间卡住。

像这种如果一方已经关闭或异常终止连接，而另一方却不知道，我们将这样的TCP连接称为半打开的。

解决意外中断办法都是利用保活机制。而保活机制分又可以让底层实现也可自己实现。

**1、自己编写心跳包程序**

简单的说也就是在自己的程序中加入一条线程，定时向对端发送数据包，查看是否有ACK，如果有则连接正常，没有的话则连接断开

**2、启动TCP编程里的keepAlive机制**

**一、双方拟定心跳（自实现）**
一般由客户端发送心跳包，服务端并不回应心跳，只是定时轮询判断一下与上次的时间间隔是否超时（超时时间自己设定）。服务器并不主动发送是不想增添服务器的通信量，减少压力。

但这会出现三种情况：

**情况1.**
客户端由于某种网络延迟等原因很久后才发送心跳（它并没有断），这时服务器若利用自身设定的超时判断其已经断开，而后去关闭socket。若客户端有重连机制，则客户端会重新连接。若不确定这种方式是否关闭了原本正常的客户端，则在ShutDown的时候一定要选择send,表示关闭发送通道，服务器还可以接收一下，万一客户端正在发送比较重要的数据呢，是不？

**情况2.**
客户端很久没传心跳，确实是自身断掉了。在其重启之前，服务端已经判断出其超时，并主动close，则四次挥手成功交互。

**情况3.**
客户端很久没传心跳，确实是自身断掉了。在其重启之前，服务端的轮询还未判断出其超时，在未主动close的时候该客户端已经重新连接。

这时候若客户端断开的时候发送了FIN包，则服务端将会处于CLOSE_WAIT状态；

这时候若客户端断开的时候未发送FIN包，则服务端处还是显示ESTABLISHED状态；

而新连接上来的客户端（也就是刚才断掉的重新连上来了）在服务端肯定是ESTABLISHED;这时候就有个问题，若利用轮询还未检测出上条旧连接已经超时（这很正常，timer总有个间隔吧），而在这时，客户端又重复的上演情况3，那么服务端将会出现大量的假的ESTABLISHED连接和CLOSE_WAIT连接。

最终结果就是新的其他客户端无法连接上来，但是利用netstat还是能看到一条连接已经建立，并显示ESTABLISHED，但始终无法进入程序代码。

个人最初感觉导致这种情况是因为假的ESTABLISHED连接和CLOSE_WAIT连接会占用较大的系统资源，程序无法再次创建连接（因为每次我发现这个问题的时候我只连了10个左右客户端却已经有40多条无效连接）。

而最近几天测试却发现有一次程序内只连接了2，3个设备，但是有8条左右的虚连接，此时已经连接不了新客户端了。

这时候我就觉得我想错了，不可能这几条连接就占用了大量连接把，如果说几十条还有可能。但是能肯定的是，这个问题的产生绝对是设备在不停的重启，而服务器这边又是简单的轮询，并不能及时处理，暂时还未能解决。

**二、利用KeepAlive**
其实keepalive的原理就是TCP内嵌的一个心跳包,

以服务器端为例，如果当前server端检测到超过一定时间（默认是 7,200,000 milliseconds，也就是2个小时）没有数据传输，那么会向client端发送一个keep-alive packet（该keep-alive packet就是ACK和当前TCP序列号减一的组合），此时client端应该为以下三种情况之一：

1. client端仍然存在，网络连接状况良好。此时client端会返回一个ACK。server端接收到ACK后重置计时器（复位存活定时器），在2小时后再发送探测。如果2小时内连接上有数据传输，那么在该时间基础上向后推延2个小时。
2. 客户端异常关闭，或是网络断开。在这两种情况下，client端都不会响应。服务器没有收到对其发出探测的响应，并且在一定时间（系统默认为1000 ms）后重复发送keep-alive packet，并且重复发送一定次数（2000 XP 2003 系统默认为5次, Vista后的系统默认为10次）。
3. 客户端曾经崩溃，但已经重启。这种情况下，服务器将会收到对其存活探测的响应，但该响应是一个复位，从而引起服务器对连接的终止。

对于应用程序来说，2小时的空闲时间太长。因此，我们需要手工开启Keepalive功能并设置合理的Keepalive参数。

全局设置可更改`/etc/sysctl.conf`,加上:
`net.ipv4.tcp_keepalive_intvl = 20`
`net.ipv4.tcp_keepalive_probes = 3`
`net.ipv4.tcp_keepalive_time = 60`

在程序中设置如下:

![img](https://pic2.zhimg.com/80/v2-dc678ec50496a74c4b7b402cdbbec385_720w.webp)

在程序中表现为,当tcp检测到对端socket不再可用时(不能发出探测包,或探测包没有收到ACK的响应包),select会返回socket可读,并且在recv时返回-1,同时置上errno为ETIMEDOUT.

### **9. Linux错误信息(errno)列表**

经常出现的错误：

22：参数错误，比如ip地址不合法，没有目标端口等

101：网络不可达，比如不能ping通

111：链接被拒绝，比如目标关闭链接等

115：当链接设置为非阻塞时，目标没有及时应答，返回此错误，socket可以继续使用。比如socket连接

原文地址：https://zhuanlan.zhihu.com/p/550687700

作者：linux

# 【NO.188】Linux编程：一个异步信号处理引起死锁问题的思考

## 1.前言

最近在维护别人的代码时，遇到一个线程死锁问题，一番折腾，最终定位到的是“信号异步处理引发死锁问题”。“信号异步处理死锁问题”是一个老生常谈的问题了，虽然问题简单，但定位起来仍需花费点时间；如果代码量大、复现概率低，还需花费更多的人力。因此，有必要回顾下这个问题，避免踩坑，包括新手和老手都可能踩坑。

死锁问题原型伪代码：

```text
void signal_handle(int signo) 
{
	pthread_mutex_lock(&mutex);
	/* todo */
	pthread_mutex_unlock(&mutex);
	return; 
}

int main(int argc, char *argv)
{
	signal(SIGALRM, signal_handle);
	pthread_mutex_init(&mutex, NULL);
	printf("Main thread id:0x%lx\n", syscall(SYS_gettid));
	for(;;)
	{
		pthread_mutex_lock(&mutex);
		/* todo */
		pthread_mutex_unlock(&mutex);
		usleep(10);
	}
	return 0; 
}
```

## 2.为什么会产生死锁

### 2.1 死锁

死锁（DeadLock) 是指两个或者两个以上的进程（线程）在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程（线程）称为死锁进程(线程）。

根据死锁的定义，死锁产生的条件是：

- 两个进程（线程）以上
- 有资源竞争

### 2.2 分析

对于信号回调函数，与主线程是同一线程（线程ID相同），不满足两个进程（线程）的条件，为什么会发生死锁呢？下面我们先通过一段代码验证信号回调函数与主线程是否为同一个线程。

```text
#include <stdio.h>
#include <unistd.h>
#include <signal.h>
#include <sys/syscall.h>

void signal_handle(int signo) 
{
	printf("Signal thread id:0x%lx\n", syscall(SYS_gettid));
	return; 
}

int main(int argc, char *argv)
{
	signal(SIGALRM, signal_handle);
	printf("Main thread id:0x%lx\n", syscall(SYS_gettid));
	alarm(1);
	sleep(1);
	return 0; 
}
```

编译执行：

```text
acuity@ubuntu:/mnt/hgfs/LSW/temp$ gcc signal.c -o signal 
acuity@ubuntu:/mnt/hgfs/LSW/temp$ ./signal
Main thread id:0x1509
Signal thread id:0x1509
```

通过测试代码可以知道，信号回调函数与主线程的ID一致，说明两者实质是同一线程。

### 2.3 结论

虽然信号回调函数与主线程是同一线程，但当主线程捕捉到信号时，主线程的执行任务会被挂起，转而去执行处理信号，并执行信号回调函数。即是产生了软中断。因此，如果主线程持有锁，此时有信号进来，CPU去处理信号回调函数；函数中由于申请不到锁资源，处于等待状态；主线程因为软中断（信号回调函数）未退出，而一直处于上锁状态。两者一直在等待资源，形成了死锁。

## 3.避免死锁

既然我们知道这种情况易产生死锁，避免死锁才是我们的根本目的。而避免死锁方式是回调函数中禁止使用锁，或者以其他方式替换处理。参考以下方式。

- 使用自旋锁（ spinlock ）代替互斥锁（mutex），程序进入了spinlock临界区，中断是会被关闭的；即是spinunlock后才会捕捉到信号，避免了死锁；
- 新建一个信号处理线程，把信号回调任务由该线程处理；信号处理线程循环调用sigwait（sigtimedwait）同步信号；
- 创建线程时，调用 pthread_sigmask 设置本线程的信号掩码，屏蔽信号捕捉。

处理复杂的任务，建议使用第二种方式。

**除此之外，一个严谨的信号处理回调函数，应该遵循以下基本原则：**

- 信号回调函数尽可能简单，确保尽快退出函数，与中断处理函数原则一样；
- 信号回调函数不能调用不可重入函数和线程不安全函数，如`malloc`、`free`、`printf`、标准I/O函数；
- 信号回调函数访问全局变量时，变量需加`volatile`修饰，避免编译器优化。

## 4.举一反三

通过上面的分析，回调函数禁止使用互斥锁。但是，一些库函数、第三方SDK、开发成员写的模块等，函数内部可能使用了互斥锁，使用时需格外注意，因为这些函数没有显式申请互斥锁，如果出现问题，将会增加查找问题的难度，无法直接通过审查代码初步发现<。比如，C库中常见的线程安全函数（内部加锁），这些函数在回调函数中使用时需格外注意：

- localtime_r，时间转换函数，localtime返回的是静态变量，不是线程安全函数，多线程访问时，值可能会被修改；后C库提供localtime_r线程安全函数，实际是内部加了锁；
- rand_r，随机数生成函数；
- strtok_r，字符串分割函数；
- asctime_r、ctime_r，时间格式化为字符函数；
- gethostbyaddr_r、gethostbyname_r，主机名称和地址转换函数。

## 5.死锁例子代码

```text
#include <stdio.h>
#include <stdint.h>
#include <unistd.h>
#include <signal.h>
#include <sys/syscall.h>
#include "pthread.h" 

pthread_mutex_t mutex;

void signal_handle(int signo) 
{
	pthread_mutex_lock(&mutex);
	printf("Signal thread id:0x%lx\n", syscall(SYS_gettid));
	pthread_mutex_unlock(&mutex);
	return; 
}

int main(int argc, char *argv)
{
	uint32_t sys = 0;
	
	signal(SIGALRM, signal_handle);
	pthread_mutex_init(&mutex, NULL);
	printf("Main thread id:0x%lx\n", syscall(SYS_gettid));
	for(;;)
	{
		pthread_mutex_lock(&mutex);
		printf("sys [%d]\n", sys++);
		if (sys == 3)
		{
			alarm(1);
			sleep(1);	/* 故意延迟，产生死锁 */
		}
		if (sys == 5)
		{
			pthread_mutex_unlock(&mutex);
			break;
		}
		pthread_mutex_unlock(&mutex);
		sleep(1);
	}
	return 0; 
}
```

原文地址：https://zhuanlan.zhihu.com/p/550607034

作者：Linux

# 【NO.189】手把手教你纯c实现异常捕获try-catch组件

## 1.try / catch / finally / throw 介绍

在java，python，c++里面都有try catch异常捕获。在try代码块里面执行的函数，如果出错有异常了，就会throw把异常抛出来，抛出来的异常被catch接收进行处理，而finally意味着无论有没有异常，都会执行finally代码块内的代码。

```text
try{
    connect_sql();//throw
}catch(){

}finally {
    
};
```

## 2.如何实现try-catch这一机制？

关于跳转，有两个跳转。那么在这里我们必然选用长跳转。

1. goto：函数内跳转，短跳转
2. setjmp/longjmp：跨函数跳转，长跳转

setjmp/longjmp这两个函数是不存在压栈出栈的，也就是说longjmp跳转到setjmp的地方后，会覆盖之前的栈。

## 3.setjmp/longjmp使用介绍(重点)

- setjmp(env)：设置跳转的位置，第一次返回0，后续返回longjmp的第二个参数
- longjmp(env, idx)：跳转到设置env的位置，第二个参数就是setjmp()的返回值

```text
#include <stdio.h>
#include <setjmp.h>


jmp_buf env;
int count = 0;

void sub_func(int idx) {
    printf("sub_func -->idx : %d\n", idx);
    //第二个参数就是setjmp()的返回值
    longjmp(env, idx);
}

int main() {
    int idx = 0;
    //设置跳转标签，第一次返回0
    count = setjmp(env);
    if (count == 0) {
        printf("count : %d\n", count);
        sub_func(++idx);
    }
    else if (count == 1) {
        printf("count : %d\n", count);
        sub_func(++idx);
    }
    else if (count == 2) {
        printf("count : %d\n", count);
        sub_func(++idx);
    }
    else {
        printf("other count \n");
    }

    return 0;
}
```



```text
count : 0
sub_func -->idx : 1
count : 1
sub_func -->idx : 2
count : 2
sub_func -->idx : 3
other count 
```

## 4.try-catch 和 setjmp/longjmp 的关系

```text
try ---> setjmp(env)

throw ---> longjmp(env,Exception)

catch(Exception)
```

![img](https://pic3.zhimg.com/80/v2-9218537f2584fa9088242f6b0f9af132_720w.webp)

我们其实可以分析出来，setjmp和count==0的地方，相当于try，后面的else if 相当于catch，最后一个else，其实并不是finally，因为finally是不管怎么样都会执行，上图我标注的其实是误导的。应该是下图这样才对。

![img](https://pic4.zhimg.com/80/v2-308dfb49a985ba5e62b9d5b313e8d587_720w.webp)

![img](https://pic1.zhimg.com/80/v2-0f3fa86e32618276f82ec6a7e4338794_720w.webp)

宏定义实现try-catch Demo

4个关键字分析出来它们的关系之后，其实我们就能用宏定义来实现了。

```text
#include <stdio.h>
#include <setjmp.h>


typedef struct _Exception {
    jmp_buf env;
    int exceptype;
} Exception;

#define Try(excep) if((excep.exceptype=setjmp(excep.env))==0)
#define Catch(excep, ExcepType) else if(excep.exceptype==ExcepType)
#define Throw(excep, ExcepType) longjmp(excep.env,ExcepType)
#define Finally

void throw_func(Exception ex, int idx) {
    printf("throw_func -->idx : %d\n", idx);
    Throw(ex, idx);
}

int main() {
    int idx = 0;

    Exception ex;
    Try(ex) {
        printf("ex.exceptype : %d\n", ex.exceptype);
        throw_func(ex, ++idx);
    }
    Catch(ex, 1) {
        printf("ex.exceptype : %d\n", ex.exceptype);
    }
    Catch(ex, 2) {
        printf("ex.exceptype : %d\n", ex.exceptype);
    }
    Catch(ex, 3) {
        printf("ex.exceptype : %d\n", ex.exceptype);
    }
    Finally{
        printf("Finally\n");
    };

    return 0;
}
```



```text
ex.exceptype : 0
throw_func -->idx : 1
ex.exceptype : 1
Finally
```

![img](https://pic4.zhimg.com/80/v2-1bc2b5615f74a8697b9b28a5d778cba3_720w.webp)



## 5.实现try-catch的三个问题

虽然现在demo版看起来像这么回事了，但是还是有两个问题：

1. 在哪个文件哪个函数哪个行抛的异常？
2. try-catch嵌套怎么做？
3. try-catch线程安全怎么做？

### 5.1 在哪个文件哪个函数哪个行抛的异常

系统提供了三个宏可以供我们使用，如果我们没有catch到异常，我们就可以打印出来

```text
__func__, __FILE__, __LINE__
```

### 5.2 try-catch嵌套怎么做？

我们知道try-catch是可以嵌套的，那么这就形成了一个栈的数据结构，现在下面有三个try，每个`setjmp`对应的都是不同的`jmp_buf`,那么我们可以定义一个jmp_buf的栈。

```text
try{
    try{
        try{

        }catch(){

        }
    }catch(){

    }
}catch(){

}finally{

};
```

那么我们很容易能写出来，既然是栈，try的时候我们就插入一个结点，catch的时候我们就pop一个出来。

```text
#define EXCEPTION_MESSAGE_LENGTH                512

typedef struct _ntyException {
    const char *name;
} ntyException;

ntyException SQLException = {"SQLException"};
ntyException TimeoutException = {"TimeoutException"};


typedef struct _ntyExceptionFrame {
    jmp_buf env;

    int line;
    const char *func;
    const char *file;

    ntyException *exception;
    struct _ntyExceptionFrame *next;

    char message[EXCEPTION_MESSAGE_LENGTH + 1];
} ntyExceptionFrame;

enum {
    ExceptionEntered = 0,//0
    ExceptionThrown,    //1
    ExceptionHandled, //2
    ExceptionFinalized//3
};
```

### 5.3 try-catch线程安全

每个线程都可以try-catch，但是我们以及知道了是个栈结构，既ExceptionStack，那么每个线程是独有一个ExceptionStack呢？还是共享同一个ExceptionStack？很明显，A线程的异常应该有A的处理，而不是由B线程处理。

```text
/* ** **** ******** **************** Thread safety **************** ******** **** ** */

#define ntyThreadLocalData                      pthread_key_t
#define ntyThreadLocalDataSet(key, value)       pthread_setspecific((key), (value))
#define ntyThreadLocalDataGet(key)              pthread_getspecific((key))
#define ntyThreadLocalDataCreate(key)           pthread_key_create(&(key), NULL)

ntyThreadLocalData ExceptionStack;

static void init_once(void) {
    ntyThreadLocalDataCreate(ExceptionStack);
}

static pthread_once_t once_control = PTHREAD_ONCE_INIT;

void ntyExceptionInit(void) {
    pthread_once(&once_control, init_once);
}
```

## 6.代码实现与解释

### 6.1 try

首先创建一个新节点入栈，然后setjmp设置一个标记，接下来就是大括号里面的操作了，如果有异常，那么就会被throw抛出来，为什么这里最后一行是if？因为longjmp的时候，返回的地方是setjmp，不要忘了！要时刻扣住longjmp和setjmp。

```text
#define Try do {                                                                        \
            volatile int Exception_flag;                                                \
            ntyExceptionFrame frame;                                                    \
            frame.message[0] = 0;                                                       \
            frame.next = (ntyExceptionFrame*)ntyThreadLocalDataGet(ExceptionStack);     \
            ntyThreadLocalDataSet(ExceptionStack, &frame);                              \
            Exception_flag = setjmp(frame.env);                                         \
            if (Exception_flag == ExceptionEntered) {
```



```text
Try{
	//...
    Throw(A, "A");
}
```

### 6.2 throw

在这里，我们不应该把throw定义成宏，而应该定义成函数。这里分两类，一类是try里面的throw，一类是没有try直接throw。

- 对于try里面的异常，我们将其状态变成ExceptionThrown，然后longjmp到setjmp的地方，由catch处理
- 对于直接抛的异常，必然没有catch去捕获，那么我们直接打印出来
- 如果第一种情况的异常，没有被catch捕获到怎么办呢？后面会被ReThrow出来，对于再次被抛出，我们就直接进行打印异常

这里的`##__VA_ARGS__`是可变参数，具体不多介绍了，不是本文重点。

```text
#define ReThrow                    ntyExceptionThrow(frame.exception, frame.func, frame.file, frame.line, NULL)
#define Throw(e, cause, ...)    ntyExceptionThrow(&(e), __func__, __FILE__, __LINE__, cause, ##__VA_ARGS__,NULL)
```



```text
void ntyExceptionThrow(ntyException *excep, const char *func, const char *file, int line, const char *cause, ...) {
    va_list ap;
    ntyExceptionFrame *frame = (ntyExceptionFrame *) ntyThreadLocalDataGet(ExceptionStack);

    if (frame) {
        //异常名
        frame->exception = excep;
        frame->func = func;
        frame->file = file;
        frame->line = line;
        //异常打印的信息
        if (cause) {
            va_start(ap, cause);
            vsnprintf(frame->message, EXCEPTION_MESSAGE_LENGTH, cause, ap);
            va_end(ap);
        }

        ntyExceptionPopStack;

        longjmp(frame->env, ExceptionThrown);
    }
        //没有被catch,直接throw
    else if (cause) {
        char message[EXCEPTION_MESSAGE_LENGTH + 1];

        va_start(ap, cause);
        vsnprintf(message, EXCEPTION_MESSAGE_LENGTH, cause, ap);
        va_end(ap);
        printf("%s: %s\n raised in %s at %s:%d\n", excep->name, message, func ? func : "?", file ? file : "?", line);
    }
    else {
        printf("%s: %p\n raised in %s at %s:%d\n", excep->name, excep, func ? func : "?", file ? file : "?", line);
    }
}
```

### 6.3 Catch

如果还是ExceptionEntered状态，说明没有异常，没有throw。如果捕获到异常了，那么其状态就是ExceptionHandled。

```text
#define Catch(nty_exception) \
                if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \
            } else if (frame.exception == &(nty_exception)) { \
                Exception_flag = ExceptionHandled;
```

### 6.4 Finally

finally也是一样，如果还是ExceptionEntered状态，说明没有异常没有捕获，那么现在状态是终止阶段。

```text
#define Finally \
                if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \
            } { \
                if (Exception_flag == ExceptionEntered)    \
                    Exception_flag = ExceptionFinalized;
```

### 6.5 EndTry

有些人看到EndTry可能会有疑问，try-catch一共不就4个关键字吗？怎么你多了一个。我们先来看看EndTry做了什么，首先如果是ExceptionEntered状态，那意味着什么？意味着没有throw，没有catch，没有finally，只有try，我们需要对这种情况进行处理，要出栈。还有一种情况，如果是ExceptionThrown状态，说明什么？没有被catch捕获到，那么我们就再次抛出，进行打印错误。至于为什么多个EndTry，写起来方便呗~

```text
#define EndTry \
                if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \
            } if (Exception_flag == ExceptionThrown) {ReThrow;} \
            } while (0)
```

### 6.6 try-catch代码

```text
/* ** **** ******** **************** try / catch / finally / throw **************** ******** **** ** */

#define EXCEPTION_MESSAGE_LENGTH                512

typedef struct _ntyException {
    const char *name;
} ntyException;

ntyException SQLException = {"SQLException"};
ntyException TimeoutException = {"TimeoutException"};


typedef struct _ntyExceptionFrame {
    jmp_buf env;

    int line;
    const char *func;
    const char *file;

    ntyException *exception;
    struct _ntyExceptionFrame *next;

    char message[EXCEPTION_MESSAGE_LENGTH + 1];
} ntyExceptionFrame;

enum {
    ExceptionEntered = 0,//0
    ExceptionThrown,    //1
    ExceptionHandled, //2
    ExceptionFinalized//3
};

#define ntyExceptionPopStack    \
    ntyThreadLocalDataSet(ExceptionStack, ((ntyExceptionFrame*)ntyThreadLocalDataGet(ExceptionStack))->next)


#define ReThrow                    ntyExceptionThrow(frame.exception, frame.func, frame.file, frame.line, NULL)
#define Throw(e, cause, ...)    ntyExceptionThrow(&(e), __func__, __FILE__, __LINE__, cause, ##__VA_ARGS__,NULL)

#define Try do {                                                                        \
            volatile int Exception_flag;                                                \
            ntyExceptionFrame frame;                                                    \
            frame.message[0] = 0;                                                       \
            frame.next = (ntyExceptionFrame*)ntyThreadLocalDataGet(ExceptionStack);     \
            ntyThreadLocalDataSet(ExceptionStack, &frame);                              \
            Exception_flag = setjmp(frame.env);                                         \
            if (Exception_flag == ExceptionEntered) {

#define Catch(nty_exception) \
                if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \
            } else if (frame.exception == &(nty_exception)) { \
                Exception_flag = ExceptionHandled;


#define Finally \
                if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \
            } { \
                if (Exception_flag == ExceptionEntered)    \
                    Exception_flag = ExceptionFinalized;
#define EndTry \
                if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \
            } if (Exception_flag == ExceptionThrown) {ReThrow;} \
            } while (0)

void ntyExceptionThrow(ntyException *excep, const char *func, const char *file, int line, const char *cause, ...) {
    va_list ap;
    ntyExceptionFrame *frame = (ntyExceptionFrame *) ntyThreadLocalDataGet(ExceptionStack);

    if (frame) {
        //异常名
        frame->exception = excep;
        frame->func = func;
        frame->file = file;
        frame->line = line;
        //异常打印的信息
        if (cause) {
            va_start(ap, cause);
            vsnprintf(frame->message, EXCEPTION_MESSAGE_LENGTH, cause, ap);
            va_end(ap);
        }

        ntyExceptionPopStack;

        longjmp(frame->env, ExceptionThrown);
    }
        //没有被catch,直接throw
    else if (cause) {
        char message[EXCEPTION_MESSAGE_LENGTH + 1];

        va_start(ap, cause);
        vsnprintf(message, EXCEPTION_MESSAGE_LENGTH, cause, ap);
        va_end(ap);
        printf("%s: %s\n raised in %s at %s:%d\n", excep->name, message, func ? func : "?", file ? file : "?", line);
    }
    else {
        printf("%s: %p\n raised in %s at %s:%d\n", excep->name, excep, func ? func : "?", file ? file : "?", line);
    }
}
```

### 6.7 Debug测试代码

```text
/* ** **** ******** **************** debug **************** ******** **** ** */

ntyException A = {"AException"};
ntyException B = {"BException"};
ntyException C = {"CException"};
ntyException D = {"DException"};

void *thread(void *args) {
    pthread_t selfid = pthread_self();
    Try
            {
                Throw(A, "A");
            }
        Catch (A)
            {
                printf("catch A : %ld\n", selfid);
            }
    EndTry;

    Try
            {
                Throw(B, "B");
            }
        Catch (B)
            {
                printf("catch B : %ld\n", selfid);
            }
    EndTry;

    Try
            {
                Throw(C, "C");
            }
        Catch (C)
            {
                printf("catch C : %ld\n", selfid);
            }
    EndTry;

    Try
            {
                Throw(D, "D");
            }
        Catch (D)
            {
                printf("catch D : %ld\n", selfid);
            }
    EndTry;

    Try
            {
                Throw(A, "A Again");
                Throw(B, "B Again");
                Throw(C, "C Again");
                Throw(D, "D Again");
            }
        Catch (A)
            {
                printf("catch A again : %ld\n", selfid);
            }
        Catch (B)
            {
                printf("catch B again : %ld\n", selfid);
            }
        Catch (C)
            {
                printf("catch C again : %ld\n", selfid);
            }
        Catch (D)
            {
                printf("catch B again : %ld\n", selfid);
            }
    EndTry;
}


#define PTHREAD_NUM        8

int main(void) {
    ntyExceptionInit();

    printf("\n\n=> Test1: Throw\n");
    {
        Throw(D, NULL);     //ntyExceptionThrow(&(D), "_function_name_", "_file_name_", 202, ((void *) 0), ((void *) 0))
        Throw(C, "null C"); //ntyExceptionThrow(&(C), "_function_name_", "_file_name_", 203, "null C", ((void *) 0))
    }
    printf("=> Test1: Ok\n\n");


    printf("\n\n=> Test2: Try-Catch Double Nesting\n");
    {
        Try
                {
                    Try
                            {
                                Throw(B, "call B");
                            }
                        Catch (B)
                            {
                                printf("catch B \n");
                            }
                    EndTry;
                    Throw(A, NULL);
                }
            Catch(A)
                {
                    printf("catch A \n");
                    printf("Result: Ok\n");
                }
        EndTry;
    }
    printf("=> Test2: Ok\n\n");


    printf("\n\n=> Test3: Try-Catch Triple  Nesting\n");
    {
        Try
                {
                    Try
                            {

                                Try
                                        {
                                            Throw(C, "call C");
                                        }
                                    Catch (C)
                                        {
                                            printf("catch C\n");
                                        }
                                EndTry;
                                Throw(B, "call B");
                            }
                        Catch (B)
                            {
                                printf("catch B\n");
                            }
                    EndTry;
                    Throw(A, NULL);
                }
            Catch(A)
                {
                    printf("catch A\n");
                }
        EndTry;
    }
    printf("=> Test3: Ok\n\n");


    printf("=> Test4: Test Thread-safeness\n");
    int i = 0;
    pthread_t th_id[PTHREAD_NUM];

    for (i = 0; i < PTHREAD_NUM; i++) {
        pthread_create(&th_id[i], NULL, thread, NULL);
    }

    for (i = 0; i < PTHREAD_NUM; i++) {
        pthread_join(th_id[i], NULL);
    }
    printf("=> Test4: Ok\n\n");

    printf("\n\n=> Test5: No Success Catch\n");
    {
        Try
                {
                    Throw(A, "no catch A ,should Rethrow");
                }
        EndTry;
    }
    printf("=> Test5: Rethrow Success\n\n");

    printf("\n\n=> Test6: Normal Test\n");
    {
        Try
                {
                    Throw(A, "call A");
                }
            Catch(A)
                {
                    printf("catch A\n");

                }
            Finally
                {
                    printf("wxf nb\n");
                };
        EndTry;
    }
    printf("=> Test6: ok\n\n");

}
```

## 7.线程安全、try-catch、Debug测试代码 汇总

```text
#include <stdio.h>
#include <setjmp.h>
#include <pthread.h>
#include <stdarg.h>


/* ** **** ******** **************** Thread safety **************** ******** **** ** */

#define ntyThreadLocalData                      pthread_key_t
#define ntyThreadLocalDataSet(key, value)       pthread_setspecific((key), (value))
#define ntyThreadLocalDataGet(key)              pthread_getspecific((key))
#define ntyThreadLocalDataCreate(key)           pthread_key_create(&(key), NULL)

ntyThreadLocalData ExceptionStack;

static void init_once(void) {
    ntyThreadLocalDataCreate(ExceptionStack);
}

static pthread_once_t once_control = PTHREAD_ONCE_INIT;

void ntyExceptionInit(void) {
    pthread_once(&once_control, init_once);
}

/* ** **** ******** **************** try / catch / finally / throw **************** ******** **** ** */

#define EXCEPTION_MESSAGE_LENGTH                512

typedef struct _ntyException {
    const char *name;
} ntyException;

ntyException SQLException = {"SQLException"};
ntyException TimeoutException = {"TimeoutException"};


typedef struct _ntyExceptionFrame {
    jmp_buf env;

    int line;
    const char *func;
    const char *file;

    ntyException *exception;
    struct _ntyExceptionFrame *next;

    char message[EXCEPTION_MESSAGE_LENGTH + 1];
} ntyExceptionFrame;

enum {
    ExceptionEntered = 0,//0
    ExceptionThrown,    //1
    ExceptionHandled, //2
    ExceptionFinalized//3
};

#define ntyExceptionPopStack    \
    ntyThreadLocalDataSet(ExceptionStack, ((ntyExceptionFrame*)ntyThreadLocalDataGet(ExceptionStack))->next)


#define ReThrow                    ntyExceptionThrow(frame.exception, frame.func, frame.file, frame.line, NULL)
#define Throw(e, cause, ...)    ntyExceptionThrow(&(e), __func__, __FILE__, __LINE__, cause, ##__VA_ARGS__,NULL)

#define Try do {                                                                        \
            volatile int Exception_flag;                                                \
            ntyExceptionFrame frame;                                                    \
            frame.message[0] = 0;                                                       \
            frame.next = (ntyExceptionFrame*)ntyThreadLocalDataGet(ExceptionStack);     \
            ntyThreadLocalDataSet(ExceptionStack, &frame);                              \
            Exception_flag = setjmp(frame.env);                                         \
            if (Exception_flag == ExceptionEntered) {

#define Catch(nty_exception) \
                if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \
            } else if (frame.exception == &(nty_exception)) { \
                Exception_flag = ExceptionHandled;


#define Finally \
                if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \
            } { \
                if (Exception_flag == ExceptionEntered)    \
                    Exception_flag = ExceptionFinalized;
#define EndTry \
                if (Exception_flag == ExceptionEntered) ntyExceptionPopStack; \
            } if (Exception_flag == ExceptionThrown) {ReThrow;} \
            } while (0)

void ntyExceptionThrow(ntyException *excep, const char *func, const char *file, int line, const char *cause, ...) {
    va_list ap;
    ntyExceptionFrame *frame = (ntyExceptionFrame *) ntyThreadLocalDataGet(ExceptionStack);

    if (frame) {
        //异常名
        frame->exception = excep;
        frame->func = func;
        frame->file = file;
        frame->line = line;
        //异常打印的信息
        if (cause) {
            va_start(ap, cause);
            vsnprintf(frame->message, EXCEPTION_MESSAGE_LENGTH, cause, ap);
            va_end(ap);
        }

        ntyExceptionPopStack;

        longjmp(frame->env, ExceptionThrown);
    }
        //没有被catch,直接throw
    else if (cause) {
        char message[EXCEPTION_MESSAGE_LENGTH + 1];

        va_start(ap, cause);
        vsnprintf(message, EXCEPTION_MESSAGE_LENGTH, cause, ap);
        va_end(ap);
        printf("%s: %s\n raised in %s at %s:%d\n", excep->name, message, func ? func : "?", file ? file : "?", line);
    }
    else {
        printf("%s: %p\n raised in %s at %s:%d\n", excep->name, excep, func ? func : "?", file ? file : "?", line);
    }
}


/* ** **** ******** **************** debug **************** ******** **** ** */

ntyException A = {"AException"};
ntyException B = {"BException"};
ntyException C = {"CException"};
ntyException D = {"DException"};

void *thread(void *args) {
    pthread_t selfid = pthread_self();
    Try
            {
                Throw(A, "A");
            }
        Catch (A)
            {
                printf("catch A : %ld\n", selfid);
            }
    EndTry;

    Try
            {
                Throw(B, "B");
            }
        Catch (B)
            {
                printf("catch B : %ld\n", selfid);
            }
    EndTry;

    Try
            {
                Throw(C, "C");
            }
        Catch (C)
            {
                printf("catch C : %ld\n", selfid);
            }
    EndTry;

    Try
            {
                Throw(D, "D");
            }
        Catch (D)
            {
                printf("catch D : %ld\n", selfid);
            }
    EndTry;

    Try
            {
                Throw(A, "A Again");
                Throw(B, "B Again");
                Throw(C, "C Again");
                Throw(D, "D Again");
            }
        Catch (A)
            {
                printf("catch A again : %ld\n", selfid);
            }
        Catch (B)
            {
                printf("catch B again : %ld\n", selfid);
            }
        Catch (C)
            {
                printf("catch C again : %ld\n", selfid);
            }
        Catch (D)
            {
                printf("catch B again : %ld\n", selfid);
            }
    EndTry;
}


#define PTHREAD_NUM        8

int main(void) {
    ntyExceptionInit();

    printf("\n\n=> Test1: Throw\n");
    {
        Throw(D, NULL);     //ntyExceptionThrow(&(D), "_function_name_", "_file_name_", 202, ((void *) 0), ((void *) 0))
        Throw(C, "null C"); //ntyExceptionThrow(&(C), "_function_name_", "_file_name_", 203, "null C", ((void *) 0))
    }
    printf("=> Test1: Ok\n\n");


    printf("\n\n=> Test2: Try-Catch Double Nesting\n");
    {
        Try
                {
                    Try
                            {
                                Throw(B, "call B");
                            }
                        Catch (B)
                            {
                                printf("catch B \n");
                            }
                    EndTry;
                    Throw(A, NULL);
                }
            Catch(A)
                {
                    printf("catch A \n");
                    printf("Result: Ok\n");
                }
        EndTry;
    }
    printf("=> Test2: Ok\n\n");


    printf("\n\n=> Test3: Try-Catch Triple  Nesting\n");
    {
        Try
                {
                    Try
                            {

                                Try
                                        {
                                            Throw(C, "call C");
                                        }
                                    Catch (C)
                                        {
                                            printf("catch C\n");
                                        }
                                EndTry;
                                Throw(B, "call B");
                            }
                        Catch (B)
                            {
                                printf("catch B\n");
                            }
                    EndTry;
                    Throw(A, NULL);
                }
            Catch(A)
                {
                    printf("catch A\n");
                }
        EndTry;
    }
    printf("=> Test3: Ok\n\n");


    printf("=> Test4: Test Thread-safeness\n");
    int i = 0;
    pthread_t th_id[PTHREAD_NUM];

    for (i = 0; i < PTHREAD_NUM; i++) {
        pthread_create(&th_id[i], NULL, thread, NULL);
    }

    for (i = 0; i < PTHREAD_NUM; i++) {
        pthread_join(th_id[i], NULL);
    }
    printf("=> Test4: Ok\n\n");

    printf("\n\n=> Test5: No Success Catch\n");
    {
        Try
                {
                    Throw(A, "no catch A ,should Rethrow");
                }
        EndTry;
    }
    printf("=> Test5: Rethrow Success\n\n");

    printf("\n\n=> Test6: Normal Test\n");
    {
        Try
                {
                    Throw(A, "call A");
                }
            Catch(A)
                {
                    printf("catch A\n");

                }
            Finally
                {
                    printf("wxf nb\n");
                };
        EndTry;
    }
    printf("=> Test6: ok\n\n");

}

void all() {
    ///try
    do {
        volatile int Exception_flag;
        ntyExceptionFrame frame;
        frame.message[0] = 0;
        frame.next = (ntyExceptionFrame *) pthread_getspecific((ExceptionStack));
        pthread_setspecific((ExceptionStack), (&frame));
        Exception_flag = _setjmp(frame.env);
        if (Exception_flag == ExceptionEntered) {
            ///
            {
                ///try
                do {
                    volatile int Exception_flag;
                    ntyExceptionFrame frame;
                    frame.message[0] = 0;
                    frame.next = (ntyExceptionFrame *) pthread_getspecific((ExceptionStack));
                    pthread_setspecific((ExceptionStack), (&frame));
                    Exception_flag = _setjmp(frame.env);
                    if (Exception_flag == ExceptionEntered) {
                        ///
                        {
                            ///Throw(B, "recall B"); --->longjmp ExceptionThrown
                            ntyExceptionThrow(&(B), "_function_name_", "_file_name_", 302, "recall B", ((void *) 0));
                            ///
                        }
                        ///Catch (B)
                        if (Exception_flag == ExceptionEntered)
                            ntyExceptionPopStack;
                    }
                    else if (frame.exception == &(B)) {
                        Exception_flag = ExceptionHandled;
                        ///
                        {    ///
                            printf("recall B \n");
                            ///
                        }
                        fin
                        if (Exception_flag == ExceptionEntered)
                            ntyExceptionPopStack;
                        if (Exception_flag == ExceptionEntered)
                            Exception_flag = ExceptionFinalized;
                        /{
                        {
                            printf("fin\n");
                        };
                        }
                        ///EndTry;
                        if (Exception_flag == ExceptionEntered)
                            ntyExceptionPopStack;
                    }
                    if (Exception_flag == ExceptionThrown)
                        ntyExceptionThrow(frame.exception, frame.func, frame.file, frame.line, ((void *) 0));
                } while (0);
                ///Throw(A, NULL); longjmp ExceptionThrown
                ntyExceptionThrow(&(A), "_function_name_", "_file_name_", 329, ((void *) 0), ((void *) 0));
                ///
            }
            ///Catch(A)
            if (Exception_flag == ExceptionEntered)
                ntyExceptionPopStack;
        }
        else if (frame.exception == &(A)) {
            Exception_flag = ExceptionHandled;
            ///
            {
                ///
                printf("\tResult: Ok\n");
                ///
            }
            /// EndTry;
            if (Exception_flag == ExceptionEntered)
                ntyExceptionPopStack;
        }
        if (Exception_flag == ExceptionThrown)
            ntyExceptionThrow(frame.exception, frame.func, frame.file, frame.line, ((void *) 0));
    } while (0);
    ///
}
```

原文地址：https://zhuanlan.zhihu.com/p/550202898

作者：linux

# 【NO.190】skynet源码结构、启动流程以及多线程工作原理

本文主要介绍skynet源码目录结构、启动流程以及其多线程工作原理。

## **1、skynet目录结构**

![img](https://pic3.zhimg.com/80/v2-c98cba27955e00194aa1180e0f3203a2_720w.webp)

- skynet-src: c语言写的skynet核心代码
- service-src: c语言编写的封装给Lua使用的服务，编译后生成的so文件在cservice中（如gate.so, harbor.so, logger.so, snlua.so）
- lualib-src: c语言编写的封装给lua使用的库，编译后生成的so文件在luaclib中（如bson.so, skynet.so, sproto.so等），提供C层级的api调用，如调用socket模块的api，调用skynet消息发送，注册回调函数的api，甚至是对C服务的调用等，并导出lua接口，供lua层使用，可以视为lua调C的媒介。
- lualib: lua编写的库
- service: lua写服务
- 3rd ：第三方库如jemalloc, lua等
- test：lua写的一些测试代码
- examples: 框架示例

只允许上层调用下层，而下层不能直接调用上层的api，这样做层次清晰

## **2、skynet启动流程**

1、启动skynet方式：终端输入./skynet exmaple/config

2、启动入口函数为skynet_main.c/main, config作为args[1]参数传入

![img](https://pic1.zhimg.com/80/v2-cdfe3c7bce21de12ff6a0f5ee66c590c_720w.webp)

3、调用skynet_start.c/skynet_start函数

![img](https://pic2.zhimg.com/80/v2-39086a348147da96f15da2854c47aa4d_720w.webp)



## 3、**skynet多线程工作原理**

skynet线程创建工作由上述skynet_start.c/start完成，主要有以下四类线程：

- monitor线程：监测所有的线程是否有卡死在某服务对某条消息的处理
- timer线程：运行定时器
- socket线程: 进行网络数据的收发
- worker线程：负责对消息队列进行调度

![img](https://pic2.zhimg.com/80/v2-4a892948c76cf96d19b977f6ab611ed1_720w.webp)

1、moniter线程

初始化该线程的key对应的私有数据块

每5s对所有工作线程进行一次检测

调用skynet_monitor_check函数检测线程是否有卡住在某条消息处理

![img](https://pic3.zhimg.com/80/v2-ed056e859d5ce9b9b2ef9c56190dd3c6_720w.webp)

2、timer定时器线程

每隔2500微秒刷新计时、唤醒等待条件触发的工作线程并检查是否有终端关闭的信号，如果有则打开log文件，将log输出至文件中，在刷新计时中会对每个时刻的链表进行相应的处理.

![img](https://pic4.zhimg.com/80/v2-45d146ebe0655b1339ffd74d5bfcb66f_720w.webp)

3、socket套接字线程

处理所有的套接字上的事件，该线程确保所有的工作线程中至少有一条工作线程是处于运行状态的，以便可以处理套接字上的事件。

![img](https://pic4.zhimg.com/80/v2-82aad05d895f4f104a53928c88204c27_720w.webp)

4、worker工作线程

从全局队列中取出服务队列对其消息进行处理，其运行函数thread_worker的工作原理：首先初始化该线程的key对应的私有数据块，然后从全局队列中取出服务队列对其消息进行处理，最后当全局队列中没有服务队列信息时进入等待状态，等待定时器线程或套接字线程触发条件；

![img](https://pic2.zhimg.com/80/v2-55008c229c2c83cf16b32d5c4d55c2dd_720w.webp)

## **4、skynet消息处理如何保证线程安全？**

最后探讨一个问题，消息处理为什么线程安全？解释如下：

- 每个worker线程，从global_mq取出的次级消息队列都是唯一的，有且只有一个服务与之对应，取出之后，在该worker线程完成所有callback调用之前，不会push回global_mq中，也就是说，在这段时间内，其他worker线程不能拿到这个次级消息队列所对应的服务，并调用callback函数，也就是说一个服务不可能同时在多条worker线程内执行callback函数，从而保证了线程安全。
- 不论是global_mq还是次级消息队列，在入队和出队操作时，都需加上spinlock，这样多个线程同时访问mq的时候，第一个访问者会进入临界区并锁住，其他线程会阻塞等待，直至该锁解除，这样也保证了线程安全。
- 我们在通过handle从skynet_context list里获取skynet_context的过程中（比如派发消息时，要要先获取skynet_context指针，再调用该服务的callback函数），需要加上一个读写锁，因为在skynet运作的过程中，获取skynet_context，比创建skynet_context的情况要多得多，因此这里用了读写锁

以上介绍了skynet源码中的目录结构以及各部分功能，接着介绍了skynet的启动流程，最后介绍了skynet的多个线程是如何进行协同工作的。

原文地址：https://zhuanlan.zhihu.com/p/549861580

作者：linux

# 【NO.191】P2P通信原理与实现(C++)

## **1.简介**

当今互联网到处存在着一些中间件(MIddleBoxes)，如NAT和防火墙，导致两个(不在同一内网)中的客户端无法直接通信。这些问题即便是到了IPV6时代也会存在，因为即使不需要NAT，但还有其他中间件如防火墙阻挡了链接的建立。

当今部署的中间件大多都是在C/S架构上设计的，其中相对隐匿的客户机主动向周知的服务端(拥有静态IP地址和DNS名称)发起链接请求。大多数中间件实现了一种非对称的通讯模型，即内网中的主机可以初始化对外的链接，而外网的主机却不能初始化对内网的链接，除非经过中间件管理员特殊配置。在中间件为常见的NAPT的情况下（也是本文主要讨论的），内网中的客户端没有单独的公网IP地址，而是通过NAPT转换，和其他同一内网用户共享一个公网IP。这种内网主机隐藏在中间件后的不可访问性对于一些客户端

软件如浏览器来说并不是一个问题，因为其只需要初始化对外的链接，从某方面来看反而还对隐私保护有好处。

然而在P2P应用中，内网主机（客户端）需要对另外的终端（Peer）直接建立链接，但是发起者和响应者可能在不同的中间件后面，两者都没有公网IP地址。而外部对NAT公网IP和端口主动的链接或数据都会因内网未请求被丢弃掉。本文讨论的就是如何跨越NAT实现内网主机直接通讯的问题。

## **2.术语**

**防火墙（Firewall）：**

防火墙主要限制内网和公网的通讯，通常丢弃未经许可的数据包。防火墙会检测(但是不修改)试图进入内网数据包的IP地址和TCP/UDP端口信息。

**网络地址转换器（NAT）：**

NAT不止检查进入数据包的头部，而且对其进行修改，从而实现同一内网中不同主机共用更少的公网IP（通常是一个）。

**基本NAT（Basic NAT）：**

基本NAT会将内网主机的IP地址映射为一个公网IP，不改变其TCP/UDP端口号。基本NAT通常只有在当NAT有公网IP池的时候才有用。

**网络地址-端口转换器（NAPT）：**

到目前为止最常见的即为NAPT，其检测并修改出入数据包的IP地址和端口号，从而允许多个内网主机同时共享一个公网IP地址。

**锥形NAT（Cone NAT）：**

在建立了一对（公网IP，公网端口）和（内网IP，内网端口）二元组的绑定之后，Cone NAT会重用这组绑定用于接下来该应用程序的所有会话（同一内网IP和端口），只要还有一个会话还是激活的。

例如，假设客户端A建立了两个连续的对外会话，从相同的内部端点（10.0.0.1:1234）到两个不同的外部服务端S1和S2。Cone NAT只为两个会话映射了一个公网端点（155.99.25.11:62000），确保客户端端口的“身份”在地址转换的时候保持不变。由于基本NAT和防火墙都不改变数据包的端口号，因此这些类型的中间件也可以看作是退化的Cone NAT。

![img](https://pic3.zhimg.com/80/v2-8eff7d9432dbc48f1ea1e341d00aee26_720w.webp)

**对称NAT（Symmetric NAT）**

对称NAT正好相反，不在所有公网-内网对的会话中维持一个固定的端口绑定。其为每个新的会话开辟一个新的端口。如下图所示：

![img](https://pic2.zhimg.com/80/v2-8de3548a9628608ab441c6f0176b8c99_720w.webp)

其中Cone NAT根据NAT如何接收已经建立的（公网IP，公网端口）对的输入数据还可以细分为以下三类：

**1) 全锥形NAT（Full Cone NAT）**

在一个新会话建立了公网/内网端口绑定之后，全锥形NAT接下来会接受对应公网端口的所有数据，无论是来自哪个（公网）终端。全锥NAT有时候也被称为“混杂”NAT（promiscuous NAT）。

**2) 受限锥形NAT（Restricted Cone NAT）**

受限锥形NAT只会转发符合某个条件的输入数据包。条件为：外部（源）IP地址匹配内网主机之前发送一个或多个数据包的结点的IP地址。受限NAT通过限制输入数据包为一组“已知的”外部IP地址，有效地精简了防火墙的规则。

**3) 端口受限锥形NAT（Port-Restricted Cone NAT）**

端口受限锥形NAT也类似，只当外部数据包的IP地址和端口号都匹配内网主机发送过的地址和端口号时才进行转发。端口受限锥形NAT为内部结点提供了和对称NAT相同等级的保护，以隔离未关联的数据。

## **3. P2P通信**

根据客户端的不同，客户端之间进行P2P传输的方法也略有不同，这里介绍了现有的穿越中间件进行P2P通信的几种技术。

### **3.1 中继（Relaying）**

这是最可靠但也是最低效的一种P2P通信实现。其原理是通过一个有公网IP的服务器中间人对两个内网客户端的通信数据进行中继和转发。如下图所示：

![img](https://pic3.zhimg.com/80/v2-ff1239bf14a5f09bfa6770e9e2d8716e_720w.webp)

客户端A和客户端B不直接通信，而是先都与服务端S建立链接，然后再通过S和对方建立的通路来中继传递的数据。这钟方法的缺陷很明显，当链接的客户端变多之后，会显著增加服务器的负担，完全没体现出P2P的优势。

### **3.2 逆向链接（Connection reversal）**

　　第二种方法在当两个端点中有一个不存在中间件的时候有效。例如，客户端A在NAT之后而客户端B拥有全局IP地址，如下图：

![img](https://pic4.zhimg.com/80/v2-c6d028056a767c384ba6f6c1330fb593_720w.webp)

客户端A内网地址为10.0.0.1，且应用程序正在使用TCP端口1234。A和服务器S建立了一个链接，服务器的IP地址为18.181.0.31，监听1235端口。NAT A给客户端A分配了TCP端口62000，地址为NAT的公网IP地址155.99.25.11，作为客户端A对外当前会话的临时IP和端口。因此S认为客户端A就是155.99.25.11:62000。而B由于有公网地址，所以对S来说B就是138.76.29.7:1234。

当客户端B想要发起一个对客户端A的P2P链接时，要么链接A的外网地址155.99.25.11:62000，要么链接A的内网地址10.0.0.1:1234，然而两种方式链接都会失败。链接10.0.0.1:1234失败自不用说，为什么链接155.99.25.11:62000也会失败呢？来自B的TCP SYN握手请求到达NAT A的时候会被拒绝，因为对NAT A来说只有外出的链接才是允许的。

在直接链接A失败之后，B可以通过S向A中继一个链接请求，从而从A方向“逆向“地建立起A-B之间的点对点链接。

很多当前的P2P系统都实现了这种技术，但其局限性也是很明显的，只有当其中一方有公网IP时链接才能建立。越来越多的情况下，通信的双方都在NAT之后，因此就要用到我们下面介绍的第三种技术了。

### **3.3 UDP打洞（UDP hole punching）**

第三种P2P通信技术，被广泛采用的，名为“P2P打洞“。P2P打洞技术依赖于通常防火墙和cone NAT允许正当的P2P应用程序在中间件中打洞且与对方建立直接链接的特性。以下主要考虑两种常见的场景，以及应用程序如何设计去完美地处理这些情况。第一种场景代表了大多数情况，即两个需要直接链接的客户端处在两个不同的NAT之后；第二种场景是两个客户端在同一个NAT之后，但客户端自己并不需要知道。

#### **3.3.1. 端点在不同的NAT之下**

假设客户端A和客户端B的地址都是内网地址，且在不同的NAT后面。A、B上运行的P2P应用程序和服务器S都使用了UDP端口1234，A和B分别初始化了与Server的UDP通信，地址映射如图所示:

![img](https://pic2.zhimg.com/80/v2-a8a51dcd4862899489aa44b225ed4821_720w.webp)

现在假设客户端A打算与客户端B直接建立一个UDP通信会话。如果A直接给B的公网地址138.76.29.7:31000发送UDP数据，NAT B将很可能会无视进入的数据（除非是Full Cone NAT），因为源地址和端口与S不匹配，而最初只与S建立过会话。B往A直接发信息也类似。

假设A开始给B的公网地址发送UDP数据的同时，给服务器S发送一个中继请求，要求B开始给A的公网地址发送UDP信息。A往B的输出信息会导致NAT A打开一个A的内网地址与与B的外网地址之间的新通讯会话，B往A亦然。一旦新的UDP会话在两个方向都打开之后，客户端A和客户端B就能直接通讯，而无须再通过引导服务器S了。

UDP打洞技术有许多有用的性质。一旦一个的P2P链接建立，链接的双方都能反过来作为“引导服务器”来帮助其他中间件后的客户端进行打洞，极大减少了服务器的负载。应用程序不需要知道中间件具体是什么（如果有的话），因为以上的过程在没有中间件或者有多个中间件的情况下也一样能建立通信链路。

#### **3.3.2. 端点在相同的NAT之下**

现在考虑这样一种情景，两个客户端A和B正好在同一个NAT之后（而且可能他们自己并不知道），因此在同一个内网网段之内。客户端A和服务器S建立了一个UDP会话，NAT为此分配了公网端口62000，B同样和S建立会话，分配到了端口62001，如下图：

![img](https://pic3.zhimg.com/80/v2-4cbcbe57e4da55070332b30f7bbb9136_720w.webp)

假设A和B使用了上节介绍的UDP打洞技术来建立P2P通路，那么会发生什么呢？首先A和B会得到由S观测到的对方的公网IP和端口号，然后给对方的地址发送信息。两个客户端只有在NAT允许内网主机对内网其他主机发起UDP会话的时候才能正常通信，我们把这种情况称之为"回环传输“(lookback translation)，因为从内部到达NAT的数据会被“回送”到内网中而不是转发到外网。例如，当A发送一个UDP数据包给B的公网地址时，数据包最初有源IP地址和端口地址10.0.0.1:1234和目的地址155.99.25.11:62001，NAT收到包后，将其转换为源155.99.25.11:62000（A的公网地址）和目的10.1.1.3:1234，然后再转发给B。即便NAT支持回环传输，这种转换和转发在此情况下也是没必要的，且有可能会增加A与B的对话延时和加重NAT的负担。

对于这个问题，解决方案是很直观的。当A和B最初通过S交换地址信息时，他们应该包含自身的IP地址和端口号（从自己看），同时也包含从服务器看的自己的地址和端口号。然后客户端同时开始从对方已知的两个的地址中同时开始互相发送数据，并使用第一个成功通信的地址作为对方地址。如果两个客户端在同一个NAT后，发送到对方内网地址的数据最有可能先到达，从而可以建立一条不经过NAT的通信链路；如果两个客户端在不同的NAT之后，发送给对方内网地址的数据包根本就到达不了对方，但仍然可以通过公网地址来建立通路。值得一提的是，虽然这些数据包通过某种方式验证，但是在不同NAT的情况下完全有可能会导致A往B发送的信息发送到其他A内网网段中无关的结点上去的。

#### **3.3.3. 固定端口绑定**

UDP打洞技术有一个主要的条件：**只有当**两个NAT都是Cone NAT（或者非NAT的防火墙）时才能工作。因为其维持了一个给定的（内网IP，内网UDP）二元组和（公网IP， 公网UDP）二元组固定的端口绑定，只要该UDP端口还在使用中，就不会变化。如果像对称NAT一样，给每个新会话分配一个新的公网端口，就会导致UDP应用程序无法使用跟外部端点已经打通了的通信链路。由于Cone NAT是当今最广泛使用的，尽管有一小部分的对称NAT是不支持打洞的，UDP打洞技术也还是被广泛采纳应用。

## **4. 具体实现**

　　如果理解了上面所说的内容，那么代码实现起来倒很简单了 。这里采用C++的异步IO库来实现引导服务器和P2P客户端的简单功能，目的是打通两个客户端的通信链路，使两个不同局域网之间的客户端可以实现直接通信。

### **4.1 引导服务端设计**

引导服务器运行在一个有公网地址的设备上，并且接收指定端口的来自客户的命令（这里是用端口号2333）。

客户端其实可以而且也最好应该与服务器建立TCP链接，但我这里为了图方便，也只采用了UDP的通信方式。服务端监听2333端口的命令，然后执行相应的操作，目前包含的命令有:

login， 客户端登录，使得其记录在服务器traker中，让其他peer可以对其发出链接请求。

logout，客户端登出，使其对peer隐藏。因为服务器不会追踪客户端的登录状态。

list，客户端查看目前的登录用户。

punch <client>， 对指定用户（序号）进行打洞。

help， 查看有哪些可用的命令。

### **4.2 P2P客户端设计**

一般的网络编程，都是客户端比服务端要难，因为要处理与服务器的通信同时还要处理来自用户的事件；对于P2P客户端来说更是如此，因为P2P客户端不止作为客户端，同时也作为对等连接的服务器端。

这里的大体思路是，输入命令传输给服务器之后，接收来自服务器的反馈，并执行相应代码。例如A想要与B建立通信链路，先给服务器发送punch命令以及给B发送数据，服务器接到命令后给B发送punch_requst信息以及A的端点信息，B收到之后向A发送数据打通通路，然后A与B就可以进行P2P通信了。经测试，打通通路后即便把服务器关闭，A与B也能正常通信。

一个UDP打洞的例子见 [https://github.com/pannzh/P2P-Over-MiddleBoxes-Demo](https://link.zhihu.com/?target=https%3A//github.com/pannzh/P2P-Over-MiddleBoxes-Demo)

关于TCP打洞，有一点需要提的是，因为TCP是基于连接的，所以任何未经连接而发送的数据都会被丢弃，这导致在recv的时候是无法直接从peer端读取数据。

其实这对UDP也一样，如果对UDP的socket进行了connect，其也会忽略连接之外的数据，详见`connect(2)`。

所以，如果我们要进行TCP打洞，通常需要重用本地的endpoint来发起新的TCP连接，这样才能将已经打开的NAT利用起来。具体来说，则是要设置socket的`SO_REUSEADDR`或`SO_REUSEPORT`属性，根据系统不同，其实现也不尽一致。一般来说，TCP打洞的步骤如下：

\- A 发送 SYN 到 B （出口地址，下同），从而创建NAT A的一组映射
\- B 发送 SYN 到 A， 创建NAT B的一组映射
\- 根据时序不同，两个SYN中有一个会被对方的NAT丢弃，另一个成功通过NAT
\- 通过NAT的SYN报文被其中一方收到，即返回SYNACK， 完成握手
\- 至此，TCP的打洞成功，获得一个不依赖于服务器的链接

原文地址：https://zhuanlan.zhihu.com/p/546797713

作者：linux

# 【NO.192】linux：线程池的作用、应用场景、工作原理与纯C实现

## 1.线程池的作用

**为什么会有线程池，到底解决了什么问题**

1. 减少线程的创建与销毁（线程的角度）
2. 异步解耦的作用（设计的角度）

## 2.线程池的异步处理使用场景

以日志为例，在写日志loginfo(“xxx”)，与日志落盘，是两码事，它们两之间应该是异步的。那么异步解耦就是将日志当作一个任务task，将这个任务抛给线程池去处理，由线程池去负责日志落盘。对于应用程序而言，就可以提升落盘的效率。

以nginx为例，一秒几万的请求，速度很快。如果在其中加一个日志，那么qps一下子就掉下来了，因为每请求一次就需要落盘一次，那么整个服务器的性能就下降。我们可以引入一个线程池，把日志这个任务抛给线程池，对于主循环来说，就只抛任务即可，这样就可以大大提升主线程的效率。这就是线程池异步解耦的作用

不仅仅是日志落盘，还有很多地方都可以用线程池，比较耗时的操作如数据库操作，io处理等，都可以用线程池。

线程池有必要将线程与cpu做亲和性吗？ 在注重cpu处理能力的时候，可以做黏合；如果注重的是异步解耦，那么这里更加注重的是任务，没必要将线程和cpu做绑定。

## 3.线程池工作原理

### 3.1 线程池应该提供哪些api

![img](https://pic2.zhimg.com/80/v2-749b503aacab1bc0e5b83e78e867cc51_720w.webp)

我们在使用线程池的时候，是当作一个组件去使用。所以在使用组件的时候，我们首先想到的是线程池应该提供哪些api。

1. 线程池的初始化(创建) init/create
2. 往池里面抛任务push_task
3. 线程池的销毁 deinit/destroy

这三个api是最核心的api，其他可扩展的api都是可有可无的，而这三个api是一定要有的。

### 3.2 线程池的三个组件

![img](https://pic1.zhimg.com/80/v2-ba6fec48ec6d183eeac5299809e27f30_720w.webp)

想象去银行营业厅的场景。柜员：为客户提供服务；客户：是来办业务的，对于柜员来说，这些人就是任务。那么这两个形象就构建出了pthread和task。

那么这个公示牌(xxx号来几号柜台办理业务)，是谁的属性呢？告示牌的作用是管理客户和柜员有秩序工作，它不隶属于柜员，也不隶属于客户，它是一个管理工具。

1. 柜员 ---->pthread
2. 客户 ---->task
3. 告示牌–>管理柜员和客户有秩序的工作（不会出现一个任务同时被多个线程处理的情况）

这么这就自然而然的形成了3个组件，那么它们都应该有什么属性呢：

- 对于柜员来说：工号id，停止工作标识符flag
- 对于客户来说：如果办理取款需要带银行卡，如果办贷款需要带凭证等等，所以需要一个任务func()，以及对应任务的参数arg
- 对于告示牌来说：如果没有客户，那么柜员就需要在工作中等待客户的到来，所以第一个需要条件等待cond，既然要管理有秩序的工作，肯定需要mutex来保证临界资源

下面将柜员称为执行队列，客户称为任务队列，告示牌称为池管理组件。

错误理解：要使用线程就从线程池里面拿一个线程出来使用，用完再返回给线程池。这种理解是连接池的概念。而线程池是多个线程去任务队列取任务，竞争任务。

所以线程的核心就是下面的伪代码：

```text
while(1){
    get_task();
    task->func();
}
```



## 4.代码实现

### 4.1 线程池的任务队列、执行队列、池管理组件 的定义 与 添加删除

![img](https://pic1.zhimg.com/80/v2-419d04da3719cb57884861d825d03244_720w.webp)

```text
//执行队列
typedef struct NWORKER {
    pthread_t id;
    int termination;
    struct NTHREADPOLL *thread_poll;

    struct NWORKER *prev;
    struct NWORKER *next;
} worker_t;

//任务队列
typedef struct NTASK {
    void (*task_func)(void *arg);
    void *user_data;

    struct NTASK *prev;
    struct NTASK *next;
} task_t;

//池管理组件
typedef struct NTHREADPOLL {
    worker_t *workers;
    task_t *tasks;

    pthread_cond_t cond;
    pthread_mutex_t mutex;
} thread_poll_t;
```



```text
//头插法
#define LL_ADD(item, list)do{   \
    item->prev=NULL;            \
    item->next=list;            \
    if(list!=NULL){             \
        list->prev=item;        \
    }                           \
    list=item                   \
}while(0)


#define LL_REMOVE(item, list)do{        \
    if(item->prev!=NULL){               \
        item->prev->next=item->next;    \
    }                                   \
    if(item->next!=NULL){               \
        item->next->prev=item->prev;    \
    }                                   \
    if(list==item){                     \
        list=item->next;                \
    }                                   \
    item->prev=item->next=NULL;         \
}while(0)
```

### 4.2 三个api

创建其实就是创建thread_poll_t结构体，然后按照给定的宏创建线程和worker。

push就是给task队列增加一个任务，然后用signal通知cond。

销毁将所有线程的termination置1，然后广播cond即可。

```text
//return access create thread num;
int thread_poll_create(thread_poll_t *thread_poll, int thread_num) {
    if (thread_num < 1)thread_num = 1;
    memset(thread_poll, 0, sizeof(thread_poll_t));
    //init cond
    pthread_cond_t blank_cond = PTHREAD_COND_INITIALIZER;
    memcpy(&thread_poll->cond, &blank_cond, sizeof(pthread_cond_t));
    //init mutex
    pthread_mutex_t blank_mutex = PTHREAD_MUTEX_INITIALIZER;
    memcpy(&thread_poll->mutex, &blank_mutex, sizeof(pthread_mutex_t));

    // one thread one worker
    int idx = 0;
    for (idx = 0; idx < thread_num; idx++) {
        worker_t *worker = malloc(sizeof(worker_t));
        if (worker == NULL) {
            perror("worker malloc err\n");
            return idx;
        }
        memset(worker, 0, sizeof(worker_t));
        worker->thread_poll = thread_poll;

        int ret = pthread_create(&worker->id, NULL, thread_callback, worker);
        if (ret) {
            perror("pthread_create err\n");
            free(worker);
            return idx;
        }
        LL_ADD(worker, thread_poll->workers);
    }
    return idx;
}

int thread_poll_push_task(thread_poll_t *thread_poll, task_t *task) {
    pthread_mutex_lock(&thread_poll->mutex);
    LL_ADD(task, thread_poll->tasks);
    pthread_cond_signal(&thread_poll->cond);
    pthread_mutex_unlock(&thread_poll->mutex);
}

int thread_destroy(thread_poll_t *thread_poll) {
    worker_t *worker = NULL;
    for (worker = thread_poll->workers; worker != NULL; worker = worker->next) {
        worker->termination = 1;
    }
    pthread_mutex_lock(&thread_poll->mutex);
    pthread_cond_broadcast(&thread_poll->cond);
    pthread_mutex_unlock(&thread_poll->mutex);
}
```

### 4.3 线程的回调函数

线程要做的就是取任务，执行任务。取任务从任务队列里面取。

```text
task_t *get_task(worker_t *worker) {
    while (1) {
        pthread_mutex_lock(&worker->thread_poll->mutex);
        while (worker->thread_poll->workers == NULL) {
            if (worker->termination)break;
            pthread_cond_wait(&worker->thread_poll->cond, &worker->thread_poll->mutex);
        }
        if (worker->termination) {
            pthread_mutex_unlock(&worker->thread_poll->mutex);
            return NULL;
        }
        task_t *task = worker->thread_poll->tasks;
        if (task) {
            LL_REMOVE(task, worker->thread_poll->tasks);
        }
        pthread_mutex_unlock(&worker->thread_poll->mutex);
        if (task != NULL) {
            return task;
        }
    }
};

void *thread_callback(void *arg) {
    worker_t *worker = (worker_t *) arg;
    while (1) {
        task_t *task = get_task(worker);
        if (task == NULL) {
            free(worker);
            pthread_exit("thread termination\n");
        }
        task->task_func(task);
    }
}
```

### 4.4 测试代码

这里我们创建了1000个task，开了10个thread。记住task以及task的参数，由task的func来销毁。

```text
//
// Created by 68725 on 2022/7/25.
//

#include <pthread.h>
#include <memory.h>
#include <malloc.h>
#include <stdio.h>
#include <unistd.h>


//头插法
#define LL_ADD(item, list)do{   \
    item->prev=NULL;            \
    item->next=list;            \
    if(list!=NULL){             \
        list->prev=item;        \
    }                           \
    list=item;                  \
}while(0)

#define LL_REMOVE(item, list)do{        \
    if(item->prev!=NULL){               \
        item->prev->next=item->next;    \
    }                                   \
    if(item->next!=NULL){               \
        item->next->prev=item->prev;    \
    }                                   \
    if(list==item){                     \
        list=item->next;                \
    }                                   \
    item->prev=item->next=NULL;         \
}while(0)

//执行队列
typedef struct NWORKER {
    pthread_t id;
    int termination;
    struct NTHREADPOLL *thread_poll;

    struct NWORKER *prev;
    struct NWORKER *next;
} worker_t;

//任务队列
typedef struct NTASK {
    void (*task_func)(void *arg);

    void *user_data;

    struct NTASK *prev;
    struct NTASK *next;
} task_t;

//池管理组件
typedef struct NTHREADPOLL {
    worker_t *workers;
    task_t *tasks;

    pthread_cond_t cond;
    pthread_mutex_t mutex;
} thread_poll_t;

task_t *get_task(worker_t *worker) {
    while (1) {
        pthread_mutex_lock(&worker->thread_poll->mutex);
        while (worker->thread_poll->workers == NULL) {
            if (worker->termination)break;
            pthread_cond_wait(&worker->thread_poll->cond, &worker->thread_poll->mutex);
        }
        if (worker->termination) {
            pthread_mutex_unlock(&worker->thread_poll->mutex);
            return NULL;
        }
        task_t *task = worker->thread_poll->tasks;
        if (task) {
            LL_REMOVE(task, worker->thread_poll->tasks);
        }
        pthread_mutex_unlock(&worker->thread_poll->mutex);
        if (task != NULL) {
            return task;
        }
    }
};

void *thread_callback(void *arg) {
    worker_t *worker = (worker_t *) arg;
    while (1) {
        task_t *task = get_task(worker);
        if (task == NULL) {
            free(worker);
            pthread_exit("thread termination\n");
        }
        task->task_func(task);
    }
}

//return access create thread num;
int thread_poll_create(thread_poll_t *thread_poll, int thread_num) {
    if (thread_num < 1)thread_num = 1;
    memset(thread_poll, 0, sizeof(thread_poll_t));
    //init cond
    pthread_cond_t blank_cond = PTHREAD_COND_INITIALIZER;
    memcpy(&thread_poll->cond, &blank_cond, sizeof(pthread_cond_t));
    //init mutex
    pthread_mutex_t blank_mutex = PTHREAD_MUTEX_INITIALIZER;
    memcpy(&thread_poll->mutex, &blank_mutex, sizeof(pthread_mutex_t));

    // one thread one worker
    int idx = 0;
    for (idx = 0; idx < thread_num; idx++) {
        worker_t *worker = malloc(sizeof(worker_t));
        if (worker == NULL) {
            perror("worker malloc err\n");
            return idx;
        }
        memset(worker, 0, sizeof(worker_t));
        worker->thread_poll = thread_poll;

        int ret = pthread_create(&worker->id, NULL, thread_callback, worker);
        if (ret) {
            perror("pthread_create err\n");
            free(worker);
            return idx;
        }
        LL_ADD(worker, thread_poll->workers);
    }
    return idx;
}

int thread_poll_push_task(thread_poll_t *thread_poll, task_t *task) {
    pthread_mutex_lock(&thread_poll->mutex);
    LL_ADD(task, thread_poll->tasks);
    pthread_cond_signal(&thread_poll->cond);
    pthread_mutex_unlock(&thread_poll->mutex);
}

int thread_destroy(thread_poll_t *thread_poll) {
    worker_t *worker = NULL;
    for (worker = thread_poll->workers; worker != NULL; worker = worker->next) {
        worker->termination = 1;
    }
    pthread_mutex_lock(&thread_poll->mutex);
    pthread_cond_broadcast(&thread_poll->cond);
    pthread_mutex_unlock(&thread_poll->mutex);
}

void counter(task_t *task) {
    int idx = *(int *) task->user_data;
    printf("idx:%d  pthread_id:%llu\n", idx, pthread_self());
    free(task->user_data);
    free(task);
}

#define THREAD_COUNT 10
#define TASK_COUNT 1000

int main() {
    thread_poll_t thread_poll = {0};
    int ret = thread_poll_create(&thread_poll, THREAD_COUNT);
    if (ret != THREAD_COUNT) {
        thread_destroy(&thread_poll);
    }
    int i = 0;
    for (i = 0; i < TASK_COUNT; i++) {
        //create task
        task_t *task = (task_t *) malloc(sizeof(task_t));
        if (task == NULL) {
            perror("task malloc err\n");
            exit(1);
        }
        task->task_func = counter;
        task->user_data = malloc(sizeof(int));
        *(int *) task->user_data = i;
        //push task
        thread_poll_push_task(&thread_poll, task);
    }
    getchar();
    thread_destroy(&thread_poll);
}
```

## 5.nginx线程池实现对比分祈

### 5.1 线程池初始化对比

cond初始化，mutex初始化，创建线程

![img](https://pic3.zhimg.com/80/v2-cd5ae45c487a4d7d8981aeefb5d3d526_720w.webp)

### 5.2 线程回调函数对比

取任务，执行任务

![img](https://pic3.zhimg.com/80/v2-e140f2483490bcfcade3996c1c5a8f0e_720w.webp)

### 5.3 push任务对比

nginx是将任务插到尾部，我们做的是插到头部

![img](https://pic1.zhimg.com/80/v2-3a018f7208432c888283cf2e5f527184_720w.webp)

### 5.4 线程数量的抉择

线程到底初始化多少呢？如果是计算密集型就不用太多的线程，如果是任务密集型可以多几个。以下是经验值，不一定一定按照这个来。

- 计算密集型：强计算，计算时间较长，线程数量与cpu核心数成比例即可，如1：1。
- 任务密集型：处理任务，io操作。可以开多一点，如cpu核心数的2倍。

### 5.5 线程池的动态扩缩

随着任务越来越多，线程不够用怎么办？我们可以开一个监控线程，设n=running线程 / 总线程。当n>上水位时，监控线程创建几个线程；当n<下水位时，监控线程销毁几个线程。可以设置30%和70%。

原文地址：https://zhuanlan.zhihu.com/p/546291862

作者：linux

# 【NO.193】[ C++ ] 一篇带你了解C++中动态内存管理

在我们日常写代码的过程中，我们对内存空间的需求有时候在程序运行的时候才能知道，这时候我们就需要使用动态开辟内存的方法。

## 1、C/C++程序的内存开辟

首先我们先了解一下C/C++程序内存分配的几个区域：

```text
int globalVar = 1;
static int staticGlobalVar = 1;
void Test()
{
	static int staticVar = 1;
	int localVar = 1;
	int num1[10] = { 1, 2, 3, 4 };
	char char2[] = "abcd";
	const char* pChar3 = "abcd";
	int* ptr1 = (int*)malloc(sizeof(int) * 4);
	int* ptr2 = (int*)calloc(4, sizeof(int));
	int* ptr3 = (int*)realloc(ptr2, sizeof(int) * 4);
	free(ptr1);
	free(ptr3);
}
```

![img](https://pic2.zhimg.com/80/v2-7a074855730329beb22b517478d294dd_720w.webp)

\1. 栈区（stack）： 在执行函数时，函数内局部变量的存储单元都可以在栈上创建，函数执行结束时这些存储单元自动被释放。栈内存分配运算内置于处理器的指令集中，效率很高，但是分配的内存容量有限。 栈区主要存放运行函数而分配的局部变量、函数参数、返回数据、返回地址等。

\2. 堆区（heap）： 一般由程序员分配释放， 若程序员不释放，程序结束时可能由 OS 回收 。分配方式类似于链表。

\3. 数据段（静态区） （ static ）存放全局变量、静态数据。程序结束后由系统释放。

\4. 代码段： 存放函数体（类成员函数和全局函数）的二进制代码。

这幅图中，我们可以发现普通的局部变量是在栈上分配空间的，在栈区中创建的变量出了作用域去就会自动销毁。但是被static修饰的变量是存放在数据段(静态区)，在数据段上创建的变量直到程序结束才销毁，所以数据段上的数据生命周期变长了。

## 2.C语言中动态内存管理方式：malloc/calloc/realloc/free

在C语言中，我们经常会用到malloc,calloc和realloc来进行动态的开辟内存；同时，C语言还提供了一个函数free，专门用来做动态内存的释放和回收。其中他们三个的区别也是我们需要特别所强调区别的。

### 2.1malloc、calloc、realloc区别？

malloc函数是向内存申请一块连续可用的空间，并返回指向这块空间的指针。

calloc与malloc的区别只在于calloc会在返回地址之前把申请的空间的每个字节初始化为0。

realloc函数可以做到对动态开辟内存大小的调整。

我们通过这三个函数的定义也可以进行功能的区分：

```text
void Test ()
{
int* p1 = (int*) malloc(sizeof(int));
free(p1);
 
int* p2 = (int*)calloc(4, sizeof (int));
int* p3 = (int*)realloc(p2, sizeof(int)*10);
 
free(p3 );
}
```



## 3.C++内存管理方式

我们都知道，C++语言是兼容C语言的，因此C语言中内存管理方式在C++中可以继续使用。但是有些地方就无能为力了，并且使用起来也可能比较麻烦。因此，C++拥有自己的内管管理方式：通过new和delete操作符进行动态内存管理。

### 3.1 new/delete操作内置类型

```text
int main()
{
	// 动态申请一个int类型的空间
	int* ptr1 = new int;
 
	// 动态申请一个int类型的空间并初始化为10
	int* ptr2 = new int(10);
 
	// 动态申请3个int类型的空间(数组)
	int* ptr3 = new int[3];
 
	// 动态申请3个int类型的空间,初始化第一个空间值为1
	int* ptr4 = new int[3]{ 1 };
 
	delete ptr1;
	delete ptr2;
	delete[] ptr3;
	delete[] ptr4;
 
	return 0;
}
```

我们首先通过画图分析进行剖析代码：

![img](https://pic1.zhimg.com/80/v2-689fdaf75639c236668fa9a9ab2d0cac_720w.webp)

我们在监视窗口看看这3个变量

![img](https://pic3.zhimg.com/80/v2-da3a633d1718cc1f356e1116ab5fa162_720w.webp)

![img](https://pic4.zhimg.com/80/v2-27d18dde5aa447e0ba15160cdb674efb_720w.webp)

注意：申请和释放单个元素的空间，使用new和delete操作符，申请和释放连续的空间，使用new[]和delete[]，要匹配起来使用。

### 3.2 new和delete操作自定义类型

```text
class A {
public:
	A(int a = 0)
		: _a(a)
	{
		cout << "A():" << this << endl;
	}
	~A()
	{
		cout << "~A():" << this << endl;
	}
private:
	int _a;
};
int main()
{
	A* p1 = (A*)malloc(sizeof(A));
	A* p2 = new A(1);
	free(p1);
	delete p2;
 
	return 0;
}
```

在这段代码中，p1是我们使用malloc开辟的，p2是通过new来开辟的。我们编译运行这段代码。

![img](https://pic1.zhimg.com/80/v2-20ec18eb90d468ec53c9a1fc55dbfb44_720w.webp)

**注意：在申请自定义类型的空间时，new会自动调用构造函数，delete时会调用析构函数，而malloc和free不会。**

### 3.3new和malloc处理失败

```text
int main()
{
	void* p0 = malloc(1024 * 1024 * 1024);
	cout << p0 << endl;
 
	//malloc失败，返回空指针
	void* p1 = malloc(1024 * 1024 * 1024);
	cout << p1 << endl;
 
	try
	{
		//new失败，抛异常
		void* p2 = new char[1024 * 1024 * 1024];
		cout << p2 << endl;
	}
	catch (const exception& e)
	{
		cout << e.what() << endl;
	}
 
	return 0;
}
```

![img](https://pic2.zhimg.com/80/v2-048a708b4503b8cf4ed70f5459741491_720w.webp)

我们能够发现，malloc失败时会返回空指针，而new失败时，会抛出异常。

## 4.operator new与operator delete函数

### 4.1 operator new与operator delete函数

C++标准库还提供了operator new和operator delete函数，但是这两个函数并不是对new和delete的重载，operator new和operator delete是两个库函数。(这里C++大佬设计时这样取名确实很容易混淆)

**4.1.1 我们看看operator new库里面的源码**

![img](https://pic2.zhimg.com/80/v2-fd3d59c2cae26c43c304ecfecb5ede79_720w.webp)

```text
void* __CRTDECL operator new(size_t size) _THROW1(_STD bad_alloc) {
	// try to allocate size bytes
	void* p;
	while ((p = malloc(size)) == 0)
		if (_callnewh(size) == 0)
		{
			// report no memory
			// 如果申请内存失败了，这里会抛出bad_alloc 类型异常
			static const std::bad_alloc nomem;
			_RAISE(nomem);
		}
	return (p);
}
```

库里面operator new的作用是封装了malloc，如果malloc失败，抛出异常。

**4.1.2 operator delete库里面的源码**

该函数最终是通过free来释放空间的

![img](https://pic3.zhimg.com/80/v2-ab40afe70f6b97692dd72472f7e57b12_720w.webp)

```text
//operator delete 源码
void operator delete(void* pUserData) {
	_CrtMemBlockHeader* pHead;
	RTCCALLBACK(_RTC_Free_hook, (pUserData, 0));
	if (pUserData == NULL)
		return;
	_mlock(_HEAP_LOCK);  /* block other threads */
	__TRY
		        /* get a pointer to memory block header */
		pHead = pHdr(pUserData);
	         /* verify block type */
	_ASSERTE(_BLOCK_TYPE_IS_VALID(pHead->nBlockUse));
	_free_dbg(pUserData, pHead->nBlockUse);
	__FINALLY
		_munlock(_HEAP_LOCK);  /* release other threads */
	__END_TRY_FINALLY
		return;
}
 
/*
free的实现
*/
#define   free(p)               _free_dbg(p, _NORMAL_BLOCK)
```

**4.1.3 operator new和operator delete的价值(重点)**

```text
class A {
public:
	A(int a = 0)
		: _a(a)
	{
		cout << "A():" << this << endl;
	}
	~A()
	{
		cout << "~A():" << this << endl;
	}
private:
	int _a;
};
int main()
{
	//跟malloc功能一样，失败以后抛出异常
	A* ps1 = (A*)operator new(sizeof(A));
	operator delete(ps1);
 
	A* ps2 = (A*)malloc(sizeof(A));
	free(ps2);
 
	A* ps3 = new A;
	delete ps3;
 
	return 0;
}
```

我们使用new的时候，new要开空间，要调用构造函数。new可以转换成call malloc，call 构造函数。但是call malloc 一旦失败，会返回空指针或者错误码。在面向对象的语言中更喜欢使用异常。而operator new相比较malloc的不同就在于如果一旦失败会抛出异常，因此new的底层实现是调用operator new，operator new会调用malloc(如果失败抛出异常)，再调用构造函数。

我们通过汇编看一下ps3

![img](https://pic3.zhimg.com/80/v2-d289598f8aac95c8bf07e415ebd8efca_720w.webp)

operator delete同理。

总结：通过上述两个全局函数的实现知道，operator new 实际也是通过malloc来申请空间，如果malloc申请空间成功就直接返回，否则执行用户提供的空间不足应对措施，如果用户提供该措施就继续申请，否则就抛异常。operator delete 最终是通过free来释放空间的。

### 4.2 重载operator new 与 operator delete（了解）

专属的operator new技术，提高效率。应用：内存池

```text
class A {
public:
	A(int a = 0)
		: _a(a)
	{
		cout << "A():" << this << endl;
	}
 
	// 专属的operator new
	void* operator new(size_t n)
	{
		void* p = nullptr;
		p = allocator<A>().allocate(1);
		cout << "memory pool allocate" << endl;
		return p;
	}
 
	void operator delete(void* p)
	{
		allocator<A>().deallocate((A*)p, 1);
		cout << "memory pool deallocate" << endl;
 
	}
 
	~A()
	{
		cout << "~A():" << this << endl;
	}
private:
	int _a;
};
int main()
{
	int n = 0;
	cin >> n;
	for (int i = 0; i < n; ++i)
	{
		A* ps1 = new A; //operator new + A的构造函数
	}
 
	return 0;
}
```

![img](https://pic4.zhimg.com/80/v2-5b5a98246fa90142f90759d1bc76722f_720w.webp)

注意：一般情况下不需要对operator new和operator delete进行重载，除非在申请和释放空间时候有某些特殊的需求。比如：在使用new和delete申请和释放空间时，打印一些日志信息，可以简单帮助用户来检测是否存在内存泄漏。

## 5.new 和 delete 的实现原理

### 5.1 内置类型

如果申请的是内置类型的空间，new和malloc，delete和free基本类似，不同的地方是：new/delete申请和释放的是单个元素的空间，new[]和delete[]申请的是连续空间，而且new在申请空间失败时会抛异常，malloc会返回NULL。

### 5.2 自定义类型

**5.2.1 new原理**

1、调用operator new函数申请空间

2、再调用构造函数，完成对对象的构造。

**5.2.2 delete原理**

1、先调用析构函数，完成对对象中资源的清理工作。

2、调用operator delete函数释放对象的空间

### 5.2.3 new T[N]原理

1、先调用operator new[]函数，在operator new[]中世纪调用operator new函数完成N个对象空间的申请

2、在申请的空间上执行N次构造函数

**5.2.4 delete[]原理**

1、在释放的对象空间上执行N次析构函数，完成对N个对象中资源的清理

2、调用operator delete[]释放空间，实际在operator delete[]中调用operator delete来释放空间。

## 6.malloc/free和new/delete的异同

**6.1malloc/free和new/delete的共同点**

都是从堆上申请空间，都需要用户手动释放空间。

**6.2malloc/free和new/delete的不同点**

1：malloc 和 free 是函数， new 和 delete 是操作符

2：malloc 申请的空间不会初始化， new 可以初始化

3：malloc 申请空间时，需要手动计算空间大小并传递， new 只需在其后跟上空间的类型即可，如果是多个对象，[] 中指定对象个数即可

4：malloc 的返回值为 void*, 在使用时必须强转， new 不需要，因为 new 后跟的是空间的类型

5：malloc 申请空间失败时，返回的是 NULL ，因此使用时必须判空， new 不需要，但是 new 需要捕获异常

6： 申请自定义类型对象时， malloc/free 只会开辟空间，不会调用构造函数与析构函数，而 new在申请空间后会调用构造函数完成对象的初始化，delete 在释放空间前会调用析构函数完成空间中资源的清理

原文地址：https://zhuanlan.zhihu.com/p/545869650

作者：linux

# 【NO.194】我们常说的短连接长连接和socket和http到底有什么关系

**最近有人问我长连接和短连接的问题，顺便写文一篇仅供对这几个概念还不是很清楚的朋友学习**

**在开始讲之前先来熟悉几个概念**

## **1.socket**

Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议.如今大多数基于网络的软件，如浏览器，即时通讯工具甚至是P2P下载都是基于Socket实现的

![img](https://pic1.zhimg.com/80/v2-3c00fe44041391e0d0f3f05795ebb938_720w.webp)

**Socket通信实例：**

![img](https://pic3.zhimg.com/80/v2-dc3c52243f3aa879eed70c84a36e1f2e_720w.webp)

主机 A 的应用程序要能和主机 B 的应用程序通信，必须通过 Socket 建立连接，而建立 Socket 连接必须需要底层 TCP/IP 协议来建立 TCP 连接。建立 TCP 连接需要底层 IP 协议来寻址网络中的主机。我们知道网络层使用的 IP 协议可以帮助我们根据 IP 地址来找到目标主机，但是一台主机上可能运行着多个应用程序，如何才能与指定的应用程序通信就要通过 TCP 或 UPD 的地址也就是端口号来指定。这样就可以通过一个 Socket 实例唯一代表一个主机上的一个应用程序的通信链路了。

## **2.tcp和udp**

**UDP**，也就是用户数据报协议。UDP是一种无连接的协议，这就意味着我们每次发送数据报时，需要同时发送本机的socket描述符和接收端的socket描述符。因此，我们在每次通信时都需要发送额外的数据

**TCP**，也就是传输控制协议。和UDP不同，TCP是一种基于连接的协议。在使用流通信之前，我们必须在通信的一对socket之间建立连接。其中一个socket作为服务器进行监听连接请求。另一个则作为客户端进行连接请求。一旦两个socket建立好了连接，他们可以单向或双向进行数据传输。

## **3.tcp/ip**

TCP/IP是个协议组，可分为三个层次：网络层、传输层和应用层。

在网络层有IP协议、ICMP协议、ARP协议、RARP协议和BOOTP协议。

在传输层中有TCP协议与UDP协议。

在应用层有:TCP包括FTP、HTTP、TELNET、SMTP等协议 UDP包括DNS、TFTP等协议

## **4.TCP连接的建立和关闭**

当网络通信时采用TCP协议时，在真正的读写操作之前，server与client之间必须建立一个连接，当读写操作完成后，双方不再需要这个连接时它们可以释放这个连接，连接的建立是需要三次握手的，而释放则需要4次握手，所以说每个连接的建立都是需要资源消耗和时间消耗的

经典的三次握手示意图：

![img](https://pic2.zhimg.com/80/v2-c4fd5ee79541653872e7fc492f6a0925_720w.webp)

## **5.短连接**

**连接->传输数据->关闭连接**

**HTTP中的短连接：**是无状态的，浏览器和服务器每进行一次HTTP操作，就建立一次连接，但任务结束就中断连接。

也可以这样说----短连接是指SOCKET连接后发送后接收完数据后马上断开连接。

HTTP的长连接和短连接本质上是TCP长连接和短连接。HTTP属于应用层协议，在传输层使用TCP协议，在网络层使用IP协议。IP协议主要解决网络路由和寻址问题，TCP协议主要解决如何在IP层之上可靠的传递数据包，使在网络上的另一端收到发端发出的所有包，并且顺序与发出顺序一致。

HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头有加入这行代码：

> Connection:keep-alive

HTTP长连接的过期时间

![img](https://pic3.zhimg.com/80/v2-eeb20564044f77a8c236a588444ef43e_720w.webp)

上图中的Keep-Alive: timeout=20，表示这个TCP通道可以保持20秒。另外还可能有max=XXX，表示这个长连接最多接收XXX次请求就断开。对于客户端来说，如果服务器没有告诉客户端超时时间也没关系，服务端可能主动发起四次握手断开TCP连接，客户端能够知道该TCP连接已经无效；另外TCP还有心跳包来检测当前连接是否还活着，方法很多，避免浪费资源。

**tcp中的短连接：**我们模拟一下**TCP**短连接的情况，client向server发起连接请求，server接到请求，然后双方建立连接。client向server发送消息，server回应client，然后一次读写就完成了，这时候双方任何一个都可以发起close操作，不过一般都是client先发起close操作。为什么呢，一般的server不会回复完client后立即关闭连接的，当然不排除有特殊的情况。从上面的描述看，短连接一般只会在client/server间传递一次读写操作

**短连接的优点是**：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段

## **6.长连接**

连接->传输数据->保持连接 -> 传输数据-> 传输中 ->关闭连接。

长连接指建立SOCKET连接后不管是否使用都保持连接，但安全性较差。

**http中的长连接：**HTTP也可以建立长连接的，使用Connection:keep-alive，HTTP 1.1默认进行持久连接。HTTP1.1和HTTP1.0相比较而言，最大的区别就是增加了持久连接支持(貌似最新的 http1.0 可以显示的指定 keep-alive),但还是无状态的，或者说是不可以信任的。

**tcp中的长连接：**接下来我们再模拟一下长连接的情况，client向server发起连接，server接受client连接，双方建立连接。Client与server完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。

首先说一下TCP/IP中的TCP保活功能，保活功能主要为服务器应用提供服务，服务器应用希望知道客户主机是否崩溃，从而可以代表客户使用资源。如果客户已经消失，使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，则服务器将应远等待客户端的数据，保活功能就是试图在服务器端检测到这种半开放的连接。

如果一个给定的连接在两小时内没有任何的动作，则服务器就向客户发一个探测报文段，客户主机必须处于以下4个状态之一：

1. 客户主机依然正常运行，并从服务器可达。客户的TCP响应正常，而服务器也知道对方是正常的，服务器在两小时后将保活定时器复位。
2. 客户主机已经崩溃，并且关闭或者正在重新启动。在任何一种情况下，客户的TCP都没有响应。服务端将不能收到对探测的响应，并在75秒后超时。服务器总共发送10个这样的探测 ，每个间隔75秒。如果服务器没有收到一个响应，它就认为客户主机已经关闭并终止连接。
3. 客户主机崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。
4. 客户机正常运行，但是服务器不可达，这种情况与2类似，TCP能发现的就是没有收到探查的响应。

## 7.什么时候用长连接，短连接？

长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。

而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。

![img](https://pic1.zhimg.com/80/v2-3d3863753676ace3cab9e951f146dd18_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/545256993

作者：Linux

# 【NO.195】一文掌握谷歌 C++ 单元测试框架 GoogleTest

## 1.简介

![img](https://pica.zhimg.com/v2-fab7c4bac9ffec62280760d444ae6ac0_720w.jpg?source=d16d100b)

GoogleTest（简称 GTest） 是 Google 开源的一个跨平台的（Liunx、Mac OS X、Windows等）的 C++ 单元测试框架，可以帮助程序员测试 C++ 程序的结果预期。不仅如此，它还提供了丰富的断言、致命和非致命判断、参数化、”死亡测试”等等。

GoogleTest 官网：[GoogleTest User’s Guide](http://link.zhihu.com/?target=https%3A//google.github.io/googletest/)

GitHub 仓库：[https://github.com/google/googletest](http://link.zhihu.com/?target=https%3A//github.com/google/googletest)

## 2.单元测试

单元测试（unit testing），是指对软件中的最小可测试单元进行检查和验证。至于单元的大小或范围，并没有一个明确的标准，单元可以是一个函数、方法、类、功能模块或者子系统。

单元测试通常和白盒测试联系到一起，如果单从概念上来讲两者是有区别的，不过我们通常所说的单元测试和白盒测试都认为是和代码有关系的，所以在某些语境下也通常认为这两者是同一个东西。还有一种理解方式，单元测试和白盒测试就是对开发人员所编写的代码进行测试。

## 3.优势

测试是独立的和可重复的。GoogleTest 使每个测试用例运行在不同的对象上，从而使测试之间相互独立。当测试失败时，GoogleTest 允许单独运行它以进行快速调试。

测试有良好的组织，可以反映被测试代码的结构。 GoogleTest 将相关测试划分到一个测试组内，组内的测试能共享数据，使测试易于维护。

测试是可移植的和可重复使用的。 与平台无关的代码，其测试代码也应该和平台无关，GoogleTest 能在不同的操作系统下工作，并且支持不同的编译器。

当测试用例执行失败时，提供尽可能多的有效信息，以便定位问题。 GoogleTest 不会在第一次测试失败时停止。相反，它只会停止当前测试并继续下一个测试。还可以设置报告非致命故障的测试，然后继续当前测试。因此，您可以在单个运行-编辑-编译周期中检测和修复多个错误。

测试框架应该将测试编写者从琐事中解放出来，让他们专注于测试内容。 GoogleTest 自动跟踪所有定义的测试，不需要用户为了运行它们而进行枚举。

测试高效、快速。GoogleTest 能在测试用例之间复用测试资源，只需支付一次设置/拆分成本，并且不会使测试相互依赖，这样的机制使单元测试更加高效。

## 4.环境搭建

安装 GoogleTest

### 4.1Bazel

首先创建一个工作区目录：

```text
$ mkdir my_workspace && cd my_workspace
```

接着在工作区的根目录中创建一个 WORKSPACE 文件，并在其中引入外部依赖 GoogleTest，此时 Bazel 会自动去 Github 上拉取文件：

```text
load("@bazel_tools//tools/build_defs/repo:http.bzl", "http_archive")

http_archive(
  name = "com_google_googletest",
  urls = ["https://github.com/google/googletest/archive/609281088cfefc76f9d0ce82e1ff6c30cc3591e5.zip"],
  strip_prefix = "googletest-609281088cfefc76f9d0ce82e1ff6c30cc3591e5",
)
```

接着选取一个目录作为包目录，在该目录下进行代码的编写，例如一个 [http://hello_test.cc](http://link.zhihu.com/?target=http%3A//hello_test.cc) 文件：

```text
#include <gtest/gtest.h>

// Demonstrate some basic assertions.
TEST(HelloTest, BasicAssertions) {
  // Expect two strings not to be equal.
  EXPECT_STRNE("hello", "world");
  // Expect equality.
  EXPECT_EQ(7 * 6, 42);
}
```

在同目录下的 BUILD 文件中说明目标编译的规则：

```text
cc_test(
  name = "hello_test",
  size = "small",
  srcs = ["hello_test.cc"],
  deps = ["@com_google_googletest//:gtest_main"],
)
```

此时执行以下命令即可构建并运行测试程序：

```text
bazel test --test_output=all //:hello_test 
```

### 4.2Cmake

首先创建一个目录：

```text
$ mkdir my_project && cd my_project
```

接着创建 CMakeLists.txt 文件，并声明对 GoogleTest 的依赖，此时 Cmake 会自动去下载对应的库：

```text
cmake_minimum_required(VERSION 3.14)
project(my_project)

# GoogleTest requires at least C++11
set(CMAKE_CXX_STANDARD 11)

include(FetchContent)
FetchContent_Declare(
  googletest
  URL https://github.com/google/googletest/archive/609281088cfefc76f9d0ce82e1ff6c30cc3591e5.zip
)
# For Windows: Prevent overriding the parent project's compiler/linker settings
set(gtest_force_shared_crt ON CACHE BOOL "" FORCE)
FetchContent_MakeAvailable(googletest)
```

此时我们就可以在代码中使用 GoogleTest，我们创建一个 [http://hello_test.cc](http://link.zhihu.com/?target=http%3A//hello_test.cc) 文件：

```text
#include <gtest/gtest.h>

// Demonstrate some basic assertions.
TEST(HelloTest, BasicAssertions) {
  // Expect two strings not to be equal.
  EXPECT_STRNE("hello", "world");
  // Expect equality.
  EXPECT_EQ(7 * 6, 42);
}
```

然后在 CMakeLists.txt 的末尾加上 [http://hello_test.cc](http://link.zhihu.com/?target=http%3A//hello_test.cc) 的构建规则：

```text
enable_testing()

add_executable(
  hello_test
  hello_test.cc
)
target_link_libraries(
  hello_test
  gtest_main
)

include(GoogleTest)
gtest_discover_tests(hello_test)
```

运行下面的命令构建并允许测试程序：

```text
cmake -S . -B build

cmake --build build

cd build && ctest
```



### 4.3安装示例项目

从 GoogleTest 的 GitHub 中下载官方提供的示例项目：

```text
git clone https://github.com/google/googletest.git
```

示例项目位于`googletest/googletest/samples/`目录，示例内容可参考官网的说明：[samples](http://link.zhihu.com/?target=https%3A//google.github.io/googletest/samples.html)

## 5.GoogleTest 实战

**断言**

**断言的概念**

GoogleTest 中，是通过断言（Assertion）来判断代码实现的功能是否符合预期。断言的结果分为成功（success）、非致命错误（non-fatal failture）和致命错误（fatal failture）。

- 成功：断言成功，程序的行为符合预期，程序允许继续向下运行。可以在代码中调用 SUCCEED() 来表示这个结果。
- 非致命错误：断言失败，但是程序没有直接中止，而是继续向下运行。可以在代码中调用 FAIL() 来表示这个结果。
- 致命错误：中止当前函数或者程序崩溃。可以在代码中调用 ADD_FAILURE() 来表示这个结果。

GoogleTest 的断言是类似于函数调用的宏。通过对其行为进行断言来测试一个类或函数。当断言失败时，GoogleTest 会打印断言的源文件和行号位置以及失败消息。还可以提供自定义失败消息，该消息将附加到 GoogleTest 的消息中。

**EXPECT 与 ASSERT**

宏的格式有两种：

- `EXPECT_*`：在失败时会产生非致命故障，不会中止当前功能。
- `ASSERT_*`：在失败时会产生致命错误，并中止当前函数，同一用例后面的语句将不再执行。

通常 `EXPECT_*` 是首选，因为它们允许在测试中报告多个。但是如果在某条件不成立，程序就无法运行时，就应该使用 `ASSERT_*`。

另一方面，因为 `ASSERT_*` 是直接从当前函数返回的，可能会导致一些内存、文件资源没有释放，因此存在内存泄漏的问题。

GoogleTest 提供了一组断言，用于以各种方式验证代码的行为。包括检查布尔条件、基于关系运算符比较值、验证字符串值、浮点值等等。甚至还有一些断言可以通过提供自定义谓词来验证更复杂的状态。有关 GoogleTest 提供的断言的完整列表，可以参考官方文档：Assertions。

**自定义失败信息**

如果想要提供自定义的失败信息，只需要使用流操作符 << 将这些信息流到断言宏中，例如：

```text
ASSERT_EQ(x.size(), y.size()) << "Vectors x and y are of unequal length";

for (int i = 0; i < x.size(); ++i) {
  EXPECT_EQ(x[i], y[i]) << "Vectors x and y differ at index " << i;
}
```

任何可以流向 std::ostream 的东西都可以流向断言宏，特别是 C 语言的字符串（char*）和 std::string 对象。如果一个宽字符串（wchar_t*，Windows 上 UNICODE 模式下的 TCHAR*，或 std::wstring）被流向一个断言，它将在打印时被转换成 UTF-8。

## 6.功能测试

### 6.1 TEST

如果想要创建测试，可以使用宏函数 `TEST`，它具有以下特点：

- `TEST` 是一个没有返回值的宏函数。
- 我们可以在该函数中使用断言来检测代码是否有效，测试的结果由断言决定。如果测试中的任何断言失败（致命或非致命），或者如果测试崩溃，则整个测试失败。否则，它会成功。

函数定义如下：

```text
TEST(TestSuiteName, TestName) {
  ... test body ...
}
```

TestSuiteName 对应测试用例集名称，TestName 是归属的测试用例名称。这两个名称都必须是有效的 C++ 标识符，并且它们不应包含任何下划线。测试的全名由其包含的测试用例集及其测试名称组成。来自不同测试用例集的测试可以具有相同的名称。

这里以官方提供的 Sample1 中的阶乘函数为例：

http://sample1.cc 中的阶乘函数定义如下：

```text
int Factorial(int n) {
  int result = 1;
  for (int i = 1; i <= n; i++) {
    result *= i;
  }

  return result;
}
```

[http://sample1_unittest.cc](http://link.zhihu.com/?target=http%3A//sample1_unittest.cc) 中即为测试代码，这里为了能够更好的组织测试用例，将数据根据正负数划分为三个测试用例集，每一个用例集中都是相同类型的测试用例。

```text
// 测试负数
TEST(FactorialTest, Negative) {
  EXPECT_EQ(1, Factorial(-5));
  EXPECT_EQ(1, Factorial(-1));
  EXPECT_GT(Factorial(-10), 0);
}

// 测试0
TEST(FactorialTest, Zero) { EXPECT_EQ(1, Factorial(0)); }

// 测试正数
TEST(FactorialTest, Positive) {
  EXPECT_EQ(1, Factorial(1));
  EXPECT_EQ(2, Factorial(2));
  EXPECT_EQ(6, Factorial(3));
  EXPECT_EQ(40320, Factorial(8));
}
```

当我们执行测试时，输出如下：

```text
[==========] Running 6 tests from 2 test cases.
[----------] Global test environment set-up.
[----------] 3 tests from FactorialTest
[ RUN      ] FactorialTest.Negative
[       OK ] FactorialTest.Negative (0 ms)
[ RUN      ] FactorialTest.Zero
[       OK ] FactorialTest.Zero (0 ms)
[ RUN      ] FactorialTest.Positive
[       OK ] FactorialTest.Positive (0 ms)
[----------] 3 tests from FactorialTest (2 ms total)
```

这就表示我们的代码通过了所有的测试用例。

### 6.2 TEST_F

如果发现自己编写了两个或多个对相似数据进行操作的测试，可以使用 test fixture 来为多个测试重用这些相同的配置。

> fixture，其语义是固定的设施，而 test fixture 在 GoogleTest 中的作用就是为每个 TEST 都执行一些同样的操作。

其对应的宏函数是`TEST_F`，函数定义如下：

```text
TEST_F(TestFixtureName, TestName) {
  ... test body ...
}
```

TestFixtureName 对应一个 test fixture 类的名称。因此我们需要自己去定义一个这样的类，并让它继承`testing::Test`类，然后根据我们的需要实现下面这两个虚函数：

- `virtual void SetUp()`：类似于构造函数，在 `TEST_F` 之前运行；
- `virtual void TearDown()`：类似于析构函数，在 `TEST_F` 之后运行。

此外`testing::Test`还提供了两个 static 函数：

- `static void SetUpTestSuite()`：在第一个 `TEST` 之前运行
- `static void TearDownTestSuite()`：在最后一个 `TEST` 之后运行

除了这两种，还有一种全局事件，即继承`testing::Environment`，并实现下面两个虚函数：

- `virtual void SetUp()`：在所有用例之前运行；
- `virtual void TearDown()`：在所有用例之后运行。

这里以官方提供的 Sample3 中实现的 Queue 为例，其实现如下：

```text
#ifndef GOOGLETEST_SAMPLES_SAMPLE3_INL_H_
#define GOOGLETEST_SAMPLES_SAMPLE3_INL_H_

#include <stddef.h>

template <typename E>  // E is the element type
class Queue;

template <typename E>  // E is the element type
class QueueNode {
  friend class Queue<E>;

 public:
  const E& element() const { return element_; }

  QueueNode* next() { return next_; }
  const QueueNode* next() const { return next_; }

 private:
  explicit QueueNode(const E& an_element)
      : element_(an_element), next_(nullptr) {}
    
  const QueueNode& operator=(const QueueNode&);
  QueueNode(const QueueNode&);

  E element_;
  QueueNode* next_;
};

template <typename E>  // E is the element type.
class Queue {
 public:
  Queue() : head_(nullptr), last_(nullptr), size_(0) {}

  ~Queue() { Clear(); }

  void Clear() {
    if (size_ > 0) {
      // 1. Deletes every node.
      QueueNode<E>* node = head_;
      QueueNode<E>* next = node->next();
      for (;;) {
        delete node;
        node = next;
        if (node == nullptr) break;
        next = node->next();
      }

      head_ = last_ = nullptr;
      size_ = 0;
    }
  }

  size_t Size() const { return size_; }

  QueueNode<E>* Head() { return head_; }
  const QueueNode<E>* Head() const { return head_; }

  QueueNode<E>* Last() { return last_; }
  const QueueNode<E>* Last() const { return last_; }

  void Enqueue(const E& element) {
    QueueNode<E>* new_node = new QueueNode<E>(element);

    if (size_ == 0) {
      head_ = last_ = new_node;
      size_ = 1;
    } else {
      last_->next_ = new_node;
      last_ = new_node;
      size_++;
    }
  }

  E* Dequeue() {
    if (size_ == 0) {
      return nullptr;
    }

    const QueueNode<E>* const old_head = head_;
    head_ = head_->next_;
    size_--;
    if (size_ == 0) {
      last_ = nullptr;
    }

    E* element = new E(old_head->element());
    delete old_head;

    return element;
  }

  template <typename F>
  Queue* Map(F function) const {
    Queue* new_queue = new Queue();
    for (const QueueNode<E>* node = head_; node != nullptr;
         node = node->next_) {
      new_queue->Enqueue(function(node->element()));
    }

    return new_queue;
  }

 private:
  QueueNode<E>* head_;  // The first node of the queue.
  QueueNode<E>* last_;  // The last node of the queue.
  size_t size_;         // The number of elements in the queue.

  Queue(const Queue&);
  const Queue& operator=(const Queue&);
};
#endif  // GOOGLETEST_SAMPLES_SAMPLE3_INL_H_
```

接着看看测试用例是如何编写的，首先声明了一个 test fixture 类，在这个类中实现了一些测试时用到的辅助函数，以及使用`SetUp`预置了一些测试数据。（除了有特殊需求，则不需要实现`TearDown`，因为析构函数已经帮我们释放了资源）

```text
class QueueTestSmpl3 : public testing::Test {
 protected: 
    
  void SetUp() override {
    q1_.Enqueue(1);
    q2_.Enqueue(2);
    q2_.Enqueue(3);
  }

  // 一个辅助函数
  static int Double(int n) { return 2 * n; }

  // 测试 Queue::Map() 时用到的辅助函数
  void MapTester(const Queue<int>* q) {

    const Queue<int>* const new_q = q->Map(Double);

    ASSERT_EQ(q->Size(), new_q->Size());

    for (const QueueNode<int>*n1 = q->Head(), *n2 = new_q->Head();
         n1 != nullptr; n1 = n1->next(), n2 = n2->next()) {
      EXPECT_EQ(2 * n1->element(), n2->element());
    }
    delete new_q;
  }

  Queue<int> q0_;
  Queue<int> q1_;
  Queue<int> q2_;
};
```

接着看看它的`TEST_F`：

```text
/ 测试默认构造函数
TEST_F(QueueTestSmpl3, DefaultConstructor) {
  EXPECT_EQ(0u, q0_.Size());
}

// 测试出队
TEST_F(QueueTestSmpl3, Dequeue) {
  int* n = q0_.Dequeue();
  EXPECT_TRUE(n == nullptr);

  n = q1_.Dequeue();
  ASSERT_TRUE(n != nullptr);
  EXPECT_EQ(1, *n);
  EXPECT_EQ(0u, q1_.Size());
  delete n;

  n = q2_.Dequeue();
  ASSERT_TRUE(n != nullptr);
  EXPECT_EQ(2, *n);
  EXPECT_EQ(1u, q2_.Size());
  delete n;
}

// 测试Map函数
TEST_F(QueueTestSmpl3, Map) {
  MapTester(&q0_);
  MapTester(&q1_);
  MapTester(&q2_);
}
}  // namespace
```

这里以 DefaultConstructor 为例，来分析一下它的执行流程：

1. QueueTestSmpl3 调用构造函数，构造对象。
2. QueueTestSmpl3 对象调用 SetUp 函数初始化测试配置。
3. DefaultConstructor 开始执行并结束测试。
4. QueueTestSmpl3 对象调用隐式生成的 TearDown 进行清理。
5. QueueTestSmpl3 调用析构函数，析构对象。

### 6.3运行测试

#### 6.3.1 调用测试

与其他 C++ 框架不同，TEST 和 TEST_F 会隐式向 GoogleTest 注册这些测试函数，因此我们不需要为了运行这些它们而进行枚举。

定义完测试后，你可以用 RUN_ALL_TESTS 来运行它们，此时会运行所有的测试，如果全部成功则返回 0，反之则返回 1。其执行流程如下：

1. 保存所有 GoogleTest 标志的状态。
2. 为第一个测试创建一个 test fixture 对象。
3. 通过 SetUp 初始化 test fixture 对象。
4. 在 test fixture 对象上进行测试。
5. 通过 TearDown 清理 test fixture。
6. 删除 test fixture。
7. 恢复所有 GoogleTest 标志的状态。
8. 对下一个测试重复上述步骤，直到所有测试都运行完毕。

如果发生致命故障，则将跳过后续步骤。

#### 6.3.2编写 main 函数

用户不需要编写自己的 `main` 函数，编译器会自动将它链接到 `gtest_main`。如果用户有特殊需求，需要编写一个 `main` 函数，则需要在返回时调用 `RUN_ALL_TESTS()` 来运行所有测试，例如：

```text
int main(int argc, char **argv) {
  printf("Running main() from %s\n", __FILE__);
  testing::InitGoogleTest(&argc, argv);
  return RUN_ALL_TESTS(); 
}
```

`testing::InitGoogleTest`函数会解析 GoogleTest 标志的命令行，并删除所有识别的标志。这允许用户通过各种标志控制测试程序的行为。您必须在调用`RUN_ALL_TESTS`之前调用此函数 ，否则标志将无法正确初始化。

原文地址：https://zhuanlan.zhihu.com/p/544491071

作者：linux

# 【NO.196】redis、nginx、memcached等网络编程模型详解

说到网络编程，就要把下面四个方面处理好。

## 1.网络连接

分为两种：服务端处理接收客户端的连接，服务端作为客户端连接第三方服务

来自客户端的连接，监听accept有收到EPOLLIN事件，或者当前服务器连接上游服务器，进行connect时返回-1，errno为EINPROGRESS，此时再收到EPOLLOUT事件就代表连接上了，因为三次握手最后是需要回复ack给上游服务器。（Connect非阻塞 ，在图中箭头出表示建立成功（需要注册写事件：最终客户端还要给服务器发送个ack确认请求才能建立成功））

![img](https://pic1.zhimg.com/80/v2-2cd7005df924cfb5fdeaf291e993be5c_720w.webp)



```text
int clientfd=accept(listenfd, addr, sz);

//举例为非阻塞io， 阻塞io成功直接返回0
int connectfd=socket(AF_INET,SOCK_STREAM,0);
int ret=connect(connectfd,(struct sockaddr *)&addr,sizeof(addr));
// ret==-1 && errno==EINPROGERESS 正在建立连接
//  ret==-1 && errno==EISCONN 连接建立成功
```

## 2. 网络断开

当客户端断开时，服务端read返回0，或者收到EPOLLRDHUP事件，如果服务端要支持半关闭状态，就关闭读端shutdown(SHUT_RD)，如果不需要支持，直接close即可，大部分都是直接close，一般close前也会进行类似释放资源的操作，如果这步操作比较耗时，可以异步处理，否则导致服务端出现大量的close_wait状态。

如果是服务端主动断开连接时，通过shutdown(SHUT_WR)发送FIN包给客户端，**此时再调用write会返回-1，errno为EPIPE，代表写通道已经关闭。**这里要注意close和shutdown的区别，close时如果fd的引用不为0，是不会真正的释放资源的，比如fd1=dup(fd2)，close(fd2)不会对fd1造成影响，而shutdown则跳过了前面的引用计数检查，直接对网络进行操作，多线程多进程下用close比较好。断开连接时，如果发现接收缓冲区还有数据，直接丢弃，并回复RST包，如果是发送缓冲区还有数据，则会取消nagle进行发送，末尾加上FIN包，如果开启了SO_LINGER，则会在linger_time内等待FIN包的ack，这样保证发送缓冲区的数据被对端接收到。

```text
//主动关闭
close(fd);
shutdown(fd,SHUT_RDWR);
//主动关闭本地读端，对端写段关闭
shutdown(fd,SHUT_RD);
//主动关闭本地写端，对端读端关闭
shutdown(fd,SHUT_WR);

//被动：读端关闭
//有的网络编程需要支持半关闭状态
int n=read(fd,buf,sz);
if(n==0)
{
	close_read(fd);
	//write();
	//close(fd);
}
//被动： 写端关闭
int n=write(fd,buf,sz);
if(n==-1 && errno==EPIPE)
{
	close_write(fd);
	//close(fd);
}
```

## 3. 消息到达

如果read大于0，接收数据正常，处理对应的业务逻辑即可，如果read等于0，说明对端发送了FIN包，如果read小于0，此时要根据errno进行下一步的判断处理，如果是EWOULDBLOCK或者EAGAIN，说明接收缓冲区还没有数据，直接重试即可，如果是EINTR，说明被信号中断了，因为信号中断的优先级比系统调用高，此时也是重试read即可（read从内核态到用户态：正向错误（被信号打断），如EWOULDBLOCK EINRT 还可以正常的读下一次，其他错误直接close），如果是ETIMEDOUT，说明探活超时了，每个socket都有一个tcp_keepalive_timer，当超过tcp_keepalive_time没有进行数据交换时，开始发送探活包，如果探测失败，间隔tcp_keepalive_intvl时间发送下一次探活包，最多连续发送tcp_keepalive_probes次，如果都失败了，则关闭连接，返回ETIMEDOUT错误。

这些探活都是在传输层进行的，如果应用层的进程有死锁或者阻塞，它是检测不到的，这种情况需要在应用层自行加入心跳包机制来进行检测。一般客户端与数据库之间，反向代理与服务器直接直接用探活机制就行，但数据库之间主从复制以及客户端与服务器之间需要加入心跳机制，以防进程有阻塞。

```text
int n=read(fd,buf,sz);
if(n<0)
{
	//n==-1
	if(errno==EINTR || errno==EWOULDBLOCK)
		break;
	close(fd);
}
else if(n==0)
{
	close(fd);
}
else
{
	//处理buf
}
```

## 4. 消息发送

第四个是消息发送，write大于0，消息放入了发送缓冲区，write小于0，同样要分errno的情况处理，如果错误码为EWOULDBLOCK，说明发送缓冲区还装不下你要发送的数据，需要重试，如果是EINTR，说明write系统调用被信号中断了，同样进行重试处理，如果是EPIPE，说明写通道已经关闭了。

```text
int n=write(fd,buf,sz);
if(n==-1)
{
	if(errno==EINTR || errno==EWOULDBLOCK)
		return;
	close(fd);
}
```

**常见网络io模型**

阻塞io和非阻塞io指的是内核**数据准备阶段要不要阻塞等待**，如果内核数据准备好了，将数据从内核拷贝至用户空间还是阻塞的，所以它们都为同步io

![img](https://pic4.zhimg.com/80/v2-a15a416b83fad517ef2d8970fe7ab04b_720w.webp)

阻塞io和非阻塞io：

（1）阻塞在网络线程
（2）连接的fd阻塞属性决定了io函数是否阻塞
（3）具体差异在：io函数在数据未到达时是否立刻返回；

```text
//默认情况下，fd是阻塞的，设置非阻塞的方法如下：
int flag=fcntl(fd,F_GETFL,0);
fcntl(fd,F_SETFL,flag | O_NONBLOCK);
```



## 5.reactor的应用

目前大部分高性能网络中间件都是采用io多路复用加事件处理的机制，也就是reactor模型

### 5.1redis（单reactor）

redis是单reactor模型，只有一个epoll对象，主线程就是一个循环，不断的处理epoll事件， 首先处理accept事件，将接入的连接绑定读事件处理函数后加入epoll中，等epoll检测到连接有读事件到来时，触发读事件处理函数，但这个函数并没有真正的去读数据，而是将该有读事件到来的连接放入clients_pending_read任务队列中，主线程循环到下一次epoll_wait前，再将这些任务队列中的连接分配给各个io线程本地的任务队列io_threads_list处理，在io线程里，不断对io_threads_pending[i]原子变量进行判断，看有没有值，有就代表主线程给派了任务，然后根据io_threads_op任务类型对任务进行读或写处理，先说读处理部分，主要读取客户端数据并进行解析命令处理，将命令读到该连接对应的querybuf中，主线程忙轮询，等待所有io线程解析命令完成，再主线程开始执行命令，因为这些都是内存操作，所以单线程就可以，如果用多线程的话还有锁的问题，执行完命令后将相应结果写入连接对应的buf数组中，如果放不下就放入reply链表中，然后再将各个连接的响应客户端的任务放入clients_pending_write任务队列中，主线程再分配给各个io线程进行写处理，将数据响应给客户端，主线程此时也是忙轮询，等待所有io线程完成，如果最后发现还有数据没发送完，就注册epollout写事件sendReplyToClient，等客户端可写时再把数据发送完。可以看出网络io相关的操作使用了多线程处理，但是命令执行等纯内存操作都是单线程完成的，全程只有一个eventLoop即相当于epollfd，这种就是单reactor模型，每个io线程都有自己的任务队列io_threads_list，所以也没有多线程竞争的问题。

![img](https://pic2.zhimg.com/80/v2-3f1d06072e1621c65a1460def82a87e9_720w.webp)

### 5.2 nginx网络模型（多进程）

nginx采用的是单reactor多进程模型，因为每个连接都是处理的无状态数据，故可以通过多进程实现，多进程之间共享epollfd，在内核2.6以前，accept还存在惊群问题，即如果有连接到来，多个进程的epoll_wait都能监测到，这样多个进程都处理了该相同的连接，这是有问题的，所以nginx采用了文件锁的方式，在
ngx_process_events_and_timers函数中可以看出，哪个进程先获得了这把锁ngx-accept_mutex，就开始监听EPOLLIN事件，并进行epoll_wait接收对应的事件，接收到的事件先不处理，先放入一个队列中，如果是accept事件，就放入accept对应的ngx_posted_accept_events队列中，其它事件放入另外一个ngx_posted_events队列，然后再处理accept队列的事件回调函数，到这里才能释放文件锁，再去处理非accept队列里面的事件回调函数handler。可以看出nginx其实也是只有一个epollfd，只是被多个进程共享了，这样多个进程可以并行处理事件。

![img](https://pic3.zhimg.com/80/v2-093df57fcda7f1e853931fdae78ebf56_720w.webp)

### 5.3 memcached网络模型（多线程）

memcached是基于libevent来实现网络模型的，它是多线程多reactor模型，相比前面两种，它是有多个reactot模型的，也就是说有多个epoll进行事件循环，它也是将网络接入和网络读写io单独分离的方式，主线程主要处理网络的接入，主要看server_sockets函数，有accept事件后会调用event_handler回调函数，该事件是通过调用conn_new函数注册在主线程的event_base类型变量main_base上，所以主线程陷入事件循环后会监听accept事件。该event_handler回调函数里面做的事情就是调用drive_machine，看这个名字就知道是个状态机处理，所以accept事件来时就处理conn_listening下的事情，主要是进行accept得到客户端的sfd，然后通过round_robin算法选择一个工作线程，将该fd信息打包成CQ_ITEM放在工作线程的连接事件队列ev_queue中，最后通过pipe通知那个工作队列有连接事件过来了，其实就是往pipe中发一个”c”字符，工作线程此时处理事件回调thread_libevent_process，该回调主要是从连接事件队列ev_queue中取出主线程传过来的那个item，再调用conn_new处理该item，conn_new之前主线程也是调用的这个，主要是用来绑定fd的读写事件到event_base上去，工作线程则绑定到该线程对应的那个event_base上，这个event_base对应一个epoll，所以memcached是有多个epoll，因为每个工作线程都有自己的event_base，绑定完后，后续的读写事件回调也是event_handler函数，里面再调用drive_machine函数，只是连接的状态变成了conn_read和conn_write，这就是状态机的好处，代码逻辑很清晰。总的来说，主线程处理accept，工作线程处理后续的通信read和write，思路很清晰。

![img](https://pic3.zhimg.com/80/v2-1a0c5033e1aaeada122ba72579a76e42_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/542361454

作者：Linux

# 【NO.197】【底层原理】一层层剥开文件系统的面纱，彻底理解Linux文件系统

## 1.**概述**

提到文件系统，Linux的老江湖们对这个概念当然不会陌生，然而刚接触Linux的新手们就会被文件系统这个概念弄得晕头转向，恰好我当年正好属于后者。

从windows下转到Linux的童鞋听到最多的应该是fat32和ntfs(在windows 2000之后所出现的一种新型的日志文件系统)，那个年代经常听到说“我要把C盘格式化成ntfs格式，D盘格式化成fat32格式”。

一到Linux下，很多入门Linux的书籍中当牵扯到文件系统这个术语时，二话不说，不管三七二十一就给出了下面这个图，然后逐一解释一下每个目录是拿来干啥的、里面会放什么类型的文件就完事儿了，弄得初学者经常“丈二和尚摸不着头脑”。

![img](https://pic2.zhimg.com/80/v2-ae87d45869f254d33c7b411209dda275_720w.webp)

本文的目的就是和大家分享一下我当初是如何学习Linux的文件系统的，也算是一个“老”油条的一些心得吧。

“文件系统”的主语是“文件”，那么文件系统的意思就是“用于管理文件的(管理)系统”，在大多数操作系统教材里，“文件是数据的集合”这个基本点是一致的，而这些数据最终都是存储在存储介质里，如硬盘、光盘、U盘等。

另一方面，用户在管理数据时也是文件为基本单位，他们所关心的问题是：

- • 1.我的文件在什么地方放着？
- • 2.我如何将数据存入某个文件？
- • 3.如何从文件里将数据读出来？
- • 4.不再需要的文件怎么将其删除？

简而言之，文件系统就是一套用于定义文件的命名和组织数据的规范，其根本目的是便对文件进行查询和存取。

## 2.**虚拟文件系统VFS**

在Linux早期设计阶段，文件系统与内核代码是整合在一起的，这样做的缺点是显而易见的。假如，我的系统只能识别ext3格式的文件系统，我的U盘是fat32格式，那么很不幸的是我的U盘将不会被我的系统所识别，

为了支持不同种类的文件系统，Linux采用了在Unix系统中已经广泛采用的设计思想，通过虚拟文件系统VFS来屏蔽下层各种不同类型文件系统的实现细节和差异。

其实VFS最早是由Sun公司提出的，其基本思想是将各种文件系统的公共部分抽取出来，形成一个抽象层。对用户的应用程序而言，VFS提供了文件系统的系统调用接口。而对具体的文件系统来说，VFS通过一系列统一的外部接口屏蔽了实现细节，使得对文件的操作不再关心下层文件系统的类型，更不用关心具体的存储介质，这一切都是透明的。

## 3.**ext2文件系统**

虚拟文件系统VFS是对各种文件系统的一个抽象层，抽取其共性，以便对外提供统一管理接口，便于内核对不同种类的文件系统进行管理。那么首先我们得看一下对于一个具体的文件系统，我们该关注重点在哪里。

对于存储设备(以硬盘为例)上的数据，可分为两部分：

- • 用户数据：存储用户实际数据的部分；
- • 管理数据：用于管理这些数据的部分，这部分我们通常叫它元数据(metadata)。

我们今天要讨论的就是这些元数据。这里有个概念首先需要明确一下：块设备。所谓块设备就是以块为基本读写单位的设备，支持缓冲和随机访问。每个文件系统提供的mk2fs.xx工具都支持在构建文件系统时由用户指定块大小，当然用户不指定时会有一个缺省值。

我们知道一般硬盘的每个扇区512字节，而多个相邻的若干扇区就构成了一个簇，从文件系统的角度看这个簇对应的就是我们这里所说块。用户从上层下发的数据首先被缓存在块设备的缓存里，当写满一个块时数据才会被发给硬盘驱动程序将数据最终写到存储介质上。如果想将设备缓存中数据立即写到存储介质上可以通过sync命令来完成。

块越大存储性能越好，但浪费比较严重；块越小空间利用率较高，但性能相对较低。如果你不是专业的“骨灰级”玩儿家，在存储设备上构建文件系统时，块大小就用默认值。通过命令“tune2fs -l /dev/sda1”可以查看该存储设备上文件系统所使用的块大小：

```text
[root@localhost ~]# 
```

该命令已经暴露了文件系统的很多信息，接下我们将详细分析它们。

下图是我的虚拟机的情况，三块IDE的硬盘。容量分别是：

hda: 37580963840/(1024*1024*1024)=35GB
hdb: 8589934592/(1024*1024*1024)=8GB
hdd: 8589934592/(1024*1024*1024)=8GB

![img](https://pic2.zhimg.com/80/v2-25e955eaa9b81308369401b65a73da21_720w.webp)

如果这是三块实际的物理硬盘的话，厂家所标称的容量就分别是37.5GB、8.5GB和8.5GB。可能有些童鞋觉得虚拟机有点“假”，那么我就来看看实际硬盘到底是个啥样子。

**主角1**：西部数据 500G SATA接口 CentOS 5.5

实际容量：500107862016B = 465.7GB

![img](https://pic1.zhimg.com/80/v2-12e949b371bdadbcf209026e6d15f3cc_720w.webp)

**主角2**：希捷 160G SCSI接口 CentOS 5.5

实际容量：160041885696B=149GB

![img](https://pic2.zhimg.com/80/v2-8e1789d23801e2e024130dc52a3f2345_720w.webp)

大家可以看到，VMware公司的水平还是相当不错的，虚拟硬盘和物理硬盘“根本”看不出差别，毕竟属于云平台基础架构支撑者的风云人物嘛。

以硬盘/dev/hdd1为例，它是我新增的一块新盘，格式化成ext2后，根目录下只有一个lost+found目录，让我们来看一下它的布局情况，以此来开始我们的文件系统之旅。

![img](https://pic2.zhimg.com/80/v2-be66f95ad94aaf5adebe45a0b5be92f1_720w.webp)

对于使用了ext2文件系统的分区来说，有一个叫superblock的结构 ，superblock的大小为1024字节，其实ext3的superblock也是1024字节。下面的小程序可以证明这一点：

```text
#include <stdio.h>
#include <linux/ext2_fs.h>
#include <linux/ext3_fs.h>

int main(int argc,char** argv){
    printf("sizeof of ext2 superblock=%d\n",sizeof(struct ext2_super_block));
    printf("sizeof of ext3 superblock=%d\n",sizeof(struct ext3_super_block));
        return 0;
}

******************【运行结果】******************
sizeof of ext2 superblock=1024
sizeof of ext3 superblock=1024
```

硬盘的第一个字节是从0开始编号，所以第一个字节是byte0，以此类推。/dev/hdd1分区头部的1024个字节(从byte0~byte1023)都用0填充，因为/dev/hdd1不是主引导盘。superblock是从byte1024开始，占1024B存储空间。我们用dd命令把superblock的信息提取出来：

```text
dd if=/dev/hdd1 of=./hdd1sb bs=1024 skip=1 count=1
```

上述命令将从/dev/hdd1分区的byte1024处开始，提取1024个字节的数据存储到当前目录下的hdd1sb文件里，该文件里就存储了我们superblock的所有信息，上面的程序稍加改造，我们就可以以更直观的方式看到superblock的输出了如下：

```text
#include <stdio.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <unistd.h>
#include <string.h>
#include <linux/ext2_fs.h>
#include <linux/ext3_fs.h>

int main(int argc,char** argv){
    printf("sizeof of ext2 superblock=%d\n",sizeof(struct ext2_super_block));
    printf("sizeof of ext3 superblock=%d\n",sizeof(struct ext3_super_block));
    char buf[1024] = {0};
    int fd = -1;
    struct ext2_super_block hdd1sb;
    memset(&hdd1sb,0,1024);

    if(-1 == (fd=open("./hdd1sb",O_RDONLY,0777))){
        printf("open file error!\n");
        return 1;
    }

    if(-1 == read(fd,buf,1024)){
        printf("read error!\n");
        close(fd);
        return 1;
    }

    memcpy((char*)&hdd1sb,buf,1024);
    printf("inode count : %ld\n",hdd1sb.s_inodes_count);
    printf("block count : %ld\n",hdd1sb.s_blocks_count);
    printf("Reserved blocks count : %ld\n",hdd1sb.s_r_blocks_count);
    printf("Free blocks count : %ld\n",hdd1sb.s_free_blocks_count);
    printf("Free inodes count : %ld\n",hdd1sb.s_free_inodes_count);
    printf("First Data Block : %ld\n",hdd1sb.s_first_data_block);
    printf("Block size : %ld\n",1<<(hdd1sb.s_log_block_size+10));
    printf("Fragment size : %ld\n",1<<(hdd1sb.s_log_frag_size+10));
    printf("Blocks per group : %ld\n",hdd1sb.s_blocks_per_group);
    printf("Fragments per group : %ld\n",hdd1sb.s_frags_per_group);
    printf("Inodes per group : %ld\n",hdd1sb.s_inodes_per_group);
    printf("Magic signature : 0x%x\n",hdd1sb.s_magic);
    printf("size of inode structure : %d\n",hdd1sb.s_inode_size);
    close(fd);
    return 0;
}

******************【运行结果】******************
inode count : 1048576
block count : 2097065
Reserved blocks count : 104853
Free blocks count : 2059546
Free inodes count : 1048565
First Data Block : 0
Block size : 4096
Fragment size : 4096
Blocks per group : 32768
Fragments per group : 32768
Inodes per group : 16384
Magic signature : 0xef53
size of inode structure : 128
```

可以看出，superblock 的作用就是记录文件系统的类型、block大小、block总数、inode大小、inode总数、group的总数等信息。

对于ext2/ext3文件系统来说数字签名Magic signature都是0xef53，如果不是那么它一定不是ext2/ext3文件系统。这里我们可以看到，我们的/dev/hdd1确实是ext2文件系统类型。hdd1中一共包含1048576个inode节点(inode编号从1开始)，每个inode节点大小为128字节，所有inode消耗的存储空间是1048576×128=128MB；总共包含2097065个block，每个block大小为4096字节，每32768个block组成一个group，所以一共有2097065/32768=63.99，即64个group(group编号从0开始，即Group0～Group63)。所以整个/dev/hdd1被划分成了64个group，详情如下：

![img](https://pic3.zhimg.com/80/v2-1add941687546beed247f78dfdb06ae6_720w.webp)

用命令tune2fs可以验证我们之前的分析：

![img](https://pic4.zhimg.com/80/v2-deea264e9580e9eff97e29007228986b_720w.webp)

再通过命令dumpe2fs /dev/hdd1的输出，可以得到我们关注如下部分：

![img](https://pic3.zhimg.com/80/v2-48b410e8e9a5ef11322b66c8b22232b2_720w.webp)

接下来以Group0为例，主superblock在Group0的block0里，根据前面的分析，我们可以画出主superblock在block0中的位置如下：

![img](https://pic2.zhimg.com/80/v2-8c46fce54041065fd17a524ec62163a5_720w.webp)

因为superblock是如此之重要，一旦它出错你的整个系统就玩儿完了，所以文件系统中会存在磁盘的多个不同位置会存在主superblock的备份副本，一旦系统出问题后还可以通过备份的superblock对文件系统进行修复。

第一版ext2文件系统的实现里，每个Group里都存在一份superblock的副本，然而这样做的负面效果也是相当明显，那就是严重降低了磁盘的空间利用率。所以在后续ext2的实现代码中，选择用于备份superblock的Group组号的原则是3的N次方、5的N次方、7的N次方其中N=0,1,2,3…。根据这个公式我们来计算一下/dev/hdd1中备份有supeblock的Group号：

![img](https://pic1.zhimg.com/80/v2-0ed5310d027884a44b6e9fae2a4b20cc_720w.webp)

也就是说Group1、3、5、7、9、25、27、49里都保存有superblock的拷贝，如下：

![img](https://pic1.zhimg.com/80/v2-1c7354c9cd57e2cd1dbd14c76c37e478_720w.webp)

用block号分别除以32768就得到了备份superblock的Group号，和我们在上面看到的结果一致。我们来看一下/dev/hdd1中Group和block的关系：

![img](https://pic2.zhimg.com/80/v2-45a40cb5dcd984c792ee0a941936a5c1_720w.webp)

从上图中我们可以清晰地看出在使用了ext2文件系统的分区上，包含有主superblock的Group、备份superblock的Group以及没有备份superblock的Group的布局情况。存储了superblock的Group中有一个组描述符(Group descriptors)紧跟在superblock所在的block后面，占一个block大小；同时还有个Reserved GDT跟在组描述符的后面。

Reserved GDT的存在主要是支持ext2文件系统的resize功能，它有自己的inode和data block，这样一来如果文件系统动态增大，Reserved GDT就正好可以腾出一部分空间让Group descriptor向下扩展。

接下来我们来认识一下superblock，inode，block，group，group descriptor，block bitmap，inode table这些家伙。

## 4.**superblock**

这个东西确实很重要，前面我们已经见识过。为此，文件系统还特意精挑细选的找了N多后备Group，在这些Group中都存有superblock的副本，你就知道它有多重要了。

说白了，superblock 的作用就是记录文件系统的类型、block大小、block总数、inode大小、inode总数、group的总数等等。

## 5.**group descriptors**

千万不要以为这就是一个组描述符，看到descriptor后面加了个s就知道这是N多描述符的集合。确实，这是文件系统中所有group的描述符所构成的一个数组，它的结构定义在include/linux/ext2_fs.h中：

```text
//Structure of a blocks group descriptor
struct ext2_group_desc
{
     __le32   bg_block_bitmap;             /* group中block bitmap所在的第一个block号 */
     __le32   bg_inode_bitmap;            /* group中inode bitmap 所在的第一个block号 */
     __le32   bg_inode_table;                /* group中inodes table 所在的第一个block号 */
     __le16   bg_free_blocks_count;    /* group中空闲的block总数 */
     __le16   bg_free_inodes_count;   /* group中空闲的inode总数*/
     __le16   bg_used_dirs_count;       /* 目录数 */
     __le16   bg_pad;
     __le32   bg_reserved[3];
};
```

下面的程序可以帮助了解一下/dev/hdd1中所有group的descriptor的详情：

```text
#define B_LEN 32  //一个struct ext2_group_desc{}占固定32字节
int main(int argc,char** argv){
    char buf[B_LEN] = {0};
    int i=0,fd = -1;
    struct ext2_group_desc gd;
    memset(&gd,0,B_LEN);

    if(-1 == (fd=open(argv[1],O_RDONLY,0777))){
        printf("open file error!\n");
        return 1;
    }

    while(i<64){    //因为我已经知道了/dev/hdd1中只有64个group
        if(-1 == read(fd,buf,B_LEN)){
            printf("read error!\n");
            close(fd);
            return 1;
        }

        memcpy((char*)&gd,buf,B_LEN);
        printf("========== Group %d: ==========\n",i);
        printf("Blocks bitmap block %ld \n",gd.bg_block_bitmap);
        printf("Inodes bitmap block %ld \n",gd.bg_inode_bitmap);
        printf("Inodes table block %ld \n",gd.bg_inode_table);
        printf("Free blocks count %d \n",gd.bg_free_blocks_count);
        printf("Free inodes count %d \n",gd.bg_free_inodes_count);
        printf("Directories count %d \n",gd.bg_used_dirs_count);

        memset(buf,0,B_LEN);
        i++;
    }

    close(fd);
    return 0;
}
```

运行结果和dumpe2fs /dev/hdd1的输出对比如下：

![img](https://pic1.zhimg.com/80/v2-ac6f1a54ccc09be8d3033174bfe887fc_720w.webp)

其中，文件gp0decp是由命令“dd if=/dev/hdd1 of=./gp0decp bs=4096 skip=1 count=1”生成。每个group descriptor里记录了该group中的inode table的起始block号，因为inode table有可能会占用连续的多个block；空闲的block、inode数等等。

## 6.**block bitmap:**

在文件系统中每个对象都有一个对应的inode节点(这句话有些不太准确，因为符号链接和它的目标文件共用一个inode)，里存储了一个对象(文件或目录)的信息有权限、所占字节数、创建时间、修改时间、链接数、属主ID、组ID，如果是文件的话还会包含文件内容占用的block总数以及block号。inode是从1编号，这一点不同于block。

需要格外注意。另外，/dev/hdd1是新挂载的硬盘，格式化成ext2后并没有任何数据，只有一个lost+found目录。接下来我们用命令“dd if=/dev/hdd1 of=./gp0 bs=4096 count=32768”将Group0里的所有数据提取出来。

前面已经了解了Group0的一些基本信息如下：

```text
Group 0: (Blocks 0-32767)
  Primary superblock at 0, Group descriptors at 1-1
  Reserved GDT blocks at 2-512
  Block bitmap at 513 (+513), Inode bitmap at 514 (+514)
  Inode table at 515-1026 (+515)
  31739 free blocks, 16374 free inodes, 1 directories       #包含一个目录
  Free blocks: 1028-1031, 1033-32767      #一共有31739个空闲block
  Free inodes: 11-16384      #一共有16374个空闲inode
```

一个block bitmap占用一个block大小，而block bitmap中每个bit表示一个对应block的占用情况，0表示对应的block为空，为1表示相应的block中存有数据。在/dev/hdd1中，一个group里最多只能包含8×4096=32768个block，这一点我们已经清楚了。接下来我们来看一下Group0的block bitmap，如下：

![img](https://pic4.zhimg.com/80/v2-5da230c80fdc5d3979b34d5a55aa2b93_720w.webp)

发现block bitmap的前128字节和第129字节的低4位都为1，说明发现Group0中前128×8+4=1028个block，即block0block1027都已被使用了。第129字节的高4位为0，表示block1028block1031四个block是空闲的；第130字节的最低位是1，说明block1032被占用了；从block1033～block32767的block bitmap都是0，所以这些block都是空闲的，和上表输出的结果一致。

## 7.**inode bitmap**

和block bitmap类似，innode bitmap的每个比特表示相应的inode是否被使用。Group0的inode bitmap如下：

![img](https://pic4.zhimg.com/80/v2-eb47b78376ad3799d043d32bf6dfb0a3_720w.webp)

/dev/hdd1里inode总数为1048576，要被均分到64个Group里，所以每个Group中都包含了16384个inode。要表示每个Group中16384个inode，inode bitmap总共需要使用2048(16384/8)字节。inode bitmap本身就占据了一个block，所以它只用到了该block中的前2048个字节，剩下的2048字节都被填充成1，如上图所示。

我们可以看到Group0中的inode bitmap前两个字节分别是ff和03，说明Group0里的前11个inode已经被使用了。其中前10个inode被ext2预留起来，第11个inode就是lost+found目录，如下：

![img](https://pic3.zhimg.com/80/v2-0b860113a662c6e62394ca1f8bba5682_720w.webp)

## 8.**inode table**

那么每个Group中的所有inode到底存放在哪里呢？答案就是inode table。它是每个Group中所有inode的聚合地。

因为一个inode占128字节，所以每个Group里的所有inode共占16384×128=2097152字节，总共消耗了512个block。Group的group descriptor里记录了inode table的所占block的起始号，所以就可以唯一确定每个Group里inode table所在的block的起始号和结束号了。inode的结构如下：

![img](https://pic1.zhimg.com/80/v2-f0ea640278ea7b8c930259deb3d6450c_720w.webp)

这里我们主要关注的其中的数据block指针部分。前12个block指针直接指向了存有数据的block号；第13个block指针所指向的block中存储的并不是数据而是由其他block号，这些block号所对应的block里存储的才是真正的数据，即所谓的两级block指针；第14个block为三级block指针；第15个block为四级block指针。最后效果图如下：

![img](https://pic1.zhimg.com/80/v2-834fcf227df700c51aafadf5105fd4d0_720w.webp)

一个block为4096字节，每个块指针4字节，所以一个block里最多可以容纳4096/4=1024个block指针，我们可以计算出一个inode最大能表示的单个文件的最大容量如下：

| 直接block指针(字节) | 两级block指针(字节) | 三 级block指针(字节) | 四 级block指针(字节) | 单个文件的最大容量(字节) |
| ------------------- | ------------------- | -------------------- | -------------------- | ------------------------ |
| 12×409              | 4096/4×4096         | 40962/4×4096         | 40963/4×4096         | 4TB                      |

所以，我们可以得出不同block大小，对单个文件最大容量的影响。假设block大小为X字节，则:

![img](https://pic3.zhimg.com/80/v2-894719b1a2629869ea334fca958b9f5e_720w.webp)

如下表所示:

| block大小(字节) | 单个文件容量(字节)      |
| --------------- | ----------------------- |
| 1024            | 17247240192字节(16GB)   |
| 2048            | 275415826432字节(256GB) |
| 4096            | 4402345672704字节(4TB)  |

最后来一张全家福：

![img](https://pic2.zhimg.com/80/v2-0fa107fecca0531ca1f07284b58974a5_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/541968430

作者：Linux

# 【NO.198】互斥锁、自旋锁、原子操作的原理、区别及应用场景

## 1.互斥锁

**原理：**

互斥锁属于sleep-waiting类型的锁，例如在一个双核的机器上有两个线程（线程A和线程B）,它们分别运行在Core0和Core1上。假设线程A想要通过pthread_mutex_lock操作去得到一个临界区的锁，而此时这个锁正被线程B所持有，那么线程A就会被阻塞，Core0会在此时进行上下文切换(Context Switch)将线程A置于等待队列中，此时Core0就可以运行其它的任务而不必进行忙等待。

**适用场景：**

因互斥锁会引起线程的切换，效率较低。使用互斥锁会引起线程阻塞等待，不会一直占用这cpu，因此当锁的内容较多，切换不频繁时，建议使用互斥锁

**使用方法：**

```text
/*初始化一个互斥锁*/
int pthread_mutex_init(pthread_mutex_t *restrict  mutex,const pthread_mutexattr_t *restrict attr);
参数：
	mutex:互斥锁地址。类型是pthread_mutex_t.
	attr:设置互斥量的属性，通常采用默认属性，即将attr设为NULL。
	可以使用宏PTHREAD_MUTEX_INITALIZER静态初始化互斥锁，如：
	pthread_mutex_t mutex=PTHREAD_MUTEX_INITIALIZER;
	这种方法等价于NULL指定的attr参数调用pthread_mutex_init()来完成动态初始化，不同之处在于PTHREAD_MUTEX_INITIALIZER宏不进行错误检查。
返回值：
	成功：0
	失败：非0
```



```text
/*销毁指定的一个互斥锁，释放资源*/
int pthread_mutex_destroy(pthread_mutex_t *mutex);
参数：
	mutex：互斥锁地址
返回值：
	成功：0
	失败：非0
```



```text
/*对互斥锁上锁，若互斥锁已经上锁，则调用这阻塞，直到互斥锁解锁后再上锁*/
int pthread_mutex_lock(pthread_mutex_t *mutex)
参数：
	mutex:互斥锁地址
返回值：
	成功：0
	失败：非0
```



```text
/*对指定的互斥锁解锁*/
int pthread_mutex_unlock(pthread_mutex_t *mutex);
参数：
	mutex:互斥锁地址
返回值：
	成功：0
	失败：非0
```

## 2.自旋锁

**原理：**

Spin lock（自旋锁）属于busy-waiting类型的锁，如果线程A是使用pthread_spin_lock操作去请求锁，那么线程A就会一直在Core0上进行忙等待并不停的进行锁请求，直到得到这个锁为止。自旋锁不会引起调用者睡眠，如果自旋锁已经被别的执行单元保持，调用者就一直循环在那里看是否该自旋锁的保持者已经释放了锁。

**使用场景：**

“自旋锁”的作用是为了解决某项资源的互斥使用。因为自旋锁不会引起调用者睡眠，所以自旋锁的效率远高于互斥锁。因此如果锁的内容较少，阻塞的时间较短，使用自旋锁比较好。

自旋锁一直占用着CPU，他在未获得锁的情况下，一直运行（自旋），所以占用着CPU，如果不能在很短的时间内获得锁，这无疑会使CPU效率降低。因此我们要慎重使用自旋锁，自旋锁只有在内核可抢占式或SMP的情况下才真正需要，在单CPU且不可抢占式的内核下，自旋锁的操作为空操作。自旋锁适用于锁使用者保持锁时间比较短的情况下。

**使用方法**

```text
/*初始化一个自旋锁*/
int pthread_spin_init(pthread_spinlock_t *lock, int pshared);
参数：
	pthread_spinlock_t ：初始化自旋锁
	pshared取值：
		PTHREAD_PROCESS_SHARED：该自旋锁可以在多个进程中的线程之间共享。（可以被其他进程中的线程看到）
		PTHREAD_PROCESS_PRIVATE:仅初始化本自旋锁的线程所在的进程内的线程才能够使用该自旋锁。
返回值：
	若成功，返回0；否则，返回错误编号
```



```text
/*销毁一个锁*/
int pthread_spin_destroy(pthread_spinlock_t *lock);
返回值：
	若成功，返回0；否则，返回错误编号
```



```text
/*用来获取（锁定）指定的自旋锁. 如果该自旋锁当前没有被其它线程所持有，则调用该函数的线程获得该自旋锁.否则该函数在获得自旋锁之前不会返回。*/
int pthread_spin_lock(pthread_spinlock_t *lock);
返回值：
	若成功，返回0；否则，返回错误编号
```



```text
/*解锁*/
int pthread_spin_unlock(pthread_spinlock_t *lock);
返回值：
	若成功，返回0；否则，返回错误编号
```



## 3.原子操作

所谓原子操作，就是该操作绝不会在执行完毕前被任何其他任务或事件打断，也就说，它的最小的执行单位，不可能有比它更小的执行单位。因此这里的原子实际是使用了物理学里的物质微粒的概念。

原子操作需要硬件的支持，因此是架构相关的，其API和原子类型的定义都定义在内核源码树的include/asm/atomic.h文件中，它们都使用汇编语言实现，因为C语言并不能实现这样的操作。

原子操作主要用于实现资源计数，很多引用计数(refcnt)就是通过原子操作实现的。

## 4.总结分析

Mutex（互斥锁）：

sleep-waiting类型的锁

与自旋锁相比它需要消耗大量的系统资源来建立锁；随后当线程被阻塞时，线程的调度状态被修改，并且线程被加入等待线程队列；最后当锁可用时，在获取锁之前，线程会被从等待队列取出并更改其调度状态；但是在线程被阻塞期间，它不消耗CPU资源。

互斥锁适用于那些可能会阻塞很长时间的场景。

1、 临界区有IO操作

2 、临界区代码复杂或者循环量大

3 、临界区竞争非常激烈

4、 单核处理器

Spin lock（自旋锁）：

busy-waiting类型的锁

对于自旋锁来说，它只需要消耗很少的资源来建立锁；随后当线程被阻塞时，它就会一直重复检查看锁是否可用了，也就是说当自旋锁处于等待状态时它会一直消耗CPU时间。

自旋锁适用于那些仅需要阻塞很短时间的场景

## 5. 代码实例

```text
/*实现count从0自增到100万*/
#include<stdio.h>
#include<pthread.h>
#include<unistd.h>
#define THREAD_COUNT 10
pthread_mutex_t mutex;
pthread_spinlock_t spinlock;
//原子操作
int inc(int *value,int add)
{
    int old;
	__asm__ volatile(
		"lock; xaddl %2, %1;"
		: "=a" (old)
		: "m" (*value), "a"(add)
		: "cc", "memory"
	);
    return old;
}
void *thread_callback(void *arg)
{
    int *pcount=(int *)arg;
    int i=0;
    while(i++<100000)
    {
    #if 0
        (*pcount)++;
    #elif 0
        pthread_mutex_lock(&mutex);
        (*pcount)++;
        pthread_mutex_unlock(&mutex);
    #elif 0
        pthread_spin_lock(&spinlock);
        (*pcount)++;
        pthread_spin_unlock(&spinlock);
    #else
        inc(pcount,1);
    #endif
        usleep(1);//休眠一微秒
    }
}
int main()
{
    
    pthread_t threadid[THREAD_COUNT]={0};
    pthread_mutex_init(&mutex,NULL);
    //自旋锁
    pthread_spin_init(&spinlock,PTHREAD_PROCESS_SHARED);
    int i=0;
    int count=0;
    for(i=0;i<THREAD_COUNT;i++)
    {
        pthread_create(&threadid[i],NULL,thread_callback,&count);
    }
    for(i=0;i<100;i++)
    {
        printf("count: %d\n",count);
        sleep(1);
    }
    return 0;
    
}
```

原文地址：https://zhuanlan.zhihu.com/p/541703486

作者：linux

# 【NO.199】DNS异步请求池原理与实现

## 1.同步与异步的区别

要设计异步请求池，首先要明白什么是异步、同步。

![img](https://pic3.zhimg.com/80/v2-915fc4ca05461891e8a2bbb0c16ccdea_720w.webp)

异步：就是发送完消息，不用等待结果的返回。发送消息的线程 和 处理消息的线程 是并行的。

同步：就是在发送消息后要等待返回结果，返回结果没有回来的时候这个线程是等待（阻塞）的状态，发送消息的线程 和 处理消息的线程 是串行的。

从上面的概念可以得知做成异步的好处是：

1、不需要等待（阻塞）返回结果，可以尽快的处理其他任务。

2、发送消息的线程 和 处理消息的线程 是并行的，这样可以减少一次发送到接收结果的程序运行时间。

## 2.为什么要做异步请求池

当业务服务器访问需要等待的服务时，业务访问线程需要等待挂起直到该服务给出反馈，例如对mysql,redis,http,dns等服务的请求，这些请求的返回需要等待一段时间，大大加深了业务服务器的承载压力，也会影响其总体性能，异步请求池就是为解决这个问题应运而生的。

## 3.如何做异步请求池

![img](https://pic2.zhimg.com/80/v2-bf49dabc0ab65c1c9dbb3a879dc9ec85_720w.webp)

如上图，**请求池是属于请求端（客户端）的**，请求端 发送连接 给 被请求端，被请求端处理完成后发送结果给请求端。异步请求的话，在发送完连接后，请求端的这个发送线程就可以处理其他任务，无需等待结果。

![img](https://pic2.zhimg.com/80/v2-b8fad2723bbe2bf4cda12f6c30181f41_720w.webp)

现在是异步请求池，说明是要建立多个连接。然后都**不需要等待结果的返回。**当结果有返回的时候，请求端在找到对应的发连接的信息，把结果转发给它。

也就是说，当建立多个连接，就会有多个fd，然后，把这些发送完消息的fd，存到epoll中进行管理。当被请求端发送结果过来的时候，epoll可以找到对应的fd，进行处理。 （ 需要注意的是调用 建立连接函数 的是一个线程，处理接收到结果消息的是一个线程。 ）

异步请求池的实现组成4元组：1.提交commit，2.线程回调函数，3.init 4.释放

工作流程：

**1、init**

epoll创建用来监听fd变化；

线程创建用来接受返回数据；

**2、提交commit**

创建socket，建立连接

封装协议

调用发送数据到对应的第三方服务器

发送完成后，将发送的fd设置为可读事件添加到epoll管理

**3、线程回调函数**

epoll_wait()检查哪些fd可读

遍历可读fd，recv接收数据

读出的数据按照对应的协议进行解析并进行操作

**4、释放销毁**

对应fd关闭close

线程退出释放

## 4.代码实现DNS异步请求池

**初始化init**

初始化函数dns_async_client_init，不需要接收参数。其中主要是**创建 epoll的fd，使用此fd来管理 commit (连接)的 fd，还有要创建处理结果的线程，线程函数是dns_async_client_proc**。

```text
struct async_context {
	int epfd;
};
//dns_async_client_init()
//epoll init
//thread init
/*struct async_context *ctx = dns_async_client_init();*/
struct async_context *dns_async_client_init(void) {

	int epfd = epoll_create(1); // 
	if (epfd < 0) return NULL;

	struct async_context *ctx = calloc(1, sizeof(struct async_context));
	if (ctx == NULL) {
		close(epfd);
		return NULL;
	}
	ctx->epfd = epfd;

	pthread_t thread_id;
	int ret = pthread_create(&thread_id, NULL, dns_async_client_proc, ctx);
	if (ret) {
		perror("pthread_create");
		return NULL;
	}
	usleep(1); //child go first

	return ctx;
}
```

**提交commit**

建立连接函数是dns_async_client_commit，接收参数：struct async_context* ctx、 const char* domain、async_result_cb cb。

首先说一下domain，这个是请求的url。cb是针对此函数中要创建的fd的接收到的结果处理的回调函数。ctx表示async_context结构体指针，此结构中存的是epoll的fd。在此函数还有很多dns的代码，这里就不进行介绍了，百度上应该有很多的。

```text
//dns_async_client_commit(ctx, domain)
//socket init
//dns_request
//sendto dns send
/*dns_async_client_commit(ctx, domain[i], dns_async_client_result_callback);*/
int dns_async_client_commit(struct async_context* ctx, const char *domain, async_result_cb cb) {

	int sockfd = socket(AF_INET, SOCK_DGRAM, 0);
	if (sockfd < 0) {
		perror("create socket failed\n");
		exit(-1);
	}

	printf("url:%s\n", domain);

	set_block(sockfd, 0); //nonblock

	struct sockaddr_in dest;
	bzero(&dest, sizeof(dest));
	dest.sin_family = AF_INET;
	dest.sin_port = htons(53);
	dest.sin_addr.s_addr = inet_addr(DNS_SVR);
	
	int ret = connect(sockfd, (struct sockaddr*)&dest, sizeof(dest));
	//printf("connect :%d\n", ret);

	struct dns_header header = {0};
	dns_create_header(&header);

	struct dns_question question = {0};
	dns_create_question(&question, domain);


	char request[1024] = {0};
	int req_len = dns_build_request(&header, &question, request);
	int slen = sendto(sockfd, request, req_len, 0, (struct sockaddr*)&dest, sizeof(struct sockaddr));

	struct ep_arg *eparg = (struct ep_arg*)calloc(1, sizeof(struct ep_arg));
	if (eparg == NULL) return -1;
	eparg->sockfd = sockfd;
	eparg->cb = cb;

	struct epoll_event ev;
	ev.data.ptr = eparg;
	ev.events = EPOLLIN;

	ret = epoll_ctl(ctx->epfd, EPOLL_CTL_ADD, sockfd, &ev); 
	//printf(" epoll_ctl ADD: sockfd->%d, ret:%d\n", sockfd, ret);

	return ret;	
}
```

**线程回调函数**

dns_async_client_proc函数是处理被请求端返回结果的，是处理所有fd的是否接收到消息的函数，此函数中会一直循环处理epoll_wait，判断是不是有fd来消息了。

epoll_wait循环的判断epoll的fd对应的红黑树中是不是有client的fd来消息了，epoll_wait函数中最后的参数-1表示的是阻塞等待 如果有fd来消息了会返回有多少个fd来消息了，并且把这个些fd从红黑树中移动到一个链表中。nready表示有多少个fd来了消息。

```text
typedef void (*async_result_cb)(struct dns_item *list, int count);
/*typedef void (*async_result_cb)(struct dns_item *list, int count);*/
/*dns_async_client_commit(ctx, domain[i], dns_async_client_result_callback);*/
static void dns_async_client_result_callback(struct dns_item *list, int count) {
	int i = 0;

	for (i = 0;i < count;i ++) {
		printf("name:%s, ip:%s\n", list[i].domain, list[i].ip);
	}
}
//dns_async_client_proc()
//epoll_wait
//result callback
/*int ret = pthread_create(&thread_id, NULL, dns_async_client_proc, ctx);*/
/*接受请求的监听*/
static void* dns_async_client_proc(void *arg) {
	struct async_context *ctx = (struct async_context*)arg;

	int epfd = ctx->epfd;

	while (1) {

		struct epoll_event events[ASYNC_CLIENT_NUM] = {0};

		int nready = epoll_wait(epfd, events, ASYNC_CLIENT_NUM, -1);
		if (nready < 0) {
			if (errno == EINTR || errno == EAGAIN) {
				continue;
			} else {
				break;
			}
		} else if (nready == 0) {
			continue;
		}

		printf("nready:%d\n", nready);
		int i = 0;
		for (i = 0;i < nready;i ++) {

			struct ep_arg *data = (struct ep_arg*)events[i].data.ptr;
			int sockfd = data->sockfd;

			char buffer[1024] = {0};
			struct sockaddr_in addr;
			size_t addr_len = sizeof(struct sockaddr_in);
			int n = recvfrom(sockfd, buffer, sizeof(buffer), 0, (struct sockaddr*)&addr, (socklen_t*)&addr_len);

			struct dns_item *domain_list = NULL;
			int count = dns_parse_response(buffer, &domain_list);

			data->cb(domain_list, count); //call cb
			
			int ret = epoll_ctl(epfd, EPOLL_CTL_DEL, sockfd, NULL);
			//printf("epoll_ctl DEL --> sockfd:%d\n", sockfd);

			close(sockfd); /

			dns_async_client_free_domains(domain_list, count);
			free(data);

		}
		
	}
	
}
```

**销毁释放**

```text
int dns_async_clinet_destroy( struct async_context * ctx )
{
close(ctx->epfd);
pthread_cancel(ctx->threadId);
return 0;
}
```

原文地址：https://zhuanlan.zhihu.com/p/541548631

作者：linux

# 【NO.200】[底层原理]Socket 究竟是什么? 为啥网络离不开 Socket？

## 1.**前言**

一说到网络，大家必然会想到 TCP、UDP、Http、三握四挥等，但是一说 Socket，大家可能会有点模糊了，只知道网络中会用到，但是 Socket 究竟是什么? 套接字又是啥？为啥网络离不开 Socket？

## 2.**Socket 是什么?**

Socket 其实就是套接字，大部分人对于 Socket 的理解就是它可以实现一个简单的网络通信，但是它**「具体解决了哪些问题？有什么实际的作用？为什么会有一个 Socket 出现？」**

![img](https://pic4.zhimg.com/80/v2-4368018a29a51156bf2f708782a9460f_720w.webp)

Socket 其实是在**「应用层与传输层之间的一个产物」**，它把传输层的很多复杂操作封装成一些简单的接口，来让应用层调用以此来实现进程在网络中的通信，Socket 是对端口通信开发的工具,它要更底层一些。

> Socket 其实类似于一台洗碗机，它的功能就是洗碗(网络通信)，如果没有它，你可能需要自己手动去洗碗(手动调用传输层、应用层之间的各个 api)，但是有了它你只需要点击开关、调整时长就行了(封装了 api)，你可以不需要它，但是如果没有它，洗碗(应用层与传输层之间的交互)将变得非常繁琐。

![img](https://pic3.zhimg.com/80/v2-d637527bfa14e29f80304dcda6c740b6_720w.webp)

一次完整的网络通信必不可少的会经过物理传输层的网线和网卡，网络传输层的 IP 协议可以知道要将数据传送给哪台机器，但是在计算机系统中会运行不同进程，那要如何把**「网卡中的网络数据识别出来是给哪个进程的」**，这其实就是 Socket 设计的想解决的一点了。

Socket 是**「对 TCP/IP 或者 UDP/IP 协议的封装」**，Socket 本身其实就是一个调用接口。通过这个接口我们在开发网络应用程序的时候，就可以不用关心底层是怎么实现的，减轻开发的难度。

## **3.Socket 运行流程**

### 3.1 **基于 TCP**

![img](https://pic4.zhimg.com/80/v2-7f0376657c465a412f4a43ecb47a8ec3_720w.webp)

### 3.2 **Server**

- socket():表示创建一个 socket，底层会生成一个文件描述符，用来表示该 socket
- bind():用来绑定服务的端口，地址，这里一般都是以固定的为主，因为在客户端连接的时候需要指定
- listen():当绑定完成之后，listen 就会监听这个端口的数据包
- accept():相当于一个开关，表示我准备好了，可以接受请求了，但是这里会一直阻塞，直到客户端连接成功
- read():读取客户端发送过来的内容
- write():客户端写入要返回的数据
- close():断开连接，**「四次挥手」**

### 3.3 **Client**

- socket():表示创建一个 socket，底层会生成一个文件描述符，用来表示该 socket
- connet():表示与指定地址进行连接，在此之前，会随机创建自己的端口，tcp 的**「三次握手就是从这里开始」**的
- write():客户端写入要发送的数据
- read():客户端读取服务端返回的数据
- close():断开连接，**「四次挥手」**，给客户端发送断开连接的信息

### 3.4 **基于 UDP**

![img](https://pic4.zhimg.com/80/v2-b84ab0043b969ee98aebefb4b7e1199b_720w.webp)

这里我就不细写了，其实大同小异，从流程图上就可以看到

因为 UDP 是无状态的，所以对于服务端来说没有连接，并且其会在调用 Recvfrom() 方法后就收客户端的请求，并一直阻塞，直到收到信息

## 4.**Socket TCP 是如何建立连接的**

在 Socket 绑定完服务器的地址后，就开始和服务器建立连接了，TCP 建立连接的方式其实就是大名鼎鼎三次握手了

![img](https://pic4.zhimg.com/80/v2-5f3599a8b866e214d9bbf01d9257c713_720w.webp)

- 第一次握手:A 的 TCP 进程创建一个 传输控制块 TCB ，然后向 B 发出连接请求报文段。之后将同步位 SYN 设置为 1，同时选择一个初始序列号 seq=x，这时客户端 A 进入到 SYN-SENT（同步已发送）状态。
- 第二次握手:B 收到连接请求报文段，如果同意建立连接，则向 A 发送确认。在确认报文段中 同步位 SYN=1、确认位 ACK=1、确认号 ack=x+1，同时也为自己选择一个初始序列号 seq=y，这时服务器 B 进入 SYN-RCVID 状态。
- 第三次握手:A 收到 B 的确认以后，再向 B 发出确认。确认报文 ACK=1、确认号ack=y+1。这时A进入到 ESTAB-LISHED 状态。当B接收到A的确认后，也进入 ESTAB-LISHED 状态。连接建立完成

### 4.1**三次握手发生在 socket 的哪几个函数中**

![img](https://pic1.zhimg.com/80/v2-c58c69377a495f6f76d77728dc5b8040_720w.webp)

- 当客户端调用 connect 时，触发了连接请求，向服务器发送了SYN 信号，这时 connect 进入阻塞状态；
- 服务器监听到连接请求，即收到 SYN，调用 accept 函数接收，进入阻塞状态，在此之前会尽力 socket、bind、listen 函数；然后返回相关的 syn 以及 ack 信号
- 客户端接受到服务端的信息，此时 connect 完成，解除阻塞状态，并且向服务端发送 ack 信号
- 服务端收到 ack， accept 阻塞解除，完成连接

在建立连接之后，connect() 就已经执行完毕了，服务端就可以向客户端发送数据了。

## 5.**Socket TCP 是如何断开连接的**

![img](https://pic3.zhimg.com/80/v2-9e7c0041718ccd6d0e3d86920144794a_720w.webp)

- 第一次挥手:A 先发送连接释放报文段，段首部的终止控制位 FIN=1，序号seq=u（等于A前面发送数据的最后一个序号加1）；然后 A 进入 FIN-WAIT-1（终止等待1）状态，等待 B 的确认。

- 第二次挥手:B 收到 A 的连接释放报文段后，立刻发出确认报文段，确认号 ack=u+1，序号 seq=v（等于 B 前面发送数据的最后一个序号加1）；然后 B 进入 CLOSE-WAIT（关闭等待）状态。

- 第三次挥手:A 收到 B 的确认报文段后进入到 FIN-WAIT-2（终止等待2）状态，继续等待 B 发出连接释放报文段；

- - 若 B 已经没有数据要发送，B 就会向 A 发送连接释放报文段，段首部的终止控制位 FIN=1，序号 seq=w（半关闭状态可能又发送了一些数据），确认号 ack=u+1，这时B进入 LAST-ACK（最后确认）状态，等待A的确认。

- 第四次挥手:A收到B的连接释放报文段并发出确认，确认段中 确认位 ACK=1，确认号 ack=w+1，序号 seq=u+1；然后 A 进入到TIME-WAIT（时间等待）状态。当 B 再接收到该确认段后，B 就进入 CLOSED状态。

### 5.1**第四次挥手后为何要等待 2MSL**

首先 2MSL 的时间是从客户端(A)接收到 FIN 后发送 ACK 开始计时的。如果在 TIME-WAIT 时间内，因为客户端(A)的 ACK 没有传输到服务端(B)，客户端(A)又接收到了服务端(B)重发的 FIN 报文，那么 2MSL 时间会被重置。等待 2MSL 原因如下

- 1.得原来连接的数据包消失

- - 如果B没有收到自己的ACK，会超时重传FiN那么A再次接到重传的FIN，会再次发送ACK
  - 如果B收到自己的ACK，也不会再发任何消息

在最后一次挥手后 A 并不知道 B 是否接到自己的信息， 包括 ACK 是以上哪两种情况，A 都需要等待，要取这**「两种情况等待时间的最大值，以应对最坏的情况发生」**，这个最坏情况是：去向ACK消息最大存活时间（MSL) + 来向FIN消息的最大存活时间(MSL)。这刚好是2MSL，这个时间，足以使得原来连接的数据包在网络中消失。

- 2.保证 ACK 能被服务端接收到从而正确关闭链接

因为这个 ACK 是有可能丢失的，会导致服务器收不到对 FIN-ACK 确认报文。假设客户端不等待 2MSL ，而是在发送完 ACK 之后直接释放关闭，一但这个 ACK 丢失的话，服务器就无法正常的进入关闭连接状态。

原文地址：https://zhuanlan.zhihu.com/p/540665324

作者：linux



# 【NO.201】后端开发-MySQL数据库相关流程图原理图

前言

整理了一些 MySQL 数据库相关流程图/原理图。做一下笔记，分享给大家一起学习。

## **1. MySQL 主从复制原理图**

MySQL 主从复制原理是大厂后端的高频面试题，了解 MySQL 主从复制原理非常有必要。

### **1.1.主从复制原理简言之，就三步曲，如下**：

- 主数据库有个 bin-log 二进制文件，记录了所有增删改 SQL 语句（binlog线程）；
- 从数据库把主数据库的 bin-log 文件的 SQL 语句复制过来（I/O线程）；
- 从数据库的 relay-log 重做日志文件中再执行一次这些 SQL 语句（SQL 执行线程）。

如下图所示：

![img](https://pic3.zhimg.com/80/v2-9e6ad5e7b79c989624c8bff3a59abf56_720w.webp)

上图主从复制分了五个步骤进行：

- 步骤一：主库的更新事件（update、insert、delete）被写到 binlog；
- 步骤二：从库发起连接，连接到主库；
- 步骤三：此时主库创建一个 binlog dump thread，把 binlog 的内容发送到从库；
- 步骤四：从库启动之后，创建一个 I/O 线程，读取主库传过来的 binlog 内容并写入到 relay log；
- 步骤五：还会创建一个 SQL 线程，从 relay log 里面读取内容，从 Exec_Master_Log_Pos 位置开始执行读取到的更新事件，将更新内容写入到 slave 的 DB。

## **2. MySQL 逻辑架构图**

如果能在脑海中构建出 MySQL 各组件之间如何协同工作的架构图，就会有助于深入理解 MySQL 服务器。

![img](https://pic4.zhimg.com/80/v2-acdcb671ec74382ee3b3dd6757591943_720w.webp)

MySQL 逻辑架构图主要分三层：

### 2.1.**第一层负责连接处理，授权认证，安全等。**

- 每个客户端连接都会在服务器进程中拥有一个线程，服务器维护了一个线程池，因此不需要为每一个新建的连接创建或者销毁线程；
- 当客户端连接到Mysql服务器时，服务器对其进行认证，通过用户名和密码认证，也可以通过SSL证书进行认证；
- 一旦客户端连接成功，服务器会继续验证客户端是否具有执行某个特定查询的权限。

### 2.2.**第二层负责编译并优化SQL。**

- 这一层包括查询解析，分析，优化，缓存以及所有的的内置函数；
- 对于SELECT语句，在解析查询前，服务器会先检查查询缓存，如果能在其中找到对应的查询结果，则无需再进行查询解析、优化等过程，直接返回查询结果；
- 所有跨存储引擎的功能都在这一层实现:存储过程，触发器，视图。

### 2.3.**第三层是存储引擎。**

- 存储引擎负责在 MySQL 中存储数据、提取数据；
- 存储引擎通过 API 与上层进行通信，这些 API 屏蔽了不同存储引擎之间的差异，使得这些差异对上层查询过程透明；
- 存储引擎不会去解析 SQL，不同存储引擎之间也不会相互通信，而只是简单地响应上层服务器的请求。

## **3. InnoDB 逻辑存储结构图**

从 InnoDB 存储引擎的逻辑存储结构看，所有数据都被逻辑地存放在一个空间中，称之为表空间（tablespace）。表空间又由段（segment）、区（extent）、页（page）组成。页在一些文档中有时候也称为块（block）。

InnoDB 逻辑存储结构图如下：

![img](https://pic2.zhimg.com/80/v2-f1a4f12cba911e1969cf96de1ec46cc5_720w.webp)

### 3.1.**表空间（tablespace）**

- 表空间是 Innodb 存储引擎逻辑的最高层，所有的数据都存放在表空间中；
- 默认情况下 Innodb 存储引擎有一个共享表空间 ibdata1，即所有数据都存放在这个表空间中内；
- 如果启用了 innodb_file_per_table 参数，需要注意的是每张表的表空间内存放的只是数据、索引、和插入缓冲 Bitmap，其他类的数据，比如回滚（undo）信息、插入缓冲检索页、系统事物信息，二次写缓冲等还是放在原来的共享表内的。

### 3.2.**段（segment）**

- 表空间由段组成，常见的段有数据段、索引段、回滚段等；
- InnoDB 存储引擎表是索引组织的，因此数据即索引，索引即数据。数据段即为 B+ 树的叶子结点，索引段即为 B+ 树的非索引结点；
- 在 InnoDB 存储引擎中对段的管理都是由引擎自身所完成，DBA 不能也没必要对其进行控制。

### 3.3.**区（extent）**

- 区是由连续页组成的空间，在任何情况下每个区的大小都为 1MB；
- 为了保证区中页的连续性，InnoDB 存储引擎一次从磁盘申请 4~5 个区
- 默认情况下，InnoDB 存储引擎页的大小为 16KB，一个区中一共 64 个连续的区。

### 3.4.**页（page）**

- 页是 InnoDB 磁盘管理的最小单位；
- 在 InnoDB 存储引擎中，默认每个页的大小为 16KB；
- 从 InnoDB1.2.x 版本开始，可以通过参数 innodb_page_size 将页的大小设置为 4K、8K、16K；
- InnoDB 存储引擎中，常见的页类型有：数据页、undo页、系统页、事务数据页、插入缓冲位图页、插入缓冲空闲列表页等。

## **4. InnoDB页结构相关示意图**

### **4.1. InnoDB 页结构单体图**

InnoDB 数据页由以下 7 部分组成，如图所示：

![img](https://pic1.zhimg.com/80/v2-22247e57a4846b91b593a11a61969474_720w.webp)

其中 File Header、Page Header、File Trailer 的大小是固定的，分别为 38、56、8 字节，这些空间用来标记该页的一些信息，如 Checksum，数据页所在 B+ 树索引的层数等。User Records、Free Space、Page Directory 这些部分为实际的行记录存储空间，因此大小是动态的。

下边我们用表格的方式来大致描述一下这 7 个部分：

![img](https://pic4.zhimg.com/80/v2-ce5aa5984c75971afd3841dbcc4ae08f_720w.webp)

### **4.2.记录在页中的存储流程图**

每当我们插入一条记录，都会从 Free Space 部分，也就是尚未使用的存储空间中申请一个记录大小的空间划分到 User Records 部分，当 Free Space 部分的空间全部被 User Records 部分替代掉之后，也就意味着这个页使用完了，如果还有新的记录插入的话，就需要去申请新的页了，这个过程的图示如下：

![img](https://pic1.zhimg.com/80/v2-bc189de5ff76acb2f1b71a5b520819bc_720w.webp)

### **4.3. 不同Innodb页构成的数据结构图**

一张表中可以有成千上万条记录，一个页只有 16KB，所以可能需要好多页来存放数据。不同页其实构成了一条双向链表，File Header 是 InnoDB 页的第一部分，它的 FIL_PAGE_PREV 和 FIL_PAGE_NEXT 就分别代表本页的上一个和下一个页的页号，即链表的上一个以及下一个节点指针。

![img](https://pic2.zhimg.com/80/v2-428f45f5f3dd18a5e20ee638c39ab589_720w.webp)

## **5. Innodb索引结构图**

我们先看一份数据表样本，假设 Col1 是主键，如下：

![img](https://pic3.zhimg.com/80/v2-1d37141d43713954445c2f9ecd6ffa3e_720w.webp)

### **5.1. B+ 树聚集索引结构图**

![img](https://pic2.zhimg.com/80/v2-76459a4b2c901e4f45c75bb4dddfc385_720w.webp)

- 聚集索引就是以主键创建的索引；
- 聚集索引在叶子节点存储的是表中的数据。

### **5.2. 非聚集索引结构图**

假设索引列为 Col3，索引结构图如下：

![img](https://pic1.zhimg.com/80/v2-5ceb13fc9d46f6bb5658e549302665e0_720w.webp)

- 非聚集索引就是以非主键创建的索引；
- 非聚集索引在叶子节点存储的是主键和索引列；
- 使用非聚集索引查询出数据时，拿到叶子上的主键再去查到想要查找的数据。(拿到主键再查找这个过程叫做回表)；
- 假设所查询的列，刚好都是索引对应的列，不用再回表查，那么这个索引列，就叫覆盖索引。

### **5.3. InnoDB 锁类型思维导图**

![img](https://pic2.zhimg.com/80/v2-533141961e3f37fdd9e9a972f9260ea1_720w.webp)

### **5.4. 加锁机制**

乐观锁与悲观锁是两种并发控制的思想，可用于解决丢失更新问题。

**乐观锁**

- 每次去取数据，都很乐观，觉得不会出现并发问题；
- 因此，访问、处理数据每次都不上锁；
- 但是在更新的时候，再根据版本号或时间戳判断是否有冲突，有则处理，无则提交事务。

**悲观锁**

- 每次去取数据，很悲观，都觉得会被别人修改，会有并发问题；
- 因此，访问、处理数据前就加排他锁；
- 在整个数据处理过程中锁定数据，事务提交或回滚后才释放锁。

### **5.5. 锁粒度**

- **表锁**： 开销小，加锁快；锁定力度大，发生锁冲突概率高，并发度最低;不会出现死锁；
- **行锁**： 开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高；
- **页锁**： 开销和加锁速度介于表锁和行锁之间；会出现死锁；锁定粒度介于表锁和行锁之间，并发度一般。

### **5.6. 兼容性**

**共享锁**

- 又称读锁（S 锁）；
- 一个事务获取了共享锁，其他事务可以获取共享锁，不能获取排他锁，其他事务可以进行读操作，不能进行写操作；
- SELECT … LOCK IN SHARE MODE 显示加共享锁。

**排他锁**

- 又称写锁（X 锁）。
- 如果事务 T 对数据 A 加上排他锁后，则其他事务不能再对 A 加任何类型的封锁。获准排他锁的事务既能读数据，又能修改数据。
- SELECT … FOR UPDATE 显示添加排他锁。

### **5.7. 锁模式**

- **记录锁**： 在行相应的索引记录上的锁，锁定一个行记录；
- **gap 锁**： 是在索引记录间歇上的锁，锁定一个区间；
- **next-key 锁**： 是记录锁和在此索引记录之前的 gap 上的锁的结合，锁定行记录+区间；
- **意向锁**：是为了支持多种粒度锁同时存在。

原文链接：https://zhuanlan.zhihu.com/p/353834021

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.202】HTTP—TCP/IP—SOCKET理解及浅析

## 1.一个完整的HTTP请求的过程

此举例为抛砖引玉，引导大家进入思考状态。

当你按输入[http://www.baidu.com](https://link.zhihu.com/?target=http%3A//www.baidu.com) ，浏览器接收到这个消息之后，浏览器根据自己的算法识别出你要访问的URL,为您展示出来搜索页面和**广告**，那么这些经历了哪些过程呢？

### **1.1.大致过程如下：**

- （1）浏览器查询 DNS，获取域名对应的IP地址； 具体过程包括浏览器搜索自身的DNS缓存、搜索操作系统的DNS缓存、读取本地的Host文件和向本地DNS服 务器进行查询等。
- （2）浏览器获得域名对应的IP地址以后，浏览器向服务器请求建立链接，发起三次握手；
- （3）TCP/IP链接建立起来后，浏览器向服务器发送HTTP请求；
- （4）服务器接收到这个请求，并根据路径参数映射到特定的请求处理器进行处理，并将处理结果及相应的视图返回给浏览器；
- （5）浏览器解析并渲染视图，若遇到对js文件、css文件及图片等静态资源的引用，则重复上述步骤并向服务器请求这些资源；
- （6）浏览器根据其请求到的资源、数据渲染页面，最终向用户呈现一个完整的页面。

**下面，我们从底到上来一层层理解这个问题。**

## 2.网络参考模型

**开放式系统互联通信参考模型**（英语：**O**pen **S**ystem **I**nterconnection Reference Model，缩写：OSI；简称为**OSI模型**）是一种概念模型，由国际标准化组织提出，一个试图使各种计算机在世界范围内互连为网络的标准框架。定义于ISO/IEC 7498-1。（摘自维基百科）

|  7   |  应用层 application layer  | 例如HTTP、SMTP、SNMP、FTP、Telnet、SIP、SSH、NFS、RTSP、XMPP、Whois、ENRP、TLS |
| :--: | :------------------------: | :----------------------------------------------------------: |
|  6   | 表示层 presentation layer  |                例如XDR、ASN.1、SMB、AFP、NCP                 |
|  5   |    会话层 session layer    | 例如ASAP、ISO 8327 / CCITT X.225、RPC、NetBIOS、ASP、IGMP、Winsock、BSD sockets |
|  4   |   传输层 transport layer   |            例如TCP、UDP、RTP、SCTP、SPX、ATP、IL             |
|  3   |    网络层 network layer    | 例如IP、ICMP、IPX、BGP、OSPF、RIP、IGRP、EIGRP、ARP、RARP、X.25 |
|  2   | 数据链路层 data link layer | 例如以太网、令牌环、HDLC、帧中继、ISDN、ATM、IEEE 802.11、FDDI、PPP |
|  1   |   物理层 physical layer    |                    例如线路、无线电、光纤                    |

### **2.1.通常人们认为OSI模型的最上面三层（应用层、表示层和会话层）在TCP/IP组中是一个应用层。**

由于TCP/IP有一个相对较弱的会话层，由TCP和RTP下的打开和关闭连接组成，并且在TCP和UDP下的各种应用提供不同的端口号，这些功能能够被单个的应用程序（或者那些应用程序所使用的库）增加。与此相似的是，IP是按照将它下面的网络当作一个黑盒子的思想设计的，这样在讨论TCP/IP的时候就可以把它当作一个独立的层。

### **2.2.TCP/IP 参考模型**

|  4   |          应用层 application layer           | 例如HTTP、FTP、DNS （如BGP和RIP这样的路由协议，尽管由于各种各样的原因它们分别运行在TCP和UDP上，仍然可以将它们看作网络层的一部分） |
| :--: | :-----------------------------------------: | :----------------------------------------------------------: |
|  3   |           传输层 transport layer            | 例如TCP、UDP、RTP、SCTP （如OSPF这样的路由协议，尽管运行在IP上也可以看作是网络层的一部分） |
|  2   |          网络互连层 internet layer          | 对于TCP/IP来说这是因特网协议（IP） （如ICMP和IGMP这样的必须协议尽管运行在IP上，也仍然可以看作是网络互连层的一部分；ARP不运行在IP上） |
|  1   | 网络访问(链接)层 Network Access(link) layer |                 例如以太网、Wi-Fi、MPLS等。                  |

**下面一张图更有助于你的理解**

![img](https://pic4.zhimg.com/80/v2-34632647cb55abaf0dc9fedd7e75cb2f_720w.webp)

## 3.HTTP 协议与 TCP/IP 协议

**HTTP 是 TCP/IP 参考模型中应用层的其中一种实现。**HTTP 协议的网络层基于 IP 协议，传输层基于 TCP 协议：HTTP 协议是基于 TCP/IP 协议的应用层协议。

TCP/IP 协议需要向程序员提供可编程的 API，该 API 就是 Socket，它是对 TCP/IP 协议的一个重要的实现，几乎所有的计算机系统都提供了对 TCP/IP 协议族的 Socket 实现。

**Socket是进程通讯的一种方式，即调用这个网络库的一些API函数实现分布在不同主机的相关进程之间的数据交换。**

- 流格式套接字（SOCK_STREAM） 流格式套接字（Stream Sockets）也叫“面向连接的套接字”，它基于 TCP 协议，在代码中使用 SOCK_STREAM 表示。
- 数据报格式套接字（SOCK_DGRAM） 数据报格式套接字（Datagram Sockets）也叫“无连接的套接字”，基于 UDP 协议，在代码中使用 SOCK_DGRAM 表示。 TCP与UDP 协议区别与优劣势 TCP 是面向连接的传输协议，建立连接时要经过三次握手，断开连接时要经过四次握手，中间传输数据时也要回复 ACK 包确认，多种机制保证了数据能够正确到达，不会丢失或出错。 UDP 是非连接的传输协议，没有建立连接和断开连接的过程，它只是简单地把数据丢到网络中，也不需要 ACK 包确认。 如果只考虑可靠性，TCP 的确比 UDP 好。但 UDP 在结构上比 TCP 更加简洁，不会发送 ACK 的应答消息， 也不 会给数据包分配 Seq 序号，所以 UDP 的传输效率有时会比 TCP 高出很多，编程中实现 UDP 也比 TCP 简单。 与 UDP 相比，TCP 的生命在于流控制，这保证了数据传输的正确性。

最后需要说明的是：TCP 的速度无法超越 UDP，但在收发某些类型的数据时有可能接近 UDP。例如，每次交换的数据量越大，TCP 的传输速率就越接近于 UDP。

## 4.TCP/IP 协议、HTTP 协议和 Socket 有什么区别？

**从包含范围来看，它们的继承关系是这样的：**

![img](https://pic1.zhimg.com/80/v2-d159a33b45ea76b184581c8b102f2a5c_720w.webp)

**从横向来看，它们的继承关系是这样的：**

![img](https://pic1.zhimg.com/80/v2-400a8532c1150d5162128c050e90d0b4_720w.webp)

关于TCP/IP和HTTP协议的关系，有一段比较容易理解的介绍：

**我们在传输数据时，可以只使用（传输层）TCP/IP协议，但是那样的话，如果没有应用层，便无法识别数据内容，如果想要使传输的数据有意义，则必须使用到应用层协议，应用层协议有很多，比如HTTP、FTP、TELNET等，也可以自己定义应用层协议。WEB使用HTTP协议作应用层协议，以封装HTTP文本信息，然后使用TCP/IP做传输层协议将它发到网络上。**

Socket是什么呢，实际上S**ocket是对TCP/IP协议的封装，Socket本身并不是协议，而是一个调用接口（API），通过Socket，我们才能使用TCP/IP协议。**

TCP/IP只是一个协议栈，就像操作系统的运行机制一样，必须要具体实现，同时还要提供对外的操作接口。这个就像操作系统会提供标准的编程接口，比如win32编程接口一样，TCP/IP也要提供可供程序员做网络开发所用的接口，这就是Socket编程接口。”

## 5.TCP/IP 和 HTTP 的数据结构

HTTP 作为 TCP/IP 参考模型的应用层，把 HTTP 放到 TCP/IP 参考模型中，它们的继承结构是这样的：

![img](https://pic3.zhimg.com/80/v2-874e1a93f2a12ef9f29f01bfd7150472_720w.webp)

在 TCP/IP 参考模型中它们的整体的数据结构是：IP 作为以太网的直接底层，IP 的头部和数据合起来作为以太网的数据，同样的 TCP/UDP 的头部和数据合起来作为 IP 的数据，HTTP 的头部和数据合起来作为 TCP/UDP 的数据。

![img](https://pic2.zhimg.com/80/v2-f8f09846c3cb3a85110aa8c476f2ebad_720w.webp)

**IP 的数据结构和交互流程**

我们都知道在一个成功的 HTTP 请求中，服务端可以在一个请求中获取到客户端 IP 地址，也可以获取到客户端请求的主机的 IP 地址。然而这是怎么做到的呢？这就有赖于 IP 协议了，在 IP 协议中规定了，IP 的头部必须包含源 IP 地址和目的 IP 地址，这也是为什么在 TCP/IP 参考模型中IP 处在网络互联层，其中一个原因就是可以定位服务端地址和客户端地址，我们来看一下 IP 的数据结构：

![img](https://pic2.zhimg.com/80/v2-543fe5bc79e1464ccaafb795822c4b31_720w.webp)

可以很清晰的看到源 IP 地址和目的 IP 地址，在 IP 的头部各占 32 位，而 IPV4 的 IP 地址是用点式十进制表示的，例如：192.168.1.1，在 IP 头部用二进制表示的话，刚好是 4 个字节 32 位。

32 位可以表示的 IP 地址是有限的，使用了 IP 地址转换技术 NAT。例如 ABC 三个小区的所有设备可能公用了一个**公网 IP**，通过 **NAT 技术分给每一户一个私有 IP** 地址，大家在小区内交流时可能使用的是私有 IP 地址，但是向外交流时就用公网 IP。

## 6.TCP 的数据结构和交互流程

我们通常说的 HTTP 的 3 次握手和 4 次挥手都是由 TCP 来完成的，其实这都没 HTTP 什么事，但是有不少人喜欢这么说，严格来说我们应该说 TCP 的 3 次握手 4 次挥手。要搞清楚 TCP 的交互流程，首先要清楚 TCP 的数据结构，接下来我们来看一下 TCP 的数据结构：

![img](https://pic3.zhimg.com/80/v2-8f58bf59758005dc89d9d61c385a09b6_720w.webp)

上述 TCP 的数据结构图对于后面理解 HTTP 的交互流程非常重要，我们要记住 5 个关键的位置：

> SYN：建立连接标识 ACK：响应标识 FIN：断开连接标识 seq：seq number，发送序号 ack：ack number，响应序号

服务端应用启动后，会在指定端口监听客户端的连接请求，当客户端尝试创建一个到服务端指定端口的 TCP 连接，服务端收到请求后接受数据并处理完业务后，会向客户端作出响应，客户端收到响应后接受响应数据，然后断开连接，一个完整的请求流程就完成了。这样的一个完整的 TCP 的**生命周期会经历以下 4 个步骤**：

> 1,建立 TCP 连接，3 次握手
>
> 客户端发送SYN, seq=x，进入 SYN_SEND 状态
>
> 服务端回应SYN, ACK, seq=y, ack=x+1，进入 SYN_RCVD 状态
>
> 客户端回应ACK, seq=x+1, ack=y+1，进入 ESTABLISHED 状态，服务端收到后进入 ESTABLISHED 状态 2,进行数据传输
>
> 客户端发送ACK, seq=x+1, ack=y+1, len=m
>
> 服务端回应ACK, seq=y+1, ack=x+m+1, len=n
>
> 客户端回应ACK, seq=x+m+1, ack=y+n+1
>
> 3,断开 TCP 连接， 4 次挥手
>
> 主机 A 发送FIN, ACK, seq=x+m+1, ack=y+n+1，进入 FNI_WAIT_1 状态
>
> 主机 B 回应ACK, seq=y+n+1, ack=x+m+1，进入 CLOSE_WAIT 状态，主机 A 收到后 进入 FIN_WAIT_2 状态
>
> 主机 B 发送FIN, ACK, seq=y+n+1, ack=x+m+1，进入 LAST_ACK 状态
>
> 主机 A 回应ACk, seq=x+m+1, ack=y+n+1，进入 TIME_WAIT 状态，等待主机 B 可能要求重传 ACK 包，主机 B 收到后关闭连接，进入 CLOSED 状态或者要求主机 A 重传 ACK，客户端在一定的时间内没收到主机 B 重传 ACK 包的要求后，断开连接进入 CLOSED 状态
>
> 为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？
>
> 虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假设网络是不可靠的，一切都可能发生，比如有可能最后一个ACK丢失。所以TIME_WAIT状态是用来重发可能丢失的ACK报文。

![img](https://pic2.zhimg.com/80/v2-246cd0f85cb90edb426e1d3647deeba5_720w.webp)

客户端与服务端建立连接、传输数据和断开连接等全靠这几个标识，比如 SYN 也可以被用来作为 DOS 攻击的一个手段，FIN 可以用来扫描服务端指定端口。

## 7.HTTP 的数据结构

Socket 是 TCP/IP 的可编程 API，HTTP 的可编程 API 的实现要依赖 Socket。HTTP 是超文本传输协议，HTTP 的头和数据看起来更加直观，在大多数情况下，它们都是字符或者字符串，所以对于大多数人来说理解 HTTP 的头和数据格式显得很简单。确实，HTTP 的数据格式理解起来非常容易，上部分是头，下部分是身体。

HTTP 的请求时的数据结构和响应时的数据结构整体上是一样的，但是有一些细微的区别，我们先来看一下 HTTP 请求时的数据结构：

![img](https://pic2.zhimg.com/80/v2-eabaf2ef62bebebd61b8b4eff18eeff5_720w.webp)

HTTP 响应时的数据结构：

![img](https://pic2.zhimg.com/80/v2-8273bb87cca06d1bf9fe745897991109_720w.webp)

现在我们使用谷歌浏览器请求某度，按下Ｆ１２，来对比理解上述结构图，下面是请求某度

![img](https://pic2.zhimg.com/80/v2-173efc2744ae1de476fe58fa93dcc989_720w.webp)

我们就可以简单的理解 HTTP 的数据结构了。

## 8.Linux下的socket演示程序

下面用最基础的Socket来进行服务端与客户端的交互，让你理解的更为清晰。

**接口详解**：

|    方法名    |                     用途                      |
| :----------: | :-------------------------------------------: |
|  socket()：  |                  创建socket                   |
|   bind()：   | 绑定socket到本地地址和端口，通常由服务端调用  |
|  listen()：  |             TCP专用，开启监听模式             |
|  accept()：  |  TCP专用，服务器等待客户端连接，一般是阻塞态  |
| connect()：  |         TCP专用，客户端主动连接服务器         |
|   send()：   |               TCP专用，发送数据               |
|   recv()：   |               TCP专用，接收数据               |
|  sendto()：  |     UDP专用，发送数据到指定的IP地址和端口     |
| recvfrom()： | UDP专用，接收数据，返回数据远端的IP地址和端口 |
|  close()：   |                  关闭socket                   |

## 9.基于TCP协议实现CS端

使用Socket进行网络通信的过程

① 服务器程序将一个套接字绑定到一个特定的端口，并通过此套接字等待和监听客户的连接请求。

② 客户程序根据服务器程序所在的主机和端口号发出连接请求。

③ 如果一切正常，服务器接受连接请求。并获得一个新的绑定到不同端口地址的套接字。

④ 客户和服务器通过读、写套接字进行通讯。

![img](https://pic4.zhimg.com/80/v2-34899140a3b1f807c9a95d22d7e318d3_720w.webp)

**客户机/服务器模式**

在TCP/IP网络应用中，通信的两个进程间相互作用的主要模式是客户机/服务器模式*(client/server)，即客户像服务其提出请求，服务器接受到请求后，提供相应的服务。

服务器：

（1）首先服务器方要先启动，打开一个通信通道并告知本机，它愿意在某一地址和端口上接收客户请求

（2）等待客户请求到达该端口

（3）接收服务请求，处理该客户请求，服务完成后，关闭此新进程与客户的通信链路，并终止

（4）返回第二步，等待另一个客户请求

（5）关闭服务器

**客户方：**

（1）打开一个通信通道，并连接到服务器所在的主机特定的端口

（2）向服务器发送请求，等待并接收应答，继续提出请求

（3）请求结束后关闭通信信道并终止

具体实现，新建服务端socket_server_tcp.c

具体代码如下： socket_server_tcp.c

```
//
// Created by android on 19-8-9.
//
#include <stdio.h>
#include <string.h>
#include <arpa/inet.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <netinet/ip.h>
#define PORT 3040        //端口号
#define BACKLOG 5    //最大监听数
int main() {
    int iSocketFD = 0;  //socket句柄
    int iRecvLen = 0;   //接收成功后的返回值
    int new_fd = 0;    //建立连接后的句柄
    char buf[4096] = {0}; //
    struct sockaddr_in stLocalAddr = {0}; //本地地址信息结构图，下面有具体的属性赋值
    struct sockaddr_in stRemoteAddr = {0}; //对方地址信息
    socklen_t socklen = 0;
    iSocketFD = socket(AF_INET, SOCK_STREAM, 0); //建立socket SOCK_STREAM代表以tcp方式进行连接
    if (0 > iSocketFD) {
        printf("创建socket失败！\n");
        return 0;
    }
    stLocalAddr.sin_family = AF_INET;  /*该属性表示接收本机或其他机器传输*/
    stLocalAddr.sin_port = htons(PORT); /*端口号*/
    stLocalAddr.sin_addr.s_addr = htonl(INADDR_ANY); /*IP，括号内容表示本机IP*/
    //绑定地址结构体和socket
    if (0 > bind(iSocketFD, (void *) &stLocalAddr, sizeof(stLocalAddr))) {
        printf("绑定失败！\n");
        return 0;
    }
    //开启监听 ，第二个参数是最大监听数
    if (0 > listen(iSocketFD, BACKLOG)) {
        printf("监听失败！\n");
        return 0;
    }
    printf("iSocketFD: %d\n", iSocketFD);
    //在这里阻塞知道接收到消息，参数分别是socket句柄，接收到的地址信息以及大小 
    new_fd = accept(iSocketFD, (void *) &stRemoteAddr, &socklen);
    if (0 > new_fd) {
        printf("接收失败！\n");
        return 0;
    } else {
        printf("接收成功！\n");
        //发送内容，参数分别是连接句柄，内容，大小，其他信息（设为0即可） 
        send(new_fd, "这是服务器接收成功后发回的信息!", sizeof("这是服务器接收成功后发回的信息!"), 0);
    }
    printf("new_fd: %d\n", new_fd);
    iRecvLen = recv(new_fd, buf, sizeof(buf), 0);
    if (0 >= iRecvLen)    //对端关闭连接 返回0
    {
        printf("对端关闭连接或者接收失败！\n");
    } else {
        printf("buf: %s\n", buf);
    }
    close(new_fd);
    close(iSocketFD);
    return 0;
}
```

新建客户端端socket_client_tcp.c socket_client_tcp.c

```
//
// Created by android on 19-8-9.
//
#include <stdio.h>
#include <string.h>
#include <arpa/inet.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <netinet/ip.h>
#define PORT 3040            //目标地址端口号
#define ADDR "10.6.191.177" //目标地址IP
int main() {
    int iSocketFD = 0; //socket句柄
    unsigned int iRemoteAddr = 0;
    struct sockaddr_in stRemoteAddr = {0}; //对端，即目标地址信息
    socklen_t socklen = 0;
    char buf[4096] = {0}; //存储接收到的数据
    iSocketFD = socket(AF_INET, SOCK_STREAM, 0); //建立socket
    if (0 > iSocketFD) {
        printf("创建socket失败！\n");
        return 0;
    }
    stRemoteAddr.sin_family = AF_INET;
    stRemoteAddr.sin_port = htons(PORT);
    inet_pton(AF_INET, ADDR, &iRemoteAddr);
    stRemoteAddr.sin_addr.s_addr = iRemoteAddr;
    //连接方法： 传入句柄，目标地址，和大小
    if (0 > connect(iSocketFD, (void *) &stRemoteAddr, sizeof(stRemoteAddr))) {
        printf("连接失败！\n");
        //printf("connect failed:%d",errno);//失败时也可打印errno
    } else {
        printf("连接成功！\n");
        recv(iSocketFD, buf, sizeof(buf), 0); ////将接收数据打入buf，参数分别是句柄，储存处，最大长度，其他信息（设为0即可）。 
        printf("Received:%s\n", buf);
    }
    close(iSocketFD);//关闭socket
    return 0;
}
```

下面是我的编译及运行效果：

![img](https://pic2.zhimg.com/80/v2-921c7544e7a415e18022a60e0f631f9d_720w.webp)

编译命令如下：

```
gcc -o server socket_server_tcp.c
 gcc -o client socket_client_tcp.c
 #运行命令
  ./server  #首先启动
  ./client #次之启动
```

## 10.基于UDP协议实现CS端

**基于UDP（面向无连接）的socket编程——**数据报式套接字（SOCK_DGRAM) 网络间通信AF_INET，典型的TCP/IP四型模型的通信过程

服务器：（多线程的【每10秒会打印一行#号】 与 循环监听） socket_server_udp.c

```
//
// Created by android on 19-8-9.
//
#include <stdio.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <netinet/ip.h>
#include <pthread.h>
void * test(void *pvData)
{
    while(1)
    {
        sleep(５);
        printf("################################\n");
    }
    return NULL;
}
int main(void)
{
    pthread_t stPid = 0;
    int iRecvLen = 0;
    int iSocketFD = 0;
    char acBuf[4096] = {0};
    struct sockaddr_in stLocalAddr = {0};
    struct sockaddr_in stRemoteAddr = {0};
    socklen_t iRemoteAddrLen = 0;
    /* 创建socket */
    iSocketFD = socket(AF_INET, SOCK_DGRAM, 0);
    if(iSocketFD < 0)
    {
        printf("创建socket失败!\n");
        return 0;
    }
    /* 填写地址 */
    stLocalAddr.sin_family = AF_INET;
    stLocalAddr.sin_port   = htons(12345);
    stLocalAddr.sin_addr.s_addr = 0;
    /* 绑定地址 */
    if(0 > bind(iSocketFD, (void *)&stLocalAddr, sizeof(stLocalAddr)))
    {
        printf("绑定地址失败!\n");
        close(iSocketFD);
        return 0;
    }
    pthread_create(&stPid, NULL, test, NULL);   //实现了多线程
    while(1)     //实现了循环监听
    {
        iRecvLen = recvfrom(iSocketFD, acBuf, sizeof(acBuf), 0, (void *)&stRemoteAddr, &iRemoteAddrLen);
        printf("iRecvLen: %d\n", iRecvLen);
        printf("acBuf:%s\n", acBuf);
    }
    close(iSocketFD);
    return 0;
}
```

客户端： socket_client_udp.c

```
//
// Created by android on 19-8-9.
//
#include <stdio.h>
#include <string.h>
#include <sys/types.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <netinet/ip.h>
#include <arpa/inet.h>
int main(void)
{
    int iRecvLen = 0;
    int iSocketFD = 0;
    int iRemotAddr = 0;
    char acBuf[4096] = {0};
    struct sockaddr_in stLocalAddr = {0};
    struct sockaddr_in stRemoteAddr = {0};
    socklen_t iRemoteAddrLen = 0;
    /* 创建socket */
    iSocketFD = socket(AF_INET, SOCK_DGRAM, 0);
    if(iSocketFD < 0)
    {
        printf("创建socket失败!\n");
        return 0;
    }
    /* 填写服务端地址 */
    stLocalAddr.sin_family = AF_INET;
    stLocalAddr.sin_port   = htons(12345);
    inet_pton(AF_INET, "10.6.191.177", (void *)&iRemotAddr);
    stLocalAddr.sin_addr.s_addr = iRemotAddr;
    iRecvLen = sendto(iSocketFD, "这是一个测试字符串", strlen("这是一个测试字符串"), 0, (void *)&stLocalAddr, sizeof(stLocalAddr));
    close(iSocketFD);
    return 0;
}
```

测试：

1、编译服务器：因为有多线程，所以服务器端进程要进行pthread编译

```
gcc socket_server_udp.c -pthread -g -o server_udp #客户端和上方相同
```

执行结果如下：

右下为客户端重复执行

![img](https://pic4.zhimg.com/80/v2-13f09e9418e8595ce925753af38b78c3_720w.webp)

服务器端有主线程和辅线程，主线程，打印客户端发送的请求；辅线程每隔５秒钟打印一排#号。

原文链接：https://zhuanlan.zhihu.com/p/354316892

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.203】C++后端程序员必须彻底搞懂Nginx，从原理到实战详解

本文首先介绍 Nginx 的反向代理、负载均衡、动静分离和高可用的原理，随后详解 Nginx 的配置文件，最后通过实际案例实现 Nginx 反向代理和负载均衡的具体配置。学会 Nginx ，一篇足够了。

## 1. 简介

**Nginx** 是开源的轻量级 Web 服务器、反向代理服务器，以及负载均衡器和 HTTP 缓存器。其特点是高并发，高性能和低内存。
**Nginx** 专为性能优化而开发，性能是其最重要的考量，实现上非常注重效率，能经受高负载的考验，最大能支持 50000 个并发连接数。 Nginx 还支持热部署，它的使用特别容易，几乎可以做到 7x24 小时不间断运行。 Nginx 的网站用户有：百度、淘宝、京东、腾讯、新浪、网易等。

## 2. 反向代理

### 2.1 正向代理

**Nginx** 不仅可以做反向代理，实现负载均衡，还能用做正向代理来进行上网等功能。

![img](https://pic4.zhimg.com/80/v2-587440070f083e4c6f73c3491ce9b54f_720w.webp)

### 2.2 反向代理

客户端对代理服务器是无感知的，客户端不需要做任何配置，用户只请求反向代理服务器，反向代理服务器选择目标服务器，获取数据后再返回给客户端。反向代理服务器和目标服务器对外而言就是一个服务器，只是暴露的是代理服务器地址，而隐藏了真实服务器的IP地址。

![img](https://pic2.zhimg.com/80/v2-e97ffd9d3bc482f891e87266a5bf5ab1_720w.webp)

## 3. 负载均衡

将原先请求集中到单个服务器上的情况改为增加服务器的数量，然后将请求分发到各个服务器上，将负载分发到不同的服务器，即负载均衡。

![img](https://pic3.zhimg.com/80/v2-0a2403d192f45dfc60cc1e36a6cb03fe_720w.webp)

## 4. 动静分离

为了加快网站的解析速度，可以把静态页面和动态页面由不同的服务器来解析，加快解析速度，降低原来单个服务器的压力。

![img](https://pic1.zhimg.com/80/v2-863bdaf857975baee5c42fb31adc2fc0_720w.webp)

## 5. 高可用

为了提高系统的可用性和容错能力，可以增加nginx服务器的数量，当主服务器发生故障或宕机，备份服务器可以立即充当主服务器进行不间断工作。

![img](https://pic2.zhimg.com/80/v2-6ee22d09fe07287addc4611e96c8f659_720w.webp)

## 6. Nginx配置文件

### 6.1 文件结构

Nginx 配置文件由三部分组成。

```
...              #全局块
events {         #events块
   ...
}
http      #http块
{
    ...   #http全局块
    server        #server块
    { 
        ...       #server全局块
        location [PATTERN]   #location块
        {
            ...
        }
        location [PATTERN] 
        {
            ...
        }
    }
    server
    {
      ...
    }
    ...     #http全局块
}
```

- **第一部分 全局块**
  主要设置一些影响 nginx 服务器整体运行的配置指令。
  比如： worker_processes 1; ， worker_processes 值越大，可以支持的并发处理量就越多。
- **第二部分 events块**
  events 块涉及的指令主要影响Nginx服务器与用户的网络连接。
  比如： worker_connections 1024; ，支持的最大连接数。
- **第三部分 http块**
  http 块又包括 http 全局块和 server 块，是服务器配置中最频繁的部分，包括配置代理、缓存、日志定义等绝大多数功能。 **server块**：配置虚拟主机的相关参数。 **location块**：配置请求路由，以及各种页面的处理情况。

### 6.2 配置文件

```
########### 每个指令必须有分号结束。#################
#user administrator administrators;  #配置用户或者组，默认为nobody nobody。
#worker_processes 2;  #允许生成的进程数，默认为1
#pid /nginx/pid/nginx.pid;   #指定nginx进程运行文件存放地址
error_log log/error.log debug;  #制定日志路径，级别。这个设置可以放入全局块，http块，server块，级别以此为：debug|info|notice|warn|error|crit|alert|emerg
events {
    accept_mutex on;   #设置网路连接序列化，防止惊群现象发生，默认为on
    multi_accept on;  #设置一个进程是否同时接受多个网络连接，默认为off
    #use epoll;      #事件驱动模型，select|poll|kqueue|epoll|resig|/dev/poll|eventport
    worker_connections  1024;    #最大连接数，默认为512
}
http {
    include       mime.types;   #文件扩展名与文件类型映射表
    default_type  application/octet-stream; #默认文件类型，默认为text/plain
    #access_log off; #取消服务日志    
    log_format myFormat '$remote_addr–$remote_user [$time_local] $request $status $body_bytes_sent $http_referer $http_user_agent $http_x_forwarded_for'; #自定义格式
    access_log log/access.log myFormat;  #combined为日志格式的默认值
    sendfile on;   #允许sendfile方式传输文件，默认为off，可以在http块，server块，location块。
    sendfile_max_chunk 100k;  #每个进程每次调用传输数量不能大于设定的值，默认为0，即不设上限。
    keepalive_timeout 65;  #连接超时时间，默认为75s，可以在http，server，location块。
    upstream mysvr {   
      server 127.0.0.1:7878;
      server 192.168.10.121:3333 backup;  #热备
    }
    error_page 404 https://www.baidu.com; #错误页
    server {
        keepalive_requests 120; #单连接请求上限次数。
        listen       4545;   #监听端口
        server_name  127.0.0.1;   #监听地址       
        location  ~*^.+$ {       #请求的url过滤，正则匹配，~为区分大小写，~*为不区分大小写。
           #root path;  #根目录
           #index vv.txt;  #设置默认页
           proxy_pass  http://mysvr;  #请求转向mysvr 定义的服务器列表
           deny 127.0.0.1;  #拒绝的ip
           allow 172.18.5.54; #允许的ip           
        } 
    }
}   
```

## 7.配置实例

### 7.1 反向代理

#### 7.1.1 实战一

**实现效果：**
在浏览器输入 *[http://www.abc.com](https://link.zhihu.com/?target=http%3A//www.abc.com)* , 从 nginx 服务器跳转到 linux 系统 tomcat 主页面。
**具体配置：**

```
server {
        listen       80;   
        server_name  192.168.4.32;   #监听地址
        location  / {       
           root html;  #/html目录
           proxy_pass http://127.0.0.1:8080;  #请求转向
           index  index.html index.htm;      #设置默认页       
        } 
    }
```

#### 7.1.2 实战二

**实现效果：**
根据在浏览器输入的路径不同，跳转到不同端口的服务中。
**具体配置：**

```
server {
        listen       9000;   
        server_name  192.168.4.32;   #监听地址       
        location  ~ /example1/ {  
           proxy_pass http://127.0.0.1:5000;         
        } 
        location  ~ /example2/ {  
           proxy_pass http://127.0.0.1:8080;         
        } 
    }
```

**location** 指令说明：

- **~ :** 表示uri包含正则表达式，且区分大小写。
- **~\* :** 表示uri包含正则表达式，且不区分大小写。
- **= :** 表示uri不含正则表达式，要求严格匹配。

### 7.2 负载均衡

#### 7.2.1 实战一

**实现效果：**
在浏览器地址栏输入 *[http://192.168.4.32/example/a.html](https://link.zhihu.com/?target=http%3A//192.168.4.32/example/a.html)* ，平均到 5000 和 8080 端口中，实现负载均衡效果。
**具体配置：**

```
upstream myserver {         server 192.167.4.32:5000;      server 192.168.4.32:8080;    }    server {        listen       80;   #监听端口        server_name  192.168.4.32;   #监听地址        location  / {                  root html;  #html目录           index index.html index.htm;  #设置默认页           proxy_pass  http://myserver;  #请求转向 myserver 定义的服务器列表              }     }复制代码
```

**nginx 分配服务器策略**

- **轮询**（默认）
  按请求的时间顺序依次逐一分配，如果服务器down掉，能自动剔除。
- **权重**
  weight 越高，被分配的客户端越多，默认为 1。比如： upstream myserver { server 192.167.4.32:5000 weight=10; server 192.168.4.32:8080 weight=5; } 复制代码
- **ip**
  按请求 ip 的 hash 值分配，每个访客固定访问一个后端服务器。比如： upstream myserver { ip_hash; server 192.167.4.32:5000; server 192.168.4.32:8080; } 复制代码
- **fair**
  按后端服务器的响应时间来分配，响应时间短的优先分配到请求。比如： upstream myserver { fair; server 192.167.4.32:5000; server 192.168.4.32:8080; } 复制代码

恭喜！目前为止你已经掌握了 Nginx 的基本原理，并且能够配置反向代理和负载均衡。

原文链接：https://zhuanlan.zhihu.com/p/354598764

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.204】图文详解Linux的IO模型和相关技术

## 1.阻塞IO模型（Blocking I/O）

![img](https://pic4.zhimg.com/80/v2-449ec624b210fb47dc9e8edbc6766713_720w.webp)

Linux 内核一开始提供了 read 与 write 阻塞式操作。

- 当客户端连接时，会在对应进程的文件描述符目录（/proc/进程号/fd）生成对应的文件描述符（0 标准输入；1 标准输出；2 标准错误输出；），比如 fd 8 , fd 9；
- 应用程序需要读取的时候，通过系统调用 read (fd8)读取，如果数据还没到来，此应用程序的进程或线程会阻塞等待。

```
man 2 read
概述
     #include <unistd.h>
     ssize_t read(int fd, void *buf, size_t count);
描述
     read() 从文件描述符 fd 中读取 count 字节的数据并放入从 buf 开始的缓冲区中.
     如果 count 为零,read()返回0,不执行其他任何操作. 如果 count 大于SSIZE_MAX,那么结果将不可预料.
返回值
     成功时返回读取到的字节数(为零表示读到文件描述符), 此返回值受文件剩余字节数限制.当返回值小于指定的字节数时 并不意味着错误;这可能是因为当前可读取的字节数小于指定的 字节数(比如已经接近文件结尾,或
     者正在从管道或者终端读取数 据,或者 read()被信号中断). 发生错误时返回-1,并置 errno 为相应值.在这种情况下无法得知文件偏移位置是否有变化.
```

## 2.问题

如果出现了很多的客户端连接，比如1000个，那么应用程序就会启用1000个进程或线程阻塞等待。此时会出现性能问题：

- CPU 会不停的切换，造成进程或线程上下文切换开销，实际读取IO的时间占比会下降，造成**CPU算力浪费**。 因此，推动了 non-blocking I/O 的诞生。

## 3.非阻塞IO模型（non-blocking I/O）

![img](https://pic4.zhimg.com/80/v2-c17b56b9412def02a32b4117396c61b3_720w.webp)

此时，Linux 内核一开始提供了 read 与 write 非阻塞式操作，可以通过socket设置SOCK_NONBLOCK标记 。

- 此时应用程序就不需要每一个文件描述符一个线程去处理，可以只有一个线程不停轮询去读取read，如果没有数据到来，也会直接返回。
- 如果有数据，则可以调度去处理业务逻辑。

```
Since  Linux  2.6.27, the type argument serves a second purpose: in addition to specifying a socket type, it may include the bitwise OR of any of the following values, to modify the behavior of
       socket():
       SOCK_NONBLOCK   Set the O_NONBLOCK file status flag on the open file description (see open(2)) referred to by the new file descriptor.  Using this flag saves extra calls to fcntl(2) to  achieve
                       the same result.
```

从这里可以看出来 socket Linux 2.6.27内核开始支持非阻塞模式。

**问题**

同理，当出现了很多的客户端连接，比如1000个，那就会触发1000次**系统调用**。（1000次系统调用开销也很客观）

因此，有了 select。

## 4.IO复用模型（I/O multiplexing） - select

![img](https://pic3.zhimg.com/80/v2-4f594168cfcfdd76df2de8fadae86fda_720w.webp)

此时，Linux 内核一开始提供了 select 操作，可以把1000次的系统调用，简化为一次系统调用，轮询发生在内核空间。

- select系统调用会返回可用的 fd集合，应用程序此时只需要遍历可用的 fd 集合， 去读取数据进行业务处理即可。man 2 select复制代码

```
SYNOPSIS
       #include <sys/select.h>
       int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
DESCRIPTION
       select() allows a program to monitor multiple file descriptors, waiting until one or more of the file descriptors become "ready" for some class of I/O operation (e.g., input possible). A file
       descriptor is considered ready if it is possible to perform a corresponding I/O operation (e.g., read(2), or a sufficiently small write(2)) without blocking.
       select() can monitor only file descriptors numbers that are less than FD_SETSIZE; poll(2) and epoll(7) do not have this limitation. See BUGS.
```

可以看到支持传输多个文件描述符交由内核轮询。

## 5.问题

虽然从1000次系统调用，降为一次系统调用的开销，但是系统调用开销中需要传参1000个文件描述符。这也会造成一定的内存开销。

因此，有了 epoll。

```
select() can monitor only file descriptors numbers that are less than FD_SETSIZE; poll(2) and epoll(7) do not have this limitation. See BUGS.
```

## 6.IO复用模型（I/O multiplexing） - epoll

![img](https://pic3.zhimg.com/80/v2-1345ab8e9d12a20f51f0a8f9590a0a92_720w.webp)

```
man epoll
man 2 epoll_create
man 2 epoll_ctl
man 2 epoll_wait
```

- epoll：

```
SYNOPSIS
       #include <sys/epoll.h>
DESCRIPTION
       The  epoll  API  performs  a  similar task to poll(2): monitoring multiple file descriptors to see if I/O is possible on any of them.  The epoll API can be used either as an edge-triggered or a
       level-triggered interface and scales well to large numbers of watched file descriptors.
       The central concept of the epoll API is the epoll instance, an in-kernel data structure which, from a user-space perspective, can be considered as a container for two lists:
       • The interest list (sometimes also called the epoll set): the set of file descriptors that the process has registered an interest in monitoring.
       • The ready list: the set of file descriptors that are "ready" for I/O.  The ready list is a subset of (or, more precisely, a set of references to) the file descriptors in  the  interest  list.
         The ready list is dynamically populated by the kernel as a result of I/O activity on those file descriptors.
```

- epoll_create ：

内核会产生一个epoll 实例数据结构并返回一个文件描述符epfd。

- epoll_ctl ：

对文件描述符 fd 和 其监听事件 epoll_event 进行注册，删除，或者修改其监听事件 epoll_event 。

```
SYNOPSIS
       #include <sys/epoll.h>
       int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
DESCRIPTION
       This system call is used to add, modify, or remove entries in the interest list of the epoll(7) instance referred to by the file descriptor epfd. It requests that the operation op be performed
       for the target file descriptor, fd.
       Valid values for the op argument are:
       EPOLL_CTL_ADD
              Add an entry to the interest list of the epoll file descriptor, epfd. The entry includes the file descriptor, fd, a reference to the corresponding open file description (see epoll(7)
              and open(2)), and the settings specified in event.
       EPOLL_CTL_MOD
              Change the settings associated with fd in the interest list to the new settings specified in event.
       EPOLL_CTL_DEL
          Remove (deregister) the target file descriptor fd from the interest list. The event argument is ignored and can be NULL (but see BUGS below).
```

- epoll_wait ：

阻塞等待注册的事件发生，返回事件的数目，并将触发的可用事件写入epoll_events数组中。

## 7.其他IO优化技术

```
man 2 mmap
man 2 sendfile
man 2 fork
```

## 8.mmap：

就是在用户的虚拟地址空间中寻找空闲的一段地址进行对文件的操作，不必再调用read、write系统调用，它的最终目的是将磁盘中的文件映射到用户进程的虚拟地址空间，实现用户进程对文件的直接读写，减少了文件复制的开销，提高了用户的访问效率。

以读为例：

![img](https://pic4.zhimg.com/80/v2-ab3c4b0d055c6e20b65e6f01aa7beb9b_720w.webp)

- 使用场景

kafka的数据文件就是用的mmap，写入文件，可以不经过用户空间到内核的拷贝，直接内核空间落盘。

再比如Java中的MappedByteBuffer底层在Linux就是mmap。

**sendfile：**

![img](https://pic3.zhimg.com/80/v2-814f6e1929983a8d521bd23c85f3c636_720w.webp)

sendfile系统调用在两个文件描述符之间直接传递数据(完全在内核中操作)，从而避免了数据在内核缓冲区和用户缓冲区之间的拷贝，操作效率很高，被称之为零拷贝。

- 使用场景

比如 kafka，消费者进行消费时，kafka直接调用 sendfile（Java中的FileChannel.transferTo），实现内核数据从内存或数据文件中读出，直接发送到网卡，而不需要经过用户空间的两次拷贝，实现了所谓”零拷贝”。

再比如Tomcat、Nginx、Apache等web服务器返回静态资源等，将数据用网络发送出去，都运用了sendfile。

## 9.fork

```
man 2 fork
```

创建子进程有三种方式：

- fork，调用后，子进程有自己的pid和task_struct结构，基于父进程的所有数据资源进行副本拷贝，主要是复制自己的指针，并不会复制父进程的虚存空间，并且父子进程同时进行，变量互相隔离，互不干扰。

现在Linux中是采取了Copy-On-Write(COW，**写时复制**)技术，为了降低开销，fork最初并不会真的产生两个不同的拷贝，因为在那个时候，大量的数据其实完全是一样的。 写时复制是在推迟真正的数据拷贝。若后来确实发生了写入，那意味着父进程和子进程的数据不一致了，于是产生复制动作，每个进程拿到属于自己的那一份，这样就可以降低系统调用的开销。

```
NOTES
       Under  Linux,  fork()  is implemented using copy-on-write pages, so the only penalty that it incurs is the time and memory required to duplicate the parent's page tables, and to create a unique
       task structure for the child.
```

- vfork，vfork系统调用不同于fork，用vfork创建的子进程与父进程共享地址空间，也就是说子进程完全运行在父进程的地址空间上，也就是子进程对虚拟地址空间任何数据的修改同样为父进程所见。并且vfork完子进程，父进程是阻塞等待子进程结束才会继续。
- clone，可以认为是fork 与 vfork的混合用法。由用户通过参clone_flags 的设置来决定哪些资源共享，哪些资源副本拷贝。 由标志CLONE_VFORK来决定子进程在执行时父进程是阻塞还是运行，若没有设置该标志，则父子进程同时运行，设置了该标志，则父进程挂起，直到子进程结束为止。
- 总结fork的用途 一个进程希望对自身进行副本拷贝，从而父子进程能同时执行不同段的代码。 比如 redis的RDB持久化就是采用的就是fork，保证副本拷贝的时点准确，并且速度快，不影响父进程继续提供服务。vfork的用途 用vfork创建的进程主要目的是用exec函数先执行另外的程序。clone的用途 用于有选择地设置父子进程之间哪些资源需要共享，哪些资源需要副本拷贝。

原文链接：https://zhuanlan.zhihu.com/p/355781849

作者：Hu先生的Linux

# 【NO.205】详解从网络IO到IO多路复用

## 1.前言

这篇文章，会重点介绍linux的BIO、NIO和IO多路复用。

## 2.Netcat软件的基本使用

Netcat（简写nc）是一个强大的网络命令工具，能够在linux中执行与TCP、UDP相关的操作，例如端口扫描，端口重定向、端口监听甚至远程连接

在这里，我们使用 nc 来模拟一台接收message的服务器，和一台发送message的客户端

1、安装 nc 软件

```
sudo yum install -y nc
```

2、使用 nc 创建一台监听9999端口的服务器

```
nc -l -p 9999     # -l表示listening，监听
```

启动成功后 nc 进行阻塞
3、新建一个bash，使用 nc 创建一个发送message的客户端

```
nc localhost 9999
```

在控制台上输入要发送的信息，查看服务端是否接收到
4、查看上面的nc进程中的文件描述符

```
ps -ef | grep nc  # 查看nc的进程号，这里假设是2603ls /proc/2603/fd  # 查看2603进程下的文件描述符
```

![img](https://pic4.zhimg.com/80/v2-5c08411a05b47338cef4612ad0cb36db_720w.webp)

可以看到这个进程下有一个socket，这就是nc的客户端和服务端之间创建的一个socket
经过这一系列的操作，相信我们对Netcat软件有了基本的了解，下面来介绍BIO

## 3.strace追踪系统调用

**strace软件说明：** 它是一个可以追踪系统调用和信号的软件，通过它我们来了解BIO

**环境说明：** 这里演示的都是基于老版本的linux，因为新版本的linux都不用BIO了，演示不出来

1、使用strace来追踪系统调用

```
sudo yum install -y strace              # 安装strace软件mkdir ~/strace                          # 新建一个目录，存放追踪的信息cd ~/strace                             # 进入到这个目录strace -ff -o out nc -l -p 8080   # 使用strace追踪后边的命令进行的系统调用                                  # -ff 表示追踪后面命令创建的进程及子进程的所有系统调用，                                  # 并根据进程id号分开输出到文件                                  # -o  表示追踪到的信息输出到指定名称的文件，这里是out
```

2、查看服务端创建的系统调用

在上一步进入的目录下，出现了一个 out.pid 文件，里的内容都是 nc -l -p 9999 这个命令执行后的系统调用过程，使用vim命令来查看

```
vim out.92459    # nc进程id为92459
```

![img](https://pic4.zhimg.com/80/v2-0a2ae8886fa5f866c8a73b5599a2fb5b_720w.webp)

这里accept()方法进行了**阻塞**，它要等待其他socket对它进行连接

3、客户端连接，查看系统调用

退出vim，使用tail来进行查看

```
tail -f out.92459
```

-f 参数：当文件有追加的内容，可以实时地打印在控制台，这样就能很方便来查看客户端连接后进行的系统调用

```
nc localhost 8080
```

查看系统调用

![img](https://pic2.zhimg.com/80/v2-146235fda1321b3126a4fb294d243cf1_720w.webp)

- 这里客户端连接后，accept() 方法获取到客户端连接并返回文件描述符4，这个4就是服务端新创建的socket，用于和这个客户端进行通信
- 之后使用多路复用器poll来监听服务端上文件描述符4和0，0是标准输入文件描述符，哪个有事件发生就读取哪个文件描述符，如果都没有事件发生就进行堵塞

4、客户端发送message，查看系统调用

![img](https://pic1.zhimg.com/80/v2-d5ca3d6c0767fef34bcdb35ef5442274_720w.webp)

客户端向服务端发送数据，服务端就能从socket中监听到有事件发生，就能进行相应的处理，处理完继续堵塞，等待下一个事件发生

5、服务端发送数据到客户端，查看系统调用

![img](https://pic1.zhimg.com/80/v2-5953662de6fd22abf7be4d38d07b0c38_720w.webp)

服务端发数据，肯定从键盘输入，也就是标准输入0，从0中读取到数据发送给socket 4

## 4.BIO（阻塞式IO）

在我们第三节中，我们使用 strace 工具查看了 nc 软件使用过程中的系统调用，其实上一节中体现的就是BIO，我们把上面的一系列系统调用总结一下，根据直观的理解BIO

1、单线程模式

1.1、过程演示

1、服务端启动

![img](https://pic1.zhimg.com/80/v2-eb9f0ab1fd7d34ca21f3081ad62c1ee0_720w.webp)

启动服务端，等待socket连接，accept()方法阻塞
2、客户端连接,未发送数据

![img](https://pic2.zhimg.com/80/v2-bc319d7d1d5883cac82e8e952d936b2d_720w.webp)

连接客户端，accept() 方法执行，未收到client1发送的数据，read()方法阻塞
3、另一个客户端连接

![img](https://pic3.zhimg.com/80/v2-5903f2d54f065046e5f48d2847fabfae_720w.webp)

由于read()方法阻塞，无法执行到accept()方法，所以这样cpu一次只能处理一个socket

1.2、存在的问题

上面的模型存在很大的问题，如果客户端与服务端建立了连接，客户端迟迟不发数据，进程就会一直堵塞在read()方法上，这样其他客户端也不能进行连接，也就是一次只能处理一个客户端，对客户很不友好

1.3、如何解决

其实要解决这个问题很简单，利用多线程就可以，只要连接了一个socket，操作系统分配一个线程来处理，这样read()方法堵塞在每个线程上，不堵塞主线程，就能操作多个socket了，有哪个线程中的socket有数据，就读哪个socket

2、多线程模式

1.1 过程演示

![img](https://pic4.zhimg.com/80/v2-d0ec08275964e6f1d84bf134bda5e4cf_720w.webp)

- 程序服务端只负责监听是否有客户端连接，使用 accept() 阻塞
- 客户端1连接服务端，就开辟一个线程（thread1）来执行 read() 方法，程序服务端继续监听
- 客户端2连接服务端，也开辟一个线程，执行read()方法
- 任何一个线程上的socket有数据发送过来，read()就能立马读到，cpu就能进行处理

1.2、存在的问题

上面这个多线程模型，看似已经十分的完美，其实也有很大的问题。每来一个客户端，就要开辟一个线程，如果来1万个客户端，那就要开辟1万个线程。在操作系统中，用户态不能直接开辟线程，需要调用cpu的80软中断，让内核来创建的一个线程，这其中还涉及到用户状态的切换（上下文的切换），十分耗资源。

1.3、如何解决

第一个办法：使用线程池，这个在客户端连接少的情况下可以使用，但是用户量大的情况下，你不知道线程池要多大，太大了内存可能不够，也不可行
第二个办法：因为read()方法堵塞了，所有要开辟多个线程，如果什么方法能使read()方法不堵塞，这样就不用开辟多个线程了，这就用到了另一个IO模型，NIO（非阻塞式IO）

## 5.NIO（非阻塞式IO）

### 5.1.过程演示

1、服务端刚创建，没有客户端连接

![img](https://pic4.zhimg.com/80/v2-b0f6c03e8f6f0bf0ef3562d96bbe8c9f_720w.webp)

在NIO中，accept()方法也是非阻塞的，它在一个while死循环中
2、当有一个客户端进行连接时

![img](https://pic4.zhimg.com/80/v2-07459a0889c0f129265d7df0909a085b_720w.webp)

3、当有第二个客户端进行连接时

![img](https://pic2.zhimg.com/80/v2-76357b06bd9b438bedc87588537b03bd_720w.webp)

### 5.2.总结

**在NIO模式中，一切都是非阻塞的：**

- accept()方法是非阻塞的，如果没有客户端连接，就返回error
- read()方法是非阻塞的，如果read()方法读取不到数据就返回error，如果读取到数据时只阻塞read()方法读数据的时间

**在NIO模式中，只有一个线程：**

- 当一个客户端与服务端进行连接，这个socket就会加入到一个数组中，隔一段时间遍历一次，看这个socket的read()方法能否读到数据
- 这样一个线程就能处理多个客户端的连接和读取了

### 5.3.存在的问题

NIO成功的解决了BIO需要开启多线程的问题，NIO中一个线程就能解决多个socket，看似已经 perfect，但是还存在问题。
这个模型在客户端少的时候十分好用，但是客户端如果很多，比如有1万个客户端进行连接，那么每次循环就要遍历1万个socket，如果一万个socket中只有10个socket有数据，也会变量一万个socket，就会做很多无用功。而且这个遍历过程是在用户态进行的，用户态判断socket是否有数据还是调用内核的read()方法实现的，这就涉及到用户态和内核态的切换，每遍历一个就要切换一次，开销很大
因为这些问题的存在，IO多路复用应运而生

## 6.IO Multiplexing（IO多路复用）

IO多路复用有三种实现方式，select、poll、epoll，现在让我们来看看这三种实现的真面目吧

### 6.1select

![img](https://pic4.zhimg.com/80/v2-aaadcec4d7860dc79f067813846d1a7f_720w.webp)

这里还有select代码实现的代码例子

![img](https://pic2.zhimg.com/80/v2-67d68c53a4caff2b666eda2c428506c1_720w.webp)

1.1 优点

select 其实就是把NIO中用户态要遍历的 fd 数组拷贝到了内核态，让内核态来遍历，因为用户态判断socket是否有数据还是要调用内核态的，所有拷贝到内核态后，这样遍历判断的时候就不用一直用户态和内核态频繁切换了
从代码中可以看出，select系统调用后，返回了一个置位后的&rset，这样用户态只需进行很简单的二进制比较，就能很快知道哪些socket需要read数据，有效提高了效率

1.2 存在的问题

1、bitmap最大1024位，一个进程最多只能处理1024个客户端
2、&rset不可重用，每次socket有数据就相应的位会被置位
3、文件描述符数组拷贝到了内核态，仍然有开销
4、select并没有通知用户态哪一个socket有数据，仍然需要O(n)的遍历

### 6.2.poll

2.1 代码例子

![img](https://pic2.zhimg.com/80/v2-5317298c00307900115412c5bbf19115_720w.webp)

在poll中，文件描述符有一份独立的数据结构pollfd，传入poll中的是pollfd的数组，其他的实现逻辑和select一样

2.2 优点

1、poll使用pollfd数组来代替select中的bitmap，数组没有1024的限制，可以一次管理更多的client
2、当pollfds数组中有事件发生，相应的revents置位为1，遍历的时候又置位回0，实现了pollfd数组的重用

2.3 缺点

poll 解决了select缺点中的前两条，其本质原理还是select的方法，还存在select中原来的问题
1、pollfds数组拷贝到了内核态，仍然有开销
2、poll并没有通知用户态哪一个socket有数据，仍然需要O(n)的遍历

### 6.3.epoll

3.1 代码例子

![img](https://pic1.zhimg.com/80/v2-e2628c6c8ca5105a063effa1080b239c_720w.webp)

3.2 事件通知机制

1、当有网卡上有数据到达了，首先会放到DMA（内存中的一个buffer，网卡可以直接访问这个数据区域）中
2、网卡向cpu发起中断，让cpu先处理网卡的事
3、中断号在内存中会绑定一个回调，哪个socket中有数据，回调函数就把哪个socket放入就绪链表中

3.3 详细过程

首先epoll_create创建epoll实例，它会创建所需要的红黑树，以及就绪链表，以及代表epoll实例的文件句柄，其实就是在内核开辟一块内存空间，所有与服务器连接的socket都会放到这块空间中，这些socket以红黑树的形式存在，同时还会有一块空间存放就绪链表；红黑树存储所监控的文件描述符的节点数据，就绪链表存储就绪的文件描述符的节点数据；
epoll_ctl添加新的描述符，首先判断是红黑树上是否有此文件描述符节点，如果有，则立即返回。如果没有， 则在树干上插入新的节点，并且告知内核注册回调函数。当接收到某个文件描述符过来数据时，那么内核将该节点插入到就绪链表里面。
epoll_wait将会接收到消息，并且将数据拷贝到用户空间，清空链表。

3.4 水平触发和边沿触发

Level_triggered(水平触发)：当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据一次性全部读写完(如读写缓冲区太小)，那么下次调用 epoll_wait()时，它还会通知你在上没读写完的文件描述符上继续读写，当然如果你一直不去读写，它会一直通知你！！！如果系统中有大量你不需要读写的就绪文件描述符，而它们每次都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率！！！
Edge_triggered(边缘触发)：当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用epoll_wait()时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你！！！这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符！！！

3.5 优点

epoll是现在最先进的IO多路复用器，Redis、Nginx，linux中的Java NIO都使用的是epoll
1、一个socket的生命周期中只有一次从用户态拷贝到内核态的过程，开销小
2、使用event事件通知机制，每次socket中有数据会主动通知内核，并加入到就绪链表中，不需要遍历所有的socket

原文链接：https://zhuanlan.zhihu.com/p/354937550

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)



# 【NO.206】C++后端程序员必须彻底搞懂Nginx，从原理到实战（高级篇）

本文为 Nginx 实操高级篇。通过配置 Nginx 配置文件，实现正向代理、反向代理、负载均衡、Nginx 缓存、动静分离和高可用 Nginx 6种功能，并对 Nginx 的原理作进一步的解析。当需要使用 Nginx 配置文件时，参考本文实例即可，建议收藏。

## 1. 正向代理

正向代理的代理对象是客户端。正向代理就是代理服务器替客户端去访问目标服务器。

### 1.1. 实战一

**实现效果：**
在浏览器输入 *[http://www.google.com](https://link.zhihu.com/?target=http%3A//www.google.com)* , 浏览器跳转到*[www.google.com](http://www.google.com/)* 。
**具体配置：**

```
server{
    resolver 8.8.8.8;
    listen 80;
    location / {
        proxy_pass http://$http_host$request_uri;
    }
}
```

在需要访问外网的客户端上执行以下一种操作即可：

```
1. 方法1（推荐）
export http_proxy=http://你的正向代理服务器地址：代理端口   
2. 方法2
vim ~/.bashrc
export http_proxy=http://你的正向代理服务器地址：代理端口
```

## 2. 反向代理

反向代理指代理后端服务器响应客户端请求的一个中介服务器，代理的对象是服务端。

### 2.1 .实战一

**实现效果：**
在浏览器输入 *[http://www.abc.com](https://link.zhihu.com/?target=http%3A//www.abc.com)* , 从 nginx 服务器跳转到 linux 系统 tomcat 主页面。
**具体配置：**

```
server {
        listen       80;   
        server_name  192.168.4.32;   #监听地址
        location  / {       
           root html;  #/html目录
           proxy_pass http://127.0.0.1:8080;  #请求转向
           index  index.html index.htm;      #设置默认页       
        } 
    }
```

### 2.2. 实战二

**实现效果：**
根据在浏览器输入的路径不同，跳转到不同端口的服务中。
**具体配置：**

```
server {
        listen       9000;   
        server_name  192.168.4.32;   #监听地址       
        location  ~ /example1/ {  
           proxy_pass http://127.0.0.1:5000;         
        } 
        location  ~ /example2/ {  
           proxy_pass http://127.0.0.1:8080;         
        } 
    }
```

**location** 指令说明：

- **~ :** 表示uri包含正则表达式，且区分大小写。
- **~\* :** 表示uri包含正则表达式，且不区分大小写。
- **= :** 表示uri不含正则表达式，要求严格匹配。

## 3 负载均衡

### 3.1. 实战一

**实现效果：**
在浏览器地址栏输入 *[http://192.168.4.32/example/a.html](https://link.zhihu.com/?target=http%3A//192.168.4.32/example/a.html)* ，平均到 5000 和 8080 端口中，实现负载均衡效果。
**具体配置：**

```
upstream myserver {   
      server 192.167.4.32:5000;
      server 192.168.4.32:8080;
    }
    server {
        listen       80;   #监听端口
        server_name  192.168.4.32;   #监听地址
        location  / {       
           root html;  #html目录
           index index.html index.htm;  #设置默认页
           proxy_pass  http://myserver;  #请求转向 myserver 定义的服务器列表      
        } 
    }
```

**nginx 分配服务器策略**

- **轮询**（默认）
  按请求的时间顺序依次逐一分配，如果服务器down掉，能自动剔除。
- **权重**
  weight 越高，被分配的客户端越多，默认为 1。比如： upstream myserver { server 192.167.4.32:5000 weight=10; server 192.168.4.32:8080 weight=5; } 复制代码
- **ip**
  按请求 ip 的 hash 值分配，每个访客固定访问一个后端服务器。比如： upstream myserver { ip_hash; server 192.167.4.32:5000; server 192.168.4.32:8080; } 复制代码
- **fair**
  按后端服务器的响应时间来分配，响应时间短的优先分配到请求。比如： upstream myserver { fair; server 192.168.4.32:5000; server 192.168.4.32:8080; } 复制代码

## 4. Nginx 缓存

### 4.1. 实战一

**实现效果：**
在3天内，通过浏览器地址栏访问 *[http://192.168.4.32/a.jpg](https://link.zhihu.com/?target=http%3A//192.168.4.32/a.jpg)* ，不会从服务器抓取资源，3天后（过期）则从服务器重新下载。
**具体配置：**

```
# http 区域下添加缓存区配置
proxy_cache_path /tmp/nginx_proxy_cache levels=1 keys_zone=cache_one:512m inactive=60s max_size=1000m;
# server 区域下添加缓存配置
location ~ \.(gif|jpg|png|htm|html|css|js)(.*) {
     proxy_pass http://192.168.4.32:5000；#如果没有缓存则转向请求
     proxy_redirect off;
     proxy_cache cache_one;
     proxy_cache_valid 200 1h;            #对不同的 HTTP 状态码设置不同的缓存时间
     proxy_cache_valid 500 1d;
     proxy_cache_valid any 1m;
     expires 3d;
}
```

**expires** 是给一个资源设定一个过期时间，通过 expires 参数设置，可以使浏览器缓存过期时间之前的内容，减少与服务器之间的请求和流量。也就是说无需去服务端验证，直接通过浏览器自身确认是否过期即可，所以不会产生额外的流量。此种方法非常适合不经常变动的资源。

## 5. 动静分离

### 5.1. 实战一

**实现效果：**
通过浏览器地址栏访问 *[http://www.abc.com/a.html](https://link.zhihu.com/?target=http%3A//www.abc.com/a.html)* ，访问静态资源服务器的静态资源内容。通过浏览器地址栏访问 *[http://www.abc.com/a.jsp](https://link.zhihu.com/?target=http%3A//www.abc.com/a.jsp)* ，访问动态资源服务器的动态资源内容。
**具体配置：**

```
upstream static {   
    server 192.167.4.31:80;
}
upstream dynamic {   
    server 192.167.4.32:8080;
}
server {
    listen       80;   #监听端口
    server_name  www.abc.com; 监听地址
    # 拦截动态资源
    location ~ .*\.(php|jsp)$ {
       proxy_pass http://dynamic;
    }
    # 拦截静态资源
    location ~ .*\.(jpg|png|htm|html|css|js)$ {       
       root /data/;  #html目录
       proxy_pass http://static;
       autoindex on;;  #自动打开文件列表
    }  
}
```

## 6. 高可用

一般情况下，通过 nginx 主服务器访问后台目标服务集群，当主服务器挂掉后，自动切换至备份服务器，此时由备份服务器充当主服务器的角色，访问后端目标服务器。

### 6.1 .实战一

**实现效果：**
准备两台 nginx 服务器，通过浏览器地址栏访问虚拟 ip 地址，把主服务器的 nginx 停止，再次访问虚拟 ip 地址仍旧有效。
**具体配置：**
（1）在两台 nginx 服务器上安 keepalived。
keepalived 相当于一个路由，它通过一个脚本来检测当前服务器是否还活着，如果还活着则继续访问，否则就切换到另一台备份服务器。

```
# 安装 keepalived
yum install keepalived -y
# 检查版本
rpm -q -a keepalived
keepalived-1.3.5-16.el7.x86_64
```

（2）修改主备服务器 /etc/keepalived/keepalivec.conf 配置文件（可直接替换），完成高可用主从配置。
keepalived 将 nginx 服务器绑定到一个虚拟 ip ， nginx 高可用集群对外统一暴露这个虚拟 ip，客户端都是通过访问这个虚拟 ip 来访问 nginx 服务器 。

```
global_defs {
    notification_email {
        acassen@firewall.loc
        failover@firewall.loc
        sysadmin@firewall.loc
    }
    notification_email_from_Alexandre.Cassen@firewall.loc
    smtp_server 192.168.4.32
    smtp_connect_timeout 30
    router_id LVS_DEVEL  # 在 /etc/hosts 文件中配置，通过它能访问到我们的主机
}
vrrp_script_chk_http_port {   
    script "/usr/local/src/nginx_check.sh"
    interval 2      # 检测脚本执行的时间间隔
    weight 2        # 权重每次加2
}
vrrp_instance VI_1 {
    interface ens7f0 # 网卡，需根据情况修改
    state MASTER    # 备份服务器上将 MASTER 改为 BACKUP
    virtual_router_id 51 # 主备机的 virtual_router_id 必须相同
    priority 100   # 主备机取不同的优先级，主机值较大，备份机值较小
    advert_int 1  # 每隔多长时间（默认1s）发送一次心跳，检测服务器是否还活着
    authentication {
      auth_type PASS
      auth_pass 1111
    }
    virtual_ipaddress {
        192.168.1.100 # VRRP H 虚拟地址，可以绑定多个
    }
}
```

**字段说明**

- router_id： 在 /etc/hosts 文件中配置，通过它能访问到我们的主机。 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 127.0.0.1 LVS_DEVEL 复制代码
- interval： 设置脚本执行的间隔时间
- weight： 当脚本执行失败即 keepalived 或 nginx 挂掉时，权重增加的值（可为负数）。
- interface： 输入 ifconfig 命令查看当前的网卡名是什么。 ens7f0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 192.168.4.32 netmask 255.255.252.0 broadcast 192.168.7.255 inet6 fe80::e273:9c3c:e675:7c60 prefixlen 64 scopeid 0x20 … … 复制代码

（3）在 /usr/local/src 目录下添加检测脚本 nginx_check.sh。

```
#!/bin/bash
A=`ps -C nginx -no-header |wc -l`
if [ $A -eq 0 ];then
    /usr/local/nginx/sbin/nginx
    sleep 2
    if [ ps -C nginx -no-header |wc -l` -eq 0 ];then
        killall keepalived
    fi
fi
```

（4）启动两台服务器的 nginx 和 keepalived。

```
# 启动 nginx
./nginx
# 启动 keepalived
systemctl start keepalived.service
```

（5）查看虚拟 ip 地址 ip a 。把主服务器 192.168.4.32 nginx 和 keepalived停止，再访问虚拟 ip 查看高可用效果。

## 7. 原理解析

![img](https://pic2.zhimg.com/80/v2-3856323981ade758ea0aba92d26c0c01_720w.webp)

Nginx 启动之后，在 Linux 系统中有两个进程，一个为 master，一个为 worker。master 作为管理员不参与任何工作，只负责给多个 worker 分配不同的任务（worker 一般有多个）。

```
ps -ef |grep nginx
root     20473     1  0  2019 ?        00:00:00 nginx: master process /usr/sbin/nginx
nginx     4628 20473  0 Jan06 ?        00:00:00 nginx: worker process
nginx     4629 20473  0 Jan06 ?        00:00:00 nginx: worker process
```

**worker 是如何工作的？**
客户端发送一个请求首先要经过 master，管理员收到请求后会将请求通知给 worker，多个 worker 以**争抢**的机制来抢夺任务，得到任务的 worker 会将请求经由 tomcat 等做请求转发、反向代理、访问数据库等（nginx 本身是不直接支持 java 的）。

![img](https://pic2.zhimg.com/80/v2-7bcd042eb50b14229f52738c472a7685_720w.webp)

**一个 master 和多个 worker 的好处？**

- 可以使用 nginx -s reload 进行热部署。
- 每个 worker 是独立的进程，如果其中一个 worker 出现问题，其它 worker 是独立运行的，会继续争抢任务，实现客户端的请求过程，而不会造成服务中断。

**设置多少个 worker 合适？**
Nginx 和 redis 类似，都采用了 io 多路复用机制，每个 worker 都是一个独立的进程，每个进程里只有一个主线程，通过异步非阻塞的方式来处理请求，每个 worker 的线程可以把一个 cpu 的性能发挥到极致，因此，**worker 数和服务器的 cpu 数相等是最为适宜的**。

**思考：**
（1）发送一个请求，会占用 worker 几个连接数？
（2）有一个 master 和 4个 worker，每个 worker 支持的最大连接数为 1024，该系统支持的最大并发数是多少？

恭喜！目前为止你已经掌握了 Nginx 6种功能的配置方式，并和我一起进一步探讨了 Nginx 的原理。最后两个面试中可能会问到的思考题，欢迎大家评论区积极讨论。如果本文对你有所帮助，点赞互相鼓励一下吧~

原文链接：https://zhuanlan.zhihu.com/p/356100901

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.207】详解内存池设计与实现

## 1.前言

作为C++程序员，想必对于内存操作这一块是比较熟悉和操作比较频繁的；

比如申请一个对象，使用new，申请一块内存使用malloc等等；

但是，往往会有一些困扰烦恼着大家，主要体现在两部分：

- 申请内存后忘记释放，造成内存泄漏
- 内存不能循环使用，造成大量内存碎片

这两个原因会影响我们程序长期平稳的运行，也有可能会导致程序的崩溃；

## 2.内存池

> 内存池是池化技术中的一种形式。通常我们在编写程序的时候回使用 new delete 这些关键字来向操作系统申请内存，而这样造成的后果就是每次申请内存和释放内存的时候，都需要和操作系统的系统调用打交道，从堆中分配所需的内存。如果这样的操作太过频繁，就会找成大量的内存碎片进而降低内存的分配性能，甚至出现内存分配失败的情况。
> 而内存池就是为了解决这个问题而产生的一种技术。从内存分配的概念上看，内存申请无非就是向内存分配方索要一个指针，当向操作系统申请内存时，操作系统需要进行复杂的内存管理调度之后，才能正确的分配出一个相应的指针。而这个分配的过程中，我们还面临着分配失败的风险。
> 所以，每一次进行内存分配，就会消耗一次分配内存的时间，设这个时间为 T，那么进行 n 次分配总共消耗的时间就是 nT；如果我们一开始就确定好我们可能需要多少内存，那么在最初的时候就分配好这样的一块内存区域，当我们需要内存的时候，直接从这块已经分配好的内存中使用即可，那么总共需要的分配时间仅仅只有 T。当 n 越大时，节约的时间就越多。

## 3.内存池设计

![img](https://pic3.zhimg.com/80/v2-589698ace8dbf7801cb79a2c21a41762_720w.webp)

内存池设计实现中主要分为以下几部分：

- 重载new
- 创建内存节点
- 创建内存池
- 管理内存池

下面，比较详细的来说说设计细节：

重载new就不说了，直接从内存节点开始；

> 内存池节点

内存池节点需要包含以下几点元素：

1. 所属池子（pMem），因为后续在内存池管理中可以直接调用申请内存和释放内存
2. 下一个节点（pNext），这里主要是使用链表的思路，将所有的内存块关联起来；
3. 节点是否被使用（bUsed），这里保证每次使用前，该节点是没有被使用的；
4. 是否属于内存池（bBelong），主要是一般内存池维护的空间都不是特别大，但是用户申请了特别大的内存时，就走正常的申请流程，释放时也就正常释放；

> 内存池设计

内存池设计就是上面的图片类似，主要包含以下几点元素：

1. 内存首地址（_pBuffer），也就是第一块内存，这样以后方面寻找后面的内存块；
2. 内存块头（_pHeader），也就是上面说的内存池节点；
3. 内存块大小（_nSize），也就是每个节点多大；
4. 节点数（_nBlock），及时有多少个节点；

这里面需要的注意的是，申请内存块的时候，需要加上节点头，但是申请完后返回给客户使用的需要去掉头；但是释放的时候，需要前移到头，不然就会出现异常；

**释放内存：**

释放内存的时候，将使用过的内存置为false，然后指向头部，将头部作为下一个节点，这样的话，节点每次回收就可以相应的被找到；

> 内存池管理

内存池创建后，会根据节点大小和个数创建相应的内存池；

内存池管理主要就是根据不同的需求创建不同的内存池，以达到管理的目的；

这里主要有一个概念：数组映射

**数组映射**就是不同的范围内，选择不同的内存池；

添一段代码：

```
 void InitArray(int nBegin,int nEnd, MemoryPool*pMemPool)
 {
  for (int i = nBegin; i <= nEnd; i++)
  {
   _Alloc[i] = pMemPool;
  }
 }
```

根据范围进行绑定；

## 4.内存池实现

ManagerPool.hpp

```
#ifndef _MEMORYPOOL_HPP_
#define _MEMORYPOOL_HPP_
#include <iostream>
#include <mutex>
////一个内存块的最大内存大小,可以扩展
#define MAX_MEMORY_SIZE 256
class MemoryPool;
//内存块
struct MemoryBlock
{
 MemoryBlock* pNext;//下一块内存块
 bool bUsed;//是否使用
 bool bBelong;//是否属于内存池
 MemoryPool* pMem;//属于哪个池子
};
class MemoryPool
{
public:
 MemoryPool(size_t nSize=128,size_t nBlock=10)
 {
  //相当于申请10块内存，每块内存是1024
  _nSize = nSize;
  _nBlock = nBlock;
  _pHeader = NULL;
  _pBuffer = NULL;
 }
 virtual ~MemoryPool()
 {
  if (_pBuffer != NULL)
  {
   free(_pBuffer);
  }
 }
 //申请内存
 void* AllocMemory(size_t nSize)
 {
  std::lock_guard<std::mutex> lock(_mutex);
  //如果首地址为空，说明没有申请空间
  if (_pBuffer == NULL)
  {
   InitMemory();
  }
  MemoryBlock* pRes = NULL;
  //如果内存池不够用时，需要重新申请内存
  if (_pHeader == NULL)
  {
   pRes = (MemoryBlock*)malloc(nSize+sizeof(MemoryBlock));
   pRes->bBelong = false;
   pRes->bUsed = false;
   pRes->pNext = NULL;
   pRes->pMem = NULL;
  }
  else
  {
   pRes = _pHeader;
   _pHeader = _pHeader->pNext;
   pRes->bUsed = true;
  }
  //返回只返回头后面的信息
  return ((char*)pRes + sizeof(MemoryBlock));
 }
 //释放内存
 void FreeMemory(void* p)
 {
  std::lock_guard<std::mutex> lock(_mutex);
  //和申请内存刚好相反，这里需要包含头，然后全部释放
  MemoryBlock* pBlock = ((MemoryBlock*)p - sizeof(MemoryBlock));
  if (pBlock->bBelong)
  {
   pBlock->bUsed = false;
   //循环链起来
   pBlock->pNext = _pHeader;
   pBlock = _pHeader;
  }
  else
  {
   //不属于内存池直接释放就可以
   free(pBlock);
  }
 }
 //初始化内存块
 void InitMemory()
 {
  if (_pBuffer)
   return;
  //计算每块的大小
  size_t PoolSize = _nSize + sizeof(MemoryBlock);
  //计算需要申请多少内存
  size_t BuffSize = PoolSize * _nBlock;
  _pBuffer = (char*)malloc(BuffSize);
  //初始化头
  _pHeader = (MemoryBlock*)_pBuffer;
  _pHeader->bUsed = false;
  _pHeader->bBelong = true;
  _pHeader->pMem = this;
  //初始化_nBlock块，并且用链表的形式连接
  //保存头指针
  MemoryBlock* tmp1 = _pHeader;
  for (size_t i = 1; i < _nBlock; i++)
  {
   MemoryBlock* tmp2 = (MemoryBlock*)(_pBuffer + i*PoolSize);
   tmp2->bUsed = false;
   tmp2->pNext = NULL;
   tmp2->bBelong = true;
   _pHeader->pMem = this;
   tmp1->pNext = tmp2;
   tmp1 = tmp2;
  }
 }
public:
 //内存首地址（第一块内存的地址）
 char* _pBuffer;
 //内存块头
 MemoryBlock* _pHeader;
 //内存块大小
 size_t _nSize;
 //多少块
 size_t _nBlock;
 std::mutex _mutex;
};
//可以使用模板传递参数
template<size_t nSize,size_t nBlock>
class MemoryPoolor:public MemoryPool
{
public:
 MemoryPoolor()
 {
  _nSize = nSize;
  _nBlock = nBlock;
 }
};
//需要重新对内存池就行管理
class ManagerPool
{
public:
 static ManagerPool& Instance()
 {
  static ManagerPool memPool;
  return memPool;
 }
 void* AllocMemory(size_t nSize)
 {
  if (nSize < MAX_MEMORY_SIZE)
  {
   return _Alloc[nSize]->AllocMemory(nSize);
  }
  else
  {
   MemoryBlock* pRes = (MemoryBlock*)malloc(nSize + sizeof(MemoryBlock));
   pRes->bBelong = false;
   pRes->bUsed = true;
   pRes->pMem = NULL;
   pRes->pNext = NULL;
   return ((char*)pRes + sizeof(MemoryBlock));
  }
 }
 //释放内存
 void FreeMemory(void* p)
 {
  MemoryBlock* pBlock = (MemoryBlock*)((char*)p - sizeof(MemoryBlock));
  //释放内存池
  if (pBlock->bBelong)
  {
   pBlock->pMem->FreeMemory(p);
  }
  else
  {
   free(pBlock);
  }
 }
private:
 ManagerPool()
 {
  InitArray(0,128, &_memory128);
  InitArray(129, 256, &_memory256);
 }
 ~ManagerPool()
 {
 }
 void InitArray(int nBegin,int nEnd, MemoryPool*pMemPool)
 {
  for (int i = nBegin; i <= nEnd; i++)
  {
   _Alloc[i] = pMemPool;
  }
 }
 //可以根据不同内存块进行分配
 MemoryPoolor<128, 1000> _memory128;
 MemoryPoolor<256, 1000> _memory256;
 //映射数组
 MemoryPool* _Alloc[MAX_MEMORY_SIZE + 1];
};
#endif
```

OperatorMem.hpp

```
#ifndef _OPERATEMEM_HPP_
#define _OPERATEMEM_HPP_
#include <iostream>
#include <stdlib.h>
#include "MemoryPool.hpp"
void* operator new(size_t nSize)
{
 return ManagerPool::Instance().AllocMemory(nSize);
}
void operator delete(void* p)
{
 return ManagerPool::Instance().FreeMemory(p);
}
void* operator new[](size_t nSize)
{
 return ManagerPool::Instance().AllocMemory(nSize);
}
void operator delete[](void* p)
{
 return ManagerPool::Instance().FreeMemory(p);
}
#endif
```

mian.cpp

```
#include "OperateMem.hpp"
using namespace std;
int main()
{
 char* p = new char[128];
 delete[] p;
 return 0;
}
```

原文链接：https://zhuanlan.zhihu.com/p/356612864

作者：Hu先生的Linux

# 【NO.208】C++ 中的多线程的使用和线程池建设

C++ 11 引入了 std::thread 标准库，方便了多线程相关的开发工作。

说到多线程开发，可不仅仅是创建一个新线程就好了，不可避免的要涉及到线程同步的问题。

而保证线程同步，实现线程安全，就要用到相关的工具了，比如信号量、互斥量、条件变量、原子变量等等。

这些名词概念都是来操作系统里面引申来的，并不是属于哪一种编程语言所特有的，在不同语言上的表现形式不一样，但其背后的原理是一致的。

C++ 11 同样引入了 mutex、condition_variable、future 等实现线程安全的类，下面就来一一了解它们。

## 1.mutex

mutex 作为互斥量，提供了独占所有权的特性。

一个线程将互斥量锁住，直到调用 unlock 之前，该线程都是拥有该锁的，而其他线程访问被锁住的互斥量，则会被阻塞住。

使用示例：

```
#include <thread>
#include <iostream>
int num = 0;
std::mutex mutex;
void plus(){
    std::lock_guard<std::mutex> guard(mutex);
    std::cout << num++ <<std::endl;
}
int main(){
    std::thread threads[10];
    for (auto & i : threads) {
        i = std::thread(plus);
    }
    for (auto & thread : threads) {
        thread.join();
    }
    return 0;
}
```

如上代码，创建了 10 个线程，每个线程都会先打印 num 的值，然后再将 num 变量加一，依次打印 0 到 9 。

众所周知，+1 的操作不是线程安全的，实际包含了三步，先读取，再加一，最后赋值。但是因为使用了互斥量 mutex ，保证独占性，所以结果都是按照顺序递增打印的。

如果不使用互斥量，那么可能前一个线程还没有赋值完，后一个线程就进行了读取，最后的结果就是随机不可预料的。

## 2.condition_variable

condition_variable 作为条件变量，它会调用 wait 函数等待某个条件满足，如果不满足的话，就通过 unique_lock 来锁住当前线程，当前线程就处于阻塞状态，直到其他线程调用了条件变量的 nofity 函数来唤醒。

使用示例：

```
#include <iostream>
#include <thread>
int num = 0;
std::mutex mutex;
std::condition_variable cv;
void plus(int target){
    std::unique_lock<std::mutex> lock(mutex);
    cv.wait(lock,[target]{return num == target;});
    num++;
    std::cout << target <<std::endl;
    cv.notify_all();
}
int main(){
    std::thread threads[10];
    for (int i = 0; i < 10; ++i) {
        threads[i] = std::thread(plus,9-i);
    }
    for (auto & thread : threads) {
        thread.join();
    }
    return 0;
}
```

同样是创建了 10 个线程，每个线程都会有一个 target 参数，代表该线程要打印的数值，按照 9 -> 0 顺序创建线程，最后运行结果是依次打印 0 -> 9 。

每个线程运行时都会先调用 wait 函数等待 num == target 这一条件满足，一旦满足就会将 num 加一，并打印 target 值，然后唤醒下一个满足条件的值。

通过改变条件变量的 wait 函数唤醒条件，就可以实现不同的多线程模式，比如常见的生产者-消费者模型。

## 3.condiation_variable 实现生产消费者模式

```
#include <iostream>
#include <thread>
#include <queue>
int main(){
    std::queue<int> products;
    std::condition_variable cv_pro,cv_con;
    std::mutex mtx;
    bool end = false;
    std::thread producer([&]{
        for (int i = 0; i < 10; ++i) {
            std::unique_lock<std::mutex> lock(mtx);
            cv_pro.wait(lock,[&]{return products.empty();});
            products.push(i);
            cv_con.notify_all();
        }
        cv_con.notify_all();
        end = true;
    });
    std::thread consumer([&]{
        while (!end){
            std::unique_lock<std::mutex> lock(mtx);
            cv_con.wait(lock,[&]{return !products.empty();});
            int d = products.front();
            products.pop();
            std::cout << d << std::endl;
            cv_pro.notify_all();
        }
    });
    producer.join();
    consumer.join();
    return 0;
}
```

## 4.future & promise

future 这一特性在日常开发中用的比较少，它可以用来获取异步任务的结果，也可以当做一种线程间同步的手段。

假设现在程序要创建一个线程去执行耗时操作，并且等耗时操作结束了之后要拿到返回值，那么可以通过 condiation_variable 来实现，在异步线程执行完了之后调用 notify 方法来唤醒主线程，同样也可以通过 future 来实现。

当程序通过特定方法创建了一个异步操作之后会返回一个 future ，该 future 可以访问到异步线程的状态。

在异步线程里面设置某个共享状态的值，与该共享状态相关联的 future 就可以通过 get 方法来获取结果，get() 方法会阻塞调用线程，从而等待异步线程完成设置。

future 的 get 方法其实就相当于 condiation_variable 的 wait 方法，而异步线程设置共享状态的值的方法就相当于 condiation_variable 的 notify 方法。

future 的创建有如下三种方式：

## 5.std::promise

promise 就像它的字面意思一样，代表承诺，说明它一定会在异步线程设置共享状态的，而 future 就耐心等待好了。

promise 和 future 的调用流程如下图所示：

![img](https://pic3.zhimg.com/80/v2-2c181a1721c16d767afd22c4bd75da22_720w.webp)

代码示例如下：

```
#include <iostream>
#include <thread>
#include <chrono>
#include <future>
void task(std::promise<int>& promise){
    std::this_thread::sleep_for(std::chrono::seconds(1));
    promise.set_value(10);
}
int main(){
    std::promise<int> promise;
    std::future<int> future = promise.get_future();
    std::thread t(task,std::ref(promise));
    int result = future.get();
    std::cout << "thread result is " << result << std::endl;
    t.join();
    return 0;
}
```

promise 通过 get_future 方法获取与该 promise 关联的 future 对象，并且通过 set_value 方法设置共享状态的值。

## 6.std::packaged_task

packaged_task 可以用来包装一个可调用的对象 ，并且能作为线程的运行函数，有点类似于 std::function 。

但不同的是，它将其包装的可调用对象的执行结果传递一个相关联的 future ，从而实现状态的共享，future 通过 get 方法来等待可调用对象执行结束。

如下代码所示：

```
#include <iostream>
#include <thread>
#include <chrono>
#include <future>
int task(){
    std::this_thread::sleep_for(std::chrono::seconds(1));
    return 10;
}
int main(){
    std::packaged_task<int(void)> packaged_task(task);
    std::future<int> future = packaged_task.get_future();
    std::thread thread(std::move(packaged_task));
    int result = future.get();
    std::cout << "thread result is " << result << std::endl;
    thread.join();
    return 0;
}
```

packaged_task 通过 get_future 方法来获得相关联的 future 对象。

## 7.std::async

async 也能创建 future ，并且它更像是对 std::thread，std::packaged_task，std::promise 的封装。

如下代码所示：

```
#include <iostream>
#include <thread>
#include <chrono>
#include <future>
int task(){
    std::this_thread::sleep_for(std::chrono::seconds(1));
    return 10;
}
int main(){
    std::future<int> future = std::async(std::launch::async,task);
    int result = future.get();
    std::cout << "thread result is " << result << std::endl;
    return 0;
}
```

通过 async 直接创建异步线程并且获取相关联的 future 对象，连 thread 创建线程的操作都省了。

async 有两种执行策略，launch::async 和 launch::deferred 。其中前者是立即执行，后者是等调用 future.get() 方法时才会创建线程执行任务。

## 8.线程池的建设

了解了以上的线程相关操作类，就可以进一步进阶，通过它们来打造一个线程池了。

关于线程池建设，根据具体业务和使用场景，会有很多不同支持，但有些本质内容还是不会变的。

线程池的出发点当然是为了减少在频繁创建和销毁线程上所花的时间和系统资源的开销，表现形式上就是有一池子的线程，向线程池提交任务，最终分配到某个线程上去执行。

如下图所示，就是一个简易线程池的雏形，有任务 Task，有 Thread Pool 来分发任务，也有 Worker Thread 来最终执行任务，麻雀虽小五脏俱全。

![img](https://pic2.zhimg.com/80/v2-319f94e3ec12352720f1fd7b79cb9df9_720w.webp)

接下来就详细拆解以上部分内容。

## 9.任务类型

任务 Task 的类型根据业务需求可以有多种定义，主要差别在于任务需要的参数类型以及返回值类型。

另外，任务本身也可以有一些属性来标识该属性的类型，应该放到什么样的线程去执行等等。

简单起见，定义一个简单的 Task 类型，无需参数，也不需要返回值类型。

```
using task = std::function<void()>;
```

## 10.线程数量

一个线程池该有多少线程呢？如果数量太多，会导致资源浪费，有些线程不一定能充分使用。如果太少就会导致频繁创建新线程。

一个灵活的线程池应该是可以动态改变线程数量的，参考 Java线程池实现原理及其在美团业务中的实践。

在 Java 的 ThreadPoolExecutor 中通过 corePoolSize 和 maximumPoolSize 来限制线程的数量，线程数量会在 [0 ~ corePoolSize] 和 [corePoolSize ~ maximumPoolSize] 之间波动。

当任务吃紧，线程和缓存都满了，就会申请线程，数量达到 [corePoolSize ~ maximumPoolSize] 范围，一旦任务松懈，就会释放一些空闲线程，数量回落到 [0 ~ corePoolSize] 范围，如果任务持续吃紧，那么就会拒绝任务了。

当然，也有其他确定线程数量的策略，根据具体的业务需求来核定，比如根据 CPU 多核来决定线程数量多少。

简单起见，这里就以固定线程数量作为演示了。

```
size_t N = std::thread::hardware_concurrency();
```

## 11.任务缓存

假设现在已经固定了 N 个线程，并且每个线程都有任务在执行，这时有来了一个新任务，那么该怎么处理呢？ 这时候就需要任务缓存机制了（当然也可以直接拒绝该任务）。

任务缓存也分多种形式：

1. 全局缓存
2. 线程缓存
3. 全局缓存 + 线程缓存

## 12.全局缓存

全局缓存，顾名思义就是在线程池有一个全局的缓存队列，凡是进入到线程池的任务都会先进到全局缓存中，然后由全局缓存进行任务的分发，最后由不同的工作线程去执行任务。

## 13.线程缓存

线程缓存，顾名思义就是在每个工作线程都有一个缓存队列，然后线程不断循环处理自己缓存队列上的任务。凡是进入到线程池的任务，都会由线程池进到调度和分发，然后进入到工作线程对应的缓存队列中，最终被执行结束

## 14.全局缓存 + 线程缓存

全局缓存 + 线程缓存 就是将上面两者结合起来了，用如下图来汇总演示：

![img](https://pic4.zhimg.com/80/v2-86860cfc3b0dbd61667b3e3f69eb14db_720w.webp)

这种缓存方式算是比较复杂的情形了，适用于那种计算量大且快速执行的情形，一般来说还是全局缓存用的比较普遍。

## 15.缓存队列

有了任务缓存，那么还应该定义一下缓存队列。毫无疑问，缓存队列必须是线程安全的，因为它要在多个工作线程之间共享任务。

缓存队列的形式有很多，比如阻塞队列，双向链表的阻塞队列等等，这里定义一个简单的队列，把 std::queue 做一下线程安全的封装。

```
#pragma once
#include <mutex>
#include <queue>
// Thread safe implementation of a Queue using an std::queue
template <typename T>
class SafeQueue {
private:
  std::queue<T> m_queue;
  std::mutex m_mutex;
public:
  SafeQueue() {
  }
  bool empty() {
    std::unique_lock<std::mutex> lock(m_mutex);
    return m_queue.empty();
  }
  int size() {
    std::unique_lock<std::mutex> lock(m_mutex);
    return m_queue.size();
  }
  void enqueue(T& t) {
    std::unique_lock<std::mutex> lock(m_mutex);
    m_queue.push(t);
  }
  bool dequeue(T& t) {
    std::unique_lock<std::mutex> lock(m_mutex);
    if (m_queue.empty()) {
      return false;
    }
    t = std::move(m_queue.front());
    m_queue.pop();
    return true;
  }
};
```

定义了 enqueue 和 dequeue 方法向队列中塞任务和取任务，通过加锁来保证线程安全。

## 16.线程调度

线程池最核心的部分也就是线程调度了，假设使用了全局缓存的形式，那么如何把全局缓存中的任务分发给空闲线程呢？

实际上从某种角度来说，全局缓存的线程池也可以认为是一个单生产者-多消费者模式，全局缓存就是生产者，而多个线程就是多个消费者了。

在前面的代码实践中已经写了一个单生产者-单消费者模式，当生产者生产了 Task 之后，就通过 notify 方法来唤醒消费者，从而将 Task 分配到消费者去执行。由于只有一个消费者，那唤醒的就是唯一的那个了。

那假若有多个消费者，唤醒的又是哪一个呢？答案是随机的。调用 notify_one 方法会随机唤醒一个线程，调用 notify_all 则会唤醒全部线程。

但是唤醒并不代表线程就会消费 Task，一个 Task 对应多个线程，线程唤醒之后会去全局缓存抢夺 Task 任务，一旦得手就执行，而其他没有抢到的线程则继续挂起，等待下一次的唤醒。

线程池中线程的运行状态如下图所示：

![img](https://pic1.zhimg.com/80/v2-e48bb269cb994225ba783e6cefc7ef14_720w.webp)

本质上，线程池还是通过 notify 方法来唤醒线程，从而实现任务分发和调度的。

这种方式具有一定的随机性，不能确保到底唤醒了哪个线程，可以根据业务需要定制相关的调度逻辑，比如只唤醒某些具有共同属性的线程，或者根据 Task 任务的要求来唤醒指定线程，更可以不通过 notify 的方式，直接把任务派发给对应的线程去执行。

根据上述流程图就可以给出工作线程运行的代码了：

```
class WorkerThread {
private:
    int m_id;
    ThreadPool *m_pool;
public:
    WorkerThread(ThreadPool *pool, int id) : m_pool(pool), m_id(id) {
    }
    void operator()() {
        task func;
        bool dequeued;
        while (!m_pool->m_shutdown) {
            std::unique_lock<std::mutex> lock(m_pool->m_mutex);
            if (m_pool->m_queue.empty()){
                m_pool->m_condition_variable.wait(lock);
            }
            dequeued = m_pool->m_queue.dequeue(func);
            if (dequeued) {
                func();
            }
        }
    }
};
```

下面就是一个简单的线程池代码实践：

```
#ifndef THREAD_POOL_THREADPOOL_H
#define THREAD_POOL_THREADPOOL_H
#include <functional>
#include <future>
#include <mutex>
#include <condition_variable>
#include <thread>
#include <queue>
#include "SafeQueue.h"
using task = std::function<void()>;
class ThreadPool {
public:
    ThreadPool(size_t thread_num = std::thread::hardware_concurrency()) : m_threads(
            std::vector<std::thread>(thread_num)), m_shutdown(false) {
    }
    void init() {
        for (int i = 0; i < m_threads.size(); ++i) {
            m_threads[i] = std::thread(WorkerThread(this, i));
        }
    }
    void shutdown() {
        m_shutdown = true;
        m_condition_variable.notify_all();
        for (int i = 0; i < m_threads.size(); ++i) {
            if (m_threads[i].joinable()) {
                m_threads[i].join();
            }
        }
    }
    std::future<void> submit(task t){
        auto p_task = std::make_shared<std::packaged_task<void()>>(t);
        task wrapper_task = [p_task](){
             (*p_task)();
        };
        m_queue.enqueue(wrapper_task);
        m_condition_variable.notify_one();
        return p_task->get_future();
    }
private:
    class WorkerThread {
    private:
        int m_id;
        ThreadPool *m_pool;
    public:
        WorkerThread(ThreadPool *pool, int id) : m_pool(pool), m_id(id) {
        }
        void operator()() {
            task func;
            bool dequeued;
            while (!m_pool->m_shutdown) {
                std::unique_lock<std::mutex> lock(m_pool->m_mutex);
                if (m_pool->m_queue.empty()){
                    m_pool->m_condition_variable.wait(lock);
                }
                dequeued = m_pool->m_queue.dequeue(func);
                if (dequeued) {
                    func();
                }
            }
        }
    };
    bool m_shutdown;
    SafeQueue<task> m_queue;
    std::vector<std::thread> m_threads;
    std::mutex m_mutex;
    std::condition_variable m_condition_variable;
};
#endif //THREAD_POOL_THREADPOOL_H
```

通过 submit 方法提交任务到全局缓存队列中，然后唤醒线程去消费任务执行。

## 17.小结

关于 C++ 多线程的使用还有很多知识点，以上只是介绍了部分内容，还有很多不足之处，后续再补充了。

原文链接：https://zhuanlan.zhihu.com/p/357893078

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.209】mysql锁机制详解

## 1.前言

　　之前项目中用到事务，需要保证数据的强一致性，期间也用到了mysql的锁，但当时对mysql的锁机制只是管中窥豹，所以本文打算总结一下mysql的锁机制。

　　本文主要论述关于mysql锁机制，mysql版本为5.7，引擎为innodb，由于实际中关于innodb锁相关的知识及加锁方式很多，所以没有那么多精力罗列所有场景下的加锁过程并加以分析，仅根据现在了解的知识，结合官方文档，说说自己的理解，如果发现有不对的地方，欢迎指正。

## 2.概述

　　总的来说，InnoDB共有七种类型的锁：

- 共享/排它锁(Shared and Exclusive Locks)
- 意向锁(Intention Locks)
- 记录锁(Record Locks)
- 间隙锁(Gap Locks)
- 临键锁(Next-key Locks)
- 插入意向锁(Insert Intention Locks)
- 自增锁(Auto-inc Locks)

## 3.mysql锁详解

### 3.1. 共享/排它锁(Shared and Exclusive Locks)

- 共享锁（Share Locks，记为S锁），读取数据时加S锁
- 排他锁（eXclusive Locks，记为X锁），修改数据时加X锁

　　使用的语义为：

- 共享锁之间不互斥，简记为：读读可以并行
- 排他锁与任何锁互斥，简记为：写读，写写不可以并行

　　可以看到，一旦写数据的任务没有完成，数据是不能被其他任务读取的，这对并发度有较大的影响。对应到数据库，可以理解为，写事务没有提交，读相关数据的select也会被阻塞，这里的select是指加了锁的，普通的select仍然可以读到数据(快照读)。

### 3.2. 意向锁(Intention Locks)

　　InnoDB为了支持多粒度锁机制(multiple granularity locking)，即允许行级锁与表级锁共存，而引入了意向锁(intention locks)。意向锁是指，未来的某个时刻，事务可能要加共享/排它锁了，先提前声明一个意向。

1. 意向锁是一个表级别的锁(table-level locking)；
2. 意向锁又分为：
3. 意向共享锁(intention shared lock, IS)，它预示着，事务有意向对表中的某些行加共享S锁；
4. 意向排它锁(intention exclusive lock, IX)，它预示着，事务有意向对表中的某些行加排它X锁；

　　加锁的语法为：

```
select ... lock in share mode;　　要设置IS锁；select ... for update;　　　　　　 要设置IX锁；
```

　　事务要获得某些行的S/X锁，必须先获得表对应的IS/IX锁，意向锁仅仅表明意向，意向锁之间相互兼容，兼容互斥表如下：

|      |  IS   |  IX   |
| :--: | :---: | :---: |
|  IS  | 兼 容 | 兼 容 |
|  IX  | 兼 容 | 兼 容 |

　　虽然意向锁之间互相兼容，但是它与共享锁/排它锁互斥，其兼容互斥表如下:

|      |   S   |   X   |
| :--: | :---: | :---: |
|  IS  | 兼 容 | 互 斥 |
|  IX  | 互 斥 | 互 斥 |

　　排它锁是很强的锁，不与其他类型的锁兼容。这其实很好理解，修改和删除某一行的时候，必须获得强锁，禁止这一行上的其他并发，以保障数据的一致性。

### 3.3. 记录锁(Record Locks)

　　记录锁，它封锁索引记录，例如(其中id为pk)：

```
create table lock_example(id smallint(10),name varchar(20),primary key id)engine=innodb;
```

　　数据库隔离级别为RR，表中有如下数据：

```
10, zhangsan
20, lisi
30, wangwu
select * from t where id=1 for update;
```

　　其实这里是先获取该表的意向排他锁(IX)，再获取这行记录的排他锁(我的理解是因为这里直接命中索引了)，以阻止其他事务插入，更新，删除id=1的这一行。

### 3.4. 间隙锁(Gap Locks)

　　间隙锁，它封锁索引记录中的间隔，或者第一条索引记录之前的范围，又或者最后一条索引记录之后的范围。依然是上面的例子，InnoDB，RR：

```
select * from lock_example
    where id between 8 and 15 
    for update;
```

　　这个SQL语句会封锁区间(8,15)，以阻止其他事务插入id位于该区间的记录。

　　间隙锁的主要目的，就是为了防止其他事务在间隔中插入数据，以导致“不可重复读”。**如果把事务的隔离级别降级为读提交(Read Committed, RC)，间隙锁则会自动失效。**

### 3.5. 临键锁(Next-key Locks)

　　临键锁，是记录锁与间隙锁的组合，它的封锁范围，既包含索引记录，又包含索引区间。

　　默认情况下，innodb使用next-key locks来锁定记录。但当查询的索引含有唯一属性的时候，Next-Key Lock 会进行优化，将其降级为Record Lock，即仅锁住索引本身，不是范围。

　　举个例子，依然是如上的表lock_example，但是id降级为普通索引(key)，也就是说即使这里声明了要加锁(for update)，而且命中的是索引，但是因为索引在这里没有UK约束，所以innodb会使用next-key locks，数据库隔离级别RR：

```
事务A执行如下语句，未提交：
select * from lock_example where id = 20 for update;
事务B开始，执行如下语句，会阻塞：
insert into lock_example values('zhang',15);
insert into lock_example values('zhang',15);
```

　　如上的例子，事务A执行查询语句之后，默认给id=20这条记录加上了next-key lock，所以事务B插入10(包括)到30(不包括)之间的记录都会阻塞。临键锁的主要目的，也是为了避免幻读(Phantom Read)。如果把事务的隔离级别**降级为RC，临键锁则也会失效**。

### 3.6. 插入意向锁(Insert Intention Locks)

　　对已有数据行的修改与删除，必须加强互斥锁(X锁)，那么对于数据的插入，是否还需要加这么强的锁，来实施互斥呢？插入意向锁，孕育而生。

　　插入意向锁，是间隙锁(Gap Locks)的一种（所以，也是实施在索引上的），它是专门针对insert操作的。多个事务，在同一个索引，同一个范围区间插入记录时，如果插入的位置不冲突，不会阻塞彼此。

> Insert Intention Lock signals the intent to insert in such a way that multiple transactions inserting into the same index gap need not wait for each other if they are not inserting at the same position within the gap.

　　举个例子(表依然是如上的例子lock_example，数据依然是如上)，事务A先执行，在10与20两条记录中插入了一行，还未提交：

```
insert into t values(11, xxx);
```

　　事务B后执行，也在10与20两条记录中插入了一行：

```
insert into t values(12, ooo);
```

　　因为是插入操作，虽然是插入同一个区间，但是插入的记录并不冲突，所以使用的是插入意向锁，此处A事务并不会阻塞B事务。

### 3.7. 自增锁(Auto-inc Locks)

　　自增锁是一种特殊的表级别锁（table-level lock），专门针对事务插入AUTO_INCREMENT类型的列。最简单的情况，如果一个事务正在往表中插入记录，所有其他事务的插入必须等待，以便第一个事务插入的行，是连续的主键值。

> AUTO-INC lock is a special table-level lock taken by transactions inserting into tables with AUTO_INCREMENT columns. In the simplest case, if one transaction is inserting values into the table, any other transactions must wait to do their own inserts into that table, so that rows inserted by the first transaction receive consecutive primary key values.

　　举个例子(表依然是如上的例子lock_example)，但是id为AUTO_INCREMENT，数据库表中数据为：

```
1, zhangsan
2, lisi
3, wangwu
```

　　事务A先执行，还未提交： insert into t(name) values(xxx);

　　事务B后执行： insert into t(name) values(ooo);

　　此时事务B插入操作会阻塞，直到事务A提交。

## 4.总结

　　以上总结的7种锁，个人理解可以按两种方式来区分：

**1. 按锁的互斥程度来划分，可以分为共享、排他锁；**

- 共享锁(S锁、IS锁)，可以提高读读并发；
- 为了保证数据强一致，InnoDB使用强互斥锁(X锁、IX锁)，保证同一行记录修改与删除的串行性；

**2. 按锁的粒度来划分，可以分为：**

- 表锁：意向锁(IS锁、IX锁)、自增锁；
- 行锁：记录锁、间隙锁、临键锁、插入意向锁；

　　其中

1. InnoDB的细粒度锁(即行锁)，是实现在索引记录上的(我的理解是如果未命中索引则会失效)；　　
2. 记录锁锁定索引记录；间隙锁锁定间隔，防止间隔中被其他事务插入；临键锁锁定索引记录+间隔，防止幻读；
3. InnoDB使用插入意向锁，可以提高插入并发；
4. 间隙锁(gap lock)与临键锁(next-key lock)**只在RR以上的级别生效，RC下会失效**；



原文链接：https://zhuanlan.zhihu.com/p/356422542

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.210】TCP/IP协议详解

## 1.为什么会有TCP/IP协议

在世界上各地，各种各样的电脑运行着各自不同的操作系统为大家服务，这些电脑在表达同一种信息的时候所使用的方法是千差万别。就好像圣经中上帝打乱了各地人的口音，让他们无法合作一样。计算机使用者意识到，计算机只是单兵作战并不会发挥太大的作用。只有把它们联合起来，电脑才会发挥出它最大的潜力。于是人们就想方设法的用电线把电脑连接到了一起。

但是简单的连到一起是远远不够的，就好像语言不同的两个人互相见了面，完全不能交流信息。因而他们需要定义一些共通的东西来进行交流，TCP/IP就是为此而生。TCP/IP不是一个协议，而是一个协议族的统称。里面包括了IP协议，IMCP协议，TCP协议，以及我们更加熟悉的http、ftp、pop3协议等等。电脑有了这些，就好像学会了外语一样，就可以和其他的计算机终端做自由的交流了。

## 2.TCP/IP协议分层

![img](https://pic2.zhimg.com/80/v2-2b4cdf4061debe76f410458b22dbcfa9_720w.webp)

[TCP分层2.jpg]

TCP/IP协议族按照层次由上到下，层层包装。

**应用层**:
向用户提供一组常用的应用程序，比如电子邮件、文件传输访问、远程登录等。远程登录TELNET使用TELNET协议提供在网络其它主机上注册的接口。TELNET会话提供了基于字符的虚拟终端。文件传输访问FTP使用FTP协议来提供网络内机器间的文件拷贝功能。

**传输层**:
提供应用程序间的通信。其功能包括：一、格式化信息流；二、提供可靠传输。为实现后者，传输层协议规定接收端必须发回确认，并且假如分组丢失，必须重新发送。

**网络层** ：
负责相邻计算机之间的通信。其功能包括三方面。
一、处理来自传输层的分组发送请求，收到请求后，将分组装入IP数据报，填充报头，选择去往信宿机的路径，然后将数据报发往适当的网络接口。

二、处理输入数据报：首先检查其合法性，然后进行寻径–假如该数据报已到达信宿机，则去掉报头，将剩下部分交给适当的传输协议；假如该数据报尚未到达信宿，则转发该数据报。

三、处理路径、流控、拥塞等问题。

**网络接口层**：
这是TCP/IP软件的最低层，负责接收IP数据报并通过网络发送之，或者从网络上接收物理帧，抽出IP数据报，交给IP层。

## 3.IP 是无连接的

IP 用于计算机之间的通信。

IP 是无连接的通信协议。它不会占用两个正在通信的计算机之间的通信线路。这样，IP 就降低了对网络线路的需求。每条线可以同时满足许多不同的计算机之间的通信需要。

通过 IP，消息（或者其他数据）被分割为小的独立的包，并通过因特网在计算机之间传送。

IP 负责将每个包路由至它的目的地。

## 4.IP地址

每个计算机必须有一个 IP 地址才能够连入因特网。

每个 IP 包必须有一个地址才能够发送到另一台计算机。

网络上每一个节点都必须有一个独立的Internet地址（也叫做IP地址）。现在，通常使用的IP地址是一个32bit的数字，也就是我们常说的IPv4标准，这32bit的数字分成四组，也就是常见的255.255.255.255的样式。IPv4标准上，地址被分为五类，我们常用的是B类地址。具体的分类请参考其他文档。需要注意的是IP地址是网络号+主机号的组合，这非常重要。

CP/IP 使用 32 个比特来编址。一个计算机字节是 8 比特。所以 TCP/IP 使用了 4 个字节。
一个计算机字节可以包含 256 个不同的值：
00000000、00000001、00000010、00000011、00000100、00000101、00000110、00000111、00001000 ……. 直到 11111111。
现在，你知道了为什么 TCP/IP 地址是介于 0 到 255 之间的 4 个数字。

![img](https://pic1.zhimg.com/80/v2-6b90d0da44ac6f184d4e696afa6d2a54_720w.webp)

## 5.TCP 使用固定的连接

TCP 用于应用程序之间的通信。

当应用程序希望通过 TCP 与另一个应用程序通信时，它会发送一个通信请求。这个请求必须被送到一个确切的地址。在双方“握手”之后，TCP 将在两个应用程序之间建立一个全双工 (full-duplex) 的通信。

这个全双工的通信将占用两个计算机之间的通信线路，直到它被一方或双方关闭为止。

UDP 和 TCP 很相似，但是更简单，同时可靠性低于 TCP。

## 6.IP 路由器

当一个 IP 包从一台计算机被发送，它会到达一个 IP 路由器。

IP 路由器负责将这个包路由至它的目的地，直接地或者通过其他的路由器。

在一个相同的通信中，一个包所经由的路径可能会和其他的包不同。而路由器负责根据通信量、网络中的错误或者其他参数来进行正确地寻址。

## 7.域名

12 个阿拉伯数字很难记忆。使用一个名称更容易。

用于 TCP/IP 地址的名字被称为域名。[http://w3school.com.cn](https://link.zhihu.com/?target=http%3A//w3school.com.cn) 就是一个域名。

当你键入一个像 [http://www.w3school.com.cn](https://link.zhihu.com/?target=http%3A//www.w3school.com.cn) 这样的域名，域名会被一种 DNS 程序翻译为数字。

在全世界，数量庞大的 DNS 服务器被连入因特网。DNS 服务器负责将域名翻译为 TCP/IP 地址，同时负责使用新的域名信息更新彼此的系统。

当一个新的域名连同其 TCP/IP 地址一同注册后，全世界的 DNS 服务器都会对此信息进行更新。

## 8.TCP/IP

TCP/IP 意味着 TCP 和 IP 在一起协同工作。

TCP 负责应用软件（比如你的浏览器）和网络软件之间的通信。

IP 负责计算机之间的通信。

TCP 负责将数据分割并装入 IP 包，然后在它们到达的时候重新组合它们。

IP 负责将包发送至接受者。

## 9.TCP报文格式

![img](https://pic2.zhimg.com/80/v2-b383d3936b12243a587d029d673526e1_720w.webp)

TCP报文格式1.jpg

16位源端口号：16位的源端口中包含初始化通信的端口。源端口和源IP地址的作用是标识报文的返回地址。

16位目的端口号：16位的目的端口域定义传输的目的。这个端口指明报文接收计算机上的应用程序地址接口。

32位序号：32位的序列号由接收端计算机使用，重新分段的报文成最初形式。当SYN出现，序列码实际上是初始序列码（Initial Sequence Number，ISN），而第一个数据字节是ISN+1。这个序列号（序列码）可用来补偿传输中的不一致。

32位确认序号：32位的序列号由接收端计算机使用，重组分段的报文成最初形式。如果设置了ACK控制位，这个值表示一个准备接收的包的序列码。

4位首部长度：4位包括TCP头大小，指示何处数据开始。

保留（6位）：6位值域，这些位必须是0。为了将来定义新的用途而保留。

标志：6位标志域。表示为：紧急标志、有意义的应答标志、推、重置连接标志、同步序列号标志、完成发送数据标志。按照顺序排列是：URG、ACK、PSH、RST、SYN、FIN。

16位窗口大小：用来表示想收到的每个TCP数据段的大小。TCP的流量控制由连接的每一端通过声明的窗口大小来提供。窗口大小为字节数，起始于确认序号字段指明的值，这个值是接收端正期望接收的字节。窗口大小是一个16字节字段，因而窗口大小最大为65535字节。

16位校验和：16位TCP头。源机器基于数据内容计算一个数值，收信息机要与源机器数值 结果完全一样，从而证明数据的有效性。检验和覆盖了整个的TCP报文段：这是一个强制性的字段，一定是由发送端计算和存储，并由接收端进行验证的。

16位紧急指针：指向后面是优先数据的字节，在URG标志设置了时才有效。如果URG标志没有被设置，紧急域作为填充。加快处理标示为紧急的数据段。

选项：长度不定，但长度必须为1个字节。如果没有选项就表示这个1字节的域等于0。

数据：该TCP协议包负载的数据。

在上述字段中，6位标志域的各个选项功能如下。

## 10.URG：紧急标志。紧急标志为”1”表明该位有效。

ACK：确认标志。表明确认编号栏有效。大多数情况下该标志位是置位的。TCP报头内的确认编号栏内包含的确认编号（w+1）为下一个预期的序列编号，同时提示远端系统已经成功接收所有数据。

PSH：推标志。该标志置位时，接收端不将该数据进行队列处理，而是尽可能快地将数据转由应用处理。在处理Telnet或rlogin等交互模式的连接时，该标志总是置位的。

RST：复位标志。用于复位相应的TCP连接。

SYN：同步标志。表明同步序列编号栏有效。该标志仅在三次握手建立TCP连接时有效。它提示TCP连接的服务端检查序列编号，该序列编号为TCP连接初始端（一般是客户端）的初始序列编号。在这里，可以把TCP序列编号看作是一个范围从0到4，294，967，295的32位计数器。通过TCP连接交换的数据中每一个字节都经过序列编号。在TCP报头中的序列编号栏包括了TCP分段中第一个字节的序列编号。

FIN：结束标志。

## 11.TCP三次握手

所谓三次握手（Three-Way Handshake）即建立TCP连接，就是指建立一个TCP连接时，需要客户端和服务端总共发送3个包以确认连接的建立。在socket编程中，这一过程由客户端执行connect来触发，整个流程如下图所示：

![img](https://pic4.zhimg.com/80/v2-3be181a83c2f287ba8daa86c7250b76b_720w.webp)

TCP三次握手.png

（1）第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。

（2）第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。

（3）第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。

简单来说，就是

1、建立连接时，客户端发送SYN包（SYN=i）到服务器，并进入到SYN-SEND状态，等待服务器确认

2、服务器收到SYN包，必须确认客户的SYN（ack=i+1）,同时自己也发送一个SYN包（SYN=k）,即SYN+ACK包，此时服务器进入SYN-RECV状态

3、客户端收到服务器的SYN+ACK包，向服务器发送确认报ACK（ack=k+1）,此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手，客户端与服务器开始传送数据。

**SYN攻击**：

在三次握手过程中，Server发送SYN-ACK之后，收到Client的ACK之前的TCP连接称为半连接（half-open connect），此时Server处于SYN_RCVD状态，当收到ACK后，Server转入ESTABLISHED状态。SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server回复确认包，并等待Client的确认，由于源地址是不存在的，因此，Server需要不断重发直至超时，这些伪造的SYN包将产时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络堵塞甚至系统瘫痪。SYN攻击时一种典型的DDOS攻击，检测SYN攻击的方式非常简单，即当Server上有大量半连接状态且源IP地址是随机的，则可以断定遭到SYN攻击了，使用如下命令可以让之现行：

```
#netstat -nap | grep SYN_RECV
```

## 12.TCP四次挥手

所谓四次挥手（Four-Way Wavehand）即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务端任一方执行close来触发，整个流程如下图所示：

![img](https://pic3.zhimg.com/80/v2-fca5111d5d4b853a480e0b1fcb96c17a_720w.webp)

TCP四次挥手.png

由于TCP连接时全双工的，因此，每个方向都必须要单独进行关闭，这一原则是当一方完成数据发送任务后，发送一个FIN来终止这一方向的连接，收到一个FIN只是意味着这一方向上没有数据流动了，即不会再收到数据了，但是在这个TCP连接上仍然能够发送数据，直到这一方向也发送了FIN。首先进行关闭的一方将执行主动关闭，而另一方则执行被动关闭，上图描述的即是如此。

（1）第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送，Client进入FIN_WAIT_1状态。

（2）第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1（与SYN相同，一个FIN占用一个序号），Server进入CLOSE_WAIT状态。

（3）第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送，Server进入LAST_ACK状态。

（4）第四次挥手：Client收到FIN后，Client进入TIME_WAIT状态，接着发送一个ACK给Server，确认序号为收到序号+1，Server进入CLOSED状态，完成四次挥手。

## 13.为什么建立连接是三次握手，而关闭连接却是四次挥手呢？

这是因为服务端在LISTEN状态下，收到建立连接请求的SYN报文后，把ACK和SYN放在一个报文里发送给客户端。而关闭连接时，当收到对方的FIN报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方也未必全部数据都发送给对方了，所以己方可以立即close，也可以发送一些数据给对方后，再发送FIN报文给对方来表示同意现在关闭连接，因此，己方ACK和FIN一般都会分开发送。

## 14.为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？

原因有二：
一、保证TCP协议的全双工连接能够可靠关闭
二、保证这次连接的重复数据段从网络中消失

先说第一点，如果Client直接CLOSED了，那么由于IP协议的不可靠性或者是其它网络原因，导致Server没有收到Client最后回复的ACK。那么Server就会在超时之后继续发送FIN，此时由于Client已经CLOSED了，就找不到与重发的FIN对应的连接，最后Server就会收到RST而不是ACK，Server就会以为是连接错误把问题报告给高层。这样的情况虽然不会造成数据丢失，但是却导致TCP协议不符合可靠连接的要求。所以，Client不是直接进入CLOSED，而是要保持TIME_WAIT，当再次收到FIN的时候，能够保证对方收到ACK，最后正确的关闭连接。

再说第二点，如果Client直接CLOSED，然后又再向Server发起一个新连接，我们不能保证这个新连接与刚关闭的连接的端口号是不同的。也就是说有可能新连接和老连接的端口号是相同的。一般来说不会发生什么问题，但是还是有特殊情况出现：假设新连接和已经关闭的老连接端口号是一样的，如果前一次连接的某些数据仍然滞留在网络中，这些延迟数据在建立新连接之后才到达Server，由于新连接和老连接的端口号是一样的，又因为TCP协议判断不同连接的依据是socket pair，于是，TCP协议就认为那个延迟的数据是属于新连接的，这样就和真正的新连接的数据包发生混淆了。所以TCP连接还要在TIME_WAIT状态等待2倍MSL，这样可以保证本次连接的所有数据都从网络中消失。

原文链接：https://zhuanlan.zhihu.com/p/362949484

作者：Hu先生的Linux