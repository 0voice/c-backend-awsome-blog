# 【NO.251】如何优雅地记录操作日志？

操作日志几乎存在于每个系统中，而这些系统都有记录操作日志的一套 API。操作日志和系统日志不一样，操作日志必须要做到简单易懂。所以如何让操作日志不和业务逻辑耦合，如何让操作日志的内容易于理解，让操作日志的接入更加简单？上面这些都是本文要回答的问题，主要围绕着如何“优雅”地记录操作日志展开描述。

## 1. 操作日志的使用场景

![例子](https://p1.meituan.net/travelcube/ae35fb1babaab193c1dd0b1bbbe9f07d96643.png)

例子



**系统日志和操作日志的区别**

**系统日志**：系统日志主要是为开发排查问题提供依据，一般打印在日志文件中；系统日志的可读性要求没那么高，日志中会包含代码的信息，比如在某个类的某一行打印了一个日志。

**操作日志**：主要是对某个对象进行新增操作或者修改操作后记录下这个新增或者修改，操作日志要求可读性比较强，因为它主要是给用户看的，比如订单的物流信息，用户需要知道在什么时间发生了什么事情。再比如，客服对工单的处理记录信息。

操作日志的记录格式大概分为下面几种： * 单纯的文字记录，比如：2021-09-16 10:00 订单创建。 * 简单的动态的文本记录，比如：2021-09-16 10:00 订单创建，订单号：NO.11089999，其中涉及变量订单号“NO.11089999”。 * 修改类型的文本，包含修改前和修改后的值，比如：2021-09-16 10:00 用户小明修改了订单的配送地址：从“金灿灿小区”修改到“银盏盏小区” ，其中涉及变量配送的原地址“金灿灿小区”和新地址“银盏盏小区”。 * 修改表单，一次会修改多个字段。

## 2. 实现方式

### 2.1 使用 Canal 监听数据库记录操作日志

Canal 是一款基于 MySQL 数据库增量日志解析，提供增量数据订阅和消费的开源组件，通过采用监听数据库 Binlog 的方式，这样可以从底层知道是哪些数据做了修改，然后根据更改的数据记录操作日志。

这种方式的优点是和业务逻辑完全分离。缺点也很明显，局限性太高，只能针对数据库的更改做操作日志记录，如果修改涉及到其他团队的 RPC 的调用，就没办法监听数据库了，举个例子：给用户发送通知，通知服务一般都是公司内部的公共组件，这时候只能在调用 RPC 的时候手工记录发送通知的操作日志了。

### 2.2 通过日志文件的方式记录

```
log.info("订单创建")
log.info("订单已经创建，订单编号:{}", orderNo)
log.info("修改了订单的配送地址：从“{}”修改到“{}”， "金灿灿小区", "银盏盏小区")
```

这种方式的操作记录需要解决三个问题。

**问题一：操作人如何记录**

借助 SLF4J 中的 MDC 工具类，把操作人放在日志中，然后在日志中统一打印出来。首先在用户的拦截器中把用户的标识 Put 到 MDC 中。

```java
@Component
public class UserInterceptor extends HandlerInterceptorAdapter {
  @Override
  public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception {
    //获取到用户标识
    String userNo = getUserNo(request);
    //把用户 ID 放到 MDC 上下文中
    MDC.put("userId", userNo);
    return super.preHandle(request, response, handler);
  }

  private String getUserNo(HttpServletRequest request) {
    // 通过 SSO 或者Cookie 或者 Auth信息获取到 当前登陆的用户信息
    return null;
  }
}
```

其次，把 userId 格式化到日志中，使用 %X{userId} 可以取到 MDC 中用户标识。

```
<pattern>"%d{yyyy-MM-dd HH:mm:ss.SSS} %t %-5level %X{userId} %logger{30}.%method:%L - %msg%n"</pattern>
```

**问题二：操作日志如何和系统日志区分开**

通过配置 Log 的配置文件，把有关操作日志的 Log 单独放到一日志文件中。

```xml
//不同业务日志记录到不同的文件
<appender name="businessLogAppender" class="ch.qos.logback.core.rolling.RollingFileAppender">
    <File>logs/business.log</File>
    <append>true</append>
    <filter class="ch.qos.logback.classic.filter.LevelFilter">
        <level>INFO</level>
        <onMatch>ACCEPT</onMatch>
        <onMismatch>DENY</onMismatch>
    </filter>
    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
        <fileNamePattern>logs/业务A.%d.%i.log</fileNamePattern>
        <maxHistory>90</maxHistory>
        <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
            <maxFileSize>10MB</maxFileSize>
        </timeBasedFileNamingAndTriggeringPolicy>
    </rollingPolicy>
    <encoder>
        <pattern>"%d{yyyy-MM-dd HH:mm:ss.SSS} %t %-5level %X{userId} %logger{30}.%method:%L - %msg%n"</pattern>
        <charset>UTF-8</charset>
    </encoder>
</appender>
        
<logger name="businessLog" additivity="false" level="INFO">
    <appender-ref ref="businessLogAppender"/>
</logger>
```

然后在 Java 代码中单独的记录业务日志。

```java
//记录特定日志的声明
private final Logger businessLog = LoggerFactory.getLogger("businessLog");
 
//日志存储
businessLog.info("修改了配送地址");
```

**问题三：如何生成可读懂的日志文案**

可以采用 LogUtil 的方式，也可以采用切面的方式生成日志模板，后续内容将会进行介绍。这样就可以把日志单独保存在一个文件中，然后通过日志收集可以把日志保存在 Elasticsearch 或者数据库中，接下来看下如何生成可读的操作日志。

### 2.3 通过 LogUtil 的方式记录日志

```java
  LogUtil.log(orderNo, "订单创建", "小明")模板
  LogUtil.log(orderNo, "订单创建，订单号"+"NO.11089999",  "小明")
  String template = "用户%s修改了订单的配送地址：从“%s”修改到“%s”"
  LogUtil.log(orderNo, String.format(tempalte, "小明", "金灿灿小区", "银盏盏小区"),  "小明")
```

> 这里解释下为什么记录操作日志的时候都绑定了一个 OrderNo，因为操作日志记录的是：某一个“时间”“谁”对“什么”做了什么“事情”。当查询业务的操作日志的时候，会查询针对这个订单的的所有操作，所以代码中加上了 OrderNo，记录操作日志的时候需要记录下操作人，所以传了操作人“小明”进来。

上面看起来问题并不大，在修改地址的业务逻辑方法中使用一行代码记录了操作日志，接下来再看一个更复杂的例子：

```java
private OnesIssueDO updateAddress(updateDeliveryRequest request) {
    DeliveryOrder deliveryOrder = deliveryQueryService.queryOldAddress(request.getDeliveryOrderNo());
    // 更新派送信息，电话，收件人，地址
    doUpdate(request);
    String logContent = getLogContent(request, deliveryOrder);
    LogUtils.logRecord(request.getOrderNo(), logContent, request.getOperator);
    return onesIssueDO;
}

private String getLogContent(updateDeliveryRequest request, DeliveryOrder deliveryOrder) {
    String template = "用户%s修改了订单的配送地址：从“%s”修改到“%s”";
    return String.format(tempalte, request.getUserName(), deliveryOrder.getAddress(), request.getAddress);
}
```

可以看到上面的例子使用了两个方法代码，外加一个 getLogContent 的函数实现了操作日志的记录。当业务变得复杂后，记录操作日志放在业务代码中会导致业务的逻辑比较繁杂，最后导致 LogUtils.logRecord() 方法的调用存在于很多业务的代码中，而且类似 getLogContent() 这样的方法也散落在各个业务类中，对于代码的可读性和可维护性来说是一个灾难。下面介绍下如何避免这个灾难。

### 2.4 方法注解实现操作日志

为了解决上面问题，一般采用 AOP 的方式记录日志，让操作日志和业务逻辑解耦，接下来看一个简单的 AOP 日志的例子。

```java
@LogRecord(content="修改了配送地址")
public void modifyAddress(updateDeliveryRequest request){
    // 更新派送信息 电话，收件人、地址
    doUpdate(request);
}
```

我们可以在注解的操作日志上记录固定文案，这样业务逻辑和业务代码可以做到解耦，让我们的业务代码变得纯净起来。可能有同学注意到，上面的方式虽然解耦了操作日志的代码，但是记录的文案并不符合我们的预期，文案是静态的，没有包含动态的文案，因为我们需要记录的操作日志是： 用户%s修改了订单的配送地址，从“%s”修改到“%s”。接下来，我们介绍一下如何优雅地使用 AOP 生成动态的操作日志。

## 3. 优雅地支持 AOP 生成动态的操作日志

### 3.1 动态模板

一提到动态模板，就会涉及到让变量通过占位符的方式解析模板，从而达到通过注解记录操作日志的目的。模板解析的方式有很多种，这里使用了 SpEL（Spring Expression Language，Spring表达式语言）来实现。我们可以先写下期望的记录日志的方式，然后再看下能否实现这样的功能。

```java
@LogRecord(content = "修改了订单的配送地址：从“#oldAddress”, 修改到“#request.address”")
public void modifyAddress(updateDeliveryRequest request, String oldAddress){
    // 更新派送信息 电话，收件人、地址
    doUpdate(request);
}
```

通过 SpEL 表达式引用方法上的参数，可以让变量填充到模板中达到动态的操作日志文本内容。 但是现在还有几个问题需要解决： * 操作日志需要知道是哪个操作人修改的订单配送地址。 * 修改订单配送地址的操作日志需要绑定在配送的订单上，从而可以根据配送订单号查询出对这个配送订单的所有操作。 * 为了在注解上记录之前的配送地址是什么，在方法签名上添加了一个和业务无关的 oldAddress 的变量，这样就不优雅了。

为了解决前两个问题，我们需要把期望的操作日志使用形式改成下面的方式：

```java
@LogRecord(
     content = "修改了订单的配送地址：从“#oldAddress”, 修改到“#request.address”",
     operator = "#request.userName", bizNo="#request.deliveryOrderNo")
public void modifyAddress(updateDeliveryRequest request, String oldAddress){
    // 更新派送信息 电话，收件人、地址
    doUpdate(request);
}
```

修改后的代码在注解上添加两个参数，一个是操作人，一个是操作日志需要绑定的对象。但是，在普通的 Web 应用中用户信息都是保存在一个线程上下文的静态方法中，所以 operator 一般是这样的写法（假定获取当前登陆用户的方式是 UserContext.getCurrentUser()）。

```java
operator = "#{T(com.meituan.user.UserContext).getCurrentUser()}"
```

这样的话，每个 @LogRecord 的注解上的操作人都是这么长一串。为了避免过多的重复代码，我们可以把注解上的 operator 参数设置为非必填，这样用户可以填写操作人。但是，如果用户不填写我们就取 UserContext 的 user（下文会介绍如何取 user ）。最后，最简单的日志变成了下面的形式：

```java
@LogRecord(content = "修改了订单的配送地址：从“#oldAddress”, 修改到“#request.address”", 
           bizNo="#request.deliveryOrderNo")
public void modifyAddress(updateDeliveryRequest request, String oldAddress){
    // 更新派送信息 电话，收件人、地址
    doUpdate(request);
}
```

接下来，我们需要解决第三个问题：为了记录业务操作记录添加了一个 oldAddress 变量，不管怎么样这都不是一个好的实现方式，所以接下来，我们需要把 oldAddress 变量从修改地址的方法签名上去掉。但是操作日志确实需要 oldAddress 变量，怎么办呢？

要么和产品经理 PK 一下，让产品经理把文案从“修改了订单的配送地址：从 xx 修改到 yy” 改为 “修改了订单的配送地址为：yy”。但是从用户体验上来看，第一种文案更人性化一些，显然我们不会 PK 成功的。那么我们就必须要把这个 oldAddress 查询出来然后供操作日志使用了。还有一种解决办法是：把这个参数放到操作日志的线程上下文中，供注解上的模板使用。我们按照这个思路再改下操作日志的实现代码。

```java
@LogRecord(content = "修改了订单的配送地址：从“#oldAddress”, 修改到“#request.address”",
        bizNo="#request.deliveryOrderNo")
public void modifyAddress(updateDeliveryRequest request){
    // 查询出原来的地址是什么
    LogRecordContext.putVariable("oldAddress", DeliveryService.queryOldAddress(request.getDeliveryOrderNo()));
    // 更新派送信息 电话，收件人、地址
    doUpdate(request);
}
```

这时候可以看到，LogRecordContext 解决了操作日志模板上使用方法参数以外变量的问题，同时避免了为了记录操作日志修改方法签名的设计。虽然已经比之前的代码好了些，但是依然需要在业务代码里面加了一行业务逻辑无关的代码，如果有“强迫症”的同学还可以继续往下看，接下来我们会讲解自定义函数的解决方案。下面再看另一个例子：

```java
@LogRecord(content = "修改了订单的配送员：从“#oldDeliveryUserId”, 修改到“#request.userId”",
        bizNo="#request.deliveryOrderNo")
public void modifyAddress(updateDeliveryRequest request){
    // 查询出原来的地址是什么
    LogRecordContext.putVariable("oldDeliveryUserId", DeliveryService.queryOldDeliveryUserId(request.getDeliveryOrderNo()));
    // 更新派送信息 电话，收件人、地址
    doUpdate(request);
}
```

这个操作日志的模板最后记录的内容是这样的格式：修改了订单的配送员：从 “10090”，修改到 “10099”，显然用户看到这样的操作日志是不明白的。用户对于用户 ID 是 10090 还是 10099 并不了解，用户期望看到的是：修改了订单的配送员：从“张三（18910008888）”，修改到“小明（13910006666）”。用户关心的是配送员的姓名和电话。但是我们方法中传递的参数只有配送员的 ID，没有配送员的姓名可电话。我们可以通过上面的方法，把用户的姓名和电话查询出来，然后通过 LogRecordContext 实现。

但是，“强迫症”是不期望操作日志的代码嵌入在业务逻辑中的。接下来，我们考虑另一种实现方式：自定义函数。如果我们可以通过自定义函数把用户 ID 转换为用户姓名和电话，那么就能解决这一问题，按照这个思路，我们把模板修改为下面的形式：

```java
@LogRecord(content = "修改了订单的配送员：从“{deliveryUser{#oldDeliveryUserId}}”, 修改到“{deveryUser{#request.userId}}”",
        bizNo="#request.deliveryOrderNo")
public void modifyAddress(updateDeliveryRequest request){
    // 查询出原来的地址是什么
    LogRecordContext.putVariable("oldDeliveryUserId", DeliveryService.queryOldDeliveryUserId(request.getDeliveryOrderNo()));
    // 更新派送信息 电话，收件人、地址
    doUpdate(request);
}
```

其中 deliveryUser 是自定义函数，使用大括号把 Spring 的 SpEL 表达式包裹起来，这样做的好处：一是把 SpEL（Spring Expression Language，Spring表达式语言）和自定义函数区分开便于解析；二是如果模板中不需要 SpEL 表达式解析可以容易的识别出来，减少 SpEL 的解析提高性能。这时候我们发现上面代码还可以优化成下面的形式：

```java
@LogRecord(content = "修改了订单的配送员：从“{queryOldUser{#request.deliveryOrderNo()}}”, 修改到“{deveryUser{#request.userId}}”",
        bizNo="#request.deliveryOrderNo")
public void modifyAddress(updateDeliveryRequest request){
    // 更新派送信息 电话，收件人、地址
    doUpdate(request);
}
```

这样就不需要在 modifyAddress 方法中通过 LogRecordContext.putVariable() 设置老的快递员了，通过直接新加一个自定义函数 queryOldUser() 参数把派送订单传递进去，就能查到之前的配送人了，只需要让方法的解析在 modifyAddress() 方法执行之前运行。这样的话，我们让业务代码又变得纯净了起来，同时也让“强迫症”不再感到难受了。

## 4. 代码实现解析

### 4.1 代码结构

![img](https://p1.meituan.net/travelcube/bde9c178c76e131cefae3e7d7fcf428993663.png)

上面的操作日志主要是通过一个 AOP 拦截器实现的，整体主要分为 AOP 模块、日志解析模块、日志保存模块、Starter 模块；组件提供了4个扩展点，分别是：自定义函数、默认处理人、业务保存和查询；业务可以根据自己的业务特性定制符合自己业务的逻辑。

### 4.2 模块介绍

有了上面的分析，已经得出一种我们期望的操作日志记录的方式，那么接下来看看如何实现上面的逻辑。实现主要分为下面几个步骤： * AOP 拦截逻辑 * 解析逻辑 * 模板解析 * LogContext 逻辑 * 默认的 operator 逻辑 * 自定义函数逻辑 * 默认的日志持久化逻辑 * Starter 封装逻辑

#### 4.2.1 AOP 拦截逻辑

这块逻辑主要是一个拦截器，针对 @LogRecord 注解分析出需要记录的操作日志，然后把操作日志持久化，这里把注解命名为 @LogRecordAnnotation。接下来，我们看下注解的定义：

```java
@Target({ElementType.METHOD})
@Retention(RetentionPolicy.RUNTIME)
@Inherited
@Documented
public @interface LogRecordAnnotation {
    String success();

    String fail() default "";

    String operator() default "";

    String bizNo();

    String category() default "";

    String detail() default "";

    String condition() default "";
}
```

注解中除了上面提到参数外，还增加了 fail、category、detail、condition 等参数，这几个参数是为了满足特定的场景，后面还会给出具体的例子。

| 参数名    | 描述                             | 是否必填 |
| :-------- | :------------------------------- | :------- |
| success   | 操作日志的文本模板               | 是       |
| fail      | 操作日志失败的文本版本           | 否       |
| operator  | 操作日志的执行人                 | 否       |
| bizNo     | 操作日志绑定的业务对象标识       | 是       |
| category  | 操作日志的种类                   | 否       |
| detail    | 扩展参数，记录操作日志的修改详情 | 否       |
| condition | 记录日志的条件                   | 否       |

为了保持简单，组件的必填参数就两个。业务中的 AOP 逻辑大部分是使用 @Aspect 注解实现的，但是基于注解的 AOP 在 Spring boot 1.5 中兼容性是有问题的，组件为了兼容 Spring boot1.5 的版本我们手工实现 Spring 的 AOP 逻辑。

![img](https://p1.meituan.net/travelcube/73d57fecf2ae9ae16e0b0879e58d005e30455.png)

切面选择 `AbstractBeanFactoryPointcutAdvisor` 实现，切点是通过 `StaticMethodMatcherPointcut` 匹配包含 `LogRecordAnnotation` 注解的方法。通过实现 `MethodInterceptor` 接口实现操作日志的增强逻辑。

下面是拦截器的切点逻辑：

```java
public class LogRecordPointcut extends StaticMethodMatcherPointcut implements Serializable {
    // LogRecord的解析类
    private LogRecordOperationSource logRecordOperationSource;
    
    @Override
    public boolean matches(@NonNull Method method, @NonNull Class<?> targetClass) {
          // 解析 这个 method 上有没有 @LogRecordAnnotation 注解，有的话会解析出来注解上的各个参数
        return !CollectionUtils.isEmpty(logRecordOperationSource.computeLogRecordOperations(method, targetClass));
    }

    void setLogRecordOperationSource(LogRecordOperationSource logRecordOperationSource) {
        this.logRecordOperationSource = logRecordOperationSource;
    }
}
```

切面的增强逻辑主要代码如下：

```java
@Override
public Object invoke(MethodInvocation invocation) throws Throwable {
    Method method = invocation.getMethod();
    // 记录日志
    return execute(invocation, invocation.getThis(), method, invocation.getArguments());
}

private Object execute(MethodInvocation invoker, Object target, Method method, Object[] args) throws Throwable {
    Class<?> targetClass = getTargetClass(target);
    Object ret = null;
    MethodExecuteResult methodExecuteResult = new MethodExecuteResult(true, null, "");
    LogRecordContext.putEmptySpan();
    Collection<LogRecordOps> operations = new ArrayList<>();
    Map<String, String> functionNameAndReturnMap = new HashMap<>();
    try {
        operations = logRecordOperationSource.computeLogRecordOperations(method, targetClass);
        List<String> spElTemplates = getBeforeExecuteFunctionTemplate(operations);
        //业务逻辑执行前的自定义函数解析
        functionNameAndReturnMap = processBeforeExecuteFunctionTemplate(spElTemplates, targetClass, method, args);
    } catch (Exception e) {
        log.error("log record parse before function exception", e);
    }
    try {
        ret = invoker.proceed();
    } catch (Exception e) {
        methodExecuteResult = new MethodExecuteResult(false, e, e.getMessage());
    }
    try {
        if (!CollectionUtils.isEmpty(operations)) {
            recordExecute(ret, method, args, operations, targetClass,
                    methodExecuteResult.isSuccess(), methodExecuteResult.getErrorMsg(), functionNameAndReturnMap);
        }
    } catch (Exception t) {
        //记录日志错误不要影响业务
        log.error("log record parse exception", t);
    } finally {
        LogRecordContext.clear();
    }
    if (methodExecuteResult.throwable != null) {
        throw methodExecuteResult.throwable;
    }
    return ret;
}
```

拦截逻辑的流程：

![img](https://p0.meituan.net/travelcube/697eba86de7419230bdccdfb986b6cd550305.png)

可以看到，操作日志的记录持久化是在方法执行完之后执行的，当方法抛出异常之后会先捕获异常，等操作日志持久化完成后再抛出异常。在业务的方法执行之前，会对提前解析的自定义函数求值，解决了前面提到的需要查询修改之前的内容。

#### 4.2.2 解析逻辑

**模板解析**

Spring 3 提供了一个非常强大的功能：Spring EL，SpEL 在 Spring 产品中是作为表达式求值的核心基础模块，它本身是可以脱离 Spring 独立使用的。举个例子：

```java
public static void main(String[] args) {
        SpelExpressionParser parser = new SpelExpressionParser();
        Expression expression = parser.parseExpression("#root.purchaseName");
        Order order = new Order();
        order.setPurchaseName("张三");
        System.out.println(expression.getValue(order));
}
```

这个方法将打印 “张三”。LogRecord 解析的类图如下：

![img](https://p0.meituan.net/travelcube/49f3049c4d965d23ab14c80840c3356753525.png)

**解析核心类**：`LogRecordValueParser` 里面封装了自定义函数和 SpEL 解析类 `LogRecordExpressionEvaluator`。

```java
public class LogRecordExpressionEvaluator extends CachedExpressionEvaluator {

    private Map<ExpressionKey, Expression> expressionCache = new ConcurrentHashMap<>(64);

    private final Map<AnnotatedElementKey, Method> targetMethodCache = new ConcurrentHashMap<>(64);

    public String parseExpression(String conditionExpression, AnnotatedElementKey methodKey, EvaluationContext evalContext) {
        return getExpression(this.expressionCache, methodKey, conditionExpression).getValue(evalContext, String.class);
    }
}
```

`LogRecordExpressionEvaluator` 继承自 `CachedExpressionEvaluator` 类，这个类里面有两个 Map，一个是 expressionCache 一个是 targetMethodCache。在上面的例子中可以看到，SpEL 会解析成一个 Expression 表达式，然后根据传入的 Object 获取到对应的值，所以 expressionCache 是为了缓存方法、表达式和 SpEL 的 Expression 的对应关系，让方法注解上添加的 SpEL 表达式只解析一次。 下面的 targetMethodCache 是为了缓存传入到 Expression 表达式的 Object。核心的解析逻辑是上面最后一行代码。

```java
getExpression(this.expressionCache, methodKey, conditionExpression).getValue(evalContext, String.class);
```

`getExpression` 方法会从 expressionCache 中获取到 @LogRecordAnnotation 注解上的表达式的解析 Expression 的实例，然后调用 `getValue` 方法，`getValue` 传入一个 evalContext 就是类似上面例子中的 order 对象。其中 Context 的实现将会在下文介绍。

**日志上下文实现**

下面的例子把变量放到了 LogRecordContext 中，然后 SpEL 表达式就可以顺利的解析方法上不存在的参数了，通过上面的 SpEL 的例子可以看出，要把方法的参数和 LogRecordContext 中的变量都放到 SpEL 的 `getValue` 方法的 Object 中才可以顺利的解析表达式的值。下面看下如何实现：

```java
@LogRecord(content = "修改了订单的配送员：从“{deveryUser{#oldDeliveryUserId}}”, 修改到“{deveryUser{#request.getUserId()}}”",
            bizNo="#request.getDeliveryOrderNo()")
public void modifyAddress(updateDeliveryRequest request){
    // 查询出原来的地址是什么
    LogRecordContext.putVariable("oldDeliveryUserId", DeliveryService.queryOldDeliveryUserId(request.getDeliveryOrderNo()));
    // 更新派送信息 电话，收件人、地址
    doUpdate(request);
}
```

在 LogRecordValueParser 中创建了一个 EvaluationContext，用来给 SpEL 解析方法参数和 Context 中的变量。相关代码如下：

```java
EvaluationContext evaluationContext = expressionEvaluator.createEvaluationContext(method, args, targetClass, ret, errorMsg, beanFactory);
```

在解析的时候调用 `getValue` 方法传入的参数 evalContext，就是上面这个 EvaluationContext 对象。下面是 LogRecordEvaluationContext 对象的继承体系：

![img](https://p1.meituan.net/travelcube/611f884a134745a0c9eaadcb84e9bbc873694.png)

LogRecordEvaluationContext 做了三个事情： * 把方法的参数都放到 SpEL 解析的 RootObject 中。 * 把 LogRecordContext 中的变量都放到 RootObject 中。 * 把方法的返回值和 ErrorMsg 都放到 RootObject 中。

LogRecordEvaluationContext 的代码如下：

```java
public class LogRecordEvaluationContext extends MethodBasedEvaluationContext {

    public LogRecordEvaluationContext(Object rootObject, Method method, Object[] arguments,
                                      ParameterNameDiscoverer parameterNameDiscoverer, Object ret, String errorMsg) {
       //把方法的参数都放到 SpEL 解析的 RootObject 中
       super(rootObject, method, arguments, parameterNameDiscoverer);
       //把 LogRecordContext 中的变量都放到 RootObject 中
        Map<String, Object> variables = LogRecordContext.getVariables();
        if (variables != null && variables.size() > 0) {
            for (Map.Entry<String, Object> entry : variables.entrySet()) {
                setVariable(entry.getKey(), entry.getValue());
            }
        }
        //把方法的返回值和 ErrorMsg 都放到 RootObject 中
        setVariable("_ret", ret);
        setVariable("_errorMsg", errorMsg);
    }
}
```

下面是 LogRecordContext 的实现，这个类里面通过一个 ThreadLocal 变量保持了一个栈，栈里面是个 Map，Map 对应了变量的名称和变量的值。

```java
public class LogRecordContext {

    private static final InheritableThreadLocal<Stack<Map<String, Object>>> variableMapStack = new InheritableThreadLocal<>();
   //其他省略....
}
```

上面使用了 InheritableThreadLocal，所以在线程池的场景下使用 LogRecordContext 会出现问题，如果支持线程池可以使用阿里巴巴开源的 TTL 框架。那这里为什么不直接设置一个 ThreadLocal> 对象，而是要设置一个 Stack 结构呢？我们看一下这么做的原因是什么。

```java
@LogRecord(content = "修改了订单的配送员：从“{deveryUser{#oldDeliveryUserId}}”, 修改到“{deveryUser{#request.getUserId()}}”",
        bizNo="#request.getDeliveryOrderNo()")
public void modifyAddress(updateDeliveryRequest request){
    // 查询出原来的地址是什么
    LogRecordContext.putVariable("oldDeliveryUserId", DeliveryService.queryOldDeliveryUserId(request.getDeliveryOrderNo()));
    // 更新派送信息 电话，收件人、地址
    doUpdate(request);
}
```

上面代码的执行流程如下：

![img](https://p1.meituan.net/travelcube/45b7cacd228bc3cf835a8a2a83eb50fa90211.png)

看起来没有什么问题，但是使用 LogRecordAnnotation 的方法里面嵌套了另一个使用 LogRecordAnnotation 方法的时候，流程就变成下面的形式：

![img](https://p0.meituan.net/travelcube/9c91465d4674a4d233840ce82d9390f7143948.png)

可以看到，当方法二执行了释放变量后，继续执行方法一的 logRecord 逻辑，此时解析的时候 ThreadLocal>的 Map 已经被释放掉，所以方法一就获取不到对应的变量了。方法一和方法二共用一个变量 Map 还有个问题是：如果方法二设置了和方法一相同的变量两个方法的变量就会被相互覆盖。所以最终 LogRecordContext 的变量的生命周期需要是下面的形式：

![img](https://p0.meituan.net/travelcube/6e115c8a747b371915ec9760948346a231049.png)

LogRecordContext 每执行一个方法都会压栈一个 Map，方法执行完之后会 Pop 掉这个 Map，从而避免变量共享和覆盖问题。

**默认操作人逻辑**

在 LogRecordInterceptor 中 IOperatorGetService 接口，这个接口可以获取到当前的用户。下面是接口的定义：

```java
public interface IOperatorGetService {

    /**
     * 可以在里面外部的获取当前登陆的用户，比如 UserContext.getCurrentUser()
     *
     * @return 转换成Operator返回
     */
    Operator getUser();
}
```

下面给出了从用户上下文中获取用户的例子：

```java
public class DefaultOperatorGetServiceImpl implements IOperatorGetService {

    @Override
    public Operator getUser() {
    //UserUtils 是获取用户上下文的方法
         return Optional.ofNullable(UserUtils.getUser())
                        .map(a -> new Operator(a.getName(), a.getLogin()))
                        .orElseThrow(()->new IllegalArgumentException("user is null"));
        
    }
}
```

组件在解析 operator 的时候，就判断注解上的 operator 是否是空，如果注解上没有指定，我们就从 IOperatorGetService 的 getUser 方法获取了。如果都获取不到，就会报错。

```java
String realOperatorId = "";
if (StringUtils.isEmpty(operatorId)) {
    if (operatorGetService.getUser() == null || StringUtils.isEmpty(operatorGetService.getUser().getOperatorId())) {
        throw new IllegalArgumentException("user is null");
    }
    realOperatorId = operatorGetService.getUser().getOperatorId();
} else {
    spElTemplates = Lists.newArrayList(bizKey, bizNo, action, operatorId, detail);
}
```

**自定义函数逻辑**

自定义函数的类图如下：

![img](https://p0.meituan.net/travelcube/14f29b09174a43fe0fff3e67226de25766597.png)

下面是 IParseFunction 的接口定义：`executeBefore` 函数代表了自定义函数是否在业务代码执行之前解析，上面提到的查询修改之前的内容。

```java
public interface IParseFunction {

  default boolean executeBefore(){
    return false;
  }

  String functionName();

  String apply(String value);
}
```

ParseFunctionFactory 的代码比较简单，它的功能是把所有的 IParseFunction 注入到函数工厂中。

```java
public class ParseFunctionFactory {
  private Map<String, IParseFunction> allFunctionMap;

  public ParseFunctionFactory(List<IParseFunction> parseFunctions) {
    if (CollectionUtils.isEmpty(parseFunctions)) {
      return;
    }
    allFunctionMap = new HashMap<>();
    for (IParseFunction parseFunction : parseFunctions) {
      if (StringUtils.isEmpty(parseFunction.functionName())) {
        continue;
      }
      allFunctionMap.put(parseFunction.functionName(), parseFunction);
    }
  }

  public IParseFunction getFunction(String functionName) {
    return allFunctionMap.get(functionName);
  }

  public boolean isBeforeFunction(String functionName) {
    return allFunctionMap.get(functionName) != null && allFunctionMap.get(functionName).executeBefore();
  }
}
```

DefaultFunctionServiceImpl 的逻辑就是根据传入的函数名称 functionName 找到对应的 IParseFunction，然后把参数传入到 IParseFunction 的 `apply` 方法上最后返回函数的值。

```java
public class DefaultFunctionServiceImpl implements IFunctionService {

  private final ParseFunctionFactory parseFunctionFactory;

  public DefaultFunctionServiceImpl(ParseFunctionFactory parseFunctionFactory) {
    this.parseFunctionFactory = parseFunctionFactory;
  }

  @Override
  public String apply(String functionName, String value) {
    IParseFunction function = parseFunctionFactory.getFunction(functionName);
    if (function == null) {
      return value;
    }
    return function.apply(value);
  }

  @Override
  public boolean beforeFunction(String functionName) {
    return parseFunctionFactory.isBeforeFunction(functionName);
  }
}
```

#### 4.2.3 日志持久化逻辑

同样在 LogRecordInterceptor 的代码中引用了 ILogRecordService，这个 Service 主要包含了日志记录的接口。

```java
public interface ILogRecordService {
    /**
     * 保存 log
     *
     * @param logRecord 日志实体
     */
    void record(LogRecord logRecord);

}
```

业务可以实现这个保存接口，然后把日志保存在任何存储介质上。这里给了一个 2.2 节介绍的通过 log.info 保存在日志文件中的例子，业务可以把保存设置成异步或者同步，可以和业务放在一个事务中保证操作日志和业务的一致性，也可以新开辟一个事务，保证日志的错误不影响业务的事务。业务可以保存在 Elasticsearch、数据库或者文件中，用户可以根据日志结构和日志的存储实现相应的查询逻辑。

```java
@Slf4j
public class DefaultLogRecordServiceImpl implements ILogRecordService {

    @Override
//    @Transactional(propagation = Propagation.REQUIRES_NEW)
    public void record(LogRecord logRecord) {
        log.info("【logRecord】log={}", logRecord);
    }
}
```

#### 4.2.4 Starter 逻辑封装

上面逻辑代码已经介绍完毕，那么接下来需要把这些组件组装起来，然后让用户去使用。在使用这个组件的时候只需要在 Springboot 的入口上添加一个注解 @EnableLogRecord(tenant = “com.mzt.test”)。其中 tenant 代表租户，是为了多租户使用的。

```java
@SpringBootApplication(exclude = DataSourceAutoConfiguration.class)
@EnableTransactionManagement
@EnableLogRecord(tenant = "com.mzt.test")
public class Main {

    public static void main(String[] args) {
        SpringApplication.run(Main.class, args);
    }
}
```

再看下 EnableLogRecord 的代码，代码中 Import 了 `LogRecordConfigureSelector.class`，在 `LogRecordConfigureSelector` 类中暴露了 `LogRecordProxyAutoConfiguration` 类。

```java
@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@Documented
@Import(LogRecordConfigureSelector.class)
public @interface EnableLogRecord {

    String tenant();
    
    AdviceMode mode() default AdviceMode.PROXY;
}
```

`LogRecordProxyAutoConfiguration` 就是装配上面组件的核心类了，代码如下：

```java
@Configuration
@Slf4j
public class LogRecordProxyAutoConfiguration implements ImportAware {

  private AnnotationAttributes enableLogRecord;


  @Bean
  @Role(BeanDefinition.ROLE_INFRASTRUCTURE)
  public LogRecordOperationSource logRecordOperationSource() {
    return new LogRecordOperationSource();
  }

  @Bean
  @ConditionalOnMissingBean(IFunctionService.class)
  public IFunctionService functionService(ParseFunctionFactory parseFunctionFactory) {
    return new DefaultFunctionServiceImpl(parseFunctionFactory);
  }

  @Bean
  public ParseFunctionFactory parseFunctionFactory(@Autowired List<IParseFunction> parseFunctions) {
    return new ParseFunctionFactory(parseFunctions);
  }

  @Bean
  @ConditionalOnMissingBean(IParseFunction.class)
  public DefaultParseFunction parseFunction() {
    return new DefaultParseFunction();
  }


  @Bean
  @Role(BeanDefinition.ROLE_INFRASTRUCTURE)
  public BeanFactoryLogRecordAdvisor logRecordAdvisor(IFunctionService functionService) {
    BeanFactoryLogRecordAdvisor advisor =
            new BeanFactoryLogRecordAdvisor();
    advisor.setLogRecordOperationSource(logRecordOperationSource());
    advisor.setAdvice(logRecordInterceptor(functionService));
    return advisor;
  }

  @Bean
  @Role(BeanDefinition.ROLE_INFRASTRUCTURE)
  public LogRecordInterceptor logRecordInterceptor(IFunctionService functionService) {
    LogRecordInterceptor interceptor = new LogRecordInterceptor();
    interceptor.setLogRecordOperationSource(logRecordOperationSource());
    interceptor.setTenant(enableLogRecord.getString("tenant"));
    interceptor.setFunctionService(functionService);
    return interceptor;
  }

  @Bean
  @ConditionalOnMissingBean(IOperatorGetService.class)
  @Role(BeanDefinition.ROLE_APPLICATION)
  public IOperatorGetService operatorGetService() {
    return new DefaultOperatorGetServiceImpl();
  }

  @Bean
  @ConditionalOnMissingBean(ILogRecordService.class)
  @Role(BeanDefinition.ROLE_APPLICATION)
  public ILogRecordService recordService() {
    return new DefaultLogRecordServiceImpl();
  }

  @Override
  public void setImportMetadata(AnnotationMetadata importMetadata) {
    this.enableLogRecord = AnnotationAttributes.fromMap(
            importMetadata.getAnnotationAttributes(EnableLogRecord.class.getName(), false));
    if (this.enableLogRecord == null) {
      log.info("@EnableCaching is not present on importing class");
    }
  }
}
```

这个类继承 ImportAware 是为了拿到 EnableLogRecord 上的租户属性，这个类使用变量 logRecordAdvisor 和 logRecordInterceptor 装配了 AOP，同时把自定义函数注入到了 logRecordAdvisor 中。

**对外扩展类**：分别是`IOperatorGetService`、`ILogRecordService`、`IParseFunction`。业务可以自己实现相应的接口，因为配置了 @ConditionalOnMissingBean，所以用户的实现类会覆盖组件内的默认实现。

## 5. 总结

这篇文章介绍了操作日志的常见写法，以及如何让操作日志的实现更加简单、易懂；通过组件的四个模块，介绍了组件的具体实现。对于上面的组件介绍，大家如果有疑问，也欢迎在文末留言，我们会进行答疑。

## 6. 作者简介

站通，2020年加入美团，基础研发平台/研发质量及效率部工程师。

## 7. 参考资料

- [Canal](https://github.com/alibaba/canal)
- [spring-framework](https://spring.io/projects/spring-framework)
- [Spring Expression Language (SpEL)](https://docs.spring.io/spring-framework/docs/3.2.x/spring-framework-reference/html/expressions.html)
- [ThreadLocal、InheritableThreadLocal、TransmittableThreadLocal三者之间区别](https://blog.csdn.net/weixin_43954303/article/details/113837928?spm=1001.2014.3001.5501)

## 8. 招聘信息

美团研发质量及效率部 ，致力于建设业界一流的持续交付平台，现招聘基础组件方向相关的工程师，坐标北京/上海。欢迎感兴趣的同学加入。可投递简历至：chao.yu@meituan.com（邮件主题请注明：美团研发质量及效率部）。

原文作者：美团技术团队

原文链接：https://tech.meituan.com/2021/09/16/operational-logbook.html

# 【NO.252】美团基于知识图谱的剧本杀标准化建设与应用

剧本杀作为爆发式增长的新兴业务，在商家上单、用户选购、供需匹配等方面存在不足，供给标准化能为用户、商家、平台三方创造价值，助力业务增长。本文介绍了美团到店综合业务数据团队从0到1快速建设剧本杀供给标准化的过程及算法方案。我们将美团到店综合知识图谱（GENE，GEneral NEeds net）覆盖至剧本杀行业，构建剧本杀知识图谱实现供给标准化建设，包括剧本杀供给挖掘、标准剧本库构建、供给与标准剧本关联等环节，并在多个场景进行应用落地，希望给大家带来一些帮助或启发。

## 1.背景

剧本杀行业近年来呈爆发式增长态势，然而由于剧本杀是新兴行业，平台已有的类目体系和产品形态，越来越难以满足飞速增长的用户和商户需求，主要表现在下面三个方面：

- **平台类目缺失**：平台缺少专门的“剧本杀”类目，中心化流量入口的缺失，导致用户决策路径混乱，难以建立统一的用户认知。
- **用户决策效率低**：剧本杀的核心是剧本，由于缺乏标准的剧本库，也未建立标准剧本和供给的关联关系，导致剧本信息展示和供给管理的规范化程度低，影响了用户对剧本选择决策的效率。
- **商品上架繁琐**：商品信息需要商户人工一一录入，没有可用的标准模板用以信息预填，导致商户在平台上架的剧本比例偏低，上架效率存在较大的提升空间。

为了解决上述痛点，业务需要进行剧本杀的供给标准化建设：首先建立“剧本杀”新类目，并完成相应的供给（包括商户、商品、内容）的类目迁移。以此为基础，以剧本为核心，搭建标准剧本库，并关联剧本杀供给，继而建立剧本维度的信息分发渠道、评价评分和榜单体系，满足用户“以剧本找店”的决策路径。

值得指出的是，供给标准化是简化用户认知、帮助用户决策、促进供需匹配的重要抓手，标准化程度的高低对平台业务规模的大小有着决定性影响。具体到剧本杀行业，供给标准化建设是助力剧本杀业务持续增长的重要基础，而标准剧本库的搭建是剧本杀供给标准化的关键。由于基于规格如「城限」、背景如「古风」、题材如「情感」等剧本属性无法确定具体的剧本，但剧本名称如「舍离」则能起唯一标识的作用。因此，标准剧本库的搭建，首先是标准剧本名称的建设，其次是规格、背景、题材、难度、流派等标准剧本属性的建设。

综上，美团到店综合业务数据团队与业务同行，助力业务进行剧本杀的供给标准化建设。在建设过程中，涉及了剧本名称、剧本属性、类目、商户、商品、内容等多种类型的实体，以及它们之间的多元化关系构建。而知识图谱作为一种揭示实体及实体间关系的语义网络，用以解决该问题显得尤为合适。特别地，我们已经构建了**美团到店综合知识图谱（GENE，GEneral NEeds net）**，因此，我们基于GENE的构建经验快速进行剧本杀这一新业务的知识图谱构建，从0到1实现剧本杀标准化建设，从而改善供给管理和供需匹配，为用户、商户、平台三方创造出更大的价值。

## 2.解决方案

我们构建的GENE，围绕本地生活用户的综合性需求，以行业体系、需求对象、具象需求、场景要素和场景需求五个层次逐层递进，覆盖了玩乐、医美、教育、亲子、结婚等多个业务，体系设计和技术细节可见[美团到店综合知识图谱](https://mp.weixin.qq.com/s/wKZJ3toGlDQM5PKvNj7I7w)相关的文章。剧本杀作为一项新兴的美团到店综合业务，体现了用户在玩乐上的新需求，天然适配GENE的体系结构。因此，我们将GENE覆盖至剧本杀新业务，沿用相同的思路来进行相应知识图谱的构建，以实现相应的供给标准化。

基于知识图谱来实现剧本杀标准化建设的关键，是以标准剧本为核心构建剧本杀知识图谱。图谱体系设计如图1所示，具体地，首先在行业体系层进行剧本杀新类目的构建，挖掘剧本杀供给，并建立供给（包括商户、商品、内容）与类目的从属关系。在此基础上，在需求对象层，进一步实现标准剧本名称这一核心对象节点和其剧本属性节点的挖掘以及关系构建，建立标准剧本库，最后将标准剧本库的每个标准剧本与供给和用户建立关联关系。此外，具象需求、场景要素、场景需求三层则实现了对用户在剧本杀上的具象的服务需求和场景化需求的显性表达，这部分由于与剧本杀供给标准化建设的联系不多，在这里不做展开介绍。

![图 1](https://p0.meituan.net/travelcube/85f8afccbfc0027d184612f057846976160102.png)

图 1



剧本杀知识图谱中用于供给标准化部分的具体样例如下图2所示。其中，标准剧本名称是核心节点，围绕它的各类标准剧本属性节点包括题材、规格、流派、难度、背景、别称等。同时，标准剧本之间可能构建诸如“同系列”等类型的关系，比如「舍离」和「舍离2」。此外，标准剧本还会与商品、商户、内容、用户之间建立关联关系。

我们基于剧本杀知识图谱的这些节点和关系进行供给标准化，在图谱构建过程中，包括了**剧本杀供给挖掘**、**标准剧本库构建**、**供给与标准剧本关联**三个主要步骤，下面对三个步骤的实现细节以及涉及的算法进行介绍。

![图 2](https://p1.meituan.net/travelcube/20c11ad1e4722cb880cecd30512cbb51377721.png)

图 2



## 3.实现方法

### 3.1 剧本杀供给挖掘

剧本杀作为新兴的业务，已有的行业类目树中并没有相应的类目，无法直接根据类目获取剧本杀的相关供给（包括商户、商品和内容）。因此，我们需要首先进行剧本杀供给的挖掘，即从当前与剧本杀行业相近类目的供给中挖掘出剧本杀的相关供给。

对于剧本杀的商户供给挖掘，需要判断商户是否提供剧本杀服务，判别依据包括了商户名、商品名及商品详情、商户UGC三个来源的文本语料。这个本质上是一个多源数据的分类问题，然而由于缺乏标注的训练样本，我们没有直接采用端到端的多源数据分类模型，而是依托业务输入，采用无监督匹配和有监督拟合相结合的方式高效实现，具体的判别流程如下图3所示，其中：

- **无监督匹配**：首先构造剧本杀相关的关键词词库，分别在商户名、商品名及商品详情、商户UGC三个来源的文本语料中进行精确匹配，并构建基于BERT[1]的通用语义漂移判别模型进行匹配结果过滤。最后根据业务规则基于各来源的匹配结果计算相应的匹配分数。
- **有监督拟合**：为了量化不同来源匹配分数对最终判别结果的影响，由运营先人工标注少量商户分数，用以表征商户提供剧本杀服务的强弱。在此基础上，我们构造了一个线性回归模型，拟合标注的商户分数，获取各来源的权重，从而实现对剧本杀商户的精准挖掘。

![图 3](https://p0.meituan.net/travelcube/ecbe5a7f06932347787dfba7c58d3e1e88910.png)

图 3



采用上述方式，实现了桌面和实景两种剧本杀商户的挖掘，准确率和召回率均达到了要求。基于剧本杀商户的挖掘结果，能够进一步对商品进行挖掘，并创建剧本杀类目，从而为后续剧本杀知识图谱构建及标准化建设打好了数据基础。

### 3.2 标准剧本库构建

标准剧本作为整个剧本杀知识图谱的核心，在剧本杀供给标准化建设中扮演着重要的角色。我们基于剧本杀商品相似聚合的方式，结合人工审核来挖掘标准剧本，并从相关发行方获取剧本授权，从而构建标准剧本库。标准剧本由两部分构成，一个是标准剧本名称，另一个是标准剧本属性。因此，标准剧本库构建也分为标准剧本名称的挖掘和标准剧本属性的挖掘两个部分。

#### 3.2.1 标准剧本名称的挖掘

我们根据剧本杀商品的特点，先后采用了规则聚合、语义聚合和多模态聚合三种方法进行挖掘迭代，从数十万剧本杀商品的名称中聚合得到数千标准剧本名称。下面分别对三种聚合方法进行介绍。

**规则聚合**

同一个剧本杀商品在不同商户的命名往往不同，存在较多的不规范和个性化。一方面，同一个剧本名称本身就可以有多种叫法，例如「舍离」、「舍离壹」、「舍离1」就是同一个剧本；另一方面，剧本杀商品名除了包含剧本名称外，商家很多时候也会加入剧本的规格和题材等属性信息以及吸引用户的描述性文字，例如「《舍离》情感本」。所以我们首先考虑剧本杀商品的命名特点，设计相应的清洗策略对剧本杀商品名称进行清洗后再聚合。

![图 4](https://p0.meituan.net/travelcube/1c51463aad9bc8dbb2fbaec6a1d7c928111611.png)

图 4



我们除了梳理常见的非剧本词，构建词库进行规则过滤外，也尝试将其转换为命名实体识别问题[2]，采用序列标注对字符进行“是剧本名”与“不是剧本名”两个类别的区分。对于清洗后的剧本杀商品名称，则通过基于最长公共子序列（LCS）的相似度计算规则，结合阈值筛选对其进行聚合，例如「舍离」、「舍离壹」、「舍离1」最后均聚在一起。整个流程如上图4所示，采用规则聚合的方式，能够在建设初期帮助业务快速对剧本杀商品名称进行聚合。

**语义聚合**

规则聚合的方式虽然简单好用，但由于剧本名称的多样性和复杂性，我们发现聚合结果中仍然存在一些问题：1）不属于同一个剧本的商品被聚合，例如「舍离」和「舍离2」是同一个系列的两个不同剧本，却被聚合在一起。2）属于同一个剧本的商品没有聚合，例如，商品名使用剧本的简称缩写（「唐人街名侦探和猫」和「唐探猫」）或出现错别字（「弗洛伊德之锚」和「佛洛依德之锚」）等情况时则难以规则聚合。

针对这上述这两种问题，我们进一步考虑使用商品名称语义匹配的方式，从文本语义相同的角度来进行聚合。常用的文本语义匹配模型分为交互式和双塔式两种类型。交互式是把两段文本一起输入进编码器，在编码的过程中让其相互交换信息后再进行判别；双塔式模型是用一个编码器分别给两个文本编码出向量，然后基于两个向量进行判别。

由于商品数量众多，采用交互式的方法需要将商品名称两两组合后再进行模型预测，效率较为低下，为此，我们采用双塔式的方法来实现，以Sentence-BERT[3]的模型结构为基础，将两个商品名称文本分别通过BERT提取向量后，再使用余弦距离来衡量两者的相似度，完整结构如下图5所示：

![图 5](https://p0.meituan.net/travelcube/5de93d94b44a9030ee4b9b02447180d1179920.png)

图 5



在训练模型的过程中，我们首先基于规则聚合的结果，通过同聚簇内生成正例和跨聚簇交叉生成负例的方式，构造粗粒度的训练样本，完成初版模型的训练。在此基础上，进一步结合主动学习，对样本数据进行完善。此外，我们还根据上文提到的规则聚合出现的两种问题，针对性的批量生成样本。具体地，通过在商品名称后添加同系列编号，以及使用错字、别字和繁体字替换等方式来实现样本的自动构造。

**多模态聚合**

通过语义聚合的方式实现了从商品名称文本语义层面的同义聚合，然而我们通过对聚合结果再分析后发现还存在一些问题：两个商品属于同一个剧本，但仅从商品名称的角度是无法判别。例如，「舍离2」和「断念」从语义的角度无法聚合，但是它们本质上是一个剧本「舍离2·断念」。虽然这两个商品的名称各异，但是它们的图像往往是相同或相似的，为此，我们考虑引入商品的图像信息来进行辅助聚合。

一个简单的方法是，使用CV领域成熟的预训练模型作为图像编码器进行特征提取，直接计算两个商品的图像相似度。为了统一商品图像相似度计算和商品名称语义匹配的结果，我们尝试构建一个剧本杀商品的多模态匹配模型，充分利用商品名称和图像信息来进行匹配。模型沿用语义聚合中使用的双塔式结构，整体结构如下图6所示：

![图 6](https://p1.meituan.net/travelcube/39425a5146a64a077b46ded64e465ade270440.png)

图 6



在多模态匹配模型中，剧本杀商品的名称和图像分别通过文本编码器和图像编码器得到对应的向量表示后，再进行拼接作为最终的商品向量，最后使用余弦相似度来衡量商品之间的相似度。其中：

- **文本编码器**：使用文本预训练模型BERT[1]作为文本编码器，将输出平均池化后作为文本的向量表示。
- **图像编码器**：使用图像预训练模型EfficientNet[4]作为图像编码器，提取网络最后一层输出作为图像的向量表示。

在训练模型的过程中，文本编码器会进行Finetune，而图像编码器则固定参数，不参与训练。对于训练样本构建，我们以语义聚合的结果为基础，以商品图像相似度来圈定人工标注样本的范围。具体地，对于同聚簇内商品图像相似度高的直接生成正例，跨聚簇交叉的商品图像相似度低的直接生成负例，而对于剩余的样本对则交由人工进行标注确定。通过多模态聚合，弥补了仅使用文本匹配的不足，与其相比准确率提升了5%，进一步提升了标准剧本的挖掘效果。

#### 3.2.2 标准剧本属性的挖掘

标准剧本的属性包括了剧本的背景、规格、流派、题材、难度等十余个维度。由于商户在剧本杀商品上单的时候会录入商品的这些属性值，所以对于标准剧本属性的挖掘，本质上是对该标准剧本对应的所有聚合商品的属性的挖掘。

在实际过程中，我们通过投票统计的方式来进行挖掘，即对于标准剧本的某个属性，通过对应的聚合商品在该属性上的属性值进行投票，选择投票最高的属性值，作为该标准剧本的候选属性值，最后由人工审核确认。此外，在标准剧本名称挖掘的过程中，我们发现同一个剧本的叫法多种多样，为了对标准剧本能有更好的描述，还进一步为标准剧本增加了一个别称的属性，通过对标准剧本对应的所有聚合商品的名称进行清洗和去重来获取。

### 3.3 供给与标准剧本关联

在完成标准剧本库构建后，还需要建立剧本杀的商品、商户和内容三种供给，与标准剧本的关联关系，从而使剧本杀的供给实现标准化。由于通过商品和标准剧本的关联关系，可以直接获取该商品对应商户和标准剧本的关系，所以我们只需要对商品和内容进行标准剧本关联。

#### 3.3.1 商品关联

在3.2节中，我们通过聚合存量剧本杀商品的方式来进行标准剧本的挖掘，在这个过程中其实已经构建了存量商品和标准剧本的关联关系。对于后续新增加的商品，我们还需要将其和标准剧本进行匹配，以建立两者之间的关联关系。而对于与标准剧本无法关联的商品，我们则自动进行标准剧本名称和属性的挖掘，经由人工审核后再加入标准剧本库。

整个商品关联流程如下图7所示，首先对商品名称进行清洗再进行匹配关联。在匹配环节，我们基于商品和标准剧本的名称及图像的多模态信息，对两者进行匹配判别。

![图 7](https://p0.meituan.net/travelcube/a93bcc3308d9594c6646702bba0592cc324207.jpg)

图 7



与商品之间的匹配不同，商品与标准剧本的关联不需要保持匹配的对称性。为了保证关联的效果，我们在3.2.1节的多模态匹配模型的结构基础上进行修改，将商品和标准剧本的向量拼接后通过全连接层和softmax层计算两者关联的概率。训练样本则直接根据存量商品和标准剧本的关联关系构造。通过商品关联，我们实现了绝大部分剧本杀商品的标准化。

#### 3.3.2 内容关联

对于剧本杀内容关联标准剧本，主要针对用户产生的内容（UGC，例如用户评价）这一类型的内容和标准剧本的关联。由于一段UGC文本通常包含多个句子，且其中只有部分句子会提及标准剧本相关信息，所以我们将UGC与标准剧本的匹配，细化为其子句粒度的匹配，同时出于效率和效果的平衡的考虑，进一步将匹配过程分为了召回和排序两个阶段，如下图8所示：

![图 8](https://p0.meituan.net/travelcube/e61e0c3d5e1a58bac99ee2db604d42f2236584.png)

图 8



在召回阶段，将UGC文本进行子句拆分，并根据标准剧本名称及其别称，在子句集合中进行精确匹配，对于匹配中的子句则将进入到排序阶段进行精细化的关联关系判别。

在排序阶段，将关联关系判别转换为一个Aspect-based的分类问题，参考属性级情感分类的做法[5]，构建基于BERT句间关系分类的匹配模型，将实际命中UGC子句的标准剧本别称和对应的UGC子句用[SEP]相连后输入，通过在BERT后增加全连接层和softmax层来实现是否关联的二分类，最后对模型输出的分类概率进行阈值筛选，获取UGC关联的标准剧本。

与上文中涉及的模型训练不同，UGC和标准剧本的匹配模型无法快速获取大量训练样本。考虑到训练样本的缺乏，所以首先通过人工少量标注数百个样本，在此基础上，除了采用主动学习外，我们还尝试对比学习，基于Regularized Dropout[6]方法，对模型两次Dropout的输出进行正则约束。最终在训练样本不到1K的情况下，UGC关联标准剧本的准确率达到上线要求，每个标准剧本关联的UGC数量也得到了大幅提升。

## 4.应用实践

当前剧本杀知识图谱，以数千标准剧本为核心，关联百万供给。剧本杀供给标准化建设的结果已在美团多个业务场景上进行了初步的应用实践。下面介绍具体的应用方式和应用效果。

### 4.1 类目构建

通过剧本杀供给挖掘，帮助业务识别出剧本杀商户，从而助力剧本杀新类目和相应剧本杀列表页的构建。剧本杀类目迁移、休闲娱乐频道页的剧本杀入口、剧本杀列表页均已上线，其中，频道页剧本杀ICON固定第三行首位，提供了中心化流量入口，有助于建立统一的用户认知。上线示例如图9所示（(a)休闲娱乐频道页剧本杀入口，(b)剧本杀列表页）。

![图 9](https://p0.meituan.net/travelcube/88455be613870c3a7904791e125f457e504565.png)

图 9



### 4.2 个性化推荐

剧本杀知识图谱包含的标准剧本及属性节点，以及其与供给和用户的关联关系，可应用于剧本杀各页面的推荐位。一方面应用于剧本列表页热门剧本推荐（图10(a)），另一方面还应用于剧本详情页的商品在拼场次推荐（图10(b)左）、可玩门店推荐（图10(b)左）和相关剧本推荐模块（图10(b)右）。这些推荐位的应用，帮助培养了用户在平台找剧本的心智，优化了用户认知和选购体验，提高了用户和供给的匹配效率。

![图10](https://p0.meituan.net/travelcube/0b5de62f0bc4d5652ca35a103770e00c1090463.png)

图10



以剧本列表页的热门剧本推荐模块为例，剧本杀知识图谱包含的节点和关系除了可以直接用于剧本的召回，还可以进一步在精排阶段进行应用。在精排中，我们基于剧本杀知识图谱，结合用户行为，参考Deep Interest Network（DIN）[7]模型结构，尝试对用户访问剧本的序列和访问商品的序列进行建模，构建双通道DIN模型，深度刻画用户兴趣，实现剧本的个性化分发。其中商品访问序列部分，通过商品与标准剧本的关联关系将其转为为剧本序列，与候选剧本采用Attention方式进行建模，具体模型结构如下图11所示：

![图 11](https://p0.meituan.net/travelcube/934012bd28270d479a3b813829008cf5190044.png)

图 11



### 4.3 信息外露和筛选

基于剧本杀知识图谱中的节点和关系，在剧本杀列表页和在剧本列表页增加相关标签筛选项，并外露剧本的属性和关联的供给信息，相关应用如下图12所示。这些标签筛选项和信息的外露，为用户提供了规范的信息展示，降低了用户决策成本，更加方便了用户选店和选剧本。

![图 12](https://p0.meituan.net/travelcube/b2ad113041cb5799e90f506a3ade5c7a548132.png)

图 12



### 4.4 评分和榜单

在剧本详情页，内容和标准剧本的关联关系参与到剧本的评分计算中（图13(a)）。在此基础上，基于剧本维度，形成经典必玩和近期热门的剧本榜单，如图13(b)所示，从而为用户的剧本选择决策提供了更多的帮助。

![图 13](https://p0.meituan.net/travelcube/263c1694bb5e227159392b1a365cf561911089.png)

图 13



## 5.总结展望

面对剧本杀这一新兴行业，我们快速响应业务，以标准剧本为核心节点，结合行业特点，通过剧本杀供给挖掘、标准剧本库构建、供给与标准剧本关联，构建相应的知识图谱，从0到1逐步推进剧本杀的供给标准化建设，力求以简单而有效的方法来解决剧本杀业务的问题。

目前剧本杀知识图谱已在剧本杀多个业务场景中取得应用成果，赋能剧本杀业务持续增长，显著提升了用户体验。在未来的工作中，我们将不断进行优化和探索：

- **标准剧本库的持续完善**：优化标准剧本名称和属性以及相应的供给关联关系，保证标准剧本库的质与量俱佳，并尝试引入外部的知识补充当前的标准化结果。
- **剧本杀场景化**：当前剧本杀知识图谱主要以“剧本”这类用户的具象需求对象为主，后续将深入挖掘用户的场景化需求，探索剧本杀和其他行业的联动，更好的助力剧本杀行业的发展。
- **更多的应用探索**：将图谱数据应用于搜索等模块，在更多的应用场景中提升供给匹配效率，从而创造出更大的价值。

## 6.参考文献

[1] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.

[2] Lample G, Ballesteros M, Subramanian S, et al. Neural architectures for named entity recognition[J]. arXiv preprint arXiv:1603.01360, 2016.

[3] Reimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks[J]. arXiv preprint arXiv:1908.10084, 2019.

[4] Tan M, Le Q. EfficientNet: Rethinking model scaling for convolutional neural networks[C]//International Conference on Machine Learning. PMLR, 2019: 6105-6114.

[5] Sun C, Huang L, Qiu X. Utilizing BERT for aspect-based sentiment analysis via constructing auxiliary sentence[J]. arXiv preprint arXiv:1903.09588, 2019.

[6] Liang X, Wu L, Li J, et al. R-Drop: Regularized Dropout for Neural Networks[J]. arXiv preprint arXiv:2106.14448, 2021.

[7] Zhou G, Zhu X, Song C, et al. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2018: 1059-1068.

## 7.作者简介

李翔、陈焕、志华、晓阳、王奇等，均来自美团到店平台技术部到综业务数据团队。

## 8.招聘信息

美团到店平台技术部-到综业务数据团队，长期招聘算法（自然语言处理/推荐算法）、数据仓库、数据科学、系统开发等岗位同学，坐标上海。欢迎感兴趣的同学发送简历至：licong.yu@meituan.com。

原文作者：美团技术团队

原文链接：https://tech.meituan.com/2021/10/28/standardization-of-larp-games-based-on-knowledge-graph.html

# 【NO.253】美团商品知识图谱的构建及应用

## 1.背景

### 1.1 美团大脑

近年来，人工智能正在快速地改变人们的生活，背后其实有两大技术驱动力：**深度学习**和**知识图谱**。我们将深度学习归纳为隐性的模型，它通常是面向某一个具体任务，比如说下围棋、识别猫、人脸识别、语音识别等等。通常而言，在很多任务上它能够取得很优秀的结果，同时它也有一些局限性，比如说它需要海量的训练数据，以及强大的计算能力，难以进行跨任务的迁移，并且不具有较好的可解释性。在另一方面，知识图谱作为显式模型，同样也是人工智能的一大技术驱动力，它能够广泛地适用于不同的任务。相比深度学习，知识图谱中的知识可以沉淀，具有较强的可解释性，与人类的思考更加贴近，为隐式的深度模型补充了人类的知识积累，和深度学习互为补充。因此，全球很多大型的互联网公司都在知识图谱领域积极进行布局。

![图1 人工智能两大驱动力](https://p0.meituan.net/travelcube/9e103e034af6351bde26ae1b4cce3167531676.png)

图1 人工智能两大驱动力



美团连接了数亿用户和数千万商户，背后也蕴含着丰富的日常生活相关知识。2018年，美团知识图谱团队开始构建美团大脑，着力于利用知识图谱技术赋能业务，进一步改善用户体验。具体来说，美团大脑会对美团业务中涉及到的千万级别商家、亿级别的菜品/商品、数十亿的用户评论，以及背后百万级别的场景进行深入的理解和结构化的知识建模，构建人、店、商品、场景之间的知识关联，从而形成生活服务领域大规模的知识图谱。现阶段，美团大脑已覆盖了数十亿实体，数百亿三元组，在餐饮、外卖、酒店、金融等场景中验证了知识图谱的有效性。

![图2 美团大脑](https://p1.meituan.net/travelcube/cf613a933f133ff00bd75b37f5d30c681071820.png)

图2 美团大脑



### 1.2 在新零售领域的探索

美团逐步突破原有边界，在生活服务领域探索新的业务，不仅局限于通过外卖、餐饮帮大家“吃得更好”，近年来也逐步拓展到零售、出行等其他领域，帮大家“生活更好”。在零售领域中，美团先后落地了美团闪购、美团买菜、美团优选、团好货等一系列相应的业务，逐步实现“万物到家”的愿景。为了更好地支持美团的新零售业务，我们需要对背后的零售商品建立知识图谱，积累结构化数据，深入对零售领域内商品、用户、属性、场景等的理解，以便能更好地为用户提供零售商品领域内的服务。

相比于围绕商户的餐饮、外卖、酒店的等领域，零售商品领域对于知识图谱的建设和应用提出了更大的挑战。一方面，商品数量更加庞大，覆盖的领域范围也更加宽广。另一方面，商品本身所具有的显示信息往往比较稀疏，很大程度上需要结合生活中的常识知识来进行推理，方可将隐藏在背后的数十维的属性进行补齐，完成对商品完整的理解。在下图的例子中，“乐事黄瓜味”这样简单的商品描述其实就对应着丰富的隐含信息，只有对这些知识进行了结构化提取和相应的知识推理后，才能够更好的支持下游搜索、推荐等模块的优化。

![图3 商品结构化信息的应用](https://p0.meituan.net/travelcube/67f21c3f78a95e6469007b42ddc22ca6982740.png)

图3 商品结构化信息的应用



### 1.3 商品图谱建设的目标

我们针对美团零售业务的特点，制定了多层级、多维度、跨业务的零售商品知识图谱体系。

![图4 商品知识图谱体系](https://p0.meituan.net/travelcube/0b9af8cae3f71d8296a6aa3eb191a8e3407927.png)

图4 商品知识图谱体系



**多层级**

在不同业务的不同应用场景下，对于“商品”的定义会有所差别，需要对各个不同颗粒度的商品进行理解。因此，在我们的零售商品知识图谱中，建立了五层的层级体系，具体包括： - **L1-商品SKU/SPU**：对应业务中所售卖的商品颗粒度，是用户交易的对象，往往为商户下挂的商品，例如“望京家乐福所售卖的蒙牛低脂高钙牛奶250ml盒装”。这一层级也是作为商品图谱的最底层的基石，将业务商品库和图谱知识进行打通关联。 - **L2-标准商品**：描述商品本身客观事实的颗粒度，例如“蒙牛低脂高钙牛奶250ml盒装”，无论通过什么渠道在什么商户购买，商品本身并没有任何区别。商品条形码则是在标准商品这层的客观依据。在这一层级上，我们可以建模围绕标准商品的客观知识，例如同一个标准商品都会具有同样的品牌、口味、包装等属性。 - **L3-抽象商品**：进一步我们将标准商品向上抽象的商品系列，例如“蒙牛低脂高钙牛奶”。在这一层级中，我们不再关注商品具体的包装、规格等，将同系列的商品聚合为抽象商品，承载了用户对于商品的主观认知，包括用户对商品系列的别名俗称、品牌认知、主观评价等。 - **L4-主体品类**：描述商品主体的本质品类，列如“鸡蛋”、“奶油草莓”、“台式烤肠”等。这一层作为商品图谱的后台类目体系，以客观的方式对商品领域的品类进行建模，承载了用户对于商品的需求，例如各品牌各产地的鸡蛋都能够满足用户对于鸡蛋这个品类的需求。 - **L5-业务类目**：相比于主体品类的后台类目体系，业务类目作为前台类目体系会依据业务当前的发展阶段进行人工定义和调整，各个业务会根据当前业务阶段的特点和需求建立对应的前台类目体系。

**多维度**

- **商品属性视角**：围绕商品本身，我们需要有海量的属性维度来对商品进行描述。商品属性维度主要分为两类：一类是通用的属性维度，包括品牌、规格、包装、产地等；另一类是品类特有的属性维度，例如对于牛奶品类我们会关注脂肪含量（全脂/低脂/脱脂牛奶）、存储方式（常温奶、冷藏奶）等。商品属性主要是刻画了商品的客观知识，往往会建立在标准商品这一层级上。
- **用户认知视角**：除了客观的商品属性维度以外，用户往往对于商品会有一系列的主观认知，例如商品的别名俗称（“小黑瓶”、“快乐水”）、对于商品的评价（“香甜可口”、“入口即化”、“性价比高”）、商品的清单/榜单（“进口食品榜单”、“夏季消暑常备”）等维度。这些主观认知往往会建立在抽象商品这一层级上。
- **品类/类目视角**：从品类/类目的视角来看，不同品类/类目也会有各自不同的关注点。在这一层级上，我们会建模各个品类/类目下有哪些典型的品牌、用户关注哪些典型属性、不同品类的复购周期是多长时间等。

**跨业务**

美团大脑商品知识图谱的目标是希望能够对客观世界中的商品知识进行建模，而非局限于单个业务之中。在商品图谱的五层体系中，标准商品、抽象商品、品类体系都是与业务解耦的，围绕着客观商品所建立的，包括围绕这些层级建立的各维度数据也均是刻画了商品领域的客观知识。

在应用于各个业务当中时，我们将客观的图谱知识向上关联至业务前台类目，向下关联至业务商品SPU/SKU，则可以完成各个业务数据的接入，实现各个业务数据和客观知识之间的联通，提供更加全面的跨业务的全景数据视角。利用这样的数据，在用户方面我们可以更加全面的建模、分析用户对于业务、品类的偏好，对于价格、品质等的敏感程度，在商品方面我们可以更准确的建模各品类的复购周期、地域/季节/节日偏好等。

### 1.4 商品图谱建设的挑战

商品知识图谱的构建的挑战主要来源于以下三个方面：

1. **信息来源质量低**：商品本身所具有的信息比较匮乏，往往以标题和图片为主。尤其在美团闪购这样LBS的电商场景下，商户需要上传大量的商品数据，对于商品信息的录入存在很多信息不完整的情况。在标题和图片之外，商品详情虽然也蕴含着大量的知识信息，但是其质量往往参差不齐，并且结构各异，从中进行知识挖掘难度极高。
2. **数据维度多**：在商品领域有众多的数据维度需要进行建设。以商品属性部分为例，我们不仅需要建设通用属性，诸如品牌、规格、包装、口味等维度，同时还要覆盖各个品类/类目下特定关注的属性维度，诸如脂肪含量、是否含糖、电池容量等，整体会涉及到上百维的属性维度。因此，数据建设的效率问题也是一大挑战。
3. **依赖常识/专业知识**：人们在日常生活中因为有很丰富的常识知识积累，可以通过很简短的描述获取其背后隐藏的商品信息，例如在看到“乐事黄瓜”这样一个商品的时候知道其实是乐事黄瓜味的薯片、看到“唐僧肉”的时候知道其实这不是一种肉类而是一种零食。因此，我们也需要探索结合常识知识的语义理解方法。同时，在医药、个护等领域中，图谱的建设需要依赖较强的专业知识，例如疾病和药品之间的关系，并且此类关系对于准确度的要求极高，需要做到所有知识都准确无误，因此也需要较好的专家和算法相结合的方式来进行高效的图谱构建。

## 2.商品图谱建设

在了解了图谱建设的目标和挑战后，接下来我们将介绍商品图谱数据建设的具体方案。

### 2.1 层级体系建设

**品类体系建设**

本质品类描述了商品本质所属的最细类别，它聚合了一类商品，承载了用户最终的消费需求，如“高钙牛奶”、“牛肉干”等。本质品类与类目也是有一定的区别，类目是若干品类的集合，它是抽象后的品类概念，不能够明确到具体的某类商品品类上，如“乳制品”、“水果”等。

**品类打标**：对商品图谱的构建来说，关键的一步便是建立起商品和品类之间的关联，即对商品打上品类标签。通过商品和品类之间的关联，我们可以建立起商品库中的商品与用户需求之间的关联，进而将具体的商品展示到用户面前。下面简单介绍下品类打标方法：

1. **品类词表构建**：品类打标首先需要构建一个初步的商品品类词表。首先，我们通过对美团的各个电商业务的商品库、搜索日志、商户标签等数据源进行分词、NER、新词发现等操作，获得初步的商品候选词。然后，通过标注少量的样本进行二分类模型的训练（判断一个词是否是品类）。此外，我们通过结合主动学习的方法，从预测的结果中挑选出难以区分的样本，进行再次标注，继续迭代模型，直到模型收敛。
2. **品类打标**：首先，我们通过对商品标题进行命名实体识别，并结合上一步中的品类词表来获取商品中的候选品类，如识别“蒙牛脱脂牛奶 500ml”中的“脱脂牛奶”、“牛奶”等。然后，在获得了商品以及对应的品类之后，我们利用监督数据训练品类打标的二分类模型，输入商品的SPU_ID和候选品类TAG构成的Pair，即，对它进行是否匹配的预测。具体的，我们一方面利用结合业务中丰富的半结构化语料构建围绕标签词的统计特征，另一方面利用命名实体识别、基于BERT的语义匹配等模型产出高阶相关性特征，在此基础上，我们将上述特征输入到终判模型中进行模型训练。
3. **品类标签后处理**：在这一步中，我们对模型打上的品类进行后处理的一些策略，如基于图片相关性、结合商品标题命名实体识别结果等的品类清洗策略。

通过上述的三个步骤，我们便可以建立起商品与品类之间的联系。

**品类体系**：品类体系由品类和品类间关系构成。常见的品类关系包括同义词和上下位等。在构建品类体系的过程中，常用的以下几种方法来进行关系的补全。我们主要使用下面的一些方法： 1. **基于规则的品类关系挖掘**。在百科等通用语料数据中，有些品类具有固定模式的描述，如“玉米又名苞谷、苞米棒子、玉蜀黍、珍珠米等”、“榴莲是著名热带水果之一”，因此，可以使用规则从中提取同义词和上下位。 2. **基于分类的品类关系挖掘**。类似于上文中提到的品类打标方法，我们将同义词和上下位构建为的样本，通过在商品库、搜索日志、百科数据、UGC中挖掘的统计特征以及基于Sentence-BERT得到的语义特征，使用二分类模型进行品类关系是否成立的判断。对于训练得到的分类模型，我们同样通过主动学习的方式，选出结果中的难分样本，进行二次标注，进而不断迭代数据，提高模型性能。 3. **基于图的品类关系推理**。在获得了初步的同义词、上下位关系之后，我们使用已有的这些关系构建网络，使用GAE、VGAE等方法对网络进行链路预测，从而进行图谱边关系的补全。

![图5 商品图谱品类体系的构建](https://p1.meituan.net/travelcube/0cd71be91cad215eddd22456f8902de8249465.png)

图5 商品图谱品类体系的构建



**标准/抽象商品**

标准商品是描述商品本身客观事实的颗粒度，和销售渠道和商户无关，而商品条形码是标准商品这层的客观依据。标品关联即将同属于某个商品条形码的业务SKU/SPU，都正确关联到该商品条形码上，从而在标准商品层级上建模相应的客观知识，例如标准商品对应的品牌、口味和包装等属性。 下面通过一个案例来说明标品关联的具体任务和方案。

**案例**：下图是一个公牛三米插线板的标准商品。商家录入信息的时候，会把商品直接关联到商品条码上。通过商户录入数据完成了一部分的标品关联，但这部分比例比较少，且存在大量的链接缺失，链接错误的问题。另外，不同的商家对于同样的标品，商品的标题的描述是千奇百怪的。我们的目标是补充缺失的链接，将商品关联到正确的标品上。

![图6 商品图谱标品关联任务](https://p0.meituan.net/travelcube/88048546ffa3d65c40dcd69ad7da3b7d624180.png)

图6 商品图谱标品关联任务



针对标品关联任务，我们构建了商品领域的同义词判别模型：通过远监督的方式利用商户已经提供的少量有关联的数据，作为已有的知识图谱构造远监督的训练样本。在模型中，正例是置信度比较高的标品码；负例是原始数据中商品名或者图像类似但不属于同一标品的SPU。构造准确率比较高的训练样本之后，通过BERT模型进行同义词模型训练。最后，通过模型自主去噪的方式，使得最终的准确率能够达到99%以上。总体能做到品牌，规格，包装等维度敏感。

![图7 商品图谱标品关联方法](https://p0.meituan.net/travelcube/b6b701f9982109208c3d2d90127d7ccb648102.png)

图7 商品图谱标品关联方法



抽象商品是用户认知的层面，作为用户所评论的对象，这一层对用户偏好建模更加有效。同时，在决策信息的展示上，抽象商品粒度也更符合用户认知。例如下图所示冰淇淋的排行榜中，罗列了用户认知中抽象商品对应的SKU，然后对应展示不同抽象商品的特点、推荐理由等。抽象商品层整体的构建方式，和标准商品层比较类似，采用标品关联的模型流程，并在数据构造部分进行规则上的调整。

![图8 商品图谱抽象商品聚合](https://p0.meituan.net/travelcube/9f00a0826aec9c2898c349f88f4ba482694147.png)

图8 商品图谱抽象商品聚合



### 2.2 属性维度建设

对一个商品的全面理解，需要涵盖各个属性维度。例如“乐事黄瓜味薯片”，需要挖掘它对应的品牌、品类、口味、包装规格、标签、产地以及用户评论特色等属性，才能在商品搜索、推荐等场景中精准触达用户。商品属性挖掘的源数据主要包含商品标题、商品图片和半结构化数据三个维度。

![图9 商品图谱属性建设](https://p1.meituan.net/travelcube/0c2aad7c8b02b70512b4ec51734589dd705593.png)

图9 商品图谱属性建设



商品标题包含了对于商品最重要的信息维度，同时，商品标题解析模型可以应用在查询理解中，对用户快速深入理解拆分，为下游的召回排序也能提供高阶特征。因此，这里我们着重介绍一下利用商品标题进行属性抽取的方法。

商品标题解析整体可以建模成文本序列标注的任务。例如，对于商品标题“乐事黄瓜薯片”，目标是理解标题文本序列中各个成分，如乐事对应品牌，黄瓜对应口味，薯片是品类，因此我们使用命名实体识别（NER）模型进行商品标题解析。然而商品标题解析存在着三大挑战：（1）上下文信息少；（2）依赖常识知识；（3）标注数据通常有较多的噪音。为了解决前两个挑战，我们首先尝试在模型中引入了图谱信息，主要包含以下三个维度：

- **节点信息**：将图谱实体作为词典，以Soft-Lexicon方式接入，以此来缓解NER的边界切分错误问题。
- **关联信息**：商品标题解析依赖常识知识，例如在缺乏常识的情况下，仅从标题“乐事黄瓜薯片”中，我们无法确认“黄瓜”是商品品类还是口味属性。因此，我们引入知识图谱的关联数据缓解了常识知识缺失的问题：在知识图谱中，乐事和薯片之间存在着“品牌-售卖-品类”的关联关系，但是乐事跟黄瓜之间则没有直接的关系，因此可以利用图结构来缓解NER模型常识知识缺少的问题。具体来说，我们利用Graph Embedding的技术对图谱进行的嵌入表征，利用图谱的图结构信息对图谱中的单字，词进行表示，然后将包含了图谱结构信息的嵌入表示和文本语义的表征进行拼接融合，再接入到NER模型之中，使得模型能够既考虑到语义，也考虑到常识知识的信息。
- **节点类型信息**：同一个词可以代表不同的属性，比如“黄瓜”既可以作为品类又可以作为属性。因此，对图谱进行Graph Embedding建模的时候，我们根据不同的类型对实体节点进行拆分。在将图谱节点表征接入NER模型中时，再利用注意力机制根据上下文来选择更符合语义的实体类型对应的表征 ，缓解不同类型下词语含义不同的问题，实现不同类型实体的融合。

![图10 商品图谱标题解析](https://p0.meituan.net/travelcube/d9623de64d23d70f9b0761f45f596166488327.png)

图10 商品图谱标题解析



接下来我们探讨如何缓解标注噪音的问题。在标注过程中，少标漏标或错标的问题无法避免，尤其像在商品标题NER这种标注比较复杂的问题上，尤为显著。对于标注数据中的噪音问题，采用以下方式对噪音标注优化：不再采取原先非0即1的Hard的训练方式，而是采用基于置信度数据的Soft训练方式，然后再通过Bootstrapping的方式迭代交叉验证，然后根据当前的训练集的置信度进行调整。我们通过实验验证，使用Soft训练+Bootstrapping多轮迭代的方式，在噪声比例比较大的数据集上，模型效果得到了明显提升。具体的方法可参见我们在NLPCC 2020比赛中的论文《Iterative Strategy for Named Entity Recognition with Imperfect Annotations》。

![图11 基于噪音标注的NER优化](https://p0.meituan.net/travelcube/34f146e4c5c20840089a867d24a188c8319107.png)

图11 基于噪音标注的NER优化



### 2.3 效率提升

知识图谱的构建往往是针对于各个领域维度的数据单独制定的挖掘方式。这种挖掘方式重人工，比较低效，针对每个不同的领域、每个不同的数据维度，我们都需要定制化的去建设任务相关的特征及标注数据。在商品场景下，挖掘的维度众多，因此效率方面的提高也是至关重要的。我们首先将知识挖掘任务建模为三类分类任务，包括节点建模、关系建模以及节点关联。在整个模型的训练过程中，最需要进行效率优化的其实就是上述提到的两个步骤：（1）针对任务的特征提取；（2）针对任务的数据标注。

![图12 知识挖掘任务建模](https://p0.meituan.net/travelcube/e3f1f413dead7e7f3781749fce0dba36571254.png)

图12 知识挖掘任务建模



针对特征提取部分，我们摒弃了针对不同挖掘任务做定制化特征挖掘的方式，而是尝试将特征和任务解耦，构建跨任务通用的图谱挖掘特征体系，利用海量的特征库来对目标的节点/关系/关联进行表征，并利用监督训练数据来进行特征的组合和选择。具体的，我们构建的图谱特征体系主要由四个类型的特征组构成： 1. 规则模板型特征主要是利用人工先验知识，融合规则模型能力。 2. 统计分布型特征，可以充分利用各类语料，基于不同语料不同层级维度进行统计。 3. 句法分析型特征则是利用NLP领域的模型能力，引入分词、词性、句法等维度特征。 4. 嵌入表示型特征，则是利用高阶模型能力，引入BERT等语义理解模型的能力。

![图13 知识挖掘特征体系](https://p0.meituan.net/travelcube/77ef23cced777b2cbd12968c79ad0e57636871.png)

图13 知识挖掘特征体系



针对数据标注部分，我们主要从三个角度来提升效率。 1. 通过半监督学习，充分的利用未标注的数据进行预训练。 2. 通过主动学习技术，选择对于模型来说能够提供最多信息增益的样本进行标注。 3. 利用远程监督方法，通过已有的知识构造远监督样本进行模型训练，尽可能的发挥出已有知识的价值。

### 2.4 人机结合-专业图谱建设

当前医药健康行业结构性正在发生变化，消费者更加倾向于使用在线医疗解决方案和药品配送服务，因此医药业务也逐渐成为了美团的重要业务之一。相比于普通商品知识图谱的建设，药品领域知识具有以下两个特点：（1）具有极强的专业性，需要有相关背景知识才能判断相应的属性维度，例如药品的适用症状等。（2）准确度要求极高，对于强专业性知识不允许出错，否则更容易导致严重后果。因此我们采用将智能模型和专家知识结合的方式来构建药品知识图谱。

药品图谱中的知识可以分为弱专业知识和强专业知识两类，弱专业知识即一般人能够较容易获取和理解的知识，例如药品的使用方法、适用人群等；而强专业知识则是需要具有专业背景的人才能够判断的知识，例如药品的主治疾病、适应症状等。由于这两类数据对专家的依赖程度不同，因此我们分别采取不同的挖掘链路：

- **弱专业知识**：对于药品图谱的弱专业知识挖掘，我们从说明书、百科知识等数据源中提取出相应的信息，并结合通过专家知识沉淀出来的规则策略，借助通用语义模型从中提取相应的知识，并通过专家的批量抽检，完成数据的建设。
- **强专业知识**：对于药品图谱的强专业知识挖掘，为了确保相关知识百分百准确，我们通过模型提取出药品相关属性维度的候选后，将这些候选知识给到专家进行全量质检。在这里，我们主要是通过算法的能力，尽可能减少专业药师在基础数据层面上的精力花费，提高专家从半结构化语料中提取专业知识的效率。

在药品这类专业性强的领域，专业知识的表述和用户习惯往往存在差异。因此我们除了挖掘强弱专业知识外，还需要填补专业知识和用户之间的差异，才能将药品图谱更好的与下游应用结合。为此，我们从用户行为日志以及领域日常对话等数据源中，挖掘了疾病、症状和功效的别名数据，以及药品通用名的俗称数据，来打通用户习惯和专业表述之间的通路。

![图14 人机结合的专业知识挖掘](https://p0.meituan.net/travelcube/6c03e3f1eb156f138d50565d17c4dcba838119.png)

图14 人机结合的专业知识挖掘



## 3.商品图谱的落地应用

自从谷歌将知识图谱应用于搜索引擎，并显著提升了搜索质量与用户体验，知识图谱在各垂直领域场景都扮演起了重要的角色。在美团商品领域中，我们也将商品图谱有效的应用在围绕商品业务的搜索、推荐、商家端、用户端等多个下游场景当中，接下来我们举几个典型的案例进行介绍。

### 3.1 结构化召回

商品图谱的数据，对于商品的理解很有帮助。例如，在商品搜索中，如用户在搜索头疼腰疼时，通过结构化的知识图谱，才能知道什么药品是有止疼功效的；用户在搜索可爱多草莓、黄瓜薯片时，需要依赖图谱的常识知识来理解用户真正需求是冰淇淋和薯片，而不是草莓和黄瓜。

![图15 基于图谱的结构化召回](https://p0.meituan.net/travelcube/44079fcc1e7587a6df37093a9bea1ec1494227.png)

图15 基于图谱的结构化召回



### 3.2 排序模型泛化性

图谱的类目信息、品类信息、属性信息，一方面可以作为比较强有力的相关性的判断方法和干预手段，另一方面可以提供不同粗细粒度的商品聚合能力，作为泛化性特征提供到排序模型，能有效地提升排序模型的泛化能力，对于用户行为尤为稀疏的商品领域来说则具有着更高的价值。具体的特征使用方式则包括： 1. 通过各颗粒度进行商品聚合，以ID化特征接入排序模型。 2. 在各颗粒度聚合后进行统计特征的建设。 3. 通过图嵌入表示的方式，将商品的高维向量表示和排序模型结合。

![图16 基于图谱的排序优化](https://p0.meituan.net/travelcube/3253ed72719c06177550fc3ab3ba0640435014.png)

图16 基于图谱的排序优化



### 3.3 多模态图谱嵌入

现有的研究工作已经在多个领域中证明了，将知识图谱的数据进行嵌入表示，以高维向量表示的方式和排序模型结合，可以有效地通过引入外部知识达到缓解排序/推荐场景中数据稀疏以及冷启动问题的效果。然而，传统的图谱嵌入的工作往往忽视了知识图谱中的多模态信息，例如商品领域中我们有商品的图片、商品的标题、商家的介绍等非简单的图谱节点型的知识，这些信息的引入也可以进一步提升图谱嵌入对推荐/排序的信息增益。

![图17 基于多模态图谱的推荐-背景](https://p1.meituan.net/travelcube/32038708cca851aeb805f0de73f314a6398773.png)

图17 基于多模态图谱的推荐-背景



现有的图谱嵌入方法在应用到多模态图谱表征的时候会存在一些问题，因为在多模态场景下，图谱中边的含义不再是单纯的语义推理关系，而是存在多模态的信息补充的关系，因此我们也针对多模态图谱的特点，提出了MKG Entity Encoder和MKG Attention Layer来更好的建模多模态知识图谱，并将其表征有效的接入至推荐/排序模型中，具体方法可以参考我们在CIKM 2020发表了的论文《Multi-Modal Knowledge Graphs for Recommender Systems》。

![图18 基于图谱的排序优化-模型](https://p0.meituan.net/travelcube/7f5c24dd26c3183dd9764b0f4b4b8e45379428.png)

图18 基于图谱的排序优化-模型



### 3.4 用户/商家端优化

商品图谱在用户端提供显式化的可解释性信息，辅助用户进行决策。具体的呈现形式包括筛选项、特色标签、榜单、推荐理由等。筛选项的维度受当前查询词对应品类下用户关注的属性类别决定，例如，当用户搜索查询词为薯片时，用户通常关注的是它的口味、包装、净含量等，我们将会根据供给数据在这些维度下的枚举值展示筛选项。商品的特色标签则来源于标题、商品详情页信息与评论数据的提取，以简洁明了的结构化数据展示商品特色。商品的推荐理由通过评论抽取与文本生成两种渠道获得，与查询词联动，以用户视角给出商品值得买的原因，而榜单数据则更为客观，以销量等真实数据，反应商品品质。

在商家端，即商家发布侧，商品图谱则提供了基于商品标题的实时预测能力，帮助商家进行类目的挂载、属性信息的完善。例如，商家填写标题“德国进口德亚脱脂纯牛奶12盒”后，商品图谱提供的在线类目预测服务可将其挂载到“食品饮料-乳制品-纯牛奶”类目，并通过实体识别服务，得到商品的“产地-德国”，“是否进口-进口”，“品牌-德亚”，“脂肪含量-脱脂”，“规格-12盒”的属性信息，预测完成后，由商家确认发布，降低商家对商品信息的维护成本，并提升发布商品的信息质量。

## 4.作者简介

雪智，凤娇，姿雯，匡俊，林森，武威等，均来自美团平台搜索与NLP部NLP中心。

## 5.招聘信息

美团大脑知识图谱团队大量岗位持续招聘中，实习、校招、社招均可，坐标北京/上海，欢迎感兴趣的同学加入我们，利用自然语言和知识图谱技术，帮大家吃得更好，生活更好。简历可投递至：caoxuezhi@meituan.com。

原文作者：美团技术团队

原文链接：https://tech.meituan.com/2021/09/02/meituan-commodity-nlp-practice.html

# 【NO.254】GraphQL及元数据驱动架构在后端BFF中的实践

## 1 BFF的由来

BFF一词来自Sam Newman的一篇博文《[Pattern:Backends For Frontends](https://samnewman.io/patterns/architectural/bff/)》，指的是服务于前端的后端。BFF是解决什么问题的呢？据原文描述，随着移动互联网的兴起，原适应于桌面Web的服务端功能希望同时提供给移动App使用，而在这个过程中存在这样的问题：

- 移动App和桌面Web在UI部分存在差异。
- 移动App涉及不同的端，不仅有iOS、还有Android，这些不同端的UI之间存在差异。
- 原有后端功能和桌面Web UI之间已经存在了较大的耦合。

因为端的差异性存在，服务端的功能要针对端的差异进行适配和裁剪，而服务端的业务功能本身是相对单一的，这就产生了一个矛盾——服务端的单一业务功能和端的差异性诉求之间的矛盾。那么这个问题怎么解决呢？这也是文章的副标题所描述的”Single-purpose Edge Services for UIs and external parties”，引入BFF，由BFF来针对多端差异做适配，这也是目前业界广泛使用的一种模式。

![图1 BFF示意图](https://p0.meituan.net/travelcube/4908c43bad40c3ad4cb8d497a611b672200549.png)

图1 BFF示意图



在实际业务的实践中，导致这种端差异性的原因有很多，有技术的原因，也有业务的原因。比如，用户的客户端是Android还是iOS，是大屏还是小屏，是什么版本。再比如，业务属于哪个行业，产品形态是什么，功能投放在什么场景，面向的用户群体是谁等等。这些因素都会带来面向端的功能逻辑的差异性。

在这个问题上，笔者所在团队负责的商品展示业务有一定的发言权，同样的商品业务，在C端的展示功能逻辑，深刻受到商品类型、所在行业、交易形态、投放场所、面向群体等因素的影响。同时，面向消费者端的功能频繁迭代的属性，更是加剧并深化了这种矛盾，使其演化成了一种服务端单一稳定与端的差异灵活之间的矛盾，这也是商品展示（商品展示BFF）业务系统存在的必然性原因。本文主要在美团到店商品展示场景的背景下，介绍面临的一些问题及解决思路。

## 2 BFF背景下的核心矛盾

BFF这层的引入是解决服务端单一稳定与端的差异灵活诉求之间的矛盾，这个矛盾并不是不存在，而是转移了。由原来后端和前端之间的矛盾转移成了BFF和前端之间的矛盾。笔者所在团队的主要工作，就是和这种矛盾作斗争。下面以具体的业务场景为例，结合当前的业务特点，说明在BFF的生产模式下，我们所面临的具体问题。下图是两个不同行业的团购货架展示模块，这两个模块我们认为是两个商品的展示场景，它们是两套独立定义的产品逻辑，并且会各自迭代。

![图2 展示场景](https://p0.meituan.net/travelcube/c88cff6db3d2156f4659781c4ff88841203070.png)

图2 展示场景



在业务发展初期，这样的场景不多。BFF层系统“烟囱式”建设，功能快速开发上线满足业务的诉求，在这样的情况下，这种矛盾表现的不明显。而随着业务发展，行业的开拓，形成了许许多多这样的商品展示功能，矛盾逐渐加剧，主要表现在以下两个方面：

- 业务支撑效率：随着商品展示场景变得越来越多，API呈爆炸趋势，业务支撑效率和人力成线性关系，系统能力难以支撑业务场景的规模化拓展。
- 系统复杂度高：核心功能持续迭代，内部逻辑充斥着`if…else…`，代码过程式编写，系统复杂度较高，难以修改和维护。

那么这些问题是怎么产生的呢？这要结合“烟囱式”系统建设的背景和商品展示场景所面临的业务，以及系统特点来进行理解。

**特点一：外部依赖多、场景间取数存在差异、用户体验要求高**

图例展示了两个不同行业的团购货架模块，这样一个看似不大的模块，后端在BFF层要调用20个以上的下游服务才能把数据拿全，这是其一。在上面两个不同的场景中，需要的数据源集合存在差异，而且这种差异普遍存在，这是其二，比如足疗团购货架需要的某个数据源，在丽人团购货架上不需要，丽人团购货架需要的某个数据源，足疗团购货架不需要。尽管依赖下游服务多，同时还要保证C端的用户体验，这是其三。

这几个特点给技术带来了不小的难题：1）聚合大小难控制，聚合功能是分场景建设？还是统一建设？如果分场景建设，必然存在不同场景重复编写类似聚合逻辑的问题。如果统一建设，那么一个大而全的数据聚合中必然会存在无效的调用。2）聚合逻辑的复杂性控制问题，在这么多的数据源的情况下，不仅要考虑业务逻辑怎么写，还要考虑异步调用的编排，在代码复杂度未能良好控制的情况下，后续聚合的变更修改将会是一个难题。

**特点二：展示逻辑多、场景之间存在差异，共性个性逻辑耦合**

我们可以明显地识别某一类场景的逻辑是存在共性的，比如团单相关的展示场景。直观可以看出基本上都是展示团单维度的信息，但这只是表象。实际上在模块的生成过程中存在诸多的差异，比如以下两种差异：

- 字段拼接逻辑差异：比如以上图中两个团购货架的团购标题为例，同样是标题，在丽人团购货架中的展示规则是：**[类型] + 团购标题**，而在足疗团购货架的展示规则是：**团购标题**。
- 排序过滤逻辑差异：比如同样是团单列表，A场景按照销量倒排序，B场景按照价格排序，不同场景的排序逻辑不同。

诸如此类的**展示逻辑**的差异性还有很多。类似的场景实际上在内部存在很多差异的逻辑，后端如何应对这种差异性是一个难题，下面是最常见的一种写法，通过读取具体的条件字段来做判断实现逻辑路由，如下所示：

```java
if(category == "丽人") {
  title = "[" + category + "]" + productTitle;
} else if (category == "足疗") {
  title = productTitle；
}
```

这种方案在功能实现方面没有问题，也能够复用共同的逻辑。但是实际上在场景非常多的情况下，将会有非常多的差异性判断逻辑叠加在一起，功能一直会被持续迭代的情况下，可以想象，系统将会变得越来越复杂，越来越难以修改和维护。

**总结**：在BFF这层，不同商品展示场景存在差异。在业务发展初期，系统通过独立建设的方式支持业务快速试错，在这种情况下，业务差异性带来的问题不明显。而随着业务的不断发展，需要搭建及运营的场景越来越多，呈规模化趋势。此时，业务对技术效率提出了更高的要求。**在这种场景多、场景间存在差异的背景下，如何满足场景拓展效率同时能够控制系统的复杂性，就是我们业务场景中面临的核心问题**。

## 3 BFF应用模式分析

目前业界针对此类的解决方案主要有两种模式，一种是后端BFF模式，另一种是前端BFF模式。

### 3.1 后端BFF模式

后端BFF模式指的是BFF由后端同学负责，这种模式目前最广泛的实践是基于GraphQL搭建的后端BFF方案，具体是：后端将展示字段封装成展示服务，通过GraphQL编排之后暴露给前端使用。如下图所示：

![图3 后端BFF模式](https://p0.meituan.net/travelcube/8af9b3d43145aeb0c7733d9fc5c13270193362.png)

图3 后端BFF模式



这种模式最大的特性和优势是，当展示字段已经存在的情况下，后端不需要关心前端差异性需求，按需查询的能力由GraphQL支持。这个特性可以很好地应对不同场景存在展示字段差异性这个问题，前端直接基于GraphQL按需查询数据即可，后端不需要变更。同时，借助GraphQL的编排和聚合查询能力，后端可以将逻辑分解在不同的展示服务中，因此在一定程度上能够化解BFF这层的复杂性。

但是基于这种模式，仍然存在几个问题：展示服务颗粒度问题、数据图划分问题以及字段扩散问题，下图是基于当前模式的具体案例：

![图4 后端BFF模式（案例）](https://p0.meituan.net/travelcube/c5f78c0063a14d902c45ef7c5742c47b78108.png)

图4 后端BFF模式（案例）



**1）展示服务颗粒度设计问题**

这种方案要求展示逻辑和取数逻辑封装在一个模块中，形成一个展示服务（Presentation Service），如上图所示。而实际上展示逻辑和取数逻辑是多对多的关系，还是以前文提到的例子说明：

> **背景**：有两个展示服务，分别封装了商品标题和商品标签的查询能力。 **情景**：此时PM提了一个需求，希望商品在某个场景的标题以“[类型]+商品标题”的形式展示，此时商品标题的拼接依赖类型数据，而此时类型数据商品标签展示服务中已经调用了。 **问题**：商品标题展示服务自己调用类型数据还是将两个展示服务合并到一起？

以上描述的问题的是展示服务颗粒度把控的问题，我们可以怀疑上述的示例是不是因为展示服务的颗粒度过小？那么反过来看一看，如果将两个服务合并到一起，那么势必又会存在冗余。这是展示服务设计的难点，**核心原因在于，展示逻辑和取数逻辑本身是多对多的关系，结果却被设计放在了一起**。

**2）数据图划分问题**

通过GraphQL将多个展示服务的数据聚合到一张图（GraphQL Schema）中，形成一个数据视图，需要数据的时候只要数据在图中，就可以基于Query按需查询。那么问题来了，这个图应该怎么组织？是一张图还是多张图？图过大的话，势必带来复杂的数据关系维护问题，图过小则将会降低方案本身的价值。

**3）展示服务内部复杂性 + 模型扩散问题**

上文提到过一个商品标题的展示存在不同拼接逻辑的情况，在商品展示场景，这种逻辑特别普遍。比如同样是价格，A行业展示优惠后价格，B行业展示优惠前价格；同样是标签位置，C行业展示服务时长，而D行业展示商品特性等。那么问题来了，展示模型如何设计？以标题字段为例，是在展示模型上放个`title`字段就可以，还是分别放个`title`和`titleWithCategory`？如果是前者那么服务内部必然会存在`if…else…`这种逻辑，用于区分`title`的拼接方式，这同样会导致展示服务内部的复杂性。如果是多个字段，那么可以想象，展示服务的模型字段也将会不断扩散。

**总结**：后端BFF模式能够在一定程度上化解后端逻辑的复杂性，同时提供一个展示字段的复用机制。但是仍然存在未决问题，如展示服务的颗粒度设计问题，数据图的划分问题，以及展示服务内部的复杂性和字段扩散问题。目前这种模式实践的代表有Facebook、爱彼迎、eBay、爱奇艺、携程、去哪儿等等。

### 3.2 前端BFF模式

前端BFF模式在Sam Newman的文章中的”And Autonomy”部分有特别的介绍，指的是BFF本身由前端团队自己负责，如下示意图所示：

![图5 前端BFF模式](https://p0.meituan.net/travelcube/0e21d87858dbe63f025e043e80e4157a355488.png)

图5 前端BFF模式



这种模式的理念是，本来能一个团队交付的需求，没必要拆成两个团队，两个团队本身带来较大的沟通协作成本。本质上，也是一种将“敌我矛盾”转化为“人民内部矛盾”的思路。前端完全接手BFF的开发工作，实现数据查询的自给自足，大大减少了前后端的协作成本。但是这种模式没有提到我们关心的一些核心问题，如：复杂性如何应对、差异性如何应对、展示模型如何设计等等问题。除此之外，这种模式也存在一些前提条件及弊端，比如较为完备的前端基础设施；前端不仅仅需要关心渲染、还需要了解业务逻辑等。

**总结**：前端BFF模式通过前端自主查询和使用数据，从而达到降低跨团队协作的成本，提升BFF研发效率的效果。目前这种模式的实践代表是阿里巴巴。

## 4 基于GraphQL及元数据的信息聚合架构设计

### 4.1 整体思路

通过对后端BFF和前端BFF两种模式的分析，我们最终选择后端BFF模式，前端BFF这个方案对目前的研发模式影响较大，不仅需要大量的前端资源，而且需要建设完善的前端基础设施，方案实施成本比较高昂。

前文提到的后端GraphQL BFF模式代入我们的具体场景虽然存在一些问题，但是总体有非常大的参考价值，比如展示字段的复用思路、数据的按需查询思路等等。在商品展示场景中，**有80%的工作集中在数据的聚合和集成部分**，并且这部分具有很强的复用价值，因此信息的查询和聚合是我们面临的主要矛盾。因此，我们的思路是：**基于GraphQL+后端BFF方案改进，实现取数逻辑和展示逻辑的可沉淀、可组合、可复用**，整体架构如下示意图所示：

![图6 基于GraphQL BFF的改进思路](https://p1.meituan.net/travelcube/89256d10f2541152542a61708bd4d22492513.png)

图6 基于GraphQL BFF的改进思路



从上图可看出，与传统GraphQL BFF方案最大的差别在于我们将GraphQL下放至数据聚合部分，由于数据来源于商品领域，领域是相对稳定的，因此数据图规模可控且相对稳定。除此之外，整体架构的核心设计还包括以下三个方面：1）取数展示分离；2）查询模型归一；3）元数据驱动架构。

我们通过取数展示分离解决展示服务颗粒度问题，同时使得展示逻辑和取数逻辑可沉淀、可复用；通过查询模型归一化设计解决展示字段扩散的问题；通过元数据驱动架构实现能力的可视化，业务组件编排执行的自动化，这能够让业务开发同学聚焦于业务逻辑的本身。下面将针对这三个部分逐一展开介绍。

### 4.2 核心设计

#### 4.2.1 取数展示分离

上文提到，在商品展示场景中，展示逻辑和取数逻辑是多对多的关系，而传统的基于GraphQL的后端BFF实践方案把它们封装在一起，这是导致展示服务颗粒度难以设计的根本原因。思考一下取数逻辑和展示逻辑的关注点是什么？取数逻辑关注怎么查询和聚合数据，而展示逻辑关注怎么加工生成需要的展示字段，它们的关注点不一样，放在一起也会增加展示服务的复杂性。因此，我们的思路是将取数逻辑和展示逻辑分离开来，单独封装成逻辑单元，分别叫取数单元和展示单元。在取数展示分离之后，GraphQL也随之下沉，用于实现数据的按需聚合，如下图所示：

![图7 取数展示分离+元数据描述](https://p1.meituan.net/travelcube/7a69bcd33c7aac1aa401f9724c78a8f184459.png)

图7 取数展示分离+元数据描述



那么取数和展示逻辑的封装颗粒度是怎么样的呢？不能太小也不能太大，在颗粒度的设计上，我们有两个核心考量：1）**复用**，展示逻辑和取数逻辑在商品展示场景中，都是可以被复用的资产，我们希望它们能沉淀下来，被单独按需使用；2）**简单**，保持简单，这样容易修改和维护。基于这两点考虑，颗粒度的定义如下：

- **取数单元**：尽量只封装1个外部数据源，同时负责对外部数据源返回的模型进行简化，这部分生成的模型我们称之为取数模型。
- **展示单元**：尽量只封装1个展示字段的加工逻辑。

分开的好处是简单且可被组合使用，那么具体如何实现组合使用呢？我们的思路是通过元数据来描述它们之间的关系，基于元数据由统一的执行框架来关联运行，具体设计下文会展开介绍。通过取数和展示的分离，元数据的关联和运行时的组合调用，可以保持逻辑单元的简单，同时又满足复用诉求，这也很好地解决了传统方案中存在的**展示服务的颗粒度问题**。

#### 4.2.2 查询模型归一

展示单元的加工结果通过什么样的接口透出呢？接下来，我们介绍一下查询接口设计的问题。

**1）查询接口设计的难点**

常见查询接口的设计模式有以下两种：

- **强类型模式**：强类型模式指的是查询接口返回的是POJO对象，每一个查询结果对应POJO中的一个明确的具有特定业务含义的字段。
- **弱类型模式**：弱类型模式指的是查询结果以K-V或JSON模式返回，没有明确的静态字段。

以上两种模式在业界都有广泛应用，且它们都有明确的优缺点。强类型模式对开发者友好，但是业务是不断迭代的，与此同时，系统沉淀的展示单元会不断丰富，在这样的情况下，接口返回的DTO中的字段将会愈来愈多，每次新功能的支持，都要伴随着接口查询模型的修改，JAR版本的升级。而JAR的升级涉及数据提供方和数据消费两方，存在明显效率问题。另外，可以想象，查询模型的不断迭代，最终将会包括成百上千个字段，难以维护。

而弱类型模式恰好可以弥补这一缺点，但是弱类型模式对于开发者来说非常不友好，接口查询模型中有哪些查询结果对于开发者来说在开发的过程中完全没有感觉，但是程序员的天性就是喜欢通过代码去理解逻辑，而非配置和文档。其实，这两种接口设计模式都存在着一个共性问题——缺少抽象，下面两节，我们将介绍在接口返回的查询模型设计方面的抽象思路及框架能力支持。

**2）查询模型归一化设计**

回到商品展示场景中，一个展示字段有多种不同的实现，如商品标题的两种不同实现方式：1）商品标题；2）[类目]+商品标题。商品标题和这两种展示逻辑的关系本质上是一种抽象-具体的关系。识别这个关键点，思路就明了了，我们的思路是对查询模型做抽象。查询模型上都是抽象的展示字段，一个展示字段对应多个展示单元，如下图所示：

![图8 查询模型归一化 + 元数据描述](https://p0.meituan.net/travelcube/e4aae6b37be45f2de690e1129cecf02154846.png)

图8 查询模型归一化 + 元数据描述



在实现层面同样基于元数据描述展示字段和展示单元之间的关系，基于以上的设计思路，可以在一定程度上减缓模型的扩散，但是还不能避免扩展。比如除了价格、库存、销量等每个商品都有的标准属性之外，不同的商品类型一般还会有这个商品特有的属性。比如密室主题拼场商品才有“几人拼”这样的描述属性，这种字段本身抽象的意义不大，且放在商品查询模型中作为一个单独的字段会导致模型扩张，针对这类问题，我们的解决思路是引入扩展属性，扩展属性专门承载这类非标准的字段。通过标准字段 + 扩展属性的方式建立查询模型，能够较好地解决**字段扩散**的问题。

#### 4.2.3 元数据驱动架构

到目前为止，我们定义了如何分解**业务逻辑单元**以及如何设计**查询模型**，并提到用元数据描述它们之间的关系。基于以上定义实现的业务逻辑及模型，都具备很强的复用价值，可以作为业务资产沉淀下来。那么，为什么用元数据描述业务功能及模型之间的关系呢？

我们引入元数据描述主要有两个目的：1）代码逻辑的自动编排，通过元数据描述业务逻辑之间的关联关系，运行时可以自动基于元数据实现逻辑之间的关联执行，从而可以消除大量的人工逻辑编排代码；2）业务功能的可视化，元数据本身描述了业务逻辑所提供的功能，如下面两个示例：

> 团单基础售价字符串展示，例：30元。 团单市场价展示字段，例：100元。

这些元数据上报到系统中，可以用于展示当前系统所提供的功能。通过元数据描述组件及组件之间关联关系，通过框架解析元数据自动进行业务组件的调用执行，形成了如下的元数据架构：

![图9 元数据驱动架构](https://p0.meituan.net/travelcube/3b60a65d11a60aa6533d8e28c5589f5b111490.png)

图9 元数据驱动架构



整体架构由三个核心部分组成：

- 业务能力：标准的业务逻辑单元，包括取数单元、展示单元和查询模型，这些都是关键的可复用资产。
- 元数据：描述业务功能（如：展示单元、取数单元）以及业务功能之间的关联关系，比如展示单元依赖的数据，展示单元映射的展示字段等。
- 执行引擎：负责消费元数据，并基于元数据对业务逻辑进行调度和执行。

通过以上三个部分有机的组合在一起，形成了一个元数据驱动风格的架构。

## 5 针对GraphQL的优化实践

### 5.1 使用简化

**1）GraphQL直接使用问题**

引入GraphQL，会引入一些额外的复杂性，比如会涉及到GraphQL带来的一些概念如：Schema、RuntimeWiring，下面是基于GraphQL原生Java框架的开发过程：

![图10 原生GraphQL使用流程](https://p0.meituan.net/travelcube/cc567229c6fc727f1ec23abf2e39981f28129.png)

图10 原生GraphQL使用流程



这些概念对于未接触过GraphQL的同学来说，增加了学习和理解的成本，而这些概念和业务领域通常没有什么关系。而我们仅仅希望使用GraphQL的按需查询特性，却被GraphQL本身拖累了，业务开发同学的关注点应该聚焦在业务逻辑本身才对，这个问题如何解决呢？

著名计算机科学家David Wheeler说了一句名言，”All problems in computer science can be solved by another level of indirection”。没有加一层解决不了的问题，本质上是需要有人来对这事负责，因此我们在原生GraphQL之上增加了一层执行引擎层来解决这些问题，目标是屏蔽GraphQL的复杂性，让开发人员只需要关注业务逻辑。

**2）取数接口标准化**

首先要简化数据的接入，原生的`DataFetcher`和`DataLoader`都是处在一个比较高的抽象层次，缺少业务语义，而在查询场景，我们能够归纳出，所有的查询都属于以下三种模式：

- **1查1**：根据一个条件查询一个结果。
- **1查N**：根据一个条件查询多个结果。
- **N查N**：一查一或一查多的批量版本。

由此，我们对查询接口进行了标准化，业务开发同学基于场景判断是那种，按需选择使用即可，取数接口标准化设计如下：

![图11 查询接口标准化](https://p0.meituan.net/travelcube/d88993d39150ee563853e9167b940b4967781.png)

图11 查询接口标准化



业务开发同学按需选择所需要使用的取数器，通过泛型指定结果类型，1查1和1查N比较简单，N查N我们对其定义为批量查询接口，用于满足”N+1”的场景，其中`batchSize`字段用于指定分片大小，`batchKey`用于指定查询Key，业务开发只需要指定参数，其他的框架会自动处理。除此之外，我们还约束了返回结果必须是`CompleteFuture`，用于满足聚合查询的全链路异步化。

**3）聚合编排自动化**

取数接口标准化使得数据源的语义更清晰，开发过程按需选择即可，简化了业务的开发。但是此时业务开发同学写好`Fetcher`之后，还需要去另一个地方去写`Schema`，而且写完`Schema`还要再写`Schema`和`Fetcher`的映射关系，业务开发更享受写代码的过程，不太愿意写完代码还要去另外一个地方取配置，并且同时维护代码和对应配置也提高了出错的可能性，能否将这些冗杂的步骤移除掉？

`Schema`和`RuntimeWiring`本质上是想描述某些信息，如果这些信息换一种方式描述是不是也可以，我们的优化思路是：在业务开发过程中标记注解，通过注解标注的元数据描述这些信息，其他的事情交给框架来做。解决思路示意图如下：

![图12 注解元数据描述Schema和RuntimeWiring](https://p0.meituan.net/travelcube/9774e011766c681380eb469fa2a379ef385680.png)

图12 注解元数据描述Schema和RuntimeWiring



### 5.2 性能优化

#### 5.2.1 GraphQL性能问题

虽然GraphQL已经开源了，但是Facebook只开源了相关标准，并没有给出解决方案。GraphQL-Java框架是由社区贡献的，基于开源的GraphQL-Java作为按需查询引擎的方案，我们发现了GraphQL应用方面的一些问题，这些问题有部分是由于使用姿势不当所导致的，也有部分是GraphQL本身实现的问题，比如我们遇到的几个典型的问题：

- 耗CPU的查询解析，包括`Schema`的解析和`Query`的解析。
- 当查询模型比较复杂特别是存在大列表时候的延时问题。
- 基于反射的模型转换CPU消耗问题。
- `DataLoader`的层级调度问题。

于是，我们对使用方式和框架做了一些优化与改造，以解决上面列举的问题。本章着重介绍我们在GraphQL-Java方面的优化和改造思路。

#### 5.2.2 GraphQL编译优化

**1）GraphQL语言原理概述**

GraphQL是一种查询语言，目的是基于直观和灵活的语法构建客户端应用程序，用于描述其数据需求和交互。GraphQL属于一种领域特定语言（DSL），而我们所使用的GraphQL-Java客户端在语言编译层面是基于ANTLR 4实现的，ANTLR 4是一种基于Java编写的语言定义和识别工具，ANTLR是一种元语言（Meta-Language），它们的关系如下：

![图13 GraphQL语言基本原理示意图](https://p0.meituan.net/travelcube/c73d451b792545e1dffbef006095b9b174686.png)

图13 GraphQL语言基本原理示意图



GraphQL执行引擎所接受的`Schema`及`Query`都是基于GraphQL定义的语言所表达的内容，GraphQL执行引擎不能直接理解GraphQL，在执行之前必须由GraphQL编译器翻译成GraphQL执行引擎可理解的文档对象。而GraphQL编译器是基于Java的，经验表明在大流量场景实时解释的情况下，这部分代码将会成为CPU热点，而且还占用响应延迟，`Schema`或`Query`越复杂，性能损耗越明显。

**2）Schema及Query编译缓存**

`Schema`表达的是数据视图和取数模型同构，相对稳定，个数也不多，在我们的业务场景一个服务也就一个。因此，我们的做法是在启动的时候就将基于`Schema`构造的GraphQL执行引擎构造好，作为单例缓存下来，对于`Query`来说，每个场景的`Query`有些差异，因此`Query`的解析结果不能作为单例，我们的做法是实现`PreparsedDocumentProvider`接口，基于`Query`作为Key将`Query`编译结果缓存下来。如下图所示：

![图14 Query缓存实现示意图](https://p1.meituan.net/travelcube/81b1c7b29e8d7eb5cd2bca3b5fdbed9c77342.png)

图14 Query缓存实现示意图



#### 5.2.3 GraphQL执行引擎优化

**1）GraphQL执行机制及问题**

我们先一起了解一下GraphQL-Java执行引擎的运行机制是怎么样的。假设在执行策略上我们选取的是`AsyncExecutionStrategy`，来看看GraphQL执行引擎的执行过程：

![图15 GraphQL执行引擎执行过程](https://p1.meituan.net/travelcube/fb8708d3fe7e68cc7d8a6fa4f4e2da8f173718.png)

图15 GraphQL执行引擎执行过程



以上时序图做了些简化，去除了一些与重点无关的信息，`AsyncExecutionStrategy`的`execute`方法是对象执行策略的异步化模式实现，是查询执行的起点，也是根节点查询的入口，`AsyncExecutionStrategy`对对象的多个字段的查询逻辑，采取的是循环+异步化的实现方式，我们从`AsyncExecutionStrategy`的`execute`方法触发，理解GraphQL查询过程如下：

1. 调用当前字段所绑定的`DataFetcher`的`get`方法，如果字段没有绑定`DataFetcher`，则通过默认的`PropertyDataFetcher`查询字段，`PropertyDataFetcher`的实现是基于反射从源对象中读取查询字段。

2. 将从`DataFetcher`查询得到结果包装成`CompletableFuture`，如果结果本身是`CompletableFuture`，那么不会包装。

3. 结果

   ```
   CompletableFuture
   ```

   完成之后，调用

   ```
   completeValue
   ```

   ，基于结果类型分别处理。

   - 如果查询结果是列表类型，那么会对列表类型进行遍历，针对每个元素在递归执行`completeValue`。
   - 如果结果类型是对象类型，那么会对对象执行`execute`，又回到了起点，也就是`AsyncExecutionStrategy的execute`。

以上是GraphQL的执行过程，这个过程有什么问题呢？下面基于图上的标记顺序一起看看GraphQL在我们的业务场景中应用和实践所遇到的问题，这些问题不代表在其他场景也是问题，仅供参考：

**问题1**：`PropertyDataFetcher`CPU热点问题，`PropertyDataFetcher`在整个查询过程中属于热点代码，而其本身的实现也有一些优化空间，在运行时`PropertyDataFetcher`的执行会成为CPU热点。（具体问题可参考GitHub上的commit和Conversion：https://github.com/graphql-java/graphql-java/pull/1815）

![图16 PropertyDataFetcher成为CPU热点](https://p1.meituan.net/travelcube/a144670328dd9116740c09f3f250c8561865320.png)

图16 PropertyDataFetcher成为CPU热点



**问题2**：列表的计算耗时问题，列表计算是循环的，对于查询结果中存在大列表的场景，此时循环会造成整体查询明显的延迟。我们举个具体的例子，假设查询结果中存在一个列表大小是1000，每个元素的处理是0.01ms，那么总体耗时就是10ms，基于GraphQL的查机制，这个10ms会阻塞整个链路。

**2）类型转换优化**

通过GraphQL查询引擎拿到的GraphQL模型，和业务实现的`DataFetcher`返回的取数模型是同构，但是所有字段的类型都会被转换成GraphQL内部类型。`PropertyDataFetcher`之所以会成为CPU热点，问题就在于这个模型转换过程，业务定义的模型到GraphQL类型模型转换过程示意图如下图所示：

![图17 业务模型到GraphQL模型转换示意图](https://p0.meituan.net/travelcube/d9d8f4a804dc8db7786846719a964e8c93390.png)

图17 业务模型到GraphQL模型转换示意图



当查询结果模型中的字段非常多的时候，比如上万个，意味着每次查询有上万次的`PropertyDataFetcher`操作，实际就反映到了CPU热点问题上，这个问题我们的解决思路是保持原有业务模型不变，将非`PropertyDataFetcher`查询的结果反过来填充到业务模型上。如下示意图所示：

![图18 查询结果模型反向填充示意图](https://p0.meituan.net/travelcube/cc20f97f76e0ddc19ac77ef724f7468498880.png)

图18 查询结果模型反向填充示意图



基于这个思路，我们通过GraphQL执行引擎拿到的结果就是业务`Fetcher`返回的对象模型，这样不仅仅解决了因字段反射转换带来的CPU热点问题，同时对于业务开发来说增加了友好性。因为GraphQL模型类似JSON模型，这种模型是缺少业务类型的，业务开发直接使用起来非常麻烦。以上优化在一个场景上试点测试，结果显示该场景的平均响应时间缩短1.457ms，平均99线缩短5.82ms，平均CPU利用率降低约12%。

**3）列表计算优化**

当列表元素比较多的时候，默认的单线程遍历列表元素计算的方式所带来的延迟消耗非常明显，对于响应时间比较敏感的场景这个延迟优化很有必要。针对这个问题我们的解决思路是充分利用CPU多核心计算的能力，将列表拆分成任务，通过多线程并行执行，实现机制如下：

![图19 列表遍历多核计算思路](https://p0.meituan.net/travelcube/c48f4bfd93da1a8dfd88dd8819f579ab47249.png)

图19 列表遍历多核计算思路



#### 5.2.4 GraphQL-DataLoader调度优化

**1）DataLoader基本原理**

先简单介绍一下DataLoader的基本原理，DataLoader有两个方法，一个是`load`，一个是`dispatch`，在解决N+1问题的场景中，DataLoader是这么用的：

![图20 DataLoader基本原理](https://p0.meituan.net/travelcube/4e58a006c1ef930dd417062d93fab4cc31912.png)

图20 DataLoader基本原理



整体分为2个阶段，第一个阶段调用`load`，调用N次，第二个阶段调用`dispatch`，调用`dispatch`的时候会真正的执行数据查询，从而达到批量查询+分片的效果。

**2）DataLoader调度问题**

GraphQL-Java对DataLoader的集成支持的实现在`FieldLevelTrackingApproach`中，`FieldLevelTrackingApproach`的实现会存在怎样的问题呢？下面基于一张图表达原生DataLoader调度机制所产生的问题：

![图21 GraphQL-Java对DataLoader调度存在的问题](https://p0.meituan.net/travelcube/92a4db95c599e605d25d8f493a5862aa62026.png)

图21 GraphQL-Java对DataLoader调度存在的问题



问题很明显，基于`FieldLevelTrackingApproach`的实现，下一层级的`DataLoader`的`dispatch`是需要等到本层级的结果都回来之后才发出。基于这样的实现，查询总耗时的计算公式等于：TOTAL = MAX（Level 1 Latency）+ MAX（Level 2 Latency）+ MAX（Level 3 Latency）+ … ，**总查询耗时等于每层耗时最大的值加起来，而实际上如果链路编排由业务开发同学自己来写的话，理论上的效果是总耗时等于所有链路最长的那个链路所耗的时间**，这个才是合理的。而`FieldLevelTrackingApproach`的实现所表现出来的结果是反常识的，至于为什么这么实现，目前我们理解可能是设计者基于简单和通用方面的考虑。

问题在于以上的实现在有些业务场景下是不能接受的，比如我们的列表场景的响应时间约束一共也就不到100ms，其中几十ms是因为这个原因搭进去的。针对这个问题的解决思路，一种方式是对于响应时间要求特别高的场景独立编排，不采用GraphQL；另一种方式是在GraphQL层面解决这个问题，保持架构的统一性。接下来，介绍一下我们是如何扩展GraphQL-Java执行引擎来解决这个问题的。

**3）DataLoader调度优化**

针对DataLoader调度的性能问题，**我们的解决思路是在最后一次调用某个`DataLoader`的`load`之后，立即调用`dispatch`方法发出查询请求**，问题是我们怎么知道哪一次的load是最后一次load呢？这个问题也是解决DataLoader调度问题的难点，以下举个例子来解释我们的解决思路：

![图22 查询对象结果示意图](https://p1.meituan.net/travelcube/2bd14ce27c7084135d0e5aed505d6ed047007.png)

图22 查询对象结果示意图



假设我们查询到的模型结构如下：根节点是`Query`下的字段，字段名叫`subjects`，`subject`引用的是个列表，`subject`下有两个元素，都是`ModelA`的对象实例，`ModelA`有两个字段，`fieldA`和`fieldB`，`subjects[0]`的`fieldA`关联是`ModelB`的一个实例，`subjects[0]`的`fieldB`关联多个`ModelC`实例。

为了方便理解，我们定义一些概念，字段、字段实例、字段实例执行完、字段实例值大小、字段实例值对象执行大小、字段实例值对象执行完等等：

- **字段**：具有唯一路径，是静态的，和运行时对象大小没有关系，如：`subjects`和`subjects/fieldA`。
- **字段实例**：字段的实例，具有唯一路径，是动态的，跟运行时对象大小有关系，如：`subjects[0]/fieldA`和`subjects[1]/fieldA`是字段`subjects/fieldA`的实例。
- **字段实例执行完**：字段实例关联的对象实例都被GraphQL执行完了。
- **字段实例值大小**：字段实例引用对象实例的个数，如以上示例，`subjects[0]/fieldA`字段实例值大小是1，`subjects[0]/fieldB`字段实例值大小是3。

除了以上定义之外，我们的业务场景还满足以下条件：

- 只有1个根节点，且根节点是列表。
- `DataLoader`一定属于某个字段，某个字段下的`DataLoader`应该被执行次数等于其下的对象实例个数。

基于以上信息，我们可以得出以下问题分析：

- 在执行字段实例的时候，我们可以知道当前字段实例的大小，字段实例的大小等于字段关联`DataLoader`在当前实例下需要执行`load`的次数，因此在执行`load`之后，我们可以知道当前对象实例是否是其所在字段实例的最后一个对象。
- 一个对象的实例可能会挂在不同的字段实例下，所以仅当当前对象实例是其所在字段实例的最后一个对象实例的时候，不代表当前对象实例是所有对象实例中的最后一个，当且仅当对象实例所在节点实例是节点的最后一个实例的时候才成立。
- 我们可从字段实例大小推算字段实例的个数，比如我们知道`subjects`的大小是2，那么就知道`subjects`字段有两个字段实例`subjects[0]`和`subjects[1]`，也就知道字段`subjects/fieldA`有两个实例，`subjects[0]/fieldA`和`subjects[1]/fieldA`，因此我们从根节点可以往下推断出某个字段实例是否执行完。

通过以上分析，我们可以得出，一个对象执行完的条件是其所在的字段实例以及其所在的字段所有的父亲字段实例都执行完，且当前执行的对象实例是其所在字段实例的最后一个对象实例的时候。基于这个判断逻辑，我们的实现方案是在每次调用完`DataFetcher`的时候，判断是否需要发起`dispatch`，如果是则发起。另外，以上时机和条件存在漏发`dispatch`的问题，有个特殊情况，当当前对象实例不是最后一个，但是剩下的对象大小都为0的时候，那么就永远不会触发当前对象关联的`DataLoader`的`load`了，所以在对象大小为0的时候，需要额外再判断一次。

根据以上逻辑分析，我们实现了`DataLoader`调用链路的最优化，达到理论最优的效果。

## 6 新架构对研发模式的影响

生产力决定生产关系，元数据驱动信息聚合架构是展示场景搭建的核心生产力，而业务开发模式和过程是生产关系，因此也会随之改变。下面我们将会从开发模式和流程两个角度来介绍新架构对研发带来的影响。

### 6.1 聚焦业务的开发模式

新架构提供了一套基于业务抽象出的标准化代码分解约束。以前开发同学对系统的理解很可能就是“查一查服务，把数据粘在一起”，而现在，研发同学对于业务的理解及代码分解思路将会是一致的。比如展示单元代表的是展示逻辑，取数单元代表的是取数逻辑。同时，很多冗杂且容易出错的逻辑已经被框架屏蔽掉了，研发同学能够有更多的精力聚焦于业务逻辑本身，比如：业务数据的理解和封装，展示逻辑的理解和编写，以及查询模型的抽象和建设。如下示意图所示：

![图23 业务开发聚焦业务本身](https://p0.meituan.net/travelcube/2f52f274a01d8a1f653710609e4587c153662.png)

图23 业务开发聚焦业务本身



### 6.2 研发流程升级

新架构不仅仅影响了研发的代码编写，同时也影响着研发流程的改进，基于元数据架构实现的可视化及配置化能力，现有研发流程和之前研发流程相比有了明显的区别，如下图所示：

![图24 基于开发框架搭建展示场景前后研发流程对比](https://p0.meituan.net/travelcube/d0f2f87b8ba492aa36d753700d7aceda157241.jpg)

图24 基于开发框架搭建展示场景前后研发流程对比



以前是“一杆子捅到底”的开发模式，每个展示场景的搭建需要经历过从接口的沟通到API的开发整个过程，基于新架构之后，系统自动具备多层复用及可视化、配置化能力。

**情况一**：这是最好的情况，此时取数功能和展示功能都已经被沉淀下来，研发同学需要做的只是创建查询方案，基于运营平台按需选择需要的展示单元，拿着查询方案ID基于查询接口就可以查到需要的展示信息了，可视化、配置化界面如下示意图所示：

![图25 可视化及文案按需选用](https://p0.meituan.net/travelcube/e25868ee8b71dea4fecd7845ba689011173259.png)

图25 可视化及文案按需选用



**情况二**：此时可能没有展示功能，但是通过运营平台查看到，数据源已经接入过，那么也不难，只需要基于现有的数据源编写一段加工逻辑即可，这段加工逻辑是非常爽的一段纯逻辑的编写，数据源列表如下示意图所示：

![图26 数据源列表可视化](https://p0.meituan.net/travelcube/1cb7b12d5828ac8b6d3e0c3e9d3a7fa8122676.png)

图26 数据源列表可视化



**情况三**：最坏的情况是此时系统不能满足当前的查询能力，这种情况比较少见，因为后端服务是比较稳定的，那么也无需惊慌，只需要按照标准规范将数据源接入进来，然后编写加工逻辑片段即可，之后这些能力是可以被持续复用的。

## 7 总结

商品展示场景的复杂性体现在：场景多、依赖多、逻辑多，以及不同场景之间存在差异。在这样的背景下，如果是业务初期，怎么快怎么来，采用“烟囱式”个性化建设的方式不必有过多的质疑。但是随着业务的不断发展，功能的不断迭代，以及场景的规模化趋势，“烟囱式”个性化建设的弊端会慢慢凸显出来，包括代码复杂度高、缺少能力沉淀等问题。

本文以基于对美团到店商品展示场景所面临的核心矛盾分析，介绍了：

- 业界不同的BFF应用模式，以及不同模式的优势和缺点。
- 基于GraphQL BFF模式改进的元数据驱动的架构方案设计。
- 我们在GraphQL实践过程中遇到的问题及解决思路。
- 新架构对研发模式产生的影响呈现。

目前，笔者所在团队负责的核心商品展示场景都已迁入新架构，基于新的研发模式，我们实现了50%以上的展示逻辑复用以及1倍以上的效率提升，希望本文对大家能够有所帮助。

## 8 参考文献

- [1]https://samnewman.io/patterns/architectural/bff/
- [2] https://www.thoughtworks.com/cn/radar/techniques/graphql-for-server-side-resource-aggregation
- [3] [了解电商后台系统，看这篇就够了](http://www.woshipm.com/pd/3712746.html)
- [4][框架定义-百度百科](https://baike.baidu.com/item/框架/1212667?fr=aladdin)
- [5] [高效研发-闲鱼在数据聚合上的探索与实践](https://mp.weixin.qq.com/s?__biz=MzU4MDUxOTI5NA==&mid=2247483947&idx=1&sn=d1b600e57cca8c7304cca77a693c0b32&chksm=fd54d63aca235f2c5abf8f65b3a0f235b55723d014aa4c7c75c48fcb5ce5c32b14fb43e5b40b&mpshare=1&scene=1&srcid=0404dHE42TNCAGO7clq9K2ep#rd)
- [6] 《系统架构-复杂系统的产品设计与开发》

## 9 招聘信息

美团到店综合研发中心长期招聘前端、后端、数据仓库、机器学习/数据挖掘算法工程师，坐标上海，欢迎感兴趣的同学发送简历至：tech@meituan.com（邮件标题注明：美团到店综合研发中心—上海）。

原文作者：美团技术团队

原文链接：https://tech.meituan.com/2021/05/06/bff-graphql.html

# 【NO.255】美团外卖实时数仓建设实践

实时数仓以端到端低延迟、SQL标准化、快速响应变化、数据统一为目标。美团外卖数据智能组总结的最佳实践是：一个通用的实时生产平台跟一个通用交互式实时分析引擎相互配合，同时满足实时和准实时业务场景。两者合理分工，互相补充，形成易开发、易维护且效率高的流水线，兼顾开发效率与生产成本，以较好的投入产出比满足业务的多样性需求。

## 1.实时场景

![img](https://p0.meituan.net/travelcube/58ea4421534a570e84372e362472cb28216607.png)

实时数据在美团外卖的场景是非常多的，主要有以下几个方面：

- **运营层面**：比如实时业务变化，实时营销效果，当日营业情况以及当日分时业务趋势分析等。
- **生产层面**：比如实时系统是否可靠，系统是否稳定，实时监控系统的健康状况等。
- **C端用户**：比如搜索推荐排序，需要实时行为、特点等特征变量的生产，给用户推荐更加合理的内容。
- **风控侧**：实时风险识别、反欺诈、异常交易等，都是大量应用实时数据的场景。

## 2.实时技术及架构

### 2.1 实时计算技术选型

目前，市面上已经开源的实时技术还是很多的，比较通用的有[Storm](https://tech.meituan.com/2016/10/21/test-of-storms-reliability.html)、Spark Streaming以及[Flink](https://tech.meituan.com/2018/10/18/meishi-data-flink.html)，技术同学在做选型时要根据公司的具体业务来进行部署。

美团外卖依托于美团整体的基础数据体系建设，从技术成熟度来讲，公司前几年主要用的是Storm。当时的Storm，在性能稳定性、可靠性以及扩展性上也是无可替代的。但随着Flink越来越成熟，从技术性能上以及框架设计优势上已经超越了Storm，从趋势来讲就像Spark替代MR一样，Storm也会慢慢被Flink替代。当然，从Storm迁移到Flink会有一个过程，我们目前有一些老的任务仍然运行在Storm上，也在不断推进任务迁移。

![img](https://p0.meituan.net/travelcube/15d2026e63ec7bdafa9c25b30eba39b6225524.png)

具体[Storm和Flink的对比](https://tech.meituan.com/2017/11/17/flink-benchmark.html)可以参考上图表格。

### 2.2 实时架构

**① Lambda架构**

![img](https://p1.meituan.net/travelcube/99dd755cb5a447dc1a547d782a997db1751042.png)

Lambda是比较经典的一款架构，以前实时的场景不是很多，以离线为主，当附加了实时场景后，由于离线和实时的时效性不同，导致技术生态是不一样的。而Lambda架构相当于附加了一条实时生产链路，在应用层面进行一个整合，双路生产，各自独立。在业务应用中，顺理成章成为了一种被采用的方式。

双路生产会存在一些问题，比如加工逻辑Double，开发运维也会Double，资源同样会变成两个资源链路。因为存在以上问题，所以又演进了一个Kappa架构。

**② Kappa架构**

![img](https://p0.meituan.net/travelcube/b56fa6bad760f1dcd3a889df4c570508795045.png)

Kappa从架构设计来讲，比较简单，生产统一，一套逻辑同时生产离线和实时。但是在实际应用场景有比较大的局限性，在业内直接用Kappa架构生产落地的案例不多见，且场景比较单一。这些问题在美团外卖这边同样会遇到，我们也会有自己的一些思考，将会在后面的章节进行阐述。

## 3.业务痛点

首先，在外卖业务上，我们遇到了一些问题和挑战。在业务早期，为了满足业务需要，一般是Case By Case地先把需求完成。业务对于实时性要求是比较高的，从时效性的维度来说，没有进行中间层沉淀的机会。在这种场景下，一般是拿到业务逻辑直接嵌入，这是能想到的简单有效的方法，在业务发展初期这种开发模式也比较常见。

![img](https://p0.meituan.net/travelcube/26bf56b7723b411de9bc33d4f4077f77236929.png)

如上图所示，拿到数据源后，我们会经过数据清洗、扩维，通过Storm或Flink进行业务逻辑处理，最后直接进行业务输出。把这个环节拆开来看，数据源端会重复引用相同的数据源，后面进行清洗、过滤、扩维等操作，都要重复做一遍。唯一不同的是业务的代码逻辑是不一样的，如果业务较少，这种模式还可以接受，但当后续业务量上去后，会出现谁开发谁运维的情况，维护工作量会越来越大，作业无法形成统一管理。而且所有人都在申请资源，导致资源成本急速膨胀，资源不能集约有效利用，因此要思考如何从整体来进行实时数据的建设。

## 4.数据特点与应用场景

那么如何来构建实时数仓呢？首先要进行拆解，有哪些数据，有哪些场景，这些场景有哪些共同特点，对于外卖场景来说一共有两大类，日志类和业务类。

![img](https://p0.meituan.net/travelcube/ec4f7547de112ce86395067db703e118210844.png)

- **日志类**：数据量特别大，半结构化，嵌套比较深。日志类的数据有个很大的特点，日志流一旦形成是不会变的，通过埋点的方式收集平台所有的日志，统一进行采集分发，就像一颗树，树根非常大，推到前端应用的时候，相当于从树根到树枝分叉的过程（从1到n的分解过程）。如果所有的业务都从根上找数据，看起来路径最短，但包袱太重，数据检索效率低。日志类数据一般用于生产监控和用户行为分析，时效性要求比较高，时间窗口一般是5min或10min，或截止到当前的一个状态，主要的应用是实时大屏和实时特征，例如用户每一次点击行为都能够立刻感知到等需求。
- **业务类**：主要是业务交易数据，业务系统一般是自成体系的，以Binlog日志的形式往下分发，业务系统都是事务型的，主要采用范式建模方式。特点是结构化，主体非常清晰，但数据表较多，需要多表关联才能表达完整业务，因此是一个n到1的集成加工过程。

而业务类实时处理，主要面临的以下几个难点：

- **业务的多状态性**：业务过程从开始到结束是不断变化的，比如从下单->支付->配送，业务库是在原始基础上进行变更的，Binlog会产生很多变化的日志。而业务分析更加关注最终状态，由此产生数据回撤计算的问题，例如10点下单，13点取消，但希望在10点减掉取消单。
- **业务集成**：业务分析数据一般无法通过单一主体表达，往往是很多表进行关联，才能得到想要的信息，在实时流中进行数据的合流对齐，往往需要较大的缓存处理且复杂。
- **分析是批量的，处理过程是流式的**：对单一数据，无法形成分析，因此分析对象一定是批量的，而数据加工是逐条的。

日志类和业务类的场景一般是同时存在的，交织在一起，无论是Lambda架构还是Kappa架构，单一的应用都会有一些问题。因此针对场景来选择架构与实践才更有意义。

## 5.实时数仓架构设计

### 5.1 实时架构：流批结合的探索

基于以上问题，我们有自己的思考。通过流批结合的方式来应对不同的业务场景。

![img](https://p0.meituan.net/travelcube/71b6343de8b78d190ef260d29e4b6a22166988.png)

如上图所示，数据从日志统一采集到消息队列，再到数据流的ETL过程，作为基础数据流的建设是统一的。之后对于日志类实时特征，实时大屏类应用走实时流计算。对于Binlog类业务分析走实时OLAP批处理。

流式处理分析业务的痛点是什么？对于范式业务，Storm和Flink都需要很大的外存，来实现数据流之间的业务对齐，需要大量的计算资源。且由于外存的限制，必须进行窗口的限定策略，最终可能放弃一些数据。计算之后，一般是存到Redis里做查询支撑，且KV存储在应对分析类查询场景中也有较多局限。

实时OLAP怎么实现？有没有一种自带存储的实时计算引擎，当实时数据来了之后，可以灵活的在一定范围内自由计算，并且有一定的数据承载能力，同时支持分析查询响应呢？随着技术的发展，目前MPP引擎发展非常迅速，性能也在飞快提升，所以在这种场景下就有了一种新的可能。这里我们使用的是Doris引擎。

这种想法在业内也已经有实践，且成为一个重要探索方向。阿里基于ADB的实时OLAP方案等。

### 5.2 实时数仓架构设计

从整个实时数仓架构来看，首先考虑的是如何管理所有的实时数据，资源如何有效整合，数据如何进行建设。

从方法论来讲，实时和离线是非常相似的。离线数仓早期的时候也是Case By Case，当数据规模涨到一定量的时候才会考虑如何治理。分层是一种非常有效的数据治理方式，所以在实时数仓如何进行管理的问题上，首先考虑的也是分层的处理逻辑，具体内容如下：

- **数据源**：在数据源的层面，离线和实时在数据源是一致的，主要分为日志类和业务类，日志类又包括用户日志、DB日志以及服务器日志等。
- **实时明细层**：在明细层，为了解决重复建设的问题，要进行统一构建，利用离线数仓的模式，建设统一的基础明细数据层，按照主题进行管理，明细层的目的是给下游提供直接可用的数据，因此要对基础层进行统一的加工，比如清洗、过滤、扩维等。
- **汇总层**：汇总层通过Flink或Storm的简洁算子直接可以算出结果，并且形成汇总指标池，所有的指标都统一在汇总层加工，所有人按照统一的规范管理建设，形成可复用的汇总结果。

![img](https://p0.meituan.net/travelcube/3a5448c81bf7f66868f8c89eca68b5ee232700.png)

总结起来，从整个实时数仓的建设角度来讲，首先数据建设的层次化要先建出来，先搭框架，然后定规范，每一层加工到什么程度，每一层用什么样的方式，当规范定义出来后，便于在生产上进行标准化的加工。由于要保证时效性，设计的时候，层次不能太多，对于实时性要求比较高的场景，基本可以走上图左侧的数据流，对于批量处理的需求，可以从实时明细层导入到实时OLAP引擎里，基于OLAP引擎自身的计算和查询能力进行快速的回撤计算，如上图右侧的数据流。

## 6.实时平台化建设

架构确定之后，我们后面考虑的是如何进行平台化的建设，实时平台化建设是完全附加于实时数仓管理之上进行的。

![img](https://p0.meituan.net/travelcube/42e18eb72f70efb20003045a4da9fbbe234495.png)

首先进行功能的抽象，把功能抽象成组件，这样就可以达到标准化的生产，系统化的保障就可以更深入的建设，对于基础加工层的清洗、过滤、合流、扩维、转换、加密、筛选等功能都可以抽象出来，基础层通过这种组件化的方式构建直接可用的数据结果流。这会产生一个问题，用户的需求多样，为了满足了这个用户，如何兼容其他的用户，因此可能会出现冗余加工的情况。从存储的维度来讲，实时数据不存历史，不会消耗过多的存储，这种冗余是可以接受的，通过冗余的方式可以提高生产效率，是一种**以空间换时间**思想的应用。

通过基础层的加工，数据全部沉淀到IDL层，同时写到OLAP引擎的基础层，再往上是实时汇总层计算，基于Storm、Flink或Doris，生产多维度的汇总指标，形成统一的汇总层，进行统一的存储分发。

当这些功能都有了以后，元数据管理，指标管理，数据安全性、SLA、数据质量等系统能力也会逐渐构建起来。

### 6.1 实时基础层功能

实时基础层的建设要解决一些问题。首先是一条流重复读的问题，一条Binlog打过来，是以DB包的形式存在的，用户可能只用其中一张表，如果大家都要用，可能存在所有人都要接这个流的问题。解决方案是可以按照不同的业务解构出来，还原到基础数据流层，根据业务的需要做成范式结构，按照数仓的建模方式进行集成化的主题建设。

![img](https://p0.meituan.net/travelcube/b1737baa389ef508cc91b1951e63d9d7217149.png)

其次要进行组件的封装，比如基础层的清洗、过滤、扩维等功能，通过一个很简单的表达入口，让用户将逻辑写出来。数据转换环节是比较灵活的，比如从一个值转换成另外一个值，对于这种自定义逻辑表达，我们也开放了自定义组件，可以通过Java或Python开发自定义脚本，进行数据加工。

### 6.2 实时特征生产功能

特征生产可以通过SQL语法进行逻辑表达，底层进行逻辑的适配，透传到计算引擎，屏蔽用户对计算引擎的依赖。就像对于离线场景，目前大公司很少通过代码的方式开发，除非一些特别的Case，所以基本上可以通过SQL化的方式表达。

![img](https://p0.meituan.net/travelcube/fe8f8c512a69f7cbe8b2b35676a3fab1119056.png)

在功能层面，把指标管理的思想融合进去，原子指标、派生指标，标准计算口径，维度选择，窗口设置等操作都可以通过配置化的方式，这样可以统一解析生产逻辑，进行统一封装。

还有一个问题，同一个源，写了很多SQL，每一次提交都会起一个数据流，比较浪费资源，我们的解决方案是，通过同一条流实现动态指标的生产，在不停服务的情况下可以动态添加指标。

所以在实时平台建设过程中，更多考虑的是如何更有效的利用资源，在哪些环节更能节约化的使用资源，这是在工程方面更多考虑的事情。

### 6.3 SLA建设

SLA主要解决两个问题，一个是端到端的SLA，一个是作业生产效率的SLA，我们采用埋点+上报的方式，由于实时流比较大，埋点要尽量简单，不能埋太多的东西，能表达业务即可，每个作业的输出统一上报到SLA监控平台，通过统一接口的形式，在每一个作业点上报所需要的信息，最后能够统计到端到端的SLA。

![img](https://p0.meituan.net/travelcube/1bdc84d2f72c50c11d18bea69a583363372328.png)

在实时生产中，由于链路非常长，无法控制所有链路，但是可以控制自己作业的效率，所以作业SLA也是必不可少的。

### 6.4 实时OLAP方案

**问题**

- **Binlog业务还原复杂**：业务变化很多，需要某个时间点的变化，因此需要进行排序，并且数据要存起来，这对于内存和CPU的资源消耗都是非常大的。
- **Binlog业务关联复杂**：流式计算里，流和流之间的关联，对于业务逻辑的表达是非常困难的。

**解决方案**

通过带计算能力的OLAP引擎来解决，不需要把一个流进行逻辑化映射，只需要解决数据实时稳定的入库问题。

![img](https://p0.meituan.net/travelcube/3d959928492bdd5d96f4a7361d576b0c371067.png)

我们这边采用的是Doris作为高性能的OLAP引擎，由于业务数据产生的结果和结果之间还需要进行衍生计算，Doris可以利用Unique模型或聚合模型快速还原业务，还原业务的同时还可以进行汇总层的聚合，也是为了复用而设计。应用层可以是物理的，也可以是逻辑化视图。

这种模式重在解决业务回撤计算，比如业务状态改变，需要在历史的某个点将值变更，这种场景用流计算的成本非常大，OLAP模式可以很好的解决这个问题。

## 7.实时应用案例

最后通过一个案例说明，比如商家要根据用户历史下单数给用户优惠，商家需要看到历史下了多少单，历史T+1的数据要有，今天实时的数据也要有，这种场景是典型的Lambda架构。我们可以在Doris里设计一个分区表，一个是历史分区，一个是今日分区，历史分区可以通过离线的方式生产，今日指标可以通过实时的方式计算，写到今日分区里，查询的时候进行一个简单的汇总。

![img](https://p0.meituan.net/travelcube/601619e9869038371459a3baf5240ae2164958.png)

这种场景看起来比较简单，难点在于商家的量上来之后，很多简单的问题都会变得复杂。后续，我们也会通过更多的业务输入，沉淀出更多的业务场景，抽象出来形成统一的生产方案和功能，以最小化的实时计算资源支撑多样化的业务需求，这也是未来我们需要达到的目的。

原文作者：美团技术团队

原文链接：https://tech.meituan.com/2021/08/26/data-warehouse-in-meituan-waimai.html

# 【NO.256】FlutterWeb性能优化探索与实践

## 1.背景

### 1.1 关于FlutterWeb

时间回拨到 2018 年，Google 首次公开 FlutterWeb Beta 版，表露出要实现一份代码、多端运行的愿景。经过无数工程师两年多的努力，在今年年初（2021 年 3 月份），Flutter 2.0 正式对外发布，它将 FlutterWeb 功能并入了 Stable Channel，意味着 Google 更加坚定了多端复用的决心。

![图1 FlutterWeb历史](https://p0.meituan.net/travelcube/74f03228b641dda28ac8e3c01f92202436263.jpg)

图1 FlutterWeb历史



当然 Google 的“野心”不是没有底气的，主要体现在它强大的跨端能力上，我们看一下 Flutter 的跨端能力在 Web 侧是如何体现的：

![图2 Flutter跨端能力](https://p0.meituan.net/travelcube/d25b2af72b128a3397334d73bd1b5c01156510.jpg)

图2 Flutter跨端能力



上图分别是 FlutterNative 和 FlutterWeb 的架构图。通过对比可以看出，应用层 Framework 是公用的，意味着在 FlutterWeb 中我们也可以直接使用 Widgets、Gestures 等组件来实现逻辑跨端。而关于渲染跨端，FlutterWeb 提供了两种模式来对齐 Engine 层的渲染能力：Canvaskit Render 和 HTML Render，下方表格对两者的区别进行了对比：

![图3 模式对比](https://p0.meituan.net/travelcube/0adf2c73ec2397521b897c67a5ae1afc142170.png)

图3 模式对比



**Canvaskit Render 模式**：底层基于 Skia 的 WebAssembly 版本，而上层使用 WebGL 进行渲染，因此能较好地保证一致性和滚动性能，但糟糕的兼容性（WebAssembly 从 Chrome 57 版本才开始支持）是我们需要面对的问题。此外 Skia 的 WebAssembly 文件大小达到了 2.5M，且 Skia 自绘引擎需要字体库支持，这意味着需要依赖超大的中文字体文件，对页面加载性能影响较大，因此目前并不推荐在 Web 中直接使用 Canvaskit Render（官方也建议将 Canvaskit Render 模式用于桌面应用）。

**HTML Render 模式**：利用 HTML + Canvas 对齐了 Engine 层的渲染能力，因此兼容性表现优秀。另外，MTFlutterWeb 对滚动性能已有过探索和实践，目前能够应对大部分业务场景。而关于加载性能，此模式下的初始包为 1.2M，是 Canvaskit Render 模式产物体积的 1/2，且我们可对编译流程进行干预，控制输出产物，因此优化空间较大。

基于以上原因，美团外卖技术团队选择在 HTML Render 模式下对 FlutterWeb 页面的性能进行优化探索。

### 1.2 业务现状

美团外卖商家端以 App、PC 等多元化的形态为商家提供了订单管理、商品维护、顾客评价、外卖课堂等一系列服务，且 App、PC 双端业务功能基本对齐。此外，我们还在 PC 上特供了针对连锁商家的多店管理功能。同时，为满足平台运营诉求，部分业务具有外投 H5 场景，例如美团外卖商家课堂，它是一个以文章、视频等形式帮助商家学习外卖运营知识、了解行业发展和跟进经营策略的内容平台，具有较强的传播属性，因此我们提供了站外分享的能力。

![图4 业务形态](https://p0.meituan.net/travelcube/8721e4664a78c49b7367ed1c9de54cd11909982.png)

图4 业务形态



为了实现多端（App、PC、H5）复用，提升研发效率，我们于 2021 年年初开始着手 [MTFlutterWeb](https://mp.weixin.qq.com/s/GjFC5_85pIk9EbKPJXZsXg) 研发体系的建设。目前，我们基于 MTFlutterWeb 完成提效的业务超过了 9 个，在 App 中，能够基于 FlutterNative 提供高性能的服务；在 PC 端和 Mobile 浏览器中，利用 FlutterWeb 做到了低成本适配，提升了产研的整体效率。

然而，加载性能问题是 MTFlutterWeb 应用推广的最大障碍。这里依然以美团外卖商家课堂业务为例，在项目之初页面完全加载时间 TP90 线达到了 6s 左右，距离我们的指标基线值（页面完全加载时间 TP90 线不高于 3s，基线值主要依据美团外卖商家端的业务场景、用户画像等来确定）有些差距，用户访问体验有很大的提升空间，因此 FlutterWeb 页面加载性能优化，是我们亟需解决的问题。

## 2.挑战

不过，想要突破 FlutterWeb 页面加载的性能瓶颈，我们面临的挑战也是巨大的。这主要体现在 FlutterWeb 缺失静态资源的优化策略，以及复杂的架构设计和编译流程。下图展示了 Flutter 业务代码被转换成 Web 平台产物的流程，我们来具体进行分析：

![图5 FlutterWeb 编译流程](https://p1.meituan.net/travelcube/ea32b330219a471097a9b3b2f8e02f6f108678.jpg)

图5 FlutterWeb 编译流程



1. **Framework、Flutter_Web_SDK**（Flutter_Web_SDK 基于 HTML、Canvas，承载 HTML Render 模式的具体实现）等底层 SDK 是可被业务代码直接引入的，帮助我们快速开发出跨端应用；
2. **flutter_tools** 是各平台（Android、iOS、Web）的编译入口，它接收 flutter build web 命令和参数并开始编译流程，同时等待处理结果回调，在回调中我们可对编译产物进行二次加工；
3. **frontend_server** 负责将 Dart 转换为 AST，生成 kernel 中间产物 app.dill 文件（实际上各平台的编译过程都会生成这样的中间产物），并交由各平台 Compiler 进行转译；
4. **Dart2JS Compiler** 是 Dart-SDK 中具体负责转译 JS 的模块，它将上述中间产物 app.dill 进行读取和解析，并注入 Math、List、Map 等 JS 工具方法，最终生产出 Web 平台所能执行的 JS 文件。
5. **编译产物**主要为 main.dart.js、index.html、images 等静态资源，FlutterWeb 对这些静态资源缺少常规 Web 项目中的优化手段，例如：文件 Hash 化、文件分片、CDN 支持等。

可以看出，要完成对 FlutterWeb 编译产物的优化，就需要干预 FlutterWeb 的众多编译模块。而为了提升整体的编译效率，大部分模块都被提前编译成了 snapshot 文件（ 一种 Dart 的编译产物，可被 Dart VM 所运行，用于提升执行效率），例如：flutter_tools.snapshot、frontend_server.snapshot、dart2js.snapshot 等，这又加大了对 FlutterWeb 编译流程进行干预的难度。

## 3.整体设计

如前文所述，为了实现逻辑、渲染跨平台，Flutter 的架构设计及编译流程都具有一定的复杂性。但由于各平台（Android、iOS、Web）的具体实现是解耦的，因此我们的思路是定位各模块（Dart-SDK、Framework、Flutter_Web_SDK、flutter_tools）的 Web 平台实现并寻求优化，整体设计图如下所示：

![图6 整体设计](https://p0.meituan.net/travelcube/f965d1903516dd6ba8613a6b691d7ef7165927.jpg)

图6 整体设计



- **SDK 瘦身**：我们分别对 FlutterWeb 所依赖的 Dart-SDK、Framework、Flutter_Web_SDK 进行了瘦身，并将这些精简版 SDK 集成合入 CI/CD（持续集成与部署）系统，为减小产物包体积奠定了基础；
- **编译优化**：此外，我们在 flutter_tools 中的编译流程做了干预，分别进行了 JS 文件分片、静态资源 Hash 化、资源文件上传 CDN 等优化，使得这些在常规 Web 应用中基础的性能优化手段得以在 FlutterWeb 中落地。同时加强了 FlutterWeb 特殊场景下的资源优化，如：字体图标精简、Runtime Manifest 隔离、Mobile/PC 分平台打包等；
- **加载优化**：在编译阶段进行静态资源优化后，我们在前端运行时，支持了资源预加载与按需加载，通过设定合理的加载时机，从而减小初始代码体积，提升页面首屏的渲染速度。

下面，我们分别对各项优化进行详细的说明。

## 4.设计与实践

### 4.1 精简 SDK

#### 4.1.1 包体积分析

工欲善其事，必先利其器，在开始做体积裁剪之前，我们需要一套类似于 [webpack-bundle-analyzer](https://www.npmjs.com/package/webpack-bundle-analyzer) 的包体积分析工具，便于直观地比较各个模块的体积占比，为优化性能提供帮助。

Dart2JS 官方提供了 [–dump-info](https://dart.dev/tools/dart2js) 命令选项来分析 JS 产物，但其表现差强人意，它并不能很好地分析各个模块的体积占比。这里更推荐使用 [source-map-explorer](https://www.npmjs.com/package/source-map-explorer) ，它的原理是通过 sourcemap 文件进行反解，能清晰地反映出每个模块的占用大小，为 SDK 的精简提供了指引。下图展示了 FlutterWeb JS 产物的反解信息（截图仅包含 Framework 和 Flutter_Web_SDK）：

![图7 反解信息](https://p0.meituan.net/travelcube/a4d3a6c3838bffe372798b1c5a644653732286.png)

图7 反解信息



#### 4.1.2 SDK 裁剪

FlutterWeb 依赖的 SDK 主要包括 Dart-SDK、Framework 和 Flutter_Web_SDK，这些 SDK 对包体积的影响是巨大的，几乎贡献了初始化包的所有大小。虽然在 Release 模式下的编译流程中，Dart Compiler 会利用 [Tree-Shaking](https://dart.dev/tools/dart2js#helping-dart2js-generate-efficient-code) 来剔除那些引入但未使用的 packages、classes、functions 等，很大程度上减少了包体积。但这些 SDK 中仍然存在一些能被进一步优化的代码。

以 Flutter Framework 为例，由于它是全平台公用的模块，因此不可避免地存在各平台的兼容逻辑（通常以 if-else、switch 等条件判断形式出现），而这部分代码是不能被 Tree-Shaking 剔除的，我们观察如下的代码：

```dart
// FileName: flutter/lib/src/rendering/editable.dart
void _handleKeyEvent(RawKeyEvent keyEvent) {
  if (kIsWeb) {
    // On web platform, we should ignore the key.
    return;
  }
  // Other codes ...
}
```

上述代码选自 Framework 中的 RenderEditable 类，当 kIsWeb 变量为真，表示当前应用运行在 Web 平台。受限于 Tree-Shaking 的机制原理，上述代码中，其它平台的兼容逻辑即注释 Other codes 的部分是无法被剔除的，但这部分代码，对 Web 平台来说却是 Dead Code（永远不可能被执行到的代码），是可以被进一步优化的。

![图8 部分功能构成](https://p1.meituan.net/travelcube/49bbeaa272a3044c362f3bbf520d14ff197851.jpg)

图8 部分功能构成



上图展示了 SDK 的一部分功能构成，从图中可以看出，FlutterWeb 依赖的这些 SDK 中包含了一些使用频率较低的功能，例如：蓝牙、USB、WebRTC、陀螺仪等功能的支持。为此，我们提供了对这些长尾功能的定制能力（这些功能默认不开启，但业务可配置），将未被启用长尾的功能进行裁剪。

通过上述分析可得，我们的思路就是对 Dead Code 进行二次剔除，以及对这些长尾功能做裁剪。基于这样的思路，我们深入 Dart-SDK、Framework 和 Flutter_Web_SDK 各个击破，最终将 JS Bundle 产物体积由 1.2M 精简至 0.7M，为 FlutterWeb 页面性能优化打下了坚实的基础。

![图9 精简成果](https://p0.meituan.net/travelcube/7ca944005eb0148f2bd163fc5da1d49815909.jpg)

图9 精简成果



#### 4.1.3 SDK 集成 CI/CD

为了提升构建效率，我们将 FlutterWeb 依赖的环境定制为 Docker 镜像，集成入 CI/CD（持续集成与部署）系统。SDK 裁剪后，我们需要更新 Docker 镜像，整个过程耗时较长且不够灵活。因此，我们将 Dart-SDK、Framework、Flutter_Web_SDK 按版本打包传至云端，在编译开始前读取 CI/CD 环境变量：sdk_version（SDK 版本号），远程拉取相应版本的 SDK 包，并替换当前 Docker 环境中的对应模块，基于以此方案实现 SDK 的灵活发布，具体流程图如下图所示：

![图10 集成CI/CD](https://p0.meituan.net/travelcube/02daa712453380dcdad7dd32b24af016101271.jpg)

图10 集成CI/CD



### 4.2 JS 分片

FlutterWeb 编译之后默认会生成 main.dart.js 文件，它囊括了 SDK 代码以及业务逻辑，这样会引起以下问题：

1. **功能无法及时更新**：为了实现浏览器的缓存优化，我们的项目开启了对静态资源的强缓存，若 main.dart.js 产物不支持 Hash 命名，可能导致程序代码不能被及时更新；
2. **无法使用 CDN**：FlutterWeb 默认仅支持相对域名的资源加载方式，无法使用当前域名以外的 CDN 域名，导致无法享受 CDN 带来的优势；
3. **首屏渲染性能不佳**：虽然我们进行了 SDK 瘦身，但 main.dart.js 文件依然维持在 0.7M 以上，单一文件加载、解析时间过长，势必会影响首屏的渲染时间。

针对文件 Hash 化和 CDN 加载的支持，我们在 flutter_tools 编译流程中对静态资源进行二次处理：遍历静态资源产物，增加文件 Hash（文件内容 MD5 值），并更新资源的引用；同时通过定制 Dart-SDK，修改了 main.dart.js、字体等静态资源的加载逻辑，使其支持 CDN 资源加载。

更详细的方案设计请参考[《Flutter Web在美团外卖的实践》](https://mp.weixin.qq.com/s/GjFC5_85pIk9EbKPJXZsXg)一文。下面我们重点介绍 main.dart.js 分片相关的一些优化策略。

#### 4.2.1 Lazy Loading

Flutter 官方提供 `deferred as` 关键字来实现 Widget 的懒加载，而 dart2js 在编译过程中可以将懒加载的 Widget 进行按需打包，这样的拆包机制叫做 Lazy Loading。借助 Lazy Loading，我们可以在路由表中使用 deferred 引入各个路由（页面），以此来达到业务代码拆离的目的，具体使用方法和效果如下所示：

```dart
// 使用方式
import 'pages/index/index.dart' deferred as IndexPageDefer;
{
  '/index': (context) => FutureBuilder(
    future: IndexPageDefer.loadLibrary(),
    builder: (context, snapshot) => IndexPageDefer.Demo(),
  )
  ... ...
}
```

![图11 效果演示](https://p1.meituan.net/travelcube/01482f3ac8d4efcd991fcc2227d1ca3b58393.png)

图11 效果演示



使用 Lazy Loading 后，业务页面的代码会被拆分到了多个 PartJS（对应图中 xxx.part.js 文件） 中。这样看似解决了业务代码与 SDK 耦合的问题，但在实际操作过程中，我们发现每次业务代码的变动，仍然会导致编译后的 main.dart.js 随之发生变化（文件 Hash 值变化）。经过定位与跟踪，我们发现这个变化的部分是 PartJS 的加载逻辑和映射关系，我们称之为 Runtime Manifest。因此，需要设计一套方案对 Runtime Manifest 进行抽离，来保证业务代码的修改对 main.dart.js 的影响达到最低。

#### 4.2.2 Runtime Manifest抽离

通过对业务代码的抽离，此时 main.dart.js 文件的构成主要包含 SDK 和 Runtime Manifest：

![图12 main.dart.js构成](https://p0.meituan.net/travelcube/26a65505405a5d92a5fd3b392689e19c45090.jpg)

图12 main.dart.js构成



那如何能将 Runtime Manifest 进行抽离呢？对比常规 Web 项目，我们的处理方式是把 SDK、Utils、三方包等基础依赖，利用 Webpack、Rollup 等打包工具进行抽离并赋予一个稳定的 Hash 值。同时，将 Runtime Manifest （分片文件的加载逻辑和映射关系）注入到 HTML 文件中，这样保证了业务代码的变动不会影响到公共包。借助常规 Web 项目的编译思路，我们深入分析了 FlutterWeb 中 Runtime Manifest 的生成逻辑和 PartJS 的加载逻辑，定制出如下的解决方案：

![图13 Runtime Manifest抽离](https://p0.meituan.net/travelcube/90aff428fd23428db2418f22fea51765140812.jpg)

图13 Runtime Manifest抽离



在上图中，Runtime Manifest 的生成逻辑位于 Dart2JS Compiler 模块，在该生成逻辑中，我们对 Runtime Manifest 代码块进行了标记，之后在 flutter_tools 中将标记的 Runtime Manifest 代码块抽离并写入 HTML 文件中（以 JS 常量形式存在）。而在 PartJS 的加载流程中，我们将 manifest 信息的读取方式改为了 JS 常量的获取。按照这样的拆分方式，业务代码的变更只会改变 Runtime Manifest 信息 ，而不会影响到 main.dart.js 公共包。

#### 4.2.3 main.dart.js 切片

经过以上引入 Lazy Loading、Runtime Manifest 抽离，main.dart.js 文件的体积稳定在 0.7M 左右，浏览器对大体积单文件的加载，会有很沉重的网络负担，所以我们设计了切片方案，充分地利用浏览器对多文件并行加载的特性，提升文件的加载效率。

具体实现方案为：将 main.dart.js 在 flutter_tools 编译过程拆分成多份纯文本文件，前端通过 XHR 的方式并行加载并按顺序拼接成 JavaScript 代码置于 < script > 标签中，从而实现切片文件的并行加载。

![图14 并行加载](https://p1.meituan.net/travelcube/44f8b27b16d30725788efbeeb2f21b5a38647.jpg)

图14 并行加载



### 4.3 预加载方案

如上一节所述，虽然我们做了很多工作来稳定 main.dart.js 的内容，但在 Flutter Tree-Shaking 的运行机制下，各个项目引用不同的 Framework Widget，就会导致每个项目生成的 main.dart.js 内容不一致。随着接入 FlutterWeb 的项目越来越多，每个业务的页面互访概率也越来越高，我们的期望是当访问 A 业务时，可以预先缓存 B 业务引用的 main.dart.js，这样当用户真正进入 B 业务时就可以节省加载资源的时间，下面为详细的技术方案。

#### 4.3.1 技术方案

我们把整体的技术方案分为编译、监听、运行三个阶段。

1. 编译阶段，在发布流水线上根据前期定制的匹配规则，筛选出符合条件的资源文件路径，生成云端 JSON 并上传；
2. 监听阶段，在 DOMContentLoaded 之后，对网络资源、事件、DOM 变动进行监听，并对监听结果根据特定规则进行分析加权，得到一个首屏加载完成的状态标识；
3. 运行阶段，在首屏加载完成之后对配置平台下发的云端 JSON 文件进行解析，对符合配置规则的资源进行 HTTP XHR 预加载，从而实现文件的预缓存功能。

下图为预缓存的整体方案设计：

![图15 预缓存方案设计](https://p0.meituan.net/travelcube/2c53abadd2e0fddc44074427dc63f89377786.jpg)

图15 预缓存方案设计



**编译阶段**

编译阶段会扩展现有的发布流水线，在 flutter build 之后增加 prefetch build 作业，这样 build 之后就可以对产物目录进行遍历和筛选，得到我们所需资源进而生成云端 JSON，为运行阶段提供数据基础。下面的流程图为编译阶段的详细方案设计：

![图16 预缓存编译阶段](https://p0.meituan.net/travelcube/4164f54b75562bf6ea3f9578dbf9843084051.jpg)

图16 预缓存编译阶段



编译阶段分为三部分：

1. 第一部分：根据不同的发布环境，初始化线上/线下的配置平台，为配置文件的读写做好准备；
2. 第二部分：下载并解析配置平台下发的资源组 JSON，筛选出符合配置规则的资源路径，更新 JSON 文件并发布到配置平台；
3. 第三部分：通过发布流水线提供的 API，把 PROJECT_ID、发布环境注入HTML文件中，为运行阶段提供全局变量以便读取。

通过对流水线编译期的整合，我们可以生成新的云端 JSON 并上传到云端，为运行阶段的下发提供数据基础。

**监听阶段**

我们知道，浏览器对文件请求的并发数量是有限制的，为了保证浏览器对当前页面的渲染处于高优先级，同时还能完成预缓存的功能，我们设计了一套对缓存文件的加载策略，在不影响当前页面加载的情况下，实现对缓存文件的加载操作。以下为详细的技术方案：

![图17 预缓存监听阶段](https://p0.meituan.net/travelcube/4e62f7df1e747de7baed6615f3e5aef3181257.jpg)

图17 预缓存监听阶段



在页面 DOMContentLoaded 之后，我们会监听三部分的的变化。

1. 第一部分是监听 DOM 的变化。这部分主要是在页面发生 Ajax 请求之后，随着MV模式的变动，DOM 也会随之发生变化。我们使用浏览器提供的 MutationObserver API 对 DOM 变化进行收集，并筛选有效节点进行深度优先遍历，计算每个 DOM 的递归权重值，低于阈值我们就认为首屏已加载完成。
2. 第二部分是监听资源的变化。我们利用浏览提供的 PerformanceObserver API，筛选出 img/script 类型的资源，在 3 秒内收集的资源没有增加时，我们认为首屏已加载完成。
3. 第三部分是监听 Event 事件。当用户发生 click、wheel、touchmove 等交互行为时，我们就认为当前页面处于一个可交互的状态，即首屏加载已完成，这样会在后续进行资源的预缓存。

通过上述步骤，我们就可以得到一个首屏渲染完成的时机，之后就可以实现预缓存功能了。以下为预缓存功能的实现。

**运行阶段**

预缓存的整体流程为：下载编译阶段生成的云端 JSON，解析出需要进行预缓存资源的 CDN 路径，最后通过 HTTP XHR 进行缓存资源进行请求，利用浏览器本身的缓存策略，把其他业务的资源文件写入。当用户访问已命中缓存的页面时，资源已被提前加载，这样可以有效地减少首屏的加载时间。下图为运行阶段的详细方案设计：

![图18 预缓存运行阶段](https://p0.meituan.net/travelcube/c5d529488ce3085a395d1dd6753d1317129511.jpg)

图18 预缓存运行阶段



在监听阶段，我们可以获取到页面的首屏渲染完成的时机，会获取到云端 JSON，首先判断该项目的缓存是否为启用状态。当该项目可用时，会根据全局变量 PROJECT_ID 进行资源数组的匹配，再以 HTTP XHR 方式进行预访问，把缓存文件写入浏览器缓存池中。至此，资源预缓存已执行完毕。

#### 4.3.2 效果展示与数据对比

当有页面间互访问命中预缓存时，浏览器会以 200（Disk Cache）的方式返回数据，这样就节省了大量资源加载的时间，下图为命中缓存后资源加载情况：

![图19 预缓存效果展示](https://p0.meituan.net/travelcube/8c8273203fdced9fff62583ddca1cc9066013.png)

图19 预缓存效果展示



目前，美团外卖商家端业务已有 10+ 个页面接入了预缓存功能，资源加载 90 线平均值由 400ms 下降到 350ms，降低了 12.5%；50 线平均值由 114ms 下降到 100ms，降低了 12%。随着项目接入接入越来越多，预缓存的效果也会越发的明显。

![图20 预缓存数据展示](https://p1.meituan.net/travelcube/7a6fd4860841b106a4b128410ac660a521693.jpg)

图20 预缓存数据展示



### 4.4 分平台打包

如前文所述，美团外卖商家业务大部分都是双端对齐的。为了实现提效的最大化，我们对 FlutterWeb 的多平台适配能力进行加强，实现了 FlutterWeb 在 PC 侧的复用。

在 PC 适配过程中，我们不可避免地需要书写双端的兼容代码，如：为了实现在列表页面中对卡片组件的复用。为此我们开发了一个适配工具 ResponsiveSystem，分别传入 PC 和 App 的各端实现，内部会区分平台完成适配：

```dart
// ResponsiveSystem 使用举例
Container(
  child: ResponsiveSystem(
    app: AppWidget(),
    pc: PCWidget(),
  ),
)
```

上述代码能较方便的实现 PC 和 App 适配，但 AppWidget 或 PCWidget 在编译过程中都将无法被 Tree-Shaking 去除，因此会影响包体积大小。对此，我们将编译流程进行优化，设计分平台打包方案：

![图21 分平台打包](https://p0.meituan.net/travelcube/2fb7fc9166be74f54d7e8b09d3fe0d66156311.jpg)

图21 分平台打包



1. 修改 flutter-cli，使其支持 –responsiveSystem 命令行参数；
2. 我们在 flutter_tools 中的 AST 分析阶段增加了额外的处理：ResponsiveSystem 关键字的匹配，同时结合编译平台（PC 或 Mobile）来进行 AST 节点的改写；
3. 去除无用 AST 节点后，生成各个平台的代码快照（每份快照仅包含单独平台代码）；
4. 根据代码快照编译生成 PC 和 App 两套 JS 产物，并进行资源隔离。而对于 images、fonts 等公用资源，我们将其打入 common 目录。

通过这样的方式，我们去除了各自平台的无用代码，避免了 PC 适配过程中引起的包体积问题。依然以美团外卖商家课堂业务（6 个页面）为例，接入分平台打包后，单平台代码体积减小 100KB 左右。

![图22 效果展示](https://p0.meituan.net/travelcube/710db8f7fd2aa75b4f35761a4492b2f325051.jpg)

图22 效果展示



### 4.5 图标字体精简

当访问 FlutterWeb 页面时，即使在业务代码中并未使用 Icon 图标，也会加载一个 920KB 的图标字体文件：MaterialIcons-Regular.woff。通过探究，我们发现是 Flutter Framework 中一些系统 UI 组件（如：CalendarDatePicker、PaginatedDataTable、PopupMenuButton 等）使用到了 Icon 图标导致，且 Flutter 为了便于开发者使用，提供了全量的 Icon 图标字体文件。

Flutter 官方提供的 `--tree-shake-icons` 命令选项是将业务使用到的 Icon 与 Flutter 内部维护的一个缩小版字体文件（大约 690KB）进行合并，能一定程度上减小字体文件大小。而我们需要的是只打包业务使用的 Icon，所以我们对官方 `tree-shake-icons` 进行了优化，设计了 Icon 的按需打包方案：

![图23 图标字体精简](https://p0.meituan.net/travelcube/51cb5d4bd817d4ca17e0d2b735015757113958.jpg)

图23 图标字体精简



1. 扫描全部业务代码以及依赖的 Plugins、Packages、Flutter Framework，分析出所有用到的 Icon；
2. 把扫描到的所有 Icon 与 material/icons.dart（该文件包含 Flutter Icon 的 unicode 编码集合）进行对比，得到精简后的图标编码列表：iconStrList；
3. 使用 FontTools 工具把 iconStrList 生成字体文件 .woff，此时的字体文件仅包含真正使用到的 Icon。

通过以上的方案，我们解决了字体文件过大带来的包体积问题，以美团外卖课堂业务（业务代码中使用了 5 个 Icon）为例，字体文件从 920KB 精简为 11.6kB。

![图24 效果展示](https://p1.meituan.net/travelcube/e6d006831983e43405eff68e45067f5d24759.jpg)

图24 效果展示



## 5.总结与展望

综上所述，我们基于 HTML Render 模式对 FlutterWeb 性能优化进行了探索和实践，主要包括 SDK（Dart-SDK、Framework、Flutter_Web_SDK）的精简，静态资源产物优化（例如：JS 分片、文件 Hash、字体图标文件精简、分平台打包等）和前端资源加载优化（预加载与按需请求）。**最终使得 JS 产物由 1.2M 减少至 0.7M（非业务代码），页面完全加载时间 TP90 线由 6s 降到了 3s**，这样的结果已能满足美团外卖商家端的大部分业务要求。而未来的规划将聚焦于以下3个方向：

1. **降低 Web 端适配成本**：目前已有 9+ 个业务借助 MTFlutterWeb 实现多端复用，但在 Web 侧（尤其是 PC 侧）的适配效率依然有优化空间，目标是将适配成本降低到 10% 以下（目前大约是 20% ）；
2. **构建 FlutterWeb 容灾体系**：Flutter 动态化包有一定的加载失败概率，而 FlutterWeb 作为兜底方案，能提升整体业务的加载成功率。此外 FlutterWeb 可以提供“免安装更新”的能力，降低 FlutterNative 老旧历史版本的维护成本；
3. **性能优化的持续推进**：性能优化的阶段性成果为 MTFlutterWeb 的应用推广巩固了基础，但依然是有进一步优化空间的，例如：目前我们仅将业务代码和 Runtime Manifest 进行了拆离，而 Framework 及 三方包在一定程度上也影响到了浏览器缓存的命中率，将这部分代码进行抽离，可进一步提升页面加载性能。

美团外卖技术团队正在基于 FlutterWeb 做更多的探索和尝试。如果你对这方面的技术也比较感兴趣，可以在文末留言，跟我们一起讨论。也欢迎大家给提出一些建议，非常感谢。

原文作者：美团技术团队

原文链接：https://tech.meituan.com/2021/12/16/flutterweb-practice-in-meituan-waimai.html

# 【NO.257】百亿规模API网关服务Shepherd的设计与实现

## 1.背景介绍

### 1.1 API网关是什么？

API网关是随着微服务（Microservice）概念兴起的一种架构模式。原本一个庞大的单体应用（All in one）业务系统被拆分成许多微服务（Microservice）系统进行独立的维护和部署，服务拆分带来的变化是API的规模成倍增长，API的管理难度也在日益增加，使用API网关发布和管理API逐渐成为一种趋势。一般来说，API网关是运行于外部请求与内部服务之间的一个流量入口，实现对外部请求的协议转换、鉴权、流控、参数校验、监控等通用功能。

### 1.2 为什么要做Shepherd API网关？

在没有Shepherd API网关之前，美团业务研发人员如果要将内部服务输出为对外的HTTP API接口。通常要搭建一个Web应用，用于完成基础的鉴权、限流、监控日志、参数校验、协议转换等工作，同时需要维护代码逻辑、基础组件的升级，研发效率相对比较低。此外，每个Web应用都需要维护机器、配置、数据库等，资源利用率也非常差。

美团内部一些业务线苦于没有现成的解决方案，根据自身业务特点，研发了业务相关的API网关。放眼业界，亚马逊、阿里巴巴、腾讯等公司也都有成熟的API网关解决方案。

因此，Shepherd API网关项目正式立项，我们的目标是为美团提供高性能、高可用、可扩展的统一API网关解决方案，让业务研发人员通过配置的方式即可对外开放功能和数据。

![image-20230109204415439](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204415439.png)



### 1.3 使用Shepherd带来的收益是什么？

从业务研发人员的角度来看，接入Shepherd API网关，能带来哪些收益呢？简而言之包括三个方面。

- 提升研发效率
  - 业务研发人员只需要通过配置的方式即可快速开放服务接口。
  - Shepherd统一提供鉴权、限流、熔断等非业务基础能力。
  - Shepherd支持业务研发人员通过开发自定义组件的方式扩展API网关能力。
- 降低沟通成本
  - 业务研发人员配置好API，可以自动生成API的前后端交互文档和客户端SDK，方便前后端开发人员进行交互、联调。
- 提升资源利用率
  - 基于Serverless的架构思想，实现API全托管，业务研发人员无需关心机器资源问题。

## 2.技术设计与实现

### 2.1 整体架构

我们先来看看Shepherd API网关的整体架构，如下图所示：

![image-20230109204433714](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204433714.png)



Shepherd API网关的**控制面**由Shepherd管理平台和Shepherd监控中心组成。管理平台主要完成API的全生命周期管理以及配置下发的工作，监控中心完成API请求监控数据的收集和业务告警功能。

Shepherd API网关的**配置中心**主要完成控制面与数据面的信息交互，通过美团统一配置服务Lion来实现。

Shepherd API网关的**数据面**也就是Shepherd 服务端。一次完整的API请求，可能是从移动应用、Web应用，合作伙伴或内部系统发起，经过Nginx负载均衡系统后，到达服务端。服务端集成了一系列的基础功能组件和业务自定义组件，通过泛化调用请求后端RPC服务、HTTP服务、函数服务或服务编排服务，最后返回响应结果。

下面我们将针对这三个主要模块做详细的介绍。

#### 2.1.1 控制面

使用API网关的控制面，业务研发人员可以轻松的完成API的全生命周期管理，如下图所示：

![image-20230109204450588](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204450588.png)



业务研发人员从创建API开始，完成参数录入、DSL脚本生成；接着可以通过文档和MOCK功能进行API测试；API测试完成后，为了保证上线稳定性，Shepherd管理平台提供了发布审批、灰度上线、版本回滚等一系列安全保证措施；API运行期间会监控API的调用失败情况、记录请求日志，一旦发现异常及时发出告警；最后，对于不再使用的API进行下线操作后，会回收API所占用的各类资源并等待重新启用。

整个生命周期，全部通过配置化、流程化的方式，由业务研发人员全自助管理，上手时间基本在10分钟以内，极大地提升了研发效率。

#### 2.1.2 配置中心

API网关的配置中心存放API的相关配置信息——使用自定义的DSL（Domain-Specific Language，领域专用语言）来描述，用于向API网关的数据面下发API的路由、规则、组件等配置变更。

配置中心的设计上使用统一配置管理服务Lion和本地缓存结合的方式，实现动态配置，不停机发布。API的配置如下图所示：

![image-20230109204505227](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204505227.png)



API配置的详细说明：

- **Name、Group**：名字、所属分组。
- **Request**：请求的域名、路径、参数等信息。
- **Response**：响应的结果组装、异常处理、Header、Cookies信息。
- **Filters、FilterConfigs**：API使用到的功能组件和配置信息。
- **Invokers**：后端服务(RPC/HTTP/Function)的请求规则和编排信息。

#### 2.1.3 数据面

**API路由**

API网关的数据面在感知到API配置后，会在内存中建立请求路径与API配置的路由信息。通常HTTP请求路径上，会包含一些路径变量，考虑到性能问题，Shepherd没有采用正则匹配的方式，而是设计了两种数据结构来存储。如下图所示：

![image-20230109204535031](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204535031.png)



一种是不包含路径变量的直接映射的MAP结构。其中，Key就是完整的域名和路径信息，Value是具体的API配置。

另外一种是包含路径变量的前缀树数据结构。通过前缀匹配的方式，先进行叶子节点精确查找，并将查找节点入栈处理，如果匹配不上，则将栈顶节点出栈，再将同级的变量节点入栈，如果仍然找不到，则继续回溯，直到找到（或没找到）路径节点并退出。

**功能组件**

当请求流量命中API请求路径进入服务端，具体处理逻辑由DSL中配置的一系列功能组件完成。网关提供了丰富的功能组件集成，包括链路追踪、实时监控、访问日志、参数校验、鉴权、限流、熔断降级、灰度分流等，如下图所示：

![image-20230109204546143](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204546143.png)



**协议转换&服务调用**

API调用的最后一步，就是协议转换以及服务调用了。网关需要完成的工作包括：获取HTTP请求参数、Context本地参数，拼装后端服务参数，完成HTTP协议到后端服务的协议转换，调用后端服务获取响应结果并转换为HTTP响应结果。

![image-20230109204556934](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204556934.png)

上图以调用后端RPC服务为例，通过JsonPath表达式获取HTTP请求不同部位的参数值，替换RPC请求参数相应部位的Value，生成服务参数DSL，最后借助RPC泛化调用完成本次服务调用。

### 2.2 高可用设计

Shepherd API网关作为接入层的基础组件，高可用性一直是业务研发人员非常关心的部分。接下来。我们就来探索一下Shepherd在高可用设计方面的实践。

#### 2.2.1 排除性能隐患

一个高可用的系统，预防故障的发生，首先要排除性能隐患，保证高性能。

Shepherd对API请求做了全异步化处理，请求通过Jetty IO线程异步提交到业务处理线程池，调用后端服务使用RPC或HTTP框架的异步方式，释放了由于网络等待引起的线程占用，使线程数不再成为网关的瓶颈。下图是使用Jetty容器时，服务端的请求线程处理逻辑：

![image-20230109204607586](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204607586.png)



我们通过域名压测网关单机的端到端QPS，发现QPS在超过2000时，会出现很多超时错误，而网关的服务端负载与性能却非常富余。调研发现，公司内其他的Web应用都存在这个问题，与Oceanus团队进行联合排查后，发现是Nginx与Web应用之间的长连接功能没有打开，且无法配置。Oceanus团队经过紧急排期，研发并上线长连接功能后，Shepherd端到端的QPS成功提升到了10000以上。

另外，我们对Shepherd服务端进行了API请求预热的优化，使得网关启动时能立刻达到最佳性能，减少毛刺的发生。然后，通过压测时的CPU热点排查，将性能瓶颈找出，减少主链路上的本地日志打印，对请求日志进行异步化、远程化改造。Shepherd端到端的QPS再次提升30%以上。

在Shepherd服务上线稳定运行一年以后，我们再次对性能进行优化，并且做了一次网络框架升级，将Jetty容器全面替换为Netty网络框架，性能提升10%以上，Shepherd端到端的QPS成功提升到15000以上。下图是使用Netty框架时，服务端的请求线程处理逻辑：

![image-20230109204621654](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204621654.png)



#### 2.2.2 服务隔离

**集群隔离**

借鉴公司缓存、任务调度等成熟组件的经验，Shepherd在设计之初，就考虑了按业务线维度进行集群隔离，也支持重要业务独立部署。如下图所示：

![image-20230109204631556](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204631556.png)



**请求隔离**

服务节点维度，Shepherd支持请求的快慢线程池隔离。快慢线程池隔离主要用于一些使用了同步阻塞组件的API，例如SSO鉴权、自定义鉴权等，可能导致长时间阻塞共享业务线程池。

快慢隔离的原理是统计API请求的处理时间，将请求处理耗时较长，超过容忍阈值的API请求隔离到慢线程池，避免影响其他正常API的调用。

除此之外，Shepherd也支持业务研发人员配置自定义线程池进行隔离。具体的线程隔离模型如下图所示：

![image-20230109204643985](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204643985.png)



#### 2.2.3 稳定性保障

Shepherd提供了一些常规的稳定性保障手段，来保证自身和后端服务的可用性。如下图所示：

![image-20230109204657646](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204657646.png)

- **流量管控**：从用户自定义UUID限流、App限流、IP限流、集群限流等多个维度提供流量保护。
- **请求缓存**：对于一些幂等的、查询频繁的、数据及时性不敏感的请求，业务研发人员可开启请求缓存功能。
- **超时管理**：每个API都设置了处理超时时间，对于超时的请求，进行快速失败的处理，避免资源占用。
- **熔断降级**：支持熔断降级功能，实时监控请求的统计信息，达到配置的失败阈值后，自动熔断，返回默认值。

#### 2.2.4 请求安全

请求安全是API网关非常重要的能力，Shepherd集成了丰富的安全相关的系统组件，包括有基础的请求签名、SSO单点登录、基于SSO鉴权的UAC/UPM访问控制、用户鉴权Passport、商家鉴权EPassport、商家权益鉴权、反爬等等。业务研发人员只需要简单配置，即可使用。

#### 2.2.5 可灰度

API网关作为请求入口，往往肩负着请求流量灰度验证的重任。

**灰度场景**

Shepherd在灰度能力上，支持灰度API自身逻辑，也支持灰度下游服务，也可以同时灰度API自身逻辑和下游服务。如下图所示：

![image-20230109204716560](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204716560.png)



灰度API自身逻辑时，通过将流量分流到不同的API版本实现灰度能力；灰度下游服务时，通过给流量打标，分流到指定的下游灰度单元中。

**灰度策略**

Shepherd支持丰富的灰度策略，可以按照比例数灰度，也可以按照特定条件灰度。

#### 2.2.6 监控告警

**立体化监控**

Shepherd提供360度的立体化监控，从业务指标、机器指标、JVM指标提供7x24小时的专业守护，如下表：

|      | 监控模块        | 主要功能                                                     |
| :--- | :-------------- | :----------------------------------------------------------- |
| 1    | 统一监控Raptor  | 实时上报请求调用信息、系统指标，负责应用层（JVM）监控、系统层（CPU、IO、网络）监控 |
| 2    | 链路追踪Mtrace  | 负责全链路参数透传、全链路追踪监控                           |
| 3    | 日志监控Logscan | 监控本地日志异常关键字：如5xx状态码、空指针异常等            |
| 4    | 远程日志中心    | API请求日志、Debug日志、组件日志等可上报远程日志中心         |
| 5    | 健康检查Scanner | 对网关节点进行心跳检测和API状态检测，及时发现异常节点和异常API |

**多维度告警**

有了全面的监控体系，自然少不了配套的告警机制，主要的告警能力包括：

|      | 告警类型         | 触发时机                                            |
| :--- | :--------------- | :-------------------------------------------------- |
| 1    | 限流告警         | API请求达到限流规则阈值触发限流告警                 |
| 2    | 请求失败告警     | 鉴权失败、请求超时、后端服务异常等触发请求失败告警  |
| 3    | 组件异常告警     | 自定义组件处理耗时长、失败率高告警                  |
| 4    | API异常告警      | API发布失败、API检查异常时触发API异常告警           |
| 5    | 健康检查失败告警 | API心跳检查失败、网关节点不通时触发健康检查失败告警 |

#### 2.2.7 故障自愈

Shepherd服务端接入了弹性伸缩模块，可根据CPU等指标进行快速扩容、缩容。除此之外，还支持快速摘除问题节点，以及更细粒度的问题组件摘除。

![image-20230109204733320](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204733320.png)



#### 2.2.8 可迁移

对于一些已经在对外提供API的Web服务，业务研发人员为了减少运维成本和后续的研发提效，考虑将其迁移到Shepherd API网关。

对于一些非核心API，可以考虑使用Oceanus的灰度发布功能直接迁移。但是对于一些核心API，上面的灰度发布功能是机器级别的，粒度较大，不够灵活，不能很好的支持灰度验证过程。

**解决方案**

Shepherd为业务研发人员提供了一个灰度SDK，接入SDK的Web服务，可在识别灰度流量后转发到Shepherd API网关进行验证。

灰度哪些API、灰度百分比可以在Shepherd管理端动态调节，实时生效，业务研发人员还可以通过SPI的方式自定义灰度策略。灰度验证通过后，再把API迁移到Shepherd API网关，保障迁移过程的稳定性。

**灰度过程**

**灰度前**：在Shepherd管理平台创建API分组，域名配置为目前使用的域名。在Oceanus上，原域名规则不变。

![image-20230109204744565](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204744565.png)



**灰度中**：在Shepherd管理平台开启灰度功能，灰度SDK将灰度流量转发到网关服务，进行验证。

![image-20230109204754366](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204754366.png)



**灰度后**：通过灰度流量验证Shepherd上的API配置符合预期后再迁移。

![image-20230109204807209](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204807209.png)



### 2.3 易用性设计

Shepherd API网关的功能强大且复杂，易用性对业务研发人员来说至关重要，我们着重来看下自动生成DSL、API操作提效的解决方案。

#### 2.3.1 自动生成DSL

业务研发人员在实际使用网关管理平台时，我们尽量通过图形化的页面配置来减轻DSL的编写负担。但服务参数转换的DSL配置，仍然需要业务研发人员手工编写。一般来说，生成服务参数DSL的流程是：

1. 引入服务的接口包依赖。
2. 拿到服务参数类定义。
3. 编写Testcase生成JSON模板。
4. 填写参数映射规则。
5. 最后手工录入管理平台，发布API。

整个过程非常繁琐，且容易出错。如果需要录入的API多达几十上百个，全部由业务研发人员手工录入的效率是非常低下的。

**解决方案**

那么能不能将服务参数DSL的生成过程给自动化呢？ 答案是可以的，业务RD只需在网关录入API文档信息，然后录入服务的Appkey、服务名、方法名信息，Shepherd管理端会从最新发布的服务框架控制台获取到服务参数的JSON Schema信息，JSON Schema定义了服务参数的类型和结构信息，管理端可根据这些信息，自动生成服务参数的JSON Mock数据。结合API文档的信息，自动替换参数名相同的Value值。 这套DSL自动生成方案，使用过程中对业务透明、标准化，业务方只需升级最新版本服务框架即可使用，极大提升研发效率，目前受到业务研发人员的广泛好评。

![image-20230109204820717](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204820717.png)



#### 2.3.2 API操作提效

**快速创建API**

API网关的核心能力是建立在API配置的基础上的，但提供强大功能的同时带来了较高的复杂性，不少业务研发人员吐槽API配置太繁琐，学习成本高。快速创建API的功能应运而生，业务研发人员只需要提供少量的信息就可以创建API。快速创建API的功能当前分为4种类型（后端RPC服务API、后端HTTP服务API、SSO CallBack API、Nest API），未来会根据业务应用场景的不同，提供更多的快速创建API类型。

**批量操作**

业务研发人员在API网关上，需要管理非常多的业务分组，每个业务分组，最多可以有200个API配置，多个API可能有很多相同的配置，如组件配置，错误码配置和跨域配置的。每个API对于相同的配置都要配置一遍，操作重复度很高。因此Shepherd支持批量操作多个API：勾选多个API后，通过【批量操作】功能可一次性完成多个API配置更新，降低业务重复配置的操作成本。

**API导入导出**

Shepherd提供在不同研发环境相互导入导出API的能力，业务研发人员在线下测试完成后，只需要使用API导入导出功能，即可将配置导出到线上生产环境，避免重复配置。

### 2.4 可扩展性设计

一个设计良好的基础组件，除了能提供强大的基础能力，还需要有良好的扩展能力。 Shepherd的可扩展性主要体现在：支持自定义组件、服务编排的能力。

#### 2.4.1 自定义组件

Shepherd提供了丰富的系统组件完成鉴权、限流、监控能力，能够满足大部分的业务需求。但仍有一些特殊的业务需求，如自定义验签、自定义结果处理等。Shepherd通过提供加载自定义组件能力，支持业务完成一些自定义逻辑的扩展。

下图是自定义组件实现的一个实例。getName中填写自定义组件申请时的名称，invoke方法中实现自定义组件的业务逻辑，如继续执行、进行页面跳转、直接返回结果、抛出异常等。

![image-20230109204838868](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204838868.png)



目前Shepherd通过自定义组件已经成功支持了美团优选、外卖、餐饮、打车等重要业务，接入的自定义组件数量有200多个。

#### 2.4.2 服务编排

一般情况下，网关上配置的一个API对应后端一个RPC或者HTTP服务。如果调用端有聚合和编排后端服务的需求，那么有多少后端服务，就必须发起多少次HTTP的请求调用。由此就会带来一些问题，调用端的HTTP请求次数过多，效率低，在调用端聚合服务的逻辑过重。

服务编排的需求应运而生，服务编排是对既有服务进行编排调用，同时对获取的数据进行处理。主要应用在数据聚合场景：一次HTTP请求返回的数据需要调用多个或多次服务（RPC或HTTP）才能获取到完整的结果。

经过前期调研，公司已经有一套成熟的服务编排框架，由客服团队开发的海盗组件（参见[《海盗中间件：美团服务体验平台对接业务数据的最佳实践》](https://tech.meituan.com/2018/07/26/sep-service-arrange.html)一文），也是美团内部的公共服务。

因此我们与海盗团队合作，设计了Shepherd的服务编排支持方案。海盗通过独立部署的方式提供服务编排能力，Shepherd与海盗之间通过RPC进行调用。这样可以解耦Shepherd与海盗，避免因服务编排能力影响集群上的其他服务，同时多一次RPC调用并不会有明显耗时增加。使用上对业务研发人员也是透明的，非常方便，业务研发人员在管理端配置好服务编排的API，通过配置中心同时下发到Shepherd服务端和海盗服务上，即可开始使用服务编排能力。整体的交互架构图如下：

![image-20230109204851489](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204851489.png)



## 3.未来规划

目前接入Shepherd API网关的API数量超过18000多个，线上运行的集群数量90多个，日均总调用次数在百亿以上。随着Shepherd API网关业务规模的不断增长，对我们的可用性、易用性、可扩展性必将提出更高的要求。未来一年，Shepherd的规划重点包括有云原生架构演进、静态网站托管、组件市场等。

### 3.1 云原生架构演进

Shepherd API网关的云原生架构演进有三个目标：简化接入网关步骤，提升业务研发人员的研发效率；减小服务端War包大小，提升安全性和稳定性；接入Serverless弹性，降低成本，提高资源利用率。

为了实现这个三个目标，我们计划整体迁移网关服务到公司的Serverless服务Nest（参见[《美团Serverless平台Nest的探索与实践》](https://tech.meituan.com/2021/04/21/nest-serverless.html)一文）上，同时通过抽取Shepherd核心功能到SDK的方式集成到业务的网关集群，业务研发人员可以只选择自己需要使用的自定义组件，从而大幅减小服务端的War包大小。

![image-20230109205319017](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109205319017.png)

### 3.2 静态网站托管

依托Shepherd API网关实现静态网站托管的目标是：**建设通用的静态网站托管解决方案，为开发者提供便捷、稳定、高扩展性的静态网站托管服务**。

静态网站托管解决方案能为业务研发人员提供的主要功能包括：托管静态网站资源，包括存储及访问；管理应用生命周期，包括自定义域配置以及身份验证和授权；CI/CD集成等。

![image-20230109204927583](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20230109204927583.png)



### 3.3 组件市场

Shepherd API网关组件市场的目标是：**合作共赢，形成开发生态，业务研发人员可将开发的自定义组件提供给其他有需要的业务研发团队使用**。

我们希望让业务研发人员参与到自定义组件的开发，完善使用文档后设置为公共组件，开放给所有使用Shepherd的业务研发人员使用，避免重复造轮子。

## 4.作者简介

充泽、志洋、李敏等，均来自美团基础技术部-基础架构团队。

## 5.招聘信息

美团基础技术部-基础架构团队诚招高级、资深技术专家，Base北京、上海。我们致力于建设美团全公司统一的高并发高性能分布式基础架构平台，涵盖数据库、分布式监控、服务治理、高性能通信、消息中间件、基础存储、容器化、集群调度等基础架构主要的技术领域。欢迎有兴趣的同学投送简历至：edp.itu.zhaopin@meituan.com。

原文作者：美团技术团队

原文链接：https://tech.meituan.com/2021/05/20/shepherd-api-gateway.html

# 【NO.258】多业务建模在美团搜索排序中的实践

## 1.引言

美团的使命是“帮大家吃得更好，生活更好”，美团 App 承载的业务包括外卖、到店餐饮、买菜、优选、酒店、旅游、休闲娱乐等各类生活服务。美团首页的搜索是美团 App 上各类生活服务最大的流量分发入口，每天为数千万的用户提供各种服务。美团搜索排序是一个典型的多业务混合排序建模问题，一个典型的多业务搜索场景是当用户搜索地点，如 “望京” 的时候，用户的需求不是很明确，此时搜索的结果页如下图 1 所示，下方的商家列表中会包含望京附近餐饮、电影、休闲娱乐、酒店等多种业务的结果，这就是一个多业务混合排序场景。

![图1 美团搜索结果页面](https://p0.meituan.net/travelcube/0396bf7d5456b9acb2b73afb9dc935da193147.jpg)

图1 美团搜索结果页面



而多业务场景存在如下几点挑战：

1. 因业务之间存在共性和特性，如何让模型兼顾这两种特性，实现更好的数据学习。比如到店餐饮对距离特征非常敏感，而旅游景点业务对距离特征相对不敏感。
2. 业务天然存在高频和低频特性（比如外卖和旅游），导致模型的训练数据中多业务样本数量不平衡。
3. 各个业务往往有自己不同的主目标，如何满足不同业务的目标，最终能够提升搜索的用户体验。

本文分享了美团搜索中的多业务排序建模优化工作，我们主要聚焦在到店商家多业务场景，后续的内容会分为以下四个部分：第一部分是对美团搜索排序分层架构进行简单介绍；第二部分会介绍多路融合层上的多业务融合建模；第三部分会介绍精排模型的多业务排序建模；最后一部分是总结和展望。希望能对从事相关工作的同学有所启发或者帮助。

## 2.排序流程简介

美团搜索系统流程如下图 2 所示，整体流程分为数据层、召回层、排序层和展示层。其中排序层分为以下几个子部分：

1. **粗排层**：使用相对简单的模型对召回候选集进行初步过滤，实现排序效果和性能的 Trade-off。
2. **多路融合层**：使用查询词特征、上下文场景特征构建配额模型，进行不同业务候选集的数量控制，实现用户需求的精确理解。
3. **精排层**：使用亿级别特征的深度学习模型，捕捉各种显式和隐式信号，实现 Item 排序分数的精准预估。
4. **重排层**：使用小模型和各种机制对精排后的结果进行调序，实现精细定向的优化。
5. **异构排序层**：使用深度学习模型对异构聚块进行排序，实现多业务的高承载。

多层排序架构设计主要是为了平衡排序效果和性能。本文后续提到的多业务建模优化工作主要从多路融合层和精排层进行介绍。

![图2 系统架构图](https://p0.meituan.net/travelcube/af2c07ea8cdb97f226d70528c99beabd284642.png)

图2 系统架构图



## 3.多业务建模实践

### 3.1 多业务配额模型（多路融合层）

随着美团业务的发展，美团搜索接入了到餐、到综、酒店、旅游等业务。对于业务意图模糊的搜索词，比如用户搜索“五道口”，需要根据用户、查询词、场景等多种因素来综合判断用户的业务意图。为了融合不同业务的召回结果，给 L2 精排一个比例合适的候选集，我们设计了一个多业务配额模型来平衡多业务召回的比例。这种基于配额对多路召回结果进行合并的做法在搜索、推荐场景中十分常用，比如淘宝首页搜索、美团推荐等。

为了多路召回的灵活接入，适配美团搜索业务的发展，我们不断迭代搜索配额模型。下面将详细介绍美团多业务配额模型的迭代过程，文章后续部分会将多业务配额模型（Multi-Business Quota Model）简称为MQM。

![图3 多路融合过程](https://p0.meituan.net/travelcube/1057cd04b138e77cfe9db5f4cd9f51b3171288.png)

图3 多路融合过程



#### 3.1.1 一维目标多业务配额

考虑到大搜结果存在多路不同业务的召回，为了刻画用户搜索 Query 对三路业务召回的意图强弱，我们采用多目标的建模方式，以每一路召回是否被点击、下单为目标进行建模，实现了多业务配额初版模型 MQM-V1。该模型输出各路召回的点击、下单联合概率，作为最终的配额分布。在特征层面，我们使用 Query 维度特征、Context 维度特征、Cross 维度特征、User 维度特征，来刻画用户在不同场景的实时个性化需求。MQM-V1 模型结构如下图 4 所示。

![图4 MQM-V1 模型结构图](https://p0.meituan.net/travelcube/c43b5fe81904440c10b3493d0575a608209034.png)

图4 MQM-V1 模型结构图



MQM-V1 版本上线后，整体线上点击率 +0.53%，各业务访购率基本持平。

#### 3.1.2 二维目标多业务配额

随着大搜召回策略的不断迭代，大搜不仅引入了按业务拆分的召回方式，而且引入了向量检索、地理位置近邻检索等跨多业务的异构召回方式，导致大搜召回策略不断增加，多业务配额模型也面临新召回源带来的冷启动问题。同时，为了加强多业务配额模型的个性化，我们参考借鉴了[6]中用户行为序列建模的方法。综上，该版本 MQM-V2 与 MQM-V1 区别如下：

- 建模目标上，从以召回方式点击的一维目标升级到召回方式叉乘业务的二维目标，使得多路融合的粒度也更细、精度更高。
- 行为序列建模模块引入Transformer Layer。
- 为了解决新召回源接入的冷启动问题，我们引入了人工经验层，包括业务先验和历史统计，综合模型输出决定每一路召回的配额。

![图5 MQM-V2 模型结构图](https://p0.meituan.net/travelcube/ca914b175af3b123dfbf9edf64f20d96290839.png)

图5 MQM-V2 模型结构图



MQM-V2 版本上线后，各业务指标率均有提升，其中旅游访购率 +2%，到餐访购率 +0.57%，到综、酒店访购率持平。

### 3.2 多业务排序模型（精排层）

从美团搜索精排模型升级为 DNN 模型，一直到 2019 年底，美团搜索的精排模型结构是业界主流的 Embedding&MLP 的范式结构，期间我们也尝试过业界提出的模型结构比如PNN[1]、DeepFM[2]、DCN[3]、AutoInt[4]、FiBiNet[5]等等。

随着迭代的进行，我们发现针对特定业务的优化难以在精排模型发挥作用，为了兼顾各个业务的特性，支持各个业务更加有效的针对性迭代优化，需要探索出一种模型结构来适配美团搜索这样的多业务场景。下面会具体介绍精排模型在多业务建模上的发展史，文章后续部分将多业务精排模型（Multi-Business Network）简称为 MBN。

#### 3.2.1 独立子网络拆分

考虑到酒店和旅游在美团大搜排序策略的流量里面占比较少，而针对小流量的相关优化在目前统一的 Embedding&MLP 模型结构里面很难体现，我们尝试了如图 6 所示的人工自定义多塔模型 MBN-V1 结构：主网络复用目前的模型结构。具体情况介绍参考[6]中的行为序列建模部分，增加酒店和旅游独立子网络；酒店子网络的输入包括酒店独有特征和主网络的打分输出，旅游子网络的输入包括旅游独有特征、主网络的打分输出、主网络最后一层 FC，酒店和旅游子塔输入不同是因为业务逻辑不同导致数据分布差异大，这是实践出的结果，最终的输出是对三个输出的加权求和。

![图6 MBN-V1 模型结构图](https://p0.meituan.net/travelcube/d70c6c71e56f50eaa05783fe3865b2e5255298.png)

图6 MBN-V1 模型结构图



针对加权求和的权重部分，我们采取了两种方式对权重进行设定：

- 第一种，采用硬切分的方式，就是说权重向量是一个 One-hot 稀疏向量：对酒店商家进行预测，只选取酒店子网络的输出，其余类推。
- 第二种，采用软切分的方式，把多业务配额模型的输出作为权重值。

线上实验发现第二种方法比第一种好，我们认为采用硬切分会导致子塔分支的参数只能被对应业务的数据更新，而各业务的数据占比不均导致学习不佳，而软切分会达到一种知识迁移的作用。最终线上的效果相比统一的 Embedding& MLP 模型，整体旅游取得了正向效果：其中整体点击率 +0.17%，其余业务访购率效果基本持平。

#### 3.2.2 子网络权重自学习

基于第一版多业务精排模型取得了初步正向效果，我们继续添加美食业务子塔，同时考虑到 MBN-V1 依赖配额模型的输出，这样会导致配额模型的变更可能会对精排模型的效果产生影响，针对这些因素我们上线了第二版多业务模型 MBN-V2，模型结构如图7所示。相比 MBN-V1 的区别如下：

- 添加美食业务的独立子网络。
- 解耦精排模型和配额模型，在精排模型中集成权重生成子网络，该子网络的输入主要是一些 Query 维度，Context 维度的特征。

![图7 MBN-V2 模型结构图](https://p0.meituan.net/travelcube/33c293b5552faf8c2d903e0b4b0fc770329080.png)

图7 MBN-V2 模型结构图



线上实验效果：MBN-V2 相比 MBN-V1，整体点击率 +0.1%，业务访购率效果基本持平。

#### 3.2.3 子网络特征自适应

在第二版模型的基础上，我们进一步添加到综业务子塔，随着子网络的变多，目前对于子网络的输入采用人工设计，这种方式需要花费大量的时间进行离线实验。考虑到目前的多业务子塔结构十分类似业界的多任务学习，我们也尝试引入业界的多任务学习结构；同时，我们针对 MBN-V2 中的权重子网络输出进行分析发现其输出的权重对不同业务商户的输出差不多，那么会带来业务的针对性优化不明显。基于上述部分，我们迭代了第三版多业务精排 MBN-V3，结构如下图 8 所示，改进点如下：

![图8 MBN-V3 模型结构图](https://p0.meituan.net/travelcube/64fd623c9871c90aaa006532fcf89717471041.png)

图8 MBN-V3 模型结构图



- 补充到综子网络，采用 MMoE[7]多任务学习结构，来自动学习特征表征输出给上层子网络，从而取代人工设计子网络的输入。
- 精排模型的损失函数除了采用用户线上反馈计算的主 LambdaLoss 外，额外添加了业务的分类交叉熵 Loss，达到预测某业务 Item 得分时，对应的业务子塔权重最大的目的。

线上实验效果：MBN-V3 相比 MBN-V2，整体点击率效果持平、美食业务访购率 +0.36%，到综业务访购率 +1.07%，酒店业务访购率 +0.27%，旅游业务访购率 +0.35%。

#### 3.2.4 多业务特征表达优化

虽然 MMoE 多任务学习结构在业界很多场景得到了应用，在我们的多业务建模场景也取得了有效验证，但是我们持续跟进业界前沿，并且结合业务场景进行落地。

我们尝试了腾讯提出的 PLE[8]结构，迭代出多业务精排 MBN-V4。 PLE 可以看成是 MMoE 的改进版，它对于每一个任务有自己特定的专家层，不同任务之间有共享的专家层，相比 MMoE 是所有专家输出的加权求和，PLE 子任务的输入是子任务独有的专家和共享专家输出的加权求和，更容易学出业务的特性；同时基于性能考虑，我们选取了单层 PLE 也即 CGC 结构，结构如下图9所示：

![图9 MBN-V4 模型结构图](https://p1.meituan.net/travelcube/22f453e8b1da32cf455d9a890a746efc479185.png)

图9 MBN-V4 模型结构图



线上实验效果：MBN-V4 相比 MBN-V3，整体点击率 +0.1%，美食业务访购率 +0.53%，其余业务访购率波动持平；我们对 MMoE 和 CGC 的专家权重进行可视化如下图 10所示，分析发现：CGC 结构的专家层权重相比 MMoE 在同一业务多条样本之间的专家权重方差更小，更加稳定，说明 CGC 相比 MMoE 在特征表示上更加有优势。

![图10 MMoE、CGC 专家权重分析](https://p0.meituan.net/travelcube/4936e5e577d753ac5c20b65c4c3f490659369.png)

图10 MMoE、CGC 专家权重分析



## 4.总结和展望

2019 年底开始，为了解决实际的多业务召回排序问题，美团搜索进行了大量探索，从工程到算法到产品形态各个层面丰富对多业务的支持。其中排序算法层面主要在多路召回融合层和精排层进行优化。

多路融合层主要完成搜索结果从结果相关到结果优质的筛选过程，需要解决不同召回方式（文本召回、推荐召回、向量召回）和不同业务召回结果的融合截断问题，直接决定了用户能浏览到的结果候选集。这其中最重要的问题就是判断用户对各个业务的需求强弱以及各业务的召回质量，对每个业务结果和召回结果确定合适的精排准入标准。

多业务配额模型通过整合用户实时需求、Query 的历史统计信息、搜索上下文信息和每个召回源质量情况，给出了每路召回、每个业务的应进入精排的比例。该模型保证了不同场景下精排候选集的结果多样性和优质性，实现了新业务、新召回方法接入的少侵入性，降低业务、召回接入成本。同时也为精排层分业务的网络结构提供了融合各个业务子网络结果的先验权重。

精排层在多路融合层的基础上进一步对多业务搜索结果进行精细化排序建模打分。用户的需求和美团业务一样是多样的，为了能充分建模各种场景下的需求，精排多业务排序模型从底层数据（丰富分业务的特征）、模型结构到业务目标融合都进行了多轮迭代。其中模型结构和相应的目标融合直接对各种大小业务、场景和相应的业务目标进行了分片建模，有效地缓解了小业务小场景在统一建模中被大业务样本淹没的问题。同时该模型支持新老业务的快速迭代，各业务可以方便独立地迭代特征、模型结构和相应的目标。

上述优化覆盖了线上全流量，在搜索用户体验和各业务价值均有明显提升，但还有很多工作可以持续优化。

- **业务独有特征利用**：目前我们采用对某些业务添加业务独有特征，其余业务对于这些缺失的独有特征给默认值，但是这样会带来很多冗余的计算量，这部分不管从效果还是性能方面都存在优化空间。
- **样本不平衡学习**：不同业务的数据量在美团搜索上差异大，如何让模型更好的学习出小业务的分布，我们正在探索迁移学习和 Meta-Learning 等方法。
- **多目标优化**：美团搜索既要兼顾用户的搜索体验，也要服务美团各个业务的战略目标，所以各个业务的主优化指标不一定一致，多目标优化也是一个持续探索的方向。

本文叙述的工作集中在美团多业务商家搜索排序上，同时随着优选、买菜、团好货、闪购等商品类业务的发展，我们也正在进行商品类多业务混排以及商家商品异构多业务混排工作。

## 5.参考资料

- [1] [Product-based neural networks for user response prediction](https://arxiv.org/pdf/1611.00144.pdf)
- [2] [DeepFM: A Factorization-Machine based Neural Network for CTR Prediction](http://www.ijcai.org/proceedings/2017/0239.pdf)
- [3] [Deep & Cross Network for Ad Click Predictions](https://arxiv.org/abs/1708.05123)
- [4] [AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks](https://arxiv.org/abs/1810.11921)
- [5] [FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction](https://arxiv.org/pdf/1905.09433.pdf)
- [6] [Transformer 在美团搜索排序中的实践](https://tech.meituan.com/2020/04/16/transformer-in-meituan.html)
- [7] [Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts](https://dl.acm.org/doi/10.1145/3219819.3220007)
- [8] [Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations](https://dl.acm.org/doi/abs/10.1145/3383313.3412236)

## 6.作者简介

培浩、肖垚、晓江、家琪、陈胜、云森、永超、利前等，均来自美团平台搜索与 NLP 部。

原文作者：美团技术团队

原文链接：https://tech.meituan.com/2021/07/08/multi-business-modeling.html

# 【NO.259】Spock单元测试框架介绍以及在美团优选的实践

## 1. 背景

XML之父Tim Bray最近在博客里有个好玩的说法：“代码不写测试就像上了厕所不洗手……单元测试是对软件未来的一项必不可少的投资。”具体来说，单元测试有哪些收益呢？

- 它是最容易保证代码覆盖率达到100%的测试。
- 可以⼤幅降低上线时的紧张指数。
- 单元测试能更快地发现问题（见下图左）。
- 单元测试的性价比最高，因为错误发现的越晚，修复它的成本就越高，而且难度呈指数式增长，所以我们要尽早地进行测试（见下图右）。
- 编码人员，一般也是单元测试的主要执行者，是唯一能够做到生产出无缺陷程序的人，其他任何人都无法做到这一点。
- 有助于源码的优化，使之更加规范，快速反馈，可以放心进行重构。

| ![img](https://p1.meituan.net/travelcube/e6e5b4bcf455e8f0827043dbe6ec04a635315.jpg) | ![pic2](https://p0.meituan.net/travelcube/4cfde1566b1928486c051bbe8287be2d42010.jpg)pic2 |
| :----------------------------------------------------------- | :----------------------------------------------------------- |
| 这张图来自微软的统计数据：Bug在单元测试阶段被发现，平均耗时3.25小时，如果漏到系统测试阶段，要花费11.5小时。 | 这张图，旨在说明两个问题：85%的缺陷都在代码设计阶段产生，而发现Bug的阶段越靠后，耗费成本就越高，指数级别的增高。 |

尽管单元测试有如此的收益，但在我们日常的工作中，仍然存在不少项目它们的单元测试要么是不完整要么是缺失的。常见的原因总结如下：代码逻辑过于复杂；写单元测试时耗费的时间较长；任务重、工期紧，或者干脆就不写了。

基于以上问题，相较于传统的JUnit单元测试，今天为大家推荐一款名为Spock的测试框架。目前，美团优选物流技术团队绝大部分后端服务已经采用了Spock作为测试框架，在开发效率、可读性和维护性方面取得了不错的收益。

不过网上Spock资料比较简单，甚至包括官网的Demo，无法解决我们项目中复杂业务场景面临的问题，通过深入学习和实践之后，本文会将一些经验分享出来，希望能够帮助大家提高开发测试的效率。

## 2. Spock是什么？和JUnit、jMock有什么区别？

Spock是一款国外优秀的测试框架，基于[BDD](https://en.wikipedia.org/wiki/Behavior-driven_development)（行为驱动开发）思想实现，功能非常强大。Spock结合Groovy动态语言的特点，提供了各种标签，并采用简单、通用、结构化的描述语言，让编写测试代码更加简洁、高效。[官方的介绍](https://spockframework.org/)如下：

![img](https://p0.meituan.net/travelcube/4403846a46a805b9cf304187317da94a52377.png)

> What is it? Spock is a testing and specification framework for Java and Groovy applications. What makes it stand out from the crowd is its beautiful and highly expressive specification language. Thanks to its JUnit runner, Spock is compatible with most IDEs, build tools, and continuous integration servers. Spock is inspired from JUnit, RSpec, jMock, Mockito, Groovy, Scala, Vulcans, and other fascinating life forms.

Spock是一个Java和Groovy`应用的测试和规范框架。之所以能够在众多测试框架中脱颖而出，是因为它优美而富有表现力的规范语言。Spock的灵感来自JUnit、RSpec、jMock、Mockito、Groovy、Scala、Vulcans。

简单来讲，Spock主要特点如下：

- 让测试代码更规范，内置多种标签来规范单元测试代码的语义，测试代码结构清晰，更具可读性，降低后期维护难度。
- 提供多种标签，比如：`given`、`when`、`then`、`expect`、`where`、`with`、`thrown`……帮助我们应对复杂的测试场景。
- 使用Groovy这种动态语言来编写测试代码，可以让我们编写的测试代码更简洁，适合敏捷开发，提高编写单元测试代码的效率。
- 遵从[BDD](https://en.wikipedia.org/wiki/Behavior-driven_development)（行为驱动开发）模式，有助于提升代码的质量。
- IDE兼容性好，自带Mock功能。

### 2.1 为什么使用Spock？ Spock和JUnit、jMock、Mockito的区别在哪里？

总的来说，JUnit、jMock、Mockito都是相对独立的工具，只是针对不同的业务场景提供特定的解决方案。其中JUnit单纯用于测试，并不提供Mock功能。

我们的服务大部分是分布式微服务架构。服务与服务之间通常都是通过接口的方式进行交互。即使在同一个服务内也会分为多个模块，业务功能需要依赖下游接口的返回数据，才能继续后面的处理流程。这里的下游不限于接口，还包括中间件数据存储比如Squirrel、DB、MCC配置中心等等，所以如果想要测试自己的代码逻辑，就必须把这些依赖项Mock掉。因为如果下游接口不稳定可能会影响我们代码的测试结果，让下游接口返回指定的结果集（事先准备好的数据），这样才能验证我们的代码是否正确，是否符合逻辑结果的预期。

尽管jMock、Mockito提供了Mock功能，可以把接口等依赖屏蔽掉，但不能对静态方法Mock。虽然PowerMock、jMockit能够提供静态方法的Mock，但它们之间也需要配合（JUnit + Mockito PowerMock）使用，并且语法上比较繁琐。工具多了就会导致不同的人写出的单元测试代码“五花八门”，风格相差较大。

Spock通过提供规范性的描述，定义多种标签（`given`、`when`、`then`、`where`等），去描述代码“应该做什么”，“输入条件是什么”，“输出是否符合预期”，从语义层面规范了代码的编写。

Spock自带Mock功能，使用简单方便（也支持扩展其他Mock框架，比如PowerMock），再加上Groovy动态语言的强大语法，能写出简洁高效的测试代码，同时能方便直观地验证业务代码的行为流转，增强工程师对代码执行逻辑的可控性。

## 3. 使用Spock解决单元测试开发中的痛点

如果在（`if/else`）分支很多的复杂场景下，编写单元测试代码的成本会变得非常高，正常的业务代码可能只有几十行，但为了测试这个功能覆盖大部分的分支场景，编写的测试代码可能远不止几十行。

之前有遇到过某个功能上线很久一直都很正常，没有出现过问题，但后来有个调用请求的数据不一样，走到了代码中一个不常用的逻辑分支时，出现了Bug。当时写这段代码的同学也认为只有很小几率才能走到这个分支，尽管当时写了单元测试，但因为时间比较紧张，分支又多，就漏掉了这个分支的测试。

尽管使用JUnit的@Parametered参数化注解或者DataProvider方式可以解决多数据分支问题，但不够直观，而且如果其中某一次分支测试Case出错了，它的报错信息也不够详尽。

这就需要一种编写测试用例高效、可读性强、占用工时少、维护成本低的测试框架。首先不能让业务人员排斥编写单元测试，更不能让工程师觉得写单元测试是在浪费时间。而且使用JUnit做测试工作量不算小。据初步统计，采用JUnit的话，它的测试代码行和业务代码行能到3:1。如果采用Spock作为测试框架的话，它的比例可缩减到1:1，能够大大提高编写测试用例的效率。

下面借用《编程珠玑》中一个计算税金的例子。

```java
public double calc(double income) {
        BigDecimal tax;
        BigDecimal salary = BigDecimal.valueOf(income);
        if (income <= 0) {
            return 0;
        }
        if (income > 0 && income <= 3000) {
            BigDecimal taxLevel = BigDecimal.valueOf(0.03);
            tax = salary.multiply(taxLevel);
        } else if (income > 3000 && income <= 12000) {
            BigDecimal taxLevel = BigDecimal.valueOf(0.1);
            BigDecimal base = BigDecimal.valueOf(210);
            tax = salary.multiply(taxLevel).subtract(base);
        } else if (income > 12000 && income <= 25000) {
            BigDecimal taxLevel = BigDecimal.valueOf(0.2);
            BigDecimal base = BigDecimal.valueOf(1410);
            tax = salary.multiply(taxLevel).subtract(base);
        } else if (income > 25000 && income <= 35000) {
            BigDecimal taxLevel = BigDecimal.valueOf(0.25);
            BigDecimal base = BigDecimal.valueOf(2660);
            tax = salary.multiply(taxLevel).subtract(base);
        } else if (income > 35000 && income <= 55000) {
            BigDecimal taxLevel = BigDecimal.valueOf(0.3);
            BigDecimal base = BigDecimal.valueOf(4410);
            tax = salary.multiply(taxLevel).subtract(base);
        } else if (income > 55000 && income <= 80000) {
            BigDecimal taxLevel = BigDecimal.valueOf(0.35);
            BigDecimal base = BigDecimal.valueOf(7160);
            tax = salary.multiply(taxLevel).subtract(base);
        } else {
            BigDecimal taxLevel = BigDecimal.valueOf(0.45);
            BigDecimal base = BigDecimal.valueOf(15160);
            tax = salary.multiply(taxLevel).subtract(base);
        }
        return tax.setScale(2, BigDecimal.ROUND_HALF_UP).doubleValue();
    }
```

能够看到上面的代码中有大量的`if-else`语句，Spock提供了where标签，可以让我们通过表格的方式来测试多种分支。

```groovy
@Unroll
def "个税计算,收入:#income, 个税:#result"() {
  expect: "when + then 的组合"
  CalculateTaxUtils.calc(income) == result

  where: "表格方式测试不同的分支逻辑"
  income || result
  -1     || 0
  0      || 0
  2999   || 89.97
  3000   || 90.0
  3001   || 90.1
  11999  || 989.9
  12000  || 990.0
  12001  || 990.2
  24999  || 3589.8
  25000  || 3590.0
  25001  || 3590.25
  34999  || 6089.75
  35000  || 6090.0
  35001  || 6090.3
  54999  || 12089.7
  55000  || 12090
  55001  || 12090.35
  79999  || 20839.65
  80000  || 20840.0
  80001  || 20840.45
}
```

![img](https://p0.meituan.net/travelcube/39c39b05b983ca33b760e060c598ec19255399.png)

上图中左边使用Spock写的单元测试代码，语法简洁，表格方式测试覆盖分支场景更加直观，开发效率高，更适合敏捷开发。

### 3.1 单元测试代码的可读性和后期维护

我们微服务场景很多时候需要依赖其他接口返回的结果，才能验证自己的代码逻辑。Mock工具是必不可少的。但jMock、Mockito的语法比较繁琐，再加上单元测试代码不像业务代码那么直观，又不能完全按照业务流程的思路写单元测试，这就让不少同学对单元测试代码可读性不够重视，最终导致测试代码难以阅读，维护起来更是难上加难。甚至很多同学自己写的单元测试，过几天再看也一样觉得“云里雾里”的。也有改了原来的代码逻辑导致单元测试执行失败的；或者新增了分支逻辑，单元测试没有覆盖到的；最终随着业务的快速迭代单元测试代码越来越难以维护。

Spock提供多种语义标签，如：`given`、`when`、`then`、`expect`、`where`、`with`、`and`等，从行为上规范了单元测试代码，每一种标签对应一种语义，让单元测试代码结构具有层次感，功能模块划分更加清晰，也便于后期的维护。

Spock自带Mock功能，使用上简单方便（Spock也支持扩展第三方Mock框架，比如PowerMock）。我们可以再看一个样例，对于如下的代码逻辑进行单元测试：

```java
public StudentVO getStudentById(int id) {
        List<StudentDTO> students = studentDao.getStudentInfo();
        StudentDTO studentDTO = students.stream().filter(u -> u.getId() == id).findFirst().orElse(null);
        StudentVO studentVO = new StudentVO();
        if (studentDTO == null) {
            return studentVO;
        }
        studentVO.setId(studentDTO.getId());
        studentVO.setName(studentDTO.getName());
        studentVO.setSex(studentDTO.getSex());
        studentVO.setAge(studentDTO.getAge());
        // 邮编
        if ("上海".equals(studentDTO.getProvince())) {
            studentVO.setAbbreviation("沪");
            studentVO.setPostCode("200000");
        }
        if ("北京".equals(studentDTO.getProvince())) {
            studentVO.setAbbreviation("京");
            studentVO.setPostCode("100000");
        }
        return studentVO;
    }
```

![img](https://p0.meituan.net/travelcube/6960c566d92d7208e01c83c22b2a5781496781.png)

比较明显，左边的JUnit单元测试代码冗余，缺少结构层次，可读性差，随着后续的迭代，势必会导致代码的堆积，维护成本会变得越来越高。右边的单元测试代码Spock会强制要求使用`given`、`when`、`then`这样的语义标签（至少一个），否则编译不通过，这样就能保证代码更加规范，结构模块化，边界范围清晰，可读性强，便于扩展和维护。而且使用了自然语言描述测试步骤，让非技术人员也能看懂测试代码（`given`表示输入条件，`when`触发动作，`then`验证输出结果）。

Spock自带的Mock语法也非常简单：`dao.getStudentInfo() >> [student1, student2]`。

两个右箭头`>>`表示模拟`getStudentInfo`接口的返回结果，再加上使用的Groovy语言，可以直接使用`[]`中括号表示返回的是`List`类型。

### 3.2 单元测试不仅仅是为了统计代码覆盖率，更重要的是验证业务代码的健壮性、业务逻辑的严谨性以及设计的合理性

在项目初期阶段，可能为了追赶进度而没有时间写单元测试，或者这个时期写的单元测试只是为了达到覆盖率的要求（比如为了满足新增代码行或者分支覆盖率统计要求）。

很多工程师写的单元测试基本都是采用Java这种强类型语言编写，各种底层接口的Mock写起来不仅繁琐而且耗时。这时的单元测试代码可能就写得比较粗糙，有粒度过大的，也有缺少单元测试结果验证的。这样的单元测试对代码的质量帮助不大，更多是为了测试而测试。最后时间没少花，可效果却没有达到。

针对有效测试用例方面，我们测试基础组件组开发了一些检测工具（作为抓手），比如去扫描大家写的单元测试，检测单元测试的断言有效性等。另外在结果校验方面，Spock表现也是十分优异的。我们可以来看接下来的场景：`void`方法，没有返回结果，如何写测试这段代码的逻辑是否正确？

如何确保单元测试代码是否执行到了`for`循环里面的语句，循环里面的打折计算又是否正确呢？

```java
  public void calculatePrice(OrderVO order){
        BigDecimal amount = BigDecimal.ZERO;
        for (SkuVO sku : order.getSkus()) {
            Integer skuId = sku.getSkuId();
            BigDecimal skuPrice = sku.getSkuPrice();
            BigDecimal discount = BigDecimal.valueOf(discountDao.getDiscount(skuId));
            BigDecimal price = skuPrice * discount;
            amount = amount.add(price);
        }
        order.setAmount(amount.setScale(2, BigDecimal.ROUND_HALF_DOWN));
    }
```

如果用Spock写的话，就会方便很多，如下图所示：

![img](https://p0.meituan.net/travelcube/28caf06f8af1824ed8e0745bd939c248133342.png)

这里，`2 * discountDao.getDiscount(_) >> 0.95 >> 0.8` 在`for`循环中一共调用了2次，第一次返回结果0.95，第二次返回结果0.8，最后再进行验证，类似于JUnit中的Assert断言。

这样的收益还是比较明显的，不仅提高了单元测试的可控性，而且方便验证业务代码的逻辑正确性和合理性，这也是BDD思想的一种体现。

## 4. Mock模拟

考虑如下场景，代码如下：

```java
@Service
public class StudentService {
    @Autowired
    private StudentDao studentDao;
    public StudentVO getStudentById(int id) {
        List<StudentDTO> students = studentDao.getStudentInfo();
        StudentDTO studentDTO = students.stream().filter(u -> u.getId() == id).findFirst().orElse(null);
        StudentVO studentVO = new StudentVO();
        if (studentDTO == null) {
            return studentVO;
        }
        studentVO.setId(studentDTO.getId());
        studentVO.setName(studentDTO.getName());
        studentVO.setSex(studentDTO.getSex());
        studentVO.setAge(studentDTO.getAge());
        // 邮编
        if ("上海".equals(studentDTO.getProvince())) {
            studentVO.setAbbreviation("沪");
            studentVO.setPostCode("200000");
        }
        if ("北京".equals(studentDTO.getProvince())) {
            studentVO.setAbbreviation("京");
            studentVO.setPostCode("100000");
        }
        return studentVO;
    }
}
```

其中`studentDao`是使用Spring注入的实例对象，我们只有拿到了返回的`students`，才能继续下面的逻辑（根据`id`筛选学生，`DTO`和`VO`转换，邮编等）。所以正常的做法是把`studentDao`的`getStudentInfo()`方法Mock掉，模拟一个指定的值，因为我们真正关心的是拿到`students`后自己代码的逻辑，这是需要重点验证的地方。按照上面的思路使用Spock编写的测试代码如下：

```groovy
class StudentServiceSpec extends Specification {
    def studentDao = Mock(StudentDao)
    def tester = new StudentService(studentDao: studentDao)

    def "test getStudentById"() {
        given: "设置请求参数"
        def student1 = new StudentDTO(id: 1, name: "张三", province: "北京")
        def student2 = new StudentDTO(id: 2, name: "李四", province: "上海")

        and: "mock studentDao返回值"
        studentDao.getStudentInfo() >> [student1, student2]

        when: "获取学生信息"
        def response = tester.getStudentById(1)

        then: "结果验证"
        with(response) {
            id == 1
            abbreviation == "京"
            postCode == "100000"
        }
    }
}
```

这里主要讲解Spock的代码（从上往下）。

`def studentDao = Mock(StudentDao)` 这一行代码使用Spock自带的Mock方法，构造一个`studentDao`的Mock对象，如果要模拟`studentDao`方法的返回，只需`studentDao.方法名() >> "模拟值"`的方式，两个右箭头的方式即可。`test getStudentById`方法是单元测试的主要方法，可以看到分为4个模块：`given`、`and`、`when`、`then`，用来区分不同单元测试代码的作用：

- `given`：输入条件（前置参数）。
- `when`：执行行为（`Mock`接口、真实调用）。
- `then`：输出条件（验证结果）。
- `and`：衔接上个标签，补充的作用。

每个标签后面的双引号里可以添加描述，说明这块代码的作用（非强制），如`when："获取信息"`。因为Spock使用Groovy作为单元测试开发语言，所以代码量上比使用Java写的会少很多，比如`given`模块里通过构造函数的方式创建请求对象。

![img](https://p0.meituan.net/travelcube/f2481f9065acf487106ca4ce5d8b7e1a45797.png)

实际上`StudentDTO.java` 这个类并没有3个参数的构造方法，是Groovy帮我们实现的。Groovy默认会提供一个包含所有对象属性的构造方法。而且调用方式上可以指定属性名，类似于`key:value`的语法，非常人性化，方便在属性多的情况下构造对象，如果使用Java写，可能就要调用很多的`setXxx()`方法，才能完成对象初始化的工作。

![img](https://p0.meituan.net/travelcube/71741f252a0c73d52bce0993c51e908d23530.png)

这个就是Spock的Mock用法，当调用`studentDao.getStudentInfo()`方法时返回一个`List`。`List`的创建也很简单，中括号`[]`即表示`List`，Groovy会根据方法的返回类型，自动匹配是数组还是`List`，而`List`里的对象就是之前`given`块里构造的`user`对象，其中 `>>` 就是指定返回结果，类似`Mockito`的`when().thenReturn()`语法，但更简洁一些。

如果要指定返回多个值的话，可以使用`3`个右箭头`>>>`，比如：`studentDao.getStudentInfo() >>> [[student1,student2],[student3,student4],[student5,student6]]`。

也可以写成这样：`studentDao.getStudentInfo() >> [student1,student2] >> [student3,student4] >> [student5,student6]`。

每次调用`studentDao.getStudentInfo()`方法返回不同的值。

```java
public List<StudentDTO> getStudentInfo(String id){
    List<StudentDTO> students = new ArrayList<>();
    return students;
}
```

这个`getStudentInfo(String id)`方法，有个参数`id`，这种情况下如果使用Spock的Mock模拟调用的话，可以使用下划线`_`匹配参数，表示任何类型的参数，多个逗号隔开，类似于`Mockito`的`any()`方法。如果类中存在多个同名方法，可以通过 `_ as参数类型` 的方式区别调用，如下面的语法：

```groovy
// _ 表示匹配任意类型参数
List<StudentDTO> students = studentDao.getStudentInfo(_);

// 如果有同名的方法，使用as指定参数类型区分
List<StudentDTO> students = studentDao.getStudentInfo(_ as String);
```

`when`模块里是真正调用要测试方法的入口`tester.getStudentById()`。`then`模块作用是验证被测方法的结果是否正确，符合预期值，所以这个模块里的语句必须是`boolean`表达式，类似于JUnit的`assert`断言机制，但不必显示地写`assert`，这也是一种约定优于配置的思想。`then`块中使用了Spock的`with`功能，可以验证返回结果`response`对象内部的多个属性是否符合预期值，这个相对于JUnit的`assertNotNull`或`assertEquals`的方式更简单一些。

#### 4.1 强大的Where

上面的业务代码有2个`if`判断，是对邮编处理逻辑：

```java
  // 邮编
  if ("上海".equals(studentDTO.getProvince())) {
       studentVO.setAbbreviation("沪");
       studentVO.setPostCode("200000");
   }
   if ("北京".equals(studentDTO.getProvince())) {
       studentVO.setAbbreviation("京");
       studentVO.setPostCode("100000");
   }
```

如果要完全覆盖这2个分支就需要构造不同的请求参数，多次调用被测试方法才能走到不同的分支。在前面，我们介绍了Spock的`where`标签可以很方便的实现这种功能，代码如下所示：

```groovy
   @Unroll
   def "input 学生id:#id, 返回的邮编:#postCodeResult, 返回的省份简称:#abbreviationResult"() {
        given: "Mock返回的学生信息"
        studentDao.getStudentInfo() >> students

        when: "获取学生信息"
        def response = tester.getStudentById(id)

        then: "验证返回结果"
        with(response) {
            postCode == postCodeResult
            abbreviation == abbreviationResult
        }
        where: "经典之处：表格方式验证学生信息的分支场景"
        id | students                    || postCodeResult | abbreviationResult
        1  | getStudent(1, "张三", "北京") || "100000"       | "京"
        2  | getStudent(2, "李四", "上海") || "200000"       | "沪"
    }

    def getStudent(def id, def name, def province) {
        return [new StudentDTO(id: id, name: name, province: province)]
    }
```

`where`模块第一行代码是表格的列名，多个列使用`|`单竖线隔开，`||`双竖线区分输入和输出变量，即左边是输入值，右边是输出值。格式如下：

```
输入参数1 | 输入参数2 || 输出结果1 | 输出结果2
```

而且`IntelliJ IDEA`支持`format`格式化快捷键，因为表格列的长度不一样，手动对齐比较麻烦。表格的每一行代表一个测试用例，即被测方法执行了2次，每次的输入和输出都不一样，刚好可以覆盖全部分支情况。比如`id`、`students`都是输入条件，其中`students`对象的构造调用了`getStudent`方法，每次测试业务代码传入不同的`student`值，`postCodeResult`、`abbreviationResult`表示对返回的`response`对象的属性判断是否正确。第一行数据的作用是验证返回的邮编是否是`100000`，第二行是验证邮编是否是`200000`。这个就是`where`+`with`的用法，更符合我们实际测试的场景，既能覆盖多种分支，又可以对复杂对象的属性进行验证，其中在定义的测试方法名，使用了Groovy的字面值特性：

![img](https://p1.meituan.net/travelcube/3c34e7616ad46b6652bebe2766d62c6530352.png)

即把请求参数值和返回结果值的字符串动态替换掉，`#id`、`#postCodeResult`、`#abbreviationResult`#号后面的变量是在方法内部定义的，实现占位符的功能。

`@Unroll`注解，可以把每一次调用作为一个单独的测试用例运行，这样运行后的单元测试结果更加直观：

![img](https://p0.meituan.net/travelcube/bff376f956a6462437d1660f5cbe86d857414.png)

而且如果其中某行测试结果不对，Spock的错误提示信息也很详细，方便进行排查（比如我们把第1条测试用例返回的邮编改成`100001`）：

![img](https://p0.meituan.net/travelcube/bc83d5d0711eb30b962558f711058095365939.png)

可以看出，第1条测试用例失败，错误信息是`postCodeResult`的预期结果和实际结果不符，业务代码逻辑返回的邮编是`100000`，而我们预期的邮编是`100001`，这样就可以排查是业务代码逻辑有问题，还是我们的断言不对。

## 5. 异常测试

我们再看下异常方面的测试，例如下面的代码：

```java
 public void validateStudent(StudentVO student) throws BusinessException {
        if(student == null){
            throw new BusinessException("10001", "student is null");
        }
        if(StringUtils.isBlank(student.getName())){
            throw new BusinessException("10002", "student name is null");
        }
        if(student.getAge() == null){
            throw new BusinessException("10003", "student age is null");
        }
        if(StringUtils.isBlank(student.getTelephone())){
            throw new BusinessException("10004", "student telephone is null");
        }
        if(StringUtils.isBlank(student.getSex())){
            throw new BusinessException("10005", "student sex is null");
        }
    }
```

`BusinessException`是封装的业务异常，主要包含`code`、`message`属性：

```java
/**
 * 自定义业务异常
 */
public class BusinessException extends RuntimeException {
    private String code;
    private String message;

    setXxx...
    getXxx...
}
```

这个大家应该都很熟悉，针对这种抛出多个不同错误码和错误信息的异常。如果使用JUnit的方式测试，会比较麻烦。如果是单个异常还好，如果是多个的话，测试代码就不太好写。

```java
    @Test
    public void testException() {
        StudentVO student = null;
        try {
            service.validateStudent(student);
        } catch (BusinessException e) {
            Assert.assertEquals(e.getCode(), "10001");
            Assert.assertEquals(e.getMessage(), "student is null");
        }

        student = new StudentVO();
        try {
            service.validateStudent(student);
        } catch (BusinessException e) {
            Assert.assertEquals(e.getCode(), "10002");
            Assert.assertEquals(e.getMessage(), "student name is null");
        }
    }
```

当然可以使用JUnit的`ExpectedException`方式：

```java
@Rule
public ExpectedException exception = ExpectedException.none();
exception.expect(BusinessException.class); // 验证异常类型
exception.expectMessage("xxxxxx"); //验证异常信息
```

或者使用`@Test(expected = BusinessException.class)` 注解，但这两种方式都有缺陷。

`@Test`方式不能指定断言的异常属性，比如`code`、`message`。`ExpectedException`的方式也只提供了`expectMessage`的API，对自定义的`code`不支持，尤其像上面的有很多分支抛出多种不同异常码的情况。接下来我们看下Spock是如何解决的。Spock内置`thrown()`方法，可以捕获调用业务代码抛出的预期异常并验证，再结合`where`表格的功能，可以很方便地覆盖多种自定义业务异常，代码如下：

```groovy
    @Unroll
    def "validate student info: #expectedMessage"() {
        when: "校验"
        tester.validateStudent(student)

        then: "验证"
        def exception = thrown(expectedException)
        exception.code == expectedCode
        exception.message == expectedMessage

        where: "测试数据"
        student           || expectedException | expectedCode | expectedMessage
        getStudent(10001) || BusinessException | "10001"      | "student is null"
        getStudent(10002) || BusinessException | "10002"      | "student name is null"
        getStudent(10003) || BusinessException | "10003"      | "student age is null"
        getStudent(10004) || BusinessException | "10004"      | "student telephone is null"
        getStudent(10005) || BusinessException | "10005"      | "student sex is null"
    }

    def getStudent(code) {
        def student = new StudentVO()
        def condition1 = {
            student.name = "张三"
        }
        def condition2 = {
            student.age = 20
        }
        def condition3 = {
            student.telephone = "12345678901"
        }
        def condition4 = {
            student.sex = "男"
        }

        switch (code) {
            case 10001:
                student = null
                break
            case 10002:
                student = new StudentVO()
                break
            case 10003:
                condition1()
                break
            case 10004:
                condition1()
                condition2()
                break
            case 10005:
                condition1()
                condition2()
                condition3()
                break
        }
        return student
    }
```

在`then`标签里用到了Spock的`thrown()`方法，这个方法可以捕获我们要测试的业务代码里抛出的异常。`thrown()`方法的入参`expectedException`，是我们自己定义的异常变量，这个变量放在`where`标签里就可以实现验证多种异常情况的功能（`Intellij Idea`格式化快捷键，可以自动对齐表格）。`expectedException`类型调用`validateUser`方法里定义的`BusinessException`异常，可以验证它所有的属性，`code`、`message`是否符合预期值。

![img](https://p0.meituan.net/travelcube/6fd52ed3170746695dd2304502688d8367835.png)

## 6. Spock静态方法测试

接下来，我们一起看下Spock如何扩展第三方PowerMock对静态方法进行测试。

Spock的单元测试代码继承自`Specification`基类，而`Specification`又是基于JUnit的注解`@RunWith()`实现的，代码如下：

![img](https://p1.meituan.net/travelcube/80992f576e33e752c798d854e1733b1238195.png)

PowerMock的`PowerMockRunner`也是继承自JUnit，所以使用PowerMock的`@PowerMockRunnerDelegate()`注解，可以指定Spock的父类`Sputnik`去代理运行PowerMock，这样就可以在Spock里使用`PowerMock`去模拟静态方法、`final`方法、私有方法等。其实Spock自带的GroovyMock可以对Groovy文件的静态方法Mock，但对Java代码支持不完整，只能Mock当前Java类的静态方法，[官方给出的解释](http://spockframework.org/spock/docs/1.3/all_in_one.html#_mocking_static_methods)如下：

![img](https://p0.meituan.net/travelcube/20ddd0aafe9dfcd5eae1f072fd311667106791.png)

如下代码：

```java
 public StudentVO getStudentByIdStatic(int id) {
        List<StudentDTO> students = studentDao.getStudentInfo();

        StudentDTO studentDTO = students.stream().filter(u -> u.getId() == id).findFirst().orElse(null);
        StudentVO studentVO = new StudentVO();
        if (studentDTO == null) {
            return studentVO;
        }
        studentVO.setId(studentDTO.getId());
        studentVO.setName(studentDTO.getName());
        studentVO.setSex(studentDTO.getSex());
        studentVO.setAge(studentDTO.getAge());

        // 静态方法调用
        String abbreviation = AbbreviationProvinceUtil.convert2Abbreviation(studentDTO.getProvince());
        studentVO.setAbbreviation(abbreviation);
        studentVO.setPostCode(studentDTO.getPostCode());

        return studentVO;
    }
```

上面使用了`AbbreviationProvinceUtil.convert2Abbreviation()`静态方法，对应的测试用例代码如下：

```Groovy
@RunWith(PowerMockRunner.class)
@PowerMockRunnerDelegate(Sputnik.class)
@PrepareForTest([AbbreviationProvinceUtil.class])
@SuppressStaticInitializationFor(["example.com.AbbreviationProvinceUtil"])
class StudentServiceStaticSpec extends Specification {
    def studentDao = Mock(StudentDao)
    def tester = new StudentService(studentDao: studentDao)

    void setup() {
        // mock静态类
        PowerMockito.mockStatic(AbbreviationProvinceUtil.class)
    }

    def "test getStudentByIdStatic"() {
        given: "创建对象"
        def student1 = new StudentDTO(id: 1, name: "张三", province: "北京")
        def student2 = new StudentDTO(id: 2, name: "李四", province: "上海")

        and: "Mock掉接口返回的学生信息"
        studentDao.getStudentInfo() >> [student1, student2]

        and: "Mock静态方法返回值"
        PowerMockito.when(AbbreviationProvinceUtil.convert2Abbreviation(Mockito.any())).thenReturn(abbreviationResult)

        when: "调用获取学生信息方法"
        def response = tester.getStudentByIdStatic(id)

        then: "验证返回结果是否符合预期值"
        with(response) {
            abbreviation == abbreviationResult
        }
        where:
        id || abbreviationResult
        1  || "京"
        2  || "沪"
    }
}
```

在`StudentServiceStaticSpec`类的头部使用`@PowerMockRunnerDelegate(Sputnik.class)`注解，交给Spock代理执行，这样既可以使用Spock +Groovy的各种功能，又可以使用PowerMock的对静态，`final`等方法的Mock。`@SuppressStaticInitializationFor(["example.com.AbbreviationProvinceUtil"])`，这行代码的作用是限制`AbbreviationProvinceUtil`类里的静态代码块初始化，因为`AbbreviationProvinceUtil`类在第一次调用时可能会加载一些本地资源配置，所以可以使用PowerMock禁止初始化。然后在`setup()`方法里对静态类进行Mock设置，`PowerMockito.mockStatic(AbbreviationProvinceUtil.class)`。最后在`test getStudentByIdStatic`测试方法里对`convert2Abbreviation()`方法指定返回默认值：`PowerMockito.when(AbbreviationProvinceUtil.convert2Abbreviation(Mockito.any())).thenReturn(abbreviationResult)`。

运行时在控制台会输出：

![img](https://p0.meituan.net/travelcube/da0606bde2a302e2da004f6d7d8b57b9206036.png)

Notifications are not supported for behaviour ALL_TESTINSTANCES_ARE_CREATED_FIRST

这是Powermock的警告信息，不影响运行结果。

如果单元测试代码不需要对静态方法、`final`方法Mock，就没必要使用PowerMock，使用Spock自带的`Mock()`就足够了。因为PowerMock的原理是在编译期通过ASM字节码修改工具修改代码，然后使用自己的`ClassLoader`加载，而加载的静态方法越多，测试耗时就会越长。

## 7. 动态Mock静态方法

考虑场景，让静态方法每次调用返回不同的值。

以下代码：

```java
public List<OrderVO> getOrdersBySource(){
        List<OrderVO> orderList = new ArrayList<>();
        OrderVO order = new OrderVO();
        if ("APP".equals(HttpContextUtils.getCurrentSource())) {
            if("CNY".equals(HttpContextUtils.getCurrentCurrency())){
                System.out.println("source -> APP, currency -> CNY");
            } else {
                System.out.println("source -> APP, currency -> !CNY");
            }
            order.setType(1);
        } else if ("WAP".equals(HttpContextUtils.getCurrentSource())) {
            System.out.println("source -> WAP");
            order.setType(2);
        } else if ("ONLINE".equals(HttpContextUtils.getCurrentSource())) {
            System.out.println("source -> ONLINE");
            order.setType(3);
        }
        orderList.add(order);
        return orderList;
}
```

这段代码的`if else`分支逻辑，主要是依据`HttpContextUtils`这个工具类的静态方法`getCurrentSource()`和`getCurrentCurrency()`的返回值来决定流程。这样的业务代码也是我们平时写单元测试时经常遇到的场景，如果能让`HttpContextUtils.getCurrentSource()`静态方法每次Mock出不同的值，就可以很方便地覆盖`if else`的全部分支逻辑。Spock的`where`标签可以方便地和PowerMock结合使用，让PowerMock模拟的静态方法每次返回不同的值，代码如下：

![img](https://p0.meituan.net/travelcube/64057c9b1e17b8b06f38c03012a67e62195579.png)

PowerMock的`thenReturn`方法返回的值是`source`和`currency`等2个变量，不是具体的数据，这2个变量对应`where`标签里的前两列`source|currency`。这样的写法，就可以在每次测试业务方法时，让`HttpContextUtils.getCurrentSource()`和`HttpContextUtils.getCurrentCurrency()`返回不同的来源和币种，就能轻松的覆盖`if`和`else`的分支代码。即Spock使用`where`表格的方式让PowerMock具有了动态Mock的功能。接下来，我们再看一下如何对于`final`变量进行Mock。

```java
public List<OrderVO> convertOrders(List<OrderDTO> orders){
        List<OrderVO> orderList = new ArrayList<>();
        for (OrderDTO orderDTO : orders) {
            OrderVO orderVO = OrderMapper.INSTANCE.convert(orderDTO);
            if (1 == orderVO.getType()) {
                orderVO.setOrderDesc("App端订单");
            } else if(2 == orderVO.getType()) {
                orderVO.setOrderDesc("H5端订单");
            } else if(3 == orderVO.getType()) {
                orderVO.setOrderDesc("PC端订单");
            }
            orderList.add(orderVO);
        }
        return orderList;
}
```

这段代码里的`for`循环第一行调用了`OrderMapper.INSTANCE.convert()`转换方法，将`orderDTO`转换为`orderVO`，然后根据`type`值走不同的分支，而`OrderMapper`是一个接口，代码如下：

```java
@Mapper
public interface OrderMapper {
    // 即使不用static final修饰，接口里的变量默认也是静态、final的
    static final OrderMapper INSTANCE = Mappers.getMapper(OrderMapper.class);

    @Mappings({})
    OrderVO convert(OrderDTO requestDTO);
}
```

`INSTANCE`是接口`OrderMapper`里定义的变量，接口里的变量默认都是`static final`的，所以我们要先把这个`INSTANCE`静态`final`变量Mock掉，这样才能调用它的方法`convert()`返回我们想要的值。`OrderMapper`这个接口是`mapstruct`工具的用法，`mapstruct`是做对象属性映射的一个工具，它会自动生成`OrderMapper`接口的实现类，生成对应的`set`、`get`方法，把`orderDTO`的属性值赋给`orderVO`属性，通常情况下会比使用反射的方式好不少。看下Spock如何写这个单元测试：

```Groovy
@Unroll
def "test convertOrders"() {
  given: "Mock掉OrderMapper的静态final变量INSTANCE，并结合Spock设置动态返回值"
  def orderMapper = Mock(OrderMapper.class)
  Whitebox.setInternalState(OrderMapper.class, "INSTANCE", orderMapper)
  orderMapper.convert(_) >> order

  when: 
  def orders = service.convertOrders([new OrderDTO()])

  then: "验证结果"
  with(orders) {
    it[0].orderDesc == desc
  }

  where: "测试数据"
  order                || desc
  new OrderVO(type: 1) || "App端订单"
  new OrderVO(type: 2) || "H5端订单"
  new OrderVO(type: 3) || "PC端订单"
}
```

- 首先使用Spock自带的`Mock()`方法，将`OrderMapper`类Mock为一个模拟对象`orderMapper`，`def orderMapper = Mock(OrderMapper.class)`。
- 然后使用PowerMock的`Whitebox.setInternalState()`，对`OrderMapper`接口的`static final`常量`INSTANCE`赋值(`Spock`不支持静态常量的`Mock`)，赋的值正是使用SpockMock的对象`orderMapper`。
- 使用Spock的Mock模拟`convert()`方法调用，`orderMapper.convert(_) >> order`，再结合`where`表格，实现动态Mock接口的功能。

主要是这3行代码：

```groovy
def orderMapper = Mock(OrderMapper.class) // 先使用Spock的Mock
Whitebox.setInternalState(OrderMapper.class, "INSTANCE", orderMapper) // 通过PowerMock把Mock对象orderMapper赋值给静态常量INSTANCE
orderMapper.convert(_) >> order // 结合where模拟不同的返回值
```

这样就可以使用Spock结合PowerMock测试静态常量，达到覆盖`if else`不同分支逻辑的功能。

## 8. 覆盖率

Jacoco是统计单元测试覆盖率的一种工具，当然Spock也自带了覆盖率统计的功能，这里使用第三方Jacoco的原因主要是国内公司使用的比较多一些，包括美团很多技术团队现在使用的也是Jacoco，所以为了兼容就以Jacoco来查看单元测试覆盖率。这里说下如何通过Jacoco确认分支是否完全覆盖到。

在pom文件里引用Jacoco的插件：`jacoco-maven-plugin`，然后执行`mvn package` 命令，成功后会在target目录下生成单元测试覆盖率的报告，点开报告找到对应的被测试类查看覆盖情况。

![img](https://p0.meituan.net/travelcube/c4b6ed707ffff42ce3c034ebd1109088440367.png)

绿色背景表示完全覆盖，黄色是部分覆盖，红色没有覆盖到。比如第`34`行黄色背景的`else if()` 判断，提示有二分之一的分支缺失，虽然它下面的代码也被覆盖了（显示为绿色），这种情况跟具体使用哪种单元测试框架没关系，因为这只是分支覆盖率统计的规则，只不过使用Spock的话，解决起来会更简单，只需在`where`下增加一行针对的测试数据即可。

## 9. DAO层测试

DAO层的测试有些不太一样，不能再使用Mock，否则无法验证SQL是否正确。对于DAO测试有一般最简的方式是直接使用`@SpringBootTest`注解启动测试环境，通过Spring创建Mybatis、Mapper实例，但这种方式并不属于单元测试，而是集成测试范畴了，因为当启用`@SpringBootTest`时，会把整个应用的上下文加载进来。不仅耗时时间长，而且一旦依赖环境上有任何问题，可能会影响启动，进而影响DAO层的测试。最后，需要到数据库尽可能隔离，因为如果大家都使用同一个Test环境的数据的话，一旦测试用例编写有问题，就可能会污染Test环境的数据。

针对以上场景，可采用以下方案： 1. 通过MyBatis的SqlSession启动mapper实例（避免通过Spring启动加载上下文信息）。 2. 通过内存数据库（如H2）隔离大家的数据库连接（完全隔离不会存在互相干扰的现象）。 3. 通过DBUnit工具，用作对于数据库层的操作访问工具。 4. 通过扩展Spock的注解，提供对于数据库Schema创建和数据Data加载的方式。如csv、xml或直接`Closure`编写等。

在pom文件增加相应的依赖。

```xml
<dependency>
     <groupId>com.h2database</groupId>
     <artifactId>h2</artifactId>
     <version>1.4.200</version>
     <scope>test</scope>
 </dependency>
 <dependency>
     <groupId>org.dbunit</groupId>
     <artifactId>dbunit</artifactId>
     <version>2.5.1</version>
     <scope>test</scope>
 </dependency>
```

增加Groovy的maven插件、资源文件拷贝以及测试覆盖率统计插件。

```xml
<!-- 测试插件 -->
<plugin>
  <groupId>org.codehaus.gmavenplus</groupId>
  <artifactId>gmavenplus-plugin</artifactId>
  <version>1.8.1</version>
  <executions>
    <execution>
      <goals>
        <goal>addSources</goal>
        <goal>addTestSources</goal>
        <goal>generateStubs</goal>
        <goal>compile</goal>
        <goal>generateTestStubs</goal>
        <goal>compileTests</goal>
        <goal>removeStubs</goal>
        <goal>removeTestStubs</goal>
      </goals>
    </execution>
  </executions>
</plugin>
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-surefire-plugin</artifactId>
  <version>3.0.0-M3</version>
  <configuration>
    <useFile>false</useFile>
    <includes>
      <include>**/*Spec.java</include>
    </includes>
    <parallel>methods</parallel>
    <threadCount>10</threadCount>
    <testFailureIgnore>true</testFailureIgnore>
  </configuration>
</plugin>
<plugin>
  <groupId>org.apache.maven.plugins</groupId>
  <artifactId>maven-resources-plugin</artifactId>
  <version>2.6</version>
  <executions>
    <execution>
      <id>copy-resources</id>
      <phase>compile</phase>
      <goals>
        <goal>copy-resources</goal>
      </goals>
      <configuration>
        <outputDirectory>${basedir}/target/resources</outputDirectory>
        <resources>
          <resource>
            <directory>${basedir}/src/main/resources</directory>
            <filtering>true</filtering>
          </resource>
        </resources>
      </configuration>
    </execution>
  </executions>
</plugin>
<plugin>
  <groupId>org.jacoco</groupId>
  <artifactId>jacoco-maven-plugin</artifactId>
  <version>0.8.2</version>
  <executions>
    <execution>
      <id>prepare-agent</id>
      <goals>
        <goal>prepare-agent</goal>
      </goals>
    </execution>
    <execution>
      <id>report</id>
      <phase>prepare-package</phase>
      <goals>
        <goal>report</goal>
      </goals>
    </execution>
    <execution>
      <id>post-unit-test</id>
      <phase>test</phase>
      <goals>
        <goal>report</goal>
      </goals>
      <configuration>
        <dataFile>target/jacoco.exec</dataFile>
        <outputDirectory>target/jacoco-ut</outputDirectory>
      </configuration>
    </execution>
  </executions>
</plugin>
```

加入对于Spock扩展的自动处理框架（用于数据`Schema`和`Data`初始化操作）。

![img](https://p0.meituan.net/travelcube/bf6258bf6608da3d9e6e3f28806ae056104189.png)

这里介绍一下主要内容，注解`@MyDbUnit`：

```java
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.METHOD)
@ExtensionAnnotation(MyDbUnitExtension.class)
@interface MyDbUnit {
    /**
     * <pre>
     * content = {
     *    your_table_name(id: 1, name: 'xxx', age: 21)
     *    your_table_name(id: 2, name: 'xxx', age: 22)
     * })
     </pre>
     * @return
     */
    Class<? extends Closure> content() default Closure.class;
    /**
     * xml存放路径(相对于测试类)
     * @return
     */
    String xmlLocation() default "";
    /**
     * csv存放路径(相对于测试类)
     * @return
     */
    String csvLocation() default "";
}
```

考虑以下代码的测试：

```java
@Repository("personInfoMapper")
public interface PersonInfoMapper {
    @Delete("delete from person_info where id=#{id}")
    int deleteById(Long id);

    @Select("select count(*) from person_info")
    int count();

    @Select("select * from user_info")
    List<PersonInfoDO> selectAll();
}
```

`Demo1` （使用`@MyDbUnit`，`content`指定导入数据内容，格式`Closure`）。

```groovy
class Demo1Spec extends MyBaseSpec {

    /**
     * 直接获取待测试的mapper
     */
    def personInfoMapper = MapperUtil.getMapper(PersonInfoMapper.class)

    /**
     * 测试数据准备，通常为sql表结构创建用的ddl，支持多个文件以逗号分隔。
     */
    def setup() {
        executeSqlScriptFile("com/xxx/xxx/xxx/......../schema.sql")
    }
    /**
     * 数据表清除，通常待drop的数据表
     */
    def cleanup() {
        dropTables("person_info")
    }

    /**
     * 直接构造数据库中的数据表,此方法适用于数据量较小的mapper sql测试
     */
    @MyDbUnit(
            content = {
                person_info(id: 1, name: "abc", age: 21)
                person_info(id: 2, name: "bcd", age: 22)
                person_info(id: 3, name: "cde", age: 23)
            }
    )
    def "demo1_01"() {
        when:
        int beforeCount = personInfoMapper.count()
        // groovy sql用于快速执行sql，不仅能验证数据结果，也可向数据中添加数据。
        def result = new Sql(dataSource).firstRow("select * from `person_info`") 
        int deleteCount = personInfoMapper.deleteById(1L)
        int afterCount = personInfoMapper.count()

        then:
        beforeCount == 3
        result.name == "abc"
        deleteCount == 1
        afterCount == 2
    }

    /**
     * 直接构造数据库中的数据表,此方法适用于数据量较小的mapper sql测试
     */
    @MyDbUnit(content = {
        person_info(id: 1, name: 'a', age: 21)
    })
    def "demo1_02"() {
        when:
        int beforeCount = personInfoMapper.count()
        def result = new Sql(dataSource).firstRow("select * from `person_info`")
        int deleteCount = personInfoMapper.deleteById(1L)
        int afterCount = personInfoMapper.count()

        then:
        beforeCount == 1
        result.name == "a"
        deleteCount == 1
        afterCount == 0
    }
}
```

![img](https://p0.meituan.net/travelcube/c3ef02e720850a7dfd896a4cba48e45e64936.png)

在`setup()`阶段，把数据库表中的`Schema`创建好，然后通过下面的`@MyDbUnit`注解的`content`属性，把数据导入到数据库中。`person_info`是表名，`id`、`name`、`age`是数据。

![img](https://p0.meituan.net/travelcube/c594497ed85fa3c3600290ec1ff9d95372574.png)

通过`MapperUtil.getMapper()`方法获取`mapper`实例。

![img](https://p0.meituan.net/travelcube/a7c0d905fdb3d37d462c78daf1a3b33b37850.png)

当测试数据量较大时，可以编写相应的数据文件，通过`@MyDbUnit`的`xmlLocation`或`csvLocation`加载文件（分别支持csv和xml格式）。

![img](https://p0.meituan.net/travelcube/ffd4e358f8b1aba82c238851df3fe460171003.png)

如通过csv加载文件，`csvLocation`指向csv文件所在文件夹。

```groovy
 @MyDbUnit(csvLocation = "com/xxx/........./data01")
    def "demo2_01"() {
        when:
        int beforeCount = personInfoMapper.count()
        def result = new Sql(dataSource).firstRow("select * from `person_info`")
        int deleteCount = personInfoMapper.deleteById(1L)
        int afterCount = personInfoMapper.count()

        then:
        beforeCount == 3
        result.name == "abc"
        deleteCount == 1
        afterCount == 2
    }
```

通过xml加载文件，`xmlLocation`指向xml文件所在路径。

```groovy
@MyDbUnit(xmlLocation = "com/xxxx/........./demo3_02.xml")
    def "demo3_02"() {
        when:
        int beforeCount = personInfoMapper.count()
        def result = new Sql(dataSource).firstRow("select * from `person_info`")
        int deleteCount = personInfoMapper.deleteById(1L)
        int afterCount = personInfoMapper.count()

        then:
        beforeCount == 1
        result.name == "a"
        deleteCount == 1
        afterCount == 0
    }
```

还可以不通过`@MyDbUnit`而使用API直接加载测试数据文件。

```groovy
class Demo4Spec extends MyBaseSpec {
    def personInfoMapper = MapperUtil.getMapper(PersonInfoMapper.class)

    /**
     * 数据表清除，通常待drop的数据表
     */
    def cleanup() {
        dropTables("person_info")
    }
    def "demo4_01"() {
        given:
        executeSqlScriptFile("com/xxxx/.........../schema.sql")
        IDataSet dataSet = MyDbUnitUtil.loadCsv("com/xxxx/.........../data01");
        DatabaseOperation.CLEAN_INSERT.execute(MyIDatabaseConnection.getInstance().getConnection(), dataSet);

        when:
        int beforeCount = personInfoMapper.count()
        def result = new Sql(dataSource).firstRow("select * from `person_info`")
        int deleteCount = personInfoMapper.deleteById(1L)
        int afterCount = personInfoMapper.count()

        then:
        beforeCount == 3
        result.name == "abc"
        deleteCount == 1
        afterCount == 2
    }

    def "demo4_02"() {
        given:
        executeSqlScriptFile("com/xxxx/.........../schema.sq")
        IDataSet dataSet = MyDbUnitUtil.loadXml("com/xxxx/.........../demo3_02.xml");
        DatabaseOperation.CLEAN_INSERT.execute(MyIDatabaseConnection.getInstance().getConnection(), dataSet);

        when:
        int beforeCount = personInfoMapper.count()
        def result = new Sql(dataSource).firstRow("select * from `person_info`")
        int deleteCount = personInfoMapper.deleteById(1L)
        int afterCount = personInfoMapper.count()

        then:
        beforeCount == 1
        result.name == "a"
        deleteCount == 1
        afterCount == 0
    }
}
```

最后为大家梳理了一些文档，供大家参考。

- [Spock Framework Reference Documentation](https://spockframework.org/spock/docs/2.0/all_in_one.html)
- [老K的Java博客](https://javakk.com/category/spock)

**作者简介**

建华，美团优选事业部工程师。

原文作者：美团技术团队

原文链接：https://tech.meituan.com/2021/08/06/spock-practice-in-meituan.html

# 【NO.260】一款可以让大型iOS工程编译速度提升50%的工具

## 1.cocoapods-hmap-prebuilt 是什么？

cocoapods-hmap-prebuilt 是美团平台迭代组自研的一款 cocoapods 插件，以 [Header Map 技术](https://clang.llvm.org/doxygen/classclang_1_1HeaderMap.html) 为基础，进一步提升代码的编译速度，完善头文件的搜索机制。

虽然以二进制组件的方式构建 App 是 HPX （美团移动端统一持续集成/交付平台）的主流解决方案，但在某些场景下（Profile、Address/Thread/UB/Coverage Sanitizer、App 级别静态检查、ObjC 方法调用兼容性检查等等），我们的构建工作还是需要以全源码编译的方式进行；而且在实际开发过程中，大多是以源码的方式进行开发，所以我们将实验对象设置为基于全源码编译的流程。

废话不多说，我们来看看它的实际使用效果！

总的来说，以美团和大众点评的全源码编译流程为实验对象的前提下，cocoapods-hmap-prebuilt 插件能将总链路提升 45% 以上的速度，在 Xcode 打包环节上能提升 50% 以上的速度，是不是有点动心了？

为了更好的理解这个插件的价值和功能，我们不妨先看一下当前的工程中存在的问题。

## 2.为什么现有的项目不够好？

目前，美团内的 App 都是基于 CocoaPods 做包管理方面的工作，所以在实际的开发过程中，CocoaPods 会在 `Pods/Header/` 目录下添加组件名目录和头文件软链，类似于下面的形式：

```sh
/Users/sketchk/Desktop/MyApp/Pods
└── Headers
    ├── Private
    │   └── AFNetworking
    │       ├── AFHTTPRequestOperation.h -> ./XXX/AFHTTPRequestOperation.h
    │       ├── AFHTTPRequestOperationManager.h -> ./XXX/AFHTTPRequestOperationManager.h
    │       ├── ...
    │       └── UIRefreshControl+AFNetworking.h -> ./XXX/UIRefreshControl+AFNetworking.h
    └── Public
        └── AFNetworking
            ├── AFHTTPRequestOperation.h -> ./XXX/AFHTTPRequestOperation.h
            ├── AFHTTPRequestOperationManager.h -> ./XXX/AFHTTPRequestOperationManager.h
            ├── ...
            └── UIRefreshControl+AFNetworking.h -> ./XXX/UIRefreshControl+AFNetworking.h
```

也正是通过这样的目录结构和软链，CocoaPods 得以在 Header Search Path 中添加如下的参数，使得预编译环节顺利进行。

```
$(inherited)
${PODS_ROOT}/Headers/Private
${PODS_ROOT}/Headers/Private/AFNetworking
${PODS_ROOT}/Headers/Public
${PODS_ROOT}/Headers/Public/AFNetworking
```

虽然这种构建 Search Path 的方式解决了预编译的问题，但在某些项目中，例如多达 400+ 组件的巨型项目中，会造成以下几点问题：

1. 大量的 Header Search Path 路径，会造成编译参数中的 `-I` 选项极速膨胀，在达到一定长度后，甚至会造成无法编译的情况
2. 目前美团的工程中，已经有近 5W 个头文件，这意味着不论是头文件的搜索过程，还是软链的创建过程，都会引起大量的文件 IO 操作，进而会产生一些耗时操作。
3. 编译时间会随着组件数量急剧增长，以美团和大众点评有 400+ 个组件的体量为参考，全源码打包耗时均在 1 小时以上。
4. 基于路径顺序查找头文件的方式有潜在的风险，例如重名头文件的情况，排在后面的头文件永远无法参与编译。
5. 由于 `${PODS_ROOT}/Headers/Private` 路径的存在，让引用其他组件的私有头文件变为了可能。

想解决上述的问题，好一点的情况下，可能会浪费 1 个小时，而不好的情况，就是让有风险的代码上线了，你说工程师会不会因此而感到头疼？

## 3.Header Map 是个啥？

还好 cocoapods-hmap-prebuilt 的出现，让这些问题变成了历史，不过要想理解它为什么能解决这些问题，我们得先理解一下什么是 Header Map。

**Header Map 其实是一组头文件信息映射表！**

为了更直观的理解 Header Map，我们可以在 Build Setting 中开启 Use Header Map 选项，真实的体验一下它。

![img](https://p0.meituan.net/travelcube/cae2af91ae8bb1dfeccb6a0b4bd5180325234.jpg)

然后在 Build Log 里获取相应组件里对应文件的编译命令，并在最后加上 `-v` 参数，来查看其运行的秘密：

```sh
$ clang <list of arguments> -c some-file.m -o some-file.o -v
```

在 console 的输出内容中，我们会发现一段有意思的内容：

![img](https://p0.meituan.net/travelcube/19f1298194085bedb77130ba48f46ac2618179.jpg)

通过上面的图，我们可以看到编译器将寻找头文件的顺序和对应路径展示出来了，而在这些路径中，我们看到了一些陌生的东西，即后缀名为 `.hmap` 的文件，后面还有个括号写着 headermap。

没错！它就是 Header Map 的实体。

此时 Clang 已经在刚才提到的 hmap 文件里塞入了一份头文件名和头文件路径的映射表，不过它是一种二进制格式的文件，为了验证这个的说法，我们可以通过 milend 编写的[hmap 工具](https://github.com/milend/hmap)来查其内容。

在执行相关命令（即 `hmap print`）后，我们可以发现这些 hmap 里保存的信息结构大致如下, 类似于一个 Key-Value 的形式，Key 值是头文件的名称，Value 是头文件的实际物理路径：

![img](https://p1.meituan.net/travelcube/1d733e24d900d936ca238a46fff145ec249110.jpg)

需要注意，映射表的键值内容会随着使用场景产生不同的变化，例如头文件引用是在 `"..."` 的形式下，还是 `<...>` 的形式下，又或是在 Build Phase 里 Header 的配置情况。例如，你将头文件设置为 Public 的时候，在某些 hmap 中，它的 Key 值就为 `PodA/ClassA`，而将其设置为 project 的时候，它的 Key 值可能就是 `ClassA`，而配置这些信息的地方，如下图所示：

![img](https://p0.meituan.net/travelcube/6fba39cda51ee74dc16db440976d63c178956.jpg)

至此我想你应该了解到 Header Map 到底是个什么东西了。

当然这种技术也不是一个什么新鲜事儿，在 Facebook 的 [buck](https://buck.build/) 工具中也提供了类似的东西，只不过文件类型变成了 `HeaderMap.java` 的样子。

此时，我估计你可能并不会对 buck 产生太多的兴趣，而是开始思考上一张图中 Headers 的 Public、Private、Project 到底代表着什么意思，好像很多同学从来没怎么关注过，以及为什么它会影响 hmap 里的内容？

## 4.Public，Private，Project 是个啥？

在 Apple 官方的 [Xcode Help - What are build phases?](https://help.apple.com/xcode/mac/current/#/dev50bab713d) 文档中，我们可以看到如下的一段解释：

> Associates public, private, or project header files with the target. Public and private headers define API intended for use by other clients, and are copied into a product for installation. For example, public and private headers in a framework target are copied into Headers and PrivateHeaders subfolders within a product. Project headers define API used and built by a target, but not copied into a product. This phase can be used once per target.

总的来说，我们可以知道一点，就是 Build Phases - Headers 中提到 Public 和 Private 是指可以供外界使用的头文件，而 Project 中的头文件是不对外使用的，也不会放在最终的产物中。

如果你继续翻阅一些资料，例如 [StackOverflow - Xcode: Copy Headers: Public vs. Private vs. Project?](https://stackoverflow.com/questions/7439192/xcode-copy-headers-public-vs-private-vs-project) 和 [StackOverflow - Understanding Xcode’s Copy Headers phase](https://stackoverflow.com/questions/10584936/understanding-xcodes-copy-headers-phase/18910393#18910393)，你会发现在早期 Xcode Help 的 Project Editor 章节里，有一段名为 Setting the Role of a Header File 的段落，里面详细记载了三个类型的区别。

> **Public**: The interface is finalized and meant to be used by your product’s clients. A public header is included in the product as readable source code without restriction. **Private**: The interface isn’t intended for your clients or it’s in early stages of development. A private header is included in the product, but it’s marked “private”. Thus the symbols are visible to all clients, but clients should understand that they’re not supposed to use them. **Project**: The interface is for use only by implementation files in the current project. A project header is not included in the target, except in object code. The symbols are not visible to clients at all, only to you.

至此，我们应该能够彻底了解了 Public、Private、Project 的区别。简而言之，Public 还是通常意义上的 Public，Private 则代表 In Progress 的含义，至于 Project 才是通常意义上的 Private 含义。

此时，你会不会联想到 CocoaPods 中 Podspec 的 Syntax 里还有 `public_header_files` 和 `private_header_files` 两个字段，它们的真实含义是否和 Xcode 里的概念冲突呢？

这里我们仔细阅读一下[官方文档的解释](https://guides.cocoapods.org/syntax/podspec.html)，尤其是 `private_header_files` 字段。

![img](https://p0.meituan.net/travelcube/7af19a16a81f67ad4db968565ee5ef00192569.png)

我们可以看到，`private_header_files` 在这里的含义是说，它本身是相对于 Public 而言的，这些头文件本义是不希望暴露给用户使用的，而且也不会产生相关文档，但是在构建的时候，会出现在最终产物中，只有既没有被 Public 和 Private 标注的头文件，才会被认为是真正的私有头文件，且不出现在最终的产物里。

看起来，CocoaPods 对于 Public 和 Private 的官方解释是和 Xcode 中的描述一致的，两处的 Private 并非我们通常理解的 Private，它的本意更应该是开发者准备对外开放，但又没完全 Ready 的头文件，更像一个 In Progress 的含义。

这一块是不是让你有点大跌眼镜？那么，在现实世界中，我们是否正确的使用了它们呢？

## 5.为什么用原生的 hmap 不能改善编译速度？

前面我们介绍了 hmap 是什么，以及怎么开启它（启用 Build Setting 中的 Use Header Map 选项），也介绍了一些影响生成 hmap 的因素（Public、Private、Project）。

那是不是我只要开启 Xcode 提供的 Use Header Map 就可以提升编译速度了呢?

很可惜，答案是否定的！

至于原因，我们就从下面的例子开始说起，假设我们有一个基于 CocoaPods 构建的全源码工程项目，它的整体结构如下：

- 首先，Host 和 Pod 是我们的两个 Project，Pods 下的 Target 的产物类型为 Static Library。
- 其次，Host 底下会有一个同名的 Target，而 Pods 目录下会有 n+1 个 Target，其中 n 取决于你依赖的组件数量，而 1 是一个名为 Pods-XXX 的 Target，最后，Pods-XXX 这个 Target 的产物会被 Host 里的 Target 所依赖。

整个结构看起来如下所示：

![img](https://p0.meituan.net/travelcube/c2ace3c2979ce3488b32c980425f3414168365.jpg)

当构建的产物类型为 Static Library 的时候，CocoaPods 在创建头文件产物过程中，它的逻辑大致如下：

- 不论 podspec 里如何设置 `public_header_files` 和 `private_header_files`，相应的头文件都会被设置为 Project 类型。
- 在 `Pods/Headers/Public` 中会保存所有被声明为 `public_header_files` 的头文件。
- 在 `Pods/Headers/Private` 中会保存所有头文件，不论是 `public_header_files` 或者 `private_header_files` 描述到，还是那些未被描述的，这个目录下是当前组件的所有头文件全集。
- 如果 podspec 里未标注 Public 和 Private 的时候，`Pods/Headers/Public` 和 `Pods/Headers/Private` 的内容一样且会包含所有头文件。

正是由于这种机制，会导致一些有意思的问题发生。

- 首先，由于所有头文件都被当做最终产物保留下来，在结合 Header Search Path 里 `Pods/Headers/Private` 路径的存在，我们完全可以引用到其他组件里的私有头文件，例如我只要使用 `#import <SomePod/Private_Header.h>` 的方式，就会命中私有文件的匹配路径。
- 其次，就是在 Static Library 的状况下，一旦我们开启了 Use Header Map，结合组件里所有头文件的类型为 Project 的情况，这个 hmap 里只会包含 `#import "ClassA.h"` 的键值引用，也就是说只有 `#import "ClassA.h"` 的方式才会命中 hmap 的策略，否则都将通过 Header Search Path 寻找其相关路径，例如下图中的 PodB，在其 build 的过程中，Xcode 会为 PodB 生成 5 个 hmap 文件，也就是说这 5 个文件只会在编译 PodB 中使用，其中 PodB 会依赖 PodA 的一些头文件，但由于 PodA 中的头文件都是 Project 类型的，所以其在 hmap 里的 Key 全部为 `ClassA.h` ，也就是说我们只能以 `#import "ClassA.h"` 的方式引入。

![img](https://p0.meituan.net/travelcube/bf6ca4218335d92f794a1eb884bd727d369531.jpg)

而我们也知道，在引用其他组件的时候，通常都会采用 `#import <A/A.h>` 的方式引入。至于为什么会用这种方式，一方面是这种写法会明确头文件的由来，避免问题，另一方面也是这种方式可以让我们在是否开启 clang module 中随意切换。当然，还有一点就是Apple 在 WWDC 里曾经不止一次建议开发者使用这种方式来引入头文件。

接着上面的话题来说，所以说在 Static Library 的情况下且以 `#import <A/A.h>` 这种标准方式引入头文件时，开启 Use Header Map 选项并不会帮我们提升编译速度。

但真的就没有办法使用 Header Map 了么？

## 6.cocoapods-hmap-prebuilt 诞生了

当然，总是有办法解决的，我们完全可以自己动手做一个基于 CocoaPods 规则下的 hmap 文件，正是基于这个想法，美团自研的 cocoapods-hmap-prebuilt 插件诞生了。

它的核心功能并不多，大概有以下几点：

- 借助 CocodPods 处理 Header Search Path 和创建头文件 soft link 的时机，构建了头文件索引表并以此生成 n+1 个 hmap 文件（n 是每个组件自己的 Private Header 信息，1 是所有组件公共的 Public Header 信息）。
- 重写 xcconfig 文件里的 Header Search Path 到对应的 hmap 文件上，一条指向组件自己的 private hmap，一条指向所有组件共用的 public hmap。
- 针对 public hmap 里的重名头文件进行了特殊处理，只允许保存`组件名/头文件名`方式的 Key-Value，排查重名头文件带来的异常行为。
- 将组件自身的 Ues Header Map 功能关闭，减少不必要的文件创建和读取。

听起来可能有点绕，内容也有点多，不过这些你都不用关心，你只需要通过以下 2 个步骤就能将其使用起来：

1. 在 Gemfile 里声明插件。
2. 在 Podfile 里使用插件。

```ruby
// this is part of Gemfile
source 'http://sakgems.sankuai.com/' do
  gem 'cocoapods-hmap-prebuilt'
  gem 'XXX'
  ...
end

// this is part of Podfile
target 'XXX' do
  plugin 'cocoapods-hmap-prebuilt'
  pod 'XXX'
  ...
end
```

除此之外，为了拓展其实用性，我们还提供了头文件补丁（解决重名头文件的定向选取）和环境变量注入（无侵入的在其他系统中使用）的能力，便于其在不同场景下的使用。

## 7.总结

至此，关于 cocoapods-hmap-prebuilt 的介绍就要结束了。

回看整个故事的开始，Header Map 是我在研究 Swift 和 Objective-C 混编过程中发现的一个很小的知识点，而且 Xcode 自身就实现了一套基于 Header Map 的功能，在实际的使用过程中，它的表现并不理想。

但幸运的是，在后续的探索的过程中，我们发现了为什么 Xcode 的 Header Map 没有生效，以及为什么它与 CocoaPods 出现了不兼容的情况，虽然它的原理并不复杂，核心点就是将文件查找和读取等 IO 操作编变成了内存读取操作，但结合实际的业务场景，我们发现它的收益是十分可观的。

或许这是在提醒我们，要永远对技术保持一颗好奇的心！

其实，利用 Clang Module 技术也可以解决本文一开始提到的几个问题，但它并不在这篇文章的讨论范围中，如果你对 Clang Module 或者对 Swift 与 Objective-C 混编感兴趣，欢迎阅读参考文档中的 《从预编译的角度理解 Swift 与 Objective-C 及混编机制》一文，以了解更多的详细信息。

## 8.参考文档

- [Apple - WWDC 2018 Behind the Scenes of the Xcode Build Process](https://developer.apple.com/videos/play/wwdc2018/415/)
- [Apple 的 HeaderMap.cpp 源码](https://opensource.apple.com/source/lldb/lldb-167.2/llvm/tools/clang/lib/Lex/HeaderMap.cpp.auto.html)

## 9.作者

- 思琦，笔名 [SketchK](https://github.com/SketchK)，美团 iOS 工程师，目前负责移动端 CI/CD 方面的工作及平台内 Swift 技术相关的事宜。
- 旭陶，美团 iOS 工程师，目前负责 iOS 端开发提效相关事宜。
- 霜叶，2015 年加入美团，先后从事过 Hybrid 容器、iOS 基础组件、iOS 开发工具链和客户端持续集成门户系统等工作。

原文作者：美团技术团队

原文链接：https://tech.meituan.com/2021/02/25/cocoapods-hmap-prebuilt.html

# 【NO.261】CMake基础 第1节 初识CMake

## 1.介绍

本节展示一个非常基本的hello world的例子。

本节中的文件如下：

```objectivec
A-hello-cmake$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含你希望运行的 CMake 命令

  ```cmake
  # Set the minimum version of CMake that can be used
  # To find the cmake version run
  # $ cmake --version
  cmake_minimum_required(VERSION 3.5)
  
  # Set the project name
  project (hello_cmake)
  
  # Add an executable
  add_executable(hello_cmake main.cpp)
  ```

- [main.cpp-]一个简单的"Hello World"的C++文件。

  ```cc
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
     std::cout << "Hello CMake!" << std::endl;
     return 0;
  }
  ```

## 2.概念

### 2.1 CMakeLists.txt

CMakeLists.txt是存储所有CMake命令的文件。当cmake在文件夹中运行时，它将查找此文件，如果不存在，cmake 将因错误退出。

### 2.2 最小 CMake 版本

使用 CMake 创建项目时，你可以指定支持的最低版本的 CMake。

```cmake
cmake_minimum_required(VERSION 3.5)
```

### 2.3 项目

一个CMake构建文件可以包括一个项目名称，以便在使用多个项目时更容易引用某些变量。

```scss
project (hello_cmake)
```

### 2.4 创建可执行文件

`add_executable()`命令规定，应从指定的源文件构建可执行文件，在此示例中是main.cpp。`add_executable()`函数的第一个参数是要构建的可执行文件的名称，第二个参数是要编译的源文件列表。

```cmake
add_executable(hello_cmake main.cpp)
```

| 注意 | 有些人使用的一种简写方式是使项目名称和可执行文件名称相同。这允许你按如下方式指定CMakeLists.txt。在本例中，project()函数将创建一个值为hello_cmake的变量${PROJECT_NAME}。然后可以将其传递给add_executable()函数以输出‘hello_cmake’可执行文件。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

```cmake
cmake_minimum_required(VERSION 2.6)
project (hello_cmake)
add_executable(${PROJECT_NAME} main.cpp)
```

### 2.5 二进制目录

运行cmake命令的根文件夹或顶级文件夹称为CMAKE_BINARY_DIR，是所有二进制文件的根文件夹。CMake既支持就地构建和生成二进制文件，也支持在源代码外构建和生成二进制文件。

#### 2.5.1 就地构建

就地构建将会在与源代码相同的目录结构中生成所有临时文件。这意味着所有的Makefile和目标文件都散布在你的普通代码中。要创建就地构建目标，请在根目录中运行cmake命令。例如：

```shell
A-hello-cmake$ cmake .
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/A-hello-cmake

A-hello-cmake$ tree
.
├── CMakeCache.txt
├── CMakeFiles
│   ├── 2.8.12.2
│   │   ├── CMakeCCompiler.cmake
│   │   ├── CMakeCXXCompiler.cmake
│   │   ├── CMakeDetermineCompilerABI_C.bin
│   │   ├── CMakeDetermineCompilerABI_CXX.bin
│   │   ├── CMakeSystem.cmake
│   │   ├── CompilerIdC
│   │   │   ├── a.out
│   │   │   └── CMakeCCompilerId.c
│   │   └── CompilerIdCXX
│   │       ├── a.out
│   │       └── CMakeCXXCompilerId.cpp
│   ├── cmake.check_cache
│   ├── CMakeDirectoryInformation.cmake
│   ├── CMakeOutput.log
│   ├── CMakeTmp
│   ├── hello_cmake.dir
│   │   ├── build.make
│   │   ├── cmake_clean.cmake
│   │   ├── DependInfo.cmake
│   │   ├── depend.make
│   │   ├── flags.make
│   │   ├── link.txt
│   │   └── progress.make
│   ├── Makefile2
│   ├── Makefile.cmake
│   ├── progress.marks
│   └── TargetDirectories.txt
├── cmake_install.cmake
├── CMakeLists.txt
├── main.cpp
├── Makefile
```

#### 2.5.2 源外构建

使用源外构建，你可以创建单个生成文件夹，该文件夹可以位于文件系统的任何位置。所有临时构建的目标文件都位于此目录中，以保持源码目录结构的整洁。要进行源外构建，请运行build文件夹中的cmake命令，并将其指向根CMakeLists.txt文件所在的目录。使用源外构建时，如果你想从头开始重新创建cmake环境，只需删除构建目录，然后重新运行cmake。

举个例子:

```shell
A-hello-cmake$ mkdir build

A-hello-cmake$ cd build/

A-hello-cmake/build$ make ..
make: Nothing to be done for `..'.
matrim@freyr:~/workspace/cmake-examples/01-basic/A-hello-cmake/build$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/A-hello-cmake/build

A-hello-cmake/build$ cd ..

A-hello-cmake$ tree
.
├── build
│   ├── CMakeCache.txt
│   ├── CMakeFiles
│   │   ├── 2.8.12.2
│   │   │   ├── CMakeCCompiler.cmake
│   │   │   ├── CMakeCXXCompiler.cmake
│   │   │   ├── CMakeDetermineCompilerABI_C.bin
│   │   │   ├── CMakeDetermineCompilerABI_CXX.bin
│   │   │   ├── CMakeSystem.cmake
│   │   │   ├── CompilerIdC
│   │   │   │   ├── a.out
│   │   │   │   └── CMakeCCompilerId.c
│   │   │   └── CompilerIdCXX
│   │   │       ├── a.out
│   │   │       └── CMakeCXXCompilerId.cpp
│   │   ├── cmake.check_cache
│   │   ├── CMakeDirectoryInformation.cmake
│   │   ├── CMakeOutput.log
│   │   ├── CMakeTmp
│   │   ├── hello_cmake.dir
│   │   │   ├── build.make
│   │   │   ├── cmake_clean.cmake
│   │   │   ├── DependInfo.cmake
│   │   │   ├── depend.make
│   │   │   ├── flags.make
│   │   │   ├── link.txt
│   │   │   └── progress.make
│   │   ├── Makefile2
│   │   ├── Makefile.cmake
│   │   ├── progress.marks
│   │   └── TargetDirectories.txt
│   ├── cmake_install.cmake
│   └── Makefile
├── CMakeLists.txt
├── main.cpp
```

在本教程的所有例子中，都将使用源外构建。

## 3.构建示例

以下是构建此示例的输出：

```shell
$ mkdir build

$ cd build

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /workspace/cmake-examples/01-basic/hello_cmake/build

$ make
Scanning dependencies of target hello_cmake
[100%] Building CXX object CMakeFiles/hello_cmake.dir/hello_cmake.cpp.o
Linking CXX executable hello_cmake
[100%] Built target hello_cmake

$ ./hello_cmake
Hello CMake!
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069300.html

# 【NO.262】CMake基础 第2节 分离编译

## 1.介绍

展示一个hello world示例，它使用不同的文件夹来存储源文件和头文件。

本教程中的文件包括：

```ruby
B-hello-headers$ tree
.
├── CMakeLists.txt
├── include
│   └── Hello.h
└── src
    ├── Hello.cpp
    └── main.cpp
```

- [CMakeLists.txt] - 包含你希望运行的 CMake 命令。

  ```cmake
  # Set the minimum version of CMake that can be used
  # To find the cmake version run
  # $ cmake --version
  cmake_minimum_required(VERSION 3.5)
  
  # Set the project name
  project (hello_headers)
  
  # Create a sources variable with a link to all cpp files to compile
  set(SOURCES
      src/Hello.cpp
      src/main.cpp
  )
  
  # Add an executable with the above sources
  add_executable(hello_headers ${SOURCES})
  
  # Set the directories that should be included in the build command for this target
  # when running g++ these will be included as -I/directory/path/
  target_include_directories(hello_headers
      PRIVATE 
          ${PROJECT_SOURCE_DIR}/include
  )
  ```

- [include/Hello.h] - 要包含的标题文件。

  ```cpp
  #ifndef __HELLO_H__
  #define __HELLO_H__
  
  class Hello
  {
  public:
      void print();
  };
  
  #endif
  ```

- [src/Hello.cpp] - 要编译的源文件。

  ```cpp
  #include <iostream>
  
  #include "Hello.h"
  
  void Hello::print()
  {
      std::cout << "Hello Headers!" << std::endl;
  }
  ```

- [src/main.cpp] - 主源文件。

  ```cpp
  #include "Hello.h"
  
  int main(int argc, char *argv[])
  {
      Hello hi;
      hi.print();
      return 0;
  }
  ```

## 2.概念

### 2.1 目录路径

CMake 语法指定了许多变量，这些变量可用于帮助在项目或源代码树中找到有用的目录。其中一些包括：

| 变量                     | 信息                                            |
| ------------------------ | ----------------------------------------------- |
| CMAKE_SOURCE_DIR         | 根源目录                                        |
| CMAKE_CURRENT_SOURCE_DIR | 如果使用子项目和目录，则为当前源目录。          |
| PROJECT_SOURCE_DIR       | 当前 cmake 项目的源目录。                       |
| CMAKE_BINARY_DIR         | 根二进制文件生成目录。这是运行cmake命令的目录。 |
| CMAKE_CURRENT_BINARY_DIR | 你当前所处的生成目录。                          |
| PROJECT_BINARY_DIR       | 当前项目的生成目录。                            |

### 2.2 源文件变量

创建包含源码文件的变量可使你更清楚地了解这些文件，并轻松将其添加到多个命令中，例如，add_executable()功能。

```cmake
# Create a sources variable with a link to all cpp files to compile
set(SOURCES
    src/Hello.cpp
    src/main.cpp
)

add_executable(${PROJECT_NAME} ${SOURCES})
```

| Note | 另一种替代方案是使用 GLOB 命令使用通配符模式匹配查找文件。 |
| ---- | ---------------------------------------------------------- |
|      |                                                            |

```cmake
file(GLOB SOURCES "src/*.cpp")
```

| Tip  | 对于现代的CMake，不建议对源代码使用变量。相反，通常直接在add_xxx函数中声明源代码。这对于GLOB命令尤其重要，如果你添加一个新的源代码文件，这些命令可能并不总是显示正确的结果。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 2.3 头文件目录

当你有不同的头文件目录时，可以使用`target_include_directories()`函数让编译器知道它们编译此目标时，会将这些目录添加到带有`-i`标志的编译指令中，例如`-i/directory/path`

```cmake
target_include_directories(target
    PRIVATE
        ${PROJECT_SOURCE_DIR}/include
)
```

PRIVATE标识符指定include的范围，这对库很重要，这些内容将在下一个示例中进行说明。有关该功能的更多详细信息，请访问[这里](https://cmake.org/cmake/help/v3.0/command/target_include_directories.html)

## 3.构建示例

### 3.1 标准输出

构建示例的标准输出内容如下所示：

```shell
$ mkdir build

$ cd build

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/hello_headers/build

$ make
Scanning dependencies of target hello_headers
[ 50%] Building CXX object CMakeFiles/hello_headers.dir/src/Hello.cpp.o
[100%] Building CXX object CMakeFiles/hello_headers.dir/src/main.cpp.o
Linking CXX executable hello_headers
[100%] Built target hello_headers

$ ./hello_headers
Hello Headers!
```

### 3.2 详细输出

在前面的示例中，当运行make命令时，输出仅显示构建的状态。要查看用于调试目的的完整输出，可以在运行make时添加VERBOSE=1标志。

VERBOSE 输出如下所示。可以看到，include目录已经被添加到了编译命令中。

```shell
$ make clean

$ make VERBOSE=1
/usr/bin/cmake -H/home/matrim/workspace/cmake-examples/01-basic/hello_headers -B/home/matrim/workspace/cmake-examples/01-basic/hello_headers/build --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/hello_headers/build/CMakeFiles /home/matrim/workspace/cmake-examples/01-basic/hello_headers/build/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/hello_headers/build'
make -f CMakeFiles/hello_headers.dir/build.make CMakeFiles/hello_headers.dir/depend
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/hello_headers/build'
cd /home/matrim/workspace/cmake-examples/01-basic/hello_headers/build && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /home/matrim/workspace/cmake-examples/01-basic/hello_headers /home/matrim/workspace/cmake-examples/01-basic/hello_headers /home/matrim/workspace/cmake-examples/01-basic/hello_headers/build /home/matrim/workspace/cmake-examples/01-basic/hello_headers/build /home/matrim/workspace/cmake-examples/01-basic/hello_headers/build/CMakeFiles/hello_headers.dir/DependInfo.cmake --color=
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/hello_headers/build'
make -f CMakeFiles/hello_headers.dir/build.make CMakeFiles/hello_headers.dir/build
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/hello_headers/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/hello_headers/build/CMakeFiles 1
[ 50%] Building CXX object CMakeFiles/hello_headers.dir/src/Hello.cpp.o
/usr/bin/c++    -I/home/matrim/workspace/cmake-examples/01-basic/hello_headers/include    -o CMakeFiles/hello_headers.dir/src/Hello.cpp.o -c /home/matrim/workspace/cmake-examples/01-basic/hello_headers/src/Hello.cpp
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/hello_headers/build/CMakeFiles 2
[100%] Building CXX object CMakeFiles/hello_headers.dir/src/main.cpp.o
/usr/bin/c++    -I/home/matrim/workspace/cmake-examples/01-basic/hello_headers/include    -o CMakeFiles/hello_headers.dir/src/main.cpp.o -c /home/matrim/workspace/cmake-examples/01-basic/hello_headers/src/main.cpp
Linking CXX executable hello_headers
/usr/bin/cmake -E cmake_link_script CMakeFiles/hello_headers.dir/link.txt --verbose=1
/usr/bin/c++       CMakeFiles/hello_headers.dir/src/Hello.cpp.o CMakeFiles/hello_headers.dir/src/main.cpp.o  -o hello_headers -rdynamic
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/hello_headers/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/hello_headers/build/CMakeFiles  1 2
[100%] Built target hello_headers
make[1]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/hello_headers/build'
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/hello_headers/build/CMakeFiles 0
```



原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069309.html

# 【NO.263】CMake基础 第3节 静态库

## 1.介绍

继续展示一个hello world示例，它首先创建并链接一个静态库。这是一个简化示例，这里的库和二进制文件在同一个文件夹中。通常，这些将会被放到子项目中，这些内容将会在以后描述。

本教程中的文件如下：

```objectivec
$ tree
.
├── CMakeLists.txt
├── include
│   └── static
│       └── Hello.h
└── src
    ├── Hello.cpp
    └── main.cpp
```

- [CMakeLists.txt] - 包含你希望运行的 CMake 命令

  ```cmake
  cmake_minimum_required(VERSION 3.5)
  
  project(hello_library)
  
  ############################################################
  # Create a library
  ############################################################
  
  #Generate the static library from the library sources
  add_library(hello_library STATIC 
      src/Hello.cpp
  )
  
  target_include_directories(hello_library
      PUBLIC 
          ${PROJECT_SOURCE_DIR}/include
  )
  
  
  ############################################################
  # Create an executable
  ############################################################
  
  # Add an executable with the above sources
  add_executable(hello_binary 
      src/main.cpp
  )
  
  # link the new hello_library target with the hello_binary target
  target_link_libraries( hello_binary
      PRIVATE 
          hello_library
  )
  ```

- [include/static/Hello.h] - 要包含的头文件

  ```c++
  #ifndef __HELLO_H__
  #define __HELLO_H__
  
  class Hello
  {
  public:
      void print();
  };
  
  #endif
  ```

- [src/Hello.cpp] - 要编译的源文件

  ```c++
  #include <iostream>
  
  #include "static/Hello.h"
  
  void Hello::print()
  {
      std::cout << "Hello Static Library!" << std::endl;
  }
  ```

- [src/main.cpp] - 主源文件

  ```c++
  #include "static/Hello.h"
  
  int main(int argc, char *argv[])
  {
      Hello hi;
      hi.print();
      return 0;
  }
  ```

## 2.概念

### 2.1 添加静态库

`add_library()`功能用于从某些源文件创建库。调用方式如下：

```cmake
add_library(hello_library STATIC
    src/Hello.cpp
)
```

此命令将使用add_library()调用中的源代码创建一个名为`libhello_library.a`的静态库

| 注意 | 如上一节所述，我们按照现代 CMake 的建议，将源文件直接传递给add_library调用。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 2.2 添加头文件目录

在本例中，我们使用`target_include_directory()`函数将include目录包含在库中，并将范围设置为PUBLIC。

```cmake
target_include_directories(hello_library
    PUBLIC
        ${PROJECT_SOURCE_DIR}/include
)
```

这将导致包含的目录在以下位置使用：

- 在编译该库时
- 在编译链接至该库的任何其他目标时。

作用域的含义是：

- PRIVATE - 将目录添加到此目标的include目录中
- INTERFACE - 将该目录添加到任何链接到此库的目标的include目录中（不包括自己）。
- PUBLIC - 它包含在此库中，也包含在链接此库的任何目标中。

| 提示 | 对于公共头文件，让你的include文件夹使用子目录“命名空间”通常是个好主意。传递给target_include_directories的目录将是你的Include目录树的根目录，并且你的C++文件应该包含从那里到你要使用的头文件的路径。在本例中，你可以看到我们按如下方式执行操作。使用此方法意味着，在项目中使用多个库时，头文件名冲突的可能性较小。比如： |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

```c++
#include "static/Hello.h"
```

### 2.3 链接库

在创建使用库的可执行文件时，你必须告诉编译器有关库的信息。这可以使用target_link_library()函数来完成。

```cmake
add_executable(hello_binary
    src/main.cpp
)

target_link_libraries( hello_binary
    PRIVATE
        hello_library
)
```

这告诉CMake在链接时将hello_library与hello_binary可执行文件链接起来。它还将从hello_library传递任何具有PUBLIC或INTERFACE作用范围的include目录到hello_binary。

编译器调用它的一个示例是:

```shell
/usr/bin/c++ CMakeFiles/hello_binary.dir/src/main.cpp.o -o hello_binary -rdynamic libhello_library.a
```

## 3.构建示例

```shell
$ mkdir build

$ cd build

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/C-static-library/build

$ make
Scanning dependencies of target hello_library
[ 50%] Building CXX object CMakeFiles/hello_library.dir/src/Hello.cpp.o
Linking CXX static library libhello_library.a
[ 50%] Built target hello_library
Scanning dependencies of target hello_binary
[100%] Building CXX object CMakeFiles/hello_binary.dir/src/main.cpp.o
Linking CXX executable hello_binary
[100%] Built target hello_binary

$ ls
CMakeCache.txt  CMakeFiles  cmake_install.cmake  hello_binary  libhello_library.a  Makefile

$ ./hello_binary
Hello Static Library!
```



原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069323.html

# 【NO.264】CMake基础 第4节 动态库

## 1.介绍

继续展示一个hello world示例，它将首先创建并链接一个共享库。

这里还显示了如何创建[别名目标](https://cmake.org/cmake/help/v3.0/manual/cmake-buildsystem.7.html#alias-targets)

本教程中的文件如下：

```shell
$ tree
.
├── CMakeLists.txt
├── include
│   └── shared
│       └── Hello.h
└── src
    ├── Hello.cpp
    └── main.cpp
```

- [CMakeLists.txt] - 包含要运行的 CMake 命令

  ```cmake
  cmake_minimum_required(VERSION 3.5)
  
  project(hello_library)
  
  ############################################################
  # Create a library
  ############################################################
  
  #Generate the shared library from the library sources
  add_library(hello_library SHARED 
      src/Hello.cpp
  )
  add_library(hello::library ALIAS hello_library)
  
  target_include_directories(hello_library
      PUBLIC 
          ${PROJECT_SOURCE_DIR}/include
  )
  
  ############################################################
  # Create an executable
  ############################################################
  
  # Add an executable with the above sources
  add_executable(hello_binary
      src/main.cpp
  )
  
  # link the new hello_library target with the hello_binary target
  target_link_libraries( hello_binary
      PRIVATE 
          hello::library
  )
  ```

- [include/shared/Hello.h] - 要包含的头文件

  ```cpp
  #ifndef __HELLO_H__
  #define __HELLO_H__
  
  class Hello
  {
  public:
      void print();
  };
  
  #endif
  ```

- [src/Hello.cpp] - 要编译的源文件

  ```cpp
  #include <iostream>
  
  #include "shared/Hello.h"
  
  void Hello::print()
  {
      std::cout << "Hello Shared Library!" << std::endl;
  }
  ```

- [src/main.cpp] - 具有main的源文件

  ```cpp
  #include "shared/Hello.h"
  
  int main(int argc, char *argv[])
  {
      Hello hi;
      hi.print();
      return 0;
  }
  ```

## 2.概念

### 2.1 添加共享库

与前面的静态库示例一样，add_library()函数也用于从一些源文件创建共享库。它的用法如下：

```cmake
add_library(hello_library SHARED
    src/Hello.cpp
)
```

这将使用传递给add_library()函数的源码文件创建一个名为libhello_library.so的共享库。

### 2.2 别名目标

顾名思义，别名目标是目标的替代名称，可以在只读上下文中替代真实的目标名称。

```cmake
add_library(hello::library ALIAS hello_library)
```

ALIAS类似于“同义词”。ALIAS目标只是原始目标的另一个名称。因此ALIAS目标的要求是不可修改的——您无法调整其属性、安装它等。

如下所示，这允许你在将目标链接到其他目标时使用别名引用该目标。

### 2.3 链接共享库

链接共享库与链接静态库相同。创建可执行文件时，请使用`target_link_library()`函数指向库。

```cmake
add_executable(hello_binary
    src/main.cpp
)

target_link_libraries(hello_binary
    PRIVATE
        hello::library
)
```

这告诉CMake使用别名目标名称将hello_library链接到hello_binary可执行文件。

链接器调用它的一个示例是：

```shell
/usr/bin/c++ CMakeFiles/hello_binary.dir/src/main.cpp.o -o hello_binary -rdynamic libhello_library.so -Wl,-rpath,/home/matrim/workspace/cmake-examples/01-basic/D-shared-library/build
```

## 3.构建示例

```shell
$ mkdir build

$ cd build

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/D-shared-library/build

$ make
Scanning dependencies of target hello_library
[ 50%] Building CXX object CMakeFiles/hello_library.dir/src/Hello.cpp.o
Linking CXX shared library libhello_library.so
[ 50%] Built target hello_library
Scanning dependencies of target hello_binary
[100%] Building CXX object CMakeFiles/hello_binary.dir/src/main.cpp.o
Linking CXX executable hello_binary
[100%] Built target hello_binary

$ ls
CMakeCache.txt  CMakeFiles  cmake_install.cmake  hello_binary  libhello_library.so  Makefile

$ ./hello_binary
Hello Shared Library!
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069328.html

# 【NO.265】CMake基础 第5节 安装项目 

## 1.介绍

此示例说明如何生成make install目标以在系统上安装文件和二进制文件。这基于前面的共享库示例。

本教程中的文件如下：

```ruby
$ tree
.
├── cmake-examples.conf
├── CMakeLists.txt
├── include
│   └── installing
│       └── Hello.h
├── README.adoc
└── src
    ├── Hello.cpp
    └── main.cpp
```

- [CMakeLists.txt] - 包含你希望运行的 CMake 命令

  ```cmake
  cmake_minimum_required(VERSION 3.5)
  
  project(cmake_examples_install)
  
  ############################################################
  # Create a library
  ############################################################
  
  #Generate the shared library from the library sources
  add_library(cmake_examples_inst SHARED
      src/Hello.cpp
  )
  
  target_include_directories(cmake_examples_inst
      PUBLIC 
          ${PROJECT_SOURCE_DIR}/include
  )
  
  ############################################################
  # Create an executable
  ############################################################
  
  # Add an executable with the above sources
  add_executable(cmake_examples_inst_bin
      src/main.cpp
  )
  
  # link the new hello_library target with the hello_binary target
  target_link_libraries( cmake_examples_inst_bin
      PRIVATE 
          cmake_examples_inst
  )
  
  ############################################################
  # Install
  ############################################################
  
  # Binaries
  install (TARGETS cmake_examples_inst_bin
      DESTINATION bin)
  
  # Library
  # Note: may not work on windows
  install (TARGETS cmake_examples_inst
      LIBRARY DESTINATION lib)
  
  # Header files
  install(DIRECTORY ${PROJECT_SOURCE_DIR}/include/ 
      DESTINATION include)
  
  # Config
  install (FILES cmake-examples.conf
      DESTINATION etc)
  ```

- [cmake-examples.conf] - 示例配置文件

  ```cmake
  # Sample configuration file that could be installed
  ```

- [include/installing/Hello.h] - 要包含的标题文件

  ```cpp
  #ifndef __HELLO_H__
  #define __HELLO_H__
  
  class Hello
  {
  public:
      void print();
  };
  
  #endif
  ```

- [src/Hello.cpp] - 要编译的源文件

  ```cpp
  #include <iostream>
  
  #include "installing/Hello.h"
  
  void Hello::print()
  {
      std::cout << "Hello Install!" << std::endl;
  }
  ```

- [src/main.cpp] - 主源文件

  ```cpp
  #include "installing/Hello.h"
  
  int main(int argc, char *argv[])
  {
      Hello hi;
      hi.print();
      return 0;
  }
  ```

## 2.概念

### 2.1 安装

CMake提供了添加make install目标的功能，以允许用户安装二进制文件、库和其他文件。基本安装位置由变量CMAKE_INSTALL_PREFIX控制，该变量可以使用ccmake或通过使用`cmake .. -DCMAKE_INSTALL_PREFIX=/install/location`调用cmake来设置。

安装的文件由install()函数控制。

```cmake
install (TARGETS cmake_examples_inst_bin
    DESTINATION bin)
```

将目标cmake_examples_inst_bin生成的二进制文件安装到目标目录`${CMAKE_INSTALL_PREFIX}/bin`中。

```cmake
install (TARGETS cmake_examples_inst
    LIBRARY DESTINATION lib)
```

将目标cmake_examples_inst生成的共享库安装到目标目录`${CMAKE_INSTALL_PREFIX}/lib`中。

| 注意 | 这在Windows上可能不起作用。在具有DLL目标的平台上，可能需要添加以下内容。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

```cmake
    install (TARGETS cmake_examples_inst
        LIBRARY DESTINATION lib
        RUNTIME DESTINATION bin)
install(DIRECTORY ${PROJECT_SOURCE_DIR}/include/
    DESTINATION include)
```

将针对cmake_examples_inst库进行开发的头文件安装到`${CMAKE_INSTALL_PREFIX}/include`目录中。

```cmake
install (FILES cmake-examples.conf
    DESTINATION etc)
```

将配置文件安装到目标`${CMAKE_INSTALL_PREFIX}/etc`。

在运行make install之后，CMake会生成一个install_mark.txt文件，其中包含所有已安装文件的详细信息。

| 注意 | 如果你以root身份运行make install命令，则install_mark.txt文件将归root所有。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

## 3.构建示例

```shell
$ mkdir build

$ cd build/

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/E-installing/build

$ make
Scanning dependencies of target cmake_examples_inst
[ 50%] Building CXX object CMakeFiles/cmake_examples_inst.dir/src/Hello.cpp.o
Linking CXX shared library libcmake_examples_inst.so
[ 50%] Built target cmake_examples_inst
Scanning dependencies of target cmake_examples_inst_bin
[100%] Building CXX object CMakeFiles/cmake_examples_inst_bin.dir/src/main.cpp.o
Linking CXX executable cmake_examples_inst_bin
[100%] Built target cmake_examples_inst_bin

$ sudo make install
[sudo] password for matrim:
[ 50%] Built target cmake_examples_inst
[100%] Built target cmake_examples_inst_bin
Install the project...
-- Install configuration: ""
-- Installing: /usr/local/bin/cmake_examples_inst_bin
-- Removed runtime path from "/usr/local/bin/cmake_examples_inst_bin"
-- Installing: /usr/local/lib/libcmake_examples_inst.so
-- Installing: /usr/local/etc/cmake-examples.conf

$ cat install_manifest.txt
/usr/local/bin/cmake_examples_inst_bin
/usr/local/lib/libcmake_examples_inst.so
/usr/local/etc/cmake-examples.conf

$ ls /usr/local/bin/
cmake_examples_inst_bin

$ ls /usr/local/lib
libcmake_examples_inst.so

$ ls /usr/local/etc/
cmake-examples.conf

$ LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib cmake_examples_inst_bin
Hello Install!
```

| Note | 如果/usr/local/lib不在库路径中，你可能需要在运行二进制文件之前将其添加到路径中。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

## 4.注意事项

### 4.1 覆盖默认安装位置

如前所述，默认安装位置是从CMAKE_INSTALL_PERFIX设置的，默认为`/usr/local/`

如果你想为所有用户更改这个默认位置，可以在添加任何二进制文件或库之前将以下代码添加到你的顶端CMakeLists.txt中。

```cmake
if( CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT )
  message(STATUS "Setting default CMAKE_INSTALL_PREFIX path to ${CMAKE_BINARY_DIR}/install")
  set(CMAKE_INSTALL_PREFIX "${CMAKE_BINARY_DIR}/install" CACHE STRING "The path to use for make install" FORCE)
endif()
```

此示例将默认安装位置设置为你的构建目录下。

### 4.2 目标文件夹

如果你希望通过进行安装来确认是否包含所有文件，则make install目标支持DESTDIR参数。

```shell
make install DESTDIR=/tmp/stage
```

这将为你的所有安装文件创建安装路径`${DESTDIR}/${CMAKE_INSTALL_PREFIX}`。在此示例中，它将在路径`/tmp/stage/usr/local`下安装所有文件

```bash
$ tree /tmp/stage
/tmp/stage
└── usr
    └── local
        ├── bin
        │   └── cmake_examples_inst_bin
        ├── etc
        │   └── cmake-examples.conf
        └── lib
            └── libcmake_examples_inst.so
```

### 4.3 卸载

默认情况下，CMake不会添加`make uninstall`目标。有关如何生成卸载目标的详细信息，请参阅此[常见问题解答](https://cmake.org/Wiki/CMake_FAQ#Can_I_do_.22make_uninstall.22_with_CMake.3F)

要从本例中删除文件的简单方法，可以使用：

```shell
sudo xargs rm < install_manifest.txt
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069335.html

# 【NO.266】CMake基础 第6节 生成类型

## 1.介绍

CMake有许多内置的构建配置，可用于编译你的项目。它们指定优化级别以及调试信息是否包含在二进制文件中。

提供的级别包括：

- Release - 将标志`-O3 -DNDEBUG`添加到编译器
- Debug - 添加标志`-g`
- MinSizeRel - 添加标志`-Os -DNDEBUG`
- RelWithDebInfo - 添加标志`-O2 -g -DNDEBUG`

本教程中的文件如下：

```objectivec
$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含你希望运行的 CMake 命令

  ```cmake
  # Set the minimum version of CMake that can be used
  # To find the cmake version run
  # $ cmake --version
  cmake_minimum_required(VERSION 3.5)
  
  # Set a default build type if none was specified
  if(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)
    message("Setting build type to 'RelWithDebInfo' as none was specified.")
    set(CMAKE_BUILD_TYPE RelWithDebInfo CACHE STRING "Choose the type of build." FORCE)
    # Set the possible values of build type for cmake-gui
    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS "Debug" "Release"
      "MinSizeRel" "RelWithDebInfo")
  endif()
  
  # Set the project name
  project (build_type)
  
  # Add an executable
  add_executable(cmake_examples_build_type main.cpp)
  ```

- [main.cpp] - 主源文件

  ```cpp
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
     std::cout << "Hello Build Type!" << std::endl;
     return 0;
  }
  ```

## 2.概念

### 2.1 设置生成类型

可以使用以下方法设置生成类型。

- 使用GUI工具，如ccmake/cmake-gui
- 通过命令行传递到cmake：

```shell
cmake .. -DCMAKE_BUILD_TYPE=Release
```

### 2.2 设置默认生成类型

CMake提供的默认构建类型是不包含用于优化的编译器标志。对于某些项目，你可能希望设置默认生成类型，以便不必记住设置它。为此，你可以将以下代码添加到顶级CMakeLists.txt中。

```cmake
if(NOT CMAKE_BUILD_TYPE AND NOT CMAKE_CONFIGURATION_TYPES)
  message("Setting build type to 'RelWithDebInfo' as none was specified.")
  set(CMAKE_BUILD_TYPE RelWithDebInfo CACHE STRING "Choose the type of build." FORCE)
  # Set the possible values of build type for cmake-gui
  set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS "Debug" "Release"
    "MinSizeRel" "RelWithDebInfo")
endif()
```

### 2.3 set_property

在指定域中设置一个命名属性

```cmake
set_property(  <GLOBAL                            |
                DIRECTORY [dir]                   |
                TARGET    [target1 [target2 ...]] |
                SOURCE    [src1 [src2 ...]]       |
                TEST      [test1 [test2 ...]]     |
                CACHE     [entry1 [entry2 ...]]>
               [APPEND][APPEND_STRING]
               PROPERTY <name>[value1 [value2 ...]])
```

在某个域中对零个或多个对象设置一个属性。第一个参数决定该属性设置所在的域。它必须为下面中的其中之一：

GLOBAL域是唯一的，并且不接特殊的任何名字。

DIRECTORY域默认为当前目录，但也可以用全路径或相对路径指定其他的目录（前提是该目录已经被CMake处理）。

TARGET域可命名零或多个已经存在的目标。

SOURCE域可命名零或多个源文件。注意：源文件属性只对在相同目录下的目标是可见的(CMakeLists.txt)。

TEST域可命名零或多个已存在的测试。

CACHE域必须命名零或多个已存在条目的cache.

必选项PROPERTY后面紧跟着要设置的属性的名字。其他的参数用于构建以分号隔开的列表形式的属性值。如果指定了APPEND选项，则指定的列表将会追加到任何已存在的属性值当中。如果指定了APPEND_STRING选项，则会将值作为字符串追加到任何已存在的属性值。

### 2.4 get_property

从作用域中的一个对象获取一个属性。第一个参数指定存储结果的变量。第二个参数确定从中获取属性的范围。它必须是以下之一：

```cmake
get_property(  <variable>
               <GLOBAL             |
                DIRECTORY [dir]    |
                TARGET    <target> |
                SOURCE    <source> |
                TEST      <test>   |
                CACHE     <entry>  |
                VARIABLE>
               PROPERTY <name>
               [SET | DEFINED |BRIEF_DOCS | FULL_DOCS])
```

相关域的说明与set_property意义相同。

必选项PROPERTY后面紧跟着要获取的属性的名字。如果指定了SET选项，则变量会被设置为一个布尔值，表明该属性是否已设置。如果指定了DEFINED选项，则变量也会被设置为一个布尔值，表明该属性是否已定义（如通过define_property）。如果定义了BRIEF_DOCS或FULL_DOCS选项，则该变量被设置为一个字符串，包含了对请求的属性的文档。如果该属性没有相关文档，则会返回NOTFOUND。

## 3.构建示例

```shell
$ mkdir build

$ cd build/

$ cmake ..
Setting build type to 'RelWithDebInfo' as none was specified.
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build

$ make VERBOSE=1
/usr/bin/cmake -H/home/matrim/workspace/cmake-examples/01-basic/F-build-type -B/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
make -f CMakeFiles/cmake_examples_build_type.dir/build.make CMakeFiles/cmake_examples_build_type.dir/depend
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
cd /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /home/matrim/workspace/cmake-examples/01-basic/F-build-type /home/matrim/workspace/cmake-examples/01-basic/F-build-type /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/cmake_examples_build_type.dir/DependInfo.cmake --color=
Dependee "/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/cmake_examples_build_type.dir/DependInfo.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/cmake_examples_build_type.dir/depend.internal".
Dependee "/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/CMakeDirectoryInformation.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/cmake_examples_build_type.dir/depend.internal".
Scanning dependencies of target cmake_examples_build_type
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
make -f CMakeFiles/cmake_examples_build_type.dir/build.make CMakeFiles/cmake_examples_build_type.dir/build
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles 1
[100%] Building CXX object CMakeFiles/cmake_examples_build_type.dir/main.cpp.o
/usr/bin/c++    -O2 -g -DNDEBUG   -o CMakeFiles/cmake_examples_build_type.dir/main.cpp.o -c /home/matrim/workspace/cmake-examples/01-basic/F-build-type/main.cpp
Linking CXX executable cmake_examples_build_type
/usr/bin/cmake -E cmake_link_script CMakeFiles/cmake_examples_build_type.dir/link.txt --verbose=1
/usr/bin/c++   -O2 -g -DNDEBUG    CMakeFiles/cmake_examples_build_type.dir/main.cpp.o  -o cmake_examples_build_type -rdynamic
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles  1
[100%] Built target cmake_examples_build_type
make[1]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles 0$ mkdir build
$ cd build/
/build$ cmake ..
Setting build type to 'RelWithDebInfo' as none was specified.
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build
/build$ make VERBOSE=1
/usr/bin/cmake -H/home/matrim/workspace/cmake-examples/01-basic/F-build-type -B/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
make -f CMakeFiles/cmake_examples_build_type.dir/build.make CMakeFiles/cmake_examples_build_type.dir/depend
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
cd /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /home/matrim/workspace/cmake-examples/01-basic/F-build-type /home/matrim/workspace/cmake-examples/01-basic/F-build-type /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/cmake_examples_build_type.dir/DependInfo.cmake --color=
Dependee "/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/cmake_examples_build_type.dir/DependInfo.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/cmake_examples_build_type.dir/depend.internal".
Dependee "/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/CMakeDirectoryInformation.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles/cmake_examples_build_type.dir/depend.internal".
Scanning dependencies of target cmake_examples_build_type
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
make -f CMakeFiles/cmake_examples_build_type.dir/build.make CMakeFiles/cmake_examples_build_type.dir/build
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles 1
[100%] Building CXX object CMakeFiles/cmake_examples_build_type.dir/main.cpp.o
/usr/bin/c++    -O2 -g -DNDEBUG   -o CMakeFiles/cmake_examples_build_type.dir/main.cpp.o -c /home/matrim/workspace/cmake-examples/01-basic/F-build-type/main.cpp
Linking CXX executable cmake_examples_build_type
/usr/bin/cmake -E cmake_link_script CMakeFiles/cmake_examples_build_type.dir/link.txt --verbose=1
/usr/bin/c++   -O2 -g -DNDEBUG    CMakeFiles/cmake_examples_build_type.dir/main.cpp.o  -o cmake_examples_build_type -rdynamic
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles  1
[100%] Built target cmake_examples_build_type
make[1]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/F-build-type/build'
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/F-build-type/build/CMakeFiles 0
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069657.html

# 【NO.267】CMake基础 第7节 编译标志

## 1.引言

CMake支持以多种不同方式设置编译标志：

- 使用target_compile_definitions（）函数
- 使用CMAKE_C_FLAGS和CMAKE_CXX_FLAGS变量。

本教程中的文件如下：

```shell
$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令

  ```cmake
  cmake_minimum_required(VERSION 3.5)
  
  # Set a default C++ compile flag
  set (CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -DEX2" CACHE STRING "Set C++ Compiler Flags" FORCE)
  
  # Set the project name
  project (compile_flags)
  
  # Add an executable
  add_executable(cmake_examples_compile_flags main.cpp)
  
  target_compile_definitions(cmake_examples_compile_flags 
      PRIVATE EX3
  )
  ```

- [main.cpp] - 具有main的源文件

  ```cpp
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
     std::cout << "Hello Compile Flags!" << std::endl;
  
     // only print if compile flag set
  #ifdef EX2
    std::cout << "Hello Compile Flag EX2!" << std::endl;
  #endif
  
  #ifdef EX3
    std::cout << "Hello Compile Flag EX3!" << std::endl;
  #endif
  
     return 0;
  }
  ```

## 2.概念

### 2.1 设置每个目标的C++标志

在现代CMake中设置C++标志的推荐方式是使用每个目标的标志，这些标志可以通过`target_compile_definitions()`函数的作用域（或者说接口范围）递到其他目标（INTERFACE或PUBLIC）。这将填充库的`INTERFACE_COMPILE_DEFINITIONS`，并根据作用域将定义传递到链接的目标。

```cmake
target_compile_definitions(cmake_examples_compile_flags
    PRIVATE EX3
)
```

这将导致编译器在编译目标时添加定义 -DEX3。

如果目标是库，并且已经选择了作用域PUBLIC或者INTERFACE，则该定义也将包含在链接该目标的任何可执行文件中。

对于编译器选项，你还可以使用`target_compile_options()`函数。

### 2.2 设置默认C++标志

`CMAKE_CXX_FLAGS`的默认值为空或包含生成类型的相应标志。

要设置其他默认编译标志，可以将以下内容添加到顶级CMakeLists.txt。

```cmake
set (CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -DEX2" CACHE STRING "Set C++ Compiler Flags" FORCE)
```

与CMAKE_CXX_FLAGS类似的其他选项包括：

- 使用CMAKE_C_FLAGS设置 C 编译器标志
- 使用CMAKE_LINKER_FLAGS设置链接器标志

| Note | 上述命令中的值 `CACHE STRING "Set C++ Compiler Flags" FORCE` 用于强制在CMakeCache.txt文件中设置此变量。有关更多详细信息，请参阅[此处](https://cmake.org/cmake/help/v3.0/command/set.html)。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

一旦设置，CMAKE_C_FLAGS和CMAKE_CXX_FLAGS将为该目录或任何包含的子目录中的所有目标全局设置编译器标志/定义。现在不建议将此方法用于一般用途，最好使用`target_compile_definitions`函数。

### 2.3 设置CMake标志

与构建类型类似，可以使用以下方法设置全局C++编译器标志。

- 使用GUI工具，如ccmake/cmake-gui
- 传递到cmake

```shell
cmake .. -DCMAKE_CXX_FLAGS="-DEX3"
```

## 3.构建示例

```shell
$ mkdir build

$ cd build/

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build

$ make VERBOSE=1
/usr/bin/cmake -H/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags -B/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build/CMakeFiles /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build'
make -f CMakeFiles/cmake_examples_compile_flags.dir/build.make CMakeFiles/cmake_examples_compile_flags.dir/depend
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build'
cd /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build/CMakeFiles/cmake_examples_compile_flags.dir/DependInfo.cmake --color=
Dependee "/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build/CMakeFiles/cmake_examples_compile_flags.dir/DependInfo.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build/CMakeFiles/cmake_examples_compile_flags.dir/depend.internal".
Dependee "/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build/CMakeFiles/CMakeDirectoryInformation.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build/CMakeFiles/cmake_examples_compile_flags.dir/depend.internal".
Scanning dependencies of target cmake_examples_compile_flags
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build'
make -f CMakeFiles/cmake_examples_compile_flags.dir/build.make CMakeFiles/cmake_examples_compile_flags.dir/build
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build/CMakeFiles 1
[100%] Building CXX object CMakeFiles/cmake_examples_compile_flags.dir/main.cpp.o
/usr/bin/c++    -DEX2   -o CMakeFiles/cmake_examples_compile_flags.dir/main.cpp.o -c /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/main.cpp
Linking CXX executable cmake_examples_compile_flags
/usr/bin/cmake -E cmake_link_script CMakeFiles/cmake_examples_compile_flags.dir/link.txt --verbose=1
/usr/bin/c++    -DEX2    CMakeFiles/cmake_examples_compile_flags.dir/main.cpp.o  -o cmake_examples_compile_flags -rdynamic
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build/CMakeFiles  1
[100%] Built target cmake_examples_compile_flags
make[1]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build'
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/G-compile-flags/build/CMakeFiles 0
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069664.html

# 【NO.268】CMake基础 第8节 包含第三方库

## 1.介绍

几乎所有重要的项目都需要包含第三方库、头文件或程序。CMake支持使用`find_package()`函数查找这些工具的路径。这将从`CMAKE_MODULE_PATH`中的文件夹列表中搜索格式为`FindXXX.cmake`的CMake模块。在Linux上，默认搜索路径将包含`/usr/share/cmake/Modules`。在我的系统上，这包括对大约1420个通用第三方库的支持。

本教程中的文件如下：

```objectivec
$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令

  ```cmake
  cmake_minimum_required(VERSION 3.5)
  
  # Set the project name
  project (third_party_include)
  
  
  # find a boost install with the libraries filesystem and system
  find_package(Boost 1.46.1 REQUIRED COMPONENTS filesystem system)
  
  # check if boost was found
  if(Boost_FOUND)
      message ("boost found")
  else()
      message (FATAL_ERROR "Cannot find Boost")
  endif()
  
  # Add an executable
  add_executable(third_party_include main.cpp)
  
  # link against the boost libraries
  target_link_libraries( third_party_include
      PRIVATE
          Boost::filesystem
  )
  ```

- [main.cpp] - 具有main的源文件

  ```cpp
  #include <iostream>
  #include <boost/shared_ptr.hpp>
  #include <boost/filesystem.hpp>
  
  int main(int argc, char *argv[])
  {
      std::cout << "Hello Third Party Include!" << std::endl;
  
      // use a shared ptr
      boost::shared_ptr<int> isp(new int(4));
  
      // trivial use of boost filesystem
      boost::filesystem::path path = "/usr/share/cmake/modules";
      if(path.is_relative())
      {
          std::cout << "Path is relative" << std::endl;
      }
      else
      {
          std::cout << "Path is not relative" << std::endl;
      }
  
     return 0;
  }
  ```

## 2.要求

此示例要求将Boost库安装在默认系统位置。

```shell
sudo apt-get install libboost-all-dev -y
```

## 3.概念

### 3.1 查找一个包

如上所述，`find_package()`函数将从`CMAKE_MODULE_PATH`中的文件夹列表中搜索格式为`FindXXX.cmake`的CMake模块。`find_package`的参数的确切格式将取决于你要查找的模块。这通常记录在文件`FindXXX.cmake`的顶部

下面是查找Boost的基本示例：

```cmake
find_package(Boost 1.46.1 REQUIRED COMPONENTS filesystem system)
```

这些参数是：

- Boost -库的名称。这是用于查找模块文件FindBoost.cmake的一部分。
- 1.46.1 - 要查找的Boost的最低版本。
- REQUIRED - 告诉模块这是必需的，如果失败，则找不到该模块。
- COMPONENTS - 要查找的库列表。

Boost includes可以接受更多参数，还可以利用其他变量。更复杂的设置将在后面的示例中提供。

### 3.2 检查是否找到该包

大多数包含的软件包都会设置一个变量`XXX_FOUND`，该变量可用于检查该软件包在系统上是否可用。

在本例中，变量为`BOOST_FOUND`：

```cmake
if(Boost_FOUND)
    message ("boost found")
    include_directories(${Boost_INCLUDE_DIRS})
else()
    message (FATAL_ERROR "Cannot find Boost")
endif()
```

### 3.3 导出变量

在找到包之后，它通常会导出变量，这些变量可以告诉用户在哪里可以找到库、头文件或可执行文件。与`XXX_FOUND`变量类似，它们是特定于包的，通常记录在`FindXXX.cmake`文件的顶部。

本例中导出的变量包括：

- `Boost_INCLUDE_DIRS` - Boost头文件的路径

在某些情况下，你还可以通过使用ccmake或cmake-gui检查缓存来检查这些变量。

### 3.4 别名/导入目标

大多数现代CMake库在其模块文件中导出别名目标。导入目标的好处在于，它们还可以填充头文件目录和链接库。

例如，从CMake的3.5版开始，Boost模块就支持此功能。

类似于将你自己的别名目标用于库，模块中的别名可以让引用找到的目标变得更容易。

在Boost的例子中，所有目标通过使用标识符`Boost::`加子模块的名字来导出。例如，你可以使用：

- `Boost::boost` 仅适用于库的头文件
- `Boost::system` 对于Boost系统库
- `Boost::filesystem` 对于文件系统库

与你自己的目标一样，这些目标包含它们的依赖项，因此链接到 `Boost::filesystem` 将自动添加 `Boost::boost`和`Boost::system`依赖。

要链接到导入的目标，可以使用以下命令：

```cmake
  target_link_libraries( third_party_include
      PRIVATE
          Boost::filesystem
  )
```

### 3.5 非别名目标

虽然大多数现代库使用导入的目标，但并非所有模块都已更新。在库尚未更新的情况下，你通常会发现以下变量可用：

- xxx_INCLUDE_DIRS - 指向库的include目录的变量
- xxx_LIBRARY - 指向库路径的变量.

然后，可以将这些文件添加到target_include_directory和target_link_library中：

```cmake
# Include the boost headers
target_include_directories( third_party_include
    PRIVATE ${Boost_INCLUDE_DIRS}
)

# link against the boost libraries
target_link_libraries( third_party_include
    PRIVATE
    ${Boost_SYSTEM_LIBRARY}
    ${Boost_FILESYSTEM_LIBRARY}
)
```

## 4.构建示例

```shell
$ mkdir build

$ cd build/

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Boost version: 1.54.0
-- Found the following Boost libraries:
--   filesystem
--   system
boost found
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/H-third-party-library/build

$ make
Scanning dependencies of target third_party_include
[100%] Building CXX object CMakeFiles/third_party_include.dir/main.cpp.o
Linking CXX executable third_party_include
[100%] Built target third_party_include
matrim@freyr:~/workspace/cmake-examples/01-basic/H-third-party-library/build$ ./
CMakeFiles/          third_party_include
matrim@freyr:~/workspace/cmake-examples/01-basic/H-third-party-library/build$ ./third_party_include
Hello Third Party Include!
Path is not relative
$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Boost version: 1.54.0
-- Found the following Boost libraries:
--   filesystem
--   system
boost found
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/H-third-party-library/build

$ make
Scanning dependencies of target third_party_include
[100%] Building CXX object CMakeFiles/third_party_include.dir/main.cpp.o
Linking CXX executable third_party_include
[100%] Built target third_party_include

$ ./third_party_include
Hello Third Party Include!
Path is not relative
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069669.html

# 【NO.269】CMake基础 第9节 使用Clang编译

## 1.引言

当使用CMake构建时，可以设置C和C++编译器。此示例与hello-cmake示例相同，只是它显示了将编译器从默认的GCC更改为clang的最基本方法。

本教程中的文件如下：

```objectivec
$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令。

  ```cmake
  # Set the minimum version of CMake that can be used
  # To find the cmake version run
  # $ cmake --version
  cmake_minimum_required(VERSION 3.5)
  
  # Set the project name
  project (hello_cmake)
  
  # Add an executable
  add_executable(hello_cmake main.cpp)
  ```

- [main.cpp] - 一个简单的“Hello World”C++文件。

  ```cpp
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
     std::cout << "Hello CMake!" << std::endl;
     return 0;
  }
  ```

## 2.概念

### 2.1 编译器选项

CMake公开了用于控制编译和链接代码的程序的选项。这些程序包括:

- CMAKE_C_COMPILER - 用于编译c代码的程序.
- CMAKE_CXX_COMPILER - 用于编译c++代码的程序.
- CMAKE_LINKER - 用于链接二进制文件的程序.

| Note | 在本例中，clang-3.6是通过命令`sudo apt-get install clang-3.6`安装的。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

| Note | 这是调用clang的最基本、最简单的方式。未来的示例将展示调用编译器的更好方法。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 2.2 设置标志

如第6节示例中所述，你可以使用cmake gui或通过命令行来设置CMake选项。

下面是通过命令行向编译器传递参数的示例。

```cmake
cmake .. -DCMAKE_C_COMPILER=clang-3.6 -DCMAKE_CXX_COMPILER=clang++-3.6
```

在设置这些选项之后，当你运行make时，clang将用于编译你的二进制文件。这可以从make输出中的以下几行看出。

```shell
/usr/bin/clang++-3.6     -o CMakeFiles/hello_cmake.dir/main.cpp.o -c /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/main.cpp
Linking CXX executable hello_cmake
/usr/bin/cmake -E cmake_link_script CMakeFiles/hello_cmake.dir/link.txt --verbose=1
/usr/bin/clang++-3.6       CMakeFiles/hello_cmake.dir/main.cpp.o  -o hello_cmake -rdynamic
```

## 3.构建示例

下面是示例输出：

```shell
$ mkdir build.clang

$ cd build.clang/

$ cmake .. -DCMAKE_C_COMPILER=clang-3.6 -DCMAKE_CXX_COMPILER=clang++-3.6
-- The C compiler identification is Clang 3.6.0
-- The CXX compiler identification is Clang 3.6.0
-- Check for working C compiler: /usr/bin/clang-3.6
-- Check for working C compiler: /usr/bin/clang-3.6 -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/clang++-3.6
-- Check for working CXX compiler: /usr/bin/clang++-3.6 -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang

$ make VERBOSE=1
/usr/bin/cmake -H/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang -B/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
make -f CMakeFiles/hello_cmake.dir/build.make CMakeFiles/hello_cmake.dir/depend
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
cd /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/hello_cmake.dir/DependInfo.cmake --color=
Dependee "/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/hello_cmake.dir/DependInfo.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/hello_cmake.dir/depend.internal".
Dependee "/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/CMakeDirectoryInformation.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/hello_cmake.dir/depend.internal".
Scanning dependencies of target hello_cmake
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
make -f CMakeFiles/hello_cmake.dir/build.make CMakeFiles/hello_cmake.dir/build
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles 1
[100%] Building CXX object CMakeFiles/hello_cmake.dir/main.cpp.o
/usr/bin/clang++-3.6     -o CMakeFiles/hello_cmake.dir/main.cpp.o -c /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/main.cpp
Linking CXX executable hello_cmake
/usr/bin/cmake -E cmake_link_script CMakeFiles/hello_cmake.dir/link.txt --verbose=1
/usr/bin/clang++-3.6       CMakeFiles/hello_cmake.dir/main.cpp.o  -o hello_cmake -rdynamic
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles  1
[100%] Built target hello_cmake
make[1]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles 0

$ ./hello_cmake
Hello CMake!
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069674.html

# 【NO.270】CMake基础 第10节 使用ninja构建

## 1.介绍

如前所述，CMake是一个元（meta）构建系统，可用于为许多其他构建工具创建构建文件。这个例子展示了如何让CMake使用ninja构建工具。

本教程中的文件如下：

```shell
$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令

  ```cmake
  # Set the minimum version of CMake that can be used
  # To find the cmake version run
  # $ cmake --version
  cmake_minimum_required(VERSION 3.5)
  
  # Set the project name
  project (hello_cmake)
  
  # Add an executable
  add_executable(hello_cmake main.cpp)
  ```

- [main.cpp] - 一个简单的“Hello World”CPP文件

  ```cpp
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
     std::cout << "Hello CMake!" << std::endl;
     return 0;
  }
  ```

## 2.概念

### 2.1 生成器

CMake生成器负责为底层构建系统编写输入文件(例如Makefile)。运行`cmake--help`将显示可用的生成器。对于cmake v2.8.12.2，我的系统支持的生成器包括：

```shell
Generators

The following generators are available on this platform:
  Unix Makefiles              = Generates standard UNIX makefiles.
  Ninja                       = Generates build.ninja files (experimental).
  CodeBlocks - Ninja          = Generates CodeBlocks project files.
  CodeBlocks - Unix Makefiles = Generates CodeBlocks project files.
  Eclipse CDT4 - Ninja        = Generates Eclipse CDT 4.0 project files.
  Eclipse CDT4 - Unix Makefiles
                              = Generates Eclipse CDT 4.0 project files.
  KDevelop3                   = Generates KDevelop 3 project files.
  KDevelop3 - Unix Makefiles  = Generates KDevelop 3 project files.
  Sublime Text 2 - Ninja      = Generates Sublime Text 2 project files.
  Sublime Text 2 - Unix Makefiles
                              = Generates Sublime Text 2 project files.Generators
```

正如本文所述，CMake包括不同类型的生成器，如命令行生成器、IDE生成器和其他生成器。

#### 2.1.1 命令行生成工具生成器

这些生成器用于命令行构建工具，如Make和Ninja。在使用CMake生成构建系统之前，必须配置所选的工具链。

支持的生成器包括：

- Borland Makefiles
- MSYS Makefiles
- MinGW Makefiles
- NMake Makefiles
- NMake Makefiles JOM
- Ninja
- Unix Makefiles
- Watcom WMake

#### 2.1.2 IDE构建工具生成器

这些生成器用于集成开发环境，其中包括它们自己的编译器。例如Visual Studio和Xcode，它们本身就包含一个编译器。

支持的生成器包括：

- Visual Studio 6
- Visual Studio 7
- Visual Studio 7 .NET 2003
- Visual Studio 8 2005
- Visual Studio 9 2008
- Visual Studio 10 2010
- Visual Studio 11 2012
- Visual Studio 12 2013
- Xcode

#### 2.1.3 其他生成器

这些生成器创建配置并与其他IDE工具共同工作，并且必须包含在IDE或命令行生成器中。

支持的生成器包括：

- CodeBlocks
- CodeLite
- Eclipse CDT4
- KDevelop3
- Kate
- Sublime Text 2

| Note | 在本例中，ninja是通过命令`sudo apt-get install ninja-build`安装的。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 2.2 调用生成器

要调用CMake生成器，可以使用-G命令行开关，例如：

```undefined
cmake .. -G Ninja
```

完成上述操作后，CMake将生成所需的Ninja构建文件，这些文件可以通过使用Ninja命令运行。

```shell
$ cmake .. -G Ninja

$ ls
build.ninja  CMakeCache.txt  CMakeFiles  cmake_install.cmake  rules.ninja
```

## 3.构建示例

下面是构建此示例的示例输出。

```shell
$ mkdir build.ninja

$ cd build.ninja/

$ cmake .. -G Ninja
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler using: Ninja
-- Check for working C compiler using: Ninja -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler using: Ninja
-- Check for working CXX compiler using: Ninja -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/J-building-with-ninja/build.ninja

$ ninja -v
[1/2] /usr/bin/c++     -MMD -MT CMakeFiles/hello_cmake.dir/main.cpp.o -MF "CMakeFiles/hello_cmake.dir/main.cpp.o.d" -o CMakeFiles/hello_cmake.dir/main.cpp.o -c ../main.cpp
[2/2] : && /usr/bin/c++      CMakeFiles/hello_cmake.dir/main.cpp.o  -o hello_cmake  -rdynamic && :

$ ls
build.ninja  CMakeCache.txt  CMakeFiles  cmake_install.cmake  hello_cmake  rules.ninja

$ ./hello_cmake
Hello CMake!
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069678.html

# 【NO.271】CMake基础 第9节 使用Clang编译

## 1.引言

当使用CMake构建时，可以设置C和C++编译器。此示例与hello-cmake示例相同，只是它显示了将编译器从默认的GCC更改为clang的最基本方法。

本教程中的文件如下：

```objectivec
$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令。

  ```cmake
  # Set the minimum version of CMake that can be used
  # To find the cmake version run
  # $ cmake --version
  cmake_minimum_required(VERSION 3.5)
  
  # Set the project name
  project (hello_cmake)
  
  # Add an executable
  add_executable(hello_cmake main.cpp)
  ```

- [main.cpp] - 一个简单的“Hello World”C++文件。

  ```cpp
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
     std::cout << "Hello CMake!" << std::endl;
     return 0;
  }
  ```

## 2.概念

### 2.1 编译器选项

CMake公开了用于控制编译和链接代码的程序的选项。这些程序包括:

- CMAKE_C_COMPILER - 用于编译c代码的程序.
- CMAKE_CXX_COMPILER - 用于编译c++代码的程序.
- CMAKE_LINKER - 用于链接二进制文件的程序.

| Note | 在本例中，clang-3.6是通过命令`sudo apt-get install clang-3.6`安装的。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

| Note | 这是调用clang的最基本、最简单的方式。未来的示例将展示调用编译器的更好方法。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 2.2 设置标志

如第6节示例中所述，你可以使用cmake gui或通过命令行来设置CMake选项。

下面是通过命令行向编译器传递参数的示例。

```cmake
cmake .. -DCMAKE_C_COMPILER=clang-3.6 -DCMAKE_CXX_COMPILER=clang++-3.6
```

在设置这些选项之后，当你运行make时，clang将用于编译你的二进制文件。这可以从make输出中的以下几行看出。

```shell
/usr/bin/clang++-3.6     -o CMakeFiles/hello_cmake.dir/main.cpp.o -c /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/main.cpp
Linking CXX executable hello_cmake
/usr/bin/cmake -E cmake_link_script CMakeFiles/hello_cmake.dir/link.txt --verbose=1
/usr/bin/clang++-3.6       CMakeFiles/hello_cmake.dir/main.cpp.o  -o hello_cmake -rdynamic
```

## 3.构建示例

下面是示例输出：

```shell
$ mkdir build.clang

$ cd build.clang/

$ cmake .. -DCMAKE_C_COMPILER=clang-3.6 -DCMAKE_CXX_COMPILER=clang++-3.6
-- The C compiler identification is Clang 3.6.0
-- The CXX compiler identification is Clang 3.6.0
-- Check for working C compiler: /usr/bin/clang-3.6
-- Check for working C compiler: /usr/bin/clang-3.6 -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/clang++-3.6
-- Check for working CXX compiler: /usr/bin/clang++-3.6 -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang

$ make VERBOSE=1
/usr/bin/cmake -H/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang -B/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
make -f CMakeFiles/hello_cmake.dir/build.make CMakeFiles/hello_cmake.dir/depend
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
cd /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/hello_cmake.dir/DependInfo.cmake --color=
Dependee "/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/hello_cmake.dir/DependInfo.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/hello_cmake.dir/depend.internal".
Dependee "/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/CMakeDirectoryInformation.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles/hello_cmake.dir/depend.internal".
Scanning dependencies of target hello_cmake
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
make -f CMakeFiles/hello_cmake.dir/build.make CMakeFiles/hello_cmake.dir/build
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles 1
[100%] Building CXX object CMakeFiles/hello_cmake.dir/main.cpp.o
/usr/bin/clang++-3.6     -o CMakeFiles/hello_cmake.dir/main.cpp.o -c /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/main.cpp
Linking CXX executable hello_cmake
/usr/bin/cmake -E cmake_link_script CMakeFiles/hello_cmake.dir/link.txt --verbose=1
/usr/bin/clang++-3.6       CMakeFiles/hello_cmake.dir/main.cpp.o  -o hello_cmake -rdynamic
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles  1
[100%] Built target hello_cmake
make[1]: Leaving directory `/home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang'
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/01-basic/I-compiling-with-clang/build.clang/CMakeFiles 0

$ ./hello_cmake
Hello CMake!
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069674.html

# 【NO.272】CMake基础 第10节 使用ninja构建

## 1.介绍

如前所述，CMake是一个元（meta）构建系统，可用于为许多其他构建工具创建构建文件。这个例子展示了如何让CMake使用ninja构建工具。

本教程中的文件如下：

```shell
$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令

  ```cmake
  # Set the minimum version of CMake that can be used
  # To find the cmake version run
  # $ cmake --version
  cmake_minimum_required(VERSION 3.5)
  
  # Set the project name
  project (hello_cmake)
  
  # Add an executable
  add_executable(hello_cmake main.cpp)
  ```

- [main.cpp] - 一个简单的“Hello World”CPP文件

  ```cpp
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
     std::cout << "Hello CMake!" << std::endl;
     return 0;
  }
  ```

## 2.概念

### 2.1 生成器

CMake生成器负责为底层构建系统编写输入文件(例如Makefile)。运行`cmake--help`将显示可用的生成器。对于cmake v2.8.12.2，我的系统支持的生成器包括：

```shell
Generators

The following generators are available on this platform:
  Unix Makefiles              = Generates standard UNIX makefiles.
  Ninja                       = Generates build.ninja files (experimental).
  CodeBlocks - Ninja          = Generates CodeBlocks project files.
  CodeBlocks - Unix Makefiles = Generates CodeBlocks project files.
  Eclipse CDT4 - Ninja        = Generates Eclipse CDT 4.0 project files.
  Eclipse CDT4 - Unix Makefiles
                              = Generates Eclipse CDT 4.0 project files.
  KDevelop3                   = Generates KDevelop 3 project files.
  KDevelop3 - Unix Makefiles  = Generates KDevelop 3 project files.
  Sublime Text 2 - Ninja      = Generates Sublime Text 2 project files.
  Sublime Text 2 - Unix Makefiles
                              = Generates Sublime Text 2 project files.Generators
```

正如本文所述，CMake包括不同类型的生成器，如命令行生成器、IDE生成器和其他生成器。

#### 2.1.1 命令行生成工具生成器

这些生成器用于命令行构建工具，如Make和Ninja。在使用CMake生成构建系统之前，必须配置所选的工具链。

支持的生成器包括：

- Borland Makefiles
- MSYS Makefiles
- MinGW Makefiles
- NMake Makefiles
- NMake Makefiles JOM
- Ninja
- Unix Makefiles
- Watcom WMake

#### 2.1.2 IDE构建工具生成器

这些生成器用于集成开发环境，其中包括它们自己的编译器。例如Visual Studio和Xcode，它们本身就包含一个编译器。

支持的生成器包括：

- Visual Studio 6
- Visual Studio 7
- Visual Studio 7 .NET 2003
- Visual Studio 8 2005
- Visual Studio 9 2008
- Visual Studio 10 2010
- Visual Studio 11 2012
- Visual Studio 12 2013
- Xcode

#### 2.1.3 其他生成器

这些生成器创建配置并与其他IDE工具共同工作，并且必须包含在IDE或命令行生成器中。

支持的生成器包括：

- CodeBlocks
- CodeLite
- Eclipse CDT4
- KDevelop3
- Kate
- Sublime Text 2

| Note | 在本例中，ninja是通过命令`sudo apt-get install ninja-build`安装的。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 2.2 调用生成器

要调用CMake生成器，可以使用-G命令行开关，例如：

```undefined
cmake .. -G Ninja
```

完成上述操作后，CMake将生成所需的Ninja构建文件，这些文件可以通过使用Ninja命令运行。

```shell
$ cmake .. -G Ninja

$ ls
build.ninja  CMakeCache.txt  CMakeFiles  cmake_install.cmake  rules.ninja
```

## 3.构建示例

下面是构建此示例的示例输出。

```shell
$ mkdir build.ninja

$ cd build.ninja/

$ cmake .. -G Ninja
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler using: Ninja
-- Check for working C compiler using: Ninja -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler using: Ninja
-- Check for working CXX compiler using: Ninja -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/01-basic/J-building-with-ninja/build.ninja

$ ninja -v
[1/2] /usr/bin/c++     -MMD -MT CMakeFiles/hello_cmake.dir/main.cpp.o -MF "CMakeFiles/hello_cmake.dir/main.cpp.o.d" -o CMakeFiles/hello_cmake.dir/main.cpp.o -c ../main.cpp
[2/2] : && /usr/bin/c++      CMakeFiles/hello_cmake.dir/main.cpp.o  -o hello_cmake  -rdynamic && :

$ ls
build.ninja  CMakeCache.txt  CMakeFiles  cmake_install.cmake  hello_cmake  rules.ninja

$ ./hello_cmake
Hello CMake!
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069678.html

# 【NO.273】CMake基础 第11节 导入目标 

## 1.介绍

正如前面在第8节中提到的，较新版本的CMake允许你使用导入的别名目标链接第三方库。

本教程中的文件如下：

```objectivec
$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令

  ```cmake
  cmake_minimum_required(VERSION 3.5)
  
  # Set the project name
  project (imported_targets)
  
  
  # find a boost install with the libraries filesystem and system
  find_package(Boost 1.46.1 REQUIRED COMPONENTS filesystem system)
  
  # check if boost was found
  if(Boost_FOUND)
      message ("boost found")
  else()
      message (FATAL_ERROR "Cannot find Boost")
  endif()
  
  # Add an executable
  add_executable(imported_targets main.cpp)
  
  # link against the boost libraries
  target_link_libraries( imported_targets
      PRIVATE
          Boost::filesystem
  )
  ```

- [main.cpp] - 具有main的源文件

  ```cpp
  #include <iostream>
  #include <boost/shared_ptr.hpp>
  #include <boost/filesystem.hpp>
  
  int main(int argc, char *argv[])
  {
      std::cout << "Hello Third Party Include!" << std::endl;
  
      // use a shared ptr
      boost::shared_ptr<int> isp(new int(4));
  
      // trivial use of boost filesystem
      boost::filesystem::path path = "/usr/share/cmake/modules";
      if(path.is_relative())
      {
          std::cout << "Path is relative" << std::endl;
      }
      else
      {
          std::cout << "Path is not relative" << std::endl;
      }
  
     return 0;
  }
  ```

## 2.要求

此示例需要以下条件:

- CMake v3.5+
- 安装在默认系统位置的Boost库

## 3.概念

### 3.1 导入目标

导入目标是由FindXXX模块导出的只读目标（例如Boost::filesystem）。

要包括Boost文件系统，你可以执行以下操作：

```cmake
  target_link_libraries( imported_targets
      PRIVATE
          Boost::filesystem
  )
```

这将自动链接`Boost::FileSystem`和`Boost::System`库，同时还包括`Boost include`目录（即不必手动添加include目录）。

## 4.构建示例

```shell
$ mkdir build

$ cd build/

$ cmake ..
-- The C compiler identification is GNU 5.4.0
-- The CXX compiler identification is GNU 5.4.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Boost version: 1.58.0
-- Found the following Boost libraries:
--   filesystem
--   system
boost found
-- Configuring done
-- Generating done
-- Build files have been written to: /data/code/01-basic/K-imported-targets/build

$ make
Scanning dependencies of target imported_targets
[ 50%] Building CXX object CMakeFiles/imported_targets.dir/main.cpp.o
[100%] Linking CXX executable imported_targets
[100%] Built target imported_targets


$ ./imported_targets
Hello Third Party Include!
Path is not relative
```



原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069682.html

# 【NO.274】CMake基础 第12节 设置C++标准

## 1.介绍

自从C++11和C++14发布以来，一个常见的用例是调用编译器来使用这些标准。随着CMake的发展，它添加了一些功能来使这一点变得更容易，而CMake的新版本已经改变了实现这一点的方式。下面的示例显示了设置C++标准的三种不同方法，以及可以使用哪些版本的CMake。

这些例子包括：

- [common-method]. 一个简单的版本，应该可以与大多数版本的CMake一起使用
- [cxx-standard]. 使用CMake v3.1中引入的`CMAKE_CXX_STANDARD`变量
- [compile-features]. 使用CMakev3.1中引入的`target_compile_features`函数

## 2.一个简单的版本

此示例显示了设置C++标准的通用方法。这可以与大多数版本的CMake一起使用。但是，如果你使用CMake的最新版本，则可以用更方便的方法。

本教程中的文件如下：

```objectivec
A-hello-cmake$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令。

  ```cmake
  # Set the minimum version of CMake that can be used
  # To find the cmake version run
  # $ cmake --version
  cmake_minimum_required(VERSION 2.8)
  
  # Set the project name
  project (hello_cpp11)
  
  # try conditional compilation
  
  #  Check whether the CXX compiler supports a given flag.
  ## CHECK_CXX_COMPILER_FLAG(<flag> <var>)
  #  <flag> - the compiler flag
  #  <var>  - variable to store the result
  #  This internally calls the check_cxx_source_compiles macro and sets CMAKE_REQUIRED_DEFINITIONS to <flag>
  include(CheckCXXCompilerFlag)
  CHECK_CXX_COMPILER_FLAG("-std=c++11" COMPILER_SUPPORTS_CXX11)
  CHECK_CXX_COMPILER_FLAG("-std=c++0x" COMPILER_SUPPORTS_CXX0X)
  
  # check results and add flag
  if(COMPILER_SUPPORTS_CXX11)#
      set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++11")
  elseif(COMPILER_SUPPORTS_CXX0X)#
      set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++0x")
  else()
      message(STATUS "The compiler ${CMAKE_CXX_COMPILER} has no C++11 support. Please use a different C++ compiler.")
  endif()
  
  # Add an executable
  add_executable(hello_cpp11 main.cpp)
  ```

- [main.cpp] - 一个针对C++11的简单的“Hello World”CPP文件。

  ```cpp
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
      auto message = "Hello C++11";
      std::cout << message << std::endl;
      return 0;
  }
  ```

### 2.1 概念

#### 2.1.1 检查编译标志

CMake支持尝试使用传递给函数CMAKE_CXX_COMPILER_FLAG的任何标志编译程序。然后将结果存储在你传入的变量中。

例如:

```cmake
include(CheckCXXCompilerFlag)
CHECK_CXX_COMPILER_FLAG("-std=c++11" COMPILER_SUPPORTS_CXX11)
```

此示例将尝试使用标志`-std=c++11`编译程序，并将结果存储在变量`COMPILER_SUPPORTS_CXX11`中。

`include(CheckCXXCompilerFlag)`这一行告诉CMake包含此函数以使其可用。

#### 2.1.2 添加标志

一旦确定编译是否支持标志，就可以使用标准的cmake方法将该标志添加到目标。在本例中，我们使用CMAKE_CXX_FLAGS将该标志传递到所有目标。

```cmake
if(COMPILER_SUPPORTS_CXX11)#
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++11")
elseif(COMPILER_SUPPORTS_CXX0X)#
    set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++0x")
else()
    message(STATUS "The compiler ${CMAKE_CXX_COMPILER} has no C++11 support. Please use a different C++ compiler.")
endif()
```

上面的示例只检查编译标志的GCC版本，并支持从C++11回退到标准化前的C++0x标志。在实际使用中，你可能希望检查C14，或者添加对不同编译设置方法的支持，例如`-std=gnu11`。

### 2.2 构建示例

下面是构建此示例的示例输出。

```shell
$ mkdir build
$ cd build

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Performing Test COMPILER_SUPPORTS_CXX11
-- Performing Test COMPILER_SUPPORTS_CXX11 - Success
-- Performing Test COMPILER_SUPPORTS_CXX0X
-- Performing Test COMPILER_SUPPORTS_CXX0X - Success
-- Configuring done
-- Generating done
-- Build files have been written to: /data/code/01-basic/L-cpp-standard/i-common-method/build

$ make VERBOSE=1
/usr/bin/cmake -H/data/code/01-basic/L-cpp-standard/i-common-method -B/data/code/01-basic/L-cpp-standard/i-common-method/build --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /data/code/01-basic/L-cpp-standard/i-common-method/build/CMakeFiles /data/code/01-basic/L-cpp-standard/i-common-method/build/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory `/data/code/01-basic/L-cpp-standard/i-common-method/build'
make -f CMakeFiles/hello_cpp11.dir/build.make CMakeFiles/hello_cpp11.dir/depend
make[2]: Entering directory `/data/code/01-basic/L-cpp-standard/i-common-method/build'
cd /data/code/01-basic/L-cpp-standard/i-common-method/build && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /data/code/01-basic/L-cpp-standard/i-common-method /data/code/01-basic/L-cpp-standard/i-common-method /data/code/01-basic/L-cpp-standard/i-common-method/build /data/code/01-basic/L-cpp-standard/i-common-method/build /data/code/01-basic/L-cpp-standard/i-common-method/build/CMakeFiles/hello_cpp11.dir/DependInfo.cmake --color=
Dependee "/data/code/01-basic/L-cpp-standard/i-common-method/build/CMakeFiles/hello_cpp11.dir/DependInfo.cmake" is newer than depender "/data/code/01-basic/L-cpp-standard/i-common-method/build/CMakeFiles/hello_cpp11.dir/depend.internal".
Dependee "/data/code/01-basic/L-cpp-standard/i-common-method/build/CMakeFiles/CMakeDirectoryInformation.cmake" is newer than depender "/data/code/01-basic/L-cpp-standard/i-common-method/build/CMakeFiles/hello_cpp11.dir/depend.internal".
Scanning dependencies of target hello_cpp11
make[2]: Leaving directory `/data/code/01-basic/L-cpp-standard/i-common-method/build'
make -f CMakeFiles/hello_cpp11.dir/build.make CMakeFiles/hello_cpp11.dir/build
make[2]: Entering directory `/data/code/01-basic/L-cpp-standard/i-common-method/build'
/usr/bin/cmake -E cmake_progress_report /data/code/01-basic/L-cpp-standard/i-common-method/build/CMakeFiles 1
[100%] Building CXX object CMakeFiles/hello_cpp11.dir/main.cpp.o
/usr/bin/c++    -std=c++11   -o CMakeFiles/hello_cpp11.dir/main.cpp.o -c /data/code/01-basic/L-cpp-standard/i-common-method/main.cpp
Linking CXX executable hello_cpp11
/usr/bin/cmake -E cmake_link_script CMakeFiles/hello_cpp11.dir/link.txt --verbose=1
/usr/bin/c++    -std=c++11    CMakeFiles/hello_cpp11.dir/main.cpp.o  -o hello_cpp11 -rdynamic
make[2]: Leaving directory `/data/code/01-basic/L-cpp-standard/i-common-method/build'
/usr/bin/cmake -E cmake_progress_report /data/code/01-basic/L-cpp-standard/i-common-method/build/CMakeFiles  1
[100%] Built target hello_cpp11
make[1]: Leaving directory `/data/code/01-basic/L-cpp-standard/i-common-method/build'
/usr/bin/cmake -E cmake_progress_start /data/code/01-basic/L-cpp-standard/i-common-method/build/CMakeFiles 0
```

## 3.使用CMAKE_CXX_STANDARD变量

### 3.1 介绍

此示例说明如何使用CMAKE_CXX_STANDARD变量设置C++标准。这是从CMake v3.1开始提供的。

本教程中的文件如下：

```objectivec
A-hello-cmake$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令

  ```cmake
  # Set the minimum version of CMake that can be used
  # To find the cmake version run
  # $ cmake --version
  cmake_minimum_required(VERSION 3.1)
  
  # Set the project name
  project (hello_cpp11)
  
  # set the C++ standard to C++ 11
  set(CMAKE_CXX_STANDARD 11)
  
  # Add an executable
  add_executable(hello_cpp11 main.cpp)
  ```

- [main.cpp] - 一个针对C++11的简单的“Hello World”CPP文件

  ```cpp
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
      auto message = "Hello C++11";
      std::cout << message << std::endl;
      return 0;
  }
  ```

### 3.2 概念

#### 3.2.1 使用CXX_STANDARD属性

设置CMAKE_CXX_STANDARD变量会导致所有目标上的CXX_STANDARD属性改变。这会影响CMake在编译时设置适当的标志。

| Note | `CMAKE_CXX_STANDARD`变量会回退到最接近的不会失败的适当标准。例如，如果请求`-std=gnu11`，最后可能变成`-std=gnu0x`。这可能会在编译时导致意外故障。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

### 3.3 构建示例

下面是构建此示例的示例输出：

```shell
$ mkdir build
$ cd build


$ cmake ..
-- The C compiler identification is GNU 5.4.0
-- The CXX compiler identification is GNU 5.4.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Configuring done
-- Generating done
-- Build files have been written to: /data/code/01-basic/L-cpp-standard/ii-cxx-standard/build

$ make VERBOSE=1
/usr/bin/cmake -H/data/code/01-basic/L-cpp-standard/ii-cxx-standard -B/data/code/01-basic/L-cpp-standard/ii-cxx-standard/build --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /data/code/01-basic/L-cpp-standard/ii-cxx-standard/build/CMakeFiles /data/code/01-basic/L-cpp-standard/ii-cxx-standard/build/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory '/data/code/01-basic/L-cpp-standard/ii-cxx-standard/build'
make -f CMakeFiles/hello_cpp11.dir/build.make CMakeFiles/hello_cpp11.dir/depend
make[2]: Entering directory '/data/code/01-basic/L-cpp-standard/ii-cxx-standard/build'
cd /data/code/01-basic/L-cpp-standard/ii-cxx-standard/build && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /data/code/01-basic/L-cpp-standard/ii-cxx-standard /data/code/01-basic/L-cpp-standard/ii-cxx-standard /data/code/01-basic/L-cpp-standard/ii-cxx-standard/build /data/code/01-basic/L-cpp-standard/ii-cxx-standard/build /data/code/01-basic/L-cpp-standard/ii-cxx-standard/build/CMakeFiles/hello_cpp11.dir/DependInfo.cmake --color=
Dependee "/data/code/01-basic/L-cpp-standard/ii-cxx-standard/build/CMakeFiles/hello_cpp11.dir/DependInfo.cmake" is newer than depender "/data/code/01-basic/L-cpp-standard/ii-cxx-standard/build/CMakeFiles/hello_cpp11.dir/depend.internal".
Dependee "/data/code/01-basic/L-cpp-standard/ii-cxx-standard/build/CMakeFiles/CMakeDirectoryInformation.cmake" is newer than depender "/data/code/01-basic/L-cpp-standard/ii-cxx-standard/build/CMakeFiles/hello_cpp11.dir/depend.internal".
Scanning dependencies of target hello_cpp11
make[2]: Leaving directory '/data/code/01-basic/L-cpp-standard/ii-cxx-standard/build'
make -f CMakeFiles/hello_cpp11.dir/build.make CMakeFiles/hello_cpp11.dir/build
make[2]: Entering directory '/data/code/01-basic/L-cpp-standard/ii-cxx-standard/build'
[ 50%] Building CXX object CMakeFiles/hello_cpp11.dir/main.cpp.o
/usr/bin/c++     -std=gnu++11 -o CMakeFiles/hello_cpp11.dir/main.cpp.o -c /data/code/01-basic/L-cpp-standard/ii-cxx-standard/main.cpp
[100%] Linking CXX executable hello_cpp11
/usr/bin/cmake -E cmake_link_script CMakeFiles/hello_cpp11.dir/link.txt --verbose=1
/usr/bin/c++      CMakeFiles/hello_cpp11.dir/main.cpp.o  -o hello_cpp11 -rdynamic
make[2]: Leaving directory '/data/code/01-basic/L-cpp-standard/ii-cxx-standard/build'
[100%] Built target hello_cpp11
make[1]: Leaving directory '/data/code/01-basic/L-cpp-standard/ii-cxx-standard/build'
```

## 4.使用target_compile_features函数

### 4.1 介绍

此示例说明如何使用`target_compile_features`函数设置C++标准。

本教程中的文件如下：

```objectivec
A-hello-cmake$ tree
.
├── CMakeLists.txt
├── main.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令

  ```cmake
  # Set the minimum version of CMake that can be used
  # To find the cmake version run
  # $ cmake --version
  cmake_minimum_required(VERSION 3.1)
  
  # Set the project name
  project (hello_cpp11)
  
  # Add an executable
  add_executable(hello_cpp11 main.cpp)
  
  # set the C++ standard to the appropriate standard for using auto
  target_compile_features(hello_cpp11 PUBLIC cxx_auto_type)
  
  # Print the list of known compile features for this version of CMake
  message("List of compile features: ${CMAKE_CXX_COMPILE_FEATURES}")
  ```

- [main.cpp] - 一个针对C++11的简单的“Hello World”C++文件

  ```cpp
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
      auto message = "Hello C++11";
      std::cout << message << std::endl;
      return 0;
  }
  ```

### 4.2 概念

#### 4.2.1 使用target_compile_features

在目标上调用target_compile_features函数将检查传入的功能，并由CMake确定正确的用于目标的编译器标志。

```cmake
target_compile_features(hello_cpp11 PUBLIC cxx_auto_type)
```

与其他`target_*`函数一样，你可以指定所选目标的功能范围。这将填充目标的`INTERFACE_COMPILE_FEATURES`属性。可用功能列表可从`CMAKE_CXX_COMPILE_FEATURES`变量中找到。你可以使用以下代码获取可用功能的列表：

```cmake
message("List of compile features: ${CMAKE_CXX_COMPILE_FEATURES}")
```

### 4.3 构建示例

下面是构建此示例的输出。

```shell
$ mkdir build
$ cd build

$ cmake ..
-- The C compiler identification is GNU 5.4.0
-- The CXX compiler identification is GNU 5.4.0
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
List of compile features: cxx_template_template_parameters;cxx_alias_templates;cxx_alignas;cxx_alignof;cxx_attributes;cxx_auto_type;cxx_constexpr;cxx_decltype;cxx_decltype_incomplete_return_types;cxx_default_function_template_args;cxx_defaulted_functions;cxx_defaulted_move_initializers;cxx_delegating_constructors;cxx_deleted_functions;cxx_enum_forward_declarations;cxx_explicit_conversions;cxx_extended_friend_declarations;cxx_extern_templates;cxx_final;cxx_func_identifier;cxx_generalized_initializers;cxx_inheriting_constructors;cxx_inline_namespaces;cxx_lambdas;cxx_local_type_template_args;cxx_long_long_type;cxx_noexcept;cxx_nonstatic_member_init;cxx_nullptr;cxx_override;cxx_range_for;cxx_raw_string_literals;cxx_reference_qualified_functions;cxx_right_angle_brackets;cxx_rvalue_references;cxx_sizeof_member;cxx_static_assert;cxx_strong_enums;cxx_thread_local;cxx_trailing_return_types;cxx_unicode_literals;cxx_uniform_initialization;cxx_unrestricted_unions;cxx_user_literals;cxx_variadic_macros;cxx_variadic_templates;cxx_aggregate_default_initializers;cxx_attribute_deprecated;cxx_binary_literals;cxx_contextual_conversions;cxx_decltype_auto;cxx_digit_separators;cxx_generic_lambdas;cxx_lambda_init_captures;cxx_relaxed_constexpr;cxx_return_type_deduction;cxx_variable_templates
-- Configuring done
-- Generating done
-- Build files have been written to: /data/code/01-basic/L-cpp-standard/iii-compile-features/build


$ make VERBOSE=1
/usr/bin/cmake -H/data/code/01-basic/L-cpp-standard/iii-compile-features -B/data/code/01-basic/L-cpp-standard/iii-compile-features/build --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /data/code/01-basic/L-cpp-standard/iii-compile-features/build/CMakeFiles /data/code/01-basic/L-cpp-standard/iii-compile-features/build/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory '/data/code/01-basic/L-cpp-standard/iii-compile-features/build'
make -f CMakeFiles/hello_cpp11.dir/build.make CMakeFiles/hello_cpp11.dir/depend
make[2]: Entering directory '/data/code/01-basic/L-cpp-standard/iii-compile-features/build'
cd /data/code/01-basic/L-cpp-standard/iii-compile-features/build && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /data/code/01-basic/L-cpp-standard/iii-compile-features /data/code/01-basic/L-cpp-standard/iii-compile-features /data/code/01-basic/L-cpp-standard/iii-compile-features/build /data/code/01-basic/L-cpp-standard/iii-compile-features/build /data/code/01-basic/L-cpp-standard/iii-compile-features/build/CMakeFiles/hello_cpp11.dir/DependInfo.cmake --color=
Dependee "/data/code/01-basic/L-cpp-standard/iii-compile-features/build/CMakeFiles/hello_cpp11.dir/DependInfo.cmake" is newer than depender "/data/code/01-basic/L-cpp-standard/iii-compile-features/build/CMakeFiles/hello_cpp11.dir/depend.internal".
Dependee "/data/code/01-basic/L-cpp-standard/iii-compile-features/build/CMakeFiles/CMakeDirectoryInformation.cmake" is newer than depender "/data/code/01-basic/L-cpp-standard/iii-compile-features/build/CMakeFiles/hello_cpp11.dir/depend.internal".
Scanning dependencies of target hello_cpp11
make[2]: Leaving directory '/data/code/01-basic/L-cpp-standard/iii-compile-features/build'
make -f CMakeFiles/hello_cpp11.dir/build.make CMakeFiles/hello_cpp11.dir/build
make[2]: Entering directory '/data/code/01-basic/L-cpp-standard/iii-compile-features/build'
[ 50%] Building CXX object CMakeFiles/hello_cpp11.dir/main.cpp.o
/usr/bin/c++     -std=gnu++11 -o CMakeFiles/hello_cpp11.dir/main.cpp.o -c /data/code/01-basic/L-cpp-standard/iii-compile-features/main.cpp
[100%] Linking CXX executable hello_cpp11
/usr/bin/cmake -E cmake_link_script CMakeFiles/hello_cpp11.dir/link.txt --verbose=1
/usr/bin/c++      CMakeFiles/hello_cpp11.dir/main.cpp.o  -o hello_cpp11 -rdynamic
make[2]: Leaving directory '/data/code/01-basic/L-cpp-standard/iii-compile-features/build'
[100%] Built target hello_cpp11
make[1]: Leaving directory '/data/code/01-basic/L-cpp-standard/iii-compile-features/build'
/usr/bin/cmake -E cmake_progress_start /data/code/01-basic/L-cpp-standard/
```



原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069685.html

# 【NO.275】CMake基础 第13节 构建子项目



## 1.介绍

此示例说明如何设置包含子项目的CMake项目。顶层CMakeLists.txt调用子目录中的CMakeLists.txt以创建以下内容：

- sublibrary1 - 静态库
- sublibrary2 - 头文件库
- subbinary - 可执行文件

此示例中包含的文件包括：

```mipsasm
$ tree
.
├── CMakeLists.txt
├── subbinary
│   ├── CMakeLists.txt
│   └── main.cpp
├── sublibrary1
│   ├── CMakeLists.txt
│   ├── include
│   │   └── sublib1
│   │       └── sublib1.h
│   └── src
│       └── sublib1.cpp
└── sublibrary2
    ├── CMakeLists.txt
    └── include
        └── sublib2
            └── sublib2.h
```

- [CMakeLists.txt] - 顶级CMakeLists.txt

  ```cmake
  cmake_minimum_required (VERSION 3.5)
  
  project(subprojects)
  
  # Add sub directories
  add_subdirectory(sublibrary1)
  add_subdirectory(sublibrary2)
  add_subdirectory(subbinary)
  ```

- [subbinary/CMakeLists.txt] - 生成可执行文件

  ```cmake
  project(subbinary)
  
  # Create the executable
  add_executable(${PROJECT_NAME} main.cpp)
  
  # Link the static library from subproject1 using it's alias sub::lib1
  # Link the header only library from subproject2 using it's alias sub::lib2
  # This will cause the include directories for that target to be added to this project
  target_link_libraries(${PROJECT_NAME}
      sub::lib1
      sub::lib2
  )
  ```

- [subbinary/main.cpp] - 可执行文件的源代码

  ```cpp
  #include "sublib1/sublib1.h"
  #include "sublib2/sublib2.h"
  
  int main(int argc, char *argv[])
  {
      sublib1 hi;
      hi.print();
  
      sublib2 howdy;
      howdy.print();
      
      return 0;
  }
  ```

- [sublibrary1/CMakeLists.txt] - 创建静态库

  ```cmake
  # Set the project name
  project (sublibrary1)
  
  # Add a library with the above sources
  add_library(${PROJECT_NAME} src/sublib1.cpp)
  add_library(sub::lib1 ALIAS ${PROJECT_NAME})
  
  target_include_directories( ${PROJECT_NAME}
      PUBLIC ${PROJECT_SOURCE_DIR}/include
  )
  ```

- [sublibrary1/include/sublib1/sublib1.h]

  ```cpp
  #ifndef __SUBLIB_1_H__
  #define __SUBLIB_1_H__
  
  class sublib1
  {
  public:
      void print();
  };
  
  #endif
  ```

- [sublibrary1/src/sublib1.cpp]

  ```cpp
  #include <iostream>
  
  #include "sublib1/sublib1.h"
  
  void sublib1::print()
  {
      std::cout << "Hello sub-library 1!" << std::endl;
  }
  ```

- [sublibrary2/CMakeLists.txt] - 设置仅含头文件的库

  ```cmake
  # Set the project name
  project (sublibrary2)
  
  add_library(${PROJECT_NAME} INTERFACE)
  add_library(sub::lib2 ALIAS ${PROJECT_NAME})
  
  target_include_directories(${PROJECT_NAME}
      INTERFACE
          ${PROJECT_SOURCE_DIR}/include
  )
  ```

- [sublibrary2/include/sublib2/sublib2.h]

  ```cpp
  #ifndef __SUBLIB_2_H__
  #define __SUBLIB_2_H__
  
  #include <iostream>
  
  class sublib2
  {
  public:
      void print()
      {
          std::cout << "Hello header only sub-library 2!" << std::endl;
      }
  };
  
  #endif
  ```

| Tip  | 在本例中，我将头文件移动到每个项目include目录下的一个子文件夹中，同时将目标include保留为根include文件夹。这是防止文件名冲突的好主意，因为你必须包含如下所示的文件：`#include“subib1/subib1.h”`。这也意味着，如果你为其他用户安装库，默认安装位置将是`/usr/local/include/subib1/subib1.h`。 |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

## 2.概念

### 2.1 添加子目录

CMakeLists.txt文件可以包含和调用含CMakeLists.txt的子目录

```cmake
add_subdirectory(sublibrary1)
add_subdirectory(sublibrary2)
add_subdirectory(subbinary)
```

### 2.2 引用子项目目录

当使用project()命令创建项目时，CMake将自动创建许多变量，这些变量可用于引用有关项目的详细信息。然后，其他子项目或主项目可以使用这些变量。例如，引用你可以使用的不同项目的源目录。

```cmake
    ${sublibrary1_SOURCE_DIR}
    ${sublibrary2_SOURCE_DIR}
```

CMake创建的变量包括：

| Variable           | Info                                                         |
| ------------------ | ------------------------------------------------------------ |
| PROJECT_NAME       | 由当前`project()`设置的项目名称                              |
| CMAKE_PROJECT_NAME | 由project()命令设置的第一个项目的名称，即顶级项目            |
| PROJECT_SOURCE_DIR | 当前项目的源目录                                             |
| PROJECT_BINARY_DIR | 当前项目的生成目录                                           |
| name_SOURCE_DIR    | 名为“name”的项目的源目录。在本例中，创建的源目录将是`sublibrary1_SOURCE_DIR`、`sublibrary2_SOURCE_DIR`和`subbinary_SOURCE_DIR` |
| name_BINARY_DIR    | 名为“name”的项目的二进制目录。在本例中，创建的二进制目录为`sublibrary1_BINARY_DIR` 、`sublibrary2_BINARY_DIR`和`subbinary_BINARY_DIR`。 |

### 2.3 头文件库

如果你有一个被创建为只包含头文件的库，cmake支持接口目标，以允许在没有任何构建输出的情况下创建目标。有关更多详细信息，请单击[此处](https://cmake.org/cmake/help/v3.4/command/add_library.html#interface-libraries)。

```cmake
add_library(${PROJECT_NAME} INTERFACE)
```

在创建目标时，你还可以使用INTERFACE作用域包含该目标的目录。INTERFACE作用域用于制定目标要求，这些要求在链接此目标的任何库中使用，但不用于目标本身的编译。如下示例，链接至此目标的任何目标都将包含一个include目录，但此目标本身并不进行编译（即不产生任何实体内容）：

```cmake
target_include_directories(${PROJECT_NAME}
    INTERFACE
        ${PROJECT_SOURCE_DIR}/include
)
```

### 2.4 从子项目中引用库

如果某个子项目创建库，则其他项目可以通过在`target_link_library()`命令中调用该项目的名称来引用该库。这意味着你不必引用新库的完整路径，它将作为依赖项被添加。

```cmake
target_link_libraries(subbinary
    PUBLIC
        sublibrary1
)
```

或者，你可以创建一个别名目标，使你可以在只读上下文中引用该目标。

要创建别名目标运行，请执行以下操作：

```cmake
add_library(sublibrary2)
add_library(sub::lib2 ALIAS sublibrary2)
```

要引用别名，只需如下所示：

```cmake
target_link_libraries(subbinary
    sub::lib2
)
```

### 2.5 包含来自子项目的头文件目录

当从子项目添加库时，从`cmake v3`开始，不需要使用它们在二进制文件的include目录中添加项目include目录。

这由创建库时`target_include_directory()`命令中的作用域控制。在本例中，因为子二进制可执行文件链接了subibrary1和subibrary2库，所以它将自动包括`${subibrary1_source_DIR}/include`和`${subibrary2_source_DIR}/include`文件夹，因为它们是随库的PUBLIC和INTERFACE范围导出的。

## 3.构建示例

```shell
$ mkdir build

$ cd build/

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/02-sub-projects/A-basic/build

$ make
Scanning dependencies of target sublibrary1
[ 50%] Building CXX object sublibrary1/CMakeFiles/sublibrary1.dir/src/sublib1.cpp.o
Linking CXX static library libsublibrary1.a
[ 50%] Built target sublibrary1
Scanning dependencies of target subbinary
[100%] Building CXX object subbinary/CMakeFiles/subbinary.dir/main.cpp.o
Linking CXX executable subbinary
[100%] Built target subbinary
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069693.html

# 【NO.276】CMake基础 第14节 在文件中进行变量替换

## 1.介绍

在调用cmake期间，可以创建使用CMakeLists.txt和cmake缓存中的变量的文件。在cmake生成期间，文件被复制到新位置，并替换所有cmake变量。

本教程中的文件如下：

```dos
$ tree
.
├── CMakeLists.txt
├── main.cpp
├── path.h.in
├── ver.h.in
```

- [CMakeLists.txt] - 包含要运行的CMake命令

  ```cmake
  cmake_minimum_required(VERSION 3.5)
  
  # Set the project name
  project (cf_example)
  
  # set a project version
  set (cf_example_VERSION_MAJOR 0)
  set (cf_example_VERSION_MINOR 2)
  set (cf_example_VERSION_PATCH 1)
  set (cf_example_VERSION "${cf_example_VERSION_MAJOR}.${cf_example_VERSION_MINOR}.${cf_example_VERSION_PATCH}")
  
  # Call configure files on ver.h.in to set the version.
  # Uses the standard ${VARIABLE} syntax in the file
  configure_file(ver.h.in ${PROJECT_BINARY_DIR}/ver.h)
  
  # configure the path.h.in file.
  # This file can only use the @VARIABLE@ syntax in the file
  configure_file(path.h.in ${PROJECT_BINARY_DIR}/path.h @ONLY)
  
  # Add an executable
  add_executable(cf_example
      main.cpp
  )
  
  # include the directory with the new files
  target_include_directories( cf_example
      PUBLIC
          ${CMAKE_BINARY_DIR}
  )
  ```

- [main.cpp] - 具有main的源文件

  ```cpp
  #include <iostream>
  #include "ver.h"
  #include "path.h"
  
  int main(int argc, char *argv[])
  {
      std::cout << "Hello Version " << ver << "!" << std::endl;
      std::cout << "Path is " << path << std::endl;
     return 0;
  }
  ```

- [path.h.in] - 包含构建目录路径的文件

  ```cpp
  #ifndef __PATH_H__
  #define __PATH_H__
  
  // version variable that will be substituted by cmake
  // This shows an example using the @ variable type
  const char* path = "@CMAKE_SOURCE_DIR@";
  
  #endif
  ```

- [ver.h.in] - 包含项目版本的文件

  ```cpp
  #ifndef __VER_H__
  #define __VER_H__
  
  // version variable that will be substituted by cmake
  // This shows an example using the $ variable type
  const char* ver = "${cf_example_VERSION}";
  
  #endif
  ```

## 2.概念

### 2.1 配置文件

要在文件中进行变量替换，可以使用CMake中的`configure_file()`函数。此函数的核心参数是源文件和目标文件。

```cmake
configure_file(ver.h.in ${PROJECT_BINARY_DIR}/ver.h)

configure_file(path.h.in ${PROJECT_BINARY_DIR}/path.h @ONLY)
```

上面的第一个示例允许使用像CMake变量一样的`${}`语法或ver.h.in文件中的`@@`定义变量。生成后，新文件ver.h将在`PROJECT_BINARY_DIR`中可用。

```cpp
const char* ver = "${cf_example_VERSION}";
```

第二个示例只允许在path.h.in文件中使用`@@`语法定义变量。生成后，将在`PROJECT_BINARY_DIR`中提供新文件path.h。

```cpp
const char* path = "@CMAKE_SOURCE_DIR@";
```

## 3.构建示例

```shell
$ mkdir build

$ cd build/

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/03-code-generation/configure-files/build

$ ls
CMakeCache.txt  CMakeFiles  cmake_install.cmake  Makefile  path.h  ver.h

$ cat path.h
#ifndef __PATH_H__
#define __PATH_H__

// version variable that will be substituted by cmake
// This shows an example using the @ variable type
const char* path = "/home/matrim/workspace/cmake-examples/03-code-generation/configure-files";

#endif

$ cat ver.h
#ifndef __VER_H__
#define __VER_H__

// version variable that will be substituted by cmake
// This shows an example using the $ variable type
const char* ver = "0.2.1";

#endif

$ make
Scanning dependencies of target cf_example
[100%] Building CXX object CMakeFiles/cf_example.dir/main.cpp.o
Linking CXX executable cf_example
[100%] Built target cf_example

$ ./cf_example
Hello Version 0.2.1!
Path is /home/matrim/workspace/cmake-examples/03-code-generation/configure-files
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069698.html

# 【NO.277】CMake基础 第15节 使用Protobuf生成源文件

## 1.介绍

这个例子展示了如何使用Protobuf生成源文件。Protocol Buffers是Google提供的一种数据序列化格式。用户提供带有数据描述的.proto文件。然后使用Protobuf编译器，可以将该原始文件翻译成包括C++在内的多种语言的源代码。

本教程中的文件如下：

```objectivec
$ tree
.
├── AddressBook.proto
├── CMakeLists.txt
├── main.cpp
```

- [AddressBook.proto] - 来自main protocol buffer示例的proto文件

  ```protobuf
  package tutorial;
  
  message Person {
    required string name = 1;
    required int32 id = 2;
    optional string email = 3;
  
    enum PhoneType {
      MOBILE = 0;
      HOME = 1;
      WORK = 2;
    }
  
    message PhoneNumber {
      required string number = 1;
      optional PhoneType type = 2 [default = HOME];
    }
  
    repeated PhoneNumber phone = 4;
  }
  
  message AddressBook {
    repeated Person person = 1;
  }
  ```

- [CMakeLists.txt] - 包含要运行的CMake命令

  ```cmake
  cmake_minimum_required(VERSION 3.5)
  
  # Set the project name
  project (protobuf_example)
  
  # find the protobuf compiler and libraries
  find_package(Protobuf REQUIRED)
  
  # check if protobuf was found
  if(PROTOBUF_FOUND)
      message ("protobuf found")
  else()
      message (FATAL_ERROR "Cannot find Protobuf")
  endif()
  
  # Generate the .h and .cxx files
  PROTOBUF_GENERATE_CPP(PROTO_SRCS PROTO_HDRS AddressBook.proto)
  
  # Print path to generated files
  message ("PROTO_SRCS = ${PROTO_SRCS}")
  message ("PROTO_HDRS = ${PROTO_HDRS}")
  
  # Add an executable
  add_executable(protobuf_example
      main.cpp
      ${PROTO_SRCS}
      ${PROTO_HDRS})
  
  target_include_directories(protobuf_example
      PUBLIC
      ${PROTOBUF_INCLUDE_DIRS}
      ${CMAKE_CURRENT_BINARY_DIR}
  )
  
  # link the exe against the libraries
  target_link_libraries(protobuf_example
      PUBLIC
      ${PROTOBUF_LIBRARIES}
  )
  ```

- [main.cpp] - protobuf示例的源文件.

  ```cpp
  #include <iostream>
  #include <fstream>
  #include <string>
  #include "AddressBook.pb.h"
  using namespace std;
  
  // This function fills in a Person message based on user input.
  void PromptForAddress(tutorial::Person* person) {
    cout << "Enter person ID number: ";
    int id;
    cin >> id;
    person->set_id(id);
    cin.ignore(256, '\n');
  
    cout << "Enter name: ";
    getline(cin, *person->mutable_name());
  
    cout << "Enter email address (blank for none): ";
    string email;
    getline(cin, email);
    if (!email.empty()) {
      person->set_email(email);
    }
  
    while (true) {
      cout << "Enter a phone number (or leave blank to finish): ";
      string number;
      getline(cin, number);
      if (number.empty()) {
        break;
      }
  
      tutorial::Person::PhoneNumber* phone_number = person->add_phone();
      phone_number->set_number(number);
  
      cout << "Is this a mobile, home, or work phone? ";
      string type;
      getline(cin, type);
      if (type == "mobile") {
        phone_number->set_type(tutorial::Person::MOBILE);
      } else if (type == "home") {
        phone_number->set_type(tutorial::Person::HOME);
      } else if (type == "work") {
        phone_number->set_type(tutorial::Person::WORK);
      } else {
        cout << "Unknown phone type.  Using default." << endl;
      }
    }
  }
  
  // Main function:  Reads the entire address book from a file,
  //   adds one person based on user input, then writes it back out to the same
  //   file.
  int main(int argc, char* argv[]) {
    // Verify that the version of the library that we linked against is
    // compatible with the version of the headers we compiled against.
    GOOGLE_PROTOBUF_VERIFY_VERSION;
  
    if (argc != 2) {
      cerr << "Usage:  " << argv[0] << " ADDRESS_BOOK_FILE" << endl;
      return -1;
    }
  
    tutorial::AddressBook address_book;
  
    {
      // Read the existing address book.
      fstream input(argv[1], ios::in | ios::binary);
      if (!input) {
        cout << argv[1] << ": File not found.  Creating a new file." << endl;
      } else if (!address_book.ParseFromIstream(&input)) {
        cerr << "Failed to parse address book." << endl;
        return -1;
      }
    }
  
    // Add an address.
    PromptForAddress(address_book.add_person());
  
    {
      // Write the new address book back to disk.
      fstream output(argv[1], ios::out | ios::trunc | ios::binary);
      if (!address_book.SerializeToOstream(&output)) {
        cerr << "Failed to write address book." << endl;
        return -1;
      }
    }
  
    // Optional:  Delete all global objects allocated by libprotobuf.
    google::protobuf::ShutdownProtobufLibrary();
  
    return 0;
  }
  ```

## 2.要求

此示例需要安装protocol buffers二进制文件和库。可以使用以下命令将其安装在Ubuntu上。

```shell
sudo apt-get install protobuf-compiler libprotobuf-dev
```

## 3.概念

### 3.1 导出变量

由CMake Protobuf包导出并在此示例中使用的变量包括:

- `PROTOBUF_FOUND` - 如果安装了Protocol Buffers
- `PROTOBUF_INCLUDE_DIRS` - protobuf的头文件
- `PROTOBUF_LIBRARIES` - protobuf库

此外，还可以通过查看FindProtobuf.cmake文件顶部的内容找到定义的更多变量。

### 3.2 生成源代码

Protobuf CMake包包含许多帮助函数，以简化代码生成。在本例中，我们生成的是C++源代码，使用以下代码：

```cmake
PROTOBUF_GENERATE_CPP(PROTO_SRCS PROTO_HDRS AddressBook.proto)
```

这些参数包括:

- PROTO_SRCS - 存储.pb.cc文件的变量名称
- PROTO_HDRS- 存储.pb.h文件的变量名称
- AddressBook.proto - 从中生成代码的.proto文件

### 3.3 生成文件

在调用`PROTOBUF_GENERATE_CPP`函数之后，你将拥有上面提到的变量。这些将被标记为自定义命令的输出，该命令将调用Protobuf编译器来生成它们。

要生成这些文件，你应该将它们添加到库或可执行文件中。

例如：

```cmake
add_executable(protobuf_example
    main.cpp
    ${PROTO_SRCS}
    ${PROTO_HDRS})
```

这将导致在对该可执行文件目标调用make时调用protocol buf编译器。

对.proto文件进行更改后，将再次自动生成关联的源文件。但是，如果没有对.proto文件进行任何更改，并且你重新运行make，则不会执行任何操作。

## 4.构建示例

```shell
$ mkdir build

$ cd build/

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Looking for include file pthread.h
-- Looking for include file pthread.h - found
-- Looking for pthread_create
-- Looking for pthread_create - not found
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - found
-- Found Threads: TRUE
-- Found PROTOBUF: /usr/lib/x86_64-linux-gnu/libprotobuf.so
protobuf found
PROTO_SRCS = /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/AddressBook.pb.cc
PROTO_HDRS = /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/AddressBook.pb.h
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build

$ ls
CMakeCache.txt  CMakeFiles  cmake_install.cmake  Makefile

$ make VERBOSE=1
/usr/bin/cmake -H/home/matrim/workspace/cmake-examples/03-code-generation/protobuf -B/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
make -f CMakeFiles/protobuf_example.dir/build.make CMakeFiles/protobuf_example.dir/depend
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles 1
[ 33%] Running C++ protocol buffer compiler on AddressBook.proto
/usr/bin/protoc --cpp_out /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build -I /home/matrim/workspace/cmake-examples/03-code-generation/protobuf /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/AddressBook.proto
cd /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /home/matrim/workspace/cmake-examples/03-code-generation/protobuf /home/matrim/workspace/cmake-examples/03-code-generation/protobuf /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/protobuf_example.dir/DependInfo.cmake --color=
Dependee "/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/protobuf_example.dir/DependInfo.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/protobuf_example.dir/depend.internal".
Dependee "/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/CMakeDirectoryInformation.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/protobuf_example.dir/depend.internal".
Scanning dependencies of target protobuf_example
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
make -f CMakeFiles/protobuf_example.dir/build.make CMakeFiles/protobuf_example.dir/build
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles 2
[ 66%] Building CXX object CMakeFiles/protobuf_example.dir/main.cpp.o
/usr/bin/c++    -I/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build    -o CMakeFiles/protobuf_example.dir/main.cpp.o -c /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/main.cpp
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles 3
[100%] Building CXX object CMakeFiles/protobuf_example.dir/AddressBook.pb.cc.o
/usr/bin/c++    -I/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build    -o CMakeFiles/protobuf_example.dir/AddressBook.pb.cc.o -c /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/AddressBook.pb.cc
Linking CXX executable protobuf_example
/usr/bin/cmake -E cmake_link_script CMakeFiles/protobuf_example.dir/link.txt --verbose=1
/usr/bin/c++       CMakeFiles/protobuf_example.dir/main.cpp.o CMakeFiles/protobuf_example.dir/AddressBook.pb.cc.o  -o protobuf_example -rdynamic -lprotobuf -lpthread
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles  1 2 3
[100%] Built target protobuf_example
make[1]: Leaving directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles 0
$ make VERBOSE=1
/usr/bin/cmake -H/home/matrim/workspace/cmake-examples/03-code-generation/protobuf -B/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build --check-build-system CMakeFiles/Makefile.cmake 0
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/progress.marks
make -f CMakeFiles/Makefile2 all
make[1]: Entering directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
make -f CMakeFiles/protobuf_example.dir/build.make CMakeFiles/protobuf_example.dir/depend
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles 1
[ 33%] Running C++ protocol buffer compiler on AddressBook.proto
/usr/bin/protoc --cpp_out /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build -I /home/matrim/workspace/cmake-examples/03-code-generation/protobuf /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/AddressBook.proto
cd /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build && /usr/bin/cmake -E cmake_depends "Unix Makefiles" /home/matrim/workspace/cmake-examples/03-code-generation/protobuf /home/matrim/workspace/cmake-examples/03-code-generation/protobuf /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/protobuf_example.dir/DependInfo.cmake --color=
Dependee "/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/protobuf_example.dir/DependInfo.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/protobuf_example.dir/depend.internal".
Dependee "/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/CMakeDirectoryInformation.cmake" is newer than depender "/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles/protobuf_example.dir/depend.internal".
Scanning dependencies of target protobuf_example
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
make -f CMakeFiles/protobuf_example.dir/build.make CMakeFiles/protobuf_example.dir/build
make[2]: Entering directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles 2
[ 66%] Building CXX object CMakeFiles/protobuf_example.dir/main.cpp.o
/usr/bin/c++    -I/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build    -o CMakeFiles/protobuf_example.dir/main.cpp.o -c /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/main.cpp
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles 3
[100%] Building CXX object CMakeFiles/protobuf_example.dir/AddressBook.pb.cc.o
/usr/bin/c++    -I/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build    -o CMakeFiles/protobuf_example.dir/AddressBook.pb.cc.o -c /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/AddressBook.pb.cc
Linking CXX executable protobuf_example
/usr/bin/cmake -E cmake_link_script CMakeFiles/protobuf_example.dir/link.txt --verbose=1
/usr/bin/c++       CMakeFiles/protobuf_example.dir/main.cpp.o CMakeFiles/protobuf_example.dir/AddressBook.pb.cc.o  -o protobuf_example -rdynamic -lprotobuf -lpthread
make[2]: Leaving directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
/usr/bin/cmake -E cmake_progress_report /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles  1 2 3
[100%] Built target protobuf_example
make[1]: Leaving directory `/home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build'
/usr/bin/cmake -E cmake_progress_start /home/matrim/workspace/cmake-examples/03-code-generation/protobuf/build/CMakeFiles 0

$ ls
AddressBook.pb.cc  CMakeCache.txt  cmake_install.cmake  protobuf_example
AddressBook.pb.h   CMakeFiles      Makefile

$ ./protobuf_example test.db
test.db: File not found.  Creating a new file.
Enter person ID number: 11
Enter name: John Doe
Enter email address (blank for none): wolly@sheep.ie
Enter a phone number (or leave blank to finish):

$ ls
AddressBook.pb.cc  CMakeCache.txt  cmake_install.cmake  protobuf_example
AddressBook.pb.h   CMakeFiles      Makefile             test.db
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069714.html

# 【NO.278】CMake基础 第16节 创建deb文件 

## 1.介绍

此示例显示如何使用deb格式生成Linux安装程序。

本教程中的文件如下：

```ruby
$ tree
.
├── cmake-examples.conf
├── CMakeLists.txt
├── include
│   └── Hello.h
└── src
    ├── Hello.cpp
    └── main.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令。

  ```cmake
  cmake_minimum_required(VERSION 3.5)
  
  project(cmake_examples_deb)
  
  # set a project version
  set (deb_example_VERSION_MAJOR 0)
  set (deb_example_VERSION_MINOR 2)
  set (deb_example_VERSION_PATCH 2)
  set (deb_example_VERSION "${deb_example_VERSION_MAJOR}.${deb_example_VERSION_MINOR}.${deb_example_VERSION_PATCH}")
  
  
  ############################################################
  # Create a library
  ############################################################
  
  #Generate the shared library from the library sources
  add_library(cmake_examples_deb SHARED src/Hello.cpp)
  
  target_include_directories(cmake_examples_deb
      PUBLIC
          ${PROJECT_SOURCE_DIR}/include
  )
  ############################################################
  # Create an executable
  ############################################################
  
  # Add an executable with the above sources
  add_executable(cmake_examples_deb_bin src/main.cpp)
  
  # link the new hello_library target with the hello_binary target
  target_link_libraries( cmake_examples_deb_bin
      PUBLIC
          cmake_examples_deb
  )
  
  ############################################################
  # Install
  ############################################################
  
  # Binaries
  install (TARGETS cmake_examples_deb_bin
      DESTINATION bin)
  
  # Library
  # Note: may not work on windows
  install (TARGETS cmake_examples_deb
      LIBRARY DESTINATION lib)
  
  # Config
  install (FILES cmake-examples.conf
      DESTINATION etc)
  
  ############################################################
  # Create DEB
  ############################################################
  
  # Tell CPack to generate a .deb package
  set(CPACK_GENERATOR "DEB")
  
  # Set a Package Maintainer.
  # This is required
  set(CPACK_DEBIAN_PACKAGE_MAINTAINER "Thom Troy")
  
  # Set a Package Version
  set(CPACK_PACKAGE_VERSION ${deb_example_VERSION})
  
  # Include CPack
  include(CPack)
  ```

- [cmake-examples.conf] - 示例配置文件。

  ```ini
  # Sample configuration file that could be installed
  ```

- [include/Hello.h] - 要包含的头文件。

  ```cpp
  #ifndef __HELLO_H__
  #define __HELLO_H__
  
  class Hello
  {
  public:
      void print();
  };
  
  #endif
  ```

- [src/Hello.cpp] - 要编译的源文件。

  ```cpp
  #include <iostream>
  
  #include "Hello.h"
  
  void Hello::print()
  {
      std::cout << "Hello Install!" << std::endl;
  }
  ```

- [src/main.cpp] - 具有main的源文件。

  ```cpp
  #include "Hello.h"
  
  int main(int argc, char *argv[])
  {
      Hello hi;
      hi.print();
      return 0;
  }
  ```

## 2.概念

### 2.1 生成器

`make package`目标可以使用CPack生成器来创建安装程序。

对于Debian包，你可以使用以下命令告诉CMake创建一个生成器：

```cmake
set(CPACK_GENERATOR "DEB")
```

在设置了描述软件包的各种设置之后，你必须使用以下命令告诉CMake包含CPack生成器。

```cmake
include(CPack)
```

包含后，通常使用make install目标安装的所有文件现在都可以打包到Debian包中。

### 2.2 Debian包设置

CPack公开了软件包的各种设置。在本例中，我们设置了以下内容：

```cmake
# Set a Package Maintainer.
# This is required
set(CPACK_DEBIAN_PACKAGE_MAINTAINER "Thom Troy")

# Set a Package Version
set(CPACK_PACKAGE_VERSION ${deb_example_VERSION})
```

它设置维护人员和版本。下面指定了更多Debian特定的设置。

| Variable                           | Info                                                         |
| ---------------------------------- | ------------------------------------------------------------ |
| CPACK_DEBIAN_PACKAGE_MAINTAINER    | Maintainer information                                       |
| CPACK_PACKAGE_DESCRIPTION_SUMMARY  | Package short description                                    |
| CPACK_PACKAGE_DESCRIPTION          | Package description                                          |
| CPACK_DEBIAN_PACKAGE_DEPENDS       | For advanced users to add custom scripts.                    |
| CPACK_DEBIAN_PACKAGE_CONTROL_EXTRA | The build directory you are currently in.                    |
| CPACK_DEBIAN_PACKAGE_SECTION       | Package section (see [here](http://packages.debian.org/stable/)) |
| CPACK_DEBIAN_PACKAGE_VERSION       | Package version                                              |

## 3.构建示例

```shell
$ mkdir build

$ cd build/

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/06-installer/deb/build

$ make help
The following are some of the valid targets for this Makefile:
... all (the default if no target is provided)
... clean
... depend
... cmake_examples_deb
... cmake_examples_deb_bin
... edit_cache
... install
... install/local
... install/strip
... list_install_components
... package
... package_source
... rebuild_cache
... src/Hello.o
... src/Hello.i
... src/Hello.s
... src/main.o
... src/main.i
... src/main.s

$ make package
Scanning dependencies of target cmake_examples_deb
[ 50%] Building CXX object CMakeFiles/cmake_examples_deb.dir/src/Hello.cpp.o
Linking CXX shared library libcmake_examples_deb.so
[ 50%] Built target cmake_examples_deb
Scanning dependencies of target cmake_examples_deb_bin
[100%] Building CXX object CMakeFiles/cmake_examples_deb_bin.dir/src/main.cpp.o
Linking CXX executable cmake_examples_deb_bin
[100%] Built target cmake_examples_deb_bin
Run CPack packaging tool...
CPack: Create package using DEB
CPack: Install projects
CPack: - Run preinstall target for: cmake_examples_deb
CPack: - Install project: cmake_examples_deb
CPack: Create package
CPack: - package: /home/matrim/workspace/cmake-examples/06-installer/deb/build/cmake_examples_deb-0.2.2-Linux.deb generated.

$ ls
CMakeCache.txt  cmake_examples_deb-0.2.2-Linux.deb  cmake_examples_deb_bin  CMakeFiles  cmake_install.cmake  CPackConfig.cmake  _CPack_Packages  CPackSourceConfig.cmake  install_manifest.txt  libcmake_examples_deb.so  Makefile
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15069925.html

# 【NO.279】CMake基础 第17节 Clang分析器

## 1.介绍

此示例说明如何调用Clang Static Analyzer以使用scan-build工具执行静态分析。

此示例中包含的文件包括：

```shell
$ tree
.
├── CMakeLists.txt
├── subproject1
│   ├── CMakeLists.txt
│   └── main1.cpp
└── subproject2
    ├── CMakeLists.txt
    └── main2.cpp
```

- [CMakeLists.txt] - 顶级CMakeLists.txt。

  ```cmake
  cmake_minimum_required (VERSION 3.5)
  
  project(cppcheck_analysis)
  
  # Use debug build as recommended
  set(CMAKE_BUILD_TYPE Debug)
  
  # Have cmake create a compile database
  set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
  
  # Add sub directories
  add_subdirectory(subproject1)
  add_subdirectory(subproject2)
  ```

- [subproject1/CMakeLists.txt] - C子项目1的Make命令。

  ```cmake
  # Set the project name
  project (subproject1)
  
  # Add an executable with the above sources
  add_executable(${PROJECT_NAME} main1.cpp)
  ```

- [subproject1/main.cpp] - 子项目的源代码，没有错误。

  ```cpp
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
     std::cout << "Hello Main1!" << std::endl;
     return 0;
  }
  ```

- [subproject2/CMakeLists.txt] - C子项目2的Make命令。

  ```cmake
  # Set the project name
  project (subproject2)
  
  # Add an executable with the above sources
  add_executable(${PROJECT_NAME} main2.cpp)
  ```

- [subproject2/main2.cpp] - 包含错误的子项目的源代码。

  ```cpp
  #include <iostream>
  
  int main(int argc, char *argv[])
  {
     std::cout << "Hello Main2!" << std::endl;
     int* x = NULL;
     std::cout << *x << std::endl;
     return 0;
  }
  ```

## 2.要求

要运行此示例，必须安装CLANG分析器和scan-build工具。可以使用以下命令将其安装在Ubuntu上。

```shell
$ sudo apt-get install clang
$ sudo apt-get install clang-tools
```

该工具以下方式可用：

```shell
$ scan-build-3.6
```

## 3.概念

### 3.1 scan-build

要运行clang静态分析器，你可以在运行编译器的同时使用工具Scan-Build来运行分析器。 这会覆盖CC和CXX环境变量，并用它自己的工具替换它们。要运行它，你可以执行以下操作：

```shell
$ scan-build-3.6 cmake ..
$ scan-build-3.6 make
```

默认情况下，这将运行你的平台的标准编译器，即LINUX上的GCC。但是，如果要覆盖此设置，可以将命令更改为：

```shell
$ scan-build-3.6 --use-cc=clang-3.6 --use-c++=clang++-3.6 -o ./scanbuildout/ make
```

### 3.2 scan-build输出

scan-build仅在编译时输出警告，还将生成包含错误详细分析的HTML文件列表。

```shell
$ cd scanbuildout/
$ tree
.
└── 2017-07-03-213514-3653-1
    ├── index.html
    ├── report-42eba1.html
    ├── scanview.css
    └── sorttable.js

1 directory, 4 files
```

默认情况下，这些文件输出到`/tmp/scanbuildout/{run folder}`。你可以使用 `scan-build -o /output/folder`文件夹更改此设置。

## 4.构建示例

```shell
$ mkdir build

$ cd build/

$ scan-build-3.6 -o ./scanbuildout make
scan-build: Using '/usr/lib/llvm-3.6/bin/clang' for static analysis
make: *** No targets specified and no makefile found.  Stop.
scan-build: Removing directory '/data/code/clang-analyzer/build/scanbuildout/2017-07-03-211632-937-1' because it contains no reports.
scan-build: No bugs found.
devuser@91457fbfa423:/data/code/clang-analyzer/build$ scan-build-3.6 -o ./scanbuildout cmake ..
scan-build: Using '/usr/lib/llvm-3.6/bin/clang' for static analysis
-- The C compiler identification is GNU 5.4.0
-- The CXX compiler identification is GNU 5.4.0
-- Check for working C compiler: /usr/share/clang/scan-build-3.6/ccc-analyzer
-- Check for working C compiler: /usr/share/clang/scan-build-3.6/ccc-analyzer -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Detecting C compile features
-- Detecting C compile features - done
-- Check for working CXX compiler: /usr/share/clang/scan-build-3.6/c++-analyzer
-- Check for working CXX compiler: /usr/share/clang/scan-build-3.6/c++-analyzer -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found CPPCHECK: /usr/local/bin/cppcheck
cppcheck found. Use cppccheck-analysis targets to run it
-- Configuring done
-- Generating done
-- Build files have been written to: /data/code/clang-analyzer/build
scan-build: Removing directory '/data/code/clang-analyzer/build/scanbuildout/2017-07-03-211641-941-1' because it contains no reports.
scan-build: No bugs found.

$ $ scan-build-3.6 -o ./scanbuildout make
scan-build: Using '/usr/lib/llvm-3.6/bin/clang' for static analysis
Scanning dependencies of target subproject1
[ 25%] Building CXX object subproject1/CMakeFiles/subproject1.dir/main1.cpp.o
[ 50%] Linking CXX executable subproject1
[ 50%] Built target subproject1
Scanning dependencies of target subproject2
[ 75%] Building CXX object subproject2/CMakeFiles/subproject2.dir/main2.cpp.o
/data/code/clang-analyzer/subproject2/main2.cpp:7:17: warning: Dereference of null pointer (loaded from variable 'x')
   std::cout << *x << std::endl;
                ^~
1 warning generated.
[100%] Linking CXX executable subproject2
[100%] Built target subproject2
scan-build: 1 bug found.
scan-build: Run 'scan-view /data/code/clang-analyzer/build/scanbuildout/2017-07-03-211647-1172-1' to examine bug reports.

$ cd scanbuildout/
$ tree
.
└── 2017-07-03-213514-3653-1
    ├── index.html
    ├── report-42eba1.html
    ├── scanview.css
    └── sorttable.js

1 directory, 4 files
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15072093.html

# 【NO.280】CMake基础 第18节 Boost单元测试框架

## 1.介绍

使用CTest，你可以生成`make test`目标来运行自动化单元测试。这个例子展示了如何找到Boost单元测试框架，创建测试并运行它们。

本教程中的文件如下：

```shell
$ tree
.
├── CMakeLists.txt
├── Reverse.h
├── Reverse.cpp
├── Palindrome.h
├── Palindrome.cpp
├── unit_tests.cpp
```

- [CMakeLists.txt] - 包含要运行的CMake命令。

  ```cmake
  cmake_minimum_required(VERSION 3.5)
  
  # Set the project name
  project (boost_unit_test)
  
  
  # find a boost install with the libraries unit_test_framework
  find_package(Boost 1.46.1 REQUIRED COMPONENTS unit_test_framework)
  
  # Add an library for the example classes
  add_library(example_boost_unit_test
      Reverse.cpp
      Palindrome.cpp
  )
  
  target_include_directories(example_boost_unit_test
      PUBLIC
          ${CMAKE_CURRENT_SOURCE_DIR}
  )
  
  target_link_libraries(example_boost_unit_test
      PUBLIC
          Boost::boost
  )
  
  #############################################
  # Unit tests
  
  # enable CTest testing
  enable_testing()
  
  # Add a testing executable
  add_executable(unit_tests unit_tests.cpp)
  
  target_link_libraries(unit_tests
      example_boost_unit_test
      Boost::unit_test_framework
  )
  
  target_compile_definitions(unit_tests
      PRIVATE
          BOOST_TEST_DYN_LINK
  )
  
  add_test(test_all unit_tests)
  ```

- [Reverse.h] / [.cpp] - 反转字符串的类。

  ```cpp
  #ifndef __REVERSE_H__
  #define __REVERSE_H__
  
  #include <string>
  
  /**
   * Trivial class whose only function is to reverse a string.
   * Should use std::reverse instead but want to have example with own class
   */
  class Reverse
  {
  public:
      std::string reverse(std::string& toReverse);
  };
  #endif
  ```

  ```cpp
  #include "Reverse.h"
  
  std::string Reverse::reverse(std::string& toReverse)
  {
      std::string ret;
  
      for(std::string::reverse_iterator rit=toReverse.rbegin(); rit!=toReverse.rend(); ++rit)
      {
          ret.insert(ret.end(), *rit);
      }
      return ret;
  }
  ```

- [Palindrome.h] / [.cpp] - 测试字符串是否为回文的类。

  ```cpp
  #ifndef __PALINDROME_H__
  #define __PALINDROME_H__
  
  #include <string>
  
  /**
   * Trivial class to check if a string is a palindrome.
   */
  class Palindrome
  {
  public:
      bool isPalindrome(const std::string& toCheck);
  };
  
  #endif
  ```

  ```cpp
  #include "Palindrome.h"
  
  bool Palindrome::isPalindrome(const std::string& toCheck)
  {
  
      if (toCheck == std::string(toCheck.rbegin(), toCheck.rend())) {
          return true;
      }
  
      return false;
  }
  ```

- [unit_tests.cpp] - 使用Boost单元测试框架的单元测试。

  ```cpp
  #include <string>
  #include "Reverse.h"
  #include "Palindrome.h"
  
  #define BOOST_TEST_MODULE VsidCommonTest
  #include <boost/test/unit_test.hpp>
  
  BOOST_AUTO_TEST_SUITE( reverse_tests )
  
  BOOST_AUTO_TEST_CASE( simple )
  {
      std::string toRev = "Hello";
  
      Reverse rev;
      std::string res = rev.reverse(toRev);
  
      BOOST_CHECK_EQUAL( res, "olleH" );
  
  }
  
  
  BOOST_AUTO_TEST_CASE( empty )
  {
      std::string toRev;
  
      Reverse rev;
      std::string res = rev.reverse(toRev);
  
      BOOST_CHECK_EQUAL( res, "" );
  }
  
  BOOST_AUTO_TEST_SUITE_END()
  
  
  BOOST_AUTO_TEST_SUITE( palindrome_tests )
  
  BOOST_AUTO_TEST_CASE( is_palindrome )
  {
      std::string pal = "mom";
      Palindrome pally;
  
      BOOST_CHECK_EQUAL( pally.isPalindrome(pal), true );
  
  }
  
  BOOST_AUTO_TEST_SUITE_END()
  ```

## 2.要求

此示例要求将Boost库安装在默认系统位置。

使用的库是Boost单元测试框架。

## 3.概念

### 3.1 启用测试

要启用测试，你必须在顶级CMakeLists.txt中包含以下行。

```cmake
enable_testing()
```

这将启用对当前文件夹及其下面所有文件夹的测试。

### 3.2 添加测试可执行文件

此步骤的要求将取决于你的单元测试框架。在Boost的示例中，你可以创建包含要运行的所有单元测试的二进制文件。

```cmake
add_executable(unit_tests unit_tests.cpp)

target_link_libraries(unit_tests
    example_boost_unit_test
    Boost::unit_test_framework
)

target_compile_definitions(unit_tests
    PRIVATE
        BOOST_TEST_DYN_LINK
)
```

在上面的代码中，添加了一个单元测试二进制文件，它链接到Boost单元测试框架，并包含一个定义来告诉它我们正在使用动态链接。

### 3.3 添加测试

要添加测试，可以调用`add_test()`函数。这将创建一个命名测试，该测试将运行提供的命令。

```cmake
add_test(test_all unit_tests)
```

在本例中，我们创建了一个名为test_all的测试，该测试将运行由调用`add_executable`创建的unit_test可执行文件创建的可执行文件。

## 4.构建示例

```shell
$ mkdir build

$ cd build/

$ cmake ..
-- The C compiler identification is GNU 4.8.4
-- The CXX compiler identification is GNU 4.8.4
-- Check for working C compiler: /usr/bin/cc
-- Check for working C compiler: /usr/bin/cc -- works
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++
-- Check for working CXX compiler: /usr/bin/c++ -- works
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Boost version: 1.54.0
-- Found the following Boost libraries:
--   unit_test_framework
-- Configuring done
-- Generating done
-- Build files have been written to: /home/matrim/workspace/cmake-examples/05-unit-testing/boost/build

$ make
Scanning dependencies of target example_boost_unit_test
[ 33%] Building CXX object CMakeFiles/example_boost_unit_test.dir/Reverse.cpp.o
[ 66%] Building CXX object CMakeFiles/example_boost_unit_test.dir/Palindrome.cpp.o
Linking CXX static library libexample_boost_unit_test.a
[ 66%] Built target example_boost_unit_test
Scanning dependencies of target unit_tests
[100%] Building CXX object CMakeFiles/unit_tests.dir/unit_tests.cpp.o
Linking CXX executable unit_tests
[100%] Built target unit_tests

$ make test
Running tests...
Test project /home/matrim/workspace/cmake-examples/05-unit-testing/boost/build
    Start 1: test_all
1/1 Test #1: test_all .........................   Passed    0.00 sec

100% tests passed, 0 tests failed out of 1

Total Test time (real) =   0.01 sec
```

如果更改代码并导致单元测试产生错误。然后，在运行测试时，你将看到以下输出。

```shell
$ make test
Running tests...
Test project /home/matrim/workspace/cmake-examples/05-unit-testing/boost/build
    Start 1: test_all
1/1 Test #1: test_all .........................***Failed    0.00 sec

0% tests passed, 1 tests failed out of 1

Total Test time (real) =   0.00 sec

The following tests FAILED:
          1 - test_all (Failed)
Errors while running CTest
make: *** [test] Error 8
```

原文作者：[橘崽崽啊](https://www.cnblogs.com/juzaizai/)

原文链接：https://www.cnblogs.com/juzaizai/p/15072101.html



# 【NO.281】五种网络IO模型详解

## 1. IO操作本质

数据复制的过程中不会消耗CPU

\# 1 内存分为内核缓冲区和用户缓冲区

\# 2 用户的应用程序不能直接操作内核缓冲区，需要将数据从内核拷贝到用户才能使用

\# 3 而IO操作、网络请求加载到内存的数据一开始是放在内核缓冲区的

![img](https://pic3.zhimg.com/80/v2-767a42e4fc58f1543acb005db92dcd3a_720w.webp)

## 2.IO模型

### 2.1. BIO – 阻塞模式I/O

用户进程从发起请求，到最终拿到数据前，一直挂起等待； 数据会由用户进程完成拷贝

```
'''
举个例子：一个人去 商店买一把菜刀，
他到商店问老板有没有菜刀（发起系统调用）
如果有（表示在内核缓冲区有需要的数据）
老板直接把菜刀给买家（从内核缓冲区拷贝到用户缓冲区）
这个过程买家一直在等待
如果没有，商店老板会向工厂下订单（IO操作，等待数据准备好）
工厂把菜刀运给老板（进入到内核缓冲区）
老板把菜刀给买家（从内核缓冲区拷贝到用户缓冲区）
这个过程买家一直在等待
是同步io
'''
```

![img](https://pic4.zhimg.com/80/v2-dd42dccb828c389e1b7bf0b77bd968e7_720w.webp)

```
import socket
server = socket.socket()
server.bind(('127.0.0.1',8080))
server.listen(5)
while True:
    conn, addr = server.accept()
    while True:
        try:
            data = conn.recv(1024)
            if len(data) == 0:break
            print(data)
            conn.send(data.upper())
        except ConnectionResetError as e:
            break
    conn.close()
# 在服务端开设多进程或者多线程 进程池线程池 其实还是没有解决IO问题    
该等的地方还是得等 没有规避
只不过多个人等待的彼此互不干扰
```

### 2.2. NIO – 非阻塞模式I/O

用户进程发起请求，如果数据没有准备好，那么立刻告知用户进程未准备好；此时用户进程可选择继续发起请求、或者先去做其他事情，稍后再回来继续发请求，直到被告知数据准备完毕，可以开始接收为止； 数据会由用户进程完成拷贝

```
'''
举个例子：一个人去 商店买一把菜刀，
他到商店问老板有没有菜刀（发起系统调用）
老板说没有，在向工厂进货（返回状态）
买家去别地方玩了会，又回来问，菜刀到了么（发起系统调用）
老板说还没有（返回状态）
买家又去玩了会（不断轮询）
最后一次再问，菜刀有了（数据准备好了）
老板把菜刀递给买家（从内核缓冲区拷贝到用户缓冲区）
整个过程轮询+等待：轮询时没有等待，可以做其他事，从内核缓冲区拷贝到用户缓冲区需要等待
是同步io
同一个线程，同一时刻只能监听一个socket，造成浪费，引入io多路复用，同时监听读个socket
'''
```

![img](https://pic3.zhimg.com/80/v2-9b42ac950d82df33a70b5f73d225af9e_720w.webp)

```
"""
要自己实现一个非阻塞IO模型
"""
import socket
import time
server = socket.socket()
server.bind(('127.0.0.1', 8081))
server.listen(5)
server.setblocking(False)
# 将所有的网络阻塞变为非阻塞
r_list = []
del_list = []
while True:
    try:
        conn, addr = server.accept()
        r_list.append(conn)
    except BlockingIOError:
        # time.sleep(0.1)
        # print('列表的长度:',len(r_list))
        # print('做其他事')
        for conn in r_list:
            try:
                data = conn.recv(1024)  # 没有消息 报错
                if len(data) == 0:  # 客户端断开链接
                    conn.close()  # 关闭conn
                    # 将无用的conn从r_list删除
                    del_list.append(conn)
                    continue
                conn.send(data.upper())
            except BlockingIOError:
                continue
            except ConnectionResetError:
                conn.close()
                del_list.append(conn)
        # 挥手无用的链接
        for conn in del_list:
            r_list.remove(conn)
        del_list.clear()
# 客户端
import socket
client = socket.socket()
client.connect(('127.0.0.1',8081))
while True:
    client.send(b'hello world')
    data = client.recv(1024)
    print(data)
```

### 2.3. IO Multiplexing - I/O多路复用模型

类似BIO，只不过找了一个代理，来挂起等待，并能同时监听多个请求； 数据会由用户进程完成拷贝

```
'''
举个例子：多个人去 一个商店买菜刀，
多个人给老板打电话，说我要买菜刀（发起系统调用）
老板把每个人都记录下来（放到select中）
老板去工厂进货（IO操作）
有货了，再挨个通知买到的人，来取刀（通知/返回可读条件）
买家来到商店等待，老板把到给买家（从内核缓冲区拷贝到用户缓冲区）
多路复用：老板可以同时接受很多请求（select模型最大1024个，epoll模型），
但是老板把到给买家这个过程，还需要等待，
是同步io
强调：
 1. 如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。
 2. 在多路复用模型中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。
 结论: select的优势在于可以处理多个连接，不适用于单个连接
'''
```

![img](https://pic4.zhimg.com/80/v2-ddb74c49bab40321c8058b72f1513203_720w.webp)

```
"""
当监管的对象只有一个的时候 其实IO多路复用连阻塞IO都比比不上！！！
但是IO多路复用可以一次性监管很多个对象
server = socket.socket()
conn,addr = server.accept()
监管机制是操作系统本身就有的 如果你想要用该监管机制(select)
需要你导入对应的select模块
"""
import socket
import select
server = socket.socket()
server.bind(('127.0.0.1',8080))
server.listen(5)
server.setblocking(False)
read_list = [server]
while True:
    r_list, w_list, x_list = select.select(read_list, [], [])
    """
    帮你监管
    一旦有人来了 立刻给你返回对应的监管对象
    """
    # print(res)  # ([<socket.socket fd=3, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 8080)>], [], [])
    # print(server)
    # print(r_list)
    for i in r_list:  #
        """针对不同的对象做不同的处理"""
        if i is server:
            conn, addr = i.accept()
            # 也应该添加到监管的队列中
            read_list.append(conn)
        else:
            res = i.recv(1024)
            if len(res) == 0:
                i.close()
                # 将无效的监管对象 移除
                read_list.remove(i)
                continue
            print(res)
            i.send(b'heiheiheiheihei')
 # 客户端
import socket
client = socket.socket()
client.connect(('127.0.0.1',8080))
while True:
    client.send(b'hello world')
    data = client.recv(1024)
    print(data)
```

### 2.4. IO Multiplexing - I/O多路复用模型

```
IO复用：为了解释这个名词，首先来理解下复用这个概念，复用也就是共用的意思，这样理解还是有些抽象，为此，咱们来理解下复用在通信领域的使用，在通信领域中为了充分利用网络连接的物理介质，往往在同一条网络链路上采用时分复用或频分复用的技术使其在同一链路上传输多路信号，到这里我们就基本上理解了复用的含义，即公用某个“介质”来尽可能多的做同一类(性质)的事，那IO复用的“介质”是什么呢？为此我们首先来看看服务器编程的模型，客户端发来的请求服务端会产生一个进程来对其进行服务，每当来一个客户请求就产生一个进程来服务，然而进程不可能无限制的产生，因此为了解决大量客户端访问的问题，引入了IO复用技术，即：一个进程可以同时对多个客户请求进行服务。也就是说IO复用的“介质”是进程(准确的说复用的是select和poll，因为进程也是靠调用select和poll来实现的)，复用一个进程(select和poll)来对多个IO进行服务，虽然客户端发来的IO是并发的但是IO所需的读写数据多数情况下是没有准备好的，因此就可以利用一个函数(select和poll)来监听IO所需的这些数据的状态，一旦IO有数据可以进行读写了，进程就来对这样的IO进行服务。
理解完IO复用后，我们在来看下实现IO复用中的三个API(select、poll和epoll)的区别和联系
select，poll，epoll都是IO多路复用的机制，I/O多路复用就是通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知应用程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。三者的原型如下所示：
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
 1.select的第一个参数nfds为fdset集合中最大描述符值加1，fdset是一个位数组，其大小限制为__FD_SETSIZE（1024），位数组的每一位代表其对应的描述符是否需要被检查。第二三四参数表示需要关注读、写、错误事件的文件描述符位数组，这些参数既是输入参数也是输出参数，可能会被内核修改用于标示哪些描述符上发生了关注的事件，所以每次调用select前都需要重新初始化fdset。timeout参数为超时时间，该结构会被内核修改，其值为超时剩余的时间。
 select的调用步骤如下：
（1）使用copy_from_user从用户空间拷贝fdset到内核空间
（2）注册回调函数__pollwait
（3）遍历所有fd，调用其对应的poll方法（对于socket，这个poll方法是sock_poll，sock_poll根据情况会调用到tcp_poll,udp_poll或者datagram_poll）
（4）以tcp_poll为例，其核心实现就是__pollwait，也就是上面注册的回调函数。
（5）__pollwait的主要工作就是把current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于tcp_poll 来说，其等待队列是sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数 据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时current便被唤醒了。
（6）poll方法返回时会返回一个描述读写操作是否就绪的mask掩码，根据这个mask掩码给fd_set赋值。
（7）如果遍历完所有的fd，还没有返回一个可读写的mask掩码，则会调用schedule_timeout是调用select的进程（也就是 current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout 指定），还是没人唤醒，则调用select的进程会重新被唤醒获得CPU，进而重新遍历fd，判断有没有就绪的fd。
（8）把fd_set从内核空间拷贝到用户空间。
总结下select的几大缺点：
（1）每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
（2）同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
（3）select支持的文件描述符数量太小了，默认是1024
2．  poll与select不同，通过一个pollfd数组向内核传递需要关注的事件，故没有描述符个数的限制，pollfd中的events字段和revents分别用于标示关注的事件和发生的事件，故pollfd数组只需要被初始化一次。
 poll的实现机制与select类似，其对应内核中的sys_poll，只不过poll向内核传递pollfd数组，然后对pollfd中的每个描述符进行poll，相比处理fdset来说，poll效率更高。poll返回后，需要对pollfd中的每个元素检查其revents值，来得指事件是否发生。
3．直到Linux2.6才出现了由内核直接支持的实现方法，那就是epoll，被公认为Linux2.6下性能最好的多路I/O就绪通知方法。epoll可以同时支持水平触发和边缘触发（Edge Triggered，只告诉进程哪些文件描述符刚刚变为就绪状态，它只说一遍，如果我们没有采取行动，那么它将不会再次告知，这种方式称为边缘触发），理论上边缘触发的性能要更高一些，但是代码实现相当复杂。epoll同样只告知那些就绪的文件描述符，而且当我们调用epoll_wait()获得就绪文件描述符时，返回的不是实际的描述符，而是一个代表就绪描述符数量的值，你只需要去epoll指定的一个数组中依次取得相应数量的文件描述符即可，这里也使用了内存映射（mmap）技术，这样便彻底省掉了这些文件描述符在系统调用时复制的开销。另一个本质的改进在于epoll采用基于事件的就绪通知方式。在select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行扫描，而epoll事先通过epoll_ctl()来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的回调机制，迅速激活这个文件描述符，当进程调用epoll_wait()时便得到通知。
epoll既然是对select和poll的改进，就应该能避免上述的三个缺点。那epoll都是怎么解决的呢？在此之前，我们先看一下epoll 和select和poll的调用接口上的不同，select和poll都只提供了一个函数——select或者poll函数。而epoll提供了三个函 数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；epoll_ctl是注 册要监听的事件类型；epoll_wait则是等待事件的产生。
　　对于第一个缺点，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定 EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝 一次。
　　对于第二个缺点，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在 epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调 函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用 schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）。
　　对于第三个缺点，epoll没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子, 在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。
总结：
（1）select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用 epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在 epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的 时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间，这就是回调机制带来的性能提升。
（2）select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要 一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内 部定义的等待队列），这也能节省不少的开销。
###############
这三种IO多路复用模型在不同的平台有着不同的支持，而epoll在windows下就不支持，好在我们有selectors模块，帮我们默认选择当前平台下最合适的
##############
#服务端
from socket import *
import selectors
sel=selectors.DefaultSelector()
def accept(server_fileobj,mask):
    conn,addr=server_fileobj.accept()
    sel.register(conn,selectors.EVENT_READ,read)
def read(conn,mask):
    try:
        data=conn.recv(1024)
        if not data:
            print('closing',conn)
            sel.unregister(conn)
            conn.close()
            return
        conn.send(data.upper()+b'_SB')
    except Exception:
        print('closing', conn)
        sel.unregister(conn)
        conn.close()
server_fileobj=socket(AF_INET,SOCK_STREAM)
server_fileobj.setsockopt(SOL_SOCKET,SO_REUSEADDR,1)
server_fileobj.bind(('127.0.0.1',8088))
server_fileobj.listen(5)
server_fileobj.setblocking(False) #设置socket的接口为非阻塞
sel.register(server_fileobj,selectors.EVENT_READ,accept) #相当于网select的读列表里append了一个文件句柄server_fileobj,并且绑定了一个回调函数accept
while True:
    events=sel.select() #检测所有的fileobj，是否有完成wait data的
    for sel_obj,mask in events:
        callback=sel_obj.data #callback=accpet
        callback(sel_obj.fileobj,mask) #accpet(server_fileobj,1)
#客户端
from socket import *
c=socket(AF_INET,SOCK_STREAM)
c.connect(('127.0.0.1',8088))
while True:
    msg=input('>>: ')
    if not msg:continue
    c.send(msg.encode('utf-8'))
    data=c.recv(1024)
    print(data.decode('utf-8'))
```

### 2.5. AIO – 异步I/O模型

发起请求立刻得到回复，不用挂起等待； 数据会由内核进程主动完成拷贝

```
'''
举个例子：还是买菜刀
现在是网上下单到商店（系统调用）
商店确认（返回）
商店去进货（io操作）
商店收到货把货发个卖家（从内核缓冲区拷贝到用户缓冲区）
买家收到货（指定信号）
整个过程无等待
异步io
AIO框架在windows下使用windows IOCP技术，在Linux下使用epoll多路复用IO技术模拟异步IO
市面上多数的高并发框架，都没有使用异步io而是用的io多路复用，因为io多路复用技术很成熟且稳定，并且在实际的使用过程中，异步io并没有比io多路复用性能提升很多，没有达到很明显的程度
并且，真正的AIO编码难度比io多路复用高很多
'''
```

![img](https://pic4.zhimg.com/80/v2-d7d6e252a28ac2ae5163e881b388e11b_720w.webp)

### 2.6.select poll 和epoll

```
#  1 select poll 和epoll都是io多路复用技术
select, poll , epoN都是io多路复用的机制。I/O多路复用就是通过一种机 制个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select, poll , epoll本质上都是同步I/O ,因为他们都需要在读写事件就绪后自己负责进行读写， 也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异 步I/O的实现会负责把数据从内核拷贝到用户空间。
# 2 select
select函数监视的文件描述符分3类，分别是writefds、readfds、和 exceptfds。调用后select函数会阻塞，直到有描述副就绪（有数据可读、 可写、或者有except）,或者超时（timeout指定等待时间，如果立即返回 设为null即可），函数返回。当select函数返回后，可以通过遍历fdset,来 找到就绪的描述符。
select目前几乎在所有的平台上支持，其良好跨平台支持也是它的一个 优点。select的一个缺点在于单个进程能够监视的文件描述符的数量存在最大限制，在Linux上一般为1024 ,可以通过修改宏定义甚至重新编译内核的 方式提升这一限制，但是这样也会造成效率的降低。
# 3 poll
不同于select使用三个位图来表示三个fdset的方式，poll使用一个 pollfd的指针实现。
pollfd结构包含了要监视的event和发生的event,不再使用select '参数-值'传递的方式。同时，pollfd并没有最大数量限制（但是数量过大后 性能也是会下降）。和select函数一样，poll返回后，需要轮询pollfd来获取就绪的描述符。
从上面看，select和poll都需要在返回后，通过遍历文件描述符来获取 已经就绪的socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降
# 4 epoll
epoll是在linux2.6内核中提出的，是之前的select和poll的增强版本。相对 于select和poll来说，epoll更加灵活，没有描述符限制。epoll使用一个文 件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。
# 5 更好的例子理解
老师检查同学作业，一班50个人，一个一个问，同学，作业写完了没？select，poll
老师检查同学作业，一班50个人，同学写完了主动举手告诉老师，老师去检查 epoll
# 6 总结
在并发高的情况下，连接活跃度不高，epoll比select好，网站http的请求，连了就断掉
并发性不高，同时连接很活跃，select比epoll好，websocket的连接，长连接，游戏开发
```

## 3.同步I/O与异步I/O

- 同步I/O
- - 概念：导致请求进程阻塞的I/O操作，直到I/O操作任务完成
  - 类型：BIO、NIO、IO Multiplexing
- 异步I/O
- - 概念：不导致进程阻塞的I/O操作
  - 类型：AIO

注意：

- 同步I/O与异步I/O判断依据是，是否会导致用户进程阻塞
- BIO中socket直接阻塞等待（用户进程主动等待，并在拷贝时也等待）
- NIO中将数据从内核空间拷贝到用户空间时阻塞（用户进程主动询问，并在拷贝时等待）
- IO Multiplexing中select等函数为阻塞、拷贝数据时也阻塞（用户进程主动等待，并在拷贝时也等待）
- AIO中从始至终用户进程都没有阻塞（用户进程是被动的）

## 4.并发-并行-同步-异步-阻塞-非阻塞

```
# 1 并发
并发是指一个时间段内，有几个程序在同一个cpu上执行，但是同一时刻，只有一个程序在cpu上运行
跑步，鞋带开了，停下跑步，系鞋带
# 2 并行
指任意时刻点上，有多个程序同时运行在多个cpu上
跑步，边跑步边听音乐
# 3 同步：
指代码调用io操作时，必须等待io操作完成才返回的调用方式
# 4 异步
异步是指代码调用io操作时，不必等io操作完成就返回调用方式
# 6 阻塞
指调用函数时候，当前线程别挂起
# 6 非阻塞
指调用函数时候，当前线程不会被挂起，而是立即返回
# 区别：
同步和异步是消息通讯的机制
阻塞和非阻塞是函数调用机制
```

## 5.IO设计模式

```
Reactor模式，基于同步I/O实现
- Proactor模式，基于异步I/O实现
```

Reactor模式通常采用IO多路复用机制进行具体实现

```
- kqueue、epoll、poll、select等机制
```

Proactor模式通常采用OS Asynchronous IO(AIO)的异步机制进行实现

```
- 前提是对应操作系统支持AIO，比如支持异步IO的linux(不太成熟)、具备IOCP的windows server(非常成熟)
```

Reactor模式和Proactor模式都是事件驱动，主要实现步骤：

1. 事件注册：将事件与事件处理器进行分离。将事件注册到事件循环中，将事件处理器单独管理起来，记录其与事件的对应关系。
2. 事件监听：启动事件循环，一旦事件已经就绪/完成，就立刻通知事件处理器
3. 事件分发：当收到事件就绪/完成的信号，便立刻激活与之对应的事件处理器
4. 事件处理：在进程/线程/协程中执行事件处理器

使用过程中，用户通常只负责**定义事件和事件处理器**并将其注册以及一开始的**事件循环的启动**，这个过程就会是以异步的形式执行任务。

### 5.1.Reactor模式

![img](https://pic2.zhimg.com/80/v2-3e46412eb280f1728c26a7dddcd8492d_720w.webp)

### 5.2.Proactor模式

![img](https://pic2.zhimg.com/80/v2-a1f360c7abb19fc51a111fcc28f42c11_720w.webp)

### 5.3.对比分析

Reactor模型处理耗时长的操作会造成事件分发的阻塞，影响到后续事件的处理；

Proactor模型实现逻辑复杂；依赖操作系统对异步的支持，目前实现了纯异步操作的操作系统少，实现优秀的如windows IOCP，但由于其windows系统用于服务器的局限性，目前应用范围较小；而Unix/Linux系统对纯异步的支持有限，因而应用事件驱动的主流还是基于select/epoll等实现的reactor模式

Python中：如asyncio、gevent、tornado、twisted等异步模块都是依据事件驱动模型设计，更多的都是使用reactor模型，其中部分也支持proactor模式，当然需要根据当前运行的操作系统环境来进行手动配置

原文链接：https://zhuanlan.zhihu.com/p/375177483

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.282】Redis不是一直号称单线程效率也很高吗，为什么又采用多线程了？

Redis是目前广为人知的一个内存数据库，在各个场景中都有着非常丰富的应用，前段时间Redis推出了6.0的版本，在新版本中采用了多线程模型。

因为我们公司使用的内存数据库是自研的，按理说我对Redis的关注其实并不算多，但是因为Redis用的比较广泛，所以我需要了解一下这样方便我进行面试。

总不能候选人用过Redis，但是我非要问人家阿里的Tair是怎么回事吧。

所以，在Redis 6.0 推出之后，我想去了解下为什么采用多线程，现在采用的多线程和以前版本有什么区别？为什么这么晚才使用多线程？

**Redis不是已经采用了多路复用技术吗？不是号称很高的性能了吗？为啥还要采用多线程模型呢？**

本文就来分析下这些问题以及背后的思考。

## 1.Redis为什么最开始被设计成单线程的？

Redis作为一个成熟的分布式缓存框架，它由很多个模块组成，如网络请求模块、索引模块、存储模块、高可用集群支撑模块、数据操作模块等。

很多人说Redis是单线程的，就认为Redis中所有模块的操作都是单线程的，其实这是不对的。

我们所说的Redis单线程，指的是”其网络IO和键值对读写是由一个线程完成的”，也就是说，**Redis中只有网络请求模块和数据操作模块是单线程的。而其他的如持久化存储模块、集群支撑模块等是多线程的。**

所以说，Redis中并不是没有多线程模型的，早在Redis 4.0的时候就已经针对部分命令做了多线程化。

那么，为什么网络操作模块和数据存储模块最初并没有使用多线程呢？

这个问题的答案比较简单！因为：”没必要！”

为什么没必要呢？我们先来说一下，什么情况下要使用多线程？

## 2.多线程适用场景

一个计算机程序在执行的过程中，主要需要进行两种操作分别是读写操作和计算操作。

其中读写操作主要是涉及到的就是I/O操作，其中包括网络I/O和磁盘I/O。计算操作主要涉及到CPU。

**而多线程的目的，就是通过并发的方式来提升I/O的利用率和CPU的利用率。**

那么，Redis需不需要通过多线程的方式来提升提升I/O的利用率和CPU的利用率呢？

首先，我们可以肯定的说，Redis不需要提升CPU利用率，因为**Redis的操作基本都是基于内存的，CPU资源根本就不是Redis的性能瓶颈。**

**所以，通过多线程技术来提升Redis的CPU利用率这一点是完全没必要的。**

那么，使用多线程技术来提升Redis的I/O利用率呢？是不是有必要呢？

Redis确实是一个I/O操作密集的框架，他的数据操作过程中，会有大量的网络I/O和磁盘I/O的发生。要想提升Redis的性能，是一定要提升Redis的I/O利用率的，这一点毋庸置疑。

但是，**提升I/O利用率，并不是只有采用多线程技术这一条路可以走！**

## 3.多线程的弊端

我们在很多文章中介绍过一些Java中的多线程技术，如内存模型、锁、CAS等，这些都是Java中提供的一些在多线程情况下保证线程安全的技术。

> 线程安全：是编程中的术语，指某个函数、函数库在并发环境中被调用时，能够正确地处理多个线程之间的共享变量，使程序功能正确完成。

和Java类似，所有支持多线程的编程语言或者框架，都不得不面对的一个问题，那就是如何解决多线程编程模式带来的共享资源的并发控制问题。

**虽然，采用多线程可以帮助我们提升CPU和I/O的利用率，但是多线程带来的并发问题也给这些语言和框架带来了更多的复杂性。而且，多线程模型中，多个线程的互相切换也会带来一定的性能开销。**

所以，在提升I/O利用率这个方面上，Redis并没有采用多线程技术，而是选择了**多路复用 I/O**技术。

## 4.小结

Redis并没有在网络请求模块和数据操作模块中使用多线程模型，主要是基于以下四个原因：

- 1、Redis 操作基于内存，绝大多数操作的性能瓶颈不在 CPU
- 2、使用单线程模型，可维护性更高，开发，调试和维护的成本更低
- 3、单线程模型，避免了线程间切换带来的性能开销
- 4、在单线程中使用多路复用 I/O技术也能提升Redis的I/O利用率

还是要记住：Redis并不是完全单线程的，只是有关键的网络IO和键值对读写是由一个线程完成的。

## 5.Redis的多路复用

多路复用这个词，相信很多人都不陌生。我之前的很多文章中也够提到过这个词。

其中在介绍Linux IO模型的时候我们提到过它、在介绍HTTP/2的原理的时候，我们也提到过他。

那么，Redis的多路复用技术和我们之前介绍的又有什么区别呢？

这里先讲讲**Linux多路复用技术，就是多个进程的IO可以注册到同一个管道上，这个管道会统一和内核进行交互。当管道中的某一个请求需要的数据准备好之后，进程再把对应的数据拷贝到用户空间中。**

![img](https://pic3.zhimg.com/80/v2-be7c461d57b013bfb3cae65f3c3a451e_720w.webp)

多看一遍上面这张图和上面那句话，后面可能还会用得到。

也就是说，通过一个线程来处理多个IO流。

IO多路复用在Linux下包括了三种，select、poll、epoll，抽象来看，他们功能是类似的，但具体细节各有不同。

其实，Redis的IO多路复用程序的所有功能都是通过包装操作系统的IO多路复用函数库来实现的。每个IO多路复用函数库在Redis源码中都有对应的一个单独的文件。

![img](https://pic2.zhimg.com/80/v2-b73f6e41e102ad45f9288e1d7ee47b7d_720w.webp)

在Redis 中，每当一个套接字准备好执行连接应答、写入、读取、关闭等操作时，就会产生一个文件事件。因为一个服务器通常会连接多个套接字，所以多个文件事件有可能会并发地出现。

![img](https://pic2.zhimg.com/80/v2-8ce7292c2a748b9eecd2f67d98c47a21_720w.webp)

一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。

所以，Redis选择使用多路复用IO技术来提升I/O利用率。

而之所以Redis能够有这么高的性能，不仅仅和采用多路复用技术和单线程有关，此外还有以下几个原因：

- 1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。
- 2、数据结构简单，对数据操作也简单，如哈希表、跳表都有很高的性能。
- 3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU
- 4、使用多路I/O复用模型

## 6.为什么Redis 6.0 引入多线程

2020年5月份，Redis正式推出了6.0版本，这个版本中有很多重要的新特性，其中多线程特性引起了广泛关注。

但是，需要提醒大家的是，**Redis 6.0中的多线程，也只是针对处理网络请求过程采用了多线程，而数据的读写命令，仍然是单线程处理的。**

但是，不知道会不会有人有这样的疑问：

**Redis不是号称单线程也有很高的性能么？**

**不是说多路复用技术已经大大的提升了IO利用率了么，为啥还需要多线程？**

主要是因为我们对Redis有着更高的要求。

根据测算，Redis 将所有数据放在内存中，内存的响应时长大约为 100 纳秒，对于小数据包，Redis 服务器可以处理 80,000 到 100,000 QPS，这么高的对于 80% 的公司来说，单线程的 Redis 已经足够使用了。

但随着越来越复杂的业务场景，有些公司动不动就上亿的交易量，因此需要更大的 QPS。

为了提升QPS，很多公司的做法是部署Redis集群，并且尽可能提升Redis机器数。但是这种做法的资源消耗是巨大的。

而经过分析，限制Redis的性能的主要瓶颈出现在网络IO的处理上，虽然之前采用了多路复用技术。但是我们前面也提到过，**多路复用的IO模型本质上仍然是同步阻塞型IO模型**。

下面是多路复用IO中select函数的处理过程：

![img](https://pic4.zhimg.com/80/v2-608acec5591c97149f47e2c31312018b_720w.webp)

￼从上图我们可以看到，**在多路复用的IO模型中，在处理网络请求时，调用 select （其他函数同理）的过程是阻塞的，也就是说这个过程会阻塞线程，如果并发量很高，此处可能会成为瓶颈。**

虽然现在很多服务器都是多个CPU核的，但是对于Redis来说，因为使用了单线程，在一次数据操作的过程中，有大量的CPU时间片是耗费在了网络IO的同步处理上的，并没有充分的发挥出多核的优势。

**如果能采用多线程，使得网络处理的请求并发进行，就可以大大的提升性能。多线程除了可以减少由于网络 I/O 等待造成的影响，还可以充分利用 CPU 的多核优势。**

所以，Redis 6.0采用多个IO线程来处理网络请求，网络请求的解析可以由其他线程完成，然后把解析后的请求交由主线程进行实际的内存读写。提升网络请求处理的并行度，进而提升整体性能。

但是，Redis 的多 IO 线程只是用来处理网络请求的，对于读写命令，Redis 仍然使用单线程来处理。

**那么，在引入多线程之后，如何解决并发带来的线程安全问题呢？**

这就是为什么我们前面多次提到的”Redis 6.0的多线程只用来处理网络请求，而数据的读写还是单线程”的原因。

Redis 6.0 只有在网络请求的接收和解析，以及请求后的数据通过网络返回给时，使用了多线程。而数据读写操作还是由单线程来完成的，所以，这样就不会出现并发问题了。

原文链接：https://zhuanlan.zhihu.com/p/375535653

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.283】C++ 协程的近况、设计与实现中的细节和决策

时至2018年的今天，C++ 在互联网服务端开发方向依然占据着相当大的份额；百度，腾讯，甚至以java为主流开发语言的阿里都在大规模使用C++做互联网服务端开发，而这恰恰是本文想要讨论的范畴。

第1章 C++协程近况简介
协程分两种，无栈协程(stackless)和有栈协程(stackful)，前者无法解决异步回调模式中上下文保存与恢复的问题，在此不做论述，文中后续提到的协程均指有栈协程。

## 1.第1节.旧时代

在2014年以前，C++服务端开发是以异步回调模型为主流，业务流程中每一个需要等待IO处理的节点都需要切断业务处理流程、保存当前处理的上下文、设置回调函数，等IO处理完成后再恢复上下文、接续业务处理流程。

在一个典型的互联网业务处理流程中，这样的行为节点多达十几个甚至数十个(微服务间的rpc请求、与redis之类的高速缓存的交互、与mysql\mongodb之类的DB交互、调用第三方HttpServer的接口等等)；被切割的支离破碎的业务处理流程带来了几个常见的难题：

每个流程都要定义一个上下文struct，并手动保存与恢复；

每次回调都会切断栈上变量的生命周期，导致需要延续使用的变量必须申请到堆上或存入上下文结构中；

由于C++是无GC的语言，碎片化的逻辑给内存管理也带来了更多挑战；

回调式的逻辑是“不知何时会被触发”的，用户状态管理也会有更多挑战；

这些具体的难题综合起来，在工程化角度呈现出的效果就是：代码编写复杂，开发周期长，维护困难，BUG多且防不胜防。

## 2.第2节.新时代

2014年腾讯的微信团队开源了一个C风格的协程框架libco，并在次年的架构师峰会上做了宣讲，使业内都认识到异步回调模式升级为协程模式的必要性，从此开启了C++互联网服务端开发的协程时代。BAT三家旗下的各个小部门、业内很多与时俱进的互联网公司都纷纷自研协程框架，一时呈百花齐放之态。

笔者所在的公司当时也试用了一段时间libco，修修补补很多次，终究是因为问题太多而放弃，改用了自研的libgo作为协程开发框架。

聊协程就不能不提到主打协程功能和CSP模式的golang语言，google从09年发布golang至今，经过近10个年头的发酵，已成为互联网服务端开发主流开发语言之一，许多项目和开发者从C++、java、php等语言转向golang。笔者自研的libgo也汲取了golang的设计理念和多年的实践经验。

## 3.本文后续针对C++协程框架的设计与实现、与golang这种语言级别支持的协程的差距在哪里、怎样尽力弥补这种差距等方面展开讨论。

第2章.协程库的设计与实现
个人认为，C++协程库从实现完善程度上分为以下几个层次

1.API级
实现协程上下文切换api，或添加一些便于使用的封装； 特点：没有协程调度。

代表作：boost.context, boost.coroutine, ucontext(unix), fiber(windows)

这一层次的协程库，仅仅提供了一个底层api，要想拿来做项目，还有非常非常遥远的距离；不过这些协程api可以为我们实现自己的协程库提供一个良好的基础。

2.玩具级
实现了协程调度，无需用户手动处理协程上下文切换；特点：没有HOOK

代表作：libmill

这一层次的协程库，实现了协程调度(类似于操作系统有了进程调度机制);稍好一些的意识到了阻塞网络io与协程的不协调之处，自己实现了一套网络io相关函数；

但是这也意味着涉及网络的第三方库全部不可用了，比如你想用redis？不好意思，hiredis不能用了，要自己轮一个；你想用mysql？不好意思，mysqlclient不能用了，要自己轮一个。放弃整个C/C++生态全部自己轮，这个玩笑开的有点大，所以只能称之为“玩具级”。

3.工业级
以部分正确的方式HOOK了网络io相关的syscall，可以少改甚至不改代码的兼容大多数第三方库；特点：没有完整生态

代表作：libco

这一层次的协程库，但是hook的不够完善，未能完全模拟syscall的行为，只能兼容行为符合预想的同步模型的第三方库，这虽然只能覆盖一部分的第三方库，但是通过严苛的源码审查、付出代价高昂的测试成本，也可以勉强用于实际项目开发了；

但其他机制不够完善：协程间通讯、协程同步、调试等，因此对开发人员的要求很高，深谙底层机制才能写出没有问题的代码；再加上hook不完善带来的隐患，开发过程可谓是步步惊心、如履薄冰。

4.框架级
以100%行为模拟的方式HOOK了网络io相关的syscall，可以完全不改代码兼容大多数第三方库；依照专为协程而生的语言的使用经验，提供了协程开发所必须的完整生态；

代表作：libgo

这一层次的协程库，能够100%模拟被hook的syscall的行为，能够兼容任何网络io行为的同步模型的第三方库；由于协程开发生态的完善，对开发人员的要求变得很低，新手也可以写出高效稳定的代码。但由于C++的灵活性，用户行为是不受限的，所以依然存在几个边边角角的难点需要开发者注意：没有gc（开发者要了解协程的调度时机和生命期），TLS的问题，用户不按套路出牌、把逻辑代码run在协程之外，粗粒度的线程锁等等。

5.语言级
语言级的协程实现

代表作：golang语言

这一层次的协程库，开发者的一切行为都是受限行为，可以实现无死角的完善的协程。

下面会尽可能详尽的讨论libgo设计中的每一个重要决策，并会列举一些其他协程库的决策的优劣与实现方式

## 4.协程上下文切换

协程上下文切换有很多种实现方式：

1.使用操作系统提供的api：ucontext、fiber
这种方式是最安全可靠的，但是性能比较差。（切换性能大概在200万次/秒左右)

2.使用setjump、longjump：
代表作：libmill

3.自己写汇编码实现
这种方式的性能可以很好，但是不同系统、甚至不同版本的linux都需要不同的汇编码，兼容性奇差无比，代表作：libco

4.使用boost.coroutine
这种方式的性能很好，boost也帮忙处理了各种平台架构的兼容性问题，缺陷是这东西随着boost的升级，并不是向后兼容的，不推荐使用

5.使用boost.context
性能、兼容性都是当前最佳的，推荐使用。（切换性能大概在1.25亿次/秒左右)

libgo在这一块的方案是1+5：

不愿意依赖boost库的用户直接编译即可选择第1种方案；

追求更佳性能的用户编译时使用cmake参数-DENABLE_BOOST_CONTEXT=ON即可选择第5种方案

第2节.协程栈
我们通常会创建数量非常庞大的协程来支持高并发，协程栈内存占用情况就变成一个不容忽视的问题了；

如果采用线程栈相同的大栈方案（linux系统默认8MB），启动1000个协程就要8GB内存，启动10w个协程就要800GB内存，而每个协程真正使用的栈内存可以几百kb甚至几kb，内存使用率极低，这显然是不可接受的；

如果采用减少协程栈的大小，比如设为128kb，启动1000个协程要128MB内存，启动10w个协程要12.8GB内存，这是一个合理的设置；但是，我们知道有很多人喜欢直接在栈上申请一个64kb的char数组做缓冲区，即使开发者非常小心的不这样奢侈的使用栈内存，也难免第三方库做这样的行为，而只需两层嵌套就会栈溢出了。

栈内存不可太大，也不可太小，这其中是很难权衡的，一旦定死这个值，就只能针对特定的场景，无法做到通用化了； 针对协程栈的内存问题，一般有以下几种方案。

静态栈(Static Stack)

固定大小的栈，存在上述的难以权衡的问题；

但是如果把问题限定在某一个范围，比如说我就只用来写微信后台、并且严格review每一个引入的第三方库的源码，确保其全部谨慎使用栈内存，这种方案也是可以作为实际项目来使用的。

典型代表：libco，它设置了128KB大小的堆栈，15年的时候我们把它引入我们当时的项目中，其后出现过多次栈溢出的问题。

分段栈(Segmented Stack)

gcc提供的“黄金链接器”支持一种允许栈内存不连续的编译参数，实现原理是在每个函数调用开头都插入一段栈内存检测的代码，如果栈内存不够用了就申请一块新的内存，作为栈内存的延续。

这种方案本应是最佳的实现，但如果遇到的第三方库没有使用这种方式来编译(注意:glibc也是这里提到的”第三方库”)，那就无法在其中检测栈内存是否需要扩展，栈溢出的风险很大。

拷贝栈(Copy Stack)

每次检测到栈内存不够用时，申请一块更大的新内存，将现有的栈内存copy过去，就像std::vector那样扩展内存。

在某些语言上是可以实现这样的机制，但C++ 是有指针的，栈内存的Copy会导致指向其内存地址的指针失效；又因为其指针的灵活性(可以加减运算)，修改对应的指针成为了一种几乎不可能实现的事情(参照c++ 为什么没办法实现gc原理,详见《C++11新特性解析与应用》第5章 5.2.4节)。

共享栈(Shared Stack)

申请一块大内存作为共享栈(比如：8MB)，每次开始运行协程之前，先把协程栈的内存copy到共享栈中，运行结束后再计算协程栈真正使用的内存，copy出来保存起来，这样每次只需保存真正使用到的栈内存量即可。

这种方案极大程度上避免了内存的浪费，做到了用多少占多少，同等内存条件下，可以启动的协程数量更多，

libco

使用这种方案单机启动了上千万协程。

但是这种方案的缺陷也同样明显：

1.协程切换慢：每次协程切换，都需要2次Copy协程栈内存，这个内存量基本上都在1KB以上，通常是几十kb甚至几百kb，这样的2次Copy要花费很长的时间。

2.栈上引用失效导致隐蔽的bug：例如下面的代码

点击此处添加图片说明文字

bar这个协程函数里面，启动了一个新的协程，然后bar等待新协程结束后再退出；当切换到新协程时，由于bar协程的栈已经被copy到了其他位置，栈上分配的变量a已经失效，此时调用a.foo就会出现难以预料的结果。

这样的场景在开发中数不胜数，比如：某个处理流程需要聚合多个后端的结果、父协程对子协程做一些计数类的操作等等等等

有人说我可以把变量a分配到堆上，这样的改法确实可以解决这个已经发现的bug；那其他没发现的怎么办呢，难道每个变量都放到堆上以提前规避这个坑？这显然是不切实际的。

早期的libgo也使用过共享栈的方式，也正是因为作者在实际开发中遇到了这样的问题，才放弃了共享栈的方式。

虚拟内存栈(Virtual Memory Stack)

既然前面提到的4种协程栈都有这样那样的弊端，那么有没有一种方案能够相对完美的解决这个问题？答案就是虚拟内存栈。

Linux、Windows、MacOS三大主流操作系统都有这样一个虚拟内存机制：进程申请的内存并不会立即被映射成物理内存，而是仅管理于虚拟内存中，真正对其读写时会触发缺页中断，此时才会映射为物理内存。

比如：我在进程中malloc了1MB的内存，但是不做读写，那么物理内存占用是不会增加的；当我读写这块内存的第一个字节时，系统才会将这1MB内存中的第一页(默认页大小4KB)映射为物理内存，此时物理内存的占用会增加4KB，以此类推，可以做到用多少占多少，冗余不超过一个内存页大小。

基于这样一个机制，libgo为每个协程malloc 1MB的虚拟内存作为协程栈(这个值是可以定制化的)；不做读写操作就不会占用物理内存，协程栈使用了多少才会占用多少物理内存，实现了与共享栈近似的内存使用率，并且不存在共享栈的两大弊端。

典型代表：

libgo

第3节.协程调度
像操作系统的进程调度一样，协程调度也有多种方案可选，也有公平调度和不公平调度之分。

栈式调度

栈式调度是典型的不公平调度：协程队列是一个栈式的结构，每次创建的协程都置于栈顶，并且会立即暂停当前协程并切换至子协程中运行，子协程运行结束(或其他原因导致切换出来)后，继续切换回来执行父协程；越是处于栈底部的协程(越早创建的协程)，被调度到的机会越少；

甚至某些场景下会产生隐晦的死循环导致永远在栈顶的两个协程间切来切去，其他协程全部无法执行。

典型代表：

libco

星切调度(非对称协程调度)

调度线程 -> 协程A -> 调度线程 -> 协程B -> 调度线程 -> …

调度线程居中，协程画在周围，调度顺序图看起来就像是星星一样，因此戏称为星切。

将当前可调度的协程组织成先进先出的队列(runnable list)，顺序pop出来做调度；新创建的协程排入队尾，调度一次后如果状态依然是可调度(runnable)的协程则排入队尾，调度一次后如果状态变为阻塞，那阻塞事件触发后也一样排入队尾，是为公平调度。

典型代表：

libgo

环切调度(对称协程调度)

调度线程 -> 协程A -> 协程B -> 协程C -> 协程D -> 调度线程 -> …

调度线程居中，协程画在周围，调度顺序图看起来呈环状，因此戏称为环切。

从调度顺序上可以发现，环切的切换次数仅为星切的一半，可以带来更高的整体切换速度；但是多线程调度、WorkSteal方面会带来一定的挑战。

这种方案也是libgo后续优化的一个方向

多线程调度、负载均衡与WorkSteal

本节的内容其实不是协程库的必选项，互联网服务端开发领域现在主流方案都是微服务，单线程多进程的模型不会有额外的负担。

但是某些场景下多进程会有很昂贵的额外成本(比如：开发一个数据库)，只能用多线程来解决，libgo为了有更广阔的适用性，实现了多线程调度和Worksteal。同时也突破了传统协程库仅用来处理网络io密集型业务的局限，也能适用于cpu密集型业务，充当并行编程库来使用。

libgo的多线程调度采用N:M模型，调度线程数量可以动态增加，但不能减少； 每个调度线程持有一个Processer(后文简称: P)，每个P持有3个runnable协程队列(普通队列、IO触发队列、亲缘性队列)，其中普通队列保存的是可以被偷取的协程；当某个P空闲时，会去其他P的队列尾部偷取一些协程过来执行，以此实现负载均衡。

为了IO方面降低线程竞争，libgo会为每个调度线程在必要的时候单独创建一个epoll；

关于每个epoll的使用，会在后面的本章第4节.HOOK-网络io中展开详细论述；其他关于多线程的设计会贯穿全文的逐个介绍。

第4节.HOOK
是否有HOOK是一个协程库定位到玩具级和工业级之间的重要分水岭； HOOK的底层实现是否遵从HOOK的基本守则；决定着用户是如履薄冰的使用一个漏洞百出的协程库？还是可以挥洒自如的使用一个稳定健壮的协程库？

基本守则：HOOK接口表现出来的行为与被HOOK的接口保持100%一致

HOOK是一个精细活，需要繁琐的边界条件测试，不但要保证返回值与原函数一致，相应的errno也要一致，做的与原函数越像，能够支持的三方库就越多； 但只要不做到100%，使用时就总是要提心吊胆的，因为你无法辨识哪些三方库的哪些逻辑分支会遇到BUG！

比如我们在试用libco的时候就遇到这样一个问题：

点击此处添加图片说明文字

众所周知，新建的socket默认都是阻塞式的，isNonBlock应该为false。但是当这段代码执行于libco的协程中时，被hook后的结果isNonBlock居然是true！

连接成功后，read的行为更是怪异，既不是阻塞式的无限等待，也不是非阻塞式的立即返回；而是阻塞1秒后返回-1！

如果第三方库有表情的话，此时一定是一脸懵逼的。。。

而且libco的HOOK不能支持真正的全静态链接，这也是我们放弃它的一个重要因素。

网络io

libgo的HOOK设计与实现严格的遵守着HOOK的基本守则，在linux系统上hook的socket函数列表如下：

connect、accept read、readv、recv、recvfrom、recvmsg write、writev、send、sendto、sendmsg poll、select、__poll、close

fcntl、ioctl、getsockopt、setsockopt dup、dup2、dup3

协程挂起：

如果协程对一个或多个socket的IO阻塞操作(read/write/poll/select)无法立即完成，那么协程会被设置为io-block状态并保存到io-wait队列中，将当期协程的sentry保存在socket的等待队列中，然后将这一个或多个socket添加到当前线程所属的epoll中;

协程唤醒：

如果这一个或多个socket被epoll监听到协程关心的事件触发了，对应的协程就会被唤醒(设置成runnable状态)，并追加到所属P的IO触发队列尾部，等待再次被调度。

唤醒后的清理：

协程被唤醒后的首次调度，会从socket的等待队列中清除当期协程的sentry，如果socket读写事件对应的等待队列被清空且没有设置为ET模式，则会调用epoll_ctl清理epoll对socket的对应监听事件。

显而易见，调用void set_et_mode(int fd);接口将频繁读写的socket设置成et模式可以减少epoll相关的系统调用，提升性能；libgonet就做了这样的优化。

关于阻塞、非阻塞的问题，libgo是这样解决的：

为了实现协程的挂起，socket是必须被转换成非阻塞模式的，libgo在其上封装了一个状态：

user_nonblock

，表示用户是否主动设置过nonblock，并hook相关函数，屏蔽掉socket真实的阻塞状态，对用户呈现user_nonblock。

如果用户设置过nonblock，即user_nonblock == true，则对用户呈现一个非阻塞socket的所有特质(调用读写函数都不会阻塞，而是立即返回)。

如果用户没有设置过nonblock，即socket的真实状态是非阻塞的，但是user_nonblock == false，此时对用户呈现一个阻塞式socket的所有特质(调用读写函数不能立即完成就阻塞等待，并且阻塞时间等同于RCVTIMEO或SNDTIMEO)。

为了可以正确维护user_nonblock状态，就必须把dup、dup2、dup3这几个复制fd的函数给hook了，另外fcntl也是可以复制fd的，也要做出类似的处理。

libgo的HOOK不但可以100%模拟原生syscall的行为，还可以做一些原生syscall没能实现的功能，比如：带超时设置的connect。

在libgo的协程中调用connect之前，可以先调用void set_connect_timeout(int milliseconds);接口设置connect的超时时长。

DNS

libgo在linux系统上hook的dns函数列表如下：

gethostbyname

gethostbyname2

gethostbyname_r

gethostbyname2_r

gethostbyaddr

gethostbyaddr_r

其中，形如getXXbyYY的三个函数是其对应的getXXbyYY_r函数外层封装了一个TLS缓冲区的实现；

HOOK后的实现中，libgo使用CLS替代了原生syscall里的TLS的功能。

通过观察glibc源码发现，形如getXXbyYY_r的三个函数内部还使用了一个存在struct thread_info结构体中的TLS变量缓存调用远程dns服务器使用的socket，实测中发现libco提供的HOOK *_res*state函数的方案是无效的，getXXbyYY_r会并发乱序的读写同一个socket，导致混乱的结果或长久的阻塞。

libgo针对这个问题HOOK了getXXbyYY_r系列函数，在函数入口使用了一个线程私有的协程锁，解决了同一个线程的getXXbyYY_r乱序读写同一个socket的问题；又由于P中的IO触发队列的存在，getXXbyYY_r由于内部的__poll挂起再重新唤醒后，保证了会在原线程完成后续代码的执行。

signal

linux上的signal是有着不可重入属性的，在signal处理函数中处理复杂的操作极易出现死锁，libgo提供了解决这个问题的编译参数：

点击此处添加图片说明文字

其他会导致阻塞的syscall

libgo还HOOK了三个sleep函数：sleep、usleep、nanosleep

在协程中直接使用这三个sleep函数，可以让当前协程挂起相应的时间。

第5节.完整生态

依照golang近10年的实践经验来看，我们很容易发现协程是核心功能，但只有协程是远远不够的。 我们还需要很多周边生态来辅助协程更好地完成并发任务。

Channel

和线程一样，协程间也是需要交换数据。

很多时候我们需要一个能够屏蔽协程同步、多线程调度等各种底层细节的，简单的，保证数据有序传递的通讯方式，golang中channel的设计就刚好满足了我们的需求。

libgo仿照golang制作了Channel功能，通过如下代码：

点击此处添加图片说明文字

即创建了一个不带额外缓冲区的、传递int的channel，重载了操作符<<和>>，使用

点击此处添加图片说明文字

向其写入一个整数1，正如golang中channel的行为一样，此时如果没有另一个协程使用

点击此处添加图片说明文字

尝试读取，当前协程会被挂起等待。

如果使用

则表示从channel中读取一个元素，但是不再使用它。 channel的这种挂起协程等待的特性，也通常用于父协程等待子协程处理完成后再向下执行。

也可以使用

创建一个带有长度为10的缓冲区的channel，正如golang中channel的行为一样，对这样的channel进行写操作，缓冲区写满之前协程不会挂起。

这适用于有大批量数据需要传递的场景。

协程锁、协程读写锁

在任何C++协程库的使用中，都应该慎重使用或禁用线程锁，比如下面的代码

协程A首先被调度，加锁后调用sleep导致当前协程挂起，注意此时mtx已然是被锁定的。

然后协程B被调度，要等待mtx被解锁才能继续执行下去，由于mtx是线程锁，会阻塞调度线程，协程A再也不会有机会被调度，从而形成死锁。

这是一个典型的边角问题，因为我们无法阻止C++程序员在使用协程库的同时再使用线程同步机制。

其实我们可以提供一个协程锁来解决这一问题，比如下面的代码

代码与前一个例子几乎一样，唯一的区别是mtx的锁类型从线程锁变成了libgo提供的协程锁。

协程A首先被调度，加锁后调用sleep导致当前协程挂起，注意此时mtx已然是被锁定的。

然后协程B被调度，要等待mtx被解锁才能继续执行下去，由于mtx是协程锁，协程锁在等待时会挂起当前协程而不是阻塞线程，协程A在sleep时间结束后会被唤醒并被调度，协程A退出foo函数时会解锁，解锁的行为又会唤醒协程B，协程B被调度时再次锁定mtx，然后顺利完成整个逻辑。

libgo还提供了协程读写锁：

co_rwmutex

另外，即便开发者有意识的规避第一个例子那样的场景，也很容易踩到另外一个线程锁导致的坑，比如在使用zookeeper-client这样会启动后台线程来call回调函数的第三方库时：

看起来好像没什么问题，但其实routine里面的线程锁会阻塞整个调度线程，使得其他协程都无法被及时调度。

针对这种情况最优雅的处理方式就是使用Channel，因为libgo提供的Channel不仅可以用于协程间交换数据，也可以用于协程与线程间交换数据，可以说是专门针对zk这类起后台线程的第三方库设计的。

定时器

libgo框架的主调度器提供了一个基于红黑树的定时器，会在调度线程的主循环中被执行，这样的设计可以与epoll更好地协同工作，无论是定时器还是epoll监听的fd都可以最及时的触发。

使用co_timer_add接口可以添加一个定时任务，co_timer_add接口接受两个参数，第一个参数是可以是std::chrono::system_clock::time_point，也可以是std::chrono::steady_clock::time_point，还可以是std::chrono库里的一个duration。第二个参数接受一个回调函数，可以是函数指针、仿函数、lambda等等；

当第一个参数使用system_clock::time_point时，表示定时任务跟随系统时间的变化而变化，可以通过调整操作系统的时间设置提前或延缓定时任务的执行。

当第一个参数使用另外两种类型时，定时任务不随系统时间的变化而变化。

co_timer_add接口返回一个co::TimerId类型的定时任务id，可以用来取消定时任务。

取消定时任务有种方式：co_timer_cancel和co_timer_block_cancel，均会返回一个bool类型表示是否取消成功。

使用co_timer_cancel，会立即返回，即使定时任务正在被执行。

使用co_timer_block_cancel，如果定时任务正在被执行，则会阻塞地等待任务完成后返回false；否则会立即返回；

需要注意的是co_timer_block_cancel的阻塞行为是使用自旋锁实现的，如果定时任务耗时较长，co_timer_block_cancel的阻塞行为不但会阻塞当前调度线程，还会产生高昂的cpu开销；这个接口是设计用来在libgo内部使用的，请用户谨慎使用！

CLS(Coroutine Local Storage)(协程本地存储)

CLS类似于TLS(Thread Local Storage)；

这个功能是HOOK DNS函数族的基石，没有CLS的协程库是无法HOOK DNS函数族的。

libgo

提供了一个行为是TLS超集的CLS功能，CLS变量可以定义在全局作用域、块作用域(函数体内)、类的静态成员，除此TLS也支持的这三种场景外，还可以作为类的非静态成员。

注：

libco

也有CLS功能，但是仅支持全局作用域

CLS的使用方式参见tutorail文件夹下的sample13_cls.cpp教程代码。

线程池

除了前文提到的各种边角问题之外，还有一个非常常见的边角问题：文件IO 笔者曾经努力尝试过HOOK文件IO操作，但很不幸linux系统中，文件fd是无法使用poll、select、epoll正确监听可读可写状态的；linux提供的异步文件IO系统调用nio又不支持操作系统的文件缓存，不适合用来实现HOOK(这会导致用户的所有文件IO都不经过系统缓存而直接操作硬盘，这是一种不恰当的做法)。

除此之外也还会有其他不能HOOK或未被HOOK的阻塞syscall，因此需要一个线程池机制来解决这种阻塞行为对协程调度的干扰。

libgo提供了一个宏：co_await，来辅助用户完成线程池与协程的交互。

在协程中使用

可以把func投递到线程池中，并且挂起当前协程，直到func完成后协程会被唤醒，继续执行下去。 也可以使用

等待bar在线程池中完成，并将bar的返回值写入变量a中。 co_await也同样可以在协程之外被调用。

另外，为了用户更灵活的定制线程数量，也为了libgo不偷起后台线程的操守；线程池并不会自行启动，需要用户自行启动一个或多个线程执行co_sched.GetThreadPool().RunLoop();

调试

libgo作为框架级的协程库，调试机制是必不可少的。

1.可以设置co_sched.GetOptions().debug打印一些log，具体flag见config.h

2.可以设置一个协程事件监听器，详见tutorial文件夹下的sample12_listener.cpp教程代码

3.编译时添加cmake参数：-DENABLE_DEBUGGER=ON 开启debug信息收集后，可以使用co::CoDebugger类获取一些调试信息，详见debugger.h的注释

4.后续还会提供更多调试手段

协程之外(运行在线程上的代码)

前文提到了很多功能都可以在线程上执行：Channel、co_await、co_mutex、定时器、CLS

跨平台

libgo支持三大主流系统：linux、windows、mac-os

linux是主打平台，也是libgo运行性能最好的平台，master分支永远支持linux

win分支支持windows系统，会不定期的将master分支的新功能合入其中

mac的情况同windows

（个人开发者精力有限，还请见谅！）

上层封装

笔者另有一个开源库：libgonet，是基于libgo封装的linux协程网络库，使用起来极为方便。

如果你要开发一个网络服务或rpc框架，更推荐从libgonet写起，毕竟即使有协程，socket相关的处理也并不轻松。

未来的发展方向

1.目前是使用go、go_stack、go_dispatch三个不同的宏来设置协程的属性，这种方式不够灵活，后续要改成： go stack(1024 * 1024) dispatch(::co::egod_robin) func; 这样的语法形式，可以更灵活的定制协程属性。

2.基于(1)的新语法，实现“协程亲缘性”功能，将协程绑定到指定线程上，并防止被steal。

3.优化协程切换速度：

A）使用环切调度替代现在的星切调度(CoYeild时选择下一个切换目标)，必要时才切换回线程处理epoll、定时器、sleep等逻辑，同时协调好多线程调度

B）调度器的Run函数里面做了很多协程切换之外的事情，尽量降低这部分在非必要时的cpu消耗，比如：有任务加入定时器是设置一个tls标记为true，只有标记为true时才去处理定时器相关逻辑。

C）调度器中的runnable队列使用了自旋锁，没有竞争时对原子变量的操作也是比较昂贵的，runnable队列可以优化成多写一读，仅在写入端加锁的队列。

4.协程对象Task内存布局调优，tls池化，每个池使用多写一读链表队列，申请时仅在当前线程的池中申请，可以免锁，释放时均衡每个线程的池水水位，可以塞入其他线程的池中。

5.libgo之外，会进一步寻找和当前已经比较成熟的非协程的开发框架的结合方案，让还未能用上协程的用户低成本的用上协程。

原文链接：https://zhuanlan.zhihu.com/p/375761396

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.284】网络四层、七层负载均衡的区别

## 1.简介

1. ** 所谓四层就是基于IP+端口的负载均衡；七层就是基于URL等应用层信息的负载均衡；**同理，还有基于MAC地址的二层负载均衡和基于IP地址的三层负载均衡。 换句换说，二层负载均衡会通过一个虚拟MAC地址接收请求，然后再分配到真实的MAC地址；三层负载均衡会通过一个虚拟IP地址接收请求，然后再分配到真实的IP地址；四层通过虚拟IP+端口接收请求，然后再分配到真实的服务器；七层通过虚拟的URL或主机名接收请求，然后再分配到真实的服务器。
2. ** 所谓的四到七层负载均衡，就是在对后台的服务器进行负载均衡时，依据四层的信息或七层的信息来决定怎么样转发流量。** 比如四层的负载均衡，就是通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。七层的负载均衡，就是在四层的基础上（没有四层是绝对不可能有七层的），再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡。举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。
3. 负载均衡器通常称为四层交换机或七层交换机。四层交换机主要分析IP层及TCP/UDP层，实现四层流量负载均衡。七层交换机除了支持四层负载均衡以外，还有分析应用层的信息，如HTTP协议URI或Cookie信息。
4. 负载均衡分为L4 switch（四层交换），即在OSI第4层工作，就是TCP层啦。此种Load Balance不理解应用协议（如HTTP/FTP/MySQL等等）。例子：LVS，F5。
5. 另一种叫做L7 switch（七层交换），OSI的最高层第7层，应用层。此时，该Load Balancer能理解应用协议。例子： haproxy，MySQL Proxy。

注意：上面的很多Load Balancer既可以做四层交换，也可以做七层交换。

![img](https://pic1.zhimg.com/80/v2-964499099da9b1a7a0a71a42c9868414_720w.webp)

## 2.区别

1. 技术原理上
   所谓四层负载均衡，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。
   以常见的TCP为例，负载均衡设备在接收到第一个来自客户端的SYN 请求时，即通过上述方式选择一个最佳的服务器，并对报文中目标IP地址进行修改(改为后端服务器IP），直接转发给该服务器。TCP的连接建立，即三次握手是客户端和服务器直接建立的，负载均衡设备只是起到一个类似路由器的转发动作。在某些部署情况下，为保证服务器回包可以正确返回给负载均衡设备，在转发报文的同时可能还会对报文原来的源地址进行修改。 所谓七层负载均衡，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。以常见的TCP为例，负载均衡设备如果要根据真正的应用层内容再选择服务器，只能先代理最终的服务器和客户端建立连接(三次握手)后，才可能接受到客户端发送的真正应用层内容的报文，然后再根据该报文中的特定字段，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。负载均衡设备在这种情况下，更类似于一个代理服务器。负载均衡和前端的客户端以及后端的服务器会分别建立TCP连接。所以从这个技术原理上来看，七层负载均衡明显的对负载均衡设备的要求更高，处理七层的能力也必然会低于四层模式的部署方式。
2. 应用场景
   七层应用负载的好处，是使得整个网络更智能化。例如访问一个网站的用户流量，可以通过七层的方式，将对图片类的请求转发到特定的图片服务器并可以使用缓存技术；将对文字类的请求可以转发到特定的文字服务器并可以使用压缩技术。当然这只是七层应用的一个小案例，从技术原理上，这种方式可以对客户端的请求和服务器的响应进行任意意义上的修改，极大的提升了应用系统在网络层的灵活性。很多在后台，例如Nginx或者Apache上部署的功能可以前移到负载均衡设备上，例如客户请求中的Header重写，服务器响应中的关键字过滤或者内容插入等功能。
   另外一个常常被提到功能就是安全性。网络中最常见的SYN Flood攻击，即黑客控制众多源客户端，使用虚假IP地址对同一目标发送SYN攻击，通常这种攻击会大量发送SYN报文，耗尽服务器上的相关资源，以达到Denial of Service(*DoS*)的目的。从技术原理上也可以看出，四层模式下这些SYN攻击都会被转发到后端的服务器上；而七层模式下这些SYN攻击自然在负载均衡设备上就截止，不会影响后台服务器的正常运营。另外负载均衡设备可以在七层层面设定多种策略，过滤特定报文，例如SQL Injection等应用层面的特定攻击手段，从应用层面进一步提高系统整体安全。
   现在的7层负载均衡，主要还是着重于应用HTTP协议，所以其应用范围主要是众多的网站或者内部信息平台等基于B/S开发的系统。 4层负载均衡则对应其他TCP应用，例如基于C/S开发的ERP等系统。

## 3.Nginx、LVS及HAProxy负载均衡软件的优缺点

负载均衡 （Load Balancing） 建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力，同时能够提高网络的灵活性和可用性。

Nginx/LVS/HAProxy是目前使用最广泛的三种负载均衡软件。

一般对负载均衡的使用是随着网站规模的提升根据不同的阶段来使用不同的技术。具体的应用需求还得具体分析，如果是中小型的Web应用，比如日PV小于1000万，用Nginx就完全可以了；如果机器不少，可以用DNS轮询，LVS所耗费的机器还是比较多的；大型网站或重要的服务，且服务器比较多时，可以考虑用LVS。

一种是通过硬件来进行，常见的硬件有比较昂贵的F5和Array等商用的负载均衡器，它的优点就是有专业的维护团队来对这些服务进行维护、缺点就是花销太大，所以对于规模较小的网络服务来说暂时还没有需要使用；另外一种就是类似于Nginx/LVS/HAProxy的基于 Linux的开源免费的负载均衡软件，这些都是通过软件级别来实现，所以费用非常低廉。

目前关于网站架构一般比较合理流行的架构方案：Web前端采用Nginx/HAProxy+ Keepalived作负载均衡器；后端采用 MySQL数据库一主多从和读写分离，采用LVS+Keepalived的架构。当然要根据项目具体需求制定方案。

下面说说各自的特点和适用场合。

### 3.1.Nginx的优点是：

1. 工作在网络的7层之上，可以针对http应用做一些分流的策略，比如针对域名、目录结构，它的正则规则比HAProxy更为强大和灵活，这也是它目前广泛流行的主要原因之一，Nginx单凭这点可利用的场合就远多于LVS了。
2. Nginx对网络稳定性的依赖非常小，理论上能ping通就就能进行负载功能，这个也是它的优势之一；相反LVS对网络稳定性依赖比较大。
3. Nginx安装和配置比较简单，测试起来比较方便，它基本能把错误用日志打印出来。LVS的配置、测试就要花比较长的时间了，LVS对网络依赖比较大。
4. 可以承担高负载压力且稳定，在硬件不差的情况下一般能支撑几万次的并发量，负载度比LVS相对小些。
5. Nginx可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点，不过其中缺点就是不支持url来检测。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx会把上传切到另一台服务器重新处理，而LVS就直接断掉了，如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而不满。
6. Nginx不仅仅是一款优秀的负载均衡器/反向代理软件，它同时也是功能强大的Web应用服务器。LNMP也是近几年非常流行的web架构，在高流量的环境中稳定性也很好。
7. Nginx现在作为Web反向加速缓存越来越成熟了，速度比传统的Squid服务器更快，可以考虑用其作为反向代理加速器。
8. Nginx可作为中层反向代理使用，这一层面Nginx基本上无对手，唯一可以对比Nginx的就只有 lighttpd了，不过 lighttpd目前还没有做到Nginx完全的功能，配置也不那么清晰易读，社区资料也远远没Nginx活跃。
9. Nginx也可作为静态网页和图片服务器，这方面的性能也无对手。还有Nginx社区非常活跃，第三方模块也很多。

### 3.2.Nginx的缺点是：

1. Nginx仅能支持http、https和Email协议，这样就在适用范围上面小些，这个是它的缺点。
2. 对后端服务器的健康检查，只支持通过端口来检测，不支持通过url来检测。不支持Session的直接保持，但能通过ip_hash来解决。

LVS：使用Linux内核集群实现一个高性能、高可用的负载均衡服务器，它具有很好的可伸缩性（Scalability)、可靠性（Reliability)和可管理性（Manageability)。

### 3.3.LVS的优点是：

1. 抗负载能力强、是工作在网络4层之上仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的，对内存和cpu资源消耗比较低。
2. 配置性比较低，这是一个缺点也是一个优点，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率。
3. 工作稳定，因为其本身抗负载能力很强，自身有完整的双机热备方案，如LVS+Keepalived。
4. 无流量，LVS只分发请求，而流量并不从它本身出去，这点保证了均衡器IO的性能不会受到大流量的影响。
5. 应用范围比较广，因为LVS工作在4层，所以它几乎可以对所有应用做负载均衡，包括http、数据库、在线聊天室等等。

### 3.4.LVS的缺点是：

1. 软件本身不支持正则表达式处理，不能做动静分离；而现在许多网站在这方面都有较强的需求，这个是Nginx/HAProxy+Keepalived的优势所在。
2. 如果是网站应用比较庞大的话，LVS/DR+Keepalived实施起来就比较复杂了，特别后面有 Windows Server的机器的话，如果实施及配置还有维护过程就比较复杂了，相对而言，Nginx/HAProxy+Keepalived就简单多了。

### 3.5.HAProxy的特点是：

1. HAProxy也是支持虚拟主机的。
2. HAProxy的优点能够补充Nginx的一些缺点，比如支持Session的保持，Cookie的引导；同时支持通过获取指定的url来检测后端服务器的状态。
3. HAProxy跟LVS类似，本身就只是一款负载均衡软件；单纯从效率上来讲HAProxy会比Nginx有更出色的负载均衡速度，在并发处理上也是优于Nginx的。
4. HAProxy支持TCP协议的负载均衡转发，可以对MySQL读进行负载均衡，对后端的MySQL节点进行检测和负载均衡，大家可以用LVS+Keepalived对MySQL主从做负载均衡。
5. HAProxy负载均衡策略非常多，HAProxy的负载均衡算法现在具体有如下8种：
   ① roundrobin，表示简单的轮询，这个不多说，这个是负载均衡基本都具备的；
   ② static-rr，表示根据权重，建议关注；
   ③ leastconn，表示最少连接者先处理，建议关注；
   ④ source，表示根据请求源IP，这个跟Nginx的IP_hash机制类似，我们用其作为解决session问题的一种方法，建议关注；
   ⑤ ri，表示根据请求的URI；
   ⑥ rl_param，表示根据请求的URl参数’balance url_param’ requires an URL parameter name；
   ⑦ hdr(name)，表示根据HTTP请求头来锁定每一次HTTP请求；
   ⑧ rdp-cookie(name)，表示根据据cookie(name)来锁定并哈希每一次TCP请求。

### 3.6.Nginx和LVS对比的总结：

1. Nginx工作在网络的7层，所以它可以针对http应用本身来做分流策略，比如针对域名、目录结构等，相比之下LVS并不具备这样的功能，所以Nginx单凭这点可利用的场合就远多于LVS了；但Nginx有用的这些功能使其可调整度要高于LVS，所以经常要去触碰触碰，触碰多了，人为出问题的几率也就会大。
2. Nginx对网络稳定性的依赖较小，理论上只要ping得通，网页访问正常，Nginx就能连得通，这是Nginx的一大优势！Nginx同时还能区分内外网，如果是同时拥有内外网的节点，就相当于单机拥有了备份线路；LVS就比较依赖于网络环境，目前来看服务器在同一网段内并且LVS使用direct方式分流，效果较能得到保证。另外注意，LVS需要向托管商至少申请多一个ip来做Visual IP，貌似是不能用本身的IP来做VIP的。要做好LVS管理员，确实得跟进学习很多有关网络通信方面的知识，就不再是一个HTTP那么简单了。
3. Nginx安装和配置比较简单，测试起来也很方便，因为它基本能把错误用日志打印出来。LVS的安装和配置、测试就要花比较长的时间了；LVS对网络依赖比较大，很多时候不能配置成功都是因为网络问题而不是配置问题，出了问题要解决也相应的会麻烦得多。
4. Nginx也同样能承受很高负载且稳定，但负载度和稳定度差LVS还有几个等级：Nginx处理所有流量所以受限于机器IO和配置；本身的bug也还是难以避免的。
5. Nginx可以检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点。目前LVS中 ldirectd也能支持针对服务器内部的情况来监控，但LVS的原理使其不能重发请求。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx会把上传切到另一台服务器重新处理，而LVS就直接断掉了，如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而恼火。
6. Nginx对请求的异步处理可以帮助节点服务器减轻负载，假如使用 apache直接对外服务，那么出现很多的窄带链接时apache服务器将会占用大 量内存而不能释放，使用多一个Nginx做apache代理的话，这些窄带链接会被Nginx挡住，apache上就不会堆积过多的请求，这样就减少了相当多的资源占用。这点使用squid也有相同的作用，即使squid本身配置为不缓存，对apache还是有很大帮助的。
7. Nginx能支持http、https和email（email的功能比较少用），LVS所支持的应用在这点上会比Nginx更多。在使用上，一般最前端所采取的策略应是LVS，也就是DNS的指向应为LVS均衡器，LVS的优点令它非常适合做这个任务。重要的ip地址，最好交由LVS托管，比如数据库的 ip、webservice服务器的ip等等，这些ip地址随着时间推移，使用面会越来越大，如果更换ip则故障会接踵而至。所以将这些重要ip交给 LVS托管是最为稳妥的，这样做的唯一缺点是需要的VIP数量会比较多。Nginx可作为LVS节点机器使用，一是可以利用Nginx的功能，二是可以利用Nginx的性能。当然这一层面也可以直接使用squid，squid的功能方面就比Nginx弱不少了，性能上也有所逊色于Nginx。Nginx也可作为中层代理使用，这一层面Nginx基本上无对手，唯一可以撼动Nginx的就只有lighttpd了，不过lighttpd目前还没有能做到 Nginx完全的功能，配置也不那么清晰易读。另外，中层代理的IP也是重要的，所以中层代理也拥有一个VIP和LVS是最完美的方案了。具体的应用还得具体分析，如果是比较小的网站（日PV小于1000万），用Nginx就完全可以了，如果机器也不少，可以用DNS轮询，LVS所耗费的机器还是比较多的；大型网站或者重要的服务，机器不发愁的时候，要多多考虑利用LVS。

现在对网络负载均衡的使用是随着网站规模的提升根据不同的阶段来使用不同的技术：

第一阶段：利用Nginx或HAProxy进行单点的负载均衡，这一阶段服务器规模刚脱离开单服务器、单数据库的模式，需要一定的负载均衡，但是仍然规模较小没有专业的维护团队来进行维护，也没有需要进行大规模的网站部署。这样利用Nginx或HAproxy就是第一选择，此时这些东西上手快， 配置容易，在七层之上利用HTTP协议就可以。这时是第一选择。

第二阶段：随着网络服务进一步扩大，这时单点的Nginx已经不能满足，这时使用LVS或者商用Array就是首要选择，Nginx此时就作为LVS或者Array的节点来使用，具体LVS或Array的是选择是根据公司规模和预算来选择，Array的应用交付功能非常强大，本人在某项目中使用过，性价比也远高于F5，商用首选，但是一般来说这阶段相关人才跟不上业务的提升，所以购买商业负载均衡已经成为了必经之路。

第三阶段：这时网络服务已经成为主流产品，此时随着公司知名度也进一步扩展，相关人才的能力以及数量也随之提升，这时无论从开发适合自身产品的定制，以及降低成本来讲开源的LVS，已经成为首选，这时LVS会成为主流。

最终形成比较理想的基本架构为：Array/LVS — Nginx/Haproxy — Squid/Varnish — AppServer。

原文链接：https://zhuanlan.zhihu.com/p/377120309

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.285】Redis源码分析

## 1.前言

- 前言
- 宏观梳理
- 启动过程
- 网络层
- 协议层
- 业务层
- 在保存到dict 的过程中，数据的形态也一直在变化
- 定义新的数据类型
- 小结

参考《Apache Kafka源码分析》——server服务端网络开发的基本套路

![img](https://pic4.zhimg.com/80/v2-868579ca14eef865508d823f20a701e3_720w.webp)

## 2.宏观梳理

![img](https://pic3.zhimg.com/80/v2-baa9dbd535267f82d8ca60376d9fc536_720w.webp)

整个轴线是redisServer 初始化并启动eventloop， eventLoop 创建redisClient 及驱动processCommand 方法进而 执行redisCommand 向 dict 中保存数据

![img](https://pic4.zhimg.com/80/v2-58399000afde64d3834eb7bf8e13f483_720w.webp)

本文 以一个SET KEY VALUE 来分析redis的 启动和保存流程

## 3.启动过程

redis.c

```
int main(int argc, char **argv) {
    ...
    // 初始化服务器
    initServerConfig();
    ...
    // 将服务器设置为守护进程
    if (server.daemonize) daemonize();
    // 创建并初始化服务器数据结构
    initServer();
    ...
    // 运行事件处理器，一直到服务器关闭为止
    aeSetBeforeSleepProc(server.el,beforeSleep);
    aeMain(server.el);
    // 服务器关闭，停止事件循环
    aeDeleteEventLoop(server.el);
    return 0
}
```

## 4.网络层

Redis的网络监听没有采用libevent等，而是自己实现了一套简单的机遇event驱动的API，具体见ae.c。事件处理器的主循环

```
void aeMain(aeEventLoop *eventLoop) {
    eventLoop->stop = 0;
    while (!eventLoop->stop) {
        // 如果有需要在事件处理前执行的函数，那么运行它
        if (eventLoop->beforesleep != NULL)
            eventLoop->beforesleep(eventLoop);
        // 开始处理事件
        aeProcessEvents(eventLoop, AE_ALL_EVENTS);
    }
}
```

Redis 中的事件循环

![img](https://pic3.zhimg.com/80/v2-a77b95e1cf74346ff7b68fda351605f2_720w.webp)

```
int aeProcessEvents(aeEventLoop *eventLoop, int flags)
{
    struct timeval tv, *tvp;
    ... 
    // 获取最近的时间事件
    if (flags & AE_TIME_EVENTS && !(flags & AE_DONT_WAIT))
        shortest = aeSearchNearestTimer(eventLoop);
    if (shortest) {
        // 如果时间事件存在的话，那么根据最近可执行时间事件和现在时间的时间差来决定文件事件的阻塞时间
        // 计算距今最近的时间事件还要多久才能达到，并将该时间距保存在 tv 结构中
        aeGetTime(&now_sec, &now_ms);
    } else {
        // 执行到这一步，说明没有时间事件，那么根据 AE_DONT_WAIT 是否设置来决定是否阻塞，以及阻塞的时间长度
    }
    // 处理文件事件，阻塞时间由 tvp 决定
    // 类似于 java nio 中的select
    numevents = aeApiPoll(eventLoop, tvp);
    for (j = 0; j < numevents; j++) {
        // 从已就绪数组中获取事件
        aeFileEvent *fe = &eventLoop->events[eventLoop->fired[j].fd];
        int mask = eventLoop->fired[j].mask;
        int fd = eventLoop->fired[j].fd;
        // 读事件
        if (fe->mask & mask & AE_READABLE) {
            fe->rfileProc(eventLoop,fd,fe->clientData,mask);
        }
        // 写事件
        if (fe->mask & mask & AE_WRITABLE) {
            if (!rfired || fe->wfileProc != fe->rfileProc)
                fe->wfileProc(eventLoop,fd,fe->clientData,mask);
        }
    }
    // 执行时间事件
    if (flags & AE_TIME_EVENTS)
        processed += processTimeEvents(eventLoop);
}
```

这个event loop的逻辑可不孤单，netty中也有类似的EventLoop 中的 Loop 到底是什么？

Redis 中会处理两种事件：时间事件和文件事件。在每个事件循环中 Redis 都会先处理文件事件，然后再处理时间事件直到整个循环停止。 aeApiPoll 可看做文件事件的生产者（还有一部分文件事件来自accept等），processEvents 和 processTimeEvents 作为 Redis 中发生事件的消费者，每次都会从“事件池”（aeEventLoop的几个列表字段）中拉去待处理的事件进行消费。

## 5.协议层

Redis 通信协议

我们以读事件为例，但发现数据可读时，执行了 fe->rfileProc(eventLoop,fd,fe->clientData,mask);，那么rfileProc 的执行逻辑是啥呢？

1. initServer ==> aeCreateFileEvent. 初始化server 时，创建aeCreateFileEvent（aeFileEvent的一种），当accept （可读事件的一种）就绪时，触发aeCreateFileEvent->rfileProc 方法 也就是 acceptTcpHandler

```
// redis.c 
 void initServer() {
     ...
     // 为 TCP 连接关联连接应答（accept）处理器，用于接受并应答客户端的 connect() 调用
         for (j = 0; j < server.ipfd_count; j++) {
         if (aeCreateFileEvent(server.el, server.ipfd[j], AE_READABLE,acceptTcpHandler,NULL) == AE_ERR){...}
         }
     ...
 }
```

1. 创建客户端，并绑定读事件到loop：acceptTcpHandler ==> createClient ==> aeCreateFileEvent ==> readQueryFromClient

```
void acceptTcpHandler(aeEventLoop *el, int fd, void *privdata, int mask) {  int cport, cfd, max = MAX_ACCEPTS_PER_CALL;
 ...
     while(max--) {
         // accept 客户端连接
         cfd = anetTcpAccept(server.neterr, fd, cip, sizeof(cip), &cport);
         if (cfd == ANET_ERR) {
             ...
             return;
         }
         // 为客户端创建客户端状态（redisClient）
         acceptCommonHandler(cfd,0);
     }
 }
 static void acceptCommonHandler(int fd, int flags) {
     // 创建客户端
     redisClient *c;
     if ((c = createClient(fd)) == NULL) {
         ...
         close(fd); /* May be already closed, just ignore errors */
         return;
     }
     // 如果新添加的客户端令服务器的最大客户端数量达到了，那么向新客户端写入错误信息，并关闭新客户端
     // 先创建客户端，再进行数量检查是为了方便地进行错误信息写入
     ...
 }
 redisClient *createClient(int fd) {
     // 分配空间
     redisClient *c = zmalloc(sizeof(redisClient));
     if (fd != -1) {
         ...
         //绑定读事件到事件 loop （开始接收命令请求）
         if (aeCreateFileEvent(server.el,fd,AE_READABLE,
             readQueryFromClient, c) == AE_ERR){
             // 清理/关闭资源退出
         }
     }
     // 初始化redisClient其它数据
 }
```

1. 拼接和分发命令数据 readQueryFromClient ==> processInputBuffer ==> processCommand

```
networking.c
 void readQueryFromClient(aeEventLoop *el, int fd, void *privdata, int mask) {
     redisClient *c = (redisClient*) privdata;
     // 获取查询缓冲区当前内容的长度
     // 如果读取出现 short read ，那么可能会有内容滞留在读取缓冲区里面
     // 这些滞留内容也许不能完整构成一个符合协议的命令，
     qblen = sdslen(c->querybuf);
     // 如果有需要，更新缓冲区内容长度的峰值（peak）
     if (c->querybuf_peak < qblen) c->querybuf_peak = qblen;
     // 为查询缓冲区分配空间
     c->querybuf = sdsMakeRoomFor(c->querybuf, readlen);
     // 读入内容到查询缓存
     nread = read(fd, c->querybuf+qblen, readlen);
     // 读入出错
     // 遇到 EOF
     if (nread) {
         // 根据内容，更新查询缓冲区（SDS） free 和 len 属性
         // 并将 '\0' 正确地放到内容的最后
         sdsIncrLen(c->querybuf,nread);
         // 记录服务器和客户端最后一次互动的时间
         c->lastinteraction = server.unixtime;
         // 如果客户端是 master 的话，更新它的复制偏移量
         if (c->flags & REDIS_MASTER) c->reploff += nread;
     } else {
         // 在 nread == -1 且 errno == EAGAIN 时运行
         server.current_client = NULL;
         return;
     }
     // 查询缓冲区长度超出服务器最大缓冲区长度
     // 清空缓冲区并释放客户端
     // 从查询缓存重读取内容，创建参数，并执行命令
     // 函数会执行到缓存中的所有内容都被处理完为止
     processInputBuffer(c);
     server.current_client = NULL;
 }
 // 处理客户端输入的命令内容
 void processInputBuffer(redisClient *c) {
     // 尽可能地处理查询缓冲区中的内容
     while(sdslen(c->querybuf)) {
         ...
         // 判断请求的类型
         // 简单来说，多条查询是一般客户端发送来的，
         // 而内联查询则是 TELNET 发送来的
         if (!c->reqtype) {
             if (c->querybuf[0] == '*') {
                 // 多条查询
                 c->reqtype = REDIS_REQ_MULTIBULK;
             } else {
                 // 内联查询
                 c->reqtype = REDIS_REQ_INLINE;
             }
         }
         // 将缓冲区中的内容转换成命令，以及命令参数
         if (c->reqtype == REDIS_REQ_INLINE) {
             if (processInlineBuffer(c) != REDIS_OK) break;
         } else if (c->reqtype == REDIS_REQ_MULTIBULK) {
             if (processMultibulkBuffer(c) != REDIS_OK) break;
         } else {
             redisPanic("Unknown request type");
         }
         ...
     }
 }
 redis.c
 int processCommand(redisClient *c) {
     // 特别处理 quit 命令
     // 查找命令，并进行命令合法性检查，以及命令参数个数检查
     c->cmd = c->lastcmd = lookupCommand(c->argv[0]->ptr);
     // 没找到指定的命令 或 参数个数错误 直接返回
     // 检查认证信息
     // 如果开启了集群模式，那么在这里进行转向操作。
     // 如果设置了最大内存，那么检查内存是否超过限制，并做相应的操作
     // 如果这是一个主服务器，并且这个服务器之前执行 BGSAVE 时发生了错误
     // 那么不执行写命令
     // 如果服务器没有足够多的状态良好服务器
     // 并且 min-slaves-to-write 选项已打开
     // 如果这个服务器是一个只读 slave 的话，那么拒绝执行写命令
     // 在订阅于发布模式的上下文中，只能执行订阅和退订相关的命令
     /* Only allow INFO and SLAVEOF when slave-serve-stale-data is no and
     * we are a slave with a broken link with master. */
     // 如果服务器正在载入数据到数据库，那么只执行带有 REDIS_CMD_LOADING
     // 标识的命令，否则将出错
     /* Lua script too slow? Only allow a limited number of commands. */
     // Lua 脚本超时，只允许执行限定的操作，比如 SHUTDOWN 和 SCRIPT KILL
     /* Exec the command */
     if (c->flags & REDIS_MULTI &&
         c->cmd->proc != execCommand && c->cmd->proc != discardCommand &&
         c->cmd->proc != multiCommand && c->cmd->proc != watchCommand)
     {
         // 在事务上下文中除 EXEC 、 DISCARD 、 MULTI 和 WATCH 命令之外
         // 其他所有命令都会被入队到事务队列中
         queueMultiCommand(c);
         addReply(c,shared.queued);
     } else {
         // 执行命令
         call(c,REDIS_CALL_FULL);
         c->woff = server.master_repl_offset;
         // 处理那些解除了阻塞的键
         if (listLength(server.ready_keys))
             handleClientsBlockedOnLists();
     }
     return REDIS_OK;
 }
```

![img](https://pic3.zhimg.com/80/v2-173535b22864f336a6727233c320cdfe_720w.webp)

## 6.业务层

```
redis.c
// 调用命令的实现函数，执行命令
void call(redisClient *c, int flags) {
    // start 记录命令开始执行的时间
    // 记录命令开始执行前的 FLAG
    // 如果可以的话，将命令发送到 MONITOR
    /* Call the command. */
    c->flags &= ~(REDIS_FORCE_AOF|REDIS_FORCE_REPL);
    redisOpArrayInit(&server.also_propagate);
    // 保留旧 dirty 计数器值
    dirty = server.dirty;
    // 计算命令开始执行的时间
    start = ustime();
    // 执行实现函数
    c->cmd->proc(c);
    // 计算命令执行耗费的时间
    duration = ustime()-start;
    // 计算命令执行之后的 dirty 值
    dirty = server.dirty-dirty;
    ...
    // 如果有需要，将命令放到 SLOWLOG 里面
    // 更新命令的统计信息
    ...
    server.stat_numcommands++;
}
redis.c
struct redisCommand redisCommandTable[] = {
    {"get",getCommand,2,"r",0,NULL,1,1,1,0,0},
    {"set",setCommand,-3,"wm",0,NULL,1,1,1,0,0},
    ...
}
```

![img](https://pic4.zhimg.com/80/v2-da1bb12d12143388e220b11bc0abdefb_720w.webp)

```
t_string.c
/* SET key value [NX] [XX] [EX <seconds>] [PX <milliseconds>] */
void setCommand(redisClient *c) {
    int j;
    robj *expire = NULL;
    int unit = UNIT_SECONDS;
    int flags = REDIS_SET_NO_FLAGS;
    // 设置选项参数
    // 尝试对值对象进行编码
    c->argv[2] = tryObjectEncoding(c->argv[2]);
    setGenericCommand(c,flags,c->argv[1],c->argv[2],expire,unit,NULL,NULL);
}
void setGenericCommand(redisClient *c, int flags, robj *key, robj *val, robj *expire, int unit, robj *ok_reply, robj *abort_reply) {
    long long milliseconds = 0; /* initialized to avoid any harmness warning */
    // 取出过期时间
    // 如果设置了 NX 或者 XX 参数，那么检查条件是否不符合这两个设置
    // 在条件不符合时报错，报错的内容由 abort_reply 参数决定
    // 将键值关联到数据库
    setKey(c->db,key,val);
    // 将数据库设为脏
    // 为键设置过期时间
    if (expire) setExpire(c->db,key,mstime()+milliseconds);
    // 发送事件通知
    // 设置成功，向客户端发送回复
}
db.c
void setKey(redisDb *db, robj *key, robj *val) {
    // 添加或覆写数据库中的键值对
    if (lookupKeyWrite(db,key) == NULL) {
        dbAdd(db,key,val);
    } else {
        dbOverwrite(db,key,val);
    }
    incrRefCount(val);
    // 移除键的过期时间
    removeExpire(db,key);
    // 发送键修改通知
    signalModifiedKey(db,key);
}
```

前面说过， 命令实现函数会将命令回复保存到客户端的输出缓冲区里面， 并为客户端的套接字关联命令回复处理器， 当客户端套接字变为可写状态时， 服务器就会执行命令回复处理器， 将保存在客户端输出缓冲区中的命令回复发送给客户端。

当命令回复发送完毕之后， 回复处理器会清空客户端状态的输出缓冲区， 为处理下一个命令请求做好准备。

## 7.在保存到dict 的过程中，数据的形态也一直在变化

相关的数据结构

```
struct redisClient {
    // 查询缓冲区
    sds querybuf;
    // 参数数量
    int argc;
    // 参数对象数组
    robj **argv;    
}
typedef struct redisObject {
    // 类型
    unsigned type:4;
    // 编码
    unsigned encoding:4;
    // 对象最后一次被访问的时间
    unsigned lru:REDIS_LRU_BITS; /* lru time (relative to server.lruclock) */
    // 引用计数
    int refcount;
    // 指向实际值的指针
    void *ptr;
} robj;
```

转换的代码

```
networking.c
// 将 c->querybuf 中的协议内容转换成 c->argv 中的参数对象
int processMultibulkBuffer(redisClient *c) {
    // 读入命令的参数个数
    // 比如 *3\r\n$3\r\nSET\r\n... 将令 c->multibulklen = 3
    if (c->multibulklen == 0) {
        // 检查缓冲区的内容第一个 "\r\n"
        newline = strchr(c->querybuf,'\r');
        if (newline == NULL) {
            ...
            return REDIS_ERR;
        }
        // 协议的第一个字符必须是 '*'
        // 将参数个数，也即是 * 之后， \r\n 之前的数字取出并保存到 ll 中
        // 比如对于 *3\r\n ，那么 ll 将等于 3
        ok = string2ll(c->querybuf+1,newline-(c->querybuf+1),&ll);
        // 参数的数量超出限制
        // 设置参数数量
        // 根据参数数量，为各个参数对象分配空间
        if (c->argv) zfree(c->argv);
        c->argv = zmalloc(sizeof(robj*)*c->multibulklen);
    }
    // 从 c->querybuf 中读入参数，并创建各个参数对象到 c->argv
    while(c->multibulklen) {
        // 读入参数长度
        if (c->bulklen == -1) {
            // 确保 "\r\n" 存在
            // 确保协议符合参数格式，检查其中的 $...
            // 读取长度
            // 比如 $3\r\nSET\r\n 将会让 ll 的值设置 3
            ok = string2ll(c->querybuf+pos+1,newline-(c->querybuf+pos+1),&ll);
            ...
            // 参数的长度
            c->bulklen = ll;
        }
        // 读入参数
        // 确保内容符合协议格式
        // 为参数创建字符串对象  
        if (pos == 0 &&
            c->bulklen >= REDIS_MBULK_BIG_ARG &&
            (signed) sdslen(c->querybuf) == c->bulklen+2){
            c->argv[c->argc++] = createObject(REDIS_STRING,c->querybuf);
            sdsIncrLen(c->querybuf,-2); /* remove CRLF */
            c->querybuf = sdsempty();
            /* Assume that if we saw a fat argument we'll see another one
            * likely... */
            c->querybuf = sdsMakeRoomFor(c->querybuf,c->bulklen+2);
            pos = 0;
        } else {
            c->argv[c->argc++] =
                createStringObject(c->querybuf+pos,c->bulklen);
            pos += c->bulklen+2;
        }
        // 清空参数长度
        // 减少还需读入的参数个数
        c->multibulklen--;    
    }
    // 从 querybuf 中删除已被读取的内容
    // 如果本条命令的所有参数都已读取完，那么返回
    // 如果还有参数未读取完，那么就协议内容有错
}
```

object.c

```
robj *createObject(int type, void *ptr) {
    robj *o = zmalloc(sizeof(*o));
    o->type = type;
    o->encoding = REDIS_ENCODING_RAW;
    o->ptr = ptr;
    o->refcount = 1;
    /* Set the LRU to the current lruclock (minutes resolution). */
    o->lru = LRU_CLOCK();
    return o;
}
```

1. 最开始命令数据在redisClient->querybuf 中以字符串形式存在

![img](https://pic3.zhimg.com/80/v2-152908a774b7986df3ac61dbcc327a42_720w.webp)

1. processMultibulkBuffer 然后字符串 数据被拆分为 redisObject 保存在 redisClient->argv[1],redisClient->argv[2]，当然redisObject 的类型仍被标记为字符串

![img](https://pic3.zhimg.com/80/v2-d85b7022c61da392481b62200729b926_720w.webp)

1. t_string.c setCommand 对值对象进行编码
2. 到db.c 时，setKey(robj *key,robj *val)
3. dict.c dictAdd(void *key, void *val) key 已被转换为 sds。

## 8.定义新的数据类型

来自 《Redis核心技术与实现》

1. 定义新数据类型的底层结构，可以自己创建和命名.h 和 .c 文件
2. 在 RedisObject 的 type 属性中，增加这个新类型的定义。在 Redis 的 server.h 文件中
3. 开发新类型的创建和释放函数。主要是用 zmalloc 做底层结构分配空间。
4. 开发新类型的命令操作。在 server.c 文件中的 redisCommandTable 里面，把新增命令和实现函数关联起来。

## 9.小结

**如果你看到一个新东西，却没有理清它的逻辑，直到打通你已熟悉的东西（学名叫已有的知识体系），那肯定是没有真正理解它**。 在redis 源码分析这里，你已知的是各种内存操作（即业务层部分），未知的网络层到业务层的通路。

原文链接：https://zhuanlan.zhihu.com/p/376338968

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)



# 【NO.286】后端开发程序员须彻底搞懂的 IO 底层原理

## 1.混乱的 IO 概念

IO是Input和Output的缩写，即输入和输出。广义上的围绕计算机的输入输出有很多：鼠标、键盘、扫描仪等等。而我们今天要探讨的是在计算机里面，主要是作用在内存、网卡、硬盘等硬件设备上的输入输出操作。

谈起IO的模型，大多数人脑子里肯定是一坨混乱的概念，“阻塞”、“非阻塞”，“同步”、“异步”有什么区别？很多同学傻傻分不清，有尝试去搜索相关资料去探究真相，结果又被淹没在茫茫的概念之中。

这里尝试简单地去解释下为啥会出现这种现象，其中一个很重要的原因就是大家看到的资料对概念的解释都站在了不同的角度，有的站在了底层内核的视角，有的直接在java层面或者Netty框架层面给大家介绍API，所以给大家造成了一定程度的困扰。

所以在开篇之前，还是要说下本文所站的视角，我们将会从底层内核的层面给大家讲解下IO。因为万变不离其宗，只有了解了底层原理，不管语言层面如何花里胡哨，我们都能以不变应万变。

## 2.用户空间和内核空间

为了便于大家理解复杂的IO以及零拷贝相关的技术，我们还是得花点时间在回顾下操作系统相关的知识。这一节我们重点看下用户空间和内核空间，基于此后面我们才能更好地聊聊多路复用和零拷贝。

![img](https://pic4.zhimg.com/80/v2-07ddbe9b3bd904da1569c21153ff5e63_720w.webp)

**硬 件 层（Hardware）**

> 包括和我们熟知的和IO相关的CPU、内存、磁盘和网卡几个硬件；

**内核空间（Kernel Space）**

> 计算机开机后首先会运行内核程序，内核程序占用的一块私有的空间就是内核空间，并且可支持访问CPU所有的指令集（ring0 - ring3）以及所有的内存空间、IO及硬件设备；

**用户空间（User Space）**

> 每个普通的用户进程都有一个单独的用户空间，用户空间只能访问受限的资源（CPU的“保护模式”）也就是说用户空间是无法直接操作像内存、网卡和磁盘等硬件的；

如上所述，那我们可能会有疑问，用户空间的进程想要去访问或操作磁盘和网卡该怎么办呢？

为此，操作系统在内核中开辟了一块唯一且合法的调用入口“System Call Interface”，也就是我们常说的系统调用，系统调用为上层用户提供了一组能够操作底层硬件的API。这样一来，用户进程就可以通过系统调用访问到操作系统内核，进而就能够间接地完成对底层硬件的操作。这个访问的过程也即用户态到内核态的切换。常见的系统调用有很多，比如：内存映射mmap()、文件操作类的open()、IO读写read()、write()等等。

## 3.IO模型

### 3.1. BIO（Blocking IO）

我们先看一下大家都熟悉的BIO模型的 Java 伪代码：

```
ServerSocket serverSocket = new ServerSocket(8080);        // step1: 创建一个ServerSocket，并监听8080端口
while(true) {                                              // step2: 主线程进入死循环
    Socket socket = serverSocket.accept();                 // step3: 线程阻塞，开启监听
    BufferedReader reader = new BufferedReader(nwe InputStreamReader(socket.getInputStream()));
    System.out.println("read data: " + reader.readLine()); // step4: 数据读取
    PrintWriter print = new PrintWriter(socket.getOutputStream(), true);
    print.println("write data");                           // step5: socket数据写入
}
```

这段代码可以简单理解成一下几个步骤：

- 创建ServerSocket，并监听8080端口；
- 主线程进入死循环，用来阻塞监听客户端的连接，socket.accept()；
- 数据读取，socket.read()；
- 写入数据，socket.write()；

**问题**

以上三个步骤：accept(…)、read(…)、write(…)都会造成线程阻塞。上述这个代码使用了单线程，会导致主线程会直接夯死在阻塞的地方。

**优化**

我们要知道一点“**进程的阻塞是不会消耗CPU资源的**”，所以在多核的环境下，我们可以创建多线程，把接收到的请求抛给多线程去处理，这样就有效地利用了计算机的多核资源。甚至为了避免创建大量的线程处理请求，我们还可以进一步做优化，创建一个线程池，利用池化技术，对暂时处理不了的请求做一个缓冲。

### 3.2.“C10K”问题

> “C10K”即“client 10k”用来指代数量庞大的客户端；

BIO看上去非常的简单，事实上采用“BIO+线程池”来处理少量的并发请求还是比较合适的，也是最优的。但是面临数量庞大的客户端和请求，这时候使用多线程的弊端就逐渐凸显出来了：

- 严重依赖线程，线程还是比较耗系统资源的（一个线程大约占用1M的空间）；
- 频繁地创建和销毁代价很大，因为涉及到复杂的系统调用；
- 线程间上下文切换的成本很高，因为发生线程切换前，需要保留上一个任务的状态，以便切回来的时候，可以再次加载这个任务的状态。如果线程数量庞大，会造成线程做上下文切换的时间甚至大于线程执行的时间，CPU负载变高。

### 3.3.NIO非阻塞模型

下面开始真正走向Java NIO或者Netty框架所描述的“**非阻塞**”，NIO叫Non-Blocking IO或者New IO，由于BIO可能会引入的大量线程，所以可以简单地理解NIO处理问题的方式是通过单线程或者少量线程达到处理大量客户端请求的目的。为了达成这个目的，首先要做的就是把阻塞的过程非阻塞化。要想做到非阻塞，那必须得要有内核的支持，同时需要对用户空间的进程暴露系统调用函数。所以，这里的“非阻塞”可以理解成系统调用API级别的，而真正底层的IO操作都是阻塞的，我们后面会慢慢介绍。

事实上，内核已经对“非阻塞”做好了支持，举个我们刚刚说的的accept()方法阻塞的例子（Tips：java中的accept方法对应的系统调用函数也叫accept），看下官方文档对其非阻塞部分的描述。

![img](https://pic3.zhimg.com/80/v2-05a1bbbb74e3e33d1e8d4cf691a5ed4e_720w.webp)

官方文档对accetp()系统调用的描述是通过把”**flags**“参数设成”**SOCK_NONBLOCK**“就可以达到非阻塞的目的，非阻塞之后线程会一直处理轮询调用，这时候可以通过每次返回特殊的异常码“**EAGAIN**”或”**EWOULDBLOCK**“告诉主程序还没有连接到达可以继续轮询。

我们可以很容易想象程序非阻塞之后的一个大致过程。所以，非阻塞模式有个最大的特点就是：**用户进程需要不断去主动询问内核数据准备好了没有！**

下面我们通过一段伪代码，看下这个调用过程：

```
// 循环遍历
while(1) {
    // 遍历fd集合
    for (fdx in range(fd1, fdn)) {
        // 如果fdx有数据
        if (null != fdx.data) {
            // 进行读取和处理
            read(fdx)&handle(fdx);
        }
    }
}
```

这种调用方式也暴露出非阻塞模式的最大的弊端，就是需要让用户进程不断切换到内核态，对连接状态或读写数据做轮询。有没有一种方式来简化用户空间for循环轮询的过程呢？那就是我们下面要重点介绍的**IO多路复用模型**。

### 3.4.IO多路复用模型

非阻塞模型会让用户进程一直轮询调用系统函数，频繁地做内核态切换。想要做优化其实也比较简单，我们假想个业务场景，A业务系统会调用B的基础服务查询单个用户的信息。随着业务的发展，A的逻辑变复杂了，需要查100个用户的信息。很明显，A希望B提供一个批量查询的接口，用集合作为入参，一次性把数据传递过去就省去了频繁的系统间调用。

多路复用实际也差不多就是这个实现思路，只不过入参这个“集合”需要你注册/填写感兴趣的事件，读fd、写fd或者连接状态的fd等，然后交给内核帮你进行处理。

那我们就具体来看看多路复用里面大家都可能听过的几个系统调用 **- select()、poll()、epoll()。**

#### 3.4.1.select()

**select()** 构造函数信息如下所示：

```
/**
 * select()系统调用
 *
 * 参数列表：
 *     nfds       - 值为最大的文件描述符+1
 *    *readfds    - 用户检查可读性
 *    *writefds   - 用户检查可写性
 *    *exceptfds  - 用于检查外带数据
 *    *timeout    - 超时时间的结构体指针
 */
int select（int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout）;
```

官方文档对**select()**的描述：

> DESCRIPTION
> select() and pselect() allow a program to monitor multiple file descriptors, waiting until one or more of the file descriptors become “ready” for some class of I/O operation (e.g.,input possible). A file descriptor is considered ready if it is possible to perform the corresponding I/O operation (e.g., read(2)) without blocking.

select()允许程序监控多个fd，阻塞等待直到一个或多个fd到达”就绪”状态。

内核使用**select()**为用户进程提供了类似批量的接口，函数本身也会一直阻塞直到有fd为就绪状态返回。下面我们来具体看下**select()**函数实现，以便我们更好地分析它有哪些优缺点。在**select()**函数的构造器里，我们很容易看到”**fd_set**“这个入参类型。它是用位图算法bitmap实现的，使用了一个大小固定的数组（fd_set设置了FD_SETSIZE固定长度为**1024**），数组中的每个元素都是0和1这样的二进制byte，0,1映射fd对应位置上是否有读写事件，举例：如果fd == 5，那么fd_set = 000001000。

同时 **fd_set** 定义了四个宏来处理bitmap：

> FD_ZERO(&set); // 初始化，清空的作用，使集合中不含任何fd
> FD_SET(fd, &set); // 将fd加入set集合，给某个位置赋值的操作
> FD_CLR(fd, &set); // 将fd从set集合中清除，去掉某个位置的值
> FD_ISSET(fd, &set); // 校验某位置的fd是否在集合中

使用bitmap算法的好处非常明显，运算效率高，占用内存少（使用了一个byte，8bit）。我们用伪代码和图片来描述下用户进程调用select()的过程：

![img](https://pic3.zhimg.com/80/v2-a207409d2d902768399028322ae95cfe_720w.webp)

假设fds为{1, 2, 3, 5, 7}对应的bitmap为”01110101”，抛给内核空间轮询，当有读写事件时重新标记同时停止阻塞，然后整体返回用户空间。由此我们可以看到select()系统调用的弊端也是比较明显的：

- 复杂度O(n)，轮询的任务交给了内核来做，复杂度并没有变化，数据取出后也需要轮询哪个fd上发生了变动；
- 用户态还是需要不断切换到内核态，直到所有的fds数据读取结束，整体开销依然很大；
- fd_set有大小的限制，目前被硬编码成了**1024**；
- fd_set不可重用，每次操作完都必须重置；

#### 3.4.2 poll()

**poll()** 构造函数信息如下所示：

```
/**
 * poll()系统调用
 *
 * 参数列表：
 *    *fds         - pollfd结构体
 *     nfds        - 要监视的描述符的数量
 *     timeout     - 等待时间
 */
int poll（struct pollfd *fds, nfds_t nfds, int *timeout）;
### pollfd的结构体
struct pollfd{
　int fd；// 文件描述符
　short event；// 请求的事件
　short revent；// 返回的事件
}
```

官方文档对**poll()**的描述：

> DESCRIPTION
> poll() performs a similar task to select(2): it waits for one of a set of file descriptors to become ready to perform I/O.

poll() 非常像select()，它也是阻塞等待直到一个或多个fd到达”就绪”状态。

看官方文档描述可以知道，**poll()**和**select()**是非常相似的，唯一的区别在于**poll()**摒弃掉了位图算法，使用自定义的结构体**pollfd**，在**pollfd**内部封装了fd，并通过event变量注册感兴趣的可读可写事件（**POLLIN、POLLOUT**），最后把 **pollfd** 交给内核。当有读写事件触发的时候，我们可以通过轮询 **pollfd**，判断revent确定该fd是否发生了可读可写事件。

老样子我们用伪代码来描述下用户进程调用 **poll()** 的过程：

![img](https://pic1.zhimg.com/80/v2-e737ae55f45e932681bd7b969d975964_720w.webp)

**poll()** 相对于**select()**，主要的优势是使用了pollfd的结构体：

- 没有了bitmap大小1024的限制；
- 通过结构体中的revents置位；

但是用户态到内核态切换及O(n)复杂度的问题依旧存在。

#### 3.4.3 epoll()

epoll()应该是目前最主流，使用范围最广的一组多路复用的函数调用，像我们熟知的Nginx、Redis都广泛地使用了此种模式。接下来我们重点分析下，epoll()的实现采用了“三步走”策略，它们分别是**epoll_create()、epoll_ctl()、epoll_wait()。**

4.3.1 epoll_create()

```
/**
 * 返回专用的文件描述符
 */
int epoll_create（int size）;
```

用户进程通过 **epoll_create()** 函数在内核空间里面创建了一块空间（为了便于理解，可以想象成创建了一块白板），并返回了描述此空间的fd。

4.3.2 epoll_ctl()

```
/**
 * epoll_ctl()系统调用
 *
 * 参数列表：
 *     epfd       - 由epoll_create()返回的epoll专用的文件描述符
 *     op         - 要进行的操作例如注册事件,可能的取值:注册-EPOLL_CTL_ADD、修改-EPOLL_CTL_MOD、删除-EPOLL_CTL_DEL
 *     fd         - 关联的文件描述符
 *     event      - 指向epoll_event的指针
 */
int epoll_ctl（int epfd, int op, int fd , struce epoll_event *event ）;
```

刚刚我们说通过**epoll_create()**可以创建一块具体的空间“白板”，那么通过**epoll_ctl()** 我们可以通过自定义的epoll_event结构体在这块”白板上”注册感兴趣的事件了。

- 注册 - EPOLL_CTL_ADD
- 修改 - EPOLL_CTL_MOD
- 删除 - EPOLL_CTL_DEL

4.3.3 epoll_wait()

```
 /**
 * epoll_wait()返回n个可读可写的fds
 *
 * 参数列表：
 *     epfd           - 由epoll_create()返回的epoll专用的文件描述符
 *     epoll_event    - 要进行的操作例如注册事件,可能的取值:注册-EPOLL_CTL_ADD、修改-EPOLL_CTL_MOD、删除-EPOLL_CTL_DEL
 *     maxevents      - 每次能处理的事件数
 *     timeout        - 等待I/O事件发生的超时值；-1相当于阻塞，0相当于非阻塞。一般用-1即可
 */
int epoll_wait（int epfd, struce epoll_event *event , int maxevents, int timeout）; 
```

**epoll_wait()** 会一直阻塞等待，直到硬盘、网卡等硬件设备数据准备完成后发起**硬中断**，中断CPU，CPU会立即执行数据拷贝工作，数据从磁盘缓冲传输到内核缓冲，同时将准备完成的fd放到就绪队列中供用户态进行读取。用户态阻塞停止，接收到**具体数量**的可读写的fds，返回用户态进行数据处理。

整体过程可以通过下面的伪代码和图示进一步了解：

![img](https://pic4.zhimg.com/80/v2-5020b6d682a0088464dae8a94f601cc7_720w.webp)

**epoll()** 基本上完美地解决了 **poll()** 函数遗留的两个问题：

- 没有了频繁的用户态到内核态的切换；
- O(1)复杂度，返回的”nfds”是一个确定的可读写的数量，相比于之前循环n次来确认，复杂度降低了不少；

## 4.同步、异步

细心的朋友可能会发现，本篇文章一直在解释“**阻塞**”和“**非阻塞**”，“**同步**”、“**异步**”的概念没有涉及，其实在很多场景下同步&异步和阻塞&非阻塞基本上是一个同义词。阻塞和非阻塞适合从系统调用API层面来看，就像我们本文介绍的select()、poll()这样的系统调用，同步和异步更适合站在应用程序的角度来看。应用程序在同步执行代码片段的时候结果不会立即返回，这时候底层IO操作不一定是阻塞的，也完全有可能是非阻塞。所以说：

- **阻塞和非阻塞：**读写没有就绪或者读写没有完成，函数是否要一直等待还是采用轮询；
- **同步和异步：**同步是读写由应用程序完成。异步是读写由操作系统来完成，并通过回调的机制通知应用程序。

这边顺便提两种大家可能会经常听到的模式：**Reactor和Preactor。**

- **Reactor 模式：主动模式。**
- **Preactor 模式：被动模式。**

## 5.总结

本篇文章从底层讲解了下从BIO到NIO的一个过程，着重介绍了IO多路复用的几个系统调用select()、poll()、epoll()，分析了下各自的优劣，技术都是持续发展演进的，目前也有很多的痛点。

原文链接：[彻底搞懂 IO 底层原理 - vivo互联网技术 - 博客园](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/vivotech/p/14059895.html)

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.287】Linux网络编程-UDP和TCP协议详解

## 1.引言

> 网络协议是每个程序员都要掌握的基础知识,干啥都离不开网络,就算在家里新买了个路由器不是吗,同事连不上网,你的女朋友手机没有网看剧了正看到高潮部分,到那时候你打开百度……那嫌弃的你的眼神仿佛在说,就这?程序员连个网都不会修?以上都是臆想,以实际为准.虽然看完本文章,你还是需要去百度怎么修复网络问题,但是你已经知道为什么会出现这种问题了!

## 2. UDP

UDP协议全称是用户数据报协议,在网络中它与TCP协议一样用于处理数据包,是一种无连接的协议.在OSI中,第四层传输层,处于IP协议的上一层UDP有不提供数据包分组,组装和不能对数据包进行排序的缺点,也就是说,当报文发送后,无法监控其是否完整安全到达的,就想一个坏掉了的水龙头,你不论怎么让他停止他都只会输出,也不管你的桶满没满,就像爱一样

### 2.1.特点

#### 2.1.1. 面向无连接:

首先UDP是不需要和TCP一样在发送数据前进行三次握手建立连接的,想发数据就可以开始发送了.并且也只是主句报文的搬运工,不会对数据报文进行任何拆分和拼接操作

**具体来说:**

- 在发送端,应用层将数据传递给UDP协议,UDP只会给数据增加一个UDP头标识下UDP,然后就传递给网络层了
- 在接收端,网络层将数据传递给传输层,UDP只去除IP报头就传递给应用层,不会任何拼接操作

#### 2.2.2. 有单播,多播,广播的功能

UDP不止支持一对一的传输方式,同样支持一对多,多对多,多对一的方式,也就是说UDP提供了单播,多播和广播的功能.

#### 2.2.3. UDP是面向报文的

发送方的UDP读应用程序交下来的报文,在添加首部后就向下交付IP层.UDP对应用层交下来的报文,既不合并,也不拆分,而是保留这些报文的边界.因此,应用程序必须选择合适大小的报文

#### 2.2.4. 不可靠性

首先不可靠性体现在无连接上,通信不需要建立连接,想发就发,这样的情况肯定不可靠

并且受到什么数据就传递什么数据,并且也不会备份数据,发送数据也不会关心对方是否已经正确接收到数据了

再者网络环境时好时坏,但是UDP因为没有拥塞控制,一直会以恒定的速度发送数据,即使网络条件不好,也不会对发送数据进行调整.这样实现的弊端就是在网络条件不好的情况下会导致丢包,但是有点也很明显,比如电话会议等等最好就是UDP

#### 2.2.5. 头部开销小,传输数据报文是很高效的.

![img](https://pic4.zhimg.com/80/v2-fb3c7e36466e526b5fcecd40ab83fe07_720w.webp)

因此UDP的头部开销小,只有八字节,相比TCP的至少二十字节要少得多,在传输数据报文时是很高效的

## 3. TCP/IP网络模型

计算机网络设备相互通信,双方必须基于相同的方法.比如如何探测到通信目标,由那一边先发起通信,使用哪种语言进行通信,怎样结束通信等规则都需要实现确定.不同硬件,操作系统之间的通信,所有的这一切都需要一种规则,而我们吧这种规则成为协议.

**TCP/IP**是互联网相关的各类协议的总称,比如:**TCP，UDP，IP，FTP，HTTP，ICMP，SMTP **等都属于 TCP/IP 族内的协议

大家一定知道OSI七层模型,**TCP/IP**概念模型是这样的

![img](https://pic1.zhimg.com/80/v2-a267695371b6b140c23b2a4eedc42b0c_720w.webp)

- 应用层:负责向用户提供应用程序.比如HTTP,FTP,Telnet,DNS,SMTP等
- 传输层:负责对报文进行分组和重组,并以TCP或UDP协议格式封装报文
- 网络层:负责路由以及把分组报文发送给目标网络或主机
- 链路层:负责封装和接缝IP报文,发送和接收ARP/RARP报文等

## 4.TCP

当一台计算机想要与另一台计算机通讯时,两台计算机之间的通信需要畅通且可靠,这样才能保证正确收发数据.例如当你收文件时候不希望收到的是一个损坏的文件,发过来的小电影是无法放映的,或者直接变成马赛克,当然也可能本来就有马赛克,这不是我们希望得到的,于是就用到了TCP
TCP协议全称是传输控制协议,,这是一种面向连接的,可靠的,基于字节流的传输层通信协议

### 4.1.TCP的连接过程

![img](https://pic3.zhimg.com/80/v2-7fc134a072a49086ff8e86325bccc2b2_720w.webp)

TCP报文也分为首部和数据两部分,首部默认情况下一般是20字节长度,但在20字节长度,但在一些请求情况下,会使用”可选字段”,这时,首部长度会有所增加,但最长不超过60字节

**源端口 - 16bit**
　　来源处的端口号；端口号有65536个，即$2^{16}$。
**目的端口 - 16bit**
　　目的处的端口号
**序号 - 32bit**
　　TCP在对数据进行分段的时候，会给每一个TCP报文段添加一个序号，序号字段的值其实是该文段所发送的数据的第一个字节的序号。这么做的原因是，TCP是面向连接的可靠服务，这个序号可以保证数据在传输过程中保持有序性，接受端可以通过这个序号确认收到的数据的完整性和先后顺序；
**确认号 - 32bit**
　　确认号，是期望收到对方的下一个报文段的数据的第一个字节的序号；
**数据偏移 - 4bit**
　　其实它本质上就是“首部长度”，因为“数据偏移”是指TCP报文段的数据部分的起始处距离TCP报文段的起始处的距离。（仍然很拗口，但相信你能明白）。
　　数据偏移总共占4bit，因此最大能表示的数值为15。但TCP的报文头部至少为20字节。因此数据偏移的单位是“4字节”，此处的设计和IP数据报的设计是完全相同的，所以说TCP报文段首部的长度最长为15×4=60字节，且首部长度必须为4字节的整数倍。
**保留字段 - 6bit**
　　IETF文档指出，这6bit在标准中是保留字段，留待以后使用，必须为0。我猜测，有两个目的，第一个是预留除URG/ACK/PSH/RST/SYN/FIN/之外的冗余功能位；第二个是为了对齐字节位。
**控制位 - 6bit**
又称为TCP flag，该字段从左到右分为以下六个字段，指明包的类型。同时用于控制TCP的状态机，同时ACK和SYN与三次握手协议有关，FIN与四次挥手协议有关。
　　**① 紧急字段URG - 1bit**
　　　　当URG=1时，此字段告诉系统此报文段中有紧急数据，应尽快传送。
　　**② 确认字段ACK - 1bit**
　　　　当ACK=1时，表示确认，且确认号有效；当ACK=0时，确认号字段无效。
　　**③ 推送字段PSH - 1bit**
　　　　当PSH=1时，则报文段会被尽快地交付给目的方，不会对这样的报文段使用缓存策略。
　　**④ 复位字段RST - 1bit**
　　　　当RST为1时，表明TCP连接中出现了严重的差错，必须释放连接，然后再重新建立连接。
　　**⑤ 同步字段SYN - 1bit**
　　　　当SYN=1时，表示发起一个连接请求。
　　**⑥ 终止字段FIN - 1bit**
　　　　用来释放连接。当FIN=1时，表明此报文段的发送端的数据已发送完成，并要求释放连接。
**窗口字段 - 16bit**
　　此字段用来控制对方发送的数据量，单位为字节。
　　一般TCP连接的其中一端会根据自身的缓存空间大小来确定自己的接收窗口大小，然后告知另一端以确定另一端的发送窗口大小。该字段与TCP的流量控制服务有关。
**校验和字段 - 16bit**
　　与IP协议的检验和不同，TCP的这个校验和是针对首部和数据两部分的。
**紧急指针字段 - 16bit**
　　紧急指针指出在本报文段中的紧急数据的最后一个字节的序号。

### 4.2.三次握手四次挥手(这词都听吐了,换个叫法:一键三连,取消三连加取关/滑稽))

还有一点其实人家是Three-way handshake,三步握手四步挥手的,谁握手握三次,挥手挥四次的,/吃瓜
**TIP:注意箭头的指向,结合文字进行理解,箭头没有问题**

![img](https://pic3.zhimg.com/80/v2-8d376a864a363e7c99f41d556270a706_720w.webp)

## 5. 一键三连

在三连之前服务器和客户端都为CLOSED状态.通信开始前,双方都得创建各自的传输控制块(TCB)
服务器创建完TCB后便进入LISTEN状态此时准备接受客户端发来的连接请求

### 5.1.点赞(第一次握手)

客户端向服务端发送连接请求报文段.该保温段的头部中SYN=1,ACK=0,seq=X.请求发送后客户端便进入SYN-SENT状态

- SYN = 1,ACK = 0表示该报文段位连接请求报文
- x为本次TCP通信的字节流的初始序号

**TIP:**TCP规定SYN = 1的报文段不能有数据部分,但要消耗一个序号

### 5.2.投币(第二次握手)

服务端收到连接请求报文段后,如果同意连接,则会发送一个应答:SYN = 1,ACK = 1,seq = y,ack = x + 1
该应答发送完成后便进入SYN-RCVD状态.

- SYN=1，ACK=1表示该报文段为连接同意的应答报文
- seq=y表示服务端作为发送者时，发送字节流的初始序号
- ack=x+1表示服务端希望下一个数据报发送序号从x+1开始的字节

### 5.3.收藏(第三次握手)

当客户端收到连接同意的应答后,还要向服务端发送一个确认报文段,表示服务端发来的连接同意应答已经收到
该报文头部为:ACK = 1, seq = x + 1, ack = y + 1.
客户端发完这个报文后便进入ESTABLSHED状态,服务端收到这个应答后也进入ESTABLISHED状态,此时连接的建立完成

## 6. 取消三连加取关

TCP连接的释放一共需要四部,因为TCP连接时双向的,因此在四次挥手中,前两次挥手用于断开一个方向的连接,后两次挥手用于断开另一方向连接

### 6.1.取消点赞(第一次挥手)

若客户端认为数据发送完成,则它需要向服务端发送连接释放请求.该请求只有报文头,头中携带的主要参数为:
FIN = 1,seq = u此时,客户端将进入FIN-WAIT-1状态

- FIN = 1表示该保温是一个连接释放请求
- seq = u,u - 1是A向B发送的最后一个字节的序号

### 6.2.取消投币(第二次挥手)

服务端收到连接释放请求后,会通知相应的应用程序,告诉它客户端向服务端这个方向的连接已经释放.此时服务端进入CLOSE-WAIT状态,并向客户端发送连接释放的回答,其报文头包含:
ACK = 1,seq = v, ack = u + 1

- ACK=1：除TCP连接请求报文段以外，TCP通信过程中所有数据报的ACK都为1，表示应答。
- seq=v，v-1是B向A发送的最后一个字节的序号。
- ack=u+1表示希望收到从第u+1个字节开始的报文段，并且已经成功接收了前u个字节

客户端收到应答进入FIN-WAIT-2状态,等待服务端发送连接释放请求
第二次挥手完成后,客户端到服务端方向的连接已经释放,服务端不会接受数据,客户端也不会发送数据,但服务端到客户端的连接仍然存在,服务端可以继续向客户端发送数据

### 6.3.取消收藏(第三次挥手)

当服务端发送完所有数据后,向客户端发送连接释放请求,请求头:FIN = 1,ACK, seq = w,ack = u+1.服务端便进入LAST-ACK状态.

### 6.4.取消关注(第四次挥手)

当客户端收到释放请求后向B发送确认应答,此时客户端进入TiME-WAIT状态,该状态会持续2MSL(2分钟)时间,若该时间没有服务端重发请求的话,就进入CLOSED状态,撤销TCB.当服务端收到确认应答后,也进入CLOSED状态撤销TCB

原文链接：https://zhuanlan.zhihu.com/p/378181872

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.288】从底层原理出发，了解Linux内核之内存管理

本文讲解更加底层，基本都是从Linux内核出发，会更深入。所以当你都读完，然后再次审视这些功能的实现和设计时，我相信你会有种豁然开朗的感觉。

## 1.页

**内核把物理页作为内存管理的基本单元**。

尽管处理器的最小处理单位是字（或者字节），但是**MMU**（内存管理单元，管理内存并把**虚拟地址转换为物理地址的硬件**）通常以页为单位进行处理。所以从虚拟内存看，页也是最小单元。

体系不同，支持的页大小不同。大多数**32位**体系结构支持**4KB**的页，而**64位**体系结构一般会支持**8KB**的页。

内核用struct page结构体**表示系统中的每个页**，包含很多项比如页的状态（有没有脏，有没有被锁定）、引用计数（-1表示没有使用）等等。

page结构和物理页相关，和虚拟内存无关。所以**它的描述是短暂的，仅仅记录当前的使用状况，当然也不会描述其中的数据**。

内核用这个结构来管理系统中所有的页，所以**内核知道哪些页是空闲的，如果在使用中拥有者又是谁**。

这个**拥有者有四种：用户空间进程、动态分配内存的内核数据、静态内核代码以及页高速缓存**。

## 2.区

有些页是有特定用途的。比如内存中有些页是专门用于DMA的。

内核使用区的概念将**具有相似特性的页进行分组**。**区是一种逻辑上的分组的概念**，而没有物理上的意义。

区的实际使用和分布是与体系结构相关的。在**x86体系结构中主要分为3个区：ZONE_DMA，ZONE_NORMAL，ZONE_HIGHMEM**。

ZONE_DMA区中的页用来进行DMA（直接内存访问）时使用。

ZONE_HIGHMEM是高端内存，**其中的页不能永久的映射到内核地址空间**，也就是说，没有虚拟地址。

剩余的内存就属于ZONE_NORMAL区，叫低端内存。

不是所有体系都定义全部区，有些体系结构，比如**x86-64可以映射和处理64位的内存空间**，所以它**没有ZONE_HIGHMEM区**，所有的物理内存都都处于ZONE_DMA和ZONE_NORMAL区。

每个区都用结构体struct zone表示。

## 3.接口

### 3.1.获得页

获得页使用的接口是**alloc_pages**函数与***_get\*free_page**函数。后者也是调用了前者，只不过在获得了struct page结构体后使用page_address函数获得了虚拟地址。

我们在使用这些接口获取页的时候可能会面对一个问题，我们获得的**这些页若是给用户态用**，虽然这些页中的数据都是随机产生的垃圾数据，不过，虽然概率很低，但是也有可能会包含某些敏感信息。所以，更谨慎些，我们可以**将获得的页都填充为0**。这会用到**get_zeroed_page**函数。而这个函数又用到了*_get*free_pages函数。

所以**这三个函数最终都是使用了alloc_pages函数**。

### 3.2.释放页

当我们不再需要某些页时可以使用下面的函数释放它们：
*_free*pages（struct page *page, unsigned int order）
free_pages（unsigned long addr, unsigned int order）
free_page（unsigned long addr）

以上这些接口都是**以页为单位**进行内存分配与释放的。

### 3.3.kmalloc与vmalloc

在实际中内核需要的内存不一定是整个页，可能只是**以字节为单位**的一片区域。这两个函数就是实现这样的目的。

不同之处在于，**kmalloc分配的是虚拟地址连续**，**物理地址也连续**的一片区域，**vmalloc分配的是虚拟地址连续，物理地址不一定连续**的一片区域。

对应的**释放**内存的函数是**kfree与vfree**。

## 4.slab层

以页为最小单位分配内存对于内核管理系统中的物理内存来说的确比较方便，但内核自身最常使用的内存却往往是很小的内存块——比如存放**文件描述符、进程描述符、虚拟内存区域描述符**等行为**所需的内存都远不及一页，一个整页中可以聚集多个**这些小块内存。

为了满足内核对这种小内存块的需要，Linux系统采用了一种被称为**slab分配器**（也称作**slab层**）的技术。slab分配器的实现相当复杂，但原理不难，其**核心思想就是“存储池”的运用**。**内存片段（小块内存）被看作对象，当被使用完后，并不直接释放而是被缓存到“存储池”里，留做下次使用，这无疑避免了频繁创建与销毁对象所带来的额外负载。**

slab分配器扮演了通用数据结构缓存层的角色。

slab层把不同的对象划分为所谓高速缓存组，其中每个高速缓存组都存放不同类型的对象，每种对象对应一个高速缓存。

**常见的高速缓存组有：进程描述符（task_struct结构体），索引节点对象（struct inode），目录项对象（struct dentry），通用页对象等等**。

这些高速缓存又被划分为slab。slab由一个或多个物理连续的页组成，一般仅仅由一页组成。**每个高速缓存可以由多个slab（页）组成**。

每个高速缓存都使用**struct kmem_cache**结构表示，这个结构包含三个链表：slabs_full、slabs_partial和slabs_empty，均放在kmem_list3结构体内。这些链表的每个元素为slab描述符即struct slab结构体。

每个高速缓存需要创建新的slab即新的页，还是通过上面提到的***_get\*free_page**()来实现的。通过最终调用**free_pages**()释放内存页。

一个高速缓存的创建和销毁使用**kmem_cache_create与kmem_cache_destroy**。

高速缓存中的对象的分配和释放使用**kmem_cache_alloc与kmem_cache_free**。

从上看出，slab层仍然是建立在页的基础之上，可以总结为**slab层将 空闲页 分解成 众多相同长度的小块内存 以供 同类型的数据结构 使用**。

## 5.进程地址空间

以上我们讲述了内核如何管理内存，内核内存分配机制包括了页分配器和slab分配器。内核除了管理本身的内存外，也必须管理用户空间中进程的内存。

我们称这个内存为进程地址空间，也就是系统中每个用户空间进程所看到的内存。Linux系统采用虚拟内存技术，所有进程以虚拟方式共享内存。Linux中主要采用分页机制而不是分段机制。

### 5.1.地址空间布局

![img](https://pic3.zhimg.com/80/v2-385c670ecf458f050c2c2469f50e889a_720w.webp)



进程内存区域可以包含各种内存对象，从下往上依次为：

（1）可执行文件**代码**的内存映射，称为代码段。只读可执行。

（2）可执行文件的**已初始化全局变量**的内存映射，称为数据段。后续都是可读写。

（3）包含**未初始化的全局变量**，就是bass段的零页的内存映射。

（4）**堆区**，动态内存分配区域；包括任何匿名的内存映射，比如malloc分配的内存。

（5）**栈区**，用于进程用户空间栈的零页内存映射，这里不要和进程内核栈混淆，进程的内核栈独立存在并由内核维护，因为内核管理着所有进程。所以内核管理着内核栈，内核栈管理着进程。

（6）**其他**可能存在的：**内存映射文件**；**共享内存段**；C库或者动态链接库等**共享库**的代码段、数据段和bss也会被载入进程的地址空间。

### 5.2. 内存描述符

内核使用**内存描述符mm_struct**结构体表示进程的地址空间，该结构体包含了**和进程地址空间有关的全部信息**。

```
1 struct mm_struct {
 2         struct vm_area_struct  *mmap;               /* list of memory areas */
 3         struct rb_root         mm_rb;               /* red-black tree of VMAs */
 4         struct vm_area_struct  *mmap_cache;         /* last used memory area */
 5         unsigned long          free_area_cache;     /* 1st address space hole */
 6         pgd_t                  *pgd;                /* page global directory */
 7         atomic_t               mm_users;            /* address space users */
 8         atomic_t               mm_count;            /* primary usage counter */
 9         int                    map_count;           /* number of memory areas */
10         struct rw_semaphore    mmap_sem;            /* memory area semaphore */
11         spinlock_t             page_table_lock;     /* page table lock */
12         struct list_head       mmlist;              /* list of all mm_structs */
13         unsigned long          start_code;          /* start address of code */
14         unsigned long          end_code;            /* final address of code */
15         unsigned long          start_data;          /* start address of data */
16         unsigned long          end_data;            /* final address of data */
17         unsigned long          start_brk;           /* start address of heap */
18         unsigned long          brk;                 /* final address of heap */
19         unsigned long          start_stack;         /* start address of stack */
20         unsigned long          arg_start;           /* start of arguments */
21         unsigned long          arg_end;             /* end of arguments */
22         unsigned long          env_start;           /* start of environment */
23         unsigned long          env_end;             /* end of environment */
24         unsigned long          rss;                 /* pages allocated */
25         unsigned long          total_vm;            /* total number of pages */
26         unsigned long          locked_vm;           /* number of locked pages */
27         unsigned long          def_flags;           /* default access flags */
28         unsigned long          cpu_vm_mask;         /* lazy TLB switch mask */
29         unsigned long          swap_address;        /* last scanned address */
30         unsigned               dumpable:1;          /* can this mm core dump? */
31         int                    used_hugetlb;        /* used hugetlb pages? */
32         mm_context_t           context;             /* arch-specific data */
33         int                    core_waiters;        /* thread core dump waiters */
34         struct completion      *core_startup_done;  /* core start completion */
35         struct completion      core_done;           /* core end completion */
36         rwlock_t               ioctx_list_lock;     /* AIO I/O list lock */
37         struct kioctx          *ioctx_list;         /* AIO I/O list */
38         struct kioctx          default_kioctx;      /* AIO default I/O context */
39 };
```

**mmap**和**mm_rb**描述的对象是一样的：该地址空间中全部内存区域（all **m**emory **a**reas）。

mmap是以**链表**的形式存放，而mm_rb是以**红黑树**存放，前者有利于**遍历**所有数据，而后者有利于**快速搜索定位**到某个地址。所有的mm_struct结构体都通过自身的**mmlist域**连接在一个双向链表中，该链表的**首元素**是**init_mm**内存描述符，它代表**init进程**的**地址空间**。

再往下看，可以看到地址空间几个区（堆栈）对应的变量的定义。

我们再回顾下在内核进程管理中，进程描述符task_struct是在内核空间中缓存，也就是我们上面描述的slab层。

而task_struct中有个mm域指向的就是该进程使用的内存描述符，再通过current->mm便可以指向当前进程的内存描述符。fork函数利用copy_mm()函数就实现了复制父进程的内存描述符，而子进程中的mm_struct结构体实际是通过文件kernel/fork.c中的allocate_mm()宏从**mm_cachep slab缓存**中**分配**得到的。通常，每个进程都有唯一的mm_struct结构体。

因为进程描述符和进程的内存描述符都是处于slab层，所以它们元素的分配和释放都由slab分配器来管理。

### 5.3. 虚拟内存区域

内存区域由**vm_area_struct**结构体描述，见上面的**mmap域**，内存区域在内核中也经常被称作**虚拟内存区域（Virtual Memory Area，VMA）**。

它描述了指定地址空间内连续区间上的一个独立内存范围。

内核将每个内存区域作为一个单独的内存对象管理，每个内存区域都拥有一致的属性。结构体如下：

```
1 struct vm_area_struct {
 2         struct mm_struct             *vm_mm;        /* associated mm_struct */
 3         unsigned long                vm_start;      /* VMA start, inclusive */
 4         unsigned long                vm_end;        /* VMA end , exclusive */
 5         struct vm_area_struct        *vm_next;      /* list of VMA's */
 6         pgprot_t                     vm_page_prot;  /* access permissions */
 7         unsigned long                vm_flags;      /* flags */
 8         struct rb_node               vm_rb;         /* VMA's node in the tree */
 9         union {         /* links to address_space->i_mmap or i_mmap_nonlinear */
10                 struct {
11                         struct list_head        list;
12                         void                    *parent;
13                         struct vm_area_struct   *head;
14                 } vm_set;
15                 struct prio_tree_node prio_tree_node;
16         } shared;
17         struct list_head             anon_vma_node;     /* anon_vma entry */
18         struct anon_vma              *anon_vma;         /* anonymous VMA object */
19         struct vm_operations_struct  *vm_ops;           /* associated ops */
20         unsigned long                vm_pgoff;          /* offset within file */
21         struct file                  *vm_file;          /* mapped file, if any */
22         void                         *vm_private_data;  /* private data */
23 };
```

每个内存描述符都对应于地址进程空间中的唯一区间。**vm_mm**域指向和VMA相关的mm_struct结构体。

一个内存区域的地址范围是**[vm_start, vm_end)，vm_next**指向该进程的下一个内存区域**。**

两个独立的进程将同一个文件映射到各自的地址空间，它们分别都会有一个vm_area_struct结构体来标志自己的内存区域；但是如果两个线程共享一个地址空间，那么它们也同时共享其中的所有vm_area_struct结构体。

在上面的**vm_flags域**中存放的是VMA标志，标志了**内存区域所包含的页面的行为和信息**。和物理页访问权限不同，VMA标志反映了内核处理页面所需要遵循的**行为准则**，而不是硬件要求。而且vm_flags同时包含了内存区域中每个页面的消息或者内存区域的整体信息，而不是具体的独立页面。如下表所述：

![img](https://pic4.zhimg.com/80/v2-37fdf4acc93ac82a2747655e163b0183_720w.webp)

开头三个标志表示代码在该内存区域的可读、可写和可执行权限。

第四个标志VM_SHARD说明了该区域包含的映射是否可以在多进程间共享，如果被设置了，表示共享映射；否则未被设置，表示私有映射。

其中很多状态在实际使用中都非常有用。

### 5.4. mmap()和do_mmap()：创建地址空间

内核使用do_mmap()函数创建一个新的线性地址空间。但如果创建的地址区间和一个已经存在的地址区间**相邻**，并且它们具有相同的访问权限的话，那么两个区间将**合并**为一个。如果不能合并，那么就确实需要创建一个**新的vma**了，但无论哪种情况，do_mmap()函数都会将一个地址区间加入到进程的地址空间中。这个函数定义在linux/mm.h中，如下

```
unsigned long do_mmap(struct file *file, unsigned long addr, unsigned long len, unsigned long prot,unsigned long flag, unsigned long offset)
```

这个函数中由**file指定文件**，具体映射的是文件中从偏移offset处开始，长度为len字节的范围内的数据，如果**file参数是NULL并且offset参数也是0**，那么就代表这次映射没有和文件相关，该情况被称作**匿名映射**(file-backed mapping)。如果**指定了文件和偏移量**，那么该映射被称为**文件映射**(file-backed mapping)。

其中参数prot指定内存区域中页面的访问权限：可读、可写、可执行。

flag参数指定了VMA标志，这些标志指定类型并改变映射的行为，请见上一小节。

如果系统调用do_mmap的参数中有无效参数，那么它返回一个负值；否则，它会在虚拟内存中分配一个合适的新内存区域，如果有可能的话，将新区域和临近区域进行**合并**，否则内核从**vm_area_cachep**长字节**(slab)缓存**中分配一个vm_area_struct结构体，并且使用vma_link()函数将**新分配的内存区域添加到**地址空间的内存区域**链表和红黑树**中，随后还要更新内存描述符中的total_vm域，然后才返回新分配的地址区间的初始地址。

在**用户空间**，我们可以通过**mmap()系统调用获取**内核函数**do_mmap()**的功能。

### 5.5. munmap()和do_munmap()：删除地址空间

do_mummp()函数**从特定的进程地址空间中删除指定地址空间**，该函数定义在文件linux/mm.h中，如下：

```
int do_munmap(struct mm_struct *mm, unsigned long start, size_t len)
```

第一个参数指定要删除区域所在的地址空间，删除从地址start开始，长度为len字节的地址空间，如果成功，返回0，否则返回负的错误码。

与之相对应的**用户空间系统调用是munmap**，它是对do_mummp()函数的一个简单封装。

### 5.6. malloc()的实现

我们知道malloc()是C库中实现的。C库对内存分配的管理还有calloc()、realloc()、free()等函数。

事实上，**malloc函数是以brk()或者mmap()系统调用实现的**。

brk和sbrk主要的工作是实现虚拟内存到内存的映射。在Linux系统上，程序被载入内存时，内核为用户进程地址空间建立了代码段、数据段和堆栈段，在数据段与堆栈段之间的空闲区域用于动态内存分配。我们回到内存结构mm_struct中的成员变量start_code和end_code是进程代码段的起始和终止地址，start_data和 end_data是进程数据段的起始和终止地址，start_stack是进程堆栈段起始地址，start_brk是进程动态内存分配起始地址（堆的起始地址），还有一个 **brk**（**堆的当前最后地址**），**就是动态内存分配当前的终止地址**。所以C库的malloc()在Linux上的基本实现是通过内核的brk系统调用。brk()是一个非常简单的系统调用，内核再执行sys_brk()函数进行内存分配*，*只是简单地改变mm_struct结构的成员变量brk的值。而sbrk不是系统调用，是C库函数。系统调用通常提供一种最小功能，而库函数通常提供比较复杂的功能。

下面我们整理一下在进程空间堆中用brk()方式进行动态内存分配的**流程**：

　　　C库函数malloc()调用Linux系统调用函数brk()，brk()执行系统调用陷入到内核，内核执行sys_brk()函数，sys_brk()函数调用do_brk()进行内存分配

malloc()———->brk()—–|—–>sys_brk()———–>do_brk()————>vma_merge()/kmem_cache_zalloc()

用户空间——> | 内核空间

　　　　　　　　　　　 系统调用———->

mmap()系统调用也可以实现动态内存分配功能，即5.4节我们提到的**匿名映射**。

那什么时候调用brk()，什么时候调用mmap()呢？通过阈值**M_MMAP_THRESHOLD**来决定。该值**默认128KB**。可以通过mallopt()来进行修改设置。

所以**当需要分配的内存大于该阈值，选择mmap()；否则小于等于该阈值，选择brk()分配。**

最后，mmap分配的内存在调用munmap后会立即返回给系统，而brk/sbrk而受**M_TRIM_THRESHOLD**的影响。该环境变量同样通过mallopt()来设置，该值代表的意义是释放内存的最少字节数。

但brk/sbrk分配的内存是否立即归还给系统，不仅受M_TRIM_THRESHOLD的影响，还要看高地址端（**brk处**）的内存是否已经释放：

假如依次malloc了str1、str2、str3（str3在最上端，结束地址为**brk**），即使它们都是brk/sbrk分配的，如果没有释放str3，只释放了str1和str2，就算两者加起来超过了M_TRIM_THRESHOLD，因为str3的存在，str1和str2也不能立即归还可以系统，即**这些内存都被str3给“拴住”**了。

此时，str1和str2的内存只是简单的标记为“未使用”，如果这两处内存是相邻的则会进行合并，这种算法也称为“伙伴内存算法(buddy memory allocation scheme)”。这种算法高速简单，但同时也会生成碎片。包括**内碎片**（一次分配内存不够整页，最后一页剩下的空间）和**外碎片**（多次或者反复分配造成中间的空闲页太小不够后续的一次分配）。

从上可以看出，在一定条件下，假如释放了str3的内存，堆的大小是可以紧缩的。

最后我们以一张图结束今天的主题，内存分配流程图：

![img](https://pic3.zhimg.com/80/v2-b97519314cbd73699faf89f1e879f376_720w.webp)

原文链接：https://zhuanlan.zhihu.com/p/378935966

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.289】图解 epoll 是如何工作的及epoll实现原理

## 1.本文包含以下内容：

- epoll是如何工作的

## 2.本文不包含以下内容：

- epoll 的用法
- epoll 的缺陷

> 我实在非常喜欢像epoll这样使用方便、原理不深却有大用处的东西，即使它可能已经比较老了

## 3.select 和 poll 的缺点

epoll 对于动辄需要处理上万连接的网络服务应用的意义可以说是革命性的。对于普通的本地应用，select 和 poll可能就很好用了，但对于像C10K这类高并发的网络场景，select 和 poll就捉襟见肘了。

看看他们的**API**

```
int select(int nfds, fd_set *readfds, fd_set *writefds,
           fd_set *exceptfds, struct timeval *timeout);
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```

它们有一个共同点，用户需要将**监控**的文件描述符集合打包当做参数传入，每次调用时，这个集合都会从用户空间**拷贝**到内核空间，这么做的原因是内核对这个集合是无记忆的。对于绝大部分应用，这是一种十足的浪费，因为应用需要监控的描述符在大部分时间内基本都是不变的, 也许会有变化, 但都不大.

## 4.epoll 对此的改进

epoll对此的改进也正是它的实现方式, 它需要完成以下两件事

1. 描述符添加 — 内核可以记下用户关心哪些文件的哪些事件.
2. 事件发生 — 内核可以记下哪些文件的哪些事件真正发生了, 当用户前来获取时, 能把结果提供给用户.

## 5.描述符添加

既然要有记忆, 那么理所当然的内核需要需要一个数据结构来记, 这个数据结构简单点就像下面这个图中的epoll_instance, 它有一个链表头，链表上的元素epoll_item就是用户添加上去的， 每一项都记录了描述符fd和感兴趣的事件组合event

![img](https://pic1.zhimg.com/80/v2-43706d2e07d4856895d5894e63456258_720w.webp)

## 6.事件发生

事件有多种类型, 其中POLLIN表示的**可读**事件是用户使用的最多的。比如:

- 当一个 TCP 的socket收到报文，它会变得可读；
- 当一个pipe受到对端发送的数据，它会变得可读;
- 当一个timerfd对应的定时器超时，它会变得可读;

那么现在需要将这些**可读**事件和前面的epoll_instance关联起来。linux中，每一个文件描述符在内核都有一个struct file结构对应, 这个struct file有一个private_data指针，根据文件的实际类型，它们指向不同的数据结构。

![img](https://pic4.zhimg.com/80/v2-946574349952527a99d7ed1153b4268f_720w.webp)

那么我能想到的最方便的做法就是epoll_item中增加一个指向struct file的指针，在struct file中增加一个指回epoll item的指针。

![img](https://pic2.zhimg.com/80/v2-02c7a1ed9a784063d6beb55d24608e25_720w.webp)

为了能记录有事件发生的文件，我们还需要在epoll_instance中增加一个就绪链表readylist，在private_data指针指向的各种数据结构中增加一个指针回指到 struct file，在epoll item中增加一个挂接点字段，当一个文件可读时，就把它对应的epoll item挂接到epoll_instance

![img](https://pic1.zhimg.com/80/v2-1e2730dffcd14bec029c7b6621f9a380_720w.webp)

在这之后，用户通过系统调用下来读取readylist就可以知道哪些文件就绪了。

好了，以上纯属我个人一拍脑袋想到的epoll大概的工作方式，其中一定包含不少缺陷。

不过真实的epoll的实现思想上与上面也差不多，下面来说一下

## 7.创建 epoll 实例

如同上面的epoll_instance，内核需要一个数据结构保存记录用户的注册项，这个结构在内核中就是struct eventpoll， 当用户使用epoll_create(2)或者epoll_create1(2)时，内核fs/eventpoll.c实际就会创建一个这样的结构.

```
error = ep_alloc(&ep);
```

这个结构中比较重要的部分就是几个链表了，不过实例刚创建时它们都是空的，后续可以看到它们的作用

epoll_create()最终会向用户返回一个文件描述符，用来方便用户之后操作该 **epoll 实例**，所以在创建**epoll 实例**之后，内核就会分配一个文件描述符fd和对应的struct file结构

```
fd = get_unused_fd_flags(O_RDWR | (flags & O_CLOEXEC));
file = anon_inode_getfile("[eventpoll]", &eventpoll_fops, ep,
                           O_RDWR | (flags & O_CLOEXEC));
```

最后就是把它们和刚才的**epoll 实例** 关联起来，然后向用户返回fd

```
ep->file = file;
fd_install(fd, file);
return fd;
```

完成后，**epoll 实例** 就成这样了。

![img](https://pic1.zhimg.com/80/v2-b98f65fbaa09a2e54887b413e18ace7c_720w.webp)

## 8.向 epoll 实例添加一个文件描述符

用户可以通过 epoll_ctl(2)向 **epoll 实例** 添加要监控的描述符和感兴趣的事件。如同前面的epoll item，内核实际创建的是一个叫struct epitem的结构作为注册表项。如下图所示

![img](https://pic4.zhimg.com/80/v2-e0c347b88a04db59154ad9e86c66223f_720w.webp)

为了在描述符很多时的也能有较高的搜索效率, **epoll 实例** 以红黑树的形式来组织每个struct epitem (取代上面例子中链表)。struct epitem结构中ffd是用来记录关联文件的字段, 同时它也作为该表项添加到红黑树上的**Key**；

rdllink的作用是当fd对应的文件准备好 (关心的事件发生) 时，内核会将它作为挂载点挂接到**epoll 实例**中ep->rdllist链表上
fllink的作用是作为挂载点挂接到fd对应的文件的file->f_tfile_llink链表上，一般这个链表最多只有一个元素，除非发生了dup。
pwqlist是一个链表头，用来连接 poll wait queue。虽然它是链表，但其实链表上最多只会再挂接一个元素。

创建struct epitem的代码在fs/evnetpoll.c的ep_insert()中

```
if (!(epi = kmem_cache_alloc(epi_cache, GFP_KERNEL)))
    return -ENOMEM;
```

之后会进行各个字段初始化

```
INIT_LIST_HEAD(&epi->rdllink);
INIT_LIST_HEAD(&epi->fllink);
INIT_LIST_HEAD(&epi->pwqlist);
epi->ep = ep;
ep_set_ffd(&epi->ffd, tfile, fd);
epi->event = *event;
epi->nwait = 0;
epi->next = EP_UNACTIVE_PTR;
```

然后是设置局部变量epq

```
struct ep_pqueue epq;
epq.epi = epi;
init_poll_funcptr(&epq.pt, ep_ptable_queue_proc);
```

epq的数据结构是struct ep_pqueue, 它是poll table的一层包装 (加了一个struct epitem* 的指针)

```
struct  ep_pqueue{
    poll_table pt;
    struct epitem* epi;
}
```

poll table包含一个函数和一个事件掩码

```
typedef void (*poll_queue_proc)(struct file *, wait_queue_head_t *, struct poll_table_struct *);
typedef struct poll_table_struct {
    poll_queue_proc _qproc;
    unsigned long _key;   
}poll_table;
```

这个poll table用在哪里呢 ? 答案是, 用在了struct file_operations的poll操作 (这和本文开始说的select`poll`不是一个东西)

![img](https://pic3.zhimg.com/80/v2-bf3b1e32263171fa24f306cc5e427556_720w.webp)

```
struct file_operations { 
   unsigned int (*poll)(struct file*,  struct poll_table_struct*);
}
```

不同的文件有不同poll实现方式, 但一般它们的实现方式差不多是下面这种形式

```
static unsigned int XXXX_poll(struct file *file, poll_table *wait)
{
    私有数据 = file->private_data;
    unsigned int events = 0;
    poll_wait(file, &私有数据->wqh, wait);
    if (文件可读了)
        events |= POLLIN;
    return events;
}
```

它们主要实现两个功能

1. 将**XXX**放到文件私有数据的等待队列上 (一般file->private_data中都有一个等待队列头wait_queue_head_t wqh), 至于**XXX**是啥, 各种类型文件实现各异, 取决于poll_table参数
2. 查询是否真的有事件了, 若有则返回.

> 有兴趣的读者可以 timerfd_poll() 或者 pipe_poll() 它们的实现

poll_wait的实现很简单, 就是调用poll_table中设置的函数, 将文件私有的等待队列当作了参数.

```
static inline void poll_wait(struct file * filp, wait_queue_head_t * wait_address, poll_table *p)
{
    if (p && p->_qproc && wait_address)
        p->_qproc(filp, wait_address, p);
}
```

回到 ep_insert()

所以这里设置的poll_table就是ep_ptable_queue_proc().

然后

```
revents = ep_item_poll(epi, &epq.pt)
```

看其实现可以看到, 其实就是主动去调用文件的poll函数. 这里以 TCP socket文件为例好了 (毕竟网络应用是最广泛的)

```
unsigned int tcp_poll(struct file *file, struct socket *sock, poll_table *wait)  
{
    sock_poll_wait(file, sk_sleep(sk), wait);  
}
```

可以看到, 最终还是调用到了poll_wait(), 所以注册的ep_ptable_queue_proc()会执行

```
struct epitem *epi = ep_item_from_epqueue(pt);
    struct eppoll_entry *pwq; 
    pwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL)
```

这里面, 又分配了一个struct eppoll_entry结构. 其实它和struct epitem 结构是一一对应的.

随后就是一些初始化

```
init_waitqueue_func_entry(&pwq->wait, ep_poll_callback); 
    pwq->whead = whead;  
    pwq->base = epi;
    add_wait_queue(whead, &pwq->wait) 
    list_add_tail(&pwq->llink, &epi->pwqlist);  
    epi->nwait++;
```

这其中比较重要的是设置pwd->wait.func = ep_poll_callback。

现在, struct epitem 和struct eppoll_entry的关系就像下面这样

![img](https://pic3.zhimg.com/80/v2-b960812c88258eef4d6fe19310f890d2_720w.webp)

## 9.文件可读之后

对于 TCP socket, 当收到对端报文后, 最初设置的sk->sk_data_ready函数将被调用

```
void sock_init_data(struct socket *sock, struct sock *sk)
{
     sk->sk_data_ready  =   sock_def_readable;
}
```

经过层层调用, 最终会调用到 *_wake*up_common 这里面会遍历挂在socket.wq上的等待队列上的函数

```
static void __wake_up_common(wait_queue_head_t *q, unsigned int mode,
            int nr_exclusive, int wake_flags, void *key)
{
    wait_queue_t *curr, *next;
    list_for_each_entry_safe(curr, next, &q->task_list, task_list) {
        unsigned flags = curr->flags;
        if (curr->func(curr, mode, wake_flags, key) &&
                (flags & WQ_FLAG_EXCLUSIVE) && !--nr_exclusive)
            break;
    }
}
```

于是, 顺着图中的这条红色轨迹, 就会调用到我们设置的ep_poll_callback, 那么接下来就是要让epoll实例能够知有文件已经可读了

![img](https://pic3.zhimg.com/80/v2-61c08c19fa08c288eb94aaddf416670e_720w.webp)

先从入参中取出当前表项epi和ep

```
struct epitem *epi = ep_item_from_wait(wait);
    struct eventpoll *ep = epi->ep;
```

再把epi挂到ep的就绪队列

```
if (!ep_is_linked(&epi->rdllink)) {
        list_add_tail(&epi->rdllink, &ep->rdllist)
    }
```

接着唤醒阻塞在 (如果有) 该epoll实例的用户.

```
waitqueue_active(&ep->wq)
```

## 10.用户获取事件

谁有可能阻塞在epoll实例的等待队列上呢? 当然就是使用epoll_wait来从epoll实例获取发生了感兴趣事件的的描述符的用户.
epoll_wait会调用到ep_poll()函数.

```
if (!ep_events_available(ep)) {
        init_waitqueue_entry(&wait, current);
        __add_wait_queue_exclusive(&ep->wq, &wait);
```

如果没有事件, 我们就将自己挂在epoll实例的等待队列上然后睡去…..
如果有事件, 那么我们就要将事件返回给用户

```
ep_send_events(ep, events, maxevents)
```

原文链接：https://zhuanlan.zhihu.com/p/379923948

作者：Hu先生的Linux

# 【NO.290】现在后端开发都在用什么数据库存储数据？

## 1.**Oracle：**

传统行业，尤其是政府，医疗，学校和大企业，基本上还是Oracle应用最广，其次就是DB2。反而是WebLogic和WebSphere这些中间件基本上随着经典javaee的没落，已经逐步退出历史舞台，被富前端和微服务框架的轻量级组合所替代。

![img](https://pic2.zhimg.com/80/v2-c28ba0a898250f1d38646e6b112d9f5d_720w.webp)

## **2.MySQL：**

传统行业的很多新项目也大量开始应用MySQL，因为轻量级数据库的前期成本很低，可以保证项目预算够用，所以主要是新项目居多，面向互联网连接的项目也居多。这些系统一般不会像Oracle一样承担关键性业务的数据存储，所以选择什么样的数据库都是开发公司自己的选择决定。

目前有大量企业都开始上云，大家买云服务以阿里云ecs为主，总体上阿里云还是比较稳定，那么对于云上数据库的稳定有要求的企业一般都会选择阿里云主打的的rds系列，MySQL居多，PostgreSQL也开始逐渐被认可。

![img](https://pic4.zhimg.com/80/v2-25d949cfc42546361c4b898b3d611bff_720w.webp)

## **3.PostgreSQL：**

说到PostgreSQL，的确这两年PG风头正劲，以前我的文章也提到过我做过的互联网医疗产品，其架构设计就选择采用了PostgreSQL，主要就是看中PostgreSQL在生产上的稳定性极高，而且成本很低。尤其是精通Linux服务的架构师，对于PostgreSQL更容易掌握。

更具体地说就是使用PostgreSQL的关键因素主要还是业务数据很关键，因为我们当时承载的是互联网医疗数据，医疗数据自身属性就很关键！所以稳定和安全都是刚性要求，同时要平衡成本与互联网方式的灵活性，所以才否定了MySQL方案，坚决执行了PostgreSQL方案。

![img](https://pic3.zhimg.com/80/v2-caf72f6736537eeb5dbaeaec90df03fe_720w.webp)

## **4.Hadoop HDFS：**

大数据类项目的主数据集还是以Hadoop HDFS作为基础存储设施。尽管现在很热的讨论就是Hadoop已经是日落黄昏，可以选择其他更快的NoSQL存储方案。实际上，大数据工程师在最终落地的执行上，还是很诚实的选择了Hadoop，因为其成熟度，稳定性是最终考量的标准。

![img](https://pic1.zhimg.com/80/v2-59fec4be130d5d2b463b7371dbe6f10c_720w.webp)

## **5.Elasticsearch：**

ELK家族的Elasticsearch目前被大量作为日志监测分析的主数据集去使用，甚至都忽视了它本身是搜索引擎的这个事实，在电子商务网站，内容发布网站以及社交媒体网站，Elasticsearch作为专业搜索引擎，还是稳坐第一把交椅。

![img](https://pic4.zhimg.com/80/v2-bddd1101bb7a2cd70c59e652486f452f_720w.webp)

## **6.实时/时序数据库：**

工业能源以及其他物联网行业，实时、时序数据库正在逐步采用开源的解决方案，例如[http://Druid.io](https://link.zhihu.com/?target=http%3A//Druid.io)、InfluxDB，OpenTSDB，还是目前存储物联网数据最好的开源选择方案。[http://Druid.io](https://link.zhihu.com/?target=http%3A//Druid.io)是实时与历史一整套实时库解决方案；InfluxDB目前热度非常高的时序数据库，自己独立实现了一套原生的集群存储结构；OpenTSDB主要依赖HBase分布式数据库与HDFS分布式文件系统。另外提一句，清华推出的开源时序数据库IOTDB，目前已经升级成[http://Apache.org](https://link.zhihu.com/?target=http%3A//Apache.org)的顶级项目。

![img](https://pic1.zhimg.com/80/v2-c591107988eaa72599469bed52ad0af8_720w.webp)

## **7. Hadoop HBase：**

Hadoop hbase作为列簇存储，也是毫秒级的k-v存储，越来越适应通用场景下的实时数据分析了，可能哪个领域都有能用到它，支撑实时处理的联机分析以及小型批处理业务。它的分布式一致性，存储hdfs的稳定性，都是关键性业务数据进行实时分析的极佳方案。

![img](https://pic1.zhimg.com/80/v2-ce2ee6774d8eade80a54e04f52641190_720w.webp)

## **8.关系数据库并行能力：**

关系数据库也是在不断改进中前进，尤其是轻量级数据库的改进，MySQL8的cluster特性，PostgreSQL11的并行特性，都是不同手段想要达到同一个目的：那就是关系库都在想尽一切办法，不必让用户脱离关系型数据库，非得拥抱NoSQL才能追求到海量数据的并行处理能力，同时也能降低用户替换导致的巨大升级成本。

![img](https://pic4.zhimg.com/80/v2-86f5978dc8a84e46656b92f16d5f13cf_720w.webp)

## **9.MongoDB：**

另一种是关系数据库自身的改进或者引入MongoDB进行部分替代，例如电子商务的订单业务数据，互联网医疗的健康档案数据，内容发布的文章数据，都能实现MongoDB的文档化替代，这不仅更符合业务的文档化模型，而且能保证事务的前提下，实现海量数据的支撑。

![img](https://pic4.zhimg.com/80/v2-0c9d43c81aecd876d61a0f0b72ec17f3_720w.webp)

## **10.关系数据库并行能力：**

关系数据库也是在不断改进中前进，尤其是轻量级数据库的改进，MySQL8的cluster特性，PostgreSQL11的并行特性，都是不同手段想要达到同一个目的：那就是关系库都在想尽一切办法，不必让用户脱离关系型数据库，非得拥抱NoSQL才能追求到海量数据的并行处理能力，同时也能降低用户替换导致的巨大升级成本。

![img](https://pic4.zhimg.com/80/v2-41248d3f4ec4e12450acc5bf7c90ceef_720w.webp)

非得拥抱NoSQL才能追求到海量数据的并行处理能力，同时也能降低用户替换导致的巨大升级成本。

![img](https://pic4.zhimg.com/80/v2-41248d3f4ec4e12450acc5bf7c90ceef_720w.webp)

原文链接：https://zhuanlan.zhihu.com/p/382160856

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)



# 【NO.291】Redis多线程原理详解

本篇文章为你解答以下问题：

- **0：redis单线程的实现流程是怎样的？**
- **1：redis哪些地方用到了多线程，哪些地方是单线程？**
- **2：redis多线程是怎么实现的？**
- **3：redis多线程是怎么做到无锁的？**

## **1.redis单线程的实现流程是怎样的？**

Redis一开始是单线程模型，在一个线程中要同时处理两种事件：文件事件和时间事件

文件事件主要是网络I/O的读写，请求的接收和回复

时间事件就是单次/多次执行的定时器，如主从复制、定时删除过期数据、字典rehash等

redis所有核心功能都是跑在主线程中的，像aof文件落盘操作是在子线程中执行的，那么在高并发情况下它是怎么做到高性能的呢？

由于这两种事件在同一个线程中执行，就会出现互相影响的问题，如时间事件到了还在等待/执行文件事件，或者文件事件已经就绪却在执行时间事件，这就是单线程的缺点，所以在实现上要将这些影响降到最低。那么redis是怎么实现的呢？

定时执行的时间事件保存在一个链表中，由于链表中任务没有按照执行时间排序，所以每次需要扫描单链表，找到最近需要执行的任务，时间复杂度是O(N)，redis敢这么实现就是因为这个链表很短，大部分定时任务都是在serverCron方法中被调用。从现在开始到最近需要执行的任务的开始时间，时长定位T，这段时间就是属于文件事件的处理时间，以epoll为例，执行epoll_wait最多等待的时长为T，如果有就绪任务epoll会返回所有就绪的网络任务，存在一个数组中，这时我们知道了所有就绪的socket和对应的事件（读、写、错误、挂断），然后就可以接收数据，解析，执行对应的命令函数。

如果最近要执行的定时任务时间已经过了，那么epoll就不会阻塞，直接返回已经就绪的网络事件，即不等待。

总之单线程，定时事件和网络事件还是会互相影响的，正在处理定时事件网络任务来了，正在处理网络事件定时任务的时间到了。所以redis必须保证每个任务的处理时间不能太长。

redis处理流程如下：

1：服务启动，开始网络端口监听，等待客户端请求

2：客户端想服务端发起连接请求，创建客户端连接对象，完成连接

3：将socket信息注册到epoll，设置超时时间为时间事件的周期时长，等待客户端发起请求

4：客户端发起操作数据库请求(如GET)

5：epoll收到客户端的请求，可能多个，按照顺序处理请求

6：接收请求参数，接收完成后解析请求协议，得到请求命令

7：执行请求命令，即操作redis数据库

8：将结果返回给客户端

## **2.redis哪些地方用到了多线程，哪些地方是单线程？**

Redis多线程和单线程模型对比如下图：

![img](https://pic2.zhimg.com/80/v2-7c6a1daa50f26845374d16f3d40747d1_720w.webp)

从上图中可以看出只有以下3个地方用的是多线程，其他地方都是单线程：

1：接收请求参数

2：解析请求参数

3：请求响应，即将结果返回给client

很明显以上3点各个请求都是互相独立互不影响的，很适合用多线程，特别是请求体/响应体很大的时候，更能体现多线程的威力。而操作数据库是请求之间共享的，如果使用多线程的话适合读写锁。而操作数据库本身是很快的（就是对map的增删改查），单线程不一定就比多线程慢，当然也有可能是作者偷懒，懒得实现罢了，但这次的多线程模型还是值得我们学习一下的。

## **3.redis多线程是怎么实现的？**

先大致说一下多线程的流程：

1：服务器启动时启动一定数量线程，服务启动的时候可以指定线程数，每个线程对应一个队列（list *io_threads_list[128]），最多128个线程。

2：服务器收到的每个请求都会放入全局读队列clients_pending_read，同时将队列中的元素分发到每个线程对应的队列io_threads_list中，这些工作都是在主线程中执行的。

3：每个线程（包括主线程和子线程）接收请求参数并做解析，完事后在client中设置一个标记CLIENT_PENDING_READ，标识参数解析完成，可以操作数据库了。（主线程和子线程都会执行这个步骤）

4：主线程遍历队列clients_pending_read，发现设有CLIENT_PENDING_READ标记的，就操作数据库

5：操作完数据库就是响应client了，响应是一组函数addReplyXXX，在client中设置标记CLIENT_PENDING_WRITE，同时将client加入全局写队列clients_pending_write

6：主线程将全局队列clients_pending_write以轮训的方式将任务分发到每个线程对应的队列io_threads_list

7：所有线程将遍历自己的队列io_threads_list，将结果发送给client

## **4.redis多线程是怎么做到无锁的？**

上面说了多线程的地方都是互相独立互不影响的。但是每个线程的队列就存在两个两个线程访问的情况：主线程向队列中写数据，子线程消费，redis的实现有点反直觉。按正常思路来说，主线程在往队列中写数据的时候加锁；子线程复制队列&并将队列清空，这个两个动作是加锁的，子线程消费复制后的队列，这个过程是不需要加锁的，按理来说主线程和子线程的加锁动作都是非常快的。但是redis并没有这么实现，那么他是怎么实现的呢？

redis多线程的模型是主线程负责搜集任务，放入全局读队列clients_pending_read和全局写队列clients_pending_write，主线程在将队列中的任务以轮训的方式分发到每个线程对应的队列（list *io_threads_list[128]）

1：一开始子线程的队列都是空，主线程将全对队列中的任务分发到每个线程的队列，并设置一个队列有数据的标记（*Atomic unsigned long io*threads_pending[128]），io_threads_pending[1]=5表示第一个线程的队列中有5个元素

2：子线程死循环轮训检查io_threads_pending[index] > 0，有数据就开始处理，处理完成之后将io_threads_pending[index] = 0，没数据继续检查

3：主线程将任务分发到子线程的队列中，自己处理自己队列中的任务，处理完成后，等待所有子线程处理完所有任务，继续收集任务到全局队列，在将任务分发给子线程，这样就避免了主线程和子线程同时访问队列的情况，主线程向队列写的时候子线程还没开始消费，子线程在消费的时候主线程在等待子线程消费完，子线程消费完后主线程才会往队列中继续写，就不用加锁了。因为任务是平均分配到每个队列的，所以每个队列的处理时间是接近的，等待的时间会很短。

## **5.源码执行流程**

为了方便你看源码，这里加上一些代码的执行流程

启动socket监听，注册连接处理函数，连接成功后创建连接对象connection，创建client对象，通过aeCreateFileEvent注册client的读事件

```
main -> initServer -> acceptTcpHandler -> anetTcpAccept -> anetGenericAccept -> accept(获取到socket连接句柄)
connCreateAcceptedSocket -> connCreateSocket -> 创建一个connection对象
acceptCommonHandler -> createClient创建client连接对象 -> connSetReadHandler -> aeCreateFileEvent -> readQueryFromClient
main -> aeMain -> aeProcessEvents -> aeApiPoll(获取可读写的socket) -> readQueryFromClient(如果可读) -> processInputBuffer -> processCommandAndResetClient(多线程下这个方法在当前流程下不会执行，而由主线程执行)
```

在多线程模式下，readQueryFromClient会将client信息加入server.clients_pending_read队列，listAddNodeHead(server.clients_pending_read,c);

主线程会将server.clients_pending_read中的数据分发到子线程的队列(io_threads_list)中，子线程会调用readQueryFromClient就行参数解析，主线程分发完任务后，会执行具体的操作数据库的命令，这块是单线程

如果参数解析完成会在client->flags中加一个标记CLIENT_PENDING_COMMAND，在主线程中先判断client->flags & CLIENT_PENDING_COMMAND > 0，说明参数解析完成，才会调用processCommandAndResetClient，之前还担心如果子线程还在做参数解析，主线程就开始执行命令难道不会有问题吗？现在一切都清楚了

```
main -> aeMain -> aeProcessEvents -> beforeSleep -> handleClientsWithPendingReadsUsingThreads -> processCommandAndResetClient -> processCommand -> call
```

读是多次读：socket读缓冲区有数据，epoll就会一直触发读事件，所以读可能是多次的

写是一次写：往socket写数据是在子线程中执行的，直接循环直到数据写完位置，就算某个线程阻塞了，也不会像单线程那样导致所有任务都阻塞

```
执行完相关命令后，就是将结果返回给client，回复client是一组函数，我们以addReply为例，说一下执行流程，执行addReply还是单线程的，将client信息插入全局队列server.clients_pending_write。
addReply -> prepareClientToWrite -> clientInstallWriteHandler -> listAddNodeHead(server.clients_pending_write,c)
在主线程中将server.clients_pending_write中的数据以轮训的方式分发到多个子线程中
beforeSleep -> handleClientsWithPendingWritesUsingThreads -> 将server.clients_pending_write中的数据以轮训的方式分发到多个线程的队列中io_threads_list
list *io_threads_list[IO_THREADS_MAX_NUM];是数组双向链表，一个线程对应其中一个队列
子线程将client中的数据发给客户端，所以是多线程
server.c -> main -> initThreadedIO(启动一定数量的线程) -> IOThreadMain(线程执行的方法) -> writeToClient -> connWrite -> connSocketWrite
```

网络操作对应的一些方法，所有connection对象的type字段都是指向CT_Socket

```
ConnectionType CT_Socket = {
    .ae_handler = connSocketEventHandler,
    .close = connSocketClose,
    .write = connSocketWrite,
    .read = connSocketRead,
    .accept = connSocketAccept,
    .connect = connSocketConnect,
    .set_write_handler = connSocketSetWriteHandler,
    .set_read_handler = connSocketSetReadHandler,
    .get_last_error = connSocketGetLastError,
    .blocking_connect = connSocketBlockingConnect,
    .sync_write = connSocketSyncWrite,
    .sync_read = connSocketSyncRead,
    .sync_readline = connSocketSyncReadLine
};
```

原文链接：https://zhuanlan.zhihu.com/p/382713883

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.292】工作年限、成长路线、进阶技术。怎样才能成为架构师？

## 1.前言

你只面向工作学习吗？

如果说编程只是单纯的承接产品需求开发系统功能，那么基本可以把程序开发简单理解成按照需求PRD，定义属性、创建方法、调用展示，这三个步骤。

尤其是在一些大公司中，会有易用的、完善的、标准的架构体系和运维服务，例如：RPC、MQ、Redis集群、分布式任务、配置中心、分库分表组件、网关等搭配出来的系统架构。也因此让程序员做到只关心业务功能开发！

让程序员只关心业务开发，有成熟的系统架构、有标准的开发流程、有通用的功能设计，对于团队效能提升来说是非常好的事。但一部分程序员正因为有这样的好事，让日复一日的岁月做着同样的事，最后成为工具人。

*如果是框架和中间件的存在，是了让程序员只关心业务开发。那为什么你面试的时候会被问到核心组件的设计和原理呢？* 在这个年代，别放弃学习是你几乎唯一的生存途径。

## 2.你的成长阶段目标？

![img](https://pic4.zhimg.com/80/v2-fe2d27b18865ec50f02fd950b361243b_720w.webp)

就编程开发这条道路而言，每一个成长阶段的目标都会有它随着带来的难以攻克的难。

- 上学阶段，对突如其来的奇怪知识，想把它在自己电脑运行起来，就很难。
- 工作1~3年，以前掌握的都是毛皮，接下来需要有深度的学习，而深入后都将与数学硬碰硬。
- 工作3~5年，看以前理论性的知识也没那么难，但怎么实际要解决一些复杂项目，还是专心脑干。
- 工作5~7年，薪资与职位都会成为这个阶段非常难以突破的瓶颈，积累不足、沉淀不够，现状不满！
- 工作7~10年，以前觉得什么都难学，现在可能让你有空闲时间都难。并不一定年龄到了，本事就到了。

随着年龄的增长，每一阶段都有难以跨越的难。而那些看上去突破了瓶颈，达到了你想要的高度的人。其实每一个阶段，他们都跑在前面。

但就单纯的技术成长而言，其实理论知识并不难，只要你学就还能会，只是付出的时间成本不同罢了。但过了理论知识这一关后，接下来要面对的是创造能力，也就是为什么你感觉自己会了那么多技术内容，但是实际开发时却总感觉写不出好代码的阶段。

会了核心技术但又写不出好代码，就很像是：会汉字但写不出诗词歌赋、懂色彩但绘不出山河大川、能蹦跳但舞不出摇曳生姿。

所以，多实战一些项目代码，多看一些设计模式，会让你更好的理解代码该怎么用，也就能提升突破当前的阶段屏障。

## 3.怎么成长为架构师？

![img](https://pic4.zhimg.com/80/v2-85c6144f048101033160c79de8010fb3_720w.webp)

讲到架构师，架构师的成长更多的取决你们的研发组是否需要一个架构师，也同时需要你在这个岗位起到应有的作用。

**那么上图对于架构师的能力概况，有哪些具体的事项呢？**

1. 定得了规范、设计了架构。
2. 有一定的技术深入和广度，改的了bug、处理得了事故。
3. 带了了小组推进项目落地，也能协同其他组配合。
4. 了解运营和业务规划，提前介入产品开发阶段。
5. 懂得了业务和运营，了解数据指标和各项ROI。
6. 架构更多的是经验和经历的结合，而不是一个单项内容的单一渠道。
7. 不是没有架构师就没有架构，有时候是一个公司或者小组承接的项目并没有那么大，使用成型架构模式即可。
8. 但如果有非常复杂的场景设计，都是十几个系统的分组安排开发，提供服务，支持几万秒杀，几十万日活，在扩展到上百万DAU，就需要有架构师来把控。
9. 再比如：从下单、到交易、到支付、到结算、到活动、到玩法、怎么支持。这个体量的复杂度才需要有架构权衡。
10. 没有绝对的对和绝对的错，只是什么时候更适合罢了。多学一些，别给自己设定边界，才更好突围！

**做好架构，远看是部门效率，近看是解决烂代码！**很多时候的急，可能让整个工程烂掉。烂的越来越多，最终也会影响业务发展。那么这些烂代码都怎么来的呢？

1. bug很多时候是接手了的烂代码或者别人的思路没有继续继承。
2. 业务需求简单开始就写的没有扩展性，后面也不断的堆积。
3. 没有很好的结构和命名、也从不格式化。
4. 预期不到将来业务走向，设计不出合理的扩展性系统。
5. 炫技大于整体规划和设计，一个新技能的引入，但缺少相应的匹配。
6. 没有设计，功能都是流程式，需要啥就写ifelse。
7. 总想一把梭，没关系的，心里有抱怨，部门有急功近利，不给你长时间的铺垫，没有有人带，写不出好东西。
8. 组内缺少相应的流程规范和评审，设计评审、代码评审，也没与标杆项目可以参考。
9. 懂几个jdk源码从不是写好代码的根本只是基本功。就像老木匠用斧子，新木匠用电锯，但做出来的东西，有的就好，有的就不好。
10. 没有永远好的代码，如果像代码更好，就需要一直维护，一直改造。
11. 没有业务对应的体量，不谈QPS、TPS、TP99、TP999，服务健康度，很多空谈都是耍流氓。

**烂**，来自于很多方面，业务、产品、研发，三方共同努力才能更好的减少烂的出现，而这些也是每一个研发都应该努力的方向，也几乎是你要成为架构师的必经之路。

## 4.总结

写了这么多主要是想帮助那些和我一样在这条路上持续拼搏的同好，可能大家都会在这些阶段迷茫过：上学时技术怎么学、求职时简历怎么写、工作时个人怎么成长等等。所以很多时候更多的仍然是自己的克制和自己的选择！

原文链接：https://zhuanlan.zhihu.com/p/382942954

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.293】malloc函数背后的实现原理——内存池

## 1.前言

相对于栈而言，堆这片内存面临着一个稍微复杂的行为模式：在任意时刻，程序可能发出请求，要么申请一段内存，要么释放一段已经申请过的内存，而且申请的大小从几个字节到几个GB都有可能，我们不能假设程序一次申请多少堆空间，因此，堆的管理显得较为复杂。

那么，使用 malloc() 在堆上分配内存到底是如何实现的呢？

一种做法是把 malloc() 的内存管理交给系统内核去做，既然内核管理着进程的地址空间，那么如果它提供一个系统调用，可以让 malloc() 使用这个系统调用去申请内存，不就可以了吗？当然这是一种理论上的做法，但实际上这样做的性能比较差，因为每次程序申请或者释放堆空间都要进行系统调用。我们知道系统调用的性能开销是比较大的，当程序对堆的操作比较频繁时，这样做的结果会严重影响程序的性能。

比较好的做法就是 malloc() 向操作系统申请一块适当大小的堆空间，然后由 malloc() 自己管理这块空间。

malloc() 相当于向操作系统“批发”了一块较大的内存空间，然后“零售”给程序用。当全部“售完”或程序有大量的内存需求时，再根据实际需求向操作系统“进货”。当然 malloc() 在向程序零售堆空间时，必须管理它批发来的堆空间，不能把同一块地址出售两次，导致地址的冲突。于是 malloc() 需要一个算法来管理堆空间，这个算法就是堆的分配算法。

## 2.malloc()和free()的分配算法

在程序运行过程中，堆内存从低地址向高地址连续分配，随着内存的释放，会出现不连续的空闲区域，如下图所示：

![img](https://pic4.zhimg.com/80/v2-2995b85b5fc71f3479ef6bf611cb46fb_720w.webp)

图1：已分配内存和空闲内存相间出现

带阴影的方框是已被分配的内存，白色方框是空闲内存或已被释放的内存。程序需要内存时，malloc() 首先遍历空闲区域，看是否有大小合适的内存块，如果有，就分配，如果没有，就向操作系统申请（发生系统调用）。为了保证分配给程序的内存的连续性，malloc() 只会在一个空闲区域中分配，而不能将多个空闲区域联合起来。

内存块（包括已分配和空闲的）的结构类似于链表，它们之间通过指针连接在一起。在实际应用中，一个内存块的结构如下图所示：

![img](https://pic1.zhimg.com/80/v2-a653b6c31d5ab6f280c47a98bdbea970_720w.webp)

图2：内存块的结构

next 是指针，指向下一个内存块，used 用来表示当前内存块是否已被使用。这样，整个堆区就会形成如下图所示的链表：

![img](https://pic2.zhimg.com/80/v2-5160545f625e7bee403ed19df8aff0fd_720w.webp)

图3：类似链表的内存管理方式

现在假设需要为程序分配100个字节的内存，当搜索到图中第一个空闲区域（大小为200个字节）时，发现满足条件，那么就在这里分配。这时候 malloc() 会把第一个空闲区域拆分成两部分，一部分交给程序使用，剩下的部分任然空闲，如下图所示：

![img](https://pic4.zhimg.com/80/v2-b036577488569901d6de9a7cf76162af_720w.webp)

图4：为程序分配100个字节的内存

仍然以图3为例，当程序释放掉第三个内存块时，就会形成新的空闲区域，free() 会将第二、三、四个连续的空闲区域合并为一个，如下图所示：

![img](https://pic4.zhimg.com/80/v2-9f949b9b98f7cd468290c09f7f5097e7_720w.webp)

图5：释放第三个内存块

可以看到，malloc() 和 free() 所做的工作主要是对已有内存块的分拆和合并，并没有频繁地向操作系统申请内存，这大大提高了内存分配的效率。

另外，由于单向链表只能向一个方向搜索，在合并或拆分内存块时不方便，所以大部分 malloc() 实现都会在内存块中增加一个 pre 指针指向上一个内存块，构成双向链表，如下图所示：

![img](https://pic1.zhimg.com/80/v2-4ea1602990993e3974b3849cf2b7ec24_720w.webp)

链表是一种经典的堆内存管理方式，经常被用在教学中，很多C语言教程都会提到“栈内存的分配类似于数据结构中的栈，而堆内存的分配却类似于数据结构中的链表”就是源于此。

链表式内存管理虽然思路简单，容易理解，但存在很多问题，例如：
一旦链表中的 pre 或 next 指针被破坏，整个堆就无法工作，而这些数据恰恰很容易被越界读写所接触到。
小的空闲区域往往不容易再次分配，形成很多内存碎片。
经常分配和释放内存会造成链表过长，增加遍历的时间。

针对链表的缺点，后来人们提出了位图和对象池的管理方式，而现在的 malloc() 往往采用多种方式复合而成，不同大小的内存块往往采用不同的措施，以保证内存分配的安全和效率。

## 3.内存池

不管具体的分配算法是怎样的，为了减少系统调用，减少物理内存碎片，malloc() 的整体思想是先向操作系统申请一块大小适当的内存，然后自己管理，这就是内存池（Memory Pool）。

内存池的研究重点不是向操作系统申请内存，而是对已申请到的内存的管理，这涉及到非常复杂的算法，是一个永远也研究不完的课题，除了C标准库自带的 malloc()，还有一些第三方的实现，比如 Goolge 的 tcmalloc 和 jemalloc。

我们知道，C/C++是编译型语言，没有内存回收机制，程序员需要自己释放不需要的内存，这在给程序带来了很大灵活性的同时，也带来了不少风险，例如C/C++程序经常会发生内存泄露，程序刚开始运行时占用内存很少，随着时间的推移，内存使用不断增加，导致整个计算机运行缓慢。

内存泄露的问题往往难于调试和发现，或者只有在特定条件下才会复现，这给代码修改带来了不少障碍。为了提高程序的稳定性和健壮性，后来的 Java、Python、C#、JavaScript、PHP 等使用了虚拟机机制的非编译型语言都加入了垃圾内存自动回收机制，这样程序员就不需要管理内存了，系统会自动识别不再使用的内存并把它们释放掉，避免内存泄露。可以说，这些高级语言在底层都实现了自己的内存池，也即有自己的内存管理机制。

## 4.池化技术

在计算机中，有很多使用“池”这种技术的地方，除了内存池，还有连接池、线程池、对象池等。以服务器上的线程池为例，它的主要思想是：先启动若干数量的线程，让它们处于睡眠状态，当接收到客户端的请求时，唤醒池中某个睡眠的线程，让它来处理客户端的请求，当处理完这个请求，线程又进入睡眠状态。

所谓“池化技术”，就是程序先向系统申请过量的资源，然后自己管理，以备不时之需。之所以要申请过量的资源，是因为每次申请该资源都有较大的开销，不如提前申请好了，这样使用时就会变得非常快捷，大大提高程序运行效率。

原文链接：https://linuxcpp.0voice.com/?id=475

作者：[HG](https://linuxcpp.0voice.com/?auth=10)

# 【NO.294】深入理解MYSQL索引优化：多列索引

## 1.索引是什么

是存储引擎用于找到数据的一种数据结构。

## 2.索引的性能

在数据量小的时候，一个坏的索引往往作用没有那么明显，但是在数据量比较大的时候一个坏的索引和好的索引有巨大的区别。

> 在查询优化的时候应该首先考虑索引优化。这个是最简单的，也是效果最好。

## 3.索引的执行流程

索引 => 索引值 => 数据行

```
mysql> explain select first_name from actor where actor_id = 5;
+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+
| id | select_type | table | partitions | type  | possible_keys | key     | key_len | ref   | rows | filtered | Extra |
+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+
|  1 | SIMPLE      | actor | NULL       | const | PRIMARY       | PRIMARY | 2       | const |    1 |   100.00 | NULL  |
+----+-------------+-------+------------+-------+---------------+---------+---------+-------+------+----------+-------+
1 row in set, 1 warning (0.00 sec)
```

通过explian可以看到where条件后面是使用了主键索引。

## 4.多列索引

如果是多列组成的索引，那么在使用索引的时候要考虑索引的顺序。
多列索引有如下原则

- 最左匹配原则，如果缺失了索引的最左侧列，那么索引不生效
- 中间不可中断原则，如果中间定义的索引没有在where条件里面体现，那么后面的字段过滤将不会用到索引
- 范围中断原则，只要使用了范围查询，那么其右侧的索引都无法正常使用

## 5.多列索引的验证

### 5.1.建表语句

```
create table person
(
    A INT(10) not null,
    B INT(10) not null,
    C INT(10) not null,
    version VARCHAR(20) not null
);
create index A
    on person (A, B, C);
```

### 5.2.填充数据的存储过程

```
delimiter //
//
CREATE DEFINER=`dev`@`%` PROCEDURE `insert_person`(IN item int)
BEGIN
DECLARE counter INT;
SET counter = item;
WHILE counter >= 1000 DO
INSERT INTO person VALUES(left(counter, 2),left(counter,3), counter,counter );
SET counter = counter - 1;
END WHILE;
END;
//
delimiter ;
call insert_person(100000);
```

### 5.3.数据详情

```
mysql> select count(*) from person;
+----------+
| count(*) |
+----------+
|   100000 |
+----------+
1 row in set (0.04 sec)
```

## 6.场景分析

### 6.1.索引生效

#### 6.1.1. 符合多列索引生效的三个原则，全部用上了索引

(a) 只对A做等值查询

```
mysql> explain select * from person  where A = 12;
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref   | rows | filtered | Extra |
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-------+
|  1 | SIMPLE      | person | NULL       | ref  | A             | A    | 4       | const | 1100 |   100.00 | NULL  |
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-------+
1 row in set, 1 warning (0.00 sec)
```

> 可以看到是用到了A的索引的。

(b)只对A做范围查询,并且结果集覆盖索引的时候。

```
mysql> explain select A,B,C from person  where A > 13;
+----+-------------+--------+------------+-------+---------------+------+---------+------+-------+----------+--------------------------+
| id | select_type | table  | partitions | type  | possible_keys | key  | key_len | ref  | rows  | filtered | Extra                    |
+----+-------------+--------+------------+-------+---------------+------+---------+------+-------+----------+--------------------------+
|  1 | SIMPLE      | person | NULL       | range | A             | A    | 4       | NULL | 49555 |   100.00 | Using where; Using index |
+----+-------------+--------+------------+-------+---------------+------+---------+------+-------+----------+--------------------------+
1 row in set, 1 warning (0.00 sec)
```

（c）对A,B做等值查询

```
mysql> explain select A,B,C from person  where A = 13 and b = 129;
+----+-------------+--------+------------+------+---------------+------+---------+-------------+------+----------+-------------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref         | rows | filtered | Extra       |
+----+-------------+--------+------------+------+---------------+------+---------+-------------+------+----------+-------------+
|  1 | SIMPLE      | person | NULL       | ref  | A             | A    | 8       | const,const |    1 |   100.00 | Using index |
+----+-------------+--------+------------+------+---------------+------+---------+-------------+------+----------+-------------+
1 row in set, 1 warning (0.00 sec)
```

(d) 对A做等值，B做范围

```
mysql> explain select A,B,C from person  where A = 13 and b > 129;
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+--------------------------+
| id | select_type | table  | partitions | type  | possible_keys | key  | key_len | ref  | rows | filtered | Extra                    |
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+--------------------------+
|  1 | SIMPLE      | person | NULL       | range | A             | A    | 8       | NULL | 1100 |   100.00 | Using where; Using index |
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+--------------------------+
```

(e) ABC等值查询

```
mysql> explain select A,B,C from person  where A = 12 and b = 129 and c = 1294;
+----+-------------+--------+------------+------+---------------+------+---------+-------------------+------+----------+-------------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref               | rows | filtered | Extra       |
+----+-------------+--------+------------+------+---------------+------+---------+-------------------+------+----------+-------------+
|  1 | SIMPLE      | person | NULL       | ref  | A             | A    | 12      | const,const,const |    1 |   100.00 | Using index |
+----+-------------+--------+------------+------+---------------+------+---------+-------------------+------+----------+-------------+
1 row in set, 1 warning (0.00 sec)
```

(f) 对AB做等值，C范围查询

```
mysql> explain select A,B,C from person  where A = 12 and b = 129 and c > 1294;
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+--------------------------+
| id | select_type | table  | partitions | type  | possible_keys | key  | key_len | ref  | rows | filtered | Extra                    |
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+--------------------------+
|  1 | SIMPLE      | person | NULL       | range | A             | A    | 12      | NULL |  105 |   100.00 | Using where; Using index |
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+--------------------------+
1 row in set, 1 warning (0.00 sec)
```

## 7.索引部分生效

满足最左原则，但是不满足中间不中断或者范围中断。
(a) 对A做等值，C做等值或者是范围

```
mysql> explain select * from person where  A = 12 and c = 1230;
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-----------------------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref   | rows | filtered | Extra                 |
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-----------------------+
|  1 | SIMPLE      | person | NULL       | ref  | A             | A    | 4       | const | 1100 |    10.00 | Using index condition |
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-----------------------+
1 row in set, 1 warning (0.00 sec)
mysql> explain select * from person where  A = 12 and c >1230;
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-----------------------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref   | rows | filtered | Extra                 |
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-----------------------+
|  1 | SIMPLE      | person | NULL       | ref  | A             | A    | 4       | const | 1100 |    33.33 | Using index condition |
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-----------------------+
1 row in set, 1 warning (0.00 sec)
```

(b)对A做等值，B做范围，C做等值或者范围，B做了范围查询之后，无法对C正常使用索引

```
mysql> explain select * from person where  A = 12 and b > 123 and c >1230;
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+-----------------------+
| id | select_type | table  | partitions | type  | possible_keys | key  | key_len | ref  | rows | filtered | Extra                 |
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+-----------------------+
|  1 | SIMPLE      | person | NULL       | range | A             | A    | 8       | NULL |  660 |    33.33 | Using index condition |
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+-----------------------+
1 row in set, 1 warning (0.00 sec)
mysql> explain select * from person where  A = 12 and b = 123 and c >1230;
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+-----------------------+
| id | select_type | table  | partitions | type  | possible_keys | key  | key_len | ref  | rows | filtered | Extra                 |
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+-----------------------+
|  1 | SIMPLE      | person | NULL       | range | A             | A    | 12      | NULL |  109 |   100.00 | Using index condition |
+----+-------------+--------+------------+-------+---------------+------+---------+------+------+----------+-----------------------+
1 row in set, 1 warning (0.00 sec)
```

(c) 对A做范围查询，BC做等值或者范围，那么BC将不会使用索引

```
mysql> explain select A,B,C  from person where  A > 12 and b > 123 and c >1230;
+----+-------------+--------+------------+-------+---------------+------+---------+------+-------+----------+--------------------------+
| id | select_type | table  | partitions | type  | possible_keys | key  | key_len | ref  | rows  | filtered | Extra                    |
+----+-------------+--------+------------+-------+---------------+------+---------+------+-------+----------+--------------------------+
|  1 | SIMPLE      | person | NULL       | range | A             | A    | 4       | NULL | 49555 |    11.11 | Using where; Using index |
+----+-------------+--------+------------+-------+---------------+------+---------+------+-------+----------+--------------------------+
1 row in set, 1 warning (0.00 sec)
```

> 需要注意的是这个地方ABC只是普通的索引如果是select * from,那么将不会使用索引。因为还有一个字段不再索引持有的数据上。

## 8.多列索引失效：

（a）查询条件中不包含最左列

```
mysql> explain select * from person where  b = 123 and c >1230;
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref  | rows  | filtered | Extra       |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
|  1 | SIMPLE      | person | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 99110 |     3.33 | Using where |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
1 row in set, 1 warning (0.00 sec)
mysql> explain select * from person where  b = 123
    -> ;
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref  | rows  | filtered | Extra       |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
|  1 | SIMPLE      | person | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 99110 |    10.00 | Using where |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
1 row in set, 1 warning (0.00 sec)
mysql> explain select * from person where  b >123;
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref  | rows  | filtered | Extra       |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
|  1 | SIMPLE      | person | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 99110 |    33.33 | Using where |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
1 row in set, 1 warning (0.00 sec)
mysql> explain select * from person where  c =123;
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref  | rows  | filtered | Extra       |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
|  1 | SIMPLE      | person | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 99110 |    10.00 | Using where |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
1 row in set, 1 warning (0.00 sec)
mysql> explain select * from person where  c >123;
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref  | rows  | filtered | Extra       |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
|  1 | SIMPLE      | person | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 99110 |    33.33 | Using where |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
1 row in set, 1 warning (0.01 sec)
```

(b) 含有最左列，但是索引的列带条件

```
mysql> explain select * from person where  a + 1 =12;
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref  | rows  | filtered | Extra       |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
|  1 | SIMPLE      | person | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 99110 |   100.00 | Using where |
+----+-------------+--------+------------+------+---------------+------+---------+------+-------+----------+-------------+
1 row in set, 1 warning (0.00 sec)
## 从哪里带条件哪里就不开始使用索引
mysql> explain select * from person where  a =12 and b + 1 = 123;
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-----------------------+
| id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref   | rows | filtered | Extra                 |
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-----------------------+
|  1 | SIMPLE      | person | NULL       | ref  | A             | A    | 4       | const | 1100 |   100.00 | Using index condition |
+----+-------------+--------+------------+------+---------------+------+---------+-------+------+----------+-----------------------+
1 row in set, 1 warning (0.00 sec)
```

## 9.order by使用索引

(a) order by 字段的顺序和table中多列索引定义的顺序一样。

```
mysql> explain select * from person1 order by a,b, c;
+----+-------------+---------+------------+------+---------------+------+---------+------+-------+----------+----------------+
| id | select_type | table   | partitions | type | possible_keys | key  | key_len | ref  | rows  | filtered | Extra          |
+----+-------------+---------+------------+------+---------------+------+---------+------+-------+----------+----------------+
|  1 | SIMPLE      | person1 | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 98999 |   100.00 | Using filesort |
+----+-------------+---------+------------+------+---------------+------+---------+------+-------+----------+----------------+
1 row in set, 1 warning (0.00 sec)
mysql> explain select a,b,c from person1 order by a,b, c;
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-------------+
| id | select_type | table   | partitions | type  | possible_keys | key         | key_len | ref  | rows  | filtered | Extra       |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-------------+
|  1 | SIMPLE      | person1 | NULL       | index | NULL          | index_A_B_C | 21      | NULL | 98999 |   100.00 | Using index |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-------------+
1 row in set, 1 warning (0.00 sec)
```

## 10.order by 不使用索引

(a) order by多个字段，每个字段都是独立的索引

```
mysql> create table person2 select * from person1;
Query OK, 99001 rows affected (0.68 sec)
Records: 99001  Duplicates: 0  Warnings: 0
mysql> alter table add key(a);
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'add key(a)' at line 1
mysql> alter table add index_a key(a);
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'add index_a key(a)' at line 1
mysql> alter table add index  key(a);
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'add index  key(a)' at line 1
mysql> alter table add index index_a key(a);
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'add index index_a key(a)' at line 1
mysql> alter table add index index_a(a);
ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'add index index_a(a)' at line 1
mysql> alter table person2 add index index_a(a);
Query OK, 0 rows affected (0.15 sec)
Records: 0  Duplicates: 0  Warnings: 0
mysql> alter table person2 add index index_b(b);
Query OK, 0 rows affected (0.15 sec)
Records: 0  Duplicates: 0  Warnings: 0
mysql> explain select * from person2 order by a,b;
+----+-------------+---------+------------+------+---------------+------+---------+------+-------+----------+----------------+
| id | select_type | table   | partitions | type | possible_keys | key  | key_len | ref  | rows  | filtered | Extra          |
+----+-------------+---------+------------+------+---------------+------+---------+------+-------+----------+----------------+
|  1 | SIMPLE      | person2 | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 99110 |   100.00 | Using filesort |
+----+-------------+---------+------------+------+---------------+------+---------+------+-------+----------+----------------+
1 row in set, 1 warning (0.00 sec)
```

(b) order by语句中同时含有desc和asc

```
mysql> explain select a,b,c from person1 order by a desc ;
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-------------+
| id | select_type | table   | partitions | type  | possible_keys | key         | key_len | ref  | rows  | filtered | Extra       |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-------------+
|  1 | SIMPLE      | person1 | NULL       | index | NULL          | index_A_B_C | 21      | NULL | 98999 |   100.00 | Using index |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-------------+
1 row in set, 1 warning (0.01 sec)
mysql> explain select a,b,c from person1 order by a desc ,b asc;
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-----------------------------+
| id | select_type | table   | partitions | type  | possible_keys | key         | key_len | ref  | rows  | filtered | Extra                       |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-----------------------------+
|  1 | SIMPLE      | person1 | NULL       | index | NULL          | index_A_B_C | 21      | NULL | 98999 |   100.00 | Using index; Using filesort |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-----------------------------+
1 row in set, 1 warning (0.00 sec)
```

(c) order by 里面有计算字段

```
mysql> explain select a,b,c from person1 order by a desc ,b  + 1
    -> ;
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-----------------------------+
| id | select_type | table   | partitions | type  | possible_keys | key         | key_len | ref  | rows  | filtered | Extra                       |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-----------------------------+
|  1 | SIMPLE      | person1 | NULL       | index | NULL          | index_A_B_C | 21      | NULL | 98999 |   100.00 | Using index; Using filesort |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-----------------------------+
1 row in set, 1 warning (0.00 sec)
```

(d) orderby 里面顺序和索引的顺序不同

```
mysql> explain select a,b,c from person1 order by b,a;
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-----------------------------+
| id | select_type | table   | partitions | type  | possible_keys | key         | key_len | ref  | rows  | filtered | Extra                       |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-----------------------------+
|  1 | SIMPLE      | person1 | NULL       | index | NULL          | index_A_B_C | 21      | NULL | 98999 |   100.00 | Using index; Using filesort |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-----------------------------+
1 row in set, 1 warning (0.00 sec)
```

(e) group by 和orderby 的顺序不同

```
mysql> explain select a,b,c from person1 group by a,b,c order by a,b;
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-------------+
| id | select_type | table   | partitions | type  | possible_keys | key         | key_len | ref  | rows  | filtered | Extra       |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-------------+
|  1 | SIMPLE      | person1 | NULL       | index | index_A_B_C   | index_A_B_C | 21      | NULL | 98999 |   100.00 | Using index |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+-------------+
1 row in set, 1 warning (0.00 sec)
mysql> explain select a,b,c from person1 group by a,b,c order by b,a;
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+----------------------------------------------+
| id | select_type | table   | partitions | type  | possible_keys | key         | key_len | ref  | rows  | filtered | Extra                                        |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+----------------------------------------------+
|  1 | SIMPLE      | person1 | NULL       | index | index_A_B_C   | index_A_B_C | 21      | NULL | 98999 |   100.00 | Using index; Using temporary; Using filesort |
+----+-------------+---------+------------+-------+---------------+-------------+---------+------+-------+----------+----------------------------------------------+
1 row in set, 1 warning (0.00 sec)
```

> 上面的查询顺序一样，没有filesort，下面的不一样，就用了

(f) join查询的时候order只能按照主键的来排序，否则就会失效

```
mysql> explain select *  from person3 t1 left join person4 t2 on t1.version = t2.version where t1.version = 1200  order by t1.version;
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+-------------+
| id | select_type | table | partitions | type   | possible_keys | key     | key_len | ref            | rows | filtered | Extra       |
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+-------------+
|  1 | SIMPLE      | t1    | NULL       | index  | PRIMARY       | PRIMARY | 22      | NULL           |  101 |    10.00 | Using where |
|  1 | SIMPLE      | t2    | NULL       | eq_ref | PRIMARY       | PRIMARY | 22      | dev.t1.version |    1 |   100.00 | NULL        |
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+-------------+
2 rows in set, 3 warnings (0.00 sec)
mysql> explain select *  from person3 t1 left join person4 t2 on t1.version = t2.version where t1.version = 1200  order by t2.version;
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+----------------------------------------------+
| id | select_type | table | partitions | type   | possible_keys | key     | key_len | ref            | rows | filtered | Extra                                        |
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+----------------------------------------------+
|  1 | SIMPLE      | t1    | NULL       | ALL    | PRIMARY       | NULL    | NULL    | NULL           |  101 |    10.00 | Using where; Using temporary; Using filesort |
|  1 | SIMPLE      | t2    | NULL       | eq_ref | PRIMARY       | PRIMARY | 22      | dev.t1.version |    1 |   100.00 | NULL                                         |
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+----------------------------------------------+
2 rows in set, 3 warnings (0.00 sec)
mysql> show create table person3;
+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Table   | Create Table                                                                                                                                                                                                |
+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| person3 | CREATE TABLE `person3` (
  `A` varchar(10) DEFAULT NULL,
  `B` int(50) NOT NULL,
  `C` int(50) NOT NULL,
  `version` varchar(20) NOT NULL,
  PRIMARY KEY (`version`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1 |
+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.00 sec)
mysql> show create table person4;
+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Table   | Create Table                                                                                                                                                                                                |
+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| person4 | CREATE TABLE `person4` (
  `A` varchar(10) DEFAULT NULL,
  `B` int(50) NOT NULL,
  `C` int(50) NOT NULL,
  `version` varchar(20) NOT NULL,
  PRIMARY KEY (`version`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1 |
+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.00 sec)
mysql> explain select *  from person3 t1 left join person4 t2 on t1.version = t2.version order by t1.version;
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+-------+
| id | select_type | table | partitions | type   | possible_keys | key     | key_len | ref            | rows | filtered | Extra |
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+-------+
|  1 | SIMPLE      | t1    | NULL       | index  | NULL          | PRIMARY | 22      | NULL           |  101 |   100.00 | NULL  |
|  1 | SIMPLE      | t2    | NULL       | eq_ref | PRIMARY       | PRIMARY | 22      | dev.t1.version |    1 |   100.00 | NULL  |
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+-------+
2 rows in set, 1 warning (0.00 sec)
mysql> explain select *  from person3 t1 left join person4 t2 on t1.version = t2.version order by t2.version;
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+---------------------------------+
| id | select_type | table | partitions | type   | possible_keys | key     | key_len | ref            | rows | filtered | Extra                           |
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+---------------------------------+
|  1 | SIMPLE      | t1    | NULL       | ALL    | NULL          | NULL    | NULL    | NULL           |  101 |   100.00 | Using temporary; Using filesort |
|  1 | SIMPLE      | t2    | NULL       | eq_ref | PRIMARY       | PRIMARY | 22      | dev.t1.version |    1 |   100.00 | NULL                            |
+----+-------------+-------+------------+--------+---------------+---------+---------+----------------+------+----------+---------------------------------+
2 rows in set, 1 warning (0.00 sec)
## 不加where也是一样的。
```

优化此种情况

```
mysql> explain select * from person3 t1 left join person4 t2 on t1.version = t2.version join(select version from person3 order by a desc) a_order on t1.version = a_order.version;
+----+-------------+---------+------------+--------+---------------+---------+---------+----------------+------+----------+-------+
| id | select_type | table   | partitions | type   | possible_keys | key     | key_len | ref            | rows | filtered | Extra |
+----+-------------+---------+------------+--------+---------------+---------+---------+----------------+------+----------+-------+
|  1 | SIMPLE      | t1      | NULL       | ALL    | PRIMARY       | NULL    | NULL    | NULL           |  101 |   100.00 | NULL  |
|  1 | SIMPLE      | person3 | NULL       | eq_ref | PRIMARY       | PRIMARY | 22      | dev.t1.version |    1 |   100.00 | NULL  |
|  1 | SIMPLE      | t2      | NULL       | eq_ref | PRIMARY       | PRIMARY | 22      | dev.t1.version |    1 |   100.00 | NULL  |
+----+-------------+---------+------------+--------+---------------+---------+---------+----------------+------+----------+-------+
3 rows in set, 1 warning (0.00 sec)
```

原文链接：https://zhuanlan.zhihu.com/p/384216746

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.295】C++高性能服务器框架——日志系统详解

## 1.日志文件系统

对文件系统进行修改时，需要进行很多操作。这些操作可能中途被打断，也就是说，这些操作不是“不可中断”(atomic)的。如果操作被打断，就可能造成文件系统出现不一致的状态。

例如：删除文件时，先要从目录树中移除文件的标示，然后收回文件占用的空间。如果在这两步之间操作被打断，文件占用的空间就无法收回。文件系统认为它是被占用的，但实际上目录树中已经找不到使用它的文件了。

在非日志文件系统中，要检查并修复类似的错误必须对整个文件系统的数据结构进行检查。这个操作可能会花费很长的时间。

为了避免这样的错误，日志文件系统分配了一个称为日志的区域来提前记录要对文件系统做的更改。在崩溃后，只要读取日志重新执行未完成的操作，文件系统就可以恢复一致。这种恢复是原子的，因为只存在几种情况：

- 不需要重新执行：这个事务被标记为已经完成
- 成功重新执行：根据日志，这个事务被重新执行
- 无法重新执行：这个事务会被撤销，就如同这个事务没有发生过一样
- 日志本身不完整：事务还没有被完全写入日志，他会被简单忽略

## 2.日志系统的设计

### 2.1.为什么需要日志

对于一些高频操作（如心跳包、定时器、界面绘制下的某些高频重复的行为），可能在少量次数下无法触发我们想要的行为，而通过断点的暂停方式，我们不得不重复操作几十次、上百次甚至更多，这样排查问题效率是非常低下的。对于这类操作，我们可以通过打印日志，将当时的程序行为上下文现场记录下来，然后从日志系统中找出某次不正常行为的上下文信息。

### 2.2.日志系统的技术上的实现

**同步写日志**

所谓同步写日志，指的是在输出日志的地方，将日志即时写入到文件中去。采用这种方式，势必造成CPU等待，进而导致主线程“卡”在写文件处，进而造成界面卡帧。

但是，很多时候我们不用担心这种问题，主要有两个原因：其一，是用户感觉不到这种同步写文件造成的延迟；其二，是客户端除了UI线程外，还有其他与界面无关的工作线程，在这些线程中写文件一般不会对用户的体验产生什么影响。

**多线程同步写日志出现的问题一——不同线程的日志事件时间序列错乱**

产生这种问题的主要原因是由于多个线程同时写日志到同一个文件时，产生日志的时间和实际写入磁盘的时间不是一个原子操作。我们可以这样来理解，一个线程T1在t1时刻产生了日志，另一个线程T2在t2时刻也产生了一个日志（t2 > t1）。但是由于一些原因导致线程T1发生阻塞，并没有把日志即刻写入到文件中，而线程T2并没有发生阻塞，即刻就把日志信息写入到了文件中。这种情况的存在就会导致不同线程的日志事件时间序列错乱。

**多线程同步写日志出现的问题二——不同线程的日志输出错乱拼接**

假设线程A的日志信息为AAAAAA，线程B的日志信息为BBBBBB，线程C的日志信息为CCCCCC。那么会不会产生一种情况使得日志文件中的输出结果为AABBCCAABBCCAABBCC。

实际上，在类Unix系统上（包括Linux），同一个进程内针对同一个FIEL*的操作是线程安全的，也就是说在Linux系统上不会产生上述的情况发生。

在Windows系统上，由于对FILE*的操作并不是线程安全的，可能会发生上述情况。

这种同步日志的实现方式，一般用于低频写日志的软件系统中，所以可以认为这种多线程同时写日志到同一个文件中是可行的。

**异步写日志**

所谓异步写日志，就是通过一些线程同步技术将日志先暂存下来，然后再通过一个或多个专门的日志写入线程将这些缓存的日志写入到磁盘中。

实现的时候可以使用一个队列来存储其他线程产生的日志，日志线程从该队列中取出日志，然后将日志内容写入文件。

## 3.日志系统的实现

设计一个日志系统需要考虑哪些问题？

既然是日志系统肯定需要记录日志（LogEvent），那么我们就需要一个类来表达日志的概念，这个类至少应该包含两个属性，一个是时间戳，另一个是消息本身。

其次是日志输出（LogAppender），可以输出到不同的地方，控制台、文件。

然后是将日志信息进行格式化输出（LogFormatter），LogAppender 可以引用 LogFormatter 这样就可以将 LogEvent 事件中的日志消息经过 LogFormatter 进行格式化，然后再由 LogAppender 输出。

如果想要获取日志，必须得先获取一个什么东西，这个东西可以成为 Logger，此外，还可以使用 LoggerManager 对这些不同的 Logger 进行管理。

LogLevel 类定义了日志的级别。

LogEventWarp 对 LogEvent 进行了封装，当 LogEventWarp 析构的时候，能够触发 LogEvent 将日志信息写入到指定位置。

### 3.1.类图

![img](https://pic3.zhimg.com/80/v2-3d0b04abd65c2142a1b5bbd1db46ee12_720w.webp)

### 3.2.日志写入流程图

![img](https://pic3.zhimg.com/80/v2-bab7eccf97c1cd34de90211fd47232ce_720w.webp)

### 3.3.LogEvent

LogEvent 是日志事件，所有的日志信息都是由 LogEvent 来管理，同时 LogEvent 也提供了格式化写入的功能。

```
// 日志事件
class LogEvent {
   public:
    // 指向日志事件的指针
    typedef std::shared_ptr<LogEvent> ptr;
    /*
     * 构造函数
     * 传入 Logger 指针，可以将该日志事件写入到对应的 Logger 中
     */
    LogEvent(std::shared_ptr<Logger> logger, LogLevel::Level level, const char* file, 
    ~LogEvent() {}
    // 获取该日志事件所对应的 Logger 类
    std::shared_ptr<Logger> getLogger() const {  return m_logger; }
    // 获取日志级别
    LogLevel::Level getLevel() const { return m_level; }
    // 获取文件名
    const char* getFileName() const { return m_file; }
    // 获取行号
    int32_t getLine() const { return m_line; }
    // 获取运行的时间
    uint32_t getElapse() const { return m_elapse; }
    // 获取线程 id
    uint32_t getThreadId() const { return m_threadId; }
    // 获取协程 id
    uint32_t getFiberId() const { return m_fiberId; }
    // 获取当前时间
    uint32_t getTime() const { return m_time; }
    // 获取日志内容
    std::string getContent() const { return m_ss.str(); }
    // 以 stringstream 的形式获取日志内容
    std::stringstream& getSS() { return m_ss; }
    // 将日志内容进行格式化
    void format(const char* fmt, ...);
    void format(const char* fmt, va_list al);
   private:
    const char* m_file = nullptr;  // 文件名
    int32_t m_line = 0;            // 行号
    uint32_t m_elapse = 0;         // 程序启动开始到现在的毫秒数
    uint32_t m_threadId = 0;       // 线程id
    uint32_t m_fiberId = 0;        // 协程id
    uint64_t m_time;               // 时间戳
    std::stringstream m_ss;             // 日志流
    std::shared_ptr<Logger> m_logger;   // 指向 Logger 类的指针
    LogLevel::Level m_level;            // 该日志事件的级别
};
```

下面的这两个函数提供了一种获取变长参数的方法，具体的原理可以查看这篇博客，大致的意思就是通过传入的第一个参数 fmt 来确定每一个变长参数在内存中的位置，进而获取参数的取值。

(gdb) p fmt
$3 = 0x414228 “test macro fmt error %s”

从上面调试的结果可以看出该函数获取变长参数的具体做法是在给定字符串的最后加上变长参数的格式化输出。

vasprintf 函数将变长参数的内容输出到 buf 中，若成功则返回输出内容的长度，若失败则返回 -1.

```
/**
 * 获取省略号指定的参数
 */
void LogEvent::format(const char* fmt, ...) {
    va_list al; 
    va_start(al, fmt);
    format(fmt, al);
    va_end(al);
}
/**
 * 将参数输出到m_ss中（格式化写入）
 */
void LogEvent::format(const char* fmt, va_list al) {
    char* buf = nullptr;
    int len = vasprintf(&buf, fmt, al);
    if (len != -1) {
        m_ss << std::string(buf, len);
        free(buf);
    }   
}
```

上面的这种格式化输出主要用在了下面的这个宏定义里面

```
#define RAINBOW_LOG_FMT_LEVEL(logger, level, fmt, ...) \
    if (logger->getLevel() <= level) \
        rainbow::LogEventWrap(rainbow::LogEvent::ptr(new rainbow::LogEvent(logger, level, \
                        __FILE__, __LINE__, 0, rainbow::GetThreadId(), \
                        rainbow::GetFiberId(), time(0)))).getEvent()->format(fmt, __VA_ARGS__)
```

### 3.4.LogAppender

通过该类派生出的不同子类可以将日志信息输出到不同的位置。一个 Logger 可以有多个 Appender，LogAppender 有单独的日志级别，因此可以通过设置不同级别的 Appender 从而将日志输出到不同的位置。此外每一个 LogAppender 也都会有自己单独的日志格式，从而方便进行管理。

还可以通过 scoket 套接字，将日志输出到服务器上。

```
// 日志输出到目的地（控制台、文件）
class LogAppender {
   public:
    typedef std::shared_ptr<LogAppender> ptr;
    LogAppender();
    virtual ~LogAppender() {}
    // 纯虚函数，子类必须实现该方法
    virtual void log(std::shared_ptr<Logger> logger, LogLevel::Level level,
                     LogEvent::ptr event) = 0;
    // 按照给定的格式序列化输出
    void setFormatter(LogFormatter::ptr val) { m_formatter = val; }
    // 获取日志格式
    LogFormatter::ptr getFormatter() const { return m_formatter; }
    LogLevel::Level getLevel() { return m_level; }
    void setLevel(const LogLevel::Level& level);
   protected:
    LogLevel::Level m_level;
    LogFormatter::ptr m_formatter;
};
// 输出到控制台的Appender
class StdoutLogAppender : public LogAppender {
   public:
    typedef std::shared_ptr<StdoutLogAppender> ptr;
    virtual void log(Logger::ptr logger, LogLevel::Level level,
                     LogEvent::ptr event) override;
};
// 定义输出到文件的Appender
class FileLogAppender : public LogAppender {
   public:
    typedef std::shared_ptr<FileLogAppender> ptr;
    virtual void log(Logger::ptr logger, LogLevel::Level level,
                     LogEvent::ptr event) override;
    FileLogAppender(const std::string& filename);
    // 重新打开文件，如果文件打开成功则返回true
    bool reopen();
   private:
    std::string m_filename;
    std::ofstream m_filestream;
};
```

### 3.5.LogFormatter

日志格式器（LogFormatter）可以将传入的日志格式进行解析，并可以和 LogEvent 指针结合将特定格式的日志信息输出到 stringstream 中，等待 LogEventWrap 析构的时候将日志信息写入到指定的 Appender 中。

```
// 日志格式器
class LogFormatter {
   public:
    // 指向该类的指针
    typedef std::shared_ptr<LogFormatter> ptr;
    // 日志格式
    LogFormatter(const std::string& pattern);
　　// 对日志进行解析，并返回格式化之后的string
    std::string format(std::shared_ptr<Logger> ptr, LogLevel::Level level,
                       LogEvent::ptr event);
    std::ostream& format(std::ostream& ofs, std::shared_ptr<Logger> ptr, LogLevel::Level level, LogEvent::ptr event);
   public:
    class FormatItem {
       public:
        FormatItem(const std::string& fmt = ""){};
        virtual ~FormatItem() {}
        virtual void format(std::ostream& os, std::shared_ptr<Logger> logger,
                            LogLevel::Level level, LogEvent::ptr event) = 0;
        // 注意这里的指针类型是FormatItem类型的指针
        typedef std::shared_ptr<FormatItem> ptr;
    };  
    void init();
   private:
　　// 日志格式
    std::string m_pattern;
　　// 根据日志格式解析出的日志格式单元类的指针存放至数组中
    std::vector<FormatItem::ptr> m_items;
};
```

### 3.6.Logger

日志可以用 Logger 类来进行表示，每一个 Logger 类含有多个 LoggerAppender，可以通过指针把 Logger 类传递给 LogEvent 从而使日志事件能够获取 Logger 类的一些信息。

std::enable_shared_from_this 能让一个对象（假设其名为 t ，且已被一个 std::shared_ptr 对象 pt 管理）安全地生成其他额外的 std::shared_ptr 实例（假设名为 pt1, pt2, … ） ，它们与 pt 共享对象 t 的所有权。

```
// 日志器
class Logger : public std::enable_shared_from_this<Logger> {
   public:
    typedef std::shared_ptr<Logger> ptr;
    Logger(const std::string& name = "root");
    // 只有满足日直级别的日志才会被输出
    void log(LogLevel::Level level, LogEvent::ptr event);
    void debug(LogEvent::ptr event);
    void info(LogEvent::ptr event);
    void warn(LogEvent::ptr event);
    void error(LogEvent::ptr event);
    void fatal(LogEvent::ptr event);
    void addAppender(LogAppender::ptr appender);
    void delAppender(LogAppender::ptr appender);
    LogLevel::Level getLevel() const { return m_level; }
    void setAppender(LogLevel::Level val) { m_level = val; }
    const std::string getName() const { return this->m_name; }
   private:
    std::string m_name = "root";       // 日志名称
    LogLevel::Level m_level;  // 日志器的级别
    std::list<LogAppender::ptr> m_appenders;  // Appender集合
    LogFormatter::ptr m_formatter;            // 日志格式
};
```

### 3.7.LoggerManager

管理所有的日志器，并且支持通过日志器的名字获取日志器。

```
// 管理所有的日志器
class LoggerManager {
public:
    LoggerManager();
    Logger::ptr getLogger(const std::string& name);
    void init();
    Logger::ptr getRoot() const { return m_root; }
private:
    // 通过日志器的名字获取日志器
    std::map<std::string, Logger::ptr> m_loggers;
    Logger::ptr m_root;
};
/**
 * 日志器管理类，单例模式
 */
typedef rainbow::Singleton<LoggerManager> LoggerMgr;
}  // namespace rainbow
```

原文链接：https://linuxcpp.0voice.com/?id=569

作者：[HG](https://linuxcpp.0voice.com/?auth=10)

# 【NO.296】熬夜肝了这一份C++开发详细学习路线

一般开发岗主流的就是 Java 后台开发，前端开发以及 C++ 后台开发，现在 Go 开发也是越来越多了，今天把 C++ 后台开发学习路线补上。

写之前先来回答几个问题

### **1.C++ 后台开发有哪些岗位？**

C++ 后台开发的岗位还是很多的，例如游戏引擎开发，游戏服务端开发，音视频服务端/客户端开发，数据库内核开发等等，而且 C++ 也能用来写深度学习，做硬件底层这些。

总之，C++ 后台开发的岗位，还是很丰富的，大家不用担心找不到合适的岗位。

### **2.C++ 后台开发岗位需求量大吗？**

一般大公司大需求量会多一些，小公司需求量较少。

说到岗位需求量，那肯定是 Java 的岗位需求量是最大的，当然，学 Java 的人也是最多的，假如你要学习 C++，那我觉得你要定位大公司可能会好一点，进大公司反而会比 Java 容易。
假如你觉得自己实力很一般，够不着大公司，那我觉得你可以考虑学习 Java，因为大部分小公司，Java 岗位多一些。

但是呢，假如你是应届生，那么语言其实也不是特别重要，只要你 把计算机基础和算法学好，就算你是学 Java 的，也可以去面 C++；学 C++ 的也可以去面 Java。

我当时是学 Java 的，不过秋招那会还面了几个 C++ 岗位，直接和面试官说我不会 C++ 就可以了，他会问你其他的知识。

下面跟大家说一说 C++ 后台开发学习路线，为了方便大家做规划，每一个模块的学习，我都会说下大致的学习时间

C/C++后台开发学习路线总结图

![img](https://pic3.zhimg.com/80/v2-8c5a0c0d084a216d21b300724d87c306_720w.webp)

#### 2.1.C++ 基础

假如你有 C 语言基础，那么这块感觉花个三四个月就能拿下了，假如你是零基础的，估计还得学两三个月的 C 语言，也就是说，得花半年时间才行，没有 C 语言基础的看这个 C 语言教程：[一份评价超高的 C 语言入门教程]
C++ 这块，重点需要学习的就是一些**关键字**、**面向对象**以及 **STL 容器**的知识，特别是 STL，还得研究下他们的一些源码，下面我总结一下一些比较重要的知识（其实是根据面试结果来挑选）。

1. 指针与引用的区别，C 与 C++ 的区别，struct 与 class 的区别
2. struct 内存对齐问题，sizeof 与 strlen 区别
3. 面向对象的三大特性：封装、继承、多态
4. 类的访问权限：private、protected、public
5. 类的构造函数、析构函数、赋值函数、拷贝函数
6. 移动构造函数与拷贝构造函数对比
7. 内存分区：全局区、堆区、栈区、常量区、代码区
8. 虚函数实现动态多态的原理、虚函数与纯虚函数的区别
9. 深拷贝与浅拷贝的区别
10. 一些关键字：static, const, extern, volatile 等
11. 四种类型转换：static_cast、dynamic_cast、const_cast、reinterpret_cast
12. 静态与多态：重写、重载、模板
13. 四种智能指针及底层实现：auto_ptr、unique_ptr、shared_ptr、weak_ptr
14. 右值引用
15. std::move函数
16. 迭代器原理与迭代器失效问题
17. 一些重要的 STL：vector, list, map, set 等。
18. 容器对比，如 map 与 unordered_map 对比，set 与 unordered_set 对比，vector 与 list 比较等。
19. STL容器空间配置器

等等。
根据书来学就可以了，然后学到一些重点，可以重点关注一下。
书籍推荐：

1、《C++Primer》，这本书内容很多的，把前面基础的十几章先看一看，不用从头到尾全啃，后面可以**字典**来使用。

2、《STL 源码剖析》，必看书籍，得知道常见 STL 的原理，建议看个两三遍。

3、《深度探索C++对象模型》，这本主要讲解**面向对象**的相关知识，可以帮你扫清各种迷雾。

#### 2.2.计算机网络

无论你是从事啥岗位，无论是校招还是社招，计算机网络基本都会问，特特是腾讯，字节，shopee，小米等这些非 Java 系的公司，问的更多。这块认真学，**一个半月**就可以搞定了。

计算机网络就是一堆协议的构成，下面是一些比较重要的知识点，学的时候可以重点关注下。

**物理层、链路层**：

1. MTU，MAC地址，以太网协议。
2. 广播与 ARP 协议

**网络层**

1. ip 地址分类
2. IP 地址与 MAC 地址区别
3. 子网划分，子网掩码
4. ICMP 协议及其应用
5. 路由寻址
6. 局域网，广域网区别

**传输层**（主要就是 TCP）

1. TCP首部报文格式（SYN、ACK、FIN、RST必须知道）
2. TCP滑动窗口原理，TCP 超时重传时间选择
3. TCP 拥塞控制，TCP 流量控制
4. TCP 三次握手与四次挥手以及状态码的变化
5. TCP连接释放中TIME_WAIT状态的作用
6. SYN 泛洪攻击
7. TCP 粘包，心跳包
8. UDP 如何实现可靠传输
9. UDP 与 TCP 的区别
10. UDP 以及 TCP 的应用场景

**应用层**

1. DNS 原理以及应用
2. HTTP 报文格式，HTTP1.0、HTTP1.1、HTTP2.0 之间的区别
3. HTTP 请求方法的区别：GET、HEAD、POST、PUT、DELETE
4. HTTP 状态码
5. HTTP 与 HTTPS 的区别
6. 数字证书，对称加密与非对称加密
7. cookie与session区别
8. 输入一个URL到显示页面的流程（越详细越好，搞明白这个，网络这块就差不多了）

书籍推荐：零基础可以先看《图解HTTP》，当然，也可以直接看《计算机网网络：自顶向下》这本书，这本书建议看两遍以及以上，还有时间的可以看《TCP/IP详解卷1：协议》。

### 3.操作系统

操作系统和计算机网络差不多，不过计算机网络会问的多一些，操作系统会少一些，学到时候如果可以带着问题去学是最好的，例如

咋就还有进程和线程之分？为什么要有挂起、运行、阻塞等这么多种状态？怎么就还有悲观锁和乐观锁，他们的本质区别？

进程咋还会出现死锁，都有哪些处理策略？进程都有哪些调度算法？

虚拟内存解决了什么问题？为啥每个进程的内存地址就是独立的呢？

为啥 cpu 很快而内存很慢？磁盘怎么就更慢了？

总结起来大致：
1、进程与线程区别
2、线程同步的方式：互斥锁、自旋锁、读写锁、条件变量
3、互斥锁与自旋锁的底层区别
4、孤儿进程与僵尸进程
5、死锁及避免
6、多线程与多进程比较
7、进程间通信：PIPE、FIFO、消息队列、信号量、共享内存、socket
8、管道与消息队列对比
9、fork进程的底层：读时共享，写时复制
10、线程上下文切换的流程
11、进程上下文切换的流程
12、进程的调度算法
13、阻塞IO与非阻塞IO
14、同步与异步的概念
15、静态链接与动态链接的过程
16、虚拟内存概念（非常重要）
17、MMU地址翻译的具体流程
18、缺页处理过程
19、缺页置换算法：最久未使用算法、先进先出算法、最佳置换算法

书籍推荐：《现代操作系统》

这里也有一门合并的视频：[C/C++后台开发学习视频](https://link.zhihu.com/?target=https%3A//ke.qq.com/course/417774%3FflowToken%3D1013189)

### 4.MySQL(一个月左右)

数据库一般主流的有 MySQL 和 Oracle，不过建议大家学习 MySQL 了，因为大部分公司都是使用 MySQL，也是属于面试必问，而且工作中 MySQL 也是接触的最多的，毕竟工作 crud 才是常态。

下面这些是我认为比较重要的知识点：

1、一条 sql 语句是如何执行的？也就是说，从客户端执行了一条 sql 命令，服务端会进行哪些处理？（例如验证身份，是否启用缓存啥的）。

2、索引相关：索引是如何实现的？多种引擎的实现区别？聚族索引，非聚族索引，二级索引，唯一索引、最左匹配原则等等（非常重要）。

3、事务相关：例如事务的隔离是如何实现的？事务是如何保证原子性？不同的事务看到的数据怎么就不一样了？难道每个事务都拷贝一份视图？MVCC 的实现原理（重要）等等。

4、各种锁相关：例如表锁，行锁，间隙锁，共享锁，排他锁。这些锁的出现主要是用来解决哪些问题？（重要）

5、日志相关：redolog，binlog，undolog，这些日志的实现原理，为了解决怎么问题？日志也是非常重要的吧，面试也问的挺多。

6、数据库的主从备份、如何保证数据不丢失、如何保证高可用等等。

7、一些故障排查的命令，例如慢查询，sql 的执行计划，索引统计的刷新等等。

对于 2-4 这四个相关知识，面试被问到的频率是最高的，有时候面试会让你说一说索引，如果你知道的多的话就可以疯狂扯一波了，记得我当时总结了一套扯的模版：

先说从 B 树角度说为啥索引会快-》趁机说一下索引的其他实现方式-〉不同引擎在索引实现上的不同-》系统是如果判断是否要使用索引的-〉明明加了索引却不走索引？

只有你对各种数据结构和索引原理都懂，你才能扯的起来，对于事物和锁也是，当时面试官问了我事务是如何保证一致性的，刚好我研究过 ，redolog，binlog，undolog 这些日志，然后和面试官扯了好久。

书籍：《MySQL必知必会》和《MySQL技术内幕》

### 5.网络编程

网络编程这块，有些公司还是问的挺多的，特别是 IO 多路复用，同步非同步 IO，阻塞非阻塞啥的，当时面腾讯基本每次都问，，，，学习 C++ 这块还是要重视一下，下面我说一下比较重要的吧。

1、IO多路复用：select、poll、epoll的区别（非常重要，几乎必问，回答得越底层越好，要会使用）
2、手撕一个最简单的server端服务器（socket、bind、listen、accept这四个API一定要非常熟练）
3、线程池
4、基于事件驱动的reactor模式
5、边沿触发与水平触发的区别
6、非阻塞IO与阻塞IO区别

书籍：可以看一看《Unix网络编程》

### 6.数据结构与算法

数据结构与算法，我觉得是需要花最多时间的，因为算法这块，很难快速突击，从基础数据结构与各种算法思想到 leetcode 刷题，如果你零基础，那真的需要挺久的，不过你有一些基础，可能会快一点，看你想掌握到什么程度了。

我这里大致说一下学习流程吧

1、先跟着书学**基础数据结构与算法**：链表，队列，栈，哈希表，二叉树，图，十大排序，二分查找。

2、之后了解一下算法思想：递归，深度与广度搜索，枚举，动态规划这些。

入门数据结构推荐《数据结构与算法分析：c语言描述版》这本书，学的过程中，也可以配合刷题，一般刷《剑指 offer》 + LeetCode 刷个两三百就差不多了，没时间到就先刷 《剑指 offer》吧。

### 7.项目

项目是必须要做的了，Java 的项目教程满天飞，不过 C++ 的会少一些，不过大家可以跟着书，或者 github 上找或者自己花点钱买一个付费视频吧。

推荐自学项目：实现 http服务器（ github 一堆源码、音视频服务器）、实现一个聊天系统(这块有些书就有附带)

### 8.学习顺序

我建议有时间的，可以先入门下 C++ ，然后就是开始学习数据结构与算法，算法这块长期保持刷题，然后一边深入学习 C++，之后学习计算机网络，操作系统，在之后学习网络编程，项目这块放到最后面。

如果时间比较紧的，算法这块可以放松一点，C++ 和项目可以优先，计算机基础可以突击学习，通过视频或者别人总结的笔记突击。

总之，这一套学下来，感觉需要大概8个月的时间，当然，这个不好衡量，还得看你自己掌握了哪些基础。

### 9.总结

学了之后要验证自己学得如何，可以来小编的网站看看这些面试题，通过面试题查漏补缺

小破站网址：后续会越来越完善，包括各种算法也都会更新，建议大家收藏。

总之，关于校招，学习路线，面试题等等，很多我在网站都更新了，包括个人经历，大家迷茫没事做时，可以多打开看看。

最后，大家加油，努力学两年，争取日后那个好的 offer.

原文链接：[https://juejin.cn/post/69972481](https://link.zhihu.com/?target=https%3A//juejin.cn/post/6997248187413037070)

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.297】什么是DPDK？DPDK的原理及学习路线总结

## 1.什么是DPDK

　　对于用户来说，它可能是一个性能出色的包数据处 理加速软件库；对于开发者来说，它可能是一个实践包处理新想法的创 新工场；对于性能调优者来说，它可能又是一个绝佳的成果分享平台。　

　　DPDK用软件的方式在通用多核处理器上演绎着数据包处理的新篇 章，而对于数据包处理，多核处理器显然不是唯一的平台。支撑包处理 的主流硬件平台大致可分为三个方向。
　　·硬件加速器
　　·网络处理器
　　·多核处理器

　　在类似 IA（Intel Architecture）多核处理器为目标的平台上，网络数据包处理远早于DPDK而存在。从商业版的 Windows到开源的Linux操作系统，所有跨主机通信几乎都会涉及网络 协议栈以及底层网卡驱动对于数据包的处理。然而，低速网络与高速网 络处理对系统的要求完全不一样。

## 2.DPDK原理

网络设备（路由器、交换机、媒体网关、SBC、PS网关等）需要在瞬间进行大量的报文收发，因此在传统的网络设备上，往往能够看到专门的NP（Network Process）处理器，有的用FPGA，有的用ASIC。这些专用器件通过内置的硬件电路（或通过编程形成的硬件电路）高效转发报文，只有需要对报文进行深度处理的时候才需要CPU干涉。

但在公有云、NFV等应用场景下，基础设施以CPU为运算核心，往往不具备专用的NP处理器，操作系统也以通用Linux为主，网络数据包的收发处理路径如下图所示：

![img](https://pic4.zhimg.com/80/v2-39d9c5742815718ffc3e616342c75c9b_720w.webp)

在虚拟化环境中，路径则会更长：

![img](https://pic3.zhimg.com/80/v2-ee0175b746bbf5eed76de9f29bb4bbda_720w.webp)

由于包处理任务存在内核态与用户态的切换，以及多次的内存拷贝，系统消耗变大，以CPU为核心的系统存在很大的处理瓶颈。为了提升在通用服务器（COTS）的数据包处理效能，Intel推出了服务于IA（Intel Architecture）系统的DPDK技术。

DPDK是Data Plane Development Kit的缩写。简单说，DPDK应用程序运行在操作系统的User Space，利用自身提供的数据面库进行收发包处理，绕过了Linux内核态协议栈，以提升报文处理效率。

## 3.DPDK源码目录结构　

　　lib/ : DPDK的库源代码
　　drivers/ : DPDK轮询模式驱动程序源代码
　　app/ : DPDK应用程序源代码
　　examples/ : DPDK的一些应用程序例子源代码
　　config/ : DPDK关于arm和x86平台的一些编译配置
　　buildtools/ : DPDK一些编译配置的脚本
　　mk/ : DPDK的Makefile
　　usertools/ : DPDK提供给用户的一些实用工具

## 4.常用术语及缩写

　　ACL：Access Control List，访问控制列表，是路由器和交换机接口的指令列表，用来控制端口进出的数据包；简而言之就是用来控制数据流。
　　SSL：Secure Sockets Layer，安全套接层，是为网络通信提供安全及数据完整性的一种安全协议，在传输层对网络连接进行加密。
　　RSS：Receive Side Scaling，是一种能够在多处理器系统下使接收报文在多个CPU之间高效分发的网卡驱动技术。
　　NUMA：Non Uniform Memory Access Architecture，非统一内存访问架构；
　　QOS：Quality of Service，服务质量，指一个网络能够利用各种基础技术，为指定的网络通信提供更好的服务能力, 是网络的一种安全机制， 是用来解决网络延迟和阻塞等问题的一种技术。
　　NIC：Network Interface Card，网卡，网卡是局域网中最基本的部件之一，它是连接计算机与网络的硬件设备。
　　PCI：Peripheral Component Interconnect，计算机一种标准总线，NIC就是使用的这种总线方式。
　　PMD：Poll Mode Drive，轮询模式驱动，DPDK就是采用的这种模式。
　　RTE：Run Time Environment，通过PMD实现快速分组处理数据的一个框架。
　　MPLS：Multi-Protocol Label Switching，多协议标签交换，是一种用于快速数据包交换和路由的体系，它为网络数据流量提供了目标、路由地址、转发和交换等能力。更特殊的是，它具有管理各种不同形式通信流的机制。

## 5.DPDK框架简介

　　DPDK为IA上的高速包处理而设计。

　　图1-6所示的DPDK主要模块分 解展示了以基础软件库的形式，为上层应用的开发提供一个高性能的基 础I/O开发包。它大量利用了有助于包处理的软硬件特性，如大页、缓 存行对齐、线程绑定、预取、NUMA、IA最新指令的利用、Intel DDIO、内存交叉访问等。
　　核心库Core Libs，提供系统抽象、大页内存、缓存池、定时器及无 锁环等基础组件。
　　PMD库，提供全用户态的驱动，以便通过轮询和线程绑定得到极高 的网络吞吐，支持各种本地和虚拟的网卡。
　　Classify库，支持精确匹配（Exact Match）、最长匹配（LPM）和 通配符匹配（ACL），提供常用包处理的查表操作。
　　QoS库，提供网络服务质量相关组件，如限速（Meter）和调度 （Sched）。

![img](https://pic1.zhimg.com/80/v2-617431c4136778d6ccc9462016a76f10_720w.webp)

## 6.DPDK的轮询模式

　　DPDK采用了轮询或者轮询混杂中断的模式来进行收包和发包，此 前主流运行在操作系统内核态的网卡驱动程序基本都是基于异步中断处 理模式。

## 　　1、异步中断模式

　　当有包进入网卡收包队列后，网卡会产生硬件 （MSIX/MSI/INTX）中断，进而触发CPU中断，进入中断服务程序，在 中断服务程序（包含下半部）来完成收包的处理。当然为了改善包处理 性能，也可以在中断处理过程中加入轮询，来避免过多的中断响应次 数。总体而言，基于异步中断信号模式的收包，是不断地在做中断处 理，上下文切换，每次处理这种开销是固定的，累加带来的负荷显而易 见。在CPU比I/O速率高很多时，这个负荷可以被相对忽略，问题不 大，但如果连接的是高速网卡且I/O频繁，大量数据进出系统，开销累 加就被充分放大。中断是异步方式，因此CPU无需阻塞等待，有效利用 率较高，特别是在收包吞吐率比较低或者没有包进入收包队列的时候， CPU可以用于其他任务处理。
当有包需要发送出去的时候，基于异步中断信号的驱动程序会准备 好要发送的包，配置好发送队列的各个描述符。在包被真正发送完成 时，网卡同样会产生硬件中断信号，进而触发CPU中断，进入中断服务 程序，来完成发包后的处理，例如释放缓存等。与收包一样，发送过程 也会包含不断地做中断处理，上下文切换，每次中断都带来CPU开销； 同上，CPU有效利用率高，特别是在发包吞吐率比较低或者完全没有发 包的情况。

## 　　2、轮询模式

　　DPDK起初的纯轮询模式是指收发包完全不使用任何中断，集中所 有运算资源用于报文处理。但这不是意味着DPDK不可以支持任何中 断。根据应用场景需要，中断可以被支持，最典型的就是链路层状态发 生变化的中断触发与处理。

原文链接：https://zhuanlan.zhihu.com/p/397919872

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.298】腾讯同事内推的那位Linux C/C++后端开发同学面试没过......

最近同事内推了一位 Linux C/C++ 后端开发的同学到我们公司面试，很遗憾这位工作了两年的同学面试表现不是很好。我问了如下一些问题：

> “redis持久化机制，redis销毁方式机制，mq实现原理，c++虚函数，hash冲突的解决，memcached一致性哈希，socket函数select的缺陷，epoll模型，同步互斥，异步非阻塞，回调的概念，innodb索引原理，单向图最短路径，动态规划算法。”

为了帮助更多的同学拿到满意的 offer，我整理了一下发出来，那么 Linux C/C++ 岗位一般会问哪些知识点呢？

## 1.思路分析

除了关于 c++ 虚函数这个问题以外，其他的大多数问题都与哪种编程语言关系不大，大多数是原理性和基础性的问题，少数是工作经验问题，我试着给大家分析分析。

## 2.语言基础

C++ 虚函数这是面试初、中级 C ++ 职位一个概率95%以上的面试题。一般有以下几种问法：

1. 在有继承关系的父子类中，构建和析构一个子类对象时，父子构造函数和析构函数的执行顺序分别是怎样的？
2. 在有继承关系的类体系中，父类的构造函数和析构函数一定要申明为 virtual 吗？如果不申明为 virtual 会怎样？
3. 什么是 C++ 多态？C++ 多态的实现原理是什么？
4. 什么是虚函数？虚函数的实现原理是什么？
5. 什么是虚表？虚表的内存结构布局如何？虚表的第一项（或第二项）是什么？
6. 菱形继承（类D同时继承B和C，B和C又继承自A）体系下，虚表在各个类中的布局如何？如果类B和类C同时有一个成员变了m，m如何在D对象的内存地址上分布的？是否会相互覆盖？

另外，时至今日，你一定要熟悉 C++11/14/17 常用的语言特性和类库，这里简单地列一下：

- 统一的类成员初始化语法与 std::initializer_list
- 注解标签（attributes）
- final/override/=default/=delete 语法
- auto 关键字
- Range-based 循环语法
- 结构化绑定
- stl 容器新增的实用方法
- std::thread
- 线程局部存储 thread_local
- 线程同步原理 std::mutex、std::condition_variable 等
- 原子操作类
- 智能指针类
- std::bind/std::function

C++11/14 网上的资料已经很多了，C++17 的资料不多，重头戏还是 C++11 引入的各种实用特性，这就给读者推荐一本我读过的：

- 《深入理解 C++11：C++11 新特性解析与应用》
- 《深入应用 C++11：代码优化与工程级应用》
- 《C++17 完全指南》
- 《Cpp 17 in Detail》

## 3.算法与数据结构基础

说到算法和数据结构，对于社招人士和对于应届生一般是不一样的，对于大的互联网公司和一般的小的企业也是不一样的。下面根据我当面试官面试别人和找工作被别人面试经验来谈一谈。

先说考察的内容，除了一些特殊的岗位，常见的算法和数据结构面试问题有如下：

### 3.1.排序（常考的排序按频率考排序为：快速排序 > 冒泡排序 > 归并排序 > 桶排序）

一般对于对算法基础有要求的公司，如果你是应届生或者工作经验在一至三年内，以上算法如果写不出来，给面试官的影响会非常不好，甚至直接被 pass 掉。对于工作三年以上的社会人士，如果写不出来，但是能分析出其算法复杂度、最好和最坏的情况下的复杂度，说出算法大致原理，在多数面试官面前也可以过的。注意，如果你是学生，写不出来或者写的不对，基本上面试过不了。

### 3.2.二分查找

二分查找的算法尽量要求写出来。当然，大多数面试官并不会直接问你二分查找，而是结合具体的场景，例如如何求一个数的平方根，这个时候你要能想到是二分查找。我在2017年年底，面试agora时，面试官问了一个问题：如何从所有很多的ip地址中快速找个某个ip地址。

### 3.3.链表

无论是应届生还是工作年限不长的社会人士，琏表常见的操作一定要熟练写出来，如链表的查找、定位、反转、连接等等。还有一些经典的问题也经常被问到，如两个链表如何判断有环（我在2017年面试饿了么二面、上海黄金交易所一面被问过）。链表的问题一般不难，但是链表的问题存在非常多的“坑”，如很多人不注意边界检查、空链表、返回一个链表的函数应该返回链表的头指针等等。

### 3.4.队列与栈

对于应届生来说一般这一类问的比较少，但是对于社会人士尤其是中高级岗位开发，会结合相关的问题问的比较多，例如让面试者利用队列写一个多线程下的生产者和消费者程序，全面考察的多线程的资源同步与竞态问题（下文介绍多线程面试题时详细地介绍）。
栈一般对于基础要求高的面试，会结合函数调用实现来问。即函数如何实现的，包括函数的调用的几种常见调用方式、参数的入栈顺序、内存栈在地址从高向低扩展、栈帧指针和栈顶指针的位置、函数内局部变量在栈中的内存分布、函数调用结束后，调用者和被调用者谁和如何清理栈等等。某年面试京东一基础部门，面试官让写从0加到100这样一个求和算法，然后写其汇编代码。

### 3.5.哈希表

哈希表是考察最多的数据结构之一。常见的问题有哈希冲突的检测、让面试者写一个哈希插入函数等等。基本上一场面试下来不考察红黑树基本上就会问哈希表，而且问题可浅可深。我印象比较深刻的是，当年面试百度广告推荐部门时，二面问的一些关于哈希表的问题。当时面试官时先问的链表，接着问的哈希冲突的解决方案，后来让写一个哈希插入算法，这里需要注意的是，你的算法中插入的元素一定要是通用元素，所以对于 C++ 或者 Java 语言，一定要使用模板这一类参数作为哈希插入算法的对象。然后，就是哈希表中多个元素冲突时，某个位置的元素使用链表往后穿成一串的方案。最终考察 linux 下 malloc（下面的ptmalloc） 函数在频繁调用造成的内存碎片问题，以及开源方案解决方案 tcmalloc 和 jemalloc。总体下来，面试官是一步步引导你深入。（有兴趣的读者可以自行搜索，网上有很多相关资料）

### 3.6.树

面试高频的树是红黑树，也有一部分是B树（B+树）。
红黑树一般的问的深浅不一，大多数面试官只要能说出红黑树的概念、左旋右旋的方式、分析出查找和插入的平均算法复杂度和最好最坏时的算法复杂度，并不要写面试者写出具体代码实现。一般 C++ 面试问 stl 的map，java 面试问 TreeMap 基本上就等于开始问你红黑树了，要有心里准备。笔者曾经面试爱奇艺被问过红黑树。
B树一般不会直接问，问的最多的形式是通过问 MySQL 索引实现原理（数据库知识点将在下文中讨论）。笔者面试腾讯看点部门二面被问到过。

### 3.7.图

图的问题就我个人面试从来没遇到过，不过据我某位哥哥所说，他在进三星电子之前有一道面试题就是深度优先和广度优先问题。

### 3.8.其他的一些算法

如A*寻路、霍夫曼编码也偶尔会在某一个领域的公司的面试中被问到，如宝开（《植物大战僵尸》的母公司， 在上海人民广场附近有分公司）。

## 4.编码基本功

还有一类面试题不好分类，笔者姑且将其当作是考察编码基本功，这类问题既可以考察算法也可以考察你写代码基本素养，这些素养不仅包括编码风格、计算机英语水平、调试能力等，还包括你对细节的掌握和易错点理解，如有意识地对边界条件的检查和非法值的过滤。请读者看以下的代码执行结果是什么？

```
for(char i = 0; i < 256; ++i) {   printf("%d\n", i);}
```

Copy

下面再列举几个常见的编码题：

1. 实现一个 memmov 函数
   这个题目考查点在于 memmov 函数与 memcpy 函数的区别，这两者对于源地址与目标地址内存有重叠的这一情况的处理方式是不一样的。

2. 实现strcpy或strcpy函数
   这个函数写出来没啥难度，但是除了边界条件需要检查以外，还有一个容易被忽视的地方即其返回值一定要是目标内存地址，以支持所谓的链式拷贝。即：
   strcpy(dest3, strcpy(dest2, strcpy(dest1, src1)));

3. 实现atoi函数
   这个函数的签名如下：
   int atoi(const char* p);

   容易疏忽的地方有如下几点：

- 小数点问题，如数字0.123和.123都是合法的；
- 正负号问题，如+123和-123；
- 考虑如何识别第一个非法字符问题，如123Z89，则应转换成应该123。

## 5.多线程开发基础

现如今的多核CPU早已经是司空见惯，而多线程编程早已经是“飞入寻常百姓家”。对于大多数桌面应用（与 Web 开发相对），尤其是像后台开发这样的岗位，且面试者是社会人员（有一定的工作经验），如果面试者不熟悉多线程编程，那么一般会被直接 pass 掉。

这里说的“熟悉多线程编程”到底熟悉到什么程度呢？一般包括：知道何种场合下需要新建新的线程、线程如何创建和等待、线程与进程的关系、线程局部存储（TLS 或者叫 thread local）、多线程访问资源产生静态的原因和解决方案等等、熟练使用所在操作系统平台提供的线程同步的各种原语。

对于 C++ 开发者，你需要：

- 对于 Windows 开发者，你需要熟练使用 Interlock系列函数、CriticalSection、Event、Mutex、Semphore等API 函数和两个重要的函数 WaitForSingleObject、WaitForMultipleObjects。
- 对于linux 开发者，你需要熟练使用 mutex、semphore、condition_variable、read-write-lock 等操作系统API。

对于 Java，你需要熟悉使用 synchronized关键字、CountDownLatch、CyclicBarrier、Semaphore以及java.util.concurrent 等包中的大多数线程同步对象。

## 6.数据库

数据库知识一般在大的互联网企业对应届生不做硬性要求，对于小的互联网企业或社会人士一般有一定的要求。其要求一般包括：

1. 熟悉基本 SQL 操作
   包括增删改查（insert、delete、update、select语句），排序 order，条件查询（where 子语句），限制查询结果数量（LIMIT语句）等
2. 稍微高级一点的 SQL 操作（如Group by，in，join，left join，多表联合查询，别名的使用，select 子语句等）
3. 索引的概念、索引的原理、索引的创建技巧
4. 数据库本身的操作，建库建表，数据的导入导出
5. 数据库用户权限控制（权限机制）
6. MySQL的两种数据库引擎的区别
7. SQL 优化技巧

以上属于对开发的基本的数据库知识要求，你可以找一本相关入门级的数据库图书学习即可。

高级开发除了以上要求还要熟悉高可用 MySQL、主从同步、读写分离、分表分库等技术，这些技术的细节一定要清楚，它们是你成为技术专家或者高级架构的必备知识。我们在实际面试时，在讨论高可用服务服务方案时，很多面试者也会和我们讨论到这些技术，但是不少面试者只知道这些技术的大致思想，细节往往说不清楚，细节不会就意味着你的高可用方案无法落地，企业需要可以落地的方案。

这些技术我首推《高性能 MySQL》这本书，这本书高级开发者一定要通读的，另外还有 2 本非常好的图书也推荐一下：一本是《MySQL 排错指南》，读完这本书以后，你会对整个“数据库世界”充满了清晰的认识；另外一本是《数据库索引设计与优化》，这本书读起来非常舒服，尤其是对于喜欢算法和数据结构的同学来说。

## 7.网络编程

网络编程这一块，对于应届生或者初级岗位一般只会问一些基础网络通信原理（如三次握手和四次挥手）的socket 基础 API 的使用，客户端与服务器端网络通信的流程（回答 【客户端创建socket -> 连接server ->收发数据；服务器端创建socket -> 绑定ip和端口号 -> 启动侦听 ->接受客户端连接 ->与客户端通信收发数据】即可）、TCP 与 UDP的区别等等。

对于工作经验三年以内的社会人士或者一些中级面试者一般会问一些稍微重难点问题，如 select 函数的用法，非阻塞 connect 函数的写法，epoll 的水平和边缘模式、阻塞socket与非阻塞socket的区别、send/recv函数的返回值情形、reuse_addr选项等等。Windows 平台可能还会问 WSAEventSelect 和 WSAAsyncSelect 函数的用法、完成端口（IOCP模型）。

对于三年以上尤其是“号称”自己设计过服务器、看过开源网络通信库代码的面试者，面试官一般会深入问一些问题，这类问题要么是实际项目中常见的难题或者网络通信细节，根据我的经验，一般有这样一些问题：

1. nagle算法；
2. keepalive选项；
3. Linger选项；
4. 对于某一端出现大量CLOSE_WAIT 或者 TIME_WAIT如何解决；
5. 通讯协议如何设计或如何解决数据包的粘包与分片问题；
6. 心跳机制如何设计；（可能不会直接问问题本身，如问如何检查死链）
7. 断线重连机制如何设计；
8. 对 IO Multiplexing 技术的理解；
9. 收发数据包正确的方式，收发缓冲区如何设计；
10. 优雅关闭；
11. 定时器如何设计；
12. epoll 的实现原理。

举个例子，让读者感受一下，笔者曾去BiliBili被问过这样一个问题：如果A机器与B机器网络 connect 成功后从未互发过数据，此时其中一机器突然断电，则另外一台机器与断电的机器之间的网络连接处于哪种状态？

不知道读者是否能答出来。

网络编程对于已经工作了的或者时间不是很充裕的同学来说，如果想入门或者上手，不建议去读一些大部头的图书，容易坚持不下，最后放弃。

建议找一些通俗易懂又可快速实践的书，这里推荐韩国人尹圣雨写的《TCP/IP 网络编程》这本书，这本书尤其适合非科班出身或者网络编程小白的同学，常见的 socket API 以及网络通信模式都有介绍，且同时包括 Linux 和 Windows 两个操作系统平台。

我刚工作那会儿，做股票行情服务器的底层服务开发，需要熟悉网络编程，那会儿天天下班抱着这本书看，建议小白把书中的网络通信代码都自己敲一遍。

我们面试一些同学时，发现很多同学写的网络通信程序在本机测试没问题，一拿到局域网或者测试环境就不能正常工作，这本书会告诉你答案。

## 8.内存数据库技术

时下以NoSql key-value为思想的内存数据库大行其道，广泛地用于各种后台项目开发。所以熟悉一种或几种内存数据库程序已经是面试后台开发的基本要求，而这当中以 redis 和 memcached 为最典型代表，这里以 redis 为例。

- 第一层面一般是对 redis 的基础用法的考察
  如考察 redis 支持的基础数据类型、redis的数据持久化、事务等。
- 第二层面不仅考察 redis 的基础用法，还会深入到 redis 源码层面上，如 redis 的网络通信模型、redis 各种数据结构的实现等等。
- redis高可用、cluster、哨兵策略等。

笔者以为，无论是从找工作应付面试还是从提高技术的角度，redis 是一个非常值得学习的开源软件，希望广大读者有意识地去了解、学习它。

## 9.项目经验

除了社会招聘和一些小型的企业，一般的大型互联网公司对应届生不会做过多的项目经验要求，而是希望他们算法与数据结构等基础扎实、动手实践能力强即可。对于一般的小公司，对于应届生会要求其至少熟练使用一门编程语言以及相应的开发工具，号称熟悉linux C++ 开发的面试者，不熟悉 GDB 调试基本上不是真正的熟悉 linux C++ 开发；号称熟悉汇编或者反汇编，不熟悉 IDA 或者 OllyDbg，基本上也是名不符实的；号称熟悉 VC++ 开发，连F8、F9、F10、F11、F12等快捷键不熟悉也是难以经得住面试官的提问的；号称熟悉 Java 开发的却对 IDEA 或 eclipse 陌生，这也是说不过去的。

这里给一些学历不算好，学校不是非常有名，尤其是二本以下的广大想进入 IT 行业的同学一个建议，在大学期间除了要学好计算机专业基础知识以外，一定要熟练使用一门编程语言以及相应的开发工具。

关于项目经验，许多面试者认为一定要是自己参与的项目，其实也可以来源于你学习和阅读他人源码或开源软件的源码，如果你能理解并掌握这些开源软件中的思想和技术，在面试的时候能够与面试官侃侃而谈，面试官也会非常满意的。

很多同学可能纠结大学或者研究生期间要不要跟着导师做一些项目。当然，如果这些项目是课程要求，那么你必须得参加；如果这些项目是可以选择性的，尤其是一些仅仅拿着第三方的库进行所谓的包装和加工，那么建议可以少参加一些。

## 10.思路总结

不知道通过我上面的技术分析，聪明的读者是否已经明确本文开头“成都-go-戒炸鸡”同学提出的面试题中，哪些是技术面试重难点，哪些又是技术开发的重难点呢？

## 11.技术比重与薪资

这里根据我自己招人的经验来谈一谈技术水平与薪资，就上面的面试题来看：

- 第一层次：如果面试者能答出上面面试题中的C++基础问题和算法与数据结构题目（如 C++ 函数与hash冲突的解决、innodb索引原理，单向图最短路径，动态规划算法等），可以认为面试者是一个合格的初、中级开发者，薪资范围一般在6 ～ 12k（注意：这里上海为参考标准）。
- 第二层次：在第一层次基础之上，如果面试者还能答出上述面试题中网络编程相关的或者多线程相关的问题（如socket函数select的缺陷，epoll模型，同步互斥，异步非阻塞，回调的概念等），可以认为面试者是个基础不错的中级开发者，薪资范围一般在14～22k之间。
- 第三层次：在前两个层次之间，如果面试者还能回答出上述问题中关于redis、memcached和mq实现原理，说明面试者是一个有着不错项目经验并且对一些常用开源项目也有一定的理解，薪资可以给到22k +。

## 12.总结

工资收入是每个人的秘密，一般不轻易对外人道也。这里笔者冒天下之大不韪，只想说明一点——对于普通开发人员，提高薪资最好的捷径就是提高自己的技术，无论是“面向搜索引擎编程”还是“面向工资编程”终将得不偿失，聪明的你一定会深谋远虑的。

原文链接：https://zhuanlan.zhihu.com/p/383586929

作者：[Hu先生的Linux](https://www.zhihu.com/people/huhu520-10)

# 【NO.299】从四线城市程序员，到深圳大厂架构师，我只用了半年

从苦逼的程序员，到现在的**Linux高级互联网架构师，**要问身份的转变给我带来了什么实质上的利益，那肯定是**薪水**了。除此之外就是**面子**，毕竟在大厂比在不知名小公司要长脸的多。

主要还是去年在家上班那段时间，感觉到了小公司的种种不便，最让人难以忍受的就是在家996随时待命，还只发底薪，真是令人窒息的操作，让我只想赶紧逃离这个公司。

![img](https://pic1.zhimg.com/80/v2-510e73918b0cd1069f60d0b2f1f7639c_720w.webp)

但成年人的世界从来就不是可以任性的，我也自知我的水平没办法胜任更好的岗位，于是我决定**工作之余提升自己**。

边上班边学习其实挺苦的，幸好时间不长，我也熬了过来。现在每个月看到工资卡上比之前涨了几倍的数字，觉得**当时的努力是很值**的。

咳咳，扯远了啊，言归正传，就跟大家分享一下，我是**怎样进阶**的。

## **1.** **学好C语言**

作为一名程序员，C++的难度在我看来是top级的，多少次被这玩意折腾的怀疑人生。但是！不入虎穴焉得虎子，它的确很难，但是它的**含金量和竞争力**同样也是没话说的。

会与不会，很多时候就是薪资高低的决定因素。

![img](https://pic2.zhimg.com/80/v2-a6a9b09b56496a588782fb33806eb029_720w.webp)

要学习C++，那就一定要先打牢C语言的基础，这是至关重要的前提条件。

## **2.** **看书**

有了扎实的C语言基础之后，可以开始学习C++了。

给大伙推荐一些我觉得不错的书。

▪ **《C++ Primer》 及习题册**

如果只想看一本教材，那我强烈推荐这本。《C++ Primer》 非常全面，方方面面都考虑到了，可读性也很强，很适合初学者。它的习题册也一定要买，搭配使用事半功倍。

![img](https://pic1.zhimg.com/80/v2-4f9573d37434595aef464cd0a4b5627c_720w.webp)

**▪ 《21天学通C++》**

听这个名字就知道，这也是一本适合初学者的书，难度没有上一本那样大，但基本知识都交代了，适合作为学习C++的第一本书。

![img](https://pic3.zhimg.com/80/v2-580d5740d933ea60dda9f01e9320ea4e_720w.webp)

▪ **《Effective C++》、《More Effective C++》**

这两本是初学者看完、练完《C++ Primer》后，用来提升的教材。告诉程序员在使用C++时应该怎么解决问题、要注意什么、避免什么，进阶之路必备好书。

![img](https://pic4.zhimg.com/80/v2-b1faf8d048e7b5c9662049e917f69fb3_720w.webp)

![img](https://pic1.zhimg.com/80/v2-35ccf2c850b39c5d2370a378021783c8_720w.webp)

## **3.** **看教程**

根据我自己的学习经验，其实单纯看书挺枯燥的，很多时候就是看不下去，所以我会**结合一些教程来**看。

![img](https://pic3.zhimg.com/80/v2-c71979939e6b6a7f6d608098bf103062_720w.webp)

也有很多一样在学习C++的同仁，可以**相互交流每天打卡**，有队友学起来才更有激情嘛。而且会在群里聊聊**行情**什么的，也有**项目实操**，是锻炼的好机会。

## **4.** **学习资料**

来点干货，这是群里大牛整理的**腾讯核心技术学习路线（T1-T9）**

![img](https://pic1.zhimg.com/80/v2-806d7ae9d425ebf381e284802f7f1118_720w.webp)

**腾讯职级技术学习提升路线详情版**：

![img](https://pic1.zhimg.com/80/v2-806d7ae9d425ebf381e284802f7f1118_720w.webp)

相比很多人学C++学的怀疑人生，我学习的过程其实**没有走多少弯路**，毕竟一开始就找到了优质的教程和学习资料，而且大牛带飞嘛，结果自然不同凡响。

我是感觉学习任何一门技术都不能闭门造车，因为学习过程中很多问题不是你一个人遇到过，**多和同仁交流**，钻牛角尖的概率低很多。

另外，作为一个过来人，也想提醒大家：想要学习C++，一定要努力且有耐心，不可能一天就能走到罗马，唯一可以做的，就是**立刻出发**。

原文链接：https://zhuanlan.zhihu.com/p/356387701

作者：Linux服务器研究

# 【NO.300】详解 epoll 原理【Redis，Netty，Nginx实现高性能IO的核心原理】

## 1.【Redis，Netty，Nginx 等实现高性能IO的核心原理】

### 1.1.I/O

![img](https://pic1.zhimg.com/80/v2-f2d47755f938a2b5a7d4d2e7c75458a8_720w.webp)
输入输出(input/output)的对象可以是文件(file)， 网络(socket)，进程之间的管道(pipe)。在linux系统中，都用文件描述符(fd)来表示。

### 1.2.I/O 多路复用（multiplexing）

![img](https://pic4.zhimg.com/80/v2-c2417f7394fdc1bef0b34623dee68923_720w.webp)

I/O 多路复用的本质，是通过一种机制（系统内核缓冲I/O数据），让单个进程可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或写就绪），能够通知程序进行相应的读写操作。

select、poll 和 epoll 都是 Linux API 提供的 IO 复用方式。
Linux中提供的epoll相关函数如下：

```
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

## 2.Unix五种IO模型

我们在进行编程开发的时候，经常会涉及到同步，异步，阻塞，非阻塞，IO多路复用等概念，这里简单总结一下。

Unix网络编程中的五种IO模型：

```
Blocking IO - 阻塞IO
NoneBlocking IO - 非阻塞IO
IO multiplexing - IO多路复用
signal driven IO - 信号驱动IO
asynchronous IO - 异步IO
```

对于一个network IO，它会涉及到两个系统对象：

1. Application 调用这个IO的进程
2. kernel 系统内核

那他们经历的两个交互过程是：

阶段1： wait for data 等待数据准备；
阶段2： copy data from kernel to user 将数据从内核拷贝到用户进程中。

之所以会有同步、异步、阻塞和非阻塞这几种说法就是根据程序在这两个阶段的处理方式不同而产生的。

## 3.事件

可读事件，当文件描述符关联的内核读缓冲区可读，则触发可读事件。
(可读：内核缓冲区非空，有数据可以读取)

可写事件，当文件描述符关联的内核写缓冲区可写，则触发可写事件。
(可写：内核缓冲区不满，有空闲空间可以写入）

## 4.通知机制

通知机制，就是当事件发生的时候，则主动通知。通知机制的反面，就是轮询机制。

## 5.epoll 的通俗解释

结合以上三条，epoll的通俗解释是：

> 当文件描述符的内核缓冲区非空的时候，发出可读信号进行通知，当写缓冲区不满的时候，发出可写信号通知的机制。

## 6.epoll 数据结构 + 算法

epoll 的核心数据结构是：1个红黑树和1个链表。还有3个核心API。如下图所示：

![img](https://pic1.zhimg.com/80/v2-6c7a55178a7b2f5454aa36a9ba4a12e0_720w.webp)

## 7.就绪列表的数据结构

就绪列表引用着就绪的socket，所以它应能够快速的插入数据。

程序可能随时调用epoll_ctl添加监视socket，也可能随时删除。当删除时，若该socket已经存放在就绪列表中，它也应该被移除。（事实上，每个epoll_item既是红黑树节点，也是链表节点，删除红黑树节点，自然删除了链表节点）

所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列（对应上图的rdllist）。

## 8.epoll 索引结构

既然epoll将“维护监视队列”和“进程阻塞”分离，也意味着需要有个数据结构来保存监视的socket。至少要方便的添加和移除，还要便于搜索，以避免重复添加。红黑树是一种自平衡二叉查找树，搜索、插入和删除时间复杂度都是O(log(N))，效率较好。epoll 使用了红黑树作为索引结构。

Epoll在linux内核中源码主要为 eventpoll.c 和 eventpoll.h 主要位于fs/eventpoll.c 和 include/linux/eventpool.h, 具体可以参考linux3.16。

下述为部分关键数据结构摘要, 主要介绍epitem 红黑树节点 和eventpoll 关键入口数据结构，维护着链表头节点ready list header和红黑树根节点RB-Tree root。

```
/*
 * Each file descriptor added to the eventpoll interface will
 * have an entry of this type linked to the "rbr" RB tree.
 * Avoid increasing the size of this struct, there can be many thousands
 * of these on a server and we do not want this to take another cache line.
 */
struct epitem {
    union {
        /* RB tree node links this structure to the eventpoll RB tree */
        struct rb_node rbn;
        /* Used to free the struct epitem */
        struct rcu_head rcu;
    };

    /* List header used to link this structure to the eventpoll ready list */
    struct list_head rdllink;

    /*
     * Works together "struct eventpoll"->ovflist in keeping the
     * single linked chain of items.
     */
    struct epitem *next;

    /* The file descriptor information this item refers to */
    struct epoll_filefd ffd;

    /* Number of active wait queue attached to poll operations */
    int nwait;

    /* List containing poll wait queues */
    struct list_head pwqlist;

    /* The "container" of this item */
    struct eventpoll *ep;

    /* List header used to link this item to the "struct file" items list */
    struct list_head fllink;

    /* wakeup_source used when EPOLLWAKEUP is set */
    struct wakeup_source __rcu *ws;

    /* The structure that describe the interested events and the source fd */
    struct epoll_event event;
};

/*
 * This structure is stored inside the "private_data" member of the file
 * structure and represents the main data structure for the eventpoll
 * interface.
 */
struct eventpoll {
    /* Protect the access to this structure */
    spinlock_t lock;

    /*
     * This mutex is used to ensure that files are not removed
     * while epoll is using them. This is held during the event
     * collection loop, the file cleanup path, the epoll file exit
     * code and the ctl operations.
     */
    struct mutex mtx;

    /* Wait queue used by sys_epoll_wait() */
    wait_queue_head_t wq;

    /* Wait queue used by file->poll() */
    wait_queue_head_t poll_wait;

    /* List of ready file descriptors */
    struct list_head rdllist;

    /* RB tree root used to store monitored fd structs */
    struct rb_root rbr;

    /*
     * This is a single linked list that chains all the "struct epitem" that
     * happened while transferring ready events to userspace w/out
     * holding ->lock.
     */
    struct epitem *ovflist;

    /* wakeup_source used when ep_scan_ready_list is running */
    struct wakeup_source *ws;

    /* The user that created the eventpoll descriptor */
    struct user_struct *user;

    struct file *file;

    /* used to optimize loop detection check */
    int visited;
    struct list_head visited_list_link;
};
```

epoll使用RB-Tree红黑树去监听并维护所有文件描述符，RB-Tree的根节点。

调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个 红黑树 用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件.

当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已.

## 9.epoll API

### 9.1. int epoll_create(int size)

功能：

内核会产生一个epoll 实例数据结构并返回一个文件描述符，这个特殊的描述符就是epoll实例的句柄，后面的两个接口都以它为中心（即epfd形参）。size参数表示所要监视文件描述符的最大值，不过在后来的Linux版本中已经被弃用（同时，size不要传0，会报invalid argument错误）

### 9.2. int epoll_ctl(int epfd， int op， int fd， struct epoll_event *event)

功能：

将被监听的描述符添加到红黑树或从红黑树中删除或者对监听事件进行修改

```
typedef union epoll_data {
void *ptr; /* 指向用户自定义数据 */
int fd; /* 注册的文件描述符 */
uint32_t u32; /* 32-bit integer */
uint64_t u64; /* 64-bit integer */
} epoll_data_t;
struct epoll_event {
uint32_t events; /* 描述epoll事件 */
epoll_data_t data; /* 见上面的结构体 */
};
```

对于需要监视的文件描述符集合，epoll_ctl对红黑树进行管理，红黑树中每个成员由描述符值和所要监控的文件描述符指向的文件表项的引用等组成。

op参数说明操作类型：

EPOLL_CTL_ADD：向interest list添加一个需要监视的描述符EPOLL_CTL_DEL：从interest list中删除一个描述符EPOLL_CTL_MOD：修改interest list中一个描述符struct epoll_event结构描述一个文件描述符的epoll行为。在使用epoll_wait函数返回处于ready状态的描述符列表时，

data域是唯一能给出描述符信息的字段，所以在调用epoll_ctl加入一个需要监测的描述符时，一定要在此域写入描述符相关信息events域是bit mask，描述一组epoll事件，在epoll_ctl调用中解释为：描述符所期望的epoll事件，可多选。常用的epoll事件描述如下：

EPOLLIN：描述符处于可读状态EPOLLOUT：描述符处于可写状态EPOLLET：将epoll event通知模式设置成edge triggeredEPOLLONESHOT：第一次进行通知，之后不再监测EPOLLHUP：本端描述符产生一个挂断事件，默认监测事件EPOLLRDHUP：对端描述符产生一个挂断事件EPOLLPRI：由带外数据触发EPOLLERR：描述符产生错误时触发，默认检测事件

### 9.3. int epoll_wait(int epfd， struct epoll_event *events， int maxevents， int timeout);

功能：

阻塞等待注册的事件发生，返回事件的数目，并将触发的事件写入events数组中。

events: 用来记录被触发的events，其大小应该和maxevents一致

maxevents: 返回的events的最大个数处于ready状态的那些文件描述符会被复制进ready list中，epoll_wait用于向用户进程返回ready list。

events和maxevents两个参数描述一个由用户分配的struct epoll event数组，调用返回时，内核将ready list复制到这个数组中，并将实际复制的个数作为返回值。

注意，如果ready list比maxevents长，则只能复制前maxevents个成员；反之，则能够完全复制ready list。

另外，struct epoll event结构中的events域在这里的解释是：

> 在被监测的文件描述符上实际发生的事件。

参数timeout描述在函数调用中阻塞时间上限，单位是ms：

timeout = -1表示调用将一直阻塞，直到有文件描述符进入ready状态或者捕获到信号才返回；timeout = 0用于非阻塞检测是否有描述符处于ready状态，不管结果怎么样，调用都立即返回；timeout > 0表示调用将最多持续timeout时间，如果期间有检测对象变为ready状态或者捕获到信号则返回，否则直到超时。epoll的两种触发方式

epoll监控多个文件描述符的I/O事件。epoll支持边缘触发(edge trigger，ET)或水平触发（level trigger，LT)，通过epoll_wait等待I/O事件，如果当前没有可用的事件则阻塞调用线程。

select和poll只支持LT工作模式，epoll的默认的工作模式是LT模式。

## 10.epoll更高效的原因

select和poll的动作基本一致，只是poll采用链表来进行文件描述符的存储，而select采用fd标注位来存放，所以select会受到最大连接数的限制，而poll不会。

select、poll、epoll虽然都会返回就绪的文件描述符数量, 但是select和poll并不会明确指出是哪些文件描述符就绪，而epoll会。

造成的区别就是，系统调用返回后，调用select和poll的程序需要遍历监听的整个文件描述符找到是谁处于就绪，而epoll则直接处理即可（直接监听到了哪些文件描述符就绪）。

select、poll都需要将有关文件描述符的数据结构拷贝进内核，最后再拷贝出来。**而epoll创建的有关文件描述符的数据结构本身就存于内核态中，系统调用返回时利用 mmap() 文件映射内存加速与内核空间的消息传递：即 epoll 使用 mmap() 减少复制开销。**

select、poll采用 **轮询** 的方式来检查文件描述符是否处于就绪态，而epoll采用回调机制。造成的结果就是，随着fd的增加，select和poll的效率会线性降低，而epoll不会受到太大影响，除非活跃的socket很多。

epoll的边缘触发模式效率高，系统不会充斥大量不关心的就绪文件描述符。

虽然epoll的性能最好，但是在**连接数少并且连接都十分活跃**的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调。

## 11.epoll与select、poll的对比

### 11.1. 用户态将文件描述符传入内核的方式

select：创建3个文件描述符集并拷贝到内核中，分别监听读、写、异常动作。这里受到单个进程可以打开的fd数量限制，默认是1024。

poll：将传入的struct pollfd结构体数组拷贝到内核中进行监听。

epoll：执行epoll_create，会在**内核**的高速cache区中，建立一颗红黑树以及就绪链表(该链表存储已经就绪的文件描述符)。接着用户执行的epoll_ctl函数，添加文件描述符会在红黑树上增加相应的结点。

### 11.2. 内核态检测文件描述符读写状态的方式

select：采用**轮询**方式，遍历所有fd，最后返回一个描述符读写操作是否就绪的mask掩码，根据这个掩码给fd_set赋值。

poll：同样采用**轮询**方式，查询每个fd的状态，如果就绪则在等待队列中加入一项并继续遍历。

epoll：采用**事件回调**机制。在执行 epoll_ctl 的add操作时，不仅将文件描述符放到红黑树上，而且也注册了回调函数;内核在检测到某文件描述符可读/可写时会调用回调函数，该回调函数将文件描述符放在就绪链表中。

### 11.3. 找到就绪的文件描述符并传递给用户态的方式

select：将之前传入的fd_set拷贝传出到**用户态**并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。

poll：将之前传入的 fd 数组拷贝传出**用户态**，并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。

epoll：epoll_wait 只用观察**就绪链表**中有无数据即可，最后将链表的数据返回给数组, 并返回就绪的数量。**内核**，将就绪的文件描述符，放在传入的数组中。然后，依次遍历，处理即可。这里返回的文件描述符，是通过 mmap() ，让内核和用户空间，共享同一块内存实现传递的，减少了不必要的拷贝。

### 11.4. 重复监听的处理方式

select：将新的监听文件描述符集合拷贝传入内核中，继续以上步骤。
poll：将新的struct pollfd结构体数组拷贝传入内核中，继续以上步骤。
epoll：无需重新构建红黑树，直接沿用已存在的即可。

## 12.epoll 水平触发与边缘触发

epoll事件有两种模型：

边沿触发：edge-triggered (ET)
水平触发：level-triggered (LT)

## 13.水平触发(level-triggered)

socket接收缓冲区不为空， 有数据可读， 读事件一直触发。
socket发送缓冲区不满， 可以继续写入数据， 写事件一直触发。

## 14.边沿触发(edge-triggered)

socket的接收缓冲区状态变化时，触发读事件，即空的接收缓冲区刚接收到数据时触发读事件。
socket的发送缓冲区状态变化时，触发写事件，即满的缓冲区刚空出空间时触发读事件。

边沿触发仅触发一次，水平触发会一直触发。

## 15.事件宏

EPOLLIN ： 表示对应的文件描述符可以读（包括对端SOCKET正常关闭）；
EPOLLOUT： 表示对应的文件描述符可以写；
EPOLLPRI： 表示对应的文件描述符有紧急的数据可读（这里应该表示有带外数据到来）；
EPOLLERR： 表示对应的文件描述符发生错误；
EPOLLHUP： 表示对应的文件描述符被挂断；
EPOLLET： 将 EPOLL设为边缘触发(Edge Triggered)模式（默认为水平触发），这是相对于水平触发(Level Triggered)来说的。
EPOLLONESHOT： 只监听一次事件，当监听完这次事件之后，如果还需要继续监听这个socket的话，需要再次把这个socket加入到EPOLL队列里
libevent 采用水平触发， nginx 采用边沿触发

JDK并没有实现边缘触发，Netty重新实现了epoll机制，采用边缘触发方式；另外像Nginx也采用边缘触发。

JDK在Linux已经默认使用epoll方式，但是JDK的epoll采用的是水平触发，而Netty重新实现了epoll机制，采用边缘触发方式，netty epoll transport 暴露了更多的nio没有的配置参数，如 TCP_CORK, SO_REUSEADDR等等；另外像Nginx也采用边缘触发。

## 16.mmap() 文件映射内存

mmap是一种内存映射文件（文件映射内存，建立映射关系）的方法，即将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。

实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上（参考：Linux文件读写与缓存），即完成了对文件的操作而不必再调用read,write等系统调用函数。

> Dirty Page： 页缓存对应文件中的一块区域，如果页缓存和对应的文件区域内容不一致，则该页缓存叫做脏页（Dirty Page）。对页缓存进行修改或者新建页缓存，只要没有刷磁盘，都会产生脏页。Linux支持以下5种缓冲区类型：
> Clean 未使用、新创建的缓冲区
> Locked 被锁住、等待被回写
> Dirty 包含最新的有效数据，但还没有被回写
> Shared 共享的缓冲区
> Unshared 原来被共享但现在不共享

相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。如下图所示：

![img](https://pic4.zhimg.com/80/v2-88d705451c9b9f6d21102813786bc8ef_720w.webp)

由上图可以看出，进程的虚拟地址空间，由多个虚拟内存区域构成。虚拟内存区域是进程的虚拟地址空间中的一个同质区间，即具有同样特性的连续地址范围。上图中所示的text数据段（代码段）、初始数据段、BSS数据段、堆、栈和内存映射，都是一个独立的虚拟内存区域。而为内存映射服务的地址空间处在堆栈之间的空余部分。

linux内核使用vm_area_struct结构来表示一个独立的虚拟内存区域，由于每个不同质的虚拟内存区域功能和内部机制都不同，因此一个进程使用多个vm_area_struct结构来分别表示不同类型的虚拟内存区域。各个vm_area_struct结构使用链表或者树形结构链接，方便进程快速访问，如下图所示：

![img](https://pic1.zhimg.com/80/v2-2765fa33d5a4553c76041f0925b8ec9c_720w.webp)

vm_area_struct结构中包含区域起始和终止地址以及其他相关信息，同时也包含一个vm_ops指针，其内部可引出所有针对这个区域可以使用的系统调用函数。这样，进程对某一虚拟内存区域的任何操作需要用要的信息，都可以从vm_area_struct中获得。mmap函数就是要创建一个新的vm_area_struct结构，并将其与文件的物理磁盘地址相连。

## 17.mmap在write和read时会发生什么

### **17.1.write**

- 1.进程(用户态)将需要写入的数据直接copy到对应的mmap地址(内存copy)
- 2.若mmap地址未对应物理内存，则产生缺页异常，由内核处理
- 3.若已对应，则直接copy到对应的物理内存
- 4.由操作系统调用，将脏页回写到磁盘（通常是异步的）

因为物理内存是有限的，mmap在写入数据超过物理内存时，操作系统会进行页置换，根据淘汰算法，将需要淘汰的页置换成所需的新页，所以mmap对应的内存是可以被淘汰的（若内存页是”脏”的，则操作系统会先将数据回写磁盘再淘汰）。这样，就算mmap的数据远大于物理内存，操作系统也能很好地处理，不会产生功能上的问题。

### **17.2.read**

![img](https://pic3.zhimg.com/80/v2-ee003c210c5d77d97df12bf4c2b3b666_720w.webp)

从图中可以看出，mmap要比普通的read系统调用少了一次copy的过程。因为read调用，进程是无法直接访问kernel space的，所以在read系统调用返回前，内核需要将数据从内核复制到进程指定的buffer。但mmap之后，进程可以直接访问mmap的数据(page cache)。

## 18.总结

一张图总结一下select,poll,epoll的区别：

![img](https://pic1.zhimg.com/80/v2-5a2a074550dda2645bc3f0a65d66f108_720w.webp)

epoll是Linux目前大规模网络并发程序开发的首选模型。在绝大多数情况下性能远超select和poll。目前流行的高性能web服务器Nginx正式依赖于epoll提供的高效网络套接字轮询服务。但是，在并发连接不高的情况下，多线程+阻塞I/O方式可能性能更好。

既然select，poll，epoll都是I/O多路复用的具体的实现，之所以现在同时存在，其实他们也是不同历史时期的产物：

- select出现是1984年在BSD里面实现的

- 14年之后也就是1997年才实现了poll，其实拖那么久也不是效率问题， 而是那个时代的硬件实在太弱，一台服务器处理1千多个链接简直就是神一样的存在了，select很长段时间已经满足需求

- 2002, 大神 Davide Libenzi 实现了epoll。

  

  原文链接：https://zhuanlan.zhihu.com/p/364832778

  作者：[Linux服务器研究](https://www.zhihu.com/people/shao-nian-bu-nian-shao-zhu-80)



# 【NO.301】关于TCP的CLOSING状态和CLOSE_WAIT状态浅析

很多资料讲了关于TCP的CLOSING和CLOSE_WAIT状态以及所谓的优雅关闭的细节，多数侧重与Linux的内核实现(除了《UNIX网络编程》)。本文不注重代码细节，只关注逻辑。所使用的工具，tcpdump，packetdrill以及ss。

关于ss可以先多说几句，它展示的信息跟netstat差不多，只不过更加详细。netstat的信息是通过procfs获取的，本质上来讲就是遍历/proc/net/netstat文件的内容，然后将其组织成可读的形式展示出来，然而ss则可以针对特定的五元组信息提供更加详细的内容，它不再通过procfs，而是用过Netlink来提取特定socket的信息，对于TCP而言，它可以提取到甚至tcp_info这种详细的信息，它包括cwnd，ssthresh，rtt，rto等。

本文展示的逻辑使用了以下三样工具：

1).packetdrill

使用packetdrill构造出一系列的包序列，使得TCP进入CLOSING状态或者CLOSE_WAIT状态。

2).tcpdump/tshark

抓取packetdrill注入的数据包以及协议栈反馈的包，以确认数据包序列确实如TCP标准所述的那样。

3).ss/netstat

通过ss抓取packetdrill相关套接字的tcp_info，再次确认细节。

我想，我使用上述的三件套解析了CLOSING状态之后，接下来的CLOSE_WAIT状态就可以当作练习了。

我来一个一个说。

## 1.关于CLOSING状态

首先我来描述一下而不是细说概念。

什么是CLOSING状态呢？我们来看一下下面的局部状态图：

![img](https://pic4.zhimg.com/80/v2-7545d0378e1a20cf51a0ba2c6a476ed3_720w.webp)

也就是说，当两端都主动发送FIN的时候，并且在收到对方对自己发送的FIN之前收到了对方发送的FIN的时候，两边就都进入了CLOSING状态，这个在状态图上显示的很清楚。这个用俗话说就是”同时关闭“。时序图我就不给出了，请自行搜索或者自己画。

有很多人都说，这种状态的TCP连接在系统中存在了好长时间并百思不得其解。这到底是为什么呢？通过状态图和时序图，我们知道，在进入CLOSING状态后，只要收到了对方对自己的FIN的ACK，就可以双双进入TIME_WAIT状态，因此，如果RTT处在一个可接受的范围内，发出的FIN会很快被ACK从而进入到TIME_WAIT状态，CLOSING状态应该持续的时间特别短。

以下是packetdrill脚本，很简单的一个脚本：

```text
0.000 socket(..., SOCK_STREAM, IPPROTO_TCP) = 3
0.000 setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
0.000 bind(3, ..., ...) = 0
0.000 listen(3, 1) = 0

0.100 < S 0:0(0) win 32792 <mss 1460,sackOK,nop,nop,nop,wscale 7>
0.100 > S. 0:0(0) ack 1 win 5840 <mss 1460,nop,nop,sackOK,nop,wscale 7>
0.200 < . 1:1(0) ack 1 win 257
0.200 accept(3, ..., ...) = 4

// 象征性写入一些数据，装的像一点一个正常的TCP连接：握手-传输-挥手
0.250 write(4, ..., 1000) = 1000

0.300 < . 1:1(0) ack 1001 win 257

// 主动断开，发送FIN
0.400 close(4) = 0
// 在未对上述close的FIN进行ACK前，先FIN
0.500 < F. 1:1(0) ack 1001 win 260
// 至此，成功进入同时关闭的CLOSING状态。

// 由于packetdrill不能用dup调用，也好用多线程，为了维持进程不退出，只能等待
10000.000 close(4) = 0
```

同时，我启用tcpdump抓包，确认了TCP状态图的细节，即，还没有收到对方对FIN的ACK时，收到了对方的FIN：

![img](https://pic4.zhimg.com/80/v2-8048285969108595a8592e4da74716df_720w.webp)

有个异常，没有收到FIN的ACK(packetdrill没有回复，这正常，因为脚本里本来就没有这个语句)，然而也没有看到重传，此时该连接应该是处于CLOSING状态了，用ss来确认：

CLOSING 1 1 192.168.0.1:webcache 192.0.2.1:54442

cubic wscale:7,7 rto:2000 rtt:50/25 ssthresh:2 send 467.2Kbps rcv_space:5840

果然，进入了CLOSING状态且没有消失，时不我待，当过了2秒以后，ss的结果变成了：

CLOSING 1 1 192.168.0.1:webcache 192.0.2.1:54442

cubic wscale:7,7 rto:4000 rtt:50/25 ssthresh:2 send 467.2Kbps rcv_space:5840

明显在退避！如果继续观察，你会发现rto退避到了64秒之多。在我的场景中，CLOSING状态的套接字维持了两分钟之久。

然而，为什么呢？为什么CLOSING状态会维持这么久？为什么它没有继续维持下去直到永久呢？

很明显，一端的FIN发出去后，没有收到ACK，因此会退避重发，知道4次退避，即2*2*2*2*2*2秒之久。现在的问题是，为什么重发FIN始终不成功呢？要是成功了的话，估计ACK瞬间也就回来了，那么CLOSING状态也就可以进入TIME_WAIT了，但是没有成功重传FIN！

到此为止，我们知道，进入CLOSING状态之后，两边都会等待接收自己FIN的ACK，一旦收到ACK，就会进入TIME_WAIT，如此反复，如果收不到ACK，则会不断重传FIN，直到忍无可忍，将socket销毁。现在，我们集中于解释为什么重传没有成功，但是请记住，并不是每次都这样，只是在我这个packetdrill构造的场景中会有重传不成功，不然如果大概率不成功的话。岂不是每个CLOSING状态都要维持很长时间？？！！



在我的场景下，通过hook重传函数以及抓包确认，发现所有的重传虽然退避了，但是都没有真正将数据包发送出去，究其原因，最终确认问题出在以下代码上：

```text
if (atomic_read(&sk->sk_wmem_alloc) >
    min(sk->sk_wmem_queued + (sk->sk_wmem_queued >> 2), sk->sk_sndbuf))
    return -EAGAIN;
```

在Linux协议栈的实现中，tcp_retransmit_skb由tcp_retransmit_timer调用，即便是这里出了些问题没有重传成功，也还是会退避的，退避超时到期后，继续在这里出错，直到”不可容忍“销毁socket。

我们可以得知，不管如何CLOSING状态的TCP连接即便没有收到对自己FIN的ACK，也不会永久保持下去，保持多久取决于自己发送FIN时刻的RTT，然后RTT计算出的RTO按照最大的退避次数来退避，直到最终执行了固定次数的退避后，算出来的那个比较大的超时时间到期，然后TCP socket就销毁了。

因此，CLOSING状态并不可怕，起码，不管怎样，它有一个可控的销毁时限。

...

现在我来解释重传不成功的细节。

我们知道，根据上述的代码段，sk_wmem_alloc要足够大，大到它比sk_wmem_queued+sk_wmem_queued/4更大的时候，才会返回错误造成重传不成功，然而我们的packetdrill脚本中构造的TCP连接的生命周期中仅仅传输了1000个字节的数据，并且这1000个字节的数据得到了ACK，然后就结束了连接。一个socket保有一个sk_wmem_alloc字段，在skb交给这个socket的时候，该字段会增加skb长度的大小(skb本身大小包括skb数据大小)，然而当skb不再由该socket持有的时候，也就是其被更底层的逻辑接管之后，socket的sk_wmem_alloc字段自然会减去skb长度的大小，这一切的过程由以下的函数决定，即skb_set_owner_w和skb_orphan。我们来看一下这两个函数：

```text
static inline void skb_set_owner_w(struct sk_buff *skb, struct sock *sk)
{
    skb_orphan(skb);
    skb->sk = sk;
    // sock_wfree回调中会递减sk_wmem_alloc相应的大小，其大小就是skb->truesize
    skb->destructor = sock_wfree;
    /*
     * We used to take a refcount on sk, but following operation
     * is enough to guarantee sk_free() wont free this sock until
     * all in-flight packets are completed
     */
    atomic_add(skb->truesize, &sk->sk_wmem_alloc);
}
static inline void skb_orphan(struct sk_buff *skb)
{
    // 调用回调函数，递减sk_wmem_alloc
    if (skb->destructor)
        skb->destructor(skb);
    skb->destructor = NULL;
    skb->sk        = NULL;
}
```

也就是说，只要skb_orphan在skb通向网卡的路径上被正确调用，就会保证sk_wmem_alloc的值随着skb进入socket的管辖时而增加，而被实际发出后而减少。但是根据我的场景，事实好像不是这样，sk_wmem_alloc的值只要发送一个skb就会增加，丝毫没有减少的迹象...这是为什么呢？

有的时候，当你对某个逻辑理解足够深入后，一定要相信自己的判断，内核存在BUG！内核并不完美。我使用的是2.6.32老内核，这个内核我已经使用了6年多，这是我在这个内核上发现的第4个BUG了。

请注意，我的这个场景中，我使用了packetdrill来构造数据包，而packetdrill使用了tun网卡。为什么使用真实网卡甚至使用loopback网卡就不会有问题呢？这进一步引导我去调查tun的代码，果不其然，在其hard_xmit回调中没有调用skb_orphan！也就说说，但凡使用2.6.32内核版本tun驱动的，都会遇到这个问题呢。在tun的xmit中加入skb_orphan之后，问题消失，抓包你会发现大量的FIN重传包，这些重传随着退避而间隔加大(注意，用ss命令比对一下rto字段的值和tcpdump抓取的实际值)：

![img](https://pic3.zhimg.com/80/v2-5efba967d3c83a13ff0e2fc6b0c0a82e_720w.webp)

(为了验证这个，我修改了packetdrill脚本，中间增加了很多的数据传输，以便尽快重现sk_wmem_alloc在使用tun时不递减的问题)于是，我联系了前公司的同事，让他们修改OpenVPN使用的tun驱动代码，因为当时确实出现过关于TCP使用OpenVPN隧道的重传问题，然而，得到的答复却是，xmit函数中已经有skb_orphan了...然后我看了下代码，发现，公司的代码已经不存在问题了，因为我在前年搞tun多队列的时候，已经移植了3.9.6的tun驱动，这个问题已经被修复。

自己曾经做的事情，已然不再忆起...

## 2.关于CLOSE_WAIT状态

和CLOSING状态不同，CLOSE_WAIT状态可能会持续更久更久的时间，导致无用的socket无法释放，这个时间可能与应用进程的生命周期一样久！

我们先看一下CLOSE_WAIT的局部状态图。

![img](https://pic4.zhimg.com/80/v2-9dc0dd527d6b01bec0b401d8178071e3_720w.webp)

然后我来构造一个packetdrill脚本：

```text
0.000 socket(..., SOCK_STREAM, IPPROTO_TCP) = 3
0.000 setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0
0.000 bind(3, ..., ...) = 0
0.000 listen(3, 1) = 0

0.100 < S 0:0(0) win 32792 <mss 1460,sackOK,nop,nop,nop,wscale 7>
0.100 > S. 0:0(0) ack 1 win 14600 <mss 1460,nop,nop,sackOK,nop,wscale 7>
0.200 < . 1:1(0) ack 1 win 257
0.200 accept(3, ..., ...) = 4
// 什么也不发了，直接断开
0.350 < F. 1:1(0) ack 1 win 260
// 协议栈会对这个FIN进行ACK，然则应用程序不关闭连接的话...
//0.450 close(4) = 0
// 该连接就会变成CLOSE_WAIT，并且只要其socket引用计数不为0，就一直僵死在那里
2000.000 close(4) = 0
```

同样的，我来展示抓包结果：

![img](https://pic4.zhimg.com/80/v2-05ed52c83129233427a63721cefc46eb_720w.webp)

最后，和描述CLOSING状态不同的是，隔了N个小时之后，我来看ss -ip的结果：

CLOSE-WAIT 1 0 192.168.0.1:webcache 192.0.2.1:53753 users:(("ppp",2399,8))

cubic wscale:7,7 rto:300 rtt:100/50 ato:40 cwnd:10 send 1.2Mbps rcv_space:14600

这个CLOSE_WAIT还在！这是为什么呢？

很遗憾，上述的packetdrill脚本并不能直观地展示这个现象，还得靠我说一说。

CLOSE_WAIT是一端在收到FIN之后，发送自己的FIN之前所处的状态，那么很显然，如果一个进程/线程始终不发送FIN，那么在该连接所隶属的socket的生命周期内，这个socket就会一直存在，我们知道，在UNIX/Linux/WinSock中，socket作为一个描述符出现，只要进程/线程继续持有它，它就会一直存在，因此大多数情况下进程/线程的生命周期内，此TCP套接字就会始终处在CLOSE_WAIT状态。进程/线程长时间持有不需要的socket描述符，更多的并不是有意的，而是在进行诸如fork/clone之类的系统调用后，dup了父亲的文件描述符，然后在孩子那里又没有及时关闭，另外的原因就是编程者对socket描述符的close接口以及shutdown接口不是很理解了。

现在，我们用一个问题来继续我们的讨论。

**什么时候进程在超长的生命周期内不会如愿关闭TCP从而发送FIN呢？**

我的答案比较直接： 不能指望close会发送FIN！

相信很多人在想断开一个TCP连接的时候，都会调用close吧。并且这种做法几乎都是正确的，以至于很多人都把这作为一种标准的做法。但是这是不对的！Why？！在《UNIX网络编程》中，曾经提到了所谓的”优雅关闭TCP连接“，何谓优雅？？！如果你充分理解close，shutdown，应该就会知道，CLOSE_WAIT出现，你应该可以给出一些解释。

**close调用**

close的参数只是一个文件描述符号，它不理解这个文件真正的细节，它只是一个文件系统内范畴的一个调用，它只是关闭文件描述符，保证此进程不会在读取它而已。如果你关闭了文件描述符4，即close(4)，你知道4代表的文件会作何反应吗？？文件系统并不知道4号描述符代表的文件到底是什么，更不知道有多少进程共享这个底层的”实体“，所以一个进程层面上逻辑根本没有权力去彻底关闭一个socket。如果你想了解close的细节，更应该去看看UNIX文件抽象或者文件系统的细节，而不是socket。请参见位于fs/open.c中的：

```text
SYSCALL_DEFINE1(close, unsigned int, fd)
{
    ...
    fdt = files_fdtable(files);
    ...
    filp = fdt->fd[fd];
    ...
    retval = filp_close(filp, files);
    ...
    return retval;
    ...
}
EXPORT_SYMBOL(sys_close);
```

在filp_close中会有fput调用：

```text
void fput(struct file *file)
{
    if (atomic_long_dec_and_test(&file->f_count))
        __fput(file);
}
```

看到那个引用计数了吗？只有当这个文件的引用计数变成0的时候，才会调用底层的关闭逻辑，对于socket而言，如果仍然还有一个进程或者线程持有这个socket对应的文件系统的描述符，那么即便你调用了close，也不会进入了socket的close逻辑，它在文件系统层面就返回了！

**shutdown调用**

这个才是真正关闭一个TCP连接的调用！shutdown并没有文件系统的语义，它专门针对内核层的TCP socket。因此，调用shutdown的逻辑，才是真正关闭了与之共享信道的tcp socket。

所谓的优雅关闭，就是在调用close之前， 首先自己调用shutdown(RD or WD)。这样的时序才是关闭TCP的必由之路！

如果你想优雅关闭一个TCP连接，请先用shutdown，然后后面跟一个close。不过有点诡异的是，Linux的shutdown(SHUT_RD)貌似没有任何效果，不过这无所谓了，本来对于读不读的，就不属于TCP的范畴，只有SHUT_WR才会实际发送一个FIN给对方。

原文地址：https://zhuanlan.zhihu.com/p/538326325

作者：linux

# 【NO.302】Linux 网络性能优化-C10K、C1000K、C10M 问题总结

## 1.C10K 问题以及优化方法简介

C10K 表示单机同时处理 1 万个请求 (并发连接 1 万) 的问题；

**【1.1】主要待解决的问题**

问题一，怎样在一个线程内处理多个请求，即要在一个线程内响应多个网络 I/O；

问题二，怎么更节省资源地处理客户请求，即要用更少的线程来服务这些请求；

**【1.2】事件 IO 通知的方式**

两种 I/O 事件通知的方式，水平触发和边缘触发；

- 水平触发，只要文件描述符可以非阻塞地执行 I/O，就会触发通知，即应用程序可以随时检查文件描述符的状态，然后再根据状态，进行 I/O 操作；
- 边缘触发，只有在文件描述符的状态发生改变时，才发送一次通知，此时，应用程序需要尽可能多地执行 I/O，直到无法继续读写，才可以停止；若 I/O 没执行完，或者因为某种原因没来得及处理，那么这次通知也就丢失了；

**【1.3】优化方案**

【1.3.1】I/O 模型优化

使用非阻塞 I/O 和水平触发通知 (比如使用 select 或者 poll)

根据水平触发的特性，select 和 poll 可以从文件描述符列表中，找出可以执行 I/O 操作的文件描述符，然后进行真正的网络 I/O 操作，由于 I/O 是非阻塞的，一个线程中就可以同时监控一批套接字的文件描述符，从而实现单线程处理多请求的目的；

**缺陷**

- 应用程序使用 select 和 poll 时，需要对这些文件描述符列表进行轮询，在请求数多的时候就会比较耗时；
- select 使用固定长度的位向量，表示文件描述符的集合，存在最大描述符数量的限制；并且，在 select 内部，检查套接字状态使用轮询的方法，处理耗时跟描述符数量是 O(N) 的关系；
- 应用程序每次调用 select 和 poll 时，需要把文件描述符的集合，从用户空间传入内核空间，由内核修改后，再传出到用户空间中，内核空间与用户空间切换增加了处理成本；

使用非阻塞 I/O 和边缘触发通知 (比如 epoll)

epoll 使用红黑树，在内核中管理文件描述符的集合，从而就不需要应用程序在每次操作时都传入、传出该集合；

epoll 使用事件驱动的机制，只关注有 I/O 事件发生的文件描述符，不需要轮询扫描整个集合；

**缺陷**

- 边缘触发只在文件描述符可读或可写事件发生时才通知，从而应用程序就需要尽可能多地执行 I/O 并要处理更多的异常事件；

使用异步 I/O (Asynchronous I/O，简称为 AIO)

异步 I/O 允许应用程序同时发起很多 I/O 操作，而不用等待这些操作完成，而在 I/O 完成后，系统会用事件通知 (比如信号或者回调函数)的方式，通知应用程序；应用程序接到通知才会去查询 I/O 操作的结果；

**缺陷**

- 目前该方式不太完善，使用比较困难

**【1.3.2】工作模型优化**

\1. 主进程 + 多个 worker 子进程

- 主进程执行 bind() + listen() 后，创建多个子进程；
- 在每个子进程中，通过 accept() 或 epoll_wait()，来处理相同的套接字；

![img](https://pic2.zhimg.com/80/v2-52c9d25906664f38ef89d67a17847149_720w.webp)

**惊群问题**

accept() 和 epoll_wait() 调用的惊群问题，即当网络 I/O 事件发生时，多个进程被同时唤醒，但实际上只有一个进程来响应这个事件，其他被唤醒的进程都会重新休眠；

解决

- accept() 的惊群问题，已经在 Linux 2.6 中解决；
- epoll 的问题，在 Linux 4.5 通过 EPOLLEXCLUSIVE 解决；
- Nginx 在每个 worker 进程中，都增加一个了全局锁(accept_mutex)，这些 worker 进程需要首先竞争到锁，只有竞争到锁的进程，才会加入到 epoll 中，这样就确保只有一个 worker 子进程被唤醒；

**2. 监听到相同端口的多进程模型**

该方式下，所有的进程都监听相同的端口并且开启 SO_REUSEPORT 选项，由内核负责将请求负载均衡到这些监听进程中

![img](https://pic1.zhimg.com/80/v2-cf57e4f185fe6f464a79e0eb9af04380_720w.webp)



## 2.C1000K 问题与优化

**物理资源**

100 万个请求需要大量的系统资源；

- 内存，假设每个请求需要 16KB 内存，则总共就需要大约 15 GB 内存；
- 带宽，假设只有 20% 活跃连接，即使每个连接只需要 1KB/s 的吞吐量，总共需要 1.6 Gb/s 的吞吐量；需要配置万兆网卡，或者基于多网卡 Bonding 承载更大的吞吐量；

**软件资源**

大量的连接会占用大量的软件资源；

- 比如文件描述符的数量、连接状态的跟踪(CONNTRACK)、网络协议栈的缓存大小(比如套接字读写缓存、TCP 读写缓存)等等；

**大量请求带来的中断处理**

- 需要多队列网卡、中断负载均衡、CPU 绑定、RPS/RFS (软中断负载均衡到多个 CPU 核上)，以及将网络包的处理卸载 (Offload) 到网络设备(如 TSO/GSO、LRO/GRO、VXLAN OFFLOAD) 等各种硬件和软件的优化；

C1000K 的解决方法，本质上是构建在 epoll 的非阻塞 I/O 模型上，除了 I/O 模型之外，还需要从应用程序到 Linux 内核、再到 CPU、内存和网络等各个层次的深度优化，特别是需要借助硬件，来卸载那些原来通过软件处理的大量功能；

## **3.C10M 问题与优化**

核心思想是跳过内核协议栈的冗长路径，把网络包直接送到要处理的应用程序中处理，这里有两种常见的机制，DPDK 和 XDP；

**DPDK**

- DPDK 是用户态网络的标准，跳过内核协议栈，直接由用户态进程通过轮询的方式处理网络接收；

![img](https://pic2.zhimg.com/80/v2-67721fb75c9e824a20f9d50ce1c3a551_720w.webp)

在 PPS 非常高的场景中，查询时间比实际工作时间少了很多，绝大部分时间都在处理网络包；而跳过内核协议栈后，就省去了繁杂的硬中断、软中断再到 Linux 网络协议栈逐层处理的过程，应用程序可以针对应用的实际场景，有针对性地优化网络包的处理逻辑，而不需要关注所有的细节；此外，DPDK 通过大页、CPU 绑定、内存对齐、流水线并发等多种机制，优化网络包的处理效率；

原文地址：https://zhuanlan.zhihu.com/p/537843366

作者：Linux

# 【NO.303】C语言回调函数到底是什么？如何使用回调函数？

## **1.什么是回调函数？**

回调函数，光听名字就比普通函数要高大上一些，那到底什么是回调函数呢？恕我读得书少，没有在那本书上看到关于回调函数的定义。我在百度上搜了一下，发现众说纷纭，有很大一部分都是使用类似这么一个场景来说明：A君去B君店里买东西，恰好缺货，A君留下号码给B君，有货时通知A君。感觉这个让人更容易想到的是异步操作，而不是回调。

另外还有两句英文让我印象深刻：1) If you call me, I will call you back; 2) Don't call me, I will call you. 看起来好像很有道理，但是仔细一想，普通函数不也可以做到这两点吗？所以，我觉得这样的说法都不是很妥当，因为我觉得这些说法都没有把回调函数的特点表达出来，也就是都看不到和普通函数到底有什么差别。

不过，百度百科的解析我觉得还算不错（虽然经常吐槽百度搜索...）：回调函数就是一个通过函数指针调用的函数。如果你把函数的指针（地址）作为参数传递给另一个函数，当这个指针被用来调用其所指向的函数时，我们就说这是回调函数。

下面先说说我的看法。我们可以先在字面上先做个分解，对于"回调函数"，中文其实可以理解为这么两种意思：1) 被回调的函数；2) 回头执行调用动作的函数。那这个回头调用又是什么鬼？

**先来看看来自维基百科的对回调（Callback）的解析**：In computer programming, a callback is any executable code that is passed as an argument to other code, which is expected to call back (execute) the argument at a given time. This execution may be immediate as in a synchronous callback, or it might happen at a later time as in an asynchronous callback. 也就是说，把一段可执行的代码像参数传递那样传给其他代码，而这段代码会在某个时刻被调用执行，这就叫做回调。如果代码立即被执行就称为同步回调，如果在之后晚点的某个时间再执行，则称之为异步回调。关于同步和异步，这里不作讨论，请查阅相关资料。

**再来看看来自Stack Overflow某位大神简洁明了的表述：**A "callback" is any function that is called by another function which takes the first function as a parameter。也就是说，函数 F1 调用函数 F2 的时候，函数 F1 通过参数给 函数 F2 传递了另外一个函数 F3 的指针，在函数 F2 执行的过程中，函数F2 调用了函数 F3，这个动作就叫做回调（Callback），而先被当做指针传入、后面又被回调的函数 F3 就是回调函数。到此应该明白回调函数的定义了吧？

## **2.为什么要使用回调函数？**

很多朋友可能会想，为什么不像普通函数调用那样，在回调的地方直接写函数的名字呢？这样不也可以吗？为什么非得用回调函数呢？有这个想法很好，因为在网上看到解析回调函数的很多例子，其实完全可以用普通函数调用来实现的。要回答这个问题，我们先来了解一下回到函数的好处和作用，那就是解耦，对，就是这么简单的答案，就是因为这个特点，普通函数代替不了回调函数。所以，在我眼里，这才是回调函数最大的特点。来看看维基百科上面我觉得画得很好的一张图片。

![img](https://pic2.zhimg.com/80/v2-6b541dce8cfd665f1199cedca95287a5_720w.webp)

```text
    #include<stdio.h>
    #include<softwareLib.h> // 包含Library Function所在读得Software library库的头文件

    int Callback() // Callback Function
{
        // TODO
        return 0;
    }
    int main() // Main program
{
        // TODO
        Library(Callback);
        // TODO
        return 0;
    }
```

乍一看，回调似乎只是函数间的调用，和普通函数调用没啥区别，但仔细一看，可以发现两者之间的一个关键的不同：在回调中，主程序把回调函数像参数一样传入库函数。这样一来，只要我们改变传进库函数的参数，就可以实现不同的功能，这样有没有觉得很灵活？并且丝毫不需要修改库函数的实现，这就是解耦。

再仔细看看，主函数和回调函数是在同一层的，而库函数在另外一层，想一想，如果库函数对我们不可见，我们修改不了库函数的实现，也就是说不能通过修改库函数让库函数调用普通函数那样实现，那我们就只能通过传入不同的回调函数了，这也就是在日常工作中常见的情况。现在再把main()、Library()和Callback()函数套回前面 F1、F2和F3函数里面，是不是就更明白了？

明白了回调函数的特点，是不是也可以大概知道它应该在什么情况下使用了？没错，你可以在很多地方使用回调函数来代替普通的函数调用，但是在我看来，如果需要降低耦合度的时候，更应该使用回调函数。

## **3.怎么使用回调函数？**

知道了什么是回调函数，了解了回调函数的特点，那么应该怎么使用回调函数？下面来看一段简单的可以执行的同步回调函数代码。

```text
    #include<stdio.h>

    int Callback_1() // Callback Function 1
{
        printf("Hello, this is Callback_1 \n");
        return 0;
    }

    int Callback_2() // Callback Function 2
{
        printf("Hello, this is Callback_2 \n");
        return 0;
    }

    int Callback_3() // Callback Function 3
{
        printf("Hello, this is Callback_3 \n");
        return 0;
    }

    int Handle(int (*Callback)())
{
        printf("Entering Handle Function.\n ");
        Callback();
        printf("Leaving Handle Function.\n ");
    }

    int main()
{
        printf("Entering Main Function.\n ");
        Handle(Callback_1);
        Handle(Callback_2);
        Handle(Callback_3);
        printf("Leaving Main Function.\n");
        return 0;
    }
```

运行结果：

> Entering Main Function.
> Entering Handle Function.
> Hello, this is Callback_1
> Leaving Handle Function.
> Entering Handle Function.
> Hello, this is Callback_2
> Leaving Handle Function.
> Entering Handle Function.
> Hello, this is Callback_3
> Leaving Handle Function.
> Leaving Main Function.

可以看到，Handle()函数里面的参数是一个指针，在main()函数里调用Handle()函数的时候，给它传入了函数Callback_1()/Callback_2()/Callback_3()的函数名，这时候的函数名就是对应函数的指针，也就是说，回调函数其实就是函数指针的一种用法。现在再读一遍这句话：A "callback" is any function that is called by another function which takes the first function as a parameter，是不是就更明白了呢？

## **4.怎么使用带参数的回调函数？**

眼尖的朋友可能发现了，前面的例子里面回调函数是没有参数的，那么我们能不能回调那些带参数的函数呢？答案是肯定的。那么怎么调用呢？我们稍微修改一下上面的例子就可以了：

```text
      #include<stdio.h>

    int Callback_1(int x) // Callback Function 1
{
        printf("Hello, this is Callback_1: x = %d ", x);
        return 0;
    }

    int Callback_2(int x) // Callback Function 2
{
        printf("Hello, this is Callback_2: x = %d ", x);
        return 0;
    }

    int Callback_3(int x) // Callback Function 3
{
        printf("Hello, this is Callback_3: x = %d ", x);
        return 0;
    }

    int Handle(int y, int (*Callback)(int))
{
        printf("Entering Handle Function. ");
        Callback(y);
        printf("Leaving Handle Function. ");
    }

    int main()
{
        int a = 2;
        int b = 4;
        int c = 6;
        printf("Entering Main Function. ");
        Handle(a, Callback_1);
        Handle(b, Callback_2);
        Handle(c, Callback_3);
        printf("Leaving Main Function. ");
        return 0;
    }
```

运行结果：

> Entering Main Function.Entering Handle Function.Hello, this is Callback_1: x = 2Leaving Handle Function.Entering Handle Function.Hello, this is Callback_2: x = 4Leaving Handle Function.Entering Handle Function.Hello, this is Callback_3: x = 6Leaving Handle Function.Leaving Main Function.

可以看到，并不是直接把int Handle(int (*Callback)()) 改成 int Handle(int (*Callback)(int)) 就可以的，而是通过另外增加一个参数来保存回调函数的参数值，像这里 int Handle(int y, int (*Callback)(int)) 的参数 y。同理，可以使用多个参数的回调函数。

## **5.参考练习**

```text
#include <stdio.h>
typedef  void (*listen)(int);

listen mlisten[3];

void register_observer(listen obs)
{
  for(int i=0;i<3;i++)
  {
    if(mlisten[i] == 0)
    {
      mlisten[i] = obs;
      return ;
    }  
  }
}

void listen0(int i)
{
  printf("listen0 received i=%d\n",i);
}
void listen1(int i)
{
  printf("listen1 received i=%d\n",i);
}
void listen2(int i)
{
  printf("listen2 received i=%d\n",i);
}

void notify_all_observer(int val)
{
  for(int i=0;i<sizeof(mlisten)/sizeof(mlisten[0]);i++)
  {
    if(mlisten[i] != 0)
    {
      mlisten[i](val);
    }  
  }
}
int main()
{
  int i=0;
  printf("lis1:%d\n",listen0);
  register_observer(listen0);
  printf("lis2:%d\n",listen1);
  register_observer(listen1);
  register_observer(listen2);

  while(1)
  {
    scanf("%d\n", &i);
    printf("%d\n",i);
    notify_all_observer(i);
  }
}
```

原文地址：https://zhuanlan.zhihu.com/p/537404427

作者：linux

# 【NO.304】腾讯面试题：十亿数据如何去重？红黑树到hash再到布隆过滤器

在开始之前我们先思考以下几个问题

a1.在使⽤word⽂档时，word如何判断某个单词是否拼写正确？

a2.网络爬虫程序，怎么让它不去爬相同的url⻚⾯？

a3.垃圾邮件（短信）过滤算法如何设计？

a4.公安办案时，如何判断某嫌疑⼈是否在⽹逃名单中？

a5.缓存穿透问题如何解决？

对于前四个问题，这些大量的数据我们如何进行存储判断

- 应该将需要对比的数据以何种方式存储才能提高对比速率？
- 如何才能确保进行数据对比时候的高效性？

如果数据量比较少，我们可以直接挨个与数据库存储的信息进行对比得到答案，但如果数据量十分庞大，甚至达到上亿的级别，我们还能直接与数据库的信息进行对比吗？

想要明白上一点，我们先来聊聊关于缓存穿透的问题，它是如何发生的，以及我们该如何解决它。

![img](https://pic1.zhimg.com/80/v2-97ea446c2cf0fa5b4b2a8ac68ce90f90_720w.webp)

当server端向数据库请求数据时，中间缓存组件(redis)和数据库(此处是MySQL）都不存在相应数据，此时如果请求量较大，由于中间缓冲组件(redis)没有对应的缓存数据，于是压力全部给到数据库上，导致压力过大出现系统瘫痪的情况被称之为缓存穿透。

需求：

从海量数据中查询某字符串是否存在。

学过C++的朋友都知道set和map，但你清除它们的内部是如何实现的吗？

set和map

c++标准库（STL）中的set和map结构都是采⽤红⿊树实现的，它增删改查的时间复杂度是O(log2 N）；

图结构示例：

![img](https://pic1.zhimg.com/80/v2-42d699e7090c4232db531c455df97b60_720w.webp)

对于严格平衡二叉搜索树(AVL)，100w条数据组成的红黑树，只需要比较20次就能找到该值；对于10亿条数据只需要比较30次就能找到该数据；也就是查找次数跟树的维度是一致的；

对于红黑树来说平衡的是⿊节点⾼度，所以研究⽐较次数需要考虑树的⾼度差，最好情况某条树链路全是⿊节点，假设此时⾼度为h1，最差情况某条树链路全是⿊红节点间隔，那么此时树⾼度为2*h1;

在红⿊树中每⼀个节点都存储key和val字段，key是⽤来做⽐较的字段；红⿊树并没有要求key字段唯⼀，在set和map实现过程中限制了key字段唯⼀。我们来看nginx的红⿊树实现：

```text
// 这个是截取 nginx 的红⿊树的实现，这段代码是 insert 操作中的⼀部分，执⾏完这个函数还需要检测插⼊节点后是否平衡（主要是看他的⽗节点是否也是红⾊节点）
// 调⽤ ngx_rbtree_insert_value 时，temp传的参数为 红⿊树的根节点，node传的参数为待插⼊的节点
void ngx_rbtree_insert_value(ngx_rbtree_node_t *temp, ngx_rbtree_node_t
*node,
 	ngx_rbtree_node_t *sentinel) 
 {
 ngx_rbtree_node_t **p;
  for ( ;; ) {
	 p = (node->key < temp->key) ? &temp->left : &temp->right;// 这⾏很重要
	 if (*p == sentinel) {
	 break;
   }
   temp = *p;
 }
 *p = node;
 node->parent = temp;
 node->left = sentinel;
 node->right = sentinel;
 ngx_rbt_red(node);
}
// 不插⼊相同节点 如果插⼊相同 让它变成修改操作 此时 红⿊树当中就不会有相同的key了
定时器 key 时间戳
// 如果我们插⼊key = 12，如上图红⿊树，12号节点应该在哪个位置？ 如果我们要实现插⼊存在的节点变成修改操作，该怎么改上⾯的函数
void ngx_rbtree_insert_value_ex(ngx_rbtree_node_t *temp, ngx_rbtree_node_t
*node,  ngx_rbtree_node_t *sentinel) {
  ngx_rbtree_node_t **p;
   for ( ;; ) {
// {-------------add-------------
  if (node->key == temp->key) {
  temp->value = node->value;
 return;
 }
// }-------------add-------------
 p = (node->key < temp->key) ? &temp->left : &temp->right;// 这⾏很重要
  if (*p == sentinel) {
    break;
  }
	 temp = *p;
  }
  *p = node;
  node->parent = temp;
  node->left = sentinel;
  node->right = sentinel;
  ngx_rbt_red(node);
}
```

另外set和map的关键区别是set不存储val字段；

优点：存储效率⾼，访问速度⾼效；

缺点：对于数据量⼤且查询字符串⽐较⻓且查询字符串相似时将会是噩梦；

unordered_map

**过渡点:**

通过上边的内容我们已经知道了map内部是由红黑树实现的，时间复杂度相对较低了，但如果是一个字符单词，甚至是一个url(很长的字符串),如果数据量较大时，比如10亿条数据需要⽐较30次能找到该数据，但对于字符串的比较相对也是比较耗时的，有没有一种办法能避免或者减少比较呢，于是出现了hash结构(通过数组和hash函数来实现的一种数据结构)。

map内部是红黑树实现的，unordered_map则采用了hashtable



c++标准库（STL）中的unordered_map<string, bool>是采⽤hashtable实现的；

构成：数组+hash函数；

它是将字符串通过hash函数⽣成⼀个整数再映射到数组当中；它增删改查的时间复杂度是o(1);

图结构示例：

![img](https://pic4.zhimg.com/80/v2-02591ffa6eea47a41dea979d26213cbf_720w.webp)

hash函数的作⽤：避免插⼊的时候字符串的⽐较；hash函数计算出来的值通过对数组⻓度的取模

能随机分布在数组当中；

hash函数⼀般返回的是64位整数，将多个⼤数映射到⼀个⼩数组中，必然会产⽣冲突；

**如何选取hash函数？**

1. 选取计算速度快；
2. 哈希相似字符串能保持强随机分布性（防碰撞）；

murmurhash1，murmurhash2，murmurhash3，siphash（redis6.0当中使⽤，rust等⼤多数语⾔选⽤的hash算法来实现hashmap），cityhash都具备强随机分布性；测试地址如下：

[GitHub - aappleby/smhasher: Automatically exported from code.google.com/p/smhasher](https://link.zhihu.com/?target=https%3A//github.com/aappleby/smhasher)

负载因⼦：数组存储元素的个数/数组⻓度；负载因⼦越⼩，冲突越⼩；负载因⼦越⼤，冲突越⼤；

hash冲突解决⽅案：

链表法

引⼊链表来处理哈希冲突；也就是将冲突元素⽤链表链接起来；这也是常⽤的处理冲突的⽅式；但是可能出现⼀种极端情况，冲突元素⽐较多，该冲突链表过⻓，这个时候可以将这个链表转换为红⿊树；由原来链表时间复杂度 o(n) 转换为红⿊树时间复杂度 ；那么判

断该链表过⻓的依据是多少？可以采⽤超过256（经验值）个节点的时候将链表结构转换为红⿊树结构；

开放寻址法

将所有的元素都存放在哈希表的数组中，不使⽤额外的数据结构；⼀般使⽤线性探查的思路解决；

1 当插⼊新元素的时，使⽤哈希函数在哈希表中定位元素位置；

\2. 检查数组中该槽位索引是否存在元素。如果该槽位为空，则插⼊，否则3；

3 在 2 检测的槽位索引上加⼀定步⻓接着检查2；

加⼀定步⻓分为以下两种：

![img](https://pic4.zhimg.com/80/v2-5964c0a456f7ff55ae4c3340a4fe9a33_720w.webp)

这两种都会导致同类hash聚集；也就是近似值它的hash值也近似，那么它的数组槽位也靠近，形成hash聚集；第⼀种同类聚集冲突在前，**第⼆种只是将聚集冲突延后**；

另外还可以使⽤**双重哈希**来解决上⾯出现hash聚集现象：

![img](https://pic4.zhimg.com/80/v2-ab96b799847a9787c724166b1fa78763_720w.webp)

重点：何为同类hash聚集；也就是近似值它的hash值也近似，比如（nihao,nihaoa）这两个字符串由于很接近，所有通过一个hash函数得到值可能比较解决，就会出现hash聚集的情况，所以我们采用了双重哈希来解决这个问题。 双重哈希其实就是（双重散列分布算法）来保证相近的字符串通过hash函数得到的结果具有强随机分布性（防碰撞）从而避免hash聚集。

具体原理请点击：[双重哈希原理](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/organic/p/6283476.html)

同样的hashtable中节点存储了key和val，hashtable并没有要求key的⼤⼩顺序，我们同样可以修改代码让插⼊存在的数据变成修改操作；

优点：访问速度更快；不需要进⾏字符串⽐较；

缺点：需要引⼊策略避免冲突，存储效率不⾼；空间换时间；

**细节总结**

到目前为止，我们已经明白了红黑树以及hash对于数据查找带来的各自的优势，但它们都有一个最大的共同缺点，就是无法解决十分庞大的数据问题。因为不管是红黑树还是hashtable，它们都需要存储具体字符串，如果是十亿的数据，显然提供不了几百G的内存给它存放；所以在不断的发掘一种不用存放key值，同时具有hash结构的优点的数据结构。于是出现了我们今天的主角

**布隆过滤器**

定义：布隆过滤器是⼀种概率型数据结构，它的特点是⾼效的插⼊和查询，能明确告知某个字符串⼀定不存在或者可能存在

布隆过滤器相⽐传统的查询结构（例如：hash，set，map等数据结构）更加⾼效，占⽤空间更⼩；但是其缺点是它返回的结果是概率性的，也就是说结果存在误差的，虽然这个误差是可控的；

同时它不⽀持删除操作(至于为何不支持，继续往后看)

组成：位图（bit数组）+ n个hash函数

![img](https://pic3.zhimg.com/80/v2-f96586a424dadb2ba6765aab2f50117a_720w.webp)

原理：当⼀个元素加⼊位图时，通过k个hash函数将这个元素映射到位图的k个点，并把它们置为1；当检索时，再通过k个hash函数运算检测位图的k个点是否都为1；如果有不为1的点，那么认为不存在；如果全部为1，则可能存在（存在误差）；

![img](https://pic1.zhimg.com/80/v2-df8e2903a871f414a9b34fa0ef976bd8_720w.webp)

在位图中每个槽位只有两种状态（0或者1），⼀个槽位被设置为1状态，但不明确它被设置了多少次；也就是不知道被多少个str1哈希映射以及是被哪个hash函数映射过来的；所以不⽀持删除操作；

在实际应⽤过程中，布隆过滤器该如何使⽤？要选择多少个hash函数，要分配多少空间的位图，存储多少元素？另外如何控制假阳率（布隆过滤器能明确⼀定不存在，不能明确⼀定存在，那么存在的判断是有误差的，假阳率就是错误判断存在的概率）？

![img](https://pic1.zhimg.com/80/v2-0abf3e8791f0959ffc2caf0180d35e60_720w.webp)

假定我们选取这四个值为：
n = 4000
p = 0.000000001
m = 172532
k = 30

- 四个值的关系：

![img](https://pic2.zhimg.com/80/v2-8bca720422fa42f27ac8f2f9505f8c91_720w.webp)

![img](https://pic4.zhimg.com/80/v2-e5ca5e8976286a362569d793248e61d7_720w.webp)

![img](https://pic1.zhimg.com/80/v2-1f0dfcafcfe1bd15d1b1c2fa1600f8a0_720w.webp)

在实际应⽤中，我们确定n和p，通过上⾯的计算算出m和k；也可以在⽹站上选取合适的值：[计算m和k的网站(点我https://hur.st/bloomfilter）](https://link.zhihu.com/?target=https%3A//hur.st/bloomfilter)

已知k，如何选择k个hash函数？

**布隆过滤器的细节点：**

1:这里的k个hash函数是什么意思，真的是k个不同的哈希函数吗？其实通过下边的代码我们就可以明白，实际上只有一个哈希函数，但上边讲hashtable的时候我们说过可以采用双重hash的结构来避免同类哈希聚集，所以下边的代码我们可以看出同样采用了双重hash的结构，同时使用一个循环来实现这些命中的点的不同位置(以防命中同一个点)

2:还有一个细节点要明白：当⼀个元素加⼊位图时，通过k个hash函数将这个元素映射到位图的k个点，并把它们全置为1；当检索时，再通过k个hash函数运算检测位图的k个点是否都为1；如果有不为1的点，那么认为不存在；如果全部为1，则可能存在（存在误差）； 这里一定要明白，加入过程和检索过程是不一样的，加入是全设为1，检索是判断是否全为1，如果不是全为1，说明从来没有加入过，所以一定不存在，而就算全是1，也不一定存在，因为确实有可能同一个数据通过多次哈希都映射到了相同的几个位图点上。

// 采⽤⼀个hash函数，给hash传不同的种⼦偏移值

```text
// #define MIX_UINT64(v) ((uint32_t)((v>>32)^(v)))
uint64_t hash1 = MurmurHash2_x64(key, len, Seed);
uint64_t hash2 = MurmurHash2_x64(key, len, MIX_UINT64(hash1));
for (i = 0; i < k; i++) // k 是hash函数的个数
{
 Pos[i] = (hash1 + i*hash2) % m; // m 是位图的⼤⼩
}
```

// 通过这种⽅式来模拟 k 个hash函数 跟我们前⾯开放寻址法 双重hash是⼀样的思路

[应⽤源码http://gitlab.0voice.com/0voice/bloomfilter/tree/master：](https://link.zhihu.com/?target=http%3A//gitlab.0voice.com/0voice/bloomfilter/tree/master)

**大总结：**

本文章学习了从红黑树到hashtable再到布隆过滤器的演变，阐述了它们各自的优缺点，那么我们也应该深入了解它们各自都使用于哪些场景下：

对于普通类型的数据，可以采用红黑树来快速进行查找，而对于字符串类型的数据，如果采用红黑树进行查找，由于字符串的对比会花费较多的比较时间，所以我们采用了更好的方法hashtable(数组和hash函数配合)，从而避免或者减少了字符串的比较，提高查找效率，再后来引申到对于海量数据的存储管理，这两种结构都不适合，因为要进行存储的数据量实在太大，所以出现了布隆过滤器这种不需要存储key值就能进行判断的结构。

原文地址：https://zhuanlan.zhihu.com/p/536749623

作者：Linux

# 【NO.305】从进入内核态看linux内存管理

知乎上一个比较有意思的话题：如何理解「进入内核态」，要回答好这个问题需要对内存管理及程序的运行机制有比较深刻的了解，比如你需要了解内存的分段，分页，中断，特权级等机制，信息量比较大，本文将会从 Intel CPU 的发展历史讲起，循序渐近地帮助大家彻底掌握这一概念，相信大家看了肯定有帮助，本文目录如下

- CPU 运行机制

- Intel CPU 历史发展史

- - 分段
  - 保护模式

- 特权级

- - 系统调用
  - 中断

- 分段内存的优缺点

- 内存分页

- 总结

### 1.**CPU 运行机制**

我们先简单地回顾一下 CPU 的工作机制，重新温习一下一些基本概念，因为我在查阅资料的过程发现一些网友对寻址，CPU 是几位的概念理解得有些模糊，理解了这些概念再去看 CPU 的发展史就不会再困惑

CPU 是如何工作的呢？它是根据一条条的机器指令来执行的，而机器指令= 操作码+操作数，操作数主要有三类：寄存器地址、内存地址或立即数（即常量）。

我们所熟悉的程序就是一堆指令和数据的集合，当打开程序时，装载器把程序中的指令和数据加载到内存中，然后 CPU 到内存中一条条地取指令，然后再译码，执行。

在内存中是以字节为基本单位来读写数据的，我们可以把内存看作是一个个的小格子（一般我们称其为内存单元），而每个小格子是一个字节，那么对于 B8 0123H 这条指令来说，它在内存中占三字节，如下，CPU 该怎么找到这些格子呢，我们需要给这些格子编号，这些编号也就是我们说的**内存地址**，根据内存地址就是可以定位指令所在位置，从而取出里面的数据

![img](https://pic3.zhimg.com/80/v2-f6f463a43d44875665f6929ac0f63e66_720w.webp)

**如图示**：内存被分成了一个个的格子，每个格子一个字节，20000~20002 分别为对应格子的编号（即内存地址）

CPU 执行指令主要分为以下几个步骤

1. **取指令**，CPU 怎么知道要去取哪条指令呢，它里面有一个 IP 寄存器指向了对应要取的指令的内存地址， 然后这个内存地址会通过**地址总线**找到对应的格子，我们把这个过程称为**寻址**，不难发现**寻址能力决定于地址总线的位宽**，假设地址总线位数为 20 位，那么内存的可寻址空间为 2^20 * 1Byte = 1M，将格子（内存单元）里面的数据（指令）取出来后，再通过**数据总线**发往 CPU 中的指令缓存区（指令寄存器），那么一次能传多少数据呢，**取决于数据总线的位宽**，如果数据总线为 16 位，那么一次可以传 16 bit 也就是两个字节。
2. **译码**：指令缓冲区中的指令经过译码以确定该进行什么操作
3. **执行**：译码后会由控制单元向运算器发送控制指令进行操作（比如执行加减乘除等），执行是由运算器操纵数据也就是操作数进行计算，而操作数保存在存储单元（即片内的缓存和寄存器组）中，由于操作数有可能是内存地址，所以执行中可能需要到内存中获取数据（这个过程称为**访存**），执行后的结果保存在寄存器或**写回**内存中

![img](https://pic1.zhimg.com/80/v2-948bc3d6e1cd7dd11f31c155fe8c409c_720w.webp)

以指令 mov ax, 0123H 为例，它表示将数据 0123H 存到寄存器 AX 中，在此例中 AX 为 16 位寄存器，一次可以操作 16 位也就是 2 Byte 的数据，所以我们将其称为 16 位 CPU，**CPU 是多少位取决于它一次执行指令的数据带宽，而数据带宽又取决于通用寄存 器的位宽**

1. **更新 IP**：执行完一条指令后,更新 IP 中的值，将其指向下一条指令的起始地址，然后重复步骤 1

由以上总结可知**寻址能力与寄存器位数有关**

接下来我们以执行四条指令为例再来仔细看下 CPU 是如何执行指令的，动图如下：

![动图封面](https://pic1.zhimg.com/v2-0658cbabdab9b04787ebebd256fed160_b.jpg)



看到上面这个动图，细心地你可能会发现两个问题

1. 前文说指令地址是根据 IP 来获取的吗，但上图显示指令地址却是由「CS 左移四位 + IP」计算而来的，与我们所阐述的指令保存在 IP 寄存器中似乎有些出入，这是怎么回事呢？
2. 动图显示的地址是真实物理地址，这样进程之间可以互相访问/改写对方的物理地址，显然是不安全的，那如何才能做到安全访问或者说进程间内存的隔离呢

以上两点其实只要我们了解一下 CPU 的发展历史就明白解决方案了，有了以上的铺垫，在明白了**寻址**，**16/32/64 位 CPU** 等术语的含义后，再去了解 CPU 的发展故事会更容易得多，话不多说，发车

### 2.**Intel CPU 历史发展史**

1971 年世界上第一块 4 位 CPU-4004 微处理器横空出世，1974 年 Intel 研发成功了 8 位 CPU-8080，这两款 CPU 都是使用的绝对物理地址来寻址的，指令地址只存在于 IP 寄存器中（即只使用 IP 寄存器即可确定内存地址）。由于是使用绝对物理地址寻址，也就意味着进程之间的内存数据可能会互相覆盖，很不安全，所以这两者只支持单进程

### 3.**分段**

1978 年英特尔又研究成功了第一款 16 位 CPU - 8086，这款 CPU 可以说是 x86 系列的鼻祖了，设计了 16 位的寄存器和 20 位的地址总线，所以内存地址可以达到 2^20 Byte 即 1M，极大地扩展了地址空间，但是问题来了，由于寄存器只有 16 位，那么 16 位的 IP 寄存器如何能寻址 20 位的地址呢，首先 Intel 工程师设计了一种分段的方法：1M 内存可以分为 16 个大小为 64 K 的段，那么内存地址就可以由「段的起始地址（也叫**段基址**） + **段内偏移**（IP 寄存器中的值）」组成，对于进程说只需要关心 4 个段 ，`代码段`，`数据段`，`堆栈段`，`附加段`，这几个段的段基址分别保存在 CS，DS，SS，ES 这四个寄存器中

![img](https://pic2.zhimg.com/80/v2-a767e55a3c58d445f7a1baa4a2134f41_720w.webp)

这四个寄存器也是 16 位，那怎么访问 20 位的内存地址呢，实现也很简单，将每个寄存器的值左移四位，然后再加上段内偏移即为寻址地址，CPU 都是取代码段 中的指令来执行的，我们以代码段内的寻址为例来计算内存地址，指令的地址 = CS << 4 + IP ，这种方式做到了 20 位的寻址，只要改变 CS，IP 的值，即可实现在 0 到最大地址 0xFFFFF 全部 20 位地址的寻址

举个例子：假设 CS 存的数据为 0x2000,IP 为 0x0003,那么对应的指令地址为

![img](https://pic2.zhimg.com/80/v2-80216f1528b4e7452a265f0c0e05d559_720w.webp)

图示为真实的物理地址计算方式，从中可知， CS 其实保存的是真实物理地址的高 16 位

分段的初衷是为了解决寻址问题，但本质上段寄存器中保存的还是真实物理地址的段基础，且可以随意指定，所以它也无法支持多进程，因为这意味着进程可以随意修改 CS：IP 将其指向任意地址，很可能会覆盖正在运行的其他进程的内存，造成灾难性后果。

我们把这种使用真实物理地址且未加任何限制的寻址方式称为**实模式**（real mode，即实际地址模式）

### 4.**保护模式**

实模式上的物理地址由段寄存器中的段基址:IP 计算而来，而段基址可由用户随意指定，显然非常不安全，于是 Intel 在之后推出了 80286 中启用了保护模式，这个保护是怎么做的呢

首先段寄存器保存的不再是段基址了，而是段选择子（Selector），其结构如下

![img](https://pic4.zhimg.com/80/v2-87607804ad7b2fe0088027d166d2622b_720w.webp)

其中第 3 到 15 位保存的是描述符索引，此索引会根据 TI 的值是 0 还是 1 来选择是到 GDT（全局描述符表，一般也称为段表）还是 LDT 来找段描述符，段描述符保存的是段基址和段长度，找到段基址后再加上保存在 IP 寄存器中的段偏移量即为物理地址，段描述符的长度统一为 8 个字节，而 GDT/LDT 表的基地址保存在 gdtr/ldtr 寄存器中，以 GDT （此时 TI 值为 0）为例来看看此时 CPU 是如何寻址的

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='1080' height='768'></svg>)

可以看到程序中的地址是由段选择子：段内偏移量组成的，也叫**逻辑地址**，在只有分段内存管理的情况下它也被称为**虚拟内存**

GDT 及段描述符的分配都是由操作系统管理的，进程也无法更新 CS 等寄存器中值，这样就避免了直接操作其他进程以及自身的物理地址，达到了保护内存的效果，从而为多进程运行提供了可能，我们把这种寻址方式称为**保护模式**

那么保护模式是如何实现的呢，细心的你可能发现了上图中在段选择子和段描述符中里出现了 **RPL** 和 **DPL** 这两个新名词，这两个表示啥意思呢？这就涉及到一个概念：**特权级**

### 5.**特权级**

我们知道 CPU 是根据机器指令来执行的，但这些指令有些是非常危险的，比如**清内存**，**置时钟**，**分配系统资源**等，这些指令显然不能让普通的进程随意执行，应该始终控制在操作系统中执行，所以要把操作系统和普通的用户进程区分开来

我们把一个进程的虚拟地址划分为两个空间，**用户空间**和**内核空间**，用户空间即普通进程所处空间，内核空间即操作系统所处空间

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='395' height='317'></svg>)

当 CPU 运行于用户空间（执行用户空间的指令）时，它处于用户态，只能执行普通的 CPU 指令 ，当 CPU 运行于内核空间（执行内核空间的指令）时，它处于**内核态**，可以执行清内存，置时钟，读写文件等特权指令，那怎么区分 CPU 是在用户态还是内核态呢，CPU 定义了四个特权等级，如下，从 0 到 3，特权等级依次递减，当特权级为 0 时，CPU 处于内核态，可以执行任何指令，当特权级为 3 时，CPU 处于用户态，**在 Linux 中只用了 Ring 0，Ring 3 两个特权等级**

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='563' height='417'></svg>)

那么问题来了，怎么知道 CPU 处于哪一个特权等级呢，还记得上文中我们提到的段选择子吗

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='446' height='153'></svg>)

其中的 RPL 表示请求特权（(Requested privilege level)）我们把当前保存于 CS 段寄存器的段选择子中的 RPL 称为 CPL（current priviledge level），即当前特权等级，可以看到 RPL 有两位，刚好对应着 0,1,2,3 四个特权级，而上文提到的 DPL 表示段描述符中的特权等级（Descriptor privilege level）知道了这两个概念也就知道保护模式的实现原理了，CPU 会在两个关键点上对内存进行保护

1. 目标段选择子被加载时
2. 当通过线性地址（在只有段式内存情况下，线性地址为物理地址）访问一个内存页时。由此可见，保护也反映在内存地址转换的过程之中，既包括分段又包括分页（后文分提到分页）

CPU 是怎么保护内存的呢，它会对 CPL，RPL，DPL 进行如下检查

![img](data:image/svg+xml;utf8,<svg xmlns='http://www.w3.org/2000/svg' width='605' height='304'></svg>)

只有 CPL <= DPL 且 RPL <= DPL 时，才会加载目标代码段执行，否则会报一般保护异常 （General-protection exception）

那么特权等级（也就是 CPL）是怎么变化的呢，我们之前说了 CPU 运行于用户空间时，处于用户态，特权等级为 3，运行于内核空间时，处于内核态，特权等级为 0，所以也可以换个问法 CPU 是如何从用户空间切换到内核空间或者从内核空间切换到用户空间的，这就涉及到一个概念：**系统调用**

### 6.**系统调用**

我们知道用户进程虽然不能执行特权指令，但有时候也需要执行一些读写文件，发送网络包等操作，而这些操作又只能让操作系统来执行，那该怎么办呢，可以让操作系统提供接口，让用户进程来调用即可，我们把这种方式叫做**系统调用**，系统调用可以直接由应用程序调用，或者通过调用一些公用函数库或 shell（这些函数库或 shell 都封装了系统调用接口）等也可以达到间接调用系统调用的目的。通过系统调用，应用程序实现了**陷入（trap）内核态**的目的，这样就从用户态切换到了内核态中，如下

![img](https://pic3.zhimg.com/80/v2-4d9db473e02d4fffbe4fe78367cb0a06_720w.webp)

应用程序通过系统调用陷入内核态

那么系统调用又是怎么实现的呢，主要是靠**中断**实现的，接下来我们就来了解一下什么是中断

### 7.**中断**

陷入内核态的系统调用主要是通过一种 **trap gate**（陷阱门）来实现的，它其实是软件中断的一种，由 CPU 主动触发给自己一个中断向量号，然后 CPU 根据此中断向量号就可以去中断向量表找到对应的**门描述符**，门描述符与 GDT 中的段描述符相似，也是 8 个字节，门描述符中包含段选择子，段内偏移，DPL 等字段 ，然后再根据段选择子去 GDT（或者 LDT，下图以 GDT 为例） 中查找对应的段描述符，再找到段基地址，然后根据中断描述符表的段内偏移即可找到中断处理例程的入口点,整个中断处理流程如下

![img](https://pic4.zhimg.com/80/v2-366e353b58c263cfe1d74c7d5de8cc27_720w.webp)

**画外音**：上图中门描述符和段描述符只画出了关键的几个字段，省略了其它次要字段

当然了，不是随便发一个中断向量都能被执行，只有满足一定条件的中断才允许被普通的应用程序调用，从发出软件中断再到执行中断对应的代码段会做如下的检查

![img](https://pic4.zhimg.com/80/v2-96ee238ba675b970d27860fb2782728f_720w.webp)

一般应用程序发出软件中断对应的向量号是大家熟悉的 int 0x80（int 代表 interrupt），它的门描述符中的 DPL 为 3,所以能被所有的用户程序调用，而它对应的目标代码段描述符中的 DPL 为 0，所以当通过中断门检查后（即 CPL <= 门描述符中的 DPL 成立），CPU 就会将 CS 寄存器中的 RPL（3） 替换为目标代码段描述符的 DPL（0），替换后的 CPL 也就变成了 0，通过这种方式完成了从用户态到内核态的替换，当中断代码执行后执行 iret 指令又会切换回用户态

另外当执行中断程序时，还需要首先把当前用户进程中对应的堆栈，返回地址等信息，以便切回到用户态时能恢复现场

可以看到 int 80h 这种软件中断的执行又是检查特权级，又是从用户态切换到内核态，又是保存寄存器的值，可谓是非常的耗时，光看一下以下图示就知道像 int 0x80 这样的软件中断开销是有多大了

![img](https://pic1.zhimg.com/80/v2-1f02dd51825cd75b4bc9394f042a26e0_720w.webp)

系统调用

所以后来又开发出了 SYSENTER`/`SYSCALL 这样快速系统调用的指令，它们取消了权限检查，也不需要在中断描述表（Interrupt Descriptor Table、IDT）中查找系统调用对应的执行过程，也不需要保存堆栈和返回地址等信息，而是直接进入*CPL 0*，并将新值加载到与代码和堆栈有关的寄存器当中（cs，eip，ss 和 esp），所以极大地提升了性能

### 8.**分段内存的优缺点**

使用了保护模式后，程序员就可以在代码中使用了**段选择子：段偏移量**的方式来寻址，这不仅让多进程运行成为了可能，而且也解放了程序员的生产力，我们完全可以认为程序拥有所有的内存空间（虚拟空间），因为段选择子是由操作系统分配的，只要操作系统保证不同进程的段的虚拟空间映射到不同的物理空间上，不要重叠即可，也就是说虽然各个程序的虚拟空间是一样的，但由于它们映射的物理地址是不同且不重叠的，所以是能正常工作的，但是为了方便映射，一般要求在物理空间中分配的段是连续的（这样只要维护映射关系的起始地址和对应的空间大小即可）

![img](https://pic1.zhimg.com/80/v2-f85af2b41f4dbaba65d1340cde849df8_720w.webp)

段式内存管理-虚拟空间与实际物理内存的映射

但段式内存管理缺点也很明显：内存碎片可能很大，举个例子

![img](https://pic1.zhimg.com/80/v2-13efae555312c8f873143e9d89b59670_720w.webp)

如上图示，连续加载了三个程序到内存中，如果把 Chrome 关闭了，此时内存中有两段 128 M的空闲内存，但如果此时要加载一个 192 M 的程序 X 却有心无力了 ，因为段式内存需要划分出一块**连续的**内存空间，此时你可以选择把占 256 M 的 Python 程序先 swap 到磁盘中，然后紧跟着 512 M 内存的后面划分出 256 M 内存，再给 Python 程序 swap 到这块物理内存中，这样就腾出了连续的 256 M 内存，从而可以加载程序 X 了，但这种频繁地将几十上百兆内存与硬盘进行 swap 显然会对性能造成严重的影响，毕竟谁都知道内存和硬盘的读写速度可是一个天上一个地上，如果一定要交换，能否每次 swap 得能少一点，比如只有几 K，这样就能满足我们的需求，分页内存管理就诞生了

### 9.**内存分页**

1985 年 intel 推出了 32 位处理器 80386，也是首款支持分页内存的 CPU

和分段这样连续分配一整段的空间给程序相比，分页是把整个物理空间切成一段段固定尺寸的大小，当然为了映射，虚拟地址也需要切成一段段固定尺寸的大小，这种固定尺寸的大小我们一般称其为页，在 LInux 中一般每页的大小为 4KB，这样虚拟地址和物理地址就通过页来映射起来了

![img](https://pic4.zhimg.com/80/v2-15cd86c78debe98b2bd2a769c3a4698f_720w.webp)

当然了这种映射关系是需要一个映射表来记录的，这样才能把虚拟地址映射到物理内存中，给定一个虚拟地址，它最终肯定在某个物理页内，所以虚拟地址一般由「页号+页内偏移」组成，而映射表项需要包含物理内存的页号，这样只要将页号对应起来，再加上页内偏移，即可获取最终的物理内存

![img](https://pic3.zhimg.com/80/v2-c15d7f741f112db2867918683e3f43fa_720w.webp)

于是问题来了，映射表（也称页表）该怎么设计呢,我们以 32 位虚拟地址位置来看看，假设页大小为 4K（2^12），那么至少需要 2^20 也就是 100 多万个页表项才能完全覆盖所有的虚拟地址，假设每一个页表项 4 个字节，那就意味着为一个进程的虚拟地址就需要准备 2^20 * 4 B = 4 M 的页表大小，如果有 100 个进程，就意味着光是页表就要占用 400M 的空间了，这显然是非常巨大的开销，那该怎么解决这个页表空间占用巨大的问题呢

我们注意到现在的做法是一次性为进程分配了占用其所有虚拟空间的页表项，但实际上一个进程根本用不到这么巨大的虚拟空间，所以这种分配方式无疑导致很多分配的页表项白白浪费了，那该怎么办，答案是**分级管理，等真正需要分配物理空间的时候再分配**，其实大家可以想想我们熟悉的 windows 是怎么分配的，是不是一开始只分配了 C 盘，D盘，E盘，等要存储的时候，先确定是哪个盘，再在这个盘下分配目录，然后再把文件存到这个目录下，并不会一开始就把所有盘的空间给分配完的

![img](https://pic4.zhimg.com/80/v2-87502935e04b9a9717000361aae85617_720w.webp)

同样的道理，以 32 位虚拟地址为例，我们也可以对页表进行分级管理, 页表项 2^20 = 2^10 * 2^10 = 1024 * 1024，我们把一个页表分成两级页表，第一级页表 1024 项，每一项都指向一个包含有 1024 个页表项的二级页表

![img](https://pic2.zhimg.com/80/v2-46bb9e8bb0005f2ce5ffd1c091324f5d_720w.webp)

图片来自《图解系统》

这样只有在一级页表中的页表项被分配的时候才会分配二级页表，极大的节省了空间，我们简单算下，假设 4G 的虚拟空间进程只用了 20%（已经很大了，大部分用不到这么多），那么由于一级页表空间为 1024 *4 = 4K，总的页表空间为 4K+ 0.2 * 4M = 0.804M，相比于原来的 4M 是个巨大的提升！

那么对于分页保护模式又是如何起作用的呢，同样以 32 位为例，它的二级页表项（也称 page table entry）其实是以下结构

![img](https://pic4.zhimg.com/80/v2-d66413adbe4158a5e49b595e597e442f_720w.webp)

注意第三位（也就是 2 对应的位置）有个 U/S，它其实就是代表特权级，表示的是用户/超级用户标志。为 1 时，允许所有特权级别的程序访问；为 0 时，仅允许特权级为0、1、2（Linux 中没有 1，2）的程序（也就是内核）访问。页目录中的这个位对其所映射的所有页面起作用

既然分页这么好，那么分段是不是可以去掉了呢，理论上确实可以，但 Intel 的 CPU 严格执行了 backward compatibility（回溯兼容），也就是说最新的 CPU 永远可以运行针对早期 CPU 开发的程序，否则早期的程序就得针对新 CPU 架构重新开发了（早期程序针对的是 CPU 的段式管理进行开发），这无论对用户还是开发者都是不能接受的（别忘了安腾死亡的一大原因就是由于不兼容之前版本的指令），兼容性虽然意味着每款新的 CPU 都得兼容老的指令，所背的历史包袱越来越重，但对程序来说能运行肯定比重新开发好，所以既然早期的 CPU 支持段，那么自从 80386 开始的所有 CPU 也都得支持段，而分页反而是可选的，也就意味着这些 CPU 的内存管理都是段页式管理，逻辑地址要先经过段式管理单元转成线性地址（也称虚拟地址），然后再经过页式管理单元转成物理内存，如下

![img](https://pic1.zhimg.com/80/v2-ad2df8cd742651f9220d2dacf16f40e4_720w.webp)

分页是可选项

在 Linux 中，虽然也是段页式内存管理，但它统一把 CS，DS，SS，ES 的段基址设置为了 0，段界限也设置为了整个虚拟内存的长度，所有段都分布在同一个地址空间，这种内存模式也叫平坦内存模型（flat memory model）

![img](https://pic4.zhimg.com/80/v2-e5e27bea90bc299a9c9efe283cf5bcf3_720w.webp)

平坦内存模型

我们知道逻辑地址由段选择子：段内偏移地址组成，既然段选择子指向的段基地址为 0，那也就意味着段内偏移地址即为即为线性地址（也就是虚拟地址），由此可知 Linux 中所有程序的代码都使用了虚拟地址，通过这种方式巧妙地绕开了分段管理，分段只起到了访问控制和权限的作用（别忘了各种权限检查依赖 DPL，RPL 等特权字段，特权极转移也依赖于段选择子中的 DPL 来切换的）

### 10.**总结**

看完本文相信大家对实模式，保护模式，特权级转换，分段，分页等概念应该有了比较清晰的认识。

我们简单总结一下，CPU 诞生之间，使用的绝对物理内存来寻址（也就是实模式），随后随着 8086 的诞生，由于工艺的原因，虽然地址总线是 20 位，但寄存器却只有 16 位，一个难题出现了，16 位的寄存器该怎么寻址 20 位的内存地址呢，于是段的概念被提出了，段的出现虽然解决了寻址问题，但本质上 CS << 4 + IP 的寻址方式依然还是绝对物理地址，这样的话由于地址会互相覆盖，显然无法做到多进程运行，于是保护模式被提出了，保护就是为了物理内存免受非法访问，于是用户空间，内核空间，特权级也被提出来了，段寄存器里保存的不再是段基址，而是段选择子，由操作系统分配，用户也无法随意修改段选择子，必须通过中断的形式才能从用户态陷入内核态，中断执行的过程也需要经历特权级的检查，检查通过之后特权级从 3 切换到了 0，于是就可以放心合法的执行特权指令了。可以看到，通过操作系统分配段选择子+中断的方式内存得到了有效保护，但是分段可能造成内存碎片过大以致频繁 swap 会影响性能的问题，于是分页出现了，保护模式+分页终于可以让多进程，高效调度成为了可能

原文地址：https://zhuanlan.zhihu.com/p/536026548

作者：linux

# 【NO.306】从6种I/O模式谈谈协程的作用

假设磁盘上有10个文件，你需要读取的内存，那么你该怎么用代码实现呢？

在接着往下看之前，先自己想一想这个问题，看看自己能想出几种方法，各自有什么样的优缺点。想清楚了吗(还在看吗)，想清楚了我们继续往下看。

### **1.最简单的方法——串行**

这可能是大多数同学都能想到的最简单方法，那就是一个一个的读取，读完一个接着读下一个。用代码表示是这样的：

```text
for file in files:
  result = file.read()
  process(result)
```

是不是非常简单，我们假设每个文件读取需要1分钟，那么10个文件总共需要10分钟才能读取完成。这种方法有什么问题呢？实际上这种方法只有一个问题，那就是**慢**。除此之外，其它都是优点：

1. 代码简单，容易理解
2. 可维护性好，这代码交给谁都能维护的了(论程序员的核心竞争力在哪里)

那么慢的问题该怎么解决呢？有的同学可能已经想到了，为啥要一个一个读取呢？并行读取不就可以加快速度了吗。

### 2.**稍好的方法，并行**

那么，该怎么并行读取文件呢？显然，地球人都知道，线程就是用来并行的。我们可以同时开启10个线程，每个线程中读取一个文件。用代码实现就是这样的：

```text
def read_and_process(file):
  result = file.read()
  process(result)

def main():
  files = [fileA，fileB，fileC......]
  for file in files:
     create_thread(read_and_process,
                   file).run()
  # 等待这些线程执行完成
```

怎么样，是不是也非常简单。那么这种方法有什么问题吗？在开启10个线程这种问题规模下没有问题。现在我们把问题难度加大，假设有10000个文件，需要处理该怎么办呢？有的同学可能想10个文件和10000个文件有什么区别吗，直接创建10000个线程去读不可以吗？实际上这里的问题其实是说创建多个线程有没有什么问题。我们知道，虽然线程号称“轻量级进程”，虽然是轻量级但当数量足够可观时依然会有性能问题。这里的问题主要有这样几个方面：

1. 创建线程需要消耗系统资源，像内存等(想一想为什么？)
2. 调度开销，尤其是**当线程数量较多且都比较繁忙时**(同样想一想为什么？)
3. 创建多个线程不一定能加快I/O(如果此时设备处理能力已经饱和)

既然线程有这样那样的问题，那么还有没有更好的方法？答案是肯定的，并行编程不一定只能依赖线程这种技术。

### **3.事件驱动 + 异步**

没错，即使在单个线程中，使用事件驱动+异步也可以实现IO并行处理，Node.js就是非常典型的例子。为什么单线程也可以做到并行呢？这是基于这样两个事实：

1. 相对于CPU的处理速度来说，IO是非常慢的
2. IO不怎么需要计算资源

因此，当我们发起IO操作后为什么要一直等着IO执行完成呢？**在IO执行完之前的这段时间处理其它IO难道不香吗**？这就是为什么单线程也可以并行处理多个IO的本质所在。回到我们的例子，该怎样用事件驱动+异步来改造上述程序呢？实际上非常简单。首先我们需要创建一个event loop，这个非常简单：

```text
event_loop = EventLoop()
```

然后，我们需要往event loop中加入原材料，也就是需要监控的event，就像这样：

```text
def add_to_event_loop(event_loop, file):
   file.asyn_read() # 文件异步读取
   event_loop.add(file)
```

注意当执行file.asyn_read这行代码时会**立即返回**，不会阻塞线程，当这行代码返回时可能文件还没有真正开始读取，这就是所谓的异步。file.asyn_read这行代码的真正目的仅仅是**发起IO**，而不是等待IO执行完成。此后我们将该IO放到event loop中进行监控，也就是event_loop.add(file)这行代码的作用。一切准备就绪，接下来就可以等待event的到来了：

```text
while event_loop:
   file = event_loop.wait_one_IO_ready()
   process(file.result)
```

我们可以看到，event_loop会一直等待直到有文件读取完成（event_loop.wait_one_IO_ready()），这时我们就能得到读完的文件了，接下来处理即可。全部代码如下所示：

```text
def add_to_event_loop(event_loop, file):
   file.asyn_read() # 文件异步读取
   event_loop.add(file)

def main():
  files = [fileA，fileB，fileC ...]
  event_loop = EventLoop()
  for file in files:
      add_to_event_loop(event_loop, file)
      
  while event_loop:
     file = event_loop.wait_one_IO_ready()
     process(file.result)
```

### **4.多线程 VS 单线程 + event loop**

接下来我们看下程序执行的效果。在多线程情况下，假设有10个文件，每个文件读取需要1秒，那么很简单，并行读取10个文件需要1秒。那么对于单线程+event loop呢？我们再次看下event loop + 异步版本的代码：

```text
def add_to_event_loop(event_loop, file):
   file.asyn_read() # 文件异步读取
   event_loop.add(file)

def main():
  files = [fileA，fileB，fileC......]
  event_loop = EventLoop()
  for file in files:
      add_to_event_loop(event_loop, file)
      
  while event_loop:
     file = event_loop.wait_one_IO_ready()
     process(file.result)
```

对于add_to_event_loop，由于文件异步读取，因此该函数可以瞬间执行完成，真正耗时的函数其实就是event loop的等待函数，也就是这样：

```text
file = event_loop.wait_one_IO_ready()
```

我们知道，一个文件的读取耗时是1秒，因此该函数在1s后才能返回，但是，但是，接下来是重点。但是虽然该函数wait_one_IO_ready会等待1s，不要忘了，我们利用这两行代码同时发起了10个IO操作请求。

```text
for file in files:  add_to_event_loop(event_loop, file)
```

因此在event_loop.wait_one_IO_ready等待的1s期间，剩下的9个IO也完成了，也就是说event_loop.wait_one_IO_ready函数只是在第一次循环时会等待1s，但是此后的9次循环会直接返回，**原因就在于剩下的9个IO也完成了**。因此整个程序的执行耗时也是1秒。是不是很神奇，我们只用一个线程就达到了10个线程的效果。这就是event loop + 异步的威力所在。

### 5.**一个好听的名字：Reactors模式**

本质上，我们上述给出的event loop简单代码片段做的事情本质上和生物一样：给出刺激，做出反应。我们这里的给出event，然后处理event。这本质上就是所谓的Reactors模式。现在你应该明白所谓的Reactors模式是怎么一回事了吧。所谓的一些看上去复杂的异步框架其核心不过就是这里给出的代码片段，只是这些框架可以支持更加复杂的多阶段任务处理以及各种类型的IO。而我们这里给出的代码片段只能处理文件读取这一类IO。

### **6.把回调也加进来**

如果我们需要处理各种类型的IO上述代码片段会有什么问题吗？问题就在于上述代码片段就不会这么简单了，针对不同类型会有不同的处理方法，因此上述process方法需要判断IO类型然后有针对性的处理，这会使得代码越来越复杂，越来越难以维护。幸好我们也有应对策略，这就是回调。我们可以把IO完成后的处理任务封装到回调函数中，**然后和IO一并注册到event loop**。就像这样：

```text
def IO_type_1(event_loop, io):
  io.start()
  
  def callback(result):
    process_IO_type_1(result)
    
  event_loop.add((io, callback))
```

这样，event_loop在检测到有IO完成后就可以把该IO和关联的callback处理函数一并检索出来，直接调用callback函数就可以了。

```text
while event_loop:
   io, callback = event_loop.wait_one_IO_ready()
   callback(io.result)
```

看到了吧，这样event_loop内部就极其简洁了，even_loop根本就不关心该怎么处理该IO结果，这是注册的callback该关心的事情，event_loop需要做的仅仅就是拿到event以及相应的处理函数callback，然后调用该callback函数就可以了。现在我们可以同单线程来并发编程了，也使用callback对IO处理进行了抽象，使得代码更加容易维护，想想看还有没有什么问题？

### 7.**回调函数的问题**

虽然回调函数使得event loop内部更加简洁，但依然有其它问题，让我们来仔细看看回调函数：

```text
def start_IO_type_1(event_loop, io):
  io.start()
  
  def callback(result):
    process_IO_type_1(result)
    
  event_loop.add((io, callback))
```

从上述代码中你能看到什么问题吗？在上述代码中，一次IO处理过程被分为了两部分：

1. 发起IO
2. IO处理

其中第2部分放到了回调函数中，这样的异步处理天然不容易理解，这和我们熟悉的发起IO，等待IO完成、处理IO结果的同步模块有很大差别。这里的给的例子很简单，所以你可能不以为意，但是当处理的任务非常复杂时，可能会出现回调函数中嵌套回调函数，也就是回调地狱，这样的代码维护起来会让你怀疑为什么要称为一名苦逼的码农。

### 8.**问题出在哪里**

让我们再来仔细的看看问题出在了哪里？同步编程模式下很简单，但是同步模式下发起IO，线程会被阻塞，这样我们就不得不创建多个线程，但是创建过多线程又会有性能问题。这样为了发起IO后不阻塞当前线程我们就不得不采用异步编程+event loop。在这种模式下，异步发起IO不会阻塞调用线程，我们可以使用单线程加异步编程的方法来实现多线程效果，但是在这种模式下处理一个IO的流程又不得不被拆分成两部分，这样的代码违反程序员直觉，因此难以维护。那么很自然的，有没有一种方法既能有同步编程的简单理解又会有异步编程的非阻塞呢？答案是肯定的，这就是协程。

### **9.Finally！终于到了协程**

利用协程我可以以同步的形式来异步编程。这是什么意思呢？我们之所以采用异步编程是为了发起IO后不阻塞当前线程，而是用协程，程序员可以自行决定在什么时刻挂起当前协程，这样也不会阻塞当前线程。而协程最棒的一点就在于**挂起后可以暂存执行状态**，**恢复运行后可以在挂起点继续运行**，这样我们就不再需要像回调那样将一个IO的处理流程拆分成两部分了。因此我们可以在发起异步IO，这样不会阻塞当前线程，同时在发起异步IO后挂起当前协程，当IO完成后恢复该协程的运行，这样我们就可以实现同步的方式来异步编程了。接下来我们就用协程来改造一下回调版本的IO处理方式：

```text
def start_IO_type_1(io):
  io.start() # IO异步请求
  yield      # 暂停当前协程 
  process_IO_type_1(result) # 处理返回结果
```

此后我们要把该协程放到event loop中监控起来：

```text
def add_to_event_loop(io, event_loop):
  coroutine = start_IO_type_1(io)
  next(coroutine)
  event_loop.add(coroutine)
```

最后，当IO完成后event loop检索出相应的协程并恢复其运行：

```text
while event_loop:
   coroutine = event_loop.wait_one_IO_ready()
   next(coroutine)
```

现在你应该看出来了吧，上述代码中没有回调，也没有把处理IO的流程拆成两部分，整体的代码都是以同步的方式来编写，最棒的是依然能达到异步的效果。实际上你会看到，采用协程后我们依然需要基于事件编程的event loop，因为本质上**协程并没有改变IO的异步处理本质**，只要IO是异步处理的那么我们就必须依赖event loop来监控IO何时完成，只不过我们采用协程消除了对回调的依赖，整体编程方式上还是采用程序员最熟悉也最容易理解的同步方式。

### **10.总结**

看上去简简单单的IO实际上一点都不简单吧。为了高效进行IO操作，我们采用的技术是这样演进的：

1. 单线程串行 + 阻塞式IO(同步)
2. 多线程并行 + 阻塞式IO(并行)
3. 单线程 + 非阻塞式IO(异步) + event loop
4. 单线程 + 非阻塞式IO(异步) + event loop + 回调
5. Reactor模式(更好的单线程 + 非阻塞式IO+ event loop + 回调)
6. 单线程 + 非阻塞式IO(异步) + event loop + 协程

最终我们采用协程技术获取到了异步编程的高效以及同步编程的简单理解，这也是当今**高性能服务器**常用的一种技术组合。希望这篇文章能对你理解高效IO有所帮助。

原文地址：https://zhuanlan.zhihu.com/p/532807036

作者：linux

# 【NO.307】数据从应用层的应用进程到最后的网络包是怎么一步步封装的？TCP怎么拆分？IP怎么分片？

![img](https://pic1.zhimg.com/80/v2-82994ab8b40a148b258251469813c26c_720w.webp)

上图是一个整体的网络包的结构，可以看到网络包层层封装的结构

### 1.因此如果问一句，那数据从应用层的应用进程到最后的网络包是怎么一步步封装的呢？

答：比较概括性的回答是：（为便于讨论，假设是一个web服务）

首先应用层的应用进程将http请求报文下发到运输层

运输层的TCP协议栈给http请求报文添加TCP头部封装成TCP报文段，然后下发到网络层

网络层的IP协议栈给TCP报文段添加上IP头部封装成IP包，然后下发到数据链路层

数据链路层也有各种协议，假设是以太网协议，那么会给IP包加上帧头和帧尾，形成帧，然后下发到物理层

物理层则将帧视为字节流，完全转化为相应的物理信号(电信号或光信号等)，在线缆、光纤等媒介中传播

### 2.进一步提问：那运输层收到应用层下发的数据后，TCP协议是怎么看待以及怎么打包封装的？什么时候会拆分？

1、TCP视应用层下发的http请求报文为字节流，TCP协议会根据规定的MSS（最大报文长度：规定了TCP报文所能携带的数据载荷的大小）判断是否进行拆分

2、如果字节流长度大于MSS，则需要进行拆分，拆分成合理的几块，然后为每一块添加上合适的TCP头部

3、TCP头部中的序号字段就是为此服务的，最终形成一个个TCP报文段，接收方对应的运输层收到这些TCP报文段后可以根据TCP头部信息进行组装还原原来的http请求报文

![img](https://pic2.zhimg.com/80/v2-d032d1c079979626fdc6d5f6c67f29f5_720w.webp)

### 3.再进一步提问：那网络层收到运输层下发的TCP报文段后，IP协议是怎么看待以及怎么打包封装的？什么时候会拆分？

1、IP协议会根据输出端口（其实也就是被链路层的各种协议类型所规定）的MTU（最大传输单元：它规定了IP网络包的最大长度包括首部和数据载荷）进行判断是否进行分片；例如以太网的MTU=1500字节

2、若TCP报文段小于MTU 减 IP头部长度，则IP不需要对TCP报文段分片

3、若TCP报文段大于MTU 减 IP头部长度，则IP需要对TCP报文段分片

分片是IP对整个TCP报文段一视同仁，也就是不会区分是TCP头部还是数据载荷，如下图所示

IP头部字段中的标识、标志、片偏移字段就是为此服务的

接收端的网络层收到这些分片过的IP数据报，会根据IP头部中的字段信息，对分片进行组装，还原TCP报文段后交给上层也即运输层

![img](https://pic3.zhimg.com/80/v2-ff5c9d06d1e634fa0b530375422dce22_720w.webp)

注意：

- IP分片是在TCP拆分后又进行了一次拆分；在接收端刚好相反，先IP组装分片，然后交给运输层TCP，之后再组装TCP数据报文段，最后交给应用层的应用进程
- TCP拆分：依据是MSS（最大报文段长度）
- IP分片：依据是各种转发端口的MTU（最大传输单元）

### 4.**补充**

![img](https://pic2.zhimg.com/80/v2-ae01c72d5a3acaba7ffb1cbfd4a5e419_720w.webp)

以太网帧的最大数据帧长度为1518字节

帧头包含目标MAC地址(6个字节)、源MAC地址(6个字节)、上层协议类型(2个字节)共14字节

帧的尾部是FCS校验位(4个字节)

故以太网帧的MTU = 1518 - 14 - 4 = 1500 字节

又因为IP头部最小为20字节，TCP头部最小为20字节，以及在实际的应用中，通常TCP会加一个12字节的时间戳，这些共计52字节

所以单个TCP每次能打包的最大数据量为MTU - 52 = 1448字节

原文地址：https://zhuanlan.zhihu.com/p/532302700

作者：linux

# 【NO.308】谈谈QUIC协议原理

QUIC，又名HTTP3，是近年来诞生的非常厉害的传输协议，它利用UDP解决了当前基于TCP协议的HTTP的许多问题，提升了在弱网环境下的网络通信体验。让我们来一探究竟！

## 1.QUIC是啥？

### **1.1 什么是QUIC？**

QUIC(Quick UDP Internet Connection)是谷歌推出的一套基于UDP的传输协议，它实现了TCP + HTTPS + HTTP/2的功能，目的是保证可靠性的同时降低网络延迟。因为UDP是一个简单传输协议，基于UDP可以摆脱TCP传输确认、重传慢启动等因素，建立安全连接只需要一的个往返时间，它还实现了HTTP/2多路复用、头部压缩等功能。

众所周知UDP比TCP传输速度快，TCP是可靠协议，但是代价是双方确认数据而衍生的一系列消耗。其次TCP是系统内核实现的，如果升级TCP协议，就得让用户升级系统，这个的门槛比较高，而QUIC在UDP基础上由客户端自由发挥，只要有服务器能对接就可以。

![img](https://pic4.zhimg.com/80/v2-69ad83e099598128d1319144948c0463_720w.webp)

图1-1 HTTP与QUIC

### **1.2 HTTP协议发展**

**1.2.1 HTTP历史进程**

◎HTTP 0.9（1991年）只支持get方法不支持请求头；

◎HTTP 1.0（1996年）基本成型，支持请求头、富文本、状态码、缓存、连接无法复用；

◎HTTP 1.1（1999年）支持连接复用、分块发送、断点续传；

◎HTTP 2.0（2015年）二进制分帧传输、多路复用、头部压缩、服务器推送等；

◎HTTP 3.0（2018年）QUIC 于2013年实现、2018年正式更名为HTTP3；

**1.2.2 HTTP1.和HTTP1.1**

◎**队头阻塞**：下个请求必须在前一个请求返回后才能发出，导致带宽无法被充分利用，后续请求被阻塞（HTTP 1.1 尝试使用流水线（Pipelining）技术，但先天 FIFO（先进先出）机制导致当前请求的执行依赖于上一个请求执行的完成，容易引起队头阻塞，并没有从根本上解决问题）；

◎**协议开销大**：header里携带的内容过大，且不能压缩，增加了传输的成本；

◎**单向请求**：只能单向请求，客户端请求什么，服务器返回什么；

**HTTP 1.0 和 HTTP 1.1 的区别：**

| HTTP 1.0                                               | HTTP1.1                                                      |
| ------------------------------------------------------ | ------------------------------------------------------------ |
| 仅支持保持短暂的TCP连接（连接无法复用）                | 默认支持长连接（请求可复用TCP连接）                          |
| 不支持断点续传                                         | 支持断点续传（通过在 Header 设置参数）                       |
| 前一个请求响应到达之后下一个请求才能发送，存在队头阻塞 | 优化了缓存控制策略                                           |
| ------                                                 | 管道化，可以一次发送多个请求，但是响应仍是顺序返回，仍然无法解决队头阻塞的问题 |
| ------                                                 | 新增错误状态码通知                                           |
| ------                                                 | 请求消息和响应消息都支持Host头域                             |

**1.2.3 HTTP2**

解决 HTTP1 的一些问题，但是解决不了底层 TCP 协议层面上的队头阻塞问题。

1.**二进制传输**：二进制格式传输数据解析起来比文本更高效；

2.**多路复用**：重新定义底层 http 语义映射，允许同一个连接上使用请求和响应双向数据流。同一域名只需占用一个 TCP 连接，通过数据流（Stream）以帧为基本协议单位，避免了因频繁创建连接产生的延迟，减少了内存消耗，提升了使用性能，并行请求，且慢的请求或先发送的请求不会阻塞其他请求的返回；

3.**Header压缩**：减少请求中的冗余数据，降低开销；

4.**服务端可以主动推送**：提前给客户端推送必要的资源，这样就可以相对减少一点延迟时间；

5.**流优先级**：数据传输优先级可控，使网站可以实现更灵活和强大的页面控制；

6.**可重置**：能在不中断 TCP 连接的情况下停止数据的发送；

**缺点**：HTTP 2中，多个请求在一个TCP管道中的，出现了丢包时，HTTP 2的表现反倒不如HTTP 1.1了。因为 TCP 为了保证可靠传输，有个特别的“丢包重传”机制，丢失的包必须要等待重新传输确认，HTTP 2出现丢包时，整个 TCP 都要开始等待重传，那么就会阻塞该TCP连接中的所有请求。而对于 HTTP 1.1 来说，可以开启多个 TCP 连接，出现这种情况反到只会影响其中一个连接，剩余的 TCP 连接还可以正常传输数据。

**1.2.4 HTTP3 —— HTTP Over QUIC**

HTTP 是建立在 TCP 协议之上，所有 HTTP 协议的瓶颈及其优化技巧都是基于 TCP 协议本身的特性，HTTP2 虽然实现了多路复用，底层 TCP 协议层面上的问题并没有解决（HTTP 2.0 同一域名下只需要使用一个 TCP 连接。但是如果这个连接出现了丢包，会导致整个 TCP 都要开始等待重传，后面的所有数据都被阻塞了），而 HTTP3 的 QUIC 就是为解决 HTTP2 的 TCP 问题而生。

## 2.QUIC的关键特性

关于 QUIC 的原理，相关介绍的文章很多，这里再列举一下 QUIC 的重要特性。这些特性是 QUIC 得以被广泛应用的关键。不同业务也可以根据业务特点利用 QUIC 的特性去做一些优化。同时，这些特性也是我们去提供 QUIC 服务的切入点。

### **2.1 连接迁移**

**2.1.1 TCP的连接重连之痛**

一条 TCP 连接是由**四元组**标识的（**源 IP，源端口，目的 IP，目的端口**）。什么叫连接迁移呢？就是当其中任何一个元素发生变化时，这条连接依然维持着，能够保持业务逻辑不中断。当然这里面主要关注的是客户端的变化，因为客户端不可控并且网络环境经常发生变化，而服务端的 IP 和端口一般都是固定的。

比如大家使用手机在 WIFI 和 4G 移动网络切换时，客户端的 IP 肯定会发生变化，需要重新建立和服务端的 TCP 连接。

又比如大家使用公共 NAT 出口时，有些连接竞争时需要重新绑定端口，导致客户端的端口发生变化，同样需要重新建立 TCP 连接。

所以从 TCP 连接的角度来讲，这个问题是无解的。

**2.1.2 基于UDP的QUIC连接迁移实现**

当用户的地址发生变化时，如 WIFI 切换到 4G 场景，基于 TCP 的 HTTP 协议无法保持连接的存活。QUIC 基于连接 ID 唯一识别连接。当源地址发生改变时，QUIC 仍然可以保证连接存活和数据正常收发。

那 QUIC 是如何做到连接迁移呢？很简单，QUIC是基于UDP协议的，任何一条 QUIC 连接不再以 IP 及端口四元组标识，而是以一个 64 位的随机数作为 ID 来标识，这样就算 IP 或者端口发生变化时，只要 ID 不变，这条连接依然维持着，上层业务逻辑感知不到变化，不会中断，也就不需要重连。

由于这个 ID 是客户端随机产生的，并且长度有 64 位，所以冲突概率非常低。

![img](https://pic2.zhimg.com/80/v2-3abedd9b0021a181ac82c86dc6dc7229_720w.webp)

图2-1 TCP 和 QUIC 在 Wi-Fi 和 cellular 切换时，唯一标识的不同情况

### **2.2 低连接延时**

**2.2.1 TLS的连接时延问题**

以一次简单的浏览器访问为例，在地址栏中输入[https://www.abc.com](https://link.zhihu.com/?target=https%3A//www.abc.com)，实际会产生以下动作：

◎DNS递归查询[http://www.abc.com](https://link.zhihu.com/?target=http%3A//www.abc.com)，获取地址解析的对应IP；

◎TCP握手，我们熟悉的TCP三次握手需要需要1个RTT；

◎TLS握手，以目前应用最广泛的TLS 1.2而言，需要2个RTT。对于非首次建连，可以选择启用会话重用，则可缩小握手时间到1个RTT；

◎HTTP业务数据交互，假设[http://abc.com](https://link.zhihu.com/?target=http%3A//abc.com)的数据在一次交互就能取回来。那么业务数据的交互需要1个RTT；经过上面的过程分析可知，要完成一次简短的HTTPS业务数据交互，需要经历：新连接**4RTT + DNS**；会话重用 **3RTT + DNS**。

所以，对于数据量小的请求而言，单一次的请求握手就占用了大量的时间，对于用户体验的影响非常大。同时，在用户网络不佳的情况下，RTT延时会变得较高，极其影响用户体验。

下图对比了TLS各版本与场景下的延时对比：

![img](https://pic3.zhimg.com/80/v2-03e173ad36607e4e074070b6f8d974a6_720w.webp)

图2-2 TLS各个版本握手时延

从对比我们可以看到，即使用上了TLS 1.3，精简了握手过程，最快能做到0-RTT握手(首次是1-RTT)；但是对用户感知而言, 还要加上1RTT的TCP握手开销。

Google有提出Fastopen的方案来使得TCP非首次握手就能附带用户数据，但是由于TCP实现僵化，无法升级应用，相关RFC到现今都是experimental状态。这种分层设计带来的延时,有没有办法进一步降低呢? QUIC通过合并加密与连接管理解决了这个问题，我们来看看其是如何实现真正意义上的0-RTT的握手, 让与server进行第一个数据包的交互就能带上用户数据。

**2.2.2 真0-RTT的QUIC握手**

QUIC 由于基于 UDP，无需 TCP 连接，在最好情况下，短连接下 QUIC 可以做到 0RTT 开启数据传输。而基于 TCP 的 HTTPS，即使在最好的 TLS1.3 的 early data 下仍然需要 1RTT 开启数据传输。而对于目前线上常见的 TLS1.2 完全握手的情况，则需要 3RTT 开启数据传输。对于 RTT 敏感的业务，QUIC 可以有效的降低连接建立延迟。

究其原因一方面是TCP和TLS分层设计导致的：分层的设计需要每个逻辑层次分别建立自己的连接状态。另一方面是TLS的握手阶段复杂的密钥协商机制导致的。要降低建连耗时，需要从这两方面着手。

QUIC具体握手过程如下：

1.客户端判断本地是否已有服务器的全部配置参数（证书配置信息），如果有则直接跳转到(5)，否则继续 ；

2.客户端向服务器发送inchoate client hello(CHLO)消息，请求服务器传输配置参数；

3.服务器收到CHLO，回复rejection(REJ)消息，其中包含服务器的部分配置参数；

4.客户端收到REJ，提取并存储服务器配置参数，跳回到(1) ；

5.客户端向服务器发送full client hello消息，开始正式握手，消息中包括客户端选择的公开数。此时客户端根据获取的服务器配置参数和自己选择的公开数，可以计算出初始密钥K1；

6.服务器收到full client hello，如果不同意连接就回复REJ，同(3)；如果同意连接，根据客户端的公开数计算出初始密钥K1，回复server hello(SHLO)消息，SHLO用初始密钥K1加密，并且其中包含服务器选择的一个临时公开数；

7.客户端收到服务器的回复，如果是REJ则情况同(4)；如果是SHLO，则尝试用初始密钥K1解密，提取出临时公开数；

8.客户端和服务器根据临时公开数和初始密钥K1，各自基于SHA-256算法推导出会话密钥K2；

9.双方更换为使用会话密钥K2通信，初始密钥K1此时已无用，QUIC握手过程完毕。之后会话密钥K2更新的流程与以上过程类似，只是数据包中的某些字段略有不同。

![img](https://pic4.zhimg.com/80/v2-8b08891d21a460870e63ba248449262b_720w.webp)

图2-3 QUIC 0-RTT 握手

### **2.3 可自定义的拥塞控制**

Quic使用可插拔的拥塞控制，相较于TCP，它能提供更丰富的拥塞控制信息。比如对于每一个包，不管是原始包还是重传包，都带有一个新的序列号(seq)，这使得Quic能够区分ACK是重传包还是原始包，从而避免了TCP重传模糊的问题。Quic同时还带有收到数据包与发出ACK之间的时延信息。这些信息能够帮助更精确的计算RTT。此外，Quic的ACK Frame 支持256个NACK 区间，相比于TCP的SACK(Selective Acknowledgment)更弹性化，更丰富的信息会让client和server 哪些包已经被对方收到。

QUIC 的传输控制不再依赖内核的拥塞控制算法，而是实现在应用层上，这意味着我们根据不同的业务场景，实现和配置不同的拥塞控制算法以及参数。GOOGLE 提出的 BBR 拥塞控制算法与 CUBIC 是思路完全不一样的算法，在弱网和一定丢包场景，BBR 比 CUBIC 更不敏感，性能也更好。在 QUIC 下我们可以根据业务随意指定拥塞控制算法和参数，甚至同一个业务的不同连接也可以使用不同的拥塞控制算法。

![动图封面](https://pic1.zhimg.com/v2-3ea60e2729362a5926d38fd2e4de207c_b.jpg)



图2-4 BBR拥塞弱网下算法效果对比

### **2.4 无队头阻塞**

**2.4.1 TCP的队头阻塞问题**

虽然 HTTP2 实现了多路复用，但是因为其基于面向字节流的 TCP，因此一旦丢包，将会影响多路复用下的所有请求流。QUIC 基于 UDP，在设计上就解决了队头阻塞问题。

TCP 队头阻塞的主要原因是数据包超时确认或丢失阻塞了当前窗口向右滑动，我们最容易想到的解决队头阻塞的方案是不让超时确认或丢失的数据包将当前窗口阻塞在原地。QUIC也正是采用上述方案来解决TCP 队头阻塞问题的。

TCP 为了保证可靠性，使用了基于字节序号的 Sequence Number 及 Ack 来确认消息的有序到达。

![img](https://pic3.zhimg.com/80/v2-94a87ccc33c70aa2781902b799a3d882_720w.webp)

图2-5 HTTP2队头阻塞

如上图，应用层可以顺利读取stream1中的内容，但由于stream2中的第三个segment发生了丢包，TCP 为了保证数据的可靠性，需要发送端重传第 3 个 segment 才能通知应用层读取接下去的数据。所以即使stream3 stream4的内容已顺利抵达，应用层仍然无法读取，只能等待stream2中丢失的包进行重传。

在弱网环境下，HTTP2的队头阻塞问题在用户体验上极为糟糕。

**2.4.2 QUIC的无队头阻塞解决方案**

QUIC 同样是一个可靠的协议，它使用 Packet Number 代替了 TCP 的 Sequence Number，并且每个 Packet Number 都严格递增，也就是说就算 Packet N 丢失了，重传的 Packet N 的 Packet Number 已经不是 N，而是一个比 N 大的值，比如Packet N+M。

QUIC 使用的Packet Number 单调递增的设计，可以让数据包不再像TCP 那样必须有序确认，QUIC 支持乱序确认，当数据包Packet N 丢失后，只要有新的已接收数据包确认，当前窗口就会继续向右滑动。待发送端获知数据包Packet N 丢失后，会将需要重传的数据包放到待发送队列，重新编号比如数据包Packet N+M 后重新发送给接收端，对重传数据包的处理跟发送新的数据包类似，这样就不会因为丢包重传将当前窗口阻塞在原地，从而解决了队头阻塞问题。那么，既然重传数据包的Packet N+M 与丢失数据包的Packet N 编号并不一致，我们怎么确定这两个数据包的内容一样呢？

QUIC使用Stream ID 来标识当前数据流属于哪个资源请求，这同时也是数据包多路复用传输到接收端后能正常组装的依据。重传的数据包Packet N+M 和丢失的数据包Packet N 单靠Stream ID 的比对一致仍然不能判断两个数据包内容一致，还需要再新增一个字段Stream Offset，标识当前数据包在当前Stream ID 中的字节偏移量。

有了Stream Offset 字段信息，属于同一个Stream ID 的数据包也可以乱序传输了（HTTP/2 中仅靠Stream ID 标识，要求同属于一个Stream ID 的数据帧必须有序传输），通过两个数据包的Stream ID 与 Stream Offset 都一致，就说明这两个数据包的内容一致。

![img](https://pic3.zhimg.com/80/v2-6b4361f49c2fa38aa86ee1ef41d4c0e6_720w.webp)

图2-6 QUIC无队头阻塞

## 3.QUIC协议组成

QUIC 的 packet 除了个别报文比如 PUBLIC_RESET 和 CHLO，所有报文头部都是经过认证的，报文 Body 都是经过加密的。这样只要对 QUIC 报文任何修改，接收端都能够及时发现，有效地降低了安全风险。

如图3-1所示，红色部分是 Stream Frame 的报文头部，有认证。绿色部分是报文内容，全部经过加密。

![img](https://pic3.zhimg.com/80/v2-269613d4dbb0292a0cec2cdca59b24c6_720w.webp)

图3-1 QUIC的协议组成

◎**Flags**：用于表示Connection ID长度、Packet Number长度等信息；

◎**Connection ID**：客户端随机选择的最大长度为64位的无符号整数。但是，长度可以协商；

◎**QUIC Version**：QUIC协议的版本号，32位的可选字段。如果Public Flag & FLAG_VERSION != 0，这个字段必填。客户端设置Public Flag中的Bit0为1，并且填写期望的版本号。如果客户端期望的版本号服务端不支持，服务端设置Public Flag中的Bit0为1，并且在该字段中列出服务端支持的协议版本（0或者多个），并且该字段后不能有任何报文；

◎**Packet Number**：长度取决于Public Flag中Bit4及Bit5两位的值，最大长度6字节。发送端在每个普通报文中设置Packet Number。发送端发送的第一个包的序列号是1，随后的数据包中的序列号的都大于前一个包中的序列号；

◎**Stream ID**：用于标识当前数据流属于哪个资源请求；

◎**Offset**：标识当前数据包在当前Stream ID 中的字节偏移量；

QUIC报文的大小需要满足路径MTU的大小以避免被分片。当前QUIC在IPV6下的最大报文长度为1350，IPV4下的最大报文长度为1370。

## 4.结语

QUIC具有众多优点，它融合了UDP协议的速度、性能与TCP的安全与可靠，同时也解决了HTTP1、HTTP1.1、HTTP2中引入的一些缺点，大大优化了互联网传输体验。

原文地址：https://zhuanlan.zhihu.com/p/531813080

作者：linux

# 【NO.309】Redis基本数据结构及底层实现原理

**前言**

面试必问之Redis，大部分人都知道Redis的几种数据类型，也知道怎么用。但具体底层是怎么实现的呢，面试过程中面试官问：Redis底层是怎么实现的，你能答上来吗？

## 1.Redis支持的数据类型

一、Redis支持的数据类型

Redis 主要有以下几种数据类型：

- String 字符串对象
- Hash 哈希Map对象
- List 列表对象
- Set 集合对象
- ZSet 有序集合

还有三种特殊数据类型:

- geospatial: Redis 在 3.2 推出 Geo 类型，该功能可以推算出地理位置信息，两地之间的距离。
- hyperloglog:基数：数学上集合的元素个数，是不能重复的。这个数据结构常用于统计网站的 UV。
- bitmap: bitmap 就是通过最小的单位 bit 来进行0或者1的设置，表示某个元素对应的值或者状态。一个 bit 的值，或者是0，或者是1；也就是说一个 bit 能存储的最多信息是2。bitmap 常用于统计用户信息比如活跃粉丝和不活跃粉丝、登录和未登录、是否打卡等。

补充说明：

基数是一种算法。举个例子 一本英文著作由数百万个单词组成，你的内存却不足以存储它们，那么我们先分析下业务。英文单词本身是有限的，在这本书的几百万个单词中有许许多多重复单词，扣去重复的单词，这本书中也就 千到 万多个单词而己，那么内存就足够存储它 了。比如数字集合｛ l,2,5 1,5,9 ｝的基数集合为｛ 1,2,5 ｝那么 基数（不重复元素）就是基数的作用是评估大约需要准备多少个存储单元去存储数据，基数并不是存储元素，存储元素消耗内存空间比较大，而是给某个有重复元素的数据集合（ 般是很大的数据集合〉评估需要的空间单元数。

![img](https://pic2.zhimg.com/80/v2-a29461535103d79589cb4dbeafebd435_720w.webp)

几种特殊类型的使用场景会在文末详细地补充介绍，请耐心看完。

## 2.redisObject对象

Redis存储的所有值对象在内部都定义为redisObject结构体，内部结构如下图所示：

![img](https://pic1.zhimg.com/80/v2-f6189e01322c8ead0f5d07676c5ca330_720w.webp)

Redis存储的包括string,hash,list,set,zset在内的所有数据类型，都使用redisObject来封装的。

下面针对每个字段做详细说明：

1.type字段:表示当前对象使用的数据类型，Redis主要支持5种数据类型:string,hash,list,set,zset。可以使用type {key}命令查看对象所属类型，type命令返回的是值对象类型，键都是string类型。

2.encoding字段:表示Redis内部编码类型，encoding在Redis内部使用，代表当前对象内部采用哪种数据结构实现。理解Redis内部编码方式对于优化内存非常重要，同一个对象采用不同的编码实现内存占用存在明显差异，具体细节见之后编码优化部分。

3.lru字段:记录对象最后一次被访问的时间，当配置了 maxmemory和maxmemory-policy=volatile-lru | allkeys-lru 时，用于辅助LRU算法删除键数据。可以使用object idletime {key}命令在不更新lru字段情况下查看当前键的空闲时间。开发提示：可以使用scan + object idletime 命令批量查询哪些键长时间未被访问，找出长时间不访问的键进行清理降低内存占用。

4.refcount字段:记录当前对象被引用的次数，用于通过引用次数回收内存，当refcount=0时，可以安全回收当前对象空间。使用object refcount {key}获取当前对象引用。当对象为整数且范围在[0-9999]时，Redis可以使用共享对象的方式来节省内存。具体细节见之后共享对象池部分。

5.ptr字段:与对象的数据内容相关，如果是整数直接存储数据，否则表示指向数据的指针。Redis在3.0之后对值对象是字符串且长度<=39字节的数据，内部编码为embstr类型，字符串sds和redisObject一起分配，从而只要一次内存操作。开发提示：高并发写入场景中，在条件允许的情况下建议字符串长度控制在39字节以内，减少创建redisObject内存分配次数从而提高性能。

可以简单的理解成下图：

![img](https://pic2.zhimg.com/80/v2-e0aeccf7ce2e6e23caddedd7fae6429d_720w.webp)

每一种类型都有自己特有的数据结构，下面我们要探讨的就是每种数据类型的具体的底层结构。

## 3.String

- string 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。
- value其实不仅是String，也可以是数字
- string 类型是二进制安全的，可以包含任何数据，比如jpg图片或者序列化的对象
- string 类型的值最大能存储 512MB
- 常用命令：get、set、incr、decr、mget等

**应用场景**

常规key-value缓存应用。常规计数: 微博数, 粉丝数。

**String 底层实现**

字符串是我们日常工作中用得最多的对象类型，它对应的编码可以是int、raw和embstr。通过 object encoding key 命令来查看具体的编码格式：

![img](https://pic1.zhimg.com/80/v2-bc4129437b63c1751c3b8578776c6f38_720w.webp)

如果一个字符串对象保存的是不超过long类型的整数值，此时编码类型即为int，其底层数据结构直接就是long类型。例如执行set number 10086，就会创建int编码的字符串对象作为number键的值。

![img](https://pic2.zhimg.com/80/v2-8d23810026e6ae1ac89b8b23b329c9c9_720w.webp)

如果字符串对象保存的是一个长度大于39字节的字符串，此时编码类型即为raw，其底层数据结构是简单动态字符串(SDS)。

如果长度小于等于39个字节，编码类型则为embstr，底层数据结构就是embstr编码SDS。下面，我们详细理解下什么是简单动态字符串。

**SDS**

SDS是"simple dynamic string"的缩写。redis中所有场景中出现的字符串，由SDS来实现。

![img](https://pic4.zhimg.com/80/v2-42e8434873abb9efeeb2031bbfbfb0eb_720w.webp)

free:还剩多少空间 len:字符串长度 buf:存放的字符数组。

在源码的 src目录下，找到了 sds.h 这样一个文件，规定了 SDS 的结构：

```text
struct sdsshr<T>{
    T len;//数组长度
    T alloc;//数组容量
    unsigned  flags;//sdshdr类型
    char buf[];//数组内容
}
```

可以看出，SDS 的结构有点类似于 Java 中的 ArrayList。buf[]表示真正存储的字符串内容，alloc 表示所分配的数组的长度，len 表示字符串的实际长度，并且由于 len 这个属性的存在，Redis 可以在 O(1)的时间复杂度内获取数组长度。

**空间预分配**

为减少修改字符串带来的内存重分配次数，sds采用了“一次管够”的策略：

- 若修改之后sds长度小于1MB,则多分配现有len长度的空间
- 若修改之后sds长度大于等于1MB，则扩充除了满足修改之后的长度外，额外多1MB空间

由于Redis的字符串是动态字符串，可以修改，内部结构类似于Java的ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配。如上图所示，内部为当前字符串实际分配的空间capacity，一般高于实际字符串长度len。

![img](https://pic3.zhimg.com/80/v2-49ad3d15d55b9d89627f852d723d0412_720w.webp)

假设我们要存储的结构是：

```text
{
    "name": "xiaowang",
    "age": "35"
}
```

如果此时将此用户信息的name改为“xiaoli”，再存到redis中，redis是不需要重新分配空间的，使用已分配空间即可。

**惰性空间释放**

为避免缩短字符串时候的内存重分配操作，sds在数据减少时，并不立刻释放空间。

**SDS与C字符串的区别**

C语言使用长度为N+1的字符数组来表示长度为N的字符串，并且字符串的最后一个元素是空字符\0。Redis采用SDS相对于C字符串有如下几个优势：

- 常数复杂度获取字符串长度
- 杜绝缓冲区溢出
- 减少修改字符串时带来的内存重分配次数
- 二进制安全

**raw和embstr编码的SDS区别**

长度大于39字节的字符串，编码类型为raw，底层数据结构是简单动态字符串(SDS)。

比如当我们执行set story "Long, long, long ago there lived a king ..."(长度大于39)之后，Redis就会创建一个raw编码的String对象。数据结构如下：

![img](https://pic1.zhimg.com/80/v2-34a8f7eb0e27c97fa5dc43bead1bfd54_720w.webp)

长度小于等于39个字节的字符串，编码类型为embstr，底层数据结构则是embstr编码SDS。

embstr编码是专门用来保存短字符串的，它和raw编码最大的不同在于：raw编码会调用两次内存分配分别创建redisObject结构和sdshdr结构，而embstr编码则是只调用一次内存分配，在一块连续的空间上同时包含redisObject结构和sdshdr`结构。

![img](https://pic2.zhimg.com/80/v2-124275da03659bd4c16ba0074e0564d9_720w.webp)

**编码转换**

int编码和embstr编码的字符串对象在条件满足的情况下会自动转换为raw编码的字符串对象。

- 对于int编码来说，当我们修改这个字符串为不再是整数值的时候，此时字符串对象的编码就会从int变为raw；
- 对于embstr编码来说，只要我们修改了字符串的值，此时字符串对象的编码就会从embstr变为raw。embstr编码的字符串对象可以认为是只读的，因为Redis为其编写任何修改程序。当我们要修改embstr编码字符串时，都是先将转换为raw编码，然后再进行修改。

**Redis字符串结构特点**

- O(1) 时间复杂度获取：字符串长度，已用长度，未用长度。
- 可用于保存字节数组，支持安全的二进制数据存储。
- 内部实现空间预分配机制，降低内存再分配次数。
- 惰性删除机制，字符串缩减后的空间不释放，作为预分配空间保留。

## 4.List

列表对象的编码可以是linkedlist或者ziplist，对应的底层数据结构是链表和压缩列表。

默认情况下，当列表对象保存的所有字符串元素的长度都小于64字节，且元素个数小于512个时，列表对象采用的是ziplist编码，否则使用linkedlist编码。可以通过配置文件修改该上限值。

**链表**

提供了高效的节点重排能力以及顺序性的节点访问方式。在Redis中，每个链表节点使用listNode结构表示：

```text
typedef struct listNode {
    // 前置节点
    struct listNode *prev;
    // 后置节点
    struct listNode *next;
    // 节点值
    void *value;
} listNode
```

多个listNode通过prev和next指针组成双端链表，如下图所示：



![img](https://pic2.zhimg.com/80/v2-a6bb9af8ff3343c28842ecee72868f2d_720w.webp)

为了操作起来比较方便，Redis使用了list结构持有链表。list结构为链表提供了表头指针head、表尾指针tail，以及链表长度计数器len，而dup、free和match成员则是实现多态链表所需类型的特定函数。

```text
typedef struct list {
    // 表头节点
    listNode *head;
    // 表尾节点
    listNode *tail;
    // 链表包含的节点数量
    unsigned long len;
    // 节点复制函数
    void *(*dup)(void *ptr);
    // 节点释放函数
    void (*free)(void *ptr);
    // 节点对比函数
    int (*match)(void *ptr, void *key);
} list;
```

![img](https://pic3.zhimg.com/80/v2-74d57bfb24b6c9418a3a9fe484f784ea_720w.webp)

Redis链表实现的特征总结如下：

- 双端：链表节点带有prev和next指针，获取某个节点的前置节点和后置节点的复杂度都是O(n)。
- 无环：表头节点的prev指针和表尾节点的next指针都指向NULL，对链表的访问以NULL为终点。
- 带表头指针和表尾指针：通过list结构的head指针和tail指针，程序获取链表的表头节点和表尾节点的复杂度为O(1)。
- 带链表长度计数器：程序使用list结构的len属性来对list持有的节点进行计数，程序获取链表中节点数量的复杂度为O(1)。
- 多态：链表节点使用void*指针来保存节点值，可以保存各种不同类型的值。

**压缩列表**

压缩列表。redis的列表键和哈希键的底层实现之一。此数据结构是为了节约内存而开发的。和各种语言的数组类似，它是由连续的内存块组成的，这样一来，由于内存是连续的，就减少了很多内存碎片和指针的内存占用，进而节约了内存。

![img](https://pic1.zhimg.com/80/v2-3bbf2bc934d9351e75afcd29dedf1680_720w.webp)

entry的结构是这样的：

![img](https://pic2.zhimg.com/80/v2-c4c20af18eaed68f84bd38813ed64f89_720w.webp)

压缩列表记录了各组成部分的类型、长度以及用途:

![img](https://pic2.zhimg.com/80/v2-3ae8233fbdad51600e76e49208b264c1_720w.webp)

## 5.Hash

哈希对象的编码可以是ziplist或者hashtable。

哈希对象保存的所有键值对的键和值的字符串长度都小于 64 字节并且保存的键值对数量小于 512 个，使用ziplist 编码；否则使用hashtable；

**hash-ziplist**

ziplist底层使用的是压缩列表实现，前面已经详细介绍了压缩列表的实现原理。每当有新的键值对要加入哈希对象时，先把保存了键的节点推入压缩列表表尾，然后再将保存了值的节点推入压缩列表表尾。比如，我们执行如下三条HSET命令：

```text
HSET profile name "tom"
HSET profile age 25
HSET profile career "Programmer"
```

如果此时使用ziplist编码，那么该Hash对象在内存中的结构如下：

![img](https://pic3.zhimg.com/80/v2-393b86ab3257d0078043a79d91e5e8e6_720w.webp)

**hash-hashtable**

hashtable 编码的哈希对象使用字典dictht作为底层实现。字典是一种保存键值对的数据结构。

```text
typedef struct dictht{
    // 哈希表数组
    dictEntry **table;
    // 哈希表大小
    unsigned long size;

    // 哈希表大小掩码，用于计算索引值
    // 总是等于 size-1
    unsigned long sizemask;
    // 该哈希表已有节点数量
    unsigned long used;
} dictht
```

table属性是一个数组，数组中的每个元素都是一个指向dictEntry结构的指针，每个dictEntry结构保存着一个键值对。size属性记录了哈希表的大小，即table数组的大小。used属性记录了哈希表目前已有节点数量。sizemask总是等于size-1，这个值主要用于数组索引。比如下图展示了一个大小为4的空哈希表。

![img](https://pic3.zhimg.com/80/v2-b2fc8a2c2e1eac193e79c41e6cb378b2_720w.webp)

**哈希表节点**

哈希表节点使用dictEntry结构表示，每个dictEntry结构都保存着一个键值对：

```text
typedef struct dictEntry {
    // 键
    void *key;
    // 值
    union {
        void *val;
        unit64_t u64;
        nit64_t s64;
    } v;
    // 指向下一个哈希表节点，形成链表
    struct dictEntry *next;
} dictEntry;
```

key属性保存着键值对中的键，而v属性则保存了键值对中的值。值可以是一个指针，一个uint64_t整数或者是int64_t整数。next属性指向了另一个dictEntry节点，在数组桶位相同的情况下，将多个dictEntry节点串联成一个链表，以此来解决键冲突问题。(链地址法)

**字典**

Redis字典由dict结构表示：

```text
typedef struct dict {
    // 类型特定函数
    dictType *type;
    // 私有数据
    void *privdata;
    // 哈希表
    dictht ht[2];
    //rehash索引
    // 当rehash不在进行时，值为-1
    int rehashidx;
}
```

ht是大小为2，且每个元素都指向dictht哈希表。一般情况下，字典只会使用ht[0]哈希表，ht[1]哈希表只会在对ht[0]哈希表进行rehash时使用。rehashidx记录了rehash的进度，如果目前没有进行rehash，值为-1。

![img](https://pic2.zhimg.com/80/v2-c64e440195ff4a807fa701348cac3c09_720w.webp)

**rehash**

为了使hash表的负载因子(ht[0]).used/ht[0]).size)维持在一个合理范围，当哈希表保存的元素过多或者过少时，程序需要对hash表进行相应的扩展和收缩。rehash（重新散列）操作就是用来完成hash表的扩展和收缩的。rehash的步骤如下：

1. 为ht[1]哈希表分配空间，如果是扩展操作，那么ht[1]的大小为第一个大于ht[0].used*2的2^n。比如ht[0].used=5，那么此时ht[1]的大小就为16。(大于10的第一个2^n的值是16)如果是收缩操作，那么ht[1]的大小为第一个大于ht[0].used的2^n。比如ht[0].used=5，那么此时ht[1]的大小就为8。(大于5的第一个2^n的值是8)
2. 将保存在ht[0]中的所有键值对rehash到ht[1]中。
3. 迁移完成之后，释放掉ht[0]，并将现在的ht[1]设置为ht[0]，在ht[1]新创建一个空白哈希表，为下一次rehash做准备。

**哈希表的扩展和收缩时机：**

当服务器没有执行BGSAVE或者BGREWRITEAOF命令时，负载因子大于等于1触发哈希表的扩展操作。

当服务器在执行BGSAVE或者BGREWRITEAOF命令，负载因子大于等于5触发哈希表的扩展操作。

当哈希表负载因子小于0.1，触发哈希表的收缩操作。

**渐进式rehash**

前面讲过，扩展或者收缩需要将ht[0]里面的元素全部rehash到ht[1]中，如果ht[0]元素很多，显然一次性rehash成本会很大，从影响到Redis性能。为了解决上述问题，Redis使用了渐进式rehash技术，具体来说就是分多次，渐进式地将ht[0]里面的元素慢慢地rehash到ht[1]中。下面是渐进式rehash的详细步骤：

1. 为ht[1]分配空间。
2. 在字典中维持一个索引计数器变量rehashidx，并将它的值设置为0，表示rehash正式开始。
3. 在rehash进行期间，每次对字典执行添加、删除、查找或者更新时，除了会执行相应的操作之外，还会顺带将ht[0]在rehashidx索引位上的所有键值对rehash到ht[1]中，rehash完成之后，rehashidx值加1。
4. 随着字典操作的不断进行，最终会在某个时刻迁移完成，此时将rehashidx值置为-1，表示rehash结束。

渐进式rehash一次迁移一个桶上所有的数据，设计上采用分而治之的思想，将原本集中式的操作分散到每个添加、删除、查找和更新操作上，从而避免集中式rehash带来的庞大计算。

因为在渐进式rehash时，字典会同时使用ht[0]和ht[1]两张表，所以此时对字典的删除、查找和更新操作都可能会在两个哈希表进行。比如，如果要查找某个键时，先在ht[0]中查找，如果没找到，则继续到ht[1]中查找。

**hash对象中的hashtable**

```text
HSET profile name "tom"
HSET profile age 25
HSET profile career "Programmer"
```

还是上述三条命令，保存数据到Redis的哈希对象中，如果采用hashtable编码保存的话，那么该Hash对象在内存中的结构如下：

![img](https://pic2.zhimg.com/80/v2-af0ae1983921d4676344cec4f2a960ad_720w.webp)

## 6.Set

集合对象的编码可以是intset或者hashtable。

当集合对象保存的元素都是整数，并且个数不超过512个时，使用intset编码，否则使用hashtable编码。

**set-intset**

整数集合(intset)是Redis用于保存整数值的集合抽象数据结构，它可以保存类型为int16_t、int32_t或者int64_t的整数值，并且保证集合中的数据不会重复。Redis使用intset结构表示一个整数集合。

```text
typedef struct intset {
    // 编码方式
    uint32_t encoding;
    // 集合包含的元素数量
    uint32_t length;
    // 保存元素的数组
    int8_t contents[];
} intset;
```

contents数组是整数集合的底层实现：整数集合的每个元素都是contents数组的一个数组项，各个项在数组中按值大小从小到大有序排列，并且数组中不包含重复项。虽然contents属性声明为int8_t类型的数组，但实际上，contents数组不保存任何int8_t类型的值，数组中真正保存的值类型取决于encoding。如果encoding属性值为INTSET_ENC_INT16，那么contents数组就是int16_t类型的数组，以此类推。

当新插入元素的类型比整数集合现有类型元素的类型大时，整数集合必须先升级，然后才能将新元素添加进来。这个过程分以下三步进行。

1. 根据新元素类型，扩展整数集合底层数组空间大小。
2. 将底层数组现有所有元素都转换为与新元素相同的类型，并且维持底层数组的有序性。
3. 将新元素添加到底层数组里面。

还有一点需要注意的是，整数集合不支持降级，一旦对数组进行了升级，编码就会一直保持升级后的状态。

举个栗子，当我们执行SADD numbers 1 3 5向集合对象插入数据时，该集合对象在内存的结构如下：

![img](https://pic1.zhimg.com/80/v2-a20ed4b2ace422201981cf8312768c64_720w.webp)

**set-hashtable**

hashtable编码的集合对象使用字典作为底层实现，字典的每个键都是一个字符串对象，每个字符串对象对应一个集合元素，字典的值都是NULL。当我们执行SADD fruits "apple" "banana" "cherry"向集合对象插入数据时，该集合对象在内存的结构如下：

![img](https://pic4.zhimg.com/80/v2-93a5b34d2985c38f9bc327a3f804c88f_720w.webp)

## 7.Zset

有序集合的编码可以是ziplist或者skiplist。

当有序集合保存的元素个数小于128个，且所有元素成员长度都小于64字节时，使用ziplist编码，否则，使用skiplist编码。

**zset-ziplist**

ziplist编码的有序集合使用压缩列表作为底层实现，每个集合元素使用两个紧挨着一起的两个压缩列表节点表示，第一个节点保存元素的成员(member)，第二个节点保存元素的分值(score)。

压缩列表内的集合元素按照分值从小到大排列。如果我们执行ZADD price 8.5 apple 5.0 banana 6.0 cherry命令，向有序集合插入元素，该有序集合在内存中的结构如下：

![img](https://pic2.zhimg.com/80/v2-58799bceef062c08b18d6c092525c805_720w.webp)

**zset-skiplist**

skiplist编码的有序集合对象使用zset结构作为底层实现，一个zset结构同时包含一个字典和一个跳跃表。

```text
typedef struct zset {

    zskiplist *zs1;
    dict *dict;
}
```

继续介绍之前，我们先了解一下什么是跳跃表。

**跳跃表**

跳跃表(skiplist)是一种有序的数据结构，它通过在每个节点中维持多个指向其他节点的指针，从而达到快速访问节点的目的。

Redis的跳跃表由zskiplistNode和zskiplist两个结构定义，zskiplistNode结构表示跳跃表节点，zskiplist保存跳跃表节点相关信息，比如节点的数量，以及指向表头和表尾节点的指针等。

**跳跃表节点 zskiplistNode**

跳跃表节点zskiplistNode结构定义如下：

```text
typedef struct zskiplistNode {
    // 后退指针
    struct zskiplistNode *backward;
    // 分值
    double score;
    // 成员对象
    robj *obj;
    // 层
    struct zskiplistLevel {
        // 前进指针
        struct zskiplistNode *forward;
        // 跨度
        unsigned int span;
    } level[];
} zskiplistNode;
```

下图是一个层高为5，包含4个跳跃表节点(1个表头节点和3个数据节点)组成的跳跃表:

![img](https://pic4.zhimg.com/80/v2-7e1efd9467a252e3fba13bca85f1ddcb_720w.webp)

1. 每次创建一个新的跳跃表节点的时候，会根据幂次定律(越大的数出现的概率越低)随机生成一个1-32之间的值作为当前节点的"层高"。每层元素都包含2个数据，前进指针和跨度。
   前进指针:每层都有一个指向表尾方向的前进指针，用于从表头向表尾方向访问节点。
   跨度:层的跨度用于记录两个节点之间的距离。
2. 后退指针(BW)
   节点的后退指针用于从表尾向表头方向访问节点，每个节点只有一个后退指针，所以每次只能后退一个节点。
3. 分值和成员
   节点的分值(score)是一个double类型的浮点数，跳跃表中所有节点都按分值从小到大排列。节点的成员(obj)是一个指针，指向一个字符串对象。在跳跃表中，各个节点保存的成员对象必须是唯一的，但是多个节点的分值确实可以相同。

需要注意的是，表头节点不存储真实数据，并且层高固定为32，从表头节点第一个不为NULL最高层开始，就能实现快速查找。

**跳跃表 zskiplist**

实际上，仅靠多个跳跃表节点就可以组成一个跳跃表，但是Redis使用了zskiplist结构来持有这些节点，这样就能够更方便地对整个跳跃表进行操作。比如快速访问表头和表尾节点，获得跳跃表节点数量等等。zskiplist结构定义如下：

```text
typedef struct zskiplist {
    // 表头节点和表尾节点
    struct skiplistNode *header, *tail;
    // 节点数量
    unsigned long length;
    // 最大层数
    int level;
} zskiplist;
```

下图是一个完整的跳跃表结构示例：

![img](https://pic4.zhimg.com/80/v2-89b03e4542c1b22156ee9936ed27b1d7_720w.webp)

**有序集合对象的skiplist实现**

前面讲过，skiplist编码的有序集合对象使用zset结构作为底层实现，一个zset结构同时包含一个字典和一个跳跃表。

```text
typedef struct zset {
    zskiplist *zs1;
    dict *dict;
}
```

zset结构中的zs1跳跃表按分值从小到大保存了所有集合元素，每个跳跃表节点都保存了一个集合元素。通过跳跃表，可以对有序集合进行基于score的快速范围查找。zset结构中的dict字典为有序集合创建了从成员到分值的映射，字典的键保存了成员，字典的值保存了分值。通过字典，可以用O(1)复杂度查找给定成员的分值。

假如还是执行ZADD price 8.5 apple 5.0 banana 6.0 cherry命令向zset保存数据，如果采用skiplist编码方式的话，该有序集合在内存中的结构如下：

![img](https://pic1.zhimg.com/80/v2-cf193c3801410ca94c3fe61c1edb9158_720w.webp)

## 8.总结

Redis底层数据结构主要包括简单动态字符串(SDS)、链表、字典、跳跃表、整数集合和压缩列表六种类型，并且基于这些基础数据结构实现了字符串对象、列表对象、哈希对象、集合对象以及有序集合对象五种常见的对象类型。每一种对象类型都至少采用了2种数据编码，不同的编码使用的底层数据结构也不同。

原文地址：https://zhuanlan.zhihu.com/p/531323771

作者：linux

# 【NO.310】linux后端开发-定时器设计详解

定时器模块是后端服务常用的功能之一，用于需要周期性的执行某些任务的场景。设计定时器模块的设计方法非常多，但关键是定时器的效率问题。让我们先从最简单的开始吧。

## **1.最简单的定时器**

一个最简单的定时器功能可以按如下思路实现：

```text
void WebSocketServer::doCheckHeartbeat()
{
    while (m_bRunning)
    {
        //休眠3秒
        sleep(3000)
        
        //检测所有会话的心跳
        checkSessionHeartbeat();
    }
}
```

上述代码在一个新的线程中每隔 3 秒对所有连接会话做心跳检测。这是一个非常简单的实现逻辑，读者可能会觉得有点不可思议，直接使用 sleep 函数休眠 3 秒作为时间间隔，这未免有点“简单粗暴”了吧？这段代码来源于我们之前一个商业项目，并且它至今工作的很好。所以凡事都没有那么绝对，在一些特殊场景下我们确实可以按这种思路来实现定时器，只不过 sleep 函数可能换成一些可以指定超时或等待时间的、让线程挂起或等待的函数（如 select、poll 等）。

但是上述实现定时器的方法毕竟适用场景太少，也不能与 one thread one loop 结构相结合，one thread one loop 结构中的定时器才是我们本文的重点。你可以认为前面介绍的定时器实现只是个热身，现在让我们正式开始。

## **2.定时器设计的基本思路**

根据实际的场景需求，我们的定时器对象一般需要一个唯一标识、过期时间、重复次数、定时器到期时触发的动作，因此一个定时器对象可以设计成如下结构：

```text
typedef std::function<void()> TimerCallback;

//定时器对象
class Timer
{
    Timer() ;
    ~Timer();

    void run()
    {
        callback();
    }
    
    //其他实现下文会逐步完善...

private:
    //定时器的id，唯一标识一个定时器
    int64_t         m_id;
    //定时器的到期时间
    time_t          m_expiredTime;
    //定时器重复触发的次数
    int32_t         m_repeatedTimes;
    //定时器触发后的回调函数
    TimerCallback   m_callback;
};
```

在 one loop one thread 结构中，我们提到使用用到定时器的程序结构：

```text
while (!m_bQuitFlag)
{
	check_and_handle_timers();
	
	epoll_or_select_func();

	handle_io_events();

	handle_other_things();
}
```

我们在函数 **check_and_handle_timers()** 中对各个定时器对象进行处理（检测是否到期，如果到期调用相应的定时器函数），我们先从最简单的情形开始讨论，将定时器对象放在一个 std::list 对象中：

```text
//m_listTimers可以是EventLoop的成员变量
std::list<Timer*> m_listTimers;

void EventLoop::check_and_handle_timers()
{
    for (auto& timer : m_listTimers)
    {
        //判断定时器是否到期
        if (timer->isExpired())
        {
            timer->run();
        }
    }
}
```

为了方便管理所有定时器对象，我们可以专门新建一个 **TimerManager** 类去对定时器对象进行管理，该对象提供了增加、移除和判断定时器是否到期等接口：

```text
class TimerManager
{
public:
    TimerManager() = default;
    ~TimerManager() = default;

    /** 添加定时器
     * @param repeatedCount 重复次数
     * @param 触发间隔
     * @
     * @return 返回创建成功的定时器id
     */
    int64_t addTimer(int32_t repeatedCount, int64_t interval, const TimerCallback& timerCallback);

    /** 移除指定id的定时器
     * @param timerId 待移除的定时器id
     * @return 成功移除定时器返回true，反之返回false
     */
    bool removeTimer(int64_t timerId);

    /** 检测定时器是否到期，如果到期则触发定时器函数
     */
    void checkAndHandleTimers();


private:
    std::list<Timer*> m_listTimers;

};
```

这样 **check_and_handle_timers()** 调用实现变成如下形式：

```text
void EventLoop::check_and_handle_timers()
{
    //m_timerManager可以是EventLoop的成员变量
    m_timerManager.checkAndHandleTimers();
}
```

**addTimer**、**removeTimer**、**checkAndHandleTimers** 的实现如下：

```text
int64_t TimerManager::addTimer(int32_t repeatedCount, int64_t interval, const TimerCallback& timerCallback)
{
    Timer* pTimer = new Timer(repeatedCount, interval, timerCallback);

    m_listTimers.push_back(pTimer);

    return pTimer->getId();
}

bool TimerManager::removeTimer(int64_t timerId)
{
    for (auto iter = m_listTimers.begin(); iter != m_listTimers.end(); ++iter)
    {
        if ((*iter)->getId() == timerId)
        {
            m_listTimers.erase(iter);
            return true;
        }
    }

    return false;
}

void TimerManager::checkAndHandleTimers()
{
    Timer* deletedTimer;
    for (auto iter = m_listTimers.begin(); iter != m_listTimers.end(); )
    {
        if ((*iter)->isExpired())
        {
            (*iter)->run();
            
            if ((*iter)->getRepeatedTimes() == 0)
            {
                //定时器不需要重复，从集合中移除该对象
                deletedTimer = *iter;
                iter = m_listTimers.erase(iter);
                delete deletedTimer;
                continue;
            }
            else 
            {
                ++iter;
            }
        }
    }
}
```

给 **addTimer** 函数传递必要的参数后创建一个 Timer 对象，并返回唯一标识该定时器对象的 id，后续步骤就可以通过定时器 id 来操作这个定时器对象。

我这里的定时器 id 使用了一个单调递增的 int64_t 的整数，你也可以使用其他类型，如 uid，只要能唯一区分每个定时器对象即可。当然，在我这里的设计逻辑中，可能多个线程多个 EventLoop，每一个 EventLoop 含有一个 m_timerManager 对象，但我希望所有的定时器的 id 能够全局唯一，所以我这里每次生成定时器 id 时使用了一个整型原子变量的 id 基数，我将它设置为 Timer 对象的静态成员变量，每次需要生成新的定时器 id 时将其递增 1 即可，这里我利用 C++ 11 的 std::mutex 对 **s_initialId** 进行保护。

```text
//Timer.h
class Timer
{
public:
	Timer::Timer(int32_t repeatedTimes, int64_t interval, const TimerCallback& timerCallback);
	~Timer() {}
	
	bool isExpired();

    void run()
    {
        callback();
    }
    
	//其他无关代码省略...
	
public:
	//生成一个唯一的id
	static int64_t generateId();
	
private:
    //定时器id基准值，初始值为 0
    static int64_t     s_initialId{0};
    //保护s_initialId的互斥体对象
    static std::mutex  s_mutex{};	   
};
```



```text
//Timer.cpp
int64_t Timer::generateId()
{
	int64_t tmpId;
	s_mutex.lock();
	++s_initialId;
	tmpId = s_initialId;
	s_mutex.unlock();
	
	return tmpId;	
}

Timer::Timer(int32_t repeatedTimes, int64_t interval, const TimerCallback& timerCallback)
{
    m_repeatedTimes = repeatedTimes;
    m_interval = interval;

    //当前时间加上触发间隔得到下一次的过期时间
    m_expiredTime = (int64_t)time(nullptr) + interval;

    m_callback = timerCallback;

    m_id = Timer::generateId();
}
```

定时器的下一次过期时间 **m_expiredTime** 是添加定时器的时间点加上触发间隔 **interval**，即上述代码第 **9** 行，也就是说我这里使用绝对时间点作为定时器的过期时间，读者在自己的实现时也可以使用相对时间间隔。

在我的实现中，定时器还有个表示触发次数的变量：**m_repeatedCount**，m_repeatedCount 为 -1 时表示不限制触发次数（即一直触发次数），m_repeatedCount 大于 0 时，每触发一次，m_repeatedCount 递减 1，一直到 m_repeatedCount 等于 0 从定时器集合中移除。

```text
void TimerManager::checkAndHandleTimers()
{
    Timer* deletedTimer;
    for (auto iter = m_listTimers.begin(); iter != m_listTimers.end(); )
    {
        if ((*iter)->isExpired())
        {
            //执行定时器事件
            (*iter)->run();
            
            if ((*iter)->getRepeatedTimes() == 0)
            {
                //定时器不需要重复触发从集合中移除该对象
                deletedTimer = *iter;
                iter = m_listTimers.erase(iter);
                delete deletedTimer;
                continue;
            }
            else 
            {
                ++iter;
            }
        }
    }
}
```

上述代码中我们先遍历定时器对象集合，然后调用 **Timer::isExpired()** 函数判断当前定时器对象是否到期，该函数的实现如下：

```text
bool Timer::isExpired()
{
    int64_t now = time(nullptr);
    return now >= m_expiredTime;
}
```

实现很简单，即用定时器的到期时间与当前系统时间做比较。

如果一个定时器已经到期了，则执行定时器 **Timer::run()**，该函数不仅调用定时器回调函数，还更新定时器对象的状态信息（如触发的次数和下一次触发的时间点）：

```text
void Timer::run()
{
    m_callback();

    if (m_repeatedTimes >= 1)
    {
        --m_repeatedTimes;
    }
		
		//计算下一次的触发时间
    m_
```

除了定时器触发次数变为 0 时会从定时器列表中移除，也可以调用**removeTimer()** 函数主动从定时器列表中移除一个定时器对象：

```text
bool TimerManager::removeTimer(int64_t timerId)
{
    for (auto iter = m_listTimers.begin(); iter != m_listTimers.end(); ++iter)
    {
        if ((*iter)->getId() == timerId)
        {
            m_listTimers.erase(iter);
            return true;
        }
    }

    return false;
}
```

**removeTimer()** 函数成功通过一个定时器 id 成功移除一个定时器对象时会返回 true，反之返回 false。

我们再贴下完整的代码：

**Timer.h**

```text
#ifndef __TIMER_H__
#define __TIMER_H__

#include <functional>

typedef std::function<void()> TimerCallback;

class Timer
{
public:
    /**
     * @param repeatedTimes 定时器重复次数，设置为-1表示一直重复下去
     * @param interval      下一次触发的时间间隔
     * @param timerCallback 定时器触发后的回调函数
     */
    Timer(int32_t repeatedTimes, int64_t interval, const TimerCallback& timerCallback);
    ~Timer();

    int64_t getId()
    {
        return m_id;
    }

    bool isExpired();

    int32_t getRepeatedTimes()
    {
        return m_repeatedTimes;
    }

    void run();

    //其他实现暂且省略
    
public:
	//生成一个唯一的id
	static int64_t generateId();

private:
    //定时器的id，唯一标识一个定时器
    int64_t                     m_id;
    //定时器的到期时间
    time_t                      m_expiredTime;
    //定时器重复触发的次数
    int32_t                     m_repeatedTimes;
    //定时器触发后的回调函数
    TimerCallback               m_callback;
    //触发时间间隔                
    int64_t                     m_interval;

    //定时器id基准值，初始值为 0
    static int64_t     			s_initialId{0};
    //保护s_initialId的互斥体对象
    static std::mutex  			s_mutex{};	
};

#endif //!__TIMER_H__
```

**Timer.cpp**

```text
#include "Timer.h"
#include <time.h>

int64_t Timer::generateId()
{
	int64_t tmpId;
	s_mutex.lock();
	++s_initialId;
	tmpId = s_initialId;
	s_mutex.unlock();
	
	return tmpId;	
}

Timer::Timer(int32_t repeatedTimes, int64_t interval, const TimerCallback& timerCallback)
{
    m_repeatedTimes = repeatedTimes;
    m_interval = interval;

    //当前时间加上触发间隔得到下一次的过期时间
    m_expiredTime = (int64_t)time(nullptr) + interval;

    m_callback = timerCallback;

    //生成一个唯一的id
    m_id = Timer::generateId();
}

bool Timer::isExpired() const
{
    int64_t now = time(nullptr);
    return now >= m_expiredTime;
}

void Timer::run()
{
    m_callback();

    if (m_repeatedTimes >= 1)
    {
        --m_repeatedTimes;
    }

    m_expiredTime += m_interval;
}
```

**TimerManager.h**

```text
#ifndef __TIMER_MANAGER_H__
#define __TIMER_MANAGER_H__

#include <stdint.h>
#include <list>

#include "Timer.h"

void defaultTimerCallback()
{

}

class TimerManager
{
public:
    TimerManager() = default;
    ~TimerManager() = default;

    /** 添加定时器
     * @param repeatedCount 重复次数
     * @param interval      触发间隔
     * @param timerCallback 定时器回调函数
     * @return              返回创建成功的定时器id
     */
    int64_t addTimer(int32_t repeatedCount, int64_t interval, const TimerCallback& timerCallback);

    /** 移除指定id的定时器
     * @param timerId 待移除的定时器id
     * @return 成功移除定时器返回true，反之返回false
     */
    bool removeTimer(int64_t timerId);

    /** 检测定时器是否到期，如果到期则触发定时器函数
     */
    void checkAndHandleTimers();


private:
    std::list<Timer*> m_listTimers;
};

#endif //!__TIMER_MANAGER_H__
```

**TimerManager.cpp**

```text
#include "TimerManager.h"
    
int64_t TimerManager::addTimer(int32_t repeatedCount, int64_t interval, const TimerCallback& timerCallback)
{
    Timer* pTimer = new Timer(repeatedCount, interval, timerCallback);

    m_listTimers.push_back(pTimer);

    return pTimer->getId();
}

bool TimerManager::removeTimer(int64_t timerId)
{
    for (auto iter = m_listTimers.begin(); iter != m_listTimers.end(); ++iter)
    {
        if ((*iter)->getId() == timerId)
        {
            m_listTimers.erase(iter);
            return true;
        }
    }

    return false;
}

void TimerManager::checkAndHandleTimers()
{
    Timer* deletedTimer;
    for (auto iter = m_listTimers.begin(); iter != m_listTimers.end(); )
    {
        if ((*iter)->isExpired())
        {
            (*iter)->run();
            
            if ((*iter)->getRepeatedTimes() == 0)
            {
                //定时器不需要触发从集合中移除该对象
                deletedTimer = *iter;
                iter = m_listTimers.erase(iter);
                delete deletedTimer;
                continue;
            }
            else 
            {
                ++iter;
            }
        }
    }
}
```

以上就是定时器的设计的基本思路，你一定要明白在在这个流程中一个定时器对象具有哪些属性，以及定时器对象该如何管理。当然，这里自顶向下一共三层结构，分别是 EventLoop、TimerManager、Timer，其中 TimerManager 对象不是必需的，在一些设计中直接用 EventLoop 封装相应方法对 Timer 对象进行管理。

理解了 one thread one loop 中定时器的设计之后，我们来看下上述定时器实现中的性能问题。

## **3.定时器效率优化**

上述定时器实现中存在严重的性能问题，即每次我们检测定时器对象是否触发都要遍历定时器集合，移除定时器对象时也需要遍历定时器集合，其实我们可以将定时器按过期时间从小到大排序，这样我们检测定时器对象时，只要从最小的过期时间开始检测，一旦找到过期时间大于当前时间的定时器对象，后面的定时器对象就不需要再判断了。

### **3.1 定时器对象集合的数据结构优化一**

我们可以在每次将定时器对象添加到集合时自动进行排序，如果我们仍然使用 std::list 作为定时器集合，我们可以给 std::list 自定义一个排序函数（从小到大排序）。实现如下：

```text
//Timer.h
class Timer
{
public:
		//无关代码省略...

    int64_t getExpiredTime() const
    {
        return m_expiredTime;
    }
};
```



```text
//TimerManager.h
struct TimerCompare  
{  
    bool operator() (const Timer* lhs, const Timer* rhs)  
    {  
        return lhs->getExpiredTime() <  rhs->getExpiredTime();
    }
}
```

每次添加定时器时调用下自定义排序函数对象 **TimerCompare** （代码第 8 行）：

```text
int64_t TimerManager::addTimer(int32_t repeatedCount, int64_t interval, const TimerCallback& timerCallback)
{
    Timer* pTimer = new Timer(repeatedCount, interval, timerCallback);

    m_listTimers.push_back(pTimer);

    //对定时器对象按过期时间从小到大排序
    m_listTimers.sort(TimerCompare());

    return pTimer->getId();
}
```

现在我们检测排序定时器是否触发就可以按过期时间从最小的一直找到大于当前系统时间的定时器对象就可以结束了：

```text
void TimerManager::checkAndHandleTimers()
{
    //遍历过程中是否调整了部分定时器的过期时间
    bool adjusted = false;
    Timer* deletedTimer;
    
    for (auto iter = m_listTimers.begin(); iter != m_listTimers.end(); )
    {
        if ((*iter)->isExpired())
        {
            (*iter)->run();
            
            if ((*iter)->getRepeatedTimes() == 0)
            {
                //定时器不需要再触发时从集合中移除该对象
                deletedTimer = *iter;
                iter = m_listTimers.erase(iter);
                delete deletedTimer;
                continue;
            }
            else 
            {
                ++iter;
                //标记下集合中有定时器调整了过期时间
                adjusted = true;
            }
        }
        else
        {
            //找到大于当前系统时间的定时器对象就不需要继续往下检查了，退出循环
            break;
        }// end if      
    }// end for-loop

    //由于调整了部分定时器的过期时间，需要重新排序一下
    if (adjusted)
    {
        m_listTimers.sort(TimerCompare());
    }
}
```

上述代码中有个细节需要注意：假设现在系统时刻是 now，定时器集合中定时器过期时间从小到大依次为 t1、t2、t3、t4、t5 ... tn，假设 now < t4 且 now > t5，即 t1、t2、t3、t4 对应的定时器会触发，触发后，会从 t1、t2、t3、t4 减去对应的时间间隔，减去之后，新的 t1’、t2‘、t3‘、t4’ 就不一定小于 t5 ~ tn 了，因此需要再次对定时器集合进行排序。但是，存在一种情形：如果 t1~t5 正好触发后其对应的触发次数变为 0，因此需要从定时器列表中移除它们，这种情形下又不需要对定时器列表进行排序。因此上述代码使用了一个 adjusted 变量以记录是否有过期时间被更新且未从列表中移除的定时器对象，如果有则之后再次对定时器集合进行排序。

上述设计虽然解决了定时器遍历效率低下的问题，但是没法解决移除一个定时器仍然需要遍历的问题， 使用链表结构的 std::list 插入非常方便但定位某个具体的元素效率就比较低了。我们将 std::list 换成 std::map 再试试，当然我们仍然需要对 std::map 中的定时器对象按过期时间进行自定义排序。

### **3.2 定时器对象集合的数据结构优化二**

Timer.h 和 Timer.cpp 文件保持不变，修改后的 TimerManager.h 与 TimerManager.cpp 文件如下：

**TimerManager.h**

```text
#ifndef __TIMER_MANAGER_H__
#define __TIMER_MANAGER_H__

#include <stdint.h>
#include <map>

#include "Timer.h"

struct TimerCompare  
{  
    bool operator () (const Timer* lhs, const Timer* rhs)  
    {  
        return lhs->getExpiredTime() <  rhs->getExpiredTime();
    }
}; 

void defaultTimerCallback()
{

}

class TimerManager
{
public:
    TimerManager() = default;
    ~TimerManager() = default;

    /** 添加定时器
     * @param repeatedCount 重复次数
     * @param interval      触发间隔
     * @param timerCallback 定时器回调函数
     * @return              返回创建成功的定时器id
     */
    int64_t addTimer(int32_t repeatedCount, int64_t interval, const TimerCallback& timerCallback);

    /** 移除指定id的定时器
     * @param timerId 待移除的定时器id
     * @return 成功移除定时器返回true，反之返回false
     */
    bool removeTimer(int64_t timerId);

    /** 检测定时器是否到期，如果到期则触发定时器函数
     */
    void checkAndHandleTimers();


private:
    //key是定时器id，value是定时器对象，注意模板的第三个参数是自定义排序对象TimerCompare
    std::map<int64_t, Timer*, TimerCompare>   m_mapTimers;
};

#endif //!__TIMER_MANAGER_H__
```

**TimerManager.cpp**

```text
#include "TimerManager.h"
    
int64_t TimerManager::addTimer(int32_t repeatedCount, int64_t interval, const TimerCallback& timerCallback)
{
    Timer* pTimer = new Timer(repeatedCount, interval, timerCallback);
    int64_t timerId = pTimer->getId();

    //插入时会自动按TimerCompare对象进行排序
    m_mapTimers[timerId] = pTimer;

    return timerId;
}

bool TimerManager::removeTimer(int64_t timerId)
{
    auto iter = m_mapTimers.find(timerId);
    if (iter != m_mapTimers.end())
    {
        m_mapTimers.erase(iter);
        return true;
    }

    return false;
}

void TimerManager::checkAndHandleTimers()
{
    //遍历过程中是否调整了部分定时器的过期时间
    bool adjusted = false;
    for (auto iter = m_mapTimers.begin(); iter != m_mapTimers.end(); )
    {
        if (iter->second->isExpired())
        {
            iter->second->run();
            
            if (iter->second->getRepeatedTimes() == 0)
            {
                //定时器不需要触发从集合中移除该对象
                iter = m_mapTimers.erase(iter);
                delete (iter->second);
                continue;
            }
            else 
            {
                ++iter;
                //标记下集合中有定时器调整了过期时间
                adjusted = true;
            }
        }
        else
        {
            //找到大于当前系统时间的定时器对象就不需要继续往下检查了，退出循环
            break;
        }
        
    }

    //由于调整了部分定时器的过期时间，需要重新排序一下
    if (adjusted)
    {
        std::map<int64_t, Timer*, TimerCompare> localMapTimers;    
        for (const auto& iter : m_mapTimers)
        {
            localMapTimers[iter.first] = iter.second;
        }

        m_mapTimers.clear();
        m_mapTimers.swap(localMapTimers);
    }
}
```

上述实现中，无论使用 std::list 还是使用 std::map 后，当某个定时器对象需要调整过期时间后仍然要对整体进行排序，这样效率非常低。

### **3.3 定时器对象集合的数据结构优化三**

实际上，为了追求定时器的效率，我们一般有两种常用的方法，**时间轮**和**时间堆**。

**时间轮**

我们先来介绍时间轮的如何实现。

**时间轮**的基本思想是将从现在时刻 t 加上一个时间间隔 interval，以 interval 为步长，将各个定时器对象的过期时间按步长分布在不同的时间槽（time slot）中，当一个时间槽中出现多个定时器对象时，这些定时器对象按加入槽中的顺序串成链表，时间轮的示意图如下：

![img](https://pic1.zhimg.com/80/v2-ab5726c14e6c621ce1933db19b4719b8_720w.webp)

因为每个时间槽的时间间隔是一定的，因此对时间轮中的定时器对象的检测会有两种方法：

第一种方法在每次检测时判断当前系统时间处于哪个时间槽中，比该槽序号小的槽中的所有定时器都已到期，执行对应的定时器函数之后，移除不需要重新触发的定时器，或重新计算需要下一次触发的定时器对象的时间并重新计算将其移到新的时间槽中。这个适用于我们上文说的 one loop one thread 结构。

第二种方法，即每次检测时假设当前的时间与之前相比跳动了一个时间轮的间隔，这种方法适用场景比较小，也不适用于我们这里介绍的 one loop one thread 结构。

时间轮的本质实际上将一个链表按时间分组，虽然提高了一些效率，但是效率上还是存在一个的问题，尤其是当某个时间槽对应的链表较长时。

**时间堆**

再来说**时间堆**，所谓时间堆其实就是利用数据结构中的小根堆（Min Heap）来组织定时器对象，根据到期时间的大小来组织。小根堆示意图如下：

![img](https://pic2.zhimg.com/80/v2-e4534d8f6d68a8e226615bd6ff540a05_720w.webp)

如图所示，图中小根堆的各个节点代表一个定时器对象，它们按过期时间从小到大排列。使用小根堆在管理定时器对象和执行效率上都要优于前面方案中 std::list 和 std::map，这是目前一些主流网络库中涉及到定时器部分的实现，如 Libevent。我在实际项目中会使用 stl 提供的优先队列，即 std::priority_queue，作为定时器的实现，使用 std::priority_queue 的排序方式是从小到大，这是因为 std::priority 从小到大排序时其内部实现数据结构也是小根堆。

## **4.对时间的缓存**

在使用定时器功能时，我们免不了要使用获取系统时间的函数，而在大多数操作系统上获取系统时间的函数属于系统调用，一次系统调用相对于 one thread one loop 结构中的其他逻辑来说可能耗时更多，因此为了提高效率，在一些对时间要求精度不是特别高的情况，我们可能会缓存一些时间，在较近的下次如果需要系统时间，可以使用上次缓存的时间，而不是再次调用获取系统时间的函数，目前不少网络库和商业服务在定时器逻辑这一块都使用这一策略。

上述逻辑的伪码如下：

```text
while (!m_bQuitFlag)
{
	//在这里第一次获取系统时间，并缓存之
	get_system_time_and_cache();
	
	//利用上一步获取的系统时间做一些耗时短的事情
	do_something_fast_with_system_timer();
	
	//这里可以不用再次获取系统时间，而是利用第一步缓存的时间作为当前系统时间
	use_cached_time_to_check_and_handle_timers();
	
	epoll_or_select_func();

	handle_io_events();

	handle_other_things();
}
```

## **5.小结**

定时器的基本实现原理和逻辑并不复杂，核心关键点是如何设计出高效的定时器对象集合数据结构，使每次的从定时器集合中增加、删除、修改和遍历定时器对象更高效。另外，为了进一步提高定时器逻辑的执行效率，在某些场景下可能会利用上次缓存的时间去代替再一次的获取系统时间的系统调用。

定时器的设计还有其他一些需要考虑的问题，例如如果服务器机器时间被认为调提前或者延后了怎么解决，以及定时器事件的时间精度等问题。

原文地址：https://zhuanlan.zhihu.com/p/530162197

作者：linux

# 【NO.311】C++高性能大规模服务器开发实践

本文摘录自腾讯高级工程师在「全球C++及系统软件技术大会」上的专题演讲。

## 1.Lego简介

![img](https://pic3.zhimg.com/80/v2-fd9bebe0c1b7846a436e3374f574c6fa_720w.webp)

首先介绍一下 CDN。非常早期的时候有一个大牛创建了一个公司叫阿卡曼，他把服务器部署到全球各地，然后把源站的内容缓存在就近的服务器。比如我们广东，可能在深圳有个机房，我们就缓存了一份跟源站一模一样的内容。这样用户在访问这个内容的时候不用“跋山涉水”到北京的服务器去拿需要的视频了，只要在本地就能服务。所以 CDN 其实就是网络世界的快递工。

那么为什么说 CDN 大规模且需要高性能呢？首先，我们 CDN 每时每刻都会跑大概 100t 流量，我相信这个流量级别在国内也算是数一数二的。腾讯，包括腾讯云本身的访问量都非常大，QPS 基本上都是千万级，所以我们服务的是一个流量这么大，并且 QPS 并发量这么高的服务器。存储量更不用说，都是10万级别的，量级非常大。腾讯 CDN 支持的场景也非常多，业务纷繁多样，各种类型都有。

但今天我不讲怎么去运营这种大规模服务器，我主要会讲单机层面怎么去设计服务器，来尽可能地提升单机的性能和服务能力。

![img](https://pic1.zhimg.com/80/v2-7a313ffbdc993745b690c3e2126e2ac4_720w.webp)

服务器肯定要支持高性能、高QPS、高能量，但除了这一部分，我们更要考虑的是可扩展性这一块。特别要提前讲一点，其实传统的服务器基于异步调用是非常难以去写、难以去维护的，写过异步回调的同事应该知道。所以我们希望这个服务器是所有新人，包括所有开发者，都能够尽快上手的。

那怎么去衡量呢？其实就是一个写 Nginx 服务器的人需要多久能够精通，能够去把控完整的代码让它跑起来，并且出了问题能够快速排查处理？一般这种都要按月算，但是用我们的架构，我们希望新人过来，按周、按天就能够上手去写代码，去学习，去尽快投入到工作，说白了就是尽可能高地产效。

另外就是CDN友好，它的功能很多，特别是可能大家觉得除了缓存之外也没什么东西，其实产品非常多。比如直播带货，在CDN里会有很多的功能需求，比如怎么尽快地把流、把内容发放给用户，尽可能减少延迟；比如抢红包，你可能要等个10秒钟，但有人可能一瞬间、1毫秒就看到了。所以在CDN上面会不停地迭代很多新的功能特效，比如编解码，比如直播的一些特性，比如电商需要的一些边缘计算能力的开放特性。

## 2.传统Web框架

回过头来看一下传统Web框架有哪些呢？这里抽取了三个比较典型的去做对比。

![img](https://pic3.zhimg.com/80/v2-da7c0dcfc1496267cc4c6e368b572d92_720w.webp)

比如像 Nginx 就是经典的异步回调。异步回调本质上就是一个状态机，有很多的事件。比如很多网络收发的事件、网络磁盘的事件来了之后，内部怎么去流转这个状态，怎么尽快处理。这种框架的好处是性能非常高，现在大家都认为 Nginx 是服务器里性能最高的一个代表。

另外，最早协程没有 C++，所以我们找的是 Libco，是微信做的一个协程框架。我后面会讲一下协程，在考虑选型的时候有考虑这个协程，但它有其他方面的一些问题，包括怎么去开销的部分。

最后一个是 ATS。ATS 里有个 Continuation，是 CPS 编程模型，C++ 里其实也有，叫 Future/Promise。

![img](https://pic1.zhimg.com/80/v2-ae18c3c57a9b538cf109615da6d9cd94_720w.webp)

我们先来具体看一下传统异步框架。

首先它是基于事件的，另外它事件的转换去维护的时候很复杂，比如 Nginx 内部就有11个阶段，再加上各种 handler，你要去处理的事件非常多，所以为什么异步回调非常难处理，就是需要人工去维护这个状态机。你要去处理每个事件，要预想到每个流程，要知道它的每个状态，来哪个事件都要能够处理。

另外就是 Coroutine，它的主要问题在哪呢？不管任何协程都有开销，比如我们有很多的 stackless、stackful。stackful 需要去存这个协议、存这个栈的内容，那么这时候内存就有问题了。你的编程模型会受限于占空间的大小，你要写多大？会不会溢出？比如 Libco 就是用 128k。stackless 会限制你开发的模式，比如 C++ Coroutine 现在就是 stackless 的一种模式，但 stackless 对编程者也是有要求的。

还有在协程本身做切换的时候，我们认为它还是有不小的开销，切换的时候会拷贝很多内存，就会有很多 memory copy；比如这个栈可能要恢复，一些 register 可能也要重新赋值，这些恢复的开销我们测试下来还是挺可观的，特别在高 QPS 的时候。

![img](https://pic3.zhimg.com/80/v2-431acfadeda1101b51c57a2b6670b5fa_720w.webp)

然后回到我们的主题，Continuation-Passing Style 是什么概念？你可以认为是一些 Callback 的组合，Callback 一个一个去做，但在 ATS 里并不能完全实现。所以当时我们认为基于 ATS 去做这个事情的复杂度也很高，但它的想法非常好。基于回调的话它的性能相对来说是比较好的，同时它的方式对开发者非常友好。其实就是一个一个函数往后调，用法接近于串行的程序写法。早期在 C++ 里做这个事情非常复杂，到了 C++ 11 之后有了一些特性的支持才使得这个事情变得非常简单自然，包括 C++11 的一些 Lamdba, declytype 的概念。所以早期我们也没有发现比较好的，到后面才决定去做这个事情。

## 3.Lego架构实现

![img](https://pic1.zhimg.com/80/v2-4f80312a0b5170294bcd3352baf2890c_720w.webp)

然后看一下大的架构图大概是这样子，上面有一些模块，就是乐高的积木，下面很多 Epoll 的线程，基于这个线程，上面也会分层。当然这个是传统架构图，我们还有一些异步的事情。

![img](https://pic1.zhimg.com/80/v2-970fbc9d50296d9dcac83d4e518fdf14_720w.webp)

先看一下为什么异步回调这个事情会比较麻烦。比如我处理一个事情要写很多 handler，这是预状态机里的事件,怎么跳的线，要转到什么状态。异步回调里面就写这个状态机里的这个线。这个时候就很麻烦，状态非常多，每个事件都要去处理。很多时候你是不知道异常怎么跳过来的，你可能没考虑到或者考虑不完全，这时候突然就陷到了异常状态。特别在网络事件非常多的时候，比如客户端关闭、超时，比如甚至到源站，比如到远端的服务器超时等等，你完全没法考虑全，所以导致你写的时候可能认为有几个事件但写漏了几个，再去做调试就会非常复杂，而且这个状态跳转的过程也很难去维护。

另外一个非常明显的问题是代码非常分散。比如我这个事件写在哪了，每个事件都要去处理的话，我的代码是按事件去划呢，还是按状态去划分。所以我们后来就选用了 Future 和 Promise 的方式。

![img](https://pic4.zhimg.com/80/v2-956a0d71bf58e5038e0add3a744ee2b7_720w.webp)

Future 是什么概念？就是有一个事件我们认为未来会完成，但我不关心这个东西怎么完成或什么时候完成。Promise 就是这个东西能够复制回去，然后回调回 Future。

Future/Promise 跟 Continuation-Passing 有什么联系呢？Continuation-Passing 的意思是通过后续逻辑可以一个个函数不停地往后去叠加。比如我有一个处理请求的函数，接着可以再往后面传 Lambda 函数，这样在读完 Body 之后，ReadBody 返回 Future，Future在完成之后调 .Then 后面的函数。我完全不需要关心你下面的状态怎么扭转的，我只关心你业务的逻辑。这是一个串行的流程，读完 Body 写 Response，返回一些数据。然后我通过不停地去写错误、写 .Then，把整个逻辑串联起来。这就是 Continuation-Passing 的概念，也是最基础的一部分。

我们还设计了一个 Finally，我们在写代码的时候有一些异常希望在最后处理，这在 Python、Java 都有类似的概念，就是在最后 Cache 所有的异常，这样程序运行到任何状态都能够正常处理一些资源，关掉一些链接，关掉一些 file、handler，这些事情都在 Finally 去做处理。

![img](https://pic4.zhimg.com/80/v2-ef3f3f45a35082df0dedd44629b5b77b_720w.webp)

Future 和 Promise 对比是什么样子？一个直观对比，是异步回调提出一个状态机，在不停地写连线的时候去写代码，我的连线就是我的代码，然后我在代码里面会触发这个事情到什么样的状态，不停地去写，这么一个逻辑，当然维护就比较复杂。我们希望做到的方式是通过 CPS 能够把整个事情拉平，把逻辑变成串行，让用户或者编程人员不需要关心我下面的事情是怎么完成的，只要我帮你完成这个事情，让你能往后继续走，就有点像写应用程序一样，完全不需要关心服务器内部是怎么做的。这样子，新的程序员不需要像在状态机那样了解很多，比如服务器内部实现跳转怎么做，每个接口的含义是什么，服务器本身内部的编程方式等等。我只要给到你一些函数，并且你自己通过 .Then Callback 的串联方式就把整个逻辑串联起来。这是两个编程模式的直观对比。

![img](https://pic4.zhimg.com/80/v2-dc961c9ece090ea8e1023e953fc86ff7_720w.webp)

异常部分，刚也讲到了 Finally，但其实我个人认为 C++ 本身的异常不应该变成常态。我自己的观点是Fail-Fast，如果出现 Exception 非常严重就直接让服务器当掉，当然这跟应用场景有关系。像 CDN 是一个完全分布式系统，有几十万台机器，当掉一台也能够通过其他系统恢复，所以我们倒是希望服务器正常的一些异常，就是特别 Critical 的异常能够尽可能出来，而且异常本身的性能开销是非常大的。当然我不知道现在怎么样，C++ 20 应该没对这部分做优化。原来的异常会不停地去 Unwind 这个 stack，里面会存很多信息，导致其性能比较重，而且特别在网络经常出现异常的情况下，异常是常态，更不能用性能开销这么大的东西。

所以我个人在写服务器的时候不赞同去做try、cache，而是用更传统的一些方式对 Exception 结构重新做设计，在 .Then Callback 时不停地往下层抛，所以在设计方式上就有点不一样。就是我们希望后面的人去处理异常时，比如读取一个 http 请求的 body 或者读取头部时出现异常，在往回发响应数据的时候，可以自己判断出现这个异常周围应该怎么处理，而不是往上层发起这个请求的地方去做，因为我们后面会更理解这个逻辑，更理解出现异常之后你应该怎么去做处理。所以我们在做异常的时候定制了比较轻量化的异常的一个类，然后不停地往下面去抛，下面的去做处理。

刚才也说到我们有个 Finally 去做这一部分的完整清理，包括一些数据的清理，不管出现任何异常，在最后一步都能保证资源的释放，不会出现内存泄露，句柄泄漏，包括忘了关 socket 的这种异常，这是我们希望做的一个比较轻量化、高性能的 Exception 框架，可能跟 C++ 原有的 Exception 区别就比较大。

![img](https://pic1.zhimg.com/80/v2-ca3e8ac0eef179c10e9ccf3796cd1430_720w.webp)

那做完这个事情之后，我们最开始看到的异步回调会在你处理完一个请求和要做的其他事情后就把所有的逻辑浓缩到一个函数里面，这样比原来分成三个函数去看的时候会更加清晰。当一个请求来的时候我去做请求处理，再跳转另外一个事件，之后再去发起一些业务逻辑，这里写的是像源站 CDN 的一个场景，比如我要去另外一个地方获取一些数据的逻辑，最后我甚至要写一个错误处理，所有东西都放到一个函数里面，这样逻辑其实是更清晰的，新人一眼就能看懂这个代码逻辑是干什么的，不用到处去看。这就是我们选取 Future/Promise 时达到的一个效果。

![img](https://pic1.zhimg.com/80/v2-166a3c4be6973fc208fed4b8e4260494_720w.webp)

Future/Promise 讲起来非常简单，那它背后的实现是什么样子？虽然说起来它只是一个函数，但这个函数到底层怎么去保证。它其实就是一个代码块，我们在做的时候就变成了对代码块的调度，虽然程序员看到 ReadRequest 这么一个函数会认为它是异步的，但异步怎么执行呢？它到底层是怎么样实现的呢？一个异步代码块在一个框架底层的时候，我们会做一个调度器，把所有生成的 ReadRequest 的 Future 变成一个个 task，从底层去做调度，这样就达到了在上面写完之后，下面自动去扭转这个状态去做这些事情。

![img](https://pic4.zhimg.com/80/v2-fa9bf359b6b9d14fe392db18ff1f0023_720w.webp)

另外我们还做了很多的性能优化，包括 Future Folding 这一部分，跟我们调度器是非常相关的。因为我们以前遇到过 Future 去不停地做 .Then 的时候，最开始是以批量压入的方式去做的，后面发现这么写的话，比如你的 copy 非常多的时候，不停压栈就会发现一些内存的溢出，这个跟框架就比较相关了。但是刚说到 Folding 的部分也提到一点就是 Future 在代码里面会大量的创建，所以我们选取了这个编程模型之后就发现有个问题，大量创建 Future 之后内存是会炸的，比如并发1万个请求，业务逻辑有十个，内部可能要创建10万个 Future/Promise 的对象，所以导致内存不停的申请示范，开销非常大。所以在这过程中我们就发现需要去做一些解决方案，经过研究之后发现有两个，一个最简单的是替换 malloc库，后来发现 jemalloc 是比较好的，特别在这个场景下。另外，我们针对每一个进程去分配一个 ThreadLocal 的内存池，让内存在本地就有些缓存，不需要去内核申请很多内存。如果对内存分配比较熟悉的话就知道malloc，tmalloc有很多的内存管理，包括去内存，用 brk 申请大一点的内存，所以我们的做法是提前申请一大批，自己去管，完全不用库去关心这些事情。

![img](https://pic3.zhimg.com/80/v2-4673e0dcad9728b490476d3875193e26_720w.webp)

另外一个Future/Promise 遇到的问题是局部变量 holder 的问题。这里有个 case 。程序员写代码时，在局部变量创建完之后可能做很多事情，可能去解析请求的用户来源是谁，解析完后面肯定要用，那我肯定会往后面的 copy 去传，但这样其实是有问题的。就是因为这些代码全是异步的，是一个个代码块在底层的调度器做调度，那这就有一个问题是代码块在底层做调度的 stack 上分配的变量可能已经被释放了，所以会导致这个 Future 在调度的时候 task 变量已经没有了。所以在做这个事情的时候我们就发现写 Future/Promise 对程序员有一个要求是尽可能用栈上的变量。这是我们后来才发现的一个比较小的区隔方式，它会导致你没办法跟原来一样写栈变量往下传，很多时候要用堆上的一些变量去传递。

![img](https://pic1.zhimg.com/80/v2-7a238c277f09e2af1f5dc7440c8b24d0_720w.webp)

上面讲的是我们编程模型的选择，下面我想分享一下怎么达到高性能。昨天有些演讲嘉宾也说了，现在英特尔已经发现搞不定单 CPU 去赫兹不停往上升，摩尔定律终不终结其实大家心里都是有一个小小的问号了。就单 CPU 来看，它的性能已经很难再往上提了，但现在厂商怎么做呢，一个 CPU 尽可能 pack 更多的 CPU 进去，比如早些年我看这个24核、32核、64核感觉挺多了，现在 AMD 直接有两三百核来提升单机服务器的性能，但是我们在实践的时候却发现我们在用这么高端、这么大量 CPU 的机器的时候，性能却是没有呈线性提升的，就是在服务器 Scale Up 之后，性能不是平行扩展的。

后来我们发现一个比较大的问题，之前也有很多大牛说的一些Cache 问题，就是这个内存和 CPU 之间的高速通路其实是非常拥堵的，特别在 CPU 变多了之后这条路就更堵了，所以在写高性能程序的时候，Cache 非常的重要。另外就是锁，当 CPU 变多了之后，这些锁之间的竞争开销会导致机器没办法完全发挥出硬件的性能。那现在业界算是公认比较好的能够解决现在单机性能瓶颈的一个模型就是 Shared-Nothing。每个 CPU 核上尽可能只干独立完整的一件事，当然这个事情说起来比较简单。大家写程序都会发现有很多需要共享，就是全局的一些变量，比如我的用户信息肯定是全局共享，不可能每一个 CPU 存一份，因为这样会导致很多不一致的情况。比如最简单的金融付费，一个用户付完费了，其他核心肯定也要看到这个结果，那么这就有一个问题：全局的东西怎么办？

![img](https://pic1.zhimg.com/80/v2-843247f9a1ac7ad276df37b807732880_720w.webp)

这个是 Shared-Nothing 简单的框架图，就是每个进程和线程之间都尽可能规避这个东西。每个东西做一个事情怎么做呢？其实我们现在网络层就是通过 Reuse Port 去做，非常简单，直接这么去导就行了，通过这样我们网络收发就可以线性了，但全局变量就不行，那我们怎么做？就是有一些异步的进程去处理这一部分的能力，通过 Reuse Port，网络部分有独立的线程去收发，但非常重、非常复杂的东西就通过独立的 worker 去处理这些全局的内容，但在这个过程中是没有锁的，中间会通过无锁队列去传递我想要的信息。比如我需要获取一个用户的信息，通过无锁队列这边网络收发完之后，再向这边发送一个消息，之后全局的数据处理完之后再通过无锁队列返回来，这样中间是没有任何的锁和增强的开销的，这样就达到全局数据的共享，这就是我们解决全局数据的方法理念。

但如果读取非常频繁的情况下怎么去解决呢？我们在每一个收发线程上还会做一些 ThreadLocal 的快照，发过去之后拿回来的东西在本地做缓存，每个线程上去做缓存，这样就解决了需要频繁读写的问题。熟悉 Kernel Reuse Port 的同学可能知道 Reuse Port在处理很多 case 的时候是没办法完整地持续分发的，它只会根据 IP 和 Port 去做。但这有一个问题是，比如现在在4G和wifi之间做切换，IP 和 Port 会变对吧，那变了之后再去做分发的时候，做Shared-Nothing 很重要的一个事情是用户请求一定要转发到同一个线程进程里面。那如果网络一变，它通过 Reuse Port 的方式就不会转发到同一个里面去了，那这种问题在正常 TCP 链接的部分不会出现，因为 TCP 就是根据 IP 和 Port去定义链接的，但在新的一些业务场景比如 Quic、UDP，会出现一个问题：它不是通过 IP 和 Port 来定义一个链接，而是通过协议自带的信息去做，所以在原有的内核的 Reuse Port 方式就完全失效了，没办法做 Shared-Nothing，于是我们在中间内核做了一些模块，根据一些 UDP 协议信息做转发，选择一些 ID 去做。这也是我们在做 Shared-Nothing 时遇到的链接转发的一个问题。

![img](https://pic1.zhimg.com/80/v2-0979ccc137ab4a8bbce431cb3452f9fc_720w.webp)

这里我还想讲我们在写的时候遇到另外一个 Shared-Nothing 的问题是原子变量。Share Pointer 大家应该都不陌生，但其实问题挺多的，就是我去做 Atomic Pointer 的时候，它只保证了引用计数的原子性，但在多个线程同时去读写 Share Pointer 时，它不是原子的，不保护那个指针，只保护它的 reference counter。C++ 后来也出了那个 Atomic Share Pointer，早期是通过 atomic_load 的方式来保护。这是现有的做 Pointer 在一个线程之间能够达到安全共享的两种方式。

但虽然有这些东西，用了之后我们发现问题非常多：第一，它的 referencecounter 会导致 Cache 出现问题。比如用 Share Pointer 时一个典型的场景：多个代码块之间都要读取这个指针，但在用这个指针时，原子变量增加和读写都要刷 Cache，导致 Cache 失效，那么就算我在单一线程里面做共享的时候也会发现这个东西导致 CPU 性能大规模地下降，所以在没有做跨线程这种 case 的时候，我们把 Reference Pointer 改成了正常的数据和变量，就不用那个原子的引用计数了，只用一个简单的 int 去做引用计数。

那我怎么规避不同进程之间共享呢？其实我不规避，因为我会保证它只在本进程的不同的上下文去共享，然后针对不同进程的共享，我们会采取跟 atomic_load 一样的方式，会去加一个锁，但是这个锁是有点类似于全局的部分，全局有一个 Local 的池，这个池子里我会尽可能规避这个开销。

## 4.未来展望

![img](https://pic4.zhimg.com/80/v2-41d773f47d368d12fdd7acc5d2ba25ff_720w.webp)

简单总结一下，性能部分我们主要是通过 Shared-Nothing 去解决，中间也解决了很多问题，包括 Shared_Ptr、内存管理，包括 Exception Handle 的问题，这一部分是要不停地去深挖每一个细节。性能是需要不停地去优化一个个 case 解决的，不是有一个架构就能解决所有问题。另外是可维护性，我们是通过 Future/Promise 来解决。我们抛弃了原有异步回调的方式，通过 Future/Promise 去不停地去串联代码，尽可能达到线性写代码的方式。

![img](https://pic2.zhimg.com/80/v2-f49da904c91c78d786f0e74a088ec1f5_720w.webp)

当然 Future/Promise 有一个问题是，大家都这么串行写代码，最后代码会非常长，可能会集中在一个代码块里面，这样对代码的规范管理是要求非常高的，所以在选择工具的时候，大家可能要考虑一下配套的代码管理工具，包括代码的规范。我们认为这个在写 Future/Promise 的时候会写很多，即使有非常好的代码规范和运营方式也会出现很难维护的问题，所以我们希望做到的方式是它能够完全没有任何东西，有点像现在 C++ Coroutine 做的事情，就是在一个代码块里就把事情全部解决。

![img](https://pic3.zhimg.com/80/v2-1e2cd032e7b4a12210fea6093ccb6da6_720w.webp)

但现在 Coroutine 问题也挺多，C++ Coroutine 的代码除了前面加 co_await 之外，跟正常的代码没有差别，但这对开发人员要求非常的高，你需要去实现 Awaitable 的成员函数，更像是说针对库开发者提供的，而不是向程序员开发提供的。比如你去实现四个异步函数，就要实现四遍 Awaitable 的对象，那我要实现10个、20个呢？这个复杂度非常高。另外一个问题是做 Awaitable 的时候，比如我的调度函数里面有一个 func 1，再去掉 func 2，再去掉 func 3。假设 func 3 要去做成 C++20 Coroutine 的时候，反过来可能 func 2 用户也不需要知道我这东西也能去 co_await，那 func 2 即使内部没有其他东西是 Coroutine，我也需要去实现 Awaitable，相当于整条调用链上的东西都需要去实现，即使我只有一个函数需要做异步。

![img](https://pic4.zhimg.com/80/v2-c50b1495b4bdd65a4414d9faf2930c77_720w.webp)

可能我们未来服务器上有很多东西要去做，包括现在云原生比较火的一些概念，就是这服务器怎么去做到开包即用，怎么去给到用户就能用上。上午冯富秋老师也说我们怎么尽可能开放硬件上的能力，软件跟硬件怎么去做结合，怎么尽可能去提升这部分的最大化收益。这些是现在包括 Kernel 和社区也都在讨论的 Kernel Bypassing，就是怎么绕过内核，尽可能直接触达硬件。另外就是新的一些硬件也在不停出现， 比如解密卡，包括 FPGA 这一部分，就是我可以在硬件上写我程序，写完之后你直接去用。

最后，很多时候我们希望这个服务是优化部分，就是一些可中断的服务，可能有一些东西不是特别的重要，这一部分是可以抛弃的，可以中断掉的，就比如我日常离线的一些计算，离线的一些压缩，在正常服务器执行过程中，假设低负载可以尽可能利用的时候，我可能会去跑，但如果没有事情的时候我就希望他尽早停掉，就是可中断服务的支持。

原文地址：https://zhuanlan.zhihu.com/p/528344890

作者：linux

# 【NO.312】gRPC C++开发环境搭建****

**特别需要强调,grpc需要6.3以上的gcc/g++版本，如果低于此版本的需要参考文档进行升级。**

1. cmake 、gcc的版本， ubuntu16.04默认的版本不支持。

## **1.安装必要的依赖工具**

安装必要的依赖工具

```text
sudo apt-get install autoconf automake libtool
```

如果cmake低于3.15， gcc/g++ 低于 7.0 ，请根据文档进行安装。查看版本的方式

```text
# 检查cmake版本
cmake -version
# 检查gcc/g++版本
gcc -v
g++ -v
```

### **1.1 安装 cmake**

可以下载更新的版本：

最低版本为3.15。

**1. 卸载已经安装的旧版的CMake**

```text
sudo apt-get autoremove cmake
```

**2. 文件下载解压**

```text
wget https://cmake.org/files/v3.23/cmake-3.23.0-linux-x86_64.tar.gz
```

解压：

```bash
tar zxf cmake-3.23.0-linux-x86_64.tar.gz
```

查看解压后目录:

```bash
tree -L 2 cmake-3.23.0-linux-x86_64


cmake-3.23.0-linux-x86_64
├── bin
│   ├── ccmake
│   ├── cmake
│   ├── cmake-gui
│   ├── cpack
│   └── ctest
├── doc
│   └── cmake
├── man
│   ├── man1
│   └── man7
└── share
    ├── aclocal
    ├── applications
    ├── bash-completion
    ├── cmake-3.23
    ├── emacs
    ├── icons
    ├── mime
    └── vim
```

bin下面有各种cmake家族的产品程序.

**3. 创建软链接**

注: 文件路径是可以指定的, 一般选择在`/opt` 或 `/usr` 路径下, 这里选择`/opt`

```bash
sudo mv cmake-3.23.0-linux-x86_64 /opt/cmake-3.23.0
sudo ln -sf /opt/cmake-3.23.0/bin/*  /usr/bin/
```

**4. 测试版本**

```text
ubuntu@VM-16-11-ubuntu:~/rpc$ cmake -version
cmake version 3.23.0


CMake suite maintained and supported by Kitware (kitware.com/cmake).
```

### **1.2 安装gcc/gdb**

升级gcc和gdb的版本，至少需要6.3以上的版本。

| Operating System               | Architectures | Versions            | Support Level        |
| ------------------------------ | ------------- | ------------------- | -------------------- |
| Linux - Debian, Ubuntu, CentOS | x86, x64      | clang 6+, GCC 6.3+  | Officially Supported |
| Windows 10+                    | x86, x64      | Visual Studio 2017+ | Officially Supported |
| MacOS                          | x86, x64      | XCode 12+           | Officially Supported |
| Linux - Others                 | x86, x64      | clang 6+, GCC 6.3+  | Best Effort          |

**注意：如果已经是高于7.0 不需要再次安装。**

目标： 安装 gcc g++ 7的安装包

1. 安装

```text
sudo apt-get install -y software-properties-common
sudo add-apt-repository ppa:ubuntu-toolchain-r/test
sudo apt update
sudo apt install g++-7 -y
```

1. 建立软连接并检查

```text
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 60 \
                         --slave /usr/bin/g++ g++ /usr/bin/g++-7 
sudo update-alternatives --config gcc
gcc -v
g++ -v
```

**显示7.5的版本。**

## **2.编译grpc**

推荐使用cmake的方式进行编译。 grpc安装过程比较依赖网络的通畅性（容易被墙），我是租用了**腾讯云香港服务器下载的grpc源代码**，他不仅是grpc源码本身，还依赖了很多第三方库，比如protobufer。大家可以直接使用我提供的源码包（**900+MB**，记得先解压）进行编译。

如果不能翻墙，直接采用我提供的压缩包 grpc-v1.45.2.tar.bz2，则解压

```text
解压方式: 
tar -jxf grpc-v1.45.2.tar.bz2
```

解压完直接跳到步骤 4.**编译和安装**。如果能翻墙则可以从 步骤1. **下载源码**开始。

1. **下载源码**

```text
git clone  https://github.com/grpc/grpc
```

1. **查看版本并选择合适的版本，这里选择v1.45.2相对较新的版本**

```text
git tag
git checkout v1.45.2
```

查看此时grpc目录内容的大小du -h --max-depth=1， 可以看到**427M左右**

```text
ubuntu@VM-16-11-ubuntu:~/rpc/grpc$ du -h --max-depth=1
348M    ./.git
32K ./summerofcode
1.5M    ./doc
6.5M    ./tools
4.0K    ./spm-core-include
24M ./test
80K ./cmake
3.0M    ./third_party
4.0K    ./spm-cpp-include
1.5M    ./templates
8.0K    ./.bazelci
1.9M    ./include
5.0M    ./examples
34M ./src
268K    ./etc
64K ./.github
284K    ./bazel
427M    .
```

1. **下载第三方依赖库**，下载完后会发现整个grpc目录内容明显变大

```text
git submodule update --init
```

再次查看 目录大小，**占用了1.3G**。

```text
ubuntu@VM-16-11-ubuntu:~/rpc/grpc$ du -h --max-depth=1
899M    ./.git
32K ./summerofcode
1.5M    ./doc
6.5M    ./tools
4.0K    ./spm-core-include
24M ./test
80K ./cmake
291M    ./third_party
4.0K    ./spm-cpp-include
1.5M    ./templates
8.0K    ./.bazelci
1.9M    ./include
5.0M    ./examples
34M ./src
268K    ./etc
64K ./.github
284K    ./bazel
1.3G    
```

1. **编译和安装**

```text
mkdir -p cmake/build
cd cmake/build
cmake ../..
make
sudo make install
```

## **3.protobuf安装**

**不用手动安装protobuf，不然版本可能和grcp不匹配**，必须在 grpc 执行 git submodule update --init 命令之后生成的 **third_party/protobuf** 里面编译安装对应的 protobuf。

```text
cd third_party/protobuf/
./autogen.sh 
./configure --prefix=/usr/local
make


sudo make install
sudo ldconfig  # 使得新安装的动态库能被加载


protoc --version
显示3.19.4
```

## **4.测试环境**

编译helloworld

```text
cd grpc/examples/cpp/helloworld/
mkdir build
cd build/
cmake ..
make登录后复制
```

启动服务和客户端

```text
# 启动服务端，监听在50051端口
./greeter_server
Server listening on 0.0.0.0:50051
# 启动客户端，服务端返回Hello world
./greeter_client 
Greeter received: Hello world
```

## **5.参考**

[ubuntu搭建grpc for C++开发环境wx5bb365de633ed的技术博客51CTO博客](https://link.zhihu.com/?target=https%3A//blog.51cto.com/u_13999641/2913394) **该文档提供修改grpc第三方库下载地址的方式进行安装。**

**注：**需要**GRPC源码包** 加群（**[812855908](https://link.zhihu.com/?target=https%3A//jq.qq.com/%3F_wv%3D1027%26k%3DVwGNWvaf)**）获取

## **6.辅助-使用scp命令，远程上传下载文件/文件夹**

**这里只是提供一种方式供大家可以在服务器之间传递文件，不是该节课程的内容，仅供参考。**

1. **从服务器下载文件**

```c
scp username@servername:/path/filename /local/path
```

例如: scp [ubuntu](https://link.zhihu.com/?target=https%3A//so.csdn.net/so/search%3Fq%3Dubuntu%26spm%3D1001.2101.3001.7020)@192.168.1.222:/ubuntu/data/data.txt /desktop/ubuntu 把192.168.1.222上的/ubuntu/data/data.txt 的文件下载到/desktop/ubuntu目录中

1. **上传本地文件到服务器**

```c
scp /local/path/local_filename username@servername:/path
```

例如: scp /ubuntu/learning/deeplearning.doc ubuntu@192.168.1.222:/ubuntu/learning 把本机/ubuntu/learning/目录下的deeplearning.doc文件上传到192.168.1.222这台服务器上的/ubuntu/learning目录中

1. **从服务器下载整个目录**

```c
scp -r username@servername:/path /path
```

例如: scp -r ubuntu@192.168.1.222:/home/ubuntu/data /local/local_dir “-r”命令是文件夹目录，把当前/home/ubuntu/data目录下所有文件下载到本地/local/local_dir目录中

1. **上传目录到服务器**

```c
scp  -r  /path  username@servername:/path
```

例如: scp -r /ubuntu/test ubuntu@192.168.1.222:/ubuntu/tx “-r”命令是文件夹目录，把当前/ubuntu/test目录下所有文件上传到服务器的/ubuntu/tx/目录中

原文地址：https://zhuanlan.zhihu.com/p/528131323

作者：linux

# 【NO.313】TCP 三次握手的性能优化

今天分析下 TCP 三次握手中有哪些可以优化的地方，进而提升握手的性能。

![img](https://pic3.zhimg.com/80/v2-2ce43a6a499366ee5da1f776f98c1066_720w.webp)

## 1.客户端的优化

三次握手的首要目的就是为了同步序列号。有了序列号才可以进行后续的可靠性的传输。在 TCP 中有很多功能都是依赖序列号实现的，比如流量控制、消息重传等。

在三次握手中序列号的同步是通过SYN报文同步的（SYN 全称Synchronize Sequence Number）。

三次握手的过程由协议栈自动来实现的。客户端调用 connect() 发起向服务端发送 SYN 同步报文，同时客户端的状态变为 SYN_SENT 状态，然后等待服务端回应报文。

一般情况下，客户端会在几毫秒内就会收到服务端回应的 ACK 报文。当在异常情况下，客户端迟迟收不到回应报文，则客户端会进行超时重传，而重传次数由参数 tcp_syn_retries 参数控制，默认6次。

```text
# cat /proc/sys/net/ipv4/tcp_syn_retries
6
```

当第 1 次重试发生在 1s 后，后续会以翻倍的方式在第 2、4、8、16、32 秒共做 6 次重试，最后一次重试后会等待 64 秒，若还没有收到对端发送的 ACK 报文，则中止三次握手。所以，总的耗时共 127 秒，超过 2 min。

```text
//发送重传后，需要检测当前资源使用情况
static int tcp_write_timeout(struct sock *sk)
{
struct inet_connection_sock *icsk = inet_csk(sk);
struct tcp_sock *tp = tcp_sk(sk);
int retry_until;
int mss;
/*在建立连接阶段超时，需要检测使用的路由缓存项，并获取重试次数的最大值*/
if ((1 << sk->sk_state) & (TCPF_SYN_SENT | TCPF_SYN_RECV)) {
if (icsk->icsk_retransmits)
dst_negative_advice(&sk->sk_dst_cache);
// 获取重传次数 
retry_until = icsk->icsk_syn_retries ? : sysctl_tcp_syn_retries;
} else {
...
}

/*当重传次数达到建立连接重传上限，超时重传上限或确认连接异常期间重传上限三种上限之一时，
都必须关闭套接口，并需要报告相应的错误*/
if (icsk->icsk_retransmits >= retry_until) {
/* Has it gone just too far? */
tcp_write_err(sk);
return 1;
}
return 0;
}
```

因此可以根据实际生产环境中，可以适当的调低重传次数，以便尽快的把错误暴露给应用程序。

## 2.服务端的优化

当服务端收到客户端发来的 SYN 报文后，服务端回复 SYN+ACK 报文，不仅确认了客户端的序列号，同时把本端的序列号发送给客户端。

此时服务端的状态为 SYN_RCV。在该状态下，服务器创建一个请求对象并加入到 SYN 半连接队列中，该队列中维护未完成的握手信息。当该半连接队列溢出后，服务器将无法建立起新的连接。

![img](https://pic2.zhimg.com/80/v2-f8f3f2b1b3b01eb7526ac40c9d206ea5_720w.webp)



当 SYN 半连接队列满时，服务器端直接丢弃数据包，此时客户端感知不到报文被 server 丢弃，依靠重传定时器重传。

```text
int tcp_rcv_state_process(struct sock *sk, struct sk_buff *skb,
struct tcphdr *th, unsigned len)
{
...

switch (sk->sk_state) {
case TCP_CLOSE:
goto discard;

case TCP_LISTEN:
if(th->ack)
return 1;

if(th->rst)
goto discard;

//判断是否为 SYN 包
if(th->syn) {
//调用 tcp_v4_conn_request
if (icsk->icsk_af_ops->conn_request(sk, skb) < 0)
return 1;


kfree_skb(skb);
return 0;
}
goto discard;

case TCP_SYN_SENT:

...

return 0;
}


int tcp_v4_conn_request(struct sock *sk, struct sk_buff *skb)
{
...

__u32 isn = TCP_SKB_CB(skb)->when; //重传时间戳
struct dst_entry *dst = NULL;
#ifdef CONFIG_SYN_COOKIES
int want_cookie = 0;
#else
#define want_cookie 0 /* Argh, why doesn't gcc optimize this :( */
#endif

/* Never answer to SYNs send to broadcast or multicast */
if (((struct rtable *)skb->dst)->rt_flags &
(RTCF_BROADCAST | RTCF_MULTICAST))
goto drop;

//查看半连接队列是否已满，满了报文丢弃
if (inet_csk_reqsk_queue_is_full(sk) && !isn) {
if (sysctl_tcp_syncookies) {
want_cookie = 1;
} else

goto drop;
}


//在全连接队列满的情况下，如果有 young_ack，那么直接丢弃
if (sk_acceptq_is_full(sk) && inet_csk_reqsk_queue_young(sk) > 1)
goto drop;

//分配 request_sock 内核对象
req = reqsk_alloc(&tcp_request_sock_ops);
if (!req)
goto drop;

...

drop:
return 0;
}
```

从代码中可知，tcp 两个队列满了（半连接队列和全连接队列），造成 SYN 报文被丢弃。

可以通过如下命令获取由于队列已满而导致连接失败的次数。

```text
# netstat -s | grep "SYNstoLISTEN"
1192450 SYNs to LISTEN sockets dropped
```

以上命令显示的是由于队列溢出导致 SYN 被丢弃的个数。该值是个累计值，若该数值在持续增加，可以调大 SYN 半连接队列。

半连接队列大小有 tcp_max_syn_backlog 参数控制。

```text
//默认队列最大值为128
# cat /proc/sys/net/ipv4/tcp_max_syn_backlog
128

//设置队列为1024
# echo 1024 > /proc/sys/net/ipv4/tcp_max_syn_backlog

# cat /proc/sys/net/ipv4/tcp_max_syn_backlog
1024
```

从代码中可以看到，当 SYN 半连接队列已满时，有可能会丢弃。但是我们发现，当系统开启 syncookies 时，可以在不使用 SYN 半连接队列的情况下成功建立连接。

syncookies 的工作原理是这样的：服务器根据源地址和目的地址以及客户端序列号信息等生成一个hash值作为服务端的初始序列号，放在本端的SYN+ACK 的回应报文中。当客户端回应 ACK 报文时携带该值，服务端收到 ACK 报文后，取出该值进行校验，若合法，则成功建立连接。

![img](https://pic4.zhimg.com/80/v2-b2ccf5512fe90cea01f79d1adbe86073_720w.webp)

Linux下通过修改 tcp_syncookies 参数来进行开启或关闭。

其中值为 0 表示关闭该功能， 1 表示仅当 SYN 半连接队列已满时，在启用它，2 表示无条件开启。

SYN Flood 攻击就是通过消耗光服务器上的半连接队列来使得正常的用户连接请求无法被响应。因此，应当把 tcp_syncookies 设置为 1，仅在队列满时再启用。

```text
# cat /proc/sys/net/ipv4/tcp_syncookies
1
```

当客户端收到服务端发来的 SYN+ACK 报文后，就会恢复 ACK , 同时本端状态转换为 ESTABLISHED，表示连接成功。而服务端直到接收到客户端发来的ACK 后状态才会变成 ESTABLISHED。

若服务端迟迟收不到 ACK 时，就会超时超时重传 SYN+ACK 报文。当网络繁忙不稳定时，报文丢失就会很严重，因此应该调大重发次数，反之则可以调小。有关重传次数是有参数 tcp_synack_retries 参数控制。

```text
# cat /proc/sys/net/ipv4/tcp_synack_retries
5
```

该参数默认是 5 次，与客户端重发 SYN 类似，它的重试会经历 1、2、4、8、16 秒，最后一次重试后要等待 32 秒，若仍收不到 ACK 报文，才会关闭连接，故共需等待 63 秒。

当服务端收到 ACK 后，此时内核会把连接请求从半连接队列中移出，然后移动到全连接 accept 队列中，等待进程调用 accept 函数把连接取出来。若进程不能及时调用 accept 函数，就会造成 accept 队列溢出，最终会导致建立好的 TCP 连接被丢弃。

```text
struct sock *tcp_check_req(struct sock *sk,struct sk_buff *skb,
struct request_sock *req,
struct request_sock **prev)
{
...

/* In sequence, PAWS is OK. */

if (tmp_opt.saw_tstamp && !after(TCP_SKB_CB(skb)->seq, tcp_rsk(req)->rcv_isn + 1))
req->ts_recent = tmp_opt.rcv_tsval;

...

//创建子 socket 
child = inet_csk(sk)->icsk_af_ops->syn_recv_sock(sk, skb,
req, NULL); //tcp_v4_syn_recv_sock
//当全连接队列满了，会返回空
if (child == NULL)
goto listen_overflow;

//清理半连接队列
inet_csk_reqsk_queue_unlink(sk, req, prev);
inet_csk_reqsk_queue_removed(sk, req);
//把request_sock和生成的sock进行关联，并把request_sock添加到全连接队列
inet_csk_reqsk_queue_add(sk, req, child);
return child;

listen_overflow:
/*sysctl_tcp_abort_on_overflow为0，则直接丢弃客户端发来的ack，返回空，若为1，会走到
下面embryonic_reset标签处，会发送rst报文给对对端。
因此可得:
当收到三次握手中client发送的ack报文后，若全连接队列满了，会有如下处理:
1、sysctl_tcp_abort_on_overflow 为0 server会扔掉client发来的ack，不给对端回应。
2、若为1，则会发送rst报文给对端，废掉本次握手和连接。
*/
if (!sysctl_tcp_abort_on_overflow) {
inet_rsk(req)->acked = 1;
return NULL;
}

embryonic_reset:
NET_INC_STATS_BH(LINUX_MIB_EMBRYONICRSTS);
if (!(flg & TCP_FLAG_RST))
req->rsk_ops->send_reset(sk, skb);

inet_csk_reqsk_queue_drop(sk, req, prev);
return NULL;
}
```

代码中可知，当全连接队列满时，服务器端默认会把 ACK 报文丢弃，不给对端进行回应。

我们也可以设置给对端发送 RST 复位报文进行回应，告诉客户端连接已经建立失败。打开这一功能需要将 tcp_abort_on_overflow 参数设置为 1。

```text
//默认值为0 关闭
# cat /proc/sys/net/ipv4/tcp_abort_on_overflow
0

//打开后，当accept队列满了会回应 RST
# echo 1 > /proc/sys/net/ipv4/tcp_abort_on_overflow
```

通常情况下，应该把 tcp_abort_on_overflow 设置为0，因为这样可以更有利于应对突发流量。

举个例子，当 accept 队列满导致服务器丢掉了 ACK，与此同时，客户端的连接状态却是 ESTABLISHED，客户端进程就在建立好的连接上发送请求。只要服务器没有为请求回复 ACK，客户端的请求就会被多次「重发」。如果服务器上的进程只是短暂的繁忙造成 accept 队列满，那么当 accept 队列有空位时，再次接收到的请求报文由于含有 ACK，仍然会触发服务器端成功建立连接。

所以，tcp_abort_on_overflow 设为 0 可以提高连接建立的成功率，只有你非常肯定 TCP 全连接队列会长期溢出时，才能设置为 1 以尽快通知客户端。

那么怎么调整 accept 队列的长度呢？

accept 队列的长度取决于 somaxconn 和 backlog 之间的最小值，也就是 min(somaxconn, backlog)，其中：

- somaxconn 是 Linux 内核的参数，默认值是 128，可以通过 net.core.somaxconn 来设置其值；

```text
//获取参数值
# sysctl -a | grep net.core.somaxconn
128

//设置长度
# sysctl -w net.core.somaxconn=1024

# sysctl -a | grep net.core.somaxconn
1024
```

- backlog 是 listen (int sockfd, int backlog) 函数中的 backlog 大小；Tomcat、Nginx、Apache 常见的 Web 服务的 backlog 默认值都是 511。

获取accept 队列的长度如下：

```text
/*
-l 显示正在进程的socket
-n 不解析服务名称
-t 只显示 tcp socket
*/
# ss -lnt
State Recv_Q Send_Q Local Address:Port Peer Address:Port
LISTEN 0 1024 *:8090 *:*
...
```

- Recv-Q：当前 accept 队列的大小，也就是当前已完成三次握手并等待服务端 accept() 的 TCP 连接；
- Send-Q：accept 队列最大长度，上面的输出结果说明监听 8088 端口的 TCP 服务，accept 队列的最大长度为 128；

如何查看由于 accep 队列满而导致的连接丢弃？

当 accept 队列满时，服务端则会丢掉后续的 TCP 连接，丢掉的 TCP 连接的个数会统计起来，可以通过如下命令获取：

```text
// 查看tcp accept 队列溢出情况
# netstat -s | grep overflowed
1202 times the listen queue of a socket overflowed
```

其中 1202 times 表示 accept 队列溢出的次数，是个累计值。可以每隔几秒查看一次，若该值一直在增加，说明 accept 队列满了。

如果持续不断地有连接因为 accept 队列溢出被丢弃，就应该调大 backlog 以及 somaxconn 参数。

通过上面可知，可以通过如下设置进行防御 SYN 攻击

- 增大半连接队列。
- 开启 tcp_syncookies 功能。
- 减少SYN+ACK 重传次数。

## 3.通过 TFO 技术绕过三次握手过程

以上只是对三次握手的过程进程优化，但三次握手建立的连接造成的后果就是，当HTTP请求时，必须在一次RTT (Round Trip Time ， 从客户端到服务端一个往返时间) 后才能发送数据。

在 Linux 3.7 内核版本之后，提供了 TCP Fast Open 功能，这个功能可以减少 TCP 连接建立的时延。该功能就是客户端可以在首个 SYN 报文中就携带请求，这就节省了一个 RTT 的时间。

为了能够让客户端在 SYN 报文中携带请求数据，首先要解决服务端的信任问题 。因为此时服务端的 SYN 报文还没有发给客户端，所以客户端是否能够建立起连接还未可知，但此时服务器需要假定连接已经建立成功，并把请求交付给进程去处理，所以服务器必须能够信任这个客户端。

那么 TFO 怎么达成这一目的呢？它把通讯分为 2 个阶段，第一阶段为首次建立连接，这时走三次握手过程，但是在客户端的 SYN 报文中会明确告诉服务端它想使用 TFO 功能，这样服务器就会把客户端 IP 地址用只有自己知道的密钥进行加密，作为 Cookies 携带在返回的 SYN+ACK 报文中，客户端收到后会将 Cookie 缓存在本地。

之后，若客户端再次向服务端建立连接，就可以在第一个 SYN 报文中携带请求数据，同时还要附上缓存的 Cookie。很显然，这种通信方式不能再采用“先connect 再 write 请求”这种编程方式，而要改用 sendto 或者 sendmsg 函数才能实现。

服务器收到后，会用自己的秘钥验证 Cookie 是否合法，验证通过后连接才算建立成功，再把请求交给进程处理，同时给客户端返回 SYN+ACK。虽然客户端收到后还会返回 ACK, 但服务器不等收到 ACK 就可以发送 HTTP 响应了，这就减少了握手带来的1个 RTT 的时间消耗。

![img](https://pic3.zhimg.com/80/v2-4e1412ba7485f9299c167206fc12a75a_720w.webp)

为了防止 SYN 泛洪攻击，服务器的 TFO 实现必须能够自动化的定时更新密钥。

Linux 可以通过修改 tcp_fastopen 参数进行打开 TFO 功能.

```text
# cat /proc/sys/net/ipv4/tcp_fastopen
1
```

由于只有客户端和服务端同时支持时，TFO功能才能使用，所以 tcp_fastopen 参数是按比特位控制的。其中

- 第 1 个比特位为1 时，表示作为客户端时支持 TFO;
- 第 2 个比特位为1时，表示作为服务器时支持 TFO;

所以当 tcp_fastopen 的值为3时（比特为0x11）就表示完全支持 TFO 功能。

原文地址：https://zhuanlan.zhihu.com/p/527441102

作者：linux

# 【NO.314】redis7.0源码阅读：Redis中的IO多线程（线程池）

## 1.Redis中的IO多线程原理

![img](https://pic1.zhimg.com/80/v2-692b92d99f7f593e8f48505a26585604_720w.webp)

服务端收到一条信息，给它deconde成一条命令

然后根据命令获得一个结果(reply)

然后将结果encode后，发送回去

![img](https://pic2.zhimg.com/80/v2-8195df68ac78569eea083770801aab01_720w.webp)

redis的单线程是指，命令执行(logic)都是在单线程中运行的

接受数据read和发送数据write都是可以在io多线程（线程池）中去运行

在Redis中，生产者也可以作为消费者，反之亦然，没有明确界限。

![img](https://pic2.zhimg.com/80/v2-3f1dc75e56371654568370361c721ecd_720w.webp)

## 2.设置io多线程（调试设置）

在redis.conf中

设置io-threads-do-reads yes就可以开启io多线程

设置io-threads 2,设置为2（为了方便调试,真正使用的时候，可以根据需要设置），其中一个为主线程，另外一个是io线程

![img](https://pic3.zhimg.com/80/v2-e8ed7ea9f3fecc749e958e9e24f65a5a_720w.webp)

在networking.c中找到stopThreadedIOIfNeeded，如果在redis-cli中输入一条命令，是不会执行多线程的，因为它会判断，如果pending（需要做的命令）个数比io线程数少，就不会执行多线程

因此提前return 0，确保执行多线程,便于调试

```text
int stopThreadedIOIfNeeded(void) {
    int pending = listLength(server.clients_pending_write);

    /* Return ASAP if IO threads are disabled (single threaded mode). */
    if (server.io_threads_num == 1) return 1;
    return 0;//为了调试，提前退出（自己添加的一行）
    if (pending < (server.io_threads_num*2)) {
        if (server.io_threads_active) stopThreadedIO();
        return 1;
    } else {
        return 0;
    }
}
```

到此为止，只需要，运行redis-server,在networking.c的 readQueryFromClient中打个断点，然后在redis-cli中输入任意set key value就可以进入io多线程，进行调试

下图可以看到箭头指向的两个线程，一个是主线程，另一个是io线程

![img](https://pic1.zhimg.com/80/v2-5022d9afdf925c375a3412ab18f10d00_720w.webp)



## 3.Redis中的IO线程池

### 3.1读取任务`readQueryFromClient`

`postponeClientRead(c)`就是判断io多线程模式，并将任务添加到 任务队列中

```text
void readQueryFromClient(connection *conn) { 
    client *c = connGetPrivateData(conn);
    int nread, big_arg = 0;
    size_t qblen, readlen;

    /* Check if we want to read from the client later when exiting from
     * the event loop. This is the case if threaded I/O is enabled. */
    if (postponeClientRead(c)) return; 
	//后面省略......
}
```

### 3.2主线程将 待读客户端 添加到Read任务队列（生产者）`postponeClientRead`

如果是io多线程模式，那么将任务添加到任务队列。
（这个函数名的意思，延迟读，就是将任务加入到任务队列，后续去执行）

```text
int postponeClientRead(client *c) {
    if (server.io_threads_active &&
        server.io_threads_do_reads &&
        !ProcessingEventsWhileBlocked &&
        !(c->flags & (CLIENT_MASTER|CLIENT_SLAVE|CLIENT_BLOCKED)) &&
        io_threads_op == IO_THREADS_OP_IDLE)
    {
        listAddNodeHead(server.clients_pending_read,c);//往任务队列中插入任务
        c->pending_read_list_node = listFirst(server.clients_pending_read);
        return 1;
    } else {
        return 0;
    }
}
```

### 3.3多线程Read IO任务 `handleClientsWithPendingReadsUsingThreads`

基本原理和多线程Write IO是一样的，直接看多线程Write IO就行了。

其中`processInputBuffer`是解析协议

```text
int handleClientsWithPendingReadsUsingThreads(void) {
    if (!server.io_threads_active || !server.io_threads_do_reads) return 0;
    int processed = listLength(server.clients_pending_read);
    if (processed == 0) return 0;

    /* Distribute the clients across N different lists. */
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_read,&li);
    int item_id = 0;

    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;
    }

    /* Give the start condition to the waiting threads, by setting the
     * start condition atomic var. */
    io_threads_op = IO_THREADS_OP_READ;
    for (int j = 1; j < server.io_threads_num; j++) {
        int count = listLength(io_threads_list[j]);
        setIOPendingCount(j, count);
    }

    /* Also use the main thread to process a slice of clients. */
    listRewind(io_threads_list[0],&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        readQueryFromClient(c->conn);
    }
    listEmpty(io_threads_list[0]);

    /* Wait for all the other threads to end their work. */
    while(1) {
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += getIOPendingCount(j);
        if (pending == 0) break;
    }

    io_threads_op = IO_THREADS_OP_IDLE;

    /* Run the list of clients again to process the new buffers. */
    while(listLength(server.clients_pending_read)) {
        ln = listFirst(server.clients_pending_read);
        client *c = listNodeValue(ln);
        listDelNode(server.clients_pending_read,ln);
        c->pending_read_list_node = NULL;

        serverAssert(!(c->flags & CLIENT_BLOCKED));

        if (beforeNextClient(c) == C_ERR) {
            /* If the client is no longer valid, we avoid
             * processing the client later. So we just go
             * to the next. */
            continue;
        }

        /* Once io-threads are idle we can update the client in the mem usage buckets */
        updateClientMemUsageBucket(c);

        if (processPendingCommandsAndResetClient(c) == C_ERR) {
            /* If the client is no longer valid, we avoid
             * processing the client later. So we just go
             * to the next. */
            continue;
        }

        if (processInputBuffer(c) == C_ERR) {
            /* If the client is no longer valid, we avoid
             * processing the client later. So we just go
             * to the next. */
            continue;
        }

        /* We may have pending replies if a thread readQueryFromClient() produced
         * replies and did not install a write handler (it can't).
         */
        if (!(c->flags & CLIENT_PENDING_WRITE) && clientHasPendingReplies(c))
            clientInstallWriteHandler(c);
    }

    /* Update processed count on server */
    server.stat_io_reads_processed += processed;

    return processed;
}
```

### 3.4多线程write IO任务（消费者）handleClientsWithPendingWritesUsingThreads

1.判断是否有必要开启IO多线程

2.如果没启动IO多线程，就启动IO多线程

3.负载均衡：write任务队列，均匀分给不同io线程

4.启动io子线程

5.主线程执行io任务

6.主线程等待io线程写结束

```text
/* This function achieves thread safety using a fan-out -> fan-in paradigm:
 * Fan out: The main thread fans out work to the io-threads which block until
 * setIOPendingCount() is called with a value larger than 0 by the main thread.
 * Fan in: The main thread waits until getIOPendingCount() returns 0. Then
 * it can safely perform post-processing and return to normal synchronous
 * work. */
int handleClientsWithPendingWritesUsingThreads(void) {
    int processed = listLength(server.clients_pending_write);
    if (processed == 0) return 0; /* Return ASAP if there are no clients. */

    /* If I/O threads are disabled or we have few clients to serve, don't
     * use I/O threads, but the boring synchronous code. */
    if (server.io_threads_num == 1 || stopThreadedIOIfNeeded()) {//判断是否有必要开启IO多线程
        return handleClientsWithPendingWrites();
    }

    /* Start threads if needed. */
    if (!server.io_threads_active) startThreadedIO();//开启io多线程

    /* Distribute the clients across N different lists. */
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_write,&li);//创建一个迭代器li，用于遍历任务队列clients_pending_write
    int item_id = 0;//默认是0，先分配给主线程去做（生产者也可能是消费者），如果设置成1，则先让io线程1去做
    //io_threads_list[0] 主线程
    //io_threads_list[1] io线程
    //io_threads_list[2] io线程   
    //io_threads_list[3] io线程   
    //io_threads_list[4] io线程
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);//取出一个任务
        c->flags &= ~CLIENT_PENDING_WRITE;

        /* Remove clients from the list of pending writes since
         * they are going to be closed ASAP. */
        if (c->flags & CLIENT_CLOSE_ASAP) {//表示该客户端的输出缓冲区超过了服务器允许范围,将在下一次循环进行一个关闭,也不返回任何信息给客户端，删除待读客户端
            listDelNode(server.clients_pending_write, ln);
            continue;
        }

        /* Since all replicas and replication backlog use global replication
         * buffer, to guarantee data accessing thread safe, we must put all
         * replicas client into io_threads_list[0] i.e. main thread handles
         * sending the output buffer of all replicas. */
        if (getClientType(c) == CLIENT_TYPE_SLAVE) {
            listAddNodeTail(io_threads_list[0],c);
            continue;
        }
        //负载均衡：将任务队列中的任务 添加 到不同的线程消费队列中去，每个线程就可以从当前线程的消费队列中取任务就行了
        //这样做的好处是，避免加锁。当前是在主线程中，进行分配任务
        //通过取余操作，将任务均分给不同io线程
        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;
    }

    /* Give the start condition to the waiting threads, by setting the
     * start condition atomic var. */
    io_threads_op = IO_THREADS_OP_WRITE;
    for (int j = 1; j < server.io_threads_num; j++) {
        int count = listLength(io_threads_list[j]);
        setIOPendingCount(j, count);//设置io线程启动条件，启动io线程
    }

    /* Also use the main thread to process a slice of clients. */
    listRewind(io_threads_list[0],&li);//让主线程去处理一部分任务（io_threads_list[0]）
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);
        writeToClient(c,0);
    }
    listEmpty(io_threads_list[0]);

    /* Wait for all the other threads to end their work. */
    while(1) {//剩下的任务io_threads_list[1]，io_threads_list[2].....给io线程去做，等待io线程完成任务
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += getIOPendingCount(j);//等待io线程结束，并返回处理的数量
        if (pending == 0) break;
    }

    io_threads_op = IO_THREADS_OP_IDLE;

    /* Run the list of clients again to install the write handler where
     * needed. */
    listRewind(server.clients_pending_write,&li);
    while((ln = listNext(&li))) {
        client *c = listNodeValue(ln);

        /* Update the client in the mem usage buckets after we're done processing it in the io-threads */
        updateClientMemUsageBucket(c);

        /* Install the write handler if there are pending writes in some
         * of the clients. */
        if (clientHasPendingReplies(c) &&
                connSetWriteHandler(c->conn, sendReplyToClient) == AE_ERR)
        {
            freeClientAsync(c);
        }
    }
    listEmpty(server.clients_pending_write);

    /* Update processed count on server */
    server.stat_io_writes_processed += processed;

    return processed;
}
```

负载均衡：将任务队列中的任务 添加 到不同的线程消费队列中去，每个线程就可以从当前线程的消费队列中取任务就行了。这样做的好处是，避免加锁。当前是在主线程中，进行分配任务通过取余操作，将任务均分给不同的io线程。

## 4.线程调度

### 4.1开启io线程startThreadedIO

每个io线程都有一把锁，如果主线程把锁还回去了，那么io线程就会启动，不再阻塞

并设置io线程标识为活跃状态io_threads_active=1

```text
void startThreadedIO(void) {
    serverAssert(server.io_threads_active == 0);
    for (int j = 1; j < server.io_threads_num; j++)
        pthread_mutex_unlock(&io_threads_mutex[j]);
    server.io_threads_active = 1;
}
```

### 4.2关闭io线程`stopThreadedIO`

每个io线程都有一把锁，如果主线程拿了，那么io线程就会阻塞等待，也就是停止了IO线程
并设置io线程标识为非活跃状态`io_threads_active=0`

```text
void stopThreadedIO(void) {
    /* We may have still clients with pending reads when this function
     * is called: handle them before stopping the threads. */
    handleClientsWithPendingReadsUsingThreads();
    serverAssert(server.io_threads_active == 1);
    for (int j = 1; j < server.io_threads_num; j++)
        pthread_mutex_lock(&io_threads_mutex[j]);//
    server.io_threads_active = 0;
}
```

原文地址：https://zhuanlan.zhihu.com/p/527166907

作者：linux

# 【NO.315】linux c/c++开发：多线程并发锁：互斥锁、自旋锁、原子操作、CAS

## 1.多线程计数



![img](https://pic2.zhimg.com/80/v2-3bf36685131097c0d734fbfdb996c165_720w.webp)



背景：

火车抢票，总共10个窗口，每个窗口都同时进行10w张抢票

可以采用多线程的方式，火车票计数是公共的任务

```text
#include<pthread.h>//posix线程 
#include<stdio.h>
#include<unistd.h>

#define THREAD_COUNT 10  //定义线程数10


//线程入口函数
void* thread_callback(void* arg){ 
    int* pcount=(int*)arg;
    int i=0;
    while(i++<100000){
        (*pcount)++;
        usleep(1);//单位微秒
    }
}

//10个窗口，同时对count进行++操作
int main(){

    pthread_t threadid[THREAD_COUNT]={0};//初始化线程id

    int count=0;
    for(int i=0;i<THREAD_COUNT;i++){//创建10个线程
        //第一个参数：返回线程。 第二个参数：线程的属性（堆栈）。第三个：线程的入口函数。第四个：主线程往子线程传的参数
        pthread_create(&threadid[i],NULL,thread_callback,&count);//count是传入thread_callback内的
    }

    for(int i=0;i<100;i++){
        printf("count: %d\n",count);
        sleep(1);//单位秒
    }
}
```

虽然包含了线程的头文件<pthread.h>，可是编译的时候却报错“对pthread_create未定义的引用“，原来时因为 pthread库不是Linux系统默认的库，连接时需要使用库libpthread.a,所以在使用pthread_create创建线程时，在编译中要加-lpthread参数

可以使用下面方法编译

```text
g++ thread_count.cpp -o thread_count -lpthread
```

执行后，发现，按理说要执行到100w,可是停到99w多就结束了。



![img](https://pic3.zhimg.com/80/v2-0cda2303c652427d1b4f7365b9f83636_720w.webp)



## 2.发现问题

理想状态，线程应该是这样的



![img](https://pic2.zhimg.com/80/v2-e1b1af397983a3b35508f5598dc26881_720w.webp)



但实际上存在，执行完线程1MOV操作后，线程1切换到线程2。导致两个线程的操作，本应该50->52，但是结果确实50->51



![img](https://pic1.zhimg.com/80/v2-b906f842d2827de23bc26d0dd64d27f8_720w.webp)



count是一个临界资源（两个线程共享一个变量），因此为了避免上述这种情况发生，要加锁



## 3.互斥锁

当一个线程在执行一个指令的时候，另一个线程进不来。

相当于把count++转化为汇编的3行命令给打包在一起。

定义互斥锁

```text
pthread_mutex_t mutex;//定义互斥锁
```

初始化互斥锁

```text
pthread_mutex_init(&mutex,NULL);//互斥锁初始化（第二个参数是 锁的属性）
```

加锁/解锁

```text
//加了互斥锁
 pthread_mutex_lock(&mutex);
 (*pcount)++;
 pthread_mutex_unlock(&mutex);
```

完整代码

```text
#include<pthread.h>
#include<stdio.h>
#include<unistd.h>

#define THREAD_COUNT 10

pthread_mutex_t mutex;//定义互斥锁

void* thread_callback(void* arg){
    int* pcount=(int*)arg;
    int i=0;
    while(i++<100000){
#if 0
        (*pcount)++;
#else   
        //加了互斥锁
        pthread_mutex_lock(&mutex);
        (*pcount)++;
        pthread_mutex_unlock(&mutex);
#endif
        usleep(1);//单位微秒

    }
}

//10个窗口，同时对count进行++操作
int main(){

    pthread_t threadid[THREAD_COUNT]={0};//初始化线程id
    pthread_mutex_init(&mutex,NULL);//互斥锁初始化（第二个参数是 锁的属性）

    int count=0;
    for(int i=0;i<THREAD_COUNT;i++){//创建10个线程
        //第一个参数：返回线程。 第二个参数：线程的属性（堆栈）。第三个：线程的入口函数。第四个：主线程往子线程传的参数
        pthread_create(&threadid[i],NULL,thread_callback,&count);//count是传入thread_callback内的
    }

    for(int i=0;i<100;i++){
        printf("count: %d\n",count);
        sleep(1);//单位秒
    }
}
```

## 4.自旋锁

在写法上和互斥锁基本上没有差别

定义自旋锁

```text
pthread_spinlock_t spinlock;//定义自旋锁
```

初始化自旋锁

```text
pthread_spin_init(&spinlock,PTHREAD_PROCESS_SHARED);//自旋锁初始化（第二个参数是 进程共享）
```

自旋锁（加锁/解锁）

```text
//加了自旋锁
pthread_spin_lock(&spinlock);
(*pcount)++;
pthread_spin_unlock(&spinlock);
```

完整代码

```text
#include<pthread.h>
#include<stdio.h>
#include<unistd.h>

#define THREAD_COUNT 10
// pthread_mutex_t mutex;
pthread_spinlock_t spinlock;//定义自旋锁
void* thread_callback(void* arg){
    int* pcount=(int*)arg;
    int i=0;
    while(i++<100000){
#if 0
        (*pcount)++;
#elif 0 
        //加了互斥锁
        pthread_mutex_lock(&mutex);
        (*pcount)++;
        pthread_mutex_unlock(&mutex);
#else 
        //加了自旋锁
        pthread_spin_lock(&spinlock);
        (*pcount)++;
        pthread_spin_unlock(&spinlock);
#endif
        usleep(1);

    }
}

int main(){

    pthread_t threadid[THREAD_COUNT]={0};
    // pthread_mutex_init(&mutex,NULL);
    pthread_spin_init(&spinlock,PTHREAD_PROCESS_SHARED);//自旋锁初始化（第二个参数是 进程共享）
    int count=0;
    for(int i=0;i<THREAD_COUNT;i++){
        pthread_create(&threadid[i],NULL,thread_callback,&count);
    }

    for(int i=0;i<100;i++){
        printf("count: %d\n",count);
        sleep(1);//单位秒
    }
}
```

## 5.互斥锁和自旋锁的对比

使用场景：

当锁的内容很少的时候，继续等待的时间代价比 线程切换的时间代价更小的 时候，选择使用自旋锁（因为互斥锁会切换线程，等待重新调度请求，判断锁是否被占用，如果占用，继续阻塞，并切换到其他线程。如果切换线程的代价比 等待的代价大，可以使用自旋锁。否则使用互斥锁）。

锁的内容比较多的时候，使用互斥锁。（比如，线程安全的红黑树，可以使用mutex）

也就是说：

锁的内容少/没有系统调用，等待的时间代价少-》用自旋锁

锁的内容多，等待时间代价大-》用互斥锁

## 6.原子操作

如果把这三条指令变成一条，那么就不会出现，这种问题了。



![img](https://pic2.zhimg.com/80/v2-9d67d540d39f8546df58047b8cc9082d_720w.webp)



原子操作：单条cpu指令实现（因此使用范围有限，必须要是cpu指令中有的指令）



![img](https://pic3.zhimg.com/80/v2-3bde1236456587c396114c80d8311a3e_720w.webp)



完整代码

```text
#include<pthread.h>
#include<stdio.h>
#include<unistd.h>

#define THREAD_COUNT 10
// pthread_mutex_t mutex;
// pthread_spinlock_t spinlock;


//用一个函数去实现
int increase(int *value,int add){
    int old;
    __asm__ volatile(
        "lock;xaddl %2,%1;"
        :"=a"(old)
        :"m"(*value),"a"(add)
        :"cc","memory"
    );

    return old;
}

void* thread_callback(void* arg){
    int* pcount=(int*)arg;
    int i=0;
    while(i++<100000000){
#if 0
        (*pcount)++;
#elif 0 
        //加了互斥锁
        pthread_mutex_lock(&mutex);
        (*pcount)++;
        pthread_mutex_unlock(&mutex);
#elif 0 
        //加了自旋锁
        pthread_spin_lock(&spinlock);
        (*pcount)++;
        pthread_spin_unlock(&spinlock);
#else
		//原子操作
        increase(pcount,1);
#endif
        usleep(1);

    }
}

int main(){

    pthread_t threadid[THREAD_COUNT]={0};
    // pthread_mutex_init(&mutex,NULL);
    // pthread_spin_init(&spinlock,PTHREAD_PROCESS_SHARED);//自旋锁初始化（第二个参数是 进程共享）
    int count=0;
    for(int i=0;i<THREAD_COUNT;i++){
        pthread_create(&threadid[i],NULL,thread_callback,&count);
    }

    for(int i=0;i<100;i++){
        printf("count: %d\n",count);
        sleep(1);//单位秒
    }
}
```

## 7.其他：CAS

CAS：compare and swap

cpu有这样一条指令cmpxchg(a,b,c)，(其实就是原子操作的原理)。

它的意思是

```text
if(a==b){
	a=c;
}
```

下面例子中，instance就是a，NULL是b，c是malloc(sizeof(object))

```text
if(instance==NULL){
	instance=malloc(sizeof(object));
}
```

使用cas实现求和

```text
#include<stdio.h>
#include<stdlib.h>
#include<unistd.h>
#include<string.h>
#include<pthread.h>


#define CAS(a_ptr, a_oldVal, a_newVal) __sync_bool_compare_and_swap(a_ptr, a_oldVal, a_newVal)
// #define AtomicAdd(a_ptr,a_count) __sync_fetch_and_add (a_ptr, a_count)
#define THREAD_COUNT 10


void* callback(void* arg){
    int* pcount=(int*)arg;
    for(int i=0;i<100000;i++){
        while(true){
            int current=(*pcount);
            if(CAS(pcount,current,current+1)) break;
        }
        usleep(1);
    }
    return (void*)nullptr;
}

int main(int argc,char** argv){
    pthread_t threadid[THREAD_COUNT]={0};
    int count=0;
    for(int i=0;i<THREAD_COUNT;i++){//创建10个线程
        pthread_create(&threadid[i],NULL,callback,&count);
    }
    for(int i=0;i<100;i++){
        printf("count: %d\n",count);
        sleep(1);//单位秒
    }
}
```

原文地址：https://zhuanlan.zhihu.com/p/527010381

作者：linux

# 【NO.316】作为程序员，如何彻底理解高并发中的协程

作为程序员，想必你多多少少听过**协程**这个词，这项技术近年来越来越多的出现在程序员的视野当中，尤其高性能高并发领域。当你的同学、同事提到协程时**如果你的大脑一片空白，对其毫无概念。。。**

![img](https://pic1.zhimg.com/80/v2-8e644b4905f84d7869da06d7eab4cc60_720w.webp)

那么这篇文章正是为你量身打造的。话不多说，今天的主题就是作为程序员，你应该如何彻底理解协程。

**普通的函数**

我们先来看一个普通的函数，这个函数非常简单：

```text
def func():
   print("a")
   print("b")
   print("c")
```

这是一个简单的普通函数，当我们调用这个函数时会发生什么？

1. 调用func
2. func开始执行，直到return
3. func执行完成，返回函数A

是不是很简单，函数func执行直到返回，并打印出：

```text
a
b
c
```

So easy，有没有，有没有！

![动图封面](https://pic1.zhimg.com/v2-2682e7b6f1d6311effccb552410a3f9c_b.jpg)



很好！注意这段代码是用python写的，**但本篇关于协程的讨论适用于任何一门语言**，**因为协程并不是一种语言的特性**。而我们只不过恰好使用了python来用作示例，因其足够简单。那么协程是什么呢？

**从普通函数到协程**

接下来，我们就要从普通函数过渡到协程了。和普通函数只有一个返回点不同，协程可以有**多个返回点**。这是什么意思呢？

```text
void func() {
  print("a")
  暂停并返回
  print("b")
  暂停并返回
  print("c")
}
```

普通函数下，只有当执行完print("c")这句话后函数才会返回，但是在协程下当执行完print("a")后func就会因“暂停并返回”这段代码返回到调用函数。有的同学可能会一脸懵逼，这有什么神奇的吗？我写一个return也能返回，就像这样：

```text
void func() {
  print("a")
  return
  print("b")
  暂停并返回
  print("c")
}
```

直接写一个return语句确实也能返回，**但这样写的话return后面的代码都不会被执行到了**。协程之所以神奇就神奇在当我们从协程返回后**还能继续调用该协程**，并且是**从该协程的上一个返回点后继续执行**。这足够神奇吧，就好比孙悟空说一声“定”，函数就被暂停了：

```text
void func() {
  print("a")
  定
  print("b")
  定
  print("c")
}
```

这时我们就可以返回到调用函数，当调用函数什么时候想起该协程后可以再次调用该协程，该协程会从上一个返回点继续执行。Amazing，有没有，集中注意力，千万不要翻车。

只不过孙大圣使用的口诀“定”字，在编程语言中一般叫做yield(其它语言中可能会有不同的实现，但本质都是一样的)。需要注意的是，当普通函数返回后，进程的地址空间中不会再保存该函数运行时的任何信息，而协程返回后，函数的运行时信息是需要保存下来的，那么函数的运行时状态到底在内存中是什么样子呢，关于这个问题你可以参考**[这里](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU2NTYyOTQ4OQ%3D%3D%26mid%3D2247484963%26idx%3D1%26sn%3D542d3bec57c6a9dfc17c83005fd2c030%26chksm%3Dfcb9817dcbce086b10cb44cad7c9777b0088fb8d9d6baf71ae36a9b03e1f8ef5bec62b79d6f7%26scene%3D21%23wechat_redirect)**。接下来，我们就用实际的代码看一看协程。

**Show Me The Code**

下面我们使用一个真实的例子来讲解，语言采用python，不熟悉的同学不用担心，这里不会有理解上的门槛。在python语言中，这个“定”字同样使用关键词yield，这样我们的func函数就变成了：

```text
void func() {
  print("a")
  yield
  print("b")
  yield
  print("c")
}
```

注意，这时我们的func就不再是简简单单的函数了，而是升级成为了协程，那么我们该怎么使用呢，很简单：

```text
def A():
  co = func() # 得到该协程
  next(co)    # 调用协程
  print("in function A") # do something
  next(co)    # 再次调用该协程
```

我们看到虽然func函数没有return语句，也就是说虽然没有返回任何值，但是我们依然可以写co = func()这样的代码，意思是说co就是我们拿到的协程了。接下来我们调用该协程，使用next(co)，运行函数A看看执行到第3行的结果是什么：

```text
a
```

显然，和我们的预期一样，协程func在print("a")后因执行yield而暂停并返回函数A。接下来是第4行，这个毫无疑问，A函数在做一些自己的事情，因此会打印：

```text
a
in function A
```

接下来是重点的一行，当执行第5行再次调用协程时该打印什么呢？如果func是普通函数，那么会执行func的第一行代码，也就是打印a。但func不是普通函数，而是协程，我们之前说过，协程会在上一个返回点继续运行，因此这里应该执行的是func函数第一个yield之后的代码，也就是print("b")。

```text
a
in function A
b
```

看到了吧，协程是一个很神奇的函数，它会自己记住之前的执行状态，当再次调用时会从上一次的返回点继续执行。

**图形化解释**

为了让你更加彻底的理解协程，我们使用图形化的方式再看一遍，首先是普通的函数调用：

![img](https://pic1.zhimg.com/80/v2-c2866a7fb52187265c5556a4ef43faf0_720w.webp)

在该图中，方框内表示该函数的指令序列，如果该函数不调用任何其它函数，那么应该从上到下依次执行，但函数中可以调用其它函数，因此其执行并不是简单的从上到下，箭头线表示执行流的方向。从图中我们可以看到，我们首先来到funcA函数，执行一段时间后发现调用了另一个函数funcB，这时控制转移到该函数，执行完成后回到main函数的调用点继续执行。这是普通的函数调用。接下来是协程。

![img](https://pic3.zhimg.com/80/v2-af1d2bd803afd382c29a344df986f5b2_720w.webp)

在这里，我们依然首先在funcA函数中执行，运行一段时间后调用协程，协程开始执行，直到第一个挂起点，此后就像普通函数一样返回funcA函数，funcA函数执行一些代码后再次调用该协程，注意，协程这时就和普通函数不一样了，协程并不是从第一条指令开始执行而是**从上一次的挂起点开始执行**，执行一段时间后遇到第二个挂起点，这时协程再次像普通函数一样返回funcA函数，funcA函数执行一段时间后整个程序结束。

![img](https://pic3.zhimg.com/80/v2-5d4060d402805294bfa5c34385bf7cd6_720w.webp)



**函数只是协程的一种特例**

怎么样，神奇不神奇，**和普通函数不同的是，协程能知道自己上一次执行到了哪里**。现在你应该明白了吧，协程会在函数被暂停运行时保存函数的运行状态，并可以从保存的状态中恢复并继续运行。很熟悉的味道有没有，这不就是操作系统对**[线程](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU2NTYyOTQ4OQ%3D%3D%26mid%3D2247484768%26idx%3D1%26sn%3D049db350af9e5eea5cf3523ceb83f447%26chksm%3Dfcb9823ecbce0b28ca28d021e68c78138cde4a1b86ea7209c0c667d3d544d223d8b2aecbccec%26scene%3D21%23wechat_redirect)**的调度嘛，线程也可以被暂停，操作系统保存线程运行状态然后去调度其它线程，此后该线程再次被分配CPU时还可以继续运行，就像没有被暂停过一样。只不过线程的调度是操作系统实现的，这些对程序员都不可见，而协程是在用户态实现的，对程序员可见。这就是为什么有的人说可以把协程理解为用户态线程的原因。

也就是说现在程序员可以扮演操作系统的角色了，你可以自己控制协程在什么时候运行，什么时候暂停，也就是说协程的调度权在你自己手上。**在协程这件事儿上，调度你说了算**。当你在协程中写下yield的时候就是想要暂停该协程，当使用next()时就是要再次运行该协程。现在你应该理解为什么说函数只是协程的一种特例了吧，函数其实只是没有挂起点的协程而已。

**协程的历史**

有的同学可能认为协程是一种比较新的技术，然而其实协程这种概念早在1958年就已经提出来了，**要知道这时线程的概念都还没有提出来**。到了1972年，终于有编程语言实现了这个概念，这两门编程语言就是Simula 67 以及Scheme。



![img](https://pic3.zhimg.com/80/v2-69adba72bd637aa5c4592ad52adaf386_720w.webp)



但协程这个概念始终没有流行起来，甚至在1993年还有人考古一样专门写论文挖出协程这种古老的技术。因为这一时期还没有线程，如果你想在操作系统写出并发程序那么你将不得不使用类似协程这样的技术，后来线程开始出现，操作系统终于开始原生支持程序的并发执行，就这样，协程逐渐淡出了程序员的视线。直到近些年，随着互联网的发展，尤其是移动互联网时代的到来，服务端对高并发的要求越来越高，协程再一次重回技术主流，各大编程语言都已经支持或计划开始支持协程。那么协程到底是如何实现的呢？

**协程是如何实现的**

让我们从问题的本质出发来思考这个问题。协程的本质是什么呢？其实就是可以被暂停以及可以被恢复运行的函数。那么可以被暂停以及可以被恢复意味着什么呢？看过篮球比赛的同学想必都知道(没看过的也能知道)，篮球比赛也是可以被随时暂停的，暂停时大家需要记住球在哪一方，各自的站位是什么，等到比赛继续的时候大家回到各自的位置，裁判哨子一响比赛继续，就像比赛没有被暂停过一样。

![img](https://pic2.zhimg.com/80/v2-7790ac94fc3581fe7a01f579c953a5a9_720w.webp)

看到问题的关键了吗，比赛之所以可以被暂停也可以继续是因为比赛状态被记录下来了（站位、球在哪一方），这里的状态就是计算机科学中常说的上下文，context。回到协程。协程之所以可以被暂停也可以继续，那么一定要记录下被暂停时的状态，也就是上下文，当继续运行的时候要恢复其上下文(状态)，那么接下来很自然的一个问题就是，函数运行时的状态是什么？这个关键的问题的答案就在《**[函数运行起来后在内存中是什么样子的](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU2NTYyOTQ4OQ%3D%3D%26mid%3D2247484963%26idx%3D1%26sn%3D542d3bec57c6a9dfc17c83005fd2c030%26chksm%3Dfcb9817dcbce086b10cb44cad7c9777b0088fb8d9d6baf71ae36a9b03e1f8ef5bec62b79d6f7%26scene%3D21%23wechat_redirect)**》这篇文章中，函数运行时所有的状态信息都位于函数运行时栈中。函数运行时栈就是我们需要保存的状态，也就是所谓的上下文，如图所示：

![img](https://pic3.zhimg.com/80/v2-3018077c86c24fd3b79abe9e85c964ea_720w.webp)

从图中我们可以看出，该进程中只有一个线程，栈区中有四个栈帧，main函数调用A函数，A函数调用B函数，B函数调用C函数，当C函数在运行时整个进程的状态就如图所示。现在我们已经知道了函数的运行时状态就保存在栈区的栈帧中，接下来重点来了哦。既然函数的运行时状态保存在栈区的栈帧中，那么如果我们想暂停协程的运行就必须保存整个栈帧的数据，那么我们该将整个栈帧中的数据保存在哪里呢？想一想这个问题，整个进程的内存区中哪一块是专门用来长时间(进程生命周期)存储数据的？是不是大脑又一片空白了？

![img](https://pic1.zhimg.com/80/v2-8e644b4905f84d7869da06d7eab4cc60_720w.webp)

先别空白！很显然，这就是堆区啊，heap，我们可以将栈帧保存在堆区中，那么我们该怎么在堆区中保存数据呢？希望你还没有晕，在堆区中开辟空间就是我们常用的C语言中的malloc或者C++中的new。我们需要做的就是在堆区中申请一段空间，让后把协程的整个栈区保存下，当需要恢复协程的运行时再从堆区中copy出来恢复函数运行时状态。再仔细想一想，为什么我们要这么麻烦的来回copy数据呢？实际上，我们需要做的是直接把协程的运行需要的栈帧空间直接开辟在堆区中，这样都不用来回copy数据了，如图所示。

![img](https://pic3.zhimg.com/80/v2-0e66529d63362d8a7922b5479f669de6_720w.webp)

从图中我们可以看到，该程序中开启了两个协程，这两个协程的栈区都是在堆上分配的，这样我们就可以随时中断或者恢复协程的执行了。有的同学可能会问，那么进程地址空间最上层的栈区现在的作用是什么呢？这一区域依然是用来保存函数栈帧的，只不过这些函数并不是运行在协程而是普通线程中的。现在你应该看到了吧，在上图中实际上有3个执行流：

1. 一个普通线程
2. 两个协程

虽然有3个执行流但我们创建了几个线程呢？**一个线程**。现在你应该明白为什么要使用协程了吧，使用协程理论上我们可以**开启无数并发执行流，只要堆区空间足够**，同时还没有创建线程的开销，所有协程的调度、切换都发生在用户态，这就是为什么协程也被称作用户态线程的原因所在。掌声在哪里？

![动图封面](https://pic2.zhimg.com/v2-871188900160cf278461a4c9f010bbcd_b.jpg)



因此即使你创建了N多协程，但在操作系统看来依然只有一个线程，也就是说协程对操作系统来说是不可见的。这也许是为什么协程这个概念比线程提出的要早的原因，可能是写普通应用的程序员比写操作系统的程序员最先遇到需要多个并行流的需求，那时可能都还没有操作系统的概念，或者操作系统没有并行这种需求，所以非操作系统程序员只能自己动手实现执行流，也就是协程。现在你应该对协程有一个清晰的认知了吧。

![img](https://pic3.zhimg.com/80/v2-dccacb3e83e38ab4136591cf60ae70ce_720w.webp)

**总结**

到这里你应该已经理解协程到底是怎么一回事了，但是，依然有一个问题没有解决，为什么协程这种技术又一次重回视线，协程适用于什么场景下呢？该怎么使用呢？

原文地址：https://zhuanlan.zhihu.com/p/523579316

作者：linux

# 【NO.317】Redis 这么强？该如何进行性能优化呢？

在一些网络服务的系统中，Redis 的性能，可能是比 MySQL 等硬盘数据库的性能更重要的课题。比如微博，把热点微博，最新的用户关系，都存储在 Redis 中，大量的查询击中 Redis，而不走 MySQL。

那么，针对 Redis 服务，我们能做哪些性能优化呢？或者说，应该避免哪些性能浪费呢？

## 1.**Redis 性能的基本面**

在讨论优化之前，我们需要知道，Redis 服务本身就有一些特性，比如单线程运行。除非修改 Redis 的源代码，不然这些特性，就是我们思考性能优化的基本面。

那么，有哪些 Redis 基本特性需要我们考虑呢？Redis 的项目介绍中概括了它特性：

> Redis is an in-memory database that persists on disk. The data model is key-value, but many different kind of values are supported.

首先，Redis 使用操作系统提供的虚拟内存来存储数据。而且，这个操作系统一般就是指 Unix。Windows 上也能运行 Redis，但是需要特殊处理。如果你的操作系统使用交换空间，那么 Redis 的数据可能会被实际保存在硬盘上。

其次，Redis 支持持久化，可以把数据保存在硬盘上。很多时候，我们也确实有必要进行持久化来实现备份，数据恢复等需求。但持久化不会凭空发生，它也会占用一部分资源。

第三，Redis 是用 key-value 的方式来读写的，而 value 中又可以是很多不同种类的数据；更进一步，一个数据类型的底层还有被存储为不同的结构。不同的存储结构决定了数据增删改查的复杂度以及性能开销。

最后，在上面的介绍中没有提到的是，Redis 大多数时候是单线程运行的（single-threaded)，即同一时间只占用一个 CPU，只能有一个指令在运行，并行读写是不存在的。很多操作带来的延迟问题，都可以在这里找到答案。

关于最后这个特性，为什么 Redis 是单线程的，却能有很好的性能，两句话概括是：Redis 利用了多路 I/O 复用机制，处理客户端请求时，不会阻塞主线程；Redis 单纯执行（大多数指令）一个指令不到 1 微秒，如此，单核 CPU 一秒就能处理 1 百万个指令（大概对应着几十万个请求吧），用不着实现多线程（网络才是瓶颈。更多这方面的解释可以查看官网说明。

## 2.**优化网络延时**

Redis 的官方博客在几个地方都说，性能瓶颈更可能是网络，那么我们如何优化网络上的延时呢？

首先，如果你们使用单机部署（应用服务和 Redis 在同一台机器上）的话，使用 Unix 进程间通讯来请求 Redis 服务，速度比 localhost 局域网（学名 loopback）更快。官方文档是这么说的，想一想，理论上也应该是这样的。

但很多公司的业务规模不是单机部署能支撑的，所以还是得用 TCP。

Redis 客户端和服务器的通讯一般使用 TCP 长链接。如果客户端发送请求后需要等待 Redis 返回结果再发送下一个指令，客户端和 Redis 的多个请求就构成下面的关系：

![img](https://pic4.zhimg.com/80/v2-750dbd36409530fc6100659843b67aef_720w.webp)

利用 wireshark 抓包得到此流程图

备注：如果不是你要发送的 key 特别长，一个 TCP 包完全能放下 Redis 指令，所以只画了一个 push 包

这样这两次请求中，客户端都需要经历一段网络传输时间。

但如果有可能，完全可以使用 multi-key 类的指令来合并请求，比如两个 GET key 可以用 MGET key1 key2 合并。这样在实际通讯中，请求数也减少了，延时自然得到好转。

如果不能用 multi-key 指令来合并，比如一个 SET，一个 GET 无法合并。怎么办？

Redis 中有至少这样两个方法能合并多个指令到一个 request 中，一个是 MULTI/EXEC，一个是 script。前者本来是构建 Redis 事务的方法，但确实可以合并多个指令为一个 request，它到通讯过程如下。至于 script，最好利用缓存脚本的 sha1 hash key 来调起脚本，这样通讯量更小。

![img](https://pic1.zhimg.com/80/v2-7071c85b3c4497aba5a16ed596022518_720w.webp)

使用了 multi/exec 之后的通讯过程

这样确实更能减少网络传输时间，不是么？但如此以来，就必须要求这个 transaction / script 中涉及的 key 在同一个 node 上，所以要酌情考虑。

如果上面的方法我们都考虑过了，还是没有办法合并多个请求，我们还可以考虑合并多个 responses。比如把 2 个回复信息合并：

![img](https://pic2.zhimg.com/80/v2-d1e1f00b6c4215d89d92645e719bc069_720w.webp)

使用 pipeline 之后的流程图

这样，理论上可以省去 1 次回复所用的网络传输时间。这就是 pipeline 做的事情。举个 ruby 客户端使用 pipeline 的例子：

```text
require 'redis'
@redis = Redis.new()
@redis.pipelined do
    @redis.get 'key1'
    @redis.set 'key2' 'some value'
end
# => [1, 2]
```

据说，有些语言的客户端，甚至默认就使用 pipeline 来优化延时问题，比如 node_redis[7]。

另外，不是任意多个回复信息都可以放进一个 TCP 包中，如果请求数太多，回复的数据很长（比如 get 一个长字符串），TCP 还是会分包传输，但使用 pipeline，依然可以减少传输次数。

pipeline 和上面的其他方法都不一样的是，它不具有原子性。所以在 cluster 状态下的集群上，实现 pipeline 比那些原子性的方法更有可能。

**小结一下：**

- 使用 unix 进程间通信，如果单机部署
- 使用 multi-key 指令合并多个指令，减少请求数，如果有可能的话
- 使用 transaction、script 合并 requests 以及 responses
- 使用 pipeline 合并 response

## 3.**警惕执行时间长的操作**

在大数据量的情况下，有些操作的执行时间会相对长，比如 KEYS *，LRANGE mylist 0 -1，以及其他算法复杂度为 O(n) 的指令。因为 Redis 只用一个线程来做数据查询，如果这些指令耗时很长，就会阻塞 Redis，造成大量延时。

尽管官方文档中说 KEYS * 的查询挺快的，（在普通笔记本上）扫描 1 百万个 key，只需 40 毫秒，但几十 ms 对于一个性能要求很高的系统来说，已经不短了，更何况如果有几亿个 key（一台机器完全可能存几亿个 key，比如一个 key 100字节，1 亿个 key 只有 10GB），时间更长。

所以，尽量不要在生产环境的代码使用这些执行很慢的指令，这一点 Redis 的作者在博客中也提到了。另外，运维同学查询 Redis 的时候也尽量不要用。甚至，Redis Essential 这本书建议利用 rename-command KEYS '' 来禁止使用这个耗时的指令。

除了这些耗时的指令，Redis 中 transaction，script，因为可以合并多个 commands 为一个具有原子性的执行过程，所以也可能占用 Redis 很长时间，需要注意。

如果你想找出生产环境使用的「慢指令」，那么可以利用 SLOWLOG GET count 来查看最近的 count 个执行时间很长的指令。至于多长算长，可以通过在 redis.conf 中设置 slowlog-log-slower-than 来定义。

除此之外，在很多地方都没有提到的一个可能的慢指令是 DEL，但 redis.conf 文件的注释中倒是说了。长话短说就是 DEL 一个大的 object 时候，回收相应的内存可能会需要很长时间（甚至几秒），所以，建议用 DEL 的异步版本：UNLINK。后者会启动一个新的 thread 来删除目标 key，而不阻塞原来的线程。

更进一步，当一个 key 过期之后，Redis 一般也需要同步的把它删除。其中一种删除 keys 的方式是，每秒 10 次的检查一次有设置过期时间的 keys，这些 keys 存储在一个全局的 struct 中，可以用 server.db->expires 访问。检查的方式是：

1. 从中随机取出 20 个 keys
2. 把过期的删掉。
3. 如果刚刚 20 个 keys 中，有 25% 以上（也就是 5 个以上）都是过期的，Redis 认为，过期的 keys 还挺多的，继续重复步骤 1，直到满足退出条件：某次取出的 keys 中没有那么多过去的 keys。

这里对于性能的影响是，如果真的有很多的 keys 在同一时间过期，那么 Redis 真的会一直[9]循环执行删除，占用主线程。

对此，Redis 作者的建议是警惕 EXPIREAT 这个指令，因为它更容易产生 keys 同时过期的现象。我还见到过一些建议是给 keys 的过期时间设置一个随机波动量。最后，redis.conf 中也给出了一个方法，把 keys 的过期删除操作变为异步的，即，在 redis.conf 中设置 lazyfree-lazy-expire yes[10]。

## 4.**优化数据结构、使用正确的算法**

一种数据类型（比如 string，list）进行增删改查的效率是由其底层的存储结构决定的。

我们在使用一种数据类型时，可以适当关注一下它底层的存储结构及其算法，避免使用复杂度太高的方法。举两个例子：

1. ZADD 的时间复杂度是 O(log(N))，这比其他数据类型增加一个新元素的操作更复杂，所以要小心使用。
2. 若 Hash 类型的值的 fields 数量有限，它很有可能采用 ziplist 这种结构做存储，而 ziplist 的查询效率可能没有同等字段数量的 hashtable 效率高，在必要时，可以调整 Redis 的存储结构。

除了时间性能上的考虑，有时候我们还需要节省存储空间。比如上面提到的 ziplist 结构，就比 hashtable 结构节省存储空间[11]。但节省空间的数据结构，其算法的复杂度可能很高。所以，这里就需要在具体问题面前做出权衡。

如何做出更好的权衡？我觉得得深挖 Redis 的存储结构才能让自己安心。这方面的内容我们下次再说。

以上这三点都是编程层面的考虑，写程序时应该注意啊。下面这几点，也会影响 Redis 的性能，但解决起来，就不只是靠代码层面的调整了，还需要架构和运维上的考虑。

## 5.**考虑操作系统和硬件是否影响性能**

Redis 运行的外部环境，也就是操作系统和硬件显然也会影响 Redis 的性能。在官方文档中，就给出了一些例子：

1. CPU：Intel 多种 CPU 都比 AMD 皓龙系列好
2. 虚拟化：实体机比虚拟机好，主要是因为部分虚拟机上，硬盘不是本地硬盘，监控软件导致 fork 指令的速度慢（持久化时会用到 fork），尤其是用 Xen 来做虚拟化时。
3. 内存管理：在 linux 操作系统中，为了让 translation lookaside buffer，即 TLB，能够管理更多内存空间（TLB 只能缓存有限个 page），操作系统把一些 memory page 变得更大，比如 2MB 或者 1GB，而不是通常的 4096 字节，这些大的内存页叫做 huge pages。同时，为了方便程序员使用这些大的内存 page，操作系统中实现了一个 transparent huge pages（THP）机制，使得大内存页对他们来说是透明的，可以像使用正常的内存 page 一样使用他们。但这种机制并不是数据库所需要的，可能是因为 THP 会把内存空间变得紧凑而连续吧，就像 mongodb 的文档中明确说的，数据库需要的是稀疏的内存空间，所以请禁掉 THP 功能。Redis 也不例外，但 Redis 官方博客上给出的理由是：使用大内存 page 会使 bgsave 时，fork 的速度变慢；如果 fork 之后，这些内存 page 在原进程中被修改了，他们就需要被复制（即 copy on write），这样的复制会消耗大量的内存（毕竟，人家是 huge pages，复制一份消耗成本很大）。所以，请禁止掉操作系统中的 transparent huge pages 功能。
4. 交换空间：当一些内存 page 被存储在交换空间文件上，而 Redis 又要请求那些数据，那么操作系统会阻塞 Redis 进程，然后把想要的 page，从交换空间中拿出来，放进内存。这其中涉及整个进程的阻塞，所以可能会造成延时问题，一个解决方法是禁止使用交换空间（Redis Essentials 中如是建议[12]，如果内存空间不足，请用别的方法处理）。

## 6.**考虑持久化带来的开销**

Redis 的一项重要功能就是持久化，也就是把数据复制到硬盘上。基于持久化，才有了 Redis 的数据恢复等功能。

但维护这个持久化的功能，也是有性能开销的。

首先说，RDB 全量持久化。

这种持久化方式把 Redis 中的全量数据打包成 rdb 文件放在硬盘上。但是执行 RDB 持久化过程的是原进程 fork 出来一个子进程，而 fork 这个系统调用是需要时间的，根据Redis Lab 6 年前做的实验，在一台新型的 AWS EC2 m1.small^13 上，fork 一个内存占用 1GB 的 Redis 进程，需要 700+ 毫秒，而这段时间，redis 是无法处理请求的。

虽然现在的机器应该都会比那个时候好，但是 fork 的开销也应该考虑吧。为此，要使用合理的 RDB 持久化的时间间隔，不要太频繁。

接下来，我们看另外一种持久化方式：AOF 增量持久化。

这种持久化方式会把你发到 redis server 的指令以文本的形式保存下来（格式遵循 redis protocol），这个过程中，会调用两个系统调用，一个是 write(2)，同步完成，一个是 fsync(2)，异步完成。

这两部都可能是延时问题的原因：

1. write 可能会因为输出的 buffer 满了，或者 kernal 正在把 buffer 中的数据同步到硬盘，就被阻塞了。
2. fsync 的作用是确保 write 写入到 aof 文件的数据落到了硬盘上，在一个 7200 转/分的硬盘上可能要延时 20 毫秒左右，消耗还是挺大的。更重要的是，在 fsync 进行的时候，write 可能会被阻塞。

其中，write 的阻塞貌似只能接受，因为没有更好的方法把数据写到一个文件中了。但对于 fsync，Redis 允许三种配置，选用哪种取决于你对备份及时性和性能的平衡：

- always：当把 appendfsync 设置为 always，fsync 会和客户端的指令同步执行，因此最可能造成延时问题，但备份及时性最好。
- everysec：每秒钟异步执行一次 fsync，此时 redis 的性能表现会更好，但是 fsync 依然可能阻塞 write，算是一个折中选择。
- no：redis 不会主动出发 fsync （并不是永远不 fsync，那是不太可能的），而由 kernel 决定何时 fsync

## 7.**使用分布式架构 —— 读写分离、数据分片**

以上，我们都是基于单台，或者单个 Redis 服务进行优化。下面，我们考虑当网站的规模变大时，利用分布式架构来保障 Redis 性能的问题。

首先说，哪些情况下不得不（或者最好）使用分布式架构：

1. 数据量很大，单台服务器内存不可能装得下，比如 1 个 T 这种量级
2. 需要服务高可用
3. 单台的请求压力过大

解决这些问题可以采用数据分片或者主从分离，或者两者都用（即，在分片用的 cluster 节点上，也设置主从结构）。

这样的架构，可以为性能提升加入新的切入点：

1. 把慢速的指令发到某些从库中执行
2. 把持久化功能放在一个很少使用的从库上
3. 把某些大 list 分片

其中前两条都是根据 Redis 单线程的特性，用其他进程（甚至机器）做性能补充的方法。

当然，使用分布式架构，也可能对性能有影响，比如请求需要被转发，数据需要被不断复制分发。（待查）

## 8.**后话**

其实还有很多东西也影响 Redis 的性能，比如 active rehashing（keys 主表的再哈希，每秒 10 次，关掉它可以提升一点点性能），但是这篇博客已经写的很长了。而且，更重要不是收集已经被别人提出的问题，然后记忆解决方案；而是掌握 Redis 的基本原理，以不变应万变的方式决绝新出现的问题。

原文地址：https://zhuanlan.zhihu.com/p/522746567

作者：Linux

# 【NO.318】腾讯面试官用「B+树」虐哭我了

我们知道当系统要处理的数据量非常庞大的时候，数据不可能全部存放于内存，需要借助磁盘来完成存储和检索。在数据库中支持很多种索引方式，常见有哈希索引、全文索引和B+树索引。今天将和大家分享使用B+树作为索引的优缺点。

面试很多互联网公司，都会问这个问题，也许我们看过太多面经内容，但是基本上答案千篇一律，对于面试官而言也是基本上听腻了，是多么希望能听到不一样的解答，那么今天希望这篇文章可以给你不一样的答案。

今天分享的几点如下：

![img](https://pic4.zhimg.com/80/v2-104c46a12f7b68d440530dc72551e65f_720w.webp)

## 1.**数据从磁盘读写与内存读写有哪些不同**

> 我们平时接触的有机械硬盘和固态硬盘。内存属于半导体器件，对于内存，我们知道内存地址就可以通过地址拿到数据，也就是内存的随机访问特性。访问速度快但是贵，所以内存空间一般比较小。

对于磁盘，属于机械器件。每当磁盘访问数据的时候，都需要等磁盘盘片旋转到磁头，才能读取相应的数据，即使磁盘的转速很快，但是和内存的随机访问相比还是渣渣。

所见，如果是随机读写，其性能差距是非常大的。那如果是顺序访问大量数据的时候，磁盘的性能和内存其实差距就不大了，这是为啥？

磁盘的最小读写单位是扇区，现在磁盘扇区一般是4k个字节，对于操作系统，一次性会读取多个扇区，至此操作系统的最小读取单位就是块。

每当我们从磁盘读取一个数据，操作系统就会一次性读取整个块，那么对于大量的顺序读写来说，磁盘效率会比随机读写高很多。

假设现在你有个有序数组，全部以块的方式存放在磁盘中，现在我们通过二分查找的方式查找元素A。首先我们找到中间元素，并从块中取出，将其从磁盘放入内存中，然后再内存中进行二分查找。

在进行下一步的时候，如果查找的元素在其他块中，我们需要继续从磁盘读出到内存中。这样反反复复的从磁盘到内存，其效率将非常的低。所以我们需要想办法让访问磁盘的次数尽可能的低。

## 2.**数据和索引分离**

> 我们以公安系统为例。系统中的用户非常多，每个用户除了姓名，年龄等基本信息外，当然还有一个唯一标识的ID，我们拿到这个ID，就可以知道对应的基本信息。但是每个用户的基本信息太多不可能全部存放在内存中，因此考虑存储于磁盘中。

![img](https://pic2.zhimg.com/80/v2-5ef83022526f59489d63ee51c0e1b9e9_720w.webp)

用户数据

- 采用有序数组的方式，其中分别存储用户ID和用户信息所在磁盘的位置，这样我只需要存放两个元素，直接存放于内存。如下图所示

![img](https://pic4.zhimg.com/80/v2-9347a157b74f20ec993e347482ddade3_720w.webp)

有序数组

但是在数据频繁变化的场景中，有序数组的弊端就出现了。大部分情况还是考虑使用二叉检索树或者哈希表的方式。但是哈希表又不支持区间查询，因此更多的使用二叉检索树的方式。如下图所示：

![img](https://pic3.zhimg.com/80/v2-6c050de8353d625b249050f1348077a6_720w.webp)

如果索引太多，依然不能完全存放于内存中，那我们是不是可以考虑将索引也存放于磁盘中？如何高效的在磁盘中组织索引的结构？这就引入了B+树。

## **3.B+树**

- 让节点大小等于块大小

> 我们知道操作系统在对磁盘进行访问的时候，通常是按照块的方式读取。如果当前你需要读取的数据只有几个字节，但是磁盘依然会将整个块读出来，这样子是不是读写效率就很低呢。在B+树中，大佬采用让一个节点大小等于一个块的大小，节点中存放的不是一个元素，而是一个有序的数组，这样充分利用操作系统的套路，使得读取效率的最大化

- 内部节点与叶子节点

> 内部节点和叶子节点虽然是一样的结构，但是其存储的内容有所区别。内部节点存放key以及维持树形结构的指针，它并不存放key对应的数据。而叶子节点存放key和对应的数据，不存放维持树形结构的指针，这样使得节点空间的利用最大化。

![img](https://pic2.zhimg.com/80/v2-d43f8d4acc9e1b450f2e5c8301fc9715_720w.webp)

内部节点与叶子节点

- B+树使用双向链表的方式，具有良好的范围查询能力和灵活的调整能力

综上三点，B+树是一颗完全平衡的m阶多叉树。

![img](https://pic2.zhimg.com/80/v2-e36a724c8884348d30cca34f25932129_720w.webp)

m阶多叉树

## 4.**B+树的检索方案**

> 刚才吹了一波B+树多么的牛逼，到底是怎么检索的？具体的查找过程是这样的：我们先确认要寻找的查询值，位于数组中哪两个相邻元素中间，然后我们将第一个元素对应的指针读出，获得下一个 block 的位置。读出下一个 block 的节点数据后，我们再对它进行同样处理。这样，B+ 树会逐层访问内部节点，直到读出叶子节点。对于叶子节点中的数组，直接使用二分查找算法，我们就可以判断查找的元素是否存在。如果存在，我们就可以得到该查询值对应的存储数据。如果这个数据是详细信息的位置指针，那我们还需要再访问磁盘一次，将详细信息读出。

B+树是一个m阶的多叉树，所以B+树中的一个节点可以存放m个元素的数组，ok，这样的话，只需要几层的b+树就可以索引数据量很大的数了。比如1个2k的节点可以存放200个元素，那么一个4层的B+树就能存放200^4，即16亿个元素。

如果只有四层，意味着我们最多访问磁盘4次，假设目前每个节点为2k，那么第一层就一个节点也就2k，第二层节点最多200个元素，一共就是0.8M。第三层200^2，也就是40000个节点，一共80M。对于当前的计算机而言，我们完全可以将前面三层存放于内存中，只需要将第四层存放于磁盘中，这样我们只需要和磁盘打一次交道就分手，也就是面试想知道的为什么要分为内部节点与叶子节点。

## 5.**B+树如何进行动态的调整**

> 上面介绍了B+树的结构和查询原理，现在我们看看B+树增加和删除是怎么个情况

现在我们以三个元素的B+树 为例，假设目前我们要插入ID为6=5的元素，第一步先查找对应的叶子节点，如果叶子节点没有满，直接插入即可

![img](https://pic1.zhimg.com/80/v2-47604db1871b26e7d2e78389652cc1e8_720w.webp)

插入元素6

如果我们插入的元素是10？按道理我们应该插入到9后面，但是节点已经满了，所以我们需要采取其他的方式。方法是将此叶子节点进行分裂，即生成一个新的节点，然后将数据在两个节点中平分。

![img](https://pic3.zhimg.com/80/v2-c076b04a6965196222b27532bf3aa23a_720w.webp)

节点分裂

很明显，叶子节点的分裂影响到了父节点，如果父节点也是满的，也要进行分裂

![img](https://pic4.zhimg.com/80/v2-6572b96960b4c930826f894f0a96153b_720w.webp)

节点分裂

## 6.**总结**

> 从大问题拆分为小问题并逐个解决是我们在生活学习重要的本领，比较复杂的B+树其实也就是基本的数据结构(数组，链表，树)组成，其检索过程实际上就是二分查找，所以如果B+树完全载入内存，它的检索效率和有序数组/二叉检索树差不多，但是却更加复杂。B+树最大的优点在于它将索引存放在磁盘，让检索技术摆脱了内存限制，另外通过将索引和数据分离的方式，将索引的数组大小保持在较小范围，这样精简索引。

原文地址：https://zhuanlan.zhihu.com/p/522591122

作者：Linux

# 【NO.319】超专业解析|linux文件系统的底层架构及其工作原理

我们先看一张图：

![img](https://pic4.zhimg.com/80/v2-2096c5164accdc536bf0fc253125b4cf_720w.webp)

这张图大体上描述了 Linux 系统上，应用程序对磁盘上的文件进行读写时，从上到下经历了哪些事情。

这篇文章就以这张图为基础，介绍 Linux 在 I/O 上做了哪些事情。

## 1.什么是文件系统

文件系统，本身是对存储设备上的文件，进行组织管理的机制。组织方式不同，就会形成不同的文件系统。比如常见的 Ext4、XFS、ZFS 以及网络文件系统 NFS 等等。

但是不同类型的文件系统标准和接口可能各有差异，我们在做应用开发的时候却很少关心系统调用以下的具体实现，大部分时候都是直接系统调用 open, read, write, close 来实现应用程序的功能，不会再去关注我们具体用了什么文件系统（UFS、XFS、Ext4、ZFS），磁盘是什么接口（IDE、SCSI，SAS，SATA 等），磁盘是什么存储介质（HDD、SSD）

应用开发者之所以这么爽，各种复杂细节都不用管直接调接口，是因为内核为我们做了大量的有技术含量的脏活累活。开始的那张图看到 Linux 在各种不同的文件系统之上，虚拟了一个 VFS，目的就是统一各种不同文件系统的标准和接口，让开发者可以使用相同的系统调用来使用不同的文件系统。

## 2.文件系统如何工作（VFS）

### 2.1Linux 系统下的文件

在 Linux 中一切皆文件。不仅普通的文件和目录，就连块设备、套接字、管道等，也都要通过统一的文件系统来管理。

```text
用 ls -l 命令看最前面的字符可以看到这个文件是什么类型

brw-r--r-- 1 root    root    1, 2 4月  25 11:03 bnod // 块设备文件
crw-r--r-- 1 root    root    1, 2 4月  25 11:04 cnod // 符号设备文件
drwxr-xr-x 2 wrn3552 wrn3552    6 4月  25 11:01 dir // 目录
-rw-r--r-- 1 wrn3552 wrn3552    0 4月  25 11:01 file // 普通文件
prw-r--r-- 1 root    root       0 4月  25 11:04 pipeline // 有名管道
srwxr-xr-x 1 root    root       0 4月  25 11:06 socket.sock // socket文件
lrwxrwxrwx 1 root    root       4 4月  25 11:04 softlink -> file // 软连接
-rw-r--r-- 2 wrn3552 wrn3552 0 4月  25 11:07 hardlink // 硬链接（本质也是普通文件）
```

Linux 文件系统设计了两个数据结构来管理这些不同种类的文件：

- inode(index node)：索引节点
- dentry(directory entry)：目录项

### 2.2inode 和 dentry

**inode**

inode 是用来记录文件的 metadata，所谓 metadata 在 Wikipedia 上的描述是 data of data，其实指的就是文件的各种属性，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。

```text
wrn3552@novadev:~/playground$ stat file
  文件：file
  大小：0               块：0          IO 块：4096   普通空文件
设备：fe21h/65057d      Inode：32828       硬链接：2
权限：(0644/-rw-r--r--)  Uid：( 3041/ wrn3552)   Gid：( 3041/ wrn3552)
最近访问：2021-04-25 11:07:59.603745534 +0800
最近更改：2021-04-25 11:07:59.603745534 +0800
最近改动：2021-04-25 11:08:04.739848692 +0800
创建时间：-
```

inode 和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以，inode 同样占用磁盘空间，只不过相对于文件来说它大小固定且大小不算大。

**dentry**

dentry 用来记录文件的名字、inode 指针以及与其他 dentry 的关联关系。

```text
wrn3552@novadev:~/playground$ tree
.
├── dir
│   └── file_in_dir
├── file
└── hardlink
```

- 文件的名字：像 dir、file、hardlink、file_in_dir 这些名字是记录在 dentry 里的
- inode 指针：就是指向这个文件的 inode
- 与其他 dentry 的关联关系：其实就是每个文件的层级关系，哪个文件在哪个文件下面，构成了文件系统的目录结构

不同于 inode，dentry 是由内核维护的一个内存数据结构，所以通常也被叫做 dentry cache。

### 2.3文件是如何存储在磁盘上的

![img](https://pic4.zhimg.com/80/v2-7cba2e800a10ba6fef52dfe733ca7faf_720w.webp)

这里有张图解释了文件是如何存储在磁盘上的，首先，磁盘再进行文件系统格式化的时候，会分出来 3 个区：

1. Superblock
2. inode blocks
3. data blocks

（其实还有 boot block，可能会包含一些 bootstrap 代码，在机器启动的时候被读到，这里忽略）其中 inode blocks 放的都是每个文件的 inode，data blocks 里放的是每个文件的内容数据。这里关注一下 superblock，它包含了整个文件系统的 metadata，具体有：

1. inode/data block 总量、使用量、剩余量
2. 文件系统的格式，属主等等各种属性

superblock 对于文件系统来说非常重要，如果 superblock 损坏了，文件系统就挂载不了了，相应的文件也没办法读写。既然 superblock 这么重要，那肯定不能只有一份，坏了就没了，它在系统中是有很多副本的，在 superblock 损坏的时候，可以使用 fsck（File System Check and repair）来恢复。回到上面的那张图，可以很清晰地看到文件的各种属性和文件的数据是如何存储在磁盘上的：

1. dentry 里包含了文件的名字、目录结构、inode 指针
2. inode 指针指向文件特定的 inode（存在 inode blocks 里）
3. 每个 inode 又指向 data blocks 里具体的 logical block，这里的 logical block 存的就是文件具体的数据

这里解释一下什么是 logical block：

1. 对于不同存储介质的磁盘，都有最小的读写单元

- /sys/block/sda/queue/physical_block_size

1. HDD 叫做 sector（扇区），SSD 叫做 page（页面）
2. 对于 hdd 来说，每个 sector 大小 512Bytes
3. 对于 SSD 来说每个 page 大小不等（和 cell 类型有关），经典的大小是 4KB
4. 但是 Linux 觉得按照存储介质的最小读写单元来进行读写可能会有效率问题，所以支持在文件系统格式化的时候指定 block size 的大小，一般是把几个 physical_block 拼起来就成了一个 logical block

- /sys/block/sda/queue/logical_block_size

1. 理论上应该是 logical_block_size >= physical_block_size，但是有时候我们会看到 physical_block_size = 4K，logical_block_size = 512B 情况，其实这是因为磁盘上做了一层 512B 的仿真（emulation）（详情可参考 512e 和 4Kn）

## 3.ZFS

这里简单介绍一个广泛应用的文件系统 ZFS，一些数据库应用也会用到 ZFS，先看一张 zfs 的层级结构图：

![img](https://pic3.zhimg.com/80/v2-08deeb365ad0dff3b74f77a1eea88dd6_720w.webp)

这是一张从底向上的图：

1. 将若干物理设备 disk 组成一个虚拟设备 vdev（同时，disk 也是一种 vdev）
2. 再将若干个虚拟设备 vdev 加到一个 zpool 里
3. 在 zpool 的基础上创建 zfs 并挂载（zvol 可以先不看，我们没有用到）

### 3.1ZFS 的一些操作

**创建 zpool**

```text
root@:~ # zpool create tank raidz /dev/ada1 /dev/ada2 /dev/ada3 raidz /dev/ada4 /dev/ada5 /dev/ada6
root@:~ # zpool list tank
NAME    SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP  HEALTH  ALTROOT
tank     11G   824K  11.0G        -         -     0%     0%  1.00x  ONLINE  -
root@:~ # zpool status tank
  pool: tank
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        tank        ONLINE       0     0     0
          raidz1-0  ONLINE       0     0     0
            ada1    ONLINE       0     0     0
            ada2    ONLINE       0     0     0
            ada3    ONLINE       0     0     0
          raidz1-1  ONLINE       0     0     0
            ada4    ONLINE       0     0     0
            ada5    ONLINE       0     0     0
            ada6    ONLINE       0     0     0
```

- 创建了一个名为 tank 的 zpool
- 这里的 raidz 同 RAID5

除了 raidz 还支持其他方案：

![img](https://pic3.zhimg.com/80/v2-97fcd91d59dac67ae7a706c9976957da_720w.webp)

**创建 zfs**

```text
root@:~ # zfs create -o mountpoint=/mnt/srev tank/srev
root@:~ # df -h tank/srev
Filesystem    Size    Used   Avail Capacity  Mounted on
tank/srev     7.1G    117K    7.1G     0%    /mnt/srev
```

- 创建了一个 zfs，挂载到了 /mnt/srev
- 这里没有指定 zfs 的 quota，创建的 zfs 大小即 zpool 大小

**对 zfs 设置 quota**

```text
root@:~ # zfs set quota=1G tank/srev
root@:~ # df -h tank/srev
Filesystem    Size    Used   Avail Capacity  Mounted on
tank/srev     1.0G    118K    1.0G     0%    /mnt/srev
```

### 3.2ZFS 特性

#### 3.2.1Pool 存储

上面的层级图和操作步骤可以看到 zfs 是基于 zpool 创建的，zpool 可以动态扩容意味着存储空间也可以动态扩容，而且可以创建多个文件系统，文件系统共享完整的 zpool 空间无需预分配。

#### 3.2.2 事务文件系统

zfs 的写操作是事务的，意味着要么就没写，要么就写成功了，不会像其他文件系统那样，应用打开了文件，写入还没保存的时候断电，导致文件为空。zfs 保证写操作事务采用的是 copy on write 的方式：

![img](https://pic2.zhimg.com/80/v2-e5bb34a634ab9e57db917caa76f0ea55_720w.webp)

- 当 block B 有修改变成 B1 的时候，普通的文件系统会直接在 block B 原地进行修改变成 B1
- zfs 则会再另一个地方写 B1，然后再在后面安全的时候对原来的 B 进行回收
- 这样结果就不会出现 B 被打开而写失败的情况，大不了就是 B1 没写成功

这个特性让 zfs 在断电后不需要执行 fsck 来检查磁盘中是否存在写操作失败需要恢复的情况，大大提升了应用的可用性。

#### 3.2.3 ARC 缓存

ZFS 中的 ARC(Adjustable Replacement Cache) 读缓存淘汰算法，是基于 IBM 的 ARP(Adaptive Replacement Cache) 演化而来。在一些文件系统中实现的标准 LRU 算法其实是有缺陷的：比如复制大文件之类的线性大量 I/O 操作，导致缓存失效率猛增（大量文件只读一次，放到内存不会被再读，坐等淘汰）。

另外，缓存可以根据时间来进行优化（LRU，最近最多使用），也可以根据频率进行优化（LFU，最近最常使用），这两种方法各有优劣，但是没办法适应所有场景。

ARC 的设计就是尝试在 LRU 和 LFU 之间找到一个平衡，根据当前的 I/O workload 来调整用 LRU 多一点还是 LFU 多一点。

ARC 定义了 4 个链表：

1. LRU list：最近最多使用的页面，存具体数据
2. LFU list：最近最常使用的页面，存具体数据
3. Ghost list for LRU：最近从 LRU 表淘汰下来的页面信息，不存具体数据，只存页面信息
4. Ghost list for LFU：最近从 LFU 表淘汰下来的页面信息，不存具体数据，只存页面信息

ARC 工作流程大致如下：

1. LRU list 和 LFU list 填充和淘汰过程和标准算法一样
2. 当一个页面从 LRU list 淘汰下来时，这个页面的信息会放到 LRU ghost 表中
3. 如果这个页面一直没被再次引用到，那么这个页面的信息最终也会在 LRU ghost 表中被淘汰掉
4. 如果这个页面在 LRU ghost 表中未被淘汰的时候，被再一次访问了，这时候会引起一次幽灵（phantom）命中
5. phantom 命中的时候，事实上还是要把数据从磁盘第一次放缓存
6. 但是这时候系统知道刚刚被 LRU 表淘汰的页面又被访问到了，说明 LRU list 太小了，这时它会把 LRU list 长度加一，LFU 长度减一
7. 对于 LFU 的过程也与上述过程类似

### 3.3 ZFS 参考资料

关于 ZFS 详细介绍可以参考：

- 这篇文章

## 4.磁盘类型

磁盘根据不同的分类方式，有各种不一样的类型。

### 4.1 磁盘的存储介质

根据磁盘的存储介质可以分两类（大家都很熟悉）：

- HDD（机械硬盘）
- SSD（固态硬盘）

### 4.2 磁盘的接口

根据磁盘接口分类：

- IDE (Integrated Drive Electronics)
- SCSI (Small Computer System Interface)
- SAS (Serial Attached SCSI)
- SATA (Serial ATA)
- ...

不同的接口，往往分配不同的设备名称。比如， IDE 设备会分配一个 hd 前缀的设备名，SCSI 和 SATA 设备会分配一个 sd 前缀的设备名。如果是多块同类型的磁盘，就会按照 a、b、c 等的字母顺序来编号。

### 4.3 Linux 对磁盘的管理

其实在 Linux 中，磁盘实际上是作为一个块设备来管理的，也就是以块为单位读写数据，并且支持随机读写。每个块设备都会被赋予两个设备号，分别是主、次设备号。主设备号用在驱动程序中，用来区分设备类型；而次设备号则是用来给多个同类设备编号。

```text
g18-"299" on ~# ls -l /dev/sda*
brw-rw---- 1 root disk 8,  0 Apr 25 15:53 /dev/sda
brw-rw---- 1 root disk 8,  1 Apr 25 15:53 /dev/sda1
brw-rw---- 1 root disk 8, 10 Apr 25 15:53 /dev/sda10
brw-rw---- 1 root disk 8,  2 Apr 25 15:53 /dev/sda2
brw-rw---- 1 root disk 8,  5 Apr 25 15:53 /dev/sda5
brw-rw---- 1 root disk 8,  6 Apr 25 15:53 /dev/sda6
brw-rw---- 1 root disk 8,  7 Apr 25 15:53 /dev/sda7
brw-rw---- 1 root disk 8,  8 Apr 25 15:53 /dev/sda8
brw-rw---- 1 root disk 8,  9 Apr 25 15:53 /dev/sda9
```

- 这些 sda 磁盘主设备号都是 8，表示它是一个 sd 类型的块设备
- 次设备号 0-10 表示这些不同 sd 块设备的编号

### 4.4 Generic Block Layer

![img](https://pic3.zhimg.com/80/v2-cdb9d8d80b5e7b0466cd157b04e554aa_720w.webp)

和 VFS 类似，为了对上层屏蔽不同块设备的差异，内核在文件系统和块设备之前抽象了一个 Generic Block Layer（通用块层），有时候一些人也会把下面的 I/O 调度层并到通用块层里表述。

这两层主要做两件事：

1. 跟 VFS 的功能类似。向上，为文件系统和应用程序，提供访问块设备的标准接口；向下，把各种异构的磁盘设备抽象为统一的块设备，并提供统一框架来管理这些设备的驱动程序
2. 对 I/O 请求进行调度，通过重新排序、合并等方式，提高磁盘读写效率

下图是一个完整的 I/O 栈全景图：

![img](https://pic4.zhimg.com/80/v2-509ac45dbd2c9c141d41a0a01a65eb37_720w.webp)

可以看到中间的 Block Layer 其实就是 Generic Block Layer，在图中可以看到 Block Layer 的 I/O 调度分为两类，分别表示单队列和多队列的调度：

- I/O scheduler
- blkmq

### 4.5 I/O 调度

老版本的内核里只支持单队列的 I/O scheduler，在 3.16 版本的内核开始支持多队列 blkmq，这里介绍几种经典的 I/O 调度策略。

单队列 I/O scheduler：

- NOOP：事实上是个 FIFO 的队列，只做基本的请求合并
- CFQ：Completely Fair Queueing，完全公平调度器，给每个进程维护一个 I/O 调度队列，按照时间片来均匀分布每个进程 I/O 请求，
- DeadLine：为读和写请求创建不同的 I/O 队列，确保达到 deadline 的请求被优先处理

多队列 blkmq：

- bfq：Budget Fair Queueing，也是公平调度器，不过不是按时间片来分配，而是按请求的扇区数量（带宽）
- kyber：维护两个队列（同步/读、异步/写），同时严格限制发到这两个队列的请求数以保证相应时间
- mq-deadline：多队列版本的 deadline
- 具体各种 I/O 调度策略可以参考 IOSchedulers
- 关于 blkmq 可以参考 Linux Multi-Queue Block IO Queueing Mechanism (blk-mq) Details
- 多队列调度可以参考 Block layer introduction part 2: the request layer

### 4.6性能指标

一般来说 I/O 性能指标有这几个：

- 使用率：ioutil，指的是磁盘处理 I/O 的时间百分比，ioutil 只看有没有 I/O 请求，不看 I/O 请求的大小。ioutil 越高表示一直都有 I/O 请求，不代表磁盘无法响应新的 I/O 请求
- IOPS：每秒的 I/O 请求数
- 吞吐量/带宽：每秒的 I/O 请求大小，通常是 MB/s 或者 GB/s 为单位
- 响应时间：I/O 请求发出到收到响应的时间
- 饱和度：指的是磁盘处理 I/O 的繁忙程度。这个指标比较玄学，没有直接的数据可以表示，一般是根据平均队列请求长度或者响应时间跟基准测试的结果进行对比来估算

（在做基准测试时，还会分顺序/随机、读/写进行排列组合分别去测 IOPS 和带宽）

上面的指标除了饱和度外，其他都可以在监控系统中看到。Linux 也提供了一些命令来输出不同维度的 I/O 状态：

- iostat -d -x：看各个设备的 I/O 状态，数据来源 /proc/diskstats
- pidstat -d：看近处的 I/O
- iotop：类似 top，按 I/O 大小对进程排序

原文地址：https://zhuanlan.zhihu.com/p/522051456

作者：Linux

# 【NO.320】Linux 高性能服务 epoll 的本质，真的不简单（含实例源码）

**设想一个场景**：有100万用户同时与一个进程保持着TCP连接，而每一时刻只有几十个或几百个TCP连接是活跃的(接收TCP包)，也就是说在每一时刻进程只需要处理这100万连接中的一小部分连接。

那么，如何才能高效的处理这种场景呢？进程是否在每次询问操作系统收集有事件发生的TCP连接时，把这100万个连接告诉操作系统，然后由操作系统找出其中有事件发生的几百个连接呢？实际上，在 Linux2.4 版本以前，那时的select 或者 poll 事件驱动方式是这样做的。

这里有个非常明显的问题，即在某一时刻，进程收集有事件的连接时，其实这100万连接中的大部分都是没有事件发生的。

因此如果每次收集事件时，都把100万连接的套接字传给操作系统（这首先是用户态内存到内核态内存的大量复制），而由操作系统内核寻找这些连接上有没有未处理的事件，将会是巨大的资源浪费，然后select和poll就是这样做的，因此它们最多只能处理几千个并发连接。

而epoll不这样做，它在Linux内核中申请了一个简易的文件系统，把原先的一个select或poll调用分成了3部分：

```text
int epoll_create(int size);  
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);  
int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);
```

1. 调用 epoll_create 建立一个 epoll 对象（在epoll文件系统中给这个句柄分配资源）；
2. 调用 epoll_ctl 向 epoll 对象中添加这100万个连接的套接字；
3. 调用 epoll_wait 收集发生事件的连接。

这样只需要在进程启动时建立 1 个 epoll 对象，并在需要的时候向它添加或删除连接就可以了，因此，在实际收集事件时，epoll_wait 的效率就会非常高，因为调用 epoll_wait 时并没有向它传递这100万个连接，内核也不需要去遍历全部的连接。

## 1.epoll原理详解

当某一进程调用 epoll_create 方法时，Linux 内核会创建一个 eventpoll 结构体，这个结构体中有两个成员与epoll的使用方式密切相关，如下所示：

```text
struct eventpoll {
  ...
  /*红黑树的根节点，这棵树中存储着所有添加到epoll中的事件，
  也就是这个epoll监控的事件*/
  struct rb_root rbr;
  /*双向链表rdllist保存着将要通过epoll_wait返回给用户的、满足条件的事件*/
  struct list_head rdllist;
  ...
};
```

我们在调用 epoll_create 时，内核除了帮我们在 epoll 文件系统里建了个 file 结点，在内核 cache 里建了个红黑树用于存储以后 epoll_ctl 传来的 socket 外，还会再建立一个 rdllist 双向链表，用于存储准备就绪的事件，当 epoll_wait 调用时，仅仅观察这个 rdllist 双向链表里有没有数据即可。

有数据就返回，没有数据就 sleep，等到 timeout 时间到后即使链表没数据也返回。所以，epoll_wait 非常高效。

所有添加到epoll中的事件都会与设备(如网卡)驱动程序建立回调关系，也就是说相应事件的发生时会调用这里的回调方法。这个回调方法在内核中叫做ep_poll_callback，它会把这样的事件放到上面的rdllist双向链表中。

在epoll中对于每一个事件都会建立一个epitem结构体，如下所示：

```text
struct epitem {
  ...
  //红黑树节点
  struct rb_node rbn;
  //双向链表节点
  struct list_head rdllink;
  //事件句柄等信息
  struct epoll_filefd ffd;
  //指向其所属的eventepoll对象
  struct eventpoll *ep;
  //期待的事件类型
  struct epoll_event event;
  ...
}; // 这里包含每一个事件对应着的信息。
```

当调用 epoll_wait 检查是否有发生事件的连接时，只是检查eventpoll对象中的rdllist双向链表是否有epitem元素而已，如果rdllist链表不为空，则这里的事件复制到用户态内存（使用共享内存提高效率）中，同时将事件数量返回给用户。

因此epoll_waitx效率非常高。epoll_ctl在向epoll对象中添加、修改、删除事件时，从rbr红黑树中查找事件也非常快，也就是说epoll是非常高效的，它可以轻易地处理百万级别的并发连接。

![img](https://pic1.zhimg.com/80/v2-8d549b1073c918e353db932027137d00_720w.webp)

【总结】：

一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。

- 执行 epoll_create() 时，创建了红黑树和就绪链表；
- 执行 epoll_ctl() 时，如果增加 socket 句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据；
- 执行 epoll_wait() 时立刻返回准备就绪链表里的数据即可。

![img](https://pic4.zhimg.com/80/v2-6192547df61e17906b163e931ca699af_720w.webp)

## 2.epoll 的两种触发模式

epoll有EPOLLLT和EPOLLET两种触发模式，LT是默认的模式，ET是“高速”模式。

- LT（水平触发）模式下，只要这个文件描述符还有数据可读，每次 epoll_wait都会返回它的事件，提醒用户程序去操作；
- ET（边缘触发）模式下，在它检测到有 I/O 事件时，通过 epoll_wait 调用会得到有事件通知的文件描述符，对于每一个被通知的文件描述符，如可读，则必须将该文件描述符一直读到空，让 errno 返回 EAGAIN 为止，否则下次的 epoll_wait 不会返回余下的数据，会丢掉事件。如果ET模式不是非阻塞的，那这个一直读或一直写势必会在最后一次阻塞。

还有一个特点是，epoll使用“事件”的就绪通知方式，通过epoll_ctl注册fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd，epoll_wait便可以收到通知。

![img](https://pic3.zhimg.com/80/v2-838120c04fb6132faebd24b103ce5286_720w.webp)

**【epoll为什么要有EPOLLET触发模式？】：**

如果采用 EPOLLLT 模式的话，系统中一旦有大量你不需要读写的就绪文件描述符，它们每次调用epoll_wait都会返回，这样会大大降低处理程序检索自己关心的就绪文件描述符的效率。

而采用EPOLLET这种边缘触发模式的话，当被监控的文件描述符上有可读写事件发生时，epoll_wait()会通知处理程序去读写。

如果这次没有把数据全部读写完(如读写缓冲区太小)，那么下次调用epoll_wait()时，它不会通知你，也就是它只会通知你一次，直到该文件描述符上出现第二次可读写事件才会通知你！！！这种模式比水平触发效率高，系统不会充斥大量你不关心的就绪文件描述符。

【总结】：

- ET模式（边缘触发）只有数据到来才触发，不管缓存区中是否还有数据，缓冲区剩余未读尽的数据不会导致epoll_wait返回；
- LT 模式（水平触发，默认）只要有数据都会触发，缓冲区剩余未读尽的数据会导致epoll_wait返回。

## 3.epoll反应堆模型

【epoll模型原来的流程】：

```text
epoll_create(); // 创建监听红黑树
epoll_ctl(); // 向书上添加监听fd
epoll_wait(); // 监听
有监听fd事件发送--->返回监听满足数组--->判断返回数组元素--->
lfd满足accept--->返回cfd---->read()读数据--->write()给客户端回应。
```

【epoll反应堆模型的流程】：

```text
epoll_create(); // 创建监听红黑树
epoll_ctl(); // 向书上添加监听fd
epoll_wait(); // 监听
有客户端连接上来--->lfd调用acceptconn()--->将cfd挂载到红黑树上监听其读事件--->
epoll_wait()返回cfd--->cfd回调recvdata()--->将cfd摘下来监听写事件--->
epoll_wait()返回cfd--->cfd回调senddata()--->将cfd摘下来监听读事件--->...--->
```

![img](https://pic4.zhimg.com/80/v2-afd7dedb208533f8d43754f200fe411f_720w.webp)

【Demo】：

```text
#include <stdio.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <arpa/inet.h>
#include <fcntl.h>
#include <unistd.h>
#include <errno.h>
#include <string.h>
#include <stdlib.h>
#include <time.h>

#define MAX_EVENTS 1024 /*监听上限*/
#define BUFLEN  4096    /*缓存区大小*/
#define SERV_PORT 6666  /*端口号*/

void recvdata(int fd,int events,void *arg);
void senddata(int fd,int events,void *arg);

/*描述就绪文件描述符的相关信息*/
struct myevent_s
{
    int fd;             //要监听的文件描述符
    int events;         //对应的监听事件，EPOLLIN和EPLLOUT
    void *arg;          //指向自己结构体指针
    void (*call_back)(int fd,int events,void *arg); //回调函数
    int status;         //是否在监听:1->在红黑树上(监听), 0->不在(不监听)
    char buf[BUFLEN];   
    int len;
    long last_active;   //记录每次加入红黑树 g_efd 的时间值
};

int g_efd;      //全局变量，作为红黑树根
struct myevent_s g_events[MAX_EVENTS+1];    //自定义结构体类型数组. +1-->listen fd

/*
 * 封装一个自定义事件，包括fd，这个fd的回调函数，还有一个额外的参数项
 * 注意：在封装这个事件的时候，为这个事件指明了回调函数，一般来说，一个fd只对一个特定的事件
 * 感兴趣，当这个事件发生的时候，就调用这个回调函数
 */
void eventset(struct myevent_s *ev, int fd, void (*call_back)(int fd,int events,void *arg), void *arg)
{
    ev->fd = fd;
    ev->call_back = call_back;
    ev->events = 0;
    ev->arg = arg;
    ev->status = 0;
    if(ev->len <= 0)
    {
        memset(ev->buf, 0, sizeof(ev->buf));
        ev->len = 0;
    }
    ev->last_active = time(NULL); //调用eventset函数的时间
    return;
}

/* 向 epoll监听的红黑树 添加一个文件描述符 */
void eventadd(int efd, int events, struct myevent_s *ev)
{
    struct epoll_event epv={0, {0}};
    int op = 0;
    epv.data.ptr = ev; // ptr指向一个结构体（之前的epoll模型红黑树上挂载的是文件描述符cfd和lfd，现在是ptr指针）
    epv.events = ev->events = events; //EPOLLIN 或 EPOLLOUT
    if(ev->status == 0)       //status 说明文件描述符是否在红黑树上 0不在，1 在
    {
        op = EPOLL_CTL_ADD; //将其加入红黑树 g_efd, 并将status置1
        ev->status = 1;
    }
    if(epoll_ctl(efd, op, ev->fd, &epv) < 0) // 添加一个节点
        printf("event add failed [fd=%d],events[%d]\n", ev->fd, events);
    else
        printf("event add OK [fd=%d],events[%0X]\n", ev->fd, events);
    return;
}

/* 从epoll 监听的 红黑树中删除一个文件描述符*/
void eventdel(int efd,struct myevent_s *ev)
{
    struct epoll_event epv = {0, {0}};
    if(ev->status != 1) //如果fd没有添加到监听树上，就不用删除，直接返回
        return;
    epv.data.ptr = NULL;
    ev->status = 0;
    epoll_ctl(efd, EPOLL_CTL_DEL, ev->fd, &epv);
    return;
}

/*  当有文件描述符就绪, epoll返回, 调用该函数与客户端建立链接 */
void acceptconn(int lfd,int events,void *arg)
{
    struct sockaddr_in cin;
    socklen_t len = sizeof(cin);
    int cfd, i;
    if((cfd = accept(lfd, (struct sockaddr *)&cin, &len)) == -1)
    {
        if(errno != EAGAIN && errno != EINTR)
        {
            sleep(1);
        }
        printf("%s:accept,%s\n",__func__, strerror(errno));
        return;
    }
    do
    {
        for(i = 0; i < MAX_EVENTS; i++) //从全局数组g_events中找一个空闲元素，类似于select中找值为-1的元素
        {
            if(g_events[i].status ==0)
                break;
        }
        if(i == MAX_EVENTS) // 超出连接数上限
        {
            printf("%s: max connect limit[%d]\n", __func__, MAX_EVENTS);
            break;
        }
        int flag = 0;
        if((flag = fcntl(cfd, F_SETFL, O_NONBLOCK)) < 0) //将cfd也设置为非阻塞
        {
            printf("%s: fcntl nonblocking failed, %s\n", __func__, strerror(errno));
            break;
        }
        eventset(&g_events[i], cfd, recvdata, &g_events[i]); //找到合适的节点之后，将其添加到监听树中，并监听读事件
        eventadd(g_efd, EPOLLIN, &g_events[i]);
    }while(0);

    printf("new connect[%s:%d],[time:%ld],pos[%d]",inet_ntoa(cin.sin_addr), ntohs(cin.sin_port), g_events[i].last_active, i);
    return;
}

/*读取客户端发过来的数据的函数*/
void recvdata(int fd, int events, void *arg)
{
    struct myevent_s *ev = (struct myevent_s *)arg;
    int len;

    len = recv(fd, ev->buf, sizeof(ev->buf), 0);    //读取客户端发过来的数据

    eventdel(g_efd, ev);                            //将该节点从红黑树上摘除

    if (len > 0) 
    {
        ev->len = len;
        ev->buf[len] = '\0';                        //手动添加字符串结束标记
        printf("C[%d]:%s\n", fd, ev->buf);                  

        eventset(ev, fd, senddata, ev);             //设置该fd对应的回调函数为senddata    
        eventadd(g_efd, EPOLLOUT, ev);              //将fd加入红黑树g_efd中,监听其写事件    

    } 
    else if (len == 0) 
    {
        close(ev->fd);
        /* ev-g_events 地址相减得到偏移元素位置 */
        printf("[fd=%d] pos[%ld], closed\n", fd, ev-g_events);
    } 
    else 
    {
        close(ev->fd);
        printf("recv[fd=%d] error[%d]:%s\n", fd, errno, strerror(errno));
    }   
    return;
}

/*发送给客户端数据*/
void senddata(int fd, int events, void *arg)
{
    struct myevent_s *ev = (struct myevent_s *)arg;
    int len;

    len = send(fd, ev->buf, ev->len, 0);    //直接将数据回射给客户端

    eventdel(g_efd, ev);                    //从红黑树g_efd中移除

    if (len > 0) 
    {
        printf("send[fd=%d], [%d]%s\n", fd, len, ev->buf);
        eventset(ev, fd, recvdata, ev);     //将该fd的回调函数改为recvdata
        eventadd(g_efd, EPOLLIN, ev);       //重新添加到红黑树上，设为监听读事件
    }
    else 
    {
        close(ev->fd);                      //关闭链接
        printf("send[fd=%d] error %s\n", fd, strerror(errno));
    }
    return ;
}

/*创建 socket, 初始化lfd */

void initlistensocket(int efd, short port)
{
    struct sockaddr_in sin;

    int lfd = socket(AF_INET, SOCK_STREAM, 0);
    fcntl(lfd, F_SETFL, O_NONBLOCK);                //将socket设为非阻塞

    memset(&sin, 0, sizeof(sin));               //bzero(&sin, sizeof(sin))
    sin.sin_family = AF_INET;
    sin.sin_addr.s_addr = INADDR_ANY;
    sin.sin_port = htons(port);

    bind(lfd, (struct sockaddr *)&sin, sizeof(sin));

    listen(lfd, 20);

    /* void eventset(struct myevent_s *ev, int fd, void (*call_back)(int, int, void *), void *arg);  */
    eventset(&g_events[MAX_EVENTS], lfd, acceptconn, &g_events[MAX_EVENTS]);    

    /* void eventadd(int efd, int events, struct myevent_s *ev) */
    eventadd(efd, EPOLLIN, &g_events[MAX_EVENTS]);  //将lfd添加到监听树上，监听读事件

    return;
}

int main()
{
    int port=SERV_PORT;

    g_efd = epoll_create(MAX_EVENTS + 1); //创建红黑树,返回给全局 g_efd
    if(g_efd <= 0)
            printf("create efd in %s err %s\n", __func__, strerror(errno));

    initlistensocket(g_efd, port); //初始化监听socket

    struct epoll_event events[MAX_EVENTS + 1];  //定义这个结构体数组，用来接收epoll_wait传出的满足监听事件的fd结构体
    printf("server running:port[%d]\n", port);

    int checkpos = 0;
    int i;
    while(1)
    {
    /*    long now = time(NULL);
        for(i=0; i < 100; i++, checkpos++)
        {
            if(checkpos == MAX_EVENTS);
                checkpos = 0;
            if(g_events[checkpos].status != 1)
                continue;
            long duration = now -g_events[checkpos].last_active;
            if(duration >= 60)
            {
                close(g_events[checkpos].fd);
                printf("[fd=%d] timeout\n", g_events[checkpos].fd);
                eventdel(g_efd, &g_events[checkpos]);
            }
        } */
        //调用eppoll_wait等待接入的客户端事件,epoll_wait传出的是满足监听条件的那些fd的struct epoll_event类型
        int nfd = epoll_wait(g_efd, events, MAX_EVENTS+1, 1000);
        if (nfd < 0)
        {
            printf("epoll_wait error, exit\n");
            exit(-1);
        }
        for(i = 0; i < nfd; i++)
        {
            //evtAdd()函数中，添加到监听树中监听事件的时候将myevents_t结构体类型给了ptr指针
            //这里epoll_wait返回的时候，同样会返回对应fd的myevents_t类型的指针
            struct myevent_s *ev = (struct myevent_s *)events[i].data.ptr;
            //如果监听的是读事件，并返回的是读事件
            if((events[i].events & EPOLLIN) &&(ev->events & EPOLLIN))
            {
                ev->call_back(ev->fd, events[i].events, ev->arg);
            }
            //如果监听的是写事件，并返回的是写事件
            if((events[i].events & EPOLLOUT) && (ev->events & EPOLLOUT))
            {
                ev->call_back(ev->fd, events[i].events, ev->arg);
            }
        }
    }
    return 0;
}
```

原文地址：https://zhuanlan.zhihu.com/p/521375710

作者：linux

# 【NO.321】c++开发工作中常见的几种内存泄漏场景汇总

> 内存泄漏（Memory Leak）是指程序中已动态分配的堆内存由于某种原因程序未释放或无法释放，造成系统内存的浪费，导致程序运行速度减慢甚至系统崩溃等严重后果。

作为C/C++程序员，谁还不写Bug，Bug里面的王者要数内存泄漏，内存泄漏具有其独有的属性，比如说：隐蔽性强、难以排查、占用资源不断累积等特点，更甚者是会让人想要摔键盘……

本文主要是对工作中经常遇到的内存泄漏场景进行总结，与君共勉。

## **1.在构造函数中抛出异常**

```text
class Test
{
public:
    Test(int iFlag)
    {
        m_pBuf = new char[4*1024*1024];
        if(!iFlag)
        {
            throw("Exception:验证构造函数抛出异常");
        }
    }
    ~Test()
    { 
        std::cout<<"释放类中申请的资源"<<std::endl;
        delete [] m_pBuf;
    }
private:
    char *m_pBuf;
};


int main()
{
    Test cTest(0);
    return 0;
}
```

如上面的代码所示，代码在编译器中编译正常，声明cTest类实例后如果传入参数为真，则可以正常运行且类中new的资源可以正常释放。但是当传入参数为0时，运行代码后抛出异常。进程退出，异常信息如下图所示：



![img](https://pic2.zhimg.com/80/v2-5ad841016aad40461c8a6feb1b40a0bd_720w.webp)



从结果可以看出，抛出异常后代码退出，但是类的析构函数没有被调用。这也说明如果在构造函数中抛出异常，类的析构函数是不会被调用的。所以如果要在构造函数中使用抛出异常，那么切记，一定要在抛出异常前对申请的资源进行正确的释放。反之，就像上面的代码一样，产生内存泄漏的风险。如果要将上面的代码修改正确，可以做如下修改：

```text
Test(int iFlag)
    {
        m_pBuf = new char[4*1024*1024];
        if(!iFlag)
        {
            delete [] m_pBuf;
            throw("Exception:验证构造函数抛出异常");
        }
    }
```

## **2.匿名对象产生的内存泄漏**

如下面的代码所示，代码功能定义一个临时的对象，定义好后没有使用指针对其进行指向，在程序退出时，临时对象申请的资源就不会进行释放，使用内存检测工具后，就会提示内存泄漏风险。

```text
int main()
{
    std::cout<< (*new std::string("hello world"))<<std::endl;
    return 0;
}
```

如代码所示，上面这段代码既能通过编译，又能输出正确的结果，但是却存在内存泄漏风险。这是因为在上面的代码中使用了new在堆上申请了资源且成功分配了空间。如果在代码运行结束后没有使用delete进行释放，那么就会产生内存泄漏。如果想逃过内存泄漏工具的检测，只要将*new std::string("hello world")赋值给一个指针，然后在退出前对指针进行释放即可。

## **3.基类中的析构函数引发的内存泄露**

在C++中，如果子类的对象是通过基类的指针进行删除，如果基类的析构函数不是虚拟的，那么子类的析构函数可能不会被调用，从而导致派生类资源没有被释放，进而产生内存泄漏。如下面的代码所示：

```text
class TBase
{
public:
    virtual int DoSomeThing(){std::cout<<"TBase::DoSomeThing"<<std::endl;}
    virtual ~TBase(){}
};


class Test:public TBase
{
public:
    Test()
    {
        m_pBuf = new char[12];
        memset(m_pBuf,0x0,sizeof(m_pBuf));
    }
    ~Test()
    { 
        std::cout<<"Test子类释放类中申请的资源"<<std::endl;
        delete [] m_pBuf;
    }
    
    int DoSomeThing(){
        std::cout<<"Test::DoSomeThing"<<m_pBuf<<std::endl;
        return 0;
    }
private:
    char *m_pBuf;
};


int main()
{
    TBase *pBase = new Test();
    delete pBase;
    return 0;
}
```

如代码所示，在main函数中定义了一个基类的指针，并指向其子类对象，随后对基类指针进行释放，本意是想通过对基类指针释放同时也调用子类的析构函数释放子类资源。但是事与愿违，运行后，并没有调用子类的析构函数。这是因为，在基类中并没有定义析构函数，在这种情况下，编译器会为我们默认生成一个析构函数，但还不够智能，生成的析构函数不是虚拟的，这样在对基类指针进行析构时就不能调用子类析构函数释放资源。如果想要达到我们想要的效果，只需将基类中的析构函数定义成虚析构即可。修改后运行结果如图所示：



![img](https://pic2.zhimg.com/80/v2-2960313f603ebbd6b62d8a1056f5f46d_720w.webp)



可见，子类中的资源得到正常释放。

## **4.void\*指针产生的内存泄漏**

如下面的代码所示，定义一个类对象后，本意想在类外统一定义一个资源释放接口对资源进行释放，但是事情却没有向我们想象的地方进行。依旧使用上面的代码，我们在这里只新增一个资源释放接口并在main函数中修改调用。

```text
void deleteObj(void *pObj)
{
    delete pObj;
    std::cout<<"delete pObj"<<std::endl;
}
int main()
{
    Test *pBase = new Test();
    deleteObj(pBase);
    return 0;
}
```

上面的代码乍一看是没什么问题的，但是仔细考虑下，将pBase传入到deleteObj后，pBase被转换成void*类型，然后再释放。但是这样做就破坏了delete的工作原理，delete删除对象时，先调用对象的析构函数，再delete指针对象，上面的代码在将pBase转换成void*后，delete获取不到析构对象的类型就不能正确调用对象的析构函数，因此导致析构对象可能产生内存泄漏。

## **5.容器元素产生的内存泄漏**

容器元素产生的内存泄漏主要是当容器中的元素为指针时，每次new一个对象都会将指针保存在容器中，清理容器时，容器中的指针对象不会同时被清理。

在编码时，如果容器中保存的是对象，那么容器会自动对对象进行清理，但如果是指针，则需要编码人员手动对容器中保存的指针进行清理，如下面的代码所示,在这里依旧复用前面章节的代码，本次只对main函数进行改造：

```text
int main()
{
    std::vector<Test *> vTest;
    for(int i=0;i<10;i++)
    {
        vTest.push_back(new Test());
    }
    vTest.clear();
    return 0;
}
```

如上所示，程序结束时仅使用clear方法对vector资源进行清理，但是，保存在vector中的指针对象并没有被清理掉，需要我们手动进行处理，要想得到正确的代码，按照如下方式进行修改：

```text
for(int i=0;i<vTest.size();i++)
{
    delete vTest[i];
}
```

代码运行后，从结果可知，资源被正确释放，如下图所示：

![img](https://pic2.zhimg.com/80/v2-e5c1bd03fb70322e1c7a20927eb0aedd_720w.webp)

## **6.写在最后**

开发有bug是正常的，要学会学习bug，不断的总结编程过程中遇到的问题，才能让我们更好的解决问题，最终把问题消灭在写代码之前。

学习地址：https://zhuanlan.zhihu.com/p/519933572

作者：linux

# 【NO.322】手写线程池与性能分析

**手写线程池与性能分析**

作为一款服务器而言,很多服务器的源码，最底层最底层这些东西，是很通用的

比如我们之前讲过的网络,以及池式结构,最底层的这些组件是提供给我们应用层写业务逻辑的

在工作中接触的并不是很多，但是在面试的时候会问很多，工作中是以写业务为主，比如说工作中要你写个线程池这种可能性不大，比如说要你写一个连接池的可能性也不大，大量的时间可能是在写业务为主,那但是这个东西了，他在面试的时候,又是非常非常管用的东西。

简历里面肯定写一些通用的组件为主。

4个池式结构

![img](https://pic4.zhimg.com/80/v2-3031ddffcf5d4e7de5c50b28d0f10ab3_720w.webp)

先跟大家解释一下就是这些东西到底是用来做什么的?

对于一款服务器而言，然后n多的客户端连接上这个,然后再发每一个数据的时候，客户端给服务器发送的请求，发送的每一帧请求。

服务端这边接收完了之后进行处理，在recv()完了之后，那这里面我们正在进行解析,解析完数据之后，比如说我们写一些日志，那在这种同步的过程中间，写日志的时候

![img](https://pic4.zhimg.com/80/v2-862fdd12f75d9ddead56210fa047a7ef_720w.webp)

大概是这么一个流程，那对于parser这个过程只是把这个协议解析出来,解析出来之后，我们还有一系列的动作，比如说我们要去数据库查询，或者说我们要记录一些日志,或者说对数据库进行增删改查操作，

还有在这个过程中进行这些crud的操作的过程中，那会发现当这个日志记录的时候，它的时间周期会比较长，

就是在这一步落盘的时候，要写磁盘速度比较慢，对数据库操作的时候也会比较慢，所以很多的时候在这里大家就会引入一个东西，消息队列

抛到一个消息队列里面，然后再由消息队列进行去处理，或者说我通知给另外一个进程，由另外一个进程或者另外一个线程来对它进行操作，对这个日志进行罗盘来进行crud操作

那这个过程中间还有一种做法就是我们直接引入一个线程池，把这些日志或者这些crud当做一个一个的任务，把它抛给线程池，由线程池来处理这些任务,线程池就是解决这么一个问题，线程池的使用场景还是非常多的，

![img](https://pic2.zhimg.com/80/v2-4288cbb4bc7f5845c1d1bb4ec83d4c25_720w.webp)

比如说写日志

我们对服务器的一些业务需要计算，需要一些计算结果，计算结果计算的时间周期比较长的时候，我们也可以引入另外的线程，反正说我们在开线程的过程中间都可以想到线程池,第三个就是crud,不光是在服务器上面

**1.线程池到底是什么东西？**

我们在这里跟大家分析一下，它的使用场景先讲了，那我们再来分析一下它到底怎么做?

首先第一个它是由一个一个的任务所组成，

比如说我们写日志是一个任务，我们做计算也好，把一个计算任务把它抛进去，对数据库操作，我们也可以把它当做一次任务。

这里肯定就是有一个这个东西:任务队列

任务队列是提供给外界去用的时候，我们一个个的任务把它抛给任务队列

这时我们外面写的应用程序调用的接口,调用接口时候就直接可以写成一个push_tasks，把这任务给抛出去。

![img](https://pic3.zhimg.com/80/v2-0def69ce53fe4c39eb6de916ed173362_720w.webp)

**2.那任务在什么时候执行呢?**

这边当然还有一个执行队列,就是每1个节点，这里6个框，每1个框你就可以理解为是1个线程。

然后这一系列的线程就组成了一个集合，这个集合，就变成了一个执行队列，那这个执行队列执行什么东西呢,就从任务队列里面取任务，并且进行执行

第一个问题，

多个线程同时在一个任务队列里面，取任务,需要加锁，这是第一个问题。那这个任务队列就是这里几个线程的临界资源,取任务需要对这个任务队列进行加锁

第二个问题

举例的话就可能好理解，惊群那个案例一样，在中午的时候，你去一个食堂吃饭的时候，这个打饭的这个窗口上面，这6个阿姨相当于是去执行一个一个任务。

在这个过程中，每一次取任务的时候，他们肯定形成了某一种契约，就是每一次谁拿一个走，但是这里还有一个问题是什么？就是如果在下午三四点钟食堂没有人了，那这6个阿姨都在休息是吧？那这个状态在线程里面如何去表示这个休息的状态呢？

就是当任务不足的时候，如何去进行休息，线程如何操作?

条件变量 这是第二个。

在执行条件变量的时候是什么意思呢?这个状态进入挂起,进入一个条件等待,等待一个什么条件呢?等待这个任务队列不为空的一个条件满足，然后中间的线程取消休息，他这时候条件满足,条件等待等待任务队列不为空

这个线程池已经慢慢的整个原理性的问题，讲的差不多，可以看到线程池现在能理解透这两个问题。

那再来一个问题，线程池的工作原理?

把刚刚这个模式

一个任务队列加上一系列的线程，从这个任务队列取任务并且执行，

放在这里

就是我们每读出一个数据之后，然后把这个数据我们可以连同解析一起,把它当做一个任务,再由线程池来进行执行,就这么一个动作

那我们再来回过头来捋一下，回过头来再来思考一下。

网络这一层，整个一个服务器从他接收数据，网络数据的处理分为三个部分

就是从网卡里面接收数据到处理数据，这个过程总共有三个阶段。

![img](https://pic2.zhimg.com/80/v2-b7e1a6f923d56ef37d1a779629fcd311_720w.webp)

第一是检测io里面是否有数据，是否可读,是否可写，io事件是否就绪，这是第一个。

检测完了之后，

第二对它进行读写操作，

第三步读写完之后对数据进行解析与操作。

那这三个步骤里第一个步骤，网络数据最前面这层通过epoll

第二个是我们通过recv/send

第三步parser我们进行解析，那这个解析的流程就会比较多，

这三个过程，这是对每一帧网络数据都要经过这三个流程

那这个过程中间就引起了一个现象，写一个比如说多线程，然后多线程网络框架那这个东西怎么去理解，哪个地方用的多线程，

![img](https://pic1.zhimg.com/80/v2-4b701bbb11e0ac4c21884729c7de5c98_720w.webp)

这个流程可以通过这么一个数据的流向，

![img](https://pic1.zhimg.com/80/v2-914479c3fee678b107be5ea37c0eeca8_720w.webp)

这个线程池，在这个里面,我们怎么跟数据流程去结合?

举个例子，先假设我们已经有了线程池，现在不用去管线程池的实现，

![img](https://pic4.zhimg.com/80/v2-7088cd2cc9c7c1a0dfa4650973fe9313_720w.webp)

是一个线程，我们要引入线程池，引入线程池的话，各位那我们可以怎么做？有这么两种做法，两种做法都是可以的。

第一种我们直接把每一个任务把它抛到线程池里面，这是第一种做法。现在也就是说我们读、写、以及解析，都在这个线程

这是第一种做法。



第二种做法我们也可以是这样，

我们把recv放过来，我们再接着剩下的buffer，放线程池

![img](https://pic3.zhimg.com/80/v2-abfed434a3e91a35d37fd378365bedee_720w.webp)

讨论一下，

第一种第二种第三种都是可以实现的。

第一种做法就是一个单线程，第二种做法我们是把读写io以及解析数据这两个任务抛到线程池

第三种我们只把解析完的数据抛到线程池里面

但是每一种都有每一种优点和缺点，各位都有优点的缺点，

但是我们现在来分析一下这三种做法它的优点和缺点分别是什么？

第一种大家可以看到它是一个单线程，

如果对于任务比较轻的纯内存操作,这个是ok的，这里面不涉及到比如说写日志，它只是业务比较复杂而已，因为它在读写的时候不耗时的,可以考虑第三种

那第二种这种做法把fd抛给另外一个线程，他解决了什么问题?

如果这种情况是针对于哪一种？

针对于io的操作时间比较长，比如说这个fd是一个阻塞的，就可以采用第二种方式，

这三种做法里面速度最快的，不影响主循环第二种是最快的，但是

第二种有一个很大的问题，有一个大的问题，多个线程共用一个fd现象,

这个现象是怎么来的？

我不是一个任务，我把它抛给线程池吗?那怎么出现多个线程池共用一个fd的现象?

epoll检测完之后把这个 fd直接抛给了线程池，现在由线程池再进行处理，现在两个任务分别发出去，但是时间间隔不是很长，那也就是说这个 fd检测完之后，这个fd很有可能分给另外一线程

它的缺点就会出现一个现象，就是有可能因为这两个线程你都把它抛给了线程池来处理，所有的数据操作我们是不管的，那大家就会出现一个现象。

就是可能a线程对他进行读,对他进行写的时候，线程b很有可能把数据给closer关闭了。

就是线程a在准备数据的时候或者数据就绪的时候，

那结果线程b进行close掉了

因为这个fd大家可以看到我们这个代码上面我们是不进行任何的处理，而对应的只是把所有的fd都给有线程处理

这几个现象里面，既然这一个它的效率不影响主循环的情况下的效率是最高的，但是这个 fd共享就是比较难的。

我们该如何去解决这个问题，或者说有没有解的角度，fd不应该是在多线程共享

多线程网络框架，它的其实对于底层来说，就是把这三个组件，你哪些东西把它丢给线程池里面，

把检测io放到多个线的里面，ok的。

把读写lo放到多个线程里面也是ok的，

把协议解析以及操作放到多线程里面也是ok的

始终保持这三点

线程池的核心有俩部分组成一个任务队列,第二个是我们讲到的执行队列,第三个部分我们需要管理组件,有秩序的工作。

第一个，加锁,互斥锁

第二个,条件变量

就是通过这俩方面使它有秩序的管理

我们来分析这个任务队列由那些东西组成?

有个前提，任务队列它是任务的一个集合，首先我们先把任务描述清楚，然后再来描述这个集合,两层关系

那任务里面包含那些东西呢?

我们怎么去把它封装成一个一个任务？

第一个首先每一个人任务都不一样,存款或者有些取款，或者有些可能去打印流水等等

这里需要一个回调函数callback();这个回调函数是由任务自己本身去实现的,把它抽象出来,

第二个还有一个参数，如果你想要去办存款，你得带上你的现金，如果你去办取款，你得带上你的银行卡，所以每一个任务执行，每一个回调函数都需要带不同的参数

![img](https://pic1.zhimg.com/80/v2-36c3e344320546b5a84fe2a7e6bcfbf0_720w.webp)

这个任务我们封装完之后，我们怎么把它变成一个队列，为它加上前后指针,我们就能够把它串起来。

![img](https://pic2.zhimg.com/80/v2-7107094c350f303e70a5514ab1800ecd_720w.webp)

一个回调函数,以及一个回调函数对应的参数在加上前驱后继,我们只要记录一个头指针?

如果当我们任务很少时,我们一次性开了很多线程我们怎么去退出?

中间就用一些进行休息，这里有一个状态terminate,

关于线程的退出,pthread_exit(thid);退出线程id这个是OK的,

返回的时候，如果我们有线程加入pthread_join(thid)等待它返回的话，父线程针对子线程返回的时候它会等待这个结果,pthread_detach()分离后俩个就失去了亲缘关系,pthread_join(thid)就等待不出结果,

terminate这里线程退出我们是比较友好一点,pthread_cancel()退出比较粗暴,exit()函数我们压根就不知道这是什么状态,调用就直接关闭了,我们在执行阶段就能看到他的委婉，terminate可以走完整个流程完整的退出,会更加的优雅

![img](https://pic3.zhimg.com/80/v2-6d99e5138c4925204b838de36ffe046a_720w.webp)

这个管理组件里面大概包含这4个项目

![img](https://pic1.zhimg.com/80/v2-c57770d1bfc1dd89b80856d98d702684_720w.webp)

![img](https://pic1.zhimg.com/80/v2-98781ba2cc745c9c3cf23aaff1b1a000_720w.webp)

关于队列我们添加和删除一个节点需要使用宏定义,为什么选用宏定义去添加删除节点呢?

可以注意到NJOB和NWORKER是俩个不同的类型,不同的类型我们对她进行同样的操作,在C++里面使用模板来做是OK的但是在C代码里面没有模板这个概念,也就是模板的底层应该是宏定义,

![img](https://pic4.zhimg.com/80/v2-e6c9d9e0c45664715300a34f93594baf_720w.webp)

**这个do /while(0)是什么意思?**

宏定义的这个过程它跟内联是一样的,就是把代码copy过去,但是一个是在预编译的时候,一个是在编译的时候

由于do/while(0)这个语法就决定了它需要携带一个分号,这个地方采用头插法

![img](https://pic3.zhimg.com/80/v2-1c74ffbc8444eb179f89ec271afdeffa_720w.webp)

删除也是类似的

![img](https://pic2.zhimg.com/80/v2-4bf43ab3c4d55e7915a1e039481dab39_720w.webp)

所以后面不管是对NJOB的操作或者是NWORKER的操作都可以借助这俩个宏,

那线程池它需要具备用哪些接口？需要哪些API线程池才能够去使用?

首先第一个肯定有的一个是初始化，给出两个参数，第一个参数是线程池的对象,第二个是说我们创建多少个线程池

![img](https://pic4.zhimg.com/80/v2-cd3f5bfd2d3c5cb78a303f22a5e4f733_720w.webp)

初始化的时候我们只需要创建一个东西，就是这个线程池的数量有多少？

![img](https://pic4.zhimg.com/80/v2-2f4385e0de18b7f761052922a9cb992f_720w.webp)

线程创建我们只依赖于线程数量。

第二个函数,我们要做的就是往线程池里面抛一个任务,就是把一个task抛到对应的任务队列里面

![img](https://pic2.zhimg.com/80/v2-23bf97589d315da2d95fd611c07445c5_720w.webp)

第三个接口，既然有创建当然就有销毁，

![img](https://pic2.zhimg.com/80/v2-e5ffcfd0dd64535b8c41f4a48ef7a1ad_720w.webp)

三个接口是对外的使用，理解这三个接口就可以了。

一个线程池它由那些东西组成?

第一个一个任务队列,第二个一个执行队列,第三个一个管理这俩个队列正常运行的一个组件

那这个在创建的时候呢,我们需要把执行队列初始化完,与之对应的互斥量条件变量创建完,



那在创建这个任务队列的时候，创建这个执行队列的时候，每一个任务每一个执行每一个worker是需要跟这个线程一一对应上的，然后把这个work直接把它抛到线程里面，每一个线程拿这个worker去工作,初始化可以说是4个部分，

1.参数是否合法

2.mutex

3.cond

4.worker

创建完之后线程池就可以开始工作了。

线程的入口函数和任务的回调函数之间是什么关系?

线程回调函数是不是就等同于任务回调函数?

![img](https://pic1.zhimg.com/80/v2-8eecc7bbeeb15427f09fcb59924fc414_720w.webp)

这里面这个绿色的就相当于线程的入口函数，它是不断的去任务队列里面取任务执行,不断就是循环,取完任务之后,执行的时候就是执行回调函数

![img](https://pic4.zhimg.com/80/v2-4a90db55d53be1711c2ded13b97bdcd3_720w.webp)

这个移除就相当于是把这个头节点给移除掉。

我们开的线程比较多的时候在那个地方进行放缩?怎么释放一些线程

接下来丢一个任务进去他能做那些事情?

![img](https://pic1.zhimg.com/80/v2-0911ccd332d981b2b842a06681034b5c_720w.webp)

现在多个线程等待这个条件,通知的时候会有惊群现象吗?

![img](https://pic2.zhimg.com/80/v2-74a7f9348c63f87fa4415936796ec825_720w.webp)

**nginx线程池源码**

ngx_thread_pool_cycle

防止线程被其它的东西屏蔽这些终止,就是防止线程pthread_sigmask被其它东西影响做了一个屏蔽

![img](https://pic2.zhimg.com/80/v2-a30c6c766287bf7235f4a103d0d0104d_720w.webp)

这里的cycle就是线程的入口函数,然后对应这里面的任务就是一个永真的循环,然后判断任务队列是不是为空,

![img](https://pic4.zhimg.com/80/v2-3179692d09dcaf7e0c80eb9fd0ba9b37_720w.webp)

条件等待往下面走是封装的接口,如果不为空的话从任务队列里面取出一个任务出来,出来之后解锁,之后就是handler,就是每一个任务的回调函数,逻辑都是一样的

![img](https://pic2.zhimg.com/80/v2-a7ac94e708d542588b4d4f678fe9239d_720w.webp)

看源码就可以这样对比,先把基础主体把握住,

第二个就是往线程池里面post抛一个任务进去

![img](https://pic3.zhimg.com/80/v2-df771566dd45fdf2508d0d1a95d8e746_720w.webp)

方法逻辑都是把任务加进去然后再去通知,

这个东西是什么意思?**last它有俩个指针

![img](https://pic4.zhimg.com/80/v2-af4a2e95d89143822ca746d7ded4fb63_720w.webp)

这是往队列里面添加一个节点然后waiting++

![img](https://pic2.zhimg.com/80/v2-646bcca82aa5b525ea13de576b997df1_720w.webp)

它主要是统计任务的数量方便我们去放缩任务,但任务量很多我们可以增加线程,很少我们可以减少线程,

那这个二级指针到底做了什么?

![img](https://pic2.zhimg.com/80/v2-666814fd097273f7fa6b11454f3c1f55_720w.webp)

这个二级指针就是这个

**就是拿着它的第一个字段指向的下一个内存,**为什么这么做好处在哪?

线程池本身没有性能优化,要前后对比,不加入线程池与加入线程池的区别,大概有6倍

多个线程共用一个fd怎么解,参考协程

做线程放缩怎么做?添加一个变量waiting,每添加任务++,判断任务数量和线程数量比例,线程数量退出,任务数量和线程数量统计,比如线程放缩策略4:6

```text
#include <stdio.h>
#include <stdlib.h>
#include <string.h>

#include <pthread.h>



#define LL_ADD(item, list) do { 	\
	item->prev = NULL;				\
	item->next = list;				\
	list = item;					\
} while(0)

#define LL_REMOVE(item, list) do {						\
	if (item->prev != NULL) item->prev->next = item->next;	\
	if (item->next != NULL) item->next->prev = item->prev;	\
	if (list == item) list = item->next;					\
	item->prev = item->next = NULL;							\
} while(0)


typedef struct NWORKER {
	pthread_t thread;
	int terminate;
	struct NWORKQUEUE *workqueue;
	struct NWORKER *prev;
	struct NWORKER *next;
} nWorker;

typedef struct NJOB {
	void (*job_function)(struct NJOB *job);
	void *user_data;
	struct NJOB *prev;
	struct NJOB *next;
} nJob;

typedef struct NWORKQUEUE {
	struct NWORKER *workers;
	struct NJOB *waiting_jobs;
	pthread_mutex_t jobs_mtx;
	pthread_cond_t jobs_cond;
} nWorkQueue;

typedef nWorkQueue nThreadPool;

static void *ntyWorkerThread(void *ptr) {
	nWorker *worker = (nWorker*)ptr;

	while (1) {
		pthread_mutex_lock(&worker->workqueue->jobs_mtx);

		while (worker->workqueue->waiting_jobs == NULL) {
			if (worker->terminate) break;
			pthread_cond_wait(&worker->workqueue->jobs_cond, &worker->workqueue->jobs_mtx);
		}

		if (worker->terminate) {
			pthread_mutex_unlock(&worker->workqueue->jobs_mtx);
			break;
		}
		
		nJob *job = worker->workqueue->waiting_jobs;
		if (job != NULL) {
			LL_REMOVE(job, worker->workqueue->waiting_jobs);
		}
		
		pthread_mutex_unlock(&worker->workqueue->jobs_mtx);

		if (job == NULL) continue;

		job->job_function(job);
	}

	free(worker);
	pthread_exit(NULL);
}



int ntyThreadPoolCreate(nThreadPool *workqueue, int numWorkers) {

	if (numWorkers < 1) numWorkers = 1;
	memset(workqueue, 0, sizeof(nThreadPool));
	
	pthread_cond_t blank_cond = PTHREAD_COND_INITIALIZER;
	memcpy(&workqueue->jobs_cond, &blank_cond, sizeof(workqueue->jobs_cond));
	
	pthread_mutex_t blank_mutex = PTHREAD_MUTEX_INITIALIZER;
	memcpy(&workqueue->jobs_mtx, &blank_mutex, sizeof(workqueue->jobs_mtx));

	int i = 0;
	for (i = 0;i < numWorkers;i ++) {
		nWorker *worker = (nWorker*)malloc(sizeof(nWorker));
		if (worker == NULL) {
			perror("malloc");
			return 1;
		}

		memset(worker, 0, sizeof(nWorker));
		worker->workqueue = workqueue;

		int ret = pthread_create(&worker->thread, NULL, ntyWorkerThread, (void *)worker);
		if (ret) {
			
			perror("pthread_create");
			free(worker);

			return 1;
		}

		LL_ADD(worker, worker->workqueue->workers);
	}

	return 0;
}


void ntyThreadPoolShutdown(nThreadPool *workqueue) {
	nWorker *worker = NULL;

	for (worker = workqueue->workers;worker != NULL;worker = worker->next) {
		worker->terminate = 1;
	}

	pthread_mutex_lock(&workqueue->jobs_mtx);

	workqueue->workers = NULL;
	workqueue->waiting_jobs = NULL;

	pthread_cond_broadcast(&workqueue->jobs_cond);

	pthread_mutex_unlock(&workqueue->jobs_mtx);
	
}

void ntyThreadPoolQueue(nThreadPool *workqueue, nJob *job) {

	pthread_mutex_lock(&workqueue->jobs_mtx);

	LL_ADD(job, workqueue->waiting_jobs);
	
	pthread_cond_signal(&workqueue->jobs_cond);
	pthread_mutex_unlock(&workqueue->jobs_mtx);
	
}




/************************** debug thread pool **************************/
//sdk  --> software develop kit
// 提供SDK给其他开发者使用

#if 1

#define KING_MAX_THREAD			80
#define KING_COUNTER_SIZE		1000

void king_counter(nJob *job) {

	int index = *(int*)job->user_data;

	printf("index : %d, selfid : %lu\n", index, pthread_self());
	
	free(job->user_data);
	free(job);
}



int main(int argc, char *argv[]) {

	nThreadPool pool;

	ntyThreadPoolCreate(&pool, KING_MAX_THREAD);
	
	int i = 0;
	for (i = 0;i < KING_COUNTER_SIZE;i ++) {
		nJob *job = (nJob*)malloc(sizeof(nJob));
		if (job == NULL) {
			perror("malloc");
			exit(1);
		}
		
		job->job_function = king_counter;
		job->user_data = malloc(sizeof(int));
		*(int*)job->user_data = i;

		ntyThreadPoolQueue(&pool, job);
		
	}

	getchar();
	printf("\n");

	
}

#endif
```



```text
/*
 * Copyright (C) Nginx, Inc.
 * Copyright (C) Valentin V. Bartenev
 */


#ifndef _NGX_THREAD_POOL_H_INCLUDED_
#define _NGX_THREAD_POOL_H_INCLUDED_


#include <ngx_config.h>
#include <ngx_core.h>
#include <ngx_event.h>


struct ngx_thread_task_s {
    ngx_thread_task_t   *next;
    ngx_uint_t           id;
    void                *ctx;
    void               (*handler)(void *data, ngx_log_t *log);
    ngx_event_t          event;
};


typedef struct ngx_thread_pool_s  ngx_thread_pool_t;


ngx_thread_pool_t *ngx_thread_pool_add(ngx_conf_t *cf, ngx_str_t *name);
ngx_thread_pool_t *ngx_thread_pool_get(ngx_cycle_t *cycle, ngx_str_t *name);

ngx_thread_task_t *ngx_thread_task_alloc(ngx_pool_t *pool, size_t size);
ngx_int_t ngx_thread_task_post(ngx_thread_pool_t *tp, ngx_thread_task_t *task);


#endif /* _NGX_THREAD_POOL_H_INCLUDED_ */
```



```text
/*
 * Copyright (C) Nginx, Inc.
 * Copyright (C) Valentin V. Bartenev
 * Copyright (C) Ruslan Ermilov
 */


#include <ngx_config.h>
#include <ngx_core.h>
#include <ngx_thread_pool.h>


typedef struct {
    ngx_array_t               pools;
} ngx_thread_pool_conf_t;


typedef struct {
    ngx_thread_task_t        *first;
    ngx_thread_task_t       **last;
} ngx_thread_pool_queue_t;

#define ngx_thread_pool_queue_init(q)                                         \
    (q)->first = NULL;                                                        \
    (q)->last = &(q)->first


struct ngx_thread_pool_s {
    ngx_thread_mutex_t        mtx;
    ngx_thread_pool_queue_t   queue;
    ngx_int_t                 waiting;
    ngx_thread_cond_t         cond;

    ngx_log_t                *log;

    ngx_str_t                 name;
    ngx_uint_t                threads;
    ngx_int_t                 max_queue;

    u_char                   *file;
    ngx_uint_t                line;
};


static ngx_int_t ngx_thread_pool_init(ngx_thread_pool_t *tp, ngx_log_t *log,
    ngx_pool_t *pool);
static void ngx_thread_pool_destroy(ngx_thread_pool_t *tp);
static void ngx_thread_pool_exit_handler(void *data, ngx_log_t *log);

static void *ngx_thread_pool_cycle(void *data);
static void ngx_thread_pool_handler(ngx_event_t *ev);

static char *ngx_thread_pool(ngx_conf_t *cf, ngx_command_t *cmd, void *conf);

static void *ngx_thread_pool_create_conf(ngx_cycle_t *cycle);
static char *ngx_thread_pool_init_conf(ngx_cycle_t *cycle, void *conf);

static ngx_int_t ngx_thread_pool_init_worker(ngx_cycle_t *cycle);
static void ngx_thread_pool_exit_worker(ngx_cycle_t *cycle);


static ngx_command_t  ngx_thread_pool_commands[] = {

    { ngx_string("thread_pool"),
      NGX_MAIN_CONF|NGX_DIRECT_CONF|NGX_CONF_TAKE23,
      ngx_thread_pool,
      0,
      0,
      NULL },

      ngx_null_command
};


static ngx_core_module_t  ngx_thread_pool_module_ctx = {
    ngx_string("thread_pool"),
    ngx_thread_pool_create_conf,
    ngx_thread_pool_init_conf
};


ngx_module_t  ngx_thread_pool_module = {
    NGX_MODULE_V1,
    &ngx_thread_pool_module_ctx,           /* module context */
    ngx_thread_pool_commands,              /* module directives */
    NGX_CORE_MODULE,                       /* module type */
    NULL,                                  /* init master */
    NULL,                                  /* init module */
    ngx_thread_pool_init_worker,           /* init process */
    NULL,                                  /* init thread */
    NULL,                                  /* exit thread */
    ngx_thread_pool_exit_worker,           /* exit process */
    NULL,                                  /* exit master */
    NGX_MODULE_V1_PADDING
};


static ngx_str_t  ngx_thread_pool_default = ngx_string("default");

static ngx_uint_t               ngx_thread_pool_task_id;
static ngx_atomic_t             ngx_thread_pool_done_lock;
static ngx_thread_pool_queue_t  ngx_thread_pool_done;


static ngx_int_t
ngx_thread_pool_init(ngx_thread_pool_t *tp, ngx_log_t *log, ngx_pool_t *pool)
{
    int             err;
    pthread_t       tid;
    ngx_uint_t      n;
    pthread_attr_t  attr;

    if (ngx_notify == NULL) {
        ngx_log_error(NGX_LOG_ALERT, log, 0,
               "the configured event method cannot be used with thread pools");
        return NGX_ERROR;
    }

    ngx_thread_pool_queue_init(&tp->queue);

    if (ngx_thread_mutex_create(&tp->mtx, log) != NGX_OK) {
        return NGX_ERROR;
    }

    if (ngx_thread_cond_create(&tp->cond, log) != NGX_OK) {
        (void) ngx_thread_mutex_destroy(&tp->mtx, log);
        return NGX_ERROR;
    }

    tp->log = log;

    err = pthread_attr_init(&attr);
    if (err) {
        ngx_log_error(NGX_LOG_ALERT, log, err,
                      "pthread_attr_init() failed");
        return NGX_ERROR;
    }

    err = pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED);
    if (err) {
        ngx_log_error(NGX_LOG_ALERT, log, err,
                      "pthread_attr_setdetachstate() failed");
        return NGX_ERROR;
    }

#if 0
    err = pthread_attr_setstacksize(&attr, PTHREAD_STACK_MIN);
    if (err) {
        ngx_log_error(NGX_LOG_ALERT, log, err,
                      "pthread_attr_setstacksize() failed");
        return NGX_ERROR;
    }
#endif

    for (n = 0; n < tp->threads; n++) {
        err = pthread_create(&tid, &attr, ngx_thread_pool_cycle, tp);
        if (err) {
            ngx_log_error(NGX_LOG_ALERT, log, err,
                          "pthread_create() failed");
            return NGX_ERROR;
        }
    }

    (void) pthread_attr_destroy(&attr);

    return NGX_OK;
}


static void
ngx_thread_pool_destroy(ngx_thread_pool_t *tp)
{
    ngx_uint_t           n;
    ngx_thread_task_t    task;
    volatile ngx_uint_t  lock;

    ngx_memzero(&task, sizeof(ngx_thread_task_t));

    task.handler = ngx_thread_pool_exit_handler;
    task.ctx = (void *) &lock;

    for (n = 0; n < tp->threads; n++) {
        lock = 1;

        if (ngx_thread_task_post(tp, &task) != NGX_OK) {
            return;
        }

        while (lock) {
            ngx_sched_yield();
        }

        task.event.active = 0;
    }

    (void) ngx_thread_cond_destroy(&tp->cond, tp->log);

    (void) ngx_thread_mutex_destroy(&tp->mtx, tp->log);
}


static void
ngx_thread_pool_exit_handler(void *data, ngx_log_t *log)
{
    ngx_uint_t *lock = data;

    *lock = 0;

    pthread_exit(0);
}


ngx_thread_task_t *
ngx_thread_task_alloc(ngx_pool_t *pool, size_t size)
{
    ngx_thread_task_t  *task;

    task = ngx_pcalloc(pool, sizeof(ngx_thread_task_t) + size);
    if (task == NULL) {
        return NULL;
    }

    task->ctx = task + 1;

    return task;
}


ngx_int_t
ngx_thread_task_post(ngx_thread_pool_t *tp, ngx_thread_task_t *task)
{
    if (task->event.active) {
        ngx_log_error(NGX_LOG_ALERT, tp->log, 0,
                      "task #%ui already active", task->id);
        return NGX_ERROR;
    }

    if (ngx_thread_mutex_lock(&tp->mtx, tp->log) != NGX_OK) {
        return NGX_ERROR;
    }

    if (tp->waiting >= tp->max_queue) {
        (void) ngx_thread_mutex_unlock(&tp->mtx, tp->log);

        ngx_log_error(NGX_LOG_ERR, tp->log, 0,
                      "thread pool \"%V\" queue overflow: %i tasks waiting",
                      &tp->name, tp->waiting);
        return NGX_ERROR;
    }

    task->event.active = 1;

    task->id = ngx_thread_pool_task_id++;
    task->next = NULL;

    if (ngx_thread_cond_signal(&tp->cond, tp->log) != NGX_OK) {
        (void) ngx_thread_mutex_unlock(&tp->mtx, tp->log);
        return NGX_ERROR;
    }

    *tp->queue.last = task;
    tp->queue.last = &task->next;

    tp->waiting++;

    (void) ngx_thread_mutex_unlock(&tp->mtx, tp->log);

    ngx_log_debug2(NGX_LOG_DEBUG_CORE, tp->log, 0,
                   "task #%ui added to thread pool \"%V\"",
                   task->id, &tp->name);

    return NGX_OK;
}


static void *
ngx_thread_pool_cycle(void *data)
{
    ngx_thread_pool_t *tp = data;

    int                 err;
    sigset_t            set;
    ngx_thread_task_t  *task;

#if 0
    ngx_time_update();
#endif

    ngx_log_debug1(NGX_LOG_DEBUG_CORE, tp->log, 0,
                   "thread in pool \"%V\" started", &tp->name);

    sigfillset(&set);

    sigdelset(&set, SIGILL);
    sigdelset(&set, SIGFPE);
    sigdelset(&set, SIGSEGV);
    sigdelset(&set, SIGBUS);

    err = pthread_sigmask(SIG_BLOCK, &set, NULL);
    if (err) {
        ngx_log_error(NGX_LOG_ALERT, tp->log, err, "pthread_sigmask() failed");
        return NULL;
    }

    for ( ;; ) {
        if (ngx_thread_mutex_lock(&tp->mtx, tp->log) != NGX_OK) {
            return NULL;
        }

        /* the number may become negative */
        tp->waiting--;

        while (tp->queue.first == NULL) {
            if (ngx_thread_cond_wait(&tp->cond, &tp->mtx, tp->log)
                != NGX_OK)
            {
                (void) ngx_thread_mutex_unlock(&tp->mtx, tp->log);
                return NULL;
            }
        }

        task = tp->queue.first;
        tp->queue.first = task->next;

        if (tp->queue.first == NULL) {
            tp->queue.last = &tp->queue.first;
        }

        if (ngx_thread_mutex_unlock(&tp->mtx, tp->log) != NGX_OK) {
            return NULL;
        }

#if 0
        ngx_time_update();
#endif

        ngx_log_debug2(NGX_LOG_DEBUG_CORE, tp->log, 0,
                       "run task #%ui in thread pool \"%V\"",
                       task->id, &tp->name);

        task->handler(task->ctx, tp->log);

        ngx_log_debug2(NGX_LOG_DEBUG_CORE, tp->log, 0,
                       "complete task #%ui in thread pool \"%V\"",
                       task->id, &tp->name);

        task->next = NULL;

        ngx_spinlock(&ngx_thread_pool_done_lock, 1, 2048);

        *ngx_thread_pool_done.last = task;
        ngx_thread_pool_done.last = &task->next;

        ngx_memory_barrier();

        ngx_unlock(&ngx_thread_pool_done_lock);

        (void) ngx_notify(ngx_thread_pool_handler);
    }
}


static void
ngx_thread_pool_handler(ngx_event_t *ev)
{
    ngx_event_t        *event;
    ngx_thread_task_t  *task;

    ngx_log_debug0(NGX_LOG_DEBUG_CORE, ev->log, 0, "thread pool handler");

    ngx_spinlock(&ngx_thread_pool_done_lock, 1, 2048);

    task = ngx_thread_pool_done.first;
    ngx_thread_pool_done.first = NULL;
    ngx_thread_pool_done.last = &ngx_thread_pool_done.first;

    ngx_memory_barrier();

    ngx_unlock(&ngx_thread_pool_done_lock);

    while (task) {
        ngx_log_debug1(NGX_LOG_DEBUG_CORE, ev->log, 0,
                       "run completion handler for task #%ui", task->id);

        event = &task->event;
        task = task->next;

        event->complete = 1;
        event->active = 0;

        event->handler(event);
    }
}


static void *
ngx_thread_pool_create_conf(ngx_cycle_t *cycle)
{
    ngx_thread_pool_conf_t  *tcf;

    tcf = ngx_pcalloc(cycle->pool, sizeof(ngx_thread_pool_conf_t));
    if (tcf == NULL) {
        return NULL;
    }

    if (ngx_array_init(&tcf->pools, cycle->pool, 4,
                       sizeof(ngx_thread_pool_t *))
        != NGX_OK)
    {
        return NULL;
    }

    return tcf;
}


static char *
ngx_thread_pool_init_conf(ngx_cycle_t *cycle, void *conf)
{
    ngx_thread_pool_conf_t *tcf = conf;

    ngx_uint_t           i;
    ngx_thread_pool_t  **tpp;

    tpp = tcf->pools.elts;

    for (i = 0; i < tcf->pools.nelts; i++) {

        if (tpp[i]->threads) {
            continue;
        }

        if (tpp[i]->name.len == ngx_thread_pool_default.len
            && ngx_strncmp(tpp[i]->name.data, ngx_thread_pool_default.data,
                           ngx_thread_pool_default.len)
               == 0)
        {
            tpp[i]->threads = 32;
            tpp[i]->max_queue = 65536;
            continue;
        }

        ngx_log_error(NGX_LOG_EMERG, cycle->log, 0,
                      "unknown thread pool \"%V\" in %s:%ui",
                      &tpp[i]->name, tpp[i]->file, tpp[i]->line);

        return NGX_CONF_ERROR;
    }

    return NGX_CONF_OK;
}


static char *
ngx_thread_pool(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
{
    ngx_str_t          *value;
    ngx_uint_t          i;
    ngx_thread_pool_t  *tp;

    value = cf->args->elts;

    tp = ngx_thread_pool_add(cf, &value[1]);

    if (tp == NULL) {
        return NGX_CONF_ERROR;
    }

    if (tp->threads) {
        ngx_conf_log_error(NGX_LOG_EMERG, cf, 0,
                           "duplicate thread pool \"%V\"", &tp->name);
        return NGX_CONF_ERROR;
    }

    tp->max_queue = 65536;

    for (i = 2; i < cf->args->nelts; i++) {

        if (ngx_strncmp(value[i].data, "threads=", 8) == 0) {

            tp->threads = ngx_atoi(value[i].data + 8, value[i].len - 8);

            if (tp->threads == (ngx_uint_t) NGX_ERROR || tp->threads == 0) {
                ngx_conf_log_error(NGX_LOG_EMERG, cf, 0,
                                   "invalid threads value \"%V\"", &value[i]);
                return NGX_CONF_ERROR;
            }

            continue;
        }

        if (ngx_strncmp(value[i].data, "max_queue=", 10) == 0) {

            tp->max_queue = ngx_atoi(value[i].data + 10, value[i].len - 10);

            if (tp->max_queue == NGX_ERROR) {
                ngx_conf_log_error(NGX_LOG_EMERG, cf, 0,
                                   "invalid max_queue value \"%V\"", &value[i]);
                return NGX_CONF_ERROR;
            }

            continue;
        }
    }

    if (tp->threads == 0) {
        ngx_conf_log_error(NGX_LOG_EMERG, cf, 0,
                           "\"%V\" must have \"threads\" parameter",
                           &cmd->name);
        return NGX_CONF_ERROR;
    }

    return NGX_CONF_OK;
}


ngx_thread_pool_t *
ngx_thread_pool_add(ngx_conf_t *cf, ngx_str_t *name)
{
    ngx_thread_pool_t       *tp, **tpp;
    ngx_thread_pool_conf_t  *tcf;

    if (name == NULL) {
        name = &ngx_thread_pool_default;
    }

    tp = ngx_thread_pool_get(cf->cycle, name);

    if (tp) {
        return tp;
    }

    tp = ngx_pcalloc(cf->pool, sizeof(ngx_thread_pool_t));
    if (tp == NULL) {
        return NULL;
    }

    tp->name = *name;
    tp->file = cf->conf_file->file.name.data;
    tp->line = cf->conf_file->line;

    tcf = (ngx_thread_pool_conf_t *) ngx_get_conf(cf->cycle->conf_ctx,
                                                  ngx_thread_pool_module);

    tpp = ngx_array_push(&tcf->pools);
    if (tpp == NULL) {
        return NULL;
    }

    *tpp = tp;

    return tp;
}


ngx_thread_pool_t *
ngx_thread_pool_get(ngx_cycle_t *cycle, ngx_str_t *name)
{
    ngx_uint_t                i;
    ngx_thread_pool_t       **tpp;
    ngx_thread_pool_conf_t   *tcf;

    tcf = (ngx_thread_pool_conf_t *) ngx_get_conf(cycle->conf_ctx,
                                                  ngx_thread_pool_module);

    tpp = tcf->pools.elts;

    for (i = 0; i < tcf->pools.nelts; i++) {

        if (tpp[i]->name.len == name->len
            && ngx_strncmp(tpp[i]->name.data, name->data, name->len) == 0)
        {
            return tpp[i];
        }
    }

    return NULL;
}


static ngx_int_t
ngx_thread_pool_init_worker(ngx_cycle_t *cycle)
{
    ngx_uint_t                i;
    ngx_thread_pool_t       **tpp;
    ngx_thread_pool_conf_t   *tcf;

    if (ngx_process != NGX_PROCESS_WORKER
        && ngx_process != NGX_PROCESS_SINGLE)
    {
        return NGX_OK;
    }

    tcf = (ngx_thread_pool_conf_t *) ngx_get_conf(cycle->conf_ctx,
                                                  ngx_thread_pool_module);

    if (tcf == NULL) {
        return NGX_OK;
    }

    ngx_thread_pool_queue_init(&ngx_thread_pool_done);

    tpp = tcf->pools.elts;

    for (i = 0; i < tcf->pools.nelts; i++) {
        if (ngx_thread_pool_init(tpp[i], cycle->log, cycle->pool) != NGX_OK) {
            return NGX_ERROR;
        }
    }

    return NGX_OK;
}


static void
ngx_thread_pool_exit_worker(ngx_cycle_t *cycle)
{
    ngx_uint_t                i;
    ngx_thread_pool_t       **tpp;
    ngx_thread_pool_conf_t   *tcf;

    if (ngx_process != NGX_PROCESS_WORKER
        && ngx_process != NGX_PROCESS_SINGLE)
    {
        return;
    }

    tcf = (ngx_thread_pool_conf_t *) ngx_get_conf(cycle->conf_ctx,
                                                  ngx_thread_pool_module);

    if (tcf == NULL) {
        return;
    }

    tpp = tcf->pools.elts;

    for (i = 0; i < tcf->pools.nelts; i++) {
        ngx_thread_pool_destroy(tpp[i]);
    }
}
```

原文地址：https://zhuanlan.zhihu.com/p/519748706

作者：linux

# 【NO.323】Redis6.0多线程模型总结

## **1. Redis6.0之前的版本真的是单线程吗？**

Redis在处理客户端的请求时，包括获取 (socket 读)、解析、执行、内容返回 (socket 写) 等都由一个顺序串行的主线程处理，这就是所谓的“单线程”。但如果严格来讲从Redis4.0之后并不是单线程，除了主线程外，它也有后台线程在处理一些较为缓慢的操作，例如清理脏数据、无用连接的释放、大 key 的删除等等。其中执行命令阶段，由于 Redis 是单线程来处理命令的，所有每一条到达服务端的命令不会立刻执行，所有的命令都会进入一个 Socket 队列中，当 socket 可读则交给单线程事件分发器逐个被执行。如下图所示：

![img](https://pic4.zhimg.com/80/v2-b995dd6422b49e6fb10c522f12085aa3_720w.webp)

## **2. Redis6.0之前为什么一直不使用多线程？**

官方曾做过类似问题的回复：使用Redis时，几乎不存在CPU成为瓶颈的情况， Redis主要受限于内存和网络。例如在一个普通的Linux系统上，Redis通过使用pipelining每秒可以处理100万个请求，所以如果应用程序主要使用O(N)或O(log(N))的命令，它几乎不会占用太多CPU。

使用了单线程后，可维护性高。多线程模型虽然在某些方面表现优异，但是它却引入了程序执行顺序的不确定性，带来了并发读写的一系列问题，增加了系统复杂度、同时可能存在线程切换、甚至加锁解锁、死锁造成的性能损耗。Redis通过AE事件模型以及IO多路复用等技术，处理性能非常高，因此没有必要使用多线程。单线程机制使得 Redis 内部实现的复杂度大大降低，Hash 的惰性 Rehash、Lpush 等等 “线程不安全” 的命令都可以无锁进行。

## **3.Redis6.0为什么要引入多线程呢？**

Redis将所有数据放在内存中，内存的响应时长大约为100纳秒，对于小数据包，Redis服务器可以处理80,000到100,000 QPS，这也是Redis处理的极限了，对于80%的公司来说，单线程的Redis已经足够使用了。

但随着越来越复杂的业务场景，有些公司动不动就上亿的交易量，因此需要更大的QPS。常见的解决方案是在分布式架构中对数据进行分区并采用多个服务器，但该方案有非常大的缺点，例如要管理的Redis服务器太多，维护代价大；某些适用于单个Redis服务器的命令不适用于数据分区；数据分区无法解决热点读/写问题；数据偏斜，重新分配和放大/缩小变得更加复杂等等。

从Redis自身角度来说，因为读写网络的read/write系统调用占用了Redis执行期间大部分CPU时间，瓶颈主要在于网络的 IO 消耗, 优化主要有两个方向:

- 提高网络 IO 性能，典型的实现比如使用 DPDK 来替代内核网络栈的方式
- 使用多线程充分利用多核，典型的实现比如 Memcached。

协议栈优化的这种方式跟 Redis 关系不大，支持多线程是一种最有效最便捷的操作方式。所以总结起来，redis支持多线程主要就是两个原因：

- 可以充分利用服务器 CPU 资源，目前主线程只能利用一个核
- 多线程任务可以分摊 Redis 同步 IO 读写负荷

## **4.Redis6.0默认是否开启了多线程？**

Redis6.0的多线程默认是禁用的，只使用主线程。如需开启需要修改redis.conf配置文件：io-threads-do-reads yes

![img](https://pic3.zhimg.com/80/v2-b04eb451c09fd913bdc91cb77ddb9c9a_720w.webp)

## **5.Redis6.0多线程开启时，线程数如何设置？**

开启多线程后，还需要设置线程数，否则是不生效的。同样修改redis.conf配置文件。关于线程数的设置，官方有一个建议：4 核的机器建议设置为 2 或 3 个线程，8核的建议设置为 6 个线程，线程数一定要小于机器核数。线程数并不是越大越好，官方认为超过了 8 个基本就没什么意义了。

![img](https://pic4.zhimg.com/80/v2-111dd2a9e8ff54c2075bcf9cffc9616b_720w.webp)



## **6.Redis6.0多线程的实现机制？**

(1).流程如下：

1. 主线程获取 socket 放入等待列表
2. 将 socket 分配给各个 IO 线程（并不会等列表满）
3. 主线程阻塞等待 IO 线程（多线程）读取 socket 完毕
4. 主线程执行命令 - 单线程（如果命令没有接收完毕，会等 IO 下次继续）
5. 主线程阻塞等待 IO 线程（多线程）将数据回写 socket 完毕（一次没写完，会等下次再写）
6. 解除绑定，清空等待队列

(2).特点如下：

- IO 线程要么同时在读 socket，要么同时在写，不会同时读或写
- IO 线程只负责读写 socket 解析命令，不负责命令处理（主线程串行执行命令）
- IO 线程数可自行配置

![img](https://pic2.zhimg.com/80/v2-ef87bcc16bc8f85b64d7521d9eae3445_720w.webp)

流程简述如下：

- 1、主线程负责接收建立连接请求，获取 socket 放入全局等待读处理队列
- 2、主线程处理完读事件之后，通过 RR(Round Robin) 将这些连接分配给这些 IO 线程
- 3、主线程阻塞等待 IO 线程读取 socket 完毕
- 4、主线程通过单线程的方式执行请求命令，请求数据读取并解析完成，但并不执行
- 5、主线程阻塞等待 IO 线程将数据回写 socket 完毕
- 6、解除绑定，清空等待队列

![img](https://pic2.zhimg.com/80/v2-694b6077316b20e3f22cceda6a887eed_720w.webp)

该设计有如下特点：

- IO 线程要么同时在读 socket，要么同时在写，不会同时读或写
- IO 线程只负责读写 socket 解析命令，不负责命令处理

## **7.开启多线程后，是否会存在线程并发安全问题？**

从上面的实现机制可以看出，Redis的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程顺序执行。所以我们不需要去考虑控制 key、lua、事务，LPUSH/LPOP 等等的并发及线程安全问题。

## **8.redis单线程模型（6.0之前）**

Redis客户端对服务端的每次调用都经历了发送命令，执行命令，返回结果三个过程。其中执行命令阶段，由于Redis是单线程来处理命令的，所有每一条到达服务端的命令不会立刻执行，所有的命令都会进入一个队列中，然后逐个被执行。并且多个客户端发送的命令的执行顺序是不确定的。但是可以确定的是不会有两条命令被同时执行，不会产生并发问题，这就是Redis的单线程基本模型。

![img](https://pic4.zhimg.com/80/v2-a9771885d162d9b237a2e555d5161663_720w.webp)

PS：可以修改redis的最大链接数，默认为10000，如下图，如果要修改的话，直接修改配置文件重点maxclients即可。

![img](https://pic1.zhimg.com/80/v2-ca1a6dc988fae6d3d505331404166cb0_720w.webp)

### **8.1 什么是非阻塞IO？**

非阻塞 IO 在 Socket 对象上提供了一个选项Non_Blocking ，当这个选项打开时，读写方法不会阻塞，而是能读多少读多少，能写多少写多少。

能读多少取决于内核为 Socket 分配的读缓冲区的大小，能写多少取决于内核为 Socket 分配的写缓冲区的剩余空间大小。读方法和写方法都会通过返回值来告知程序实际读写了多少字节数据。

有了非阻塞 IO 意味着线程在读写 IO 时可以不必再阻塞了，读写可以瞬间完成然后线程可以继续干别的事了。

补充阻塞IO概念：

当我们调用 Scoket 的读写方法，默认它们是阻塞的。

read() 方法要传递进去一个参数 n，表示读取这么多字节后再返回，如果没有读够 n 字节线程就会阻塞，直到新的数据到来或者连接关闭了， read 方法才可以返回，线程才能继续处理。

write() 方法会首先把数据写到系统内核为 Scoket 分配的写缓冲区中，当写缓存区满溢，即写缓存区中的数据还没有写入到磁盘，就有新的数据要写道写缓存区时，write() 方法就会阻塞，直到写缓存区中有空闲空间。

### **8.2. 什么是IO多路复用？**

背景：

非阻塞 IO 有个问题，那就是单个线程要处理多个读写请求，处理某个客户端的的读数据的请求，结果读了一部分就返回了，线程如何知道什么时候才应该继续读数据。处理写请求的时候，如果缓冲区满了，写不完，剩下的数据何时才应该继续写？在什么时候处理什么请求？redis 单线程处理多个IO请求时就用到了IO多路复用技术。

原理：

如下图，redis 需要处理 3 个 IO 请求，同时把 3 个请求的结果返回给客户端，所以总共需要处理 6 个 IO 事件，由于 redis 是单线程模型，同一时间只能处理一个 IO 事件，于是 redis 需要在合适的时间暂停对某个 IO 事件的处理，转而去处理另一个 IO 事件，这样 redis 就好比一个开关，当开关拨到哪个 IO 事件这个电路上，就处理哪个 IO 事件，其他 IO 事件就暂停处理了。这就是IO多路复用技术。

以上是大致的理解下 IO 多路复用技术，在系统底层，IO 多路复用有 3 种实现机制：select、poll、epoll。

![img](https://pic1.zhimg.com/80/v2-12575653e93cf1b9713535ea9e6d608c_720w.webp)

### **8.3 什么是文件处理器？**

- Redis 基于 Reactor 模式开发了自己的网络事件处理器： 这个处理器被称为文件事件处理器（file event handler）
- 文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。
- 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。
- 文件事件处理器以单线程方式运行，但通过使用 I/O 多路复用程序来监听多个套接字，文件事件处理器既实现了高性能的网络通信模型，又可以很好地与 redis 服务器中其他同样以单线程方式运行的模块进行对接， 这保持了 Redis 内部单线程设计的简单性。

**总结与思考**

随着互联网的飞速发展，互联网业务系统所要处理的线上流量越来越大，Redis 的单线程模式会导致系统消耗很多 CPU 时间在网络 I/O 上从而降低吞吐量，要提升 Redis 的性能有两个方向：

- 优化网络 I/O 模块
- 提高机器内存读写的速度

后者依赖于硬件的发展，暂时无解。所以只能从前者下手，网络 I/O 的优化又可以分为两个方向：

- 零拷贝技术或者 DPDK 技术
- 利用多核优势

**模型缺陷**

Redis 的多线程网络模型实际上并不是一个标准的 Multi-Reactors/Master-Workers模型。
Redis 的多线程方案中，I/O 线程任务仅仅是通过 socket 读取客户端请求命令并解析，却没有真正去执行命令。
所有客户端命令最后还需要回到主线程去执行，因此对多核的利用率并不算高，而且每次主线程都必须在分配完任务之后忙轮询等待所有 I/O 线程完成任务之后才能继续执行其他逻辑。
在我看来，Redis 目前的多线程方案更像是一个折中的选择：既保持了原系统的兼容性，又能利用多核提升 I/O 性能。

原文地址：https://zhuanlan.zhihu.com/p/519383521

作者：linux

# 【NO.324】进程的同步、互斥、通信的区别，进程与线程同步的区别

先来看几个问题。

1. 进程同步与互斥的区别？
2. 进程的同步方式有哪些？
3. 进程的通信方式有哪些？
4. 进程同步与通信的区别是什么？
5. 线程的同步/通信与进程的同步/通信有区别吗？

在好多教材上（包括国内与国外的）也没有明确这些概念，现在对每个问题还没有准确的答案，下面将自己的理解记下来，以后再补充。

## 1.进程互斥、同步的概念

进程互斥、同步的概念是并发进程下存在的概念，有了并发进程，就产生了资源的竞争与协作，从而就要通过进程的互斥、同步、通信来解决资源的竞争与协作问题。

下面是根据《操作系统教程》3.1.4 中的介绍，整理的进程互斥、同步的概念。

在多道程序设计系统中，同一时刻可能有许多进程，这些进程之间存在两种基本关系：竞争关系和协作关系。

进程的互斥、同步、通信都是基于这两种基本关系而存在的。

为了解决进程间竞争关系（间接制约关系）而引入进程互斥；

为了解决进程间**松散的协作**关系( **直接制约关系**)而引入进程同步；

为了解决进程间**紧密的协作**关系而引入进程通信。

**第一种是竞争关系**

系统中的多个进程之间彼此无关，它们并不知道其他进程的存在，并且也不受其他进程执行的影响。例如，批处理系统中建立的多个用户进程， 分时系统中建立的多个终端进程。由于这些进程共用了一套计算机系统资源，因而， 必然要出现多个进程竞争资源的问题。当多个进程竞争共享硬设备、存储器、处理器 和文件等资源时，操作系统必须协调好进程对资源的争用。

资源竞争出现了两个控制问题：

一个是死锁 （deadlock ）问题，一组进程如果都获得了部分资源，还想要得到其他进程所占有的资源，最终所有的进程将陷入死锁。

另一个是饥饿（starvation ）问题，这是指这样一种情况：一个进程由于其他进程总是优先于它而被无限期拖延。

操作系统需要保证诸进程能互斥地访问临界资源，既要解决饥饿问题，又要解决死锁问题。
进程的互斥（mutual exclusion ）是解决进程间竞争关系( **间接制约关系**) 的手段。 进程互斥指若干个进程要使用同一共享资源时，任何时刻最多允许一个进程去使用，其他要使用该资源的进程必须等待，直到占有资源的进程释放该资源。

**第二种是协作关系**

某些进程为完成同一任务需要分工协作，由于合作的每一个进程都是独立地以不可预知的速度推进，这就需要相互协作的进程在某些协调点上协 调各自的工作。当合作进程中的一个到达协调点后，在尚未得到其伙伴进程发来的消息或信号之前应阻塞自己，直到其他合作进程发来协调信号或消息后方被唤醒并继续执行。这种协作进程之间相互等待对方消息或信号的协调关系称为进程同步。

进程间的协作可以是双方不知道对方名字的间接协作，例如，通过共享访问一个[缓冲区](https://www.zhihu.com/search?q=缓冲区&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"494474629"})进行松散式协作；也可以是双方知道对方名字，直接通过通信机制进行紧密协作。允许进程协同工作有利于共享信息、有利于加快计算速度、有利于实现模块化程序设计。

进程的同步（Synchronization）是解决进程间协作关系( **直接制约关系**) 的手段。

进程同步指两个以上进程基于某个条件来协调它们的活动。一个进程的执行依赖于另一个协作进程的消息或信号，当一个进程没有得到来自于另一个进程的消息或信号时则需等待，直到消息或信号到达才被唤醒。

不难看出，进程互斥关系是一种特殊的进程同步关系，即逐次使用互斥共享资源，也是对进程使用资源次序上的一种协调。

## 2.进程通信的概念

并发进程之间的交互必须满足两个基本要求：同步和通信。

进程竞争资源时要实施互斥，互斥是一种特殊的同步，实质上需要解决好进程同步问题，进程同步是一种进程通信，通过修改信号量，进程之间可建立起联系，相互协调运行和协同工作。但是[信号量](https://www.zhihu.com/search?q=信号量&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"494474629"})与PV操作只能传递信号，没有传递数据的能力。有些情况下进程之间交换的信息量虽很少，例如，仅仅交换某个状态信息，但很多情况下进程之间需要交换大批数据，例如，传送一批信息或整个文件，这可以通过一种新的通信机制来完成，**进程之间互相交换信息的工作称之为进程通信IPC （InterProcess Communication）（主要是指大量数据的交换）。**

**进程间通信的方式很多，包括：**

1 mmap（文件映射）

2 信号

3 管道

4 共享内存

5 消息队列（重要）

6 信号量集（与signal无关）

7 网络（套接字）

## 3.进程同步的方法

前面提到，进程互斥关系是一种特殊的进程同步关系，下面给出常见的进程同步的方法，实际上也可用于进程的互斥（个人理解）。

Linux 下常见的进程同步方法有：

1、信号量

2、管程

3、 互斥量（基于共享内存的快速用户态 ）

4、文件锁（通过 fcntl 设定，针对文件）

针对线程（pthread）的还有 pthread_mutex 和 pthread_cond（条件变量）。

**线程的同步方法：**

1、信号量

2、互斥量

3、临界区

4、事件

同步机制：

四种进程或线程同步互斥的控制方法
1、临界区:通过对多线程的[串行化](https://www.zhihu.com/search?q=串行化&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"494474629"})来访问公共资源或一段代码，速度快，适合控制数据访问。
2、互斥量:为协调共同对一个共享资源的单独访问而设计的。
3、信号量:为控制一个具有有限数量用户资源而设计。
4、事 件:用来通知线程有一些事件已发生，从而启动后继任务的开始。

## 4.**科普**

**1.临界资源**

临界资源是一次仅允许一个进程使用的共享资源。*各进程采取互斥的方式，实现共享的资源称作临界资源。*属于临界资源的硬件有，打印机，[磁带机](https://www.zhihu.com/search?q=磁带机&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"494474629"})等；软件有消息队列，变量，数组，缓冲区等。诸进程间采取互斥方式，实现对这种资源的共享。

***2.临界区：***

*每个进程中访问临界资源的那段代码称为临界区（*criticalsection*），每次只允许一个进程进入临界区，进入后，不允许其他进程进入。不论是硬件临界资源还是软件临界资源，多个进程必须互斥的对它进行访问。多个进程涉及到同一个临界资源的的临界区称为相关临界区。*使用临界区时，一般不允许其运行时间过长，只要运行在临界区的线程还没有离开，其他所有进入此临界区的线程都会被挂起而进入等待状态，并在一定程度上影响程序的运行性能。

临界区是一种轻量级的同步机制，与互斥和事件这些内核同步对象相比，临界区是用户态下的对象，即只能在同一进程中实现线程互斥。因无需在用户态和核心态之间切换，所以工作效率比较互斥来说要高很多。虽然临界区同步速度很快，但却只能用来同步本 进程内的线程，而不可用来同步多个进程中的线程。


**临界区（Critical Section）**


保证在某一时刻只有一个线程能访问数据的简便办法。在任意时刻只允许一个线程对共享资源进行访问。如果有多个线程试图同时访问临界区，那么 在有一个线程进入后其他所有试图访问此临界区的线程将被挂起，并一直持续到进入临界区的线程离开。临界区在被释放后，其他线程可以继续抢占，并以此达到用原子方式操 作共享资源的目的。
临界区包含两个操作原语：
EnterCriticalSection（） 进入临界区
LeaveCriticalSection（） 离开临界区
EnterCriticalSection（） 语句执行后代码将进入临界区以后无论发生什么，必须确保与之匹配的 LeaveCriticalSection（）都能够被执行到。否则临界区保护的共享资源将永远不会被释放。虽然临界区同步速度很快，但却只能用来同步本 进程内的线程，而不可用来同步多个进程中的线程。
MFC提供了很多功能完备的类，我用MFC实现了临界区。MFC为临界区提供有一个 CCriticalSection类，使用该类进行线程同步处理是 非常简单的。只需在线程函数中用CCriticalSection类成员函数Lock（）和UnLock（）标定出被保护代码片段即可。Lock（）后代 码用到的资源自动被视为临界区内的资源被保护。UnLock后别的线程才能访问这些资源。

**互斥量（Mutex）**

互斥量跟临界区很相似，只有拥有互斥对象的线程才具有访问资源的权限，由于互斥对象只有一个，因此就决定了任何情况下此共享资源都不会同时被多个线程所访问。当前占据资源的线程在任务处理完后应将拥有的互斥对象交出，以便其他线程在获得后得以访问资源。互斥量比临界区复杂。因为使用互斥不仅仅能够在同一应用程序不同线程中实现资源的安全共享，而且可以在不同应用程序的线程之间实现对资源的安全共享。

互斥量包含的几个操作原语：
CreateMutex（） 创建一个互斥量
OpenMutex（） 打开一个互斥量
ReleaseMutex（） 释放互斥量
WaitForMultipleObjects（） 等待互斥量对象

**信号量（Semaphores）**

信号量对象对线程的同步方式与前面几种方法不同，信号允许多个线程同时使用共享资源 ，这与操作系统中的PV操作相同。它指出了同时访问共享 资源的线程 最大数目。它允许多个线程在同一时刻访问同一资源，但是需要限制在同一时刻访问此资源的最大线程数目。在用CreateSemaphore（）创建信号量 时即要同时指出允许的最大资源计数和当前可用资源计数。一般是将当前可用资源计数设置为最大资源计数，每增加一个线程对共享资源的访问，当前可用资源计数 就会减1，只要当前可用资源计数是大于0的，就可以发出信号量信号。但是当前可用计数减小到0时则说明当前占用资源的线程数已经达到了所允许的最大数目， 不能在允许其他线程的进入，此时的信号量信号将无法发出。线程在处理完共享资源后，应在离开的同时通过ReleaseSemaphore（）函数将当前可 用资源计数加1。在任何时候当前可用资源计数决不可能大于最大资源计数。

PV操作及信号量的概念都是由荷兰科学家E.W.Dijkstra提出的。信号量S是一个整数，S大于等于零时代表可供并发进程使用的资源实体数，但S小于零时则表示正在等待使用共享资源的进程数。
P操作 申请资源：
　　（1）S减1；
　　（2）若S减1后仍大于等于零，则进程继续执行；
　　（3）若S减1后小于零，则该进程被阻塞后进入与该信号相对应的队列中，然后转入进程调度。

V操作 释放资源：
　　（1）S加1；
　　（2）若相加结果大于零，则进程继续执行；
　　（3）若相加结果小于等于零，则从该信号的[等待队列](https://www.zhihu.com/search?q=等待队列&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"494474629"})中唤醒一个等待进程，然后再返回原进程继续执行或转入进程调度。

　　信号量包含的几个操作原语：
　　CreateSemaphore（） 创建一个信号量
　　OpenSemaphore（） 打开一个信号量
　　ReleaseSemaphore（） 释放信号量
　　WaitForSingleObject（） 等待信号量

**事件（Event）**

事件对象也可以通过通知操作的方式来保持线程的同步。并且可以实现不同进程中的线程同步操作。
信号量包含的几个操作原语：

　　CreateEvent（） 创建一个事件
　　OpenEvent（） 打开一个事件
　　SetEvent（） 回置事件
　　WaitForSingleObject（） 等待一个事件
　　WaitForMultipleObjects（）　　　　　　　　 等待多个事件
　　　　WaitForMultipleObjects 函数原型：
　　　　　WaitForMultipleObjects（
　　　　　IN DWORD nCount, // 等待句柄数
　　　　　IN CONST HANDLE *lpHandles, //指向句柄数组
　　　　　IN BOOL bWaitAll, //是否完全等待标志
　　　　　IN DWORD dwMilliseconds //等待时间
　　　　　）

参 数nCount指定了要等待的内核对象的数目，存放这些内核对象的数组由lpHandles来指向。fWaitAll对指定的这nCount个内核对象的两种等待方式进行了指定，为TRUE时当所有对象都被通知时函数才会返回，为FALSE则只要其中任何一个得到通知就可以返回。 dwMilliseconds在这里的作用与在WaitForSingleObject（）中的作用是完全一致的。如果等待超时，函数将返回 WAIT_TIMEOUT。

总结：

1． 互斥量与临界区的作用非常相似，但互斥量是可以命名的，也就是说它可以跨越进程使用。所以创建互斥量需要的资源更多，所以如果只为了在进程内部是用的话使用临界区会带来速度上的优势并能够减少资源占用量 。因为互斥量是跨进程的互斥量一旦被创建，就可以通过名字打开它。

2． 互斥量（Mutex），信号灯（Semaphore），事件（Event）都可以被跨越进程使用来进行同步数据操作，而其他的对象与数据同步操作无关，但对于进程和线程来讲，如果进程和线程在运行状态则为无信号状态，在退出后为有信号状态。所以可以使用WaitForSingleObject来等待进程和 线程退出。

3． 通过互斥量可以指定资源被独占的方式使用，但如果有下面一种情况通过互斥量就无法处理，比如现在一位用户购买了一份三个并发访问许可的数据库系统，可以根据用户购买的访问许可数量来决定有多少个线程/进程能同时进行数据库操作，这时候如果利用互斥量就没有办法完成这个要求，信号灯对象可以说是一种资源计数器。

Win32 中关于进程和线程的协调工作是由同步机制来完成的，同步机制相当于线程间的红绿灯。

**一. 同步和异步**

举个例子：

PostMessage()，是把消息放到对方的消息队列中，然后不管三七二十一，就回到原调用点继续执行，这就是异步。

SendMessage()，就像调用一般性函数，直到调用的函数结束，才会回到原点，这就是同步行为。

**二.临界区**

如果一个线程已经进入某个临界区，则另一个线程就绝不能够进入同一个临界区。

```text
//初始化一个临界区 
VOID InitializeCriticalSection(  
  LPCRITICAL_SECTION lpCriticalSection  // critical section 
);  
 
//消除一个临界区 
VOID DeleteCriticalSection(  
  LPCRITICAL_SECTION lpCriticalSection   // critical section 
);  
 
//进入临界区 
VOID EnterCriticalSection(  
  LPCRITICAL_SECTION lpCriticalSection  // critical section 
);  
 
//离开临界区 
VOID LeaveCriticalSection(  
  LPCRITICAL_SECTION lpCriticalSection   // critical section 
);  
 
例如：  
CRITICAL_SECTION gCriticalSection;  
 
void Function()  
{  
    InitializeCriticalSection(&gCriticalSection);  
 
    EnterCriticalSection(&gCriticalSection);  
 
 //Do something here 
 
    LeaveCriticalSection(&gCriticalSection);  
 
    DeleteCriticalSection(&gCriticalSection);  
}
```

一旦线程进入一个临界区，则它就可以一再的重复进入该临界区，当然每个进入操作都必须对应离开操作。

也就是EnterCriticalSection( )，可以嵌套。

但是千万不要在临界区中调用 sleep()，或任何 Wait..() 函数。

临界区的缺点是：没有办法知道进入临界区中的那个线程是生是死。如果那个线程在进入临界区后当掉了，而且没有退出来，那么系统就没有办法消除掉此临界区。

**三. 互斥量**

Mutexes 用途和 Critical Section 非常类似，线程拥有 mutex 就好象线程进入 critical section 一样，但是它牺牲速度以增加弹性。

一旦没有任何线程拥有那个 mutex，这个 mutex 便处于激发状态

它与临界区的区别是：

\1. Mutexes 操作要比 Critical Section 费时的多。

\2. Mutexes 可以跨进程使用，Critical Section 则只能在同一进程中使用。

\3. 等待一个 Mutex 时，你可以指定"结束等待"的时间长度，而 Critical Section 则不行。

```text
HANDLE CreateMutex(  
  LPSECURITY_ATTRIBUTES lpMutexAttributes,  // 安全属性，默认为NULL 
 BOOL bInitialOwner,                       // initial owner 
 LPCTSTR lpName                            // mutex 的名称，是一个字符串 
);  
//返回值：如果成功返回 handle，否则返回 NULL 
 
HANDLE OpenMutex(  
 DWORD dwDesiredAccess,  // access 
 BOOL bInheritHandle,    // inheritance option 
 LPCTSTR lpName          // object name 
);  
//打开一个已经存在的 mutex 
 
BOOL ReleaseMutex(  
 HANDLE hMutex   // handle to mutex 
);  
 
//调用过程如下： 
CreateMutex(); //创建 
WaitForXXXObject(); //等待 
 
ReleaseMutex(); //释放 
CloseHandle(); //关闭 
```

说明：

\1. Mutex 的拥有权：

Mutex 的拥有权并非属于那个产生它的线程，而是那个最后对些 Mutex 进行 WaitXXX() 操作并且尚未进行 ReleaseMutex() 操作的线程。

\2. Mutex 被舍弃：

如果线程在结束前没有调用 ReleaseMutex()，比如线程调用了 EXitThread() 或者因为当掉而结束。这时的 mutex 不会被摧毁，而是被视为"未被拥有"以及"未被激发"的状态，在下一个 WaitXXX() 中线程会被以WAIT_ABANDONED_0 （WAIT_ABANDONED_0_n + 1 ）来通知。

\3. 最初拥有者：

CreateMutex()，第二个参数 bInitialOwner，允许你指定现行线程是否立刻拥有产生出来的 mutex。

如果没有指定立刻拥有的情况：

```text
HANDLE hMutex = CreateMutex(NULL, FALSE, "Sample Name");  
 
int result = WaitForSingleObject(hMutex, INFINITE);
```

可能发生，在 CreateMutex 完成之后，发生了 context switch，[执行权](https://www.zhihu.com/search?q=执行权&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"494474629"})切换到另一个线程，那么其它进程就有可能在 mutex 的产生者调用 WaitForSingleObject( ) 之前，锁住这个 mutex 对象。

通信机制：

管道、FIFO、消息队列、信号量、共享内存是进程的通信机制，教材上没有线程的通信机制这样的说法，但可以肯定这几种方法是进程的通信方式，且其中的信号量既可用于进程、线程的同步，又可用于进程的通信

管道与[管程](https://www.zhihu.com/search?q=管程&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"494474629"})是不同的，管程是进程同步的方式，而管道则是进程通信的方式。

进程通信也就是所谓的IPC问题，主要是指进程间交换数据的方式。进程通信包括高级通信与低级通信，其中进程同步与互斥属于低级通信，主要用于插U农地控制信号；高级通信包括三种：共享存储系统（有的地方称作共享内存区）、消息传递系统（有的地方称作消息队列）、管道。

信号量是进程同步与互斥的常用方法，也可以作为低级的进程通信方法，用于传递控制信号。

简而言之，进程间通信方式主要包括**管道、FIFO、消息队列、信号量、共享内存。**

1.管道，还有命名管道和非命名管道(即匿名管道)之分，非命名管道(即匿名管道)只能用于父子进程通讯，命名管道可用于非父子进程，命名管道就是FIFO，管道是先进先出的通讯方式

2.消息队列，是用于两个进程之间的通讯，首先在一个进程中创建一个消息队列，然后再往消息队列中写数据，而另一个进程则从那个消息队列中取数据。需要注意的是，消息队列是用创建文件的方式建立的，如果一个进程向某个消息队列中写入了数据之后，另一个进程并没有取出数据，即使向消息队列中写数据的进程已经结束，保存在消息队列中的数据并没有消失，也就是说下次再从这个消息队列读数据的时候，就是上次的数据！！！！

3.信号量，它与WINDOWS下的信号量是一样的，所以就不用多说了

4.共享内存，类似于WINDOWS下的DLL中的共享变量，但LINUX下的共享内存区不需要像DLL这样的东西，只要首先创建一个共享内存区，其它进程按照一定的步骤就能访问到这个共享内存区中的数据，当然可读可写

以上几种方式的比较：

1.管道：速度慢，容量有限，只有父子进程能通讯
2.FIFO：任何进程间都能通讯，但速度慢
3.消息队列：容量受到系统限制，**且要注意第一次读的时候，要考虑上一次没有读完数据的问题**
4.信号量：不能传递复杂消息，只能用来同步
5.共享内存区：能够很容易控制容量，速度快，但要保持同步，比如一个进程在写的时候，另一个进程要注意读写的问题，相当于线程中的线程安全，当然，共享内存区同样可以用作线程间通讯，不过没这个必要，线程间本来就已经共享了同一进程内的一块内存

　 本质上，信号量是一个计数器，它用来记录对某个资源（如共享内存）的存取状况。一般说来，为了获得共享资源，进程需要执行下列操作：

　　（1）测试控制该资源的信号量；

　　（2）若此信号量的值为正，则允许进行使用该资源，进程将进号量减1；

　　（3）若此信号量为0，则该资源目前不可用，进程进入睡眠状态，直至信号量值大于0，进程被唤醒，转入步骤（1）；

　　（4）当进程不再使用一个信号量控制的资源时，信号量值加1，如果此时有进程正在睡眠等待此信号量，则唤醒此进程。
套接字通信并不为Linux所专有，在所有提供了TCP/IP协议栈的操作系统中几乎都提供了socket，而所有这样操作系统，对套接字的编程方法几乎是完全一样的

三、进程/线程**同步机制与进程间通信机制比较**

很明显2者有类似，但是差别很大

同步主要是临界区、互斥、信号量、事件

进程间通信是管道、内存共享、消息队列、信号量、socket

共通之处是，信号量和消息（事件）

## 5.小结：

1. 进程互斥、同步与通信的关系：进程竞争资源时要实施互斥，互斥是一种特殊的同步，实质上需要解决好进程同步问题，进程同步是一种进程通信，由此看来，进程互斥、同步都可以看做进程的通信；
2. 信号量是进程同步与互斥的常用方法，也可以作为低级的进程通信方法，用于传递控制信号；
3. 管道与管程是不同的，管程是进程同步的方式，而管道则是进程通信的方式；

原文地址：https://zhuanlan.zhihu.com/p/518838485

作者：linux

# 【NO.325】通过Redis学习事件驱动设计

## 1.**为什么C程序员都要阅读Redis源码**

主要原因就是『简洁』。如果你用源码编译过Redis，你会发现十分轻快，一步到位。其他语言的开发者可能不会了解这种痛，作为C/C++程序员，如果你源码编译安装过Nginx/Grpc/Thrift/Boost等开源产品，你会发现有很多依赖，而依赖也有自己的依赖，十分苦恼。通常半天一天就耗进去了。由衷地羡慕 npm/maven/pip/composer/...这些包管理器。而Redis则给人惊喜，一行make了此残生。

除了安装过程简洁，代码也十分简洁。使用纯C语言编写，每个模块功能都划分的很清晰。

废话不多说，本文要介绍的是Redis里的事件处理功能，与Memcache引入libevent这一臃肿的事件库不同，Redis自己实现了一个小型轻量的事件驱动库——AE。阅读它的源码是一次非常好的学习体验。

## 2.**干净整齐的跨平台兼容**

文件名说明ae.h/ae.c主要文件，根据OS平台的不同依赖以下不同文件：

| 文件        | 平台         |
| ----------- | ------------ |
| ae_epoll.c  | Linux平台    |
| ae_kqueue.c | BSD平台      |
| ae_evport.c | Solaris平台  |
| ae_select.c | 其他Unix平台 |

虽然源码文件看起来不少，但是实际上ae_epoll.c、 ae_kqueue.c、 ae_evport.c、 ae_select.c 这4个文件的功能是完全一样的，提供一致的API接口，给ae.c文件调用。这是由于实现高性能的事件驱动的API（称之为**polling API**）不存在ANSI或POSIX的标准，不同的OS内核有着自己的实现。比如Linux内核的epoll，BSD内核中的kqueue。

ae.c中有：

```text
#ifdef HAVE_EVPORT
#include "ae_evport.c"
#else
    #ifdef HAVE_EPOLL
    #include "ae_epoll.c"
    #else
        #ifdef HAVE_KQUEUE
        #include "ae_kqueue.c"
        #else
        #include "ae_select.c"
        #endif
    #endif
#endif
```

请注意这里include的都是源文件，而非头文件。为什么这样？大家可以自己思考一下。

这些HAVE的宏，都是由在config.h中定义的。依据不同的操作系统，引入这4个文件中的某一个。从功能上来说，这样设计的目的与GOF设计模式中“适配器模式”（修改成一致接口）或“外观模式”（抽象复杂接口为简单接口）的思想类似，但实际上我个人感觉更类似于《POSA》（卷二）中提到的“包装门面模式”（Wrapper Facade）。Anyway，这个编程思想值得学习。

## 3.**aeEventLoop:用C++去设计，用C编码**

十几年前，以Linux之父炮轰C++为开端，社区内展开了一场C与C++孰是孰非的论战。而在国内，以原CSDN总编刘江援引此文为始，把战火烧到了国内。孟岩、云风、pongba几位大佬都身陷其中。后来以孟岩的一句『用C设计，用C++编码』在国内为这场论战定下基调。

反观Redis，他是纯C编码，但是融入了面向对象的思想。和上述观点截然相反，可谓是『用C++去设计，用C编码』。当然本文目的并非挑起语言之争，各种语言自有其利弊，开源项目的语言选择也主要是由于项目作者的个人经历和主观意愿。

定义在ae.h中的结构体 aeEventLoop 是AE库中最核心的数据结构，并且它采用了面向对象的设计思想：ae.h 中声明了多个函数，其第一个参数都是一个aeEventLoop指针，用于操纵aeEventLoop结构体。
从这个角度来说，可以将该结构体理解为面向对象语言中的类，而操纵它的函数则可以视为其成员函数。（其实C++的class编译之后大概也是类似的模式）

| 函数                 | 说明                                   |
| -------------------- | -------------------------------------- |
| aeCreateEventLoop    | 初始化一个事件循环结构体（eventLoop)   |
| aeGetSetSize         | 返回当前setsize的值                    |
| aeResizeSetSize      | 改变setsize的值（重新分配空间）        |
| aeDeleteEventLoop    | 删除事件循环，释放内存空间             |
| aeStop               | 停止事件循环，即stop值设为1            |
| aeProcessEvents      | ae核心：事件处理逻辑                   |
| aeMain               | 启动事件循环，事件循环的入口           |
| aeSetBeforeSleepProc | 注册回调函数，每次主循环在休眠前被调用 |

**aeCreateEventLoop** 和 **aeDeleteEventLoop** 可以视为“类”**aeEventLoop**的构造和析构函数，其他为成员函数。

在程序中调用AE库的时候，一般是依次调用：

1. aeCreateEventLoop
2. 给EventLoop注册文件事件or时间事件
3. aeSetBeforeSleepProc
4. aeMain
5. aeDeleteEventLoop

**AE的两种事件**

事件处理，是有别于多线程/多进程的并发模型。我也都知道Redis是单线程的。它的性能主要依靠异步事件处理功能来实现。虽然事件处理通常和网络编程混作一谈，但其实事件处理本身不一定是为网络编程服务的，它主要是服务于IO，网络通信是IO，文件读写同样是。当然Unix中万物皆文件了，socket也是一种fd。

AE支持两种事件：

1. 文件事件（IO）
2. 时间事件（毫秒级）

这两种事件都作为aeEventLoop的结构体成员存在。

aeEventLoop各成员说明:

```text
typedef struct aeEventLoop {
    int maxfd;                      /* 当前注册的最大fd */
    int setsize;                    /* 监视的fd的最大数量 */
    long long timeEventNextId;      /* 下一个时间事件的ID */
    time_t lastTime;                /* 上次时间事件处理时间 */
    aeFileEvent *events;            /* 已注册文件事件数组 */
    aeFiredEvent *fired;            /* 就绪的文件事件数组 */
    aeTimeEvent *timeEventHead;     /* 时间事件链表的头 */
    int stop;                       /* 是否停止（0：否；1：是）*/
    void *apidata;                  /* 各平台polling API所需的特定数据 */
    aeBeforeSleepProc *beforesleep; /* 事件循环休眠开始的处理函数 */
    aeBeforeSleepProc *aftersleep;  /* 事件循环休眠结束的处理函数 */
} aeEventLoop;
```

文件事件，主要依靠两个数组。一个是注册的文件事件数组，一个是已就绪的文件事件数组。

```text
typedef struct aeFileEvent {
    int mask; /* one of AE_(READABLE|WRITABLE|BARRIER) */
    aeFileProc *rfileProc;
    aeFileProc *wfileProc;
    void *clientData;
} aeFileEvent;
```



```text
typedef struct aeFiredEvent {
    int fd;
    int mask;
} aeFiredEvent;
```

## 4.**单词Fired有通知的意思，在这里你可以理解为“就绪”**

每个文件事件，其读写设置了不同的处理函数。另外mask表示事件的触发类型。当每次polling API返回就绪之后（比如epoll_wait返回），就绪会被设置到**aeFireEvent**，然后反查**aeFileEvent**获得处理函数并处理。你会发现aeFileEvent结构体里并没有记录fd。其实这是使用了HASH策略，aeEventLoop的成员 aeFileEvent数组的下标即是fd，便于快速查找。

时间事件，本质就是定时器任务，其数据结构采用一个双向链表。链表每个结点为**aeTimeEvent**结构体，主要包含事件的ID（递增）、就绪的时间，处理函数、清理函数、客户数据。

```text
typedef struct aeTimeEvent {
    long long id; /* time event identifier. */
    long when_sec; /* seconds */
    long when_ms; /* milliseconds */
    aeTimeProc *timeProc;
    aeEventFinalizerProc *finalizerProc;
    void *clientData;
    struct aeTimeEvent *prev;
    struct aeTimeEvent *next;
} aeTimeEvent;
```

每个事件循环中，每个时间事件的ID唯一且递增，主要依赖aeEventLoop里的**timeEventNextId**来维护这个ID的递增关系。创建新的时间事件时（**aeCreateTimeEvent**）会赋值，由于只考虑了单线程，所以没有加锁逻辑，大家也不要贸然把AE用在多线程环境中。

when_sec和 when_ms 记录了时间事件的就绪时间（秒+毫秒），即当当前时间大于等于这个时间的时候，该时间事件应被处理。

时间事件的处理过程（**processTimeEvents**）主要就是：继续遍历链表，如果发现节点状态为AE_DELETED_EVENT_ID则删除该节点。如果判断当前时间已经超过节点的就绪时间就开始处理。处理函数的返回值可以指定，后续不再处理该事件（NOMORE），则该节点会被置为AE_DELETED_EVENT_ID。如果下次还需要处理，则更新该节点的时间为下次就绪时间。

## 5.**事件循环的处理逻辑**

再用一张图，回顾一下EventLoop中的两种事件，基本可以做如下理解。一个链表，一个数组。文件事件中的数组不是线性填满的，因为是采用的HASH策略，将fd作为数组下标了。

![img](https://pic4.zhimg.com/80/v2-a2e2ae526b9439fe288b8283ef6810c7_720w.webp)

aeProcessEvents 是aeEventLoop在循环过程中的的实际处理逻辑。函数原型如下：

```text
int aeProcessEvents(aeEventLoop *eventLoop, int flags);
```

flags标记，表示本次需要处理的事件类型标记和是否阻塞标记。

| 标记位         | 含义                     |
| -------------- | ------------------------ |
| AE_TIME_EVENTS | 时间事件标记             |
| AE_FILE_EVENTS | 文件事件标记             |
| AE_DONT_WAIT   | 立即返回不阻塞等待的标记 |

aeProcessEvents 具体实现代码我就不贴了。它巧妙的地方是一次柔和了文件和时间事件的两种处理过程。在函数之初，会查找时间事件的链表，找到最近就绪时间事件，然后用它的就绪时间减去当前时间的时间差作为 polling API 的休眠时间（epoll_wait的timeout参数）。然后休眠等待polling api返回。在返回之后先执行aftersleep的的处理逻辑，然后执行这段休眠时间内就绪的文件事件，最后再处理就绪的时间事件。返回值是处理过的事件总数。

也就说AE会尽量在一次处理过程中，将时间事件和文件事件一次性处理。你也许会问如果没有时间事件怎么办。当然没关系，在aeProcessEvents开始部分就根据标记位进行了判断。上面的逻辑是在文件事件和时间事件都存在的情况下，如果仅存在文件事件，则看是否设置了不阻塞的标记（AE_DONT_WAIT），若有，则polling 的超时时间设置为0。如无，即可以阻塞，则设置为-1，则polling API会阻塞直到有文件事件发生。

## 6.**ae_epoll:Linux上epoll的封装**

前文说道Redis适配各种Unix-like的操作系统。它将内核强相关的事件API（polling API）部分单独抽出来，包装出了相同的接口给AE的对外API调用。在Linux系统上的API实现为：ae_epoll.c，建议在阅读这个文件源码之前先好好回顾一下epoll的API，这样更助于快速理解。相信工作后大家写业务逻辑，应该很少接触epoll了。可以阅读这个wik，快速回顾epoll的api：LinuxAPI：epoll

ae_epoll.c 完全被 ae.c调用。各函数调用关系如下(aeApi开头的都是ae_epoll.c中的函数)：
ae.c

- aeCreateEventLoop

- - aeApiCreate

- aeResizeSetSize

- - aeApiResize

- aeDeleteEventLoop

- - aeApiFree

- aeCreateFileEvent

- - aeApiAddEvent

- aeDeleteFileEvent

- - aeApiDelEvent

- aeProcessEvent

- - aeApiPoll

- aeGetApiName

- - aeApiName

除了**aeApiName**()以外，其他函数第一个参数也都是aeEventLoop * 。用面向对象的思想来看这也是aeEventLoop的成员函数。试想若是C++，则可能会被处理成父子两个类，而aeApi系列的函数是纯虚的。
aeApi的函数也是可以做到顾名即可思义。比如：

- aeApiCreate在堆上进行内存的分配，封装epoll_create创建epfd，并写入aeEventLoop。
- aeApiAddEvent、aeApiDelEvent是封装的epoll_ctl来对aeEventLoop的监控的epoll事件进行添加和删除。
- aeApiPoll是封装的epoll_wait开启事件循环，并且每次取出就绪的fd存入aeEventLoop的fired数组中，并置位相应的mask（读or写）
- aeApiResize、aeApiFree分别进行的是内存的重分配、资源的清理（关闭epfd，free内存）和epoll本身关联不大。

## 7.**Jim：AE的缘起**

阅读完AE代码，可能只需要一下午的时间，你会惊叹于作者的设计功力。其实里面也没有太多花哨的东西，但就是如此简洁清晰的给你呈现了一个完成度如此之高的事件驱动处理库。但我想即使大家都熟悉epoll、熟悉kqueue、熟悉数据结构也不一定能设计出来AE，所以把程序员比作代码的设计师、建筑师是丝毫不为过的。

AE的设计灵感其实也是受另外一个开源项目影响，它就是 Jim。

Redis的ae.c的开篇注释中就已注明：

```text
A simple event-driven programming library. 
Originally I wrote this code for the Jim's event-loop (Jim is a Tcl interpreter)
but later translated it in form of a library for easy reuse.
 
```

Redis作者说他最初是为了Jim项目的事件驱动功能编写了一套代码，后来将其改造成了一个易于复用的库。

所以AE其实可以脱离Redis而存在。这种松耦合，是一种设计的魅力。

Jim的源码在Github上有它的镜像，其中事件循环的代码在此：

[https://github.com/msteveb/jimtcl/blob/master/jim-eventloop.c](https://link.zhihu.com/?target=https%3A//github.com/msteveb/jimtcl/blob/master/jim-eventloop.c)

快速阅读一下Jim的代码，AE整体处理逻辑确实与Jim相似。但AE也不乏创新，比如抽象出polling API这层，达到了多平台的兼容和解耦，而Jim则强耦合了select。

我们常说『站在巨人的肩膀』，虽然Jim不是巨人，但作者通过为他编写代码，从而启发了AE，即使Jim最终被世人遗忘，而它的血肉也化作了土壤，滋养后来人，这就是开源运动的意义所在，也是魅力所在。学习的本质，其实就是模仿，然后改进，这不是抄袭，这是传承。

原文地址：https://zhuanlan.zhihu.com/p/517974884

作者：linux

# 【NO.326】TCP通信接收数据不完整的解决方法

## 1.TCP协议、Socket编程流程

**TCP/IP协议及socket封装**

![img](https://pic1.zhimg.com/80/v2-afaf8f598167f12d10b204275b21c848_720w.webp)

**套接字的编程流程：**

![img](https://pic4.zhimg.com/80/v2-5c2b1525ffcdab8903ea1c3913efe873_720w.webp)

![img](https://pic4.zhimg.com/80/v2-ae29c79d65b65bc7c62b83efd22d3b1b_720w.webp)

![img](https://pic1.zhimg.com/80/v2-b6cc5fc9eb85b03c2dd259440db5a100_720w.webp)

## 2.Send 和 Recv的基本介绍

**2.1 Send函数**

```text
int send( SOCKET s, const char FAR *buf, int len, int flags );  
```

不论是客户还是服务器应用程序都用send函数来向TCP连接的另一端发送数据。客户程序一般用send函数向服务器发送请求，而服务器则通常用send函数来向客户程序发送应答。

参数说明：

```text
第一个参数指定发送端套接字描述符；
第二个参数指明一个存放应用程序要发送数据的缓冲区；
第三个参数指明实际要发送的数据的字节数；
第四个参数一般置0。
```

这里只描述同步Socket的send函数的执行流程。当调用该函数时，

send先比较待发送数据的长度len和套接字s的发送缓冲的长度，

（1）如果len大于s的发送缓冲区的长度，该函数返回SOCKET_ERROR；

（2）如果len小于或者等于s的发送缓冲区的长度，那么send先检查协议

是否正在发送s的发送缓冲中的数据，如果是就等待协议把数据发送完，如果协议还没有开始发送s的发送缓冲中的数据或者s的发送缓冲中没有数据，那么 send就比较s的发送缓冲区的剩余空间和len，（2.1）如果len大于剩余空间大小send就一直等待协议把s的发送缓冲中的数据发送完，（2.2）如果len小于剩余空间大小send就仅仅把buf中的数据copy到剩余空间里（注意并不是send把s的发送缓冲中的数据传到连接的另一端的，而是协议传的，send仅仅是把buf中的数据copy到s的发送缓冲区的剩余空间里）。

如果send函数copy数据成功，就返回实际copy的字节数，

如果send在copy数据时出现错误，那么send就返回SOCKET_ERROR；如果send在等待协议传送数据时网络断开的话，那么send函数也返回SOCKET_ERROR。

要注意send函数把buf中的数据成功copy到s的发送缓冲的剩余空间里后它就返回了，但是此时这些数据并不一定马上被传到连接的另一端。如果协议在后续的传送过程中出现网络错误的话，那么下一个Socket函数就会返回SOCKET_ERROR。(每一个除send外的Socket函数在执行的最开始总要先等待套接字的发送缓冲中的数据被协议传送完毕才能继续，如果在等待时出现网络错误，那么该Socket函数就返回 SOCKET_ERROR）

注意：在Unix系统下，如果send在等待协议传送数据时网络断开的话，调用send的进程会接收到一个SIGPIPE信号，进程对该信号的默认处理是进程终止。

通过测试发现，异步socket的send函数在网络刚刚断开时还能发送返回相应的字节数，同时使用select检测也是可写的，但是过几秒钟之后，再send就会出错了，返回-1。select也不能检测出可写了。

**2.2 Recv函数**

```text
int recv( SOCKET s,  char FAR *buf, int len, int flags);  
```

不论是客户还是服务器应用程序都用recv函数从TCP连接的另一端接收数据。

参数说明：

```text
第一个参数指定接收端套接字描述符；
第二个参数指明一个缓冲区，该缓冲区用来存放recv函数接收到的数据；
第三个参数指明buf的长度；
第四个参数一般置0。
```

这里只描述同步Socket的recv函数的执行流程。当应用程序调用recv函数时，recv先等待s的发送缓冲中的数据被协议传送完毕，

(1)如果协议在传送s的发送缓冲中的数据时出现网络错误，那么recv函数返回SOCKET_ERROR，

(2)如果s的发送缓冲中没有数据或者数据被协议成功发送完毕后，recv先检查套接字s的接收缓冲区，如果s接收缓冲区中没有数据或者协议正在接收数据，那么recv就一直等待，直到协议把数据接收完毕。当协议把数据接收完毕，recv函数就把s的接收缓冲中的数据copy到buf中

特别提醒：

协议接收到的数据可能大于buf的长度，所以在这种情况下要调用几次recv函数才能把s的接收缓冲中的数据copy完。recv函数仅仅是copy数据，真正的接收数据是协议来完成的,recv函数返回其实际copy的字节数。

如果recv在copy时出错，那么它返回SOCKET_ERROR；

如果recv函数在等待协议接收数据时网络中断了，那么它返回0。

注意：在Unix系统下，如果recv函数在等待协议接收数据时网络断开了，那么调用recv的进程会接收到一个SIGPIPE信号，进程对该信号的默认处理是进程终止。

```text
int send (
   SOCKET s,             
   const char FAR * buf, 
   int len,              
   int flags             
);
```

## 3.常见问题

问题1：send函数每次最多可以发送多少数据？是int的最大值吗？

答：不是int的最大值

问题二：如果buffer中的数据过大，我也只需要调用一次send函数，而底层到底是一次传输成功还是陆续传输我不用管了吗？

答：recv到的数据流可能是断断续续的，你要把他们放在一起然后解码。

问题三：阻塞和非阻塞的区别？

答：Send分为阻塞和非阻塞，

阻塞模式下，如果正常的话，会直到把你所需要发送的数据发完再返回；

非阻塞模式，会根据你的socket在底层的可用缓冲区的大小，来将你的缓冲区当中的数据拷贝过去，有多大缓冲区就拷贝多少，缓冲区满了就立即返回，这个时候的返回值，只表示拷贝到缓冲区多少数据，但是并不代表发送多少数据，同时剩下的部分需要你再次调用send才会再一次拷贝到底层缓冲区。

特别注意：You can use setsockopt to enlarge the buffer.

作为一个套接字，它拥有两个缓冲，接收数据缓冲和发送数据缓冲（此缓冲不同与你自己定义的缓冲），当有数据到达时，首先进入的

就是接收数据缓冲，然后用户从这个缓冲中将数据读出来，这就是套接字接受的过程，这个缓冲的大小可以自己用**SetSocketOpt()**设定，

同时操作系统对它有一个默认大小，如果对方在很短时间内发送大量数据到达这个套接子时，可能它没来得及接收完，因此接收缓冲处于

满的状态，再有数据来的时候就进不去了，因此对方的 SEND可能就返回错误，在对方发送的数据量很小时不会出现这种情况，当数据量很大时，情况就很明显了，很容易造成收不到的情况。同样，发送方的发送缓冲也有相对应的问题。

问题四：缓冲区怎么理解？

答：socket缓冲区每一个socket在被创建之后，系统都会给它分配两个缓冲区，即输入缓冲区和输出缓冲区。

![img](https://pic4.zhimg.com/80/v2-16e9f0ce9bc0aa531c74919eab48556f_720w.webp)

send函数并不是直接将数据传输到网络中，而是负责将数据写入输出缓冲区，数据从输出缓冲区发送到目标主机是由TCP协议完成的。数据写入到输出缓冲区之后，send函数就可以返回了，数据是否发送出去，是否发送成功，何时到达目标主机，都不由它负责了，而是由协议负责。

recv函数也是一样的，它并不是直接从网络中获取数据，而是从输入缓冲区中读取数据。

输入输出缓冲区，系统会为每个socket都单独分配，并且是在socket创建的时候自动生成的。一般来说，默认的输入输出缓冲区大小为8K。套接字关闭的时候，输出缓冲区的数据不会丢失，会由协议发送到另一方；而输入缓冲区的数据则会丢失。

## 4. 对于数据接收不完整的情况，可以考虑从以下几个方面解决：

1：利用SetSocketOpt()函数将接收方套接子接收缓冲设为足够大小；

具体操作：在send()的时候，返回的是实际发送出去的字节(同步)或发送到socket缓冲区的字节(异步);系统默认的状态发送和接收一次为8688字节(约为8.5K)；在实际的过程中发送数据和接收数据量比较大，可以设置socket缓冲区，而避免了send(),recv()不断的循环收发：

```text
// 接收缓冲区
int nRecvBuf=32*1024;//设置为32K
setsockopt(s,SOL_SOCKET,SO_RCVBUF,(const char*)&nRecvBuf,sizeof(int));
//发送缓冲区
int nSendBuf=32*1024;//设置为32K
setsockopt(s,SOL_SOCKET,SO_SNDBUF,(const char*)&nSendBuf,sizeof(int));
```

2.基于winsock API，比较实用，自己写的，简单又粗暴同时还有技巧~

这样包装的目的显而易见，防止send或者 recv不完整，这样你想发一个

几MB直接调用下面方法就okay，不会少发~

```text
bool SendAll(SOCKET &sock, char*buffer, int size)
{
    while (size>0)
    {
        int SendSize= send(sock, buffer, size, 0);
        if(SOCKET_ERROR==SendSize)
            return false;
        size = size - SendSize;//用于循环发送且退出功能
        buffer+=SendSize;//用于计算已发buffer的偏移量
    }
    return true;
}

bool RecvAll(SOCKET &sock, char*buffer, int size)
{
    while (size>0)//剩余部分大于0
    {
        int RecvSize= recv(sock, buffer, size, 0);
        if(SOCKET_ERROR==RecvSize)
            return false;
        size = size - RecvSize;
        buffer+=RecvSize;
    }
    return true;
}
```

3.设置为阻塞方式：

阻塞就是干不完不准回来！

非阻塞就是你先干,我现看看有其他事没有,完了告诉我一声！

sock默认为阻塞模式，下面的代码可对sock设置为非阻塞模式

```text
int flags = fcntl(sock, F_GETFL, 0);     
fcntl(sock, F_SETFL, flags | O_NONBLOCK); 
```

假设当前代码为服务器，并且已经执行过如下代码，

当sock为阻塞模式，调用accept会阻塞直到一个请求到来

当sock为非阻塞模式，accept会返回-1，errno设置为EAGAIN或者EWOULDBLOCK

3.在recv函数之前加sleep（0.01）函数,而不是recv之后，但是感觉这样没什么效果。

4：在发送方进行数据发送时判断发送是否成功，如果不成功重发；

5：要求接收方收到数据后给发送方回应，发送方只在收到回应后才发送下一条数据。

原文地址：https://zhuanlan.zhihu.com/p/516284323

作者：linux

# 【NO.327】图解｜揭开协程的神秘面纱

## 1.**协程概念的诞生**

先抛一个粗浅的结论：**协程从广义来说是一种设计理念，我们常说的只是具体的实现**。

理解好思想，技术点就很简单了，关于协程道与术的区别：

![img](https://pic1.zhimg.com/80/v2-7b02d658003cc3869d09256414166b28_720w.webp)

## 2.**上古神器COBOL**

协程概念的出现比线程更早，甚至可以追溯到20世纪50年代，提协程就必须要说到一门生命力极强的最早的高级编程语言COBOL。

最开始我以为COBOL这门语言早就消失在历史长河中，但是我错了。

> COBOL语言，是一种面向过程的高级程序设计语言，主要用于数据处理，是国际上应用最广泛的一种高级语言。COBOL是英文Common Business-Oriented Language的缩写，原意是面向商业的通用语言。
> 截止到今年在全球范围内大约有1w台大型机中有3.8w+遗留系统中约2000亿行代码是由COBOL写的，占比高达65%，同时在美国很多政府和企业机构都是基于COBOL打造的，影响力巨大。

时间拉回1958年，美国计算机科学家梅尔文·康威(Melvin Conway)就开始钻研**基于磁带存储的COBOL的编译器优化问题**，这在当时是个非常热门的话题，不少青年才俊都扑进去了，包括图灵奖得主唐纳德·尔文·克努斯教授(Donald Ervin Knuth)也写了一个优化后的编译器。

看看这两位的简介，我沉默了：

> 梅尔文·康威(Melvin Conway)也是一位超级大佬，著名的康威定律提出者。

![img](https://pic3.zhimg.com/80/v2-b30b4fe1a398febff5ea6a0852163c1a_720w.webp)

> 唐纳德·尔文·克努斯是算法和程序设计技术的先驱者，1974年的图灵奖得主，计算机排版系统TeX和字型设计系统METAFONT的发明者，他因这些成就和大量创造性的影响深远的著作而誉满全球，《计算机程序设计的艺术》被《美国科学家》杂志列为20世纪最重要的12本物理科学类专著之一。

![img](https://pic3.zhimg.com/80/v2-266718c39734705a158199dcbc81c9aa_720w.webp)

那究竟是什么问题让这群天才们投入这么大的精力呢？快来看看！

## 3.**COBOL编译器的技术难题**

我们都是知道高级编程语言需要借助编译器来生成二进制可执行文件，编译器的基本步骤包括：**读取字符流、词法分析、语法分析、语义分析、代码生成器、代码优化器等**。

这种管道式的流程，上一步的输出作为下一步的输入，将中间结果存储在内存即可，这在现代计算机上毫无压力，但是受限于软硬件水平，在几十年前的COBOL语言却是很难的。

![img](https://pic1.zhimg.com/80/v2-1c7728a46528731e9d65988e3547ad40_720w.webp)

在1958年的时候，当时的存储还不发达，磁带作为存储器是1951年在计算机中得到应用的，所以那个时代的COBOL很依赖于磁带。

![img](https://pic2.zhimg.com/80/v2-eabcd221357dfd15175215b6a6968129_720w.webp)

其实，我在网上找了很多资料去看当时的编译器有什么问题，只找到了一条：编译器无法做到读一次磁带就可以完成整个编译过程，也就是所谓的one-pass编译器还没有产生。

> 当时的COBOL程序被写在一个磁带上，而磁带不支持随机读写，只能顺序读，而当时的内存又不可能把整个磁带的内容都装进去，所以一次读取没编译完就要再从头读。

于是，我脑补了COBOL编译器和磁带之间可能的两种multi-pass形式的交互情况：

- **可能情况一**
  对于COBOL的编译器来说，要完成词法分析、语法分析就要从磁带上读取程序的源代码，在之前的编译器中词法分析和语法分析是相互独立的，这就意味着：

- - 词法分析时需要将磁带从头到尾过一遍
  - 语法分析时需要将磁带从头到尾过一遍

![img](https://pic1.zhimg.com/80/v2-636fd1fdf9ab2af8d43addbb92016688_720w.webp)

- **可能情况二**
  听过磁带的朋友们一定知道磁带的两个基本操作：倒带和快进。
  在完成编译器的词法分析和语法分析两件事情时，需要磁带反复的倒带和快进去寻找两类分析所需的部分，类似于磁盘的寻道，磁头需要反复移动横跳，并且当时的磁带不一定支持随机读写。

![img](https://pic4.zhimg.com/80/v2-5ff1f42913b1cfbb671e48f3fb3b07ab_720w.webp)

从一些资料可以看到，COBOL当时编译器各个环节相互独立的，这种软硬件的综合限制导致无法实现one-pass编译。

## 4.**协同式解决方案**

在梅尔文·康威的编译器设计中**将词法分析和语法分析合作运行，而不再像其他编译器那样相互独立，两个模块交织运行，编译器的控制流在词法分析和语法分析之间来回切换**：

- 当词法分析模块基于词素产生足够多的词法单元Token时就控制流转给语法分析
- 当语法分析模块处理完所有的词法单元Token时将控制流转给词法分析模块
- 词法分析和语法分析各自维护自身的运行状态，并且具备主动让出和恢复的能力

可以看到这个方案的核心思想在于：

> 梅尔文·康威构建的这种协同工作机制，需要参与者让出（yield）控制流时，记住自身状态，以便在控制流返回时能从上次让出的位置恢复（resume）执行。简言之，`协程的全部精神就在于控制流的主动让出和恢复`。

![img](https://pic3.zhimg.com/80/v2-81da3a0a58524a83630314bf22c8ff52_720w.webp)

这种协作式的任务流和计算机中断非常像，在当时条件的限制下，由梅尔文·康威提出的这种让出/恢复模式的协作程序被认为是最早的协程概念，并且基于这种思想可以打造新的COBOL编译器。

在1963年，梅尔文·康威也发表了一篇论文来说明自己的这种思想，虽然半个多世纪过去了，有幸我还是找到了这篇论文：

> [https://melconway.com/Home/pdf/compiler.pdf](https://link.zhihu.com/?target=https%3A//melconway.com/Home/pdf/compiler.pdf)

![img](https://pic3.zhimg.com/80/v2-9d0bb5874dea9348e9e847fb963d6476_720w.webp)

说实话这paper真是有点难，时间过于久远，很难有共鸣，最后我放弃了，要不然我或许能搞明白之前编译器的具体问题了。

![img](https://pic4.zhimg.com/80/v2-bedb6f7c704f6a9c92b13c2c7369e7db_720w.webp)

## **5.怀才不遇的协程**

虽然协程概念出现的时间比线程还要早，但是协程一直都没有正是登上舞台，真是有点怀才不遇的赶脚。

我们上学的时候，老师就讲过一些软件设计思想，其中主流语言崇尚自顶向下top-down的编程思想:

> 对要完成的任务进行分解，先对最高层次中的问题进行定义、设计、编程和测试，而将其中未解决的问题作为一个子任务放到下一层次中去解决。
> 这样逐层、逐个地进行定义、设计、编程和测试，直到所有层次上的问题均由实用程序来解决，就能设计出具有层次结构的程序。

C语言就是典型的top-down思想的代表，在main函数作为入口，各个模块依次形成层次化的调用关系，同时各个模块还有下级的子模块，同样有层次调用关系。

但是**协程这种相互协作调度的思想和top-down是不合的，在协程中各个模块之间存在很大的耦合关系，并不符合高内聚低耦合的编程思想**，相比之下top-down使程序结构清晰、层次调度明确，代码可读性和维护性都很不错。

与线程相比，协作式任务系统让调用者自己来决定什么时候让出，比操作系统的抢占式调度所需要的时间代价要小很多，后者为了能恢复现场会在切换线程时保存相当多的状态，并且会非常频繁地进行切换，资源消耗更大。

综合来说，**协程完全是用户态的行为，由程序员自己决定什么时候让出控制权，保存现场和切换恢复使用的资源也非常少，同时对提高处理器效率来说也是完全符合的**。

那么不禁要问：协程看着不错，为啥没成为主流呢？

- **协程的思想和当时的主流不符合**
- **抢占式的线程可以解决大部分的问题，让使用者感受的痛点不足**

换句话说：协程能干的线程干得也不错，线程干的不好的地方，使用者暂时也不太需要，所以协程就这样怀才不遇了。

> 其实，协程虽然在x86架构上没有折腾出大风浪，由于抢占式任务系统依赖于CPU硬件的支持，对硬件要求比较高，对于一些嵌入式设备来说，协同调度再合适不过了，所以协程在另外一个领域也施展了拳脚。

## 6.**协程的雄起**

**我们对于CPU的压榨从未停止。**

对于CPU来说，任务分为两大类：**计算密集型和IO密集型**。

![img](https://pic2.zhimg.com/80/v2-907ef5d13bd2d1c0064b32f4ad050619_720w.webp)

计算密集型已经可以最大程度发挥CPU的作用，但是IO密集型一直是提高CPU利用率的难点。

 **IO密集型任务之痛**

对于IO密集型任务，在抢占式调度中也有对应的解决方案：**异步+回调**。

也就是遇到IO阻塞，比如下载图片时会立即返回，等待下载完成将结果进行回调处理，交付给发起者。

![img](https://pic3.zhimg.com/80/v2-60299ba2b6518c395adbd761cc57f0ba_720w.webp)

> 就像你常去早餐店，油条还没好，你和老板很熟悉就先交了钱去座位玩手机了，等你的油条好了，服务员就端过去了，这就是典型的异步+回调。

虽然异步+回调在现实生活中看着也很简单，但是在程序设计上却很让人头痛，在某些场景下会让整个程序的可读性非常差，而且也不好写，相反同步IO虽然效率低，但是很好写，

![img](https://pic2.zhimg.com/80/v2-9a02a501267263aa506b409b9e09dc75_720w.webp)

还是以为异步图片下载为例，图片服务中台提供了异步接口，发起者请求之后立即返回，图片服务此时给了发起者一个唯一标识ID，等图片服务完成下载后把结果放到一个消息队列，此时需要发起者不断消费这个MQ才能拿到下载结果。

整个过程相比同步IO来说，**原来整体的逻辑被拆分为好几个部分，各个子部分有状态的迁移，对大部分程序员来说维护状态简直就是噩梦，日后必然是bug的高发地**。

## 7.**用户态协同调度**

随着网络技术的发展和高并发要求，对于抢占式调度对IO型任务处理的低效逐渐受到重视，终于协程的机会来了。

![img](https://pic3.zhimg.com/80/v2-9c7caf4ea20cf3bdbeb4a040c49d94c2_720w.webp)

协程将IO的处理权交给了程序员，遇到IO被阻塞时就交出控制权给其他协程，等其他协程处理完再把控制权交回来。

**通过yield方式转移执行权的多个协程之间并非调用者和被调用者的关系，而是彼此平等、对称、合作的关系。**

协程一直没有占上风的原因，除了设计思想的矛盾，还有一些其他原因，毕竟协程也不是银弹，来看看协程有什么问题：

- 协程无法利用多核，需要配合进程来使用才可以在多CPU上发挥作用
- 线程的回调机制仍然有巨大生命力，协程无法全部替代
- 控制权需要转移可能造成某些协程的饥饿，抢占式更加公平
- 协程的控制权由用户态决定可能转移给某些恶意的代码，抢占式由操作系统来调度更加安全

综上来说，**协程和线程并非矛盾，协程的威力在于IO的处理，恰好这部分是线程的软肋，由对立转换为合作才能开辟新局面**。

## 8.**拥抱协程的编程语言**

网络操作、文件操作、数据库操作、消息队列操作等重IO操作，是任何高级编程语言无法避开的问题，也是提高程序效率的关键。

![img](https://pic2.zhimg.com/80/v2-0c966fcb1b08711af03b4697fbd3f9bd_720w.webp)

像Java、C/C++、Python这些老牌语言也陆续开始借助于第三方包来支持协程，来解决自身语言的不足。

像Golang这种新生选手，在语言层面原生支持了协程，可以说是彻底拥抱协程，这也造就了Go的高并发能力。

我们来分别看看它们是怎么实现协程的，以及实现协程的关键点是什么。

### 8.1**Python**

Python对协程的支持也经历了多个版本，从部分支持到完善支持一直在演进：

- Python2.x对协程的支持比较有限，生成器yield实现了一部分但不完全
- 第三方库gevent对协程的实现有比较好，但不是官方的
- Python3.4加入了asyncio模块
- 在Python3.5中又提供了async/await语法层面的支持
- Python3.6中asyncio模块更加完善和稳
- Python3.7开始async/await成为保留关键字

我们以最新的async/await来说明Python的协程是如何使用的：

```text
import asyncio
from pathlib import Path
import logging
from urllib.request import urlopen, Request
import os
from time import time
import aiohttp
 
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
 
 
CODEFLEX_IMAGES_URLS = ['https://codeflex.co/wp-content/uploads/2021/01/pandas-dataframe-python-1024x512.png',
                        'https://codeflex.co/wp-content/uploads/2021/02/github-actions-deployment-to-eks-with-kustomize-1024x536.jpg',
                        'https://codeflex.co/wp-content/uploads/2021/02/boto3-s3-multipart-upload-1024x536.jpg',
                        'https://codeflex.co/wp-content/uploads/2018/02/kafka-cluster-architecture.jpg',
                        'https://codeflex.co/wp-content/uploads/2016/09/redis-cluster-topology.png']
 
 
async def download_image_async(session, dir, img_url):
    download_path = dir / os.path.basename(img_url)
    async with session.get(img_url) as response:
        with download_path.open('wb') as f:
            while True:
                chunk = await response.content.read(512)
                if not chunk:
                    break
                f.write(chunk)
    logger.info('Downloaded: ' + img_url)
 
 
async def main():
    images_dir = Path("codeflex_images")
    Path("codeflex_images").mkdir(parents=False, exist_ok=True)
 
    async with aiohttp.ClientSession() as session:
        tasks = [(download_image_async(session, images_dir, img_url)) for img_url in CODEFLEX_IMAGES_URLS]
        await asyncio.gather(*tasks, return_exceptions=True)
 
 
if __name__ == '__main__':
    start = time()
     
    event_loop = asyncio.get_event_loop()
    try:
        event_loop.run_until_complete(main())
    finally:
        event_loop.close()
 
    logger.info('Download time: %s seconds', time() - start)
```

这段代码展示了如何使用async/await来实现图片的并发下载功能。

- 在普通的函数def前面加async关键字就变成异步/协程函数，调用该函数并不会运行，而是返回一个协程对象，后续在event_loop中执行
- await表示等待task执行完成，也就是yeild让出控制权，同时asyncio使用事件循环event_loop来实现整个过程，await需要在async标注的函数中使用
- event_loop事件循环充当管理者的角色，将控制权在几个协程函数之间切换

### 8.2**C++**

在C++20引入协程框架，但是很不成熟，换句话说是给写协程库的大佬用的最底层的东西，用起来就很复杂门槛比较高。

C++作为高性能服务器开发语言的无冕之王，各大公司也做了很多尝试来使用协程功能，比如**boost.coroutine、微信的libco、libgo、云风用C实现的协程库**等。

说实话，C++协程相关的东西有点复杂，有兴趣的朋友可以看以前的文章，在此不展开了。

### 8.3**Go**

go中的协程被称为goroutine，被认为是用户态更轻量级的线程，协程对操作系统而言是透明的，也就是操作系统无法直接调度协程，因此必须有个中间层来接管goroutine。

goroutine仍然是基于线程来实现的，因为线程才是CPU调度的基本单位，在go语言内部维护了一组数据结构和N个线程，协程的代码被放进队列中来由线程来实现调度执行，这就是著名的GMP模型。

- **G:Goroutine**

> 每个Gotoutine对应一个G结构体，G存储Goroutine的运行堆栈，状态，以及任务函数，可重用函数实体G需要保存到P的队列或者全局队列才能被调度执行。

- **M:machine**

> M是线程的抽象，代表真正执行计算的资源，在绑定有效的P后，进入调度执行循环，M会从P的本地队列来执行，

- **P:Processor**

> P是一个抽象的概念，不是物理上的CPU而是表示逻辑处理器。当一个P有任务，需要创建或者唤醒一个系统线程M去处理它队列中的任务。
> P决定同时执行的任务的数量，GOMAXPROCS限制系统线程执行用户层面的任务的数量。
> 对M来说，P提供了相关的执行环境，入内存分配状态，任务队列等。

![img](https://pic4.zhimg.com/80/v2-227eb1bdf5bf77e5ee52fefa3e083ac7_720w.webp)

**GMP模型运行的基本过程**：

- 首先创建一个G对象，然后G被保存在P的本地队列或者全局队列
- 这时P会唤醒一个M，M寻找一个空闲的P将G移动到它自己，然后M执行一个调度循环：调用G对象->执行->清理线程->继续寻找Goroutine。
- 在M的执行过程中，上下文切换随时发生。当切换发生，任务的执行现场需要被保护，这样在下一次调度执行可以进行现场恢复。
- M的栈保存在G对象，只有现场恢复需要的寄存器(SP,PC等)，需要被保存到G对象。

![img](https://pic3.zhimg.com/80/v2-6c9a0039b8bf0a7a77e017b17089a822_720w.webp)

## 9.**总结**

本文通过1960年对COBOL语言编译器的one-pass问题的介绍，让大家看到了协同式程序的最早背景以及主动让出/恢复的重要理念。

紧接着介绍了主流的自顶向下的软件设计思想和协程思想的矛盾所在，并且抢占式程序调度的蓬勃发展，以及存在的问题。

继续介绍了关于IO密集型任务对于提升CPU效率的阻碍，抢占式调度对于IO密集型问题的异步+回调的解决方案，以及协程的处理，展示了协程在IO密集型任务上处理的重大优势。

最后说明了当前抢占式调度+协程IO密集型处理的方案，包括Python、C++和go的语言层面对于协程的支持和实现。

本文特别具体的内容并不多，旨在介绍协程思想及其优势所在，对于各个语言的协程实现细节并未展开。

原文地址：https://zhuanlan.zhihu.com/p/515916638

作者：linux

# 【NO.328】文件的 io 栈，你真的知道了吗

![img](https://pic3.zhimg.com/80/v2-d8b3fada99567316e1f28ad3ce929af6_720w.webp)

## 1.**请简单描述一下文件的 io 栈？**

同事问到你，你能立马就讲个道道出来吗？这个问题可以往深入讲，也可往浅出讲。最主要的还是心里有把尺，要有整体的把握。

我们今天分三个小步走来分享这个问题的思考。

1. 首先，要明确一条清晰的 IO 栈路线；
2. 其次，要了解每一个途径地点的大致用途；
3. 最后，可以深入了解内核调用的代码路线；

## 2.**文件 IO 的内核路线**

IO 从用户态走系统调用进到内核，内核的路径：`VFS → 文件系统 → 块层 → SCSI 层` 。这里提一点，Linux 的 “文件” 的概念已经升华了，一切皆文件，网络 IO 其实进到内核也是走 VFS 。

这条路径可以完全记到心里，更深入的细节可以后续基于这个框架去补充。那么接下来我们稍微了解下这 IO 路径途径的 4 个节点分别做哪些事情？

![img](https://pic3.zhimg.com/80/v2-c0972da246cf1e4c29f298c0776fa56a_720w.webp)

内核 IO 栈示意图

## 3.**内核 IO 路线的节点**

**1VFS 层**

VFS （ Virtual File System 、Virtual FileSystem Switch ）层是 Linux 针对文件概念封装的一层通用逻辑，它做的事情其实非常简单，就是把所有文件系统的共性的东西抽象出来，比如 file ，inode ，dentry 等结构体，针对这些结构体抽象出通用的 api 接口，然后具体的文件系统则只需要按照接口去实现这些接口即可，在 IO 下来的时候，VFS 层使用到文件所在的文件系统的对应接口。

**它的作用：为上层抽象统一的操作界面，在 IO 路径上切换不同的文件系统。**

假设现在你想要写个内核文件系统，那么只需要按照 Linux 预设的一些 api 接口，实现起来就行了。

**2文件系统**

VFS 把 IO 给到具体的文件系统，文件系统主要做啥呢？

**它的作用：对上抽象一个文件的概念，把数据按照策略存储到块设备上。**

文件系统管理的是一个线性的空间（分区，块设备），而用户看到的却是文件的概念，这一层的转化就是文件系统来做的。它负责把用户的数据按照自己制定的规则存储到块设备上。比如是按照 4K 切块存，还是按照 1M 切块存储，这些都是文件系统自己说了算。

它这一层就是做了一层空间的映射转化，文件的虚拟空间到实际线性设备的映射。**这层映射最关键的是 address_space 相关的接口来做。**

**3块层**

块层其实在真实的硬件之上又抽象了一层，屏蔽不同的硬件驱动，块设备看起来就是一个线性空间而已。块层主要还是 IO 调度策略的实现，尽可能收集批量 IO 聚合下发，让 IO 尽可能的顺序，合并 IO 请求减少 IO 次数等等；

**划重点：块层主要做的是 IO 调度策略的一些优化。**比如最出名的电梯算法就是在这里。

因为所有的 IO 都会汇聚下来，那么在块层做调度优化是最合适的。Linux 也允许用户自行配置这里的调度策略，比如 CFQ，Deadline，NOOP 等策略。

**4SCSI 层**

SCSI 层这个就不用多说了，这个就是硬件的驱动而已，本质就是个翻译器。SCSI 层里面按照细分又会细分多层出来。它是给你的磁盘做最后一道程序，SCSI 层负责和磁盘硬件做转换，IO 交给它就能顺利的到达磁盘硬件。

**5IO 之旅小结**

基本上梳理出上面的主干，这个问题就有解了。后续的就是工作中遇到了某些问题，再针对某个问题细化研究，去查资料，去看内核代码。

比如，这里抛出来一个问题：page cache 是怎么回事？

这个就要去文件系统层看一看了。

## 4.**Page Cache 梳理**

> 基于 Linux 版本 3.10

**1Page Cache 在哪一层？**

page cache 是发生在文件系统这里。通常我们**确保数据落盘有两种方式**：

1. Writeback 回刷数据的方式：write 调用 + sync 调用；
2. Direct IO 直刷数据的方式；

在文件系统这一层，当处理完了一些自己的逻辑之后，需要把数据写到块层去，无论是直刷还是回刷的方式，都是用到 address_space_operations 里面实现的方法：

```text
struct address_space_operations {
    // 回刷的方式，走 Page Cache
    int (*write_begin)(struct file *, struct address_space *mapping, loff_t pos, unsigned len, unsigned flags, struct page **pagep, void **fsdata);
    int (*write_end)(struct file *, struct address_space *mapping, loff_t pos, unsigned len, unsigned copied, struct page *page, void *fsdata);
    // 回刷的方式，走 Page Cache
    int (*writepage)(struct page *page, struct writeback_control *wbc);
    int (*readpage)(struct file *, struct page *);
    int (*writepages)(struct address_space *, struct writeback_control *);
    int (*readpages)(struct file *filp, struct address_space *mapping, struct list_head *pages, unsigned nr_pages);
    void (*readahead)(struct readahead_control *);

    // 直刷的方式
    ssize_t (*direct_IO)(int, struct kiocb *, const struct iovec *iov, loff_t offset, unsigned long nr_segs);

    // ...
};
```

**下面举一个最简单的栗子，比如 Minix 文件系统：**

如果实现一个走 Page Cache 回刷功能的文件系统，那么至少要实现 .write_begin，.write_end，.write_page，.read_page 的接口。巧了，minix 就实现了这几个接口：

```text
static const struct address_space_operations minix_apos = {
    .readpage = minix_readpage,
    .writepage = minix_writepage,
    .write_begin = minix_write_begin,
    .write_end = generic_write_end,
    .bmap = minix_bmap,
};
```

所以，从上面的实现来看，minix 是具备 buffer write 的能力的，当写一个数据的时候，调用栈是：

```text
SYSCALL_DEFINE3(write)
    vfs_write
        .write = do_sync_write // minix 复用了公共的 write
            generic_file_aio_write
                generic_file_buffered_write 
                    generic_perform_write   // 重要函数！！
```

用户的数据**最终在 generic_perform_write 函数里写到了内存中**，并且 IO 至此完成。简单说下 generic_perform_write ，它做这几件事情：

1. 调用 minix_write_begin 分配出 page 内存，并且 page 对应到 buffer head，对应到底层块设备的地址；
2. 把用户数据 copy 到 page 内存中，这样数据就从用户态到内核态了；
3. 对应的 page 设置“脏”的标记，这样就能被识别到了；

**文件系统怎么把 “文件” 的偏移翻译成块设备地址的偏移呢？**

有一个非常重要的函数：**minix_get_block** 就是干这件事的。这个函数将在 minix_write_begin 里面被调用到。这个函数会创建一些 buffer head 的结构体出来，这些结构体将会对应到块的物理位置。page 和 buffer head 关联，所以自然 page 和块物理位置也确认了。

数据在 generic_perform_write 写到内存后，用户的 write 调用就完成了，这种在 Page 的内存的数据我们叫做脏数据（脏页），后面就是等待异步的回刷。

**触发脏数据回刷的方式有多种：**

1. 时间久了，比如超过 30 秒的，那必须赶紧刷，因为夜长梦多；
2. 量足够大了，比如脏页超过 100M 了，那必须赶紧刷，因为量越大，掉电丢数据的损失越大；
3. 有人调用了 Sync ，那必须赶紧刷；

刷这些脏数据，内核是作为任务交给了 kworker 的线程去做。简单来讲就是这是 kworker 会去挑选块设备对应的的一些脏“文件”，把这些对应的 page 内存刷到磁盘。

**很多人可能会疑惑，那回刷又是怎么实现的呢？**

无论回刷的触发点是哪个，**回刷的实现还是要回到文件系统**，也就是文件系统提供的 .write_page 或者 .write_pages 的接口。比如 minix 实现了 minix_write_page 的接口。

回刷非常简单，因为 page 对应要写的地址已经在 minix_write_begin 的时候确定了（物理位置已经绑定好了）。所以只需要对应写下去就行了。

一点题外话，ext4 这个文件系统为了性能考虑，有一种 delay alloc 的选项，把物理位置的绑定不放在用户的路径（ .write_begin ） ，而是放在异步回刷的时候 .write_pages 。

如果是 Direct IO 的方式，那么就简单一点：

```text
SYSCALL_DEFINE3(write)
    vfs_write
        .write = do_sync_write // minix 复用了公共的 write
            generic_file_aio_write
                generic_file_direct_write // direct io 的重要函数
                    .direct_IO // 要具体文件系统支持
```

direct IO 就不会走先写 page cache ，再异步回刷的方式了，它直接就把用户数据 copy 到内核，封装成块层需要的 IO 结构丢下去等结果即可。

但，并不是所有的文件系统都会实现 direct IO，比如 minix 就没有实现，ext2/ext3/ext4 等文件系统倒是实现了，感兴趣的可以去看 ext2_direct_IO 的实现。

## 5.**总结**

1. IO 栈：**VFS - 文件系统 - 块层 - SCSI 驱动层**；
2. VFS 负责通用的文件抽象语义，管理并切换文件系统；
3. 文件系统负责抽象出“文件的概念”，**维护“文件”数据到块层的位置映射**，怎么摆放数据，怎么抽象文件都是文件系统说了算；
4. 块层对底层硬件设备做一层统一的抽象，最重要的是做一些 **IO 调度的策略**。比如，尽可能收集批量 IO 聚合下发，让 IO 尽可能的顺序，合并 IO 请求减少 IO 次数等等；
5. SCSI 层则是负责最后对硬件磁盘的对接，驱动层，**本质就是个翻译器**；
6. 文件的 buffer write 要实现 .write_begin，.write_page 等接口，**前者用于分配 page 并绑定块层物理空间，后者用户异步回刷的时候调用**（注意，非常规的优化在回刷的时候才去绑定物理空间）；
7. 文件系统 .write_begin 调用分配物理位置的时候依赖于 **get_block** 的实现，物理位置分配好之后，page 会对应到特定的 buffer head 结构，buffer head 结构则对应到具体的块设备位置；
8. **direct IO 直接在用户路径上刷数据到磁盘，不走 PageCache 的逻辑，**但并不是所有文件系统都会实现它；

原文地址：https://zhuanlan.zhihu.com/p/515818951

作者：linux

# 【NO.329】如何用300行代码实现一个完整的线程池

开源项目Workflow中有个重要的基础模块：

**代码仅300行的C语言线程池**。

本文会伴随源码分析，而**逻辑完备**、**对称无差别**的特点于第3部分开始

欢迎跳阅， 或直接到Github主页上围观代码

**[https://github.com/sogou/workflow/blob/master/src/kernel/thrdpool.c](https://link.zhihu.com/?target=https%3A//github.com/sogou/workflow/blob/master/src/kernel/thrdpool.c)**

## **1.Workflow的thrdpool**

Workflow的大招：计算通信融为一体的异步调度模式，而计算的核心：Executor调度器，就是基于这个线程池实现的。可以说，一个通用而高效的线程池，是我们写C/C++代码时离不开的基础模块。

**thrdpool**代码位置在**src/kernel/**，不仅可以直接拿来使用，同时也适合阅读学习。

而更重要的，秉承Workflow项目本身一贯的严谨极简的作风，这个thrdpool代码极致简洁，**实现逻辑上亦非常完备，结构精巧，处处严谨，**复杂的并发处理依然可以**对称无差别，**不得不让我惊叹：

妙啊！！！

你可能会很好奇，线程池还能写出什么别致的新思路吗？先列出一些，你们细品：

- `特点1`：创建完线程池后，无需记录任何线程id或对象，线程池可以通过**一个等一个的方式优雅地去结束**所有线程；
  → 也就是说，每一个**线程**都是对等的
- `特点2`：**线程任务可以由另一个线程任务调起**；甚至线程池正在**被销毁时也可以提交下一个任务**；（这很重要，因为线程本身很可能是不知道线程池的状态的；
  → 即，每一个**任务**也是对等的
- `特点3`：同理，**线程任务也可以销毁这个线程池**；（非常完整～
  → 每一种**行为**也是对等的，包括**destroy**

我真的迫不及待为大家深层解读一下，这个我愿称之为**“逻辑完备”的线程池**。

## **2.前置知识**

第一部分我先从最基本的内容梳理一些个人理解，有基础的小伙伴可以直接跳过。如果有不准确的地方，欢迎大家指正交流～

为什么需要线程池？（其实思路不仅对线程池，对任何有限资源的调度管理都是类似的）

我们知道，**通过pthread**或者**std::thread**创建线程，就可以实现多线程并发执行我们的代码。

但是CPU的核数是固定的，所以真正并行执行的最大值也是固定的，**过多的线程除了频繁创建产生overhead以外，还会导致对系统资源进行争抢**，这些都是不必要的浪费。

因此我们可以管理有限个线程，**循环且合理地利用它们**。

那么线程池一般包含哪些内容呢？

- 首先是管理若干个工具人线程；
- 其次是管理交给线程去执行的任务，这个一般会有一个队列；
- 再然后线程之间需要一些同步机制，比如mutex、condition等；
- 最后就是各线程池实现上自身需要的其他内容了；

接下来我们看看**Workflow**的**thrdpool**是怎么做的。

## **3.代码概览**

以下共7步常用思路，足以让我们把代码飞快过一遍。

**第1步：先看头文件，有什么接口。**

我们打开`thrdpool.h`，只需关注这三个：

```text
// 创建线程池
thrdpool_t *thrdpool_create(size_t nthreads, size_t stacksize);
// 把任务交给线程池的入口
int thrdpool_schedule(const struct thrdpool_task *task, thrdpool_t *pool); 
// 销毁线程池
void thrdpool_destroy(void (*pending)(const struct thrdpool_task *), thrdpool_t *pool);
```

**第2步：接口上有什么数据结构。**

即，我们如何描述一个交给线程池的任务。

```text
struct thrdpool_task
{                                                                    
    void (*routine)(void *);  // 函数指针
    void *context;            // 上下文
};  
```

**第3步：再看实现.c，内部数据结构。**

```text
struct __thrdpool
{
    struct list_head task_queue;// 任务队列
    size_t nthreads;  // 线程个数
    size_t stacksize; // 构造线程时的参数
    pthread_t tid;    // 运行期间记录的是个zero值
    pthread_mutex_t mutex;
    pthread_cond_t cond;
    pthread_key_t key;
    pthread_cond_t *terminate;
};
```

**没有一个多余，每一个成员都很到位：**

1. **tid**：线程id，**整个线程池只有一个**，它不会奇怪地去记录任何一个线程的id，这样就不完美了 ，它平时运行的时候是**空值**，退出的时候，它是用来**实现链式等待的关键**。
2. **mutex** 和 **cond**是常见的线程间同步的工具，其中这个cond是用来给**生产者和消费者**去操作任务队列用的。
3. **key**：是线程池的key，然后会赋予给每个由线程池创建的线程作为他们的thread local，**用于区分这个线程是否是线程池创建的**。
4. 一个**pthread_cond_t \*terminate**，这有两个用途：不仅是退出时的标记位 ，而且还是调用退出的那个人要等待的condition。

以上各个成员的用途，好像说了，又好像没说，是因为**几乎每一个成员都值得深挖一下**，所以我们记住它们，后面看代码的时候就会豁然开朗！

**第4步：接口都调用了什么核心函数。**

```text
thrdpool_t *thrdpool_create(size_t nthreads, size_t stacksize)
{
    thrdpool_t *pool;
    ret = pthread_key_create(&pool->key, NULL);
    if (ret == 0)
    {
        // 去掉了其他代码，但是注意到刚才的tid和terminate的赋值
        memset(&pool->tid, 0, sizeof (pthread_t));
        pool->terminate = NULL;
        if (__thrdpool_create_threads(nthreads, pool) >= 0)
            return pool;
        ...
```

这里可以看到`__thrdpool_create_threads()`里边最关键的，就是循环创建**nthreads**个线程。

```text
        while (pool->nthreads < nthreads)                                       
        {                                                                       
            ret = pthread_create(&tid, &attr, __thrdpool_routine, pool);
            ...
```

**第5步：略读核心函数的功能。**

所以我们在上一步知道了，每个线程执行的是`__thrdpool_routine()`不难想象，它会**不停从队列拿任务出来执行**：

```text
static void *__thrdpool_routine(void *arg)                                      
{                                                                               
    ...                                   
    while (1)                                                                   
    {
        // 1. 从队列里拿一个任务出来，没有就等待
        pthread_mutex_lock(&pool->mutex);
        while (!pool->terminate && list_empty(&pool->task_queue))
            pthread_cond_wait(&pool->cond, &pool->mutex);
        // 2. 线程池结束的标志位，记住它，先跳过
        if (pool->terminate) 
            break;

        // 3. 如果能走到这里，恭喜你，拿到了任务～ 
        entry = list_entry(*pos, struct __thrdpool_task_entry, list);
        list_del(*pos);
        // 4. 先解锁
        pthread_mutex_unlock(&pool->mutex); 

        task_routine = entry->task.routine;
        task_context = entry->task.context;
        free(entry); 
        // 5. 再执行
        task_routine(task_context); 

        // 6. 这里也先记住它，意思是线程池里的线程可以销毁线程池
        if (pool->nthreads == 0) 
        {                                                                       
            /* Thread pool was destroyed by the task. */
            free(pool);
            return NULL;
        }                                                                       
    }
    ... // 后面还有魔法，留下一章解读~~~
```

**第6步：把函数之间的关系联系起来。**

刚才看到的`__thrdpool_routine()`就是线程的核心函数了，它可以和谁关联起来呢？可以和接口`thrdpool_schedule()`关联上

我们说过，线程池上有个队列管理任务：

- 每个执行**routine**的线程，都是消费者
- 每个发起**schedule**的线程，都是生产者

我们已经看过消费者了，来看看生产者的代码：

```text
inline void __thrdpool_schedule(const struct thrdpool_task *task, void *buf,
                                thrdpool_t *pool)
{
    struct __thrdpool_task_entry *entry = (struct __thrdpool_task_entry *)buf;  

    entry->task = *task;
    pthread_mutex_lock(&pool->mutex);
    // 添加到队列里
    list_add_tail(&entry->list, &pool->task_queue);
    // 叫醒在等待的线程
    pthread_cond_signal(&pool->cond);
    pthread_mutex_unlock(&pool->mutex);
}
```

说到这里，`特点2`就非常清晰了：开篇说的`特点2`是说，”**线程任务可以由另一个线程任务调起**”。

只要对队列的管理做得好，显然消费者所执行的函数里也可以生产

**第7步：看其他情况的处理**

对于线程池来说就是比如销毁的情况。

接口`thrdpool_destroy()`实现非常简单：

```text
void thrdpool_destroy(void (*pending)(const struct thrdpool_task *),            
                      thrdpool_t *pool)                                         
{
    ...
    // 1. 内部会设置pool->terminate，并叫醒所有等在队列拿任务的线程
    __thrdpool_terminate(in_pool, pool);

    // 2. 把队列里还没有执行的任务都拿出来，通过pending返回给用户
    list_for_each_safe(pos, tmp, &pool->task_queue)                             
    {
        entry = list_entry(pos, struct __thrdpool_task_entry, list);            
        list_del(pos);                                                          
        if (pending)                                                            
            pending(&entry->task);                                              
        ... // 后面就是销毁各种内存，同样有魔法~
```

在退出的时候，我们那些**已经提交但是还没有被执行的任务**是绝对不能就这么扔掉了的，于是我们可以传入一个`pending()`函数，**上层可以做自己的回收、回调、或任何保证上层逻辑完备的事情。**

**设计的完整性，无处不在。**

接下来我们就可以跟着我们的核心问题，针对性地看看每个特点都是怎么实现的。

## **4.特点1: 一个等待一个的优雅退出**

这里提出一个问题：**线程池要退出，如何结束所有线程？**

一般线程池的实现都是需要记录下所有的线程id，或者thread对象，以便于我们去用**join**方法等待它们结束。

不严格地用join收拾干净会有什么问题？最直观的，模块退出时很可能会报**内存泄漏**

但是我们刚才看，**pool里并没有记录所有的tid呀**？正如开篇说的，**pool上只有一个tid，而且还是个空的值**。

而`特点1`正是**Workflow thrdpool**的答案：

**无需记录所有线程，我可以让线程挨个自动退出、且一个等待下一个，最终达到调用完thrdpool_destroy()后内存回收干净的目的。**

这里先给一个简单的图，假设发起destroy的人是main线程，我们如何做到一个等一个退出：

![img](https://pic2.zhimg.com/80/v2-67d125eef067cfc14bba9f09c4b05285_720w.webp)

外部线程，比如main，发起destroy

步骤如下：

1. 线程的退出，由thrdpool_destroy()设置**pool->terminate**开始。
2. 我们每个线程，在**while(1)** 里会第一时间发现terminate，线程池要退出了，然后会**break**出这个while循环。
3. 注意这个时候，**还持有着mutex锁**，我们拿出pool上唯一的那个tid，放到我的临时变量，我会根据拿出来的值做不同的处理。**且我会把我自己的tid放上去**，然后再解mutex锁。
4. 那么很显然，**第一个从pool上拿tid的人，会发现这是个0值，就可以直接结束了**，不用负责等待任何其他人，但我在完全结束之前需要有人负责等待我的结束，所以我会把我的id放上去。
5. 而如果发现自己从pool里拿到的tid不是0值，**说明我要负责join上一个人**，并且把我的tid放上去，**让下一个人负责我**。
6. 最后的那个人，是**那个发现pool->nthreads为0的人，那么我就可以通过这个terminate**（它本身是个condition）去通知发起destroy的人。
7. 最后发起者就可以退了。

是不是非常**有意思**！！！非常**优雅**的做法！！！

所以我们会发现，其实**大家不太需要知道太多信息，只需要知道我要负责的上一个人**。

当然每一步都是非常严谨的，结合刚才跳过的第一段魔法 感受一下：

```text
static void *__thrdpool_routine(void *arg)                                         
{
    while (1)
    {   // 1.注意这里还持有锁
        pthread_mutex_lock(&pool->mutex); 
        ... // 等着队列拿任务出来
        // 2. 这既是标识位，也是发起销毁的那个人所等待的condition
        if (pool->terminate) 
            break;
        ... // 执行拿到的任务
    }

    /* One thread joins another. Don't need to keep all thread IDs. */
    // 3. 把线程池上记录的那个tid拿下来，我来负责上一人
    tid = pool->tid;
    // 4. 把我自己记录到线程池上，下一个人来负责我
    pool->tid = pthread_self();
    // 5. 每个人都减1，最后一个人负责叫醒发起detroy的人
    if (--pool->nthreads == 0) 
        pthread_cond_signal(pool->terminate);
        
    // 6. 这里可以解锁进行等待了
    pthread_mutex_unlock(&pool->mutex);
    // 7. 只有第一个人拿到0值
    if (memcmp(&tid, &__zero_tid, sizeof (pthread_t)) != 0) 
        // 8. 只要不0值，我就要负责等上一个结束才能退
        pthread_join(tid, NULL); 
                                                                                
    return NULL; // 9. 退出，干干净净～
}
```

## **5.特点2：线程任务可以由另一个线程任务调起**

在第二部分我们看过源码，只要队列管理得好，线程任务里提交下一个任务是完全OK的。

这很合理。

那么问题来了，`特点1`又说，我们每个线程，**是不需要知道太多线程池的状态和信息的**。而线程池的销毁是个过程，如果在这个过程间提交任务会怎么样呢？

因此`特点2`的一个重要解读是：**线程池被销毁时也可以提交下一个任务**。而且刚才提过，还没有被执行的任务，可以通过我们传入的pending()函数拿回来。

简单看看销毁时的严谨做法：

```text
static void __thrdpool_terminate(int in_pool, thrdpool_t *pool)                 
{                                                                          
    pthread_cond_t term = PTHREAD_COND_INITIALIZER;
    pthread_mutex_lock(&pool->mutex);
    // 1. 加锁设置标志位，之后的添加任务不会被执行，但可以pending拿到
    pool->terminate = &term;
    // 2. 广播所有等待的消费者
    pthread_cond_broadcast(&pool->cond); 
                                                                            
    if (in_pool) // 3. 这里的魔法等下讲>_<~
    {                                                                           
        /* Thread pool destroyed in a pool thread is legal. */                  
        pthread_detach(pthread_self());                                         
        pool->nthreads--;                                                       
    }                                                                           
    // 4. 如果还有线程没有退完，我会等，注意这里是while
    while (pool->nthreads > 0) 
        pthread_cond_wait(&term, &pool->mutex);                                 

    pthread_mutex_unlock(&pool->mutex);
    
    // 5.同样地等待打算退出的上一个人                
    if (memcmp(&pool->tid, &__zero_tid, sizeof (pthread_t)) != 0)               
        pthread_join(pool->tid, NULL); 
}
```

## **6.特点3：同样可以在线程任务里销毁这个线程池**

既然线程任务可以做任何事情，理论上，**线程任务也可以销毁线程池**❓

作为一个逻辑完备的线程池，大胆一点，我们把问号去掉。

而且，**销毁并不会结束当前任务，**
**它会等这个任务执行完**。

想象一下，刚才的**__thrdpool_routine()**，**while(1)**里拿出来的那个任务，做的事情竟然是发起**thrdpool_destroy().**..

把上面的图大胆改一下：

![img](https://pic2.zhimg.com/80/v2-f92d5821fa9beb009af7b471edd865ad_720w.webp)

我们让一个routine来destroy线程池

如果发起销毁的人，**是我们自己内部的线程**，那么我们就不是等**n**个，而是等**n-1**，少了一个外部线程等待我们。如何实现才能让这些逻辑都完美融合呢？我们把刚才跳过的**三段魔法串起来**看看。

**第一段魔法，销毁的发起者。**

如果发起销毁的人是线程池内部的线程，那么它具有较强的自我管理意识

（因为前面说了，会等它这个任务执行完）而我们可以放心大胆地**pthread_detach**，无需任何人join它等待它结束。

```text
static void __thrdpool_terminate(int in_pool, thrdpool_t *pool)                 
{ 
    ...
    // 每个由线程池创建的线程都设置了一个key，由此判断是否是in_pool
    if (in_pool) 
    {                                                                           
        /* Thread pool destroyed in a pool thread is legal. */                  
        pthread_detach(pthread_self());
        pool->nthreads--;
    }        
```

**第二段魔法：线程池谁来free？**

一定是发起销毁的那个人。所以这里用**in_pool**来判断是否是外部的人：

```text
void thrdpool_destroy(void (*pending)(const struct thrdpool_task *),
                                       thrdpool_t *pool)
{
    // 已经调用完第一段，且挨个pending(未执行的task)了
    // 销毁其他内部分配的内存
    ...
    // 如果不是内部线程发起的销毁，要负责回收线程池内存
    if (!in_pool) 
        free(pool);
}
```

**那现在不是main线程发起的销毁呢**？发起的销毁的那个内部线程，怎么能保证我可以在最后关头**把所有资源回收干净、调free(pool)、最后功成身退呢**？

在前面阅读源码第5步，其实我们看过，**__thrdpool_routine()里有free的地方。**

于是现在三段魔法终于串起来了。

**第三段魔法：严谨的并发。**

```text
static void *__thrdpool_routine(void *arg)
{
    while (1) 
    {   // ...
        task_routine(task_context); // 如果routine里做的事情，是销毁线程池...
        // 注意这个时候，其他内存都已经被destroy里清掉了，万万不可以再用什么mutex、cond
        if (pool->nthreads == 0) 
        {
            /* Thread pool was destroyed by the task. */
            free(pool);
            return NULL;                                            
        }
        ...
```

非常重要的一点，**由于并发，我们是不知道谁先操作的。假设我们稍微改一改这个顺序，就又是另一番逻辑**。

比如我作为一个内部线程，在**routine()**里调用**destroy()**期间，发现还有线程没有执行完，我就要等在我的terminate上，待最后看到**nthreads==0**的那个人叫醒我。

然后，我的代码继续执行，函数栈就会从**destroy()**回到**routine()**，也就是上面那几行，再然后就可以**free(pool)**;由于这时候我已经放飞自我detach了，于是一切顺利结束。

你看，无论如何都可以完美地销毁线程池：

![img](https://pic3.zhimg.com/80/v2-91d43dbfbeff3147ac7554b54cd102e6_720w.webp)

并发是复杂多变的，代码是简洁统一的

是不是**太妙了**！我写到这里已经要感动哭了！

## **7. 简单的用法**

这个线程池只有两个文件:`thrdpool.h` 和 `thrdpool.c`，而且只依赖内核的数据结构`list.h`。我们把它拿出来玩，自己写一段代码：

```text
void my_routine(void *context)                                                   
{
   // 我们要执行的函数                    
    printf("task-%llu start.\n", reinterpret_cast<unsigned long long>(context); );
}                                                                               
                                                                                
void my_pending(const struct thrdpool_task *task) 
{
  // 线程池销毁后，没执行的任务会到这里
    printf("pending task-%llu.\n", reinterpret_cast<unsigned long long>(task->context););                                    
} 

int main()                                                                         
{
    thrdpool_t *thrd_pool = thrdpool_create(3, 1024); // 创建                            
    struct thrdpool_task task;
    unsigned long long i;
                               
    for (i = 0; i < 5; i++)
    {
        task.routine = &my_routine;                                             
        task.context = reinterpret_cast<void *>(i);                             
        thrdpool_schedule(&task, thrd_pool); // 调用
    }
    getchar(); // 卡住主线程，按回车继续
    thrdpool_destroy(&my_pending, thrd_pool); // 结束
    return 0;                                                                   
} 
```

再打印几行log，直接编译就可以跑起来：

![img](https://pic1.zhimg.com/80/v2-4f67da756812009525d50964401e67c8_720w.webp)

妈妈再也不用担心我的C语言作业

简单程度堪比大一上学期C语言作业。

## **8.并发与结构之美**

最后谈谈感受。

看完之后我很后悔为什么没有早点看为什么不早点就可以获得知识的感觉，并且在浅层看懂之际，我知道自己肯定没有完全理解到里边的精髓，毕竟我不能**深刻地理解到设计者当时对并发的构思和模型上的选择**。

我只能说，没有十多年**顶级的系统调用和并发编程的功底**写不出这样的代码，没有**极致的审美与对品控的偏执**也写不出这样的代码。

**并发编程**有很多说道，就正如退出这个这么简单的事情，想要做到退出时回收干净却很难。如果说你写业务逻辑自己管线程，退出什么的sleep(1)都无所谓，**但如果说做框架的人不能把自己的框架做得完美无暇逻辑自洽**
**就难免让人感觉差点意思**。

而这个thrdpool，它作为一个线程池，是如此的**逻辑完备**，**用最对称简洁的方式去面对复杂的并发**。

**再次让我深深地感到震撼：我们身边那些原始的、底层的、基础的代码，还有很多新思路，还可以写得如此美。**

Workflow项目源码地址**：**
**[https://github.com/sogou/workflow](https://link.zhihu.com/?target=https%3A//github.com/sogou/workflow)**

原文地址：https://zhuanlan.zhihu.com/p/515224253

作者：linux

# 【NO.340】从一次线上问题说起，详解 TCP 半连接队列、全连接队列

## 1.**前言**

某次大促值班 ing，对系统稳定性有着充分信心、心态稳如老狗的笔者突然收到上游反馈有万分几的概率请求我们 endpoint 会出现 Connection timeout 。此时系统侧的 apiserver 集群水位在 40%，离极限水位还有着很大的距离，当时通过紧急扩容 apiserver 集群后错误率降为了 0。事后进行了详细的问题排查，定位分析到问题根因出现在系统连接队列被打满导致，之前笔者对 TCP 半连接队列、全连接队列不太了解，只依稀记得 《TCP/IP 详解》中好像有好像提到过这两个名词。

目前网上相关资料都比较零散，并且有些是过时或错误的结论，笔者在调查问题时踩了很多坑。痛定思痛，笔者查阅了大量资料并做了众多实验进行验证，梳理了这篇 TCP 半连接队列、全连接详解，当你细心阅读完这篇文章后相信你可以对 TCP 半连接队列、全连接队列有更充分的认识。

本篇文章将结合理论知识、内核代码、操作实验为你呈现如下内容：

- 半连接队列、全连接队列介绍
- 常用命令介绍
- 全连接队列实战 —— 最大长度控制、全连接队列溢出实验、实验结果分析...
- 半连接队列实战 —— 最大长度控制、半连接队列溢出实验、实验结果分析...
- ...

**半连接队列、全连接队列**

![img](https://pic2.zhimg.com/80/v2-e21cdf919369e042376c3dda64220c15_720w.webp)

在 TCP 三次握手的过程中，Linux 内核会维护两个队列，分别是：

- 半连接队列 (SYN Queue)
- 全连接队列 (Accept Queue)

正常的 TCP 三次握手过程：

1、Client 端向 Server 端发送 SYN 发起握手，Client 端进入 SYN_SENT 状态

2、Server 端收到 Client 端的 SYN 请求后，Server 端进入 SYN_RECV 状态，此时内核会**将连接存储到半连接队列(SYN Queue)**，并向 Client 端回复 SYN+ACK

3、Client 端收到 Server 端的 SYN+ACK 后，Client 端回复 ACK 并进入 ESTABLISHED 状态

4、Server 端收到 Client 端的 ACK 后，内核**将连接从半连接队列(SYN Queue)中取出，添加到全连接队列(Accept Queue)**，Server 端进入 ESTABLISHED 状态

5、Server 端应用进程**调用 accept 函数时，将连接从全连接队列(Accept Queue)中取出**

**半连接队列和全连接队列都有长度大小限制，超过限制时内核会将连接 Drop 丢弃或者返回 RST 包。**

## 2.**相关指标查看**

**ss 命令**

通过 ss 命令可以查看到全连接队列的信息

```text
# -n 不解析服务名称
# -t 只显示 tcp sockets
# -l 显示正在监听(LISTEN)的 sockets

$ ss -lnt
State      Recv-Q Send-Q    Local Address:Port         Peer Address:Port
LISTEN     0      128       [::]:2380                  [::]:*
LISTEN     0      128       [::]:80                    [::]:*
LISTEN     0      128       [::]:8080                  [::]:*
LISTEN     0      128       [::]:8090                  [::]:*

$ ss -nt
State      Recv-Q Send-Q    Local Address:Port         Peer Address:Port
ESTAB      0      0         [::ffff:33.9.95.134]:80                   [::ffff:33.51.103.59]:47452
ESTAB      0      536       [::ffff:33.9.95.134]:80                  [::ffff:33.43.108.144]:37656
ESTAB      0      0         [::ffff:33.9.95.134]:80                   [::ffff:33.51.103.59]:38130
ESTAB      0      536       [::ffff:33.9.95.134]:80                   [::ffff:33.51.103.59]:38280
ESTAB      0      0         [::ffff:33.9.95.134]:80                   [::ffff:33.51.103.59]:38204
```

**对于 LISTEN 状态的 socket**

- **Recv-Q：**当前全连接队列的大小，即已完成三次握手等待应用程序 accept() 的 TCP 链接
- **Send-Q：**全连接队列的最大长度，即全连接队列的大小

**对于非 LISTEN 状态的 socket**

- **Recv-Q：**已收到但未被应用程序读取的字节数
- **Send-Q：**已发送但未收到确认的字节数

相关内核代码：

```text
// https://github.com/torvalds/linux/blob/master/net/ipv4/tcp_diag.c
static void tcp_diag_get_info(struct sock *sk, struct inet_diag_msg *r,
            void *_info)
{
  struct tcp_info *info = _info;

  if (inet_sk_state_load(sk) == TCP_LISTEN) { // socket 状态是 LISTEN 时
    r->idiag_rqueue = READ_ONCE(sk->sk_ack_backlog);  // 当前全连接队列大小
    r->idiag_wqueue = READ_ONCE(sk->sk_max_ack_backlog); // 全连接队列最大长度
  } else if (sk->sk_type == SOCK_STREAM) {    // socket 状态不是 LISTEN 时
    const struct tcp_sock *tp = tcp_sk(sk);

    r->idiag_rqueue = max_t(int, READ_ONCE(tp->rcv_nxt) -
               READ_ONCE(tp->copied_seq), 0);    // 已收到但未被应用程序读取的字节数
    r->idiag_wqueue = READ_ONCE(tp->write_seq) - tp->snd_una;   // 已发送但未收到确认的字节数
  }
  if (info)
    tcp_get_info(sk, info);
}
```

**netstat 命令**通过netstat -s命令可以查看 TCP 半连接队列、全连接队列的溢出情况

```text
$ netstat -s | grep -i "listen"
    189088 times the listen queue of a socket overflowed
    30140232 SYNs to LISTEN sockets dropped
```

上面输出的数值是累计值，分别表示有多少 TCP socket 链接因为全连接队列、半连接队列满了而被丢弃

- 189088 times the listen queue of a socket overflowed 代表有 189088 次全连接队列溢出
- 30140232 SYNs to LISTEN sockets dropped 代表有 30140232 次半连接队列溢出

在排查线上问题时，**如果一段时间内相关数值一直在上升，则表明半连接队列、全连接队列有溢出情况**

## 3.**实战 —— 全连接队列**

**全连接队列最大长度控制**TCP 全连接队列的最大长度由min(somaxconn, backlog)控制，其中：

- somaxconn 是 Linux 内核参数，由 /proc/sys/net/core/somaxconn 指定
- backlog 是 TCP 协议中 listen 函数的参数之一，即 int listen(int sockfd, int backlog) 函数中的 backlog 大小。**在 Golang 中，listen 的 backlog 参数使用的是****/proc/sys/net/core/somaxconn****文件中的值**。

相关内核代码：

```text
// https://github.com/torvalds/linux/blob/master/net/socket.c

/*
 *  Perform a listen. Basically, we allow the protocol to do anything
 *  necessary for a listen, and if that works, we mark the socket as
 *  ready for listening.
 */
int __sys_listen(int fd, int backlog)
{
  struct socket *sock;
  int err, fput_needed;
  int somaxconn;

  sock = sockfd_lookup_light(fd, &err, &fput_needed);
  if (sock) {
    somaxconn = sock_net(sock->sk)->core.sysctl_somaxconn;  // /proc/sys/net/core/somaxconn
    if ((unsigned int)backlog > somaxconn)
      backlog = somaxconn;   // TCP 全连接队列最大长度 min(somaxconn, backlog)

    err = security_socket_listen(sock, backlog);
    if (!err)
      err = sock->ops->listen(sock, backlog);

    fput_light(sock->file, fput_needed);
  }
  return err;
}
```

**实验**

服务端 server 代码

```text
package main

import (
  "log"
  "net"
  "time"
)

func main() {
  l, err := net.Listen("tcp", ":8888")
  if err != nil {
    log.Printf("failed to listen due to %v", err)
  }
  defer l.Close()
  log.Println("listen :8888 success")

  for {
    time.Sleep(time.Second * 100)
  }
}
```

在测试环境查看 somaxconn 的值为 128

```text
$ cat /proc/sys/net/core/somaxconn
128
```

启动服务端，通过ss -lnt | grep :8888确认全连接队列大小

```text
LISTEN     0      128       [::]:8888                  [::]:*
```

全连接队列最大长度为 128现在更新 somaxconn 值为 1024，再重新启动服务端。1、更新/etc/sysctl.conf文件，该文件为内核参数配置文件a.新增一行 net.core.somaxconn=1024 2、执行 sysctl -p使配置生效

```text
$ sudo sysctl -p
net.core.somaxconn = 1024
```

3、检查/proc/sys/net/core/somaxconn文件，确认 somaxconn 为更新后的 1024

```text
$ cat /proc/sys/net/core/somaxconn
1024
```

重新启动服务端， 通过ss -lnt | grep :8888确认全连接队列大小

```text
$ ss -lnt | grep 8888
LISTEN     0      1024      [::]:8888                  [::]:*
```

可以看到，现在全链接队列最大长度为 1024，成功更新。**全连接队列溢出**下面来验证下全连接队列溢出会发生什么情况，可以通过让服务端应用只负责 Listen 对应端口而不执行accept()TCP 连接，使 TCP 全连接队列溢出。

**实验物料**

服务端 server 代码

```text
// server 端监听 8888 tcp 端口

package main

import (
  "log"
  "net"
  "time"
)

func main() {
  l, err := net.Listen("tcp", ":8888")
  if err != nil {
    log.Printf("failed to listen due to %v", err)
  }
  defer l.Close()
  log.Println("listen :8888 success")

  for {
    time.Sleep(time.Second * 100)
  }
}
```

客户端 client 代码

```text
// client 端并发请求 10 次 server 端，成功建立 tcp 连接后向 server 端发送数据
package main

import (
  "context"
  "log"
  "net"
  "os"
  "os/signal"
  "sync"
  "syscall"
  "time"
)

var wg sync.WaitGroup

func establishConn(ctx context.Context, i int) {
  defer wg.Done()
  conn, err := net.DialTimeout("tcp", ":8888", time.Second*5)
  if err != nil {
    log.Printf("%d, dial error: %v", i, err)
    return
  }
  log.Printf("%d, dial success", i)
  _, err = conn.Write([]byte("hello world"))
  if err != nil {
    log.Printf("%d, send error: %v", i, err)
    return
  }
  select {
  case <-ctx.Done():
    log.Printf("%d, dail close", i)
  }
}

func main() {
  ctx, cancel := context.WithCancel(context.Background())
  for i := 0; i < 10; i++ {
    wg.Add(1)
    go establishConn(ctx, i)
  }

  go func() {
    sc := make(chan os.Signal, 1)
    signal.Notify(sc, syscall.SIGINT)
    select {
    case <-sc:
      cancel()
    }
  }()

  wg.Wait()
  log.Printf("client exit")
}
```

为了方便实验，将 somaxconn 全连接队列最大长度更新为 5：

1、更新/etc/sysctl.conf文件，将net.core.somaxconn更新为 52、执行sysctl -p使配置生效

```text
$ sudo sysctl -p
net.core.somaxconn = 5
```

**实验结果**

**客户端日志输出**

```text
2021/10/11 17:24:48 8, dial success
2021/10/11 17:24:48 3, dial success
2021/10/11 17:24:48 4, dial success
2021/10/11 17:24:48 6, dial success
2021/10/11 17:24:48 5, dial success
2021/10/11 17:24:48 2, dial success
2021/10/11 17:24:48 1, dial success
2021/10/11 17:24:48 0, dial success
2021/10/11 17:24:48 7, dial success
2021/10/11 17:24:53 9, dial error: dial tcp 33.9.192.157:8888: i/o timeout
```

**客户端 socket 情况**

```text
tcp        0      0 33.9.192.155:40372      33.9.192.157:8888       ESTABLISHED
tcp        0      0 33.9.192.155:40376      33.9.192.157:8888       ESTABLISHED
tcp        0      0 33.9.192.155:40370      33.9.192.157:8888       ESTABLISHED
tcp        0      0 33.9.192.155:40366      33.9.192.157:8888       ESTABLISHED
tcp        0      0 33.9.192.155:40374      33.9.192.157:8888       ESTABLISHED
tcp        0      0 33.9.192.155:40368      33.9.192.157:8888       ESTABLISHED
```

**服务端 socket 情况**

```text
tcp6      11      0 33.9.192.157:8888       33.9.192.155:40376      ESTABLISHED
tcp6      11      0 33.9.192.157:8888       33.9.192.155:40370      ESTABLISHED
tcp6      11      0 33.9.192.157:8888       33.9.192.155:40368      ESTABLISHED
tcp6      11      0 33.9.192.157:8888       33.9.192.155:40372      ESTABLISHED
tcp6      11      0 33.9.192.157:8888       33.9.192.155:40374      ESTABLISHED
tcp6      11      0 33.9.192.157:8888       33.9.192.155:40366      ESTABLISHED

tcp    LISTEN     6      5      [::]:8888               [::]:*      
```

**抓包结果**对客户端、服务端抓包后，**发现出现了三种情况，分别是：**

- client 成功与 server 端建立 tcp socket 连接，发送数据成功
- client 认为成功与 server 端建立 tcp socket 连接，发送数据失败，一直在 RETRY；server 端认为 tcp 连接未建立，一直在发送 SYN+ACK
- client 向 server 发送 SYN 未得到响应，一直在 RETRY

**全连接队列实验结果分析**上述实验结果出现了三种情况，我们分别对抓包内容进行分析

**情况一：Client 成功与 Server 端建立 tcp socket 链接，发送数据成功**

![img](https://pic2.zhimg.com/80/v2-8b7d194ffd3159d0476b748de85d60d1_720w.webp)

上图可以看到如下请求：

- Client 端向 Server 端发送 SYN 发起握手
- Server 端收到 Client 端 SYN 后，向 Client 端回复 SYN+ACK，**socket 连接存储到半连接队列(SYN Queue)**
- Client 端收到 Server 端 SYN+ACK 后，向 Server 端回复 ACK，Client 端进入 ESTABLISHED 状态
- Server 端收到 Client 端 ACK 后，进入 ESTABLISHED 状态，**socket 连接存储到全连接队列(Accept Queue)**
- **Client 端向 Server 端发送数据 [PSH, ACK]，Server 端确认接收到数据 [ACK]**

这种情况就是正常的请求，即全连接队列、半连接队列未满，client 成功与 server 建立了 tcp 链接，并成功发送数据。

**情况二：Client 认为成功与 Server 端建立 tcp socket 连接，后续发送数据失败，持续 RETRY；Server 端认为 TCP 连接未建立，一直在发送SYN+ACK**

![img](https://pic4.zhimg.com/80/v2-b4b6d727e400acaf3e3f4bb26c84df83_720w.webp)

上图可以看到如下请求：

- Client 端向 Server 端发送 SYN 发起握手
- Server 端收到 Client 端 SYN 后，向 Client 端回复 SYN+ACK，**socket 连接存储到半连接队列(SYN Queue)**
- Client 端收到 Server 端 SYN+ACK 后，向 Server 端回复 ACK，**Client 端进入 ESTABLISHED状态(重要：此时仅仅是 Client 端认为 tcp 连接建立成功)**
- **由于 Client 端认为 TCP 连接已经建立完成，所以向 Server 端发送数据 [PSH，ACK]，但是一直未收到 Server 端的确认 ACK，所以一直在 RETRY**
- **Server 端一直在 RETRY 发送 SYN+ACK**

为什么会出现上述情况？Server 端为什么一直在 RETRY 发送 SYN+ACK？Server 端不是已经收到了 Client 端的 ACK 确认了吗？**上述情况是由于 Server 端 socket 连接进入了半连接队列，在收到 Client 端 ACK 后，本应将 socket 连接存储到全连接队列，但是全连接队列已满，所以 Server 端 DROP 了该 ACK 请求。之所以 Server 端一直在 RETRY 发送 SYN+ACK，是因为 DROP 了 client 端的 ACK 请求，所以 socket 连接仍旧在半连接队列中，等待 Client 端回复 ACK。tcp_abort_on_overflow 参数控制全连接队列满DROP 请求是默认行为**，可以通过设置/proc/sys/net/ipv4/tcp_abort_on_overflow使 Server 端在全连接队列满时，向 Client 端发送 RST 报文。tcp_abort_on_overflow 有两种可选值：

- 0：如果全连接队列满了，Server 端 DROP Client 端回复的 ACK
- 1：如果全连接队列满了，Server 端向 Client 端发送 RST 报文，终止 TCP socket 链接 (TODO：后续有时间补充下该实验)

**为什么实验结果中当前全连接队列大小 > 全连接队列最大长度配置？**上述结果中可以看到 Listen 状态的 socket 链接：

- Recv-Q 当前全连接队列的大小是 6
- Send-Q 全连接队列最大长度是 5

```text
State      Recv-Q Send-Q    Local Address:Port         Peer Address:Port
LISTEN     6      5         [::]:8888                  [::]:*
```

为什么全连接队列大小 > 全连接队列最大长度配置呢？

经过多次实验发现，**能够进入全连接队列的 Socket 最大数量始终比配置的全连接队列最大长度 + 1。**结合其他文章以及内核代码，发现**内核在判断全连接队列是否满的情况下，使用的是 > 而非 >=** (具体是为什么没有找到相关资源 : ) )。相关内核代码：

```text
/* Note: If you think the test should be:
 *  return READ_ONCE(sk->sk_ack_backlog) >= READ_ONCE(sk->sk_max_ack_backlog);
 * Then please take a look at commit 64a146513f8f ("[NET]: Revert incorrect accept queue backlog changes.")
 */
static inline bool sk_acceptq_is_full(const struct sock *sk)
{
  return READ_ONCE(sk->sk_ack_backlog) > READ_ONCE(sk->sk_max_ack_backlog);
}
```

**情况三：Client 向 Server 发送 SYN 未得到相应，一直在 RETRY**

![img](https://pic4.zhimg.com/80/v2-ea7771215d3a5f870cce2c0a89e4c62b_720w.webp)

上图可以看到如下请求：

- Client 端向 Server 端发送 SYN 发起握手，未得到 Server 回应，一直在 RETRY

（这种情况涉及到半连接队列，这里先给上述情况发生的原因结论，具体内容将在下文半连接队列中展开。）

**发生上述情况的原因由以下两方面导致：**

1、开启了/proc/sys/net/ipv4/tcp_syncookies功能

2、全连接队列满了

## 4.**实战 —— 半连接队列**

**半连接队列最大长度控制**翻阅了很多博文，查找关于半连接队列最大长度控制的相关内容，大多含糊其辞或不准确，经过不懈努力，最终找到了比较确切的内容。

很多博文中说半连接队列最大长度由/proc/sys/net/ipv4/tcp_max_syn_backlog参数指定，**实际上只有在 linux 内核版本小于2.6.20时，半连接队列才等于 backlog 的大小**。

这块的源码比较复杂，这里给一下大体的计算方式，详细的内容可以参考附录中的相关博文。半连接队列长度的计算过程：

```text
bbacklog = min(somaxconn, backlog)
nr_table_entries = backlog
nr_table_entries = min(backlog, sysctl_max_syn_backlog)
nr_table_entries = max(nr_table_entries, 8)
// roundup_pow_of_two: 将参数向上取整到最小的 2^n，注意这里存在一个 +1
nr_table_entries = roundup_pow_of_two(nr_table_entries + 1)
max_qlen_log = max(3, log2(nr_table_entries))
max_queue_length = 2^max_qlen_log
```

可以看到，**半连接队列的长度由三个参数指定：**

- 调用 listen 时，传入的 backlog
- /proc/sys/net/core/somaxconn 默认值为 128
- /proc/sys/net/ipv4/tcp_max_syn_backlog 默认值为 1024

我们假设listen传入的 backlog = 128 (Golang 中调用 listen 时传递的 backlog 参数使用的是/proc/sys/net/core/somaxconn)，其他配置采用默认值，来计算下半连接队列的最大长度

```text
backlog = min(somaxconn, backlog) = min(128, 128) = 128
nr_table_entries = backlog = 128
nr_table_entries = min(backlog, sysctl_max_syn_backlog) = min(128, 1024) = 128
nr_table_entries = max(nr_table_entries, 8) = max(128, 8) = 128
nr_table_entries = roundup_pow_of_two(nr_table_entries + 1) = 256
max_qlen_log = max(3, log2(nr_table_entries)) = max(3, 8) = 8
max_queue_length = 2^max_qlen_log = 2^8 = 256
```

可以得到半队列大小是 256。

**判断是否 Drop SYN 请求**当 Client 端向 Server 端发送 SYN 报文后，Server 端会将该 socket 连接存储到半连接队列(SYN Queue)，如果 Server 端判断半连接队列满了则会将连接 Drop 丢弃。

那么 Server 端是如何判断半连接队列是否满的呢？除了上面一小节提到的半连接队列最大长度控制外，还和 /proc/sys/net/ipv4/tcp_syncookies 参数有关。(tcp_syncookies 的作用是为了防止 SYN Flood 攻击的，下文会给出相关链接介绍)

**流程图**

判断是否 Drop SYN 请求的流程图：

![img](https://pic1.zhimg.com/80/v2-6eca09cd764b14ed3ffcc7e1e8617148_720w.webp)

上图是整理了多份资料后，整理出来的判断是否 Drop SYN 请求的流程图。

**注意：第一个判断条件 「当前半连接队列是否已超过半连接队列最大长度」在不同内核版本中的判断不一样，Linux4.19.91 内核判断的是当前半连接队列长度是否 >= 全连接队列最大长度。**

相关内核代码：

```text
static inline int inet_csk_reqsk_queue_is_full(const struct sock *sk)
{
  return inet_csk_reqsk_queue_len(sk) >= sk->sk_max_ack_backlog;
}
```

我们假设如下参数，来计算下当 Client 端只发送 SYN 包，理论上 Server 端何时会 Drop SYN 请求：

- 调用 listen 时传入的 backlog = 1024
- /proc/sys/net/core/somaxconn 值为 1024
- /proc/sys/net/ipv4/tcp_max_syn_backlog 值为 128

当 /proc/sys/net/ipv4/tcp_syncookies 值为 0 时

- 计算出的半连接队列最大长度为 256
- 当半连接队列长度增长至 96 后，再新增 SYN 请求，就会触发 Drop SYN 请求

当/proc/sys/net/ipv4/tcp_syncookies值为 1 时1.计算出的半连接队列最大长度为 2562.由于开启了tcp_syncookies

- 当全连接队列未满时，永远不会 Drop 请求 (注意：经实验发现这个理论是错误的，实验发现只要半连接队列的大小 > 全连接队列最大长度就会触发 Drop SYN 请求)
- 当全连接队列满了后，即全连接队列大小到 1024 后，就会触发 Drop SYN 请求

PS：/proc/sys/net/ipv4/tcp_syncookies的取值还可以为 2，笔者没有详细实验。

**回顾全连接队列实验结果**

在上文全连接队列实验中，有一类实验结果是：client 向 Server 发送 SYN 未得到响应，一直在 RETRY。

**发生上述情况的原因由以下两方面导致：**

\1. 开启了/proc/sys/net/ipv4/tcp_syncookies功能

\2. 全连接队列满了

**半连接队列溢出实验**

上文我们已经知道如何计算理论上半连接队列何时会溢出，下面我们来具体实验下(Golang 调用 listen 时传入的 backlog 值为 somaxconn)

**实验一：syncookies=0，somaxconn=1024，tcp_max_syn_backlog=128**

理论上：

- 计算出的半连接队列最大长度为 256
- 当半连接队列长度增长至 96 后，后续 SYN 请求就会触发 Drop

将相关参数的配置更新

```text
$ sudo sysctl -p
net.core.somaxconn = 1024
net.ipv4.tcp_max_syn_backlog = 128
net.ipv4.tcp_syncookies = 0
```

启动服务端 Server 监听 8888 端口(代码参考全连接队列实验物料)客户端 Client 发起 SYN Flood 攻击：

```text
$ sudo hping3 -S 33.9.192.157 -p 8888 --flood
HPING 33.9.192.157 (eth0 33.9.192.157): S set, 40 headers + 0 data bytes
hping in flood mode, no replies will be shown
```

查看服务端 Server 8888端口处于 SYN_RECV 状态的 socket 最大个数：

```text
[zechen.hg@function-compute033009192157.na63 /home/zechen.hg]
$ sudo netstat -nat | grep :8888 | grep SYN_RECV  | wc -l
96

[zechen.hg@function-compute033009192157.na63 /home/zechen.hg]
$ sudo netstat -nat | grep :8888 | grep SYN_RECV  | wc -l
96
```

实验结果符合预期，当半连接队列长度增长至 96 后，后续 SYN 请求就会触发 Drop。

**实验二：syncookies = 0，somaxconn=128，tcp_max_syn_backlog=512**

理论上：

- 计算出的半连接队列最大长度为 256，由于笔者实验机器上的**内核版本是 4.19.91，所以当半连接队列长度 >= 全连接队列最大长度时**，内核就认为半连接队列溢出了
- 所以当半连接队列长度增长至 128 后，后续 SYN 请求就会触发 DROP

将相关参数的配置更新

```text
$ sudo sysctl -p
net.core.somaxconn = 128
net.ipv4.tcp_max_syn_backlog = 512
net.ipv4.tcp_syncookies = 0
```

启动服务端 Server 监听 8888 端口(代码参考全连接队列实验物料) 客户端 Client 发起 SYN Flood 攻击：

```text
$ sudo hping3 -S 33.9.192.157 -p 8888 --flood
HPING 33.9.192.157 (eth0 33.9.192.157): S set, 40 headers + 0 data bytes
hping in flood mode, no replies will be shown
```

查看服务端 Server 8888端口处于 SYN_RECV 状态的 socket 最大个数：

```text
[zechen.hg@function-compute033009192157.na63 /home/zechen.hg]
$ sudo netstat -nat | grep :8888 | grep SYN_RECV  | wc -l
128

[zechen.hg@function-compute033009192157.na63 /home/zechen.hg]
$ sudo netstat -nat | grep :8888 | grep SYN_RECV  | wc -l
128
```

实验结果符合预期，当半连接队列长度增长至 128 后，后续 SYN 请求就会触发 Drop

**实验三：syncookies = 1，somaxconn=128，tcp_max_syn_backlog=512**

理论上：

- 当全连接队列未满，syncookies = 1，理论上 SYN 请求永远不会被 Drop

将相关参数的配置更新

```text
$ sudo sysctl -p
net.core.somaxconn = 128
net.ipv4.tcp_max_syn_backlog = 512
net.ipv4.tcp_syncookies = 1
```

启动服务端 Server 监听 8888 端口(代码参考全连接队列实验物料)客户端 Client 发起 SYN Flood 攻击：

```text
$ sudo hping3 -S 33.9.192.157 -p 8888 --flood
HPING 33.9.192.157 (eth0 33.9.192.157): S set, 40 headers + 0 data bytes
hping in flood mode, no replies will be shown
```

查看服务端 Server 8888端口处于 SYN_RECV 状态的 socket 最大个数：

```text
[zechen.hg@function-compute033009192157.na63 /home/zechen.hg]
$ sudo netstat -nat | grep :8888 | grep SYN_RECV  | wc -l
128

[zechen.hg@function-compute033009192157.na63 /home/zechen.hg]
$ sudo netstat -nat | grep :8888 | grep SYN_RECV  | wc -l
128
```

**实验发现即使syncookies=1，当半连接队列长度 > 全连接队列最大长度时，就会触发 DROP SYN 请求！！！(TODO：有时间阅读下相关内核源码，再分析下)**

继续做实验，将 somaxconn 更新为 5

```text
$ sudo sysctl -p
net.core.somaxconn = 5
net.ipv4.tcp_max_syn_backlog = 512
net.ipv4.tcp_syncookies = 1
```

发起 SYN Flood 攻击后，查看服务端 Server 8888端口处于 SYN_RECV 状态的 socket 最大个数：

```text
[zechen.hg@function-compute033009192157.na63 /home/zechen.hg]
$ sudo netstat -nat | grep :8888 | grep SYN_RECV  | wc -l
5

[zechen.hg@function-compute033009192157.na63 /home/zechen.hg]
$ sudo netstat -nat | grep :8888 | grep SYN_RECV  | wc -l
5
```

确实 即使 syncookies=1，当半连接队列长度 > 全连接最大长度时，就会触发 DROP SYN 请求。

**实验四：syncookies = 1，somaxconn=256，tcp_max_syn_backlog=128**

理论上：

- 当半连接队列大小到 256 后，后触发 DROP SYN 请求

将相关参数的配置更新

```text
$ sudo sysctl -p
net.core.somaxconn = 256
net.ipv4.tcp_max_syn_backlog = 128
net.ipv4.tcp_syncookies = 1
```

启动服务端 Server 监听 8888 端口(代码参考全连接队列实验物料)。

客户端 Client 发起 SYN Flood 攻击:

```text
$ sudo hping3 -S 33.9.192.157 -p 8888 --flood
HPING 33.9.192.157 (eth0 33.9.192.157): S set, 40 headers + 0 data bytes
hping in flood mode, no replies will be shown
```

查看服务端 Server 8888端口处于 SYN_RECV 状态的 socket 最大个数：

```text
[zechen.hg@function-compute033009192157.na63 /home/zechen.hg]
$ sudo netstat -nat | grep :8888 | grep SYN_RECV  | wc -l
256

[zechen.hg@function-compute033009192157.na63 /home/zechen.hg]
$ sudo netstat -nat | grep :8888 | grep SYN_RECV  | wc -l
256
```

实验结果符合预期，当半连接队列长度增长至 256 后，后续 SYN 请求就会触发 Drop。

## 5.**回顾线上问题**

再回顾值班时遇到的 Connection timeout 问题，当时相关系统参数配置为：

- net.core.somaxconn = 128
- net.ipv4.tcp_max_syn_backlog = 512
- net.ipv4.tcp_syncookies = 1
- net.ipv4.tcp_abort_on_overflow = 0

所以出现 Connection timeout 有两种可能情况：

1、半连接队列未满，全连接队列满，Client 端向 Server 端发起 SYN 被 DROP (参考全连接队列实验结果情况三分析、半连接队列溢出实验情况三)

2、全连接队列未满，半连接队列大小超过全链接队列最大长度(参考半连接队列溢出实验情况三、半连接队列溢出实验情况四)

问题的最快修复方式是将 net.core.somaxconn 调大，以及 net.ipv4.tcp_abort_on_overflow 设置为 1，net.ipv4.tcp_abort_on_overflow 设置为 1 是为了让 client fail fast。

## 6.**总结**

半连接队列溢出、全连接队列溢出这类问题很容易被忽略，同时这类问题又很致命。当半连接队列、全连接队列溢出时 Server 端，从监控上来看系统 cpu 水位、内存水位、网络连接数等一切正常，然而却会持续影响 Client 端业务请求。对于高负载上游使用短连接的情况，出现这类问题的可能性更大。

本文详细梳理了 TCP 半连接队列、全连接队列的理论知识，同时结合 Linux 相关内核代码以及详细的动手实验，讲解了 TCP 半连接队列、全连接队列的相关原理、溢出判断、问题分析等内容，希望大家在阅读后可以对 TCP 半连接队列、全连接队列有更充分的认识。

原文地址：https://zhuanlan.zhihu.com/p/514391329

作者：linux

# 【NO.341】浅析进程间通信的几种方式（含实例源码）

## 1.为什么进程间需要通信？

1).数据传输

一个进程需要将它的数据发送给另一个进程;

2).资源共享

多个进程之间共享同样的资源;

3).通知事件

一个进程需要向另一个或一组进程发送消息，通知它们发生了某种事件;

4).进程控制

有些进程希望完全控制另一个进程的执行(如Debug进程)，该控制进程希望能够拦截另一个进程的所有操作，并能够及时知道它的状态改变。

基于以上几个原因，所以就有了进程间通信的概念，那仫进程间通信的原理是什仫呢？目前有哪几种进程间通信的机制？他们是如何实现进程间通信的呢？在这篇文章中我会就这几个问题进行详细的讲解。

## 2.进程间通信的原理

每个进程各自有不同的用户地址空间,任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核,在内核中开辟一块缓冲区,进程1把数据从用户空间拷到内核缓冲区,进程2再从内核缓冲区把数据读走,内核提供的这种机制称为进程间通信机制。

主要的过程如下图所示：



![img](https://pic3.zhimg.com/80/v2-19d4ac7dccc826737fab858d8e84de52_720w.webp)

## 3.进程间通信的几种方式

### 3.1 管道(pipe)

管道又名匿名管道，这是一种最基本的IPC机制，由pipe函数创建：

> \#include <unistd.h>
> int pipe(int pipefd[2]);

返回值：成功返回0，失败返回-1；

调用pipe函数时在内核中开辟一块缓冲区用于通信,它有一个读端，一个写端：pipefd[0]指向管道的读端，pipefd[1]指向管道的写端。所以管道在用户程序看起来就像一个打开的文件,通过read(pipefd[0])或者write(pipefd[1])向这个文件读写数据，其实是在读写内核缓冲区。

使用管道的通信过程：



![img](https://pic3.zhimg.com/80/v2-1bb5c4fb3afa6e16259816dda05d756e_720w.webp)







![img](https://pic4.zhimg.com/80/v2-32ffb053af31b54b2eede40c3a34f40b_720w.webp)







![img](https://pic3.zhimg.com/80/v2-46a0bc1d51bccdeead5b65aee2a9c66e_720w.webp)









1.父进程调用pipe开辟管道,得到两个文件描述符指向管道的两端。

2.父进程调用fork创建子进程,那么子进程也有两个文件描述符指向同一管道。

3.父进程关闭管道读端,子进程关闭管道写端。父进程可以往管道里写,子进程可以从管道里读,管道是用环形队列实现的,数据从写端流入从读端流出,这样就实现了进程间通信。

管道出现的四种特殊情况：

1.写端关闭，读端不关闭；

那么管道中剩余的数据都被读取后,再次read会返回0,就像读到文件末尾一样。

2.写端不关闭，但是也不写数据，读端不关闭；

此时管道中剩余的数据都被读取之后再次read会被阻塞，直到管道中有数据可读了才重新读取数据并返回；

3.读端关闭，写端不关闭；

此时该进程会收到信号SIGPIPE，通常会导致进程异常终止。

4.读端不关闭，但是也不读取数据，写端不关闭；

此时当写端被写满之后再次write会阻塞，直到管道中有空位置了才会写入数据并重新返回。

使用管道的缺点：

1.两个进程通过一个管道只能实现单向通信，如果想双向通信必须再重新创建一个管道或者使用sockpair才可以解决这类问题；

2.只能用于具有亲缘关系的进程间通信，例如父子，兄弟进程。

一个简单的关于管道的例子：

代码实现如下：

> \#include<stdio.h>
> \#include<unistd.h>
> \#include<stdlib.h>
> \#include<string.h>
> int main()
> {
> int _pipe[2]={0,0};
> int ret=pipe(_pipe); //创建管道
> if(ret == -1)
> {
> perror("create pipe error");
> return 1;
> }
> printf("_pipe[0] is %d,_pipe[1] is %d\n",_pipe[0],_pipe[1]);
> pid_t id=fork(); //父进程fork子进程
> if(id < 0)
> {
> perror("fork error");
> return 2;
> }
> else if(id == 0) //child,写
> {
> printf("child writing\n");
> close(_pipe[0]);
> int count=5;
> const char *msg="i am from XATU";
> while(count--)
> {
> write(_pipe[1],msg,strlen(msg));
> sleep(1);
> }
> close(_pipe[1]);
> exit(1);
> }
> else //father,读
> {
> printf("father reading\n");
> close(_pipe[1]);
> char msg[1024];
> int count=5;
> while(count--)
> {
> ssize_t s=read(_pipe[0],msg,sizeof(msg)-1);
> if(s > 0){
> msg[s]='\0';
> printf("client# %s\n",msg);
> }
> else{
> perror("read error");
> exit(1);
> }
> }
> if(waitpid(id,0,NULL) != -1){
> printf("wait success\n");
> }
> }
> return 0;
> }





![img](https://pic1.zhimg.com/80/v2-58d405bcbe0ffc294033b7dbe3795c68_720w.webp)



### 3.2 命名管道(FIFO)

上一种进程间通信的方式是匿名的，所以只能用于具有亲缘关系的进程间通信，命名管道的出现正好解决了这个问题。FIFO不同于管道之处在于它提供一个路径名与之关联，以FIFO的文件形式存储文件系统中。命名管道是一个设备文件，因此即使进程与创建FIFO的进程不存在亲缘关系，只要可以访问该路径，就能够通过FIFO相互通信。

命名管道的创建与读写：

1).是在程序中使用系统函数建立命名管道；

2).是在Shell下交互地建立一个命名管道，Shell方式下可使用mknod或mkfifo命令来创建管道，两个函数均定义在头文件sys/stat.h中；

> \#include <sys/types.h>
> \#include <sys/stat.h>
> \#include <fcntl.h>
> \#include <unistd.h>
> int mknod(const char *pathname, mode_t mode, dev_t dev);
> \#include <sys/types.h>
> \#include <sys/stat.h>
> int mkfifo(const char *pathname, mode_t mode);

返回值：都是成功返回0，失败返回-1；

path为创建的命名管道的全路径名；

mod为创建的命名管道的模式，指明其存取权限；

dev为设备值，该值取决于文件创建的种类，它只在创建设备文件时才会用到；

mkfifo函数的作用：在文件系统中创建一个文件，该文件用于提供FIFO功能，即命名管道。

命名管道的特点：

1.命名管道是一个存在于硬盘上的文件，而管道是存在于内存中的特殊文件。所以当使用命名管道的时候必须先open将其打开。

2.命名管道可以用于任何两个进程之间的通信，不管这两个进程是不是父子进程，也不管这两个进程之间有没有关系。

一个简单的关于命名管道的例子：

代码实现如下：

server.c

> \#include<stdio.h>
> \#include<stdlib.h>
> \#include<sys/types.h>
> \#include<sys/stat.h>
> \#include<fcntl.h>
> void testserver()
> {
> int namepipe=mkfifo("myfifo",S_IFIFO|0666); //创建一个存取权限为0666的命名管道
> if(namepipe == -1){
> perror("mkfifo error");
> exit(1);
> }
> int fd=open("./myfifo",O_RDWR); //打开该命名管道
> if(fd == -1){
> perror("open error");
> exit(2);
> }
> char buf[1024];
> while(1)
> {
> printf("sendto# ");
> fflush(stdout);
> ssize_t s=read(0,buf,sizeof(buf)-1); //从标准输入获取消息
> if(s > 0){
> buf[s-1]='\0'; //过滤掉从标准输入中获取的换行
> if(write(fd,buf,s) == -1){ //把该消息写入到命名管道中
> perror("write error");
> exit(3);
> }
> }
> }
> close(fd);
> }
> int main()
> {
> testserver();
> return 0;
> }

client.c

> \#include<stdio.h>
> \#include<stdlib.h>
> \#include<sys/types.h>
> \#include<sys/stat.h>
> \#include<fcntl.h>
> void testclient()
> {
> int fd=open("./myfifo",O_RDWR);
> if(fd == -1){
> perror("open error");
> exit(1);
> }
> char buf[1024];
> while(1){
> ssize_t s=read(fd,buf,sizeof(buf)-1);
> if(s > 0){
> printf("client# %s\n",buf);
> }
> else{ //读失败或者是读取到字符结尾
> perror("read error");
> exit(2);
> }
> }
> close(fd);
> }
> int main()
> {
> testclient();
> return 0;
> }





![img](https://pic4.zhimg.com/80/v2-2cf9e404ef04c996b5179d83a37f962b_720w.webp)



### 3.3 消息队列(msg)

由于内容较多，以后再详细分享

### 3.4 信号量(sem)

什仫是信号量？

信号量的本质是一种数据操作锁，用来负责数据操作过程中的互斥，同步等功能。

信号量用来管理临界资源的。它本身只是一种外部资源的标识，不具有数据交换功能，而是通过控制其他的通信资源实现进程间通信。 可以这样理解，信号量就相当于是一个计数器。当有进程对它所管理的资源进行请求时，进程先要读取信号量的值：大于0，资源可以请求；等于0，资源不可以用，这时进程会进入睡眠状态直至资源可用。

当一个进程不再使用资源时，信号量+1(对应的操作称为V操作)，反之当有进程使用资源时，信号量-1(对应的操作为P操作)。对信号量的值操作均为原子操作。

为什仫要使用信号量？

为了防止出现因多个程序同时访问一个共享资源而引发的一系列问题，我们需要一种方法，它可以通过生成并使用令牌来授权，在任一时刻只能有一个执行线程访问代码的临界区域。

什仫是临界区？什仫是临界资源？

临界资源：一次只允许一个进程使用的资源。

临界区：访问临界资源的程序代码片段。

信号量的工作原理？

P(sv)：如果sv的值大于零，就给它减1；如果它的值为零，就挂起该进程的执行等待操作；

V(sv)：如果有其他进程因等待sv而被挂起，就让它恢复运行，如果没有进程因等待sv而挂起，就给它加1；

举个例子，就是两个进程共享信号量sv，一旦其中一个进程执行了P(sv)操作，它将得到信号量，并可以进入临界区，使sv减1。而第二个进程将被阻止进入临界区，因为当它试图执行P(sv)时，sv为0，它会被挂起以等待第一个进程离开临界区域并执行V(sv)释放信号量，这时第二个进程就可以恢复执行了。

与信号量有关的函数操作？

1).创建/获取一个信号量集合

> \#include <sys/types.h>
> \#include <sys/ipc.h>
> \#include <sys/sem.h>
> int semget(key_t key, int nsems, int semflg);

返回值：成功返回信号量集合的semid，失败返回-1。

key:可以用函数key_t ftok(const char *pathname, int proj_id);来获取。

nsems:这个参数表示你要创建的信号量集合中的信号量的个数。信号量只能以集合的形式创建。

semflg:同时使用IPC_CREAT和IPC_EXCL则会创建一个新的信号量集合。若已经存在的话则返回-1。单独使用IPC_CREAT的话会返回一个新的或者已经存在的信号量集合。

2).信号量结合的操作

> \#include <sys/types.h>
> \#include <sys/ipc.h>
> \#include <sys/sem.h>
> int semop(int semid, struct sembuf *sops, unsigned nsops);
> int semtimedop(int semid, struct sembuf *sops, unsigned nsops,struct timespec *timeout);

返回值：成功返回0，失败返回-1；

semid：信号量集合的id；

> struct sembuf *sops;
> struct sembuf
> {
> unsigned short sem_num; /* semaphore number */
> short sem_op; /* semaphore operation */
> short sem_flg; /* operation flags */
> }

sem_num：为信号量是以集合的形式存在的，就相当于所有信号在一个数组里面，sem_num表示信号量在集合中的编号；

sem_op：示该信号量的操作(P操作还是V操作)。如果其值为正数，该值会加到现有的信号内含值中。通常用于释放所控资源的使用权；如果sem_op的值为负数，而其绝对值又大于信号的现值，操作将会阻塞，直到信号值大于或等于sem_op的绝对值。通常用于获取资源的使用权 。

sem_flg:信号操作标志，它的取值有两种：IPC_NOWAIT和SEM_UNDO。

IPC_NOWAIT:对信号量的操作不能满足时，semop()不会阻塞，而是立即返回，同时设定错误信息；

SEM_UNDO: 程序结束时(不管是正常还是不正常)，保证信号值会被设定；

nsops:表示要操作信号量的个数。因为信号量是以集合的形式存在，所以第二个参数可以传一个数组，同时对一个集合中的多个信号量进行操作。

semop()调用之前的值。这样做的目的在于避免程序在异常的情况下结束未将锁定的资源解锁(死锁)，造成资源永远锁定。

3).int semctl(int semid,int semnum,int cmd,...);

semctl()在semid标识的信号量集合上，或者该信号量集合上第semnum个信号量上执行cmd指定的控制命令。根据cmd不同，这个函数有三个或四个参数，当有第四个参数时，第四个参数的类型是union。

> union semun{
> int val; //使用的值
> struct semid_ds *buf; //IPC_STAT、IPC_SET使用缓存区
> unsigned short *array; //GETALL、SETALL使用的缓存区
> struct seminfo *__buf; //IPC_INFO(linux特有)使用缓存区
> };

返回值：成功返回0，失败返回-1；

semid:信号量集合的编号。

semnum:信号量在集合中的标号。

4).信号量类似消息队列也是随内核的，除非用命令才可以删除该信号量

ipcs -s //查看创建的信号量集合的个数

ipcrm -s semid //删除一个信号量集合

一个简单的关于信号量的例子？

父进程中打印BB，子进程中打印AA。利用信号量机制使得AA和BB之间不出现乱序。此时的显示器就是临界资源，我们需要在父子进程的临界区进行加锁。

comm.h

> \#ifndef _COMM_H_
> \#define _COMM_H_
> \#include<stdio.h>
> \#include<unistd.h>
> \#include<stdlib.h>
> \#include<sys/types.h>
> \#include<sys/ipc.h>
> \#include<sys/sem.h>
> \#include<sys/types.h>
> \#include<sys/wait.h>
> \#define PATHNAME "."
> \#define PROJID 0x6666
> union semun{
> int val; /* Value for SETVAL */
> struct semid_ds *buf; /* Buffer for IPC_STAT, IPC_SET */
> unsigned short *array; /* Array for GETALL, SETALL */
> struct seminfo *__buf; /* Buffer for IPC_INFO(Linux-specific) */
> };
> int CreateSemSet(int num);//创建信号量
> int GetSemSet(); //获取信号量
> int InitSem(int sem_id,int which);
> int P(int sem_id,int which); //p操作
> int V(int sem_id,int which); //v操作
> int DestroySemSet(int sem_id);//销毁信号量
> \#endif //_COMM_H_
> comm.c
> \#include"comm.h"
> static commSemSet(int num,int flag)
> {
> key_t key=ftok(PATHNAME,PROJID);
> if(key == -1)
> {
> perror("ftok error");
> exit(1);
> }
> int sem_id=semget(key,num,flag);
> if(sem_id == -1)
> {
> perror("semget error");
> exit(2);
> }
> return sem_id;
> }
> int CreateSemSet(int num)
> {
> return commSemSet(num,IPC_CREAT|IPC_EXCL|0666);
> }
> int InitSem(int sem_id,int which)
> {
> union semun un;
> un.val=1;
> int ret=semctl(sem_id,which,SETVAL,un);
> if(ret < 0)
> {
> perror("semctl");
> return -1;
> }
> return 0;
> }
> int GetSemSet()
> {
> return commSemSet(0,IPC_CREAT);
> }
> static int SemOp(int sem_id,int which,int op)
> {
> struct sembuf buf;
> buf.sem_num=which;
> buf.sem_op=op;
> buf.sem_flg=0; //
> int ret=semop(sem_id,&buf,1);
> if(ret < 0)
> {
> perror("semop error");
> return -1;
> }
> return 0;
> }
> int P(int sem_id,int which)
> {
> return SemOp(sem_id,which,-1);
> }
> int V(int sem_id,int which)
> {
> return SemOp(sem_id,which,1);
> }
> int DestroySemSet(int sem_id)
> {
> int ret=semctl(sem_id,0,IPC_RMID);
> if(ret < 0)
> {
> perror("semctl error");
> return -1;
> }
> return 0;
> }
> SemSet.c
> \#include"comm.h"
> void testSemSet()
> {
> int sem_id=CreateSemSet(1); //创建信号量
> InitSem(sem_id,0);
> pid_t id=fork();
> if(id < 0){
> perror("fork error");
> exit(1);
> }
> else if(id == 0){ //child，打印AA
> printf("child is running,pid=%d,ppid=%d\n",getpid(),getppid());
> while(1)
> {
> P(sem_id,0); //p操作，信号量的值减1
> printf("A");
> usleep(10031);
> fflush(stdout);
> printf("A");
> usleep(10021);
> fflush(stdout);
> V(sem_id,0); //v操作，信号量的值加1
> }
> }
> else //father，打印BB
> {
> printf("father is running,pid=%d,ppid=%d\n",getpid(),getppid());
> while(1)
> {
> P(sem_id,0);
> printf("B");
> usleep(10051);
> fflush(stdout);
> printf("B");
> usleep(10003);
> fflush(stdout);
> V(sem_id,0);
> }
> wait(NULL);
> }
> DestroySemSet(sem_id);
> }
> int main()
> {
> testSemSet();
> return 0;
> }

### 3.5 共享内存(shm)

共享内存的原理图：



![img](https://pic3.zhimg.com/80/v2-37e557bf77de8334d999bc63bf936272_720w.webp)





与共享内存有关的函数：

1). 创建共享内存

> \#include <sys/ipc.h>
> \#include <sys/shm.h>
> int shmget(key_t key, size_t size, int shmflg);

返回值：成功返回共享内存的id，失败返回-1；

key：和上面介绍的信号量的semget函数的参数key一样；

size：表示要申请的共享内存的大小，一般是4k的整数倍；

flags：IPC_CREAT和IPC_EXCL一起使用，则创建一个新的共享内存，否则返回-1。IPC_CREAT单独使用时返回一个共享内存，有就直接返回，没有就创建。

2).挂接函数

> void *shmat(int shmid);

返回值：返回这块内存的虚拟地址；

shmat的作用是将申请的共享内存挂接在该进程的页表上，是将虚拟内存和物理内存相对应；

3).去挂接函数

> int shmdt(const void *shmaddr);

返回值：失败返回-1；

shmdt的作用是去挂接，将这块共享内存从页表上剥离下来，去除两者的映射关系；

shmaddr:表示这块物理内存的虚拟地址。

4).int shmctl(int shmid,int cmd,const void* addr);

shmctl用来设置共享内存的属性。当cmd是IPC_RMID时可以用来删除一块共享内存。

5).共享内存类似消息队列和信号量，它的生命周期也是随内核的，除非用命令才可以删除该共享内存；

ipcs -m //查看创建的共享内存的个数

ipcrm -m shm_id //删除共享内存

一个简单的关于共享内存的例子：

利用共享内存实现在serve这个进程中向共享内存中写入数据A，从client读出数据。

comm.h

> \#ifndef __COMM__
> \#define __COMM__
> \#include<stdio.h>
> \#include<sys/types.h>
> \#include<sys/ipc.h>
> \#include<sys/shm.h>
> \#include<unistd.h>
> \#define PATHNAME "."
> \#define PROCID 0x6666
> \#define SIZE 4096*1
> int CreatShm();
> int GetShm();
> //int AtShm();
> //int DtShm();
> int DestroyShm(int shm_id);
> \#endif
> comm.c
> \#include"comm.h"
> static int CommShm(int flag)
> {
> key_t key=ftok(PATHNAME,PROCID);
> if(key < 0)
> {
> perror("ftok");
> return -1;
> }
> int shm_id=shmget(key,SIZE,flag);
> if(shm_id < 0)
> {
> perror("shmget");
> return -2;
> }
> return shm_id;
> }
> int CreatShm()
> {
> return CommShm(IPC_CREAT|IPC_EXCL|0666);
> }
> int GetShm()
> {
> return CommShm(IPC_CREAT);
> }
> //int AtShm();
> //int DtShm();
> int DestroyShm(int shm_id)
> {
> int ret=shmctl(shm_id,IPC_RMID,NULL);
> if(ret < 0)
> {
> perror("shmctl");
> return -1;
> }
> return 0;
> }
> server.c
> \#include"comm.h"
> void testserver()
> {
> int shm_id=CreatShm();
> printf("shm_id=%d\n",shm_id);
> char *mem=(char *)shmat(shm_id,NULL,0);
> while(1)
> {
> sleep(1);
> printf("%s\n",mem);
> }
> shmdt(mem);
> DestroyShm(shm_id);
> }
> int main()
> {
> testserver();
> return 0;
> }
> client.c
> \#include"comm.h"
> void testclient()
> {
> int shm_id=GetShm();
> char *mem=(char *)shmat(shm_id,NULL,0);
> int index=0;
> while(1)
> {
> sleep(1);
> mem[index++]='A';
> index %= (SIZE-1);
> mem[index]='\0';
> }
> shmdt(mem);
> DestroyShm(shm_id);
> }
> int main()
> {
> testclient();
> return 0;
> }

共享内存的特点：

共享内存是这五种进程间通信方式中效率最高的。但是因为共享内存没有提供相应的互斥机制，所以一般共享内存都和信号量配合起来使用。

为什仫共享内存的方式比其他进程间通信的方式效率高？

消息队列，FIFO，管道的消息传递方式一般为 ：

1).服务器获取输入的信息；

2).通过管道，消息队列等写入数据至内存中，通常需要将该数据拷贝到内核中；

3).客户从内核中将数据拷贝到自己的客户端进程中；

4).然后再从进程中拷贝到输出文件；

上述过程通常要经过4次拷贝，才能完成文件的传递。

而共享内存只需要：

1).输入内容到共享内存区

2).从共享内存输出到文件

上述过程不涉及到内核的拷贝，这些进程间数据的传递就不再通过执行任何进入内核的系统调用来传递彼此的数据，节省了时间，所以共享内存是这五种进程间通信方式中效率最高的。

原文地址：https://zhuanlan.zhihu.com/p/94856678

作者：linux

# 【NO.342】超详细的网络抓包神器 tcpdump 使用指南

tcpdump 是一款强大的网络抓包工具，它使用 libpcap 库来抓取网络数据包，这个库在几乎在所有的 Linux/Unix 中都有。熟悉 tcpdump 的使用能够帮助你分析调试网络数据，本文将通过一个个具体的示例来介绍它在不同场景下的使用方法。不管你是系统管理员，程序员，云原生工程师还是 yaml 工程师，掌握 tcpdump 的使用都能让你如虎添翼，升职加薪。

## 1. 基本语法和使用方法

tcpdump 的常用参数如下：

```text
$ tcpdump -i eth0 -nn -s0 -v port 80
```

- **-i** : 选择要捕获的接口，通常是以太网卡或无线网卡，也可以是 vlan 或其他特殊接口。如果该系统上只有一个网络接口，则无需指定。
- **-nn** : 单个 n 表示不解析域名，直接显示 IP；两个 n 表示不解析域名和端口。这样不仅方便查看 IP 和端口号，而且在抓取大量数据时非常高效，因为域名解析会降低抓取速度。
- **-s0** : tcpdump 默认只会截取前 96 字节的内容，要想截取所有的报文内容，可以使用 -s number， number 就是你要截取的报文字节数，如果是 0 的话，表示截取报文全部内容。
- **-v** : 使用 -v，-vv 和 -vvv 来显示更多的详细信息，通常会显示更多与特定协议相关的信息。
- port 80 : 这是一个常见的端口过滤器，表示仅抓取 80 端口上的流量，通常是 HTTP。

额外再介绍几个常用参数：

- **-p** : 不让网络接口进入混杂模式。默认情况下使用 tcpdump 抓包时，会让网络接口进入混杂模式。一般计算机网卡都工作在非混杂模式下，此时网卡只接受来自网络端口的目的地址指向自己的数据。当网卡工作在混杂模式下时，网卡将来自接口的所有数据都捕获并交给相应的驱动程序。如果设备接入的交换机开启了混杂模式，使用 -p 选项可以有效地过滤噪声。
- **-e** : 显示数据链路层信息。默认情况下 tcpdump 不会显示数据链路层信息，使用 -e 选项可以显示源和目的 MAC 地址，以及 VLAN tag 信息。例如：

```text
$ tcpdump -n -e -c 5 not ip6

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on br-lan, link-type EN10MB (Ethernet), capture size 262144 bytes
18:27:53.619865 24:5e:be:0c:17:af > 00:e2:69:23:d3:3b, ethertype IPv4 (0x0800), length 1162: 192.168.100.20.51410 > 180.176.26.193.58695: Flags [.], seq 2045333376:2045334484, ack 3398690514, win 751, length 1108
18:27:53.626490 00:e2:69:23:d3:3b > 24:5e:be:0c:17:af, ethertype IPv4 (0x0800), length 68: 220.173.179.66.36017 > 192.168.100.20.51410: UDP, length 26
18:27:53.626893 24:5e:be:0c:17:af > 00:e2:69:23:d3:3b, ethertype IPv4 (0x0800), length 1444: 192.168.100.20.51410 > 220.173.179.66.36017: UDP, length 1402
18:27:53.628837 00:e2:69:23:d3:3b > 24:5e:be:0c:17:af, ethertype IPv4 (0x0800), length 1324: 46.97.169.182.6881 > 192.168.100.20.59145: Flags [P.], seq 3058450381:3058451651, ack 14349180, win 502, length 1270
18:27:53.629096 24:5e:be:0c:17:af > 00:e2:69:23:d3:3b, ethertype IPv4 (0x0800), length 54: 192.168.100.20.59145 > 192.168.100.1.12345: Flags [.], ack 3058451651, win 6350, length 0
5 packets captured
```

### 1.1 显示 ASCII 字符串

-A 表示使用 ASCII 字符串打印报文的全部数据，这样可以使读取更加简单，方便使用 grep 等工具解析输出内容。-X 表示同时使用十六进制和 ASCII 字符串打印报文的全部数据。这两个参数不能一起使用。例如：

```text
$ tcpdump -A -s0 port 80
```

### 1.2 抓取特定协议的数据

后面可以跟上协议名称来过滤特定协议的流量，以 UDP 为例，可以加上参数 udp 或 protocol 17，这两个命令意思相同。

```text
$ tcpdump -i eth0 udp
$ tcpdump -i eth0 proto 17
```

同理，tcp 与 protocol 6 意思相同。

### 1.3 抓取特定主机的数据

使用过滤器 host 可以抓取特定目的地和源 IP 地址的流量。

```text
$ tcpdump -i eth0 host 10.10.1.1
```

也可以使用 src 或 dst 只抓取源或目的地：

```text
$ tcpdump -i eth0 dst 10.10.1.20
```

### 1.4 将抓取的数据写入文件

使用 tcpdump 截取数据报文的时候，默认会打印到屏幕的默认输出，你会看到按照顺序和格式，很多的数据一行行快速闪过，根本来不及看清楚所有的内容。不过，tcpdump 提供了把截取的数据保存到文件的功能，以便后面使用其他图形工具（比如 wireshark，Snort）来分析。

-w 选项用来把数据报文输出到文件：

```text
$ tcpdump -i eth0 -s0 -w test.pcap
```

### 1.5 行缓冲模式

如果想实时将抓取到的数据通过管道传递给其他工具来处理，需要使用 -l 选项来开启行缓冲模式（或使用 -c 选项来开启数据包缓冲模式）。使用 -l 选项可以将输出通过立即发送给其他命令，其他命令会立即响应。

```text
$ tcpdump -i eth0 -s0 -l port 80 | grep 'Server:'
```

### 1.6 组合过滤器

过滤的真正强大之处在于你可以随意组合它们，而连接它们的逻辑就是常用的 与/AND/&& 、 或/OR/|| 和 非/not/!。

```text
and or &&
or or ||
not or !
```

## 2. 过滤器

关于 tcpdump 的过滤器，这里有必要单独介绍一下。

机器上的网络报文数量异常的多，很多时候我们只关系和具体问题有关的数据报（比如访问某个网站的数据，或者 icmp 超时的报文等等），而这些数据只占到很小的一部分。把所有的数据截取下来，从里面找到想要的信息无疑是一件很费时费力的工作。而 tcpdump 提供了灵活的语法可以精确地截取关心的数据报，简化分析的工作量。这些选择数据包的语句就是过滤器（filter）！

### 2.1 Host 过滤器

Host 过滤器用来过滤某个主机的数据报文。例如：

```text
$ tcpdump host 1.2.3.4复制代码
```

该命令会抓取所有发往主机 1.2.3.4 或者从主机 1.2.3.4 发出的流量。如果想只抓取从该主机发出的流量，可以使用下面的命令：

```text
$ tcpdump src host 1.2.3.4复制代码
```

### 2.2 Network 过滤器

Network 过滤器用来过滤某个网段的数据，使用的是 CIDR 模式。可以使用四元组（x.x.x.x）、三元组（x.x.x）、二元组（x.x）和一元组（x）。四元组就是指定某个主机，三元组表示子网掩码为 255.255.255.0，二元组表示子网掩码为 255.255.0.0，一元组表示子网掩码为 255.0.0.0。例如，

抓取所有发往网段 192.168.1.x 或从网段 192.168.1.x 发出的流量：

```text
$ tcpdump net 192.168.1
```

抓取所有发往网段 10.x.x.x 或从网段 10.x.x.x 发出的流量：

```text
$ tcpdump net 10
```

和 Host 过滤器一样，这里也可以指定源和目的：

```text
$ tcpdump src net 10
```

也可以使用 CIDR 格式：

```text
$ tcpdump src net 172.16.0.0/12
```

### 2.3 Proto 过滤器

Proto 过滤器用来过滤某个协议的数据，关键字为 proto，可省略。proto 后面可以跟上协议号或协议名称，支持 icmp, igmp, igrp, pim, ah, esp, carp, vrrp, udp和 tcp。因为通常的协议名称是保留字段，所以在于 proto 指令一起使用时，必须根据 shell 类型使用一个或两个反斜杠（/）来转义。Linux 中的 shell 需要使用两个反斜杠来转义，MacOS 只需要一个。

例如，抓取 icmp 协议的报文：

```text
$ tcpdump -n proto \\icmp
# 或者
$ tcpdump -n icmp
```

### 2.4 Port 过滤器

Port 过滤器用来过滤通过某个端口的数据报文，关键字为 port。例如：

```text
$ tcpdump port 389
```

## 3. 理解 tcpdump 的输出

截取数据只是第一步，第二步就是理解这些数据，下面就解释一下 tcpdump 命令输出各部分的意义。

```text
21:27:06.995846 IP (tos 0x0, ttl 64, id 45646, offset 0, flags [DF], proto TCP (6), length 64)
    192.168.1.106.56166 > 124.192.132.54.80: Flags [S], cksum 0xa730 (correct), seq 992042666, win 65535, options [mss 1460,nop,wscale 4,nop,nop,TS val 663433143 ecr 0,sackOK,eol], length 0

21:27:07.030487 IP (tos 0x0, ttl 51, id 0, offset 0, flags [DF], proto TCP (6), length 44)
    124.192.132.54.80 > 192.168.1.106.56166: Flags [S.], cksum 0xedc0 (correct), seq 2147006684, ack 992042667, win 14600, options [mss 1440], length 0

21:27:07.030527 IP (tos 0x0, ttl 64, id 59119, offset 0, flags [DF], proto TCP (6), length 40)
    192.168.1.106.56166 > 124.192.132.54.80: Flags [.], cksum 0x3e72 (correct), ack 2147006685, win 65535, length 0
```

最基本也是最重要的信息就是数据报的源地址/端口和目的地址/端口，上面的例子第一条数据报中，源地址 ip 是 192.168.1.106，源端口是 56166，目的地址是 124.192.132.54，目的端口是 80。 > 符号代表数据的方向。

此外，上面的三条数据还是 tcp 协议的三次握手过程，第一条就是 SYN 报文，这个可以通过 Flags [S] 看出。下面是常见的 TCP 报文的 Flags:

- [S] : SYN（开始连接）
- [.] : 没有 Flag
- [P] : PSH（推送数据）
- [F] : FIN （结束连接）
- [R] : RST（重置连接）

而第二条数据的 [S.] 表示 SYN-ACK，就是 SYN 报文的应答报文。

## 4. 例子

下面给出一些具体的例子，每个例子都可以使用多种方法来获得相同的输出，你使用的方法取决于所需的输出和网络上的流量。我们在排障时，通常只想获取自己想要的内容，可以通过过滤器和 ASCII 输出并结合管道与 grep、cut、awk 等工具来实现此目的。

例如，在抓取 HTTP 请求和响应数据包时，可以通过删除标志 SYN/ACK/FIN 来过滤噪声，但还有更简单的方法，那就是通过管道传递给 grep。在达到目的的同时，我们要选择最简单最高效的方法。下面来看例子。

### 4.1 提取 HTTP 用户代理

从 HTTP 请求头中提取 HTTP 用户代理：

```text
$ tcpdump -nn -A -s1500 -l | grep "User-Agent:"
```

通过 egrep 可以同时提取用户代理和主机名（或其他头文件）：

```text
$ tcpdump -nn -A -s1500 -l | egrep -i 'User-Agent:|Host:'
```

### 4.2 只抓取 HTTP GET 和 POST 流量

抓取 HTTP GET 流量：

```text
$ tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x47455420'
```

也可以抓取 HTTP POST 请求流量：

```text
$ tcpdump -s 0 -A -vv 'tcp[((tcp[12:1] & 0xf0) >> 2):4] = 0x504f5354'
```

注意：该方法不能保证抓取到 HTTP POST 有效数据流量，因为一个 POST 请求会被分割为多个 TCP 数据包。

上述两个表达式中的十六进制将会与 GET 和 POST 请求的 ASCII 字符串匹配。例如，tcp[((tcp[12:1] & 0xf0) >> 2):4] 首先会确定我们感兴趣的字节的位置（在 TCP header 之后），然后选择我们希望匹配的 4 个字节。

### 4.3 提取 HTTP 请求的 URL

提取 HTTP 请求的主机名和路径：

```text
$ tcpdump -s 0 -v -n -l | egrep -i "POST /|GET /|Host:"

tcpdump: listening on enp7s0, link-type EN10MB (Ethernet), capture size 262144 bytes
    POST /wp-login.php HTTP/1.1
    Host: dev.example.com
    GET /wp-login.php HTTP/1.1
    Host: dev.example.com
    GET /favicon.ico HTTP/1.1
    Host: dev.example.com
    GET / HTTP/1.1
    Host: dev.example.com
```

### 4.4 提取 HTTP POST 请求中的密码

从 HTTP POST 请求中提取密码和主机名：

```text
$ tcpdump -s 0 -A -n -l | egrep -i "POST /|pwd=|passwd=|password=|Host:"

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on enp7s0, link-type EN10MB (Ethernet), capture size 262144 bytes
11:25:54.799014 IP 10.10.1.30.39224 > 10.10.1.125.80: Flags [P.], seq 1458768667:1458770008, ack 2440130792, win 704, options [nop,nop,TS val 461552632 ecr 208900561], length 1341: HTTP: POST /wp-login.php HTTP/1.1
.....s..POST /wp-login.php HTTP/1.1
Host: dev.example.com
.....s..log=admin&pwd=notmypassword&wp-submit=Log+In&redirect_to=http%3A%2F%2Fdev.example.com%2Fwp-admin%2F&testcookie=1
```

### 4.5 提取 Cookies

提取 Set-Cookie（服务端的 Cookie）和 Cookie（客户端的 Cookie）：

```text
$ tcpdump -nn -A -s0 -l | egrep -i 'Set-Cookie|Host:|Cookie:'

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on wlp58s0, link-type EN10MB (Ethernet), capture size 262144 bytes
Host: dev.example.com
Cookie: wordpress_86be02xxxxxxxxxxxxxxxxxxxc43=admin%7C152xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxfb3e15c744fdd6; _ga=GA1.2.21343434343421934; _gid=GA1.2.927343434349426; wordpress_test_cookie=WP+Cookie+check; wordpress_logged_in_86be654654645645645654645653fc43=admin%7C15275102testtesttesttestab7a61e; wp-settings-time-1=1527337439
```

### 4.6 抓取 ICMP 数据包

查看网络上的所有 ICMP 数据包：

```text
$ tcpdump -n icmp

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on enp7s0, link-type EN10MB (Ethernet), capture size 262144 bytes
11:34:21.590380 IP 10.10.1.217 > 10.10.1.30: ICMP echo request, id 27948, seq 1, length 64
11:34:21.590434 IP 10.10.1.30 > 10.10.1.217: ICMP echo reply, id 27948, seq 1, length 64
11:34:27.680307 IP 10.10.1.159 > 10.10.1.1: ICMP 10.10.1.189 udp port 59619 unreachable, length 115
```

### 4.7 抓取非 ECHO/REPLY 类型的 ICMP 数据包

通过排除 echo 和 reply 类型的数据包使抓取到的数据包不包括标准的 ping 包：

```text
$ tcpdump 'icmp[icmptype] != icmp-echo and icmp[icmptype] != icmp-echoreply'

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on enp7s0, link-type EN10MB (Ethernet), capture size 262144 bytes
11:37:04.041037 IP 10.10.1.189 > 10.10.1.20: ICMP 10.10.1.189 udp port 36078 unreachable, length 156
```

### 4.8 抓取 SMTP/POP3 协议的邮件

可以提取电子邮件的正文和其他数据。例如，只提取电子邮件的收件人：

```text
$ tcpdump -nn -l port 25 | grep -i 'MAIL FROM\|RCPT TO'
```

### 4.9 抓取 NTP 服务的查询和响应

```text
$ tcpdump dst port 123

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes
21:02:19.112502 IP test33.ntp > 199.30.140.74.ntp: NTPv4, Client, length 48
21:02:19.113888 IP 216.239.35.0.ntp > test33.ntp: NTPv4, Server, length 48
21:02:20.150347 IP test33.ntp > 216.239.35.0.ntp: NTPv4, Client, length 48
21:02:20.150991 IP 216.239.35.0.ntp > test33.ntp: NTPv4, Server, length 48
```

### 4.10 抓取 SNMP 服务的查询和响应

通过 SNMP 服务，渗透测试人员可以获取大量的设备和系统信息。在这些信息中，系统信息最为关键，如操作系统版本、内核版本等。使用 SNMP 协议快速扫描程序 onesixtyone，可以看到目标系统的信息：

```text
$ onesixtyone 10.10.1.10 public

Scanning 1 hosts, 1 communities
10.10.1.10 [public] Linux test33 4.15.0-20-generic #21-Ubuntu SMP Tue Apr 24 06:16:15 UTC 2018 x86_64
```

可以通过 tcpdump 抓取 GetRequest 和 GetResponse：

```text
$ tcpdump -n -s0  port 161 and udp
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on wlp58s0, link-type EN10MB (Ethernet), capture size 262144 bytes
23:39:13.725522 IP 10.10.1.159.36826 > 10.10.1.20.161:  GetRequest(28)  .1.3.6.1.2.1.1.1.0
23:39:13.728789 IP 10.10.1.20.161 > 10.10.1.159.36826:  GetResponse(109)  .1.3.6.1.2.1.1.1.0="Linux testmachine 4.15.0-20-generic #21-Ubuntu SMP Tue Apr 24 06:16:15 UTC 2018 x86_64"
```

### 4.11 切割 pcap 文件

当抓取大量数据并写入文件时，可以自动切割为多个大小相同的文件。例如，下面的命令表示每 3600 秒创建一个新文件 capture-(hour).pcap，每个文件大小不超过 200*1000000 字节：

```text
$ tcpdump  -w /tmp/capture-%H.pcap -G 3600 -C 200
```

这些文件的命名为 capture-{1-24}.pcap，24 小时之后，之前的文件就会被覆盖。

### 4.12 抓取 IPv6 流量

可以通过过滤器 ip6 来抓取 IPv6 流量，同时可以指定协议如 TCP：

```text
$ tcpdump -nn ip6 proto 6
```

从之前保存的文件中读取 IPv6 UDP 数据报文：

```text
$ tcpdump -nr ipv6-test.pcap ip6 proto 17
```

### 4.13 检测端口扫描

在下面的例子中，你会发现抓取到的报文的源和目的一直不变，且带有标志位 [S] 和 [R]，它们与一系列看似随机的目标端口进行匹配。当发送 SYN 之后，如果目标主机的端口没有打开，就会返回一个 RESET。这是 Nmap 等端口扫描工具的标准做法。

```text
$ tcpdump -nn

21:46:19.693601 IP 10.10.1.10.60460 > 10.10.1.199.5432: Flags [S], seq 116466344, win 29200, options [mss 1460,sackOK,TS val 3547090332 ecr 0,nop,wscale 7], length 0
21:46:19.693626 IP 10.10.1.10.35470 > 10.10.1.199.513: Flags [S], seq 3400074709, win 29200, options [mss 1460,sackOK,TS val 3547090332 ecr 0,nop,wscale 7], length 0
21:46:19.693762 IP 10.10.1.10.44244 > 10.10.1.199.389: Flags [S], seq 2214070267, win 29200, options [mss 1460,sackOK,TS val 3547090333 ecr 0,nop,wscale 7], length 0
21:46:19.693772 IP 10.10.1.199.389 > 10.10.1.10.44244: Flags [R.], seq 0, ack 2214070268, win 0, length 0
21:46:19.693783 IP 10.10.1.10.35172 > 10.10.1.199.1433: Flags [S], seq 2358257571, win 29200, options [mss 1460,sackOK,TS val 3547090333 ecr 0,nop,wscale 7], length 0
21:46:19.693826 IP 10.10.1.10.33022 > 10.10.1.199.49153: Flags [S], seq 2406028551, win 29200, options [mss 1460,sackOK,TS val 3547090333 ecr 0,nop,wscale 7], length 0
21:46:19.695567 IP 10.10.1.10.55130 > 10.10.1.199.49154: Flags [S], seq 3230403372, win 29200, options [mss 1460,sackOK,TS val 3547090334 ecr 0,nop,wscale 7], length 0
21:46:19.695590 IP 10.10.1.199.49154 > 10.10.1.10.55130: Flags [R.], seq 0, ack 3230403373, win 0, length 0
21:46:19.695608 IP 10.10.1.10.33460 > 10.10.1.199.49152: Flags [S], seq 3289070068, win 29200, options [mss 1460,sackOK,TS val 3547090335 ecr 0,nop,wscale 7], length 0
21:46:19.695622 IP 10.10.1.199.49152 > 10.10.1.10.33460: Flags [R.], seq 0, ack 3289070069, win 0, length 0
21:46:19.695637 IP 10.10.1.10.34940 > 10.10.1.199.1029: Flags [S], seq 140319147, win 29200, options [mss 1460,sackOK,TS val 3547090335 ecr 0,nop,wscale 7], length 0
21:46:19.695650 IP 10.10.1.199.1029 > 10.10.1.10.34940: Flags [R.], seq 0, ack 140319148, win 0, length 0
21:46:19.695664 IP 10.10.1.10.45648 > 10.10.1.199.5060: Flags [S], seq 2203629201, win 29200, options [mss 1460,sackOK,TS val 3547090335 ecr 0,nop,wscale 7], length 0
21:46:19.695775 IP 10.10.1.10.49028 > 10.10.1.199.2000: Flags [S], seq 635990431, win 29200, options [mss 1460,sackOK,TS val 3547090335 ecr 0,nop,wscale 7], length 0
21:46:19.695790 IP 10.10.1.199.2000 > 10.10.1.10.49028: Flags [R.], seq 0, ack 635990432, win 0, length 0
```

### 4.14 过滤 Nmap NSE 脚本测试结果

本例中 Nmap NSE 测试脚本 http-enum.nse 用来检测 HTTP 服务的合法 URL。

在执行脚本测试的主机上：

```text
$ nmap -p 80 --script=http-enum.nse targetip
```

在目标主机上：

```text
$ tcpdump -nn port 80 | grep "GET /"

GET /w3perl/ HTTP/1.1
GET /w-agora/ HTTP/1.1
GET /way-board/ HTTP/1.1
GET /web800fo/ HTTP/1.1
GET /webaccess/ HTTP/1.1
GET /webadmin/ HTTP/1.1
GET /webAdmin/ HTTP/1.1
```

### 4.15 抓取 DNS 请求和响应

向 Google 公共 DNS 发起的出站 DNS 请求和 A 记录响应可以通过 tcpdump 抓取到：

```text
$ tcpdump -i wlp58s0 -s0 port 53

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on wlp58s0, link-type EN10MB (Ethernet), capture size 262144 bytes
14:19:06.879799 IP test.53852 > google-public-dns-a.google.com.domain: 26977+ [1au] A? play.google.com. (44)
14:19:07.022618 IP google-public-dns-a.google.com.domain > test.53852: 26977 1/0/1 A 216.58.203.110 (60)
```

### 4.16 抓取 HTTP 有效数据包

抓取 80 端口的 HTTP 有效数据包，排除 TCP 连接建立过程的数据包（SYN / FIN / ACK）：

```text
$ tcpdump 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)'
```

### 4.17 将输出内容重定向到 Wireshark

通常 Wireshark（或 tshark）比 tcpdump 更容易分析应用层协议。一般的做法是在远程服务器上先使用 tcpdump 抓取数据并写入文件，然后再将文件拷贝到本地工作站上用 Wireshark 分析。

还有一种更高效的方法，可以通过 ssh 连接将抓取到的数据实时发送给 Wireshark 进行分析。以 MacOS 系统为例，可以通过 brew cask install wireshark 来安装，然后通过下面的命令来分析：

```text
$ ssh root@remotesystem 'tcpdump -s0 -c 1000 -nn -w - not port 22' | /Applications/Wireshark.app/Contents/MacOS/Wireshark -k -i - 
```

例如，如果想分析 DNS 协议，可以使用下面的命令：

```text
$ ssh root@remotesystem 'tcpdump -s0 -c 1000 -nn -w - port 53' | /Applications/Wireshark.app/Contents/MacOS/Wireshark -k -i -
```

抓取到的数据：

![img](https://pic3.zhimg.com/80/v2-c045644c1a9a88acc047182474609da2_720w.webp)

-c 选项用来限制抓取数据的大小。如果不限制大小，就只能通过 ctrl-c 来停止抓取，这样一来不仅关闭了 tcpdump，也关闭了 wireshark。

### 4.18 找出发包最多的 IP

找出一段时间内发包最多的 IP，或者从一堆报文中找出发包最多的 IP，可以使用下面的命令：

```text
$ tcpdump -nnn -t -c 200 | cut -f 1,2,3,4 -d '.' | sort | uniq -c | sort -nr | head -n 20

tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on enp7s0, link-type EN10MB (Ethernet), capture size 262144 bytes
200 packets captured
261 packets received by filter
0 packets dropped by kernel
    108 IP 10.10.211.181
     91 IP 10.10.1.30
      1 IP 10.10.1.50
```

- **cut -f 1,2,3,4 -d '.'** : 以 . 为分隔符，打印出每行的前四列。即 IP 地址。
- **sort | uniq -c** : 排序并计数
- **sort -nr** : 按照数值大小逆向排序

### 4.19 抓取用户名和密码

本例将重点放在标准纯文本协议上，过滤出于用户名和密码相关的报文：

```text
$ tcpdump port http or port ftp or port smtp or port imap or port pop3 or port telnet -l -A | egrep -i -B5 'pass=|pwd=|log=|login=|user=|username=|pw=|passw=|passwd=|password=|pass:|user:|username:|password:|login:|pass |user '
```

### 4.20 抓取 DHCP 报文

最后一个例子，抓取 DHCP 服务的请求和响应报文，67 为 DHCP 端口，68 为客户机端口。

```text
$ tcpdump -v -n port 67 or 68

tcpdump: listening on enp7s0, link-type EN10MB (Ethernet), capture size 262144 bytes
14:37:50.059662 IP (tos 0x10, ttl 128, id 0, offset 0, flags [none], proto UDP (17), length 328)
    0.0.0.0.68 > 255.255.255.255.67: BOOTP/DHCP, Request from 00:0c:xx:xx:xx:d5, length 300, xid 0xc9779c2a, Flags [none]
      Client-Ethernet-Address 00:0c:xx:xx:xx:d5
      Vendor-rfc1048 Extensions
        Magic Cookie 0x63825363
        DHCP-Message Option 53, length 1: Request
        Requested-IP Option 50, length 4: 10.10.1.163
        Hostname Option 12, length 14: "test-ubuntu"
        Parameter-Request Option 55, length 16: 
          Subnet-Mask, BR, Time-Zone, Default-Gateway
          Domain-Name, Domain-Name-Server, Option 119, Hostname
          Netbios-Name-Server, Netbios-Scope, MTU, Classless-Static-Route
          NTP, Classless-Static-Route-Microsoft, Static-Route, Option 252
14:37:50.059667 IP (tos 0x10, ttl 128, id 0, offset 0, flags [none], proto UDP (17), length 328)
    0.0.0.0.68 > 255.255.255.255.67: BOOTP/DHCP, Request from 00:0c:xx:xx:xx:d5, length 300, xid 0xc9779c2a, Flags [none]
      Client-Ethernet-Address 00:0c:xx:xx:xx:d5
      Vendor-rfc1048 Extensions
        Magic Cookie 0x63825363
        DHCP-Message Option 53, length 1: Request
        Requested-IP Option 50, length 4: 10.10.1.163
        Hostname Option 12, length 14: "test-ubuntu"
        Parameter-Request Option 55, length 16: 
          Subnet-Mask, BR, Time-Zone, Default-Gateway
          Domain-Name, Domain-Name-Server, Option 119, Hostname
          Netbios-Name-Server, Netbios-Scope, MTU, Classless-Static-Route
          NTP, Classless-Static-Route-Microsoft, Static-Route, Option 252
14:37:50.060780 IP (tos 0x0, ttl 64, id 53564, offset 0, flags [none], proto UDP (17), length 339)
    10.10.1.1.67 > 10.10.1.163.68: BOOTP/DHCP, Reply, length 311, xid 0xc9779c2a, Flags [none]
      Your-IP 10.10.1.163
      Server-IP 10.10.1.1
      Client-Ethernet-Address 00:0c:xx:xx:xx:d5
      Vendor-rfc1048 Extensions
        Magic Cookie 0x63825363
        DHCP-Message Option 53, length 1: ACK
        Server-ID Option 54, length 4: 10.10.1.1
        Lease-Time Option 51, length 4: 86400
        RN Option 58, length 4: 43200
        RB Option 59, length 4: 75600
        Subnet-Mask Option 1, length 4: 255.255.255.0
        BR Option 28, length 4: 10.10.1.255
        Domain-Name-Server Option 6, length 4: 10.10.1.1
        Hostname Option 12, length 14: "test-ubuntu"
        T252 Option 252, length 1: 10
        Default-Gateway Option 3, length 4: 10.10.1.1
```

## 5. 总结

本文主要介绍了 tcpdump 的基本语法和使用方法，并通过一些示例来展示它强大的过滤功能。将 tcpdump 与 wireshark 进行组合可以发挥更强大的功效，本文也展示了如何优雅顺滑地结合 tcpdump 和 wireshark。如果你想了解更多的细节，可以查看 tcpdump 的 man 手册。

原文地址：https://zhuanlan.zhihu.com/p/482617730

作者：linux

# 【NO.343】原来 mmap 这么简单

在本文中，我们主要介绍 `mmap` 的原理。

## 1.传统的读写文件

一般来说，修改一个文件的内容需要如下3个步骤：

- 把文件内容读入到内存中。
- 修改内存中的内容。
- 把内存的数据写入到文件中。

过程如图 1 所示：

![img](https://pic4.zhimg.com/80/v2-eb49f938d177e2c1eff8fd7cb46c10ff_720w.webp)

如果使用代码来实现上面的过程，代码如下：

```text
read(fd, buf, 1024);  // 读取文件的内容到buf
...                   // 修改buf的内容
write(fd, buf, 1024); // 把buf的内容写入到文件
```

从图 1 中可以看出，`页缓存(page cache)` 是读写文件时的中间层，内核使用 `页缓存` 与文件的数据块关联起来。所以应用程序读写文件时，实际操作的是 `页缓存`。

## 2.使用 mmap 读写文件

从传统读写文件的过程中，我们可以发现有个地方可以优化：如果可以直接在用户空间读写 `页缓存`，那么就可以免去将 `页缓存` 的数据复制到用户空间缓冲区的过程。

那么，有没有这样的技术能实现上面所说的方式呢？答案是肯定的，就是 `mmap`。

使用 `mmap` 系统调用可以将用户空间的虚拟内存地址与文件进行映射（绑定），对映射后的虚拟内存地址进行读写操作就如同对文件进行读写操作一样。原理如图 2 所示：

![img](https://pic1.zhimg.com/80/v2-af305d81ca549ec80da028064d853380_720w.webp)

前面我们介绍过，读写文件都需要经过 `页缓存`，所以 `mmap` 映射的正是文件的 `页缓存`，而非磁盘中的文件本身。由于 `mmap` 映射的是文件的 `页缓存`，所以就涉及到同步的问题，即 `页缓存` 会在什么时候把数据同步到磁盘。

Linux 内核并不会主动把 `mmap` 映射的 `页缓存` 同步到磁盘，而是需要用户主动触发。同步 `mmap` 映射的内存到磁盘有 4 个时机：

- 调用 `msync` 函数主动进行数据同步（主动）。
- 调用 `munmap` 函数对文件进行解除映射关系时（主动）。
- 进程退出时（被动）。
- 系统关机时（被动）。

## 3.mmap的使用方式

下面我们介绍一下怎么使用 `mmap`，`mmap` 函数的原型如下：

```text
void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);
```

下面介绍一下 `mmap` 函数的各个参数作用：

- `addr`：指定映射的虚拟内存地址，可以设置为 NULL，让 Linux 内核自动选择合适的虚拟内存地址。

- `length`：映射的长度。

- `prot`：映射内存的保护模式，可选值如下：

- - `PROT_EXEC`：可以被执行。
  - `PROT_READ`：可以被读取。
  - `PROT_WRITE`：可以被写入。
  - `PROT_NONE`：不可访问。

- `flags`：指定映射的类型，常用的可选值如下：

- - `MAP_FIXED`：使用指定的起始虚拟内存地址进行映射。
  - `MAP_SHARED`：与其它所有映射到这个文件的进程共享映射空间（可实现共享内存）。
  - `MAP_PRIVATE`：建立一个写时复制（Copy on Write）的私有映射空间。
  - `MAP_LOCKED`：锁定映射区的页面，从而防止页面被交换出内存。
  - ...

- `fd`：进行映射的文件句柄。

- `offset`：文件偏移量（从文件的何处开始映射）。

介绍完 `mmap` 函数的原型后，我们现在通过一个简单的例子介绍怎么使用 `mmap`：

```text
int fd = open(filepath, O_RDWR, 0644);                           // 打开文件
void *addr = mmap(NULL, 8192, PROT_WRITE, MAP_SHARED, fd, 4096); // 对文件进行映射
```

在上面例子中，我们先通过 `open` 函数以可读写的方式打开文件，然后通过 `mmap` 函数对文件进行映射，映射的方式如下：

- `addr` 参数设置为 NULL，表示让操作系统自动选择合适的虚拟内存地址进行映射。
- `length` 参数设置为 8192 表示映射的区域为 2 个内存页的大小（一个内存页的大小为 4 KB）。
- `prot` 参数设置为 `PROT_WRITE` 表示映射的内存区为可读写。
- `flags` 参数设置为 `MAP_SHARED` 表示共享映射区。
- `fd` 参数设置打开的文件句柄。
- `offset` 参数设置为 4096 表示从文件的 4096 处开始映射。

`mmap` 函数会返回映射后的内存地址，我们可以通过此内存地址对文件进行读写操作。我们通过图 3 展示上面例子在内核中的结构：

![img](https://pic2.zhimg.com/80/v2-7234375f0ef628e6a38c9c64fe26d991_720w.webp)

## 4.总结

本文主要介绍了 `mmap` 的原理和使用方式，通过本文我们可以知道，使用 `mmap` 对文件进行读写操作时可以减少内存拷贝的次数，并且可以减少系统调用的次数，从而提高对读写文件操作的效率。

由于内核不会主动同步 `mmap` 所映射的内存区中的数据，所以在某些特殊的场景下可能会出现数据丢失的情况（如断电）。为了避免数据丢失，在使用 `mmap` 的时候可以在适当时主动调用 `msync` 函数来同步映射内存区的数据。

原文地址：https://zhuanlan.zhihu.com/p/429455424

作者：linux

# 【NO.344】深入理解 http 反向代理（nginx）

## 1.要理解什么是 **反向代理(reverse proxy)** , 自然你得先知道什么是 **正向代理(forward proxy)**.

> 另外需要说的是, 一般提到反向代理, 通常是指 http 反向代理, 但反向代理的范围可以更大, 比如 tcp 反向代理, 在这里, 不打算讨论 tcp 之类的反向代理, 当文中说到反向代理时, 指的就是 http 反向代理.

正向代理通常直接称为 **代理(proxy)**, 无需强调它是正向的, 在 http 协议中, 代理即指正向代理.

## 2.**直接访问**

而要谈论什么是正向代理, 则需要先讨论"直接访问"的形式.

> 也就是没有任何代理的模式.

事实上, 直接访问对于很多的小网站来说是最常见的方式. 直接访问用我们日常购物来比喻的话就是类似于"厂家直销", 你直接向生产厂家下单, 没有经过任何的中间商.

从系统的角度看, "直接访问"就是浏览器的请求直接到了最终生成网页的服务器, 中间没有经过任何的 http 代理服务器. 那么代理或者说更啰嗦点的"正向代理"又是什么情况呢?

## 3.**正向代理(forward proxy)**

还是用购物来比喻的话, 你从商店里而不是直接从厂家购买一个商品就类似于代理的模式.

> 比如, 你从商店里买了一盒方便面, 显然, 你很清楚, 商店本身是不生产方便面的, 它不过是个"中间商"而已, 转了一手, 顺便赚点你的钱, 店里的方便面也是它从厂商那边进货来的. 当然, 有些店雁过拔毛拔得有点狠, 那就变成让人厌恶的"中奸商"了.

那么, 对于浏览器的请求处理来说, 一个代理, 更确切的说一台代理服务器, 扮演的角色也是类似的, 它也就是一个请求的中间商而已.

一个正向代理服务器并没有直接响应请求的能力, 就像商店不生产方便面一样, 它不过是把请求转发到最终的网页服务器上, 再把后者的响应再转发给请求者, 也就是浏览器, 如下图所示:

![img](https://pic1.zhimg.com/80/v2-d7909445c4e9deadc7c662af0b2367d8_720w.webp)

但这里还有个问题, 你知道你家周围有哪些小商店, 你想买些东西的时候直接去找这些"代理商"即可, 问题是浏览器怎么知道代理服务器在哪?

> 最终的服务器浏览器是知道的, 比如你输入我的域名 "[http://xiaogd.net](https://link.zhihu.com/?target=http%3A//xiaogd.net)", 通过 DNS 系统浏览器就能查到对应的 ip 地址是 118.89.55.54, 可浏览器怎么知道哪里有代理服务器, 以及请求是否要经过代理服务器呢?

答案就是你要**主动地**告诉浏览器, 这个过程通常称为"配置代理服务器".

> 后面将看到, 这是正向代理与反向代理的一个重大的区别.
> 这是在 IE 浏览器上配置代理服务器的一个示意图:

![img](https://pic2.zhimg.com/80/v2-8894269e904906174bc0f4aad7d32151_720w.webp)

### 3.1 **为什么要启用代理?**

自然, 有人可能会问, 直接访问不香吗? 为何还要这么麻烦经过代理服务器去转手呢? 原因可以有以下这么一些.

一是出于安全审计及控制方面的一些考虑. 在一些组织内, web 相关的端口如 80 及 443 是被封禁的, 你在里面直接是根本上不去网的, 这时你要想上网, 就只能配置组织为你指定的一台内网的代理服务器了.

> 当然了, 代理服务器本身则没有被限制, 它是可以访问外部的网络的.

如此一来, 你的所有上网请求都要经过代理服务器, 而这个代理是由组织控制的, 就可以对请求进行审计了:

- 比如发现你往外面的一个网站上传组织内部的保密资料, 就出手阻止你;
- 又或发现你访问了一个不安全的网站, 可能会导致你的电脑中毒, 于是出手拦截了;
- 又或者是发现你在访问与工作无关的娱乐网站, 于是就给你阻断了~~(为了你的绩效及 KPI 达标, 组织也是操碎了心呀!)

还有一些原因则是出于加速或节省带宽等的考虑. 因为有的代理服务器它不仅能转发, 还能对网页以及其它一些资源进行缓存.

比如我以前在学校念书时, 就曾被学校告知在宿舍上网可以配置代理服务器, 我猜测原因可能是学校整体对外的带宽是有限的.

举个例子, 假如现在很多同学都要去上 [http://qq.com](https://link.zhihu.com/?target=http%3A//qq.com) 的主页, 那么第一个同学请求时, 代理服务器就可以把主页缓存起来一段时间, 碰到后面还要同学想访问这个主页, 就无需再去请求了, 代理服务器直接返回缓存的请求.

> 自然, 缓存也会有一个失效期的, 不会一直缓存下去, 否则内容就得不到更新了.
> 至于多长时间更新缓存, 怎么更新等这些就属于具体的缓存策略问题了.

当然了, 现在很多网页主页都有个性化推荐, 又或者直接就是要登录的, 那通常就无法缓存了, 所以现在配置代理服务器的行为现在也不那么时兴了, 当然也可能现在带宽也提高了, 另外很多人也不懂也不想去知道怎么配置这个代理服务器. 但另一方面, 很多静态资源还是可以缓存的, 比如图片呀, js, css 之类的文件等等, 所以用好了代理服务器依然还是可以发挥作用的.

最后再说一个原因, 由于国家已经决定, 有些国外的技术网站是访问不了的, 而我们又想上去查阅资料解决手中的 bug, 这时就需要一些科学的手段, 直接访问不行, 就必须通过代理来"曲径通幽"了.

> 严格来讲, 这里面很多的代理就是更广义的代理了, 而不是狭义上的 http 代理, 但原理是类似的, 也算是代理模式的一种体现. 经我们配置或一些智能插件的辅助, 浏览器知道直接对某些网站的请求会泥牛入海, 就像掉入了黑洞, 这时就要让这些请求"走代理"以绕过防火墙的限制了.
>
> 对于简单的代理配置来说, 就是配置一台代理服务器地址即可, 但这样有个问题, 那就是所有的请求都会走代理, 有一些高级的代理插件还允许你配置具体的规则, 即你可以配置哪些地址要走代理, 哪些又不走代理, 通常还会带一些预定义的规则, 各种白名单, 黑名单, 你还可以自己添加新的规则.

总之呢, 代理就是这么个中间的角色, 通过它间接访问到了所需资源, 而浏览器也是知道这么个角色的存在的, 因为你需要主动为浏览器去配置并启用. 那么这就是代理, 又或者说"正向代理".

## 4.**反向代理(reverse proxy)**

明白了直接访问, 明白了所谓的正向代理, 下面就可以来说说反向代理是怎么回事了.

反向代理与正向代理的一个很大区别就是, 它不需要客户端(浏览器)去做什么配置, 并没有什么配置代理服务器的操作.

如果说正向代理是主动配置, 主动走代理, 那么反向代理则是"**被代理**", 从这点上看, 反向代理有时又称为"**透明代理**", 也即是浏览器都不知道自己被代理了, 浏览器以为发给它响应的就是最终的网页服务器, 其实不过是个"代理".

还是举购物的例子来比喻. 有时你在网上购物会看到有商家声称自己就是厂家, 东西都很便宜, 属于厂家直销, 于是你下单了. 过段时间, 你又发现有另一家店声称自己才是真正的厂家直销, 然后你仔细看了两家店铺的信息, 才发现前一个商家是假的, 它不是真的厂家.

但为啥这个假的厂家直销它还是这么便宜呢? 以至于价格跟真的厂家直销的没啥区别. 原因可能则是店家直接就是坐落在厂家旁边, 然后他可能与厂家有那么点关系, 认识里面一些人之类的, 这让他能以很便宜的价格从厂家拿到货, 又因为离得近, 几乎没有任何物流成本, 从某种层面看, 它声称厂家直销也不算怎么骗人. 当然严格来说, 它属于伪厂家直销, 他依然还是个代理商

> 它声称是李逵, 其实它是李鬼.

用一个图对比一下这两种情形:

![img](https://pic3.zhimg.com/80/v2-e586e25766c7b42a0e4d020fe34ccf0e_720w.webp)

那么这样的一种模式就有点 **反向代理** 的味道了, 你以为自己买到了直销, 其实你还是"被代理"了, 还是经过了中间商.

> 只是这个中间商对你来说不是那么明显, 甚至说对你是透明的, 把你蒙在了鼓里.
> 虽然都是"代理", 这跟线下店面购买还是很不同的, 在线下你去商店买时, 你很清楚自己经过了代理的中间商, 也即是商店本身, 但在远程线上这种声称自己是厂家直销的情形, 有时你还真不好判断自己是不是被代理了.

那么 http 的反向代理其实也是这样一个道理. 比如你访问我的网站 [https://xiaogd.net](https://link.zhihu.com/?target=https%3A//xiaogd.net), 然后你看下主页的请求里的服务器信息, 它告诉你响应这个主页请求的是一台 Nginx server, 如下图所示:

![img](https://pic4.zhimg.com/80/v2-3103efa19c2c3c5ba6300045713de4a7_720w.webp)

问题是 Nginx 是最终生成这个网页的 server 吗? 其实不是的! 如果你了解 Nginx, 就会知道它通常只是一个静态资源服务器, 而我的网站主页是一个动态生成的内容, 其实你要是认真看过我网站底部的一个声明, 如下图所示:

![img](https://pic2.zhimg.com/80/v2-5102e77c304c67a5d7ae2d5ed3d199e5_720w.webp)

就会明白这个主页其实是 php 的一个叫 wordpress 的建站应用去生成的. 在我的云主机的内部, Nginx 其实是将主页的请求转发给一个所谓的 php-fpm 网关

> 这个 php-fpm 网关基本可以看作是个 php 的 web 服务器, 不过严格来说它用的协议不是 http, 而是一种内部简化的 fastcgi 协议.
> 如果你要较真的话, 这可以算是 反向代理 模式, 但整体不全是 http 反向代理, 但对外而言则确实是.

从它那里取得最终响应的内容, 并再次转发给浏览器, 整个情形见如下的示意图:

![img](https://pic2.zhimg.com/80/v2-405c6291f5ce6dd27433d62ee0aecc91_720w.webp)

这是内部配置的一个情况:

```text
location ~ .php$ {
    root           /ftp/wwwroot;
    fastcgi_pass   127.0.0.1:9000;
    fastcgi_index  index.php;
    fastcgi_param  SCRIPT_FILENAME  $document_root/$fastcgi_script_name;
    include        fastcgi_params;
}
```

请求被转发到内部一个在 9000 端口上监听的 php 应用服务器.

从外部浏览器的角度看, 请求直接发给了 Nginx server, 响应也从 Nginx server 里回来了, 中间没有任何的(正向)代理. 至于说你内部请求又被怎么转发了, 显然浏览器是无从知道也不需要去知道的.

站在整个体系设计者的角度去看, 当然但很多请求 Nginx 其实是没有能力去响应的, 它只不过在内部把它代理给了另一个内部的 php 应用服务器, 内部的 php 应用服务器才是最终的响应生成者.

在整个体系里面, Nginx 的角色就是一个"反向代理"服务器, 浏览器被代理了, 但它无从知道自己是否被代理了, 这一切对它而言是透明的, 反正它自己是没有主动走(正向)代理的.

当然了, 你现在知道了我内部的配置, 如果直接访问 [http://xiaogd.net:9000](https://link.zhihu.com/?target=http%3A//xiaogd.net%3A9000), 那就是真正的"直接访问"了, 那就绕过了 Nginx.

> 不过需要说明的一点是, 直接访问是访问不通的, 因为 9000 端口并没有对外放开. 但是在内部是可以访问到的, 比如这样尝试用 wget 去访问:
> wget localhost:9000
>
> 这样就是真正的"直接访问"了, 没有任何的代理, 既没有正向代理, 也没有反向代理.
> 需要说明的一点是, 用 wget 这样去获取响应还是会报错, 因为 wget 使用的是 http 协议, php 的 cgi 网关实际使用的是 fastcgi 协议, 是一个比 http 更为简化的协议, 作为内部通讯更加高效, 不过 wget 不支持这个协议, 但 Nginx 能理解这个协议, 整个过程是这样的:
> browser -- [http] --> Nginx -- [fastcgi] --> php-fpm
> 严格来说, 不完全是 http 代理, 内部的反向代理实际用的是 fastcgi 网关协议, 不过这个原理还是一样的, 如果内部用一个比如 tomcat 来响应, 那么全程就都可以是 http 协议.
> browser -- [http] --> Nginx -- [http] --> tomcat
> 而如果在内部发请求 80, 比如 `wget localhost` 那就还是被反向代理, 请求先到在 80 端口监听的 Nginx, Nginx 再转给 php-fpm.
> 另: 关于端口及缺省端口相关知识, 可以参考这篇深入理解端口.

### 4.1 **为什么要使用反向代理?**

那么到了这一步我们又面临一个新的问题, 那就是为啥要整这个反向代理呢? 类似于碰到正向代理时的诘问那样, 直接访问不香吗? 为啥还要走这个反向代理? 关于正向代理前面已经解释了一些原因, 而反向代理的出现, 正像这个世界上没有无缘无故的爱与恨一样, 自然也有它存在的原因.

一个很直接的原因就是利用反向代理可以作为内部 **负载均衡(load balance)** 的手段.

举个例子来说, 假如我现在开发了一个 java web 的应用作为我的网站后台, 我直接部署它到 tomcat 服务器上, 让 tomcat 监听 80 端口, 直接对外服务. 一开始访问量也不大, 所以这样也是没有问题的, 如下图所示:

![img](https://pic4.zhimg.com/80/v2-f11e4464f4f5754e09fce249956f5933_720w.webp)

> 注: 因为 http 协议的缺省端口就是 80, 所以用户输入地址时可以省略这个端口号, 也即只需这样: [http://xiaogd.net](https://link.zhihu.com/?target=http%3A//xiaogd.net), 而不是繁琐的像这样: [http://xiaogd.net:80](https://link.zhihu.com/?target=http%3A//xiaogd.net%3A80), 关于缺省端口的话题, 还是可以参考前面所提的 深入理解端口.

但过一段时间之后, 访问量可能上来了, 一个 tomcat 进程处理不过来, 那怎么办呢? 于是我打算再起一个新的 tomcat 进程, 但这样就面临一个问题, 只有一个 80 端口, 它已经被第一个 tomcat 进程占用, 如果还要再起另外一个, 则只能选用其它的端口, 比如 8080.

当使用另外一个端口时, 确实可以启动两个 tomcat 的进程, 但用户想访问到第二个 tomcat 进程的服务, 却要这样去访问: [http://xiaogd.net:8080](https://link.zhihu.com/?target=http%3A//xiaogd.net%3A8080). 显然, 这样的方案是有问题的, 用户根本不知道 8080 端口上服务的存在, 就算你有办法告诉用户, 用户也可能不太理解, 用户同时也很怕麻烦的, 为啥要我输入一个冒号加 8080 呢?

此外, 就算有些用户愿意如你所说转向访问 8080 端口, 你还是不能很好的控制把访问量平均地分配在两个 tomcat 上, 毕竟这是用户随机决定的, 也许很多用户又突然涌过来了 8080 端口的应用上, 造成了这边的拥挤.

又或者只有很少的用户愿意听从你的劝告转到新的 8080 端口上, 访问还是集中在旧的 80 端口上的, 这样旧的应用上响应还是很缓慢, 而新的应用却因为没几个用户访问而显得空闲, 没有得到充分的使用.

那么, 在这种情况下, 反向代理的好处就体现出来了, 具体的操作是这样的, 让 Nginx 作为一个前置的反向代理, 监听在 80 端口上; 而第一个 tomcat 则躲到幕后, 同时它也不再监听 80 端口(需要让给 Nginx), 而改为监听一个其它没有被使用的端口, 比如 8081, 然后让 Nginx 转发请求给它处理.

当然了, 如果只有一个 tomcat, 配置大概是这样的:

```text
location / {
    proxy_pass   http://127.0.0.1:8080;
}
```

请求处理的流程是这样的:

> 请求: browser -- [http] --> Nginx -- [http] --> tomcat
> 响应: browser <-- [http] -- Nginx <-- [http] -- tomcat

自然, 这种情形下反向代理似乎不太必要, 还加多了一个环节, 响应速度反而慢了.

但如果有两个 tomcat, 情况就不一样了, 此时就可以在 Nginx 这个反向代理的层面, 启用负载均衡的策略, 大概的配置如下:

```text
http {
    upstream myapp1 {
        server 127.0.0.1:8080;
        server 127.0.0.1:8081;
    }

    server {
        listen 80;

        location / {
            proxy_pass http://myapp1;
        }
    }
}
```

此时, 如果同时涌入了很多请求, Nginx 会把一半的请求交给 8080 端口上的 tomcat, 另一半的请求交给 8081 端口上的 tomcat, 如下图所示:

![img](https://pic4.zhimg.com/80/v2-c17dfa0e01b4e46dec8e9a3890afcf2f_720w.webp)

对外来看, 所有请求还是 Nginx 来处理, 用户不需要去做选择, 也不需要知道什么 8080, 8081 端口上应用的存在, 他们还是继续访问原来的网址 [http://xiaogd.net](https://link.zhihu.com/?target=http%3A//xiaogd.net) 即可, 无需做任何改变.

如果你在云上有好几台主机, 甚至还可以将其组成一个内网, 然后将 tomcat 部署在不同的主机上. 比如有三台主机的话, 一台运行 Nginx 监听 80 端口, 其余两台运行 tomcat, 分别监听 8080 和 8081 端口, 同时接受并处理 Nginx 反向代理过来的请求, 如下图所示:

![img](https://pic3.zhimg.com/80/v2-6b725a880fb51562ca7293ee3f2c7026_720w.webp)

如果两台 tomcat 主机的配置不同, 比如一台的性能更强劲些, 还可以调整负载的比例(即权重, weight), 让性能更强的一台承担更多的请求:

```text
http {
    upstream myapp1 {
        server 192.168.0.20:8080 weight=3;
        server 192.168.0.21:8080 weight=2;
    }

    server {
        listen 80;

        location / {
            proxy_pass http://myapp1;
        }
    }
}
```

如上配置 3:2 的权重比, 让其中一台承担 60% 的请求, 而另一台性能较差的则承担 40%, 也即每 5 个请求, 3 个会被转到 ip 为 20 的主机上, 2 个会转到 ip 为 21 的主机上.

自然, 有人可能还会有疑问, 所有请求都还是要经过 Nginx, 它能处理得过来吗? 答案是可以的, 因为它的功能仅仅是转发, 这就有点像美团外卖, 虽然它每天接受成千上万的人的点餐, 但它自己不需要去买菜, 洗菜, 切菜, 炒菜等, 它仅仅需要把订单交给饭店餐馆, 然后把它们做好的饭菜配送出去, 也即那些耗时的做饭过程都交给了饭店餐馆处理.

在这种反向代理的模式中, 同样的, 生成网页这个重任交到了隐藏在背后的 tomcat, 生成一个复杂的动态网页可能需要经过一些复杂的计算, 要查询数据库, 要拼凑各个页面组件, 可能会比较耗时, 但这些请求被两个 tomcat 应用并发地处理了, 因此响应的速度还是得到了保证, 而这些就是反向代理能给我们带来的好处.

## 5.**总结**

至此, 关于直接访问, (正向)代理以及反向代理就介绍完了, 最后总结下三种情形及与购物例子的比喻.

在直接访问的情形中, 浏览器直接访问了最终生成响应的服务器, 类似我们以厂家直销的方式从厂商购物, 如下图所示:

![img](https://pic2.zhimg.com/80/v2-506cdb23c8cb46bbba46632d3ae505f1_720w.webp)

在(正向)代理的情形中, 浏览器主动访问代理服务器, 通过它间接获取最终响应, 类似我们从商店购物, 而商店的物品又是从厂家购来的, 如下图所示:

![img](https://pic3.zhimg.com/80/v2-1d8b77d8fcdd701ea52413e8aa5b208a_720w.webp)

在反向代理的情形中, 从浏览器的角度看还是类似于直接访问, 但它的请求在服务端被透明的代理了. 类似于我们在网上从一个声称是厂家直销的"伪厂家"那里购物, 这个伪厂家实际还是把我们的订单转给了真正的厂家, 并从中拿了货给我们, 只是我们无从知道这一切幕后的交易, 如下图所示:

![img](https://pic4.zhimg.com/80/v2-3e594abe628f3de3172887f1d564baa3_720w.webp)

在一个复杂的网络中, 浏览器的请求还可能先被正向代理了, 然后又被反向代理了, 如下图所示::

![img](https://pic1.zhimg.com/80/v2-5214c0c94a4cf358dd7f73b15f75fc48_720w.webp)

关于 http 正向代理和反向代理就讲到这里.

原文地址：https://zhuanlan.zhihu.com/p/464965616

作者：Linux

# 【NO.345】C++使用protobuf实现序列化与反序列化

## 1.protobuf简介：

### **1.1 protobuf的定义：**

protobuf是用来干嘛的？

protobuf是一种用于 对结构数据进行序列化的工具，从而实现 数据存储和交换。

（主要用于网络通信中 收发两端进行消息交互。所谓的“结构数据”是指类似于struct结构体的数据，可用于表示一个网络消息。当结构体中存在函数指针类型时，直接对其存储或传输相当于是“浅拷贝”，而对其序列化后则是“深拷贝”。）

序列化： 将结构数据或者对象转换成能够用于存储和传输的格式。

反序列化： 在其他的计算环境中，将序列化后的数据还原为数据结构和对象。

从“序列化”字面上的理解，似乎使用C语言中的struct结构体就可以实现序列化的功能：将结构数据填充到定义好的结构体中的对应字段即可，接收方再对结构体进行解析。

在单机的不同进程间通信时，使用struct结构体这种方法实现“序列化”和“反序列化”的功能问题不大，但是，在网络编程中，即面向网络中不同主机间的通信时，则不能使用struct结构体，原因在于：

（1）跨语言平台，例如发送方是用C语言编写的程序，接收方是用Java语言编写的程序，不同语言的struct结构体定义方式不同，不能直接解析；

（2）struct结构体存在 内存对齐 和 CPU不兼容的问题。

因此，在网络编程中，实现“序列化”和“反序列化”功能需要使用通用的组件，如 Json、XML、protobuf 等。

### **1.2 protobuf的优缺点：**

1.2.1 优点：

① 性能高效：

与XML相比，protobuf更小（3 ~ 10倍）、更快（20 ~ 100倍）、更为简单。

② 语言无关、平台无关：

protobuf支持Java、C++、Python等多种语言，支持多个平台。

③ 扩展性、兼容性强：

只需要使用protobuf对结构数据进行一次描述，即可从各种数据流中读取结构数据，更新数据结构时不会破坏原有的程序。

Protobuf与XML、Json的性能对比：

测试10万次序列化：

![img](https://pic1.zhimg.com/80/v2-cbc8f342d5c6275f7cf9a8379eceb70c_720w.webp)

测试10万次反序列化：

![img](https://pic2.zhimg.com/80/v2-cca7ea55d6d3b7fc5fe276fb9851fa59_720w.webp)

1.2.2 缺点：

① 自解释性较差，数据存储格式为二进制，需要通过 .proto 文件才能了解到内部的数据结构；

② 不适合用来对 基于文本的标记文档（如HTML） 建模。

### **1.3 protobuf中的数据类型限定修饰符：**

protobuf 2 中有三种数据类型限定修饰符：

```text
required, optional, repeated
```

required表示字段必选，optional表示字段可选，repeated表示一个数组类型。

其中， required 和 optional 已在 proto3 弃用了。

### **1.4 protobuf中常用的数据类型：**

```text
bool,		布尔类型

double,		64位浮点数
float,		32位浮点数

int32,		32位整数
int64,		64位整数
uint64,		64位无符号整数
sint32,		32位整数，处理负数效率更高
sint64,		64位整数，处理负数效率更高

string,		只能处理ASCII字符
bytes,		用于处理多字节的语言字符
enum,		枚举类型
```

## 2.protobuf的使用流程：

下载protobuf压缩包后，解压、配置、编译、安装，即可使用 protoc 命令 查看Linux中是否安装成功：

```text
[root@linux] protoc --version
libprotoc 3.15.8
```

使用protobuf时，需要先根据应用需求编写 .proto 文件 定义消息体格式，例如：

```text
syntax = "proto3";
package tutorial;

option optimize_for = LITE_RUNTIME;

message Person {
	int32 id = 1;
	repeated string name = 2;
}
```

其中，syntax 关键字表示使用的protobuf的版本，如不指定则默认使用 "proto2"；package关键字 表示“包”，生成目标语言文件后对应C++中的namespace命名空间，用于防止不同的消息类型间的命名冲突。

（syntax单词字面含义：句法，句法规则，语构）

然后使用 protobuf编译器（protoc命令）将编写好的 .proto 文件生成 目标语言文件（例如目标语言是C++，则会生成 .cc 和 .h 文件），例如：

```text
[root@linux] protoc -I=$SRC_DIR $SRC_DIR/xxx.proto --cpp_out=$DST_DIR
```

其中：

> $SRC_DIR 表示 .proto文件所在的源目录；
> $DST_DIR 表示生成目标语言代码的目标目录；
> xxx.proto 表示要对哪个.proto文件进行解析；
> --cpp_out 表示生成C++代码。

编译完成后，将会在目标目录中生成 xxx.pb.h 和 [pb.cc，](https://link.zhihu.com/?target=http%3A//xxx.pb.cc) 文件，将其引入到我们的C++工程中即可实现使用protobuf进行序列化：

在C++源文件中包含 xxx.pb.h 头文件，在g++编译时链接 [http://xxx.pb.cc](https://link.zhihu.com/?target=http%3A//xxx.pb.cc)源文件即可：

```text
g++ main_test.cpp pb.cc， -o main_test -lprotobuf
```

## 3.C++使用protobuf实现序列化的示例：

在protobuf源码中的 /examples 目录下有官方提供的protobuf使用示例：addressbook.proto

参考官方示例实现C++使用protobuf进行序列化和反序列化：

addressbook.proto :

```text
syntax = "proto3";
package tutorial;

option optimize_for = LITE_RUNTIME;

message Person {
	string name = 1;
	int32 id = 2;
	string email = 3;

	enum PhoneType {
		MOBILE = 0;
		HOME = 1;
		WORK = 2;
	}
	
	message PhoneNumber {
		string number = 1;
		PhoneType type = 2;
	}

	repeated PhoneNumber phones = 4;
}
```

生成的addressbook.pb.h 文件内容摘要：

```text
namespace tutorial {
	class Person;
	class Person_PhoneNumber;
};

class Person_PhoneNumber : public MessageLite {
public:
	Person_PhoneNumber();
	virtual ~Person_PhoneNumber();
public:
	//string number = 1;
	void clear_number();
	const string& number() const;
	void set_number(const string& value);
	
	//int32 id = 2;
	void clear_id();
	int32 id() const;
	void set_id(int32 value);

	//string email = 3; 
	//...
};
```

add_person.cpp :

```text
#include <iostream>
#include <fstream>
#include <string>
#include "pbs/addressbook.pb.h"
using namespace std;

void serialize_process() {
	cout << "serialize_process" << endl;
	tutorial::Person person;
	person.set_name("Obama");
	person.set_id(1234);
	person.set_email("1234@qq.com");

	tutorial::Person::PhoneNumber *phone1 = person.add_phones();
	phone1->set_number("110");
	phone1->set_type(tutorial::Person::MOBILE);

	tutorial::Person::PhoneNumber *phone2 = person.add_phones();
	phone2->set_number("119");
	phone2->set_type(tutorial::Person::HOME);

	fstream output("person_file", ios::out | ios::trunc | ios::binary);

	if( !person.SerializeToOstream(&output) ) {
		cout << "Fail to SerializeToOstream." << endl;
	}

	cout << "person.ByteSizeLong() : " << person.ByteSizLong() << endl;
}

void parse_process() {
	cout << "parse_process" << endl;
	tutorial::Person result;
	fstream input("person_file", ios::in | ios::binary);

	if(!result.ParseFromIstream(&input)) {
		cout << "Fail to ParseFromIstream." << endl;
	}

	cout << result.name() << endl;
	cout << result.id() << endl;
	cout << Buy and Sell Domain Names() << endl;
	for(int i = 0; i < result.phones_size(); ++i) {
		const tutorial::Person::PhoneNumber &person_phone = result.phones(i);

		switch(person_phone.type()) {
			case tutorial::Person::MOBILE :
				cout << "MOBILE phone : ";
				break;
			case tutorial::Person::HOME :
				cout << "HOME phone : ";
				break;
			case tutorial::Person::WORK :
				cout << "WORK phone : ";
				break;
			default:
				cout << "phone type err." << endl;
		}
		cout << person_phone.number() << endl;
	}
}

int main(int argc, char *argv[]) {
	serialize_process();
	parse_process();
	
	google::protobuf::ShutdownProtobufLibrary();	//删除所有已分配的内存（Protobuf使用的堆内存）
	return 0;
}
```

输出结果：

```text
[serialize_process]
person.ByteSizeLong() : 39
[parse_process]
Obama
1234
1234@qq.com
MOBILE phone : 110
HOME phone : 119
```

### **3.1 protobuf提供的序列化和反序列化的API接口函数：**

```text
class MessageLite {
public:
	//序列化：
	bool SerializeToOstream(ostream* output) const;
	bool SerializeToArray(void *data, int size) const;
	bool SerializeToString(string* output) const;
	
	//反序列化：
	bool ParseFromIstream(istream* input);
	bool ParseFromArray(const void* data, int size);
	bool ParseFromString(const string& data);
};
```

三种序列化的方法没有本质上的区别，只是序列化后输出的格式不同，可以供不同的应用场景使用。

序列化的API函数均为const成员函数，因为序列化不会改变类对象的内容， 而是将序列化的结果保存到函数入参指定的地址中。

### **3.2 .proto文件中的 option 选项：**

.proto文件中的option选项用于配置protobuf编译后生成目标语言文件中的代码量，可设置为 SPEED， CODE_SIZE， LITE_RUNTIME 三种。

默认option选项为 SPEED，常用的选项为 LITE_RUNTIME。

三者的区别在于：

> ① SPEED（默认值）：
> 表示生成的代码运行效率高，但是由此生成的代码编译后会占用更多的空间。
>
> ② CODE_SIZE：
> 与SPEED恰恰相反，代码运行效率较低，但是由此生成的代码编译后会占用更少的空间，
> 通常用于资源有限的平台，如Mobile。
>
> ③ LITE_RUNTIME：
> 生成的代码执行效率高，同时生成代码编译后的所占用的空间也非常少。
> 这是以牺牲Protobuf提供的反射功能为代价的。
> 因此我们在C++中链接Protobuf库时仅需链接libprotobuf-lite，而非protobuf。

SPEED 和 LITE_RUNTIME相比，在于调试级别上，例如 msg.SerializeToString(&str); 在 SPEED 模式下会利用反射机制打印出详细字段和字段值，但是 LITE_RUNTIME 则仅仅打印字段值组成的字符串。

因此：可以在调试阶段使用 SPEED 模式，而上线以后提升性能使用 LITE_RUNTIME 模式优化。

最直观的区别是使用三种不同的 option 选项时，编译后产生的 .pb.h 中自定义的类所继承的 protobuf类不同：

```text
//1. SPEED模式：（自定义的类继承自 Message 类）
// .proto 文件：
option optimize_for = SPEED;
// .pb.h 文件：
class Person : public ::PROTOBUF_NAMESPACE_ID::Message {};

//2. CODE_SIZE模式：（自定义的类继承自 Message 类）
// .proto 文件：
option optimize_for = CODE_SIZE;
// .pb.h 文件：
class Person : public ::PROTOBUF_NAMESPACE_ID::Message {};

//3. LITE_RUNTIME模式：（自定义的类继承自 MessageLite 类）
// .proto 文件：
option optimize_for = LITE_RUNTIME;
// .pb.h 文件：
class Person : public ::PROTOBUF_NAMESPACE_ID::MessageLite {};
```

## 4.protobuf的编码和存储方式：

① protobuf 将消息里的每个字段进行编码后，再利用TLV或者TV的方式进行数据存储；

② protobuf 对于不同类型的数据会使用不同的编码和存储方式；

③ protobuf 的编码和存储方式是其性能优越、数据体积小的原因。

原文地址：https://zhuanlan.zhihu.com/p/425528252

作者：linux

# 【NO.346】Redis原理和机制详解

## 1.什么是Redis?

Redis 是开源免费的，遵守BSD协议，是一个高性能的key-value非关系型数据库。

## 2.Redis特点：

Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。

Redis不仅仅支持简单的key-value类型的数据，同时还提供String，list，set，zset，hash等数据结构的存储。

Redis支持数据的备份，即master-slave模式的数据备份。

性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。

原子 – Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。

丰富的特性 – Redis还支持 publish/subscribe, 通知, 设置key有效期等等特性。

## 3.redis作用:

可以减轻数据库压力，查询内存比查询数据库效率高。

## 4.Redis应用：

token生成、session共享、分布式锁、自增id、验证码等。

## 5.比较重要的3个可执行文件：

redis-server：Redis服务器程序

redis-cli：Redis客户端程序，它是一个命令行操作工具。也可以使用telnet根据其纯文本协议操作。

redis-benchmark：Redis性能测试工具，测试Redis在你的系统及配置下的读写性能。

## 6.I/O复用模型和Reactor 设计模式

Redis内部实现采用epoll+自己实现的简单的事件框架。 epoll中的读、写、关闭、连接都转化成了事件，然后利用epoll的多路复用特性， 绝不在io上浪费一点时间：

**1、I/O 多路复用的封装**

I/O 多路复用其实是在单个线程中通过记录跟踪每一个sock（I/O流） 的状态来管理多个I/O流。



![img](https://pic3.zhimg.com/80/v2-bf25427a55b4fa1d9856edbcdfa29f0a_720w.webp)



因为 Redis 需要在多个平台上运行，同时为了最大化执行的效率与性能，所以会根据编译平台的不同选择不同的 I/O 多路复用函数作为子模块，提供给上层统一的接口。

redis的多路复用， 提供了select, epoll, evport, kqueue几种选择，在编译的时候来选择一种。

select是POSIX提供的， 一般的操作系统都有支撑；
epoll 是LINUX系统内核提供支持的；
evport是Solaris系统内核提供支持的；
kqueue是Mac 系统提供支持的；

```text
#ifdef HAVE_EVPORT
#include "ae_evport.c"
#else
    #ifdef HAVE_EPOLL
    #include "ae_epoll.c"
    #else
        #ifdef HAVE_KQUEUE
        #include "ae_kqueue.c"
        #else
        #include "ae_select.c"
        #endif
    #endif
#endif
```

为了将所有 IO 复用统一，Redis 为所有 IO 复用统一了类型名 aeApiState，对于 epoll 而言，类型成员就是调用 epoll_wait所需要的参数
接下来就是一些对epoll接口的封装了：包括创建 epoll(epoll_create)，注册事件(epoll_ctl)，删除事件(epoll_ctl)，阻塞监听(epoll_wait)等
创建 epoll 就是简单的为 aeApiState 申请内存空间，然后将返回的指针保存在事件驱动循环中
注册事件和删除事件就是对 epoll_ctl 的封装，根据操作不同选择不同的参数
阻塞监听是对 epoll_wait 的封装，在返回后将激活的事件保存在事件驱动中

**2、Reactor 设计模式：事件驱动循环流程**

**Redis 服务采用 Reactor 的方式来实现文件事件处理器（每一个网络连接其实都对应一个文件描述符）**

当 main 函数初始化工作完成后，就需要进行事件驱动循环，而在循环中，会调用 IO 复用函数进行监听
在初始化完成后，main 函数调用了 aeMain 函数，传入的参数就是服务器的事件驱动

Redis 对于时间事件是采用链表的形式记录的，这导致每次寻找最早超时的那个事件都需要遍历整个链表，容易造成性能瓶颈。而 libevent 是采用最小堆记录时间事件，寻找最早超时事件只需要 O(1) 的复杂度



![img](https://pic2.zhimg.com/80/v2-1928c86722f8d82fbf9c4dc8aa90e7e9_720w.webp)



如上图，IO多路复用模型使用了Reactor设计模式实现了这一机制。

通过Reactor的方式，可以将用户线程轮询IO操作状态的工作统一交给handle_events事件循环进行处理。

用户线程注册事件处理器之后可以继续执行做其他的工作（异步），而Reactor线程负责调用内核的select/epoll函数检查socket状态。当有socket被激活时，则通知相应的用户线程（或执行用户线程的回调函数），执行handle_event进行数据读取、处理的工作。由于select/epoll函数是阻塞的，因此多路IO复用模型也被称为异步阻塞IO模型。注意，这里的所说的阻塞是指select函数执行时线程被阻塞，而不是指socket。一般在使用IO多路复用模型时，socket都是设置为NONBLOCK的，不过这并不会产生影响，因为用户发起IO请求时，数据已经到达了，用户线程一定不会被阻塞。

**3、redis线程模型：**

**如图所示：**



![img](https://pic4.zhimg.com/80/v2-cdbd833e4e8ac5405f36f8f4ebd7500f_720w.webp)



简单来说，就是。我们的redis-client在操作的时候，会产生具有不同事件类型的socket。在服务端，有一段I/0多路复用程序，将其置入队列之中。然后，IO事件分派器，依次去队列中取，转发到不同的事件处理器中。

## 7.redis数据结构

> **存储字符串**
> 1.set key value：设定key持有指定的字符串value，如果该key存在则进行覆盖操作,总是返回OK
> 2.get key: 获取key的value。如果与该key关联的value不是String类型，redis将返回错误信息，因为get命令只能用于获取String value；如果该key不存在，返回null。
> 3.getset key value：先获取该key的值，然后在设置该key的值。
> 4.incr key：将指定的key的value原子性的递增1. 如果该key不存在，其初始值为0，在incr之后其值为1。如果value的值不能转成整型，如hello，该操作将执行失败并返回相应的错误信息
> 5.decr key：将指定的key的value原子性的递减1.如果该key不存在，其初始值为0，在incr之后其值为-1。如果value的值不能转成整型，如hello，该操作将执 行失败并返回相应的错误信息。
> 6.incrby key increment：将指定的key的value原子性增加increment，如果该key不存在，器初始值为0，在incrby之后，该值为increment。如果该值不能转成 整型，如hello则失败并返回错误信息
> 7.decrby key decrement：将指定的key的value原子性减少decrement，如果该key不存在，器初始值为0，在decrby之后，该值为decrement。如果该值不能 转成整型，如hello则失败并返回错误信息
> 8.append key value：如果该key存在，则在原有的value后追加该值；如果该key 不存在，则重新创建一个key/value



> **存储list类型**
> 1.lpush key value1 value2...：在指定的key所关联的list的头部插入所有的values，如果该key不存在，该命令在插入的之前创建一个与该key关联的空链表，之后再向该链表的头部插入数据。插入成功，返回元素的个数。
> 2.rpush key value1、value2…：在该list的尾部添加元素
> 3.lrange key start end：获取链表中从start到end的元素的值，start、end可为负数，若为-1则表示链表尾部的元素，-2则表示倒数第二个，依次类推…
> 4.lpushx key value：仅当参数中指定的key存在时（如果与key管理的list中没有值时，则该key是不存在的）在指定的key所关联的list的头部插入value。
> 5.rpushx key value：在该list的尾部添加元素
> 6.lpop key：返回并弹出指定的key关联的链表中的第一个元素，即头部元素
> 7.rpop key：从尾部弹出元素
> 8.rpoplpush resource destination：将链表中的尾部元素弹出并添加到头部
> 9.llen key：返回指定的key关联的链表中的元素的数量。
> 10.lset key index value：设置链表中的index的脚标的元素值，0代表链表的头元素，-1代表链表的尾元素。



> **存储Set**
> 添加或删除元素
> 1.sadd key values[value1、value2……]:向set中添加数据，如果该key的值有则不会重复添加
> 例如:sadd myset a b c
> 2.srem key members[member1、menber2…]:删除set中的指定成员
> 例如:srem myset 1 2 3
> 获得集合中的元素
> 1.smembers key :获取set中所有的成员
> smembers myset
> 2.sismember key menber :判断参数中指定的成员是否在该set中，1表示存在，0表示不存在或者该key本身就不存在(无论集合中有多少元素都可以极速的返回结果)
> 集合的差集运算 A-B
> sdiff key1 key2 … : 返回key1与key2中相差的成员，而且与key的顺序有关。即返回差集。
> 集合的交集运算 
> sinter key1 key2 key3… :返回交集
> 集合的并集运算 
> sunion key1 key2 key3… : 返回并集
> 扩展命令(了解)
> scard key : 获取set中的成员数量
> 例子:scard myset
> srandmember key : 随机返回set中的一个成员
> sdiffstore destination key1 key2 …: 将key1 key2 相差的成员存储到destination中
> sinterstore destination key[key…] : 将返回的交集存储在destination上
> suninonstore destination key[key…] : 将返回的并集存储在destination上



> **存储hash**
> 1.赋值
> hset key field value : 为指定的key设定field/value对
> hmset key field1 value1 field2 value2 field3 value3 为指定的key设定多个field/value对
> 2.取值
> hget key field : 返回指定的key中的field的值
> hmget key field1 field2 field3 : 获取key中的多个field值
> hkeys key : 获取所有的key
> hvals key :获取所有的value
> hgetall key : 获取key中的所有field 中的所有field-value
> 3.删除
> hdel key field[field…] : 可以删除一个或多个字段，返回是被删除的字段个数
> del key : 删除整个list
> 4.增加数字
> hincrby key field increment ：设置key中field的值增加increment，如: age增加20
> hincrby myhash age 5
> 自学命令:
> hexists key field : 判断指定的key中的field是否存在
> hlen key : 获取key所包含的field的数量
> hkeys key ：获得所有的key 
> hkeys myhash
> hvals key ：获得所有的value
> hvals myhash



> **存储sortedset**
> 1.添加元素
> zadd key score member score2 member2…:将所有成员以及该成员的分数存放到sorted-set中。如果该元素已经存在则会用新的分数替换原有的分数。返回值是新加入到集合中的元素个数。(根据分数升序排列)
> 2.获得元素
> zscore key member ：返回指定成员的分数
> zcard key ：获得集合中的成员数量
> 3.删除元素
> zrem key member[member…] ：移除集合中指定的成员，可以指定多个成员
> 4.范围查询
> zrange key strat end [withscores]：获取集合中角标为start-end的成员，[withscore]参数表明返回的成员包含其分数。
> zremrangebyrank key start stop ：按照排名范围删除元素
> zremrangescore key min max ：按照分数范围删除元素
> 扩展命令(了解)
> zrangebyscore key min max [withscore] [limit offset count] ：返回分数在[min,max]的成员并按照分数从低到高排序。[withscore]：显示分数；[limit offset count]；offset，表明从脚标为offset的元素开始并返回count个成员
> zincrby key increment member ：设置指定成员的增加分数。返回值是修改后的分数
> zcount key min max：获取分数在[min，max]之间的成员个数
> zrank key member：返回成员在集合中的排名(从小到大)
> zrevrank key member ：返回成员在集合中的排名(从大到小)



> key的通用操作 
> keys pattern : 获取所有与pattern匹配的key ，返回所有与该key匹配的keys。 *表示任意一个或者多个字符， ?表示任意一个字符
> del key1 key2… ：删除指定的key 
> del my1 my2 my3
> exists key ：判断该key是否存在，1代表存在，0代表不存在
> rename key newkey ：为key重命名
> expire key second：设置过期时间，单位秒
> ttl key：获取该key所剩的超时时间，如果没有设置超时，返回-1，如果返回-2表示超时不存在。
> persist key:持久化key
> 192.168.25.153:6379> expire Hello 100
> (integer) 1
> 192.168.25.153:6379> ttl Hello
> (integer) 77
> type key：获取指定key的类型。该命令将以字符串的格式返回。返回的字符串为string 、list 、set 、hash 和 zset，如果key不存在返回none。
> 例如: type newcompany
> none

## 8.redis的数据类型，以及每种数据类型的使用场景

(一)String
这个其实没啥好说的，最常规的set/get操作，value可以是String也可以是数字。一般做**一些复杂的计数功能的缓存。**

(二)hash
这里value存放的是结构化的对象，比较方便的就是操作其中的某个字段。博主在做**单点登录**的时候，就是用这种数据结构存储用户信息，以cookieId作为key，设置30分钟为缓存过期时间，能很好的模拟出类似session的效果。

(三)list
使用List的数据结构，可以**做简单的消息队列的功能**。另外还有一个就是，可以利用lrange命令，**做基于redis的分页功能**，性能极佳，用户体验好。

(四)set
因为set堆放的是一堆不重复值的集合。所以可以做**全局去重的功能**。为什么不用JVM自带的Set进行去重？因为我们的系统一般都是集群部署，使用JVM自带的Set，比较麻烦，难道为了一个做一个全局去重，再起一个公共服务，太麻烦了。
另外，就是利用交集、并集、差集等操作，可以**计算共同喜好，全部的喜好，自己独有的喜好等功能**。

(五)sorted set

sorted set多了一个权重参数score,集合中的元素能够按score进行排列。可以做**排行榜应用，取TOP N操作**。另外，参照另一篇《分布式之延时任务方案解析》，该文指出了sorted set可以用来做**延时任务**。最后一个应用就是可以做**范围查找**。

## 9.redis的过期策略以及内存淘汰机制

**分析**:这个问题其实相当重要，到底redis有没用到家，这个问题就可以看出来。比如你redis只能存5G数据，可是你写了10G，那会删5G的数据。怎么删的，这个问题思考过么？还有，你的数据已经设置了过期时间，但是时间到了，内存占用率还是比较高，有思考过原因么?
**回答**:
redis采用的是**定期删除+惰性删除策略**。
**为什么不用定时删除策略?**
定时删除,用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key,因此没有采用这一策略.
**定期删除+惰性删除是如何工作的呢?**
定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。
于是，惰性删除派上用场。也就是说在你获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除。
**采用定期删除+惰性删除就没其他问题了么?**
不是的，如果定期删除没删除key。然后你也没及时去请求key，也就是说惰性删除也没生效。这样，redis的内存会越来越高。那么就应该采用**内存淘汰机制**。
在redis.conf中有一行配置

```text
# maxmemory-policy allkeys-lru
```

该配置就是配内存淘汰策略的(什么，你没配过？好好反省一下自己)
1）noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。**应该没人用吧。**
2）allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。**推荐使用。**
3）allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。**应该也没人用吧，你不删最少使用Key,去随机删。**
4）volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。**这种情况一般是把redis既当缓存，又做持久化存储的时候才用。不推荐**
5）volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。**依然不推荐**
6）volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。**不推荐**
ps：如果没有设置 expire 的key, 不满足先决条件(prerequisites); 那么 volatile-lru, volatile-random 和 volatile-ttl 策略的行为, 和 noeviction(不删除) 基本上一致。

## 10.redis和数据库双写一致性问题

**分析**:一致性问题是分布式常见问题，还可以再分为最终一致性和强一致性。数据库和缓存双写，就必然会存在不一致的问题。答这个问题，先明白一个前提。就是**如果对数据有强一致性要求，不能放缓存。**我们所做的一切，只能保证最终一致性。另外，我们所做的方案其实从根本上来说，只能说**降低不一致发生的概率**，无法完全避免。因此，有强一致性要求的数据，不能放缓存。
**回答**:首先，采取正确更新策略，先更新数据库，再删缓存。其次，因为可能存在删除缓存失败的问题，提供一个补偿措施即可，例如利用消息队列。

## 11.如何应对缓存穿透和缓存雪崩问题

**分析**:这两个问题，说句实在话，一般中小型传统软件企业，很难碰到这个问题。如果有大并发的项目，流量有几百万左右。这两个问题一定要深刻考虑。
**回答**:如下所示

**缓存穿透：**即黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常。

**解决方案**:
(一)利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试
(二)采用异步更新策略，无论key是否取到值，都直接返回。value值中维护一个缓存失效时间，缓存如果过期，异步起一个线程去读数据库，更新缓存。需要做**缓存预热**(项目启动前，先加载缓存)操作。
(三)提供一个能迅速判断请求是否有效的拦截机制，比如，利用布隆过滤器，内部维护一系列合法有效的key。迅速判断出，请求所携带的Key是否合法有效。如果不合法，则直接返回。



**缓存雪崩**，即缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常。

**解决方案**:
(一)给缓存的失效时间，加上一个随机值，避免集体失效。
(二)使用互斥锁，但是该方案吞吐量明显下降了。
(三)双缓存。我们有两个缓存，缓存A和缓存B。缓存A的失效时间为20分钟，缓存B不设失效时间。自己做缓存预热操作。然后细分以下几个小点

- I 从缓存A读数据库，有则直接返回
- II A没有数据，直接从B读数据，直接返回，并且异步启动一个更新线程。
- III 更新线程同时更新缓存A和缓存B。

## 12.如何解决redis的并发竞争key问题

**分析**:这个问题大致就是，同时有多个子系统去set一个key。这个时候要注意什么呢？大家思考过么。需要说明一下，博主提前百度了一下，发现答案基本都是推荐用redis事务机制。博主**不推荐使用redis的事务机制。**因为我们的生产环境，基本都是redis集群环境，做了数据分片操作。你一个事务中有涉及到多个key操作的时候，这多个key不一定都存储在同一个redis-server上。因此，**redis的事务机制，十分鸡肋。**

**回答:**如下所示
(1)如果对这个key操作，**不要求顺序**
这种情况下，准备一个分布式锁，大家去抢锁，抢到锁就做set操作即可，比较简单。
(2)如果对这个key操作，**要求顺序**
假设有一个key1,系统A需要将key1设置为valueA,系统B需要将key1设置为valueB,系统C需要将key1设置为valueC.
期望按照key1的value值按照 valueA-->valueB-->valueC的顺序变化。这种时候我们在数据写入数据库的时候，需要保存一个时间戳。假设时间戳如下

```text
系统A key 1 {valueA  3:00}
系统B key 1 {valueB  3:05}
系统C key 1 {valueC  3:10}
```

那么，假设这会系统B先抢到锁，将key1设置为{valueB 3:05}。接下来系统A抢到锁，发现自己的valueA的时间戳早于缓存中的时间戳，那就不做set操作了。以此类推。

其他方法，比如利用队列，将set方法变成串行访问也可以。总之，灵活变通。

原文地址：https://zhuanlan.zhihu.com/p/222697530

作者：linux

# 【NO.347】网络丢包故障如何定位？如何解决？

## 1.引言

本期分享一个比较常见的网络问题--丢包。例如我们去ping一个网站，如果能ping通，且网站返回信息全面，则说明与网站服务器的通信是畅通的，如果ping不通，或者网站返回的信息不全等，则很可能是数据被丢包了，类似情况想必大家都不陌生。针对网络丢包，本人提供一些常见的丢包故障定位方法，希望能够帮助大家对网络丢包有更多的认识，遇到丢包莫要慌，且跟着一起来涨姿(知)势(识)···

## 2.什么是丢包

数据在Internet上是以数据包为单位传输的，单位为字节，数据在网络上传输，受网络设备，网络质量等原因的影响，使得接收到的数据少于发送出去的数据，造成丢包。

## 3.数据包接收、发送原理



![img](https://pic1.zhimg.com/80/v2-5851f0ed436b2689116ff419629aba90_720w.webp)



**发送数据包：**

1.应用程序的数据包，在TCP层增加TCP报文头，形成可传输的数据包。
2.在IP层增加IP报头，形成IP报文。
3.经过数据网卡驱动程序将IP包再添加14字节的MAC头，构成frame（暂⽆CRC），frame（暂⽆CRC）中含有发送端和接收端的MAC地址。
4.驱动程序将frame（暂⽆CRC）拷贝到网卡的缓冲区，由网卡处理。
5.⽹卡为frame（暂⽆CRC）添加头部同步信息和CRC校验，将其封装为可以发送的packet，然后再发送到网线上，这样说就完成了一个IP报文的发送了，所有连接到这个网线上的网卡都可以看到该packet。

**接收数据包：**



![img](https://pic3.zhimg.com/80/v2-e9e88cb3c8f7ab7dde9bde14c2db85be_720w.webp)



1.⽹卡收到⽹线上的packet，⾸先检查packet的CRC校验，保证完整性，然后将packet头去掉，得到frame。（⽹卡会检查MAC包内的⽬的MAC地址是否和本⽹卡的MAC地址⼀样，不⼀样则会丢弃。）
2.⽹卡将frame拷贝到预分配的ring buffer缓冲。
3.⽹卡驱动程序通知内核处理，经过TCP/IP协议栈层层解码处理。
4.应⽤程序从socket buffer 中读取数据。

## 4.核心思路

了解了收发包的原理，可以了解到丢包原因主要会涉及⽹卡设备、⽹卡驱动、内核协议栈三⼤类。以下我们将遵循“从下到上分层分析（各层可能性出现的丢包场景），然后查看关键信息，最终得出分析结果”的原则展开介绍。

## 5.目录--网络丢包情形概览

**> 硬件网卡丢包**

**> 网卡驱动丢包**

**> 以太网链路层丢包**

**> 网络IP层丢包**

**> 传输层UDP/TCP丢包**

**> 应用层socket丢包**

针对以上6种情形，分别作出如下详述~

### 5.1 硬件网卡丢包

**Ring Buffer溢出**



![img](https://pic4.zhimg.com/80/v2-7834b3041d3203255d317be31ff07743_720w.webp)



如图所示，物理介质上的数据帧到达后首先由NIC（网络适配器）读取，写入设备内部缓冲区Ring Buffer中，再由中断处理程序触发Softirq从中消费，Ring Buffer的大小因网卡设备而异。当网络数据包到达（生产）的速率快于内核处理（消费）的速率时，Ring Buffer很快会被填满，新来的数据包将被丢弃；

查看：

通过ethtool或/proc/net/dev可以查看因Ring Buffer满而丢弃的包统计，在统计项中以fifo标识：

```text
$ ethtool -S eth0|grep rx_fifo
rx_fifo_errors: 0
$ cat /proc/net/dev
Inter-|Receive | Transmitface |bytes packets errs drop fifo frame compressed 
multicast|bytes packets errs drop fifo colls carrier compressed
eth0: 17253386680731 42839525880 0 0 0 0 0 244182022 14879545018057 41657801805 0 0 0 0 0 0
```

\# 查看eth0网卡Ring Buffer最大值和当前设置

```text
$ ethtool -g eth0
```

解决方案：修改网卡eth0接收与发送硬件缓存区大小

```text
$ ethtool -G eth0 rx 4096 tx 4096
```

**网卡端口协商丢包**

\1. 查看网卡丢包统计：ethtool -S eth1/eth0



![img](https://pic1.zhimg.com/80/v2-91ba6c3a7b851589a290a89302b2db68_720w.webp)



\2. 查看网卡配置状态：ethtool eth1/eth0



![img](https://pic2.zhimg.com/80/v2-580391ef0739dc7c4d3370fc8e892b49_720w.webp)



主要查看网卡和上游网络设备协商速率和模式是否符合预期；

解决方案：

1 重新自协商： ethtool -r eth1/eth0;

2 如果上游不支持自协商，可以强制设置端口速率：

```text
ethtool -s eth1 speed 1000 duplex full autoneg off
```



**网卡流控丢包**

\1. 查看流控统计：

```text
ethtool -S eth1 | grep control
```



![img](https://pic1.zhimg.com/80/v2-8538144528f7c467a31fd01f7d2c5d54_720w.webp)



rx_flow_control_xon是在网卡的RX Buffer满或其他网卡内部的资源受限时，给交换机端口发送的开启流控的pause帧计数。对应的，tx_flow_control_xoff是在资源可用之后发送的关闭流控的pause帧计数。

2 .查看网络流控配置：ethtool -a eth1



![img](https://pic3.zhimg.com/80/v2-19677d3d4fb38f06235120f6aeb43786_720w.webp)



解决方案：关闭网卡流控

```text
ethtool -A ethx autoneg off //自协商关闭
ethtool -A ethx tx off //发送模块关闭
ethtool -A ethx rx off //接收模块关闭
```

### 5.2 报文mac地址丢包

一般计算机网卡都工作在非混杂模式下，此时网卡只接受来自网络端口的目的地址指向自己的数据，如果报文的目的mac地址不是对端的接口的mac地址，一般都会丢包，一般这种情况很有可能是源端设置静态arp表项或者动态学习的arp表项没有及时更新，但目的端mac地址已发生变化（换了网卡），没有更新通知到源端（比如更新报文被丢失，中间交换机异常等情况）；

查看：

1.目的端抓包，tcpdump可以开启混杂模式，可以抓到对应的报文，然后查看mac地址；

2.源端查看arp表或者抓包（上一跳设备），看发送的mac地址是否和下一跳目的端的mac地址一致；

解决方案：

1.刷新arp表然后发包触发arp重新学习（可能影响其他报文，增加延时，需要小心操作）；

2.可以在源端手动设置正确的静态的arp表项；

**其他网卡异常丢包**

这类异常比少见，但如果都不是上面哪些情况，但网卡统计里面任然有丢包计数，可以试着排查一下：

**网卡firmware版本:**

排查一下网卡phy芯片firmware是不是有bug，安装的版本是不是符合预期，查看 ethtool -i eth1:



![img](https://pic4.zhimg.com/80/v2-4a10244059932c745e1ea9ac45736bf3_720w.webp)



和厂家提case询问是不是已知问题，有没有新版本等；

**网线接触不良：**

如果网卡统计里面存在crc error 计数增长，很可能是网线接触不良，可以通知网管排查一下：

```text
ethtool -S eth0
```



![img](https://pic4.zhimg.com/80/v2-8324d1187d3d1ff4055d7e4b5a1ef1c3_720w.webp)



解决方案：一般试着重新插拔一下网线，或者换一根网线，排查插口是否符合端口规格等;

**报文长度丢包**

网卡有接收正确报文长度范围，一般正常以太网报文长度范围：64-1518，发送端正常情况会填充或者分片来适配，偶尔会发生一些异常情况导致发送报文不正常丢包；

查看：

```text
ethtool -S eth1|grep length_errors
```



![img](https://pic1.zhimg.com/80/v2-adbceb55e95a9dc2fbc10d8ea6cc3170_720w.webp)



解决方案：

1 调整接口MTU配置，是否开启支持以太网巨帧；

2 发送端开启PATH MTU进行合理分片；

简单总结一下网卡丢包：



![img](https://pic3.zhimg.com/80/v2-77885628288b9e551483031f79b7d846_720w.webp)



### 5.3 网卡驱动丢包

查看：ifconfig eth1/eth0 等接口



![img](https://pic4.zhimg.com/80/v2-50c859c7a6480ae61a3075694d21e4f7_720w.webp)



1.RX errors: 表示总的收包的错误数量，还包括too-long-frames错误，Ring Buffer 溢出错误，crc 校验错误，帧同步错误，fifo overruns 以及 missed pkg 等等。

2.RX dropped: 表示数据包已经进入了 Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃。

3.RX overruns: 表示了 fifo 的 overruns，这是由于 Ring Buffer(aka Driver Queue) 传输的 IO 大于 kernel 能够处理的 IO 导致的，而 Ring Buffer 则是指在发起 IRQ 请求之前的那块 buffer。很明显，overruns 的增大意味着数据包没到 Ring Buffer 就被网卡物理层给丢弃了，而 CPU 无法即使的处理中断是造成 Ring Buffer 满的原因之一，上面那台有问题的机器就是因为 interruprs 分布的不均匀(都压在 core0)，没有做 affinity 而造成的丢包。

\4. RX frame: 表示 misaligned 的 frames。

\5. 对于 TX 的来说，出现上述 counter 增大的原因主要包括 aborted transmission, errors due to carrirer, fifo error, heartbeat erros 以及 windown error，而 collisions 则表示由于 CSMA/CD 造成的传输中断。



**驱动溢出丢包**

netdev_max_backlog是内核从NIC收到包后，交由协议栈（如IP、TCP）处理之前的缓冲队列。每个CPU核都有一个backlog队列，与Ring Buffer同理，当接收包的速率大于内核协议栈处理的速率时，CPU的backlog队列不断增长，当达到设定的netdev_max_backlog值时，数据包将被丢弃。

查看:

通过查看/proc/net/softnet_stat可以确定是否发生了netdev backlog队列溢出：



![img](https://pic4.zhimg.com/80/v2-e2dc15cb2d3e6e18d048495e05483853_720w.webp)



其中：每一行代表每个CPU核的状态统计，从CPU0依次往下；每一列代表一个CPU核的各项统计：第一列代表中断处理程序收到的包总数；第二列即代表由于netdev_max_backlog队列溢出而被丢弃的包总数。从上面的输出可以看出，这台服务器统计中，并没有因为netdev_max_backlog导致的丢包。

解决方案：

netdev_max_backlog的默认值是1000，在高速链路上，可能会出现上述第二统计不为0的情况，可以通过修改内核参数
net.core.netdev_max_backlog来解决：

```text
$ sysctl -w net.core.netdev_max_backlog=2000
```

**单核负载高导致丢包**

单核CPU软中断占有高, 导致应用没有机会收发或者收包比较慢，即使调整netdev_max_backlog队列大小仍然会一段时间后丢包，处理速度跟不上网卡接收的速度;

查看：mpstat -P ALL 1



![img](https://pic1.zhimg.com/80/v2-fa0cba07837a5de86e4066910f13c14c_720w.webp)



单核软中断占有100%，导致应用没有机会收发或者收包比较慢而丢包；

**解决方案**：

1.调整网卡RSS队列配置：

查看：ethtool -x ethx；

调整：ethtool -X ethx xxxx；

2.看一下网卡中断配置是否均衡 cat /proc/interrupts

调整：

```text
1） irqbalance 调整；
# 查看当前运行情况
service irqbalance status
# 终止服务
service irqbalance stop
2） 中断绑CPU核 echo mask > /proc/irq/xxx/smp_affinity
```

3.根据CPU和网卡队列个数调整网卡多队列和RPS配置

-CPU大于网卡队列个数：

查看网卡队列 ethtool -x ethx；

协议栈开启RPS并设置RPS；

```text
echo $mask（CPU配置）> /sys/class/net/$eth/queues/rx-$i/rps_cpus
echo 4096（网卡buff）> /sys/class/net/$eth/queues/rx-$i/rps_flow_cnt
2）CPU小于网卡队列个数，绑中断就可以，可以试着关闭RPS看一下效果：
echo 0 > /sys/class/net/<dev>/queues/rx-<n>/rps_cpus
```

4.numa CPU调整，对齐网卡位置，可以提高内核处理速度，从而给更多CPU给应用收包，减缓丢包概率；

查看网卡numa位置：

```text
ethtool -i eth1|grep bus-info
lspci -s bus-info -vv|grep node
```

上面中断和RPS设置里面mask需要重新按numa CPU分配重新设置;

5.可以试着开启中断聚合（看网卡是否支持）

查看 :

```text
ethtool -c ethx
Coalesce parameters for eth1:
Adaptive RX: on  TX: on
stats-block-usecs: 0
sample-interval: 0
pkt-rate-low: 0
pkt-rate-high: 0


rx-usecs: 25
rx-frames: 0
rx-usecs-irq: 0
rx-frames-irq: 256


tx-usecs: 25
tx-frames: 0
tx-usecs-irq: 0
tx-frames-irq: 256


rx-usecs-low: 0
rx-frame-low: 0
tx-usecs-low: 0
tx-frame-low: 0


rx-usecs-high: 0
rx-frame-high: 0
tx-usecs-high: 0
tx-frame-high: 0
```

调整：

```text
ethtool -C ethx adaptive-rx on
```

简单总结一下网卡驱动丢包处理：



![img](https://pic3.zhimg.com/80/v2-c541bbfdda72c14ad7948e2eff08989a_720w.webp)



### 5.4 内核协议栈丢包

**以太网链路层丢包**

**neighbor系统arp丢包**

**arp_ignore配置丢包**

arp_ignore参数的作用是控制系统在收到外部的arp请求时，是否要返回arp响应。arp_ignore参数常用的取值主要有0，1，2，3~8较少用到；

查看：sysctl -a|grep arp_ignore



![img](https://pic3.zhimg.com/80/v2-c8dc39df7f8246cacfa5692c44ea0a5e_720w.webp)



**解决方案**：根据实际场景设置对应值；

0：响应任意网卡上接收到的对本机IP地址的arp请求（包括环回网卡上的地址），而不管该目的IP是否在接收网卡上。

1：只响应目的IP地址为接收网卡上的本地地址的arp请求。

2：只响应目的IP地址为接收网卡上的本地地址的arp请求，并且arp请求的源IP必须和接收网卡同网段。

3：如果ARP请求数据包所请求的IP地址对应的本地地址其作用域（scope）为主机（host），则不回应ARP响应数据包，如果作用域为全局（global）或链路（link），则回应ARP响应数据包。



![img](https://pic1.zhimg.com/80/v2-1e153aa9df985000afd03633a01e5808_720w.webp)





![img](https://pic2.zhimg.com/80/v2-09affee6e5ffada7598c140df8d9c459_720w.webp)



**arp_filter配置丢包**

在多接口系统里面（比如腾讯云的弹性网卡场景），这些接口都可以回应arp请求，导致对端有可能学到不同的mac地址，后续报文发送可能由于mac地址和接收报文接口mac地址不一样而导致丢包，arp_filter主要是用来适配这种场景；

查看：

```text
sysctl -a | grep arp_filter
```



![img](https://pic1.zhimg.com/80/v2-19c8d804c4cd71a7d345bccae4ad414c_720w.webp)



解决方案：

```text
根据实际场景设置对应的值，一般默认是关掉此过滤规则，特殊情况可以打开；
0：默认值，表示回应arp请求的时候不检查接口情况；
1：表示回应arp请求时会检查接口是否和接收请求接口一致，不一致就不回应；
```

**arp表满导致丢包**

比如下面这种情况，由于突发arp表项很多 超过协议栈默认配置，发送报文的时候部分arp创建失败，导致发送失败，从而丢包：



![img](https://pic4.zhimg.com/80/v2-232434337ad998a41a6ce4f53e8e2273_720w.webp)



查看：

- 查看arp状态：cat /proc/net/stat/arp_cache ，table_fulls统计：
- 查看dmesg消息（内核打印）：



![img](https://pic4.zhimg.com/80/v2-f1bdb9d80feda7ef195a9d65faca286f_720w.webp)



```text
dmesg|grep neighbour
neighbour: arp_cache: neighbor table overflow!
```

- 查看当前arp表大小：ip n|wc -l

查看系统配额：

```text
sysctl -a |grep net.ipv4.neigh.default.gc_thresh
gc_thresh1：存在于ARP高速缓存中的最少层数，如果少于这个数，垃圾收集器将不会运行。缺省值是128。


gc_thresh2 ：保存在 ARP 高速缓存中的最多的记录软限制。垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。缺省值是 512。
gc_thresh3 ：保存在 ARP 高速缓存中的最多记录的硬限制，一旦高速缓存中的数目高于此，垃圾收集器将马上运行。缺省值是1024。
```

一般在内存足够情况下，可以认为gc_thresh3 值是arp 表总大小；



![img](https://pic4.zhimg.com/80/v2-3c47ceb5dfaaf041a5109ff9114646df_720w.webp)



**解决方案**：根据实际arp最大值情况（比如访问其他子机最大个数），调整arp表大小

```text
$ sudo sysctl -w net.ipv4.neigh.default.gc_thresh1=1024
$ sudo sysctl -w net.ipv4.neigh.default.gc_thresh2=2048
$ sudo sysctl -w net.ipv4.neigh.default.gc_thresh3=4096
$ sudo sysctl  -p
```

**arp请求缓存队列溢出丢包**

查看：

```text
cat /proc/net/stat/arp_cache ，unresolved_discards是否有新增计数
```

解决方案：根据客户需求调整缓存队列大小unres_qlen_bytes：



![img](https://pic1.zhimg.com/80/v2-d25caeccc0aa2ebc5678d7163a732014_720w.webp)



### 5.5 网络IP层丢包

**接口ip地址配置丢包**

\1. 本机服务不通，检查lo接口有没有配置地址是127.0.0.1；

2 .本机接收失败， 查看local路由表：ip r show table local|grep 子机ip地址；这种丢包一般会出现在多IP场景，子机底层配置多ip失败，导致对应ip收不到包而丢包；



![img](https://pic2.zhimg.com/80/v2-ae7532b75c48908cb5d4e754f06f27ed_720w.webp)



解决方案：

1.配置正确接口ip地址；比如ip a add 1.1.1.1 dev eth0

2.如果发现接口有地址还丢包，可能是local路由表没有对应条目，紧急情况下，可以用手工补上：

比如ip r add local 本机ip地址 dev eth0 table local ；

### 5.6 路由丢包

**路由配置丢包**

查看：

1.查看配置 路由是否设置正确（是否可达），是否配置策略路由（在弹性网卡场景会出现此配置）ip rule：



![img](https://pic3.zhimg.com/80/v2-be579c18bd1eeae9902287f9ea005272_720w.webp)



然后找到对应路由表。查看路由表：



![img](https://pic3.zhimg.com/80/v2-87e87f2ded85854d40bba215cd9899ca_720w.webp)



或者直接用 ip r get x.x.x.x，让系统帮你查找是否存在可达路由，接口是否符合预期；

2.查看系统统计信息：

```text
netstat -s|grep "dropped because of missing route"
```

解决方案：重新配置正确的路由；

**反向路由过滤丢包**

反向路由过滤机制是Linux通过反向路由查询，检查收到的数据包源IP是否可路由（Loose mode）、是否最佳路由（Strict mode），如果没有通过验证，则丢弃数据包，设计的目的是防范IP地址欺骗攻击。

查看：

rp_filter提供三种模式供配置：

0 - 不验证

1 - RFC3704定义的严格模式：对每个收到的数据包，查询反向路由，如果数据包入口和反向路由出口不一致，则不通过

2 - RFC3704定义的松散模式：对每个收到的数据包，查询反向路由，如果任何接口都不可达，则不通过

查看当前rp_filter策略配置：

$cat /proc/sys/net/ipv4/conf/eth0/rp_filter

如果这里设置为1，就需要查看主机的网络环境和路由策略是否可能会导致客户端的入包无法通过反向路由验证了。

从原理来看这个机制工作在网络层，因此，如果客户端能够Ping通服务器，就能够排除这个因素了。

解决方案：

根据实际网络环境将rp_filter设置为0或2：

```text
$ sysctl -w net.ipv4.conf.all.rp_filter=2或
$ sysctl -w net.ipv4.conf.eth0.rp_filter=2
```

### 5.7 防火墙丢包

**客户设置规则导致丢包**

查看：

```text
iptables -nvL |grep DROP ;
```

解决方案： 修改防火墙规则；

**连接跟踪导致丢包**

**连接跟踪表溢出丢包**

kernel 用 ip_conntrack 模块来记录 iptables 网络包的状态，并把每条记录保存到 table 里（这个 table 在内存里，可以通过/proc/net/ip_conntrack 查看当前已经记录的总数），如果网络状况繁忙，比如高连接，高并发连接等会导致逐步占用这个 table 可用空间，一般这个 table 很大不容易占满并且可以自己清理，table 的记录会一直呆在 table 里占用空间直到源 IP 发一个 RST 包，但是如果出现被攻击、错误的网络配置、有问题的路由/路由器、有问题的网卡等情况的时候，就会导致源 IP 发的这个 RST 包收不到，这样就积累在 table 里，越积累越多直到占满。无论，哪种情况导致table变满，满了以后就会丢包，出现外部无法连接服务器的情况。内核会报如下错误信息：kernel: ip_conntrack: table full, dropping packet；

查看当前连接跟踪数 :

```text
cat /proc/sys/net/netfilter/nf_conntrack_max
```

解决方案：

```text
增大跟踪的最大条数
net.netfilter.nf_conntrack_max  = 3276800
减少跟踪连接的最大有效时间
net.netfilter.nf_conntrack_tcp_timeout_established = 1200
net.netfilter.nf_conntrack_udp_timeout_stream = 180
net.netfilter.nf_conntrack_icmp_timeout = 30
```

### 5.8 ct创建冲突失导致丢包

查看：当前连接跟踪统计：cat 
/proc/net/stat/nf_conntrack，可以查各种ct异常丢包统计



![img](https://pic4.zhimg.com/80/v2-7356c5bbf469b2630bd59c074efc2d27_720w.webp)



解决方案：内核热补丁修复或者更新内核版本（合入补丁修改）；

### 5.9 传输层UDP/TCP丢包

**tcp 连接跟踪安全检查丢包**

丢包原因：由于连接没有断开，但服务端或者client之前出现过发包异常等情况（报文没有经过连接跟踪模块更新窗口计数），没有更新合法的window范围，导致后续报文安全检查被丢包；协议栈用
nf_conntrack_tcp_be_liberal 来控制这个选项：

1：关闭，只有不在tcp窗口内的rst包被标志为无效；

0：开启; 所有不在tcp窗口中的包都被标志为无效；

查看：

查看配置 ：

```text
sysctl -a|grep nf_conntrack_tcp_be_liberal 
net.netfilter.nf_conntrack_tcp_be_liberal = 1
```

查看log：

一般情况下netfiler模块默认没有加载log，需要手动加载;



```text
modprobe ipt_LOG11
sysctl -w net.netfilter.nf_log.2=ipt_LOG
```

然后发包后在查看syslog；

解决方案：根据实际抓包分析情况判断是不是此机制导致的丢包，可以试着关闭试一下；

### 5.10 分片重组丢包

情况总结：**超时**

查看：

```text
netstat -s|grep timeout
601 fragments dropped after timeout
```

解决方法：调整超时时间

```text
net.ipv4.ipfrag_time = 30
sysctl -w net.ipv4.ipfrag_time=60
```

**frag_high_thresh, 分片的内存超过一定阈值会导致系统安全检查丢包**

查看：

```text
netstat -s|grep reassembles
8094 packet reassembles failed
```

解决方案：调整大小

```text
net.ipv4.ipfrag_high_thresh 
net.ipv4.ipfrag_low_thresh
```

**分片安全距检查离丢包**

查看：

```text
netstat -s|grep reassembles
8094 packet reassembles failed
```

解决方案： 把ipfrag_max_dist设置为0，就关掉此安全检查



![img](https://pic3.zhimg.com/80/v2-ec3b7052d7451b6ecbd03c40218dac8e_720w.webp)



pfrag_max_dist特性，在一些场景下其实并不适用：

1.有大量的网络报文交互

2.发送端的并发度很高，同时SMP架构，导致很容易造成这种乱序情况；



**分片hash bucket冲突链太长超过系统默认值128**

查看：

```text
dmesg|grep “Dropping fragment”
inet_frag_find: Fragment hash bucket 128 list length grew over limit. Dropping fragment.
```

解决方案：热补丁调整hash大小；

**系统内存不足，创建新分片队列失败**

查看方法：

```text
netstat -s|grep reassembles
8094 packet reassembles failed
```

dropwatch查看丢包位置 ：



![img](https://pic4.zhimg.com/80/v2-3cd61b778807e36cfb3e99887c7cb25b_720w.webp)



解决方案：

a.增大系统网络内存：

```text
net.core.rmem_default 
net.core.rmem_max 
net.core.wmem_default
```

b.系统回收内存：

紧急情况下，可以用 /proc/sys/vm/drop_caches, 去释放一下虚拟内存；

```text
To free pagecache:
# echo 1 > /proc/sys/vm/drop_caches
To free dentries and inodes:
# echo 2 > /proc/sys/vm/drop_caches
To free pagecache, dentries and inodes:
echo 3 > /proc/sys/vm/drop_caches
```

### 5.11 MTU丢包



![img](https://pic2.zhimg.com/80/v2-ae138a2f95101add812aea71ea1617fd_720w.webp)



查看：

1.检查接口MTU配置，ifconfig eth1/eth0，默认是1500；

2.进行MTU探测，然后设置接口对应的MTU值；

解决方案：

\1. 根据实际情况，设置正确MTU值；

\2. 设置合理的tcp mss，启用TCP MTU Probe:

```text
cat /proc/sys/net/ipv4/tcp_mtu_probing:
tcp_mtu_probing - INTEGER Controls TCP Packetization-Layer Path MTU Discovery.
Takes three values:
0 - Disabled 
1 - Disabled by default, enabled when an ICMP black hole detected
2 - Always enabled, use initial MSS of tcp_base_mss.
```

### 5.12 tcp层丢包

**TIME_WAIT过多丢包**

大量TIMEWAIT出现，并且需要解决的场景，在高并发短连接的TCP服务器上，当服务器处理完请求后立刻按照主动正常关闭连接。。。这个场景下，会出现大量socket处于TIMEWAIT状态。如果客户端的并发量持续很高，此时部分客户端就会显示连接不上；

查看：

查看系统log ：

```text
dmsg
TCP: time wait bucket table overflow；
```

查看系统配置：

```text
sysctl -a|grep tcp_max_tw_buckets
net.ipv4.tcp_max_tw_buckets = 16384
```

解决方案：

\1. tw_reuse，tw_recycle 必须在客户端和服务端timestamps 开启时才管用（默认打开）

\2. tw_reuse 只对客户端起作用，开启后客户端在1s内回收；

\3. tw_recycle对客户端和服务器同时起作用，开启后在3.5*RTO 内回收，RTO 200ms~ 120s具体时间视网络状况。内网状况比tw_reuse稍快，公网尤其移动网络大多要比tw_reuse 慢，优点就是能够回收服务端的TIME_WAIT数量；

在服务端，如果网络路径会经过NAT节点，不要启用net.ipv4.tcp_tw_recycle，会导致时间戳混乱，引起其他丢包问题；

\4. 调整tcp_max_tw_buckets大小，如果内存足够：

```text
sysctl -w net.ipv4.tcp_max_tw_buckets=163840；
```

**时间戳异常丢包**

当多个客户端处于同一个NAT环境时，同时访问服务器，不同客户端的时间可能不一致，此时服务端接收到同一个NAT发送的请求，就会出现时间戳错乱的现象，于是后面的数据包就被丢弃了，具体的表现通常是是客户端明明发送的SYN，但服务端就是不响应ACK。在服务器借助下面的命令可以来确认数据包是否有不断被丢弃的现象。

检查：

```text
netstat -s | grep rejects
```

解决方案：

如果网络路径会经过NAT节点，不要启用net.ipv4.tcp_tw_recycle；

**TCP队列问题导致丢包**

**原理：**

**tcp状态机（三次握手）**



![img](https://pic1.zhimg.com/80/v2-b5fdafc659ca988900140dc2f28ef528_720w.webp)



**协议处理：**



![img](https://pic2.zhimg.com/80/v2-622b97c791235cfc5d75fae818836121_720w.webp)



**一个是半连接队列（syn queue）：**

在三次握手协议中，服务器维护一个半连接队列，该队列为每个客户端的SYN包开设一个条目(服务端在接收到SYN包的时候，就已经创建了request_sock结构，存储在半连接队列中)，该条目表明服务器已收到SYN包，并向客户发出确认，正在等待客户的确认包（会进行第二次握手发送SYN＋ACK的包加以确认）。这些条目所标识的连接在服务器处于Syn_RECV状态，当服务器收到客户的确认包时，删除该条目，服务器进入ESTABLISHED状态。该队列为SYN队列，长度为max(64,
/proc/sys/net/ipv4/tcp_max_syn_backlog), 机器的tcp_max_syn_backlog值在/proc/sys/net/ipv4/tcp_max_syn_backlog下配置;

**一个是全连接队列（accept queue）：**

第三次握手时，当server接收到ACK 报之后， 会进入一个新的叫 accept 的队列，该队列的长度为 min(backlog, somaxconn)，默认情况下，somaxconn 的值为 128，表示最多有 129 的 ESTAB 的连接等待 accept()，而 backlog 的值则应该是由 int listen(int sockfd, int backlog) 中的第二个参数指定，listen 里面的 backlog 可以有我们的应用程序去定义的;

查看：

连接建立失败,syn丢包：

```text
netstat -s |grep -i listen
SYNs to LISTEN sockets dropped
```

也会受到连接满丢包影响

解决方案： 增加大小 tcp_max_syn_backlog

连接满丢包

-xxx times the listen queue of a socket overflowed

查看：

- 查看accept队列大小 ：net.core.somaxconn
- ss -lnt查询socket队列 ：LISTEN 状态: Recv-Q 表示的当前等待服务端调用 accept 完成三次握手的 listen backlog 数值，也就是说，当客户端通过 connect() 去连接正在 listen() 的服务端时，这些连接会一直处于这个 queue 里面直到被服务端 accept()；Send-Q 表示的则是最大的 listen backlog 数值，这就就是上面提到的 min(backlog, somaxconn) 的值，
- 看一下是不是应用程序设置限制， int listen(int sockfd, int backlog)；

解决方案：

- Linux内核参进行优化，可以缓解压力 tcp_abort_on_overflow=1
- 调整net.core.somaxconn大小;
- 应用程序设置问题，通知客户程序修改；

**syn flood攻击丢包**

目前，Linux下默认会进行5次重发SYN-ACK包，重试的间隔时间从1s开始，下次的重试间隔时间是前一次的双倍，5次的重试时间间隔为1s, 2s, 4s, 8s, 16s，总共31s，第5次发出后还要等32s都知道第5次也超时了，所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 63s，TCP才会把断开这个连接。由于，SYN超时需要63秒，那么就给攻击者一个攻击服务器的机会，攻击者在短时间内发送大量的SYN包给Server(俗称 SYN flood 攻击)，用于耗尽Server的SYN队列。对于应对SYN 过多的问题;

查看： 查看syslog： kernel: [3649830.269068] TCP: Possible SYN flooding on port xxx. Sending cookies. Check SNMP counters.

解决方案：

- 增大tcp_max_syn_backlog
- 减少tcp_synack_retries
- 启用tcp_syncookies
- 启用tcp_abort_on_overflow， tcp_abort_on_overflow修改成 1，1表示第三步的时候如果全连接队列满了，server发送一个reset包给client，表示废掉这个握手过程和这个连接（本来在server端这个连接就还没建立起来）；

**PAWS机制丢包**

原理：PAWS(Protect Against Wrapped Sequence numbers)，高带宽下，TCP序列号可能在较短的时间内就被重复使用(recycle/wrapped)
就可能导致同一条TCP流在短时间内出现序号一样的两个合法的数据包及其确认包。

查看：

```text
$netstat -s |grep -e "passive connections rejected because of time 
stamp" -e "packets rejects in established connections because of 
timestamp” 
387158 passive connections rejected because of time stamp
825313 packets rejects in established connections because of timestamp
```

通过sysctl查看是否启用了tcp_tw_recycle及tcp_timestamp:

```text
$ sysctl net.ipv4.tcp_tw_recycle
net.ipv4.tcp_tw_recycle = 1 
$ sysctl net.ipv4.tcp_timestamps
net.ipv4.tcp_timestamps = 1
```

\1. tcp_tw_recycle参数。它用来快速回收TIME_WAIT连接，不过如果在NAT环境下会引发问题;

\2. 当多个客户端通过NAT方式联网并与服务端交互时，服务端看到的是同一个IP，也就是说对服务端而言这些客户端实际上等同于一个，可惜由于这些客户端的时间戳可能存在差异，于是乎从服务端的视角看，便可能出现时间戳错乱的现象，进而直接导致时间戳小的数据包被丢弃。如果发生了此类问题，具体的表现通常是是客户端明明发送的SYN，但服务端就是不响应ACK。

解决方案：

在NAT环境下，清除tcp时间戳选项，或者不开启tcp_tw_recycle参数；

**TLP问题丢包**

TLP主要是为了解决尾丢包重传效率的问题，TLP能够有效的避免较长的RTO超时，进而提高TCP性能，详细参考文章：

[http://perthcharles.github.io/2015/10/31/wiki-network-tcp-tlp/](https://link.zhihu.com/?target=http%3A//perthcharles.github.io/2015/10/31/wiki-network-tcp-tlp/)；

但在低时延场景下（短连接小包量），TLP与延迟ACK组合可能会造成无效重传，导致客户端感发现大量假重传包，加大了响应延迟；

查看：

查看协议栈统计：

```text
netstat -s |grep TCPLossProbes
```

查看系统配置：

```text
sysctl -a | grep tcp_early_retrans
```



![img](https://pic2.zhimg.com/80/v2-9ebc9757263871572c8b4d21fa89c789_720w.webp)



解决方案：

1.关掉延迟ack，打开快速ack；

2.linux实现nodelay语意不是快速ack，只是关闭nagle算法；

3.打开快速ack选项，socket里面有个 TCP_QUICKACK 选项， 需要每次recv后再设置一次。

**内存不足导致丢包**

查看：

查看log：

```text
dmesg|grep “out of memory”
```

查看系统配置：

```text
cat /proc/sys/net/ipv4/tcp_mem
cat /proc/sys/net/ipv4/tcp_rmem
cat /proc/sys/net/ipv4/tcp_wmem
```

解决方案：

根据TCP业务并发流量，调整系统参数，一般试着增大2倍或者其他倍数来看是否缓解；

```text
sysclt -w net.ipv4.tcp_mem=
sysclt -w net.ipv4.tcp_wmem=
sysclt -w net.ipv4.tcp_rmem=
sysctl -p
```

**TCP超时丢包**

查看：

抓包分析一下网络RTT：



![img](https://pic4.zhimg.com/80/v2-f5e293fd0fe3487a95e662f000f24483_720w.webp)



用其他工具测试一下当前端到端网络质量（hping等）；

```text
# hping -S 9.199.10.104 -A
HPING 9.199.10.104 (bond1 9.199.10.104): SA set, 40 headers + 0 data bytes
len=46 ip=9.199.10.104 ttl=53 DF id=47617 sport=0 flags=R seq=0 win=0 rtt=38.3 ms
len=46 ip=9.199.10.104 ttl=53 DF id=47658 sport=0 flags=R seq=1 win=0 rtt=38.3 ms
len=46 ip=9.199.10.104 ttl=53 DF id=47739 sport=0 flags=R seq=2 win=0 rtt=30.4 ms
len=46 ip=9.199.10.104 ttl=53 DF id=47842 sport=0 flags=R seq=3 win=0 rtt=30.4 ms
len=46 ip=9.199.10.104 ttl=53 DF id=48485 sport=0 flags=R seq=4 win=0 rtt=38.7 ms
len=46 ip=9.199.10.104 ttl=53 DF id=49274 sport=0 flags=R seq=5 win=0 rtt=34.1 ms
len=46 ip=9.199.10.104 ttl=53 DF id=49491 sport=0 flags=R seq=6 win=0 rtt=30.3 ms
```

解决方案：

- 关闭Nagle算法，减少小包延迟；
- 关闭延迟ack:

```text
sysctl -w net.ipv4.tcp_no_delay_ack=1
```

**TCP乱序丢包**

此时TCP会无法判断是数据包丢失还是乱序，因为丢包和乱序都会导致接收端收到次序混乱的数据包，造成接收端的数据空洞。TCP会将这种情况暂定为数据包的乱序，因为乱序是时间问题（可能是数据包的迟到），而丢包则意味着重传。当TCP意识到包出现乱序的情况时，会立即ACK，该ACK的TSER部分包含的TSEV值会记录当前接收端收到有序报文段的时刻。这会使得数据包的RTT样本值增大，进一步导致RTO时间延长。这对TCP来说无疑是有益的，因为TCP有充分的时间判断数据包到底是失序还是丢了来防止不必要的数据重传。当然严重的乱序则会让发送端以为是丢包一旦重复的ACK超过TCP的阈值，便会触发超时重传机制，以及时解决这种问题；

查看：抓包分析是否存在很多乱序报文：



![img](https://pic4.zhimg.com/80/v2-686210eed2073d7dfdfd36745d0e80fb_720w.webp)



解决方案：如果在多径传输场景或者网络质量不好，可以通过修改下面值来提供系统对TCP无序传送的容错率：



![img](https://pic1.zhimg.com/80/v2-65d7941aca8d30c54bf93c723a01fcec_720w.webp)



### 5.13 拥塞控制丢包

在互联网发展的过程当中，TCP算法也做出了一定改变，先后演进了

Reno、NewReno、Cubic和Vegas，这些改进算法大体可以分为基于丢包和基于延时的拥塞控制算法。基于丢包的拥塞控制算法以Reno、NewReno为代表，它的主要问题有Buffer bloat和长肥管道两种，基于丢包的协议拥塞控制机制是被动式的，其依据网络中的丢包事件来做网络拥塞判断。即使网络中的负载很高，只要没有产生拥塞丢包，协议就不会主动降低自己的发送速度。最初路由器转发出口的Buffer 是比较小的，TCP在利用时容易造成全局同步，降低带宽利用率，随后路由器厂家由于硬件成本下降不断地增加Buffer，基于丢包反馈的协议在不丢包的情况下持续占用路由器buffer，虽然提高了网络带宽的利用率，但同时也意味着发生拥塞丢包后，网络抖动性加大。另外对于带宽和RTT都很高的长肥管道问题来说，管道中随机丢包的可能性很大，TCP的默认buffer设置比较小加上随机丢包造成的cwnd经常下折，导致带宽利用率依旧很低； BBR（Bottleneck Bandwidth and Round-trip propagation time）是一种基于带宽和延迟反馈的拥塞控制算法。目前已经演化到第二版，是一个典型的封闭反馈系统，发送多少报文和用多快的速度发送这些报文都是在每次反馈中不断调节。在BBR提出之前，拥塞控制都是基于事件的算法，需要通过丢包或延时事件驱动；BBR提出之后，拥塞控制是基于反馈的自主自动控制算法，对于速率的控制是由算法决定，而不由网络事件决定，BBR算法的核心是找到最大带宽（Max BW）和最小延时（Min RTT）这两个参数，最大带宽和最小延时的乘积可以得到BDP(Bandwidth Delay Product), 而BDP就是网络链路中可以存放数据的最大容量。BDP驱动Probing State Machine得到Rate quantum和cwnd，分别设置到发送引擎中就可以解决发送速度和数据量的问题。

Linux 4.9内核首次采用BBR拥塞控制算法第一个版本，BBR抗丢包能力比其他算法要强，但这个版本在某些场景下面有问题（缺点），BBR在实时音视频领域存在的问题，深队列竞争不过Cubic。

问题现象就是：在深队列场景，BBR的ProbeRTT阶段只发4个包，发送速率下降太多会引发延迟加大和卡顿问题。

查看：

```text
ss -sti //在源端 ss -sti|grep 10.125.42.49:47699 -A 3 （ 10.125.42.49:47699 是目的端地址和端口号）
```



![img](https://pic1.zhimg.com/80/v2-cb01dbb092af4f71cc9ed4cf0766eda0_720w.webp)





![img](https://pic3.zhimg.com/80/v2-1f78761d69a65e84284d871cfaafb38a_720w.webp)



解决方案：

- ProbeRTT并不适用实时音视频领域，因此可以选择直接去除，或者像BBRV2把probe RTT缩短到2.5s一次，使用0.5xBDP发送；
- 如果没有特殊需求，切换成稳定的cubic算法；

### 5.14 UDP层丢包

**收发包失败丢包**

查看：netstat 统计

如果有持续的 receive buffer errors/send buffer errors 计数；



![img](https://pic3.zhimg.com/80/v2-ec63c0a62afbca78b84868b8c2d95eae_720w.webp)



解决方案：

1. CPU负载（多核绑核配置），网络负载（软中断优化，调整驱动队列netdev_max_backlog），内存配置（协议栈内存）；
2. 按峰值在来，增大buffer缓存区大小：

```text
net.ipv4.udp_mem = xxx
net.ipv4.udp_rmem_min = xxx
net.ipv4.udp_wmem_min = xxx
```

\3. 调整应用设计：

- UDP本身就是无连接不可靠的协议，适用于报文偶尔丢失也不影响程序状态的场景，比如视频、音频、游戏、监控等。对报文可靠性要求比较高的应用不要使用 UDP，推荐直接使用 TCP。当然，也可以在应用层做重试、去重保证可靠性
- 如果发现服务器丢包，首先通过监控查看系统负载是否过高，先想办法把负载降低再看丢包问题是否消失
- 如果系统负载过高，UDP丢包是没有有效解决方案的。如果是应用异常导致CPU、memory、IO 过高，请及时定位异常应用并修复；如果是资源不够，监控应该能及时发现并快速扩容
- 对于系统大量接收或者发送UDP报文的，可以通过调节系统和程序的 socket buffer size 来降低丢包的概率
- 应用程序在处理UDP报文时，要采用异步方式，在两次接收报文之间不要有太多的处理逻辑

### 5.15 应用层socket丢包

**socket缓存区接收丢包**

查看：

\1. 抓包分析是否存在丢包情况；

\2. 查看统计：

```text
netstat -s|grep "packet receive errors"
```

解决方案：

调整socket缓冲区大小：

```text
socket配置（所有协议socket）：
# Default Socket Receive Buffer
net.core.rmem_default = 31457280
# Maximum Socket Receive Buffer
net.core.rmem_max = 67108864
```

具体大小调整原理：

缓冲区大小没有任何设置值是最佳的，因为最佳大小随具体情况而不同

缓冲区估算原理：在数据通信中，带宽时延乘积（英语：bandwidth-delay product；或称带宽延时乘积、带宽延时积等）指的是一个数据链路的能力（每秒比特）与来回通信延迟（单位秒）的乘积。[1][2]其结果是以比特（或字节）为单位的一个数据总量，等同在任何特定时间该网络线路上的最大数据量——已发送但尚未确认的数据。

**BDP = 带宽 \* RTT**

可以通过计算当面节点带宽和统计平均时延来估算BDP，即缓冲区的大小，可以参考下面常见场景估计：



![img](https://pic1.zhimg.com/80/v2-06c7ac3b26f553c0d6d861ba491ebbf8_720w.webp)



**应用设置tcp连接数大小丢包**

查看：

请参考上面TCP连接队列分析；

解决方案：

设置合理的连接队列大小，当第三次握手时，当server接收到ACK 报之后， 会进入一个新的叫 accept 的队列，该队列的长度为 min(backlog, somaxconn)，默认情况下，somaxconn 的值为 128，表示最多有 129 的 ESTAB 的连接等待 accept()，而 backlog 的值则应该是由 int listen(int sockfd, int backlog) 中的第二个参数指定，listen 里面的 backlog 可以有我们的应用程序去定义的；

**应用发送太快导致丢包**

查看统计：

```text
netstat -s|grep "send buffer errors
```

解决方案：

- ICMP/UDP没有流控机制，需要应用设计合理发送方式和速度，照顾到底层buff大小和CPU负载以及网络带宽质量；
- 设置合理的sock缓冲区大小：

```text
setsockopt(s,SOL_SOCKET,SO_SNDBUF,  i(const char*)&nSendBuf,sizeof(int));
```

- 调整系统socket缓冲区大小：

```text
# Default Socket Send Buffer
   net.core.wmem_default = 31457280
   # Maximum Socket Send Buffer
   net.core.wmem_max = 33554432
```

**附：简单总结一下内核协议栈丢包：**



![img](https://pic1.zhimg.com/80/v2-e273e4680a89be35eee8724f56fbbaa8_720w.webp)



## 6.相关工具介绍

1.dropwatch工具

原理： 监听 kfree_skb（把网络报文丢弃时会调用该函数）函数或者事件吗，然后打印对应调用堆栈；想要详细了解 linux 系统在执行哪个函数时丢包的话，可以使用 dropwatch 工具，它监听系统丢包信息，并打印出丢包发生的函数：



![img](https://pic3.zhimg.com/80/v2-00f41347991ba42ad28c67deb745b1ba_720w.webp)



\2. tcpdump工具

原理: tcpdump 是一个Unix下一个功能强大的网络抓包工具，它允许用户拦截和显示发送或收到过网络连接到该计算机的TCP/IP和其他数据包



![img](https://pic3.zhimg.com/80/v2-47c7d4b5aecdaa9527af834bad7f48b2_720w.webp)



抓包命令参考：

[https://www.tcpdump.org/manpages/tcpdump.1.html](https://link.zhihu.com/?target=https%3A//www.tcpdump.org/manpages/tcpdump.1.html)

数据包分析：

1.用wireshark工具分析 参考：Wireshark数据包分析实战.pdf

2.可以转化生成CSV数据，用Excel或者shell去分析特定场景报文；

3.可以在linux上用tshark命令行工具进行分析:

[https://www.wireshark.org/docs/man-pages/tshark.html](https://link.zhihu.com/?target=https%3A//www.wireshark.org/docs/man-pages/tshark.html)

## 7.总结

本文只是分析大部分可能会丢包节点，提供了单个节点丢包排查和相关的解决方案, 丢包问题牵扯网络链路各个组件，尤其是在云网络时代，网络拓扑复杂多变，涉及运营商网络，IDC网络，专线等underlay网络，边界网关，VPC网络，CLB负载均衡等云上overlay网络，各种丢包问题排障起来非常复杂且困难，但掌握网络通信基本原理后，可以分解网络拓扑，对通信节点进行逐一排查，也可以找到丢包位置，后续会更加深入介绍云计算时代，云上网络丢包排查方法，网络架构解析等，达到任何丢包问题都可以快速排查和定位解决，帮助客户快速恢复业务，下期再会。

原文地址：https://zhuanlan.zhihu.com/p/502027581

作者：linux

# 【NO.348】Linux进程地址空间与进程内存布局详解

## **1.进程空间分布概述**

对于一个进程，其空间分布如下图所示：

![img](https://pic3.zhimg.com/80/v2-6be4316d656d82282974fd348cedfb22_720w.webp)

**程序段(Text):**程序代码在内存中的映射，存放函数体的二进制代码。

**初始化过的数据(Data):**在程序运行初已经对变量进行初始化的数据。

**未初始化过的数据(BSS):**在程序运行初未对变量进行初始化的数据。

**栈 (Stack):**存储局部、临时变量，函数调用时，存储函数的返回指针，用于控制函数的调用和返回。在程序块开始时自动分配内存,结束时自动释放内存，其操作方式类似于数据结构中的栈。

**堆 (Heap):**存储动态内存分配,需要程序员手工分配,手工释放.注意它与数据结构中的堆是两回事，分配方式类似于链表。

**注：**1.Text, BSS, Data段在编译时已经决定了进程将占用多少VM

可以通过size，知道这些信息：

\2. 正常情况下，Linux进程不能对用来存放程序代码的内存区域执行写操作，即程序代码是以只读的方式加载到内存中，但它可以被多个进程安全的共享。

## **2.内核空间和用户空间**

Linux的虚拟地址空间范围为0～4G，Linux内核将这4G字节的空间分为两部分， 将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF）供内核使用，称为“内核空间”。而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF）供各个进程使用，称为“用户空间。因为每个进程可以通过系统调用进入内核，因此，Linux内核由系统内的所有进程共享。于是，从具体进程的角度来看，每个进程可以拥有4G字节的虚拟空间。

Linux使用两级保护机制：0级供内核使用，3级供用户程序使用，每个进程有各自的私有用户空间（0～3G），这个空间对系统中的其他进程是不可见的，最高的1GB字节虚拟内核空间则为所有进程以及内核所共享。 
内核空间中存放的是内核代码和数据，而进程的用户空间中存放的是用户程序的代码和数据。不管是内核空间还是用户空间，它们都处于虚拟空间中。 虽然内核空间占据了每个虚拟空间中的最高1GB字节，但映射到物理内存却总是从最低地址（0x00000000），另外， 使用虚拟地址可以很好的保护 内核空间被用户空间破坏，虚拟地址到物理地址转换过程有操作系统和CPU共同完成(操作系统为CPU设置好页表，CPU通过MMU单元进行地址转换)。

**注**：多任务操作系统中的每一个进程都运行在一个属于它自己的内存沙盒中，这个 沙盒就是虚拟地址空间（virtual address space），在32位模式下，它总是一个4GB的内存地址块。这些虚拟地址通过页表（page table）映射到物理内存，页表由操作系统维护并被处理器引用。每个进程都拥有一套属于它自己的页表。

进程内存空间分布如下图所示：

![img](https://pic4.zhimg.com/80/v2-2d31e707f6670ab981c4e0716456591b_720w.webp)

通常32位Linux内核地址空间划分0~3G为用户空间，3~4G为内核空间

**注**: 1.这里是32位内核地址空间划分，64位内核地址空间划分是不同的

\2. 现代的操作系统都处于32位保护模式下。每个进程一般都能寻址4G的物理空间。但是我们的物理内存一般都是几百M，进程怎么能获得4G 的物理空间呢？这就是使用了虚拟地址的好处，通常我们使用一种叫做虚拟内存的技术来实现，因为可以使用硬盘中的一部分来当作内存使用 。

![img](https://pic3.zhimg.com/80/v2-1f31b9c8de4bf414624170c5e6eb354e_720w.webp)

Linux系统对自身进行了划分，一部分核心软件独立于普通应用程序，运行在较高的特权级别上，它们驻留在被保护的内存空间上，拥有访问硬件设备的所有权限，Linux将此称为内核空间。
相对地，应用程序则是在“用户空间”中运行。运行在用户空间的应用程序只能看到允许它们使用的部分系统资源，并且不能使用某些特定的系统功能，也不能直接访问内核空间和硬件设备，以及其他一些具体的使用限制。
将用户空间和内核空间置于这种非对称访问机制下有很好的安全性，能有效抵御恶意用户的窥探，也能防止质量低劣的用户程序的侵害，从而使系统运行得更稳定可靠。

内核空间在页表中拥有较高的特权级（ring2或以下），因此只要用户态的程序试图访问这些页，就会导致一个页错误（page fault）。在Linux中，内核空间是持续存在的，并且在所有进程中都映射到同样的物理内存，内核代码和数据总是可寻址的，随时准备处理中断和系统调用。与之相反，用户模式地址空间的映射随着进程切换的发生而不断的变化，如下图所示：

![img](https://pic3.zhimg.com/80/v2-ba911e470742281847407be0800f4cea_720w.webp)

上图中蓝色区域表示映射到物理内存的虚拟地址，而白色区域表示未映射的部分。可以看出，Firefox使用了相当多的虚拟地址空间，因为它占用内存较多。

## **3.进程内存布局**

Linux进程标准的内存段布局，如下图所示，地址空间中的各个条带对应于不同的内存段（memory segment），如：堆、栈之类的。

![img](https://pic3.zhimg.com/80/v2-9268ae5118fd1989d3e7606f2ad46bae_720w.webp)

![img](https://pic1.zhimg.com/80/v2-c1ae5ac03fdbc6c0cff74b55de620e20_720w.webp)

**注**：这些段只是简单的虚拟内存地址空间范围，与Intel处理器的段没有任何关系。

几乎每个进程的虚拟地址空间中各段的分布都与上图完全一致， 这就给远程发掘程序漏洞的人打开了方便之门。一个发掘过程往往需要引用绝对内存地址：栈地址，库函数地址等。远程攻击者必须依赖地址空间分布的一致性，来探索出这些地址。如果让他们猜个正着，那么有人就会被整了。因此，地址空间的随机排布方式便逐渐流行起来，Linux通过对栈、内存映射段、堆的起始地址加上随机的偏移量来打乱布局。但不幸的是，32位地址空间相当紧凑，这给随机化所留下的空间不大，削弱了这种技巧的效果。

**栈**

进程地址空间中最顶部的段是栈，大多数编程语言将之用于存储函数参数和局部变量。调用一个方法或函数会将一个新的栈帧（stack frame）压入到栈中，这个栈帧会在函数返回时被清理掉。由于栈中数据严格的遵守FIFO的顺序，这个简单的设计意味着不必使用复杂的数据结构来追踪栈中的内容，只需要一个简单的指针指向栈的顶端即可，因此压栈（pushing）和退栈（popping）过程非常迅速、准确。进程中的每一个线程都有属于自己的栈。

通过不断向栈中压入数据，超出其容量就会耗尽栈所对应的内存区域，这将触发一个页故障（page fault），而被Linux的expand_stack()处理，它会调用acct_stack_growth()来检查是否还有合适的地方用于栈的增长。如果栈的大小低于RLIMIT_STACK（通常为8MB），那么一般情况下栈会被加长，程序继续执行，感觉不到发生了什么事情。这是一种将栈扩展到所需大小的常规机制。然而，如果达到了最大栈空间的大小，就会栈溢出（stack overflow），程序收到一个段错误（segmentation fault）。

**注**:动态栈增长是唯一一种访问未映射内存区域而被允许的情形，其他任何对未映射内存区域的访问都会触发页错误，从而导致段错误。一些被映射的区域是只读的，因此企图写这些区域也会导致段错误。

**内存映射段**

在栈的下方是内存映射段，内核将文件的内容直接映射到内存。任何应用程序都可以通过Linux的mmap()系统调用或者Windows的CreateFileMapping()/MapViewOfFile()请求这种映射。内存映射是一种方便高效的文件I/O方式，所以它被用来加载动态库。创建一个不对应于任何文件的匿名内存映射也是可能的，此方法用于存放程序的数据。在Linux中，如果你通过malloc()请求一大块内存，C运行库将会创建这样一个匿名映射而不是使用堆内存。“大块”意味着比MMAP_THRESHOLD还大，缺省128KB，可以通过mallocp()调整。

**堆**

与栈一样，堆用于运行时内存分配；但不同的是，堆用于存储那些生存期与函数调用无关的数据。大部分语言都提供了堆管理功能。在C语言中，堆分配的接口是malloc()函数。如果堆中有足够的空间来满足内存请求，它就可以被语言运行时库处理而不需要内核参与，否则，堆会被扩大，通过brk()系统调用来分配请求所需的内存块。堆管理是很复杂的，需要精细的算法来应付我们程序中杂乱的分配模式，优化速度和内存使用效率。处理一个堆请求所需的时间会大幅度的变动。实时系统通过特殊目的分配器来解决这个问题。堆在分配过程中可能会变得零零碎碎，如下图所示：

![img](https://pic3.zhimg.com/80/v2-7e00f9959267247bcff8a1fe99fea25e_720w.webp)

一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收 。注意它与数据结构中的堆是两回事，分配方式类似于链表。

**BBS和数据段**

在C语言中，BSS和数据段保存的都是静态（全局）变量的内容。区别在于BSS保存的是未被初始化的静态变量内容，他们的值不是直接在程序的源码中设定的。BSS内存区域是匿名的，它不映射到任何文件。如果你写static intcntActiveUsers，则cntActiveUsers的内容就会保存到BSS中去。

**数据段保存在源代码中已经初始化的静态变量的内容。数据段不是匿名的，它映射了一部分的程序二进制镜像，也就是源代码中指定了初始值的静态变量。**所以，如果你写static int cntActiveUsers=10，则cntActiveUsers的内容就保存在了数据段中，而且初始值是10。尽管数据段映射了一个文件，但它是一个私有内存映射，这意味着更改此处的内存不会影响被映射的文件。

你可以通过阅读文件/proc/pid_of_process/maps来检验一个Linux进程中的内存区域。记住：一个段可能包含许多区域。比如，每个内存映射文件在mmap段中都有属于自己的区域，动态库拥有类似BSS和数据段的额外区域。有时人们提到“数据段”，指的是全部的数据段+BSS+堆。

你还可以通过nm和objdump命令来察看二进制镜像，打印其中的符号，它们的地址，段等信息。最后需要指出的是，前文描述的虚拟地址布局在linux中是一种“灵活布局”，而且作为默认方式已经有些年头了，它假设我们有值RLIMT_STACK。但是，当没有该值得限制时，Linux退回到“经典布局”，如下图所示：

![img](https://pic1.zhimg.com/80/v2-bf4b7a692d98b1c46694ca8fc74bc158_720w.webp)

C语言程序实例分析如下所示：

```text
	 #include<stdio.h>  
	 #include <malloc.h>  
	   
	 void print(char *,int);  
	 int main()  
	{  
	      char *s1 = "abcde";  //"abcde"作为字符串常量存储在常量区 s1、s2、s5拥有相同的地址
	      char *s2 = "abcde";  
	      char s3[] = "abcd";  
	      long int *s4[100];  
	      char *s5 = "abcde";  
	      int a = 5;  
	      int b =6;//a,b在栈上，&a>&b地址反向增长  
	   
	     printf("variables address in main function: s1=%p  s2=%p s3=%p s4=%p s5=%p a=%p b=%p \n",   
	             s1,s2,s3,s4,s5,&a,&b); 
	     printf("variables address in processcall:n");  
         print("ddddddddd",5);//参数入栈从右至左进行,p先进栈,str后进 &p>&str  
	     printf("main=%p print=%p \n",main,print);  
	     //打印代码段中主函数和子函数的地址，编译时先编译的地址低，后编译的地址高main<print  
	 }  
 
	 void print(char *str,int p)  
	{  
	     char *s1 = "abcde";  //abcde在常量区，s1在栈上  
	     char *s2 = "abcde";  //abcde在常量区，s2在栈上 s2-s1=6可能等于0，编译器优化了相同的常量，只在内存保存一份  
	     //而&s1>&s2  
	     char s3[] = "abcdeee";//abcdeee在常量区，s3在栈上，数组保存的内容为abcdeee的一份拷贝  
	    long int *s4[100];  
	     char *s5 = "abcde";  
	     int a = 5;  
	     int b =6;  
	     int c;  
	     int d;           //a,b,c,d均在栈上，&a>&b>&c>&d地址反向增长  
	    char *q=str; 
	    int m=p;         
	    char *r=(char *)malloc(1);  
	    char *w=(char *)malloc(1) ;  // r<w 堆正向增长  
	  
	    printf("s1=%p s2=%p s3=%p s4=%p s5=%p a=%p b=%p c=%p d=%p str=%p q=%p p=%p m=%p r=%p w=%p \n",  
	            s1,s2,s3,s4,s5,&a,&b,&c,&d,&str,q,&p,&m,r,w); 
		/* 栈和堆是在程序运行时候动态分配的，局部变量均在栈上分配。
		    栈是反向增长的，地址递减；malloc等分配的内存空间在堆空间。堆是正向增长的，地址递增。  
			r,w变量在栈上(则&r>&w)，r,w所指内容在堆中(即r<w)。*/ 
	 } 
```

**附录：**

栈与堆的区别



![img](https://pic3.zhimg.com/80/v2-8c840fd0c0ffa528abebc1be14d2a71e_720w.webp)

原文地址：https://zhuanlan.zhihu.com/p/348171413

作者：linux

# 【NO.349】浅谈有栈协程与无栈协程

如今虽不敢说协程已经是红的发紫，但确实是越来越受到了大家的重视。Golang中的已经是只有goroutine，以至于很多go程序员是只知有协程，不知有线程了。就连C++也在最新的C++20中原生支持协程。更不用说很多活跃的语言如python，java等，也都是支持协程的。尽管这些协程可能名称不同，甚至用法也不同，但它们都可以被划分为两大类，一类是有(stackful) 协程，例如 goroutine，libco；一类是无栈 (stackless) 协程，例如C++的协程。

这里我们想说的一点是**所谓的有栈，无栈并不是说这个协程运行的时候有没有栈，而是说协程之间是否存在调用栈（callbackStack）**。其实仔细一想即可，但凡是个正在运行的程序，不管你是协程也好，线程也好，怎么可能在运行的时候不使用栈空间呢，调用参数往哪搁，局部变量往哪搁。我们知道基本所有的主流语言在调用另外一个函数的时候都存在一个调用栈，我们来解释一下调用栈这个词：

![img](https://pic4.zhimg.com/80/v2-f6b66fa3498ea7cab40e16e046af924b_720w.webp)


这幅图是有两个栈帧的调用栈，我在这篇文章中对栈帧下过定义，即：**函数的栈帧是指esp和ebp之间的一块地址**。拿上图来说ebp存储着Frame Pointer指向的地址，Return Address当然就是我们在执行完最新的栈帧以后下一步要执行的指令地址。esp当然就是当前指向栈顶的指针了。

## 1.有栈协程

很多地方又把协程称为subroutine，subroutine是什么，就是函数。上古时期的计算机科学家们早就给出了概念，**coroutine就是可以中断并恢复执行的subroutine**，从这个角度来看协程拥有调用栈并不是一个奇怪的事情。我们再来思考coroutine与subroutinue相比有什么区别，你会发现区别仅有一个，就是coroutinue可以中断并恢复，对应的操作就是yield/resume，这样看来subroutinue不过是coroutinue的一个子集罢了。也就是说把协程当做一个特殊的函数调用，有栈协程就是我们理想中协程该有的模样。

既然把其当做一个特殊的函数调用，对我们来说最严峻的挑战就是如何像切换函数一样去切换协程，难点在于除了像函数一样切换出去，还要在某种条件满足的时候切换回来，我们的做法可以是在协程内部存储自身的上下文，并在需要切换的时候把上下文切换就可以了，我们知道上下文其实本质上就是寄存器，所以保存上下文实际上就是把寄存器的值保存下来，有两种方法，一种是使用汇编，libco就使用了这种方法。还有一种是使用ucontext.h，这个封装好的库也可以帮我们完成相关工作。

汇编的话我们来看一看libco中对于32位机器的上下文切换操作是如何完成的：

```text
// 获取第一个参数
    movl 4(%esp), %eax 
    // 参数的类型我们暂且理解为一个拥有八个指针的数组，即regs
	| regs[7] |
	| regs[6] |
	| regs[5] |
	| regs[4] |
	| regs[3] |
	| regs[2] |
	| regs[1] |
	| regs[0] |
	--------------   <---EAX

    movl %esp,  28(%eax)  
    movl %ebp, 24(%eax)
    movl %esi, 20(%eax)
    movl %edi, 16(%eax)
    movl %edx, 12(%eax)
    movl %ecx, 8(%eax)
    movl %ebx, 4(%eax)
	// 想想看，这里eax加偏移不就是对应了regs中的值吗？这样就把所有寄存器中的值保存在了参数中
 
	
	// ESP偏移八位就是第二个参数的偏移了，这样我们就可以把第二个参数regs中的上下文切换到寄存器中了
    movl 8(%esp), %eax 
    movl 4(%eax), %ebx
    movl 8(%eax), %ecx
    movl 12(%eax), %edx  
    movl 16(%eax), %edi
    movl 20(%eax), %esi
    movl 24(%eax), %ebp
    movl 28(%eax), %esp

	ret
	// 这样我们就完成了一次协程的切换
```

我们可以看到其实就是参数中传入两个协程的上下文结构，然后第一个参数执行保存上下文，然后把第二个参数的上下文存入寄存器，这样就执行了两个协程的切换。

当然我们上面提到了调用栈，那么既然有调用栈，那么肯定有一个执行的顺序，即一定要把栈顶的协程全部运行完才可以运行下一层的协程，这样说可能比较抽象，我们举一个简单的例子：

主协程A中执行协程B，此时调用栈是在[A，B]和[A]之间切换，因为B会主动让出执行权，然后调用栈上此时就只有一个A了

B协程中执行C,D协程,此时调用栈是在[A，B，C]，[A，B]，[A，B，D]之间转换的，

这样看来我们总是只能在调用栈顶的协程运行完以后才能去执行更低一层的协程，当然，这也是典型的**非对称协程，即协程之间有明显的调用关系**。

当然在我的描述中也可以看出有栈协程涉及到对于寄存器的保存和修改，也涉及到对每一个协程栈（实际运行的栈）的分配。对于寄存器来说，现代寄存器基本都是上百个字节的数据，还有每一个协程的栈，如果选择了共享栈，又涉及到对栈上数据的拷贝，显然在效率上来说相比无栈协程的确是有一些损失的。

## 2.无栈协程

那么所谓的无栈协程是什么呢？其实**无栈协程的本质就是一个状态机（state machine）**，它可以理解为在另一个角度去看问题，即**同一协程协程的切换本质不过是指令指针寄存器的改变**。这里推荐一篇文章，其内容是用C语言实现一个协程，其实就是一个无栈协程的实现。

我们来看一个使用libco的协程的例子，当然libco是一个有栈协程：

```text
void* test(void* para){
	co_enable_hook_sys();
	int i = 0;
	poll(0, 0, 0. 1000); // 协程切换执行权，1000ms后返回
	i++;
	poll(0, 0, 0. 1000); // 协程切换执行权，1000ms后返回
	i--;
	return 0;
}

int main(){
	stCoRoutine_t* routine;
	co_create(&routine, NULL, test, 0);// 创建一个协程
	co_resume(routine); 
	co_eventloop( co_get_epoll_ct(),0,0 );
	return 0;
}
```

这段代码实际的意义就是主协程跑一个协程去执行test函数，在test中我们需要两次从协程中切换出去，这里对应了两个poll操作（hook机制，有兴趣的朋友可以点击这里），hook后的poll所做的事情就是把当前协程的CPU执行权切换到调用栈的上一层，并在超时或注册的fd就绪时返回（当然样例这里就只是超时了）。那么无栈协程跑相同的代码是怎么样的呢？其实就是翻译成类似于以下代码：

```text
struct test_coroutine {
    int i;
    int __state = 0;
    void MoveNext() {
        switch(__state) {
        case 0:
            return frist();
        case 1:
            return second();
        case 2:
        	return third();
        }
    }
    void frist() {
        i = 0;
        __state = 1;
    }
    void second() {
        i++;
        _state = 2;
    }
    void third() {
    	i--;
    }
};
```

我们可以看到相比与有栈协程中的test函数，这里把整个协程抽象成一个类，以原本需要执行切换的语句处为界限，把函数划分为几个部分，并在某一个部分执行完以后进行状态转移，在下一次调用此函数的时候就会执行下一部分，这样的话我们就完全没有必要像有栈协程那样显式的执行上下文切换了，我们只需要一个简易的调度器来调度这些函数即可。

从执行时栈的角度来看，其实所有的协程共用的都是一个栈，即系统栈，也就也不必我们自行去给协程分配栈，因为是函数调用，我们当然也不必去显示的保存寄存器的值，而且**相比有栈协程把局部变量放在新开的空间上，无栈协程直接使用系统栈使得CPU cache局部性更好，同时也使得无栈协程的中断和函数返回几乎没有区别**，这样也可以凸显出无栈协程的高效。

## 3.对称协程与非对称协程

其实对于“对称”这个名词，阐述的实际是协程之间的关系，用大白话来说就是对称协程就是说协程之间人人平等，没有谁调用谁一说，大家都是一样的，而非对称协程就是协程之间存在明显的调用关系。

简单来说就是这样：

- 对称协程 Symmetric Coroutine：任何一个协程都是相互独立且平等的，调度权可以在任意协程之间转移。
- 非对称协程 Asymmetric Coroutine：协程出让调度权的目标只能是它的调用者，即协程之间存在调用和被调用关系。

其实两者的实现我觉得其实差异不大，非对称协程其实就是拥有调用栈，而非对称协程则是大家都平等，不需要调用栈，只需要一个数据结构存储所有未执行完的协程即可。至于哪种更优？我觉得分情况，如果你使用协程的目的是为了优化一些IO密集型应用，那么协程切换出去的时候就是它等待事件到来的时候，此时你就算切换过去也没有什么意义，还不如等到事件到来的时候自动切换回去。

其实上面说的是有一些问题，因为这个执行权的切换实际上是（调用者–被调用者）之间的切换，对称就是它们之间都是平等的，就是假如A协程执行了B，C协程，那么B协程可以切换回A，也可以切换回C。而非对称只能是B切换回A，A切换回C，C再切换回A，以此类推。

这样看起来显然非对称协程相比之下更为符合我们的认知，因为对称协程目前我不知道如何选择一个合适的协程来获得CPU执行权，正如上面所说，此协程可能正在等待事件。当然如果调度算法足够优秀的话，对称协程也是可取的。

原文地址：https://zhuanlan.zhihu.com/p/347445164

作者：Linux

# 【NO.350】Nginx 性能优化（吐血总结）

## 1.性能优化考虑点

当我需要进行性能优化时，说明我们服务器无法满足日益增长的业务。性能优化是一个比较大的课题，需要从以下几个方面进行探讨

- 当前系统结构瓶颈
- 了解业务模式
- 性能与安全

### 1.1 当前系统结构瓶颈

首先需要了解的是当前系统瓶颈，用的是什么，跑的是什么业务。里面的服务是什么样子，每个服务最大支持多少并发。比如针对Nginx而言，我们处理静态资源效率最高的瓶颈是多大？

可以通过查看当前cpu负荷，内存使用率，进程使用率来做简单判断。还可以通过操作系统的一些工具来判断当前系统性能瓶颈，如分析对应的日志，查看请求数量。

也可以通过nginx http_stub_status_module模块来查看对应的连接数，总握手次数，总请求数。也可以对线上进行压力测试，来了解当前的系统的性能，并发数，做好性能评估。

### 1.2 了解业务模式

虽然我们是在做性能优化，但还是要熟悉业务，最终目的都是为业务服务的。我们要了解每一个接口业务类型是什么样的业务，比如电子商务抢购模式，这种情况平时流量会很小，但是到了抢购时间，流量一下子就会猛涨。也要了解系统层级结构，每一层在中间层做的是代理还是动静分离，还是后台进行直接服务。需要我们对业务接入层和系统层次要有一个梳理

### 1.3 性能与安全

性能与安全也是一个需要考虑的因素，往往大家注重性能忽略安全或注重安全又忽略性能。比如说我们在设计防火墙时，如果规则过于全面肯定会对性能方面有影响。如果对性能过于注重在安全方面肯定会留下很大隐患。所以大家要评估好两者的关系，把握好两者的孰重孰轻，以及整体的相关性。权衡好对应的点。

## 2.系统与Nginx性能优化

大家对相关的系统瓶颈及现状有了一定的了解之后，就可以根据影响性能方面做一个全体的评估和优化。

- 网络（网络流量、是否有丢包，网络的稳定性都会影响用户请求）
- 系统（系统负载、饱和、内存使用率、系统的稳定性、硬件磁盘是否有损坏）
- 服务（连接优化、内核性能优化、http服务请求优化都可以在nginx中根据业务来进行设置）
- 程序（接口性能、处理请求速度、每个程序的执行效率）
- 数据库、底层服务

上面列举出来每一级都会有关联，也会影响整体性能，这里主要关注的是Nginx服务这一层。

### 2.1 文件句柄

linux/Unix上，一切皆文件，每一次用户发起请求就会生成一个文件句柄，文件句柄可以理解为就是一个索引，所以文件句柄就会随着请求量的增多，而进程调用的频率增加，文件句柄的产生就越多，系统对文件句柄默认的限制是1024个，对Nginx来说非常小了，需要改大一点

**（1）设置方式**

- 系统全局性修改
- 用户局部性修改
- 进程局部性修改

（2）系统全局性修改和用户局部性修改

```text
vim /etc/security/limits.conf
```

在End of file前面添加4个参数

![img](https://pic2.zhimg.com/80/v2-a40ab09229527a47632b52f105b6c8c9_720w.webp)

- soft：软控制，到达设定值后，操作系统不会采取措施，只是发提醒
- hard：硬控制，到达设定值后，操作系统会采取机制对当前进程进行限制，这个时候请求就会受到影响
- root：这里代表root用户（系统全局性修改）
- *：代表全局，即所有用户都受此限制（用户局部性修改）
- nofile：指限制的是文件数的配置项。后面的数字即设定的值，一般设置10000左右

尤其在企业新装的系统，这个地方应该根据实际情况进行设置，可以设置全局的，也可以设置用户级别的

**（3）进程局部性修改**

```text
vim /etc/nginx/nginx.conf
```

每个进程的最大文件打开数，所以最好与ulimit -n的值保持一致。

```text
worker_rlimit_nofile 35535; #进程限制
```

![img](https://pic3.zhimg.com/80/v2-797db8e4186ca36d83d0ccb0cb96229e_720w.webp)

### 2.2 cpu的亲和配置

cpu的亲和能够使nginx对于不同的work工作进程绑定到不同的cpu上面去。就能够减少在work间不断切换cpu，把进程通常不会在处理器之间频繁迁移，进程迁移的频率小，来减少性能损耗。

![img](https://pic3.zhimg.com/80/v2-d46fcec7e3c0eb31d8ca80ab0b732e7e_720w.webp)

**（1）具体设置**

Nginx运行工作进程个数一般设置CPU的核心或者核心数x2。如果不了解cpu的核数，可以top命令之后按1看出来，也可以查看/proc/cpuinfo文件 grep ^processor /proc/cpuinfo | wc -l。

```text
[root@lx~]# vi/usr/local/nginx1.10/conf/nginx.conf
worker_processes 4;
[root@lx~]# /usr/local/nginx1.10/sbin/nginx-s reload
[root@lx~]# ps -aux | grep nginx |grep -v grep
root 9834 0.0 0.0 47556 1948 ?     Ss 22:36 0:00 nginx: master processnginx
www 10135 0.0 0.0 50088 2004 ?       S   22:58 0:00 nginx: worker process
www 10136 0.0 0.0 50088 2004 ?       S   22:58 0:00 nginx: worker process
www 10137 0.0 0.0 50088 2004 ?       S   22:58 0:00 nginx: worker process
www 10138 0.0 0.0 50088 2004 ?       S   22:58 0:00 nginx: worker process
```

比如4核配置：

```text
worker_processes 4;
worker_cpu_affinity 0001 0010 0100 1000
```

比如8核配置：

```text
worker_processes 8;
worker_cpu_affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000;
```

worker_processes最多开启8个，8个以上性能提升不会再提升了，而且稳定性变得更低，所以8个进程够用了。

为每个进程分配cpu，上例中将8个进程分配到8个cpu，当然可以写多个，或者将一个进程分配到多个cpu。

在nginx 1.9版本之后，就帮我们自动绑定了cpu;

```text
worker_cpu_affinity auto;
```

**（2）相关命令**

查看cpu核心数

```text
cat /proc/cpuinfo|grep "cpu cores"|uniq
```

显示物理cpu数量：

```text
cat /proc/cpuinfo | grep "physical id"|sort|uniq|wc -l
```

查看cpu使用率

```text
top  回车后按 1
```

查看nginx使用cpu核心和对应的nginx进程号

```text
ps -eo pid,args,psr | grep [n]ginx
```

### 2.3 事件处理模型优化

nginx的连接处理机制在于不同的操作系统会采用不同的I/O模型，Linux下，nginx使用epoll的I/O多路复用模型，在freebsd使用kqueue的IO多路复用模型，在solaris使用/dev/pool方式的IO多路复用模型，在windows使用的icop等等。要根据系统类型不同选择不同的事务处理模型，我们使用的是Centos，因此将nginx的事件处理模型调整为epoll模型。

```text
events {
    worker_connections  10240;    //
    use epoll;
}
```

说明：在不指定事件处理模型时，nginx默认会自动的选择最佳的事件处理模型服务。

### 2.4 设置work_connections 连接数

```text
 worker_connections  10240;
```

### 2.5 keepalive timeout会话保持时间

```text
keepalive_timeout  60;
```

### 2.6 GZIP压缩性能优化

```text
gzip on;       #表示开启压缩功能
gzip_min_length  1k; #表示允许压缩的页面最小字节数，页面字节数从header头的Content-Length中获取。默认值是0，表示不管页面多大都进行压缩，建议设置成大于1K。如果小于1K可能会越压越大
gzip_buffers     4 32k; #压缩缓存区大小
gzip_http_version 1.1; #压缩版本
gzip_comp_level 6; #压缩比率， 一般选择4-6，为了性能gzip_types text/css text/xml application/javascript;　　#指定压缩的类型 gzip_vary on;　#vary header支持
```

### 2.7 proxy超时设置

```text
proxy_connect_timeout 90;
proxy_send_timeout  90;
proxy_read_timeout  4k;
proxy_buffers 4 32k;
proxy_busy_buffers_size 64k
```

### 2.8 高效传输模式

```text
sendfile on; # 开启高效文件传输模式。
tcp_nopush on; #需要在sendfile开启模式才有效，防止网路阻塞，积极的减少网络报文段的数量。将响应头和正文的开始部分一起发送，而不一个接一个的发送。
```

### 2.9 Linux系统内核层面

Nginx要达到最好的性能，出了要优化Nginx服务本身之外，还需要在nginx的服务器上的内核参数。

这些参数追加到/etc/sysctl.conf,然后执行sysctl -p 生效。

1）调节系统同时发起的tcp连接数

```text
net.core.somaxconn = 262144
```

2）允许等待中的监听

```text
net.core.somaxconn = 4096 
```

3） tcp连接重用

```text
net.ipv4.tcp_tw_recycle = 1 
net.ipv4.tcp_tw_reuse = 1   
```

4）不抵御洪水攻击

```text
net.ipv4.tcp_syncookies = 0  
net.ipv4.tcp_max_orphans = 262144  #该参数用于设定系统中最多允许存在多少TCP套接字不被关联到任何一个用户文件句柄上，主要目的为防止Ddos攻击
```

5）最大文件打开数

在命令行中输入如下命令，即可设置Linux最大文件打开数。

```text
ulimit -n 30000
```

以上，就把Nginx服务器高性能优化的配置介绍完了，大家可以根据我提供的方法，每个参数挨个设置一遍，看看相关的效果。这些都是一点点试出来的，这样才能更好的理解各个参数的意义。

## 3.nginx通用配置优化

```text
#将nginx进程设置为普通用户，为了安全考虑
user nginx; 

#当前启动的worker进程，官方建议是与系统核心数一致
worker_processes 2;
#方式一，就是自动分配绑定
worker_cpu_affinity auto;

#日志配置成warn
error_log /var/log/nginx/error.log warn; 
pid /var/run/nginx.pid;

#针对 nginx 句柄的文件限制
worker_rlimit_nofile 35535;
#事件模型
events {
    #使用epoll内核模型
    use epoll;
    #每一个进程可以处理多少个连接，如果是多核可以将连接数调高 worker_processes * 1024
    worker_connections 10240;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;

    charset utf-8;  #设置字符集

    #设置日志输出格式，根据自己的情况设置
    log_format  main  '$http_user_agent' '$remote_addr - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent" "$http_x_forwarded_for" '
                      '"$args" "$request_uri"';

    access_log  /var/log/nginx/access.log  main;

    sendfile        on;   #对静态资源的处理比较有效
    #tcp_nopush     on;   #如果做静态资源服务器可以打开

    keepalive_timeout  65; 

    ########
    #Gzip module
    gzip  on;    #文件压缩默认可以打开

    include /etc/nginx/conf.d/*.conf;
}
```

## 4.实战配置

1、整体配置

```text
worker_processes  1;
pid  /var/run/nginx.pid;

events {
    worker_connections  2048;
	multi_accept on;
	use epoll;
}

http {
    include       /etc/nginx/mime.types;
    default_type  application/octet-stream;
					  
	log_format main '{"@timestamp":"$time_iso8601",'   
	'"host":"$server_addr",'
	'"clientip":"$remote_addr",'
	'"size":$body_bytes_sent,'
	'"responsetime":$request_time,'
	'"upstreamtime":"$upstream_response_time",'
	'"upstreamhost":"$upstream_addr",'
	'"http_host":"$host",'
	'"url":"$uri",'
	'"xff":"$http_x_forwarded_for",'
	'"referer":"$http_referer",'
	'"agent":"$http_user_agent",'
	'"status":"$status"}';
	
	sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
	
    server_names_hash_bucket_size 128;
    server_names_hash_max_size 512;
    keepalive_timeout  65;
    client_header_timeout 15s;
    client_body_timeout 15s;
    send_timeout 60s;
	
	limit_conn_zone $binary_remote_addr zone=perip:10m;
	limit_conn_zone $server_name zone=perserver:10m;
	limit_conn perip 2;
	limit_conn perserver 20;
	limit_rate 300k; 

    proxy_cache_path /data/nginx-cache levels=1:2 keys_zone=nginx-cache:20m max_size=50g inactive=168h;
	
	client_body_buffer_size 512k;
	client_header_buffer_size 4k;
	client_max_body_size 512k;
	large_client_header_buffers 2 8k;
	proxy_connect_timeout 5s;
	proxy_send_timeout 120s;
	proxy_read_timeout 120s;
	proxy_buffer_size 16k;
	proxy_buffers 4 64k;
	proxy_busy_buffers_size 128k;
	proxy_temp_file_write_size 128k;
	proxy_next_upstream http_502 http_504 http_404 error timeout invalid_header;
	
	gzip on;
	gzip_min_length 1k;
	gzip_buffers 4 16k;
	gzip_http_version 1.1;
	gzip_comp_level 4;
	gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;
	gzip_vary on;
	gzip_disable "MSIE [1-6].";

    include /etc/nginx/conf.d/*.conf;
}
```

2、负载均衡

```text
upstream ygoapi{ 
  server 0.0.0.0:8082 fail_timeout=5 max_fails=3;
  server 0.0.0.0:8083 fail_timeout=5 max_fails=3;
  ip_hash;
}
```

3、HTTP 配置

```text
#隐藏版本信息
server_tokens off;
server {
    listen       80;
    server_name  素材管理平台;
    charset utf-8;
	
	#重定向HTTP请求到HTTPS
	return 301 https://$server_name$request_uri;
}
```

4、HTTPS 配置

```text
准备条件：需要先去下载 HTTPS 证书
server {
    listen 443;
    server_name 素材管理平台;
    ssl on;
    ssl_certificate cert/证书名称.pem;
    ssl_certificate_key cert/证书名称.key;
    ssl_session_timeout 5m;
    # SSL协议配置
    ssl_protocols SSLv2 SSLv3 TLSv1.2;
    ssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv2:+EXP;
    ssl_prefer_server_ciphers on;
	
	valid_referers none blocked server_names
               *.ygoclub.com;
			   
	#日志配置
    access_log  /Users/jackson/Desktop/www.ygoclub.com-access.log  main gzip=4 flush=5m;
    error_log  /Users/jackson/Desktop/www.ygoclub.com-error.log  error;
	
	location ~ .*\.(eot|svg|ttf|woff|jpg|jpeg|gif|png|ico|cur|gz|svgz|mp4|ogg|ogv|webm) {
		proxy_cache nginx-cache;
		proxy_cache_valid 200 304 302 5d;
		proxy_cache_key '$host:$server_port$request_uri';
		add_header X-Cache '$upstream_cache_status from $host';
		#所有静态文件直接读取硬盘
		root /usr/share/nginx/html;
		expires 30d; #缓存30天
	}

	location ~ .*\.(js|css)?$
	{
		proxy_cache nginx-cache;
		proxy_cache_valid 200 304 302 5d;
		proxy_cache_key '$host:$server_port$request_uri';
		add_header X-Cache '$upstream_cache_status from $host';
		#所有静态文件直接读取硬盘
		root /usr/share/nginx/html;
		expires      12h;
	}

    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
    }
    
	location /druid {
        proxy_set_header X-Real-IP $remote_addr;
        proxy_cache nginx-cache;
        proxy_cache_valid 200 10m;
        proxy_pass http://ygoapi/druid;
	}
	
	location /api {
       proxy_set_header X-Real-IP $remote_addr;
       proxy_pass http://ygoapi/api;
	}
	
    # redirect server error pages to the static page /50x.html
    error_page   500 502 503 504  /50x.html;
    location = /50x.html {
        root   /usr/share/nginx/html;
    }
}
```

## 5.ab接口压力测试工具

ab是Apache超文本传输协议(HTTP)的性能测试工具。其设计意图是描绘当前所安装的Apache的执行性能，主要是显示你安装的Apache每秒可以处理多少个请求。

```text
yum install httpd-tools -y
ab -n 2000 -c 2 http://127.0.0.1/
```

- -n ：总的请求数
- -c ：并发数
- -k 是否开启长连接

![img](https://pic4.zhimg.com/80/v2-5ac2b2c212ce5885270e8f3b35650a23_720w.webp)

1、参数选项

（1）完整测试报告

这段展示的是web服务器的信息，可以看到服务器采用的是nginx，[域名是wan.bigertech.com](https://link.zhihu.com/?target=http%3A//%E5%9F%9F%E5%90%8D%E6%98%AFwan.bigertech.com)，端口是80

![img](https://pic4.zhimg.com/80/v2-d42dd87fc0fcd4207628f4e0fabe5fdb_720w.webp)

（2）服务器信息

这段是关于请求的文档的相关信息，所在位置“/”，文档的大小为338436 bytes（此为http响应的正文长度）

![img](https://pic4.zhimg.com/80/v2-35e864681af15b78a40f0085d03fede3_720w.webp)

（3）文档信息

这段展示了压力测试的几个重要指标

![img](https://pic4.zhimg.com/80/v2-bffc316bf636dfd25604dad9a1cbf203_720w.webp)

![img](https://pic1.zhimg.com/80/v2-a1b516e0cb4306f476536352593a0fd8_720w.webp)

（4）这段表示网络上消耗的时间的分解

![img](https://pic4.zhimg.com/80/v2-bc53c081e005c0bdb3f00918cbe05d0b_720w.webp)

（5）网络消耗时间

这段是每个请求处理时间的分布情况，50%的处理时间在4930ms内，66%的处理时间在5008ms内…，重要的是看90%的处理时间。

![img](https://pic2.zhimg.com/80/v2-1746332fd0e44f689b361e50681c74b1_720w.webp)

响应情况

原文地址：https://zhuanlan.zhihu.com/p/456376971

作者：linux 